,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Compound interest Differential Equation,Compound interest Differential Equation,,"A college student starts a savings account with an initial balance of $\$0$. He plans to save money at a continuous rate of $\$200$ per week. Also, at every week he plans to increase this rate by $\$10$. (ex. At the 4th month he would be saving at a rate of $\$240$ per week). Additionally, the college student finds a bank account that pays continuously compounded interest at a rate of $4\%$ per year. Estimate the time it'll take for the college student to save $\$500,000$.  Hint: set up and solve a differential equation and plot the solution to make the final estimate. My attempt: The differential equation is hard to set up. Let $S =$ amount saved. Let $t =$ time. $$\frac{dS}{dt} = \frac{0.04}{52}(200 + 10t)$$ I tried this differential equation but it doesn't satisfy the initial condition. Can someone help me come up with the differential equation? Thanks!","A college student starts a savings account with an initial balance of $\$0$. He plans to save money at a continuous rate of $\$200$ per week. Also, at every week he plans to increase this rate by $\$10$. (ex. At the 4th month he would be saving at a rate of $\$240$ per week). Additionally, the college student finds a bank account that pays continuously compounded interest at a rate of $4\%$ per year. Estimate the time it'll take for the college student to save $\$500,000$.  Hint: set up and solve a differential equation and plot the solution to make the final estimate. My attempt: The differential equation is hard to set up. Let $S =$ amount saved. Let $t =$ time. $$\frac{dS}{dt} = \frac{0.04}{52}(200 + 10t)$$ I tried this differential equation but it doesn't satisfy the initial condition. Can someone help me come up with the differential equation? Thanks!",,"['ordinary-differential-equations', 'differential']"
1,Combining two differential equations,Combining two differential equations,,"I have two differential equations that are connected by an equation, $L_1\frac{d^2I_1}{dt^2} + \frac{1}{C_1}I_1=\frac{dV}{dt}$ $L_2\frac{d^2I_2}{dt^2} + \frac{1}{C_2}I_2=\frac{dV}{dt}$ $I_1+I_2=I$ where $L_1, L_2, C_1, C_2$ are constants. As one might see, the system that is described by the differential equations consists of two LC circuits that are connected in parallel. How do I write this as one equation without $I_1$ and $I_2$? I've tried several things but I think I'm missing something, every time I end up with two equations that I can't seem to substitute in each other. Thank you. This is what I've done. The aim is to model the behavior of a circuit by V and I with a differential equation, the circuit consists of two LC circuits in parallel, this results in: Kirchhoffs laws, $I = I_1 + I_2$ $V = V_{L_1} + V_{C_1}$ $V = V_{L_2} + V_{C_2}$ Capacitor and inductor equations $I_1 = C_1 \frac{dV_{C_1}}{dt}$ $I_2 = C_2 \frac{dV_{C_2}}{dt}$ $V_{L_1} = L_1 \frac{dI_1}{dt}$ $V_{L_2} = L_2 \frac{dI_2}{dt}$ Differentiating the voltage laws and substituting for the capacitor and inductor equations results in $\frac{dV}{dt} = \frac{dV_{L_1}}{dt} + \frac{dV_{C_1}}{dt} = L_1 \frac{d^2I_1}{dt^2} + \frac{1}{C_1}I_1$ $\frac{dV}{dt} = \frac{dV_{L_2}}{dt} + \frac{dV_{C_2}}{dt} = L_2 \frac{d^2I_2}{dt^2} + \frac{1}{C_2}I_2$ Only thing that is left is to substitute with the current law, substituting with $I_2 = I - I_1$ $\frac{dV}{dt} = L_1 \frac{d^2I_1}{dt^2} + \frac{1}{C_1}I_1$ $\frac{dV}{dt} = L_2 \frac{d^2I}{dt^2} - L_2 \frac{d^2I_1}{dt^2} + \frac{1}{C_2}I - \frac{1}{C_2}I_1$ This is where I get stuck, how do I combine these two?","I have two differential equations that are connected by an equation, $L_1\frac{d^2I_1}{dt^2} + \frac{1}{C_1}I_1=\frac{dV}{dt}$ $L_2\frac{d^2I_2}{dt^2} + \frac{1}{C_2}I_2=\frac{dV}{dt}$ $I_1+I_2=I$ where $L_1, L_2, C_1, C_2$ are constants. As one might see, the system that is described by the differential equations consists of two LC circuits that are connected in parallel. How do I write this as one equation without $I_1$ and $I_2$? I've tried several things but I think I'm missing something, every time I end up with two equations that I can't seem to substitute in each other. Thank you. This is what I've done. The aim is to model the behavior of a circuit by V and I with a differential equation, the circuit consists of two LC circuits in parallel, this results in: Kirchhoffs laws, $I = I_1 + I_2$ $V = V_{L_1} + V_{C_1}$ $V = V_{L_2} + V_{C_2}$ Capacitor and inductor equations $I_1 = C_1 \frac{dV_{C_1}}{dt}$ $I_2 = C_2 \frac{dV_{C_2}}{dt}$ $V_{L_1} = L_1 \frac{dI_1}{dt}$ $V_{L_2} = L_2 \frac{dI_2}{dt}$ Differentiating the voltage laws and substituting for the capacitor and inductor equations results in $\frac{dV}{dt} = \frac{dV_{L_1}}{dt} + \frac{dV_{C_1}}{dt} = L_1 \frac{d^2I_1}{dt^2} + \frac{1}{C_1}I_1$ $\frac{dV}{dt} = \frac{dV_{L_2}}{dt} + \frac{dV_{C_2}}{dt} = L_2 \frac{d^2I_2}{dt^2} + \frac{1}{C_2}I_2$ Only thing that is left is to substitute with the current law, substituting with $I_2 = I - I_1$ $\frac{dV}{dt} = L_1 \frac{d^2I_1}{dt^2} + \frac{1}{C_1}I_1$ $\frac{dV}{dt} = L_2 \frac{d^2I}{dt^2} - L_2 \frac{d^2I_1}{dt^2} + \frac{1}{C_2}I - \frac{1}{C_2}I_1$ This is where I get stuck, how do I combine these two?",,"['ordinary-differential-equations', 'systems-of-equations', 'differential']"
2,Find a series solution to $(x^2-2)y''+6xy'+4y=0$.,Find a series solution to .,(x^2-2)y''+6xy'+4y=0,"Find a series solution to $(x^2-2)y''+6xy'+4y=0$. A. Find the recurrence relation to $a_n$: My answer is $a_{n+2}=a_n\cdot \frac{n+4}{2(n+2)}$  which is correct. B. Using A , write two independent solutions to the ODE. So, for the odd n's: $$a_3=\frac{1}{2} \frac{5}{3}a_1 \\ a_5=\frac{1}{2}\frac{7}{5}a_3=\frac{1}{2}^2\frac{7}{3}a_1 \\ a_7=\frac{1}{2}\frac{9}{7}a_5=(\frac{1}{2})^3\cdot \frac{9}{3}a_1$$ So that if I iterate right, the $2m+1$'th term is $$a_{2m+1}=(\frac{1}{2})^{m+1}\cdot (m+5)\cdot \frac{a_1}{3}$$  Which means the a solution is $$y_1(x)=\sum_{m=0}^\infty (\frac{1}{2})^{m+1}\cdot (m+5)\cdot \frac{a_1}{3}x^{2m+1}$$But, the answer in the textbook is slightly different:  $$y(x)=\sum_{m=0}^\infty \frac{2m+3}{2^m}x^{2m+1}$$ Same with even terms: I came up with $$y_2(x)=\sum_{m=0}^\infty \frac{m+2}{2^{m+1}}x^{2m}$$ and the solution I have is $$y(x)=\sum_{n=0}^\infty \frac{m+1}{2^m}x^{2m}$$ So what's is wrong in my solution? C. Find the Solution to the ODE when $y(0)=1, \ y'(0)=0$ as a series solution around $x=0$. So how do I know that $a_0=1, \ a_1=0$ and not vise versa?","Find a series solution to $(x^2-2)y''+6xy'+4y=0$. A. Find the recurrence relation to $a_n$: My answer is $a_{n+2}=a_n\cdot \frac{n+4}{2(n+2)}$  which is correct. B. Using A , write two independent solutions to the ODE. So, for the odd n's: $$a_3=\frac{1}{2} \frac{5}{3}a_1 \\ a_5=\frac{1}{2}\frac{7}{5}a_3=\frac{1}{2}^2\frac{7}{3}a_1 \\ a_7=\frac{1}{2}\frac{9}{7}a_5=(\frac{1}{2})^3\cdot \frac{9}{3}a_1$$ So that if I iterate right, the $2m+1$'th term is $$a_{2m+1}=(\frac{1}{2})^{m+1}\cdot (m+5)\cdot \frac{a_1}{3}$$  Which means the a solution is $$y_1(x)=\sum_{m=0}^\infty (\frac{1}{2})^{m+1}\cdot (m+5)\cdot \frac{a_1}{3}x^{2m+1}$$But, the answer in the textbook is slightly different:  $$y(x)=\sum_{m=0}^\infty \frac{2m+3}{2^m}x^{2m+1}$$ Same with even terms: I came up with $$y_2(x)=\sum_{m=0}^\infty \frac{m+2}{2^{m+1}}x^{2m}$$ and the solution I have is $$y(x)=\sum_{n=0}^\infty \frac{m+1}{2^m}x^{2m}$$ So what's is wrong in my solution? C. Find the Solution to the ODE when $y(0)=1, \ y'(0)=0$ as a series solution around $x=0$. So how do I know that $a_0=1, \ a_1=0$ and not vise versa?",,"['ordinary-differential-equations', 'power-series']"
3,Differential Equations: Say whether the equation has bounded solutions at $x = 0$,Differential Equations: Say whether the equation has bounded solutions at,x = 0,Its been forever since I've done Diff EQ and I can't remember how to go about solving this problem: Say whether the equation has bounded solutions at $x = 0$ and whether all solutions are bounded: $$ (x^2 + x^3)u'' + (x + x^2)u' + u = 0 $$ Any guidance would be greatly appreciated.,Its been forever since I've done Diff EQ and I can't remember how to go about solving this problem: Say whether the equation has bounded solutions at $x = 0$ and whether all solutions are bounded: $$ (x^2 + x^3)u'' + (x + x^2)u' + u = 0 $$ Any guidance would be greatly appreciated.,,['ordinary-differential-equations']
4,Laplace transform for solving differential equation ? help?,Laplace transform for solving differential equation ? help?,,"I have a differential equation: $$y'' + 4y' + 3y = 6t + 14, $$ with initial conditions $y(1) = 1,~y'(1) = 0.5$. Can I use the Laplace transform to solve this equation whose initial condition is not at $0$ ?","I have a differential equation: $$y'' + 4y' + 3y = 6t + 14, $$ with initial conditions $y(1) = 1,~y'(1) = 0.5$. Can I use the Laplace transform to solve this equation whose initial condition is not at $0$ ?",,['ordinary-differential-equations']
5,Solve $2tx'(t)-x(t)=\ln x'(t)$ [duplicate],Solve  [duplicate],2tx'(t)-x(t)=\ln x'(t),"This question already has answers here : Second-order non-linear ODE (3 answers) Closed 7 years ago . Solve $2tx'(t)-x(t)=\ln \left[x'(t)\right]$ That would be an easy Clairaut's equation if $tx'(t)$ wasn't multiplied by $2$. But unfortunately it is, and I have no idea what to do here.","This question already has answers here : Second-order non-linear ODE (3 answers) Closed 7 years ago . Solve $2tx'(t)-x(t)=\ln \left[x'(t)\right]$ That would be an easy Clairaut's equation if $tx'(t)$ wasn't multiplied by $2$. But unfortunately it is, and I have no idea what to do here.",,['ordinary-differential-equations']
6,Find the fundamental matrix of a system of ODEs?,Find the fundamental matrix of a system of ODEs?,,"To linearize a system, in one of the steps I am required to find the fundamental matrix $\Phi$(t) of a system such that $\Phi$(0)=I. The example system my professor used: $\dot{x} = x - y - x^3 - xy^2$ $\dot{y} = x + y - x^2y - y^3$ $\dot{z} = \lambda z$ Then $\vec\gamma(t) = (\cos{t}, \sin{t}, 0)^T $ is a periodic orbit of the system. We compute $D\vec f(\vec x)= \left( \begin{array}{ccc} 1-3x^2-y^2 & -1-2xy & 0 \\ 1-2xy & 1-x^2-3y^2 & 0 \\ 0 & 0 & \lambda \end{array} \right)  $ Then the linearization is given by $\frac{d}{dt}\vec x=A(t)\vec x $ where $A(t) = D\vec f(\vec\gamma(t)) =  \left( \begin{array}{ccc} -2\cos^2{t} & -1-\sin^2{t} & 0 \\ 1-\sin^2{t} & -2\sin^2{t} & 0 \\ 0 & 0 & \lambda \end{array} \right)$ This all makes sense until he says ""Clearly, this is its fundamental matrix:"" $ \Phi(t) =  \left( \begin{array}{ccc} e^{-2t}\cos{t} & -\sin{t} & 0 \\ e^{-2t}\sin{t} & \cos{t} & 0 \\ 0 & 0 & e^{\lambda t} \end{array} \right)$ Am I missing something? How does one get from the periodic matrix A(t) to the fundamental matrix?","To linearize a system, in one of the steps I am required to find the fundamental matrix $\Phi$(t) of a system such that $\Phi$(0)=I. The example system my professor used: $\dot{x} = x - y - x^3 - xy^2$ $\dot{y} = x + y - x^2y - y^3$ $\dot{z} = \lambda z$ Then $\vec\gamma(t) = (\cos{t}, \sin{t}, 0)^T $ is a periodic orbit of the system. We compute $D\vec f(\vec x)= \left( \begin{array}{ccc} 1-3x^2-y^2 & -1-2xy & 0 \\ 1-2xy & 1-x^2-3y^2 & 0 \\ 0 & 0 & \lambda \end{array} \right)  $ Then the linearization is given by $\frac{d}{dt}\vec x=A(t)\vec x $ where $A(t) = D\vec f(\vec\gamma(t)) =  \left( \begin{array}{ccc} -2\cos^2{t} & -1-\sin^2{t} & 0 \\ 1-\sin^2{t} & -2\sin^2{t} & 0 \\ 0 & 0 & \lambda \end{array} \right)$ This all makes sense until he says ""Clearly, this is its fundamental matrix:"" $ \Phi(t) =  \left( \begin{array}{ccc} e^{-2t}\cos{t} & -\sin{t} & 0 \\ e^{-2t}\sin{t} & \cos{t} & 0 \\ 0 & 0 & e^{\lambda t} \end{array} \right)$ Am I missing something? How does one get from the periodic matrix A(t) to the fundamental matrix?",,"['matrices', 'ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
7,Solving a system of first order differential equations,Solving a system of first order differential equations,,"So, I have (another) problem with differential equations (from an optimal control problem). I am trying to solve the following system of DEs (is this even a system?): $$ \lambda'(t) = r \lambda(t) + x' h(x(t)) \\ \lambda = g(x(t)) \\ \lambda(T) = 0, \quad x(0)=0 $$ with  $x: [0,T] \to \mathbb{R}_+$ and $\lambda: [0,T] \to \mathbb{R}$ So, I thought, the 2nd equation is really simple (and $g$ has nice properties, including being cont. differentiable), so I tried to just try to get one DE in $x'$ and replace $\lambda$ with $g(x)$ and $\lambda'$ with $\frac{d g(x(t))}{d t} = x'g'(x(t))$. When I now try to solve the resulting initial value problem: $$ x'(t) g'(x(t)) = r g(x(t)) + x'(t) h(x(t)), \quad x(0) = 0$$ So this is the point when I realized: something went wrong. I completely ignored $\lambda(T) = 0$ and when trying to solve an example I get a solution which seems to be for a problem without $\lambda(T) = 0$ (big surprise!), which is clearly wrong for a finite $T$. So finally my question(s): What did I do wrong? And how can I fix it? Is there a general way to solve the initial value problem I stated at the beginning for nice $g$ and $h$?","So, I have (another) problem with differential equations (from an optimal control problem). I am trying to solve the following system of DEs (is this even a system?): $$ \lambda'(t) = r \lambda(t) + x' h(x(t)) \\ \lambda = g(x(t)) \\ \lambda(T) = 0, \quad x(0)=0 $$ with  $x: [0,T] \to \mathbb{R}_+$ and $\lambda: [0,T] \to \mathbb{R}$ So, I thought, the 2nd equation is really simple (and $g$ has nice properties, including being cont. differentiable), so I tried to just try to get one DE in $x'$ and replace $\lambda$ with $g(x)$ and $\lambda'$ with $\frac{d g(x(t))}{d t} = x'g'(x(t))$. When I now try to solve the resulting initial value problem: $$ x'(t) g'(x(t)) = r g(x(t)) + x'(t) h(x(t)), \quad x(0) = 0$$ So this is the point when I realized: something went wrong. I completely ignored $\lambda(T) = 0$ and when trying to solve an example I get a solution which seems to be for a problem without $\lambda(T) = 0$ (big surprise!), which is clearly wrong for a finite $T$. So finally my question(s): What did I do wrong? And how can I fix it? Is there a general way to solve the initial value problem I stated at the beginning for nice $g$ and $h$?",,"['ordinary-differential-equations', 'systems-of-equations', 'optimal-control']"
8,Let $\eta (x)=\int_0^\infty e^{at}\xi(\phi_t(x)) dt$ then $\eta$ is a $C^1$ function,Let  then  is a  function,\eta (x)=\int_0^\infty e^{at}\xi(\phi_t(x)) dt \eta C^1,"Consider the following problem. Suppose that $a>0, r >0$ and $\xi:\mathbb R \to [o,\infty)$ is a $C^2$ which vanishes in the complement of the interval $(-r,r)$. Also suppose that $\xi(0)=\xi'(0)=0$. Show that if $r$ is sufficiently small and $\phi_t$ is the flow of the differential equation $\dot x=-ax+\xi(x)$, then $$\eta (x)=\int_0^\infty e^{at}\xi(\phi_t(x))dt$$ is a $C^1$ function. I am having trouble proving this. What I have done so far . Since $\phi_t$ is the flow of the DE then $\dot{\phi_t}=-ax+\xi(x)$. Also $\phi_0(x)=x$ by properties of flow. Since $(0,0)$ is a rest point of the given differential equation and the linearization of the system about $(0,0)$ is $\dot x=-ax$ it follows that the rest point $(0,0)$ is asymptotically stable. Thus, $\lim_{t\to \infty} \phi_t(x)=0$. Then I tried to integrate the given function by parts to see if I can explicitly find it. Here is where I got stuck. In particular I don't see how to use the 'vanishes outside the interval $(-r,r)$. (So I guess $\xi$ has compact support $[-r,r]$??. I have used up all the other hypothesis of the problem. (I think??). Should I simply use the definition for derivative instead? Even so, how do I show that the derivative is continuous? Question : Can anybody give some pointers as to what I am doing wrong/right?. In particular where does having compact support come into play?. Any hints/suggestions?","Consider the following problem. Suppose that $a>0, r >0$ and $\xi:\mathbb R \to [o,\infty)$ is a $C^2$ which vanishes in the complement of the interval $(-r,r)$. Also suppose that $\xi(0)=\xi'(0)=0$. Show that if $r$ is sufficiently small and $\phi_t$ is the flow of the differential equation $\dot x=-ax+\xi(x)$, then $$\eta (x)=\int_0^\infty e^{at}\xi(\phi_t(x))dt$$ is a $C^1$ function. I am having trouble proving this. What I have done so far . Since $\phi_t$ is the flow of the DE then $\dot{\phi_t}=-ax+\xi(x)$. Also $\phi_0(x)=x$ by properties of flow. Since $(0,0)$ is a rest point of the given differential equation and the linearization of the system about $(0,0)$ is $\dot x=-ax$ it follows that the rest point $(0,0)$ is asymptotically stable. Thus, $\lim_{t\to \infty} \phi_t(x)=0$. Then I tried to integrate the given function by parts to see if I can explicitly find it. Here is where I got stuck. In particular I don't see how to use the 'vanishes outside the interval $(-r,r)$. (So I guess $\xi$ has compact support $[-r,r]$??. I have used up all the other hypothesis of the problem. (I think??). Should I simply use the definition for derivative instead? Even so, how do I show that the derivative is continuous? Question : Can anybody give some pointers as to what I am doing wrong/right?. In particular where does having compact support come into play?. Any hints/suggestions?",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
9,general solution of elastic beam equation,general solution of elastic beam equation,,"For the following equation; $t^4 \dfrac{d^2u}{dt^2} + \lambda^2 u = 0, \quad \lambda >0, ~ t>0,$ where $u(t)$ is real valued function. by using the change of variables $t=\dfrac{1}{\tau} , \quad u(t)=\dfrac{v(\tau)}{\tau} ,$ how can we find the general solution of the above equation.","For the following equation; $t^4 \dfrac{d^2u}{dt^2} + \lambda^2 u = 0, \quad \lambda >0, ~ t>0,$ where $u(t)$ is real valued function. by using the change of variables $t=\dfrac{1}{\tau} , \quad u(t)=\dfrac{v(\tau)}{\tau} ,$ how can we find the general solution of the above equation.",,['ordinary-differential-equations']
10,Central manifold theorem => Stable/unstable manifold?,Central manifold theorem => Stable/unstable manifold?,,"I'm a bit confused why we always separate the stable/unstable manifold theorem and the central manifold theorem. The stable/unstable manifold theorem applies to a hyperbolic point ($\mathrm{Re}(\lambda)\neq 0$) and states (roughly) that there is a unique stable manifold and a unique unstable manifold, the dimensions of which corresponds to the dimensions of the stable and unstable manifolds, repectively. The center manifold states that there is a stable and an unstable manifold, unique, and a (possibly non-unique) center manifold, the dimension of which is the dimension of the center subspace (it of course also states other things which are not of interested here). Question : Is there additional information is the stable/unstable manifold theorem that is not included in the center manifold theorem, applied to the particular case when the dimension of the central manifold equals to $0$?","I'm a bit confused why we always separate the stable/unstable manifold theorem and the central manifold theorem. The stable/unstable manifold theorem applies to a hyperbolic point ($\mathrm{Re}(\lambda)\neq 0$) and states (roughly) that there is a unique stable manifold and a unique unstable manifold, the dimensions of which corresponds to the dimensions of the stable and unstable manifolds, repectively. The center manifold states that there is a stable and an unstable manifold, unique, and a (possibly non-unique) center manifold, the dimension of which is the dimension of the center subspace (it of course also states other things which are not of interested here). Question : Is there additional information is the stable/unstable manifold theorem that is not included in the center manifold theorem, applied to the particular case when the dimension of the central manifold equals to $0$?",,"['ordinary-differential-equations', 'manifolds', 'dynamical-systems']"
11,Ordinary differential equation­,Ordinary differential equation­,,$$\dfrac{dy}{dx}-\dfrac{\tan y}{1+x}=(1+x)e^x\sin y$$ I tried $\sin y=t$ but failed. It seems to immune to methods I know of or I am just unable to make the right substitution... Wolfram alpha responds with standard computation time exceeded. Any hints or suggestions will be appreciated.,$$\dfrac{dy}{dx}-\dfrac{\tan y}{1+x}=(1+x)e^x\sin y$$ I tried $\sin y=t$ but failed. It seems to immune to methods I know of or I am just unable to make the right substitution... Wolfram alpha responds with standard computation time exceeded. Any hints or suggestions will be appreciated.,,"['calculus', 'ordinary-differential-equations']"
12,Computation of the Frenet-Serret trihedron in $\Bbb L^3$ (Lorentz-Minkowski space),Computation of the Frenet-Serret trihedron in  (Lorentz-Minkowski space),\Bbb L^3,"Consider $\Bbb L^3 = (\Bbb R^3, \langle , \rangle)$, with the convention $$\langle (x_1,y_1,z_1), (x_2,y_2,z_2)\rangle = x_1x_2+y_1y_2 - z_1z_2$$ and $\| v \| = \sqrt{|\langle v, v \rangle|}$. Let $\alpha: I \subset \Bbb R\rightarrow \Bbb L^3$ be a spacelike curve, parametrized by arc-length, and $\mathbf{T}(s) := \alpha '(s)$ be the tangent vector to $\alpha$ at $s$. Suppose $\mathbf{T}'(s)$ is lightlike for all $s$. Then we define the normal vector $\mathbf{N}(s) := \mathbf{T}'(s)$, and the binormal vector $\mathbf{B}(s)$ as the unique lightlike vector orthogonal to $\mathbf{T}(s)$ such that $\langle \mathbf{N}(s), \mathbf{B}(s) \rangle = 1$. We do not define the curvature in this case. I have no problem accepting these definitions, and I can even prove the Frenet-Serret equations: $$\begin{pmatrix} \mathbf{T}' \\ \mathbf{N}' \\ \mathbf{B}' \end{pmatrix} = \begin{pmatrix} 0 & 1 & 0 \\ 0 & \tau & 0 \\ -1 & 0 & -\tau \end{pmatrix} \begin{pmatrix} \mathbf{T} \\ \mathbf{N} \\ \mathbf{B} \end{pmatrix}$$ where $\tau$ will be the torsion of the curve. However, I am having some trouble understanding how to compute the binormal vector in practical cases. Surely I could write $\alpha(s) = (x(s), y(s), z(s))$, and $\mathbf{B}(s) = (b_1(s), b_2(s), b_3(s))$, and try to solve the system $$\left\{\begin{array}{c} b_1(s)~x'(s) + b_2(s)~y'(s) - b_3(s)~z'(s) = 0 \\ b_1(s)~x''(s) + b_2(s)~y''(s) - b_3(s)~z''(s) = 1 \\ (b_1(s))^2 + (b_2(s))^2 - (b_3(s))^2 = 0 \end{array} \right.$$ I think that in general, it's very hard to solve this. Can someone give some insight about this, or make an example? I have the same problem if $\alpha$ is lightlike and pseudo-parametrized by arc-length, but if I can understand this case, I can probably manage the other one by myself. There shouldn't be much difference. Thank you! (If it happens to help, I'm using this text )","Consider $\Bbb L^3 = (\Bbb R^3, \langle , \rangle)$, with the convention $$\langle (x_1,y_1,z_1), (x_2,y_2,z_2)\rangle = x_1x_2+y_1y_2 - z_1z_2$$ and $\| v \| = \sqrt{|\langle v, v \rangle|}$. Let $\alpha: I \subset \Bbb R\rightarrow \Bbb L^3$ be a spacelike curve, parametrized by arc-length, and $\mathbf{T}(s) := \alpha '(s)$ be the tangent vector to $\alpha$ at $s$. Suppose $\mathbf{T}'(s)$ is lightlike for all $s$. Then we define the normal vector $\mathbf{N}(s) := \mathbf{T}'(s)$, and the binormal vector $\mathbf{B}(s)$ as the unique lightlike vector orthogonal to $\mathbf{T}(s)$ such that $\langle \mathbf{N}(s), \mathbf{B}(s) \rangle = 1$. We do not define the curvature in this case. I have no problem accepting these definitions, and I can even prove the Frenet-Serret equations: $$\begin{pmatrix} \mathbf{T}' \\ \mathbf{N}' \\ \mathbf{B}' \end{pmatrix} = \begin{pmatrix} 0 & 1 & 0 \\ 0 & \tau & 0 \\ -1 & 0 & -\tau \end{pmatrix} \begin{pmatrix} \mathbf{T} \\ \mathbf{N} \\ \mathbf{B} \end{pmatrix}$$ where $\tau$ will be the torsion of the curve. However, I am having some trouble understanding how to compute the binormal vector in practical cases. Surely I could write $\alpha(s) = (x(s), y(s), z(s))$, and $\mathbf{B}(s) = (b_1(s), b_2(s), b_3(s))$, and try to solve the system $$\left\{\begin{array}{c} b_1(s)~x'(s) + b_2(s)~y'(s) - b_3(s)~z'(s) = 0 \\ b_1(s)~x''(s) + b_2(s)~y''(s) - b_3(s)~z''(s) = 1 \\ (b_1(s))^2 + (b_2(s))^2 - (b_3(s))^2 = 0 \end{array} \right.$$ I think that in general, it's very hard to solve this. Can someone give some insight about this, or make an example? I have the same problem if $\alpha$ is lightlike and pseudo-parametrized by arc-length, but if I can understand this case, I can probably manage the other one by myself. There shouldn't be much difference. Thank you! (If it happens to help, I'm using this text )",,"['ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'mathematical-physics', 'semi-riemannian-geometry']"
13,Solving 1D telegrapher's equation by reduction to two-dimensional wave equation,Solving 1D telegrapher's equation by reduction to two-dimensional wave equation,,"The solution $w : \mathbb R \times \mathbb R_{+} \to \mathbb R$ of the Cauchy problem for the telegrapher's equation $$  w_{tt} - c^2 w_{xx} + c^2 \lambda^2 w = 0 $$ with $$  w(x,0) = 0, \qquad w_t(x,0) = \psi(x) $$ is given by $$  (*) \quad w(x,t) = \frac{1}{2c}\int_{x - ct}^{x+ct} J_0(\lambda s) \psi(y) dy, \quad s^2 = c^2 t^2 - (x-y)^2, $$ where $J_0$ denotes the $0$-th Bessel function with formula $$  J_0(z) = \frac{2}{\pi} \int_0^{\pi/2} \cos(z \sin \theta) d\theta. $$ Prove (*) with the approach $u(x_1, x_2, t) = \cos(\lambda x_2) w(x_1, t)$ and the solution formula for the Cauchy-Problem of the two-dimensional wave equation.","The solution $w : \mathbb R \times \mathbb R_{+} \to \mathbb R$ of the Cauchy problem for the telegrapher's equation $$  w_{tt} - c^2 w_{xx} + c^2 \lambda^2 w = 0 $$ with $$  w(x,0) = 0, \qquad w_t(x,0) = \psi(x) $$ is given by $$  (*) \quad w(x,t) = \frac{1}{2c}\int_{x - ct}^{x+ct} J_0(\lambda s) \psi(y) dy, \quad s^2 = c^2 t^2 - (x-y)^2, $$ where $J_0$ denotes the $0$-th Bessel function with formula $$  J_0(z) = \frac{2}{\pi} \int_0^{\pi/2} \cos(z \sin \theta) d\theta. $$ Prove (*) with the approach $u(x_1, x_2, t) = \cos(\lambda x_2) w(x_1, t)$ and the solution formula for the Cauchy-Problem of the two-dimensional wave equation.",,"['analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
14,"Solution to Schrödinger equation $ \partial_t f(x,t) = -\partial_x^2 f(x,t) + \delta(t)V(x)f(x,t).$",Solution to Schrödinger equation," \partial_t f(x,t) = -\partial_x^2 f(x,t) + \delta(t)V(x)f(x,t).","I want to solve  $$ i\partial_t f(x,t) = -\partial_x^2 f(x,t) + \delta(t-t_0)V(x)f(x,t),$$ for any $V \in C^{\infty}[-1,1]$ and $f: [-1,1] \times \mathbb{R_{\ge 0}} \rightarrow \mathbb{C}$. I would consider this to be solved if I have one/two ODEs that just depend on either $x$ or $t$ (or an integral transform etc.). My boundary conditions shall be taken such, that I specify a $f(.,0)$ for some $t_0 \in \mathbb{R}$. and look how this solution propagates under this ODE. The other boundary conditions shall be time-independent. I also noticed that this equation is similar to many diffusion equations and found a similar differential operator here at http://en.wikipedia.org/wiki/Green%27s_function#Table_of_Green.27s_functions This question is a particular example of my unsolved question here: https://math.stackexchange.com/questions/843106/solve-pde-by-getting-two-odes","I want to solve  $$ i\partial_t f(x,t) = -\partial_x^2 f(x,t) + \delta(t-t_0)V(x)f(x,t),$$ for any $V \in C^{\infty}[-1,1]$ and $f: [-1,1] \times \mathbb{R_{\ge 0}} \rightarrow \mathbb{C}$. I would consider this to be solved if I have one/two ODEs that just depend on either $x$ or $t$ (or an integral transform etc.). My boundary conditions shall be taken such, that I specify a $f(.,0)$ for some $t_0 \in \mathbb{R}$. and look how this solution propagates under this ODE. The other boundary conditions shall be time-independent. I also noticed that this equation is similar to many diffusion equations and found a similar differential operator here at http://en.wikipedia.org/wiki/Green%27s_function#Table_of_Green.27s_functions This question is a particular example of my unsolved question here: https://math.stackexchange.com/questions/843106/solve-pde-by-getting-two-odes",,['real-analysis']
15,Higher Order Functional Equations,Higher Order Functional Equations,,"A common point of study is the theory of functional equations first encountered in Calculus and from there built up with the calculus of finite differences (And ultimately functional analysis) which treats problems such as the one below $$ a_0(x)  + 2 f(x) + 3 f(x+1) = 0 $$ Where the objective is to find the Function $f(x)$ that satisfies the above law. Consider now the Idea that the above expression can be rewritten as: $$ g(x,f(x)) = 0 $$ Where G denotes a 'functional' of both f and x. We could very well have problems such as: $$ g(x,f(x) + 1) + 3*g(x,f(x)) + 3*g(x,f'(x+1)) = 0 $$ Where we are attempting to determine the functional g such that over all numbers x, and all functions f, the above is satisfied. Naturally this leads to increasingly complex notions of 2nd level functionals (who input first level functionals etc...) And this is where my question arises: What field of math is the theory of all the functionals, functionals of functionals etc... studied under? It seems the Calculus of Variations only explores up to the second level and not nearly as widespread over the range of equations as modern Differential and Partial Differential Equation theory are. And neither of the above 3 explore much past finite differences to other strange functional equations.","A common point of study is the theory of functional equations first encountered in Calculus and from there built up with the calculus of finite differences (And ultimately functional analysis) which treats problems such as the one below $$ a_0(x)  + 2 f(x) + 3 f(x+1) = 0 $$ Where the objective is to find the Function $f(x)$ that satisfies the above law. Consider now the Idea that the above expression can be rewritten as: $$ g(x,f(x)) = 0 $$ Where G denotes a 'functional' of both f and x. We could very well have problems such as: $$ g(x,f(x) + 1) + 3*g(x,f(x)) + 3*g(x,f'(x+1)) = 0 $$ Where we are attempting to determine the functional g such that over all numbers x, and all functions f, the above is satisfied. Naturally this leads to increasingly complex notions of 2nd level functionals (who input first level functionals etc...) And this is where my question arises: What field of math is the theory of all the functionals, functionals of functionals etc... studied under? It seems the Calculus of Variations only explores up to the second level and not nearly as widespread over the range of equations as modern Differential and Partial Differential Equation theory are. And neither of the above 3 explore much past finite differences to other strange functional equations.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'functional-equations', 'calculus-of-variations']"
16,ODE standard form.,ODE standard form.,,"I noticed that whenever mathematicians talk about Legendre polynomials they bring the ODE to the form $(1-x^2)f''(x)-2xf'(x)+n(n+1)f(x)=0$. When solving Poisson's equation, this form is not the most intuitive one, because you only get this one after substituting $x=\cos(t)$. My question is: Is there a reason Mathematicians always tend to have an ODE in a particular form and if yes: What determines this standard form? If anything is unclear, please let me know.","I noticed that whenever mathematicians talk about Legendre polynomials they bring the ODE to the form $(1-x^2)f''(x)-2xf'(x)+n(n+1)f(x)=0$. When solving Poisson's equation, this form is not the most intuitive one, because you only get this one after substituting $x=\cos(t)$. My question is: Is there a reason Mathematicians always tend to have an ODE in a particular form and if yes: What determines this standard form? If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
17,"Let $Z=Z(x,y)$ be a solution of $\frac{\partial z}{\partial x}\frac{\partial z}{\partial y}$ = 1",Let  be a solution of  = 1,"Z=Z(x,y) \frac{\partial z}{\partial x}\frac{\partial z}{\partial y}","Let $Z=Z(x,y)$ be a solution of $$\frac{\partial z}{∂x}\frac{\partial z}{\partial y} = 1$$ passing through $(0,0,0)$. Then $Z(0,1)$ is 0 1 2 4 By Charpit Method I get the solution as $Z=ax+by+c$ whare $a, b, c$ are constant and $ab=1$. since $Z$ passes through the origin then $c=0$. finally we get the solution  $Z=ax+by$. then $Z(0,1)= b$. Ather that what will I do?","Let $Z=Z(x,y)$ be a solution of $$\frac{\partial z}{∂x}\frac{\partial z}{\partial y} = 1$$ passing through $(0,0,0)$. Then $Z(0,1)$ is 0 1 2 4 By Charpit Method I get the solution as $Z=ax+by+c$ whare $a, b, c$ are constant and $ab=1$. since $Z$ passes through the origin then $c=0$. finally we get the solution  $Z=ax+by$. then $Z(0,1)= b$. Ather that what will I do?",,['ordinary-differential-equations']
18,How to solve this system of inhomogeneous differential equations,How to solve this system of inhomogeneous differential equations,,"In some past exam papers for the Maths course that I attend,I found this example and I would really appreciate if someone looked at my solution. It goes like this: Find general solution to  $$ y_1' = 2 y_1 - y_2 + (x+1) e^{3x}, \\ y_2' = y_1 + 4 y_2 + 2 x e^{3x}. $$ First of all, I set up the fundamental matrix and found its eigenvalues. I got that the matrix has repeated roots namely $r=3$. As I could not find two linearly independent eigenvectors (I had just one), I used this method I had found online to obtain another eigenvector given just one (not convinced that the method is $100 \%$ legitimate though). My eigenvectors are thus $v_1 = (1,-1)$ and $v_2 = (0,-1)$. Therefore my homogeneous solution matrix is  $$ y  = c_1 e^{3x} v_1 + c_2 (x e^{3x} v_1 + e^{3x} v_2). $$ Then, I searched for the inverse of this matrix and got $$ \left( \array{(1+x) e^{-3x} &x e^{-3x} \\ -e^{-3x} &-e^{-3x}} \right) $$ Multiplying this matrix with the inhomogeneous matrix $( (x+1) e^{3x} , 2 x e^{3x})$ I obtained $(3 x^2x+1, -3x-1)$, which upon integrating, I got $(x^3 +x^2+x, -\frac{3}{2} x-x)$. Adding this matrix and the homogeneous matrix, I obtained the general solution. Is this correct?","In some past exam papers for the Maths course that I attend,I found this example and I would really appreciate if someone looked at my solution. It goes like this: Find general solution to  $$ y_1' = 2 y_1 - y_2 + (x+1) e^{3x}, \\ y_2' = y_1 + 4 y_2 + 2 x e^{3x}. $$ First of all, I set up the fundamental matrix and found its eigenvalues. I got that the matrix has repeated roots namely $r=3$. As I could not find two linearly independent eigenvectors (I had just one), I used this method I had found online to obtain another eigenvector given just one (not convinced that the method is $100 \%$ legitimate though). My eigenvectors are thus $v_1 = (1,-1)$ and $v_2 = (0,-1)$. Therefore my homogeneous solution matrix is  $$ y  = c_1 e^{3x} v_1 + c_2 (x e^{3x} v_1 + e^{3x} v_2). $$ Then, I searched for the inverse of this matrix and got $$ \left( \array{(1+x) e^{-3x} &x e^{-3x} \\ -e^{-3x} &-e^{-3x}} \right) $$ Multiplying this matrix with the inhomogeneous matrix $( (x+1) e^{3x} , 2 x e^{3x})$ I obtained $(3 x^2x+1, -3x-1)$, which upon integrating, I got $(x^3 +x^2+x, -\frac{3}{2} x-x)$. Adding this matrix and the homogeneous matrix, I obtained the general solution. Is this correct?",,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'systems-of-equations']"
19,Analytical solutions of Thomas Fermi equation,Analytical solutions of Thomas Fermi equation,,The Thomas Fermi model of atoms and nuclei is used in many applications of atomic and nuclear physics. The ODE related to this model is: $$\frac{d^2}{dx^2}\phi(x)=x^{-\frac{1}{2}}\phi(x)^{3/2}$$ with boundary conditions: $$\phi(0)=1$$ and: $$\phi(\infty)=0$$ The best attempt to solve it is given by Majorana: http://arxiv.org/pdf/physics/0111167v1.pdf but unfortunatly also Majorana wasn't able to solve it analytically. Is there today some advanced method to find an analytical solution of this equation or the only possible way to solve it is a numerical method? Thanks.,The Thomas Fermi model of atoms and nuclei is used in many applications of atomic and nuclear physics. The ODE related to this model is: $$\frac{d^2}{dx^2}\phi(x)=x^{-\frac{1}{2}}\phi(x)^{3/2}$$ with boundary conditions: $$\phi(0)=1$$ and: $$\phi(\infty)=0$$ The best attempt to solve it is given by Majorana: http://arxiv.org/pdf/physics/0111167v1.pdf but unfortunatly also Majorana wasn't able to solve it analytically. Is there today some advanced method to find an analytical solution of this equation or the only possible way to solve it is a numerical method? Thanks.,,"['ordinary-differential-equations', 'boundary-value-problem']"
20,Door mechanism differential equation,Door mechanism differential equation,,"I have been wondering about a door mechanism I have seen. It has a wire attached to the upper corner of the door and from there to the corresponding corner in the door frame, where a weight hangs from it on a ""frictionless"" roller. So when the door is opened the weight will rise and then be pulled down by gravity when you let go of the door, closing it. Considering a door with width $L$ and moment of inertia $J$ what is  the angle of the door $A$ as a function of time with some initial angle $A_0$? So after doing some math I get to $A''(t)=k\cos(A(t)/2)$ where $k$ is a bunch of constants How do I solve that? Thanks","I have been wondering about a door mechanism I have seen. It has a wire attached to the upper corner of the door and from there to the corresponding corner in the door frame, where a weight hangs from it on a ""frictionless"" roller. So when the door is opened the weight will rise and then be pulled down by gravity when you let go of the door, closing it. Considering a door with width $L$ and moment of inertia $J$ what is  the angle of the door $A$ as a function of time with some initial angle $A_0$? So after doing some math I get to $A''(t)=k\cos(A(t)/2)$ where $k$ is a bunch of constants How do I solve that? Thanks",,"['ordinary-differential-equations', 'classical-mechanics']"
21,Solving a differential equation using Laplace transform,Solving a differential equation using Laplace transform,,"The problem has two parts: 1. Solve the initial value problem: $$ y''+y=\sum_{j=0}^\infty \delta_{2j\pi}(t) $$ with the initial conditions: $y(0)=y'(0)=0$ 2.Show that if $2n\pi<t<2(n+1)\pi$ for some integer n, then $y(t)=(n+1)sin(t)$. I only managed to partially solve the first part. I got: $$\mathcal{L} \{y(t)\}=\frac{e^{2\pi s}}{(e^{2\pi s}-1)(s^{2}+1)}$$ I could really use some help finding the inverse Laplace transform of this and solving the second part. Thank you in advance!","The problem has two parts: 1. Solve the initial value problem: $$ y''+y=\sum_{j=0}^\infty \delta_{2j\pi}(t) $$ with the initial conditions: $y(0)=y'(0)=0$ 2.Show that if $2n\pi<t<2(n+1)\pi$ for some integer n, then $y(t)=(n+1)sin(t)$. I only managed to partially solve the first part. I got: $$\mathcal{L} \{y(t)\}=\frac{e^{2\pi s}}{(e^{2\pi s}-1)(s^{2}+1)}$$ I could really use some help finding the inverse Laplace transform of this and solving the second part. Thank you in advance!",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
22,How do I solve this differential equation?,How do I solve this differential equation?,,"$y^{(7)}+4y^{(6)}+8y^{(5)}+9y^{(4)}+8y^{(3)}+8y^{(2)}+8y^{(1)}+4y=e^{-x} (5sinx-cosx) $ The characteristic equation $ \lambda ^7 +4\lambda ^6+8\lambda ^5+ 9\lambda ^4 +8\lambda ^3+8\lambda ^2+8\lambda +4= (\lambda ^2 +2\lambda +2)^2(\lambda ^3+1) $  has equal roots $\lambda _1=\lambda _2= i-1 ; \lambda _3=\lambda_4=-i-1 ; \lambda_5 =-1 ; \lambda_6= \frac{1}{2}+\frac{i\sqrt 3}{2}  ; \lambda _7= \frac{1}{2}-\frac{i\sqrt 3}{2} $ Hence, I can write solution of the homogeneous equation. Then , because $\lambda_1 $ and  are $\lambda_3 $ double roots $e^{-x} (5sinx-cosx) $ , I'm looking for  solutions form : $ y^{**}(t)=t^2(a \cos t+b \sin t)$ Does anybody can do it faster than expected $7$ derivative $y^{**}(t)$ ?","$y^{(7)}+4y^{(6)}+8y^{(5)}+9y^{(4)}+8y^{(3)}+8y^{(2)}+8y^{(1)}+4y=e^{-x} (5sinx-cosx) $ The characteristic equation $ \lambda ^7 +4\lambda ^6+8\lambda ^5+ 9\lambda ^4 +8\lambda ^3+8\lambda ^2+8\lambda +4= (\lambda ^2 +2\lambda +2)^2(\lambda ^3+1) $  has equal roots $\lambda _1=\lambda _2= i-1 ; \lambda _3=\lambda_4=-i-1 ; \lambda_5 =-1 ; \lambda_6= \frac{1}{2}+\frac{i\sqrt 3}{2}  ; \lambda _7= \frac{1}{2}-\frac{i\sqrt 3}{2} $ Hence, I can write solution of the homogeneous equation. Then , because $\lambda_1 $ and  are $\lambda_3 $ double roots $e^{-x} (5sinx-cosx) $ , I'm looking for  solutions form : $ y^{**}(t)=t^2(a \cos t+b \sin t)$ Does anybody can do it faster than expected $7$ derivative $y^{**}(t)$ ?",,['ordinary-differential-equations']
23,Closed form for a sequence defined recursively,Closed form for a sequence defined recursively,,"Let $a_k$ be a sequence such that $a_0=0, a_1=0, a_2=1, a_3=1$ and $$a_{k+4}=-\frac{a_{k}+ka_{k+2}}{(k+1)(k+2)}$$ for $k\ge 0$. My question is: Is a closed form formula for $a_n, n\ge 4$ possible? This problem arises from my attempt to  find the power series solution for $y''+xy'+x^2y=0$.","Let $a_k$ be a sequence such that $a_0=0, a_1=0, a_2=1, a_3=1$ and $$a_{k+4}=-\frac{a_{k}+ka_{k+2}}{(k+1)(k+2)}$$ for $k\ge 0$. My question is: Is a closed form formula for $a_n, n\ge 4$ possible? This problem arises from my attempt to  find the power series solution for $y''+xy'+x^2y=0$.",,"['sequences-and-series', 'ordinary-differential-equations', 'recurrence-relations']"
24,Show that $x' = Ax$ is an attractor if end only if there is a quadratic form $q$ positive definite such that $Dq(x) . Ax < 0$ for all $x \neq 0$,Show that  is an attractor if end only if there is a quadratic form  positive definite such that  for all,x' = Ax q Dq(x) . Ax < 0 x \neq 0,Show that $x' = Ax$ is an attractor if end only if there is a quadratic form $q$ positive definite such that $$Dq(x) . Ax < 0$$ for all $x \neq 0$ Definition: a linear system $x' = Ax$ called attractor if for all $x \in \mathbb{R^n}$ called $$\lim_{t \rightarrow \infty}  e^{tA} = 0$$,Show that $x' = Ax$ is an attractor if end only if there is a quadratic form $q$ positive definite such that $$Dq(x) . Ax < 0$$ for all $x \neq 0$ Definition: a linear system $x' = Ax$ called attractor if for all $x \in \mathbb{R^n}$ called $$\lim_{t \rightarrow \infty}  e^{tA} = 0$$,,"['ordinary-differential-equations', 'exponentiation', 'exponential-function']"
25,Finding the Asymptotic Curves of a Given Surface,Finding the Asymptotic Curves of a Given Surface,,"I have to find the asymptotic curves of the surface given by $$z = a \left( \frac{x}{y} + \frac{y}{x} \right),$$ for constant $a \neq 0$. I guess that what was meant by that statement is that surface $S$ can be locally parametrized by $$X(u,v) = \left( u, v, a \left( \frac{u}{v} + \frac{u}{v} \right) \right).$$  Do you think that my parametrization is correct (meaning that I read the description of the surface correctly), and do you know of a more convenient parametrization? Assuming that parametrization, I derived the following ($E$, $F$, $G$, are the coefficients of the first fundamental form; $e$, $f$, $g$ are coefficients of the second fundamental form; $N$ is the normal vector to surface $S$ at a point; these quantities are all functions of local coordinates $(u,v)$): $$E = 1 + a^2 \left( \frac{1}{v} - \frac{v}{u^2} \right)^2,$$  $$F = -\frac{a^2 (u^2 - v^2)^2}{u^3 v^3},$$ $$G = 1 + a^2 \left( \frac{1}{u} - \frac{u}{v^2} \right)^2.$$ $$N = \frac{1}{\sqrt{E G - F^2}} \left( a \left( \frac{v}{u^2}-\frac{1}{v} \right), a \left( \frac{u}{v^2}-\frac{1}{u} \right), 1 \right).$$ $$X_{u,u} = \left( 0,0, \frac{2 a v}{u^3} \right), X_{u,v} = \left( 0,0, -a \left( \frac{1}{u^2} + \frac{1}{v^2} \right) \right), X_{v,v} = \left( 0, 0, \frac{2 a u}{v^3} \right).$$ $$e = \frac{2 a v}{u^3 \sqrt{E G - F^2}},$$ $$f = - \frac{a (\frac{1}{u^2} + \frac{1}{v^2})}{\sqrt{E G - F^2}},$$ $$g = \frac{2 a u}{v3 \sqrt{E G - F^2}}.$$ Thus, the Gaussian curvature (from these calculations) is: $$K = -\frac{a^2 u^4 v^4 (u^2 - v^2)^2}{(u^4 v^4 +    a^2 (u^2 - v^2)^2 (u^2 + v^2))^2}.$$ And the mean curvature would be: $$H = \frac{a u^3 v^3 (u^4 + v^4)}{(u^4 v^4 + a^2 (u^2 - v^2)^2 (u^2 + v^2))^{3/2  }}.$$ So, the principal curvatures are: $$k_{\pm} = H \pm \sqrt{H^2 - K} = a u^2 v^2 \frac{u v (u^4 + v^4) \pm \sqrt{(u^2 + v^2) (a^2 (u^2 - v^2)^4 + u^2 v^2 (u^6 + v^6))}}{(u^4 v^4 + a^2 (u^2 - v^2)^2 (u^2 + v^2))^{3/2}}.$$ In order to find the asymptotic curves, but trying to avoid the differential equation, I was hoping to find the angles $\theta (u,v)$ such that the normal curvature would always be $0$.  In other words I was trying: $0 = k_n = k_{+} \cos{(\theta)}^2 + k_{-} \sin{(\theta)}^2$, and solving for $\theta$. Assuming sufficient niceness, this calculate would result in: $$(u v (u^4 + v^4) + \sqrt{(u^2 + v^2) (a^2 (u^2 - v^2)^4 + u^2 v^2 (u^6 + v^6))}) \cos{(\theta)}^2 + (u v (u^4 + v^4) - \sqrt{(u^2 + v^2) (a^2 (u^2 - v^2)^4 + u^2 v^2 (u^6 + v^6))}) \sin{(\theta)}^2$$ First of all, is this approach (solving for $\theta$ rather than solving the differential equation) valid? If it is, after I find that angle $\theta$, determined by location $(u,v)$ on $S$, what more work do I have to do? How do I find the equations for the asymptotic curves based on this angle? If this whole method was for naught, how does one solve the differential equation. in this case, of: $$e (u')^2 + 2f u' v' + g (v')^2 = 2a v^4 (u')^2 - 2a u^3 v^3 \left( \frac{1}{u^2} + \frac{1}{v^2} \right)u' v' + 2a u^4 (v')^2 = 0?$$ (Again, assuming sufficient niceness.) (See: https://math.stackexchange.com/questions/762195/differential-equation-for-the-asymptotic-directions-of-a-given-surface ) Thank you!","I have to find the asymptotic curves of the surface given by $$z = a \left( \frac{x}{y} + \frac{y}{x} \right),$$ for constant $a \neq 0$. I guess that what was meant by that statement is that surface $S$ can be locally parametrized by $$X(u,v) = \left( u, v, a \left( \frac{u}{v} + \frac{u}{v} \right) \right).$$  Do you think that my parametrization is correct (meaning that I read the description of the surface correctly), and do you know of a more convenient parametrization? Assuming that parametrization, I derived the following ($E$, $F$, $G$, are the coefficients of the first fundamental form; $e$, $f$, $g$ are coefficients of the second fundamental form; $N$ is the normal vector to surface $S$ at a point; these quantities are all functions of local coordinates $(u,v)$): $$E = 1 + a^2 \left( \frac{1}{v} - \frac{v}{u^2} \right)^2,$$  $$F = -\frac{a^2 (u^2 - v^2)^2}{u^3 v^3},$$ $$G = 1 + a^2 \left( \frac{1}{u} - \frac{u}{v^2} \right)^2.$$ $$N = \frac{1}{\sqrt{E G - F^2}} \left( a \left( \frac{v}{u^2}-\frac{1}{v} \right), a \left( \frac{u}{v^2}-\frac{1}{u} \right), 1 \right).$$ $$X_{u,u} = \left( 0,0, \frac{2 a v}{u^3} \right), X_{u,v} = \left( 0,0, -a \left( \frac{1}{u^2} + \frac{1}{v^2} \right) \right), X_{v,v} = \left( 0, 0, \frac{2 a u}{v^3} \right).$$ $$e = \frac{2 a v}{u^3 \sqrt{E G - F^2}},$$ $$f = - \frac{a (\frac{1}{u^2} + \frac{1}{v^2})}{\sqrt{E G - F^2}},$$ $$g = \frac{2 a u}{v3 \sqrt{E G - F^2}}.$$ Thus, the Gaussian curvature (from these calculations) is: $$K = -\frac{a^2 u^4 v^4 (u^2 - v^2)^2}{(u^4 v^4 +    a^2 (u^2 - v^2)^2 (u^2 + v^2))^2}.$$ And the mean curvature would be: $$H = \frac{a u^3 v^3 (u^4 + v^4)}{(u^4 v^4 + a^2 (u^2 - v^2)^2 (u^2 + v^2))^{3/2  }}.$$ So, the principal curvatures are: $$k_{\pm} = H \pm \sqrt{H^2 - K} = a u^2 v^2 \frac{u v (u^4 + v^4) \pm \sqrt{(u^2 + v^2) (a^2 (u^2 - v^2)^4 + u^2 v^2 (u^6 + v^6))}}{(u^4 v^4 + a^2 (u^2 - v^2)^2 (u^2 + v^2))^{3/2}}.$$ In order to find the asymptotic curves, but trying to avoid the differential equation, I was hoping to find the angles $\theta (u,v)$ such that the normal curvature would always be $0$.  In other words I was trying: $0 = k_n = k_{+} \cos{(\theta)}^2 + k_{-} \sin{(\theta)}^2$, and solving for $\theta$. Assuming sufficient niceness, this calculate would result in: $$(u v (u^4 + v^4) + \sqrt{(u^2 + v^2) (a^2 (u^2 - v^2)^4 + u^2 v^2 (u^6 + v^6))}) \cos{(\theta)}^2 + (u v (u^4 + v^4) - \sqrt{(u^2 + v^2) (a^2 (u^2 - v^2)^4 + u^2 v^2 (u^6 + v^6))}) \sin{(\theta)}^2$$ First of all, is this approach (solving for $\theta$ rather than solving the differential equation) valid? If it is, after I find that angle $\theta$, determined by location $(u,v)$ on $S$, what more work do I have to do? How do I find the equations for the asymptotic curves based on this angle? If this whole method was for naught, how does one solve the differential equation. in this case, of: $$e (u')^2 + 2f u' v' + g (v')^2 = 2a v^4 (u')^2 - 2a u^3 v^3 \left( \frac{1}{u^2} + \frac{1}{v^2} \right)u' v' + 2a u^4 (v')^2 = 0?$$ (Again, assuming sufficient niceness.) (See: https://math.stackexchange.com/questions/762195/differential-equation-for-the-asymptotic-directions-of-a-given-surface ) Thank you!",,"['ordinary-differential-equations', 'differential-geometry', 'surfaces']"
26,Existence and uniqueness of initial value problem in differential equation,Existence and uniqueness of initial value problem in differential equation,,"consider the following equation: $$ y'=y^{\frac{1}{3}}, \,y(0)=0 $$ My question is how can I prove the existence and uniqueness of solutions of this initial value problem without solving the equation? Can anyone help me with this? Thanks!","consider the following equation: $$ y'=y^{\frac{1}{3}}, \,y(0)=0 $$ My question is how can I prove the existence and uniqueness of solutions of this initial value problem without solving the equation? Can anyone help me with this? Thanks!",,['ordinary-differential-equations']
27,Cauchy-Euler Equation of order $n$,Cauchy-Euler Equation of order,n,"What I wish to prove is that for a Cauchy-Euler equation of order $n$, the substitution $x=e^{t}$ transforms it into a linear differential equation with constant coefficients. To put it as a theorem: Consider the Euler equation of order $n$   \begin{equation} x^{n}y^{(n)}+a_{n-1}x^{n-1}y^{(n-1)}+\cdots+a_{1}xy'+a_{0}y=0 \end{equation}   The substitution $x=e^{t}$ transforms this into a linear differential equations with constant coefficients. What I have tried I have been drawn to use a inductive proof. Having examined the first three derivatives I am lead to believe that the $n$:th derivative (with substitution $x=e^{t}$) can be written as $y^{(n)}=e^{-nt}\prod_{k=1}^{n}(D-k+1)y$, where $D$ denotes the differential operator. For the base case $n=1$ we have  \begin{equation} \frac{dy}{dt}=\frac{dy}{dx}\frac{dx}{dt}=\frac{dy}{dx}e^{t}=\frac{dy}{dx}x \iff\frac{1}{x}\frac{dy}{dt}=\frac{dy}{dx} \end{equation} which means that  \begin{equation} a_{1}x\frac{dy}{dx}+a_{0}y=0 \iff a_{1}\frac{dy}{dt}+a_{0}y=0 \end{equation} As such, the base case clearly transforms into a linear differential equation. Now, suppose that the substitution $x=e^{t}$ transforms the $n-1$ first derivatives into a linear combination. We wish to show that this implies that $n$:th derivative can also be transformed into a linear combination \begin{align} \frac{d^{n}y}{dx^{n}}&=\frac{dy}{dx}\left[ \frac{d^{n-1}y}{dx^{n-1}} \right] \\ &=\frac{d}{dx}\left[e^{-(n-1)t}\prod_{k=1}^{n-1}(D-k+1)y  \right] \\ &=\frac{d/dt}{e^{t}}\left[e^{-(n-1)} \right] \\ &=e^{-t}\left[-(n-1)e^{-(n-1)t}\prod_{k=1}^{n-1}(D-k+1)y+e^{-(n-1)t}\frac{d}{dt}\prod_{k=1}^{n-1}(D-k+1)y \right] \\ &= \cdots \\ &=e^{-nt}\prod_{k=1}^{n}(D-k+1)y \end{align} I am not completely sure as to how I should complete the dotted line; what exactly does the derivative of the product of differential operators become?","What I wish to prove is that for a Cauchy-Euler equation of order $n$, the substitution $x=e^{t}$ transforms it into a linear differential equation with constant coefficients. To put it as a theorem: Consider the Euler equation of order $n$   \begin{equation} x^{n}y^{(n)}+a_{n-1}x^{n-1}y^{(n-1)}+\cdots+a_{1}xy'+a_{0}y=0 \end{equation}   The substitution $x=e^{t}$ transforms this into a linear differential equations with constant coefficients. What I have tried I have been drawn to use a inductive proof. Having examined the first three derivatives I am lead to believe that the $n$:th derivative (with substitution $x=e^{t}$) can be written as $y^{(n)}=e^{-nt}\prod_{k=1}^{n}(D-k+1)y$, where $D$ denotes the differential operator. For the base case $n=1$ we have  \begin{equation} \frac{dy}{dt}=\frac{dy}{dx}\frac{dx}{dt}=\frac{dy}{dx}e^{t}=\frac{dy}{dx}x \iff\frac{1}{x}\frac{dy}{dt}=\frac{dy}{dx} \end{equation} which means that  \begin{equation} a_{1}x\frac{dy}{dx}+a_{0}y=0 \iff a_{1}\frac{dy}{dt}+a_{0}y=0 \end{equation} As such, the base case clearly transforms into a linear differential equation. Now, suppose that the substitution $x=e^{t}$ transforms the $n-1$ first derivatives into a linear combination. We wish to show that this implies that $n$:th derivative can also be transformed into a linear combination \begin{align} \frac{d^{n}y}{dx^{n}}&=\frac{dy}{dx}\left[ \frac{d^{n-1}y}{dx^{n-1}} \right] \\ &=\frac{d}{dx}\left[e^{-(n-1)t}\prod_{k=1}^{n-1}(D-k+1)y  \right] \\ &=\frac{d/dt}{e^{t}}\left[e^{-(n-1)} \right] \\ &=e^{-t}\left[-(n-1)e^{-(n-1)t}\prod_{k=1}^{n-1}(D-k+1)y+e^{-(n-1)t}\frac{d}{dt}\prod_{k=1}^{n-1}(D-k+1)y \right] \\ &= \cdots \\ &=e^{-nt}\prod_{k=1}^{n}(D-k+1)y \end{align} I am not completely sure as to how I should complete the dotted line; what exactly does the derivative of the product of differential operators become?",,"['ordinary-differential-equations', 'proof-writing', 'induction', 'self-learning']"
28,Counterexample to Peano's theorem in infinite dimension,Counterexample to Peano's theorem in infinite dimension,,"Would you like a counter example that Peano's theorem does not apply to spaces with infinite dimension. Peano theorem: Let E be a space with finite dimension, consider a point $(t_0,x_0) \in \Re \times E$, constants $ a, b > $ 0 and a apliacação $$F: [t_0 - a, t_0 + a] \times B_b[x_0] \longrightarrow E$$ For every $ M> $ 0 checking: $$sup {||F(t,x)||;(t,x) \in [t_0 - a, t_0 + a] \times B_b[x_0]} < M$$ the Cauchy problem: $$x'(t)=F(t,x(t))$$ $$x(t_0)=x_0$$ admits at least one solution in inetrvalo: $$[t_0 - \min(a,\frac{b}{M}),t_0 + \min(a,\frac{b}{M}) $$ Will be of great help. Thank you very much.","Would you like a counter example that Peano's theorem does not apply to spaces with infinite dimension. Peano theorem: Let E be a space with finite dimension, consider a point $(t_0,x_0) \in \Re \times E$, constants $ a, b > $ 0 and a apliacação $$F: [t_0 - a, t_0 + a] \times B_b[x_0] \longrightarrow E$$ For every $ M> $ 0 checking: $$sup {||F(t,x)||;(t,x) \in [t_0 - a, t_0 + a] \times B_b[x_0]} < M$$ the Cauchy problem: $$x'(t)=F(t,x(t))$$ $$x(t_0)=x_0$$ admits at least one solution in inetrvalo: $$[t_0 - \min(a,\frac{b}{M}),t_0 + \min(a,\frac{b}{M}) $$ Will be of great help. Thank you very much.",,"['analysis', 'ordinary-differential-equations']"
29,Predictor-Corrector for Adams-Moulton,Predictor-Corrector for Adams-Moulton,,What is the order of the corrector of Adams-Moulton type required in order to apply Milne's method for estimating the error in PECE mode? Find the coefficient of the leading term in the truncation error for the third order implicit Adams-Moulton linear multistep scheme \begin{equation} y_{n+3}=y_{n+2}+ \frac{h}{12}(5 f_{n+3}+8f_{n+2}-f_{n+1})  \end{equation} and deduce from the notes the value of Milne's error estimate of the error in this case. => first part of this question really confuse me. I know for the order of corrector of PECE is 3 for Adams-Moulton but I really don't know how to apply Milne's method to estimate the error in PECE mode. now for the second part of question Third order implicit Adams-Moulton linear multistep scheme \begin{equation} y_{n+3}=y_{n+2}+ \frac{h}{12}(5 f_{n+3}+8f_{n+2}-f_{n+1})  \end{equation} The LTE is given by $T_n$ After calculating in paper with massive cancellation I have got LTE as \begin{equation} hT_n= \left(\frac{27}{8} -\frac{2}{3}-\frac{11}{4}\right) h^4 y_{iv}+O(h^4)+O(h^5) \end{equation} \begin{equation} hT_n= -\frac{1}{24} h^4 y_{iv}+O(h^4)+O(h^5)\end{equation} \begin{equation} T_n= -\frac{1}{24} h^3 y_{iv}+O(h^3)+O(h^4) \end{equation} Therefore the coefficient of leading term is $C_{p}=-\frac{1}{24}$ For the last part of the question Milne's error estimate is given by \begin{equation} e_{n+1}= C_{p} h^{p+1}y^{p+1}+O(h^{p+2}) \end{equation} \begin{equation} e_{n+1}= -\frac{1}{24} h^3 y^{4}+O(h^5)\end{equation} Someone please kindly  check my solution and reply me if I got wrong.,What is the order of the corrector of Adams-Moulton type required in order to apply Milne's method for estimating the error in PECE mode? Find the coefficient of the leading term in the truncation error for the third order implicit Adams-Moulton linear multistep scheme \begin{equation} y_{n+3}=y_{n+2}+ \frac{h}{12}(5 f_{n+3}+8f_{n+2}-f_{n+1})  \end{equation} and deduce from the notes the value of Milne's error estimate of the error in this case. => first part of this question really confuse me. I know for the order of corrector of PECE is 3 for Adams-Moulton but I really don't know how to apply Milne's method to estimate the error in PECE mode. now for the second part of question Third order implicit Adams-Moulton linear multistep scheme \begin{equation} y_{n+3}=y_{n+2}+ \frac{h}{12}(5 f_{n+3}+8f_{n+2}-f_{n+1})  \end{equation} The LTE is given by $T_n$ After calculating in paper with massive cancellation I have got LTE as \begin{equation} hT_n= \left(\frac{27}{8} -\frac{2}{3}-\frac{11}{4}\right) h^4 y_{iv}+O(h^4)+O(h^5) \end{equation} \begin{equation} hT_n= -\frac{1}{24} h^4 y_{iv}+O(h^4)+O(h^5)\end{equation} \begin{equation} T_n= -\frac{1}{24} h^3 y_{iv}+O(h^3)+O(h^4) \end{equation} Therefore the coefficient of leading term is $C_{p}=-\frac{1}{24}$ For the last part of the question Milne's error estimate is given by \begin{equation} e_{n+1}= C_{p} h^{p+1}y^{p+1}+O(h^{p+2}) \end{equation} \begin{equation} e_{n+1}= -\frac{1}{24} h^3 y^{4}+O(h^5)\end{equation} Someone please kindly  check my solution and reply me if I got wrong.,,"['calculus', 'algebra-precalculus', 'analysis', 'ordinary-differential-equations', 'mathematical-modeling']"
30,Differential Equation has a unique solution periodic,Differential Equation has a unique solution periodic,,"Let $A(t)$ continuous and periodic of period $S$ in $\mathbb{R}$. Suppose $x' = Ax$ has $\varphi \equiv 0$ as the only periodic solution of period $S$. Show that there exists $\delta> 0$ such that for every continuous function $f:\mathbb{R} \times E \longrightarrow E $, $S$ period of periodic with variable the first $|D_2f(t,x)|<\delta$ to all $(t,x)$ so $x' = A(t)x + f(t,x)$ has a unique periodic solution $\varphi_f$ of period $S$ also prove that if $f\rightarrow0$ uniformly then $\varphi \rightarrow 0$ uniformly.","Let $A(t)$ continuous and periodic of period $S$ in $\mathbb{R}$. Suppose $x' = Ax$ has $\varphi \equiv 0$ as the only periodic solution of period $S$. Show that there exists $\delta> 0$ such that for every continuous function $f:\mathbb{R} \times E \longrightarrow E $, $S$ period of periodic with variable the first $|D_2f(t,x)|<\delta$ to all $(t,x)$ so $x' = A(t)x + f(t,x)$ has a unique periodic solution $\varphi_f$ of period $S$ also prove that if $f\rightarrow0$ uniformly then $\varphi \rightarrow 0$ uniformly.",,"['analysis', 'ordinary-differential-equations', 'periodic-functions']"
31,AUTO Software for ODE's: references or forums?,AUTO Software for ODE's: references or forums?,,"I'm learning the AUTO software that does numerical continuation of ODE's by following these two references: The official manual found here www.dam.brown.edu/.../auto07p.pdf Lecture notes found at indy.cs.concordia.ca/auto/notes.pdf The examples are understandable, but going from there to defining your own problem presents a very steep learning curve. So I'm looking for other good references, or perhaps even an online forum for people using this software. Any suggestions? Cheers!","I'm learning the AUTO software that does numerical continuation of ODE's by following these two references: The official manual found here www.dam.brown.edu/.../auto07p.pdf Lecture notes found at indy.cs.concordia.ca/auto/notes.pdf The examples are understandable, but going from there to defining your own problem presents a very steep learning curve. So I'm looking for other good references, or perhaps even an online forum for people using this software. Any suggestions? Cheers!",,"['ordinary-differential-equations', 'numerical-methods', 'math-software', 'bifurcation']"
32,Are any solutions lost when solving non-exact differential equations?,Are any solutions lost when solving non-exact differential equations?,,"I have just started studying differential equations, one of the problems I found while I was practicing is ""Consider the equation  $$ (y^2 + 2xy)dx - x^2 dy=0  $$ (a) Show that this equation is not exact. (b) Show that multiplying both sides of the equation by $$ 1/y^2 $$ yields a new equation that is exact. (c) Use the solution of the resulting exact equation to solve the original equation. (d) Were any solutions lost in the process? I've worked it out and found that the general solution is: $$ y= x^2 / c-x $$ What I don't understand is (d). What is meant by ""Any solutions lost in the process""? How would a solution be lost, and why? And what am I missing? Thanks.","I have just started studying differential equations, one of the problems I found while I was practicing is ""Consider the equation  $$ (y^2 + 2xy)dx - x^2 dy=0  $$ (a) Show that this equation is not exact. (b) Show that multiplying both sides of the equation by $$ 1/y^2 $$ yields a new equation that is exact. (c) Use the solution of the resulting exact equation to solve the original equation. (d) Were any solutions lost in the process? I've worked it out and found that the general solution is: $$ y= x^2 / c-x $$ What I don't understand is (d). What is meant by ""Any solutions lost in the process""? How would a solution be lost, and why? And what am I missing? Thanks.",,['ordinary-differential-equations']
33,Method of dominant balance and perturbation theory,Method of dominant balance and perturbation theory,,"We know perturbation theory express the desired solution of differential equations in terms of a formal power series in some ""small"" perturbation parameters: $y=y_0+\epsilon ^1 y_1+\epsilon ^2 y_2+\cdots$ Where $y_0$ is the solution to the exactly solvable system obtained by dropping out terms with $\epsilon$ in the original equation. On the other hand, the method of dominant balance seems don't require the existence of small parameter $\epsilon$. It substitutes $y=e^{s(x)}$ into the equation and drop the negligible terms to get the dominant part $s_0(x)$. And does it iteratively with the substitution like $y(x)\sim e^{s_0(x)+c(x)}$ to get sub-dominant parts. Unlike perturbation theory, at every step we must re-evaluate the equation to determine the dominant terms (since the substitutions are different, we get different equations every time). Here comes my question$-$is there any way to view the method of dominant balance as part of the perturbation theory? Or what is the connection between these two methods?","We know perturbation theory express the desired solution of differential equations in terms of a formal power series in some ""small"" perturbation parameters: $y=y_0+\epsilon ^1 y_1+\epsilon ^2 y_2+\cdots$ Where $y_0$ is the solution to the exactly solvable system obtained by dropping out terms with $\epsilon$ in the original equation. On the other hand, the method of dominant balance seems don't require the existence of small parameter $\epsilon$. It substitutes $y=e^{s(x)}$ into the equation and drop the negligible terms to get the dominant part $s_0(x)$. And does it iteratively with the substitution like $y(x)\sim e^{s_0(x)+c(x)}$ to get sub-dominant parts. Unlike perturbation theory, at every step we must re-evaluate the equation to determine the dominant terms (since the substitutions are different, we get different equations every time). Here comes my question$-$is there any way to view the method of dominant balance as part of the perturbation theory? Or what is the connection between these two methods?",,"['ordinary-differential-equations', 'asymptotics', 'nonlinear-system', 'perturbation-theory']"
34,Laplace Transform of the Wave Equation,Laplace Transform of the Wave Equation,,"I am given a damped wave equation $u_{tt}(t,x)+2u_t(t,x)=u_{xx}(t,x); \forall t>0$ Now I know the laplace transform of this given the initial conditions, $u(0,x)=\sin x, u_t(0,x)=0;$ is  $\tilde u_{xx}(s,x)-(s^2+2s)\tilde u(s,x)+(2+s)\sin x=0$ Now to solve this, I will use method of second order linear homogenous with constant coefficients, however my question is how can solve the wave equation if I am not given any boundary conditions? All help appreciated!","I am given a damped wave equation $u_{tt}(t,x)+2u_t(t,x)=u_{xx}(t,x); \forall t>0$ Now I know the laplace transform of this given the initial conditions, $u(0,x)=\sin x, u_t(0,x)=0;$ is  $\tilde u_{xx}(s,x)-(s^2+2s)\tilde u(s,x)+(2+s)\sin x=0$ Now to solve this, I will use method of second order linear homogenous with constant coefficients, however my question is how can solve the wave equation if I am not given any boundary conditions? All help appreciated!",,"['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform']"
35,Show that this initial-value problem has a unique solution,Show that this initial-value problem has a unique solution,,"I am trying to show that the following initial-value problem $$\frac{dx}{dt} = - x + tx^{1/2}; \quad x(2) = 2$$ has a unique solution on $I = [2,3]$. By letting $f(t,x) = - x + tx^{1/2}$ and $(t_0 , x_0) = (2,2)$ , I consider the rectangle $R$: $$\{ (t,x) : |t-t_0| \le \alpha , |x-x_0| \le \beta \}$$ which is $$\{ (t,x) : |t-2| \le \alpha , |x-2| \le \beta \}$$ The idea is that if all t lying in the range $|t-t_0| \le \min \{ \alpha , {\beta\over M} \}$ where $|f(t,x)| \le M$ , then this problem has a unique solution. But I found difficulties in finding $\alpha$, $\beta$ and $M$ because of two variables and also x could not be negative, could any one helps? Thanks.","I am trying to show that the following initial-value problem $$\frac{dx}{dt} = - x + tx^{1/2}; \quad x(2) = 2$$ has a unique solution on $I = [2,3]$. By letting $f(t,x) = - x + tx^{1/2}$ and $(t_0 , x_0) = (2,2)$ , I consider the rectangle $R$: $$\{ (t,x) : |t-t_0| \le \alpha , |x-x_0| \le \beta \}$$ which is $$\{ (t,x) : |t-2| \le \alpha , |x-2| \le \beta \}$$ The idea is that if all t lying in the range $|t-t_0| \le \min \{ \alpha , {\beta\over M} \}$ where $|f(t,x)| \le M$ , then this problem has a unique solution. But I found difficulties in finding $\alpha$, $\beta$ and $M$ because of two variables and also x could not be negative, could any one helps? Thanks.",,"['ordinary-differential-equations', 'numerical-methods']"
36,impossible ODE using delta functions?,impossible ODE using delta functions?,,"I'm working on the problems in the book ""Asymptotic Methods of Differential Equations"", by Roscoe White. It's a pretty legit book, and all the problems are quite non-trivial and very rich. However, two of the problems in Chapter 2 (which talks about exact solutions and has quite a bit on delta functions and Greens functions), are really vexing me at the moment. They are stated quite unceremoniously as: $6.$ solve $x^3 y' = 2y^2 + 3x^2 = 0$ $7.$ solve $y' = 2xy + y = 0$ for $y = y(x)$. (I triple checked the two lines above, they are typed exactly as written in the pages of the book) Now since the chapter that this problem set was placed in talks extensively about Greens functions, it seems that I'm supposed to construct some crazy solution out of delta functions and step functions. However, after trying for about half an hour, I tried checking these equations for consistency, and noted that if you differentiate the second equation in problem $6$ and multiply by $x^3$, it seems that there is a contradiction. Similarly, if you differentiate the second equation in Problem 7, it seems that the only possible solution is the trivial $y = 0$. Now if this were some random problem set cobbled together by a TA, then I would probably just move on and assume it was sloppiness on the part of whoever created these. However, the context of the problems tells me that I'm missing something, and there may an non-trivial solution to these. Can anyone take a look and tell me what I'm supposed to do here? Thanks.","I'm working on the problems in the book ""Asymptotic Methods of Differential Equations"", by Roscoe White. It's a pretty legit book, and all the problems are quite non-trivial and very rich. However, two of the problems in Chapter 2 (which talks about exact solutions and has quite a bit on delta functions and Greens functions), are really vexing me at the moment. They are stated quite unceremoniously as: $6.$ solve $x^3 y' = 2y^2 + 3x^2 = 0$ $7.$ solve $y' = 2xy + y = 0$ for $y = y(x)$. (I triple checked the two lines above, they are typed exactly as written in the pages of the book) Now since the chapter that this problem set was placed in talks extensively about Greens functions, it seems that I'm supposed to construct some crazy solution out of delta functions and step functions. However, after trying for about half an hour, I tried checking these equations for consistency, and noted that if you differentiate the second equation in problem $6$ and multiply by $x^3$, it seems that there is a contradiction. Similarly, if you differentiate the second equation in Problem 7, it seems that the only possible solution is the trivial $y = 0$. Now if this were some random problem set cobbled together by a TA, then I would probably just move on and assume it was sloppiness on the part of whoever created these. However, the context of the problems tells me that I'm missing something, and there may an non-trivial solution to these. Can anyone take a look and tell me what I'm supposed to do here? Thanks.",,"['ordinary-differential-equations', 'dirac-delta']"
37,How to integrate/differentiate parameters in differential equations,How to integrate/differentiate parameters in differential equations,,"I have an ODE, which I would be fine about solving, were it not for the parameter: $$(\omega^2+x^2)\frac{dy}{dx}=y$$ I'm given that $\omega>0$ is a parameter. Separating the variables gives: $$\int\frac{1}{y}dy=\int\frac{1}{\omega^2+x^2}dx$$ Essentially, my question is: can I just treat $\omega$ as a constant, so yielding the integral on RHS to be $\frac{1}{\omega}\arctan\frac{x}{\omega}$? (If this is wrong, and I'm to treat $\omega$  as a variable, how do I solve this DE?) Thanks.","I have an ODE, which I would be fine about solving, were it not for the parameter: $$(\omega^2+x^2)\frac{dy}{dx}=y$$ I'm given that $\omega>0$ is a parameter. Separating the variables gives: $$\int\frac{1}{y}dy=\int\frac{1}{\omega^2+x^2}dx$$ Essentially, my question is: can I just treat $\omega$ as a constant, so yielding the integral on RHS to be $\frac{1}{\omega}\arctan\frac{x}{\omega}$? (If this is wrong, and I'm to treat $\omega$  as a variable, how do I solve this DE?) Thanks.",,['ordinary-differential-equations']
38,Relationship between a class of non-linear differential equations and algebraic geometry.,Relationship between a class of non-linear differential equations and algebraic geometry.,,"I was just thinking about non-linear differential equations of a single variable, $F(f(x))=0$ that are polynomial in the derivatives of $f$.  For example: $$ 2\left(\frac{d^3f}{dx^3}\right)^5 - \frac{1}{7}\left(\frac{df}{dx}\right)^2  + \frac{d^2f}{dx^2} = 0$$ This can be considered as a polynomial in three variables, $x,y,$ and $z$: $$ 2x^5 - \frac{1}{7}y^2  + z = 0$$ This polynomial has a set of roots that has various properties which it derives from algebraic geometry, and my question is: Can someone explain what sort of things from algebraic geometry influence the solution set of the original differential equation? Thanks! P.S. I found this , but it's a pretty hard paper so it's gonna take awhile to work through anything, is this paper relevant to my question?","I was just thinking about non-linear differential equations of a single variable, $F(f(x))=0$ that are polynomial in the derivatives of $f$.  For example: $$ 2\left(\frac{d^3f}{dx^3}\right)^5 - \frac{1}{7}\left(\frac{df}{dx}\right)^2  + \frac{d^2f}{dx^2} = 0$$ This can be considered as a polynomial in three variables, $x,y,$ and $z$: $$ 2x^5 - \frac{1}{7}y^2  + z = 0$$ This polynomial has a set of roots that has various properties which it derives from algebraic geometry, and my question is: Can someone explain what sort of things from algebraic geometry influence the solution set of the original differential equation? Thanks! P.S. I found this , but it's a pretty hard paper so it's gonna take awhile to work through anything, is this paper relevant to my question?",,"['ordinary-differential-equations', 'algebraic-geometry']"
39,How Prove that $f$ is unique,How Prove that  is unique,f,"Assume that there exsits a smooth positive function $f$ on $(0,1)$ satisfying the differential equation $$-f''-\dfrac{f'}{r}+\dfrac{f}{r^2}=f(1-f^2)$$ together with with boundary conditions $f(0)=0$ and $f(1)=1$. Prove that $f$ is unique. the book give follow solution:Let $f_{1}$ and $f_{2}$ be two positive functions   satisfying the hypotheses. Dividing the differential equation by $f$ and subtracting   the corresponding equations, we obtain    $$-f''_{1}-\dfrac{f'_{1}}{r}+\dfrac{f_{1}}{r^2}=f_{1}(1-f^2_{1})$$   $$-f''_{2}-\dfrac{f'_{2}}{r}+\dfrac{f_{2}}{r^2}=f_{2}(1-f^2_{2})$$   $$\Longrightarrow -\dfrac{f''_{1}}{f_{1}}+\dfrac{f''_{2}}{f_{2}}-\dfrac{1}{r}\left(\dfrac{f'_{1}}{f_{1}}-\dfrac{f'_{2}}{f_{2}}\right)=-(f^2_{1}-f^2_{2})\tag 1$$ Multiplying the above equality by $r(f^2_{1}-f^2_{2})$ and integrating over $(0,1)$ yields $$\int_{0}^{1}\left(f'_{1}-\dfrac{f_{2}}{f_{1}}f'_{2}\right)^2rdr+\int_{0}^{1}\left(f'_{2}-\dfrac{f_{1}}{f_{2}}f'_{1}\right)^2rdr=-\int_{0}^{1}(f^2_{1}-f^2_{2})^2rdr\tag2$$ Therefore $f_{1}=f_{2}$ My Question: $(1)\Longrightarrow (2)$,I can't understand How get it? Thank you because when $(1)$ multiplying $r(f^2_{1}-f^2_{2})$ and integrating over $(0,1)$ then $$\int_{0}^{1}(f^2_{1}-f^2_{2})\left(-\dfrac{f''_{1}}{f_{1}}+\dfrac{f''_{2}}{f_{2}}\right)rdr-\int_{0}^{1}(f^2_{1}-f^2_{2})\left((\dfrac{f'_{1}}{f_{1}}-\dfrac{f'_{2}}{f_{2}}\right)dr=-\int_{0}^{1}(f^2_{1}-f^2_{2})^2rdr$$ so we only prove this $$\int_{0}^{1}(f^2_{1}-f^2_{2})\left(-\dfrac{f''_{1}}{f_{1}}+\dfrac{f''_{2}}{f_{2}}\right)rdr-\int_{0}^{1}(f^2_{1}-f^2_{2})\left((\dfrac{f'_{1}}{f_{1}}-\dfrac{f'_{2}}{f_{2}}\right)dr=\int_{0}^{1}\left(f'_{1}-\dfrac{f_{2}}{f_{1}}f'_{2}\right)^2rdr+\int_{0}^{1}\left(f'_{2}-\dfrac{f_{1}}{f_{2}}f'_{1}\right)^2rdr$$ maybe use integration by parts ? But I can't.Thank you","Assume that there exsits a smooth positive function $f$ on $(0,1)$ satisfying the differential equation $$-f''-\dfrac{f'}{r}+\dfrac{f}{r^2}=f(1-f^2)$$ together with with boundary conditions $f(0)=0$ and $f(1)=1$. Prove that $f$ is unique. the book give follow solution:Let $f_{1}$ and $f_{2}$ be two positive functions   satisfying the hypotheses. Dividing the differential equation by $f$ and subtracting   the corresponding equations, we obtain    $$-f''_{1}-\dfrac{f'_{1}}{r}+\dfrac{f_{1}}{r^2}=f_{1}(1-f^2_{1})$$   $$-f''_{2}-\dfrac{f'_{2}}{r}+\dfrac{f_{2}}{r^2}=f_{2}(1-f^2_{2})$$   $$\Longrightarrow -\dfrac{f''_{1}}{f_{1}}+\dfrac{f''_{2}}{f_{2}}-\dfrac{1}{r}\left(\dfrac{f'_{1}}{f_{1}}-\dfrac{f'_{2}}{f_{2}}\right)=-(f^2_{1}-f^2_{2})\tag 1$$ Multiplying the above equality by $r(f^2_{1}-f^2_{2})$ and integrating over $(0,1)$ yields $$\int_{0}^{1}\left(f'_{1}-\dfrac{f_{2}}{f_{1}}f'_{2}\right)^2rdr+\int_{0}^{1}\left(f'_{2}-\dfrac{f_{1}}{f_{2}}f'_{1}\right)^2rdr=-\int_{0}^{1}(f^2_{1}-f^2_{2})^2rdr\tag2$$ Therefore $f_{1}=f_{2}$ My Question: $(1)\Longrightarrow (2)$,I can't understand How get it? Thank you because when $(1)$ multiplying $r(f^2_{1}-f^2_{2})$ and integrating over $(0,1)$ then $$\int_{0}^{1}(f^2_{1}-f^2_{2})\left(-\dfrac{f''_{1}}{f_{1}}+\dfrac{f''_{2}}{f_{2}}\right)rdr-\int_{0}^{1}(f^2_{1}-f^2_{2})\left((\dfrac{f'_{1}}{f_{1}}-\dfrac{f'_{2}}{f_{2}}\right)dr=-\int_{0}^{1}(f^2_{1}-f^2_{2})^2rdr$$ so we only prove this $$\int_{0}^{1}(f^2_{1}-f^2_{2})\left(-\dfrac{f''_{1}}{f_{1}}+\dfrac{f''_{2}}{f_{2}}\right)rdr-\int_{0}^{1}(f^2_{1}-f^2_{2})\left((\dfrac{f'_{1}}{f_{1}}-\dfrac{f'_{2}}{f_{2}}\right)dr=\int_{0}^{1}\left(f'_{1}-\dfrac{f_{2}}{f_{1}}f'_{2}\right)^2rdr+\int_{0}^{1}\left(f'_{2}-\dfrac{f_{1}}{f_{2}}f'_{1}\right)^2rdr$$ maybe use integration by parts ? But I can't.Thank you",,['ordinary-differential-equations']
40,Smooth paths and homotopies,Smooth paths and homotopies,,"In applications of the fundamental group(oid) to smooth manifolds it is sometimes useful to have paths which are smooth, rather than merely continuous. For example, if we consider the local system of solutions of some linear partial differential equation, we can pull it back along a path, where it can be viewed as the solutions of an ordinary differential equation. The ""nicer"" the path is, the ""nicer"" will be the coefficients of the resulting ordinary differential equation. It seems impossible to simply define this issue away by only allowing smooth paths in the first place, since a concatenation of smooth paths need not be smooth. Another possible problem: if we want to also restrict to smooth homotopies, we have to figure out what this means at the corners of the square. Here's a couple of specific questions, since the previous paragraphs are so vague. Does every homotopy class of paths have a smooth representative? If two smooth paths are homotopic, can the homotopy be chosen to be smooth? How does one deal with the corners of the square? I imagine that the answers to these questions are written down somewhere.","In applications of the fundamental group(oid) to smooth manifolds it is sometimes useful to have paths which are smooth, rather than merely continuous. For example, if we consider the local system of solutions of some linear partial differential equation, we can pull it back along a path, where it can be viewed as the solutions of an ordinary differential equation. The ""nicer"" the path is, the ""nicer"" will be the coefficients of the resulting ordinary differential equation. It seems impossible to simply define this issue away by only allowing smooth paths in the first place, since a concatenation of smooth paths need not be smooth. Another possible problem: if we want to also restrict to smooth homotopies, we have to figure out what this means at the corners of the square. Here's a couple of specific questions, since the previous paragraphs are so vague. Does every homotopy class of paths have a smooth representative? If two smooth paths are homotopic, can the homotopy be chosen to be smooth? How does one deal with the corners of the square? I imagine that the answers to these questions are written down somewhere.",,"['ordinary-differential-equations', 'reference-request', 'fundamental-groups']"
41,Problems implementing Euler's Method on a second order ODE,Problems implementing Euler's Method on a second order ODE,,"I am trying to teach myself some numerical methods and having more issues than expected trying to solve $$y'' = -y$$ with initial conditions $y'(0) = 0$ and $y(0) = 1$. As I understand the problem, based on the information found here Solve a second order DEQ using Euler's method in MATLAB , I made the substitution $u = y'$ and reduced the problem to the following coupled system: \begin{align*} \begin{cases} y' = u;\\ u' = -y. \end{cases} \end{align*} So in this case, since $y'(0) = 0$, this implies $u=0$ and $y(0) = 1$ implies $u'(0) = -1$. With these initial conditions, I have the vector $[y_{0},u_{0}] = [0,-1]^{T}$. I tried to use a forward Euler method on this system using \begin{align*} y_{n+1} = y_{n} + hu_{n}\\ u_{n+1} = u_{n} - hy_{n} \end{align*} with $y_{0} = 0$, $u_{0} = -1$ but at this point I am not sure how I can extract the solution to my original differential equation. I am storing my $y_{i}$'s and $u_{i}'s$ in a $2\times n$ matrix where the first row contains $[y_{0},y_{1},y_{2},\dots, y_{n}]$ and the second row contains the same information for $u_{i}$'s in the same manner. It appears to me, that my matrix should contain the solution to my original differential equation in the first row, but this cannot be the case because the solution to the differential equation is $y(t) = \cos(t)$, and  $\cos(t_{0}) = \cos(0) = 1\neq 0$. Similarly, it cannot be the second row because $\cos(0) = 1 \neq -1$. If I change the initial condition to $[y_{0},u_{0}]^{T} =[0,1]$ and I take the solution to be the second row corresponding to the $u_{i}$'s, it looks like I get a reasonable answer, but I cannot understand whether this is simply a numerical fluke or if I made a flawed calculation.","I am trying to teach myself some numerical methods and having more issues than expected trying to solve $$y'' = -y$$ with initial conditions $y'(0) = 0$ and $y(0) = 1$. As I understand the problem, based on the information found here Solve a second order DEQ using Euler's method in MATLAB , I made the substitution $u = y'$ and reduced the problem to the following coupled system: \begin{align*} \begin{cases} y' = u;\\ u' = -y. \end{cases} \end{align*} So in this case, since $y'(0) = 0$, this implies $u=0$ and $y(0) = 1$ implies $u'(0) = -1$. With these initial conditions, I have the vector $[y_{0},u_{0}] = [0,-1]^{T}$. I tried to use a forward Euler method on this system using \begin{align*} y_{n+1} = y_{n} + hu_{n}\\ u_{n+1} = u_{n} - hy_{n} \end{align*} with $y_{0} = 0$, $u_{0} = -1$ but at this point I am not sure how I can extract the solution to my original differential equation. I am storing my $y_{i}$'s and $u_{i}'s$ in a $2\times n$ matrix where the first row contains $[y_{0},y_{1},y_{2},\dots, y_{n}]$ and the second row contains the same information for $u_{i}$'s in the same manner. It appears to me, that my matrix should contain the solution to my original differential equation in the first row, but this cannot be the case because the solution to the differential equation is $y(t) = \cos(t)$, and  $\cos(t_{0}) = \cos(0) = 1\neq 0$. Similarly, it cannot be the second row because $\cos(0) = 1 \neq -1$. If I change the initial condition to $[y_{0},u_{0}]^{T} =[0,1]$ and I take the solution to be the second row corresponding to the $u_{i}$'s, it looks like I get a reasonable answer, but I cannot understand whether this is simply a numerical fluke or if I made a flawed calculation.",,"['ordinary-differential-equations', 'numerical-methods']"
42,Constant of motion in a high dimensional analogue of the Lotka-Volterra system.,Constant of motion in a high dimensional analogue of the Lotka-Volterra system.,,"Suppose I would extend the Lotka Volterra system to the the $n$-dim first order ODE \begin{eqnarray*} \dot{x}_{1} &=& x_1(x_2-\alpha_1)   \\ \dot{x}_{2} &=& x_2(x_3-\alpha_2)   \\  \vdots  \; \;  &=&  \; \; \vdots \\  \dot{x}_{n}  &=&  x_n(x_1-\alpha_n)    \end{eqnarray*} where $\alpha_i$ are constants and $n \geq 2$. Note that for $n=2$ we get the Lotka Volterra system upto a scaling. In the sense of Lotka-Volterra the above model is a  population model for $n$-different species where the population of each species depends on one other and where finally the last is coupled back to the first. For $n=2$ finding the constant of motion  ( a non-trivial function $f$ such that for all solution curves $x$ we have that $f(x)=$ constant) is straightforward. However, I am not completely sure how to to this for the $n$-dimension case or if it even exists. Any help is welcome!","Suppose I would extend the Lotka Volterra system to the the $n$-dim first order ODE \begin{eqnarray*} \dot{x}_{1} &=& x_1(x_2-\alpha_1)   \\ \dot{x}_{2} &=& x_2(x_3-\alpha_2)   \\  \vdots  \; \;  &=&  \; \; \vdots \\  \dot{x}_{n}  &=&  x_n(x_1-\alpha_n)    \end{eqnarray*} where $\alpha_i$ are constants and $n \geq 2$. Note that for $n=2$ we get the Lotka Volterra system upto a scaling. In the sense of Lotka-Volterra the above model is a  population model for $n$-different species where the population of each species depends on one other and where finally the last is coupled back to the first. For $n=2$ finding the constant of motion  ( a non-trivial function $f$ such that for all solution curves $x$ we have that $f(x)=$ constant) is straightforward. However, I am not completely sure how to to this for the $n$-dimension case or if it even exists. Any help is welcome!",,"['ordinary-differential-equations', 'dynamical-systems']"
43,Differential equation with random variable,Differential equation with random variable,,"How can I derive analytically or compute numerically the solution to following differential equation $$ dy/dt = y\cdot X\cdot (y\cdot X - g(y,X))\cdot X $$ where X is a random variable (e.g. from a normal distribution) and g is a known function of y and X, e.g. $g=E[(y\cdot X)^2]$ ?","How can I derive analytically or compute numerically the solution to following differential equation $$ dy/dt = y\cdot X\cdot (y\cdot X - g(y,X))\cdot X $$ where X is a random variable (e.g. from a normal distribution) and g is a known function of y and X, e.g. $g=E[(y\cdot X)^2]$ ?",,"['ordinary-differential-equations', 'stochastic-processes', 'numerical-methods']"
44,boundary conditions after change of variables,boundary conditions after change of variables,,"Given the nonlinear boundary value problem on $[0,1]$ $$ a_1 y'^2 - a_2y'^{5/2} - a_3y'' + y''y'^{1/2} = 0 \quad y(0) = 0, y(1) = 1 \tag 1 $$ If I change variables $s=y'^{1/2}$, then (1) becomes the system $$ \begin{cases}         a_1s^4 - a_2s^5 - 2a_3ss' + 2s^2s' &= 0\\         s^2 &= y'     \end{cases} $$ Question:  What is the boundary condition for $s(0)$ after the change of variables?","Given the nonlinear boundary value problem on $[0,1]$ $$ a_1 y'^2 - a_2y'^{5/2} - a_3y'' + y''y'^{1/2} = 0 \quad y(0) = 0, y(1) = 1 \tag 1 $$ If I change variables $s=y'^{1/2}$, then (1) becomes the system $$ \begin{cases}         a_1s^4 - a_2s^5 - 2a_3ss' + 2s^2s' &= 0\\         s^2 &= y'     \end{cases} $$ Question:  What is the boundary condition for $s(0)$ after the change of variables?",,['ordinary-differential-equations']
45,Can't match boundary conditions on a perturbation series solution to a non-linear ODE?,Can't match boundary conditions on a perturbation series solution to a non-linear ODE?,,"I'm trying to generate a naive perturbation series solution (with all associated secular terms included) to the Rayleigh equation: \begin{equation} \frac{d^2y}{dt^2} + y = \epsilon \bigg(\frac{dy}{dt} - \frac{1}{3}\bigg(\frac{dy}{dt}\bigg)^3\bigg).  \end{equation} If I plug in the perturbation series \begin{equation} y = y_0 + \epsilon y_1 + \mathcal{O}(\epsilon^2), \end{equation} and solve for the $y_n$ order by order, I get $y_0 = Ae^{it}$ + complex conjugate, and, courtesy of Mathematica, \begin{equation} y_1 = \alpha e^{it} + \beta t e^{it} + \gamma e^{3it} + \text{complex conjugate}, \end{equation} where \begin{equation} \alpha = \frac{iA}{4}\big(1 - A\bar{A}) + C, \end{equation} where $C$ is the (complex) constant of integration specific to $y_1$.  The other two constants are \begin{equation} \beta = \frac{A}{2}\big(1 - A\bar{A}\big), \end{equation} and \begin{equation} \gamma = -\frac{iA^3}{24}. \end{equation} PROBLEM:  I'm having difficulty matching boundary conditions.  Say my initial conditions are $y(0) = y0$ and $dy/dt(0) = \text{v}0$.  I believe that in order for my series solution to be valid for all $\epsilon$, I must have $y_0(0) = y0$, $dy_0/dt(0) = \text{v}0$, and $y_n(0) = 0$ and $dy_n/dt(0) = 0$ for all $n \ge 1$. QUESTION 1:  Is that true in general for perturbation series?  i.e. that $y_n(0) = 0$ and $dy_n/dt(0) = 0$ for all $n \ge 1$? QUESTION 2:  When I go to match $y_1(0) = 0$, I seem to require that $\alpha + \gamma = 0$, so $C$ should be whatever makes $\alpha = iA^3/24$.  But this seems to mean that I have used my first boundary condition ($y_1(0) = 0$) to determine both the real and imaginary parts of $C$.  In particular, I have not used the condition $dy_1/dt(0) = 0$ to help set C, and as one might expect, the resulting solution \begin{equation} y_1 = \frac{iA^3}{24}e^{it} + \frac{A}{2}\big(1 - A\bar{A}\big)te^{it} - \frac{iA^3}{24}e^{3it} + \text{complex conjugate} \end{equation} yields $dy_1/dt(0) \ne 0$.  Where's the catch?  I have some reason to believe this is the right expression for $y_1$ since a published research article uses this form, but I don't understand how it's possible for $dy_1/dt(0)$ to not equal 0.  Any insight would be greatly appreciated.","I'm trying to generate a naive perturbation series solution (with all associated secular terms included) to the Rayleigh equation: \begin{equation} \frac{d^2y}{dt^2} + y = \epsilon \bigg(\frac{dy}{dt} - \frac{1}{3}\bigg(\frac{dy}{dt}\bigg)^3\bigg).  \end{equation} If I plug in the perturbation series \begin{equation} y = y_0 + \epsilon y_1 + \mathcal{O}(\epsilon^2), \end{equation} and solve for the $y_n$ order by order, I get $y_0 = Ae^{it}$ + complex conjugate, and, courtesy of Mathematica, \begin{equation} y_1 = \alpha e^{it} + \beta t e^{it} + \gamma e^{3it} + \text{complex conjugate}, \end{equation} where \begin{equation} \alpha = \frac{iA}{4}\big(1 - A\bar{A}) + C, \end{equation} where $C$ is the (complex) constant of integration specific to $y_1$.  The other two constants are \begin{equation} \beta = \frac{A}{2}\big(1 - A\bar{A}\big), \end{equation} and \begin{equation} \gamma = -\frac{iA^3}{24}. \end{equation} PROBLEM:  I'm having difficulty matching boundary conditions.  Say my initial conditions are $y(0) = y0$ and $dy/dt(0) = \text{v}0$.  I believe that in order for my series solution to be valid for all $\epsilon$, I must have $y_0(0) = y0$, $dy_0/dt(0) = \text{v}0$, and $y_n(0) = 0$ and $dy_n/dt(0) = 0$ for all $n \ge 1$. QUESTION 1:  Is that true in general for perturbation series?  i.e. that $y_n(0) = 0$ and $dy_n/dt(0) = 0$ for all $n \ge 1$? QUESTION 2:  When I go to match $y_1(0) = 0$, I seem to require that $\alpha + \gamma = 0$, so $C$ should be whatever makes $\alpha = iA^3/24$.  But this seems to mean that I have used my first boundary condition ($y_1(0) = 0$) to determine both the real and imaginary parts of $C$.  In particular, I have not used the condition $dy_1/dt(0) = 0$ to help set C, and as one might expect, the resulting solution \begin{equation} y_1 = \frac{iA^3}{24}e^{it} + \frac{A}{2}\big(1 - A\bar{A}\big)te^{it} - \frac{iA^3}{24}e^{3it} + \text{complex conjugate} \end{equation} yields $dy_1/dt(0) \ne 0$.  Where's the catch?  I have some reason to believe this is the right expression for $y_1$ since a published research article uses this form, but I don't understand how it's possible for $dy_1/dt(0)$ to not equal 0.  Any insight would be greatly appreciated.",,"['sequences-and-series', 'ordinary-differential-equations', 'complex-numbers', 'perturbation-theory', 'boundary-value-problem']"
46,Merton's Problem Stochastic Differential Equation,Merton's Problem Stochastic Differential Equation,,"Solve the following numerical case of Merton's optimal portfolio selection problem: find an optimal policy function $(s, y) \mapsto u(s, y)$ such that for the Ito diusion determined by $dX_t =X_t(u(t, Xt) + 1)dt + 4u(t, Xt)X_tdB_t, X_0 = 1$, the expectation $E^{0,1}[(X_3)^{\frac{1}{2}}$] is maximal. Could anyone please explain me how I could solve this using the Bellman equation in continuous time? ( $-W_t = \sup_{v \in V}\left[ f(t,x,v) + u(t,x,v)W_x + \frac{1}{2}\Sigma^2W_{xx} \right] $ )","Solve the following numerical case of Merton's optimal portfolio selection problem: find an optimal policy function $(s, y) \mapsto u(s, y)$ such that for the Ito diusion determined by $dX_t =X_t(u(t, Xt) + 1)dt + 4u(t, Xt)X_tdB_t, X_0 = 1$, the expectation $E^{0,1}[(X_3)^{\frac{1}{2}}$] is maximal. Could anyone please explain me how I could solve this using the Bellman equation in continuous time? ( $-W_t = \sup_{v \in V}\left[ f(t,x,v) + u(t,x,v)W_x + \frac{1}{2}\Sigma^2W_{xx} \right] $ )",,"['ordinary-differential-equations', 'stochastic-processes']"
47,solve nonlinear second order ODE,solve nonlinear second order ODE,,"I obtained Nonlinear second order differential equation as $y\cdot y''+y'^2-m\cdot y^{-a}y'^2+k=0$, Where $y'= \dfrac{dy}{dx}$, $y''=\dfrac{d^2y}{dx^2}$.  I could not obtain the solution so please help me for getting the solution of above equations with proper initial conditions.","I obtained Nonlinear second order differential equation as $y\cdot y''+y'^2-m\cdot y^{-a}y'^2+k=0$, Where $y'= \dfrac{dy}{dx}$, $y''=\dfrac{d^2y}{dx^2}$.  I could not obtain the solution so please help me for getting the solution of above equations with proper initial conditions.",,"['integration', 'ordinary-differential-equations', 'numerical-methods']"
48,Differential Equation for CDF,Differential Equation for CDF,,Consider the following differential equation $$F(cx) = F(x) + x F'(x)$$ for  $c>1$. Does this differential equation belong to a some well known class? Is there a way to find all the solutions $F(\cdot)$ of this equation that are also cumulative distribution functions? $F(x) = x^a$ for a properly chosen $a$ is a solution. Is it unique in the class of cumulative distribution functions?,Consider the following differential equation $$F(cx) = F(x) + x F'(x)$$ for  $c>1$. Does this differential equation belong to a some well known class? Is there a way to find all the solutions $F(\cdot)$ of this equation that are also cumulative distribution functions? $F(x) = x^a$ for a properly chosen $a$ is a solution. Is it unique in the class of cumulative distribution functions?,,"['ordinary-differential-equations', 'probability-distributions', 'functional-equations']"
49,Lyapunov Function and $\omega$-limit sets,Lyapunov Function and -limit sets,\omega,"I will ask you about a particular equation but what I would really enjoy is an (if possible, comprehensive) answer to the following question : How can we, using a Lyapunov function, study the $\omega$-limit set of a certain (possibly non-linear) ordinary differential equation. The only way I know is using LaSalle Principle and Poincaré-Bendixson theorem (or some consequences of it). I will present the problem in question that I would like to serve as an exemple for more detailed answers. If nobody has time or patience for such an answer, I would appreciate a LOT just to see some solution for this particular problem, since I haven't seen none. The equation is as usual $\frac{d}{dt}x=F(x)$, where $F(x,y)=(y,-ax-bx^{2}-cy)$, with $a,b,c$ positive constants. I want to describe the $\omega$-limit set of $(x,y)\in\mathbb{R^{2}}$. The suggestion on this exercise is to compute a Lyapunov function of the form : $Ax^{2}+Bx^{4}+Cy^{2}$, with $A,B,C$ positive constants. Thank you so much in advance :)","I will ask you about a particular equation but what I would really enjoy is an (if possible, comprehensive) answer to the following question : How can we, using a Lyapunov function, study the $\omega$-limit set of a certain (possibly non-linear) ordinary differential equation. The only way I know is using LaSalle Principle and Poincaré-Bendixson theorem (or some consequences of it). I will present the problem in question that I would like to serve as an exemple for more detailed answers. If nobody has time or patience for such an answer, I would appreciate a LOT just to see some solution for this particular problem, since I haven't seen none. The equation is as usual $\frac{d}{dt}x=F(x)$, where $F(x,y)=(y,-ax-bx^{2}-cy)$, with $a,b,c$ positive constants. I want to describe the $\omega$-limit set of $(x,y)\in\mathbb{R^{2}}$. The suggestion on this exercise is to compute a Lyapunov function of the form : $Ax^{2}+Bx^{4}+Cy^{2}$, with $A,B,C$ positive constants. Thank you so much in advance :)",,['ordinary-differential-equations']
50,differential equation 2,differential equation 2,,"Help me please to resolve this question Let the second order differential equation $$u''+a(x)u=0..........(1)$$ where $a \in \mathcal{C}^1([0,+\infty[)$ How we 1-prouve that if $a(x) \rightarrow +\infty$ when $x \rightarrow + \infty$, then all solutions of this equation are bounded on $[0,+\infty[$ 2- Prouve that if all solutions of the precedent equation are bounded in $[0,+\infty[$ and if $b(x) \rightarrow 0$ when  $x \rightarrow +\infty$ or $\int_0^{\infty} |b(s)|ds < \infty$, then, all the solutions of the equation $$u'' + (a(x) + b(x)) u = 0............(2)$$ are bounded on $[0,+\infty[$.","Help me please to resolve this question Let the second order differential equation $$u''+a(x)u=0..........(1)$$ where $a \in \mathcal{C}^1([0,+\infty[)$ How we 1-prouve that if $a(x) \rightarrow +\infty$ when $x \rightarrow + \infty$, then all solutions of this equation are bounded on $[0,+\infty[$ 2- Prouve that if all solutions of the precedent equation are bounded in $[0,+\infty[$ and if $b(x) \rightarrow 0$ when  $x \rightarrow +\infty$ or $\int_0^{\infty} |b(s)|ds < \infty$, then, all the solutions of the equation $$u'' + (a(x) + b(x)) u = 0............(2)$$ are bounded on $[0,+\infty[$.",,['ordinary-differential-equations']
51,Existence and Uniqueness of complex ODE's,Existence and Uniqueness of complex ODE's,,"I'm wondering if there is a theorem for the existence and uniqueness of complex ODE's. If there is, would someone mind explaining the general breadth of the theorem and/or directions to an online source detailing this theorem? That would be great.","I'm wondering if there is a theorem for the existence and uniqueness of complex ODE's. If there is, would someone mind explaining the general breadth of the theorem and/or directions to an online source detailing this theorem? That would be great.",,"['complex-analysis', 'ordinary-differential-equations']"
52,What does $d/dx$ actually mean?,What does  actually mean?,d/dx,"I'm starting to learn about differential equations, and I'm having trouble mentally adjusting to working with differentials as separate quantities. (I took calculus in high school and college but I don't remember ever learning about differentials or differential equations, so the whole concept is refusing to stick in my brain.) Mechanically I can do the work and get the right answer. But I don't really understand why it's correct to do the operations that I'm doing. My problem is that I can't quite grasp why it's OK to do arithmetic on differentials just because Leibniz's notation happens to make it ""look right"". I mostly get (at least, I can accept) why we can take a DE like this: $dy/dx = -xy$ and multiply through by $dx$, divide by $y$, then anti-differentiate the resulting two parts. I have always thought that $dy/dx$ was just a different form of notation for a function, $f'(x)$ that described the rate of change of y vs. x. But I can accept that it's actually a ratio of two infinitesimal numeric values. Where my understanding fails is with this form of the same differential equation: $(d/dx + x)y = 0$ at which point we ""distributed"" the $y$ to get the subsequent step. But I have no idea what $d/dx$ actually means. In my mind, I have treated it as an operation applied to functions: you ""apply"" $d/dx$ to a function written in terms of $x$ to get it's derivative. How, then can we add to and multiply by an operation? To me, that second equation looks exactly as if you had written $(! + x)y = 0$ and expanded that to $y! = -xy$, which obviously makes no sense. Why then, does it ""work"" for $d/dx$? What numeric quantity is $d/dx$ supposed to be?","I'm starting to learn about differential equations, and I'm having trouble mentally adjusting to working with differentials as separate quantities. (I took calculus in high school and college but I don't remember ever learning about differentials or differential equations, so the whole concept is refusing to stick in my brain.) Mechanically I can do the work and get the right answer. But I don't really understand why it's correct to do the operations that I'm doing. My problem is that I can't quite grasp why it's OK to do arithmetic on differentials just because Leibniz's notation happens to make it ""look right"". I mostly get (at least, I can accept) why we can take a DE like this: $dy/dx = -xy$ and multiply through by $dx$, divide by $y$, then anti-differentiate the resulting two parts. I have always thought that $dy/dx$ was just a different form of notation for a function, $f'(x)$ that described the rate of change of y vs. x. But I can accept that it's actually a ratio of two infinitesimal numeric values. Where my understanding fails is with this form of the same differential equation: $(d/dx + x)y = 0$ at which point we ""distributed"" the $y$ to get the subsequent step. But I have no idea what $d/dx$ actually means. In my mind, I have treated it as an operation applied to functions: you ""apply"" $d/dx$ to a function written in terms of $x$ to get it's derivative. How, then can we add to and multiply by an operation? To me, that second equation looks exactly as if you had written $(! + x)y = 0$ and expanded that to $y! = -xy$, which obviously makes no sense. Why then, does it ""work"" for $d/dx$? What numeric quantity is $d/dx$ supposed to be?",,['ordinary-differential-equations']
53,"Show that root $x\equiv 0$ of $\dfrac{dx}{dt}=F(t,x)$ is uniformly stable (uniformly asymptotically stable)",Show that root  of  is uniformly stable (uniformly asymptotically stable),"x\equiv 0 \dfrac{dx}{dt}=F(t,x)","I have a problem: For the system of equations: $$\bf \dfrac{dx}{dt}=F(t,x) \tag 1$$ where $F$ is continuous in $I \times D \subset\mathbb{R}\times \mathbb{R}^n$ and   $F(t,0)\equiv0$, $F(t+\omega,x)=F(t,x), \omega >0$, it means that $F$ is periodic function.   Prove that, if root $x\equiv 0$ of (1) is stable ( asymptotically   stable ) then it is uniformly stable ( uniformly asymptotically   stable ). Uniformly Stable : If any given $\epsilon >0$, $\exists \delta=\delta(\epsilon)>0$: $$\|x(t_0)\|< \delta \implies \|x(t)\|<\epsilon, \forall t\ge t_0 >0$$ I have thought about my problem, I used the definition uniformly stable/ uniformly asymptotically stable. But I'm having trouble when I try to find a solution to the problem, and I still have no solution. Any help will be appreciated. Thanks!","I have a problem: For the system of equations: $$\bf \dfrac{dx}{dt}=F(t,x) \tag 1$$ where $F$ is continuous in $I \times D \subset\mathbb{R}\times \mathbb{R}^n$ and   $F(t,0)\equiv0$, $F(t+\omega,x)=F(t,x), \omega >0$, it means that $F$ is periodic function.   Prove that, if root $x\equiv 0$ of (1) is stable ( asymptotically   stable ) then it is uniformly stable ( uniformly asymptotically   stable ). Uniformly Stable : If any given $\epsilon >0$, $\exists \delta=\delta(\epsilon)>0$: $$\|x(t_0)\|< \delta \implies \|x(t)\|<\epsilon, \forall t\ge t_0 >0$$ I have thought about my problem, I used the definition uniformly stable/ uniformly asymptotically stable. But I'm having trouble when I try to find a solution to the problem, and I still have no solution. Any help will be appreciated. Thanks!",,"['ordinary-differential-equations', 'control-theory']"
54,Domain for PDE Solution,Domain for PDE Solution,,"I'm working on PDE exercises for solutions via characteristics. The problems ask for the solution and its domain. For $$y^{-1}u_x + u_y = u^2, \quad u(x,1) = x^2$$ I found the solution to be  $$ u(x,y) = \left( \frac{1}{(x- \ln |y|)^2} - y +1 \right) ^{-1}$$. So far, so good, but I can't find a clear way to express the domain. The best I've come up with is that it's ""the union of open connected sets $U \subset \mathbb{R}^2$ such that $x \neq \ln |y|$ and $(x - \ln |y|)^2(y-1) \neq 1$ for all $(x,y) \in U$ and there exists a point $(x_0, 1) \in U$.    I.e., nothing in the solution breaks, and part of the initial data is in the set. For another,  $$ u_x + u^{1/2} u_y = 0, \quad u(x,0) = x^2+1,$$ I get the implicit formula $u(x,y) = (x-\frac{y}{u(x,y)^{1/2}})^2 +1$, and here again, I don't see a good way to describe the domain. Is there some other way to express the solutions, or another way to think about them, which would make the domain clearer? I'm running into this problem over and over, and would appreciate any guidance. Thanks.","I'm working on PDE exercises for solutions via characteristics. The problems ask for the solution and its domain. For $$y^{-1}u_x + u_y = u^2, \quad u(x,1) = x^2$$ I found the solution to be  $$ u(x,y) = \left( \frac{1}{(x- \ln |y|)^2} - y +1 \right) ^{-1}$$. So far, so good, but I can't find a clear way to express the domain. The best I've come up with is that it's ""the union of open connected sets $U \subset \mathbb{R}^2$ such that $x \neq \ln |y|$ and $(x - \ln |y|)^2(y-1) \neq 1$ for all $(x,y) \in U$ and there exists a point $(x_0, 1) \in U$.    I.e., nothing in the solution breaks, and part of the initial data is in the set. For another,  $$ u_x + u^{1/2} u_y = 0, \quad u(x,0) = x^2+1,$$ I get the implicit formula $u(x,y) = (x-\frac{y}{u(x,y)^{1/2}})^2 +1$, and here again, I don't see a good way to describe the domain. Is there some other way to express the solutions, or another way to think about them, which would make the domain clearer? I'm running into this problem over and over, and would appreciate any guidance. Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations']"
55,A probable inspiring proof to Poincare lemma,A probable inspiring proof to Poincare lemma,,"Poincare lemma says if a smooth $p$-form $\omega$ is closed, then $\omega$ must be exact. Let's put it in another way, it says the solution of $d\omega=0$ is $\omega=d\eta$ for some $(p-1)$-form $\eta$ So my idea is trying to apply Frobenius theorem to provide solution to differential equations? But I don't know How.","Poincare lemma says if a smooth $p$-form $\omega$ is closed, then $\omega$ must be exact. Let's put it in another way, it says the solution of $d\omega=0$ is $\omega=d\eta$ for some $(p-1)$-form $\eta$ So my idea is trying to apply Frobenius theorem to provide solution to differential equations? But I don't know How.",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations']"
56,Causality in Dirac delta forced harmonic oscillator,Causality in Dirac delta forced harmonic oscillator,,"If I take the simple forced harmonic oscillator equation, apply the Fourier transform to both sides, and assuming the forcing function is a Dirac delta function (at the origin) I get: $ F(s) = \frac 1 {1-4\pi^2s^2} $ If I then take the inverse Fourier transform I see my wave like behaviour as expected (after t=0). However what I do not understand is why is there oscillatory behaviour before t=0? That seems to violate causality. Similar postings seem to suggest just defining f(t) to be zero before the impulse, or using the Laplace transform. This is closely related to other postings, but it is the causality issue which is confusing me, I don't understand why the fourier method seems to break it. Thanks.","If I take the simple forced harmonic oscillator equation, apply the Fourier transform to both sides, and assuming the forcing function is a Dirac delta function (at the origin) I get: $ F(s) = \frac 1 {1-4\pi^2s^2} $ If I then take the inverse Fourier transform I see my wave like behaviour as expected (after t=0). However what I do not understand is why is there oscillatory behaviour before t=0? That seems to violate causality. Similar postings seem to suggest just defining f(t) to be zero before the impulse, or using the Laplace transform. This is closely related to other postings, but it is the causality issue which is confusing me, I don't understand why the fourier method seems to break it. Thanks.",,"['ordinary-differential-equations', 'fourier-analysis']"
57,Gluing together solutions to differential equations,Gluing together solutions to differential equations,,"Edit . This question hasn't received much interest so far. Please leave suggestions for improvement in the comments. Suppose we're wanting to glue together solutions to a differential equation to obtain a solution on a larger domain. Let $S(X)$ denote the set of all solutions with domain $X,$ where $X$ is allowed to be any open set. Now I imagine that: Conjecture 1. For all open sets $X$ and $Y$ and solutions $f \in S(X)$ and $g \in S(Y)$, we have that if $f$ and $g$ agree when restricted to the domain $X \cap Y$, then $f \cup g \in S(X \cup Y).$ Is this actually true? If not, how do we fix the assertion? A related question about uniqueness. I believe that: Conjecture 2. If $X$ is not only open, but also connected, then for all $f,g \in S(X)$ we have that if $f$ and $g$ agree on some non-empty open subset of $X$, then $f=g$. Is this actually true? If not, how do we fix the assertion?","Edit . This question hasn't received much interest so far. Please leave suggestions for improvement in the comments. Suppose we're wanting to glue together solutions to a differential equation to obtain a solution on a larger domain. Let $S(X)$ denote the set of all solutions with domain $X,$ where $X$ is allowed to be any open set. Now I imagine that: Conjecture 1. For all open sets $X$ and $Y$ and solutions $f \in S(X)$ and $g \in S(Y)$, we have that if $f$ and $g$ agree when restricted to the domain $X \cap Y$, then $f \cup g \in S(X \cup Y).$ Is this actually true? If not, how do we fix the assertion? A related question about uniqueness. I believe that: Conjecture 2. If $X$ is not only open, but also connected, then for all $f,g \in S(X)$ we have that if $f$ and $g$ agree on some non-empty open subset of $X$, then $f=g$. Is this actually true? If not, how do we fix the assertion?",,['ordinary-differential-equations']
58,Hard Differential Equation. Please help.,Hard Differential Equation. Please help.,,"first of all I'm not a mathematician, so I apologize if any of my understanding and terminology isn't up to par.  Also, I've never used this website (or any of these kind of question/answer) websites before, hope it works. I'm a mech. eng having trouble with a differential equation that is crucial to my research in graduate school.  I've spend a long time trying to look up and understand tons of sources anything from books to buddies around the college. This is the differential equation: $$ \dfrac{F(t)}{\left(\frac{dy(x)}{dx}\right)^2+1}=c^2y^2\left(\frac{d^2x(t)}{dt^2}\right)^2 $$ I'm trying to solve for y which I THINK is dependent on x only, but i'm not sure.  It could be y(x) or y(t). I also think this is a 1st order non-linear non-homogenous DE, but again I'm not sure.  I initially thought it might be a partial differential equation though. Any help would be greatly appreciated!","first of all I'm not a mathematician, so I apologize if any of my understanding and terminology isn't up to par.  Also, I've never used this website (or any of these kind of question/answer) websites before, hope it works. I'm a mech. eng having trouble with a differential equation that is crucial to my research in graduate school.  I've spend a long time trying to look up and understand tons of sources anything from books to buddies around the college. This is the differential equation: $$ \dfrac{F(t)}{\left(\frac{dy(x)}{dx}\right)^2+1}=c^2y^2\left(\frac{d^2x(t)}{dt^2}\right)^2 $$ I'm trying to solve for y which I THINK is dependent on x only, but i'm not sure.  It could be y(x) or y(t). I also think this is a 1st order non-linear non-homogenous DE, but again I'm not sure.  I initially thought it might be a partial differential equation though. Any help would be greatly appreciated!",,"['ordinary-differential-equations', 'differential-geometry', 'physics', 'classical-mechanics']"
59,Fourier Transform of one variable in a two variable function.,Fourier Transform of one variable in a two variable function.,,"I have a function in two variables, that satisfies the following PDE: \begin{equation} \frac{x-x_0}{x-x_1}\Psi_{xx}+\Psi_{yy}=0 \end{equation} Initially I did use Fourier series \begin{equation*} \Psi=‎ \sum_{-\infty}^{\infty} a_k(x) e^{iky}dy, 0 \leq y \leq 2 \pi \end{equation*} and obtained the following ODE \begin{equation} \frac{x-x_0}{x-x_1}\Psi_{xx}-k^2 \Psi=0 \end{equation} However, this limits k to a discrete set, but in my case it needs to be any real number, so  my Professor adviced me to use Fourier Transform on y, and obtain a function in terms of x and k, which should give me exactly same ODE as a result, yet k will be a real number. My attempt: Using the Fourier Transform \begin{equation*} \Psi=‎ \int_{-\infty}^{\infty} \hat{\Psi}(y) e^{2 \pi ik xy}dy, \ -\infty < y < \infty \end{equation*} However, I am not able to obtain the same ODE as before (Which I need specifically). I am not so familiar with Fourier Transforms, and I do not understand what mistake am I making. Any advice would be highly appriciated. Thank you!","I have a function in two variables, that satisfies the following PDE: \begin{equation} \frac{x-x_0}{x-x_1}\Psi_{xx}+\Psi_{yy}=0 \end{equation} Initially I did use Fourier series \begin{equation*} \Psi=‎ \sum_{-\infty}^{\infty} a_k(x) e^{iky}dy, 0 \leq y \leq 2 \pi \end{equation*} and obtained the following ODE \begin{equation} \frac{x-x_0}{x-x_1}\Psi_{xx}-k^2 \Psi=0 \end{equation} However, this limits k to a discrete set, but in my case it needs to be any real number, so  my Professor adviced me to use Fourier Transform on y, and obtain a function in terms of x and k, which should give me exactly same ODE as a result, yet k will be a real number. My attempt: Using the Fourier Transform \begin{equation*} \Psi=‎ \int_{-\infty}^{\infty} \hat{\Psi}(y) e^{2 \pi ik xy}dy, \ -\infty < y < \infty \end{equation*} However, I am not able to obtain the same ODE as before (Which I need specifically). I am not so familiar with Fourier Transforms, and I do not understand what mistake am I making. Any advice would be highly appriciated. Thank you!",,"['ordinary-differential-equations', 'fourier-analysis']"
60,ODE Cauchy problem,ODE Cauchy problem,,"Consider the Cauchy problem $ x'=f(t,x)  ,x(0)=0 $, where  $$ f(t,x)=\begin{cases} \left(\frac{2}{\sqrt{\|x\|}}(x_{1}+x_{2}),\frac{2}{\sqrt{\|x\|}}(x_{2}-x_{1})\right)  &x\neq 0\\ (0,0) & x=0 \end{cases}$$ Solving the above system by converting from Cartesian coordinates $(x_{1},x_{2})$ to polar coordinates $(r,\theta)$. After solving what conclusion can you draw about?","Consider the Cauchy problem $ x'=f(t,x)  ,x(0)=0 $, where  $$ f(t,x)=\begin{cases} \left(\frac{2}{\sqrt{\|x\|}}(x_{1}+x_{2}),\frac{2}{\sqrt{\|x\|}}(x_{2}-x_{1})\right)  &x\neq 0\\ (0,0) & x=0 \end{cases}$$ Solving the above system by converting from Cartesian coordinates $(x_{1},x_{2})$ to polar coordinates $(r,\theta)$. After solving what conclusion can you draw about?",,['ordinary-differential-equations']
61,Differential equations that are also functional,Differential equations that are also functional,,"I was toying with equations of the type $f(x+\alpha)=f'(x)$ where $f$ is a real function. For example if $\alpha=\frac{\pi}{2}$ then the solutions include the function $f_{\lambda,\mu}(x)=\lambda cos(x+\mu)$. Are there more solutions? On the other hand, if I want to solve the equation for any $\alpha$, I can assume a solution of the form $f(x)=e^{\lambda x}$, and find $\lambda$ as a complex number that enables me to solve the equation... I was wondering: is the set of the solutions of dimension 2 because the derivative operator creates one dimension and the operator $\phi: f(x)\mapsto f(x+1)$ adds another one? Is there some litterature about this kind of equations? Please satisfy my curiosity if you can... Thanks!","I was toying with equations of the type $f(x+\alpha)=f'(x)$ where $f$ is a real function. For example if $\alpha=\frac{\pi}{2}$ then the solutions include the function $f_{\lambda,\mu}(x)=\lambda cos(x+\mu)$. Are there more solutions? On the other hand, if I want to solve the equation for any $\alpha$, I can assume a solution of the form $f(x)=e^{\lambda x}$, and find $\lambda$ as a complex number that enables me to solve the equation... I was wondering: is the set of the solutions of dimension 2 because the derivative operator creates one dimension and the operator $\phi: f(x)\mapsto f(x+1)$ adds another one? Is there some litterature about this kind of equations? Please satisfy my curiosity if you can... Thanks!",,['ordinary-differential-equations']
62,A photon in expanding Universe (a snail on a tree),A photon in expanding Universe (a snail on a tree),,"I want to know how far a snail can reach in expanding universe. It has a constant speed c = 1 and tree is expanding at speed $v= H_0 D$, with Hubble constant $H_0 = 1$. Here D(T) is the distance of snail from the origin that I am looking for. I want to know how it depends on time T. At every moment of time, t, D increases as $dD = H_0 D dt + c dt = (D + 1)dt$ because we add the tree growth to the snail speed. This is non-homogenous equation and I had to look into handbook to get the result $D(T) = e^T-1$. This looks right, as we have snail at the origin, D = 0, at time 0. However, I do not know how to compute inhomogeneity myself. I keep in mind only that exponentials are solutions of homogenous equations. For instance, the Hubble equation, $dD = r_0 D\:dt$, gives exponentially expanding Universe $D = r_0 e^t$. To avoid inhomogeneity (and differential equations altogether), I tried to derive the solution right in the closed form. I considered this way: in the first moment of time, snail travels $r_0 = dt$. This length starts expanding, according to Hubble law, $r(T-dt) = r_0 e^{T-dt} = e^{T-dt} dt$ to the snail's distance because it has $T-dt$ seconds to expand. This is the contribution of the first snail step to the resulting distance. During the next step, snail makes another $r_0 = dt$, but it has $dt$ less time to expand, resulting in contribution of $e^{T-2dt} dt$. The third step, will produce the same $r_0$, expanded to $e^{T-3dt} dt$ and so on, until, in the final step there will be no time to expand at all, $e^{T-T} dt = dt$. So, we need to sum up the infinitecimal plots, whose sizes grow exponentially with t. In the form $D(T) = \int r(t) dt$, where $r(t) = e^{T-\int dt = T-t}$. Likewise $\int_0^Y {y dy} = \int_0^Y {(Y-y) dy} $, I think that we'll get the same result if, instead of reducing expansion time linearly, from T to 0, it would be increased linearly, from 0 to T, $D = \int_0^T e^{T-t} dt = \int_0^T e^{t} dt = e^t|_0^T = e^T-1$ Surprisingly, while I typed this text, the closed form integral matched the diffeq solution above! I had a discrepancy on the paper. It seems not a question anymore. Is it ok to leave the solution on this site? You may just check if it is ok. So, photon with a constant speed, 1, in expanding Universe travels at $d(e^t-1)/dt = e^t$ -- exactly the speed of expanding Universe! Otherwise, you may answer why Ant on a rubber rope is consedered as a model of Universe expansion rather than my problem? I see that ant has it tree growing linearly, rather than exponintially, which cannot correspond to the Hubble Universe expansion.","I want to know how far a snail can reach in expanding universe. It has a constant speed c = 1 and tree is expanding at speed $v= H_0 D$, with Hubble constant $H_0 = 1$. Here D(T) is the distance of snail from the origin that I am looking for. I want to know how it depends on time T. At every moment of time, t, D increases as $dD = H_0 D dt + c dt = (D + 1)dt$ because we add the tree growth to the snail speed. This is non-homogenous equation and I had to look into handbook to get the result $D(T) = e^T-1$. This looks right, as we have snail at the origin, D = 0, at time 0. However, I do not know how to compute inhomogeneity myself. I keep in mind only that exponentials are solutions of homogenous equations. For instance, the Hubble equation, $dD = r_0 D\:dt$, gives exponentially expanding Universe $D = r_0 e^t$. To avoid inhomogeneity (and differential equations altogether), I tried to derive the solution right in the closed form. I considered this way: in the first moment of time, snail travels $r_0 = dt$. This length starts expanding, according to Hubble law, $r(T-dt) = r_0 e^{T-dt} = e^{T-dt} dt$ to the snail's distance because it has $T-dt$ seconds to expand. This is the contribution of the first snail step to the resulting distance. During the next step, snail makes another $r_0 = dt$, but it has $dt$ less time to expand, resulting in contribution of $e^{T-2dt} dt$. The third step, will produce the same $r_0$, expanded to $e^{T-3dt} dt$ and so on, until, in the final step there will be no time to expand at all, $e^{T-T} dt = dt$. So, we need to sum up the infinitecimal plots, whose sizes grow exponentially with t. In the form $D(T) = \int r(t) dt$, where $r(t) = e^{T-\int dt = T-t}$. Likewise $\int_0^Y {y dy} = \int_0^Y {(Y-y) dy} $, I think that we'll get the same result if, instead of reducing expansion time linearly, from T to 0, it would be increased linearly, from 0 to T, $D = \int_0^T e^{T-t} dt = \int_0^T e^{t} dt = e^t|_0^T = e^T-1$ Surprisingly, while I typed this text, the closed form integral matched the diffeq solution above! I had a discrepancy on the paper. It seems not a question anymore. Is it ok to leave the solution on this site? You may just check if it is ok. So, photon with a constant speed, 1, in expanding Universe travels at $d(e^t-1)/dt = e^t$ -- exactly the speed of expanding Universe! Otherwise, you may answer why Ant on a rubber rope is consedered as a model of Universe expansion rather than my problem? I see that ant has it tree growing linearly, rather than exponintially, which cannot correspond to the Hubble Universe expansion.",,"['integration', 'ordinary-differential-equations', 'recreational-mathematics', 'closed-form']"
63,Combining differential equations,Combining differential equations,,"Can anyone see how to combine the following 3 equations $$\dot r^2-\dot\theta^2=-\theta^2$$ $$\theta\ddot \theta-2\dot \theta^2=2(\dot r^2-\dot \theta^2)$$ $$\dot r=a \theta^2$$ to get $$\theta^2=r^2+cr+d$$ where $c,d$ are constants? I've tried various ways of substituting one into another but I just end up with very high orders of derivatives, solving of which I don't think should be necessary. What I did was (2)-(1) then substitute in (3), giving $$(a-1)\theta^2=\theta\ddot\theta-\dot\theta^2$$ So I need the RHS to be $$(a-1)r^2+Ar+B$$ for some constants $A,B$? But how?? or perhaps start differently?","Can anyone see how to combine the following 3 equations $$\dot r^2-\dot\theta^2=-\theta^2$$ $$\theta\ddot \theta-2\dot \theta^2=2(\dot r^2-\dot \theta^2)$$ $$\dot r=a \theta^2$$ to get $$\theta^2=r^2+cr+d$$ where $c,d$ are constants? I've tried various ways of substituting one into another but I just end up with very high orders of derivatives, solving of which I don't think should be necessary. What I did was (2)-(1) then substitute in (3), giving $$(a-1)\theta^2=\theta\ddot\theta-\dot\theta^2$$ So I need the RHS to be $$(a-1)r^2+Ar+B$$ for some constants $A,B$? But how?? or perhaps start differently?",,"['algebra-precalculus', 'ordinary-differential-equations']"
64,Differential equation $y'(t) = 1-y(t) e^{y(t)-1}$,Differential equation,y'(t) = 1-y(t) e^{y(t)-1},"I am interested in finding a clean explicit solution (if possible) to the differential equation $$ y'(t) = 1-y(t) e^{y(t)-1}, $$ where $0 \le t < 1$ and $0 \le y \le 1$. This can obviously be solved with direct integration, giving the solution $$ t + C = \int \frac{1}{1-ye^{y-1}} dy. $$ I have not found a solution to the integral, but that would essentially answer my question. I would like an expression for $y(t)$ that does not involve inverse functions. I tried finding a series solution but did not get very far.","I am interested in finding a clean explicit solution (if possible) to the differential equation $$ y'(t) = 1-y(t) e^{y(t)-1}, $$ where $0 \le t < 1$ and $0 \le y \le 1$. This can obviously be solved with direct integration, giving the solution $$ t + C = \int \frac{1}{1-ye^{y-1}} dy. $$ I have not found a solution to the integral, but that would essentially answer my question. I would like an expression for $y(t)$ that does not involve inverse functions. I tried finding a series solution but did not get very far.",,"['integration', 'ordinary-differential-equations', 'dynamical-systems']"
65,System of many non-linear (quadratic) first order O.D.E. (numerical strategy or simplification),System of many non-linear (quadratic) first order O.D.E. (numerical strategy or simplification),,"I have a large system (N>100) of equations $\frac{d\vec{P}}{dt}= A(t) + B(t) \vec{P} + \vec{P}^T C(t) \vec{P}$ where $\vec{P}$ is a vector of N functions of the variable t. What is the correct name of these kind of equations in Mathematical Ecology? What do you guys suggest to do to numerically solve this equation as efficiently as possible? I am not familiar at all with numerical packages or strategies. PS1: I am currently using Mathematica method NDSolve (with the option ""EquationSimplification""->""Solve"" which I am not sure what it does). PS2: For sake of completeness, the actual structure of this quadratic form is $\frac{dP_i}{dt}= \sum_j [\alpha^1_{ij}(t) P_i(1-P_j)+\alpha^2_{ij}(t) (1-P_i)P_j+\alpha^3_{ij}(t) P_i P_j+\alpha^4_{ij}(t) (1-P_i)(1-P_j) ]$ Has this form a name?  It is pretty common in semiconductor physics (non-equilibrium kinetic equation for Fermionic open quantum systems).","I have a large system (N>100) of equations $\frac{d\vec{P}}{dt}= A(t) + B(t) \vec{P} + \vec{P}^T C(t) \vec{P}$ where $\vec{P}$ is a vector of N functions of the variable t. What is the correct name of these kind of equations in Mathematical Ecology? What do you guys suggest to do to numerically solve this equation as efficiently as possible? I am not familiar at all with numerical packages or strategies. PS1: I am currently using Mathematica method NDSolve (with the option ""EquationSimplification""->""Solve"" which I am not sure what it does). PS2: For sake of completeness, the actual structure of this quadratic form is $\frac{dP_i}{dt}= \sum_j [\alpha^1_{ij}(t) P_i(1-P_j)+\alpha^2_{ij}(t) (1-P_i)P_j+\alpha^3_{ij}(t) P_i P_j+\alpha^4_{ij}(t) (1-P_i)(1-P_j) ]$ Has this form a name?  It is pretty common in semiconductor physics (non-equilibrium kinetic equation for Fermionic open quantum systems).",,"['ordinary-differential-equations', 'numerical-methods', 'nonlinear-optimization']"
66,minimization problem on differential equations - optimal control,minimization problem on differential equations - optimal control,,"I am trying to minimize an time-integral of a linear function with respect to differential equations. The problem is formally defined as follows: Given $\lambda< \mu_1, \mu_2$ fixed parameters(thus $x(s)$ will hit 0 at some point in time and stay there) and $c_1, c_2$ time independent, fixed cost coefficients : $v(x_1, x_2)   = \min_{u,p} \int\limits_T \ [c_1 X_1(s) + c_2 X_2(s)]ds$ \ subject to $\dot{X_1}(s) = p(s)\lambda  - u(s)\mu_1$; $\dot{X_2}(s) = (1- p(s))\lambda  - (1-u(s))\mu_2$; $ X_i(s) = x_i$ $0   \leq u(s) \leq 1$ $0   \leq p(s) \leq 1$ Thus $u(s), p(s)$ are the controls or decision variables. What would be the methodology to follow in this case? If I understood properly, for this system to reach optimality HJB equation must be satisfied : $\min_{u,p} (p(s)\lambda  - u(s)\mu_1)\frac{\partial v}{\partial x1} + (1- p(s))\lambda  - (1-u(s))\frac{\partial v}{\partial x2} + c_1 x_1 + c_2 x_2 = 0 $ But without knowing the exact form of $v$ I can not check whether the equation above holds or not. I guess one way to progress is to guess optimal $v$, but then again how such an argument would work without being circular. Any help, references, comments appreciated... Thanks in advance..","I am trying to minimize an time-integral of a linear function with respect to differential equations. The problem is formally defined as follows: Given $\lambda< \mu_1, \mu_2$ fixed parameters(thus $x(s)$ will hit 0 at some point in time and stay there) and $c_1, c_2$ time independent, fixed cost coefficients : $v(x_1, x_2)   = \min_{u,p} \int\limits_T \ [c_1 X_1(s) + c_2 X_2(s)]ds$ \ subject to $\dot{X_1}(s) = p(s)\lambda  - u(s)\mu_1$; $\dot{X_2}(s) = (1- p(s))\lambda  - (1-u(s))\mu_2$; $ X_i(s) = x_i$ $0   \leq u(s) \leq 1$ $0   \leq p(s) \leq 1$ Thus $u(s), p(s)$ are the controls or decision variables. What would be the methodology to follow in this case? If I understood properly, for this system to reach optimality HJB equation must be satisfied : $\min_{u,p} (p(s)\lambda  - u(s)\mu_1)\frac{\partial v}{\partial x1} + (1- p(s))\lambda  - (1-u(s))\frac{\partial v}{\partial x2} + c_1 x_1 + c_2 x_2 = 0 $ But without knowing the exact form of $v$ I can not check whether the equation above holds or not. I guess one way to progress is to guess optimal $v$, but then again how such an argument would work without being circular. Any help, references, comments appreciated... Thanks in advance..",,"['ordinary-differential-equations', 'optimization', 'convex-optimization', 'control-theory']"
67,Pure differential equation whose solution is a siluroid?,Pure differential equation whose solution is a siluroid?,,"I am trying to find a differential equation for the siluroid that DOES NOT contain explicitly $\theta$, $\sin\theta$, or $\cos\theta$, but only $\rho$, $\dot\rho$, $\ddot\rho$. The siluroid equation is $\rho = 4n\cos\theta\cos2\theta$ where $n\in\mathbb{N}$ Thank you in advance.","I am trying to find a differential equation for the siluroid that DOES NOT contain explicitly $\theta$, $\sin\theta$, or $\cos\theta$, but only $\rho$, $\dot\rho$, $\ddot\rho$. The siluroid equation is $\rho = 4n\cos\theta\cos2\theta$ where $n\in\mathbb{N}$ Thank you in advance.",,"['ordinary-differential-equations', 'polar-coordinates']"
68,Spectrum of the Hill Operator $L(y)= -y''+ v(x) y $,Spectrum of the Hill Operator,L(y)= -y''+ v(x) y ,"Consider the eigenvalue equation for the Hill operator  $$L(y)= -y''+ v(x) y = \lambda y, \quad x\in \mathbb{R},$$ where $v(x)$ is any potential and $\lambda$ is the spectral parameter. If $v(x) \equiv 0$, the spectrum of $L$ subject to periodic boundary conditions ( bc ): $y(0)=y(\pi)$, $ y'(0)=y'(\pi)$ and anti-periodic bc : $ y(0)=-y(\pi)$, $ y'(0)=-y'(\pi)$ coincides with the spectrum subject to Dirichlet bc : $y(0)=y(\pi)=0$; that is $\lambda_n=n^2$, $n \in \mathbb{N}$, if $n$ is even or $n$ is odd, respectively. My concern is about the inverse problem: If we know that the spectrum with respect to above boundary conditions coincides as above, what can we say about the potential $v(x)$? Does it have to be identically zero? Thanks!","Consider the eigenvalue equation for the Hill operator  $$L(y)= -y''+ v(x) y = \lambda y, \quad x\in \mathbb{R},$$ where $v(x)$ is any potential and $\lambda$ is the spectral parameter. If $v(x) \equiv 0$, the spectrum of $L$ subject to periodic boundary conditions ( bc ): $y(0)=y(\pi)$, $ y'(0)=y'(\pi)$ and anti-periodic bc : $ y(0)=-y(\pi)$, $ y'(0)=-y'(\pi)$ coincides with the spectrum subject to Dirichlet bc : $y(0)=y(\pi)=0$; that is $\lambda_n=n^2$, $n \in \mathbb{N}$, if $n$ is even or $n$ is odd, respectively. My concern is about the inverse problem: If we know that the spectrum with respect to above boundary conditions coincides as above, what can we say about the potential $v(x)$? Does it have to be identically zero? Thanks!",,"['functional-analysis', 'ordinary-differential-equations', 'operator-theory', 'spectral-theory']"
69,Solve a differential equation using Fourier series,Solve a differential equation using Fourier series,,"Assume I have a second order differential equation $\ddot{x} = F(x,\dot{x})$ (or an equivalent equation of first order) and that I know there is a periodic solution to it (for simplicity's sake, assume the period is $1$). Is there a way to solve this kind of problem passing through Fourier series? My Idea would be to try to rephrase this as a differential equation of the Fourier coefficients and solve for an equilibrium (i.e. a periodic solution). Is this possible? The problem which gave me the idea is the following: There is an electron (point particle, mass $m$, charge $e$) in a space filled by a planar EM-wave, so $\vec{E} = (E\sin(kx-\omega t),0,0)$ and $\vec{B} = (0,B\sin(kx-\omega t),0)$ where $cB=E$. The electron is excited by the wave and emits EM-radiation. Find the emitted radiation. Now, the Lorenz force gives us the equation $m\ddot{x} = e(\vec{E}+\dot{x}\wedge\vec{B})$ to solve. Note: in the problem I can neglige the contribution of the magnetic field (since $B\ll E$), so I get standard oscillatory motion.","Assume I have a second order differential equation $\ddot{x} = F(x,\dot{x})$ (or an equivalent equation of first order) and that I know there is a periodic solution to it (for simplicity's sake, assume the period is $1$). Is there a way to solve this kind of problem passing through Fourier series? My Idea would be to try to rephrase this as a differential equation of the Fourier coefficients and solve for an equilibrium (i.e. a periodic solution). Is this possible? The problem which gave me the idea is the following: There is an electron (point particle, mass $m$, charge $e$) in a space filled by a planar EM-wave, so $\vec{E} = (E\sin(kx-\omega t),0,0)$ and $\vec{B} = (0,B\sin(kx-\omega t),0)$ where $cB=E$. The electron is excited by the wave and emits EM-radiation. Find the emitted radiation. Now, the Lorenz force gives us the equation $m\ddot{x} = e(\vec{E}+\dot{x}\wedge\vec{B})$ to solve. Note: in the problem I can neglige the contribution of the magnetic field (since $B\ll E$), so I get standard oscillatory motion.",,"['ordinary-differential-equations', 'fourier-analysis', 'physics', 'fourier-series']"
70,Rescaling an ODE,Rescaling an ODE,,"I have a Cauchy problem $$ \left\{ \begin{array}{l} \dot{x} = f(x) \\ x(0) = x_0 \end{array} \right. $$ with $x = x(t)$. Suppose to have $y(t) = a x\left(\frac{t}{b}\right)$, where $a$ and $b$ are real constants. I would like to write another Cauchy problem for the variable $y$: $$ \left\{ \begin{array}{l} \dot{y} = g(y, a, b) \\ y(0) = y_0(x_0, a, b) \end{array} \right. $$ It is easy to show that $y(0) = y_0(x_0, a, b) = ax_0$. What can I say about $g(y, a, b)$?","I have a Cauchy problem $$ \left\{ \begin{array}{l} \dot{x} = f(x) \\ x(0) = x_0 \end{array} \right. $$ with $x = x(t)$. Suppose to have $y(t) = a x\left(\frac{t}{b}\right)$, where $a$ and $b$ are real constants. I would like to write another Cauchy problem for the variable $y$: $$ \left\{ \begin{array}{l} \dot{y} = g(y, a, b) \\ y(0) = y_0(x_0, a, b) \end{array} \right. $$ It is easy to show that $y(0) = y_0(x_0, a, b) = ax_0$. What can I say about $g(y, a, b)$?",,['ordinary-differential-equations']
71,Error analysis for 4th order Runge-Kutta,Error analysis for 4th order Runge-Kutta,,Could anyone explain to me how to reduce the error propagated by using Runge-Kutta of order 4? Or can anyone give me a nice reference to it?,Could anyone explain to me how to reduce the error propagated by using Runge-Kutta of order 4? Or can anyone give me a nice reference to it?,,"['ordinary-differential-equations', 'reference-request', 'numerical-methods', 'approximation', 'runge-kutta-methods']"
72,Approximate Differential Equation?,Approximate Differential Equation?,,"Let $x \in \mathbb{R}$ be a variable and $c\in\mathbb{R}$ a parameter. Also, let $f(x,c)$ be a function dependent on $x$ and $c$. Furthermore, define a Differential Equation which is solved by the function $y(x)$: $$\left(\frac{d}{dx}\right)^2 y(x) + f(x,c)y(x)=0$$ Suppose, the general Differential Equation is too hard to solve, but for practical reasons only the solution with $c\approx 0$ is required. Naively I would expand $f(x,c)$ around $c=0$ and then compute the approximate Differential Equation. To the order $O(c^1)$ for example I would have to solve the following (where a prime denotes a derivative in respect to $c$): $$\left(\frac{d}{dx}\right)^2 y(x) + \big(f(x,0)+f'(x,0)~c + O(c^2)\big)y(x)=0$$ If $f(x,0)$ and $f'(x,0)$ turn out to be simple enough and if we neglect the $O(c^2)$ terms, this approximated equation might turn out to be solvable. Does this give a mathematically valid approximation for the solution $y(x)$? Maybe there are some subtleties which I did not consider? For instance, I have the feeling that the solution $y(x)$ should also be considered as a function of $c$ (as in $y(x,c)$) and be somehow involved in the expansion. It would be nice if someone knowledgeable could shed some light onto this.","Let $x \in \mathbb{R}$ be a variable and $c\in\mathbb{R}$ a parameter. Also, let $f(x,c)$ be a function dependent on $x$ and $c$. Furthermore, define a Differential Equation which is solved by the function $y(x)$: $$\left(\frac{d}{dx}\right)^2 y(x) + f(x,c)y(x)=0$$ Suppose, the general Differential Equation is too hard to solve, but for practical reasons only the solution with $c\approx 0$ is required. Naively I would expand $f(x,c)$ around $c=0$ and then compute the approximate Differential Equation. To the order $O(c^1)$ for example I would have to solve the following (where a prime denotes a derivative in respect to $c$): $$\left(\frac{d}{dx}\right)^2 y(x) + \big(f(x,0)+f'(x,0)~c + O(c^2)\big)y(x)=0$$ If $f(x,0)$ and $f'(x,0)$ turn out to be simple enough and if we neglect the $O(c^2)$ terms, this approximated equation might turn out to be solvable. Does this give a mathematically valid approximation for the solution $y(x)$? Maybe there are some subtleties which I did not consider? For instance, I have the feeling that the solution $y(x)$ should also be considered as a function of $c$ (as in $y(x,c)$) and be somehow involved in the expansion. It would be nice if someone knowledgeable could shed some light onto this.",,"['ordinary-differential-equations', 'approximation']"
73,Links to pdf-articles or books where there is an information on some linear integral operator,Links to pdf-articles or books where there is an information on some linear integral operator,,"Please write me links to pdf-articles or books where there is some information on properties of operators like these: $$ (Af)(x,y)=\int_{D}\frac{f(z) \, dz}{|x-z| |z-y|} $$ or $$ (Bf)(x,y)=\int_D \frac{f(z) \, dz}{|x-z|^{\alpha} |z-y|^\beta}. $$ Here $z\in D\subset\mathbb{R}^{3}$, $D$ is a smooth bounded domain of $\mathbb{R}^{3}$; $x\in P\subset\mathbb{R}^{3}$, $y\in Q\subset\mathbb{R}^{3}$; $P\cap Q=\emptyset$, $Q\cap D=\emptyset$, $D\cap P=\emptyset$; $f:D\to\mathbb{R}$; $\alpha>0$, $\beta>0$. Thank you very much in advance!","Please write me links to pdf-articles or books where there is some information on properties of operators like these: $$ (Af)(x,y)=\int_{D}\frac{f(z) \, dz}{|x-z| |z-y|} $$ or $$ (Bf)(x,y)=\int_D \frac{f(z) \, dz}{|x-z|^{\alpha} |z-y|^\beta}. $$ Here $z\in D\subset\mathbb{R}^{3}$, $D$ is a smooth bounded domain of $\mathbb{R}^{3}$; $x\in P\subset\mathbb{R}^{3}$, $y\in Q\subset\mathbb{R}^{3}$; $P\cap Q=\emptyset$, $Q\cap D=\emptyset$, $D\cap P=\emptyset$; $f:D\to\mathbb{R}$; $\alpha>0$, $\beta>0$. Thank you very much in advance!",,"['reference-request', 'ordinary-differential-equations']"
74,Find $\alpha$ such that $y'=\sqrt{1+y^4}-|y|^\alpha$ has global solutions,Find  such that  has global solutions,\alpha y'=\sqrt{1+y^4}-|y|^\alpha,"How do I find $\alpha$ such that $y'=\sqrt{1+y^4}-|y|^\alpha$ has global solutions? For example, imposing $y'=0$ for $\alpha=4$ we get that for solutions with starting point in $$[-(\frac{1+\sqrt{5}}{2})^{1/4}, +(\frac{1+\sqrt{5}}{2})^{1/4}]$$ have global solutions while the others, with a comparison, can be shown to have vertical asymptotes. In general, I can ""see"", but I'm not able to carry out all the calculations needed, that when $2\alpha>4$, the equation $y'=0$ has solutions, thus we can find an interval inside which the solutions are global, while outside they are not (how to prove?) When $\alpha=2$ all the solutions are global, this I can prove by showing that $y'\rightarrow 0 \;(y\rightarrow \infty)$. When $\alpha<2$ we have $y'\rightarrow \infty \;(y\rightarrow \infty)$ but I can't really conclude something here. My suggestion is that there are no global solutions (maybe all?).","How do I find $\alpha$ such that $y'=\sqrt{1+y^4}-|y|^\alpha$ has global solutions? For example, imposing $y'=0$ for $\alpha=4$ we get that for solutions with starting point in $$[-(\frac{1+\sqrt{5}}{2})^{1/4}, +(\frac{1+\sqrt{5}}{2})^{1/4}]$$ have global solutions while the others, with a comparison, can be shown to have vertical asymptotes. In general, I can ""see"", but I'm not able to carry out all the calculations needed, that when $2\alpha>4$, the equation $y'=0$ has solutions, thus we can find an interval inside which the solutions are global, while outside they are not (how to prove?) When $\alpha=2$ all the solutions are global, this I can prove by showing that $y'\rightarrow 0 \;(y\rightarrow \infty)$. When $\alpha<2$ we have $y'\rightarrow \infty \;(y\rightarrow \infty)$ but I can't really conclude something here. My suggestion is that there are no global solutions (maybe all?).",,['ordinary-differential-equations']
75,ODE: continuous dependence on parameters,ODE: continuous dependence on parameters,,"Is it true that the solutions of the problem: $$\begin{cases} \frac{\text{d}}{\text{d} s} [s^{2-2/N} u^\prime (s)] + \frac{\lambda}{c_N^2}\ u(s)=0 \\ u(\bar{s})=1\\ u^\prime (\bar{s})=-\frac{\lambda}{c_N^2}\ \bar{s}^{2/N-1}\end{cases}$$ (here $N\in \mathbb{N}$, $N\geq 2$ and $c_N>0$ is a suitable constant) depend continuously, or even smoothly, on both parameters $\lambda$ and $\bar{s}$? Where can I learn about continuous or smooth dependence for this kind of problems?","Is it true that the solutions of the problem: $$\begin{cases} \frac{\text{d}}{\text{d} s} [s^{2-2/N} u^\prime (s)] + \frac{\lambda}{c_N^2}\ u(s)=0 \\ u(\bar{s})=1\\ u^\prime (\bar{s})=-\frac{\lambda}{c_N^2}\ \bar{s}^{2/N-1}\end{cases}$$ (here $N\in \mathbb{N}$, $N\geq 2$ and $c_N>0$ is a suitable constant) depend continuously, or even smoothly, on both parameters $\lambda$ and $\bar{s}$? Where can I learn about continuous or smooth dependence for this kind of problems?",,"['reference-request', 'ordinary-differential-equations']"
76,Condition so that $y''+p(x)y'+q(x)y=0$ can be converted in a ODE with constant coefficients,Condition so that  can be converted in a ODE with constant coefficients,y''+p(x)y'+q(x)y=0,"I have to find a necessary and a sufficient condition for the functions $p$ and $q$ so that the linear differential equation : $y''+p(x)y'+q(x)y=0$ can be converted in a linear differential equation with constant coefficients by changing the independent variable of the equation. Can someone help with this or point me in the right direction? Thanks in advance! [EDIT:] First of all, I'm not sure if I should post this as an answer to my own question or as an edit. My apologies. With the tip of Antonio I've did the following, but I'm still not sure if it's completely correct. $\Phi(t)=y(x(t))$ $\Phi^{'}(t)=y^{'}(x(t)).x^{'}(t)$ $\Phi^{''}(t)=y^{''}(x(t)).(x^{'}(t))^{2}+y^{'}(x(t)).x^{''}(t)$ $y^{''}(t)+p(x)y^{'}(t)+q(x)y(t)=0$ $\frac{1}{q(x)}y^{''}(t)+\frac{p(x)}{q(x)}y^{'}(t)+y(t)=0$ $\frac{1}{q(x)}=(x^{'}(t))^{2}\Longrightarrow x^{'}(t)=\frac{1}{\sqrt{q(x)}}$ Condition 1: $\forall x:q(x)>0 $ $x^{''}(t)=\frac{x^{'}(t).q^{'}(x)}{2\sqrt{q(x)}}$ $x^{''}(t)=\frac{q^{'}(x)}{2.q(x)}$ Condition 2: $p(x)=\frac{1}{2}q^{'}(x)$","I have to find a necessary and a sufficient condition for the functions $p$ and $q$ so that the linear differential equation : $y''+p(x)y'+q(x)y=0$ can be converted in a linear differential equation with constant coefficients by changing the independent variable of the equation. Can someone help with this or point me in the right direction? Thanks in advance! [EDIT:] First of all, I'm not sure if I should post this as an answer to my own question or as an edit. My apologies. With the tip of Antonio I've did the following, but I'm still not sure if it's completely correct. $\Phi(t)=y(x(t))$ $\Phi^{'}(t)=y^{'}(x(t)).x^{'}(t)$ $\Phi^{''}(t)=y^{''}(x(t)).(x^{'}(t))^{2}+y^{'}(x(t)).x^{''}(t)$ $y^{''}(t)+p(x)y^{'}(t)+q(x)y(t)=0$ $\frac{1}{q(x)}y^{''}(t)+\frac{p(x)}{q(x)}y^{'}(t)+y(t)=0$ $\frac{1}{q(x)}=(x^{'}(t))^{2}\Longrightarrow x^{'}(t)=\frac{1}{\sqrt{q(x)}}$ Condition 1: $\forall x:q(x)>0 $ $x^{''}(t)=\frac{x^{'}(t).q^{'}(x)}{2\sqrt{q(x)}}$ $x^{''}(t)=\frac{q^{'}(x)}{2.q(x)}$ Condition 2: $p(x)=\frac{1}{2}q^{'}(x)$",,"['calculus', 'ordinary-differential-equations']"
77,How to calculate Floquet exponents?,How to calculate Floquet exponents?,,"I want to apply Floquet theory to analyse the stability of the periodic solutions for a system of differential equations. I understand the theoretical portion, but how can I actually find the Floquet exponents? I searched a lot but I couldn't find any simple example. Would someone help me with an example for a two by two system of ODEs? I will be grateful. PS: For example my differential equation is: $$\dot{x}=\mu-d x,$$ $\mu$ and $d$ are periodic functions of t. I am adding more work that I did on this problem. I made it homogeneous equation by using a transformation $y=x-\frac{\mu}{d}$. The equation becomes: $$\dot{y}=-d y$$ Solving this gives me: $$y=c_0 e^{\int{-d(t)dt}}$$ Now the $$\phi(t)=e^{\int{-d(t)dt}}$$ and $C=1$. Here $\phi(t)$ is the fundamental matrix. This means that the Floquet multiplier is $1$. Is it so?","I want to apply Floquet theory to analyse the stability of the periodic solutions for a system of differential equations. I understand the theoretical portion, but how can I actually find the Floquet exponents? I searched a lot but I couldn't find any simple example. Would someone help me with an example for a two by two system of ODEs? I will be grateful. PS: For example my differential equation is: $$\dot{x}=\mu-d x,$$ $\mu$ and $d$ are periodic functions of t. I am adding more work that I did on this problem. I made it homogeneous equation by using a transformation $y=x-\frac{\mu}{d}$. The equation becomes: $$\dot{y}=-d y$$ Solving this gives me: $$y=c_0 e^{\int{-d(t)dt}}$$ Now the $$\phi(t)=e^{\int{-d(t)dt}}$$ and $C=1$. Here $\phi(t)$ is the fundamental matrix. This means that the Floquet multiplier is $1$. Is it so?",,['ordinary-differential-equations']
78,"Behaviour of $r'=r-r^3 , \theta'=(\sin\theta)^2+a$",Behaviour of,"r'=r-r^3 , \theta'=(\sin\theta)^2+a",What are the local and global behavior of solutions of $r'=r-r^3$ $\theta'=(\sin\theta)^2+a$ at the bifurcation value $a=-1$?,What are the local and global behavior of solutions of $r'=r-r^3$ $\theta'=(\sin\theta)^2+a$ at the bifurcation value $a=-1$?,,['ordinary-differential-equations']
79,Integrating angular velocity to obtain orientation,Integrating angular velocity to obtain orientation,,"Suppose that $\gamma:[0,1]\to \operatorname{SO}(3)$ is a path in the space of orientation preserving rotations of $\mathbb R^3$.  It is classical that we can find a corresponding $\omega:[0,1]\to \mathbb R^3$, the angular velocity, such that if $r(t)=\gamma(t)r(0)$, then $r'(t)=\omega(t)\times r(t)$ (where $\times$ is the cross product).  Phrased differently, $\frac{d}{dt}\left(\gamma(t)\right)\gamma^{-1}(t)$ is skew-symmetric with respect to an orthonormal basis.  The correspondence between these two statements comes from the formula $$\omega\times v =\pmatrix{0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0  }v.$$ It is worth pointing out that this formula underlies an isomorphism of Lie algebras $(\mathbb R^3,\times)\cong \mathfrak{so}_3$, given by $x\mapsto ad_x$ Assuming whatever niceness we need (likely just continuity), given $\omega(t)$, there is a unique solution to the differential equation $$\gamma'(t)=\omega(t)\gamma(t);\quad \gamma(0)=I.$$ My question: Is there any nice expression for $\gamma$ in terms of $\omega$, the way there is when solving first order linear differential equations? What I have tried so far: This is a first order linear system of differential equations, and so there should be some well established theory regarding the solution of such systems.  However, I have only seen the constant coefficient case before, and my initial attempts to generalize the solution have met with some resistance.  In particular, the formula $\frac{d}{dt}e^{A(t)}=A'(t)e^{A(t)}$ doesn't hold for matrices unless $[A(t),A'(t)]=0$, being replaced by the more complicated formula $\frac{d}{dt}e^{A(t)}=e^{\operatorname{Ad}_{A(t)}}(A'(t))e^{A(t)}$.  As such, a solution of the form $e^{A(t)}$ would have to satisfy $$\omega(t)=e^{\operatorname{Ad}_{A(t)}}(A'(t)).$$ The problem can be simplified somewhat since $A(t)$ must be skew-symmetric.  By using the realization of the bracket as coming from the cross product and combining that with properties of the cross product, $e^{\operatorname{Ad}_X}Y$ can be rewritten in more explicit, if not simpler, terms.  However, it does not seem to make the problem any more solvable.","Suppose that $\gamma:[0,1]\to \operatorname{SO}(3)$ is a path in the space of orientation preserving rotations of $\mathbb R^3$.  It is classical that we can find a corresponding $\omega:[0,1]\to \mathbb R^3$, the angular velocity, such that if $r(t)=\gamma(t)r(0)$, then $r'(t)=\omega(t)\times r(t)$ (where $\times$ is the cross product).  Phrased differently, $\frac{d}{dt}\left(\gamma(t)\right)\gamma^{-1}(t)$ is skew-symmetric with respect to an orthonormal basis.  The correspondence between these two statements comes from the formula $$\omega\times v =\pmatrix{0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0  }v.$$ It is worth pointing out that this formula underlies an isomorphism of Lie algebras $(\mathbb R^3,\times)\cong \mathfrak{so}_3$, given by $x\mapsto ad_x$ Assuming whatever niceness we need (likely just continuity), given $\omega(t)$, there is a unique solution to the differential equation $$\gamma'(t)=\omega(t)\gamma(t);\quad \gamma(0)=I.$$ My question: Is there any nice expression for $\gamma$ in terms of $\omega$, the way there is when solving first order linear differential equations? What I have tried so far: This is a first order linear system of differential equations, and so there should be some well established theory regarding the solution of such systems.  However, I have only seen the constant coefficient case before, and my initial attempts to generalize the solution have met with some resistance.  In particular, the formula $\frac{d}{dt}e^{A(t)}=A'(t)e^{A(t)}$ doesn't hold for matrices unless $[A(t),A'(t)]=0$, being replaced by the more complicated formula $\frac{d}{dt}e^{A(t)}=e^{\operatorname{Ad}_{A(t)}}(A'(t))e^{A(t)}$.  As such, a solution of the form $e^{A(t)}$ would have to satisfy $$\omega(t)=e^{\operatorname{Ad}_{A(t)}}(A'(t)).$$ The problem can be simplified somewhat since $A(t)$ must be skew-symmetric.  By using the realization of the bracket as coming from the cross product and combining that with properties of the cross product, $e^{\operatorname{Ad}_X}Y$ can be rewritten in more explicit, if not simpler, terms.  However, it does not seem to make the problem any more solvable.",,"['ordinary-differential-equations', 'lie-groups']"
80,Complex nonlinear differential equation,Complex nonlinear differential equation,,"I have the following nonlinear differential equation: $$\ddot z(t)-\sin(z(t))=0$$ where $z(t)$ is a complex variable. The solution of the same equation with $z(t)$ real, is a function of Jacobi amplitude integral. What happens when $z(t)=z_0(t)+iz_1(t)$?","I have the following nonlinear differential equation: $$\ddot z(t)-\sin(z(t))=0$$ where $z(t)$ is a complex variable. The solution of the same equation with $z(t)$ real, is a function of Jacobi amplitude integral. What happens when $z(t)=z_0(t)+iz_1(t)$?",,"['complex-analysis', 'ordinary-differential-equations']"
81,The linearization of a gradient vector field along a heteroclinic connection,The linearization of a gradient vector field along a heteroclinic connection,,"A gradient vector field $X$ in $\mathbb{R}^n$ has two equilibria  $x_1, x_2$. The vector field   defines a cooperative dynamical system. The linearization about $x_1$ has one positive eigenvalues and n-1 negative, while  about $x_2$, n negative eigenvalues.  Let $x(t)$ be the heteroclinic connection of the two equilibria (the mountain pass). My question is: is it  true that the matrix $D_{x(t)}X$ of the linearization of the gradient system along $x(t)$, $\dot{\xi}=D_{x(t)}X\: \xi$, has a decreasing trace  with respect to t? Thank you.","A gradient vector field $X$ in $\mathbb{R}^n$ has two equilibria  $x_1, x_2$. The vector field   defines a cooperative dynamical system. The linearization about $x_1$ has one positive eigenvalues and n-1 negative, while  about $x_2$, n negative eigenvalues.  Let $x(t)$ be the heteroclinic connection of the two equilibria (the mountain pass). My question is: is it  true that the matrix $D_{x(t)}X$ of the linearization of the gradient system along $x(t)$, $\dot{\xi}=D_{x(t)}X\: \xi$, has a decreasing trace  with respect to t? Thank you.",,['ordinary-differential-equations']
82,Homogeneity lemma,Homogeneity lemma,,"I am studying Homegeneity lemma.  I am not understanding the following paragraph: Given any fixed unit vector $c \in S^n$ , consider the differential equations $$\frac{dx_i}{dt} = c f(x_1,x_2,\ldots,x_n)$$ for $i=1,2,\ldots,n$ , where $f$ is a smooth function from $\mathbb{R}^n$ to $\mathbb{R}$ with $f(x)=0$ for outside the unit sphere and on the sphere and $f(x)>0$ for inside the unit interval. For any $y \in \mathbb{R}^n$ these equations have a unique solution $x = x(t)$ , defined all real numbers which satisfies the initial condition $x(0)=y$ . We will use the notation $x ( t ) = F_t (y)$ for this solution. Then clearly $F_t(y)$ is defined for all $t$ and $y$ and depends smoothly on $t$ and $y$ , $F_0(y) =y$ , $F_{t+s}(y) = F_t \circ F_s(y)$ .","I am studying Homegeneity lemma.  I am not understanding the following paragraph: Given any fixed unit vector , consider the differential equations for , where is a smooth function from to with for outside the unit sphere and on the sphere and for inside the unit interval. For any these equations have a unique solution , defined all real numbers which satisfies the initial condition . We will use the notation for this solution. Then clearly is defined for all and and depends smoothly on and , , .","c \in S^n \frac{dx_i}{dt} = c f(x_1,x_2,\ldots,x_n) i=1,2,\ldots,n f \mathbb{R}^n \mathbb{R} f(x)=0 f(x)>0 y \in \mathbb{R}^n x = x(t) x(0)=y x ( t ) = F_t (y) F_t(y) t y t y F_0(y) =y F_{t+s}(y) = F_t \circ F_s(y)","['ordinary-differential-equations', 'differential-geometry', 'differential-topology']"
83,Prove that the first positive root of the solution to the Lane-Emden equation increases steadily with $n$.,Prove that the first positive root of the solution to the Lane-Emden equation increases steadily with .,n,"Let $\lambda$ be the first positive value for which $y=0$ where $y(x)$ satisfy the following differential equation $$ y''+\frac{2}{x}y'+y^n=0,\qquad\text{where }n\in\mathbb{R},\ y(0)=1,\text{ and }\ y'(0)=0. $$ This equation is known as the Lane-Emden equation and has analytic solutions for $n=0,1,5$. For $n=0$,  $$ y(x)=1-\frac{x^2}{6}\Rightarrow\lambda=\sqrt{6} $$ For $n=1$,  $$ y(x)=\frac{\sin{x}}{x}\Rightarrow\lambda=\pi $$ And for $n = 5$,  $$ y(x)=\frac{1}{\sqrt{1+\frac{x^2}{3}}}\Rightarrow\lambda=\infty $$ We want to show that $\lambda$ increases steadily as $n$ goes from $0$ to $5$. We can verify numerically that it is indeed true but it has not yet been proved analytically.","Let $\lambda$ be the first positive value for which $y=0$ where $y(x)$ satisfy the following differential equation $$ y''+\frac{2}{x}y'+y^n=0,\qquad\text{where }n\in\mathbb{R},\ y(0)=1,\text{ and }\ y'(0)=0. $$ This equation is known as the Lane-Emden equation and has analytic solutions for $n=0,1,5$. For $n=0$,  $$ y(x)=1-\frac{x^2}{6}\Rightarrow\lambda=\sqrt{6} $$ For $n=1$,  $$ y(x)=\frac{\sin{x}}{x}\Rightarrow\lambda=\pi $$ And for $n = 5$,  $$ y(x)=\frac{1}{\sqrt{1+\frac{x^2}{3}}}\Rightarrow\lambda=\infty $$ We want to show that $\lambda$ increases steadily as $n$ goes from $0$ to $5$. We can verify numerically that it is indeed true but it has not yet been proved analytically.",,['ordinary-differential-equations']
84,Collision of eigenvalues of a linear ODE (Krein collisions),Collision of eigenvalues of a linear ODE (Krein collisions),,"I am trying to understand the so called Krein collisions in Hamiltonian mechanics but I shall formulate the question in a rather general way. Suppose we have the following linear ODE: $ \dot{v}= Kv $ in a space of dimension  $2n$  where $K$ is a constant coefficient matrix with real valued coefficients satisfying the following properties: the characteristic polynomial of $K$ is even and if $\lambda$ is an eigenvalue of multiplicity $k$, then $-\lambda$ is also an eigenvalue of multiplicity $k$ and if $0$ is an eigenvalue, then it has even multiplicity. Now suppose that the eigenvalues depend continuously on a parameter $\mu$ and suppose the system has an equilibrium point for which it is linearly stable (i.e, all the eigenvalues start off on the imaginary axis because the even nature of the characteristic polynomial prevents a scenario where the eigenvalues can only have a negative real part). Assume also that zero cannot be an eigenvalue associated with the equilibrium point. It is then asserted that the eigenvalues cannot leave the imaginary axis unless they first collide. Why is this necessarily true? I am trying to envision the following scenario. Consider a four dimensional system so that at $\mu=\mu_0$, there are two eigenvalues $\pm 2i$ of multiplicity $2$ which then move, as $\mu$ varies towards $\pm i$ and at $\mu=\mu_0 + T$, they are at $\pm i$. Now why cannot they split off so into a quartet of eigenvalues (the so called Krein quartet) $\pm i \pm \varepsilon$ at the next instant of time. Thank you.","I am trying to understand the so called Krein collisions in Hamiltonian mechanics but I shall formulate the question in a rather general way. Suppose we have the following linear ODE: $ \dot{v}= Kv $ in a space of dimension  $2n$  where $K$ is a constant coefficient matrix with real valued coefficients satisfying the following properties: the characteristic polynomial of $K$ is even and if $\lambda$ is an eigenvalue of multiplicity $k$, then $-\lambda$ is also an eigenvalue of multiplicity $k$ and if $0$ is an eigenvalue, then it has even multiplicity. Now suppose that the eigenvalues depend continuously on a parameter $\mu$ and suppose the system has an equilibrium point for which it is linearly stable (i.e, all the eigenvalues start off on the imaginary axis because the even nature of the characteristic polynomial prevents a scenario where the eigenvalues can only have a negative real part). Assume also that zero cannot be an eigenvalue associated with the equilibrium point. It is then asserted that the eigenvalues cannot leave the imaginary axis unless they first collide. Why is this necessarily true? I am trying to envision the following scenario. Consider a four dimensional system so that at $\mu=\mu_0$, there are two eigenvalues $\pm 2i$ of multiplicity $2$ which then move, as $\mu$ varies towards $\pm i$ and at $\mu=\mu_0 + T$, they are at $\pm i$. Now why cannot they split off so into a quartet of eigenvalues (the so called Krein quartet) $\pm i \pm \varepsilon$ at the next instant of time. Thank you.",,"['ordinary-differential-equations', 'classical-mechanics']"
85,"Proving $\sup_{K(0,1)}|u|\leq \frac{1}{4}\sup_{K(0,1)}|f|$ using Green function",Proving  using Green function,"\sup_{K(0,1)}|u|\leq \frac{1}{4}\sup_{K(0,1)}|f|","I have a question to the following problem. Let $f\in C(K(0,1))\subset \mathbb{R}^2.$ Prove that $$\sup_{K(0,1)}|u|\leq \frac{1}{4}\sup_{K(0,1)}|f|,$$ if $u(x,y)$ is a solution to the problem: $$\Delta u=f,  \ \mbox{on}  \ K(0,1),$$ $$u(x,y)=0 \ \mbox{the boundary of} \ K(0,1).$$ Let $G(x,y,x',y')$ be the appropriate Green function for $K(0,1).$ I tried to solve the problem, but came to a problem. My solution goes like this: $$|u(x,y)|=|\int_{K(0,1)}G(x,y,x',y')f(x',y')~dx'~dy'|\leq \sup_{K(0,1)}|f|\int_{K(0,1)}|G(x,y,x',y')|~dx'~dy'.$$ But now I have a problem. How to get rid of the absolute value inside the integral. Because, if it were not there, then $$\int_{K(0,1)}G(x,y,x',y')~dx'~dy'=\frac{x^2+y^2-1}{4},$$ which is the unique solution to the problem: $$\Delta u=1,  \ \mbox{on}  \ K(0,1),$$ $$u(x,y)=0 \text{ the boundary of } K(0,1).$$ Then it is easy to establish, that $|\frac{x^2+y^2-1}{4}|\leq \frac{1}{4}$ on $K(0,1).$ Can someone please tell me how to write a fully correct solution to this problem.","I have a question to the following problem. Let $f\in C(K(0,1))\subset \mathbb{R}^2.$ Prove that $$\sup_{K(0,1)}|u|\leq \frac{1}{4}\sup_{K(0,1)}|f|,$$ if $u(x,y)$ is a solution to the problem: $$\Delta u=f,  \ \mbox{on}  \ K(0,1),$$ $$u(x,y)=0 \ \mbox{the boundary of} \ K(0,1).$$ Let $G(x,y,x',y')$ be the appropriate Green function for $K(0,1).$ I tried to solve the problem, but came to a problem. My solution goes like this: $$|u(x,y)|=|\int_{K(0,1)}G(x,y,x',y')f(x',y')~dx'~dy'|\leq \sup_{K(0,1)}|f|\int_{K(0,1)}|G(x,y,x',y')|~dx'~dy'.$$ But now I have a problem. How to get rid of the absolute value inside the integral. Because, if it were not there, then $$\int_{K(0,1)}G(x,y,x',y')~dx'~dy'=\frac{x^2+y^2-1}{4},$$ which is the unique solution to the problem: $$\Delta u=1,  \ \mbox{on}  \ K(0,1),$$ $$u(x,y)=0 \text{ the boundary of } K(0,1).$$ Then it is easy to establish, that $|\frac{x^2+y^2-1}{4}|\leq \frac{1}{4}$ on $K(0,1).$ Can someone please tell me how to write a fully correct solution to this problem.",,['ordinary-differential-equations']
86,What's this called? $\mathbb{C}[d/dx]$,What's this called?,\mathbb{C}[d/dx],The 'ring of differential operators wrt x' ? Thx.,The 'ring of differential operators wrt x' ? Thx.,,"['ordinary-differential-equations', 'ring-theory', 'notation']"
87,Check my solution - Modelling of a spring with Differential Equation,Check my solution - Modelling of a spring with Differential Equation,,"I am doing some work with differential equations. I have solved the following problem but am uncertain if I'm doing it correctly. Could someone look over it for me and check if I'm doing something wrong or not? Thanks in advance! The problem is as follows: $\hskip 1in$ My Solution: $$\frac{\partial u(x,t)}{\partial t} = \sum_{n=1}^{\infty} \sin(n \pi x)\big(a_n 100\pi \cos(100\pi t)-b_n 100\pi \sin(100 \pi t)\big)$$ From initial condition $\displaystyle\frac{\partial u(x,0)}{\partial t} = 0$: $$\frac{\partial u(x,0)}{\partial t} = \sum_{n=1}^{\infty} \sin(n \pi x)[a_n  100\pi] = 0 \\ \implies  a_n = 0$$ Therefore: $$u(x,t) = \sum_{n=1}^{\infty} \sin(n \pi x)\big(b_n\cos(100\pi t)\big).$$ Since $$b_m = \frac{2}{L} \int_{0}^{L}  u(x,0) \sin\left( \frac{m\pi x}{L}\right)dx, \quad\text{and}\quad  u(x,0) = 4\sin(3\pi x)),$$ it follows that $$b_m = \frac{2}{1} \int_{0}^{1}  4\sin(3\pi x) \sin\left( \frac{m\pi x}{1}\right)dx.$$ Due to property of sine function where $$\int_{0}^{L} \ \sin\left( \frac{n\pi x}{L}\right) \sin\left( \frac{m\pi x}{L}\right)dx= \begin{cases}L/2 & \text{if } n=m \\ 0 & \text{if } n\ne m, \end{cases}$$ when $m = 3$, $b_m = 2\cdot 4\cdot\frac{1}{2} = 4$ and $b_m=0$ otherwise. Finally, then: $$ u(x,t)= 4 \sin(3\pi x)\cos(100 \pi t).$$ If anyone could have a look over this it would be much appreciated. Thanks!","I am doing some work with differential equations. I have solved the following problem but am uncertain if I'm doing it correctly. Could someone look over it for me and check if I'm doing something wrong or not? Thanks in advance! The problem is as follows: $\hskip 1in$ My Solution: $$\frac{\partial u(x,t)}{\partial t} = \sum_{n=1}^{\infty} \sin(n \pi x)\big(a_n 100\pi \cos(100\pi t)-b_n 100\pi \sin(100 \pi t)\big)$$ From initial condition $\displaystyle\frac{\partial u(x,0)}{\partial t} = 0$: $$\frac{\partial u(x,0)}{\partial t} = \sum_{n=1}^{\infty} \sin(n \pi x)[a_n  100\pi] = 0 \\ \implies  a_n = 0$$ Therefore: $$u(x,t) = \sum_{n=1}^{\infty} \sin(n \pi x)\big(b_n\cos(100\pi t)\big).$$ Since $$b_m = \frac{2}{L} \int_{0}^{L}  u(x,0) \sin\left( \frac{m\pi x}{L}\right)dx, \quad\text{and}\quad  u(x,0) = 4\sin(3\pi x)),$$ it follows that $$b_m = \frac{2}{1} \int_{0}^{1}  4\sin(3\pi x) \sin\left( \frac{m\pi x}{1}\right)dx.$$ Due to property of sine function where $$\int_{0}^{L} \ \sin\left( \frac{n\pi x}{L}\right) \sin\left( \frac{m\pi x}{L}\right)dx= \begin{cases}L/2 & \text{if } n=m \\ 0 & \text{if } n\ne m, \end{cases}$$ when $m = 3$, $b_m = 2\cdot 4\cdot\frac{1}{2} = 4$ and $b_m=0$ otherwise. Finally, then: $$ u(x,t)= 4 \sin(3\pi x)\cos(100 \pi t).$$ If anyone could have a look over this it would be much appreciated. Thanks!",,"['ordinary-differential-equations', 'physics', 'fourier-series', 'mathematical-modeling']"
88,Gompertz growth equation,Gompertz growth equation,,":) Hi! I'm almost finished with a homework problem, but I cannot quite finish it. The problem is as follows: Given the Gompertz growth equation $$\frac{dN}{dt}=K(t)N(t),\ N(0)=N_0 \\ \frac{dK}{dt}=-\alpha K(t),\ K(0)=\beta,$$  I shall first determined a closed-form expression, which will be $K(t)=\beta e^{-\alpha t}$ and $$\frac{dN}{dt}=\beta e^{-\alpha t} N(t) \Leftrightarrow \\ \int\frac{1}{N(t)}dN=\int \beta e^{-\alpha t}dt \Leftrightarrow \\ \log[N(t)]=\frac{-\beta}{\alpha} e^{-\alpha t} + C \Leftrightarrow \\ N(t)=\exp\left( \frac{-\beta}{\alpha}e^{-\alpha t}\right) \frac{N_0}{\exp\left(\frac{-\beta}{\alpha}\right)};$$  the last factor is due to the initial condition. Then: Determine the limit $$B:=\lim_{t\rightarrow \infty}N(t), $$ it will be $B=\frac{N_0}{\exp\left(\frac{-\alpha}{\beta}\right)}$ (right?) Then: Show that this is equivalent to $$\frac{dN}{dt}=\alpha N \log\frac{B}{N},$$ I also managed that (the LHS is $KN$, then solve the logarithm), but THEN: For the cases $N_0>B, N_0 <B$, determine $\lim_{t\rightarrow -\infty }N(t)$. If I plug in $t=-\infty$ into $N(t)$, then I get just $0$. Or not? And with my solution for $B$, I get $\exp\left( \frac{-\beta}{\alpha}\right)<> 1$, but since $\alpha, \beta >0$, the exp cannot be smaller than one? I either have a blonde moment, or my solutions are wrong, but magically the equivalence in the $\log$-equation works! Can someone defuse my brain? :) -marie",":) Hi! I'm almost finished with a homework problem, but I cannot quite finish it. The problem is as follows: Given the Gompertz growth equation $$\frac{dN}{dt}=K(t)N(t),\ N(0)=N_0 \\ \frac{dK}{dt}=-\alpha K(t),\ K(0)=\beta,$$  I shall first determined a closed-form expression, which will be $K(t)=\beta e^{-\alpha t}$ and $$\frac{dN}{dt}=\beta e^{-\alpha t} N(t) \Leftrightarrow \\ \int\frac{1}{N(t)}dN=\int \beta e^{-\alpha t}dt \Leftrightarrow \\ \log[N(t)]=\frac{-\beta}{\alpha} e^{-\alpha t} + C \Leftrightarrow \\ N(t)=\exp\left( \frac{-\beta}{\alpha}e^{-\alpha t}\right) \frac{N_0}{\exp\left(\frac{-\beta}{\alpha}\right)};$$  the last factor is due to the initial condition. Then: Determine the limit $$B:=\lim_{t\rightarrow \infty}N(t), $$ it will be $B=\frac{N_0}{\exp\left(\frac{-\alpha}{\beta}\right)}$ (right?) Then: Show that this is equivalent to $$\frac{dN}{dt}=\alpha N \log\frac{B}{N},$$ I also managed that (the LHS is $KN$, then solve the logarithm), but THEN: For the cases $N_0>B, N_0 <B$, determine $\lim_{t\rightarrow -\infty }N(t)$. If I plug in $t=-\infty$ into $N(t)$, then I get just $0$. Or not? And with my solution for $B$, I get $\exp\left( \frac{-\beta}{\alpha}\right)<> 1$, but since $\alpha, \beta >0$, the exp cannot be smaller than one? I either have a blonde moment, or my solutions are wrong, but magically the equivalence in the $\log$-equation works! Can someone defuse my brain? :) -marie",,"['ordinary-differential-equations', 'limits']"
89,Prove that $C e^x$ is the only set of functions for which $f(x) = f'(x)$,Prove that  is the only set of functions for which,C e^x f(x) = f'(x),"I was wondering on the following and I probably know the answer already: NO . Is there another number with similar properties as $e$ ? So that the derivative of $ e^x$ is the same as the function itself. I can guess that it's probably not, because otherwise $e$ wouldn't be that special, but is there any proof of it?","I was wondering on the following and I probably know the answer already: NO . Is there another number with similar properties as ? So that the derivative of is the same as the function itself. I can guess that it's probably not, because otherwise wouldn't be that special, but is there any proof of it?",e  e^x e,"['calculus', 'ordinary-differential-equations', 'derivatives', 'exponential-function']"
90,positively homogeneous asymptotic expansion associated to the symbol of a pseudodifferential operator,positively homogeneous asymptotic expansion associated to the symbol of a pseudodifferential operator,,"I am currently reading about pseudodifferential operators and their symbols, and I came across the notion of classical pseudodifferential operators. For these it is possible to find an asymptotic expansion of the symbol in terms of positively homogeneous functions. So, according to the notes that I am reading we say that a pseudodifferential operator $A \in \Psi^m$ is classical if the symbol $\sigma_A(x, \xi)$ admits an asymptotic expansion \begin{equation} \sigma_A(x,\xi) \backsim \sum_{k \geq 0} a_{m - k} \, (x,\xi) \qquad (|\xi| \to \infty) \end{equation} where each $a_{m - k}$  satisfies the equation  \begin{equation} a_{m - k}\,\,(x,\lambda \xi) = \lambda^{(m - k)} \,\, a_{m - k}\,\,(x,\xi)\quad \text{for } \lambda > 0 \end{equation} Now, there is a comment in the notes that such an expansion has the special property of being unique (as opposed to general asymptotic expansions where the terms are not positively homogeneous). Unfortunately I cannot deduce myself why this is so. If someone could give some hints or details of the reason that would be really helpful, thanks a lot! EDIT: One guess I have is that, instead of the uniqueness of the expansion, what is acutally unique (or well defined in the context of positively homogeneous terms) is the leading homogeneous term $a_m$, i.e. the principal symbol. This is not the case of a general asymptotic expansion, that's why I suppose that this is what the lecture notes are referring to .. might that be a correct interpretation of the comment I mention ?","I am currently reading about pseudodifferential operators and their symbols, and I came across the notion of classical pseudodifferential operators. For these it is possible to find an asymptotic expansion of the symbol in terms of positively homogeneous functions. So, according to the notes that I am reading we say that a pseudodifferential operator $A \in \Psi^m$ is classical if the symbol $\sigma_A(x, \xi)$ admits an asymptotic expansion \begin{equation} \sigma_A(x,\xi) \backsim \sum_{k \geq 0} a_{m - k} \, (x,\xi) \qquad (|\xi| \to \infty) \end{equation} where each $a_{m - k}$  satisfies the equation  \begin{equation} a_{m - k}\,\,(x,\lambda \xi) = \lambda^{(m - k)} \,\, a_{m - k}\,\,(x,\xi)\quad \text{for } \lambda > 0 \end{equation} Now, there is a comment in the notes that such an expansion has the special property of being unique (as opposed to general asymptotic expansions where the terms are not positively homogeneous). Unfortunately I cannot deduce myself why this is so. If someone could give some hints or details of the reason that would be really helpful, thanks a lot! EDIT: One guess I have is that, instead of the uniqueness of the expansion, what is acutally unique (or well defined in the context of positively homogeneous terms) is the leading homogeneous term $a_m$, i.e. the principal symbol. This is not the case of a general asymptotic expansion, that's why I suppose that this is what the lecture notes are referring to .. might that be a correct interpretation of the comment I mention ?",,"['ordinary-differential-equations', 'functional-analysis', 'fourier-analysis', 'operator-theory']"
91,Showing uniqueness of solution to IVP under certain conditions,Showing uniqueness of solution to IVP under certain conditions,,"Consider the IVP $\mathbf{\dot x} = f(\mathbf{x},t)$ where $\mathbf{x}(0) = 0$, $f$ is continuous in some neighborhood of $(x,t) = (0,0)$ and $|f(x,t)-f(y,t)| \leq \frac{|x-y|}{t^\alpha}$.  I would like to show the uniquenesss of the solution if $\alpha\in (0,1)$. It seems like I would like to show that $f$ is locally Lipschitz in $x$ when $t = 0$ if $\alpha\in(0,1)$, (since it is clearly locally Lipschitz for all $t > 0$) but I haven't been able to tackle that by elementary methods (or any other, really).  Another idea might be to bound the sequence of Picard iterates, but that seems very messy.  A third idea would be to appeal to general facts about moduli of continuity.  The fact that a solution is unique when the integral $\int_0^x\frac{dx}{\omega(x)}$ diverges for the modulus $\omega$ seems almost useful, except that in this case my modulus is a function of both $x$ and $t$.  Am I in the right ballpark with these approaches, or is there something very obvious that I'm missing? Also, this is homework, so I would be grateful for a small hint rather than a full solution.  Thanks in advance!","Consider the IVP $\mathbf{\dot x} = f(\mathbf{x},t)$ where $\mathbf{x}(0) = 0$, $f$ is continuous in some neighborhood of $(x,t) = (0,0)$ and $|f(x,t)-f(y,t)| \leq \frac{|x-y|}{t^\alpha}$.  I would like to show the uniquenesss of the solution if $\alpha\in (0,1)$. It seems like I would like to show that $f$ is locally Lipschitz in $x$ when $t = 0$ if $\alpha\in(0,1)$, (since it is clearly locally Lipschitz for all $t > 0$) but I haven't been able to tackle that by elementary methods (or any other, really).  Another idea might be to bound the sequence of Picard iterates, but that seems very messy.  A third idea would be to appeal to general facts about moduli of continuity.  The fact that a solution is unique when the integral $\int_0^x\frac{dx}{\omega(x)}$ diverges for the modulus $\omega$ seems almost useful, except that in this case my modulus is a function of both $x$ and $t$.  Am I in the right ballpark with these approaches, or is there something very obvious that I'm missing? Also, this is homework, so I would be grateful for a small hint rather than a full solution.  Thanks in advance!",,['ordinary-differential-equations']
92,Lebesgue Line Integrals - Parametric Change of Variables,Lebesgue Line Integrals - Parametric Change of Variables,,"Consider the following Lebesgue integral in $\mathbb{R}^n$ $$ \int_C f(x) dx $$ Where $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is measurable and $C$ is a measurable subset of $\mathbb{R}^n$ that can be defined by the simple, differentiable, parametric curve $y(t)$ over the closed interval $[a,b]$. Does the following result (from Riemann integrals) hold? $$ \int_C f(x) dx = \int_b^a f(y(t)) |y'(t)| dt $$ Where $|v|$ denotes the 2-norm of the vector $v$. If so, I'm also wondering if one could also integrate over a set of disjoint curves to compute an integral over a larger set. To show what I mean, we first adopt an expanded notation for our parametric curve: $y(t,x)$. This simply denotes the particular curve, parameterized by $t$ who's image includes $x$.  Since we assume these curves are disjoint, $y(t,x) = y(t,z) \ $ if and only if there exists a $t$ such that $\ y(t,x) = z \ $ (and conversely swapping x and z). Since it is difficult to define a measure over these curves, I propose the a simple method: We define: $$g(x) = \frac{\int_b^a y(t,x) |y'(t,x)| dt}{\int_b^a |y'(t,x)| dt}$$ From what I learned in Riemann integration, the numerator here is the line integral over $y(t,x)$ and the denominator is the arc length.  Intuitively, the denominator is there to compensate for the fact that lines will be ""duplicated"" an amount equal to their measure in the following integral: $$ \int_X f(x) dx = \int_X g(x) dx $$ Where $X \ $ is a measurable subset of $\mathbb{R}^n$ and all the intervals $[a,b]$ are constructed in such a way as to never take the curve out of $X$. It seems that the above equality holds because the set of parametric curves form a partition of $X\ $ (detailed below). My primary concern is that, while the curves are disjoint, when considered together they can compress the measure and this compression is not fully compensated from by the norm of the gradient.  Certainly the $n$-dimensional change of variables theorem could be used, but I cannot see a way to write the set of parametric curves as a single injective transformation. We can show in general that if $B \ $ and $C \ $ form a partition of $A \ $ and: $$   g(x) = \begin{cases}            \frac{\int_B f(u) du}{\int_B du} & \text{if } \ x \in B \\            \frac{\int_C f(u) du}{\int_C du} & \text{if } \ x \in C    \end{cases} $$ Where $du = dx$. Consider the following construction: $$ \int_A g(x) dx = \int_B g(x) dx + \int_C g(x) dx$$ $$ = \int_B \frac{\int_B f(u) du}{\int_B du} dx + \int_C \frac{\int_C f(u) du}{\int_C du} dx$$  We note the inner integrals are constant w.r.t. $x$, yielding: $$ = \int_B f(u) du \frac{\int_B dx}{\int_B du} + \int_C f(u) du \frac{\int_B dx}{\int_B du}$$  $$ = \int_B f(u) du + \int_C f(u) du = \int_A f(u) du$$","Consider the following Lebesgue integral in $\mathbb{R}^n$ $$ \int_C f(x) dx $$ Where $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is measurable and $C$ is a measurable subset of $\mathbb{R}^n$ that can be defined by the simple, differentiable, parametric curve $y(t)$ over the closed interval $[a,b]$. Does the following result (from Riemann integrals) hold? $$ \int_C f(x) dx = \int_b^a f(y(t)) |y'(t)| dt $$ Where $|v|$ denotes the 2-norm of the vector $v$. If so, I'm also wondering if one could also integrate over a set of disjoint curves to compute an integral over a larger set. To show what I mean, we first adopt an expanded notation for our parametric curve: $y(t,x)$. This simply denotes the particular curve, parameterized by $t$ who's image includes $x$.  Since we assume these curves are disjoint, $y(t,x) = y(t,z) \ $ if and only if there exists a $t$ such that $\ y(t,x) = z \ $ (and conversely swapping x and z). Since it is difficult to define a measure over these curves, I propose the a simple method: We define: $$g(x) = \frac{\int_b^a y(t,x) |y'(t,x)| dt}{\int_b^a |y'(t,x)| dt}$$ From what I learned in Riemann integration, the numerator here is the line integral over $y(t,x)$ and the denominator is the arc length.  Intuitively, the denominator is there to compensate for the fact that lines will be ""duplicated"" an amount equal to their measure in the following integral: $$ \int_X f(x) dx = \int_X g(x) dx $$ Where $X \ $ is a measurable subset of $\mathbb{R}^n$ and all the intervals $[a,b]$ are constructed in such a way as to never take the curve out of $X$. It seems that the above equality holds because the set of parametric curves form a partition of $X\ $ (detailed below). My primary concern is that, while the curves are disjoint, when considered together they can compress the measure and this compression is not fully compensated from by the norm of the gradient.  Certainly the $n$-dimensional change of variables theorem could be used, but I cannot see a way to write the set of parametric curves as a single injective transformation. We can show in general that if $B \ $ and $C \ $ form a partition of $A \ $ and: $$   g(x) = \begin{cases}            \frac{\int_B f(u) du}{\int_B du} & \text{if } \ x \in B \\            \frac{\int_C f(u) du}{\int_C du} & \text{if } \ x \in C    \end{cases} $$ Where $du = dx$. Consider the following construction: $$ \int_A g(x) dx = \int_B g(x) dx + \int_C g(x) dx$$ $$ = \int_B \frac{\int_B f(u) du}{\int_B du} dx + \int_C \frac{\int_C f(u) du}{\int_C du} dx$$  We note the inner integrals are constant w.r.t. $x$, yielding: $$ = \int_B f(u) du \frac{\int_B dx}{\int_B du} + \int_C f(u) du \frac{\int_B dx}{\int_B du}$$  $$ = \int_B f(u) du + \int_C f(u) du = \int_A f(u) du$$",,"['integration', 'ordinary-differential-equations', 'measure-theory']"
93,Grand Prix Race,Grand Prix Race,,"Driver A has boon leading archrival B for a while by a steady 3 miles. Only 2 miles from the finish, driver A ran out of gas and decelerated thereafter at ta rate proportional to the square of his remaining speed. One mile later,driver A's speed was exactly halved.If driver B's speed remained constant,who won the race? i have tried the set up the relation$d^2x\over dt^2$=$K ({dx\over dt})^2$ and integrate it but dont know how to do.","Driver A has boon leading archrival B for a while by a steady 3 miles. Only 2 miles from the finish, driver A ran out of gas and decelerated thereafter at ta rate proportional to the square of his remaining speed. One mile later,driver A's speed was exactly halved.If driver B's speed remained constant,who won the race? i have tried the set up the relation$d^2x\over dt^2$=$K ({dx\over dt})^2$ and integrate it but dont know how to do.",,['ordinary-differential-equations']
94,Finding $\mathbf r(t)$ for the parameterized two-body equations of motion,Finding  for the parameterized two-body equations of motion,\mathbf r(t),"I'm trying to understand the equations of two-body motion. Namely, given the position, velocity and mass of two orbiting bodies at time $t$, how can I explicitly find their position and velocity for any arbitrary time? First place I looked was the Wikipedia article , which I followed until it got to ""Solving the equation for $\mathbf r(t)$ is the key to the two-body problem; general solution methods are described below."" Below, it talked about the motion being planar and/or a ""central force"", but I couldn't figure out how to get an $\mathbf r(t)$ function out of anything there. The question two-body problem circular orbits seems relevant, but only answers a specific sort of case. Finally, I found this article . I feel like what I'm looking for might be hidden in here, possibly equations (17) and (18). But I can't manage to get an $\mathbf r(t)$ out of them (is there a relationship between $\mathbf r(t)$ and $\dfrac{\mathrm d \mathbf r}{\mathrm dt}$?) Any help would be appreciated. Please forgive me if this is blindingly obvious. Many thanks.","I'm trying to understand the equations of two-body motion. Namely, given the position, velocity and mass of two orbiting bodies at time $t$, how can I explicitly find their position and velocity for any arbitrary time? First place I looked was the Wikipedia article , which I followed until it got to ""Solving the equation for $\mathbf r(t)$ is the key to the two-body problem; general solution methods are described below."" Below, it talked about the motion being planar and/or a ""central force"", but I couldn't figure out how to get an $\mathbf r(t)$ function out of anything there. The question two-body problem circular orbits seems relevant, but only answers a specific sort of case. Finally, I found this article . I feel like what I'm looking for might be hidden in here, possibly equations (17) and (18). But I can't manage to get an $\mathbf r(t)$ out of them (is there a relationship between $\mathbf r(t)$ and $\dfrac{\mathrm d \mathbf r}{\mathrm dt}$?) Any help would be appreciated. Please forgive me if this is blindingly obvious. Many thanks.",,"['ordinary-differential-equations', 'physics']"
95,"Differential equation, eigenvalues and eigenfunctions","Differential equation, eigenvalues and eigenfunctions",,"How does one find all the permissible values of $b$ for $-{d\over dx}(-e^{ax}y')-ae^{ax}y=be^{ax}y$ with boundary conditions $y(0)=y(1)=0$? I assume we have a discrete set of $\{b_n\}$ where they can be regarded as eigenvalues? After that how does one find the corresponding $\{y_n\}$? I am sure we substitute the $\{b_n\}$ into the equation, but then I still don't know how this equation is solved. Please help! Thanks. Update: Perhaps it is easier to find the permissible $b$'s if we write the equation in the form $y''+ay'+(a+b)y=0$? Update 2: OK, so the general solution is $y(x)=A\exp[\frac{-a+\sqrt{a^2+4(a+b)}}{2}]+B\exp[\frac{-a-\sqrt{a^2+4(a+b)}}{2}]$ And then the BC's mean that $A+B=0$ and $\exp[\frac{-a+\sqrt{a^2+4(a+b)}}{2}]-\exp[\frac{-a-\sqrt{a^2+4(a+b)}}{2}]=0$, therefore we need $\sqrt{a^2+4(a+b)}=0$ i.e. $b={-1\over 4}(a^2+4a)$? Am I right? Is this the only permissible $b$? Final update: This problem has been resolved. Thanks anyway!","How does one find all the permissible values of $b$ for $-{d\over dx}(-e^{ax}y')-ae^{ax}y=be^{ax}y$ with boundary conditions $y(0)=y(1)=0$? I assume we have a discrete set of $\{b_n\}$ where they can be regarded as eigenvalues? After that how does one find the corresponding $\{y_n\}$? I am sure we substitute the $\{b_n\}$ into the equation, but then I still don't know how this equation is solved. Please help! Thanks. Update: Perhaps it is easier to find the permissible $b$'s if we write the equation in the form $y''+ay'+(a+b)y=0$? Update 2: OK, so the general solution is $y(x)=A\exp[\frac{-a+\sqrt{a^2+4(a+b)}}{2}]+B\exp[\frac{-a-\sqrt{a^2+4(a+b)}}{2}]$ And then the BC's mean that $A+B=0$ and $\exp[\frac{-a+\sqrt{a^2+4(a+b)}}{2}]-\exp[\frac{-a-\sqrt{a^2+4(a+b)}}{2}]=0$, therefore we need $\sqrt{a^2+4(a+b)}=0$ i.e. $b={-1\over 4}(a^2+4a)$? Am I right? Is this the only permissible $b$? Final update: This problem has been resolved. Thanks anyway!",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
96,Special forms of ODEs,Special forms of ODEs,,"In my previous question , @Gerben suggested that it is more likely that WA recognizes an ODE in""Sturm-Liouville"" form. Is there a reason for this particular form being preferred to the usual $a(x)y''+b(x)y'+c(x)y=0$ form? Thanks. (In particular, are the equations easier to solve in this form?)","In my previous question , @Gerben suggested that it is more likely that WA recognizes an ODE in""Sturm-Liouville"" form. Is there a reason for this particular form being preferred to the usual $a(x)y''+b(x)y'+c(x)y=0$ form? Thanks. (In particular, are the equations easier to solve in this form?)",,['ordinary-differential-equations']
97,Differential equation with some constraints,Differential equation with some constraints,,"I'd like $\alpha,\beta,\gamma$ as functions of $t$, satisfying the following conditions: $$ \begin{align} \alpha+\beta+\gamma & = 0 \\ \sin^2\alpha + \sin^2\beta + \sin^2\gamma & = c^2 \\ \left| \frac{d}{dt}(\sin\alpha,\sin\beta,\sin\gamma)\right| & = 1 \end{align} $$ I'm thinking of $c^2$ as small .  At the very least that means $<2$, and intuitively it means $\ll 2$.  Some geometry shows that there is a qualitative change in the nature of the solutions when $c^2$ goes from $<2$ to $>2$.","I'd like $\alpha,\beta,\gamma$ as functions of $t$, satisfying the following conditions: $$ \begin{align} \alpha+\beta+\gamma & = 0 \\ \sin^2\alpha + \sin^2\beta + \sin^2\gamma & = c^2 \\ \left| \frac{d}{dt}(\sin\alpha,\sin\beta,\sin\gamma)\right| & = 1 \end{align} $$ I'm thinking of $c^2$ as small .  At the very least that means $<2$, and intuitively it means $\ll 2$.  Some geometry shows that there is a qualitative change in the nature of the solutions when $c^2$ goes from $<2$ to $>2$.",,['ordinary-differential-equations']
98,"How does an integrating factor geometrically ""uncurl"" a vector field?","How does an integrating factor geometrically ""uncurl"" a vector field?",,"We know that certain 1-D forms $m(x,y,z)\,dx + n(x,y,z)\,dy + p(x,y,z)\,dz$ admit integrating factors as we teach in basic differential equations. How does the integrating factor geometrically turn this ""unlayered"" situation into the ""layered situation"" of level surfaces with the new vector field being the gradient field of of a suitably differentiable function $f(x,y,z)$ of $3$ variables.  I know that the general question of integrability is very complicated. Just a good intuitive example would suffice. I know how an integrating factor which is essentially a continuous ""reweighting"" of the vector field can make the required the field ""exact"" algebraically . What I want to get a feel for is ""What is happening geometrically ?"". How is a vector field which is wandering around and many times trying to form closed loops and does not ""layer"" transformed geometrically into a vector field which is inwardly or outwardly directed by the ""layered"" set of level surfaces when before there were no natural surfaces to assign to the vector field?","We know that certain 1-D forms $m(x,y,z)\,dx + n(x,y,z)\,dy + p(x,y,z)\,dz$ admit integrating factors as we teach in basic differential equations. How does the integrating factor geometrically turn this ""unlayered"" situation into the ""layered situation"" of level surfaces with the new vector field being the gradient field of of a suitably differentiable function $f(x,y,z)$ of $3$ variables.  I know that the general question of integrability is very complicated. Just a good intuitive example would suffice. I know how an integrating factor which is essentially a continuous ""reweighting"" of the vector field can make the required the field ""exact"" algebraically . What I want to get a feel for is ""What is happening geometrically ?"". How is a vector field which is wandering around and many times trying to form closed loops and does not ""layer"" transformed geometrically into a vector field which is inwardly or outwardly directed by the ""layered"" set of level surfaces when before there were no natural surfaces to assign to the vector field?",,['ordinary-differential-equations']
99,methods of solving differential/functional iteration equations,methods of solving differential/functional iteration equations,,"Let $f^{[n]}(x)$ be the $n$-th functional iterate of $f(x)$, so that $f^{[1]}(x)=f(x)$ and $f^{[n+1]}(x)=f(f^{[n]}(x)$. And let $f^{(n)}(x) = \frac{d^{n}}{dx^{n}} \left(f(x)\right)$ Has there been any research into solving equations like: $$f^{[n]}(x)=f^{(n)}(x)$$ The case $n=1$ reduces to the exponential. What about $n>1$? Note: I do not mean that there is one $f$ which solves the above equation for all values of $n$, but I am stuck on how to notate each solution, given the preponderance of superscripted $n$s.","Let $f^{[n]}(x)$ be the $n$-th functional iterate of $f(x)$, so that $f^{[1]}(x)=f(x)$ and $f^{[n+1]}(x)=f(f^{[n]}(x)$. And let $f^{(n)}(x) = \frac{d^{n}}{dx^{n}} \left(f(x)\right)$ Has there been any research into solving equations like: $$f^{[n]}(x)=f^{(n)}(x)$$ The case $n=1$ reduces to the exponential. What about $n>1$? Note: I do not mean that there is one $f$ which solves the above equation for all values of $n$, but I am stuck on how to notate each solution, given the preponderance of superscripted $n$s.",,"['analysis', 'reference-request', 'ordinary-differential-equations', 'functional-analysis']"
