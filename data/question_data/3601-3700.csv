,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"A Vitali set is non-measurable, direct proof, without using countable additivity","A Vitali set is non-measurable, direct proof, without using countable additivity",,"I am teaching a measure theory class, where we are in the process of constructing Lebesgue measure on $\mathbb{R}$ via the usual Caratheodory outer measure construction. As motivation, we began by constructing a Vitali set $V \subset [0,1)$ which has the property that $\bigcup_{q \in \mathbb{Q} \cap [0,1)} V \oplus q = [0,1)$, where $\oplus$ is the usual ""addition mod 1"", and the sets $V \oplus q$ are pairwise disjoint.  This led us to conclude that Lebesgue measure cannot measure every set; i.e. there is no measure defined on all sets which is countably additive, translation invariant, and has $m([0,1)) = 1$. We have also constructed Lebesgue outer measure $m^*$ in the usual way, by defining $m^*(A) = \inf\left\{\sum_{i} b_i - a_i : A \subseteq \bigcup_i [a_i, b_i]\right\}$, and proved that it is countably subadditive, translation invariant, and that $m^*([a,b]) = b-a$. Now the typical next step is to define a set $E$ to be measurable if for every $A \subset \mathbb{R}$ we have $m^*(A) = m^*(A \cap E) + m^*(A \cap E^c)$.  We will of course show that when $m^*$ is restricted to the measurable sets, it is countably additive.  It would then follow, indirectly, that the Vitali set $V$ cannot have been measurable. But since this is a lot of work, I would like to start by proving directly that $V$ is not measurable; i.e. find a set $A$ such that $m^*(A) < m^*(A \cap V) + m^*(A \cap V^c)$.  This should help to motivate the definition of ""measurable"". I presume $A = [0,1)$ should work, so that we should try to prove $1 < m^*(V) + m^*([0,1) \setminus V)$.  Of course it is clear from countable subadditivity that we must have $m^*(V) > 0$ and $m^*([0,1) \setminus V) > 0$.  But I don't immediately see how to prove the sum exceeds 1. So in short: Is there a simple proof that $m^*(V) + m^*([0,1) \setminus V) > 1$, using only the basic properties of outer measure $m^*$, and in particular not using the countable additivity of Lebesgue measure? I also saw Outer Measure of the complement of a Vitali Set in [0,1] equal to 1 .  It's hard to follow without the textbook in question, but it seems to use the fact that open sets are measurable, and that $m^*(A) = \inf\{m^*(U) : A \subset U, U \text{ open}\}$.  Again, I would like to avoid that if possible.","I am teaching a measure theory class, where we are in the process of constructing Lebesgue measure on $\mathbb{R}$ via the usual Caratheodory outer measure construction. As motivation, we began by constructing a Vitali set $V \subset [0,1)$ which has the property that $\bigcup_{q \in \mathbb{Q} \cap [0,1)} V \oplus q = [0,1)$, where $\oplus$ is the usual ""addition mod 1"", and the sets $V \oplus q$ are pairwise disjoint.  This led us to conclude that Lebesgue measure cannot measure every set; i.e. there is no measure defined on all sets which is countably additive, translation invariant, and has $m([0,1)) = 1$. We have also constructed Lebesgue outer measure $m^*$ in the usual way, by defining $m^*(A) = \inf\left\{\sum_{i} b_i - a_i : A \subseteq \bigcup_i [a_i, b_i]\right\}$, and proved that it is countably subadditive, translation invariant, and that $m^*([a,b]) = b-a$. Now the typical next step is to define a set $E$ to be measurable if for every $A \subset \mathbb{R}$ we have $m^*(A) = m^*(A \cap E) + m^*(A \cap E^c)$.  We will of course show that when $m^*$ is restricted to the measurable sets, it is countably additive.  It would then follow, indirectly, that the Vitali set $V$ cannot have been measurable. But since this is a lot of work, I would like to start by proving directly that $V$ is not measurable; i.e. find a set $A$ such that $m^*(A) < m^*(A \cap V) + m^*(A \cap V^c)$.  This should help to motivate the definition of ""measurable"". I presume $A = [0,1)$ should work, so that we should try to prove $1 < m^*(V) + m^*([0,1) \setminus V)$.  Of course it is clear from countable subadditivity that we must have $m^*(V) > 0$ and $m^*([0,1) \setminus V) > 0$.  But I don't immediately see how to prove the sum exceeds 1. So in short: Is there a simple proof that $m^*(V) + m^*([0,1) \setminus V) > 1$, using only the basic properties of outer measure $m^*$, and in particular not using the countable additivity of Lebesgue measure? I also saw Outer Measure of the complement of a Vitali Set in [0,1] equal to 1 .  It's hard to follow without the textbook in question, but it seems to use the fact that open sets are measurable, and that $m^*(A) = \inf\{m^*(U) : A \subset U, U \text{ open}\}$.  Again, I would like to avoid that if possible.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
1,Computing $\lim_{\epsilon \rightarrow 0} \int_0^\infty \frac{\sin x}{x} \arctan{\frac{x}{\epsilon}}dx$,Computing,\lim_{\epsilon \rightarrow 0} \int_0^\infty \frac{\sin x}{x} \arctan{\frac{x}{\epsilon}}dx,"I'm not exactly sure how to get started computing the limit of the improper Riemann integral $$\lim_{\epsilon \rightarrow 0} \int_0^\infty \frac{\sin x}{x} \arctan\left(\frac{x}{\epsilon}\right)dx.$$ Using the result that $\int_0^\infty \frac{\sin x}{x} dx = \pi/2$, is there a way to interchange the limit and the integral to get $\pi^2/4$?","I'm not exactly sure how to get started computing the limit of the improper Riemann integral $$\lim_{\epsilon \rightarrow 0} \int_0^\infty \frac{\sin x}{x} \arctan\left(\frac{x}{\epsilon}\right)dx.$$ Using the result that $\int_0^\infty \frac{\sin x}{x} dx = \pi/2$, is there a way to interchange the limit and the integral to get $\pi^2/4$?",,"['real-analysis', 'improper-integrals']"
2,Difficult problem in Riemann integrals,Difficult problem in Riemann integrals,,"Could anyone help me with the following problem? Because i have stuck. Let $f:[a,b]\rightarrow [0,\infty)$ be continuous and not the zero function. Prove that $$\lim_{n\to \infty} \frac{\int\limits_a^b f^{n+1}(x)\, \mathrm{d}x}{\int\limits_a^b f^n(x)\, \mathrm{d}x}=\sup_{x\in[a,b]}f(x)$$ Some of my thoughts: Let $\displaystyle M=\sup_{x\in[a,b]}f(x)$ and $\displaystyle m=\inf_{x\in[a,b]}f(x)$ . Moreover define the sequences $\{I_n\}_{n=0}^{\infty}$ and $\{a_n\}_{n=0}^{\infty}$ , So, $\displaystyle I_n=\int\limits_a^b f^{n}(x)\, \mathrm{d}x$ and $\displaystyle a_n=\frac{I_{n+1}}{I_n}$ . It easy to prove that $$m(b-a)\leq I_n \leq M(b-a)$$ and $$a_n \leq M\left(\frac{M}{m}\right)^n.$$ First I tried to use the above inequalities to prove it by definition or somehow with the squeeze theorem, but unfortunately $\displaystyle M\left(\frac{M}{m}\right)^n \xrightarrow[ n \to \infty]{} \infty$ Then I thought about the integral mean value theorem , but again nothing new came up. So do you have any suggestions, thoughts, hints or\and solutions?","Could anyone help me with the following problem? Because i have stuck. Let be continuous and not the zero function. Prove that Some of my thoughts: Let and . Moreover define the sequences and , So, and . It easy to prove that and First I tried to use the above inequalities to prove it by definition or somehow with the squeeze theorem, but unfortunately Then I thought about the integral mean value theorem , but again nothing new came up. So do you have any suggestions, thoughts, hints or\and solutions?","f:[a,b]\rightarrow [0,\infty) \lim_{n\to \infty} \frac{\int\limits_a^b f^{n+1}(x)\, \mathrm{d}x}{\int\limits_a^b f^n(x)\, \mathrm{d}x}=\sup_{x\in[a,b]}f(x) \displaystyle M=\sup_{x\in[a,b]}f(x) \displaystyle m=\inf_{x\in[a,b]}f(x) \{I_n\}_{n=0}^{\infty} \{a_n\}_{n=0}^{\infty} \displaystyle I_n=\int\limits_a^b f^{n}(x)\, \mathrm{d}x \displaystyle a_n=\frac{I_{n+1}}{I_n} m(b-a)\leq I_n \leq M(b-a) a_n \leq M\left(\frac{M}{m}\right)^n. \displaystyle M\left(\frac{M}{m}\right)^n \xrightarrow[ n \to \infty]{} \infty","['calculus', 'real-analysis', 'integration']"
3,Polar Coordinates in $\mathbb R^n$,Polar Coordinates in,\mathbb R^n,"After proving Fubini-Tonelli theorem a formula on polar coordinates in $\mathbb R^n$ is given in my class as follows. Let $f$ be a real-valued integrable function on $\mathbb R^n$ and $S^{n-1}$ be the surface of $n$-dimensional unit ball with respect to Euclidean norm. Then define a new function $\tilde f(r, \omega) :\mathbb R^+ \times S^{n-1} \to \mathbb R$ by $$\tilde f(r, \omega) = f(r\omega).$$ In addition, let $E$ be a measurable subset of $\mathbb R^n$ and $r>0$. Define $E_r \subset S^{n-1}$ as $$E_r := \{\omega\in S^{n-1}:r\omega \in E\}.$$ Then finally the polar coordinate formula in $\mathbb R^n$ is as follows. $$ \int_E f(x) dx = \int_0^\infty \left( \int_{E_r} \tilde f(r, \omega)d\omega \right) r^{n-1} dr. $$ I have the following questions about this formula. How is this formula related to the two-dimensional case? Recall in two dimensional case, we have $x=r\cos\theta$ and $y=r\sin\theta$ and $$\iint f(x, y) dxdy = \iint f(r\cos\theta, r\sin\theta) rdrd\theta.$$ Hence, the general formula is similar but still different, especially the ingegrad $\tilde f(r, \omega)$. How do I intepret this, please? How can $\mathbb R^n$ be written as $\mathbb R^+ \times S^{n-1} $? Comparing with the two dimensional situation, $r$ seems to be the same radius. However, the angle $\theta$ in $2$-d case is now replaced by $\omega \in S^{n-1}$. How can this be treated as angle like argument, please? Why and how do we define $\tilde f(r, \omega) = f(r\omega)$? How to interpret the definition of $E_r := \{\omega\in S^{n-1}:r\omega \in E\}$? Could anyone explain these to me, please? More detailed answers are appreciated since I know very little here. Thank you!","After proving Fubini-Tonelli theorem a formula on polar coordinates in $\mathbb R^n$ is given in my class as follows. Let $f$ be a real-valued integrable function on $\mathbb R^n$ and $S^{n-1}$ be the surface of $n$-dimensional unit ball with respect to Euclidean norm. Then define a new function $\tilde f(r, \omega) :\mathbb R^+ \times S^{n-1} \to \mathbb R$ by $$\tilde f(r, \omega) = f(r\omega).$$ In addition, let $E$ be a measurable subset of $\mathbb R^n$ and $r>0$. Define $E_r \subset S^{n-1}$ as $$E_r := \{\omega\in S^{n-1}:r\omega \in E\}.$$ Then finally the polar coordinate formula in $\mathbb R^n$ is as follows. $$ \int_E f(x) dx = \int_0^\infty \left( \int_{E_r} \tilde f(r, \omega)d\omega \right) r^{n-1} dr. $$ I have the following questions about this formula. How is this formula related to the two-dimensional case? Recall in two dimensional case, we have $x=r\cos\theta$ and $y=r\sin\theta$ and $$\iint f(x, y) dxdy = \iint f(r\cos\theta, r\sin\theta) rdrd\theta.$$ Hence, the general formula is similar but still different, especially the ingegrad $\tilde f(r, \omega)$. How do I intepret this, please? How can $\mathbb R^n$ be written as $\mathbb R^+ \times S^{n-1} $? Comparing with the two dimensional situation, $r$ seems to be the same radius. However, the angle $\theta$ in $2$-d case is now replaced by $\omega \in S^{n-1}$. How can this be treated as angle like argument, please? Why and how do we define $\tilde f(r, \omega) = f(r\omega)$? How to interpret the definition of $E_r := \{\omega\in S^{n-1}:r\omega \in E\}$? Could anyone explain these to me, please? More detailed answers are appreciated since I know very little here. Thank you!",,"['real-analysis', 'analysis', 'functional-analysis', 'measure-theory', 'multivariable-calculus']"
4,Continuity of Real Number line,Continuity of Real Number line,,"What is the property of real numbers that allows them to be seen as a continuous line, and how natural numbers, rational numbers lack this property?","What is the property of real numbers that allows them to be seen as a continuous line, and how natural numbers, rational numbers lack this property?",,"['real-analysis', 'real-numbers']"
5,Prove that a uniformly convergent convergent sequence of $N^\text{th}$ degree polynomials must converge to some $N^\text{th}$ degree polynomial,Prove that a uniformly convergent convergent sequence of  degree polynomials must converge to some  degree polynomial,N^\text{th} N^\text{th},"So here's the question I'm trying to answer: Suppose $p_n(x) = \sum_{k=1}^N a_k^{(n)} x^k$ is a sequence of polynomials such that $p_n \to f$ uniformly over $[0,1]$ for some function $f:[0,1] \to \mathbb{R}$.  Prove that $f$ must itself be an $N^\text{th}$ degree polynomial. I've already shown that if each $a_k^{(n)} \to a_k$, then $p_n(x) \to p(x) = \sum_{k=1}^N a_k x^k$  uniformly (earlier part of the problem).  I'm thinking that there's some way to show that if $p_n \to f$, then $a_k^{(n)}$ converges for each $k$.  This certainly works for $k = 0$, since we can guarantee that the sequence $a_k^{(n)} = p_n(0)$ is Cauchy.  I've gotten stuck in trying to extend this to other coefficients; I'm thinking there's some trick involving subtracting the $a_0^{(n)}$ off and dividing by $x$, maybe some fancy induction along those lines. Other potentially helpful thoughts: we can guarantee that $f$ is continuous since it is the uniform limit of continuous functions. Remember also that we have a compact domain, so that all of these functions are bounded and achieve their max/min. Any comments, hints, or answers are very much appreciated.","So here's the question I'm trying to answer: Suppose $p_n(x) = \sum_{k=1}^N a_k^{(n)} x^k$ is a sequence of polynomials such that $p_n \to f$ uniformly over $[0,1]$ for some function $f:[0,1] \to \mathbb{R}$.  Prove that $f$ must itself be an $N^\text{th}$ degree polynomial. I've already shown that if each $a_k^{(n)} \to a_k$, then $p_n(x) \to p(x) = \sum_{k=1}^N a_k x^k$  uniformly (earlier part of the problem).  I'm thinking that there's some way to show that if $p_n \to f$, then $a_k^{(n)}$ converges for each $k$.  This certainly works for $k = 0$, since we can guarantee that the sequence $a_k^{(n)} = p_n(0)$ is Cauchy.  I've gotten stuck in trying to extend this to other coefficients; I'm thinking there's some trick involving subtracting the $a_0^{(n)}$ off and dividing by $x$, maybe some fancy induction along those lines. Other potentially helpful thoughts: we can guarantee that $f$ is continuous since it is the uniform limit of continuous functions. Remember also that we have a compact domain, so that all of these functions are bounded and achieve their max/min. Any comments, hints, or answers are very much appreciated.",,['real-analysis']
6,distance between sets in a metric space,distance between sets in a metric space,,"I was given this innocent looking homework question. Given two nonempty sets $A,B \subseteq X$ where $(X,d)$ is a metric space. Show that $\mathrm{dist}(A,B) = \inf \{d(x,y) \mid x \in A, y \in B \}$ is well-defined. Suppose $A \cap B = \emptyset$ . Suppose $A$ is closed and $B$ is compact. Show that $\mathrm{dist}(A, B) > 0$ . Aren't both (1) and (2) properties of the fact that $S = \{ d(x,y) \mid x \in A, y \in B \}$ is a subset of $P = \{ x \mid x \ge 0 \}$ , which is bounded? for (1): $S \subseteq P$ and $P$ bounded implies $S$ is bounded. Hence $\mathrm{inf} S$ exists. Since $\mathrm{inf} S$ is unique, we conclude that $\mathrm{dist}(A, B)$ is well defined. for (2): Since $\mathrm{inf} P \ge 0$ and $S \subseteq P$ , $\mathrm{dist}(A, B) \ge 0$ . Since $A \cap B = \emptyset$ , $x \ne y$ $\forall x \in A, y \in B$ . Then $d(x,y) \ne 0$ . Hence $\mathrm{dist}(A,B) > 0$ . Am I missing something very very obvious? Where does compactness come into play?","I was given this innocent looking homework question. Given two nonempty sets where is a metric space. Show that is well-defined. Suppose . Suppose is closed and is compact. Show that . Aren't both (1) and (2) properties of the fact that is a subset of , which is bounded? for (1): and bounded implies is bounded. Hence exists. Since is unique, we conclude that is well defined. for (2): Since and , . Since , . Then . Hence . Am I missing something very very obvious? Where does compactness come into play?","A,B \subseteq X (X,d) \mathrm{dist}(A,B) = \inf \{d(x,y) \mid x \in A, y \in B \} A \cap B = \emptyset A B \mathrm{dist}(A, B) > 0 S = \{ d(x,y) \mid x \in A, y \in B \} P = \{ x \mid x \ge 0 \} S \subseteq P P S \mathrm{inf} S \mathrm{inf} S \mathrm{dist}(A, B) \mathrm{inf} P \ge 0 S \subseteq P \mathrm{dist}(A, B) \ge 0 A \cap B = \emptyset x \ne y \forall x \in A, y \in B d(x,y) \ne 0 \mathrm{dist}(A,B) > 0","['real-analysis', 'metric-spaces']"
7,How does one get better at real analysis proofs?,How does one get better at real analysis proofs?,,"How does one proceed through a math proof in real analysis? My instructor always says make a diagram, but I am not a visual learner. It seems that whenever I write out the definition of an assumption, then I cannot make the next logical step. Also, when I go to try to verify that my proof is correct, I ask myself questions like, ""why must this be true?"" but the proof does not end up not being air tight. For those that have had real analysis, what did you do to master proofs and do the exercises?","How does one proceed through a math proof in real analysis? My instructor always says make a diagram, but I am not a visual learner. It seems that whenever I write out the definition of an assumption, then I cannot make the next logical step. Also, when I go to try to verify that my proof is correct, I ask myself questions like, ""why must this be true?"" but the proof does not end up not being air tight. For those that have had real analysis, what did you do to master proofs and do the exercises?",,"['real-analysis', 'soft-question']"
8,Strictly convex function: how often can its second derivative be zero?,Strictly convex function: how often can its second derivative be zero?,,"It's a basic fact that a twice-differentiable function from $\mathbb{R}$ to $\mathbb{R}$ is strictly convex if its derivative is positive everywhere. The converse is not true: consider, e.g., $f(x) = x^4$, which is strictly convex, with $f ''(0)=0$. Is there a partial converse, however? Is it true, e.g., that a strictly convex twice-differentiable function from $\mathbb{R}$ to $\mathbb{R}$ can have zero second derivative at at most one point? Thanks for your help!","It's a basic fact that a twice-differentiable function from $\mathbb{R}$ to $\mathbb{R}$ is strictly convex if its derivative is positive everywhere. The converse is not true: consider, e.g., $f(x) = x^4$, which is strictly convex, with $f ''(0)=0$. Is there a partial converse, however? Is it true, e.g., that a strictly convex twice-differentiable function from $\mathbb{R}$ to $\mathbb{R}$ can have zero second derivative at at most one point? Thanks for your help!",,"['calculus', 'real-analysis', 'convex-analysis']"
9,Speed of convergence of a Riemann sum,Speed of convergence of a Riemann sum,,"Let $f(x)=x^d$ $(d\in(-1,0))$. We know that $$\sum_{i=1}^n \frac{1}{n}\left(\frac{i}{n}\right)^d\xrightarrow{n\rightarrow\infty}\int_0^1x^d dx=\frac{1}{d+1}.$$ My question is the following: Can we say something about the speed of convergence? Something like $$\left|\int_0^1x^d dx-\sum_{i=1}^n \frac{1}{n}\left(\frac{i}{n}\right)^d\right|\in O(n^d)?$$ I know that the last expression might be wrong. I just wanted to give you an idea what I am looking for. Thanks!","Let $f(x)=x^d$ $(d\in(-1,0))$. We know that $$\sum_{i=1}^n \frac{1}{n}\left(\frac{i}{n}\right)^d\xrightarrow{n\rightarrow\infty}\int_0^1x^d dx=\frac{1}{d+1}.$$ My question is the following: Can we say something about the speed of convergence? Something like $$\left|\int_0^1x^d dx-\sum_{i=1}^n \frac{1}{n}\left(\frac{i}{n}\right)^d\right|\in O(n^d)?$$ I know that the last expression might be wrong. I just wanted to give you an idea what I am looking for. Thanks!",,"['real-analysis', 'analysis', 'integration', 'riemann-sum']"
10,Is there a problem when defining exponential with negative base?,Is there a problem when defining exponential with negative base?,,"Well, this question may seem silly at first, but I'll make my point clear. Suppose $n \in \Bbb N$ and suppose $a \in \Bbb R$ is any number. Then the definition of $a^n$ is clear for any $a$ we choose. Indeed we define: $$a^n = \prod_{k=1}^na$$ And even if $a$ is negative this has a meaning. Then we extend the definition for $n \in \Bbb Z$ and for $n \in \Bbb Q$. When we are to define to $n \in\Bbb R$ we define it as: $$a^x=e^{x\ln a}$$ That's fine, but $\ln $ is a function defined on $\Bbb R^+$ so that if we try to compute $(-5)^\pi$ we'll get into trouble because this would be: $$(-5)^{\pi}=e^{\pi \ln(-5)}$$ But $\ln (-5)$ is undefined. In that case, the function that $f : A \subset \Bbb R^2 \to \Bbb R$ given by $f(a,x) = a^x$ would be undefined if $a < 0$, so that $A = \Bbb R^+ \times \Bbb R$. What I thought was: we can extend this function when $a$ is negative and $x$ is rational. In that case we would set it to the old definition of exponentiation, since we would have a real raised to a rational. So, for negative base and irrational exponent the exponential remains undefined? Thanks very much in advance!","Well, this question may seem silly at first, but I'll make my point clear. Suppose $n \in \Bbb N$ and suppose $a \in \Bbb R$ is any number. Then the definition of $a^n$ is clear for any $a$ we choose. Indeed we define: $$a^n = \prod_{k=1}^na$$ And even if $a$ is negative this has a meaning. Then we extend the definition for $n \in \Bbb Z$ and for $n \in \Bbb Q$. When we are to define to $n \in\Bbb R$ we define it as: $$a^x=e^{x\ln a}$$ That's fine, but $\ln $ is a function defined on $\Bbb R^+$ so that if we try to compute $(-5)^\pi$ we'll get into trouble because this would be: $$(-5)^{\pi}=e^{\pi \ln(-5)}$$ But $\ln (-5)$ is undefined. In that case, the function that $f : A \subset \Bbb R^2 \to \Bbb R$ given by $f(a,x) = a^x$ would be undefined if $a < 0$, so that $A = \Bbb R^+ \times \Bbb R$. What I thought was: we can extend this function when $a$ is negative and $x$ is rational. In that case we would set it to the old definition of exponentiation, since we would have a real raised to a rational. So, for negative base and irrational exponent the exponential remains undefined? Thanks very much in advance!",,['real-analysis']
11,Differentiability implies Lipschitz continuity,Differentiability implies Lipschitz continuity,,"Let $f:[0,1]\to\mathbb{R}$ be a continuous function and suppose $f$ is differentiable at $x_0\in [0,1]$. Is it true that there exists $L>0$ such that $\lvert f(x)-f(x_0)\lvert\leq L\lvert x-x_0\lvert$? I know that local continuously differentiable implies local Lipschitz continuity. Is this still true in the case given above?","Let $f:[0,1]\to\mathbb{R}$ be a continuous function and suppose $f$ is differentiable at $x_0\in [0,1]$. Is it true that there exists $L>0$ such that $\lvert f(x)-f(x_0)\lvert\leq L\lvert x-x_0\lvert$? I know that local continuously differentiable implies local Lipschitz continuity. Is this still true in the case given above?",,"['real-analysis', 'functional-analysis']"
12,How do I prove $f=0$ almost everywhere?,How do I prove  almost everywhere?,f=0,"During one of the problems in Rudin I was asked to show $f=0$ a.e. Here $f$ satisfies this condition: $$f(x)=\frac{1}{x}\int^{x}_{0}f(t)dt$$ almost everywhere and is in $L^{p}(0,\infty)$. So constant functions would not work. I tried to prove by contradiction, and a few imaginary counter-examples' failure convinced me this is true. But what is a good way of proving this statement? Since we know $f\in L^{p}$ I am thinking about using Holder's inequality, but in our case it is difficult to apply (since the other side is larger ). We can assume $f\in C_{c}(0,\infty)$ since this is dense in $L^{p}$, but I still do not know how to prove this statement.","During one of the problems in Rudin I was asked to show $f=0$ a.e. Here $f$ satisfies this condition: $$f(x)=\frac{1}{x}\int^{x}_{0}f(t)dt$$ almost everywhere and is in $L^{p}(0,\infty)$. So constant functions would not work. I tried to prove by contradiction, and a few imaginary counter-examples' failure convinced me this is true. But what is a good way of proving this statement? Since we know $f\in L^{p}$ I am thinking about using Holder's inequality, but in our case it is difficult to apply (since the other side is larger ). We can assume $f\in C_{c}(0,\infty)$ since this is dense in $L^{p}$, but I still do not know how to prove this statement.",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
13,Absolute continuity inside the interval extends to the endpoint of the interval under some constraints,Absolute continuity inside the interval extends to the endpoint of the interval under some constraints,,"The below problem appeared on the UW-Madison Analysis qualifying exam in January 2020. The proof that $f$ is absolutely continuous (AC) on the whole interval is still not posted, although the requested counterexample has been given. I don't seem to find version of this problem in the site, but I am sure this is pretty standard type of question. The problem goes likes this: Let $f$ be of bounded variation on $[0,1]$ and AC on $[\varepsilon,1]$ for all $\varepsilon >0$ . $f$ is also continuous at $0$ . Prove $f$ is absolutely continuous on  whole interval $[0,1]$ . Moreover I am looking for a counterexample in the case when the bounded variation of $f $ is dropped. Here is what I think, Using continuity, I can find $\delta>0$ for given $\epsilon >0$ which bounds the sum in the definition of AC upto $\delta$ . Then in the interval $[\delta,1]$ , I can use given hypothesis. But I am not totally comfortable writing this rigorously. For the counter example I can use $f(0)=0$ and $f(x)= x\sin (1/x)$ for $x$ not equal to $0$ . I would love to see the rigorous proof and rigorous proof of counterexample. Thank you in advance.","The below problem appeared on the UW-Madison Analysis qualifying exam in January 2020. The proof that is absolutely continuous (AC) on the whole interval is still not posted, although the requested counterexample has been given. I don't seem to find version of this problem in the site, but I am sure this is pretty standard type of question. The problem goes likes this: Let be of bounded variation on and AC on for all . is also continuous at . Prove is absolutely continuous on  whole interval . Moreover I am looking for a counterexample in the case when the bounded variation of is dropped. Here is what I think, Using continuity, I can find for given which bounds the sum in the definition of AC upto . Then in the interval , I can use given hypothesis. But I am not totally comfortable writing this rigorously. For the counter example I can use and for not equal to . I would love to see the rigorous proof and rigorous proof of counterexample. Thank you in advance.","f f [0,1] [\varepsilon,1] \varepsilon >0 f 0 f [0,1] f  \delta>0 \epsilon >0 \delta [\delta,1] f(0)=0 f(x)= x\sin (1/x) x 0","['real-analysis', 'bounded-variation', 'absolute-continuity']"
14,Does $\mu^{*}(E)=1$ imply $\mu^{*}(E^{c})=0$ when $\mu$ is an outer measure and the measure of the space is $1$,Does  imply  when  is an outer measure and the measure of the space is,\mu^{*}(E)=1 \mu^{*}(E^{c})=0 \mu 1,"Let $(X,S,\mu)$ be a measure space s.t. $\mu(X)=1$. Let $\mu^{*}$ be defined on $X$ by: $$\forall E\subseteq X:\,\mu^{*}(E):=\text{inf}\left\{\sum_{i=1}^{\infty}\mu(A_{i})\,|\, A_{i}\in S,E\subseteq\cup A_{i}\right\}$$ I have a set $E$ s.t. $\mu^{*}(E)=1$, does this mean $\mu^{*}(E^{c})=0$? I have tried to work with the definition, given $\epsilon>0$ there is $N\in\mathbb{N}$ and $\{A_{i}\}_{i=1}^{N}\subseteq S$ s.t. $\sum_{i=1}^{N}\mu(A_{i})\geq1-\epsilon$ and $E\subseteq\cup_{i=1}^{N}A_{i}$ I want to use the $A_{i}$'s to get some set $B$ s.t $E^{c}\subseteq B$ and $\mu(B)<\epsilon$, but I didn't manage to find such a set. Can someone please help me understand if this claim is true, and if so how to prove it?","Let $(X,S,\mu)$ be a measure space s.t. $\mu(X)=1$. Let $\mu^{*}$ be defined on $X$ by: $$\forall E\subseteq X:\,\mu^{*}(E):=\text{inf}\left\{\sum_{i=1}^{\infty}\mu(A_{i})\,|\, A_{i}\in S,E\subseteq\cup A_{i}\right\}$$ I have a set $E$ s.t. $\mu^{*}(E)=1$, does this mean $\mu^{*}(E^{c})=0$? I have tried to work with the definition, given $\epsilon>0$ there is $N\in\mathbb{N}$ and $\{A_{i}\}_{i=1}^{N}\subseteq S$ s.t. $\sum_{i=1}^{N}\mu(A_{i})\geq1-\epsilon$ and $E\subseteq\cup_{i=1}^{N}A_{i}$ I want to use the $A_{i}$'s to get some set $B$ s.t $E^{c}\subseteq B$ and $\mu(B)<\epsilon$, but I didn't manage to find such a set. Can someone please help me understand if this claim is true, and if so how to prove it?",,['real-analysis']
15,Summation of divergent series of Euler: $0!-1!+2!-3!+\cdots$,Summation of divergent series of Euler:,0!-1!+2!-3!+\cdots,"Consider the series $$\sum\limits_{k=0}^\infty (-1)^kk!x^k\in\mathbb{R}[[x]]$$ Let $s_n(x)$ be partial sum, and let $\omega_{k,n}=(k!^2(n-k)!)^{-1}$. Prove that $$\lim\limits_{n\to\infty}\dfrac{s_0\omega_{0,n}+\cdots + s_n\omega_{n,n} }{\omega_{0,n}+\dots+\omega_{n,n}}=\int\limits_0^\infty\dfrac{e^{-t}}{xt+1}dt$$","Consider the series $$\sum\limits_{k=0}^\infty (-1)^kk!x^k\in\mathbb{R}[[x]]$$ Let $s_n(x)$ be partial sum, and let $\omega_{k,n}=(k!^2(n-k)!)^{-1}$. Prove that $$\lim\limits_{n\to\infty}\dfrac{s_0\omega_{0,n}+\cdots + s_n\omega_{n,n} }{\omega_{0,n}+\dots+\omega_{n,n}}=\int\limits_0^\infty\dfrac{e^{-t}}{xt+1}dt$$",,"['real-analysis', 'sequences-and-series']"
16,Rudin Theorem $1.11$,Rudin Theorem,1.11,"After spending a few hours trying to understand Theorem $1.11$ in Rudin's Principles of Mathematical Analysis, I still don't follow the proof. $1.11$ Theorem Suppose $S$ is an ordered set with the least-upper-bound property, $B \subset S$, $B$ is not empty, and $B$ is bounded below. Let $L$ be the set of all lower bounds of $B$. Then $\alpha = \sup L$ exists in $S$, and $\alpha = \inf B$. This is what I understand so far: $B$ is bounded below means that $L$ is not empty and $L = \{ y \ | \ y \leq x \ \forall x \in B \}$. Then every $x \in B$ is an upper bound of $L$, which means that $L$ is bounded above. Since $L \subset S$, $L$ not empty, and $L$ is bounded above that implies that $\sup L = \alpha \in S$. And because $\alpha = \sup L$, $\gamma < \alpha$ implies that $\gamma$ is not an upper bound of $L$ and $\gamma \notin B$ since every element of $B$ is an upper bound of $L$. This is where I get confused: Since $B$ is bounded below, there exists an $\omega \in S$ such that $\omega \leq x \ \forall x \in B$. Then Rudin claims, ""It follows that $\alpha \leq x$ for every $x \in B$."" Can someone explain why that is true or at least give me a hint? Thanks in advance.","After spending a few hours trying to understand Theorem $1.11$ in Rudin's Principles of Mathematical Analysis, I still don't follow the proof. $1.11$ Theorem Suppose $S$ is an ordered set with the least-upper-bound property, $B \subset S$, $B$ is not empty, and $B$ is bounded below. Let $L$ be the set of all lower bounds of $B$. Then $\alpha = \sup L$ exists in $S$, and $\alpha = \inf B$. This is what I understand so far: $B$ is bounded below means that $L$ is not empty and $L = \{ y \ | \ y \leq x \ \forall x \in B \}$. Then every $x \in B$ is an upper bound of $L$, which means that $L$ is bounded above. Since $L \subset S$, $L$ not empty, and $L$ is bounded above that implies that $\sup L = \alpha \in S$. And because $\alpha = \sup L$, $\gamma < \alpha$ implies that $\gamma$ is not an upper bound of $L$ and $\gamma \notin B$ since every element of $B$ is an upper bound of $L$. This is where I get confused: Since $B$ is bounded below, there exists an $\omega \in S$ such that $\omega \leq x \ \forall x \in B$. Then Rudin claims, ""It follows that $\alpha \leq x$ for every $x \in B$."" Can someone explain why that is true or at least give me a hint? Thanks in advance.",,"['real-analysis', 'analysis', 'order-theory', 'supremum-and-infimum', 'upper-lower-bounds']"
17,Image of closed ball under degenerate integral operator is a closed set,Image of closed ball under degenerate integral operator is a closed set,,"I will be putting a bounty on this problem as soon as it lets me. For those who want to understand where the problem came from I encourage reading the edits, as I cut out several failed attempts and no longer relevant definitions from the problem statement to avoid clutter. Consider the integral operator $K: C([0,1])\to C([0,1])$ $$Kf(x) = \int_0^1k(x,y)f(y)dy$$ where $k(x,y) = x^2+2xy+y^2$. Show that the image $I:=K(\overline{B}(0,1))$ of the closed unit ball in $C([0,1])$ using the supremum (maximum) norm $\left\Vert \cdot \right\Vert_\infty$ is closed. Any of the following will be rewarded the bounty: Proof that $I$ is sequentially closed by showing if $Kf_n(x) \to F(x) \in C([0,1])$ then there is a formula for $f \in C([0,1])$ with $\left\Vert f \right\Vert_\infty\leq1$ such that $Kf(x) = F(x)$. Proof that $I$ is sequentially closed by showing if $Kf_n(x) \to F(x) \in C([0,1])$ then there there exists $f\in C([0,1])$ with $\left\Vert f \right\Vert_\infty\leq1$ such that $Kf(x) = F(x)$, i.e. a nonconstructive proof (perhaps one could apply the Baire Category Theorem?). Proof that $I$ is closed by showing that $C([0,1])\setminus I$ is open. Proof that $I$ is compact. Proof that $I$ is sequentially compact. Proof that $I$ is closed by some more clever method I haven't thought of. Things I have proven which may or may not be useful to help you help me solve this: $C([0,1])$ with the maximum norm is a Banach space. If $f\in C([0,1])$ then $Kf \in C([0,1])$. If $Kf_n(x) \to F(x) \in C([0,1])$ then the convergence is uniform. If $Kf_n(x) \to F(x) \in C([0,1])$ this does not necessarily imply $f_n$ has a convergent subsequence (counterexample $f_ n(x)=\sin(nx)$). $K(\overline{B}(0,1))$ is bounded by $\overline{B}(0,7/3)$, hence $K$ is a continuous linear operator. If $Kf_n(x) \to F(x) \in C([0,1])$ then $F(x) = a+bx+cx^2$ for some $a\in[-1/3,1/3],b\in[-1,1],c\in[-1,1]$. If $Kf_n(x) \to F(x)=a+bx+cx^2$ then there is a function $f\in C([0,1])$ such that $Kf(x) = F(x)$, but it does not necessarily satisfy $\left\Vert f \right\Vert_\infty \leq 1$. The function is given by $$f(x)=(30a -18b+9c)+(-180a+96b-36c)x+(180a-90b+30c)x^2$$ and an example of when it fails then norm condition is if $$f_n(x)=e^{-x^{5+1/n}}.$$ Thank you all for the help. I have been working on this problem for more than 30 hours, I'm sure that together we can solve it.","I will be putting a bounty on this problem as soon as it lets me. For those who want to understand where the problem came from I encourage reading the edits, as I cut out several failed attempts and no longer relevant definitions from the problem statement to avoid clutter. Consider the integral operator $K: C([0,1])\to C([0,1])$ $$Kf(x) = \int_0^1k(x,y)f(y)dy$$ where $k(x,y) = x^2+2xy+y^2$. Show that the image $I:=K(\overline{B}(0,1))$ of the closed unit ball in $C([0,1])$ using the supremum (maximum) norm $\left\Vert \cdot \right\Vert_\infty$ is closed. Any of the following will be rewarded the bounty: Proof that $I$ is sequentially closed by showing if $Kf_n(x) \to F(x) \in C([0,1])$ then there is a formula for $f \in C([0,1])$ with $\left\Vert f \right\Vert_\infty\leq1$ such that $Kf(x) = F(x)$. Proof that $I$ is sequentially closed by showing if $Kf_n(x) \to F(x) \in C([0,1])$ then there there exists $f\in C([0,1])$ with $\left\Vert f \right\Vert_\infty\leq1$ such that $Kf(x) = F(x)$, i.e. a nonconstructive proof (perhaps one could apply the Baire Category Theorem?). Proof that $I$ is closed by showing that $C([0,1])\setminus I$ is open. Proof that $I$ is compact. Proof that $I$ is sequentially compact. Proof that $I$ is closed by some more clever method I haven't thought of. Things I have proven which may or may not be useful to help you help me solve this: $C([0,1])$ with the maximum norm is a Banach space. If $f\in C([0,1])$ then $Kf \in C([0,1])$. If $Kf_n(x) \to F(x) \in C([0,1])$ then the convergence is uniform. If $Kf_n(x) \to F(x) \in C([0,1])$ this does not necessarily imply $f_n$ has a convergent subsequence (counterexample $f_ n(x)=\sin(nx)$). $K(\overline{B}(0,1))$ is bounded by $\overline{B}(0,7/3)$, hence $K$ is a continuous linear operator. If $Kf_n(x) \to F(x) \in C([0,1])$ then $F(x) = a+bx+cx^2$ for some $a\in[-1/3,1/3],b\in[-1,1],c\in[-1,1]$. If $Kf_n(x) \to F(x)=a+bx+cx^2$ then there is a function $f\in C([0,1])$ such that $Kf(x) = F(x)$, but it does not necessarily satisfy $\left\Vert f \right\Vert_\infty \leq 1$. The function is given by $$f(x)=(30a -18b+9c)+(-180a+96b-36c)x+(180a-90b+30c)x^2$$ and an example of when it fails then norm condition is if $$f_n(x)=e^{-x^{5+1/n}}.$$ Thank you all for the help. I have been working on this problem for more than 30 hours, I'm sure that together we can solve it.",,"['real-analysis', 'functional-analysis', 'integral-transforms']"
18,Proving sets are measurable,Proving sets are measurable,,"The problem statement, all variables and given/known data The question is from Stein and Shakarchi, Real Analysis 2 , Chapter 1, Problem 5: Suppose $E$ is measurable with $m(E) < \infty$, and $E=E_1\cup E_2$, $E_1\cap E_2=\emptyset$. Prove: a) If $m(E) = m^{*}(E_1) + m^{*}(E_2)$, then $E_1$ and $E_2$ are measurable. b) In particular, if $E \subset Q$, where $Q$ is a finite cube, then $E$ is measurable if and only if $m(Q) = m^{*}(E) + m^{*}(Q − E)$. The definition of a 'measurable set' given in the book is that for any $\epsilon > 0$ there exists an open set $O$ with $E \subset O$ and $m^{*}(O − E) \leq \epsilon$, so I'm looking for a set of implications that lead me back to this definition. all i could prove is that if $E$ measurable from my definition up, iff $ m(A) = m( A \cap E) + m(A \cap E^{c}) $ Thanks in advance for any help you can give me - it's very much appreciated.","The problem statement, all variables and given/known data The question is from Stein and Shakarchi, Real Analysis 2 , Chapter 1, Problem 5: Suppose $E$ is measurable with $m(E) < \infty$, and $E=E_1\cup E_2$, $E_1\cap E_2=\emptyset$. Prove: a) If $m(E) = m^{*}(E_1) + m^{*}(E_2)$, then $E_1$ and $E_2$ are measurable. b) In particular, if $E \subset Q$, where $Q$ is a finite cube, then $E$ is measurable if and only if $m(Q) = m^{*}(E) + m^{*}(Q − E)$. The definition of a 'measurable set' given in the book is that for any $\epsilon > 0$ there exists an open set $O$ with $E \subset O$ and $m^{*}(O − E) \leq \epsilon$, so I'm looking for a set of implications that lead me back to this definition. all i could prove is that if $E$ measurable from my definition up, iff $ m(A) = m( A \cap E) + m(A \cap E^{c}) $ Thanks in advance for any help you can give me - it's very much appreciated.",,"['real-analysis', 'measure-theory']"
19,"$f:\mathbb R\to [0,\infty)$ is a 3 times differentiable and $\max_{x\in\mathbb R}|f'''(x)|\le 1$. Prove that: $f''(x)+\sqrt[3]{\frac{3}{2}f(x)}\ge0 $",is a 3 times differentiable and . Prove that:,"f:\mathbb R\to [0,\infty) \max_{x\in\mathbb R}|f'''(x)|\le 1 f''(x)+\sqrt[3]{\frac{3}{2}f(x)}\ge0 ","I have a problem in Analysis: Let $f:(-\infty,\infty)\to [0,\infty)$ be a three times differentiable and satisfy: $$\max_{x\in\mathbb R}|f'''(x)|\le 1$$ Prove that: $$f''(x)+\sqrt[3]{\frac{3}{2}f(x)}\ge0,\quad \forall x\in\mathbb R$$ Here is my solution: The Taylor series of $f(y)$ at $y=x$ is $f(y)=f(x) + f'(x)(y-x)+\dfrac{f''(x)(y-x)^2}{2}+\dfrac{f'''(\xi)(y-x)^3}{6}$ with $\xi\in \mathbb R$ . Because $f(x)\ge 0,\forall x\in\mathbb R$ and $\displaystyle\max_{x\in\mathbb R}|f'''(x)|\le 1$ , so $$0\le f(x)+f'(x)(y-x)+\frac{f''(x)(y-x)^2}{2}+\frac{(y-x)^3}{6}$$ At this point, I don't know how to continue solving the problem. Can you help me with it?","I have a problem in Analysis: Let be a three times differentiable and satisfy: Prove that: Here is my solution: The Taylor series of at is with . Because and , so At this point, I don't know how to continue solving the problem. Can you help me with it?","f:(-\infty,\infty)\to [0,\infty) \max_{x\in\mathbb R}|f'''(x)|\le 1 f''(x)+\sqrt[3]{\frac{3}{2}f(x)}\ge0,\quad \forall x\in\mathbb R f(y) y=x f(y)=f(x) + f'(x)(y-x)+\dfrac{f''(x)(y-x)^2}{2}+\dfrac{f'''(\xi)(y-x)^3}{6} \xi\in \mathbb R f(x)\ge 0,\forall x\in\mathbb R \displaystyle\max_{x\in\mathbb R}|f'''(x)|\le 1 0\le f(x)+f'(x)(y-x)+\frac{f''(x)(y-x)^2}{2}+\frac{(y-x)^3}{6}","['real-analysis', 'calculus', 'derivatives', 'taylor-expansion']"
20,Prove that $\measuredangle\gamma= 90^{\circ}$,Prove that,\measuredangle\gamma= 90^{\circ},"Given a triangle, if $\sin^{2}\alpha+ \sin^{2}\beta=\!\sin\gamma, \max\!\left ( \measuredangle\alpha- l\measuredangle\beta, \measuredangle\beta- l\measuredangle\alpha \right )\leq 90^{\circ}, \left | l \right |\leq 3$ so $$\measuredangle\gamma= 90^{\circ}$$ with three angles $\alpha, \beta, \gamma$ and $l= constant.$ I have posted that triangle equality for so long.. also had a case proof $,\quad l= 0$ is the only successful one. Now let me show. By substitution $,\quad\sin\alpha:= \frac{2a\left ( a+ 1 \right )}{2a\left ( a+ 1 \right )+ 1}, \sin\beta:= \frac{2b+ 1}{2b^{2}+ 2b+ 1}$ with positives $a, b$ $$\Rightarrow\sin^{2}\alpha+ \sin^{2}\beta- \left ( \sin\gamma \right )_{= 1}= \frac{4\left ( a- b \right )\left ( a+ b+ 1 \right )\left ( 2ab+ a+ b \right )\left ( 2ab+ a+ b+ 1 \right )}{\left ( 2a^{2}+ 2a+ 1 \right )^{2}\left ( 2b^{2}+ 2b+ 1 \right )^{2}}\Rightarrow$$ $$\Rightarrow a= b\Rightarrow\sin^{2}\alpha+ \sin^{2}\beta= 1\Rightarrow\measuredangle\gamma= 90^{\circ}$$","Given a triangle, if so with three angles and I have posted that triangle equality for so long.. also had a case proof is the only successful one. Now let me show. By substitution with positives","\sin^{2}\alpha+ \sin^{2}\beta=\!\sin\gamma, \max\!\left ( \measuredangle\alpha- l\measuredangle\beta, \measuredangle\beta- l\measuredangle\alpha \right )\leq 90^{\circ}, \left | l \right |\leq 3 \measuredangle\gamma= 90^{\circ} \alpha, \beta, \gamma l= constant. ,\quad l= 0 ,\quad\sin\alpha:= \frac{2a\left ( a+ 1 \right )}{2a\left ( a+ 1 \right )+ 1}, \sin\beta:= \frac{2b+ 1}{2b^{2}+ 2b+ 1} a, b \Rightarrow\sin^{2}\alpha+ \sin^{2}\beta- \left ( \sin\gamma \right )_{= 1}= \frac{4\left ( a- b \right )\left ( a+ b+ 1 \right )\left ( 2ab+ a+ b \right )\left ( 2ab+ a+ b+ 1 \right )}{\left ( 2a^{2}+ 2a+ 1 \right )^{2}\left ( 2b^{2}+ 2b+ 1 \right )^{2}}\Rightarrow \Rightarrow a= b\Rightarrow\sin^{2}\alpha+ \sin^{2}\beta= 1\Rightarrow\measuredangle\gamma= 90^{\circ}","['real-analysis', 'calculus']"
21,Splitting a continuous monotonically-increasing function $f(x)$ as $h(x)+h(x+\epsilon) = f(x)$,Splitting a continuous monotonically-increasing function  as,f(x) h(x)+h(x+\epsilon) = f(x),"Given a continuous monotonically-increasing function $f: [0,1]\to \mathbb{R}$ and a parameter $\epsilon>0$ , does there exist a continuous monotonically-increasing function $h$ such that, for all $x\in[0,1]$ : $$h(x)+h(x+\epsilon) = f(x)?$$ If $\epsilon=0$ then $h(x)=f(x)/2$ . But when $\epsilon>0$ , the function $f$ should be split into two parts with a ""phase difference"" of $\epsilon$ . It seems easy, but I could not find the formula for this $h$ .","Given a continuous monotonically-increasing function and a parameter , does there exist a continuous monotonically-increasing function such that, for all : If then . But when , the function should be split into two parts with a ""phase difference"" of . It seems easy, but I could not find the formula for this .","f: [0,1]\to \mathbb{R} \epsilon>0 h x\in[0,1] h(x)+h(x+\epsilon) = f(x)? \epsilon=0 h(x)=f(x)/2 \epsilon>0 f \epsilon h","['real-analysis', 'continuity', 'monotone-functions']"
22,Infinite Series $\sum_{n=1}^\infty\frac{H_n}{n^5 2^n}$,Infinite Series,\sum_{n=1}^\infty\frac{H_n}{n^5 2^n},"Given the n th harmonic number $ H_n = \sum_{j=1}^{n} \frac{1}{j}$ , we get from this post that apparently, $$\sum_{n=1}^{\infty}\frac{H_n}{n^k}z^n= S_{k-1,2}(z) + \rm{Li}_{\,k+1}(z)$$ for $-1\leq z\leq 1$ , and with Nielsen generalized polylogarithm $S_{n,p}(z)$ and polylogarithm $\rm{Li}_n(z)$ . Hence for small $k$ , $$\sum_{n=1}^{\infty}\frac{H_n}{n^2\, 2^n}= S_{1,2}\big(\tfrac12\big)+\rm{Li}_3\big(\tfrac12\big)$$ $$\sum_{n=1}^{\infty}\frac{H_n}{n^3\, 2^n}= S_{2,2}\big(\tfrac12\big)+\rm{Li}_4\big(\tfrac12\big)$$ $$\sum_{n=1}^{\infty}\frac{H_n}{n^4\, 2^n}= S_{3,2}\big(\tfrac12\big)+\rm{Li}_5\big(\tfrac12\big)$$ and so on. Explicitly, given $a=\ln 2$ , $$S_{1,2}\big(\tfrac12\big) +\tfrac1{6}a^3-\tfrac18 \zeta(3)=0 $$ $$S_{2,2}\big(\tfrac12\big) +\tfrac1{168}a^4+\tfrac17a^2\,\rm{Li}_2\big(\tfrac12\big)+\tfrac17a\,\rm{Li}_3\big(\tfrac12\big)-\tfrac18\zeta(4) = 0$$ which are discussed in this and this post. And by yours truly, $$S_{3,2}\big(\tfrac12\big) -A+B  = 0$$ $$A = \tfrac{41}{840}a^5+\tfrac5{21}a^3\,\rm{Li}_2\big(\tfrac12\big)+\tfrac47a^2\,\rm{Li}_3\big(\tfrac12\big)+a\,\rm{Li}_4\big(\tfrac12\big) + \rm{Li}_5\big(\tfrac12\big) $$ $$B=\tfrac12\zeta(2)\zeta(3)+\tfrac18a\,\zeta(4)-\tfrac1{32}\zeta(5)$$ Q: What, however, is the explicit evaluation in ordinary polylogs of the next steps, namely $S_{4,2}\big(\tfrac12\big)$ and $S_{5,2}\big(\tfrac12\big)$ ? P.S. Try as I might, they resist being evaluated and there are indications these higher order integrals may not be expressible by ordinary polylogs.","Given the n th harmonic number , we get from this post that apparently, for , and with Nielsen generalized polylogarithm and polylogarithm . Hence for small , and so on. Explicitly, given , which are discussed in this and this post. And by yours truly, Q: What, however, is the explicit evaluation in ordinary polylogs of the next steps, namely and ? P.S. Try as I might, they resist being evaluated and there are indications these higher order integrals may not be expressible by ordinary polylogs."," H_n = \sum_{j=1}^{n} \frac{1}{j} \sum_{n=1}^{\infty}\frac{H_n}{n^k}z^n= S_{k-1,2}(z) + \rm{Li}_{\,k+1}(z) -1\leq z\leq 1 S_{n,p}(z) \rm{Li}_n(z) k \sum_{n=1}^{\infty}\frac{H_n}{n^2\, 2^n}= S_{1,2}\big(\tfrac12\big)+\rm{Li}_3\big(\tfrac12\big) \sum_{n=1}^{\infty}\frac{H_n}{n^3\, 2^n}= S_{2,2}\big(\tfrac12\big)+\rm{Li}_4\big(\tfrac12\big) \sum_{n=1}^{\infty}\frac{H_n}{n^4\, 2^n}= S_{3,2}\big(\tfrac12\big)+\rm{Li}_5\big(\tfrac12\big) a=\ln 2 S_{1,2}\big(\tfrac12\big) +\tfrac1{6}a^3-\tfrac18 \zeta(3)=0  S_{2,2}\big(\tfrac12\big) +\tfrac1{168}a^4+\tfrac17a^2\,\rm{Li}_2\big(\tfrac12\big)+\tfrac17a\,\rm{Li}_3\big(\tfrac12\big)-\tfrac18\zeta(4) = 0 S_{3,2}\big(\tfrac12\big) -A+B  = 0 A = \tfrac{41}{840}a^5+\tfrac5{21}a^3\,\rm{Li}_2\big(\tfrac12\big)+\tfrac47a^2\,\rm{Li}_3\big(\tfrac12\big)+a\,\rm{Li}_4\big(\tfrac12\big) + \rm{Li}_5\big(\tfrac12\big)  B=\tfrac12\zeta(2)\zeta(3)+\tfrac18a\,\zeta(4)-\tfrac1{32}\zeta(5) S_{4,2}\big(\tfrac12\big) S_{5,2}\big(\tfrac12\big)","['real-analysis', 'sequences-and-series', 'closed-form', 'harmonic-numbers', 'zeta-functions']"
23,Does there exist a bijection $f$ from $\mathbb{N}$ to $\mathbb{Q}^+$ such that $\lim_{n \to \infty} \frac{f(n+1)}{f(n)}$ exists?,Does there exist a bijection  from  to  such that  exists?,f \mathbb{N} \mathbb{Q}^+ \lim_{n \to \infty} \frac{f(n+1)}{f(n)},Does there exist a bijection $f$ from $\mathbb{N}$ to $\mathbb{Q}^+$ such that $$\lim_{n \to \infty} \frac{f(n+1)}{f(n)}$$ exists? My guess that no such $f$ exists.,Does there exist a bijection from to such that exists? My guess that no such exists.,f \mathbb{N} \mathbb{Q}^+ \lim_{n \to \infty} \frac{f(n+1)}{f(n)} f,"['real-analysis', 'rational-numbers']"
24,"Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Prove $|S| < \infty$",Let . Prove,"S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\} |S| < \infty","Question: Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$ . Prove $|S| < \infty$ . Notice this is not true in $\mathbb R[X]$ , as $|x-a|\leq2^x $ , $a\in[0,1]$ shows. After experimenting a few rounds on Desmos, I have found no $p(x)\in \mathbb Z[X]$ with degree $\geq3$ satisfy this property. It looks like a piece of cake, but it turns out that the behavior of $|p(x)|$ is quite chaotic (If we drop the absolute value, the problem will become uninteresting). Of course, $2^x$ is going to eventually dominate all polynomials. But how can I prove all but finitely many polynomials dominate $2^x$ at the beginning? I think I've underestimated the difficulty of the problem. Any hint is appreciated. Follow-up question: Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Find $|S|$.","Question: Let . Prove . Notice this is not true in , as , shows. After experimenting a few rounds on Desmos, I have found no with degree satisfy this property. It looks like a piece of cake, but it turns out that the behavior of is quite chaotic (If we drop the absolute value, the problem will become uninteresting). Of course, is going to eventually dominate all polynomials. But how can I prove all but finitely many polynomials dominate at the beginning? I think I've underestimated the difficulty of the problem. Any hint is appreciated. Follow-up question: Let $S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\}$. Find $|S|$.","S=\{p(x) \in \mathbb Z[X] :|p(x)| \leq 2^x, \forall x\in \mathbb N\} |S| < \infty \mathbb R[X] |x-a|\leq2^x  a\in[0,1] p(x)\in \mathbb Z[X] \geq3 |p(x)| 2^x 2^x","['real-analysis', 'analysis', 'polynomials']"
25,How do I prove that the second derivative of a function $f:M\to\mathbb{R}$ defined on a surface $M\subset\mathbb{R}^n$ is well defined?,How do I prove that the second derivative of a function  defined on a surface  is well defined?,f:M\to\mathbb{R} M\subset\mathbb{R}^n,"QUESTION. How do I prove that the second derivative of a function $f:M\to\mathbb{R}$ defined on a surface $M\subset\mathbb{R}^n$ is well defined? QUESTION. If, as  says the user Amitai Yuval, the second derivative of a function defined on a $ M $ surface can not be well defined in general what is the explanation for this impossibility? By a second derivative I mean an application $D^{(2)}f$ that at each point $ p \in M $ associates a well-defined bilinear aplication $$ D^{(2)}f(p):T_pM \times T_pM \to\mathbb{R}.  $$ Let me make a background in a few steps to try to make the question more precise. Step one. Here $ M $ designates a $C^k$ surface of dimension $ m $ within the Euclidean space $ \mathbb{R}^n$ , i.e . a set in $\mathbb{R}^n$ which satisfies the following properties: there is a family of open sets $\{O_i\}_{i\in I}$ such that $M\subset \bigcup_{i\in I} O_i$ for any $U=M\cap O_i$ there are a open set $U_0$ in $\mathbb{R}^m$ and a $C^k$ ( $k\geq 2$ ) parametrization $\varphi:U_0\to U$ . Step two. I'm assuming that, fixed a point $ p \in M $ , for any two parametrizations $\varphi:U_0\to U\subset M$ and $\psi:V_0\to V\subset M$ such that $p\in V\cap U$ it is proved that $$ (\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V) $$ and $$ ( \psi^{-1}\circ \varphi):\varphi^{-1}(U\cap V)\to \psi^{-1}(U\cap V) $$ are $C^k$ ( $k\geq 2$ ) diffeormorphisms. Step three. I am also assuming that the tangent plane $T_pM$ is the well defined vector space given by any parameterization $\varphi:U_0\to U\subset M$ such that $\varphi(a)=p$ as $$ T_pM= D\varphi(a)(\mathbb{R}^m). $$ By well defined I mean $ D\varphi(a)(\mathbb{R}^m)=D\psi(b)(\mathbb{R}^m)$ for any other parameterization $\psi:V_0\to V\subset M$ such that $\psi(b)=p$ . Step four. A function $ f: M \to \mathbb{R}$ is $C^r$ ( $1\leq r<k$ ) differentiable at point $p$ if there is a parameterization $\varphi:U_0\to U\subset M$ with $\varphi(a)=p$ such that $f\circ\varphi$ is $C^r$ differentiable in $a$ . Once the parameter change $(\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V)$ is differentiable it follows that the application $f\circ\psi$ is differentiable in $b$ for all parameterization $\psi:V_0\to V\subset M$ such that $\psi(b)=p$ . Step five If $ f:M\to \mathbb{R} $ is $C^r$ $(r>1)$ differentiable at point $ p \in M$ then its derivative at that point is the linear transformation $ Df(p):T_pM\to\mathbb{R} $ defined as follows. Let's take a parameterization $\varphi:U_0\to U\subset M$ with $\varphi(a)=p$ . Given a vector $u\in T_p M $ there exists unique a vector $ \mu \in \mathbb{R}^m$ such that $u=Df(a)\mu$ .The derivative of $ f $ at point $ p $ is then simply defined by $$ Df(p)\cdot u =D (f\circ \varphi)(a)\cdot\mu $$ Step six. The linear transformation of step five is well defined. That is, if $\psi:V_0\to V$ is any other parameterization  with $\psi(b)=p$ and $u=D\psi(b)\zeta$ for some vector $\zeta\in\mathbb{R}^m$ , then $$ D (f\circ \varphi)(a)\cdot\mu= D (f\circ \psi)(b)\cdot\zeta. $$ Indeed, $\psi=\varphi\circ(\varphi^{-1}\circ\psi)$ at where $(\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V)$ is a $C^{k}$ diffeomorphism such that $(\varphi^{-1}\circ\psi)(b)=a$ . We have \begin{align}      D\varphi(a)\cdot \mu=& u \\                     =& D\psi(b)\cdot \zeta \\                     =& D((\varphi\circ \varphi^{-1})\circ\psi )(b)\cdot \zeta\\                     =& D(\varphi\circ(\varphi^{-1}\circ\psi))(b)\cdot \zeta\\                     =& D\varphi(a)\cdot \Big(D(\varphi^{-1}\circ\psi)(b)\cdot\zeta\Big) \end{align} Since $ D \varphi(a)$ is injective we have $\mu=D(\varphi^{-1}\circ\psi)(b)\cdot\zeta$ . Therefore, \begin{align} D(f\circ\psi)(b)\zeta =& D(f\circ(\varphi\circ\varphi^{-1})\circ\psi)(b)\cdot\zeta \\                     =& D(f\circ \varphi\circ(\varphi^{-1}\circ\psi))(b)\cdot\zeta \\                     =& D(f\circ \varphi)(a) \cdot\Big( D(\varphi^{-1}\circ\psi)(b)\cdot\zeta\Big) \\                     =& D(f\circ \varphi)(a) \cdot\mu \end{align} Let us end the background here and return to the question. Below is my attempt to answer the question. By analogy to the first order derivative the second derivative would work as follows. DEFINITION. Let $M$ a $C^k$ ( $k>2$ ) surface. Let $ f:M\to \mathbb{R} $ is $C^2$ differentiable at point $ p \in M$ . Its second derivative $D^{(2)}f$ associates each point $p$ the bilinear transformation $$  D^{(2)}f(p):T_pM\times T_pM\to\mathbb{R}  $$ such that for all parameterization $\varphi:U_0\to U\subset M$ , with $\varphi(a)=p$ , and all vectors $u,v\in T_p M $ $$ D^{(2)}f(p)\cdot (u,v) \mathop{=}^{_{\rm def}}D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu) $$ with $ \mu,\nu \in \mathbb{R}^m$ such that $ u=D\varphi(a)\mu \quad \mbox{ and }\quad v=D\varphi(a)\nu. $ Similarly to the case of the first derivative, the second derivative will be well defined if for any other parameterization $\psi:V_0\to V$ such that $\psi(b)=p$ , $$ D\psi(b)\eta=u \quad \mbox{ and } \quad D\psi(b)\zeta=v $$ for some $\eta\in\mathbb{R}^m$ and for some $\zeta\in\mathbb{R}^m$ we have $$ D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)=D^{(2)} (f\circ \psi)(b)\cdot(\eta,\zeta) $$ To show the above equality I tried to imitate step six. We have \begin{align} D^{(2)} (f\circ \psi)(b)\cdot(\eta,\zeta) =&  D^{(2)} (f\circ(\varphi\circ\varphi^{-1})\circ \psi)(b)\cdot(\eta,\zeta) \\ =&  D^{(2)} (f\circ\varphi\circ(\varphi^{-1}\circ \psi))(b)\cdot(\eta,\zeta) \end{align} But in this last equality I can not use the chain rule. On the other hand the development of the second derivative $D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)$ in terms of the partial derivatives $\frac{\partial^{2}f}{\partial x_i\partial x_j}(a)$ was more productive. Look \begin{align} D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu) =& \left[ 	\frac{\partial}{\partial \nu}  	\left( 		\frac{\partial}{\partial \mu} (f\circ\varphi)  	\right) \right](a) \\ =& \left[ 	\frac{\partial}{\partial \nu}  	\left( 		\sum_{j=1}^{m}\mu_j\cdot \frac{\partial}{\partial x_j} (f\circ\varphi)  	\right) \right](a) \\ =& \sum_{i=1}^m\sum_{j=1}^{m} \nu_i\cdot\mu_j\cdot \frac{\partial^2}{\partial x_i \partial x_j} (f\circ\varphi)(a)  \\ \end{align} By $\frac{\partial}{\partial \mu} (f\circ\varphi)(a)= D(f\circ \varphi)(a) \cdot\mu =u= D(f\circ\psi)(b)\eta= \frac{\partial}{\partial \eta} (f\circ\psi)(b)$ we have \begin{align} D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu) =& \left[ 	\frac{\partial}{\partial \nu}  	\left( 		\frac{\partial}{\partial \mu} (f\circ\varphi)  	\right) \right](a) \\ =& \left[ 	\frac{\partial}{\partial \nu}  	\left( 		\frac{\partial}{\partial \eta} (f\circ\psi) 	\right) \right](b) \\ =& \left[ 	\frac{\partial}{\partial \nu}  	\left( 		\sum_{j=1}^{m}\eta_j\cdot \frac{\partial}{\partial x_j} (f\circ\psi)  	\right) \right](b) \\ =& \sum_{i=1}^m\sum_{j=1}^{m} \nu_i\cdot\eta_j\cdot \frac{\partial^2}{\partial x_i \partial x_j} (f\circ\psi)(b)  \\ =& D^{(2)}(f\circ\psi)(a)\cdot(\eta,\nu)  \end{align} By an entirely analogous calculation and applying the Schwarz theorem we have $$ D^{(2)}(f\circ\varphi)(a)\cdot(\mu,\nu)= D^{(2)}(f\circ\psi)(a)\cdot(\mu,\zeta). $$ QUESTION. The question then becomes the following. How to use the identities below \begin{align} D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)=&D^{(2)}(f\circ\psi)(a)\cdot(\eta,\nu)  \\ D^{(2)}(f\circ\varphi)(a)\cdot(\mu,\nu)=&D^{(2)}(f\circ\psi)(a)\cdot(\mu,\zeta). \end{align} to prove the  well definiteness of $ D^{(2)}f$ ?","QUESTION. How do I prove that the second derivative of a function defined on a surface is well defined? QUESTION. If, as  says the user Amitai Yuval, the second derivative of a function defined on a surface can not be well defined in general what is the explanation for this impossibility? By a second derivative I mean an application that at each point associates a well-defined bilinear aplication Let me make a background in a few steps to try to make the question more precise. Step one. Here designates a surface of dimension within the Euclidean space , i.e . a set in which satisfies the following properties: there is a family of open sets such that for any there are a open set in and a ( ) parametrization . Step two. I'm assuming that, fixed a point , for any two parametrizations and such that it is proved that and are ( ) diffeormorphisms. Step three. I am also assuming that the tangent plane is the well defined vector space given by any parameterization such that as By well defined I mean for any other parameterization such that . Step four. A function is ( ) differentiable at point if there is a parameterization with such that is differentiable in . Once the parameter change is differentiable it follows that the application is differentiable in for all parameterization such that . Step five If is differentiable at point then its derivative at that point is the linear transformation defined as follows. Let's take a parameterization with . Given a vector there exists unique a vector such that .The derivative of at point is then simply defined by Step six. The linear transformation of step five is well defined. That is, if is any other parameterization  with and for some vector , then Indeed, at where is a diffeomorphism such that . We have Since is injective we have . Therefore, Let us end the background here and return to the question. Below is my attempt to answer the question. By analogy to the first order derivative the second derivative would work as follows. DEFINITION. Let a ( ) surface. Let is differentiable at point . Its second derivative associates each point the bilinear transformation such that for all parameterization , with , and all vectors with such that Similarly to the case of the first derivative, the second derivative will be well defined if for any other parameterization such that , for some and for some we have To show the above equality I tried to imitate step six. We have But in this last equality I can not use the chain rule. On the other hand the development of the second derivative in terms of the partial derivatives was more productive. Look By we have By an entirely analogous calculation and applying the Schwarz theorem we have QUESTION. The question then becomes the following. How to use the identities below to prove the  well definiteness of ?","f:M\to\mathbb{R} M\subset\mathbb{R}^n  M  D^{(2)}f  p \in M  
D^{(2)}f(p):T_pM \times T_pM \to\mathbb{R}. 
  M  C^k  m   \mathbb{R}^n \mathbb{R}^n \{O_i\}_{i\in I} M\subset \bigcup_{i\in I} O_i U=M\cap O_i U_0 \mathbb{R}^m C^k k\geq 2 \varphi:U_0\to U  p \in M  \varphi:U_0\to U\subset M \psi:V_0\to V\subset M p\in V\cap U 
(\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V)
 
( \psi^{-1}\circ \varphi):\varphi^{-1}(U\cap V)\to \psi^{-1}(U\cap V)
 C^k k\geq 2 T_pM \varphi:U_0\to U\subset M \varphi(a)=p 
T_pM= D\varphi(a)(\mathbb{R}^m).
  D\varphi(a)(\mathbb{R}^m)=D\psi(b)(\mathbb{R}^m) \psi:V_0\to V\subset M \psi(b)=p  f: M \to \mathbb{R} C^r 1\leq r<k p \varphi:U_0\to U\subset M \varphi(a)=p f\circ\varphi C^r a (\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V) f\circ\psi b \psi:V_0\to V\subset M \psi(b)=p  f:M\to \mathbb{R}  C^r (r>1)  p \in M  Df(p):T_pM\to\mathbb{R}  \varphi:U_0\to U\subset M \varphi(a)=p u\in T_p M   \mu \in \mathbb{R}^m u=Df(a)\mu  f   p  
Df(p)\cdot u =D (f\circ \varphi)(a)\cdot\mu
 \psi:V_0\to V \psi(b)=p u=D\psi(b)\zeta \zeta\in\mathbb{R}^m 
D (f\circ \varphi)(a)\cdot\mu= D (f\circ \psi)(b)\cdot\zeta.
 \psi=\varphi\circ(\varphi^{-1}\circ\psi) (\varphi^{-1}\circ\psi):\psi^{-1}(U\cap V)\to \varphi^{-1}(U\cap V) C^{k} (\varphi^{-1}\circ\psi)(b)=a \begin{align}
     D\varphi(a)\cdot \mu=& u \\
                    =& D\psi(b)\cdot \zeta \\
                    =& D((\varphi\circ \varphi^{-1})\circ\psi )(b)\cdot \zeta\\
                    =& D(\varphi\circ(\varphi^{-1}\circ\psi))(b)\cdot \zeta\\
                    =& D\varphi(a)\cdot \Big(D(\varphi^{-1}\circ\psi)(b)\cdot\zeta\Big)
\end{align}  D \varphi(a) \mu=D(\varphi^{-1}\circ\psi)(b)\cdot\zeta \begin{align}
D(f\circ\psi)(b)\zeta =& D(f\circ(\varphi\circ\varphi^{-1})\circ\psi)(b)\cdot\zeta \\
                    =& D(f\circ \varphi\circ(\varphi^{-1}\circ\psi))(b)\cdot\zeta \\
                    =& D(f\circ \varphi)(a) \cdot\Big( D(\varphi^{-1}\circ\psi)(b)\cdot\zeta\Big) \\
                    =& D(f\circ \varphi)(a) \cdot\mu
\end{align} M C^k k>2  f:M\to \mathbb{R}  C^2  p \in M D^{(2)}f p  
D^{(2)}f(p):T_pM\times T_pM\to\mathbb{R} 
 \varphi:U_0\to U\subset M \varphi(a)=p u,v\in T_p M  
D^{(2)}f(p)\cdot (u,v) \mathop{=}^{_{\rm def}}D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)
  \mu,\nu \in \mathbb{R}^m 
u=D\varphi(a)\mu \quad \mbox{ and }\quad v=D\varphi(a)\nu.
 \psi:V_0\to V \psi(b)=p 
D\psi(b)\eta=u \quad \mbox{ and } \quad D\psi(b)\zeta=v
 \eta\in\mathbb{R}^m \zeta\in\mathbb{R}^m 
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)=D^{(2)} (f\circ \psi)(b)\cdot(\eta,\zeta)
 \begin{align}
D^{(2)} (f\circ \psi)(b)\cdot(\eta,\zeta)
=& 
D^{(2)} (f\circ(\varphi\circ\varphi^{-1})\circ \psi)(b)\cdot(\eta,\zeta)
\\
=& 
D^{(2)} (f\circ\varphi\circ(\varphi^{-1}\circ \psi))(b)\cdot(\eta,\zeta)
\end{align} D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu) \frac{\partial^{2}f}{\partial x_i\partial x_j}(a) \begin{align}
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\frac{\partial}{\partial \mu} (f\circ\varphi) 
	\right)
\right](a)
\\
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\sum_{j=1}^{m}\mu_j\cdot \frac{\partial}{\partial x_j} (f\circ\varphi) 
	\right)
\right](a)
\\
=&
\sum_{i=1}^m\sum_{j=1}^{m}
\nu_i\cdot\mu_j\cdot \frac{\partial^2}{\partial x_i \partial x_j} (f\circ\varphi)(a) 
\\
\end{align} \frac{\partial}{\partial \mu} (f\circ\varphi)(a)= D(f\circ \varphi)(a) \cdot\mu =u= D(f\circ\psi)(b)\eta= \frac{\partial}{\partial \eta} (f\circ\psi)(b) \begin{align}
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\frac{\partial}{\partial \mu} (f\circ\varphi) 
	\right)
\right](a)
\\
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\frac{\partial}{\partial \eta} (f\circ\psi)
	\right)
\right](b)
\\
=&
\left[
	\frac{\partial}{\partial \nu} 
	\left(
		\sum_{j=1}^{m}\eta_j\cdot \frac{\partial}{\partial x_j} (f\circ\psi) 
	\right)
\right](b)
\\
=&
\sum_{i=1}^m\sum_{j=1}^{m}
\nu_i\cdot\eta_j\cdot \frac{\partial^2}{\partial x_i \partial x_j} (f\circ\psi)(b) 
\\
=&
D^{(2)}(f\circ\psi)(a)\cdot(\eta,\nu) 
\end{align} 
D^{(2)}(f\circ\varphi)(a)\cdot(\mu,\nu)= D^{(2)}(f\circ\psi)(a)\cdot(\mu,\zeta).
 \begin{align}
D^{(2)} (f\circ \varphi)(a)\cdot(\mu,\nu)=&D^{(2)}(f\circ\psi)(a)\cdot(\eta,\nu) 
\\
D^{(2)}(f\circ\varphi)(a)\cdot(\mu,\nu)=&D^{(2)}(f\circ\psi)(a)\cdot(\mu,\zeta).
\end{align}  D^{(2)}f","['real-analysis', 'multivariable-calculus', 'derivatives', 'differential-geometry', 'differential-topology']"
26,Asymptotic estimation of $A_n$,Asymptotic estimation of,A_n,"Let $A_n$ represent the number of integers that can be written as the product of two element of $[[1,n]]$ . I am looking for an asymptotic estimation of $A_n$ . First, I think it’s a good start to look at the exponent $\alpha$ such that : $$A_n = o(n^\alpha)$$ I think we have : $2 \leq \alpha $ . To prove this lower bound we use the fact that the number of primer numbers $\leq n$ is about $ \frac{n}{\log n}$ . Hence we have the trivial lower bound (assuming $n$ is big enough) : $$ \binom{ E(\frac{n}{\log n})}{2} = o(n^2)$$ Now is it possible to get a good asymptotic for $A_n$ and not just this lower bound ?  Is what I’ve done so far correct ? Thank you !","Let represent the number of integers that can be written as the product of two element of . I am looking for an asymptotic estimation of . First, I think it’s a good start to look at the exponent such that : I think we have : . To prove this lower bound we use the fact that the number of primer numbers is about . Hence we have the trivial lower bound (assuming is big enough) : Now is it possible to get a good asymptotic for and not just this lower bound ?  Is what I’ve done so far correct ? Thank you !","A_n [[1,n]] A_n \alpha A_n = o(n^\alpha) 2 \leq \alpha  \leq n  \frac{n}{\log n} n  \binom{ E(\frac{n}{\log n})}{2} = o(n^2) A_n","['calculus', 'real-analysis', 'combinatorics', 'number-theory', 'asymptotics']"
27,Integral equation,Integral equation,,"For any $f:\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function, prove that $\int_0^4f(x(x-3)^2)dx=2\int_1^3f(x(x-3)^2)dx$. I tried many things, but nothing worked out. I observed that $g(x)+g(4-x)=4$ where $g(x)=x(x-3)^2$. Hence this action $x \rightarrow 4-x$ acts in some peculiar way. Please, any help would be great.","For any $f:\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function, prove that $\int_0^4f(x(x-3)^2)dx=2\int_1^3f(x(x-3)^2)dx$. I tried many things, but nothing worked out. I observed that $g(x)+g(4-x)=4$ where $g(x)=x(x-3)^2$. Hence this action $x \rightarrow 4-x$ acts in some peculiar way. Please, any help would be great.",,"['calculus', 'real-analysis', 'definite-integrals']"
28,Proving the *Caratheodory Criterion* for *Lebesgue Measurability*,Proving the *Caratheodory Criterion* for *Lebesgue Measurability*,,"I'm following Terry Tao 's An introduction to measure theory . Here he has defined lebesgue measurability as: Definition 1 (Lebesgue measurability): A set $E$ is said to be lebesgue measurable if for every $\epsilon > 0$, $\exists$ an open set $U$, containing $E$, such that $m^*(U \setminus E) \leq \epsilon$. where $m^*$ denotes the outer lebesgue measure . Then he has stated the Caratheodory Criterion : Definition 2 (Lebesgue measurability): A set $E$ is said to be lebesgue measurable if for every elementary set $A$, we have    $$m(A)=m^*(A \cap E)+m^*(A \setminus E)$$ where an elementary set is a finite union of boxes . The problem is to show the equivalence of the two definitions. I have shown that Def $(1)$ $\implies$ Def $(2)$, but having difficulty in showing the reverse, i.e. Def $(2)$ $\implies$ Def $(1)$. Any help (full/brief solution or even some hints) would be greatly appreciated! Thanks in advance.","I'm following Terry Tao 's An introduction to measure theory . Here he has defined lebesgue measurability as: Definition 1 (Lebesgue measurability): A set $E$ is said to be lebesgue measurable if for every $\epsilon > 0$, $\exists$ an open set $U$, containing $E$, such that $m^*(U \setminus E) \leq \epsilon$. where $m^*$ denotes the outer lebesgue measure . Then he has stated the Caratheodory Criterion : Definition 2 (Lebesgue measurability): A set $E$ is said to be lebesgue measurable if for every elementary set $A$, we have    $$m(A)=m^*(A \cap E)+m^*(A \setminus E)$$ where an elementary set is a finite union of boxes . The problem is to show the equivalence of the two definitions. I have shown that Def $(1)$ $\implies$ Def $(2)$, but having difficulty in showing the reverse, i.e. Def $(2)$ $\implies$ Def $(1)$. Any help (full/brief solution or even some hints) would be greatly appreciated! Thanks in advance.",,"['real-analysis', 'measure-theory']"
29,Continuous function maps $F_{\sigma}$ sets to $F_{\sigma}$ sets,Continuous function maps  sets to  sets,F_{\sigma} F_{\sigma},"Prove if $X\subset \Bbb{R}$ is $F_{\sigma}$ (can be written as a countable union of closed sets) and $f$ is continuous then $f(X)$ is $F_{\sigma}$. Proof: Let $X=\cup C_i$ where $C_i$ is closed. Then define $D_{i,n}=C_i \cap [-n,n]$. Then $D_{i,n}$ is compact (closed because intersection of two closed sets, and bounded by $n$). Note that $X=\cup D_{i,n}$. Because $f$ is continuous, it maps compact sets to compact sets. So $f(X)=\cup f( D_{i,n})$ which is a countable union of closed sets. Does this look okay? This is a previous qualifying exam question .","Prove if $X\subset \Bbb{R}$ is $F_{\sigma}$ (can be written as a countable union of closed sets) and $f$ is continuous then $f(X)$ is $F_{\sigma}$. Proof: Let $X=\cup C_i$ where $C_i$ is closed. Then define $D_{i,n}=C_i \cap [-n,n]$. Then $D_{i,n}$ is compact (closed because intersection of two closed sets, and bounded by $n$). Note that $X=\cup D_{i,n}$. Because $f$ is continuous, it maps compact sets to compact sets. So $f(X)=\cup f( D_{i,n})$ which is a countable union of closed sets. Does this look okay? This is a previous qualifying exam question .",,[]
30,How to estimate $ \left(1 + \sqrt{2} + \sqrt{3} + \dots + \sqrt{n} \right) - \frac{2}{3} n \sqrt{n}$?,How to estimate ?, \left(1 + \sqrt{2} + \sqrt{3} + \dots + \sqrt{n} \right) - \frac{2}{3} n \sqrt{n},"How do I estimate the error for the sum of reciprocals of square roots.  From calculus we know that: $$ \int_0^n \sqrt{x} \, dx  = \frac{2}{3} x\sqrt{x} \;\;\Bigg|_{x=0}^{x=n} = \frac{2}{3}n\sqrt{n}$$ I forget the name - midpoint rule , trapezoid rule ?? - basically we want to approximate the integral as a Riemann sum.  How do we estimate the error? $$ \left(1 + \sqrt{2} + \sqrt{3} + \dots +  \sqrt{n} \right)  - \frac{2}{3} n \sqrt{n}$$ To give you a sense of how much we are losing on this approximation, let's   draw two pictures. We are losing all the gray stuff in our approximation, which is quite a lot!  I don't really care about the integral, what's important is the difference between all the stuff we are adding and square root of $n$. The yellow triangle has base $1$ and height $\sqrt{14} - \sqrt{13}$ so the area is: $$ A = \frac{1}{2} \times b \times h = \frac{1}{2} \times 1 \times (\sqrt{\color{#E0E070}{14}} - \sqrt{\color{#E0E070}{13}}) = \frac{1}{2}(\sqrt{n+1} - \sqrt{n}) \approx \frac{1}{4 \sqrt{n}}$$ This suggests the total of all errors is about $\propto \sqrt{\color{#D22}{n}}$ which is not a small amount.  Can anyone get the constant of proportionality? The Euler-Maclaurin machine does crank out such error estimates, but the square root function is not so strange. Can we derive such an estimate in this specific case using basic standard inequalities?","How do I estimate the error for the sum of reciprocals of square roots.  From calculus we know that: $$ \int_0^n \sqrt{x} \, dx  = \frac{2}{3} x\sqrt{x} \;\;\Bigg|_{x=0}^{x=n} = \frac{2}{3}n\sqrt{n}$$ I forget the name - midpoint rule , trapezoid rule ?? - basically we want to approximate the integral as a Riemann sum.  How do we estimate the error? $$ \left(1 + \sqrt{2} + \sqrt{3} + \dots +  \sqrt{n} \right)  - \frac{2}{3} n \sqrt{n}$$ To give you a sense of how much we are losing on this approximation, let's   draw two pictures. We are losing all the gray stuff in our approximation, which is quite a lot!  I don't really care about the integral, what's important is the difference between all the stuff we are adding and square root of $n$. The yellow triangle has base $1$ and height $\sqrt{14} - \sqrt{13}$ so the area is: $$ A = \frac{1}{2} \times b \times h = \frac{1}{2} \times 1 \times (\sqrt{\color{#E0E070}{14}} - \sqrt{\color{#E0E070}{13}}) = \frac{1}{2}(\sqrt{n+1} - \sqrt{n}) \approx \frac{1}{4 \sqrt{n}}$$ This suggests the total of all errors is about $\propto \sqrt{\color{#D22}{n}}$ which is not a small amount.  Can anyone get the constant of proportionality? The Euler-Maclaurin machine does crank out such error estimates, but the square root function is not so strange. Can we derive such an estimate in this specific case using basic standard inequalities?",,"['calculus', 'real-analysis', 'numerical-methods']"
31,How to show that the Dini Derivatives of a measurable function is measurable?,How to show that the Dini Derivatives of a measurable function is measurable?,,"Let $f:(0,1)\to\mathbb R$ be measurable. Then, the (right upper) Dini derivative $$ D^+ f(x) = \limsup_{h\to 0^+} \frac{f(x+h) - f(x)}{h} $$ is also measurable (a well known result of Banach). Can someone give me a source in English (or German) or a proof sketch? If it makes thing much easier, we can assume $f$ is monotone (in that case please don't argue that $f$ is differentiable a.e. That feels like cheating :)","Let $f:(0,1)\to\mathbb R$ be measurable. Then, the (right upper) Dini derivative $$ D^+ f(x) = \limsup_{h\to 0^+} \frac{f(x+h) - f(x)}{h} $$ is also measurable (a well known result of Banach). Can someone give me a source in English (or German) or a proof sketch? If it makes thing much easier, we can assume $f$ is monotone (in that case please don't argue that $f$ is differentiable a.e. That feels like cheating :)",,"['real-analysis', 'measure-theory', 'derivatives']"
32,Total variation measure vs. total variation function,Total variation measure vs. total variation function,,"Let $a, b \in \mathbb{R}$ with $a < b$ and define the compact interval $I := [a, b]$. Let $g, h : \mathbb{R} \rightarrow \mathbb{R}$ be non-decreasing and right-continuous on $I$ and constant on $(-\infty, a]$ and on $[b, \infty)$, separately, and define $f := g - h$, so that, over every compact interval, $g - h$ is a Jordan decomposition of the bounded-variation function $f$. Denote by $\mu_g$ the Lebesgue-Stieltjes measure on $(\mathbb{R}, \mathcal{B})$ engendered by $g$ and denote by $\mu_h$ the one engendered by $h$, and define the (finite) signed measure $\mu_f := \mu_g - \mu_h$. Denote by $\mu_f'$ the total variation measure corresponding to $\mu_f$, i.e. if $\mu_f = \mu_f^+ - \mu_f^-$ is the unique Jordan decomposition of $\mu_f$, then $\mu_f' = \mu_f^+ + \mu_f^-$. Denote by $v_f : [a, \infty) \rightarrow [0, \infty]$ the total variation function of $f$ on the ray $[a, \infty)$, i.e. for every $t \in [a, \infty)$, $$ v_f(t) := \sup \left\{\sum_{k = 1}^n \left|f(t_k) - f(t_{k - 1})\right| :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\}\right\} $$ It is well known that $v_f(a) = 0$, that $v_f$ is non-decreasing and, since $f$ is right-continuous, so is $v_f$. Extend $v_f$ to the entire real line by setting $v_f(t) := 0$ for $t \in (-\infty, a)$. The extended $v_f$ remains non-decreasing and right-continuous. Denote by $\mu_v$ the Lebesgue-Stieltjes measure engendered by $v_f$. Is it the case that $\mu_f' = \mu_v$? Attempts at a solution Attempt #1 Since $\mu_f'(I^c) = \mu_v(I^c) = 0$, it remains to show that $\mu_f' = \mu_v$ over $(I, \mathcal{B}(I))$. All I've managed to show so far is that for $t \in [a, b]$, $\mu_v([a, t]) \leq \mu_f'([a, t])$. Indeed, according to proposition 10.23 in Yeh's ""Real Analysis"", 2nd edition , for every $E \in \mathcal{B}(I)$, $$ \mu_f'(E) = \sup \left\{\sum_{k = 1}^n |\mu_f(E_k)| :\mid E = \bigcup_{k = 1}^n E_k; E_1, \dots, E_n \in \mathcal{B}(I) \text{ pairwise disjoint}, n \in \{1, 2, \dots\}\right\} $$ Therefore, for every $t \in I$, $$ \begin{split} \mu_v([a, t]) & = \lim_{n \rightarrow \infty} \mu_v\left((a - \frac{1}{n}, t]\right) \\ & = \lim_{n \rightarrow \infty} \left(v_f(t) - v_f(a - \frac{1}{n})\right) \\ & = v_f(t) \\ & \leq \mu_f'([a, t]) \end{split} $$ If we can show the converse inequality, namely that for every $t \in I$, $\mu_f'([a, t]) \leq \mu_v([a, t])$, we may proceed to conclude that $\mu_v = \mu_f'$ by using Dynkin's $\pi$-$\lambda$ theorem . Attempt #2 Define, for $t \in [0, \infty)$, $$ \begin{align} v_f^+(f) & := \sup \left\{\sum_{k = 1}^n\max\left(f(t_k) - f(t_{k - 1}), 0\right) :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\} \right\} \\ v_f^-(f) & := \sup \left\{\sum_{k = 1}^n-\min\left(f(t_k) - f(t_{k - 1}), 0\right) :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\} \right\} \end{align} $$ Then it can be seen (see, for instance, lemma 5.2.3 on p. 99 and the beginning of the proof of theorem 5.2.4 on p. 100 of Royden's ""Real Analysis"", 2nd edition (!), Macmillan Library Reference, 1968) that $v_f^+$ and $v_f^-$ are non-decreasing on $[0, \infty)$ and that for $t \in [0, \infty)$, $$ \begin{align} f(t) & = v_f^+(t) - v_f^-(t) \\ v_f(t) & = v_f^+(t) + v_f^-(t) \end{align} $$ It can also be shown that $v_f^+$ and $v_f^-$ are right continuous, by emulating the analogous proof for $v_f$ (see for instance theorem 12.22 on p. 266 of Yeh's ""Real Analysis"", 2nd edition). We may extend $v_f^+$ and $v_f^-$ to the entire real line by defining $v_f^+(t) := v_f^-(t) := 0$ for $t \in (-\infty, 0)$. The extended functions remain non-decreasing and right-continuous. If we denote the Lebesgue-Stieltjes measures engendered by $v_f^+$ and $v_f^-$ by $\mu_v^+$ and $\mu_v^-$, respectively, we have that $\mu_f = \mu_v^+ - \mu_v^-$ (indeed, as shown here , if $\phi_1, \phi_2, \varphi_1, \varphi_2 : \mathbb{R} \rightarrow \mathbb{R}$ are non-decreasing, right-continuous functions such that $\phi_1 - \phi_2 = \varphi_1 - \varphi_2$, then, denoting the Lebesgue-Stieltjes measures they engender by $\mu_{\phi_1}, \mu_{\phi_2}, \mu_{\varphi_1}, \mu_{\varphi_2}$, respectively, we have $\mu_{\phi_1} - \mu_{\phi_2} = \mu_{\varphi_1} - \mu_{\varphi_2}$, as long as both sides of the equation are well defined, i.e. as long as at least one measure on each side of the equation is finite), and, furthermore, it may be verified using Dynkin's $\pi$-$\lambda$ theorem starting from the $\pi$-system of half open-half closed finite intervals, that $\mu_v = \mu_v^+ + \mu_v^-$. Therefore, if we can prove that $\mu_v^+ \perp \mu_v^-$, i.e. that $\mu_v^+$ and $\mu_v^-$ are mutually singular, then, by the uniqueness of the Jordan decomposition, $\mu_v^+ - \mu_v^-$ is the Jordan decomposition of $\mu_f$, i.e. $\mu_f^+ = \mu_v^+$ and $\mu_f^- = \mu_v^-$, and therefore we have, by definition of $\mu_f'$, that $\mu_v = \mu_f^+ + \mu_f^- = \mu_f'$, as desired. So, all that is left to show is that $\mu_v^+ \perp \mu_v^-$. Related posts A similar question was asked in this forum almost two years ago , but has remained unanswered.","Let $a, b \in \mathbb{R}$ with $a < b$ and define the compact interval $I := [a, b]$. Let $g, h : \mathbb{R} \rightarrow \mathbb{R}$ be non-decreasing and right-continuous on $I$ and constant on $(-\infty, a]$ and on $[b, \infty)$, separately, and define $f := g - h$, so that, over every compact interval, $g - h$ is a Jordan decomposition of the bounded-variation function $f$. Denote by $\mu_g$ the Lebesgue-Stieltjes measure on $(\mathbb{R}, \mathcal{B})$ engendered by $g$ and denote by $\mu_h$ the one engendered by $h$, and define the (finite) signed measure $\mu_f := \mu_g - \mu_h$. Denote by $\mu_f'$ the total variation measure corresponding to $\mu_f$, i.e. if $\mu_f = \mu_f^+ - \mu_f^-$ is the unique Jordan decomposition of $\mu_f$, then $\mu_f' = \mu_f^+ + \mu_f^-$. Denote by $v_f : [a, \infty) \rightarrow [0, \infty]$ the total variation function of $f$ on the ray $[a, \infty)$, i.e. for every $t \in [a, \infty)$, $$ v_f(t) := \sup \left\{\sum_{k = 1}^n \left|f(t_k) - f(t_{k - 1})\right| :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\}\right\} $$ It is well known that $v_f(a) = 0$, that $v_f$ is non-decreasing and, since $f$ is right-continuous, so is $v_f$. Extend $v_f$ to the entire real line by setting $v_f(t) := 0$ for $t \in (-\infty, a)$. The extended $v_f$ remains non-decreasing and right-continuous. Denote by $\mu_v$ the Lebesgue-Stieltjes measure engendered by $v_f$. Is it the case that $\mu_f' = \mu_v$? Attempts at a solution Attempt #1 Since $\mu_f'(I^c) = \mu_v(I^c) = 0$, it remains to show that $\mu_f' = \mu_v$ over $(I, \mathcal{B}(I))$. All I've managed to show so far is that for $t \in [a, b]$, $\mu_v([a, t]) \leq \mu_f'([a, t])$. Indeed, according to proposition 10.23 in Yeh's ""Real Analysis"", 2nd edition , for every $E \in \mathcal{B}(I)$, $$ \mu_f'(E) = \sup \left\{\sum_{k = 1}^n |\mu_f(E_k)| :\mid E = \bigcup_{k = 1}^n E_k; E_1, \dots, E_n \in \mathcal{B}(I) \text{ pairwise disjoint}, n \in \{1, 2, \dots\}\right\} $$ Therefore, for every $t \in I$, $$ \begin{split} \mu_v([a, t]) & = \lim_{n \rightarrow \infty} \mu_v\left((a - \frac{1}{n}, t]\right) \\ & = \lim_{n \rightarrow \infty} \left(v_f(t) - v_f(a - \frac{1}{n})\right) \\ & = v_f(t) \\ & \leq \mu_f'([a, t]) \end{split} $$ If we can show the converse inequality, namely that for every $t \in I$, $\mu_f'([a, t]) \leq \mu_v([a, t])$, we may proceed to conclude that $\mu_v = \mu_f'$ by using Dynkin's $\pi$-$\lambda$ theorem . Attempt #2 Define, for $t \in [0, \infty)$, $$ \begin{align} v_f^+(f) & := \sup \left\{\sum_{k = 1}^n\max\left(f(t_k) - f(t_{k - 1}), 0\right) :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\} \right\} \\ v_f^-(f) & := \sup \left\{\sum_{k = 1}^n-\min\left(f(t_k) - f(t_{k - 1}), 0\right) :\mid a = t_0 \leq t_1 \leq \cdots \leq t_n = t, n \in \{1, 2, \dots\} \right\} \end{align} $$ Then it can be seen (see, for instance, lemma 5.2.3 on p. 99 and the beginning of the proof of theorem 5.2.4 on p. 100 of Royden's ""Real Analysis"", 2nd edition (!), Macmillan Library Reference, 1968) that $v_f^+$ and $v_f^-$ are non-decreasing on $[0, \infty)$ and that for $t \in [0, \infty)$, $$ \begin{align} f(t) & = v_f^+(t) - v_f^-(t) \\ v_f(t) & = v_f^+(t) + v_f^-(t) \end{align} $$ It can also be shown that $v_f^+$ and $v_f^-$ are right continuous, by emulating the analogous proof for $v_f$ (see for instance theorem 12.22 on p. 266 of Yeh's ""Real Analysis"", 2nd edition). We may extend $v_f^+$ and $v_f^-$ to the entire real line by defining $v_f^+(t) := v_f^-(t) := 0$ for $t \in (-\infty, 0)$. The extended functions remain non-decreasing and right-continuous. If we denote the Lebesgue-Stieltjes measures engendered by $v_f^+$ and $v_f^-$ by $\mu_v^+$ and $\mu_v^-$, respectively, we have that $\mu_f = \mu_v^+ - \mu_v^-$ (indeed, as shown here , if $\phi_1, \phi_2, \varphi_1, \varphi_2 : \mathbb{R} \rightarrow \mathbb{R}$ are non-decreasing, right-continuous functions such that $\phi_1 - \phi_2 = \varphi_1 - \varphi_2$, then, denoting the Lebesgue-Stieltjes measures they engender by $\mu_{\phi_1}, \mu_{\phi_2}, \mu_{\varphi_1}, \mu_{\varphi_2}$, respectively, we have $\mu_{\phi_1} - \mu_{\phi_2} = \mu_{\varphi_1} - \mu_{\varphi_2}$, as long as both sides of the equation are well defined, i.e. as long as at least one measure on each side of the equation is finite), and, furthermore, it may be verified using Dynkin's $\pi$-$\lambda$ theorem starting from the $\pi$-system of half open-half closed finite intervals, that $\mu_v = \mu_v^+ + \mu_v^-$. Therefore, if we can prove that $\mu_v^+ \perp \mu_v^-$, i.e. that $\mu_v^+$ and $\mu_v^-$ are mutually singular, then, by the uniqueness of the Jordan decomposition, $\mu_v^+ - \mu_v^-$ is the Jordan decomposition of $\mu_f$, i.e. $\mu_f^+ = \mu_v^+$ and $\mu_f^- = \mu_v^-$, and therefore we have, by definition of $\mu_f'$, that $\mu_v = \mu_f^+ + \mu_f^- = \mu_f'$, as desired. So, all that is left to show is that $\mu_v^+ \perp \mu_v^-$. Related posts A similar question was asked in this forum almost two years ago , but has remained unanswered.",,"['real-analysis', 'measure-theory', 'bounded-variation']"
33,"Hölder continuous functions are of 1st category in $C[0,1]$",Hölder continuous functions are of 1st category in,"C[0,1]","I'm trying to show that the Hölder continuous functions in $[0,1]$ are a set of first category in $C[0,1]$. Does it suffice to show that they are not an open subset of $C[0,1]$? Let $\varepsilon>0$ and $f\in C[0,1]$ be Hölder. Since each Hölder continuous function is uniformly continuous, does it suffice to show that there is a non-uniformly continuous function in the ball $B_{\varepsilon}(f)\subset C[0,1]$? Can such a function be constructed or can I only prove its existence?","I'm trying to show that the Hölder continuous functions in $[0,1]$ are a set of first category in $C[0,1]$. Does it suffice to show that they are not an open subset of $C[0,1]$? Let $\varepsilon>0$ and $f\in C[0,1]$ be Hölder. Since each Hölder continuous function is uniformly continuous, does it suffice to show that there is a non-uniformly continuous function in the ball $B_{\varepsilon}(f)\subset C[0,1]$? Can such a function be constructed or can I only prove its existence?",,"['real-analysis', 'general-topology', 'functional-analysis', 'baire-category', 'holder-spaces']"
34,"if $|f(n+1)-f(n)|\leq 2001$, $|g(n+1)-g(n)|\leq 2001$, $|(fg)(n+1)-(fg)(n)|\leq 2001$ then $\min\{f(n),g(n)\}$ is bounded","if , ,  then  is bounded","|f(n+1)-f(n)|\leq 2001 |g(n+1)-g(n)|\leq 2001 |(fg)(n+1)-(fg)(n)|\leq 2001 \min\{f(n),g(n)\}","The following question was proposed at MOP 2001 A function $f:\mathbb{N}\to\mathbb{N}$ is called cautious if $|f(n+1)-f(n)|\leq 2001$ for all $ n\in\mathbb{N} $. Suppose that $f,g,h$ are cautious functions such that $h(n)=f(n)g(n)$ for all $ n\in\mathbb{N} $. Prove that there is a constant $C$ such that $\min\{f(n),g(n)\}<C$ for all $n$. Note that it if $f(n)=g(n)$ for all $n$ we can choose $C=2001$. We can also suppose WLOG that each one of $|\sqrt{f(n+1)}-\sqrt{f(n)}|,|\sqrt{g(n+1)}-\sqrt{g(n)}|,|\sqrt{h(n+1)}-\sqrt{h(n)}|$ can be arbitrarily small. If, for example, $|\sqrt{f(n+1)}-\sqrt{f(n)}|>d>0$ for all $n$ just choose $C=(2001/d)^2$. However I don't know how to proceed in the general case. Any help would be appreciated, thanks.","The following question was proposed at MOP 2001 A function $f:\mathbb{N}\to\mathbb{N}$ is called cautious if $|f(n+1)-f(n)|\leq 2001$ for all $ n\in\mathbb{N} $. Suppose that $f,g,h$ are cautious functions such that $h(n)=f(n)g(n)$ for all $ n\in\mathbb{N} $. Prove that there is a constant $C$ such that $\min\{f(n),g(n)\}<C$ for all $n$. Note that it if $f(n)=g(n)$ for all $n$ we can choose $C=2001$. We can also suppose WLOG that each one of $|\sqrt{f(n+1)}-\sqrt{f(n)}|,|\sqrt{g(n+1)}-\sqrt{g(n)}|,|\sqrt{h(n+1)}-\sqrt{h(n)}|$ can be arbitrarily small. If, for example, $|\sqrt{f(n+1)}-\sqrt{f(n)}|>d>0$ for all $n$ just choose $C=(2001/d)^2$. However I don't know how to proceed in the general case. Any help would be appreciated, thanks.",,"['real-analysis', 'functions']"
35,Sturm-Liouville problem and periodic boundary conditions,Sturm-Liouville problem and periodic boundary conditions,,"I was wondering about this: I know that if a 1-d Sturm-Liouville operator is limit circle or limit point then the eigenvalues are simple ( so no degenerated spectrum). But in the case of periodic boundary conditions it is actually possible that two eigenvalues agree, that's why people talk about spectral gaps, I think. Now if I have a regular Sturm-Liouville problem, then my operator is l.c. at both end-points, right?-So my spectrum should be simple. Thus, I have troubles to understand how the fact that two eigenvalues may agree for periodic boundary conditions fits into this l.c. and l.p. picture?","I was wondering about this: I know that if a 1-d Sturm-Liouville operator is limit circle or limit point then the eigenvalues are simple ( so no degenerated spectrum). But in the case of periodic boundary conditions it is actually possible that two eigenvalues agree, that's why people talk about spectral gaps, I think. Now if I have a regular Sturm-Liouville problem, then my operator is l.c. at both end-points, right?-So my spectrum should be simple. Thus, I have troubles to understand how the fact that two eigenvalues may agree for periodic boundary conditions fits into this l.c. and l.p. picture?",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'spectral-theory']"
36,"Does there exist a function $f: \mathbb{R} \to \mathbb{R}$ that is differentiable only at $0$ and at $\frac{1}{n}$, $n \in \mathbb{N}$?","Does there exist a function  that is differentiable only at  and at , ?",f: \mathbb{R} \to \mathbb{R} 0 \frac{1}{n} n \in \mathbb{N},"How to determine the existence of the function $f: \mathbb{R} \rightarrow \mathbb{R}$, which is differentiable only at  $0$ and at $\frac{1}{n}$, $n \in \mathbb{N}$? It's more than enough to give an example but I have some difficulties with it. Or even if it doesn't exist: how to find a hint that may be useful?","How to determine the existence of the function $f: \mathbb{R} \rightarrow \mathbb{R}$, which is differentiable only at  $0$ and at $\frac{1}{n}$, $n \in \mathbb{N}$? It's more than enough to give an example but I have some difficulties with it. Or even if it doesn't exist: how to find a hint that may be useful?",,"['calculus', 'real-analysis', 'derivatives', 'examples-counterexamples']"
37,An alternative proof for sum of alternating series evaluates to $\frac{\pi}{4}\sec\left(\frac{a\pi}{4}\right)$,An alternative proof for sum of alternating series evaluates to,\frac{\pi}{4}\sec\left(\frac{a\pi}{4}\right),"How does one prove the given series? $$\sum_{n=0}^\infty\left(\frac{(-1)^n}{4n-a+2}+\frac{(-1)^n}{4n+a+2}\right)=\frac{\pi}{4}\sec\left(\frac{a\pi}{4}\right)$$ This series came up in xpaul's calculation during the process of answering my homework problem . I really appreciate his help for me but I am looking a method to prove the above series using a real analysis method because the link he cited ( Ron G's answer ) to help me to prove it is using the residue theorem. I worked for a while on this today but was unsuccessful.  $$\begin{align}\sum_{n=0}^\infty\left(\frac{(-1)^n}{4n-a+2}+\frac{(-1)^n}{4n+a+2}\right)&=4\sum_{n=0}^\infty(-1)^n\frac{2n+1}{(4n+2)^2-a^2}\\&=\sum_{n=0}^\infty(-1)^n\frac{2n+1}{(2n+1)^2-\left(\frac{a}{2}\right)^2}\end{align}$$ Comparing with Taylor series for secant, hyperbolic secant, or any other well known series forms but I could not get any of them to work,  perhaps someone else can? I would like a nice proof and avoiding residue method in order to complete my homework's answer. Would you help me? Any help would be appreciated. Thanks in advance.","How does one prove the given series? $$\sum_{n=0}^\infty\left(\frac{(-1)^n}{4n-a+2}+\frac{(-1)^n}{4n+a+2}\right)=\frac{\pi}{4}\sec\left(\frac{a\pi}{4}\right)$$ This series came up in xpaul's calculation during the process of answering my homework problem . I really appreciate his help for me but I am looking a method to prove the above series using a real analysis method because the link he cited ( Ron G's answer ) to help me to prove it is using the residue theorem. I worked for a while on this today but was unsuccessful.  $$\begin{align}\sum_{n=0}^\infty\left(\frac{(-1)^n}{4n-a+2}+\frac{(-1)^n}{4n+a+2}\right)&=4\sum_{n=0}^\infty(-1)^n\frac{2n+1}{(4n+2)^2-a^2}\\&=\sum_{n=0}^\infty(-1)^n\frac{2n+1}{(2n+1)^2-\left(\frac{a}{2}\right)^2}\end{align}$$ Comparing with Taylor series for secant, hyperbolic secant, or any other well known series forms but I could not get any of them to work,  perhaps someone else can? I would like a nice proof and avoiding residue method in order to complete my homework's answer. Would you help me? Any help would be appreciated. Thanks in advance.",,"['calculus', 'real-analysis', 'sequences-and-series', 'alternative-proof']"
38,Sums in $\mathbb N^3$,Sums in,\mathbb N^3,"Assume that $a_{n,m,k}\geq 0$ is a sequence, $n,m,k\in \mathbb N$, such that $$\sum_{n,m,k\in \mathbb N} a_{n,m,k}^2 <\infty$$ i.e. it is in $\ell^2 (\mathbb N^3)$. I want to prove the following: We can find some path(s) (in the grid of $\mathbb N^3$) escaping to   $\infty$, such that the weights $a_{n,m,k}$ of the points $(n,m,k)$ of   the path have finite sum. In other words, I'm claiming that we find a   path $\gamma $ on the grid $\mathbb N^3$ (with edges added), such that   $\gamma$ doesn't stay inside any ball, and such that   $$\sum_{a_{n,m,k}\in \gamma} a_{n,m,k} <+\infty$$ For instance, it would suffice to show that $\sum_{k} a_{n,m,k}<\infty $ for some fixed $n,m$ or $\sum_{m} a_{n,m,k}<\infty $ for some fixed $n,k$ or $\sum_{n} a_{n,m,k}<\infty $ for some fixed $m,k$ Do you think that such a statement is true? This is not possible for $\mathbb N^2$, since the sequence $a_{n,m}= \frac{1}{(n+m)\log(n+m)}$ provides a counterexample, namely it is square summable, but for fixed $n$ or $m$ the sums are infinite.","Assume that $a_{n,m,k}\geq 0$ is a sequence, $n,m,k\in \mathbb N$, such that $$\sum_{n,m,k\in \mathbb N} a_{n,m,k}^2 <\infty$$ i.e. it is in $\ell^2 (\mathbb N^3)$. I want to prove the following: We can find some path(s) (in the grid of $\mathbb N^3$) escaping to   $\infty$, such that the weights $a_{n,m,k}$ of the points $(n,m,k)$ of   the path have finite sum. In other words, I'm claiming that we find a   path $\gamma $ on the grid $\mathbb N^3$ (with edges added), such that   $\gamma$ doesn't stay inside any ball, and such that   $$\sum_{a_{n,m,k}\in \gamma} a_{n,m,k} <+\infty$$ For instance, it would suffice to show that $\sum_{k} a_{n,m,k}<\infty $ for some fixed $n,m$ or $\sum_{m} a_{n,m,k}<\infty $ for some fixed $n,k$ or $\sum_{n} a_{n,m,k}<\infty $ for some fixed $m,k$ Do you think that such a statement is true? This is not possible for $\mathbb N^2$, since the sequence $a_{n,m}= \frac{1}{(n+m)\log(n+m)}$ provides a counterexample, namely it is square summable, but for fixed $n$ or $m$ the sums are infinite.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
39,"Let $S$ be the Schwartz class. Show that if $f,g\in S$, then $fg\in S$ and $f*g\in S$, where $*$ denotes convolution.","Let  be the Schwartz class. Show that if , then  and , where  denotes convolution.","S f,g\in S fg\in S f*g\in S *","Let $S$ be the Schwartz class. Show that if $f,g\in S$, then $fg\in S$ and $f*g\in S$, where $*$ denotes convolution. To differentiate $fg$, we may apply Leibniz's rule ( http://en.wikipedia.org/wiki/General_Leibniz_rule ). And then maybe induct on the order of the derivative. Is there something useful that can be applied to differentiating $f*g$? I guess there's a product rule for convolution. But after using product rule, there's still a convolution sign.","Let $S$ be the Schwartz class. Show that if $f,g\in S$, then $fg\in S$ and $f*g\in S$, where $*$ denotes convolution. To differentiate $fg$, we may apply Leibniz's rule ( http://en.wikipedia.org/wiki/General_Leibniz_rule ). And then maybe induct on the order of the derivative. Is there something useful that can be applied to differentiating $f*g$? I guess there's a product rule for convolution. But after using product rule, there's still a convolution sign.",,"['real-analysis', 'analysis', 'measure-theory', 'convolution', 'schwartz-space']"
40,Uniqueness of Fourier transform in $L^1$,Uniqueness of Fourier transform in,L^1,"The Fourier transform of an $L^1$ function is defined by $$\hat{f}(y)=\int_\mathbb{R}f(x)e^{-ixy}dx$$ Is it true that for functions $f,g\in L^1$, if $\hat{f}=\hat{g}$, then $f=g$?","The Fourier transform of an $L^1$ function is defined by $$\hat{f}(y)=\int_\mathbb{R}f(x)e^{-ixy}dx$$ Is it true that for functions $f,g\in L^1$, if $\hat{f}=\hat{g}$, then $f=g$?",,"['real-analysis', 'fourier-analysis']"
41,Product of increasing functions is integrable in two dimensions,Product of increasing functions is integrable in two dimensions,,"We say $f:[0,1]\rightarrow\mathbb{R}$ is increasing if $f(x_1)\le f(x_2)$ whenever $x_1<x_2$. If $f,g:[0,1]\rightarrow \mathbb{R}$ are increasing and non-negative, show that the function $h(x,y)=f(x)g(y)$ is integrable over $[0,1]^2$. I'm using the Darboux definition of integration, so I want to prove that for any $\epsilon>0$, there exists a partition $P$ of $Q$ such that $U(f,P)-L(f,P)<\epsilon$. Equivalently, there exists a partition $P$ of $Q$ such that $$\left|\sum_Rv(R)(M_R(f)-m_R(f))\right|<\epsilon,$$ where $M_R(f)$ is the supremum of the values of $f$ inside the rectangle $R$, and $m_R(f)$ is the corresponding value for infimum. (Here, $R$ ranges over all subrectangles formed by the partition $P$.) Toward that end, I tried to take the partition $P$ to be $[0,\dfrac1n,\dfrac2n,\ldots,1]\times[0,\dfrac1n,\dfrac2n,\ldots,1]$. Since the function $h(x,y)$ is increasing in $x$ and $y$, the maximum of any square is attained at the top-right corner, and the minimum at the bottom-left corner. Therefore, $\left|\sum_Rv(R)(M_R(f)-m_R(f))\right|$ is equal to $$\dfrac{1}{n^2}\left(\sum_{i=1}^{n}f(\dfrac in)g(1)+\sum_{i=1}^{n-1}f(1)g(\dfrac in)-\sum_{i=0}^{n-1}f(\dfrac in)g(0)-\sum_{i=1}^{n-1}f(0)g(\dfrac in)\right)$$ I'm not sure if this is the right partition to take. Does this sum approach $0$ as $n\rightarrow\infty$?","We say $f:[0,1]\rightarrow\mathbb{R}$ is increasing if $f(x_1)\le f(x_2)$ whenever $x_1<x_2$. If $f,g:[0,1]\rightarrow \mathbb{R}$ are increasing and non-negative, show that the function $h(x,y)=f(x)g(y)$ is integrable over $[0,1]^2$. I'm using the Darboux definition of integration, so I want to prove that for any $\epsilon>0$, there exists a partition $P$ of $Q$ such that $U(f,P)-L(f,P)<\epsilon$. Equivalently, there exists a partition $P$ of $Q$ such that $$\left|\sum_Rv(R)(M_R(f)-m_R(f))\right|<\epsilon,$$ where $M_R(f)$ is the supremum of the values of $f$ inside the rectangle $R$, and $m_R(f)$ is the corresponding value for infimum. (Here, $R$ ranges over all subrectangles formed by the partition $P$.) Toward that end, I tried to take the partition $P$ to be $[0,\dfrac1n,\dfrac2n,\ldots,1]\times[0,\dfrac1n,\dfrac2n,\ldots,1]$. Since the function $h(x,y)$ is increasing in $x$ and $y$, the maximum of any square is attained at the top-right corner, and the minimum at the bottom-left corner. Therefore, $\left|\sum_Rv(R)(M_R(f)-m_R(f))\right|$ is equal to $$\dfrac{1}{n^2}\left(\sum_{i=1}^{n}f(\dfrac in)g(1)+\sum_{i=1}^{n-1}f(1)g(\dfrac in)-\sum_{i=0}^{n-1}f(\dfrac in)g(0)-\sum_{i=1}^{n-1}f(0)g(\dfrac in)\right)$$ I'm not sure if this is the right partition to take. Does this sum approach $0$ as $n\rightarrow\infty$?",,['real-analysis']
42,How prove that $xyz+\sqrt{x^2y^2+y^2z^2+x^2z^2}\ge \frac{4}{3}\sqrt{xyz(x+y+z)}$,How prove that,xyz+\sqrt{x^2y^2+y^2z^2+x^2z^2}\ge \frac{4}{3}\sqrt{xyz(x+y+z)},"let $x,y,z>0$ ,and such that $x^2+y^2+z^2=1$ ,prove that $$xyz+\sqrt{x^2y^2+y^2z^2+x^2z^2}\ge \dfrac{4}{3}\sqrt{xyz(x+y+z)}$$ Does this have a nice solution? Thank you everyone.","let ,and such that ,prove that Does this have a nice solution? Thank you everyone.","x,y,z>0 x^2+y^2+z^2=1 xyz+\sqrt{x^2y^2+y^2z^2+x^2z^2}\ge \dfrac{4}{3}\sqrt{xyz(x+y+z)}","['real-analysis', 'inequality', 'radicals', 'substitution', 'uvw']"
43,Do the sequences from the ratio and root tests converge to the same limit? [duplicate],Do the sequences from the ratio and root tests converge to the same limit? [duplicate],,"This question already has an answer here : Inequality involving $\limsup$ and $\liminf$: $ \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n)$ (1 answer) Closed 8 years ago . For example, if we have: $$\sum_{n=1}^{\infty}a_n$$ Where the ratio test is satisfied. That is $\exists L$ s.t. $$\lim_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right| = L < 1$$ Does this mean that for the same $L$ that the root test's terms converge as well? That is, is it true that: $$\lim_{n \to \infty} \left| a_n^{1/n} \right| = L$$ This is for a homework problem, but the homework is not to prove the general case; I was just wondering if this was true or not.","This question already has an answer here : Inequality involving $\limsup$ and $\liminf$: $ \liminf(a_{n+1}/a_n) \le \liminf a_n^{1/n} \le \limsup a_n^{1/n} \le \limsup(a_{n+1}/a_n)$ (1 answer) Closed 8 years ago . For example, if we have: $$\sum_{n=1}^{\infty}a_n$$ Where the ratio test is satisfied. That is $\exists L$ s.t. $$\lim_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right| = L < 1$$ Does this mean that for the same $L$ that the root test's terms converge as well? That is, is it true that: $$\lim_{n \to \infty} \left| a_n^{1/n} \right| = L$$ This is for a homework problem, but the homework is not to prove the general case; I was just wondering if this was true or not.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
44,Prove $\left(\frac{n+1}{\text{e}}\right)^n<n!<\text{e}\left(\frac{n+1}{\text{e}}\right)^{n+1}$ [closed],Prove  [closed],\left(\frac{n+1}{\text{e}}\right)^n<n!<\text{e}\left(\frac{n+1}{\text{e}}\right)^{n+1},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How to prove this inequality? $$\left(\frac{n+1}{\text{e}}\right)^n<n!<\text{e}\left(\frac{n+1}{\text{e}}\right)^{n+1}$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How to prove this inequality? $$\left(\frac{n+1}{\text{e}}\right)^n<n!<\text{e}\left(\frac{n+1}{\text{e}}\right)^{n+1}$$",,"['calculus', 'real-analysis', 'inequality']"
45,Sequences of simple functions converging to $f$,Sequences of simple functions converging to,f,"Proposition Let $f$ be a bounded measurable function on $E$.   Show that there are $\{\phi_n(x)\}$ and $\{\psi_n(x)\}$ - sequences of simple functions on $E$ such that $\{\phi_n(x)\}$ is increasing and $\{\psi_n(x)\}$ is decreasing and each of these converge to $f$ uniformly on $E$. By the simple approximation lemma I know that both of the mentioned sequences exists such that $\phi_n(x) \leq f \leq \psi_n(x)$ and $\psi_n(x)-\phi_n(x) < \frac{1}{n}$ for each $n \in \mathbb{N}$. Using uniform continuity: for some $\varepsilon > 0$ there exists an $n \geq N$ such that $|\phi_n(x)-f| < \frac{1}{n}$.  Since simple functions take on a finite number of values, it is reasonable to take a $\max$.  So I let $\phi(x)=\max\{\phi_n(x)\}$. I guess I'm not sure how to pull together the lemma and the definition of simple function.","Proposition Let $f$ be a bounded measurable function on $E$.   Show that there are $\{\phi_n(x)\}$ and $\{\psi_n(x)\}$ - sequences of simple functions on $E$ such that $\{\phi_n(x)\}$ is increasing and $\{\psi_n(x)\}$ is decreasing and each of these converge to $f$ uniformly on $E$. By the simple approximation lemma I know that both of the mentioned sequences exists such that $\phi_n(x) \leq f \leq \psi_n(x)$ and $\psi_n(x)-\phi_n(x) < \frac{1}{n}$ for each $n \in \mathbb{N}$. Using uniform continuity: for some $\varepsilon > 0$ there exists an $n \geq N$ such that $|\phi_n(x)-f| < \frac{1}{n}$.  Since simple functions take on a finite number of values, it is reasonable to take a $\max$.  So I let $\phi(x)=\max\{\phi_n(x)\}$. I guess I'm not sure how to pull together the lemma and the definition of simple function.",,['real-analysis']
46,Demonstrate Cantor set contains points other than interval endpoints.,Demonstrate Cantor set contains points other than interval endpoints.,,"I am stumped on a problem in a text book.  This is not homework.  I'm a physicist doing some self study on Lebesgue integrals and Fourier theory.  I'm starting with the basics, and reading up on measure theory. The problem is to show that $\frac{1}{4}$ is an element of the Cantor set.  My first thought would be t0 find a ternary expansion consisting of only 0's and 2's. However, what I'm having trouble with is imagining that anything remains following the infinite intersection creating the Cantor set other than the interval endpoints.  I imagine that if I pick a real number not lying on some interval endpoint I could find an $N$ large enough that the portion of the real line the number belongs to would be deleted.  I'd like to see why this argument breaks down.","I am stumped on a problem in a text book.  This is not homework.  I'm a physicist doing some self study on Lebesgue integrals and Fourier theory.  I'm starting with the basics, and reading up on measure theory. The problem is to show that $\frac{1}{4}$ is an element of the Cantor set.  My first thought would be t0 find a ternary expansion consisting of only 0's and 2's. However, what I'm having trouble with is imagining that anything remains following the infinite intersection creating the Cantor set other than the interval endpoints.  I imagine that if I pick a real number not lying on some interval endpoint I could find an $N$ large enough that the portion of the real line the number belongs to would be deleted.  I'd like to see why this argument breaks down.",,['real-analysis']
47,Solve the equation: $e^x=mx^2$,Solve the equation:,e^x=mx^2,"I need to find out the maximum possible number of real roots of the equation: $$e^x=mx^2$$ where m is a real parameter. I'm interested in some easy approaches. Moreover, is it possible to solve it without using derivatives at all? Thanks.","I need to find out the maximum possible number of real roots of the equation: $$e^x=mx^2$$ where m is a real parameter. I'm interested in some easy approaches. Moreover, is it possible to solve it without using derivatives at all? Thanks.",,['real-analysis']
48,"Locally lipschitz on $[a,b]$ implies lipschitz",Locally lipschitz on  implies lipschitz,"[a,b]","Suppose a function $f:\mathbb{R}\rightarrow\mathbb{R}$ is locally Lipschitz.  Prove that $f$ is Lipschitz on $[a,b]$ . Here is what I have so far: Let $[a, b]$ be some closed, bounded interval. Since f is locally Lipschitz, for each $x\in[a; b]$ , we may find some $U_x$ and some $M_x$ such that $|f(y)-f(z)| < M|x-y|$ .  Let $\mathcal{U}$ denote the collection of all such open neighborhoods $U_x$ . Then $\mathcal{U}$ is an open cover of $[a, b]$ . By the Heine-Borel Theorem, $[a, b]$ is compact, so $\mathcal{U}$ has a finite subcover, $\mathcal{V}$ . Label the members of $\mathcal{V}$ as $U_{x_1}, U_{x_2},\ldots,U_{x_n}$ . Then $[a, b]$ and for each $U_{x_k}$ , we associate a corresponding $M_{x_k}$ such that if $y, z\in U_{x_k}$ , then $|f(y)-f(z)|< M_{x_k}|y-z|$ .  Let $M = \max\{M_{x_1},\ldots,M_{x_n}\}$ .  Let $y$ and $z$ be some points in $[a, b]$ with $z < y$ . If both $y$ and $z$ lie in the same neighborhood in $\mathcal{V}$ , then we are done. This is as far as I got.  I do not know how to handle the case when $y$ and $z$ are in different neighborhoods.","Suppose a function is locally Lipschitz.  Prove that is Lipschitz on . Here is what I have so far: Let be some closed, bounded interval. Since f is locally Lipschitz, for each , we may find some and some such that .  Let denote the collection of all such open neighborhoods . Then is an open cover of . By the Heine-Borel Theorem, is compact, so has a finite subcover, . Label the members of as . Then and for each , we associate a corresponding such that if , then .  Let .  Let and be some points in with . If both and lie in the same neighborhood in , then we are done. This is as far as I got.  I do not know how to handle the case when and are in different neighborhoods.","f:\mathbb{R}\rightarrow\mathbb{R} f [a,b] [a, b] x\in[a; b] U_x M_x |f(y)-f(z)| < M|x-y| \mathcal{U} U_x \mathcal{U} [a, b] [a, b] \mathcal{U} \mathcal{V} \mathcal{V} U_{x_1}, U_{x_2},\ldots,U_{x_n} [a, b] U_{x_k} M_{x_k} y, z\in U_{x_k} |f(y)-f(z)|< M_{x_k}|y-z| M = \max\{M_{x_1},\ldots,M_{x_n}\} y z [a, b] z < y y z \mathcal{V} y z",['real-analysis']
49,Is a uniformly continuous function vanishing at $0$ bounded by $a|x|+c$?,Is a uniformly continuous function vanishing at  bounded by ?,0 a|x|+c,"Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be uniformly continuous with $g(0)=0,c\geq 0, c \in \mathbb{R}$. Show: $$\exists a\geq 0 \in \mathbb{R}: \forall x \in \mathbb{R}: |g(x)| \leq a \cdot |x|+c$$ I could also say $g(x) \in \mathcal{O}(x)$. Notes: I could not make up any counterexample so I guess it could be true, all uniformly continuous functions I know grow too slowly. My approach: Given $\epsilon > 0$, we have that: $$\exists \delta(\epsilon): |x-y|<\delta=> |g(x)-g(y)|<\epsilon$$ because of the continuity of $g$. Now choose $n=\text{max}\{n \in \mathbb{N}: (n-1)\delta/2\leq|x|\}$. Obviously, such an $n$ exists, and $n > 0$. We also easily see that an upper bound for $n$ is $n \leq \frac{2}{\delta}|x|+1$. Now we use this to separate $|x|$ into $n-1$ distinct parts of size $s<\delta/2$, and the last part which is smaller than $\delta$ : $$|x|=|x_1-x_0|+|x_2-x_1|+|x_3-x_2|+...+|x_n-x_{n-1}| < (n-1)\delta/2 + \delta = (n+1)\delta/2.$$ $$\begin{align} \Rightarrow  |g(x)| & =|g(x_1)-g(x_0)+g(x_2)-g(x_1)+g(x_3)-g(x_2)+...+g(x_n)-g(x_{n-1})| \\ & \leq |g(x_1)-g(x_0)|+|g(x_2)-g(x_1)|+|g(x_3)-g(x_2)|+...+|g(x_n)-g(x_{n-1})| \\ & \lt n \cdot \epsilon \leq (\frac{2}{\delta}|x|+1) \cdot \epsilon = \frac{2\epsilon}{\delta} \cdot |x|+\epsilon \end{align}$$ So we can see that the constant $c$ we were given can be set as the $\epsilon := c$, and that was also the reason why generally speaking $c>0$. Then we can choose $a := \frac{2\epsilon}{\delta}$, as our $\delta$ only depends on the $\epsilon$, and we have that $|g(x)| \leq a \cdot |x| + c$ for $c > 0$. $\quad \square$","Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be uniformly continuous with $g(0)=0,c\geq 0, c \in \mathbb{R}$. Show: $$\exists a\geq 0 \in \mathbb{R}: \forall x \in \mathbb{R}: |g(x)| \leq a \cdot |x|+c$$ I could also say $g(x) \in \mathcal{O}(x)$. Notes: I could not make up any counterexample so I guess it could be true, all uniformly continuous functions I know grow too slowly. My approach: Given $\epsilon > 0$, we have that: $$\exists \delta(\epsilon): |x-y|<\delta=> |g(x)-g(y)|<\epsilon$$ because of the continuity of $g$. Now choose $n=\text{max}\{n \in \mathbb{N}: (n-1)\delta/2\leq|x|\}$. Obviously, such an $n$ exists, and $n > 0$. We also easily see that an upper bound for $n$ is $n \leq \frac{2}{\delta}|x|+1$. Now we use this to separate $|x|$ into $n-1$ distinct parts of size $s<\delta/2$, and the last part which is smaller than $\delta$ : $$|x|=|x_1-x_0|+|x_2-x_1|+|x_3-x_2|+...+|x_n-x_{n-1}| < (n-1)\delta/2 + \delta = (n+1)\delta/2.$$ $$\begin{align} \Rightarrow  |g(x)| & =|g(x_1)-g(x_0)+g(x_2)-g(x_1)+g(x_3)-g(x_2)+...+g(x_n)-g(x_{n-1})| \\ & \leq |g(x_1)-g(x_0)|+|g(x_2)-g(x_1)|+|g(x_3)-g(x_2)|+...+|g(x_n)-g(x_{n-1})| \\ & \lt n \cdot \epsilon \leq (\frac{2}{\delta}|x|+1) \cdot \epsilon = \frac{2\epsilon}{\delta} \cdot |x|+\epsilon \end{align}$$ So we can see that the constant $c$ we were given can be set as the $\epsilon := c$, and that was also the reason why generally speaking $c>0$. Then we can choose $a := \frac{2\epsilon}{\delta}$, as our $\delta$ only depends on the $\epsilon$, and we have that $|g(x)| \leq a \cdot |x| + c$ for $c > 0$. $\quad \square$",,"['real-analysis', 'analysis']"
50,Differentiability of Convolutions,Differentiability of Convolutions,,"Let $f(x) \in L^p(\mathbb{R})$ and $K \in C^m(\mathbb{R})$. Can I then say that $(f \ast K) (x) = \int_{\mathbb{R}} f(t) K(x-t) dt$ is in $C^m$? I know that this is true if $K$ has compact support, but I was wondering if it is possible to have a stronger result (perhaps $K$ vanishing at $\infty$?).","Let $f(x) \in L^p(\mathbb{R})$ and $K \in C^m(\mathbb{R})$. Can I then say that $(f \ast K) (x) = \int_{\mathbb{R}} f(t) K(x-t) dt$ is in $C^m$? I know that this is true if $K$ has compact support, but I was wondering if it is possible to have a stronger result (perhaps $K$ vanishing at $\infty$?).",,['real-analysis']
51,"Completeness of $X$ with normal distribution $N(\theta,\theta^2)$ with equal mean and standard deviation: Integral of scale of a function",Completeness of  with normal distribution  with equal mean and standard deviation: Integral of scale of a function,"X N(\theta,\theta^2)","Suppose that $f:\mathbb{R} \to \mathbb{R}$ is a Borel function such that, for every $a > 0$ , we always have: $$\begin{equation*} \int_{\mathbb{R}}f(ax)\exp \left(-\frac{1}{2}(x-1)^2\right)dx =0 \end{equation*}$$ Does it follow that $f = 0$ almost surely ? Does the answer change if we limit the attention only to $f \in L^{\infty}$ ? The background of the question is that, suppose $X$ obeys the distribution $N(\theta, \theta^2)$ for some $\theta> 0$ and we wish to determine if $X$ is a (boundly) completes statistic for $\theta$ . Expanding the definition out with a change of variable will give the statement of the problem above. I am looking for an analysis solution . Intuitively, the exponential centers at $1$ and the scale is not centered at $1$ . A counterexample would be surprising but I cannot prove it.","Suppose that is a Borel function such that, for every , we always have: Does it follow that almost surely ? Does the answer change if we limit the attention only to ? The background of the question is that, suppose obeys the distribution for some and we wish to determine if is a (boundly) completes statistic for . Expanding the definition out with a change of variable will give the statement of the problem above. I am looking for an analysis solution . Intuitively, the exponential centers at and the scale is not centered at . A counterexample would be surprising but I cannot prove it.","f:\mathbb{R} \to \mathbb{R} a > 0 \begin{equation*}
\int_{\mathbb{R}}f(ax)\exp \left(-\frac{1}{2}(x-1)^2\right)dx =0
\end{equation*} f = 0 f \in L^{\infty} X N(\theta, \theta^2) \theta> 0 X \theta 1 1","['real-analysis', 'probability', 'functional-analysis', 'analysis', 'statistics']"
52,Test the convergence of the series $1+\frac{2^2}{3^2}+\frac{2^2.4^2}{3^2.5^2}+\frac{2^2.4^2.6^2}{3^2.5^2.7^2}+\cdots$,Test the convergence of the series,1+\frac{2^2}{3^2}+\frac{2^2.4^2}{3^2.5^2}+\frac{2^2.4^2.6^2}{3^2.5^2.7^2}+\cdots,"Test the convergence of the series $$1+\frac{2^2}{3^2}+\frac{2^2.4^2}{3^2.5^2}+\frac{2^2.4^2.6^2}{3^2.5^2.7^2}+\cdots$$ I tried solving this problem using Gauss's Test which says, If $\sum u_n$ is a series of positive real numbers, such that, $\frac{u_n}{u_{n+1}}=1+\frac{a}{n}+\frac{b_n}{n^p},$ where, $p>1$ and $b_n$ is a bounded sequence,then,  the series converge if $a>1$ , else if $a\leq 1$ the series diverge. So, the series given here, is $1+\frac{2^2}{3^2}+\frac{2^2.4^2}{3^2.5^2}+\frac{2^2.4^2.6^2}{3^2.5^2.7^2}+\cdots$ . Ignoring the first term of the series, we consider $\sum u_n$ where, $u_n=\frac{2^2.4^2.6^2...(2n)^2}{3^2.5^2.7^2.(2n+1)^2},\forall n\in\Bbb N.$ Then we have, $u_{n+1}=\frac{2^2.4^2.6^2...(2n+2)^2}{3^2.5^2.7^2.(2n+3)^2}.$ We note, that $\lim\frac{u_n}{u_{n+1}}=\lim\frac{(2n+3)^2}{(2n+2)^2}=1+\frac 1{n+1}+\frac{\frac 14}{(n+1)^2}.$ From Gauss's Test, we observe, here $p=2\gt 1$ and $b_n=\frac 14$ is a constant and hence a bounded sequence. Also, $a=1,$ due to which the given series is divergent. I hope that my answer is correct. However, I don't seem to have an explanation for why I could apply Gauss's Test. This is because, Gauss's Test, say, that $\frac{u_n}{u_{n+1}}$ should be of the following form : $$1+\frac{a}{\color{red}{n}}+\frac{b_n}{\color{red}{n^p}}.$$ But in my solution, $\frac{u_n}{u_{n+1}}$ came out to be of the form : $$1+\frac{a}{\color{green}{n+1}}+\frac{b_n}{\color{green}{(n+1)^p}},$$ (,where $p=2,b_n=\frac 14, a=1$ ). But then the denominators were of the form $(n+1)$ and not $n$ as was ""described"" in the test. So, does this create any problem with my solution ? If not, then why it does not matter ?","Test the convergence of the series I tried solving this problem using Gauss's Test which says, If is a series of positive real numbers, such that, where, and is a bounded sequence,then,  the series converge if , else if the series diverge. So, the series given here, is . Ignoring the first term of the series, we consider where, Then we have, We note, that From Gauss's Test, we observe, here and is a constant and hence a bounded sequence. Also, due to which the given series is divergent. I hope that my answer is correct. However, I don't seem to have an explanation for why I could apply Gauss's Test. This is because, Gauss's Test, say, that should be of the following form : But in my solution, came out to be of the form : (,where ). But then the denominators were of the form and not as was ""described"" in the test. So, does this create any problem with my solution ? If not, then why it does not matter ?","1+\frac{2^2}{3^2}+\frac{2^2.4^2}{3^2.5^2}+\frac{2^2.4^2.6^2}{3^2.5^2.7^2}+\cdots \sum u_n \frac{u_n}{u_{n+1}}=1+\frac{a}{n}+\frac{b_n}{n^p}, p>1 b_n a>1 a\leq 1 1+\frac{2^2}{3^2}+\frac{2^2.4^2}{3^2.5^2}+\frac{2^2.4^2.6^2}{3^2.5^2.7^2}+\cdots \sum u_n u_n=\frac{2^2.4^2.6^2...(2n)^2}{3^2.5^2.7^2.(2n+1)^2},\forall n\in\Bbb N. u_{n+1}=\frac{2^2.4^2.6^2...(2n+2)^2}{3^2.5^2.7^2.(2n+3)^2}. \lim\frac{u_n}{u_{n+1}}=\lim\frac{(2n+3)^2}{(2n+2)^2}=1+\frac 1{n+1}+\frac{\frac 14}{(n+1)^2}. p=2\gt 1 b_n=\frac 14 a=1, \frac{u_n}{u_{n+1}} 1+\frac{a}{\color{red}{n}}+\frac{b_n}{\color{red}{n^p}}. \frac{u_n}{u_{n+1}} 1+\frac{a}{\color{green}{n+1}}+\frac{b_n}{\color{green}{(n+1)^p}}, p=2,b_n=\frac 14, a=1 (n+1) n","['real-analysis', 'sequences-and-series', 'solution-verification']"
53,Topology basis consisting of convex sets in metric spaces,Topology basis consisting of convex sets in metric spaces,,"Let $(X,d)$ be a metric space. For all points $x,y \in X$ we define the metric segment between them as the following set: $$\left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \}$$ We then say that a set $S\subseteq X$ is convex if for all $x,y \in S$ it holds true that $\left [ x,y \right ] \subseteq S$ . Denote by $\tau$ the topology on $X$ induced by the metric $d$ . My question is does there exist a family $\mathcal{B}\subseteq \tau$ of convex sets such that $\mathcal{B}$ is a basis for the topology $\tau$ ? It should be noted that open and closed balls in metric spaces are not necessarily convex sets. Also, arbitrary intersection of convex sets in metric spaces is a convex set. Thus, we can define convex hulls.","Let be a metric space. For all points we define the metric segment between them as the following set: We then say that a set is convex if for all it holds true that . Denote by the topology on induced by the metric . My question is does there exist a family of convex sets such that is a basis for the topology ? It should be noted that open and closed balls in metric spaces are not necessarily convex sets. Also, arbitrary intersection of convex sets in metric spaces is a convex set. Thus, we can define convex hulls.","(X,d) x,y \in X \left [ x,y \right ] =  \left \{ z \in X : d(x,z)+d(z,y)=d(x,y)\right \} S\subseteq X x,y \in S \left [ x,y \right ] \subseteq S \tau X d \mathcal{B}\subseteq \tau \mathcal{B} \tau","['real-analysis', 'general-topology', 'metric-spaces', 'convexity-spaces']"
54,Is $\int_{\mathbb{R}^2} e^{-u} \Delta u < \infty$?,Is ?,\int_{\mathbb{R}^2} e^{-u} \Delta u < \infty,"Question. Let $u : \mathbb{R}^2 \to \mathbb{R}$ be a smooth function such that $$ u(x, y) > 0 \quad\text{and}\quad \Delta u(x, y) > 0 $$ for all $(x, y) \in \mathbb{R}^2$ , where $\Delta := \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}$ is the Laplacian. Is the integral $$ \int_{-\infty}^\infty\int_{-\infty}^\infty e^{-u(x,y)} \Delta u(x, y) \; dx\, dy $$ finite? I tried to no avail to find a counterexample where $u$ is rotationally symmetric, i.e. $u(x, y) = f(x^2 + y^2)$ for some $f$ . The problem reduces to the following: Subquestion. Let $f : [0, \infty) \to \mathbb{R}$ be a smooth function such that $f(t) > 0$ and $t f''(t) + f'(t) > 0$ for all $t \in [0, \infty)$ . Is $$ \int_0^\infty e^{-f(t)} (t f''(t) + f'(t)) dt $$ finite? But it is still not clear to me whether or not this initegral will be finite under the given conditions. I tried many different $f$ , but the integral always converge.","Question. Let be a smooth function such that for all , where is the Laplacian. Is the integral finite? I tried to no avail to find a counterexample where is rotationally symmetric, i.e. for some . The problem reduces to the following: Subquestion. Let be a smooth function such that and for all . Is finite? But it is still not clear to me whether or not this initegral will be finite under the given conditions. I tried many different , but the integral always converge.","u : \mathbb{R}^2 \to \mathbb{R} 
u(x, y) > 0 \quad\text{and}\quad \Delta u(x, y) > 0
 (x, y) \in \mathbb{R}^2 \Delta := \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} 
\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-u(x,y)} \Delta u(x, y) \; dx\, dy
 u u(x, y) = f(x^2 + y^2) f f : [0, \infty) \to \mathbb{R} f(t) > 0 t f''(t) + f'(t) > 0 t \in [0, \infty) 
\int_0^\infty e^{-f(t)} (t f''(t) + f'(t)) dt
 f","['real-analysis', 'integration']"
55,For a differentiable function $f$ show that $\{x:\limsup_{y\to x}|f'(y)|<\infty\} $ is open and dense in $\mathbb R$,For a differentiable function  show that  is open and dense in,f \{x:\limsup_{y\to x}|f'(y)|<\infty\}  \mathbb R,"As the title says, given a differentiable function $f: \ \mathbb R \to \mathbb R$ define $$E=\{x:\limsup_{y\to x}|f'(y)|<\infty\} $$ and show that $E $ is open and dense in $\mathbb R$ . Here $\limsup_{y\to x}|f'(y)| $ is defined as $\lim_{\epsilon \to 0} (\sup \{|f'(y): y \in B_{\epsilon } (x) \setminus \{x \} \}) $ . I suppose this should be solved with an application of Baire's category theorem but I'm quite at loss on how to proceed! Any help would be much appreciated!","As the title says, given a differentiable function define and show that is open and dense in . Here is defined as . I suppose this should be solved with an application of Baire's category theorem but I'm quite at loss on how to proceed! Any help would be much appreciated!",f: \ \mathbb R \to \mathbb R E=\{x:\limsup_{y\to x}|f'(y)|<\infty\}  E  \mathbb R \limsup_{y\to x}|f'(y)|  \lim_{\epsilon \to 0} (\sup \{|f'(y): y \in B_{\epsilon } (x) \setminus \{x \} \}) ,"['real-analysis', 'general-topology', 'baire-category']"
56,Applying the Fourier transform to Maxwell's equations,Applying the Fourier transform to Maxwell's equations,,"I have the following Maxwell's equations: $$\nabla \times \mathbf{h} = \mathbf{j} + \epsilon_0 \dfrac{\partial{\mathbf{e}}}{\partial{t}} + \dfrac{\partial{\mathbf{p}}}{\partial{t}},$$ $$\nabla \times \mathbf{e} = - \mu_0 \dfrac{\partial{\mathbf{h}}}{\partial{t}}$$ According to my textbook (provided as a passing comment by the author), the Fourier transform, $$F(\omega) = \int_{-\infty}^\infty f(t) e^{-j \omega t} \ dt,$$ can be applied to Maxwell's equations to go from the time domain $t$ to the angular frequency domain $\omega$ . My understanding is that this would take us from $$\nabla \times \mathbf{h} = \mathbf{j} + \epsilon_0 \dfrac{\partial{\mathbf{e}}}{\partial{t}} + \dfrac{\partial{\mathbf{p}}}{\partial{t}}$$ to $$\nabla \times \mathbf{H} = \mathbf{J} + j \omega \epsilon_0 \mathbf{E} + j \omega \mathbf{P} = \mathbf{J} + j \omega \mathbf{D}$$ and $$\nabla \times \mathbf{e} = -\mu_0 \dfrac{\partial{\mathbf{h}}}{\partial{t}}$$ to $$\nabla \times \mathbf{E} = - j \omega \mu_0 \mathbf{H}$$ I want to understand how to do this for the learning experience. I have experience with the Laplace transform but not the Fourier transform, and I cannot find anything online that goes through the steps of the transformation.  Do we just apply the Fourier transform $F(\omega)$ to every term in Maxwell's equations? How do we deal with the presence of vector terms in the context of such an integration? For instance, we have $$\nabla \times \mathbf{h} = \mathbf{\hat{i}} (\partial_y h_z - \partial_z h_y) - \mathbf{\hat{j}} (\partial_x h_z - \partial_z h_x) + \mathbf{\hat{k}} (\partial_x h_y - \partial_y h_x)$$ I would greatly appreciate it if people could please take the time to clarify this.","I have the following Maxwell's equations: According to my textbook (provided as a passing comment by the author), the Fourier transform, can be applied to Maxwell's equations to go from the time domain to the angular frequency domain . My understanding is that this would take us from to and to I want to understand how to do this for the learning experience. I have experience with the Laplace transform but not the Fourier transform, and I cannot find anything online that goes through the steps of the transformation.  Do we just apply the Fourier transform to every term in Maxwell's equations? How do we deal with the presence of vector terms in the context of such an integration? For instance, we have I would greatly appreciate it if people could please take the time to clarify this.","\nabla \times \mathbf{h} = \mathbf{j} + \epsilon_0 \dfrac{\partial{\mathbf{e}}}{\partial{t}} + \dfrac{\partial{\mathbf{p}}}{\partial{t}}, \nabla \times \mathbf{e} = - \mu_0 \dfrac{\partial{\mathbf{h}}}{\partial{t}} F(\omega) = \int_{-\infty}^\infty f(t) e^{-j \omega t} \ dt, t \omega \nabla \times \mathbf{h} = \mathbf{j} + \epsilon_0 \dfrac{\partial{\mathbf{e}}}{\partial{t}} + \dfrac{\partial{\mathbf{p}}}{\partial{t}} \nabla \times \mathbf{H} = \mathbf{J} + j \omega \epsilon_0 \mathbf{E} + j \omega \mathbf{P} = \mathbf{J} + j \omega \mathbf{D} \nabla \times \mathbf{e} = -\mu_0 \dfrac{\partial{\mathbf{h}}}{\partial{t}} \nabla \times \mathbf{E} = - j \omega \mu_0 \mathbf{H} F(\omega) \nabla \times \mathbf{h} = \mathbf{\hat{i}} (\partial_y h_z - \partial_z h_y) - \mathbf{\hat{j}} (\partial_x h_z - \partial_z h_x) + \mathbf{\hat{k}} (\partial_x h_y - \partial_y h_x)","['real-analysis', 'integration', 'multivariable-calculus', 'fourier-transform', 'electromagnetism']"
57,Find $\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}$,Find,\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2},Find $$M:=\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}$$ There's a solution here that uses complex numbers which I didn't understand and I was wondering if the following is also a correct method. My proposed solution $$\begin{align} &\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}\\ =&\sum_{n=1}^{\infty}\tan^{-1}\frac{(1+n)+(1-n)}{1-(1+n)(1-n)}\\ =&\sum_{n=1}^{\infty}(\tan^{-1}(1+n)+\tan^{-1}(1-n))\\ =&\sum_{n=1}^{\infty}(\tan^{-1}(n+1)-\tan^{-1}(n-1)) \end{align} $$ And this implies $$M=\lim_{m\to\infty}(\tan^{-1}(m+1)+\tan^{-1}m-\tan^{-1}1-\tan^{-1}0)=\frac{3\pi}{4}$$,Find There's a solution here that uses complex numbers which I didn't understand and I was wondering if the following is also a correct method. My proposed solution And this implies,"M:=\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2} \begin{align}
&\sum_{n=1}^{\infty}\tan^{-1}\frac{2}{n^2}\\
=&\sum_{n=1}^{\infty}\tan^{-1}\frac{(1+n)+(1-n)}{1-(1+n)(1-n)}\\
=&\sum_{n=1}^{\infty}(\tan^{-1}(1+n)+\tan^{-1}(1-n))\\
=&\sum_{n=1}^{\infty}(\tan^{-1}(n+1)-\tan^{-1}(n-1))
\end{align}
 M=\lim_{m\to\infty}(\tan^{-1}(m+1)+\tan^{-1}m-\tan^{-1}1-\tan^{-1}0)=\frac{3\pi}{4}","['real-analysis', 'sequences-and-series', 'trigonometry', 'proof-verification', 'summation']"
58,"If $f$ derivable on $[a,b]$ does $\int_a^t f'(x)dx=f(t)-f(a)$ true?",If  derivable on  does  true?,"f [a,b] \int_a^t f'(x)dx=f(t)-f(a)","Let $f:[a,b]\longrightarrow \mathbb R$ a derivable function. Is it true that for all $t\in [a,b]$ we have that $$f(t)=f(a)+\int_a^t f'(x)dx \ \ ?$$ The thing is since $f'$ is not supposed continuous, there is no reason for me for $f'$ to be Riemann integrable. So my questions are the followings one : Q1) In Riemann sense, is the formula correct (if we don't have other hypothesis on $f'$). If no, do you have a counter example ? Q2) If we assume $f'$ Riemann integrable, is the formula correct (in Riemann sense). If no, do you have a counter example ? Q3) In Lebesgue sense, is the formula correct (if we don't have other hypothesis on $f'$). If no, do you have a counter example ? Q4) If we assume $f'$ Lebesgue integrable, is the formula correct (in Lebesgue sense). If no, do you have a counter example ?","Let $f:[a,b]\longrightarrow \mathbb R$ a derivable function. Is it true that for all $t\in [a,b]$ we have that $$f(t)=f(a)+\int_a^t f'(x)dx \ \ ?$$ The thing is since $f'$ is not supposed continuous, there is no reason for me for $f'$ to be Riemann integrable. So my questions are the followings one : Q1) In Riemann sense, is the formula correct (if we don't have other hypothesis on $f'$). If no, do you have a counter example ? Q2) If we assume $f'$ Riemann integrable, is the formula correct (in Riemann sense). If no, do you have a counter example ? Q3) In Lebesgue sense, is the formula correct (if we don't have other hypothesis on $f'$). If no, do you have a counter example ? Q4) If we assume $f'$ Lebesgue integrable, is the formula correct (in Lebesgue sense). If no, do you have a counter example ?",,"['real-analysis', 'integration', 'lebesgue-integral', 'riemann-integration']"
59,Motivation behind proof,Motivation behind proof,,"The following problem is taken from IMC $2017$ Day $1$, August $2, 2017$. Let $f:\mathbb{R}\to(0,\infty)$ be a differentiable function, and suppose that there exists a constant $L>0$ such that    $$|f'(x)-f'(y)|\leq L|x-y|$$   for all $x,y.$   Prove that    $$(f'(x))^2<2Lf(x)$$   holds for all $x.$ The official solution goes as follows: Solution: Notice that $f'$ satisfies the Lipschitz-property, so $f'$ is continuous and therefore locally integrable. Consider an arbitrary $x\in\mathbb{R}$ and let $d=f'(x).$   We need to prove $f(x)>\frac{d^2}{2L}.$ If $d=0,$ then the statement is trivial. If $d>0,$ then the condition provides $f'(x-t) \geq d-Lt;$ this estimate is positive for $1\leq t <\frac{d}{L}.$   By integrating over that interval,    $$f(x)>f(x)-f(x-\frac{d}{L})=\int_0^{\frac{d}{L}}f'(x-t) \,dt \geq \int_0^{\frac{d}{L}}(d-Lt)\,dt=\frac{d^2}{2L}.$$   If $d<0,$ then apply $f'(x+t)\leq d+Lt=-|d|+Lt$ and repeat the same argument as    $$f(x)>f(x)-f(x+\frac{|d|}{L}) = \int_0^{\frac{|d|}{L}}(-f'(x+t))\,dt \geq \int_0^{\frac{|d|}{L}}(|d|-Lt)\, dt = \frac{d^2}{2L}.$$ Question: What is a motivation behind the proof above? Yes, I can fully understand the solution if I read line by line. However, I attempted the problem myself for 1 hour and could not get anywhere. It would be good if someone can tell me a motivation of considering the proof above. It is okay if you want to provide an alternative solution to the problem. An alternative solution will give us another way of thinking the problem.","The following problem is taken from IMC $2017$ Day $1$, August $2, 2017$. Let $f:\mathbb{R}\to(0,\infty)$ be a differentiable function, and suppose that there exists a constant $L>0$ such that    $$|f'(x)-f'(y)|\leq L|x-y|$$   for all $x,y.$   Prove that    $$(f'(x))^2<2Lf(x)$$   holds for all $x.$ The official solution goes as follows: Solution: Notice that $f'$ satisfies the Lipschitz-property, so $f'$ is continuous and therefore locally integrable. Consider an arbitrary $x\in\mathbb{R}$ and let $d=f'(x).$   We need to prove $f(x)>\frac{d^2}{2L}.$ If $d=0,$ then the statement is trivial. If $d>0,$ then the condition provides $f'(x-t) \geq d-Lt;$ this estimate is positive for $1\leq t <\frac{d}{L}.$   By integrating over that interval,    $$f(x)>f(x)-f(x-\frac{d}{L})=\int_0^{\frac{d}{L}}f'(x-t) \,dt \geq \int_0^{\frac{d}{L}}(d-Lt)\,dt=\frac{d^2}{2L}.$$   If $d<0,$ then apply $f'(x+t)\leq d+Lt=-|d|+Lt$ and repeat the same argument as    $$f(x)>f(x)-f(x+\frac{|d|}{L}) = \int_0^{\frac{|d|}{L}}(-f'(x+t))\,dt \geq \int_0^{\frac{|d|}{L}}(|d|-Lt)\, dt = \frac{d^2}{2L}.$$ Question: What is a motivation behind the proof above? Yes, I can fully understand the solution if I read line by line. However, I attempted the problem myself for 1 hour and could not get anywhere. It would be good if someone can tell me a motivation of considering the proof above. It is okay if you want to provide an alternative solution to the problem. An alternative solution will give us another way of thinking the problem.",,"['real-analysis', 'contest-math', 'proof-explanation']"
60,"Prove : If $\sum_na_nb_n$ converges whenever $\sum b_n^2 \lt \infty,$ then $\sum a_n^2<\infty$",Prove : If  converges whenever  then,"\sum_na_nb_n \sum b_n^2 \lt \infty, \sum a_n^2<\infty","Suppose that $a_n$ is a sequence of real numbers  such that $\sum_na_nb_n$ converges whenever $\sum_n b_n^2 \lt \infty$. Show that $\sum_{n=1}^{\infty}a_n^2 \lt \infty$. My try: I defined $T: l^2 \to \mathbb{R}$ by sending $(b_1,b_2,\ldots,b_n,\ldots,) \to \sum_{n=1}^{\infty}a_nb_n$. Then $T$ is linear. I want to show that $T$ is bounded and then that will give us the result for $b^n=(a_1,a_2,\ldots,a_n,0,\ldots,0)$, $$T\left(\frac{b^n}{\sqrt{\sum_{j=1}^n a_j^2}}\right)=\frac{a_1^2+a_2^2+\ldots a_n^2}{\sqrt{\sum_{j=1}^n a_j^2}}=\sqrt{\sum_{j=1}^n a_j^2} \le \|T\|, \forall n \in \mathbb{N}$$ which implies that $$\sum_{j=1}^{\infty} a_j^2 =\lim_{n \to \infty} \sum_{j=1}^n a_j^2 \le \|T\|^2 \lt \infty$$ The only thing which remains to be shown now is that $T$ is bounded for which I tried to evoke the Closed Graph Theorem. Suppose that $b^n=(b^n(1),b^n(2),\ldots,b^n(j),\ldots) \in l^2 $ converge to $0$ and $T(b^n) \to y$. Let $\epsilon \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $\|b^n\|_2 \lt \frac{\epsilon}{2}$ which in particular implies that $|b^n(j)| \lt \frac{\epsilon}{2}$ for all $j$ and all $n \ge n_0$. Since $T(b^n) \to y$, there exists $n_1 \in \mathbb{N}$ such that for all $n \ge n_1$, $|T(b^n)-y| \lt \frac{\epsilon}{2}$. Then for all $n \ge \max{(n_0,n_1)}$$$|y| \le |y-T(b^n)|+|T(b^n)| \lt \epsilon+\frac{\epsilon}{2}(|a_1|+|a_2|+\ldots+|a_k|)$$ (Note: Since $T(b^n) \lt \infty$, the tail of the series goes to $0$ which means that $|\sum_{j \ge k} a_jb^n(j)| \lt \frac{\epsilon}{2}$ ) This is not what I intended to show. Can I conclude from here that $y=0$ since $\epsilon \gt 0$ is arbitrary? For me the problem is that apriori, I don't have a way to get away with the $|a_j|$'s, since they depend on the choice of $b^n$. Note: There is an answer to this question here: If $\sum a_n b_n <\infty$ for all $(b_n)\in \ell^2$ then $(a_n) \in \ell^2$ . But I wanted to know if I can go via this route and get to the answer and if not, why so. Thanks for the help!!","Suppose that $a_n$ is a sequence of real numbers  such that $\sum_na_nb_n$ converges whenever $\sum_n b_n^2 \lt \infty$. Show that $\sum_{n=1}^{\infty}a_n^2 \lt \infty$. My try: I defined $T: l^2 \to \mathbb{R}$ by sending $(b_1,b_2,\ldots,b_n,\ldots,) \to \sum_{n=1}^{\infty}a_nb_n$. Then $T$ is linear. I want to show that $T$ is bounded and then that will give us the result for $b^n=(a_1,a_2,\ldots,a_n,0,\ldots,0)$, $$T\left(\frac{b^n}{\sqrt{\sum_{j=1}^n a_j^2}}\right)=\frac{a_1^2+a_2^2+\ldots a_n^2}{\sqrt{\sum_{j=1}^n a_j^2}}=\sqrt{\sum_{j=1}^n a_j^2} \le \|T\|, \forall n \in \mathbb{N}$$ which implies that $$\sum_{j=1}^{\infty} a_j^2 =\lim_{n \to \infty} \sum_{j=1}^n a_j^2 \le \|T\|^2 \lt \infty$$ The only thing which remains to be shown now is that $T$ is bounded for which I tried to evoke the Closed Graph Theorem. Suppose that $b^n=(b^n(1),b^n(2),\ldots,b^n(j),\ldots) \in l^2 $ converge to $0$ and $T(b^n) \to y$. Let $\epsilon \gt 0$. Then there exists $n_0 \in \mathbb{N}$ such that for all $n \ge n_0$, we have $\|b^n\|_2 \lt \frac{\epsilon}{2}$ which in particular implies that $|b^n(j)| \lt \frac{\epsilon}{2}$ for all $j$ and all $n \ge n_0$. Since $T(b^n) \to y$, there exists $n_1 \in \mathbb{N}$ such that for all $n \ge n_1$, $|T(b^n)-y| \lt \frac{\epsilon}{2}$. Then for all $n \ge \max{(n_0,n_1)}$$$|y| \le |y-T(b^n)|+|T(b^n)| \lt \epsilon+\frac{\epsilon}{2}(|a_1|+|a_2|+\ldots+|a_k|)$$ (Note: Since $T(b^n) \lt \infty$, the tail of the series goes to $0$ which means that $|\sum_{j \ge k} a_jb^n(j)| \lt \frac{\epsilon}{2}$ ) This is not what I intended to show. Can I conclude from here that $y=0$ since $\epsilon \gt 0$ is arbitrary? For me the problem is that apriori, I don't have a way to get away with the $|a_j|$'s, since they depend on the choice of $b^n$. Note: There is an answer to this question here: If $\sum a_n b_n <\infty$ for all $(b_n)\in \ell^2$ then $(a_n) \in \ell^2$ . But I wanted to know if I can go via this route and get to the answer and if not, why so. Thanks for the help!!",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'banach-spaces']"
61,Equality in Hardy's inequality via Hölder's,Equality in Hardy's inequality via Hölder's,,"I'm working on Exercise 3.14 in Rudin's Real and Complex Analysis. I was able to answer part (a): that for real $p$ satisfying $1<p<\infty$, for every function $f$ in $L^p(0,\infty)$, when $F$ is defined by $$ F(x)=\frac{1}{x}\int_{0}^{x} f(t)~dt, $$ then $$ \lVert F\rVert_p\leq\frac{p}{p-1}\lVert f\rVert_p. $$ My proof uses Rudin's suggestion to show the inequality first for nonnegative compactly supported continuous functions, and then the general case. For the nonnegative compactly supported continuous case, I used Hölder's inequality on $$ \int_{0}^{\infty} F^{p-1}(x)f(x)~dx, $$ and I used Fatou's lemma and a density argument for the general case. What I'm stuck on is part (b): showing equality holds in Hardy's inequality only when $f$ vanishes almost everywhere. I was able to do this with the additional  assumption that $f$ is nonnegative, compactly supported, and continuous, by using the necessary and sufficient condition for equality in Hölder's. I wasn't able to extend this to the general case. Any suggestions? (I saw a related question here which uses the Fubini theorem, but Rudin doesn't cover Fubini until Chapter 8.)","I'm working on Exercise 3.14 in Rudin's Real and Complex Analysis. I was able to answer part (a): that for real $p$ satisfying $1<p<\infty$, for every function $f$ in $L^p(0,\infty)$, when $F$ is defined by $$ F(x)=\frac{1}{x}\int_{0}^{x} f(t)~dt, $$ then $$ \lVert F\rVert_p\leq\frac{p}{p-1}\lVert f\rVert_p. $$ My proof uses Rudin's suggestion to show the inequality first for nonnegative compactly supported continuous functions, and then the general case. For the nonnegative compactly supported continuous case, I used Hölder's inequality on $$ \int_{0}^{\infty} F^{p-1}(x)f(x)~dx, $$ and I used Fatou's lemma and a density argument for the general case. What I'm stuck on is part (b): showing equality holds in Hardy's inequality only when $f$ vanishes almost everywhere. I was able to do this with the additional  assumption that $f$ is nonnegative, compactly supported, and continuous, by using the necessary and sufficient condition for equality in Hölder's. I wasn't able to extend this to the general case. Any suggestions? (I saw a related question here which uses the Fubini theorem, but Rudin doesn't cover Fubini until Chapter 8.)",,"['real-analysis', 'inequality']"
62,Could Euclid have proven that real number multiplication is commutative?,Could Euclid have proven that real number multiplication is commutative?,,"In Euclid's day, the modern notion of real number did not exist; Euclid did not believe that the length of a line segment was a quantity measurable by number.  But he did think it made sense to talk about the ratio of two lengths.  In fact, he devotes Book V of his Elements to the study of such ratios, using the so-called Eudoxian theory of proportions.  Here's how it works. Let $w$ and $x$ be two magnitudes of the same kind (for instance two length), and let $y$ and $z$ be two magnitudes of the same kind (for instance two areas). Then according to Euclid, the ratio of $w$ to $x$ is said to be equal to the ratio of $y$ to $z$ if for all positive integers $m$ and $n$, if $nw$ is greater, equal, or less than $mx$, then $ny$ is greater, equal, or less than $mz$, respectively.  Or to put it in more modern language, $w/x = y/z$ if the same rational numbers $m/n$ are less than both, the same rational numbers are equal to both, and the same rational numbers are greater than both. In other words, a ratio is defined by the classes of rational numbers which are less than, equal to, and greater than it.  If you've studied real analysis; this should look familiar to you: it is how the real number system is constructed using Dedekind cuts!  In fact, Dedekind took the Eudoxian theory of proportions in Euclid's book V as the inspiration for his Dedekind cut construction.  So to sum up, while Euclid wouldn't have thought of them as numbers, his notion of ""ratios"" corresponds to our notion of ""positive real numbers"". Now with that background, I would like to try to prove using Euclid's system that multiplication of real numbers is commutative.  First let me explain how the product of two ratios is defined.  (Euclid uses the product in a few propositions including this one .)  We say that the product of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/s$, $y/z = s/t$, and $r/t = u/v$. So in order to prove the commutativity of multiplication, we would need to prove the following: Suppose that $b/c = e/f$ and $a/b = f/g$.  Then $a/c =e/g$. If we could prove that, then that would mean that the product of $b/c$ and $a/b$ is equal to the product of $a/b$ and $b/c$, which would immediately imply the commutativity of multiplication in general. So how would I go about proving that?  Euclid's Book V contains a lot of theorems about ratios that are potentially relevant, but I'm not sure how to proceed. EDIT:  I just posted a follow-up question on the distributive property.","In Euclid's day, the modern notion of real number did not exist; Euclid did not believe that the length of a line segment was a quantity measurable by number.  But he did think it made sense to talk about the ratio of two lengths.  In fact, he devotes Book V of his Elements to the study of such ratios, using the so-called Eudoxian theory of proportions.  Here's how it works. Let $w$ and $x$ be two magnitudes of the same kind (for instance two length), and let $y$ and $z$ be two magnitudes of the same kind (for instance two areas). Then according to Euclid, the ratio of $w$ to $x$ is said to be equal to the ratio of $y$ to $z$ if for all positive integers $m$ and $n$, if $nw$ is greater, equal, or less than $mx$, then $ny$ is greater, equal, or less than $mz$, respectively.  Or to put it in more modern language, $w/x = y/z$ if the same rational numbers $m/n$ are less than both, the same rational numbers are equal to both, and the same rational numbers are greater than both. In other words, a ratio is defined by the classes of rational numbers which are less than, equal to, and greater than it.  If you've studied real analysis; this should look familiar to you: it is how the real number system is constructed using Dedekind cuts!  In fact, Dedekind took the Eudoxian theory of proportions in Euclid's book V as the inspiration for his Dedekind cut construction.  So to sum up, while Euclid wouldn't have thought of them as numbers, his notion of ""ratios"" corresponds to our notion of ""positive real numbers"". Now with that background, I would like to try to prove using Euclid's system that multiplication of real numbers is commutative.  First let me explain how the product of two ratios is defined.  (Euclid uses the product in a few propositions including this one .)  We say that the product of $w/x$ and $y/z$ is equal to $u/v$ if there exist magnitudes $r,s,$ and $t$ such that $w/x = r/s$, $y/z = s/t$, and $r/t = u/v$. So in order to prove the commutativity of multiplication, we would need to prove the following: Suppose that $b/c = e/f$ and $a/b = f/g$.  Then $a/c =e/g$. If we could prove that, then that would mean that the product of $b/c$ and $a/b$ is equal to the product of $a/b$ and $b/c$, which would immediately imply the commutativity of multiplication in general. So how would I go about proving that?  Euclid's Book V contains a lot of theorems about ratios that are potentially relevant, but I'm not sure how to proceed. EDIT:  I just posted a follow-up question on the distributive property.",,"['real-analysis', 'geometry', 'euclidean-geometry', 'math-history', 'real-numbers']"
63,Bounded and closed but not compact in rational numbers,Bounded and closed but not compact in rational numbers,,"I'm sorry if topic repeated. I solved this problem. And I want to know is my solution true? Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space, with $d(p,q)=|p-q|$. Let $E=\{p\in \mathbb{Q}: 2<p^2<3\}$. Show that $E$ is closed and bounded in $\mathbb{Q}$, but $E$ is not compact. Is $E$ open in $\mathbb{Q}$? Proof: Boundness: It easy to check that $E$ is bounded. Because $E\subset B_2(0)=\{z\in \mathbb{Q}:d(z,0)<2\}$ - open ball in $\mathbb{Q}$. Closed: Also $E$ is closed because $E^c=\{p\in \mathbb{Q}: p^2<2\quad  \text{or} \quad p^2>3 \}$ is open set. Because if $z\in E^c$ then $z\in \mathbb{Q}\cap \{z: z^2<2\}$ or $z\in \mathbb{Q}\cap \{z: z^2>3\}$. And for both cases $\exists n\in \mathbb{N}$ s.t. $(z+1/n)^2<2$ or $(z-1/n)^2>3$ because both inequalities are solvable for large $n$. Taking $n$ so large we got $N_{1/n}(z)\subset E^c$ where $N_{1/n}(z)=\{q\in \mathbb{Q}: d(q,z)<1/n\}$. So $E^c$ is open in $\mathbb{Q}$. Therefore $E$ is closed in $\mathbb{Q}$. Compactness: We'll prove that $E$ is not compact in $\mathbb{Q}$. It means that exists some open cover of $E$ which contains no finite subcover. Let $$G_n=\{\mathbb{Q}\cap [-\sqrt{3}, -\sqrt{2}-\frac{1}{n}]\}\cup \{\mathbb{Q} \cap [\sqrt{2}, \sqrt{3}-\frac{1}{n}]\}$$ for $n\geqslant 4$. It's easy to verify that $\{G_n\}$ is an open of $E$. But it contains no finite subcover. Therefore, $E$ is not compact in $\mathbb{Q}$. Openess: Set $E$ is open. If $z\in E$ then $z\in \mathbb{Q}$ and $2<z^2<3$. Then exists $n$ so large s.t. $2<(z-1/n)^2<(z+1/n)^2<3$. Hence $N_{1/n}(z)\subset E$.","I'm sorry if topic repeated. I solved this problem. And I want to know is my solution true? Regard $\mathbb{Q}$, the set of all rational numbers, as a metric space, with $d(p,q)=|p-q|$. Let $E=\{p\in \mathbb{Q}: 2<p^2<3\}$. Show that $E$ is closed and bounded in $\mathbb{Q}$, but $E$ is not compact. Is $E$ open in $\mathbb{Q}$? Proof: Boundness: It easy to check that $E$ is bounded. Because $E\subset B_2(0)=\{z\in \mathbb{Q}:d(z,0)<2\}$ - open ball in $\mathbb{Q}$. Closed: Also $E$ is closed because $E^c=\{p\in \mathbb{Q}: p^2<2\quad  \text{or} \quad p^2>3 \}$ is open set. Because if $z\in E^c$ then $z\in \mathbb{Q}\cap \{z: z^2<2\}$ or $z\in \mathbb{Q}\cap \{z: z^2>3\}$. And for both cases $\exists n\in \mathbb{N}$ s.t. $(z+1/n)^2<2$ or $(z-1/n)^2>3$ because both inequalities are solvable for large $n$. Taking $n$ so large we got $N_{1/n}(z)\subset E^c$ where $N_{1/n}(z)=\{q\in \mathbb{Q}: d(q,z)<1/n\}$. So $E^c$ is open in $\mathbb{Q}$. Therefore $E$ is closed in $\mathbb{Q}$. Compactness: We'll prove that $E$ is not compact in $\mathbb{Q}$. It means that exists some open cover of $E$ which contains no finite subcover. Let $$G_n=\{\mathbb{Q}\cap [-\sqrt{3}, -\sqrt{2}-\frac{1}{n}]\}\cup \{\mathbb{Q} \cap [\sqrt{2}, \sqrt{3}-\frac{1}{n}]\}$$ for $n\geqslant 4$. It's easy to verify that $\{G_n\}$ is an open of $E$. But it contains no finite subcover. Therefore, $E$ is not compact in $\mathbb{Q}$. Openess: Set $E$ is open. If $z\in E$ then $z\in \mathbb{Q}$ and $2<z^2<3$. Then exists $n$ so large s.t. $2<(z-1/n)^2<(z+1/n)^2<3$. Hence $N_{1/n}(z)\subset E$.",,"['real-analysis', 'general-topology']"
64,"Prob. 8, Sec. 3.5 in Erwin Kreyszig's Introductory Functoinal Anlaysis With Applications","Prob. 8, Sec. 3.5 in Erwin Kreyszig's Introductory Functoinal Anlaysis With Applications",,"Erwin Kreyszig's Introductory Functoinal Anlaysis With Applications Prob. 8, Sec. 3.5 $\DeclareMathOperator{\span}{span}$ Let $(e_k)$ be an orthonormal sequence in a Hilbert space $H$ , and let $M = \span (e_k)$ . Let $x \in H$ . If $$x = \sum_{k=1}^\infty \langle x, e_k \rangle e_k,$$ then $x \in \overline{\span(e_k)}$ because in this case the sequence $(s_n)$ in $\span(e_k)$ , where $s_n =  \sum_{k=1}^n \langle x, e_k \rangle e_k$ , converges to $x$ . How to show the converse? That is, how to show that if $x \in \overline{\span(e_k)}$ , then the series $\sum_{k=1}^\infty \langle x, e_k \rangle e_k$ converges (in the norm induced by the inner product on $H$ ) and has sum $x$ ? My effort: Suppose $x \in \overline{\span(e_k)}$ . Then there is a sequence $(x_n)$ in $\span(e_k)$ that converges to $x$ . Let $x_n = \sum_{k=1}^{m_n} \alpha_{nk} e_k$ for each $n= 1, 2, 3, \ldots$ , where $\alpha_{nk}$ are scalars and the $m_n$ are natural numbers. Then, using the orthonormality of the $e_k$ , we can conclude that $\alpha_{nk} = \langle x_n, e_k \rangle$ for each $n=1, 2, 3, \ldots$ and for each $k= 1, \ldots, m_n$ . So $$x_n = \sum_{k=1}^{m_n} \langle x_n, e_k \rangle e_k. $$ What next? Can we say the following? For each fixed $k$ , $$\langle x_n, e_k \rangle \to \langle x, e_k \rangle \  \mbox{ as } \ n \to \infty. $$ How to show that $$x = \sum_{k=1}^\infty \langle x, e_k \rangle e_k?$$ I also know that the series $\sum \langle x, e_k \rangle e_k$ does converge.","Erwin Kreyszig's Introductory Functoinal Anlaysis With Applications Prob. 8, Sec. 3.5 Let be an orthonormal sequence in a Hilbert space , and let . Let . If then because in this case the sequence in , where , converges to . How to show the converse? That is, how to show that if , then the series converges (in the norm induced by the inner product on ) and has sum ? My effort: Suppose . Then there is a sequence in that converges to . Let for each , where are scalars and the are natural numbers. Then, using the orthonormality of the , we can conclude that for each and for each . So What next? Can we say the following? For each fixed , How to show that I also know that the series does converge.","\DeclareMathOperator{\span}{span} (e_k) H M = \span (e_k) x \in H x = \sum_{k=1}^\infty \langle x, e_k \rangle e_k, x \in \overline{\span(e_k)} (s_n) \span(e_k) s_n =  \sum_{k=1}^n \langle x, e_k \rangle e_k x x \in \overline{\span(e_k)} \sum_{k=1}^\infty \langle x, e_k \rangle e_k H x x \in \overline{\span(e_k)} (x_n) \span(e_k) x x_n = \sum_{k=1}^{m_n} \alpha_{nk} e_k n= 1, 2, 3, \ldots \alpha_{nk} m_n e_k \alpha_{nk} = \langle x_n, e_k \rangle n=1, 2, 3, \ldots k= 1, \ldots, m_n x_n = \sum_{k=1}^{m_n} \langle x_n, e_k \rangle e_k.  k \langle x_n, e_k \rangle \to \langle x, e_k \rangle \  \mbox{ as } \ n \to \infty.  x = \sum_{k=1}^\infty \langle x, e_k \rangle e_k? \sum \langle x, e_k \rangle e_k","['real-analysis', 'analysis', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
65,Invertiblity of the Derivative Matrix ?! (to use Inverse function Theorem),Invertiblity of the Derivative Matrix ?! (to use Inverse function Theorem),,"In the Analysis2 midterm exam, we had the following problem: Let the equation $a_nx^n+\cdots+a_1x+a_0=0$ has $n$ simple real roots (distinct) $\{\alpha_1,\cdots,\alpha_n\}$. Prove that the above equation has still $n$ distinct real roots when the change in coefficients is small enough ! I'm pretty sure that $(a_1,\cdots,a_n,\alpha)\mapsto a_n\alpha^n+\cdots+a_1\alpha+a_0$ plus implicit function theorem will work. But it didn't came to my mind. Instead, I thought that the coefficients are $C^\infty$ function of the roots by Vietta Theorem . So I hoped the map $\psi:(\alpha_1,\cdots,\alpha_n)\mapsto(a_0,\cdots,a_{n-1})$ has a full rank derivative at the current roots and start to apply Inverse function Theorem to conclude that, locally, $(\alpha_1,\cdots,\alpha_n)$ is $C^\infty$ diffeomorphism map of $(a_1,\cdots,a_n)$. Hence, I conjectured the following proposition : Conjecture. We know by Vietta's Theorem that :   $$\left\{\begin{array}{ll} \psi_1=a_0=(-1)^n \alpha_1\cdots\alpha_n\\ \psi_2=a_1=(-1)^{n-1} \displaystyle\sum_{r=1}^n \alpha_1\cdots\hat{\alpha_r}\cdots\alpha_n\\ \vdots\\ \psi_{n-1}=a_{n-2}=\displaystyle\sum_{i,j=1}^n \alpha_i\alpha_j\\ \psi_{n}=a_{n-1}=-(\alpha_1+\cdots+\alpha_n) \end{array}\right.$$ Then the matrix    $$D_{(\alpha_1,\cdots,\alpha_n)}\psi=\left[\begin{matrix} \frac{\partial\psi_1}{\partial\alpha_1}&\cdots&\frac{\partial\psi_1}{\partial\alpha_n}\\ \vdots&\ddots\\ \frac{\partial\psi_n}{\partial\alpha_1}&&\frac{\partial\psi_n}{\partial\alpha_n} \end{matrix}\right]= \left(\begin{matrix} (-1)^n\alpha_2\cdots\alpha_{n}&(-1)^n\alpha_1\alpha_3\cdots\alpha_{n}&\color{red}{\cdots}&(-1)^n\alpha_1\cdots\alpha_{n-1}\\ \color{red}{\vdots}&\color{red}{\ddots}&\color{red}{\vdots}\\ \alpha_2+\cdots+\alpha_{n}&\alpha_1+\alpha_3+\cdots+\alpha_{n}&\color{red}{\cdots}&\alpha_1+\cdots+\alpha_{n-1}\\ -1&-1&\color{red}{\cdots}&-1 \end{matrix}\right)$$   is Invertible , whenever $\alpha_j$s are pairwise distinct. For case $n=2$ and $n=3$, I prove that $\det\big( D_{(\alpha_1,\cdots,\alpha_n)}\psi\big)=0$ if and only if $\alpha_i=\alpha_j$ for some $i\neq j$. But I don't know what to do for general $n$. Is it a famous matrix ? Is this conjecture correct for general $n$? Proof for $n=2$ and $n=3$ : n=2 :$\quad D_{(\alpha,\beta)}\psi=\left[\begin{matrix} \beta&\alpha\\ -1&-1 \end{matrix}\right]$. So, $\boxed{\det(D_{\alpha,\beta}\psi)=0 \leftrightarrow \alpha=\beta\rightarrow\bot}$ n=3 $ :\quad D_{(\alpha,\beta,\gamma)}\psi=\left[\begin{matrix} -\beta\gamma&-\alpha\gamma&-\alpha\beta\\ \beta+\gamma&\alpha+\gamma&\alpha+\beta\\ -1&-1&-1 \end{matrix}\right] $. Now by computing determinant respect to the last row: $\begin{align} \det(D_{(\alpha,\beta,\gamma)}\psi)= +\big[-\alpha^2\gamma-\alpha\beta\gamma+\alpha^2\beta+\alpha\beta\gamma\big]&-\big[-\alpha\beta\gamma-\beta^2\gamma+\alpha\beta^2+\alpha\beta\gamma\big]\\ &-\big[-\alpha\beta\gamma-\beta\gamma^2+\alpha\beta\gamma+\alpha\gamma^2\big] \end{align}$. $$\Rightarrow\det(D_{(\alpha,\beta,\gamma)}\psi)= [\alpha-\beta]\color{red}{\gamma^2}+[\beta^2-\alpha^2]\color{red}{\gamma}+[\alpha\beta(\alpha-\beta)] $$ Now the equation $\det(D_{(\alpha,\beta,\gamma)}\psi)=0$, while $\alpha\neq\beta$, becomes the following quadratic equation respect to $\gamma$ : $$\boxed{(\gamma-\alpha)(\gamma-\beta)=\color{red}{\gamma^2}-(\alpha+\beta)\color{red}{\gamma}+\alpha\beta=0 \leftrightarrow \gamma=\alpha\vee\gamma=\beta\longrightarrow\bot}$$","In the Analysis2 midterm exam, we had the following problem: Let the equation $a_nx^n+\cdots+a_1x+a_0=0$ has $n$ simple real roots (distinct) $\{\alpha_1,\cdots,\alpha_n\}$. Prove that the above equation has still $n$ distinct real roots when the change in coefficients is small enough ! I'm pretty sure that $(a_1,\cdots,a_n,\alpha)\mapsto a_n\alpha^n+\cdots+a_1\alpha+a_0$ plus implicit function theorem will work. But it didn't came to my mind. Instead, I thought that the coefficients are $C^\infty$ function of the roots by Vietta Theorem . So I hoped the map $\psi:(\alpha_1,\cdots,\alpha_n)\mapsto(a_0,\cdots,a_{n-1})$ has a full rank derivative at the current roots and start to apply Inverse function Theorem to conclude that, locally, $(\alpha_1,\cdots,\alpha_n)$ is $C^\infty$ diffeomorphism map of $(a_1,\cdots,a_n)$. Hence, I conjectured the following proposition : Conjecture. We know by Vietta's Theorem that :   $$\left\{\begin{array}{ll} \psi_1=a_0=(-1)^n \alpha_1\cdots\alpha_n\\ \psi_2=a_1=(-1)^{n-1} \displaystyle\sum_{r=1}^n \alpha_1\cdots\hat{\alpha_r}\cdots\alpha_n\\ \vdots\\ \psi_{n-1}=a_{n-2}=\displaystyle\sum_{i,j=1}^n \alpha_i\alpha_j\\ \psi_{n}=a_{n-1}=-(\alpha_1+\cdots+\alpha_n) \end{array}\right.$$ Then the matrix    $$D_{(\alpha_1,\cdots,\alpha_n)}\psi=\left[\begin{matrix} \frac{\partial\psi_1}{\partial\alpha_1}&\cdots&\frac{\partial\psi_1}{\partial\alpha_n}\\ \vdots&\ddots\\ \frac{\partial\psi_n}{\partial\alpha_1}&&\frac{\partial\psi_n}{\partial\alpha_n} \end{matrix}\right]= \left(\begin{matrix} (-1)^n\alpha_2\cdots\alpha_{n}&(-1)^n\alpha_1\alpha_3\cdots\alpha_{n}&\color{red}{\cdots}&(-1)^n\alpha_1\cdots\alpha_{n-1}\\ \color{red}{\vdots}&\color{red}{\ddots}&\color{red}{\vdots}\\ \alpha_2+\cdots+\alpha_{n}&\alpha_1+\alpha_3+\cdots+\alpha_{n}&\color{red}{\cdots}&\alpha_1+\cdots+\alpha_{n-1}\\ -1&-1&\color{red}{\cdots}&-1 \end{matrix}\right)$$   is Invertible , whenever $\alpha_j$s are pairwise distinct. For case $n=2$ and $n=3$, I prove that $\det\big( D_{(\alpha_1,\cdots,\alpha_n)}\psi\big)=0$ if and only if $\alpha_i=\alpha_j$ for some $i\neq j$. But I don't know what to do for general $n$. Is it a famous matrix ? Is this conjecture correct for general $n$? Proof for $n=2$ and $n=3$ : n=2 :$\quad D_{(\alpha,\beta)}\psi=\left[\begin{matrix} \beta&\alpha\\ -1&-1 \end{matrix}\right]$. So, $\boxed{\det(D_{\alpha,\beta}\psi)=0 \leftrightarrow \alpha=\beta\rightarrow\bot}$ n=3 $ :\quad D_{(\alpha,\beta,\gamma)}\psi=\left[\begin{matrix} -\beta\gamma&-\alpha\gamma&-\alpha\beta\\ \beta+\gamma&\alpha+\gamma&\alpha+\beta\\ -1&-1&-1 \end{matrix}\right] $. Now by computing determinant respect to the last row: $\begin{align} \det(D_{(\alpha,\beta,\gamma)}\psi)= +\big[-\alpha^2\gamma-\alpha\beta\gamma+\alpha^2\beta+\alpha\beta\gamma\big]&-\big[-\alpha\beta\gamma-\beta^2\gamma+\alpha\beta^2+\alpha\beta\gamma\big]\\ &-\big[-\alpha\beta\gamma-\beta\gamma^2+\alpha\beta\gamma+\alpha\gamma^2\big] \end{align}$. $$\Rightarrow\det(D_{(\alpha,\beta,\gamma)}\psi)= [\alpha-\beta]\color{red}{\gamma^2}+[\beta^2-\alpha^2]\color{red}{\gamma}+[\alpha\beta(\alpha-\beta)] $$ Now the equation $\det(D_{(\alpha,\beta,\gamma)}\psi)=0$, while $\alpha\neq\beta$, becomes the following quadratic equation respect to $\gamma$ : $$\boxed{(\gamma-\alpha)(\gamma-\beta)=\color{red}{\gamma^2}-(\alpha+\beta)\color{red}{\gamma}+\alpha\beta=0 \leftrightarrow \gamma=\alpha\vee\gamma=\beta\longrightarrow\bot}$$",,"['real-analysis', 'linear-algebra', 'derivatives', 'determinant', 'implicit-function-theorem']"
66,Is the sequence defined by the recurrence $ a _ { n + 2 } = \frac 1 { a _ { n + 1 } } + \frac 1 { a _ n } $ convergent? [duplicate],Is the sequence defined by the recurrence  convergent? [duplicate], a _ { n + 2 } = \frac 1 { a _ { n + 1 } } + \frac 1 { a _ n } ,"This question already has answers here : Proof of existence of a limit for the sequence recursively-defined with $a_1=1$, $a_2=1$ and $a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}$ for $n\ge2$ (5 answers) Closed 7 years ago . Let $ a _ 0 = 1 $ , $ a _ 1 = 1 $ and $ a _ { n + 2 } = \frac 1 { a _ { n + 1 } } + \frac 1 { a _ n } $ for every natural number $ n $ . How can I prove that this sequence is convergent? I know that if it's convergent, it converges to $ \sqrt 2 $ since if $ \lim \limits _ { n \to \infty } a _ n = a $ then: $$ \lim _ { n \to \infty } \left ( a _ { n + 2 } - \frac 1 { a _ { n + 1 } } - \frac 1 { a _ n } \right) = a - \frac 2 a = 0 \text ; $$ $$ \therefore \quad a ^ 2 = 2 \text . $$ Now it's easy to see that every $ a _ n $ is positive, so $ a \ge 0 $ and thus $ a = \sqrt 2 $ . Assuming the sequence is convergent, I can calculate an estimation of the rate of convergence too. Let $ \epsilon _ n := a _ n - \sqrt 2 $ . We have: $$ \epsilon _ { n + 2 } = \frac 1 { a _ { n + 1 } } - \frac 1 { \sqrt 2 } + \frac 1 { a _ n } - \frac 1 { \sqrt 2 } = - \frac { a _ { n + 1 } - \sqrt 2 } { \sqrt 2 a _ { n + 1 } } - \frac { a _ n - \sqrt 2 } { \sqrt 2 a _ n } = - \frac { \epsilon _ { n + 1 } } { \sqrt 2 a _ { n + 1 } } - \frac { \epsilon _ n } { \sqrt 2 a _ n } \text . $$ Now because $ a _ n \sim \sqrt 2 + \epsilon _ n $ and $ \lim \limits _ { n \to \infty } \epsilon _ n = 0 $ , therefore from the above equation: $$ \epsilon _ { n + 2 } \lesssim - \frac { \epsilon _ { n + 1 } + \epsilon _ n } 2 \text , $$ which yields $ \epsilon _ n \lesssim \alpha \left ( \frac { - 1 - \sqrt 7 i } 4 \right) ^ n + \beta \left( \frac { - 1 + \sqrt 7 i } 4 \right) ^ n $ for some complex constants $ \alpha $ and $ \beta $ , using induction on $ n $ . Equivalently, we have $ \epsilon _ n \lesssim \left( \frac 1 { \sqrt 2 } \right) ^ n \bigl( A \cos ( n \theta ) + B \sin ( n \theta ) \bigr) $ for $ \theta = \arctan \frac { \sqrt 7 } 4 $ and some real constants $ A $ and $ B $ , since $ \left| \frac { - 1 \pm \sqrt 7 i } 4 \right| = \frac 1 { \sqrt 2 } $ and $ \arg \frac { - 1 \pm \sqrt 7 i } 4 = \pi \mp \theta $ . Hence we get the rough estimation $ | \epsilon _ n | \lesssim C 2 ^ { - \frac n 2 } $ for some real constant $ C $ , and $ \frac 1 { \sqrt 2 } $ is a good guess for the rate of convergence. ( Edit: Thanks to Alex Ravsky for the confirming graphs in his answer .) Edit (some more of my thoughts): Let $ b _ n := \min \left\{ a _ n , a _ { n + 1 } , \frac 2 { a _ n } , \frac 2 { a _ { n + 1 } } \right\} $ . It's easy to see that $ b _ n \le a _ n \le \frac 2 { b _ n } $ and $ b _ n \le a _ { n + 1 } \le \frac 2 { b _ n } $ . Now using induction we can prove that $ b _ n \le a _ { n + m } \le \frac 2 { b _ n } $ . Especially, $ a _ { n + 2 } \ge b _ n $ and $ \frac 2 { a _ { n + 2 } } \ge b _ n $ which yields $ b _ { n + 1 } \ge b _ n $ . The problem can be solved if I show that the sequence $ ( b _ n ) _ { n = 0 } ^ \infty $ increases to $ \sqrt 2 $ .","This question already has answers here : Proof of existence of a limit for the sequence recursively-defined with $a_1=1$, $a_2=1$ and $a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}$ for $n\ge2$ (5 answers) Closed 7 years ago . Let , and for every natural number . How can I prove that this sequence is convergent? I know that if it's convergent, it converges to since if then: Now it's easy to see that every is positive, so and thus . Assuming the sequence is convergent, I can calculate an estimation of the rate of convergence too. Let . We have: Now because and , therefore from the above equation: which yields for some complex constants and , using induction on . Equivalently, we have for and some real constants and , since and . Hence we get the rough estimation for some real constant , and is a good guess for the rate of convergence. ( Edit: Thanks to Alex Ravsky for the confirming graphs in his answer .) Edit (some more of my thoughts): Let . It's easy to see that and . Now using induction we can prove that . Especially, and which yields . The problem can be solved if I show that the sequence increases to ."," a _ 0 = 1   a _ 1 = 1   a _ { n + 2 } = \frac 1 { a _ { n + 1 } } + \frac 1 { a _ n }   n   \sqrt 2   \lim \limits _ { n \to \infty } a _ n = a   \lim _ { n \to \infty } \left ( a _ { n + 2 } - \frac 1 { a _ { n + 1 } } - \frac 1 { a _ n } \right) = a - \frac 2 a = 0 \text ;   \therefore \quad a ^ 2 = 2 \text .   a _ n   a \ge 0   a = \sqrt 2   \epsilon _ n := a _ n - \sqrt 2   \epsilon _ { n + 2 } = \frac 1 { a _ { n + 1 } } - \frac 1 { \sqrt 2 } + \frac 1 { a _ n } - \frac 1 { \sqrt 2 } = - \frac { a _ { n + 1 } - \sqrt 2 } { \sqrt 2 a _ { n + 1 } } - \frac { a _ n - \sqrt 2 } { \sqrt 2 a _ n } = - \frac { \epsilon _ { n + 1 } } { \sqrt 2 a _ { n + 1 } } - \frac { \epsilon _ n } { \sqrt 2 a _ n } \text .   a _ n \sim \sqrt 2 + \epsilon _ n   \lim \limits _ { n \to \infty } \epsilon _ n = 0   \epsilon _ { n + 2 } \lesssim - \frac { \epsilon _ { n + 1 } + \epsilon _ n } 2 \text ,   \epsilon _ n \lesssim \alpha \left ( \frac { - 1 - \sqrt 7 i } 4 \right) ^ n + \beta \left( \frac { - 1 + \sqrt 7 i } 4 \right) ^ n   \alpha   \beta   n   \epsilon _ n \lesssim \left( \frac 1 { \sqrt 2 } \right) ^ n \bigl( A \cos ( n \theta ) + B \sin ( n \theta ) \bigr)   \theta = \arctan \frac { \sqrt 7 } 4   A   B   \left| \frac { - 1 \pm \sqrt 7 i } 4 \right| = \frac 1 { \sqrt 2 }   \arg \frac { - 1 \pm \sqrt 7 i } 4 = \pi \mp \theta   | \epsilon _ n | \lesssim C 2 ^ { - \frac n 2 }   C   \frac 1 { \sqrt 2 }   b _ n := \min \left\{ a _ n , a _ { n + 1 } , \frac 2 { a _ n } , \frac 2 { a _ { n + 1 } } \right\}   b _ n \le a _ n \le \frac 2 { b _ n }   b _ n \le a _ { n + 1 } \le \frac 2 { b _ n }   b _ n \le a _ { n + m } \le \frac 2 { b _ n }   a _ { n + 2 } \ge b _ n   \frac 2 { a _ { n + 2 } } \ge b _ n   b _ { n + 1 } \ge b _ n   ( b _ n ) _ { n = 0 } ^ \infty   \sqrt 2 ","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'recurrence-relations']"
67,On a proof of Riesz-Fischer Theorem,On a proof of Riesz-Fischer Theorem,,"Questions : [See below for context.] $\rm\color{#c00}{a)}$ First, is the proof presented below $100$ % correct ? $\rm\color{#c00}{b)}$ How would one justify the LHS of $(2)$ ? Are my thoughts correct ? Thoughts : One way I see it is in three times : first we note that $(\cdot)^{1/p}$ is continuous, then we apply the Monotone Convergence Theorem (MON) and finally we note that $|\cdot|^p$ is continuous, so that all in all we can bring the limit all the way in (even if $\displaystyle\sum_{k\geq1} |f_k|$ is infinite in this case). Another way I see it is by noting that every norm is (Lipschitz) continuous, so that we can effectively bring the limit inside (even if it is infinite in this case). $\rm\color{#c00}{c)}$ Why must we take the absolute value of $f_k$ when investigating $\displaystyle\sum|f_k|$ ? Are my thoughts correct ? Thoughts : I think that MON would not apply for $\rm\color{#c00}{b)}$ and I think that if we did not take the absolute value, then we could not speak, a priori , of the limit, that is, $\displaystyle\sum_{k\geq1} f_k$, in $(2)$. Taking the absolute value gives a series with positive terms and such a series either converges or diverges to $\infty$. A series with negative terms could diverge by oscillating. $\rm\color{#c00}{d)}$ In $(4)$, defining $F$ by $0$ is purely arbitrary, right ? We could define $F$ to be anything on the null set, right ? Also, $\displaystyle\sum_{k\geq1} f_k$ does exist on the given set since absolute convergence implies convergence for series of numbers, right ? $\rm\color{#c00}{e)}$ The equality $(5)$ is true since   $$ F-\sum_{k=1}^{n}f_k=\sum_{k=n+1}^{\infty}f_k $$   almost everywhere, right ? $\rm\color{#c00}{f)}$ Would it be fair to say that what was sufficient to prove was $\displaystyle\sum_{k\geq1}|f_k|<\infty$ a.e., [ added ] and that the reason the proof is not trivial is that $L^p(\mu)$ contains, by definition, finite-valued functions ? I mean, in measure theory we work with functions with range in the extended real line, $[-\infty,\infty]$, and if we allowed the functions in $L^p(\mu)$ to take infinite values, then in fact every series would be convergent (trivially) in $L^p(\mu)$. Is this correct ? Def. : Let $(X,\cal{A},\mu)$ be a measured space. Then for $1\leq p<\infty$ we define $$ L^p(\mu):=\{f:X\to\mathbb{R}\text{ measurable and s.t. }\|f\|_p<\infty\}, $$ where $\|f\|_p:=\left(\int|f|^pd\mu\right)^{1/p}$. As usual we really take equivalence classes of functions differing only on a null set. Thm (Riesz-Fischer) : $(L^p(\mu),\|\cdot\|_p)$ is complete for $1\leq p<\infty$. Dem. : We know it suffices to show that every absolutely convergent series converges. Let $(f_k)_{k\geq1}\subset L^p(\mu)$ be a sequence such that $$ \sum_{k=1}^{\infty}\|f_k\|_p<\infty.\tag{0} $$ Since, by Minkowski's inequality, one has $$ \left\|\sum_{k=1}^n|f_k|\right\|_p\leq\sum_{k=1}^n\|f_k\|_p,\tag{1} $$ letting $n\to\infty$ gives $$ \left\|\sum_{k=1}^{\infty}|f_k|\right\|_p\leq\sum_{k=1}^{\infty}\|f_k\|_p<\infty,\tag{2} $$ thus $$ \sum_{k=1}^{\infty}|f_k|<\infty\quad\text{a.e.}\tag{3} $$ Now define $$ F:=\begin{cases}\displaystyle\sum_{k\geq1}f_k&\text{on }\left\{\displaystyle\sum_{k\geq1}|f_k|<\infty\right\},\\0&\text{otherwise}.\end{cases}\tag{4} $$ Then $F$ is in $L^p(\mu)$ since $|F|\leq\displaystyle\sum_{k\geq1}|f_k|$ and by $(2)$. Also, $F$ is such that $$ \sum_{k=1}^{\infty}f_k=F, $$ because \begin{align} \left\|F-\sum_{k=1}^{n}f_k\right\|_p&=\left\|\sum_{k=n+1}^{\infty}f_k\right\|_p\tag{5}\\ &\leq\left\|\sum_{k=n+1}^{\infty}|f_k|\right\|_p\\ &\stackrel{(2)}{\leq}\sum_{k=n+1}^{\infty}\|f_k\|_p\xrightarrow[(n\to\infty)]{(0)}0.\quad\text{Q.E.D.} \end{align}","Questions : [See below for context.] $\rm\color{#c00}{a)}$ First, is the proof presented below $100$ % correct ? $\rm\color{#c00}{b)}$ How would one justify the LHS of $(2)$ ? Are my thoughts correct ? Thoughts : One way I see it is in three times : first we note that $(\cdot)^{1/p}$ is continuous, then we apply the Monotone Convergence Theorem (MON) and finally we note that $|\cdot|^p$ is continuous, so that all in all we can bring the limit all the way in (even if $\displaystyle\sum_{k\geq1} |f_k|$ is infinite in this case). Another way I see it is by noting that every norm is (Lipschitz) continuous, so that we can effectively bring the limit inside (even if it is infinite in this case). $\rm\color{#c00}{c)}$ Why must we take the absolute value of $f_k$ when investigating $\displaystyle\sum|f_k|$ ? Are my thoughts correct ? Thoughts : I think that MON would not apply for $\rm\color{#c00}{b)}$ and I think that if we did not take the absolute value, then we could not speak, a priori , of the limit, that is, $\displaystyle\sum_{k\geq1} f_k$, in $(2)$. Taking the absolute value gives a series with positive terms and such a series either converges or diverges to $\infty$. A series with negative terms could diverge by oscillating. $\rm\color{#c00}{d)}$ In $(4)$, defining $F$ by $0$ is purely arbitrary, right ? We could define $F$ to be anything on the null set, right ? Also, $\displaystyle\sum_{k\geq1} f_k$ does exist on the given set since absolute convergence implies convergence for series of numbers, right ? $\rm\color{#c00}{e)}$ The equality $(5)$ is true since   $$ F-\sum_{k=1}^{n}f_k=\sum_{k=n+1}^{\infty}f_k $$   almost everywhere, right ? $\rm\color{#c00}{f)}$ Would it be fair to say that what was sufficient to prove was $\displaystyle\sum_{k\geq1}|f_k|<\infty$ a.e., [ added ] and that the reason the proof is not trivial is that $L^p(\mu)$ contains, by definition, finite-valued functions ? I mean, in measure theory we work with functions with range in the extended real line, $[-\infty,\infty]$, and if we allowed the functions in $L^p(\mu)$ to take infinite values, then in fact every series would be convergent (trivially) in $L^p(\mu)$. Is this correct ? Def. : Let $(X,\cal{A},\mu)$ be a measured space. Then for $1\leq p<\infty$ we define $$ L^p(\mu):=\{f:X\to\mathbb{R}\text{ measurable and s.t. }\|f\|_p<\infty\}, $$ where $\|f\|_p:=\left(\int|f|^pd\mu\right)^{1/p}$. As usual we really take equivalence classes of functions differing only on a null set. Thm (Riesz-Fischer) : $(L^p(\mu),\|\cdot\|_p)$ is complete for $1\leq p<\infty$. Dem. : We know it suffices to show that every absolutely convergent series converges. Let $(f_k)_{k\geq1}\subset L^p(\mu)$ be a sequence such that $$ \sum_{k=1}^{\infty}\|f_k\|_p<\infty.\tag{0} $$ Since, by Minkowski's inequality, one has $$ \left\|\sum_{k=1}^n|f_k|\right\|_p\leq\sum_{k=1}^n\|f_k\|_p,\tag{1} $$ letting $n\to\infty$ gives $$ \left\|\sum_{k=1}^{\infty}|f_k|\right\|_p\leq\sum_{k=1}^{\infty}\|f_k\|_p<\infty,\tag{2} $$ thus $$ \sum_{k=1}^{\infty}|f_k|<\infty\quad\text{a.e.}\tag{3} $$ Now define $$ F:=\begin{cases}\displaystyle\sum_{k\geq1}f_k&\text{on }\left\{\displaystyle\sum_{k\geq1}|f_k|<\infty\right\},\\0&\text{otherwise}.\end{cases}\tag{4} $$ Then $F$ is in $L^p(\mu)$ since $|F|\leq\displaystyle\sum_{k\geq1}|f_k|$ and by $(2)$. Also, $F$ is such that $$ \sum_{k=1}^{\infty}f_k=F, $$ because \begin{align} \left\|F-\sum_{k=1}^{n}f_k\right\|_p&=\left\|\sum_{k=n+1}^{\infty}f_k\right\|_p\tag{5}\\ &\leq\left\|\sum_{k=n+1}^{\infty}|f_k|\right\|_p\\ &\stackrel{(2)}{\leq}\sum_{k=n+1}^{\infty}\|f_k\|_p\xrightarrow[(n\to\infty)]{(0)}0.\quad\text{Q.E.D.} \end{align}",,"['real-analysis', 'functional-analysis', 'limits', 'measure-theory', 'proof-verification']"
68,Proving completeness of Nikodym Metric,Proving completeness of Nikodym Metric,,"I'm trying to prove completeness directly of the metric given by $d(A, B) = \mu (A \triangle B)$ on a finite measure space $(X, M, \mu)$. Edit: I should make clear that I'm referring to completeness of the actual Nikodym Metric $M/\sim$ under the relation $A \sim B$ if and only if $\mu(A \triangle B) = 0$. I leave a number of minor details left implicit with respect to this relation (like when I'm talking about a class vs a representative), but they're all rather mundane. Let $M_1, M_2, ...$ be a Cauchy sequence in our metric space. Take a subsequence such that for all $m, p > n$, $d(A_m, A_p) < 1/2^n$, where $A_1, A_2, ...$ denotes our subsequence. Let $A = \limsup M_i$. Then $A$ is also the $\limsup$ of the $A_i$'s, since $x$ belongs to infinitely many sets in one sequence if and only if it belongs to infinitely many in the other. Edit 2: The above bolded argument is erroneous. The $\limsup$ of the $A$'s should differ from the $\limsup$ of the $M$'s only on a set of measure zero, but we may very well lose elements when we pass to a $\limsup$ of the subsequence. Let $n \in N$. We use measure continuity from above on $A \setminus A_n$ and from below on $A_n \setminus A$. \begin{align*} \mu(A \setminus A_n) &= \mu(( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty A_j \setminus A_n))  \\ &= \mu( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty (A_j \setminus A_n))  \\ &= \lim_{k \rightarrow \infty} \mu(\bigcup_{j=k}^\infty (A_j \setminus A_n)) \end{align*} At this point I'm stuck - using subadditivity of $\mu$ does not help, as $A_j$ might grow further from $A_n$ as $j \rightarrow \infty$, so that the measure of their difference will grow, making the infinite sum infinite for every $k$. Bounding the other difference is quite simple, however. Using DeMorgan's laws: \begin{align*} \mu(A_n \setminus A) &= \mu(A_n \setminus ( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty A_j)) \\ &= \mu(A_n \cap ( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty A_j)^c) \\ &= \mu( \bigcup_{k \geq 1}^\infty \bigcap_{j=k}^\infty (A_n \setminus A_j))  \\ &= \lim_{k \rightarrow \infty} \mu(\bigcap_{j=k}^\infty (A_n \setminus A_j)) \\ &< 1/2^n \end{align*} If we had the same bound on the first difference as well, then we'd have $\mu(A_n \triangle A) = \mu(A_n \setminus A) + \mu(A \setminus A_n) < 1/2^{n-1} \rightarrow 0$ as $n \rightarrow \infty$ Which would prove convergence of the subsequence to $A$. But a Cauchy sequence with a convergent subsequence converges to the same limit, which would prove the result.","I'm trying to prove completeness directly of the metric given by $d(A, B) = \mu (A \triangle B)$ on a finite measure space $(X, M, \mu)$. Edit: I should make clear that I'm referring to completeness of the actual Nikodym Metric $M/\sim$ under the relation $A \sim B$ if and only if $\mu(A \triangle B) = 0$. I leave a number of minor details left implicit with respect to this relation (like when I'm talking about a class vs a representative), but they're all rather mundane. Let $M_1, M_2, ...$ be a Cauchy sequence in our metric space. Take a subsequence such that for all $m, p > n$, $d(A_m, A_p) < 1/2^n$, where $A_1, A_2, ...$ denotes our subsequence. Let $A = \limsup M_i$. Then $A$ is also the $\limsup$ of the $A_i$'s, since $x$ belongs to infinitely many sets in one sequence if and only if it belongs to infinitely many in the other. Edit 2: The above bolded argument is erroneous. The $\limsup$ of the $A$'s should differ from the $\limsup$ of the $M$'s only on a set of measure zero, but we may very well lose elements when we pass to a $\limsup$ of the subsequence. Let $n \in N$. We use measure continuity from above on $A \setminus A_n$ and from below on $A_n \setminus A$. \begin{align*} \mu(A \setminus A_n) &= \mu(( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty A_j \setminus A_n))  \\ &= \mu( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty (A_j \setminus A_n))  \\ &= \lim_{k \rightarrow \infty} \mu(\bigcup_{j=k}^\infty (A_j \setminus A_n)) \end{align*} At this point I'm stuck - using subadditivity of $\mu$ does not help, as $A_j$ might grow further from $A_n$ as $j \rightarrow \infty$, so that the measure of their difference will grow, making the infinite sum infinite for every $k$. Bounding the other difference is quite simple, however. Using DeMorgan's laws: \begin{align*} \mu(A_n \setminus A) &= \mu(A_n \setminus ( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty A_j)) \\ &= \mu(A_n \cap ( \bigcap_{k \geq 1}^\infty \bigcup_{j=k}^\infty A_j)^c) \\ &= \mu( \bigcup_{k \geq 1}^\infty \bigcap_{j=k}^\infty (A_n \setminus A_j))  \\ &= \lim_{k \rightarrow \infty} \mu(\bigcap_{j=k}^\infty (A_n \setminus A_j)) \\ &< 1/2^n \end{align*} If we had the same bound on the first difference as well, then we'd have $\mu(A_n \triangle A) = \mu(A_n \setminus A) + \mu(A \setminus A_n) < 1/2^{n-1} \rightarrow 0$ as $n \rightarrow \infty$ Which would prove convergence of the subsequence to $A$. But a Cauchy sequence with a convergent subsequence converges to the same limit, which would prove the result.",,"['real-analysis', 'measure-theory', 'metric-spaces', 'cauchy-sequences']"
69,Existence of a Minimal Cover,Existence of a Minimal Cover,,"I'm well aware that for the sequence $x_n=\frac{1}{n}$, $\text{inf }x_n=0$ but $0 \notin (x_n)$. This made me think about something similar but when we are no longer thinking about existence of a number in a sequence but something a bit different. Consider the definition of exterior measure for a set $E \subset\mathbb{R}^d$, $$m_*(E)=\text{inf }\sum_{j=1}^\infty |Q_j|$$ where the infimum is taken over all countable coverings $E\subset \bigcup_{j=1}^\infty Q_j$ by closed cubes. Does this necessarily imply there is a cover $\{Q_\alpha^*\}_\alpha$ such that $$\text{inf }\sum_{j=1}^\infty \left|Q_j\right|=\sum_{j=1}^\infty \left|Q_j^*\right|$$ I suspect this is not always the case. I'm wondering about the following: $1.$ Can one find examples of a set $E\subset \mathbb{R}^d$ (nonfinite) such there is such a ""minimal"" cover $\{Q_\alpha^*\}$? $2$. What conditions - if any - on $E$ force there to always be such a ""minimal"" cover? $3$. What about when one removes the restriction of $E \subset \mathbb{R}^d$? Are there cases where one clearly can/can't find such ""minimal"" covers for general metric spaces?","I'm well aware that for the sequence $x_n=\frac{1}{n}$, $\text{inf }x_n=0$ but $0 \notin (x_n)$. This made me think about something similar but when we are no longer thinking about existence of a number in a sequence but something a bit different. Consider the definition of exterior measure for a set $E \subset\mathbb{R}^d$, $$m_*(E)=\text{inf }\sum_{j=1}^\infty |Q_j|$$ where the infimum is taken over all countable coverings $E\subset \bigcup_{j=1}^\infty Q_j$ by closed cubes. Does this necessarily imply there is a cover $\{Q_\alpha^*\}_\alpha$ such that $$\text{inf }\sum_{j=1}^\infty \left|Q_j\right|=\sum_{j=1}^\infty \left|Q_j^*\right|$$ I suspect this is not always the case. I'm wondering about the following: $1.$ Can one find examples of a set $E\subset \mathbb{R}^d$ (nonfinite) such there is such a ""minimal"" cover $\{Q_\alpha^*\}$? $2$. What conditions - if any - on $E$ force there to always be such a ""minimal"" cover? $3$. What about when one removes the restriction of $E \subset \mathbb{R}^d$? Are there cases where one clearly can/can't find such ""minimal"" covers for general metric spaces?",,"['real-analysis', 'general-topology', 'measure-theory', 'covering-spaces']"
70,Can we inverse the mean values theorem?,Can we inverse the mean values theorem?,,"The mean values theorem says that there exists a $c∈(u,v)$ such that $$f(v)-f(u)=f′(c)(v-u)$$ My question is: Can we inverse this situation, i.e., given the function $f$ and the real $c$, can we find $u,v$ such that $$f(v)-f(u)=f′(c)(v-u)$$ holds true?","The mean values theorem says that there exists a $c∈(u,v)$ such that $$f(v)-f(u)=f′(c)(v-u)$$ My question is: Can we inverse this situation, i.e., given the function $f$ and the real $c$, can we find $u,v$ such that $$f(v)-f(u)=f′(c)(v-u)$$ holds true?",,"['real-analysis', 'derivatives']"
71,Infinite self-convolution for a function,Infinite self-convolution for a function,,"I have a mathematical problem that leads me to a particular necessity. I need to calculate the convolution of a function for itself for a certain amount of times. So consider a generic function $f : \mathbb{R} \mapsto \mathbb{R}$ and consider these hypothesis: $f$ is continuos in $\mathbb{R}$. $f$ is bound, so: $\exists A \in \mathbb{R} : |f(x)| \leq A, \forall x \in \mathbb{R}$. $f$ is integral-defined, so its area is a real number: $\exists \int_a^bf(x)\mathrm{d}x < \infty, \forall a,b \in \mathbb{R}$. Which implies that such a function at ifinite tends to zero. Probability mass functions: Such functions fit the constraints given before. So it might get easier for you to consider $f$ also like the pmf of some continuos r.v. Consider the convolution operation: $a(x) \ast b(x) = c(x)$. I name the variable always $x$. Consider now the following function: $$ F^{(n)}(x) = f(x) \ast f(x) \ast \dots \ast f(x), \text{for n times} $$ I want to evaluate $F^{(\infty)}(x)$. And I would like to know whether there is a generic final result given a function like $f$. My trials I tried a little in Mathematica using the Gaussian distribution. What happens is that, as $n$ increases, the bell stretches and its peak always gets lower and lower until the function almost lies all over the x axis. It seems like $F^{(\infty)}(x)$ tends to $y=0$ function... As $n$ increases, the curves gets lower and lower.","I have a mathematical problem that leads me to a particular necessity. I need to calculate the convolution of a function for itself for a certain amount of times. So consider a generic function $f : \mathbb{R} \mapsto \mathbb{R}$ and consider these hypothesis: $f$ is continuos in $\mathbb{R}$. $f$ is bound, so: $\exists A \in \mathbb{R} : |f(x)| \leq A, \forall x \in \mathbb{R}$. $f$ is integral-defined, so its area is a real number: $\exists \int_a^bf(x)\mathrm{d}x < \infty, \forall a,b \in \mathbb{R}$. Which implies that such a function at ifinite tends to zero. Probability mass functions: Such functions fit the constraints given before. So it might get easier for you to consider $f$ also like the pmf of some continuos r.v. Consider the convolution operation: $a(x) \ast b(x) = c(x)$. I name the variable always $x$. Consider now the following function: $$ F^{(n)}(x) = f(x) \ast f(x) \ast \dots \ast f(x), \text{for n times} $$ I want to evaluate $F^{(\infty)}(x)$. And I would like to know whether there is a generic final result given a function like $f$. My trials I tried a little in Mathematica using the Gaussian distribution. What happens is that, as $n$ increases, the bell stretches and its peak always gets lower and lower until the function almost lies all over the x axis. It seems like $F^{(\infty)}(x)$ tends to $y=0$ function... As $n$ increases, the curves gets lower and lower.",,"['real-analysis', 'integration', 'convolution']"
72,Using Leibniz Integral Rule on infinite region,Using Leibniz Integral Rule on infinite region,,"I am trying to take the derivative with respect to $a$ of some function $I(a)=\int_{0}^{\infty}f(a,x)dx$.  I would like to make sure that I am using the Leiniz Integral Rule correctly.  Various web sources indicate a set of conditions that must hold for $f(x,a)$ and $\frac{\partial f(x,a)}{\partial a}$ when integration is done over infinite region.  From reading this source (see Theorem 10.3 on page 13) the conditions that $f(x,a)$ and $\frac{\partial f(x,a)}{\partial a}$ must obey are: $f(x,a)$ and $\frac{\partial f(x,a)}{\partial a}$ are continuous over $x\in[0,\infty)$ and around $a$ that we are interested in. There exists an integrable function (over $x$) $g(x)$ such that $|\frac{\partial f(x,a)}{\partial a}|\leq g(x)$. There exists an integrable function (over $x$) $h(x)$ such that $|f(x,a)|\leq h(x)$. Integrable here means $\int_{-\infty}^{\infty}g(x)dx<\infty$. However, another source seems to omit condition 3 above.  I am wondering which source is correct.  If there are ""both correct"", when is condition 3 necessary?","I am trying to take the derivative with respect to $a$ of some function $I(a)=\int_{0}^{\infty}f(a,x)dx$.  I would like to make sure that I am using the Leiniz Integral Rule correctly.  Various web sources indicate a set of conditions that must hold for $f(x,a)$ and $\frac{\partial f(x,a)}{\partial a}$ when integration is done over infinite region.  From reading this source (see Theorem 10.3 on page 13) the conditions that $f(x,a)$ and $\frac{\partial f(x,a)}{\partial a}$ must obey are: $f(x,a)$ and $\frac{\partial f(x,a)}{\partial a}$ are continuous over $x\in[0,\infty)$ and around $a$ that we are interested in. There exists an integrable function (over $x$) $g(x)$ such that $|\frac{\partial f(x,a)}{\partial a}|\leq g(x)$. There exists an integrable function (over $x$) $h(x)$ such that $|f(x,a)|\leq h(x)$. Integrable here means $\int_{-\infty}^{\infty}g(x)dx<\infty$. However, another source seems to omit condition 3 above.  I am wondering which source is correct.  If there are ""both correct"", when is condition 3 necessary?",,"['real-analysis', 'analysis']"
73,$1/|x|^n$ is not integrable,is not integrable,1/|x|^n,"Let $\mu $ be a positive Borel measure on $% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}$ such that $\mu \left( B\left( a,r\right) \right) \leq Cr^{n}$ for some  $n\in (0,d]$ and for any ball $B\left( a,r\right) $ in $% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}$. Could you help me to prove that $\int_{% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}}\frac{1}{\left\vert x\right\vert ^{n}}d\mu \left( x\right) =\infty $? My effort: $\int_{% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}}\frac{1}{\left\vert x\right\vert ^{n}}d\mu \left( x\right) \geq \int_{% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}\backslash B\left( 0,1\right) }\frac{1}{\left\vert x\right\vert ^{n}}% d\mu \left( x\right) =\sum_{k=0}^{\infty }\int_{B\left( 0,2^{k+1}\right) \backslash B\left( 0,2^{k}\right) }\frac{1}{\left\vert x\right\vert ^{n}}% d\mu \left( x\right) \geq \sum_{k=0}^{\infty }\frac{1}{\left( 2^{k+1}\right) ^{n}}\mu \left( B\left( 0,2^{k+1}\right) \backslash B\left( 0,2^{k}\right) \right) $.","Let $\mu $ be a positive Borel measure on $% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}$ such that $\mu \left( B\left( a,r\right) \right) \leq Cr^{n}$ for some  $n\in (0,d]$ and for any ball $B\left( a,r\right) $ in $% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}$. Could you help me to prove that $\int_{% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}}\frac{1}{\left\vert x\right\vert ^{n}}d\mu \left( x\right) =\infty $? My effort: $\int_{% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}}\frac{1}{\left\vert x\right\vert ^{n}}d\mu \left( x\right) \geq \int_{% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{d}\backslash B\left( 0,1\right) }\frac{1}{\left\vert x\right\vert ^{n}}% d\mu \left( x\right) =\sum_{k=0}^{\infty }\int_{B\left( 0,2^{k+1}\right) \backslash B\left( 0,2^{k}\right) }\frac{1}{\left\vert x\right\vert ^{n}}% d\mu \left( x\right) \geq \sum_{k=0}^{\infty }\frac{1}{\left( 2^{k+1}\right) ^{n}}\mu \left( B\left( 0,2^{k+1}\right) \backslash B\left( 0,2^{k}\right) \right) $.",,"['real-analysis', 'measure-theory', 'harmonic-analysis']"
74,Limit of an integral (remainder term of a Euler-Maclaurin expansion),Limit of an integral (remainder term of a Euler-Maclaurin expansion),,"For every $x > 0$, define $$I(x) = \int_1^\infty \left(\{t\} - \frac{1}{2}\right)\frac{x}{e^{xt}-1}\,dt.$$ where $\{x\} = x - \lfloor x\rfloor$ denotes the fractional part of $x$. How to justify that $I(x)$ converges to the improper integral $$ \int_1^\infty  \left(\{t\}-\frac{1}{2}\right)\frac{dt}{t} $$ when $x \searrow 0$ ? Of course $\dfrac{x}{e^{tx}-1}$ converges to $\dfrac{1}{t}$ for every $t$, but since $$ \int_1^\infty \left|\{t\} - \frac{1}{2}\right|\frac{dt}{t} = \sum_{n=1}^\infty \int_{-1/2}^{1/2} \frac{|u|\,du}{n+\frac{1}{2}+u} \geq \left(\sum_{n=2}^\infty \frac{1}{n}\right)\int_{-1/2}^{1/2}|u|\,du =  +\infty, $$ there is no hope to apply Lebesgue's theorem. I noticed that this is some type of Abelian-like result. Here is a proof. Let $f(t)=\left(\{t\}-\dfrac{1}{2}\right)\dfrac{1}{t}$. It is known that $F(T) = \int_1^T f(t)\,dt$ converges to $\ell = \dfrac{1}{2}\ln(2\pi)-1$ as $T$ tends to $+\infty$. Using the following integration by parts formula: $$\int_T^\infty f(t)\frac{tx}{e^{tx}-1}\,dt = -(F(T)-\ell)\frac{Tx}{e^{Tx}-1}-\int_T^\infty (F(t)-\ell)\frac{d}{dt}\frac{tx}{e^{tx}-1}\,dt$$  where $\dfrac{d}{dt}\dfrac{tx}{e^{tx}-1}=\dfrac{xe^{tx}(1-tx-e^{-tx})}{(e^{tx}-1)} \leq 0$, we get $$ \left|\int_T^\infty f(t)\dfrac{tx}{e^{tx}-1}\,dt\right| \leq 2\sup_{t\geq T}|F(t)-\ell|\dfrac{Tx}{e^{Tx}-1}\leq 2 \sup_{t\geq T}|F(t)-\ell|. $$ Hence, for every $T > 1$ and $x >0$, $$ \left|\int_1^\infty f(t)\dfrac{xt}{e^{xt}-1}\,dt - \ell\right| \leq \left|\int_1^T f(t)\left(1-\dfrac{xt}{e^{xt}-1}\right)\,dt\right| + 3\sup_{t\geq T}|F(t)-\ell|. $$ The result easily follows from this inequality.","For every $x > 0$, define $$I(x) = \int_1^\infty \left(\{t\} - \frac{1}{2}\right)\frac{x}{e^{xt}-1}\,dt.$$ where $\{x\} = x - \lfloor x\rfloor$ denotes the fractional part of $x$. How to justify that $I(x)$ converges to the improper integral $$ \int_1^\infty  \left(\{t\}-\frac{1}{2}\right)\frac{dt}{t} $$ when $x \searrow 0$ ? Of course $\dfrac{x}{e^{tx}-1}$ converges to $\dfrac{1}{t}$ for every $t$, but since $$ \int_1^\infty \left|\{t\} - \frac{1}{2}\right|\frac{dt}{t} = \sum_{n=1}^\infty \int_{-1/2}^{1/2} \frac{|u|\,du}{n+\frac{1}{2}+u} \geq \left(\sum_{n=2}^\infty \frac{1}{n}\right)\int_{-1/2}^{1/2}|u|\,du =  +\infty, $$ there is no hope to apply Lebesgue's theorem. I noticed that this is some type of Abelian-like result. Here is a proof. Let $f(t)=\left(\{t\}-\dfrac{1}{2}\right)\dfrac{1}{t}$. It is known that $F(T) = \int_1^T f(t)\,dt$ converges to $\ell = \dfrac{1}{2}\ln(2\pi)-1$ as $T$ tends to $+\infty$. Using the following integration by parts formula: $$\int_T^\infty f(t)\frac{tx}{e^{tx}-1}\,dt = -(F(T)-\ell)\frac{Tx}{e^{Tx}-1}-\int_T^\infty (F(t)-\ell)\frac{d}{dt}\frac{tx}{e^{tx}-1}\,dt$$  where $\dfrac{d}{dt}\dfrac{tx}{e^{tx}-1}=\dfrac{xe^{tx}(1-tx-e^{-tx})}{(e^{tx}-1)} \leq 0$, we get $$ \left|\int_T^\infty f(t)\dfrac{tx}{e^{tx}-1}\,dt\right| \leq 2\sup_{t\geq T}|F(t)-\ell|\dfrac{Tx}{e^{Tx}-1}\leq 2 \sup_{t\geq T}|F(t)-\ell|. $$ Hence, for every $T > 1$ and $x >0$, $$ \left|\int_1^\infty f(t)\dfrac{xt}{e^{xt}-1}\,dt - \ell\right| \leq \left|\int_1^T f(t)\left(1-\dfrac{xt}{e^{xt}-1}\right)\,dt\right| + 3\sup_{t\geq T}|F(t)-\ell|. $$ The result easily follows from this inequality.",,"['real-analysis', 'integration', 'limits']"
75,convolution of a function with itself equals itself,convolution of a function with itself equals itself,,"In a homework question,  I was asked to show: (1) in $L^1(R)$, if $f*f = f$, then $f$ must be a zero function. (2) In $L^2(R)$, find a function $f*f=f$. I don't know how to proceed. for (1), $f*f=f$ gives $\widehat{f*f}=\hat{f}$, which is equal to $\hat{f}\cdot\hat{f}=\hat{f}$, but this does not guarantee the result. I tried to prove by contradiction, no success.  for (2), I don't know where to proceed. Is there any help that I could get? Thanks.","In a homework question,  I was asked to show: (1) in $L^1(R)$, if $f*f = f$, then $f$ must be a zero function. (2) In $L^2(R)$, find a function $f*f=f$. I don't know how to proceed. for (1), $f*f=f$ gives $\widehat{f*f}=\hat{f}$, which is equal to $\hat{f}\cdot\hat{f}=\hat{f}$, but this does not guarantee the result. I tried to prove by contradiction, no success.  for (2), I don't know where to proceed. Is there any help that I could get? Thanks.",,"['real-analysis', 'analysis', 'functional-analysis', 'examples-counterexamples', 'convolution']"
76,How to find an irrational number in this case?,How to find an irrational number in this case?,,"From Baire category theorem, we see that $\mathbb{Q}$ can not be a $G_{\delta}$. But consider the following construction: Let us consider $\mathbb{Q}\cap [0,1]$, putting all the elements in the set in a sequence, denoted $\{a_n\}$. We define $$V_i=\bigcup_{j}[a_j-1/2^{i+j},a_j+1/2^{i+j}]\cap [0,1].$$ Notice that $\mathbb{Q}\subset V_i$. So we define $$V=\bigcap_{i} V_i.$$ We have $\mathbb{Q}\subset V$, and $V$  is a zero-measure set. Although it is clear that $V\neq \mathbb{Q}$, I cannot find any irrational number in $V$. Is there someway to find an irrational number that is in V, which proves $V\neq \mathbb{Q}$? Thank you.","From Baire category theorem, we see that $\mathbb{Q}$ can not be a $G_{\delta}$. But consider the following construction: Let us consider $\mathbb{Q}\cap [0,1]$, putting all the elements in the set in a sequence, denoted $\{a_n\}$. We define $$V_i=\bigcup_{j}[a_j-1/2^{i+j},a_j+1/2^{i+j}]\cap [0,1].$$ Notice that $\mathbb{Q}\subset V_i$. So we define $$V=\bigcap_{i} V_i.$$ We have $\mathbb{Q}\subset V$, and $V$  is a zero-measure set. Although it is clear that $V\neq \mathbb{Q}$, I cannot find any irrational number in $V$. Is there someway to find an irrational number that is in V, which proves $V\neq \mathbb{Q}$? Thank you.",,"['real-analysis', 'measure-theory']"
77,Distances between closed sets on metric spaces,Distances between closed sets on metric spaces,,"Which says that $\mathbb R^n$ the distance between a point $b$ and a set $X$ defined by$$ \inf \left\{ d(b,x) \mid x \in X \right\} $$ The proposition: If $X$ is closed, this distance is reached at some point in the set $X$. To prove it I assumed without loss of generality that $X$ is also bounded, because if not, it intersected with (e.g.) $$ B_b (n) = \left\{ x \in \mathbb R^n \mid d(b,x) \leqslant n \right\}, $$ where $n$ for example, can be chosen as the first natural number such that the intersection is not empty, and obviously the rest of the points of $X$, are at a greater distance, and will not be candidates. Then define $$ f(x) = \left| x - b \right| \quad \text{for }x \in X $$ and since the domain is compact we conclude the result, that reaches its minimum at some point in the set. This demonstration clearly not true for any metric space, because  being closed and bounded is not enough to be compact in general, but anyway maybe it can be shown in a more general way. That is my question, is this true?","Which says that $\mathbb R^n$ the distance between a point $b$ and a set $X$ defined by$$ \inf \left\{ d(b,x) \mid x \in X \right\} $$ The proposition: If $X$ is closed, this distance is reached at some point in the set $X$. To prove it I assumed without loss of generality that $X$ is also bounded, because if not, it intersected with (e.g.) $$ B_b (n) = \left\{ x \in \mathbb R^n \mid d(b,x) \leqslant n \right\}, $$ where $n$ for example, can be chosen as the first natural number such that the intersection is not empty, and obviously the rest of the points of $X$, are at a greater distance, and will not be candidates. Then define $$ f(x) = \left| x - b \right| \quad \text{for }x \in X $$ and since the domain is compact we conclude the result, that reaches its minimum at some point in the set. This demonstration clearly not true for any metric space, because  being closed and bounded is not enough to be compact in general, but anyway maybe it can be shown in a more general way. That is my question, is this true?",,"['real-analysis', 'metric-spaces']"
78,Chicken-Egg problem with Fubini’s theorem,Chicken-Egg problem with Fubini’s theorem,,"Fubini's theorem states that if you have a double integral over a function $f(x,y)$, then you can compute the integral as an iterated integral, if $f(x,y)$ is in $\mathcal{L}^1$. But to find out if $f$ is in $\mathcal{L}^1$ you need to compute the double integral. What am I missing? The examples I found all apply Fubini's theorem without checking that $f(x,y)$ is in $\mathcal{L}^1$. Many thanks for any clarification!","Fubini's theorem states that if you have a double integral over a function $f(x,y)$, then you can compute the integral as an iterated integral, if $f(x,y)$ is in $\mathcal{L}^1$. But to find out if $f$ is in $\mathcal{L}^1$ you need to compute the double integral. What am I missing? The examples I found all apply Fubini's theorem without checking that $f(x,y)$ is in $\mathcal{L}^1$. Many thanks for any clarification!",,"['real-analysis', 'measure-theory']"
79,Does the sequence converge? $\displaystyle a_n=n\sum_{k=1}^\infty\frac{1}{4^{2k+1}}\left(1-\frac{1}{4^k} \right)^{n-2}$,Does the sequence converge?,\displaystyle a_n=n\sum_{k=1}^\infty\frac{1}{4^{2k+1}}\left(1-\frac{1}{4^k} \right)^{n-2},"For every $n\geq 3$ , let $$ a_n=n\sum_{k=1}^\infty\frac{1}{4^{2k+1}}\left(1-\frac{1}{4^k} \right)^{n-2} $$ So for example, $$ a_3=3\sum_{k=1}^\infty 2^{-4k-2}-2^{-6k-2}=\frac{3}{4}\left(\frac{1}{15}-\frac{1}{63}\right) $$ I am curious if $a_n$ converges as $n\rightarrow\infty$ , and if so, to what limit. One thing I've tried is to simplify the terms using the binomial expansion: $$\begin{align} a_n&=n\sum_{k=1}^\infty2^{-2(2k+1)}\sum_{m=0}^{n-2}{n-2\choose m}(-2^{-2k})^m\\&=\frac{n}{4}\sum_{m=0}^{n-2}{n-2\choose m}(-1)^m\sum_{k=1}^\infty 2^{-(2m+4)k}\\&=\frac{n}{4}\sum_{m=0}^{n-2}{n-2\choose m}\frac{(-1)^m}{4^{m+2}-1}\end{align} $$ But I don't know how to simplify it further and to evaluate whether or not it converges. Any hint would be greatly appreciated.","For every , let So for example, I am curious if converges as , and if so, to what limit. One thing I've tried is to simplify the terms using the binomial expansion: But I don't know how to simplify it further and to evaluate whether or not it converges. Any hint would be greatly appreciated.","n\geq 3 
a_n=n\sum_{k=1}^\infty\frac{1}{4^{2k+1}}\left(1-\frac{1}{4^k} \right)^{n-2}
 
a_3=3\sum_{k=1}^\infty 2^{-4k-2}-2^{-6k-2}=\frac{3}{4}\left(\frac{1}{15}-\frac{1}{63}\right)
 a_n n\rightarrow\infty \begin{align}
a_n&=n\sum_{k=1}^\infty2^{-2(2k+1)}\sum_{m=0}^{n-2}{n-2\choose m}(-2^{-2k})^m\\&=\frac{n}{4}\sum_{m=0}^{n-2}{n-2\choose m}(-1)^m\sum_{k=1}^\infty 2^{-(2m+4)k}\\&=\frac{n}{4}\sum_{m=0}^{n-2}{n-2\choose m}\frac{(-1)^m}{4^{m+2}-1}\end{align}
","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'convergence-divergence']"
80,"For which values of the parameter, $f(x, y)$ is differentiable at $(0, 0)$?","For which values of the parameter,  is differentiable at ?","f(x, y) (0, 0)","For which values of $\alpha$ , the following function is differentiable at the origin? $$f(x, y) = \begin{cases} \frac{x^2y}{(x^4+y^2)^{\alpha}} & (x, y) \neq (0, 0) \\\\ 0 & (x, y) = (0, 0)\end{cases}$$ attempts I know that continuity is not a sufficient condition for a function to be differentiable at a point, yet it's necessary because if $f$ is not continuous then it's not differentiable (at a point). Then I at least need the continuity, when restricting to the lines though the origin $y = mx$ I get $$\lim_{x\to 0} f(x, mx) = \lim_{x\to 0} \frac{mx^3}{(x^{2\alpha}(x^2+m^2)} = 0 \quad \iff \alpha < \frac{3}{2}$$ Now, I know that for some $f$ , if $\exists$ the partial derivatives at a point of $f$ and they are continuous too, then $f$ is differentiable at that point. Is there a rapider way to check for the differentiability when $f$ depends on a parameter, or should I check for existence and continuity of $\partial f$ ? Thank you.","For which values of , the following function is differentiable at the origin? attempts I know that continuity is not a sufficient condition for a function to be differentiable at a point, yet it's necessary because if is not continuous then it's not differentiable (at a point). Then I at least need the continuity, when restricting to the lines though the origin I get Now, I know that for some , if the partial derivatives at a point of and they are continuous too, then is differentiable at that point. Is there a rapider way to check for the differentiability when depends on a parameter, or should I check for existence and continuity of ? Thank you.","\alpha f(x, y) = \begin{cases} \frac{x^2y}{(x^4+y^2)^{\alpha}} & (x, y) \neq (0, 0) \\\\ 0 & (x, y) = (0, 0)\end{cases} f y = mx \lim_{x\to 0} f(x, mx) = \lim_{x\to 0} \frac{mx^3}{(x^{2\alpha}(x^2+m^2)} = 0 \quad \iff \alpha < \frac{3}{2} f \exists f f f \partial f","['real-analysis', 'multivariable-calculus', 'derivatives']"
81,An interesting series related to primes satisfying $\sum_n x_{nk} = 0$ for all $k$,An interesting series related to primes satisfying  for all,\sum_n x_{nk} = 0 k,"Consider the series $\sum x_n$ where if $n$ is composite, say $n=p_1^{k_1}....p_m^{k_m}$ then $x_n=x_{p_1}^{k_1}....x_{p_m}^{k_m}$ and if $n$ is prime, $x_p=-x_1-...-x_{p-1}$ . I want to determine whether an $x_1\neq 0$ exists such that $\sum x_n=0$ . It feels like this will be true for all $|x_1|<1$ or none(more clear after getting a feel for the series). This would have the interesting consequence that $\sum x_{kn}=0$ for all integers $k$ . I have also written/copied from the internet a generator for the sequence in python, found here https://pastebin.com/jcqvSS0W The sequence feels more related to prime gaps rather than anything to do with primes being prime, which makes me suspect this is a difficult question.","Consider the series where if is composite, say then and if is prime, . I want to determine whether an exists such that . It feels like this will be true for all or none(more clear after getting a feel for the series). This would have the interesting consequence that for all integers . I have also written/copied from the internet a generator for the sequence in python, found here https://pastebin.com/jcqvSS0W The sequence feels more related to prime gaps rather than anything to do with primes being prime, which makes me suspect this is a difficult question.",\sum x_n n n=p_1^{k_1}....p_m^{k_m} x_n=x_{p_1}^{k_1}....x_{p_m}^{k_m} n x_p=-x_1-...-x_{p-1} x_1\neq 0 \sum x_n=0 |x_1|<1 \sum x_{kn}=0 k,"['real-analysis', 'sequences-and-series', 'number-theory', 'prime-numbers']"
82,Counterexample to Tonelli's theorem,Counterexample to Tonelli's theorem,,"Does there exist a $C^{\infty}$ function $f: \mathbb{R}^2 \to \mathbb{R}$ such that $\int dx \int dy \,f(x,y)$ diverges to infinity but $\int dy \int dx\, f(x,y)$ converges? Note that if $f$ is non-negative this is impossible by Tonelli's theorem.",Does there exist a function such that diverges to infinity but converges? Note that if is non-negative this is impossible by Tonelli's theorem.,"C^{\infty} f: \mathbb{R}^2 \to \mathbb{R} \int dx \int dy \,f(x,y) \int dy \int dx\, f(x,y) f","['real-analysis', 'integration', 'examples-counterexamples', 'fubini-tonelli-theorems']"
83,Continuous function with irrational period - summation over naturals converges to 0?,Continuous function with irrational period - summation over naturals converges to 0?,,"A friend gave me this problem as a ""Christmas gift"": If $f$ is periodic with irrational period $r$ and $\int_0^r f(x) \mathrm{d}x = 0$ , prove or disprove that $\left\{\sum\limits_{i=1}^n f(i)\right\}_{n \in \mathbb{N}}$ is bounded. One construction that works is $f(x) = \begin{cases} 1 &\mbox{if } x \equiv k \pmod{r}, k \in \mathbb{Z} \\ 0 & \mbox{otherwise } \end{cases}$ (I guess this only works if we use the Lebesgue integral); another is $f(x) = \tan(x-\frac{\pi}{2})$ . However, he accidentally forgot to specify that $f$ must be continuous, which invalidates both of these constructions. Since $\mathbb{N} \pmod{r}$ is uniformly distributed in $[0, r)$ , I'm pretty sure that we have $$\lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^{n}f(i) = \int_{0}^{r}f(x)\mathrm{d}x = 0.$$ Unfortunately, this is weaker than the original statement. My question is, is the original statement true, and if not, what's a counterexample? I'm not sure if this problem can be solved with elementary knowledge, or if it would require some more theory.","A friend gave me this problem as a ""Christmas gift"": If is periodic with irrational period and , prove or disprove that is bounded. One construction that works is (I guess this only works if we use the Lebesgue integral); another is . However, he accidentally forgot to specify that must be continuous, which invalidates both of these constructions. Since is uniformly distributed in , I'm pretty sure that we have Unfortunately, this is weaker than the original statement. My question is, is the original statement true, and if not, what's a counterexample? I'm not sure if this problem can be solved with elementary knowledge, or if it would require some more theory.","f r \int_0^r f(x) \mathrm{d}x = 0 \left\{\sum\limits_{i=1}^n f(i)\right\}_{n \in \mathbb{N}} f(x) = \begin{cases} 1 &\mbox{if } x \equiv k \pmod{r}, k \in \mathbb{Z} \\
0 & \mbox{otherwise } \end{cases} f(x) = \tan(x-\frac{\pi}{2}) f \mathbb{N} \pmod{r} [0, r) \lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^{n}f(i) = \int_{0}^{r}f(x)\mathrm{d}x = 0.",['real-analysis']
84,Evaluate $\lim\limits_{n \to \infty} nx_n.$,Evaluate,\lim\limits_{n \to \infty} nx_n.,"Suppose $$x_2 \in \left(0,\frac{\pi}{2}\right), x_{n+1}=\left(1-\frac{1}{n}\right)\sin x_n(n \ge 2).$$ Evaluate $\lim\limits_{n \to \infty} nx_n.$ Note that $$x_{n+1}=\left(1-\frac{1}{n}\right)\sin x_n\le \sin x_n\le x_n,$$ and $$|x_{n+1}|=\left|\left(1-\frac{1}{n}\right)\sin x_n\right|\le 1,$$ it follows that $\{x_n\}$ converges. Therefore, we can readily obtain $x_n \to 0$ . As for the limit wanted, we can consider apply Stolz theorem . But it's too complicated.","Suppose Evaluate Note that and it follows that converges. Therefore, we can readily obtain . As for the limit wanted, we can consider apply Stolz theorem . But it's too complicated.","x_2 \in \left(0,\frac{\pi}{2}\right), x_{n+1}=\left(1-\frac{1}{n}\right)\sin x_n(n \ge 2). \lim\limits_{n \to \infty} nx_n. x_{n+1}=\left(1-\frac{1}{n}\right)\sin x_n\le \sin x_n\le x_n, |x_{n+1}|=\left|\left(1-\frac{1}{n}\right)\sin x_n\right|\le 1, \{x_n\} x_n \to 0","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
85,Is the collection $\mathcal{M}$ of $\mu$-measurable sets maximal so that $\mu|_{\mathcal{M}}$ is a measure?,Is the collection  of -measurable sets maximal so that  is a measure?,\mathcal{M} \mu \mu|_{\mathcal{M}},"Let $\mu:2^{X} \to [0, \infty]$ be an outer measure. The collection $\mathcal{M}$ of $\mu$ -measurable sets are then defined as those sets $A$ satisfying $\mu(S)=\mu(S \cap A) + \mu(S \setminus A)$ for each $S \subset X$ . It's proven in any measure theory course that $\mathcal{M}$ is a $\sigma$ -algebra, with $\mu|_{\mathcal{M}}$ a complete measure. I've always found this to be an elusive definition. This has been discussed elsewhere . A more natural approach to defining the collection $\mathcal{M}$ of $\mu$ -measurable sets, in my mind, is that we want it to satisfy the following property: Let $\mathcal{U}_{\mu} \subset 2^{2^{X}}$ denote the collection of $\sigma$ -algebras $\mathcal{F}$ of $X$ with the property that $\mu|_{\mathcal{F}}$ is a complete measure on $\mathcal{F}$ . Then $\mathcal{M}$ is inclusion-wise maximal in $\mathcal{U}_{\mu}$ . In simple terms, we want $\mathcal{M}$ to be the largest set possible on which $\mu$ is a measure. This isn't a good definition for $\mathcal{M}$ , unfortunately, since we have no guarantee a priori that $\mathcal{U}_{\mu}$ has a unique maximal element. So I have some questions: Is $\mathcal{M}$ , as defined in the first paragraph, in fact inclusion-wise maximal in $\mathcal{U}_{\mu}$ ? Does $\mathcal{U}_{\mu}$ has a unique inclusion-wise maximal element? If the question to the latter question is ""no"", then what can we say about inclusion-wise maximal members of $\mathcal{U}_{\mu}$ distinct from $\mathcal{M}$ as defined in paragraph one?","Let be an outer measure. The collection of -measurable sets are then defined as those sets satisfying for each . It's proven in any measure theory course that is a -algebra, with a complete measure. I've always found this to be an elusive definition. This has been discussed elsewhere . A more natural approach to defining the collection of -measurable sets, in my mind, is that we want it to satisfy the following property: Let denote the collection of -algebras of with the property that is a complete measure on . Then is inclusion-wise maximal in . In simple terms, we want to be the largest set possible on which is a measure. This isn't a good definition for , unfortunately, since we have no guarantee a priori that has a unique maximal element. So I have some questions: Is , as defined in the first paragraph, in fact inclusion-wise maximal in ? Does has a unique inclusion-wise maximal element? If the question to the latter question is ""no"", then what can we say about inclusion-wise maximal members of distinct from as defined in paragraph one?","\mu:2^{X} \to [0, \infty] \mathcal{M} \mu A \mu(S)=\mu(S \cap A) + \mu(S \setminus A) S \subset X \mathcal{M} \sigma \mu|_{\mathcal{M}} \mathcal{M} \mu \mathcal{U}_{\mu} \subset 2^{2^{X}} \sigma \mathcal{F} X \mu|_{\mathcal{F}} \mathcal{F} \mathcal{M} \mathcal{U}_{\mu} \mathcal{M} \mu \mathcal{M} \mathcal{U}_{\mu} \mathcal{M} \mathcal{U}_{\mu} \mathcal{U}_{\mu} \mathcal{U}_{\mu} \mathcal{M}","['real-analysis', 'measure-theory']"
86,The sequence $A_n=\prod_{k=1}^n\left(1+\frac{k}{n^2}\right)$ is decreasing,The sequence  is decreasing,A_n=\prod_{k=1}^n\left(1+\frac{k}{n^2}\right),"Let $A$ be the sequence of real numbers defined by : $$\forall n\in\mathbb{N}^\star,\,A_n=\prod_{k=1}^n\left(1+\frac{k}{n^2}\right)$$ I know how to prove that this sequence converges to $\sqrt e$ , using the following inequalities : $$\forall t>0,\,t-\frac{t^2}2\leqslant\ln(1+t)\leqslant t$$ I found numerical evidence that $(A_n)$ is decreasing, but wasn't able to prove it. Any help will be appreciated.","Let be the sequence of real numbers defined by : I know how to prove that this sequence converges to , using the following inequalities : I found numerical evidence that is decreasing, but wasn't able to prove it. Any help will be appreciated.","A \forall n\in\mathbb{N}^\star,\,A_n=\prod_{k=1}^n\left(1+\frac{k}{n^2}\right) \sqrt e \forall t>0,\,t-\frac{t^2}2\leqslant\ln(1+t)\leqslant t (A_n)","['real-analysis', 'sequences-and-series', 'inequality', 'products']"
87,Evaluate $\int _0^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx$ with real methods,Evaluate  with real methods,\int _0^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx,"I started like this: $$\int _0^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx\: =\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx+\overset {x=\frac{1}{x}}{\int _1^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx}$$ $$=2\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx+5G$$ where $G$ is Catalan's constant. But the integral left is very hard to calculate. As suggested by Zacky one can use the sub $x=\frac{1-t}{1+t}$ and get these integrals. $$\ln 2\int _0^1\frac{1}{1+t^2}\:dt+\ln 5\int _0^1\frac{1}{1+t^2}\:dt+\int _0^1\frac{\ln \left(t^2+1-\frac{2}{\sqrt{5}}\right)}{1+t^2}\:dt$$ $$+\int _0^1\frac{\ln \left(t^2+1+\frac{2}{\sqrt{5}}\right)}{1+t^2}\:dt-5\int _0^1\frac{\ln \left(1+t\right)}{1+t^2}\:dt$$ To evaluate those one can use the identity $$\int _0^1\frac{\ln \left(b+ax^2\right)}{1+x^2}\:dx=\frac{\pi }{2}\ln \left(\sqrt{a}+\sqrt{b}\right)+\text{Ti}_2\left(\frac{\sqrt{a}-\sqrt{b}}{\sqrt{a}+\sqrt{b}}\right)-G$$ In the end, the integral evaluates to \begin{align} \int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx &= -\frac{3\pi }{8}\ln 2+\frac{\pi }{4}\ln 5-2G\\ &+\frac{\pi }{2}\ln \left(1+\sqrt{1-\frac{2}{\sqrt{5}}}\right)+\text{Ti}_2\left(\frac{\sqrt{1-\frac{2}{\sqrt{5}}}-1}{\sqrt{1-\frac{2}{\sqrt{5}}}+1}\right)\\ &+\frac{\pi }{2}\ln \left(1+\sqrt{1+\frac{2}{\sqrt{5}}}\right)+\text{Ti}_2\left(\frac{\sqrt{1+\frac{2}{\sqrt{5}}}-1}{\sqrt{1+\frac{2}{\sqrt{5}}}+1}\right) \end{align} But, I have no idea how to simplify the $\text{Ti}_2\left(z\right)$ terms, which seems possible because I found that a similar version can be expressed without these $$\int _0^{\infty }\frac{\ln \left(1+x^5\right)}{\left(1+x^2\right)^2}\:dx=-\frac{5\pi }{8}-\frac{7\pi }{40}\ln \left(2\right)+\frac{\pi }{5}\ln \left(4+\sqrt{10-2\sqrt{5}}\right) +\frac{\pi }{10}\ln \left(43+7\sqrt{5}+4\sqrt{130+38\sqrt{5}}\right)+\frac{G}{10}$$ NB: $\displaystyle x\in\mathbb{R},\text{Ti}_2(x)=\int_0^x \frac{\arctan t}{t}dt$","I started like this: where is Catalan's constant. But the integral left is very hard to calculate. As suggested by Zacky one can use the sub and get these integrals. To evaluate those one can use the identity In the end, the integral evaluates to But, I have no idea how to simplify the terms, which seems possible because I found that a similar version can be expressed without these NB:","\int _0^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx\:
=\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx+\overset {x=\frac{1}{x}}{\int _1^{\infty }\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx} =2\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx+5G G x=\frac{1-t}{1+t} \ln 2\int _0^1\frac{1}{1+t^2}\:dt+\ln 5\int _0^1\frac{1}{1+t^2}\:dt+\int _0^1\frac{\ln \left(t^2+1-\frac{2}{\sqrt{5}}\right)}{1+t^2}\:dt +\int _0^1\frac{\ln \left(t^2+1+\frac{2}{\sqrt{5}}\right)}{1+t^2}\:dt-5\int _0^1\frac{\ln \left(1+t\right)}{1+t^2}\:dt \int _0^1\frac{\ln \left(b+ax^2\right)}{1+x^2}\:dx=\frac{\pi }{2}\ln \left(\sqrt{a}+\sqrt{b}\right)+\text{Ti}_2\left(\frac{\sqrt{a}-\sqrt{b}}{\sqrt{a}+\sqrt{b}}\right)-G \begin{align}
\int _0^1\frac{\ln \left(x^5+1\right)}{x^2+1}\:dx
&= -\frac{3\pi }{8}\ln 2+\frac{\pi }{4}\ln 5-2G\\
&+\frac{\pi }{2}\ln \left(1+\sqrt{1-\frac{2}{\sqrt{5}}}\right)+\text{Ti}_2\left(\frac{\sqrt{1-\frac{2}{\sqrt{5}}}-1}{\sqrt{1-\frac{2}{\sqrt{5}}}+1}\right)\\
&+\frac{\pi }{2}\ln \left(1+\sqrt{1+\frac{2}{\sqrt{5}}}\right)+\text{Ti}_2\left(\frac{\sqrt{1+\frac{2}{\sqrt{5}}}-1}{\sqrt{1+\frac{2}{\sqrt{5}}}+1}\right)
\end{align} \text{Ti}_2\left(z\right) \int _0^{\infty }\frac{\ln \left(1+x^5\right)}{\left(1+x^2\right)^2}\:dx=-\frac{5\pi }{8}-\frac{7\pi }{40}\ln \left(2\right)+\frac{\pi }{5}\ln \left(4+\sqrt{10-2\sqrt{5}}\right)
+\frac{\pi }{10}\ln \left(43+7\sqrt{5}+4\sqrt{130+38\sqrt{5}}\right)+\frac{G}{10} \displaystyle x\in\mathbb{R},\text{Ti}_2(x)=\int_0^x \frac{\arctan t}{t}dt","['real-analysis', 'calculus', 'integration', 'improper-integrals']"
88,Prove that $a_1 \cos(b_1 x) + \dots + a_n \cos(b_n x)$ has zero,Prove that  has zero,a_1 \cos(b_1 x) + \dots + a_n \cos(b_n x),"Given is function $$f(x)=a_1 \cos(b_1 x) + \dots + a_n \cos(b_n x) $$ where $\forall_{i} a_i > 0 $ and $\forall_{i} b_i > 0 $ Prove that $$\exists_{x_0} f(x_0) = 0 $$ My try My plan: 1. Find a function $g$ such that $g' = f$ 2. Prove that $\exists_{x_1, x_2} g(x_1) = g(x_2) = 0 $ 3. From Rolle's theorem it follows that $$\exists_{c \in (x_1,x_2)} g'(c)=f(c)=0$$ 1. $$g(x) = \frac{a_1}{b_1} \sin(b_1 x) + \dots + \frac{a_n}{b_n} \sin(b_n x)  $$ 2. Without lost of generality: $$b_1 \ge \dots \ge b_n$$ First zero is just $g(0) = 0$ But what about second 0? I think that I can find two arguments such that $g(arg_1) >0$ $g(arg_2) < 0$ and use Darboux theorem. arg1 $$ g(\frac{\pi}{b_1}) = \frac{a_1}{b_1} \sin(\frac{b_1 \cdot\pi}{b_1}) + \dots + \frac{a_n}{b_n} \sin(\frac{b_n \cdot\pi}{b_1}) $$ each argument is between $ 0 $ and $\pi$ so it will be greater than $0$ stuck But I can't find second argument...",Given is function where and Prove that My try My plan: 1. Find a function such that 2. Prove that 3. From Rolle's theorem it follows that 1. 2. Without lost of generality: First zero is just But what about second 0? I think that I can find two arguments such that and use Darboux theorem. arg1 each argument is between and so it will be greater than stuck But I can't find second argument...,"f(x)=a_1 \cos(b_1 x) + \dots + a_n \cos(b_n x)  \forall_{i} a_i > 0  \forall_{i} b_i > 0  \exists_{x_0} f(x_0) = 0  g g' = f \exists_{x_1, x_2} g(x_1) = g(x_2) = 0  \exists_{c \in (x_1,x_2)} g'(c)=f(c)=0 g(x) = \frac{a_1}{b_1} \sin(b_1 x) + \dots + \frac{a_n}{b_n} \sin(b_n x)   b_1 \ge \dots \ge b_n g(0) = 0 g(arg_1) >0 g(arg_2) < 0  g(\frac{\pi}{b_1}) = \frac{a_1}{b_1} \sin(\frac{b_1 \cdot\pi}{b_1}) + \dots + \frac{a_n}{b_n} \sin(\frac{b_n \cdot\pi}{b_1})   0  \pi 0","['real-analysis', 'derivatives', 'trigonometry']"
89,$n^3+n<3^n$ for $n \geq4$ by induction.,for  by induction.,n^3+n<3^n n \geq4,To prove that $n^3+n<3^n$ for $n \geq4$ by induction. I have proved the fact but it became very long and I have to use two more induction proof within the proof. Can someone give a better solution by induction? Thank You.,To prove that $n^3+n<3^n$ for $n \geq4$ by induction. I have proved the fact but it became very long and I have to use two more induction proof within the proof. Can someone give a better solution by induction? Thank You.,,"['calculus', 'real-analysis', 'number-theory', 'induction']"
90,Non-negative concave functions are non-decreasing,Non-negative concave functions are non-decreasing,,"Let $f : [0, \infty) \longrightarrow [0, \infty)$ be concave with $f (0) = 0$ and $f (x) > 0$ for $x > 0$. Show that $f$ is non-decreasing. It is clear that a concave function $f$ can be decreasing. Intuitively, $f$ must then become negative beyond some point, though. Once it has turned ""right"" (i.e., downwards) concavity prevents it from avoiding the abscissa. I'm looking for a reasonably elegant proof. I've tried to formalize my intuition as follows, but I'm afraid my argument isn't very rigorous (if it is right in the first place). Maybe there are also different and better approaches than mine. Here's my attempt: Concacity implies that $f (\lambda x + (1 - \lambda) z) \ge \lambda f (x) + f ((1 - \lambda) z)$ for $0 \le x < z$ and $\lambda \in (0, 1)$ or, alternatively, $$\frac{f (z) - f (y)}{z - y} \le \frac{f (z) - f (x)}{z - x} \le \frac{f (y) - f (x)}{y - x}$$ for $0 \le x < y < z$. (Let $1 - \lambda := \frac{y - x}{z - x}$, so that $\lambda x + (1 - \lambda) z = y$ to restate concavity without $\lambda$.) It follows that $$f (z) \le \frac{f (y) - f (x)}{y - x} (z - y) + f (y).$$ Graphically speaking, $f (z)$ must be on or below the secant through $f (x)$ and $f (y)$ for all $z > y$. Suppose that $f (y) < f (x)$ for some $x < y$. The secant is then strictly decreasing—and so is $f$ for $z > y$. As a result, $f$ must turn negative where the secant intersects the abscissa or before. This contradicts the assumption. Hence, $f (y) \ge f (x)$ for all $0 \le x < y$. Note: I feel that the argument requires $f$ to be continuous, although I don't see how to apply the intermediate value theorem rigorously. Continuity follows from concavity, though.","Let $f : [0, \infty) \longrightarrow [0, \infty)$ be concave with $f (0) = 0$ and $f (x) > 0$ for $x > 0$. Show that $f$ is non-decreasing. It is clear that a concave function $f$ can be decreasing. Intuitively, $f$ must then become negative beyond some point, though. Once it has turned ""right"" (i.e., downwards) concavity prevents it from avoiding the abscissa. I'm looking for a reasonably elegant proof. I've tried to formalize my intuition as follows, but I'm afraid my argument isn't very rigorous (if it is right in the first place). Maybe there are also different and better approaches than mine. Here's my attempt: Concacity implies that $f (\lambda x + (1 - \lambda) z) \ge \lambda f (x) + f ((1 - \lambda) z)$ for $0 \le x < z$ and $\lambda \in (0, 1)$ or, alternatively, $$\frac{f (z) - f (y)}{z - y} \le \frac{f (z) - f (x)}{z - x} \le \frac{f (y) - f (x)}{y - x}$$ for $0 \le x < y < z$. (Let $1 - \lambda := \frac{y - x}{z - x}$, so that $\lambda x + (1 - \lambda) z = y$ to restate concavity without $\lambda$.) It follows that $$f (z) \le \frac{f (y) - f (x)}{y - x} (z - y) + f (y).$$ Graphically speaking, $f (z)$ must be on or below the secant through $f (x)$ and $f (y)$ for all $z > y$. Suppose that $f (y) < f (x)$ for some $x < y$. The secant is then strictly decreasing—and so is $f$ for $z > y$. As a result, $f$ must turn negative where the secant intersects the abscissa or before. This contradicts the assumption. Hence, $f (y) \ge f (x)$ for all $0 \le x < y$. Note: I feel that the argument requires $f$ to be continuous, although I don't see how to apply the intermediate value theorem rigorously. Continuity follows from concavity, though.",,['real-analysis']
91,Does $f'(x)>0$ a.e. imply that $f$ is strictly monotone?,Does  a.e. imply that  is strictly monotone?,f'(x)>0 f,"Let us assume that $f:\mathbb{R}\to \mathbb{R}$ is differentiable and $f'(x)>0$ almost everywhere. If $f'\in L^1_{loc}$, then FTC implies that for any $x,a\in \mathbb{R}$, $$ f(x)-f(a)=\int_a^x f'(t)dt. $$ Therefore, we have $f(x)\geq f(a)$, because $\int_a^x f'(t)dt\geq \frac{1}{n} m(\{x;f'(x)>\frac 1 n\})$ for any $n\in \mathbb{N}$. Then we know that $f$ has to be strictly monotone. Otherwise, there exits an interval $(c,d)\subset [a,b]$ such that $f(x)$ is constant over $(c,d)$, which is impossibe because $f'(x)=0$ is zero over this interval and hence $f'(x)>0$ a.e. fails. How about removing the assumption that $f'\in L^1_{loc}$? Does $f'(x)>0$ almost everywhere imply that $f$ is strictly monotone","Let us assume that $f:\mathbb{R}\to \mathbb{R}$ is differentiable and $f'(x)>0$ almost everywhere. If $f'\in L^1_{loc}$, then FTC implies that for any $x,a\in \mathbb{R}$, $$ f(x)-f(a)=\int_a^x f'(t)dt. $$ Therefore, we have $f(x)\geq f(a)$, because $\int_a^x f'(t)dt\geq \frac{1}{n} m(\{x;f'(x)>\frac 1 n\})$ for any $n\in \mathbb{N}$. Then we know that $f$ has to be strictly monotone. Otherwise, there exits an interval $(c,d)\subset [a,b]$ such that $f(x)$ is constant over $(c,d)$, which is impossibe because $f'(x)=0$ is zero over this interval and hence $f'(x)>0$ a.e. fails. How about removing the assumption that $f'\in L^1_{loc}$? Does $f'(x)>0$ almost everywhere imply that $f$ is strictly monotone",,['real-analysis']
92,How to prove the Lebesgue density theorem using martingales?,How to prove the Lebesgue density theorem using martingales?,,"The Lebesgue density theorem says that for almost every $x \in A \subset [0,1]$ with $A$ Lebesgue measurable $$\lim_{h \to 0^+} \frac{|A \cap (x-h, x+h) |}{2h}=1 \tag{1}.$$ Here, ""almost every"" is with respect to Lebesgue measure, which we denote by $|\cdot|$. This looks an awful lot like martingale convergence and I thought I could prove (1) with martingale methods, but I need a little help concluding. First, if $|A|=0$, then (1) is immediate, so it suffices to prove the result for $A$ in the Borel sigma-algebra $\mathscr{B}$ of $[0,1]$. Switching to ""probabilistic notation"", let $(\Omega, \mathscr{B}, P)$ be Lebesgue measure on the Borel subsets of $[0,1]$. We need a filtration $(\mathcal{F}_n)$ that generates $\mathscr{B}$. We'll use the standard dyadic partitions, that is, each $\mathcal{F}_n$ is generated by the partition $\{[0, 1/2^n),[1/2^n, 2/2^n)...,[1 - 1/2^n , 1) \}$. Then, by martingale convergence for conditional probabilities, for almost every $x$ $$P(A \mid \mathcal{F}_n)(x) \to \mathbf{1}_A(x) \tag{2}.$$ Let $F_n(x)$ be the cell of the partition generating $\mathcal{F}_n$ that contains $x$. By the properties of the conditional probability and (2), for almost every $x \in A$ $$\frac{P(A \cap F_n(x))}{P(F_n(x))} \to 1 \tag{3}.$$ This looks a lot like (1), but I'm having trouble concluding. The difficulty is that (1) considers every open interval centered at $x$, whereas (3) considers only dyadic intervals. Now, it seems likely that (1) follows rather quickly by some argument about approximating open intervals by dyadic ones, but I'm having trouble saying this in a precise way. Any help on this point would be appreciated.","The Lebesgue density theorem says that for almost every $x \in A \subset [0,1]$ with $A$ Lebesgue measurable $$\lim_{h \to 0^+} \frac{|A \cap (x-h, x+h) |}{2h}=1 \tag{1}.$$ Here, ""almost every"" is with respect to Lebesgue measure, which we denote by $|\cdot|$. This looks an awful lot like martingale convergence and I thought I could prove (1) with martingale methods, but I need a little help concluding. First, if $|A|=0$, then (1) is immediate, so it suffices to prove the result for $A$ in the Borel sigma-algebra $\mathscr{B}$ of $[0,1]$. Switching to ""probabilistic notation"", let $(\Omega, \mathscr{B}, P)$ be Lebesgue measure on the Borel subsets of $[0,1]$. We need a filtration $(\mathcal{F}_n)$ that generates $\mathscr{B}$. We'll use the standard dyadic partitions, that is, each $\mathcal{F}_n$ is generated by the partition $\{[0, 1/2^n),[1/2^n, 2/2^n)...,[1 - 1/2^n , 1) \}$. Then, by martingale convergence for conditional probabilities, for almost every $x$ $$P(A \mid \mathcal{F}_n)(x) \to \mathbf{1}_A(x) \tag{2}.$$ Let $F_n(x)$ be the cell of the partition generating $\mathcal{F}_n$ that contains $x$. By the properties of the conditional probability and (2), for almost every $x \in A$ $$\frac{P(A \cap F_n(x))}{P(F_n(x))} \to 1 \tag{3}.$$ This looks a lot like (1), but I'm having trouble concluding. The difficulty is that (1) considers every open interval centered at $x$, whereas (3) considers only dyadic intervals. Now, it seems likely that (1) follows rather quickly by some argument about approximating open intervals by dyadic ones, but I'm having trouble saying this in a precise way. Any help on this point would be appreciated.",,"['real-analysis', 'probability-theory', 'measure-theory', 'martingales']"
93,$H^1$ compactly embedded in weighted $L^2$ space,compactly embedded in weighted  space,H^1 L^2,"I'm working on the following assignment: Define the weighted $L^2$-space $L^2_s$ on $\mathbb{R}^n$ to be the completion of $C_{\text{comp}}^{\infty}(\mathbb{R}^n)$ with the norm   $$ \|f\|_{L^2_s}=\left(\int_{\mathbb{R}^n}(1+|x|^2)^s|f(x)|^2d^nx\right)^{1/2}. $$   Show that $H^1\subset L^2_s$ is a compact embedding when $s<0$. Here ""compact embedding"" means that every uniformly bounded sequence in $H^1$ has a subsequence which is Cauchy in $L^2_s$. But since $L^2_s$ is just $L^2(\mathbb{R}^n,\mu_s)$ with the measure $d\mu_s=(1+x^2)^sd\mathcal{L}^n$ (here $\mathcal{L}^n$ is Lebesgue measure), $L^2_s$ is complete and this is the same as saying every uniformly bounded sequence in $H^1$ has a convergent subsequence in $L^2_s$. Thoughts I've spent the last 2-3 days thinking about this problem, but to no avail. It's driving me insane. Here are a couple of my thoughts/observations: For small enough $n$ and $|s|$ large enough, $L^2_s=L^2(\mathbb{R}^n,\mu_s)$ is a finite measure space since $s<0$. The Fourier transform is a unitary isomorphism from $L^2_s=L^2(\mathbb{R}^n,\mu_s)$ to $H^s$. $\|f\|_{L^2_s}\leq\|f\|_{L^2}\leq\|f\|_{H^1}$, so given a uniformly bounded subsequence $\{f_k\}\subset H^1$, $\big\{\|f_k\|_{L^2_s}\big\}$ is uniformly bounded and hence we can find a subsequence $\{f_{k_j}\}$ such that $\big\{\|f_{k_j}\|_{L^2_s}\big\}_{j\in\mathbb{N}}$ converges, but the problem is this doesn't necessarily mean that $\{f_{k_j}\}$ converges in $L^2_s$ I wanted to use the Rellich compactness theorem, but this only holds when there exists a compact $K\subset\mathbb{R}^n$ such that $\text{supp }f_k\subset K$ for all $k$. I thought using the theorem on closed balls centered at zero whose radii go to infinity and then diagonalizing, but I don't believe this should work. For simplicity, I was thinking about the case where $n=1$. In this case I have the Sobolev embedding theorem which shows that $H^1(\mathbb{R}^n)\subset C_0(\mathbb{R}^n)$, where $C_0(\mathbb{R}^n)$ is the family of continuous functions on $\mathbb{R}^n$ which vanish at infinity. Morrey's inequality also shows here that the elements of $H^1$ are H$\ddot{\text{o}}$lder continuous, so a uniformly bounded sequence in $H^1$ would be equicontinuous, but I don't think necessarily bounded, so this ruins trying to use Arzela-Ascoli in any significant way. I feel like, as with most things in my life, I'm making this more complicated than it needs to be. I feel as though a push in the right direction would hugely beneficial. Any help is greatly appreciated. Please only hints as this is an assignment for class.","I'm working on the following assignment: Define the weighted $L^2$-space $L^2_s$ on $\mathbb{R}^n$ to be the completion of $C_{\text{comp}}^{\infty}(\mathbb{R}^n)$ with the norm   $$ \|f\|_{L^2_s}=\left(\int_{\mathbb{R}^n}(1+|x|^2)^s|f(x)|^2d^nx\right)^{1/2}. $$   Show that $H^1\subset L^2_s$ is a compact embedding when $s<0$. Here ""compact embedding"" means that every uniformly bounded sequence in $H^1$ has a subsequence which is Cauchy in $L^2_s$. But since $L^2_s$ is just $L^2(\mathbb{R}^n,\mu_s)$ with the measure $d\mu_s=(1+x^2)^sd\mathcal{L}^n$ (here $\mathcal{L}^n$ is Lebesgue measure), $L^2_s$ is complete and this is the same as saying every uniformly bounded sequence in $H^1$ has a convergent subsequence in $L^2_s$. Thoughts I've spent the last 2-3 days thinking about this problem, but to no avail. It's driving me insane. Here are a couple of my thoughts/observations: For small enough $n$ and $|s|$ large enough, $L^2_s=L^2(\mathbb{R}^n,\mu_s)$ is a finite measure space since $s<0$. The Fourier transform is a unitary isomorphism from $L^2_s=L^2(\mathbb{R}^n,\mu_s)$ to $H^s$. $\|f\|_{L^2_s}\leq\|f\|_{L^2}\leq\|f\|_{H^1}$, so given a uniformly bounded subsequence $\{f_k\}\subset H^1$, $\big\{\|f_k\|_{L^2_s}\big\}$ is uniformly bounded and hence we can find a subsequence $\{f_{k_j}\}$ such that $\big\{\|f_{k_j}\|_{L^2_s}\big\}_{j\in\mathbb{N}}$ converges, but the problem is this doesn't necessarily mean that $\{f_{k_j}\}$ converges in $L^2_s$ I wanted to use the Rellich compactness theorem, but this only holds when there exists a compact $K\subset\mathbb{R}^n$ such that $\text{supp }f_k\subset K$ for all $k$. I thought using the theorem on closed balls centered at zero whose radii go to infinity and then diagonalizing, but I don't believe this should work. For simplicity, I was thinking about the case where $n=1$. In this case I have the Sobolev embedding theorem which shows that $H^1(\mathbb{R}^n)\subset C_0(\mathbb{R}^n)$, where $C_0(\mathbb{R}^n)$ is the family of continuous functions on $\mathbb{R}^n$ which vanish at infinity. Morrey's inequality also shows here that the elements of $H^1$ are H$\ddot{\text{o}}$lder continuous, so a uniformly bounded sequence in $H^1$ would be equicontinuous, but I don't think necessarily bounded, so this ruins trying to use Arzela-Ascoli in any significant way. I feel like, as with most things in my life, I'm making this more complicated than it needs to be. I feel as though a push in the right direction would hugely beneficial. Any help is greatly appreciated. Please only hints as this is an assignment for class.",,"['real-analysis', 'sobolev-spaces', 'lp-spaces']"
94,Closed form for $\int x^ne^{-x^m} \ dx\ ?$,Closed form for,\int x^ne^{-x^m} \ dx\ ?,"While entertaining myself by answering a question , the following problem arose. For what natural numbers $n,m$ does the following undefined integral have a closed form $$\int x^ne^{-x^m} \ dx\ ?$$ Closed form means that the antiderivative consists only of powers of $x^{...}$ and $x$ in $e^{-x^{...}}$. I created the following matrix showing for different pairs of $n$ and $m$ the nature of the antiderivative. $$\begin{matrix} & m&1&2&3&4&5&6&7\\ n\\ 1&&\checkmark&\checkmark&\Gamma&\text{erf}&\Gamma&\Gamma&\Gamma\\ 2&&\checkmark&\text{erf}&\checkmark&\Gamma&\Gamma&\text{erf}&\Gamma&\\ 3&&\checkmark&\checkmark&\Gamma&\checkmark&\Gamma&\Gamma&\Gamma&\\ 4&&\checkmark&\text{erf}&\Gamma&\Gamma&\checkmark&\Gamma&\Gamma\\ 5&&\checkmark&\checkmark&\checkmark&\text{erf}&\Gamma&\checkmark&\Gamma\\ 6&&\checkmark&\text{erf}&\Gamma&\Gamma&\Gamma&\Gamma&\checkmark\\ 7&&\checkmark&\checkmark&\Gamma&\checkmark&\Gamma&\Gamma&\Gamma\\ \end{matrix}$$ $$$$ The $\checkmark$ sign stands for a closed form, ""erf"" signals that the antiderivative contains the  erf function , and $\Gamma$ signals that the antiderivative contains the upper incomplete $\Gamma$ function . I have no clue. Does anybody?","While entertaining myself by answering a question , the following problem arose. For what natural numbers $n,m$ does the following undefined integral have a closed form $$\int x^ne^{-x^m} \ dx\ ?$$ Closed form means that the antiderivative consists only of powers of $x^{...}$ and $x$ in $e^{-x^{...}}$. I created the following matrix showing for different pairs of $n$ and $m$ the nature of the antiderivative. $$\begin{matrix} & m&1&2&3&4&5&6&7\\ n\\ 1&&\checkmark&\checkmark&\Gamma&\text{erf}&\Gamma&\Gamma&\Gamma\\ 2&&\checkmark&\text{erf}&\checkmark&\Gamma&\Gamma&\text{erf}&\Gamma&\\ 3&&\checkmark&\checkmark&\Gamma&\checkmark&\Gamma&\Gamma&\Gamma&\\ 4&&\checkmark&\text{erf}&\Gamma&\Gamma&\checkmark&\Gamma&\Gamma\\ 5&&\checkmark&\checkmark&\checkmark&\text{erf}&\Gamma&\checkmark&\Gamma\\ 6&&\checkmark&\text{erf}&\Gamma&\Gamma&\Gamma&\Gamma&\checkmark\\ 7&&\checkmark&\checkmark&\Gamma&\checkmark&\Gamma&\Gamma&\Gamma\\ \end{matrix}$$ $$$$ The $\checkmark$ sign stands for a closed form, ""erf"" signals that the antiderivative contains the  erf function , and $\Gamma$ signals that the antiderivative contains the upper incomplete $\Gamma$ function . I have no clue. Does anybody?",,"['calculus', 'real-analysis', 'gamma-function']"
95,"Can we find a function $f:\mathbb{R}\rightarrow \mathbb{R}$ that is open, closed, but not continuous?","Can we find a function  that is open, closed, but not continuous?",f:\mathbb{R}\rightarrow \mathbb{R},"The question is described in the title. To clarify, I'm looking for an example where the domain of such a function is $\mathbb{R}$ , so the example here will not work. The highest upvote answer here gives an example of discontinuous open map from $\mathbb{R}$ to $\mathbb{R}$ , but that map can hardly be closed (closed set does not need to contain any interval!). I have difficulty generalizing the construction and know very little about any sufficient condition that leads to a closed map without continuity (piecewise constant will work, but that kind of map cannot be open!). I'll appreciate any example, non-constructive prove/disprove, or any reference that covers this. *In case I need to clarify this, we use std. metric topology on $\mathbb{R}$ .","The question is described in the title. To clarify, I'm looking for an example where the domain of such a function is , so the example here will not work. The highest upvote answer here gives an example of discontinuous open map from to , but that map can hardly be closed (closed set does not need to contain any interval!). I have difficulty generalizing the construction and know very little about any sufficient condition that leads to a closed map without continuity (piecewise constant will work, but that kind of map cannot be open!). I'll appreciate any example, non-constructive prove/disprove, or any reference that covers this. *In case I need to clarify this, we use std. metric topology on .",\mathbb{R} \mathbb{R} \mathbb{R} \mathbb{R},['calculus']
96,Using Fatou's Lemma to Prove Monotone Convergence Theorem,Using Fatou's Lemma to Prove Monotone Convergence Theorem,,"Monotone Convergence Theorem- If $\{f_n\}$ is a sequence in $L^{+}$ such that $f_j\leq f_{j+1}$ for all $j$, and $f = \lim_{n\rightarrow \infty}f_n(=\sup_{n}f_n)$, then $\int f = \lim_{n\rightarrow\infty}\int f_n$ Fatou's Lemma - If $\{f_n\}$ is any sequence in $L^{+}$, then $$\int f = \int(\lim\inf f_n)\leq \lim\inf\int f_n$$ Let $\{f_n\}_{n\in\mathbb{N}}\subset L^{+}$, then by Fatou's Lemma $$\int f = \int (\lim_{n\rightarrow \infty} f_n) \leq \lim_{n\rightarrow \infty}\inf\int f_n$$ we know that $f = \lim_{n\rightarrow \infty}f_n (=\sup_{n}f_n)$, and that $f_n\leq f$ hence $\int f_n \leq \int f$ for all $n\in \mathbb{N}$, it is clear then that $\lim_{n\rightarrow \infty}\sup\int f_n \leq \int f$. Therefore, $$\lim_{n\rightarrow \infty}\sup\int f_n\leq \int f = \int \lim_{n\rightarrow \infty}\inf f_n \leq \lim_{n\rightarrow \infty}\inf\int f_n = \lim_{n\rightarrow \infty}\int f_n$$ so, $\int f = \lim_{n\rightarrow \infty}\int f_n$ Just want to make sure this is correct, any suggestions is greatly appreciated.","Monotone Convergence Theorem- If $\{f_n\}$ is a sequence in $L^{+}$ such that $f_j\leq f_{j+1}$ for all $j$, and $f = \lim_{n\rightarrow \infty}f_n(=\sup_{n}f_n)$, then $\int f = \lim_{n\rightarrow\infty}\int f_n$ Fatou's Lemma - If $\{f_n\}$ is any sequence in $L^{+}$, then $$\int f = \int(\lim\inf f_n)\leq \lim\inf\int f_n$$ Let $\{f_n\}_{n\in\mathbb{N}}\subset L^{+}$, then by Fatou's Lemma $$\int f = \int (\lim_{n\rightarrow \infty} f_n) \leq \lim_{n\rightarrow \infty}\inf\int f_n$$ we know that $f = \lim_{n\rightarrow \infty}f_n (=\sup_{n}f_n)$, and that $f_n\leq f$ hence $\int f_n \leq \int f$ for all $n\in \mathbb{N}$, it is clear then that $\lim_{n\rightarrow \infty}\sup\int f_n \leq \int f$. Therefore, $$\lim_{n\rightarrow \infty}\sup\int f_n\leq \int f = \int \lim_{n\rightarrow \infty}\inf f_n \leq \lim_{n\rightarrow \infty}\inf\int f_n = \lim_{n\rightarrow \infty}\int f_n$$ so, $\int f = \lim_{n\rightarrow \infty}\int f_n$ Just want to make sure this is correct, any suggestions is greatly appreciated.",,"['real-analysis', 'measure-theory', 'proof-verification']"
97,"If $\sum A_n$ converges, does $\sum A_n x^n$ converge uniformly on $[0,1]$?","If  converges, does  converge uniformly on ?","\sum A_n \sum A_n x^n [0,1]","Question: If $\sum_{n=0}^\infty A_n$ converges, does the series $\sum A_n x^n$ converge uniformly on $[0,1]$? Pointwise convergence is easy enough to see, and intuitively I think the series should converge uniformly as well, but I'm having trouble showing that in proof. Indeed, if we fix $\varepsilon > 0$ and procure a $N$ by the convergence of $\sum A_n$, it is not universally true that $|\sum_{n=q}^p A_n x^n| \leq |\sum_{n=q}^p A_n| < \varepsilon$ whenever $p \geq q \geq N$. I also tried to use the statement $|\sum_N^\infty A_n x^n| < |\sum_N^\infty x^n|$ past some point where $|A_n| < 1$, but this only gives pointwise convergence. Perhaps we can break up the interval in $[0, \xi]$ and $[\xi, 1]$, the first interval converging uniformly using the geometric property and the second somehow else?","Question: If $\sum_{n=0}^\infty A_n$ converges, does the series $\sum A_n x^n$ converge uniformly on $[0,1]$? Pointwise convergence is easy enough to see, and intuitively I think the series should converge uniformly as well, but I'm having trouble showing that in proof. Indeed, if we fix $\varepsilon > 0$ and procure a $N$ by the convergence of $\sum A_n$, it is not universally true that $|\sum_{n=q}^p A_n x^n| \leq |\sum_{n=q}^p A_n| < \varepsilon$ whenever $p \geq q \geq N$. I also tried to use the statement $|\sum_N^\infty A_n x^n| < |\sum_N^\infty x^n|$ past some point where $|A_n| < 1$, but this only gives pointwise convergence. Perhaps we can break up the interval in $[0, \xi]$ and $[\xi, 1]$, the first interval converging uniformly using the geometric property and the second somehow else?",,"['real-analysis', 'sequences-and-series', 'uniform-convergence']"
98,"Taylor series not converging, other example than $\exp(-1/x^2)$?","Taylor series not converging, other example than ?",\exp(-1/x^2),"The usual example for non-converging Taylor series is $g(x) = \exp(-1/x^2) \; \forall x \neq 0, g(0) = 0$: the Taylor series around $x=0$ is zero, but $g$ isn't zero for any $x \neq 0$. What's not so nice about this example are the derivatives:  \begin{align*}g'(x) &= \frac{2}{x^3}\exp(-1/x^2), \\ g''(x) &= \left(-\frac{6}{x^4}+\frac{4}{x^6}\right)\exp(-1/x^2), \\ \ldots  \end{align*} So obviously we have to calculate the derivative of $g$ in $x = 0$ by using the definition of the derivative. It is not possible to get it by ""applying the rules"" (e.g. $2\exp(-1/x^2)/x^3$ isn't defined for $x=0$). The question is: Can there be a function $f\colon \mathbb{R}\rightarrow \mathbb{R}$ which has a Taylor series with radius of convergence $0$, but whose derivatives in the Taylor series can be calculated easily by just using the ""usual"" rules (derivatives of polynomials, product rule, chain rule, derivatives of $\exp, \sin, \ldots$)?","The usual example for non-converging Taylor series is $g(x) = \exp(-1/x^2) \; \forall x \neq 0, g(0) = 0$: the Taylor series around $x=0$ is zero, but $g$ isn't zero for any $x \neq 0$. What's not so nice about this example are the derivatives:  \begin{align*}g'(x) &= \frac{2}{x^3}\exp(-1/x^2), \\ g''(x) &= \left(-\frac{6}{x^4}+\frac{4}{x^6}\right)\exp(-1/x^2), \\ \ldots  \end{align*} So obviously we have to calculate the derivative of $g$ in $x = 0$ by using the definition of the derivative. It is not possible to get it by ""applying the rules"" (e.g. $2\exp(-1/x^2)/x^3$ isn't defined for $x=0$). The question is: Can there be a function $f\colon \mathbb{R}\rightarrow \mathbb{R}$ which has a Taylor series with radius of convergence $0$, but whose derivatives in the Taylor series can be calculated easily by just using the ""usual"" rules (derivatives of polynomials, product rule, chain rule, derivatives of $\exp, \sin, \ldots$)?",,"['calculus', 'real-analysis', 'convergence-divergence', 'taylor-expansion']"
99,A question about the vector space spanned by shifts of a given function,A question about the vector space spanned by shifts of a given function,,"Let $E$ be the space of continuous real functions and $f\in E$ Let $T_t$ denote the shift operator: $T_t(f)(x)=f(t+x)$ Let $T(f)$ be the linear span of the set $\{T_t(f) \;|\; t\in \mathbb R\}$ Suppose $T(f)$ is finite-dimensional. Prove that $f$ is differentiable and that $f'\in T(f)$ I have very few clues about what should be done to solve this problem. Here's my work so far. Let $T_{a_1}(f),\ldots,T_{a_n}(f)$ be a basis of $T(f)$ . Let $x\in \mathbb R$ . There exists $\lambda_1,\ldots,\lambda_n$ such that $T_{x}(f)=\sum_{k=1}^n \lambda_i T_{a_i}(f)$ Since $\displaystyle \frac{f(x+h)-f(x)}{h}=\frac{T_{x}(f)(h)-T_{x}(f)(0)}{h}=\sum_{k=1}^n \lambda_i \frac{T_{a_i}(f)(h)-T_{a_i}(f)(0)}{h}$ , it suffices to prove that $f$ is differentiable at $ a_1,\ldots,a_n$ , but that's still difficult.","Let be the space of continuous real functions and Let denote the shift operator: Let be the linear span of the set Suppose is finite-dimensional. Prove that is differentiable and that I have very few clues about what should be done to solve this problem. Here's my work so far. Let be a basis of . Let . There exists such that Since , it suffices to prove that is differentiable at , but that's still difficult.","E f\in E T_t T_t(f)(x)=f(t+x) T(f) \{T_t(f) \;|\; t\in \mathbb R\} T(f) f f'\in T(f) T_{a_1}(f),\ldots,T_{a_n}(f) T(f) x\in \mathbb R \lambda_1,\ldots,\lambda_n T_{x}(f)=\sum_{k=1}^n \lambda_i T_{a_i}(f) \displaystyle \frac{f(x+h)-f(x)}{h}=\frac{T_{x}(f)(h)-T_{x}(f)(0)}{h}=\sum_{k=1}^n \lambda_i \frac{T_{a_i}(f)(h)-T_{a_i}(f)(0)}{h} f  a_1,\ldots,a_n","['real-analysis', 'linear-algebra', 'functional-analysis', 'derivatives']"
