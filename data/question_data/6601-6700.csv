,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What can we say about $a_n$ if $\sum a_n/n$ converges?,What can we say about  if  converges?,a_n \sum a_n/n,"Suppose that $\sum_{n=1}^{\infty} a_n/n$ converges, with $a_n \geq 0$ but not necessarily decreasing. What can we say about $a_n$? We can't say $a_n \to 0$. (Consider $a_n=1$ for n square, 0 otherwise.) But we can say that for any $\epsilon>0$, there are an infinite number of $a_n \leq \epsilon$. Is there a name for this property? What else can we say about $a_n$?","Suppose that $\sum_{n=1}^{\infty} a_n/n$ converges, with $a_n \geq 0$ but not necessarily decreasing. What can we say about $a_n$? We can't say $a_n \to 0$. (Consider $a_n=1$ for n square, 0 otherwise.) But we can say that for any $\epsilon>0$, there are an infinite number of $a_n \leq \epsilon$. Is there a name for this property? What else can we say about $a_n$?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
1,Domains for which the divergence theorem holds,Domains for which the divergence theorem holds,,"In the book Elliptic partial differential equations of second order written by Gilbarg and Trudinger, I saw the following sentence on page 17 in section 2.4 Green’s Representation: As a prelude to existence considerations we derive now some further consequences of the divergence theorem, namely, Green identities. Let   $\Omega$ be a domain for which the divergence theorem holds and let $u$ and $v$ be $C^2(\bar\Omega)$ functions. It is well known that the divergence theorem holds when $\Omega$ is a bounded domain with $C^1$ boundary. Are there any other domain than a bounded one with $C^1$ boundary for which the theorem holds? I would be grateful if you could give any comment for this question.","In the book Elliptic partial differential equations of second order written by Gilbarg and Trudinger, I saw the following sentence on page 17 in section 2.4 Green’s Representation: As a prelude to existence considerations we derive now some further consequences of the divergence theorem, namely, Green identities. Let   $\Omega$ be a domain for which the divergence theorem holds and let $u$ and $v$ be $C^2(\bar\Omega)$ functions. It is well known that the divergence theorem holds when $\Omega$ is a bounded domain with $C^1$ boundary. Are there any other domain than a bounded one with $C^1$ boundary for which the theorem holds? I would be grateful if you could give any comment for this question.",,"['real-analysis', 'multivariable-calculus', 'vector-analysis', 'geometric-measure-theory']"
2,Completeness of Holder Space,Completeness of Holder Space,,"How do I show that the space of complex-valued functions on $[0,1]$ such that $$|f(x) -f(y)| < C|x-y|^\frac{1}{2}$$ with norm $$\|f\| = \sup_{x \in [0,1]} |f(x)| + \inf C$$ where inf is over the constants such that the holder bound holds, is a Banach space? $C$ depends on the function.","How do I show that the space of complex-valued functions on $[0,1]$ such that $$|f(x) -f(y)| < C|x-y|^\frac{1}{2}$$ with norm $$\|f\| = \sup_{x \in [0,1]} |f(x)| + \inf C$$ where inf is over the constants such that the holder bound holds, is a Banach space? $C$ depends on the function.",,"['real-analysis', 'functional-analysis', 'analysis', 'banach-spaces']"
3,Integral identity using the transformation formula,Integral identity using the transformation formula,,"Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be integrable. I want to show that $$ \int_\mathbb{R}f(x)\,\mathrm{d}\lambda(x) = \int_\mathbb{R} f\left(x-\frac{1}{x}\right)\,\mathrm{d}\lambda(x).$$ I tried to use the transformation formula, but I did not get the identity. I would appreciate any hints.","Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be integrable. I want to show that $$ \int_\mathbb{R}f(x)\,\mathrm{d}\lambda(x) = \int_\mathbb{R} f\left(x-\frac{1}{x}\right)\,\mathrm{d}\lambda(x).$$ I tried to use the transformation formula, but I did not get the identity. I would appreciate any hints.",,"['real-analysis', 'integration']"
4,Can numerical methods be used to prove a root does NOT exist?,Can numerical methods be used to prove a root does NOT exist?,,"Let's say I have a continuous function $f : I \to \mathbb R$ for $I = [a,b]$ and I want to decide if it has a root or not in $I$. Pretend that I can evaluate it anywhere but cannot use analytical methods to learn anything more about it. I could obtain a grid of points $a = p_1 < \dots < p_n = b$, evaluate $f$ on each point, and make a scatterplot with some kind of interpolation. From this I may find a root, but if I don't see a root can I ever be confident that one actually does not exist? Can I know that there isn't wild behavior between some pair of points that I missed? Eg maybe for some $i$ the function dips down below $0$ really rapidly right after $p_i$ and returns right before $p_{i+1}$, so it looks flat but just because I've missed something. My question: what are the circumstances under which we can use a finite number of finite precision function evaluations to prove a root does not exist? My current guess is that if $f$ is Lipschitz then we could use its Lipschitz constant $K$ to make our grid fine enough that there's no way that $f$ could have a root between pairs of grid points if it doesn't visibly have one in the interpolated scatterplot. But if $K$ is large or $f$ is really close to $0$ then we may have a situation where the grid is required to be finer than finite precision can do. I also wonder if convexity would do the trick (which since it's stronger than Lipschitz on $I$ seems like a natural thing to try next).","Let's say I have a continuous function $f : I \to \mathbb R$ for $I = [a,b]$ and I want to decide if it has a root or not in $I$. Pretend that I can evaluate it anywhere but cannot use analytical methods to learn anything more about it. I could obtain a grid of points $a = p_1 < \dots < p_n = b$, evaluate $f$ on each point, and make a scatterplot with some kind of interpolation. From this I may find a root, but if I don't see a root can I ever be confident that one actually does not exist? Can I know that there isn't wild behavior between some pair of points that I missed? Eg maybe for some $i$ the function dips down below $0$ really rapidly right after $p_i$ and returns right before $p_{i+1}$, so it looks flat but just because I've missed something. My question: what are the circumstances under which we can use a finite number of finite precision function evaluations to prove a root does not exist? My current guess is that if $f$ is Lipschitz then we could use its Lipschitz constant $K$ to make our grid fine enough that there's no way that $f$ could have a root between pairs of grid points if it doesn't visibly have one in the interpolated scatterplot. But if $K$ is large or $f$ is really close to $0$ then we may have a situation where the grid is required to be finer than finite precision can do. I also wonder if convexity would do the trick (which since it's stronger than Lipschitz on $I$ seems like a natural thing to try next).",,"['real-analysis', 'numerical-methods', 'roots']"
5,Are there real numbers such that $ \frac{1}{\sqrt{8}}< \liminf n^2 \left| \xi - \frac{m}{n}\right|< \frac{1}{\sqrt{5}} $?,Are there real numbers such that ?, \frac{1}{\sqrt{8}}< \liminf n^2 \left| \xi - \frac{m}{n}\right|< \frac{1}{\sqrt{5}} ,"What can we say about the set of real numbers lying between $\sqrt{5}$ and  $\sqrt{8}$ in the Markov spectrum ? $$ \left\{ \xi \in \mathbb{R} : \frac{1}{\sqrt{8}n^2}< \left| \xi - \frac{m}{n}\right|< \frac{1}{\sqrt{5}n^2} \text{ for infinitely many  }m,n\in \mathbb{Z}\right\}\subseteq \mathbb{R}    $$ What is the Hausdorff dimension of this set? put another way we're looking for real numbers such that $$  \frac{1}{\sqrt{8}}< \liminf_{n \to \infty} n^2 \left| \xi - \frac{m}{n}\right|< \frac{1}{\sqrt{5}}   $$ Perhaps this can be solved with continued fractions . Related If $x\notin\mathbb Q$, then $\left|x-\frac{p}{q}\right|<\frac{1}{q^2}$ for infinitely many $\frac{p}{q}$? Are the Real numbers really Complete?","What can we say about the set of real numbers lying between $\sqrt{5}$ and  $\sqrt{8}$ in the Markov spectrum ? $$ \left\{ \xi \in \mathbb{R} : \frac{1}{\sqrt{8}n^2}< \left| \xi - \frac{m}{n}\right|< \frac{1}{\sqrt{5}n^2} \text{ for infinitely many  }m,n\in \mathbb{Z}\right\}\subseteq \mathbb{R}    $$ What is the Hausdorff dimension of this set? put another way we're looking for real numbers such that $$  \frac{1}{\sqrt{8}}< \liminf_{n \to \infty} n^2 \left| \xi - \frac{m}{n}\right|< \frac{1}{\sqrt{5}}   $$ Perhaps this can be solved with continued fractions . Related If $x\notin\mathbb Q$, then $\left|x-\frac{p}{q}\right|<\frac{1}{q^2}$ for infinitely many $\frac{p}{q}$? Are the Real numbers really Complete?",,"['real-analysis', 'fractals', 'continued-fractions', 'cantor-set']"
6,Show the equivalence of arc length definitions,Show the equivalence of arc length definitions,,"Definition 1: Let $r: [a,b] \to \Bbb R^d$ be a continuous differentiable function. Then the arc length is given by $$L(r) = \int_a^b || r'(t) || \, dt$$ Definition 2: Let $r: [a,b] \to \Bbb R^d$ be a continuous function. Then the arc length is given by $$ V(r) = \sup_P \sum_{k=1}^n || r(x_k)-r(x_{k-1}) ||$$ where the supremum is taken over all partitions $P = \{a=x_0 \lt x_1 \lt \ldots \lt x_n = b \}$ of $[a,b]$. How can I show that for a continuous differentiable $r(t)$ the two definitions are equivalent, i.e. $L(r)=V(r)$? What I've done so far: I found this question , which shows that I can convert the supremum to a limit $$V(r) = \sup_P \sum_{k=1}^n || r(x_k)-r(x_{k-1}) || = \lim_{n \to \infty} \sum_{k=1}^n || r(x_k)-r(x_{k-1}) ||$$ by choosing an appropriate sequence of partitions $P_n$ of which I take the $x_k$'s. This gives $$ \lim_{n \to \infty} \sum_{k=1}^n || r(x_k)-r(x_{k-1}) || = \lim_{n \to \infty} \sum_{k=1}^n || \frac{r(x_k)-r(x_{k-1})}{x_k-x_{k-1}} || (x_k-x_{k-1})$$ Now I somehow need to show that $$\lim_{n \to \infty} \sum_{k=1}^n || \frac{r(x_k)-r(x_{k-1})}{x_k-x_{k-1}} || (x_k-x_{k-1}) = \int_a^b ||r'(t)|| \, dt$$ How can I justify this step of converting the sum to an intergral and taking the limit of the inside simultaneously?","Definition 1: Let $r: [a,b] \to \Bbb R^d$ be a continuous differentiable function. Then the arc length is given by $$L(r) = \int_a^b || r'(t) || \, dt$$ Definition 2: Let $r: [a,b] \to \Bbb R^d$ be a continuous function. Then the arc length is given by $$ V(r) = \sup_P \sum_{k=1}^n || r(x_k)-r(x_{k-1}) ||$$ where the supremum is taken over all partitions $P = \{a=x_0 \lt x_1 \lt \ldots \lt x_n = b \}$ of $[a,b]$. How can I show that for a continuous differentiable $r(t)$ the two definitions are equivalent, i.e. $L(r)=V(r)$? What I've done so far: I found this question , which shows that I can convert the supremum to a limit $$V(r) = \sup_P \sum_{k=1}^n || r(x_k)-r(x_{k-1}) || = \lim_{n \to \infty} \sum_{k=1}^n || r(x_k)-r(x_{k-1}) ||$$ by choosing an appropriate sequence of partitions $P_n$ of which I take the $x_k$'s. This gives $$ \lim_{n \to \infty} \sum_{k=1}^n || r(x_k)-r(x_{k-1}) || = \lim_{n \to \infty} \sum_{k=1}^n || \frac{r(x_k)-r(x_{k-1})}{x_k-x_{k-1}} || (x_k-x_{k-1})$$ Now I somehow need to show that $$\lim_{n \to \infty} \sum_{k=1}^n || \frac{r(x_k)-r(x_{k-1})}{x_k-x_{k-1}} || (x_k-x_{k-1}) = \int_a^b ||r'(t)|| \, dt$$ How can I justify this step of converting the sum to an intergral and taking the limit of the inside simultaneously?",,"['real-analysis', 'differential-geometry', 'curves', 'arc-length']"
7,"Embeddings between Hölder spaces $ C^{0,\beta} \hookrightarrow C^{0, \alpha} .$",Embeddings between Hölder spaces," C^{0,\beta} \hookrightarrow C^{0, \alpha} .","Let $ \Omega \subset \mathbb R^n $ be an open subset and let $ 0 < \alpha < \beta \leq 1.$ We consider the space of Hölder continuous functions $C^{0, \alpha}$ which is a Banach space endowed with the norm $$ \| f\|_{C^{0, \alpha}} := \| f \|_{\infty} + \sup_{ x,y \in \Omega \\ x \neq y} \frac{ |f(x) -f(y)|}{|x-y|^\alpha}. $$ My questions has to do with the embedding $ C^{0,\beta} \hookrightarrow C^{0, \alpha} .$ If $ \Omega $ is bounded, then I can prove the estimate $ \| f\|_{C^{0, \alpha}} \leq \text{diam}(\Omega)^{\beta -\alpha} \| f\|_{C^{0, \beta}} ,$ which in turn implies that the embedding is bounded, i.e. continuous. Question: How can I show that the embedding is still continuous in the case where $ \Omega $ is unbounded ? Any help would be really appreciated.","Let $ \Omega \subset \mathbb R^n $ be an open subset and let $ 0 < \alpha < \beta \leq 1.$ We consider the space of Hölder continuous functions $C^{0, \alpha}$ which is a Banach space endowed with the norm $$ \| f\|_{C^{0, \alpha}} := \| f \|_{\infty} + \sup_{ x,y \in \Omega \\ x \neq y} \frac{ |f(x) -f(y)|}{|x-y|^\alpha}. $$ My questions has to do with the embedding $ C^{0,\beta} \hookrightarrow C^{0, \alpha} .$ If $ \Omega $ is bounded, then I can prove the estimate $ \| f\|_{C^{0, \alpha}} \leq \text{diam}(\Omega)^{\beta -\alpha} \| f\|_{C^{0, \beta}} ,$ which in turn implies that the embedding is bounded, i.e. continuous. Question: How can I show that the embedding is still continuous in the case where $ \Omega $ is unbounded ? Any help would be really appreciated.",,"['real-analysis', 'functional-analysis', 'analysis', 'holder-spaces', 'fractional-sobolev-spaces']"
8,Proof of weak convergence of empirical measure,Proof of weak convergence of empirical measure,,"Let's say that we have a complete and separable metric space in other words a Polish space $(S, d)$. If we say that we have $n$ I.I.D random elements $(Y_0, Y_1 ...)$ in our space $S$ with a common distribution that we'll denote $P$. If we define an empirical measure $P_{n,w}$ using observations $(Y_1(w), ..., Y_n(w))$. $$P_{n,w} = \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}(w)$$ Where the $\delta_x$ is a measure that places a unit mass on $x$. How do we show that given that S is seperable and complete, then $P_{n,w} \rightarrow P$ as $n$ goes to $\infty$ The convergence is weak. Could we use the equivalences from Portmanteau theorem's to prove this?","Let's say that we have a complete and separable metric space in other words a Polish space $(S, d)$. If we say that we have $n$ I.I.D random elements $(Y_0, Y_1 ...)$ in our space $S$ with a common distribution that we'll denote $P$. If we define an empirical measure $P_{n,w}$ using observations $(Y_1(w), ..., Y_n(w))$. $$P_{n,w} = \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}(w)$$ Where the $\delta_x$ is a measure that places a unit mass on $x$. How do we show that given that S is seperable and complete, then $P_{n,w} \rightarrow P$ as $n$ goes to $\infty$ The convergence is weak. Could we use the equivalences from Portmanteau theorem's to prove this?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'weak-convergence']"
9,Monotone function with $f(\mathbb{R}) = \mathbb{R} \backslash \mathbb{Q}$,Monotone function with,f(\mathbb{R}) = \mathbb{R} \backslash \mathbb{Q},"I want to prove per contradiction, that there doesn't exist a strictly monotone function $f:\mathbb{R} \to \mathbb{R}$ with $$ f(\mathbb{R}) = \mathbb{R} \backslash \mathbb{Q} $$ but I'm not sure if this argumentation is right. Assume there exists such a function $f$. Let be $ a \in \mathbb{R} \backslash \mathbb{Q}$ and $(\frac{a}{n})_{n \in \mathbb{N}} \in (\mathbb{R} \backslash \mathbb{Q})^\mathbb{N}$. Then there exist $b_n \in \mathbb{R}: f(b_n) = \frac{a}{n}$. $$ \lim_{n \to \infty} f(b_n) = \lim_{n\to \infty} \frac{a}{n} = 0 \notin \mathbb{R} \backslash \mathbb{Q} $$ But because of the monotony of $f$, for a $b \in \mathbb{R}$ $$ \lim_{x \to b} f(x)$$ has to exists and so it has to be in $\mathbb{R}\backslash \mathbb{Q}$. Is there some example for non-strictly monotone functions? (it has to be $\mathrm{im}(f) = \mathbb{R}\backslash\mathbb{Q})$","I want to prove per contradiction, that there doesn't exist a strictly monotone function $f:\mathbb{R} \to \mathbb{R}$ with $$ f(\mathbb{R}) = \mathbb{R} \backslash \mathbb{Q} $$ but I'm not sure if this argumentation is right. Assume there exists such a function $f$. Let be $ a \in \mathbb{R} \backslash \mathbb{Q}$ and $(\frac{a}{n})_{n \in \mathbb{N}} \in (\mathbb{R} \backslash \mathbb{Q})^\mathbb{N}$. Then there exist $b_n \in \mathbb{R}: f(b_n) = \frac{a}{n}$. $$ \lim_{n \to \infty} f(b_n) = \lim_{n\to \infty} \frac{a}{n} = 0 \notin \mathbb{R} \backslash \mathbb{Q} $$ But because of the monotony of $f$, for a $b \in \mathbb{R}$ $$ \lim_{x \to b} f(x)$$ has to exists and so it has to be in $\mathbb{R}\backslash \mathbb{Q}$. Is there some example for non-strictly monotone functions? (it has to be $\mathrm{im}(f) = \mathbb{R}\backslash\mathbb{Q})$",,['real-analysis']
10,Analysis: Spotting why Proof/Derivation is Wrong (part in which proof is wrong),Analysis: Spotting why Proof/Derivation is Wrong (part in which proof is wrong),,"I'm having trouble figuring out what the error is in the above derivative.  I have no idea what is happening in the first line, how an application of the Mean Value Theorem gives such an expression to the right. I just verified, some of the other parts and they seem to be okay as $\lim_{x \to 0} x\sin(1/x)$ is indeed $0$ so limit of $2\xi\sin(1/\xi)$ would also be $0$. So, honestly I'm not sure what the error is in the above derivation. I could really use some help.  Thank you.","I'm having trouble figuring out what the error is in the above derivative.  I have no idea what is happening in the first line, how an application of the Mean Value Theorem gives such an expression to the right. I just verified, some of the other parts and they seem to be okay as $\lim_{x \to 0} x\sin(1/x)$ is indeed $0$ so limit of $2\xi\sin(1/\xi)$ would also be $0$. So, honestly I'm not sure what the error is in the above derivation. I could really use some help.  Thank you.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
11,Show that $f$ is bounded if $f(x)+f''(x)=-xg(x)f'(x)$ where $g\ge0$,Show that  is bounded if  where,f f(x)+f''(x)=-xg(x)f'(x) g\ge0,Let $f$ be a twice differentiable real valued function such that  $f(x)+f''(x)=-xg(x)f'(x)$ Where $g(x)\geq 0$ for all real $x$ Show that $|f(x) |$ is a bounded function.,Let $f$ be a twice differentiable real valued function such that  $f(x)+f''(x)=-xg(x)f'(x)$ Where $g(x)\geq 0$ for all real $x$ Show that $|f(x) |$ is a bounded function.,,"['calculus', 'real-analysis']"
12,$\cos(\theta_n) \to \cos(\theta)$ and $\sin(\theta_n) \to \sin(\theta)$. How to show that $\theta_n \to \theta$?,and . How to show that ?,\cos(\theta_n) \to \cos(\theta) \sin(\theta_n) \to \sin(\theta) \theta_n \to \theta,"$\cos(\theta_n) \to \cos(\theta)$ and $\sin(\theta_n) \to \sin(\theta)$ where $\theta_n$ , $\theta \in (-\pi, \pi)$. How to show that $\theta_n \to \theta$ ? Can I use the continuity of $\cos^{-1}$ or $\sin^{-1}$ (only one of them) to get the result? If not, then how to proceed? Edit: As we know, range of $\arcsin $ is $[-\pi/2, \pi/2]$, we can use continuity of $\arcsin $ to get $\theta_n \to \theta$ when they belongs to  $[-\pi/2, \pi/2]$, in other cases use continuity of $\arccos$. Is this argument correct?","$\cos(\theta_n) \to \cos(\theta)$ and $\sin(\theta_n) \to \sin(\theta)$ where $\theta_n$ , $\theta \in (-\pi, \pi)$. How to show that $\theta_n \to \theta$ ? Can I use the continuity of $\cos^{-1}$ or $\sin^{-1}$ (only one of them) to get the result? If not, then how to proceed? Edit: As we know, range of $\arcsin $ is $[-\pi/2, \pi/2]$, we can use continuity of $\arcsin $ to get $\theta_n \to \theta$ when they belongs to  $[-\pi/2, \pi/2]$, in other cases use continuity of $\arccos$. Is this argument correct?",,"['real-analysis', 'sequences-and-series', 'continuity']"
13,Linear equation with square root,Linear equation with square root,,Linear equation in two variables must have the form $ax+by=c$. But can we see $\sqrt{x+y}=2$ as a linear equation because if we square both sides we get $x+y=4$ and both equations have the same solution set?,Linear equation in two variables must have the form $ax+by=c$. But can we see $\sqrt{x+y}=2$ as a linear equation because if we square both sides we get $x+y=4$ and both equations have the same solution set?,,"['real-analysis', 'linear-algebra']"
14,Definition of a bounded subset in a normed vector space,Definition of a bounded subset in a normed vector space,,"I'm having a hard time trying to understand what is exactly a bounded set (or subset) in a normed vector space. This is where I am so far in my ""understanding"": Let $(V, \|\cdot\|)$ be a normed vector space. We can define $\|y-x\| =: d(x,y) =: d$ such that $(V,d)$ is a metric space. In a metric space (such as in $(V,d)$), a set $A$ is bounded if it is contained in a ball of finite radius, that is: $A \subset V$ is bounded if there exists $x \in V$ and $0 < R < \infty$ such that $A \subset B(x,R)$, i.e. such that $d(x,y) \le R$ for all $y \in A$. A set in a topological vector space (such as a normed vector space) is called bounded if every neighborhood of the zero vector can be inflated to include the set. (Source: Wikipedia ) Apparently, a subset $A$ of a normed vector space $(V, \|\cdot\|)$ is said to be bounded if there exists $C > 0$ such that $\|v\| \le C$ for all $v \in A$. What I don't understand is that, to me, it would be ""logical"" to say that a subset $A$ of a normed vector space $(V, \|\cdot\|)$ is bounded if there exists $x \in V$ and $C>0$ such that $d(x,y) = \|y-x\| \le C$ for all $y \in A$. But it seems like the ""real"" definition of a bounded (sub)set of a normed vector space is considering only $x=0$, that is, $x$ being the zero vector. Why is that? Furthermore, I'm not sure to understand the definition given by Wikipedia. Since a neighborhood is a set containing an open set containing a particular point, and considering that this point could be the zero vector, how can we use that information to get the actual definition of a bounded subset of a normed vector space (i.e. the last definition I mentioned in my list above)? As you can see, I'm quite confused about all this... And I'm a little bit exhausted since I've tried to get it for hours now. Any help would be greatly appreciated...!","I'm having a hard time trying to understand what is exactly a bounded set (or subset) in a normed vector space. This is where I am so far in my ""understanding"": Let $(V, \|\cdot\|)$ be a normed vector space. We can define $\|y-x\| =: d(x,y) =: d$ such that $(V,d)$ is a metric space. In a metric space (such as in $(V,d)$), a set $A$ is bounded if it is contained in a ball of finite radius, that is: $A \subset V$ is bounded if there exists $x \in V$ and $0 < R < \infty$ such that $A \subset B(x,R)$, i.e. such that $d(x,y) \le R$ for all $y \in A$. A set in a topological vector space (such as a normed vector space) is called bounded if every neighborhood of the zero vector can be inflated to include the set. (Source: Wikipedia ) Apparently, a subset $A$ of a normed vector space $(V, \|\cdot\|)$ is said to be bounded if there exists $C > 0$ such that $\|v\| \le C$ for all $v \in A$. What I don't understand is that, to me, it would be ""logical"" to say that a subset $A$ of a normed vector space $(V, \|\cdot\|)$ is bounded if there exists $x \in V$ and $C>0$ such that $d(x,y) = \|y-x\| \le C$ for all $y \in A$. But it seems like the ""real"" definition of a bounded (sub)set of a normed vector space is considering only $x=0$, that is, $x$ being the zero vector. Why is that? Furthermore, I'm not sure to understand the definition given by Wikipedia. Since a neighborhood is a set containing an open set containing a particular point, and considering that this point could be the zero vector, how can we use that information to get the actual definition of a bounded subset of a normed vector space (i.e. the last definition I mentioned in my list above)? As you can see, I'm quite confused about all this... And I'm a little bit exhausted since I've tried to get it for hours now. Any help would be greatly appreciated...!",,"['real-analysis', 'definition', 'normed-spaces']"
15,What regularity conditions on partial derivatives are equivalent to differentiability?,What regularity conditions on partial derivatives are equivalent to differentiability?,,"This question is intended to kind of rekindle this old question , which was apparently very hard didn't receive a satisfactory answer. I'm aware that the hope of a definite answer is quite slim, but I'm still very curious to know: Suppose $f:U\to \Bbb R$ is a function, $U$ is an open subset of $\Bbb R^n$, and $x_0\in U$, and the partial derivatives $\partial_i|_{x_0}f,\,i=1,\cdots,n$ exist. Then what regularity conditions on $\partial_i|_{x_0}f,\,i=1,\cdots,n$ are a sufficient and necessary condition for $f$ to be differentiable at $x_0$? The mere existence of $\partial_i|_{x_0}f$ is far from enough. The continuity of all of them is over-sufficient. The weakest sufficient condition AFAIK is this one , i.e. the continuity of all but one of them, which is nevertheless still over-sufficient. Given that we can easily construct a function differentiable at $x_0$ yet has all its partial derivatives discontinuous at $x_0$, like $$f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\dfrac{1}{\sqrt{x^2+y^2}}\right) & \text{ if $(x,y) \ne (0,0)$}\\0 &  \text{ if $(x,y) = (0,0)$}.\end{cases}$$ this problem I believe is intrinsically hard. Has any research been done that can shed some light on this complicated problem?","This question is intended to kind of rekindle this old question , which was apparently very hard didn't receive a satisfactory answer. I'm aware that the hope of a definite answer is quite slim, but I'm still very curious to know: Suppose $f:U\to \Bbb R$ is a function, $U$ is an open subset of $\Bbb R^n$, and $x_0\in U$, and the partial derivatives $\partial_i|_{x_0}f,\,i=1,\cdots,n$ exist. Then what regularity conditions on $\partial_i|_{x_0}f,\,i=1,\cdots,n$ are a sufficient and necessary condition for $f$ to be differentiable at $x_0$? The mere existence of $\partial_i|_{x_0}f$ is far from enough. The continuity of all of them is over-sufficient. The weakest sufficient condition AFAIK is this one , i.e. the continuity of all but one of them, which is nevertheless still over-sufficient. Given that we can easily construct a function differentiable at $x_0$ yet has all its partial derivatives discontinuous at $x_0$, like $$f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\dfrac{1}{\sqrt{x^2+y^2}}\right) & \text{ if $(x,y) \ne (0,0)$}\\0 &  \text{ if $(x,y) = (0,0)$}.\end{cases}$$ this problem I believe is intrinsically hard. Has any research been done that can shed some light on this complicated problem?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
16,Every complex number has 2 square roots - Rudin,Every complex number has 2 square roots - Rudin,,"I am using Baby Rudin for self study and as many of you are aware, it is one thing to follow Rudin's calculations to verify that his proof is correct. It is quite another to work backwards to see how he came up with his proof in the first place. I find that unless I do the latter, the former does absolutely nothing for me. Chapter 1 exercise 10 is as follows: Suppose $z = a + bi$, $w = u + vi$ and  $a = \left(\frac{|w| + u}{2}\right)^{1/2}$, $b = \left(\frac{|w| - u}{2}\right)^{1/2}$. Prove that $z^2 = w$ if $v \geq 0$ and $\overline{z}^2 = w$ if $v \leq 0$. Conclude that every complex number(with one exception) has two complex square roots. It is pretty easy to verify this calculation: If $v \geq 0$ then $(a+bi)^2 = (-a - bi)^2= a^2 - b^2 + 2abi = \frac{|w| + u}{2} + \frac{|w| - u}{2} + 2i\left(\frac{|w| + u}{2}\right)^{1/2}\left(\frac{|w| - u}{2}\right)^{1/2} = u + 2i \sqrt{\frac{|w|^2 - u^2}{4}} = u + 2i\sqrt{\frac{u^2 + v^2 - u^2}{4}} = u + 2i \sqrt{\frac{v^2}{4}} = u + 2i\frac{|v|}{2} = u + |v|i = u + vi$ On the other hand if $v \leq 0$ then it is similar to show that $(a - bi)^2 = (-a + bi)^2 = u - |v|i$ This completes the proof. Now for the backwards derivation attempt: We want $(a + bi)^2 = u + vi \implies a^2 - b^2 + 2abi = u + vi \implies a^2 - b^2 = u$ and $2ab = v$. So essentially we have a system of equations to solve for $a$ and $b$. I have been stuck on this for a couple hours. Maybe I am missing something obvious, but any help would be greatly appreciated.","I am using Baby Rudin for self study and as many of you are aware, it is one thing to follow Rudin's calculations to verify that his proof is correct. It is quite another to work backwards to see how he came up with his proof in the first place. I find that unless I do the latter, the former does absolutely nothing for me. Chapter 1 exercise 10 is as follows: Suppose $z = a + bi$, $w = u + vi$ and  $a = \left(\frac{|w| + u}{2}\right)^{1/2}$, $b = \left(\frac{|w| - u}{2}\right)^{1/2}$. Prove that $z^2 = w$ if $v \geq 0$ and $\overline{z}^2 = w$ if $v \leq 0$. Conclude that every complex number(with one exception) has two complex square roots. It is pretty easy to verify this calculation: If $v \geq 0$ then $(a+bi)^2 = (-a - bi)^2= a^2 - b^2 + 2abi = \frac{|w| + u}{2} + \frac{|w| - u}{2} + 2i\left(\frac{|w| + u}{2}\right)^{1/2}\left(\frac{|w| - u}{2}\right)^{1/2} = u + 2i \sqrt{\frac{|w|^2 - u^2}{4}} = u + 2i\sqrt{\frac{u^2 + v^2 - u^2}{4}} = u + 2i \sqrt{\frac{v^2}{4}} = u + 2i\frac{|v|}{2} = u + |v|i = u + vi$ On the other hand if $v \leq 0$ then it is similar to show that $(a - bi)^2 = (-a + bi)^2 = u - |v|i$ This completes the proof. Now for the backwards derivation attempt: We want $(a + bi)^2 = u + vi \implies a^2 - b^2 + 2abi = u + vi \implies a^2 - b^2 = u$ and $2ab = v$. So essentially we have a system of equations to solve for $a$ and $b$. I have been stuck on this for a couple hours. Maybe I am missing something obvious, but any help would be greatly appreciated.",,['real-analysis']
17,"Does the notion of ""rotation"" depend on a choice of metric?","Does the notion of ""rotation"" depend on a choice of metric?",,"Consider the statement : The Euclidean metric on $\mathbb{R}^n$ is rotationally invariant. I interpret this to mean (is this interpretation correct?): The Euclidean metric on $\mathbb{R}^n$ is invariant under the action of the orthogonal group $O(n)$. However, the orthogonal group $O(n)$ is defined in terms of the Euclidean metric (as the group of all self-maps $\mathbb{R}^n \to \mathbb{R}^n$ which preserve Euclidean distance and fix the origin). This suggests that we are implicitly using the following definition of ""rotation"": Rotations are the set of all (orientation-preserving) isometries of $\mathbb{R}^n$ which fix the origin. Question: Why is the first claim ""the Euclidean metric on $\mathbb{R}^n$ is rotationally invariant"" noteworthy/not trivial if we are implicitly using this definition/notion of rotation? (I.e., of course the metric is preserved by a group of isometries.) When we define ""rotations"", how are we not implicitly choosing a preferred metric on $\mathbb{R}^n$? /Question Clarifying example: In contrast, The taxicab metric on $\mathbb{R}^n$ is not rotationally invariant. In other words, The taxicab metric on $\mathbb{R}^n$ is not invariant under the action of $O(n)$. But what if we consider, instead of $O(n)$, what I will call $T(n)$ (""taxicab orthogonal group"") of all self-maps $\mathbb{R}^n \to \mathbb{R}^n$ which preserve taxicab distance and fix the origin? It seems fairly clear that we have: The taxicab metric on $\mathbb{R}^n$ is invariant under $T(n)$. or in other words The taxicab metric on $\mathbb{R}^n$ is ""taxicab-rotationally invariant"". Note: This is a very dumb question, so if you have any suggestions for how it could be improved, or if it should just be deleted, please say so (nicely).","Consider the statement : The Euclidean metric on $\mathbb{R}^n$ is rotationally invariant. I interpret this to mean (is this interpretation correct?): The Euclidean metric on $\mathbb{R}^n$ is invariant under the action of the orthogonal group $O(n)$. However, the orthogonal group $O(n)$ is defined in terms of the Euclidean metric (as the group of all self-maps $\mathbb{R}^n \to \mathbb{R}^n$ which preserve Euclidean distance and fix the origin). This suggests that we are implicitly using the following definition of ""rotation"": Rotations are the set of all (orientation-preserving) isometries of $\mathbb{R}^n$ which fix the origin. Question: Why is the first claim ""the Euclidean metric on $\mathbb{R}^n$ is rotationally invariant"" noteworthy/not trivial if we are implicitly using this definition/notion of rotation? (I.e., of course the metric is preserved by a group of isometries.) When we define ""rotations"", how are we not implicitly choosing a preferred metric on $\mathbb{R}^n$? /Question Clarifying example: In contrast, The taxicab metric on $\mathbb{R}^n$ is not rotationally invariant. In other words, The taxicab metric on $\mathbb{R}^n$ is not invariant under the action of $O(n)$. But what if we consider, instead of $O(n)$, what I will call $T(n)$ (""taxicab orthogonal group"") of all self-maps $\mathbb{R}^n \to \mathbb{R}^n$ which preserve taxicab distance and fix the origin? It seems fairly clear that we have: The taxicab metric on $\mathbb{R}^n$ is invariant under $T(n)$. or in other words The taxicab metric on $\mathbb{R}^n$ is ""taxicab-rotationally invariant"". Note: This is a very dumb question, so if you have any suggestions for how it could be improved, or if it should just be deleted, please say so (nicely).",,"['real-analysis', 'geometry', 'metric-spaces', 'euclidean-geometry', 'rotations']"
18,absolute value of supremum is smaller than or equal to supremum of absolute values,absolute value of supremum is smaller than or equal to supremum of absolute values,,"Is the following statement correct? ""The absolute value of supremum of a set is smaller than or equal to the supremum of the absolute values of the first set"" It would be helpful to proof that the absolute value of the integral of a function is smaller than or equal the integral of the absolute value.","Is the following statement correct? ""The absolute value of supremum of a set is smaller than or equal to the supremum of the absolute values of the first set"" It would be helpful to proof that the absolute value of the integral of a function is smaller than or equal the integral of the absolute value.",,"['real-analysis', 'integration', 'proof-writing', 'proof-explanation', 'supremum-and-infimum']"
19,Solving: $(b+c)^2=2011+bc$,Solving:,(b+c)^2=2011+bc,"Solve $$(b+c)^2=2011+bc$$ for integers $b$ and $c$. My tiny thoughts: $(b+c)^2=2011+bc\implies b^2+c^2+bc-2011=0\implies b^2+bc+c^2-2011=0$ Solving in $b$ as Quadratic.$$\implies b=\frac{-c\pm \sqrt{8044-3c^2}} {2}.$$ So $8044-3c^2=k^2$, as $b$ and $c$ are integers. We also have inequalities: $8044>3c^2,8044>3b^2\\ \ \ \ \ 51>c\ \ \ \  , \ \ \ \ 51>b$ How to proceed further. Help.","Solve $$(b+c)^2=2011+bc$$ for integers $b$ and $c$. My tiny thoughts: $(b+c)^2=2011+bc\implies b^2+c^2+bc-2011=0\implies b^2+bc+c^2-2011=0$ Solving in $b$ as Quadratic.$$\implies b=\frac{-c\pm \sqrt{8044-3c^2}} {2}.$$ So $8044-3c^2=k^2$, as $b$ and $c$ are integers. We also have inequalities: $8044>3c^2,8044>3b^2\\ \ \ \ \ 51>c\ \ \ \  , \ \ \ \ 51>b$ How to proceed further. Help.",,"['real-analysis', 'algebra-precalculus', 'quadratics']"
20,Real methods for evaluating $\int_{0}^{\infty}\frac{\log x \sin x}{e^x}\mathrm{d}x$,Real methods for evaluating,\int_{0}^{\infty}\frac{\log x \sin x}{e^x}\mathrm{d}x,"Recently I came across the following integral -  $$\int_{0}^{\infty}\frac{\log x \sin x}{e^x}\mathrm{d}x$$ One way to do it is to rewrite $\sin$ in terms of exponent, and then to relate to derivative of Gamma function. However this invokes complex analysis. Are there other ways using only real analysis techniques for solving this integral. In the standard approach we try to manipulate $\int_{0}^{\infty}x^ae^{-x} \sin x\mathrm{d}x$ to arrive at the Gamma function, but I do not have any idea, how to do it without rewriting $\sin x$. And if it is only rewriting it, and using some trivial arithmetic with complex numbers as if they were reals, it is OK, but I do not see how to do it. Maybe we should work with the original integral. However I think that we should at some point come up with the Gamma, as the answer contains Euler-Macheroni constant, which is related to derivatives of Gamma, and I do not see how otherwise we could reach this particular constant.","Recently I came across the following integral -  $$\int_{0}^{\infty}\frac{\log x \sin x}{e^x}\mathrm{d}x$$ One way to do it is to rewrite $\sin$ in terms of exponent, and then to relate to derivative of Gamma function. However this invokes complex analysis. Are there other ways using only real analysis techniques for solving this integral. In the standard approach we try to manipulate $\int_{0}^{\infty}x^ae^{-x} \sin x\mathrm{d}x$ to arrive at the Gamma function, but I do not have any idea, how to do it without rewriting $\sin x$. And if it is only rewriting it, and using some trivial arithmetic with complex numbers as if they were reals, it is OK, but I do not see how to do it. Maybe we should work with the original integral. However I think that we should at some point come up with the Gamma, as the answer contains Euler-Macheroni constant, which is related to derivatives of Gamma, and I do not see how otherwise we could reach this particular constant.",,"['real-analysis', 'integration', 'gamma-function']"
21,"Do Riemann-Stieltjes integrals ""iterate""?","Do Riemann-Stieltjes integrals ""iterate""?",,"Let's say we define: $$h(x) = \int_a^x f(t)dg(t),$$ then do we have for integrable functions $a$ that: $$\int_a^b a(u) dh(u) = \int_a^b a(u)f(u)dg(u) ?$$ I would like to know whether this holds for either the Riemann-Stieltjes or the Lebesgues-Stieltjes integral or any similar integral. Also for the sake of simplicity, feel free to assume that all relevant functions are as ""nice"" as you want, e.g. real-analytic. A yes/no answer would suffice, as would references which either prove or disprove such a result. Attempt: In ""nice"" cases, we hope that the behavior of the Riemann-Stieltjes sums will predict the behavior for the integrals, i.e. that the behavior will be respected/preserved by the appropriate limits. So let's write now instead: $$\int_a^b a(u) dh(u) \approx \sum_{i=0}^{n-1} a(x_i) (h(x_{i+1}) - h(x_i)) $$ Then by definition of $h$ we have that: $$h(x) \approx \sum_{j=0}^{m-1} f(t_j) (g(t_{j+1}) - g(t_j))$$ In particular for each $i$ we have that (setting $t_m = x_i, t_{m+1}=x_{i+1}$, etc.): $$h(x_{i+1}) - h(x_i)  \approx \sum_{j=0}^{m} f(t_j) (g(t_{j+1}) - g(t_j)) - \sum_{j=0}^{m-1} f(t_j) (g(t_{j+1}) - g(t_j)) = f(x_i)(g(x_{i+1})-g(x_i))$$ so that substituting into the above: $$\int_a^b a(u) dh(u) \approx \sum_{i=0}^{n-1} a(x_i) (h(x_{i+1}) - h(x_i)) \approx \sum_{i=0}^{n-1}a(x_i)f(x_i)(g(x_{i+1})-g(x_i)) \approx \int_a^b a(u)f(u)dg(u). $$ Of course, the above ""argument"" is extremely sloppy and would require considerable effort to be made rigorous, assuming that is even possible. But hopefully it suggests why I think the above result may be true -- I had hoped to find it or something similar on the Wikipedia page for the Riemann-Stieltjes integral , but it is not. Also one might expect the identity to be true by sloppily ""applying"" the fundamental theorem of calculus ($h(x)``=""\int_a^x f(t)g'(t)dt$ so $h'(u)``=""f(u)g'(u)$), i.e. when $$\int_a^b a(u)dh(u) ``="" \int_a^b a(u) h'(u) du ``="" \int_a^b  a(u) f(u) g'(u) du ``="" \int_a^b a(u) f(u) dg(u). $$","Let's say we define: $$h(x) = \int_a^x f(t)dg(t),$$ then do we have for integrable functions $a$ that: $$\int_a^b a(u) dh(u) = \int_a^b a(u)f(u)dg(u) ?$$ I would like to know whether this holds for either the Riemann-Stieltjes or the Lebesgues-Stieltjes integral or any similar integral. Also for the sake of simplicity, feel free to assume that all relevant functions are as ""nice"" as you want, e.g. real-analytic. A yes/no answer would suffice, as would references which either prove or disprove such a result. Attempt: In ""nice"" cases, we hope that the behavior of the Riemann-Stieltjes sums will predict the behavior for the integrals, i.e. that the behavior will be respected/preserved by the appropriate limits. So let's write now instead: $$\int_a^b a(u) dh(u) \approx \sum_{i=0}^{n-1} a(x_i) (h(x_{i+1}) - h(x_i)) $$ Then by definition of $h$ we have that: $$h(x) \approx \sum_{j=0}^{m-1} f(t_j) (g(t_{j+1}) - g(t_j))$$ In particular for each $i$ we have that (setting $t_m = x_i, t_{m+1}=x_{i+1}$, etc.): $$h(x_{i+1}) - h(x_i)  \approx \sum_{j=0}^{m} f(t_j) (g(t_{j+1}) - g(t_j)) - \sum_{j=0}^{m-1} f(t_j) (g(t_{j+1}) - g(t_j)) = f(x_i)(g(x_{i+1})-g(x_i))$$ so that substituting into the above: $$\int_a^b a(u) dh(u) \approx \sum_{i=0}^{n-1} a(x_i) (h(x_{i+1}) - h(x_i)) \approx \sum_{i=0}^{n-1}a(x_i)f(x_i)(g(x_{i+1})-g(x_i)) \approx \int_a^b a(u)f(u)dg(u). $$ Of course, the above ""argument"" is extremely sloppy and would require considerable effort to be made rigorous, assuming that is even possible. But hopefully it suggests why I think the above result may be true -- I had hoped to find it or something similar on the Wikipedia page for the Riemann-Stieltjes integral , but it is not. Also one might expect the identity to be true by sloppily ""applying"" the fundamental theorem of calculus ($h(x)``=""\int_a^x f(t)g'(t)dt$ so $h'(u)``=""f(u)g'(u)$), i.e. when $$\int_a^b a(u)dh(u) ``="" \int_a^b a(u) h'(u) du ``="" \int_a^b  a(u) f(u) g'(u) du ``="" \int_a^b a(u) f(u) dg(u). $$",,"['calculus', 'real-analysis', 'integration', 'reference-request', 'stieltjes-integral']"
22,Construct a set with different upper and lower Lebesgue density at zero.,Construct a set with different upper and lower Lebesgue density at zero.,,"For $\delta >0,$let $I(\delta)$ be the segment $(- \delta, \delta) \subset \mathbb{R}.$ Given $\alpha,\beta,$ and $0 \leq \alpha < \beta \leq 1,$ construct a measurable set $E \subset \mathbb{R}$ so that the upper and lower limits of  $$m(E \cap I(\delta))/2 \delta$$ are $\beta$ and $\alpha$ repsectively as $\delta \rightarrow 0.$ This is a question from Rudin's book on Real and Complex Analysis and it has been taken up before here: Lebesgue measurable subset of $\mathbb{R}$ with given metric density at zero .  Yuval Filmus has posted an answer which I'm trying to verify, but I can't make it to work. In his answer, he sets $E$ to be the duplication around zero of $$\bigcup_{n \geq 1} \left[\frac{1}{(2n)!}-\alpha\left(\frac{1}{(2n)!} - \frac{1}{(2n+1)!}\right),\frac{1}{(2n)!}\right] \cup \left[\frac{1}{(2n+1)!}-\beta\left(\frac{1}{(2n+1)!} - \frac{1}{(2n+2)!}\right),\frac{1}{(2n+1)!}\right].$$ I have no problem with showing that $\beta$ and $\alpha$ can occur as limits, but I can't show that any limit must lie between $\beta$ and $\alpha.$ My questions are thus: 1. Is the construction of Yuval correct? Does this work? 2. If not, what is an example that does work? My attempt with Yuval's example Let us try to show that any limit must lie between $\alpha$ and $\beta.$ Say that $0< r <1$ and that $ \dfrac{1}{(2n+1)!} < r \leq \dfrac{1}{(2n)!}.$ Then we have that $$m(E \cap I(1/(2n+1)!)) \leq m(E \cap I(r) ) \leq m(E \cap I(1/(2n)!)).$$ We have that $$m(E \cap I(1/(2n+1)!)) = 2\sum_{k=n+1}^\infty  \beta \frac{2k}{(2k+1)!} + 2 \sum_{k=n+2}^\infty \alpha \frac{2k}{(2k+1)!}$$ while $$m(E \cap I(1/(2n)!)) = 2\sum_{k=n}^\infty  \alpha \frac{2k}{(2k+1)!} + 2 \sum_{k=n+1}^\infty \beta \frac{2k}{(2k+1)!}.$$ We have  $$m(E \cap I(1/(2n+1)!))/(2r) \leq m(E \cap I(r) )/(2r) \leq m(E \cap I(1/(2n)!))/(2r).$$ We now want an upper and a lower bound on $m(E \cap I(r))/(2r).$For the upper bound, the obvious thing would be, since $(2n)!/2 \leq 1/(2r) < (2n+1)!/2$ would be to calculate  $$(2n+1)!/2m(E \cap I(1/(2n)!)).$$ This is $$(2n+1)!\sum_{k=n}^\infty  \alpha \frac{2k}{(2k+1)!} + (2n+1)! \sum_{k=n+1}^\infty \beta \frac{2k}{(2k+1)!}.$$ But this upper bound is much too crude to give us anything valuable. I also tried by taking the midpoint of the interval $[1/(2n+1)!,1/(2n)!]$ but that similarily seemed to give me nothing.","For $\delta >0,$let $I(\delta)$ be the segment $(- \delta, \delta) \subset \mathbb{R}.$ Given $\alpha,\beta,$ and $0 \leq \alpha < \beta \leq 1,$ construct a measurable set $E \subset \mathbb{R}$ so that the upper and lower limits of  $$m(E \cap I(\delta))/2 \delta$$ are $\beta$ and $\alpha$ repsectively as $\delta \rightarrow 0.$ This is a question from Rudin's book on Real and Complex Analysis and it has been taken up before here: Lebesgue measurable subset of $\mathbb{R}$ with given metric density at zero .  Yuval Filmus has posted an answer which I'm trying to verify, but I can't make it to work. In his answer, he sets $E$ to be the duplication around zero of $$\bigcup_{n \geq 1} \left[\frac{1}{(2n)!}-\alpha\left(\frac{1}{(2n)!} - \frac{1}{(2n+1)!}\right),\frac{1}{(2n)!}\right] \cup \left[\frac{1}{(2n+1)!}-\beta\left(\frac{1}{(2n+1)!} - \frac{1}{(2n+2)!}\right),\frac{1}{(2n+1)!}\right].$$ I have no problem with showing that $\beta$ and $\alpha$ can occur as limits, but I can't show that any limit must lie between $\beta$ and $\alpha.$ My questions are thus: 1. Is the construction of Yuval correct? Does this work? 2. If not, what is an example that does work? My attempt with Yuval's example Let us try to show that any limit must lie between $\alpha$ and $\beta.$ Say that $0< r <1$ and that $ \dfrac{1}{(2n+1)!} < r \leq \dfrac{1}{(2n)!}.$ Then we have that $$m(E \cap I(1/(2n+1)!)) \leq m(E \cap I(r) ) \leq m(E \cap I(1/(2n)!)).$$ We have that $$m(E \cap I(1/(2n+1)!)) = 2\sum_{k=n+1}^\infty  \beta \frac{2k}{(2k+1)!} + 2 \sum_{k=n+2}^\infty \alpha \frac{2k}{(2k+1)!}$$ while $$m(E \cap I(1/(2n)!)) = 2\sum_{k=n}^\infty  \alpha \frac{2k}{(2k+1)!} + 2 \sum_{k=n+1}^\infty \beta \frac{2k}{(2k+1)!}.$$ We have  $$m(E \cap I(1/(2n+1)!))/(2r) \leq m(E \cap I(r) )/(2r) \leq m(E \cap I(1/(2n)!))/(2r).$$ We now want an upper and a lower bound on $m(E \cap I(r))/(2r).$For the upper bound, the obvious thing would be, since $(2n)!/2 \leq 1/(2r) < (2n+1)!/2$ would be to calculate  $$(2n+1)!/2m(E \cap I(1/(2n)!)).$$ This is $$(2n+1)!\sum_{k=n}^\infty  \alpha \frac{2k}{(2k+1)!} + (2n+1)! \sum_{k=n+1}^\infty \beta \frac{2k}{(2k+1)!}.$$ But this upper bound is much too crude to give us anything valuable. I also tried by taking the midpoint of the interval $[1/(2n+1)!,1/(2n)!]$ but that similarily seemed to give me nothing.",,['real-analysis']
23,"Why do we need finiteness of the first set in ""continuity from above""?","Why do we need finiteness of the first set in ""continuity from above""?",,"If $E_1 \supset E_2 \supset ...$ and $\mu(E_1)<\infty$ then $\mu(\bigcap  E_j)=\lim \mu(E_j)$. But why need $\mu(E_1)<\infty$? Is $(-\infty,-n)$ an counter example?","If $E_1 \supset E_2 \supset ...$ and $\mu(E_1)<\infty$ then $\mu(\bigcap  E_j)=\lim \mu(E_j)$. But why need $\mu(E_1)<\infty$? Is $(-\infty,-n)$ an counter example?",,"['real-analysis', 'probability', 'measure-theory']"
24,Convergence in norm but not almost everywhere of $f_{t_n}=f(\cdot-t_n)$ to $f$,Convergence in norm but not almost everywhere of  to,f_{t_n}=f(\cdot-t_n) f,"My question is : is there a $L^1(\mathbf R)$ function $f$ and a sequence $(t_n)_n$ of reals with limit $0$ such that $f_{t_n}=f(\cdot-t_n)$ does not converges to $f$ a.e. ? There are several variations of this questions which are pretty much equivalent : one can drop the $L^1$ hypothesis and just assume mesurability of $f$, one could also assume that $f$ is the indicator function of a measurable subset of $[0;1]$. What i already know : The first thing to say is that $f_\tau$ converges to $f$ in $L^1$ norm when $\tau$ goes to zero. One way to prove that is to say that this is true for compactly suported continuous functions (because of absolute continuity) and conclude by density. Since the $f_{t_n}$ are converging in $L^1$ norm there exists a subsequence $t_{\sigma(n)}$ such that $f_{t_{\sigma(n)}}$ converges almost everywhere to $f$. However one cannot directly use this result to conclude with the convergence a.e. of $f_{t_n}$, indeed there are many examples of sequences of functions that are converging in $L^1$ norm but not convergent a.e. In fact this show that the a.e. convergence does not corespond to any topological convergence, since the caracterisation of convergence with sub-sub-sequences doesn't apply here. Another thing to note is that if $f$ is continuous at $x$ then $f_{t_n}(x)\to f(x)$ automatically, so if we search some $f$ such that $f_{t_n}$ does not converges a.e. to $f$ it has to be a function discontinuous on a set with positive measure. In fact for every function $g=0$ a.e. one must have that $f+g$ is discontinuous on a set with positive measure. I believe the answer to my question is yes, but it's mostly an intuition, i don't even have euristic arguments for that. So i tried to find an example, the two kind of $f$ i have thought of are : $f=\chi_{C}$ where $C$ is a fat cantor set or something like this kind of set and $$f(x)=\chi_{[0;1]}\sum_{k=0}^\infty \frac{2^{-k}}{|x-q_k|^{\alpha}}$$ where $q_n$ is an enumeration of the rationals of $[0;1]$. The problem is that it's hard to find the behavior of $|f_{t_n}(x)-f(x)|$ for general $x$ and such patological functions...","My question is : is there a $L^1(\mathbf R)$ function $f$ and a sequence $(t_n)_n$ of reals with limit $0$ such that $f_{t_n}=f(\cdot-t_n)$ does not converges to $f$ a.e. ? There are several variations of this questions which are pretty much equivalent : one can drop the $L^1$ hypothesis and just assume mesurability of $f$, one could also assume that $f$ is the indicator function of a measurable subset of $[0;1]$. What i already know : The first thing to say is that $f_\tau$ converges to $f$ in $L^1$ norm when $\tau$ goes to zero. One way to prove that is to say that this is true for compactly suported continuous functions (because of absolute continuity) and conclude by density. Since the $f_{t_n}$ are converging in $L^1$ norm there exists a subsequence $t_{\sigma(n)}$ such that $f_{t_{\sigma(n)}}$ converges almost everywhere to $f$. However one cannot directly use this result to conclude with the convergence a.e. of $f_{t_n}$, indeed there are many examples of sequences of functions that are converging in $L^1$ norm but not convergent a.e. In fact this show that the a.e. convergence does not corespond to any topological convergence, since the caracterisation of convergence with sub-sub-sequences doesn't apply here. Another thing to note is that if $f$ is continuous at $x$ then $f_{t_n}(x)\to f(x)$ automatically, so if we search some $f$ such that $f_{t_n}$ does not converges a.e. to $f$ it has to be a function discontinuous on a set with positive measure. In fact for every function $g=0$ a.e. one must have that $f+g$ is discontinuous on a set with positive measure. I believe the answer to my question is yes, but it's mostly an intuition, i don't even have euristic arguments for that. So i tried to find an example, the two kind of $f$ i have thought of are : $f=\chi_{C}$ where $C$ is a fat cantor set or something like this kind of set and $$f(x)=\chi_{[0;1]}\sum_{k=0}^\infty \frac{2^{-k}}{|x-q_k|^{\alpha}}$$ where $q_n$ is an enumeration of the rationals of $[0;1]$. The problem is that it's hard to find the behavior of $|f_{t_n}(x)-f(x)|$ for general $x$ and such patological functions...",,"['real-analysis', 'measure-theory', 'lp-spaces']"
25,Is the closure of a countable $G_\delta$ set countable?,Is the closure of a countable  set countable?,G_\delta,"This is in Cantor space ($2^\omega$ with the usual topology). In the course of trying to prove something else, I've found myself wanting to show that whenever $X$ is a countable $G_\delta$ set, the closure $\overline{X}$ is also countable. This seems perfectly reasonable (keep in mind that countable $G_\delta$ sets are nowhere dense), but I can't seem to prove it (note that the closure of a countable nowhere dense set can easily have size continuum - take the endpoints of a Cantor-like set). Even worse, I vaguely remember having this problem on an analysis exam years ago and getting it right . I'm sure I'm just having a silly moment (I was unsuccessful this morning in my quest to secure coffee) , but: is the closure of a countable $G_\delta$ subset of Cantor space, itself countable?","This is in Cantor space ($2^\omega$ with the usual topology). In the course of trying to prove something else, I've found myself wanting to show that whenever $X$ is a countable $G_\delta$ set, the closure $\overline{X}$ is also countable. This seems perfectly reasonable (keep in mind that countable $G_\delta$ sets are nowhere dense), but I can't seem to prove it (note that the closure of a countable nowhere dense set can easily have size continuum - take the endpoints of a Cantor-like set). Even worse, I vaguely remember having this problem on an analysis exam years ago and getting it right . I'm sure I'm just having a silly moment (I was unsuccessful this morning in my quest to secure coffee) , but: is the closure of a countable $G_\delta$ subset of Cantor space, itself countable?",,"['real-analysis', 'general-topology']"
26,"A continuous function, with discontinuous derivative, but the limit must exist.","A continuous function, with discontinuous derivative, but the limit must exist.",,"I was reading this question . The simplest examples of continuous functions, with discontinuous derivatives in some point, are usually of the form: $$ f(x) = \begin{cases}    x^2 \sin(1/x) &\mbox{if } x \neq 0 \\ 0 & \mbox{if } x=0.  \end{cases} $$ The derivative of $f$ is  $$ f'(x) = \begin{cases}    2 x \sin \left(\frac{1}{x}\right)-\cos \left(\frac{1}{x}\right)&\mbox{if } x \neq 0 \\ 0 & \mbox{if } x=0, \end{cases} $$ The derivative is discontinuous because the limit of $\cos \left(\frac{1}{x}\right)$ does not exist for $x\rightarrow 0$. Is there an example where the derivative is still discontinuous but with existing limit? Thanks","I was reading this question . The simplest examples of continuous functions, with discontinuous derivatives in some point, are usually of the form: $$ f(x) = \begin{cases}    x^2 \sin(1/x) &\mbox{if } x \neq 0 \\ 0 & \mbox{if } x=0.  \end{cases} $$ The derivative of $f$ is  $$ f'(x) = \begin{cases}    2 x \sin \left(\frac{1}{x}\right)-\cos \left(\frac{1}{x}\right)&\mbox{if } x \neq 0 \\ 0 & \mbox{if } x=0, \end{cases} $$ The derivative is discontinuous because the limit of $\cos \left(\frac{1}{x}\right)$ does not exist for $x\rightarrow 0$. Is there an example where the derivative is still discontinuous but with existing limit? Thanks",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
27,Bounded function and second derivative implies bounded derivative.,Bounded function and second derivative implies bounded derivative.,,"Suppose that $f$ is a twice differentiable real-valued function on the real line such that $|f(x)| \le 1$ and $|f''(x)|\le 1$ for all $x$. Find, with proof, a constant $b$ such that $|f'(x)| < b$ for all $x$. My instinct is to use the Mean Value Theorem, which gives me $$\frac{|f(x)-f(y)|}{|x-y|}=|f'(\xi)|, \quad \text{for some } \xi \text{ between } x \text{ and } y,$$ and $$\frac{|f'(s)-f'(t)|}{|s-t|}=|f''(\nu)| \le 1, \quad \text{for some } \nu \text{ between } s \text{ and } t.$$ From here, I have been unable to find a expression for the upper bound of $|f'(x)|$ in terms of $|f(x)|$ and $|f''(x)|$. Any suggestions on where to go from here, or perhaps a different way to approach the problem?","Suppose that $f$ is a twice differentiable real-valued function on the real line such that $|f(x)| \le 1$ and $|f''(x)|\le 1$ for all $x$. Find, with proof, a constant $b$ such that $|f'(x)| < b$ for all $x$. My instinct is to use the Mean Value Theorem, which gives me $$\frac{|f(x)-f(y)|}{|x-y|}=|f'(\xi)|, \quad \text{for some } \xi \text{ between } x \text{ and } y,$$ and $$\frac{|f'(s)-f'(t)|}{|s-t|}=|f''(\nu)| \le 1, \quad \text{for some } \nu \text{ between } s \text{ and } t.$$ From here, I have been unable to find a expression for the upper bound of $|f'(x)|$ in terms of $|f(x)|$ and $|f''(x)|$. Any suggestions on where to go from here, or perhaps a different way to approach the problem?",,"['real-analysis', 'derivatives']"
28,Intuition for proof of monotone class theorem?,Intuition for proof of monotone class theorem?,,"Here is the monotone class theorem from my real analysis textbook. Suppose $\mathcal{A}_0$ is an algebra, $\mathcal{A}$ is the smallest $\sigma$ -algebra containing $\mathcal{A}_0$ , and $\mathcal{M}$ is the smallest monotone class containing $\mathcal{A}_0$ . Then $\mathcal{M} = \mathcal{A}$ . Here is the proof in the book. A $\sigma$ -algebra is clearly a monotone class, so $\mathcal{M} \subset \mathcal{A}$ . We must show $\mathcal{A} \subset \mathcal{M}$ . Let $\mathcal{N}_1 = \{A \in \mathcal{M} : A^c \in \mathcal{M}\}$ . Note $\mathcal{N}_1$ is contained in $\mathcal{M}$ and contains $\mathcal{A}_0$ . If $A_i \uparrow A$ and each $A_i \in \mathcal{N}_1$ , then each $A_i^c \in \mathcal{M}$ and $A_i^c \downarrow A^c$ . Since $\mathcal{M}$ is a monotone class, $A^c \in \mathcal{M}$ , and so $A \in \mathcal{N}_1$ . Similarly, if $A_i \downarrow A$ and each $A_i \in \mathcal{N}_1$ , then $A \in \mathcal{N}_1$ . Therefore $\mathcal{N}_1$ is a monotone class. Hence $\mathcal{N}_1 = \mathcal{M}$ , and we conclude $\mathcal{M}$ is closed under the operation of taking complements. Let $\mathcal{N}_2 = \{A \in \mathcal{M} : A \cap B \in \mathcal{M} \text{ for all }B \in \mathcal{A}_0\}$ . Note the following: $\mathcal{N}_2$ is contained in $\mathcal{M}$ and $\mathcal{N}_2$ contains $\mathcal{A}_0$ because $\mathcal{A}_0$ is an algebra. If $A_i \uparrow A$ , each $A_i \in \mathcal{N}_2$ , and $B \in \mathcal{A}_0$ , then $A \cap B = \cup_{i = 1}^\infty (A_i \cap B)$ . Because $\mathcal{M}$ is a monotone class, $A \cap B \in \mathcal{M}$ , which implies $A \in \mathcal{N}_2$ . We use a similar argument when $A_i \downarrow A$ . Therefore $\mathcal{N}_2$ is a monotone class, and we conclude $\mathcal{N}_2 = \mathcal{M}$ . In other words, if $B \in \mathcal{A}_0$ and $A \in \mathcal{M}$ , then $A \cap B \in \mathcal{M}$ . Let $\mathcal{N}_3 = \{A \in \mathcal{M} : A \cap B \in \mathcal{M} \text{ for all }B \in \mathcal{M}\}$ . As in the preceding paragraph, $\mathcal{N}_3$ is a monotone class contained in $\mathcal{M}$ . By the last sentence of the preceding paragraph, $\mathcal{N}_3$ contains $\mathcal{A}_0$ . Hence $\mathcal{N}_3 = \mathcal{M}$ . We thus have that $\mathcal{M}$ is a monotone class closed under the operations of taking complements and taking finite intersections. If $A_1, A_2, \ldots$ are elements of $\mathcal{M}$ , then $B_n = A_1 \cap \ldots \cap A_n \in \mathcal{M}$ for each $n$ and $B_n \downarrow \cap_{i = 1}^\infty A_i$ . Since $\mathcal{M}$ is a monotone class, we have that $\cap_{i = 1}^\infty A_i \in \mathcal{M}$ . If $A_1, A_2, \ldots $ are in $\mathcal{M}$ , then $A_1^c, A_2^c, \ldots$ are in $\mathcal{M}$ , hence $\cap_{i = 1}^\infty A_i^c \in \mathcal{M}$ , and then $$\cup_{i = 1}^\infty A_i = (\cap_{i = 1}^\infty A_i^c)^c \in \mathcal{M}.$$ This shows that $\mathcal{M}$ is a $\sigma$ -algebra, and so $A \subset \mathcal{M}$ . The proof of this theorem is rather technical. I have a few questions about it. What is the underlying intuition behind the proof? What are the one to three key ideas this proof boils down to? What is the geometric significance of this result/how can I visualize it? Thanks in advance!","Here is the monotone class theorem from my real analysis textbook. Suppose is an algebra, is the smallest -algebra containing , and is the smallest monotone class containing . Then . Here is the proof in the book. A -algebra is clearly a monotone class, so . We must show . Let . Note is contained in and contains . If and each , then each and . Since is a monotone class, , and so . Similarly, if and each , then . Therefore is a monotone class. Hence , and we conclude is closed under the operation of taking complements. Let . Note the following: is contained in and contains because is an algebra. If , each , and , then . Because is a monotone class, , which implies . We use a similar argument when . Therefore is a monotone class, and we conclude . In other words, if and , then . Let . As in the preceding paragraph, is a monotone class contained in . By the last sentence of the preceding paragraph, contains . Hence . We thus have that is a monotone class closed under the operations of taking complements and taking finite intersections. If are elements of , then for each and . Since is a monotone class, we have that . If are in , then are in , hence , and then This shows that is a -algebra, and so . The proof of this theorem is rather technical. I have a few questions about it. What is the underlying intuition behind the proof? What are the one to three key ideas this proof boils down to? What is the geometric significance of this result/how can I visualize it? Thanks in advance!","\mathcal{A}_0 \mathcal{A} \sigma \mathcal{A}_0 \mathcal{M} \mathcal{A}_0 \mathcal{M} = \mathcal{A} \sigma \mathcal{M} \subset \mathcal{A} \mathcal{A} \subset \mathcal{M} \mathcal{N}_1 = \{A \in \mathcal{M} : A^c \in \mathcal{M}\} \mathcal{N}_1 \mathcal{M} \mathcal{A}_0 A_i \uparrow A A_i \in \mathcal{N}_1 A_i^c \in \mathcal{M} A_i^c \downarrow A^c \mathcal{M} A^c \in \mathcal{M} A \in \mathcal{N}_1 A_i \downarrow A A_i \in \mathcal{N}_1 A \in \mathcal{N}_1 \mathcal{N}_1 \mathcal{N}_1 = \mathcal{M} \mathcal{M} \mathcal{N}_2 = \{A \in \mathcal{M} : A \cap B \in \mathcal{M} \text{ for all }B \in \mathcal{A}_0\} \mathcal{N}_2 \mathcal{M} \mathcal{N}_2 \mathcal{A}_0 \mathcal{A}_0 A_i \uparrow A A_i \in \mathcal{N}_2 B \in \mathcal{A}_0 A \cap B = \cup_{i = 1}^\infty (A_i \cap B) \mathcal{M} A \cap B \in \mathcal{M} A \in \mathcal{N}_2 A_i \downarrow A \mathcal{N}_2 \mathcal{N}_2 = \mathcal{M} B \in \mathcal{A}_0 A \in \mathcal{M} A \cap B \in \mathcal{M} \mathcal{N}_3 = \{A \in \mathcal{M} : A \cap B \in \mathcal{M} \text{ for all }B \in \mathcal{M}\} \mathcal{N}_3 \mathcal{M} \mathcal{N}_3 \mathcal{A}_0 \mathcal{N}_3 = \mathcal{M} \mathcal{M} A_1, A_2, \ldots \mathcal{M} B_n = A_1 \cap \ldots \cap A_n \in \mathcal{M} n B_n \downarrow \cap_{i = 1}^\infty A_i \mathcal{M} \cap_{i = 1}^\infty A_i \in \mathcal{M} A_1, A_2, \ldots  \mathcal{M} A_1^c, A_2^c, \ldots \mathcal{M} \cap_{i = 1}^\infty A_i^c \in \mathcal{M} \cup_{i = 1}^\infty A_i = (\cap_{i = 1}^\infty A_i^c)^c \in \mathcal{M}. \mathcal{M} \sigma A \subset \mathcal{M}","['real-analysis', 'general-topology']"
29,"Real Analysis, Folland Problem 1.5.29 Lebesgue measurable set","Real Analysis, Folland Problem 1.5.29 Lebesgue measurable set",,"1.5.29 - Let $E$ be a Lebesgue measurable set. a.) If $E\subset N$ where $N$ is the nonmeasurable set described in section 1.1, then $m(E) = 0$ . b.) If $m(E) > 0$ , then $E$ contains a nonmeasurable set. (It suffices to assume $E\subset [0,1]$ . In the notation of section 1.1, $E = \bigcup_{r\in R}E\cap N_r$ ). Proof a.) Let $E\subset N$ and $N\subset [0,1)$ . In the notation of section 1.1, let $$E_r = \{x + r: x\in E\cap [0,1-r)\}\cup \{x+r-1:x\in E\cap [1-r,1)\}$$ Then for all $r\in R = \mathbb{Q}\cap [0,1)$ , since $E_r \subset N_r$ and the collection $\{N_r\}_{r\in R}$ is a disjoint collection, it follows that $\{E_r\}_{r\in R}$ is a disjoint collection. By the translation invariance and finite additivty of Lebesgue measure, we have that each $E_r$ is measurable and $m(E_r) = m(E)$ for all $r\in\mathbb{R}$ . So $$1\geq m\left(\bigcup_{r\in R}E_r\right) = \sum_{r\in R}m(E_r) = \sum_{r\in R}m(E)$$ and therefore $m(E) = 0$ ? Not sure if this is right. Proof b.) Suppose that $E\subset [0,1]$ is a subset with the property that $E$ is measurable. Then for each $r\in R$ , the set $E\cap N_r$ is measurable. By the translation invariance and finite additivity of Lebesgue measure, the set $E_{1-r}\cap N$ is therefore measurable, and hence must have measure $0$ by part a.). Since the collection $\{N_r\}_{r\in R}$ is disjoint we have that $$m(E) = m\left(\bigcup_{r\in R}(E\cap N_r)\right) = \sum_{r\in R}m(E\cap N_r) = \sum_{r\in R}m(E_{1-r}\cap N) = 0$$ Not sure if this is right either. Any suggestions on these is greatly appreciated.","1.5.29 - Let be a Lebesgue measurable set. a.) If where is the nonmeasurable set described in section 1.1, then . b.) If , then contains a nonmeasurable set. (It suffices to assume . In the notation of section 1.1, ). Proof a.) Let and . In the notation of section 1.1, let Then for all , since and the collection is a disjoint collection, it follows that is a disjoint collection. By the translation invariance and finite additivty of Lebesgue measure, we have that each is measurable and for all . So and therefore ? Not sure if this is right. Proof b.) Suppose that is a subset with the property that is measurable. Then for each , the set is measurable. By the translation invariance and finite additivity of Lebesgue measure, the set is therefore measurable, and hence must have measure by part a.). Since the collection is disjoint we have that Not sure if this is right either. Any suggestions on these is greatly appreciated.","E E\subset N N m(E) = 0 m(E) > 0 E E\subset [0,1] E = \bigcup_{r\in R}E\cap N_r E\subset N N\subset [0,1) E_r = \{x + r: x\in E\cap [0,1-r)\}\cup \{x+r-1:x\in E\cap [1-r,1)\} r\in R = \mathbb{Q}\cap [0,1) E_r \subset N_r \{N_r\}_{r\in R} \{E_r\}_{r\in R} E_r m(E_r) = m(E) r\in\mathbb{R} 1\geq m\left(\bigcup_{r\in R}E_r\right) = \sum_{r\in R}m(E_r) = \sum_{r\in R}m(E) m(E) = 0 E\subset [0,1] E r\in R E\cap N_r E_{1-r}\cap N 0 \{N_r\}_{r\in R} m(E) = m\left(\bigcup_{r\in R}(E\cap N_r)\right) = \sum_{r\in R}m(E\cap N_r) = \sum_{r\in R}m(E_{1-r}\cap N) = 0","['real-analysis', 'measure-theory']"
30,Prove series converge for almost every $x$,Prove series converge for almost every,x,"Let $f\in L^p(\mathbb{R})$, $1<p<\infty$, and let $\alpha>1-\frac{1}{p}$. Show that the series $$\sum_{n=1}^{\infty}\int_n^{n+n^{-\alpha}} |f(x+y)|dy$$ converges for a.e. $x\in \mathbb{R}$. The question came from old qualifying exam . Although not specified, in the context of the test measure on $\mathbb{R}$ is always Lebesgue measure. I define $q=(1-\frac{1}{p})^{-1}$. The integral can be estimated by Hölder's inequality $$\begin{align}\int_n^{n+n^{-\alpha}} |f(x+y)|dy&=\int_{n+x}^{n+n^{-\alpha}+x} |f(y)|dy\\&=||f||_1\\&\leq||f||_p||1||_q\\&=\left(\int_{n+x}^{n+n^{-\alpha}+x} |f(y)|^pdy\right)^{\frac{1}{p}}(n^{-\alpha})^{\frac{1}{q}}\end{align}$$ Here $||\cdot||_p$ means the norm on $L^p \left([n+x,n+n^{-\alpha}+x]\right)$. Suppose we can prove the integral $\int_{n+x}^{n+n^{-\alpha}+x} |f(y)|^pdy$ decays faster than $n^{-(1+\frac{1}{q})}$ then the $n^{th}$ term should decay faster than $\frac{1}{n}$ and therefore converge. However this is the farthest I can get, any idea how to proceed?","Let $f\in L^p(\mathbb{R})$, $1<p<\infty$, and let $\alpha>1-\frac{1}{p}$. Show that the series $$\sum_{n=1}^{\infty}\int_n^{n+n^{-\alpha}} |f(x+y)|dy$$ converges for a.e. $x\in \mathbb{R}$. The question came from old qualifying exam . Although not specified, in the context of the test measure on $\mathbb{R}$ is always Lebesgue measure. I define $q=(1-\frac{1}{p})^{-1}$. The integral can be estimated by Hölder's inequality $$\begin{align}\int_n^{n+n^{-\alpha}} |f(x+y)|dy&=\int_{n+x}^{n+n^{-\alpha}+x} |f(y)|dy\\&=||f||_1\\&\leq||f||_p||1||_q\\&=\left(\int_{n+x}^{n+n^{-\alpha}+x} |f(y)|^pdy\right)^{\frac{1}{p}}(n^{-\alpha})^{\frac{1}{q}}\end{align}$$ Here $||\cdot||_p$ means the norm on $L^p \left([n+x,n+n^{-\alpha}+x]\right)$. Suppose we can prove the integral $\int_{n+x}^{n+n^{-\alpha}+x} |f(y)|^pdy$ decays faster than $n^{-(1+\frac{1}{q})}$ then the $n^{th}$ term should decay faster than $\frac{1}{n}$ and therefore converge. However this is the farthest I can get, any idea how to proceed?",,"['real-analysis', 'sequences-and-series']"
31,Prove a sequence is a Cauchy and thus convergent,Prove a sequence is a Cauchy and thus convergent,,"Suppose that $0<\alpha<1$ and that $\{ x_n\}$ is a sequence which satisfies $$|x_{n+1}-x_n| \le \alpha^n$$ $$n= 1,2,....$$ Prove that $\{x_n\}$ is a Cauchy sequence and thus converges. Give an example of a sequence $\{ y_n\}$ s.t $y_n \to \infty $ but $$|y_{n+1}-y_n| \to 0$$ as $n \to \infty$ So here's my take so far, in order to understand from the beginning the definition of Cauchy is $\{v_n\}_n$ is a Cauchy sequence if for all $\varepsilon>0$ there exists $N\in \Bbb N$ such that for all natural numbers $n,m\geq N$ : $|v_n-v_m|<\varepsilon$ . and I said if $n > m$ $$|x_n-x_m| \le |x_n-x_{n-1}|+|x_{n-1}-x_{n-2}|....+|x_{m+1}-x_m|$$ $$\le \alpha^{n-1}+\alpha^{n-2}+.............+\alpha^{m}$$ $$= 1-\frac{\alpha^{n-m}}{1-\alpha}$$ but from here I'm not quite convinced how I could process further... Could i get some help?","Suppose that and that is a sequence which satisfies Prove that is a Cauchy sequence and thus converges. Give an example of a sequence s.t but as So here's my take so far, in order to understand from the beginning the definition of Cauchy is is a Cauchy sequence if for all there exists such that for all natural numbers : . and I said if but from here I'm not quite convinced how I could process further... Could i get some help?","0<\alpha<1 \{ x_n\} |x_{n+1}-x_n| \le \alpha^n n= 1,2,.... \{x_n\} \{ y_n\} y_n \to \infty  |y_{n+1}-y_n| \to 0 n \to \infty \{v_n\}_n \varepsilon>0 N\in \Bbb N n,m\geq N |v_n-v_m|<\varepsilon n > m |x_n-x_m| \le |x_n-x_{n-1}|+|x_{n-1}-x_{n-2}|....+|x_{m+1}-x_m| \le \alpha^{n-1}+\alpha^{n-2}+.............+\alpha^{m} = 1-\frac{\alpha^{n-m}}{1-\alpha}","['real-analysis', 'sequences-and-series', 'cauchy-sequences']"
32,Construction of a continuous function which maps some point in the interior of an open set to the boundary of the Range,Construction of a continuous function which maps some point in the interior of an open set to the boundary of the Range,,"I was studying the Inverse function theorem when I came across the following problems : (Let the closed set $V$ i.e the range have non-empty interior) Does there exist a continuous onto function from an open set $U$ in $\mathbb{R}^n $ to a closed set $V$ in $\mathbb{R}^m$  such that some points in the interior of $U$ get mapped to the boundary of $V$? Does there exist a continuous $1-1$ map from an open set $U$ in $\mathbb{R}^n $ to a closed set $V$ in $\mathbb{R}^m$  such that some points in the interior of $U$ get mapped to the boundary of $V$? If there are examples in $C(\mathbb{R})$ i.e continuous functions from $\mathbb{R}$ to $\mathbb{R}$, then that would be great too! Though I do need some example in the general case too. Simpler examples will be really appreciated. Thanks in advance. Edit: The case (1) can be dealt with using any ""cut-off"" function.  e.g  let $U,V$ two balls around $0$ in $\mathbb{R}^n $ with radius $r(>1)$ and $1$, and be open and closed respectively. Let $f: U \rightarrow V $ such that $x \in V \implies f(x)=x$ and $x \in U-V \implies f(x)= x/||x|| $.","I was studying the Inverse function theorem when I came across the following problems : (Let the closed set $V$ i.e the range have non-empty interior) Does there exist a continuous onto function from an open set $U$ in $\mathbb{R}^n $ to a closed set $V$ in $\mathbb{R}^m$  such that some points in the interior of $U$ get mapped to the boundary of $V$? Does there exist a continuous $1-1$ map from an open set $U$ in $\mathbb{R}^n $ to a closed set $V$ in $\mathbb{R}^m$  such that some points in the interior of $U$ get mapped to the boundary of $V$? If there are examples in $C(\mathbb{R})$ i.e continuous functions from $\mathbb{R}$ to $\mathbb{R}$, then that would be great too! Though I do need some example in the general case too. Simpler examples will be really appreciated. Thanks in advance. Edit: The case (1) can be dealt with using any ""cut-off"" function.  e.g  let $U,V$ two balls around $0$ in $\mathbb{R}^n $ with radius $r(>1)$ and $1$, and be open and closed respectively. Let $f: U \rightarrow V $ such that $x \in V \implies f(x)=x$ and $x \in U-V \implies f(x)= x/||x|| $.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'continuity']"
33,Let $x$ be a real number. Prove the existence of a unique integer $a$ such that $a \leq x < a+1$,Let  be a real number. Prove the existence of a unique integer  such that,x a a \leq x < a+1,"Let $x\in \mathbb{R}$ , Using the Well-Ordering Property of $\mathbb{N}$ and the Archimedean Property of $\mathbb{R}$, show that there exist a unique $a \in  \mathbb{Z}$ such that $a \leq x < a+1$ My approach so far: Suppose $x$ greater than some Integer $a$. $x\geq a$ By Archimedean Property of $\mathbb{R}$, there exist $n_{x} \in \mathbb{N},  x<n_{x}$ Combining those 2 inequality I have $a\leq x < n_{x}$. But I do not know how to proceed from here, any help or insights is deeply appreciated. Thank you for reading my post.","Let $x\in \mathbb{R}$ , Using the Well-Ordering Property of $\mathbb{N}$ and the Archimedean Property of $\mathbb{R}$, show that there exist a unique $a \in  \mathbb{Z}$ such that $a \leq x < a+1$ My approach so far: Suppose $x$ greater than some Integer $a$. $x\geq a$ By Archimedean Property of $\mathbb{R}$, there exist $n_{x} \in \mathbb{N},  x<n_{x}$ Combining those 2 inequality I have $a\leq x < n_{x}$. But I do not know how to proceed from here, any help or insights is deeply appreciated. Thank you for reading my post.",,"['real-analysis', 'elementary-number-theory', 'real-numbers', 'integers']"
34,Evaluating Limits - Finding Multiple Results Only one of which is Correct,Evaluating Limits - Finding Multiple Results Only one of which is Correct,,"When I calculate the limit  $$\lim_{x\to\infty}\sqrt{x^2+x} - \sqrt {x^2-x}$$ I get $2$ answers for this question: $1$ and $0$ but $1$ is the right answer. I don't know why this is the case, however. If you multiply by the conjugate divided by the conjugate (1), you take the radical out of top and get it in the bottom and then if you factor out $x$ from both and cancel it with top u get $2/2$ which is $1$. But if you just factor you get  $$\lim_{x\to\infty}  x (\sqrt {1+ 1/x} - \sqrt{1 - 1/x}.$$ This simplifies to 0. So how would you know which method to use if you didn't know the right answer?","When I calculate the limit  $$\lim_{x\to\infty}\sqrt{x^2+x} - \sqrt {x^2-x}$$ I get $2$ answers for this question: $1$ and $0$ but $1$ is the right answer. I don't know why this is the case, however. If you multiply by the conjugate divided by the conjugate (1), you take the radical out of top and get it in the bottom and then if you factor out $x$ from both and cancel it with top u get $2/2$ which is $1$. But if you just factor you get  $$\lim_{x\to\infty}  x (\sqrt {1+ 1/x} - \sqrt{1 - 1/x}.$$ This simplifies to 0. So how would you know which method to use if you didn't know the right answer?",,"['calculus', 'real-analysis', 'limits']"
35,When does Rudin's change of variables formula break?,When does Rudin's change of variables formula break?,,"Here is Rudin's change of variables Theorem: My question is this: what are examples where $\varphi$ not being strictly increasing or not mapping the interval $[a,b]$ to $[A,B]$ breaks the theorem? For example, I am considering $\varphi$ a parabola with minimum point in the unit interval. Using Rudin's special case remark ($\alpha(x)=x$ and $\beta(x)=\varphi$) I keep computing $$\int_0^1 f(x)dx=\int_0^1f(\varphi(y))\varphi'(y)dy$$ with various choices of $f$, and I keep getting these integrals are equal. Does this mean $\varphi$ doesn't need such strict conditions? If we let $a$ be the minimum point this seems surprising since $\varphi$ is only strictly increasing on $[a,1]$.","Here is Rudin's change of variables Theorem: My question is this: what are examples where $\varphi$ not being strictly increasing or not mapping the interval $[a,b]$ to $[A,B]$ breaks the theorem? For example, I am considering $\varphi$ a parabola with minimum point in the unit interval. Using Rudin's special case remark ($\alpha(x)=x$ and $\beta(x)=\varphi$) I keep computing $$\int_0^1 f(x)dx=\int_0^1f(\varphi(y))\varphi'(y)dy$$ with various choices of $f$, and I keep getting these integrals are equal. Does this mean $\varphi$ doesn't need such strict conditions? If we let $a$ be the minimum point this seems surprising since $\varphi$ is only strictly increasing on $[a,1]$.",,"['real-analysis', 'definite-integrals']"
36,If $f\in L^{+}$ and $\int f < \infty$ then there exists a null set and a $\sigma$-finite set,If  and  then there exists a null set and a -finite set,f\in L^{+} \int f < \infty \sigma,"This comes from Chapter 2, Real Analysis, by Folland Proposition 2.20 - If $f\in L^{+}$ and $\int f < \infty$ then $\{x:f(x) = \infty\}$ is a null set and $\{x:f(x) > 0\}$ is $\sigma$-finite proof (1st part): Let $E = \{x:f(x) = \infty\}$, then $E$ is measurable. Define a simple function $\phi_n = n1_{E} \ \ \forall n\geq 1$ with $0 \leq \phi_n \leq f$, so $$\int f \geq \int \phi_n = n\mu(E)$$ Thus, $$\frac{1}{n}\int f \geq \mu(E) \ \ \forall n\geq 1$$ Since, $0\leq \int f < \infty$ it follows that $\mu(E) = 0$ Proof (2nd part): Now, set $$\{x:f(x) > 0\} = \bigcup_{n\in\mathbb{N}}\{f(x) > 1/n\}$$ For each $n$, set $\phi_n = \frac{1}{n}1_{\{f(x) > 1/n\}}$ with $0\leq \phi_n \leq f$, so $$\int f \geq \int \phi_n = \frac{1}{n}\mu(\{f(x) > 1/n\})$$ Thus, $$n\int f \geq \mu(\{f(x) > 1/n\})$$ Since, $0\leq \mu(\{f(x) > 1/n\}) \leq n\int f < \infty \ \ \forall n$ so, $\{x:f(x) > 0 \}$ is $\sigma$-finite.","This comes from Chapter 2, Real Analysis, by Folland Proposition 2.20 - If $f\in L^{+}$ and $\int f < \infty$ then $\{x:f(x) = \infty\}$ is a null set and $\{x:f(x) > 0\}$ is $\sigma$-finite proof (1st part): Let $E = \{x:f(x) = \infty\}$, then $E$ is measurable. Define a simple function $\phi_n = n1_{E} \ \ \forall n\geq 1$ with $0 \leq \phi_n \leq f$, so $$\int f \geq \int \phi_n = n\mu(E)$$ Thus, $$\frac{1}{n}\int f \geq \mu(E) \ \ \forall n\geq 1$$ Since, $0\leq \int f < \infty$ it follows that $\mu(E) = 0$ Proof (2nd part): Now, set $$\{x:f(x) > 0\} = \bigcup_{n\in\mathbb{N}}\{f(x) > 1/n\}$$ For each $n$, set $\phi_n = \frac{1}{n}1_{\{f(x) > 1/n\}}$ with $0\leq \phi_n \leq f$, so $$\int f \geq \int \phi_n = \frac{1}{n}\mu(\{f(x) > 1/n\})$$ Thus, $$n\int f \geq \mu(\{f(x) > 1/n\})$$ Since, $0\leq \mu(\{f(x) > 1/n\}) \leq n\int f < \infty \ \ \forall n$ so, $\{x:f(x) > 0 \}$ is $\sigma$-finite.",,"['real-analysis', 'measure-theory', 'proof-verification']"
37,A Lipschitz transform maps measurable set to measurable,A Lipschitz transform maps measurable set to measurable,,"Prove that a Lipschitz transform $T: \mathbb{R}^n \to \mathbb{R}^n$ maps measurable set to measurable. Assume the only thing that we know about Lipschitz transform is that we can find $M>0$ such that for any $x,y \in \mathbb{R}^d$, $|T(x)-T(y)|\leq M |x-y| \tag{1}$ To use any property of Lipschitz transforms rather than above, we have to prove it. We can prove it as follows: $T(\cdot)$ maps every Null set to a Null set (I proved). $T(\cdot)$ maps every $F_{\sigma}$ set to a $F_{\sigma}$ set. Every measurable set can be written as union of a $F_{\sigma}$ and a Null set. (I proved) I proved step 2 when $m(F_{\sigma})<\infty$. For a general $m(F_{\sigma})$, we have to first show that the $F_{\sigma}$ can be written as countable union of compact sets, I don't know how to do. Also, the map of each compact set is compact, which I don't know how to prove using just using property (1). Any idea? (I found a similar question-solution that doesn't address my needs here $f$ maps measurable sets to measurable sets )","Prove that a Lipschitz transform $T: \mathbb{R}^n \to \mathbb{R}^n$ maps measurable set to measurable. Assume the only thing that we know about Lipschitz transform is that we can find $M>0$ such that for any $x,y \in \mathbb{R}^d$, $|T(x)-T(y)|\leq M |x-y| \tag{1}$ To use any property of Lipschitz transforms rather than above, we have to prove it. We can prove it as follows: $T(\cdot)$ maps every Null set to a Null set (I proved). $T(\cdot)$ maps every $F_{\sigma}$ set to a $F_{\sigma}$ set. Every measurable set can be written as union of a $F_{\sigma}$ and a Null set. (I proved) I proved step 2 when $m(F_{\sigma})<\infty$. For a general $m(F_{\sigma})$, we have to first show that the $F_{\sigma}$ can be written as countable union of compact sets, I don't know how to do. Also, the map of each compact set is compact, which I don't know how to prove using just using property (1). Any idea? (I found a similar question-solution that doesn't address my needs here $f$ maps measurable sets to measurable sets )",,"['real-analysis', 'lebesgue-measure', 'transformation']"
38,Approximating $x^m$ by exponentials,Approximating  by exponentials,x^m,"What's a nice explicit example of a sequence of functions $f_n$: $\mathbb{R}\longrightarrow\mathbb{R}$ of the form $$a_1e^{c_1x}+\ldots + a_ke^{c_kx},\;\;\; a_i, c_i\in\mathbb{R}$$ that converges to $x^m$ uniformly on every $[a, b]$? One such example is obtainable by taking the power series for $\log$, composing it with $\exp$ and then using some product of series formula $m$ times, but that results in horrible expressions. There may be something simpler since we are allowing the $c_i$ to be arbitrary reals instead of positive integers.","What's a nice explicit example of a sequence of functions $f_n$: $\mathbb{R}\longrightarrow\mathbb{R}$ of the form $$a_1e^{c_1x}+\ldots + a_ke^{c_kx},\;\;\; a_i, c_i\in\mathbb{R}$$ that converges to $x^m$ uniformly on every $[a, b]$? One such example is obtainable by taking the power series for $\log$, composing it with $\exp$ and then using some product of series formula $m$ times, but that results in horrible expressions. There may be something simpler since we are allowing the $c_i$ to be arbitrary reals instead of positive integers.",,"['calculus', 'real-analysis']"
39,Real orthogonal matrices with fixed entry value has zero Haar-measure?,Real orthogonal matrices with fixed entry value has zero Haar-measure?,,"Consider the set of real orthogonal matrices of size $n \times n$ such that one entry, say $a_{i,j}$ for fixed $i$ and $j$, satisfies $a_{i,j}=0$. Has this set zero Haar-measure? A simple proof, in affirmative (which I believe) or negative case, would be appreciated. Many Thanks in advance.","Consider the set of real orthogonal matrices of size $n \times n$ such that one entry, say $a_{i,j}$ for fixed $i$ and $j$, satisfies $a_{i,j}=0$. Has this set zero Haar-measure? A simple proof, in affirmative (which I believe) or negative case, would be appreciated. Many Thanks in advance.",,"['real-analysis', 'matrices', 'measure-theory', 'lie-groups', 'topological-groups']"
40,All distances are rational prove the set is countable,All distances are rational prove the set is countable,,"Question: Assume $E$ is a subset of $R^n$ , and $x,y\in E\Rightarrow d(x,y)\in \mathbb Q$ , show that $E$ is at most countable. ( $d$ is the Euclidean distance) I believe I've solved the $n=2$ case, which was 2013 real analysis qualifying exam problem at UCI. For general $n$ it was stated on the exam that the statement is true, without asking a proof. I'm wondering if anyone can give a more general solution because my solution for $n=2$ is difficult to extend to any $n$ . My solution for $n=2$ : If $E=\emptyset$ we are done. If not, take $x\in E$ , by the assumption $E\subset \underset{q\geq0,q\in Q}{\cup}\partial B(x,q)$ where $\partial B(x,q)=\{y\in R^2\mid d(x,y)=\sqrt{x^2+y^2}=q\}$ . Because countable union of countable sets is countable, and $Q$ is countable, it's sufficient to prove $C_q=E\cap \partial B(x,q)$ is countable for each $q$ . For any given $q$ , if $C_q=\emptyset$ we are done. If not, let $p_1\in C_q$ , if $p_2\in C_q$ , we can define $\theta=\angle p_2xp_1$ , $d(p_1,p_2)=2q\sin{\frac{\theta}{2}}\in Q\Rightarrow \sin{\frac{\theta}{2}}\in Q$ . Therefore it's sufficient to prove at most countably many $\theta$ satisfies $\sin{\frac{\theta}{2}}\in Q$ . To prove the last statement, for any given $s\in Q$ The solution set of $\sin{\frac{\theta}{2}}= s$ is $\emptyset$ (when $|s|>1$ ) or $\{\theta\mid \theta=2\arcsin{s}+4n\pi\}\cup \{\theta\mid \theta=2\pi-2\arcsin{s}+4n\pi\}$ where $n\in Z$ . The solution set is clearly countable, because $Q$ is countable, the solution set of $\sin{\frac{\theta}{2}}\in Q$ is countable union of countable set, therefore countable.","Question: Assume is a subset of , and , show that is at most countable. ( is the Euclidean distance) I believe I've solved the case, which was 2013 real analysis qualifying exam problem at UCI. For general it was stated on the exam that the statement is true, without asking a proof. I'm wondering if anyone can give a more general solution because my solution for is difficult to extend to any . My solution for : If we are done. If not, take , by the assumption where . Because countable union of countable sets is countable, and is countable, it's sufficient to prove is countable for each . For any given , if we are done. If not, let , if , we can define , . Therefore it's sufficient to prove at most countably many satisfies . To prove the last statement, for any given The solution set of is (when ) or where . The solution set is clearly countable, because is countable, the solution set of is countable union of countable set, therefore countable.","E R^n x,y\in E\Rightarrow d(x,y)\in \mathbb Q E d n=2 n n=2 n n=2 E=\emptyset x\in E E\subset \underset{q\geq0,q\in Q}{\cup}\partial B(x,q) \partial B(x,q)=\{y\in R^2\mid d(x,y)=\sqrt{x^2+y^2}=q\} Q C_q=E\cap \partial B(x,q) q q C_q=\emptyset p_1\in C_q p_2\in C_q \theta=\angle p_2xp_1 d(p_1,p_2)=2q\sin{\frac{\theta}{2}}\in Q\Rightarrow \sin{\frac{\theta}{2}}\in Q \theta \sin{\frac{\theta}{2}}\in Q s\in Q \sin{\frac{\theta}{2}}= s \emptyset |s|>1 \{\theta\mid \theta=2\arcsin{s}+4n\pi\}\cup \{\theta\mid \theta=2\pi-2\arcsin{s}+4n\pi\} n\in Z Q \sin{\frac{\theta}{2}}\in Q",['real-analysis']
41,Smooth extension of a continuous function on the boundary of a domain,Smooth extension of a continuous function on the boundary of a domain,,"Let $\Omega$ be a open, bounded set in $\mathbb{R}^n$. Suppose $g$ is a continuous function defined on the boundary $\partial \Omega$. Then, is it possible to show that there exists a function $f$ defined (and continuous) on $\Omega \cup \partial \Omega$ such that $f$ is smooth in $\Omega$ and $f$ agrees on $g$ on $\partial \Omega$? Also, how much smoothness (e.g. $C^k$) can we get?","Let $\Omega$ be a open, bounded set in $\mathbb{R}^n$. Suppose $g$ is a continuous function defined on the boundary $\partial \Omega$. Then, is it possible to show that there exists a function $f$ defined (and continuous) on $\Omega \cup \partial \Omega$ such that $f$ is smooth in $\Omega$ and $f$ agrees on $g$ on $\partial \Omega$? Also, how much smoothness (e.g. $C^k$) can we get?",,['real-analysis']
42,"Why is the graph of $f(x) = 1/x$ when $x \in (0,\infty)$, $0$ else, closed?","Why is the graph of  when ,  else, closed?","f(x) = 1/x x \in (0,\infty) 0","Consider the function $$f(x) = \begin{cases}\frac{1}x&&\text{if }x\in(0,\infty)\\0&&\text{else}\end{cases}$$ I know that this is the typical example of a discontinuous function whose graph is closed, but I can't see why it is closed. I understand that I'm probably just being dumb here, but if $x_n\to0^+$, we have $f(x_n) \to \infty$, so the graph doesn't seem closed to me... I've also tried to reason this by proving that the complement of the graph is open, so that for any $(x,y) \not\in G(f)$, where $G(f)$ is the graph of $f$, we can find an open ball around $(x,y) \not\in G(f)$. The reasoning behind this approach makes sense to me, but I'm still really concerned about my conceptual misunderstanding behind the limit point definition proof.","Consider the function $$f(x) = \begin{cases}\frac{1}x&&\text{if }x\in(0,\infty)\\0&&\text{else}\end{cases}$$ I know that this is the typical example of a discontinuous function whose graph is closed, but I can't see why it is closed. I understand that I'm probably just being dumb here, but if $x_n\to0^+$, we have $f(x_n) \to \infty$, so the graph doesn't seem closed to me... I've also tried to reason this by proving that the complement of the graph is open, so that for any $(x,y) \not\in G(f)$, where $G(f)$ is the graph of $f$, we can find an open ball around $(x,y) \not\in G(f)$. The reasoning behind this approach makes sense to me, but I'm still really concerned about my conceptual misunderstanding behind the limit point definition proof.",,"['real-analysis', 'functional-analysis']"
43,"If two continuous maps of an interval commute, then they agree at some point","If two continuous maps of an interval commute, then they agree at some point",,"Let $f,g:[0,1] \rightarrow [0,1]$ be continuous functions such that $f\circ g =g\circ f$. Prove that there exists $x \in [0,1]$ such that $f(x)=g(x)$","Let $f,g:[0,1] \rightarrow [0,1]$ be continuous functions such that $f\circ g =g\circ f$. Prove that there exists $x \in [0,1]$ such that $f(x)=g(x)$",,"['real-analysis', 'fixed-point-theorems']"
44,Question 7.7 in measure theory on Radon measure from Folland's Real Analysis Second Edition,Question 7.7 in measure theory on Radon measure from Folland's Real Analysis Second Edition,,"Hello all I was presented with this question from Folland's real analysis second edition on Radon measures which I am stuck on and so would really appreciate the help on. I m a novice in Radon measures especially in the concepts and abstractions so any help would be appreciated. It is problem #7 on page 220 It reads as follows: Just in case, the definition of Radon Measure used in the book is: A Radon measure on X is a Borel measure that is finite on all compact sets, outer regular on all Borel sets, and inner regular on all open sets. I also know for a fact that Radon measures are also inner regular on all their sigma finite sets. I have tried to attack the problem many times but to no avail as for some reason I cannot seem to incorporate the given assumption that X is sigma finite. Also the assumption is X is locally compact Hausdorff space. I would really appreciate the help Thanks EDIT: I figured it is obviously finite on compact sets Possible direction I have: I have studied in the Folland book that all $ \sigma-finite $ Radon measures are regular therefore for all Borel sets A and E we have $ \mu_A(E)=\mu(E \cap A) $ is the supremum on measure of all compact subsets in $ E \cap A $. How do I move further? I need inner regularity on open sets E meaning the supremum on measures of compact subsets of E not $ A \cap E $ as I have.","Hello all I was presented with this question from Folland's real analysis second edition on Radon measures which I am stuck on and so would really appreciate the help on. I m a novice in Radon measures especially in the concepts and abstractions so any help would be appreciated. It is problem #7 on page 220 It reads as follows: Just in case, the definition of Radon Measure used in the book is: A Radon measure on X is a Borel measure that is finite on all compact sets, outer regular on all Borel sets, and inner regular on all open sets. I also know for a fact that Radon measures are also inner regular on all their sigma finite sets. I have tried to attack the problem many times but to no avail as for some reason I cannot seem to incorporate the given assumption that X is sigma finite. Also the assumption is X is locally compact Hausdorff space. I would really appreciate the help Thanks EDIT: I figured it is obviously finite on compact sets Possible direction I have: I have studied in the Folland book that all $ \sigma-finite $ Radon measures are regular therefore for all Borel sets A and E we have $ \mu_A(E)=\mu(E \cap A) $ is the supremum on measure of all compact subsets in $ E \cap A $. How do I move further? I need inner regularity on open sets E meaning the supremum on measures of compact subsets of E not $ A \cap E $ as I have.",,"['real-analysis', 'general-topology', 'measure-theory']"
45,"""Inverse"" of nondecreasing, right-continuous function?","""Inverse"" of nondecreasing, right-continuous function?",,"Suppose $F : \mathbb{R} \to \mathbb{R}$ is a nondecreasing and right-continuous function. Define $G : [\inf F,\sup F] \to \overline{\mathbb{R}}$ by $G(p)=\inf \{ x : F(x) \geq p \}$, with the convention $\inf \emptyset = +\infty$. If $F$ is invertible then this is the inverse of $F$, but it makes sense even if $F$ is neither surjective nor injective. Is there a standard term for this $G$? An application of this idea is to sampling random variables. Specifically, given $X$ distributed as $U(0,1)$ and a CDF $F$, $G(X)$ has the CDF $F$. If there is a standard term for $G$ only in this context, I would be happy to hear that as well.","Suppose $F : \mathbb{R} \to \mathbb{R}$ is a nondecreasing and right-continuous function. Define $G : [\inf F,\sup F] \to \overline{\mathbb{R}}$ by $G(p)=\inf \{ x : F(x) \geq p \}$, with the convention $\inf \emptyset = +\infty$. If $F$ is invertible then this is the inverse of $F$, but it makes sense even if $F$ is neither surjective nor injective. Is there a standard term for this $G$? An application of this idea is to sampling random variables. Specifically, given $X$ distributed as $U(0,1)$ and a CDF $F$, $G(X)$ has the CDF $F$. If there is a standard term for $G$ only in this context, I would be happy to hear that as well.",,"['real-analysis', 'probability', 'terminology']"
46,$\int_a^b f(x) g'(x) dx = 0$ implies $f$ is constant,implies  is constant,\int_a^b f(x) g'(x) dx = 0 f,"Given $f$ is continuous on $[a,b]$, $\forall g$ which is a continuously differentiable function on $[a,b]$, with $g(a)=g(b)=0$, the following equation is satisfied: $\int_a^b f(x) g'(x) dx = 0$. I want to show that $f$ is a constant. This is a question similar to this , but that question ask $\int_a^b f(x) g(x) dx = 0$. I have tried taking $g = (x-a)(b-x)$, but since I don't know whether $f(x)$ is differentiable, I cannot take $g = f(x-a)(b-x)$ as in that question. Thank you.","Given $f$ is continuous on $[a,b]$, $\forall g$ which is a continuously differentiable function on $[a,b]$, with $g(a)=g(b)=0$, the following equation is satisfied: $\int_a^b f(x) g'(x) dx = 0$. I want to show that $f$ is a constant. This is a question similar to this , but that question ask $\int_a^b f(x) g(x) dx = 0$. I have tried taking $g = (x-a)(b-x)$, but since I don't know whether $f(x)$ is differentiable, I cannot take $g = f(x-a)(b-x)$ as in that question. Thank you.",,"['calculus', 'real-analysis']"
47,Where is Cauchy's wrong proof?,Where is Cauchy's wrong proof?,,"Allegedly, Cauchy mistakingly ""proved"" that pointwise convergence of continuous functions is continuous. I saw this somewhere in a book, and it is also in wikipedia: Uniform convergence. In his Cours d'analyse of 1821, Cauchy ""proved"" that if a sum of continuous functions converges pointwise, then its limit is also continuous. However, Abel observed three years later that this is not the case. For the conclusion to hold, ""pointwise convergence"" must be replaced with ""uniform convergence"". 1 There are many counterexamples. For example, a Fourier series of sine and cosine functions, all continuous, may converge to a discontinuous function such as a step function. Well, I searched in the book mentioned (or rather, this one ) and didn't find it. Where is it?","Allegedly, Cauchy mistakingly ""proved"" that pointwise convergence of continuous functions is continuous. I saw this somewhere in a book, and it is also in wikipedia: Uniform convergence. In his Cours d'analyse of 1821, Cauchy ""proved"" that if a sum of continuous functions converges pointwise, then its limit is also continuous. However, Abel observed three years later that this is not the case. For the conclusion to hold, ""pointwise convergence"" must be replaced with ""uniform convergence"". 1 There are many counterexamples. For example, a Fourier series of sine and cosine functions, all continuous, may converge to a discontinuous function such as a step function. Well, I searched in the book mentioned (or rather, this one ) and didn't find it. Where is it?",,"['real-analysis', 'reference-request', 'soft-question']"
48,What does it mean by a density argument?,What does it mean by a density argument?,,"When I read some books, it states ""a density argument"", what does it mean? When applying to symmetric function, what does it mean?","When I read some books, it states ""a density argument"", what does it mean? When applying to symmetric function, what does it mean?",,"['real-analysis', 'soft-question']"
49,Proving an Ultrametric Space and Its Properties,Proving an Ultrametric Space and Its Properties,,"Let $X$ be the collection of all sequences of positive integers. If $x=(n_j)_{j=1}^\infty$ and $y=(m_j)_{j=1}^\infty$ are two elements of $X$, set $$k(x,y)=\inf\{j:n_j\neq m_j\}$$ and $$d(x,y)= \begin{cases} 0 & \text{if $x=y$} \\ \frac{1}{k(x,y)} & \text{if $x \neq y$} \end{cases}$$ I have already shown that $d$ is a metric on $X$. Now, I must prove the following: (a) $d(x,z) \leq \max\{d(x,y),d(y,z)\}$ (b) Any open ball $D(x,r)$ is a closed subset of $X$. (c) If $y \in D(x,r)$, then $D(x,r)=D(y,r)$. (d) If $D(x,r_1) \cap D(y,r_2) \neq \emptyset$, then either $D(x,r_1) \subset D(y,r_2)$ or $D(y,r_2) \subset D(x,r_1)$. Edit: I have now posted an answer to my own question. Any helpful comments or corrections are welcome.","Let $X$ be the collection of all sequences of positive integers. If $x=(n_j)_{j=1}^\infty$ and $y=(m_j)_{j=1}^\infty$ are two elements of $X$, set $$k(x,y)=\inf\{j:n_j\neq m_j\}$$ and $$d(x,y)= \begin{cases} 0 & \text{if $x=y$} \\ \frac{1}{k(x,y)} & \text{if $x \neq y$} \end{cases}$$ I have already shown that $d$ is a metric on $X$. Now, I must prove the following: (a) $d(x,z) \leq \max\{d(x,y),d(y,z)\}$ (b) Any open ball $D(x,r)$ is a closed subset of $X$. (c) If $y \in D(x,r)$, then $D(x,r)=D(y,r)$. (d) If $D(x,r_1) \cap D(y,r_2) \neq \emptyset$, then either $D(x,r_1) \subset D(y,r_2)$ or $D(y,r_2) \subset D(x,r_1)$. Edit: I have now posted an answer to my own question. Any helpful comments or corrections are welcome.",,['real-analysis']
50,"How to prove if $u\in W^{1,p}$, then $|u|\in W^{1,p}$?","How to prove if , then ?","u\in W^{1,p} |u|\in W^{1,p}","How to prove if $u\in W^{1,p}$, then $|u|\in W^{1,p}$? Since $|u|\in L_p$, I only need to show weak derivative of $|u|$ exists and $D|u| \in L_p$. Can anyone give me some hint? Thanks!","How to prove if $u\in W^{1,p}$, then $|u|\in W^{1,p}$? Since $|u|\in L_p$, I only need to show weak derivative of $|u|$ exists and $D|u| \in L_p$. Can anyone give me some hint? Thanks!",,"['real-analysis', 'weak-derivatives']"
51,Convergence of a sequence with assumption that exponential subsequences converge?,Convergence of a sequence with assumption that exponential subsequences converge?,,"Problem One of my best friends asked me to think about the following problem: Suppose a sequence $\{a_n\}_{n=1}^\infty$ satisfies $\lim_{n\to\infty}a_{\lfloor\alpha^n\rfloor}=0$ for each $\alpha>1$. Is it true that $\lim_{n\to\infty}a_n=0$? He told me that the preceding proposition (if true) implies Kolmogorov's strong law of large numbers in probability theory. I don't know how, but it's irrelevant. Thoughts Suppose $E\subseteq(1,+\infty)$ is countable (for example, $E=(1,+\infty)\cap\mathbb Q$) and $\lim_{n\to\infty}a_{\lfloor\alpha^n\rfloor}=0$ for each $\alpha\in E$, we cannot conclude that $\lim_{n\to\infty}a_n=0$. In fact, we can choose an infinite set $S\subseteq\mathbb Z_{>0}$ such that $S\cap\{\lfloor\alpha^n\rfloor\colon n\in\mathbb Z_{>0}\}$ is finite for each $\alpha\in E$ as follows: Suppose $E=\{\alpha_1,\alpha_2,\dotsc\}$. We choose $S$ inductively. Suppose $T_0=\mathbb Z_{>0}$. Given $T_{n-1}$, we set $s_n=\min T_{n-1}$ and $T_n=(T_{n-1}\setminus\{\lfloor\alpha_n^k\rfloor\colon k\in\mathbb Z_{>0}\})\setminus\{s_n\}$. By a density argument, it's easy to see that $T_n$ are infinite therefore the process doesn't terminate. Let $S=\{s_n\colon n\in\mathbb Z_{>0}\}$. It's apparent that $\#(S\cap\{\lfloor\alpha_n^m\rfloor\colon m\in\mathbb Z_{>0}\})\le n$. Given $S$, we set $a_n=1/n$ if $n\not\in S$, and $a_n=1$ if $n\in S$, then $\lim_{n\to\infty}a_{\lfloor\alpha^n\rfloor}=0$ for each $\alpha\in E$, but $\lim_{n\to\infty}a_n$ doesn't exist. Here we choose $S$ by a diagonal process, therefore we cannot mimic the construction when $E$ is uncountable. In fact, the falsehood of the original statement is equivalent to the existence of $S$, therefore it's essential combinatorial, or related to some topological structure of $\mathbb Z_{>0}$ (say, compactness or Baire category, etc). I have no idea on the general case. Any idea? Thanks!","Problem One of my best friends asked me to think about the following problem: Suppose a sequence $\{a_n\}_{n=1}^\infty$ satisfies $\lim_{n\to\infty}a_{\lfloor\alpha^n\rfloor}=0$ for each $\alpha>1$. Is it true that $\lim_{n\to\infty}a_n=0$? He told me that the preceding proposition (if true) implies Kolmogorov's strong law of large numbers in probability theory. I don't know how, but it's irrelevant. Thoughts Suppose $E\subseteq(1,+\infty)$ is countable (for example, $E=(1,+\infty)\cap\mathbb Q$) and $\lim_{n\to\infty}a_{\lfloor\alpha^n\rfloor}=0$ for each $\alpha\in E$, we cannot conclude that $\lim_{n\to\infty}a_n=0$. In fact, we can choose an infinite set $S\subseteq\mathbb Z_{>0}$ such that $S\cap\{\lfloor\alpha^n\rfloor\colon n\in\mathbb Z_{>0}\}$ is finite for each $\alpha\in E$ as follows: Suppose $E=\{\alpha_1,\alpha_2,\dotsc\}$. We choose $S$ inductively. Suppose $T_0=\mathbb Z_{>0}$. Given $T_{n-1}$, we set $s_n=\min T_{n-1}$ and $T_n=(T_{n-1}\setminus\{\lfloor\alpha_n^k\rfloor\colon k\in\mathbb Z_{>0}\})\setminus\{s_n\}$. By a density argument, it's easy to see that $T_n$ are infinite therefore the process doesn't terminate. Let $S=\{s_n\colon n\in\mathbb Z_{>0}\}$. It's apparent that $\#(S\cap\{\lfloor\alpha_n^m\rfloor\colon m\in\mathbb Z_{>0}\})\le n$. Given $S$, we set $a_n=1/n$ if $n\not\in S$, and $a_n=1$ if $n\in S$, then $\lim_{n\to\infty}a_{\lfloor\alpha^n\rfloor}=0$ for each $\alpha\in E$, but $\lim_{n\to\infty}a_n$ doesn't exist. Here we choose $S$ by a diagonal process, therefore we cannot mimic the construction when $E$ is uncountable. In fact, the falsehood of the original statement is equivalent to the existence of $S$, therefore it's essential combinatorial, or related to some topological structure of $\mathbb Z_{>0}$ (say, compactness or Baire category, etc). I have no idea on the general case. Any idea? Thanks!",,"['real-analysis', 'sequences-and-series', 'general-topology', 'combinatorics']"
52,Is $f(x)x$ convex for increasing function $f$?,Is  convex for increasing function ?,f(x)x f,"Suppose that $f:(0,\infty)\to[0,1]$ is strictly increasing and infinitely differentiable. I have an intuition that  $$ g(x)=f(x)x $$  should be convex in $x$ (i.e. increasing at accelerating rate) because $g(x)$ can be interpreted as a proportion of a number both of which go up when the number goes up, but I can't prove it nor come up with a counterexample. I tried differentiating: $$ g'(x)=xf'(x)+f(x)\implies g''(x)=xf''(x)+2\underbrace{f'(x)}_{>0}\overset{?}{>}0. $$ Thank you for your help.","Suppose that $f:(0,\infty)\to[0,1]$ is strictly increasing and infinitely differentiable. I have an intuition that  $$ g(x)=f(x)x $$  should be convex in $x$ (i.e. increasing at accelerating rate) because $g(x)$ can be interpreted as a proportion of a number both of which go up when the number goes up, but I can't prove it nor come up with a counterexample. I tried differentiating: $$ g'(x)=xf'(x)+f(x)\implies g''(x)=xf''(x)+2\underbrace{f'(x)}_{>0}\overset{?}{>}0. $$ Thank you for your help.",,"['calculus', 'real-analysis', 'algebra-precalculus', 'convex-analysis']"
53,Generalised derivative of Cantor staircase,Generalised derivative of Cantor staircase,,"If we consider the Cantor staircase function , let us say $f:[0,1]\to\mathbb{R}$, as a distribution, I was wondering whether there is an explicit way to express its generalised derivative as a distribution, which is defined by$$\varphi\mapsto-\int_{-\infty}^{+\infty}f(x)\varphi'(x)dx$$where $\varphi$ is a fundamental function, i.e. an infinitely differentiable function equal to 0 outside a finite interval. I am not sure whether there is a ""more direct"" way to express it and I have not been able to find one -I already find it quite difficult to handle the definition of $f$, which I find quite clear here -, but I think it is an interesting issue. Thank you very much for any contribution!","If we consider the Cantor staircase function , let us say $f:[0,1]\to\mathbb{R}$, as a distribution, I was wondering whether there is an explicit way to express its generalised derivative as a distribution, which is defined by$$\varphi\mapsto-\int_{-\infty}^{+\infty}f(x)\varphi'(x)dx$$where $\varphi$ is a fundamental function, i.e. an infinitely differentiable function equal to 0 outside a finite interval. I am not sure whether there is a ""more direct"" way to express it and I have not been able to find one -I already find it quite difficult to handle the definition of $f$, which I find quite clear here -, but I think it is an interesting issue. Thank you very much for any contribution!",,"['real-analysis', 'functional-analysis', 'distribution-theory']"
54,Product rule for Hessian matrix,Product rule for Hessian matrix,,"Let $f: \mathbb{R}^n \to \mathbb{R}$ and $g: \mathbb{R}^n \to \mathbb{R}$. Is there a general formula for the Hessian matrix of their product? That is, what is $H(f(x) g(x))$, where $H(f(x)) = \left(\frac{\partial^2 f}{\partial x_i \partial x_j}\right)_{i,j = 1 \dots n}$?","Let $f: \mathbb{R}^n \to \mathbb{R}$ and $g: \mathbb{R}^n \to \mathbb{R}$. Is there a general formula for the Hessian matrix of their product? That is, what is $H(f(x) g(x))$, where $H(f(x)) = \left(\frac{\partial^2 f}{\partial x_i \partial x_j}\right)_{i,j = 1 \dots n}$?",,"['real-analysis', 'analysis', 'derivatives', 'partial-derivative', 'products']"
55,$\mathbb{Q}^2$ is dense in $\mathbb{R}^2$,is dense in,\mathbb{Q}^2 \mathbb{R}^2,I know that $\mathbb{Q}$ is dense in $\mathbb{R}$. What is the next step to prove that $\mathbb{Q}^2$ is also dense in $\mathbb{R}^2$? Any hints are welcomed.,I know that $\mathbb{Q}$ is dense in $\mathbb{R}$. What is the next step to prove that $\mathbb{Q}^2$ is also dense in $\mathbb{R}^2$? Any hints are welcomed.,,['real-analysis']
56,Supremum of sum of two sequences: $\sup (x_n+y_n) \le \sup x_n + \sup y_n$ [closed],Supremum of sum of two sequences:  [closed],\sup (x_n+y_n) \le \sup x_n + \sup y_n,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Prove that $\sup\{x_n+y_n\}\leq \sup\{x_n\}+\sup\{y_n\}$, if both sups are finite. Furthermore, prove that $\limsup\{x_n+y_n\}\leq \limsup\{x_n\}+\limsup\{y_n\}$ if both limsups are finite.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Prove that $\sup\{x_n+y_n\}\leq \sup\{x_n\}+\sup\{y_n\}$, if both sups are finite. Furthermore, prove that $\limsup\{x_n+y_n\}\leq \limsup\{x_n\}+\limsup\{y_n\}$ if both limsups are finite.",,"['real-analysis', 'inequality', 'supremum-and-infimum']"
57,"A subset of $[0,1]\times[0,1]$ containing at most one point from each horizontal and vertical section whose boundary is $[0,1]\times[0,1]$ [duplicate]",A subset of  containing at most one point from each horizontal and vertical section whose boundary is  [duplicate],"[0,1]\times[0,1] [0,1]\times[0,1]","This question already has answers here : Boundary Question in $\mathbb{R}^{2}$ (Manifolds) (2 answers) Closed 4 years ago . How can one build a subset $A\subset [0,1]\times[0,1]$ containing at the most one point from each horizontal and each vertical section and whose boundary (frontier) is $[0,1]\times[0,1]$? I don't know how to build this set. I know that if $A$ contains points in each quarter of the square it's enough.","This question already has answers here : Boundary Question in $\mathbb{R}^{2}$ (Manifolds) (2 answers) Closed 4 years ago . How can one build a subset $A\subset [0,1]\times[0,1]$ containing at the most one point from each horizontal and each vertical section and whose boundary (frontier) is $[0,1]\times[0,1]$? I don't know how to build this set. I know that if $A$ contains points in each quarter of the square it's enough.",,"['real-analysis', 'general-topology']"
58,$f:\mathbb{R}\to \mathbb{R}$ continuous and $\lim_{h \to 0^{+}} \frac{f(x+2h)-f(x+h)}{h}=0$ $\implies f=$ constant.,continuous and   constant.,f:\mathbb{R}\to \mathbb{R} \lim_{h \to 0^{+}} \frac{f(x+2h)-f(x+h)}{h}=0 \implies f=,Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function with the property that $$\lim_{h \to 0^{+}} \dfrac{f(x+2h)-f(x+h)}{h}=0$$ for all $x \in \mathbb{R}$. Prove that $f$ is constant.,Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function with the property that $$\lim_{h \to 0^{+}} \dfrac{f(x+2h)-f(x+h)}{h}=0$$ for all $x \in \mathbb{R}$. Prove that $f$ is constant.,,"['real-analysis', 'analysis', 'limits', 'continuity', 'contest-math']"
59,convergente series $\sum_{n=1}^{+\infty}a_n$ such that all the power diverges,convergente series  such that all the power diverges,\sum_{n=1}^{+\infty}a_n,"This question arose from here , Can we find a convergente series $\sum_{n=1}^{+\infty}a_n$ such that all the series $\sum_{n=1}^{+\infty}a_n^k$ for $k\ge2$ diverges ?","This question arose from here , Can we find a convergente series $\sum_{n=1}^{+\infty}a_n$ such that all the series $\sum_{n=1}^{+\infty}a_n^k$ for $k\ge2$ diverges ?",,['real-analysis']
60,Weak topology is not metrizable: what's wrong with this proof?,Weak topology is not metrizable: what's wrong with this proof?,,"Let $(X,\|\cdot\|)$ be an infinite-dimensional normed vector space. Suppose that the weak topology of $X$ is metrizable by a metric $d$.  Denote by $B^d(x,r)$ the open balls with respect to $d$; they are therefore weakly open. We have that for every $n$ the ball $B^d(0,\frac{1}{n})$ contains a non-trivial subspace. We could then argue as follows: Choose in each $B^d(0,\frac{1}{n})$ an $x_n$ such that $\|x_n\|=n$. We have $x_n\rightharpoonup x$  but $\|x_n\|\to \infty$. Which is an absurd, because we know that the sequence $(\|x_n\|)_{n=1}^\infty$ must be bounded. My question lies in the fact that in the Brezis book (Exercise 3.8) there is a proof which uses Baire's theorem! I don't know why they use such a complicated demonstration when there is so much simpler proof, if my proof is correct of course.","Let $(X,\|\cdot\|)$ be an infinite-dimensional normed vector space. Suppose that the weak topology of $X$ is metrizable by a metric $d$.  Denote by $B^d(x,r)$ the open balls with respect to $d$; they are therefore weakly open. We have that for every $n$ the ball $B^d(0,\frac{1}{n})$ contains a non-trivial subspace. We could then argue as follows: Choose in each $B^d(0,\frac{1}{n})$ an $x_n$ such that $\|x_n\|=n$. We have $x_n\rightharpoonup x$  but $\|x_n\|\to \infty$. Which is an absurd, because we know that the sequence $(\|x_n\|)_{n=1}^\infty$ must be bounded. My question lies in the fact that in the Brezis book (Exercise 3.8) there is a proof which uses Baire's theorem! I don't know why they use such a complicated demonstration when there is so much simpler proof, if my proof is correct of course.",,"['real-analysis', 'general-topology', 'functional-analysis', 'proof-verification', 'weak-convergence']"
61,Reduction to standard form.,Reduction to standard form.,,"I was wondering whether this ODE has been studied yet or whether there is anything we can say about its solutions? $$(1-t^2)u_{tt}-tu_t+4\left[n\beta (2t^2-1)+  \beta^2 (2t^2-1)^2+C\right]u=0$$ $C$ is a free parameter. So if you know a function that would fulfill this equation only for particular $C$, this would be perfectly fine. I am interested in its solutions on $[-1,1]$. I should include the motivation/reference here: The equation is motivated by Physics (Quantum Mechanics) and you might want to see this great answer that gives us (meanwhile complete hints) about the structure of the solution (due to O.L. (thank you!)) The problem is that the approach taken by O.L. does not offer an analytical representation of the solution. He was able to show that some types of polynomials will give you the solution, but still you have to throw this ' guess' into the equation. I suspect that the solutions form a nice orthogonal basis of $L^2[-1,1]$, but was incapable of constructing them by recursion or explicit representation. Now, I also got the hint to consider symmetries in my ODE and separately consider even and odd solutions so that I could reduce my ODE to a confluent Heun's equation .  If I follow this hint and substitute $s=t^2$ and $u(t)= v(s)$ I get $$v''(s)+\frac{1}{2}\left(\frac{1}{s}+ \frac{1}{s-1}\right) v'(s) + \left(n \beta \frac{(2s-1)}{s(1-s)} + \frac{\beta^2(2s-1)^2}{s(1-s)} + C\right) v(s)=0.$$ This is very close to the confluent Heun's equation but not exactly the form that we are looking for (since $s^2$ is appearing in the nominator of the right term in the parenthesis. So probably we need to substitute even more(anything like $v(s) = exp(\alpha s)w(s)$ might help), but actually I don't see how to go further. So is there anybody who knows how to finish this and who is able to construct the solutions from here?(and how $C$ has to be chosen in order to find a solution) If anything is unclear, please let me know and just to point this out: If you are able to reduce this ODE to the confluent Heun equation, then this answers my question totally.","I was wondering whether this ODE has been studied yet or whether there is anything we can say about its solutions? $$(1-t^2)u_{tt}-tu_t+4\left[n\beta (2t^2-1)+  \beta^2 (2t^2-1)^2+C\right]u=0$$ $C$ is a free parameter. So if you know a function that would fulfill this equation only for particular $C$, this would be perfectly fine. I am interested in its solutions on $[-1,1]$. I should include the motivation/reference here: The equation is motivated by Physics (Quantum Mechanics) and you might want to see this great answer that gives us (meanwhile complete hints) about the structure of the solution (due to O.L. (thank you!)) The problem is that the approach taken by O.L. does not offer an analytical representation of the solution. He was able to show that some types of polynomials will give you the solution, but still you have to throw this ' guess' into the equation. I suspect that the solutions form a nice orthogonal basis of $L^2[-1,1]$, but was incapable of constructing them by recursion or explicit representation. Now, I also got the hint to consider symmetries in my ODE and separately consider even and odd solutions so that I could reduce my ODE to a confluent Heun's equation .  If I follow this hint and substitute $s=t^2$ and $u(t)= v(s)$ I get $$v''(s)+\frac{1}{2}\left(\frac{1}{s}+ \frac{1}{s-1}\right) v'(s) + \left(n \beta \frac{(2s-1)}{s(1-s)} + \frac{\beta^2(2s-1)^2}{s(1-s)} + C\right) v(s)=0.$$ This is very close to the confluent Heun's equation but not exactly the form that we are looking for (since $s^2$ is appearing in the nominator of the right term in the parenthesis. So probably we need to substitute even more(anything like $v(s) = exp(\alpha s)w(s)$ might help), but actually I don't see how to go further. So is there anybody who knows how to finish this and who is able to construct the solutions from here?(and how $C$ has to be chosen in order to find a solution) If anything is unclear, please let me know and just to point this out: If you are able to reduce this ODE to the confluent Heun equation, then this answers my question totally.",,"['calculus', 'real-analysis']"
62,Lebesgue integrability and measurable functions,Lebesgue integrability and measurable functions,,"Let $f$ be a nonnegative function on the reals. What does the (Lebesgue) measurability of $f$ have to do with the (Lebesgue) integrability of $\int f$? I've spent some time studying the definition at Wikipedia , and I don't see how measurability enters the equation (other than the fact that it is explicitly mentioned at the start). If $f$ is such that every simple function less than $f$ has integral less than some $x\in\Bbb R$, then $\int f\in\Bbb R$ is defined, regardless of the measurability of $f$. What ""essential properties"" are lost without the assumption of $f$ being measurable? Full discosure: I've asked this question before , but I put too much tangentially-related text on the page, and so probably lost some viewers.","Let $f$ be a nonnegative function on the reals. What does the (Lebesgue) measurability of $f$ have to do with the (Lebesgue) integrability of $\int f$? I've spent some time studying the definition at Wikipedia , and I don't see how measurability enters the equation (other than the fact that it is explicitly mentioned at the start). If $f$ is such that every simple function less than $f$ has integral less than some $x\in\Bbb R$, then $\int f\in\Bbb R$ is defined, regardless of the measurability of $f$. What ""essential properties"" are lost without the assumption of $f$ being measurable? Full discosure: I've asked this question before , but I put too much tangentially-related text on the page, and so probably lost some viewers.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
63,Proving Nonhomogeneous ODE is Bounded,Proving Nonhomogeneous ODE is Bounded,,"I am trying to prove the following: Let $x(t)$ be a solution of the IVP  $$ \dot x=A(t)x+h(t), $$  where $A(t), h(t)$ continuous on $1\le t<\infty$. Further assume that $$ \int_1^\infty \| A(t)\|\,dt < \infty\quad \text{and}\quad \int_1^\infty \|h(t)\|\,dt < \infty. $$ Prove that $x(t)$ is bounded for $t\ge1$. I have some work, but I am doubtful on its legitmacy.  I wrote the solution $x(t)$ in integral form so: $$ x(t)=\frac1\mu\left(C+\int_\tau^t \mu(s)h(s)\,ds\,\right). $$ Then using the intial condition that $x(\tau)=\xi$ we have $C=\xi\mu(\tau)$, so the previous formula is now: $$ x(t,\tau,\xi)=\exp\Big(\int_\tau^t A(s)\,ds\Big)\xi +\int_\tau^t \exp\Big(\int_s^t A(w)\,dw\Big)\,h(s)\,ds. $$ Then clearly $\|x(t)\|$ is bounded only when it's integral equation is bounded which requires both $\int_1^\infty \|A(t)\|\,dt < \infty$ and $\int_1^\infty \|h(t)\|\,dt < \infty$. I am just not sure if this is the correct way to go about this proof. Any guidance would be greatly appreciated.","I am trying to prove the following: Let $x(t)$ be a solution of the IVP  $$ \dot x=A(t)x+h(t), $$  where $A(t), h(t)$ continuous on $1\le t<\infty$. Further assume that $$ \int_1^\infty \| A(t)\|\,dt < \infty\quad \text{and}\quad \int_1^\infty \|h(t)\|\,dt < \infty. $$ Prove that $x(t)$ is bounded for $t\ge1$. I have some work, but I am doubtful on its legitmacy.  I wrote the solution $x(t)$ in integral form so: $$ x(t)=\frac1\mu\left(C+\int_\tau^t \mu(s)h(s)\,ds\,\right). $$ Then using the intial condition that $x(\tau)=\xi$ we have $C=\xi\mu(\tau)$, so the previous formula is now: $$ x(t,\tau,\xi)=\exp\Big(\int_\tau^t A(s)\,ds\Big)\xi +\int_\tau^t \exp\Big(\int_s^t A(w)\,dw\Big)\,h(s)\,ds. $$ Then clearly $\|x(t)\|$ is bounded only when it's integral equation is bounded which requires both $\int_1^\infty \|A(t)\|\,dt < \infty$ and $\int_1^\infty \|h(t)\|\,dt < \infty$. I am just not sure if this is the correct way to go about this proof. Any guidance would be greatly appreciated.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'dynamical-systems', 'matrix-calculus']"
64,Analysis/Inequality question about proving an infinite product greater than 0,Analysis/Inequality question about proving an infinite product greater than 0,,"This is from David Williams' book Probability using Martingales. I'm self-studying. Question Prove that if $$0\leq p_n < 1 \quad\text{ and }\quad S:=\sum p_n < \infty$$ then $$\prod (1-p_n) > 0$$ Hint: First show that if $S<1$, then $\prod (1-p_n)\geq 1-S$. I was able to prove the hint using induction. Assume $\prod\limits_{n=1}^N (1-p_n) \geq 1-\sum\limits_{n=1}^N p_n$. Consider $\prod\limits_{n=1}^{N+1}(1-p_n) \geq (1-\sum\limits_{n=1}^N p_n)(1-p_{N+1})=1-\sum\limits_{n=1}^{N+1}p_n+p_{N+1}\sum\limits_{n=1}^{N}p_n \geq 1-\sum\limits_{n=1}^{N+1}p_n$. But I'm unable to use this to prove the general result for arbitrary $S$. Any guidance would be appreciated. I'm also surprised that he asks this question after stating the 2nd Borel Cantelli lemma, I don't see the connection.","This is from David Williams' book Probability using Martingales. I'm self-studying. Question Prove that if $$0\leq p_n < 1 \quad\text{ and }\quad S:=\sum p_n < \infty$$ then $$\prod (1-p_n) > 0$$ Hint: First show that if $S<1$, then $\prod (1-p_n)\geq 1-S$. I was able to prove the hint using induction. Assume $\prod\limits_{n=1}^N (1-p_n) \geq 1-\sum\limits_{n=1}^N p_n$. Consider $\prod\limits_{n=1}^{N+1}(1-p_n) \geq (1-\sum\limits_{n=1}^N p_n)(1-p_{N+1})=1-\sum\limits_{n=1}^{N+1}p_n+p_{N+1}\sum\limits_{n=1}^{N}p_n \geq 1-\sum\limits_{n=1}^{N+1}p_n$. But I'm unable to use this to prove the general result for arbitrary $S$. Any guidance would be appreciated. I'm also surprised that he asks this question after stating the 2nd Borel Cantelli lemma, I don't see the connection.",,"['real-analysis', 'measure-theory', 'inequality', 'self-learning', 'infinite-product']"
65,"Showing that $f:(0,2\pi]\to S^1$ is not a homeomorphism",Showing that  is not a homeomorphism,"f:(0,2\pi]\to S^1","I want to show that $f:(0,2\pi]\to S^1$ defined by $t\to (\sin t, \cos t)$ is not a homeomorphism. I will do this by showing that its inverse is not continuous. The inverse is defined by $(\sin t, \cos t)\to t$. So at $(0,1)$ we want to have $\lvert 2(1-\cos t)\rvert <\delta \Rightarrow \lvert t-2\pi\rvert<\epsilon$ for any $\epsilon$. If we take $\epsilon=\pi/2$, no matter how close $\cos t$ and $1$ are there will always be $t$'s close to $0$. So $f^{-1}$ is not continuous. What do you think? Are there any mistakes?","I want to show that $f:(0,2\pi]\to S^1$ defined by $t\to (\sin t, \cos t)$ is not a homeomorphism. I will do this by showing that its inverse is not continuous. The inverse is defined by $(\sin t, \cos t)\to t$. So at $(0,1)$ we want to have $\lvert 2(1-\cos t)\rvert <\delta \Rightarrow \lvert t-2\pi\rvert<\epsilon$ for any $\epsilon$. If we take $\epsilon=\pi/2$, no matter how close $\cos t$ and $1$ are there will always be $t$'s close to $0$. So $f^{-1}$ is not continuous. What do you think? Are there any mistakes?",,"['real-analysis', 'general-topology']"
66,Convergence of a sequence with $(x_n^2)$ is non-increasing.,Convergence of a sequence with  is non-increasing.,(x_n^2),"Let $(x_n)$ a real sequence such that $(x_n^2)$ is non-increasing and $(x_{n+1}-x_n)$ converges to $0$. Prove that $(x_n)$ converges. My attempt: $(x_n)^2$ is a decreasing sequence and $(x_n^2)\geq 0$ therefore the sequence converges to $l\geq 0$ If $l=0$ therefore $(x_n)$ converges to $0$. I haven't managed to do the case $l\neq 0$, I tried with $\epsilon$ but I didn't succeed. Thank you in advance for your help,","Let $(x_n)$ a real sequence such that $(x_n^2)$ is non-increasing and $(x_{n+1}-x_n)$ converges to $0$. Prove that $(x_n)$ converges. My attempt: $(x_n)^2$ is a decreasing sequence and $(x_n^2)\geq 0$ therefore the sequence converges to $l\geq 0$ If $l=0$ therefore $(x_n)$ converges to $0$. I haven't managed to do the case $l\neq 0$, I tried with $\epsilon$ but I didn't succeed. Thank you in advance for your help,",,['real-analysis']
67,"Let $f:[a,b]\rightarrow \mathbb{R} $ be differentiable with $f'(a) = f'(b)$. There exist a $c\in(a,b)$ such that $f'(c) = \frac{f(c) - f(a) }{c -a}$.",Let  be differentiable with . There exist a  such that .,"f:[a,b]\rightarrow \mathbb{R}  f'(a) = f'(b) c\in(a,b) f'(c) = \frac{f(c) - f(a) }{c -a}","Can someone help me with the following problem? Let $f:[a,b]\rightarrow \mathbb{R} $ be differentiable and suppose that $f'(a) = f'(b)$. Show that there exist a $c\in(a,b)$ such that $f'(c) = \frac{f(c) - f(a) }{c -a}$. My idea is to take first the case $f'(a) = f'(b) = 0$. In this case, consider  $$\phi(x) = \frac{f(x)-f(a)}{x-a}, \quad \forall x \in (a,b],$$ and $\phi(a) = 0$. Is obvious that $\phi$ is continuous in $(a,b]$. While in $a$, the continuity of $\phi(a)$ follows from the fact $$\lim_{x\to a^{+}} \phi(x) = f'(a) = 0 = \phi(a).$$ Therefore, $\phi$ is continuous in $[a,b]$. So, $\phi$ attains its maximum and its minimum in $[a,b]$. If one of them is in $(a,b)$, let $c$ be this point. So, since $\phi$ is differentiable in $(a,b)$, I know that we must have $\phi'(c) = 0$. But $$\phi'(x) = \frac{f'(x)}{x-a} - \frac{f(x)-f(a)}{(x-a)^2}.$$ So $$\phi'(c) = 0 \Rightarrow f'(c) = \frac{f(c) - f(a) }{c -a}.$$ The problem is that I can't ensure that a point of maximum or minimum must be in $(a,b)$. Note that, once I solve this case ($f'(a) = 0$), the general case is solved just considering $g(x) = f(x) - f'(a) x$. Because $g'(a) = g'(b) = 0$. And then if there exists a $c\in(a,b)$ with  $$ g'(c) = \frac{g(c)-g(a)}{c-a},$$ then $$f'(c) - f'(a) = \frac{f(c) - f'(a)c -f(a) + f'(a)a}{c-a} = \frac{f(c) -f(a)}{c-a} - f'(a).$$ Therefore, $$f'(c) = \frac{f(c) - f(a) }{c -a}.$$","Can someone help me with the following problem? Let $f:[a,b]\rightarrow \mathbb{R} $ be differentiable and suppose that $f'(a) = f'(b)$. Show that there exist a $c\in(a,b)$ such that $f'(c) = \frac{f(c) - f(a) }{c -a}$. My idea is to take first the case $f'(a) = f'(b) = 0$. In this case, consider  $$\phi(x) = \frac{f(x)-f(a)}{x-a}, \quad \forall x \in (a,b],$$ and $\phi(a) = 0$. Is obvious that $\phi$ is continuous in $(a,b]$. While in $a$, the continuity of $\phi(a)$ follows from the fact $$\lim_{x\to a^{+}} \phi(x) = f'(a) = 0 = \phi(a).$$ Therefore, $\phi$ is continuous in $[a,b]$. So, $\phi$ attains its maximum and its minimum in $[a,b]$. If one of them is in $(a,b)$, let $c$ be this point. So, since $\phi$ is differentiable in $(a,b)$, I know that we must have $\phi'(c) = 0$. But $$\phi'(x) = \frac{f'(x)}{x-a} - \frac{f(x)-f(a)}{(x-a)^2}.$$ So $$\phi'(c) = 0 \Rightarrow f'(c) = \frac{f(c) - f(a) }{c -a}.$$ The problem is that I can't ensure that a point of maximum or minimum must be in $(a,b)$. Note that, once I solve this case ($f'(a) = 0$), the general case is solved just considering $g(x) = f(x) - f'(a) x$. Because $g'(a) = g'(b) = 0$. And then if there exists a $c\in(a,b)$ with  $$ g'(c) = \frac{g(c)-g(a)}{c-a},$$ then $$f'(c) - f'(a) = \frac{f(c) - f'(a)c -f(a) + f'(a)a}{c-a} = \frac{f(c) -f(a)}{c-a} - f'(a).$$ Therefore, $$f'(c) = \frac{f(c) - f(a) }{c -a}.$$",,"['calculus', 'real-analysis']"
68,Necessity of a hypothesis in the fundamental theorem of calculus,Necessity of a hypothesis in the fundamental theorem of calculus,,"Baby Rudin's Fundamental Theorem of Calculus (Theorem 6.21), in my professor's words,states: Let $f: [a,b] \to \mathbb{R}$ be a Riemann integrable function. If $F: [a,b] \to \mathbb{R}$ is an antiderivative of $f$, then $\int_a^b \! f(x) \, \mathrm{d}x = F(b)-F(a)$. During the proof, one of my peers asked if the hypothesis that $f$ is Riemann integrable was needed since we have right after that the derivative of $F$ is little $f$. That is, does the second hypothesis imply the first? $F$ is differentiable, so it's continuous on $[a,b]$, and furthermore bounded. Does this then imply that $f$ is also continuous and bounded? If it does, that mean we can exclude the first hypothesis, or is necessary?","Baby Rudin's Fundamental Theorem of Calculus (Theorem 6.21), in my professor's words,states: Let $f: [a,b] \to \mathbb{R}$ be a Riemann integrable function. If $F: [a,b] \to \mathbb{R}$ is an antiderivative of $f$, then $\int_a^b \! f(x) \, \mathrm{d}x = F(b)-F(a)$. During the proof, one of my peers asked if the hypothesis that $f$ is Riemann integrable was needed since we have right after that the derivative of $F$ is little $f$. That is, does the second hypothesis imply the first? $F$ is differentiable, so it's continuous on $[a,b]$, and furthermore bounded. Does this then imply that $f$ is also continuous and bounded? If it does, that mean we can exclude the first hypothesis, or is necessary?",,['calculus']
69,Can You Construct a Syndetic Set with an Undefined Density?,Can You Construct a Syndetic Set with an Undefined Density?,,"Let $A \subset \mathbb{N}$. Enumerate $A = \{A_1, A_2,...\}$ such that $A_1 \le A_2 \le ...$. We say that $A$ is syndetic if there exists some $M \geq 0$ such that $A_{i+1} - A_i \le M$ for all $i =1,2,..$ (that is, ""the gaps of $A$ are uniformly bounded""). The natural density of $A$, if it exists, is defined to be $$d(A) = \lim_{N \to \infty} \frac{|A \cap \{1,2,..., N\}|}{N} = \lim_{N \to \infty} \frac{N}{A_N}.$$ It is possible that the limit does not exist. The examples of this phenomenon that I've seen all use the same idea. You need to have a set which first contains a lot of elements of $\{1,2,..N\}$, then misses a lot of $\{N, N+1, ..., N'\}$ then has a lot of $\{N' + 1, ..., N''\}$, etc. A common example is given by $$A = [2^3, 2^5] \cup [2^7, 2^9] \cup ... \cup [2^{4k-1}, 2^{4k+1}] \cup ...$$ Such examples cannot be syndetic. In the specific given example the problem is that the gaps $[2^{4k+1}, 2^{4k + 5}]$ are not bounded as $k \to \infty$. So my question is: how can one construct a syndetic set with no natural density (if possible)? Even better: can you construct $A \subset \mathbb{N}$ such that $A$ and $\mathbb{N} \setminus A$ are syndetic and such that $A$ has no density? Thank you very much in advance.","Let $A \subset \mathbb{N}$. Enumerate $A = \{A_1, A_2,...\}$ such that $A_1 \le A_2 \le ...$. We say that $A$ is syndetic if there exists some $M \geq 0$ such that $A_{i+1} - A_i \le M$ for all $i =1,2,..$ (that is, ""the gaps of $A$ are uniformly bounded""). The natural density of $A$, if it exists, is defined to be $$d(A) = \lim_{N \to \infty} \frac{|A \cap \{1,2,..., N\}|}{N} = \lim_{N \to \infty} \frac{N}{A_N}.$$ It is possible that the limit does not exist. The examples of this phenomenon that I've seen all use the same idea. You need to have a set which first contains a lot of elements of $\{1,2,..N\}$, then misses a lot of $\{N, N+1, ..., N'\}$ then has a lot of $\{N' + 1, ..., N''\}$, etc. A common example is given by $$A = [2^3, 2^5] \cup [2^7, 2^9] \cup ... \cup [2^{4k-1}, 2^{4k+1}] \cup ...$$ Such examples cannot be syndetic. In the specific given example the problem is that the gaps $[2^{4k+1}, 2^{4k + 5}]$ are not bounded as $k \to \infty$. So my question is: how can one construct a syndetic set with no natural density (if possible)? Even better: can you construct $A \subset \mathbb{N}$ such that $A$ and $\mathbb{N} \setminus A$ are syndetic and such that $A$ has no density? Thank you very much in advance.",,"['real-analysis', 'sequences-and-series']"
70,looking for a diffeomorphism (not C1),looking for a diffeomorphism (not C1),,"Let $f\colon\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ diffeomorphism with $f(B[0,1])\subset B[0,1]$ and $| \det f^{\prime}(x) |<1/2$ for all $x\in B[0,1]$ then for every continuous function $h\colon B[0,1] \rightarrow \mathbb{R}^{n}$ $$\lim\limits_{n \to \infty } \int_{f^n(B[0,1])}h(x)dx =0$$ The statement works when $ f $ is a diffeomorphism of class $C^1$. Wherefore seek some diffeomorphism (not C1) to serve as counterexample, but I can not think of any, exist? or question the way it is stated can be proved? thanks for the help","Let $f\colon\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ diffeomorphism with $f(B[0,1])\subset B[0,1]$ and $| \det f^{\prime}(x) |<1/2$ for all $x\in B[0,1]$ then for every continuous function $h\colon B[0,1] \rightarrow \mathbb{R}^{n}$ $$\lim\limits_{n \to \infty } \int_{f^n(B[0,1])}h(x)dx =0$$ The statement works when $ f $ is a diffeomorphism of class $C^1$. Wherefore seek some diffeomorphism (not C1) to serve as counterexample, but I can not think of any, exist? or question the way it is stated can be proved? thanks for the help",,"['real-analysis', 'analysis', 'multivariable-calculus', 'functions']"
71,An inequality involving multi-index,An inequality involving multi-index,,"I came across these inequalities while learning about Schwartz functions (Classical Fourier Analysis, Grafakos) and I have no idea how to prove this: For $x \in \mathbb{R}^{n}$ and $\alpha = (\alpha_{1}, \ldots, \alpha_{n}) \in \mathbb{N}^{n}$, we set $$ x^{\alpha} = x_{1}^{\alpha_{1}}\cdots x_{n}^{\alpha_{n}}.$$ Then prove that there exists a constant $c_{n,\alpha}$ such that $$\left| x^{\alpha}\right| \leq c_{n,\alpha}|x|^{|\alpha|}$$ where $|\alpha| = \alpha_{1} + \cdots + \alpha_{n}$. Conversely, for every $k \in \mathbb{N}$, there exists a $C_{n,k}$ such that $$|x|^{k} \leq C_{n,k}\sum\limits_{|\beta| = k}|x^{\beta}|$$ Any help would be appreciated.","I came across these inequalities while learning about Schwartz functions (Classical Fourier Analysis, Grafakos) and I have no idea how to prove this: For $x \in \mathbb{R}^{n}$ and $\alpha = (\alpha_{1}, \ldots, \alpha_{n}) \in \mathbb{N}^{n}$, we set $$ x^{\alpha} = x_{1}^{\alpha_{1}}\cdots x_{n}^{\alpha_{n}}.$$ Then prove that there exists a constant $c_{n,\alpha}$ such that $$\left| x^{\alpha}\right| \leq c_{n,\alpha}|x|^{|\alpha|}$$ where $|\alpha| = \alpha_{1} + \cdots + \alpha_{n}$. Conversely, for every $k \in \mathbb{N}$, there exists a $C_{n,k}$ such that $$|x|^{k} \leq C_{n,k}\sum\limits_{|\beta| = k}|x^{\beta}|$$ Any help would be appreciated.",,"['real-analysis', 'multivariable-calculus', 'inequality']"
72,Prove that $e^a e^b = e^{a+b}$,Prove that,e^a e^b = e^{a+b},"I've read the argument in Rudin, but I think I need a little clarification \begin{align} e^a e^b &= \sum_{k=0}^{\infty} \frac{a^k}{k!} \sum_{m=0}^{\infty} \frac{b^m}{m!}\\ &= \sum_{n=0}^{\infty} \frac{ n!}{n!}  \sum_{k=0}^{n} \frac{a^k}{k!} \frac{b^{n-k}}{(n-k)!} \\ &= \sum_{n=0}^{\infty} \frac{(a+b)^n}{n!}\\ &=e^{a+b}\\ \end{align} I'd like to understand how we get from the first line to the second. Of course $m =n-k$, but how do we ensure that we haven't missed any addends in the limit changing process? Changing limits: $m=0 \rightarrow n-k=0 \rightarrow k=n$ and $m=\infty \rightarrow n-k=\infty \rightarrow k=0$, assuming $n \to \infty$. \begin{align} \frac{b^m}{m!} \sum_{k=0}^{\infty} \frac{a^k}{k!}  &= \sum_{k=0}^{\infty} \frac{a^k}{k!} \frac{b^{m}}{m!} \\ &=\sum_{k=0}^{?} \frac{a^k}{k!} \frac{b^{n-k}}{(n-k)!} \\ \end{align} how to justify that ""?""$=n$ ?","I've read the argument in Rudin, but I think I need a little clarification \begin{align} e^a e^b &= \sum_{k=0}^{\infty} \frac{a^k}{k!} \sum_{m=0}^{\infty} \frac{b^m}{m!}\\ &= \sum_{n=0}^{\infty} \frac{ n!}{n!}  \sum_{k=0}^{n} \frac{a^k}{k!} \frac{b^{n-k}}{(n-k)!} \\ &= \sum_{n=0}^{\infty} \frac{(a+b)^n}{n!}\\ &=e^{a+b}\\ \end{align} I'd like to understand how we get from the first line to the second. Of course $m =n-k$, but how do we ensure that we haven't missed any addends in the limit changing process? Changing limits: $m=0 \rightarrow n-k=0 \rightarrow k=n$ and $m=\infty \rightarrow n-k=\infty \rightarrow k=0$, assuming $n \to \infty$. \begin{align} \frac{b^m}{m!} \sum_{k=0}^{\infty} \frac{a^k}{k!}  &= \sum_{k=0}^{\infty} \frac{a^k}{k!} \frac{b^{m}}{m!} \\ &=\sum_{k=0}^{?} \frac{a^k}{k!} \frac{b^{n-k}}{(n-k)!} \\ \end{align} how to justify that ""?""$=n$ ?",,"['real-analysis', 'power-series']"
73,"Does $f, f' \in L^1([0, \infty))$ imply that $\lim_{x \to \infty} xf(x) = 0$?",Does  imply that ?,"f, f' \in L^1([0, \infty)) \lim_{x \to \infty} xf(x) = 0","Does $\int_0^\infty |f(x)| \, dx$ and $\int_0^\infty |f'(x)| \, dx$ being finite imply that $\lim_{x \to \infty} xf(x) = 0$? (Context: I am working through an analytic number theory textbook. In a lemma, the book assumes the conditions on $f$ and $f'$. The term $xf(x)$ appears from an integration by parts, but is tossed out, which must mean that $\lim_{x\to\infty} xf(x) = 0$. However, I've tried to prove this and was not successful. I also cannot think of any counterexamples.)","Does $\int_0^\infty |f(x)| \, dx$ and $\int_0^\infty |f'(x)| \, dx$ being finite imply that $\lim_{x \to \infty} xf(x) = 0$? (Context: I am working through an analytic number theory textbook. In a lemma, the book assumes the conditions on $f$ and $f'$. The term $xf(x)$ appears from an integration by parts, but is tossed out, which must mean that $\lim_{x\to\infty} xf(x) = 0$. However, I've tried to prove this and was not successful. I also cannot think of any counterexamples.)",,"['real-analysis', 'analysis']"
74,Is the Minkowski sum of two Lebesgue measurable set measurable?,Is the Minkowski sum of two Lebesgue measurable set measurable?,,"Let $E,F \subseteq \mathbb R$ be Lebesgue measrable. Is the sum $E+F=\{x+y:x \in E,y \in F \}$ Lebesgue measurable as well?","Let $E,F \subseteq \mathbb R$ be Lebesgue measrable. Is the sum $E+F=\{x+y:x \in E,y \in F \}$ Lebesgue measurable as well?",,"['real-analysis', 'analysis', 'measure-theory']"
75,Extension of continuous function on a closed subset of $\mathbb{R}$,Extension of continuous function on a closed subset of,\mathbb{R},"Any help with this problem is appreciated. It comes up in the context of Lusin's theorem, where it was assumed to be true. Suppose $E$ is a closed subset of $\mathbb{R}$ and $f:E\rightarrow \mathbb{R}$ is continuous. There exists a continuous function $g: \mathbb{R} \rightarrow \mathbb{R}$ such that $f(x)=g(x)$ for $x \in E$ and $g$ satisfies $$\sup_{x \in \mathbb{R}} |g(x)| \leq \sup_{x \in E} |f(x)|\;.$$","Any help with this problem is appreciated. It comes up in the context of Lusin's theorem, where it was assumed to be true. Suppose $E$ is a closed subset of $\mathbb{R}$ and $f:E\rightarrow \mathbb{R}$ is continuous. There exists a continuous function $g: \mathbb{R} \rightarrow \mathbb{R}$ such that $f(x)=g(x)$ for $x \in E$ and $g$ satisfies $$\sup_{x \in \mathbb{R}} |g(x)| \leq \sup_{x \in E} |f(x)|\;.$$",,"['real-analysis', 'continuity']"
76,Delta in continuity,Delta in continuity,,"Let $f: [a,b]\to\mathbb{R}$ be continuous, prove that it is uniform continuous. I know using compactness it is almost one liner, but I want to prove it without using compactness. However, I can use the theorem that every continuous function achieves max and min on a closed bounded interval. I propose proving that some choices of $\delta$ can be continuous on $[a,b]$ , for example but not restricted to: For an arbitrary $\epsilon>0$ , for each $x\in[a,b]$ set $\Delta_x=\{0<\delta<b-a \;|\;|x-y|<\delta\Longrightarrow |f(x)-f(y)| <\epsilon\}$ , denote $\delta_x = \sup \Delta_x $ . Basically $\delta_x$ is the radius of largest neighborhood of $x$ that will be mapped into a subset of neighborhood radius epsilon of $f(x)$ . I'm trying to show that $\delta_x$ is continuous on $[a,b]$ with fixed $\epsilon$ . My progress is that I can show $\delta_y$ is bounded below if $y$ is close enough to $x$ , but failed to find its upper bound that is related to its distance with $x$ . Maybe either you could help me with this $\delta_x$ proof, or another cleaner proof without compactness (but allowed max and min). Thanks so much.","Let be continuous, prove that it is uniform continuous. I know using compactness it is almost one liner, but I want to prove it without using compactness. However, I can use the theorem that every continuous function achieves max and min on a closed bounded interval. I propose proving that some choices of can be continuous on , for example but not restricted to: For an arbitrary , for each set , denote . Basically is the radius of largest neighborhood of that will be mapped into a subset of neighborhood radius epsilon of . I'm trying to show that is continuous on with fixed . My progress is that I can show is bounded below if is close enough to , but failed to find its upper bound that is related to its distance with . Maybe either you could help me with this proof, or another cleaner proof without compactness (but allowed max and min). Thanks so much.","f: [a,b]\to\mathbb{R} \delta [a,b] \epsilon>0 x\in[a,b] \Delta_x=\{0<\delta<b-a \;|\;|x-y|<\delta\Longrightarrow |f(x)-f(y)| <\epsilon\} \delta_x = \sup \Delta_x  \delta_x x f(x) \delta_x [a,b] \epsilon \delta_y y x x \delta_x","['real-analysis', 'metric-spaces']"
77,Measurable Function - Past Exam question,Measurable Function - Past Exam question,,"This is an exercise from a past exam I'm using to try to help me study. Let $f$ be measurable and bounded on $[0,1]$ satisfying $$f(x+y)=f(x)+f(y);\quad f(1)=1.$$ I'm trying to show that $f(x)=x$. We're given a hint to show it is continuous by using the hypothesis in a ""mildly clever"" way, show that it is the identity on the rational points, then extend by continuity. I am able to show that the function is the identity on $[0,1]\cap\mathbb{Q}$ without any trouble, but I'm not sure how to show it's continuous. From there, showing the function is the identity would be easy since it follows from the fact that the rational numbers are dense in $\mathbb{R}$.","This is an exercise from a past exam I'm using to try to help me study. Let $f$ be measurable and bounded on $[0,1]$ satisfying $$f(x+y)=f(x)+f(y);\quad f(1)=1.$$ I'm trying to show that $f(x)=x$. We're given a hint to show it is continuous by using the hypothesis in a ""mildly clever"" way, show that it is the identity on the rational points, then extend by continuity. I am able to show that the function is the identity on $[0,1]\cap\mathbb{Q}$ without any trouble, but I'm not sure how to show it's continuous. From there, showing the function is the identity would be easy since it follows from the fact that the rational numbers are dense in $\mathbb{R}$.",,"['real-analysis', 'measure-theory']"
78,Simple question regarding continuous functions on $\mathbb{Q}$,Simple question regarding continuous functions on,\mathbb{Q},"I am a self-studying masochist and I came across an interesting example (to me), that I think will help me with further results. I have little experience writing rigorous proofs, so any explicit help is appreciated. What I am trying to prove is the following: Find a measurable indicator function that cannot be approximated by a sequence of continuous functions (demonstrate). For this I believe the ""answer"" is some form of $1_{\mathbb{Q}}$ on a finite interval. I believe this function is both measurable and not a pointwise limit of continuous functions. How can I show that if I have a continuous function arbitrarily close to 1 on the rationals, it cannot also be arbitrarily close to 0 on the irrationals? I do know that continuous functions are uniformly continuous on finite intervals, but I cant seem to put all the pieces together. Any detail would be greatly appreciated. Thanks!","I am a self-studying masochist and I came across an interesting example (to me), that I think will help me with further results. I have little experience writing rigorous proofs, so any explicit help is appreciated. What I am trying to prove is the following: Find a measurable indicator function that cannot be approximated by a sequence of continuous functions (demonstrate). For this I believe the ""answer"" is some form of $1_{\mathbb{Q}}$ on a finite interval. I believe this function is both measurable and not a pointwise limit of continuous functions. How can I show that if I have a continuous function arbitrarily close to 1 on the rationals, it cannot also be arbitrarily close to 0 on the irrationals? I do know that continuous functions are uniformly continuous on finite intervals, but I cant seem to put all the pieces together. Any detail would be greatly appreciated. Thanks!",,"['real-analysis', 'measure-theory']"
79,"""Converse"" to composition of measurable functions is measurable","""Converse"" to composition of measurable functions is measurable",,"Here is a restatement of a problem in a textbook I encountered. I'm well beyond the age of doing homework and this is purely for self-study. Exercise : Let $f : (X,\Sigma_1) \to (Y, \Sigma_2)$ and       $h:(X,\Sigma_1) \to (\mathbb R, \mathcal B)$ be       measurable maps where in the latter case $\mathcal B$ denotes the       Borel $\sigma$-algebra over $\mathbb R$. Let $\Sigma_f =     \sigma(f)$. Show that $h$ is $\Sigma_f$-measurable if and only if there exists $g : (Y,\Sigma_2) \to     (\mathbb R, \mathcal B)$ such that $h(x) = g(f(x))$ for all $x \in     X$. One direction of the proof is easy. Suppose such a $g$ exists. Then, for all $B \in \mathcal B$, $h^{-1}(B) = f^{-1}( g^{-1}(B) )$ and so $h^{-1} \in \Sigma_f$. There seem to be some holes in the opposite direction which I can't quite fill. For all $z \in \mathbb R$, I defined $$ A_z = \{x: h(x) = z\}.  $$ Then $A_z \in \Sigma_1$ since the singletons $\{z\}$ are Borel-measurable. Also, for $z \neq z'$, it is true that $A_z \cap A_{z'} = \emptyset$. Now, if $h$ is $\Sigma_f$-measurable, then $A_z = f^{-1}(B_z)$ for some $B_z \in \Sigma_2$. But then, for $z \neq z'$, we have that $B_z \cap B_{z'} = \emptyset$ as well, so the $\{B_z\}_{z \in \mathbb R}$ sets partitions $Y$ modulo the portion not in the image of $f$. Now, set $g(y) = z$ on $B_z$ and set $g(y) = 0$ on $y \in N_0 := Y \setminus \cup_{z \in \mathbb R} B_z$. It seems reasonable to claim that $N_0$ is a measurable set by considering that $N_0 = Y \setminus \cup_n C_n$ where $f^{-1}(C_n) = h^{-1}((-\infty,n))$ and $C_n \in \Sigma_2$ by assumption. But, this only seems to show that we can construct a well-defined $g$. It doesn't seem to prove that it is measurable ! To get measurability we need to show something in addition to this, like $\{y: g(y) \leq z\} \in \Sigma_2$ for all $z \in \mathbb R$. For $z < 0$ it seems we should be able to get a correspondence between $\{y: g(y) \leq z\}$ and $C_z$ where $C_z \in \Sigma_2$ satisfies $f^{-1}(C_z) = h^{-1}((-\infty,z))$. For $z \geq 0$, I think it would be something like $\{y : g(y) \leq z\} = C_z \cup N_0$, I think. I can't quite seem to make the argument go through. Questions: Is this on the right track? If so, how do we finish it off? (It seems a little ""too constructive"" for a typical measure-theoretic argument.) Is there some other more clever or direct argument? If so, what is it?","Here is a restatement of a problem in a textbook I encountered. I'm well beyond the age of doing homework and this is purely for self-study. Exercise : Let $f : (X,\Sigma_1) \to (Y, \Sigma_2)$ and       $h:(X,\Sigma_1) \to (\mathbb R, \mathcal B)$ be       measurable maps where in the latter case $\mathcal B$ denotes the       Borel $\sigma$-algebra over $\mathbb R$. Let $\Sigma_f =     \sigma(f)$. Show that $h$ is $\Sigma_f$-measurable if and only if there exists $g : (Y,\Sigma_2) \to     (\mathbb R, \mathcal B)$ such that $h(x) = g(f(x))$ for all $x \in     X$. One direction of the proof is easy. Suppose such a $g$ exists. Then, for all $B \in \mathcal B$, $h^{-1}(B) = f^{-1}( g^{-1}(B) )$ and so $h^{-1} \in \Sigma_f$. There seem to be some holes in the opposite direction which I can't quite fill. For all $z \in \mathbb R$, I defined $$ A_z = \{x: h(x) = z\}.  $$ Then $A_z \in \Sigma_1$ since the singletons $\{z\}$ are Borel-measurable. Also, for $z \neq z'$, it is true that $A_z \cap A_{z'} = \emptyset$. Now, if $h$ is $\Sigma_f$-measurable, then $A_z = f^{-1}(B_z)$ for some $B_z \in \Sigma_2$. But then, for $z \neq z'$, we have that $B_z \cap B_{z'} = \emptyset$ as well, so the $\{B_z\}_{z \in \mathbb R}$ sets partitions $Y$ modulo the portion not in the image of $f$. Now, set $g(y) = z$ on $B_z$ and set $g(y) = 0$ on $y \in N_0 := Y \setminus \cup_{z \in \mathbb R} B_z$. It seems reasonable to claim that $N_0$ is a measurable set by considering that $N_0 = Y \setminus \cup_n C_n$ where $f^{-1}(C_n) = h^{-1}((-\infty,n))$ and $C_n \in \Sigma_2$ by assumption. But, this only seems to show that we can construct a well-defined $g$. It doesn't seem to prove that it is measurable ! To get measurability we need to show something in addition to this, like $\{y: g(y) \leq z\} \in \Sigma_2$ for all $z \in \mathbb R$. For $z < 0$ it seems we should be able to get a correspondence between $\{y: g(y) \leq z\}$ and $C_z$ where $C_z \in \Sigma_2$ satisfies $f^{-1}(C_z) = h^{-1}((-\infty,z))$. For $z \geq 0$, I think it would be something like $\{y : g(y) \leq z\} = C_z \cup N_0$, I think. I can't quite seem to make the argument go through. Questions: Is this on the right track? If so, how do we finish it off? (It seems a little ""too constructive"" for a typical measure-theoretic argument.) Is there some other more clever or direct argument? If so, what is it?",,"['real-analysis', 'measure-theory', 'probability-theory', 'elementary-set-theory']"
80,Total Variation and Integral,Total Variation and Integral,,"Let $f$ be a function of bounded variation on $[a, b]$ and $T_{a}^{b}(f)$ its total variation. We do not assume that $f$ is continuous. Show that $$\int_{a}^{b}|f'(t)|\, dt \leq T_{a}^{b}(f).$$ I know that if we assume that $f$ is continuous, then the above equation is true because we have the ability to use the Mean Value Theorem. What can I do if we don't assume $f$ is continuous?","Let $f$ be a function of bounded variation on $[a, b]$ and $T_{a}^{b}(f)$ its total variation. We do not assume that $f$ is continuous. Show that $$\int_{a}^{b}|f'(t)|\, dt \leq T_{a}^{b}(f).$$ I know that if we assume that $f$ is continuous, then the above equation is true because we have the ability to use the Mean Value Theorem. What can I do if we don't assume $f$ is continuous?",,"['real-analysis', 'bounded-variation']"
81,limit of a sequence and principle of induction,limit of a sequence and principle of induction,,I try to evaluate this limit: $$\lim_{n\to+\infty} \frac{1\cdot3\cdot5\cdots(2n-1)}{2\cdot4\cdot6\cdots(2n)}$$ I considered this inequality $$\frac{1}{4n}\le\left [ \frac{1\cdot3\cdot5\cdots(2n-1)}{2\cdot4\cdot6\cdots(2n)} \right]^2\le \frac{1}{2n+1}$$ and so $$\lim_{n\to+\infty} \frac{1\cdot3\cdot5\cdots(2n-1)}{2\cdot4\cdot6\cdots(2n)}=0$$ my questions are: 1)-  how do I prove the inequality with the principle of induction? 2)-  there is another way to solve this limit?,I try to evaluate this limit: $$\lim_{n\to+\infty} \frac{1\cdot3\cdot5\cdots(2n-1)}{2\cdot4\cdot6\cdots(2n)}$$ I considered this inequality $$\frac{1}{4n}\le\left [ \frac{1\cdot3\cdot5\cdots(2n-1)}{2\cdot4\cdot6\cdots(2n)} \right]^2\le \frac{1}{2n+1}$$ and so $$\lim_{n\to+\infty} \frac{1\cdot3\cdot5\cdots(2n-1)}{2\cdot4\cdot6\cdots(2n)}=0$$ my questions are: 1)-  how do I prove the inequality with the principle of induction? 2)-  there is another way to solve this limit?,,"['calculus', 'real-analysis', 'sequences-and-series']"
82,When $L^p \subset L^q$ for $p <q$.,When  for .,L^p \subset L^q p <q,"Assume that $(X, \mathfrak{B}, m)$ is a measure space such that there exists a constant $\alpha>0$ such that for every $E \in \mathfrak{B}$ the following holds: $$ m(E)=0 \  \ or \ \ m(E)\geq \alpha.$$ Is it then true that for every $1\leq p \leq q \leq \infty$ $$L^p((X, \mathfrak{B}, m) \subset L^q(X, \mathfrak{B}, m)?$$ I know that it is true for spaces $l^p$ (it is a particular case of $L^p$ when $X=\mathbb{N}$, $\mathfrak{B}=2^\mathbb{N}$ and $m$ is a counting measure) -proof is for example here How do you show that $l_p \subset l_q$ for $p \leq q$? . My question is related to the last theorem in the another answer https://math.stackexchange.com/a/66038/20924 . Here is its proof, but not all is clear for me.   I have one doubt. It seems that here the following equalities are used: $$\|f\|_{L^p}=\sum_{j=1}^n a_j m(E_j)^{1/p},$$ $$\|f\|_{L^q}=\sum_{j=1}^n a_j m(E_j)^{1/q}$$  for $f(x)=\sum_{j=1}^n a_j \chi_{E_j}$, where $E_j$ are pairwise disjoint, which are generally not not true even for $l^p$ and $l^q$.","Assume that $(X, \mathfrak{B}, m)$ is a measure space such that there exists a constant $\alpha>0$ such that for every $E \in \mathfrak{B}$ the following holds: $$ m(E)=0 \  \ or \ \ m(E)\geq \alpha.$$ Is it then true that for every $1\leq p \leq q \leq \infty$ $$L^p((X, \mathfrak{B}, m) \subset L^q(X, \mathfrak{B}, m)?$$ I know that it is true for spaces $l^p$ (it is a particular case of $L^p$ when $X=\mathbb{N}$, $\mathfrak{B}=2^\mathbb{N}$ and $m$ is a counting measure) -proof is for example here How do you show that $l_p \subset l_q$ for $p \leq q$? . My question is related to the last theorem in the another answer https://math.stackexchange.com/a/66038/20924 . Here is its proof, but not all is clear for me.   I have one doubt. It seems that here the following equalities are used: $$\|f\|_{L^p}=\sum_{j=1}^n a_j m(E_j)^{1/p},$$ $$\|f\|_{L^q}=\sum_{j=1}^n a_j m(E_j)^{1/q}$$  for $f(x)=\sum_{j=1}^n a_j \chi_{E_j}$, where $E_j$ are pairwise disjoint, which are generally not not true even for $l^p$ and $l^q$.",,"['real-analysis', 'measure-theory', 'lp-spaces']"
83,"Convergence of a sequence, $a_n=\sum_1^nn/(n^2+k)$","Convergence of a sequence,",a_n=\sum_1^nn/(n^2+k),"Let $ a_{n} = \sum_{k=1}^{n} \frac{n}{n^{2}+k}$ . I would like to know whether the given sequence converges. I see  that, $ a_{n} = \sum_{k=1}^{n} \frac{n}{n^{2}+k}= \sum_{k=1}^{n} \frac{1}{n+\frac{k}{n}}.$ When $n$ gets sufficiently large the contribution by the $ \frac{k}{n} $ term is diminishing and $ a_{n} < \sum_{k=1}^{n} \frac{1}{n} = 1 $. Thank you.","Let $ a_{n} = \sum_{k=1}^{n} \frac{n}{n^{2}+k}$ . I would like to know whether the given sequence converges. I see  that, $ a_{n} = \sum_{k=1}^{n} \frac{n}{n^{2}+k}= \sum_{k=1}^{n} \frac{1}{n+\frac{k}{n}}.$ When $n$ gets sufficiently large the contribution by the $ \frac{k}{n} $ term is diminishing and $ a_{n} < \sum_{k=1}^{n} \frac{1}{n} = 1 $. Thank you.",,"['real-analysis', 'sequences-and-series', 'limits', 'summation']"
84,Questions about Fubini's theorem,Questions about Fubini's theorem,,"I learned the following from Hunter's Applied Analysis . Denote the Schwartz space  $${\mathcal S}({\mathbb R}^n):=\{\varphi\in C^{\infty}({\mathbb R}^n):\sup_{x\in{\mathbb R}^n}|x^{\alpha}\partial^{\beta}\varphi(x)|<\infty\quad\text{for every}\quad\alpha,\beta\in{\mathbb Z}_+^n\}.$$ If $\varphi\in{\mathcal S}({\mathbb R}^n)$, then the Fourier transform $\hat{\varphi}:{\mathbb R}^n\to{\mathbb C}$ is the function defined by  $$\hat{\varphi}(\xi):=\frac{1}{(2\pi)^{n/2}}\int_{{\mathbb R}^n}\varphi(x)e^{-i\xi\cdot x}dx,\quad \xi\in{\mathbb R}^n.$$ Suppose that $f,\varphi\in{\mathcal S}$. Then we have  $$\begin{align}\int\hat{f}(\xi)\varphi(\xi)d\xi&=\int\frac{1}{(2\pi)^{n/2}}\bigg(\int f(x)e^{-i\xi\cdot x}dx\bigg)\varphi(\xi)d\xi\\ &=\int f(x)\frac{1}{(2\pi)^{n/2}}\bigg(\int\varphi(\xi)e^{-i\xi\cdot x }d\xi\bigg)dx\\ &=\int f(x)\hat{\varphi}(x)dx. \end{align}$$ This is the motivation of the definition of the Fourier transform of tempered distributions. Here is my question : How can I get the second equality by Fubini's theorem? The form I know about the theorem is  $$\int_{A}\bigg(\int_B f(x,y)dy\bigg)dx =\int_{B}\bigg(\int_A f(x,y)dx\bigg)dy =\int_{A\times B}f(x,y)d(x,y)$$ But I am wondering what is $f(x,y)$ in the case above.","I learned the following from Hunter's Applied Analysis . Denote the Schwartz space  $${\mathcal S}({\mathbb R}^n):=\{\varphi\in C^{\infty}({\mathbb R}^n):\sup_{x\in{\mathbb R}^n}|x^{\alpha}\partial^{\beta}\varphi(x)|<\infty\quad\text{for every}\quad\alpha,\beta\in{\mathbb Z}_+^n\}.$$ If $\varphi\in{\mathcal S}({\mathbb R}^n)$, then the Fourier transform $\hat{\varphi}:{\mathbb R}^n\to{\mathbb C}$ is the function defined by  $$\hat{\varphi}(\xi):=\frac{1}{(2\pi)^{n/2}}\int_{{\mathbb R}^n}\varphi(x)e^{-i\xi\cdot x}dx,\quad \xi\in{\mathbb R}^n.$$ Suppose that $f,\varphi\in{\mathcal S}$. Then we have  $$\begin{align}\int\hat{f}(\xi)\varphi(\xi)d\xi&=\int\frac{1}{(2\pi)^{n/2}}\bigg(\int f(x)e^{-i\xi\cdot x}dx\bigg)\varphi(\xi)d\xi\\ &=\int f(x)\frac{1}{(2\pi)^{n/2}}\bigg(\int\varphi(\xi)e^{-i\xi\cdot x }d\xi\bigg)dx\\ &=\int f(x)\hat{\varphi}(x)dx. \end{align}$$ This is the motivation of the definition of the Fourier transform of tempered distributions. Here is my question : How can I get the second equality by Fubini's theorem? The form I know about the theorem is  $$\int_{A}\bigg(\int_B f(x,y)dy\bigg)dx =\int_{B}\bigg(\int_A f(x,y)dx\bigg)dy =\int_{A\times B}f(x,y)d(x,y)$$ But I am wondering what is $f(x,y)$ in the case above.",,['real-analysis']
85,Convergence of Sequences,Convergence of Sequences,,"I came across the following problems on convergence of sequences during the course of my self-study of real analysis: Suppose $a_n \to a$. Define $$s_n = \frac{1}{n}\sum_{k=1}^{n} a_k$$ Prove that $s_n \to a$. So $(a_n-a)$ is a null sequence. I want to show that $(s_n-a)$ is a null sequence. By a previous exercise, I know that $(x_n)$ is a null sequence $\implies$ $(y_n)$ is a null sequence where $y_{n} = (x_1+ \cdots+ x_n)/n$. So can we do something analogous to ""adding $a$ to both sides"" to get the desired result? Show that the sequence $$a_n = \left(1- \frac{1}{2} \right) \left(1- \frac{1}{3} \right) \cdots \left(1- \frac{1}{n+1} \right)$$ is convergent. So $a_1 = \frac{1}{2}$, $a_2 = \frac{1}{3}, \dots, a_n = \frac{1}{n+1}$. So I conjecture that $(a_n)$ is a null sequence. In other words, for each $\epsilon >0$, $|a_n| \leq \epsilon$ for all $n>N$. Let $\epsilon = \frac{1}{n}$. Choose $N = n+1$. Then the convergence follows? Prove that the sequence $$a_n = \frac{1}{n+1}+ \frac{1}{n+2} + \dots + \frac{1}{n+n}$$ is convergent to a limit $\leq 1$. So $a_{n} < a_{n+1}$ for all $n$. Then I need to show that it is bounded above by $1$. To show this should I consider $(1-a_n)$? All the terms are $<1$.","I came across the following problems on convergence of sequences during the course of my self-study of real analysis: Suppose $a_n \to a$. Define $$s_n = \frac{1}{n}\sum_{k=1}^{n} a_k$$ Prove that $s_n \to a$. So $(a_n-a)$ is a null sequence. I want to show that $(s_n-a)$ is a null sequence. By a previous exercise, I know that $(x_n)$ is a null sequence $\implies$ $(y_n)$ is a null sequence where $y_{n} = (x_1+ \cdots+ x_n)/n$. So can we do something analogous to ""adding $a$ to both sides"" to get the desired result? Show that the sequence $$a_n = \left(1- \frac{1}{2} \right) \left(1- \frac{1}{3} \right) \cdots \left(1- \frac{1}{n+1} \right)$$ is convergent. So $a_1 = \frac{1}{2}$, $a_2 = \frac{1}{3}, \dots, a_n = \frac{1}{n+1}$. So I conjecture that $(a_n)$ is a null sequence. In other words, for each $\epsilon >0$, $|a_n| \leq \epsilon$ for all $n>N$. Let $\epsilon = \frac{1}{n}$. Choose $N = n+1$. Then the convergence follows? Prove that the sequence $$a_n = \frac{1}{n+1}+ \frac{1}{n+2} + \dots + \frac{1}{n+n}$$ is convergent to a limit $\leq 1$. So $a_{n} < a_{n+1}$ for all $n$. Then I need to show that it is bounded above by $1$. To show this should I consider $(1-a_n)$? All the terms are $<1$.",,['real-analysis']
86,Can we bound from above sub-solutions of Volterra integral equations? (Nonlinear Gronwall's Lemma),Can we bound from above sub-solutions of Volterra integral equations? (Nonlinear Gronwall's Lemma),,"Gronwall's lemma says the following. Assume that $v\in C^0([t_0, T])$ is a nonnegative function. If $u \in C^0([t_0, T])$ satisfies the integral inequality $$u(t) \le c + \int_{t_0}^t u(s)v(s)\, ds,\qquad t \in [t_0, T]$$ where $c\in\mathbb{R}$, then $$u(t) \le c \exp\left(\int_{t_0}^t v(s)\, ds\right), \qquad t \in [t_0, T].$$ In other words, sub-solutions of the linear integral equation $$w(t)=c+ \int_{t_0}^t v(s)w(s)\, ds, \qquad t \in [t_0, T]$$ are dominated by solutions of the same equation, provided that the coefficient $v$ is nonnegative . Question What can we say about the general Volterra equation    $$\tag{1}\ w(t) = c + \int_0^t F(s, w(s))\, ds,\qquad t \in [0, T]?$$   Under what conditions on $F$ is a sub-solution of (1) dominated by a solution?","Gronwall's lemma says the following. Assume that $v\in C^0([t_0, T])$ is a nonnegative function. If $u \in C^0([t_0, T])$ satisfies the integral inequality $$u(t) \le c + \int_{t_0}^t u(s)v(s)\, ds,\qquad t \in [t_0, T]$$ where $c\in\mathbb{R}$, then $$u(t) \le c \exp\left(\int_{t_0}^t v(s)\, ds\right), \qquad t \in [t_0, T].$$ In other words, sub-solutions of the linear integral equation $$w(t)=c+ \int_{t_0}^t v(s)w(s)\, ds, \qquad t \in [t_0, T]$$ are dominated by solutions of the same equation, provided that the coefficient $v$ is nonnegative . Question What can we say about the general Volterra equation    $$\tag{1}\ w(t) = c + \int_0^t F(s, w(s))\, ds,\qquad t \in [0, T]?$$   Under what conditions on $F$ is a sub-solution of (1) dominated by a solution?",,"['real-analysis', 'integral-equations']"
87,function is smooth iff the composition with any smooth curve is again smooth,function is smooth iff the composition with any smooth curve is again smooth,,I'm stuck on the following part of a proof: Let $\phi: \mathbb R^m \to \mathbb R^n$ be a function such that $\gamma'(t) := \phi(\gamma(t))$ is smooth for every smooth function $\gamma: \mathbb R \to \mathbb R^m$. I want to show that $\phi$ is smooth under these assumptions. Could someone give me a pointer? Thanks in advance! S.L.,I'm stuck on the following part of a proof: Let $\phi: \mathbb R^m \to \mathbb R^n$ be a function such that $\gamma'(t) := \phi(\gamma(t))$ is smooth for every smooth function $\gamma: \mathbb R \to \mathbb R^m$. I want to show that $\phi$ is smooth under these assumptions. Could someone give me a pointer? Thanks in advance! S.L.,,['real-analysis']
88,Sandwiching the Lp norm sequence of random variable,Sandwiching the Lp norm sequence of random variable,,"Let X be a random variable with $\Vert X\Vert_p = E[\vert X\vert^p]^{1/p}<\infty$ for all $p\geq 1$ . Assume for some fixed $r$ , and for all $q,s$ satisfying $1\leq q < r < s$ , $$\lim_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/q}}=0$$ and $$\limsup_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/s}}=\infty$$ Does this imply that there is some $K>0$ such that $$\limsup_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/r}}=K$$","Let X be a random variable with for all . Assume for some fixed , and for all satisfying , and Does this imply that there is some such that","\Vert X\Vert_p = E[\vert X\vert^p]^{1/p}<\infty p\geq 1 r q,s 1\leq q < r < s \lim_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/q}}=0 \limsup_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/s}}=\infty K>0 \limsup_{p\to\infty} \frac{\Vert X\Vert_p}{p^{1/r}}=K","['real-analysis', 'probability', 'probability-theory', 'lp-spaces']"
89,Does every triangle satisfy $(a+b-c) \ge 8s\sum_{k=1}^{\infty}C_k \left(\frac{A}{s^2}\right)^{2k}$?,Does every triangle satisfy ?,(a+b-c) \ge 8s\sum_{k=1}^{\infty}C_k \left(\frac{A}{s^2}\right)^{2k},"For a fixed semi-perimeter $s$ and a fixed area $A$ , we can find infinitely many triangles of sides $a,b$ and $c$ whose semi-perimeter is $s$ and area is $A$ . I wanted to improve the triangle inequality by finding a expression for the minimum value of $a+b-c$ in terms of $s$ and $A$ . Partial progress was made in this question and also this question . I obtained the following inequality experimentally using Monte Carlo simulation. $$ a+b-c \ge \frac{8A^2}{s^3} + \frac{64A^4}{s^7} + \frac{896A^6}{s^{11}} $$ The first term of $(1)$ is proved in first of the above links. $(1)$ suggests the folowing conjecture: Conjecture : There exists constants $C_k$ such that $$ \min(a+b-c) =  8s\sum_{k=1}^{\infty}C_k \left(\frac{A}{s^2}\right)^{2k}\tag 1 $$ Can this conjecture be proved and can we express $C_k$ in terms of $k$ ? We have $C_1 = 1, C_2 = 8$ and $C_3 = 112$ . Interestingly, there are several sequences in OEIS which begin with $1,8,112$ . SageMath Code for estimating $C_4$ import random as rn  R  = 1 x1 = 0 y1 = R min1 = 10**999 stop = False i = 1  while stop == False:     x2 = R*rn.random()     y2 = (R^2 - x2^2)^0.5     x3 = R*rn.random()     y3 = (R^2 - x3^2)^0.5     a = (((x2)**2 + (y1 - y2)**2)**0.5)     b = (((x2 - x3)**2 + (y2 - y3)**2)**0.5)     c = (((x3 - x1)**2 + (y3 - y1)**2)**0.5)     s    = ((a+b+c)/2)     area = ((s*(s-a)*(s-b)*(s-c))**0.5)              try:         test1 = ((a+b-c) - (8*area^2/s^3 + 64*area^4/s^7 + 896*area^6/s^11)) / (area^8/s^15)     except:         continue      if test1 < min1:         min1 = test1         print(a,b,c,min1/8)              if min1 <= 0:             print('exception', min1.n(), p)             stop = True             break                  if i%10^7 == 0:         print(i)     i = i + 1","For a fixed semi-perimeter and a fixed area , we can find infinitely many triangles of sides and whose semi-perimeter is and area is . I wanted to improve the triangle inequality by finding a expression for the minimum value of in terms of and . Partial progress was made in this question and also this question . I obtained the following inequality experimentally using Monte Carlo simulation. The first term of is proved in first of the above links. suggests the folowing conjecture: Conjecture : There exists constants such that Can this conjecture be proved and can we express in terms of ? We have and . Interestingly, there are several sequences in OEIS which begin with . SageMath Code for estimating import random as rn  R  = 1 x1 = 0 y1 = R min1 = 10**999 stop = False i = 1  while stop == False:     x2 = R*rn.random()     y2 = (R^2 - x2^2)^0.5     x3 = R*rn.random()     y3 = (R^2 - x3^2)^0.5     a = (((x2)**2 + (y1 - y2)**2)**0.5)     b = (((x2 - x3)**2 + (y2 - y3)**2)**0.5)     c = (((x3 - x1)**2 + (y3 - y1)**2)**0.5)     s    = ((a+b+c)/2)     area = ((s*(s-a)*(s-b)*(s-c))**0.5)              try:         test1 = ((a+b-c) - (8*area^2/s^3 + 64*area^4/s^7 + 896*area^6/s^11)) / (area^8/s^15)     except:         continue      if test1 < min1:         min1 = test1         print(a,b,c,min1/8)              if min1 <= 0:             print('exception', min1.n(), p)             stop = True             break                  if i%10^7 == 0:         print(i)     i = i + 1","s A a,b c s A a+b-c s A 
a+b-c \ge \frac{8A^2}{s^3} + \frac{64A^4}{s^7} + \frac{896A^6}{s^{11}}
 (1) (1) C_k  \min(a+b-c) =  8s\sum_{k=1}^{\infty}C_k \left(\frac{A}{s^2}\right)^{2k}\tag 1  C_k k C_1 = 1, C_2 = 8 C_3 = 112 1,8,112 C_4","['real-analysis', 'geometry', 'inequality', 'asymptotics', 'triangles']"
90,Proving $ \cos x + \frac{2p}{\pi}\sin x \geq 1 - 2 \left(\frac{x}{\pi}\right)^p$ for $p>0$,Proving  for, \cos x + \frac{2p}{\pi}\sin x \geq 1 - 2 \left(\frac{x}{\pi}\right)^p p>0,"I came across the following inequality: $$ \cos x + \frac{2p}{\pi}\sin x \geq 1 - 2 \left(\frac{x}{\pi}\right)^p, $$ which should be valid for any $0\leq x \leq \pi$ and $p\in\mathbb R$ according to the graphs. I managed to prove it myself for $p\leq0$ as follows: If $p=0$ it's trivial, and if $p<0$ , I look at the difference $$f(x) = \cos x + \frac{2p}{\pi}\sin x - 1 + 2\left(\frac{x}{\pi}\right)^p$$ Noticing that $f(\pi)=0$ , it is enough to show $-f'(x)\geq0$ . Then, $$-f'(x) = \sin x - \frac{2p}{\pi}\left(\cos x + \left(\frac{x}{\pi}\right)^{p - 1}\right)$$ and all terms are nonnegative for $x\in[0,\frac{\pi}{2}]$ . If $x\in[\frac{\pi}{2},\pi]$ then still $\cos x + \left(\frac{x}{\pi}\right)^{p - 1}\geq0$ as the latter term is at least $1$ . This method doesn't help me for the case $p>0$ , though, as the function is no longer monotone. I tried to calculate (among other things) a taylor and fourier series, but found it wasn't much use for general $p$ . Any help or hints will be appreciated.","I came across the following inequality: which should be valid for any and according to the graphs. I managed to prove it myself for as follows: If it's trivial, and if , I look at the difference Noticing that , it is enough to show . Then, and all terms are nonnegative for . If then still as the latter term is at least . This method doesn't help me for the case , though, as the function is no longer monotone. I tried to calculate (among other things) a taylor and fourier series, but found it wasn't much use for general . Any help or hints will be appreciated.","
\cos x + \frac{2p}{\pi}\sin x \geq 1 - 2 \left(\frac{x}{\pi}\right)^p,
 0\leq x \leq \pi p\in\mathbb R p\leq0 p=0 p<0 f(x) = \cos x + \frac{2p}{\pi}\sin x - 1 + 2\left(\frac{x}{\pi}\right)^p f(\pi)=0 -f'(x)\geq0 -f'(x) = \sin x - \frac{2p}{\pi}\left(\cos x + \left(\frac{x}{\pi}\right)^{p - 1}\right) x\in[0,\frac{\pi}{2}] x\in[\frac{\pi}{2},\pi] \cos x + \left(\frac{x}{\pi}\right)^{p - 1}\geq0 1 p>0 p","['real-analysis', 'trigonometry', 'inequality', 'self-learning']"
91,"Convergence, continuity and differentiability of $f(x)=\frac{1}{x}-\frac{1}{x+1}+\frac{1}{x+2}-\frac{1}{x+3}+\ldots$","Convergence, continuity and differentiability of",f(x)=\frac{1}{x}-\frac{1}{x+1}+\frac{1}{x+2}-\frac{1}{x+3}+\ldots,"I am self-learning Real Analysis from the text, Understanding Analysis by Stephen Abbott. I'd like someone to verify, if my below proof and deductions are rigorous and technically correct. [Abbott 6.4.6] Let \begin{equation*} f( x) =\frac{1}{x} -\frac{1}{x+1} +\frac{1}{x+2} -\frac{1}{x+3} +\frac{1}{x+4} -\dotsc  \end{equation*} Show that $\displaystyle f$ is defined for all $\displaystyle x >0$ . Is $\displaystyle f$ continuous on $\displaystyle ( 0,\infty )$ ? How about differentiable? Proof . Well-definedness of $\displaystyle f$ . Define \begin{equation*} f_{n}( x) =\frac{( -1)^{n}}{x+n} \end{equation*} Then, \begin{equation*} f( x) =\sum _{n=0}^{\infty } f_{n}( x) \end{equation*} Let $\displaystyle x_{0}$ be an arbitrary point, such that $\displaystyle x_{0}  >0$ . Fix $\displaystyle x=x_{0}$ . We have: \begin{equation*} \frac{1}{x_{0}} \geq \frac{1}{x_{0} +1} \geq \frac{1}{x_{0} +2} \geq \dotsc \geq 0 \end{equation*} Moreover, \begin{equation*} \lim \frac{1}{x_{0} +n} =0 \end{equation*} By the Alternating Series Test for convergence, $\displaystyle \sum _{n=0}^{\infty } f_{n}( x)$ converges pointwise on $\displaystyle x >0$ . Continuity of $\displaystyle f$ . Let $\displaystyle [ a,\infty )$ be any interval such that $\displaystyle a >0$ . Let us group each of pair of terms of $\displaystyle f$ and write: \begin{equation*} \begin{array}{ c l } f( x) & =\left(\frac{1}{x} -\frac{1}{x+1}\right) +\left(\frac{1}{x+2} -\frac{1}{x+3}\right) +\left(\frac{1}{x+4} -\frac{1}{x+5}\right) +\dotsc  \end{array} \end{equation*} Define: \begin{equation*} g_{n}( x) =\frac{1}{( x+2n)} -\frac{1}{( x+2n+1)} =\frac{1}{( x+2n)( x+2n+1)} \end{equation*} Then, \begin{equation*} f( x) =\sum _{n=0}^{\infty } g_{n}( x) \end{equation*} Now, $\displaystyle g_{0}( x) =\frac{1}{a( a+1)} =M_{0}$ . Moreover, \begin{equation*} 0\leq g_{n}( x) \leq \frac{1}{( 2n)( 2n+1)} \leq \frac{1}{4n^{2}} =M_{n} \end{equation*} for all $\displaystyle n\geq 1$ . Since $\displaystyle \sum _{n=0}^{\infty } M_{n}$ converges, by the Weierstrass $\displaystyle M$ -Test, $\displaystyle \sum_{n=0}^{\infty } g_{n}( x)$ converges uniformly on $\displaystyle [ a,\infty )$ for any $\displaystyle a >0$ . Since each $\displaystyle g_{n}( x)$ is continuous for $\displaystyle x >0$ , by the Term-by-term continuity theorem, $\displaystyle f( x)$ is continuous on $\displaystyle [ a,\infty )$ , where $\displaystyle a >0$ . Thus, $\displaystyle f$ is continuous on $\displaystyle ( 0,\infty )$ . Differentiability of $\displaystyle f$ . Since each $\displaystyle g_{n}( x)$ is differentiable for $\displaystyle x >0$ , by the Term-by-Term differentiability theorem, $\displaystyle f$ is differentiable on $\displaystyle [ a,\infty )$ where $\displaystyle a >0$ . Thus, $\displaystyle f$ is differentiable on $\displaystyle ( 0,\infty )$ .","I am self-learning Real Analysis from the text, Understanding Analysis by Stephen Abbott. I'd like someone to verify, if my below proof and deductions are rigorous and technically correct. [Abbott 6.4.6] Let Show that is defined for all . Is continuous on ? How about differentiable? Proof . Well-definedness of . Define Then, Let be an arbitrary point, such that . Fix . We have: Moreover, By the Alternating Series Test for convergence, converges pointwise on . Continuity of . Let be any interval such that . Let us group each of pair of terms of and write: Define: Then, Now, . Moreover, for all . Since converges, by the Weierstrass -Test, converges uniformly on for any . Since each is continuous for , by the Term-by-term continuity theorem, is continuous on , where . Thus, is continuous on . Differentiability of . Since each is differentiable for , by the Term-by-Term differentiability theorem, is differentiable on where . Thus, is differentiable on .","\begin{equation*}
f( x) =\frac{1}{x} -\frac{1}{x+1} +\frac{1}{x+2} -\frac{1}{x+3} +\frac{1}{x+4} -\dotsc 
\end{equation*} \displaystyle f \displaystyle x >0 \displaystyle f \displaystyle ( 0,\infty ) \displaystyle f \begin{equation*}
f_{n}( x) =\frac{( -1)^{n}}{x+n}
\end{equation*} \begin{equation*}
f( x) =\sum _{n=0}^{\infty } f_{n}( x)
\end{equation*} \displaystyle x_{0} \displaystyle x_{0}  >0 \displaystyle x=x_{0} \begin{equation*}
\frac{1}{x_{0}} \geq \frac{1}{x_{0} +1} \geq \frac{1}{x_{0} +2} \geq \dotsc \geq 0
\end{equation*} \begin{equation*}
\lim \frac{1}{x_{0} +n} =0
\end{equation*} \displaystyle \sum _{n=0}^{\infty } f_{n}( x) \displaystyle x >0 \displaystyle f \displaystyle [ a,\infty ) \displaystyle a >0 \displaystyle f \begin{equation*}
\begin{array}{ c l }
f( x) & =\left(\frac{1}{x} -\frac{1}{x+1}\right) +\left(\frac{1}{x+2} -\frac{1}{x+3}\right) +\left(\frac{1}{x+4} -\frac{1}{x+5}\right) +\dotsc 
\end{array}
\end{equation*} \begin{equation*}
g_{n}( x) =\frac{1}{( x+2n)} -\frac{1}{( x+2n+1)} =\frac{1}{( x+2n)( x+2n+1)}
\end{equation*} \begin{equation*}
f( x) =\sum _{n=0}^{\infty } g_{n}( x)
\end{equation*} \displaystyle g_{0}( x) =\frac{1}{a( a+1)} =M_{0} \begin{equation*}
0\leq g_{n}( x) \leq \frac{1}{( 2n)( 2n+1)} \leq \frac{1}{4n^{2}} =M_{n}
\end{equation*} \displaystyle n\geq 1 \displaystyle \sum _{n=0}^{\infty } M_{n} \displaystyle M \displaystyle \sum_{n=0}^{\infty } g_{n}( x) \displaystyle [ a,\infty ) \displaystyle a >0 \displaystyle g_{n}( x) \displaystyle x >0 \displaystyle f( x) \displaystyle [ a,\infty ) \displaystyle a >0 \displaystyle f \displaystyle ( 0,\infty ) \displaystyle f \displaystyle g_{n}( x) \displaystyle x >0 \displaystyle f \displaystyle [ a,\infty ) \displaystyle a >0 \displaystyle f \displaystyle ( 0,\infty )","['real-analysis', 'sequences-and-series', 'solution-verification', 'uniform-convergence', 'sequence-of-function']"
92,Is every function eligible as a boundary condition?,Is every function eligible as a boundary condition?,,"Consider the Laplace equation on unit square domain: $$ u_{xx} + u_{yy} = 0, 0 < x < 1, 0 < y < 1 $$ I wondered whether there is any boundary condition that wouldn't let this equation have a solution. Suppose the boundary condition is $u(t,0) = u(t,1) = u(0,t) = u(1,t) = f(t)$ for some real-valued function $f$ . I thought $f$ would need to be ""pathological"". The candidates I came up in mind were: Dirichlet's function: Though this candidate might let the equation seem unsolvable at first glance, my intuition tells that the solution would be $u = 0$ anyway, for Dirichlet's function is zero almost everywhere. Thomae's function: This suffers from the same argument as above. Not to mention that Thomae's function is Riemann-integrable. Cantor's staircase: This really would make the equation seem weird, but still, my intuition tells that the usual methods of solving PDE, such as separation of variables, superposition principle, and Fourier transform, would go through well anyway. Minkowski's question-mark: Again, this suffers from the same argument as above. Weierstrass' function: Would being nowhere differentiable make it? Hell if I know. Indicator function on Vitali set: Now I'm talking about non-measurability. Dang. Is there an example of such $f$ ? Or to loosen the requirements, is there any instance of Laplace equation whose boundary condition doesn't let a solution to be there?","Consider the Laplace equation on unit square domain: I wondered whether there is any boundary condition that wouldn't let this equation have a solution. Suppose the boundary condition is for some real-valued function . I thought would need to be ""pathological"". The candidates I came up in mind were: Dirichlet's function: Though this candidate might let the equation seem unsolvable at first glance, my intuition tells that the solution would be anyway, for Dirichlet's function is zero almost everywhere. Thomae's function: This suffers from the same argument as above. Not to mention that Thomae's function is Riemann-integrable. Cantor's staircase: This really would make the equation seem weird, but still, my intuition tells that the usual methods of solving PDE, such as separation of variables, superposition principle, and Fourier transform, would go through well anyway. Minkowski's question-mark: Again, this suffers from the same argument as above. Weierstrass' function: Would being nowhere differentiable make it? Hell if I know. Indicator function on Vitali set: Now I'm talking about non-measurability. Dang. Is there an example of such ? Or to loosen the requirements, is there any instance of Laplace equation whose boundary condition doesn't let a solution to be there?","
u_{xx} + u_{yy} = 0, 0 < x < 1, 0 < y < 1
 u(t,0) = u(t,1) = u(0,t) = u(1,t) = f(t) f f u = 0 f","['real-analysis', 'partial-differential-equations', 'examples-counterexamples']"
93,Billingsley Problem 9.4: Applications of the law of iterated logarithms,Billingsley Problem 9.4: Applications of the law of iterated logarithms,,"Here is question from Billingsley Problems 9.4. Let $\{X_n\}_n$ be i.i.d. simple random variables with mean $0$ and variance $1$ . Then law of iterated logarithms holds. Set $S_n = X_1 + \cdots + X_n$ . From $$P\bigg(\limsup_{n} \frac{S_n}{\sqrt{2n\log\log(n)}} = 1\bigg) = 1$$ and $$P\bigg(\liminf_{n} \frac{S_n}{\sqrt{2n\log\log(n)}} = -1\bigg) = 1,$$ together with the uniform bounded-ness of the $X_n$ , deduce that with probability $1$ the set of limit points of the sequence $$\bigg\{\frac{S_n}{\sqrt{2n\log\log(n)}}\bigg\}$$ is the closed interval from —1 to + 1. I am not sure where to start. Assumptions tell you where you the smallest and largest the limit can be, but takeng $x \in [-1,1]$ , how do I build a sequence of the form $$\{x_k\}_{k} = \bigg\{\frac{S_{n_k}}{\sqrt{2n_k\log\log(n_k)}}\bigg\}_{k}$$ such that $x_k \to x$ ? Maybe this is not a good way to approach the problem.","Here is question from Billingsley Problems 9.4. Let be i.i.d. simple random variables with mean and variance . Then law of iterated logarithms holds. Set . From and together with the uniform bounded-ness of the , deduce that with probability the set of limit points of the sequence is the closed interval from —1 to + 1. I am not sure where to start. Assumptions tell you where you the smallest and largest the limit can be, but takeng , how do I build a sequence of the form such that ? Maybe this is not a good way to approach the problem.","\{X_n\}_n 0 1 S_n = X_1 + \cdots + X_n P\bigg(\limsup_{n} \frac{S_n}{\sqrt{2n\log\log(n)}} = 1\bigg) = 1 P\bigg(\liminf_{n} \frac{S_n}{\sqrt{2n\log\log(n)}} = -1\bigg) = 1, X_n 1 \bigg\{\frac{S_n}{\sqrt{2n\log\log(n)}}\bigg\} x \in [-1,1] \{x_k\}_{k} = \bigg\{\frac{S_{n_k}}{\sqrt{2n_k\log\log(n_k)}}\bigg\}_{k} x_k \to x","['real-analysis', 'probability', 'probability-theory']"
94,"T/F: If $(x_n)$ is a positive real sequence s.t. $\sum x_n$ converges, then $\exists N$ s.t. $x_{\left\lceil\frac{1}{x_N }\right\rceil}<\frac{1}{N}.$","T/F: If  is a positive real sequence s.t.  converges, then  s.t.",(x_n) \sum x_n \exists N x_{\left\lceil\frac{1}{x_N }\right\rceil}<\frac{1}{N}.,"Proposition : Suppose $(x_n)$ is a positive real sequence such that $\displaystyle\sum_n x_n$ converges, then there exists $N\in\mathbb{N}$ such that $\ \large{ x_{ \left\lceil \frac{1}{x_N } \right\rceil } } < \frac{1}{N}. $ Attempt $1$ : $\ \displaystyle\sum_n x_n$ converges $\implies \neg \left( x_n \geq \frac{1}{n}\  \forall n\in\mathbb{N} \right). $ Therefore $\exists N\ $ such that $x_N < \frac{1}{N},\ \implies \left\lceil \frac{1}{x_N } \right\rceil > N.$ Now what? I'm also also not making much progress via proof by contradiction. But it feels like it must be true somehow...","Proposition : Suppose is a positive real sequence such that converges, then there exists such that Attempt : converges Therefore such that Now what? I'm also also not making much progress via proof by contradiction. But it feels like it must be true somehow...","(x_n) \displaystyle\sum_n x_n N\in\mathbb{N} \ \large{ x_{ \left\lceil \frac{1}{x_N } \right\rceil } } < \frac{1}{N}.  1 \ \displaystyle\sum_n x_n \implies \neg \left( x_n \geq \frac{1}{n}\  \forall n\in\mathbb{N} \right).  \exists N\  x_N < \frac{1}{N},\ \implies \left\lceil \frac{1}{x_N } \right\rceil > N.","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'recreational-mathematics', 'problem-solving']"
95,Does a Lower Bounded Differentiable Function Always Have Arbitrarily Small Gradient?,Does a Lower Bounded Differentiable Function Always Have Arbitrarily Small Gradient?,,"I am studying optimization and the following question occurred to me: Suppose $f:\mathbb{R}^n\to\mathbb{R}$ is differentiable and bounded below. Is it true that for every $\epsilon>0$ , there exists $x\in\mathbb{R}^n$ such that $\|\nabla f(x)\|<\epsilon$ ? I am able to show that this is true for $n=1$ . Let $f^*=\inf_{x\in\mathbb{R}}f(x)$ , then there exists a sequence $\{x_n\}$ such that $f(x_n)\to f^*$ . There are two cases. Case 1. If $\{x_n\}$ has a limit point $x^*$ , then $f(x^*)=f^*$ , so $x^*$ is a global minimizer and $f'(x^*)=0$ . Case 2. If $\{x_n\}$ does not have a limit point, then $|x_n|\to\infty$ . By Mean Value Theorem, there exists $\xi_n$ such that $f(x_n)-f(0)=f'(\xi_n)x_n$ . Thus $|f'(\xi_n)|=|f(x_n)-f(0)|/|x_n|\to0$ . In $\mathbb{R}^n$ , the analysis of case 1 is the same, but case 2 does not generalize naturally, because MVT becomes $f(x_n)-f(0)=\nabla f(\xi_n)^\top x_n$ . Is there a way to get around this?","I am studying optimization and the following question occurred to me: Suppose is differentiable and bounded below. Is it true that for every , there exists such that ? I am able to show that this is true for . Let , then there exists a sequence such that . There are two cases. Case 1. If has a limit point , then , so is a global minimizer and . Case 2. If does not have a limit point, then . By Mean Value Theorem, there exists such that . Thus . In , the analysis of case 1 is the same, but case 2 does not generalize naturally, because MVT becomes . Is there a way to get around this?",f:\mathbb{R}^n\to\mathbb{R} \epsilon>0 x\in\mathbb{R}^n \|\nabla f(x)\|<\epsilon n=1 f^*=\inf_{x\in\mathbb{R}}f(x) \{x_n\} f(x_n)\to f^* \{x_n\} x^* f(x^*)=f^* x^* f'(x^*)=0 \{x_n\} |x_n|\to\infty \xi_n f(x_n)-f(0)=f'(\xi_n)x_n |f'(\xi_n)|=|f(x_n)-f(0)|/|x_n|\to0 \mathbb{R}^n f(x_n)-f(0)=\nabla f(\xi_n)^\top x_n,"['real-analysis', 'multivariable-calculus', 'optimization']"
96,Determinant of Jacobian of matrix multiplication,Determinant of Jacobian of matrix multiplication,,"Let $A \in \mathbb R^{n \times n}$ . We consider the map $$f_A : \mathbb R^{n \times n} \to \mathbb R^{n \times n} ,\quad X \mapsto AX.$$ By considering easy examples of $A$ one comes up quite fast with the conjecture $\det(D f_A) = \det(A)^n$ . Here $D f_A$ is the Jacobian matrix of $f_A$ . Is there an elegant proof which does not result in a long confusing computation? Idea (edit): I just came up with an idea. Obviously, the statement has only to be proven for non-singular $A$ . They are generated by elementary matrices. So it suffices to consider them since we have $f_A \circ f_B = f_{AB}$ .","Let . We consider the map By considering easy examples of one comes up quite fast with the conjecture . Here is the Jacobian matrix of . Is there an elegant proof which does not result in a long confusing computation? Idea (edit): I just came up with an idea. Obviously, the statement has only to be proven for non-singular . They are generated by elementary matrices. So it suffices to consider them since we have .","A \in \mathbb R^{n \times n} f_A : \mathbb R^{n \times n} \to \mathbb R^{n \times n} ,\quad X \mapsto AX. A \det(D f_A) = \det(A)^n D f_A f_A A f_A \circ f_B = f_{AB}","['real-analysis', 'linear-algebra', 'determinant', 'jacobian']"
97,Convergence of integral on every measurable subset,Convergence of integral on every measurable subset,,"Let $\{f_n\}$ be sequence in $L^2[0,1]$ and $f$ be a Lebesgue measurable function such that for every Lebesgue measurable set $E$ in $[0,1]$ , $\lim_{n \to \infty} \int_{E}f_n dx = \int_{E}fdx$ . Assume also that $\sup_{n}\int_{0}^{1} \lvert f_n \rvert^2 dx < \infty$ . I want to show that $f \in L^2[0,1]$ . My thought so far: If we manage to show there is a subsequence of $f_n$ converging pointwise a.e. on $[0,1]$ to $f$ , we're done. Indeed, if this is true, then by Fatou's lemma, we have $$\int_{0}^{1} \lvert f \rvert^2 dx \leqslant \liminf \int_{0}^{1} \lvert f_{n_k}\rvert^2 dx \leqslant \sup_{n}\int_{0}^{1} \lvert f_n \rvert^2 dx < \infty$$ But I don't know how to achieve that. I tried to show $f_n \to f$ in $L^1$ or in measure but I failed. Any insight would be appreciated.","Let be sequence in and be a Lebesgue measurable function such that for every Lebesgue measurable set in , . Assume also that . I want to show that . My thought so far: If we manage to show there is a subsequence of converging pointwise a.e. on to , we're done. Indeed, if this is true, then by Fatou's lemma, we have But I don't know how to achieve that. I tried to show in or in measure but I failed. Any insight would be appreciated.","\{f_n\} L^2[0,1] f E [0,1] \lim_{n \to \infty} \int_{E}f_n dx = \int_{E}fdx \sup_{n}\int_{0}^{1} \lvert f_n \rvert^2 dx < \infty f \in L^2[0,1] f_n [0,1] f \int_{0}^{1} \lvert f \rvert^2 dx \leqslant \liminf \int_{0}^{1} \lvert f_{n_k}\rvert^2 dx \leqslant \sup_{n}\int_{0}^{1} \lvert f_n \rvert^2 dx < \infty f_n \to f L^1","['real-analysis', 'measure-theory']"
98,Extremal of Compact Banach Space Embedding,Extremal of Compact Banach Space Embedding,,"Let $(X, \|\cdot	\|_X)$ and $(Y,\|\cdot	\|_Y)$ be (nontrivial real) Banach spaces such that $X \subseteq Y$ . We say $X$ is compactly embedded into $Y$ if the following two conditions hold : There exists a constant $C>0$ such that $\|	x\|_Y \leq C\|x	\|_X$ for every $x \in X$ . If a sequence $\{x_n\}_{n \geq 1} \subseteq X$ is bounded with respect to the norm $\|\cdot	\|_X$ , then its closure in $Y$ is a compact subset of $Y$ . The best constant $C$ is given by: \begin{equation*} C = \inf_{x \in X, x \neq 0} \frac{\|	x\|_Y}{\|	x\|_X} \end{equation*} Suppose $X$ is compactly embedded into $Y$ . Does it follow that the above best constant is attained? That is, does there exist a nonzero element $x \in X$ such that $C= \frac{\|	x\|_Y}{\|	x\|_X}$ ? If not, under what additional assumption will such an extremal element exist? My attempt: For each positive integer $n$ , there exists a nonzero element $x_n \in X$ such that: \begin{equation*} C \leq \frac{\|	x_n\|_Y}{\|	x_n\|_X} < C + \frac{1}{n} \end{equation*} Put $y_n = \frac{x_n}{\|	x_n\|_X}$ . Then $y_n \in X$ , $\|	y_n\|_X=1$ , and: \begin{equation*} C \leq \frac{\|	y_n\|_Y}{\|	y_n\|_X}=\|	y_n\|_Y < C + \frac{1}{n} \end{equation*} By property $2$ , there is a $y \in Y$ and a subsequence $\{n_k\}_{k\geq 1}$ such that $y_{n_k} \to y$ as $k \to \infty$ with respect to the $\|	\cdot \|_Y$ norm. By continuity of norm and squeeze theorem, we have $\|	y \|_Y = C>0$ so that $y \neq 0$ . Now all I need to prove is that $y \in X$ . But I am stuck on this step. Clearly $\{y_{n_k}\}_{k \geq 0}$ is a Cauchy sequence in $\|	\cdot \|_Y$ but I have no idea about how to make it Cauchy in $X$ . On the other hand, I am unable to come up with counterexamples .","Let and be (nontrivial real) Banach spaces such that . We say is compactly embedded into if the following two conditions hold : There exists a constant such that for every . If a sequence is bounded with respect to the norm , then its closure in is a compact subset of . The best constant is given by: Suppose is compactly embedded into . Does it follow that the above best constant is attained? That is, does there exist a nonzero element such that ? If not, under what additional assumption will such an extremal element exist? My attempt: For each positive integer , there exists a nonzero element such that: Put . Then , , and: By property , there is a and a subsequence such that as with respect to the norm. By continuity of norm and squeeze theorem, we have so that . Now all I need to prove is that . But I am stuck on this step. Clearly is a Cauchy sequence in but I have no idea about how to make it Cauchy in . On the other hand, I am unable to come up with counterexamples .","(X, \|\cdot	\|_X) (Y,\|\cdot	\|_Y) X \subseteq Y X Y C>0 \|	x\|_Y \leq C\|x	\|_X x \in X \{x_n\}_{n \geq 1} \subseteq X \|\cdot	\|_X Y Y C \begin{equation*}
C = \inf_{x \in X, x \neq 0} \frac{\|	x\|_Y}{\|	x\|_X}
\end{equation*} X Y x \in X C= \frac{\|	x\|_Y}{\|	x\|_X} n x_n \in X \begin{equation*}
C \leq \frac{\|	x_n\|_Y}{\|	x_n\|_X} < C + \frac{1}{n}
\end{equation*} y_n = \frac{x_n}{\|	x_n\|_X} y_n \in X \|	y_n\|_X=1 \begin{equation*}
C \leq \frac{\|	y_n\|_Y}{\|	y_n\|_X}=\|	y_n\|_Y < C + \frac{1}{n}
\end{equation*} 2 y \in Y \{n_k\}_{k\geq 1} y_{n_k} \to y k \to \infty \|	\cdot \|_Y \|	y \|_Y = C>0 y \neq 0 y \in X \{y_{n_k}\}_{k \geq 0} \|	\cdot \|_Y X","['real-analysis', 'functional-analysis', 'optimization', 'banach-spaces']"
99,Showing that the best approximating linear map for a Lipschitz function is also Lipschitz,Showing that the best approximating linear map for a Lipschitz function is also Lipschitz,,"For $d,n \in \mathbb{N}$ with $1 \leq d<n$ , let $f: \mathbb{R}^d \to \mathbb{R}^n$ be a Lipschitz map with some constant $L \geq 1$ . Let $B=B(0,r)$ for some $r>0$ and define the quantity $$ \Omega(B) = \inf_{A} \left( \frac{1}{|B|} \int_{B} \left(  \frac{|f(y)-A(y)|}{r} \right)^2 dy \right)^{\frac{1}{2}}, $$ where the infimum is over all linear maps $A: \mathbb{R}^{d} \to \mathbb{R}^{n}$ and $|B|$ is just the $d$ -Lebesgue measure of $B$ . Suppose that $\Omega(B) < \epsilon$ where $\epsilon$ is as small as we wish. I am trying to show that the infimizing map $A$ for $\Omega(B)$ is also Lipschitz with constant, say, $2L$ . This intuitively seems to be true as $\Omega(B)$ being small means that $A$ must approximate an $L$ -Lipschitz function $f$ very well inside $B$ and so should not deviate much from $f$ . I suspect even without $\Omega(B) < \epsilon$ assumption, the best approximating map should still be $2L$ -Lipschitz. For example if we had defined $``L^{\infty}""$ version of this quantity by $$\Omega_{\infty}(B) = \inf_{A} \frac{|| f-A ||_{L^{\infty}(B)}}{r}$$ then $ \Omega_{\infty}(B) \leq \frac{|| f-f(0) ||_{L^{\infty}(B)}}{r}  \leq L$ so that for the map $A$ that realizes infimum in $\Omega_{\infty}(B)$ , $|f(x)-A(x)| \leq L r$ for any $x \in B$ and from here it is not difficult to show that $A$ is $2L$ -Lipschitz. I am having troubles with the map $A$ when defined for integral version $\Omega(B)$ though. Some help would be appreciated. References: These quantities originate from Dorronsoro's paper .","For with , let be a Lipschitz map with some constant . Let for some and define the quantity where the infimum is over all linear maps and is just the -Lebesgue measure of . Suppose that where is as small as we wish. I am trying to show that the infimizing map for is also Lipschitz with constant, say, . This intuitively seems to be true as being small means that must approximate an -Lipschitz function very well inside and so should not deviate much from . I suspect even without assumption, the best approximating map should still be -Lipschitz. For example if we had defined version of this quantity by then so that for the map that realizes infimum in , for any and from here it is not difficult to show that is -Lipschitz. I am having troubles with the map when defined for integral version though. Some help would be appreciated. References: These quantities originate from Dorronsoro's paper .","d,n \in \mathbb{N} 1 \leq d<n f: \mathbb{R}^d \to \mathbb{R}^n L \geq 1 B=B(0,r) r>0  \Omega(B) = \inf_{A} \left( \frac{1}{|B|} \int_{B} \left(  \frac{|f(y)-A(y)|}{r} \right)^2 dy \right)^{\frac{1}{2}},  A: \mathbb{R}^{d} \to \mathbb{R}^{n} |B| d B \Omega(B) < \epsilon \epsilon A \Omega(B) 2L \Omega(B) A L f B f \Omega(B) < \epsilon 2L ``L^{\infty}"" \Omega_{\infty}(B) = \inf_{A} \frac{|| f-A ||_{L^{\infty}(B)}}{r}  \Omega_{\infty}(B) \leq \frac{|| f-f(0) ||_{L^{\infty}(B)}}{r}  \leq L A \Omega_{\infty}(B) |f(x)-A(x)| \leq L r x \in B A 2L A \Omega(B)","['real-analysis', 'functional-analysis', 'harmonic-analysis', 'lipschitz-functions']"
