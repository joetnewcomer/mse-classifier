,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Converting ODE to Variational Problem (for numerical solution),Converting ODE to Variational Problem (for numerical solution),,"This might be a stupid question, but I cannot find the answer anywhere and as an engineer I don't have the mathematical foundation to investigate this properly myself. So, If I have a simple ODE, say for example a harmonic oscillator like $u''+u=0$ , I know that I can try to convert it into a variational problem by finding a Lagrange function $F$ , so that if I plug it into to the Euler-Lagrange differential equation $$\frac{\partial F}{\partial u}-\frac{\mathrm{d}}{\mathrm{d}x}\frac{\partial F}{\partial u'}+\frac{\mathrm{d^{2}}}{\mathrm{d}x^{2}}\frac{\partial F}{\partial u''}=0$$ I get my ODE back. In this case one easily finds that e.g. $$F=\left(u'\right)^2-u^2$$ fulfills this condition. However now I considered the naive idea of simply just taking the following integral $$\int_{x_{1}}^{x_{2}}(u''+u)^{2}\mathrm{d}x\rightarrow\text{min.}$$ So basically to take $F:=(\text{some ODE})^2$ . Naively this seems to me like in its minimum the ODE is satisfied. To test this, I implemented a numerical solver that takes a polynomial ansatz and solves the coefficients to find an approximation of the solution (here a sine) of the ODE between $0$ and $\pi/2$ . And it seems to actually work but the solution isn't exactly the same as for $F=\left(u'\right)^2-u^2$ - it is a little bit less accurate (so clearly not the optimal solution yielded by the ansatz)! However, if I take piece-wise linear functions, this native functional completely fails to provide an approximate solution, while $F=\left(u'\right)^2-u^2$ works perfectly. To investigate a little further I plugged in $F=(u''+u)^{2}$ into the Euler-Lagrange eq. and realize that I get $$2(u''+u)+2\left(u''+u\right)''=0$$ which seems to satisfy the same solution as my original problem. So my two questions that arise from all this: 1) Why is it that the naive idea of doing $F=(\text{some ODE})^2$ seems to be working somewhat but not always, if I want to numerically solve an ODE via variational method? 2) Consequently: If I have an ODE of the form $f(u, u', u'', ..., x)=0$ , all ODEs of the form $f(u, u', u'', ..., x) + \left(f(u, u', u'', ..., x)\right)' + \left(f(u, u', u'', ..., x)\right)'' + ... =0$ seem to have the same solution as the original ODE, but they are in fact of higher order. What is the significance of these ODEs and their solutions? Apologies of the lengthy post but I would really appreciate any input on this. As I said, I am an engineer and quite out of my depth here. Thank you in advance!","This might be a stupid question, but I cannot find the answer anywhere and as an engineer I don't have the mathematical foundation to investigate this properly myself. So, If I have a simple ODE, say for example a harmonic oscillator like , I know that I can try to convert it into a variational problem by finding a Lagrange function , so that if I plug it into to the Euler-Lagrange differential equation I get my ODE back. In this case one easily finds that e.g. fulfills this condition. However now I considered the naive idea of simply just taking the following integral So basically to take . Naively this seems to me like in its minimum the ODE is satisfied. To test this, I implemented a numerical solver that takes a polynomial ansatz and solves the coefficients to find an approximation of the solution (here a sine) of the ODE between and . And it seems to actually work but the solution isn't exactly the same as for - it is a little bit less accurate (so clearly not the optimal solution yielded by the ansatz)! However, if I take piece-wise linear functions, this native functional completely fails to provide an approximate solution, while works perfectly. To investigate a little further I plugged in into the Euler-Lagrange eq. and realize that I get which seems to satisfy the same solution as my original problem. So my two questions that arise from all this: 1) Why is it that the naive idea of doing seems to be working somewhat but not always, if I want to numerically solve an ODE via variational method? 2) Consequently: If I have an ODE of the form , all ODEs of the form seem to have the same solution as the original ODE, but they are in fact of higher order. What is the significance of these ODEs and their solutions? Apologies of the lengthy post but I would really appreciate any input on this. As I said, I am an engineer and quite out of my depth here. Thank you in advance!","u''+u=0 F \frac{\partial F}{\partial u}-\frac{\mathrm{d}}{\mathrm{d}x}\frac{\partial F}{\partial u'}+\frac{\mathrm{d^{2}}}{\mathrm{d}x^{2}}\frac{\partial F}{\partial u''}=0 F=\left(u'\right)^2-u^2 \int_{x_{1}}^{x_{2}}(u''+u)^{2}\mathrm{d}x\rightarrow\text{min.} F:=(\text{some ODE})^2 0 \pi/2 F=\left(u'\right)^2-u^2 F=\left(u'\right)^2-u^2 F=(u''+u)^{2} 2(u''+u)+2\left(u''+u\right)''=0 F=(\text{some ODE})^2 f(u, u', u'', ..., x)=0 f(u, u', u'', ..., x) + \left(f(u, u', u'', ..., x)\right)' + \left(f(u, u', u'', ..., x)\right)'' + ... =0","['ordinary-differential-equations', 'numerical-methods', 'calculus-of-variations', 'differential']"
1,"ODE of second order, proving that polynomials at $t_0$ are zero","ODE of second order, proving that polynomials at  are zero",t_0,"The following ODE is given: $$y''(t) + p(t)y'(t) + q(t)y(t)=0$$ When $p(t), q(t)$ are continuous functions. We are given two linear independent solutions $y_1(t), y_2(t)$ and also $y_1''(t_0) = y_2''(t_0) = 0$ . I need to prove that $p(t_0) = q(t_0) = 0$ . What I've tried is just placing zero in the second derivative for each function in the ODE, and working with the Wronskian. However I end up with $$p(t)(y_1'(t_0) - y_2'(t_0)) + q(t)(y_1(t_0) - y_2(t_0))$$ which is not the Wronskian. Any help?","The following ODE is given: When are continuous functions. We are given two linear independent solutions and also . I need to prove that . What I've tried is just placing zero in the second derivative for each function in the ODE, and working with the Wronskian. However I end up with which is not the Wronskian. Any help?","y''(t) + p(t)y'(t) + q(t)y(t)=0 p(t), q(t) y_1(t), y_2(t) y_1''(t_0) = y_2''(t_0) = 0 p(t_0) = q(t_0) = 0 p(t)(y_1'(t_0) - y_2'(t_0)) + q(t)(y_1(t_0) - y_2(t_0))",['ordinary-differential-equations']
2,Find the Floquet multipliersfor the Markus & Yamabe system,Find the Floquet multipliersfor the Markus & Yamabe system,,"I need some help with the following excersice.Find the minimum period and the Floquet multipliers $\bf\lambda_{1},\lambda_{2}$ of the following matrix. $A(t)=\begin{bmatrix}-1+\frac{3}{2}cos^2(t) & 1-\frac{3}{2}cos(t)sin(t)\\-1-\frac{3}{2}cos(t)sin(t) & -1+\frac{3}{2}sin^2(t)\end{bmatrix}$ It is obvious that the minimum period of A is $\pi$ ,just by using the double-angle formulas $\cos^2(t)=\frac{1+cos(2t)}{2},sin^2(t)=\frac{1-cos(2t)}{2}$ .Furthermore by using the relation $$\prod_{i=1}^2\lambda_{i}=e^{\intop_0^T trace(A(s))ds}$$ we get easily that $\prod_{i=1}^2\lambda_{i}=e^{-\frac{\pi}{2}}$ .But now i am stuck!","I need some help with the following excersice.Find the minimum period and the Floquet multipliers of the following matrix. It is obvious that the minimum period of A is ,just by using the double-angle formulas .Furthermore by using the relation we get easily that .But now i am stuck!","\bf\lambda_{1},\lambda_{2} A(t)=\begin{bmatrix}-1+\frac{3}{2}cos^2(t) & 1-\frac{3}{2}cos(t)sin(t)\\-1-\frac{3}{2}cos(t)sin(t) & -1+\frac{3}{2}sin^2(t)\end{bmatrix} \pi \cos^2(t)=\frac{1+cos(2t)}{2},sin^2(t)=\frac{1-cos(2t)}{2} \prod_{i=1}^2\lambda_{i}=e^{\intop_0^T trace(A(s))ds} \prod_{i=1}^2\lambda_{i}=e^{-\frac{\pi}{2}}","['ordinary-differential-equations', 'dynamical-systems']"
3,Stuck in solving the Hermite DE?,Stuck in solving the Hermite DE?,,"As a part of solving the Schr√∂dinger equation 1 for quantum Harmonic oscillator: \begin{equation}\tag{1} 	\left(-\frac{d^2}{d\tilde{x}^2} + \tilde{x}^2\right) \tilde{\psi}(\tilde{x}) = \tilde{E} \tilde{\psi}(\tilde{x}) \iff \frac{d^2 \tilde{\psi}(\tilde{x}) }{d\tilde{x}^2} = (\tilde{x}^2 - \tilde{E}) \tilde{\psi}(\tilde{x})  \end{equation} I'm looking for a solution of the form: \begin{equation} 	\tilde{\psi}(\tilde{x}) = \tilde{x}^{k} e^{-\frac{\tilde{x}^2}{2}} = h(\tilde{x})e^{-\frac{\tilde{x}^2}{2}} \end{equation} which, when substituted in (1) leads to the Hermite differential equation: \begin{equation} 	\frac{d^{2}h}{d\tilde{x}^2} - 2\tilde{x}\frac{dh}{d\tilde{x}} + (\tilde{E} - 1)h = 0 \end{equation} I'm trying to construct a power series solution $h = \sum_{k = 0}^{\infty} a_{k} \tilde{x}^{k}$ . Substituting back in the equation: \begin{equation} 	\sum_{k = 2}^{\infty} k (k - 1) a_{k} \tilde{x}^{k-2} - 2\tilde{x}\sum_{k = 1}^{\infty} k a_{k} \tilde{x}^{k-1} +  (\tilde{E} - 1)\sum_{k = 0}^{\infty} a_{k} \tilde{x}^{k} = 0 \end{equation} After a shift on the first sum and peeling of the initial term of the first and the last sum: \begin{equation} 	2a_{0} + \sum_{k = 1}^{\infty} (k + 2)(k + 1) a_{k+2} \tilde{x}^{k} - 2\sum_{k = 1}^{\infty} k a_{k} \tilde{x}^{k} + a_{0}(\tilde{E} - 1) + (\tilde{E} - 1)\sum_{k = 1}^{\infty} a_{k} \tilde{x}^{k} = 0 \end{equation} Powers and indexes match, so: \begin{equation} 	2a_{0} + a_{0}(\tilde{E} - 1) + \sum_{k = 1}^{\infty}[(k + 2)(k + 1) a_{k+2} - (2k - \tilde{E} + 1) a_{k} ]\tilde{x}^{k} = 0 \end{equation} Finally, all the coefficients must be equal to zero, so: \begin{equation} a_{k+2} = \frac{(2k - \tilde{E} + 1)}{(k + 2)(k + 1)} a_{k} \end{equation} First (smaller) problem. From the above, $\tilde{E} = 2k + 1$ . But from the peeled of terms $a_{0}(\tilde{E} + 1) = 0$ . What should I do with the second relation? Second (main) problem. I expect that by solving the recurrence relation I should somehow deduce that the solution, $h(\tilde{x})$ , are the Hermite polynomials, however, I have no idea how to do that. Could you please suggest how to do that? 1.The unit-free time-independent 1D.","As a part of solving the Schr√∂dinger equation 1 for quantum Harmonic oscillator: I'm looking for a solution of the form: which, when substituted in (1) leads to the Hermite differential equation: I'm trying to construct a power series solution . Substituting back in the equation: After a shift on the first sum and peeling of the initial term of the first and the last sum: Powers and indexes match, so: Finally, all the coefficients must be equal to zero, so: First (smaller) problem. From the above, . But from the peeled of terms . What should I do with the second relation? Second (main) problem. I expect that by solving the recurrence relation I should somehow deduce that the solution, , are the Hermite polynomials, however, I have no idea how to do that. Could you please suggest how to do that? 1.The unit-free time-independent 1D.","\begin{equation}\tag{1}
	\left(-\frac{d^2}{d\tilde{x}^2} + \tilde{x}^2\right) \tilde{\psi}(\tilde{x}) = \tilde{E} \tilde{\psi}(\tilde{x}) \iff \frac{d^2 \tilde{\psi}(\tilde{x}) }{d\tilde{x}^2} = (\tilde{x}^2 - \tilde{E}) \tilde{\psi}(\tilde{x}) 
\end{equation} \begin{equation}
	\tilde{\psi}(\tilde{x}) = \tilde{x}^{k} e^{-\frac{\tilde{x}^2}{2}} = h(\tilde{x})e^{-\frac{\tilde{x}^2}{2}}
\end{equation} \begin{equation}
	\frac{d^{2}h}{d\tilde{x}^2} - 2\tilde{x}\frac{dh}{d\tilde{x}} + (\tilde{E} - 1)h = 0
\end{equation} h = \sum_{k = 0}^{\infty} a_{k} \tilde{x}^{k} \begin{equation}
	\sum_{k = 2}^{\infty} k (k - 1) a_{k} \tilde{x}^{k-2} - 2\tilde{x}\sum_{k = 1}^{\infty} k a_{k} \tilde{x}^{k-1} +  (\tilde{E} - 1)\sum_{k = 0}^{\infty} a_{k} \tilde{x}^{k} = 0
\end{equation} \begin{equation}
	2a_{0} + \sum_{k = 1}^{\infty} (k + 2)(k + 1) a_{k+2} \tilde{x}^{k} - 2\sum_{k = 1}^{\infty} k a_{k} \tilde{x}^{k} + a_{0}(\tilde{E} - 1) + (\tilde{E} - 1)\sum_{k = 1}^{\infty} a_{k} \tilde{x}^{k} = 0
\end{equation} \begin{equation}
	2a_{0} + a_{0}(\tilde{E} - 1) + \sum_{k = 1}^{\infty}[(k + 2)(k + 1) a_{k+2} - (2k - \tilde{E} + 1) a_{k} ]\tilde{x}^{k} = 0
\end{equation} \begin{equation}
a_{k+2} = \frac{(2k - \tilde{E} + 1)}{(k + 2)(k + 1)} a_{k}
\end{equation} \tilde{E} = 2k + 1 a_{0}(\tilde{E} + 1) = 0 h(\tilde{x})","['ordinary-differential-equations', 'recurrence-relations', 'power-series']"
4,"Asymptotic for $y'' + \frac{\epsilon y'}{y^2} - y' = 0$, $y(-\infty) = 1$, $y(+\infty) = \epsilon$.","Asymptotic for , , .",y'' + \frac{\epsilon y'}{y^2} - y' = 0 y(-\infty) = 1 y(+\infty) = \epsilon,"Asymptotic for $y'' + \frac{\epsilon y'}{y^2} - y' = 0$, $y(-\infty) = 1$, $y(+\infty) = \epsilon$. I started with a regular expansion for  $$y^2y'' + \epsilon y' - y^2 y' = 0$$ and  $$y = y_0 + \epsilon y_1 + O(\epsilon^2)$$  with $y_0(-\infty) = 1$, $y_0(+\infty) = 0$ and $y_1(-\infty) = 0$, $y_1 (+\infty) = 1$. The zero order ODE is  $$ y_0'' -  y_0' = 0, $$ this gives $y_0 = Ae^{x} + B$, but I can this can not satisfy the boundary condition. How should I approach differently? So this problem is a singular perturbation problem. We can not use WKB because this is not linear, and I am not familar with explicit boundary layer calculation on an unbounded domain.","Asymptotic for $y'' + \frac{\epsilon y'}{y^2} - y' = 0$, $y(-\infty) = 1$, $y(+\infty) = \epsilon$. I started with a regular expansion for  $$y^2y'' + \epsilon y' - y^2 y' = 0$$ and  $$y = y_0 + \epsilon y_1 + O(\epsilon^2)$$  with $y_0(-\infty) = 1$, $y_0(+\infty) = 0$ and $y_1(-\infty) = 0$, $y_1 (+\infty) = 1$. The zero order ODE is  $$ y_0'' -  y_0' = 0, $$ this gives $y_0 = Ae^{x} + B$, but I can this can not satisfy the boundary condition. How should I approach differently? So this problem is a singular perturbation problem. We can not use WKB because this is not linear, and I am not familar with explicit boundary layer calculation on an unbounded domain.",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
5,Existence of Global Defining Function for Hypersurface,Existence of Global Defining Function for Hypersurface,,"Let $M$ be a smooth manifold, and $\Sigma$ a hypersurface of $M$. (That is, $\Sigma$ is smoothly a embedded subset of $M$ with codimension $1$.) By a defining function for $\Sigma$, we mean some $f \in C^\infty(M)$ such that $\Sigma = \{p: f(p) = 0\}$, and for any $s\in \Sigma$, $\nabla f(s) \neq 0$. Suppose $\Sigma$ admits a global unit normal vector field. Using the inverse function theorem for $\mathbb R^n$, I can show the existence of local defining functions, moreover I can then construct local defining functions satisfying $\nabla f(s) = n(s)$ for $s \in \Sigma$. Therefore, functions that are ""defined locally on points nearby"" will be ""similar to first order"". However, it is not true that we can simply stitch them together to form a global defining function, since there is no guarantee that the functions actually agree on their overlaps. Question: Does the existence of a global unit normal vector field ensure the existence of a global defining function? On second thought, every hypersurface of $\mathbb R^n$ should admit global unit normal vector fields. I usually do differential geometry in indefinite signatures where this isn't true in general.","Let $M$ be a smooth manifold, and $\Sigma$ a hypersurface of $M$. (That is, $\Sigma$ is smoothly a embedded subset of $M$ with codimension $1$.) By a defining function for $\Sigma$, we mean some $f \in C^\infty(M)$ such that $\Sigma = \{p: f(p) = 0\}$, and for any $s\in \Sigma$, $\nabla f(s) \neq 0$. Suppose $\Sigma$ admits a global unit normal vector field. Using the inverse function theorem for $\mathbb R^n$, I can show the existence of local defining functions, moreover I can then construct local defining functions satisfying $\nabla f(s) = n(s)$ for $s \in \Sigma$. Therefore, functions that are ""defined locally on points nearby"" will be ""similar to first order"". However, it is not true that we can simply stitch them together to form a global defining function, since there is no guarantee that the functions actually agree on their overlaps. Question: Does the existence of a global unit normal vector field ensure the existence of a global defining function? On second thought, every hypersurface of $\mathbb R^n$ should admit global unit normal vector fields. I usually do differential geometry in indefinite signatures where this isn't true in general.",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'smooth-manifolds', 'submanifold']"
6,Can a hyperbolic fixed point be inside a homoclinic loop on a plane?,Can a hyperbolic fixed point be inside a homoclinic loop on a plane?,,"Consider a system of odes on a plane. It is obvious that a fixed point of centre-type stability (imaginary e-values) can exist inside a homoclinic loop to a hyperbolic saddle fixed point. For example, take the Hamiltonian $H(x,y) = \frac{1}{2}(y^{2}-x^{2}) + x^{3}$. Here the hyperbolic saddle point at origin has a ""fish"" shaped homoclinic loop enclosing the centre fixed point at $(1/3, 0)$. What I'm wondering is whether a homoclinic loop can enclose fixed points other than centre type, i.e. can a ""sink / source"" or a ""saddle"" exist inside a homoclinic loop? And can this situation be generalised to $\mathbb{R}^{n}$? I cannot think of a way to either prove / disprove this. I suspect Poincare-Bendixson and index theory could be used, but not sure how.","Consider a system of odes on a plane. It is obvious that a fixed point of centre-type stability (imaginary e-values) can exist inside a homoclinic loop to a hyperbolic saddle fixed point. For example, take the Hamiltonian $H(x,y) = \frac{1}{2}(y^{2}-x^{2}) + x^{3}$. Here the hyperbolic saddle point at origin has a ""fish"" shaped homoclinic loop enclosing the centre fixed point at $(1/3, 0)$. What I'm wondering is whether a homoclinic loop can enclose fixed points other than centre type, i.e. can a ""sink / source"" or a ""saddle"" exist inside a homoclinic loop? And can this situation be generalised to $\mathbb{R}^{n}$? I cannot think of a way to either prove / disprove this. I suspect Poincare-Bendixson and index theory could be used, but not sure how.",,"['ordinary-differential-equations', 'dynamical-systems', 'classical-mechanics', 'ergodic-theory', 'bifurcation']"
7,Solving $ y'' - y' = -3$ via undetermined coefficients.,Solving  via undetermined coefficients., y'' - y' = -3,"I stumbled on a problem when solving this ODE. The method of undetermined coefficients say that I can suppose the solution in the form $y_p = k$. Then: $(k)''-(k)'=-3 \Rightarrow 0=3$, which is absurd. When I suppose the solution in the form $kx$ it works, and I find the particular solution $3x$, but why I need to suppose a particular solution in a degree 1 if the RHS of the ODE is of degree 0?","I stumbled on a problem when solving this ODE. The method of undetermined coefficients say that I can suppose the solution in the form $y_p = k$. Then: $(k)''-(k)'=-3 \Rightarrow 0=3$, which is absurd. When I suppose the solution in the form $kx$ it works, and I find the particular solution $3x$, but why I need to suppose a particular solution in a degree 1 if the RHS of the ODE is of degree 0?",,[]
8,How do I solve this specific ordinary differential equation?,How do I solve this specific ordinary differential equation?,,"I have no clue how to solve this ode , I've been watching videos, searching online, but I just can't, someone please help me. $$2xyy' = x^2 + xy$$","I have no clue how to solve this ode , I've been watching videos, searching online, but I just can't, someone please help me. $$2xyy' = x^2 + xy$$",,"['calculus', 'ordinary-differential-equations', 'analysis']"
9,Another attempt at solving a PDE with the method of characteristics,Another attempt at solving a PDE with the method of characteristics,,"I want to use the method of characteristics to obtain the solution to this PDE, $$\frac{\partial F}{\partial t}=\left(z-t\right)\left(\beta z-\gamma\right)\frac{\partial F}{\partial z}$$ which I've seen is of the form $$F\left(t,z\right)=F\left(\left(\frac{\beta\left(z-1\right)}{\beta z-\gamma}\right)e^{\left(\beta-\gamma\right)t}\right).$$ Attempt: I was trying by identifying the PDE with one of the form $$a\left(t,z\right)F_{z}+b\left(t,z\right)F_{t}=g\left(t,z,F\right),$$ deriving the solution $h\left(t,z\right)=c_{1}$ for $$\frac{dz}{dt}=\frac{a\left(t,z\right)}{b\left(t,z\right)}$$ then the solution $j\left(t,z,F\right)=c_{2}$ for $$\frac{dF}{dt}=\frac{g\left(t,z,F\right)}{b\left(t,z\right)}$$ and writting $j\left(t,z,F\right)=K\left(h\left(t,z\right)\right)$. However, I got stuck when I stomped upon the following ODE to solve, $$z'\left(t\right)=-\beta z^{2}\left(t\right)+\beta tz\left(t\right)+\gamma z\left(t\right)-\gamma t.$$ How can I solve the problem?","I want to use the method of characteristics to obtain the solution to this PDE, $$\frac{\partial F}{\partial t}=\left(z-t\right)\left(\beta z-\gamma\right)\frac{\partial F}{\partial z}$$ which I've seen is of the form $$F\left(t,z\right)=F\left(\left(\frac{\beta\left(z-1\right)}{\beta z-\gamma}\right)e^{\left(\beta-\gamma\right)t}\right).$$ Attempt: I was trying by identifying the PDE with one of the form $$a\left(t,z\right)F_{z}+b\left(t,z\right)F_{t}=g\left(t,z,F\right),$$ deriving the solution $h\left(t,z\right)=c_{1}$ for $$\frac{dz}{dt}=\frac{a\left(t,z\right)}{b\left(t,z\right)}$$ then the solution $j\left(t,z,F\right)=c_{2}$ for $$\frac{dF}{dt}=\frac{g\left(t,z,F\right)}{b\left(t,z\right)}$$ and writting $j\left(t,z,F\right)=K\left(h\left(t,z\right)\right)$. However, I got stuck when I stomped upon the following ODE to solve, $$z'\left(t\right)=-\beta z^{2}\left(t\right)+\beta tz\left(t\right)+\gamma z\left(t\right)-\gamma t.$$ How can I solve the problem?",,"['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
10,Writing Differential equations to describe a system,Writing Differential equations to describe a system,,"I have a system that models the interaction between a pathogen an the immune response. If $P$ is the pathogen and $I$ represents immune response, the differential equations of the system are: $$ \begin{alignat*}{1} {\mathrm{d}P\over \mathrm{d}t} &= r_1P \left(1-{P\over k}\right)-d_1P \left({I\over I+\sigma}\right)\\ {\mathrm{d}I\over \mathrm{d}t} &= r_2I \left({P \over P+\sigma_2}\right)-d_2I \end{alignat*} $$ This is somewhat similar to this article . $r_i$, $k$, and $\sigma$ are constants. $\sigma$¬†represents the pathogen density when the immune response is at half its maximal capacity. $d_i$¬†is the killing rate through immune response. I want to change these equations so that some of the pathogens that interact with the immune response do not get killed. So I want it to be modelled such that immune-system cells will engulf pathogens, but a portion of pathogens can survive within the immune-system cells and will not get killed. If $P_S$ represents the population that survives and if $\alpha$ the proportion of the pathogen that interacts with the immune system and can survive killing, if I change the equations in the following way will it be correct? $$ \begin{alignat*}{1} {\mathrm{d}P\over \mathrm{d}t} &= r_1P\left(1-{P\over k}\right)-(1-\alpha)d_1P\left({I\over I+\sigma}\right)\\ {\mathrm{d}I\over \mathrm{d}t} &= r_2I\left({P \over P+\sigma_2}\right)-d_2I  \\ {\mathrm{d}P_s\over \mathrm{d}t} &= \alpha d_1 P\left({I\over I+\sigma}\right)    \end{alignat*} $$ But I am not sure if I should use the death rate $d_1$ for the surviving population $P_s$? Or is there any other way to show that a proportion of the pathogens that interact with the immune response move into a different compartment that includes surviving pathogen?","I have a system that models the interaction between a pathogen an the immune response. If $P$ is the pathogen and $I$ represents immune response, the differential equations of the system are: $$ \begin{alignat*}{1} {\mathrm{d}P\over \mathrm{d}t} &= r_1P \left(1-{P\over k}\right)-d_1P \left({I\over I+\sigma}\right)\\ {\mathrm{d}I\over \mathrm{d}t} &= r_2I \left({P \over P+\sigma_2}\right)-d_2I \end{alignat*} $$ This is somewhat similar to this article . $r_i$, $k$, and $\sigma$ are constants. $\sigma$¬†represents the pathogen density when the immune response is at half its maximal capacity. $d_i$¬†is the killing rate through immune response. I want to change these equations so that some of the pathogens that interact with the immune response do not get killed. So I want it to be modelled such that immune-system cells will engulf pathogens, but a portion of pathogens can survive within the immune-system cells and will not get killed. If $P_S$ represents the population that survives and if $\alpha$ the proportion of the pathogen that interacts with the immune system and can survive killing, if I change the equations in the following way will it be correct? $$ \begin{alignat*}{1} {\mathrm{d}P\over \mathrm{d}t} &= r_1P\left(1-{P\over k}\right)-(1-\alpha)d_1P\left({I\over I+\sigma}\right)\\ {\mathrm{d}I\over \mathrm{d}t} &= r_2I\left({P \over P+\sigma_2}\right)-d_2I  \\ {\mathrm{d}P_s\over \mathrm{d}t} &= \alpha d_1 P\left({I\over I+\sigma}\right)    \end{alignat*} $$ But I am not sure if I should use the death rate $d_1$ for the surviving population $P_s$? Or is there any other way to show that a proportion of the pathogens that interact with the immune response move into a different compartment that includes surviving pathogen?",,"['ordinary-differential-equations', 'dynamical-systems', 'mathematical-modeling']"
11,Stability of a fixed point in a nonlinear system with no linear part,Stability of a fixed point in a nonlinear system with no linear part,,"The following nonlinear system has a fixed point in the origin. I want to know if this fixed point is stable: \begin{align*} &\dot{\alpha}=\alpha^2-2\beta(\alpha+\beta)\\ &\dot{\beta}=\beta^2-2\alpha(\alpha+\beta) \end{align*} Note that the linearization of the system leads to $\dot{\alpha}=\dot{\beta}=0$ to first order, so this method is useless. To find out the stability, I wrote this equations as: \begin{align*} \dot{\alpha}=\left(\alpha,\beta\right) \cdot A\cdot \left(\begin{array}{c} \alpha\\ \beta \end{array}\right),\quad A=\left(\begin{array}{cc} 1 & -1\\ -1 & -2 \end{array}\right) \end{align*} \begin{align*} \dot{\beta}=\left(\alpha,\beta\right) \cdot B\cdot \left(\begin{array}{c} \alpha\\ \beta \end{array}\right),\quad B=\left(\begin{array}{cc} -2 & -1\\ -1 & 1 \end{array}\right) \end{align*} Since $A$ and $B$ are real and symmetric, they are diagonalizable, with real eigenvalues. It turns out that $A$ and $B$ have one eigenvalue positive and the other negative (in fact, $A$ and $B$ have the same eigenvalues), explicitly: \begin{align*} &\text{Eigenvalues of $A$:}\quad \frac{1}{2} \left(-1-\sqrt{13}\right),\frac{1}{2} \left(\sqrt{13}-1\right)\\ &\text{Eigenvalues of $B$:}\quad \text{same of $A$} \end{align*} If, for example, the eigenvalues of $A$ were both positive, then $\dot{\alpha}$ would be positive for all points except origin, and then the origin would be unstable (because trajectories starting out of the origin would increase their $\alpha$'s to infinity). But when both $\dot{\alpha}$ and $\dot{\beta}$ are positive or negative depending on the point $(\alpha,\beta)$, as in this case, how can you determine the stability of the fixed point? EDIT: The phase portrait is:","The following nonlinear system has a fixed point in the origin. I want to know if this fixed point is stable: \begin{align*} &\dot{\alpha}=\alpha^2-2\beta(\alpha+\beta)\\ &\dot{\beta}=\beta^2-2\alpha(\alpha+\beta) \end{align*} Note that the linearization of the system leads to $\dot{\alpha}=\dot{\beta}=0$ to first order, so this method is useless. To find out the stability, I wrote this equations as: \begin{align*} \dot{\alpha}=\left(\alpha,\beta\right) \cdot A\cdot \left(\begin{array}{c} \alpha\\ \beta \end{array}\right),\quad A=\left(\begin{array}{cc} 1 & -1\\ -1 & -2 \end{array}\right) \end{align*} \begin{align*} \dot{\beta}=\left(\alpha,\beta\right) \cdot B\cdot \left(\begin{array}{c} \alpha\\ \beta \end{array}\right),\quad B=\left(\begin{array}{cc} -2 & -1\\ -1 & 1 \end{array}\right) \end{align*} Since $A$ and $B$ are real and symmetric, they are diagonalizable, with real eigenvalues. It turns out that $A$ and $B$ have one eigenvalue positive and the other negative (in fact, $A$ and $B$ have the same eigenvalues), explicitly: \begin{align*} &\text{Eigenvalues of $A$:}\quad \frac{1}{2} \left(-1-\sqrt{13}\right),\frac{1}{2} \left(\sqrt{13}-1\right)\\ &\text{Eigenvalues of $B$:}\quad \text{same of $A$} \end{align*} If, for example, the eigenvalues of $A$ were both positive, then $\dot{\alpha}$ would be positive for all points except origin, and then the origin would be unstable (because trajectories starting out of the origin would increase their $\alpha$'s to infinity). But when both $\dot{\alpha}$ and $\dot{\beta}$ are positive or negative depending on the point $(\alpha,\beta)$, as in this case, how can you determine the stability of the fixed point? EDIT: The phase portrait is:",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
12,How to solve this integro-differential equation?,How to solve this integro-differential equation?,,"I came across this integro-differential equation to solve $$\frac{du(x;t)}{dt}=-\lambda\int_0^xu(\xi;t)\;d\xi\tag{1}$$ under the initial condition $u(x;0)=f(x)$ where $x$ is a parameter, $\lambda$ is a constant, and $0<t<\infty$. My first thought is that I can just directly integrate the equation to obtain $$u(x;t)=-\lambda\int_0^t\int_0^xu(\xi;\tau)\;d\xi\,d\tau\;.\tag{2}$$ This equation is highly implicit and an explicit expression is desired. So then I thought of using Laplace transforms instead. Let $U(\cdot)$ be the Laplace transform of $u(\cdot)$, and $s$ be the complex co-variable of the real variable $t$, then $$s\,U(x;s)-f(x)=-\lambda\,\int_0^xU(\xi;s)\,d\xi\tag{3}$$ which can be converted to the differential equation $$s\,U'(x;s)+\lambda\,U(x;s)=f'(x)\tag{4}$$ where the derivative is now with respect to $x$. Equation $(4)$ is easily solvable. Though I am uncertain if I did the following Laplace transform correctly, $$\mathscr{L}\left\{\int_0^xu(\xi;t)\,d\xi\right\}=\int_0^xU(\xi;s)\,d\xi\;.\tag{5}$$ I figured since the integration is over the parameter instead of the transforming variable I could bring it into the integral under the heuristic that the Laplace transform of a sum is the sum of the Laplace transforms, but I am unsure of this. Any insight on any of this or alternative methods to solve Equation $(1)$ is welcome.","I came across this integro-differential equation to solve $$\frac{du(x;t)}{dt}=-\lambda\int_0^xu(\xi;t)\;d\xi\tag{1}$$ under the initial condition $u(x;0)=f(x)$ where $x$ is a parameter, $\lambda$ is a constant, and $0<t<\infty$. My first thought is that I can just directly integrate the equation to obtain $$u(x;t)=-\lambda\int_0^t\int_0^xu(\xi;\tau)\;d\xi\,d\tau\;.\tag{2}$$ This equation is highly implicit and an explicit expression is desired. So then I thought of using Laplace transforms instead. Let $U(\cdot)$ be the Laplace transform of $u(\cdot)$, and $s$ be the complex co-variable of the real variable $t$, then $$s\,U(x;s)-f(x)=-\lambda\,\int_0^xU(\xi;s)\,d\xi\tag{3}$$ which can be converted to the differential equation $$s\,U'(x;s)+\lambda\,U(x;s)=f'(x)\tag{4}$$ where the derivative is now with respect to $x$. Equation $(4)$ is easily solvable. Though I am uncertain if I did the following Laplace transform correctly, $$\mathscr{L}\left\{\int_0^xu(\xi;t)\,d\xi\right\}=\int_0^xU(\xi;s)\,d\xi\;.\tag{5}$$ I figured since the integration is over the parameter instead of the transforming variable I could bring it into the integral under the heuristic that the Laplace transform of a sum is the sum of the Laplace transforms, but I am unsure of this. Any insight on any of this or alternative methods to solve Equation $(1)$ is welcome.",,"['ordinary-differential-equations', 'laplace-transform', 'integro-differential-equations']"
13,Hermite Polynomials (different forms of writing them),Hermite Polynomials (different forms of writing them),,"I was seeing this question in which 2 definitions of the Hermite polynomials are given: $$H_n(x) = (-1)^ne^{x^2}\frac{d^n}{dx^n}e^{-x^2}$$ and $$ H_n(x) = \left( 2x - \frac{d}{dx} \right)^n(1) $$ Now, I found another definition: $$ H_n(x) = e^{x^2/2}\left(x-\frac{d}{dx}\right)^n e^{-x^2/2} $$ There is plenty of literature on the first two definitions, but I have found none on the third one. I would like to know if there is a way of deducing the first or second definitions from the third one. I tried expanding in a binomial expansion but got nowhere. I appreciate your help. Or is there any way of showing that the last expression matches the coefficients of the generating function expansion?","I was seeing this question in which 2 definitions of the Hermite polynomials are given: $$H_n(x) = (-1)^ne^{x^2}\frac{d^n}{dx^n}e^{-x^2}$$ and $$ H_n(x) = \left( 2x - \frac{d}{dx} \right)^n(1) $$ Now, I found another definition: $$ H_n(x) = e^{x^2/2}\left(x-\frac{d}{dx}\right)^n e^{-x^2/2} $$ There is plenty of literature on the first two definitions, but I have found none on the third one. I would like to know if there is a way of deducing the first or second definitions from the third one. I tried expanding in a binomial expansion but got nowhere. I appreciate your help. Or is there any way of showing that the last expression matches the coefficients of the generating function expansion?",,"['ordinary-differential-equations', 'polynomials', 'hermite-polynomials']"
14,"Is a fixed point, which is attracting on a manifold $M$, always globally stable if $M$ is globally attracting?","Is a fixed point, which is attracting on a manifold , always globally stable if  is globally attracting?",M M,"Lets start with a toy example system of equations \begin{align} \frac{dx}{dt}&=x(1-x)-xy\\ \frac{dy}{dt}&=-y \end{align} I'd like to show $(1,0)$ has a basin of attraction that includes all of $B:=\{(x,y): 0<x<1, y\geq 0 \}$. The x-axis is obviously attracting on $B$, given $dy/dt=-y$. And on the x-axis, the point x=1 attracts all positive $x$. How can we prove that all trajectories with initial conditions in $B$ flow to $(1,0)$. And does this concept generalize to systems as follows? The general question in 2D, given a dynamical system \begin{align} \frac{dx}{dt}=f(x,y)\\ \frac{dy}{dt}=g(x,y) \end{align} with a globally attracting curve $C:=\{(x,y): y=h(x)\}$, such that all points $(x,y)$ not on $C$ approach $C$. My question is, if there exists a unique fixed point $(x^*,y^*)$ on $C$ such that it attracts all points on the curve $C$, can we call that fixed point globally stable. Intuitively it seems like it has to be. I'm wondering if anyone has a proof for this proposition or a counterexample?","Lets start with a toy example system of equations \begin{align} \frac{dx}{dt}&=x(1-x)-xy\\ \frac{dy}{dt}&=-y \end{align} I'd like to show $(1,0)$ has a basin of attraction that includes all of $B:=\{(x,y): 0<x<1, y\geq 0 \}$. The x-axis is obviously attracting on $B$, given $dy/dt=-y$. And on the x-axis, the point x=1 attracts all positive $x$. How can we prove that all trajectories with initial conditions in $B$ flow to $(1,0)$. And does this concept generalize to systems as follows? The general question in 2D, given a dynamical system \begin{align} \frac{dx}{dt}=f(x,y)\\ \frac{dy}{dt}=g(x,y) \end{align} with a globally attracting curve $C:=\{(x,y): y=h(x)\}$, such that all points $(x,y)$ not on $C$ approach $C$. My question is, if there exists a unique fixed point $(x^*,y^*)$ on $C$ such that it attracts all points on the curve $C$, can we call that fixed point globally stable. Intuitively it seems like it has to be. I'm wondering if anyone has a proof for this proposition or a counterexample?",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'stability-in-odes']"
15,"Seemingly simple differential equation, $y'=(4x+y)/(x+4y)$","Seemingly simple differential equation,",y'=(4x+y)/(x+4y),"My friend has asked me for help solving the following differential equation (of which the explicit solutions are supposedly derivable): $$\frac{dy}{dx}=\frac{4x+y}{x+4y}\tag{1}$$ I have tried hitting it with every technique I know, but none of my efforts have proved fruitful. Could you help me out? All I can end up with is a complicated implicit solution. I will now outline every approach I have taken. (Note: I have tagged every equation for referencing convenience). [Homogeneous Substitution] This equation is manifestly homogeneous when put in the following form: $$\frac{dy}{dx}=\frac{4+\frac{y}{x}}{1+4\frac{y}{x}}\tag{2}$$ So let's try the substitution $u=y/x$. We can work out that $y'=xu'+u$, so plugging everything in gives us: $$xu'+u=\frac{4+u}{1+4u}\tag{3}$$ $$\implies \frac{u'}{\frac{4+u}{1+4u}-u}=\frac{1}{x}\tag{4}$$ I can solve this by integrating both sides with respect to $x$ (left side requires heavy algebraic massaging): $$\ln \left|(1-u)^{-5/8}(1+u)^{-3/8}\right|=\ln |x| +C\tag{5}$$ After re-substituting $u=y/x$, this then simplifies down to $$(x-y)^5(x+y)^3=C\tag{6}$$ So the solution seems to be the solution to a eight-order polynomial, which I'm not sure can be solved for explicitly (well, it should be because why else would it show up on an entry-level DFQ homework assignment?). On top of that, a few of my previous steps have implicitly imposed domain restraints on my solution (e.g. every step where I divide by a quantity that could be zero). I've plotted the solution below, where I have taken note of the homogeneity of the original equation (the case where $C=0$ is special though - then the solutions are just $y=\pm x$). [Integrating Factor - Exact Equation] Ok, maybe the exact form of the solution is buried in that polynomial. Let's see if this can be made exact. Putting it in the standard form gives us: $$f(x,y)dx+g(x,y)dy=0,$$ $$~~\textrm{where}~ f(x,y)=(4x+y)~~\textrm{and}~~ g(x,y)=-(x+4y)\tag{7}$$ This isn't an exact equation by itself ($f_y\neq g_x$). Moreover, no integrating factor of one variable (either $x$ or $y$) will work because calculating the integrating factor involves calculating an integral like: $$\int \frac{\frac{\partial f}{\partial y}-\frac{\partial g}{\partial x}}{g(x,y)}dx\tag{8}$$ and we can't do that integral explicitly because the numerator is a constant for the specific $f(x,y)$ and $g(x,y)$ in this problem, while the denominator is a full function of $x$ and $y$. So this seems to be a no go. [Laplace/D'Alembert Equation Form] A [Laplace or d'Alembert equation][2] (no, not the Laplace's equation or d'Alembert's solution/formula) is first-order ordinary differential equation of the type $$y=xf(y')+g(y')\tag{9}$$ which can be morphed into the simpler linear equation: $$\frac{dx}{d(y')}=\left(\frac{f'(y')}{y'-f(y')}\right)x+\frac{g'(y')}{y'-f(y')}\tag{10}$$ I have found that I can manipulate the original differential equation into the desired form: $$y=\left(\frac{y'-4}{1-4y'}\right)x\tag{11}$$ from which we can read off (well, calculate really) the corresponding differential equation for $x(y')$: $$\frac{dx}{d(y')}=-\frac{15x}{4(1-y'^2)(1-4y')}\tag{12}$$ This is separable. Solving this gives me: $$\ln \left|\frac{(1-y')^{5/8}(1+y')^{3/8}}{4y'-1}\right|=\ln |x| +C\tag{13}$$ this seems to be the most highly nonlinear and implicit first-order differential equation that I have ever seen. I don't know what I could realistically do from here. ~~ADDENDUM~~ These are the exact instructions from the assignment $$\textrm{1. Find all solutions:}~~~~\frac{dy}{dx}=\frac{4x+y}{x+4y} $$ https://en.wikibooks.org/wiki/Ordinary_Differential_Equations/d%27Alembert","My friend has asked me for help solving the following differential equation (of which the explicit solutions are supposedly derivable): $$\frac{dy}{dx}=\frac{4x+y}{x+4y}\tag{1}$$ I have tried hitting it with every technique I know, but none of my efforts have proved fruitful. Could you help me out? All I can end up with is a complicated implicit solution. I will now outline every approach I have taken. (Note: I have tagged every equation for referencing convenience). [Homogeneous Substitution] This equation is manifestly homogeneous when put in the following form: $$\frac{dy}{dx}=\frac{4+\frac{y}{x}}{1+4\frac{y}{x}}\tag{2}$$ So let's try the substitution $u=y/x$. We can work out that $y'=xu'+u$, so plugging everything in gives us: $$xu'+u=\frac{4+u}{1+4u}\tag{3}$$ $$\implies \frac{u'}{\frac{4+u}{1+4u}-u}=\frac{1}{x}\tag{4}$$ I can solve this by integrating both sides with respect to $x$ (left side requires heavy algebraic massaging): $$\ln \left|(1-u)^{-5/8}(1+u)^{-3/8}\right|=\ln |x| +C\tag{5}$$ After re-substituting $u=y/x$, this then simplifies down to $$(x-y)^5(x+y)^3=C\tag{6}$$ So the solution seems to be the solution to a eight-order polynomial, which I'm not sure can be solved for explicitly (well, it should be because why else would it show up on an entry-level DFQ homework assignment?). On top of that, a few of my previous steps have implicitly imposed domain restraints on my solution (e.g. every step where I divide by a quantity that could be zero). I've plotted the solution below, where I have taken note of the homogeneity of the original equation (the case where $C=0$ is special though - then the solutions are just $y=\pm x$). [Integrating Factor - Exact Equation] Ok, maybe the exact form of the solution is buried in that polynomial. Let's see if this can be made exact. Putting it in the standard form gives us: $$f(x,y)dx+g(x,y)dy=0,$$ $$~~\textrm{where}~ f(x,y)=(4x+y)~~\textrm{and}~~ g(x,y)=-(x+4y)\tag{7}$$ This isn't an exact equation by itself ($f_y\neq g_x$). Moreover, no integrating factor of one variable (either $x$ or $y$) will work because calculating the integrating factor involves calculating an integral like: $$\int \frac{\frac{\partial f}{\partial y}-\frac{\partial g}{\partial x}}{g(x,y)}dx\tag{8}$$ and we can't do that integral explicitly because the numerator is a constant for the specific $f(x,y)$ and $g(x,y)$ in this problem, while the denominator is a full function of $x$ and $y$. So this seems to be a no go. [Laplace/D'Alembert Equation Form] A [Laplace or d'Alembert equation][2] (no, not the Laplace's equation or d'Alembert's solution/formula) is first-order ordinary differential equation of the type $$y=xf(y')+g(y')\tag{9}$$ which can be morphed into the simpler linear equation: $$\frac{dx}{d(y')}=\left(\frac{f'(y')}{y'-f(y')}\right)x+\frac{g'(y')}{y'-f(y')}\tag{10}$$ I have found that I can manipulate the original differential equation into the desired form: $$y=\left(\frac{y'-4}{1-4y'}\right)x\tag{11}$$ from which we can read off (well, calculate really) the corresponding differential equation for $x(y')$: $$\frac{dx}{d(y')}=-\frac{15x}{4(1-y'^2)(1-4y')}\tag{12}$$ This is separable. Solving this gives me: $$\ln \left|\frac{(1-y')^{5/8}(1+y')^{3/8}}{4y'-1}\right|=\ln |x| +C\tag{13}$$ this seems to be the most highly nonlinear and implicit first-order differential equation that I have ever seen. I don't know what I could realistically do from here. ~~ADDENDUM~~ These are the exact instructions from the assignment $$\textrm{1. Find all solutions:}~~~~\frac{dy}{dx}=\frac{4x+y}{x+4y} $$ https://en.wikibooks.org/wiki/Ordinary_Differential_Equations/d%27Alembert",,"['calculus', 'ordinary-differential-equations']"
16,Deriving Rodrigues' formula,Deriving Rodrigues' formula,,"Consider the function $$f_n(x)=(x^2-1)^n$$..........(20) Differentiating this equation we get the second order differential equation, $$(1-x^2)f_n''+2(n-1)xf_n'+2nf_n=0$$..................(22) We wish to differentiate this n times by use of Leibniz's formula, $$\frac{d^n}{dx^n}A(x)B(x)=\sum^n_{k=0}\frac{n!}{k!(n-k)!}\frac{d^kA}{dx^k}\frac{d^{n-k}B}{dx^{n-k}}$$......................(23) Applying this to (22) we easily get $$(1-x^2)f_n^{(n+2)}-2xf_n^{(n+1)}+n(n+1)f_n^{(n)}=0$$......................................(24) which is exactly Lergendre's differential equation (1-49). This equation is therefore satisfied by the polynomials $$y=\frac{d^n}{dx^n}(x^2-1)^n$$.....................(25) The Legendre polynomials $P_n(x)$ are normalized by the requirement $P_n(1)=1$. Using $$y=2^nn!$$...............(26) for x=1, We get $$P_n(x)=\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$$...........................(27) How does the author relate (24) with (25)? Where did he get equation (25)? And how do you do this normalization? Is this normalization the same as that in physics whereby we ensure that the probability does not exceed 1? This is the source .","Consider the function $$f_n(x)=(x^2-1)^n$$..........(20) Differentiating this equation we get the second order differential equation, $$(1-x^2)f_n''+2(n-1)xf_n'+2nf_n=0$$..................(22) We wish to differentiate this n times by use of Leibniz's formula, $$\frac{d^n}{dx^n}A(x)B(x)=\sum^n_{k=0}\frac{n!}{k!(n-k)!}\frac{d^kA}{dx^k}\frac{d^{n-k}B}{dx^{n-k}}$$......................(23) Applying this to (22) we easily get $$(1-x^2)f_n^{(n+2)}-2xf_n^{(n+1)}+n(n+1)f_n^{(n)}=0$$......................................(24) which is exactly Lergendre's differential equation (1-49). This equation is therefore satisfied by the polynomials $$y=\frac{d^n}{dx^n}(x^2-1)^n$$.....................(25) The Legendre polynomials $P_n(x)$ are normalized by the requirement $P_n(1)=1$. Using $$y=2^nn!$$...............(26) for x=1, We get $$P_n(x)=\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$$...........................(27) How does the author relate (24) with (25)? Where did he get equation (25)? And how do you do this normalization? Is this normalization the same as that in physics whereby we ensure that the probability does not exceed 1? This is the source .",,"['ordinary-differential-equations', 'derivatives', 'legendre-polynomials']"
17,"Determine a solution of an ODE by ""inspection""","Determine a solution of an ODE by ""inspection""",,"I'm checking Zill's A First Course in Differential Equations with Modeling Applications , and there's an exercise that says: From the following problems determine by inspection at least two solutions of the given IVP. $y'=3y^{2/3},\,y(0)=0$ $xy'=2y,\,y(0)=0$ I don't quite understand what in means by "" by inspection "", what's the difference between just finding the solutions and determine by inspection?","I'm checking Zill's A First Course in Differential Equations with Modeling Applications , and there's an exercise that says: From the following problems determine by inspection at least two solutions of the given IVP. $y'=3y^{2/3},\,y(0)=0$ $xy'=2y,\,y(0)=0$ I don't quite understand what in means by "" by inspection "", what's the difference between just finding the solutions and determine by inspection?",,['ordinary-differential-equations']
18,Deriving the Airy functions from first principles,Deriving the Airy functions from first principles,,"I have just started reading about the Airy functions and am stuck on a particular step of their derivation. But first here is some background information to give this question some meaning, more information can be found from a previous question of mine: The general solution to Bessel's differential equation: $$\fbox{$y^{\prime\prime}+\left(\frac{1-2a}{x}\right)y^{\prime}+\left[\left(bcx^{c-1}\right)^2+\frac{a^2-p^2c^2}{x^2}\right]y=0$}\tag{1}$$ is $$\fbox{$y=x^aZ_p\left(bx^c\right)$}\tag{2}$$ where $Z_p$ stands for $J_p$ or $N_p$ or any linear combination of them, and $a,b,c,p$ are constants. $J_p$ is called the Bessel function of the first kind of order $p$ and $N_p$ is any combination of $J_p$ and $J_{‚àíp}$: $$N_p(x)=\frac{\cos(\pi p)J_p(x)-J_{-p}(x)}{\sin(\pi p)}\tag{3}$$ So equation $(2)$ can be written as $$y=x^{a}\left[AJ_{p}\left(bx^c\right)+BN_{p}\left(bx^c\right)\right]\tag{4}$$ or $$y=x^{a}\left[AJ_{p}\left(bx^c\right)+B\left(\frac{\cos(\pi p)J_p\left(bx^c\right)-J_{-p}\left(bx^c\right)}{\sin(\pi p)}\right)\right]\tag{5}$$ The Airy differential equation is $$y^{\prime\prime}-xy=0\tag{6}$$ and has solution $$y=x^{1/2}Z_{1/3}\left(\frac23 ix^{3/2}\right)\tag{7}$$ By using $$I_p(x)=i^{-p}J_p(ix)\tag{8}$$   $$K_p(x)=\frac{\pi}{2}i^{p+1}\left[J_p(ix)+i\left(\frac{\cos(\pi p)J_p(ix)-J_{-p}(ix)}{\sin(\pi p)}\right)\right]\tag{9}$$ My objective is to show that $(7)$ can be written in terms of $I_{1/3}$ and $K_{1/3}$ to obtain $$Ai(x)=\frac{1}{\pi}\sqrt{\frac{x}{3}}K_{1/3}\left(\frac23x^{3/2}\right)\tag{10}$$   $$Bi(x)=\sqrt{\frac{x}{3}}\left[I_{-1/3}\left(\frac23x^{3/2}\right)+I_{1/3}\left(\frac23x^{3/2}\right)\right]\tag{11}$$ Starting from $(7)$: $$\begin{align}y&=x^{1/2}Z_{1/3}\left(\frac23 ix^{3/2}\right)\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23 ix^{3/2}\right)+BN_{1/3}\left(\frac23 ix^{3/2}\right)\right]\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23ix^{3/2}\right)+B\left(\frac{\cos\left(\frac{\pi}{3}\right)J_{1/3}\left(\frac23ix^{3/2}\right)-J_{-1/3}\left(\frac23ix^{3/2}\right)}{\sin\left(\frac{\pi}{3}\right)}\right)\right]\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23ix^{3/2}\right)+B\left(\frac{\frac12J_{1/3}\left(\frac23ix^{3/2}\right)-J_{-1/3}\left(\frac23ix^{3/2}\right)}{\left(\frac{\sqrt3}{2}\right)}\right)\right]\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23ix^{3/2}\right)+\frac{BJ_{1/3}\left(\frac23ix^{3/2}\right)}{\sqrt3}-\frac{2BJ_{-1/3}\left(\frac23ix^{3/2}\right)}{\sqrt3}\right]\\&=x^{1/2}\left[Ai^{1/3}I_{1/3}\left(\frac23x^{3/2}\right)+\frac{i^{1/3}BI_{1/3}\left(\frac23x^{3/2}\right)}{\sqrt3}-\frac{2i^{-1/3}BI_{-1/3}\left(\frac23x^{3/2}\right)}{\sqrt3}\right]\tag{a}\end{align}$$ where in $(\mathrm{a})$ I used $(8)$ rearranged as $J_p(ix)=i^{p}I_p(x)$ I don't understand how to proceed with this calculation as I am unsure how to use $(9)$; I also have no idea what $i(x)$ means. Is $i$ really a function of $x$? Is there anyone that could provide some hints or advice on how I can continue this calculation to obtain $(10)$ and $(11)$? Below are some images showing some of the relevant formulae to this question:","I have just started reading about the Airy functions and am stuck on a particular step of their derivation. But first here is some background information to give this question some meaning, more information can be found from a previous question of mine: The general solution to Bessel's differential equation: $$\fbox{$y^{\prime\prime}+\left(\frac{1-2a}{x}\right)y^{\prime}+\left[\left(bcx^{c-1}\right)^2+\frac{a^2-p^2c^2}{x^2}\right]y=0$}\tag{1}$$ is $$\fbox{$y=x^aZ_p\left(bx^c\right)$}\tag{2}$$ where $Z_p$ stands for $J_p$ or $N_p$ or any linear combination of them, and $a,b,c,p$ are constants. $J_p$ is called the Bessel function of the first kind of order $p$ and $N_p$ is any combination of $J_p$ and $J_{‚àíp}$: $$N_p(x)=\frac{\cos(\pi p)J_p(x)-J_{-p}(x)}{\sin(\pi p)}\tag{3}$$ So equation $(2)$ can be written as $$y=x^{a}\left[AJ_{p}\left(bx^c\right)+BN_{p}\left(bx^c\right)\right]\tag{4}$$ or $$y=x^{a}\left[AJ_{p}\left(bx^c\right)+B\left(\frac{\cos(\pi p)J_p\left(bx^c\right)-J_{-p}\left(bx^c\right)}{\sin(\pi p)}\right)\right]\tag{5}$$ The Airy differential equation is $$y^{\prime\prime}-xy=0\tag{6}$$ and has solution $$y=x^{1/2}Z_{1/3}\left(\frac23 ix^{3/2}\right)\tag{7}$$ By using $$I_p(x)=i^{-p}J_p(ix)\tag{8}$$   $$K_p(x)=\frac{\pi}{2}i^{p+1}\left[J_p(ix)+i\left(\frac{\cos(\pi p)J_p(ix)-J_{-p}(ix)}{\sin(\pi p)}\right)\right]\tag{9}$$ My objective is to show that $(7)$ can be written in terms of $I_{1/3}$ and $K_{1/3}$ to obtain $$Ai(x)=\frac{1}{\pi}\sqrt{\frac{x}{3}}K_{1/3}\left(\frac23x^{3/2}\right)\tag{10}$$   $$Bi(x)=\sqrt{\frac{x}{3}}\left[I_{-1/3}\left(\frac23x^{3/2}\right)+I_{1/3}\left(\frac23x^{3/2}\right)\right]\tag{11}$$ Starting from $(7)$: $$\begin{align}y&=x^{1/2}Z_{1/3}\left(\frac23 ix^{3/2}\right)\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23 ix^{3/2}\right)+BN_{1/3}\left(\frac23 ix^{3/2}\right)\right]\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23ix^{3/2}\right)+B\left(\frac{\cos\left(\frac{\pi}{3}\right)J_{1/3}\left(\frac23ix^{3/2}\right)-J_{-1/3}\left(\frac23ix^{3/2}\right)}{\sin\left(\frac{\pi}{3}\right)}\right)\right]\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23ix^{3/2}\right)+B\left(\frac{\frac12J_{1/3}\left(\frac23ix^{3/2}\right)-J_{-1/3}\left(\frac23ix^{3/2}\right)}{\left(\frac{\sqrt3}{2}\right)}\right)\right]\\&=x^{1/2}\left[AJ_{1/3}\left(\frac23ix^{3/2}\right)+\frac{BJ_{1/3}\left(\frac23ix^{3/2}\right)}{\sqrt3}-\frac{2BJ_{-1/3}\left(\frac23ix^{3/2}\right)}{\sqrt3}\right]\\&=x^{1/2}\left[Ai^{1/3}I_{1/3}\left(\frac23x^{3/2}\right)+\frac{i^{1/3}BI_{1/3}\left(\frac23x^{3/2}\right)}{\sqrt3}-\frac{2i^{-1/3}BI_{-1/3}\left(\frac23x^{3/2}\right)}{\sqrt3}\right]\tag{a}\end{align}$$ where in $(\mathrm{a})$ I used $(8)$ rearranged as $J_p(ix)=i^{p}I_p(x)$ I don't understand how to proceed with this calculation as I am unsure how to use $(9)$; I also have no idea what $i(x)$ means. Is $i$ really a function of $x$? Is there anyone that could provide some hints or advice on how I can continue this calculation to obtain $(10)$ and $(11)$? Below are some images showing some of the relevant formulae to this question:",,"['ordinary-differential-equations', 'special-functions', 'intuition', 'bessel-functions', 'airy-functions']"
19,Laplace Transform with initial value,Laplace Transform with initial value,,Use the Laplace transform to solve the following initial value   problem: $$y'' + y = 2t$$ with $y(\pi/4) = \pi / 2 $ and $y'(\pi/4) = 2 - \sqrt{2}$. I understand this type of problems but with initial values for $y(0)$ and $y'(0)$ .. How could I solve it with $y(\pi /4)$ and $y'(\pi /4)$ ?,Use the Laplace transform to solve the following initial value   problem: $$y'' + y = 2t$$ with $y(\pi/4) = \pi / 2 $ and $y'(\pi/4) = 2 - \sqrt{2}$. I understand this type of problems but with initial values for $y(0)$ and $y'(0)$ .. How could I solve it with $y(\pi /4)$ and $y'(\pi /4)$ ?,,"['ordinary-differential-equations', 'laplace-transform']"
20,Sturm Liouville eigenvalue problem,Sturm Liouville eigenvalue problem,,"I have the following Sturm-Liouville eigenvalue problem, $$\frac{d}{dr}\left(r \frac{d u}{dr} \right) + k^2 r u = 0, \, \, u(R) = 0, \, u(0) < \infty$$ where $k^2$ the eigenvalues and $u(r)$ the associated eigenfunctions. I was able to prove that the operator $L = \frac{d}{dr}\left(r \frac{d }{dr} \right), (Lu + k^2ru=0),$ is autoadjoint under the scalar product $\int_0^R f\, g dr$, and that the eigenfunctions of different eigenvalues are orthogonal under $\int_0^r f \, g \, r dr$. However I want to solve the eigenvalue problem, so I want to find the eigenvalues and its corresponding eigenfunctions. This is actually the Bessel equation of zero order, so I have tried to solve this with a Frobenius series; $u(r) = \sum_{l = 0}^{\infty} a_l z^{l+c}$ (due to we have a singular regular point at $r=0$), so finally I have obtained the following expression, $u(r) = \sum_{l = 0}^{\infty} \frac{(-1)^l}{(l!)^2} \left( \frac{k r}{2}\right)^{2l}$ How do I obtain the eigenvalues $k^2$?","I have the following Sturm-Liouville eigenvalue problem, $$\frac{d}{dr}\left(r \frac{d u}{dr} \right) + k^2 r u = 0, \, \, u(R) = 0, \, u(0) < \infty$$ where $k^2$ the eigenvalues and $u(r)$ the associated eigenfunctions. I was able to prove that the operator $L = \frac{d}{dr}\left(r \frac{d }{dr} \right), (Lu + k^2ru=0),$ is autoadjoint under the scalar product $\int_0^R f\, g dr$, and that the eigenfunctions of different eigenvalues are orthogonal under $\int_0^r f \, g \, r dr$. However I want to solve the eigenvalue problem, so I want to find the eigenvalues and its corresponding eigenfunctions. This is actually the Bessel equation of zero order, so I have tried to solve this with a Frobenius series; $u(r) = \sum_{l = 0}^{\infty} a_l z^{l+c}$ (due to we have a singular regular point at $r=0$), so finally I have obtained the following expression, $u(r) = \sum_{l = 0}^{\infty} \frac{(-1)^l}{(l!)^2} \left( \frac{k r}{2}\right)^{2l}$ How do I obtain the eigenvalues $k^2$?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'sturm-liouville']"
21,Phase plots of solutions for repeated eigenvalues,Phase plots of solutions for repeated eigenvalues,,I have a question with respect to phase plots of repeated eigenvalue cases. For instance suppose that one is given a matrix with the following: $$\overrightarrow{y'} = \begin{pmatrix} 3 & -4 \\ 1 & -1  \end{pmatrix} \overrightarrow{y}$$ whereby the solution yields: $$\overrightarrow{y} = c_1e^t\binom{2}1 + c_2(te^t\binom{2}1 + e^t\binom{1}0)$$ I understand that it is asymptotically unstable but what I am unclear is the direction and whether one should plot it according to the following two figures How does one distinguish which way the curves come out of the origin from just from analyzing the system ? I am aware that the plot should actually be the 2nd picture but I am not sure why so some help with respect to this technique would be appreciated.,I have a question with respect to phase plots of repeated eigenvalue cases. For instance suppose that one is given a matrix with the following: $$\overrightarrow{y'} = \begin{pmatrix} 3 & -4 \\ 1 & -1  \end{pmatrix} \overrightarrow{y}$$ whereby the solution yields: $$\overrightarrow{y} = c_1e^t\binom{2}1 + c_2(te^t\binom{2}1 + e^t\binom{1}0)$$ I understand that it is asymptotically unstable but what I am unclear is the direction and whether one should plot it according to the following two figures How does one distinguish which way the curves come out of the origin from just from analyzing the system ? I am aware that the plot should actually be the 2nd picture but I am not sure why so some help with respect to this technique would be appreciated.,,"['ordinary-differential-equations', 'stability-in-odes']"
22,Existence of solutions to first order ODE,Existence of solutions to first order ODE,,"The fundamental theorem of autonomous ODE states that if $V:\Bbb R^n\to\Bbb R^n$ is a smooth map, then the initial value problem $$ \begin{aligned} \dot{y}^i(t) &= V^i(y^1(t),\ldots,y^n(t)),&i=1,\ldots,n \\ y^i(t_0) &= c^i, &i=1,\ldots,n \end{aligned}\tag{1} $$ for $t_0\in\Bbb R$ and $c=(c^1,\ldots,c^n)\in\Bbb R^n$ has the following existence property: Existence: For any $t_0\in\Bbb R$ and $x_0\in\Bbb R^n$, there exist an open interval $J$ containing $t_0$ and an open subset $U$ containing $x_0$ such that for each $c\in U$, there is a smooth map $y:J\to\Bbb R^n$ that solves $(1)$. Now here is my question: Question: Suppose we already know that a solution exists with initial value $y(t_0)=x_0$ on an interval $J_0$ containing $t_0$. Does the interval $J$ above can be assumed to contain $J_0$? A priori, there is noting telling us that in the statement of the theorem. My question can be rephrased as follows. Reformulation of the Question: Let $y:J\to\Bbb R^n$ be a smooth solution to $(1)$ with initial value $y(t_0)=x_0$. Is there an open set $U$ containing $x_0$ such that for all $c\in U$ there is a smooth solution $z:J\to\Bbb R^n$ to $(1)$ with initial value $z(t_0)=c$? Edit: And what about the case where $J$ is a compact interval?","The fundamental theorem of autonomous ODE states that if $V:\Bbb R^n\to\Bbb R^n$ is a smooth map, then the initial value problem $$ \begin{aligned} \dot{y}^i(t) &= V^i(y^1(t),\ldots,y^n(t)),&i=1,\ldots,n \\ y^i(t_0) &= c^i, &i=1,\ldots,n \end{aligned}\tag{1} $$ for $t_0\in\Bbb R$ and $c=(c^1,\ldots,c^n)\in\Bbb R^n$ has the following existence property: Existence: For any $t_0\in\Bbb R$ and $x_0\in\Bbb R^n$, there exist an open interval $J$ containing $t_0$ and an open subset $U$ containing $x_0$ such that for each $c\in U$, there is a smooth map $y:J\to\Bbb R^n$ that solves $(1)$. Now here is my question: Question: Suppose we already know that a solution exists with initial value $y(t_0)=x_0$ on an interval $J_0$ containing $t_0$. Does the interval $J$ above can be assumed to contain $J_0$? A priori, there is noting telling us that in the statement of the theorem. My question can be rephrased as follows. Reformulation of the Question: Let $y:J\to\Bbb R^n$ be a smooth solution to $(1)$ with initial value $y(t_0)=x_0$. Is there an open set $U$ containing $x_0$ such that for all $c\in U$ there is a smooth solution $z:J\to\Bbb R^n$ to $(1)$ with initial value $z(t_0)=c$? Edit: And what about the case where $J$ is a compact interval?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
23,Liouville's theorem and the Wronskian,Liouville's theorem and the Wronskian,,"Liouville's theorem states that under the action of the equations of motion, the phase volume is conserved. The equations of motion are the flow ODE's generated by a Hamiltonian field $X$ and the solutions to Hamilton's equations are the integral curves of the system. The effect of a symplectomorphism $\phi_t$ is that it would take $(p^i,q_i) \to (p^j,q_j)$ or $\omega \to \omega'$ and that would be the contribution of the Jacobian of the transformation. If the flow $\phi_t$ changes variables from time, say, $t$ to time $0$. The Jacobian would be $$ J(t) = \left| \begin{array}{ccc} \frac{\partial p_1(t) }{\partial p_1(0) }  & \ldots & \frac{\partial  p_1(t) }{\partial q_{n}(0) }  \\ \vdots  & \ddots  & \vdots  \\ \frac{\partial p_{n}(t)  }{\partial q_1(0) }  &\ldots & \frac{\partial q_{n}(t)  }{\partial q_{n}(0)  } \  \end{array} \right| $$ Now, the derivatives appearing in the Jacobian satisfy a linear system of ordinary differential equations and moreover, viewed as a system of ordinary differential equations, it is easy to see that the Jacobian we need is also the Wronskian $W$ for the system of differential equations. My first question is why is this true? I know that the definition of the Wronskian of two differential functions $f,g$ is $W(f,g)=f'g-g'f$. Then I read that Wroskian's of linear equations satisfy the vector ODE \item Wroskian's of linear equations satisfy the vector ODE \begin{equation*} \frac{d}{dt}{\bf{y}} = {\bf{M}}(t) {\bf{y}} \end{equation*} What exactly is this equation? Then I read that \begin{equation*} W(t) = W(0) \exp\left( \int_0^t dt \, \text{Tr}{\bf{M}}(0) \right) \end{equation*} and for our case this trace vanishes, thus $J(0)=J(t)=1$ which shows that the contribution of the Jacobian over volume integrals is trivial. Can you help me to understand the above statements? Unfortunately I have lost the reference out of which I read those.","Liouville's theorem states that under the action of the equations of motion, the phase volume is conserved. The equations of motion are the flow ODE's generated by a Hamiltonian field $X$ and the solutions to Hamilton's equations are the integral curves of the system. The effect of a symplectomorphism $\phi_t$ is that it would take $(p^i,q_i) \to (p^j,q_j)$ or $\omega \to \omega'$ and that would be the contribution of the Jacobian of the transformation. If the flow $\phi_t$ changes variables from time, say, $t$ to time $0$. The Jacobian would be $$ J(t) = \left| \begin{array}{ccc} \frac{\partial p_1(t) }{\partial p_1(0) }  & \ldots & \frac{\partial  p_1(t) }{\partial q_{n}(0) }  \\ \vdots  & \ddots  & \vdots  \\ \frac{\partial p_{n}(t)  }{\partial q_1(0) }  &\ldots & \frac{\partial q_{n}(t)  }{\partial q_{n}(0)  } \  \end{array} \right| $$ Now, the derivatives appearing in the Jacobian satisfy a linear system of ordinary differential equations and moreover, viewed as a system of ordinary differential equations, it is easy to see that the Jacobian we need is also the Wronskian $W$ for the system of differential equations. My first question is why is this true? I know that the definition of the Wronskian of two differential functions $f,g$ is $W(f,g)=f'g-g'f$. Then I read that Wroskian's of linear equations satisfy the vector ODE \item Wroskian's of linear equations satisfy the vector ODE \begin{equation*} \frac{d}{dt}{\bf{y}} = {\bf{M}}(t) {\bf{y}} \end{equation*} What exactly is this equation? Then I read that \begin{equation*} W(t) = W(0) \exp\left( \int_0^t dt \, \text{Tr}{\bf{M}}(0) \right) \end{equation*} and for our case this trace vanishes, thus $J(0)=J(t)=1$ which shows that the contribution of the Jacobian over volume integrals is trivial. Can you help me to understand the above statements? Unfortunately I have lost the reference out of which I read those.",,"['ordinary-differential-equations', 'differential-geometry', 'symplectic-geometry']"
24,What is the meaning of $\frac{d}{dx}+x$ in $(\frac{d}{dx}+x)y=0$?,What is the meaning of  in ?,\frac{d}{dx}+x (\frac{d}{dx}+x)y=0,"I believe I understand the meaning of the infinitesimals $dx$ and $dy$ . I understand that $dx/dy$ is the ratio of an infinitely small change in $x$ to an infinitly small change in $y$ . However, I can not imagine what $(\frac{d}{dx} +x)y=0$ is trying to say. If it says $\frac{d}{dx}y$ I understand that this would be the notation for the derivative of $y$ . However, $\frac{d}{dx}$ is not paired with any real number of which it would be sensible to take the derivative, it is all by itself.  What is interesting is that the professor takes the statement $(\frac{d}{dx} +x)y$ and multiplies out $y$ , making $\frac{dy}{dx}+xy$ which is a statement that I can make sense of. However, I can not understand how this was a legal move. Here is a timestamped link to the video in which the problem pops up.","I believe I understand the meaning of the infinitesimals and . I understand that is the ratio of an infinitely small change in to an infinitly small change in . However, I can not imagine what is trying to say. If it says I understand that this would be the notation for the derivative of . However, is not paired with any real number of which it would be sensible to take the derivative, it is all by itself.  What is interesting is that the professor takes the statement and multiplies out , making which is a statement that I can make sense of. However, I can not understand how this was a legal move. Here is a timestamped link to the video in which the problem pops up.",dx dy dx/dy x y (\frac{d}{dx} +x)y=0 \frac{d}{dx}y y \frac{d}{dx} (\frac{d}{dx} +x)y y \frac{dy}{dx}+xy,"['ordinary-differential-equations', 'derivatives']"
25,Phase portrait of system of nonlinear ODEs,Phase portrait of system of nonlinear ODEs,,"How can we sketch by hand the phase portrait of a system of nonlinear ODEs like the following? $$\begin{align} \dot{x} &= 2 - 8x^2-2y^2\\ \dot{y} &= 6xy\end{align}$$ I can easily find the equilibria, which are $$\left\{ (0, \pm 1), \left(\pm \frac{1}{2}, 0\right) \right\}$$ The corresponding stable subspace for $\left(\pm \frac{1}{2}, 0\right)$ is $$\mbox{span} \left\{ \left(\frac{2i}{\sqrt{6}}, 1 \right), \left(-\frac{2i}{\sqrt{6}}, 1 \right) \right\}$$ and the unstable subspace for $(0, \pm 1)$ is $$\mbox{span} \left\{ (0, 1), (1, 0) \right\}$$ respectively. But I can't see how to use these pieces of information to sketch the phase portrait. Any help would really be appreciated!","How can we sketch by hand the phase portrait of a system of nonlinear ODEs like the following? $$\begin{align} \dot{x} &= 2 - 8x^2-2y^2\\ \dot{y} &= 6xy\end{align}$$ I can easily find the equilibria, which are $$\left\{ (0, \pm 1), \left(\pm \frac{1}{2}, 0\right) \right\}$$ The corresponding stable subspace for $\left(\pm \frac{1}{2}, 0\right)$ is $$\mbox{span} \left\{ \left(\frac{2i}{\sqrt{6}}, 1 \right), \left(-\frac{2i}{\sqrt{6}}, 1 \right) \right\}$$ and the unstable subspace for $(0, \pm 1)$ is $$\mbox{span} \left\{ (0, 1), (1, 0) \right\}$$ respectively. But I can't see how to use these pieces of information to sketch the phase portrait. Any help would really be appreciated!",,['ordinary-differential-equations']
26,Solve the differential equation $\frac{dy}{dx}=\frac{2x+y}{y}$,Solve the differential equation,\frac{dy}{dx}=\frac{2x+y}{y},"Solve $$\frac{dy}{dx}=\frac{2x+y}{y}$$ Let $$\frac{dx}{dt}=y \\ \frac{dy}{dt}=2x+y$$ Plugin $\frac{dx}{dt}=y$ into $\frac{dy}{dt}=2x+y$ I get $$2x+\frac{dx}{dt}=\frac{dy}{dt}$$ Using the fact that $\frac{d^2x}{dt^2}=\frac{dy}{dt}$ I get the 2nd order system: $$\frac{d^2x}{dt^2}-\frac{dx}{dt}-2x=0$$ Solving this I get: $(r-2)(r+1)=0$ , so $r=2,=1$ $$x(t)=Ae^{2t}+Be^{-1t}$$ How do I get my solution in terms of $y,x$? I can't seem to integrate $\frac{dy}{dx}$ by separating variables directly.","Solve $$\frac{dy}{dx}=\frac{2x+y}{y}$$ Let $$\frac{dx}{dt}=y \\ \frac{dy}{dt}=2x+y$$ Plugin $\frac{dx}{dt}=y$ into $\frac{dy}{dt}=2x+y$ I get $$2x+\frac{dx}{dt}=\frac{dy}{dt}$$ Using the fact that $\frac{d^2x}{dt^2}=\frac{dy}{dt}$ I get the 2nd order system: $$\frac{d^2x}{dt^2}-\frac{dx}{dt}-2x=0$$ Solving this I get: $(r-2)(r+1)=0$ , so $r=2,=1$ $$x(t)=Ae^{2t}+Be^{-1t}$$ How do I get my solution in terms of $y,x$? I can't seem to integrate $\frac{dy}{dx}$ by separating variables directly.",,['ordinary-differential-equations']
27,Lie Groups Exponential Map,Lie Groups Exponential Map,,"Hallo Math StackExchange users, I have a little question concerning the exponential map and how one can apply this to infinitesimal transformations. I am an engineer in 2nd year and I want to write my bachelor thesis in fluid mechanics. I am using the book Applications of Lie Groups to Differential Equations (Peter J. Olver). As recommended by the author I started to read from Chapter 2.2 and was able to follow. But now I am stuck at something. It's about the heat equation; I could follow the text until the derivation of the generators of symmetry. $$ v_1 = \partial_x, \quad v_2 = \partial_t, \quad v_3 = u\partial_u, \quad v_4 = x\partial_x+2t\partial_t, \quad v_5 = 2t\partial_x-xu\partial_u, \quad \ldots $$ After this passage the one Parameter Groups $G_i$ which are generated by the $v_i$ are calculated by using $(x',t',u')=\exp(\epsilon v_i) (x,t,u)$. It is concluded that $G_1: (x+\epsilon,t,u)$, $G_2: (x,t+\epsilon,u)$, $G_3: (x,t,e^\epsilon u)$, $G_4: (e^\epsilon x, e^{2\epsilon}t,u)$, ... . Can someone explain how I can actually calculate these groups? I tried this interpretation: $\exp(\epsilon v_i)(x,t,u)=(1+\epsilon v_i)(x,t,u) =(x+\epsilon v_i \cdot x,t+\epsilon v_i \cdot t, u \epsilon v_i \cdot u)$. I could recover $G_1$ and $G_2$ using this approach, while interpreting the operation between the exponential and the $(x,t,u)$ as a simple dot product. But from there I have no clue how to show that for $G_3$ and the other $G_i$. I tried this for $v_3 = u\partial_u$. $\exp(\epsilon u\partial_u)(x,t,u)=(1+\epsilon u\partial_u+\frac{(\epsilon u\partial_u)^2}{2!}+...)(x,t,u)=(x,t,u+u\epsilon_u u)=(x,t,u+\epsilon u)$. I left out the higher order terms as I thought, that $\partial_u \partial_u u = 0$. If I had kept them in the series I could have recovered $G_3: (x,t,e^\epsilon u)$, but I could not explain it to myself why this should make any sense. But the textbook says $G_3: (x,t,e^\epsilon u)$. Could someone explain me (step by step) how one can get these groups? The book is really doing a bad job at explaining these steps. There is no explanation how to use this exponential map algorithmically. Thank you for taking your time and reading my post :). I would be very thankful for any answer :D.","Hallo Math StackExchange users, I have a little question concerning the exponential map and how one can apply this to infinitesimal transformations. I am an engineer in 2nd year and I want to write my bachelor thesis in fluid mechanics. I am using the book Applications of Lie Groups to Differential Equations (Peter J. Olver). As recommended by the author I started to read from Chapter 2.2 and was able to follow. But now I am stuck at something. It's about the heat equation; I could follow the text until the derivation of the generators of symmetry. $$ v_1 = \partial_x, \quad v_2 = \partial_t, \quad v_3 = u\partial_u, \quad v_4 = x\partial_x+2t\partial_t, \quad v_5 = 2t\partial_x-xu\partial_u, \quad \ldots $$ After this passage the one Parameter Groups $G_i$ which are generated by the $v_i$ are calculated by using $(x',t',u')=\exp(\epsilon v_i) (x,t,u)$. It is concluded that $G_1: (x+\epsilon,t,u)$, $G_2: (x,t+\epsilon,u)$, $G_3: (x,t,e^\epsilon u)$, $G_4: (e^\epsilon x, e^{2\epsilon}t,u)$, ... . Can someone explain how I can actually calculate these groups? I tried this interpretation: $\exp(\epsilon v_i)(x,t,u)=(1+\epsilon v_i)(x,t,u) =(x+\epsilon v_i \cdot x,t+\epsilon v_i \cdot t, u \epsilon v_i \cdot u)$. I could recover $G_1$ and $G_2$ using this approach, while interpreting the operation between the exponential and the $(x,t,u)$ as a simple dot product. But from there I have no clue how to show that for $G_3$ and the other $G_i$. I tried this for $v_3 = u\partial_u$. $\exp(\epsilon u\partial_u)(x,t,u)=(1+\epsilon u\partial_u+\frac{(\epsilon u\partial_u)^2}{2!}+...)(x,t,u)=(x,t,u+u\epsilon_u u)=(x,t,u+\epsilon u)$. I left out the higher order terms as I thought, that $\partial_u \partial_u u = 0$. If I had kept them in the series I could have recovered $G_3: (x,t,e^\epsilon u)$, but I could not explain it to myself why this should make any sense. But the textbook says $G_3: (x,t,e^\epsilon u)$. Could someone explain me (step by step) how one can get these groups? The book is really doing a bad job at explaining these steps. There is no explanation how to use this exponential map algorithmically. Thank you for taking your time and reading my post :). I would be very thankful for any answer :D.",,"['ordinary-differential-equations', 'lie-groups']"
28,From Gravity Equation-of-Motion to General Solution in Polar Coordinates,From Gravity Equation-of-Motion to General Solution in Polar Coordinates,,"I'm having trouble getting the general solution of this differential equation. The gravitational equation of motion is, for constants $M$ and $G$ and position vector $\vec{r}$, $$\frac{d^2}{d t^2}\vec{r} = -\frac{MG}{r^2}\hat{r} $$ By using 2D polar coordinates (one angle $\theta$ and one ""distance from origin"" $r$), one can calculate $\frac{d^2}{dt^2}\vec{r}$ by taking two time derivatives of $\vec{r}=r\hat{r}$. The hat notiation $\hat{r}$ is the unit vector pointing from $\vec{r}$ in the direction of increasing $r$. Similarly, $\hat{\theta}$ is the unit vector pointing from $\vec{r}$ in the direction of increasing $\theta$. By appropriate application of the chain rule of derivatives (remembering to take derivatives of the unit vectors themselves as well), one can derive that (dot means time-dirivative) $$\frac{d^2}{d t^2}\vec{r} = -\frac{MG}{r^2}\hat{r} = \hat{\theta}(2\dot{r}\dot{\theta}+r\ddot{\theta})+\hat{r}(\ddot{r}-r\dot{\theta}^2) $$ Which means $$\begin{equation} -\frac{MG}{r^2}=\ddot{r}-r\dot{\theta}^2 \tag{1} \end{equation} $$ and $$\begin{equation} 0=2\dot{r}\dot{\theta}+r\ddot{\theta} \tag{2} \end{equation} $$ The right hand side of the last equation (2) turns out to be the time derivative of the angular-momentum-per-unit-mass $h$ (to a factor of $r$): $$\frac{d}{d t}h=\frac{d}{d t}(r^2\dot{\theta})=2r\dot{r}\dot{\theta}+r^2\ddot{\theta}=0 $$ Showing that h is constant in time. My question is, using what has been laid out, how does one combine the two differential equations (1) and (2) to get something that looks like it can be solved? I have looked at references, but certain steps in the derivations seem to lack explanation. From what I've seen, it seems important to eliminate $t$ from the equations (1) and (2) to yield an equation with only $r$ and $\theta$. Thank you for any help, I have been tearing my hair out over this. --- Update --- It appears the substitution $u=\frac{1}{r}$ leads to a $\ddot{r}$ in terms of $\frac{d^2 u}{d^2 \theta}$.","I'm having trouble getting the general solution of this differential equation. The gravitational equation of motion is, for constants $M$ and $G$ and position vector $\vec{r}$, $$\frac{d^2}{d t^2}\vec{r} = -\frac{MG}{r^2}\hat{r} $$ By using 2D polar coordinates (one angle $\theta$ and one ""distance from origin"" $r$), one can calculate $\frac{d^2}{dt^2}\vec{r}$ by taking two time derivatives of $\vec{r}=r\hat{r}$. The hat notiation $\hat{r}$ is the unit vector pointing from $\vec{r}$ in the direction of increasing $r$. Similarly, $\hat{\theta}$ is the unit vector pointing from $\vec{r}$ in the direction of increasing $\theta$. By appropriate application of the chain rule of derivatives (remembering to take derivatives of the unit vectors themselves as well), one can derive that (dot means time-dirivative) $$\frac{d^2}{d t^2}\vec{r} = -\frac{MG}{r^2}\hat{r} = \hat{\theta}(2\dot{r}\dot{\theta}+r\ddot{\theta})+\hat{r}(\ddot{r}-r\dot{\theta}^2) $$ Which means $$\begin{equation} -\frac{MG}{r^2}=\ddot{r}-r\dot{\theta}^2 \tag{1} \end{equation} $$ and $$\begin{equation} 0=2\dot{r}\dot{\theta}+r\ddot{\theta} \tag{2} \end{equation} $$ The right hand side of the last equation (2) turns out to be the time derivative of the angular-momentum-per-unit-mass $h$ (to a factor of $r$): $$\frac{d}{d t}h=\frac{d}{d t}(r^2\dot{\theta})=2r\dot{r}\dot{\theta}+r^2\ddot{\theta}=0 $$ Showing that h is constant in time. My question is, using what has been laid out, how does one combine the two differential equations (1) and (2) to get something that looks like it can be solved? I have looked at references, but certain steps in the derivations seem to lack explanation. From what I've seen, it seems important to eliminate $t$ from the equations (1) and (2) to yield an equation with only $r$ and $\theta$. Thank you for any help, I have been tearing my hair out over this. --- Update --- It appears the substitution $u=\frac{1}{r}$ leads to a $\ddot{r}$ in terms of $\frac{d^2 u}{d^2 \theta}$.",,"['ordinary-differential-equations', 'derivatives', 'polar-coordinates']"
29,When can you take the limit of a parameter before solving the differential equation?,When can you take the limit of a parameter before solving the differential equation?,,"Short example: consider the differential equation \begin{align*} f'(x)=\frac{k^2}{k^2+k+1}xf(x) \end{align*} where $k$ is a parameter. Wolfram Alpha tells me that the solution to this equation is  \begin{align*} f(x)=ce^\frac{k^2 x^2}{2(k^2+k+1)} \end{align*} If I then take the limit as $k\rightarrow \infty$, the solution converges to \begin{align*} f(x)=ce^\frac{x^2}{2}. \end{align*} This is the same solution as if I had simply taken the limit before I tried to solve the differential equation, that is I instead solved \begin{align*} f'(x)=xf(x). \end{align*} My question is, then, when is it appropriate to take a such a limit of a parameter BEFORE solving the differential equation. I haven't been able to find many references to this question. One book I found online (Ritger and Rose - ""Differential Equations with Applications,"" p. 69) claims that ""taking the limit of a parameter in a differential equation and then solving the differential equation is not always the same as solving the differential equation and then taking the limit of the parameter"" but doesn't give references or conditions. Any help (even just pointing me to the appropriate reference) would be appreciated!","Short example: consider the differential equation \begin{align*} f'(x)=\frac{k^2}{k^2+k+1}xf(x) \end{align*} where $k$ is a parameter. Wolfram Alpha tells me that the solution to this equation is  \begin{align*} f(x)=ce^\frac{k^2 x^2}{2(k^2+k+1)} \end{align*} If I then take the limit as $k\rightarrow \infty$, the solution converges to \begin{align*} f(x)=ce^\frac{x^2}{2}. \end{align*} This is the same solution as if I had simply taken the limit before I tried to solve the differential equation, that is I instead solved \begin{align*} f'(x)=xf(x). \end{align*} My question is, then, when is it appropriate to take a such a limit of a parameter BEFORE solving the differential equation. I haven't been able to find many references to this question. One book I found online (Ritger and Rose - ""Differential Equations with Applications,"" p. 69) claims that ""taking the limit of a parameter in a differential equation and then solving the differential equation is not always the same as solving the differential equation and then taking the limit of the parameter"" but doesn't give references or conditions. Any help (even just pointing me to the appropriate reference) would be appreciated!",,"['ordinary-differential-equations', 'reference-request']"
30,Using an inverse operator to find a particular solution to a differential equation.,Using an inverse operator to find a particular solution to a differential equation.,,"I am just learning about inverse operators in solving a differential equation, but I don't understand exactly how they work. For example, find a particular solution to $$4y''-3y'+9y=5x^2$$ using inverse operators. The above equation is equivalent to $$(4D^2-3D+9)y=5x^2$$ Now the way to solve this would be to use the inverse operator as follows: $$y_p=(4D^2-3D+9)^{-1}5x^2$$ or $$y_p=\frac {1}{4D^2-3D+9}5x^2$$ The book I am reading uses ""simple division"" and arrives at a result, but I don't quite understand how that works. What is a step-by-step method to solve the above problem?","I am just learning about inverse operators in solving a differential equation, but I don't understand exactly how they work. For example, find a particular solution to $$4y''-3y'+9y=5x^2$$ using inverse operators. The above equation is equivalent to $$(4D^2-3D+9)y=5x^2$$ Now the way to solve this would be to use the inverse operator as follows: $$y_p=(4D^2-3D+9)^{-1}5x^2$$ or $$y_p=\frac {1}{4D^2-3D+9}5x^2$$ The book I am reading uses ""simple division"" and arrives at a result, but I don't quite understand how that works. What is a step-by-step method to solve the above problem?",,['ordinary-differential-equations']
31,Eigenvalues of a Plane Curve Laplace-Beltrami Operator,Eigenvalues of a Plane Curve Laplace-Beltrami Operator,,"Given a closed plane curve $C$, which is a one-dimentional manifold, what are the eigenvalues of Laplace-Beltrami operator defined on this curve? I know that the LB eigenvalue problem for unit circle is equivalent to the regular Laplacian eigenvalue problem for  the interval of the length $2\pi$ with periodic Boundary conditions. In a sense, the LB operator on circle can be viewed as a Laplacian of a function depending on the arc length.  Therefore the eigenvalues of LB operator on a circle of arbitrary radius will be equal to the eigenvalues of unit circle LB, rescaled by the square of the radius. Does this also hold for any ""good"" closed plane curve? To summarize, let me state again the QUESTION : Do the eigenvalues of Laplace-Beltrami operator defined on an arbitrary closed one dimensional smooth manifold embedded in $\mathbb{R}^2$ depend only on the (arc) length of this manifold? I perceive it to be counterintuitive for two distinct closed plane curves of the same total length to always have equal LB eigenvalues, but I could neither prove nor disprove this statement.","Given a closed plane curve $C$, which is a one-dimentional manifold, what are the eigenvalues of Laplace-Beltrami operator defined on this curve? I know that the LB eigenvalue problem for unit circle is equivalent to the regular Laplacian eigenvalue problem for  the interval of the length $2\pi$ with periodic Boundary conditions. In a sense, the LB operator on circle can be viewed as a Laplacian of a function depending on the arc length.  Therefore the eigenvalues of LB operator on a circle of arbitrary radius will be equal to the eigenvalues of unit circle LB, rescaled by the square of the radius. Does this also hold for any ""good"" closed plane curve? To summarize, let me state again the QUESTION : Do the eigenvalues of Laplace-Beltrami operator defined on an arbitrary closed one dimensional smooth manifold embedded in $\mathbb{R}^2$ depend only on the (arc) length of this manifold? I perceive it to be counterintuitive for two distinct closed plane curves of the same total length to always have equal LB eigenvalues, but I could neither prove nor disprove this statement.",,"['ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'derivatives', 'manifolds']"
32,Algebraic Curves and Second Order Differential Equations,Algebraic Curves and Second Order Differential Equations,,"I am curious if there are any examples of functions that are solutions to second order differential equations, that also parametrize an algebraic curve. I am aware that the Weierstrass $\wp$ - Elliptic function satisfies a differential equation. We can then interpret this Differential Equation as an algebraic equation, with solutions found on elliptic curves. However this differential equations is of the first order. So, are there periodic function(s)  $F(x)$  that satisfy a second order differential equation, such that we can say these parametrize an algebraic curve? Could a Bessel function be one such solution?","I am curious if there are any examples of functions that are solutions to second order differential equations, that also parametrize an algebraic curve. I am aware that the Weierstrass $\wp$ - Elliptic function satisfies a differential equation. We can then interpret this Differential Equation as an algebraic equation, with solutions found on elliptic curves. However this differential equations is of the first order. So, are there periodic function(s)  $F(x)$  that satisfy a second order differential equation, such that we can say these parametrize an algebraic curve? Could a Bessel function be one such solution?",,"['ordinary-differential-equations', 'algebraic-geometry', 'algebraic-curves', 'periodic-functions']"
33,$f ' (x) = f(x - (x+1)^t + 1)$,,f ' (x) = f(x - (x+1)^t + 1),Let $x > 0 $ and $c $ a given real $> 0.$ Let $t $ be between $0 $ and $1.$ How to find $f(x)$ or good asymptotics for $ f(x)$ such that $$ f ' (x) = f(x - (x+1)^t + 1) $$ And $ f(1) = 1 + c$. Also $f$ is a nonlinear function and twice differentiable for $x > 0$.,Let $x > 0 $ and $c $ a given real $> 0.$ Let $t $ be between $0 $ and $1.$ How to find $f(x)$ or good asymptotics for $ f(x)$ such that $$ f ' (x) = f(x - (x+1)^t + 1) $$ And $ f(1) = 1 + c$. Also $f$ is a nonlinear function and twice differentiable for $x > 0$.,,"['calculus', 'ordinary-differential-equations', 'asymptotics']"
34,Distributional linear differential equations,Distributional linear differential equations,,"What are the most general distributional solutions $u \in \mathcal{D}'(\mathbb{R})$ to $-\frac{d^n u}{dx^n} + c_{n-1}\frac{d^{n-1}u}{dx^{n-1}} + ... + c_0 u = 0$; $-x\frac{d^n u}{dx^n} + c_{n-1}x\frac{d^{n-1}u}{dx^{n-1}} + ... + c_0x u = 0$; where the $c_i$'s are constants. For the first one, using the definition of the distributional derivative, I came to the conclusion that it suffices to solve the $n$th-order characteristic polynomial and to fix $u =$ continuous function that is solution to differential operator $-\frac{d^n }{dx^n} + c_{n-1}\frac{d^{n-1}}{dx^{n-1}} + ... + c_0  = 0$. Is it OK ? As for the second one I'm stuck... What is the general strategy to solve a problem of this kind ?","What are the most general distributional solutions $u \in \mathcal{D}'(\mathbb{R})$ to $-\frac{d^n u}{dx^n} + c_{n-1}\frac{d^{n-1}u}{dx^{n-1}} + ... + c_0 u = 0$; $-x\frac{d^n u}{dx^n} + c_{n-1}x\frac{d^{n-1}u}{dx^{n-1}} + ... + c_0x u = 0$; where the $c_i$'s are constants. For the first one, using the definition of the distributional derivative, I came to the conclusion that it suffices to solve the $n$th-order characteristic polynomial and to fix $u =$ continuous function that is solution to differential operator $-\frac{d^n }{dx^n} + c_{n-1}\frac{d^{n-1}}{dx^{n-1}} + ... + c_0  = 0$. Is it OK ? As for the second one I'm stuck... What is the general strategy to solve a problem of this kind ?",,"['analysis', 'ordinary-differential-equations', 'distribution-theory']"
35,Understanding singularities of differential equations,Understanding singularities of differential equations,,"Is there a simple reference which explains how to see geometrically an algebraic differential equation ? I tried to read ""√âquation diff√©rentielles √† points singuliers r√©guliers"" of Pierre Deligne but it was a bit too complicated for me. I only follow a basic course on Riemann surfaces, is there some reference or books which can helps me to understand it better this paper of Deligne (or more generally what is the geometry hidden behind theses equations and these singularities ? Thanks in advance and sorry if my question is a bit unclear.","Is there a simple reference which explains how to see geometrically an algebraic differential equation ? I tried to read ""√âquation diff√©rentielles √† points singuliers r√©guliers"" of Pierre Deligne but it was a bit too complicated for me. I only follow a basic course on Riemann surfaces, is there some reference or books which can helps me to understand it better this paper of Deligne (or more generally what is the geometry hidden behind theses equations and these singularities ? Thanks in advance and sorry if my question is a bit unclear.",,['ordinary-differential-equations']
36,Solve Heat Equation using Fourier Transform (non homogeneous),Solve Heat Equation using Fourier Transform (non homogeneous),,"I know how to solve heat equation where it's like $u_t=k\cdot u_{xx}$ (using Fourier Transform or using Separation of Variables) but this exercise is really difficult for me. I have this: $$u_t(x,t)=k \cdot u_{xx}(x,t)-a\cdot k \cdot u(x,t)$$ $$u_x(0,t)=0$$ $$u(x,0) = f(x)$$ with $x>0, t>0$ and $a, k$ are positive constants. I have to find $u(x,t)$ and propose a possible $f(x)$ Any help? Thanks I was told I cannot use Fourier Transform, I have to use Fourier Cosine Transform, and I don't know why","I know how to solve heat equation where it's like $u_t=k\cdot u_{xx}$ (using Fourier Transform or using Separation of Variables) but this exercise is really difficult for me. I have this: $$u_t(x,t)=k \cdot u_{xx}(x,t)-a\cdot k \cdot u(x,t)$$ $$u_x(0,t)=0$$ $$u(x,0) = f(x)$$ with $x>0, t>0$ and $a, k$ are positive constants. I have to find $u(x,t)$ and propose a possible $f(x)$ Any help? Thanks I was told I cannot use Fourier Transform, I have to use Fourier Cosine Transform, and I don't know why",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-transform', 'heat-equation', 'integral-transforms']"
37,Confused by how to derive the derivative of $f(\boldsymbol{x})=g(\boldsymbol{y})$,Confused by how to derive the derivative of,f(\boldsymbol{x})=g(\boldsymbol{y}),"I was watching an online tutorial and saw this derivation. It seems the the author took the derivative with respect to y on left side and to x on right side. I thought dx should always be in the denominator and should on both side of the equation. Is it partial derivative? Or maybe my misunderstanding of the notation? Could anyone explain how this works? FYI the link of the tutorial is https://www.youtube.com/watch?v=aXBFKKh54Es&list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU&index=98 , the differentials was taken at around 2'20"" Much appreciated! Happy New Year.","I was watching an online tutorial and saw this derivation. It seems the the author took the derivative with respect to y on left side and to x on right side. I thought dx should always be in the denominator and should on both side of the equation. Is it partial derivative? Or maybe my misunderstanding of the notation? Could anyone explain how this works? FYI the link of the tutorial is https://www.youtube.com/watch?v=aXBFKKh54Es&list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU&index=98 , the differentials was taken at around 2'20"" Much appreciated! Happy New Year.",,['ordinary-differential-equations']
38,Existence and uniqueness of soluctions of $y'=xy^{2/3}$,Existence and uniqueness of soluctions of,y'=xy^{2/3},"It is asked to analyze the existance and uniqueness of solutions of the ode at every point $(x_o, y_o)$ $$y' = 3y^{2/3}$$ My attempt: We consider the initial condition $ y(x_o)=y_o$. If we consider the subset of $R^2$ such that $y\neq 0$, then we can verify the function $F(x,y)=3y^{2/3}$ is continuous and lipschitzian, and then admit an unique solution y such that $y(x_o)=y_o$ Now, if $y = 0,$ clearly $y=0$ is a solution of this ODE. We could solve the ODE by separating the variables too and obtain $y=(x+c)^3$ But my doubt is: If $y=0,$ wasnt 0 the only solution or the solution obtained separating the vsrizbles is also a solution? Applying the initial condition, we should have $y=(x-x_0)^3$, but is it a solution? Thanks in advanced!","It is asked to analyze the existance and uniqueness of solutions of the ode at every point $(x_o, y_o)$ $$y' = 3y^{2/3}$$ My attempt: We consider the initial condition $ y(x_o)=y_o$. If we consider the subset of $R^2$ such that $y\neq 0$, then we can verify the function $F(x,y)=3y^{2/3}$ is continuous and lipschitzian, and then admit an unique solution y such that $y(x_o)=y_o$ Now, if $y = 0,$ clearly $y=0$ is a solution of this ODE. We could solve the ODE by separating the variables too and obtain $y=(x+c)^3$ But my doubt is: If $y=0,$ wasnt 0 the only solution or the solution obtained separating the vsrizbles is also a solution? Applying the initial condition, we should have $y=(x-x_0)^3$, but is it a solution? Thanks in advanced!",,['ordinary-differential-equations']
39,projectile motion (with height) complicated,projectile motion (with height) complicated,,"When a child standing on a horizontal path throws a ball, it leaves   her hand from a point that is 90cm vertically above the path. The ball   clears a 4.5 m high wall that is 10.5 m away from where it was thrown. Show that the least velocity required for this to occur is the same as   the velocity acquired by a body falling 7.35m under gravity. My solution : Case1 : 7.35m falling body $$ v^2=u^2+2as \\ v^2=0+2(-9.8)(-7.35) \\ v=\sqrt{144.06} \\ v=12 \, \mbox{m}/\mbox{s} $$ Case 2: projectile motion $$ t=0 \Rightarrow \\ \mathbf{V} = u \cos \alpha \, \mathbf{i} + u \sin \alpha \, \mathbf{j} \\  \mathbf{r} = 0.9 \, \mathbf{j} $$ $$ \mathbf{V} = u \cos \alpha \, \mathbf{i} +  [ u \sin\alpha -9.8t] \, \mathbf{j} \\ \mathbf{r}= (u\cos \alpha t) \, \mathbf{i} + (0.9+ u \sin \alpha t-4.9 t^2) \, \mathbf{j}  $$ I end up with the equations: usina-9.8t=0 u=(9.8t)/sina......1 10.5=ucosat......2 4.50.9+usinat-4.9t^2......3 wrong answer where i let a=45 let angle a=45 0.9+usin(45)t-4.9t^2=4.5_________1 ucos(45)t=10.5  => y=10.5/[cos(45)*t] sub 2 to 1 -4.9t^2+10.5/[cos(45)*t]*sin(45)*t-3.6=0 -4.9t^2+10.5tan(45)-3.6=0 -4.9t^2=-6.9 t=1.1867 so y=10.5/[cos(45)*1.1867] =12.51 wrong answer^ which is... not  = to 12","When a child standing on a horizontal path throws a ball, it leaves   her hand from a point that is 90cm vertically above the path. The ball   clears a 4.5 m high wall that is 10.5 m away from where it was thrown. Show that the least velocity required for this to occur is the same as   the velocity acquired by a body falling 7.35m under gravity. My solution : Case1 : 7.35m falling body $$ v^2=u^2+2as \\ v^2=0+2(-9.8)(-7.35) \\ v=\sqrt{144.06} \\ v=12 \, \mbox{m}/\mbox{s} $$ Case 2: projectile motion $$ t=0 \Rightarrow \\ \mathbf{V} = u \cos \alpha \, \mathbf{i} + u \sin \alpha \, \mathbf{j} \\  \mathbf{r} = 0.9 \, \mathbf{j} $$ $$ \mathbf{V} = u \cos \alpha \, \mathbf{i} +  [ u \sin\alpha -9.8t] \, \mathbf{j} \\ \mathbf{r}= (u\cos \alpha t) \, \mathbf{i} + (0.9+ u \sin \alpha t-4.9 t^2) \, \mathbf{j}  $$ I end up with the equations: usina-9.8t=0 u=(9.8t)/sina......1 10.5=ucosat......2 4.50.9+usinat-4.9t^2......3 wrong answer where i let a=45 let angle a=45 0.9+usin(45)t-4.9t^2=4.5_________1 ucos(45)t=10.5  => y=10.5/[cos(45)*t] sub 2 to 1 -4.9t^2+10.5/[cos(45)*t]*sin(45)*t-3.6=0 -4.9t^2+10.5tan(45)-3.6=0 -4.9t^2=-6.9 t=1.1867 so y=10.5/[cos(45)*1.1867] =12.51 wrong answer^ which is... not  = to 12",,"['ordinary-differential-equations', 'classical-mechanics']"
40,Hopf bifurcation and limit cycle,Hopf bifurcation and limit cycle,,"I am studying bifurcation and had a system like this: $$dx/dt=ux-y-x(x^2+y^2),$$ $$dy/dt=x+uy-y(x^2+y^2).$$ I want to determine whether a Hopf bifurcation would occur. I wrote the system into polar coordinates: $$dr/dt=ur-r^3,$$ $$d\theta/dt=1.$$ So I have a unstable limit cycle at $$r=\sqrt{u},$$ when u is positive. Can I then conclude that a Hopf bifurcation do occur? Since the spiral inside and outside the limit cycle towards different direction? But then I am confused by the question ""at what value of $u$ a Hopf bifurcation occurs""? What does that mean? Thanks!","I am studying bifurcation and had a system like this: $$dx/dt=ux-y-x(x^2+y^2),$$ $$dy/dt=x+uy-y(x^2+y^2).$$ I want to determine whether a Hopf bifurcation would occur. I wrote the system into polar coordinates: $$dr/dt=ur-r^3,$$ $$d\theta/dt=1.$$ So I have a unstable limit cycle at $$r=\sqrt{u},$$ when u is positive. Can I then conclude that a Hopf bifurcation do occur? Since the spiral inside and outside the limit cycle towards different direction? But then I am confused by the question ""at what value of $u$ a Hopf bifurcation occurs""? What does that mean? Thanks!",,"['ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations', 'bifurcation']"
41,Gronwall type inequality,Gronwall type inequality,,Is there a Gronwall-type inequality for bounding $u(t)$ such that $$\vert \partial_t u(t)\vert\leq C \{ u(t)+u(t)^\alpha\}$$ with $\alpha>1$ ?,Is there a Gronwall-type inequality for bounding $u(t)$ such that $$\vert \partial_t u(t)\vert\leq C \{ u(t)+u(t)^\alpha\}$$ with $\alpha>1$ ?,,"['real-analysis', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'gronwall-type-inequality']"
42,"Separation of variables - ""formal"" notation?","Separation of variables - ""formal"" notation?",,"When using separation of variables technique to solve differential equations, I sometimes have both f(x) and g(y) on the right side, and then I divide by g(y) to separate them.... but how can I explain formally that I am allowed to do this? You could say that g(y) does not equal 0.... but often, g(y) = 0 IS a solution to a differential equation. So what then? It's not a question directly related to an assignment, it's just that my teacher wants us to be very formal and explain every move we make instead of just copying down the methods from the book.","When using separation of variables technique to solve differential equations, I sometimes have both f(x) and g(y) on the right side, and then I divide by g(y) to separate them.... but how can I explain formally that I am allowed to do this? You could say that g(y) does not equal 0.... but often, g(y) = 0 IS a solution to a differential equation. So what then? It's not a question directly related to an assignment, it's just that my teacher wants us to be very formal and explain every move we make instead of just copying down the methods from the book.",,['ordinary-differential-equations']
43,Second-order inhomogeneous differential equation $y''\:-\:4y'\:+\:2y\:=\:2x^2$,Second-order inhomogeneous differential equation,y''\:-\:4y'\:+\:2y\:=\:2x^2,"I'm trying to solving a 2nd-order inhomogeneous differential equation, but I'm not sure with my answer since I'm only learned it by myself & it has nothing to do with school or homework, so I have no one to ask or correct my answer. Please if you want to help me,  i would be really happy & appreciate your help & advice. So here's my problem $y''\:-\:4y'\:+\:2y\:=\:2x^2$ and here's my step to answer I know that to solve this 2nd-order inhomogeneous differential equation the general solution is $y\:=\:y_c\:+\:y_p$ where $y_c$ is the general solution of the homogeneous equation. And $y_p$ is a particular solution First, I find for $y_c$ through $y''\:-\:4y'\:+\:2y\:=\:0$ since it's in the form of different roots of the auxiliary equation, i found that $m_1\:=\:2+\sqrt{2}$ and $m_2\:=\:2-\sqrt{2}$ , so the general solution of the homogeneous equation is $y_c=c_1e^{\left(2+\sqrt{2}\right)x}+c_2e^{\left(2-\sqrt{2}\right)x}$ Second, since it's a polynomial problem so i find for $y_p$ through $y_p=Ax^2+Bx+C$ first and second derivative of $y_p$ are $y'_p=2Ax\:+\:B\:\:\:\:\:\:\:and\:\:\:\:\:\:\:\:y""_p=2A$ so now i have to find the undetermined coefficients $A$, $B$, and $C$. I find these coefficients through substitute $y_p$ into $y$, $y'_p$ into $y'$, and $y''_p$ into $y""$, thus $y''\:-\:4y'\:+\:2y\:=\:\left(2A\right)x^2+\left(2B-8A\right)x\:+\:\left(2A-4B+2C\right)=2x^2$ So, $A\:=\:\frac{1}{2}\:,\:B\:=\:0,\:C\:=\:\frac{1}{2}$ and then substitute this into $y_p$, I find the particular solution which is $y_p=\frac{1}{2}\left(x^2+1\right)$ and finally I find the general solution in the form of $y\:=\:c_1e^{\left(2+\sqrt{2}\right)x}+c_2e^{\left(2-\sqrt{2}\right)x}+\frac{1}{2}\left(x^2+1\right)$ This is my answer, am I correct or not? Thank you for your time!","I'm trying to solving a 2nd-order inhomogeneous differential equation, but I'm not sure with my answer since I'm only learned it by myself & it has nothing to do with school or homework, so I have no one to ask or correct my answer. Please if you want to help me,  i would be really happy & appreciate your help & advice. So here's my problem $y''\:-\:4y'\:+\:2y\:=\:2x^2$ and here's my step to answer I know that to solve this 2nd-order inhomogeneous differential equation the general solution is $y\:=\:y_c\:+\:y_p$ where $y_c$ is the general solution of the homogeneous equation. And $y_p$ is a particular solution First, I find for $y_c$ through $y''\:-\:4y'\:+\:2y\:=\:0$ since it's in the form of different roots of the auxiliary equation, i found that $m_1\:=\:2+\sqrt{2}$ and $m_2\:=\:2-\sqrt{2}$ , so the general solution of the homogeneous equation is $y_c=c_1e^{\left(2+\sqrt{2}\right)x}+c_2e^{\left(2-\sqrt{2}\right)x}$ Second, since it's a polynomial problem so i find for $y_p$ through $y_p=Ax^2+Bx+C$ first and second derivative of $y_p$ are $y'_p=2Ax\:+\:B\:\:\:\:\:\:\:and\:\:\:\:\:\:\:\:y""_p=2A$ so now i have to find the undetermined coefficients $A$, $B$, and $C$. I find these coefficients through substitute $y_p$ into $y$, $y'_p$ into $y'$, and $y''_p$ into $y""$, thus $y''\:-\:4y'\:+\:2y\:=\:\left(2A\right)x^2+\left(2B-8A\right)x\:+\:\left(2A-4B+2C\right)=2x^2$ So, $A\:=\:\frac{1}{2}\:,\:B\:=\:0,\:C\:=\:\frac{1}{2}$ and then substitute this into $y_p$, I find the particular solution which is $y_p=\frac{1}{2}\left(x^2+1\right)$ and finally I find the general solution in the form of $y\:=\:c_1e^{\left(2+\sqrt{2}\right)x}+c_2e^{\left(2-\sqrt{2}\right)x}+\frac{1}{2}\left(x^2+1\right)$ This is my answer, am I correct or not? Thank you for your time!",,['ordinary-differential-equations']
44,Initial-value problem for non-linear partial differential equation $y_x^2=k/y_t^2-1$,Initial-value problem for non-linear partial differential equation,y_x^2=k/y_t^2-1,"For this problem, $y$ is a function of two variables: one space variable $x$ and one time variable $t$. $k > 0$ is some constant. And $x$ takes is value in the interval $[0, 1]$ and $t \ge 0$. At the initial time, $y$ follows a parabolic profile , like $y(x, 0) = 1 - (x-\frac{1 }{2})^2$. Finally, $y$ satisfies this PDE: $$ \left(\frac{\partial y} {\partial x}\right)^2  = \frac{k}{\left(\frac{\partial y} {\partial t}\right)^2} - 1.$$ Does anyone have an idea how to solve this problem (and find the expression of $y(x,t)$) ? About: The problem arise in physics, when studying the temporal shift of a front of iron particles in a magnetic field. Edit: I solved it numerically on a (badly-designed) 1st-order numerical scheme with a small space & time discretization, with the initial condition I wanted ( in Octave/Matlab , in Python and in OCaml + GNUplot ). The numerical result was enough to confirm the theory and the experiment (the observation done in the lab), so I did not try any further to solve it analytically. See here for an animation of the front of iron matter, and here for more details (in French) .","For this problem, $y$ is a function of two variables: one space variable $x$ and one time variable $t$. $k > 0$ is some constant. And $x$ takes is value in the interval $[0, 1]$ and $t \ge 0$. At the initial time, $y$ follows a parabolic profile , like $y(x, 0) = 1 - (x-\frac{1 }{2})^2$. Finally, $y$ satisfies this PDE: $$ \left(\frac{\partial y} {\partial x}\right)^2  = \frac{k}{\left(\frac{\partial y} {\partial t}\right)^2} - 1.$$ Does anyone have an idea how to solve this problem (and find the expression of $y(x,t)$) ? About: The problem arise in physics, when studying the temporal shift of a front of iron particles in a magnetic field. Edit: I solved it numerically on a (badly-designed) 1st-order numerical scheme with a small space & time discretization, with the initial condition I wanted ( in Octave/Matlab , in Python and in OCaml + GNUplot ). The numerical result was enough to confirm the theory and the experiment (the observation done in the lab), so I did not try any further to solve it analytically. See here for an animation of the front of iron matter, and here for more details (in French) .",,"['ordinary-differential-equations', 'partial-differential-equations']"
45,Interpretation of generalized eigenvector in orbits,Interpretation of generalized eigenvector in orbits,,"First of all, this is my fourth question about dynamical systems in a week, sorry for that. Considering a linear bidimensional dynamical (autonomous) system, the orbits can be plotted in the phase state as so: Let me specify that I already managed to solve the dynamical equations in every case (regarding eigenvalues), and plot similar orbits. My question is on the third image: there, there is one double (positive) eigenvalue. The dimension of the unstable manifold is therefore $2$. It is spanned by an eigenvector (here, $(1,0)$ for e.g.) and a generalized eigenvector. My question is, is there a link between the shape of the trajectories and a generalized eigenvector? For the eigenvector the link is quite obvious (also in the first and last cases in the figure), but I am not able to show a generalized eigenvector from the orbits. Thank you edit : I wrote the dynamical equations for complex conjugate eigenvalues, and: - the (un)stable manifold is of obviously bidimensional (stable/unstable manifold theorem); - but nothing prevents me from trying to find a 1D-submanifold of the (un)stable manifold, by seeking a parametric curve of the form $(x,h(x))$ and writing that it is invariant ($h$ is written as its Taylor expansion up to a finite degree). The interesting thing is that there is no such curve! As I have not read anything about this, I'd be willing to have your opinions: in some cases (1 and 4 in the figure), the (un)stable manifold is of dimension 2, and two submanifolds of dimension 1 can always be calculated. If the eigenvalues are complex (not real) (case 2 in above figure), the (un)stable manifold is of dimension 2, but this manifold cannot be ""decomposed"" in two (un)stable submanifolds. I'm not sure what I'm taking about when writing about decomposition in submanifolds... Highlight welcome!","First of all, this is my fourth question about dynamical systems in a week, sorry for that. Considering a linear bidimensional dynamical (autonomous) system, the orbits can be plotted in the phase state as so: Let me specify that I already managed to solve the dynamical equations in every case (regarding eigenvalues), and plot similar orbits. My question is on the third image: there, there is one double (positive) eigenvalue. The dimension of the unstable manifold is therefore $2$. It is spanned by an eigenvector (here, $(1,0)$ for e.g.) and a generalized eigenvector. My question is, is there a link between the shape of the trajectories and a generalized eigenvector? For the eigenvector the link is quite obvious (also in the first and last cases in the figure), but I am not able to show a generalized eigenvector from the orbits. Thank you edit : I wrote the dynamical equations for complex conjugate eigenvalues, and: - the (un)stable manifold is of obviously bidimensional (stable/unstable manifold theorem); - but nothing prevents me from trying to find a 1D-submanifold of the (un)stable manifold, by seeking a parametric curve of the form $(x,h(x))$ and writing that it is invariant ($h$ is written as its Taylor expansion up to a finite degree). The interesting thing is that there is no such curve! As I have not read anything about this, I'd be willing to have your opinions: in some cases (1 and 4 in the figure), the (un)stable manifold is of dimension 2, and two submanifolds of dimension 1 can always be calculated. If the eigenvalues are complex (not real) (case 2 in above figure), the (un)stable manifold is of dimension 2, but this manifold cannot be ""decomposed"" in two (un)stable submanifolds. I'm not sure what I'm taking about when writing about decomposition in submanifolds... Highlight welcome!",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'manifolds', 'dynamical-systems']"
46,Find $f$ such that $\frac{\mathrm d^2}{\mathrm dx^2}f(x)=f\left(\sqrt{x}\right)$.,Find  such that .,f \frac{\mathrm d^2}{\mathrm dx^2}f(x)=f\left(\sqrt{x}\right),"Which non-constant functions $f$ (if any) satisfy $\dfrac{\mathrm d^2}{\mathrm dx^2}f(x)=f\left(\sqrt{x}\right)$ for $x>0$ ? I suspect there is no $f$ which satisfies the differential equation, but I cannot prove this.","Which non-constant functions (if any) satisfy for ? I suspect there is no which satisfies the differential equation, but I cannot prove this.",f \dfrac{\mathrm d^2}{\mathrm dx^2}f(x)=f\left(\sqrt{x}\right) x>0 f,"['calculus', 'ordinary-differential-equations', 'functional-equations']"
47,"Due to numerical inaccuracy, the solution of a boundary value problems becomes negative","Due to numerical inaccuracy, the solution of a boundary value problems becomes negative",,"I treat a toy example to get my point across. In reality I have to deal with a much more complex model. Let us consider a one dimensional boundary value problem using the bvp5c solver in Matlab. Two liquids enter at different points, move in opposite directions and react with each other. Liquid 1 enters at $x = 0$ and moves in the positive direction. Liquid 2 enters at $x = 1$ and moves in the negative direction. We are interested in the equilibrium distribution of the two concentrations over the domain $[0,1]$. Let $c_1(x)$ be the concentration of liquid 1 at the point $x$ and similarly for liquid two. The differential equations that need to be solved are the following \begin{align} \frac{dc_1(x)}{dx} &= -(c_1(x) + c_2(x)), \\ \frac{dc_2(x)}{dx} &= K c_1(x) c_2(x), \end{align} with $K$ a constant that we can choose. Note that the $c_1(x)$ goes down for increasing $x$ and $c_2(x)$ goes down for decreasing $x$, which is exactly as we expect. The two boundary values are $c_1(0) = 10$ and $c_2(1) = 6$. In Matlab, one models this as a boundary value problem with two boundaries and two components. By solving this problem for $K = 1$ we obtain a nice solution shown in the following figure. However, by increasing $K$ to, say, $K = 10^5$ we run into a numerical inaccuracy, see this figure. I also receive a warning message from Matlab, see the bottom of this post. The concentration $c_2(x)$ drops below zero, while that should not be possible according to the second differential equation. My intuition is that this happens because Matlab takes discrete sizes of $dx$ and computes the derivative in a point (which is large) and updates the solution, making the concentration negative. One could combat this phenomenon by increasing the maximum allowed mesh size, but this is not a solution that can be incorporated in my actual model due to the computation time and memory issues. How to make sure that both concentrations are nonnegative ? Matlab code The script that runs the bvp5c solver x = linspace(0,1,100); % the mesh yinit = [ 5 0 ]; % an initial guess for the solution for both liquids K = 1; % the factor K in the second differential equation  solinit = bvpinit(x,yinit); % setting the mesh and the initial guess   % Calling the bvp solver. First argument are the ODEs, the second argument  % are the boundary conditions at x = 0 (for liquid 1) and at x = 1 (for % liquid 2), the last argument specifies the mesh and initial guess sol = bvp5c(@(x,c)Concentrations(x,c,K),@(ca,cb)BoundaryCond(ca,cb),solinit); The function with the ODEs. The dependence on $x$ is implied in the input of the function. function dcdx = Concentrations(x,c,K)  dcdx(1) = -(c(1) + c(2)); % differential equation for liquid 1 dcdx(2) = K*c(1)*c(2); % differential equation for liquid 2 The function with boundary values. When there are two boundary values, Matlab refers to them as $a$ and $b$, which are $x = 0$ and $x = 1$, respectively. function res = BoundaryCond(ca,cb)  res = [ ca(1)-10; cb(2)-6 ]; % the concentration of liquid 1 at boundary a is 10 and the concentration of liquid 2 at boundary b is 6. Error message for $K = 10^5$ Warning: Unable to meet the tolerance without using more than 5000 mesh points.  The last mesh of 892 points and the solution are available in the output argument.  The maximum error is 536.678, while requested accuracy is 0.001.  > In bvp5c at 339","I treat a toy example to get my point across. In reality I have to deal with a much more complex model. Let us consider a one dimensional boundary value problem using the bvp5c solver in Matlab. Two liquids enter at different points, move in opposite directions and react with each other. Liquid 1 enters at $x = 0$ and moves in the positive direction. Liquid 2 enters at $x = 1$ and moves in the negative direction. We are interested in the equilibrium distribution of the two concentrations over the domain $[0,1]$. Let $c_1(x)$ be the concentration of liquid 1 at the point $x$ and similarly for liquid two. The differential equations that need to be solved are the following \begin{align} \frac{dc_1(x)}{dx} &= -(c_1(x) + c_2(x)), \\ \frac{dc_2(x)}{dx} &= K c_1(x) c_2(x), \end{align} with $K$ a constant that we can choose. Note that the $c_1(x)$ goes down for increasing $x$ and $c_2(x)$ goes down for decreasing $x$, which is exactly as we expect. The two boundary values are $c_1(0) = 10$ and $c_2(1) = 6$. In Matlab, one models this as a boundary value problem with two boundaries and two components. By solving this problem for $K = 1$ we obtain a nice solution shown in the following figure. However, by increasing $K$ to, say, $K = 10^5$ we run into a numerical inaccuracy, see this figure. I also receive a warning message from Matlab, see the bottom of this post. The concentration $c_2(x)$ drops below zero, while that should not be possible according to the second differential equation. My intuition is that this happens because Matlab takes discrete sizes of $dx$ and computes the derivative in a point (which is large) and updates the solution, making the concentration negative. One could combat this phenomenon by increasing the maximum allowed mesh size, but this is not a solution that can be incorporated in my actual model due to the computation time and memory issues. How to make sure that both concentrations are nonnegative ? Matlab code The script that runs the bvp5c solver x = linspace(0,1,100); % the mesh yinit = [ 5 0 ]; % an initial guess for the solution for both liquids K = 1; % the factor K in the second differential equation  solinit = bvpinit(x,yinit); % setting the mesh and the initial guess   % Calling the bvp solver. First argument are the ODEs, the second argument  % are the boundary conditions at x = 0 (for liquid 1) and at x = 1 (for % liquid 2), the last argument specifies the mesh and initial guess sol = bvp5c(@(x,c)Concentrations(x,c,K),@(ca,cb)BoundaryCond(ca,cb),solinit); The function with the ODEs. The dependence on $x$ is implied in the input of the function. function dcdx = Concentrations(x,c,K)  dcdx(1) = -(c(1) + c(2)); % differential equation for liquid 1 dcdx(2) = K*c(1)*c(2); % differential equation for liquid 2 The function with boundary values. When there are two boundary values, Matlab refers to them as $a$ and $b$, which are $x = 0$ and $x = 1$, respectively. function res = BoundaryCond(ca,cb)  res = [ ca(1)-10; cb(2)-6 ]; % the concentration of liquid 1 at boundary a is 10 and the concentration of liquid 2 at boundary b is 6. Error message for $K = 10^5$ Warning: Unable to meet the tolerance without using more than 5000 mesh points.  The last mesh of 892 points and the solution are available in the output argument.  The maximum error is 536.678, while requested accuracy is 0.001.  > In bvp5c at 339",,"['ordinary-differential-equations', 'numerical-methods', 'matlab', 'systems-of-equations', 'boundary-value-problem']"
48,Asymptotic estimate of an oscillatory differential equation,Asymptotic estimate of an oscillatory differential equation,,"Let $f\in C^1(\mathbb{R} ,\mathbb{R} )$ and satisfying the condition:   $$ \forall x >0, \quad f(x)>0, \forall x<0 , \quad f(x)<0 $$ Let $(\alpha, \beta) \in \mathbb{R^2}$. 1) Show that the maximal solution of the differential equation:   $$ y''+f(y')+y=0, \quad y(0)=\alpha, \quad y'(0)=\beta $$   is defined on $[0,+\infty)$ 2) What can we say about the asymptotic behavior when the weather tends to $+ \infty$? My attempt (I know that is not enough): Physically I know that $y''+y=0$ is an ODE for harmonic oscillator and  geometrically we can draw the solution as a phase portrait. By Cauchy-Lipshitz theorem we know that the maximal solution is defined and unique. What's happen when we introduce $f(y')$ ? Can we understand the phenomenon with a drawing/Physically? Does anybody know how to obtain a good asymptotic expression ?","Let $f\in C^1(\mathbb{R} ,\mathbb{R} )$ and satisfying the condition:   $$ \forall x >0, \quad f(x)>0, \forall x<0 , \quad f(x)<0 $$ Let $(\alpha, \beta) \in \mathbb{R^2}$. 1) Show that the maximal solution of the differential equation:   $$ y''+f(y')+y=0, \quad y(0)=\alpha, \quad y'(0)=\beta $$   is defined on $[0,+\infty)$ 2) What can we say about the asymptotic behavior when the weather tends to $+ \infty$? My attempt (I know that is not enough): Physically I know that $y''+y=0$ is an ODE for harmonic oscillator and  geometrically we can draw the solution as a phase portrait. By Cauchy-Lipshitz theorem we know that the maximal solution is defined and unique. What's happen when we introduce $f(y')$ ? Can we understand the phenomenon with a drawing/Physically? Does anybody know how to obtain a good asymptotic expression ?",,['calculus']
49,Seeking an analytic solution to a first order nonlinear ordinary differential equation,Seeking an analytic solution to a first order nonlinear ordinary differential equation,,Does anyone have suggestions on how to tackle the following equation: $$\left(\frac{dy}{dx}\right)^2 = B(\sin(y(x)))^2 - C\sin(y(x)) + D $$ The constants are real and nonzero,Does anyone have suggestions on how to tackle the following equation: $$\left(\frac{dy}{dx}\right)^2 = B(\sin(y(x)))^2 - C\sin(y(x)) + D $$ The constants are real and nonzero,,['ordinary-differential-equations']
50,Is this periodic solution unique? (ODE),Is this periodic solution unique? (ODE),,"So, for the ODE \begin{align} x' = -x^{3} + \sin t, \end{align} we can show that there exists a $2\pi$ periodic solution.  To do this, we denote by \begin{align} x(t,\alpha) \end{align} The solution $x(t)$ of the ODE such that $x(0) = \alpha$.  Then, let $\alpha \in [-2,2]$.  Consider the function $f(\alpha) = x(2\pi, \alpha)$ \begin{align} x > 1 &\Rightarrow x' < 0 \\ x < 1 &\Rightarrow x' > 0. \end{align} Thus, solutions beginning in $[-2,2]$ stay there and we can use Brouwer's fixed point theorem to show that there exists a fixed point of $f$.  Therefore there is some $\alpha^{*}$ such that $f(\alpha^{*}) = \alpha^{*}$, which represents the periodic solution $x(t,\alpha)$. Now, my question: Is this $2\pi$- periodic solution unique? Are there other $2\pi$ periodic solutions of this ODE?  I have started by trying to subtract two periodic solutions from one another but this hasn't taken me anywhere productive.","So, for the ODE \begin{align} x' = -x^{3} + \sin t, \end{align} we can show that there exists a $2\pi$ periodic solution.  To do this, we denote by \begin{align} x(t,\alpha) \end{align} The solution $x(t)$ of the ODE such that $x(0) = \alpha$.  Then, let $\alpha \in [-2,2]$.  Consider the function $f(\alpha) = x(2\pi, \alpha)$ \begin{align} x > 1 &\Rightarrow x' < 0 \\ x < 1 &\Rightarrow x' > 0. \end{align} Thus, solutions beginning in $[-2,2]$ stay there and we can use Brouwer's fixed point theorem to show that there exists a fixed point of $f$.  Therefore there is some $\alpha^{*}$ such that $f(\alpha^{*}) = \alpha^{*}$, which represents the periodic solution $x(t,\alpha)$. Now, my question: Is this $2\pi$- periodic solution unique? Are there other $2\pi$ periodic solutions of this ODE?  I have started by trying to subtract two periodic solutions from one another but this hasn't taken me anywhere productive.",,"['ordinary-differential-equations', 'fixed-point-theorems', 'initial-value-problems']"
51,gradient flow programming (matlab),gradient flow programming (matlab),,"Consider $$f(x,y)= x \ y \ e^{-x^2-y^2}$$ I need to find the gradient flow of $f$ starting at the point (0,1) then (2,4). I know that this entails solving the following ODE $$\dot X(t)= - \nabla f(X(t))$$ I am using this code, but am getting strange results: x_old = [0 0] x_new = [2 6]  eps = 0.01 precision = 0.000001; fx = @(x,y) ((1-2*x.*x).*y*exp(-x.^2 - y.^2)); fy=  @(x,y) ((1-2*y.*y).*x*exp(-x.^2 - y.^2)); while abs(x_new - x_old) > precision     x_old = x_new;     x_new = x_old - eps * [fx(x_old(1),x_old(2)) fy(x_old(1),x_old(2))]; end x_new EDIT: Now am using ODE45 but am getting the initial data as a solution no matter where I start! function Fv=funsys(t,Y) Fv(1,1)=(1-2*Y(1).*Y(1)).*Y(2).*exp(-Y(1).^2 - Y(2).^2); Fv(2,1)=(1-2*Y(2).*Y(2)).*Y(1).*exp(-Y(1).^2 - Y(2).^2); Then call ode45 [tv,Yv]=ode45('funsys',[0 1],[6,2], options) What am I missing??","Consider I need to find the gradient flow of starting at the point (0,1) then (2,4). I know that this entails solving the following ODE I am using this code, but am getting strange results: x_old = [0 0] x_new = [2 6]  eps = 0.01 precision = 0.000001; fx = @(x,y) ((1-2*x.*x).*y*exp(-x.^2 - y.^2)); fy=  @(x,y) ((1-2*y.*y).*x*exp(-x.^2 - y.^2)); while abs(x_new - x_old) > precision     x_old = x_new;     x_new = x_old - eps * [fx(x_old(1),x_old(2)) fy(x_old(1),x_old(2))]; end x_new EDIT: Now am using ODE45 but am getting the initial data as a solution no matter where I start! function Fv=funsys(t,Y) Fv(1,1)=(1-2*Y(1).*Y(1)).*Y(2).*exp(-Y(1).^2 - Y(2).^2); Fv(2,1)=(1-2*Y(2).*Y(2)).*Y(1).*exp(-Y(1).^2 - Y(2).^2); Then call ode45 [tv,Yv]=ode45('funsys',[0 1],[6,2], options) What am I missing??","f(x,y)= x \ y \ e^{-x^2-y^2} f \dot X(t)= - \nabla f(X(t))","['ordinary-differential-equations', 'matlab', 'gradient-flows']"
52,Some Scaling Estimate for Heat Kernel,Some Scaling Estimate for Heat Kernel,,"NOTE. I have rewritten the question to summarize my current progress on this question.  The bounty is for completing what I have done so far, or by offering a more elegant solution probably based on some clever scaling/translation argument.  I have also slightly (I hope) clarified the wording in (b). Let $$G(x,t)=\frac{1}{(4\pi t)^{\frac{d}{2}}}e^{-\frac{|x|^{2}}{4t}}.$$ be the usual heat kernel. (a) Given $\alpha>0$ find constants $\beta$ and $C$ so that $$G(x+y,t)\leq CG(x,\beta t)$$ holds for every $x\in\mathbb{R}^{d}$, $t>0$ and $|y|\leq\alpha\sqrt{t}.$} (b) Deduce that for $f\in L^{1}$, $\lambda>0$, and $u(x,t)=(G(t,\cdot)*f)(x)$, that $$\mu\left(\left\{y:\text{s.t.}\;\exists t>0\;\text{whereby the estimate}|u(x,t)|\geq\lambda\;\text{holds whenever}\;x\in B(y,\alpha\sqrt{t})\right\}\right)\leq\frac{||f||_{L^{1}}}{\lambda}.$$ Let us for now assume $d=1$ so $$G(x,t)=(4\pi t)^{-1/2}\exp\left(\frac{-x^{2}}{4t}\right).$$ After fixing $x\in\mathbb{R}$, $t>0$, $\alpha>0,$ and restricting $|y|\leq\alpha\sqrt{t},$ we are tasked to prove the estimate $$(4\pi t)^{-1/2}\exp\left(\frac{-|x+y|^{2}}{4t}\right)\leq C(4\pi\beta t)^{-1/2}\exp\left(\frac{-|x|^{2}}{4\beta t}\right),$$ for constants $\beta=\beta(\alpha)>0$ and $C=C(\alpha)>0.$  Cancelling the factor $(4\pi t)^{-1/2}$ from both sides reduces the estimate to $$\exp\left(\frac{-|x+y|^{2}}{4t}\right)\leq C\beta^{-1/2}\exp\left(\frac{-|x|^{2}}{4\beta t}\right).$$ Since we can multiply $C$ by $\beta^{1/2}$, we can absorb the factor $\beta^{-1/2}$ into $C$.  The estimate then further reduces to $$\exp\left(\frac{-|x+y|^{2}}{4t}\right)\leq C\exp\left(\frac{-|x|^{2}}{4\beta t}\right).$$ Since $1/(4t)$ appears in both exponentials in a symmetric way, we can also ``cancel'' them to finally get $$\exp\left(-|x+y|^{2}\right) \leq C\exp\left(-|x|^{2}/\beta\right).$$ Let us maximize the left hand side subject to the constraint $|y|\leq\alpha\sqrt{t}$, for clearly the validity of the resulting estimate will imply the present one.  The left hand side is maximized when $y=-x$, and this is valid if $|x|\leq\alpha\sqrt{t}$.  However, if $|x|>\alpha\sqrt{t}$, then $y=-\text{sgn}(x)\alpha\sqrt{t}$ is the best that can be done.   Thus we get the system of inequalities $$\left\{\begin{array}{ll} 1\leq C\exp\left(-|x|^{2}/\beta\right)&\text{if}\;|x|\leq\alpha\sqrt{t},\\ \exp\left(-|x-\text{sgn}(x)\alpha\sqrt{t}|\right)\leq C\exp\left(-|x|^{2}/\beta\right)&\text{if}\;|x|>\alpha\sqrt{t}.\end{array}\right.$$ At this point I get the sense that I am going in the wrong direction, especially since we need to solve for $\beta$ and $C$ independently of $x$ and $t$.  Perhaps I should have kept the $1/(4t)$ factor in the exponentials.  Even when I did this separately, I still ended up with a similar set of inequalities with know obvious way to get rid $x$ and $t$ in them. Any help extending this argument, or providing a more elegant alternative would be highly welcomed. As for (b), the wording and all of the parameters involved still confuses me.  However, I can at least start by writing out $u(x,t)$ as $$u(x,t)=\int_{-\infty}^{\infty}(4\pi t)^{-1/2}\exp\left(\frac{-|x-y|^{2}}{4t}\right)f(y)\;dy.$$ The appearance of the $|x-y|$ in the convolution already suggests (a) will be helpful.  Since $G(x,t)\in L^{1}$ for every $t>0$ and $f\in L^{1}$ by assumption, we see $u\in L^{1}$ for every $t>0$.  Thus $f$ satisfies the weak-type estimate (special case of Chebyshev inequality) $$\mu\left(\{x:|u(x,t)|>\lambda\}\right)\leq\frac{||f||_{1}}{\lambda}.$$ The key then seems to be relating the above set with the one in (b).","NOTE. I have rewritten the question to summarize my current progress on this question.  The bounty is for completing what I have done so far, or by offering a more elegant solution probably based on some clever scaling/translation argument.  I have also slightly (I hope) clarified the wording in (b). Let $$G(x,t)=\frac{1}{(4\pi t)^{\frac{d}{2}}}e^{-\frac{|x|^{2}}{4t}}.$$ be the usual heat kernel. (a) Given $\alpha>0$ find constants $\beta$ and $C$ so that $$G(x+y,t)\leq CG(x,\beta t)$$ holds for every $x\in\mathbb{R}^{d}$, $t>0$ and $|y|\leq\alpha\sqrt{t}.$} (b) Deduce that for $f\in L^{1}$, $\lambda>0$, and $u(x,t)=(G(t,\cdot)*f)(x)$, that $$\mu\left(\left\{y:\text{s.t.}\;\exists t>0\;\text{whereby the estimate}|u(x,t)|\geq\lambda\;\text{holds whenever}\;x\in B(y,\alpha\sqrt{t})\right\}\right)\leq\frac{||f||_{L^{1}}}{\lambda}.$$ Let us for now assume $d=1$ so $$G(x,t)=(4\pi t)^{-1/2}\exp\left(\frac{-x^{2}}{4t}\right).$$ After fixing $x\in\mathbb{R}$, $t>0$, $\alpha>0,$ and restricting $|y|\leq\alpha\sqrt{t},$ we are tasked to prove the estimate $$(4\pi t)^{-1/2}\exp\left(\frac{-|x+y|^{2}}{4t}\right)\leq C(4\pi\beta t)^{-1/2}\exp\left(\frac{-|x|^{2}}{4\beta t}\right),$$ for constants $\beta=\beta(\alpha)>0$ and $C=C(\alpha)>0.$  Cancelling the factor $(4\pi t)^{-1/2}$ from both sides reduces the estimate to $$\exp\left(\frac{-|x+y|^{2}}{4t}\right)\leq C\beta^{-1/2}\exp\left(\frac{-|x|^{2}}{4\beta t}\right).$$ Since we can multiply $C$ by $\beta^{1/2}$, we can absorb the factor $\beta^{-1/2}$ into $C$.  The estimate then further reduces to $$\exp\left(\frac{-|x+y|^{2}}{4t}\right)\leq C\exp\left(\frac{-|x|^{2}}{4\beta t}\right).$$ Since $1/(4t)$ appears in both exponentials in a symmetric way, we can also ``cancel'' them to finally get $$\exp\left(-|x+y|^{2}\right) \leq C\exp\left(-|x|^{2}/\beta\right).$$ Let us maximize the left hand side subject to the constraint $|y|\leq\alpha\sqrt{t}$, for clearly the validity of the resulting estimate will imply the present one.  The left hand side is maximized when $y=-x$, and this is valid if $|x|\leq\alpha\sqrt{t}$.  However, if $|x|>\alpha\sqrt{t}$, then $y=-\text{sgn}(x)\alpha\sqrt{t}$ is the best that can be done.   Thus we get the system of inequalities $$\left\{\begin{array}{ll} 1\leq C\exp\left(-|x|^{2}/\beta\right)&\text{if}\;|x|\leq\alpha\sqrt{t},\\ \exp\left(-|x-\text{sgn}(x)\alpha\sqrt{t}|\right)\leq C\exp\left(-|x|^{2}/\beta\right)&\text{if}\;|x|>\alpha\sqrt{t}.\end{array}\right.$$ At this point I get the sense that I am going in the wrong direction, especially since we need to solve for $\beta$ and $C$ independently of $x$ and $t$.  Perhaps I should have kept the $1/(4t)$ factor in the exponentials.  Even when I did this separately, I still ended up with a similar set of inequalities with know obvious way to get rid $x$ and $t$ in them. Any help extending this argument, or providing a more elegant alternative would be highly welcomed. As for (b), the wording and all of the parameters involved still confuses me.  However, I can at least start by writing out $u(x,t)$ as $$u(x,t)=\int_{-\infty}^{\infty}(4\pi t)^{-1/2}\exp\left(\frac{-|x-y|^{2}}{4t}\right)f(y)\;dy.$$ The appearance of the $|x-y|$ in the convolution already suggests (a) will be helpful.  Since $G(x,t)\in L^{1}$ for every $t>0$ and $f\in L^{1}$ by assumption, we see $u\in L^{1}$ for every $t>0$.  Thus $f$ satisfies the weak-type estimate (special case of Chebyshev inequality) $$\mu\left(\{x:|u(x,t)|>\lambda\}\right)\leq\frac{||f||_{1}}{\lambda}.$$ The key then seems to be relating the above set with the one in (b).",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
53,Runge-Kutta method accuracy,Runge-Kutta method accuracy,,"I got a Runge-Kutta method here and I solve this system using it. So here's  Runge-Kutta stuff \begin{align}  k_1 &= f(t_n, y_n)  \\  k_2 &= f(t_n + h/2, y_n + hk_1/2)  \\  k_3 &= f(t_n+h, y_n - hk_1 + 2hk_2)  \\\hline  y_{n+1} &= y_n + h(k_1 + 4k_2 + k_3)/6  \end{align} where $h$ is step Here's my test system \begin{align}  y'_1 &= -5y_1 - 10y_2 + 14e^{-x}  \\  y'_2 &= -10y_1 - 5y_2 + 14e^{-x}  \end{align} with exact solution $y_1(x) = y_2(x) = e^{-x}$ UPD: The initial condition here is $y_1(0) = y_2(0) = 1$ I need to solve it on $[0;4]$ . Well, I thought I solved it right, because I checked how the exact solution and these approximate solution plots looks like (on the left, on the right I zoomed plot until saw difference) Also I checked how the plot of the difference between exact solution and approximate solution depending on step (let's call it e/h ) looks like. So $e/h$ it looks like this But when I checked e/h^4 dependence it looked like these I showed it to the teacher and she said that my solution is wrong, it's not suppose to be like these! I show my code to her asked for help but she said that she doesn't understand matlab :c Have I really done something wrong? And if yes what I've done wrong? And if not how to prove that I'm right? Here's my code btw Runge-Kutta method function [ res_y ] = RungeKutta(dim, size, grid, step, f1,f2,y1, y2)         k1=zeros(dim);     k2=zeros(dim);     k3=zeros(dim);          h = step;          res_y(1,1) = y1;     res_y(2,1) = y2;          for i=1: size         k1(1)= f1(grid(i),y1,y2);        k1(2)= f2(grid(i),y1,y2);         k2(1)= f1(grid(i)+h/2, y1+h*k1(1)/2, y2+h*k1(2)/2);        k2(2)= f2(grid(i)+h/2, y1+h*k1(1)/2, y2+h*k1(2)/2);         k3(1)= f1(grid(i)+h, y1-h*k1(1)+2*h*k2(1), y2-h*k1(2)+2*h*k2(2));        k3(2)= f2(grid(i)+h, y1-h*k1(1)+2*h*k2(1), y2-h*k1(2)+2*h*k2(2));         res_y(1,i+1) = y1 + h*(k1(1) + 4*k2(1) + k3(1))/6;        res_y(2,i+1) = y1 + h*(k1(2) + 4*k2(2) + k3(2))/6;         y1 = res_y(1,i+1);         y2 = res_y(2,i+1);     end   end Main method a = 0; b = 4;     h = 0.1; % step     t = a:h:b; %grid     n = 2;      m = size(t,2);                hold on;          plot(t, exp(-t),'b-')          plot(t, exp(-t),'r--')      hold off;          y1=1; y2 = 1;          f1_ptr = @f1;% out = -5 * y1 - 10 * y2 + (14)*exp(-x);     f2_ptr = @f2;% out = -10 * y1 - 5 * y2 + (14)*exp(-x);          res_y = RungeKutta(n,m-1,t,h,f1_ptr, f2_ptr,1,1);          hold on;     plot(t,res_y);          hold off;  %e/h and e/h^4 plots   fig_a = figure; set(fig_a,'name','e/h','numbertitle','off') hold on;   counter = 0;   for h=0.001:0.01:0.1     y1=1; y2 = 1;     t = a:h:b;     m = size(t,2);     counter = counter + 1;           result_appr = RungeKutta(n,m-1,t,h,f1_ptr, f2_ptr,y1,y2);     result_exact = exp(-t);              result_difference = abs(result_appr(1, :) - result_exact);          e1(counter) = max(result_difference);     e2(counter) = max(result_difference);          hh = h*h*h*h;          ehh1(counter)=e1(counter)/hh;     ehh2(counter)=e2(counter)/hh;                end;     h=0.001:0.01:0.1;     plot(h,e1,'c');     plot(h,e2,'c'); hold off;    fig_b = figure; set(fig_b ,'name','e/h^4','numbertitle','off')  hold on;     plot(h,ehh1,'r')     plot(h,ehh2,'b') hold off; f1 function function [ out ] = f1( x, y1, y2, alpha, beta ) if nargin == 3     alpha = 5;     beta = 10; end  out = -alpha * y1 - beta * y2 + (alpha + beta - 1)*exp(-x);  end f2 function function [ out ] = f2( x, y1, y2, alpha, beta ) if nargin == 3     alpha = 5;     beta = 10; end out = -beta * y1 - alpha * y2 + (alpha + beta - 1)*exp(-x);  end","I got a Runge-Kutta method here and I solve this system using it. So here's  Runge-Kutta stuff where is step Here's my test system with exact solution UPD: The initial condition here is I need to solve it on . Well, I thought I solved it right, because I checked how the exact solution and these approximate solution plots looks like (on the left, on the right I zoomed plot until saw difference) Also I checked how the plot of the difference between exact solution and approximate solution depending on step (let's call it e/h ) looks like. So it looks like this But when I checked e/h^4 dependence it looked like these I showed it to the teacher and she said that my solution is wrong, it's not suppose to be like these! I show my code to her asked for help but she said that she doesn't understand matlab :c Have I really done something wrong? And if yes what I've done wrong? And if not how to prove that I'm right? Here's my code btw Runge-Kutta method function [ res_y ] = RungeKutta(dim, size, grid, step, f1,f2,y1, y2)         k1=zeros(dim);     k2=zeros(dim);     k3=zeros(dim);          h = step;          res_y(1,1) = y1;     res_y(2,1) = y2;          for i=1: size         k1(1)= f1(grid(i),y1,y2);        k1(2)= f2(grid(i),y1,y2);         k2(1)= f1(grid(i)+h/2, y1+h*k1(1)/2, y2+h*k1(2)/2);        k2(2)= f2(grid(i)+h/2, y1+h*k1(1)/2, y2+h*k1(2)/2);         k3(1)= f1(grid(i)+h, y1-h*k1(1)+2*h*k2(1), y2-h*k1(2)+2*h*k2(2));        k3(2)= f2(grid(i)+h, y1-h*k1(1)+2*h*k2(1), y2-h*k1(2)+2*h*k2(2));         res_y(1,i+1) = y1 + h*(k1(1) + 4*k2(1) + k3(1))/6;        res_y(2,i+1) = y1 + h*(k1(2) + 4*k2(2) + k3(2))/6;         y1 = res_y(1,i+1);         y2 = res_y(2,i+1);     end   end Main method a = 0; b = 4;     h = 0.1; % step     t = a:h:b; %grid     n = 2;      m = size(t,2);                hold on;          plot(t, exp(-t),'b-')          plot(t, exp(-t),'r--')      hold off;          y1=1; y2 = 1;          f1_ptr = @f1;% out = -5 * y1 - 10 * y2 + (14)*exp(-x);     f2_ptr = @f2;% out = -10 * y1 - 5 * y2 + (14)*exp(-x);          res_y = RungeKutta(n,m-1,t,h,f1_ptr, f2_ptr,1,1);          hold on;     plot(t,res_y);          hold off;  %e/h and e/h^4 plots   fig_a = figure; set(fig_a,'name','e/h','numbertitle','off') hold on;   counter = 0;   for h=0.001:0.01:0.1     y1=1; y2 = 1;     t = a:h:b;     m = size(t,2);     counter = counter + 1;           result_appr = RungeKutta(n,m-1,t,h,f1_ptr, f2_ptr,y1,y2);     result_exact = exp(-t);              result_difference = abs(result_appr(1, :) - result_exact);          e1(counter) = max(result_difference);     e2(counter) = max(result_difference);          hh = h*h*h*h;          ehh1(counter)=e1(counter)/hh;     ehh2(counter)=e2(counter)/hh;                end;     h=0.001:0.01:0.1;     plot(h,e1,'c');     plot(h,e2,'c'); hold off;    fig_b = figure; set(fig_b ,'name','e/h^4','numbertitle','off')  hold on;     plot(h,ehh1,'r')     plot(h,ehh2,'b') hold off; f1 function function [ out ] = f1( x, y1, y2, alpha, beta ) if nargin == 3     alpha = 5;     beta = 10; end  out = -alpha * y1 - beta * y2 + (alpha + beta - 1)*exp(-x);  end f2 function function [ out ] = f2( x, y1, y2, alpha, beta ) if nargin == 3     alpha = 5;     beta = 10; end out = -beta * y1 - alpha * y2 + (alpha + beta - 1)*exp(-x);  end","\begin{align}
 k_1 &= f(t_n, y_n) 
\\
 k_2 &= f(t_n + h/2, y_n + hk_1/2) 
\\
 k_3 &= f(t_n+h, y_n - hk_1 + 2hk_2) 
\\\hline
 y_{n+1} &= y_n + h(k_1 + 4k_2 + k_3)/6 
\end{align} h \begin{align}
 y'_1 &= -5y_1 - 10y_2 + 14e^{-x} 
\\
 y'_2 &= -10y_1 - 5y_2 + 14e^{-x} 
\end{align} y_1(x) = y_2(x) = e^{-x} y_1(0) = y_2(0) = 1 [0;4] e/h","['ordinary-differential-equations', 'numerical-methods', 'matlab', 'runge-kutta-methods']"
54,are all dynamical systems described by differential equations?,are all dynamical systems described by differential equations?,,"we defined in lecture a dynamical System as a one-parameter family of maps $\phi^t:M\rightarrow M$ such that $\phi^{t+s}=\phi^t\circ\phi^s$ and $\phi^0=Id$, where $M$ is some (smooth) manifold and $s,t\in (a,b)\subset\mathbb{R}$. Of course, if we consider some vector field $V:M\rightarrow TM$, then the flow of that vector field around some point $x_0\in M$ is a (local) dynamical system. Now I'm wondering if all dynamical systems can be described that way. Can we find for all dynamical systems $\phi^t$ a vector field $V$, s.t. $\phi^t$ is the flow of $V$? Maybe you know some argument. Regards","we defined in lecture a dynamical System as a one-parameter family of maps $\phi^t:M\rightarrow M$ such that $\phi^{t+s}=\phi^t\circ\phi^s$ and $\phi^0=Id$, where $M$ is some (smooth) manifold and $s,t\in (a,b)\subset\mathbb{R}$. Of course, if we consider some vector field $V:M\rightarrow TM$, then the flow of that vector field around some point $x_0\in M$ is a (local) dynamical system. Now I'm wondering if all dynamical systems can be described that way. Can we find for all dynamical systems $\phi^t$ a vector field $V$, s.t. $\phi^t$ is the flow of $V$? Maybe you know some argument. Regards",,"['ordinary-differential-equations', 'dynamical-systems']"
55,Determining what technique to use to solve a nonhomogeneous second-order PDE,Determining what technique to use to solve a nonhomogeneous second-order PDE,,"$$  u_{tt}-u_{xx}=1-x, \space\space\space x \in (0,1) $$  with the following boundary conditions: $$ u(x,0) = x^2(1-x), $$ $$ u_{t}(x,0) = 0, $$ $$ u_{x}(0,t) = 0, $$ $$ u(1,t) = 0 $$ $u_{tt}$ and $u_{xx}$ are defined respectively as the second partial derivative of $u(x,t)$ with respect to t, and the second partial derivative of $u(x,t)$ with respect to x. My first question is, I see some formulations of the wave equation use a $c^2$ coefficient in front of $u_{xx}$. How does this impact using Duhamel's integral, since that uses a 1/2c term and in my case, c = 0. What does c represent? I suspect there is a change of variables that would allow me to make this equation homogeneous, and possibly even make the boundary conditions homogeneous. I am confused on how to proceed because in every different solution I've found there seem to be extremely varied and exotic methods of solving PDEs of this type and compared to ordinary differential equations I am having trouble seeing the linear progression of these different solution types. Any help on how to proceed would be greatly appreciated.","$$  u_{tt}-u_{xx}=1-x, \space\space\space x \in (0,1) $$  with the following boundary conditions: $$ u(x,0) = x^2(1-x), $$ $$ u_{t}(x,0) = 0, $$ $$ u_{x}(0,t) = 0, $$ $$ u(1,t) = 0 $$ $u_{tt}$ and $u_{xx}$ are defined respectively as the second partial derivative of $u(x,t)$ with respect to t, and the second partial derivative of $u(x,t)$ with respect to x. My first question is, I see some formulations of the wave equation use a $c^2$ coefficient in front of $u_{xx}$. How does this impact using Duhamel's integral, since that uses a 1/2c term and in my case, c = 0. What does c represent? I suspect there is a change of variables that would allow me to make this equation homogeneous, and possibly even make the boundary conditions homogeneous. I am confused on how to proceed because in every different solution I've found there seem to be extremely varied and exotic methods of solving PDEs of this type and compared to ordinary differential equations I am having trouble seeing the linear progression of these different solution types. Any help on how to proceed would be greatly appreciated.",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative', 'heat-equation']"
56,Can we reconstruct a DE from its general solution?,Can we reconstruct a DE from its general solution?,,"If we think of a differential equation as being a 'problem' and the set of all solutions to the DE as being the 'general solution,' then my question is this: Question: Under what circumstances can we reconstruct the problem from the general solution? Lets make that more precise. Definition 1 . Whenever $X$ is a subset of $\mathbb{R}$ and $f : \mathbb{R}^n \times X \rightarrow \mathbb{R}$ a function, define that $S(f)$ is the set of all solutions $y : X \rightarrow \mathbb{R}$ to the ODE $$y^{(n)}(x) = f(y^{(n-1)}(x),\cdots,y^{(0)}(x),x),\quad x \in X.$$ We can think of $f$ as the 'problem' and $S(f)$ as the 'general solution.' Question: What conditions need to be added to $f$ and/or $X$ in the above definition to guarantee that the solution function $S$ is an injection? (That is, one-to-one). Remark . This is still a rather imprecise way of formulating the question, and any suggestions for making it better would be appreciated. Please leave a comment!","If we think of a differential equation as being a 'problem' and the set of all solutions to the DE as being the 'general solution,' then my question is this: Question: Under what circumstances can we reconstruct the problem from the general solution? Lets make that more precise. Definition 1 . Whenever $X$ is a subset of $\mathbb{R}$ and $f : \mathbb{R}^n \times X \rightarrow \mathbb{R}$ a function, define that $S(f)$ is the set of all solutions $y : X \rightarrow \mathbb{R}$ to the ODE $$y^{(n)}(x) = f(y^{(n-1)}(x),\cdots,y^{(0)}(x),x),\quad x \in X.$$ We can think of $f$ as the 'problem' and $S(f)$ as the 'general solution.' Question: What conditions need to be added to $f$ and/or $X$ in the above definition to guarantee that the solution function $S$ is an injection? (That is, one-to-one). Remark . This is still a rather imprecise way of formulating the question, and any suggestions for making it better would be appreciated. Please leave a comment!",,['ordinary-differential-equations']
57,using linearity to solve ode,using linearity to solve ode,,Solve the following diÔ¨Äerential equation for x(t) \begin{equation} \frac{dx}{dt}+ax= b\sin(kt)  \\ \end{equation} subject to the initial condition $x(0) = 0$. you may use \begin{equation} e^{ikt} = \cos(kt) + i \sin(kt) \end{equation} my question is how linearity used in order for the solution to only contain the real part? and why are we allowed to substitute \begin{equation}  e^{ikt}  \end{equation} for $b\sin(kt)$ in order to solve?,Solve the following diÔ¨Äerential equation for x(t) \begin{equation} \frac{dx}{dt}+ax= b\sin(kt)  \\ \end{equation} subject to the initial condition $x(0) = 0$. you may use \begin{equation} e^{ikt} = \cos(kt) + i \sin(kt) \end{equation} my question is how linearity used in order for the solution to only contain the real part? and why are we allowed to substitute \begin{equation}  e^{ikt}  \end{equation} for $b\sin(kt)$ in order to solve?,,['ordinary-differential-equations']
58,"A second order differential equation, $y''=y^{-3}$","A second order differential equation,",y''=y^{-3},"The exercise is about to solve $y''=1/y^3$ for $y=y(t)$ with initial values $y(0)=1$ and $y'(0)=2$. My attempts: I wrote up it as a first order diff.equation with $2$ variables by introducing $x:=y'$, so that we are looking for (the $y$-coordinate of) the trajectory of the vector field $(1/y^3, x)$, I could draw some vectors, and could 'guess' how the solution will look like. Look for the solution in form $y=c\,t^\alpha$. I got that $\alpha=1/2$ and $c^4=-4$, that is $c=\pm1\pm i$. However, I guess, real function was asked. I guess I'm missing something I should know about solving such a differential equation.","The exercise is about to solve $y''=1/y^3$ for $y=y(t)$ with initial values $y(0)=1$ and $y'(0)=2$. My attempts: I wrote up it as a first order diff.equation with $2$ variables by introducing $x:=y'$, so that we are looking for (the $y$-coordinate of) the trajectory of the vector field $(1/y^3, x)$, I could draw some vectors, and could 'guess' how the solution will look like. Look for the solution in form $y=c\,t^\alpha$. I got that $\alpha=1/2$ and $c^4=-4$, that is $c=\pm1\pm i$. However, I guess, real function was asked. I guess I'm missing something I should know about solving such a differential equation.",,['ordinary-differential-equations']
59,Elementary Proof Involving Wronskian and Existence and Uniqueness,Elementary Proof Involving Wronskian and Existence and Uniqueness,,"I‚Äôm in need of some help to understand an elementary proof involving the Wronskian of $n$ vector functions and the following existence and uniqueness theorem. Existence and Uniqueness Suppose $A(t)$ and $f(t)$ are continuous on an open interval $I$ that contains the point $t_0$ . Then, for any choice of initial vector $x_0$ , there exists a unique solution $x(t)$ on the whole interval $I$ to the initial value problem $$x‚Äô(t) = A(t)x(t) + f(t) \space \space , \space \space x(t_0) = x_0$$ The proof I‚Äôm reading is about how if $x_1, \space \ldots \space, x_n$ are linearly independent solutions on $I$ then the Wronskian is never zero. So I understand that if the Wronskian is zero this implies that the vector functions are linearly dependent which implies that $$c_1 x_1(t_0)  \space + \space \ldots \space + \space c_n x_n(t_0) = 0$$ At some point $t_0$ (ie a non-trivial solution exists at some point). Now here is what my textbook goes on to say: ‚Ä¶ However, $c_1 x_1(t_0)  \space + \space \ldots \space + \space c_n x_n(t_0) = 0$ and the vector function $z(t) \equiv 0$ are both solutions to $x‚Äô = Ax$ on I, and they agree at the point $t_0$ . So these solutions must be identical on $I$ according to the existence and uniqueness theorem (What!?). So this may be obvious to some, but I‚Äôm having a hard time understanding what the existence and uniqueness theorem means in this context and how it proves that it is zero for all $t$ . Any explanation would be appreciated.","I‚Äôm in need of some help to understand an elementary proof involving the Wronskian of vector functions and the following existence and uniqueness theorem. Existence and Uniqueness Suppose and are continuous on an open interval that contains the point . Then, for any choice of initial vector , there exists a unique solution on the whole interval to the initial value problem The proof I‚Äôm reading is about how if are linearly independent solutions on then the Wronskian is never zero. So I understand that if the Wronskian is zero this implies that the vector functions are linearly dependent which implies that At some point (ie a non-trivial solution exists at some point). Now here is what my textbook goes on to say: ‚Ä¶ However, and the vector function are both solutions to on I, and they agree at the point . So these solutions must be identical on according to the existence and uniqueness theorem (What!?). So this may be obvious to some, but I‚Äôm having a hard time understanding what the existence and uniqueness theorem means in this context and how it proves that it is zero for all . Any explanation would be appreciated.","n A(t) f(t) I t_0 x_0 x(t) I x‚Äô(t) = A(t)x(t) + f(t) \space \space , \space \space x(t_0) = x_0 x_1, \space \ldots \space, x_n I c_1 x_1(t_0)  \space + \space \ldots \space + \space c_n x_n(t_0) = 0 t_0 c_1 x_1(t_0)  \space + \space \ldots \space + \space c_n x_n(t_0) = 0 z(t) \equiv 0 x‚Äô = Ax t_0 I t",['ordinary-differential-equations']
60,Why does root multiplicity change the factors in partial fractions and homogenous differential equations?,Why does root multiplicity change the factors in partial fractions and homogenous differential equations?,,"Why does a root with multiplicity larger than one generate different terms in partial fraction expansions and in finding the solution of homogeneous linear differential equations? For example, consider the following expansion: $$\frac{1}{s^2 - 4s + 4} = \frac{A}{s - 2} + \frac{B}{(s - 2)^2}$$ I can see why the denominators in the terms must be different, but not where the exponent comes from. I was taught this as ""truth"", but I don't know how to get there myself, or how to get a general formula. The same is true for: $$y'' - 4y' + 4y = 0$$ Leading to a general solution: $$c_1 e^{2t} + c_2 te^{2t}$$ I don't know how the t appears and how what to do if the root's multiplicity was 3 (for example).","Why does a root with multiplicity larger than one generate different terms in partial fraction expansions and in finding the solution of homogeneous linear differential equations? For example, consider the following expansion: $$\frac{1}{s^2 - 4s + 4} = \frac{A}{s - 2} + \frac{B}{(s - 2)^2}$$ I can see why the denominators in the terms must be different, but not where the exponent comes from. I was taught this as ""truth"", but I don't know how to get there myself, or how to get a general formula. The same is true for: $$y'' - 4y' + 4y = 0$$ Leading to a general solution: $$c_1 e^{2t} + c_2 te^{2t}$$ I don't know how the t appears and how what to do if the root's multiplicity was 3 (for example).",,"['ordinary-differential-equations', 'partial-fractions']"
61,Show that every orbit is forward bounded,Show that every orbit is forward bounded,,This was a question in an exam on ODEs: For the planar system $$ \begin{align} x' &= 3y\\  y' &=-x^3+x-4y \end{align} $$ show that every orbit is forward bounded. I have no idea how to show this. What can I do?,This was a question in an exam on ODEs: For the planar system $$ \begin{align} x' &= 3y\\  y' &=-x^3+x-4y \end{align} $$ show that every orbit is forward bounded. I have no idea how to show this. What can I do?,,['ordinary-differential-equations']
62,Finding Lyapunov functions,Finding Lyapunov functions,,"I came across the following question in one of my professor's past exams: Find a Lyapunov function for $(0,0)$ in the system: $$ \left\{ \begin{array}{ll} \dot{x} = -x -2y + x^2\\ \dot{y} = x - 4y + xy \end{array}\right. $$ I know there is no formula for finding Lyapunov functions for a system, so how do I start solving such problems? Thanks!","I came across the following question in one of my professor's past exams: Find a Lyapunov function for $(0,0)$ in the system: $$ \left\{ \begin{array}{ll} \dot{x} = -x -2y + x^2\\ \dot{y} = x - 4y + xy \end{array}\right. $$ I know there is no formula for finding Lyapunov functions for a system, so how do I start solving such problems? Thanks!",,['ordinary-differential-equations']
63,Frechet Differentiabilty of a Functional defined on some Sobolev Space,Frechet Differentiabilty of a Functional defined on some Sobolev Space,,"How can I prove that the following Functional is Frechet Differentiable and that the Frechet derivative is continuous? $$     I(u)=\int_\Omega |u|^{p+1} dx , \quad 1<p<\frac{n+2}{n-2}      $$ where $\Omega$ is a bounded open subset of $\mathbb{R}^n$ and $I$ is a functional on $H^1_0(\Omega).$","How can I prove that the following Functional is Frechet Differentiable and that the Frechet derivative is continuous? $$     I(u)=\int_\Omega |u|^{p+1} dx , \quad 1<p<\frac{n+2}{n-2}      $$ where $\Omega$ is a bounded open subset of $\mathbb{R}^n$ and $I$ is a functional on $H^1_0(\Omega).$",,"['ordinary-differential-equations', 'calculus-of-variations']"
64,How to solve $x'(t)=\frac{x+t}{2t-x}$?,How to solve ?,x'(t)=\frac{x+t}{2t-x},"I wish to solve $x'(t)=\frac{x+t}{2t-x}$ with the initial condition $x(1)=0$. I noted that $x'(t)=f(\frac{x}{t})$ where $f(y)=\frac{y+1}{2-y}$ so I denoted $y(t)=\frac{x(t)}{t}$ and got that $y'(t)=\frac{f(y)-y}{t}$ so I can write something like $\frac{dy}{dt}=\frac{f(y)-y}{t}$ so $\frac{dy}{f(y)-y}=\frac{dt}{t}$ . Here I am a bit stuck, I know I should do something like take integrals on both sides, but I am having trouble with the initial condition. In this exercise I can leave integrals in the answer so I would like to know how to get the solution with the integral, I think this requires to calculate the boundaries of some integral (maybe of $\frac{f(y)-y}{t}$ ?) How can I continue to get the solution with integral that satisfies the initial condition ? Help is appriciated!","I wish to solve $x'(t)=\frac{x+t}{2t-x}$ with the initial condition $x(1)=0$. I noted that $x'(t)=f(\frac{x}{t})$ where $f(y)=\frac{y+1}{2-y}$ so I denoted $y(t)=\frac{x(t)}{t}$ and got that $y'(t)=\frac{f(y)-y}{t}$ so I can write something like $\frac{dy}{dt}=\frac{f(y)-y}{t}$ so $\frac{dy}{f(y)-y}=\frac{dt}{t}$ . Here I am a bit stuck, I know I should do something like take integrals on both sides, but I am having trouble with the initial condition. In this exercise I can leave integrals in the answer so I would like to know how to get the solution with the integral, I think this requires to calculate the boundaries of some integral (maybe of $\frac{f(y)-y}{t}$ ?) How can I continue to get the solution with integral that satisfies the initial condition ? Help is appriciated!",,['ordinary-differential-equations']
65,Tools for plotting behavior of differential equations,Tools for plotting behavior of differential equations,,"I have several ODEs describing the behavior of dividing particles (e.g., How to model multi-step cell differentiation ). I would like to plot these ODEs based on changing values of p over time.  I would appreciate any advice on what graphics program would be suitable for this task. Thanks.","I have several ODEs describing the behavior of dividing particles (e.g., How to model multi-step cell differentiation ). I would like to plot these ODEs based on changing values of p over time.  I would appreciate any advice on what graphics program would be suitable for this task. Thanks.",,"['ordinary-differential-equations', 'math-software', 'graphing-functions']"
66,Is there an analytical solution to this nonlinear ODE?,Is there an analytical solution to this nonlinear ODE?,,"Is there an analytical solution to the nonlinear ODE $$\frac{dx}{d\theta} = -\sqrt{\frac{x^2}{4\cos^2\theta} - \cos^2\theta}$$ over $\theta \in [0, \pi/2]$ with initial condition $x(0) = 2$? Using the substitution $y = \sin\theta$, I was able to transform this into $$\frac{dx}{dy} = -\sqrt{\frac{x^2-4(1-y^2)^2}{4(1-y^2)^2}},$$ but couldn't get much further. Context: On a typical globe, one can only see half of the earth at a time. Waldo Tobler's slides on "" Unusual Map Projections "" mention the following amusing solution to this problem: simply wrap the earth around the globe twice (see image ). In trying to find an aesthetically pleasing conformal version of this construction by changing the shape of the globe, I found myself needing to solve the above ODE to determine its cross section; specifically, mapping latitude $\theta$ to distance from the polar axis, $x(\theta)$. It can be solved numerically, of course, and that's what I did. I'm just curious about whether an analytical solution exists. The numerical solution looks a lot like $x(\theta) = 2\sqrt{\cos\theta}$, but isn't exactly the same. In case you're curious, here is a render of the final result of my hacking on all this.","Is there an analytical solution to the nonlinear ODE $$\frac{dx}{d\theta} = -\sqrt{\frac{x^2}{4\cos^2\theta} - \cos^2\theta}$$ over $\theta \in [0, \pi/2]$ with initial condition $x(0) = 2$? Using the substitution $y = \sin\theta$, I was able to transform this into $$\frac{dx}{dy} = -\sqrt{\frac{x^2-4(1-y^2)^2}{4(1-y^2)^2}},$$ but couldn't get much further. Context: On a typical globe, one can only see half of the earth at a time. Waldo Tobler's slides on "" Unusual Map Projections "" mention the following amusing solution to this problem: simply wrap the earth around the globe twice (see image ). In trying to find an aesthetically pleasing conformal version of this construction by changing the shape of the globe, I found myself needing to solve the above ODE to determine its cross section; specifically, mapping latitude $\theta$ to distance from the polar axis, $x(\theta)$. It can be solved numerically, of course, and that's what I did. I'm just curious about whether an analytical solution exists. The numerical solution looks a lot like $x(\theta) = 2\sqrt{\cos\theta}$, but isn't exactly the same. In case you're curious, here is a render of the final result of my hacking on all this.",,['ordinary-differential-equations']
67,The derivatives of the tangent generate an infinite set of linearly independent elements,The derivatives of the tangent generate an infinite set of linearly independent elements,,"The method of undetermined coefficients requires, that the input of the ODE be a function that returns to some linear combination of itself if you take the derivative enough times. Tangent is offered as an example of a function that fails this condition. The derivatives of the tangent produce functions containing higher and higher powers of $\tan^n x$. My books says they are independent, or rather, that new independent terms will always be produced by taking the derivative. I tried to check this idea with the wronskian of $\tan^n x$ and $\tan^m x$, but I got a function that is often zero, $(m-n)\tan^{n+m-1}x\sec^2 x$ so it is inconclusive. How can I simply prove independence? Also, why is the tangent so badly behaved compared to sin and cosine?  I don't know if there is a succinct answer to that, but I thought I'd ask anyway.","The method of undetermined coefficients requires, that the input of the ODE be a function that returns to some linear combination of itself if you take the derivative enough times. Tangent is offered as an example of a function that fails this condition. The derivatives of the tangent produce functions containing higher and higher powers of $\tan^n x$. My books says they are independent, or rather, that new independent terms will always be produced by taking the derivative. I tried to check this idea with the wronskian of $\tan^n x$ and $\tan^m x$, but I got a function that is often zero, $(m-n)\tan^{n+m-1}x\sec^2 x$ so it is inconclusive. How can I simply prove independence? Also, why is the tangent so badly behaved compared to sin and cosine?  I don't know if there is a succinct answer to that, but I thought I'd ask anyway.",,['ordinary-differential-equations']
68,Euler's Method for slope fields,Euler's Method for slope fields,,"I tried to do euler's method with this problem: $$\frac{dw}{dt} = (3-w)(w+1),\quad w(0) = 0,\quad \text{and }t\in [0,5].\quad \Delta t = 0.5.$$ But am getting weird results like for $y_2 = 5.25$, $y_3= -15.84375$; but checked against the book but this is not correct. Can you please work out some iterations? $y_2 = 1.5 + (3-1.5)(1.5+1)(1) = 5.25$, for example","I tried to do euler's method with this problem: $$\frac{dw}{dt} = (3-w)(w+1),\quad w(0) = 0,\quad \text{and }t\in [0,5].\quad \Delta t = 0.5.$$ But am getting weird results like for $y_2 = 5.25$, $y_3= -15.84375$; but checked against the book but this is not correct. Can you please work out some iterations? $y_2 = 1.5 + (3-1.5)(1.5+1)(1) = 5.25$, for example",,"['calculus', 'ordinary-differential-equations']"
69,"How to solve $\delta''(\vec{k},\tau)+\mathcal{H}(\tau)\delta'(\vec{k},\tau)-\dfrac{3}{2}\Omega_m(\tau)\mathcal{H}^2(\tau)\delta(\vec{k},\tau)=0$",How to solve,"\delta''(\vec{k},\tau)+\mathcal{H}(\tau)\delta'(\vec{k},\tau)-\dfrac{3}{2}\Omega_m(\tau)\mathcal{H}^2(\tau)\delta(\vec{k},\tau)=0","While self-studying a set of notes about Cosmology, I have encountered the following claim: We have the linear growth equation: $$\delta''(\vec{k},\tau)+\mathcal{H}(\tau)\delta'(\vec{k},\tau)-\dfrac{3}{2}\Omega_m(\tau)\mathcal{H}^2(\tau)\delta(\vec{k},\tau)=0 \label{1}\tag{1}$$ We can easily confirm the above differential equation has a growing and a decaying mode solution: $$\delta(\vec{k},\tau)=D_+(\tau)\delta_{+,0}(\vec{k})+D_-(\tau)\delta_{-,0}(\vec{k}) \label{2}\tag{2}$$ where $D_-(\tau)=D_{-,0}H=D_{-,0}\mathcal{H}/a$ . The growing mode solution can then be obtained as: $$D_+(\tau)=D_{+,0}H(\tau)\int^{a(\tau)}_0\dfrac{da'}{\mathcal{H}^3(a')} \label{3}\tag{3}$$ First, I will explain the notation and the relationships between the different variables, that have nothing to do with mathematics and come from the physics background of the question: $t$ and $\tau$ are two different measurements of time, respectively named coordinate time and conformal time. $a=a(t)=a(\tau)$ is a scalar that depends on time. $H=\dfrac{1}{a}\dfrac{da}{dt}$ and $\mathcal{H}=\dfrac{1}{a}\dfrac{da}{d\tau}$ , where $d\tau=\dfrac{dt}{a}$ , and therefore $\mathcal{H}=aH$ . The following equality holds: $$\dfrac{\partial\mathcal{H}}{\partial\tau}=\mathcal{H}^2\bigg(1-\dfrac{3}{2}\Omega_m\bigg)$$ I am completely lost here. First, I don't know how to solve the differential equation itself, and then, it is not clear to me either how to prove that the expression given for $D_+(\tau)$ is a solution. I assume that the notes use a very confusing notation, since I understand that the $a'$ inside the integral is not $da/d\tau$ , but simply a way to rename $a$ to that it is not represented by the same letter than the upper limit of the integral itself. So we would actually have, if I am correct: $$D_+(\tau)=D_{+,0}H(\tau)\int^{a(\tau)}_0\dfrac{da}{\mathcal{H}^3(a)}$$ In order to prove that this is a solution of \eqref{1}, we would need to find its first and second derivatives with respect to $\tau$ . However, when trying to do this by applying Leibniz rule for the derivation of an integral, given in the case of  multiple parameters by: $$\dfrac{\partial F}{\partial\lambda_k}=\int^{b(\lambda)}_{a(\lambda)}\dfrac{\partial f}{\partial\lambda_k}dx+\dfrac{\partial b}{\partial\lambda_k}\cdot f(\lambda,b(\lambda))-\dfrac{\partial a}{\partial\lambda_k}\cdot f(\lambda,a(\lambda))\ \ \ \ \text{where}\ \ \ \ F(\lambda)=\int^{b(\lambda)}_{a(\lambda)}f(\lambda,x)dx$$ I encounter a problem: we don't have here an independent integration variable and parameter, but rather an integration variable $a$ that depends on the parameter $\tau$ . So I have no idea how to proceed. I assume that the reasoning behind \eqref{2} is related to the fact that there are no derivatives with respect to $\vec{k}$ in \eqref{1}, so the solution can be written as a constant that only depends on $\vec{k}$ multiplied by the solution that depends on $\tau$ , but I don't know why we can separate that solution into a growing and a decaying function of $\tau$ . Any help would be greatly appreciated. I've dwelled on this for weeks and can't find an explanation or a way to solve the differential equation. Thank you very much in advance!","While self-studying a set of notes about Cosmology, I have encountered the following claim: We have the linear growth equation: We can easily confirm the above differential equation has a growing and a decaying mode solution: where . The growing mode solution can then be obtained as: First, I will explain the notation and the relationships between the different variables, that have nothing to do with mathematics and come from the physics background of the question: and are two different measurements of time, respectively named coordinate time and conformal time. is a scalar that depends on time. and , where , and therefore . The following equality holds: I am completely lost here. First, I don't know how to solve the differential equation itself, and then, it is not clear to me either how to prove that the expression given for is a solution. I assume that the notes use a very confusing notation, since I understand that the inside the integral is not , but simply a way to rename to that it is not represented by the same letter than the upper limit of the integral itself. So we would actually have, if I am correct: In order to prove that this is a solution of \eqref{1}, we would need to find its first and second derivatives with respect to . However, when trying to do this by applying Leibniz rule for the derivation of an integral, given in the case of  multiple parameters by: I encounter a problem: we don't have here an independent integration variable and parameter, but rather an integration variable that depends on the parameter . So I have no idea how to proceed. I assume that the reasoning behind \eqref{2} is related to the fact that there are no derivatives with respect to in \eqref{1}, so the solution can be written as a constant that only depends on multiplied by the solution that depends on , but I don't know why we can separate that solution into a growing and a decaying function of . Any help would be greatly appreciated. I've dwelled on this for weeks and can't find an explanation or a way to solve the differential equation. Thank you very much in advance!","\delta''(\vec{k},\tau)+\mathcal{H}(\tau)\delta'(\vec{k},\tau)-\dfrac{3}{2}\Omega_m(\tau)\mathcal{H}^2(\tau)\delta(\vec{k},\tau)=0 \label{1}\tag{1} \delta(\vec{k},\tau)=D_+(\tau)\delta_{+,0}(\vec{k})+D_-(\tau)\delta_{-,0}(\vec{k}) \label{2}\tag{2} D_-(\tau)=D_{-,0}H=D_{-,0}\mathcal{H}/a D_+(\tau)=D_{+,0}H(\tau)\int^{a(\tau)}_0\dfrac{da'}{\mathcal{H}^3(a')} \label{3}\tag{3} t \tau a=a(t)=a(\tau) H=\dfrac{1}{a}\dfrac{da}{dt} \mathcal{H}=\dfrac{1}{a}\dfrac{da}{d\tau} d\tau=\dfrac{dt}{a} \mathcal{H}=aH \dfrac{\partial\mathcal{H}}{\partial\tau}=\mathcal{H}^2\bigg(1-\dfrac{3}{2}\Omega_m\bigg) D_+(\tau) a' da/d\tau a D_+(\tau)=D_{+,0}H(\tau)\int^{a(\tau)}_0\dfrac{da}{\mathcal{H}^3(a)} \tau \dfrac{\partial F}{\partial\lambda_k}=\int^{b(\lambda)}_{a(\lambda)}\dfrac{\partial f}{\partial\lambda_k}dx+\dfrac{\partial b}{\partial\lambda_k}\cdot f(\lambda,b(\lambda))-\dfrac{\partial a}{\partial\lambda_k}\cdot f(\lambda,a(\lambda))\ \ \ \ \text{where}\ \ \ \ F(\lambda)=\int^{b(\lambda)}_{a(\lambda)}f(\lambda,x)dx a \tau \vec{k} \vec{k} \tau \tau","['ordinary-differential-equations', 'leibniz-integral-rule']"
70,Is linearisation of ODE around a stable equilibrium always justified?,Is linearisation of ODE around a stable equilibrium always justified?,,"Let $f:\mathbb R^n\to \mathbb R^n$ smooth. Let $\hat y\in\mathbb R^n$ be a stable equilibrium point for the ODE $y'(t)=f(y(t))$ . Namely : $f(\hat y)=0$ , for every $\epsilon>0$ there exists $\delta>0$ such that $|y(t)-\hat y|<\epsilon\ \ \forall\,t\geq0$ provided that $|y(0)-\hat y|<\delta$ . Now fix $\epsilon>0$ , fix $y_0$ such that $|y_0-\hat y|<\delta$ , and consider the Cauchy problem $$ y'(t)=f(y(t)) \ ,\quad y(0)=y_0 \ .$$ Denote by $A:=D f(\hat y)$ the jacobian matrix of $f$ at the equilibrium point $\hat y$ and consider the linearized problem around $\hat y$ : $$ z'(t) = A\, z(t)\ ,\quad z(0)=y_0-\hat y\ .$$ Suppose the matrix $A$ is negative definite . I would like to say that the linearized problem is a good approximation for the original one. A formal question could be something like: is there a uniform constant $C$ such that $$ |y(t)-\hat y-z(t)| \leq C\,\epsilon^2 \quad\forall t\geq0 \quad\forall y_0\in B(\hat y,\delta) \ ?$$ Tentative solution . Let $r>0$ and $\epsilon\in(0,r)$ . By Taylor expansion of $f$ around $\hat y$ , there exists a constant $C=C(r)$ such that $$ f(y) = A\,(y-\hat y) + \omega(y)\ ,\quad |\omega(y)|\leq C\, |y-\hat y|^2\ \forall y\in B(\hat y,r)\ .$$ Therefore $$ \frac{d}{d t}\big(y(t)-\hat y-z(t)\big) \,=\, A\,\big(y(t)-\hat y-z(t)\big) \,+\, \omega(y(t))\ .$$ Solving this as a linear equation in $y-\hat y-z$ with non-homegenous term $\omega(y(t))$ gives $$ y(t)-\hat y-z(t) \,=\, e^{tA}\,\big(y(0)-\hat y-z(0)\big) \,+\, \int_0^te^{(t-s) A}\,\omega(y(s))\,ds$$ The first term of the sum is zero. I'd like to bound the second term using the stability of equilibrium, indeed: $$ |\omega(y(s))| \leq C\, |y(s)-\hat y|^2 \leq C\,\epsilon^2 \quad\forall s\geq0 \,$$ and if could take this term out of the integral I would be left with $$ \Big\| \int_0^te^{(t-s) A} ds \Big\| \,=\, \| A^{-1}(e^{tA}-I) \| \,\leq\, \|A^{-1}\| $$ obtaining a uniform bound in $t\geq0$ , of the type $$ |y(t)-\hat y-z(t)| \,\leq\, C\,\|A^{-1}\|\,\epsilon^2 \ .$$ The problem is I don't see how to split the remainder $\omega(y(s))$ from the rest of the integral.","Let smooth. Let be a stable equilibrium point for the ODE . Namely : , for every there exists such that provided that . Now fix , fix such that , and consider the Cauchy problem Denote by the jacobian matrix of at the equilibrium point and consider the linearized problem around : Suppose the matrix is negative definite . I would like to say that the linearized problem is a good approximation for the original one. A formal question could be something like: is there a uniform constant such that Tentative solution . Let and . By Taylor expansion of around , there exists a constant such that Therefore Solving this as a linear equation in with non-homegenous term gives The first term of the sum is zero. I'd like to bound the second term using the stability of equilibrium, indeed: and if could take this term out of the integral I would be left with obtaining a uniform bound in , of the type The problem is I don't see how to split the remainder from the rest of the integral.","f:\mathbb R^n\to \mathbb R^n \hat y\in\mathbb R^n y'(t)=f(y(t)) f(\hat y)=0 \epsilon>0 \delta>0 |y(t)-\hat y|<\epsilon\ \ \forall\,t\geq0 |y(0)-\hat y|<\delta \epsilon>0 y_0 |y_0-\hat y|<\delta  y'(t)=f(y(t)) \ ,\quad y(0)=y_0 \ . A:=D f(\hat y) f \hat y \hat y  z'(t) = A\, z(t)\ ,\quad z(0)=y_0-\hat y\ . A C  |y(t)-\hat y-z(t)| \leq C\,\epsilon^2 \quad\forall t\geq0 \quad\forall y_0\in B(\hat y,\delta) \ ? r>0 \epsilon\in(0,r) f \hat y C=C(r)  f(y) = A\,(y-\hat y) + \omega(y)\ ,\quad |\omega(y)|\leq C\, |y-\hat y|^2\ \forall y\in B(\hat y,r)\ .  \frac{d}{d t}\big(y(t)-\hat y-z(t)\big) \,=\, A\,\big(y(t)-\hat y-z(t)\big) \,+\, \omega(y(t))\ . y-\hat y-z \omega(y(t))  y(t)-\hat y-z(t) \,=\, e^{tA}\,\big(y(0)-\hat y-z(0)\big) \,+\, \int_0^te^{(t-s) A}\,\omega(y(s))\,ds  |\omega(y(s))| \leq C\, |y(s)-\hat y|^2 \leq C\,\epsilon^2 \quad\forall s\geq0 \,  \Big\| \int_0^te^{(t-s) A} ds \Big\| \,=\, \| A^{-1}(e^{tA}-I) \| \,\leq\, \|A^{-1}\|  t\geq0  |y(t)-\hat y-z(t)| \,\leq\, C\,\|A^{-1}\|\,\epsilon^2 \ . \omega(y(s))","['ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
71,Solution of inhomogenous system with fundamental matrix.,Solution of inhomogenous system with fundamental matrix.,,"Could anyone show me how to calculate the general solution to $$x' = \begin{pmatrix} 3 & -1 \\ 4 & -1 \end{pmatrix}x+\begin{pmatrix} 1 \\ t \end{pmatrix}$$ where $x_1(0) = 1$ and $x_2(0) = 0$ ? I get the fundamental matrix $$e^{tA} =  e^t+(A-I_2)te^{t}$$ And then I want to use a theorem that says that the general solution will be on the form $$x(t) = F(t)c+F(t) \int_{t_0}^{t} F^{-1}(\tau)b(\tau)d\tau$$ where $F(t)$ is the fundamental matrix, and c is a column-matrix dependent on the initial value given. In this case, this would be: $$x(t) = e^{tA}\begin{pmatrix} 1 \\ 0 \end{pmatrix}+e^{tA} \int_{0}^{t} e^{-\tau A}\begin{pmatrix} 1 \\ \tau \end{pmatrix} d\tau$$ But then it feels like the computations get nasty, when you substitute $$e^t+(A-I_2)te^t$$ for $e^{tA}$ . Is there anyway to simplify this?","Could anyone show me how to calculate the general solution to where and ? I get the fundamental matrix And then I want to use a theorem that says that the general solution will be on the form where is the fundamental matrix, and c is a column-matrix dependent on the initial value given. In this case, this would be: But then it feels like the computations get nasty, when you substitute for . Is there anyway to simplify this?",x' = \begin{pmatrix} 3 & -1 \\ 4 & -1 \end{pmatrix}x+\begin{pmatrix} 1 \\ t \end{pmatrix} x_1(0) = 1 x_2(0) = 0 e^{tA} =  e^t+(A-I_2)te^{t} x(t) = F(t)c+F(t) \int_{t_0}^{t} F^{-1}(\tau)b(\tau)d\tau F(t) x(t) = e^{tA}\begin{pmatrix} 1 \\ 0 \end{pmatrix}+e^{tA} \int_{0}^{t} e^{-\tau A}\begin{pmatrix} 1 \\ \tau \end{pmatrix} d\tau e^t+(A-I_2)te^t e^{tA},"['ordinary-differential-equations', 'differential']"
72,Analytic solution to coupled nonlinear first order ODEs with quadratic terms on the right hand side,Analytic solution to coupled nonlinear first order ODEs with quadratic terms on the right hand side,,"I am trying to find the analytic solution to the following nonlinear, coupled, first order ODE: $$\frac{dx}{dt} = -2ax^2-bxy$$ $$\frac{dy}{dt} = ax^2-bxy$$ I started with calculating $\frac{dx}{dy}$ and $\frac{dy}{dx}$ , but I found $\frac{dx}{dy} \neq (\frac{dy}{dx})^{-1}$ , which means we can't change the order of derivative. I failed to decoupled the variables, and I am also not sure if such an analytical solution can be found. I looked at two paper that discussed similar cases: https://doi.org/10.1007/s11040-021-09400-7 This paper discusses a subset of the ODE system with 2nd order polynomial on the right-hand side. Unfortunately, my system is not in that set. https://doi.org/10.1063/5.0011257 This paper is in the citation of the previous one. It only focuses on ODE systems with quadratic term on the RHS, but I haven't fully understood the method yet. If someone knows how to approach the problem and is willing to give some hints, it will be much appreciated!! Thank you!! -----------------------------------------------------Update------------------------------------------------------ Inspired by one of my friends, I do the following transformation: Multiply $\frac{y}{x^2}$ to the first equation and multiply $\frac{1}{x}$ second one, I get: $$\frac{yx'}{x^2} = -2ay-b\frac{y^2}{x}$$ $$\frac{xy'}{x^2} = ax-by$$ let $u = \frac{y}{x}$ , $$\frac{yx'}{x^2} = -2aux-bu^2x$$ $$\frac{xy'}{x^2} = ax-bux$$ Then, subtract these two equations: $$\frac{yx'-xy'}{x^2} = \frac{du}{dt} = x(-bu^2-(2a-b)u-a)$$ Similarly, we can also write $$\frac{dx}{dt} = x^2(-2a-u)$$ Then we have $$\frac{du}{dx} = \frac{1}{x}\frac{-bu^2-(2a-b)u-a}{-2a-u}$$ $$\frac{1}{x} dx= \frac{2a+u}{bu^2+(2a-b)u+a}du$$ $$\int \frac{1}{x} dx = \log{x} = \int \frac{2a+u}{bu^2+(2a-b)u+a}du$$ According to Wolfram, The integration equals: Let's call it $A(u)$ . Then we know $x = e^{A(u)}$ . Plug x in the ODE, then: $$\frac{du}{dt} = e^{A(u)}(-bu^2-(2a-b)u-a)$$ However, analytic form of u can not be calculated by Wolfram... Is there any way to get around it? -----------------------------------------------------update---------------------------------------------------- The numerical solution for $a=b=1$ is, If solving the equation analytically is not possible, is there any way to compute the asymptotic behavior directly? (Here, $x$ approaches $0$ and $y$ approaches $0.16303362$ ) -----------------------------------------------------Update------------------------------------------------------ I have successfully calculated the equilibrium state, and I do agree with the answer below that there's no analytical form. Thank you for the help guys!","I am trying to find the analytic solution to the following nonlinear, coupled, first order ODE: I started with calculating and , but I found , which means we can't change the order of derivative. I failed to decoupled the variables, and I am also not sure if such an analytical solution can be found. I looked at two paper that discussed similar cases: https://doi.org/10.1007/s11040-021-09400-7 This paper discusses a subset of the ODE system with 2nd order polynomial on the right-hand side. Unfortunately, my system is not in that set. https://doi.org/10.1063/5.0011257 This paper is in the citation of the previous one. It only focuses on ODE systems with quadratic term on the RHS, but I haven't fully understood the method yet. If someone knows how to approach the problem and is willing to give some hints, it will be much appreciated!! Thank you!! -----------------------------------------------------Update------------------------------------------------------ Inspired by one of my friends, I do the following transformation: Multiply to the first equation and multiply second one, I get: let , Then, subtract these two equations: Similarly, we can also write Then we have According to Wolfram, The integration equals: Let's call it . Then we know . Plug x in the ODE, then: However, analytic form of u can not be calculated by Wolfram... Is there any way to get around it? -----------------------------------------------------update---------------------------------------------------- The numerical solution for is, If solving the equation analytically is not possible, is there any way to compute the asymptotic behavior directly? (Here, approaches and approaches ) -----------------------------------------------------Update------------------------------------------------------ I have successfully calculated the equilibrium state, and I do agree with the answer below that there's no analytical form. Thank you for the help guys!",\frac{dx}{dt} = -2ax^2-bxy \frac{dy}{dt} = ax^2-bxy \frac{dx}{dy} \frac{dy}{dx} \frac{dx}{dy} \neq (\frac{dy}{dx})^{-1} \frac{y}{x^2} \frac{1}{x} \frac{yx'}{x^2} = -2ay-b\frac{y^2}{x} \frac{xy'}{x^2} = ax-by u = \frac{y}{x} \frac{yx'}{x^2} = -2aux-bu^2x \frac{xy'}{x^2} = ax-bux \frac{yx'-xy'}{x^2} = \frac{du}{dt} = x(-bu^2-(2a-b)u-a) \frac{dx}{dt} = x^2(-2a-u) \frac{du}{dx} = \frac{1}{x}\frac{-bu^2-(2a-b)u-a}{-2a-u} \frac{1}{x} dx= \frac{2a+u}{bu^2+(2a-b)u+a}du \int \frac{1}{x} dx = \log{x} = \int \frac{2a+u}{bu^2+(2a-b)u+a}du A(u) x = e^{A(u)} \frac{du}{dt} = e^{A(u)}(-bu^2-(2a-b)u-a) a=b=1 x 0 y 0.16303362,['ordinary-differential-equations']
73,Algebraic varieties fulfilled by solutions of polynomial ODEs,Algebraic varieties fulfilled by solutions of polynomial ODEs,,"Let's assume we have a two dimensional polynomial vector field of degree $d$ $$F: \mathbb{R}^{2}\rightarrow\mathbb{R}^{2}, \quad (x,y)\mapsto \begin{pmatrix}P(x,y), \\ Q(x,y)\end{pmatrix}$$ and we are given a solution $\gamma: I\subset \mathbb{R} \rightarrow \mathbb{R}^2$ . Let's define a polynomial of the same degree $d$ $$G:\mathbb{R}^{2}\rightarrow\mathbb{R}, \quad (x,y)\mapsto G(x,y).$$ but which can have completely different coefficients and also other monomials. I.e. $x^5 \cdot y$ could be a monomial in $G$ which is not a monomial of $F$ . Do we know if there exists an initial condition $x_0$ such that the solution curve $\gamma(t,x_0)$ fulfills $$G(\gamma_x(t,x_0),\gamma_y(t,x_0)=0 \quad \forall t?$$ As an example, given $F=(-y,x)$ and $x_0=(1,0)$ , the solution fulfills $$G(x,y)=x^2+y^2-1=0$$ But $G$ is a degree higher than $F$ . But that is the only example I currently was able to come up. I am interested in any result and any new example of this kind.","Let's assume we have a two dimensional polynomial vector field of degree and we are given a solution . Let's define a polynomial of the same degree but which can have completely different coefficients and also other monomials. I.e. could be a monomial in which is not a monomial of . Do we know if there exists an initial condition such that the solution curve fulfills As an example, given and , the solution fulfills But is a degree higher than . But that is the only example I currently was able to come up. I am interested in any result and any new example of this kind.","d F: \mathbb{R}^{2}\rightarrow\mathbb{R}^{2}, \quad (x,y)\mapsto \begin{pmatrix}P(x,y), \\ Q(x,y)\end{pmatrix} \gamma: I\subset \mathbb{R} \rightarrow \mathbb{R}^2 d G:\mathbb{R}^{2}\rightarrow\mathbb{R}, \quad (x,y)\mapsto G(x,y). x^5 \cdot y G F x_0 \gamma(t,x_0) G(\gamma_x(t,x_0),\gamma_y(t,x_0)=0 \quad \forall t? F=(-y,x) x_0=(1,0) G(x,y)=x^2+y^2-1=0 G F","['real-analysis', 'ordinary-differential-equations', 'real-algebraic-geometry']"
74,Help in solving the differential equation $\frac{dy}{dx} = \frac{y(a-x)}{x(x^2y^2 - b)}$,Help in solving the differential equation,\frac{dy}{dx} = \frac{y(a-x)}{x(x^2y^2 - b)},"I am trying to find the solutions to the following differential equation $$\frac{dy}{dx} = \frac{y(a-x)}{x(x^2y^2 - b)}\quad\quad(1)$$ where $a,b\in\mathbb{R}$ . To be honest I haven't made any progress. I will, however, mention what I have tried. First of all I tried partial fraction decomposition (after supposing that $b\geq0$ ) to see if I can convert $(1)$ to a known form but didn't end up anywhere (besides, I don't really have much experience with differential equations and don't know a lot of techiques/theorems so it might be the case I missed something). I also tried to play around with $(1)$ like I demonstrate \begin{align} (1) \implies \frac{y'}{y}=\frac{a-x}{x^3y^2-xb}&\implies x^3yy'-bx\frac{y'}{y} = a -x \\ &\iff \frac{1}{2}x^3\left(y^2\right)' - \frac{b}{2}x\left(\ln y^2\right)' = a - x \\ &\iff \frac{1}{2}x^3\left(e^w\right)' - \frac{b}{2}x\left(w\right)' = a - x,\quad w=\ln y^2 \end{align} The last expression seems more solvable, but perhaps it doesn't really help that much. I had a few other ideas as well but they are all based around writing $(1)$ differently in hopes something comes up. Any solutions/hints would be appreciated. Keep in mind that I am not sure if $(1)$ actually has solutions, but I would obviously want to know if that's th case.","I am trying to find the solutions to the following differential equation where . To be honest I haven't made any progress. I will, however, mention what I have tried. First of all I tried partial fraction decomposition (after supposing that ) to see if I can convert to a known form but didn't end up anywhere (besides, I don't really have much experience with differential equations and don't know a lot of techiques/theorems so it might be the case I missed something). I also tried to play around with like I demonstrate The last expression seems more solvable, but perhaps it doesn't really help that much. I had a few other ideas as well but they are all based around writing differently in hopes something comes up. Any solutions/hints would be appreciated. Keep in mind that I am not sure if actually has solutions, but I would obviously want to know if that's th case.","\frac{dy}{dx} = \frac{y(a-x)}{x(x^2y^2 - b)}\quad\quad(1) a,b\in\mathbb{R} b\geq0 (1) (1) \begin{align}
(1) \implies \frac{y'}{y}=\frac{a-x}{x^3y^2-xb}&\implies x^3yy'-bx\frac{y'}{y} = a -x \\
&\iff \frac{1}{2}x^3\left(y^2\right)' - \frac{b}{2}x\left(\ln y^2\right)' = a - x \\
&\iff \frac{1}{2}x^3\left(e^w\right)' - \frac{b}{2}x\left(w\right)' = a - x,\quad w=\ln y^2
\end{align} (1) (1)",['ordinary-differential-equations']
75,Why do we need the Hartman-Grobman theorem & the Stable Manifold Theorem to prove that any sink is asymptotically stable & source/saddle is unstable?,Why do we need the Hartman-Grobman theorem & the Stable Manifold Theorem to prove that any sink is asymptotically stable & source/saddle is unstable?,,"I am reading Perko's book on Differential Equations and Dynamical Systems (3e) and I have the following question: Why do we need the Hartman-Grobman theorem and the Stable Manifold Theorem to prove that any sink is asymptotically stable and source/saddle is unstable? Why is Hartman-Grobman not enough? The passage in question is on p.130: The following is said earlier, which I think makes implicitly use of Hartman-Grobman: Many thanks in advance! Attempt of an answer: Sink: If we have a sink, then the eigenvalues of $Df(x_{0})$ are all less than zero. To show asymptotical stability I would apply the diffeomorphism from H-G-thm, i.e. $$\lim_{t\to\infty}||\Phi_{t}(x)-x_{0}||=\lim_{t\to\infty}||H^{-1}\circ e^{At}H(x)-x_{0}||.$$ Then (hopefully) by continuity we have $$\lim_{t\to\infty}||\Phi_{t}(x)-x_{0}||=\lim_{t\to\infty}||H^{-1}\circ e^{At}H(x)-x_{0}||=0,$$ since $\lim_{t\to\infty}e^{At}H(x)=H(x_{0})$ . Saddle/Source: If we have a saddle then there are eigenvalues with positive and negative real part. Hence, $E^{s},E^{u}\neq\emptyset$ . By the Stable Manifold Theorem it follows that $W^{s},W^{u}\neq\emptyset$ , too, since they are of the same dimension as $E^{s},E^{u}$ respectively.  Hence, the stable manifold guarantees the existence of a trajectory that leaves any $B_{\epsilon}(x_{0})$ . But then we can not have stability. I hope it makes sense. Theorems and Definitions:","I am reading Perko's book on Differential Equations and Dynamical Systems (3e) and I have the following question: Why do we need the Hartman-Grobman theorem and the Stable Manifold Theorem to prove that any sink is asymptotically stable and source/saddle is unstable? Why is Hartman-Grobman not enough? The passage in question is on p.130: The following is said earlier, which I think makes implicitly use of Hartman-Grobman: Many thanks in advance! Attempt of an answer: Sink: If we have a sink, then the eigenvalues of are all less than zero. To show asymptotical stability I would apply the diffeomorphism from H-G-thm, i.e. Then (hopefully) by continuity we have since . Saddle/Source: If we have a saddle then there are eigenvalues with positive and negative real part. Hence, . By the Stable Manifold Theorem it follows that , too, since they are of the same dimension as respectively.  Hence, the stable manifold guarantees the existence of a trajectory that leaves any . But then we can not have stability. I hope it makes sense. Theorems and Definitions:","Df(x_{0}) \lim_{t\to\infty}||\Phi_{t}(x)-x_{0}||=\lim_{t\to\infty}||H^{-1}\circ e^{At}H(x)-x_{0}||. \lim_{t\to\infty}||\Phi_{t}(x)-x_{0}||=\lim_{t\to\infty}||H^{-1}\circ e^{At}H(x)-x_{0}||=0, \lim_{t\to\infty}e^{At}H(x)=H(x_{0}) E^{s},E^{u}\neq\emptyset W^{s},W^{u}\neq\emptyset E^{s},E^{u} B_{\epsilon}(x_{0})","['ordinary-differential-equations', 'dynamical-systems']"
76,Prove instability using Lyapunov function,Prove instability using Lyapunov function,,"Consider the system: \begin{align} x' &= x^3 + xy \\ y' &= -y + y^2 + xy - x^3 \\ \end{align} I want to prove the origin is an unstable point by using the Lyapunov function $V(x,y) = \tfrac{x^4}{4} - \tfrac{y^2}{2}$ (this is a hint provided in the exercise). In order to use Chetaev instability theorem I would like to prove that there is a domain $U$ in a punctured neighborhood of $0$ such that $$V'(x,y) = \frac{\partial E}{\partial x} (x^3 + xy) + \frac{\partial E}{\partial y} (-y + y^2 + xy - x^3) = x^6 + x^4y + y^2 - y^3 -xy^2 + x^3y$$ is strictly positive, which Wolfram Alpha confirms (indeed, that function is strictly positive in a disk around $0$ ). I have tried to show that $0$ is a local minimum, but the Hessian test is not conclusive as the Hessian is positive semidefinite. I have also tried to bound the expression below by $0$ without any luck. Could you give me any hint to proceed? Thanks in advance","Consider the system: I want to prove the origin is an unstable point by using the Lyapunov function (this is a hint provided in the exercise). In order to use Chetaev instability theorem I would like to prove that there is a domain in a punctured neighborhood of such that is strictly positive, which Wolfram Alpha confirms (indeed, that function is strictly positive in a disk around ). I have tried to show that is a local minimum, but the Hessian test is not conclusive as the Hessian is positive semidefinite. I have also tried to bound the expression below by without any luck. Could you give me any hint to proceed? Thanks in advance","\begin{align}
x' &= x^3 + xy \\
y' &= -y + y^2 + xy - x^3 \\
\end{align} V(x,y) = \tfrac{x^4}{4} - \tfrac{y^2}{2} U 0 V'(x,y) = \frac{\partial E}{\partial x} (x^3 + xy) + \frac{\partial E}{\partial y} (-y + y^2 + xy - x^3) = x^6 + x^4y + y^2 - y^3 -xy^2 + x^3y 0 0 0","['ordinary-differential-equations', 'dynamical-systems']"
77,Asymptotic expansion of inhomogenous differential equation,Asymptotic expansion of inhomogenous differential equation,,"Consider $$\tag{1} y'(x)+y(x)=\frac{1}{x} $$ For reference, the exact solution is $$\tag{2} y(x)=e^{-x}(C+\operatorname{Ei}(x)) $$ Where $\operatorname{Ei}$ is the exponential integral and $C$ is the integration constant. I want to study the/a particular solution of (1) as $x \to 0^+$ . Using dominant balance, I have found $y \sim \ln x$ . This matches the logarithmic singularity carried by $\operatorname{Ei}$ . Question : is it possible to say anything about the next to leading order terms of the particular solution using asymptotic analysis? Ie. by manipulating (1) and not just expanding (2). Expanding $\operatorname{Ei}$ suggests the next term should be $\gamma$ , Euler's constant . My thoughts : I think the answer is 'no' because a constant term could be absorbed into the constant of integration, $C$ . This leaves me vaguely uneasy. Working : There are three dominant balances to consider  in (1). The consistent one uses $y\ll x^{-1}$ $$ y'\sim x^{-1} \qquad, \qquad x \to 0 $$ This can be directly integrated $$ y(x)=\ln(x)+A(x) $$ Where $A'(x)\ll x^{-1}$ as $x\to 0$ . We may substitute into (1) to find a differential equation for $A$ $$ A'+A=-\ln(x) $$ The dominant balance that neglects $A$ leads to $$ A(x)=-x \ln(x)+x+B(x) $$ Where $B'\ll \ln x$ . The differential equation for $B$ is $$\tag{3} B'+B=x \ln(x)-x $$ And now there are two consistent dominant balances to consider, and the RHS vanishes at zero. At this point I think something must have been missed, because no constant terms appear in our expansion (4) between the singular $\ln(x)$ and finite $x \ln(x)$ terms $$\tag{4} y \sim \ln(x) - x \ln(x) +x \qquad ,\qquad x \to 0 $$ After playing around with it, I noticed that continuing to neglect the non-derivative terms in (3) and beyond leads to repeated integrals over $x \ln(x)$ , which can be done. Spotting the pattern then summing up the terms, I find $$ y \sim p(x)+e^{-x}\ln(x) \qquad , \qquad x \to 0 $$ Where $p(x)$ is a series in only positive powers of $x$ . Context : The equation (1) comes from this question .","Consider For reference, the exact solution is Where is the exponential integral and is the integration constant. I want to study the/a particular solution of (1) as . Using dominant balance, I have found . This matches the logarithmic singularity carried by . Question : is it possible to say anything about the next to leading order terms of the particular solution using asymptotic analysis? Ie. by manipulating (1) and not just expanding (2). Expanding suggests the next term should be , Euler's constant . My thoughts : I think the answer is 'no' because a constant term could be absorbed into the constant of integration, . This leaves me vaguely uneasy. Working : There are three dominant balances to consider  in (1). The consistent one uses This can be directly integrated Where as . We may substitute into (1) to find a differential equation for The dominant balance that neglects leads to Where . The differential equation for is And now there are two consistent dominant balances to consider, and the RHS vanishes at zero. At this point I think something must have been missed, because no constant terms appear in our expansion (4) between the singular and finite terms After playing around with it, I noticed that continuing to neglect the non-derivative terms in (3) and beyond leads to repeated integrals over , which can be done. Spotting the pattern then summing up the terms, I find Where is a series in only positive powers of . Context : The equation (1) comes from this question .","\tag{1}
y'(x)+y(x)=\frac{1}{x}
 \tag{2}
y(x)=e^{-x}(C+\operatorname{Ei}(x))
 \operatorname{Ei} C x \to 0^+ y \sim \ln x \operatorname{Ei} \operatorname{Ei} \gamma C y\ll x^{-1} 
y'\sim x^{-1} \qquad, \qquad x \to 0
 
y(x)=\ln(x)+A(x)
 A'(x)\ll x^{-1} x\to 0 A 
A'+A=-\ln(x)
 A 
A(x)=-x \ln(x)+x+B(x)
 B'\ll \ln x B \tag{3}
B'+B=x \ln(x)-x
 \ln(x) x \ln(x) \tag{4}
y \sim \ln(x) - x \ln(x) +x \qquad ,\qquad x \to 0
 x \ln(x) 
y \sim p(x)+e^{-x}\ln(x) \qquad , \qquad x \to 0
 p(x) x","['ordinary-differential-equations', 'asymptotics']"
78,Binomial Expansion with Derivatives,Binomial Expansion with Derivatives,,"Question: We have a function $f(t)$ that is defined for all $t \in [0,T]$ . From our data, we can estimate two important parameters $\theta \in \mathbb{Z} \backslash 0$ and $n \in \mathbb{N}$ . Given $n$ , we can also estimate a function $g(n)$ . Our estimations are fairly accurate. Now it appears that there is a pattern. For example, for $\theta > 0$ , we have \begin{align} &n=1,\quad g(1)= \frac{d f(t)}{dt}+\theta f(t), \\ &n=2,\quad g(2)= \frac{d^2 f(t)}{dt^2}+\theta^2 f(t)+2\theta \frac{d f(t)}{dt}, \end{align} and so on; therefore, I think we can write it as an ordinary differential equation $g(n)=(d_t+Œ∏)^n f(t)$ where $d_t$ is a time-derivative operator. Now is there a way to explicitly determine $f(t)$ ? My Attempt 1) I tried to use Kovacic algorithm but it seems very complicated and I couldn't get anywhere with it. My Attempt 2) Although we have a nonhomogeneous equation due to $g(n)$ , if for every $n$ , we differentiate both sides, we have: \begin{align} & n=1,\quad f'' + \theta f'= 0, \\ & n=2,\quad f'''+2 \theta f''+ \theta^2 f'=0, \end{align} and so on. If we look closely, the characteristics equation for every $n$ emits two roots, $0$ and $-\theta$ so let us define $C_{1n}$ and $C_{2n}$ two constants and $f_n(t)$ , given the value of $n$ , then we can write $$f_n(t) = C_{1n} + C_{2n}e^{- \theta t}.$$ But the problem is that the only condition we have is $f_n(T)=0.$ Any suggestions or comment would be very appreciated. I'm more interested in knowing if there is a solution for $f$ and the method to get there rather than the final solution. Edit: I think I have made a mistake: since the root $-\theta$ is a repeated root in the second set of ODEs, I think the solution to $f$ is: $$f_n(t) = C_{0n} + e^{-\theta t}\sum_{k = 1}^{n} C_{kn} t^{k - 1}.$$","Question: We have a function that is defined for all . From our data, we can estimate two important parameters and . Given , we can also estimate a function . Our estimations are fairly accurate. Now it appears that there is a pattern. For example, for , we have and so on; therefore, I think we can write it as an ordinary differential equation where is a time-derivative operator. Now is there a way to explicitly determine ? My Attempt 1) I tried to use Kovacic algorithm but it seems very complicated and I couldn't get anywhere with it. My Attempt 2) Although we have a nonhomogeneous equation due to , if for every , we differentiate both sides, we have: and so on. If we look closely, the characteristics equation for every emits two roots, and so let us define and two constants and , given the value of , then we can write But the problem is that the only condition we have is Any suggestions or comment would be very appreciated. I'm more interested in knowing if there is a solution for and the method to get there rather than the final solution. Edit: I think I have made a mistake: since the root is a repeated root in the second set of ODEs, I think the solution to is:","f(t) t \in [0,T] \theta \in \mathbb{Z} \backslash 0 n \in \mathbb{N} n g(n) \theta > 0 \begin{align}
&n=1,\quad g(1)= \frac{d f(t)}{dt}+\theta f(t), \\
&n=2,\quad g(2)= \frac{d^2 f(t)}{dt^2}+\theta^2 f(t)+2\theta \frac{d f(t)}{dt},
\end{align} g(n)=(d_t+Œ∏)^n f(t) d_t f(t) g(n) n \begin{align}
& n=1,\quad f'' + \theta f'= 0, \\
& n=2,\quad f'''+2 \theta f''+ \theta^2 f'=0,
\end{align} n 0 -\theta C_{1n} C_{2n} f_n(t) n f_n(t) = C_{1n} + C_{2n}e^{- \theta t}. f_n(T)=0. f -\theta f f_n(t) = C_{0n} + e^{-\theta t}\sum_{k = 1}^{n} C_{kn} t^{k - 1}.","['calculus', 'ordinary-differential-equations']"
79,Dimension of solution space and adjoint solution space are equal,Dimension of solution space and adjoint solution space are equal,,"Consider the system of linear ODE's and its linear adjoint system given by \eqref{1} and \eqref{2} respectively ( $x\in \mathbb{R}^n)$ . $$ \begin{align} &x'(t)=M(t)x(t) \label{1} \tag{1}\\ &y'(t)=-M^\ast(t)y(t) \label{2}\tag{2} \end{align} $$ where $M$ is a continuous $n\times n$ matrix and $\omega>0$ periodic. If we denote say $S$ and $S^*$ to be the space of $\omega$ -periodic solutions to \eqref{1} and \eqref{2} respectively, is it true that their dimensions are equal? I was thinking if we can show an element of $S^*$ is in the dual of $S$ and by using the fact that the dimension of a vector space and its dual must be equal, the proof then follows trivially. I am not too sure how to solve this. Any help would be much appreciated. Thank you!","Consider the system of linear ODE's and its linear adjoint system given by \eqref{1} and \eqref{2} respectively ( . where is a continuous matrix and periodic. If we denote say and to be the space of -periodic solutions to \eqref{1} and \eqref{2} respectively, is it true that their dimensions are equal? I was thinking if we can show an element of is in the dual of and by using the fact that the dimension of a vector space and its dual must be equal, the proof then follows trivially. I am not too sure how to solve this. Any help would be much appreciated. Thank you!","x\in \mathbb{R}^n) 
\begin{align}
&x'(t)=M(t)x(t) \label{1} \tag{1}\\
&y'(t)=-M^\ast(t)y(t) \label{2}\tag{2}
\end{align}
 M n\times n \omega>0 S S^* \omega S^* S",['ordinary-differential-equations']
80,The Frenet frame is orthogonal,The Frenet frame is orthogonal,,"I have proved $P'=AP$ where $$P= \begin{pmatrix} T \\ N \\B \end{pmatrix}$$ $$A=   \begin{pmatrix}     0 & \kappa & 0 \\     -\kappa & 0 & \tau \\     0 & -\tau & 0 \\     \end{pmatrix} $$ . I am trying to show $P$ is orthogonal by this fact. I try to differentiate $PP^t$ , then $P'(P^t)+P(P^t)'=APP^t-PP^tA$ since $A$ is skew-symmetric. But I am stuck here, I am wondering if it is true that $APP^t=PP^tA$ .","I have proved where . I am trying to show is orthogonal by this fact. I try to differentiate , then since is skew-symmetric. But I am stuck here, I am wondering if it is true that .","P'=AP P= \begin{pmatrix} T \\ N \\B \end{pmatrix} A=   \begin{pmatrix}
    0 & \kappa & 0 \\
    -\kappa & 0 & \tau \\
    0 & -\tau & 0 \\
    \end{pmatrix}
 P PP^t P'(P^t)+P(P^t)'=APP^t-PP^tA A APP^t=PP^tA","['ordinary-differential-equations', 'differential-geometry', 'curves', 'frenet-frame']"
81,Must a $ùê∂^1$ curve with constant angular momentum alternate between a straight line or a circular arc?,Must a  curve with constant angular momentum alternate between a straight line or a circular arc?,ùê∂^1,"Let $\alpha:(0,L) \to \mathbb{R}^2$ be a $C^1$ curve  satisfying $|\dot \alpha|=1$ , and assume that $\alpha(t) \times \dot \alpha(t)$ is constant. Does one of the following hold? $(1)$ $\alpha$ is affine. $(2)$ $\alpha$ is a circular arc. $(3)$ $\alpha$ starts as a circular arc, then at some point becomes affine (i.e. it coincides with the tangent to the circle.) $(4)$ $\alpha$ starts affine, then at some point it swtiches to a circular arc, and after some time, it can also switch one more time to being affine (a tangent). Comments: a. I proved below that the restriction of $\alpha$ to the open set $\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}$ is $C^{\infty}$ . Since a $C^2$ curve  having constant momentum must be either affine or a circular arc , and since a circular arc satisfies $\alpha(t)\perp \dot \alpha(t)$ , it follows that $\alpha$ is affine on each connected component of $\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}$ . Is there a way to proceed from here? The difficulty seems to be to prove that $\{t\, |\,\alpha(t) \perp \dot \alpha(t)\}$ is a closed interval. Suppose for instance that $\alpha$ starts affine, and that it stays affine up to a certain point $t_0$ satisfying $\alpha(t_0) \perp \dot \alpha(t_0)$ . Suppose that $\alpha$ is not affine after $t_0$ , i.e. there is no $\epsilon>0$ such that $\alpha|_{(t_0-\epsilon,t_0+\epsilon)}$ is affine.  How can we prove that $\alpha(t) \perp \dot \alpha(t)$ still holds for sufficiently small $t>t_0$ ? Assuming this is not the case, there exists a decreasing sequence $t_n \to t_0$ satisfying $\alpha(t_n) \not \perp \dot \alpha(t_n)$ . Thus there exists $\epsilon_n>0$ , such that $\alpha|_{(t_n-\epsilon_n,t_n+\epsilon_n)}$ is affine. However, this fact alone does not imply affinity at $t_0$ . Of course, in our case, all these affine parts around $t_n$ must be tangents to a given circle. Can we use this somehow? b. There is an asymmetry between options $(3)$ and $(4)$ . In option $(4)$ there are two ""switching points"", while in $(3)$ there is only one. Here is  why there cannot be more than one switch in the scenario described in $(3)$ : On a region where $\alpha$ is part of a circle of radius $R$ centered at the origin, it satisfies $|\alpha(t) \times \dot \alpha(t)|=R$ . Switching from a line to a circle is only possible when the line is tangent to that circle. (since we are $C^1$ ). Thus, if we start from a circle, and switch to a tangent, we cannot switch again at some point from the tangent to a tangent circle centered at the origin. (the angle at the contact from the origin is not $\pi/2$ ). Alternatively, after we leave the point of contact of the tangent and circle, the distance from the origin increases, so we cannot switch to a circle centered at the origin, since such a circle would have a different radius, thus violating the constancy of $|\alpha(t) \times \dot \alpha(t)|$ . c. I proved here that if $\alpha$ is assumed to be $C^2$ , then the only options $(1)$ or $(2)$ are possible, i.e. $\alpha$ must be either a circular arc or affine. Clearly, a $C^2$ solution cannot alternate between circular and affine. I don't know how to adapt the proof to this lower regularity setting, since I used there second order derivatives of $\alpha$ . I guess that one possibility would be to use some distributional/weak derivatives instead. I think that a good starting point might be to show $\alpha$ is $C^2$ almost everywhere. Proof that that $\alpha$ is $C^2$ on $\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}$ is $C^2$ : Write $\alpha(t)=(X(t),Y(t))$ . The conditions $|\dot \alpha|=1$ and $|\alpha(t) \times \dot \alpha(t)|=c$ , translate into $$ \dot X^2+\dot Y^2=1, \\ X\dot Y-Y\dot X=c. $$ Thus $$ \dot Y=\frac{c+Y\dot X}{X}, $$ which together with $\dot Y^2=1-\dot X^2$ implies that $$ 0=\dot X^2(Y^2+X^2)+(2cY)\dot X+(c^2-X^2). $$ Since the discriminant $$ (2cY)^2-4(Y^2+X^2)(c^2-X^2)=4X^2(|\alpha|^2-c^2), $$ we find that whenever $|\alpha| > c$ , $\dot X$ is a smooth function of $X,Y,c$ . Now, we note that $c=|\alpha(t) \times \dot \alpha(t)| \le |\alpha(t)||\dot \alpha(t)|=|\alpha(t)|$ , with equality if and only if $\alpha(t) \perp \dot \alpha(t)$ . Thus $$ \{t\, |\,|\alpha(t)|>c\}=\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\}, $$ and since $\dot X$ (and similarly $\dot Y$ ) are smooth functions of $X,Y,c$ , we conclude that $\alpha$ is $C^{\infty}$ on this set.","Let be a curve  satisfying , and assume that is constant. Does one of the following hold? is affine. is a circular arc. starts as a circular arc, then at some point becomes affine (i.e. it coincides with the tangent to the circle.) starts affine, then at some point it swtiches to a circular arc, and after some time, it can also switch one more time to being affine (a tangent). Comments: a. I proved below that the restriction of to the open set is . Since a curve  having constant momentum must be either affine or a circular arc , and since a circular arc satisfies , it follows that is affine on each connected component of . Is there a way to proceed from here? The difficulty seems to be to prove that is a closed interval. Suppose for instance that starts affine, and that it stays affine up to a certain point satisfying . Suppose that is not affine after , i.e. there is no such that is affine.  How can we prove that still holds for sufficiently small ? Assuming this is not the case, there exists a decreasing sequence satisfying . Thus there exists , such that is affine. However, this fact alone does not imply affinity at . Of course, in our case, all these affine parts around must be tangents to a given circle. Can we use this somehow? b. There is an asymmetry between options and . In option there are two ""switching points"", while in there is only one. Here is  why there cannot be more than one switch in the scenario described in : On a region where is part of a circle of radius centered at the origin, it satisfies . Switching from a line to a circle is only possible when the line is tangent to that circle. (since we are ). Thus, if we start from a circle, and switch to a tangent, we cannot switch again at some point from the tangent to a tangent circle centered at the origin. (the angle at the contact from the origin is not ). Alternatively, after we leave the point of contact of the tangent and circle, the distance from the origin increases, so we cannot switch to a circle centered at the origin, since such a circle would have a different radius, thus violating the constancy of . c. I proved here that if is assumed to be , then the only options or are possible, i.e. must be either a circular arc or affine. Clearly, a solution cannot alternate between circular and affine. I don't know how to adapt the proof to this lower regularity setting, since I used there second order derivatives of . I guess that one possibility would be to use some distributional/weak derivatives instead. I think that a good starting point might be to show is almost everywhere. Proof that that is on is : Write . The conditions and , translate into Thus which together with implies that Since the discriminant we find that whenever , is a smooth function of . Now, we note that , with equality if and only if . Thus and since (and similarly ) are smooth functions of , we conclude that is on this set.","\alpha:(0,L) \to \mathbb{R}^2 C^1 |\dot \alpha|=1 \alpha(t) \times \dot \alpha(t) (1) \alpha (2) \alpha (3) \alpha (4) \alpha \alpha \{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\} C^{\infty} C^2 \alpha(t)\perp \dot \alpha(t) \alpha \{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\} \{t\, |\,\alpha(t) \perp \dot \alpha(t)\} \alpha t_0 \alpha(t_0) \perp \dot \alpha(t_0) \alpha t_0 \epsilon>0 \alpha|_{(t_0-\epsilon,t_0+\epsilon)} \alpha(t) \perp \dot \alpha(t) t>t_0 t_n \to t_0 \alpha(t_n) \not \perp \dot \alpha(t_n) \epsilon_n>0 \alpha|_{(t_n-\epsilon_n,t_n+\epsilon_n)} t_0 t_n (3) (4) (4) (3) (3) \alpha R |\alpha(t) \times \dot \alpha(t)|=R C^1 \pi/2 |\alpha(t) \times \dot \alpha(t)| \alpha C^2 (1) (2) \alpha C^2 \alpha \alpha C^2 \alpha C^2 \{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\} C^2 \alpha(t)=(X(t),Y(t)) |\dot \alpha|=1 |\alpha(t) \times \dot \alpha(t)|=c 
\dot X^2+\dot Y^2=1, \\ X\dot Y-Y\dot X=c.
 
\dot Y=\frac{c+Y\dot X}{X},
 \dot Y^2=1-\dot X^2 
0=\dot X^2(Y^2+X^2)+(2cY)\dot X+(c^2-X^2).
 
(2cY)^2-4(Y^2+X^2)(c^2-X^2)=4X^2(|\alpha|^2-c^2),
 |\alpha| > c \dot X X,Y,c c=|\alpha(t) \times \dot \alpha(t)| \le |\alpha(t)||\dot \alpha(t)|=|\alpha(t)| \alpha(t) \perp \dot \alpha(t) 
\{t\, |\,|\alpha(t)|>c\}=\{t\, |\,\alpha(t) \not\perp \dot \alpha(t)\},
 \dot X \dot Y X,Y,c \alpha C^{\infty}","['ordinary-differential-equations', 'differential-geometry', 'euclidean-geometry', 'circles', 'polygons']"
82,"Does Lyapunov theorem extend to dynamical system that have ""equilibrium sets""?","Does Lyapunov theorem extend to dynamical system that have ""equilibrium sets""?",,"Suppose that I have some dynamical system $$\dot x = f(x)$$ $f$ is locally Lipschitz, etc. I know that $f(x) = 0$ whenever $x \in \Gamma$ , where $\Gamma$ is some closed (and possibly bounded) set in $\mathbb{R}^n$ , could be a small disc, a line, a plane. In other words, not just an equilibrium. How can dynamical system theory deal with this case? Normally the approach is through Lyapunov theorem, but it is usually with respect to a single point. Is it possible to extend the Lyapunov theorem by defining a Lyapunov function $V$ such that it is $V(\Gamma) = 0$ and $V(x) > 0$ elsewhere and proceed as usual by showing $\dot V(\Gamma) = 0$ and $\dot V(x) < 0$ elsewhere? Is this possible? Are there any catch for using the above? I have went through several books but could not find this extension.","Suppose that I have some dynamical system is locally Lipschitz, etc. I know that whenever , where is some closed (and possibly bounded) set in , could be a small disc, a line, a plane. In other words, not just an equilibrium. How can dynamical system theory deal with this case? Normally the approach is through Lyapunov theorem, but it is usually with respect to a single point. Is it possible to extend the Lyapunov theorem by defining a Lyapunov function such that it is and elsewhere and proceed as usual by showing and elsewhere? Is this possible? Are there any catch for using the above? I have went through several books but could not find this extension.",\dot x = f(x) f f(x) = 0 x \in \Gamma \Gamma \mathbb{R}^n V V(\Gamma) = 0 V(x) > 0 \dot V(\Gamma) = 0 \dot V(x) < 0,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'control-theory', 'lyapunov-functions']"
83,"Twice continuously differentiable function such that $f‚Äô‚Äô-f<0, \forall x\in(0,1)$.",Twice continuously differentiable function such that .,"f‚Äô‚Äô-f<0, \forall x\in(0,1)","Let $f:[0,1]\to\Bbb R$ be twice continuously differentiable function such that $f‚Äô‚Äô(x)-f(x)<0, \forall x\in(0,1)$ and $f(0)=f(1)=0$ , then which of the following statements is/are true about $f$ ? $1.$ $f$ has at least one zero in $(0,1).$ $2.$ $f $ has at least two zeros in $(0,1).$ $3.$ $f(x)>0, \forall x\in (0,1)$ . $4.$ $f(x)<0, \forall x\in (0,1)$ . If i consider the example $x(1-x)$ on $[0,1]$ , then only option $3$ is correct one , but i want to solve the problem theoretically without using example or counter examples.  It seems that last option is false because that $f$ can‚Äôt be both negative and concave .  Please suggest how to discard rest options. Thanks .","Let be twice continuously differentiable function such that and , then which of the following statements is/are true about ? has at least one zero in has at least two zeros in . . If i consider the example on , then only option is correct one , but i want to solve the problem theoretically without using example or counter examples.  It seems that last option is false because that can‚Äôt be both negative and concave .  Please suggest how to discard rest options. Thanks .","f:[0,1]\to\Bbb R f‚Äô‚Äô(x)-f(x)<0, \forall x\in(0,1) f(0)=f(1)=0 f 1. f (0,1). 2. f  (0,1). 3. f(x)>0, \forall x\in (0,1) 4. f(x)<0, \forall x\in (0,1) x(1-x) [0,1] 3 f","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
84,Find the GS of the System of DE's $\begin{cases} x' = x-3y\\ y'=3x+7y \end{cases}$,Find the GS of the System of DE's,\begin{cases} x' = x-3y\\ y'=3x+7y \end{cases},"Find the GS of the following system of DE's where the independent variable is $t$ and $x$ and $y$ are the dependent variables \begin{cases} x' = x-3y\\ y'=3x+7y \end{cases} I know using eigenvalues and eigenvectors or operators is one way to do this. But I wish to double check my answer using a substitution method. So my work: The second DE $y'=3x+7y$ can be rewritten as $x = \cfrac{y'}{3}-\cfrac 73y$ then $x' = \cfrac{y''}{3}-\cfrac73y'$ When we plug these values of $x$ and $x'$ into the first DE ( $x' = x -3y)$ , we get with some rearranging $\cfrac{y''}{3}-\cfrac83y'+\cfrac{16}{3}y = 0$ Which has a characteristic equation of $\cfrac{r^2}{3}-\cfrac83r+\cfrac{16}{3} = 0$ with roots $r_1=4$ and $r_2 = 4$ Then the solution for $y$ is $y$ = $C_1e^{4t}+C_2te^{4t}$ Then we back sub to solve for $x$ using $x = \cfrac{y'}{3}$$-\cfrac73y$ with the solution of y we just found. We get $x =-C_1e^{4t}-C_2te^{4t} + \cfrac{C_2}{3}e^{4t} =-C_1e^{4t}-C_2te^{4t} + C_3e^{4t}$ so the GS to the homo system is \begin{cases} x = -C_1e^{4t}-C_2te^{4t}+C_3e^{4t}\\ y = C_1e^{4t}+C_2te^{4t} \end{cases} If this solution is right, then I'm confident that I understand how substitution method works for solving DE systems. (Also it would boost my confidence in using the operator method to solve this as I got the same answer as this using the operator method). I'm a little thrown off on the roots being the same but I still think my methodology is still sound. I would appreciate it if someone could tell me if I've got this right cause then I know I completely understand how to solve a system of DE's. If more work is necessary to show please let me know.","Find the GS of the following system of DE's where the independent variable is and and are the dependent variables I know using eigenvalues and eigenvectors or operators is one way to do this. But I wish to double check my answer using a substitution method. So my work: The second DE can be rewritten as then When we plug these values of and into the first DE ( , we get with some rearranging Which has a characteristic equation of with roots and Then the solution for is = Then we back sub to solve for using with the solution of y we just found. We get so the GS to the homo system is If this solution is right, then I'm confident that I understand how substitution method works for solving DE systems. (Also it would boost my confidence in using the operator method to solve this as I got the same answer as this using the operator method). I'm a little thrown off on the roots being the same but I still think my methodology is still sound. I would appreciate it if someone could tell me if I've got this right cause then I know I completely understand how to solve a system of DE's. If more work is necessary to show please let me know.","t x y \begin{cases}
x' = x-3y\\
y'=3x+7y
\end{cases} y'=3x+7y x = \cfrac{y'}{3}-\cfrac 73y x' = \cfrac{y''}{3}-\cfrac73y' x x' x' = x -3y) \cfrac{y''}{3}-\cfrac83y'+\cfrac{16}{3}y = 0 \cfrac{r^2}{3}-\cfrac83r+\cfrac{16}{3} = 0 r_1=4 r_2 = 4 y y C_1e^{4t}+C_2te^{4t} x x = \cfrac{y'}{3}-\cfrac73y x =-C_1e^{4t}-C_2te^{4t} + \cfrac{C_2}{3}e^{4t} =-C_1e^{4t}-C_2te^{4t} + C_3e^{4t} \begin{cases}
x = -C_1e^{4t}-C_2te^{4t}+C_3e^{4t}\\
y = C_1e^{4t}+C_2te^{4t}
\end{cases}",[]
85,Prove that $\lim\limits_{t \to \infty} x(t)$ exists and is an integer.,Prove that  exists and is an integer.,\lim\limits_{t \to \infty} x(t),"Let $f : \Bbb R \longrightarrow \Bbb R$ be a $\operatorname {C}^{\infty}$ -function such that $f(x) = 0$ if and only if $x \in \Bbb Z.$ Suppose the function $x : \Bbb R \longrightarrow \Bbb R$ satisfies $x'(t) = f(x(t)),$ for all $t \in \Bbb R.$ If $\Bbb Z \cap \{x(t)\ |\ t \in \Bbb R \} = \varnothing,$ then show that $\lim\limits_{t \to \infty}  x(t)$ exists and is an integer. What I have seen is that $x$ is also a $\operatorname {C}^{\infty}$ - function and since image of $x$ doesn't contain any integer so by IVP it follows that $$\{x(t)\ |\ t \in \Bbb R \} \subseteq (a,b)$$ where $a, b \in \Bbb R$ with $b-a \leq 1$ such that $(a, b) \cap \Bbb Z = \varnothing.$ In other words the image of $x$ is strictly lying between two consecutive integers. Hence $x$ is bounded and also by the definition of $x'$ and $f$ it follows that $x'(t) \neq 0,$ for all $t \in \Bbb R.$ Now how do I proceed? Any help will be highly appreciated. Thanks in advance.",Let be a -function such that if and only if Suppose the function satisfies for all If then show that exists and is an integer. What I have seen is that is also a - function and since image of doesn't contain any integer so by IVP it follows that where with such that In other words the image of is strictly lying between two consecutive integers. Hence is bounded and also by the definition of and it follows that for all Now how do I proceed? Any help will be highly appreciated. Thanks in advance.,"f : \Bbb R \longrightarrow \Bbb R \operatorname {C}^{\infty} f(x) = 0 x \in \Bbb Z. x : \Bbb R \longrightarrow \Bbb R x'(t) = f(x(t)), t \in \Bbb R. \Bbb Z \cap \{x(t)\ |\ t \in \Bbb R \} = \varnothing, \lim\limits_{t \to \infty}  x(t) x \operatorname {C}^{\infty} x \{x(t)\ |\ t \in \Bbb R \} \subseteq (a,b) a, b \in \Bbb R b-a \leq 1 (a, b) \cap \Bbb Z = \varnothing. x x x' f x'(t) \neq 0, t \in \Bbb R.","['real-analysis', 'ordinary-differential-equations', 'limits', 'proof-writing']"
86,Solve $ay''+by+c=0$ using separation of variables,Solve  using separation of variables,ay''+by+c=0,"How could be $$ay''+by+c=0$$ (for constants $a,b,c$ ) solved using separation of variables? I tried using the method on an easier equation, namely $ay''+by'+c=0$ : $$\begin{align}a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+c&=0,\, u=\frac{dy}{dx}\\a\frac{du}{dx}+bu+c&=0\\ \frac{1}{a}\frac{dx}{du}&=-\frac{1}{bu+c}\\x&=-a\int \frac{du}{bu+c}=-a\left(\frac{\ln |bu+c|}{b}+C\right)\\-\frac{b}{a}(x+aC)&=\ln |bu+c|\\|bu+c|&=e^{-\frac{b}{a}(x+aC)}\\bu+c=b\frac{dy}{dx}+c&=C'e^{iC''}e^{-\frac{b}{a}(x+aC)},\, \color{red}{C'\ge 0,\, C''\in\mathbb{R}}\\y&=\frac{1}{b}\left(C_0\int e^{-\frac{b}{a}(x+aC)}\, dx-c\int dx\right)\\&=\frac{1}{b}\left(C_0\left(-\frac{ae^{-\frac{b}{a}(x+aC)}}{b}+C'''\right)-c(x+C'''')\right).\end{align}$$ When I tried to apply this method on $ay''+by+c=0$ , I ran into problems, as the degrees of the derivatives differ by $2$ , not $1$ : $$\begin{align}a\frac{d^2 y}{dx}+by+c&=0,\, \frac{dy}{dx}=u\\a\frac{du}{dx}+b\int u\, dx+c&=0\\ \frac{dx}{du}&=-\frac{a}{b\int u\, dx+c}\\x&=-a\int\frac{du}{b\int u\, dx+c}.\end{align}$$ I'm stuck here. The solution should be $$y=Ce^{x\sqrt{\frac{b}{a}}}+C'e^{-x\sqrt{\frac{b}{a}}}-\frac{c}{b}.$$","How could be (for constants ) solved using separation of variables? I tried using the method on an easier equation, namely : When I tried to apply this method on , I ran into problems, as the degrees of the derivatives differ by , not : I'm stuck here. The solution should be","ay''+by+c=0 a,b,c ay''+by'+c=0 \begin{align}a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+c&=0,\, u=\frac{dy}{dx}\\a\frac{du}{dx}+bu+c&=0\\ \frac{1}{a}\frac{dx}{du}&=-\frac{1}{bu+c}\\x&=-a\int \frac{du}{bu+c}=-a\left(\frac{\ln |bu+c|}{b}+C\right)\\-\frac{b}{a}(x+aC)&=\ln |bu+c|\\|bu+c|&=e^{-\frac{b}{a}(x+aC)}\\bu+c=b\frac{dy}{dx}+c&=C'e^{iC''}e^{-\frac{b}{a}(x+aC)},\, \color{red}{C'\ge 0,\, C''\in\mathbb{R}}\\y&=\frac{1}{b}\left(C_0\int e^{-\frac{b}{a}(x+aC)}\, dx-c\int dx\right)\\&=\frac{1}{b}\left(C_0\left(-\frac{ae^{-\frac{b}{a}(x+aC)}}{b}+C'''\right)-c(x+C'''')\right).\end{align} ay''+by+c=0 2 1 \begin{align}a\frac{d^2 y}{dx}+by+c&=0,\, \frac{dy}{dx}=u\\a\frac{du}{dx}+b\int u\, dx+c&=0\\ \frac{dx}{du}&=-\frac{a}{b\int u\, dx+c}\\x&=-a\int\frac{du}{b\int u\, dx+c}.\end{align} y=Ce^{x\sqrt{\frac{b}{a}}}+C'e^{-x\sqrt{\frac{b}{a}}}-\frac{c}{b}.","['calculus', 'ordinary-differential-equations']"
87,Maximal Solution (ODE) of $x' = x^2 - t^2$,Maximal Solution (ODE) of,x' = x^2 - t^2,"$\DeclareMathOperator{\dom}{dom}$ Let $\gamma(t)$ be a the maximal solution of the diferential equation $x' = x^2 - t^2$ with initial condition $\gamma(0) = 0$ . Show that $\gamma(t) \leq |t|$ , for any $t \in \dom(\gamma)$ and conclude that $\dom(\gamma) = \mathbb{R}$ . Attempt: Assuming that $\gamma(t) \leq |t|$ , for any $t \in \dom(\gamma)$ , I was able to conclude that $\dom(\gamma) = \mathbb{R}$ , using the fact that $\gamma (t)$ is a maximal solution. Also, note that $(x^2 - t^2)' = 2x(x^2 - t^2) - 2t$ . I tried to analyse for $t\geq 0$ and $t<0$ , but there's not much I can infer about about $(x^2 - t^2)'$ since I don't know the sign of $x$ . Is there any algebraic manipulation that could be helpful to procced with this idea? Any help would be appreciated!","Let be a the maximal solution of the diferential equation with initial condition . Show that , for any and conclude that . Attempt: Assuming that , for any , I was able to conclude that , using the fact that is a maximal solution. Also, note that . I tried to analyse for and , but there's not much I can infer about about since I don't know the sign of . Is there any algebraic manipulation that could be helpful to procced with this idea? Any help would be appreciated!",\DeclareMathOperator{\dom}{dom} \gamma(t) x' = x^2 - t^2 \gamma(0) = 0 \gamma(t) \leq |t| t \in \dom(\gamma) \dom(\gamma) = \mathbb{R} \gamma(t) \leq |t| t \in \dom(\gamma) \dom(\gamma) = \mathbb{R} \gamma (t) (x^2 - t^2)' = 2x(x^2 - t^2) - 2t t\geq 0 t<0 (x^2 - t^2)' x,"['ordinary-differential-equations', 'analysis']"
88,"Method of Adjoints, Neural ODEs","Method of Adjoints, Neural ODEs",,"I've been trying to understand the gist behind the Chen et. al paper on neural ODE's ( https://arxiv.org/pdf/1806.07366.pdf ).  It seems like the main trick here is to be able to take derivatives of functions of an ODE solver, with respect to neural network parameters. This is done by the adjoint sensitivity method, where we solve a differential equation in order to obtain the gradients of the loss function. To understand this technique, I wanted to implement a simple version. $$ \frac{d z(t)}{dt} = f(z(t), t, \alpha) = \alpha z(t) $$ With conditions: start time $t_0$ , stop time $t_1$ , initial position $z(t_0)$ . Now based on our parameter and initial condition, we then have that our solved ode will give us $z(t) = e^{\alpha(t-t_0)}z(t_0)$ . Suppose I want to minimize the loss function: $$L = (z(t_1) -1)^2 /2$$ . That is, I only care about the value of the solved ODE at time $t_1$ , and i want its value to be 1. I can do this analytically here, so I wanted to solve using the adjoint method, and confirm that the two methods match. According to the adjoint method described in the paper, we then need to solve for the adjoint: $a(t) = \partial L/ \partial z(t)$ . We do this by solving the differential equation which $a$ satisfies: $$ \frac{d a}{d t} = -a \partial f/\partial z $$ we can do this and obtain $$ a(t) = e^{\alpha (t-t_1)} (z(t_1)-1) $$ Which we can easily see matches our boundary condition of $a(t_1) = (z(t_1)-1)$ Now my goal was to find $d L / d \alpha$ , which is given by equation (51) in the paper: $$ \frac{d L}{d \alpha} = - \int_{t_1}^{t_0} a(t) \frac{\partial f}{\partial \alpha} = - \int_{t_1}^{t_0} e^{\alpha (t-t_1)} (z(t_1)-1) z(t) = - \int_{t_1}^{t_0} e^{\alpha (t-t_1)} (z(t_1)-1) e^{\alpha(t-t_0)}z(t_0) $$ $$ = (z(t_1)-1)z(t_0) \sinh((t_1-t_0)\alpha)/\alpha $$ However, we can determine $dL/d\alpha$ analytically here: $$ \frac{dL}{d \alpha} = \frac{dL}{d z(t_1)}\frac{d z(t_1)}{d \alpha} = \big[(e^{\alpha(t_1-t_0)}z(t_0) -1)\big] \big[ (t_1-t_0)e^{\alpha(t_1-t_0)}z(t_0) \big] $$ If I plug the form for $z(t_1)$ into the adjoint result, these two should match. But I get the following Adjoint result: $(e^{\alpha(t_1-t_0)}z(t_0) -1)z(t_0) \sinh((t_1-t_0)\alpha)/\alpha$ Analytic result: $(e^{\alpha(t_1-t_0)}z(t_0) -1)z(t_0)(t_1-t_0)e^{\alpha(t_1-t_0)}$ But these do not match ! If somebody could explain why this is, I would really appreciate it. Neural ODE's seem interesting, but if I can't understand an incredibly simple toy model, I dont see how I can use them. Thanks for your time","I've been trying to understand the gist behind the Chen et. al paper on neural ODE's ( https://arxiv.org/pdf/1806.07366.pdf ).  It seems like the main trick here is to be able to take derivatives of functions of an ODE solver, with respect to neural network parameters. This is done by the adjoint sensitivity method, where we solve a differential equation in order to obtain the gradients of the loss function. To understand this technique, I wanted to implement a simple version. With conditions: start time , stop time , initial position . Now based on our parameter and initial condition, we then have that our solved ode will give us . Suppose I want to minimize the loss function: . That is, I only care about the value of the solved ODE at time , and i want its value to be 1. I can do this analytically here, so I wanted to solve using the adjoint method, and confirm that the two methods match. According to the adjoint method described in the paper, we then need to solve for the adjoint: . We do this by solving the differential equation which satisfies: we can do this and obtain Which we can easily see matches our boundary condition of Now my goal was to find , which is given by equation (51) in the paper: However, we can determine analytically here: If I plug the form for into the adjoint result, these two should match. But I get the following Adjoint result: Analytic result: But these do not match ! If somebody could explain why this is, I would really appreciate it. Neural ODE's seem interesting, but if I can't understand an incredibly simple toy model, I dont see how I can use them. Thanks for your time","
\frac{d z(t)}{dt} = f(z(t), t, \alpha) = \alpha z(t)
 t_0 t_1 z(t_0) z(t) = e^{\alpha(t-t_0)}z(t_0) L = (z(t_1) -1)^2 /2 t_1 a(t) = \partial L/ \partial z(t) a 
\frac{d a}{d t} = -a \partial f/\partial z
 
a(t) = e^{\alpha (t-t_1)} (z(t_1)-1)
 a(t_1) = (z(t_1)-1) d L / d \alpha 
\frac{d L}{d \alpha} = - \int_{t_1}^{t_0} a(t) \frac{\partial f}{\partial \alpha} = - \int_{t_1}^{t_0} e^{\alpha (t-t_1)} (z(t_1)-1) z(t) = - \int_{t_1}^{t_0} e^{\alpha (t-t_1)} (z(t_1)-1) e^{\alpha(t-t_0)}z(t_0)
 
= (z(t_1)-1)z(t_0) \sinh((t_1-t_0)\alpha)/\alpha
 dL/d\alpha 
\frac{dL}{d \alpha} = \frac{dL}{d z(t_1)}\frac{d z(t_1)}{d \alpha} = \big[(e^{\alpha(t_1-t_0)}z(t_0) -1)\big] \big[ (t_1-t_0)e^{\alpha(t_1-t_0)}z(t_0) \big]
 z(t_1) (e^{\alpha(t_1-t_0)}z(t_0) -1)z(t_0) \sinh((t_1-t_0)\alpha)/\alpha (e^{\alpha(t_1-t_0)}z(t_0) -1)z(t_0)(t_1-t_0)e^{\alpha(t_1-t_0)}","['ordinary-differential-equations', 'numerical-methods', 'neural-networks']"
89,Construction of a non-autonomous Hamiltonian diffeomorphism,Construction of a non-autonomous Hamiltonian diffeomorphism,,"Let $(M,\omega)$ be a symplectic manifold. I have read that the autonomous Hamiltonian diffeomorphisms (i.e. a Hamiltonian diffeomorphism generated by a time-independant Hamiltonian) form a proper subset of the Hamiltonian diffeomorphisms of $M$ , but I haven't been able to find a proof of that. Does anyone have a link in which a non-autonomous Hamiltonian diffeomorphism is constructed, and proven that it is indeed non-autonomous.","Let be a symplectic manifold. I have read that the autonomous Hamiltonian diffeomorphisms (i.e. a Hamiltonian diffeomorphism generated by a time-independant Hamiltonian) form a proper subset of the Hamiltonian diffeomorphisms of , but I haven't been able to find a proof of that. Does anyone have a link in which a non-autonomous Hamiltonian diffeomorphism is constructed, and proven that it is indeed non-autonomous.","(M,\omega) M","['ordinary-differential-equations', 'differential-geometry', 'symplectic-geometry']"
90,Why is the solution of $y''=\cos(x)/y$ increasing?,Why is the solution of  increasing?,y''=\cos(x)/y,"I am studying the equation: $$ y''=\frac {\cos(x)}y$$ , with y(0)>>1 and y'(0)=0 . I can't find an exact solution so I calculated it using Runge-Kutta. Intuitively I expect an oscillatory function where the average value remains constant, since y"" is oscillatory. However, the solution I get is increasing. It looks roughly like $$y(x)= y_0+x+\cos(x)$$ Am I doing something wrong or is this the real behavior? And if so, can anyone try to find an explanation of why it would be increasing? I also found that it seems extremely sensitive to the initial value y'(0). If this is negative then the solution is decreasing (this is intuitively so), but I don't get why it is so sensitive, i.e. y'(0)=-0.001 and y(0)=10 will make the solution decreasing. Thank you for any help.","I am studying the equation: , with y(0)>>1 and y'(0)=0 . I can't find an exact solution so I calculated it using Runge-Kutta. Intuitively I expect an oscillatory function where the average value remains constant, since y"" is oscillatory. However, the solution I get is increasing. It looks roughly like Am I doing something wrong or is this the real behavior? And if so, can anyone try to find an explanation of why it would be increasing? I also found that it seems extremely sensitive to the initial value y'(0). If this is negative then the solution is decreasing (this is intuitively so), but I don't get why it is so sensitive, i.e. y'(0)=-0.001 and y(0)=10 will make the solution decreasing. Thank you for any help.", y''=\frac {\cos(x)}y y(x)= y_0+x+\cos(x),['ordinary-differential-equations']
91,is there a real solution to this differential equation?,is there a real solution to this differential equation?,,"This is the equation: $\frac {d^4y}{dx^4}=-y$ I know the solution to this equation: $\frac {d^2y}{dx^2}=-y$ is this: $y(x)=a\cdot \cos(x)+b\cdot \sin(x)$ and it involves $e^{ix}$ , but I wondered if there was any solution to a higher-order version of this.","This is the equation: I know the solution to this equation: is this: and it involves , but I wondered if there was any solution to a higher-order version of this.",\frac {d^4y}{dx^4}=-y \frac {d^2y}{dx^2}=-y y(x)=a\cdot \cos(x)+b\cdot \sin(x) e^{ix},['ordinary-differential-equations']
92,Uniqueness of solution to nonlinear first-order ODE in non-standard form,Uniqueness of solution to nonlinear first-order ODE in non-standard form,,"I have an ODE of a reasonably simple form: $$ -x(y')^2 + 2yy' + x = 0 \qquad \qquad (\ast) $$ for $y$ only assumed to be a differentiable function of $x$ (note: not necessarily $C^1$ !) The proof I saw took another derivative and then used some reduction of order to get something separable. I found the solution to $(\ast)$ with my regularity assumption through doing a substitution, getting a separable DE, solving it, etc. And even then, I already knew the form of the solution, so I wasn't particularly surprised. But someone pointed out that I didn't need to actually grind out the solution, because I could check the pretty-much-known solution satisfied the DE, and then with suitable initial conditions (also known) a uniqueness theorem takes over. However, it's not clear to me what theorem to use. In principle, I could make the substitution and then apply the Picard‚ÄìLindel√∂f theorem to get the uniqueness for the new ODE, but the substitution still feels like inspired magic (it kinda is: I just tried something at random, and was surprised when it worked). Using uniqueness also feels like magic, but of a different sort. At the very least, the solution is well-motivated. So What uniqueness result for ODEs can I apply to $(\ast)$ directly?","I have an ODE of a reasonably simple form: for only assumed to be a differentiable function of (note: not necessarily !) The proof I saw took another derivative and then used some reduction of order to get something separable. I found the solution to with my regularity assumption through doing a substitution, getting a separable DE, solving it, etc. And even then, I already knew the form of the solution, so I wasn't particularly surprised. But someone pointed out that I didn't need to actually grind out the solution, because I could check the pretty-much-known solution satisfied the DE, and then with suitable initial conditions (also known) a uniqueness theorem takes over. However, it's not clear to me what theorem to use. In principle, I could make the substitution and then apply the Picard‚ÄìLindel√∂f theorem to get the uniqueness for the new ODE, but the substitution still feels like inspired magic (it kinda is: I just tried something at random, and was surprised when it worked). Using uniqueness also feels like magic, but of a different sort. At the very least, the solution is well-motivated. So What uniqueness result for ODEs can I apply to directly?","
-x(y')^2 + 2yy' + x = 0 \qquad \qquad (\ast)
 y x C^1 (\ast) (\ast)","['ordinary-differential-equations', 'reference-request']"
93,"Finding the stable, unstable, center manifolds of a nonlinear system","Finding the stable, unstable, center manifolds of a nonlinear system",,"Question: Consider the system \begin{align} \frac{dx}{dt} & = x-xy \\ \frac{dy}{dt} & = 2x-3y+y^2 \end{align} Find the stable, unstable, center manifolds about origin $(0,0)$ up to and including cubic term. Attempt: So first, I computed the Jacobian matrix to be $$J(x,y) = \begin{pmatrix} 1-y & -x \\ 2 & -3+2y \end{pmatrix}$$ Evaluated at the origin, this gives $$J(0,0) = \begin{pmatrix} 1 & 0 \\ 2 & -3 \end{pmatrix}$$ and the eigenvalues are $1$ and $-3$ , which means that the origin is a saddle (i.e. unstable). The corresponding eigenvectors are $\begin{pmatrix} 2 \\ 1 \end{pmatrix}$ and $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$ . ... ... and I'm stuck T_T . What am I supposed to do from here? Any hints would be much appreciated. ...please?","Question: Consider the system Find the stable, unstable, center manifolds about origin up to and including cubic term. Attempt: So first, I computed the Jacobian matrix to be Evaluated at the origin, this gives and the eigenvalues are and , which means that the origin is a saddle (i.e. unstable). The corresponding eigenvectors are and . ... ... and I'm stuck T_T . What am I supposed to do from here? Any hints would be much appreciated. ...please?","\begin{align}
\frac{dx}{dt} & = x-xy \\
\frac{dy}{dt} & = 2x-3y+y^2
\end{align} (0,0) J(x,y) = \begin{pmatrix} 1-y & -x \\ 2 & -3+2y \end{pmatrix} J(0,0) = \begin{pmatrix} 1 & 0 \\ 2 & -3 \end{pmatrix} 1 -3 \begin{pmatrix} 2 \\ 1 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix}","['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'nonlinear-system']"
94,Uniqueness of a nonlinear ODE,Uniqueness of a nonlinear ODE,,"I'm currently looking through qualifying exam questions and one has had me stumped for a few weeks. We are asked to determine whether or not there exists a unique solution of this differential equation in a neighborhood of $x = 0$ . \begin{equation} y'' + \frac{yy'}{x^4} + y^2 = 0 , \text{ } y(0)=y'(0)=0 \end{equation} I believe that the solution is unique with $y = 0$ as the only solution. I have managed to prove that a non-trivial solution must satisfy $y \leq 0$ , but I cannot deal with $y < 0$ case. Edit: Thank you to Ingix for noticing a mistake in my proof, which I edited below. Indeed, assume there exists $f(x)$ that solves the above ODE such that $f(x) > 0$ on say $(0,\varepsilon_1]$ , where $\varepsilon_1$ is positive. Then by MVT, we see \begin{equation} f'(\xi) = \frac{f(\varepsilon_1) - f(0)}{\varepsilon_1-0} > 0, \xi \in (0, \varepsilon_1)  \end{equation} In particular by the continuity of $\text{ }f'$ , there's a neighborhood $(\delta_{1}, \delta_{2}] \subset (0,\varepsilon_1]$ such that $\text{ } f' > 0$ where $\delta_{1} = \sup_{x \in [0,\varepsilon_1]}$ such that $\text{ }f'(x)=0$ . Then by MVT again \begin{equation} \frac{f'(\delta_2)-f'(\delta_1)}{\delta_2 - \delta_1} > 0 \end{equation} Therefore, there's a neighborhood $[\eta_1, \eta_2] \subset (\delta_1, \delta_2)$ such that $f,f',f'' > 0$ . Therefore the ODE cannot be satisfied. I am unsure how to proceed for the case $y < 0$ because the above argument would not give a contradiction. I have tried rewriting the ODE into weak form, using $u = y^2 \Rightarrow u' = 2yy'$ , and I do not see any obvious transformations such as equidimensional in $x/y$ , scale invariant, or autonomous to simplify the ODE. Any hints on how to proceed will be very helpful!","I'm currently looking through qualifying exam questions and one has had me stumped for a few weeks. We are asked to determine whether or not there exists a unique solution of this differential equation in a neighborhood of . I believe that the solution is unique with as the only solution. I have managed to prove that a non-trivial solution must satisfy , but I cannot deal with case. Edit: Thank you to Ingix for noticing a mistake in my proof, which I edited below. Indeed, assume there exists that solves the above ODE such that on say , where is positive. Then by MVT, we see In particular by the continuity of , there's a neighborhood such that where such that . Then by MVT again Therefore, there's a neighborhood such that . Therefore the ODE cannot be satisfied. I am unsure how to proceed for the case because the above argument would not give a contradiction. I have tried rewriting the ODE into weak form, using , and I do not see any obvious transformations such as equidimensional in , scale invariant, or autonomous to simplify the ODE. Any hints on how to proceed will be very helpful!","x = 0 \begin{equation} y'' + \frac{yy'}{x^4} + y^2 = 0 , \text{ } y(0)=y'(0)=0 \end{equation} y = 0 y \leq 0 y < 0 f(x) f(x) > 0 (0,\varepsilon_1] \varepsilon_1 \begin{equation} f'(\xi) = \frac{f(\varepsilon_1) - f(0)}{\varepsilon_1-0} > 0, \xi \in (0, \varepsilon_1)  \end{equation} \text{ }f' (\delta_{1}, \delta_{2}] \subset (0,\varepsilon_1] \text{ } f' > 0 \delta_{1} = \sup_{x \in [0,\varepsilon_1]} \text{ }f'(x)=0 \begin{equation} \frac{f'(\delta_2)-f'(\delta_1)}{\delta_2 - \delta_1} > 0 \end{equation} [\eta_1, \eta_2] \subset (\delta_1, \delta_2) f,f',f'' > 0 y < 0 u = y^2 \Rightarrow u' = 2yy' x/y","['real-analysis', 'ordinary-differential-equations']"
95,"How to prove $f\equiv 0$ $\forall x\in [a,b]$?$\quad$($f''Ôºãpf'ÔºãqfÔºù0$ with $f(a)Ôºùf(b)Ôºù0$)",How to prove  ?( with ),"f\equiv 0 \forall x\in [a,b] \quad f''Ôºãpf'ÔºãqfÔºù0 f(a)Ôºùf(b)Ôºù0","Define $f \in C^{2}[a,b]$ satisfying $f''Ôºãpf'ÔºãqfÔºù0$ with $f(a)Ôºùf(b)Ôºù0$ , where $p\in C^{0}[a,b]$ and $q\in C^{0}[a,b]$ are two functions. If $q\leq0$ , can we prove $f\equiv 0$ $\forall x\in [a,b]$ ? My try: If $f\not\equiv 0$ , without loss of generality, we assume that the maximum of $f$ on $[a,b]$ is greater than zero, while notating $f(x_0)Ôºù\displaystyle\max_{[a,b]} f$ . Then we have $f(x_0) > 0$ , $f'(x_0) Ôºù 0$ and $f''(x_0) \leq 0$ . I figured out that if we alter the condition $q\leq0$ into $q(x)<0$ there evidently exists contradiction. But how to analyze further with the condition $q\leq0$ ? Can we still find contradiction if $q(x_0)Ôºù0$ and $f''(x_0)Ôºù0$ ? Any ideas would be highy appreciated!","Define satisfying with , where and are two functions. If , can we prove ? My try: If , without loss of generality, we assume that the maximum of on is greater than zero, while notating . Then we have , and . I figured out that if we alter the condition into there evidently exists contradiction. But how to analyze further with the condition ? Can we still find contradiction if and ? Any ideas would be highy appreciated!","f \in C^{2}[a,b] f''Ôºãpf'ÔºãqfÔºù0 f(a)Ôºùf(b)Ôºù0 p\in C^{0}[a,b] q\in C^{0}[a,b] q\leq0 f\equiv 0 \forall x\in [a,b] f\not\equiv 0 f [a,b] f(x_0)Ôºù\displaystyle\max_{[a,b]} f f(x_0) > 0 f'(x_0) Ôºù 0 f''(x_0) \leq 0 q\leq0 q(x)<0 q\leq0 q(x_0)Ôºù0 f''(x_0)Ôºù0","['real-analysis', 'ordinary-differential-equations', 'continuity']"
96,Solution to an ODE with every $n$-moment finite.,Solution to an ODE with every -moment finite.,n,"Let $f$ be a differentiable function from $[0, \infty]$ to $\mathbb{R}$ satisfying the following: $$ f'(x)=f(2x)-f(x), $$ $$ M_n=\int_0^\infty x^n f(x) \ dx < \infty. $$ Show that there exists a non null function that satisfies the hypothesis and then enumerate all the sequences $(a_n)_{n \in \mathbb{N}}$ such that $a_n=M_n, \ \forall n \in \mathbb{N}$ . My approach was to search a function of the type $$ f(x) = \sum_n a_n  e^{b_n x} $$ with $a_n \ne 0, b_n <0$ , since such a function converges to a $\mathcal{C}^1$ function as the series converges uniformly. However doing some computation I imagine that $b_n$ as to be something like $-2^n$ , but I do not have many arguments to state that formally, let us say that I only hope it has to be something like that since no polynomial satisfies the hypothesis. Any suggestions? And moreover in such a problem how do I establish a candidate to such condition instead of strongly hoping to have at least a function that is very similar to its derivative?","Let be a differentiable function from to satisfying the following: Show that there exists a non null function that satisfies the hypothesis and then enumerate all the sequences such that . My approach was to search a function of the type with , since such a function converges to a function as the series converges uniformly. However doing some computation I imagine that as to be something like , but I do not have many arguments to state that formally, let us say that I only hope it has to be something like that since no polynomial satisfies the hypothesis. Any suggestions? And moreover in such a problem how do I establish a candidate to such condition instead of strongly hoping to have at least a function that is very similar to its derivative?","f [0, \infty] \mathbb{R} 
f'(x)=f(2x)-f(x),
 
M_n=\int_0^\infty x^n f(x) \ dx < \infty.
 (a_n)_{n \in \mathbb{N}} a_n=M_n, \ \forall n \in \mathbb{N} 
f(x) = \sum_n a_n  e^{b_n x}
 a_n \ne 0, b_n <0 \mathcal{C}^1 b_n -2^n","['calculus', 'real-analysis', 'ordinary-differential-equations', 'analysis']"
97,Solutions to $\frac{Q\partial P}{x\partial x}-\frac{P\partial Q}{x\partial x}-\frac{Q\partial P}{y\partial y}+\frac{P\partial Q}{y\partial y}=0$,Solutions to,\frac{Q\partial P}{x\partial x}-\frac{P\partial Q}{x\partial x}-\frac{Q\partial P}{y\partial y}+\frac{P\partial Q}{y\partial y}=0,"We have $$ F(x,y)= \frac{Q}{x} \frac{\partial P}{\partial x} - \frac{P}{x} \frac{\partial Q}{\partial x}   - \frac{Q}{y} \frac{\partial P}{\partial y}  + \frac{P}{y} \frac{\partial Q}{\partial y}  $$ where $$P = \sum_{i=1}^{N}(a_i x + b_i y)^2 $$ $$Q = \sum_{i=1}^{N}(c_{i} x + d_{i} y)^2$$ and $a_i, b_i, c_{i}, d_{i}$ are constant parameters defined for $1\leq i,j \leq N$. We want to find the solutions of equation $F(x,y)=0$. For a simpler case, where $P$ is defined as above and $Q=1$, by using a change of variable $m=\frac{y}{x}$, we get a quadratic equation for $m$ that could be simply solved and give the two solutions for $m$. I wonder if using same change of variable, one could obtain the solutions for the more general case (with $Q$ of the form given above or similar form). Some background: $x,y$ are coordinates of a point in a two dimensional space and define a line that passes through $(x,y)$ and the origin $(0,0)$. $P$ and $Q$ are derived from projection of some other points on this line. Using $m=y/x$ makes sense since the projections only depend on the slope of the line and not the actual values of $(x,y)$. Our goal is to find the line (defined by its slope $m$) that satisfies $F(x,y)=0$. EDIT: This problem is the special case for a more general problem to find the mapping from an $N$ dimensional space to an $M$ dimensional space that would minimize sum of distances of pairs of similar points ($P$) normalized to sum of distances of pairs of dissimilar points ($Q$). The solution given here solves it for mapping from 2D to 1D. I also posted a new question for the case mapping from 3D to 1D here which I expect to be solvable in a similar way.","We have $$ F(x,y)= \frac{Q}{x} \frac{\partial P}{\partial x} - \frac{P}{x} \frac{\partial Q}{\partial x}   - \frac{Q}{y} \frac{\partial P}{\partial y}  + \frac{P}{y} \frac{\partial Q}{\partial y}  $$ where $$P = \sum_{i=1}^{N}(a_i x + b_i y)^2 $$ $$Q = \sum_{i=1}^{N}(c_{i} x + d_{i} y)^2$$ and $a_i, b_i, c_{i}, d_{i}$ are constant parameters defined for $1\leq i,j \leq N$. We want to find the solutions of equation $F(x,y)=0$. For a simpler case, where $P$ is defined as above and $Q=1$, by using a change of variable $m=\frac{y}{x}$, we get a quadratic equation for $m$ that could be simply solved and give the two solutions for $m$. I wonder if using same change of variable, one could obtain the solutions for the more general case (with $Q$ of the form given above or similar form). Some background: $x,y$ are coordinates of a point in a two dimensional space and define a line that passes through $(x,y)$ and the origin $(0,0)$. $P$ and $Q$ are derived from projection of some other points on this line. Using $m=y/x$ makes sense since the projections only depend on the slope of the line and not the actual values of $(x,y)$. Our goal is to find the line (defined by its slope $m$) that satisfies $F(x,y)=0$. EDIT: This problem is the special case for a more general problem to find the mapping from an $N$ dimensional space to an $M$ dimensional space that would minimize sum of distances of pairs of similar points ($P$) normalized to sum of distances of pairs of dissimilar points ($Q$). The solution given here solves it for mapping from 2D to 1D. I also posted a new question for the case mapping from 3D to 1D here which I expect to be solvable in a similar way.",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'algebraic-geometry']"
98,Boundary Value Problem: $u'' + u-u^3 = 0$,Boundary Value Problem:,u'' + u-u^3 = 0,"Consider the differential equation $$\frac{d^2u}{dx^2} + u ‚àí u^3 = 0$$ where $u'(0) = u(L) = 0$ . If there is a solution which isn‚Äôt identically $0$ which satisfies $u(0) = u_0$, then find a relationship between $L$ and $u_0$. I derived, $$L =\int_0^1 \frac{dz}{\sqrt{(1-z^2)-(2/4)u_{0}(1-z^4)}}$$ where $z=u/u_{0}$ and $dz = du/u_{0}$ I was wondering if I was able to/how to approximate the minimal value of $L$ for which a non-zero solution exists.","Consider the differential equation $$\frac{d^2u}{dx^2} + u ‚àí u^3 = 0$$ where $u'(0) = u(L) = 0$ . If there is a solution which isn‚Äôt identically $0$ which satisfies $u(0) = u_0$, then find a relationship between $L$ and $u_0$. I derived, $$L =\int_0^1 \frac{dz}{\sqrt{(1-z^2)-(2/4)u_{0}(1-z^4)}}$$ where $z=u/u_{0}$ and $dz = du/u_{0}$ I was wondering if I was able to/how to approximate the minimal value of $L$ for which a non-zero solution exists.",,"['ordinary-differential-equations', 'mathematical-modeling', 'stability-in-odes']"
99,Explicit solution for ODE,Explicit solution for ODE,,"I‚Äôve been attempting to find a solution to the following differential equation but to no avail $$2q^{2}h\frac{d^{2} C}{d q^{2}}-h(1+2(v+1)q)\frac{d C}{d q}=\ln (C) - \ln{(h\exp({2h(v+1)}))}$$ Subject to the boundary condition $$C(0)=\frac{\exp(2h(1+v))-1}{2(v+1)}$$ Is it possible to find a closed form solution for this equation? Does anyone know a reference to a paper discussing ODEs of the form $f‚Äô‚Äô(x)+f‚Äô(x)+\ln(f(x))=0$ ? Thanks, J","I‚Äôve been attempting to find a solution to the following differential equation but to no avail $$2q^{2}h\frac{d^{2} C}{d q^{2}}-h(1+2(v+1)q)\frac{d C}{d q}=\ln (C) - \ln{(h\exp({2h(v+1)}))}$$ Subject to the boundary condition $$C(0)=\frac{\exp(2h(1+v))-1}{2(v+1)}$$ Is it possible to find a closed form solution for this equation? Does anyone know a reference to a paper discussing ODEs of the form $f‚Äô‚Äô(x)+f‚Äô(x)+\ln(f(x))=0$ ? Thanks, J",,"['ordinary-differential-equations', 'closed-form']"
