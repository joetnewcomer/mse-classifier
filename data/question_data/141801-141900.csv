,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that $x\mapsto e^x$ is continuous at $x_0 = 1$ ($\delta-\varepsilon$ proof),Prove that  is continuous at  ( proof),x\mapsto e^x x_0 = 1 \delta-\varepsilon,"Prove that the function $$f(x)=e^x:=\lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n$$ is continuous at $x_0=1$ using the delta epsilon definition of continuous, which is: $$\forall \varepsilon >0 \exists \delta>0 (\forall x\in D:|x-x_0|<\delta) |f(x)-f(x_0)|<\varepsilon$$ Since, in this particular context, the limit inside $e^x$ was proved to be convergent for all $x$ using the Bernoulli inequality and monotone convergence theorem, I'm struggling to see how I can apply a delta epsilon proof here.  In my limited experience, I've applied it to nothing more than simple algebraic functions, but this is pretty significantly different.  How do I start?","Prove that the function $$f(x)=e^x:=\lim_{n\to\infty}\left(1+\frac{x}{n}\right)^n$$ is continuous at $x_0=1$ using the delta epsilon definition of continuous, which is: $$\forall \varepsilon >0 \exists \delta>0 (\forall x\in D:|x-x_0|<\delta) |f(x)-f(x_0)|<\varepsilon$$ Since, in this particular context, the limit inside $e^x$ was proved to be convergent for all $x$ using the Bernoulli inequality and monotone convergence theorem, I'm struggling to see how I can apply a delta epsilon proof here.  In my limited experience, I've applied it to nothing more than simple algebraic functions, but this is pretty significantly different.  How do I start?",,"['real-analysis', 'limits', 'continuity', 'exponential-function', 'epsilon-delta']"
1,How can I 'prove' the derivative of this function?,How can I 'prove' the derivative of this function?,,"Consider the function $$ f: (-1, 1) \rightarrow \mathbb{R}, \hspace{15px} f(x) = \sum_{n=1}^{\infty}(-1)^{n+1} \cdot \frac{x^n}{n} $$ I am required to show that the derivative of this function is $$ f'(x) = \frac{1}{1+x} $$ I have attempted to do this using the elementary definition of a limit, as follows: $$ f'(x) = \lim_{a \rightarrow x} \left( \frac{f(a)-f(x)}{a-x} \right) = \lim_{a \rightarrow x} \left( \frac{ \sum_{n=1}^{\infty} \left[ (-1)^{n+1} \cdot \frac{a^n}{n} \right]- \sum_{n=1}^{\infty}\left[ (-1)^{n+1} \cdot \frac{x^n}{n} \right]}{a-x} \right) \\ = \lim_{a \rightarrow x} \left( \frac{ \sum_{n=1}^{\infty} \left[ (-1)^{n+1} \cdot \frac{a^n - x^n}{n} \right]}{a-x} \right) $$ but I am unsure of how to proceed from here (assuming my approach is correct. Can someone help me to show this?","Consider the function $$ f: (-1, 1) \rightarrow \mathbb{R}, \hspace{15px} f(x) = \sum_{n=1}^{\infty}(-1)^{n+1} \cdot \frac{x^n}{n} $$ I am required to show that the derivative of this function is $$ f'(x) = \frac{1}{1+x} $$ I have attempted to do this using the elementary definition of a limit, as follows: $$ f'(x) = \lim_{a \rightarrow x} \left( \frac{f(a)-f(x)}{a-x} \right) = \lim_{a \rightarrow x} \left( \frac{ \sum_{n=1}^{\infty} \left[ (-1)^{n+1} \cdot \frac{a^n}{n} \right]- \sum_{n=1}^{\infty}\left[ (-1)^{n+1} \cdot \frac{x^n}{n} \right]}{a-x} \right) \\ = \lim_{a \rightarrow x} \left( \frac{ \sum_{n=1}^{\infty} \left[ (-1)^{n+1} \cdot \frac{a^n - x^n}{n} \right]}{a-x} \right) $$ but I am unsure of how to proceed from here (assuming my approach is correct. Can someone help me to show this?",,"['real-analysis', 'limits', 'derivatives', 'power-series']"
2,Is $f$ continuous at zero?,Is  continuous at zero?,f,"$$\require{cancel}$$ $$f(x) = \begin{cases} \frac{\sin x}{|x|} &\text{ if }x \neq0 \\ \hspace{0.3cm}1 &\text{ if }x=0. \end{cases}$$ My Attempt 1)$$\lim_{x\rightarrow0}\frac{\sin x}{|x|} = \lim_{x\rightarrow0}\frac{\sin x}{x} \frac{x}{|x|} = 1\lim_{x \rightarrow0}\frac{x}{|x|}  \\$$ 2)$$\lim_{x\rightarrow0^{-}}\frac{x}{|x|}=-1 \hspace{0.3cm}\text{and}\hspace{0.3cm} \lim_{x\rightarrow0^{+}}\frac{x}{|x|}=1 $$ Therefore: $$1\lim_{x\rightarrow0}\frac{x}{|x|}=DNE $$ so, $f$ is not continuous at $0$. My question is does my solution actually prove that $f$ is not continuous at $0$? or is it continuous at zero because $f(x)=1$ when $x=0$?","$$\require{cancel}$$ $$f(x) = \begin{cases} \frac{\sin x}{|x|} &\text{ if }x \neq0 \\ \hspace{0.3cm}1 &\text{ if }x=0. \end{cases}$$ My Attempt 1)$$\lim_{x\rightarrow0}\frac{\sin x}{|x|} = \lim_{x\rightarrow0}\frac{\sin x}{x} \frac{x}{|x|} = 1\lim_{x \rightarrow0}\frac{x}{|x|}  \\$$ 2)$$\lim_{x\rightarrow0^{-}}\frac{x}{|x|}=-1 \hspace{0.3cm}\text{and}\hspace{0.3cm} \lim_{x\rightarrow0^{+}}\frac{x}{|x|}=1 $$ Therefore: $$1\lim_{x\rightarrow0}\frac{x}{|x|}=DNE $$ so, $f$ is not continuous at $0$. My question is does my solution actually prove that $f$ is not continuous at $0$? or is it continuous at zero because $f(x)=1$ when $x=0$?",,"['calculus', 'limits', 'continuity']"
3,What's the implication of l'Hospital's rule on rate of convergence?,What's the implication of l'Hospital's rule on rate of convergence?,,"Consider $h(x)=f(x)/g(x)$, if l'Hospital's rule is applicable, then $$\lim h(x)=\lim\frac{f'(x)}{g'(x)}.$$ Does this fact implies $h(x)$ and $f'(x)/g'(x)$ converge at the same speed? E.g. if $f'(x)/g'(x)\to L$ linearly, can we say the same about $h(x)$? If not, can we say anything in general about the rate of convergence?","Consider $h(x)=f(x)/g(x)$, if l'Hospital's rule is applicable, then $$\lim h(x)=\lim\frac{f'(x)}{g'(x)}.$$ Does this fact implies $h(x)$ and $f'(x)/g'(x)$ converge at the same speed? E.g. if $f'(x)/g'(x)\to L$ linearly, can we say the same about $h(x)$? If not, can we say anything in general about the rate of convergence?",,"['limits', 'convergence-divergence']"
4,how to find the following limit,how to find the following limit,,Problem : Find the following limit $${\displaystyle \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\sin\frac{\pi}{n^{2}}+\left(1+\frac{2}{n}\right)\sin\frac{2\pi}{n^{2}}+\ldots+\left(1+\frac{n-1}{n}\right)\sin\frac{\left(n-1\right)}{n^{2}}\pi\right]}$$. Attempt: Observe that  \begin{align*}  & \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\sin\frac{\pi}{n^{2}}+\left(1+\frac{2}{n}\right)\sin\frac{2\pi}{n^{2}}+\ldots+\left(1+\frac{n-1}{n}\right)\sin\frac{\left(n-1\right)}{n^{2}}\pi\right]\\ = & \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\left(\frac{\pi}{n^{2}}+O\left(\frac{\pi^{3}}{n^{6}}\right)\right)+\ldots+\left(1+\frac{n-1}{n}\right)\left(\left(\frac{n-1}{n^{2}}\right)\pi+O\left(\frac{\left(n-1\right)^{3}\pi^{3}}{n^{6}}\right)\right)\right]\\ = & \lim_{n\rightarrow\infty}\left[\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right)+\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)O\left(\frac{k^{3}\pi^{3}}{n^{6}}\right)\right)\right]\\ = & \lim_{n\rightarrow\infty}\left[\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right)+\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)O\left(\frac{1}{n^{3}}\right)\right]\tag{1} \end{align*} Note that  \begin{align*} \lim_{n\rightarrow\infty}\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{1}{n^{3}} & =\lim_{n\rightarrow\infty}\left(\frac{1}{n^{2}}+\sum_{k=1}^{n}\frac{k}{n^{4}}\right)\leq\lim_{k\rightarrow\infty}\left(\frac{1}{n^{2}}+\frac{1}{n^{3}}\right)=0. \end{align*} Hence  \begin{align*} \left(1\right) & =\lim_{n\rightarrow\infty}\left[\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right]=\pi\lim_{n\rightarrow\infty}\left[\sum_{k=1}^{n}\left(\frac{k}{n}+\left(\frac{k}{n}\right)^{2}\right)\frac{1}{n}\right]\\  & =\pi\int_{0}^{1}x+x^{2}dx=\frac{5\pi}{6}. \end{align*} Question I don't know if I did it correctly.,Problem : Find the following limit $${\displaystyle \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\sin\frac{\pi}{n^{2}}+\left(1+\frac{2}{n}\right)\sin\frac{2\pi}{n^{2}}+\ldots+\left(1+\frac{n-1}{n}\right)\sin\frac{\left(n-1\right)}{n^{2}}\pi\right]}$$. Attempt: Observe that  \begin{align*}  & \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\sin\frac{\pi}{n^{2}}+\left(1+\frac{2}{n}\right)\sin\frac{2\pi}{n^{2}}+\ldots+\left(1+\frac{n-1}{n}\right)\sin\frac{\left(n-1\right)}{n^{2}}\pi\right]\\ = & \lim_{n\rightarrow\infty}\left[\left(1+\frac{1}{n}\right)\left(\frac{\pi}{n^{2}}+O\left(\frac{\pi^{3}}{n^{6}}\right)\right)+\ldots+\left(1+\frac{n-1}{n}\right)\left(\left(\frac{n-1}{n^{2}}\right)\pi+O\left(\frac{\left(n-1\right)^{3}\pi^{3}}{n^{6}}\right)\right)\right]\\ = & \lim_{n\rightarrow\infty}\left[\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right)+\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)O\left(\frac{k^{3}\pi^{3}}{n^{6}}\right)\right)\right]\\ = & \lim_{n\rightarrow\infty}\left[\left(\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right)+\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)O\left(\frac{1}{n^{3}}\right)\right]\tag{1} \end{align*} Note that  \begin{align*} \lim_{n\rightarrow\infty}\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{1}{n^{3}} & =\lim_{n\rightarrow\infty}\left(\frac{1}{n^{2}}+\sum_{k=1}^{n}\frac{k}{n^{4}}\right)\leq\lim_{k\rightarrow\infty}\left(\frac{1}{n^{2}}+\frac{1}{n^{3}}\right)=0. \end{align*} Hence  \begin{align*} \left(1\right) & =\lim_{n\rightarrow\infty}\left[\sum_{k=1}^{n}\left(1+\frac{k}{n}\right)\frac{k\pi}{n^{2}}\right]=\pi\lim_{n\rightarrow\infty}\left[\sum_{k=1}^{n}\left(\frac{k}{n}+\left(\frac{k}{n}\right)^{2}\right)\frac{1}{n}\right]\\  & =\pi\int_{0}^{1}x+x^{2}dx=\frac{5\pi}{6}. \end{align*} Question I don't know if I did it correctly.,,"['calculus', 'real-analysis', 'limits', 'proof-verification']"
5,"Find $\lim_{(x,y) \to (0,0)} \frac{\tan(x^3+y^3)}{\sin(x^2+y^2)}$",Find,"\lim_{(x,y) \to (0,0)} \frac{\tan(x^3+y^3)}{\sin(x^2+y^2)}","Find $$\lim_{(x,y) \to (0,0)}  \frac{\tan(x^3+y^3)}{\sin(x^2+y^2)}$$ What do you guys think about this approach, and are there any faster and easier approaches? My approach: $\frac{\tan(x^3+y^3)(x^2+y^2)}{\sin(x^2+y^2)(x^2+y^2)}$ 2. $\frac{\tan(x^3+y^3)}{(x^2+y^2)}$ $\frac{\sin(x^3+y^3)}{\cos(x^3+y^3)\rightarrow 1(x^2+y^2)}$ $\frac{\sin(x^3+y^3)}{(x^2+y^2)}$ Polar: $rcos^3\phi + rsin^3\phi = 0 + 0 = 0$ Thus, $$\lim_{(x,y) \to (0,0)}  \frac{\tan(x^3+y^3)}{\sin(x^2+y^2)} = 0$$","Find What do you guys think about this approach, and are there any faster and easier approaches? My approach: 2. Polar: Thus,","\lim_{(x,y) \to (0,0)}  \frac{\tan(x^3+y^3)}{\sin(x^2+y^2)} \frac{\tan(x^3+y^3)(x^2+y^2)}{\sin(x^2+y^2)(x^2+y^2)} \frac{\tan(x^3+y^3)}{(x^2+y^2)} \frac{\sin(x^3+y^3)}{\cos(x^3+y^3)\rightarrow 1(x^2+y^2)} \frac{\sin(x^3+y^3)}{(x^2+y^2)} rcos^3\phi + rsin^3\phi = 0 + 0 = 0 \lim_{(x,y) \to (0,0)}  \frac{\tan(x^3+y^3)}{\sin(x^2+y^2)} = 0","['calculus', 'limits', 'multivariable-calculus']"
6,A variant of Dominated Convergence Theorem,A variant of Dominated Convergence Theorem,,"Let $(a_1,a_2,\ldots)$ be a sequence of nonnegative numbers summing up to $1$. Given a measurable space $(X,\mathscr{F})$, assume also the $f:X\to \mathbb{R}$ is a measurable function, and let $(\mu_1,\mu_2,\ldots)$ be a sequence of probability measures $\mathscr{F} \to \mathbb{R}$. Then, is it true that $$ \int_X f \mathrm{d}\left(\sum_{i\ge 1} a_i\mu_i\right)=\sum_{i\ge 1}a_i \left(\int_X f \mathrm{d}\mu_i\right)\,\,\,? $$","Let $(a_1,a_2,\ldots)$ be a sequence of nonnegative numbers summing up to $1$. Given a measurable space $(X,\mathscr{F})$, assume also the $f:X\to \mathbb{R}$ is a measurable function, and let $(\mu_1,\mu_2,\ldots)$ be a sequence of probability measures $\mathscr{F} \to \mathbb{R}$. Then, is it true that $$ \int_X f \mathrm{d}\left(\sum_{i\ge 1} a_i\mu_i\right)=\sum_{i\ge 1}a_i \left(\int_X f \mathrm{d}\mu_i\right)\,\,\,? $$",,['limits']
7,Find the limit of the vector as $~t~$ approaches $~0~$?,Find the limit of the vector as  approaches ?,~t~ ~0~,"Here's the vector: $$e^{-6t}~\vec i + \frac{t^2}{\sin^2t}~\vec j + \sin(6t)~\vec k$$ Don't I just take the limit as $t$ approaches $0$ from each individual component? As a result, I got $~\vec i + \vec j~$ , which was apparently incorrect.","Here's the vector: Don't I just take the limit as approaches from each individual component? As a result, I got , which was apparently incorrect.",e^{-6t}~\vec i + \frac{t^2}{\sin^2t}~\vec j + \sin(6t)~\vec k t 0 ~\vec i + \vec j~,"['limits', 'multivariable-calculus', 'vectors']"
8,Is this limit correct??,Is this limit correct??,,$$\lim_{h \rightarrow 0}h^2\cos \left ( \frac{1}{h} \right )=0$$ since $$-h^2 \leq h^2\cos \left ( \frac{1}{h} \right ) \leq h^2$$ and both the limits of the upper bound and lower bound are zero as $h$ goes to zero?,$$\lim_{h \rightarrow 0}h^2\cos \left ( \frac{1}{h} \right )=0$$ since $$-h^2 \leq h^2\cos \left ( \frac{1}{h} \right ) \leq h^2$$ and both the limits of the upper bound and lower bound are zero as $h$ goes to zero?,,"['limits', 'proof-verification']"
9,How can I compute the limit of this sequence: $\sqrt[n]{\sin n}$?,How can I compute the limit of this sequence: ?,\sqrt[n]{\sin n},"I need to calculate the limit of the following sequence: $$\lim _ {n \to \infty} \sqrt[n]{\sin(n)}$$ where the $n$-th root of a negative number is defined as the principal complex root. I suspect the answer to be $1$, but I do not know how to prove it.","I need to calculate the limit of the following sequence: $$\lim _ {n \to \infty} \sqrt[n]{\sin(n)}$$ where the $n$-th root of a negative number is defined as the principal complex root. I suspect the answer to be $1$, but I do not know how to prove it.",,"['limits', 'complex-numbers']"
10,My proof ɛ-δ Definition of a limit only disproves the limit... What have I done wrong?,My proof ɛ-δ Definition of a limit only disproves the limit... What have I done wrong?,,"Whilst attempting to prove that $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 $$  I came up with the following as my proof: $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-\sqrt[3]{(27)}|≤ε⇒$$ $$|\sqrt[3]{(3x-24)}|≤ε⇒|\sqrt[3]{(3(x-8))}|≤ε∴|\sqrt[3]{(3δ)}|≤ε□$$ But that's when I realized that even though  $$If δ=9, \;then \;ε≥3∵(f(x_{0}-δ)≤y_{0}-ε)$$ that $$If δ=9, \;then \;ε≥3※∵¬(f(x_{0}+δ)≤y_{0}+ε)$$ and thus my proof falls apart. Where did I make a mistake? (or maybe I am misunderstanding, and my proof is correct?) Thank you in advance! Note: I am not 100% certain that I used all of these mathematical symbols correctly... EDIT: Thanks to the comments that the wonderful people of math.stackexchange.com have provided below, I have concluded that $$|\sqrt[3]{a}-\sqrt[3]{b}| ≠ |\sqrt[3]{(a-b)}|$$ Even so, the expression could be true, but they are not always equal. Therefore, I must re-evaluate this problem, and I will update this question if I find another (correct) solution. If anyone knows how to find the correct proof, please let me know what it is! Thank you! SECOND EDIT: I have refined my proof to this: $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-\sqrt[3]{(27)}|≤ε⇒$$ $$|\sqrt[3]{(3x-24)}-\sqrt[3]{27}+\sqrt[3]{27}|≤ε⇒|\sqrt[3]{(3x-24)}|≤ε⇒$$ $$|\sqrt[3]{(3(x-8))}|≤ε∴|\sqrt[3]{(3δ)}|≤ε□$$ Yet I come out to the same answer. Unless  $$\sqrt[3]{(3x+3)}-\sqrt[3]{27} ≠ \sqrt[3]{3x+3}-\sqrt[3]{27}+\sqrt[3]{27}$$ I have no idea what's still wrong about it! Again, thank you for helping! THIRD EDIT: I figured it out! The attempt above has the same problem as the first, I just made the mistake in a different way. Here's the real answer, which actually works. $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-3|≤ε⇒$$ $$|\sqrt[3]{3x-24+27}-3|≤ε⇒|\sqrt[3]{3(x-8)+27}-3|≤ε∴\sqrt[3]{3δ+27}-3≤ε□$$ And that's it! It works! Thank you!","Whilst attempting to prove that $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 $$  I came up with the following as my proof: $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-\sqrt[3]{(27)}|≤ε⇒$$ $$|\sqrt[3]{(3x-24)}|≤ε⇒|\sqrt[3]{(3(x-8))}|≤ε∴|\sqrt[3]{(3δ)}|≤ε□$$ But that's when I realized that even though  $$If δ=9, \;then \;ε≥3∵(f(x_{0}-δ)≤y_{0}-ε)$$ that $$If δ=9, \;then \;ε≥3※∵¬(f(x_{0}+δ)≤y_{0}+ε)$$ and thus my proof falls apart. Where did I make a mistake? (or maybe I am misunderstanding, and my proof is correct?) Thank you in advance! Note: I am not 100% certain that I used all of these mathematical symbols correctly... EDIT: Thanks to the comments that the wonderful people of math.stackexchange.com have provided below, I have concluded that $$|\sqrt[3]{a}-\sqrt[3]{b}| ≠ |\sqrt[3]{(a-b)}|$$ Even so, the expression could be true, but they are not always equal. Therefore, I must re-evaluate this problem, and I will update this question if I find another (correct) solution. If anyone knows how to find the correct proof, please let me know what it is! Thank you! SECOND EDIT: I have refined my proof to this: $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-\sqrt[3]{(27)}|≤ε⇒$$ $$|\sqrt[3]{(3x-24)}-\sqrt[3]{27}+\sqrt[3]{27}|≤ε⇒|\sqrt[3]{(3x-24)}|≤ε⇒$$ $$|\sqrt[3]{(3(x-8))}|≤ε∴|\sqrt[3]{(3δ)}|≤ε□$$ Yet I come out to the same answer. Unless  $$\sqrt[3]{(3x+3)}-\sqrt[3]{27} ≠ \sqrt[3]{3x+3}-\sqrt[3]{27}+\sqrt[3]{27}$$ I have no idea what's still wrong about it! Again, thank you for helping! THIRD EDIT: I figured it out! The attempt above has the same problem as the first, I just made the mistake in a different way. Here's the real answer, which actually works. $$\lim_{x\to8} \sqrt[3]{(3x+3)} = 3 ⇔ (∀ε>0)(∃δ>0)(∀x)|x-8|<δ⇒|\sqrt[3]{(3x+3)}-3|≤ε⇒$$ $$|\sqrt[3]{3x-24+27}-3|≤ε⇒|\sqrt[3]{3(x-8)+27}-3|≤ε∴\sqrt[3]{3δ+27}-3≤ε□$$ And that's it! It works! Thank you!",,"['calculus', 'limits', 'epsilon-delta']"
11,Continuity ( Functions of 2 variables ).,Continuity ( Functions of 2 variables ).,,"Given , $$ f(x,y) = \begin{cases}        \dfrac{xy^{3}}{x^{2}+y^{6}} & (x,y)\neq(0,0) \\       0 & (x,y)=(0,0) \\  \end{cases} $$ We need to check whether the function is continuous at $(0,0)$ or not.. The solution says it is continuous at $(0,0)$. What I tried was the following; For the function to be continuous at the point $(0,0)$, the limit $$\lim_{(x,y) \to (0,0)} f(x,y)$$ should exist. Consider $$\lim_{(x,y) \to (0,0)} \frac{xy^{3}}{x^{2}+y^{6}}$$ I choose a path $y=mx^{\frac{1}{3}}$ and approach $(0,0)$ along this path, thus the above expression becomes $$\lim_{(x,y) \to (0,0)} \frac{xm^{3}x}{x^{2}+m^{6}x^{2}}$$ which comes out to be $$\frac{m^{3}}{1+m^{6}}$$ Clearly the limit isn't unique and should not exist, but the solution says that the function is continuous at $(0,0)$. How ? Can anyone help? What am I doing wrong ?","Given , $$ f(x,y) = \begin{cases}        \dfrac{xy^{3}}{x^{2}+y^{6}} & (x,y)\neq(0,0) \\       0 & (x,y)=(0,0) \\  \end{cases} $$ We need to check whether the function is continuous at $(0,0)$ or not.. The solution says it is continuous at $(0,0)$. What I tried was the following; For the function to be continuous at the point $(0,0)$, the limit $$\lim_{(x,y) \to (0,0)} f(x,y)$$ should exist. Consider $$\lim_{(x,y) \to (0,0)} \frac{xy^{3}}{x^{2}+y^{6}}$$ I choose a path $y=mx^{\frac{1}{3}}$ and approach $(0,0)$ along this path, thus the above expression becomes $$\lim_{(x,y) \to (0,0)} \frac{xm^{3}x}{x^{2}+m^{6}x^{2}}$$ which comes out to be $$\frac{m^{3}}{1+m^{6}}$$ Clearly the limit isn't unique and should not exist, but the solution says that the function is continuous at $(0,0)$. How ? Can anyone help? What am I doing wrong ?",,"['limits', 'multivariable-calculus', 'continuity']"
12,Bolzano -Weierstrass Theorem and uniform Continuity,Bolzano -Weierstrass Theorem and uniform Continuity,,"The following problem has hints, but I am unable presently to use it. Suppose $f$ is uniformly continuous on $(a,b]$ , and let $\{x_n\}$ be any fixed sequence in $(a,b]$ converging to $a$ . Show that the sequence $\{f(x_n)\}$ has a convergent subsequence (Hint: Use the Bolzano Weierstrass Theorem) Now let $L$ be the limit of the convergent subsequence of $\{f(x_n)\}$ . Prove, using the uniform continuity of $f$ on $(a,b]$ , that $$\lim_{x\to a^+}f(x)=L.$$","The following problem has hints, but I am unable presently to use it. Suppose is uniformly continuous on , and let be any fixed sequence in converging to . Show that the sequence has a convergent subsequence (Hint: Use the Bolzano Weierstrass Theorem) Now let be the limit of the convergent subsequence of . Prove, using the uniform continuity of on , that","f (a,b] \{x_n\} (a,b] a \{f(x_n)\} L \{f(x_n)\} f (a,b] \lim_{x\to a^+}f(x)=L.","['real-analysis', 'limits', 'uniform-continuity']"
13,"Limit of $\frac{\sin(x+y)}{x+y}$ as (x,y)→(0,0)","Limit of  as (x,y)→(0,0)",\frac{\sin(x+y)}{x+y},"$$ \lim\limits_{(x, y)\to (0, 0)}\frac{\sin(x+y)}{x+y} $$ I did the following $a)$ along $x$ axis, the limit is one $b)$ along $y$ axis the limit is one $c)$ along $y=x$ the limit is one Since there exists more ways to approach the origin, I know I cannot conclude from the steps given above. $d)$ along $y= -x$ $\frac{\sin(x-x)}{x-x}$ is not defined. Isn't it still possible for the function to have a limit, even though it is not defined at that point? How do I conclude whether a limit exists or doesn't in such a case? EDIT So for, $$ \lim\limits_{(x, y)\to (0, 0)}\frac{\sin(xy)}{xy} $$ Can I proceed in a similar manner and perform a substitution and state the limit is 1?","$$ \lim\limits_{(x, y)\to (0, 0)}\frac{\sin(x+y)}{x+y} $$ I did the following $a)$ along $x$ axis, the limit is one $b)$ along $y$ axis the limit is one $c)$ along $y=x$ the limit is one Since there exists more ways to approach the origin, I know I cannot conclude from the steps given above. $d)$ along $y= -x$ $\frac{\sin(x-x)}{x-x}$ is not defined. Isn't it still possible for the function to have a limit, even though it is not defined at that point? How do I conclude whether a limit exists or doesn't in such a case? EDIT So for, $$ \lim\limits_{(x, y)\to (0, 0)}\frac{\sin(xy)}{xy} $$ Can I proceed in a similar manner and perform a substitution and state the limit is 1?",,"['limits', 'multivariable-calculus', 'limits-without-lhopital']"
14,Find the limit of a $f(x)=\frac{\lfloor x^2\rfloor}{x^2}$ at an arbitrary point,Find the limit of a  at an arbitrary point,f(x)=\frac{\lfloor x^2\rfloor}{x^2},"The function f is defined $f(x)=\frac{\lfloor x^2\rfloor}{x^2}$ I need to find the limit of the function at an arbitrary point. For the continuous parts it was fine, and also for right sided limit at positive points of discontinuity (and left sided for negatives, for all of which the lim is 1), and now I'm left with left sided limit of the function at positive points of discontinuity (and vice versa for the negative part). I know the answer from intuition: $\frac{(x^2-1)}{x^2}$, but I can't find the key to the proof. My attempt is as follows (letting the point of discontinuity be $x_0>0$) First, I restrict delta such that $f(x)=\frac{(x_0^2-1)}{x^2}$ Then, if $x_0=1$ then $f(x)-L=0< \varepsilon$ (restricting $x>0$) Otherwise, I restrict my neighborhood again to $1< x< x_0$, such that $x<x_0 \rightarrow ... \rightarrow 1/x^2 - 1/x_0^2$ and therefore $f(x)-L=...=(x_0^2-1)(1/x^2 - 1/x_0^2)< \varepsilon$ And now I'm stuck... I can't seem to find the right combo to make the delta-epsilon magic to work. Thanks for any help!","The function f is defined $f(x)=\frac{\lfloor x^2\rfloor}{x^2}$ I need to find the limit of the function at an arbitrary point. For the continuous parts it was fine, and also for right sided limit at positive points of discontinuity (and left sided for negatives, for all of which the lim is 1), and now I'm left with left sided limit of the function at positive points of discontinuity (and vice versa for the negative part). I know the answer from intuition: $\frac{(x^2-1)}{x^2}$, but I can't find the key to the proof. My attempt is as follows (letting the point of discontinuity be $x_0>0$) First, I restrict delta such that $f(x)=\frac{(x_0^2-1)}{x^2}$ Then, if $x_0=1$ then $f(x)-L=0< \varepsilon$ (restricting $x>0$) Otherwise, I restrict my neighborhood again to $1< x< x_0$, such that $x<x_0 \rightarrow ... \rightarrow 1/x^2 - 1/x_0^2$ and therefore $f(x)-L=...=(x_0^2-1)(1/x^2 - 1/x_0^2)< \varepsilon$ And now I'm stuck... I can't seem to find the right combo to make the delta-epsilon magic to work. Thanks for any help!",,"['limits', 'ceiling-and-floor-functions']"
15,For what numbers does $\lim_{n\to\infty}\sin(2\pi xn!)$ converge,For what numbers does  converge,\lim_{n\to\infty}\sin(2\pi xn!),"For any real number $x\in\mathbb R$, when does the following limit converge? $$ \lim_{n\to\infty}\sin(2\pi xn!) $$ For $\frac{p}{q}=x\in\mathbb Q$ it converges to $0$ beacuse for any sufficiently large $n:xn!\in\mathbb N$ and then we get $\sin$ of a whole multiply of $2\pi$. (actually you can take $\pi$, not $2\pi$.) My question is, are there any $x\in\mathbb{R-Q}$ such that the limit converges? What about $x\in\mathbb C$?","For any real number $x\in\mathbb R$, when does the following limit converge? $$ \lim_{n\to\infty}\sin(2\pi xn!) $$ For $\frac{p}{q}=x\in\mathbb Q$ it converges to $0$ beacuse for any sufficiently large $n:xn!\in\mathbb N$ and then we get $\sin$ of a whole multiply of $2\pi$. (actually you can take $\pi$, not $2\pi$.) My question is, are there any $x\in\mathbb{R-Q}$ such that the limit converges? What about $x\in\mathbb C$?",,['limits']
16,Existence and unicity of a constant $c$ such that $\lim_{x \rightarrow \pi} \frac {x + c}{\sin x}$ is finite,Existence and unicity of a constant  such that  is finite,c \lim_{x \rightarrow \pi} \frac {x + c}{\sin x},"Supposing $c = - \pi$. Then, the quotient will assume the form $0/0$. Using L'Hospital $$\displaystyle \lim_{x \rightarrow \pi} \dfrac {x-\pi}{\sin x} = \lim_{x \rightarrow \pi} \dfrac {1}{\cos x} = -1$$ So, for $c= - \pi$, $ \dfrac {x + c}{\sin x}$ tends to a finite limit when x $ \rightarrow \pi$. What argument could I use to prove that that's the only value for c such that the limit is finite?","Supposing $c = - \pi$. Then, the quotient will assume the form $0/0$. Using L'Hospital $$\displaystyle \lim_{x \rightarrow \pi} \dfrac {x-\pi}{\sin x} = \lim_{x \rightarrow \pi} \dfrac {1}{\cos x} = -1$$ So, for $c= - \pi$, $ \dfrac {x + c}{\sin x}$ tends to a finite limit when x $ \rightarrow \pi$. What argument could I use to prove that that's the only value for c such that the limit is finite?",,"['calculus', 'real-analysis', 'limits', 'trigonometry']"
17,"limit $\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$",limit,"\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}","Let $$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$$ How do I show it's limit is $0$? I thought about: $$\frac{xy}{\|(x,y)\|} \le \frac{xy}{|x|} \le \frac{xy}{x} \le y \to 0$$ Is that correct?","Let $$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}}$$ How do I show it's limit is $0$? I thought about: $$\frac{xy}{\|(x,y)\|} \le \frac{xy}{|x|} \le \frac{xy}{x} \le y \to 0$$ Is that correct?",,"['calculus', 'limits', 'multivariable-calculus']"
18,A representation of Dirac-$\delta$,A representation of Dirac-,\delta,"Prove that $$g_\epsilon (x)=\lim_{\epsilon \to 0} \frac1 \epsilon \frac1 \pi e^{-x^2/\epsilon^2}$$ is a Dirac-$\delta$ function. This is a homework question I'm stuck with. I'm probably missing a very simple point, and can't seem to figure it out. Any help to prompt me in the right direction would be much appreciated. What I've done so far is the following. I need to show that, $$\int_{-\infty}^\infty f(t)g_\epsilon (x)dt=f(0)$$ So let $t=\epsilon x$, $$\lim_{\epsilon \to 0} \int_{-\infty}^\infty f(t) \frac1 \epsilon e^{-x^2/\epsilon^2}dt=$$ $$\stackrel{t=\epsilon x}{=}\lim_{\epsilon \to 0} \int_{-\infty}^\infty f(\epsilon x) \frac1 \epsilon \frac1 \pi e^{-x^2/\epsilon^2}\epsilon dx$$ $$=f(0) \frac1 \pi \lim_{\epsilon \to 0}  \int_{-\infty}^\infty e^{-x^2/\epsilon^2} dx$$ And since $\lim_{\epsilon \to 0} \int_{-\infty}^\infty e^{-x^2/\epsilon^2}=0$, the result is $0$, not $f(0)$. Another approach was to just set $t=x$ so that $\lim_{\epsilon \to 0} \int_{-\infty}^\infty \frac1 \epsilon e^{-x^2/\epsilon^2}=\sqrt\pi$ would cancel out with $1/\pi$, but then I'm left with $f(x)$ instead of $f(0)$.","Prove that $$g_\epsilon (x)=\lim_{\epsilon \to 0} \frac1 \epsilon \frac1 \pi e^{-x^2/\epsilon^2}$$ is a Dirac-$\delta$ function. This is a homework question I'm stuck with. I'm probably missing a very simple point, and can't seem to figure it out. Any help to prompt me in the right direction would be much appreciated. What I've done so far is the following. I need to show that, $$\int_{-\infty}^\infty f(t)g_\epsilon (x)dt=f(0)$$ So let $t=\epsilon x$, $$\lim_{\epsilon \to 0} \int_{-\infty}^\infty f(t) \frac1 \epsilon e^{-x^2/\epsilon^2}dt=$$ $$\stackrel{t=\epsilon x}{=}\lim_{\epsilon \to 0} \int_{-\infty}^\infty f(\epsilon x) \frac1 \epsilon \frac1 \pi e^{-x^2/\epsilon^2}\epsilon dx$$ $$=f(0) \frac1 \pi \lim_{\epsilon \to 0}  \int_{-\infty}^\infty e^{-x^2/\epsilon^2} dx$$ And since $\lim_{\epsilon \to 0} \int_{-\infty}^\infty e^{-x^2/\epsilon^2}=0$, the result is $0$, not $f(0)$. Another approach was to just set $t=x$ so that $\lim_{\epsilon \to 0} \int_{-\infty}^\infty \frac1 \epsilon e^{-x^2/\epsilon^2}=\sqrt\pi$ would cancel out with $1/\pi$, but then I'm left with $f(x)$ instead of $f(0)$.",,"['limits', 'dirac-delta']"
19,Solving the Hamilton-Jacobi equation (Evans),Solving the Hamilton-Jacobi equation (Evans),,"I can comprehend some but not all of the proofs. I do not understand how the limit definition of a derivative is derived in this context, and those are highlighted in $\color{#009900}{\text{green}}$. This is from PDE Evans, 2nd edition, pages 127-128. Theorem 5 (Solving the Hamilton-Jacobi equation). Suppose $x \in \mathbb{R}^n$, $t > 0$, and $u$ defined by the Hopf-Lax formula $$u(x,t)=\min_{y\in\mathbb{R}^n} \left\{tL\left(\frac{x-y}{t} \right) + g(y) \right\}$$ is differentiable at a point $(x,t) \in \mathbb{R}^n \times (0,\infty)$. Then $$u_t(x,t)+H(Du(x,t))=0.$$ Proof. 1.) Fix $v \in \mathbb{R}^n, h > 0$. Owing to Lemma 1,    \begin{align} u(x+hv,t+h)&=\min_{y \in \mathbb{R}^n} \left\{ hL\left(\frac{x+hv-y}  {h}\right)+u(y,t)\right\} \\ &\le hL(v)+u(x,t). \end{align}   Hence, $$\frac{u(x+hv,t+h)-u(x,t)}{h} \le L(v).$$   Let $h \rightarrow 0^+$, to compute $$\underbrace{v \cdot Du(x,t)+u_t(x,t)}_{\color{#009900}{\text{How is this expression obtained?}}} \le L(v).$$   This inequality is valid for all $v \in \mathbb{R}^n$, and so $$u_t(x,t)+H(Du(x,t))=u_t(x,t)+\max_{v \in \mathbb{R}^n} \{v \cdot Du(x,t)-L(v) \} \le 0. \tag{31}$$   The first equality holds since $H = L^*:=\max_{v \in \mathbb{R}^n} \{v \cdot Du(x,t)-L(v) \}$, by convex duality of Hamiltonian and Lagrangian (page 121 of the book). 2.) Now chose $z$ such that $u(x,t)=tL(\frac{x-z}{t})+g(z)$. Fix $h > 0$ and set $s=t-h,y=\frac st x+(1- \frac st)z$. Then $\frac{x-z}{t}=\frac{y-z}{s}$, and thus   \begin{align} u(x,t)-u(y,s) &\ge tL\left(\frac{y-z}{s}  \right) + g(z) - \left[sL\left(\frac{y-z}{s} \right)+g(z) \right] \\ &= (t-s)L\left(\frac{y-z}{s} \right) \\ &=hL\left(\frac{y-z}{s} \right). \end{align}    That is, $$\frac{u(x,t)-u((1-\frac ht)x+\frac htz,t-h)}{h} \ge L\left(\frac{x-z}{t} \right).$$    Let $h \rightarrow 0^+$, to see that $$\underbrace{\frac{x-z}{t} \cdot Du(x,t)+u_t(x,t)}_{\color{#009900}{\text{How is the limit definition of derivative applied here exactly?}}} \ge L\left(\frac{x-z}{t} \right).$$    Consequently, \begin{align} u_t(x,t)+H(Du(x,t))&=u_t(x,t)+\max_{v\in\mathbb{R}^n} \{v \cdot Du(x,t)-L(v) \} \\ &\ge u_t(x,t)+\frac{x-z}{t} \cdot Du(x,t)-L\left(\frac{x-z}{t} \right) \\ &\ge 0 \end{align}   This inequality and $\text{(31)}$ complete the proof.","I can comprehend some but not all of the proofs. I do not understand how the limit definition of a derivative is derived in this context, and those are highlighted in $\color{#009900}{\text{green}}$. This is from PDE Evans, 2nd edition, pages 127-128. Theorem 5 (Solving the Hamilton-Jacobi equation). Suppose $x \in \mathbb{R}^n$, $t > 0$, and $u$ defined by the Hopf-Lax formula $$u(x,t)=\min_{y\in\mathbb{R}^n} \left\{tL\left(\frac{x-y}{t} \right) + g(y) \right\}$$ is differentiable at a point $(x,t) \in \mathbb{R}^n \times (0,\infty)$. Then $$u_t(x,t)+H(Du(x,t))=0.$$ Proof. 1.) Fix $v \in \mathbb{R}^n, h > 0$. Owing to Lemma 1,    \begin{align} u(x+hv,t+h)&=\min_{y \in \mathbb{R}^n} \left\{ hL\left(\frac{x+hv-y}  {h}\right)+u(y,t)\right\} \\ &\le hL(v)+u(x,t). \end{align}   Hence, $$\frac{u(x+hv,t+h)-u(x,t)}{h} \le L(v).$$   Let $h \rightarrow 0^+$, to compute $$\underbrace{v \cdot Du(x,t)+u_t(x,t)}_{\color{#009900}{\text{How is this expression obtained?}}} \le L(v).$$   This inequality is valid for all $v \in \mathbb{R}^n$, and so $$u_t(x,t)+H(Du(x,t))=u_t(x,t)+\max_{v \in \mathbb{R}^n} \{v \cdot Du(x,t)-L(v) \} \le 0. \tag{31}$$   The first equality holds since $H = L^*:=\max_{v \in \mathbb{R}^n} \{v \cdot Du(x,t)-L(v) \}$, by convex duality of Hamiltonian and Lagrangian (page 121 of the book). 2.) Now chose $z$ such that $u(x,t)=tL(\frac{x-z}{t})+g(z)$. Fix $h > 0$ and set $s=t-h,y=\frac st x+(1- \frac st)z$. Then $\frac{x-z}{t}=\frac{y-z}{s}$, and thus   \begin{align} u(x,t)-u(y,s) &\ge tL\left(\frac{y-z}{s}  \right) + g(z) - \left[sL\left(\frac{y-z}{s} \right)+g(z) \right] \\ &= (t-s)L\left(\frac{y-z}{s} \right) \\ &=hL\left(\frac{y-z}{s} \right). \end{align}    That is, $$\frac{u(x,t)-u((1-\frac ht)x+\frac htz,t-h)}{h} \ge L\left(\frac{x-z}{t} \right).$$    Let $h \rightarrow 0^+$, to see that $$\underbrace{\frac{x-z}{t} \cdot Du(x,t)+u_t(x,t)}_{\color{#009900}{\text{How is the limit definition of derivative applied here exactly?}}} \ge L\left(\frac{x-z}{t} \right).$$    Consequently, \begin{align} u_t(x,t)+H(Du(x,t))&=u_t(x,t)+\max_{v\in\mathbb{R}^n} \{v \cdot Du(x,t)-L(v) \} \\ &\ge u_t(x,t)+\frac{x-z}{t} \cdot Du(x,t)-L\left(\frac{x-z}{t} \right) \\ &\ge 0 \end{align}   This inequality and $\text{(31)}$ complete the proof.",,"['limits', 'partial-differential-equations', 'hamilton-jacobi-equation']"
20,How to find $\lim\limits_{n\to\infty}\sum\limits_{j=1}^{n^2}\frac{n}{n^2+j^2}$,How to find,\lim\limits_{n\to\infty}\sum\limits_{j=1}^{n^2}\frac{n}{n^2+j^2},"find the limit value $$\lim_{n\to\infty}\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}$$ this following is my methods: let $$S_{n}=\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}=\sum_{j=1}^{n^2}\dfrac{1}{1+\left(\dfrac{j}{n}\right)^2}\dfrac{1}{n}$$   since   $$\int_{\dfrac{j}{n}}^{\dfrac{j+1}{n}}\dfrac{dx}{1+x^2}<\dfrac{1}{1+\left(\dfrac{j}{n}\right)^2}\cdot\dfrac{1}{n}<\int_{\dfrac{j-1}{n}}^{\dfrac{j}{n}}\dfrac{dx}{1+x^2}$$   so   $$\int_{\dfrac{1}{n}}^{\dfrac{n^2+1}{n}}\dfrac{dx}{1+x^2}<S_{n}<\int_{0}^{n}\dfrac{dx}{1+x^2}$$ and note $$\lim_{n\to\infty}\int_{\dfrac{1}{n}}^{\dfrac{n^2+1}{n}}\dfrac{dx}{1+x^2}=\lim_{n\to\infty}\int_{0}^{n}\dfrac{dx}{1+x^2}=\int_{0}^{infty}\dfrac{dx}{1+x^2}=\dfrac{\pi}{2}$$   so   $$\lim_{n\to\infty}\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}=\dfrac{\pi}{2}$$ I think this problem have other nice methods? Thank you and follow other methods $$\lim_{n\to\infty}\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}=\lim_{n\to\infty}\int_{0}^{n}\dfrac{1}{1+x^2}dx=\dfrac{\pi}{2}$$   But there is a book say This methods is wrong ,why, and where is wrong? Thank you","find the limit value $$\lim_{n\to\infty}\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}$$ this following is my methods: let $$S_{n}=\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}=\sum_{j=1}^{n^2}\dfrac{1}{1+\left(\dfrac{j}{n}\right)^2}\dfrac{1}{n}$$   since   $$\int_{\dfrac{j}{n}}^{\dfrac{j+1}{n}}\dfrac{dx}{1+x^2}<\dfrac{1}{1+\left(\dfrac{j}{n}\right)^2}\cdot\dfrac{1}{n}<\int_{\dfrac{j-1}{n}}^{\dfrac{j}{n}}\dfrac{dx}{1+x^2}$$   so   $$\int_{\dfrac{1}{n}}^{\dfrac{n^2+1}{n}}\dfrac{dx}{1+x^2}<S_{n}<\int_{0}^{n}\dfrac{dx}{1+x^2}$$ and note $$\lim_{n\to\infty}\int_{\dfrac{1}{n}}^{\dfrac{n^2+1}{n}}\dfrac{dx}{1+x^2}=\lim_{n\to\infty}\int_{0}^{n}\dfrac{dx}{1+x^2}=\int_{0}^{infty}\dfrac{dx}{1+x^2}=\dfrac{\pi}{2}$$   so   $$\lim_{n\to\infty}\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}=\dfrac{\pi}{2}$$ I think this problem have other nice methods? Thank you and follow other methods $$\lim_{n\to\infty}\sum_{j=1}^{n^2}\dfrac{n}{n^2+j^2}=\lim_{n\to\infty}\int_{0}^{n}\dfrac{1}{1+x^2}dx=\dfrac{\pi}{2}$$   But there is a book say This methods is wrong ,why, and where is wrong? Thank you",,['limits']
21,Does $\varprojlim\ ^1$ vanish whenever it doesn't have to account for non-right exactness of $\varprojlim$?,Does  vanish whenever it doesn't have to account for non-right exactness of ?,\varprojlim\ ^1 \varprojlim,"The projective limit functor is not right-exact: if $G_\bullet\rightarrowtail H_\bullet\twoheadrightarrow K_\bullet$ is a projective system of extensions, then there is a long exact sequence $$ 0\to\varprojlim G_\bullet\to\varprojlim H_\bullet\to\varprojlim K_\bullet\to\varprojlim\ ^1 G_\bullet\to\varprojlim\ ^1 H_\bullet\to\varprojlim\ ^1 K_\bullet\to\cdots. $$ Is there an example of a projective system of extensions $G_\bullet\rightarrowtail H_\bullet\twoheadrightarrow K_\bullet$ such that the map $\varprojlim H_\bullet\to\varprojlim K_\bullet$ is surjective and such that the term $\varprojlim\ ^1 G_\bullet$ is non-zero?","The projective limit functor is not right-exact: if $G_\bullet\rightarrowtail H_\bullet\twoheadrightarrow K_\bullet$ is a projective system of extensions, then there is a long exact sequence $$ 0\to\varprojlim G_\bullet\to\varprojlim H_\bullet\to\varprojlim K_\bullet\to\varprojlim\ ^1 G_\bullet\to\varprojlim\ ^1 H_\bullet\to\varprojlim\ ^1 K_\bullet\to\cdots. $$ Is there an example of a projective system of extensions $G_\bullet\rightarrowtail H_\bullet\twoheadrightarrow K_\bullet$ such that the map $\varprojlim H_\bullet\to\varprojlim K_\bullet$ is surjective and such that the term $\varprojlim\ ^1 G_\bullet$ is non-zero?",,"['limits', 'homological-algebra', 'derived-functors', 'exact-sequence']"
22,Prove that $f$ having a limit at $0$ implies that $f$ has a limit at every real number,Prove that  having a limit at  implies that  has a limit at every real number,f 0 f,"Assume $ f: \mathbb{R} \to \mathbb{R}$ is such that $f(x+y)=f(x)f(y)$ ( The class of exponential functions has this property). Prove that $f$ having a limit at $0$ implies that $f$ has a limit at every real number and is one, or $f$ is identically $0$ for every $ x \in \mathbb{R}$ This is what I have so far. I know we can demonstrate this using the exponential function where $$f(0)=a^0=1$$  Hence $$f(x+0)=f(x)f(0)=a^x*a^0=a^x=f(x)$$  If $f$ is continuous at $0$ then $$\lim_{h\to 0}f(x+h)=\lim_{h\to 0}f(x)f(h)=f(x)$$ Making $f$ continuous. Can someone please guide me?","Assume $ f: \mathbb{R} \to \mathbb{R}$ is such that $f(x+y)=f(x)f(y)$ ( The class of exponential functions has this property). Prove that $f$ having a limit at $0$ implies that $f$ has a limit at every real number and is one, or $f$ is identically $0$ for every $ x \in \mathbb{R}$ This is what I have so far. I know we can demonstrate this using the exponential function where $$f(0)=a^0=1$$  Hence $$f(x+0)=f(x)f(0)=a^x*a^0=a^x=f(x)$$  If $f$ is continuous at $0$ then $$\lim_{h\to 0}f(x+h)=\lim_{h\to 0}f(x)f(h)=f(x)$$ Making $f$ continuous. Can someone please guide me?",,"['real-analysis', 'limits']"
23,What is $\lim_{n\rightarrow \infty} \frac{1}{n^2}(\ln(\frac{2^n}{3^n})+\ln(\frac{5^n}{4^n})+\cdots+\ln(\frac{(3n-1)^n}{(n+2)^n}))$?,What is ?,\lim_{n\rightarrow \infty} \frac{1}{n^2}(\ln(\frac{2^n}{3^n})+\ln(\frac{5^n}{4^n})+\cdots+\ln(\frac{(3n-1)^n}{(n+2)^n})),"Per the title of this question, how does one go about calculating $$\lim_{n\rightarrow \infty} \frac{1}{n^2}\left(\ln\left(\frac{2^n}{3^n}\right)+\ln\left(\frac{5^n}{4^n}\right)+\cdots+\ln\left(\frac{(3n-1)^n}{(n+2)^n}\right)\right)\  ?$$ Thanks!","Per the title of this question, how does one go about calculating $$\lim_{n\rightarrow \infty} \frac{1}{n^2}\left(\ln\left(\frac{2^n}{3^n}\right)+\ln\left(\frac{5^n}{4^n}\right)+\cdots+\ln\left(\frac{(3n-1)^n}{(n+2)^n}\right)\right)\  ?$$ Thanks!",,['limits']
24,Indeterminate forms other than the 7 common ones,Indeterminate forms other than the 7 common ones,,"The following 7 indeterminate forms are all I can find in any calculus books: $$\frac{0}{0}, \frac{\infty}{\infty}, 0 \cdot \infty, \infty - \infty, 0^0, \infty^0, 1^\infty.$$ For example, by $\frac{0}{0}$ , I am referring to the limit $\lim\limits_{x\to a} \frac{f(x)}{g(x)}$ with $\lim\limits_{x\to a} f(x) = 0$ and $\lim\limits_{x\to a} g(x) = 0$ . Also, $1^\infty$ is understood to include the case $1^{-\infty}$ , just as $\frac{\infty}{\infty}$ is understood to include $-\infty$ in the numerator/denominator. Finally, the function in the base of $0^0$ is understood to be approaching $0$ from the right. In contrast, here are some forms which are not indeterminate forms: $$0^{\pm\infty}, \infty^{\pm\infty}, \frac{0}{\pm\infty}, \frac{\pm\infty}{0^+}, \frac{\pm\infty}{0^-}.$$ Then I came across this post: Are $\log_1 1$ and $\log_0 0$ indeterminate forms? I think the forms $\log_1 1$ and $\log_{0^+} {0^+}$ are also indeterminate forms, and they can respectively be transformed into the types $\frac{0}{0}$ and $\frac{\infty}{\infty}$ using $\log_{f(x)} [g(x)] = \frac{\ln g(x)}{\ln f(x)}$ . Can you list some other indeterminate forms?","The following 7 indeterminate forms are all I can find in any calculus books: For example, by , I am referring to the limit with and . Also, is understood to include the case , just as is understood to include in the numerator/denominator. Finally, the function in the base of is understood to be approaching from the right. In contrast, here are some forms which are not indeterminate forms: Then I came across this post: Are $\log_1 1$ and $\log_0 0$ indeterminate forms? I think the forms and are also indeterminate forms, and they can respectively be transformed into the types and using . Can you list some other indeterminate forms?","\frac{0}{0}, \frac{\infty}{\infty}, 0 \cdot \infty, \infty - \infty, 0^0, \infty^0, 1^\infty. \frac{0}{0} \lim\limits_{x\to a} \frac{f(x)}{g(x)} \lim\limits_{x\to a} f(x) = 0 \lim\limits_{x\to a} g(x) = 0 1^\infty 1^{-\infty} \frac{\infty}{\infty} -\infty 0^0 0 0^{\pm\infty}, \infty^{\pm\infty}, \frac{0}{\pm\infty}, \frac{\pm\infty}{0^+}, \frac{\pm\infty}{0^-}. \log_1 1 \log_{0^+} {0^+} \frac{0}{0} \frac{\infty}{\infty} \log_{f(x)} [g(x)] = \frac{\ln g(x)}{\ln f(x)}","['limits', 'indeterminate-forms']"
25,"Prove $\lim_{x\to 1} \frac{x-1}{2x^2+x-3} = \frac{1}{5}$ using the $(\epsilon, \delta)$-definition.",Prove  using the -definition.,"\lim_{x\to 1} \frac{x-1}{2x^2+x-3} = \frac{1}{5} (\epsilon, \delta)","I want to see if my proof is correct and if my choice of $\delta$ makes sense. Prove that $\lim_{x\to 1} \frac{x-1}{2x^2+x-3} = \frac{1}{5}$ . Let $\delta = \min(1, \frac{15}{2} \epsilon)$ and suppose $0 < |x-1| < \delta$ . First simplifying $|\frac{x-1}{2x^2+x-3} - \frac{1}{5}| = \frac{2}{5(2x+3)} |x-1|$ . Since $|x-1| < 1$ we have $\frac{1}{2x+3} < \frac{1}{3} $ . So $\frac{2}{5(2x+3)} |x-1| < \frac{2}{15} |x-1| < \frac{2}{15} \delta \le \frac{2}{15} \frac{15}{2} \epsilon = \epsilon$ . For further details $|x-1| < 1 \iff -1<x-1<1 \iff 0<x<2 \iff 0<2x<4 \iff 3<2x+3<7 \implies \frac{1}{2x+3} < \frac{1}{3}$",I want to see if my proof is correct and if my choice of makes sense. Prove that . Let and suppose . First simplifying . Since we have . So . For further details,"\delta \lim_{x\to 1} \frac{x-1}{2x^2+x-3} = \frac{1}{5} \delta = \min(1, \frac{15}{2} \epsilon) 0 < |x-1| < \delta |\frac{x-1}{2x^2+x-3} - \frac{1}{5}| = \frac{2}{5(2x+3)} |x-1| |x-1| < 1 \frac{1}{2x+3} < \frac{1}{3}  \frac{2}{5(2x+3)} |x-1| < \frac{2}{15} |x-1| < \frac{2}{15} \delta \le \frac{2}{15} \frac{15}{2} \epsilon = \epsilon |x-1| < 1 \iff -1<x-1<1 \iff 0<x<2 \iff 0<2x<4 \iff 3<2x+3<7 \implies \frac{1}{2x+3} < \frac{1}{3}","['real-analysis', 'calculus', 'limits', 'epsilon-delta']"
26,Does function $f(x) = kf(x-a) + (1-k)f(x-b)$ for $a / b \notin \mathbb{Q}$ always have a limit?,Does function  for  always have a limit?,f(x) = kf(x-a) + (1-k)f(x-b) a / b \notin \mathbb{Q},"(This is my attempt to simplify or generalize my previous unanswered question Proving technique for exchanging lim and (infinite) sum where the sum converges very slowly. ) Consider a function defined on positive real numbers $f: \mathbb{R_+}\to\mathbb{R}$ with the following properties for all $x>a$ , $f(x) = kf(x-a) + (1-k) f(x-b)$ , for some constants $k$ , $a$ , and $b$ where $0<k<1$ and $0<b<a$ $a / b \notin \mathbb{Q}$ (otherwise a counter-example would be $f(x) = \sin(2\pi x/\gcd(a,b))$ )) $f(x)$ is bounded $f(x)$ is continuous  (otherwise a counter-example would be the indicator function $f(x) = \mathbb{1}_{a\mathbb{Z}+b\mathbb{Z}}$ )(can we weaken this to ""discontinuity is nowhere dense""?) Can we conclude that the limit $$\lim_{x\to\infty} f(x)$$ exists? If not, what other condition we need to add to ensure the limit exists? Intuitively, this is true to me. The first functional equation means that each function value is a weighted average of some previous values, so overall it should converge to some sort of average of the ""seeding region"" $x\in(0,a]$ . But I can't really convert this intuition to a concrete proof. Another attempt I had was to try solving the functional equation. I can find the following solution basis $$f(x) = e^{tx},$$ where $t$ is a (potentially complex) solution to the equation $$ke^{-at} + (1-k) e^{-bt} = 1.$$ I don't know if this is the entire solution space, nor how to solve the coefficient for each term. Nevertheless, this equation always has a solution $t = 0$ , whose corresponding term in $f(x)$ would be a constant, which is the limit if it exists. All other solutions $t$ have negative real components (but can be arbitrarily close to 0), meaning their corresponding term in $f(x)$ shrinks and has individual limit of 0. However, I don't think I can directly say the limit of the function is the sum of the limit of each term in this case. Another path I tried to explore is to consider the function on a grid $F: \mathbb{Z}^2\to\mathbb{R}$ , $F(x, y) = f(ax+by)$ . It would be a great first step to prove that $\lim_{\mathbf{v}\to(+\infty,+\infty)}F(\mathbf{v})$ exists, and then we can ""fill in"" other values for $f(x)$ using continuity. On the grid, it can be seen that what contributes to $F(x, y)$ is a structure similar to binomial expansion: \begin{aligned} F(x, y) &= kF(x-1, y) + (1-k) F(x,y-1)\\ &= k^2 F(x-2, y) + 2k(1-k) F(x-1,y-1) + (1-k)^2 F(x,y-2)\\ &= k^3 F(x-3, y) + 3k^2(1-k) F(x-2,y-1) + 3k(1-k)^2 F(x-1, y-2) + (1-k)^3 F(x,y-3)\\ ... \end{aligned} As good as it looks, it doesn't help me with the question.","(This is my attempt to simplify or generalize my previous unanswered question Proving technique for exchanging lim and (infinite) sum where the sum converges very slowly. ) Consider a function defined on positive real numbers with the following properties for all , , for some constants , , and where and (otherwise a counter-example would be )) is bounded is continuous  (otherwise a counter-example would be the indicator function )(can we weaken this to ""discontinuity is nowhere dense""?) Can we conclude that the limit exists? If not, what other condition we need to add to ensure the limit exists? Intuitively, this is true to me. The first functional equation means that each function value is a weighted average of some previous values, so overall it should converge to some sort of average of the ""seeding region"" . But I can't really convert this intuition to a concrete proof. Another attempt I had was to try solving the functional equation. I can find the following solution basis where is a (potentially complex) solution to the equation I don't know if this is the entire solution space, nor how to solve the coefficient for each term. Nevertheless, this equation always has a solution , whose corresponding term in would be a constant, which is the limit if it exists. All other solutions have negative real components (but can be arbitrarily close to 0), meaning their corresponding term in shrinks and has individual limit of 0. However, I don't think I can directly say the limit of the function is the sum of the limit of each term in this case. Another path I tried to explore is to consider the function on a grid , . It would be a great first step to prove that exists, and then we can ""fill in"" other values for using continuity. On the grid, it can be seen that what contributes to is a structure similar to binomial expansion: As good as it looks, it doesn't help me with the question.","f: \mathbb{R_+}\to\mathbb{R} x>a f(x) = kf(x-a) + (1-k) f(x-b) k a b 0<k<1 0<b<a a / b \notin \mathbb{Q} f(x) = \sin(2\pi x/\gcd(a,b)) f(x) f(x) f(x) = \mathbb{1}_{a\mathbb{Z}+b\mathbb{Z}} \lim_{x\to\infty} f(x) x\in(0,a] f(x) = e^{tx}, t ke^{-at} + (1-k) e^{-bt} = 1. t = 0 f(x) t f(x) F: \mathbb{Z}^2\to\mathbb{R} F(x, y) = f(ax+by) \lim_{\mathbf{v}\to(+\infty,+\infty)}F(\mathbf{v}) f(x) F(x, y) \begin{aligned}
F(x, y) &= kF(x-1, y) + (1-k) F(x,y-1)\\
&= k^2 F(x-2, y) + 2k(1-k) F(x-1,y-1) + (1-k)^2 F(x,y-2)\\
&= k^3 F(x-3, y) + 3k^2(1-k) F(x-2,y-1) + 3k(1-k)^2 F(x-1, y-2) + (1-k)^3 F(x,y-3)\\
...
\end{aligned}","['calculus', 'limits', 'functions', 'functional-equations']"
27,How to calculate $\lim\limits_{x \to 0} \cos\left(2x\right)^{\frac{1}{x^{4}}}$?,How to calculate ?,\lim\limits_{x \to 0} \cos\left(2x\right)^{\frac{1}{x^{4}}},"I was hoping for some guidance on how to approach the following limit, $\lim\limits_{x \to 0} \cos\left(2x\right)^{\frac{1}{x^{4}}}$ Now for context, I already know that the answer is $0$ , but I'm trying to  be more formal in how I prove it. The way I have been approaching this is to first convert it into a combination of exponentials and logarithms, which obtains $\lim\limits_{x \to 0} \exp\left(\frac{1}{x^{4}}\ln\left(\cos2x\right)\right)$ However, the problem with this is that we eventually see that the term inside the logarithm basically goes to 0, and thus the term inside the exponential goes to $-\infty$ , because of that, I don't believe this step is completely legitimate, especially as I move the limit in and out of the function later on and apply L'Hopital's rule. So how can one rigorously handle this process? If I play it fast and loose, I can just go through with that step, bring the limit inside of the exponential, apply L'Hopital, take out the $\frac{\sin2x}{2x}$ that pops up (since it equals $1$ ), plug $0$ into the cosine term, and be left with $e^{\lim\limits_{x \to 0}{\left(-\frac{1}{x^{2}}\right)}}$ . If I naively take the limit back out, I can use a very straightforward $\epsilon-\delta$ proof to show that the whole limit equals $0$ . But again, this is the result of having essentially taken the logarithm of $0$ in that earlier step, so I don't think it's legitimate. So, in essence, how can I either navigate around this divergent limit that pops up, or how can I more rigorously approach it if it does. It's been a while since I took real analysis, so I'm kind of shaky on this. Many thanks!","I was hoping for some guidance on how to approach the following limit, Now for context, I already know that the answer is , but I'm trying to  be more formal in how I prove it. The way I have been approaching this is to first convert it into a combination of exponentials and logarithms, which obtains However, the problem with this is that we eventually see that the term inside the logarithm basically goes to 0, and thus the term inside the exponential goes to , because of that, I don't believe this step is completely legitimate, especially as I move the limit in and out of the function later on and apply L'Hopital's rule. So how can one rigorously handle this process? If I play it fast and loose, I can just go through with that step, bring the limit inside of the exponential, apply L'Hopital, take out the that pops up (since it equals ), plug into the cosine term, and be left with . If I naively take the limit back out, I can use a very straightforward proof to show that the whole limit equals . But again, this is the result of having essentially taken the logarithm of in that earlier step, so I don't think it's legitimate. So, in essence, how can I either navigate around this divergent limit that pops up, or how can I more rigorously approach it if it does. It's been a while since I took real analysis, so I'm kind of shaky on this. Many thanks!",\lim\limits_{x \to 0} \cos\left(2x\right)^{\frac{1}{x^{4}}} 0 \lim\limits_{x \to 0} \exp\left(\frac{1}{x^{4}}\ln\left(\cos2x\right)\right) -\infty \frac{\sin2x}{2x} 1 0 e^{\lim\limits_{x \to 0}{\left(-\frac{1}{x^{2}}\right)}} \epsilon-\delta 0 0,"['real-analysis', 'limits']"
28,"Infinite product of areas in a square, inscribed quarter-circle and line segments.","Infinite product of areas in a square, inscribed quarter-circle and line segments.",,"The diagram shows a square of area $An$ and an enclosed quarter-circle. Line segments are drawn from the bottom-left vertex to points that are equally spaced along the quarter-circle. The regions enclosed by the line segments and the quarter-circle have areas $a_1, a_2, a_3, ..., a_n$ . Find the value of $A$ such that $\lim\limits_{n\to\infty}\prod\limits_{k=1}^n a_k = 2$ . I have parced the problem to this: $$\lim\limits_{n\to\infty}\prod\limits_{k=1}^n Anf(k)(g(k)-g(k-1))=2$$ where $f(k)=\frac{3}{2}-\sin{\frac{k\pi}{2n}-\cos{\frac{k\pi}{2n}}}$ $g(k)=\arcsin{\left(\dfrac{\sin{\frac{k\pi}{2n}}-\cos{\frac{k\pi}{2n}}}{2\sqrt{f(k)}}\right)}$ Desmos suggests $A\approx 5.77987$ . I am looking for a closed form for $A$ . I have tried to take the log of the product and relate the resulting sum to an integral, but I do not know how to deal with the $g(k-1)$ . (This question was inspired by a related question , where the product of the lengths of the line segments approaches $2$ .) EDIT After some more exploration, it seems that $A$ also makes $\frac{1}{n}\prod\limits_{k=1}^{n}A\frac{\pi}{4}\left(\cos{\frac{k\pi}{2(n+1)}}+\sin{\frac{k\pi}{2(n+1)}}-1\right)$ converge to a positive number (approximately $0.8817$ ). EDIT2 Based on this question , it seems that $A=\frac{4}{\pi}e^{4G/\pi}\sqrt2$ where $G$ is Catalan's constant .","The diagram shows a square of area and an enclosed quarter-circle. Line segments are drawn from the bottom-left vertex to points that are equally spaced along the quarter-circle. The regions enclosed by the line segments and the quarter-circle have areas . Find the value of such that . I have parced the problem to this: where Desmos suggests . I am looking for a closed form for . I have tried to take the log of the product and relate the resulting sum to an integral, but I do not know how to deal with the . (This question was inspired by a related question , where the product of the lengths of the line segments approaches .) EDIT After some more exploration, it seems that also makes converge to a positive number (approximately ). EDIT2 Based on this question , it seems that where is Catalan's constant .","An a_1, a_2, a_3, ..., a_n A \lim\limits_{n\to\infty}\prod\limits_{k=1}^n a_k = 2 \lim\limits_{n\to\infty}\prod\limits_{k=1}^n Anf(k)(g(k)-g(k-1))=2 f(k)=\frac{3}{2}-\sin{\frac{k\pi}{2n}-\cos{\frac{k\pi}{2n}}} g(k)=\arcsin{\left(\dfrac{\sin{\frac{k\pi}{2n}}-\cos{\frac{k\pi}{2n}}}{2\sqrt{f(k)}}\right)} A\approx 5.77987 A g(k-1) 2 A \frac{1}{n}\prod\limits_{k=1}^{n}A\frac{\pi}{4}\left(\cos{\frac{k\pi}{2(n+1)}}+\sin{\frac{k\pi}{2(n+1)}}-1\right) 0.8817 A=\frac{4}{\pi}e^{4G/\pi}\sqrt2 G","['limits', 'circles', 'area', 'closed-form', 'infinite-product']"
29,"Is it sufficient to prove that $| f(x,y) – L | = 0 $ so that $\lim_{(x,y)\to(0,0)} f(x,y) =L$ is true?",Is it sufficient to prove that  so that  is true?,"| f(x,y) – L | = 0  \lim_{(x,y)\to(0,0)} f(x,y) =L","Im trying to proove the following limit by the squeeze theorem: $$  \lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}} = 0 $$ I've seen all the related questions, and I can't understand the validity of applying absolute value as it follows: Of course we have that: $|\frac{xy}{\sqrt{x^2+y^2}}|≥0$ , and also $ |\frac{xy}{\sqrt{x^2+y^2}}|=  \frac{|x|\cdot |y|}{\sqrt{x^2+y^2}} ≤                    \frac{|x|\cdot |y|}{\sqrt{y^2}} = \frac{|x|\cdot |y|}{|y|}=|x|$ , Therfore, by the squeeze theorem: $$ 0≤|\frac{xy}{\sqrt{x^2+y^2}}|≤|x|  $$ $$ \lim_{(x,y)\to(0,0)}  0≤\lim_{(x,y)\to(0,0)}  |\frac{xy}{\sqrt{x^2+y^2}}|≤\lim_{(x,y)\to(0,0)} |x|  $$ $$ 0≤\lim_{(x,y)\to(0,0)}  |\frac{xy}{\sqrt{x^2+y^2}}|≤0  $$ So finally that should be sufficient to prove that the distance $| f(x,y) – L | = 0 $ and therefore, that should be enough to say that: $$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}} = 0$$ Then my question is: is this valid? Or am I committing a mathematical atrocity.  Thank you for your time and patience.","Im trying to proove the following limit by the squeeze theorem: I've seen all the related questions, and I can't understand the validity of applying absolute value as it follows: Of course we have that: , and also , Therfore, by the squeeze theorem: So finally that should be sufficient to prove that the distance and therefore, that should be enough to say that: Then my question is: is this valid? Or am I committing a mathematical atrocity.  Thank you for your time and patience.","  \lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}} = 0  |\frac{xy}{\sqrt{x^2+y^2}}|≥0  |\frac{xy}{\sqrt{x^2+y^2}}|=  \frac{|x|\cdot |y|}{\sqrt{x^2+y^2}} ≤                    \frac{|x|\cdot |y|}{\sqrt{y^2}} = \frac{|x|\cdot |y|}{|y|}=|x|  0≤|\frac{xy}{\sqrt{x^2+y^2}}|≤|x|    \lim_{(x,y)\to(0,0)}  0≤\lim_{(x,y)\to(0,0)}  |\frac{xy}{\sqrt{x^2+y^2}}|≤\lim_{(x,y)\to(0,0)} |x|    0≤\lim_{(x,y)\to(0,0)}  |\frac{xy}{\sqrt{x^2+y^2}}|≤0   | f(x,y) – L | = 0  \lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}} = 0",['limits']
30,What is the average of ratio of the length of prime periods of $1/p$?,What is the average of ratio of the length of prime periods of ?,1/p,"For all prime $p \ge 7$ , the decimal representation of $1/p$ repeats. Let $l_p$ be the length of the period or the non-repeating part of the decimal representation of $1/p$ . It can be shown that $l_p \le p-1$ . Thus $\frac{l_p}{p-1}$ is a measure of the length of $l_p$ relative to the magnitude of $p$ . Definition : We define the relative magnitude of the length of the period of a prime $p$ as $\frac{l_p}{p-1}$ . Question 1 : What is the limiting value of $$ c_n = \frac{1}{n}\sum_{k = 1}^n\frac{l_{p_n}}{p_n-1}  $$ Question 2 : Is it true that $$ \lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^n\frac{l_{p_n}}{p_n}   = \lim_{n \to \infty}\frac{\sum_{k = 1}^n l_{p_n}}{\sum_{k = 1}^n p_n} = \prod_{p} \Big (1 - \frac{p}{p^3-1}\Big ) \approx 0.57596 $$ The plot of $c_n$ vs. $n$ for $n \le 2460000$ and for this value of, $c_n \approx 0.5763$ . Source code p = 7 s1 = s2 = s3 = k = 0 step = target = 10^4  while True:     k = k + 1     d = divisors(p-1)     l = len(d)     i = 1     while i < l:         e = d[i]         if (10^e)%p == 1:             s1 = s1 + 1/e.n()             s2 = s2 + (e/(p-1)).n()             s3 = s3 + e             break         i = i + 1          if k >= target:         print(k, s1, s2/k, s3)         target = target + step     p = next_prime(p)","For all prime , the decimal representation of repeats. Let be the length of the period or the non-repeating part of the decimal representation of . It can be shown that . Thus is a measure of the length of relative to the magnitude of . Definition : We define the relative magnitude of the length of the period of a prime as . Question 1 : What is the limiting value of Question 2 : Is it true that The plot of vs. for and for this value of, . Source code p = 7 s1 = s2 = s3 = k = 0 step = target = 10^4  while True:     k = k + 1     d = divisors(p-1)     l = len(d)     i = 1     while i < l:         e = d[i]         if (10^e)%p == 1:             s1 = s1 + 1/e.n()             s2 = s2 + (e/(p-1)).n()             s3 = s3 + e             break         i = i + 1          if k >= target:         print(k, s1, s2/k, s3)         target = target + step     p = next_prime(p)","p \ge 7 1/p l_p 1/p l_p \le p-1 \frac{l_p}{p-1} l_p p p \frac{l_p}{p-1} 
c_n = \frac{1}{n}\sum_{k = 1}^n\frac{l_{p_n}}{p_n-1} 
 
\lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^n\frac{l_{p_n}}{p_n}  
= \lim_{n \to \infty}\frac{\sum_{k = 1}^n l_{p_n}}{\sum_{k = 1}^n p_n}
= \prod_{p} \Big (1 - \frac{p}{p^3-1}\Big ) \approx 0.57596
 c_n n n \le 2460000 c_n \approx 0.5763","['limits', 'number-theory', 'elementary-number-theory', 'convergence-divergence', 'prime-numbers']"
31,Proving that limits at infinity are unique help,Proving that limits at infinity are unique help,,"I am trying to prove that the $\displaystyle \lim_{x\rightarrow \infty}f(x)=L$ is unique, if it exists. Definition: Let $f:S\rightarrow\mathbb R$ be a function, where $\infty$ is a cluster point of $S$ . We say that $f(x)$ converges to $L$ as $x$ goes to $\infty$ if  there exists $L\in\mathbb R$ such that for every $\varepsilon>0$ , there is an $M\in \mathbb R$ such that $$|f(x)-L|<\varepsilon$$ whenever $x\in S$ and $x\geq M$ . Here is what I got so far: Proof: Let $L_1$ and $L_2$ be two numbers that both satisfy the above definition. Take $\varepsilon>0$ and find $M_1\in \mathbb R$ such that if $x\in S$ and $x\geq M_1$ then $$|f(x)-L_1|<\varepsilon/2.$$ Also find $M_2\in \mathbb R$ such that if $x\in S$ and $x\geq M_2$ then $$|f(x)-L_2|<\varepsilon/2.$$ Now, put $M=\max\{M_1,M_2\}$ . Suppose $x\in S$ and $x\geq M$ (such $x$ exists since $\infty$ is a cluster point of $S$ ). Then $$|L_1-L_2|=|L_1-f(x)+f(x)-L_2|\leq |L_1-f(x)|+ |f(x)-L_2|<\varepsilon/2 + \varepsilon/2 = \varepsilon.$$ As $|L_1-L_2|<\varepsilon$ for arbitrary $\varepsilon>0$ . Then $L_1=L_2$ . $\blacksquare$ Do you think this argument is correct? Any help will be appreciated! Thanks in advanced.","I am trying to prove that the is unique, if it exists. Definition: Let be a function, where is a cluster point of . We say that converges to as goes to if  there exists such that for every , there is an such that whenever and . Here is what I got so far: Proof: Let and be two numbers that both satisfy the above definition. Take and find such that if and then Also find such that if and then Now, put . Suppose and (such exists since is a cluster point of ). Then As for arbitrary . Then . Do you think this argument is correct? Any help will be appreciated! Thanks in advanced.","\displaystyle \lim_{x\rightarrow \infty}f(x)=L f:S\rightarrow\mathbb R \infty S f(x) L x \infty L\in\mathbb R \varepsilon>0 M\in \mathbb R |f(x)-L|<\varepsilon x\in S x\geq M L_1 L_2 \varepsilon>0 M_1\in \mathbb R x\in S x\geq M_1 |f(x)-L_1|<\varepsilon/2. M_2\in \mathbb R x\in S x\geq M_2 |f(x)-L_2|<\varepsilon/2. M=\max\{M_1,M_2\} x\in S x\geq M x \infty S |L_1-L_2|=|L_1-f(x)+f(x)-L_2|\leq |L_1-f(x)|+ |f(x)-L_2|<\varepsilon/2 + \varepsilon/2 = \varepsilon. |L_1-L_2|<\varepsilon \varepsilon>0 L_1=L_2 \blacksquare","['real-analysis', 'limits', 'solution-verification', 'infinity']"
32,Why is $1- \cos ( x+ \sin (x+ \sin(x +\sin(x + \cdots))))$ the cycloid?,Why is  the cycloid?,1- \cos ( x+ \sin (x+ \sin(x +\sin(x + \cdots)))),"As many of you probably know, the cycloid is given by the parametric equation: \begin{equation} x= t-\sin t\tag{1} \label{eq:x} \end{equation} \begin{equation} y= 1- \cos t\tag{2}\label{eq:y}. \end{equation} I would like to have an equation for the cycloid that does not depend on the parameter $t$ . If we could just solve \eqref{eq:x} for $t$ we could just plug that expression into \ref{eq:y} and I would be happy. But, as far as I know we can't solve \ref{eq:x} for $t$ . However, it is easy enough to find that $t= x+ \sin t $ . We know that $\sin t$ must be somewhere between $-1,1$ , so for an approximation, we might decide to simply drop this term and substitute $t = x$ into equation \ref{eq:y} to get $1- \cos(x)$ . Looking at a graph this looks like a reasonable but very rough approximation of the actual cycloid. I noticed that there is a simple way to improve this approximation. Recall that $t= x+ \sin t $ . What happens if we just plug in the expression $x+ \sin t$ for $t$ into the right hand side to get $t= x+ \sin( x+ \sin t)$ . Again, since we are approximating, we can just let $\sin t = 0$ . Substitute $t= x+ \sin x$ into \ref{eq:y} to obtain the much better approximation to the cycloid $$ y= 1-\cos( x+ \sin(x)).$$ This has lead me to conjecture the following: Let $f_1(x) = x $ and $f_n(x) = x + \sin \big( f_{n-1} (x) \big)$ . Define $g_n(x) = 1- \cos f_n(x)$ . Then, $g(x)= \lim_{n \to \infty} g_n(x)$ is the cycloid. I do not have a proof of this and could not find this description of the cycloid anywhere (maybe because it is a pain to work with). So, I have two questions: How can I prove my conjecture? Is the recursive method to ""solve"" equation \ref{eq:x} for $x$ used elsewhere?","As many of you probably know, the cycloid is given by the parametric equation: I would like to have an equation for the cycloid that does not depend on the parameter . If we could just solve \eqref{eq:x} for we could just plug that expression into \ref{eq:y} and I would be happy. But, as far as I know we can't solve \ref{eq:x} for . However, it is easy enough to find that . We know that must be somewhere between , so for an approximation, we might decide to simply drop this term and substitute into equation \ref{eq:y} to get . Looking at a graph this looks like a reasonable but very rough approximation of the actual cycloid. I noticed that there is a simple way to improve this approximation. Recall that . What happens if we just plug in the expression for into the right hand side to get . Again, since we are approximating, we can just let . Substitute into \ref{eq:y} to obtain the much better approximation to the cycloid This has lead me to conjecture the following: Let and . Define . Then, is the cycloid. I do not have a proof of this and could not find this description of the cycloid anywhere (maybe because it is a pain to work with). So, I have two questions: How can I prove my conjecture? Is the recursive method to ""solve"" equation \ref{eq:x} for used elsewhere?","\begin{equation}
x= t-\sin t\tag{1} \label{eq:x}
\end{equation} \begin{equation}
y= 1- \cos t\tag{2}\label{eq:y}.
\end{equation} t t t t= x+ \sin t  \sin t -1,1 t = x 1- \cos(x) t= x+ \sin t  x+ \sin t t t= x+ \sin( x+ \sin t) \sin t = 0 t= x+ \sin x  y= 1-\cos( x+ \sin(x)). f_1(x) = x  f_n(x) = x + \sin \big( f_{n-1} (x) \big) g_n(x) = 1- \cos f_n(x) g(x)= \lim_{n \to \infty} g_n(x) x","['limits', 'trigonometry', 'uniform-convergence', 'curves', 'cycloid']"
33,Is there a meaningful definition of $\lim_{f(x)\to a}g(x)$?,Is there a meaningful definition of ?,\lim_{f(x)\to a}g(x),"Fairly often, I have seen the notation $\lim_{f(x)\to a}g(x)$ being used in the context of evaluating limits, e.g. $$ \lim_{x \to 0}\frac{\sin(2x)}{2x}=\lim_{2x\to0}\frac{\sin 2x}{2x}\overset{y=2x}=\lim_{y \to 0}\frac{\sin y}{y}=1 \, . $$ While the meaning of such equations is clear enough, I consider this to be a mild abuse of notation for the following reason: when we write $\lim_{x \to a}f(x)$ , the variable $x$ is bound by the limit operator; however, only single-letter variables such as $x$ and $y$ can be bound, not arbitrary algebraic expressions such as $2x$ . So is there a formal definition of $\lim_{f(x)\to a}g(x)=l$ that avoids this problem? Perhaps we say that $\lim_{f(x)\to a}g(x)=l$ if $a$ is an accumulation point of $\operatorname{dom}(g)\cap\operatorname{ran}(f)$ , and $$ \forall\varepsilon>0:\exists\delta>0:\forall x\in\operatorname{dom}(g)\cap\operatorname{ran}(f):0<|f(x)-a|<\delta\implies |g(x)-l|<\varepsilon \, . $$","Fairly often, I have seen the notation being used in the context of evaluating limits, e.g. While the meaning of such equations is clear enough, I consider this to be a mild abuse of notation for the following reason: when we write , the variable is bound by the limit operator; however, only single-letter variables such as and can be bound, not arbitrary algebraic expressions such as . So is there a formal definition of that avoids this problem? Perhaps we say that if is an accumulation point of , and","\lim_{f(x)\to a}g(x) 
\lim_{x \to 0}\frac{\sin(2x)}{2x}=\lim_{2x\to0}\frac{\sin 2x}{2x}\overset{y=2x}=\lim_{y \to 0}\frac{\sin y}{y}=1 \, .
 \lim_{x \to a}f(x) x x y 2x \lim_{f(x)\to a}g(x)=l \lim_{f(x)\to a}g(x)=l a \operatorname{dom}(g)\cap\operatorname{ran}(f) 
\forall\varepsilon>0:\exists\delta>0:\forall x\in\operatorname{dom}(g)\cap\operatorname{ran}(f):0<|f(x)-a|<\delta\implies |g(x)-l|<\varepsilon \, .
","['real-analysis', 'calculus', 'limits', 'notation', 'definition']"
34,largest $n$ such that the $n$th derivative of $f(x) = x^\alpha \sin \left(\frac{1}{x} \right)$ at $x=0$ exists.,largest  such that the th derivative of  at  exists.,n n f(x) = x^\alpha \sin \left(\frac{1}{x} \right) x=0,"Could you verify the following solution? Let, $$ f(x) = \begin{cases} x^\alpha \sin \left(\frac{1}{x} \right), &x\neq 0\\ 0, & x=0, \end{cases} $$ where $\alpha \in \Bbb R$ is a constant. Find largest $n$ such that the $n$ th derivative of $f$ at $x=0$ exists. Since, each time we differentiate $f,$ we will get terms of the form $x^i \cdot \sin(1/x)$ or $x^i \cdot \cos(1/x)$ differentiating 1st time, the lowest value of $i$ obtained is $\alpha-2$ differentiating 2nd time, the lowest value of $i$ obtained is $\alpha-4$ differentiating $n$ times,  the lowest value of $i$ obtained is $\alpha-2n$ for $f^n(0)$ to exist we need, we need $f^{n-1}$ to be differentiable.  hence we need: $$\alpha-2(n-1)>1 \implies \alpha-2n+2>1 \implies \frac{\alpha+1}{2}>n \implies n<\frac{\alpha+1}{2}$$ hence the largest $n$ such that $f^n(0)$ exists is $n=\left\lceil\frac{\alpha-1}{2}\right\rceil$ so, if $\alpha=100$ , then $n=49$ if $\alpha=1$ , then $n=0$ if $\alpha=2$ , then $n=1$ if $\alpha=3$ , then $n=1$ if $\alpha=4$ , then $n=2$ if $\alpha=5$ , then $n=2$ if $\alpha=6$ , then $n=3$ and so on... is this correct?? edit: what i am doing is this. first calculate, $f'(x)$ as follows $$ f'(x) = \begin{cases} \alpha \cdot x^{\alpha-1}\sin \left(\frac{1}{x}\right) - x^{\alpha-2}\cos(\frac{1}{x}), &x\neq 0\\ 0, & x=0, \end{cases} $$ where, $f'(0)$ is calculated using 1st principles. this way, we keep differentiating, and keep finding $f^i(x)$ and we keep evaluating $\lim_{x \to 0} \frac{f^i(x)-f^i(0)}{x-0}$ going this way, say for some $i$ , $\lim_{x \to 0} \frac{f^i(x)-f^i(0)}{x-0}$ does not exist, then our choice of $n$ is that $i$ , how is this wrong??","Could you verify the following solution? Let, where is a constant. Find largest such that the th derivative of at exists. Since, each time we differentiate we will get terms of the form or differentiating 1st time, the lowest value of obtained is differentiating 2nd time, the lowest value of obtained is differentiating times,  the lowest value of obtained is for to exist we need, we need to be differentiable.  hence we need: hence the largest such that exists is so, if , then if , then if , then if , then if , then if , then if , then and so on... is this correct?? edit: what i am doing is this. first calculate, as follows where, is calculated using 1st principles. this way, we keep differentiating, and keep finding and we keep evaluating going this way, say for some , does not exist, then our choice of is that , how is this wrong??","
f(x) =
\begin{cases}
x^\alpha \sin \left(\frac{1}{x} \right), &x\neq 0\\
0, & x=0,
\end{cases}
 \alpha \in \Bbb R n n f x=0 f, x^i \cdot \sin(1/x) x^i \cdot \cos(1/x) i \alpha-2 i \alpha-4 n i \alpha-2n f^n(0) f^{n-1} \alpha-2(n-1)>1 \implies \alpha-2n+2>1 \implies \frac{\alpha+1}{2}>n \implies n<\frac{\alpha+1}{2} n f^n(0) n=\left\lceil\frac{\alpha-1}{2}\right\rceil \alpha=100 n=49 \alpha=1 n=0 \alpha=2 n=1 \alpha=3 n=1 \alpha=4 n=2 \alpha=5 n=2 \alpha=6 n=3 f'(x) 
f'(x) =
\begin{cases}
\alpha \cdot x^{\alpha-1}\sin \left(\frac{1}{x}\right) - x^{\alpha-2}\cos(\frac{1}{x}), &x\neq 0\\
0, & x=0,
\end{cases}
 f'(0) f^i(x) \lim_{x \to 0} \frac{f^i(x)-f^i(0)}{x-0} i \lim_{x \to 0} \frac{f^i(x)-f^i(0)}{x-0} n i","['limits', 'derivatives', 'solution-verification']"
35,Show that the sum $\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f(\frac{k}{n})$ tends to $0$ when $n\to \infty$,Show that the sum  tends to  when,\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f(\frac{k}{n}) 0 n\to \infty,"For $f\in C[0,1]$ show that for all $x\in[0,1]$ $\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f(\frac{k}{n})\to 0$ when $n\to \infty$ I don´t have a idea of how start with the proof, for one hand since our sum is the Bernstein polynomial with a extra $(-1)^k$ I think that must be $0$ since the Bernstein polynomial is a approach of our $f$ and therefore when we take all the terms of the approximation the error must be $0$ . For the other hand I need how fix the problem of the extra term $(-1)^k$ I think that we Can use the absolute value of our sum and find that these limit are $0$ for conclude that our original sum tends to $0$ when $n\to \infty$ . And basically I don´t know how I should continue with a formal proof of this. I belive that I should calculate $$\lim_{n\to \infty}\left( \sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f\left(\frac{k}{n}\right)\right)$$ or find $N\in \mathbb{N}$ such that $$\left|\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f\left(\frac{k}{n}\right)\right|<\varepsilon$$ Attempt Consider only the index $k\equiv 0\pmod 2$ then we should have $$\left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|C_{n}^{2k}x^{2k}(1-x)^{n-2k}$$ since the right sumand is at most $1$ then $$\left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|$$ but when $n\to \infty$ $\frac{2k}{n}\to 0$ and hence since $f(0)=p_n(1)=0$ $$\left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|=0$$ Finally the case when $k\equiv 1 \pmod{2}$ is analogous.","For show that for all when I don´t have a idea of how start with the proof, for one hand since our sum is the Bernstein polynomial with a extra I think that must be since the Bernstein polynomial is a approach of our and therefore when we take all the terms of the approximation the error must be . For the other hand I need how fix the problem of the extra term I think that we Can use the absolute value of our sum and find that these limit are for conclude that our original sum tends to when . And basically I don´t know how I should continue with a formal proof of this. I belive that I should calculate or find such that Attempt Consider only the index then we should have since the right sumand is at most then but when and hence since Finally the case when is analogous.","f\in C[0,1] x\in[0,1] \sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f(\frac{k}{n})\to 0 n\to \infty (-1)^k 0 f 0 (-1)^k 0 0 n\to \infty \lim_{n\to \infty}\left( \sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f\left(\frac{k}{n}\right)\right) N\in \mathbb{N} \left|\sum_{k=0}^nC_n^k x^k (1-x)^{n-x}(-1)^k f\left(\frac{k}{n}\right)\right|<\varepsilon k\equiv 0\pmod 2 \left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|C_{n}^{2k}x^{2k}(1-x)^{n-2k} 1 \left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right| n\to \infty \frac{2k}{n}\to 0 f(0)=p_n(1)=0 \left| \sum_{k=0}^nC_n^{2k} x^{2k} (1-x)^{n-x} f\left(\frac{2k}{n}\right)\right|\leq \sum_{k=0}^{n}\left|f\left( \frac{2k}{n}\right) \right|=0 k\equiv 1 \pmod{2}",['real-analysis']
36,Is this computation of $\lim\limits_{x\to 0^+} x^x $ rigorous enough?,Is this computation of  rigorous enough?,\lim\limits_{x\to 0^+} x^x ,"$\lim\limits_{x\to 0^+} x^x $ For every $x\in (0,1)$ there exists $n_x\in \mathbb N$ such that $\dfrac 1{n_x+1}\lt x\lt \dfrac 1{n_x}$ It follows that $n_x\to \infty$ as $x\to 0^+$ (Refer Note ) $\displaystyle x^{\frac 1{n_x}}\lt x^x\lt x^{\frac 1{n_x+1}}\implies \left({\frac 1{n_x+1}}\right)^{\frac 1{n_x}}\lt x^x\lt \left(\frac{1}{n_x}\right)^{\frac 1{n_x+1}}$ and using $\lim\limits_{n\to \infty}n^{\frac 1n}\to 1$ it follows by squeeze theorem that $x^x\to 1$ as $x\to 0^+$ . Note: $(x\to 0^+)\implies \forall\delta\gt 0\, \exists x \gt 0\, x\lt \delta$ and by Archemedean property it follows that $\exists (n_x+1)\in \mathbb N\,\left(\dfrac 1{n_x+1}\lt \delta\right)\implies n_x+1\gt \dfrac 1\delta\\\implies n_x+1\to \infty\;\text{ as }\;\delta\gt 0\;\text{ is arbitrary.}$ Is the above enough if I don't want to use L'Hospital's rule or logarithm? Thanks.",For every there exists such that It follows that as (Refer Note ) and using it follows by squeeze theorem that as . Note: and by Archemedean property it follows that Is the above enough if I don't want to use L'Hospital's rule or logarithm? Thanks.,"\lim\limits_{x\to 0^+} x^x  x\in (0,1) n_x\in \mathbb N \dfrac 1{n_x+1}\lt x\lt \dfrac 1{n_x} n_x\to \infty x\to 0^+ \displaystyle x^{\frac 1{n_x}}\lt x^x\lt x^{\frac 1{n_x+1}}\implies \left({\frac 1{n_x+1}}\right)^{\frac 1{n_x}}\lt x^x\lt \left(\frac{1}{n_x}\right)^{\frac 1{n_x+1}} \lim\limits_{n\to \infty}n^{\frac 1n}\to 1 x^x\to 1 x\to 0^+ (x\to 0^+)\implies \forall\delta\gt 0\, \exists x \gt 0\, x\lt \delta \exists (n_x+1)\in \mathbb N\,\left(\dfrac 1{n_x+1}\lt \delta\right)\implies n_x+1\gt \dfrac 1\delta\\\implies n_x+1\to \infty\;\text{ as }\;\delta\gt 0\;\text{ is arbitrary.}","['real-analysis', 'limits', 'solution-verification']"
37,How to evaluate $\lim_{{n}\to\infty}{\sum_{{k}\leq{n}}{\left\lvert\frac{\sin{k}}{\ln{n^k}}\right\rvert}}$?,How to evaluate ?,\lim_{{n}\to\infty}{\sum_{{k}\leq{n}}{\left\lvert\frac{\sin{k}}{\ln{n^k}}\right\rvert}},"Show that $$\sum_{k=1}^n{\mspace{-2mu}\frac{\left\lvert\sin{k}\right\rvert}{k}}\sim\frac{2}{\pi}\mspace{-1.5mu}\sum_{k=1}^n{\mspace{-2mu}\frac{1}{\mspace{-1mu}k}}$$ as $n\to\infty$ . Alternatively, since $\displaystyle\frac{1}{\ln{x}}\mspace{-1.5mu}\int_0^x{\mspace{-2mu}\frac{\left\lvert\sin{t}\right\rvert}{t}\operatorname{d}\!t}$ converges to $\dfrac{2}{\pi}$ as $x\to{+\infty}$ and $\displaystyle\lim_{n\to\infty}{\frac{{\it{H}}_n}{\ln{n}}}=1$ , how can we prove that $$\sum_{{1}\leq{n}\leq{\left\lfloor{x}\right\rfloor}}{\mspace{-2mu}\frac{\left\lvert\sin{n}\right\rvert}{n}}\sim\int_0^x{\mspace{-2mu}\frac{\left\lvert\sin{t}\right\rvert}{t}\operatorname*{d}\!t}$$ as $x\to{+\infty}$ ? It may seem like just one simple application of the so-called Euler–Maclaurin formula . Nonetheless, $\dfrac{\left\lvert\sin{u}\right\rvert}{u}$ is not always continuously differentiable on $\left[0, x\right]$ , hence the use of the Euler–Maclaurin summation formula shall be inadmissible in fact. So does the Stolz–Cesàro theorem . Some ""similar"" problems can be seen in many posts such as How can we prove that … , How to prove the convergence of the series? and How to find the limit… .","Show that as . Alternatively, since converges to as and , how can we prove that as ? It may seem like just one simple application of the so-called Euler–Maclaurin formula . Nonetheless, is not always continuously differentiable on , hence the use of the Euler–Maclaurin summation formula shall be inadmissible in fact. So does the Stolz–Cesàro theorem . Some ""similar"" problems can be seen in many posts such as How can we prove that … , How to prove the convergence of the series? and How to find the limit… .","\sum_{k=1}^n{\mspace{-2mu}\frac{\left\lvert\sin{k}\right\rvert}{k}}\sim\frac{2}{\pi}\mspace{-1.5mu}\sum_{k=1}^n{\mspace{-2mu}\frac{1}{\mspace{-1mu}k}} n\to\infty \displaystyle\frac{1}{\ln{x}}\mspace{-1.5mu}\int_0^x{\mspace{-2mu}\frac{\left\lvert\sin{t}\right\rvert}{t}\operatorname{d}\!t} \dfrac{2}{\pi} x\to{+\infty} \displaystyle\lim_{n\to\infty}{\frac{{\it{H}}_n}{\ln{n}}}=1 \sum_{{1}\leq{n}\leq{\left\lfloor{x}\right\rfloor}}{\mspace{-2mu}\frac{\left\lvert\sin{n}\right\rvert}{n}}\sim\int_0^x{\mspace{-2mu}\frac{\left\lvert\sin{t}\right\rvert}{t}\operatorname*{d}\!t} x\to{+\infty} \dfrac{\left\lvert\sin{u}\right\rvert}{u} \left[0, x\right]","['real-analysis', 'calculus', 'limits', 'asymptotics', 'analytic-number-theory']"
38,Proof verification: $\lim\limits_{x\to\infty}f(x)=L\iff\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$,Proof verification:,\lim\limits_{x\to\infty}f(x)=L\iff\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L,"Problem 81(a) from Section 2.6 of James Stewart's Calculus: Early Transcendentals (8e) asks us to prove that $$\lim\limits_{x\to\infty}f(x)=\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)$$ provided that these limits exist. After thinking for a while, I convinced myself that this result can be strengthened to $$\lim\limits_{x\to\infty}f(x)=L\iff \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$$ This does indeed strengthen the result because (1) it only requires the existence of at least one of the limits in question, and (2) the existence of either limit implies the existence of the other, and thus implies that $\lim_{x\to\infty}f(x)=\lim_{x\to 0^{+}}f(1/x)$ . Here's my attempt: Let's first prove that $\lim_{x\to\infty}f(x)=L\implies \lim_{x\to 0^{+}}f(1/x)=L$ . Since $\lim_{x\to\infty}f(x)=L$ , we have that for all $\varepsilon >0$ , there exists a $\delta$ such that $x>\delta\implies |f(x)-L|<\varepsilon$ . Since $\lim_{x\to 0^{+}}1/x=\infty$ , there exists a $\delta_1>0$ such that $0<x<\delta_1\implies 1/x>\delta$ . Therefore, $$0<x<\delta_1\implies\frac{1}{x}>\delta\implies\left|f\left(\frac{1}{x}\right)-L\right|<\varepsilon$$ This shows that $$\lim\limits_{x\to\infty}f(x)=L\implies \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$$ Now let's prove the converse. Since $\lim_{x\to 0^{+}}f(1/x)=L$ , we have that for all $\varepsilon >0$ , there exists a $\delta >0$ such that $0<x<\delta\implies \left|f(1/x)-L\right|<\varepsilon$ . Since $\lim_{x\to\infty}1/x=0$ , there exists a $\delta_1$ such that $x>\delta_1 \implies\left|1/x-0\right|=1/x<\delta$ . Therefore, $$x>\delta_1\implies\frac{1}{x}<\delta\implies\left|f\left(\frac{1}{\frac{1}{x}}\right)-L\right|=|f(x)-L|<\varepsilon$$ This shows that $$\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L\implies\lim\limits_{x\to\infty}f(x)=L$$ Thus, $$\lim\limits_{x\to\infty}f(x)=L\iff\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L$$ Let me know if I made any mistakes!","Problem 81(a) from Section 2.6 of James Stewart's Calculus: Early Transcendentals (8e) asks us to prove that provided that these limits exist. After thinking for a while, I convinced myself that this result can be strengthened to This does indeed strengthen the result because (1) it only requires the existence of at least one of the limits in question, and (2) the existence of either limit implies the existence of the other, and thus implies that . Here's my attempt: Let's first prove that . Since , we have that for all , there exists a such that . Since , there exists a such that . Therefore, This shows that Now let's prove the converse. Since , we have that for all , there exists a such that . Since , there exists a such that . Therefore, This shows that Thus, Let me know if I made any mistakes!",\lim\limits_{x\to\infty}f(x)=\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right) \lim\limits_{x\to\infty}f(x)=L\iff \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L \lim_{x\to\infty}f(x)=\lim_{x\to 0^{+}}f(1/x) \lim_{x\to\infty}f(x)=L\implies \lim_{x\to 0^{+}}f(1/x)=L \lim_{x\to\infty}f(x)=L \varepsilon >0 \delta x>\delta\implies |f(x)-L|<\varepsilon \lim_{x\to 0^{+}}1/x=\infty \delta_1>0 0<x<\delta_1\implies 1/x>\delta 0<x<\delta_1\implies\frac{1}{x}>\delta\implies\left|f\left(\frac{1}{x}\right)-L\right|<\varepsilon \lim\limits_{x\to\infty}f(x)=L\implies \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L \lim_{x\to 0^{+}}f(1/x)=L \varepsilon >0 \delta >0 0<x<\delta\implies \left|f(1/x)-L\right|<\varepsilon \lim_{x\to\infty}1/x=0 \delta_1 x>\delta_1 \implies\left|1/x-0\right|=1/x<\delta x>\delta_1\implies\frac{1}{x}<\delta\implies\left|f\left(\frac{1}{\frac{1}{x}}\right)-L\right|=|f(x)-L|<\varepsilon \lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L\implies\lim\limits_{x\to\infty}f(x)=L \lim\limits_{x\to\infty}f(x)=L\iff\lim\limits_{x\to 0^{+}}f\left(\frac{1}{x}\right)=L,"['limits', 'solution-verification', 'epsilon-delta']"
39,Prove that $f(x) \to L$ and $g(x) \to M$ as $x \to x_0$ and $f(x) \leq g(x)$ implies that $L \leq M$,Prove that  and  as  and  implies that,f(x) \to L g(x) \to M x \to x_0 f(x) \leq g(x) L \leq M,"Here's what I'm trying to prove: Let $f$ and $g$ be functions. Suppose that the following hold: $f(x) \leq g(x)$ $\lim_{x \to x_0} f(x) = L$ and $\lim_{x \to x_0} g(x) = M$ Then, $L \leq M$ Proof Attempt: By hypothesis, for any $\epsilon > 0$ , there exist $\delta_1,\delta_2 > 0$ such that: $0 < |x-x_0| < \delta_1 \implies |f(x) - L| < \epsilon$ $0 < |x-x_0| < \delta_2 \implies |g(x) - M| < \epsilon$ Let $\delta = \min\{\delta_1,\delta_2\}$ . Then, if we have $0 < |x-x_0| < \delta$ , we get: $L-\epsilon < f(x) \leq g(x) < M + \epsilon$ $\implies 0 < (M-L) + 2\epsilon$ Suppose that $M-L < 0$ . Let $\epsilon = \frac{L-M}{2}$ . Then, this gives us: $0 < (M-L) + (L-M) = 0$ That is an absurdity. Hence, $M-L \geq 0$ so $M \geq L$ . That proves the desired result. Does the proof above work? If it doesn't, why? How can I fix it?","Here's what I'm trying to prove: Let and be functions. Suppose that the following hold: and Then, Proof Attempt: By hypothesis, for any , there exist such that: Let . Then, if we have , we get: Suppose that . Let . Then, this gives us: That is an absurdity. Hence, so . That proves the desired result. Does the proof above work? If it doesn't, why? How can I fix it?","f g f(x) \leq g(x) \lim_{x \to x_0} f(x) = L \lim_{x \to x_0} g(x) = M L \leq M \epsilon > 0 \delta_1,\delta_2 > 0 0 < |x-x_0| < \delta_1 \implies |f(x) - L| < \epsilon 0 < |x-x_0| < \delta_2 \implies |g(x) - M| < \epsilon \delta = \min\{\delta_1,\delta_2\} 0 < |x-x_0| < \delta L-\epsilon < f(x) \leq g(x) < M + \epsilon \implies 0 < (M-L) + 2\epsilon M-L < 0 \epsilon = \frac{L-M}{2} 0 < (M-L) + (L-M) = 0 M-L \geq 0 M \geq L","['real-analysis', 'calculus', 'limits', 'solution-verification']"
40,Simplify $f(x)=\sec (\tan^{-1} (\sin (\tan^{-1} x)))$ for $x\in\mathbb{R}$ and find its inflection points,Simplify  for  and find its inflection points,f(x)=\sec (\tan^{-1} (\sin (\tan^{-1} x))) x\in\mathbb{R},"Simplify $f(x)=\sec (\tan^{-1} (\sin (\tan^{-1} x)))$ for $x\in\mathbb{R}$ and find its inflection points, local extrema, x-intercepts, y-intercept, asymptotes, etc. So the $y$ -intercept is obviously $1$ . I'm not exactly sure if my simplification below is correct, but if it is, everything else that's required should be trivial for me (finding the local extrema and inflections points is very easy, though I'd like to know if there are any shortcuts other than evaluating the derivatives and relevant limits). So I know by drawing a simple triangle that $\sin(\tan^{-1}x)) = \dfrac{x}{\sqrt{1+x^2}}$ and $\sec(\tan^{-1}x)=\sqrt{1+x^2}.$ But then, doesn't that mean that $\sec(\tan^{-1}(\sin(\tan^{-1}x)))=\sqrt{2-\dfrac{1}{1+x^2}}$ ?","Simplify for and find its inflection points, local extrema, x-intercepts, y-intercept, asymptotes, etc. So the -intercept is obviously . I'm not exactly sure if my simplification below is correct, but if it is, everything else that's required should be trivial for me (finding the local extrema and inflections points is very easy, though I'd like to know if there are any shortcuts other than evaluating the derivatives and relevant limits). So I know by drawing a simple triangle that and But then, doesn't that mean that ?",f(x)=\sec (\tan^{-1} (\sin (\tan^{-1} x))) x\in\mathbb{R} y 1 \sin(\tan^{-1}x)) = \dfrac{x}{\sqrt{1+x^2}} \sec(\tan^{-1}x)=\sqrt{1+x^2}. \sec(\tan^{-1}(\sin(\tan^{-1}x)))=\sqrt{2-\dfrac{1}{1+x^2}},['calculus']
41,Why use the derivative and not the symmetric derivative?,Why use the derivative and not the symmetric derivative?,,"The symmetric derivative is always equal to the regular derivative when it exists, and still isn't defined for jump discontinuities. From what I can tell the only differences are that a symmetric derivative will give the 'expected slope' for removable discontinuities, and the average slope at cusps. These seem like extremely reasonable quantities to work with (especially the former), so I'm wondering why the 'typical' derivative isn't taken to be this one. What advantage is there to taking $\lim\limits_{h\to0}\frac{f(x+h)-f(x)} h$ as the main quantity of interest instead? Why would we want to use the one that's defined less often?","The symmetric derivative is always equal to the regular derivative when it exists, and still isn't defined for jump discontinuities. From what I can tell the only differences are that a symmetric derivative will give the 'expected slope' for removable discontinuities, and the average slope at cusps. These seem like extremely reasonable quantities to work with (especially the former), so I'm wondering why the 'typical' derivative isn't taken to be this one. What advantage is there to taking $\lim\limits_{h\to0}\frac{f(x+h)-f(x)} h$ as the main quantity of interest instead? Why would we want to use the one that's defined less often?",,"['calculus', 'functions', 'derivatives']"
42,The limit of functions of two variables,The limit of functions of two variables,,"The limit of $f(x,y)=\frac{xy^2}{x^2+y^4}$ as $(x,y) \longrightarrow(0,0)$ is doesn't exist, because if we take two paths: 1) Along the path, $x=0$ , $y\longrightarrow0$ : $$\lim_{(x,y)\longrightarrow(0,0)}\frac{xy^2}{x^2+y^4}=0$$ 2)  Along the path, $x=y^2$ , $y\longrightarrow0$ : $$\lim_{(x,y)\longrightarrow(0,0)}\frac{xy^2}{x^2+y^4}=\lim_{y\longrightarrow0}\frac{y^2y^2}{(y^2)^2+y^4}=\frac{1}{2}$$ But if we use polar coordinate methods of evaluating limits of functions of two variable, we get the limit of the above function becomes zero, i.e., $x=r\cos(\theta),y=r\sin(\theta)$ , we know that $x^2+y^2=r^2$ and this indicates that $r\longrightarrow0$ as $(x,y)\longrightarrow(0,0)$ , therefore $$\lim_{(x,y)\longrightarrow(0,0)}\frac{xy^2}{x^2+y^4}=\lim_{r\longrightarrow0}\frac{r^3\cos(\theta)\sin^2(\theta)}{r^2\cos^2(\theta)+r^4\sin^4(\theta)}$$ $$=\lim_{r\longrightarrow0}\frac{r\cos(\theta)\sin^2(\theta)}{\cos^2(\theta)+r^2\sin^4(\theta)}=0$$ Please someone help me on this contradictory, why this is so happened?.","The limit of as is doesn't exist, because if we take two paths: 1) Along the path, , : 2)  Along the path, , : But if we use polar coordinate methods of evaluating limits of functions of two variable, we get the limit of the above function becomes zero, i.e., , we know that and this indicates that as , therefore Please someone help me on this contradictory, why this is so happened?.","f(x,y)=\frac{xy^2}{x^2+y^4} (x,y) \longrightarrow(0,0) x=0 y\longrightarrow0 \lim_{(x,y)\longrightarrow(0,0)}\frac{xy^2}{x^2+y^4}=0 x=y^2 y\longrightarrow0 \lim_{(x,y)\longrightarrow(0,0)}\frac{xy^2}{x^2+y^4}=\lim_{y\longrightarrow0}\frac{y^2y^2}{(y^2)^2+y^4}=\frac{1}{2} x=r\cos(\theta),y=r\sin(\theta) x^2+y^2=r^2 r\longrightarrow0 (x,y)\longrightarrow(0,0) \lim_{(x,y)\longrightarrow(0,0)}\frac{xy^2}{x^2+y^4}=\lim_{r\longrightarrow0}\frac{r^3\cos(\theta)\sin^2(\theta)}{r^2\cos^2(\theta)+r^4\sin^4(\theta)} =\lim_{r\longrightarrow0}\frac{r\cos(\theta)\sin^2(\theta)}{\cos^2(\theta)+r^2\sin^4(\theta)}=0","['real-analysis', 'limits', 'multivariable-calculus']"
43,Prove a limit involving the ceiling function,Prove a limit involving the ceiling function,,"I found a pattern that I want to prove: $$f(x) = 2^{\lceil \log_2(3^x)\rceil} - 3^x\quad \{x\in\mathbb{Z}^+\} $$ $$ \lim_{x\rightarrow\infty} f(x) = \infty $$ Discussion: $$ f(x) = 2^{\lceil \log_2(3^x)\rceil} - 3^x = 3^x(2^{\lceil \log_2(3^x)\rceil-\log_2(3^x)}-1)$$ $$0<\lceil \log_2(3^x)\rceil-\log_2(3^x)<1\ \Rightarrow\ 0<f(x)<3^x$$ $f(x)$ 's lower bound is zero and its upper bound tends to infinity. As far as I know, $f(x)$ can oscillate anywhere in between. However, after checking a few first thousands values of $f$ , I am convinced that the function indeed tends to infinity. I guess the fact that $x$ is an integer plays a role in that. If $\lceil \log_2(3^x) \rceil$ was really close to $\log_2(3^x)$ , $f(x)$ would be really close to $0$ , so the limit would not hold. Experimental results suggest the limit exists, so I guess there are some restrictions on how close $\log_2(3^x)$ can be to its ceiling integer. I don't know how to proceed from that.","I found a pattern that I want to prove: Discussion: 's lower bound is zero and its upper bound tends to infinity. As far as I know, can oscillate anywhere in between. However, after checking a few first thousands values of , I am convinced that the function indeed tends to infinity. I guess the fact that is an integer plays a role in that. If was really close to , would be really close to , so the limit would not hold. Experimental results suggest the limit exists, so I guess there are some restrictions on how close can be to its ceiling integer. I don't know how to proceed from that.",f(x) = 2^{\lceil \log_2(3^x)\rceil} - 3^x\quad \{x\in\mathbb{Z}^+\}   \lim_{x\rightarrow\infty} f(x) = \infty   f(x) = 2^{\lceil \log_2(3^x)\rceil} - 3^x = 3^x(2^{\lceil \log_2(3^x)\rceil-\log_2(3^x)}-1) 0<\lceil \log_2(3^x)\rceil-\log_2(3^x)<1\ \Rightarrow\ 0<f(x)<3^x f(x) f(x) f x \lceil \log_2(3^x) \rceil \log_2(3^x) f(x) 0 \log_2(3^x),"['number-theory', 'limits']"
44,"Suppose $f: \mathbb{R} \to \mathbb{R}$ is twice differentiable. Show that $\lim_{x \to \infty} f'' (x) = 0$, given conditions.","Suppose  is twice differentiable. Show that , given conditions.",f: \mathbb{R} \to \mathbb{R} \lim_{x \to \infty} f'' (x) = 0,"Suppose $f: \mathbb{R} \to \mathbb{R}$ is twice differentiable. Show that $\lim_{x \to \infty} f''(x) = 0$, given that $\lim_{x \to \infty} f(x)$ and $\lim_{x \to \infty} f''(x)$ exist. My attempt: Let $x > 0$. By MVT, there is $c_1(x) \in (x,2x)$ and $c_2(x) \in (3x,4x)$ with $$1/x(f(2x)-f(x)) = f'(c_1(x)); \quad  1/x(f(4x)-f(3x)) = f'(c_2(x))$$ Again, by MVT, there is $c_3(x) \in (c_1(x), c_2(x))$ with $$f''(c_3(x)) ( c_2(x)-c_1(x)) = f'(c_2(x))-f'(c_1(x)) = 1/x (f(2x)-f(x)-f(4x)+f(3x))$$ Taking $\lim_{x \to \infty}$ of both sides, we find: $$\lim_{x \to \infty} f'' (c_3(x)) (c_2(x)-c_1(x)) = 0$$ Because $c_2(x) - c_1(x) > 3x - 2x = x \to \infty$, it must be the case that $\lim_{x \to \infty} f''(c_3(x)) = 0$ But $c_3(x)  > c_1(x) > x \to \infty$, which implies that $\lim_{x \to \infty} f''(x) = 0$. Is this correct?","Suppose $f: \mathbb{R} \to \mathbb{R}$ is twice differentiable. Show that $\lim_{x \to \infty} f''(x) = 0$, given that $\lim_{x \to \infty} f(x)$ and $\lim_{x \to \infty} f''(x)$ exist. My attempt: Let $x > 0$. By MVT, there is $c_1(x) \in (x,2x)$ and $c_2(x) \in (3x,4x)$ with $$1/x(f(2x)-f(x)) = f'(c_1(x)); \quad  1/x(f(4x)-f(3x)) = f'(c_2(x))$$ Again, by MVT, there is $c_3(x) \in (c_1(x), c_2(x))$ with $$f''(c_3(x)) ( c_2(x)-c_1(x)) = f'(c_2(x))-f'(c_1(x)) = 1/x (f(2x)-f(x)-f(4x)+f(3x))$$ Taking $\lim_{x \to \infty}$ of both sides, we find: $$\lim_{x \to \infty} f'' (c_3(x)) (c_2(x)-c_1(x)) = 0$$ Because $c_2(x) - c_1(x) > 3x - 2x = x \to \infty$, it must be the case that $\lim_{x \to \infty} f''(c_3(x)) = 0$ But $c_3(x)  > c_1(x) > x \to \infty$, which implies that $\lim_{x \to \infty} f''(x) = 0$. Is this correct?",,['real-analysis']
45,"Nonsmooth initial data in the conservation laws, their approximations and limits","Nonsmooth initial data in the conservation laws, their approximations and limits",,"In the book by R. LeVeque: ""Numerical methods for conservation laws"", Birkhauser, (1992), 2nd edition, in the Subsection 3.1.2 called "" Nonsmooth data "", the author talks about possibilities for finding generalized solutions of the conservation laws when we have nonsmooth initial data. On $22^{nd}$ page he says: ""One possibility is to approximate nonsmooth data $u_0(x)$ by a sequence of smooth functions $u^{\epsilon}_0(x)$ , with $\parallel  u^{\epsilon}_0(x)- u_0 (x) \parallel_{L^1} <\epsilon \: \mbox{as} \: \epsilon \rightarrow 0$ ... Unfortunately, this approach of smoothing the initial data will not work for nonlinear problems."" My (nonlinear) problem is this: let's say we have conservation law $$\begin{cases} 	u_t+f(u)_x=0, \\[2ex]  	u(x,0)=u_0 (x), \end{cases}$$ where $u_0$ is nonsmooth Riemann initial data $$u_0(x)= \begin{cases} 	u_l, x<0, \\[2ex] 	u_r, x>0, \end{cases}$$ where $u_l$ and $u_r$ are constants. We approximate the Riemann data with a smooth data that depends on some parameter $\epsilon$ , i.e. we change $u_0$ with $u^{\epsilon}_0(x)$ . In my case, the new initial data $u^{\epsilon}_0(x)$ are in the Sobolev space $H^s(\mathbb{R})$ , where $s$ is integer bigger than five. This is now a Cauchy problem with smooth data that we can solve. We find solutions of the approximate problem in some Sobolev-valued $H^s(\mathbb{R})$ space (e.g. $C([0,T];H^s(\mathbb{R}))$ , $L^p(0,T,H^s(\mathbb{R}))$ or similar).  And now we have a solution of approiximate problem that depends on parameter $\epsilon$ . In order to get back on the initial problem we should let $\epsilon\rightarrow 0$ . My question is: what do we need in order to pass to the limit $\epsilon\rightarrow 0$ (and then get a connection with a solution of the Riemann problem)? Generally I've always thought that this is possible either directly, by using some compactness lemma or with some energy estimates, but I am not sure how to do it in this case. Here on the one side we have a classical smooth solution, and  on the other side, after we let $\epsilon\rightarrow 0$ , we should get discontinuous (weak) solution possibly in some other space. So we have different type of solutions. Also if anyone know any paper/book where pde problem similar to this is studied let me know. I am interested in problems where we change initial condition to the smoooth one, than solve the ""approximation problem"" and than connected that solution with the solution of the original problem. It could be by letting $\epsilon\rightarrow 0$ directly or using some theorem.","In the book by R. LeVeque: ""Numerical methods for conservation laws"", Birkhauser, (1992), 2nd edition, in the Subsection 3.1.2 called "" Nonsmooth data "", the author talks about possibilities for finding generalized solutions of the conservation laws when we have nonsmooth initial data. On page he says: ""One possibility is to approximate nonsmooth data by a sequence of smooth functions , with ... Unfortunately, this approach of smoothing the initial data will not work for nonlinear problems."" My (nonlinear) problem is this: let's say we have conservation law where is nonsmooth Riemann initial data where and are constants. We approximate the Riemann data with a smooth data that depends on some parameter , i.e. we change with . In my case, the new initial data are in the Sobolev space , where is integer bigger than five. This is now a Cauchy problem with smooth data that we can solve. We find solutions of the approximate problem in some Sobolev-valued space (e.g. , or similar).  And now we have a solution of approiximate problem that depends on parameter . In order to get back on the initial problem we should let . My question is: what do we need in order to pass to the limit (and then get a connection with a solution of the Riemann problem)? Generally I've always thought that this is possible either directly, by using some compactness lemma or with some energy estimates, but I am not sure how to do it in this case. Here on the one side we have a classical smooth solution, and  on the other side, after we let , we should get discontinuous (weak) solution possibly in some other space. So we have different type of solutions. Also if anyone know any paper/book where pde problem similar to this is studied let me know. I am interested in problems where we change initial condition to the smoooth one, than solve the ""approximation problem"" and than connected that solution with the solution of the original problem. It could be by letting directly or using some theorem.","22^{nd} u_0(x) u^{\epsilon}_0(x) \parallel  u^{\epsilon}_0(x)- u_0 (x) \parallel_{L^1} <\epsilon \: \mbox{as} \: \epsilon \rightarrow 0 \begin{cases}
	u_t+f(u)_x=0, \\[2ex] 
	u(x,0)=u_0 (x),
\end{cases} u_0 u_0(x)= \begin{cases}
	u_l, x<0, \\[2ex]
	u_r, x>0,
\end{cases} u_l u_r \epsilon u_0 u^{\epsilon}_0(x) u^{\epsilon}_0(x) H^s(\mathbb{R}) s H^s(\mathbb{R}) C([0,T];H^s(\mathbb{R})) L^p(0,T,H^s(\mathbb{R})) \epsilon \epsilon\rightarrow 0 \epsilon\rightarrow 0 \epsilon\rightarrow 0 \epsilon\rightarrow 0","['limits', 'reference-request', 'partial-differential-equations', 'hyperbolic-equations']"
46,"A limit question of 3-variable-functions. $\lim\limits_{ (x,y,z) \to (0,0,0)} \frac {xyz^2}{x^2+y^4+z^6}$",A limit question of 3-variable-functions.,"\lim\limits_{ (x,y,z) \to (0,0,0)} \frac {xyz^2}{x^2+y^4+z^6}","$$\lim\limits_{ (x,y,z) \to (0,0,0)} \frac {xyz^2}{x^2+y^4+z^6}$$ I checked that the limit does not exist but I cannot prove that. I tried $y=mx$, $z=nx$ and also $y=x^m$, $z=x^n$ but they gave me nothing but the limit equals to zero. Thanks a lot","$$\lim\limits_{ (x,y,z) \to (0,0,0)} \frac {xyz^2}{x^2+y^4+z^6}$$ I checked that the limit does not exist but I cannot prove that. I tried $y=mx$, $z=nx$ and also $y=x^m$, $z=x^n$ but they gave me nothing but the limit equals to zero. Thanks a lot",,"['calculus', 'real-analysis', 'analysis', 'limits', 'multivariable-calculus']"
47,Are limits commutative?,Are limits commutative?,,"Generally speaking, is the following true: $$\lim_{x\to a}f'(x)=\lim_{x\to a}\left(\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\right)=\lim_{h\to 0}\left(\lim_{x\to a}\frac{f(x+h)-f(x)}{h}\right)$$","Generally speaking, is the following true: $$\lim_{x\to a}f'(x)=\lim_{x\to a}\left(\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\right)=\lim_{h\to 0}\left(\lim_{x\to a}\frac{f(x+h)-f(x)}{h}\right)$$",,['real-analysis']
48,Prove for definition $ $ $\lim_{x \to 0}\frac{1}{\cos(x)}=1$.,Prove for definition  .,  \lim_{x \to 0}\frac{1}{\cos(x)}=1,"Prove for definition $$\lim_{x\rightarrow 0}\frac{1}{\cos(x)}=1$$ Let $\epsilon >0$, $\delta =|\sec^{-1}(\frac{e^2}{4})|+1$ If $|x|<\delta$ then $|\frac{1}{\cos(x)}-1|=|\frac{1-\cos(x)}{\cos(x)}|=\frac{|1-\cos(x)|}{|\cos(x)|}\leq\frac{2}{|\cos(x)|}=|\sec(x)|2<2(|\sec^{-1}(\frac{\epsilon^2}{4})|+1)=2|\sec^{-1}(\frac{\epsilon^2}{4})|+2$ I'm stuck here. How can i prove this? $2\sec^{-1}(\frac{e^2}{4})+2<\epsilon$","Prove for definition $$\lim_{x\rightarrow 0}\frac{1}{\cos(x)}=1$$ Let $\epsilon >0$, $\delta =|\sec^{-1}(\frac{e^2}{4})|+1$ If $|x|<\delta$ then $|\frac{1}{\cos(x)}-1|=|\frac{1-\cos(x)}{\cos(x)}|=\frac{|1-\cos(x)|}{|\cos(x)|}\leq\frac{2}{|\cos(x)|}=|\sec(x)|2<2(|\sec^{-1}(\frac{\epsilon^2}{4})|+1)=2|\sec^{-1}(\frac{\epsilon^2}{4})|+2$ I'm stuck here. How can i prove this? $2\sec^{-1}(\frac{e^2}{4})+2<\epsilon$",,"['real-analysis', 'limits']"
49,"Evaluate $\lim\limits_{(x,y)\to(0,0)}\frac{(x+y)^2}{x^2+y^2}$",Evaluate,"\lim\limits_{(x,y)\to(0,0)}\frac{(x+y)^2}{x^2+y^2}","Evaluate $\displaystyle\lim_{(x,y)\to(0,0)}\dfrac{(x+y)^2}{x^2+y^2}$ Using polar, we have $x=r\cos(\theta),y=r\sin(\theta)$ Our limit becomes: $$\lim_{r\to 0}\dfrac{(r\cos(\theta)+r\sin(\theta))^2}{r^2\sin^2(\theta)+r^2\cos(\theta)}=\lim_{r\to 0}\dfrac{r^2\cos^2(\theta)+2r^2\cos(\theta)\sin(\theta)+r^2\sin^2(\theta)}{r^2}$$ Factoring and dividing removes the $r^2$ in the denominator, and we get $1$ as the limit. However this is not right. If we consider along the $x-axis$, our limit becomes $1$. If we consider along the line $y=x$, our limit becomes $1/2$, and are clearly not equal. This means that the limit does not exist but my polar said it does and it equals $1$. Where did I mess up in my polar coordinates?","Evaluate $\displaystyle\lim_{(x,y)\to(0,0)}\dfrac{(x+y)^2}{x^2+y^2}$ Using polar, we have $x=r\cos(\theta),y=r\sin(\theta)$ Our limit becomes: $$\lim_{r\to 0}\dfrac{(r\cos(\theta)+r\sin(\theta))^2}{r^2\sin^2(\theta)+r^2\cos(\theta)}=\lim_{r\to 0}\dfrac{r^2\cos^2(\theta)+2r^2\cos(\theta)\sin(\theta)+r^2\sin^2(\theta)}{r^2}$$ Factoring and dividing removes the $r^2$ in the denominator, and we get $1$ as the limit. However this is not right. If we consider along the $x-axis$, our limit becomes $1$. If we consider along the line $y=x$, our limit becomes $1/2$, and are clearly not equal. This means that the limit does not exist but my polar said it does and it equals $1$. Where did I mess up in my polar coordinates?",,"['limits', 'multivariable-calculus', 'polar-coordinates']"
50,"showing that a multivariate function is not continuous at the origin, although Wolfram Alpha says it is","showing that a multivariate function is not continuous at the origin, although Wolfram Alpha says it is",,"I am being asked to show that the following function is NOT continuous at the origin. $f(x,y) =\begin{cases}\frac{x^4y^5}{x^8 + y^{10}} &&(x,y) \not= (0,0) \\0 &&(x,y) = (0,0) \end{cases}$ Thus, I've tried to show that the following limit does not equal to zero, with different paths $y=mx, y=x^2$, and so forth, but without success. $\lim_{(x,y) \to (0,0)} \frac{x^4y^5}{x^8 + y^{10}}$ Eventually I checked with Wolfram Alpha, and it computed that the limit is indeed 0, as I have suspected, so I figured there might be some mistake in the exercise... In fact, the whole exercise is to show that even though the function is not continuous at the origin, its partial derivatives exist.  I have had several exercises like this, where Wolfram contradicted what I was asked to prove. And I know that Wolfram has been wrong before, so I would appreciate some help in the matter - Is Wolfram wrong, or the exercise? Thanks in advance.","I am being asked to show that the following function is NOT continuous at the origin. $f(x,y) =\begin{cases}\frac{x^4y^5}{x^8 + y^{10}} &&(x,y) \not= (0,0) \\0 &&(x,y) = (0,0) \end{cases}$ Thus, I've tried to show that the following limit does not equal to zero, with different paths $y=mx, y=x^2$, and so forth, but without success. $\lim_{(x,y) \to (0,0)} \frac{x^4y^5}{x^8 + y^{10}}$ Eventually I checked with Wolfram Alpha, and it computed that the limit is indeed 0, as I have suspected, so I figured there might be some mistake in the exercise... In fact, the whole exercise is to show that even though the function is not continuous at the origin, its partial derivatives exist.  I have had several exercises like this, where Wolfram contradicted what I was asked to prove. And I know that Wolfram has been wrong before, so I would appreciate some help in the matter - Is Wolfram wrong, or the exercise? Thanks in advance.",,"['calculus', 'limits', 'multivariable-calculus']"
51,Does this Abel sum related power series diverge to $-\infty$ at $1$?,Does this Abel sum related power series diverge to  at ?,-\infty 1,"Define $a_n$ for $n\geq 2$ to be $1$ if the second most significant digit of $n$ in binary is $1$ and $-1$ otherwise. For example for $n=23$, in binary $n=10111_2$ and so $a_n=-1$, becuase the second digit of $n$ was $0$. The limit in question is $\lim_{x\to 1} f(x)=\lim_{x\to 1}\sum_{n=2}^\infty a_nx^n$. Intuition: The coefficients of this power series are only $-1$ and $+1$. There is (starting from $2$) $a_n=-1,1,-1,-1,1,1,-1,-1,-1,-1,1,1,1,1,\ldots$ followed by 8 times $-1$ and so on with the number of $\pm 1$ being increasing powers of $2$. The negative powers of $x$ therefore come earlier and should be more significant, resulting the limit in being $-\infty$. Approaches: The limit and the sum sadly cannot be interchanged. I tried summing the finite geometric series, which occur in the sum. This results in $$\sum_{n=1}^\infty -x^{2^n} \frac {1-x^{2^{n-1}+1}}{1-x} +x^{2^n+2^{n-1}}\frac {1-x^{2^{n-1}+1}}{1-x} $$ and does not look promising for me. Maybe this still works somehow?! (Might have mistakes, I wrote this from memory) I found the following functional equation for $f$: $$(1+x)f(x^2)-x^2+x^3=f(x)$$ and plugging in $x=1$ yields $2f(1)=f(1)$, which leaves only $f(1)=\pm \infty,0$ . $+\infty$ is easily excluded, because $f$ is negative on $(0,1)$ and this also lacks an argument for the convergence after all. Another approach with simply the definition of limits failed miserably. But again this might be my fault. Any hints or maybe even solutions are appreciated.","Define $a_n$ for $n\geq 2$ to be $1$ if the second most significant digit of $n$ in binary is $1$ and $-1$ otherwise. For example for $n=23$, in binary $n=10111_2$ and so $a_n=-1$, becuase the second digit of $n$ was $0$. The limit in question is $\lim_{x\to 1} f(x)=\lim_{x\to 1}\sum_{n=2}^\infty a_nx^n$. Intuition: The coefficients of this power series are only $-1$ and $+1$. There is (starting from $2$) $a_n=-1,1,-1,-1,1,1,-1,-1,-1,-1,1,1,1,1,\ldots$ followed by 8 times $-1$ and so on with the number of $\pm 1$ being increasing powers of $2$. The negative powers of $x$ therefore come earlier and should be more significant, resulting the limit in being $-\infty$. Approaches: The limit and the sum sadly cannot be interchanged. I tried summing the finite geometric series, which occur in the sum. This results in $$\sum_{n=1}^\infty -x^{2^n} \frac {1-x^{2^{n-1}+1}}{1-x} +x^{2^n+2^{n-1}}\frac {1-x^{2^{n-1}+1}}{1-x} $$ and does not look promising for me. Maybe this still works somehow?! (Might have mistakes, I wrote this from memory) I found the following functional equation for $f$: $$(1+x)f(x^2)-x^2+x^3=f(x)$$ and plugging in $x=1$ yields $2f(1)=f(1)$, which leaves only $f(1)=\pm \infty,0$ . $+\infty$ is easily excluded, because $f$ is negative on $(0,1)$ and this also lacks an argument for the convergence after all. Another approach with simply the definition of limits failed miserably. But again this might be my fault. Any hints or maybe even solutions are appreciated.",,"['real-analysis', 'limits', 'power-series', 'divergent-series']"
52,Generalized Taylor derivatives test,Generalized Taylor derivatives test,,"I am seeking for a proof of the generalized derivative test to find inflection points, minima and maxima. I am seeking for a proof that I read some time ago but can't find anymore. The thesis was that if the first derivative in a point is zero, and also the second derivative is zero, then you have to continue the derivation until you get an n-th derivative with a non-zero value. If that derivative is of an even order, then you have to check if it is positive or negative and subsequently you deduct that it is a minimum or a maximum. If the order of the derivative is odd then it is an inflection point. The proof used the Taylor formula. I am sure it is a well known subject and that many questions were asked about this, but I can't find a rigorous complete proof, I found only descriptive talks. A link to a PDF is also completely okay. Thanks in advance!:)","I am seeking for a proof of the generalized derivative test to find inflection points, minima and maxima. I am seeking for a proof that I read some time ago but can't find anymore. The thesis was that if the first derivative in a point is zero, and also the second derivative is zero, then you have to continue the derivation until you get an n-th derivative with a non-zero value. If that derivative is of an even order, then you have to check if it is positive or negative and subsequently you deduct that it is a minimum or a maximum. If the order of the derivative is odd then it is an inflection point. The proof used the Taylor formula. I am sure it is a well known subject and that many questions were asked about this, but I can't find a rigorous complete proof, I found only descriptive talks. A link to a PDF is also completely okay. Thanks in advance!:)",,"['calculus', 'limits', 'proof-writing', 'taylor-expansion']"
53,A question about a limit involving nested radicals [duplicate],A question about a limit involving nested radicals [duplicate],,This question already has answers here : How do I calculate this limit: $\lim\limits_{n\to\infty}1+\sqrt[2]{2+\sqrt[3]{3+\dotsb+\sqrt[n]n}}$? (2 answers) Closed 7 years ago . $$\lim\limits_{n\rightarrow \infty}1＋\sqrt{2+\sqrt[3]{3＋…\sqrt[n]{n}}}$$ Any hint will be appreciated,This question already has answers here : How do I calculate this limit: $\lim\limits_{n\to\infty}1+\sqrt[2]{2+\sqrt[3]{3+\dotsb+\sqrt[n]n}}$? (2 answers) Closed 7 years ago . $$\lim\limits_{n\rightarrow \infty}1＋\sqrt{2+\sqrt[3]{3＋…\sqrt[n]{n}}}$$ Any hint will be appreciated,,"['limits', 'nested-radicals']"
54,Slightly changing the formal definition of continuity of $f: \mathbb{R} \to \mathbb{R}$?,Slightly changing the formal definition of continuity of ?,f: \mathbb{R} \to \mathbb{R},"I'm curious for some perspectives on why it would be wrong to change the definition of continuity of $f: \Bbb R \to \Bbb R$ in the following way: Original definition. $f : \Bbb R \to \Bbb R$ is said to be continuous at $x \in \Bbb R$ if $\forall \epsilon > 0$ $\exists \delta > 0$ such that $|x - a| < \delta \implies |f(x) - f(a)| < \epsilon$ . Altered definition. $f : \Bbb R \to \Bbb R$ is said to be continuous at $x \in \Bbb R$ if $\forall \delta > 0$ $\exists \epsilon > 0$ such that $|x - a| < \delta \implies |f(x) - f(a)| < \epsilon$ . The altered definition is more in line with what I think when I think about continuity intuitively: nearby points are sent to nearby points.  It only makes sense to me to be able to choose ""nearness"" in the domain (i.e., $\forall \delta > 0$ ) and show there is nearness in the codomain (i.e., $\exists \epsilon > 0$ ) to prove intuitively that ""nearby points are sent to nearby points"". Similarly , if $X, Y$ are topological spaces, we say $f: X \to Y$ is continuous if the preimages of open sets are open.  What would be wrong about changing the definition to say that a map is continuous if the images of open sets are open (i.e., $f$ is continuous if it is an open map)?  This is more inline with the intuitive idea of ""nearby points being sent to nearby points"" -- you pick nearness in the domain (i.e., an arbitrary open set) an show nearness in the codomain (i.e., the image is open). Does anyone have any useful remarks?","I'm curious for some perspectives on why it would be wrong to change the definition of continuity of in the following way: Original definition. is said to be continuous at if such that . Altered definition. is said to be continuous at if such that . The altered definition is more in line with what I think when I think about continuity intuitively: nearby points are sent to nearby points.  It only makes sense to me to be able to choose ""nearness"" in the domain (i.e., ) and show there is nearness in the codomain (i.e., ) to prove intuitively that ""nearby points are sent to nearby points"". Similarly , if are topological spaces, we say is continuous if the preimages of open sets are open.  What would be wrong about changing the definition to say that a map is continuous if the images of open sets are open (i.e., is continuous if it is an open map)?  This is more inline with the intuitive idea of ""nearby points being sent to nearby points"" -- you pick nearness in the domain (i.e., an arbitrary open set) an show nearness in the codomain (i.e., the image is open). Does anyone have any useful remarks?","f: \Bbb R \to \Bbb R f : \Bbb R \to \Bbb R x \in \Bbb R \forall \epsilon > 0 \exists \delta > 0 |x - a| < \delta \implies |f(x) - f(a)| < \epsilon f : \Bbb R \to \Bbb R x \in \Bbb R \forall \delta > 0 \exists \epsilon > 0 |x - a| < \delta \implies |f(x) - f(a)| < \epsilon \forall \delta > 0 \exists \epsilon > 0 X, Y f: X \to Y f","['real-analysis', 'general-topology', 'continuity']"
55,How to prove $\lim\limits_{x\to \infty}3x=\infty$?,How to prove ?,\lim\limits_{x\to \infty}3x=\infty,"How to prove $\lim\limits_{x\to \infty}3x=\infty$? First I am not sure about formal definition of $\lim\limits_{x\to \infty}f(x)=\infty$, I guess $\forall K\in \Bbb{R},\exists N\in \Bbb{R}:x\gt N\implies f(x)\gt K$ If that's the case, Let $K\in \Bbb{R}$, let $N=\frac{K}{3}$, then $x\gt \frac{K}{3}\implies 3x\gt K$. I am sure it's not this simple. Could someone givea valid one?","How to prove $\lim\limits_{x\to \infty}3x=\infty$? First I am not sure about formal definition of $\lim\limits_{x\to \infty}f(x)=\infty$, I guess $\forall K\in \Bbb{R},\exists N\in \Bbb{R}:x\gt N\implies f(x)\gt K$ If that's the case, Let $K\in \Bbb{R}$, let $N=\frac{K}{3}$, then $x\gt \frac{K}{3}\implies 3x\gt K$. I am sure it's not this simple. Could someone givea valid one?",,"['calculus', 'limits', 'epsilon-delta']"
56,Definition of limit as $x\rightarrow \infty$,Definition of limit as,x\rightarrow \infty,Every time i get confused with the definition of $\lim_{x\rightarrow \infty}f(x)=L$. I could not find a reference that will give the definition. I am trying to write what i understood. See if this is correct. By $\lim_{x\rightarrow \infty}f(x)=L$ we mean the following : Given $\epsilon >0$ there exists $R>0$ such that $|f(x)-L|<\epsilon$ for all $x>R$. By $\lim_{x\rightarrow -\infty}f(x)=L$ we mean the following : Given $\epsilon >0$ there exists $R<0$ such that $|f(x)-L|<\epsilon$ for all $x<R$. By $\lim_{x\rightarrow \infty}f(x)=\infty$ we mean the following: Given $R>0$ there exists $L>0$ such that $|f(x)|>R$ for all $x>L$ By $\lim_{x\rightarrow \infty}f(x)=-\infty$ we mean the following: Given $R<0$ there exists $L>0$ such that $f(x)<R$ for all $x>L$ By $\lim_{x\rightarrow -\infty}f(x)=\infty$ we mean the following: Given $R>0$ there exists $L<0$ such that $|f(x)|>R$ for all $x<L$ By $\lim_{x\rightarrow \infty}f(x)=-\infty$ we mean the following: Given $R<0$ there exists $L<0$ such that $f(x)<R$ for all $x<L$. Let me know if i understood somethings wrongly.,Every time i get confused with the definition of $\lim_{x\rightarrow \infty}f(x)=L$. I could not find a reference that will give the definition. I am trying to write what i understood. See if this is correct. By $\lim_{x\rightarrow \infty}f(x)=L$ we mean the following : Given $\epsilon >0$ there exists $R>0$ such that $|f(x)-L|<\epsilon$ for all $x>R$. By $\lim_{x\rightarrow -\infty}f(x)=L$ we mean the following : Given $\epsilon >0$ there exists $R<0$ such that $|f(x)-L|<\epsilon$ for all $x<R$. By $\lim_{x\rightarrow \infty}f(x)=\infty$ we mean the following: Given $R>0$ there exists $L>0$ such that $|f(x)|>R$ for all $x>L$ By $\lim_{x\rightarrow \infty}f(x)=-\infty$ we mean the following: Given $R<0$ there exists $L>0$ such that $f(x)<R$ for all $x>L$ By $\lim_{x\rightarrow -\infty}f(x)=\infty$ we mean the following: Given $R>0$ there exists $L<0$ such that $|f(x)|>R$ for all $x<L$ By $\lim_{x\rightarrow \infty}f(x)=-\infty$ we mean the following: Given $R<0$ there exists $L<0$ such that $f(x)<R$ for all $x<L$. Let me know if i understood somethings wrongly.,,"['real-analysis', 'limits']"
57,Use the definition of a limit to prove that $\lim_{y \to 0} y^3 = 0$.,Use the definition of a limit to prove that .,\lim_{y \to 0} y^3 = 0,"Attempt: The limit $\lim_{y \to 0} y^3 = 0$ exists if: $$\forall\ \epsilon >0 \ \exists\ \delta >0 \ \forall y \ |y-0|  < \delta \Longrightarrow |y^3 - 0|< \epsilon.$$ Now, I came up with the idea to use $\delta=\sqrt[3]{\epsilon}$. Would this be allowed and would it work?","Attempt: The limit $\lim_{y \to 0} y^3 = 0$ exists if: $$\forall\ \epsilon >0 \ \exists\ \delta >0 \ \forall y \ |y-0|  < \delta \Longrightarrow |y^3 - 0|< \epsilon.$$ Now, I came up with the idea to use $\delta=\sqrt[3]{\epsilon}$. Would this be allowed and would it work?",,"['limits', 'proof-verification']"
58,Prove that $\lim_{x \to 3} \sqrt{x+1} = 2$,Prove that,\lim_{x \to 3} \sqrt{x+1} = 2,"Prove that $\displaystyle\lim_{x \to 3} \sqrt{x+1} = 2$ Attempt: $0 < |x - 3| < \delta \Rightarrow |\sqrt{x+1} - 2| < \epsilon$ Well $|\sqrt{x+1} - 2| = |(\sqrt{x+1} - 2) \cdot \displaystyle\frac{\sqrt{x+1} + 2}{\sqrt{x+1}+2}| = |\frac{x-3}{\sqrt{x+1} + 2}| = |x-3| \cdot \frac{1}{|\sqrt{x+1}+2|}$ Here, the second term is the nuisance so maybe do something like the following: Suppose we want $|x-3| < 1$. Then, $2 < x < 4$. So $|x-3| \cdot \displaystyle\frac{1}{\sqrt{x+1}+2} < \frac{1}{\sqrt{3}+2} \cdot |x-3|$ Looks like we want $\delta = \min((\sqrt{3}+2)\epsilon, 1)$ Then $|\sqrt{x+1} - 2| = |x-3| \cdot \displaystyle\frac{1}{\sqrt{x+1}+2} < \frac{1}{\sqrt{3}+2} \cdot |x-3| < \frac{1}{\sqrt{3}+2} \cdot (\sqrt{3} + 2)\epsilon = \epsilon$","Prove that $\displaystyle\lim_{x \to 3} \sqrt{x+1} = 2$ Attempt: $0 < |x - 3| < \delta \Rightarrow |\sqrt{x+1} - 2| < \epsilon$ Well $|\sqrt{x+1} - 2| = |(\sqrt{x+1} - 2) \cdot \displaystyle\frac{\sqrt{x+1} + 2}{\sqrt{x+1}+2}| = |\frac{x-3}{\sqrt{x+1} + 2}| = |x-3| \cdot \frac{1}{|\sqrt{x+1}+2|}$ Here, the second term is the nuisance so maybe do something like the following: Suppose we want $|x-3| < 1$. Then, $2 < x < 4$. So $|x-3| \cdot \displaystyle\frac{1}{\sqrt{x+1}+2} < \frac{1}{\sqrt{3}+2} \cdot |x-3|$ Looks like we want $\delta = \min((\sqrt{3}+2)\epsilon, 1)$ Then $|\sqrt{x+1} - 2| = |x-3| \cdot \displaystyle\frac{1}{\sqrt{x+1}+2} < \frac{1}{\sqrt{3}+2} \cdot |x-3| < \frac{1}{\sqrt{3}+2} \cdot (\sqrt{3} + 2)\epsilon = \epsilon$",,"['calculus', 'limits', 'solution-verification']"
59,Computing the limit of an integral sequence,Computing the limit of an integral sequence,,"I've been trying for the last few hours to solve the following problem, which looks like this : Find the limit $l$ : $$l=\mathop {\lim }\limits_{n \to \infty } \,\,\,n\int\limits_0^n {\frac{{\arctan (\frac{x}{n})}}{{x(x^2  + 1)}}} \,dx$$ Use the result to compute: $$ \mathop {\lim }\limits_{n \to \infty } \,\,\,n\,(\,n\int\limits_0^n {\frac{{\arctan (\frac{x}{n})}}{{x(x^2  + 1)}}} \,dx - \frac{\pi }{2}) $$ I've tried Taylor expansion, partial fraction decomposition, but I can't really find anything useful. Some help would be really appreciated. I'm much more interested in the method than in the actual result.","I've been trying for the last few hours to solve the following problem, which looks like this : Find the limit $l$ : $$l=\mathop {\lim }\limits_{n \to \infty } \,\,\,n\int\limits_0^n {\frac{{\arctan (\frac{x}{n})}}{{x(x^2  + 1)}}} \,dx$$ Use the result to compute: $$ \mathop {\lim }\limits_{n \to \infty } \,\,\,n\,(\,n\int\limits_0^n {\frac{{\arctan (\frac{x}{n})}}{{x(x^2  + 1)}}} \,dx - \frac{\pi }{2}) $$ I've tried Taylor expansion, partial fraction decomposition, but I can't really find anything useful. Some help would be really appreciated. I'm much more interested in the method than in the actual result.",,"['calculus', 'limits', 'definite-integrals']"
60,"What's a concise word for ""the expression inside a limit""? Limitand?","What's a concise word for ""the expression inside a limit""? Limitand?",,"In $\sqrt {f}$, $f$ is the radicand. In $\sum g_i$, $g_2$ is a  summand. In $x \times y \times z$, $y$ is a multiplicand. In: $$\displaystyle \lim_{n \to +\infty} h_n(x)$$ or: $$h(x) \to \ell \quad \text {as} \quad x \to c$$ What's ""$h$"" called? The limitand?","In $\sqrt {f}$, $f$ is the radicand. In $\sum g_i$, $g_2$ is a  summand. In $x \times y \times z$, $y$ is a multiplicand. In: $$\displaystyle \lim_{n \to +\infty} h_n(x)$$ or: $$h(x) \to \ell \quad \text {as} \quad x \to c$$ What's ""$h$"" called? The limitand?",,"['limits', 'terminology']"
61,"Taking limits, legal move?","Taking limits, legal move?",,"I'm reading the following article, but there was one line where I wasn't quite sure if it was allowed or not. It's where they took the limit as $n\rightarrow \infty$ Now, they got $\lim_{n\rightarrow \infty} \left [ (2n+1)\sin \frac{x}{2n+1} \right  ]=x$ I have some issue with this because $x$ and $n$ are related by $x=(2n+1)w$, so wouldn't making $n\rightarrow \infty$ also affect $x$? Can we simply 'pause the $x$' and then take the limit in terms of $n$ purely, as they did?","I'm reading the following article, but there was one line where I wasn't quite sure if it was allowed or not. It's where they took the limit as $n\rightarrow \infty$ Now, they got $\lim_{n\rightarrow \infty} \left [ (2n+1)\sin \frac{x}{2n+1} \right  ]=x$ I have some issue with this because $x$ and $n$ are related by $x=(2n+1)w$, so wouldn't making $n\rightarrow \infty$ also affect $x$? Can we simply 'pause the $x$' and then take the limit in terms of $n$ purely, as they did?",,['limits']
62,Is there an example of using L'Hospital's Rule on a product where it doesn't work?,Is there an example of using L'Hospital's Rule on a product where it doesn't work?,,"I was reading that, when trying to solve something like: $$\lim_{x\to\infty} f(x)g(x)$$ I can rewrite is as: $$\lim_{x\to\infty} \frac{f(x)}{\frac{1}{g(x)}}$$ and use L'Hospital's Rule to solve. And, if this doesn't work, I can try using the other function as the denominator: $$\lim_{x\to\infty} \frac{g(x)}{\frac{1}{f(x)}}$$ So I wondered: are there well-known quotients of functions that don't work in either case and, if so, how do I then solve them? An example that doesn't submit to this process is: $$\lim_{x\to\infty} x.x$$ But obviously L'Hospital's Rule would not be necessary in this case.","I was reading that, when trying to solve something like: $$\lim_{x\to\infty} f(x)g(x)$$ I can rewrite is as: $$\lim_{x\to\infty} \frac{f(x)}{\frac{1}{g(x)}}$$ and use L'Hospital's Rule to solve. And, if this doesn't work, I can try using the other function as the denominator: $$\lim_{x\to\infty} \frac{g(x)}{\frac{1}{f(x)}}$$ So I wondered: are there well-known quotients of functions that don't work in either case and, if so, how do I then solve them? An example that doesn't submit to this process is: $$\lim_{x\to\infty} x.x$$ But obviously L'Hospital's Rule would not be necessary in this case.",,"['calculus', 'limits']"
63,"Limit involving $(\sin x) /x -\cos x $ and $(e^{2x}-1)/(2x)$, without l'Hôpital","Limit involving  and , without l'Hôpital",(\sin x) /x -\cos x  (e^{2x}-1)/(2x),"Find: $$\lim_{x\to 0}\ \frac{\dfrac{\sin x}{x} - \cos x}{2x \left(\dfrac{e^{2x} - 1}{2x} - 1 \right)}$$ I have factorized it in this manner in an attempt to use the formulae. I have tried to use that for $x$ tending to $0$, $\dfrac{\sin x}{x} = 1$ and that $\dfrac{e^x - 1}x$ is also $1$.","Find: $$\lim_{x\to 0}\ \frac{\dfrac{\sin x}{x} - \cos x}{2x \left(\dfrac{e^{2x} - 1}{2x} - 1 \right)}$$ I have factorized it in this manner in an attempt to use the formulae. I have tried to use that for $x$ tending to $0$, $\dfrac{\sin x}{x} = 1$ and that $\dfrac{e^x - 1}x$ is also $1$.",,"['calculus', 'limits', 'trigonometry', 'exponential-function', 'limits-without-lhopital']"
64,Prove if $\lim_{x\to +\infty}(f(x)+f'(x))=0$ then $\lim_{x\to +\infty} f(x)=0$ [duplicate],Prove if  then  [duplicate],\lim_{x\to +\infty}(f(x)+f'(x))=0 \lim_{x\to +\infty} f(x)=0,This question already has answers here : Show that $\lim_{x \to +\infty}\left(f(x)+f'(x)\right)=0 \Rightarrow \lim_{x \to +\infty} f(x)=0$ (5 answers) Closed 4 years ago . Let $f$ be a real function  continuously differentiable at $\Bbb R$ such that $$\lim_{x\to +\infty}(f(x)+f'(x))=0$$ prove that $$\lim_{x\to +\infty} f(x)=0$$ I tried tu use exponential function knowing that $$\frac{d}{dx}f(x)e^x=(f(x)+f'(x))e^x$$ but I got nothing. thanks in advance for an answer or un idea,This question already has answers here : Show that $\lim_{x \to +\infty}\left(f(x)+f'(x)\right)=0 \Rightarrow \lim_{x \to +\infty} f(x)=0$ (5 answers) Closed 4 years ago . Let be a real function  continuously differentiable at such that prove that I tried tu use exponential function knowing that but I got nothing. thanks in advance for an answer or un idea,f \Bbb R \lim_{x\to +\infty}(f(x)+f'(x))=0 \lim_{x\to +\infty} f(x)=0 \frac{d}{dx}f(x)e^x=(f(x)+f'(x))e^x,"['real-analysis', 'limits']"
65,Evaluation of limit of integrals where calculus tricks don't work,Evaluation of limit of integrals where calculus tricks don't work,,"Suppose that we want to evaluate $\displaystyle \lim_{n \rightarrow \infty} \int_0^1 \frac{n x^n}{1+x^3} \, dx$ . Clearly the standard limit theorems for integration (Lebesgue dominated convergence theorem, monotone convergence theorem, etc.) don't work here, as the answer is $1/2$ .  There's at least three ways to prove this converges to $1/2$ . $1) \ $ Set $u = x^n$ .  This converts the problem to $\displaystyle \lim_{n \rightarrow \infty} \int_0^1 \frac{u^\frac{1}{n}}{1+u^\frac{3}{n}} \, du$ and now using the bounded convergence theorem this is trivially $1/2$ $2) \ $ Integrating by parts, setting $dv = nx^n \, dx$ and $u = \frac{1}{1+x^3}$ .  Then $u(1) v(1) - u(0)v(0) = \frac{n}{2(n+1)}$ while $\displaystyle \lim_{n \rightarrow \infty} \int_0^1 v \, du = 0$ rather trivially $3) \ $ Avoiding any calculus tricks:  Fix small $\epsilon > 0$ and write $\displaystyle \int_0^1 \frac{n x^n}{1+x^3} \, dx = \int_0^{1-\epsilon} \frac{n x^n}{1+x^3} \, dx + \int_{1-\epsilon}^1 \frac{n x^n}{1+x^3} \, dx$ .  As $n \rightarrow \infty$ the first integral converges to zero by uniform convergence.  In the second one, we have $\frac12 \leq \frac{1}{1 + x^3} \leq \frac{1}{1 + (1-\epsilon)^3}$ while $\lim_{n \rightarrow \infty}\int_{1-\epsilon}^1 nx^n \, dx = 1$ . To make this precise, the easiest thing (in my opinion) is to play with $\liminf$ and $\limsup$ and then let $\epsilon \rightarrow 0^+$ , which (again in my opinion) makes performing $3)$ very useful pedagogical (as is breaking an integral up and estimating each piece, instead of using calculus). So my question is: are there similar examples as above (concrete examples where the standard limit theorems of integration don't apply) where you $\textit{can't}$ use calculus tricks to evaluate the limit, and where you $\textit{must}$ estimate like in $3)$ (or estimate in a more sophisticated manner?)","Suppose that we want to evaluate . Clearly the standard limit theorems for integration (Lebesgue dominated convergence theorem, monotone convergence theorem, etc.) don't work here, as the answer is .  There's at least three ways to prove this converges to . Set .  This converts the problem to and now using the bounded convergence theorem this is trivially Integrating by parts, setting and .  Then while rather trivially Avoiding any calculus tricks:  Fix small and write .  As the first integral converges to zero by uniform convergence.  In the second one, we have while . To make this precise, the easiest thing (in my opinion) is to play with and and then let , which (again in my opinion) makes performing very useful pedagogical (as is breaking an integral up and estimating each piece, instead of using calculus). So my question is: are there similar examples as above (concrete examples where the standard limit theorems of integration don't apply) where you use calculus tricks to evaluate the limit, and where you estimate like in (or estimate in a more sophisticated manner?)","\displaystyle \lim_{n \rightarrow \infty} \int_0^1 \frac{n x^n}{1+x^3} \, dx 1/2 1/2 1) \  u = x^n \displaystyle \lim_{n \rightarrow \infty} \int_0^1 \frac{u^\frac{1}{n}}{1+u^\frac{3}{n}} \, du 1/2 2) \  dv = nx^n \, dx u = \frac{1}{1+x^3} u(1) v(1) - u(0)v(0) = \frac{n}{2(n+1)} \displaystyle \lim_{n \rightarrow \infty} \int_0^1 v \, du = 0 3) \  \epsilon > 0 \displaystyle \int_0^1 \frac{n x^n}{1+x^3} \, dx = \int_0^{1-\epsilon} \frac{n x^n}{1+x^3} \, dx + \int_{1-\epsilon}^1 \frac{n x^n}{1+x^3} \, dx n \rightarrow \infty \frac12 \leq \frac{1}{1 + x^3} \leq \frac{1}{1 + (1-\epsilon)^3} \lim_{n \rightarrow \infty}\int_{1-\epsilon}^1 nx^n \, dx = 1 \liminf \limsup \epsilon \rightarrow 0^+ 3) \textit{can't} \textit{must} 3)","['real-analysis', 'limits', 'definite-integrals', 'lebesgue-integral']"
66,How many pairs of integers have a fixed GCD?,How many pairs of integers have a fixed GCD?,,"Let $k \ge 0, a \ge 0$ be a fixed integer and $f(x,k, a)$ be the number of positive integers pairs $(m,n), m < n \le x$ such that $\gcd(km-n, m + kn) = a$ . Question : Is there a closed form of expression $\lim_{x \to \infty}\dfrac{f(x,k,a)}{x^2}$ in terms of $a$ and $k$ ? My experimental data suggest that a closed form exists. $$ f(x,0,1) \sim 3 \pi^2 x^2 \\ f(x,1,1) \sim 2 \pi^2 x^2 \\ f(x,2,1) \sim \frac{5}{2} \pi^2 x^2 \\ f(x,3,1) \sim \frac{5}{3} \pi^2 x^2 \\ f(x,4,1) \sim \frac{17}{6} \pi^2 x^2 \\ f(x,5,1) \sim \frac{13}{7} \pi^2 x^2 \\ f(x,2,3) \sim \frac{5}{18} \pi^2 x^2 \\ f(x,1,5) \sim \frac{2}{25} \pi^2 x^2 $$ Code s = 2 j = 0 a = 3 test = 2  target = set = 10^2 while True:     r = 1     while r < s:         if gcd(a*s - r,s + a*r) == test:              j = j + 1         r = r + 1     if s > target:         target = target + set         print s, j, (pi^2*j/s^2).n()     s = s + 1","Let be a fixed integer and be the number of positive integers pairs such that . Question : Is there a closed form of expression in terms of and ? My experimental data suggest that a closed form exists. Code s = 2 j = 0 a = 3 test = 2  target = set = 10^2 while True:     r = 1     while r < s:         if gcd(a*s - r,s + a*r) == test:              j = j + 1         r = r + 1     if s > target:         target = target + set         print s, j, (pi^2*j/s^2).n()     s = s + 1","k \ge 0, a \ge 0 f(x,k, a) (m,n), m < n \le x \gcd(km-n, m + kn) = a \lim_{x \to \infty}\dfrac{f(x,k,a)}{x^2} a k 
f(x,0,1) \sim 3 \pi^2 x^2
\\
f(x,1,1) \sim 2 \pi^2 x^2
\\
f(x,2,1) \sim \frac{5}{2} \pi^2 x^2
\\
f(x,3,1) \sim \frac{5}{3} \pi^2 x^2
\\
f(x,4,1) \sim \frac{17}{6} \pi^2 x^2
\\
f(x,5,1) \sim \frac{13}{7} \pi^2 x^2
\\
f(x,2,3) \sim \frac{5}{18} \pi^2 x^2
\\
f(x,1,5) \sim \frac{2}{25} \pi^2 x^2
","['number-theory', 'limits', 'elementary-number-theory', 'prime-numbers', 'divisibility']"
67,how is the signum function neither continuous nor discontinuous at $x=0$.,how is the signum function neither continuous nor discontinuous at .,x=0,"In my book, there is a sentence that says exactly this: ""The function $\mathrm{sgn}(x)= \dfrac{x}{|x|}$ is neither continuous nor discontinuous at $x=0$. How is this possible?"" It was easy for me to tell it is not continuous at $x=0$ as there is no limit existence due to left-right limit inequality, or simply because the graph is broken at $x=0$. But I can't understand the second part, which claims it is also not discontinuous. I've always thought that ""if it is not continuous, then discontinuous"", but apparently it seems to be wrong. How is this function not discontinuous? source: A complete course: calculus(8th edition)","In my book, there is a sentence that says exactly this: ""The function $\mathrm{sgn}(x)= \dfrac{x}{|x|}$ is neither continuous nor discontinuous at $x=0$. How is this possible?"" It was easy for me to tell it is not continuous at $x=0$ as there is no limit existence due to left-right limit inequality, or simply because the graph is broken at $x=0$. But I can't understand the second part, which claims it is also not discontinuous. I've always thought that ""if it is not continuous, then discontinuous"", but apparently it seems to be wrong. How is this function not discontinuous? source: A complete course: calculus(8th edition)",,['limits']
68,Solve $\lim_\limits{x\to 0}\frac {e^{3x}-1}{e^{x}-1} $,Solve,\lim_\limits{x\to 0}\frac {e^{3x}-1}{e^{x}-1} ,I have problem with  $$\lim_{x\to 0}\frac {e^{3x}-1}{e^{x}-1} $$ I have no idea what to do first.,I have problem with  $$\lim_{x\to 0}\frac {e^{3x}-1}{e^{x}-1} $$ I have no idea what to do first.,,['limits']
69,What is $\lim_{n\to\infty} \left(\sum_{k=1}^n \frac1k\right) / \left(\sum_{k=0}^n \frac1{2k+1}\right)$?,What is ?,\lim_{n\to\infty} \left(\sum_{k=1}^n \frac1k\right) / \left(\sum_{k=0}^n \frac1{2k+1}\right),"I have the following problem: Evaluate $$ \lim_{n\to\infty}{{1+\frac12+\frac13 +\frac14+\ldots+\frac1n}\over{1+\frac13 +\frac15+\frac17+\ldots+\frac1{2n+1}}} $$ I tried making it into two sums, and tried to make it somehow into an integral, but couldn't find an integral. The sums I came up with, $$ \lim_{n\to\infty} { \sum_{k=1}^n {\frac1k} \over {\sum_{k=0}^n {\frac{1}{2k+1}}}} $$","I have the following problem: Evaluate I tried making it into two sums, and tried to make it somehow into an integral, but couldn't find an integral. The sums I came up with,", \lim_{n\to\infty}{{1+\frac12+\frac13 +\frac14+\ldots+\frac1n}\over{1+\frac13 +\frac15+\frac17+\ldots+\frac1{2n+1}}}   \lim_{n\to\infty} { \sum_{k=1}^n {\frac1k} \over {\sum_{k=0}^n {\frac{1}{2k+1}}}} ,"['real-analysis', 'limits', 'summation']"
70,Limit of $n!/n^n$ as $n$ tends to infinity [duplicate],Limit of  as  tends to infinity [duplicate],n!/n^n n,"This question already has answers here : What's the limit of the sequence $\lim\limits_{n \to\infty} \frac{n!}{n^n}$? (6 answers) Closed 8 years ago . Using a calculator, I found that $n!$ grows substantially slower than $n^n$ as $n$ tends to infinity. I guess the limit should be $0$. But I don't know how to prove it. In my textbook a hint is given that: Set $a_n=n!/n^n$ Set $m=[n/2]$(floor function), then $a_n \le (1/2)^m\le(1/2)^{n/2}$. Then by comparing to the geometric progression, the sequence $a_n$ tends to $0$. I have trouble proving the relationship $a_n \le (1/2)^m\le(1/2)^{n/2}$, (I tried to prove by considering separate cases, that is when $m$ is odd and when it is even) using induction gets me nowhere. Or is there other way to prove this limit? I made some search on web and used Stirling's approximation, but to no avail. P/S: Although some said that my question is probably duplicate, the main point in my question is understanding and proving the relationship of the inequalities, which I had trouble understanding and was not addressed in the other suggested question(the sequence $(1/n)$ was used as comparison instead of $(1/2)^{n/2}$.)","This question already has answers here : What's the limit of the sequence $\lim\limits_{n \to\infty} \frac{n!}{n^n}$? (6 answers) Closed 8 years ago . Using a calculator, I found that $n!$ grows substantially slower than $n^n$ as $n$ tends to infinity. I guess the limit should be $0$. But I don't know how to prove it. In my textbook a hint is given that: Set $a_n=n!/n^n$ Set $m=[n/2]$(floor function), then $a_n \le (1/2)^m\le(1/2)^{n/2}$. Then by comparing to the geometric progression, the sequence $a_n$ tends to $0$. I have trouble proving the relationship $a_n \le (1/2)^m\le(1/2)^{n/2}$, (I tried to prove by considering separate cases, that is when $m$ is odd and when it is even) using induction gets me nowhere. Or is there other way to prove this limit? I made some search on web and used Stirling's approximation, but to no avail. P/S: Although some said that my question is probably duplicate, the main point in my question is understanding and proving the relationship of the inequalities, which I had trouble understanding and was not addressed in the other suggested question(the sequence $(1/n)$ was used as comparison instead of $(1/2)^{n/2}$.)",,"['calculus', 'limits']"
71,Find: $\lim_{x\to\infty} \frac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x}}}}.$,Find:,\lim_{x\to\infty} \frac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x}}}}.,"Find: $\displaystyle\lim_{x\to\infty} \dfrac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x}}}}.$ Question from a book on preparation for math contests. All the tricks I know to solve this limit are not working. Wolfram Alpha struggled to find $1$ as the solution, but the solution process presented is not understandable. The answer is $1$. Hints and solutions are appreciated. Sorry if this is a duplicate.","Find: $\displaystyle\lim_{x\to\infty} \dfrac{\sqrt{x}}{\sqrt{x+\sqrt{x+\sqrt{x}}}}.$ Question from a book on preparation for math contests. All the tricks I know to solve this limit are not working. Wolfram Alpha struggled to find $1$ as the solution, but the solution process presented is not understandable. The answer is $1$. Hints and solutions are appreciated. Sorry if this is a duplicate.",,"['limits', 'contest-math', 'radicals', 'nested-radicals']"
72,Why is $\lim\limits_{n \to \infty} \frac{1}{n} \sqrt[n] {n^n}=1$ where $\lim\limits_{n \to \infty} \sqrt[n] {n!}=\infty$?,Why is  where ?,\lim\limits_{n \to \infty} \frac{1}{n} \sqrt[n] {n^n}=1 \lim\limits_{n \to \infty} \sqrt[n] {n!}=\infty,"Why is $\lim\limits_{n \to \infty}\frac{1}{n} \sqrt[n] {n^n}=1$ where $\lim\limits_{n \to \infty} \sqrt[n] {n!}=\infty$ ? We all know that $n^n > n! \ : \forall n$ so how come the factorial ""beats"" the exponent when it's nth rooted and going to infinity ? The factorial is behaving as if it's bigger than $n^n$.","Why is $\lim\limits_{n \to \infty}\frac{1}{n} \sqrt[n] {n^n}=1$ where $\lim\limits_{n \to \infty} \sqrt[n] {n!}=\infty$ ? We all know that $n^n > n! \ : \forall n$ so how come the factorial ""beats"" the exponent when it's nth rooted and going to infinity ? The factorial is behaving as if it's bigger than $n^n$.",,"['calculus', 'limits']"
73,Computing $\lim_{x \to 1}\frac{x^\frac{1}{5}-1}{x^\frac{1}{6} -1}$,Computing,\lim_{x \to 1}\frac{x^\frac{1}{5}-1}{x^\frac{1}{6} -1},I cannot figure out how to get around the zero numerator and denominator in order to compute the limit below: $$\lim_{x \to 1}\frac{\left(x^\frac{1}{5}\right)-1}{ \left( x^\frac{1}{6}\right) -1}$$ I tried: $$       \lim_{x \to 1}  \frac{   (x^\frac{1}{5} - 1) (x^\frac{1}{6}  + 1)        }{                  (x^\frac{1}{6} - 1) (x^\frac{1}{6}  + 1)        }     $$ $$\lim_{x \to 1}  \frac{   (x^\frac{1}{5} - 1) (x^\frac{1}{6}  + 1)        }{                  (x^\frac{2}{6} - 1)   } $$,I cannot figure out how to get around the zero numerator and denominator in order to compute the limit below: $$\lim_{x \to 1}\frac{\left(x^\frac{1}{5}\right)-1}{ \left( x^\frac{1}{6}\right) -1}$$ I tried: $$       \lim_{x \to 1}  \frac{   (x^\frac{1}{5} - 1) (x^\frac{1}{6}  + 1)        }{                  (x^\frac{1}{6} - 1) (x^\frac{1}{6}  + 1)        }     $$ $$\lim_{x \to 1}  \frac{   (x^\frac{1}{5} - 1) (x^\frac{1}{6}  + 1)        }{                  (x^\frac{2}{6} - 1)   } $$,,"['calculus', 'limits', 'radicals', 'factoring', 'fractions']"
74,Evaluate $\lim_{x\to49} \frac{x-49}{\sqrt{x}-7}$,Evaluate,\lim_{x\to49} \frac{x-49}{\sqrt{x}-7},Evaluate $\lim_{x\to 49} \frac{x-49}{\sqrt{x}-7}$ I'm guessing the answer is 7 but again that is only a guess. I don't know how to solve this type of problem. Please help.,Evaluate $\lim_{x\to 49} \frac{x-49}{\sqrt{x}-7}$ I'm guessing the answer is 7 but again that is only a guess. I don't know how to solve this type of problem. Please help.,,"['calculus', 'limits']"
75,Why is $\lim_\limits{x\to 0}\frac{\sin(6x)}{\sin(2x)} = \frac{6}{2}=3$?,Why is ?,\lim_\limits{x\to 0}\frac{\sin(6x)}{\sin(2x)} = \frac{6}{2}=3,"Why is $\lim_\limits{x\to 0}\frac{\sin(6x)}{\sin(2x)} = \frac{6}{2}=3$? The justification is that $\lim_\limits{x\to 0}\frac{\sin(x)}{x} = 1$ But, I am not seeing the connection. L'Hospital's rule? Is there a double angle substitution happening?","Why is $\lim_\limits{x\to 0}\frac{\sin(6x)}{\sin(2x)} = \frac{6}{2}=3$? The justification is that $\lim_\limits{x\to 0}\frac{\sin(x)}{x} = 1$ But, I am not seeing the connection. L'Hospital's rule? Is there a double angle substitution happening?",,"['calculus', 'limits']"
76,limit ${\lim_{x \to 49} \frac{\sqrt{x}-7}{x-49} }.$,limit,{\lim_{x \to 49} \frac{\sqrt{x}-7}{x-49} }.,"I have to find the limit $${\lim_{x \to 49} \frac{\sqrt{x}-7}{x-49} }.$$ I know that I cannot plug in $49$ because that would make the denominator $0$. I was told to rationalize the numerator and I did. This is what I did but I got the incorrect answer: $$\dfrac{\sqrt{x}-7}{x-49}\times\dfrac{\sqrt{x}+7}{\sqrt{x}+7}.$$ I multiplied this out and got $$\dfrac{x-49}{x\sqrt{x}+7x-49\sqrt{x}+343}.$$ Now when I plugged in $49$, the limit came out as $0$ but it was incorrect. Am I missing a step or did I do something wrong? I went over it a couple of times and I cannot catch my mistake.","I have to find the limit $${\lim_{x \to 49} \frac{\sqrt{x}-7}{x-49} }.$$ I know that I cannot plug in $49$ because that would make the denominator $0$. I was told to rationalize the numerator and I did. This is what I did but I got the incorrect answer: $$\dfrac{\sqrt{x}-7}{x-49}\times\dfrac{\sqrt{x}+7}{\sqrt{x}+7}.$$ I multiplied this out and got $$\dfrac{x-49}{x\sqrt{x}+7x-49\sqrt{x}+343}.$$ Now when I plugged in $49$, the limit came out as $0$ but it was incorrect. Am I missing a step or did I do something wrong? I went over it a couple of times and I cannot catch my mistake.",,['limits']
77,Why does $\lim_{n \to \infty} \sqrt{\frac{n}{n+1}} = 1$?,Why does ?,\lim_{n \to \infty} \sqrt{\frac{n}{n+1}} = 1,Why does $\displaystyle\lim_{n \to \infty}  \sqrt{\frac{n}{n+1}} = 1$? Shouldn't it be undetermined?,Why does $\displaystyle\lim_{n \to \infty}  \sqrt{\frac{n}{n+1}} = 1$? Shouldn't it be undetermined?,,"['calculus', 'limits']"
78,Limit calculation using derivative,Limit calculation using derivative,,"I encountered this exercise: Let $f(x)$ be a differentiable function, and suppose  that there exists some $a$ where $f'(a) \ne 0 $ . Calculate the limit: $$ \lim_{h\rightarrow0}\frac{f(a+3h)-f(a-2h)}{f(a-5h)-f(a-h)}. $$ I have no clue how I can solve this. I was trying to separate into two terms, and multiply and divide by $h$ , but it solves just the numerator limit. What can be done with the denominator limit?","I encountered this exercise: Let be a differentiable function, and suppose  that there exists some where . Calculate the limit: I have no clue how I can solve this. I was trying to separate into two terms, and multiply and divide by , but it solves just the numerator limit. What can be done with the denominator limit?",f(x) a f'(a) \ne 0   \lim_{h\rightarrow0}\frac{f(a+3h)-f(a-2h)}{f(a-5h)-f(a-h)}.  h,"['real-analysis', 'calculus', 'limits']"
79,A Tricky Limit: $\lim \limits_{x\rightarrow 0} \frac{9^x-5^x}{x}$ without L'Hospital,A Tricky Limit:  without L'Hospital,\lim \limits_{x\rightarrow 0} \frac{9^x-5^x}{x},"I'm teaching a recitation for a calculus 1 class this quarter and through some miscommunication I was under the impression that I needed to present a method to finding the limit of $$\lim_{x\rightarrow 0} \frac{9^x-5^x}{x}$$ without using L'Hospital's rule. I found rather quickly, much to my annoyance, that I was unable to find the limit without applying L'Hospital's rule. I asked several of my friends who were also unable to solve it. I was wondering if there was an elementary solution to such a limit, that is something understandable by a beginning calculus 1 student. Edit: To be more clear the students in my recitation have only just learned limits and haven't even reached derivatives yet.","I'm teaching a recitation for a calculus 1 class this quarter and through some miscommunication I was under the impression that I needed to present a method to finding the limit of $$\lim_{x\rightarrow 0} \frac{9^x-5^x}{x}$$ without using L'Hospital's rule. I found rather quickly, much to my annoyance, that I was unable to find the limit without applying L'Hospital's rule. I asked several of my friends who were also unable to solve it. I was wondering if there was an elementary solution to such a limit, that is something understandable by a beginning calculus 1 student. Edit: To be more clear the students in my recitation have only just learned limits and haven't even reached derivatives yet.",,"['calculus', 'limits', 'limits-without-lhopital']"
80,I need to figure out Why wouldn't a limit exist if we got the SAME value on each path?,I need to figure out Why wouldn't a limit exist if we got the SAME value on each path?,,"When talking about limits for functions of several variables, why isn’t it sufficient to say, $$\lim_{(x,y)\to(0,0)} f(x,y)=L$$ if $f(x,y)$ gets close to $L$ as we approach $(0,0)$ along the $x$-axis ($y = 0$) and along the $y$-axis ($x = 0$)?","When talking about limits for functions of several variables, why isn’t it sufficient to say, $$\lim_{(x,y)\to(0,0)} f(x,y)=L$$ if $f(x,y)$ gets close to $L$ as we approach $(0,0)$ along the $x$-axis ($y = 0$) and along the $y$-axis ($x = 0$)?",,"['calculus', 'limits', 'multivariable-calculus']"
81,Find the value of : $\lim_{x\to\infty}\frac{\sqrt{x+\sqrt{x+\sqrt{x}}}}{x}$,Find the value of :,\lim_{x\to\infty}\frac{\sqrt{x+\sqrt{x+\sqrt{x}}}}{x},"I saw  some resolutions here like $\sqrt{x+\sqrt{x+\sqrt{x}}}- \sqrt{x}$, but I couldn't get the point to find $\lim_{x\to\infty}\frac{\sqrt{x+\sqrt{x+\sqrt{x}}}}{x}$. I tried $\frac{1}{x}.(\sqrt{x+\sqrt{x+\sqrt{x}}})=\frac{\sqrt{x}}{x}\left(\sqrt{1+\frac{\sqrt{x+\sqrt{x}}}{x}} \right)=\frac{1}{\sqrt{x}}\left(\sqrt{1+\frac{\sqrt{x+\sqrt{x}}}{x}} \right)$ but now I got stuck. Could anyone help?","I saw  some resolutions here like $\sqrt{x+\sqrt{x+\sqrt{x}}}- \sqrt{x}$, but I couldn't get the point to find $\lim_{x\to\infty}\frac{\sqrt{x+\sqrt{x+\sqrt{x}}}}{x}$. I tried $\frac{1}{x}.(\sqrt{x+\sqrt{x+\sqrt{x}}})=\frac{\sqrt{x}}{x}\left(\sqrt{1+\frac{\sqrt{x+\sqrt{x}}}{x}} \right)=\frac{1}{\sqrt{x}}\left(\sqrt{1+\frac{\sqrt{x+\sqrt{x}}}{x}} \right)$ but now I got stuck. Could anyone help?",,"['calculus', 'limits', 'radicals', 'nested-radicals']"
82,Solve the limit $\lim\limits _{x\to 0}\frac{\sqrt{1-\cos\left(x^2\right)}}{1-\cos\left(x\right)}$,Solve the limit,\lim\limits _{x\to 0}\frac{\sqrt{1-\cos\left(x^2\right)}}{1-\cos\left(x\right)},$$\lim _{x\to \:0}\frac{\sqrt{1-\cos\left(x^2\right)}}{1-\cos\left(x\right)}=\left|\frac{0}{0}\right|$$ I think you have to multiply by the conjugate. And then make the change equivalent small. Right?,$$\lim _{x\to \:0}\frac{\sqrt{1-\cos\left(x^2\right)}}{1-\cos\left(x\right)}=\left|\frac{0}{0}\right|$$ I think you have to multiply by the conjugate. And then make the change equivalent small. Right?,,['limits']
83,Need help finding limit $\lim \limits_{x\to \infty}\left(\frac{x}{x-1}\right)^{2x+1}$,Need help finding limit,\lim \limits_{x\to \infty}\left(\frac{x}{x-1}\right)^{2x+1},Facing difficulty finding limit $$\lim \limits_{x\to \infty}\left(\frac{x}{x-1}\right)^{2x+1}$$ For starters I have trouble simplifying it Which method would help in finding this limit?,Facing difficulty finding limit $$\lim \limits_{x\to \infty}\left(\frac{x}{x-1}\right)^{2x+1}$$ For starters I have trouble simplifying it Which method would help in finding this limit?,,"['calculus', 'limits']"
84,Value of $\lim_{n \to \infty}\frac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n-1}}{n\sqrt{n}}$,Value of,\lim_{n \to \infty}\frac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n-1}}{n\sqrt{n}},"Evaluate $$\lim_{n \to \infty}\dfrac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n-1}}{n\sqrt{n}}.$$ I am trying to use the Sandwich principle here.. $\lim_{n \to \infty}\dfrac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n-1}}{n\sqrt{n}}=\lim_{n \to \infty}\dfrac{\sqrt{\dfrac{1}{n}}+\sqrt{\dfrac{2}{n}}+\sqrt{\dfrac{3}{n}}+\cdots+\sqrt{\dfrac{n-1}{n}}}{n}\ge \lim_{n \to \infty}\bigg(\sqrt{\dfrac{1}{n}}.\sqrt{\dfrac{2}{n}}.\sqrt{\dfrac{3}{n}}\cdots\sqrt{\dfrac{n-1}{n}}\bigg )^\dfrac{1}{n-1}=\lim_{n \to \infty}\bigg(\dfrac{(n-1)!}{n^n}\bigg )^\dfrac{1}{2(n-1)}$ But after this I am a little in doubt. This link may provide some light Evaluation of the limit $\lim\limits_{n \to \infty } \frac1{\sqrt n}\left(1 + \frac1{\sqrt 2 }+\frac1{\sqrt 3 }+\cdots+\frac1{\sqrt n } \right)$ but I do not understand how it would help my problem.. In continuation of @Rebello's answer here, I would like to provide an answer for the problem given in the link $\lim\limits_{n \to \infty } \frac1{\sqrt n}\left(1 + \frac1{\sqrt 2 }+\frac1{\sqrt 3 }+\cdots+\frac1{\sqrt n } \right)=\lim\limits_{n \to \infty }\sum_{k=1}^{n}{\dfrac{1}{\sqrt{kn}}}=\int_{0}^{1}\dfrac{1}{\sqrt{x}}dx+\lim\limits_{n \to \infty }\dfrac{1}{n}=2$","Evaluate I am trying to use the Sandwich principle here.. But after this I am a little in doubt. This link may provide some light Evaluation of the limit $\lim\limits_{n \to \infty } \frac1{\sqrt n}\left(1 + \frac1{\sqrt 2 }+\frac1{\sqrt 3 }+\cdots+\frac1{\sqrt n } \right)$ but I do not understand how it would help my problem.. In continuation of @Rebello's answer here, I would like to provide an answer for the problem given in the link",\lim_{n \to \infty}\dfrac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n-1}}{n\sqrt{n}}. \lim_{n \to \infty}\dfrac{\sqrt{1}+\sqrt{2}+\sqrt{3}+\cdots+\sqrt{n-1}}{n\sqrt{n}}=\lim_{n \to \infty}\dfrac{\sqrt{\dfrac{1}{n}}+\sqrt{\dfrac{2}{n}}+\sqrt{\dfrac{3}{n}}+\cdots+\sqrt{\dfrac{n-1}{n}}}{n}\ge \lim_{n \to \infty}\bigg(\sqrt{\dfrac{1}{n}}.\sqrt{\dfrac{2}{n}}.\sqrt{\dfrac{3}{n}}\cdots\sqrt{\dfrac{n-1}{n}}\bigg )^\dfrac{1}{n-1}=\lim_{n \to \infty}\bigg(\dfrac{(n-1)!}{n^n}\bigg )^\dfrac{1}{2(n-1)} \lim\limits_{n \to \infty } \frac1{\sqrt n}\left(1 + \frac1{\sqrt 2 }+\frac1{\sqrt 3 }+\cdots+\frac1{\sqrt n } \right)=\lim\limits_{n \to \infty }\sum_{k=1}^{n}{\dfrac{1}{\sqrt{kn}}}=\int_{0}^{1}\dfrac{1}{\sqrt{x}}dx+\lim\limits_{n \to \infty }\dfrac{1}{n}=2,"['calculus', 'limits']"
85,How to find $\lim _{x\to \infty \:}\left(\frac{x+3}{x+4}\right)^x$?,How to find ?,\lim _{x\to \infty \:}\left(\frac{x+3}{x+4}\right)^x,"I know the answer is $$ \lim _{x\to \infty \:}\ln\left(\frac{x+3}{x+4}\right)^x =\lim _{x\to \infty \:}x\cdot\ln\left(\frac{x+3}{x+4}\right) =\lim _{x\to \infty \:}\frac{\ln(x+3/x+4)}{1/x}=-1 $$ I know it has to do with L'Hopital rule, but I get the answer $$ \lim _{x\to \infty \:}\frac{(x+4)/(x+3)}{-1/x^2}, $$ which is wrong. Please help me.","I know the answer is I know it has to do with L'Hopital rule, but I get the answer which is wrong. Please help me.","
\lim _{x\to \infty \:}\ln\left(\frac{x+3}{x+4}\right)^x
=\lim _{x\to \infty \:}x\cdot\ln\left(\frac{x+3}{x+4}\right)
=\lim _{x\to \infty \:}\frac{\ln(x+3/x+4)}{1/x}=-1
 
\lim _{x\to \infty \:}\frac{(x+4)/(x+3)}{-1/x^2},
","['calculus', 'limits']"
86,"Limit evaluation: very tough question, cannot use L'hopitals rule","Limit evaluation: very tough question, cannot use L'hopitals rule",,"I found a very tough limits question online. The question asks you to evaluate the limit $$\lim_{x \to 0}\frac{(x+4)^\frac{3}{2}+e^{x}-9}{x}$$ without using L'Hôpitals rule. I tried to treat the top as a radical expression with the $e^x-9$ grouped and the other in root form to try to attempt rationalization. It did not work because you still get $\frac 0 0$. I tried a trick of double rationalization but that did not work, got back to the starting. Second attempt I tried to let $x=z-4$, a substitution, but it still did not lead to something that could remove a zero from the numerator. Then I tried to break this up into three fractions, by dividing $x$ into each term in the numerator, and I basically got $+\infty$, then can't do $e^x/x$ and then $-\infty$. So I have exhausted all the algebraic tricks I can think of. Anybody out there think they they can crack this one? Hope someone can. Sincerely, Palu","I found a very tough limits question online. The question asks you to evaluate the limit $$\lim_{x \to 0}\frac{(x+4)^\frac{3}{2}+e^{x}-9}{x}$$ without using L'Hôpitals rule. I tried to treat the top as a radical expression with the $e^x-9$ grouped and the other in root form to try to attempt rationalization. It did not work because you still get $\frac 0 0$. I tried a trick of double rationalization but that did not work, got back to the starting. Second attempt I tried to let $x=z-4$, a substitution, but it still did not lead to something that could remove a zero from the numerator. Then I tried to break this up into three fractions, by dividing $x$ into each term in the numerator, and I basically got $+\infty$, then can't do $e^x/x$ and then $-\infty$. So I have exhausted all the algebraic tricks I can think of. Anybody out there think they they can crack this one? Hope someone can. Sincerely, Palu",,"['calculus', 'limits', 'limits-without-lhopital']"
87,Evaluating the limit $\lim_{n \rightarrow +\infty} \frac{e^n+e^{-n}}{e^{n+1}+e^{-n-1}}$,Evaluating the limit,\lim_{n \rightarrow +\infty} \frac{e^n+e^{-n}}{e^{n+1}+e^{-n-1}},How would you solve the following limit? It's $\frac \infty \infty$ and L'Hospital doesn't seem to help: $$\lim_{n \rightarrow +\infty} \frac{e^n+e^{-n}}{e^{n+1}+e^{-n-1}}$$,How would you solve the following limit? It's $\frac \infty \infty$ and L'Hospital doesn't seem to help: $$\lim_{n \rightarrow +\infty} \frac{e^n+e^{-n}}{e^{n+1}+e^{-n-1}}$$,,"['calculus', 'limits']"
88,Any idea how to find $\lim_{x\to 0} \frac{\sqrt{1-\cos(x^2)}}{1-\cos(x)}$?,Any idea how to find ?,\lim_{x\to 0} \frac{\sqrt{1-\cos(x^2)}}{1-\cos(x)},"$$\lim_{x\to 0} \frac{\sqrt{1-\cos(x^2)}}{1-\cos(x)}$$ I am trying to solve this limit for 2 days, but still cant find the solution which is $\sqrt{2}$ (that's what is written in the solution sheet) I tried multiplying with the conjugate, tried with some identities but nothing much because of that $x^2$ in the $\cos$ . Then i tried L'Hopital because it is $\frac{0}{0}$ and still that $\cos$ in the square root is doing problems. I tried on symbolab  calculator but it  can't solve it. So can someone help me how to solve this? Thank you.","I am trying to solve this limit for 2 days, but still cant find the solution which is (that's what is written in the solution sheet) I tried multiplying with the conjugate, tried with some identities but nothing much because of that in the . Then i tried L'Hopital because it is and still that in the square root is doing problems. I tried on symbolab  calculator but it  can't solve it. So can someone help me how to solve this? Thank you.",\lim_{x\to 0} \frac{\sqrt{1-\cos(x^2)}}{1-\cos(x)} \sqrt{2} x^2 \cos \frac{0}{0} \cos,"['real-analysis', 'limits']"
89,"Show that $\lim\limits_{(x,y)\to(0,0)}\frac{x^3y-xy^3}{x^4+2y^4}$ does not exist.",Show that  does not exist.,"\lim\limits_{(x,y)\to(0,0)}\frac{x^3y-xy^3}{x^4+2y^4}","Show that $$\lim_{(x,y)\to(0,0)}\frac{x^3y-xy^3}{x^4+2y^4}$$ does not exist. I'm not even sure how to approach this. I tried factoring out $xy$ in the numerator to get $xy(x^2 - y^2)$ , but I don't think that gets me anywhere with the denominator.","Show that does not exist. I'm not even sure how to approach this. I tried factoring out in the numerator to get , but I don't think that gets me anywhere with the denominator.","\lim_{(x,y)\to(0,0)}\frac{x^3y-xy^3}{x^4+2y^4} xy xy(x^2 - y^2)","['calculus', 'limits', 'multivariable-calculus']"
90,Is there another way to compute $ \lim_{x \to 0^+} (\ln x)/{x^{1/9} } $?,Is there another way to compute ?, \lim_{x \to 0^+} (\ln x)/{x^{1/9} } ,Im trying to compute the limit $$ \lim_{x \to 0^+} \frac{ \ln x}{x^{1/9} } $$ Im trying l'hospitals it doesnt work. What trick do we need here?,Im trying to compute the limit $$ \lim_{x \to 0^+} \frac{ \ln x}{x^{1/9} } $$ Im trying l'hospitals it doesnt work. What trick do we need here?,,['calculus']
91,Evaluating $\lim_{h \to 0}\frac{(x+h)^{\frac15}-x^{\frac15}}{h}$,Evaluating,\lim_{h \to 0}\frac{(x+h)^{\frac15}-x^{\frac15}}{h},"The limit is: $$   \lim_{h \to 0}\frac{(x+h)^{\frac15}-x^{\frac15}}{h} $$ When I use calculator and substitute $h$ with $0.000001$ and $-0.000001$, the result is: $$   \frac{1}{5x^{\frac45}} $$ My question is: How to do it without calculator. Show me the steps on how it's being done.","The limit is: $$   \lim_{h \to 0}\frac{(x+h)^{\frac15}-x^{\frac15}}{h} $$ When I use calculator and substitute $h$ with $0.000001$ and $-0.000001$, the result is: $$   \frac{1}{5x^{\frac45}} $$ My question is: How to do it without calculator. Show me the steps on how it's being done.",,['limits']
92,Limit of $\lim_{x \to 0}\left (x\cdot \sin\left(\frac{1}{x}\right)\right)$ is $0$ or $1$?,Limit of  is  or ?,\lim_{x \to 0}\left (x\cdot \sin\left(\frac{1}{x}\right)\right) 0 1,WolframAlpha says $\lim_{x \to 0} x\sin\left(\dfrac{1}{x}\right)=0$ but I've found it $1$ as below: $$  \lim_{x \to 0} \left(x\sin\left(\dfrac{1}{x}\right)\right) =  \lim_{x \to 0} \left(\dfrac{1}{x}x\dfrac{\sin\left(\dfrac{1}{x}\right)}{\dfrac{1}{x}}\right)\\ =  \lim_{x \to 0} \dfrac{x}{x} \lim_{x \to 0} \dfrac{\sin\left(\dfrac{1}{x}\right)}{\dfrac{1}{x}}\\ = \lim_{x \to 0} 1  \\ = 1? $$ I wonder where I'm wrong...,WolframAlpha says $\lim_{x \to 0} x\sin\left(\dfrac{1}{x}\right)=0$ but I've found it $1$ as below: $$  \lim_{x \to 0} \left(x\sin\left(\dfrac{1}{x}\right)\right) =  \lim_{x \to 0} \left(\dfrac{1}{x}x\dfrac{\sin\left(\dfrac{1}{x}\right)}{\dfrac{1}{x}}\right)\\ =  \lim_{x \to 0} \dfrac{x}{x} \lim_{x \to 0} \dfrac{\sin\left(\dfrac{1}{x}\right)}{\dfrac{1}{x}}\\ = \lim_{x \to 0} 1  \\ = 1? $$ I wonder where I'm wrong...,,"['calculus', 'limits']"
93,Limit of $S_n$ as $n \to \infty$,Limit of  as,S_n n \to \infty,"Let $$S_n = \int_{0}^{1} \frac{nx^{n-1}}{1+x}dx$$ for $n >0$ . Then as $n \to \infty$ , the sequence $(S_n)_{n>0}$ tends to $0$ $1/2$ $1$ $+\infty$ $$S_n = \int_{0}^{1} \frac{nx^{n-1}}{1+x}dx$$ put $x= \tan^{2}t$ $dx = 2 \tan t \sec^{2} t dt$ so, $$S_n = \int_{0}^{\pi/4} \frac{n \tan^{2n-2}t}{1+\tan^{2} t} 2 \tan t \sec^{2}t dt$$ $\int_{0}^{\pi/4} 2n \tan^{2n-1}t dt$ $2n\int_{0}^{\pi/4} \tan^{2n-1}t dt$ I don't know how to proceed further to find the limit. Is there any other method to find the limit?","Let for . Then as , the sequence tends to put so, I don't know how to proceed further to find the limit. Is there any other method to find the limit?",S_n = \int_{0}^{1} \frac{nx^{n-1}}{1+x}dx n >0 n \to \infty (S_n)_{n>0} 0 1/2 1 +\infty S_n = \int_{0}^{1} \frac{nx^{n-1}}{1+x}dx x= \tan^{2}t dx = 2 \tan t \sec^{2} t dt S_n = \int_{0}^{\pi/4} \frac{n \tan^{2n-2}t}{1+\tan^{2} t} 2 \tan t \sec^{2}t dt \int_{0}^{\pi/4} 2n \tan^{2n-1}t dt 2n\int_{0}^{\pi/4} \tan^{2n-1}t dt,['limits']
94,limit of $\left(1+\frac{1}{n!}\right)^n$,limit of,\left(1+\frac{1}{n!}\right)^n,"I'm having trouble resolving the following limit: $$ \lim_{n \to \infty} \left(1+\frac{1}{n!}\right)^n $$ Intuituvely the limit is equal to 1, but the exercises requires me to resolve via calculation and I have no idea how I can accomplish this. Can someone please explain it to me?","I'm having trouble resolving the following limit: Intuituvely the limit is equal to 1, but the exercises requires me to resolve via calculation and I have no idea how I can accomplish this. Can someone please explain it to me?", \lim_{n \to \infty} \left(1+\frac{1}{n!}\right)^n ,"['limits', 'factorial']"
95,Proving Limit Rigorously,Proving Limit Rigorously,,"Find the limit $$\large \lim_{x\to \infty}(\ln x)^{\frac{20}x}$$ I understood that as x approached infinity, $20/x$ approached 0. This would mean that the limit would tend toward $1$ . However, $\ln x$ also approaches infinity as $x$ approaches infinity. Thus, I suspected the answer to be $1$ (and it indeed is the answer), however I feel like this answer is not sufficiently rigorous. How could I rigorously prove $1$ as the answer? Any ideas/hints would be appreciated. Note: I am a highschooler and my teachers often tell me to take these answers in faith. Thus, I may not understand any fancy notations that may usually be used in solving limit. Thanks!","Find the limit I understood that as x approached infinity, approached 0. This would mean that the limit would tend toward . However, also approaches infinity as approaches infinity. Thus, I suspected the answer to be (and it indeed is the answer), however I feel like this answer is not sufficiently rigorous. How could I rigorously prove as the answer? Any ideas/hints would be appreciated. Note: I am a highschooler and my teachers often tell me to take these answers in faith. Thus, I may not understand any fancy notations that may usually be used in solving limit. Thanks!",\large \lim_{x\to \infty}(\ln x)^{\frac{20}x} 20/x 1 \ln x x 1 1,"['calculus', 'limits']"
96,Limits involving factorials $\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}$,Limits involving factorials,\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}},"I am trying to calculate the following limit $$\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}$$ where $k$ can be any number between $0$ and $N$ . I thought of the following: If I take the logarithm of the expression then I get: $$\lim_{N\to\infty} \left(\log(N!)-\log((N-k)!)-k\log N\right)$$ Using the Stirling formula this can be approximated as: $$\lim_{N\to\infty} \left(N\log(N)-(N-k)\log(N-k)-k\log N\right)$$ Now there are two cases: If $k$ is $N$ , then the second term vanishes and the remaining terms cancel. If $k$ is smaller than $N$ , then I can drop the $k$ inside the second logarithm and all the terms cancel. So the limit $$\lim_{N\to\infty} \log\left(\frac{N!}{(N-k)!N^{k}}\right)=0$$ Which means that: $$\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}=1$$ I don't know if this is mathematically rigorous. Would like some help. Thanks","I am trying to calculate the following limit where can be any number between and . I thought of the following: If I take the logarithm of the expression then I get: Using the Stirling formula this can be approximated as: Now there are two cases: If is , then the second term vanishes and the remaining terms cancel. If is smaller than , then I can drop the inside the second logarithm and all the terms cancel. So the limit Which means that: I don't know if this is mathematically rigorous. Would like some help. Thanks",\lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}} k 0 N \lim_{N\to\infty} \left(\log(N!)-\log((N-k)!)-k\log N\right) \lim_{N\to\infty} \left(N\log(N)-(N-k)\log(N-k)-k\log N\right) k N k N k \lim_{N\to\infty} \log\left(\frac{N!}{(N-k)!N^{k}}\right)=0 \lim_{N\to\infty} \frac{N!}{(N-k)!N^{k}}=1,"['calculus', 'limits', 'factorial']"
97,Limit of $\frac{1}{7}e^{-2x^2}(1-4x^2)$ as $x\to\infty$,Limit of  as,\frac{1}{7}e^{-2x^2}(1-4x^2) x\to\infty,I calculated the derivative of $\frac{x}{7}*e^{-2x^2}$ and got $\frac{1}{7}e^{-2x^2}(1-4x^2)$ (I included it cause if I got that wrong calculating the rest is pointless) I don't know how to find the limit of this function: $$\frac{1}{7}e^{-2x^2}(1-4x^2)$$ I tried splitting it into two but I still don't know how to handle this $$\lim_{x \to \infty} \frac{1}{7}e^{-2x^2}+\lim_{x \to \infty} \frac{1}{7}xe^{-2x^2}*(-4x) $$,I calculated the derivative of and got (I included it cause if I got that wrong calculating the rest is pointless) I don't know how to find the limit of this function: I tried splitting it into two but I still don't know how to handle this,\frac{x}{7}*e^{-2x^2} \frac{1}{7}e^{-2x^2}(1-4x^2) \frac{1}{7}e^{-2x^2}(1-4x^2) \lim_{x \to \infty} \frac{1}{7}e^{-2x^2}+\lim_{x \to \infty} \frac{1}{7}xe^{-2x^2}*(-4x) ,"['limits', 'exponential-function']"
98,How to calculate $\lim_{x\to\infty}\frac{x}{x-\sin x}$?,How to calculate ?,\lim_{x\to\infty}\frac{x}{x-\sin x},"I tried to solve    $$ \lim_{x\to\infty}\frac{x}{x-\sin x}. $$ After dividing by $x$ I got that it equals to:  $$ \lim_{x\to\infty}\frac{1}{1-\frac{\sin x}{x}}. $$ Now, using L'hopital (0/0) I get that  $$ \lim_{x\to\infty}\frac{\sin x}{x} = \lim_{x\to\infty}\cos x $$  and the lim at infinity for $\cos x$ is not defined. So basically I get that the overall limit of  $$ \lim_{x\to\infty}\frac{x}{x-\sin x} $$ is $1$ or not defined?","I tried to solve    $$ \lim_{x\to\infty}\frac{x}{x-\sin x}. $$ After dividing by $x$ I got that it equals to:  $$ \lim_{x\to\infty}\frac{1}{1-\frac{\sin x}{x}}. $$ Now, using L'hopital (0/0) I get that  $$ \lim_{x\to\infty}\frac{\sin x}{x} = \lim_{x\to\infty}\cos x $$  and the lim at infinity for $\cos x$ is not defined. So basically I get that the overall limit of  $$ \lim_{x\to\infty}\frac{x}{x-\sin x} $$ is $1$ or not defined?",,"['calculus', 'limits']"
99,Find $\lim\limits_{x\to \infty} \frac{x\sin x}{1+x^2}$,Find,\lim\limits_{x\to \infty} \frac{x\sin x}{1+x^2},"$$\lim_{x\to \infty} \frac{x\sin x}{1+x^2}$$ Using L'hopital I get: $$\lim_{x\to \infty} \frac{x\cos x + \sin x}{2x}=\lim_{x \to \infty}\frac{\cos x}{2}$$ However, how is it possible to evaluate this limit?","$$\lim_{x\to \infty} \frac{x\sin x}{1+x^2}$$ Using L'hopital I get: $$\lim_{x\to \infty} \frac{x\cos x + \sin x}{2x}=\lim_{x \to \infty}\frac{\cos x}{2}$$ However, how is it possible to evaluate this limit?",,"['calculus', 'limits']"
