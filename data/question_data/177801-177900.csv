,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Verify proof that $\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } = -1$ [duplicate]",Verify proof that  [duplicate],"\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } = -1","This question already has an answer here : Evaluate $\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } $ (1 answer) Closed 5 years ago . I asked this question a while ago, but got no answer. Eventually I answered it myself, but I'm not completely sure whether my answer is correct or not. I don't know if it is correct to post another question about it, but I'd like my proof to be verified. So here it is: We want to show that $\forall \epsilon \gt 0 : \exists \delta \gt 0 : ||(x,y)||< \delta \implies |\frac{\ln(1-x-y)}{x+y}+1| \lt \epsilon$ . Fix $\epsilon \gt 0$ . We know that $\lim_{\phi \to 0} \frac{ln(1+\phi)}{-\phi} = -1$ . So there's $\delta_1 \gt 0$ such that $|\phi| \lt\delta_1 \implies |\frac{ln(1+\phi)}{-\phi}+1| \lt \epsilon$ . Let $\delta = \delta_1$ and $\xi = -x-y$ . Suppose $||(x,y)|| \lt \delta $ . Since all norms in $R^n$ are equivalent we can use the norm of the sum. Then: $||(x,y)|| = |x|+|y| \geq |x+y| = |\xi|$ . Since $|\xi| < \delta_1 $ , then $|\frac{ln(1+\xi)}{-\xi}+1| < \epsilon$ therefore $|\frac{ln(1-x-y)}{x+y}+1| < \epsilon  $ . So $\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } = -1$ Q.E.D.","This question already has an answer here : Evaluate $\lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } $ (1 answer) Closed 5 years ago . I asked this question a while ago, but got no answer. Eventually I answered it myself, but I'm not completely sure whether my answer is correct or not. I don't know if it is correct to post another question about it, but I'd like my proof to be verified. So here it is: We want to show that . Fix . We know that . So there's such that . Let and . Suppose . Since all norms in are equivalent we can use the norm of the sum. Then: . Since , then therefore . So Q.E.D.","\forall \epsilon \gt 0 : \exists \delta \gt 0 : ||(x,y)||< \delta \implies |\frac{\ln(1-x-y)}{x+y}+1| \lt \epsilon \epsilon \gt 0 \lim_{\phi \to 0} \frac{ln(1+\phi)}{-\phi} = -1 \delta_1 \gt 0 |\phi| \lt\delta_1 \implies |\frac{ln(1+\phi)}{-\phi}+1| \lt \epsilon \delta = \delta_1 \xi = -x-y ||(x,y)|| \lt \delta  R^n ||(x,y)|| = |x|+|y| \geq |x+y| = |\xi| |\xi| < \delta_1  |\frac{ln(1+\xi)}{-\xi}+1| < \epsilon |\frac{ln(1-x-y)}{x+y}+1| < \epsilon   \lim_{(x,y) \to (0,0), x+y \neq 0}{\frac{\ln(1-x-y)}{x+y} } = -1","['multivariable-calculus', 'proof-verification']"
1,"Continuity of $f$ at $(0,0)$ determined by different limits",Continuity of  at  determined by different limits,"f (0,0)","The function $f$ is defined as $$f(x, y) =\frac{x^2 y^2} {x^2 y^2 +|x-y|}$$ for $(x,y)\neq(0,0)$ and $0$ otherwise. We can clearly take two different paths $y=x$ and $y=-x$ which would give two different limits and thus $f$ is not continuous at $(0,0)$ .  How would introducing polar coordinates lead to such a result exactly?",The function is defined as for and otherwise. We can clearly take two different paths and which would give two different limits and thus is not continuous at .  How would introducing polar coordinates lead to such a result exactly?,"f f(x, y) =\frac{x^2 y^2} {x^2 y^2 +|x-y|} (x,y)\neq(0,0) 0 y=x y=-x f (0,0)",['multivariable-calculus']
2,Proving that the composition of a harmonic function and a Cauchy-Riemann mapping is harmonic,Proving that the composition of a harmonic function and a Cauchy-Riemann mapping is harmonic,,"Let $\mathcal{O}$ be an open subset of the plane $\mathbb{R}^{2}$ and   let the mapping $F : \mathcal{O} \rightarrow \mathbb{R}^{2}$ be    represented by $F(x, y) = (u(x, y), v(x, y))$ for $(x, y)$ in $\mathcal{O}$ . Then, we say the mapping $F : \mathcal{O} \rightarrow  \mathbb{R}^{2}$ is called a Cauchy-Riemann mapping provided that    each of the functions $u : \mathcal{O} \rightarrow \mathbb{R}$ and $v  : \mathcal{O} \rightarrow \mathbb{R}$ has continuous second-order    partial derivatives and $$\frac{\partial u}{\partial x}(x, y) =  \frac{\partial v}{\partial y}(x, y) \hspace{1em} \text{ and }  \hspace{1em} \frac{\partial u}{\partial y} = -\frac{\partial  v}{\partial x}(x, y)$$ for all $(x, y)$ in $\mathcal{O}$ . Prove that if $w : \mathbb{R}^{2} \rightarrow \mathbb{R}$ is harmonic and the mapping $F : \mathcal{O} \rightarrow \mathbb{R}^{2}$ is a Cauchy-Riemann mapping, then the function $w \circ F : \mathcal{O} \rightarrow \mathbb{R}$ is also harmonic. So I know that a function is harmonic provided that the sum of its second derivatives equals $0$ . This means we have $\nabla^2 w = 0$ . Also, we can write $F(x, y) = (u(x, y), v(x,y))$ . I don't really know how to prove this fact. We have the relations above in the definition of a Cauchy-Riemann mapping, but I'm not so sure about where to use them. I would appreciate some help.","Let be an open subset of the plane and   let the mapping be    represented by for in . Then, we say the mapping is called a Cauchy-Riemann mapping provided that    each of the functions and has continuous second-order    partial derivatives and for all in . Prove that if is harmonic and the mapping is a Cauchy-Riemann mapping, then the function is also harmonic. So I know that a function is harmonic provided that the sum of its second derivatives equals . This means we have . Also, we can write . I don't really know how to prove this fact. We have the relations above in the definition of a Cauchy-Riemann mapping, but I'm not so sure about where to use them. I would appreciate some help.","\mathcal{O} \mathbb{R}^{2} F : \mathcal{O} \rightarrow \mathbb{R}^{2} F(x, y) = (u(x, y), v(x, y)) (x, y) \mathcal{O} F : \mathcal{O} \rightarrow
 \mathbb{R}^{2} u : \mathcal{O} \rightarrow \mathbb{R} v
 : \mathcal{O} \rightarrow \mathbb{R} \frac{\partial u}{\partial x}(x, y) =
 \frac{\partial v}{\partial y}(x, y) \hspace{1em} \text{ and }
 \hspace{1em} \frac{\partial u}{\partial y} = -\frac{\partial
 v}{\partial x}(x, y) (x, y) \mathcal{O} w : \mathbb{R}^{2} \rightarrow \mathbb{R} F : \mathcal{O} \rightarrow \mathbb{R}^{2} w \circ F : \mathcal{O} \rightarrow \mathbb{R} 0 \nabla^2 w = 0 F(x, y) = (u(x, y), v(x,y))","['real-analysis', 'complex-analysis']"
3,Multivariable Limit - Converting to Polar Coordinates [duplicate],Multivariable Limit - Converting to Polar Coordinates [duplicate],,"This question already has answers here : Limit $\frac{x^2y}{x^4+y^2}$ is found using polar coordinates but it is not supposed to exist. (4 answers) Closed 4 years ago . I am new to this concept, but I do know that, using Cartesian coordinates,  if the limit is different for 2 different ""routes"", then it does not exist. I need to show that $$\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^4}$$ DNE by converting to polar. However, the result I've got is that the limit does exists, and it's equal to $0$ . Here's what I did: $$ x=r\cos\theta\\ y=r\sin\theta\\ $$ $$ \lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^4}= \lim_{r\to0}\frac{r\cos\theta (r\sin\theta)^2}{(r\cos\theta)^2+(r\sin\theta)^4} =\lim_{r\to0}\frac{r^3\cos\theta \sin^2\theta}{r^2\cos^2\theta+r^4\sin^4\theta} =\lim_{r\to0}\frac{r^\require{cancel}\cancel{3}\cos\theta \sin^2\theta}{\require{cancel}\cancel{r^2}(\cos^2\theta+r^2\sin^4\theta)} =\lim_{r\to0}\frac{r\cos\theta \sin^2\theta}{\cos^2\theta+r^2\sin^4\theta}=\frac{0}{\cos^2\theta}=0 $$ Apparently no dependency on $\theta$ ? I mean, it is possible that $\cos^2\theta=0$ . How do I continue from here? Am I missing something? Thank you very much.","This question already has answers here : Limit $\frac{x^2y}{x^4+y^2}$ is found using polar coordinates but it is not supposed to exist. (4 answers) Closed 4 years ago . I am new to this concept, but I do know that, using Cartesian coordinates,  if the limit is different for 2 different ""routes"", then it does not exist. I need to show that DNE by converting to polar. However, the result I've got is that the limit does exists, and it's equal to . Here's what I did: Apparently no dependency on ? I mean, it is possible that . How do I continue from here? Am I missing something? Thank you very much.","\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^4} 0 
x=r\cos\theta\\
y=r\sin\theta\\
 
\lim_{(x,y)\to(0,0)}\frac{xy^2}{x^2+y^4}=
\lim_{r\to0}\frac{r\cos\theta (r\sin\theta)^2}{(r\cos\theta)^2+(r\sin\theta)^4}
=\lim_{r\to0}\frac{r^3\cos\theta \sin^2\theta}{r^2\cos^2\theta+r^4\sin^4\theta}
=\lim_{r\to0}\frac{r^\require{cancel}\cancel{3}\cos\theta \sin^2\theta}{\require{cancel}\cancel{r^2}(\cos^2\theta+r^2\sin^4\theta)}
=\lim_{r\to0}\frac{r\cos\theta \sin^2\theta}{\cos^2\theta+r^2\sin^4\theta}=\frac{0}{\cos^2\theta}=0
 \theta \cos^2\theta=0","['calculus', 'limits', 'multivariable-calculus', 'polar-coordinates']"
4,$\mathbb{C}\mathbb{R}$-calculus for quadratic form,-calculus for quadratic form,\mathbb{C}\mathbb{R},"I've been working through this set of notes on differentials of $\mathbb{R}$ -valued functions of complex variables (and this MSE question), and I'm trying to work through a simple example. Do I have the following terminology and computations right, or have I messed up something? Let $A \in \mathbb{R}^{n \times n}$ with $A = A^{\top}$ . Define $z := r e^{j \theta}$ and $f(z) := \frac{1}{2} z^H A z$ . For the differential of $f$ with respect to $x$ for $x = (r,\theta)$ , we have operators $$ \frac{\partial}{\partial r}(\cdot) = \frac{\partial}{\partial z}\frac{\partial z}{\partial r}(\cdot) + \frac{\partial}{\partial \bar{z}}\frac{\partial \bar{z}}{\partial r}(\cdot) = e^{j\theta} \odot \frac{\partial}{\partial z}(\cdot) + e^{-j\theta} \odot \frac{\partial}{\partial \bar{z}}(\cdot) \tag{1} $$ and $$ \frac{\partial}{\partial \theta}(\cdot) = \frac{\partial}{\partial z}\frac{\partial z}{\partial \theta}(\cdot) + \frac{\partial}{\partial \bar{z}}\frac{\partial \bar{z}}{\partial \theta}(\cdot) = j r e^{j\theta} \odot \frac{\partial}{\partial z}(\cdot) - j r e^{-j\theta} \odot \frac{\partial}{\partial \bar{z}}(\cdot) \tag{2} $$ so that $$ \frac{\partial}{\partial r} f = e^{j\theta} \odot \frac{1}{2} z^H A + \frac{1}{2} e^{-j\theta} \odot Az \tag{3} $$ $$ \frac{\partial}{\partial r} f = \frac{1}{2} j z \odot \frac{1}{2} z^H A + \frac{1}{2} j z \odot Az \tag{4}. $$ Is it correct to say that the operators in (1) and (2) are nonlinear because $\frac{\partial}{\partial z}(\cdot)$ and $\frac{\partial}{\partial \bar{z}}(\cdot)$ are nonlinear (see eq. 20 and 51 here )? Is the $\mathbb{C}\mathbb{R}$ -Hessian w.r.t. $r$ and $\theta$ found by just applying (1) and (2) to the parts of (3) and (4) involving $z$ , and normal $\mathbb{R}$ -calculus to the parts involving $r$ and $\theta$ ?","I've been working through this set of notes on differentials of -valued functions of complex variables (and this MSE question), and I'm trying to work through a simple example. Do I have the following terminology and computations right, or have I messed up something? Let with . Define and . For the differential of with respect to for , we have operators and so that Is it correct to say that the operators in (1) and (2) are nonlinear because and are nonlinear (see eq. 20 and 51 here )? Is the -Hessian w.r.t. and found by just applying (1) and (2) to the parts of (3) and (4) involving , and normal -calculus to the parts involving and ?","\mathbb{R} A \in \mathbb{R}^{n \times n} A = A^{\top} z := r e^{j \theta} f(z) := \frac{1}{2} z^H A z f x x = (r,\theta) 
\frac{\partial}{\partial r}(\cdot) = \frac{\partial}{\partial z}\frac{\partial z}{\partial r}(\cdot) + \frac{\partial}{\partial \bar{z}}\frac{\partial \bar{z}}{\partial r}(\cdot) = e^{j\theta} \odot \frac{\partial}{\partial z}(\cdot) + e^{-j\theta} \odot \frac{\partial}{\partial \bar{z}}(\cdot) \tag{1}
 
\frac{\partial}{\partial \theta}(\cdot) = \frac{\partial}{\partial z}\frac{\partial z}{\partial \theta}(\cdot) + \frac{\partial}{\partial \bar{z}}\frac{\partial \bar{z}}{\partial \theta}(\cdot) = j r e^{j\theta} \odot \frac{\partial}{\partial z}(\cdot) - j r e^{-j\theta} \odot \frac{\partial}{\partial \bar{z}}(\cdot) \tag{2}
 
\frac{\partial}{\partial r} f = e^{j\theta} \odot \frac{1}{2} z^H A + \frac{1}{2} e^{-j\theta} \odot Az \tag{3}
 
\frac{\partial}{\partial r} f = \frac{1}{2} j z \odot \frac{1}{2} z^H A + \frac{1}{2} j z \odot Az \tag{4}.
 \frac{\partial}{\partial z}(\cdot) \frac{\partial}{\partial \bar{z}}(\cdot) \mathbb{C}\mathbb{R} r \theta z \mathbb{R} r \theta","['complex-analysis', 'multivariable-calculus', 'complex-numbers', 'vector-analysis', 'hessian-matrix']"
5,Integration by substitution in $f: \mathbb{R^2} \to \mathbb{R}$ with Jacobian method proof,Integration by substitution in  with Jacobian method proof,f: \mathbb{R^2} \to \mathbb{R},"I've searched everywhere on Google for a somewhat formal proof for the integration by substitution -- a.k.a $u$ -substitution or change of variables -- in double integral with the Jacobian determinant method but I can't find any. Anyone knows any sources for the proof? In case if the theorem is not familiar it goes something like: $$ \int\int_R f(x)d^2x=\int\int_Tf(G(u)) |\det DG|\, d^2u $$",I've searched everywhere on Google for a somewhat formal proof for the integration by substitution -- a.k.a -substitution or change of variables -- in double integral with the Jacobian determinant method but I can't find any. Anyone knows any sources for the proof? In case if the theorem is not familiar it goes something like:,"u 
\int\int_R f(x)d^2x=\int\int_Tf(G(u)) |\det DG|\, d^2u
","['integration', 'multivariable-calculus', 'change-of-variable']"
6,About change of coordinates of vector fields in smooth manifold theory,About change of coordinates of vector fields in smooth manifold theory,,"Background Suppose $M$ is a smooth $n$ -manifold and $(U,\varphi=(x^i))$ a smooth chart on $M$ . For each point $p\in U$ we know that $(\frac{\partial}{\partial x^i}|_p)_{i=1}^n$ is a basis for the tangent space $T_pM$ . If $(\tilde U,\tilde\varphi=(\tilde x^i))$ is another smooth chart on $M$ with $U \cap \tilde U\ne \emptyset$ , then for each point $p\in U \cap \tilde U$ we have the following formula for the basis - change: $$(\frac{\partial}{\partial x^1}|_p,\dots,\frac{\partial}{\partial x^n}|_p)=(\frac{\partial}{\partial \tilde x^1}|_p,\dots,\frac{\partial}{\partial \tilde x^n}|_p) \cdot \operatorname{Jac}(\tilde\varphi\circ\varphi^{-1})_{\varphi(p)} \quad[1]$$ where $\operatorname{Jac}(\tilde\varphi\circ\varphi^{-1})_{\varphi(p)}$ is the Jacobian matrix of $(\tilde\varphi\circ\varphi^{-1})$ in $\varphi(p)$ . Fact Suppose we have the following vector field on $\mathbb{R}^2$ : $X=2x \frac{\partial}{\partial x}$ . Let $f:\mathbb{R}^2 \to \mathbb{R}, (x,y)\mapsto x^2$ . We want to compute the coordinate expression for $X$ in polar coordinates (on some open subset on which they are defined) and show that it is not equal to $\frac{\partial f}{\partial r}\frac{\partial }{\partial r}+ \frac{\partial f}{\partial \theta}\frac{\partial }{\partial \theta}$ . My understanding and execution Let $M=\{(x,y)\in \mathbb{R}^2 : x>0\}$ , and $N=(0,+\infty)\times (-\frac{\pi}{2},\frac{\pi}{2})$ . Let $F=M\to N, (x,y)\mapsto (\sqrt {x^2+y^2},\arctan\frac{y}{x})$ . Then $F$ is a diffeomorphism with inverse $F^{-1}:N \to M, (r,\theta)\mapsto (r\cos\theta, r\sin\theta)$ . Issue 1: Polar coordinates are, by definition, the component functions of the map $F$ , right? So we have $F=(r, \theta)$ . Great Issue 2: I have an ambiguity on how to interpret $\frac{\partial}{\partial r}$ and $\frac{\partial}{\partial \theta}$ . (a) Should I interpret $(r,\theta)$ also as standard coordinates on $N$ and thus $\frac{\partial}{\partial r}$ and $\frac{\partial}{\partial \theta}$ are simply the ordinary partial derivatives on $N$ ? (Just like $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}$ on $M$ ?) (b) I know that if $M$ is a smooth $n$ -manifold and $(U,\varphi=(x^i))$ is a smooth chart on $M$ , then for each point $p\in U$ we have that $\frac{\partial}{\partial x^i}|_p=d\varphi^{-1}_{\phi(p)}(\partial_i|_{\phi(p)})$ where $\partial_i$ is the standard $i$ -th partial derivative on $\mathbb{R}^n$ . So thinking of my $F$ as a smooth chart, I should interpret $\frac{\partial}{\partial r}|_{(x,y)}=dF^{-1}_{F(x,y)}(\frac{\partial}{\partial x}|_{F(x,y)})$ for each $(x,y)\in M$ . Which one between (a) and (b) is the correct interpretation? Issue 3 The vector field I'm looking for, say $Y$ , should be the pushforward of $X$ by $F$ ? Issue 4 Applying formula [1] I have $$(\frac{\partial}{\partial x}|_{(x,y)},\frac{\partial}{\partial y}|_{(x,y)})=(\frac{\partial}{\partial r}|_{(x,y)},\frac{\partial}{\partial \theta}|_{(x,y)}) \cdot \operatorname{Jac}(F)_{(x,y)}$$ which i can rewrite $$(\frac{\partial}{\partial x}|_{(r\cos\theta,r\sin\theta)},\frac{\partial}{\partial y}|_{(r\cos\theta,r\sin\theta)})=(\frac{\partial}{\partial r}|_{(r\cos\theta,r\sin\theta)},\frac{\partial}{\partial \theta}|_{(r\cos\theta,r\sin\theta)}) \cdot \operatorname{Jac}(F)_{(r\cos\theta,r\sin\theta)},$$ from which I have $$\frac{\partial}{\partial x}|_{(r\cos\theta,r\sin\theta)}=\cos\theta \frac{\partial}{\partial r}|_{(r\cos\theta,r\sin\theta)}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta}|_{(r\cos\theta,r\sin\theta)};$$ finally we have $$X=2x\frac{\partial}{\partial x}=2r\cos\theta\frac{\partial}{\partial x}|_{(r\cos\theta,r\sin\theta)}=2r\cos^2\theta \frac{\partial}{\partial r}|_{(r\cos\theta,r\sin\theta)}-2\sin\theta\cos\theta\frac{\partial}{\partial \theta}|_{(r\cos\theta,r\sin\theta)}.$$ I think there is something wrong with this and whit using formula [1] in this context. Instead, using the formula for computing the pushforward of $X$ by $F$ , I got $$(F_*X)_{(r\,\theta)}=2r\cos^2\theta \frac{\partial}{\partial r}|_{(r,\theta)}-2\sin\theta\cos\theta\frac{\partial}{\partial \theta}|_{(r,\theta)}$$ which sounds correct. So is there some ""abuse of notation"" which is confusing me? Am I right in saying that strictly speaking I cannot use formula [1]? Please excuse me for the very long post, I have done the possible to make it clear.","Background Suppose is a smooth -manifold and a smooth chart on . For each point we know that is a basis for the tangent space . If is another smooth chart on with , then for each point we have the following formula for the basis - change: where is the Jacobian matrix of in . Fact Suppose we have the following vector field on : . Let . We want to compute the coordinate expression for in polar coordinates (on some open subset on which they are defined) and show that it is not equal to . My understanding and execution Let , and . Let . Then is a diffeomorphism with inverse . Issue 1: Polar coordinates are, by definition, the component functions of the map , right? So we have . Great Issue 2: I have an ambiguity on how to interpret and . (a) Should I interpret also as standard coordinates on and thus and are simply the ordinary partial derivatives on ? (Just like and on ?) (b) I know that if is a smooth -manifold and is a smooth chart on , then for each point we have that where is the standard -th partial derivative on . So thinking of my as a smooth chart, I should interpret for each . Which one between (a) and (b) is the correct interpretation? Issue 3 The vector field I'm looking for, say , should be the pushforward of by ? Issue 4 Applying formula [1] I have which i can rewrite from which I have finally we have I think there is something wrong with this and whit using formula [1] in this context. Instead, using the formula for computing the pushforward of by , I got which sounds correct. So is there some ""abuse of notation"" which is confusing me? Am I right in saying that strictly speaking I cannot use formula [1]? Please excuse me for the very long post, I have done the possible to make it clear.","M n (U,\varphi=(x^i)) M p\in U (\frac{\partial}{\partial x^i}|_p)_{i=1}^n T_pM (\tilde U,\tilde\varphi=(\tilde x^i)) M U \cap \tilde U\ne \emptyset p\in U \cap \tilde U (\frac{\partial}{\partial x^1}|_p,\dots,\frac{\partial}{\partial x^n}|_p)=(\frac{\partial}{\partial \tilde x^1}|_p,\dots,\frac{\partial}{\partial \tilde x^n}|_p) \cdot \operatorname{Jac}(\tilde\varphi\circ\varphi^{-1})_{\varphi(p)} \quad[1] \operatorname{Jac}(\tilde\varphi\circ\varphi^{-1})_{\varphi(p)} (\tilde\varphi\circ\varphi^{-1}) \varphi(p) \mathbb{R}^2 X=2x \frac{\partial}{\partial x} f:\mathbb{R}^2 \to \mathbb{R}, (x,y)\mapsto x^2 X \frac{\partial f}{\partial r}\frac{\partial }{\partial r}+ \frac{\partial f}{\partial \theta}\frac{\partial }{\partial \theta} M=\{(x,y)\in \mathbb{R}^2 : x>0\} N=(0,+\infty)\times (-\frac{\pi}{2},\frac{\pi}{2}) F=M\to N, (x,y)\mapsto (\sqrt {x^2+y^2},\arctan\frac{y}{x}) F F^{-1}:N \to M, (r,\theta)\mapsto (r\cos\theta, r\sin\theta) F F=(r, \theta) \frac{\partial}{\partial r} \frac{\partial}{\partial \theta} (r,\theta) N \frac{\partial}{\partial r} \frac{\partial}{\partial \theta} N \frac{\partial}{\partial x} \frac{\partial}{\partial y} M M n (U,\varphi=(x^i)) M p\in U \frac{\partial}{\partial x^i}|_p=d\varphi^{-1}_{\phi(p)}(\partial_i|_{\phi(p)}) \partial_i i \mathbb{R}^n F \frac{\partial}{\partial r}|_{(x,y)}=dF^{-1}_{F(x,y)}(\frac{\partial}{\partial x}|_{F(x,y)}) (x,y)\in M Y X F (\frac{\partial}{\partial x}|_{(x,y)},\frac{\partial}{\partial y}|_{(x,y)})=(\frac{\partial}{\partial r}|_{(x,y)},\frac{\partial}{\partial \theta}|_{(x,y)}) \cdot \operatorname{Jac}(F)_{(x,y)} (\frac{\partial}{\partial x}|_{(r\cos\theta,r\sin\theta)},\frac{\partial}{\partial y}|_{(r\cos\theta,r\sin\theta)})=(\frac{\partial}{\partial r}|_{(r\cos\theta,r\sin\theta)},\frac{\partial}{\partial \theta}|_{(r\cos\theta,r\sin\theta)}) \cdot \operatorname{Jac}(F)_{(r\cos\theta,r\sin\theta)}, \frac{\partial}{\partial x}|_{(r\cos\theta,r\sin\theta)}=\cos\theta \frac{\partial}{\partial r}|_{(r\cos\theta,r\sin\theta)}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta}|_{(r\cos\theta,r\sin\theta)}; X=2x\frac{\partial}{\partial x}=2r\cos\theta\frac{\partial}{\partial x}|_{(r\cos\theta,r\sin\theta)}=2r\cos^2\theta \frac{\partial}{\partial r}|_{(r\cos\theta,r\sin\theta)}-2\sin\theta\cos\theta\frac{\partial}{\partial \theta}|_{(r\cos\theta,r\sin\theta)}. X F (F_*X)_{(r\,\theta)}=2r\cos^2\theta \frac{\partial}{\partial r}|_{(r,\theta)}-2\sin\theta\cos\theta\frac{\partial}{\partial \theta}|_{(r,\theta)}","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
7,Converting between Solution forms using Green's Functions in Linear Differential Equation,Converting between Solution forms using Green's Functions in Linear Differential Equation,,"EDIT: Bounty is over tomorrow so I tried to clean up the question a bit, and put the additional work below as optional to read. I summarized the current results and the solution form I am trying to convert to. I know the solution from the book, but I am unsure how it was reached. I am nearly there, but am having trouble getting the correct form, since the final form looks so much different from the work I've done leading up to it. After a lot of hunting, I realized it's using Green's functions to show the answer, but I couldn't find anything helpful enough that was in 3 variables like my problem. So I need help formulating the solution in the proper form instead of the basic linear form. The basic idea of the system here is that there is a variable $x$ , and it changes, up and down, with $m$ the running minimum and $M$ the running maximum. Once the variable $x$ reaches a distance $L$ above $m$ OR a distance $L$ below $M$ , (for the first time) the value of the function $u$ is known. This basic idea is already imposed in the differential equation and its boundaries below. I mostly need help on figuring out how to formulate the Green's functions/integrals so the solution form looks the way it does below. Most of the work I have shown below is for solving the general system, so it can be skipped if it's not relevant for finding the solution form. I basically just solve for $a(m,M)$ and $b(m,M)$ in the linear solution form $$u(x,m,M) = a(m,M)x + b(m,M)$$ which may be helpful for the Green's functions but I am unsure. Thanks! Problem: $$ \begin{align} \partial_x^2u(x,m,M) = 0,  m < x < M \tag{1} \\ \partial_Mu(x,m,M)|_{x=M} = 0 \tag{2} \\ \partial_mu(x,m,M)|_{x=m} = 0 \tag{3} \\ u(m,m,L+m) = \Phi(m) \tag{4} \\u(M,M-L,M) = \Phi(M) \tag{5} \end{align}$$ Solution: $$u(x,m,M) = L^{-1}((M-x)\Phi(M-L) + (x-m)\Phi(m+L)) + L^{-2}{\huge(}\int_{M-L}^m(x-y)\Phi(y)dy + \int_M^{m+L}(y-x)\Phi(y)dy{\huge)}$$ So far I have the equation $$u(x,m,M) = \frac{\Phi(L+m) - \Phi(m)}{L} * x + \frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}$$ but have no idea how to get this to the solution form. I am pretty sure this is a correct equation, but if you have doubts you can check my work below. Equating the two answers, I need to find a way to show the following equality: $$(m+L - x)\Phi(m) = \\(M-x)\Phi(M-L) + L^{-1}{\huge(}\int_{M-L}^m(x-y)\Phi(y)dy + \int_M^{m+L}(y-x)\Phi(y)dy{\huge)}$$ but I am unsure if it is just better to work with the original equations. ADDITIONAL WORK BUT NOT NECESSARY Work so far (mine plus some steps in the book): Using $(1)$ , the solution form is linear in $x$ , so: $$u(x,m,M) = a(m,M)x + b(m,M) \tag{6}$$ Using $(6)$ to rewrite $(4)$ and $(5)$ : $$u(m,m,L+m) = \Phi(m) = a(m,L+m)m + b(m,L+m) \tag{7}$$ $$u(M,M-L,M) = u(L+m,m,L+m) = \Phi(L+m) =\\ a(m,L+m)(L+m) + b(m,L+m) \tag{8} $$ Equation $(8)$ comes from the fact that if $x$ reaches a distance $L$ above $m$ (for the first time), then $M = L + m$ , and so $x = M = L +m$ . From $(7)$ and $(8)$ we calculate $a(m,L+m)$ and $b(m,L+m)$ , by subtracting $(7)$ from $(8)$ to solve for $a(m,L+m)$ and then substituting $a(m,L+m)$ into $(7)$ to solve for $b(m,L+m)$ : $$a(m,L+m) = \frac{\Phi(L+m) - \Phi(m)}{L} \tag{9} $$ $$b(m,L+m) = \frac{(L+m)\Phi(m) - m\Phi(L+m)}{L} \tag{10} $$ The book says that equations $(2)$ and $(3)$ can be integrated (from $L+m$ to $M$ ) and it can be shown that $\partial_{mM}b(m,M) = 0$ . And therefore the following equation is true $$\partial_mb(m,M) = \partial_mb(m,L+m) = \partial_m{\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} \tag{11}$$ And finally, (I think many steps are skipped), the book says that by integrating $(11)$ , the solution above is reached. So I think we are supposed to use $(11)$ to calculate $b(m,M)$ , so I did the following: $$\int_{M-L}^m\partial_{m}b(m,M)dm = b(m,M) - b(M-L,M) = \int_{M-L}^m\partial_m{\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} = \\ {\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} - {\huge(}\frac{(M)\Phi(M-L) - (M-L)\Phi(M)}{L}{\huge)}$$ And so solving for $b(m,M)$ , using (from $(5)$ ) that $b(M-L,M) = \Phi(M) - a(M-L,M)*M$ and from $(4)$ and $(5)$ that $a(M-L,M) = \frac{\Phi(M) - \Phi(M-L)}{L}$ , we have: $$b(m,M) = b(M-L,M) + {\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} - {\huge(}\frac{(M)\Phi(M-L) - (M-L)\Phi(M)}{L}{\huge)} = \frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}$$ So I have $b(m,M)$ solved for, but have no idea how to get from there to the correct solution for $u(x,m,M)$ since it seems like the solution uses the boundary conditions somehow instead of directly plugging in $b(m,M)$ . Any help would be greatly appreciated, apologies in advance for the ginormous question. Thanks!","EDIT: Bounty is over tomorrow so I tried to clean up the question a bit, and put the additional work below as optional to read. I summarized the current results and the solution form I am trying to convert to. I know the solution from the book, but I am unsure how it was reached. I am nearly there, but am having trouble getting the correct form, since the final form looks so much different from the work I've done leading up to it. After a lot of hunting, I realized it's using Green's functions to show the answer, but I couldn't find anything helpful enough that was in 3 variables like my problem. So I need help formulating the solution in the proper form instead of the basic linear form. The basic idea of the system here is that there is a variable , and it changes, up and down, with the running minimum and the running maximum. Once the variable reaches a distance above OR a distance below , (for the first time) the value of the function is known. This basic idea is already imposed in the differential equation and its boundaries below. I mostly need help on figuring out how to formulate the Green's functions/integrals so the solution form looks the way it does below. Most of the work I have shown below is for solving the general system, so it can be skipped if it's not relevant for finding the solution form. I basically just solve for and in the linear solution form which may be helpful for the Green's functions but I am unsure. Thanks! Problem: Solution: So far I have the equation but have no idea how to get this to the solution form. I am pretty sure this is a correct equation, but if you have doubts you can check my work below. Equating the two answers, I need to find a way to show the following equality: but I am unsure if it is just better to work with the original equations. ADDITIONAL WORK BUT NOT NECESSARY Work so far (mine plus some steps in the book): Using , the solution form is linear in , so: Using to rewrite and : Equation comes from the fact that if reaches a distance above (for the first time), then , and so . From and we calculate and , by subtracting from to solve for and then substituting into to solve for : The book says that equations and can be integrated (from to ) and it can be shown that . And therefore the following equation is true And finally, (I think many steps are skipped), the book says that by integrating , the solution above is reached. So I think we are supposed to use to calculate , so I did the following: And so solving for , using (from ) that and from and that , we have: So I have solved for, but have no idea how to get from there to the correct solution for since it seems like the solution uses the boundary conditions somehow instead of directly plugging in . Any help would be greatly appreciated, apologies in advance for the ginormous question. Thanks!","x m M x L m L M u a(m,M) b(m,M) u(x,m,M) = a(m,M)x + b(m,M) 
\begin{align}
\partial_x^2u(x,m,M) = 0,  m < x < M \tag{1}
\\ \partial_Mu(x,m,M)|_{x=M} = 0 \tag{2}
\\ \partial_mu(x,m,M)|_{x=m} = 0 \tag{3}
\\ u(m,m,L+m) = \Phi(m) \tag{4}
\\u(M,M-L,M) = \Phi(M) \tag{5}
\end{align} u(x,m,M) = L^{-1}((M-x)\Phi(M-L) + (x-m)\Phi(m+L)) + L^{-2}{\huge(}\int_{M-L}^m(x-y)\Phi(y)dy + \int_M^{m+L}(y-x)\Phi(y)dy{\huge)} u(x,m,M) = \frac{\Phi(L+m) - \Phi(m)}{L} * x + \frac{(L+m)\Phi(m) - m\Phi(L+m)}{L} (m+L - x)\Phi(m) = \\(M-x)\Phi(M-L) + L^{-1}{\huge(}\int_{M-L}^m(x-y)\Phi(y)dy + \int_M^{m+L}(y-x)\Phi(y)dy{\huge)} (1) x u(x,m,M) = a(m,M)x + b(m,M) \tag{6} (6) (4) (5) u(m,m,L+m) = \Phi(m) = a(m,L+m)m + b(m,L+m) \tag{7} u(M,M-L,M) = u(L+m,m,L+m) = \Phi(L+m) =\\ a(m,L+m)(L+m) + b(m,L+m) \tag{8}  (8) x L m M = L + m x = M = L +m (7) (8) a(m,L+m) b(m,L+m) (7) (8) a(m,L+m) a(m,L+m) (7) b(m,L+m) a(m,L+m) = \frac{\Phi(L+m) - \Phi(m)}{L} \tag{9}  b(m,L+m) = \frac{(L+m)\Phi(m) - m\Phi(L+m)}{L} \tag{10}  (2) (3) L+m M \partial_{mM}b(m,M) = 0 \partial_mb(m,M) = \partial_mb(m,L+m) = \partial_m{\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} \tag{11} (11) (11) b(m,M) \int_{M-L}^m\partial_{m}b(m,M)dm = b(m,M) - b(M-L,M) = \int_{M-L}^m\partial_m{\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} = \\ {\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} - {\huge(}\frac{(M)\Phi(M-L) - (M-L)\Phi(M)}{L}{\huge)} b(m,M) (5) b(M-L,M) = \Phi(M) - a(M-L,M)*M (4) (5) a(M-L,M) = \frac{\Phi(M) - \Phi(M-L)}{L} b(m,M) = b(M-L,M) + {\huge(}\frac{(L+m)\Phi(m) - m\Phi(L+m)}{L}{\huge)} - {\huge(}\frac{(M)\Phi(M-L) - (M-L)\Phi(M)}{L}{\huge)} = \frac{(L+m)\Phi(m) - m\Phi(L+m)}{L} b(m,M) u(x,m,M) b(m,M)","['multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'boundary-value-problem', 'greens-function']"
8,How do I parametrise the hyperbola $xy = y^2 -1$?,How do I parametrise the hyperbola ?,xy = y^2 -1,"This equation is actually the solution to the intersection of the two surfaces $z = x^2 - y^2$ , and $z = x^2 + xy - 1$ . I am to parametrise the solution curve, which is noted in the title. A first attempt is below: \begin{align} z &= x^2 - y^2 = x^2 + xy - 1 &\\  \implies x^2 - y^2 &=  x^2 + xy - 1 &\\ -y^2 &= xy - 1 &\\ xy &= y^2 - 1 \end{align} Now, to parametrise, i.e. $x = f(t)$ , $y = f(t)$ : \begin{align} xy &= y^2 - 1 &\\ x &= \frac{y^2 - 1}{y} &\\ &= y - \frac{1}{y} \end{align} Let $y = t \in \mathbb{R}\backslash0$ , and hence we have: \begin{cases} x = t - \frac{1}{t} \\[2ex] y = t \end{cases} Will that suffice? I think not; there must be something more.","This equation is actually the solution to the intersection of the two surfaces , and . I am to parametrise the solution curve, which is noted in the title. A first attempt is below: Now, to parametrise, i.e. , : Let , and hence we have: Will that suffice? I think not; there must be something more.","z = x^2 - y^2 z = x^2 + xy - 1 \begin{align}
z &= x^2 - y^2 = x^2 + xy - 1 &\\ 
\implies x^2 - y^2 &=  x^2 + xy - 1 &\\
-y^2 &= xy - 1 &\\
xy &= y^2 - 1
\end{align} x = f(t) y = f(t) \begin{align}
xy &= y^2 - 1 &\\
x &= \frac{y^2 - 1}{y} &\\
&= y - \frac{1}{y}
\end{align} y = t \in \mathbb{R}\backslash0 \begin{cases}
x = t - \frac{1}{t} \\[2ex]
y = t
\end{cases}","['multivariable-calculus', 'parametric', 'parametrization']"
9,Question about the Definition of Smoothness at a Point,Question about the Definition of Smoothness at a Point,,"Suppose $M$ and $N$ are smooth manifolds. Let $F:M \to N$ be any map. The definition of smoothness of $F$ is the following: Def: We say that $F$ is smooth if for each $p\in M$ there exist $(U,\phi)$ smooth chart for $M$ in $p$ and $(V,\psi)$ smooth chart for $N$ such that $F(U) \subseteq V$ and $\psi^{-1} \circ F \circ \phi:\phi(U)\to \psi(V)$ is smooth in $\phi(U)$ in the sense of Ordinary Calculus. However, if I want to give a definition of smoothness at a point , I see at least two possible ways to do so: Let $p\in M$ . Definition 1) I say that $F$ is smooth at $p$ if there exist $(U,\phi)$ smooth chart for $M$ in $p$ and $(V,\psi)$ smooth chart for $N$ such that $F(U) \subseteq V$ and $\psi^{-1} \circ F \circ \phi:\phi(U)\to \psi(V)$ is smooth in $\phi(U)$ in the sense of Ordinary Calculus. Definition 2) I say that $F$ is smooth at $p$ if there exist $(U,\phi)$ smooth chart for $M$ in $p$ and $(V,\psi)$ smooth chart for $N$ such that $F(U) \subseteq V$ and $\psi^{-1} \circ F \circ \phi:\phi(U)\to \psi(V)$ is smooth in $\phi(p)$ in the sense of Ordinary Calculus. Are they equivalent? If not, which one is the correct? I noticed that: Pros of Definition 1) 1) $F:M \to N$ is smooth (in M, according to the ""Def"" above) if and only if $F$ is smooth at $p$ for each $p$ in $M$ . Cons of Definition 1) 1) Smoothness of $F$ at $p$ implies smoothness of $F$ in an entire neihborhood of $p$ (which seems a too strong requirement) 2) In Ordinary Calculus the definition of smoothness at a point does not imply the smoothness in an entire neighborhoods of that point. Pros of Definition 2) 1) It seems more similar to the definition of smoothness at a point given in Ordinary Calculus Cons of Definitions 2) 1) I don't know if $F:M \to N$ is smooth (in M, according to the ""Def"" above) if and only if $F$ is smooth at $p$ for each $p$ in $M$ .","Suppose and are smooth manifolds. Let be any map. The definition of smoothness of is the following: Def: We say that is smooth if for each there exist smooth chart for in and smooth chart for such that and is smooth in in the sense of Ordinary Calculus. However, if I want to give a definition of smoothness at a point , I see at least two possible ways to do so: Let . Definition 1) I say that is smooth at if there exist smooth chart for in and smooth chart for such that and is smooth in in the sense of Ordinary Calculus. Definition 2) I say that is smooth at if there exist smooth chart for in and smooth chart for such that and is smooth in in the sense of Ordinary Calculus. Are they equivalent? If not, which one is the correct? I noticed that: Pros of Definition 1) 1) is smooth (in M, according to the ""Def"" above) if and only if is smooth at for each in . Cons of Definition 1) 1) Smoothness of at implies smoothness of in an entire neihborhood of (which seems a too strong requirement) 2) In Ordinary Calculus the definition of smoothness at a point does not imply the smoothness in an entire neighborhoods of that point. Pros of Definition 2) 1) It seems more similar to the definition of smoothness at a point given in Ordinary Calculus Cons of Definitions 2) 1) I don't know if is smooth (in M, according to the ""Def"" above) if and only if is smooth at for each in .","M N F:M \to N F F p\in M (U,\phi) M p (V,\psi) N F(U) \subseteq V \psi^{-1} \circ F \circ \phi:\phi(U)\to \psi(V) \phi(U) p\in M F p (U,\phi) M p (V,\psi) N F(U) \subseteq V \psi^{-1} \circ F \circ \phi:\phi(U)\to \psi(V) \phi(U) F p (U,\phi) M p (V,\psi) N F(U) \subseteq V \psi^{-1} \circ F \circ \phi:\phi(U)\to \psi(V) \phi(p) F:M \to N F p p M F p F p F:M \to N F p p M","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
10,A Topological Invariant for $\pi_3(U(n))$,A Topological Invariant for,\pi_3(U(n)),"Recently, I saw a construction of topological invariant for $\pi_3(U(n))$ with $n\geq 2$ : $$ N=\frac{1}{24\pi^2}\int_{S^3} d^3x\ \epsilon^{ijk} Tr[(U^{-1}\partial_{x_i}U)(U^{-1}\partial_{x_j}U)(U^{-1}\partial_{x_k}U)]\ , $$ where $U\in U(n)$ depends on $\boldsymbol{x}=(x_1,x_2,x_3)\in S^3$ , $\epsilon^{ijk}$ is the Levi-Civita symbol, $i,j,k=1,2,3$ , and the duplicated indexes are summed over. It is claimed that $N$ is an integer, but why? Update 02/02/2019 I think I got an argument for $n=2$ . In this case, $U=e^{i \varphi} q$ with $q\in SU(2)$ . Due to the trace and the Levi-Civita symbol in $N$ , $\varphi$ does not contribute to $N$ . As $Tr[q^{\dagger}\partial_i q]=0$ and $(q^{\dagger}\partial_i q)^{\dagger}=-q^{\dagger}\partial_i q$ , $q^{\dagger}\partial_i q$ in geneeral has the form $q^{\dagger}\partial_i q=i(A_i \sigma_x +B_i \sigma_y+C_i \sigma_z )$ with $A_i,B_i,C_i$ real and $\sigma$ 's being the Pauli matrices. As a result, we have $$ N=\frac{1}{2\pi^2}\int_{S^3} d^3x\ \epsilon^{ijk} A_i B_i C_i\ . $$ Furthermore, $SU(2)$ is diffeomorphic to $S^3$ , and thus $q$ can be parametrized as the three angles $\boldsymbol{\Omega}=(\psi,\theta,\phi)$ of $S^3$ . By choosing the right convention, $N$ can be further written as $$ N=\frac{1}{2\pi^2}\int_{S^3} d^3x \left|\frac{\partial\boldsymbol{\Omega}}{\partial\boldsymbol{x}}\right| \sin(\theta)\sin^2(\phi)\ , $$ where $\left|\frac{\partial\boldsymbol{\Omega}}{\partial\boldsymbol{x}}\right|$ is the determinant of the Jacobian matrix.  Clearly, $N$ indicates how many times the $\boldsymbol{x}$ - $S^3$ warps around the $\boldsymbol{\Omega}$ - $S^3$ . For $n>2$ , I may guess that some $SU(2)$ subgroup of $U(n)$ accounts for $N$ , but I can be wrong.","Recently, I saw a construction of topological invariant for with : where depends on , is the Levi-Civita symbol, , and the duplicated indexes are summed over. It is claimed that is an integer, but why? Update 02/02/2019 I think I got an argument for . In this case, with . Due to the trace and the Levi-Civita symbol in , does not contribute to . As and , in geneeral has the form with real and 's being the Pauli matrices. As a result, we have Furthermore, is diffeomorphic to , and thus can be parametrized as the three angles of . By choosing the right convention, can be further written as where is the determinant of the Jacobian matrix.  Clearly, indicates how many times the - warps around the - . For , I may guess that some subgroup of accounts for , but I can be wrong.","\pi_3(U(n)) n\geq 2 
N=\frac{1}{24\pi^2}\int_{S^3} d^3x\ \epsilon^{ijk} Tr[(U^{-1}\partial_{x_i}U)(U^{-1}\partial_{x_j}U)(U^{-1}\partial_{x_k}U)]\ ,
 U\in U(n) \boldsymbol{x}=(x_1,x_2,x_3)\in S^3 \epsilon^{ijk} i,j,k=1,2,3 N n=2 U=e^{i \varphi} q q\in SU(2) N \varphi N Tr[q^{\dagger}\partial_i q]=0 (q^{\dagger}\partial_i q)^{\dagger}=-q^{\dagger}\partial_i q q^{\dagger}\partial_i q q^{\dagger}\partial_i q=i(A_i \sigma_x +B_i \sigma_y+C_i \sigma_z ) A_i,B_i,C_i \sigma 
N=\frac{1}{2\pi^2}\int_{S^3} d^3x\ \epsilon^{ijk} A_i B_i C_i\ .
 SU(2) S^3 q \boldsymbol{\Omega}=(\psi,\theta,\phi) S^3 N 
N=\frac{1}{2\pi^2}\int_{S^3} d^3x \left|\frac{\partial\boldsymbol{\Omega}}{\partial\boldsymbol{x}}\right| \sin(\theta)\sin^2(\phi)\ ,
 \left|\frac{\partial\boldsymbol{\Omega}}{\partial\boldsymbol{x}}\right| N \boldsymbol{x} S^3 \boldsymbol{\Omega} S^3 n>2 SU(2) U(n) N","['multivariable-calculus', 'algebraic-topology', 'differential-topology', 'lie-groups', 'vector-bundles']"
11,Implicit function theorem implies inverse function theorem proof,Implicit function theorem implies inverse function theorem proof,,"I believe this will be a very long answer if anyone tries to write the full proof or anything so I'll specify which specific parts I am having trouble with to save people's time. I want prove three things: First we know, $f: U \to V$ where $U \subset \mathbb{R^n}$ and $V \subset \mathbb{R^n}$ . And let $a \in U$ , s.t. $Df(a)$ is invertible. (1) $f$ is one-to-one and onto (hence invertible) (2) The inverse function $f^{-1}$ is of class $C^1$ (3) If $x \in \mathbb{R^n}$ and $y=f(x)\in\mathbb{R^n}$ then $D(f^{-1})(y)=(Df(x))^{-1}$ . Proof: So I know that the (2) follows directly from one of the conclusions in the implicit function theorem. I am needing help in proving that $f$ is invertible. I know from implicit function theorem that there is a function $g$ such that $g(f(a))=a$ . My idea was to prove that $f(g(b))=b$ ? But I'm not sure if this will work and if this approach can work not sure how to make it work. If this works we know that $g=f^{-1}$ . Also, for (3) what I got to was $Dg(y)=Dg(f(x))Df(x)$ by chain rule. And since we know that $g=f^{-1}$ we can simplify the above equation to: $Dg(y)=DxDf(x)$ but not sure how to work with the $DxDf(x)$ to $(Df(x))^{-1}$ . Explicit hints will be greatly appreciate, thanks in advance.","I believe this will be a very long answer if anyone tries to write the full proof or anything so I'll specify which specific parts I am having trouble with to save people's time. I want prove three things: First we know, where and . And let , s.t. is invertible. (1) is one-to-one and onto (hence invertible) (2) The inverse function is of class (3) If and then . Proof: So I know that the (2) follows directly from one of the conclusions in the implicit function theorem. I am needing help in proving that is invertible. I know from implicit function theorem that there is a function such that . My idea was to prove that ? But I'm not sure if this will work and if this approach can work not sure how to make it work. If this works we know that . Also, for (3) what I got to was by chain rule. And since we know that we can simplify the above equation to: but not sure how to work with the to . Explicit hints will be greatly appreciate, thanks in advance.",f: U \to V U \subset \mathbb{R^n} V \subset \mathbb{R^n} a \in U Df(a) f f^{-1} C^1 x \in \mathbb{R^n} y=f(x)\in\mathbb{R^n} D(f^{-1})(y)=(Df(x))^{-1} f g g(f(a))=a f(g(b))=b g=f^{-1} Dg(y)=Dg(f(x))Df(x) g=f^{-1} Dg(y)=DxDf(x) DxDf(x) (Df(x))^{-1},"['multivariable-calculus', 'implicit-function-theorem', 'inverse-function-theorem']"
12,How can this multiple integral be evaluated?,How can this multiple integral be evaluated?,,"I am stuck trying to solve the following integral: $$\int_R (y+2x^2)(y-x^2) dA$$ where $R$ is defined by the following equations: $xy=1$ , $xy=2$ , $y=x^2$ , $y=x^2-1$ with $x$ and $y$ positives. I've tried several changes of variables for example: $u=xy$ , $v=y-x^2$ or $u=y-x^2$ , $v=x^2$ but I get stuck because for the Jacobian ot for the limits of integration I have to solve a third degree equation. I know that I could solve it using Cardano's formula but it has to be an easy way to do it. Thank you very much. Merry Christmas.","I am stuck trying to solve the following integral: where is defined by the following equations: , , , with and positives. I've tried several changes of variables for example: , or , but I get stuck because for the Jacobian ot for the limits of integration I have to solve a third degree equation. I know that I could solve it using Cardano's formula but it has to be an easy way to do it. Thank you very much. Merry Christmas.",\int_R (y+2x^2)(y-x^2) dA R xy=1 xy=2 y=x^2 y=x^2-1 x y u=xy v=y-x^2 u=y-x^2 v=x^2,"['integration', 'multivariable-calculus', 'change-of-variable']"
13,Integration by part on a surface,Integration by part on a surface,,"Let $f$ be a smooth positive function from $\mathbb{R}^3$ to $\mathbb{R}$ . Can we ""simplify"" the integral $$ \int_{\mathbb{S}^2} \dfrac{\Delta f(x)}{f(x)} d \sigma(x), $$ where $\Delta f$ is the three-dimensional Laplacian of $f$ . For instance, what nice conditions on $f$ ensure that this integral have a sign?","Let be a smooth positive function from to . Can we ""simplify"" the integral where is the three-dimensional Laplacian of . For instance, what nice conditions on ensure that this integral have a sign?","f \mathbb{R}^3 \mathbb{R} 
\int_{\mathbb{S}^2} \dfrac{\Delta f(x)}{f(x)} d \sigma(x),
 \Delta f f f","['integration', 'multivariable-calculus']"
14,Do multivariate Householder methods exist?,Do multivariate Householder methods exist?,,"Newton's method can be extended to higher-order versions using Householder's method. Newton's method can also be extended to the case of multivariate inputs, sometimes called the ""Newton-Raphson method."" Do the Householder's methods likewise yield such a generalization, using higher partial derivatives?","Newton's method can be extended to higher-order versions using Householder's method. Newton's method can also be extended to the case of multivariate inputs, sometimes called the ""Newton-Raphson method."" Do the Householder's methods likewise yield such a generalization, using higher partial derivatives?",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization', 'newton-raphson']"
15,vector field: changing vector magnitudes to make it conservative,vector field: changing vector magnitudes to make it conservative,,"Consider a vector field $$\vec{F}(x,y)=P(x,y)\vec{i}+Q(x,y)\vec{j}$$ on an open and simply-connected region. Assume $P$ and $Q$ have continuous partial derivatives. Under which conditions there exists a positive-valued function $\mu (x,y)$ such that $$\mu\vec{F}(x,y)=\mu(x,y)P(x,y)\vec{i}+\mu(x,y)Q(x,y)\vec{j}$$ is a conservative vector field? Thanks for the help!",Consider a vector field on an open and simply-connected region. Assume and have continuous partial derivatives. Under which conditions there exists a positive-valued function such that is a conservative vector field? Thanks for the help!,"\vec{F}(x,y)=P(x,y)\vec{i}+Q(x,y)\vec{j} P Q \mu (x,y) \mu\vec{F}(x,y)=\mu(x,y)P(x,y)\vec{i}+\mu(x,y)Q(x,y)\vec{j}","['multivariable-calculus', 'vector-analysis']"
16,Is there an English term for a function which is locally linearizable?,Is there an English term for a function which is locally linearizable?,,"In Italian we have the notion of derivabilità and differenziabilità . In one dimension they are equivalent; in more dimensions, instead: A function is derivabile if partial derivatives exist. A function is differenziabile if it is also localmente linearizzabile (locally linearizable), that is, if $$\lim_{(x,y)\to(x_0,y_0)}\frac{f(x,y)-f(x_0,y_0)-\partial_xf(x_0,y_0)(x-x_0)-\partial_yf(x_0,y_0)(y-y_0)}{\sqrt[]{(x-x_0)^2+(y-y_0)^2}}=0$$ Is there an English term which translates differenziabile , other than locally linearizable?","In Italian we have the notion of derivabilità and differenziabilità . In one dimension they are equivalent; in more dimensions, instead: A function is derivabile if partial derivatives exist. A function is differenziabile if it is also localmente linearizzabile (locally linearizable), that is, if Is there an English term which translates differenziabile , other than locally linearizable?","\lim_{(x,y)\to(x_0,y_0)}\frac{f(x,y)-f(x_0,y_0)-\partial_xf(x_0,y_0)(x-x_0)-\partial_yf(x_0,y_0)(y-y_0)}{\sqrt[]{(x-x_0)^2+(y-y_0)^2}}=0","['real-analysis', 'multivariable-calculus', 'terminology']"
17,Reference book on differential geometry,Reference book on differential geometry,,Which books should a physics major student read before undertaking a course in differential geometry ? Which books on differential geometry are most suitable for a first reading ?,Which books should a physics major student read before undertaking a course in differential geometry ? Which books on differential geometry are most suitable for a first reading ?,,"['calculus', 'multivariable-calculus']"
18,Evaluating the Surface Integral on a Sphere for a scalar function (integral is involved in PDE),Evaluating the Surface Integral on a Sphere for a scalar function (integral is involved in PDE),,"I will begin by saying that I don't want to dissuade anyone who doesn't know PDE from helping, so if you're just here for the integral, you can skip down to ""Where I'm Stuck"" (For future reference: $\vec{x}=(x,y,z)$ , $\vec{x}_0=(x_0,y_0,z_0)$ , $\vec{x}+\vec{x}_0=(x+x_0,y+y_0,z+z_0)$ ) Integral set-up In my PDE class, we were asked to solve the homogeneous wave equation ( $U_{tt}-c^2\nabla^2 U=0$ ) for $U(\vec{x},t)$ on $\mathbb{R}^3$ where at $U(\vec{x},0)=xy^3z^2$ and $U_t(\vec{x},0)=z^2$ . The general way to solve the problem is to split the problem into 2 parts: a) Solve $V_{tt}-c^2\nabla^2 V=0$ where $V(\vec{x},0)=xy^3z^2$ and $V_t(\vec{x},0)=0$ b) Solve $W_{tt}-c^2\nabla^2 W=0$ where $W(\vec{x},0)=0$ and $W_t(\vec{x},0)=z^2$ So then $U=V+W$ . In class we derived a supposedly ""simple"" formula to solve part a ( $\psi(\vec{x})=V(\vec{x},0)=xy^3z^2$ ): $$V=\frac{\partial}{\partial t}\Bigg (\frac{1}{4\pi c^2t}\iint_{|\vec{x}_0|=ct}\psi(\vec{x}+\vec{x}_0)\,dS_{\vec{x}_0}\Bigg )$$ I'm going to ignore the partial derivative and the coefficient later on in the question because calculation of that is trivial to the problem I am having. Where I'm stuck I need to evaluate the following integral: $$\iint_{|\vec{x}_0|=ct}(x+x_0)(y+y_0)^3(z+z_0)^2\,dS_{\vec{x}_0}$$ The $|\vec{x}_0|=ct$ and $\,dS_{\vec{x}_0}$ mean that we're essentially treating $x$ , $y$ , and $z$ as numbers (instead of variables) and integrating only by the variables with 0 subscript ( $x_0$ , $y_0$ , and $z_0$ ) over a sphere of radius $ct$ (since theoretically this is for light waves). I'm not sure how I'm supposed to evaluate this, asI may have forgotten some of my calculus, but my confusion arises from the fact that typically when dealing with surface integrals, the integrand is a vector field (the result is the flux through an area). Although the integral now doesn't make physical sense to me, I can still try to work it out. Since I'm dealing with a sphere, I was thinking of doing the bounds of integration in spherical coordinates about a sphere of radius $\rho=ct$ : $$\int_0^\pi\int_0^{2\pi}(x+x_0)(y+y_0)^3(z+z_0)^2\rho^2\sin{\phi}\,d\theta\,d\phi$$ Since $\rho^2=c^2t^2$ has no relation to $\theta$ and $\phi$ , I can pull it out of the integral, leaving me with: $$c^2t^2\int_0^\pi\int_0^{2\pi}(x+x_0)(y+y_0)^3(z+z_0)^2\sin{\phi}\,d\theta\,d\phi$$ My thought from here is to expand my integrand, separate terms that don't involve any 0-subscripted variables, then use the rectangular to spherical coordinate conversions ( $x_0=\rho\sin{\phi}\cos{\theta}$ , $y_0=\rho\sin{\phi}\sin{\theta}$ , $z_0=\rho\cos{\phi}$ ), then integrate manually. Is there an easier way to do this since the expansion will have quite a few terms? Also, what will I do with the new $\rho$ 's that appear? Will they all simply be equal to $ct$ since I'm only looking at my integrand on the sphere itself? It is also worth noting that in class, it was pointed out that if I have terms of odd degree (with respect to $x_0$ , $y_0$ , and $z_0$ ) in my integrand, integrating those terms over a sphere will result in 0 due to symmetry, so that's why I'm leaning towards this path. Any other suggested methods would be helpful as I will most definitely have to be integrating similar integrals in the near future.","I will begin by saying that I don't want to dissuade anyone who doesn't know PDE from helping, so if you're just here for the integral, you can skip down to ""Where I'm Stuck"" (For future reference: , , ) Integral set-up In my PDE class, we were asked to solve the homogeneous wave equation ( ) for on where at and . The general way to solve the problem is to split the problem into 2 parts: a) Solve where and b) Solve where and So then . In class we derived a supposedly ""simple"" formula to solve part a ( ): I'm going to ignore the partial derivative and the coefficient later on in the question because calculation of that is trivial to the problem I am having. Where I'm stuck I need to evaluate the following integral: The and mean that we're essentially treating , , and as numbers (instead of variables) and integrating only by the variables with 0 subscript ( , , and ) over a sphere of radius (since theoretically this is for light waves). I'm not sure how I'm supposed to evaluate this, asI may have forgotten some of my calculus, but my confusion arises from the fact that typically when dealing with surface integrals, the integrand is a vector field (the result is the flux through an area). Although the integral now doesn't make physical sense to me, I can still try to work it out. Since I'm dealing with a sphere, I was thinking of doing the bounds of integration in spherical coordinates about a sphere of radius : Since has no relation to and , I can pull it out of the integral, leaving me with: My thought from here is to expand my integrand, separate terms that don't involve any 0-subscripted variables, then use the rectangular to spherical coordinate conversions ( , , ), then integrate manually. Is there an easier way to do this since the expansion will have quite a few terms? Also, what will I do with the new 's that appear? Will they all simply be equal to since I'm only looking at my integrand on the sphere itself? It is also worth noting that in class, it was pointed out that if I have terms of odd degree (with respect to , , and ) in my integrand, integrating those terms over a sphere will result in 0 due to symmetry, so that's why I'm leaning towards this path. Any other suggested methods would be helpful as I will most definitely have to be integrating similar integrals in the near future.","\vec{x}=(x,y,z) \vec{x}_0=(x_0,y_0,z_0) \vec{x}+\vec{x}_0=(x+x_0,y+y_0,z+z_0) U_{tt}-c^2\nabla^2 U=0 U(\vec{x},t) \mathbb{R}^3 U(\vec{x},0)=xy^3z^2 U_t(\vec{x},0)=z^2 V_{tt}-c^2\nabla^2 V=0 V(\vec{x},0)=xy^3z^2 V_t(\vec{x},0)=0 W_{tt}-c^2\nabla^2 W=0 W(\vec{x},0)=0 W_t(\vec{x},0)=z^2 U=V+W \psi(\vec{x})=V(\vec{x},0)=xy^3z^2 V=\frac{\partial}{\partial t}\Bigg (\frac{1}{4\pi c^2t}\iint_{|\vec{x}_0|=ct}\psi(\vec{x}+\vec{x}_0)\,dS_{\vec{x}_0}\Bigg ) \iint_{|\vec{x}_0|=ct}(x+x_0)(y+y_0)^3(z+z_0)^2\,dS_{\vec{x}_0} |\vec{x}_0|=ct \,dS_{\vec{x}_0} x y z x_0 y_0 z_0 ct \rho=ct \int_0^\pi\int_0^{2\pi}(x+x_0)(y+y_0)^3(z+z_0)^2\rho^2\sin{\phi}\,d\theta\,d\phi \rho^2=c^2t^2 \theta \phi c^2t^2\int_0^\pi\int_0^{2\pi}(x+x_0)(y+y_0)^3(z+z_0)^2\sin{\phi}\,d\theta\,d\phi x_0=\rho\sin{\phi}\cos{\theta} y_0=\rho\sin{\phi}\sin{\theta} z_0=\rho\cos{\phi} \rho ct x_0 y_0 z_0","['multivariable-calculus', 'partial-differential-equations', 'definite-integrals', 'surface-integrals']"
19,"Find the derivative (a) $\frac{\partial w}{\partial s}$, where $w = \frac{x-z}{y+z}$, $x =s + t$, $y = st$ and $z = s - t$","Find the derivative (a) , where , ,  and",\frac{\partial w}{\partial s} w = \frac{x-z}{y+z} x =s + t y = st z = s - t,"Find the derivative (a) $\displaystyle\frac{\partial w}{\partial s}$ , where $\displaystyle w = \frac{x-z}{y+z}$ , $x =s + t$ , $y = st$ and $z = s - t$ Solution attempt: $$\frac{\partial w}{\partial s} = \frac{\partial w}{\partial x} \cdot \frac{\partial x}{\partial s} + \frac{\partial w}{\partial y} \cdot \frac{\partial y}{\partial s} + \frac{\partial w}{\partial z} \cdot \frac{\partial z}{\partial s} = \frac{1}{y+z} \cdot (1) + \frac{z-x}{(y+x)^2}t + \frac{-(x + y)}{(y + z)^{2}} (1) = (-x + z) \left(\frac{t}{(x + y)^2} + \frac{1}{(y + z)^2} \right)$$ (b) $\frac{\partial w}{\partial r}$ , where $w = \sqrt{x^2 + y^2 +z^2}, $ y = rs $, x = st, z = rt$ Solution attempt: $$\frac{\partial w}{\partial r} = \frac{\partial w}{\partial x} \cdot \frac{\partial x}{\partial r} + \frac{\partial w}{\partial y} \cdot \frac{\partial y}{\partial r} + \frac{\partial w}{\partial z} \cdot \frac{\partial z}{\partial r} = \frac{x}{\sqrt{x^2 + y^2 + z^2}} \cdot 0 + \frac{y}{\sqrt{x^2 + y^2 + z^2}} (1) + \frac{z}{\sqrt{x^2 + y^2 + z^2}}(1) = \frac{y+z}{\sqrt{x^2 + y^2 + z^2}}$$ (c) $\frac{\partial f}{\partial z}$ , where $u = f(v)$ , $v = g(w, x, y)$ , $w = h(z)$ , $x = p(t, z)$ and $y = q(t, z)$ Solution attempt: $$\frac{\partial f}{\partial z} = \frac{\partial f}{\partial u} \cdot \frac{\partial u}{\partial z} + \frac{\partial f}{\partial v} \cdot \frac{\partial v}{\partial z} + \frac{\partial f}{\partial w} \cdot \frac{\partial w}{\partial z} + \frac{\partial f}{\partial x} \cdot \frac{\partial x}{\partial z} + \frac{\partial f}{\partial y} \cdot \frac{\partial y}{\partial z} = ???$$ Would this be right. Also, how would I get the derivative of the functions?","Find the derivative (a) , where , , and Solution attempt: (b) , where y = rs Solution attempt: (c) , where , , , and Solution attempt: Would this be right. Also, how would I get the derivative of the functions?","\displaystyle\frac{\partial w}{\partial s} \displaystyle w = \frac{x-z}{y+z} x =s + t y = st z = s - t \frac{\partial w}{\partial s} = \frac{\partial w}{\partial x} \cdot \frac{\partial x}{\partial s} + \frac{\partial w}{\partial y} \cdot \frac{\partial y}{\partial s} + \frac{\partial w}{\partial z} \cdot \frac{\partial z}{\partial s} = \frac{1}{y+z} \cdot (1) + \frac{z-x}{(y+x)^2}t + \frac{-(x + y)}{(y + z)^{2}} (1) = (-x + z) \left(\frac{t}{(x + y)^2} + \frac{1}{(y + z)^2} \right) \frac{\partial w}{\partial r} w = \sqrt{x^2 + y^2 +z^2},  , x = st, z = rt \frac{\partial w}{\partial r} = \frac{\partial w}{\partial x} \cdot \frac{\partial x}{\partial r} + \frac{\partial w}{\partial y} \cdot \frac{\partial y}{\partial r} + \frac{\partial w}{\partial z} \cdot \frac{\partial z}{\partial r} = \frac{x}{\sqrt{x^2 + y^2 + z^2}} \cdot 0 + \frac{y}{\sqrt{x^2 + y^2 + z^2}} (1) + \frac{z}{\sqrt{x^2 + y^2 + z^2}}(1) = \frac{y+z}{\sqrt{x^2 + y^2 + z^2}} \frac{\partial f}{\partial z} u = f(v) v = g(w, x, y) w = h(z) x = p(t, z) y = q(t, z) \frac{\partial f}{\partial z} = \frac{\partial f}{\partial u} \cdot \frac{\partial u}{\partial z} + \frac{\partial f}{\partial v} \cdot \frac{\partial v}{\partial z} + \frac{\partial f}{\partial w} \cdot \frac{\partial w}{\partial z} + \frac{\partial f}{\partial x} \cdot \frac{\partial x}{\partial z} + \frac{\partial f}{\partial y} \cdot \frac{\partial y}{\partial z} = ???","['multivariable-calculus', 'partial-derivative']"
20,Magnitude of gradient is rotationally invariant,Magnitude of gradient is rotationally invariant,,"Consider an image with edges, which we can take a gradient of (i.e. by subtracting pixels). Suppose we have a point $(x,0)$ on an edge of the image. Now suppose we rotate this point by an angle counterclockwise $\alpha$ to yield a new point $(x\cos(\alpha), x \sin(\alpha))$ . I'm trying to figure out how to prove that the magnitude of the gradients at the point before and after are equal. Here's what I've tried: We can see that the magnitude of the gradient of the point before only has a $y$ component. Thus it's magnitude is the partial of the image $\dfrac{\partial f(x,y)}{\partial y}$ . Now for the rotated image, we are looking for $\sqrt{({{\dfrac{\partial f(x\cos(\alpha), x\sin(\alpha))}{\partial x}}})^2 + {\dfrac{\partial f(x\cos(\alpha), x\sin(\alpha))}{\partial y}})^2}$ . We can see that the second squared term under the square root goes away by the chain rule so the expression becomes $\cos(\alpha)\sin(\alpha)\dfrac{\partial f(x,y)}{\partial x}$ ...which is clearly not what we want. Can anyone tell me what I'm doing wrong?","Consider an image with edges, which we can take a gradient of (i.e. by subtracting pixels). Suppose we have a point on an edge of the image. Now suppose we rotate this point by an angle counterclockwise to yield a new point . I'm trying to figure out how to prove that the magnitude of the gradients at the point before and after are equal. Here's what I've tried: We can see that the magnitude of the gradient of the point before only has a component. Thus it's magnitude is the partial of the image . Now for the rotated image, we are looking for . We can see that the second squared term under the square root goes away by the chain rule so the expression becomes ...which is clearly not what we want. Can anyone tell me what I'm doing wrong?","(x,0) \alpha (x\cos(\alpha), x \sin(\alpha)) y \dfrac{\partial f(x,y)}{\partial y} \sqrt{({{\dfrac{\partial f(x\cos(\alpha), x\sin(\alpha))}{\partial x}}})^2 + {\dfrac{\partial f(x\cos(\alpha), x\sin(\alpha))}{\partial y}})^2} \cos(\alpha)\sin(\alpha)\dfrac{\partial f(x,y)}{\partial x}","['multivariable-calculus', 'chain-rule']"
21,"Prove or disprove each of the follow function has limits $x \to a$ by the definition $\lim_{(x, y) \to (0, 0)} \frac{xy^2}{x^2 + y^2}$",Prove or disprove each of the follow function has limits  by the definition,"x \to a \lim_{(x, y) \to (0, 0)} \frac{xy^2}{x^2 + y^2}","Prove or disprove each of the follow function has limits $x \to a$ by the definition $\lim_{(x, y) \to (0, 0)} \frac{xy^2}{x^2 + y^2}$ Given $\epsilon > 0$ . Choose $\delta = \epsilon$ , then $||(x, y) - (0, 0)|| = \sqrt{x^2+y^2}$ , and so $\|(x, y) - (0,0)\| < \delta$ implies that $$\left| \frac{xy^2}{x^2-y^2} - 0\right| = \frac{|xy^2|}{x^2+y^2} \leq \frac{\sqrt{x^2+y^2} \cdot (x^2+y^2)}{x^2 + y^2} = \sqrt{x^2+y^2} < \delta = \epsilon$$ Therefore the limit does exist. Is this right?","Prove or disprove each of the follow function has limits by the definition Given . Choose , then , and so implies that Therefore the limit does exist. Is this right?","x \to a \lim_{(x, y) \to (0, 0)} \frac{xy^2}{x^2 + y^2} \epsilon > 0 \delta = \epsilon ||(x, y) - (0, 0)|| = \sqrt{x^2+y^2} \|(x, y) - (0,0)\| < \delta \left| \frac{xy^2}{x^2-y^2} - 0\right| = \frac{|xy^2|}{x^2+y^2} \leq \frac{\sqrt{x^2+y^2} \cdot (x^2+y^2)}{x^2 + y^2} = \sqrt{x^2+y^2} < \delta = \epsilon","['limits', 'multivariable-calculus']"
22,"In general, what information is given by $\nabla^2 f \geq 0$?","In general, what information is given by ?",\nabla^2 f \geq 0,"Since there are many directions one can take when studying this equation, I am curious: Given a function $f \in C^2$ defined on some open set, what information is given by $\nabla^2 f \geq 0$ ? Please let me know if I am missing something important for the question to make sense. Thanks in advance.","Since there are many directions one can take when studying this equation, I am curious: Given a function defined on some open set, what information is given by ? Please let me know if I am missing something important for the question to make sense. Thanks in advance.",f \in C^2 \nabla^2 f \geq 0,"['real-analysis', 'functional-analysis']"
23,Minimize quadratic function with gradient descent method,Minimize quadratic function with gradient descent method,,"Anyone could help me wih this question, please? I made an exam and I didn't know how I can solve this. $A \in \mathbb{R}^{n \times n}$ is a matrix symmetric positive definite, $b \in \mathbb{R}^{n\times 1}$ and we have $f(x) = \dfrac{1}{2}x^TAx -b^Tx$ . Give a point $x^0 \in \mathbb{R}^{n \times 1}$ , show that the gradient method descent given by $$ x^{k+1}=x^k -\left(||A||_{2}\right)^{-1} \nabla f(x^k)$$ converges to the global minimum. So, this $|| \cdot||_2$ is an Euclidean norm. I made this: $|| A||_2 = || A|| = \sup\{||Ax||:||x|| =1 \} $ , and I thought A has positive real eigenvalues $0 < \lambda_1 \leq \lambda_2 \leq \cdots\leq \lambda_n$ and that I would obtain an orthonormal basis of the matrix $A$ formed by the eigenvectors $v_1, v_2, ..., v_n$ associated with the respective eigenvalues $\lambda_i$ . So, how $||Av|| = ||\lambda v|| = |\lambda| \;||v|| = \lambda$ I suppose that $||A|| = \lambda_n = \lambda$ and so $||A||^{-1}=\dfrac{1}{\lambda}$ . Now, I have the following sequence $$ x^{k+1}=x^k -\dfrac{1}{\lambda} \nabla f(x^k). $$ I have argued that $f$ has only a local minimum and that it is global. Also said that the minimum is given by $x^{*} =A^{-1}b$ . But I could not show that the sequence above converges to $x^{*}$ . Then I thought I did not need to show that I was converging to $x^{*}$ . It is enough to show that it converged, for it guarantees that it would be for this element due to its uniqueness (only having a global minimum). But I could not solve it. It reminded me of contraction. I have already tried a recurrence by doing $$ x^{k+1} - x^0 = -\dfrac{1}{\lambda}\left( \nabla f(x^0) + \nabla f(x^1) + \cdots +\nabla f(x^k) \right) $$ but I could not move forward. I also tried to think in $||x^{k+1}-x^{k} || = ||-\frac{1}{\lambda} \nabla f(x^k) || = \frac{1}{\lambda} || \nabla f(x^k) ||$ , which was when it reminded me of a contraction. But I could not finish anything either. Incidentally, this holds if $1 / \lambda$ is less than $1$ and I could not guarantee that. Could you help me? I already did the test, but I wanted to see the solution. PS: I found this, but I can't understand. https://en.wikipedia.org/wiki/Modified_Richardson_iteration","Anyone could help me wih this question, please? I made an exam and I didn't know how I can solve this. is a matrix symmetric positive definite, and we have . Give a point , show that the gradient method descent given by converges to the global minimum. So, this is an Euclidean norm. I made this: , and I thought A has positive real eigenvalues and that I would obtain an orthonormal basis of the matrix formed by the eigenvectors associated with the respective eigenvalues . So, how I suppose that and so . Now, I have the following sequence I have argued that has only a local minimum and that it is global. Also said that the minimum is given by . But I could not show that the sequence above converges to . Then I thought I did not need to show that I was converging to . It is enough to show that it converged, for it guarantees that it would be for this element due to its uniqueness (only having a global minimum). But I could not solve it. It reminded me of contraction. I have already tried a recurrence by doing but I could not move forward. I also tried to think in , which was when it reminded me of a contraction. But I could not finish anything either. Incidentally, this holds if is less than and I could not guarantee that. Could you help me? I already did the test, but I wanted to see the solution. PS: I found this, but I can't understand. https://en.wikipedia.org/wiki/Modified_Richardson_iteration","A \in \mathbb{R}^{n \times n} b \in \mathbb{R}^{n\times 1} f(x) = \dfrac{1}{2}x^TAx -b^Tx x^0 \in \mathbb{R}^{n \times 1}  x^{k+1}=x^k -\left(||A||_{2}\right)^{-1} \nabla f(x^k) || \cdot||_2 || A||_2 = || A|| = \sup\{||Ax||:||x|| =1 \}
 0 < \lambda_1 \leq \lambda_2 \leq \cdots\leq \lambda_n A v_1, v_2, ..., v_n \lambda_i ||Av|| = ||\lambda v|| = |\lambda| \;||v|| = \lambda ||A|| = \lambda_n = \lambda ||A||^{-1}=\dfrac{1}{\lambda} 
x^{k+1}=x^k -\dfrac{1}{\lambda} \nabla f(x^k).
 f x^{*} =A^{-1}b x^{*} x^{*} 
x^{k+1} - x^0 = -\dfrac{1}{\lambda}\left( \nabla f(x^0) + \nabla f(x^1) + \cdots +\nabla f(x^k) \right)
 ||x^{k+1}-x^{k} || = ||-\frac{1}{\lambda} \nabla f(x^k) || = \frac{1}{\lambda} || \nabla f(x^k) || 1 / \lambda 1","['calculus', 'multivariable-calculus', 'optimization']"
24,How to transform the PDE?,How to transform the PDE?,,"I am doing the following problem: Transform the equation $$y\left(\frac{\partial z}{\partial x}\right) - x \left(\frac{\partial z}{\partial y} \right) = (y-x)z$$ by introuducing new independent variables: $$u = x^2 + y^2, v = \frac{1}{x} + \frac{1}{y}$$ and a new function: $$w = \ln(z) - (x + y)$$ I did everything as it is in the example I have (system of first order diferentials) and I got to this equation as the new one: $$\left(\frac{xz}{y^2}- \frac{yz}{x^2}\right) \frac{\partial w}{\partial v} = 0$$ The solutions say this is the result: $\frac{\partial w}{\partial v} = 0$ I checked multiple times what I've done and I cannot find a mistake. Am I allowed to divide by the term in the braces and treat it as a constant that's different from 0? If I can, why? It doesn't make sense to me. If we try to write it in terms of v and w, we cannot divide it at all. How to do this properly and is there another way at approaching it?","I am doing the following problem: Transform the equation by introuducing new independent variables: and a new function: I did everything as it is in the example I have (system of first order diferentials) and I got to this equation as the new one: The solutions say this is the result: I checked multiple times what I've done and I cannot find a mistake. Am I allowed to divide by the term in the braces and treat it as a constant that's different from 0? If I can, why? It doesn't make sense to me. If we try to write it in terms of v and w, we cannot divide it at all. How to do this properly and is there another way at approaching it?","y\left(\frac{\partial z}{\partial x}\right) - x \left(\frac{\partial z}{\partial y} \right) = (y-x)z u = x^2 + y^2, v = \frac{1}{x} + \frac{1}{y} w = \ln(z) - (x + y) \left(\frac{xz}{y^2}- \frac{yz}{x^2}\right) \frac{\partial w}{\partial v} = 0 \frac{\partial w}{\partial v} = 0","['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
25,Estimate gradient of a function,Estimate gradient of a function,,"Let $\mathbb{D}:= \{(x,y)\in \mathbb{R}^2:x^2+y^2<1\}$ , $S$ is open and $\mathbb{\bar{D}}\subset S$ . Let $F:S\to \mathbb{R}$ is differentiable and $|F(x)|\leq 1$ for all $x\in \mathbb{D}$ . Prove that there exists $\theta \in S$ such that $|\triangledown F(\theta )| < 4$ . I think it may be easier to solve if it is continuously differentiable. In this condition, $\triangledown F(x)$ can be seen as a $C^1$ vector field and I can find a integral curve originated from $(0,0)$ . When it is differentiable, what I know is only ""The differential mean value theorem"": $F(y)-F(x)=\triangledown F(\xi )\cdot (y-x)$ . It is hard to consider directions of these two vectors.","Let , is open and . Let is differentiable and for all . Prove that there exists such that . I think it may be easier to solve if it is continuously differentiable. In this condition, can be seen as a vector field and I can find a integral curve originated from . When it is differentiable, what I know is only ""The differential mean value theorem"": . It is hard to consider directions of these two vectors.","\mathbb{D}:= \{(x,y)\in \mathbb{R}^2:x^2+y^2<1\} S \mathbb{\bar{D}}\subset S F:S\to \mathbb{R} |F(x)|\leq 1 x\in \mathbb{D} \theta \in S |\triangledown F(\theta )| < 4 \triangledown F(x) C^1 (0,0) F(y)-F(x)=\triangledown F(\xi )\cdot (y-x)","['calculus', 'multivariable-calculus', 'vector-analysis']"
26,Average change for an implicitly defined function,Average change for an implicitly defined function,,"I was wondering if there is a way to compute the average change for an implicitly defined function such that as the interval of a change decreases, we converge to the derivative. For a ""standard"" function such result is easy to obtain. Suppose we have $y=f(x)$ and want to find the average change in $f(x)$ as $x$ changes from $x_0$ to $x_0+h$ . In that case, we have the $$ \frac{\Delta y}{\Delta x}:= \frac{y(x_0+h)-y(x_0)}{x_0+h-x_0}=\frac{f(x_0+h)-f(x_0)}{(x_0+h) - x_0},$$ and as $h \rightarrow0$ the average change in $f$ converges to $f'$ ; that is, $$ lim_{h\rightarrow 0}\frac{\Delta y}{\Delta x} = \frac{d y}{dx} $$ My question : Is there an equivalent expression/relationship for an implicitly defined function? My Attempt : Let $f(x,y(x))=0$ and suppose I want to compute the average change in $y(x)$ as $x$ varies from $x$ to $x+h$ . We know that $$ f(x+h,y(x+h))=0$$ Subtracting from the above equation $f(x,y(x))$ , and performing simple manipulations, I obtain $$ \frac{f(x+h,y(x+h))-f(x+h,y(x))}{y(x+h)-y(x)}(y(x+h)-y(x)) + \frac{f(x+h,y(x))- f(x,y(x))}{h}h =0$$ Rearranging, I obtain: $$ \frac{y(x+h))-y(x)}{(x+h)-x} = - \frac{\frac{f(x+h,y(x))- f(x,y(x))}{h}}{\frac{f(x+h,y(x+h))-f(x+h,y(x))}{y(x+h)-y(x)}} $$ If I define $\Delta y:= y(x+h)-y(x)$ and $\Delta x:= (x+h)-x$ , then I have $$ \frac{\Delta y}{\Delta x} = - \frac{\frac{f(x+h,y(x))- f(x,y(x))}{h}}{\frac{f(x+h,y(x+h))-f(x+h,y(x))}{y(x+h)-y(x)}}, $$ which seems intuitive and seems to capture the average change in y(x) over interval [x,x+h]. Next, I would like to take the limit as $h\rightarrow 0$ and show that the above expression converge to the derivative of $y(x)$ that we obtain by applying the implicit function theorem. When I take the limit as $h\rightarrow 0$ on the LHS, I can see that the numerator converges to $f_x(x,y)$ ; but I am not sure how to deal with the denominator. Does the denominator converge to $f_y(x,y)$ ? Or is there another way to link the average change in $y$ over interval $[x,x+h]$ to $dy/dx$ .","I was wondering if there is a way to compute the average change for an implicitly defined function such that as the interval of a change decreases, we converge to the derivative. For a ""standard"" function such result is easy to obtain. Suppose we have and want to find the average change in as changes from to . In that case, we have the and as the average change in converges to ; that is, My question : Is there an equivalent expression/relationship for an implicitly defined function? My Attempt : Let and suppose I want to compute the average change in as varies from to . We know that Subtracting from the above equation , and performing simple manipulations, I obtain Rearranging, I obtain: If I define and , then I have which seems intuitive and seems to capture the average change in y(x) over interval [x,x+h]. Next, I would like to take the limit as and show that the above expression converge to the derivative of that we obtain by applying the implicit function theorem. When I take the limit as on the LHS, I can see that the numerator converges to ; but I am not sure how to deal with the denominator. Does the denominator converge to ? Or is there another way to link the average change in over interval to .","y=f(x) f(x) x x_0 x_0+h  \frac{\Delta y}{\Delta x}:= \frac{y(x_0+h)-y(x_0)}{x_0+h-x_0}=\frac{f(x_0+h)-f(x_0)}{(x_0+h) - x_0}, h \rightarrow0 f f'  lim_{h\rightarrow 0}\frac{\Delta y}{\Delta x} = \frac{d y}{dx}  f(x,y(x))=0 y(x) x x x+h  f(x+h,y(x+h))=0 f(x,y(x))  \frac{f(x+h,y(x+h))-f(x+h,y(x))}{y(x+h)-y(x)}(y(x+h)-y(x)) + \frac{f(x+h,y(x))- f(x,y(x))}{h}h =0  \frac{y(x+h))-y(x)}{(x+h)-x} = - \frac{\frac{f(x+h,y(x))- f(x,y(x))}{h}}{\frac{f(x+h,y(x+h))-f(x+h,y(x))}{y(x+h)-y(x)}}  \Delta y:= y(x+h)-y(x) \Delta x:= (x+h)-x  \frac{\Delta y}{\Delta x} = - \frac{\frac{f(x+h,y(x))- f(x,y(x))}{h}}{\frac{f(x+h,y(x+h))-f(x+h,y(x))}{y(x+h)-y(x)}},  h\rightarrow 0 y(x) h\rightarrow 0 f_x(x,y) f_y(x,y) y [x,x+h] dy/dx","['calculus', 'real-analysis', 'limits', 'multivariable-calculus', 'derivatives']"
27,(Multivariable Calculus) Convert $\rho = \sin \phi$ to cylindrical and rectangular,(Multivariable Calculus) Convert  to cylindrical and rectangular,\rho = \sin \phi,"Question: Consider the surface given in spherical coordinates by $\rho = \sin(\phi)$. Convert to rectangular coordinates and cylindrical coordinates. Identify the surface. By graphing the function, I've found that it is a horn torus (circles in cross section are tangent to each other). Using some conversion formulas, I got this: $$r = \sin^2(\phi)$$ $$\theta = \theta$$ $$z = \frac{\sin(2\phi)}{2}$$ And then for rectangular (using those cylindrical values): $$x = \sin^2(\phi)\cos(\theta)$$ $$y = \sin^2(\phi)\sin(\theta)$$ $$z = \frac{\sin(2\phi)}{2}$$ I would  say this is wrong as I'm probably supposed to get it in terms of $(r, \theta, z)$, and then also into $(x, y, z)$. Unless this is correct? Thanks for the reading.","Question: Consider the surface given in spherical coordinates by $\rho = \sin(\phi)$. Convert to rectangular coordinates and cylindrical coordinates. Identify the surface. By graphing the function, I've found that it is a horn torus (circles in cross section are tangent to each other). Using some conversion formulas, I got this: $$r = \sin^2(\phi)$$ $$\theta = \theta$$ $$z = \frac{\sin(2\phi)}{2}$$ And then for rectangular (using those cylindrical values): $$x = \sin^2(\phi)\cos(\theta)$$ $$y = \sin^2(\phi)\sin(\theta)$$ $$z = \frac{\sin(2\phi)}{2}$$ I would  say this is wrong as I'm probably supposed to get it in terms of $(r, \theta, z)$, and then also into $(x, y, z)$. Unless this is correct? Thanks for the reading.",,"['multivariable-calculus', 'proof-verification']"
28,How to show orthogonality with respect to the Euclidean inner product involving a curve in $\mathbb{R}^n$ and a point not on the curve,How to show orthogonality with respect to the Euclidean inner product involving a curve in  and a point not on the curve,\mathbb{R}^n,"Specifically, the question is as follows: Let $f: \mathbb{R} \rightarrow \mathbb{R}^n$ be a differentiable mapping (a curve) with   $f^\prime(t) \ne 0$ for all $t \in \mathbb{R}$. Let $p$ be a fixed point not on the image $f$. If $q = f(t_0)$ is the point of the curve closest to $p$,   that is $| p - q| \le |p - f(t)|$ for all $t \in \mathbb{R}$,   show that the vector $(p - q)$ is orthogonal to the curve   at $q$. Here the orthogonality is with respect to the usual Euclidean inner product. I've also been given the following hint: Differentiate the function   $\varphi(t) = | p - f(t)|^2$. I understand that $p$ and $q$ are points in $\mathbb{R}^n$ and that $(p-q)$ is the vector that is between $p$ and $q$. I also understand that to show $(p - q)$ is orthogonal to the curve at $q$, I need to show that $\langle(p-q),f'(q)\rangle=0$, as orthogonality to the curve at a point is given by a $0$ inner product with the tangent to the curve at the point. My confusion lies in understanding what $\varphi(t)$ and it's derivative represent, as well as how to use them. Correct me if I'm wrong, but it seems like $\varphi(t)$ is a parabaloid (or some step function eqivalent if $f$ is discontinuous) where the vertex is the minimum value, $|p-q|$ (i.e. when $t=t_0$). So what information do I acquire from taking the derivative that I can utilize to solve this problem? In a more direct sense, I know that I want to show that $\langle(p-q),f'(q)\rangle=0$, so, equivalently, I want to show that $\sum_{i=1}^n(p_i-q_i)q_i=0$. Expanding and differentiating, I have that $\varphi'(t)=2\sum_{i=1}^nf'(t)_i(f(t)_i-p_i)$. But I don't see what to do from here. I'd like assistance in interpreting this problem (ideally geometrically and algebraically) so that I can develop a solution. This is homework, so I request guidance only.","Specifically, the question is as follows: Let $f: \mathbb{R} \rightarrow \mathbb{R}^n$ be a differentiable mapping (a curve) with   $f^\prime(t) \ne 0$ for all $t \in \mathbb{R}$. Let $p$ be a fixed point not on the image $f$. If $q = f(t_0)$ is the point of the curve closest to $p$,   that is $| p - q| \le |p - f(t)|$ for all $t \in \mathbb{R}$,   show that the vector $(p - q)$ is orthogonal to the curve   at $q$. Here the orthogonality is with respect to the usual Euclidean inner product. I've also been given the following hint: Differentiate the function   $\varphi(t) = | p - f(t)|^2$. I understand that $p$ and $q$ are points in $\mathbb{R}^n$ and that $(p-q)$ is the vector that is between $p$ and $q$. I also understand that to show $(p - q)$ is orthogonal to the curve at $q$, I need to show that $\langle(p-q),f'(q)\rangle=0$, as orthogonality to the curve at a point is given by a $0$ inner product with the tangent to the curve at the point. My confusion lies in understanding what $\varphi(t)$ and it's derivative represent, as well as how to use them. Correct me if I'm wrong, but it seems like $\varphi(t)$ is a parabaloid (or some step function eqivalent if $f$ is discontinuous) where the vertex is the minimum value, $|p-q|$ (i.e. when $t=t_0$). So what information do I acquire from taking the derivative that I can utilize to solve this problem? In a more direct sense, I know that I want to show that $\langle(p-q),f'(q)\rangle=0$, so, equivalently, I want to show that $\sum_{i=1}^n(p_i-q_i)q_i=0$. Expanding and differentiating, I have that $\varphi'(t)=2\sum_{i=1}^nf'(t)_i(f(t)_i-p_i)$. But I don't see what to do from here. I'd like assistance in interpreting this problem (ideally geometrically and algebraically) so that I can develop a solution. This is homework, so I request guidance only.",,"['real-analysis', 'multivariable-calculus']"
29,Why doesn't the limit $\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$ exist?,Why doesn't the limit  exist?,\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta},"I have two questions regarding this subject. Hope you can help me. Consider the limit of the function $f(x,y) = \frac{x^2y}{x^4+y^2}$ as (x,y) approaches (0,0): $$\lim_{(x,y) \to (0,0)} \frac{x^2y}{x^4 + y^2} = \lim_{r \to 0}\frac{r^2\cos^2\theta(r\sin\theta)}{r^4\cos^4\theta + r^2\sin^2\theta}$$ I tried the paths $y=x^2$ and $x=0$ and found different limits which means that the limit of the function doesn't exists. Now, I want to verify that through the polar coordinates. After the simplification, the polar equation reduces to: $$\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$$ At first I thought that this limit is equal to $0$ for all lines except the lines $\theta = 0$ and $\theta= \pi$ because I thought these lines would make $sin \theta$ equal to $0$ and that would cause the indeterminate form $0/0$, but after I thought about it, I came to another conclusion: Since this is a limit, $r$ only approaches to $0$ and it is never actually $0$, then the numerator of the limit is $0$ because of the $sin\theta$ part, and its denominator is not $0$ since $r$ is not $0$. Then, the limit should be just $0$. Is that right ? My other question is about using different paths on polar coordinates. For example, consider the function $r = sin\theta$. If I use this path, the equation becomes: $$\lim_{r \to 0} \frac{sin^2\theta cos^2\theta}{sin^2\theta \cos^4\theta + \sin^2\theta}$$ Simplifying ${sin^2\theta}/{sin^2\theta}$, we get, $$\lim_{r \to 0} \frac{cos^2\theta}{cos^4\theta + 1}$$ My question is: Can I use the possible $\theta$ values in this expression? For example, Because $r=sin\theta$ and $r$ approaches to $0$, $\theta$ can either be $0$ or $\pi$. Since we only have the even powers of $cos\theta$, we can just assume that $cos^2\theta$ is $1$, which makes the limit equal to:  $$\lim_{\theta \to 0} \frac{1}{1 + 1}= 1/2$$ Am I allowed to use $\theta$ value in the limit or is my work after the limit $\lim_{r \to 0} \frac{cos^2\theta}{cos^4\theta + 1}$ just wrong ? Edit: I was told that my question is a possible duplicate of another question. I am very new at this website, so I'm not sure if this is the right way to explain why my question is not a duplicate but  I'm gonna try to explain why. Mine and the other question is about the same limit; however, in the other question, the user asked why the limit doesn't exist, while I already know why, I just want to verify this in terms of polar coordinates. I also want to know if using the value of $\theta$ in different paths is valid, which is a topic the other question doesn't mention.","I have two questions regarding this subject. Hope you can help me. Consider the limit of the function $f(x,y) = \frac{x^2y}{x^4+y^2}$ as (x,y) approaches (0,0): $$\lim_{(x,y) \to (0,0)} \frac{x^2y}{x^4 + y^2} = \lim_{r \to 0}\frac{r^2\cos^2\theta(r\sin\theta)}{r^4\cos^4\theta + r^2\sin^2\theta}$$ I tried the paths $y=x^2$ and $x=0$ and found different limits which means that the limit of the function doesn't exists. Now, I want to verify that through the polar coordinates. After the simplification, the polar equation reduces to: $$\lim_{r \to 0} \frac{r (\cos^2\theta\sin\theta)}{r^2\cos^4\theta + \sin^2\theta}$$ At first I thought that this limit is equal to $0$ for all lines except the lines $\theta = 0$ and $\theta= \pi$ because I thought these lines would make $sin \theta$ equal to $0$ and that would cause the indeterminate form $0/0$, but after I thought about it, I came to another conclusion: Since this is a limit, $r$ only approaches to $0$ and it is never actually $0$, then the numerator of the limit is $0$ because of the $sin\theta$ part, and its denominator is not $0$ since $r$ is not $0$. Then, the limit should be just $0$. Is that right ? My other question is about using different paths on polar coordinates. For example, consider the function $r = sin\theta$. If I use this path, the equation becomes: $$\lim_{r \to 0} \frac{sin^2\theta cos^2\theta}{sin^2\theta \cos^4\theta + \sin^2\theta}$$ Simplifying ${sin^2\theta}/{sin^2\theta}$, we get, $$\lim_{r \to 0} \frac{cos^2\theta}{cos^4\theta + 1}$$ My question is: Can I use the possible $\theta$ values in this expression? For example, Because $r=sin\theta$ and $r$ approaches to $0$, $\theta$ can either be $0$ or $\pi$. Since we only have the even powers of $cos\theta$, we can just assume that $cos^2\theta$ is $1$, which makes the limit equal to:  $$\lim_{\theta \to 0} \frac{1}{1 + 1}= 1/2$$ Am I allowed to use $\theta$ value in the limit or is my work after the limit $\lim_{r \to 0} \frac{cos^2\theta}{cos^4\theta + 1}$ just wrong ? Edit: I was told that my question is a possible duplicate of another question. I am very new at this website, so I'm not sure if this is the right way to explain why my question is not a duplicate but  I'm gonna try to explain why. Mine and the other question is about the same limit; however, in the other question, the user asked why the limit doesn't exist, while I already know why, I just want to verify this in terms of polar coordinates. I also want to know if using the value of $\theta$ in different paths is valid, which is a topic the other question doesn't mention.",,"['limits', 'multivariable-calculus', 'polar-coordinates']"
30,Mean value theorem for vector laplacian,Mean value theorem for vector laplacian,,"It is well known that all solutions of the Laplace equation $\nabla^2 u = 0$ satisfy the mean value theorem: the average value of $u$ over a sphere equals its value at the center of the sphere. My question is: does the same theorem hold in case of vector laplacian? Let the vector field $\vec{v}$ satisfy $\nabla^2 \vec{v} = 0$. Is it true that the mean value of $\vec{v}$ over a sphere equals its value at the center of the sphere? I would imagine the answer to be positive. For example, writing $\vec{v}$ in cartesian basis reduces the vector laplacian $\nabla^2 \vec{v} = 0$ to three ordinary laplacians: $\nabla^2 v_x = \nabla^2 v_y = \nabla^2 v_z = 0$ and makes the statement trivial. However, one can imagine using some other, curvilinear basis, in which the statement is far from trivial. Therefore, I would appreciate some insight or a coordinate-free proof of the theorem.","It is well known that all solutions of the Laplace equation $\nabla^2 u = 0$ satisfy the mean value theorem: the average value of $u$ over a sphere equals its value at the center of the sphere. My question is: does the same theorem hold in case of vector laplacian? Let the vector field $\vec{v}$ satisfy $\nabla^2 \vec{v} = 0$. Is it true that the mean value of $\vec{v}$ over a sphere equals its value at the center of the sphere? I would imagine the answer to be positive. For example, writing $\vec{v}$ in cartesian basis reduces the vector laplacian $\nabla^2 \vec{v} = 0$ to three ordinary laplacians: $\nabla^2 v_x = \nabla^2 v_y = \nabla^2 v_z = 0$ and makes the statement trivial. However, one can imagine using some other, curvilinear basis, in which the statement is far from trivial. Therefore, I would appreciate some insight or a coordinate-free proof of the theorem.",,"['multivariable-calculus', 'vector-analysis', 'coordinate-systems', 'harmonic-functions', 'laplacian']"
31,Does the gradient theorem generalize to vector-valued potentials?,Does the gradient theorem generalize to vector-valued potentials?,,"The gradient theorem states that if $F$ is a differentiable scalar-valued function of an $n$-dimensional vector space, then the gradient of $F$ is a conservative vector field. Line integrals through this field are path-independent, meaning that any line integral through $F'$ that starts at point $a$ and ends at point $b$ will evaluate to the scalar $F(b) - F(a)$. Does this result generalize to cases where we start with a vector field? So say $G$ is a vector field and $G'$ is its Jacobian matrix: is the tensor field defined by $G'$ conservative in roughly the same way? Does any line integral through that field starting at $a$ and ending at $b$ now evaluate to the vector $G(b) - G(a)$? I feel this must be true because you can just argue component-wise, treating each dimension of the output of $G$ as a scalar and considering the vector field defined by each of the corresponding dimensions of $G'$. But I'm not certain that actually works out, or whether there are additional conditions that must hold. For example, must $G$ itself be a conservative vector field? Or, to put it differently, must $G'$ be a Hessian matrix?","The gradient theorem states that if $F$ is a differentiable scalar-valued function of an $n$-dimensional vector space, then the gradient of $F$ is a conservative vector field. Line integrals through this field are path-independent, meaning that any line integral through $F'$ that starts at point $a$ and ends at point $b$ will evaluate to the scalar $F(b) - F(a)$. Does this result generalize to cases where we start with a vector field? So say $G$ is a vector field and $G'$ is its Jacobian matrix: is the tensor field defined by $G'$ conservative in roughly the same way? Does any line integral through that field starting at $a$ and ending at $b$ now evaluate to the vector $G(b) - G(a)$? I feel this must be true because you can just argue component-wise, treating each dimension of the output of $G$ as a scalar and considering the vector field defined by each of the corresponding dimensions of $G'$. But I'm not certain that actually works out, or whether there are additional conditions that must hold. For example, must $G$ itself be a conservative vector field? Or, to put it differently, must $G'$ be a Hessian matrix?",,"['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'line-integrals']"
32,"exponential of second derivative, not positive?","exponential of second derivative, not positive?",,"I'm interested in whether the following function $h(x,y)=e^{\partial_x\partial_y}f(x)g(y)$, is positive in the sense that $h(x,y)\geq 0$, whenever $f(x)\geq 0$ and $g(y)\geq 0$ (positive and real). I would have thought that it was positive because the derivatives are in the exponential, but it appears not to be. Any idea why it isn't positive? Maybe because ${\partial_x\partial_y}$ is not a normal operator? But I think it is even self-adjoint (or at least symmetric), so should have real eigenvalues. To see that it is not positive one can expand the exponential $h(x,y)=f(x)g(y)+f'(x)g'(y)+\frac{1}{2}f''(x)g''(y)+...$ and take (for example) $f(x)=x^2$ and $g(y)=y^2$. Then the series terminates at second order and is $(xy+2)^2-2$, which is negative at $x=-\sqrt{2}$, $y=\sqrt{2}$ etc. To see that $\partial_x\partial_y$ is symmetric we have $\int dx dy \psi^*(x,y)\partial_x\partial_y \phi(x,y)=\int dx dy \partial_x\partial_y \psi^*(x,y) \phi(x,y)$ after integrating by parts twice, plus a boundary term. So for the boundary term to vanish we should have that $\psi^*(x,y)$ is zero at infinity, so maybe  $e^{\partial_x\partial_y}f(x)g(y)$ is positive on functions which decay to zero  at infinity? This is not the case with polynomials which are the examples I know that give negative values.","I'm interested in whether the following function $h(x,y)=e^{\partial_x\partial_y}f(x)g(y)$, is positive in the sense that $h(x,y)\geq 0$, whenever $f(x)\geq 0$ and $g(y)\geq 0$ (positive and real). I would have thought that it was positive because the derivatives are in the exponential, but it appears not to be. Any idea why it isn't positive? Maybe because ${\partial_x\partial_y}$ is not a normal operator? But I think it is even self-adjoint (or at least symmetric), so should have real eigenvalues. To see that it is not positive one can expand the exponential $h(x,y)=f(x)g(y)+f'(x)g'(y)+\frac{1}{2}f''(x)g''(y)+...$ and take (for example) $f(x)=x^2$ and $g(y)=y^2$. Then the series terminates at second order and is $(xy+2)^2-2$, which is negative at $x=-\sqrt{2}$, $y=\sqrt{2}$ etc. To see that $\partial_x\partial_y$ is symmetric we have $\int dx dy \psi^*(x,y)\partial_x\partial_y \phi(x,y)=\int dx dy \partial_x\partial_y \psi^*(x,y) \phi(x,y)$ after integrating by parts twice, plus a boundary term. So for the boundary term to vanish we should have that $\psi^*(x,y)$ is zero at infinity, so maybe  $e^{\partial_x\partial_y}f(x)g(y)$ is positive on functions which decay to zero  at infinity? This is not the case with polynomials which are the examples I know that give negative values.",,"['linear-algebra', 'functional-analysis', 'multivariable-calculus', 'derivatives', 'exponential-function']"
33,"Approach to Polar Change of Variable in $\int_0^ {ae^{{\pi}/4}} \int_{2 \log(r/a)}^{\pi/2} f(r, \theta) r dr d \theta$.",Approach to Polar Change of Variable in .,"\int_0^ {ae^{{\pi}/4}} \int_{2 \log(r/a)}^{\pi/2} f(r, \theta) r dr d \theta","$$\int_0^ {ae^{{\pi}/4}} \int_{2 \log(r/a)}^{\pi/2}  f(r, \theta) \ r \ dr \ d \theta$$ In the above integral, for evaluating and sketching it, I have to change the order of integral, but I am not able to get the right approach. Is there any  general method to handle change of order of integration in polar coordinates? The answer is $\theta$ varies from $0$ to $\pi/2$ and $r$ varies from $a$ to $ae^{\theta/2}$.","$$\int_0^ {ae^{{\pi}/4}} \int_{2 \log(r/a)}^{\pi/2}  f(r, \theta) \ r \ dr \ d \theta$$ In the above integral, for evaluating and sketching it, I have to change the order of integral, but I am not able to get the right approach. Is there any  general method to handle change of order of integration in polar coordinates? The answer is $\theta$ varies from $0$ to $\pi/2$ and $r$ varies from $a$ to $ae^{\theta/2}$.",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
34,Why is the Error surface for a 2 input neural network with 2 weights a parabolic bowl,Why is the Error surface for a 2 input neural network with 2 weights a parabolic bowl,,"I am new to machine learning and AI in general and had a quick question regarding the error function surface regarding a simple neural net: 2 input neural net After reading the following wiki: https://en.wikipedia.org/wiki/Backpropagation I understand that the error function for a specific input and output of this neural net could be described as: $$E = (t-y)^2$$ where $t$ is the expected value and $y$ is the value of the output node in the neural net. However, $y$ is a function of $w_1$, and $w_2$: $$y=x_1w_1 + x_2w_2$$ This I believe must mean that the final error function should be: $$E = (t - x_1w_1 - x_2w_2)^2$$ If I graph this function for specific values of $t, x_1$ and $x_2$, I am getting a surface that is a parabolic cylinder instead of a parabolic bowl. Is there anything that I might be looking at wrong? Thanks so much.","I am new to machine learning and AI in general and had a quick question regarding the error function surface regarding a simple neural net: 2 input neural net After reading the following wiki: https://en.wikipedia.org/wiki/Backpropagation I understand that the error function for a specific input and output of this neural net could be described as: $$E = (t-y)^2$$ where $t$ is the expected value and $y$ is the value of the output node in the neural net. However, $y$ is a function of $w_1$, and $w_2$: $$y=x_1w_1 + x_2w_2$$ This I believe must mean that the final error function should be: $$E = (t - x_1w_1 - x_2w_2)^2$$ If I graph this function for specific values of $t, x_1$ and $x_2$, I am getting a surface that is a parabolic cylinder instead of a parabolic bowl. Is there anything that I might be looking at wrong? Thanks so much.",,"['multivariable-calculus', 'machine-learning', 'neural-networks', 'artificial-intelligence']"
35,About the convergence of a series,About the convergence of a series,,"Let $\Omega_r =\{(x,y,z) \in \mathbb{R}^3 : 0<x,y,z , x^2+y^2+z^2<r^2 , x^2+y^2-z^2<0 \}$ And  $ f_r(x,y,z)= \begin{cases}        2z/(x^2+y^2+z^2)  & \text{if } (x,y,z) \in \Omega_r,  \\        0 & \text{otherwise}.      \end{cases}$ Show that the serie $\sum 2^{-n}f_n $ converges a.e. in $ \mathbb{R}^3 $ to an integrable function in $\mathbb{R}^3$ I have tried applying quotient criterion and it gives me $1/2 < 1$ so I can think that it is convergent. I have tried to find another function which is integrable and bounds my function but so for example if $ x, y$ or $z \ge1 $ then $f_n\le 1/2^{n-1} $, which is convergent and integrable and if $ z < 1 $ then $f_n< 1 /(2^{n-1}(x^2+y^2+z^2 ) $ but I don't know if that is enough. I need help. Thank you in advance.","Let $\Omega_r =\{(x,y,z) \in \mathbb{R}^3 : 0<x,y,z , x^2+y^2+z^2<r^2 , x^2+y^2-z^2<0 \}$ And  $ f_r(x,y,z)= \begin{cases}        2z/(x^2+y^2+z^2)  & \text{if } (x,y,z) \in \Omega_r,  \\        0 & \text{otherwise}.      \end{cases}$ Show that the serie $\sum 2^{-n}f_n $ converges a.e. in $ \mathbb{R}^3 $ to an integrable function in $\mathbb{R}^3$ I have tried applying quotient criterion and it gives me $1/2 < 1$ so I can think that it is convergent. I have tried to find another function which is integrable and bounds my function but so for example if $ x, y$ or $z \ge1 $ then $f_n\le 1/2^{n-1} $, which is convergent and integrable and if $ z < 1 $ then $f_n< 1 /(2^{n-1}(x^2+y^2+z^2 ) $ but I don't know if that is enough. I need help. Thank you in advance.",,"['real-analysis', 'multivariable-calculus']"
36,Evaluate the given complicated double integral,Evaluate the given complicated double integral,,"$$\int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}  e^{\frac{-1}{1-(x^2+y^2)}}\log\Big(\log\big(\frac{2}{\sqrt{(1-x)^2+(1-y)^2}}\big)\Big)\,dy\,dx.$$ I got this super hard integral while doing an exercise on distributions. The only thing I could think of is to transform it  into polar coordinates. It changes into $$\int_{0}^{2\pi} \int_{0}^{1}  e^{\frac{-1}{1-r^2}}\log\Big(\log\big(\frac{2}{\sqrt{r^2-2r\cos(\theta)-2r\sin(\theta)+2}}\big)\Big)\,r\,dr\,d\theta.$$ But even WolframAlpha couldn't compute this. Does anyone know how to evaluate this? Edit:- There is no closed form of this integral. If anyone has any suggestions regarding approximations, please comment below.","$$\int_{-1}^{1} \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}  e^{\frac{-1}{1-(x^2+y^2)}}\log\Big(\log\big(\frac{2}{\sqrt{(1-x)^2+(1-y)^2}}\big)\Big)\,dy\,dx.$$ I got this super hard integral while doing an exercise on distributions. The only thing I could think of is to transform it  into polar coordinates. It changes into $$\int_{0}^{2\pi} \int_{0}^{1}  e^{\frac{-1}{1-r^2}}\log\Big(\log\big(\frac{2}{\sqrt{r^2-2r\cos(\theta)-2r\sin(\theta)+2}}\big)\Big)\,r\,dr\,d\theta.$$ But even WolframAlpha couldn't compute this. Does anyone know how to evaluate this? Edit:- There is no closed form of this integral. If anyone has any suggestions regarding approximations, please comment below.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
37,double integral in polar coordinates formula proof,double integral in polar coordinates formula proof,,"I'm currently reading Calculus by Thomas and I cant seem to understand the argument for the following double integral in polar form formula . The text says that the double integral of a function over a region R in polar coordinates is defined as: $$\iint_R f(r,θ) dA = lim_{n\to \infty} \sum_{k=1}^n f(r_k, θ_k) \Delta A_k$$ where $\Delta A_k$ is the area of the kth polar rectangle when we divide the region R into n polar rectangles. The text then says that the area of each polar rectangle $\Delta A_k$ = $r_k$ $\Delta r$ $\Delta θ$ as per the following image from the book: sector image which gives $$\iint_R f(r,θ) dA = lim_{n\to \infty} \sum_{k=1}^n f(r_k, θ_k)r_k \Delta r \Delta θ$$ The text then says as $n\to \infty$  and  $\Delta r\to 0$ and $\Delta θ\to 0$, the sum converges to $$lim_{n\to \infty} \sum_{k=1}^n f(r_k, θ_k)r_k \Delta r \Delta θ =\iint_R f(r,θ)r dr dθ $$ Lastly the text says, "" A version of Fubini's theorem says that the limit approached by these sums can be evaluated by single integrations with respect to r and θ as $$\iint_R f(r,θ) dA = \int_{θ=a}^{θ=b} \int_{r = g_1(θ)}^{r= g_2(θ)} f(r,θ)r dr dθ  $$ "" My question is the integral in the last statement inferred from the riemann sum of the polar rectangles * height of the function or is it inferred like in case of rectangular coordinates where the inner integral represents the cross sectional area or does it come about as a particular case of a general change of variables.","I'm currently reading Calculus by Thomas and I cant seem to understand the argument for the following double integral in polar form formula . The text says that the double integral of a function over a region R in polar coordinates is defined as: $$\iint_R f(r,θ) dA = lim_{n\to \infty} \sum_{k=1}^n f(r_k, θ_k) \Delta A_k$$ where $\Delta A_k$ is the area of the kth polar rectangle when we divide the region R into n polar rectangles. The text then says that the area of each polar rectangle $\Delta A_k$ = $r_k$ $\Delta r$ $\Delta θ$ as per the following image from the book: sector image which gives $$\iint_R f(r,θ) dA = lim_{n\to \infty} \sum_{k=1}^n f(r_k, θ_k)r_k \Delta r \Delta θ$$ The text then says as $n\to \infty$  and  $\Delta r\to 0$ and $\Delta θ\to 0$, the sum converges to $$lim_{n\to \infty} \sum_{k=1}^n f(r_k, θ_k)r_k \Delta r \Delta θ =\iint_R f(r,θ)r dr dθ $$ Lastly the text says, "" A version of Fubini's theorem says that the limit approached by these sums can be evaluated by single integrations with respect to r and θ as $$\iint_R f(r,θ) dA = \int_{θ=a}^{θ=b} \int_{r = g_1(θ)}^{r= g_2(θ)} f(r,θ)r dr dθ  $$ "" My question is the integral in the last statement inferred from the riemann sum of the polar rectangles * height of the function or is it inferred like in case of rectangular coordinates where the inner integral represents the cross sectional area or does it come about as a particular case of a general change of variables.",,"['integration', 'multivariable-calculus']"
38,Show that a map $f\in C^1(\mathbb{C})$ is a polynomial of a degree less or equal to $n$.,Show that a map  is a polynomial of a degree less or equal to .,f\in C^1(\mathbb{C}) n,"Show that a map $f\in C^1(\mathbb{C})$ with $|f(z)|\leq C |z|^{\alpha}$ for some $a<n+1$, a constant $C>0$ and all $z\in\mathbb{C}$ is a polynomial of a degree less or equal to $n$. This was given to me as a hard bonus exercise as part of my real analysis/multivariable calc course (first year student). Sadly, too hard for me and we only touched upon very introductionary complex analysis via Taylor series etc. I'm not even seeing the right approach. I'd be very thankful for somebody illustrating to me how to proceed here. I've never seen a proof for an exercise of this kind before.","Show that a map $f\in C^1(\mathbb{C})$ with $|f(z)|\leq C |z|^{\alpha}$ for some $a<n+1$, a constant $C>0$ and all $z\in\mathbb{C}$ is a polynomial of a degree less or equal to $n$. This was given to me as a hard bonus exercise as part of my real analysis/multivariable calc course (first year student). Sadly, too hard for me and we only touched upon very introductionary complex analysis via Taylor series etc. I'm not even seeing the right approach. I'd be very thankful for somebody illustrating to me how to proceed here. I've never seen a proof for an exercise of this kind before.",,"['calculus', 'real-analysis', 'complex-analysis', 'multivariable-calculus', 'polynomials']"
39,An integral related to the pdf of the non-central chi-squared distribution,An integral related to the pdf of the non-central chi-squared distribution,,"Problem Statement I was attempting to produce a direct derivation of the pdf of the non-central chi-square distribution and came across the following integral, which I will denote as $g_{n-1}(x_n, \mathbf{\mu}^{(n)})$. \begin{align} \int_{(0,1)^{n-1}}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{k/2-1}}{\sqrt{1-x_k}}\ \right)\left(\ \prod^n_{k=1}\cosh\left(\mu_k\sqrt{x_n}\sqrt{\ (1-x_{k-1})\prod^{n-k}_{j=1}x_{n-j}}\right)\ \right)\ dx_1dx_2\cdots dx_{n-1} \end{align} Here $x_n>0$, $x_0:=0$ and $\mathbb{R}^n\ni\mu^{(n)}:=(\mu_1,\mu_2,\cdots,\mu_n)$. Thus far, I have succeeded in showing that if $f_n$ is the (Lebesgue) pdf of a random variable having a non-central chi-square distribution with $n$ degrees of freedom and non-centrality parameter $\delta_n:=\sum^n_{k=1}\mu_k^2$, then $$f_n(x_n, \delta_n)=\frac{e^{(-\delta_n+x_n)/2}}{(2\pi)^{n/2}}x_n^{n/2-1}g_{n-1}(x_n,\mu^{(n)})\tag{1}$$ and I am interested to know how one might complete the proof to show that $$f_n(x_n,\delta_n)=\frac{e^{(-\delta_n+x_n)/2}}{2}\left(\frac{x_n}{\delta_n}\right)^{n/4-1/2}I_{n/2-1}\left(\sqrt{\delta_nx_n}\right)\tag{2}$$ with $I_\nu$ being the modified Bessel function of the first kind. If one takes the following for granted: (*) Claim: The value of $g_{n-1}(x_n,\mu^{(n)})$ depends only on $x_n$ and $\delta_n$. then finding the value of $g_{n-1}(x_n,\mu^{(n)})$ would be a rather easy task as one could simply take $\mu_1=\sqrt{\delta_n}, \mu_2=\cdots=\mu_n=0$ without loss of generality and the integral simplifies to \begin{align} g_{n-1}(x_n, \mu^{(n)}) &=\int_{(0,1)^{n-1}}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{k/2-1}}{\sqrt{1-x_k}}\ \right)\cosh\left(\sqrt{\delta_n x_n}\sqrt{x_1\cdots x_{n-1}}\right)\ dx_1\cdots dx_{n-1}\\ &=\int_{(0,1)^{n-1}}\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{j+k/2-1}}{\sqrt{1-x_k}}\ \right)\ dx_1\cdots dx_{n-1}\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\int_{(0,1)^{n-1}}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{j+k/2-1}}{\sqrt{1-x_k}}\ \right)\ dx_1\cdots dx_{n-1}\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\ \prod^{n-1}_{k=1}\int^1_0\frac{x_k^{j+k/2-1}}{\sqrt{1-x_k}}\ dx_k\ \right)\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\ \prod^{n-1}_{k=1}\frac{\Gamma\left(j+\frac{k}{2}\right)\Gamma\left(\frac{1}{2}\right)}{\Gamma\left(j+\frac{k+1}{2}\right)} \right)\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\frac{\Gamma\left(j+\frac{1}{2}\right)}{\Gamma\left(j+\frac{n}{2}\right)}\cdot \pi^{(n-1)/2}\right)\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\frac{(2j)!}{4^jj!\Gamma\left(j+\frac{n}{2}\right)}\cdot \pi^{n/2}\right)\\ &=\pi^{n/2}\sum^\infty_{j=0}\frac{\left(\frac{\left(\sqrt{\delta_n x_n}\right)^2}{4}\right)^j}{j!\Gamma\left(j+\frac{n}{2}\right)}=\pi^{n/2}\left(\frac{2}{\sqrt{\delta_n x_n}}\right)^{n/2-1}I_{n/2-1}\left(\sqrt{\delta_n x_n}\right)\\ &=\frac{(2\pi)^{n/2}}{2(\delta_nx_n)^{n/4-1/2}}I_{n/2-1}\left(\sqrt{\delta_n x_n}\right) \end{align} which proves $(2)$ after substituting this into equation $(1)$. Hence, here are my two questions: $\text{a)}$ How might one rigorously justify the claim (*) above? I am most likely missing something but the veracity of the claim is not immediately obvious to me. $\text{b)}$ How might one evalaute the integral $g_{n-1}$ without exploiting the claim? Solution to Question b) for $n=2$ If $n=2$, we have $$g_1(x_2, \mu^{(2)})=\int^1_0\frac{\cosh(\mu_1\sqrt{x_2}\sqrt{x_1})\cosh(\mu_2\sqrt{x_2}\sqrt{1-x_1})}{\sqrt{x_1(1-x_1)}}\ dx_1$$ Let $(a,b)=(\mu_1\sqrt{x_2}, \mu_2\sqrt{x_2})$ and $2\omega=b+ia$. With the substitution $x_1=\sin^2\theta$ and then $z=e^{i\theta}$, we get \begin{align} g_1(x_2, \mu^{(2)}) &=\int^\frac{\pi}{2}_02\cosh(a\sin{\theta})\cosh(b\cos\theta)\ d\theta\\ &=\frac{1}{2}\int^{2\pi}_0\cosh(a\sin{\theta})\cosh(b\cos\theta)\ d\theta\\ &=\frac{1}{2}\int_{|z|=1}\cosh\left(a\cdot\frac{z-z^{-1}}{2i}\right)\cosh\left(b\cdot\frac{z+z^{-1}}{2}\right)\frac{dz}{iz}\\ &=\frac{1}{4}\int_{|z|=1}\left[\cosh\left(\bar{\omega}z+\omega z^{-1}\right)+\cosh\left(\omega z+\bar{\omega}z^{-1}\right)\right]\frac{dz}{iz} \end{align} Now deform the contour along the essential singularity $z=0$ to obtain \begin{align} \int_{|z|=1}\cosh\left(\bar{\omega}z+\omega z^{-1}\right)\frac{dz}{iz} &=\lim_{\epsilon\to 0^+}\int^{2\pi}_0\cosh\left(\bar{\omega}\epsilon e^{i\theta}+\omega\epsilon^{-1}e^{-i\theta}\right)\ d\theta\\ &=\lim_{\epsilon\to 0^+}\int^{2\pi}_0\sum^\infty_{k=0}\frac{1}{(2k)!}\sum^{2k}_{j=0}\binom{2k}{j}\bar{\omega}^{2k-j}\omega^j(\epsilon e^{i\theta})^{2k-2j}\ d\theta\\ &=\lim_{\epsilon\to 0^+}\sum^\infty_{k=0}\frac{1}{(2k)!}\sum^{2k}_{j=0}\binom{2k}{j}\bar{\omega}^{2k-j}\omega^j\int^{2\pi}_0(\epsilon e^{i\theta})^{2k-2j}\ d\theta\\ &=\lim_{\epsilon\to 0^+}\sum^\infty_{k=0}\frac{1}{(2k)!}\sum^{2k}_{j=0}\binom{2k}{j}\bar{\omega}^{2k-j}\omega^j(2\pi\mathbf{1}_{\{k\}}(j))\\ &=\sum^\infty_{k=0}\frac{1}{(2k)!}\binom{2k}{k}|\omega|^{2k}\cdot 2\pi\\ &=\sum^\infty_{k=0}\frac{\left(|\omega|^2\right)^k}{k!\Gamma(k+1)}\cdot 2\pi\\ &=2\pi I_0(2|\omega|) \end{align} Repeating the same argument, we clearly also have $$\int_{|z|=1}\cosh\left({\omega}z+\bar{\omega} z^{-1}\right)\frac{dz}{iz}=\int_{|z|=1}\cosh\left(\bar{\omega}z+\omega z^{-1}\right)\frac{dz}{iz}=2\pi I_0(2|\omega|)$$ and therefore, $$g_1(x_2, \mu^{(2)})=\pi I_0(2|\omega|)=\pi I_0\left(2.\sqrt{\frac{a^2+b^2}{4}}\right)=\pi I_0\left(\sqrt{x_2(\mu_1^2+\mu_2^2)}\right)=\pi I_0\left(\sqrt{\delta_2 x_2}\right)$$ Brief Derivation of Equation (1) In case anyone is interested, this is the sequence of transformations I used to arrive at equation $(1)$. Let $X_k$, $k=1,\cdots,n$, be independent random variables each distributed as $N(\mu_k,1)$. First, it is trivial to show that each $X_k^2$ has Lebesgue pdf $\frac{1}{\sqrt{2\pi x}}e^{-(x+\mu_k^2)/2}\cosh\left(\mu_k\sqrt{x}\right)\mathbf{1}_{(0,\infty)}(x)$. Integrating the joint density of $(X_1^2,\cdots,X_{n-1}^2, X_1^2+\cdots+X_n^2)$ gives us, for $x_n>0$, \begin{align} \small{f_n(x_n,\delta_n)} &\small{=\frac{e^{-(x_n+\delta_n)/2}}{(2\pi)^{n/2}}\int_{\mathbb{R}^{n-1}}\frac{\prod^{n-1}_{k=1}\cosh\left(\mu_k\sqrt{x_k-S_{n-1}\mathbf{1}_{\{n\}}(k)}\right)\mathbf{1}_{(0,\infty)}(x_k-S_{n-1}\mathbf{1}_{\{n\}}(k))}{\sqrt{x_1\cdots x_{n-1}(x_n-S_{n-1})}}\ dx_1\cdots dx_{n-1}}\\ &\small{=\frac{e^{-(x_n+\delta_n)/2}}{(2\pi)^{n/2}}\int^{x_n}_0\int^{x_{n-1}}_0\cdots\int^{x_2}_0\prod^{n}_{k=1}\frac{\cosh\left(\mu_k\sqrt{x_k-x_{k-1}}\right)}{\sqrt{x_k-x_{k-1}}}}\ dx_1\ dx_2\cdots dx_{n-1}\tag{i}\\ &\small{=\frac{e^{(-\delta_n+x_n)/2}}{(2\pi)^{n/2}}x_n^{n/2-1}g_{n-1}(x_n,\mu^{(n)})}\tag{ii} \end{align} where $x_0:=0$ and $S_{n-1}:=\sum^{n-1}_{k=1}x_k$. Explanation: $\text{(i)}$: Apply the transformation $(x_1,x_2,\cdots,x_{n-1})\mapsto (x_1, x_2-x_1,\cdots,x_{n-1}-x_{n-2})$. The determinant of the Jacobian for this transformation is $1$. $\text{(ii)}$: Apply the transformation $(x_1,x_2,\cdots,x_{n-1})\mapsto \left(\prod^n_{k=1}x_k, \prod^n_{k=2}x_k,\cdots, \prod^n_{k=n-1}x_k\right)$. The Jacobian for this transformation is an upper triangular matrix with diagonal elements $\prod^n_{k=2}x_k, \prod^n_{k=3}x_k,\cdots, \prod^n_{k=n}x_k$ and its determinant is thus $\prod^n_{k=2}x_k^{k-1}$. Further simplification gives us the stated result.","Problem Statement I was attempting to produce a direct derivation of the pdf of the non-central chi-square distribution and came across the following integral, which I will denote as $g_{n-1}(x_n, \mathbf{\mu}^{(n)})$. \begin{align} \int_{(0,1)^{n-1}}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{k/2-1}}{\sqrt{1-x_k}}\ \right)\left(\ \prod^n_{k=1}\cosh\left(\mu_k\sqrt{x_n}\sqrt{\ (1-x_{k-1})\prod^{n-k}_{j=1}x_{n-j}}\right)\ \right)\ dx_1dx_2\cdots dx_{n-1} \end{align} Here $x_n>0$, $x_0:=0$ and $\mathbb{R}^n\ni\mu^{(n)}:=(\mu_1,\mu_2,\cdots,\mu_n)$. Thus far, I have succeeded in showing that if $f_n$ is the (Lebesgue) pdf of a random variable having a non-central chi-square distribution with $n$ degrees of freedom and non-centrality parameter $\delta_n:=\sum^n_{k=1}\mu_k^2$, then $$f_n(x_n, \delta_n)=\frac{e^{(-\delta_n+x_n)/2}}{(2\pi)^{n/2}}x_n^{n/2-1}g_{n-1}(x_n,\mu^{(n)})\tag{1}$$ and I am interested to know how one might complete the proof to show that $$f_n(x_n,\delta_n)=\frac{e^{(-\delta_n+x_n)/2}}{2}\left(\frac{x_n}{\delta_n}\right)^{n/4-1/2}I_{n/2-1}\left(\sqrt{\delta_nx_n}\right)\tag{2}$$ with $I_\nu$ being the modified Bessel function of the first kind. If one takes the following for granted: (*) Claim: The value of $g_{n-1}(x_n,\mu^{(n)})$ depends only on $x_n$ and $\delta_n$. then finding the value of $g_{n-1}(x_n,\mu^{(n)})$ would be a rather easy task as one could simply take $\mu_1=\sqrt{\delta_n}, \mu_2=\cdots=\mu_n=0$ without loss of generality and the integral simplifies to \begin{align} g_{n-1}(x_n, \mu^{(n)}) &=\int_{(0,1)^{n-1}}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{k/2-1}}{\sqrt{1-x_k}}\ \right)\cosh\left(\sqrt{\delta_n x_n}\sqrt{x_1\cdots x_{n-1}}\right)\ dx_1\cdots dx_{n-1}\\ &=\int_{(0,1)^{n-1}}\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{j+k/2-1}}{\sqrt{1-x_k}}\ \right)\ dx_1\cdots dx_{n-1}\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\int_{(0,1)^{n-1}}\left(\ \prod^{n-1}_{k=1}\frac{x_k^{j+k/2-1}}{\sqrt{1-x_k}}\ \right)\ dx_1\cdots dx_{n-1}\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\ \prod^{n-1}_{k=1}\int^1_0\frac{x_k^{j+k/2-1}}{\sqrt{1-x_k}}\ dx_k\ \right)\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\ \prod^{n-1}_{k=1}\frac{\Gamma\left(j+\frac{k}{2}\right)\Gamma\left(\frac{1}{2}\right)}{\Gamma\left(j+\frac{k+1}{2}\right)} \right)\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\frac{\Gamma\left(j+\frac{1}{2}\right)}{\Gamma\left(j+\frac{n}{2}\right)}\cdot \pi^{(n-1)/2}\right)\\ &=\sum^\infty_{j=0}\frac{\left(\left(\sqrt{\delta_n x_n}\right)^2\right)^j}{(2j)!}\left(\frac{(2j)!}{4^jj!\Gamma\left(j+\frac{n}{2}\right)}\cdot \pi^{n/2}\right)\\ &=\pi^{n/2}\sum^\infty_{j=0}\frac{\left(\frac{\left(\sqrt{\delta_n x_n}\right)^2}{4}\right)^j}{j!\Gamma\left(j+\frac{n}{2}\right)}=\pi^{n/2}\left(\frac{2}{\sqrt{\delta_n x_n}}\right)^{n/2-1}I_{n/2-1}\left(\sqrt{\delta_n x_n}\right)\\ &=\frac{(2\pi)^{n/2}}{2(\delta_nx_n)^{n/4-1/2}}I_{n/2-1}\left(\sqrt{\delta_n x_n}\right) \end{align} which proves $(2)$ after substituting this into equation $(1)$. Hence, here are my two questions: $\text{a)}$ How might one rigorously justify the claim (*) above? I am most likely missing something but the veracity of the claim is not immediately obvious to me. $\text{b)}$ How might one evalaute the integral $g_{n-1}$ without exploiting the claim? Solution to Question b) for $n=2$ If $n=2$, we have $$g_1(x_2, \mu^{(2)})=\int^1_0\frac{\cosh(\mu_1\sqrt{x_2}\sqrt{x_1})\cosh(\mu_2\sqrt{x_2}\sqrt{1-x_1})}{\sqrt{x_1(1-x_1)}}\ dx_1$$ Let $(a,b)=(\mu_1\sqrt{x_2}, \mu_2\sqrt{x_2})$ and $2\omega=b+ia$. With the substitution $x_1=\sin^2\theta$ and then $z=e^{i\theta}$, we get \begin{align} g_1(x_2, \mu^{(2)}) &=\int^\frac{\pi}{2}_02\cosh(a\sin{\theta})\cosh(b\cos\theta)\ d\theta\\ &=\frac{1}{2}\int^{2\pi}_0\cosh(a\sin{\theta})\cosh(b\cos\theta)\ d\theta\\ &=\frac{1}{2}\int_{|z|=1}\cosh\left(a\cdot\frac{z-z^{-1}}{2i}\right)\cosh\left(b\cdot\frac{z+z^{-1}}{2}\right)\frac{dz}{iz}\\ &=\frac{1}{4}\int_{|z|=1}\left[\cosh\left(\bar{\omega}z+\omega z^{-1}\right)+\cosh\left(\omega z+\bar{\omega}z^{-1}\right)\right]\frac{dz}{iz} \end{align} Now deform the contour along the essential singularity $z=0$ to obtain \begin{align} \int_{|z|=1}\cosh\left(\bar{\omega}z+\omega z^{-1}\right)\frac{dz}{iz} &=\lim_{\epsilon\to 0^+}\int^{2\pi}_0\cosh\left(\bar{\omega}\epsilon e^{i\theta}+\omega\epsilon^{-1}e^{-i\theta}\right)\ d\theta\\ &=\lim_{\epsilon\to 0^+}\int^{2\pi}_0\sum^\infty_{k=0}\frac{1}{(2k)!}\sum^{2k}_{j=0}\binom{2k}{j}\bar{\omega}^{2k-j}\omega^j(\epsilon e^{i\theta})^{2k-2j}\ d\theta\\ &=\lim_{\epsilon\to 0^+}\sum^\infty_{k=0}\frac{1}{(2k)!}\sum^{2k}_{j=0}\binom{2k}{j}\bar{\omega}^{2k-j}\omega^j\int^{2\pi}_0(\epsilon e^{i\theta})^{2k-2j}\ d\theta\\ &=\lim_{\epsilon\to 0^+}\sum^\infty_{k=0}\frac{1}{(2k)!}\sum^{2k}_{j=0}\binom{2k}{j}\bar{\omega}^{2k-j}\omega^j(2\pi\mathbf{1}_{\{k\}}(j))\\ &=\sum^\infty_{k=0}\frac{1}{(2k)!}\binom{2k}{k}|\omega|^{2k}\cdot 2\pi\\ &=\sum^\infty_{k=0}\frac{\left(|\omega|^2\right)^k}{k!\Gamma(k+1)}\cdot 2\pi\\ &=2\pi I_0(2|\omega|) \end{align} Repeating the same argument, we clearly also have $$\int_{|z|=1}\cosh\left({\omega}z+\bar{\omega} z^{-1}\right)\frac{dz}{iz}=\int_{|z|=1}\cosh\left(\bar{\omega}z+\omega z^{-1}\right)\frac{dz}{iz}=2\pi I_0(2|\omega|)$$ and therefore, $$g_1(x_2, \mu^{(2)})=\pi I_0(2|\omega|)=\pi I_0\left(2.\sqrt{\frac{a^2+b^2}{4}}\right)=\pi I_0\left(\sqrt{x_2(\mu_1^2+\mu_2^2)}\right)=\pi I_0\left(\sqrt{\delta_2 x_2}\right)$$ Brief Derivation of Equation (1) In case anyone is interested, this is the sequence of transformations I used to arrive at equation $(1)$. Let $X_k$, $k=1,\cdots,n$, be independent random variables each distributed as $N(\mu_k,1)$. First, it is trivial to show that each $X_k^2$ has Lebesgue pdf $\frac{1}{\sqrt{2\pi x}}e^{-(x+\mu_k^2)/2}\cosh\left(\mu_k\sqrt{x}\right)\mathbf{1}_{(0,\infty)}(x)$. Integrating the joint density of $(X_1^2,\cdots,X_{n-1}^2, X_1^2+\cdots+X_n^2)$ gives us, for $x_n>0$, \begin{align} \small{f_n(x_n,\delta_n)} &\small{=\frac{e^{-(x_n+\delta_n)/2}}{(2\pi)^{n/2}}\int_{\mathbb{R}^{n-1}}\frac{\prod^{n-1}_{k=1}\cosh\left(\mu_k\sqrt{x_k-S_{n-1}\mathbf{1}_{\{n\}}(k)}\right)\mathbf{1}_{(0,\infty)}(x_k-S_{n-1}\mathbf{1}_{\{n\}}(k))}{\sqrt{x_1\cdots x_{n-1}(x_n-S_{n-1})}}\ dx_1\cdots dx_{n-1}}\\ &\small{=\frac{e^{-(x_n+\delta_n)/2}}{(2\pi)^{n/2}}\int^{x_n}_0\int^{x_{n-1}}_0\cdots\int^{x_2}_0\prod^{n}_{k=1}\frac{\cosh\left(\mu_k\sqrt{x_k-x_{k-1}}\right)}{\sqrt{x_k-x_{k-1}}}}\ dx_1\ dx_2\cdots dx_{n-1}\tag{i}\\ &\small{=\frac{e^{(-\delta_n+x_n)/2}}{(2\pi)^{n/2}}x_n^{n/2-1}g_{n-1}(x_n,\mu^{(n)})}\tag{ii} \end{align} where $x_0:=0$ and $S_{n-1}:=\sum^{n-1}_{k=1}x_k$. Explanation: $\text{(i)}$: Apply the transformation $(x_1,x_2,\cdots,x_{n-1})\mapsto (x_1, x_2-x_1,\cdots,x_{n-1}-x_{n-2})$. The determinant of the Jacobian for this transformation is $1$. $\text{(ii)}$: Apply the transformation $(x_1,x_2,\cdots,x_{n-1})\mapsto \left(\prod^n_{k=1}x_k, \prod^n_{k=2}x_k,\cdots, \prod^n_{k=n-1}x_k\right)$. The Jacobian for this transformation is an upper triangular matrix with diagonal elements $\prod^n_{k=2}x_k, \prod^n_{k=3}x_k,\cdots, \prod^n_{k=n}x_k$ and its determinant is thus $\prod^n_{k=2}x_k^{k-1}$. Further simplification gives us the stated result.",,"['integration', 'complex-analysis', 'multivariable-calculus', 'probability-distributions', 'definite-integrals']"
40,Second order Taylor expansion and Laplacian relation in d dimensions,Second order Taylor expansion and Laplacian relation in d dimensions,,"My question concerns how the second-order term in a Taylor expansion of a scalar-valued function of a vector can be represented by a Laplacian instead of the product of vectors times dels. The origin of this question has led me to expect the numerical coefficient on the Laplacian term will have a denominator divisible by d, the dimension of the vector space. That is because the expansion below (with some assumptions) generates a differential equation solved by a Gaussian probability distribution with a variance scaled by d. Thomas Witten in his book ""Structured Fluids"" looks at an unspecified function f of the magnitude of an $\mathbb{R^3}$ vector. He Taylor expands this function to second order to derive a differential equation. Here's a simplified version of his statement, and it's the second order term that's making me curious. $p(|\vec{r}-\vec{r}_1|) = p(|\vec{r}|)-\vec{r}_1\cdot \nabla p(|\vec{r}|) + \frac{1}{6}r_1^2\nabla^2p(|\vec{r}|) $ + ... I typically write my Taylor expansions as  $f(\vec{x}-\vec{a}) = f(\vec{x}) - \vec{a}\cdot\nabla f(\vec{x}) + \frac{1}{2}(\vec{a}\cdot\nabla) (\vec{a}\cdot\nabla) f(\vec{x}) + ...$ How can I transform from the general second-order Taylor expansion to the quoted expansion with a Laplacian and different numerical prefactor? What changes if we look at a vector argument in $\mathbb{R^d}$?","My question concerns how the second-order term in a Taylor expansion of a scalar-valued function of a vector can be represented by a Laplacian instead of the product of vectors times dels. The origin of this question has led me to expect the numerical coefficient on the Laplacian term will have a denominator divisible by d, the dimension of the vector space. That is because the expansion below (with some assumptions) generates a differential equation solved by a Gaussian probability distribution with a variance scaled by d. Thomas Witten in his book ""Structured Fluids"" looks at an unspecified function f of the magnitude of an $\mathbb{R^3}$ vector. He Taylor expands this function to second order to derive a differential equation. Here's a simplified version of his statement, and it's the second order term that's making me curious. $p(|\vec{r}-\vec{r}_1|) = p(|\vec{r}|)-\vec{r}_1\cdot \nabla p(|\vec{r}|) + \frac{1}{6}r_1^2\nabla^2p(|\vec{r}|) $ + ... I typically write my Taylor expansions as  $f(\vec{x}-\vec{a}) = f(\vec{x}) - \vec{a}\cdot\nabla f(\vec{x}) + \frac{1}{2}(\vec{a}\cdot\nabla) (\vec{a}\cdot\nabla) f(\vec{x}) + ...$ How can I transform from the general second-order Taylor expansion to the quoted expansion with a Laplacian and different numerical prefactor? What changes if we look at a vector argument in $\mathbb{R^d}$?",,"['multivariable-calculus', 'taylor-expansion', 'laplacian']"
41,Why is the Jacobian determinant of a map the reciprocal of the Jacobian determinant of the inverse map?,Why is the Jacobian determinant of a map the reciprocal of the Jacobian determinant of the inverse map?,,"Consider a mapping $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \begin{align} g(u,v) &= (x, y)\\ &=(f(u,v), \;g(u, v)) \end{align} Consider the inverse mapping $g^{-1} = \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \begin{align} g^{-1}(x,y) &= (u,v) \\ &= (h(x,y), \;I(x,y)) \end{align} Question Why is it that the determinant of the Jacobian of $g$ denoted as $|\text{Jac}\; g|$ equal to $1$ over the determinant of the Jacobian of $g^{-1}$ denoted $|\text{Jac}\; g^{-1}|$? $$ |\text{Jac}\;g| = \frac{1}{|\text{Jac}\;g^{-1}|}$$ You could do the computation. But I'm not sure how I would connect the two sides of the equality. The left side is $x_uy_v - y_ux_v$. But I can't seem to get this equal to the other side. Thoughts Consider a function $y = f(x)$. Consider it's inverse function $x = f^{-1}(y)$. The two are related as follows $$\frac{df^{-1}}{dy}\Bigg|_y = \frac{1}{\frac{df}{dx}\Big|_x} $$ where the evaluation values $x,y$ are images of one another (people write this formula in many different ways). This last equation makes intuitive sense to me. Draw a graph of $f(x)$. To find the graph of the inverse function, all you have to do is rotate this graph $90^\circ$ counterclockwise. If you draw a tangent line to the graph of $f(x)$, such an instantaneous line has some rise/run. Once you rotate the graph $90^\circ$ counterclockwise to view the inverse graph, this tangent line now has a slope of run/rise. Therefore the formula follows. Is there such an interpretation for my question? I tried writing out both determinants. However, it's not clear to me how I would related $\partial x/\partial u$ to $\partial u/\partial x$. You would think that $$\frac{\partial x}{\partial u}\Bigg|_u = \frac{1}{\frac{\partial u }{\partial x}\Big|_x} $$ This would be true given $x = f(u)$. However we have $x = f(u,v)$ and $u = h(x,y)$ and I'm not sure of the relationship between the two. Thanks in advance","Consider a mapping $g: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \begin{align} g(u,v) &= (x, y)\\ &=(f(u,v), \;g(u, v)) \end{align} Consider the inverse mapping $g^{-1} = \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \begin{align} g^{-1}(x,y) &= (u,v) \\ &= (h(x,y), \;I(x,y)) \end{align} Question Why is it that the determinant of the Jacobian of $g$ denoted as $|\text{Jac}\; g|$ equal to $1$ over the determinant of the Jacobian of $g^{-1}$ denoted $|\text{Jac}\; g^{-1}|$? $$ |\text{Jac}\;g| = \frac{1}{|\text{Jac}\;g^{-1}|}$$ You could do the computation. But I'm not sure how I would connect the two sides of the equality. The left side is $x_uy_v - y_ux_v$. But I can't seem to get this equal to the other side. Thoughts Consider a function $y = f(x)$. Consider it's inverse function $x = f^{-1}(y)$. The two are related as follows $$\frac{df^{-1}}{dy}\Bigg|_y = \frac{1}{\frac{df}{dx}\Big|_x} $$ where the evaluation values $x,y$ are images of one another (people write this formula in many different ways). This last equation makes intuitive sense to me. Draw a graph of $f(x)$. To find the graph of the inverse function, all you have to do is rotate this graph $90^\circ$ counterclockwise. If you draw a tangent line to the graph of $f(x)$, such an instantaneous line has some rise/run. Once you rotate the graph $90^\circ$ counterclockwise to view the inverse graph, this tangent line now has a slope of run/rise. Therefore the formula follows. Is there such an interpretation for my question? I tried writing out both determinants. However, it's not clear to me how I would related $\partial x/\partial u$ to $\partial u/\partial x$. You would think that $$\frac{\partial x}{\partial u}\Bigg|_u = \frac{1}{\frac{\partial u }{\partial x}\Big|_x} $$ This would be true given $x = f(u)$. However we have $x = f(u,v)$ and $u = h(x,y)$ and I'm not sure of the relationship between the two. Thanks in advance",,"['matrices', 'multivariable-calculus', 'determinant', 'jacobian']"
42,"Reasoning for the function $f(x,y)=x^{2}e^{y}$ is differentiable on $\mathbb{R}^{2}$.",Reasoning for the function  is differentiable on .,"f(x,y)=x^{2}e^{y} \mathbb{R}^{2}","Reasoning for the function $f(x,y)=x^{2}e^{y}$ is differentiable on $\mathbb{R}^{2}$. Explanation : the correct reason should be $f_{x}=2xe^{y}$ and $f_{y}=x^{2}e^{y}$ are both continuous on $\mathbb{R}^{2}$ , then $f$ is differentiable at every point on $\mathbb{R}^{2}$. But my friend told me : $f_{x}$ and $f_{y}$ both exist at each point $(x,y)\in\mathbb{R}^{2}$ and for all point $(x,y)\in\mathbb{R}^{2}$, we have $$f_{xy}=2xe^{y}=f_{yx}$$ Hence , $f$ is differentiable  on $\mathbb{R}^{2}$. I confused . Is this reason valid for testing function differentiable  ? Or is there a counterexample for this ?","Reasoning for the function $f(x,y)=x^{2}e^{y}$ is differentiable on $\mathbb{R}^{2}$. Explanation : the correct reason should be $f_{x}=2xe^{y}$ and $f_{y}=x^{2}e^{y}$ are both continuous on $\mathbb{R}^{2}$ , then $f$ is differentiable at every point on $\mathbb{R}^{2}$. But my friend told me : $f_{x}$ and $f_{y}$ both exist at each point $(x,y)\in\mathbb{R}^{2}$ and for all point $(x,y)\in\mathbb{R}^{2}$, we have $$f_{xy}=2xe^{y}=f_{yx}$$ Hence , $f$ is differentiable  on $\mathbb{R}^{2}$. I confused . Is this reason valid for testing function differentiable  ? Or is there a counterexample for this ?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
43,Proving a property with logarithm,Proving a property with logarithm,,"For a polynomial $V(x)$ of degree $r$ defined in $\mathbb{R}^d$, we denote $$A_V(x)=\sum_{1\le |\alpha|\le r}|\partial^{\alpha}_xV(x)|^{\frac{1}{|\alpha|}}$$ I want to prove the following result: there exists a constant $c>0$ such that for all $x,x_0\in \mathbb{R}^d$ $$ |x-x_0|\le \Bigl(\frac{A_V(x)}{ \log(A_V(x))}\Bigr)^{-1}$$ implies $$ \left(\frac{\dfrac{A_V(x)}{\log(A_V(x))}}{\dfrac{A_V(x_0)}{ \log(A_V(x_0))}}\right)^{\pm 1}\le c. $$ Here we consider polynomials $V$ such that $A_V(x)\neq 1$ for all $x\in \mathbb{R}^d$. Please help me to do so. Thanks in advance","For a polynomial $V(x)$ of degree $r$ defined in $\mathbb{R}^d$, we denote $$A_V(x)=\sum_{1\le |\alpha|\le r}|\partial^{\alpha}_xV(x)|^{\frac{1}{|\alpha|}}$$ I want to prove the following result: there exists a constant $c>0$ such that for all $x,x_0\in \mathbb{R}^d$ $$ |x-x_0|\le \Bigl(\frac{A_V(x)}{ \log(A_V(x))}\Bigr)^{-1}$$ implies $$ \left(\frac{\dfrac{A_V(x)}{\log(A_V(x))}}{\dfrac{A_V(x_0)}{ \log(A_V(x_0))}}\right)^{\pm 1}\le c. $$ Here we consider polynomials $V$ such that $A_V(x)\neq 1$ for all $x\in \mathbb{R}^d$. Please help me to do so. Thanks in advance",,['multivariable-calculus']
44,Study the existence of directional derivatives at a point and determine if it is differentiable,Study the existence of directional derivatives at a point and determine if it is differentiable,,"I have the following function: $$f(x,y)=\begin{cases}x^2+y&\wedge&x\geq 1\\3x-y&\wedge&x<1\end{cases}$$ and they ask me the following: Continuity domain Existence of directional derivatives in the point $(1,1)$ in all directions $\vec v=(a,b)$ What can you say about the differentiability of the function at that point? I made the map of continuity of $f$ and the differents directions of the unit versor $\vec v$ in $(1,1)$: $D_f=\mathbb R^2$. We need to calculate $f'((1,1);(a,b))$ (if exists): $\color{red}{(1)}$ If $a=0\;\;(b=\pm 1):\displaystyle\lim_{h\to 0}{\dfrac{1^2+(1+hb)-2}h}=\boxed b$ $(2)$ If $-1\leq a<0\;\;(\forall b):\displaystyle\lim_{h\to 0}{\dfrac{3+3ha-1-hb-2}h}=\displaystyle\lim_{h\to 0}{\dfrac{h(3a-b)}h}=\boxed{3a-b}$. $\color{blue}{(3)}$ If $0<a\leq 1\;\;(\forall b):\displaystyle\lim_{h\to 0}{\dfrac{{(1+ha)}^2+(1+hb)-2}h}=\displaystyle\lim_{h\to 0}{\dfrac{h(2a+ha^2+b)}h}=\boxed{2a+b}$. With this I can affirm that $f$ has directional derivatives in the point $(1,1)$, which are $$\boxed{\begin{array}{llll} f'((1,1);(0,\pm 1))&=&b&\\ f'((1,1);(a,b))&=&3a-b&\text{ if }a\in[-1,0)\\ f'((1,1);(a,b))&=&2a+b&\text{ if }a\in(0,1]. \end{array}}$$ EDIT : I'm going to skip some steps and I'll go directly to calculate the formal limit. For that I need the partials, which are $${f'}_x(1,1)=2\qquad{f'}_y(1,1)=1.$$ So by definition: $$\displaystyle\lim_{(x,y)\to (1,1)}{\dfrac{x^2+y-[2+2(x-1)+(y-1)]}{\sqrt{{(x-1)}^2+{(y-1)}^2}}}=\displaystyle\lim_{(x,y)\to (1,1)}{\dfrac{x^2+1}{\sqrt{{(x-1)}^2+{(y-1)}^2}}}=\ldots$$ and now I don't know how to proceed :( . Is it okay or did I lose my head? Thanks!","I have the following function: $$f(x,y)=\begin{cases}x^2+y&\wedge&x\geq 1\\3x-y&\wedge&x<1\end{cases}$$ and they ask me the following: Continuity domain Existence of directional derivatives in the point $(1,1)$ in all directions $\vec v=(a,b)$ What can you say about the differentiability of the function at that point? I made the map of continuity of $f$ and the differents directions of the unit versor $\vec v$ in $(1,1)$: $D_f=\mathbb R^2$. We need to calculate $f'((1,1);(a,b))$ (if exists): $\color{red}{(1)}$ If $a=0\;\;(b=\pm 1):\displaystyle\lim_{h\to 0}{\dfrac{1^2+(1+hb)-2}h}=\boxed b$ $(2)$ If $-1\leq a<0\;\;(\forall b):\displaystyle\lim_{h\to 0}{\dfrac{3+3ha-1-hb-2}h}=\displaystyle\lim_{h\to 0}{\dfrac{h(3a-b)}h}=\boxed{3a-b}$. $\color{blue}{(3)}$ If $0<a\leq 1\;\;(\forall b):\displaystyle\lim_{h\to 0}{\dfrac{{(1+ha)}^2+(1+hb)-2}h}=\displaystyle\lim_{h\to 0}{\dfrac{h(2a+ha^2+b)}h}=\boxed{2a+b}$. With this I can affirm that $f$ has directional derivatives in the point $(1,1)$, which are $$\boxed{\begin{array}{llll} f'((1,1);(0,\pm 1))&=&b&\\ f'((1,1);(a,b))&=&3a-b&\text{ if }a\in[-1,0)\\ f'((1,1);(a,b))&=&2a+b&\text{ if }a\in(0,1]. \end{array}}$$ EDIT : I'm going to skip some steps and I'll go directly to calculate the formal limit. For that I need the partials, which are $${f'}_x(1,1)=2\qquad{f'}_y(1,1)=1.$$ So by definition: $$\displaystyle\lim_{(x,y)\to (1,1)}{\dfrac{x^2+y-[2+2(x-1)+(y-1)]}{\sqrt{{(x-1)}^2+{(y-1)}^2}}}=\displaystyle\lim_{(x,y)\to (1,1)}{\dfrac{x^2+1}{\sqrt{{(x-1)}^2+{(y-1)}^2}}}=\ldots$$ and now I don't know how to proceed :( . Is it okay or did I lose my head? Thanks!",,"['limits', 'multivariable-calculus']"
45,Calculating total derivative from partial derivatives,Calculating total derivative from partial derivatives,,"I have a univariate function of a multivariate function: $$F(g(x,y,z)),$$ $$g(x,y,z)=2x+2y+z.$$ I know $F'(x)$, $F'(y)$ and $F'(z)$. I need to know $F'(g)$. How can I do that? I'm confused, but it seems like the chain rule does not work directly here, nor the total derivative as a sum of partial derivatives.","I have a univariate function of a multivariate function: $$F(g(x,y,z)),$$ $$g(x,y,z)=2x+2y+z.$$ I know $F'(x)$, $F'(y)$ and $F'(z)$. I need to know $F'(g)$. How can I do that? I'm confused, but it seems like the chain rule does not work directly here, nor the total derivative as a sum of partial derivatives.",,"['calculus', 'multivariable-calculus']"
46,Find geometric interpretation of differential operators in vector fields.,Find geometric interpretation of differential operators in vector fields.,,"I'm trying to create some notes on multivariable calculus, but It has been hard to find the geometric point of view of this: Jacobian Matrix (Derivative of a VF) Laplacian Nabla operator Curl Divergence And most commonly some useful identities such as the curl of a gradient, divergence of a gradient. Many of the book that I have look up are theorical based. And yep, many videos on YouTube show kind of the overall the physical phenomena but they assume some background on physics. :( I'm interested in explaining this operators with the help of Mathematica or any kind of graphics to show how they work on the vector field. For example, I do know that the divergence is a measurement of how much flow enters on the neighborhood of P compared to how much it leaves. And the curl measures the rotation of the vector field in the neighborhood around P.  But I can't think of a graph to explain this. Neither I have found such a explanation for the Jacobian matrix besides being the linear map which works as a the derivative nor the laplacian or nabla operators. I would love if you can share some of your favorite books or any idea, having in mind that my knowledge in physics is merely basic.","I'm trying to create some notes on multivariable calculus, but It has been hard to find the geometric point of view of this: Jacobian Matrix (Derivative of a VF) Laplacian Nabla operator Curl Divergence And most commonly some useful identities such as the curl of a gradient, divergence of a gradient. Many of the book that I have look up are theorical based. And yep, many videos on YouTube show kind of the overall the physical phenomena but they assume some background on physics. :( I'm interested in explaining this operators with the help of Mathematica or any kind of graphics to show how they work on the vector field. For example, I do know that the divergence is a measurement of how much flow enters on the neighborhood of P compared to how much it leaves. And the curl measures the rotation of the vector field in the neighborhood around P.  But I can't think of a graph to explain this. Neither I have found such a explanation for the Jacobian matrix besides being the linear map which works as a the derivative nor the laplacian or nabla operators. I would love if you can share some of your favorite books or any idea, having in mind that my knowledge in physics is merely basic.",,"['multivariable-calculus', 'vector-analysis', 'education', 'vector-fields']"
47,"If the Jacobian is the first derivative, the Hessian is the second derivative, what is the third derivative called?","If the Jacobian is the first derivative, the Hessian is the second derivative, what is the third derivative called?",,"I'm wondering about the following: If the Jacobian is the first derivative, the Hessian is the second derivative, what is the third derivative called?","I'm wondering about the following: If the Jacobian is the first derivative, the Hessian is the second derivative, what is the third derivative called?",,"['multivariable-calculus', 'derivatives', 'terminology', 'matrix-calculus']"
48,For which of the following function domain is open and bounded region in $\mathbb R^2$,For which of the following function domain is open and bounded region in,\mathbb R^2,"For which of the following function domain is open and bounded  region   in $\mathbb R^2$ 1.$f(x,y)=\sin^{-1}(x-y)$ 2.$f(x,y)=4x^2+9y^2$ 3.$f(x,y)=\frac{1}{\sqrt {16-x^2-y^2}}$ 4.$f(x,y)=\ln(x^2+y^2)$ Solutions: Argument 1: $f$ is well defined if $x-y\in [-1,1]$,which is correct for $\mathbb R^2$,which is unbounded.Hence,it is incorrect Argument 2: Since $4x^2+9y^2$ is valid for all $(x,y)\in \mathbb R^2$.Hence,the domain of $f$ is $\mathbb R^2$,which is both open and closed,but it is unbounded.Hence,option (2) is incorrect. Argument 3: $f$ is well defined if $16-x^2-y^2> 0$ i.e., $16> x^2+y^2 $,this region is open in $\mathbb R^2$ and can be bounded by the circle $x^2+y^2=17$.Hence,$16> x^2+y^2 $ is open and bounded in $\mathbb R^2$.Hence,option (3) is correct. Argument : $f$ is well defined for $\mathbb R^2$-{(0,0)},which is unbounded in $\mathbb R^2$.Hence,option (4) is incorrect . Please check my arguments...whether they are correct or not. If there are  some other arguments please do share... thank you!!","For which of the following function domain is open and bounded  region   in $\mathbb R^2$ 1.$f(x,y)=\sin^{-1}(x-y)$ 2.$f(x,y)=4x^2+9y^2$ 3.$f(x,y)=\frac{1}{\sqrt {16-x^2-y^2}}$ 4.$f(x,y)=\ln(x^2+y^2)$ Solutions: Argument 1: $f$ is well defined if $x-y\in [-1,1]$,which is correct for $\mathbb R^2$,which is unbounded.Hence,it is incorrect Argument 2: Since $4x^2+9y^2$ is valid for all $(x,y)\in \mathbb R^2$.Hence,the domain of $f$ is $\mathbb R^2$,which is both open and closed,but it is unbounded.Hence,option (2) is incorrect. Argument 3: $f$ is well defined if $16-x^2-y^2> 0$ i.e., $16> x^2+y^2 $,this region is open in $\mathbb R^2$ and can be bounded by the circle $x^2+y^2=17$.Hence,$16> x^2+y^2 $ is open and bounded in $\mathbb R^2$.Hence,option (3) is correct. Argument : $f$ is well defined for $\mathbb R^2$-{(0,0)},which is unbounded in $\mathbb R^2$.Hence,option (4) is incorrect . Please check my arguments...whether they are correct or not. If there are  some other arguments please do share... thank you!!",,"['real-analysis', 'functions', 'multivariable-calculus', 'trigonometry']"
49,How is the equation $z=xy$ a hyperbolic paraboloid?,How is the equation  a hyperbolic paraboloid?,z=xy,The general equation for a hyperbolic paraboloid is $z - z_{0} = \frac{(x - x_{0})^{2}}{a^{2}} -\frac{(y - y_{0})^{2}}{b^{2}}$. How is the equation $z=xy$ a hyperbolic paraboloid? Is there any algebra that can be applied to it to get its general form?,The general equation for a hyperbolic paraboloid is $z - z_{0} = \frac{(x - x_{0})^{2}}{a^{2}} -\frac{(y - y_{0})^{2}}{b^{2}}$. How is the equation $z=xy$ a hyperbolic paraboloid? Is there any algebra that can be applied to it to get its general form?,,['multivariable-calculus']
50,Simple 2d-graph question,Simple 2d-graph question,,"Say, I have the line in $R^3$ defined by the parametric equations, $x=t$, $y=0$, and $z=t$.  How would I draw this line in 2-d, using the x and z coordinate axis? I'm just brushing up a bit on my calculus 3. Attempt: To graph it,  I think I should just pick different values of t starting from 0 (or can I pick negative values) and see what values x and z takes on.  Thus, it would look just like the straight line going through the origin, the same as the line y=x in the x-y plane.  Am I correct?","Say, I have the line in $R^3$ defined by the parametric equations, $x=t$, $y=0$, and $z=t$.  How would I draw this line in 2-d, using the x and z coordinate axis? I'm just brushing up a bit on my calculus 3. Attempt: To graph it,  I think I should just pick different values of t starting from 0 (or can I pick negative values) and see what values x and z takes on.  Thus, it would look just like the straight line going through the origin, the same as the line y=x in the x-y plane.  Am I correct?",,"['calculus', 'algebra-precalculus']"
51,"Evaluating $\iint xy\,\mathrm{d}x\,\mathrm{d}y$ over an elliptical disk",Evaluating  over an elliptical disk,"\iint xy\,\mathrm{d}x\,\mathrm{d}y","Evaluate $$I=\iint_A xy\,\mathrm{d}x\,\mathrm{d}y$$ where $$A=\{(x,y): ax^2+2hxy+by^2\le r^2, a>0, ab-h^2>0\}$$ I tried diagonalising the quadratic form $h(x,y)=ax^2+2hxy+by^2$ to some $$g(u,v)=u^2+v^2$$ where $$\begin{pmatrix}u\\v\end{pmatrix}=\begin{pmatrix} \sqrt{a} &  \frac{h}{\sqrt{a}}\\\ 0 &  \sqrt{\frac{ab-h^2}{a}} \\\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$$ The Jacobian of the transformation $(x,y)\to(u,v)$ is $$\det(J) = \frac{1}{\sqrt{ab-h^2}}$$ The integral then reduces to $$I=\iint_{u^2+v^2\le r^2}\left(\frac{uv}{\sqrt{ab-h^2}}-\frac{hv^2}{ab-h^2}\right)\frac{1}{\sqrt{ab-h^2}}\,\mathrm{d}u\,\mathrm{d}v$$ Now I could apply a polar transformation to get the answer. But this method seems a bit cumbersome, especially solving for $x$ and $y$ from $u,v$. Is there an obvious alternate solution? The problem can also be seen as finding $\mathbb{E}(XY)$ where $(X,Y)$ is jointly uniform over $A$. EDIT. Proceeding in the way I have done so far, I get the final answer as $$I=-\frac{\pi r^4h}{4(ab-h^2)^{3/2}}$$ Is this indeed the correct answer?","Evaluate $$I=\iint_A xy\,\mathrm{d}x\,\mathrm{d}y$$ where $$A=\{(x,y): ax^2+2hxy+by^2\le r^2, a>0, ab-h^2>0\}$$ I tried diagonalising the quadratic form $h(x,y)=ax^2+2hxy+by^2$ to some $$g(u,v)=u^2+v^2$$ where $$\begin{pmatrix}u\\v\end{pmatrix}=\begin{pmatrix} \sqrt{a} &  \frac{h}{\sqrt{a}}\\\ 0 &  \sqrt{\frac{ab-h^2}{a}} \\\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}$$ The Jacobian of the transformation $(x,y)\to(u,v)$ is $$\det(J) = \frac{1}{\sqrt{ab-h^2}}$$ The integral then reduces to $$I=\iint_{u^2+v^2\le r^2}\left(\frac{uv}{\sqrt{ab-h^2}}-\frac{hv^2}{ab-h^2}\right)\frac{1}{\sqrt{ab-h^2}}\,\mathrm{d}u\,\mathrm{d}v$$ Now I could apply a polar transformation to get the answer. But this method seems a bit cumbersome, especially solving for $x$ and $y$ from $u,v$. Is there an obvious alternate solution? The problem can also be seen as finding $\mathbb{E}(XY)$ where $(X,Y)$ is jointly uniform over $A$. EDIT. Proceeding in the way I have done so far, I get the final answer as $$I=-\frac{\pi r^4h}{4(ab-h^2)^{3/2}}$$ Is this indeed the correct answer?",,"['integration', 'multivariable-calculus', 'definite-integrals']"
52,Calculation on gradient of the dot product on $\mathbb R^n$,Calculation on gradient of the dot product on,\mathbb R^n,"Let $f(x,y)$ denote the standard dot product of vectors on $\mathbb{R^n}$. I'd like to confirm the formula that I have for the gradient of the dot product is correct. First, I consider this to be a map from $\mathbb{R}^{2n}$ to $\mathbb{R}$ rather than one from $\mathbb{R^n} \times \mathbb{R}^n$ to $\mathbb{R}$. By this I mean: $$ f(x,y) = f(x_1,...,x_n,y_1,...,y_n) = \sum_{i = 1}^{n}x_iy_i $$ Taking partial derivatives with respect to the $2n$ variables, we have that: $$ \nabla f(x,y) = (y_1,...,y_n,x_1,...,x_n) = (y,x) $$ Is this solution correct? I have not seen anything like this online.","Let $f(x,y)$ denote the standard dot product of vectors on $\mathbb{R^n}$. I'd like to confirm the formula that I have for the gradient of the dot product is correct. First, I consider this to be a map from $\mathbb{R}^{2n}$ to $\mathbb{R}$ rather than one from $\mathbb{R^n} \times \mathbb{R}^n$ to $\mathbb{R}$. By this I mean: $$ f(x,y) = f(x_1,...,x_n,y_1,...,y_n) = \sum_{i = 1}^{n}x_iy_i $$ Taking partial derivatives with respect to the $2n$ variables, we have that: $$ \nabla f(x,y) = (y_1,...,y_n,x_1,...,x_n) = (y,x) $$ Is this solution correct? I have not seen anything like this online.",,"['calculus', 'real-analysis', 'linear-algebra', 'multivariable-calculus']"
53,Help me prove: directional derivative in terms of partial derivative,Help me prove: directional derivative in terms of partial derivative,,"Given a function $f(x, y)$ which has partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$, and whose partial derivatives are also differentiable, and    $\vec r = (x_r, y_r)$, I am trying to prove that the derivative of $f$ along $\vec r$ is: $Df_r (x,y) = \lim_{h\to 0} \dfrac{f(x+hx_r, y+hy_r)-f(x,y)}{h} = x_r \dfrac{\partial f}{\partial x} + y_r \dfrac{\partial f}{\partial y} = D$ I am trying to prove this by showing that for any given $\epsilon$, I can find a $\delta$ such that for all $h < \delta$, the actual rate-of-change is within $\epsilon$ of $D$. We can write $f(x+hx_r, y+hy_r)-f(x,y) = A + B$, where $A = f(x+hx_r, y)-f(x,y)$ and $B = f(x+hx_r, y+hy_r)-f(x+hx_r,y)$. I wish to make $|A + B| \le |A| + |B| \le (D + \epsilon)h$ ...[1]. Since $\frac{\partial f}{\partial x}$ exists, I can pick an $h$ to bring |A| within a factor of any $\epsilon_1$ to $x_r\frac{\partial f}{\partial x} h$. I can make the $h$ smaller, if necesssary, to bring $\frac{\partial f}{\partial y}(x+hx_r, y)$ within $\epsilon_2$ of $\frac{\partial f}{\partial y}(x, y)$. I then wish to bring $|B|$ sufficiently close to $y_r \frac{\partial f}{\partial y} h$, so that [1] is achieved. I am getting stuck at the last step. Because $\frac{\partial f}{\partial y}$ exists everywhere, for all $\epsilon_3$ there exists an $\delta_3$ such that for all $h_3 < \delta_3$, $|f(x+hx_r, y+h_3 y_r) - f(x+hx_r, y)| \le (y_r \frac{\partial f}{\partial y} h_r + \epsilon_3)h_3$. In my situation, however, $h_3$ is not a free variable, but fixed: $h_3 = h$. No matter how small I make my $h$, $\delta_3$ might be too small.","Given a function $f(x, y)$ which has partial derivatives $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$, and whose partial derivatives are also differentiable, and    $\vec r = (x_r, y_r)$, I am trying to prove that the derivative of $f$ along $\vec r$ is: $Df_r (x,y) = \lim_{h\to 0} \dfrac{f(x+hx_r, y+hy_r)-f(x,y)}{h} = x_r \dfrac{\partial f}{\partial x} + y_r \dfrac{\partial f}{\partial y} = D$ I am trying to prove this by showing that for any given $\epsilon$, I can find a $\delta$ such that for all $h < \delta$, the actual rate-of-change is within $\epsilon$ of $D$. We can write $f(x+hx_r, y+hy_r)-f(x,y) = A + B$, where $A = f(x+hx_r, y)-f(x,y)$ and $B = f(x+hx_r, y+hy_r)-f(x+hx_r,y)$. I wish to make $|A + B| \le |A| + |B| \le (D + \epsilon)h$ ...[1]. Since $\frac{\partial f}{\partial x}$ exists, I can pick an $h$ to bring |A| within a factor of any $\epsilon_1$ to $x_r\frac{\partial f}{\partial x} h$. I can make the $h$ smaller, if necesssary, to bring $\frac{\partial f}{\partial y}(x+hx_r, y)$ within $\epsilon_2$ of $\frac{\partial f}{\partial y}(x, y)$. I then wish to bring $|B|$ sufficiently close to $y_r \frac{\partial f}{\partial y} h$, so that [1] is achieved. I am getting stuck at the last step. Because $\frac{\partial f}{\partial y}$ exists everywhere, for all $\epsilon_3$ there exists an $\delta_3$ such that for all $h_3 < \delta_3$, $|f(x+hx_r, y+h_3 y_r) - f(x+hx_r, y)| \le (y_r \frac{\partial f}{\partial y} h_r + \epsilon_3)h_3$. In my situation, however, $h_3$ is not a free variable, but fixed: $h_3 = h$. No matter how small I make my $h$, $\delta_3$ might be too small.",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
54,Finding volume of hyperboloid bounded by two planes,Finding volume of hyperboloid bounded by two planes,,"I want to find the volume bounded by hyperboloid $\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}-\cfrac{z^2}{c^2} = 1$ and the planes $z=-c, z=c$. I do not know whether should I use the cylindirical coordinates or spherical coordinates. At first, I am thinking to set $x =au, y=bv,z=cw$ and now we have $u^2+v^2 -w^2 = 1$. Jacobian of this transformation is $abc$. I guess that after this change of variables If I take a cross section then it will give me a circle on $uv-$plane but I do not know the reason. If we consider the part of the volume that is inside the first octant, my intuition says that the desired volume is $\displaystyle \int_{0}^{1}\int_{0}^{1-v^2}\int_{0}^{u^2+v^2-1}g(u,v,w)abc\times dwdudv$  and I am not sure about $g$... To sum up, I appreciate if you could explain me what is going on exactly in a basic way.","I want to find the volume bounded by hyperboloid $\cfrac{x^2}{a^2}+\cfrac{y^2}{b^2}-\cfrac{z^2}{c^2} = 1$ and the planes $z=-c, z=c$. I do not know whether should I use the cylindirical coordinates or spherical coordinates. At first, I am thinking to set $x =au, y=bv,z=cw$ and now we have $u^2+v^2 -w^2 = 1$. Jacobian of this transformation is $abc$. I guess that after this change of variables If I take a cross section then it will give me a circle on $uv-$plane but I do not know the reason. If we consider the part of the volume that is inside the first octant, my intuition says that the desired volume is $\displaystyle \int_{0}^{1}\int_{0}^{1-v^2}\int_{0}^{u^2+v^2-1}g(u,v,w)abc\times dwdudv$  and I am not sure about $g$... To sum up, I appreciate if you could explain me what is going on exactly in a basic way.",,"['calculus', 'integration', 'multivariable-calculus']"
55,Calculating distance using polar coordinate metric,Calculating distance using polar coordinate metric,,"I've just come across metrics that give you the distance between two points in an orthogonal curvilinear system. I have a couple of questions I was hoping someone could answer. Does distance, or $\sqrt {ds^2}$, in $ds^2=dx^2+dy^2$ and $ds^2=dr^2+r^2d\theta^2$, have the same meaning? I tried applying these to an example and I know I've gone wrong somewhere along the way so I was also hoping someone could tell me what I've done wrong. Example: Find the distance between points $P(1,0)$ and $Q(0,1)$. In Cartesian coordinates I think $dx$ and $dy$ is the difference between the $x$ and $y$ components respectively, of $P$ and $Q$. (Is that right?) So $ds^2=(1-0)^2+(0-1)^2=2 \Rightarrow ds=\sqrt2$ Converting the Cartesian coordinates into polar coordinates $(r,\theta)$ we get $P(1,0)$ and $Q(1,\frac{\pi}{2})$. I was thinking maybe $dr$ was the difference in radii and since $P$ and $Q$ are the same distance from the origin the difference is zero so $dr=0$. As for $d\theta$ I thought it was the difference in angles. I'm not sure what $r$ in $r^2$ is in the polar coordinate metric just before $d\theta^2$. I just took it as $1$ because $P$ and $Q$ are both on the unit circle. I wonder though what would happen if $P$ was on the unit circle and $Q$ was on the circle of radius $2$ for example...then I wouldn't know what value to give $r$. Anyway using my terrible logic above I get $ds^2=dr^2 +r^2d\theta^2=(1-1)^2+1^2(\frac{\pi}{2}-0)^2=\frac{\pi^2}{4} \Rightarrow ds=\frac{\pi}{2} \neq \sqrt2$ Can someone please explain what I'm doing wrong?","I've just come across metrics that give you the distance between two points in an orthogonal curvilinear system. I have a couple of questions I was hoping someone could answer. Does distance, or $\sqrt {ds^2}$, in $ds^2=dx^2+dy^2$ and $ds^2=dr^2+r^2d\theta^2$, have the same meaning? I tried applying these to an example and I know I've gone wrong somewhere along the way so I was also hoping someone could tell me what I've done wrong. Example: Find the distance between points $P(1,0)$ and $Q(0,1)$. In Cartesian coordinates I think $dx$ and $dy$ is the difference between the $x$ and $y$ components respectively, of $P$ and $Q$. (Is that right?) So $ds^2=(1-0)^2+(0-1)^2=2 \Rightarrow ds=\sqrt2$ Converting the Cartesian coordinates into polar coordinates $(r,\theta)$ we get $P(1,0)$ and $Q(1,\frac{\pi}{2})$. I was thinking maybe $dr$ was the difference in radii and since $P$ and $Q$ are the same distance from the origin the difference is zero so $dr=0$. As for $d\theta$ I thought it was the difference in angles. I'm not sure what $r$ in $r^2$ is in the polar coordinate metric just before $d\theta^2$. I just took it as $1$ because $P$ and $Q$ are both on the unit circle. I wonder though what would happen if $P$ was on the unit circle and $Q$ was on the circle of radius $2$ for example...then I wouldn't know what value to give $r$. Anyway using my terrible logic above I get $ds^2=dr^2 +r^2d\theta^2=(1-1)^2+1^2(\frac{\pi}{2}-0)^2=\frac{\pi^2}{4} \Rightarrow ds=\frac{\pi}{2} \neq \sqrt2$ Can someone please explain what I'm doing wrong?",,"['multivariable-calculus', 'differential-geometry', 'mathematical-physics', 'polar-coordinates']"
56,"Find local min, max, saddle point of the function $f(x, y) = 3x^3 − 9x + 9xy^2$","Find local min, max, saddle point of the function","f(x, y) = 3x^3 − 9x + 9xy^2","$f(x, y) = 3x^3 − 9x + 9xy^2$ I've tried taking the partial derivatives and double derivatives of this function to solve for the local minimum, maximum values, and saddle point but haven't made much progress.","$f(x, y) = 3x^3 − 9x + 9xy^2$ I've tried taking the partial derivatives and double derivatives of this function to solve for the local minimum, maximum values, and saddle point but haven't made much progress.",,"['multivariable-calculus', 'partial-derivative', 'maxima-minima']"
57,Substitution of variables in a derivative,Substitution of variables in a derivative,,"This is more of a refresher of me, but I'm wondering how to formally approach a simple substitution of variables. I want to substitute $\omega$ for $\lambda$ in a simple derivative: $\frac{dn}{d\omega}$ to $\frac{dn}{d\lambda}$, when $\lambda = 2\pi c / \omega$. I know it is $\frac{dn}{d\omega}$ = $(-\lambda^2 / 2\pi c)\frac{dn}{d\lambda}$ and that is achieved by simpling realizing that the derivative of what is given will cancel out the multiplicative constants, but I can't remember what that process is called and feel like a bit of an idiot!","This is more of a refresher of me, but I'm wondering how to formally approach a simple substitution of variables. I want to substitute $\omega$ for $\lambda$ in a simple derivative: $\frac{dn}{d\omega}$ to $\frac{dn}{d\lambda}$, when $\lambda = 2\pi c / \omega$. I know it is $\frac{dn}{d\omega}$ = $(-\lambda^2 / 2\pi c)\frac{dn}{d\lambda}$ and that is achieved by simpling realizing that the derivative of what is given will cancel out the multiplicative constants, but I can't remember what that process is called and feel like a bit of an idiot!",,"['multivariable-calculus', 'derivatives', 'substitution']"
58,A name for the directional derivative with respect to a unit vector?,A name for the directional derivative with respect to a unit vector?,,"I finally came up with a reasonable justification for not requiring the reference vector of the directional derivative to be of unit length.  But that is another discussion. This has bothered me for a long time.  Partial derivatives in Cartesian form are directional derivatives with respect to the standard (unit) basis vectors.  If I rotate my coordinate system so that one axis shares the direction in which I am differentiating, the partial derivative with respect to the corresponding coordinate is a directional derivative with respect to the new (unit) basis vector. If I want to know how much a scalar potential changes with respect to ark length in a given direction, I find the dot product of the gradient with a unit vector in that direction. In some introductory calculus books the directional derivative is defined to be with respect to a unit vector.  There are many places in which the directional derivative with respect to a unit vector is valuable.  I there a good name for that special kind of directional derivative? I believe some authors call the directional derivative operator with respect to a unit vector the unit vector.  In other words the vector is the operator.  But that makes my head spin.","I finally came up with a reasonable justification for not requiring the reference vector of the directional derivative to be of unit length.  But that is another discussion. This has bothered me for a long time.  Partial derivatives in Cartesian form are directional derivatives with respect to the standard (unit) basis vectors.  If I rotate my coordinate system so that one axis shares the direction in which I am differentiating, the partial derivative with respect to the corresponding coordinate is a directional derivative with respect to the new (unit) basis vector. If I want to know how much a scalar potential changes with respect to ark length in a given direction, I find the dot product of the gradient with a unit vector in that direction. In some introductory calculus books the directional derivative is defined to be with respect to a unit vector.  There are many places in which the directional derivative with respect to a unit vector is valuable.  I there a good name for that special kind of directional derivative? I believe some authors call the directional derivative operator with respect to a unit vector the unit vector.  In other words the vector is the operator.  But that makes my head spin.",,['multivariable-calculus']
59,Existance of the derivative of a function of several variables whose domain is closed.,Existance of the derivative of a function of several variables whose domain is closed.,,"Define $f: [a,b]\to \Bbb R$, we can talk about the derivative of $f$ at the boundary points $a$ and $b$. I am interested in the case when $f$ is a function of several variables. Define $f:E\to \Bbb R^m$ with $E\subset\Bbb R^n$. One can talk about the derivative of $f$ only if $E$ is open. I have no idea why this is the case. The answer here: https://math.stackexchange.com/a/504567/135775 , says that if $E$ is not open the the Jacobian  of $f$ need not be unique. Can someone give an explanation of why one cannot define the derivative if $E$ is closed or maybe an example where the Jacobian is not unique.","Define $f: [a,b]\to \Bbb R$, we can talk about the derivative of $f$ at the boundary points $a$ and $b$. I am interested in the case when $f$ is a function of several variables. Define $f:E\to \Bbb R^m$ with $E\subset\Bbb R^n$. One can talk about the derivative of $f$ only if $E$ is open. I have no idea why this is the case. The answer here: https://math.stackexchange.com/a/504567/135775 , says that if $E$ is not open the the Jacobian  of $f$ need not be unique. Can someone give an explanation of why one cannot define the derivative if $E$ is closed or maybe an example where the Jacobian is not unique.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'jacobian']"
60,Area element of a spherical surface,Area element of a spherical surface,,"I am trying to find out the area element of a sphere given by the equation: $$r^2= x^2 +y^2+z^2$$ The sphere is centered around the origin of the Cartesian basis vectors $(e_x,e_y,e_z)$. The spherical-polar basis vectors are $(e_r,e_\theta,e_\phi)$ which is related to the cartesian basis vectors as follows: $$e_r=(\sin \theta \cos \phi )e_x+(\sin \theta \sin \phi )e_y+(\cos \theta )e_z \tag 1 $$ $$e_\theta =(\cos \theta \cos \phi )e_x+(\cos \theta \sin \phi )e_y−(\sin \theta )e_z \tag 2$$ $$  e_\phi =(−\sin \phi )e_x+(\cos \phi )e_y \tag 3$$ Reference image: this would give a rough idea of the setup but the basis vectors are set as in this image: . The vector surface element can be given as the cross product of $d\overrightarrow r_1$ and $d\overrightarrow r_2$ i.e. : $$dS = d\overrightarrow r_1×d\overrightarrow r_2$$ and by taking its magnitude I would get its area. What I think is that the vectors $d\overrightarrow r_1$ and $d\overrightarrow r_2$ can be given by linear combination of the basis vectors $e_r$ $e_\phi$ and $e_\theta$ and therefore:  $$d\overrightarrow r= a_r e_r + a_\phi e_\phi + a_\theta e_\theta$$ so,  $d\overrightarrow r_1$ and $d\overrightarrow r_2$ can be given as:  $$d\overrightarrow r_1= 0 e_r + d\phi e_\phi + 0 e_\theta \tag 4$$ and  $$d\overrightarrow r_2= 0 e_r + 0 e_\phi + d\theta e_\theta \tag 5$$ I can then substitute equation $(1),(2)$ & $(3)$ in $(4)$ & $(5)$ to get $d\overrightarrow r_1$ and $d\overrightarrow r_2$ in terms of the Cartesian basis vectors $(e_x,e_y,e_z)$. My question is: Am I using the correct values of the coefficients $a_r, a_\phi $ and $a_\theta$? Is this approach valid and how/where do I make use of the equation of sphere $r^2= x^2 +y^2+z^2$ in $d\overrightarrow r_1$ and $d\overrightarrow r_2$ so that I can further find their cross product? I would appreciate any help.","I am trying to find out the area element of a sphere given by the equation: $$r^2= x^2 +y^2+z^2$$ The sphere is centered around the origin of the Cartesian basis vectors $(e_x,e_y,e_z)$. The spherical-polar basis vectors are $(e_r,e_\theta,e_\phi)$ which is related to the cartesian basis vectors as follows: $$e_r=(\sin \theta \cos \phi )e_x+(\sin \theta \sin \phi )e_y+(\cos \theta )e_z \tag 1 $$ $$e_\theta =(\cos \theta \cos \phi )e_x+(\cos \theta \sin \phi )e_y−(\sin \theta )e_z \tag 2$$ $$  e_\phi =(−\sin \phi )e_x+(\cos \phi )e_y \tag 3$$ Reference image: this would give a rough idea of the setup but the basis vectors are set as in this image: . The vector surface element can be given as the cross product of $d\overrightarrow r_1$ and $d\overrightarrow r_2$ i.e. : $$dS = d\overrightarrow r_1×d\overrightarrow r_2$$ and by taking its magnitude I would get its area. What I think is that the vectors $d\overrightarrow r_1$ and $d\overrightarrow r_2$ can be given by linear combination of the basis vectors $e_r$ $e_\phi$ and $e_\theta$ and therefore:  $$d\overrightarrow r= a_r e_r + a_\phi e_\phi + a_\theta e_\theta$$ so,  $d\overrightarrow r_1$ and $d\overrightarrow r_2$ can be given as:  $$d\overrightarrow r_1= 0 e_r + d\phi e_\phi + 0 e_\theta \tag 4$$ and  $$d\overrightarrow r_2= 0 e_r + 0 e_\phi + d\theta e_\theta \tag 5$$ I can then substitute equation $(1),(2)$ & $(3)$ in $(4)$ & $(5)$ to get $d\overrightarrow r_1$ and $d\overrightarrow r_2$ in terms of the Cartesian basis vectors $(e_x,e_y,e_z)$. My question is: Am I using the correct values of the coefficients $a_r, a_\phi $ and $a_\theta$? Is this approach valid and how/where do I make use of the equation of sphere $r^2= x^2 +y^2+z^2$ in $d\overrightarrow r_1$ and $d\overrightarrow r_2$ so that I can further find their cross product? I would appreciate any help.",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
61,Joint Distribution from marginal distributions and the distribution of the sum,Joint Distribution from marginal distributions and the distribution of the sum,,Let's assume I know the marginal distributions of two random variables. Does knowing the marginal distribution of their sum or their linear combination in general gives me any information on the possible joint distribution? Can it improve the Frechet bounds for the joint? Is it possible to have general results without imposing restrictions on the type of the marginal distributions?,Let's assume I know the marginal distributions of two random variables. Does knowing the marginal distribution of their sum or their linear combination in general gives me any information on the possible joint distribution? Can it improve the Frechet bounds for the joint? Is it possible to have general results without imposing restrictions on the type of the marginal distributions?,,"['probability', 'multivariable-calculus']"
62,Showing a function is constant given $f_{xx} + f_{yy} \geq 0$ on the domain.,Showing a function is constant given  on the domain.,f_{xx} + f_{yy} \geq 0,"Saw this question when looking through an old vector calculus text and wasn't able to find a reasonably formal way of proving the following assertion: Let $f \in C^2$,  $f:U \to \mathbb{R}$ where $U$ is an open subset of $\mathbb{R}^2$ and suppose that: $$ \frac{\partial^2  f }{\partial x^2} + \frac{\partial ^2 f}{\partial y^2} \geq 0 \qquad \forall (x,y) \in U $$ Prove that if $f$ has a local maximum at a point $(a,b) \in U$, then $f(x,y)=constant$. While I figure one can argue this geometrically, I am not sure what tools to use to make prove the result. Thanks.","Saw this question when looking through an old vector calculus text and wasn't able to find a reasonably formal way of proving the following assertion: Let $f \in C^2$,  $f:U \to \mathbb{R}$ where $U$ is an open subset of $\mathbb{R}^2$ and suppose that: $$ \frac{\partial^2  f }{\partial x^2} + \frac{\partial ^2 f}{\partial y^2} \geq 0 \qquad \forall (x,y) \in U $$ Prove that if $f$ has a local maximum at a point $(a,b) \in U$, then $f(x,y)=constant$. While I figure one can argue this geometrically, I am not sure what tools to use to make prove the result. Thanks.",,"['real-analysis', 'multivariable-calculus']"
63,Clairaut's Theorem hypothesis,Clairaut's Theorem hypothesis,,"In Clairaut's theorem based on equality of mixed second order partial derivatives, in some books the hypothesis is: if $f_{xy}$ and $f_{yx}$ are defined in a disk around $(x_o,y_o)$ and they are continuous at $(x_o,y_o)$, then  $f_{xy}(x_o,y_o)=f_{yx}(x_o,y_o)$. While some books say that if  $f_{xy}$ and $f_{yx}$ are continuous throughout an open disk around $(x_o,y_o)$, then  $f_{xy}(x_o,y_o)=f_{yx}(x_o,y_o)$. Can anyone suggest that whether the continuity of the two mixed partials at the point enough to conclude the equality of the two. I have gone through the proof but towards the last they just say that the iterated limits are equal and hence $f_{xy}(x_o,y_o)=f_{yx}(x_o,y_o).$ Please explain. Thanks.","In Clairaut's theorem based on equality of mixed second order partial derivatives, in some books the hypothesis is: if $f_{xy}$ and $f_{yx}$ are defined in a disk around $(x_o,y_o)$ and they are continuous at $(x_o,y_o)$, then  $f_{xy}(x_o,y_o)=f_{yx}(x_o,y_o)$. While some books say that if  $f_{xy}$ and $f_{yx}$ are continuous throughout an open disk around $(x_o,y_o)$, then  $f_{xy}(x_o,y_o)=f_{yx}(x_o,y_o)$. Can anyone suggest that whether the continuity of the two mixed partials at the point enough to conclude the equality of the two. I have gone through the proof but towards the last they just say that the iterated limits are equal and hence $f_{xy}(x_o,y_o)=f_{yx}(x_o,y_o).$ Please explain. Thanks.",,"['calculus', 'multivariable-calculus']"
64,How does the criteria $\nabla F(x_0) = \lambda \nabla G(x_0)$ guarantee that $x_0$ is a maximum?,How does the criteria  guarantee that  is a maximum?,\nabla F(x_0) = \lambda \nabla G(x_0) x_0,"I'm attempting to figure out why Lagrange Multipliers work, and I have the following setup. Suppose we wish to maximize $F : \mathbb{R}^n\to\mathbb{R}$ with respect to the constraint $G = c$ where $G:\mathbb{R}^n\to\mathbb{R}$. Let $L$ be the level surface $G = c$. Suppose $x_0\in L$, and $u$ is tangent to $L$ at $x_0$. I want to show that if $\nabla F\cdot u > 0$ then we can move along $L$ and find a point $x_0'\in L$ with $F(x_0')>F(x_0)$, hence requiring that $\nabla F\cdot u = 0$ for $x_0$ to be a maximum. My problem is that the condition $f'(x_0)>0$ isn't enough to guarantee that there is an interval $[x_0,\ x_0+\delta]$ on which $f$ is increasing (for example, take $f(x) = x^2\sin^2\left(\frac1x\right) + \frac12 x$ for nonzero $x$, and $f(0)=0$). So essentially, if we look at $f(t) = F(x_0+tu)$, the statement $f'(0) > 0$ is not enough to assume that $f$ is strictly increasing on any neighbourhood of zero, and so $F$ isn't increasing in any neighbourhood of $x_0$ meaning we can't necessarily guarantee that it's increasing along $L$ in the direction of $u$. Hence, $\nabla F\cdot u>0$ isn't enough to guarantee that we can find another point $x_0'\in L$ near $x_0$ so that $F(x_0')>F(x_0)$. If my logic isn't flawed, then how come Lagrange multipliers work? How do we know that $\nabla F\cdot u = 0$ for all tangent vectors $u$ to $L$ at $x_0$ is enough to guarantee that $x_0$ maximizes $F$ on $L$? Is it like, $\nabla F\cdot u>0$ isn't enough to guarantee that $x_0$ isn't a maximum, but $\nabla F\cdot u = 0$ is ?","I'm attempting to figure out why Lagrange Multipliers work, and I have the following setup. Suppose we wish to maximize $F : \mathbb{R}^n\to\mathbb{R}$ with respect to the constraint $G = c$ where $G:\mathbb{R}^n\to\mathbb{R}$. Let $L$ be the level surface $G = c$. Suppose $x_0\in L$, and $u$ is tangent to $L$ at $x_0$. I want to show that if $\nabla F\cdot u > 0$ then we can move along $L$ and find a point $x_0'\in L$ with $F(x_0')>F(x_0)$, hence requiring that $\nabla F\cdot u = 0$ for $x_0$ to be a maximum. My problem is that the condition $f'(x_0)>0$ isn't enough to guarantee that there is an interval $[x_0,\ x_0+\delta]$ on which $f$ is increasing (for example, take $f(x) = x^2\sin^2\left(\frac1x\right) + \frac12 x$ for nonzero $x$, and $f(0)=0$). So essentially, if we look at $f(t) = F(x_0+tu)$, the statement $f'(0) > 0$ is not enough to assume that $f$ is strictly increasing on any neighbourhood of zero, and so $F$ isn't increasing in any neighbourhood of $x_0$ meaning we can't necessarily guarantee that it's increasing along $L$ in the direction of $u$. Hence, $\nabla F\cdot u>0$ isn't enough to guarantee that we can find another point $x_0'\in L$ near $x_0$ so that $F(x_0')>F(x_0)$. If my logic isn't flawed, then how come Lagrange multipliers work? How do we know that $\nabla F\cdot u = 0$ for all tangent vectors $u$ to $L$ at $x_0$ is enough to guarantee that $x_0$ maximizes $F$ on $L$? Is it like, $\nabla F\cdot u>0$ isn't enough to guarantee that $x_0$ isn't a maximum, but $\nabla F\cdot u = 0$ is ?",,"['real-analysis', 'multivariable-calculus', 'lagrange-multiplier']"
65,Continuously differentiable map from $\mathbb{R}^{m+n}$ to $\mathbb{R}^n$,Continuously differentiable map from  to,\mathbb{R}^{m+n} \mathbb{R}^n,"Suppose $m,n>0$, $U$ an open subset of $\mathbb{R}^{m+n}$ and let $f: U \to \mathbb{R}^n$ be continuously differentiable.  Is it possible for $f$ to be injective? My thinking is that continuous differentiability+injectivity suggests that this is sort of getting at a converse to the Inverse Function Theorem.  We know the Jacobian is not invertible at any point in $U$, and the question is asking if this implies that $f$ cannot be invertible in any neighborhood contained in $U$.","Suppose $m,n>0$, $U$ an open subset of $\mathbb{R}^{m+n}$ and let $f: U \to \mathbb{R}^n$ be continuously differentiable.  Is it possible for $f$ to be injective? My thinking is that continuous differentiability+injectivity suggests that this is sort of getting at a converse to the Inverse Function Theorem.  We know the Jacobian is not invertible at any point in $U$, and the question is asking if this implies that $f$ cannot be invertible in any neighborhood contained in $U$.",,['real-analysis']
66,Applying the Curl Thm. to a surface given its boundary,Applying the Curl Thm. to a surface given its boundary,,"The Question: Let $F(x,y,z) = (y+\sin(x),z^2+\cos(y),x^3)$ and $\mathcal{C}$ is the curve parametrized by $r(t)=(\sin(t),\cos(t),2\sin(2t))$ with $t$ ranging in $[0,2\pi[$ . Use the curl theorem to find $\displaystyle\int_\mathcal{C}F\cdot ds$ . My approach: I know, by the curl thm. that $\displaystyle\int_{\partial S}F\cdot dr =\int_{ S}\operatorname{curl}(F)\cdot d\Sigma$ I calculated the curl for $F$ and the result was $\operatorname{curl}(F)(x,y,z)=(-2z,-3x^2,-1)$ . The boundary of the surface which the theorem talks about is the curve generated by the parametrization I was given, this means I need to find a surface $S$ such that $\partial S = \mathcal{C}$ . Here lies my problem. Once I've found the surface $S$ ,  I need to parametrize it with $\alpha:D\subseteq \mathbb{R}^2\to \mathbb{R}^3$ . With this I get $\displaystyle\int_{ S}\operatorname{curl}(F)\cdot d\Sigma = \iint_D\text{curl}(F)(\alpha(u,v))\cdot (\alpha_u\times\alpha_v) dA\,$ which is a double integral that I can calculate. My problem: I can't find the surface whose boundary corresponds to $r([0,2\pi[)$ . I've already plotted the curve in Mathematica and it looks like the border of a pringle potato. My gut tells me that the surface I'm supposed to use is ""the potato"" but I can't get my head onto it. I feel like there are many surfaces which I can't imagine that have the curve $\mathcal{C}\,$ as a boundary. Still, if it were the ""potato"" I am not able to find the parametrization for it. This is where I need help. $\textbf{Concretely}$ I need to find the parametrization of the ""pringle potato"" whose boundary is my curve $\mathcal{C}$ and an explanation of why is that the correct surface to think about.","The Question: Let and is the curve parametrized by with ranging in . Use the curl theorem to find . My approach: I know, by the curl thm. that I calculated the curl for and the result was . The boundary of the surface which the theorem talks about is the curve generated by the parametrization I was given, this means I need to find a surface such that . Here lies my problem. Once I've found the surface ,  I need to parametrize it with . With this I get which is a double integral that I can calculate. My problem: I can't find the surface whose boundary corresponds to . I've already plotted the curve in Mathematica and it looks like the border of a pringle potato. My gut tells me that the surface I'm supposed to use is ""the potato"" but I can't get my head onto it. I feel like there are many surfaces which I can't imagine that have the curve as a boundary. Still, if it were the ""potato"" I am not able to find the parametrization for it. This is where I need help. I need to find the parametrization of the ""pringle potato"" whose boundary is my curve and an explanation of why is that the correct surface to think about.","F(x,y,z) = (y+\sin(x),z^2+\cos(y),x^3) \mathcal{C} r(t)=(\sin(t),\cos(t),2\sin(2t)) t [0,2\pi[ \displaystyle\int_\mathcal{C}F\cdot ds \displaystyle\int_{\partial S}F\cdot dr =\int_{ S}\operatorname{curl}(F)\cdot d\Sigma F \operatorname{curl}(F)(x,y,z)=(-2z,-3x^2,-1) S \partial S = \mathcal{C} S \alpha:D\subseteq \mathbb{R}^2\to \mathbb{R}^3 \displaystyle\int_{ S}\operatorname{curl}(F)\cdot d\Sigma = \iint_D\text{curl}(F)(\alpha(u,v))\cdot (\alpha_u\times\alpha_v) dA\, r([0,2\pi[) \mathcal{C}\, \textbf{Concretely} \mathcal{C}","['multivariable-calculus', 'parametrization', 'stokes-theorem']"
67,Is the first fundamental form the outer product of $\nabla s$ for a surface $s$?,Is the first fundamental form the outer product of  for a surface ?,\nabla s s,"Take two $2$-dimensional vectors $\boldsymbol a:=(a_1, a_2)$ and $\boldsymbol b := (b_1, b_2)$. The outer product of $\boldsymbol a$ and $\boldsymbol b$ is $$ \begin{bmatrix}     a_1b_1 & a_1b_2\\     a_2b_1 & a_2b_2 \\ \end{bmatrix}. $$ Now take a surface $S\subseteq\textbf{R}^3$ parametrized by $\boldsymbol s(u,v)$. The first fundamental form of $\boldsymbol s(u,v)$ is $$ \begin{bmatrix}     \boldsymbol s_u \cdotp \boldsymbol s_u & \boldsymbol s_u \cdotp \boldsymbol s_v\\     \boldsymbol s_v \cdotp \boldsymbol s_u & \boldsymbol s_v \cdotp \boldsymbol s_v \\ \end{bmatrix}, $$ which looks suspiciously like the outer product of $\nabla\boldsymbol s := (\boldsymbol s_u, \boldsymbol s_v)$ with itself. (The dot $\cdotp$ stands for dot product of vectors.) Does this make sense? Does it generalize to other settings, like higher dimensions, or surfaces not in parametric form?","Take two $2$-dimensional vectors $\boldsymbol a:=(a_1, a_2)$ and $\boldsymbol b := (b_1, b_2)$. The outer product of $\boldsymbol a$ and $\boldsymbol b$ is $$ \begin{bmatrix}     a_1b_1 & a_1b_2\\     a_2b_1 & a_2b_2 \\ \end{bmatrix}. $$ Now take a surface $S\subseteq\textbf{R}^3$ parametrized by $\boldsymbol s(u,v)$. The first fundamental form of $\boldsymbol s(u,v)$ is $$ \begin{bmatrix}     \boldsymbol s_u \cdotp \boldsymbol s_u & \boldsymbol s_u \cdotp \boldsymbol s_v\\     \boldsymbol s_v \cdotp \boldsymbol s_u & \boldsymbol s_v \cdotp \boldsymbol s_v \\ \end{bmatrix}, $$ which looks suspiciously like the outer product of $\nabla\boldsymbol s := (\boldsymbol s_u, \boldsymbol s_v)$ with itself. (The dot $\cdotp$ stands for dot product of vectors.) Does this make sense? Does it generalize to other settings, like higher dimensions, or surfaces not in parametric form?",,"['linear-algebra', 'multivariable-calculus', 'differential-geometry']"
68,"Is this a composition of scalar fields? Meaning of $(\phi(x,y,z,t),x,y,z)$?",Is this a composition of scalar fields? Meaning of ?,"(\phi(x,y,z,t),x,y,z)","In the following I'm interested in the mathematics, not the physical representation. The diffusion equation is  $$ \frac{\partial \phi(\mathbf{r},t)}{\partial t}=\nabla \cdot [D(\phi,\mathbf{r})\nabla\phi(\mathbf{r},t)] $$ Here $\phi$ and $D$ are multivariable functions.  I think the explicit form of $D(\phi,\mathbf{r})$ is $$ D(\phi,\mathbf{r})=D(\phi(\mathbf{r},t),\mathbf{r})=D(\phi(x,y,z,t),x,y,z) $$ But what does $D(\phi(x,y,z,t),x,y,z)$ mean, is it a function composition? $\phi(\mathbf{r},t)$ is $\mathbb R^4 \rightarrow \mathbb R$, but what is $D$?","In the following I'm interested in the mathematics, not the physical representation. The diffusion equation is  $$ \frac{\partial \phi(\mathbf{r},t)}{\partial t}=\nabla \cdot [D(\phi,\mathbf{r})\nabla\phi(\mathbf{r},t)] $$ Here $\phi$ and $D$ are multivariable functions.  I think the explicit form of $D(\phi,\mathbf{r})$ is $$ D(\phi,\mathbf{r})=D(\phi(\mathbf{r},t),\mathbf{r})=D(\phi(x,y,z,t),x,y,z) $$ But what does $D(\phi(x,y,z,t),x,y,z)$ mean, is it a function composition? $\phi(\mathbf{r},t)$ is $\mathbb R^4 \rightarrow \mathbb R$, but what is $D$?",,"['real-analysis', 'functions', 'multivariable-calculus', 'partial-differential-equations']"
69,one problem from limit [duplicate],one problem from limit [duplicate],,This question already has an answer here : Find the Set With Given Condition (1 answer) Closed 6 years ago . How will I proceed for the question 63? Can anyone help me out?,This question already has an answer here : Find the Set With Given Condition (1 answer) Closed 6 years ago . How will I proceed for the question 63? Can anyone help me out?,,['calculus']
70,"Prove that surfaces $x + 2y – lnz + 4 = 0$ and $x^2 - xy – 8x + z + 5 = 0$ are tangent at $(2,-3,1)$.",Prove that surfaces  and  are tangent at .,"x + 2y – lnz + 4 = 0 x^2 - xy – 8x + z + 5 = 0 (2,-3,1)","$x + 2y – ln(z) + 4 = 0$ $x^2 - xy – 8x + z + 5 = 0$ $\nabla [1,2,\frac{-1}{z}] $ $\nabla [2x-y-8,-x,1] $ $\nabla(P0) [1,2,-1] $ $\nabla(P0) [-1,-2,1] $ I've stuck at this point and i don't know what to do next.","$x + 2y – ln(z) + 4 = 0$ $x^2 - xy – 8x + z + 5 = 0$ $\nabla [1,2,\frac{-1}{z}] $ $\nabla [2x-y-8,-x,1] $ $\nabla(P0) [1,2,-1] $ $\nabla(P0) [-1,-2,1] $ I've stuck at this point and i don't know what to do next.",,"['real-analysis', 'multivariable-calculus']"
71,Coordinates of the point of an ellipsoid closest to the origin.,Coordinates of the point of an ellipsoid closest to the origin.,,"Given the ellipse $\frac{(x-r)^2}{a^2}  +  \frac{(y-s)^2}{b^2} =1$  $(a^2 > b^2)$. Find the coordinates of the point of the ellipsoid closest to the origin in terms of a,b,r and s. I tried three different method so far but all lead to highly complicated and lengthy computations. Anyone has a shortcut? Thanks","Given the ellipse $\frac{(x-r)^2}{a^2}  +  \frac{(y-s)^2}{b^2} =1$  $(a^2 > b^2)$. Find the coordinates of the point of the ellipsoid closest to the origin in terms of a,b,r and s. I tried three different method so far but all lead to highly complicated and lengthy computations. Anyone has a shortcut? Thanks",,"['multivariable-calculus', 'vector-analysis']"
72,How to prove the following Mean Value Theorem?,How to prove the following Mean Value Theorem?,,"Let the function $f:\left[  0,T\right]  \times\mathbb{R} \rightarrow\mathbb{R}$ be a function of time $t\in\left[  0,T\right]  $ and $x\in\mathbb{R}$, such that the partial derivatives $\partial f\left(  t,x\right)  /\partial t$ , $\partial f\left(  t,x\right)  /\partial x$ and $\partial^{2}f\left(  t,x\right)  /\partial x^{2}$ exist and are continuous for all $(t,x)\in\left[  0,T\right]  \times\mathbb{R}$. Then for any $t,t+\Delta t\in\left[  0,T\right]  $ and $x,x+\Delta x\in\mathbb{R}$, there exist constants $a,b\in\left[  0,1\right]  $ such that $$ f\left(  t+\Delta t,x+\Delta x\right)  -f\left(  t,x\right)  =\frac{\partial f\left(  t+a\Delta t,x\right)  }{\partial t}\Delta t+\frac{\partial f\left( t,x\right)  }{\partial x}\Delta x+\frac{1}{2}\frac{\partial^{2}f\left( t,x+b\Delta x\right)  }{\partial x^{2}}\left(  \Delta x\right)  ^{2}% $$ Remark, my method \begin{align*} &  f\left(  t+\Delta t,x+\Delta x\right)  -f\left(  t,x\right)  \\ &  =f\left(  t+\Delta t,x+\Delta x\right)  -f\left(  t,x+\Delta x\right) +f\left(  t,x+\Delta x\right)  -f\left(  t,x\right)  \\ &  =\frac{\partial f\left(  t+a\Delta t,x+\Delta x\right)  }{\partial t}\Delta t+\frac{\partial f\left(  t,x\right)  }{\partial x}\Delta x+\frac{1}{2}% \frac{\partial^{2}f\left(  t,x+b\Delta x\right)  }{\partial x^{2}}\left( \Delta x\right)  ^{2}% \end{align*} However, the first term is $\frac{\partial f\left(  t+a\Delta t,x+\Delta x\right)  }{\partial t}\Delta t$ not $\frac{\partial f\left(  t+a\Delta t,x\right)  }{\partial t}\Delta t$, say, the second variable in $f(\cdot,\cdot)$ is not $x$, but $x+\Delta x$.","Let the function $f:\left[  0,T\right]  \times\mathbb{R} \rightarrow\mathbb{R}$ be a function of time $t\in\left[  0,T\right]  $ and $x\in\mathbb{R}$, such that the partial derivatives $\partial f\left(  t,x\right)  /\partial t$ , $\partial f\left(  t,x\right)  /\partial x$ and $\partial^{2}f\left(  t,x\right)  /\partial x^{2}$ exist and are continuous for all $(t,x)\in\left[  0,T\right]  \times\mathbb{R}$. Then for any $t,t+\Delta t\in\left[  0,T\right]  $ and $x,x+\Delta x\in\mathbb{R}$, there exist constants $a,b\in\left[  0,1\right]  $ such that $$ f\left(  t+\Delta t,x+\Delta x\right)  -f\left(  t,x\right)  =\frac{\partial f\left(  t+a\Delta t,x\right)  }{\partial t}\Delta t+\frac{\partial f\left( t,x\right)  }{\partial x}\Delta x+\frac{1}{2}\frac{\partial^{2}f\left( t,x+b\Delta x\right)  }{\partial x^{2}}\left(  \Delta x\right)  ^{2}% $$ Remark, my method \begin{align*} &  f\left(  t+\Delta t,x+\Delta x\right)  -f\left(  t,x\right)  \\ &  =f\left(  t+\Delta t,x+\Delta x\right)  -f\left(  t,x+\Delta x\right) +f\left(  t,x+\Delta x\right)  -f\left(  t,x\right)  \\ &  =\frac{\partial f\left(  t+a\Delta t,x+\Delta x\right)  }{\partial t}\Delta t+\frac{\partial f\left(  t,x\right)  }{\partial x}\Delta x+\frac{1}{2}% \frac{\partial^{2}f\left(  t,x+b\Delta x\right)  }{\partial x^{2}}\left( \Delta x\right)  ^{2}% \end{align*} However, the first term is $\frac{\partial f\left(  t+a\Delta t,x+\Delta x\right)  }{\partial t}\Delta t$ not $\frac{\partial f\left(  t+a\Delta t,x\right)  }{\partial t}\Delta t$, say, the second variable in $f(\cdot,\cdot)$ is not $x$, but $x+\Delta x$.",,"['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
73,"Show that $f(x,y)=\frac{x^3}{x^2+y^2}$ is differentiable and that directional derivatives are linear combinations of partial derivatives",Show that  is differentiable and that directional derivatives are linear combinations of partial derivatives,"f(x,y)=\frac{x^3}{x^2+y^2}","Consider $f\colon\mathbb R^2\to\mathbb R$ with $f(0,0)=0$: $$ f(x,y)=\frac{x^3}{x^2+y^2}. $$ I need to show if $f$ is differentiable at $\vec 0$. One way of showing it, is noting that $f$ is homogeneous of degree 1. I have two questions about other approaches: 1) I would also like to show differentiability using the definition. I first calculated $D_1f(\vec0)=1$ and $D_2f(\vec 0)=0$. So I would need to show that for $\vec x=(x,y)\to (0,0)$, it holds that $$ f(\vec x)-f(\vec 0)-x=o(\Vert\vec x\Vert). $$ So basically I need $$ \lim_{(x,y)\to(0,0)}\frac{y^2}{(x^2+y^2)\sqrt{x^2+y^2}}=0. $$ I don't know how to show this limit. I can't bound stuff like $x^2+y^2\leq y^2$, because that doesn't help me anywhere. Any tips? edit I made an error, it should have been: $$ \lim_{(x,y)\to(0,0)}\frac{xy^2}{(x^2+y^2)\sqrt{x^2+y^2}}, $$ for which it is easy to show that this limit equals 0! So only my second question is still relevant (see below) 2) I also wanted to check if for $\vec u=(u_1,u_2)$, $D_{\vec u}f(\vec 0)$ is a linear combination of the partial derivatives. However, this doesn't seem to be true, for we have $$ D_{\vec u}f(\vec 0)=\lim_{t\to 0}\frac{t^3u_1^3}{t^3(u_1^2+u_2^2)}=\frac{u_1^3}{u_1^2+u_2^3}\neq u_1D_1f(\vec 0)=u_1. $$ So did something go wrong calculating the directional derivatives? $$ D_1f(\vec 0)=\lim_{t\to0}\frac{t^3}{t^3}=1 $$ and $$ D_2f(\vec 0)=\lim_{t\to0}\frac{0}{t}=0. $$ So how does the directional derivative not equal the linear combination of the partial derivatives, while $f$ is differentiable?","Consider $f\colon\mathbb R^2\to\mathbb R$ with $f(0,0)=0$: $$ f(x,y)=\frac{x^3}{x^2+y^2}. $$ I need to show if $f$ is differentiable at $\vec 0$. One way of showing it, is noting that $f$ is homogeneous of degree 1. I have two questions about other approaches: 1) I would also like to show differentiability using the definition. I first calculated $D_1f(\vec0)=1$ and $D_2f(\vec 0)=0$. So I would need to show that for $\vec x=(x,y)\to (0,0)$, it holds that $$ f(\vec x)-f(\vec 0)-x=o(\Vert\vec x\Vert). $$ So basically I need $$ \lim_{(x,y)\to(0,0)}\frac{y^2}{(x^2+y^2)\sqrt{x^2+y^2}}=0. $$ I don't know how to show this limit. I can't bound stuff like $x^2+y^2\leq y^2$, because that doesn't help me anywhere. Any tips? edit I made an error, it should have been: $$ \lim_{(x,y)\to(0,0)}\frac{xy^2}{(x^2+y^2)\sqrt{x^2+y^2}}, $$ for which it is easy to show that this limit equals 0! So only my second question is still relevant (see below) 2) I also wanted to check if for $\vec u=(u_1,u_2)$, $D_{\vec u}f(\vec 0)$ is a linear combination of the partial derivatives. However, this doesn't seem to be true, for we have $$ D_{\vec u}f(\vec 0)=\lim_{t\to 0}\frac{t^3u_1^3}{t^3(u_1^2+u_2^2)}=\frac{u_1^3}{u_1^2+u_2^3}\neq u_1D_1f(\vec 0)=u_1. $$ So did something go wrong calculating the directional derivatives? $$ D_1f(\vec 0)=\lim_{t\to0}\frac{t^3}{t^3}=1 $$ and $$ D_2f(\vec 0)=\lim_{t\to0}\frac{0}{t}=0. $$ So how does the directional derivative not equal the linear combination of the partial derivatives, while $f$ is differentiable?",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
74,Finding the unit normal to a cone.,Finding the unit normal to a cone.,,MY METHOD: I thought I would use the conventional method for finding the unit normal vector by calculating the gradient of S. Where $S: x^2 +y^2 - z^2 = 0$. $\hat n = \frac{\nabla S}{mag[\nabla S]}$ $\hat n = \frac{2x \hat i + 2y \hat j -2z \hat k}{\sqrt{(2x)^2 +(2y)^2 +(2z)^2   }}$ $\hat n = \frac{2x \hat i + 2y \hat j -2z \hat k}{\sqrt{(4x^2 +4y^2 +4z^2   }}$ $\hat n = \frac{2x \hat i + 2y \hat j -2z \hat k}{\sqrt{(4r^2 +4r^2}}$ giving $\hat n$ as: $\hat n = \frac{x \hat i + y \hat j -z \hat k}{\sqrt{2}}$ which is not equivalent to the solution above. Also does anyone know exactly what they did? I am having difficult to comprehend it. Why is are they calculating the cross product of the partials? Is it to do with multivariable chain rule - which is so can someone expand on this explanation by going more in depth? Addition Edit I saw this equation on the internet but am unsure why that is the case. Why are they multiplying again by the magnitude? Any help is much appreciated!!,MY METHOD: I thought I would use the conventional method for finding the unit normal vector by calculating the gradient of S. Where $S: x^2 +y^2 - z^2 = 0$. $\hat n = \frac{\nabla S}{mag[\nabla S]}$ $\hat n = \frac{2x \hat i + 2y \hat j -2z \hat k}{\sqrt{(2x)^2 +(2y)^2 +(2z)^2   }}$ $\hat n = \frac{2x \hat i + 2y \hat j -2z \hat k}{\sqrt{(4x^2 +4y^2 +4z^2   }}$ $\hat n = \frac{2x \hat i + 2y \hat j -2z \hat k}{\sqrt{(4r^2 +4r^2}}$ giving $\hat n$ as: $\hat n = \frac{x \hat i + y \hat j -z \hat k}{\sqrt{2}}$ which is not equivalent to the solution above. Also does anyone know exactly what they did? I am having difficult to comprehend it. Why is are they calculating the cross product of the partials? Is it to do with multivariable chain rule - which is so can someone expand on this explanation by going more in depth? Addition Edit I saw this equation on the internet but am unsure why that is the case. Why are they multiplying again by the magnitude? Any help is much appreciated!!,,"['calculus', 'integration', 'multivariable-calculus', 'vector-analysis', 'surfaces']"
75,Multi-dimensional chi-square distance,Multi-dimensional chi-square distance,,"The chi-square distance between continuous distributions $p(x)$ and $q(x)$ is, $$\int \frac{(p(x)-q(x))^2}{p(x)}dx$$ My question: what is the chi-square distance between multivariate continuous distributions $p(x_1,\ldots,x_n)$ and $q(x_1,\ldots,x_n)$? Do we simply have, $$\int\cdots\int \frac{\big(p(x_1,\ldots,x_n)-q(x_1,\ldots,x_n)\big)^2}{p(x_1,\ldots,x_n)}dx_1\ldots dx_n$$ or we should take some other considerations into account?","The chi-square distance between continuous distributions $p(x)$ and $q(x)$ is, $$\int \frac{(p(x)-q(x))^2}{p(x)}dx$$ My question: what is the chi-square distance between multivariate continuous distributions $p(x_1,\ldots,x_n)$ and $q(x_1,\ldots,x_n)$? Do we simply have, $$\int\cdots\int \frac{\big(p(x_1,\ldots,x_n)-q(x_1,\ldots,x_n)\big)^2}{p(x_1,\ldots,x_n)}dx_1\ldots dx_n$$ or we should take some other considerations into account?",,"['integration', 'multivariable-calculus', 'probability-distributions']"
76,Interpretation of eigenvectors of Hessian in context of local min/max/saddle?,Interpretation of eigenvectors of Hessian in context of local min/max/saddle?,,"Say $f \in C^2$ so we can possibly use its Hessian $H$ to determine whether $f$ has a local max, min, or saddle at a critical point $x_0$. Since $H(x_0)$ is real and symmetric, it is diagonalizable, say with eigenvector-eigenvalue pairs $(v_1,\lambda_1),\ldots,(v_n,\lambda_n)$. The second derivative test asserts that if all the $\lambda_i$ are strictly positive, then $f$ has a local min, if they are all strictly negative, then $f$ has a local max, and if there are at least one strictly positive and one strictly negative, then $f$ has a saddle point. Is there some geometric interpretation to what the $v_i$ are? Are the $v_i$ somehow directions in which the function restricted to that direction has concavity $\lambda_i$?","Say $f \in C^2$ so we can possibly use its Hessian $H$ to determine whether $f$ has a local max, min, or saddle at a critical point $x_0$. Since $H(x_0)$ is real and symmetric, it is diagonalizable, say with eigenvector-eigenvalue pairs $(v_1,\lambda_1),\ldots,(v_n,\lambda_n)$. The second derivative test asserts that if all the $\lambda_i$ are strictly positive, then $f$ has a local min, if they are all strictly negative, then $f$ has a local max, and if there are at least one strictly positive and one strictly negative, then $f$ has a saddle point. Is there some geometric interpretation to what the $v_i$ are? Are the $v_i$ somehow directions in which the function restricted to that direction has concavity $\lambda_i$?",,"['calculus', 'multivariable-calculus', 'optimization', 'stationary-point']"
77,Vector magnitude subtraction,Vector magnitude subtraction,,"Please help! This was in a textbook and I cannot seem to make sense of it. Would it not be 2? The two vectors a and b are perpendicular. If a has magnitude 8 and b has magnitude 3, what is  |a−2b|?? I","Please help! This was in a textbook and I cannot seem to make sense of it. Would it not be 2? The two vectors a and b are perpendicular. If a has magnitude 8 and b has magnitude 3, what is  |a−2b|?? I",,['multivariable-calculus']
78,Multivariate Gaussian integral over positive values,Multivariate Gaussian integral over positive values,,"Let $C$ be a positive definite matrix; the following multivariate integral is easy to calculate: $\int \frac{d^n\vec{v}}{\sqrt{(2\pi)^{n}|\det C|}}e^{-\frac{1}{2}\vec{v}^TC^{-1}\vec{v}}\vec{v}^TC^{-1}\vec{v}=n$ However, in my work on statistical mechanics I would like to calculate the integral on the positive domain of $\vec{v}$, i.e. $0\le v_i\ \forall i=1..n$: $\int_0^\infty \frac{d^n\vec{v}}{\sqrt{(2\pi)^{n}|\det C|}}e^{-\frac{1}{2}\vec{v}^TC^{-1}\vec{v}}\vec{v}^TC^{-1}\vec{v}=?\ (1)$ Alternatively, it may suffice for me to calculate a related integral: $\int \frac{d^n\vec{v}}{\sqrt{(2\pi)^{n}|\det C|}}e^{-\frac{1}{2}\vec{v}^TC^{-1}\vec{v}}|\vec{v}|^TC^{-1}|\vec{v}|=?\ (2)$ where $|\vec{v}|_i=|v_i|$. Any directions or ideas?","Let $C$ be a positive definite matrix; the following multivariate integral is easy to calculate: $\int \frac{d^n\vec{v}}{\sqrt{(2\pi)^{n}|\det C|}}e^{-\frac{1}{2}\vec{v}^TC^{-1}\vec{v}}\vec{v}^TC^{-1}\vec{v}=n$ However, in my work on statistical mechanics I would like to calculate the integral on the positive domain of $\vec{v}$, i.e. $0\le v_i\ \forall i=1..n$: $\int_0^\infty \frac{d^n\vec{v}}{\sqrt{(2\pi)^{n}|\det C|}}e^{-\frac{1}{2}\vec{v}^TC^{-1}\vec{v}}\vec{v}^TC^{-1}\vec{v}=?\ (1)$ Alternatively, it may suffice for me to calculate a related integral: $\int \frac{d^n\vec{v}}{\sqrt{(2\pi)^{n}|\det C|}}e^{-\frac{1}{2}\vec{v}^TC^{-1}\vec{v}}|\vec{v}|^TC^{-1}|\vec{v}|=?\ (2)$ where $|\vec{v}|_i=|v_i|$. Any directions or ideas?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'gaussian-integral']"
79,Expressing the solid determined by a triple integral in spherical coordinates,Expressing the solid determined by a triple integral in spherical coordinates,,"The problem prompts me to describe the solid determined by the triple integral $$ \int_{\pi/4}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3\csc\theta} f(\rho, \theta, \phi) \; \rho^2 \sin\phi \; d\rho \; d\theta \; d\phi $$ I am stuck at figuring out what $ \rho = 3\csc\theta $ (spherical coordinates) represents. In polar coordinates, $ r = \csc\theta $ is the line $ y = 1 $. Would this mean that $ \rho = 3\csc\theta $ is a rectangle of some sort? Edit: It has been confirmed that there was an error in the problem. $ \rho = 3\csc\theta $ should be $ \rho = 3\csc\phi $. The integral is therefore $$ \int_{\pi/4}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3\csc\phi} f(\rho, \theta, \phi) \; \rho^2 \sin\phi \; d\rho \; d\theta \; d\phi $$","The problem prompts me to describe the solid determined by the triple integral $$ \int_{\pi/4}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3\csc\theta} f(\rho, \theta, \phi) \; \rho^2 \sin\phi \; d\rho \; d\theta \; d\phi $$ I am stuck at figuring out what $ \rho = 3\csc\theta $ (spherical coordinates) represents. In polar coordinates, $ r = \csc\theta $ is the line $ y = 1 $. Would this mean that $ \rho = 3\csc\theta $ is a rectangle of some sort? Edit: It has been confirmed that there was an error in the problem. $ \rho = 3\csc\theta $ should be $ \rho = 3\csc\phi $. The integral is therefore $$ \int_{\pi/4}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3\csc\phi} f(\rho, \theta, \phi) \; \rho^2 \sin\phi \; d\rho \; d\theta \; d\phi $$",,"['multivariable-calculus', 'spherical-coordinates']"
80,"Prove that the second derivative are symmetric for this piecewise function at (0,0)","Prove that the second derivative are symmetric for this piecewise function at (0,0)",,"Based of Wikipedia -> Requirement of continuity I'm trying to figure out why the second derivative of the function is not symmetric. The function is: $$ f(x,y) = \begin{cases}                      \frac{xy(x^2 - y^2)}{x^2+y^2} & \mbox{ for } (x, y) \ne (0, 0)\\                       0                            & \mbox{ for } (x, y) = (0, 0).                 \end{cases} $$ (1) I want to be sure that i'm right doing the the first derivative at (0,0) $$ {\partial _xf(0,0) = } \lim_{t \to 0} \frac{f(0+t,0) - f(0,0)}{t} = 0 $$ (2) I don't understand how they obtain the second derivative, they say the second partial derivatives are not continuous at (0,0), and the symmetry fails. Can you explain how to obtain the second derivatives  $\partial _x \partial _yf$  and   $\partial _y \partial _xf$ at (0,0) and why the symmetry fails. Thank you in advance.","Based of Wikipedia -> Requirement of continuity I'm trying to figure out why the second derivative of the function is not symmetric. The function is: $$ f(x,y) = \begin{cases}                      \frac{xy(x^2 - y^2)}{x^2+y^2} & \mbox{ for } (x, y) \ne (0, 0)\\                       0                            & \mbox{ for } (x, y) = (0, 0).                 \end{cases} $$ (1) I want to be sure that i'm right doing the the first derivative at (0,0) $$ {\partial _xf(0,0) = } \lim_{t \to 0} \frac{f(0+t,0) - f(0,0)}{t} = 0 $$ (2) I don't understand how they obtain the second derivative, they say the second partial derivatives are not continuous at (0,0), and the symmetry fails. Can you explain how to obtain the second derivatives  $\partial _x \partial _yf$  and   $\partial _y \partial _xf$ at (0,0) and why the symmetry fails. Thank you in advance.",,['multivariable-calculus']
81,Second Partials test to classify a critical point,Second Partials test to classify a critical point,,"Look at the function. $h(u, v)  =  u^3 + 12uv − 6v^2$ (i) Find the critical points of h. (ii)    For each critical point in (a), find the value of D(a, b) from the Second Partials test that is used to classify the critical point. For i) I got the answer (0, 0), (-2, 2) by using the equations $h_u=-6u^2+12v=0$ and  $h_v=12u-6v=0$ How do I find the value of D(a,b) for ii)?","Look at the function. $h(u, v)  =  u^3 + 12uv − 6v^2$ (i) Find the critical points of h. (ii)    For each critical point in (a), find the value of D(a, b) from the Second Partials test that is used to classify the critical point. For i) I got the answer (0, 0), (-2, 2) by using the equations $h_u=-6u^2+12v=0$ and  $h_v=12u-6v=0$ How do I find the value of D(a,b) for ii)?",,"['calculus', 'multivariable-calculus']"
82,"Original and interesting problems about the theorems of Green, Stokes and Gauss","Original and interesting problems about the theorems of Green, Stokes and Gauss",,"Problems about these three classical theorems that we find in Calculus textbooks are usually in a low level. So, in this post, I'd like examples of original and  interesting problems involving such theorems that can be solved with a typical Calculus 3 course background.","Problems about these three classical theorems that we find in Calculus textbooks are usually in a low level. So, in this post, I'd like examples of original and  interesting problems involving such theorems that can be solved with a typical Calculus 3 course background.",,"['multivariable-calculus', 'vector-analysis', 'big-list']"
83,Calculus Word Problem: Several Variables,Calculus Word Problem: Several Variables,,"Demand for a certain kind of SUV obeys the following equation, $$D(x, y) = 21000 - \frac {\sqrt{x}}{2} - 11(0.3y-10)^{3/2}$$ where x is the price per car in dollars, y is the cost of gasoline per litre, and D is the number of cars. Suppose that the price of the car and the price of gasoline t years from now obey the following equations: $$x  =  55,200 + 100t ~~~~and~~~~      y = 136 + 10 \sqrt{t}$$ What will the rate of change of the demand be (with respect to time) 5 years from now? I would really appreciate getting some help answering this question. I truly don't know where to start with this question never mind the solution. What practice or formula should I be using to answer this question? New info First I plug t= 5 into both equations. so $x= 55700$ $y=158$ But is still don't know what to do after...","Demand for a certain kind of SUV obeys the following equation, $$D(x, y) = 21000 - \frac {\sqrt{x}}{2} - 11(0.3y-10)^{3/2}$$ where x is the price per car in dollars, y is the cost of gasoline per litre, and D is the number of cars. Suppose that the price of the car and the price of gasoline t years from now obey the following equations: $$x  =  55,200 + 100t ~~~~and~~~~      y = 136 + 10 \sqrt{t}$$ What will the rate of change of the demand be (with respect to time) 5 years from now? I would really appreciate getting some help answering this question. I truly don't know where to start with this question never mind the solution. What practice or formula should I be using to answer this question? New info First I plug t= 5 into both equations. so $x= 55700$ $y=158$ But is still don't know what to do after...",,"['calculus', 'multivariable-calculus', 'systems-of-equations']"
84,How do I know about which point Taylor Polynomial will be expanded?,How do I know about which point Taylor Polynomial will be expanded?,,"I'm going to try asking this question in two different ways, because either may be easier to understand. The first one is more specific, and the second one is more general. Specific Question Let's say that I have a function $f:\mathbb{R} \mapsto \mathbb{R} $ , and its taylor polynomial about point $P$ to degree $n$ is $P^f_n$ . Now let's say that I have another function $g: \mathbb{R}^n \mapsto \mathbb{R}$ . The domains and such of the functions assures that I can nest: $$z(\vec{x}) = f(g(\vec{x}))$$ given that $\vec{x} \in D(g)$ . If I do the same thing to the taylor polynomials, about which point will the new polynomial be expanded? To clarify, consider: $$P(\vec{x}) = P_n^f(g(\vec{x}))$$ To my knowledge, this nested polynomial is a taylor expansion of $z$ around a point, but if this is true, which one? General Question If I have a composite function $z:\mathbb{R}^n \mapsto \mathbb{R}$ and I want to find its taylor polynomial of degree $n$ around point $\vec{P}$ , how can I accomplish this using the taylor polynomials of the outer function? (And maybe even inner functions, if needed, such as with $z = \sin(\sin(x) + \sin(y))$ .) What are the exact demands that assures me that the result will be around the point $\vec{P}$ , rather than some other point? I'm mainly curious about the demands, because in the past, I've come across cases where not being certain of the demands, leading me to a completely different answer from what I expected. Basically, all I want is to be certain that a nested function is expanded around an asked point, when having the outer- and inner functions' taylor expansions. Complicated example (to visualize) Let's say that I have a slightly complicated function, such as: $$z(x, y) = \sin((x-1)^5(y+2)(x+3)(y-18)^{13})$$ and I want to find its taylor expansion around $(1,-2)$ to a certain degree. I know the taylor expansion for $\sin x$ around $x = 0$ , so one can argue that I can just substitute $(x-1)^5(y+2)(x+3)(y-18)^{13}$ in that formula, giving me the result for the expansion of $z$ around $(1, -2)$ , because: $$(x, y) = (1, -2) \implies (x-1)^5(y+2)(x+3)(y-18)^{13} = 0$$ However, this is a one-way arrow, meaning that there are other points around which it's $0$ . So couldn't you just argue that the obtained expansion, if you use this method, is around point $(-3, 18)$ as well? As you can tell, I'm somewhat confused about what's really going on.","I'm going to try asking this question in two different ways, because either may be easier to understand. The first one is more specific, and the second one is more general. Specific Question Let's say that I have a function , and its taylor polynomial about point to degree is . Now let's say that I have another function . The domains and such of the functions assures that I can nest: given that . If I do the same thing to the taylor polynomials, about which point will the new polynomial be expanded? To clarify, consider: To my knowledge, this nested polynomial is a taylor expansion of around a point, but if this is true, which one? General Question If I have a composite function and I want to find its taylor polynomial of degree around point , how can I accomplish this using the taylor polynomials of the outer function? (And maybe even inner functions, if needed, such as with .) What are the exact demands that assures me that the result will be around the point , rather than some other point? I'm mainly curious about the demands, because in the past, I've come across cases where not being certain of the demands, leading me to a completely different answer from what I expected. Basically, all I want is to be certain that a nested function is expanded around an asked point, when having the outer- and inner functions' taylor expansions. Complicated example (to visualize) Let's say that I have a slightly complicated function, such as: and I want to find its taylor expansion around to a certain degree. I know the taylor expansion for around , so one can argue that I can just substitute in that formula, giving me the result for the expansion of around , because: However, this is a one-way arrow, meaning that there are other points around which it's . So couldn't you just argue that the obtained expansion, if you use this method, is around point as well? As you can tell, I'm somewhat confused about what's really going on.","f:\mathbb{R} \mapsto \mathbb{R}  P n P^f_n g: \mathbb{R}^n \mapsto \mathbb{R} z(\vec{x}) = f(g(\vec{x})) \vec{x} \in D(g) P(\vec{x}) = P_n^f(g(\vec{x})) z z:\mathbb{R}^n \mapsto \mathbb{R} n \vec{P} z = \sin(\sin(x) + \sin(y)) \vec{P} z(x, y) = \sin((x-1)^5(y+2)(x+3)(y-18)^{13}) (1,-2) \sin x x = 0 (x-1)^5(y+2)(x+3)(y-18)^{13} z (1, -2) (x, y) = (1, -2) \implies (x-1)^5(y+2)(x+3)(y-18)^{13} = 0 0 (-3, 18)","['multivariable-calculus', 'taylor-expansion']"
85,Alternative proof of Schwarz's theorem using Fubini's theorem,Alternative proof of Schwarz's theorem using Fubini's theorem,,"I want to prove that for a function $f:\mathbb{R}^n\to\mathbb{R}$, if its mixed partial derivatives $D_{i,j}f$ and $D_{j,i}f$ exist and are continuous at a point $a$, then they are equal. I want to do it using Fubini's theorem (this is an excersise from Spivak's Calculus on manifolds ) and I would like you to tell me if the reasoning is correct. This is my approach: Suppose they are not equal, then we may assume $D_{i,j}f(a)>D_{j,i}f(a)$ and hence $D_{i,j}f(x)-D_{j,i}f(x)>0$ for all $x$ in a closed rectangle $R$ containing $a$. Let $[a,b]$ be the projection of $R$ over the $i$th coordinate axis and $[c,d]$ the projection over the $j$th coordinate axis. If we fix all the coordinates of $x$, except the $i$th and the $j$th,  then $D_{i,j}f(x)$ and $D_{j,i}f(x)$ are continuous in the compact region $[a,b]×[c,d]$, so they are integrable in that region and the same for $D_{i,j}f(x)-D_{j,i}f(x)$. If we also fix the $j$th coordinate of $D_{i,j}f(x)$ and the $i$th coordinate of $D_{j,i}f(x)$ we have (because of the FTC):    $$\int_{[a,b]}D_{i,j}f(x_i,x_j) d x_i=D_jf(b,x_j)-D_jf(a,x_j)$$   $$\int_{[c,d]}D_{j,i}f(x_i,x_j) d x_j=D_if(x_i,d)-D_if(x_i,c)$$   Now we can apply Fubini's theorem to obtain:   $$\int\int_{[a,b]×[c,d]}D_{i,j}f(x_i,x_j)=f(b,d)-f(b,c)-f(a,d)+f(a,c)$$   $$\int\int_{[a,b]×[c,d]}D_{j,i}f(x_i,x_j)=f(b,d)-f(b,c)-f(a,d)+f(a,c) $$   So, we have:   $$\int\int_{[a,b]×[c,d]}[D_{i,j}f(x_i,x_j)-D_{j,i}f(x_i,x_j)]=0$$   which contradicts the fact that $D_{i,j}f(x)-D_{j,i}f(x)>0$, so the mixed partials must be equal. Thanks","I want to prove that for a function $f:\mathbb{R}^n\to\mathbb{R}$, if its mixed partial derivatives $D_{i,j}f$ and $D_{j,i}f$ exist and are continuous at a point $a$, then they are equal. I want to do it using Fubini's theorem (this is an excersise from Spivak's Calculus on manifolds ) and I would like you to tell me if the reasoning is correct. This is my approach: Suppose they are not equal, then we may assume $D_{i,j}f(a)>D_{j,i}f(a)$ and hence $D_{i,j}f(x)-D_{j,i}f(x)>0$ for all $x$ in a closed rectangle $R$ containing $a$. Let $[a,b]$ be the projection of $R$ over the $i$th coordinate axis and $[c,d]$ the projection over the $j$th coordinate axis. If we fix all the coordinates of $x$, except the $i$th and the $j$th,  then $D_{i,j}f(x)$ and $D_{j,i}f(x)$ are continuous in the compact region $[a,b]×[c,d]$, so they are integrable in that region and the same for $D_{i,j}f(x)-D_{j,i}f(x)$. If we also fix the $j$th coordinate of $D_{i,j}f(x)$ and the $i$th coordinate of $D_{j,i}f(x)$ we have (because of the FTC):    $$\int_{[a,b]}D_{i,j}f(x_i,x_j) d x_i=D_jf(b,x_j)-D_jf(a,x_j)$$   $$\int_{[c,d]}D_{j,i}f(x_i,x_j) d x_j=D_if(x_i,d)-D_if(x_i,c)$$   Now we can apply Fubini's theorem to obtain:   $$\int\int_{[a,b]×[c,d]}D_{i,j}f(x_i,x_j)=f(b,d)-f(b,c)-f(a,d)+f(a,c)$$   $$\int\int_{[a,b]×[c,d]}D_{j,i}f(x_i,x_j)=f(b,d)-f(b,c)-f(a,d)+f(a,c) $$   So, we have:   $$\int\int_{[a,b]×[c,d]}[D_{i,j}f(x_i,x_j)-D_{j,i}f(x_i,x_j)]=0$$   which contradicts the fact that $D_{i,j}f(x)-D_{j,i}f(x)>0$, so the mixed partials must be equal. Thanks",,"['multivariable-calculus', 'proof-verification', 'proof-writing', 'alternative-proof']"
86,"Use the level curves of the function to determine if each partial derivative at the point P is positive, negative, or zero.","Use the level curves of the function to determine if each partial derivative at the point P is positive, negative, or zero.",,"It is asking me to find $f_{xx}$, $f_{yy}$, $f_x$, $f_y$, but I'm really unsure of how to determine the characteristics of the partial derivative. Please give me some guidance, thanks.","It is asking me to find $f_{xx}$, $f_{yy}$, $f_x$, $f_y$, but I'm really unsure of how to determine the characteristics of the partial derivative. Please give me some guidance, thanks.",,"['multivariable-calculus', 'partial-derivative']"
87,Is this condition necessary for a function $f : \mathbb{R}^m \to \mathbb{R}^n$ to be differentiable?,Is this condition necessary for a function  to be differentiable?,f : \mathbb{R}^m \to \mathbb{R}^n,"Consider the following definitions: A function $f : U \subseteq \mathbb{R}^m \to \mathbb{R}^n$ (where $U$ is an open set) is differentiable at a point $a \in U$ if there exists a linear map $T : \mathbb{R}^m \to \mathbb{R}^n$ such that $$\lim_{v \to 0} \dfrac{f(a+v) - f(a) - T(v)}{\|v\|} = 0$$ The directional derivative of $f$ at $a$ in the direction of $v \in \mathbb{R}^m$ is $$\dfrac{\partial f}{\partial v}(a) = \lim_{t \to 0} \dfrac{f(a+tv) - f(a)}{t}$$ Now, it is easy to show that $\dfrac{\partial f}{\partial (\alpha v)}(a) = \alpha\dfrac{\partial f}{\partial v}(a)$ for any $v$ and $\alpha \neq 0$ (if it exists). It is also easy to show that if $f$ is differentiable at $a$, then $\dfrac{\partial f}{\partial v}(a)$ exists for all $v$, with the special property that $$\forall \, v,w \in \mathbb{R^m} \qquad \dfrac{\partial f}{\partial (v + w)}(a) = \dfrac{\partial f}{\partial v}(a) + \dfrac{\partial f}{\partial w}(a)$$ So far so good. Now, there is an equivalent definition of differentiability at a point, as follows: (Alternative Definition) A function $f : U \subseteq \mathbb{R}^m \to \mathbb{R}^n$ (where $U$ is an open set) is differentiable at a point $a \in U$ if all the following conditions hold: For all $v \in \mathbb{R}^m$ the directional derivative $\dfrac{\partial f}{\partial v}(a)$ exists. For all $v,w \in \mathbb{R^m}$, we have $\dfrac{\partial f}{\partial (v + w)}(a) = \dfrac{\partial f}{\partial v}(a) + \dfrac{\partial f}{\partial w}(a)$. $\lim_\limits{v \to 0} \dfrac{f(a+v) - f(a) - \frac{\partial f}{\partial v}(a)}{\|v\|} = 0$ I also understand that this definition is equivalent to the first one. So far so good. But is the second bullet on the Alternative Definition necessary? Or does it follow from the first and third bullets?","Consider the following definitions: A function $f : U \subseteq \mathbb{R}^m \to \mathbb{R}^n$ (where $U$ is an open set) is differentiable at a point $a \in U$ if there exists a linear map $T : \mathbb{R}^m \to \mathbb{R}^n$ such that $$\lim_{v \to 0} \dfrac{f(a+v) - f(a) - T(v)}{\|v\|} = 0$$ The directional derivative of $f$ at $a$ in the direction of $v \in \mathbb{R}^m$ is $$\dfrac{\partial f}{\partial v}(a) = \lim_{t \to 0} \dfrac{f(a+tv) - f(a)}{t}$$ Now, it is easy to show that $\dfrac{\partial f}{\partial (\alpha v)}(a) = \alpha\dfrac{\partial f}{\partial v}(a)$ for any $v$ and $\alpha \neq 0$ (if it exists). It is also easy to show that if $f$ is differentiable at $a$, then $\dfrac{\partial f}{\partial v}(a)$ exists for all $v$, with the special property that $$\forall \, v,w \in \mathbb{R^m} \qquad \dfrac{\partial f}{\partial (v + w)}(a) = \dfrac{\partial f}{\partial v}(a) + \dfrac{\partial f}{\partial w}(a)$$ So far so good. Now, there is an equivalent definition of differentiability at a point, as follows: (Alternative Definition) A function $f : U \subseteq \mathbb{R}^m \to \mathbb{R}^n$ (where $U$ is an open set) is differentiable at a point $a \in U$ if all the following conditions hold: For all $v \in \mathbb{R}^m$ the directional derivative $\dfrac{\partial f}{\partial v}(a)$ exists. For all $v,w \in \mathbb{R^m}$, we have $\dfrac{\partial f}{\partial (v + w)}(a) = \dfrac{\partial f}{\partial v}(a) + \dfrac{\partial f}{\partial w}(a)$. $\lim_\limits{v \to 0} \dfrac{f(a+v) - f(a) - \frac{\partial f}{\partial v}(a)}{\|v\|} = 0$ I also understand that this definition is equivalent to the first one. So far so good. But is the second bullet on the Alternative Definition necessary? Or does it follow from the first and third bullets?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
88,What does one need measure zero for? [closed],What does one need measure zero for? [closed],,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I'm taking multivariable calculus course (third one actually) and we're introduced to measure zero. What does one need measure zero for? Measure zero of a set $A$ in $\mathbb{R}^n$ means that one can find a collection of sets whose volume can be made arbitrarily small and so that $A$ is contained in the sum over such collection. Why is this an important property?,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I'm taking multivariable calculus course (third one actually) and we're introduced to measure zero. What does one need measure zero for? Measure zero of a set $A$ in $\mathbb{R}^n$ means that one can find a collection of sets whose volume can be made arbitrarily small and so that $A$ is contained in the sum over such collection. Why is this an important property?,,['multivariable-calculus']
89,Integral of a function over the unit ball,Integral of a function over the unit ball,,"How does one compute the integral of $|x|^{- \alpha}$ over $B_1 (0) \subset R^n$, where $0 < \alpha < n$? I know the function is Lebesgue integrable because $\alpha < n$, but I'm need of an exact formula.","How does one compute the integral of $|x|^{- \alpha}$ over $B_1 (0) \subset R^n$, where $0 < \alpha < n$? I know the function is Lebesgue integrable because $\alpha < n$, but I'm need of an exact formula.",,"['real-analysis', 'integration', 'multivariable-calculus']"
90,uniform convergence of two variable-functions,uniform convergence of two variable-functions,,"Let $f(x,y)$ be a real-valued two-variable function on the plane. (1). $\lim_{x\to\infty,y\to\infty}f(x,y)$ exists. (2). for any fixed $y$, $\lim_{x\to\infty}f(x,y)$ exists and converges uniformly for all $y\in \mathbb{R}$. (3). for any fixed $x$, $\lim_{y\to\infty}f(x,y)$ exists and converges uniformly for all $x\in \mathbb{R}$. Question: Whether does (1) imply (2) and (3) or not? I want to find a sufficient condition of (2) and (3).","Let $f(x,y)$ be a real-valued two-variable function on the plane. (1). $\lim_{x\to\infty,y\to\infty}f(x,y)$ exists. (2). for any fixed $y$, $\lim_{x\to\infty}f(x,y)$ exists and converges uniformly for all $y\in \mathbb{R}$. (3). for any fixed $x$, $\lim_{y\to\infty}f(x,y)$ exists and converges uniformly for all $x\in \mathbb{R}$. Question: Whether does (1) imply (2) and (3) or not? I want to find a sufficient condition of (2) and (3).",,"['calculus', 'real-analysis', 'functions', 'multivariable-calculus', 'convergence-divergence']"
91,Partial derivative isn't continuous,Partial derivative isn't continuous,,"I would like verification on a calculation that shows $f$ below is differentiable at $(0,0)$ but $f_x$ is not continuous at $(0,0)$. $$f(x,y)=(x^2+y^2)\sin(\frac{1}{x^2+y^2})$$ when $(x,y)\neq (0,0)$ and $f(0,0)=0$. So assuming for the moment that the function is differentiable at $(0,0)$ (it is), this shows that $f_x(0,0)$ exists.  In fact, it is easy to see $f_x(0,0)=0$ by applying the definition of the derivative and using the squeeze theorem.  Then by standard computation we get that away from the origin  $$f_x(x,y)=2x\left(\sin\left(\frac{1}{x^2+y^2}\right)-\frac{\cos\left(\frac{1}{x^2+y^2}\right)}{x^2+y^2}\right)$$ Here is where I'm a little fuzzy....   I found that $\lim_{(x,0)\to (0,0)} f_x=0$ and so this isn't helpful.   I need to find a path that makes the limit not equal to zero, right?  What if I find a path where the limit doesn't exist? Is this enough?  I have a result that says $$\lim_{(x,y)\to(0,0)}g(x,y)=\lim_{r\to 0^+} g(r\cos\theta,r\sin\theta)$$ I guess I prefer working in these coordinates because then I don't have to worry about the path.  I computed the following $$\lim_{r\to 0^+} f_x(r,\theta)=2r\cos\theta\left(\sin(r^{-2})-r^{-2}\cos(r^{-2})\right)$$ Where $2r\cos\theta\sin(\frac{1}{r^2})\to0$ by the squeeze theorem and the second term's limit does not exist (right?)  So I have two questions: 1) Does this prove that $f_x$ is not continuous since this limit does not exist? Or should I work in Cartesian coordinates and find a path that shows the limit depends on the path. 2) Can I always do this switch to polar coordinates (where the above centred equation holds true)?  Does there need to be polar symmetry?  I don't believe this function has polar symmetry.","I would like verification on a calculation that shows $f$ below is differentiable at $(0,0)$ but $f_x$ is not continuous at $(0,0)$. $$f(x,y)=(x^2+y^2)\sin(\frac{1}{x^2+y^2})$$ when $(x,y)\neq (0,0)$ and $f(0,0)=0$. So assuming for the moment that the function is differentiable at $(0,0)$ (it is), this shows that $f_x(0,0)$ exists.  In fact, it is easy to see $f_x(0,0)=0$ by applying the definition of the derivative and using the squeeze theorem.  Then by standard computation we get that away from the origin  $$f_x(x,y)=2x\left(\sin\left(\frac{1}{x^2+y^2}\right)-\frac{\cos\left(\frac{1}{x^2+y^2}\right)}{x^2+y^2}\right)$$ Here is where I'm a little fuzzy....   I found that $\lim_{(x,0)\to (0,0)} f_x=0$ and so this isn't helpful.   I need to find a path that makes the limit not equal to zero, right?  What if I find a path where the limit doesn't exist? Is this enough?  I have a result that says $$\lim_{(x,y)\to(0,0)}g(x,y)=\lim_{r\to 0^+} g(r\cos\theta,r\sin\theta)$$ I guess I prefer working in these coordinates because then I don't have to worry about the path.  I computed the following $$\lim_{r\to 0^+} f_x(r,\theta)=2r\cos\theta\left(\sin(r^{-2})-r^{-2}\cos(r^{-2})\right)$$ Where $2r\cos\theta\sin(\frac{1}{r^2})\to0$ by the squeeze theorem and the second term's limit does not exist (right?)  So I have two questions: 1) Does this prove that $f_x$ is not continuous since this limit does not exist? Or should I work in Cartesian coordinates and find a path that shows the limit depends on the path. 2) Can I always do this switch to polar coordinates (where the above centred equation holds true)?  Does there need to be polar symmetry?  I don't believe this function has polar symmetry.",,"['real-analysis', 'multivariable-calculus']"
92,How to integrate this? triple integral over a region,How to integrate this? triple integral over a region,,"I need to use a triple integral to get the volume in the region bounded by $y=3-x$, $y=0$, $z=x^2$, $z=1$. I managed to plot the lines in the $xy$ plane: $y=0$ and $y=3-x$, then I got $z=0$ and $z=x^2$ in the xz plane. From this I saw that I can use the following boundaries: $x^2 \le z \le 1$ and $0 \le y \le 3-x$. However, I'm failing to see what are x boundaries. This is the Microsoft Mathematics plot I got (I had to rotate it a bit to get an idea of what was the region): Now, how should I go to find x boundaries? Is it ok to take $z=x^2$ and $z=1$ then solve $1=x^2$ to get that $x$ goes from $-1$ to $1$? In that case, this is what I did, and got a negative volume: Is that correct?","I need to use a triple integral to get the volume in the region bounded by $y=3-x$, $y=0$, $z=x^2$, $z=1$. I managed to plot the lines in the $xy$ plane: $y=0$ and $y=3-x$, then I got $z=0$ and $z=x^2$ in the xz plane. From this I saw that I can use the following boundaries: $x^2 \le z \le 1$ and $0 \le y \le 3-x$. However, I'm failing to see what are x boundaries. This is the Microsoft Mathematics plot I got (I had to rotate it a bit to get an idea of what was the region): Now, how should I go to find x boundaries? Is it ok to take $z=x^2$ and $z=1$ then solve $1=x^2$ to get that $x$ goes from $-1$ to $1$? In that case, this is what I did, and got a negative volume: Is that correct?",,"['integration', 'multivariable-calculus']"
93,"Derivative $\frac{d\left\{B(A_1, \ldots, A_M)\right\}}{d\left\{C(A_1, \ldots, A_M)\right\}}$ equals what?",Derivative  equals what?,"\frac{d\left\{B(A_1, \ldots, A_M)\right\}}{d\left\{C(A_1, \ldots, A_M)\right\}}","Question : \begin{equation} \dfrac{dB}{dC} = ? \end{equation} Context The present problem might be considered a practical exercise with respect to the ongoing discussion in [1].  In addition, [2] offers a related question in that both the question here and there are derivatives with respect to a function. Given : \begin{align} A & = f{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{A_k} = \textrm{constant   }\\ B & = g{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{B_k(A_k)}  \\ C & = h{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{C_k(A_k)}  \end{align} Approach I: Ratio of Two Differentials : The differentials of $A$, $B$, and $C$, are respectively written as \begin{align} dA & =  \sum_{k = 1}^{M}{dA_k} = 0 \quad\quad  \textrm{Eq. 1} \\ dB & =  \sum_{k = 1}^{M}{\dfrac{\partial{B_k}}{\partial{A_k}}}  \, dA_k \quad\quad  \textrm{Eq. 2} \\ dC & =  \sum_{k = 1}^{M}{\dfrac{\partial{C_k}}{\partial{A_k}}}  \, dA_k \quad\quad  \textrm{Eq. 3} \end{align} Next, arbitrarily singling out the $M$th differential, $dA_M$, we rewrite Eq. 1 as \begin{align} dA_M & =  -\sum_{k = 1}^{M-1}{dA_k} \end{align} and continue to rewrite Eqs. 2 and 3 as \begin{align} dB & =  \sum_{k = 1}^{M-1}\left[\dfrac{\partial{B_k}}{\partial{A_k}}  - \dfrac{\partial{B_M}}{\partial{A_M}} \right]  \, dA_k \\ dC & =  \sum_{k = 1}^{M-1}\left[\dfrac{\partial{C_k}}{\partial{A_k}}  - \dfrac{\partial{C_M}}{\partial{A_M}} \right]  \, dA_k \end{align} Naively or not, I write the derivative as the ratio of the differentials. My solution is thus \begin{equation} \dfrac{dB}{dC} = \dfrac{\sum_{k = 1}^{M-1}\left[\dfrac{\partial{B_k}}{\partial{A_k}}  - \dfrac{\partial{B_M}}{\partial{A_M}} \right]  \, dA_k}{\sum_{k = 1}^{M-1}\left[\dfrac{\partial{C_k}}{\partial{A_k}}  - \dfrac{\partial{C_M}}{\partial{A_M}} \right]  \, dA_k}. \end{equation} Approach II: Chain Rule : From [3], let $A=A(A_1, A_2, \ldots, A_M) = \textrm{constant}$,   $B = B(A_1, A_2, \ldots, A_M)$, and $C =  C(A_1, A_2, \ldots, A_M)$, then the partial derivative of $C$ with respect to $B$, holding $A$ constant, written $\left(\dfrac{\partial{C}}{\partial{B}}\right)_A$, can be expressed as \begin{align} \left(\dfrac{\partial{B}}{\partial{C}}\right)_A = \sum\limits_{k=1}^{M} {   \left(\dfrac{\partial{B}}{\partial{A_k}}\right)_{\substack{A_{1,\ldots, j, \ldots, M}\\ k \neq j }}  \, \left(\dfrac{\partial{A_k}}{\partial{C}}\right)_A    } \end{align} The subscripts on the parenthesis above indicate which variables are considered constants with respect to the given partial derivative.  In particular, $\substack{A_{1,\ldots, M}\\ k \neq j }$ indicates that all of the $A_j; j\in 1,\ldots, j, \ldots, M$ are considered constants, save the partial derivative in question (i.e, the $k^\textrm{th}$ one). Discussion Question: Why did I start by asking for a a derivative and end up with a partial derivative? Answer: The initial question appears to be ill-posed. As opposed to finding the derivative, what I really saught all along was $\left(\dfrac{\partial\left\{B(A_1, \ldots, A_M)\right\}}{\partial\left\{C(A_1, \ldots, A_M)\right\}}\right)_A$. I am purposefully leaving the title as a misnomer because like myself, there might be others who would not think to search for a constrained partial derivative. Conclusion Beyond the solution to my question, this question offers a demonstration that the ratio of two differentials is not a derivative. Referencess [1] Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? [2] Differentiating with respect to a function [3] CRC Math Encyclopedia, Edition XX, 336.","Question : \begin{equation} \dfrac{dB}{dC} = ? \end{equation} Context The present problem might be considered a practical exercise with respect to the ongoing discussion in [1].  In addition, [2] offers a related question in that both the question here and there are derivatives with respect to a function. Given : \begin{align} A & = f{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{A_k} = \textrm{constant   }\\ B & = g{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{B_k(A_k)}  \\ C & = h{\left(A_1, \ldots, A_M\right)} =   \sum_{k = 1}^{M}{C_k(A_k)}  \end{align} Approach I: Ratio of Two Differentials : The differentials of $A$, $B$, and $C$, are respectively written as \begin{align} dA & =  \sum_{k = 1}^{M}{dA_k} = 0 \quad\quad  \textrm{Eq. 1} \\ dB & =  \sum_{k = 1}^{M}{\dfrac{\partial{B_k}}{\partial{A_k}}}  \, dA_k \quad\quad  \textrm{Eq. 2} \\ dC & =  \sum_{k = 1}^{M}{\dfrac{\partial{C_k}}{\partial{A_k}}}  \, dA_k \quad\quad  \textrm{Eq. 3} \end{align} Next, arbitrarily singling out the $M$th differential, $dA_M$, we rewrite Eq. 1 as \begin{align} dA_M & =  -\sum_{k = 1}^{M-1}{dA_k} \end{align} and continue to rewrite Eqs. 2 and 3 as \begin{align} dB & =  \sum_{k = 1}^{M-1}\left[\dfrac{\partial{B_k}}{\partial{A_k}}  - \dfrac{\partial{B_M}}{\partial{A_M}} \right]  \, dA_k \\ dC & =  \sum_{k = 1}^{M-1}\left[\dfrac{\partial{C_k}}{\partial{A_k}}  - \dfrac{\partial{C_M}}{\partial{A_M}} \right]  \, dA_k \end{align} Naively or not, I write the derivative as the ratio of the differentials. My solution is thus \begin{equation} \dfrac{dB}{dC} = \dfrac{\sum_{k = 1}^{M-1}\left[\dfrac{\partial{B_k}}{\partial{A_k}}  - \dfrac{\partial{B_M}}{\partial{A_M}} \right]  \, dA_k}{\sum_{k = 1}^{M-1}\left[\dfrac{\partial{C_k}}{\partial{A_k}}  - \dfrac{\partial{C_M}}{\partial{A_M}} \right]  \, dA_k}. \end{equation} Approach II: Chain Rule : From [3], let $A=A(A_1, A_2, \ldots, A_M) = \textrm{constant}$,   $B = B(A_1, A_2, \ldots, A_M)$, and $C =  C(A_1, A_2, \ldots, A_M)$, then the partial derivative of $C$ with respect to $B$, holding $A$ constant, written $\left(\dfrac{\partial{C}}{\partial{B}}\right)_A$, can be expressed as \begin{align} \left(\dfrac{\partial{B}}{\partial{C}}\right)_A = \sum\limits_{k=1}^{M} {   \left(\dfrac{\partial{B}}{\partial{A_k}}\right)_{\substack{A_{1,\ldots, j, \ldots, M}\\ k \neq j }}  \, \left(\dfrac{\partial{A_k}}{\partial{C}}\right)_A    } \end{align} The subscripts on the parenthesis above indicate which variables are considered constants with respect to the given partial derivative.  In particular, $\substack{A_{1,\ldots, M}\\ k \neq j }$ indicates that all of the $A_j; j\in 1,\ldots, j, \ldots, M$ are considered constants, save the partial derivative in question (i.e, the $k^\textrm{th}$ one). Discussion Question: Why did I start by asking for a a derivative and end up with a partial derivative? Answer: The initial question appears to be ill-posed. As opposed to finding the derivative, what I really saught all along was $\left(\dfrac{\partial\left\{B(A_1, \ldots, A_M)\right\}}{\partial\left\{C(A_1, \ldots, A_M)\right\}}\right)_A$. I am purposefully leaving the title as a misnomer because like myself, there might be others who would not think to search for a constrained partial derivative. Conclusion Beyond the solution to my question, this question offers a demonstration that the ratio of two differentials is not a derivative. Referencess [1] Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? [2] Differentiating with respect to a function [3] CRC Math Encyclopedia, Edition XX, 336.",,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'differential']"
94,Computing the average of $\prod_i (1-\frac{|x_i|}{L})$ on the surface of the unit $n$-sphere,Computing the average of  on the surface of the unit -sphere,\prod_i (1-\frac{|x_i|}{L}) n,"I'm trying to compute the expected value of $ \prod_i (1-\frac{|x_i|}{L})$ on the surface of a $n$-dimensional sphere. A first step could be to integrate only on the first quadrant to take out the absolute value (since the function is symmetric). The intuition behind it (if I'm not wrong) is that for $L>1$  that product is the probability that, if we put edge-parallel planes in every dimension separated by distance $L$, a point in the surface of the unit sphere will end up in the same hypercube as the origin. For 2 dimensions it's easy because we can pass to polar coordinates: $$\frac{2}{\pi}\int_{\theta=0}^{\pi/2} (1-\frac{\cos \theta}{L})(1-\frac{\sin \theta}{L}) = 1- \frac{4L-1}{\pi L^2}$$ What about higher dimensions?","I'm trying to compute the expected value of $ \prod_i (1-\frac{|x_i|}{L})$ on the surface of a $n$-dimensional sphere. A first step could be to integrate only on the first quadrant to take out the absolute value (since the function is symmetric). The intuition behind it (if I'm not wrong) is that for $L>1$  that product is the probability that, if we put edge-parallel planes in every dimension separated by distance $L$, a point in the surface of the unit sphere will end up in the same hypercube as the origin. For 2 dimensions it's easy because we can pass to polar coordinates: $$\frac{2}{\pi}\int_{\theta=0}^{\pi/2} (1-\frac{\cos \theta}{L})(1-\frac{\sin \theta}{L}) = 1- \frac{4L-1}{\pi L^2}$$ What about higher dimensions?",,"['calculus', 'integration', 'multivariable-calculus', 'spheres', 'surface-integrals']"
95,"Soft question: How to pronounce ""Clairaut""","Soft question: How to pronounce ""Clairaut""",,"I'm preparing for a presentation tomorrow and I will refer to clairaut's theorem in multivariable calculus (mixed partials are equal) Can French speakers chime in as to how to correctly pronounce clairaut (for an english speaker). Is it pronounced ""clear-roo"" or.. Thanks!","I'm preparing for a presentation tomorrow and I will refer to clairaut's theorem in multivariable calculus (mixed partials are equal) Can French speakers chime in as to how to correctly pronounce clairaut (for an english speaker). Is it pronounced ""clear-roo"" or.. Thanks!",,"['multivariable-calculus', 'soft-question', 'terminology']"
96,Multiple integration,Multiple integration,,"We know that the double integration is used to find the area of a planer region and volume of a solid object in space. My question is why we use triple integration to find again the volume a body? (I know the importance of triple integration, any boddy plz)","We know that the double integration is used to find the area of a planer region and volume of a solid object in space. My question is why we use triple integration to find again the volume a body? (I know the importance of triple integration, any boddy plz)",,"['calculus', 'multivariable-calculus', 'multiple-integral']"
97,addition theorem for product of THREE Bessel functions: $\sum_n I_{n+m}(a) I_{n-m}(b) I_{p-n}(c)$,addition theorem for product of THREE Bessel functions:,\sum_n I_{n+m}(a) I_{n-m}(b) I_{p-n}(c),"Working on a problem of lattice Green functions (LGF) I encountered a summation shown in the title question: $$ \sum_{n=-\infty}^\infty  I_{n+m}(a) I_{n-m}(b) I_{p-n}(c) . $$  It is part of wider summation:  $$ \sum_{n=-\infty}^\infty \sum_{m=-\infty}^\infty  I_{n+m}(a) I_{n-m}(b) I_{p-n}(c) I_{q+m}(c) $$ where all the indices n, m, p, q are integers.  The above formula is part of still wider integrals: $$ \int_0^\infty dt \exp(-E t)\sum_n \sum_m  I_{n+m}(a t) I_{n-m}(b t) I_{p-n}(c t) I_{q+m}(c t) $$ or $$ \int_0^\infty dt \exp(-E t) I_w(f t) \sum_n \sum_m   I_{n+m}(a t) I_{n-m}(b t) I_{p-n}(c t) I_{q+m}(c t) $$ the former is LGF in 2 dimensions (2d) the latter - in 3d. Does any one know, how to simplify any of these formulas getting rid of summations? Or how (IF) it could be done? There is Neumann/Graf addition formula which does that for the product of TWO Bessel functions but has any of you seen an analogue for three? How to intersperse the third index into summation? Any simplified versions for $a=b$ or $p=0$ or $q=0$ or $p=0=q$ or with $J_n$ instead of $I_n$ would be also of use. I tried various formulas from Gradshtein-Ryzhik and Abramovitz-Stegun but to no avail. Any hint how to attack the problem?","Working on a problem of lattice Green functions (LGF) I encountered a summation shown in the title question: $$ \sum_{n=-\infty}^\infty  I_{n+m}(a) I_{n-m}(b) I_{p-n}(c) . $$  It is part of wider summation:  $$ \sum_{n=-\infty}^\infty \sum_{m=-\infty}^\infty  I_{n+m}(a) I_{n-m}(b) I_{p-n}(c) I_{q+m}(c) $$ where all the indices n, m, p, q are integers.  The above formula is part of still wider integrals: $$ \int_0^\infty dt \exp(-E t)\sum_n \sum_m  I_{n+m}(a t) I_{n-m}(b t) I_{p-n}(c t) I_{q+m}(c t) $$ or $$ \int_0^\infty dt \exp(-E t) I_w(f t) \sum_n \sum_m   I_{n+m}(a t) I_{n-m}(b t) I_{p-n}(c t) I_{q+m}(c t) $$ the former is LGF in 2 dimensions (2d) the latter - in 3d. Does any one know, how to simplify any of these formulas getting rid of summations? Or how (IF) it could be done? There is Neumann/Graf addition formula which does that for the product of TWO Bessel functions but has any of you seen an analogue for three? How to intersperse the third index into summation? Any simplified versions for $a=b$ or $p=0$ or $q=0$ or $p=0=q$ or with $J_n$ instead of $I_n$ would be also of use. I tried various formulas from Gradshtein-Ryzhik and Abramovitz-Stegun but to no avail. Any hint how to attack the problem?",,"['calculus', 'multivariable-calculus', 'special-functions', 'bessel-functions', 'hypergeometric-function']"
98,Find the following partial derivatives for a nested function? [closed],Find the following partial derivatives for a nested function? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question For $g(t,x) = f(t , h(t,x))$ find the following partial derivatives:  $\frac{\partial g}{\partial t}$, $\frac{\partial g}{\partial x}$, $\frac{\partial^2 g}{\partial x^2}$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question For $g(t,x) = f(t , h(t,x))$ find the following partial derivatives:  $\frac{\partial g}{\partial t}$, $\frac{\partial g}{\partial x}$, $\frac{\partial^2 g}{\partial x^2}$",,"['multivariable-calculus', 'partial-derivative', 'chain-rule']"
99,A function not constant on a connected set of critical points?,A function not constant on a connected set of critical points?,,"I was looking for a function differentiable on a set, such that every point would be critical, and still the function would not be constant(everywhere). I found this reference If someone has access to this paper, I would greatly appreciate it. I think it's different from other question on this site, where they were allowing for zero derivative-a.e. functions, and I'm not... Any help would be appreciated.","I was looking for a function differentiable on a set, such that every point would be critical, and still the function would not be constant(everywhere). I found this reference If someone has access to this paper, I would greatly appreciate it. I think it's different from other question on this site, where they were allowing for zero derivative-a.e. functions, and I'm not... Any help would be appreciated.",,"['real-analysis', 'multivariable-calculus']"
