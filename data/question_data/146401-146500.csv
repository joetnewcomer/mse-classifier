,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Compute limit using Taylor's expansion,Compute limit using Taylor's expansion,,"Using Taylor’s expansion, prove that the following limit exists and compute it. $$\lim_{x \to 0}\left(\frac {x^2}{\frac {1}{1-x} - e^x}\right)$$ In this if I am using the taylor series expansion of $e^x$ then denominator has some value as $$ 1 + (1 + x + x^2 + x^3 + \ldots) + x (1 + x + x^2 + x^3 + \ldots) $$ I am not getting how to proceed further with this.","Using Taylor’s expansion, prove that the following limit exists and compute it. $$\lim_{x \to 0}\left(\frac {x^2}{\frac {1}{1-x} - e^x}\right)$$ In this if I am using the taylor series expansion of $e^x$ then denominator has some value as $$ 1 + (1 + x + x^2 + x^3 + \ldots) + x (1 + x + x^2 + x^3 + \ldots) $$ I am not getting how to proceed further with this.",,"['calculus', 'limits', 'taylor-expansion']"
1,Prove a function $f(x)$ has a limit as $x$ approaches $1$.,Prove a function  has a limit as  approaches .,f(x) x 1,"I am trying to prove the function $f(x) = (1-x)/(1-\sqrt{x})$ has a limit as $x$ approaches 1 using an epsilon definition. I've gotten as far as finding the limit is $2$ by factoring the numerator, as well as setting up the proof, which is as follows: Let $\epsilon>0$ be given. Then there exists $\delta>0$ such that $|(1-x)/(1-\sqrt{x}) - 2| < \epsilon$ if $0 < x-1 < \delta$, $x$ element of the domain, and $1$ is an accumulation point. Now, simplifying the absolute value gives $\left|\frac{(1-x) - 2(1-\sqrt{x})}{1-\sqrt{x}}\right| < \epsilon$, but now I do not know how to proceed.","I am trying to prove the function $f(x) = (1-x)/(1-\sqrt{x})$ has a limit as $x$ approaches 1 using an epsilon definition. I've gotten as far as finding the limit is $2$ by factoring the numerator, as well as setting up the proof, which is as follows: Let $\epsilon>0$ be given. Then there exists $\delta>0$ such that $|(1-x)/(1-\sqrt{x}) - 2| < \epsilon$ if $0 < x-1 < \delta$, $x$ element of the domain, and $1$ is an accumulation point. Now, simplifying the absolute value gives $\left|\frac{(1-x) - 2(1-\sqrt{x})}{1-\sqrt{x}}\right| < \epsilon$, but now I do not know how to proceed.",,"['real-analysis', 'limits']"
2,How to prove convergence of the Arithmetic-Geometric Mean sequence? [duplicate],How to prove convergence of the Arithmetic-Geometric Mean sequence? [duplicate],,"This question already has answers here : Question about arithmetic–geometric mean [duplicate] (2 answers) Closed 8 years ago . Let $$a_0>b_0>0 $$and consider the infinite sequences $$\{a_n\}, \{b_n\}$$ where $$a_{n+1}=\frac{a_n+b_n}{2}$$ and $${b_{n+1}}={(a_nb_n)}^{1/2} $$ for $n\geq0$. Prove that the infinite sequences $\{a_n\}$, $\{b_n\}$ are convergent. Can anyone guide me for this question? Thanks!","This question already has answers here : Question about arithmetic–geometric mean [duplicate] (2 answers) Closed 8 years ago . Let $$a_0>b_0>0 $$and consider the infinite sequences $$\{a_n\}, \{b_n\}$$ where $$a_{n+1}=\frac{a_n+b_n}{2}$$ and $${b_{n+1}}={(a_nb_n)}^{1/2} $$ for $n\geq0$. Prove that the infinite sequences $\{a_n\}$, $\{b_n\}$ are convergent. Can anyone guide me for this question? Thanks!",,"['sequences-and-series', 'limits', 'convergence-divergence', 'means']"
3,Prove $\lim\limits_{n \to \infty} \frac{n^2+1}{5n^2+n+1}=\frac{1}{5}$ directly from the definition of limit.,Prove  directly from the definition of limit.,\lim\limits_{n \to \infty} \frac{n^2+1}{5n^2+n+1}=\frac{1}{5},"Prove $\lim\limits_{n \to \infty} \frac{n^2+1}{5n^2+n+1}=\frac{1}{5}$ directly   from the definition of limit. So far ive done this: Proof: It must be shown that for any $\epsilon>0$, there exists a positive integer $N$ such that for $n\in \mathbb{N}$ with $n\ge N$,  one has $$\left|\frac{n^2+1}{5n^2+n+1}-\frac{1}{5}\right| <\epsilon \text{ (or equivalently,}\frac{1}{5}-\epsilon \lt \frac{n^2+1}{5n^2+n+1} \lt \frac{1}{5} + \epsilon \text{).}$$ $$\begin{align} \left|\frac{n^2+1}{5n^2+n+1}-\frac{1}{5}\right| & = \left|\frac{(5(n^2+1))-(1(5n^2+n+1))}{5(5n^2+n+1)}\right| \\  & = \left|\frac{5n^2+5-5n^2-n-1}{25n^2+5n+5}\right| \\   & = \left|\frac{4-n}{25n^2+5n+5}\right| \\ \end{align}$$ How do i go about choosing the specific terms or factors of $n$ to compare against the numerator and denominator in this particular problem and i guess these type of problems in general? Like in the answer: https://math.stackexchange.com/a/553466/227134 How do you know to setup the inequality $4n+7\le5n$ for the numerator and so forth? I cant quite figure out what to put on the right side of $4-n\le $  ?","Prove $\lim\limits_{n \to \infty} \frac{n^2+1}{5n^2+n+1}=\frac{1}{5}$ directly   from the definition of limit. So far ive done this: Proof: It must be shown that for any $\epsilon>0$, there exists a positive integer $N$ such that for $n\in \mathbb{N}$ with $n\ge N$,  one has $$\left|\frac{n^2+1}{5n^2+n+1}-\frac{1}{5}\right| <\epsilon \text{ (or equivalently,}\frac{1}{5}-\epsilon \lt \frac{n^2+1}{5n^2+n+1} \lt \frac{1}{5} + \epsilon \text{).}$$ $$\begin{align} \left|\frac{n^2+1}{5n^2+n+1}-\frac{1}{5}\right| & = \left|\frac{(5(n^2+1))-(1(5n^2+n+1))}{5(5n^2+n+1)}\right| \\  & = \left|\frac{5n^2+5-5n^2-n-1}{25n^2+5n+5}\right| \\   & = \left|\frac{4-n}{25n^2+5n+5}\right| \\ \end{align}$$ How do i go about choosing the specific terms or factors of $n$ to compare against the numerator and denominator in this particular problem and i guess these type of problems in general? Like in the answer: https://math.stackexchange.com/a/553466/227134 How do you know to setup the inequality $4n+7\le5n$ for the numerator and so forth? I cant quite figure out what to put on the right side of $4-n\le $  ?",,"['real-analysis', 'limits']"
4,Limit of the function $(\cos{\pi x})^{2n}$ as $n\to\infty$,Limit of the function  as,(\cos{\pi x})^{2n} n\to\infty,"I just came across this question. Kindly point out where I am wrong. Finding $\lim (\cos{\pi x})^{2n}$ What I did :  $((\cos{\pi x})^2)^n$, then made $3$ categories, $x\lt 0$, $x\gt 0$ and $x=0$. but due to the ""square"" it reduces to only $2$. we know $\cos{\pi n} = (-1)^n$ so squaring gives the value as $1$. thus $\lim (\cos{\pi x})^2n = (1)^n$ . Taking limit to infinity , thus equals $1$. Is this correct ?","I just came across this question. Kindly point out where I am wrong. Finding $\lim (\cos{\pi x})^{2n}$ What I did :  $((\cos{\pi x})^2)^n$, then made $3$ categories, $x\lt 0$, $x\gt 0$ and $x=0$. but due to the ""square"" it reduces to only $2$. we know $\cos{\pi n} = (-1)^n$ so squaring gives the value as $1$. thus $\lim (\cos{\pi x})^2n = (1)^n$ . Taking limit to infinity , thus equals $1$. Is this correct ?",,"['real-analysis', 'limits']"
5,Limit proof; can $\varepsilon=\delta$?,Limit proof; can ?,\varepsilon=\delta,"I am wondering about a beginner proof I am trying to do, and also a more general question. I am working on the question, $$\lim_{(x,y) \to (0,0)}\frac{x^2y^2}{x^2+y^2}$$ So I looked at a few cases and it seemed to me that it might be reasonable that the limit is 0, so I tried to do an epsilon delta proof. that is, I am under the impression that if I can show that for any $\epsilon >0$ there exists a $\delta > 0$, such that if $0< \sqrt{x^2+y^2}< \delta $ then  $| \frac{x^2y^2}{x^2+y^2}-0|< \epsilon$ ( also I can remove the absolute value signs because we have only squares). So then I thought I could do something like, $$x^2 \le x^2+y^2 \rightarrow \frac{x^2}{x^2+y^2} \le 1 \rightarrow \frac{x^2y^2}{x^2+y^2} \le y^2 \le x^2+y^2$$ ( using that $y^2 \le x^2+y^2$ as well) But now I am not sure how to proceed, should I say let $\delta=\epsilon$ ? Is that even valid to say. Also, I have not looked at solutions or anything so I am not sure if my work is correct either, so i appreciate any comments/answers! Thankyou.","I am wondering about a beginner proof I am trying to do, and also a more general question. I am working on the question, $$\lim_{(x,y) \to (0,0)}\frac{x^2y^2}{x^2+y^2}$$ So I looked at a few cases and it seemed to me that it might be reasonable that the limit is 0, so I tried to do an epsilon delta proof. that is, I am under the impression that if I can show that for any $\epsilon >0$ there exists a $\delta > 0$, such that if $0< \sqrt{x^2+y^2}< \delta $ then  $| \frac{x^2y^2}{x^2+y^2}-0|< \epsilon$ ( also I can remove the absolute value signs because we have only squares). So then I thought I could do something like, $$x^2 \le x^2+y^2 \rightarrow \frac{x^2}{x^2+y^2} \le 1 \rightarrow \frac{x^2y^2}{x^2+y^2} \le y^2 \le x^2+y^2$$ ( using that $y^2 \le x^2+y^2$ as well) But now I am not sure how to proceed, should I say let $\delta=\epsilon$ ? Is that even valid to say. Also, I have not looked at solutions or anything so I am not sure if my work is correct either, so i appreciate any comments/answers! Thankyou.",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
6,Evaluate the following limit without L'Hopital,Evaluate the following limit without L'Hopital,,"I tried to evaluate the following limits but I just couldn't succeed, basically I can't use L'Hopital to solve this... for the second limit I tried to transform it into  $e^{\frac{2n\sqrt{n+3}ln(\frac{3n-1}{2n+3})}{(n+4)\sqrt{n+1}}}$ but still with no success... $$\lim_{n \to \infty } \frac{2n^2-3}{-n^2+7}\frac{3^n-2^{n-1}}{3^{n+2}+2^n}$$ $$\lim_{n \to \infty } \frac{3n-1}{2n+3}^{\frac{2n\sqrt{n+3}}{(n+4)\sqrt{n+1}}}$$ Any suggestions/help? :) Thanks","I tried to evaluate the following limits but I just couldn't succeed, basically I can't use L'Hopital to solve this... for the second limit I tried to transform it into  $e^{\frac{2n\sqrt{n+3}ln(\frac{3n-1}{2n+3})}{(n+4)\sqrt{n+1}}}$ but still with no success... $$\lim_{n \to \infty } \frac{2n^2-3}{-n^2+7}\frac{3^n-2^{n-1}}{3^{n+2}+2^n}$$ $$\lim_{n \to \infty } \frac{3n-1}{2n+3}^{\frac{2n\sqrt{n+3}}{(n+4)\sqrt{n+1}}}$$ Any suggestions/help? :) Thanks",,"['calculus', 'limits', 'limits-without-lhopital']"
7,Stuck with understanding transformation step in calculating limit of $n(\sqrt[n]{a}-1)$,Stuck with understanding transformation step in calculating limit of,n(\sqrt[n]{a}-1),"Although this question has already been asked in general  ( How to find $\lim\limits_{n\to\infty} n·(\sqrt[n]{a}-1)$? ) , my question is different, because I am stuck with a specific transformation step: $$\lim\limits_{n\rightarrow \infty} n(\sqrt[n]{a}-1) $$ I've got the solution to it, but I have problems understanding this step: $$\lim\limits_{n\rightarrow \infty} n(\sqrt[n]{a}-1) = \frac{\exp\left(\frac{\log a}{n}\right)-1}{\frac{1}{n}}\\$$ I know that I can rewrite $\sqrt[n]{a}$ as $e^{(1/n)*\log(a)}= \exp\left(\frac{\log a}{n}\right)$ with $\log = \ln$. Question: How do I get the $\frac{1}{n}$ in the denominator. And the final step is somehow taking the $\log a$, on both the numerator and denominator. $$ = \log a \frac{\exp\left(\frac{\log a}{n}\right)-1}{\frac{\log a}{n}}=\log a\\$$","Although this question has already been asked in general  ( How to find $\lim\limits_{n\to\infty} n·(\sqrt[n]{a}-1)$? ) , my question is different, because I am stuck with a specific transformation step: $$\lim\limits_{n\rightarrow \infty} n(\sqrt[n]{a}-1) $$ I've got the solution to it, but I have problems understanding this step: $$\lim\limits_{n\rightarrow \infty} n(\sqrt[n]{a}-1) = \frac{\exp\left(\frac{\log a}{n}\right)-1}{\frac{1}{n}}\\$$ I know that I can rewrite $\sqrt[n]{a}$ as $e^{(1/n)*\log(a)}= \exp\left(\frac{\log a}{n}\right)$ with $\log = \ln$. Question: How do I get the $\frac{1}{n}$ in the denominator. And the final step is somehow taking the $\log a$, on both the numerator and denominator. $$ = \log a \frac{\exp\left(\frac{\log a}{n}\right)-1}{\frac{\log a}{n}}=\log a\\$$",,"['algebra-precalculus', 'limits', 'convergence-divergence', 'logarithms']"
8,Convergence of $\sum_{n=1}^{\infty}\frac{\sqrt n}{2^n}$,Convergence of,\sum_{n=1}^{\infty}\frac{\sqrt n}{2^n},"In order to find if the series is convergente or divergent: $$\sum_{n=1}^{\infty}\frac{\sqrt n}{2^n}$$ I did the ratio test: $$\lim_{n\to \infty}\left|\frac{a_{n+1}}{a_n}\right|$$ I did: $$\lim_{n\to \infty}\left|\frac{\frac{\sqrt{n+1}}{2^{n+1}}}{\frac{\sqrt{n}}{2^n}}\right|= 2 > 1 $$therefore by the ratio test it should diverge, but this series converge as wolfram alpha says","In order to find if the series is convergente or divergent: $$\sum_{n=1}^{\infty}\frac{\sqrt n}{2^n}$$ I did the ratio test: $$\lim_{n\to \infty}\left|\frac{a_{n+1}}{a_n}\right|$$ I did: $$\lim_{n\to \infty}\left|\frac{\frac{\sqrt{n+1}}{2^{n+1}}}{\frac{\sqrt{n}}{2^n}}\right|= 2 > 1 $$therefore by the ratio test it should diverge, but this series converge as wolfram alpha says",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
9,"Prove if $a_n$ converges to $0$ and $b_n$ is bounded, then $a_n b_n$ converges to $0$","Prove if  converges to  and  is bounded, then  converges to",a_n 0 b_n a_n b_n 0,"We have these two hypothesis: $$\forall\epsilon_1>0, \exists n_0 | n>n_o \implies |a_n|<\epsilon_1$$ $$|b_n|<M$$ where $M$ is the sequence bound. Therefore, I've used hypothesis 2 to multiply both sides in the hypothesis 1 so we have: $$\forall\epsilon_1>0, \exists n_0 | n>n_o \implies |a_n b_n|<\epsilon_1M$$ then if we choose $\epsilon = \epsilon_1 M$ we have: $$\forall\epsilon>0, \exists n_0 | n>n_o \implies |a_n b_n|<\epsilon$$ Am I right?","We have these two hypothesis: $$\forall\epsilon_1>0, \exists n_0 | n>n_o \implies |a_n|<\epsilon_1$$ $$|b_n|<M$$ where $M$ is the sequence bound. Therefore, I've used hypothesis 2 to multiply both sides in the hypothesis 1 so we have: $$\forall\epsilon_1>0, \exists n_0 | n>n_o \implies |a_n b_n|<\epsilon_1M$$ then if we choose $\epsilon = \epsilon_1 M$ we have: $$\forall\epsilon>0, \exists n_0 | n>n_o \implies |a_n b_n|<\epsilon$$ Am I right?",,"['real-analysis', 'sequences-and-series', 'limits']"
10,Contractive condition implies the sequence is Cauchy,Contractive condition implies the sequence is Cauchy,,"$a_n$ is a sequence that satisfies the following contractive condition; $|a_{n+2}-a_{n+1}| \le c|a_{n+1}-a_n|, n\ge 1 $ for some $ 0 \lt c \lt 1.$ Show the sequence $a_n$ is a Cauchy sequence and converges. Consider the sequence $x_n$ given by $x_1=2$ and $x_{n+1} = 2+ \frac{1}{x_n}$ Show that $x_n$ is bounded below by 2 Show that $x_n$ satisfies the contractive condition in the first part and converges Find lim $x_n$ Not looking for the answers. Just strong hints to guide me to the answer. The first part of the problem itself, makes sense. I'm unsure on the steps needed to prove this though. The second part for (1), would I want to use induction to show that it keeps getting closer to 2 but never reaches it. For part (2), I may be way off, but as simple as plugging into the contractive condition above? I am confused on part 3. A lot here, I appreciate any sort of help. Thanks","$a_n$ is a sequence that satisfies the following contractive condition; $|a_{n+2}-a_{n+1}| \le c|a_{n+1}-a_n|, n\ge 1 $ for some $ 0 \lt c \lt 1.$ Show the sequence $a_n$ is a Cauchy sequence and converges. Consider the sequence $x_n$ given by $x_1=2$ and $x_{n+1} = 2+ \frac{1}{x_n}$ Show that $x_n$ is bounded below by 2 Show that $x_n$ satisfies the contractive condition in the first part and converges Find lim $x_n$ Not looking for the answers. Just strong hints to guide me to the answer. The first part of the problem itself, makes sense. I'm unsure on the steps needed to prove this though. The second part for (1), would I want to use induction to show that it keeps getting closer to 2 but never reaches it. For part (2), I may be way off, but as simple as plugging into the contractive condition above? I am confused on part 3. A lot here, I appreciate any sort of help. Thanks",,"['real-analysis', 'sequences-and-series', 'limits', 'cauchy-sequences']"
11,Limit of a sum versus individual $\lim \limits_{n \to \infty}\sum_{i=1}^n \frac{n}{n^2+i}$,Limit of a sum versus individual,\lim \limits_{n \to \infty}\sum_{i=1}^n \frac{n}{n^2+i},"Consider the series $$\lim \limits_{n \to \infty}\sum_{i=1}^n \frac{n}{n^2+i}$$ Considering individual terms, we get $$\lim \limits_{n \to \infty}\frac{n}{n^2+i}=\lim \limits_{n \to \infty}\frac{1}{n+\frac{i}{n}}=0.$$ Hence the sum would be $0$ . However, considering an alternative approach, we observe that $\frac{n}{n^2+n}<\frac{n}{n^2+i}<\frac{n}{n^2+1}$ . Therefore $$\sum_{i=1}^n \frac{n}{n^2+n}<\sum_{i=1}^n \frac{n}{n^2+i}<\sum_{i=1}^n \frac{n}{n^2+1} \implies \frac{n^2}{n^2+n}<\sum_{i=1}^n \frac{n}{n^2+i}<\frac{n^2}{n^2+1}$$ Taking limit $\lim \limits_{n \to \infty}$ on the three expressions, we find that $\lim \limits_{n \to \infty}\frac{n^2}{n^2+n}=\lim \limits_{n \to \infty}\frac{n^2}{n^2+1}=1.$ Therefore by the sandwich theorem, $\lim \limits_{n \to \infty}\sum_{i=1}^n \frac{n}{n^2+i}=1$ Since there exists a unique limit, there must be only one solution. Which approach is the correct one? There was one suggested reason that each of the individual limits that was calculated in the 1st approach had to be summed up to infinite terms. Since each of the limits were infintesimally small, we cannot predict the way they will behave when summed to infinity and therefore the approach is incorrect. However, my counter was that the limits are exact and are not infinitesimals. Therefore 0 can be summed up infinitely many times and the answer will still be zero. Is this reasoning correct/wrong? What must be the answer to the problem?","Consider the series Considering individual terms, we get Hence the sum would be . However, considering an alternative approach, we observe that . Therefore Taking limit on the three expressions, we find that Therefore by the sandwich theorem, Since there exists a unique limit, there must be only one solution. Which approach is the correct one? There was one suggested reason that each of the individual limits that was calculated in the 1st approach had to be summed up to infinite terms. Since each of the limits were infintesimally small, we cannot predict the way they will behave when summed to infinity and therefore the approach is incorrect. However, my counter was that the limits are exact and are not infinitesimals. Therefore 0 can be summed up infinitely many times and the answer will still be zero. Is this reasoning correct/wrong? What must be the answer to the problem?",\lim \limits_{n \to \infty}\sum_{i=1}^n \frac{n}{n^2+i} \lim \limits_{n \to \infty}\frac{n}{n^2+i}=\lim \limits_{n \to \infty}\frac{1}{n+\frac{i}{n}}=0. 0 \frac{n}{n^2+n}<\frac{n}{n^2+i}<\frac{n}{n^2+1} \sum_{i=1}^n \frac{n}{n^2+n}<\sum_{i=1}^n \frac{n}{n^2+i}<\sum_{i=1}^n \frac{n}{n^2+1} \implies \frac{n^2}{n^2+n}<\sum_{i=1}^n \frac{n}{n^2+i}<\frac{n^2}{n^2+1} \lim \limits_{n \to \infty} \lim \limits_{n \to \infty}\frac{n^2}{n^2+n}=\lim \limits_{n \to \infty}\frac{n^2}{n^2+1}=1. \lim \limits_{n \to \infty}\sum_{i=1}^n \frac{n}{n^2+i}=1,"['limits', 'limits-without-lhopital']"
12,Does $\lim\limits_{x \to 0}\frac{f(x)}x=0$ imply that $\lim\limits_{x \to 0}f(x)=0$?,Does  imply that ?,\lim\limits_{x \to 0}\frac{f(x)}x=0 \lim\limits_{x \to 0}f(x)=0,"If I have the limit $\lim\limits_{x \to 0}\frac{f(x)}x=0$, does it mean that  $\lim\limits_{x \to 0}f(x)=0$? I tried to use the definition of limit but unsuccessfully.","If I have the limit $\lim\limits_{x \to 0}\frac{f(x)}x=0$, does it mean that  $\lim\limits_{x \to 0}f(x)=0$? I tried to use the definition of limit but unsuccessfully.",,"['calculus', 'limits']"
13,The pedantic function $\frac{y \cdot \sin(x^5y^3+x^3)}{(x^4y^8+x^6+3y^2)\cos(x^2y)^2}$,The pedantic function,\frac{y \cdot \sin(x^5y^3+x^3)}{(x^4y^8+x^6+3y^2)\cos(x^2y)^2},"I was shown the following ""pedantic"" function.  $$     f(x,y) := \frac{y \cdot \sin(x^5y^3+x^3)}{(x^4y^8+x^6+3y^2)\cos(x^2y)^2} $$ The question is what happens as the function approaches origo. So in order for the limit to exists we need to check that the function is approaching the same limit for all directions. So along the coordinate axes we have $$   \lim_{x\to 0} f(x,0) = \lim_{y\to 0} f(y,0) =0 $$ Hmmm. Okay, lets test all straight lines through origo $y = k x$, some calculations again give $$ \lim_{x \to 0} f(x,k x) = 0 $$  So it seems the limit exists, further studies show that $$ \lim_{x \to 0} f(x,k x^n) = 0 $$  For all integers $n$, except $3$. Is this true? Conjecture Let $f(x,y)$ be defined as above then   $$     \lim_{x\to 0} f(x,k x^n) = 0 $$   for all $n \in\mathbb{N}$ except $n=3$. Here we have   $$    \lim_{x\to 0} f(x, k x^3) = \frac{k}{3k^2+1} $$   where $k \in \mathbb{R}$. Is the conjecture true? I tested $n$ up to 2000. Also are there other lines approaching origo that leads to a different answer than zero, except $y = x^3$? Can this be proven rigorously?","I was shown the following ""pedantic"" function.  $$     f(x,y) := \frac{y \cdot \sin(x^5y^3+x^3)}{(x^4y^8+x^6+3y^2)\cos(x^2y)^2} $$ The question is what happens as the function approaches origo. So in order for the limit to exists we need to check that the function is approaching the same limit for all directions. So along the coordinate axes we have $$   \lim_{x\to 0} f(x,0) = \lim_{y\to 0} f(y,0) =0 $$ Hmmm. Okay, lets test all straight lines through origo $y = k x$, some calculations again give $$ \lim_{x \to 0} f(x,k x) = 0 $$  So it seems the limit exists, further studies show that $$ \lim_{x \to 0} f(x,k x^n) = 0 $$  For all integers $n$, except $3$. Is this true? Conjecture Let $f(x,y)$ be defined as above then   $$     \lim_{x\to 0} f(x,k x^n) = 0 $$   for all $n \in\mathbb{N}$ except $n=3$. Here we have   $$    \lim_{x\to 0} f(x, k x^3) = \frac{k}{3k^2+1} $$   where $k \in \mathbb{R}$. Is the conjecture true? I tested $n$ up to 2000. Also are there other lines approaching origo that leads to a different answer than zero, except $y = x^3$? Can this be proven rigorously?",,"['real-analysis', 'limits', 'multivariable-calculus']"
14,"Determining a limit of parametrized, recursively defined sequence $a_{n+1}=1+\frac{(a_n-1)^2}{17}$","Determining a limit of parametrized, recursively defined sequence",a_{n+1}=1+\frac{(a_n-1)^2}{17},"For every $c\in [0;2]$ determine whether the sequence $\{a_n\}_{n\geq 1}$ which is defined as follows: $a_1=c$, $a_{n+1}=1+\frac{(a_n-1)^2}{17}$ for $n\geq 1$ is monotonic for sufficiently large $n$, and determine whether its limit exits and if it exists, give its value. I have no idea what to do with this problem. I was able to see that $a_{n+1}-a_n$ is a quadratic function and I also found ot that limit, if exists, is equal to either $1$ or $18$ (that's beacause if $a_n$ id convergent to $g$ then every subsequence is also convergent to $g$). So how to determine the limit for every $c\in [0;2]$?","For every $c\in [0;2]$ determine whether the sequence $\{a_n\}_{n\geq 1}$ which is defined as follows: $a_1=c$, $a_{n+1}=1+\frac{(a_n-1)^2}{17}$ for $n\geq 1$ is monotonic for sufficiently large $n$, and determine whether its limit exits and if it exists, give its value. I have no idea what to do with this problem. I was able to see that $a_{n+1}-a_n$ is a quadratic function and I also found ot that limit, if exists, is equal to either $1$ or $18$ (that's beacause if $a_n$ id convergent to $g$ then every subsequence is also convergent to $g$). So how to determine the limit for every $c\in [0;2]$?",,"['sequences-and-series', 'limits']"
15,Find $\lim \limits_{x \to \pi}\frac{\int_0^x\cos^2(t)dt}{x-\pi}\;$,Find,\lim \limits_{x \to \pi}\frac{\int_0^x\cos^2(t)dt}{x-\pi}\;,"$$\lim \limits_{x \to \pi}\frac{\int_0^x\cos^2(t)\,dt}{x-\pi}$$ I don't understand why the limit is not $\infty$ How is the limit: $1$?","$$\lim \limits_{x \to \pi}\frac{\int_0^x\cos^2(t)\,dt}{x-\pi}$$ I don't understand why the limit is not $\infty$ How is the limit: $1$?",,"['calculus', 'integration', 'limits']"
16,Evaluating $\lim_{n \to \infty} \sum\limits_{k=1}^n \frac{1}{k 2^k} $,Evaluating,\lim_{n \to \infty} \sum\limits_{k=1}^n \frac{1}{k 2^k} ,"Question : How to compute  $$ \lim_{n \to \infty} \sum\limits_{k=1}^n \frac{1}{k 2^k}? $$ Here is what I have tried so far: Define $s_n=\sum\limits_{k=1}^n \frac{1}{k 2^k}$ for every index $n$, $\{{s_n}\}$ is monotonically increasing, $\{{s_n}\}$ converges because it is monotone and bounded. $$ 0\leq s_n\leq \frac{1}{2} + \left(\frac{1}{2}\right)^2 + \cdots + \left(\frac{1}{2}\right)^n = \frac{\frac{1}{2}-\left(\frac{1}{2}\right)^{n+1}}{1-\frac{1}{2}}\leq \frac{\frac{1}{2}-0}{1-\frac{1}{2}}=1. $$ Bounded monotonically increasing sequence converges to its $\sup$. Thus $\sup \{{s_n}\}\leq 1$. How can I find $\lim_{n \to \infty} \sum\limits_{k=1}^n \frac{1}{k 2^k}$ or equivalently $\sup \{{s_n}\}$? Thank you.","Question : How to compute  $$ \lim_{n \to \infty} \sum\limits_{k=1}^n \frac{1}{k 2^k}? $$ Here is what I have tried so far: Define $s_n=\sum\limits_{k=1}^n \frac{1}{k 2^k}$ for every index $n$, $\{{s_n}\}$ is monotonically increasing, $\{{s_n}\}$ converges because it is monotone and bounded. $$ 0\leq s_n\leq \frac{1}{2} + \left(\frac{1}{2}\right)^2 + \cdots + \left(\frac{1}{2}\right)^n = \frac{\frac{1}{2}-\left(\frac{1}{2}\right)^{n+1}}{1-\frac{1}{2}}\leq \frac{\frac{1}{2}-0}{1-\frac{1}{2}}=1. $$ Bounded monotonically increasing sequence converges to its $\sup$. Thus $\sup \{{s_n}\}\leq 1$. How can I find $\lim_{n \to \infty} \sum\limits_{k=1}^n \frac{1}{k 2^k}$ or equivalently $\sup \{{s_n}\}$? Thank you.",,"['real-analysis', 'sequences-and-series']"
17,Limit without L'Hopital's rule: $\lim_{x\to0} \frac{1-\cos^3 x}{x\sin2x}$,Limit without L'Hopital's rule:,\lim_{x\to0} \frac{1-\cos^3 x}{x\sin2x},How can I solve the following problem without the use of the L'Hopitals's rule? $$\lim_{x\to0} \frac{1-\cos^3(x)}{x\sin{(2x)}}$$,How can I solve the following problem without the use of the L'Hopitals's rule? $$\lim_{x\to0} \frac{1-\cos^3(x)}{x\sin{(2x)}}$$,,"['limits', 'limits-without-lhopital']"
18,Limit of a continuous function with a parameter,Limit of a continuous function with a parameter,,"Let $f(x,\alpha)$ be continuous function on $S=(0,1]\times[0,1]$. Suppose that for every segment $[\alpha,\alpha+\Delta\alpha]\in[0,1]$ there exists $x_0=x_0(\Delta \alpha)$ s.t. for $0<x<x_0$ on a vertical segment $I_{[\alpha,\alpha+\Delta\alpha]}(x)=\{(x,\beta),\alpha\le \beta\le \alpha+\Delta\alpha\}\ \ \ $ the estimate  $$ \max_{\alpha\le \beta\le \alpha+\Delta\alpha}f(x,\beta)\ge g(x)\qquad\qquad{(1)} $$ holds, where $\lim\limits_{x\to0+}g(x)=+\infty\,$. Does it follow that $\lim\limits_{x\to0+}f(x,\alpha)=+\infty\ $ for all   $\alpha$? Or may be a.e. on $[0,1]$? It's straightforward to get from $(1)$ that for every $\alpha\in[0,1]$ there exists a sequence of points $(x_k,\alpha_k)\in S$ s.t. $(x_k,\alpha_k)\to(0,\alpha)$ and $\lim\limits_{k\to+\infty}f(x_k,\alpha_k)=+\infty$. But the sequence can approach $(0,\alpha)$  from the tangential direction and the desired result does not follow immediately.","Let $f(x,\alpha)$ be continuous function on $S=(0,1]\times[0,1]$. Suppose that for every segment $[\alpha,\alpha+\Delta\alpha]\in[0,1]$ there exists $x_0=x_0(\Delta \alpha)$ s.t. for $0<x<x_0$ on a vertical segment $I_{[\alpha,\alpha+\Delta\alpha]}(x)=\{(x,\beta),\alpha\le \beta\le \alpha+\Delta\alpha\}\ \ \ $ the estimate  $$ \max_{\alpha\le \beta\le \alpha+\Delta\alpha}f(x,\beta)\ge g(x)\qquad\qquad{(1)} $$ holds, where $\lim\limits_{x\to0+}g(x)=+\infty\,$. Does it follow that $\lim\limits_{x\to0+}f(x,\alpha)=+\infty\ $ for all   $\alpha$? Or may be a.e. on $[0,1]$? It's straightforward to get from $(1)$ that for every $\alpha\in[0,1]$ there exists a sequence of points $(x_k,\alpha_k)\in S$ s.t. $(x_k,\alpha_k)\to(0,\alpha)$ and $\lim\limits_{k\to+\infty}f(x_k,\alpha_k)=+\infty$. But the sequence can approach $(0,\alpha)$  from the tangential direction and the desired result does not follow immediately.",,"['real-analysis', 'limits', 'continuity']"
19,Evaluate $\lim_{x\rightarrow 0^{+}}(3^{x}-2^{x})^{{1}/{x}} $,Evaluate,\lim_{x\rightarrow 0^{+}}(3^{x}-2^{x})^{{1}/{x}} ,Evaluate  $$\lim_{x\rightarrow 0^{+}}(3^{x}-2^{x})^{{1}/{x}}  $$ I tried to use $\ln$ and $e$ with no success.,Evaluate  $$\lim_{x\rightarrow 0^{+}}(3^{x}-2^{x})^{{1}/{x}}  $$ I tried to use $\ln$ and $e$ with no success.,,"['calculus', 'limits', 'limits-without-lhopital']"
20,Convergence of functions with different domain,Convergence of functions with different domain,,"Question: Is there a concept of convergence for functions $f_n: D_n \rightarrow X$ with different domains to a function $f: D \rightarrow X$? I know concepts like uniform convergence or almost everywhere convergence but they require that all functions $f_n$ and their limit $f$ share the same domain... Example: Take $D_n = \{ \tfrac kn : 0 \le k \le n, k \in \mathbb N \}$. Let $f_n$ be defined via $$f_n : D_n \rightarrow \mathbb R: \tfrac kn \rightarrow \tfrac kn + \tfrac 1n$$ The limit I have in mind would be $f: [0,1]\rightarrow \mathbb R: x \mapsto x$. I guess, if one requires from the limit to be continuous, the above limit $f$ is unique. But I'm not sure, because I do not know which concept of limit I can apply...","Question: Is there a concept of convergence for functions $f_n: D_n \rightarrow X$ with different domains to a function $f: D \rightarrow X$? I know concepts like uniform convergence or almost everywhere convergence but they require that all functions $f_n$ and their limit $f$ share the same domain... Example: Take $D_n = \{ \tfrac kn : 0 \le k \le n, k \in \mathbb N \}$. Let $f_n$ be defined via $$f_n : D_n \rightarrow \mathbb R: \tfrac kn \rightarrow \tfrac kn + \tfrac 1n$$ The limit I have in mind would be $f: [0,1]\rightarrow \mathbb R: x \mapsto x$. I guess, if one requires from the limit to be continuous, the above limit $f$ is unique. But I'm not sure, because I do not know which concept of limit I can apply...",,"['general-topology', 'functional-analysis', 'limits', 'functions', 'convergence-divergence']"
21,Show that $ {x}_{k+1} - {x}_{k} \rightarrow L \Rightarrow \frac {{x}_{k}} { k} \rightarrow L $,Show that, {x}_{k+1} - {x}_{k} \rightarrow L \Rightarrow \frac {{x}_{k}} { k} \rightarrow L ,"I have to show that the implication in the title is true as $ k \rightarrow \infty $ and where $ \left< {x}_{k} \right> $ is a sequence This question is from Wade's Introduction to Analysis and I think that it is something to do with Cauchy sequences because right before the question it says ""[Cauchy]"". However, the book doesn't give any hints or anything on how to prove it. $ {x}_{k+1}- {x}_{k} -L \rightarrow 0$ as $ k \rightarrow \infty $ is a Cauchy sequence $ \Rightarrow \frac {{x}_{k+1} - {x}_{k} - L} {k} \rightarrow 0 $ as $ k \rightarrow \infty $ is a Cauchy sequence as well. Another way I was thinking about it is $ lim _{k \rightarrow \infty} {{x}_{k}} = lim_{k \rightarrow \infty} ( {x}_{k+1} - L) $ so it would be sufficient to show that $ lim _{k \rightarrow \infty} \frac {{x}_{k+1} - L}  {k} = L$ However, I'm not sure how to follow through on either in order to show the implication. Any help is greatly appreciated. Thanks in advance.","I have to show that the implication in the title is true as $ k \rightarrow \infty $ and where $ \left< {x}_{k} \right> $ is a sequence This question is from Wade's Introduction to Analysis and I think that it is something to do with Cauchy sequences because right before the question it says ""[Cauchy]"". However, the book doesn't give any hints or anything on how to prove it. $ {x}_{k+1}- {x}_{k} -L \rightarrow 0$ as $ k \rightarrow \infty $ is a Cauchy sequence $ \Rightarrow \frac {{x}_{k+1} - {x}_{k} - L} {k} \rightarrow 0 $ as $ k \rightarrow \infty $ is a Cauchy sequence as well. Another way I was thinking about it is $ lim _{k \rightarrow \infty} {{x}_{k}} = lim_{k \rightarrow \infty} ( {x}_{k+1} - L) $ so it would be sufficient to show that $ lim _{k \rightarrow \infty} \frac {{x}_{k+1} - L}  {k} = L$ However, I'm not sure how to follow through on either in order to show the implication. Any help is greatly appreciated. Thanks in advance.",,"['sequences-and-series', 'analysis', 'limits']"
22,Prove difference of summations $=\frac{e^2}{2}$,Prove difference of summations,=\frac{e^2}{2},How do I prove that \begin{align} \lim_{n\rightarrow\infty}\left\{\sum\limits_{k=1}^{n}\sum\limits_{i=1}^{\infty}\frac{k\left(2k/n\right)^i}{n\Gamma\left(i+1\right)}-\sum\limits_{k=1}^{n}\sum\limits_{j=1}^{\infty}\frac{k\left(\left(2k-1\right)/n\right)^j}{n\Gamma\left(j+1\right)}\right\}=\frac{1+e^2}{4}? \end{align} I have no idea how to attack this problem. I have little experience or know-how with double summations.,How do I prove that \begin{align} \lim_{n\rightarrow\infty}\left\{\sum\limits_{k=1}^{n}\sum\limits_{i=1}^{\infty}\frac{k\left(2k/n\right)^i}{n\Gamma\left(i+1\right)}-\sum\limits_{k=1}^{n}\sum\limits_{j=1}^{\infty}\frac{k\left(\left(2k-1\right)/n\right)^j}{n\Gamma\left(j+1\right)}\right\}=\frac{1+e^2}{4}? \end{align} I have no idea how to attack this problem. I have little experience or know-how with double summations.,,"['limits', 'summation']"
23,Show that $\lim_{n\to ∞} |a_n| = |a|$ if $a_n\to a$,Show that  if,\lim_{n\to ∞} |a_n| = |a| a_n\to a,"Let $(a_n)$ be a convergent sequence with $\lim\limits_{n\to \infty} a_n = a$.    Show that  $$\lim_{n\to \infty} |a_n| = |a|$$   Then state and disprove the converse statement. In order to prove that I would use the following inequality $||a|-|b|| \le |a-b|$. Let $\epsilon>0$ be arbitrary. $(a_n) \rightarrow a$ and thus by definition there exists a $N$ such that $$|a_n-a|< \epsilon \text{ for all } n \geq N$$ Since I want to show that $|a_n| \to |a|$,  I apply the inequality from above and get  $||a_n|-|a|| \le |a_n-a| < \epsilon$ for all $n≥N$ following $||a_n|-|a||< \epsilon$ for all $n\ge N$. As this hold for any $\epsilon$ by definition $|a|$ is a limit of $|a_n|$. My first question would be if this proof is right. My second question is concerning the converse statement. I don't quite get what the converse statement here is. Normally if $P\Rightarrow Q$ the converse is $Q\Rightarrow P$ so is the converse $$\lim_{n\to \infty} |a_n| = |a| \Rightarrow \lim_{n\to ∞} a_n = a\quad?$$","Let $(a_n)$ be a convergent sequence with $\lim\limits_{n\to \infty} a_n = a$.    Show that  $$\lim_{n\to \infty} |a_n| = |a|$$   Then state and disprove the converse statement. In order to prove that I would use the following inequality $||a|-|b|| \le |a-b|$. Let $\epsilon>0$ be arbitrary. $(a_n) \rightarrow a$ and thus by definition there exists a $N$ such that $$|a_n-a|< \epsilon \text{ for all } n \geq N$$ Since I want to show that $|a_n| \to |a|$,  I apply the inequality from above and get  $||a_n|-|a|| \le |a_n-a| < \epsilon$ for all $n≥N$ following $||a_n|-|a||< \epsilon$ for all $n\ge N$. As this hold for any $\epsilon$ by definition $|a|$ is a limit of $|a_n|$. My first question would be if this proof is right. My second question is concerning the converse statement. I don't quite get what the converse statement here is. Normally if $P\Rightarrow Q$ the converse is $Q\Rightarrow P$ so is the converse $$\lim_{n\to \infty} |a_n| = |a| \Rightarrow \lim_{n\to ∞} a_n = a\quad?$$",,"['sequences-and-series', 'limits', 'convergence-divergence']"
24,Evaluate $\lim_{n\to\infty}\prod_{k=1}^{n}\frac{2k}{2k-1}$,Evaluate,\lim_{n\to\infty}\prod_{k=1}^{n}\frac{2k}{2k-1},"How can I calculate the following limit: $$ \lim_{n\to \infty} \frac{2\cdot 4 \cdots (2n)}{1\cdot 3 \cdot 5 \cdots (2n-1)} $$ without using the root test or the ratio test for convergence? I have tried finding an upper and lower bounds on this expression, but it gives me nothing since I can't find bounds that will be ""close"" enough to one another. I have also tried using the fact that: $2\cdot 4 \cdot...\cdot (2n)=2^n n!$  and $1\cdot 3 \cdot 5 \cdot...\cdot (2n-1) =2^n (n-0.5)!$ but it also gives me nothing . Will someone please help me ? Thanks in advance","How can I calculate the following limit: $$ \lim_{n\to \infty} \frac{2\cdot 4 \cdots (2n)}{1\cdot 3 \cdot 5 \cdots (2n-1)} $$ without using the root test or the ratio test for convergence? I have tried finding an upper and lower bounds on this expression, but it gives me nothing since I can't find bounds that will be ""close"" enough to one another. I have also tried using the fact that: $2\cdot 4 \cdot...\cdot (2n)=2^n n!$  and $1\cdot 3 \cdot 5 \cdot...\cdot (2n-1) =2^n (n-0.5)!$ but it also gives me nothing . Will someone please help me ? Thanks in advance",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'infinite-product']"
25,$\lim_{x \to 0}\frac{x^{3}-\sin^{2}x\tan x}{\tan(\sin x) - \sin (\tan x)}$,,\lim_{x \to 0}\frac{x^{3}-\sin^{2}x\tan x}{\tan(\sin x) - \sin (\tan x)},Can one help finding this limit $$\lim_{x \to 0}\frac{x^{3}-\sin^{2}x\tan x}{\tan(\sin x) - \sin (\tan x)}$$ L'Hospital's rule is permited. ( Find lim:$\lim_{x\to0} \frac{\tan(\tan x) - \sin(\sin x)}{\tan x -\sin x}$ ),Can one help finding this limit $$\lim_{x \to 0}\frac{x^{3}-\sin^{2}x\tan x}{\tan(\sin x) - \sin (\tan x)}$$ L'Hospital's rule is permited. ( Find lim:$\lim_{x\to0} \frac{\tan(\tan x) - \sin(\sin x)}{\tan x -\sin x}$ ),,"['calculus', 'limits']"
26,Limit of a sum by integral: $\lim_{n\rightarrow\infty} \sum\limits_{i=1}^{n-1}\frac{i}{n^2}$,Limit of a sum by integral:,\lim_{n\rightarrow\infty} \sum\limits_{i=1}^{n-1}\frac{i}{n^2},"I have to find the following limit by using an integral, but I have no idea what to do. $\lim_{n\rightarrow\infty}(\sum\limits_{i=1}^{n-1}\frac{i}{n^2})$ I know that $\sum\limits_{i=1}^n\int_a^bf_i(x)dx = \int_a^b \sum\limits_{i=1}^\infty f(x)dx$ if $S_n\rightarrow f$ uniformly, but I don't know if that can help me here. Thanks","I have to find the following limit by using an integral, but I have no idea what to do. $\lim_{n\rightarrow\infty}(\sum\limits_{i=1}^{n-1}\frac{i}{n^2})$ I know that $\sum\limits_{i=1}^n\int_a^bf_i(x)dx = \int_a^b \sum\limits_{i=1}^\infty f(x)dx$ if $S_n\rightarrow f$ uniformly, but I don't know if that can help me here. Thanks",,"['real-analysis', 'limits', 'riemann-sum']"
27,Suppose that $(s_n)$ converges to s. Prove that $(s_n^2)$ converges to $s^2$,Suppose that  converges to s. Prove that  converges to,(s_n) (s_n^2) s^2,"I am really struggling with my proofs class, I don't really understand how to prove a statement like this, or what the epsilon is standing for.. Suppose that $(s_n)$ converges to s.  Prove that $(s_n^2)$ converges to $s^2$ directly without using the fact that $lim(s_nt_n)=st$ Suppose that $(s_n)$ converges to s. Then, since $(s_n)$ is convergent, there exists an $M_1$ such that $|S_n|<M_1$, for all $n\in \!\,\mathbb{N} \!\,$. Let $M=M_1+|S|$ (I don't completely understand this step...). Then given $ε>0$, there exists N such that n>N implies that $|s_n-s|<ε/M$. Thus for n>N, we have $|s_n^2-s^2|=|s_n-s|*|s_n+s|<(ε/M)(M)=ε$. Hence, $s_n^2$ converges to $s^2$.","I am really struggling with my proofs class, I don't really understand how to prove a statement like this, or what the epsilon is standing for.. Suppose that $(s_n)$ converges to s.  Prove that $(s_n^2)$ converges to $s^2$ directly without using the fact that $lim(s_nt_n)=st$ Suppose that $(s_n)$ converges to s. Then, since $(s_n)$ is convergent, there exists an $M_1$ such that $|S_n|<M_1$, for all $n\in \!\,\mathbb{N} \!\,$. Let $M=M_1+|S|$ (I don't completely understand this step...). Then given $ε>0$, there exists N such that n>N implies that $|s_n-s|<ε/M$. Thus for n>N, we have $|s_n^2-s^2|=|s_n-s|*|s_n+s|<(ε/M)(M)=ε$. Hence, $s_n^2$ converges to $s^2$.",,"['calculus', 'sequences-and-series', 'limits', 'epsilon-delta']"
28,How to prove $\lim_{x\rightarrow -1^+}\sqrt{x+1}=0$ using $\epsilon-\delta$ definition?,How to prove  using  definition?,\lim_{x\rightarrow -1^+}\sqrt{x+1}=0 \epsilon-\delta,Use the $\epsilon-\delta$ definition to prove: $$\lim_{x\rightarrow -1^+}\sqrt{x+1}=0$$ I don't understand what to do with: $$x\rightarrow -1^+$$,Use the $\epsilon-\delta$ definition to prove: $$\lim_{x\rightarrow -1^+}\sqrt{x+1}=0$$ I don't understand what to do with: $$x\rightarrow -1^+$$,,"['limits', 'epsilon-delta']"
29,Oblique asymptote for: $f(t) = \frac{t^2\arctan t}{t-4}$?,Oblique asymptote for: ?,f(t) = \frac{t^2\arctan t}{t-4},"Say a function $$f(t) = \frac{t^2\arctan t}{t-4}$$ Obviously, this has a vertical asymptote at $t = 4$ . However, the oblique asymptote, if there is one, I can't seem to find. What I do is I put the oblique asymptotes equation to $y = kt + m$ . I can then say that $k = \lim_{t\rightarrow\pm\infty}\frac{f(t)}{t}$ however, this equals infinity as $\lim_{t\rightarrow\pm\infty}t\arctan t = +\infty$ . Can I now safely assume there is no oblique asymptote? That's what I assumed until Wolfram told me otherwise. Answer, as explained below by Hippalectryon The mistake I made in the above text was to divide every term in my function by $t$ , which is obviously incorrect. The correct equation after dividing $f(t)$ with $t$ is: $$\frac{f(t)}{t} = \frac{t^2\arctan t}{t(t-4)}$$ Therefore we get $$\displaystyle k = \lim_{t\rightarrow\pm\infty}\frac{f(t)}{t}=\lim_{t\rightarrow\pm\infty}\frac{t^2‌​\cdot\arctan(t)}{t\cdot(t-4)}=\lim_{t\rightarrow\infty}\arctan(t)=\pi/2$$","Say a function Obviously, this has a vertical asymptote at . However, the oblique asymptote, if there is one, I can't seem to find. What I do is I put the oblique asymptotes equation to . I can then say that however, this equals infinity as . Can I now safely assume there is no oblique asymptote? That's what I assumed until Wolfram told me otherwise. Answer, as explained below by Hippalectryon The mistake I made in the above text was to divide every term in my function by , which is obviously incorrect. The correct equation after dividing with is: Therefore we get",f(t) = \frac{t^2\arctan t}{t-4} t = 4 y = kt + m k = \lim_{t\rightarrow\pm\infty}\frac{f(t)}{t} \lim_{t\rightarrow\pm\infty}t\arctan t = +\infty t f(t) t \frac{f(t)}{t} = \frac{t^2\arctan t}{t(t-4)} \displaystyle k = \lim_{t\rightarrow\pm\infty}\frac{f(t)}{t}=\lim_{t\rightarrow\pm\infty}\frac{t^2‌​\cdot\arctan(t)}{t\cdot(t-4)}=\lim_{t\rightarrow\infty}\arctan(t)=\pi/2,"['calculus', 'limits', 'asymptotics']"
30,"For $f :\mathbb R \to \mathbb R $, there exists an $(a,b)$, such that $f$ is bounded on a sequence with limit $x$, for all $x\in(a,b)$","For , there exists an , such that  is bounded on a sequence with limit , for all","f :\mathbb R \to \mathbb R  (a,b) f x x\in(a,b)","I want to prove the following. Let $f : \mathbb R  \mapsto \mathbb R $. Show that there exists an interval $(a,b) \in \mathbb R $  and $c >0 $: such that for any $x \in (a,b) $ there is a sequence $\{x _n \} $ with $x _n \to x $ and $|f(x _n )| \le c $. (no assumption on $f $ being continuous ) I have no idea how to go about to prove this... Thanks in advance!","I want to prove the following. Let $f : \mathbb R  \mapsto \mathbb R $. Show that there exists an interval $(a,b) \in \mathbb R $  and $c >0 $: such that for any $x \in (a,b) $ there is a sequence $\{x _n \} $ with $x _n \to x $ and $|f(x _n )| \le c $. (no assumption on $f $ being continuous ) I have no idea how to go about to prove this... Thanks in advance!",,"['real-analysis', 'general-topology', 'analysis', 'limits', 'baire-category']"
31,Does this simple sum converge,Does this simple sum converge,,"I'm trying to determine whether the sum $$S=\frac{2}{1}+\frac{2\cdot 5}{1\cdot 5}+\frac{2\cdot 5\cdot 8}{1\cdot 5\cdot 9}+...+\frac{2\cdot 5\cdot 8...(3n-1)}{1\cdot 5\cdot 9...(4n-3)}+...$$ converges or not. Even if it does converge, I am not interested in what it is equal to; I just need to  show it converges or diverges. I'm drawing a blank here, but my gut instinct tells me ""no."" I don't think that $a_n$ (the $n^{th}$ term in the sum) approaches $0$ when $n$ approaches infinity, which is something that must be met for all converging sums, but I don't know how to prove it, as this is a rather difficult limit. Does this sum converge or not? and how would you go about showing your answer?","I'm trying to determine whether the sum $$S=\frac{2}{1}+\frac{2\cdot 5}{1\cdot 5}+\frac{2\cdot 5\cdot 8}{1\cdot 5\cdot 9}+...+\frac{2\cdot 5\cdot 8...(3n-1)}{1\cdot 5\cdot 9...(4n-3)}+...$$ converges or not. Even if it does converge, I am not interested in what it is equal to; I just need to  show it converges or diverges. I'm drawing a blank here, but my gut instinct tells me ""no."" I don't think that $a_n$ (the $n^{th}$ term in the sum) approaches $0$ when $n$ approaches infinity, which is something that must be met for all converging sums, but I don't know how to prove it, as this is a rather difficult limit. Does this sum converge or not? and how would you go about showing your answer?",,"['sequences-and-series', 'limits', 'convergence-divergence', 'summation']"
32,Proving limits with existing results $\lim_{x\to a} \frac{\sin^2 x - \sin^2 a}{x-a} = \sin 2a$,Proving limits with existing results,\lim_{x\to a} \frac{\sin^2 x - \sin^2 a}{x-a} = \sin 2a,"So in my lecture yesterday I learnt how to prove that $\lim_{x\to +\infty} \left(1 + \dfrac{1}{x}\right)^{x} = e$, but I'm lost as to how to apply the results to prove limits. Any help would be greatly appreciated. Use the results $\lim_{x\to 0} \left(\dfrac{\sin x}{x}\right) = 1$ and $\lim_{x\to +\infty} \left(1 + \dfrac{1}{x}\right)^{x} = e $ to show that: $(i) \lim_{x\to a} \dfrac{\sin^2 x - \sin^2 a}{x-a} = \sin 2a;$ $(ii) \lim_{x\to -\infty} \left(1 + \dfrac{1}{x}\right)^{x} = e;$ $(iii) \lim_{x\to 0} (1 + x)^{\frac{1}{x}} = e$","So in my lecture yesterday I learnt how to prove that $\lim_{x\to +\infty} \left(1 + \dfrac{1}{x}\right)^{x} = e$, but I'm lost as to how to apply the results to prove limits. Any help would be greatly appreciated. Use the results $\lim_{x\to 0} \left(\dfrac{\sin x}{x}\right) = 1$ and $\lim_{x\to +\infty} \left(1 + \dfrac{1}{x}\right)^{x} = e $ to show that: $(i) \lim_{x\to a} \dfrac{\sin^2 x - \sin^2 a}{x-a} = \sin 2a;$ $(ii) \lim_{x\to -\infty} \left(1 + \dfrac{1}{x}\right)^{x} = e;$ $(iii) \lim_{x\to 0} (1 + x)^{\frac{1}{x}} = e$",,"['calculus', 'limits']"
33,Prove that $\lim_{x\to 2} \frac{1}{x} = \frac{1}{2}$,Prove that,\lim_{x\to 2} \frac{1}{x} = \frac{1}{2},"I want to prove this limit by using $(\epsilon,\delta)$ definition $$\lim_{x\to 2} \frac{1}{x} = \frac{1}{2}$$ Here is what I have done $$|\frac{1}{x} - \frac{1}{2}|<\epsilon \Leftrightarrow -\epsilon < \frac{1}{x} - \frac{1}{2} < \epsilon$$ $$\Leftrightarrow -\epsilon + \frac{1}{2} < \frac{1}{x} < \epsilon + \frac{1}{2}$$ $$\Leftrightarrow \frac{1}{-\epsilon + \frac{1}{2}} > x > \frac{1}{\epsilon + \frac{1}{2}}$$ I have problem at this point by the definition, $\epsilon$ can be any positive arbitrary number so $\frac{1}{-\epsilon + \frac{1}{2}}$ can be negative number so $x$ can be both smaller than $\frac{1}{-\epsilon + \frac{1}{2}}$ and bigger than $\frac{1}{\epsilon + \frac{1}{2}}$ then the domain of $x$ will be $[-\infty,\frac{1}{-\epsilon + \frac{1}{2}}]\cup [\frac{1}{\epsilon + \frac{1}{2}},\infty]$ But also by the definition, when $f(x)$ approach the limit, it is very small and we can chose $\epsilon$ to be sufficient small to make $\frac{1}{-\epsilon + \frac{1}{2}}>0$. So which way should I do. By the former, the thing will be a mess because the domain of x must be something like $[a,b]$ not $[a,b]\cup[c,d]$ So what should I do to prove this limit.","I want to prove this limit by using $(\epsilon,\delta)$ definition $$\lim_{x\to 2} \frac{1}{x} = \frac{1}{2}$$ Here is what I have done $$|\frac{1}{x} - \frac{1}{2}|<\epsilon \Leftrightarrow -\epsilon < \frac{1}{x} - \frac{1}{2} < \epsilon$$ $$\Leftrightarrow -\epsilon + \frac{1}{2} < \frac{1}{x} < \epsilon + \frac{1}{2}$$ $$\Leftrightarrow \frac{1}{-\epsilon + \frac{1}{2}} > x > \frac{1}{\epsilon + \frac{1}{2}}$$ I have problem at this point by the definition, $\epsilon$ can be any positive arbitrary number so $\frac{1}{-\epsilon + \frac{1}{2}}$ can be negative number so $x$ can be both smaller than $\frac{1}{-\epsilon + \frac{1}{2}}$ and bigger than $\frac{1}{\epsilon + \frac{1}{2}}$ then the domain of $x$ will be $[-\infty,\frac{1}{-\epsilon + \frac{1}{2}}]\cup [\frac{1}{\epsilon + \frac{1}{2}},\infty]$ But also by the definition, when $f(x)$ approach the limit, it is very small and we can chose $\epsilon$ to be sufficient small to make $\frac{1}{-\epsilon + \frac{1}{2}}>0$. So which way should I do. By the former, the thing will be a mess because the domain of x must be something like $[a,b]$ not $[a,b]\cup[c,d]$ So what should I do to prove this limit.",,"['calculus', 'limits', 'epsilon-delta']"
34,Recursion and Time Complexity Concept,Recursion and Time Complexity Concept,,"The question prompt is as follows: Consider the function $f(n)$ defined as:   $$f(n) = \begin{cases}n(n-1)f(n-2) & n > 1\\1 & n=0,\; n=1\end{cases}$$ How may be $g(n)$ be defined to make $f(n) \in \mathcal O(g(n))$?  (I'm supposed to choose one response from $1$ and a response from $2$) For $n>0$, define $g(n)$ as: Option 1: $g(n) = g(n-1) + n$ Option 2: $g(n) = ng(n-1)$. Then, assign: Option 1: $g(0) = 0$ Option 2: $g(0) = 100$. Now, I solved the recursive equation and got the following: $$f(n) = n!$$ If we define $g(n) = g(n-1) + n$: $$g(n) = n(n+1)/2 + \{0\text{ or }100\}$$ If we define $g(n) = n\cdot g(n-1)$: $$g(n) = n! \cdot \{0\text{ or }100\}$$ (The $\{0\text{ or }100\}$ because when $g(0)$ could be $0$ or $100$.) However, I am not seeing how either definition of $g(n)$ yields $\lim_{n\to\infty} (f(n)/g(n)) = 0$.","The question prompt is as follows: Consider the function $f(n)$ defined as:   $$f(n) = \begin{cases}n(n-1)f(n-2) & n > 1\\1 & n=0,\; n=1\end{cases}$$ How may be $g(n)$ be defined to make $f(n) \in \mathcal O(g(n))$?  (I'm supposed to choose one response from $1$ and a response from $2$) For $n>0$, define $g(n)$ as: Option 1: $g(n) = g(n-1) + n$ Option 2: $g(n) = ng(n-1)$. Then, assign: Option 1: $g(0) = 0$ Option 2: $g(0) = 100$. Now, I solved the recursive equation and got the following: $$f(n) = n!$$ If we define $g(n) = g(n-1) + n$: $$g(n) = n(n+1)/2 + \{0\text{ or }100\}$$ If we define $g(n) = n\cdot g(n-1)$: $$g(n) = n! \cdot \{0\text{ or }100\}$$ (The $\{0\text{ or }100\}$ because when $g(0)$ could be $0$ or $100$.) However, I am not seeing how either definition of $g(n)$ yields $\lim_{n\to\infty} (f(n)/g(n)) = 0$.",,"['limits', 'asymptotics', 'recursion']"
35,Limits with squares,Limits with squares,,I have problem with finding such limits: a) $\displaystyle \lim_{n \to \infty}\frac{n^2}{7^{\sqrt{n}}}$ b) $\displaystyle \lim_{n \to \infty}\frac{3^{\sqrt{n}}}{2^n} $ I think the method of solving this two is similar but I don't know what tool to use.  I tried using the fact that if $\displaystyle\lim_{n \to \infty}\frac{|a_{n+1}|}{|a_{n}|}<1$ then $\displaystyle\lim_{n \to \infty} a_n=0$ but it dosn't work,I have problem with finding such limits: a) $\displaystyle \lim_{n \to \infty}\frac{n^2}{7^{\sqrt{n}}}$ b) $\displaystyle \lim_{n \to \infty}\frac{3^{\sqrt{n}}}{2^n} $ I think the method of solving this two is similar but I don't know what tool to use.  I tried using the fact that if $\displaystyle\lim_{n \to \infty}\frac{|a_{n+1}|}{|a_{n}|}<1$ then $\displaystyle\lim_{n \to \infty} a_n=0$ but it dosn't work,,"['calculus', 'limits']"
36,Limit of floor function when $x$ goes infinity,Limit of floor function when  goes infinity,x,"Is it true that $\lim_{x \to \infty} (\left \lfloor{x}\right \rfloor -x) = 0$, or alternatively, $\lim_{x \to \infty} \left \lfloor{x}\right \rfloor=x$? If so, how can we prove it using $\varepsilon$-$\delta$ method?","Is it true that $\lim_{x \to \infty} (\left \lfloor{x}\right \rfloor -x) = 0$, or alternatively, $\lim_{x \to \infty} \left \lfloor{x}\right \rfloor=x$? If so, how can we prove it using $\varepsilon$-$\delta$ method?",,"['limits', 'ceiling-and-floor-functions', 'epsilon-delta']"
37,Calculating the limit $\lim_{c\rightarrow 1+}\sum_{j=0}^{\lfloor\frac{\log n}{\log c}\rfloor}(-1)^j\binom{z}{j}c^j$,Calculating the limit,\lim_{c\rightarrow 1+}\sum_{j=0}^{\lfloor\frac{\log n}{\log c}\rfloor}(-1)^j\binom{z}{j}c^j,"Does anyone know how to calculate, for constant values of $n$ and $z$, this limit? $$\lim_{c\rightarrow 1+}\sum_{j=0}^{\lfloor\frac{\log n}{\log c}\rfloor}(-1)^j\binom{z}{j}c^j$$ Thanks!","Does anyone know how to calculate, for constant values of $n$ and $z$, this limit? $$\lim_{c\rightarrow 1+}\sum_{j=0}^{\lfloor\frac{\log n}{\log c}\rfloor}(-1)^j\binom{z}{j}c^j$$ Thanks!",,"['sequences-and-series', 'limits']"
38,Evaluate the Limit as it approaches 1/2,Evaluate the Limit as it approaches 1/2,,"$$\lim_{x\to \frac12} \frac{2x^2-x}{|x-1/2|}$$ Hi, I'm just wondering If I answered this right. lim x-> 1/2^+= x(2x-1)/(x-1/2)              = (2x-1)/-1/2              =(2(1/2)-1)/-1/2              =0 $$\begin{align} \lim_{x\to 1/2^+}&= \frac{x(2x-1)}{x-1/2}\\              &= \frac{2x-1}{-1/2}\\              &=\frac{2(1/2)-1}{-1/2}\\              &=0 \end{align}$$ lim x-> 1/2^-= x(2x-1)/-x+1/2              =2x-1/-1+1/2              =2x/1/2              =2 Therefore lim x-> 1/2 DNE Is this correct? Thanks","$$\lim_{x\to \frac12} \frac{2x^2-x}{|x-1/2|}$$ Hi, I'm just wondering If I answered this right. lim x-> 1/2^+= x(2x-1)/(x-1/2)              = (2x-1)/-1/2              =(2(1/2)-1)/-1/2              =0 $$\begin{align} \lim_{x\to 1/2^+}&= \frac{x(2x-1)}{x-1/2}\\              &= \frac{2x-1}{-1/2}\\              &=\frac{2(1/2)-1}{-1/2}\\              &=0 \end{align}$$ lim x-> 1/2^-= x(2x-1)/-x+1/2              =2x-1/-1+1/2              =2x/1/2              =2 Therefore lim x-> 1/2 DNE Is this correct? Thanks",,"['calculus', 'limits']"
39,Terminology - Limit doesn't exist,Terminology - Limit doesn't exist,,"Take the following limit: $$ \lim_{x \to 2}  \dfrac{x+2}{x-2} $$ This doesn't exist. My textbook says it doesn't because ""The denominator approaches 0 (from both sides) while the numerator does not."" I don't understand what this means. I do understand that it doesn't exist. My thought process is that the left limit and the right limit aren't equal, so the limit doesn't exist. But I want to know what is meant by the text in the textbook. Can anybody give me an example where the numerator also approaches 0 from both sides?","Take the following limit: $$ \lim_{x \to 2}  \dfrac{x+2}{x-2} $$ This doesn't exist. My textbook says it doesn't because ""The denominator approaches 0 (from both sides) while the numerator does not."" I don't understand what this means. I do understand that it doesn't exist. My thought process is that the left limit and the right limit aren't equal, so the limit doesn't exist. But I want to know what is meant by the text in the textbook. Can anybody give me an example where the numerator also approaches 0 from both sides?",,"['limits', 'terminology']"
40,Proving that $\lim_{x\to3}\frac{x}{4x-9}=1$,Proving that,\lim_{x\to3}\frac{x}{4x-9}=1,"I am learning to prove limits with the epsilon-delta definition. This is officially the first exercise I complete on my own without peeking into the solution. My answer is different from the one in the solution - and to my horror, it is fairly larger than the one in the author's answer. But from what I gather, there is no single solution to these questions (I hope). Could you take a look at my procedure? Is it correct? Prove $$\lim_{x\to3}\frac{x}{4x-9}=1$$ We want to prove that for some $\epsilon, \delta > 0$ $$|x-3| < \delta \iff \left| \frac{x}{4x-9}-1 \right| < \epsilon$$ Have $$\left|\frac{x}{4x-9}-1 \right|< \epsilon$$ $$\implies \left|\frac{x-4x+9}{4x-9}\right|< \epsilon$$ $$\implies \left|(-3x+9) \cdot \frac{1}{4x-9}\right|< \epsilon$$ $$\implies |-3|\cdot|(x-3)| \cdot \left|\frac{1}{4x-9}\right|< \epsilon$$ I would like to get rid of that $\left|\frac{1}{4x-9}\right|$. First, notice that it is undefined for $x = \frac{9}{4}$. Now then, keeping that in mind, let $$|(x-3)| < \delta$$ Let's add another constraint: $$|(x-3)| < \delta < \frac{9}{4}$$ After all, we can't let $x$ to undefine that fraction. Anyway, the above implies that $$-\frac{9}{4} < x-3 < \frac{9}{4}$$ I want this $x-3$ to become $\frac{1}{4x-9}$, so we first multiply by $4$: $$-9 < 4x-12 < 9$$ And add $3$: $$-6 < 4x-9 < 6$$ And invert... $$-\frac{1}{6} < \frac{1}{4x-9} < \frac{1}{6}$$ Cool, now we can replace $\frac{1}{4x-9}$ and end up with $$|-3|\cdot|(x-3)| \cdot \left|-\frac{1}{6}\right|< \epsilon$$ Solve for $|(x-3)|$: $$3\cdot|(x-3)| \cdot \frac{1}{6}< \epsilon$$ $$|(x-3)|< 2\epsilon$$ Answer : $$\delta = \min\left\{\frac{9}{4},2\epsilon\right\}$$ The answer given in the solution is $$\delta = \min\left\{\frac{1}{4},\frac{2}{3}\epsilon\right\}$$ As you can see, to my horror, the ""range"" of values in my solution is drastically greater than the one in the author's solution. Hence I am concerned that my answer is probably wrong.","I am learning to prove limits with the epsilon-delta definition. This is officially the first exercise I complete on my own without peeking into the solution. My answer is different from the one in the solution - and to my horror, it is fairly larger than the one in the author's answer. But from what I gather, there is no single solution to these questions (I hope). Could you take a look at my procedure? Is it correct? Prove $$\lim_{x\to3}\frac{x}{4x-9}=1$$ We want to prove that for some $\epsilon, \delta > 0$ $$|x-3| < \delta \iff \left| \frac{x}{4x-9}-1 \right| < \epsilon$$ Have $$\left|\frac{x}{4x-9}-1 \right|< \epsilon$$ $$\implies \left|\frac{x-4x+9}{4x-9}\right|< \epsilon$$ $$\implies \left|(-3x+9) \cdot \frac{1}{4x-9}\right|< \epsilon$$ $$\implies |-3|\cdot|(x-3)| \cdot \left|\frac{1}{4x-9}\right|< \epsilon$$ I would like to get rid of that $\left|\frac{1}{4x-9}\right|$. First, notice that it is undefined for $x = \frac{9}{4}$. Now then, keeping that in mind, let $$|(x-3)| < \delta$$ Let's add another constraint: $$|(x-3)| < \delta < \frac{9}{4}$$ After all, we can't let $x$ to undefine that fraction. Anyway, the above implies that $$-\frac{9}{4} < x-3 < \frac{9}{4}$$ I want this $x-3$ to become $\frac{1}{4x-9}$, so we first multiply by $4$: $$-9 < 4x-12 < 9$$ And add $3$: $$-6 < 4x-9 < 6$$ And invert... $$-\frac{1}{6} < \frac{1}{4x-9} < \frac{1}{6}$$ Cool, now we can replace $\frac{1}{4x-9}$ and end up with $$|-3|\cdot|(x-3)| \cdot \left|-\frac{1}{6}\right|< \epsilon$$ Solve for $|(x-3)|$: $$3\cdot|(x-3)| \cdot \frac{1}{6}< \epsilon$$ $$|(x-3)|< 2\epsilon$$ Answer : $$\delta = \min\left\{\frac{9}{4},2\epsilon\right\}$$ The answer given in the solution is $$\delta = \min\left\{\frac{1}{4},\frac{2}{3}\epsilon\right\}$$ As you can see, to my horror, the ""range"" of values in my solution is drastically greater than the one in the author's solution. Hence I am concerned that my answer is probably wrong.",,"['calculus', 'limits']"
41,"Suppose $(s_n)$ converges and that $s_n \geq a$ for all but finitely many terms, show $\lim s_n \geq a$","Suppose  converges and that  for all but finitely many terms, show",(s_n) s_n \geq a \lim s_n \geq a,"I've got a few questions about the problem. Prob :Suppose $(s_n)$ converges and that $s_n \geq a$ for all but finitely many terms, show $\lim s_n \geq a$ The solution here breaks this problem up into two parts. Q1. I don't understand why is it necessary to consider the finitely many terms that $s_k < a?$ Doesn't the condition that $s_n \geq a$ for all, but finitely many terms and $(s_n)$ being convergent imply that $$\forall \epsilon > 0, \exists N_0 \in \mathbb{N} \implies \forall n > N_0 \implies |s_n - s| <\epsilon$$ So that all the terms after $N_0$ are going to be close to $s$ and therefore $$a < s+\epsilon$$ So why do we need to show the existence of $N > \max \{ M, N_0 \}$ when the definition says there is going to be an $N'$ that gives us convergence? Q2. Also what is wrong with the following ""proof""? Proof Since $(s_n)$ converges, $$\lim s_n \geq \lim a = a.$$ Ii want to say the proof is wrong because $s_n \geq a$ is not true for every $n$, if it is true for all $n$ then it may be correct? But I thought limits only care about what happens in the long term, so I am not entirely sure what is really the mistake... EDIT : I was just going to say (very roughly) $s_n \to s \implies \forall \epsilon >0, \exists N_1 \ni \forall n > N_1 s_n < s + \epsilon.$ Now for the rest of the $s_k < 1$ (where $k > N_1$), choose a new $N > \max \{ \max_{k}, N_1 \}$ so that $a \geq s_n < s + \epsilon.$ This is true for all $\epsilon >0$, so take $\epsilon = 0$","I've got a few questions about the problem. Prob :Suppose $(s_n)$ converges and that $s_n \geq a$ for all but finitely many terms, show $\lim s_n \geq a$ The solution here breaks this problem up into two parts. Q1. I don't understand why is it necessary to consider the finitely many terms that $s_k < a?$ Doesn't the condition that $s_n \geq a$ for all, but finitely many terms and $(s_n)$ being convergent imply that $$\forall \epsilon > 0, \exists N_0 \in \mathbb{N} \implies \forall n > N_0 \implies |s_n - s| <\epsilon$$ So that all the terms after $N_0$ are going to be close to $s$ and therefore $$a < s+\epsilon$$ So why do we need to show the existence of $N > \max \{ M, N_0 \}$ when the definition says there is going to be an $N'$ that gives us convergence? Q2. Also what is wrong with the following ""proof""? Proof Since $(s_n)$ converges, $$\lim s_n \geq \lim a = a.$$ Ii want to say the proof is wrong because $s_n \geq a$ is not true for every $n$, if it is true for all $n$ then it may be correct? But I thought limits only care about what happens in the long term, so I am not entirely sure what is really the mistake... EDIT : I was just going to say (very roughly) $s_n \to s \implies \forall \epsilon >0, \exists N_1 \ni \forall n > N_1 s_n < s + \epsilon.$ Now for the rest of the $s_k < 1$ (where $k > N_1$), choose a new $N > \max \{ \max_{k}, N_1 \}$ so that $a \geq s_n < s + \epsilon.$ This is true for all $\epsilon >0$, so take $\epsilon = 0$",,"['calculus', 'real-analysis', 'limits', 'inequality']"
42,Having trouble understanding why the $r$-th mean tends to the geometric mean as $r$ tends to zero,Having trouble understanding why the -th mean tends to the geometric mean as  tends to zero,r r,"I am having trouble understanding the proof of Theorem 3 in ""Inequalities"" by Hardy, Littlewood and Pólya. This theorem states that the $r$ -th mean approaches the geometric mean as $r$ approaches zero. I have seen the following post which makes things a little clearer (albeit using $o(r)$ instead of $O(r^2)$ ): Why is the $0$th power mean defined to be the geometric mean? However, I still cannot determine why: (a): $a^r = 1 + r\log(a) + O(r^2)$ as $r$ tends to zero, and (b): $\lim_{r\to 0} (1 + rx + o(r))^{1/r} = e^x$ . I have a pretty solid grasp of limits, as well as the log and exp functions, but I have never really been taught anything substantial on big/little-O notation, in particular as the variable approaches zero. Could somebody point me towards a suitable proof of (a) and (b) above please.","I am having trouble understanding the proof of Theorem 3 in ""Inequalities"" by Hardy, Littlewood and Pólya. This theorem states that the -th mean approaches the geometric mean as approaches zero. I have seen the following post which makes things a little clearer (albeit using instead of ): Why is the $0$th power mean defined to be the geometric mean? However, I still cannot determine why: (a): as tends to zero, and (b): . I have a pretty solid grasp of limits, as well as the log and exp functions, but I have never really been taught anything substantial on big/little-O notation, in particular as the variable approaches zero. Could somebody point me towards a suitable proof of (a) and (b) above please.",r r o(r) O(r^2) a^r = 1 + r\log(a) + O(r^2) r \lim_{r\to 0} (1 + rx + o(r))^{1/r} = e^x,"['limits', 'logarithms']"
43,Distribution of n binomial trials with probability 1/i as n goes to infinity,Distribution of n binomial trials with probability 1/i as n goes to infinity,,"Consider flipping $n$ independent coins -- the $i$-th coin flipped has probability of $\frac{1}{i}$ of being heads, and tails otherwise (in particular, the first coin is always heads). For example, with $3$ coins, the first coin is 100% heads, the second is 50% heads, and the third is $\frac{100}{3}$% heads. As $n$ goes to infinity, can you proof or disproof that for any given positive integer constant $k$, the probability that there will be at least $k$ heads is $1$?","Consider flipping $n$ independent coins -- the $i$-th coin flipped has probability of $\frac{1}{i}$ of being heads, and tails otherwise (in particular, the first coin is always heads). For example, with $3$ coins, the first coin is 100% heads, the second is 50% heads, and the third is $\frac{100}{3}$% heads. As $n$ goes to infinity, can you proof or disproof that for any given positive integer constant $k$, the probability that there will be at least $k$ heads is $1$?",,"['probability', 'limits']"
44,Squeeze Theorem: Finding the limit of a trig function,Squeeze Theorem: Finding the limit of a trig function,,"I'm stuck on finding the limit of a complex fraction/trig function.  Could someone please assist, or point out where I'm going wrong? Determine $$\lim\limits_{x \to 0} \frac{(x+1)\cos(\ln(x^2))}{\sqrt{(x^2+2)}}$$ For all $x$: $$-1 \le \cos(\ln(x^2)) \le 1$$ Multiply by $(x+1)$: $$-(x+1) \le (x+1)\cos(\ln(x^2)) \le (x+1)$$ Divide by $\sqrt{(x^2+2)}$: $$\frac{-(x+1)}{\sqrt{(x^2+2)}} \le \frac{(x+1)\cos(\ln(x^2))}{\sqrt{(x^2+2)}} \le \frac{(x+1)}{\sqrt{(x^2+2)}}$$ Now to find the limits: $$\lim\limits_{x \to 0} \frac{-(x+1)}{\sqrt{(x^2+2)}} = \frac{-1}{\sqrt{(2)}}$$ $$\lim\limits_{x \to 0} \frac{(x+1)}{\sqrt{(x^2+2)}} = \frac{1}{\sqrt{(2)}}$$ This is where I get stuck.  My limits are not equal, so I cannot solve using the Squeeze Theorem.  I must've skipped a step or used the wrong approach, but I'm not sure where.","I'm stuck on finding the limit of a complex fraction/trig function.  Could someone please assist, or point out where I'm going wrong? Determine $$\lim\limits_{x \to 0} \frac{(x+1)\cos(\ln(x^2))}{\sqrt{(x^2+2)}}$$ For all $x$: $$-1 \le \cos(\ln(x^2)) \le 1$$ Multiply by $(x+1)$: $$-(x+1) \le (x+1)\cos(\ln(x^2)) \le (x+1)$$ Divide by $\sqrt{(x^2+2)}$: $$\frac{-(x+1)}{\sqrt{(x^2+2)}} \le \frac{(x+1)\cos(\ln(x^2))}{\sqrt{(x^2+2)}} \le \frac{(x+1)}{\sqrt{(x^2+2)}}$$ Now to find the limits: $$\lim\limits_{x \to 0} \frac{-(x+1)}{\sqrt{(x^2+2)}} = \frac{-1}{\sqrt{(2)}}$$ $$\lim\limits_{x \to 0} \frac{(x+1)}{\sqrt{(x^2+2)}} = \frac{1}{\sqrt{(2)}}$$ This is where I get stuck.  My limits are not equal, so I cannot solve using the Squeeze Theorem.  I must've skipped a step or used the wrong approach, but I'm not sure where.",,"['calculus', 'real-analysis', 'limits', 'trigonometry']"
45,How many terms of the Taylor expansion should I develop?,How many terms of the Taylor expansion should I develop?,,"How do I know how many terms of the power series should I develop to evaluate a limit? Example : Given this limit: $L:=\displaystyle\lim_{x\to0}\left(\frac{\ln(1+x)}x-e^{-x/2}\right)\frac1{\cosh x - 1}$ I would expand the power series for each elementary function just to the first/second power: $$\ln(1+x) = x + o(x)$$ $$e^{-x/2} = 1 - x/2 + o(x)$$ $$\cosh x = 1+ x^2/2 + o(x^2)$$ and then: $\displaystyle L=\lim_{x\to0}\left(\frac{x + o(x)}x-(1-x/2 + o(x))\right)\frac1{1+x^2/2 + o(x^2) - 1}=\lim_{x\to0}\frac x2\frac 1{x^2/2}=\infty$ which of course is wrong. If instead I did: $$\ln(1+x)=x-x^2/2+x^3/3+o(x^3)$$ $$e^{-x/2}=1-x/2+x^2/8+o(x^2)$$ $$\cosh x = 1 + x^2/2 + o(x^2)$$ I would get $L=5/12$ which is the correct result. So the question is, how do I know how many terms should I compute to obtain the correct result for a given limit?","How do I know how many terms of the power series should I develop to evaluate a limit? Example : Given this limit: $L:=\displaystyle\lim_{x\to0}\left(\frac{\ln(1+x)}x-e^{-x/2}\right)\frac1{\cosh x - 1}$ I would expand the power series for each elementary function just to the first/second power: $$\ln(1+x) = x + o(x)$$ $$e^{-x/2} = 1 - x/2 + o(x)$$ $$\cosh x = 1+ x^2/2 + o(x^2)$$ and then: $\displaystyle L=\lim_{x\to0}\left(\frac{x + o(x)}x-(1-x/2 + o(x))\right)\frac1{1+x^2/2 + o(x^2) - 1}=\lim_{x\to0}\frac x2\frac 1{x^2/2}=\infty$ which of course is wrong. If instead I did: $$\ln(1+x)=x-x^2/2+x^3/3+o(x^3)$$ $$e^{-x/2}=1-x/2+x^2/8+o(x^2)$$ $$\cosh x = 1 + x^2/2 + o(x^2)$$ I would get $L=5/12$ which is the correct result. So the question is, how do I know how many terms should I compute to obtain the correct result for a given limit?",,"['limits', 'taylor-expansion']"
46,"If $a_{n+1}=\cos(a_n)$ for $n\ge0$ and $a_0 \in [-\pi/2,\pi/2]$, find $\lim_{n \to \infty}a_n$ if it exists","If  for  and , find  if it exists","a_{n+1}=\cos(a_n) n\ge0 a_0 \in [-\pi/2,\pi/2] \lim_{n \to \infty}a_n","Let $a_{n+1}=\cos(a_n)$ for $n\ge0$ and $a_0 \in [0,\pi/2]$ Find $\lim_{n \to \infty}a_n$ if it exists. I drew some sketches and it does seem like the limit exists, it's probably $x$ such that $\cos(x)=x$ I have no idea how to go about solving this, hints would really be appreciated. Thank you for your time!","Let $a_{n+1}=\cos(a_n)$ for $n\ge0$ and $a_0 \in [0,\pi/2]$ Find $\lim_{n \to \infty}a_n$ if it exists. I drew some sketches and it does seem like the limit exists, it's probably $x$ such that $\cos(x)=x$ I have no idea how to go about solving this, hints would really be appreciated. Thank you for your time!",,"['real-analysis', 'limits']"
47,Limit in $S' (\mathbb{R})$,Limit in,S' (\mathbb{R}),"Given the sequence of distributions:  $$ x^3~ \sin (nx),~~n \in  (\mathbb{N}) $$ How can i find the limit for $n \rightarrow \infty$? I tried with the usual substitution $y=nx$, but it leads to integrals that don't converge. Using instead the Riemann-Lebesgue theorem, i get that the limit is zero, but I don't think this is the right answer (Wolfram alpha says that, in the sense of functions, the limit is $x^3 -1$). Am I missing something? Why can't i use the Riemann-Lebesgue theorem?","Given the sequence of distributions:  $$ x^3~ \sin (nx),~~n \in  (\mathbb{N}) $$ How can i find the limit for $n \rightarrow \infty$? I tried with the usual substitution $y=nx$, but it leads to integrals that don't converge. Using instead the Riemann-Lebesgue theorem, i get that the limit is zero, but I don't think this is the right answer (Wolfram alpha says that, in the sense of functions, the limit is $x^3 -1$). Am I missing something? Why can't i use the Riemann-Lebesgue theorem?",,"['limits', 'distribution-theory']"
48,What am I doing wrong with this derivative? (Calculus),What am I doing wrong with this derivative? (Calculus),,"I've been doing derivatives with the formula: Definition of a Derivative: for every $x$ plugin $(x+h)$, then subtract original from the equation. This means for $x^2$, I get: $$\frac{(x+h)^2 - x^2}{h}.$$ When I factor it: $$\frac{x^2+h^2+2xh-x^2}{h},$$ then reduce: $$\frac{h^2+2xh}{h},$$ Distribute: $$\frac{h(2x+h)}{h},$$ Cancel: $$2x+h.$$ I know the derivative is $2x$, but I thought the definition was to cancel ALL $h$'s out. Am I doing something wrong? Why is there an $h$ left? Thanks","I've been doing derivatives with the formula: Definition of a Derivative: for every $x$ plugin $(x+h)$, then subtract original from the equation. This means for $x^2$, I get: $$\frac{(x+h)^2 - x^2}{h}.$$ When I factor it: $$\frac{x^2+h^2+2xh-x^2}{h},$$ then reduce: $$\frac{h^2+2xh}{h},$$ Distribute: $$\frac{h(2x+h)}{h},$$ Cancel: $$2x+h.$$ I know the derivative is $2x$, but I thought the definition was to cancel ALL $h$'s out. Am I doing something wrong? Why is there an $h$ left? Thanks",,"['calculus', 'limits', 'discrete-mathematics', 'derivatives']"
49,Dirichlet's function expressed as $\lim_{m\to\infty} \lim_{n\to\infty} \cos^{2n}(m!\pi x)$ [duplicate],Dirichlet's function expressed as  [duplicate],\lim_{m\to\infty} \lim_{n\to\infty} \cos^{2n}(m!\pi x),This question already has answers here : How to prove that $\lim(\underset{k\rightarrow\infty}{\lim}(\cos(|n!\pi x|)^{2k}))=\chi_\mathbb{Q}$ [duplicate] (2 answers) Closed 9 years ago . How can we see that Dirichlet's function $$D(x):=\lim_{m\to\infty} \lim_{n\to\infty} \cos^{2n}(m!\pi x)= \begin{cases}         1 & x\in\mathbb Q\\         0 & x\notin\mathbb Q\\         \end{cases}$$,This question already has answers here : How to prove that $\lim(\underset{k\rightarrow\infty}{\lim}(\cos(|n!\pi x|)^{2k}))=\chi_\mathbb{Q}$ [duplicate] (2 answers) Closed 9 years ago . How can we see that Dirichlet's function $$D(x):=\lim_{m\to\infty} \lim_{n\to\infty} \cos^{2n}(m!\pi x)= \begin{cases}         1 & x\in\mathbb Q\\         0 & x\notin\mathbb Q\\         \end{cases}$$,,"['calculus', 'real-analysis', 'limits', 'convergence-divergence']"
50,"uniform continuity on $(a, b]$ implies limit at $a^+$ exists and finite",uniform continuity on  implies limit at  exists and finite,"(a, b] a^+","Let a uniformly continuous function $f$ on $(a, b]$. Prove that $\lim_{x\rightarrow a^+} f(x)$ exists and finite. What I did so far: from the definition of uniform continuity: $$\forall\varepsilon >0.\exists\delta>0.\forall x,y\in(a,b]:\left| x-y \right|<\delta \Rightarrow \left| f(x)-f(y) \right| < \varepsilon$$ In particular, the statement is true for the sequence $\varepsilon_n = \frac{1}{n}$ and the interval $(a,a+\delta)$ $$\forall\varepsilon_n.\exists\delta>0.\forall x,y\in (a,a+\delta).\left| f(x) - f(y) \right| < \frac{1}{n}$$ I'm kinda stuck at this point, though I think I'm on the right path. How to proceed? Thanks.","Let a uniformly continuous function $f$ on $(a, b]$. Prove that $\lim_{x\rightarrow a^+} f(x)$ exists and finite. What I did so far: from the definition of uniform continuity: $$\forall\varepsilon >0.\exists\delta>0.\forall x,y\in(a,b]:\left| x-y \right|<\delta \Rightarrow \left| f(x)-f(y) \right| < \varepsilon$$ In particular, the statement is true for the sequence $\varepsilon_n = \frac{1}{n}$ and the interval $(a,a+\delta)$ $$\forall\varepsilon_n.\exists\delta>0.\forall x,y\in (a,a+\delta).\left| f(x) - f(y) \right| < \frac{1}{n}$$ I'm kinda stuck at this point, though I think I'm on the right path. How to proceed? Thanks.",,"['calculus', 'real-analysis', 'limits', 'continuity', 'uniform-continuity']"
51,Limits of trig functions,Limits of trig functions,,"How can I find the following problems using elementary trigonometry? $$\lim_{x\to 0}\frac{1−\cos x}{x^2}.$$ $$\lim_{x\to0}\frac{\tan x−\sin x}{x^3}. $$ Have attempted trig identities, didn't help.","How can I find the following problems using elementary trigonometry? $$\lim_{x\to 0}\frac{1−\cos x}{x^2}.$$ $$\lim_{x\to0}\frac{\tan x−\sin x}{x^3}. $$ Have attempted trig identities, didn't help.",,"['calculus', 'limits', 'trigonometry', 'derivatives']"
52,"Show that if $\sum_{n=1}^{\infty}a_n$ converges abs. and $(b_n)_{n \in \mathbb{N}}$ is bounded , then $\sum_{n=1}^{\infty} a_nb_n$ converges abs. [duplicate]","Show that if  converges abs. and  is bounded , then  converges abs. [duplicate]",\sum_{n=1}^{\infty}a_n (b_n)_{n \in \mathbb{N}} \sum_{n=1}^{\infty} a_nb_n,"This question already has an answer here : $b_n$ bounded, $\sum a_n$ converges absolutely, then $\sum a_nb_n$ also (1 answer) Closed 10 years ago . My attempt: Since $(b_n)_{n \in \mathbb{N}}$ is bounded it follows that there exists a bounded sequence $$(c_n)_{n \in \mathbb{N}}: 0 \leq |b_n| \leq |c_n|, \forall n \in \mathbb{N}$$ Therefore it follows that: $$\sum_{n=1}^{\infty} a_nb_n \leq \sum_{n=1}^{\infty} a_nc_n$$ Since $\sum_{n=1}^{\infty}a_n$ converges absolutely $\Rightarrow$ $\sum_{n=1}^{\infty}a_n$ converges $\Rightarrow (a_n)_{n \in \mathbb{N}} $ is a null sequence $\Rightarrow \exists N \in \mathbb{N}: \forall n \geq N: |a_n| \lt \frac{\epsilon}{|c|p}$, where $c:= max\{c_1,\dots,c_n,\dots\}, p \in \mathbb{N}$ Then it follows that: $$\forall n \geq N: |a_nc_n|=|a_n||c_n| \lt |a_n||c| \lt \frac{\epsilon}{|c|p}|c|=\frac{\epsilon}{p}$$ $$\Rightarrow \forall n \geq N: |a_{n+1}c_{n+1}+\dots+a_{n+p}c_{n+p}|\leq  |a_{n+1}c_{n+1}|+ \dots +|a_{n+p}c_{n+p}| \lt p\frac{\epsilon}{p}=\epsilon$$ So after Cauchy's convergence test, it follows that $\sum_{n=1}^{\infty} a_nc_n$ is a convergent majorant of $\sum_{n=1}^{\infty} a_nb_n$ and therefore $\sum_{n=1}^{\infty} a_nb_n$ converges absolutely. Was this proof correct? And a second part of that question is: If $\sum_{n=1}^{\infty}$ converges, but not absolutely, does $\sum_{n=1}^{\infty} a_nb_n$ converge? For my proof I didn't really needed $\sum_{n=1}^{\infty}$ to converge absolutely, I just used the fact that it converges when it converges absolutely and since I proved that $\sum_{n=1}^{\infty} a_nb_n$ converges absolutely, it converges as well. Is that right?","This question already has an answer here : $b_n$ bounded, $\sum a_n$ converges absolutely, then $\sum a_nb_n$ also (1 answer) Closed 10 years ago . My attempt: Since $(b_n)_{n \in \mathbb{N}}$ is bounded it follows that there exists a bounded sequence $$(c_n)_{n \in \mathbb{N}}: 0 \leq |b_n| \leq |c_n|, \forall n \in \mathbb{N}$$ Therefore it follows that: $$\sum_{n=1}^{\infty} a_nb_n \leq \sum_{n=1}^{\infty} a_nc_n$$ Since $\sum_{n=1}^{\infty}a_n$ converges absolutely $\Rightarrow$ $\sum_{n=1}^{\infty}a_n$ converges $\Rightarrow (a_n)_{n \in \mathbb{N}} $ is a null sequence $\Rightarrow \exists N \in \mathbb{N}: \forall n \geq N: |a_n| \lt \frac{\epsilon}{|c|p}$, where $c:= max\{c_1,\dots,c_n,\dots\}, p \in \mathbb{N}$ Then it follows that: $$\forall n \geq N: |a_nc_n|=|a_n||c_n| \lt |a_n||c| \lt \frac{\epsilon}{|c|p}|c|=\frac{\epsilon}{p}$$ $$\Rightarrow \forall n \geq N: |a_{n+1}c_{n+1}+\dots+a_{n+p}c_{n+p}|\leq  |a_{n+1}c_{n+1}|+ \dots +|a_{n+p}c_{n+p}| \lt p\frac{\epsilon}{p}=\epsilon$$ So after Cauchy's convergence test, it follows that $\sum_{n=1}^{\infty} a_nc_n$ is a convergent majorant of $\sum_{n=1}^{\infty} a_nb_n$ and therefore $\sum_{n=1}^{\infty} a_nb_n$ converges absolutely. Was this proof correct? And a second part of that question is: If $\sum_{n=1}^{\infty}$ converges, but not absolutely, does $\sum_{n=1}^{\infty} a_nb_n$ converge? For my proof I didn't really needed $\sum_{n=1}^{\infty}$ to converge absolutely, I just used the fact that it converges when it converges absolutely and since I proved that $\sum_{n=1}^{\infty} a_nb_n$ converges absolutely, it converges as well. Is that right?",,"['real-analysis', 'sequences-and-series', 'limits']"
53,A product for 1/e?,A product for 1/e?,,"This question is related to this question but I see that one part is really not a statistics question. That $\lim_{n \to \infty} (1 - 1/n)^n = 1/e $ is clear. What is not clear to me is under what circumstances $$\lim_{n \to \infty} \prod_{i=1}^n (1 - F_i) = 1/e. $$ Let me give some examples of candidates for $F_i$: Subdivide the standard normal curve into (say) 100 subintervals from, say,  Z = -5 to Z = 5. With each subinterval associate a number $F_i$ equal to the area under the curve in that subinterval. [removed] For $\frac{1}{2}\int_{-\pi/2}^{\pi/2}\cos x ~dx$ subdivide the interval $[ -\pi/2 < x < \pi/2]$ and find $F_i $ in a similar way. So either I am making a consistent mistake in calculating (not out of the question) or there is something general that I am missing. Can someone suggest why this is true, if it is, and how it generalizes? Thank you.","This question is related to this question but I see that one part is really not a statistics question. That $\lim_{n \to \infty} (1 - 1/n)^n = 1/e $ is clear. What is not clear to me is under what circumstances $$\lim_{n \to \infty} \prod_{i=1}^n (1 - F_i) = 1/e. $$ Let me give some examples of candidates for $F_i$: Subdivide the standard normal curve into (say) 100 subintervals from, say,  Z = -5 to Z = 5. With each subinterval associate a number $F_i$ equal to the area under the curve in that subinterval. [removed] For $\frac{1}{2}\int_{-\pi/2}^{\pi/2}\cos x ~dx$ subdivide the interval $[ -\pi/2 < x < \pi/2]$ and find $F_i $ in a similar way. So either I am making a consistent mistake in calculating (not out of the question) or there is something general that I am missing. Can someone suggest why this is true, if it is, and how it generalizes? Thank you.",,"['calculus', 'limits', 'infinite-product']"
54,$\lim_{n \to \infty} \frac1n \sum_{r=1}^n r^{\frac1r}$ [duplicate],[duplicate],\lim_{n \to \infty} \frac1n \sum_{r=1}^n r^{\frac1r},"This question already has answers here : Computing $\lim_{n\rightarrow \infty} \frac{1+\sqrt{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}$. (2 answers) Closed 6 years ago . How to evaluate $\lim_{n \to \infty} \frac1n \sum_{r=1}^n r^{\frac1r}$? I've tried finding it, and I know that without the $\frac1n$ factor, the sequence has the limit $n$. What about the series? Will it be 1, then? How to show?","This question already has answers here : Computing $\lim_{n\rightarrow \infty} \frac{1+\sqrt{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}$. (2 answers) Closed 6 years ago . How to evaluate $\lim_{n \to \infty} \frac1n \sum_{r=1}^n r^{\frac1r}$? I've tried finding it, and I know that without the $\frac1n$ factor, the sequence has the limit $n$. What about the series? Will it be 1, then? How to show?",,['limits']
55,probability that something will occur given infinite time,probability that something will occur given infinite time,,"thanks in advance for the help.  Let r be a round.  Also let N be a set of coins.  Suppose that every round I flip all of the coins.  Is it correct to say given infinite rounds the probability that there will be at least one round where all N coins are heads or tails is 1?  If so how would I express this conclusion mathematically?  I believe that I could express this conclusion (as true) using a limit, but I'm not completely convinced that using a limit approach is logically correct.","thanks in advance for the help.  Let r be a round.  Also let N be a set of coins.  Suppose that every round I flip all of the coins.  Is it correct to say given infinite rounds the probability that there will be at least one round where all N coins are heads or tails is 1?  If so how would I express this conclusion mathematically?  I believe that I could express this conclusion (as true) using a limit, but I'm not completely convinced that using a limit approach is logically correct.",,"['probability', 'limits']"
56,A problem with the domain of function in the definition of limits,A problem with the domain of function in the definition of limits,,"My Stewart's Calculus gives the following definition of limit: $f(x)$ is defined on some open interval containing $a$, except at   possibly $a$. So, $\lim_{x\to a} f(x) = L $ if and only if for every   number $\varepsilon>0$, there exists a corresponding number $\delta>0$   such that, if $0<|x-a|<\delta$ then $|f(x)-L|<\varepsilon$. I am not sure of the exact wordings of the above, but I am quite sure that its what meant it meant to say. However, my Introduction to Real Analysis: Bartle and Sherbert , gives: Let $A \subseteq \mathbb{R}$, and let $a$ be a cluster point of $A$.   For a function $f: A \mapsto \mathbb{R}$, a real number $L$ is said to be a limit of $f$ at $a$, if given any $\varepsilon>0$, there exists a $\delta>0$ such that if $x \in A$ and $0<|x-a|<\delta$, then $|f(x)-L|<\varepsilon$. Now my problem is when considering, for example, if $f: A \mapsto \mathbb{R}$, where $A=\{ x: x \text{ is irrational} \}$, and $f(x)=0$. By Stewart's definition, $f$ can't be defined on any open interval around $0$, as there will always be a rational number in it. So it fails the first condition, and $\lim_{x\to 0} f(x)$ is not defined. However, according to the definition of cluster point, there will always be one irrational $x$ that satisfies $0<|x-0|<\delta$ for every $\delta<0$. Hence $0$ would be a cluster point. Now, if $x\in A$, i.e. if $x$ is irrational and $0<|x|<\delta$ then obviously $0<\varepsilon$. So $\lim_{x\to 0} f(x)=0$. What is going on?","My Stewart's Calculus gives the following definition of limit: $f(x)$ is defined on some open interval containing $a$, except at   possibly $a$. So, $\lim_{x\to a} f(x) = L $ if and only if for every   number $\varepsilon>0$, there exists a corresponding number $\delta>0$   such that, if $0<|x-a|<\delta$ then $|f(x)-L|<\varepsilon$. I am not sure of the exact wordings of the above, but I am quite sure that its what meant it meant to say. However, my Introduction to Real Analysis: Bartle and Sherbert , gives: Let $A \subseteq \mathbb{R}$, and let $a$ be a cluster point of $A$.   For a function $f: A \mapsto \mathbb{R}$, a real number $L$ is said to be a limit of $f$ at $a$, if given any $\varepsilon>0$, there exists a $\delta>0$ such that if $x \in A$ and $0<|x-a|<\delta$, then $|f(x)-L|<\varepsilon$. Now my problem is when considering, for example, if $f: A \mapsto \mathbb{R}$, where $A=\{ x: x \text{ is irrational} \}$, and $f(x)=0$. By Stewart's definition, $f$ can't be defined on any open interval around $0$, as there will always be a rational number in it. So it fails the first condition, and $\lim_{x\to 0} f(x)$ is not defined. However, according to the definition of cluster point, there will always be one irrational $x$ that satisfies $0<|x-0|<\delta$ for every $\delta<0$. Hence $0$ would be a cluster point. Now, if $x\in A$, i.e. if $x$ is irrational and $0<|x|<\delta$ then obviously $0<\varepsilon$. So $\lim_{x\to 0} f(x)=0$. What is going on?",,"['calculus', 'real-analysis', 'limits', 'definition']"
57,Alternating series limit question [closed],Alternating series limit question [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Suppose $b_n > 0$ for all $n\geq1$ and $$\lim_{n\to\infty}n\left(\frac{b_n}{b_{n+1}}-1\right)>0,$$ show that the alternating series $\sum_{n=1}^\infty(-1)^n b_n$ converges. Any hints?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Suppose $b_n > 0$ for all $n\geq1$ and $$\lim_{n\to\infty}n\left(\frac{b_n}{b_{n+1}}-1\right)>0,$$ show that the alternating series $\sum_{n=1}^\infty(-1)^n b_n$ converges. Any hints?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
58,How to conclude that $r^t(E\cos(\theta t)+Fsin(\theta t))$ ossilates with increasing magnitude?,How to conclude that  ossilates with increasing magnitude?,r^t(E\cos(\theta t)+Fsin(\theta t)),"Given: $r^t(E\cos(\theta t)+Fsin(\theta t))$ Assume $r>1$, $E>0$, $t\ge0$ and $F$ is not known. How do we conclude that the given expression oscillates with increasing magnitude? My attempt: Since $r>1$, $r^t$ will tend to infinity as $t$ tends to infinity. Therefore, if the expression is supposed to oscillate with increasing magnitude, then $(E\cos(\theta t)+Fsin(\theta t))$ will need to change regularly change sign. I know that the $\cos$ and $sin$ function will alternate from -1 to 1 but I don't know how to apply that property to this question. Please help.","Given: $r^t(E\cos(\theta t)+Fsin(\theta t))$ Assume $r>1$, $E>0$, $t\ge0$ and $F$ is not known. How do we conclude that the given expression oscillates with increasing magnitude? My attempt: Since $r>1$, $r^t$ will tend to infinity as $t$ tends to infinity. Therefore, if the expression is supposed to oscillate with increasing magnitude, then $(E\cos(\theta t)+Fsin(\theta t))$ will need to change regularly change sign. I know that the $\cos$ and $sin$ function will alternate from -1 to 1 but I don't know how to apply that property to this question. Please help.",,"['algebra-precalculus', 'limits']"
59,About limits and successions,About limits and successions,,"This is a problem which I am not sure I solved correctly, mainly because there are some passages which are not very rigourous. Let $\{x_n\} $ be an increasing succession such that $x_n > 0$ and the   tangent at the point $x_n$ of the function $y = \cos x$ passes through   the origin. Let $c_n$ be the coefficient of the tangent line at the   point $x_n$, calculate $$\lim_{n\to \infty} x_n - n\pi$$ and $$\lim _{n \to \infty} nc_{2n}$$ My attempt Well the first thing I did was finding that it must be $\tan x_n = -\frac{1}{x_n}$ So it seems easy to see (from the graphics) from this that $x_n \to \infty$ as $n \to \infty$ This implies than $\lim_{n \to \infty} \tan x_n = 0 \Rightarrow x_n \sim n\pi$ But this is not enough to prove the first limit as it is an insufficient approximation. So I'm going to say that since $\tan {x_n} \sim x_n - n\pi$, then $$x_n = -\frac{1}{\tan x_n} \sim -\frac{1}{x_n - n\pi}$$ Basically I'm identifying $x_n$ with its second order approximation. from the last equation one easily find $$x_n = \frac{n\pi + \sqrt{n^2\pi^2 - 4}}{2}$$ So $$\lim_{n \to \infty} x_n - n\pi = -\frac{2}{n \pi} = 0$$ For the second one, $$c_{2n} = -\sin(x_{2n}) \sim \sin(-\frac{2}{n \pi}) \sim -\frac{2}{n \pi}$$ So $$\lim _{x \to \infty} nc_{2n} = n \cdot -\frac{2}{n \pi} = -\frac{2}{\pi}$$","This is a problem which I am not sure I solved correctly, mainly because there are some passages which are not very rigourous. Let $\{x_n\} $ be an increasing succession such that $x_n > 0$ and the   tangent at the point $x_n$ of the function $y = \cos x$ passes through   the origin. Let $c_n$ be the coefficient of the tangent line at the   point $x_n$, calculate $$\lim_{n\to \infty} x_n - n\pi$$ and $$\lim _{n \to \infty} nc_{2n}$$ My attempt Well the first thing I did was finding that it must be $\tan x_n = -\frac{1}{x_n}$ So it seems easy to see (from the graphics) from this that $x_n \to \infty$ as $n \to \infty$ This implies than $\lim_{n \to \infty} \tan x_n = 0 \Rightarrow x_n \sim n\pi$ But this is not enough to prove the first limit as it is an insufficient approximation. So I'm going to say that since $\tan {x_n} \sim x_n - n\pi$, then $$x_n = -\frac{1}{\tan x_n} \sim -\frac{1}{x_n - n\pi}$$ Basically I'm identifying $x_n$ with its second order approximation. from the last equation one easily find $$x_n = \frac{n\pi + \sqrt{n^2\pi^2 - 4}}{2}$$ So $$\lim_{n \to \infty} x_n - n\pi = -\frac{2}{n \pi} = 0$$ For the second one, $$c_{2n} = -\sin(x_{2n}) \sim \sin(-\frac{2}{n \pi}) \sim -\frac{2}{n \pi}$$ So $$\lim _{x \to \infty} nc_{2n} = n \cdot -\frac{2}{n \pi} = -\frac{2}{\pi}$$",,"['calculus', 'sequences-and-series', 'geometry', 'limits']"
60,Convergence of series of functions: $f_n(x)=u_n\sin(nx)$,Convergence of series of functions:,f_n(x)=u_n\sin(nx),"Let $f_n(x)=u_n\sin(nx)$  where $\displaystyle\sum f_n$  converges pointwise, and $ \displaystyle x \mapsto \sum_{n=0}^{+\infty} f_n(x)$ is continuous. Prove that $ u_n\rightarrow 0$ when n tends to $+\infty$ My 'attempt': We know that  $S_n=\displaystyle\sum_{k=1}^{n}u_k\sin(kx)$ converges to a real $a$, and  $$ S_{n+1}-S_n=u_{n+1}sin((n+1)x)  $$ converges to $0$. By induction we can prove that for all $x=(\frac{\pi}2)^k$ then $u_n$ tends to $0$. I really don't know how can I continue. Thank you in advance,","Let $f_n(x)=u_n\sin(nx)$  where $\displaystyle\sum f_n$  converges pointwise, and $ \displaystyle x \mapsto \sum_{n=0}^{+\infty} f_n(x)$ is continuous. Prove that $ u_n\rightarrow 0$ when n tends to $+\infty$ My 'attempt': We know that  $S_n=\displaystyle\sum_{k=1}^{n}u_k\sin(kx)$ converges to a real $a$, and  $$ S_{n+1}-S_n=u_{n+1}sin((n+1)x)  $$ converges to $0$. By induction we can prove that for all $x=(\frac{\pi}2)^k$ then $u_n$ tends to $0$. I really don't know how can I continue. Thank you in advance,",,"['real-analysis', 'sequences-and-series']"
61,Limit of $\frac {n^n}{n!}$ [duplicate],Limit of  [duplicate],\frac {n^n}{n!},"This question already has answers here : Proof for convergence of a given progression $a_n := n^n / n!$ (2 answers) Closed 10 years ago . I have to prove that $$\lim_{n\to \infty} \frac {n^n} {n!}=\infty$$ I've tried to look for a lower bound that also converges to $\infty$ (I don't know if I'm explainig myself correctly), but I haven't found one yet. Applying L'Hôpital is way too complicated in $n!$, and the epsilon proof does not work as I have no way whatsoever of finding N. Any ideas?","This question already has answers here : Proof for convergence of a given progression $a_n := n^n / n!$ (2 answers) Closed 10 years ago . I have to prove that $$\lim_{n\to \infty} \frac {n^n} {n!}=\infty$$ I've tried to look for a lower bound that also converges to $\infty$ (I don't know if I'm explainig myself correctly), but I haven't found one yet. Applying L'Hôpital is way too complicated in $n!$, and the epsilon proof does not work as I have no way whatsoever of finding N. Any ideas?",,"['calculus', 'limits', 'exponentiation', 'exponential-function', 'factorial']"
62,How to find this limit using integration?,How to find this limit using integration?,,What is the value of $$\lim_{n \to \infty}\frac{(\sum_{k=1}^{n} k^2 )*(\sum_{k=1}^{n} k^3 )}{(\sum_{k=1}^{n} k^6)}$$ I just know that it has to be done by converting it into an integral. I have no idea how to do it. Any other solution is also welcome.,What is the value of $$\lim_{n \to \infty}\frac{(\sum_{k=1}^{n} k^2 )*(\sum_{k=1}^{n} k^3 )}{(\sum_{k=1}^{n} k^6)}$$ I just know that it has to be done by converting it into an integral. I have no idea how to do it. Any other solution is also welcome.,,"['integration', 'sequences-and-series', 'limits', 'definite-integrals']"
63,Help me with this limit,Help me with this limit,,$$ \lim_{x\to0} {{xe^x \over e^x-1}-1 \over x}$$ I know it should equal  ${1 \over 2}$ because when i calculate with number like $0.0001$ the limit $\approx {1 \over 2}$   but i can't prove it.,$$ \lim_{x\to0} {{xe^x \over e^x-1}-1 \over x}$$ I know it should equal  ${1 \over 2}$ because when i calculate with number like $0.0001$ the limit $\approx {1 \over 2}$   but i can't prove it.,,['limits']
64,Is 'limit' synonymous with 'radius of convergence'?,Is 'limit' synonymous with 'radius of convergence'?,,"I of course read the Wikipedia article , but it sounded like such an abstract idea, that I could interpret only as a limit. The term appeared in a recent lecture apparently out of nowhere (though I was slightly late) - an example the lecturer has now given online makes it seem just like a limit to me (found the radius of convergence by doing a ratio test) but I wonder if there is some distinction in the terminologies, otherwise one wonders why anyone would favour the term over simply 'limit' - perhaps it is a series vs sequence distinction?","I of course read the Wikipedia article , but it sounded like such an abstract idea, that I could interpret only as a limit. The term appeared in a recent lecture apparently out of nowhere (though I was slightly late) - an example the lecturer has now given online makes it seem just like a limit to me (found the radius of convergence by doing a ratio test) but I wonder if there is some distinction in the terminologies, otherwise one wonders why anyone would favour the term over simply 'limit' - perhaps it is a series vs sequence distinction?",,"['sequences-and-series', 'limits', 'convergence-divergence']"
65,finding the value of the sum $\sum _{n=1}^{\infty} \frac{7^{n}}{8^{n}+2^{n}}$,finding the value of the sum,\sum _{n=1}^{\infty} \frac{7^{n}}{8^{n}+2^{n}},"I am concerned with finding the value of the sum $\sum _{n=1}^{\infty}  \frac{7^{n}}{8^{n}+2^{n}}$. There may be some easy way to do this, but I have not found a way to compute this sum, and ones like it. I cannot figure out how to turn it into a single easy-to-calculate exponential, but I am wondering if there is another way. I already know that this sum is convergent, but I want to know exactly how to calculate its value. Any help would be appreciated.","I am concerned with finding the value of the sum $\sum _{n=1}^{\infty}  \frac{7^{n}}{8^{n}+2^{n}}$. There may be some easy way to do this, but I have not found a way to compute this sum, and ones like it. I cannot figure out how to turn it into a single easy-to-calculate exponential, but I am wondering if there is another way. I already know that this sum is convergent, but I want to know exactly how to calculate its value. Any help would be appreciated.",,"['calculus', 'limits']"
66,Prove that the function is uniformly continuous,Prove that the function is uniformly continuous,,"Let $f(x)$ be a continuous function in $[0,\infty)$ there are $a,b \in \mathbb{R}$ such that $\lim_{x\to\infty} [f(x) - (ax +b)] =0$ prove that $f(x)$ is uniformly continuous in $[0,\infty)$ how i started: using that function limit definition: let $\epsilon >0$ there is a $M>$ such that for every $x>M, |f(x) - (ax +b)|<\epsilon$ in the interval $[0,M]$ the function is uniformly continuous (by weierstrass theorem). this is there part i got stuck in, i know that f(x) ""Converges"" with the $(ax+b)$ , but i cant find a $\delta$ that will prove what i need","Let be a continuous function in there are such that prove that is uniformly continuous in how i started: using that function limit definition: let there is a such that for every in the interval the function is uniformly continuous (by weierstrass theorem). this is there part i got stuck in, i know that f(x) ""Converges"" with the , but i cant find a that will prove what i need","f(x) [0,\infty) a,b \in \mathbb{R} \lim_{x\to\infty} [f(x) - (ax +b)] =0 f(x) [0,\infty) \epsilon >0 M> x>M, |f(x) - (ax +b)|<\epsilon [0,M] (ax+b) \delta","['calculus', 'analysis', 'limits', 'continuity', 'uniform-continuity']"
67,"Proof of l'Hôpital's rule, g'(0)=0","Proof of l'Hôpital's rule, g'(0)=0",,"l'Hôpital's rule for limits where $\mathbf{x\to 0}$. Let $f$ and $g$ be continuous and differentiable in a neighborhood of $x=0$. Then, if  $$\lim_{x\to 0} f(x) = \lim_{x\to 0} g(x) =0\,,$$  the following simplification can be made:  $$\lim_{x\to 0} \frac{f(x)}{g(x)}=\lim_{x\to 0} \frac{f'(x)}{g'(x)}\,,$$ provided that the limit in the right-hand side exists. Problem. The proof in my textbook assumes that $g'(0)\neq 0$. In many cases, however, that is not true, but the theorem can still be used. So somehow there must be a way to prove the theorem without this, or with some weaker requirement. Failed attempt. Rewrite with Maclaurin's formula: $$\lim_{x\to 0} \frac{f(x)}{g(x)}=\lim_{x\to 0}\frac{f(0)+f'(\xi)x}{g(0)+g'(\zeta)x}=\lim_{x\to 0}\frac{0+f'(\xi)x}{0+g'(\zeta)x}=\lim_{x\to 0}\frac{f'(\xi)}{g'(\zeta)}\,,$$ where $\xi$ and $\zeta$ are somewhere between 0 and $x$. When $x\to 0$, both $\xi$ and $\zeta$ go to zero, but when $g'(0)=0$, as some helpful people concluded here , that doesn't necessarily mean that last limit is equal to $\lim_{x\to 0}f'(x)/g'(x)$. Is this a dead end, or can I do something to make this work? Or are there any other proof methods that could be understood with basic calculus skills?","l'Hôpital's rule for limits where $\mathbf{x\to 0}$. Let $f$ and $g$ be continuous and differentiable in a neighborhood of $x=0$. Then, if  $$\lim_{x\to 0} f(x) = \lim_{x\to 0} g(x) =0\,,$$  the following simplification can be made:  $$\lim_{x\to 0} \frac{f(x)}{g(x)}=\lim_{x\to 0} \frac{f'(x)}{g'(x)}\,,$$ provided that the limit in the right-hand side exists. Problem. The proof in my textbook assumes that $g'(0)\neq 0$. In many cases, however, that is not true, but the theorem can still be used. So somehow there must be a way to prove the theorem without this, or with some weaker requirement. Failed attempt. Rewrite with Maclaurin's formula: $$\lim_{x\to 0} \frac{f(x)}{g(x)}=\lim_{x\to 0}\frac{f(0)+f'(\xi)x}{g(0)+g'(\zeta)x}=\lim_{x\to 0}\frac{0+f'(\xi)x}{0+g'(\zeta)x}=\lim_{x\to 0}\frac{f'(\xi)}{g'(\zeta)}\,,$$ where $\xi$ and $\zeta$ are somewhere between 0 and $x$. When $x\to 0$, both $\xi$ and $\zeta$ go to zero, but when $g'(0)=0$, as some helpful people concluded here , that doesn't necessarily mean that last limit is equal to $\lim_{x\to 0}f'(x)/g'(x)$. Is this a dead end, or can I do something to make this work? Or are there any other proof methods that could be understood with basic calculus skills?",,"['calculus', 'limits']"
68,Question about arithmetic–geometric mean [duplicate],Question about arithmetic–geometric mean [duplicate],,This question already has answers here : prove $\sqrt{a_n b_n}$ and $\frac{1}{2}(a_n+b_n)$ have same limit (4 answers) Closed 5 years ago . We have two sequences: $$a_{n+1}=\sqrt{a_nb_n}$$ $$b_{n+1}=\frac{a_n+b_n}{2}$$ I need to prove that those are making Cantor's Lemma.(At the end I shold get that: $\lim_{n\to \infty}a_n=\lim_{n\to \infty}b_n$ by Cantor's Lemma) Any ideas how? Thank you.,This question already has answers here : prove $\sqrt{a_n b_n}$ and $\frac{1}{2}(a_n+b_n)$ have same limit (4 answers) Closed 5 years ago . We have two sequences: $$a_{n+1}=\sqrt{a_nb_n}$$ $$b_{n+1}=\frac{a_n+b_n}{2}$$ I need to prove that those are making Cantor's Lemma.(At the end I shold get that: $\lim_{n\to \infty}a_n=\lim_{n\to \infty}b_n$ by Cantor's Lemma) Any ideas how? Thank you.,,"['calculus', 'sequences-and-series', 'limits', 'means']"
69,Limit of sums is sum of limits in a metric space,Limit of sums is sum of limits in a metric space,,"So I'm aware that in a normed space, the limit of the sums is the sum of the limits: For normed space $(X, ||.||)$, if $x_n \rightarrow a$ and $y_n \rightarrow b$, then $(x_n + y_n) \rightarrow (a+b)$ But is this true in a general metric space $(X,d)$? The reason I can prove the above statement is that $||x_n + y_n|| - ||a+b|| \leq ||x_n + y_n - a - b|| < 2\epsilon.$ But is it similarly true in metric spaces that $d(x_n + y_n, a+b) \leq d(x_n,a) + d(x_n,b)$? Is that clear? Thanks a bunch!","So I'm aware that in a normed space, the limit of the sums is the sum of the limits: For normed space $(X, ||.||)$, if $x_n \rightarrow a$ and $y_n \rightarrow b$, then $(x_n + y_n) \rightarrow (a+b)$ But is this true in a general metric space $(X,d)$? The reason I can prove the above statement is that $||x_n + y_n|| - ||a+b|| \leq ||x_n + y_n - a - b|| < 2\epsilon.$ But is it similarly true in metric spaces that $d(x_n + y_n, a+b) \leq d(x_n,a) + d(x_n,b)$? Is that clear? Thanks a bunch!",,"['analysis', 'limits', 'metric-spaces']"
70,Limit of product with unbounded sequence $\lim_{n\to\infty} \sqrt{n}(\sqrt[n]{n}-1)=0$ [duplicate],Limit of product with unbounded sequence  [duplicate],\lim_{n\to\infty} \sqrt{n}(\sqrt[n]{n}-1)=0,"This question already has answers here : How to prove $\lim_{n \to \infty} \sqrt{n}(\sqrt[n]{n} - 1) = 0$? (4 answers) Closed 7 years ago . I have to show that $\lim_{n\to\infty} \sqrt{n}(\sqrt[n]{n}-1)=0$ We proofed that $\lim_{n\to\infty} \sqrt[n]{n}=1$ My problem is, that I do not know how to solve that. That $\lim_{n\to\infty}(\sqrt[n]{n}-1)=0$ is clear. But $\lim_{n\to\infty} \sqrt{n}$ is not bounded. So I can not simply calculate: $\lim_{n\to\infty}\sqrt{n}\cdot \lim_{n\to\infty}(\sqrt[n]{n}-1)$ right? I would be thankfull for every hint. :-)","This question already has answers here : How to prove $\lim_{n \to \infty} \sqrt{n}(\sqrt[n]{n} - 1) = 0$? (4 answers) Closed 7 years ago . I have to show that $\lim_{n\to\infty} \sqrt{n}(\sqrt[n]{n}-1)=0$ We proofed that $\lim_{n\to\infty} \sqrt[n]{n}=1$ My problem is, that I do not know how to solve that. That $\lim_{n\to\infty}(\sqrt[n]{n}-1)=0$ is clear. But $\lim_{n\to\infty} \sqrt{n}$ is not bounded. So I can not simply calculate: $\lim_{n\to\infty}\sqrt{n}\cdot \lim_{n\to\infty}(\sqrt[n]{n}-1)$ right? I would be thankfull for every hint. :-)",,"['limits', 'radicals']"
71,Show $\lim\limits_{n\to\infty} \frac{2n^2-3}{3n^ 2+2n-1}=\frac23$ Using Formal Definition of Limit,Show  Using Formal Definition of Limit,\lim\limits_{n\to\infty} \frac{2n^2-3}{3n^ 2+2n-1}=\frac23,I want to show that $a_n=\frac{2n^2-3}{3n^ 2+2n-1}$ is convergent. So I did the following: \begin{align*} \left|a_n-\frac23\right|&=\left|\frac{2n^2-3}{3n^ 2+2n-1}-\frac23\right|\\ &=\left|\frac{-4n-7}{3(3n^2+2n-1)}\right| \\ &<\left|\frac{4n}{3n^2}\right|\tag{$\ast$}\\ &<\left|\frac4n\right|\\ &<\frac4N\\\ \end{align*} But I am not one hundred percent sure about ($\ast$) because $|-4n-7|=|4n+7|\not<4n$. Can somebody please explain my error in reasoning?,I want to show that $a_n=\frac{2n^2-3}{3n^ 2+2n-1}$ is convergent. So I did the following: \begin{align*} \left|a_n-\frac23\right|&=\left|\frac{2n^2-3}{3n^ 2+2n-1}-\frac23\right|\\ &=\left|\frac{-4n-7}{3(3n^2+2n-1)}\right| \\ &<\left|\frac{4n}{3n^2}\right|\tag{$\ast$}\\ &<\left|\frac4n\right|\\ &<\frac4N\\\ \end{align*} But I am not one hundred percent sure about ($\ast$) because $|-4n-7|=|4n+7|\not<4n$. Can somebody please explain my error in reasoning?,,['calculus']
72,Using the Squeeze Theorem in Sequences,Using the Squeeze Theorem in Sequences,,"My textbook has an example that says ""Show that the sequence {${c_n}$} $= (-1)^n \frac{1}{n!} $ "" converges, and find its limit. It tells me that I must ""find two convergent sequences that can be related to the given sequence"" which the textbook states that the two possibilities are $a_n = \frac{-1}{2^n}$ and $b_n = \frac{1}{2^n}$. My question is how did they find $a_n$ and $b_n$? Is there a way to find them algebraically since my text doesn't show or explain the process to do so?","My textbook has an example that says ""Show that the sequence {${c_n}$} $= (-1)^n \frac{1}{n!} $ "" converges, and find its limit. It tells me that I must ""find two convergent sequences that can be related to the given sequence"" which the textbook states that the two possibilities are $a_n = \frac{-1}{2^n}$ and $b_n = \frac{1}{2^n}$. My question is how did they find $a_n$ and $b_n$? Is there a way to find them algebraically since my text doesn't show or explain the process to do so?",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence', 'factorial']"
73,Is this a correct way to prove uniqueness using limits?,Is this a correct way to prove uniqueness using limits?,,"I have a question about the following proof: Claim: A sequence in $\mathbb{R}$ can have at most one limit. Proof: Assume a sequence $X = (x_n)$ has two limits. Call them $x$ and $x'$ . For any $\epsilon > 0$ , there exists $N$ such that $|x_n - x| < \epsilon/2$ for $n \geq N$ . There also exists $N'$ such that $|x_{n'}- x'| < \epsilon/2$ for $n' \geq N'$ . Let $M = $ max $(N, N')$ . Then for $m \geq M$ : $|x-x'| = |(x - x_m) + (x_m - x')| \leq |x_m - x| + |x_m - x'| < \epsilon/2 + \epsilon/2= \epsilon$ . Since $\epsilon$ is arbitrary, we conclude $x = x'$ . My question: is it necessary to split $\epsilon$ at all? Do we do this just because we want a clean looking proof? Here is the alternative proof which I had in mind, which may or may not be correct: Assume a sequence $X = (x_n)$ has two limits. Call them $x$ and $x'$ . For any $\epsilon > 0$ , there exists $N$ such that $|x_n - x| < \epsilon$ for $n \geq N$ . For $\epsilon_2 > 0$ , there exists $N'$ such that $|x_{n'}- x'| < \epsilon_2$ for $n' \geq N'$ . Let $M = $ max $(N, N')$ . Then for $m \geq M$ : $|x-x'| = |(x - x_m) + (x_m - x')| \leq |x_m - x| + |x_m -x'| < \epsilon + \epsilon_2= \epsilon_3$ . $\epsilon_3$ is just another positive real number. I can make $\epsilon_3$ as small as I like because I can make $\epsilon$ and $\epsilon_2$ as small as I like. So I draw the same conclusion. Is the last bit of reasoning valid? I've gotten the impression, from speaking with a professor, that I introduce a dependency on $\epsilon$ and $\epsilon_2$ , so I should be able to come up with some method of getting $\epsilon$ and $\epsilon_2$ . I'm confused. (the angle brackets from the blockquote seem to be screwing up the formatting.)","I have a question about the following proof: Claim: A sequence in can have at most one limit. Proof: Assume a sequence has two limits. Call them and . For any , there exists such that for . There also exists such that for . Let max . Then for : . Since is arbitrary, we conclude . My question: is it necessary to split at all? Do we do this just because we want a clean looking proof? Here is the alternative proof which I had in mind, which may or may not be correct: Assume a sequence has two limits. Call them and . For any , there exists such that for . For , there exists such that for . Let max . Then for : . is just another positive real number. I can make as small as I like because I can make and as small as I like. So I draw the same conclusion. Is the last bit of reasoning valid? I've gotten the impression, from speaking with a professor, that I introduce a dependency on and , so I should be able to come up with some method of getting and . I'm confused. (the angle brackets from the blockquote seem to be screwing up the formatting.)","\mathbb{R} X = (x_n) x x' \epsilon > 0 N |x_n - x| < \epsilon/2 n \geq N N' |x_{n'}- x'| < \epsilon/2 n' \geq N' M =  (N, N') m \geq M |x-x'| = |(x - x_m) + (x_m - x')| \leq |x_m - x| + |x_m - x'| < \epsilon/2 + \epsilon/2= \epsilon \epsilon x = x' \epsilon X = (x_n) x x' \epsilon > 0 N |x_n - x| < \epsilon n \geq N \epsilon_2 > 0 N' |x_{n'}- x'| < \epsilon_2 n' \geq N' M =  (N, N') m \geq M |x-x'| = |(x - x_m) + (x_m - x')| \leq |x_m - x| + |x_m -x'| < \epsilon + \epsilon_2= \epsilon_3 \epsilon_3 \epsilon_3 \epsilon \epsilon_2 \epsilon \epsilon_2 \epsilon \epsilon_2","['real-analysis', 'limits']"
74,How Many Subsequential Limits Does the Sum of two Bounded Sequences Have?,How Many Subsequential Limits Does the Sum of two Bounded Sequences Have?,,"I ask for some help or hint how to deal with this question: Suppose $a_n$,$b_n$ were bounded sequences,    $a_n$ has $k$ sub-sequential limits and $b_n$ has $m$ sub-sequential limits. Prove or provide a counterexample :   $a_n$+$b_n$ has at most $km$ sub-sequential limits. I didn't find any counterexample, but  I didn't find a right approach to prove this statement also. Probably I didn't catch the meaning of the sub-sequential limits concept, so any links to right direction will be welcome. Thanks.","I ask for some help or hint how to deal with this question: Suppose $a_n$,$b_n$ were bounded sequences,    $a_n$ has $k$ sub-sequential limits and $b_n$ has $m$ sub-sequential limits. Prove or provide a counterexample :   $a_n$+$b_n$ has at most $km$ sub-sequential limits. I didn't find any counterexample, but  I didn't find a right approach to prove this statement also. Probably I didn't catch the meaning of the sub-sequential limits concept, so any links to right direction will be welcome. Thanks.",,"['real-analysis', 'sequences-and-series', 'limits']"
75,Find an example (Limits),Find an example (Limits),,"I was asked about this problem Find a function $f:\mathbb{R}\rightarrow\mathbb{R}$ such as the limit as $x\to0$ of $f$ doesn't exist, and $\lim\limits_{x\to0}f(x)\cdot f(2x) = 0$ I think is really interesting and I would like to know what will be the idea to came out with a solution.","I was asked about this problem Find a function $f:\mathbb{R}\rightarrow\mathbb{R}$ such as the limit as $x\to0$ of $f$ doesn't exist, and $\lim\limits_{x\to0}f(x)\cdot f(2x) = 0$ I think is really interesting and I would like to know what will be the idea to came out with a solution.",,"['calculus', 'limits']"
76,Indeterminate form from calculus [closed],Indeterminate form from calculus [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question What do we mean by Indeterminate form  ? can we show $0/0$ anything as we wish  i mean is it not unique  there can be several answers for it,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question What do we mean by Indeterminate form  ? can we show $0/0$ anything as we wish  i mean is it not unique  there can be several answers for it,,"['calculus', 'limits']"
77,"Multivariate limit $\lim_{(x,y)\rightarrow (0,0)} \frac{x^{2}\sin^2y}{x^{2}+6y^{2}}$",Multivariate limit,"\lim_{(x,y)\rightarrow (0,0)} \frac{x^{2}\sin^2y}{x^{2}+6y^{2}}","So I have solved for the $x$-axis and have gotten $0$ and along the $y$-axis and gotten $\dfrac 16$. However Webassign doesn't seem to like the fact that the limit does not exist. Which leads me to presume the limit does exist. Can someone give me a hint on where to proceed from here. Thanks! $$\lim_{(x,y)\rightarrow (0,0)} \frac{x^{2}\sin^{2}(y)}{x^{2}+6y^{2}}$$","So I have solved for the $x$-axis and have gotten $0$ and along the $y$-axis and gotten $\dfrac 16$. However Webassign doesn't seem to like the fact that the limit does not exist. Which leads me to presume the limit does exist. Can someone give me a hint on where to proceed from here. Thanks! $$\lim_{(x,y)\rightarrow (0,0)} \frac{x^{2}\sin^{2}(y)}{x^{2}+6y^{2}}$$",,"['limits', 'multivariable-calculus']"
78,How to show $\lim_{p\to \infty} x_p=1$,How to show,\lim_{p\to \infty} x_p=1,"Let $f$ be nonnegative, continuous, and strictly increasing on $[0,1]$. For $p >0$, let $x_p$ be the number in $(0,1)$ which satisfies $f^p(x_p) = \int^1_0 f^p(x)dx$. Find $\lim_{p\to \infty} x_p=?$ I tried letting $f(x)=x$, $x^2$, $x^n$, etc. The limit seems converging to 1. However, for any other functions, how can I show this limit is 1? Thanks a lot, -Belen","Let $f$ be nonnegative, continuous, and strictly increasing on $[0,1]$. For $p >0$, let $x_p$ be the number in $(0,1)$ which satisfies $f^p(x_p) = \int^1_0 f^p(x)dx$. Find $\lim_{p\to \infty} x_p=?$ I tried letting $f(x)=x$, $x^2$, $x^n$, etc. The limit seems converging to 1. However, for any other functions, how can I show this limit is 1? Thanks a lot, -Belen",,"['calculus', 'real-analysis', 'limits']"
79,Commuting an $\int$ improper at its both ends and $\lim$,Commuting an  improper at its both ends and,\int \lim,"I am working on the following problem: Let $f, g$ be continuous nonnegative functions defined and improperly-integrable on $(0, \infty)$ Furthermore, assume they satisfy   $$ \lim_{x\rightarrow 0}f(x) = 0 \wedge \lim_{x\rightarrow\infty}xg(x)=0. $$   Then prove that   $$ \lim_{n\rightarrow\infty}n\int_0^\infty f(x)g(nx)dx = 0. $$ I tried to swap $\lim$ and $\int$.  I know that if the improper integral and its integrand converge uniformly, they commute.  But I am at a loss as to how to prove this condition. I would be grateful if you could help me in this regard.","I am working on the following problem: Let $f, g$ be continuous nonnegative functions defined and improperly-integrable on $(0, \infty)$ Furthermore, assume they satisfy   $$ \lim_{x\rightarrow 0}f(x) = 0 \wedge \lim_{x\rightarrow\infty}xg(x)=0. $$   Then prove that   $$ \lim_{n\rightarrow\infty}n\int_0^\infty f(x)g(nx)dx = 0. $$ I tried to swap $\lim$ and $\int$.  I know that if the improper integral and its integrand converge uniformly, they commute.  But I am at a loss as to how to prove this condition. I would be grateful if you could help me in this regard.",,"['calculus', 'integration', 'limits', 'improper-integrals']"
80,Calculating derivative by definition vs not by definition,Calculating derivative by definition vs not by definition,,"I'm not entirely sure I understand when I need to calculate a derivative using the definition and when I can do it normally.  The following two examples confused me: $$ g(x) = \begin{cases} x^2\cdot \sin(\frac {1}{x}) & x \neq 0 \\ 0 & x=0 \end{cases} $$ $$ f(x) = \begin{cases} e^{\frac {-1}{x}} & x > 0 \\ -x^2 & x\leq 0 \end{cases} $$ I understand that I can differentiate normally for any $x$ that's not 0 (in both of these examples).  I'm confused because I saw an example with $f(x)$ in which they calculated the $f'(x)$ by differentiating: $$ f'(x) = \begin{cases} \frac {1}{x^2}\cdot e^{\frac {-1}{x}}  & x > 0 \\ -2x & x\leq0 \end{cases} $$ and then they calculated $f'(0)$ not using the definition but by $ \lim_{x\to0^-} f'(x)$ $\lim_{x\to0^+} f'(x)$ For $g(x)$ though I know that $g'(0)$ exists (using definition)  but $$g'(x) = \begin{cases} 2x\cdot \sin(\frac {1}{x})-\cos(\frac {1}{x})   & x \ne 0 \\ 0 & x=0 \end{cases}$$ and you can't calculate $\lim_{x\to0^-} g'(x)$ or $\lim_{x\to0^+} g'(x)$ since $\lim_{x\to0^{+/-}} 2x\cdot \sin(\frac {1}{x})-\cos(\frac {1}{x}) $  doesn't exist. So what's the difference between these two? When can I just differentiate normally like in the first example ($f(x)$) and when do I have to use the definition like in the second example ($g(x)$)? I'd appreciate the help. Edit: When I""m referring to the definition I'm referring to the following: $\lim_{h \rightarrow 0} \dfrac{f(x+h) - f(x)}{h}$","I'm not entirely sure I understand when I need to calculate a derivative using the definition and when I can do it normally.  The following two examples confused me: $$ g(x) = \begin{cases} x^2\cdot \sin(\frac {1}{x}) & x \neq 0 \\ 0 & x=0 \end{cases} $$ $$ f(x) = \begin{cases} e^{\frac {-1}{x}} & x > 0 \\ -x^2 & x\leq 0 \end{cases} $$ I understand that I can differentiate normally for any $x$ that's not 0 (in both of these examples).  I'm confused because I saw an example with $f(x)$ in which they calculated the $f'(x)$ by differentiating: $$ f'(x) = \begin{cases} \frac {1}{x^2}\cdot e^{\frac {-1}{x}}  & x > 0 \\ -2x & x\leq0 \end{cases} $$ and then they calculated $f'(0)$ not using the definition but by $ \lim_{x\to0^-} f'(x)$ $\lim_{x\to0^+} f'(x)$ For $g(x)$ though I know that $g'(0)$ exists (using definition)  but $$g'(x) = \begin{cases} 2x\cdot \sin(\frac {1}{x})-\cos(\frac {1}{x})   & x \ne 0 \\ 0 & x=0 \end{cases}$$ and you can't calculate $\lim_{x\to0^-} g'(x)$ or $\lim_{x\to0^+} g'(x)$ since $\lim_{x\to0^{+/-}} 2x\cdot \sin(\frac {1}{x})-\cos(\frac {1}{x}) $  doesn't exist. So what's the difference between these two? When can I just differentiate normally like in the first example ($f(x)$) and when do I have to use the definition like in the second example ($g(x)$)? I'd appreciate the help. Edit: When I""m referring to the definition I'm referring to the following: $\lim_{h \rightarrow 0} \dfrac{f(x+h) - f(x)}{h}$",,"['calculus', 'limits', 'derivatives', 'definition']"
81,When is limit substitution valid?,When is limit substitution valid?,,"This question asks to find the $\lim_{x\to0}(x\tan x)^x$ . Ron Gordon and Maisam Hedyelloo make the substitution $ x\sim \tan x$ , and it works and they get the correct answer. However, if you try to make the substitution $x \sim \arcsin x$ into $\lim_{x \to 0} \large \frac {\arcsin(x)-x}{x^3}$ you get the wrong result of $0$, when in reality the limit is equal to $\frac 16$ . So when can you use this kind of substitution? Thanks. P.S. I have asked a similar question here , and I thought I had the answer to the question but now I see that it is not complete.","This question asks to find the $\lim_{x\to0}(x\tan x)^x$ . Ron Gordon and Maisam Hedyelloo make the substitution $ x\sim \tan x$ , and it works and they get the correct answer. However, if you try to make the substitution $x \sim \arcsin x$ into $\lim_{x \to 0} \large \frac {\arcsin(x)-x}{x^3}$ you get the wrong result of $0$, when in reality the limit is equal to $\frac 16$ . So when can you use this kind of substitution? Thanks. P.S. I have asked a similar question here , and I thought I had the answer to the question but now I see that it is not complete.",,"['calculus', 'algebra-precalculus', 'limits']"
82,Proof of a Property of Vertical Asymptotes,Proof of a Property of Vertical Asymptotes,,"I'm trying to understand a proof in my Calculus textbook of the following theorem: Let the functions $f$ and $g$ be continuous on an interval containing   $c$. If $f(c) \neq 0$, $g(c) = 0$, and there is an open interval   containing $c$ such that $g(x) \neq 0$ for all $x \neq c$ in the   interval, then the graph of the function $h(x) = \frac{f(x)}{g(x)}$   has a vertical asymptote at $x = c$. The proof the textbook provides is the following: Consider the case for which $f(c) > 0$, and there exists $b > c :$    $c < x < b \implies g(x) > 0$. For $M > 0$, choose $\delta_{1}$   and $\delta_{2}$ such that: $0<x-c<\delta_1 \implies \frac{f(c)}{2} <f(x)<\frac{3f(c)}{2}$ $0<x-c<\delta_2 \implies 0 < g(x) < \frac{f(c)}{2M}$ Set $\delta = min\{\delta_1, \delta_2\}$. Then $0 < x-c < \delta \implies \frac{f(x)}{g(x)} > \frac{f(c)}{2} \cdot \frac{2M}{f(c)}= > M$, which implies that $\lim_{x \to c^+}\frac{f(x)}{g(x)} = \infty$,   so $x = c$ is a vertical asymptote of the graph of $h(x)$. What I'm wondering is what the point is of Saying that there exists $b > c : c < x <  b \implies g(x) > 0$ Restricting $f(x) < \frac{3f(x)}{2}$ The way I understand the proof, $\frac{f(c)}{2}$ and $\frac{f(c)}{2M}$ were simply chosen because they cancel nicely when multiplied, correct? But what advantage comes from the other two restrictions I listed? Any help or explanation would be appreciated, thanks.","I'm trying to understand a proof in my Calculus textbook of the following theorem: Let the functions $f$ and $g$ be continuous on an interval containing   $c$. If $f(c) \neq 0$, $g(c) = 0$, and there is an open interval   containing $c$ such that $g(x) \neq 0$ for all $x \neq c$ in the   interval, then the graph of the function $h(x) = \frac{f(x)}{g(x)}$   has a vertical asymptote at $x = c$. The proof the textbook provides is the following: Consider the case for which $f(c) > 0$, and there exists $b > c :$    $c < x < b \implies g(x) > 0$. For $M > 0$, choose $\delta_{1}$   and $\delta_{2}$ such that: $0<x-c<\delta_1 \implies \frac{f(c)}{2} <f(x)<\frac{3f(c)}{2}$ $0<x-c<\delta_2 \implies 0 < g(x) < \frac{f(c)}{2M}$ Set $\delta = min\{\delta_1, \delta_2\}$. Then $0 < x-c < \delta \implies \frac{f(x)}{g(x)} > \frac{f(c)}{2} \cdot \frac{2M}{f(c)}= > M$, which implies that $\lim_{x \to c^+}\frac{f(x)}{g(x)} = \infty$,   so $x = c$ is a vertical asymptote of the graph of $h(x)$. What I'm wondering is what the point is of Saying that there exists $b > c : c < x <  b \implies g(x) > 0$ Restricting $f(x) < \frac{3f(x)}{2}$ The way I understand the proof, $\frac{f(c)}{2}$ and $\frac{f(c)}{2M}$ were simply chosen because they cancel nicely when multiplied, correct? But what advantage comes from the other two restrictions I listed? Any help or explanation would be appreciated, thanks.",,['calculus']
83,Compute this limit $\lim_{x\to0}\frac{\sin(x^2+\frac{1}{x})-\sin\frac{1}{x}}{x}$ using L'Hôpital's rule,Compute this limit  using L'Hôpital's rule,\lim_{x\to0}\frac{\sin(x^2+\frac{1}{x})-\sin\frac{1}{x}}{x},"I have asked this problem before , but I can't understand the explanation, I couldn't understand how the sin multiply for cos, and too multiply for A + and - B: $$\sin(A)-\sin(B)=2\sin\left(\frac{A-B}{2}\right)\cos\left(\frac{A+B}{2}\right)$$ and I don't understand in this step how/why the $A-B$ and $A+B$ was replaced by $\frac{x^2}{2}$ and $\frac{x^2}{2}+\frac{1}{x}$ : $$\lim_{x\to0}\frac{\sin\left(x^2+\frac1x\right)-\sin\left(\frac1x\right)}{x}= \lim_{x\to0}\frac{2\sin\left(\frac{x^2}{2}\right)\cos\left(\frac{x^2}{2}+\frac1x\right)}{x}.$$","I have asked this problem before , but I can't understand the explanation, I couldn't understand how the sin multiply for cos, and too multiply for A + and - B: and I don't understand in this step how/why the and was replaced by and :",\sin(A)-\sin(B)=2\sin\left(\frac{A-B}{2}\right)\cos\left(\frac{A+B}{2}\right) A-B A+B \frac{x^2}{2} \frac{x^2}{2}+\frac{1}{x} \lim_{x\to0}\frac{\sin\left(x^2+\frac1x\right)-\sin\left(\frac1x\right)}{x}= \lim_{x\to0}\frac{2\sin\left(\frac{x^2}{2}\right)\cos\left(\frac{x^2}{2}+\frac1x\right)}{x}.,"['calculus', 'limits']"
84,Prove that $\sin^2{x}+\sin{x^2}$ isn't periodic by using uniform continuity,Prove that  isn't periodic by using uniform continuity,\sin^2{x}+\sin{x^2},Before the problem is a proof that says a periodic function whose domain is $\mathbb{R}$ is uniformly continuous.So actually the problem is to prove $\sin^2{x}+\sin{x^2}$ isn't uniformly continuous.I hope to fellow the problem.Thanks for the zealous!,Before the problem is a proof that says a periodic function whose domain is $\mathbb{R}$ is uniformly continuous.So actually the problem is to prove $\sin^2{x}+\sin{x^2}$ isn't uniformly continuous.I hope to fellow the problem.Thanks for the zealous!,,['limits']
85,How to calculate $\lim_{x\to 1}\int_{0}^{1}\frac{dy}{\sqrt{1-y^{2}}}\frac{y^{3/2}}{\sqrt{x - y}}$ when $x>1$?,How to calculate  when ?,\lim_{x\to 1}\int_{0}^{1}\frac{dy}{\sqrt{1-y^{2}}}\frac{y^{3/2}}{\sqrt{x - y}} x>1,"Numerically, it looks that the limit is $$\lim_{x\to 1}\int_{0}^{1}\frac{dy}{\sqrt{1-y^{2}}}\frac{y^{3/2}}{\sqrt{x - y}} = \frac{1}{\sqrt{2}}\log(1 - x) + cte $$, but I have not been able to demonstrate it analytically. Does anyone have a idea on how to deal with this limit?","Numerically, it looks that the limit is $$\lim_{x\to 1}\int_{0}^{1}\frac{dy}{\sqrt{1-y^{2}}}\frac{y^{3/2}}{\sqrt{x - y}} = \frac{1}{\sqrt{2}}\log(1 - x) + cte $$, but I have not been able to demonstrate it analytically. Does anyone have a idea on how to deal with this limit?",,"['real-analysis', 'limits', 'definite-integrals']"
86,Solve limits in Lebesgue integral,Solve limits in Lebesgue integral,,"Solve the limits of below: (1) $\lim\limits_{n \to  \infty} \int_0^n (1+\frac{x}{n})^n e^{-2x}dx$. (2) $\lim\limits_{n \to \infty} \int_0^n (1-\frac{x}{n})^n e^{\frac{x}{2}}dx$. (3) $\int_0^{\infty} \frac{e^{-x}\sin^2 x}{x}dx$ (Hint:  show $f(x,y)=e^{-x}\sin2xy$ is integrable on $[0,\infty) \times [0,1]$)","Solve the limits of below: (1) $\lim\limits_{n \to  \infty} \int_0^n (1+\frac{x}{n})^n e^{-2x}dx$. (2) $\lim\limits_{n \to \infty} \int_0^n (1-\frac{x}{n})^n e^{\frac{x}{2}}dx$. (3) $\int_0^{\infty} \frac{e^{-x}\sin^2 x}{x}dx$ (Hint:  show $f(x,y)=e^{-x}\sin2xy$ is integrable on $[0,\infty) \times [0,1]$)",,"['sequences-and-series', 'limits', 'lebesgue-integral']"
87,How to show limit definition of $e^{z}$ holds if $z \in \mathbb{C}$,How to show limit definition of  holds if,e^{z} z \in \mathbb{C},"It is well known that for $x\in \mathbb{R}$ we have $$ e^{x} = \lim_{n \to \infty} \left(1+  \frac{x}{n}\right)^n. $$ This follows quickly by considering logarithms and using L'Hospital's rule. However, for $z \in \mathbb{C}$ this would involve taking complex logarithms. I am not convinced that this proof still works and I was wondering if there is another simple proof of this fact when $z \in \mathbb{C}$. EDIT: For definition I am using the Taylor series $$e^z := \sum_{k=0}^\infty \frac{z^k}{k!}.$$ I am okay with all the standard alternative definitions and properties of $e^x$ when $x \in \mathbb{R}$, so feel free to use those if the real case can somehow be extended to the complex case.","It is well known that for $x\in \mathbb{R}$ we have $$ e^{x} = \lim_{n \to \infty} \left(1+  \frac{x}{n}\right)^n. $$ This follows quickly by considering logarithms and using L'Hospital's rule. However, for $z \in \mathbb{C}$ this would involve taking complex logarithms. I am not convinced that this proof still works and I was wondering if there is another simple proof of this fact when $z \in \mathbb{C}$. EDIT: For definition I am using the Taylor series $$e^z := \sum_{k=0}^\infty \frac{z^k}{k!}.$$ I am okay with all the standard alternative definitions and properties of $e^x$ when $x \in \mathbb{R}$, so feel free to use those if the real case can somehow be extended to the complex case.",,"['real-analysis', 'limits', 'exponential-function']"
88,Question about interchanging limits,Question about interchanging limits,,"For a function $f\colon\mathbb{R}^2 \rightarrow \mathbb{R}$ and $a,b\in \mathbb{R}$, if $\lim\limits_{x \rightarrow a}{f(x,y)}$ and $\lim\limits_{y \rightarrow b}{f(x,y)}$ exist and $\lim\limits_{(x,y) \rightarrow (a,b)}{f(x,y)}=L$, then  $$\lim\limits_{x \rightarrow a}\lim\limits_{y \rightarrow b}{f(x,y)}=\lim\limits_{y \rightarrow b}\lim\limits_{x \rightarrow a}{f(x,y)}=L.$$ Can you give me a hint please?","For a function $f\colon\mathbb{R}^2 \rightarrow \mathbb{R}$ and $a,b\in \mathbb{R}$, if $\lim\limits_{x \rightarrow a}{f(x,y)}$ and $\lim\limits_{y \rightarrow b}{f(x,y)}$ exist and $\lim\limits_{(x,y) \rightarrow (a,b)}{f(x,y)}=L$, then  $$\lim\limits_{x \rightarrow a}\lim\limits_{y \rightarrow b}{f(x,y)}=\lim\limits_{y \rightarrow b}\lim\limits_{x \rightarrow a}{f(x,y)}=L.$$ Can you give me a hint please?",,"['real-analysis', 'limits']"
89,Finding Value Of Infinite Sum,Finding Value Of Infinite Sum,,Let $x$ be a real number. Find $$\lim_{n\to\infty}\dfrac{\sum_{k=1}^{n}\sqrt[x+1]{1^{x}+2^{x}+\cdots +k^{x}}}{n^{2}}$$ It seems different from calculating it by transforming definite integral.,Let $x$ be a real number. Find $$\lim_{n\to\infty}\dfrac{\sum_{k=1}^{n}\sqrt[x+1]{1^{x}+2^{x}+\cdots +k^{x}}}{n^{2}}$$ It seems different from calculating it by transforming definite integral.,,"['real-analysis', 'sequences-and-series', 'limits']"
90,Limit of Fraction of Polynomial Proof,Limit of Fraction of Polynomial Proof,,Let $p$ be a polynomial of degree $\ge 1$.  I need help proving the following statement: $$\lim_{x \to \infty} \frac{1}{p(x)} = 0$$ I'm complete lost and don't know how to approach it.,Let $p$ be a polynomial of degree $\ge 1$.  I need help proving the following statement: $$\lim_{x \to \infty} \frac{1}{p(x)} = 0$$ I'm complete lost and don't know how to approach it.,,"['calculus', 'limits']"
91,$\lim_{x\to\infty} x^{1+1/x}-x-\log x$ and $\lim_{x\to\infty}\frac{x^{1+1/x}-x}{\log x}$,and,\lim_{x\to\infty} x^{1+1/x}-x-\log x \lim_{x\to\infty}\frac{x^{1+1/x}-x}{\log x},Evaluate  $$\lim_{x\to\infty} x^{1+1/x}-x-\log x$$  and  $$\lim_{x\to\infty}\frac{x^{1+1/x}-x}{\log x}$$ Would knowing one necessarily give the other?,Evaluate  $$\lim_{x\to\infty} x^{1+1/x}-x-\log x$$  and  $$\lim_{x\to\infty}\frac{x^{1+1/x}-x}{\log x}$$ Would knowing one necessarily give the other?,,"['calculus', 'real-analysis', 'analysis', 'limits', 'contest-math']"
92,$\lim_{x \to 0} \frac{f(x)-c}{g(x)}$ for $c \in \mathbb{R}$,for,\lim_{x \to 0} \frac{f(x)-c}{g(x)} c \in \mathbb{R},"My question involves evaluating the limit of the function: $\lim_{x \to 0} \frac{f(x)-c}{g(x)}$ Where: $\lim_{x \to 0} f(x) = 0$, $\lim_{x \to 0} g(x) = 0$, $f,g \in C^{1}$ and $c \in \mathbb{R}$ It seems I could just do: $\lim_{x \to 0} \frac{f(x)}{g(x)} - \lim_{x \to 0} \frac{c}{g(x)}$ I can apply L'Hospitals to the first one, and assuming f and g are nice enough, get a nice answer, but the second one gives me $\infty$ so the whole limit must be $\infty$ as well. However, intuitively, it doesn't seem like the constant $c$ should matter that much, and I should be apply to apply L'Hospitals without first breaking up the fraction since $c^{'} = 0$ anyway. However, the strict statements of the theorems won't allow this because $\lim_{x \to 0} f(x) - c = c \neq 0$. So, which/what is the correct way to evaluate this limit?","My question involves evaluating the limit of the function: $\lim_{x \to 0} \frac{f(x)-c}{g(x)}$ Where: $\lim_{x \to 0} f(x) = 0$, $\lim_{x \to 0} g(x) = 0$, $f,g \in C^{1}$ and $c \in \mathbb{R}$ It seems I could just do: $\lim_{x \to 0} \frac{f(x)}{g(x)} - \lim_{x \to 0} \frac{c}{g(x)}$ I can apply L'Hospitals to the first one, and assuming f and g are nice enough, get a nice answer, but the second one gives me $\infty$ so the whole limit must be $\infty$ as well. However, intuitively, it doesn't seem like the constant $c$ should matter that much, and I should be apply to apply L'Hospitals without first breaking up the fraction since $c^{'} = 0$ anyway. However, the strict statements of the theorems won't allow this because $\lim_{x \to 0} f(x) - c = c \neq 0$. So, which/what is the correct way to evaluate this limit?",,"['real-analysis', 'analysis', 'limits']"
93,Limit of an integral with a periodic function,Limit of an integral with a periodic function,,"Let $f , g$ be continuous functions: $f:[0 , 2\pi]\rightarrow\mathbb{R}$ and $g:\mathbb{R}\rightarrow\mathbb{R}$. Assume $\forall x\in\mathbb{R}:g(x+2\pi)=g(x)$ and $$\int\limits_{0}^{2\pi}  \! {g(x)} \, \mathrm{d}x=0.$$ Show that $$\lim\limits_{n\rightarrow\infty} \int\limits_{0}^{2\pi}  \! {f(x) g(nx)} \, \mathrm{d}x=0.$$ I've started evaluating the difference upwards but it seems like no use: $$\left| \int\limits_{0}^{2\pi}  \! {f(x) g(nx)} \, \mathrm{d}x-0 \right|=\left| \int\limits_{0}^{2\pi}  \! {f(x) g(nx)} \, \mathrm{d}x \right|\leq\int\limits_{0}^{2\pi}  \! {\left|f(x) g(nx)\right|} \, \mathrm{d}x=\int\limits_{0}^{2\pi}  \! {\left|f(x)\right| \left|g(nx)\right|} \, \mathrm{d}x$$ $$=\left|f(\xi)\right| \int\limits_{0}^{2\pi}  \! { \left|g(nx)\right|} \, \mathrm{d}x$$ for some $\xi \in ]0 , 2\pi[$. What's next?","Let $f , g$ be continuous functions: $f:[0 , 2\pi]\rightarrow\mathbb{R}$ and $g:\mathbb{R}\rightarrow\mathbb{R}$. Assume $\forall x\in\mathbb{R}:g(x+2\pi)=g(x)$ and $$\int\limits_{0}^{2\pi}  \! {g(x)} \, \mathrm{d}x=0.$$ Show that $$\lim\limits_{n\rightarrow\infty} \int\limits_{0}^{2\pi}  \! {f(x) g(nx)} \, \mathrm{d}x=0.$$ I've started evaluating the difference upwards but it seems like no use: $$\left| \int\limits_{0}^{2\pi}  \! {f(x) g(nx)} \, \mathrm{d}x-0 \right|=\left| \int\limits_{0}^{2\pi}  \! {f(x) g(nx)} \, \mathrm{d}x \right|\leq\int\limits_{0}^{2\pi}  \! {\left|f(x) g(nx)\right|} \, \mathrm{d}x=\int\limits_{0}^{2\pi}  \! {\left|f(x)\right| \left|g(nx)\right|} \, \mathrm{d}x$$ $$=\left|f(\xi)\right| \int\limits_{0}^{2\pi}  \! { \left|g(nx)\right|} \, \mathrm{d}x$$ for some $\xi \in ]0 , 2\pi[$. What's next?",,"['integration', 'limits', 'periodic-functions']"
94,Show that $(1+ \frac{1}{n})^n$ and $(1- \frac{1}{n})^{-n}$ have the same limit,Show that  and  have the same limit,(1+ \frac{1}{n})^n (1- \frac{1}{n})^{-n},"Let $x$ be positive and $$  a_n = \left( 1 + \frac{x}{n} \right)^n \qquad b_n = \left( 1 - \frac{x}{n} \right)^{-n}. $$ Show that a) The sequence $(a_n)$ and $(b_n)$ have the same limit $\xi =: \operatorname{Exp}(x)$. Use the following steps i) $a_n < b_n$ for all $n \in \mathbb{N}$ with $n > x$. ii) for $n > x$, $(a_n)$ is monotone increasing, and $(b_n)$ monotone decreasing. iii) $b_n - a_n \to 0$ for $n \to \infty$. b) For $y = -x < 0$ $$  \lim_{n\to \infty}\left( 1 + \frac{y}{n} \right)^n = \frac{1}{Exp(x)} $$ Item ii) is simple, because $0 < \frac{x}{n} < 1$, obvisouly $a_n$ increasing. But for the rest I have no idea...","Let $x$ be positive and $$  a_n = \left( 1 + \frac{x}{n} \right)^n \qquad b_n = \left( 1 - \frac{x}{n} \right)^{-n}. $$ Show that a) The sequence $(a_n)$ and $(b_n)$ have the same limit $\xi =: \operatorname{Exp}(x)$. Use the following steps i) $a_n < b_n$ for all $n \in \mathbb{N}$ with $n > x$. ii) for $n > x$, $(a_n)$ is monotone increasing, and $(b_n)$ monotone decreasing. iii) $b_n - a_n \to 0$ for $n \to \infty$. b) For $y = -x < 0$ $$  \lim_{n\to \infty}\left( 1 + \frac{y}{n} \right)^n = \frac{1}{Exp(x)} $$ Item ii) is simple, because $0 < \frac{x}{n} < 1$, obvisouly $a_n$ increasing. But for the rest I have no idea...",,"['sequences-and-series', 'analysis', 'limits']"
95,Prove an integral limit,Prove an integral limit,,"Let $F(x),G(x)\ge 0$ be decreasing functions on $[0,+\infty)$ and $\displaystyle\lim_{x\to+\infty}x(F(x)+G(x))=0$ (1) Prove that: $\forall\varepsilon>0,\displaystyle\lim_{x\rightarrow+\infty}\displaystyle\int_{\varepsilon}^{+\infty}xF(xt)\cos{t}dt=0$ (2) And have $\displaystyle\lim_{n\to+\infty}\displaystyle\int_{0}^{+\infty}(F(t)-G(t))\cos{\dfrac{t}{n}}dt=0$ prove that: $$\displaystyle\lim_{x\to 0}\displaystyle\int_{0}^{\infty}(F(t)-G(t))\cos{(xt)}dt=0$$ the simple problem you can see. the simple problem  see: http://sms.math.ecnu.edu.cn/contest/university03/mathclass11_answer.pdf this problem from: http://wenku.baidu.com/view/0ac7ae777fd5360cba1adb69.html","Let $F(x),G(x)\ge 0$ be decreasing functions on $[0,+\infty)$ and $\displaystyle\lim_{x\to+\infty}x(F(x)+G(x))=0$ (1) Prove that: $\forall\varepsilon>0,\displaystyle\lim_{x\rightarrow+\infty}\displaystyle\int_{\varepsilon}^{+\infty}xF(xt)\cos{t}dt=0$ (2) And have $\displaystyle\lim_{n\to+\infty}\displaystyle\int_{0}^{+\infty}(F(t)-G(t))\cos{\dfrac{t}{n}}dt=0$ prove that: $$\displaystyle\lim_{x\to 0}\displaystyle\int_{0}^{\infty}(F(t)-G(t))\cos{(xt)}dt=0$$ the simple problem you can see. the simple problem  see: http://sms.math.ecnu.edu.cn/contest/university03/mathclass11_answer.pdf this problem from: http://wenku.baidu.com/view/0ac7ae777fd5360cba1adb69.html",,"['limits', 'integration']"
96,Behaviour of $\frac{x^{3} y}{x^{6} + y^{2}}$ around origin,Behaviour of  around origin,\frac{x^{3} y}{x^{6} + y^{2}},"Lets study the limit $$ \lim_{(x,y)\to(0,0)} \frac{x^3 y}{x^6 + y^2} $$ If we look at the limit along any straight line eg $y = mx$ we find that the limit tends to $0$. Studying the limit closer we test for every curve $y = x^k$, where $k$ is some integer number.  This gives that for every $k\neq 3$, the limit tends to $0$. If  $k=3$, the limit tends to $1/2$. Now this shows that no matter what straigt line, or curve (except $y=x^3$) the limit tends to zero. I also tried a few other polynomials and they all tend to zero. My assumption is the following: The limit    $$ L = \lim_{(x,y)\to(0,0)} \frac{x^3 y}{x^6 + y^2} $$   Is equal to zero if $y$ is any polynomial except $y = m x^3\,,\ x \in \mathbb{R}$. Is the claim true? If so can anyone help proving it?","Lets study the limit $$ \lim_{(x,y)\to(0,0)} \frac{x^3 y}{x^6 + y^2} $$ If we look at the limit along any straight line eg $y = mx$ we find that the limit tends to $0$. Studying the limit closer we test for every curve $y = x^k$, where $k$ is some integer number.  This gives that for every $k\neq 3$, the limit tends to $0$. If  $k=3$, the limit tends to $1/2$. Now this shows that no matter what straigt line, or curve (except $y=x^3$) the limit tends to zero. I also tried a few other polynomials and they all tend to zero. My assumption is the following: The limit    $$ L = \lim_{(x,y)\to(0,0)} \frac{x^3 y}{x^6 + y^2} $$   Is equal to zero if $y$ is any polynomial except $y = m x^3\,,\ x \in \mathbb{R}$. Is the claim true? If so can anyone help proving it?",,"['limits', 'polynomials', 'multivariable-calculus']"
97,Finding the limit of a piecewise function,Finding the limit of a piecewise function,,"I had a question about finding limits of piecewise functions through graphs. I believe, I am missing something in my fundamentals about finding limits for these functions. Firstly, I would like to confirm that the empty dot represents a ""hole"" and the point is not included in the function of the line. For example, line closest to $h(x)$ is an example. Therefore, a coloured in dot represents a point that is on the graph e.g. $(3,2)$. So now for my main question How do I determine the limit of a $x$ that has two $y$'s? For example, the $\lim_{x\to 3^+} h(x)$ The back of the book says the answer is 3 but how can that be the case when, to my knowledge, an empty dot means that the point is hole? Or if that is not the case, then why is the answer not 2? If someone could please clear this up for me, that would be great. Thanks for the help! P.S I just decided to add the questions in there for reference, I am not asking for anyone to solve them and please don't. I prefer that you give me the tools to do it myself or some hints.","I had a question about finding limits of piecewise functions through graphs. I believe, I am missing something in my fundamentals about finding limits for these functions. Firstly, I would like to confirm that the empty dot represents a ""hole"" and the point is not included in the function of the line. For example, line closest to $h(x)$ is an example. Therefore, a coloured in dot represents a point that is on the graph e.g. $(3,2)$. So now for my main question How do I determine the limit of a $x$ that has two $y$'s? For example, the $\lim_{x\to 3^+} h(x)$ The back of the book says the answer is 3 but how can that be the case when, to my knowledge, an empty dot means that the point is hole? Or if that is not the case, then why is the answer not 2? If someone could please clear this up for me, that would be great. Thanks for the help! P.S I just decided to add the questions in there for reference, I am not asking for anyone to solve them and please don't. I prefer that you give me the tools to do it myself or some hints.",,"['calculus', 'limits']"
98,"How can I show that $f(x) = (x^2)/(1-e^x)$ has global minimum at $(0, +\infty)$?",How can I show that  has global minimum at ?,"f(x) = (x^2)/(1-e^x) (0, +\infty)",I showed that $\lim f(x) = 0$ at both the $0$ end and $+\infty$ end. What is the proper way to finish the proof?,I showed that $\lim f(x) = 0$ at both the $0$ end and $+\infty$ end. What is the proper way to finish the proof?,,"['limits', 'logarithms', 'infinitesimals']"
99,Limit of square root problem [closed],Limit of square root problem [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Can you please help me out with this limit problem. Actually, I tried to solve it by the conjugate method but it didn't work with me. Thank you. $$\lim_{x \to 0}\; \bigg( \frac{1}{\sqrt{x}} - \frac{1}{\sqrt{x²+x}} \bigg)$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Can you please help me out with this limit problem. Actually, I tried to solve it by the conjugate method but it didn't work with me. Thank you. $$\lim_{x \to 0}\; \bigg( \frac{1}{\sqrt{x}} - \frac{1}{\sqrt{x²+x}} \bigg)$$",,['limits']
