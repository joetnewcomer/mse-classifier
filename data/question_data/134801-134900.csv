,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to solve this differential equation $y(t) = y′(t)+ \frac{e^{2t}}{y'(t)}$,How to solve this differential equation,y(t) = y′(t)+ \frac{e^{2t}}{y'(t)},"I don't understand what a type of this equation and which method I could use for: $$y(t) = y′(t)+ \frac{e^{2t}}{y'(t)}$$ Please, help me.","I don't understand what a type of this equation and which method I could use for: $$y(t) = y′(t)+ \frac{e^{2t}}{y'(t)}$$ Please, help me.",,['ordinary-differential-equations']
1,Bessel's Differential Equation - textbook queries:,Bessel's Differential Equation - textbook queries:,,"In order to ask this question I must first give some background information as written in my text book: Given Bessel's Differential equation: $$x^2y^{\prime\prime}+xy^{\prime}+(x^2-p^2)y=0$$ or   $$x(xy^{\prime})^{\prime}+(x^2-p^2)y=0\tag{1}$$ where $p$ is a constant, but not necessarily an integer. We find a generalized power series for $(1)$ by writing only the general terms in the series for $y$ and the derivatives we need in $(1)$, we have   $$y=\sum_{n=0}^\infty a_nx^{n+s}$$   $$y^{\prime}=\sum_{n=0}^\infty a_n(n+s)x^{n+s-1}$$   $$xy^{\prime}=\sum_{n=0}^\infty a_n(n+s)x^{n+s}$$   $$(xy^{\prime})^{\prime}=\sum_{n=0}^\infty a_n(n+s)^2x^{n+s-1}$$   $$x(xy^{\prime})^{\prime}=\sum_{n=0}^\infty a_n(n+s)^2x^{n+s}$$   We now substitute the above into $(1)$ and tabulate the coefficients of powers of $x$: \begin{array}{|c|l:l|}\hline &  x^s & x^{s+1} & x^{s+2} & \cdots & x^{s+n} \\ \hline x(xy^{\prime})^{\prime} & s^2a_0 & (1+s)^2a_1 & (2+s)^2a_2 &  & (n+s)^2a_n  \\ x^2y &  &  & a_0 & & a_{n-2}  \\ -p^2y & -p^2a_0 & -p^2a_1 & -p^2a_2 & & -p^2a_n  \\  \hdashline  \hline \end{array} The coefficient of $x^s$ gives the indical equation and the values of $s$:   $s^2-p^2=0\implies s=\pm p$. The coefficient of $x^{s+1}$ gives $a_1=0$.    The coefficient of $x^{s+2}$ gives $a_2$ in terms of $a_0$, etc. But we may as well write the general formula from the last column at this point. We get:$$\left[(n+s)^2-p^2\right]a_n+a_{n-2}=0$$ or $$a_n=-\frac{a_{n-2}}{(n+s)^2-p^2}\tag{2}$$ First we shall find the coefficients for the case $s=p$. From $(2)$ we have $$a_n=-\frac{a_{n-2}}{(n+p)^2-p^2}=-\frac{a_{n-2}}{(n^2+2np)}=-\frac{a_{n-2}}{n(n+2p)}\tag{3}$$ Since $a_1=0$, all odd $a$'s are zero. For even $a$'s it is convenient to replace $n$ by $2n$; then from $(3)$ we have $$a_{2n}=-\frac{a_{2n-2}}{2n(2n+2p)}=-\frac{a_{n-2}}{2^2n(n+p)}\tag{4}$$ The formulas for the coefficients can be simplified by the use of the $\Gamma$ function (Gamma function) notation by recalling that $\Gamma(p+1)=p\Gamma(p)$ for any $p$ so $$\Gamma(p+2)=(p+1)\Gamma(p+1)$$   $$\Gamma(p+3)=(p+2)\Gamma(p+2)=(p+2)(p+1)\Gamma(p+1)$$   $$\Gamma(p+4)=(p+3)\Gamma(p+3)=(p+3)(p+2)(p+1)\Gamma(p+1)$$ and so on. Then from $(4)$ we find $$a_2=-\frac{a_0}{2^2(1+p)}=-\frac{a_0\Gamma(1+p)}{2^2\Gamma(2+p)}$$   $$a_4=-\frac{a_2}{2^3(2+p)}=\frac{a_0}{2!\cdot2^4(1+p)(2+p)}=\frac{a_0\Gamma(1+p)}{2!\cdot2^4\Gamma(3+p)}$$   $$a_6=-\frac{a_4}{3!\cdot2(3+p)}=-\frac{a_0}{3!\cdot2^6(1+p)(2+p)(3+p)}=-\frac{a_0\Gamma(1+p)}{3!\cdot2^6\Gamma(4+p)}$$ and so on. Then the series solution (for the $s=p$ case) is $$\begin{align}&y=a_0x^p\Gamma(1+p)\left[\frac{1}{0!\cdot\Gamma(1+p)}-\frac{1}{1!\cdot\Gamma(2+p)}\left(\frac{x}{2}\right)^2+\frac{1}{2!\cdot\Gamma(3+p)}\left(\frac{x}{2}\right)^4-\frac{1}{3!\cdot\Gamma(4+p)}\left(\frac{x}{2}\right)^6+\cdots\right] \\&= a_02^p\left(\frac{x}{2}\right)^p\Gamma(1+p)\left[\frac{1}{\Gamma(1)\Gamma(1+p)}-\frac{1}{\Gamma(2)\Gamma(2+p)}\left(\frac{x}{2}\right)^2+\frac{1}{\Gamma(3)\Gamma(3+p)}\left(\frac{x}{2}\right)^4-\frac{1}{\Gamma(4)\Gamma(4+p)}\left(\frac{x}{2}\right)^6+\cdots\right]\end{align}$$ Where we have inserted $\Gamma(1)$ and $\Gamma(2)$ (which are both equal to $1$) in the first $2$ terms and written $x^p=2^p\left(\dfrac{x}{2}\right)^p$ to make the series appear more systematic. If we take $$a_0=\frac{1}{2^p\Gamma(1+p)}=\frac{1}{2^pp!}$$ then $y$ is called the Bessel function of the first kind of order $p$, and written $J_p(x)$ Then $$J_p(x)=\frac{1}{\Gamma(1)\Gamma(1+p)}\left(\frac{x}{2}\right)^p-\frac{1}{\Gamma(2)\Gamma(2+p)}\left(\frac{x}{2}\right)^{2+p}+\frac{1}{\Gamma(3)\Gamma(3+p)}\left(\frac{x}{2}\right)^{4+p}-\frac{1}{\Gamma(4)\Gamma(4+p)}\left(\frac{x}{2}\right)^{6+p}+\cdots$$ or $$\fbox{$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1+p)}\left(\frac{x}{2}\right)^{2n+p}$}\tag{5}$$ We have found just one of the two solutions of Bessel's equation, that is, the one when $s=p$; we must next find the solution when $s=-p$. It is unnecessary to go through all the details again; we can just replace $p$ by $-p$ in $(5)$. In fact, the solution when $s=-p$ is usually written $J_{-p}(x)$. So from $(5)$ we have $$\fbox{$J_{-p}(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1-p)}\left(\frac{x}{2}\right)^{2n-p}$}\tag{6}$$ I understand all of the above. My question is regarding the following extract: If $p$ is not an integer $J_p(x)$ is a series starting with $x^p$ and $J_{-p}(x)$ is a series starting with $x^{-p}$. Then $J_p(x)$ and $J_{-p}(x)$ are two independent solutions and a linear combination of them is a general solution.    $\bbox[#AFF]{\text{But if }p\text{ is}\text{ an integer, then the first few terms in }{J_{-p}(x)}\text{ will be zero, as in the denominator }\,\Gamma(n-p+1)\text{ is }\Gamma \text{ of a negative number, which is infinite.}}$   $\bbox[#AFA]{\text{You can show that }J_{-p}(x)\text{ starts with the term }x^p\text{ for integer }p\text{ just as } J_{p}(x)\text{ does,}}$$\bbox[yellow]{\text{ and that }J_{-p}(x)=(-1)^pJ_p(x)\text{ for integer }p}$. For the blue highlighted part it says the ""first few terms in $J_{-p}(x)$ will be zero""; But how do they know this? This can only be true by my logic iff $p\gt 2$. Yet we are given no restriction on $p$ other than $p \gt 0$; Why is it only the first few (two) terms that are zero? For the green highlighted part; by my logic $J_{-p}(x)$ always starts with $x^{-p}$ and not $x^p$. What am I missing here? Lastly for the yellow part: I'm sorry but I have simply no idea how to show that $J_{-p}(x)=(-1)^pJ_p(x)$, and much to my annoyance this is the most important part of this post. I have already checked the errata list for this book and I can tell you that none of the above queries of mine are due to book errors. If anyone is able to provide me with some hints/advice on addressing some/all/any of the $3$ queries above I will be most grateful. I have only just started to learn about Bessel's equation, so my knowledge is somewhat limited (apologies). Kindest Regards.","In order to ask this question I must first give some background information as written in my text book: Given Bessel's Differential equation: $$x^2y^{\prime\prime}+xy^{\prime}+(x^2-p^2)y=0$$ or   $$x(xy^{\prime})^{\prime}+(x^2-p^2)y=0\tag{1}$$ where $p$ is a constant, but not necessarily an integer. We find a generalized power series for $(1)$ by writing only the general terms in the series for $y$ and the derivatives we need in $(1)$, we have   $$y=\sum_{n=0}^\infty a_nx^{n+s}$$   $$y^{\prime}=\sum_{n=0}^\infty a_n(n+s)x^{n+s-1}$$   $$xy^{\prime}=\sum_{n=0}^\infty a_n(n+s)x^{n+s}$$   $$(xy^{\prime})^{\prime}=\sum_{n=0}^\infty a_n(n+s)^2x^{n+s-1}$$   $$x(xy^{\prime})^{\prime}=\sum_{n=0}^\infty a_n(n+s)^2x^{n+s}$$   We now substitute the above into $(1)$ and tabulate the coefficients of powers of $x$: \begin{array}{|c|l:l|}\hline &  x^s & x^{s+1} & x^{s+2} & \cdots & x^{s+n} \\ \hline x(xy^{\prime})^{\prime} & s^2a_0 & (1+s)^2a_1 & (2+s)^2a_2 &  & (n+s)^2a_n  \\ x^2y &  &  & a_0 & & a_{n-2}  \\ -p^2y & -p^2a_0 & -p^2a_1 & -p^2a_2 & & -p^2a_n  \\  \hdashline  \hline \end{array} The coefficient of $x^s$ gives the indical equation and the values of $s$:   $s^2-p^2=0\implies s=\pm p$. The coefficient of $x^{s+1}$ gives $a_1=0$.    The coefficient of $x^{s+2}$ gives $a_2$ in terms of $a_0$, etc. But we may as well write the general formula from the last column at this point. We get:$$\left[(n+s)^2-p^2\right]a_n+a_{n-2}=0$$ or $$a_n=-\frac{a_{n-2}}{(n+s)^2-p^2}\tag{2}$$ First we shall find the coefficients for the case $s=p$. From $(2)$ we have $$a_n=-\frac{a_{n-2}}{(n+p)^2-p^2}=-\frac{a_{n-2}}{(n^2+2np)}=-\frac{a_{n-2}}{n(n+2p)}\tag{3}$$ Since $a_1=0$, all odd $a$'s are zero. For even $a$'s it is convenient to replace $n$ by $2n$; then from $(3)$ we have $$a_{2n}=-\frac{a_{2n-2}}{2n(2n+2p)}=-\frac{a_{n-2}}{2^2n(n+p)}\tag{4}$$ The formulas for the coefficients can be simplified by the use of the $\Gamma$ function (Gamma function) notation by recalling that $\Gamma(p+1)=p\Gamma(p)$ for any $p$ so $$\Gamma(p+2)=(p+1)\Gamma(p+1)$$   $$\Gamma(p+3)=(p+2)\Gamma(p+2)=(p+2)(p+1)\Gamma(p+1)$$   $$\Gamma(p+4)=(p+3)\Gamma(p+3)=(p+3)(p+2)(p+1)\Gamma(p+1)$$ and so on. Then from $(4)$ we find $$a_2=-\frac{a_0}{2^2(1+p)}=-\frac{a_0\Gamma(1+p)}{2^2\Gamma(2+p)}$$   $$a_4=-\frac{a_2}{2^3(2+p)}=\frac{a_0}{2!\cdot2^4(1+p)(2+p)}=\frac{a_0\Gamma(1+p)}{2!\cdot2^4\Gamma(3+p)}$$   $$a_6=-\frac{a_4}{3!\cdot2(3+p)}=-\frac{a_0}{3!\cdot2^6(1+p)(2+p)(3+p)}=-\frac{a_0\Gamma(1+p)}{3!\cdot2^6\Gamma(4+p)}$$ and so on. Then the series solution (for the $s=p$ case) is $$\begin{align}&y=a_0x^p\Gamma(1+p)\left[\frac{1}{0!\cdot\Gamma(1+p)}-\frac{1}{1!\cdot\Gamma(2+p)}\left(\frac{x}{2}\right)^2+\frac{1}{2!\cdot\Gamma(3+p)}\left(\frac{x}{2}\right)^4-\frac{1}{3!\cdot\Gamma(4+p)}\left(\frac{x}{2}\right)^6+\cdots\right] \\&= a_02^p\left(\frac{x}{2}\right)^p\Gamma(1+p)\left[\frac{1}{\Gamma(1)\Gamma(1+p)}-\frac{1}{\Gamma(2)\Gamma(2+p)}\left(\frac{x}{2}\right)^2+\frac{1}{\Gamma(3)\Gamma(3+p)}\left(\frac{x}{2}\right)^4-\frac{1}{\Gamma(4)\Gamma(4+p)}\left(\frac{x}{2}\right)^6+\cdots\right]\end{align}$$ Where we have inserted $\Gamma(1)$ and $\Gamma(2)$ (which are both equal to $1$) in the first $2$ terms and written $x^p=2^p\left(\dfrac{x}{2}\right)^p$ to make the series appear more systematic. If we take $$a_0=\frac{1}{2^p\Gamma(1+p)}=\frac{1}{2^pp!}$$ then $y$ is called the Bessel function of the first kind of order $p$, and written $J_p(x)$ Then $$J_p(x)=\frac{1}{\Gamma(1)\Gamma(1+p)}\left(\frac{x}{2}\right)^p-\frac{1}{\Gamma(2)\Gamma(2+p)}\left(\frac{x}{2}\right)^{2+p}+\frac{1}{\Gamma(3)\Gamma(3+p)}\left(\frac{x}{2}\right)^{4+p}-\frac{1}{\Gamma(4)\Gamma(4+p)}\left(\frac{x}{2}\right)^{6+p}+\cdots$$ or $$\fbox{$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1+p)}\left(\frac{x}{2}\right)^{2n+p}$}\tag{5}$$ We have found just one of the two solutions of Bessel's equation, that is, the one when $s=p$; we must next find the solution when $s=-p$. It is unnecessary to go through all the details again; we can just replace $p$ by $-p$ in $(5)$. In fact, the solution when $s=-p$ is usually written $J_{-p}(x)$. So from $(5)$ we have $$\fbox{$J_{-p}(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1-p)}\left(\frac{x}{2}\right)^{2n-p}$}\tag{6}$$ I understand all of the above. My question is regarding the following extract: If $p$ is not an integer $J_p(x)$ is a series starting with $x^p$ and $J_{-p}(x)$ is a series starting with $x^{-p}$. Then $J_p(x)$ and $J_{-p}(x)$ are two independent solutions and a linear combination of them is a general solution.    $\bbox[#AFF]{\text{But if }p\text{ is}\text{ an integer, then the first few terms in }{J_{-p}(x)}\text{ will be zero, as in the denominator }\,\Gamma(n-p+1)\text{ is }\Gamma \text{ of a negative number, which is infinite.}}$   $\bbox[#AFA]{\text{You can show that }J_{-p}(x)\text{ starts with the term }x^p\text{ for integer }p\text{ just as } J_{p}(x)\text{ does,}}$$\bbox[yellow]{\text{ and that }J_{-p}(x)=(-1)^pJ_p(x)\text{ for integer }p}$. For the blue highlighted part it says the ""first few terms in $J_{-p}(x)$ will be zero""; But how do they know this? This can only be true by my logic iff $p\gt 2$. Yet we are given no restriction on $p$ other than $p \gt 0$; Why is it only the first few (two) terms that are zero? For the green highlighted part; by my logic $J_{-p}(x)$ always starts with $x^{-p}$ and not $x^p$. What am I missing here? Lastly for the yellow part: I'm sorry but I have simply no idea how to show that $J_{-p}(x)=(-1)^pJ_p(x)$, and much to my annoyance this is the most important part of this post. I have already checked the errata list for this book and I can tell you that none of the above queries of mine are due to book errors. If anyone is able to provide me with some hints/advice on addressing some/all/any of the $3$ queries above I will be most grateful. I have only just started to learn about Bessel's equation, so my knowledge is somewhat limited (apologies). Kindest Regards.",,"['ordinary-differential-equations', 'recurrence-relations', 'special-functions', 'bessel-functions']"
2,4th order differential equation from Euler-Lagrange,4th order differential equation from Euler-Lagrange,,"I am trying to extremise the functional $\int{[y + \frac{1}{2}y^2 - \frac{1}{2}(y^{''})^2]}dy$ and so from Euler-Lagrange I get the differential equation $1 + y + y^{(4)} = 0$ and I have no idea how to solve it. It's supposed to be an easy question so there must be a trick. I have the initial conditions $y(0) = -1, y'(0) = 0, y(\pi) = \cosh(\pi), y'(\pi) = \sinh(\pi)$. Just to check, I'm using the E-L equation, $\frac{\partial f}{\partial y} - \frac{d}{dx} \frac{\partial f}{\partial y'} + \frac{d^2}{dx^2} \frac{\partial f}{\partial y^{''}} = 0$ Can someone help? Thanks","I am trying to extremise the functional $\int{[y + \frac{1}{2}y^2 - \frac{1}{2}(y^{''})^2]}dy$ and so from Euler-Lagrange I get the differential equation $1 + y + y^{(4)} = 0$ and I have no idea how to solve it. It's supposed to be an easy question so there must be a trick. I have the initial conditions $y(0) = -1, y'(0) = 0, y(\pi) = \cosh(\pi), y'(\pi) = \sinh(\pi)$. Just to check, I'm using the E-L equation, $\frac{\partial f}{\partial y} - \frac{d}{dx} \frac{\partial f}{\partial y'} + \frac{d^2}{dx^2} \frac{\partial f}{\partial y^{''}} = 0$ Can someone help? Thanks",,"['ordinary-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation']"
3,Decoupling coupled differential equations with time dependent coefficients,Decoupling coupled differential equations with time dependent coefficients,,"Consider the following system of coupled differential equation. $$\left[ \begin{array}{c} \frac{dc_1}{dt} \\ \frac{dc_2}{dt} \end{array} \right] = \begin{bmatrix} -B & -V(t) \\ -V(t) & B \end{bmatrix} \times \left[ \begin{array}{c} c_1 \\ c_2 \end{array} \right]$$ I tried diagonalyzng the matrix using the eigenvectors of the coefficient matrix. But, since the matrix is time dependent so are its eigenvectors. So, how can one decouple this system?","Consider the following system of coupled differential equation. $$\left[ \begin{array}{c} \frac{dc_1}{dt} \\ \frac{dc_2}{dt} \end{array} \right] = \begin{bmatrix} -B & -V(t) \\ -V(t) & B \end{bmatrix} \times \left[ \begin{array}{c} c_1 \\ c_2 \end{array} \right]$$ I tried diagonalyzng the matrix using the eigenvectors of the coefficient matrix. But, since the matrix is time dependent so are its eigenvectors. So, how can one decouple this system?",,"['ordinary-differential-equations', 'coupling']"
4,What exactly is the maximal solution of an ODE and why do we care?,What exactly is the maximal solution of an ODE and why do we care?,,"I am reading these notes on the definition of a maximal solution of an ODE i.e. http://www.math.lmu.de/~philip/publications/lectureNotes/ODE.pdf But the definition is so abstract and no example is provided! From what I can gather, the maximal solution of an ODE is the solution to an ODE that exists for the longest time. But why do we care about this? For example, given $\dot x = -x$, the solution is $x(t) = K\exp(-t)$ where $K$ is some constant. Solution exists for all times. And? Can someone provide a concrete example of a differential equation where we need to care about maximal solution and why it matters?","I am reading these notes on the definition of a maximal solution of an ODE i.e. http://www.math.lmu.de/~philip/publications/lectureNotes/ODE.pdf But the definition is so abstract and no example is provided! From what I can gather, the maximal solution of an ODE is the solution to an ODE that exists for the longest time. But why do we care about this? For example, given $\dot x = -x$, the solution is $x(t) = K\exp(-t)$ where $K$ is some constant. Solution exists for all times. And? Can someone provide a concrete example of a differential equation where we need to care about maximal solution and why it matters?",,"['ordinary-differential-equations', 'terminology', 'examples-counterexamples']"
5,Adjoint of a differential operator,Adjoint of a differential operator,,"I am self-studying differential equations using MIT's publicly available materials.  One of the recitation exercises runs as follows: Define an inner or dot product on $\mathcal{C}[a,b]$ by      \begin{align} 	\langle u, v \rangle & = \int_a^bu(x)v(x)dx 	\end{align}     Suppose $L[u] = u'' + pu' + qu$ is a differential operator, and $M[u]$ is its adjoint.  Show that $\langle L[u], v\rangle = \langle u, M[v]\rangle$ for all $u, v \in \mathcal{C}^2[a,b]$ provided $u(a) = u(b) = v(a) = v(b) = 0$. In neither the textbook nor the lecture notes has the notion of an adjoint been introduced, so I went to Wikipedia (and various other online sources) for a definition.  From what I can tell,  if $T$ is a differential operator, then its adjoint $T^*$ is defined as the differential operator such that  \begin{align} \langle Tu, v \rangle = \langle u,T^*v\rangle \end{align} But if this is the case, isn't what the exercise asks for true immediately and trivially?  (And, moreover, independent of the fact that $u(a) = u(b) = v(a) = v(b) = 0$?) No doubt there's something basic I don't know.  If so, a gentle hint (rather than a complete solution) would be appreciated.","I am self-studying differential equations using MIT's publicly available materials.  One of the recitation exercises runs as follows: Define an inner or dot product on $\mathcal{C}[a,b]$ by      \begin{align} 	\langle u, v \rangle & = \int_a^bu(x)v(x)dx 	\end{align}     Suppose $L[u] = u'' + pu' + qu$ is a differential operator, and $M[u]$ is its adjoint.  Show that $\langle L[u], v\rangle = \langle u, M[v]\rangle$ for all $u, v \in \mathcal{C}^2[a,b]$ provided $u(a) = u(b) = v(a) = v(b) = 0$. In neither the textbook nor the lecture notes has the notion of an adjoint been introduced, so I went to Wikipedia (and various other online sources) for a definition.  From what I can tell,  if $T$ is a differential operator, then its adjoint $T^*$ is defined as the differential operator such that  \begin{align} \langle Tu, v \rangle = \langle u,T^*v\rangle \end{align} But if this is the case, isn't what the exercise asks for true immediately and trivially?  (And, moreover, independent of the fact that $u(a) = u(b) = v(a) = v(b) = 0$?) No doubt there's something basic I don't know.  If so, a gentle hint (rather than a complete solution) would be appreciated.",,['ordinary-differential-equations']
6,Prove that every solution of an ODE system converges to some point,Prove that every solution of an ODE system converges to some point,,"Suppose $p(t)>2$ and is continuous for all $t\in\Bbb R$,    $$x'=2y,\\ y'=-2x-p(t)y^3,$$ prove that for each solution $(x(t),y(t))$ there exists a point $(x^*,0)$ to which it converges. I guess the most probable approach is Lyapunov stability theory , which is within the range of my ODE course. But this is a non-autonomous system (with time-varying input $p(t)$), and all I have learned from class is criteria for stability for autonomous systems. It seems kinda obvious that $x^*$ is relevant to $p(t)$, but I have trouble  even proving the existence of a singular point $(x^*,0)$ for this system, let alone dealing with it analytically. The case $p(t)\equiv\text{const}$ is easy, with $(0,0)$ apparently being the only singular point which is also apparently globally Lyapunov asymptotically stable on $\Bbb R^2$, and hence globally attracting (meaning all solutions converge to this singular point.) But for a time-varying input it is entirely different. Maybe I'll need a non-autonomous version of criterion for Lyapunov stability? But since this is beyond my course's level I think it there must be some more elementary alternatives. I'd be very grateful if anybody can provide me with one. (Of course if using non-autonomous versions can't be helped I would also be glad to learn about such tricks as long as they are effective.) EDIT Terribly sorry. I made a mistake. $(x^*,0)$ may be dependent upon the solution (or the initial condition).","Suppose $p(t)>2$ and is continuous for all $t\in\Bbb R$,    $$x'=2y,\\ y'=-2x-p(t)y^3,$$ prove that for each solution $(x(t),y(t))$ there exists a point $(x^*,0)$ to which it converges. I guess the most probable approach is Lyapunov stability theory , which is within the range of my ODE course. But this is a non-autonomous system (with time-varying input $p(t)$), and all I have learned from class is criteria for stability for autonomous systems. It seems kinda obvious that $x^*$ is relevant to $p(t)$, but I have trouble  even proving the existence of a singular point $(x^*,0)$ for this system, let alone dealing with it analytically. The case $p(t)\equiv\text{const}$ is easy, with $(0,0)$ apparently being the only singular point which is also apparently globally Lyapunov asymptotically stable on $\Bbb R^2$, and hence globally attracting (meaning all solutions converge to this singular point.) But for a time-varying input it is entirely different. Maybe I'll need a non-autonomous version of criterion for Lyapunov stability? But since this is beyond my course's level I think it there must be some more elementary alternatives. I'd be very grateful if anybody can provide me with one. (Of course if using non-autonomous versions can't be helped I would also be glad to learn about such tricks as long as they are effective.) EDIT Terribly sorry. I made a mistake. $(x^*,0)$ may be dependent upon the solution (or the initial condition).",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
7,To solve differential equation ($xy^{3} + x^{2}y^{7}) \frac{dy}{dx} = 1$,To solve differential equation (,xy^{3} + x^{2}y^{7}) \frac{dy}{dx} = 1,"The ODE is ($xy^{3} + x^{2}y^{7}) \frac{dy}{dx} = 1$ I have tried everything like integrating factor,it is not homogenous and not linear differential equation..What should be done now?","The ODE is ($xy^{3} + x^{2}y^{7}) \frac{dy}{dx} = 1$ I have tried everything like integrating factor,it is not homogenous and not linear differential equation..What should be done now?",,"['calculus', 'integration', 'ordinary-differential-equations']"
8,Second solution of ODE $xy''+y'-y=0$?,Second solution of ODE ?,xy''+y'-y=0,"Suppose we have the following equation $$xy''+y'-y=0$$ where it has a regular singular point at $x=0$ and we want to derive the series solution near $x=0$. We write the ODE in canonical form: $y''+\frac{1}{x}y'-\frac{1}{x}y=0$ and then we set $$y=\sum_{n=0}^{\infty}a_nx^{n+r}.$$ This will gives us $$y=\sum_{n=0}^{\infty}a_n(n+r)(n+r-1)x^{n+r-1}+a_n(n+r)x^{n+r-1}-a_nx^{n+r}.$$ We continue deriving the indicial equation from the coefficients of the lowest power of $x$ and thus $$a_0r(r-1)+a_0r=0\implies r^2=0$$ and so $r=0$ (repeated). Also we have for $r=0,~a_n=\frac{a_{n-1}}{n^2}=\ldots=\frac{a_0}{(n!)^2}.$ Hence $$y_1=\sum_{n=0}^{\infty}\frac{x^n}{(n!)^2}.$$ For the second solution $y_2$ we may proceed either with the Wronskian technique (in this case it's rather difficult to do that partly because the calculations are quite hard to do) or by differentiating with respect to $r$. But I struggle to understand the steps and exactly what to do. So we have $$L[y]=\sum_{n=0}^{\infty}a_n(n+r)^2x^{n+r-1}-a_nx^{n+r}.$$ But how do we continue from here to get an equation of $L[y]$ and differentiate it w.r.t $r$. Thank you in advance for your help.","Suppose we have the following equation $$xy''+y'-y=0$$ where it has a regular singular point at $x=0$ and we want to derive the series solution near $x=0$. We write the ODE in canonical form: $y''+\frac{1}{x}y'-\frac{1}{x}y=0$ and then we set $$y=\sum_{n=0}^{\infty}a_nx^{n+r}.$$ This will gives us $$y=\sum_{n=0}^{\infty}a_n(n+r)(n+r-1)x^{n+r-1}+a_n(n+r)x^{n+r-1}-a_nx^{n+r}.$$ We continue deriving the indicial equation from the coefficients of the lowest power of $x$ and thus $$a_0r(r-1)+a_0r=0\implies r^2=0$$ and so $r=0$ (repeated). Also we have for $r=0,~a_n=\frac{a_{n-1}}{n^2}=\ldots=\frac{a_0}{(n!)^2}.$ Hence $$y_1=\sum_{n=0}^{\infty}\frac{x^n}{(n!)^2}.$$ For the second solution $y_2$ we may proceed either with the Wronskian technique (in this case it's rather difficult to do that partly because the calculations are quite hard to do) or by differentiating with respect to $r$. But I struggle to understand the steps and exactly what to do. So we have $$L[y]=\sum_{n=0}^{\infty}a_n(n+r)^2x^{n+r-1}-a_nx^{n+r}.$$ But how do we continue from here to get an equation of $L[y]$ and differentiate it w.r.t $r$. Thank you in advance for your help.",,['ordinary-differential-equations']
9,How do i solve $\frac{dy}{dx}- \frac {dx}{dy}= \frac {x}{y}-\frac {y}{x}$?,How do i solve ?,\frac{dy}{dx}- \frac {dx}{dy}= \frac {x}{y}-\frac {y}{x},I came up with this question in my exam. But i didn't get it right. Can someone show me how to solve this differential equation $$\frac{dy}{dx}- \frac {dx}{dy}= \frac {x}{y}-\frac {y}{x}$$,I came up with this question in my exam. But i didn't get it right. Can someone show me how to solve this differential equation $$\frac{dy}{dx}- \frac {dx}{dy}= \frac {x}{y}-\frac {y}{x}$$,,['ordinary-differential-equations']
10,Differential equation free fall in gravitational field,Differential equation free fall in gravitational field,,"For a physics problem I was told to set up a differential equation for the free fall in the gravitational field of the earth. The equation (via Newton) I've got is following: $$\ddot{r} = - G M \frac 1 {r^2}$$ where $G$ is the gravitational constant, $M$ is the mass of the earth and $r$ is the distance to earths center of gravity. I was told to use this equation to find the velocity $\dot{r}$ but I have no idea how to do that, and it seems almost impossible to solve this differential equation analytically at all?","For a physics problem I was told to set up a differential equation for the free fall in the gravitational field of the earth. The equation (via Newton) I've got is following: $$\ddot{r} = - G M \frac 1 {r^2}$$ where $G$ is the gravitational constant, $M$ is the mass of the earth and $r$ is the distance to earths center of gravity. I was told to use this equation to find the velocity $\dot{r}$ but I have no idea how to do that, and it seems almost impossible to solve this differential equation analytically at all?",,"['ordinary-differential-equations', 'physics']"
11,Wronskian which is zero at one point,Wronskian which is zero at one point,,"For the ODE $$xy''-(x+2)y'+2y=0~,$$ which has solutions $y_1 = e^x$ and $y_2 = x^2+2x+2$ , the Wronskian is $W=-e^x x^2$ . As per the known theorem, Wronskian is either identically zero (i.e. zero for all $x$ ) or is never zero. But $-e^x x^2$ is zero at $x=0$ , which appears to be in the domain of solutions of this equation (since $y(0) = y'(0)$ . Does this somehow contradict the theorem?","For the ODE which has solutions and , the Wronskian is . As per the known theorem, Wronskian is either identically zero (i.e. zero for all ) or is never zero. But is zero at , which appears to be in the domain of solutions of this equation (since . Does this somehow contradict the theorem?","xy''-(x+2)y'+2y=0~, y_1 = e^x y_2 = x^2+2x+2 W=-e^x x^2 x -e^x x^2 x=0 y(0) = y'(0)","['ordinary-differential-equations', 'differential', 'wronskian']"
12,Substitution for Differential equations,Substitution for Differential equations,,"Dear StackExchange users, I have a little question ... I just don't have a clue how that works I have the following differential equation $$\frac{dy}{dx}=xy^2-2\frac{y}{x}-\frac{1}{x^3}$$ My book says that one can substitute $(r,s) = (x^2y,\ln{|x|})$ to get $$\frac{ds}{dr}=\frac{1}{r^2-1}$$ The right-hand side of the ODE is not my problem. The right-hand side reduces to $$\exp{(-3s)}(r^2-2r-1)$$ I just don't know how to replace $dy/dx$ by an expression depending on $ds/dr$. I would be very thankful if someone could show me how that works step by step.","Dear StackExchange users, I have a little question ... I just don't have a clue how that works I have the following differential equation $$\frac{dy}{dx}=xy^2-2\frac{y}{x}-\frac{1}{x^3}$$ My book says that one can substitute $(r,s) = (x^2y,\ln{|x|})$ to get $$\frac{ds}{dr}=\frac{1}{r^2-1}$$ The right-hand side of the ODE is not my problem. The right-hand side reduces to $$\exp{(-3s)}(r^2-2r-1)$$ I just don't know how to replace $dy/dx$ by an expression depending on $ds/dr$. I would be very thankful if someone could show me how that works step by step.",,['ordinary-differential-equations']
13,Discrete time equivalent to ODE,Discrete time equivalent to ODE,,"I'm reading a paper in which it is noted that $$\frac{dv(t)}{dt} = f(t) - \varepsilon v(t)$$ has the discrete time equivalent $$v(t+1) = v(t)\exp(-\varepsilon) + \frac{f(t)}{\varepsilon}[1 - \exp(-\varepsilon)].$$ How is this derived? Update: In case I have misunderstood, my question is a simplified version of the text appearing at the top of Page 2 in this supplementary material from this paper .  Namely $$\frac{dv_i}{dt}=\frac{I_i}{N_i} - \varepsilon v_i$$ $$v_i(t+1)=v_i(t)\exp(-\varepsilon)+\frac{I_i}{\varepsilon N_i}[1-\exp(-\varepsilon)]$$ Where I understand $\tfrac{I_i}{N_i}$ to be the evolving proportion of infectious animals at a location.","I'm reading a paper in which it is noted that $$\frac{dv(t)}{dt} = f(t) - \varepsilon v(t)$$ has the discrete time equivalent $$v(t+1) = v(t)\exp(-\varepsilon) + \frac{f(t)}{\varepsilon}[1 - \exp(-\varepsilon)].$$ How is this derived? Update: In case I have misunderstood, my question is a simplified version of the text appearing at the top of Page 2 in this supplementary material from this paper .  Namely $$\frac{dv_i}{dt}=\frac{I_i}{N_i} - \varepsilon v_i$$ $$v_i(t+1)=v_i(t)\exp(-\varepsilon)+\frac{I_i}{\varepsilon N_i}[1-\exp(-\varepsilon)]$$ Where I understand $\tfrac{I_i}{N_i}$ to be the evolving proportion of infectious animals at a location.",,['ordinary-differential-equations']
14,Application of Poincaré-Bendixson theorem,Application of Poincaré-Bendixson theorem,,"Consider the system $$x' = 3xy^2-x^2y \\ y' = 5x^2y - xy^2$$ Show that the system has no periodic solutions. This is a tricky example. Linearization leads nowhere and I'm having a hard time constructing a Lyapunov function that does the trick. $V = 1/2(x^2+y^2)$ gives $$V'(x,y) = 3x^2y^2-x^3y +5x^2y^2 -xy^3 = 8x^2y^2 -xy(x^2+y^2))$$ But this doesn't tell us much nice things about the origin. If anything, it looks as though the origin is repelling since small perturbations gives us that the $8x^2y^2$ term dominates the minus term. Maybe it's possible to show that there are no elliptical orbits somehow, but that doesn't exclude other, more exotic, periodic trajectories. How to proceed...?","Consider the system $$x' = 3xy^2-x^2y \\ y' = 5x^2y - xy^2$$ Show that the system has no periodic solutions. This is a tricky example. Linearization leads nowhere and I'm having a hard time constructing a Lyapunov function that does the trick. $V = 1/2(x^2+y^2)$ gives $$V'(x,y) = 3x^2y^2-x^3y +5x^2y^2 -xy^3 = 8x^2y^2 -xy(x^2+y^2))$$ But this doesn't tell us much nice things about the origin. If anything, it looks as though the origin is repelling since small perturbations gives us that the $8x^2y^2$ term dominates the minus term. Maybe it's possible to show that there are no elliptical orbits somehow, but that doesn't exclude other, more exotic, periodic trajectories. How to proceed...?",,"['ordinary-differential-equations', 'dynamical-systems']"
15,"Finding a general solution to a differential equation, using the integration factor method","Finding a general solution to a differential equation, using the integration factor method",,"Use the method of integrating factor to solve the linear ODE $$ y' + 2xy = e^{−x^2}.$$ And verify your answer I can solve the ODE as a linear equation (mulitply both sides, subsititute, reverse product rule, integrate etc.) to obtain the answer $$ y(x) = c_1 e^{-x^2} e^{-x^2}x $$ However could someone show me how to do this question using the integrating factor method and (subsequently verifying it using that method?)","Use the method of integrating factor to solve the linear ODE $$ y' + 2xy = e^{−x^2}.$$ And verify your answer I can solve the ODE as a linear equation (mulitply both sides, subsititute, reverse product rule, integrate etc.) to obtain the answer $$ y(x) = c_1 e^{-x^2} e^{-x^2}x $$ However could someone show me how to do this question using the integrating factor method and (subsequently verifying it using that method?)",,"['calculus', 'integration', 'ordinary-differential-equations', 'multivariable-calculus']"
16,Simple differential equation( introduction but need some basic explanation),Simple differential equation( introduction but need some basic explanation),,"I have a couple of questions before I dig deeper into my calculus book. First: I have learned that  $\frac{d}{dx}\frac{x}{y}$=$\frac{y x'-x y'}{y^2}$ never really gotten a proper explanation for why suddenly when its differential equations it becomes as they have defined it in my book: $$\mathrm {d}(\frac{x}{y})=\frac{y\mathrm dx-x\mathrm dy}{y^2}$$ I see algebraically that this is simply dividing by dx, but I HATE when I do not get a proper explanation for it. Second is a more specific case from the start of the book: $$x^2 y\mathrm {dy}-(x\mathrm dy-y\mathrm dx)=0$$ Now If I divide by $x^2$ $$y \mathrm dy-\mathrm{d}\left[\frac{x}{y}\right]=0$$, Now I get what the point of doing this is:  However it is the result that throws me off:  $y^2/2-x/y=c$ But I am integrating both why is the constant positive. Is it simply just that c can take on a negative value, why does not the c's cancel out since I integrate both. I am not sure how to interpret this. Last question: $$\pm \frac{\mathrm {d}\left(x^2+y^2\right)}{2 \sqrt{x^2+y^2}}=\mathrm {dx}$$ Solving this produces $$\pm \sqrt{x^2+y^2}=c+x$$, However, I do not see how this makes it any easier unless $\mathrm {d}\left(x^2+y^2\right)=2x$, but if that's the case I am even more confused since in this case y disappears as I am used to dealing with derivatives since nowhere it is stated that $y$ is a function of $x$. But that to me conflicts what happens in what i wrote under First. Could someone please explain in detail why in the last instance: $\frac{\mathrm d\left(x^2+y^2\right)}{\mathrm dx}=2 y \frac{\mathrm dy}{\mathrm dx}+2 x$ is not the case, but simply $2x$. In one moment I can use the derivative as a fraction(FIRST) in the other (last question) I can not?","I have a couple of questions before I dig deeper into my calculus book. First: I have learned that  $\frac{d}{dx}\frac{x}{y}$=$\frac{y x'-x y'}{y^2}$ never really gotten a proper explanation for why suddenly when its differential equations it becomes as they have defined it in my book: $$\mathrm {d}(\frac{x}{y})=\frac{y\mathrm dx-x\mathrm dy}{y^2}$$ I see algebraically that this is simply dividing by dx, but I HATE when I do not get a proper explanation for it. Second is a more specific case from the start of the book: $$x^2 y\mathrm {dy}-(x\mathrm dy-y\mathrm dx)=0$$ Now If I divide by $x^2$ $$y \mathrm dy-\mathrm{d}\left[\frac{x}{y}\right]=0$$, Now I get what the point of doing this is:  However it is the result that throws me off:  $y^2/2-x/y=c$ But I am integrating both why is the constant positive. Is it simply just that c can take on a negative value, why does not the c's cancel out since I integrate both. I am not sure how to interpret this. Last question: $$\pm \frac{\mathrm {d}\left(x^2+y^2\right)}{2 \sqrt{x^2+y^2}}=\mathrm {dx}$$ Solving this produces $$\pm \sqrt{x^2+y^2}=c+x$$, However, I do not see how this makes it any easier unless $\mathrm {d}\left(x^2+y^2\right)=2x$, but if that's the case I am even more confused since in this case y disappears as I am used to dealing with derivatives since nowhere it is stated that $y$ is a function of $x$. But that to me conflicts what happens in what i wrote under First. Could someone please explain in detail why in the last instance: $\frac{\mathrm d\left(x^2+y^2\right)}{\mathrm dx}=2 y \frac{\mathrm dy}{\mathrm dx}+2 x$ is not the case, but simply $2x$. In one moment I can use the derivative as a fraction(FIRST) in the other (last question) I can not?",,"['calculus', 'ordinary-differential-equations']"
17,How to solve $y' = -2x -y$,How to solve,y' = -2x -y,"My thought: $\displaystyle\frac{dy}{dx}+x^0y=-2x$ Considering it as the form of linear equation, $\displaystyle\frac{dy}{dx}+P(x)y=Q(x)$ Multiplying $e^{\int1dx} = e^x$ on both sides, $e^x\displaystyle\frac{dy}{dx}+e^xy=-2xe^x$ $\displaystyle\frac{d}{dx}(e^xy)=-2xe^x$ $e^xy=\int-2xe^xdx$ $e^xy=-2(xe^x-e^x)$ $y=-2(x-1)+C$ It seems to me that the solution is wrong because i cannot move back to my original question from here. Is my answer correct? If not, can anyone tell me how to solve this problem with explanation.","My thought: $\displaystyle\frac{dy}{dx}+x^0y=-2x$ Considering it as the form of linear equation, $\displaystyle\frac{dy}{dx}+P(x)y=Q(x)$ Multiplying $e^{\int1dx} = e^x$ on both sides, $e^x\displaystyle\frac{dy}{dx}+e^xy=-2xe^x$ $\displaystyle\frac{d}{dx}(e^xy)=-2xe^x$ $e^xy=\int-2xe^xdx$ $e^xy=-2(xe^x-e^x)$ $y=-2(x-1)+C$ It seems to me that the solution is wrong because i cannot move back to my original question from here. Is my answer correct? If not, can anyone tell me how to solve this problem with explanation.",,['ordinary-differential-equations']
18,Why is the solution to $y' = y^n$ always in polynomial form EXCEPT when $n = 1$?,Why is the solution to  always in polynomial form EXCEPT when ?,y' = y^n n = 1,"Could someone explain (intuition-wise) why the differential equation $$y' = y^n$$ for $n \in \mathbb{N}$ seems to always some kind of polynomial solution (or a ratio of polynomials, etc.) except when $n = 1$, in which case the solution seems to be exponential? What's so special about $n = 1$ that (if you'll pardon the term) differentiates it from e.g. $n = 2$?","Could someone explain (intuition-wise) why the differential equation $$y' = y^n$$ for $n \in \mathbb{N}$ seems to always some kind of polynomial solution (or a ratio of polynomials, etc.) except when $n = 1$, in which case the solution seems to be exponential? What's so special about $n = 1$ that (if you'll pardon the term) differentiates it from e.g. $n = 2$?",,"['ordinary-differential-equations', 'polynomials', 'exponential-function']"
19,Equivalence of Solutions to Wave Equation,Equivalence of Solutions to Wave Equation,,The differential equation $$\ddot x = -\omega^2 x$$ apparently has solutions of $$x = Ae^{i\omega t} + Be^{-i\omega t} \tag{1}$$ AND $$x = A\sin(\omega t) + B\cos(\omega t) \tag{2}$$ AND $$x = A\sin(\omega t + \phi) \tag{3}$$ How are all of these equivalent?  Isn't $(1)$ a complex number where $(2)$ and $(3)$ are real?  And how is the sum of a sine wave and a cosine wave just a sine wave?,The differential equation $$\ddot x = -\omega^2 x$$ apparently has solutions of $$x = Ae^{i\omega t} + Be^{-i\omega t} \tag{1}$$ AND $$x = A\sin(\omega t) + B\cos(\omega t) \tag{2}$$ AND $$x = A\sin(\omega t + \phi) \tag{3}$$ How are all of these equivalent?  Isn't $(1)$ a complex number where $(2)$ and $(3)$ are real?  And how is the sum of a sine wave and a cosine wave just a sine wave?,,"['ordinary-differential-equations', 'trigonometry']"
20,Solving seemingly easy differential EQ,Solving seemingly easy differential EQ,,"I have the differential equation $2\frac{d^2\phi}{d^2\zeta}=e^{-\phi}$, where $\phi=\Phi/\sigma_z^2$, $\zeta=z/z_0$. I also have the fact that $ln(\rho/\rho_0)=-\Phi/\sigma_z^2$. Given the boundary conditions $\phi(0)=0=\frac{d\phi}{d\zeta}\big{|}_0$, we need to show that the solution is $\rho(z)=\rho_0sech^2(\frac{z}{2z_0})$. I show below what seems to be the correct method, but I cannot reproduce the quoted answer: $$\frac{d^2\phi}{d^2\zeta}=\frac{e^{-\phi}}{2}$$ $$\frac{d\phi}{d\zeta}-\frac{d\phi}{d\zeta}{|}_0=\frac{e^{-\phi}}{2}\zeta+c_1$$ $$\frac{d\phi}{d\zeta}=\frac{e^{-\phi}}{2}\zeta+c_1$$ to get $c_1$: $$\frac{d\phi}{d\zeta}|_0=\frac{e^{-\phi(0)}}{2}\zeta+c_1$$ $$0=\frac{e^{0}}{2}\zeta+c_1$$ thus $c_1=-.5\zeta$ $$\frac{d\phi}{d\zeta}=\frac{1}{2}\zeta({e^{-\phi}}-1)$$ $$\frac{d\phi}{({e^{-\phi}}-1)}=\frac{1}{2}\zeta d\zeta$$ letting $u\equiv e^{-\phi}-1,du\equiv -d\phi e^{-\phi}\rightarrow du\equiv -d\phi (1+u)$. thus this becomes $$\int-\frac{du}{(1+u)u}=\int\frac{1}{2}\zeta d\zeta$$ I use the partial fraction method to beget the LHS in a tractable form, and integrating this equation I get the following: $$-ln(1-e^{\phi})+ln(1-e^{\phi(0)})=\frac{\zeta^2}{4}+c_3$$ $$-ln(1-e^{\phi})-\infty=\frac{\zeta^2}{4}+c_3$$ $$-ln(1-e^{\phi(0)})-\infty=\frac{\zeta^2}{4}+c_3$$ $$\infty-\infty=\frac{\zeta^2}{4}+c_3$$ so $c_3$ is supposedly $-\zeta^2/4$. This is problematic, for then I get that $\rho/\rho_0=0$. What may be leading it astray?","I have the differential equation $2\frac{d^2\phi}{d^2\zeta}=e^{-\phi}$, where $\phi=\Phi/\sigma_z^2$, $\zeta=z/z_0$. I also have the fact that $ln(\rho/\rho_0)=-\Phi/\sigma_z^2$. Given the boundary conditions $\phi(0)=0=\frac{d\phi}{d\zeta}\big{|}_0$, we need to show that the solution is $\rho(z)=\rho_0sech^2(\frac{z}{2z_0})$. I show below what seems to be the correct method, but I cannot reproduce the quoted answer: $$\frac{d^2\phi}{d^2\zeta}=\frac{e^{-\phi}}{2}$$ $$\frac{d\phi}{d\zeta}-\frac{d\phi}{d\zeta}{|}_0=\frac{e^{-\phi}}{2}\zeta+c_1$$ $$\frac{d\phi}{d\zeta}=\frac{e^{-\phi}}{2}\zeta+c_1$$ to get $c_1$: $$\frac{d\phi}{d\zeta}|_0=\frac{e^{-\phi(0)}}{2}\zeta+c_1$$ $$0=\frac{e^{0}}{2}\zeta+c_1$$ thus $c_1=-.5\zeta$ $$\frac{d\phi}{d\zeta}=\frac{1}{2}\zeta({e^{-\phi}}-1)$$ $$\frac{d\phi}{({e^{-\phi}}-1)}=\frac{1}{2}\zeta d\zeta$$ letting $u\equiv e^{-\phi}-1,du\equiv -d\phi e^{-\phi}\rightarrow du\equiv -d\phi (1+u)$. thus this becomes $$\int-\frac{du}{(1+u)u}=\int\frac{1}{2}\zeta d\zeta$$ I use the partial fraction method to beget the LHS in a tractable form, and integrating this equation I get the following: $$-ln(1-e^{\phi})+ln(1-e^{\phi(0)})=\frac{\zeta^2}{4}+c_3$$ $$-ln(1-e^{\phi})-\infty=\frac{\zeta^2}{4}+c_3$$ $$-ln(1-e^{\phi(0)})-\infty=\frac{\zeta^2}{4}+c_3$$ $$\infty-\infty=\frac{\zeta^2}{4}+c_3$$ so $c_3$ is supposedly $-\zeta^2/4$. This is problematic, for then I get that $\rho/\rho_0=0$. What may be leading it astray?",,['ordinary-differential-equations']
21,How to solve this simple yet troublesome differential equation,How to solve this simple yet troublesome differential equation,,"$\frac{d^2y}{dx^2}-2\cdot \frac{dy}{dx}=0$ I am a newbie to differential equations, and I tried to separate variables, but had no success. MY course only covered first order equations, but I am curious about how to solve this one. Thanks!","$\frac{d^2y}{dx^2}-2\cdot \frac{dy}{dx}=0$ I am a newbie to differential equations, and I tried to separate variables, but had no success. MY course only covered first order equations, but I am curious about how to solve this one. Thanks!",,['ordinary-differential-equations']
22,Convolution with Heaviside function (integration),Convolution with Heaviside function (integration),,"To clarify notation, I use $u_n = 1$ when $x>n$, and $0$ otherwise. I am having troubles with the following convolution/integration: $u_2(t) \ast sin(\sqrt{2}t) = \int^t_0u_2(\tau) \cdot sin(\sqrt{2}(t-\tau))\ d\tau$. At first I thought of splitting the integral up so that I can make the Heaviside function some definitive value (0 or 1) on an interval, but I do not know how that would work since $t$ has no specific value. That leads me to think perhaps my problem is that I am not very good at integration. Any hints or tips will be appreciated.","To clarify notation, I use $u_n = 1$ when $x>n$, and $0$ otherwise. I am having troubles with the following convolution/integration: $u_2(t) \ast sin(\sqrt{2}t) = \int^t_0u_2(\tau) \cdot sin(\sqrt{2}(t-\tau))\ d\tau$. At first I thought of splitting the integral up so that I can make the Heaviside function some definitive value (0 or 1) on an interval, but I do not know how that would work since $t$ has no specific value. That leads me to think perhaps my problem is that I am not very good at integration. Any hints or tips will be appreciated.",,"['integration', 'ordinary-differential-equations', 'convolution']"
23,Find the unique solution to the IVP,Find the unique solution to the IVP,,Find the unique solution to the IVP $t^3y'' + e^ty' + t^4y = 0$ $y(1) = 0$ $y'(1) = 0$ Any help would be great..I am lost on how to do this problem.. at first I was going to try to divide by $t^4$ but now I'm not sure.,Find the unique solution to the IVP $t^3y'' + e^ty' + t^4y = 0$ $y(1) = 0$ $y'(1) = 0$ Any help would be great..I am lost on how to do this problem.. at first I was going to try to divide by $t^4$ but now I'm not sure.,,"['calculus', 'ordinary-differential-equations']"
24,How can we calculate the limit $\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at}b(t)dt$?,How can we calculate the limit ?,\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at}b(t)dt,"I am looking at the following exercise: Let the (linear) differential equation $y'+ay=b(x)$ where $a>0, b$ continuous on $[0,+\infty)$ and $\lim_{x \to +\infty} b(x)=l \in \mathbb{R}$. Show that each solution of the differential equation goes to $\frac{l}{a}$ while $x \to +\infty$, i.e. if $\phi$ is any solution of the differential equation, show that $\lim_{x \to +\infty} \phi(x)=\frac{l}{a}$. That's what I have tried: The solution of the differential equation will be of the form: $\phi(x)=ce^{-ax}+e^{-ax} \int_0^x e^{at}b(t) dt$ $\lim_{x \to +\infty} c e^{-ax}=0$ So, $\lim_{x \to +\infty} \phi(x)=\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at}b(t)dt$ How can we calculate the limit $\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at}b(t)dt$ ? EDIT :  $$\lim_{x \to +\infty} b(x)= l \in \mathbb{R}$$ That means that $\forall \epsilon>0$, $\exists n_0 \in \mathbb{N}$ such that $\forall n \geq n_0$: $|b(x)-l|< \epsilon \Rightarrow - \epsilon< b(x)-l< \epsilon \Rightarrow l- \epsilon<b(x)<l+ \epsilon$. We pick $\epsilon=\frac{l}{2}$ and so we have that: $b(x)>\frac{l}{2}$. Thus $\int_0^x e^{at} b(t)dt> \int_0^x e^{at} \frac{l}{2} dt=\frac{l}{2} \int_0^x e^{at}dt$ If $l>0$ then $\frac{l}{2} \int_0^x e^{at}dt \to +\infty$ and so $\int_0^x e^{at} b(t)dt$ diverges. If $l<0$ then $\frac{l}{2} \int_0^x e^{at}dt \to -\infty$ and so $\int_0^x e^{at} b(t)dt$ diverges. In both of the above cases, we can use the Fundamental Theorem Of Calculus: $$\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at} b(t) dt= \lim_{x \to +\infty} \frac{\int_0^x e^{at} b(t) dt}{e^{ax}}=\lim_{x \to +\infty} \frac{e^{ax} b(x)}{a e^{ax}} \to \frac{l}{a}$$ What can we say for the case $l=0$?","I am looking at the following exercise: Let the (linear) differential equation $y'+ay=b(x)$ where $a>0, b$ continuous on $[0,+\infty)$ and $\lim_{x \to +\infty} b(x)=l \in \mathbb{R}$. Show that each solution of the differential equation goes to $\frac{l}{a}$ while $x \to +\infty$, i.e. if $\phi$ is any solution of the differential equation, show that $\lim_{x \to +\infty} \phi(x)=\frac{l}{a}$. That's what I have tried: The solution of the differential equation will be of the form: $\phi(x)=ce^{-ax}+e^{-ax} \int_0^x e^{at}b(t) dt$ $\lim_{x \to +\infty} c e^{-ax}=0$ So, $\lim_{x \to +\infty} \phi(x)=\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at}b(t)dt$ How can we calculate the limit $\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at}b(t)dt$ ? EDIT :  $$\lim_{x \to +\infty} b(x)= l \in \mathbb{R}$$ That means that $\forall \epsilon>0$, $\exists n_0 \in \mathbb{N}$ such that $\forall n \geq n_0$: $|b(x)-l|< \epsilon \Rightarrow - \epsilon< b(x)-l< \epsilon \Rightarrow l- \epsilon<b(x)<l+ \epsilon$. We pick $\epsilon=\frac{l}{2}$ and so we have that: $b(x)>\frac{l}{2}$. Thus $\int_0^x e^{at} b(t)dt> \int_0^x e^{at} \frac{l}{2} dt=\frac{l}{2} \int_0^x e^{at}dt$ If $l>0$ then $\frac{l}{2} \int_0^x e^{at}dt \to +\infty$ and so $\int_0^x e^{at} b(t)dt$ diverges. If $l<0$ then $\frac{l}{2} \int_0^x e^{at}dt \to -\infty$ and so $\int_0^x e^{at} b(t)dt$ diverges. In both of the above cases, we can use the Fundamental Theorem Of Calculus: $$\lim_{x \to +\infty} e^{-ax} \int_0^x e^{at} b(t) dt= \lim_{x \to +\infty} \frac{\int_0^x e^{at} b(t) dt}{e^{ax}}=\lim_{x \to +\infty} \frac{e^{ax} b(x)}{a e^{ax}} \to \frac{l}{a}$$ What can we say for the case $l=0$?",,"['ordinary-differential-equations', 'limits']"
25,Show that $y_1$ and $y_2$ are not Linearly Independent,Show that  and  are not Linearly Independent,y_1 y_2,"Suppose that $y_1(x)$ and $y_2(x)$ are solutions of the differential equation $y''+py'+qy=0$ on $I$. How can I show that if $y_1$ and $y_2$ vanish at the same point then they are not linearly independent? Here is my attempt to prove the problem. Since $y_1(x)$ and $y_2(x)$ are solutions of the differential equation $y''+py'+qy=0$ on $I$, it is enough for me to show that the Wronskian of $y_1$ and $y_2$ denoted by $W(y_1,y_2)$ is zero. Now since $y_1$ and $y_2$ vanish at the same point say $p$ we have $y_1(p)=0$ and also $y_2(p)=0$. Solving for $W(y_1,y_2)(p)$ we have: $y_1(p)y_2'(p)-y_2(p)y_1'(p)=0$ since $y_1(p)=0$ and also $y_2(p)=0$. Am I correct? Thanks","Suppose that $y_1(x)$ and $y_2(x)$ are solutions of the differential equation $y''+py'+qy=0$ on $I$. How can I show that if $y_1$ and $y_2$ vanish at the same point then they are not linearly independent? Here is my attempt to prove the problem. Since $y_1(x)$ and $y_2(x)$ are solutions of the differential equation $y''+py'+qy=0$ on $I$, it is enough for me to show that the Wronskian of $y_1$ and $y_2$ denoted by $W(y_1,y_2)$ is zero. Now since $y_1$ and $y_2$ vanish at the same point say $p$ we have $y_1(p)=0$ and also $y_2(p)=0$. Solving for $W(y_1,y_2)(p)$ we have: $y_1(p)y_2'(p)-y_2(p)y_1'(p)=0$ since $y_1(p)=0$ and also $y_2(p)=0$. Am I correct? Thanks",,"['linear-algebra', 'ordinary-differential-equations']"
26,Reduce this third order ordinary differential equation to first order to use Runge Kutta,Reduce this third order ordinary differential equation to first order to use Runge Kutta,,"The ODE I'm working with is $$\dddot{x} + t^2\ddot{x} + 4x = 0$$ with $$x(0)=1, \dot{x}(0)=0, \ddot{x}=-1$$ I've written a very basic program in C++ to use the RK4 method to approximate a solution to first order ODEs. To get this particular ODE to be compatible with my code, I imagine I have to reduce the order to make this into a first order equation. However, I don't really know how to go about this as generally I have only encountered this reduction issue with second order ODEs. Can anyone help me get started or point me to somewhere with a thorough explanation/examples? I must admit my knowledge of ODEs is a bit rusty.","The ODE I'm working with is $$\dddot{x} + t^2\ddot{x} + 4x = 0$$ with $$x(0)=1, \dot{x}(0)=0, \ddot{x}=-1$$ I've written a very basic program in C++ to use the RK4 method to approximate a solution to first order ODEs. To get this particular ODE to be compatible with my code, I imagine I have to reduce the order to make this into a first order equation. However, I don't really know how to go about this as generally I have only encountered this reduction issue with second order ODEs. Can anyone help me get started or point me to somewhere with a thorough explanation/examples? I must admit my knowledge of ODEs is a bit rusty.",,"['ordinary-differential-equations', 'numerical-methods']"
27,Need help solving this differential equation,Need help solving this differential equation,,$$p^3 - 2xyp + 4y^2 = 0$$ where $p = \mathrm dy / \mathrm dx$. I don't know which type of equation it is or how to simplify it. Though there is an observation that $$(xy^2)′=y^2+2xyy′$$,$$p^3 - 2xyp + 4y^2 = 0$$ where $p = \mathrm dy / \mathrm dx$. I don't know which type of equation it is or how to simplify it. Though there is an observation that $$(xy^2)′=y^2+2xyy′$$,,"['ordinary-differential-equations', 'differential']"
28,"Why do ODEs that have solutions that have closed form solutions, *have* closed form solutions?","Why do ODEs that have solutions that have closed form solutions, *have* closed form solutions?",,"Why do certain classes of ODEs have closed form solutions? Is there something these classes have common apart from the fact that they have closed form solutions? When I say ""closed form"", I mean something vague, like there is no finite combination of usual operators (addition, multiplication, powers, trigonometric and logarithmic operators) and operands that can be used to write the solution.","Why do certain classes of ODEs have closed form solutions? Is there something these classes have common apart from the fact that they have closed form solutions? When I say ""closed form"", I mean something vague, like there is no finite combination of usual operators (addition, multiplication, powers, trigonometric and logarithmic operators) and operands that can be used to write the solution.",,['ordinary-differential-equations']
29,Solve differential equation $y'''(t)=y(t) y'(t)$.,Solve differential equation .,y'''(t)=y(t) y'(t),Solve following diferential equations $$y'''(t)=y(t) y'(t)$$ I would appreciate some help with this problem. Thanks in advance.,Solve following diferential equations $$y'''(t)=y(t) y'(t)$$ I would appreciate some help with this problem. Thanks in advance.,,['ordinary-differential-equations']
30,Annoying differential equation,Annoying differential equation,,"I feel like this should be really easy, but how do I solve $$f'(x+1)=f(x)?$$ I am extremely new to differential equations, and can't figure out what I'm missing. Most resources that I look at are in terms of $$y''+y'=e^x$$ or whatever, and none that I've seen discuss composition of functions. WolframAlpha can't figure out what I'm telling it; it doesn't seem to think that there's anything to solve.","I feel like this should be really easy, but how do I solve I am extremely new to differential equations, and can't figure out what I'm missing. Most resources that I look at are in terms of or whatever, and none that I've seen discuss composition of functions. WolframAlpha can't figure out what I'm telling it; it doesn't seem to think that there's anything to solve.",f'(x+1)=f(x)? y''+y'=e^x,"['calculus', 'ordinary-differential-equations']"
31,How to solve differential equation $dy/dx = y^2/(1+y^2)$ by inegration,How to solve differential equation  by inegration,dy/dx = y^2/(1+y^2),"My first question is, how does one solve the following differential equation: $$y' = y^2/(1+y^2)$$ My second question is, would it be possible to solve this using ordinary integration method, without relying on differential equation methods?","My first question is, how does one solve the following differential equation: $$y' = y^2/(1+y^2)$$ My second question is, would it be possible to solve this using ordinary integration method, without relying on differential equation methods?",,"['calculus', 'ordinary-differential-equations']"
32,global solutions for ODEs,global solutions for ODEs,,"I'm searching for related results for the following problem: Consider the ODE:   $$ x'=-3x+ x^2\log x,\quad x(0)=3/2.  $$   Does a solution $x(t)$ exist on $t\in[0,\infty)$? A quick research on Google returns mainly the local theory such as Picard's existence theorem. Can anybody come up with some references about global existence of solutions to ODEs? In the problem quoted above, the function  $$ f(x)=-3x+ x^2\log x $$ is locally Lipschitz. I'm not sure if this can guarantee the existence of solution on $[0,\infty)$.","I'm searching for related results for the following problem: Consider the ODE:   $$ x'=-3x+ x^2\log x,\quad x(0)=3/2.  $$   Does a solution $x(t)$ exist on $t\in[0,\infty)$? A quick research on Google returns mainly the local theory such as Picard's existence theorem. Can anybody come up with some references about global existence of solutions to ODEs? In the problem quoted above, the function  $$ f(x)=-3x+ x^2\log x $$ is locally Lipschitz. I'm not sure if this can guarantee the existence of solution on $[0,\infty)$.",,['ordinary-differential-equations']
33,Integral of [(1+2y^2)/(3-y)]dy (obtained from a differential equation),Integral of [(1+2y^2)/(3-y)]dy (obtained from a differential equation),,"This question actually arises from this Differential Equations question: Find the family of solutions for: $\displaystyle(1+2y^2)\frac{dy}{dx} + (3-y)\cos x = 0$ I ruled out the methods I've so far learned in class (linear, exact, homogeneous, Bernoulli) and decided that it was a separable equation (correct me if I'm wrong), to reach: $\displaystyle\int\frac{1+2y^2}{3-y}dy = -\int\cos x dx$ Although I like to think of Calculus as a strength of mine, I'm having difficulty with this seemingly simple integral on the left side. I've tried multiple methods of integration by parts, into double/triple integration by parts... What am I missing? Thanks","This question actually arises from this Differential Equations question: Find the family of solutions for: $\displaystyle(1+2y^2)\frac{dy}{dx} + (3-y)\cos x = 0$ I ruled out the methods I've so far learned in class (linear, exact, homogeneous, Bernoulli) and decided that it was a separable equation (correct me if I'm wrong), to reach: $\displaystyle\int\frac{1+2y^2}{3-y}dy = -\int\cos x dx$ Although I like to think of Calculus as a strength of mine, I'm having difficulty with this seemingly simple integral on the left side. I've tried multiple methods of integration by parts, into double/triple integration by parts... What am I missing? Thanks",,"['calculus', 'integration', 'ordinary-differential-equations']"
34,Second order differential equation exercise.,Second order differential equation exercise.,,"Let $y$ be a solution of the equation $$ y''(t)=y(t)-y(t)^3\,. $$ Suppose that $y\in L^2(\mathbb R)$ e $y'\in L^2(\mathbb R)$. 1) Prove that $|y(t)| \leq \sqrt{2}$ for every $t\in \mathbb R$. 2) Prove that either $y(t)=0$ for every $t\in \mathbb R$ or $y(t)$ has constant sign.","Let $y$ be a solution of the equation $$ y''(t)=y(t)-y(t)^3\,. $$ Suppose that $y\in L^2(\mathbb R)$ e $y'\in L^2(\mathbb R)$. 1) Prove that $|y(t)| \leq \sqrt{2}$ for every $t\in \mathbb R$. 2) Prove that either $y(t)=0$ for every $t\in \mathbb R$ or $y(t)$ has constant sign.",,['ordinary-differential-equations']
35,Solving the differential equation $\frac{dy}{dt} = \frac{t+1}{y+1}$,Solving the differential equation,\frac{dy}{dt} = \frac{t+1}{y+1},"I am working on the differential equation $$\frac{dy}{dt} = \frac{t+1}{y+1}, \quad y(1)=2$$ Progress so far:  $$(y+1) \, dy = (t+1)\,dt$$ $$\int y+1 \ dy = \int (t+1)\,dt$$ $$\frac{y^2}{2} + y = \frac{t^2}{2} + t + C, \quad  \langle y=2, t=1\rangle $$ $$ \frac{5}{2} = C$$ I am not sure how to solve for $y$, Thank you for your help! If you can just give me the name of the technique on how to solve for $y$ or how to setup the integral beforehand so I do not get a $y^2$ on the R.H.S that would also be sufficient. Also, most of the example online are simple so I am wondering if there are cases, in general, where we integrate and $y$ is not explicitly defined, such as this one? and can they solved through algebraic manipulation? Or is the function just numerically estimated? Thank you for your insight!","I am working on the differential equation $$\frac{dy}{dt} = \frac{t+1}{y+1}, \quad y(1)=2$$ Progress so far:  $$(y+1) \, dy = (t+1)\,dt$$ $$\int y+1 \ dy = \int (t+1)\,dt$$ $$\frac{y^2}{2} + y = \frac{t^2}{2} + t + C, \quad  \langle y=2, t=1\rangle $$ $$ \frac{5}{2} = C$$ I am not sure how to solve for $y$, Thank you for your help! If you can just give me the name of the technique on how to solve for $y$ or how to setup the integral beforehand so I do not get a $y^2$ on the R.H.S that would also be sufficient. Also, most of the example online are simple so I am wondering if there are cases, in general, where we integrate and $y$ is not explicitly defined, such as this one? and can they solved through algebraic manipulation? Or is the function just numerically estimated? Thank you for your insight!",,['ordinary-differential-equations']
36,Solving a particular system of differential equations,Solving a particular system of differential equations,,"The problem I'm trying to solve is this: $X'(t) \in \mathbb{R}^3 \,, \, \omega = (\omega_1,\omega_2,\omega_3) $ Find the general solution for $$X'(t) = \omega \times X(t)$$ After doing the cross product and rearranging a bit I got to $$\begin{pmatrix} x_1' (t) \\ x_2' (t) \\ x_3' (t) \end{pmatrix} = \begin{pmatrix} 0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 &-\omega_1 \\ -\omega_2 & \omega_1 & 0 \end{pmatrix} \begin{pmatrix} x_1 (t) \\ x_2 (t) \\ x_3 (t) \end{pmatrix}$$ Then I looked for the eigenvalues of the matrix, which are 0, $\sqrt{-\omega_1 ^2 -\omega_2 ^2 -\omega_3 ^2}$ and $-\sqrt{-\omega_1 ^2 -\omega_2 ^2 -\omega_3 ^2}$. Once I have the eigenvectors, I know how to proceed to find the general solution, but finding the eigenvectors of the second and third eigenvalues lead me to really weird looking stuff, which makes me think that there's another way of solving this. I've been thinking about this all day, and I can't think of anything else, any help would be greatly appreciated.","The problem I'm trying to solve is this: $X'(t) \in \mathbb{R}^3 \,, \, \omega = (\omega_1,\omega_2,\omega_3) $ Find the general solution for $$X'(t) = \omega \times X(t)$$ After doing the cross product and rearranging a bit I got to $$\begin{pmatrix} x_1' (t) \\ x_2' (t) \\ x_3' (t) \end{pmatrix} = \begin{pmatrix} 0 & -\omega_3 & \omega_2 \\ \omega_3 & 0 &-\omega_1 \\ -\omega_2 & \omega_1 & 0 \end{pmatrix} \begin{pmatrix} x_1 (t) \\ x_2 (t) \\ x_3 (t) \end{pmatrix}$$ Then I looked for the eigenvalues of the matrix, which are 0, $\sqrt{-\omega_1 ^2 -\omega_2 ^2 -\omega_3 ^2}$ and $-\sqrt{-\omega_1 ^2 -\omega_2 ^2 -\omega_3 ^2}$. Once I have the eigenvectors, I know how to proceed to find the general solution, but finding the eigenvectors of the second and third eigenvalues lead me to really weird looking stuff, which makes me think that there's another way of solving this. I've been thinking about this all day, and I can't think of anything else, any help would be greatly appreciated.",,"['linear-algebra', 'ordinary-differential-equations', 'multivariable-calculus']"
37,Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m $ be differentiable and $K \subset \mathbb{R}^n$ be compact and convex. Show $f$ is Lipschitz on $K$.,Let  be differentiable and  be compact and convex. Show  is Lipschitz on .,f: \mathbb{R}^n \rightarrow \mathbb{R}^m  K \subset \mathbb{R}^n f K,"The Assignment: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m $ be continuously partial differentiable and let $K \subset \mathbb{R}^n$ be compact and convex. Show $f$ is Lipschitz on $K$. A hint my tutor gave me is to use the MVT, which is why I'm trying to get an expression I can use it on. Since $ K $is compact and $f$ is continuously differentiable, the derivative will be bounded since we're in $\mathbb{R}$. I have several problems, firstly I still have no clue how/where to use the MVT and don't know where the fact that K is convex becomes important. (I do know the definition of convex, though.) I'd appreciate any help.","The Assignment: Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^m $ be continuously partial differentiable and let $K \subset \mathbb{R}^n$ be compact and convex. Show $f$ is Lipschitz on $K$. A hint my tutor gave me is to use the MVT, which is why I'm trying to get an expression I can use it on. Since $ K $is compact and $f$ is continuously differentiable, the derivative will be bounded since we're in $\mathbb{R}$. I have several problems, firstly I still have no clue how/where to use the MVT and don't know where the fact that K is convex becomes important. (I do know the definition of convex, though.) I'd appreciate any help.",,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus']"
38,To solve $ \dfrac {dy}{dx}=\dfrac 1{x^2+y^2}$,To solve, \dfrac {dy}{dx}=\dfrac 1{x^2+y^2},How do we solve $ \dfrac {dy}{dx}=\dfrac 1{x^2+y^2}$ ? In general for which positive real $b$ do the equation $\dfrac {dy}{dx}=\dfrac 1{(x^2+y^2)^b}$ admit an analytic solution ?,How do we solve $ \dfrac {dy}{dx}=\dfrac 1{x^2+y^2}$ ? In general for which positive real $b$ do the equation $\dfrac {dy}{dx}=\dfrac 1{(x^2+y^2)^b}$ admit an analytic solution ?,,[]
39,"Jacobian linearization, does it need to be around a hyperbolic fixed-point?","Jacobian linearization, does it need to be around a hyperbolic fixed-point?",,"Everything that I read about Jacobian linearization of systems of nonlinear equations is about approximations near hyperbolic fixed-points (cf. the Hartman-Grobman theorem ). It seems to me that even nonlinear systems, provided that they have a solution, qualify for Taylor expansions. I don't see why a Jacobian linearization of a surface would not hold when evaluated in the neighborhood of any point. What am I not seeing?","Everything that I read about Jacobian linearization of systems of nonlinear equations is about approximations near hyperbolic fixed-points (cf. the Hartman-Grobman theorem ). It seems to me that even nonlinear systems, provided that they have a solution, qualify for Taylor expansions. I don't see why a Jacobian linearization of a surface would not hold when evaluated in the neighborhood of any point. What am I not seeing?",,"['ordinary-differential-equations', 'nonlinear-system']"
40,How can we take the LaPlace transform of a piecewise function?,How can we take the LaPlace transform of a piecewise function?,,"How can we take the LaPlace transform of a function, given piece-wise function notation? For example,  $f(t)=\begin{cases} 0 &\mbox{for } 0<t<2\\ t&\mbox{ for } 2<t \end{cases}$ Frankly, I've read about step-functions but I can't find anything that really breaks down how these should be solved. This question gives hints: Laplace Transformations of a piecewise function But how do we REALLY come up with these integrals?","How can we take the LaPlace transform of a function, given piece-wise function notation? For example,  $f(t)=\begin{cases} 0 &\mbox{for } 0<t<2\\ t&\mbox{ for } 2<t \end{cases}$ Frankly, I've read about step-functions but I can't find anything that really breaks down how these should be solved. This question gives hints: Laplace Transformations of a piecewise function But how do we REALLY come up with these integrals?",,"['integration', 'ordinary-differential-equations', 'laplace-transform']"
41,How to solve this ODE: $\dfrac{\dot{y}}{\sqrt{1+\dot{y}^2}}=c$?,How to solve this ODE: ?,\dfrac{\dot{y}}{\sqrt{1+\dot{y}^2}}=c,"Suppose $\dot{y}$ is the derivative of y with respect to x, i.e. $\dot{y} \equiv  \dfrac{dy}{dx}$. And here comes the question: How to solve the following ODE? $$\dfrac{\dot{y}}{\sqrt{1+\dot{y}^2}}=c$$Where c is constant. I used Mathematica 9.0 to give the answer $y=\pm\dfrac{cx}{\sqrt{1-c^2}}+b(or\,a)$ where a, b are constants. But I want to know how to solve it? Could you help me?","Suppose $\dot{y}$ is the derivative of y with respect to x, i.e. $\dot{y} \equiv  \dfrac{dy}{dx}$. And here comes the question: How to solve the following ODE? $$\dfrac{\dot{y}}{\sqrt{1+\dot{y}^2}}=c$$Where c is constant. I used Mathematica 9.0 to give the answer $y=\pm\dfrac{cx}{\sqrt{1-c^2}}+b(or\,a)$ where a, b are constants. But I want to know how to solve it? Could you help me?",,['ordinary-differential-equations']
42,How to calculate $\begin{cases}y''+4y = \cos{2t}+t^2 \\y(0) = 1;y'(0) = 0\end{cases}$,How to calculate,\begin{cases}y''+4y = \cos{2t}+t^2 \\y(0) = 1;y'(0) = 0\end{cases},"I'm trying to solve this Cauchy Problem $$\begin{cases}y''+4y = \cos{2t}+t^2 \\y(0) = 1;y'(0) = 0\end{cases}$$ So far, i made the following steps: Solutions of the characteristic equation $$\lambda^2+4\lambda = 0$$ $$\lambda_{1} = 0 , \lambda_{2} = -4$$ $$Y_{om}(t) = a_1\cos{2t}+a_2\sin{2t}$$ Looking for the particular solution and then $Y_{gen}(t) = Y_{om}(t)+Y_p(t)$ $$Y_1p(t) = a\cos{2t}+b\sin{2t}$$ $$Y_2p(t) = ct^2+dt+e$$ $$Y_p(t) = a\cos{2t}+b\sin{2t}+ct^2+dt+e$$ Now i should calculate the first derivative and the second derivative of the particular solution and then replace in the equation in order to find the values ​​of the coefficients $$a,b,c,d,e$$ I tried many attempts but i can't get the result. What am I doing wrong?","I'm trying to solve this Cauchy Problem $$\begin{cases}y''+4y = \cos{2t}+t^2 \\y(0) = 1;y'(0) = 0\end{cases}$$ So far, i made the following steps: Solutions of the characteristic equation $$\lambda^2+4\lambda = 0$$ $$\lambda_{1} = 0 , \lambda_{2} = -4$$ $$Y_{om}(t) = a_1\cos{2t}+a_2\sin{2t}$$ Looking for the particular solution and then $Y_{gen}(t) = Y_{om}(t)+Y_p(t)$ $$Y_1p(t) = a\cos{2t}+b\sin{2t}$$ $$Y_2p(t) = ct^2+dt+e$$ $$Y_p(t) = a\cos{2t}+b\sin{2t}+ct^2+dt+e$$ Now i should calculate the first derivative and the second derivative of the particular solution and then replace in the equation in order to find the values ​​of the coefficients $$a,b,c,d,e$$ I tried many attempts but i can't get the result. What am I doing wrong?",,['ordinary-differential-equations']
43,Non-homogeneous 2nd order Euler-Cauchy differential equation,Non-homogeneous 2nd order Euler-Cauchy differential equation,,"I have the following differential equation to solve: $$x^2y''-2y=x^3e^x$$ I've identified the homogeneous equation as an Euler-Cauchy equation, and I've managed to obtain the complementary solution, being: $$y_c=c_1\frac{1}{x}+c_2x^2$$ But I have no idea how to obtain the particular solution. How can I do it?","I have the following differential equation to solve: $$x^2y''-2y=x^3e^x$$ I've identified the homogeneous equation as an Euler-Cauchy equation, and I've managed to obtain the complementary solution, being: $$y_c=c_1\frac{1}{x}+c_2x^2$$ But I have no idea how to obtain the particular solution. How can I do it?",,['ordinary-differential-equations']
44,How do you solve this differential equation?,How do you solve this differential equation?,,"Though I've read questions on this site and really appreciate the quality of the answers, this is my first question, so I hope it follows the site's guidelines. When working with potential energy curves from physics, I came across the equation $a(x(t)) = 6 x(t) + 5$ where $a(x(t))$ denotes acceleration as a function of position, and position is a function of time. I was wondering how one would solve this equation, if possible, to find the particle in question's trajectory as a function of time, i.e. let $x = x(t)$ and solve the equation given that $a(t) = x''(t)$.","Though I've read questions on this site and really appreciate the quality of the answers, this is my first question, so I hope it follows the site's guidelines. When working with potential energy curves from physics, I came across the equation $a(x(t)) = 6 x(t) + 5$ where $a(x(t))$ denotes acceleration as a function of position, and position is a function of time. I was wondering how one would solve this equation, if possible, to find the particle in question's trajectory as a function of time, i.e. let $x = x(t)$ and solve the equation given that $a(t) = x''(t)$.",,"['calculus', 'ordinary-differential-equations', 'physics']"
45,"Does ""$\exists \delta >0$ S.T $||x(0)-x_e||<\delta\Rightarrow \displaystyle \lim_{t\rightarrow \infty}||x(t)-x_e||=0$"" imply stability? [duplicate]","Does "" S.T "" imply stability? [duplicate]",\exists \delta >0 ||x(0)-x_e||<\delta\Rightarrow \displaystyle \lim_{t\rightarrow \infty}||x(t)-x_e||=0,"This question already has answers here : stability and asymptotic stability: unstable but asymptotically convergent solution of nonlinear system (2 answers) Closed 6 years ago . Recall the definition of stable and Asymptotically stable: A fixed point $x_e$ of a vector field is called (Lyapunov) stable if $\forall \varepsilon>0,\exists \delta(\varepsilon)$ such that $\forall ||x(0)-x_e||<\delta \Rightarrow ||x(t)-x_e||<\varepsilon,\forall t\geq 0$ A fixed point $x_e$ of a vector field is called asymptotically stable if it's stable and the condition mentioned in the title ""$\exists \delta >0$ S.T $||x(0)-x_e||<\delta\Rightarrow \displaystyle \lim_{t\rightarrow \infty}=||x(t)-x_e||=0$""  is true. So, my question is the that if the second condition in asymptotically stable implies stable? If not, what counter example can we take? I guess the second condition doesn't imply stable. ""stable"" means I start my flow close to the fixed point $x_e$ and it will be always close but not necessarily convergent to $x_e$. And ""asymptotically stable"" means, besides the stability, the flow will not only be just close to $x_e$ but also converges to it in the end. Thus, from this point of view, the second condition cannot imply ""stable"", because there definitely exists a flow that it will finally converge to $x_e$ but it won't necessarily be close to $x_e$ all the time, which means it might detour for a certain period and come back. However, I have hard time finding such flow. I would really appreciate if you can help me find an example of such flow.","This question already has answers here : stability and asymptotic stability: unstable but asymptotically convergent solution of nonlinear system (2 answers) Closed 6 years ago . Recall the definition of stable and Asymptotically stable: A fixed point $x_e$ of a vector field is called (Lyapunov) stable if $\forall \varepsilon>0,\exists \delta(\varepsilon)$ such that $\forall ||x(0)-x_e||<\delta \Rightarrow ||x(t)-x_e||<\varepsilon,\forall t\geq 0$ A fixed point $x_e$ of a vector field is called asymptotically stable if it's stable and the condition mentioned in the title ""$\exists \delta >0$ S.T $||x(0)-x_e||<\delta\Rightarrow \displaystyle \lim_{t\rightarrow \infty}=||x(t)-x_e||=0$""  is true. So, my question is the that if the second condition in asymptotically stable implies stable? If not, what counter example can we take? I guess the second condition doesn't imply stable. ""stable"" means I start my flow close to the fixed point $x_e$ and it will be always close but not necessarily convergent to $x_e$. And ""asymptotically stable"" means, besides the stability, the flow will not only be just close to $x_e$ but also converges to it in the end. Thus, from this point of view, the second condition cannot imply ""stable"", because there definitely exists a flow that it will finally converge to $x_e$ but it won't necessarily be close to $x_e$ all the time, which means it might detour for a certain period and come back. However, I have hard time finding such flow. I would really appreciate if you can help me find an example of such flow.",,"['analysis', 'ordinary-differential-equations', 'dynamical-systems', 'vector-fields']"
46,Eigenvalues of a Sturm-Liouville problem,Eigenvalues of a Sturm-Liouville problem,,"Consider the problem $y'' + \lambda y = 0$ with the following boundary conditions $y'(0)=0,$ $\,\,y(1)+y'(1)=0$.  Find the normalized eigenfunctions. The normalized eigenfunctions are $\phi(n,x) = k_n \cos \sqrt{\lambda_n}\,x$, where $k_n = \left(\frac{2}{1+\sin^2 \sqrt{\lambda_n}}\right)^{1/2}$ This corresponded to the case where $\lambda > 0$.  For $\lambda < 0,$ there exists only complex solutions and solutions to Sturm-Liouville problems necessarily have real eigenvalues. However, for $\lambda = 0$, I obtain the non trivial solution $y = c_2(1-\frac{1}{2}x)$.  My book says that $\lambda=0$ is not an eigenvalue, and yet I have found a non-trivial solution (i.e one where $c_1,c_2\neq0)$ Why is this? Many thanks.","Consider the problem $y'' + \lambda y = 0$ with the following boundary conditions $y'(0)=0,$ $\,\,y(1)+y'(1)=0$.  Find the normalized eigenfunctions. The normalized eigenfunctions are $\phi(n,x) = k_n \cos \sqrt{\lambda_n}\,x$, where $k_n = \left(\frac{2}{1+\sin^2 \sqrt{\lambda_n}}\right)^{1/2}$ This corresponded to the case where $\lambda > 0$.  For $\lambda < 0,$ there exists only complex solutions and solutions to Sturm-Liouville problems necessarily have real eigenvalues. However, for $\lambda = 0$, I obtain the non trivial solution $y = c_2(1-\frac{1}{2}x)$.  My book says that $\lambda=0$ is not an eigenvalue, and yet I have found a non-trivial solution (i.e one where $c_1,c_2\neq0)$ Why is this? Many thanks.",,['ordinary-differential-equations']
47,Prove Friedrichs' inequality,Prove Friedrichs' inequality,,"I'm trying to show that the theorem ( Friedrichs' inequality ) in my book: Assume that $\Omega$ be a bounded domain of Euclidean space $\Bbb R^n$. Suppose that $u: \Omega \to \Bbb R$ lies in the Sobolev space $W_{0}^{1,p}(\Omega)$. Then $\exists$ const $C_{\Omega}$ such that   $$\|u\|_{L_p(\Omega)} \le C_{\Omega} \left(\int_\Omega\sum_{j=1}^{n}\left| \dfrac{\partial u}{\partial x_j}\right|^p \mathrm{d}x \right)^{\dfrac{1}{p}}$$ ================================================================== I tried to use $\|u\|_{L_p(\Omega)}=\left(\int_\Omega\left |u(x)\right |^p\rm{dx} \right)^{\frac{1}{p}}$. But I have no solution. Can anyone help me? Any help will be appreciated! Thanks!","I'm trying to show that the theorem ( Friedrichs' inequality ) in my book: Assume that $\Omega$ be a bounded domain of Euclidean space $\Bbb R^n$. Suppose that $u: \Omega \to \Bbb R$ lies in the Sobolev space $W_{0}^{1,p}(\Omega)$. Then $\exists$ const $C_{\Omega}$ such that   $$\|u\|_{L_p(\Omega)} \le C_{\Omega} \left(\int_\Omega\sum_{j=1}^{n}\left| \dfrac{\partial u}{\partial x_j}\right|^p \mathrm{d}x \right)^{\dfrac{1}{p}}$$ ================================================================== I tried to use $\|u\|_{L_p(\Omega)}=\left(\int_\Omega\left |u(x)\right |^p\rm{dx} \right)^{\frac{1}{p}}$. But I have no solution. Can anyone help me? Any help will be appreciated! Thanks!",,"['ordinary-differential-equations', 'inequality', 'sobolev-spaces', 'integral-inequality']"
48,Solve the following Diffrential Equation $(3y-7x+7)dx-(3x-7y-3)dy=0$,Solve the following Diffrential Equation,(3y-7x+7)dx-(3x-7y-3)dy=0,"I want to solve the following equation: $$(3y-7x+7)dx-(3x-7y-3)dy=0$$ I will need  two new variables? or I can solve it with 1, for example set expression as $z$? What you are suggesting? thanks.","I want to solve the following equation: $$(3y-7x+7)dx-(3x-7y-3)dy=0$$ I will need  two new variables? or I can solve it with 1, for example set expression as $z$? What you are suggesting? thanks.",,['ordinary-differential-equations']
49,Calculus Proof Question (Differentiable),Calculus Proof Question (Differentiable),,"Prove that if f is a differentiable odd function then f ' is an even function. Hello, I don't completely understand what this question means? What does it mean by f is a differentiable odd function then f (prime) is even function.... aren't both the things the same thing? If someone could just explain this question to me step by step that would be good. No need to solve it. THANKS !","Prove that if f is a differentiable odd function then f ' is an even function. Hello, I don't completely understand what this question means? What does it mean by f is a differentiable odd function then f (prime) is even function.... aren't both the things the same thing? If someone could just explain this question to me step by step that would be good. No need to solve it. THANKS !",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
50,How to calculate Frenet-Serret equations,How to calculate Frenet-Serret equations,,"How to calculate Frenet-Serret equations of the helix $$\gamma : \Bbb R \to \ \Bbb R^3$$ $$\gamma (s) =\left(\cos \left(\frac{s}{\sqrt 2}\right), \sin \left(\frac{s}{\sqrt 2}\right), \left(\frac{s}{\sqrt 2}\right)\right)$$ I know the following info about Frenet-Serret equations: $$\frac{\mathrm{d}}{\mathrm{d}s} \begin{bmatrix} t \\ n \\ b \end{bmatrix} = \begin{bmatrix} 0 & \kappa & 0 \\ - \kappa & 0 & \tau \\ 0 & -\tau & 0 \end{bmatrix}\begin{bmatrix} t \\ n \\ b \end{bmatrix}$$","How to calculate Frenet-Serret equations of the helix $$\gamma : \Bbb R \to \ \Bbb R^3$$ $$\gamma (s) =\left(\cos \left(\frac{s}{\sqrt 2}\right), \sin \left(\frac{s}{\sqrt 2}\right), \left(\frac{s}{\sqrt 2}\right)\right)$$ I know the following info about Frenet-Serret equations: $$\frac{\mathrm{d}}{\mathrm{d}s} \begin{bmatrix} t \\ n \\ b \end{bmatrix} = \begin{bmatrix} 0 & \kappa & 0 \\ - \kappa & 0 & \tau \\ 0 & -\tau & 0 \end{bmatrix}\begin{bmatrix} t \\ n \\ b \end{bmatrix}$$",,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'manifolds', 'self-learning']"
51,second order ODE.,second order ODE.,,"I have this second order ODE: $$-au''+bu'=x^2$$ where $u=u(x)$, $a=\text{constant}>0$, $b=\text{constant}\ge0 $ and  $$u(0)=1 ,u(1)=0$$ I tried to solve it and I get: $$u(x)=D+Ce^{bx/a}$$ where $D,C$ are constant. Is my answer correct? How I can find $D$ and $C$? I realy need help because there are 3 other quistions depending on this one. Thanks.","I have this second order ODE: $$-au''+bu'=x^2$$ where $u=u(x)$, $a=\text{constant}>0$, $b=\text{constant}\ge0 $ and  $$u(0)=1 ,u(1)=0$$ I tried to solve it and I get: $$u(x)=D+Ce^{bx/a}$$ where $D,C$ are constant. Is my answer correct? How I can find $D$ and $C$? I realy need help because there are 3 other quistions depending on this one. Thanks.",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
52,How do I solve: $x'=x^2-1$?,How do I solve: ?,x'=x^2-1,"$x'=x^2-1$; $x(0)=1$ This is what I have done so far: $\frac{dx}{x^2-1}=dx\left(\frac{1}{2(x-1)}-\frac{1}{2(x+1)}\right)=dt$ After integrating both sides: $\frac{1}{2}\log(x-1)-\frac{1}{2}\log(x+1)=t+c$ I then should apply the initial conditions to find c: $\frac{1}{2}\log(1-1)-\frac{1}{2}\log(1+1)=0+c$ $c=\frac{1}{2}\log(0)-\frac{1}{2}\log(2)$ This is where I get a bit confused. Isn't $\log(0)=-\infty$? If so, what does c equal in this case?","$x'=x^2-1$; $x(0)=1$ This is what I have done so far: $\frac{dx}{x^2-1}=dx\left(\frac{1}{2(x-1)}-\frac{1}{2(x+1)}\right)=dt$ After integrating both sides: $\frac{1}{2}\log(x-1)-\frac{1}{2}\log(x+1)=t+c$ I then should apply the initial conditions to find c: $\frac{1}{2}\log(1-1)-\frac{1}{2}\log(1+1)=0+c$ $c=\frac{1}{2}\log(0)-\frac{1}{2}\log(2)$ This is where I get a bit confused. Isn't $\log(0)=-\infty$? If so, what does c equal in this case?",,['ordinary-differential-equations']
53,How to solve the Ordinary Differential Equation: du/dx = cos(u) + 1,How to solve the Ordinary Differential Equation: du/dx = cos(u) + 1,,"I have been trying to solve the ODE $$\frac {du}{dx} = 1 + \cos u. $$ To solve it, I divided through by $1 + \cos u$ to give $$\frac{1} {1 + \cos u } \frac{du}{dx} = 1,$$ and then tried to integrate both sides. However, I have gotten quite stuck when trying to evaluate the integral of $1/ (1 + \cos u )$. I tried multiplying the integrand by $\dfrac{\cos u -1} {\cos u -1}$, but that led to the integral becoming $$ \int \frac{\tan u} {\sin^2u} \, du - \int du$$ and I didn't know how to evaluate the first integral, $$\int \frac{\tan u} {\sin^2u} \,du.$$ So, I would really appreciate it if someone could tell me how to go about solving this ODE. Thanks!","I have been trying to solve the ODE $$\frac {du}{dx} = 1 + \cos u. $$ To solve it, I divided through by $1 + \cos u$ to give $$\frac{1} {1 + \cos u } \frac{du}{dx} = 1,$$ and then tried to integrate both sides. However, I have gotten quite stuck when trying to evaluate the integral of $1/ (1 + \cos u )$. I tried multiplying the integrand by $\dfrac{\cos u -1} {\cos u -1}$, but that led to the integral becoming $$ \int \frac{\tan u} {\sin^2u} \, du - \int du$$ and I didn't know how to evaluate the first integral, $$\int \frac{\tan u} {\sin^2u} \,du.$$ So, I would really appreciate it if someone could tell me how to go about solving this ODE. Thanks!",,"['ordinary-differential-equations', 'integration']"
54,Homogeneous differential equation $\frac{dy}{dx} = \frac{y}{x}$ solution?,Homogeneous differential equation  solution?,\frac{dy}{dx} = \frac{y}{x},I have to solve $\dfrac{dy}{dx} = \dfrac{y}{x}$. So I set $v = \dfrac{y}{x}$ and so $$ \dfrac{dy}{dx} = v $$ Then by product rule $x\dfrac{dv}{dx} + v = v$ and so $x\dfrac{dv}{dx} = 0$. But then that means there is no unique solution to the differential equation; am I wrong in my reasoning? Wolframalpha said the solution was $y(x) = cx$.,I have to solve $\dfrac{dy}{dx} = \dfrac{y}{x}$. So I set $v = \dfrac{y}{x}$ and so $$ \dfrac{dy}{dx} = v $$ Then by product rule $x\dfrac{dv}{dx} + v = v$ and so $x\dfrac{dv}{dx} = 0$. But then that means there is no unique solution to the differential equation; am I wrong in my reasoning? Wolframalpha said the solution was $y(x) = cx$.,,['ordinary-differential-equations']
55,How do you solve this differential equation using variation of parameters?,How do you solve this differential equation using variation of parameters?,,"$\color{green}{question}$: How do you solve this differential equation using variation of parameters? $$y""-\frac{2x}{x^2+1}y'+\frac{2}{x^2+1}y=6(x^2+1)$$ $\color{green}{I~tried}$ . . . $using~the~\color{blue}{Laplace~transform}~method$ . . . $$L[\int_{0}^{\infty }\frac{sinxt}{1+t^{2}}dt]$$ $$=\int_{0}^{\infty }e^{-sx}(\int_{0}^{\infty }\frac{sinxt}{1+t^{2}}dt)$$ $$=\int_{0}^{\infty }\frac{1}{1+t^{2}}(\int_{0}^{\infty }e^{-px}sinxtdx)dt\\\\\\=\int_{0}^{\infty }\frac{1}{1+t^{2}}\frac{t}{s^{2}+t^{2}}dt$$ $$=\int_{0 }^{\infty }\frac{1}{s^{2}-1}(\frac{t}{1+t^{2}}-\frac{t}{s^{2}+t^{2}})$$ $$=\frac{Lns}{s^{2}-1}$$ Is my solution correct? Should I use the inverse Laplace? How can I get a complete and correct answer? Thanks for any hint.","$\color{green}{question}$: How do you solve this differential equation using variation of parameters? $$y""-\frac{2x}{x^2+1}y'+\frac{2}{x^2+1}y=6(x^2+1)$$ $\color{green}{I~tried}$ . . . $using~the~\color{blue}{Laplace~transform}~method$ . . . $$L[\int_{0}^{\infty }\frac{sinxt}{1+t^{2}}dt]$$ $$=\int_{0}^{\infty }e^{-sx}(\int_{0}^{\infty }\frac{sinxt}{1+t^{2}}dt)$$ $$=\int_{0}^{\infty }\frac{1}{1+t^{2}}(\int_{0}^{\infty }e^{-px}sinxtdx)dt\\\\\\=\int_{0}^{\infty }\frac{1}{1+t^{2}}\frac{t}{s^{2}+t^{2}}dt$$ $$=\int_{0 }^{\infty }\frac{1}{s^{2}-1}(\frac{t}{1+t^{2}}-\frac{t}{s^{2}+t^{2}})$$ $$=\frac{Lns}{s^{2}-1}$$ Is my solution correct? Should I use the inverse Laplace? How can I get a complete and correct answer? Thanks for any hint.",,['ordinary-differential-equations']
56,Integral solution of a differential equation (verification),Integral solution of a differential equation (verification),,"I have to verify that the integral $$y(x) = \int_0^\infty \exp\left(-t - \frac{x}{\sqrt{t}}\right) dt$$ satisfies the ODE $$xy''' + 2y = 0$$ ($x > 0$). Differentiating under the integral sign three times gives $$y''' = \int_0^\infty \frac{1}{t \sqrt{t}}\exp\left(-t - \frac{x}{\sqrt{t}}\right)$$ In this form it's not obvious that this satisifes the above equation. I guess I need to use integration by parts to simplify the above integral but I can't quite see how that works. If I let $u = \exp(-t - \frac{x}{\sqrt{t}})$ and $v' = \frac{1}{t \sqrt{t}}$ then $v = \frac{-2}{\sqrt{t}}$ and $u' = \left(-1 + \frac{x}{2t\sqrt{t}}\right)\exp(-t - \frac{x}{\sqrt{t}})$ and the above becomes $$y''' = \int_0^\infty \frac{-2}{\sqrt{t}} \left(-1 + \frac{x}{2t\sqrt{t}}\right)\exp\left(-t - \frac{x}{\sqrt{t}}\right) \, dt$$ and this doesn't seem to work. Can anyone provide me with help? Hopefully I'm not just making a mistake with the differentiation/integration.","I have to verify that the integral $$y(x) = \int_0^\infty \exp\left(-t - \frac{x}{\sqrt{t}}\right) dt$$ satisfies the ODE $$xy''' + 2y = 0$$ ($x > 0$). Differentiating under the integral sign three times gives $$y''' = \int_0^\infty \frac{1}{t \sqrt{t}}\exp\left(-t - \frac{x}{\sqrt{t}}\right)$$ In this form it's not obvious that this satisifes the above equation. I guess I need to use integration by parts to simplify the above integral but I can't quite see how that works. If I let $u = \exp(-t - \frac{x}{\sqrt{t}})$ and $v' = \frac{1}{t \sqrt{t}}$ then $v = \frac{-2}{\sqrt{t}}$ and $u' = \left(-1 + \frac{x}{2t\sqrt{t}}\right)\exp(-t - \frac{x}{\sqrt{t}})$ and the above becomes $$y''' = \int_0^\infty \frac{-2}{\sqrt{t}} \left(-1 + \frac{x}{2t\sqrt{t}}\right)\exp\left(-t - \frac{x}{\sqrt{t}}\right) \, dt$$ and this doesn't seem to work. Can anyone provide me with help? Hopefully I'm not just making a mistake with the differentiation/integration.",,"['calculus', 'ordinary-differential-equations']"
57,Initial value problem $t\frac{dx}{dt}=x+\sqrt{t^2+x^2}$,Initial value problem,t\frac{dx}{dt}=x+\sqrt{t^2+x^2},"Another question on ODEs, this time just wondering how I should start with this one. $$t\frac{dx}{dt}=x+\sqrt{t^2+x^2}, \qquad x(1)=0.$$ It looks like a linear ODE and so after playing around with it I eventually got $$\frac{dx}{dt}-\frac{2}{t}x=\frac{dt}{dx}$$ to which I multiplied the integrating factor $$I(t)=t^{-2}=\frac{1}{t^2}.$$ This in turn gave me $$\frac{x}{t^2}=\int{\frac{1}{t^2}t'dt},$$ and then I used integration by parts to get $$\frac{x}{t^2}=\frac{-1}{t}+C.$$ Applying the initial conditions gave me $$x=t(t-1).$$ The answer in the book is however $$x=\frac{1}{2}(t^2-1).$$ Where did I go wrong? What should I do to solve this problem? Thanks.","Another question on ODEs, this time just wondering how I should start with this one. $$t\frac{dx}{dt}=x+\sqrt{t^2+x^2}, \qquad x(1)=0.$$ It looks like a linear ODE and so after playing around with it I eventually got $$\frac{dx}{dt}-\frac{2}{t}x=\frac{dt}{dx}$$ to which I multiplied the integrating factor $$I(t)=t^{-2}=\frac{1}{t^2}.$$ This in turn gave me $$\frac{x}{t^2}=\int{\frac{1}{t^2}t'dt},$$ and then I used integration by parts to get $$\frac{x}{t^2}=\frac{-1}{t}+C.$$ Applying the initial conditions gave me $$x=t(t-1).$$ The answer in the book is however $$x=\frac{1}{2}(t^2-1).$$ Where did I go wrong? What should I do to solve this problem? Thanks.",,['ordinary-differential-equations']
58,Uniqueness theorem in non authonomous ODE,Uniqueness theorem in non authonomous ODE,,"I am reading (the italian version of) Arnold's book on Ordinary Differential Equation. On page 29 (Chapter 1, paragraph 2: vectorial fields on the real line) problem 2 says: Given the differential equation $\dot x = v(x,t)$, where $v$ is a differentiable function, if there exists a solution $x = \varphi(t)$ which verifies the intial condition $\varphi(t_0)=x_0$, prove its unicity. Arnold gives an indication : let $y = x-\varphi(t)$, confront $y$ with a convenient equation of the form $\dot x = kx$, $k\ne 0$. My question is: how does the substitution $y=x-\varphi(t)$ works? Isn't $x$ equal to $\varphi(t)$? Should this substitution transform the non-authonomous equation in an authonomous one? How?","I am reading (the italian version of) Arnold's book on Ordinary Differential Equation. On page 29 (Chapter 1, paragraph 2: vectorial fields on the real line) problem 2 says: Given the differential equation $\dot x = v(x,t)$, where $v$ is a differentiable function, if there exists a solution $x = \varphi(t)$ which verifies the intial condition $\varphi(t_0)=x_0$, prove its unicity. Arnold gives an indication : let $y = x-\varphi(t)$, confront $y$ with a convenient equation of the form $\dot x = kx$, $k\ne 0$. My question is: how does the substitution $y=x-\varphi(t)$ works? Isn't $x$ equal to $\varphi(t)$? Should this substitution transform the non-authonomous equation in an authonomous one? How?",,['ordinary-differential-equations']
59,Differential equation of a mass on a spring,Differential equation of a mass on a spring,,"I have the following differential equation which is motivated by the dynamics of a mass on a spring: \begin{equation} my'' - ky = 0 \end{equation} I split this into a system of equations by letting $x_1=y$  and $x_2=y'$ \begin{equation} x' = \begin{pmatrix} 0&1\\\dfrac{k}{m}& 0\end{pmatrix}x \end{equation} Find an Eigenvalue: \begin{align} \det(A-\lambda I) =& 0 \\ \det\begin{pmatrix} -\lambda&1\\\dfrac{k}{m}&-\lambda\end{pmatrix} =& 0 \\ \lambda^2 - \dfrac{k}{m} =& 0 \\ \lambda = \pm\sqrt{\dfrac{k}{m}} \end{align} Find the matching eigenvector: \begin{align} (A-\lambda I)x =& 0 \\ \begin{pmatrix} -\sqrt{\frac{k}{m}}&1\\\dfrac{k}{m}&-\sqrt{\frac{k}{m}}\end{pmatrix}x =& 0 \end{align} This matrix is similar to: \begin{equation} \begin{pmatrix}-\sqrt{\frac{k}{m}}&1\\0&0\end{pmatrix}x = 0 \end{equation} $x_2$ is free so let $x_2$ = 1. We have: \begin{align} -\sqrt{\frac{k}{m}}x_1 + 1 =& 0 \\ x_1 =& \dfrac{1}{\sqrt{\dfrac{k}{m}}} \end{align} So the corresponding eigenvector is  $\begin{pmatrix}\dfrac{1}{\sqrt{\dfrac{k}{m}}}\\1\end{pmatrix}$. Any solution to the vector differential equation $x'=Ax$ is $x = e^{\lambda t}x_0$ where $\lambda$ is an eigenvalue and $x_0$ is the corresponding eigenvector. Our solution is then  \begin{equation} e^{\sqrt{\frac{k}{m}} t}\begin{pmatrix}\dfrac{1}{\sqrt{\dfrac{k}{m}}}\\1\end{pmatrix} \end{equation} I am just using technique I learned in a differential equations book. This doesn't make sense from a physics stand point where solutions should be periodic and thus would have solutions in terms of sines and cosine. Perhaps I did not setup the system correctly, because if I got imaginary eigenvalues then I could have had sines and cosine in my solution. I want to know what I did wrong, let me know what you think.","I have the following differential equation which is motivated by the dynamics of a mass on a spring: \begin{equation} my'' - ky = 0 \end{equation} I split this into a system of equations by letting $x_1=y$  and $x_2=y'$ \begin{equation} x' = \begin{pmatrix} 0&1\\\dfrac{k}{m}& 0\end{pmatrix}x \end{equation} Find an Eigenvalue: \begin{align} \det(A-\lambda I) =& 0 \\ \det\begin{pmatrix} -\lambda&1\\\dfrac{k}{m}&-\lambda\end{pmatrix} =& 0 \\ \lambda^2 - \dfrac{k}{m} =& 0 \\ \lambda = \pm\sqrt{\dfrac{k}{m}} \end{align} Find the matching eigenvector: \begin{align} (A-\lambda I)x =& 0 \\ \begin{pmatrix} -\sqrt{\frac{k}{m}}&1\\\dfrac{k}{m}&-\sqrt{\frac{k}{m}}\end{pmatrix}x =& 0 \end{align} This matrix is similar to: \begin{equation} \begin{pmatrix}-\sqrt{\frac{k}{m}}&1\\0&0\end{pmatrix}x = 0 \end{equation} $x_2$ is free so let $x_2$ = 1. We have: \begin{align} -\sqrt{\frac{k}{m}}x_1 + 1 =& 0 \\ x_1 =& \dfrac{1}{\sqrt{\dfrac{k}{m}}} \end{align} So the corresponding eigenvector is  $\begin{pmatrix}\dfrac{1}{\sqrt{\dfrac{k}{m}}}\\1\end{pmatrix}$. Any solution to the vector differential equation $x'=Ax$ is $x = e^{\lambda t}x_0$ where $\lambda$ is an eigenvalue and $x_0$ is the corresponding eigenvector. Our solution is then  \begin{equation} e^{\sqrt{\frac{k}{m}} t}\begin{pmatrix}\dfrac{1}{\sqrt{\dfrac{k}{m}}}\\1\end{pmatrix} \end{equation} I am just using technique I learned in a differential equations book. This doesn't make sense from a physics stand point where solutions should be periodic and thus would have solutions in terms of sines and cosine. Perhaps I did not setup the system correctly, because if I got imaginary eigenvalues then I could have had sines and cosine in my solution. I want to know what I did wrong, let me know what you think.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
60,Did I solve this System of differential equations right?,Did I solve this System of differential equations right?,,"My Problem is this given System of differential equations. $$y_{1}^{\prime}=5y_{1}+2y_{2} \\ y_{2}^{\prime}=-2y_{1}+y_{2}$$ I am looking for the solution. According to one of my earlier Questions , I tried the method on my own. Now i fear the solution could be wrong. (especially the eigenvectors) My Approach was: again, i analyze, it must be a ordinary, linear System of equations, with both being of first-order.  Than i built the corresponding Matrix as follows: $$\underbrace{\pmatrix{ y_1^{\prime} \\ y_2^{\prime}}}_{\large{ {\vec y^{\prime}}}} = \underbrace{\pmatrix{5 & 2 \\ -2 & 1}}_{\large{\mathbf A}}\underbrace{\pmatrix{y_1\\y_2}}_{\large{\vec y}}$$ that's why: $$\vec y^{\prime} = \pmatrix{5 & 2 \\ -2 & 1}\vec y$$ Then I determined the eigenvalues: they are $r_1 = 3$ and $r_2=3$ Knowing them, I can build the corresponding eigenvectors: they are $\vec v_1 = \pmatrix{ -1 \\ +1}$ and $\vec v_2 = \pmatrix{ 0 \\ 0}$ Now i plug into the equation: $$\vec{x} = c_1e^{r_1t}\vec{v_1}+c_2e^{r_2t}\vec{v_2} \\ \vec{x} = c_1e^{3t}\pmatrix{-1 \\ 1}+c_2e^{3t}\pmatrix{0 \\ 0}$$ this lead to my result: $$y_1 = -c_1e^{3t} + 0c_2e^{3t}\\ y_2 = c_1e^{3t} + 0c_2e^{3t} \\ \\ y_1 = -c_1e^{3t}\\ y_2 = c_1e^{3t}$$ But I doubt it's correct. My suspect are the eigenvectors, I really don't know if they are correct. And this could have lead to a wrong solution. P.S.: Edits were made to improve language and latex","My Problem is this given System of differential equations. $$y_{1}^{\prime}=5y_{1}+2y_{2} \\ y_{2}^{\prime}=-2y_{1}+y_{2}$$ I am looking for the solution. According to one of my earlier Questions , I tried the method on my own. Now i fear the solution could be wrong. (especially the eigenvectors) My Approach was: again, i analyze, it must be a ordinary, linear System of equations, with both being of first-order.  Than i built the corresponding Matrix as follows: $$\underbrace{\pmatrix{ y_1^{\prime} \\ y_2^{\prime}}}_{\large{ {\vec y^{\prime}}}} = \underbrace{\pmatrix{5 & 2 \\ -2 & 1}}_{\large{\mathbf A}}\underbrace{\pmatrix{y_1\\y_2}}_{\large{\vec y}}$$ that's why: $$\vec y^{\prime} = \pmatrix{5 & 2 \\ -2 & 1}\vec y$$ Then I determined the eigenvalues: they are $r_1 = 3$ and $r_2=3$ Knowing them, I can build the corresponding eigenvectors: they are $\vec v_1 = \pmatrix{ -1 \\ +1}$ and $\vec v_2 = \pmatrix{ 0 \\ 0}$ Now i plug into the equation: $$\vec{x} = c_1e^{r_1t}\vec{v_1}+c_2e^{r_2t}\vec{v_2} \\ \vec{x} = c_1e^{3t}\pmatrix{-1 \\ 1}+c_2e^{3t}\pmatrix{0 \\ 0}$$ this lead to my result: $$y_1 = -c_1e^{3t} + 0c_2e^{3t}\\ y_2 = c_1e^{3t} + 0c_2e^{3t} \\ \\ y_1 = -c_1e^{3t}\\ y_2 = c_1e^{3t}$$ But I doubt it's correct. My suspect are the eigenvectors, I really don't know if they are correct. And this could have lead to a wrong solution. P.S.: Edits were made to improve language and latex",,"['ordinary-differential-equations', 'integration', 'systems-of-equations']"
61,I need help solving a ODE converting it to system of linear different equations,I need help solving a ODE converting it to system of linear different equations,,"$y'''+4y''+5y'+2y=10cos(t)$ Where it is subject to a condition $y''(0)=3, y'(0)=0, y(0)=0$ Ok I'm having a very hard time to solving this b/c if one is going to solve using system of linear different equations don't we need a forcing term. I have solved for the Yc which is $Y_c=c_1v_1e^{-t}+c_2(v_1+v_2t)e^{-t}+c_3v_3e^{-2t} $ and my lambda= -1,-1,-2 and my vectors are $v_1=[1,-1,1] v_2=[1,0,-1] v_3=[1,-2,4]$ but for the Yp I don't know how solve without forcing terms. Am I doing something wrong? Help would be much appreciated. Thank You!","$y'''+4y''+5y'+2y=10cos(t)$ Where it is subject to a condition $y''(0)=3, y'(0)=0, y(0)=0$ Ok I'm having a very hard time to solving this b/c if one is going to solve using system of linear different equations don't we need a forcing term. I have solved for the Yc which is $Y_c=c_1v_1e^{-t}+c_2(v_1+v_2t)e^{-t}+c_3v_3e^{-2t} $ and my lambda= -1,-1,-2 and my vectors are $v_1=[1,-1,1] v_2=[1,0,-1] v_3=[1,-2,4]$ but for the Yp I don't know how solve without forcing terms. Am I doing something wrong? Help would be much appreciated. Thank You!",,['ordinary-differential-equations']
62,"Given the solution for a differential equation, find the corresponding differential equation","Given the solution for a differential equation, find the corresponding differential equation",,The solution of a differential equation is: $$\left \{ \begin{array}{lcl} x_1(t) & = & e^{-2t}-e^{-5t} \\ x_2(t) & = & e^{-2t}+e^{-3t}+e^{-5t} \\ x_3(t) & = & e^{-3t} + e^{-5t} \end{array}\right .$$ with initial conditions $$\left \{ \begin{array}{lcl} x_1(0) & = & 0 \\ x_2(0) & = & 3 \\ x_3(0) & = & 2 \end{array}\right .$$ What is the corresponding differential equation? I had worked another linear algebra question using back substitution and I know that is what I am going to need to do here.  In the previous problem I was substituting $A=QR$ and finding $\mathcal A$.  Just not sure which approach to use in this situation. $Ax=b$ $A=x^{-1}b$ Any help is greatly appreciated.  :),The solution of a differential equation is: $$\left \{ \begin{array}{lcl} x_1(t) & = & e^{-2t}-e^{-5t} \\ x_2(t) & = & e^{-2t}+e^{-3t}+e^{-5t} \\ x_3(t) & = & e^{-3t} + e^{-5t} \end{array}\right .$$ with initial conditions $$\left \{ \begin{array}{lcl} x_1(0) & = & 0 \\ x_2(0) & = & 3 \\ x_3(0) & = & 2 \end{array}\right .$$ What is the corresponding differential equation? I had worked another linear algebra question using back substitution and I know that is what I am going to need to do here.  In the previous problem I was substituting $A=QR$ and finding $\mathcal A$.  Just not sure which approach to use in this situation. $Ax=b$ $A=x^{-1}b$ Any help is greatly appreciated.  :),,"['linear-algebra', 'ordinary-differential-equations']"
63,Is it possible to solve this PDE,Is it possible to solve this PDE,,It would be pretty sweet if I could solve this for $A$.  Is it possible? $$\frac{dA}{dx}+\frac{dA}{d\tau}=wx\tau$$ where $w$ is a constant and $x$ is a function of $\tau$. It might help that it is also known that: $$w\tau=\frac{d^2A}{dx^2}$$ So the equation to be solved could also be expressed: $$\frac{dA}{dx}+\frac{dA}{d\tau}=\frac{d^2A}{dx^2}x$$,It would be pretty sweet if I could solve this for $A$.  Is it possible? $$\frac{dA}{dx}+\frac{dA}{d\tau}=wx\tau$$ where $w$ is a constant and $x$ is a function of $\tau$. It might help that it is also known that: $$w\tau=\frac{d^2A}{dx^2}$$ So the equation to be solved could also be expressed: $$\frac{dA}{dx}+\frac{dA}{d\tau}=\frac{d^2A}{dx^2}x$$,,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
64,Differential equation on $\Bbb R$,Differential equation on,\Bbb R,"We have a differential equation on $\Bbb R$ of the form  $$\frac {d^2}{dx^2}u = \chi_{[0,1]},$$  where $\chi_{[0,1]}$ is the characteristic function of the interval $[0, 1] ⊂ \Bbb R$. I want to find a generalized solution for this differential equation. I also want to know that will the solution be unique? Definition: If $\Omega$ is a domain in $\Bbb R^n$. We say that $u\in \mathcal D'(\Bbb R^n)$ is a generalized solution of $$\sum_{|\alpha|\le m} a_\alpha D^\alpha u=f(x)$$ in $\Omega.$ If $u$ satisfies $$\sum_{|\alpha|\le m} a_\alpha \langle D^\alpha u,\phi\rangle =\langle f(x),\phi\rangle $$ for every $\phi\in\mathcal D(\Omega).$  Here $f\in\mathcal D'(\Bbb R^n)$ and constant coefficients  $a_\alpha\in \Bbb R^n $. Thank you.","We have a differential equation on $\Bbb R$ of the form  $$\frac {d^2}{dx^2}u = \chi_{[0,1]},$$  where $\chi_{[0,1]}$ is the characteristic function of the interval $[0, 1] ⊂ \Bbb R$. I want to find a generalized solution for this differential equation. I also want to know that will the solution be unique? Definition: If $\Omega$ is a domain in $\Bbb R^n$. We say that $u\in \mathcal D'(\Bbb R^n)$ is a generalized solution of $$\sum_{|\alpha|\le m} a_\alpha D^\alpha u=f(x)$$ in $\Omega.$ If $u$ satisfies $$\sum_{|\alpha|\le m} a_\alpha \langle D^\alpha u,\phi\rangle =\langle f(x),\phi\rangle $$ for every $\phi\in\mathcal D(\Omega).$  Here $f\in\mathcal D'(\Bbb R^n)$ and constant coefficients  $a_\alpha\in \Bbb R^n $. Thank you.",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'distribution-theory']"
65,Stiff differential equation where Runge-Kutta $4$th order method can be broken,Stiff differential equation where Runge-Kutta th order method can be broken,4,"Is there a stiff differential equation that cannot be solved by the Runge-Kutta 4th order method, but which has an analytical solution for testing?","Is there a stiff differential equation that cannot be solved by the Runge-Kutta 4th order method, but which has an analytical solution for testing?",,"['ordinary-differential-equations', 'numerical-methods']"
66,Second order linear homogeneous ODE with constant coefficients,Second order linear homogeneous ODE with constant coefficients,,"In homework I was asked to find all solutions to the following ODE: $$x''+ax'+bx = 0$$ After reading, I know the following. (a) If $t^2+at+b = (t-\lambda_1)(t-\lambda_2)$ with $\lambda_1\ne\lambda_2$, then $$x(t) = c_1e^{\lambda_1 t}+c_2e^{\lambda_2 t} $$  is the general solution. (b) If $t^2+at+b = (t-\lambda)^2$ then $x(t) = (c_1+c_2t)e^{\lambda t}$ is the general solution. EDIT I know why and when these are solutions, but this is not my question. My question is how to show if $x(t)$ satisfies this equation, then it must be in one of the two forms. Not that these two forms are solutions. EDIT Set $y = (x,x')$, then $y' = F(y) = (y(2),-a\cdot y(2)-b\cdot y(1))$ If I know $F(y)$ is locally Lipschitz then by Picard–Lindelöf theorem solution is unique.","In homework I was asked to find all solutions to the following ODE: $$x''+ax'+bx = 0$$ After reading, I know the following. (a) If $t^2+at+b = (t-\lambda_1)(t-\lambda_2)$ with $\lambda_1\ne\lambda_2$, then $$x(t) = c_1e^{\lambda_1 t}+c_2e^{\lambda_2 t} $$  is the general solution. (b) If $t^2+at+b = (t-\lambda)^2$ then $x(t) = (c_1+c_2t)e^{\lambda t}$ is the general solution. EDIT I know why and when these are solutions, but this is not my question. My question is how to show if $x(t)$ satisfies this equation, then it must be in one of the two forms. Not that these two forms are solutions. EDIT Set $y = (x,x')$, then $y' = F(y) = (y(2),-a\cdot y(2)-b\cdot y(1))$ If I know $F(y)$ is locally Lipschitz then by Picard–Lindelöf theorem solution is unique.",,['ordinary-differential-equations']
67,"$y''=y$, $x''=-x$. Write this equation in terms of the first-order system",", . Write this equation in terms of the first-order system",y''=y x''=-x,I'm having trouble with my homework on higher-order equations and their equivalent systems. :( This is the problem: Write this equation in terms of the first-order system. $$\left\{ \begin{array}{l} \frac{d^2x}{dt^2}=-x \\ \frac{d^2y}{dt^2}=y \end{array} \right.$$ (Hint: Write each second-order equation as two first-order equations.,I'm having trouble with my homework on higher-order equations and their equivalent systems. :( This is the problem: Write this equation in terms of the first-order system. $$\left\{ \begin{array}{l} \frac{d^2x}{dt^2}=-x \\ \frac{d^2y}{dt^2}=y \end{array} \right.$$ (Hint: Write each second-order equation as two first-order equations.,,"['linear-algebra', 'ordinary-differential-equations']"
68,What's a singular differential equation?,What's a singular differential equation?,,"I'm reading something on Bessel functions of first-second kind as the solutions to the Bessel diff. equation, and the difference, according to the text, is whether the function is singular or not at the origin. What does that exactly mean for a diff. equation of the form: $$x^2y_{xx}+xy_x+(x^2-n^2)y=0$$ to be singular? Specifically, I want to write the solution of a cylindrical symmetrical wave as bessel functions.","I'm reading something on Bessel functions of first-second kind as the solutions to the Bessel diff. equation, and the difference, according to the text, is whether the function is singular or not at the origin. What does that exactly mean for a diff. equation of the form: $$x^2y_{xx}+xy_x+(x^2-n^2)y=0$$ to be singular? Specifically, I want to write the solution of a cylindrical symmetrical wave as bessel functions.",,['ordinary-differential-equations']
69,How to solve the following differential equation,How to solve the following differential equation,,We have the following DE: $$ \dfrac{dy}{dx} = \dfrac{x^2 + 3y^2}{2xy}$$ I don't know how to solve this. I know we need to write it as $y/x$ but I don't know how to in this case.,We have the following DE: $$ \dfrac{dy}{dx} = \dfrac{x^2 + 3y^2}{2xy}$$ I don't know how to solve this. I know we need to write it as $y/x$ but I don't know how to in this case.,,['ordinary-differential-equations']
70,Pursuit curves solution,Pursuit curves solution,,"For our math class we have to do some calculations with respect to pursuit curves. The chased object starts at point $(p,0)$ . Chaser starts at $(0,0)$ .(x,y) Speed of the chased object is $u$ . Speed chaser = $v$ . We have that for the chaser $$ \frac{dy}{dx}=\frac{ut-y}{p-x} $$ Then the length of the path is $$ s = \int \sqrt{1+(\frac{dy}{dx})^2}=vt=\frac{vy}{u}-\frac{v(p-x)dy}{u\times dx}$$ the first derivative of both sides gives $$  -\frac{u}{v}\sqrt{1+(\frac{dy}{dx})^2}=\frac{(p-x)d(\frac{dy}{dx})}{dx} $$ Now, we are asked to:","For our math class we have to do some calculations with respect to pursuit curves. The chased object starts at point . Chaser starts at .(x,y) Speed of the chased object is . Speed chaser = . We have that for the chaser Then the length of the path is the first derivative of both sides gives Now, we are asked to:","(p,0) (0,0) u v  \frac{dy}{dx}=\frac{ut-y}{p-x}   s = \int \sqrt{1+(\frac{dy}{dx})^2}=vt=\frac{vy}{u}-\frac{v(p-x)dy}{u\times dx}   -\frac{u}{v}\sqrt{1+(\frac{dy}{dx})^2}=\frac{(p-x)d(\frac{dy}{dx})}{dx} ","['calculus', 'ordinary-differential-equations', 'integration', 'derivatives']"
71,Existence and Uniqueness of solutions,Existence and Uniqueness of solutions,,"I'm pretty confused about this topic in Differential Equations.  It's a simple topic, but I just can't get the gist of it. Here are two examples that I would like for you to explain to me $\dfrac{dy}{dx} = y^\frac{1}{3},\quad  y(0) = 1$ $\dfrac{dy}{dx} = y^\frac{1}{3},\quad  y(0) = 0$ for number 1 a unique solution exist near $x = 0$, but for 2 uniqueness is not guaranteed. Please explain, boggles my mind.","I'm pretty confused about this topic in Differential Equations.  It's a simple topic, but I just can't get the gist of it. Here are two examples that I would like for you to explain to me $\dfrac{dy}{dx} = y^\frac{1}{3},\quad  y(0) = 1$ $\dfrac{dy}{dx} = y^\frac{1}{3},\quad  y(0) = 0$ for number 1 a unique solution exist near $x = 0$, but for 2 uniqueness is not guaranteed. Please explain, boggles my mind.",,['ordinary-differential-equations']
72,Solving system of linear differential equations by eigenvalues,Solving system of linear differential equations by eigenvalues,,"Using eigenvalues and eigenvectors solve system of differential equations: $$x_1'=x_1+2x_2$$ $$x_2' = 2x_1+x_2$$ And find solution for the initial conditions: $x_1(0) = 1; x_2(0) = -1$ I tried to solve it, but I don't have right results, so I can't check my solution. I would like someone to write how he would solve it and what results would he get.","Using eigenvalues and eigenvectors solve system of differential equations: $$x_1'=x_1+2x_2$$ $$x_2' = 2x_1+x_2$$ And find solution for the initial conditions: $x_1(0) = 1; x_2(0) = -1$ I tried to solve it, but I don't have right results, so I can't check my solution. I would like someone to write how he would solve it and what results would he get.",,"['linear-algebra', 'ordinary-differential-equations']"
73,Solution of Ordinary Differential equation,Solution of Ordinary Differential equation,,I am a bit stuck with this one. $y' = \displaystyle \frac{ax + by}{cx+dy}$ under the assumption that $ad \neq cb$,I am a bit stuck with this one. $y' = \displaystyle \frac{ax + by}{cx+dy}$ under the assumption that $ad \neq cb$,,['calculus']
74,When to use Frobenius method,When to use Frobenius method,,"Am I correct in believing Frobenius' method is simply a general case of a power series solution? For instance, if the Differential Equation has a regular singular point, would you be forced to use Frobenius' method instead of a regular power series?","Am I correct in believing Frobenius' method is simply a general case of a power series solution? For instance, if the Differential Equation has a regular singular point, would you be forced to use Frobenius' method instead of a regular power series?",,"['sequences-and-series', 'ordinary-differential-equations']"
75,solve the differential equation : $(x^2+xy)\frac{dy}{dx}-(3xy+y^2)=0$,solve the differential equation :,(x^2+xy)\frac{dy}{dx}-(3xy+y^2)=0,"$(x^2+xy)\displaystyle\frac{dy}{dx}-(3xy+y^2)=0$ Here is my idea ,  $\displaystyle\frac{dy}{dx}=\displaystyle\frac{y}{x}-\displaystyle\frac{2x}{x+y}+2$ Let $t=\displaystyle\frac{y}{x} $ , then $RHS=t-\displaystyle\frac{2}{1+t}+2$ But I don't know how to do the LHS Is that a right way to solve this question?? If the idea is wrong , please teach me. Thanks a lot","$(x^2+xy)\displaystyle\frac{dy}{dx}-(3xy+y^2)=0$ Here is my idea ,  $\displaystyle\frac{dy}{dx}=\displaystyle\frac{y}{x}-\displaystyle\frac{2x}{x+y}+2$ Let $t=\displaystyle\frac{y}{x} $ , then $RHS=t-\displaystyle\frac{2}{1+t}+2$ But I don't know how to do the LHS Is that a right way to solve this question?? If the idea is wrong , please teach me. Thanks a lot",,['ordinary-differential-equations']
76,Solving the time-independent Schrodinger equation for particle in a potential well,Solving the time-independent Schrodinger equation for particle in a potential well,,"I'm solving a quantum mechanics problem for the particle in a potential well, and the equation I have to solve is $$\frac{d^2\psi}{dx^2}+k\psi=0$$where $$k=\frac{2mE}{\hbar^2}$$ This seems easy enough to solve. It is a second order linear differential equation with constant coefficient of the form $$a\psi''(x)+b\psi'(x)+c\psi(x)=0$$, so I thought we were to use the characteristic equation $$ar^2+br+c=0$$and solve for roots $r_1$ and $r_2$. Doing that, I get $$r^2+k=0$$and therefore $$r=\pm \sqrt{k}$$ The general solution is given by $$\psi(x)=\exp\left(\sqrt{\frac{2mE}{\hbar^2}}x\right)+\exp\left(-\sqrt{\frac{2mE}{\hbar^2}}x\right)$$ However, when I refer to Griffiths' Introduction to Quantum Mechanics, he finds the general solution to be $$\psi(x)=A\sin kx+B\cos kx$$ Where have I gone wrong? Thanks.","I'm solving a quantum mechanics problem for the particle in a potential well, and the equation I have to solve is $$\frac{d^2\psi}{dx^2}+k\psi=0$$where $$k=\frac{2mE}{\hbar^2}$$ This seems easy enough to solve. It is a second order linear differential equation with constant coefficient of the form $$a\psi''(x)+b\psi'(x)+c\psi(x)=0$$, so I thought we were to use the characteristic equation $$ar^2+br+c=0$$and solve for roots $r_1$ and $r_2$. Doing that, I get $$r^2+k=0$$and therefore $$r=\pm \sqrt{k}$$ The general solution is given by $$\psi(x)=\exp\left(\sqrt{\frac{2mE}{\hbar^2}}x\right)+\exp\left(-\sqrt{\frac{2mE}{\hbar^2}}x\right)$$ However, when I refer to Griffiths' Introduction to Quantum Mechanics, he finds the general solution to be $$\psi(x)=A\sin kx+B\cos kx$$ Where have I gone wrong? Thanks.",,"['ordinary-differential-equations', 'quantum-mechanics']"
77,Can $y''=1/y$ be solved without numerical way?,Can  be solved without numerical way?,y''=1/y,Is there  any way to solve it without numerical way?? $$ \frac{d^2 y}{d x^2}= \frac{1}{y}$$ thanks in advance!!,Is there  any way to solve it without numerical way?? $$ \frac{d^2 y}{d x^2}= \frac{1}{y}$$ thanks in advance!!,,['ordinary-differential-equations']
78,Problem with nonhomogeneous ODE,Problem with nonhomogeneous ODE,,"I'm trying to find the general solution for the ODE $$f''(t)+f(t)=\frac16\sin^3t,\tag{1}$$ but something keeps going wrong. I first set $x_1=f$, $x_2=x_1'$, so letting $$A=\left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right],\quad x=\left[\begin{array}{c}x_1\\x_2\end{array}\right],\quad\text{and}\quad b=\left[\begin{array}{c}0\\\frac16\sin^3t\end{array}\right],$$ we have that $(1)$ is equivalent to the nonhomogeneous linear system $$x'=Ax+b.\tag{2}$$ Observing that $[i,-1]^T$ and $[i,1]^T$ are eigenvectors of $A$ corresponding (respectively) to the eigenvalues $i,-i$, I concluded (and confirmed) that the general solution to the system $x'=Ax$ has the form $$d_1e^{it}\left[\begin{array}{c}i\\-1\end{array}\right]+d_2e^{-it}\left[\begin{array}{c}i\\1\end{array}\right]$$ for some constants $d_1,d_2$. Equivalently, if $$\Phi=\left[\begin{array}{cc}ie^{it} & ie^{-it}\\-e^{it} & e^{-it}\end{array}\right]\quad\text{and}\quad d=\left[\begin{array}{c}d_1\\d_2\end{array}\right]$$ for some constants $d_1,d_2$, then $x=\Phi d$ is a solution to $x'=Ax$. Now, suppose that $\hat c=[\hat c_1,\hat c_2]^T$, where $\hat c_1=\hat c_1(t),\hat c_2=\hat c_2(t)$ are differentiable functions with constant term $0$, and let $\hat x=\Phi\hat c+\Phi d$ for some constant vector $d$. Noting that $$A\Phi=\left[\begin{array}{cc}-e^{it} & e^{-it}\\-ie^{it} & -ie^{-it}\end{array}\right]=\Phi',$$ it follows that $$(\Phi\hat c)'=\Phi'\hat c+\Phi\hat c'=A\Phi\hat c+\Phi\hat c',$$ so since $A\Phi d=(\Phi d)'$ by the work done with the homogeneous system, then $$\hat x'=(\Phi\hat c)'+(\Phi d)'=A\Phi\hat c+\Phi\hat c'+A\Phi d=A\hat x+\Phi\hat c'.$$ Thus, $$\hat x'=A\hat x+b\quad\text{if and only if}\quad\Phi\hat c'=b.$$ Now, $\Phi$ is invertible, so letting $c$ be the antiderivative (with respect to $t$) of $\Phi^{-1}b$ without integration constant, we should have that $x=\Phi c+\Phi d$ is the general solution to $(2)$, and then $x_1$ would be the general solution to $(1)$, yes? If that's all good, then I'm apparently just making a calculation error when taking the antiderivative or doing the subsequent matrix operations.... Edit : Does anybody have a different, arguably better approach to take for this problem?","I'm trying to find the general solution for the ODE $$f''(t)+f(t)=\frac16\sin^3t,\tag{1}$$ but something keeps going wrong. I first set $x_1=f$, $x_2=x_1'$, so letting $$A=\left[\begin{array}{cc}0 & 1\\-1 & 0\end{array}\right],\quad x=\left[\begin{array}{c}x_1\\x_2\end{array}\right],\quad\text{and}\quad b=\left[\begin{array}{c}0\\\frac16\sin^3t\end{array}\right],$$ we have that $(1)$ is equivalent to the nonhomogeneous linear system $$x'=Ax+b.\tag{2}$$ Observing that $[i,-1]^T$ and $[i,1]^T$ are eigenvectors of $A$ corresponding (respectively) to the eigenvalues $i,-i$, I concluded (and confirmed) that the general solution to the system $x'=Ax$ has the form $$d_1e^{it}\left[\begin{array}{c}i\\-1\end{array}\right]+d_2e^{-it}\left[\begin{array}{c}i\\1\end{array}\right]$$ for some constants $d_1,d_2$. Equivalently, if $$\Phi=\left[\begin{array}{cc}ie^{it} & ie^{-it}\\-e^{it} & e^{-it}\end{array}\right]\quad\text{and}\quad d=\left[\begin{array}{c}d_1\\d_2\end{array}\right]$$ for some constants $d_1,d_2$, then $x=\Phi d$ is a solution to $x'=Ax$. Now, suppose that $\hat c=[\hat c_1,\hat c_2]^T$, where $\hat c_1=\hat c_1(t),\hat c_2=\hat c_2(t)$ are differentiable functions with constant term $0$, and let $\hat x=\Phi\hat c+\Phi d$ for some constant vector $d$. Noting that $$A\Phi=\left[\begin{array}{cc}-e^{it} & e^{-it}\\-ie^{it} & -ie^{-it}\end{array}\right]=\Phi',$$ it follows that $$(\Phi\hat c)'=\Phi'\hat c+\Phi\hat c'=A\Phi\hat c+\Phi\hat c',$$ so since $A\Phi d=(\Phi d)'$ by the work done with the homogeneous system, then $$\hat x'=(\Phi\hat c)'+(\Phi d)'=A\Phi\hat c+\Phi\hat c'+A\Phi d=A\hat x+\Phi\hat c'.$$ Thus, $$\hat x'=A\hat x+b\quad\text{if and only if}\quad\Phi\hat c'=b.$$ Now, $\Phi$ is invertible, so letting $c$ be the antiderivative (with respect to $t$) of $\Phi^{-1}b$ without integration constant, we should have that $x=\Phi c+\Phi d$ is the general solution to $(2)$, and then $x_1$ would be the general solution to $(1)$, yes? If that's all good, then I'm apparently just making a calculation error when taking the antiderivative or doing the subsequent matrix operations.... Edit : Does anybody have a different, arguably better approach to take for this problem?",,['ordinary-differential-equations']
79,Laplace's Equation with One Inhomogeneous Boundary Condition,Laplace's Equation with One Inhomogeneous Boundary Condition,,"While solving Laplace's equation, $$ \frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}=0, $$ with Dirichlet boundary conditions $$\begin{align} u(x,0)&=f_1(x),\\ u(x,b)&=0,\\ u(0,y)&=0,\\ u(a,y)&=0, \end{align}$$ and assuming that the solution has the separable form $u(x,y)=X(x)Y(y)$, I ran into the case where I have to solve the ODE $$ Y''(y)=\lambda Y(y), $$ which has only one homogeneous boundary condition. My book immediately concludes that its solution is $$ Y(y)=c_1\sinh\frac{n\pi}a(y-b)\tag{1} $$ where $\lambda=\left(\frac{n\pi}a\right)^2$ was previously computed. I cannot understand how $(1)$ was obtained. Thanks in advance for your help! Edit 1 If it helps to know, I computed that $$ X_n(x)=A_n\sin\frac{n\pi}ax, $$ where $n=1,2,\dots$.","While solving Laplace's equation, $$ \frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}=0, $$ with Dirichlet boundary conditions $$\begin{align} u(x,0)&=f_1(x),\\ u(x,b)&=0,\\ u(0,y)&=0,\\ u(a,y)&=0, \end{align}$$ and assuming that the solution has the separable form $u(x,y)=X(x)Y(y)$, I ran into the case where I have to solve the ODE $$ Y''(y)=\lambda Y(y), $$ which has only one homogeneous boundary condition. My book immediately concludes that its solution is $$ Y(y)=c_1\sinh\frac{n\pi}a(y-b)\tag{1} $$ where $\lambda=\left(\frac{n\pi}a\right)^2$ was previously computed. I cannot understand how $(1)$ was obtained. Thanks in advance for your help! Edit 1 If it helps to know, I computed that $$ X_n(x)=A_n\sin\frac{n\pi}ax, $$ where $n=1,2,\dots$.",,"['ordinary-differential-equations', 'partial-differential-equations', 'harmonic-functions']"
80,A problem for Matlab solver,A problem for Matlab solver,,"I can numercally solve a system of two ODEs for x and p: dy1 = -y1 + y2 dy2 = -1 + y2 with the following Matlab code: %main.m [t y]=rk4sys(20,0.05); [t' y]  %rk4sys.m function [t,y] = rk4sys(n,h) C = [0;-1]; M = [-1,1;0,1]; y(1, :) = [0, 0]; f =@(t,y)(C+M*y')'; t(1) = 0;   for j = 1 : n     k1 = f(t(j), y(j,:));     k2 = f(t(j)+h/2, y(j,:)+h*k1/2 );     k3 = f(t(j)+h/2, y(j,:)+h*k2/2 );     k4 = f(t(j)+h, y(j,:)+h*k3);     y(j+1,:) = y(j,:)+h*(k1+2*(k2+k3)+k4)/6;     t(j+1)=t(j)+h;   end end The main program produces: ans =       0         0         0 0.0500   -0.0013   -0.0513 0.1000   -0.0050   -0.1052 0.1500   -0.0113   -0.1618 0.2000   -0.0201   -0.2214 0.2500   -0.0314   -0.2840 0.3000   -0.0453   -0.3499 0.3500   -0.0619   -0.4191 0.4000   -0.0811   -0.4918 0.4500   -0.1030   -0.5683 0.5000   -0.1276   -0.6487 0.5500   -0.1551   -0.7333 0.6000   -0.1855   -0.8221 0.6500   -0.2188   -0.9155 0.7000   -0.2552   -1.0138 0.7500   -0.2947   -1.1170 0.8000   -0.3374   -1.2255 0.8500   -0.3835   -1.3396 0.9000   -0.4331   -1.4596 0.9500   -0.4862   -1.5857 1.0000   -0.5431   -1.7183 Now, I have to change the element of (1,3) in the matrix above (that is 0(zero) for now) in order for the last number of the third column (that is -1.7183) to be equal to 0(zero). In other words, the element (1,3), which is 0(zero) for now, is just my initial guess. I have to change this number to meet that y2 (at t = 21), that is -1.7183 for now, be 0(zero). I can do this in Excel, but I have failed finding the way to do it in Matlab. Any help would be greatly appreciated.","I can numercally solve a system of two ODEs for x and p: dy1 = -y1 + y2 dy2 = -1 + y2 with the following Matlab code: %main.m [t y]=rk4sys(20,0.05); [t' y]  %rk4sys.m function [t,y] = rk4sys(n,h) C = [0;-1]; M = [-1,1;0,1]; y(1, :) = [0, 0]; f =@(t,y)(C+M*y')'; t(1) = 0;   for j = 1 : n     k1 = f(t(j), y(j,:));     k2 = f(t(j)+h/2, y(j,:)+h*k1/2 );     k3 = f(t(j)+h/2, y(j,:)+h*k2/2 );     k4 = f(t(j)+h, y(j,:)+h*k3);     y(j+1,:) = y(j,:)+h*(k1+2*(k2+k3)+k4)/6;     t(j+1)=t(j)+h;   end end The main program produces: ans =       0         0         0 0.0500   -0.0013   -0.0513 0.1000   -0.0050   -0.1052 0.1500   -0.0113   -0.1618 0.2000   -0.0201   -0.2214 0.2500   -0.0314   -0.2840 0.3000   -0.0453   -0.3499 0.3500   -0.0619   -0.4191 0.4000   -0.0811   -0.4918 0.4500   -0.1030   -0.5683 0.5000   -0.1276   -0.6487 0.5500   -0.1551   -0.7333 0.6000   -0.1855   -0.8221 0.6500   -0.2188   -0.9155 0.7000   -0.2552   -1.0138 0.7500   -0.2947   -1.1170 0.8000   -0.3374   -1.2255 0.8500   -0.3835   -1.3396 0.9000   -0.4331   -1.4596 0.9500   -0.4862   -1.5857 1.0000   -0.5431   -1.7183 Now, I have to change the element of (1,3) in the matrix above (that is 0(zero) for now) in order for the last number of the third column (that is -1.7183) to be equal to 0(zero). In other words, the element (1,3), which is 0(zero) for now, is just my initial guess. I have to change this number to meet that y2 (at t = 21), that is -1.7183 for now, be 0(zero). I can do this in Excel, but I have failed finding the way to do it in Matlab. Any help would be greatly appreciated.",,"['ordinary-differential-equations', 'numerical-methods', 'matlab']"
81,"Using the substitution $p=x+y$, find the general solution of $dy/dx=(3x+3y+4)/(x+y+1)$.","Using the substitution , find the general solution of .",p=x+y dy/dx=(3x+3y+4)/(x+y+1),"Using the substitution $p=x+y$, find the general solution of $$\frac{dy}{dx}=(3x+3y+4)/(x+y+1)$$ Here are my steps: Since $p=x+y$, $$\frac{3x+3y+4}{x+y+1}=\frac{3p+4}{p+1}=\frac{1}{p+1}+3$$ Therefore, integrate both sides  $$y=\ln(p+1)+3p+c$$ $$y=\ln(x+y+1)+3(x+y)+c$$ But the answer in my book is $$x+y-\frac{1}{4}\ln(4x+4y+5)=4x+c$$ Is that correct?","Using the substitution $p=x+y$, find the general solution of $$\frac{dy}{dx}=(3x+3y+4)/(x+y+1)$$ Here are my steps: Since $p=x+y$, $$\frac{3x+3y+4}{x+y+1}=\frac{3p+4}{p+1}=\frac{1}{p+1}+3$$ Therefore, integrate both sides  $$y=\ln(p+1)+3p+c$$ $$y=\ln(x+y+1)+3(x+y)+c$$ But the answer in my book is $$x+y-\frac{1}{4}\ln(4x+4y+5)=4x+c$$ Is that correct?",,"['calculus', 'ordinary-differential-equations']"
82,Boundedness of solutions of $x'+x+f(x)=0$,Boundedness of solutions of,x'+x+f(x)=0,"Let $f \in C^1(\mathbb R, \mathbb R)$ and suppose    $$ \tag{H} \vert f(x) \vert\le \frac{1}{2}\vert x \vert + 3, \quad \forall x \in \mathbb R.  $$ Then, every solution of   $$  x'(t)+x(t)+f(x(t))=0, \quad t \in \mathbb R  $$   is bounded on $[0,+\infty]$. First of all, I would like to say that the text has been copied correctly: I mean, it's really $[0,+\infty]$ so I suppose the author wants me to prove $\displaystyle \lim_{t \to +\infty} x(t) <\infty$. Indeed, boundedness on $[0,+\infty)$ is quite obvious, because we have global existence (its enough to write the equation as $x'=-x-f(x)$ and to observe that the RHS is sublinear thanks to $(H)$). So, we have to prove $\displaystyle \lim_{t \to +\infty} x(t) <\infty$. How can we do? I've got some ideas but I can't conclude. First, I observe that the problem is autonomous: this implies that solutions are either constant either monotonic. First idea: I've fixed $x_0 \in \mathbb R$ and I've written the equivalent integral equation:  $$ x(t) = x_0 - \int_0^t [x(s)+f(x(s))]ds $$ Taking the absolute value and making some rough estimates, we get  $$ \vert x(t) \vert \le \vert x_0 \vert + \left\vert \int_0^t [x(s)+f(x(s))]ds \right\vert \le \vert x_0 \vert + \int_0^t \frac{3}{2}\left\vert x(s) \right\vert +3 ds  $$ but now I don't know how to conclude. Gronwall's lemma? But how can I use it? Second idea: if $x_0 \in \mathbb R$ is s.t. $x_0 +f(x_0) \neq 0$, the solution of the Cauchy problem is not constant. I can divide both members of equation and I obtain (integrating on $[0,t]$) $$ -t=\int_0^t \frac{dx}{x+f(x)} $$ Now I let $t \to +\infty$ but... what can I conclude?","Let $f \in C^1(\mathbb R, \mathbb R)$ and suppose    $$ \tag{H} \vert f(x) \vert\le \frac{1}{2}\vert x \vert + 3, \quad \forall x \in \mathbb R.  $$ Then, every solution of   $$  x'(t)+x(t)+f(x(t))=0, \quad t \in \mathbb R  $$   is bounded on $[0,+\infty]$. First of all, I would like to say that the text has been copied correctly: I mean, it's really $[0,+\infty]$ so I suppose the author wants me to prove $\displaystyle \lim_{t \to +\infty} x(t) <\infty$. Indeed, boundedness on $[0,+\infty)$ is quite obvious, because we have global existence (its enough to write the equation as $x'=-x-f(x)$ and to observe that the RHS is sublinear thanks to $(H)$). So, we have to prove $\displaystyle \lim_{t \to +\infty} x(t) <\infty$. How can we do? I've got some ideas but I can't conclude. First, I observe that the problem is autonomous: this implies that solutions are either constant either monotonic. First idea: I've fixed $x_0 \in \mathbb R$ and I've written the equivalent integral equation:  $$ x(t) = x_0 - \int_0^t [x(s)+f(x(s))]ds $$ Taking the absolute value and making some rough estimates, we get  $$ \vert x(t) \vert \le \vert x_0 \vert + \left\vert \int_0^t [x(s)+f(x(s))]ds \right\vert \le \vert x_0 \vert + \int_0^t \frac{3}{2}\left\vert x(s) \right\vert +3 ds  $$ but now I don't know how to conclude. Gronwall's lemma? But how can I use it? Second idea: if $x_0 \in \mathbb R$ is s.t. $x_0 +f(x_0) \neq 0$, the solution of the Cauchy problem is not constant. I can divide both members of equation and I obtain (integrating on $[0,t]$) $$ -t=\int_0^t \frac{dx}{x+f(x)} $$ Now I let $t \to +\infty$ but... what can I conclude?",,"['real-analysis', 'ordinary-differential-equations']"
83,Simple proof that non-linear DE's do no not satisfy superposition?,Simple proof that non-linear DE's do no not satisfy superposition?,,I'm wondering if there's a simple proof that solutions to non-linear differential equations do not satisfy the superposition principle? Some explicit examples would also be great. Cheers!,I'm wondering if there's a simple proof that solutions to non-linear differential equations do not satisfy the superposition principle? Some explicit examples would also be great. Cheers!,,"['ordinary-differential-equations', 'examples-counterexamples']"
84,How to construct and oscillation with exponentially growing period times?,How to construct and oscillation with exponentially growing period times?,,"I'm searching for the (maybe even smooth) ""oscillating"" function $$f(t)=A\sin{\left(g(t)\right)},$$ such that there are zeroes at times $t_n=T^n$ for some fixed number $T$. So this will not really be periodic, it will be a motion which makes one full turn at exponentially growing gaps, like for example $$t_1=2\ \ \text{sec},\ \ t_2=4\ \text{sec},\ \ t_3=8\ \text{sec},\ \ t_4=16\ \text{sec},\ ...$$ Which function does that? Is there a corresponding Newtonian equation of motion?","I'm searching for the (maybe even smooth) ""oscillating"" function $$f(t)=A\sin{\left(g(t)\right)},$$ such that there are zeroes at times $t_n=T^n$ for some fixed number $T$. So this will not really be periodic, it will be a motion which makes one full turn at exponentially growing gaps, like for example $$t_1=2\ \ \text{sec},\ \ t_2=4\ \text{sec},\ \ t_3=8\ \text{sec},\ \ t_4=16\ \text{sec},\ ...$$ Which function does that? Is there a corresponding Newtonian equation of motion?",,"['real-analysis', 'ordinary-differential-equations', 'physics', 'periodic-functions']"
85,Advection diffusion equation,Advection diffusion equation,,"The advection diffusion equation is the partial differential equation $$\frac{\partial C}{\partial t} = D\frac{\partial^2 C}{\partial x^2} - v \frac{\partial C}{\partial x}$$ with the boundary conditions $$\lim_{x \to \pm \infty} C(x,t)=0$$ and initial condition $$C(x,0)=f(x).$$ How can I transform the advection diffusion equation into a linear diffusion equation by introducing new variables $x^\ast=x-vt$ and $t^\ast=t$? Thanks for any answer.","The advection diffusion equation is the partial differential equation $$\frac{\partial C}{\partial t} = D\frac{\partial^2 C}{\partial x^2} - v \frac{\partial C}{\partial x}$$ with the boundary conditions $$\lim_{x \to \pm \infty} C(x,t)=0$$ and initial condition $$C(x,0)=f(x).$$ How can I transform the advection diffusion equation into a linear diffusion equation by introducing new variables $x^\ast=x-vt$ and $t^\ast=t$? Thanks for any answer.",,"['ordinary-differential-equations', 'partial-differential-equations']"
86,Showing a series is a solution to a differential equation,Showing a series is a solution to a differential equation,,I am attempting to show that the series $y(x)\sum_{n=0}^{\infty} a_{n}x^n$ is a solution to the differential equation $(1-x)^2y''-2y=0$ provided that $(n+2)a_{n+2}-2na_{n+1}+(n-2)a_n=0$ So i have: $$y=\sum_{n=0}^{\infty} a_{n}x^n$$ $$y'=\sum_{n=0}^{\infty}na_{n}x^{n-1}$$ $$y''=\sum_{n=0}^{\infty}a_{n}n(n-1)x^{n-2}$$ then substituting these into the differential equation I get: $$(1-2x+x^2)\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n-2}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ $$\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n-2}-2\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n-1}+\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ relabeling the indexes: $$\sum_{n=-2}^{\infty}(n+2)(n+1)a_{n+2}x^{n}-2\sum_{n=-1}^{\infty}n(n+1)a_{n+1}x^{n}+\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ and then cancelling the $n=-2$ and $n=-1$ terms: $$\sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^{n}-2\sum_{n=0}^{\infty}n(n+1)a_{n+1}x^{n}+\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ but this doesn't give me what I want (I don't think) as I have $n^2$ terms as I would need $(n^2+3n+2)a_{n+2}-(2n^2+n)a_{n+1}+(n^2-n-2)a_{n}=0$ I'm not sure where I have gone wrong? Thanks very much for any help,I am attempting to show that the series $y(x)\sum_{n=0}^{\infty} a_{n}x^n$ is a solution to the differential equation $(1-x)^2y''-2y=0$ provided that $(n+2)a_{n+2}-2na_{n+1}+(n-2)a_n=0$ So i have: $$y=\sum_{n=0}^{\infty} a_{n}x^n$$ $$y'=\sum_{n=0}^{\infty}na_{n}x^{n-1}$$ $$y''=\sum_{n=0}^{\infty}a_{n}n(n-1)x^{n-2}$$ then substituting these into the differential equation I get: $$(1-2x+x^2)\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n-2}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ $$\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n-2}-2\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n-1}+\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ relabeling the indexes: $$\sum_{n=-2}^{\infty}(n+2)(n+1)a_{n+2}x^{n}-2\sum_{n=-1}^{\infty}n(n+1)a_{n+1}x^{n}+\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ and then cancelling the $n=-2$ and $n=-1$ terms: $$\sum_{n=0}^{\infty}(n+2)(n+1)a_{n+2}x^{n}-2\sum_{n=0}^{\infty}n(n+1)a_{n+1}x^{n}+\sum_{n=0}^{\infty}n(n-1)a_{n}x^{n}-2\sum_{n=0}^{\infty} a_{n}x^n=0$$ but this doesn't give me what I want (I don't think) as I have $n^2$ terms as I would need $(n^2+3n+2)a_{n+2}-(2n^2+n)a_{n+1}+(n^2-n-2)a_{n}=0$ I'm not sure where I have gone wrong? Thanks very much for any help,,['ordinary-differential-equations']
87,Maximal solution of differential or integral equation. Apply it to $\frac{d u}{d x}=\sqrt{u}$,Maximal solution of differential or integral equation. Apply it to,\frac{d u}{d x}=\sqrt{u},"What is the definition of maximal solution to an integral equation and differential equation. For example, how should I understand the maximal solution of the following differential equation $\frac{d u}{d x}=\sqrt{u}$?","What is the definition of maximal solution to an integral equation and differential equation. For example, how should I understand the maximal solution of the following differential equation $\frac{d u}{d x}=\sqrt{u}$?",,"['real-analysis', 'ordinary-differential-equations', 'integral-equations']"
88,What exactly is meant by 'Integrate the Equation of Motion'?,What exactly is meant by 'Integrate the Equation of Motion'?,,"Short Question : if a question says to 'integrate the equation of motion', what does it mean? Long Question : Question: Take a planet of mass $M$ and place a satellite at rest at a distance $R$ from the planet, where $R$ is much greater than the planet's radius.  How long does the satellite take to hit the surface of the planet? Part 1 of the question asks the reader to perform dimensional analysis.  This yields $$\textrm{Time taken }T=C\sqrt{\frac{R^3}{GM}}$$ Part 2 - integrate the equation of motion of the satellite to show that $C=\pi /2\sqrt{2}$. As far as I'm aware, the equation of motion for the satellite is $$\ddot{r}=-\frac{GM}{r^2}.$$ I've tried solving this differential equation, but to no avail ($r=\frac{9}{2}GMt^{\frac{2}{3}}$ is a particular solution, but I have no idea how to find the more general case; substituting the dimensionless quantity $\kappa=\frac{1}{GM}r^3t^{-2}$ almost worked but not quite).  I also tried using the potential $V=\frac{-GMm}{r}$ to form the equation $$T=\int_R^0{\frac{dr}{2\sqrt{\frac{GM}{r}-\frac{GM}{R}}}}.$$ However, this integral doesn't look as if it's going to give the right answer.  The $\pi$ in the given expression for $C$ seems to suggest that we're going to get an integral involving a $\sin$ substitution. So - what does 'integrate the equation of motion' mean?  Integrate which equation?  And with respect to what?","Short Question : if a question says to 'integrate the equation of motion', what does it mean? Long Question : Question: Take a planet of mass $M$ and place a satellite at rest at a distance $R$ from the planet, where $R$ is much greater than the planet's radius.  How long does the satellite take to hit the surface of the planet? Part 1 of the question asks the reader to perform dimensional analysis.  This yields $$\textrm{Time taken }T=C\sqrt{\frac{R^3}{GM}}$$ Part 2 - integrate the equation of motion of the satellite to show that $C=\pi /2\sqrt{2}$. As far as I'm aware, the equation of motion for the satellite is $$\ddot{r}=-\frac{GM}{r^2}.$$ I've tried solving this differential equation, but to no avail ($r=\frac{9}{2}GMt^{\frac{2}{3}}$ is a particular solution, but I have no idea how to find the more general case; substituting the dimensionless quantity $\kappa=\frac{1}{GM}r^3t^{-2}$ almost worked but not quite).  I also tried using the potential $V=\frac{-GMm}{r}$ to form the equation $$T=\int_R^0{\frac{dr}{2\sqrt{\frac{GM}{r}-\frac{GM}{R}}}}.$$ However, this integral doesn't look as if it's going to give the right answer.  The $\pi$ in the given expression for $C$ seems to suggest that we're going to get an integral involving a $\sin$ substitution. So - what does 'integrate the equation of motion' mean?  Integrate which equation?  And with respect to what?",,"['ordinary-differential-equations', 'classical-mechanics']"
89,Non-linear ODE: $(y')^2 + y = xy'$,Non-linear ODE:,(y')^2 + y = xy',"I'm sure it's staring at me, but how does one solve this? $$ (y')^2 + y = xy' $$ Thanks.","I'm sure it's staring at me, but how does one solve this? $$ (y')^2 + y = xy' $$ Thanks.",,['ordinary-differential-equations']
90,Integrating a differential equation?,Integrating a differential equation?,,How does $(xJ_0'(x))'+xJ_0(x)=0\implies\int\nolimits_0^1 x J_0(ax)J_0(bx) dx={bJ_0(a)J_0'(b)-aJ_0(b)J_0'(a)\over{a^2-b^2}}?$ Thanks. Perhaps int by parts? But how do I get the RHS form?,How does $(xJ_0'(x))'+xJ_0(x)=0\implies\int\nolimits_0^1 x J_0(ax)J_0(bx) dx={bJ_0(a)J_0'(b)-aJ_0(b)J_0'(a)\over{a^2-b^2}}?$ Thanks. Perhaps int by parts? But how do I get the RHS form?,,"['calculus', 'ordinary-differential-equations', 'special-functions', 'integration']"
91,ODE Initial Value Problem,ODE Initial Value Problem,,"How to solve a question like: Initial value problem where: $$\frac{dY}{dt} = \left(\begin{matrix} 1 & -1 \\ 1 & 3 \end{matrix}\right) Y(t),$$ where $Y(0) = \left(\begin{matrix} 0 \\ 2 \end{matrix}\right).$ The matrix just to the left of $Y$ is a $2\times 2$ matrix. This is for Differential Equations. I don't understand how to do this besides saying $ dx/dt=x -y$, and $dy/dt = x + 3y$.","How to solve a question like: Initial value problem where: $$\frac{dY}{dt} = \left(\begin{matrix} 1 & -1 \\ 1 & 3 \end{matrix}\right) Y(t),$$ where $Y(0) = \left(\begin{matrix} 0 \\ 2 \end{matrix}\right).$ The matrix just to the left of $Y$ is a $2\times 2$ matrix. This is for Differential Equations. I don't understand how to do this besides saying $ dx/dt=x -y$, and $dy/dt = x + 3y$.",,['ordinary-differential-equations']
92,Decomposition of 2 fourth order differential equations on 4 equations of second order,Decomposition of 2 fourth order differential equations on 4 equations of second order,,"How to make decomposition of 2 fourth order differential equations on 4 equations of second order $$C_1x''''+C_2x'''+C_3x''+C_4x'+C_5x-C_6y''-C_7y=0$$ $$D_1y''''+D_2y'''+D_3y''+D_4y'+D_5y-D_6x''-D_7x=0$$ Known constants $$C_i,  D_i$$ Thank you in advance","How to make decomposition of 2 fourth order differential equations on 4 equations of second order $$C_1x''''+C_2x'''+C_3x''+C_4x'+C_5x-C_6y''-C_7y=0$$ $$D_1y''''+D_2y'''+D_3y''+D_4y'+D_5y-D_6x''-D_7x=0$$ Known constants $$C_i,  D_i$$ Thank you in advance",,['ordinary-differential-equations']
93,Solving differential equation,Solving differential equation,,"Below is my work for a particular problem that is mixing me up, since no matter how many times, I can't get my answer to match the book solution. Given ${f}''(x)= x^{-\frac{3}{2}}$ where $f'(4)= 2$ and $f(0)= 0$, solve the differential equation. $$f'(x)= \int x^{-\frac{3}{2}} \Rightarrow \frac{x^{-\frac{3}{2}+1}}{-\frac{3}{2}+1} \Rightarrow -2x^{-\frac{1}{2}} + C$$ $$f'(4)= -2(4)^{-\frac{1}{2}}+ C= 2 \Rightarrow -4+C= 2 \Rightarrow C= 6$$ Thus, the first differential equation is $f'(x)= -2x^{-\frac{1}{2}}+6$ $$f(x)= \int -2x^{-\frac{1}{2}}+6 \Rightarrow 2(\frac{x^{-\frac{1}{2}}}{-\frac{1}{2}+1}) \Rightarrow -4x^{\frac{1}{2}}+6x+C$$ Since $f(0)= 0, C=0$, so the final differential equation should be $f(x)= -4x^{\frac{1}{2}}+6x$, but the book answer has $3x$ in place of my $6x$.  Where did I go wrong?","Below is my work for a particular problem that is mixing me up, since no matter how many times, I can't get my answer to match the book solution. Given ${f}''(x)= x^{-\frac{3}{2}}$ where $f'(4)= 2$ and $f(0)= 0$, solve the differential equation. $$f'(x)= \int x^{-\frac{3}{2}} \Rightarrow \frac{x^{-\frac{3}{2}+1}}{-\frac{3}{2}+1} \Rightarrow -2x^{-\frac{1}{2}} + C$$ $$f'(4)= -2(4)^{-\frac{1}{2}}+ C= 2 \Rightarrow -4+C= 2 \Rightarrow C= 6$$ Thus, the first differential equation is $f'(x)= -2x^{-\frac{1}{2}}+6$ $$f(x)= \int -2x^{-\frac{1}{2}}+6 \Rightarrow 2(\frac{x^{-\frac{1}{2}}}{-\frac{1}{2}+1}) \Rightarrow -4x^{\frac{1}{2}}+6x+C$$ Since $f(0)= 0, C=0$, so the final differential equation should be $f(x)= -4x^{\frac{1}{2}}+6x$, but the book answer has $3x$ in place of my $6x$.  Where did I go wrong?",,"['calculus', 'ordinary-differential-equations']"
94,How to solve $\ddot{\theta} + \mu \dot{\theta}^2=0$,How to solve,\ddot{\theta} + \mu \dot{\theta}^2=0,$\dot{\theta} \equiv \frac{d \theta(t)}{dt}$ I encountered this ODE $\ddot{\theta} + \mu \dot{\theta}^2=0$ How do I find a solution for $\theta(t)$. I know $\dot{\theta}(t=0)=\omega_0$,$\dot{\theta} \equiv \frac{d \theta(t)}{dt}$ I encountered this ODE $\ddot{\theta} + \mu \dot{\theta}^2=0$ How do I find a solution for $\theta(t)$. I know $\dot{\theta}(t=0)=\omega_0$,,[]
95,flow on a closed manifold,flow on a closed manifold,,"Given a smooth vector field on a closed smooth manifold, does the flow of that vector field exists for all time $t \in R$? This may be a well-known and elementary conclusion, but I just wnat to confirm it to me.","Given a smooth vector field on a closed smooth manifold, does the flow of that vector field exists for all time $t \in R$? This may be a well-known and elementary conclusion, but I just wnat to confirm it to me.",,['ordinary-differential-equations']
96,"When solving a third-order ODE using reduction of order, what happens when the v’ term drops out","When solving a third-order ODE using reduction of order, what happens when the v’ term drops out",,"Given question Given $y=x$ is a solution of $6x^3y’’’ - 24x^2y’’ + 48xy’-48y = 0$ . Find the general solution if $x>0$ . My work Since we are given that $y = x$ is a solution, my first approach would be to use reduction of order method. So, I would say that \begin{align} y_2 &= v*y_1 \newline y_2’ &= v + v’y_1 \newline y_2’’ &= v’ + v’ + y_1v’’ = 2v’ + y_1v’’ \newline y_2’’’&= 3v’’ + y_1v’’’ \end{align} When I plug this into the original equation I get $$ 6x^4v’’’ - 6x^3v’’ = 0 $$ Normally, when I use this method the v term will drop making my homogenous equation a function of $\frac{dv}{dx}$ ’s but here my v term and v’ term dropped out. From some googling I found the following “If you’ve done all of your work correctly this should always happen. Sometimes, as in the repeated roots case, the first derivative term will also drop out” but I’m not quite sure what this means for my work…. I’m used to making the substitution $w=v’$ . Could someone point me in the right direction?","Given question Given is a solution of . Find the general solution if . My work Since we are given that is a solution, my first approach would be to use reduction of order method. So, I would say that When I plug this into the original equation I get Normally, when I use this method the v term will drop making my homogenous equation a function of ’s but here my v term and v’ term dropped out. From some googling I found the following “If you’ve done all of your work correctly this should always happen. Sometimes, as in the repeated roots case, the first derivative term will also drop out” but I’m not quite sure what this means for my work…. I’m used to making the substitution . Could someone point me in the right direction?","y=x 6x^3y’’’ - 24x^2y’’ + 48xy’-48y = 0 x>0 y = x \begin{align}
y_2 &= v*y_1 \newline
y_2’ &= v + v’y_1 \newline
y_2’’ &= v’ + v’ + y_1v’’ = 2v’ + y_1v’’ \newline
y_2’’’&= 3v’’ + y_1v’’’
\end{align} 
6x^4v’’’ - 6x^3v’’ = 0
 \frac{dv}{dx} w=v’","['calculus', 'ordinary-differential-equations', 'reduction-of-order-ode']"
97,A question on the qualitative analysis of solution of a system of ODEs [closed],A question on the qualitative analysis of solution of a system of ODEs [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last month . Improve this question Let $f:\mathbb{R}^2 \to \mathbb{R}^2$ be a non-zero smooth vector field satisfying $\text{div} f \ne 0.$ Which of the following are necessarily true for the ODE: $\dot{\mathbf{x}}=f(\mathbf{x})$ ? (a) There are no equilibrium points (b) There are no periodic solutions (c) All the solutions are bounded (d) All the solutions are unbounded My attempt: This is actually a system of 2 ODEs. Notation: $\mathbf{x}=(x,y).$ For option (a), set $f(x,y)=(x,y).$ See that $\text{div} f \ne 0.$ Now, we can clearly see that $(0,0)$ is an equilibrium point for the system of ODE $\dot{\mathbf{x}}=f(\mathbf{x}).$ So, option (a) is NOT true in general meaning that it is NOT necessary. I am confused as to how to proceed for the remaining options. The KEY ANSWERS says that (a), (b) and (d) are the correct answers for this question. Please help on how to proceed.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last month . Improve this question Let be a non-zero smooth vector field satisfying Which of the following are necessarily true for the ODE: ? (a) There are no equilibrium points (b) There are no periodic solutions (c) All the solutions are bounded (d) All the solutions are unbounded My attempt: This is actually a system of 2 ODEs. Notation: For option (a), set See that Now, we can clearly see that is an equilibrium point for the system of ODE So, option (a) is NOT true in general meaning that it is NOT necessary. I am confused as to how to proceed for the remaining options. The KEY ANSWERS says that (a), (b) and (d) are the correct answers for this question. Please help on how to proceed.","f:\mathbb{R}^2 \to \mathbb{R}^2 \text{div} f \ne 0. \dot{\mathbf{x}}=f(\mathbf{x}) \mathbf{x}=(x,y). f(x,y)=(x,y). \text{div} f \ne 0. (0,0) \dot{\mathbf{x}}=f(\mathbf{x}).","['ordinary-differential-equations', 'nonlinear-system', 'nonlinear-dynamics']"
98,Derive the use of the Jacobi Amplitude function with the nonlinear pendulum diffequation,Derive the use of the Jacobi Amplitude function with the nonlinear pendulum diffequation,,"I've been playing around with the pendulum differential equation ${\theta}''+\frac{g}{L}\sin{\theta}=0$ , and have found many general solutions using the Jacobi Amplitude function $\text{am}(u,m)$ often multiplied with a trig function using the integral that evaluates period time. I'm somewhat of a beginner to the topic only being fluent in the first three courses of calculus as I am working on this for a physics project. All of the derivation videos I've watched make sense, but always arrive at the idea that these equations are 'really hard to solve' and only reveal how to derive the period time. I've tried applying generalizing $\theta$ to a power series and trying to get a solution for the coefficiens of $\theta$ , $a_n$ , but I couldn't solve this because of the nonlinear term. What I have so far is this: $$ \text{let } \theta(t)=\sum_{n\ge0} a_nt^n \\\text{if }\theta(0)=\theta_0=a_0 \text{ and }\theta'(0)=0=a_1 \\\text{so }\theta(t)=\theta_0+a_2t^2+a_3t^3+...+a_nt^n=\theta_0+\sum_{n\ge2}a_nt^n \\\text{and }\theta''(t)=2a_2+6a_3t+12a_4t^2+20a_5t^3+...+n(n-1)a_nt^{n-1} \\\text{then substituting}\sum_{n\ge0}n(n-1)a_nt^{n-2}+\frac{g}{L}\sin({\theta_0+\sum_{n\ge2}a_nt^n})=0 $$ how can I go from here to find that the coefficients of $a_n$ match the coefficients of some elliptic function. Also let me know if the concept for 'finding' an elliptic function in a diffeq is misconstrued or if a better approach without generalizing to a power series exists. Let me know if this question isn't clear so I can help you help me ;). PS, this is my first stack exchange post, so please let me know if there are problems with my latex or other things, thanks","I've been playing around with the pendulum differential equation , and have found many general solutions using the Jacobi Amplitude function often multiplied with a trig function using the integral that evaluates period time. I'm somewhat of a beginner to the topic only being fluent in the first three courses of calculus as I am working on this for a physics project. All of the derivation videos I've watched make sense, but always arrive at the idea that these equations are 'really hard to solve' and only reveal how to derive the period time. I've tried applying generalizing to a power series and trying to get a solution for the coefficiens of , , but I couldn't solve this because of the nonlinear term. What I have so far is this: how can I go from here to find that the coefficients of match the coefficients of some elliptic function. Also let me know if the concept for 'finding' an elliptic function in a diffeq is misconstrued or if a better approach without generalizing to a power series exists. Let me know if this question isn't clear so I can help you help me ;). PS, this is my first stack exchange post, so please let me know if there are problems with my latex or other things, thanks","{\theta}''+\frac{g}{L}\sin{\theta}=0 \text{am}(u,m) \theta \theta a_n 
\text{let } \theta(t)=\sum_{n\ge0} a_nt^n
\\\text{if }\theta(0)=\theta_0=a_0 \text{ and }\theta'(0)=0=a_1
\\\text{so }\theta(t)=\theta_0+a_2t^2+a_3t^3+...+a_nt^n=\theta_0+\sum_{n\ge2}a_nt^n
\\\text{and }\theta''(t)=2a_2+6a_3t+12a_4t^2+20a_5t^3+...+n(n-1)a_nt^{n-1}
\\\text{then substituting}\sum_{n\ge0}n(n-1)a_nt^{n-2}+\frac{g}{L}\sin({\theta_0+\sum_{n\ge2}a_nt^n})=0
 a_n","['ordinary-differential-equations', 'physics', 'elliptic-integrals', 'elliptic-functions']"
99,Writing a general solution to differential equation with Bessel functions,Writing a general solution to differential equation with Bessel functions,,"Consider a mass $m$ is placed on a horizontal level surface and attached to a spring, whose other end is attached to a vertical wall. The mass moves in a viscous medium, where the resistance force acting on it is proportional to the velocity: $\vec{F}_p=-\gamma\vec{\nu}$ , where $\gamma$ is a known positive constant. Due to aging, the spring stiffness coefficient decreases exponentially over time: $k\left ( t\right ) = k_0\mathrm{e}^{-\alpha t}$ , where $k_0$ and $\alpha$ are other known positive constants. Write down the differential equation describing the motion of the equilibrium position derived mass (in the one-dimensional case) and find its general solution $x(t)$ , expressing it in terms of Bessel functions. Hint : Use the substitution $s=\mathrm{e}^{-\alpha t/2}.$ I tried applying Newton's second law and got $m\ddot{x}=-k_0e^{-\alpha t}x-\gamma\dot{x} \implies \ddot{x}=-\frac{k_0}me^{-\alpha t}x-\frac\gamma m\dot{x}$ , if correct. Then I denote $\begin{aligned}&\omega_0=\sqrt{k_0/m}. , \beta=\frac\gamma m\end{aligned}$ , and get $\ddot{x}=-\omega_0^2e^{-\alpha t}x-\beta\dot{x}$ . I don't have ideas on how I apply the substitution and where to use the Bessel functions? Any help?","Consider a mass is placed on a horizontal level surface and attached to a spring, whose other end is attached to a vertical wall. The mass moves in a viscous medium, where the resistance force acting on it is proportional to the velocity: , where is a known positive constant. Due to aging, the spring stiffness coefficient decreases exponentially over time: , where and are other known positive constants. Write down the differential equation describing the motion of the equilibrium position derived mass (in the one-dimensional case) and find its general solution , expressing it in terms of Bessel functions. Hint : Use the substitution I tried applying Newton's second law and got , if correct. Then I denote , and get . I don't have ideas on how I apply the substitution and where to use the Bessel functions? Any help?","m \vec{F}_p=-\gamma\vec{\nu} \gamma k\left ( t\right ) = k_0\mathrm{e}^{-\alpha t} k_0 \alpha x(t) s=\mathrm{e}^{-\alpha t/2}. m\ddot{x}=-k_0e^{-\alpha t}x-\gamma\dot{x} \implies \ddot{x}=-\frac{k_0}me^{-\alpha t}x-\frac\gamma m\dot{x} \begin{aligned}&\omega_0=\sqrt{k_0/m}. , \beta=\frac\gamma m\end{aligned} \ddot{x}=-\omega_0^2e^{-\alpha t}x-\beta\dot{x}","['ordinary-differential-equations', 'physics', 'classical-mechanics', 'bessel-functions']"
