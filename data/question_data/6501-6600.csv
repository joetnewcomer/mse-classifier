,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Sequence of bounded functions or pointwise bounded,Sequence of bounded functions or pointwise bounded,,"We have sequence of function $\{f_n(x)\}$ defined on set $E$ and $n\in \mathbb{N}$. What does mean sequence of bounded functions ? Is it pointwise bounded ? Definition: We say that $\{f_n\}$ is pointwise bounded on $E$ if the sequence $\{f_n(x)\}$ is bounded for every $x\in E$, that is, if there exists a finite-valued function $\phi$ defined on $E$ such that $$|f_n(x)|<\phi(x) \quad (x\in E, n\in \mathbb{N}).$$ Can anyone explain to me please?","We have sequence of function $\{f_n(x)\}$ defined on set $E$ and $n\in \mathbb{N}$. What does mean sequence of bounded functions ? Is it pointwise bounded ? Definition: We say that $\{f_n\}$ is pointwise bounded on $E$ if the sequence $\{f_n(x)\}$ is bounded for every $x\in E$, that is, if there exists a finite-valued function $\phi$ defined on $E$ such that $$|f_n(x)|<\phi(x) \quad (x\in E, n\in \mathbb{N}).$$ Can anyone explain to me please?",,['real-analysis']
1,Where are the roots going?,Where are the roots going?,,"Each polynomial of degree $n$ has $n$ different roots. We know that $$e^z = \sum_{n=0}^\infty \frac {z^n}{n!}$$ has no roots. What is the behaviour then of the root of $S_N = \sum_{n=0}^N\frac {z^n}{n!}$? Where do they go? We could generalize this and ask Given $f(z)$ holomorphic in some $\Omega$ (feel free to take any condition you want on $\Omega$), what is the relationship between the root of $f(z)$ and the roots of the truncated expansion of $f(z)$? edit I've seen the suggested duplicate and its really interesting, though I am interested in a more general result :-)","Each polynomial of degree $n$ has $n$ different roots. We know that $$e^z = \sum_{n=0}^\infty \frac {z^n}{n!}$$ has no roots. What is the behaviour then of the root of $S_N = \sum_{n=0}^N\frac {z^n}{n!}$? Where do they go? We could generalize this and ask Given $f(z)$ holomorphic in some $\Omega$ (feel free to take any condition you want on $\Omega$), what is the relationship between the root of $f(z)$ and the roots of the truncated expansion of $f(z)$? edit I've seen the suggested duplicate and its really interesting, though I am interested in a more general result :-)",,"['calculus', 'real-analysis', 'complex-analysis', 'analysis', 'roots']"
2,Application of Dominated Convergence Theorem and Monotone Convergence Theorem,Application of Dominated Convergence Theorem and Monotone Convergence Theorem,,"Find the limit $$\lim_{n \rightarrow \infty} \int _0^n (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n})) dx$$ and justify the answer. I think that the Dominated Convergence Theorem can be applied to this problem. Since $$\int _0^n (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n})) dx = \int _0^\infty (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n}))1_{[0, n]}(x)  dx,$$ if I can find the dominating function, I will then apply DCT. After applying DCT, we will have $$\lim \int _0^\infty (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n}))1_{[0, n]}(x)  dx = \int _0^\infty \lim (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n}))1_{[0, n]}(x)  dx = \int _0^\infty e^{-x} \log (3)  dx = \log(3)e^{-x}|_{x=\infty, x=0} = - \log (3).$$ But I am not sure if $$\Big| (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n})) \Big| \leq 3e^{-x}$$ with $3e^{-x}$ integrable on $[0, \infty).$ Can someone check if my dominating function is alright ? Prove that $$\sum_{k = 1}^\infty \frac{1}{(p + k)^2} = - \int_0^1 \frac{x^p}{1-x}\log (x) dx$$ for $p>0.$ (This problem is allow to use the Fundamental Theorem of Calculus) I suppose that I should apply FTC with DCT. But I have no ideas about doing this, any hints please ?","Find the limit $$\lim_{n \rightarrow \infty} \int _0^n (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n})) dx$$ and justify the answer. I think that the Dominated Convergence Theorem can be applied to this problem. Since $$\int _0^n (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n})) dx = \int _0^\infty (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n}))1_{[0, n]}(x)  dx,$$ if I can find the dominating function, I will then apply DCT. After applying DCT, we will have $$\lim \int _0^\infty (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n}))1_{[0, n]}(x)  dx = \int _0^\infty \lim (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n}))1_{[0, n]}(x)  dx = \int _0^\infty e^{-x} \log (3)  dx = \log(3)e^{-x}|_{x=\infty, x=0} = - \log (3).$$ But I am not sure if $$\Big| (1 - \frac{x}{n})^n \log (2 + \cos(\frac{x}{n})) \Big| \leq 3e^{-x}$$ with $3e^{-x}$ integrable on $[0, \infty).$ Can someone check if my dominating function is alright ? Prove that $$\sum_{k = 1}^\infty \frac{1}{(p + k)^2} = - \int_0^1 \frac{x^p}{1-x}\log (x) dx$$ for $p>0.$ (This problem is allow to use the Fundamental Theorem of Calculus) I suppose that I should apply FTC with DCT. But I have no ideas about doing this, any hints please ?",,"['real-analysis', 'integration', 'lebesgue-integral']"
3,Name and proof of mathematical inequality,Name and proof of mathematical inequality,,"I'm reading a book on functional analysis and I've come across an inequality i.e. Suppose we have $$ a, b, c, p, q \ge 0 $$ and $$ p + q = 1 $$ then $$ a+b^pc^q \le (a+b)^p(a+c)^q$$ I'm curious on how one would prove this inequality let alone if there is a name of this inequality.  I'd like to note this is not a homework problem.  Thanks!","I'm reading a book on functional analysis and I've come across an inequality i.e. Suppose we have $$ a, b, c, p, q \ge 0 $$ and $$ p + q = 1 $$ then $$ a+b^pc^q \le (a+b)^p(a+c)^q$$ I'm curious on how one would prove this inequality let alone if there is a name of this inequality.  I'd like to note this is not a homework problem.  Thanks!",,"['real-analysis', 'linear-algebra', 'analysis', 'functional-analysis']"
4,Question about existance of global minimum in a strictly convex function,Question about existance of global minimum in a strictly convex function,,"Is it true that a strictly convex function must have a global minimum ? I know that every strictly convex function has at most one global minimum and each local minimum is a global minimum but I can not prove or disprove the statement. Strictly convex function: The real-value function that for every $x$ , $y$ $\in$ $R$ and $t$ $\in$ $(0,1)$ we have: $$f(tx + (1-t)y) < tf(x) + (1-t)f(y)$$","Is it true that a strictly convex function must have a global minimum ? I know that every strictly convex function has at most one global minimum and each local minimum is a global minimum but I can not prove or disprove the statement. Strictly convex function: The real-value function that for every $x$ , $y$ $\in$ $R$ and $t$ $\in$ $(0,1)$ we have: $$f(tx + (1-t)y) < tf(x) + (1-t)f(y)$$",,"['real-analysis', 'convex-analysis', 'convex-optimization', 'convex-geometry']"
5,Bounding $f'$ in terms of $f$ and $f''$,Bounding  in terms of  and,f' f f'',"Assume that $f: \mathbb{R} \to [0,\infty)$ is $C^2$ and $|f''(x)| \leq A$ for all $x$.  Show that the inequality $$(f'(x))^2 \le 2Af(x)$$ holds for all $x$. The hint given in the question was, ""Taylor's theorem."" I had thought in the beginning that the bound was pretty straightforward, but I've been stuck for a bit now. I have that, centering my Taylor expansion about some point $a$, gives $$f(x) = f(a) + f'(a)(x-a) + \frac{f^{''}(\psi)}{2!}(x-a)^2$$ for some $\psi$ in the interval $(x,a)$.  The last term is the Lagrange remainder. Then $$f(x) = f(a) + f'(a)(x-a) + \frac{f^{''}(\psi)}{2!}(x-a)^2$$ $$\le f(a) + f'(a)(x-a) + \frac{|f^{''}(\psi)|}{2!}(x-a)^2$$ $$\le f(a) + f'(a)(x-a) + \frac{A}{2!}(x-a)^2$$ $$\implies f(x)-f(a)\le  f'(a)(x-a) + \frac{A}{2!}(x-a)^2$$ $$\implies \frac{f(x)-f(a)}{x-a}\le  f'(a) + \frac{A}{2!}(x-a)$$ $$\implies f'(\eta)\le  f'(a) + \frac{A}{2!}(x-a)$$ (applying the Mean Value Theorem on the left hand side) and this is where I am stuck. Any hints are welcome. Thanks,","Assume that $f: \mathbb{R} \to [0,\infty)$ is $C^2$ and $|f''(x)| \leq A$ for all $x$.  Show that the inequality $$(f'(x))^2 \le 2Af(x)$$ holds for all $x$. The hint given in the question was, ""Taylor's theorem."" I had thought in the beginning that the bound was pretty straightforward, but I've been stuck for a bit now. I have that, centering my Taylor expansion about some point $a$, gives $$f(x) = f(a) + f'(a)(x-a) + \frac{f^{''}(\psi)}{2!}(x-a)^2$$ for some $\psi$ in the interval $(x,a)$.  The last term is the Lagrange remainder. Then $$f(x) = f(a) + f'(a)(x-a) + \frac{f^{''}(\psi)}{2!}(x-a)^2$$ $$\le f(a) + f'(a)(x-a) + \frac{|f^{''}(\psi)|}{2!}(x-a)^2$$ $$\le f(a) + f'(a)(x-a) + \frac{A}{2!}(x-a)^2$$ $$\implies f(x)-f(a)\le  f'(a)(x-a) + \frac{A}{2!}(x-a)^2$$ $$\implies \frac{f(x)-f(a)}{x-a}\le  f'(a) + \frac{A}{2!}(x-a)$$ $$\implies f'(\eta)\le  f'(a) + \frac{A}{2!}(x-a)$$ (applying the Mean Value Theorem on the left hand side) and this is where I am stuck. Any hints are welcome. Thanks,",,"['calculus', 'real-analysis', 'sequences-and-series', 'taylor-expansion']"
6,In which sense does Cauchy-Riemann equations link complex- and real analysis?,In which sense does Cauchy-Riemann equations link complex- and real analysis?,,"On page 12 of Stein, Shakarchi textbook 'Complex analysis', the authors state that the Cauchy-Riemann equations link complex and real analysis . I have completed courses on real and complex analysis, but I feel that this is somewhat of an over-statement. But perhaps it is just me which doesnt have a good enough overview. $$ \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \qquad \frac{\partial u}{\partial y} = - \frac{\partial v}{\partial x} $$ If anyone with a clear insight is able to concisely explain how one could justify writing something like this -- then that insight would be most valuable.","On page 12 of Stein, Shakarchi textbook 'Complex analysis', the authors state that the Cauchy-Riemann equations link complex and real analysis . I have completed courses on real and complex analysis, but I feel that this is somewhat of an over-statement. But perhaps it is just me which doesnt have a good enough overview. $$ \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \qquad \frac{\partial u}{\partial y} = - \frac{\partial v}{\partial x} $$ If anyone with a clear insight is able to concisely explain how one could justify writing something like this -- then that insight would be most valuable.",,"['real-analysis', 'complex-analysis']"
7,"How to prove $\int_{0}^{\infty}\frac{e^{-\left(\sqrt{x}-a/\sqrt{x}\right)^2}}{\sqrt{x}}dx=\sqrt{\pi},\,a>0$?",How to prove ?,"\int_{0}^{\infty}\frac{e^{-\left(\sqrt{x}-a/\sqrt{x}\right)^2}}{\sqrt{x}}dx=\sqrt{\pi},\,a>0","We know that $$\Gamma\left(\frac{1}{2}\right)=\int_{0}^{\infty}\frac{e^{-x}}{\sqrt{x}}dx=\sqrt{\pi}  $$ but it seems that, for every $a>0  $ we have $$\int_{0}^{\infty}\frac{e^{-\left(\sqrt{x}-a/\sqrt{x}\right)^{2}}}{\sqrt{x}}dx=\sqrt{\pi}$$ so the additional term $a/\sqrt{x}  $ doesn't change the value of the integral. How we can prove (or disprove) it? Thank you.","We know that $$\Gamma\left(\frac{1}{2}\right)=\int_{0}^{\infty}\frac{e^{-x}}{\sqrt{x}}dx=\sqrt{\pi}  $$ but it seems that, for every $a>0  $ we have $$\int_{0}^{\infty}\frac{e^{-\left(\sqrt{x}-a/\sqrt{x}\right)^{2}}}{\sqrt{x}}dx=\sqrt{\pi}$$ so the additional term $a/\sqrt{x}  $ doesn't change the value of the integral. How we can prove (or disprove) it? Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'closed-form']"
8,Density of $C^\infty(\mathbb{R}^n)$ in $C^0(\mathbb{R}^n)$,Density of  in,C^\infty(\mathbb{R}^n) C^0(\mathbb{R}^n),"This could be well-known, but I cannot come up with a rigorous proof. I want to prove density of $C^\infty(\mathbb{R}^n)$ in the continuous functions $C^0(\mathbb{R}^n)$ in the following sense: given a continuous function $f$, I want to select a smooth function $g$ such that $\sup |f - g|$ is as small as we want. All the tricks I can think of either deal with a compact setting, or continuous functions vanishing at infinity. Thanks!","This could be well-known, but I cannot come up with a rigorous proof. I want to prove density of $C^\infty(\mathbb{R}^n)$ in the continuous functions $C^0(\mathbb{R}^n)$ in the following sense: given a continuous function $f$, I want to select a smooth function $g$ such that $\sup |f - g|$ is as small as we want. All the tricks I can think of either deal with a compact setting, or continuous functions vanishing at infinity. Thanks!",,"['real-analysis', 'functional-analysis']"
9,Divergence of an improper integral,Divergence of an improper integral,,"Let $f:[0,\infty)\rightarrow \mathbb{R}$ be a continuous and strictly decreasing function. If $\displaystyle\lim_{x\rightarrow\infty}f(x)=0$, prove that the following integral is divergent :   $$\int_0^\infty \frac{f(x)-f(x+1)}{f(x)}dx$$ First, it's not hard to see that $f(0)>0$ and also $f(x)>0$ for each $x$. I think we have to show that behavior of $\frac{f(x)-f(x+1)}{f(x)}$ is ""similar"" to $\frac1{x+1}$  or $\frac{f(x+1)}{f(x)}$ is ""similar"" to $\frac{x}{x+1}$ for big values of $x$. But meaning of ""similar"" is not clear yet ! The most naive and rough approach is to show $\frac{f(x+1)}{f(x)}<1-k$ for some $0<k<1$ and therefore $f(x)-f(x+1)>kf(x)$. One step forward, it's good to compare $\frac{f(x+1)}{f(x)}$ and a multiple of $\frac{x}{x+1}$ or equivalently to compare $C (x+1)f(x+1)$ and $xf(x)$. But still I can't extend this idea . I tried more ways but I couldn't find a proper way to solve the problem. Any hint is appreciated !","Let $f:[0,\infty)\rightarrow \mathbb{R}$ be a continuous and strictly decreasing function. If $\displaystyle\lim_{x\rightarrow\infty}f(x)=0$, prove that the following integral is divergent :   $$\int_0^\infty \frac{f(x)-f(x+1)}{f(x)}dx$$ First, it's not hard to see that $f(0)>0$ and also $f(x)>0$ for each $x$. I think we have to show that behavior of $\frac{f(x)-f(x+1)}{f(x)}$ is ""similar"" to $\frac1{x+1}$  or $\frac{f(x+1)}{f(x)}$ is ""similar"" to $\frac{x}{x+1}$ for big values of $x$. But meaning of ""similar"" is not clear yet ! The most naive and rough approach is to show $\frac{f(x+1)}{f(x)}<1-k$ for some $0<k<1$ and therefore $f(x)-f(x+1)>kf(x)$. One step forward, it's good to compare $\frac{f(x+1)}{f(x)}$ and a multiple of $\frac{x}{x+1}$ or equivalently to compare $C (x+1)f(x+1)$ and $xf(x)$. But still I can't extend this idea . I tried more ways but I couldn't find a proper way to solve the problem. Any hint is appreciated !",,"['real-analysis', 'integration', 'analysis', 'improper-integrals']"
10,What is the dense subset in $H_0^1(\Omega)\cap H^2(\Omega)$,What is the dense subset in,H_0^1(\Omega)\cap H^2(\Omega),"I came across this problem when I try to prove that for space $H(\Omega):=H_0^1(\Omega)\cap H^2(\Omega)$, where $\Omega$ is open bounded with nice boundary, then the norm $\|u\|_1:=\|\Delta u\|_{L^2}$ is equivalent to the norm $\|u\|_{H^2}$ in the usual sense. This problem can be proved by using open mapping theorem and showing that $\|u\|_1$ is actually make $H$ as a Banach space. However, I was thinking by using the fact that the norm $\|u\|_2:=\|u\|_{L^2}+\|D^2u\|_{L^2}$ is already an equivalent norm of $\|u\|_{H^2}$ and the term $\|u\|_{L^2}$ will be taken care by Poincare. Then I only need to show that $\|D^2u\|_{L^2(\Omega)}\leq C\|\Delta u\|_{L^2(\Omega)}$ for some constant $C$. I remembered when I deal with the weak solution of Biharmonic functions, I proved that  $$ \int_\Omega \partial_i\partial_j u\cdot \partial_i\partial_j u\,dx=\int_\Omega \partial_i\partial_i u\cdot \partial_j\partial_j u\,dx \tag 1$$ by using density argument. So here I was going to use the same approach. But I can not see what function is dense in $H$ under $H^2$ norm. I don't think $C_c^\infty(\Omega)$ will work here because it will give us $H_0^2$ but not $H$. But we can not use $C^\infty(\bar \Omega)$ neither because we will lose $H_0^1$. So, what function should I use as the approximate function here? i.e., can I somehow use smooth function to approx $u\in H$ under $H^2$ norm? Update This question has been solved but I want to update that we can generalize this to $H^{2k}(\Omega)\cap H_0^k(\Omega)$. That is, the space $H^{2k}(\Omega)\cap H_0^k(\Omega)$ has equivalent norm $\| \Delta^k u\|_{L^2(\Omega)}$ and it can be proved by using regularity of Laplace equation. (The equivalent of $H^2\cap H_0^1$ can be proved by regularity in only TWO steps!)","I came across this problem when I try to prove that for space $H(\Omega):=H_0^1(\Omega)\cap H^2(\Omega)$, where $\Omega$ is open bounded with nice boundary, then the norm $\|u\|_1:=\|\Delta u\|_{L^2}$ is equivalent to the norm $\|u\|_{H^2}$ in the usual sense. This problem can be proved by using open mapping theorem and showing that $\|u\|_1$ is actually make $H$ as a Banach space. However, I was thinking by using the fact that the norm $\|u\|_2:=\|u\|_{L^2}+\|D^2u\|_{L^2}$ is already an equivalent norm of $\|u\|_{H^2}$ and the term $\|u\|_{L^2}$ will be taken care by Poincare. Then I only need to show that $\|D^2u\|_{L^2(\Omega)}\leq C\|\Delta u\|_{L^2(\Omega)}$ for some constant $C$. I remembered when I deal with the weak solution of Biharmonic functions, I proved that  $$ \int_\Omega \partial_i\partial_j u\cdot \partial_i\partial_j u\,dx=\int_\Omega \partial_i\partial_i u\cdot \partial_j\partial_j u\,dx \tag 1$$ by using density argument. So here I was going to use the same approach. But I can not see what function is dense in $H$ under $H^2$ norm. I don't think $C_c^\infty(\Omega)$ will work here because it will give us $H_0^2$ but not $H$. But we can not use $C^\infty(\bar \Omega)$ neither because we will lose $H_0^1$. So, what function should I use as the approximate function here? i.e., can I somehow use smooth function to approx $u\in H$ under $H^2$ norm? Update This question has been solved but I want to update that we can generalize this to $H^{2k}(\Omega)\cap H_0^k(\Omega)$. That is, the space $H^{2k}(\Omega)\cap H_0^k(\Omega)$ has equivalent norm $\| \Delta^k u\|_{L^2(\Omega)}$ and it can be proved by using regularity of Laplace equation. (The equivalent of $H^2\cap H_0^1$ can be proved by regularity in only TWO steps!)",,"['real-analysis', 'sobolev-spaces']"
11,Why it is not a function [duplicate],Why it is not a function [duplicate],,"This question already has answers here : Why $f(x) = \sqrt{x}$ is a function? (2 answers) Closed 9 years ago . This question is very basic one but at this moment I could not answer it. We know $y=\pm\sqrt{x}$ is not a function because one value of $x$ corresponds to more than one value of $y$. Again $y=x^2$ is a function. But we can get the first  equation from the second one, so the first one should be a function, making contradiction. Could anyone please tell me what I am missing. Thanks in advance.","This question already has answers here : Why $f(x) = \sqrt{x}$ is a function? (2 answers) Closed 9 years ago . This question is very basic one but at this moment I could not answer it. We know $y=\pm\sqrt{x}$ is not a function because one value of $x$ corresponds to more than one value of $y$. Again $y=x^2$ is a function. But we can get the first  equation from the second one, so the first one should be a function, making contradiction. Could anyone please tell me what I am missing. Thanks in advance.",,['real-analysis']
12,Is $\sum\limits_{n=1} ^\infty |x_n||y_n| ≤\left(\sum\limits_{n=1} ^\infty |x_n|\right)\left(\sum\limits_{n=1} ^\infty |y_n|\right)$?,Is ?,\sum\limits_{n=1} ^\infty |x_n||y_n| ≤\left(\sum\limits_{n=1} ^\infty |x_n|\right)\left(\sum\limits_{n=1} ^\infty |y_n|\right),Is $\sum\limits_{n=1} ^\infty |x_n||y_n| ≤ \left(\sum\limits_{n=1} ^\infty |x_n|\right)\left(\sum\limits_{n=1} ^\infty |y_n|\right)$? I want to use this in a separate exercise but I can't think if this is true let alone how to prove it and I can't find any resources to help.,Is $\sum\limits_{n=1} ^\infty |x_n||y_n| ≤ \left(\sum\limits_{n=1} ^\infty |x_n|\right)\left(\sum\limits_{n=1} ^\infty |y_n|\right)$? I want to use this in a separate exercise but I can't think if this is true let alone how to prove it and I can't find any resources to help.,,"['real-analysis', 'inequality']"
13,Is there a rationality-preserving order isomorphism between $\mathbb{Q}$ and two disjoint open intervals?,Is there a rationality-preserving order isomorphism between  and two disjoint open intervals?,\mathbb{Q},"I have a homework question in a intro logic course, part of which requires me to Find an order preserving isomorphism between $\mathbb{Q}$ and $\mathbb{Q} \cap ((0,1) \cup (2,3))$. So, I need an isomorphism between $\mathbb{Q}$ and two open intervals that preserves both order and rationality. I am familiar with how to do this for any single open interval. For example, $$f(x) = \frac{x}{1+|x|}$$ maps $\mathbb{Q}$ bijectively on to $(-1,1)$, preserving order and rationality. From this I can compose $f$ with some other simple bijection to map $\mathbb{Q}$ to any open interval. I figure it is convenient to begin again by apply the function $f$ so that the task boils down to finding a map from $(-1,1)$ on to $\mathbb{Q} \cap ((0,1) \cup (2,3))$. My initial thought was that if I attempted to split $(-1,1)$ at a rational number $q \in (-1,1)$, then I would have the dilemma of where to map $q$ to preserve the order. So then I figured that I needed to split $\mathbb{Q}$ at an irrational number $\alpha \in (-1,1)$. This way I don't need to map $\alpha$ to anything, and the order will be preserved automatically, provided I can find a map that send $(-1,\alpha)$ to $(0,1)$ and $(\alpha, 1)$ to $(2,3)$. However I cannot find a way to do this that preserves rationality. I would guess what I am trying to do is impossible, but Cantor's theorem on unbounded countable dense linear orders guarantees that some isomorphism exists. (If I am interpreting it correctly). Is there a way to get around the irrationality problem? Or am I way off?","I have a homework question in a intro logic course, part of which requires me to Find an order preserving isomorphism between $\mathbb{Q}$ and $\mathbb{Q} \cap ((0,1) \cup (2,3))$. So, I need an isomorphism between $\mathbb{Q}$ and two open intervals that preserves both order and rationality. I am familiar with how to do this for any single open interval. For example, $$f(x) = \frac{x}{1+|x|}$$ maps $\mathbb{Q}$ bijectively on to $(-1,1)$, preserving order and rationality. From this I can compose $f$ with some other simple bijection to map $\mathbb{Q}$ to any open interval. I figure it is convenient to begin again by apply the function $f$ so that the task boils down to finding a map from $(-1,1)$ on to $\mathbb{Q} \cap ((0,1) \cup (2,3))$. My initial thought was that if I attempted to split $(-1,1)$ at a rational number $q \in (-1,1)$, then I would have the dilemma of where to map $q$ to preserve the order. So then I figured that I needed to split $\mathbb{Q}$ at an irrational number $\alpha \in (-1,1)$. This way I don't need to map $\alpha$ to anything, and the order will be preserved automatically, provided I can find a map that send $(-1,\alpha)$ to $(0,1)$ and $(\alpha, 1)$ to $(2,3)$. However I cannot find a way to do this that preserves rationality. I would guess what I am trying to do is impossible, but Cantor's theorem on unbounded countable dense linear orders guarantees that some isomorphism exists. (If I am interpreting it correctly). Is there a way to get around the irrationality problem? Or am I way off?",,"['real-analysis', 'general-topology', 'model-theory']"
14,Continuous-time version of Fatou's lemma,Continuous-time version of Fatou's lemma,,"I have just read a textbook on stochastic processes that implicitly uses the fact that \begin{equation} \int \liminf_{t \to \infty} f_t \leq \liminf_{t \to \infty} \int f_t, \end{equation} for non-negative measurable functions $\{f_t\}_{t \geq 0}$. Most textbooks state this theorem (Fatou's lemma) in the discrete case. Is it valid that the proof in the discrete case can be used in the continuous case? Can monotone convergence theorem and dominated convergence theorem be generalised to the continuous case too?","I have just read a textbook on stochastic processes that implicitly uses the fact that \begin{equation} \int \liminf_{t \to \infty} f_t \leq \liminf_{t \to \infty} \int f_t, \end{equation} for non-negative measurable functions $\{f_t\}_{t \geq 0}$. Most textbooks state this theorem (Fatou's lemma) in the discrete case. Is it valid that the proof in the discrete case can be used in the continuous case? Can monotone convergence theorem and dominated convergence theorem be generalised to the continuous case too?",,"['real-analysis', 'integration', 'measure-theory']"
15,How to show $-\sup(-A)=\inf(A)$?,How to show ?,-\sup(-A)=\inf(A),"Let $\emptyset\neq A\subseteq\mathbb{R}$ a bounded set. Consider $-A=\{-a:a\in A\}$. I want to prove that $-\sup(-A)=\inf(A)$. It is easy to see that $-\inf(A)$ is an upper bound of $-A$, so $\sup(-A)\le -\inf(A)$, then $-\sup(-A)\ge \inf(A)$. How can we prove that $-\sup(-A)\le \inf(A)$? Thanks.","Let $\emptyset\neq A\subseteq\mathbb{R}$ a bounded set. Consider $-A=\{-a:a\in A\}$. I want to prove that $-\sup(-A)=\inf(A)$. It is easy to see that $-\inf(A)$ is an upper bound of $-A$, so $\sup(-A)\le -\inf(A)$, then $-\sup(-A)\ge \inf(A)$. How can we prove that $-\sup(-A)\le \inf(A)$? Thanks.",,"['real-analysis', 'supremum-and-infimum']"
16,Which negative integer powers of 2 belong to the Cantor Set?,Which negative integer powers of 2 belong to the Cantor Set?,,"Consider the Cantor set $C$, and negative integer powers $2^{-k}$. Clearly, for $k=1$,   $2^{-1} \notin C$ since $1/2 \in (1/3, 2/3)$, the first deleted open interval. It is known that $1/4 = 2^{-2} \in C$, since it alternates between the lower and upper thirds of each non-deleted interval in every iteration, i.e., it lies in $L_1 = [0, 1/3], U_2 = [2/9, 1/3] ...$ and so on. Since $2^{-3} \in (1/9, 2/9)$, a deleted interval, $2^{-3} \notin C$. A slightly more tedious procedure shows that since $2^{-4} \in (1/27, 2/27)$, $2^{-4} \notin C$. My question is as follows: Which negative powers of 2 belong to the Cantor set?  Does it contain any powers other than $1/4$? It seems easy enough to determine this question for special cases (as shown above), but I wonder about the general case. PS: The alternating interval proof for $1/4$ is from the text by Kolmogorov and Fomin.","Consider the Cantor set $C$, and negative integer powers $2^{-k}$. Clearly, for $k=1$,   $2^{-1} \notin C$ since $1/2 \in (1/3, 2/3)$, the first deleted open interval. It is known that $1/4 = 2^{-2} \in C$, since it alternates between the lower and upper thirds of each non-deleted interval in every iteration, i.e., it lies in $L_1 = [0, 1/3], U_2 = [2/9, 1/3] ...$ and so on. Since $2^{-3} \in (1/9, 2/9)$, a deleted interval, $2^{-3} \notin C$. A slightly more tedious procedure shows that since $2^{-4} \in (1/27, 2/27)$, $2^{-4} \notin C$. My question is as follows: Which negative powers of 2 belong to the Cantor set?  Does it contain any powers other than $1/4$? It seems easy enough to determine this question for special cases (as shown above), but I wonder about the general case. PS: The alternating interval proof for $1/4$ is from the text by Kolmogorov and Fomin.",,"['real-analysis', 'analysis']"
17,Proof for and Intuition behind Taylor's Theorem,Proof for and Intuition behind Taylor's Theorem,,"I notice that multiple versions of a theorem are called Taylor (univariate/multivariate, approximate/exact). But I do not find it trivial to infer proof of one version from the rest. So looking for a reference or note that gives some intuition about why it is true and also proves the specific version I am writing below. Taylor's Theorem Suppose that $f: \mathbb{R}^n \to \mathbb{R}$ is continuously differentiable and that $p \in \mathbb{R}^n$, then we   have that $$f(x+p) =f(x) + \nabla f(x+tp)^Tp$$ for some $t \in (0,1)$. Moreover if $f$ is twice continuously differentiable, we have that    $$\nabla f(x+p)= \nabla f(x)  + \int_{0}^1 \nabla^2 f(x+tp)pdt,$$ and that $$f(x+p)  =f(x) +\nabla f(x)^Tp + \frac{1}{2} p^T\nabla^2 > f(x+tp)p,$$ for some $t \in (0,1)$. It is supposed to be available in any calculus textbook.","I notice that multiple versions of a theorem are called Taylor (univariate/multivariate, approximate/exact). But I do not find it trivial to infer proof of one version from the rest. So looking for a reference or note that gives some intuition about why it is true and also proves the specific version I am writing below. Taylor's Theorem Suppose that $f: \mathbb{R}^n \to \mathbb{R}$ is continuously differentiable and that $p \in \mathbb{R}^n$, then we   have that $$f(x+p) =f(x) + \nabla f(x+tp)^Tp$$ for some $t \in (0,1)$. Moreover if $f$ is twice continuously differentiable, we have that    $$\nabla f(x+p)= \nabla f(x)  + \int_{0}^1 \nabla^2 f(x+tp)pdt,$$ and that $$f(x+p)  =f(x) +\nabla f(x)^Tp + \frac{1}{2} p^T\nabla^2 > f(x+tp)p,$$ for some $t \in (0,1)$. It is supposed to be available in any calculus textbook.",,"['calculus', 'real-analysis', 'reference-request']"
18,"A function such that $f'(x)>0$, but not strictly monotone increasing.","A function such that , but not strictly monotone increasing.",f'(x)>0,"This is Exercise 10.3.5 from Analysis Vol.1 by Terence Tao . Give an example of a subset $X \subset \mathbb{R}$ and a function $f: X \to \mathbb{R}$ which is differentiable on $X$, is such that $f'(x)>0$ for all $x \in X$, but $f$ is not strictly monotone increasing. Thanks.","This is Exercise 10.3.5 from Analysis Vol.1 by Terence Tao . Give an example of a subset $X \subset \mathbb{R}$ and a function $f: X \to \mathbb{R}$ which is differentiable on $X$, is such that $f'(x)>0$ for all $x \in X$, but $f$ is not strictly monotone increasing. Thanks.",,"['calculus', 'real-analysis', 'examples-counterexamples']"
19,"How to integrate $\int \frac{\cos x}{\sqrt{\sin2x}} \,dx$?",How to integrate ?,"\int \frac{\cos x}{\sqrt{\sin2x}} \,dx","How to integrate $$\int \frac{\cos x}{\sqrt{\sin2x}} \,dx$$ ? I have: $$\int \frac{\cos x}{\sqrt{\sin2x}} \,dx = \int \frac{\cos x}{\sqrt{2\sin x\cos x}} \,dx = \frac{1}{\sqrt2}\int \frac{\cos x}{\sqrt{\sin x}\sqrt{\cos x}} \,dx = \frac{1}{\sqrt2}\int \frac{\sqrt{\cos x}}{\sqrt{\sin x}} \,dx = \frac{1}{\sqrt2}\int \sqrt{\frac{\cos x}{\sin x}} \,dx = \frac{1}{\sqrt2}\int \sqrt{\cot x} \,dx \\ t = \sqrt{\cot x} \implies x = \cot^{-1} t^2 \implies \,dx = -\frac{2t\,dt}{1 + t^4}$$ so I have: $$-\sqrt2 \int \frac{t^2 \,dt}{1 + t^4}$$ I tried partial integration on that but it just gets more complicated. I also tried the substitution $t = \tan \frac{x}{2}$ on this one: $\frac{1}{\sqrt2}\int \sqrt{\frac{\cos x}{\sin x}} \,dx$ $$= \frac{1}{\sqrt2}\int \sqrt{\frac{\frac{1 - t^2}{1 + t^2}}{\frac{2t}{1+t^2}}} \frac{2\,dt}{1+t^2} = \frac{1}{\sqrt2}\int \sqrt{\frac{1 - t^2}{2t}}  \frac{2\,dt}{1+t^2} = \int \sqrt{\frac{1 - t^2}{t}}  \frac{\,dt}{1+t^2}$$ ... which doesn't look very promising. Any hints are appreciated!","How to integrate $$\int \frac{\cos x}{\sqrt{\sin2x}} \,dx$$ ? I have: $$\int \frac{\cos x}{\sqrt{\sin2x}} \,dx = \int \frac{\cos x}{\sqrt{2\sin x\cos x}} \,dx = \frac{1}{\sqrt2}\int \frac{\cos x}{\sqrt{\sin x}\sqrt{\cos x}} \,dx = \frac{1}{\sqrt2}\int \frac{\sqrt{\cos x}}{\sqrt{\sin x}} \,dx = \frac{1}{\sqrt2}\int \sqrt{\frac{\cos x}{\sin x}} \,dx = \frac{1}{\sqrt2}\int \sqrt{\cot x} \,dx \\ t = \sqrt{\cot x} \implies x = \cot^{-1} t^2 \implies \,dx = -\frac{2t\,dt}{1 + t^4}$$ so I have: $$-\sqrt2 \int \frac{t^2 \,dt}{1 + t^4}$$ I tried partial integration on that but it just gets more complicated. I also tried the substitution $t = \tan \frac{x}{2}$ on this one: $\frac{1}{\sqrt2}\int \sqrt{\frac{\cos x}{\sin x}} \,dx$ $$= \frac{1}{\sqrt2}\int \sqrt{\frac{\frac{1 - t^2}{1 + t^2}}{\frac{2t}{1+t^2}}} \frac{2\,dt}{1+t^2} = \frac{1}{\sqrt2}\int \sqrt{\frac{1 - t^2}{2t}}  \frac{2\,dt}{1+t^2} = \int \sqrt{\frac{1 - t^2}{t}}  \frac{\,dt}{1+t^2}$$ ... which doesn't look very promising. Any hints are appreciated!",,"['real-analysis', 'integration', 'indefinite-integrals']"
20,What is so special about the Lebesgue-Stieltjes measure,What is so special about the Lebesgue-Stieltjes measure,,"A measure $\lambda: B(\mathbb{R}^n) \rightarrow  \overline{{\mathbb{R_{\ge 0}}}}$ that is associated with a monotone increasing and right-side continuous function $F$ is called a Lebesgue-Stieltjes measure. But I am wondering, why it is not true that every measure $\lambda: B(\mathbb{R}^n) \rightarrow  \overline{{\mathbb{R_{\ge 0}}}}$ is a Lebesgue-Stieltjes measure?","A measure $\lambda: B(\mathbb{R}^n) \rightarrow  \overline{{\mathbb{R_{\ge 0}}}}$ that is associated with a monotone increasing and right-side continuous function $F$ is called a Lebesgue-Stieltjes measure. But I am wondering, why it is not true that every measure $\lambda: B(\mathbb{R}^n) \rightarrow  \overline{{\mathbb{R_{\ge 0}}}}$ is a Lebesgue-Stieltjes measure?",,"['real-analysis', 'measure-theory']"
21,How can the sum of two closed cones be not closed?,How can the sum of two closed cones be not closed?,,Can there be two closed cones $K_1$ and $K_2$ in $\mathbb{R}^3$ such that $K_1+K_2$ need not be closed?,Can there be two closed cones $K_1$ and $K_2$ in $\mathbb{R}^3$ such that $K_1+K_2$ need not be closed?,,"['real-analysis', 'general-topology', 'convex-analysis', 'convex-optimization']"
22,"Prove A is open if and only if w+A is open, A is closed if and only if w+A is closed.","Prove A is open if and only if w+A is open, A is closed if and only if w+A is closed.",,Let A be a subset of $\Bbb R^n$ and let $\mathbf w$ be a point in $\Bbb R^n$. The translate of A by $\mathbf w$ is denoted $\mathbf w$ + A and is defined by $$\mathbf w+A \equiv \{\mathbf w + \mathbf u\mid \mathbf u\text{ in A}\}.$$ $\mathbf a.$ Show that A is open if and only if $\mathbf w$ + A is open. $\mathbf b.$ Show that A is closed if and only if $\mathbf w$ + A is closed. For part $\mathbf a.$ I know that if A is open then $\mathbf u$ is an interior point of A. Is it correct to say: $$ \therefore  \mathbf w + \mathbf u \text{ must be an interior point of } \mathbf w + A \implies \mathbf w + A \text{ is open.}$$  and I'm not sure how to get started on $\mathbf b.$,Let A be a subset of $\Bbb R^n$ and let $\mathbf w$ be a point in $\Bbb R^n$. The translate of A by $\mathbf w$ is denoted $\mathbf w$ + A and is defined by $$\mathbf w+A \equiv \{\mathbf w + \mathbf u\mid \mathbf u\text{ in A}\}.$$ $\mathbf a.$ Show that A is open if and only if $\mathbf w$ + A is open. $\mathbf b.$ Show that A is closed if and only if $\mathbf w$ + A is closed. For part $\mathbf a.$ I know that if A is open then $\mathbf u$ is an interior point of A. Is it correct to say: $$ \therefore  \mathbf w + \mathbf u \text{ must be an interior point of } \mathbf w + A \implies \mathbf w + A \text{ is open.}$$  and I'm not sure how to get started on $\mathbf b.$,,['real-analysis']
23,"Inequality $\left|\,x_1\,\right|+\left|\,x_2\,\right|+\cdots+\left|\,x_p\,\right|\leq\sqrt{p}\sqrt{x^2_1+x^2_2+\cdots+x^2_p}$",Inequality,"\left|\,x_1\,\right|+\left|\,x_2\,\right|+\cdots+\left|\,x_p\,\right|\leq\sqrt{p}\sqrt{x^2_1+x^2_2+\cdots+x^2_p}","I could use some help with proving this inequality: $$\left|\,x_1\,\right|+\left|\,x_2\,\right|+\cdots+\left|\,x_p\,\right|\leq\sqrt{p}\sqrt{x^2_1+x^2_2+\cdots+x^2_p}$$ for all natural numbers p. Aside from demonstrating the truth of the statement itself, apparently $\sqrt{p}$ is the smallest possible value by which the right hand side square root expression must be multiplied by in order for the statement to be true.  I've tried various ways of doing this, and I've tried to steer clear of induction because I'm not sure that's what the exercise was designed for (from Bartle's Elements of Real Analysis), but the best I've been able to come up with is proving that the statement is true when the right hand side square root expression is multiplied by p, which seems pretty obvious anyway.  I feel like I'm staring directly at the answer and still can't see it.  Any help would be appreciated.","I could use some help with proving this inequality: for all natural numbers p. Aside from demonstrating the truth of the statement itself, apparently is the smallest possible value by which the right hand side square root expression must be multiplied by in order for the statement to be true.  I've tried various ways of doing this, and I've tried to steer clear of induction because I'm not sure that's what the exercise was designed for (from Bartle's Elements of Real Analysis), but the best I've been able to come up with is proving that the statement is true when the right hand side square root expression is multiplied by p, which seems pretty obvious anyway.  I feel like I'm staring directly at the answer and still can't see it.  Any help would be appreciated.","\left|\,x_1\,\right|+\left|\,x_2\,\right|+\cdots+\left|\,x_p\,\right|\leq\sqrt{p}\sqrt{x^2_1+x^2_2+\cdots+x^2_p} \sqrt{p}","['real-analysis', 'inequality']"
24,Show that $\frac{1}{2}-\frac{1}{2e}<\int_0^{+\infty}e^{-x^2}dx<1+\frac{1}{2e}$,Show that,\frac{1}{2}-\frac{1}{2e}<\int_0^{+\infty}e^{-x^2}dx<1+\frac{1}{2e},"Show that $$\frac{1}{2}-\frac{1}{2e}<\int_0^{+\infty}e^{-x^2}dx<1+\frac{1}{2e}$$ I know that one way to do this is to evaluate the integral in the middle, and then compare these three numbers. I wonder how can we do this without explicitly compute the integral?","Show that $$\frac{1}{2}-\frac{1}{2e}<\int_0^{+\infty}e^{-x^2}dx<1+\frac{1}{2e}$$ I know that one way to do this is to evaluate the integral in the middle, and then compare these three numbers. I wonder how can we do this without explicitly compute the integral?",,"['calculus', 'real-analysis', 'integration']"
25,Show that $\frac{(1-y^{-1})^2}{(1-y^{-1 -c})(1-y^{-1+c} )}$ is decreasing in $y > 1 $.,Show that  is decreasing in .,\frac{(1-y^{-1})^2}{(1-y^{-1 -c})(1-y^{-1+c} )} y > 1 ,"I am interested in the function $f(y) = \frac{(1-y^{-1})^2}{(1-y^{-1 -c})(1-y^{-1+c} )},$ for values of $c \in (0,1)$, and $y > 1$, and have been trying to show that the function is decreasing. I have tried differentiating the function, but this does not yield a particularly amenable formula to decide whether $f'(y) < 0$. Similarly I have considered the ratios $f(y)/f(\tilde y)$, but again have been unable to ascertain an inequality for this. I'd appreciate any other approaches to the problem.","I am interested in the function $f(y) = \frac{(1-y^{-1})^2}{(1-y^{-1 -c})(1-y^{-1+c} )},$ for values of $c \in (0,1)$, and $y > 1$, and have been trying to show that the function is decreasing. I have tried differentiating the function, but this does not yield a particularly amenable formula to decide whether $f'(y) < 0$. Similarly I have considered the ratios $f(y)/f(\tilde y)$, but again have been unable to ascertain an inequality for this. I'd appreciate any other approaches to the problem.",,"['real-analysis', 'functions']"
26,"Show that if $A\subseteq B$, then inf $B\leq$ inf $A\leq$ sup $A \leq$ sup $B$","Show that if , then inf  inf  sup  sup",A\subseteq B B\leq A\leq A \leq B,"Is my logic correct or am I missing something? Show that if $A\subseteq B$, then inf $B\leq$ inf $A\leq$ sup $A \leq$ sup $B$ Case 1. If $A \subset B$ then there exists $b_1,b_2\in B$ such that $b_1,b_2 \notin  A$. Let $a_1,a_2 \in A$ such that $a_1=$ inf A and $a_2$ = sup A. Suppose $b_1$=inf B and $b_2$=sup B than $b_1<a_1<a_2<b_2$ which implies inf $B<$ inf $A < $ sup $A<$ sup $B$ Case 2. If $A = B$ than every element in $A$ is in $B$. This implies that if $A$ is bounded above or below so is $B$ and vice versa. If the sup $B$ is defined to be the least upper bound and the inf $B$ is defined to be the greatest lowest bound. Than sup $B$ = sup $A$ and inf $B$ = inf $A$. Since $A \subseteq B$ the following equality can be written as inf $B\leq$ inf $A\leq$ sup $A \leq$ sup $B$","Is my logic correct or am I missing something? Show that if $A\subseteq B$, then inf $B\leq$ inf $A\leq$ sup $A \leq$ sup $B$ Case 1. If $A \subset B$ then there exists $b_1,b_2\in B$ such that $b_1,b_2 \notin  A$. Let $a_1,a_2 \in A$ such that $a_1=$ inf A and $a_2$ = sup A. Suppose $b_1$=inf B and $b_2$=sup B than $b_1<a_1<a_2<b_2$ which implies inf $B<$ inf $A < $ sup $A<$ sup $B$ Case 2. If $A = B$ than every element in $A$ is in $B$. This implies that if $A$ is bounded above or below so is $B$ and vice versa. If the sup $B$ is defined to be the least upper bound and the inf $B$ is defined to be the greatest lowest bound. Than sup $B$ = sup $A$ and inf $B$ = inf $A$. Since $A \subseteq B$ the following equality can be written as inf $B\leq$ inf $A\leq$ sup $A \leq$ sup $B$",,"['real-analysis', 'inequality', 'supremum-and-infimum']"
27,Integral approximation of Fourier series.,Integral approximation of Fourier series.,,"Consider a function $f$ defined on the real line. Consider the restriction of the function to the interval $[-L,L]$ and periodically extend the function using a Fourier series $$f_L(x) = \sum_{n=-\infty}^\infty \hat{f}(k_n)e^{ik_nx}$$ where $k_n=\pi n/L$ and where $\hat{f}(k_n)$ denotes the Fourier coefficient. For sufficiently large $L$, it seems plausible that we may approximate the Fourier series with an integral. In particular, let us extend $\hat{f}(k_n)$ to non-integral values through $$\hat{f}(k)=\frac{1}{2L}\int_{-L}^L f_L(x)\,e^{-ikx}\ dx $$ and write the approximation as $$\tilde{f}_L(x)=\frac{2L}{(2\pi)}\int_{-\infty}^\infty\hat{f}(k)\,e^{ikx}\ dk$$ To provide a bit of context, I first encountered such integral approximations when studying cosmology. It seems common in physics to take such approximations where we formally make the replacement $$\sum_{n=-\infty}^\infty \longrightarrow \frac{2L}{(2\pi)}\int_{-\infty}^\infty$$ in the limit of large $L$, which is called the continuum limit . My question is under what conditions is such an approximation valid? What kind of smoothness and regularity conditions need to be placed on $f$? How well does $\tilde{f}$ approximate $f$ on $[-L,L]$ (for sufficiently large $L$)? Are there any error bounds?","Consider a function $f$ defined on the real line. Consider the restriction of the function to the interval $[-L,L]$ and periodically extend the function using a Fourier series $$f_L(x) = \sum_{n=-\infty}^\infty \hat{f}(k_n)e^{ik_nx}$$ where $k_n=\pi n/L$ and where $\hat{f}(k_n)$ denotes the Fourier coefficient. For sufficiently large $L$, it seems plausible that we may approximate the Fourier series with an integral. In particular, let us extend $\hat{f}(k_n)$ to non-integral values through $$\hat{f}(k)=\frac{1}{2L}\int_{-L}^L f_L(x)\,e^{-ikx}\ dx $$ and write the approximation as $$\tilde{f}_L(x)=\frac{2L}{(2\pi)}\int_{-\infty}^\infty\hat{f}(k)\,e^{ikx}\ dk$$ To provide a bit of context, I first encountered such integral approximations when studying cosmology. It seems common in physics to take such approximations where we formally make the replacement $$\sum_{n=-\infty}^\infty \longrightarrow \frac{2L}{(2\pi)}\int_{-\infty}^\infty$$ in the limit of large $L$, which is called the continuum limit . My question is under what conditions is such an approximation valid? What kind of smoothness and regularity conditions need to be placed on $f$? How well does $\tilde{f}$ approximate $f$ on $[-L,L]$ (for sufficiently large $L$)? Are there any error bounds?",,"['real-analysis', 'fourier-analysis']"
28,proof of the chain rule for calculus,proof of the chain rule for calculus,,"I was comparing my attempt to prove the chain rule by my own and the proof given in Spivak's book but they seems to be rather different. Please tell me if I'm wrong or if I'm missing something. I really appreciate any comment. Thank you so much. Theorem: Let $I$ and $J$ be open intervals in $\mathbb{R}$. Let $a\in I$ and let $f:I\longrightarrow J$ and $g:J\longrightarrow \mathbb{R}$ functions. Suppose that $f$ is differentiable at $a$, and that $g$ is differentiable at $f(a)$. Then $g\circ f$ is differentiable at $a$ and $(g\circ f)'(a)=g'(f(a))f'(a)$ Idea and observations: We wanted to find $\lim_{h\to 0}\frac{(g\circ f)(a+h)-g\circ f(a)}{h}= \lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{h}$, which seems to be the same as $\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{f(a+h)-f(a)}\cdot \frac{f(a+h)-f(a)}{h}=g'(f(a))\cdot f'(a)$. The problem here, as Spivak suggests is that $f(a+h)$ may be equals $f(a)$ for some values of $h$ and then the division might not be defined for such values. In this case Spivak defines a new function $\phi$ such that $\phi(h)=\frac{g(f(a+h))-f(a)}{f(a+h)-f(a)}$ if  $f(a+h)\neq f(a)$ and $f'(g(a))$ otherwise. Then he proves that $\phi$ is continuous by using $\epsilon - \delta$ definitions and since $\frac{(g\circ f)(a+h)-f(a)}{h}=\phi(h)\cdot \frac{f(a+h)-f(a)}{h}$, the result follows. Here my approach is rather different by using the fact that if $\lim_{x\to a }f=b$ and $\lim_{x\to b} g=l$ then $\lim_{x\to a}g\circ f=l$ (This implies at the same time that $f'(a)=\lim_{h\to 0} \frac{f(a+h)-f(a)}{h}=\lim_{h\to a}\frac{f(h)-f(a)}{h-a}$). Proof: Since $f$ is diferentiable at $a$ then $f$ is continuous at $a$. Then $\lim_{h\to a}f(h)=f(a)$. Since  $\lim_{h\to 0}(a+h)=a$ then, $1)$ $\lim_{h\to 0}f(a+h)=f(a)$. Now, by hypothesis, $2)$ $ \lim_{h\to f(a)}\frac{g(h)-g(f(a))}{h-f(a)}=g'(f(a))$, and $3)$ $\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}=f'(a)$ therefore, from $1)$ and $2)$ $\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{f(a+h)-f(a)}=g'(f(a))$. Finally, $(g\circ f)'(a)=\lim_{h\to 0}\frac{g(f(a+h))-g\circ f(a)}{h}=\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{h}=\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{f(a+h)-f(a)}\cdot \frac{f(a+h)-f(a)}{h}=g'(f(a))\cdot f'(a)$.","I was comparing my attempt to prove the chain rule by my own and the proof given in Spivak's book but they seems to be rather different. Please tell me if I'm wrong or if I'm missing something. I really appreciate any comment. Thank you so much. Theorem: Let $I$ and $J$ be open intervals in $\mathbb{R}$. Let $a\in I$ and let $f:I\longrightarrow J$ and $g:J\longrightarrow \mathbb{R}$ functions. Suppose that $f$ is differentiable at $a$, and that $g$ is differentiable at $f(a)$. Then $g\circ f$ is differentiable at $a$ and $(g\circ f)'(a)=g'(f(a))f'(a)$ Idea and observations: We wanted to find $\lim_{h\to 0}\frac{(g\circ f)(a+h)-g\circ f(a)}{h}= \lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{h}$, which seems to be the same as $\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{f(a+h)-f(a)}\cdot \frac{f(a+h)-f(a)}{h}=g'(f(a))\cdot f'(a)$. The problem here, as Spivak suggests is that $f(a+h)$ may be equals $f(a)$ for some values of $h$ and then the division might not be defined for such values. In this case Spivak defines a new function $\phi$ such that $\phi(h)=\frac{g(f(a+h))-f(a)}{f(a+h)-f(a)}$ if  $f(a+h)\neq f(a)$ and $f'(g(a))$ otherwise. Then he proves that $\phi$ is continuous by using $\epsilon - \delta$ definitions and since $\frac{(g\circ f)(a+h)-f(a)}{h}=\phi(h)\cdot \frac{f(a+h)-f(a)}{h}$, the result follows. Here my approach is rather different by using the fact that if $\lim_{x\to a }f=b$ and $\lim_{x\to b} g=l$ then $\lim_{x\to a}g\circ f=l$ (This implies at the same time that $f'(a)=\lim_{h\to 0} \frac{f(a+h)-f(a)}{h}=\lim_{h\to a}\frac{f(h)-f(a)}{h-a}$). Proof: Since $f$ is diferentiable at $a$ then $f$ is continuous at $a$. Then $\lim_{h\to a}f(h)=f(a)$. Since  $\lim_{h\to 0}(a+h)=a$ then, $1)$ $\lim_{h\to 0}f(a+h)=f(a)$. Now, by hypothesis, $2)$ $ \lim_{h\to f(a)}\frac{g(h)-g(f(a))}{h-f(a)}=g'(f(a))$, and $3)$ $\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}=f'(a)$ therefore, from $1)$ and $2)$ $\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{f(a+h)-f(a)}=g'(f(a))$. Finally, $(g\circ f)'(a)=\lim_{h\to 0}\frac{g(f(a+h))-g\circ f(a)}{h}=\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{h}=\lim_{h\to 0}\frac{g(f(a+h))-g(f(a))}{f(a+h)-f(a)}\cdot \frac{f(a+h)-f(a)}{h}=g'(f(a))\cdot f'(a)$.",,"['calculus', 'real-analysis']"
29,"How to Prove that if $\sum_{k=0}^{\infty}a_k$ converges, $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges uniformly on $[0, 1]$.","How to Prove that if  converges,  converges uniformly on .","\sum_{k=0}^{\infty}a_k \sum_{k=0}^{\infty}{a_k}{x^k} [0, 1]","Prove that if $\sum_{k=0}^{\infty}a_k$ converges, $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges uniformly on $[0, 1]$. I already know that $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges pointwise on $[0, 1]$ by either Abel's or Dirichlet's Test for pointwise convergence. Now, can I do this proof without Dirichlet's Test for uniform convergence? I don't want to use it because, in the text, this question came before Dirichlet's Test for uniform convergence. Since we don't know that $\sum_{k=0}^{\infty}a_k$ converges absolutely, we can't say that for all $x \in [0, 1]$, $|{a_k}{x^k}| \le |a_k|$ and apply the Weiertrass M-test. Also, since we don't know if $\sum_{k=0}^{\infty}a_k$ is an alternating series, we can't say that that since $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges pointwise, let $\sum_{k=0}^{\infty}{a_k}{x^k} = A(x)$ and observe that for all $x \in [0, 1]$, $$|A(x) - \sum_{k=0}^{n}{a_k}{x^k}| \le |a_{n+1}|x^{n+1} \le |a_{n+1}| \rightarrow 0$$ as $n$ goes to infinity. A clue is sufficient.  I really want to think this problem through.  TY!","Prove that if $\sum_{k=0}^{\infty}a_k$ converges, $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges uniformly on $[0, 1]$. I already know that $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges pointwise on $[0, 1]$ by either Abel's or Dirichlet's Test for pointwise convergence. Now, can I do this proof without Dirichlet's Test for uniform convergence? I don't want to use it because, in the text, this question came before Dirichlet's Test for uniform convergence. Since we don't know that $\sum_{k=0}^{\infty}a_k$ converges absolutely, we can't say that for all $x \in [0, 1]$, $|{a_k}{x^k}| \le |a_k|$ and apply the Weiertrass M-test. Also, since we don't know if $\sum_{k=0}^{\infty}a_k$ is an alternating series, we can't say that that since $\sum_{k=0}^{\infty}{a_k}{x^k}$ converges pointwise, let $\sum_{k=0}^{\infty}{a_k}{x^k} = A(x)$ and observe that for all $x \in [0, 1]$, $$|A(x) - \sum_{k=0}^{n}{a_k}{x^k}| \le |a_{n+1}|x^{n+1} \le |a_{n+1}| \rightarrow 0$$ as $n$ goes to infinity. A clue is sufficient.  I really want to think this problem through.  TY!",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
30,Fourier transform using principal value,Fourier transform using principal value,,Can anyone help me  compute the Fourier transform of $ 1/|x|^{n-\alpha} $ in $\mathbb{R}^n $ where $ 0 < \alpha < n $ ? Somehow it becomes the principal value of $ 1/|x|^\alpha $ which I can't figure out.,Can anyone help me  compute the Fourier transform of $ 1/|x|^{n-\alpha} $ in $\mathbb{R}^n $ where $ 0 < \alpha < n $ ? Somehow it becomes the principal value of $ 1/|x|^\alpha $ which I can't figure out.,,"['real-analysis', 'fourier-analysis', 'distribution-theory', 'integral-transforms']"
31,"Continuity of argmax (moving domain and function), under uniqueness hypothesis","Continuity of argmax (moving domain and function), under uniqueness hypothesis",,"Let $f:[0,1]\times\mathbb{R}\to\mathbb{R}$ continuous and $c:\mathbb{R}\to[0,1]$ continuous. Consider $$F:\mathbb{R}\to\mathbb{R},\ \ F(x)=\max_{t\in[0,c(x)]}f(t,x)$$ Here Continuity of max (moving the domain and the function) it was proved that $F$ is continuous. Now assume also that for every $x\in\mathbb{R}$ $\textrm{argmax}_{t\in[0,c(x)]}\ f(t,x)$ is unique (that is there exists a unique point $t\in[0,c(x)]$ s.t. $f(t,x)=F(x)$ ). And consider $$T:\mathbb{R}\to\mathbb{[0,1]},\ \ T(x)=\textrm{argmax}_{t\in[0,c(x)]}\ f(t,x)$$ Is $T$ continuous too?","Let $f:[0,1]\times\mathbb{R}\to\mathbb{R}$ continuous and $c:\mathbb{R}\to[0,1]$ continuous. Consider $$F:\mathbb{R}\to\mathbb{R},\ \ F(x)=\max_{t\in[0,c(x)]}f(t,x)$$ Here Continuity of max (moving the domain and the function) it was proved that $F$ is continuous. Now assume also that for every $x\in\mathbb{R}$ $\textrm{argmax}_{t\in[0,c(x)]}\ f(t,x)$ is unique (that is there exists a unique point $t\in[0,c(x)]$ s.t. $f(t,x)=F(x)$ ). And consider $$T:\mathbb{R}\to\mathbb{[0,1]},\ \ T(x)=\textrm{argmax}_{t\in[0,c(x)]}\ f(t,x)$$ Is $T$ continuous too?",,"['real-analysis', 'general-topology']"
32,Prove sequence is not convergent in a complete metric space,Prove sequence is not convergent in a complete metric space,,"I'm looking over some previous analysis exams and I've come across this question: Consider the set $\mathbb{R}$ of real numbers and the metric function   defined as: $$d(x, y) = \begin{cases} 1 ~ \text{if} ~ x \ne y \\ 0 ~ \text{if} ~ x = y \end{cases}$$ Is the sequence $(x_n)_{n = 1}^\infty$ defined by $x_n = \frac{1}{n}$ convergent in this metric space? Is this metric space complete? For part 1: Assume that $(x_n)_{n = 1}^\infty$ is convergent to a real $a$. Then, for every $\epsilon > 0$, there exists a $N \in \mathbb{N}$ such that: $$d(x_n, a) < \epsilon ~ ~ \text{for all} ~ ~ n \geq N$$ Let $\epsilon = \frac{1}{2}$. Then $d(x_n, a) < \frac{1}{2} ~ ~ \implies ~ ~ x_n = a ~ ~ \implies ~ ~ a = \frac{1}{n} ~ ~ \text{for all} ~ ~ n \geq N$. This is a contradiction, so $(x_n)_{n = 1}^\infty$ does not converge in this metric space. For part 2: A metric space is complete iff every Cauchy sequence is convergent. A Cauchy sequence is a sequence $(x_n)_{n = 1}^\infty$ such that for every $\epsilon > 0$, there exists an $N \in \mathbb{N}$ such that: $$d(x_n, x_m) < \epsilon ~ ~ \text{for all} ~ ~ n, m \geq N$$ Reusing part 1's approach, we let $\epsilon = \frac{1}{2}$. Then if $(x_n)_{n = 1}^\infty$ is a Cauchy sequence, it must follow that $x_n = x_m$ for all $n, m \geq N$. So the sequence is constant for $n \geq N$, and as such trivially converges. We have proved that if $(x_n)_{n = 1}^\infty$ is a Cauchy sequence in this metric space, then it must be convergent. Therefore this metric space is complete. Are these proofs valid? In particular, is the use of contradiction sound in part 1 and does the reasoning follow in part 2? I've learned to be careful with analysis arguments, so could someone check over my work? I am confident about part 1 but there's something about part 2 that confuses me. The completeness property only applies to Cauchy sequences, so if my results are correct then the sequence in part 1 is not Cauchy. So did I make a mistake or was $d$ carefully chosen to defy intuition?","I'm looking over some previous analysis exams and I've come across this question: Consider the set $\mathbb{R}$ of real numbers and the metric function   defined as: $$d(x, y) = \begin{cases} 1 ~ \text{if} ~ x \ne y \\ 0 ~ \text{if} ~ x = y \end{cases}$$ Is the sequence $(x_n)_{n = 1}^\infty$ defined by $x_n = \frac{1}{n}$ convergent in this metric space? Is this metric space complete? For part 1: Assume that $(x_n)_{n = 1}^\infty$ is convergent to a real $a$. Then, for every $\epsilon > 0$, there exists a $N \in \mathbb{N}$ such that: $$d(x_n, a) < \epsilon ~ ~ \text{for all} ~ ~ n \geq N$$ Let $\epsilon = \frac{1}{2}$. Then $d(x_n, a) < \frac{1}{2} ~ ~ \implies ~ ~ x_n = a ~ ~ \implies ~ ~ a = \frac{1}{n} ~ ~ \text{for all} ~ ~ n \geq N$. This is a contradiction, so $(x_n)_{n = 1}^\infty$ does not converge in this metric space. For part 2: A metric space is complete iff every Cauchy sequence is convergent. A Cauchy sequence is a sequence $(x_n)_{n = 1}^\infty$ such that for every $\epsilon > 0$, there exists an $N \in \mathbb{N}$ such that: $$d(x_n, x_m) < \epsilon ~ ~ \text{for all} ~ ~ n, m \geq N$$ Reusing part 1's approach, we let $\epsilon = \frac{1}{2}$. Then if $(x_n)_{n = 1}^\infty$ is a Cauchy sequence, it must follow that $x_n = x_m$ for all $n, m \geq N$. So the sequence is constant for $n \geq N$, and as such trivially converges. We have proved that if $(x_n)_{n = 1}^\infty$ is a Cauchy sequence in this metric space, then it must be convergent. Therefore this metric space is complete. Are these proofs valid? In particular, is the use of contradiction sound in part 1 and does the reasoning follow in part 2? I've learned to be careful with analysis arguments, so could someone check over my work? I am confident about part 1 but there's something about part 2 that confuses me. The completeness property only applies to Cauchy sequences, so if my results are correct then the sequence in part 1 is not Cauchy. So did I make a mistake or was $d$ carefully chosen to defy intuition?",,"['real-analysis', 'metric-spaces']"
33,"Open, closed and continuous","Open, closed and continuous",,"I have some troubles to understanding something: We were asked to find a function that is open and continuous but not closed and actually I found such a function, but our tutor gave us this example $ e^x$ since this function is continuous as a mapping $$\exp: \mathbb{R} \rightarrow \mathbb{R}_{>0}$$ and the inverse function is continuous too, so this function is also open. and the counterexpample was, that $\mathbb{R}$ is closed and is mapped to $\mathbb{R}_{>0}$ which is open. then i thought, hey if the inverse function is continuous that means that the fiber of closed sets have to be closed, but $\ln^{-1}(\mathbb{R})$ is not closed at all. where am i wrong?","I have some troubles to understanding something: We were asked to find a function that is open and continuous but not closed and actually I found such a function, but our tutor gave us this example $ e^x$ since this function is continuous as a mapping $$\exp: \mathbb{R} \rightarrow \mathbb{R}_{>0}$$ and the inverse function is continuous too, so this function is also open. and the counterexpample was, that $\mathbb{R}$ is closed and is mapped to $\mathbb{R}_{>0}$ which is open. then i thought, hey if the inverse function is continuous that means that the fiber of closed sets have to be closed, but $\ln^{-1}(\mathbb{R})$ is not closed at all. where am i wrong?",,['real-analysis']
34,What is the formulation of the Least Upper Bound propierty in First Order Logic?,What is the formulation of the Least Upper Bound propierty in First Order Logic?,,"I've been readining about the completeness Godel's theorems. Accordingly, the axioms of $R$ in first order logic make up one of these sets that is complete and consistent. But I've always seen the axiom of the least upper bound formulated in second order logic. What is the equivalent formulation in first order logic?","I've been readining about the completeness Godel's theorems. Accordingly, the axioms of $R$ in first order logic make up one of these sets that is complete and consistent. But I've always seen the axiom of the least upper bound formulated in second order logic. What is the equivalent formulation in first order logic?",,"['real-analysis', 'logic', 'model-theory']"
35,Integral defines a Radon measure,Integral defines a Radon measure,,"Suppose that $\mu$ is a Radon measure on $X$. If $\phi\in L^{1}(\mu)$ and $\phi\geq 0$, show that $\nu(E)=\int_{E}\phi\;d\mu$ is a Radon measure. $\nu$ is a finite measure so we just need to check the regularity conditions. I was showing the outer regularity on all Borel sets and I did it for sets with finite $\mu$-measure. I'm not sure about the argument for sets with infinite $\mu$-measure. Likewise for inner regularity on open sets.","Suppose that $\mu$ is a Radon measure on $X$. If $\phi\in L^{1}(\mu)$ and $\phi\geq 0$, show that $\nu(E)=\int_{E}\phi\;d\mu$ is a Radon measure. $\nu$ is a finite measure so we just need to check the regularity conditions. I was showing the outer regularity on all Borel sets and I did it for sets with finite $\mu$-measure. I'm not sure about the argument for sets with infinite $\mu$-measure. Likewise for inner regularity on open sets.",,"['real-analysis', 'measure-theory']"
36,"Computing $\lim_{n \rightarrow\infty} \int_{a}^{b}\left ( f(x)\left | \sin(nx) \right | \right )$ with $f$ continuous on $[a,b]$",Computing  with  continuous on,"\lim_{n \rightarrow\infty} \int_{a}^{b}\left ( f(x)\left | \sin(nx) \right | \right ) f [a,b]","Let $a,b \in \mathbb{R}$ and $\textit{f} :[a,b] \rightarrow \mathbb{R}$ continuous on $[a,b]$. Does the sequence $\left (\int_{a}^{b} f(x)\left |\sin(nx) \right |dx \right )$ converge? If it does, what is its limit ? I know how to solve this for $\left(\int_{a}^{b} f(x)\sin(nx)dx \right )$ with integration by parts when $f$ is a class $C^1$ function. Here, I don't know how to deal with the absolute value and the non-differentiability of $\textit{f}$. Any help is appreciated, thanks in advance.","Let $a,b \in \mathbb{R}$ and $\textit{f} :[a,b] \rightarrow \mathbb{R}$ continuous on $[a,b]$. Does the sequence $\left (\int_{a}^{b} f(x)\left |\sin(nx) \right |dx \right )$ converge? If it does, what is its limit ? I know how to solve this for $\left(\int_{a}^{b} f(x)\sin(nx)dx \right )$ with integration by parts when $f$ is a class $C^1$ function. Here, I don't know how to deal with the absolute value and the non-differentiability of $\textit{f}$. Any help is appreciated, thanks in advance.",,"['real-analysis', 'integration', 'limits', 'definite-integrals']"
37,"Proof of the ""Radius of Convergence Theorem""","Proof of the ""Radius of Convergence Theorem""",,"I can't figure out how it is valid to invoke the Absolute Convergence Theorem, whose hypothesis is ""Let the power series have radius of convergence R"", to establish case c of the Radius of Convergence Theorem (basically concluding that ""the power series has a radius of convergence...""). Isn't it circular reasoning to prove X by assuming X itself? (The aforementioned ACT is invoked beside the second, and subsequently again after the third, margin note below.) I would be extremely grateful for any help resolving this. P.S. The above are excerpted from David Brannan's Mathematical Analysis.","I can't figure out how it is valid to invoke the Absolute Convergence Theorem, whose hypothesis is ""Let the power series have radius of convergence R"", to establish case c of the Radius of Convergence Theorem (basically concluding that ""the power series has a radius of convergence...""). Isn't it circular reasoning to prove X by assuming X itself? (The aforementioned ACT is invoked beside the second, and subsequently again after the third, margin note below.) I would be extremely grateful for any help resolving this. P.S. The above are excerpted from David Brannan's Mathematical Analysis.",,"['real-analysis', 'analysis', 'power-series', 'taylor-expansion']"
38,Testing for convergence $\sum_{j=1}^{\infty}\frac{1}{\sum_{i=1}^{j}p_i}$,Testing for convergence,\sum_{j=1}^{\infty}\frac{1}{\sum_{i=1}^{j}p_i},How would we test for convergence the series below? $$\sum_{j=1}^{\infty}\frac{1}{\sum_{i=1}^{j}p_i}$$ where $p_i$ is the $i$th prime number. I'd be glad to learn an elementary way. Thanks.,How would we test for convergence the series below? $$\sum_{j=1}^{\infty}\frac{1}{\sum_{i=1}^{j}p_i}$$ where $p_i$ is the $i$th prime number. I'd be glad to learn an elementary way. Thanks.,,"['calculus', 'real-analysis', 'sequences-and-series', 'prime-numbers']"
39,"The adherent values of $x_n=cos(n)$ are the interval $[-1,1]$",The adherent values of  are the interval,"x_n=cos(n) [-1,1]","This question seems really hard, I'm trying to prove that the set of the adherent values of the sequence $x_n=\cos (n)$ is the closed interval $[-1,1]$, i.e., every point of this interval is a limit of a subsequence of  $x_n$, and also the limit of any subsequence of $(x_n)$ is in $[-1,1]$ It's obvious that every adherent values is in $[-1,1]$, I'm having troubles to prove the converse, i.e., a point in $[-1,1]$ is an adherent value of $(x_n)$. I need help Thanks a lot","This question seems really hard, I'm trying to prove that the set of the adherent values of the sequence $x_n=\cos (n)$ is the closed interval $[-1,1]$, i.e., every point of this interval is a limit of a subsequence of  $x_n$, and also the limit of any subsequence of $(x_n)$ is in $[-1,1]$ It's obvious that every adherent values is in $[-1,1]$, I'm having troubles to prove the converse, i.e., a point in $[-1,1]$ is an adherent value of $(x_n)$. I need help Thanks a lot",,['real-analysis']
40,Evaluate $\lim_{x\to\infty}\left(1+\frac{\ln x}{f(x)}\right)^{f(x)/x}$,Evaluate,\lim_{x\to\infty}\left(1+\frac{\ln x}{f(x)}\right)^{f(x)/x},"Let's consider the function $f:\mathbb{R}\rightarrow(0,\infty)$ , with $f(x)\cdot \ln f(x)=e^x$ , $\forall x \in \mathbb{R}$ . Then compute $$\lim_{x\to\infty}\left(1+\frac{\ln x}{f(x)}\right)^{\dfrac{f(x)}{x}}$$ The first solution Since $$f(x)\cdot \ln f(x)=e^x, \forall x \in \mathbb{R}$$ we may easily deduce that $$\lim_{x\to\infty}f(x)=\infty$$ On the other hand $$f^2(x)> f(x)\cdot \ln f(x)=e^x$$ $$f(x)> e^{x/2}$$ and then $$0\le\lim_{x\to\infty} \frac{\ln x}{f(x)}\le\frac{\ln x}{e^{x/2}}\rightarrow 0$$ $$\lim_{x\to\infty} \frac{\ln x}{f(x)}=0$$ $$\lim_{x\to\infty} \displaystyle\frac{e^{x/2}}{x} \le \lim_{x\to\infty} \displaystyle\frac{f(x)}{x}=\infty$$ At this point we recognize that our limit case is $1^{\infty}$ , and have $$\lim_{x\to\infty}\left(1+\frac{\ln x}{f(x)}\right)^{\displaystyle\frac{f(x)}{x}}=\lim_{x\to\infty}e^{\displaystyle \frac{\ln x}{x }}=e^0=1$$ The second solution (from a brilliant friend of mine - - so sorry I missed this way) Let's take log of both sides of the limit $$\ln L = \lim_{x\to\infty} \frac{f(x)}{\ln x} \ln \left(1+\frac{\ln x}{f(x)}\right)\times \lim_{x\to\infty} \frac{\ln x}{x}=1\times 0=0$$ that is simply justified by the fact that $f(x)>>x$ (see $f(x)> e^{x/2}$ ) Question : how would you approach this question? Thank you.","Let's consider the function , with , . Then compute The first solution Since we may easily deduce that On the other hand and then At this point we recognize that our limit case is , and have The second solution (from a brilliant friend of mine - - so sorry I missed this way) Let's take log of both sides of the limit that is simply justified by the fact that (see ) Question : how would you approach this question? Thank you.","f:\mathbb{R}\rightarrow(0,\infty) f(x)\cdot \ln f(x)=e^x \forall x \in \mathbb{R} \lim_{x\to\infty}\left(1+\frac{\ln x}{f(x)}\right)^{\dfrac{f(x)}{x}} f(x)\cdot \ln f(x)=e^x, \forall x \in \mathbb{R} \lim_{x\to\infty}f(x)=\infty f^2(x)> f(x)\cdot \ln f(x)=e^x f(x)> e^{x/2} 0\le\lim_{x\to\infty} \frac{\ln x}{f(x)}\le\frac{\ln x}{e^{x/2}}\rightarrow 0 \lim_{x\to\infty} \frac{\ln x}{f(x)}=0 \lim_{x\to\infty} \displaystyle\frac{e^{x/2}}{x} \le \lim_{x\to\infty} \displaystyle\frac{f(x)}{x}=\infty 1^{\infty} \lim_{x\to\infty}\left(1+\frac{\ln x}{f(x)}\right)^{\displaystyle\frac{f(x)}{x}}=\lim_{x\to\infty}e^{\displaystyle \frac{\ln x}{x }}=e^0=1 \ln L = \lim_{x\to\infty} \frac{f(x)}{\ln x} \ln \left(1+\frac{\ln x}{f(x)}\right)\times \lim_{x\to\infty} \frac{\ln x}{x}=1\times 0=0 f(x)>>x f(x)> e^{x/2}","['calculus', 'real-analysis', 'limits', 'contest-math']"
41,Show that the product converge,Show that the product converge,,"I have never worked with infinite products before, so I have no idea how to do this. So a hint would be much appreciated. show that the product $\prod (1+\frac{(-1)^k}{\sqrt{k}})$ diverges let $e_k = 0$ if $k$ is odd and $1$ otherwise. let $b_k = \frac{e_k}{k} + \frac{(-1)^k}{\sqrt{k}}$. show that the product $\prod (1+b_k)$ converges.","I have never worked with infinite products before, so I have no idea how to do this. So a hint would be much appreciated. show that the product $\prod (1+\frac{(-1)^k}{\sqrt{k}})$ diverges let $e_k = 0$ if $k$ is odd and $1$ otherwise. let $b_k = \frac{e_k}{k} + \frac{(-1)^k}{\sqrt{k}}$. show that the product $\prod (1+b_k)$ converges.",,"['real-analysis', 'infinite-product']"
42,If $p$ is a non-zero real polynomial then the map $x\mapsto \frac{1}{p(x)}$ is uniformly continuous over $\mathbb{R}$,If  is a non-zero real polynomial then the map  is uniformly continuous over,p x\mapsto \frac{1}{p(x)} \mathbb{R},"Let $p(x)$ be a non-constant polynomial with real coefficients such that $p(x) \neq 0$ for all $x \in \mathbb{R}$. Define $f(x)=\frac{1}{p(x)}$ for all $x \in \mathbb{R}$. Prove that, for each $\epsilon >0$, there exists $\alpha>0$ such that $|f(x)|<\epsilon$ whenever $|x|>\alpha$, and $f:\mathbb{R} \to \mathbb{R}$ is a uniformly continuous function. On the way of solving the above problem I was able to prove the first assertion. But I can't get through the second claim. I know that if I'm able to show that $f$ satisfies the Lipschitz condition, then it will be done. So, I assumed that $p(x)=\sum_{i=0}^{n}a_ix^i$ and tried to show  $$|f(x)-f(y)|=\left|\frac{1}{p(x)}-\frac{1}{p(y)}\right|\\= |x-y|\Big|\frac{a_1(x+y)+a_2(x^2+xy+y^2)+\cdots +a_n(x^{n-1}+x^{n-2}y+\cdots+y^{n-1})}{p(x)p(y)}\Big|\leq |x-y|.$$ But I don't know how to show $\Big|\frac{a_1(x+y)+a_2(x^2+xy+y^2)+\cdots +a_n(x^{n-1}+x^{n-2}y+\cdots+y^{n-1})}{p(x)p(y)}\Big|\leq 1$. Obviously, this function may not be at all Lipschitz and maybe I'm just chasing wild goose; but any help regarding this will be appreciated. Regards. [Source: This problem can be found here (Question 20).]","Let $p(x)$ be a non-constant polynomial with real coefficients such that $p(x) \neq 0$ for all $x \in \mathbb{R}$. Define $f(x)=\frac{1}{p(x)}$ for all $x \in \mathbb{R}$. Prove that, for each $\epsilon >0$, there exists $\alpha>0$ such that $|f(x)|<\epsilon$ whenever $|x|>\alpha$, and $f:\mathbb{R} \to \mathbb{R}$ is a uniformly continuous function. On the way of solving the above problem I was able to prove the first assertion. But I can't get through the second claim. I know that if I'm able to show that $f$ satisfies the Lipschitz condition, then it will be done. So, I assumed that $p(x)=\sum_{i=0}^{n}a_ix^i$ and tried to show  $$|f(x)-f(y)|=\left|\frac{1}{p(x)}-\frac{1}{p(y)}\right|\\= |x-y|\Big|\frac{a_1(x+y)+a_2(x^2+xy+y^2)+\cdots +a_n(x^{n-1}+x^{n-2}y+\cdots+y^{n-1})}{p(x)p(y)}\Big|\leq |x-y|.$$ But I don't know how to show $\Big|\frac{a_1(x+y)+a_2(x^2+xy+y^2)+\cdots +a_n(x^{n-1}+x^{n-2}y+\cdots+y^{n-1})}{p(x)p(y)}\Big|\leq 1$. Obviously, this function may not be at all Lipschitz and maybe I'm just chasing wild goose; but any help regarding this will be appreciated. Regards. [Source: This problem can be found here (Question 20).]",,"['real-analysis', 'polynomials', 'continuity', 'uniform-continuity']"
43,Suggestions for Studying for Real Analysis/Linear Algebra,Suggestions for Studying for Real Analysis/Linear Algebra,,"I apologize if this question is inappropriate for this site, but I'm new here and am not entirely sure where to direct it. I've just begun a course on real analysis and linear algebra at my university and it's the first time I've ever taken a course almost entirely devoted to proof. I'm finding the material very difficult to wrap my head around, specifically, the problem I'm having involves conceptualizing or visualizing the material to allow myself to begin a given problem. We are using the book call Tools of the Trade by Paul Sally and this book presents almost no practical guidance on approaching proofs or the material itself and neither does my instructor (either in class or in private). Can anyone offer any advice or suggestions as to how I might better learn to internalize this information? If not, does anyone know of any good books that I might read to gain such insight? The material we have covered so far includes the construction of the integers, rationals, and reals (using Cauchy Sequences) along with a basic discussion of functions and set theory.","I apologize if this question is inappropriate for this site, but I'm new here and am not entirely sure where to direct it. I've just begun a course on real analysis and linear algebra at my university and it's the first time I've ever taken a course almost entirely devoted to proof. I'm finding the material very difficult to wrap my head around, specifically, the problem I'm having involves conceptualizing or visualizing the material to allow myself to begin a given problem. We are using the book call Tools of the Trade by Paul Sally and this book presents almost no practical guidance on approaching proofs or the material itself and neither does my instructor (either in class or in private). Can anyone offer any advice or suggestions as to how I might better learn to internalize this information? If not, does anyone know of any good books that I might read to gain such insight? The material we have covered so far includes the construction of the integers, rationals, and reals (using Cauchy Sequences) along with a basic discussion of functions and set theory.",,"['real-analysis', 'linear-algebra']"
44,"Proving that if $f>0$ and $\int_E f =0$, then $E$ has measure $0$","Proving that if  and , then  has measure",f>0 \int_E f =0 E 0,"I am trying to prove the following statement. Consider in $\mathbb R^n$ the $n$-dimensional Lebesgue measure. Let $f:\mathbb R^n \rightarrow [0 , \infty]$  be a measurable function. Let $E$ be a measurable subset of $\mathbb R^n$. Suppose that $\int_{E} f(x)  dx = 0$ and that $0 < f < \infty$ a.e.  Then the Lebesgue measure of $E$ is zero. Can someone please give me a suggestion? My english is horrible, sorry ( i am from Brazil )","I am trying to prove the following statement. Consider in $\mathbb R^n$ the $n$-dimensional Lebesgue measure. Let $f:\mathbb R^n \rightarrow [0 , \infty]$  be a measurable function. Let $E$ be a measurable subset of $\mathbb R^n$. Suppose that $\int_{E} f(x)  dx = 0$ and that $0 < f < \infty$ a.e.  Then the Lebesgue measure of $E$ is zero. Can someone please give me a suggestion? My english is horrible, sorry ( i am from Brazil )",,"['real-analysis', 'measure-theory']"
45,Show that a simple function is measurable if its parts are all measurable,Show that a simple function is measurable if its parts are all measurable,,"I understand that a simple function $s:\mathbb{R}^2 \to \mathbb{R}$ is any function which assumes only a finite number of distinct values.  It can also be written as a linear combination of indicator functions $s= \sum_{k=1}^N \alpha_k f_{A_k}$ with disjoint sets $A_k$ and distinct $\alpha_k \in \mathbb{R}$.  I'm trying to prove that a simple function, defined above, is measurable if and only if all of the sets $A_k$ are measurable.  Since I am still somewhat new to these types of proofs, I am having difficulty being rigorous, though I have a general idea of how to explain this.","I understand that a simple function $s:\mathbb{R}^2 \to \mathbb{R}$ is any function which assumes only a finite number of distinct values.  It can also be written as a linear combination of indicator functions $s= \sum_{k=1}^N \alpha_k f_{A_k}$ with disjoint sets $A_k$ and distinct $\alpha_k \in \mathbb{R}$.  I'm trying to prove that a simple function, defined above, is measurable if and only if all of the sets $A_k$ are measurable.  Since I am still somewhat new to these types of proofs, I am having difficulty being rigorous, though I have a general idea of how to explain this.",,"['real-analysis', 'measure-theory']"
46,"{$\int_{[1/n,1]}f$} to converge and yet $f$ is not $L$-integrable over $[0,1]$",{} to converge and yet  is not -integrable over,"\int_{[1/n,1]}f f L [0,1]","Let $f$ be a function on $[0,1]$ and continuous on $(0,1]$. I want to find a function $f$ s.t. {$\int_{[1/n,1]}f$} converges and yet $f$ is not $L$-integrable over $[0,1]$. My attempts: I've found $f(x)=(1/x)Sin(1/x)$ but I can not prove that $f$is not $L$-integrable over $[0,1]$.","Let $f$ be a function on $[0,1]$ and continuous on $(0,1]$. I want to find a function $f$ s.t. {$\int_{[1/n,1]}f$} converges and yet $f$ is not $L$-integrable over $[0,1]$. My attempts: I've found $f(x)=(1/x)Sin(1/x)$ but I can not prove that $f$is not $L$-integrable over $[0,1]$.",,"['real-analysis', 'convergence-divergence', 'continuity', 'lebesgue-integral']"
47,Rudin Series ratio and root test.,Rudin Series ratio and root test.,,"In Rudins Principles of Mathematical Analysis he says consider the following series $$\frac 12 + \frac 13 + \frac 1{2^2} + \frac 1{3^2} + \frac 1{2^3} + \frac 1{3^3} + \frac 1{2^4} + \frac 1{3^4} + \cdots$$ for which $$\liminf \limits_{n \to \infty} \dfrac{a_{n+1}}{a_n} =  \lim \limits_{n \to \infty} \left( \dfrac {2}{3} \right)^n =0, $$ $$\liminf \limits_{n \to \infty} \sqrt[n]{a_n} =  \lim \limits_{n \to \infty} \sqrt[2n]{\dfrac{1}{3^n}} = \dfrac{1}{\sqrt{3}}, $$ $$\limsup \limits_{n \to \infty} \sqrt[n]{a_n} =  \lim \limits_{n \to \infty} \sqrt[2n]{\dfrac{1}{2^n}} = \dfrac{1}{\sqrt{2}}, $$ $$\limsup \limits_{n \to \infty} \dfrac{a_{n+1}}{a_n} =  \lim \limits_{n \to \infty} \dfrac 12\left( \dfrac {3}{2} \right)^n =+\infty, $$ The root test indicates convergence; the ratio test does not apply. In the book he defines the root and ratios test for the lim sup.  I am not exactly sure how he goes from the lim sup to the lim and also why there is a $2n$ (which I assume comes from even terms of the sequence) in the root test.  Also why is he checking the lim inf?  I believe that my understanding of lim sups and infs are not well developed or I would probably understand what’s going on. Also how does he get the terms that he is taking the limit of.  A nudge in the right direction to figure this out would be much appreciated.     Thank you!!","In Rudins Principles of Mathematical Analysis he says consider the following series $$\frac 12 + \frac 13 + \frac 1{2^2} + \frac 1{3^2} + \frac 1{2^3} + \frac 1{3^3} + \frac 1{2^4} + \frac 1{3^4} + \cdots$$ for which $$\liminf \limits_{n \to \infty} \dfrac{a_{n+1}}{a_n} =  \lim \limits_{n \to \infty} \left( \dfrac {2}{3} \right)^n =0, $$ $$\liminf \limits_{n \to \infty} \sqrt[n]{a_n} =  \lim \limits_{n \to \infty} \sqrt[2n]{\dfrac{1}{3^n}} = \dfrac{1}{\sqrt{3}}, $$ $$\limsup \limits_{n \to \infty} \sqrt[n]{a_n} =  \lim \limits_{n \to \infty} \sqrt[2n]{\dfrac{1}{2^n}} = \dfrac{1}{\sqrt{2}}, $$ $$\limsup \limits_{n \to \infty} \dfrac{a_{n+1}}{a_n} =  \lim \limits_{n \to \infty} \dfrac 12\left( \dfrac {3}{2} \right)^n =+\infty, $$ The root test indicates convergence; the ratio test does not apply. In the book he defines the root and ratios test for the lim sup.  I am not exactly sure how he goes from the lim sup to the lim and also why there is a $2n$ (which I assume comes from even terms of the sequence) in the root test.  Also why is he checking the lim inf?  I believe that my understanding of lim sups and infs are not well developed or I would probably understand what’s going on. Also how does he get the terms that he is taking the limit of.  A nudge in the right direction to figure this out would be much appreciated.     Thank you!!",,['real-analysis']
48,Prove that a convex function on $\mathbb{R}^n$ is continuous [duplicate],Prove that a convex function on  is continuous [duplicate],\mathbb{R}^n,This question already has answers here : Continuity of a convex function (2 answers) Closed 8 years ago . Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a convex function on $\mathbb{R}^n$. How to prove that $f$ is continuous?,This question already has answers here : Continuity of a convex function (2 answers) Closed 8 years ago . Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be a convex function on $\mathbb{R}^n$. How to prove that $f$ is continuous?,,"['real-analysis', 'convex-analysis']"
49,Approximate $\int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2}\log(1+e^{-x}) \ \ dx $,Approximate,\int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2}\log(1+e^{-x}) \ \ dx ,"I am trying to find an approximation to $$ I = \int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2}\log(1+e^{-x}) \ \ dx. $$ My attempt is as follows: $$ \begin{align} I &= \int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2} \left( \sum_{i=1}^\infty \frac{e^{-ix}}{i} (-1)^{(i+1)} \right)\ dx\\ &= \sum_{i=1}^\infty \frac{(-1)^{(i+1)}}{i}\int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2}  e^{-ix} \ \ dx\\ &= \sum_{i=1}^\infty \frac{(-1)^{(i+1)}}{i} k_i \int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-(\mu-i \sigma^2))^2/2 \sigma^2} \ \ dx,\\ \end{align} $$ where $$ k_i = e^{(\mu -i \sigma^2)^2-\mu^2}. $$ The $k_i$ increases exponentially with increasing $i$ and thus makes the sum divergent. I don't understand why this is happening although this sum should be finite (because I don't see any problem with the original integral). P.S. I used MacLauren series in approximating natural logarithm.","I am trying to find an approximation to $$ I = \int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2}\log(1+e^{-x}) \ \ dx. $$ My attempt is as follows: $$ \begin{align} I &= \int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2} \left( \sum_{i=1}^\infty \frac{e^{-ix}}{i} (-1)^{(i+1)} \right)\ dx\\ &= \sum_{i=1}^\infty \frac{(-1)^{(i+1)}}{i}\int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2/2 \sigma^2}  e^{-ix} \ \ dx\\ &= \sum_{i=1}^\infty \frac{(-1)^{(i+1)}}{i} k_i \int_a^b \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-(\mu-i \sigma^2))^2/2 \sigma^2} \ \ dx,\\ \end{align} $$ where $$ k_i = e^{(\mu -i \sigma^2)^2-\mu^2}. $$ The $k_i$ increases exponentially with increasing $i$ and thus makes the sum divergent. I don't understand why this is happening although this sum should be finite (because I don't see any problem with the original integral). P.S. I used MacLauren series in approximating natural logarithm.",,"['calculus', 'real-analysis', 'integration', 'special-functions', 'approximation']"
50,Computing: $L =\lim_{n\rightarrow\infty}\left(\frac{\frac{n}{1}+\frac{n-1}{2}+\cdots+\frac{1}{n}}{\ln(n!)} \right)^{{\frac{\ln(n!)}{n}}} $,Computing:,L =\lim_{n\rightarrow\infty}\left(\frac{\frac{n}{1}+\frac{n-1}{2}+\cdots+\frac{1}{n}}{\ln(n!)} \right)^{{\frac{\ln(n!)}{n}}} ,"Compute the following limit: $$L =\lim_{n\rightarrow\infty}\left(\frac{\frac{n}{1}+\frac{n-1}{2}+\cdots+\frac{1}{n}}{\ln(n!)}  \right)^{{\frac{\ln(n!)}{n}}} $$ I'm looking for an easy, simple solution here, but not sure yet this is possible.  Any hint, suggestion along this way is welcome. Thanks.","Compute the following limit: $$L =\lim_{n\rightarrow\infty}\left(\frac{\frac{n}{1}+\frac{n-1}{2}+\cdots+\frac{1}{n}}{\ln(n!)}  \right)^{{\frac{\ln(n!)}{n}}} $$ I'm looking for an easy, simple solution here, but not sure yet this is possible.  Any hint, suggestion along this way is welcome. Thanks.",,"['real-analysis', 'limits']"
51,Does uniform convergence preserves absolute continuity?,Does uniform convergence preserves absolute continuity?,,"Here's a question I came up randomly just now: If a sequence of absolutely continuous functions $\{\phi_{n}\}$ converges uniformly to a function $f$, does it imply that $f$ is also absolutely continuous? I am having trouble judging if this is true. If the statement is not true, can anybody help me find a counter-example? Thank you!","Here's a question I came up randomly just now: If a sequence of absolutely continuous functions $\{\phi_{n}\}$ converges uniformly to a function $f$, does it imply that $f$ is also absolutely continuous? I am having trouble judging if this is true. If the statement is not true, can anybody help me find a counter-example? Thank you!",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
52,"sequence of absolutely continuous functions, uniform convergence","sequence of absolutely continuous functions, uniform convergence",,"Let $\{f_n\}$ be absolutely continuous functions on $[0,1]$, $f_n \geq 0$ $\forall n \in \mathbb{N}$.  Suppose that $\lim_{n \to \infty} \int^1_0 f_n dx = 0$.  Moreover, suppose that $\forall$ $\epsilon > 0$ $\exists$ $\delta > 0$ such that $\int_{E} |f_n'(x)|dx$ < $\epsilon$ $\forall n \in \mathbb{N}$,  if $m(E) < \delta$.  Prove that $f_n$ converges to $0$ uniformly on $[0,1]$. Can someone help show this? Thanking you in advance.","Let $\{f_n\}$ be absolutely continuous functions on $[0,1]$, $f_n \geq 0$ $\forall n \in \mathbb{N}$.  Suppose that $\lim_{n \to \infty} \int^1_0 f_n dx = 0$.  Moreover, suppose that $\forall$ $\epsilon > 0$ $\exists$ $\delta > 0$ such that $\int_{E} |f_n'(x)|dx$ < $\epsilon$ $\forall n \in \mathbb{N}$,  if $m(E) < \delta$.  Prove that $f_n$ converges to $0$ uniformly on $[0,1]$. Can someone help show this? Thanking you in advance.",,"['real-analysis', 'measure-theory']"
53,Primitive roots of unity,Primitive roots of unity,,"I am trying to show that, If $$f\left( x\right) =a_{0}+a_{1}x+\ldots +a_{k}x^{k}$$ then $$\dfrac {1} {n}\left\{ f\left( x\right) +f\left( wx\right) +\ldots +f\left( w^{n-1}x\right) \right\} =a_{0}+a_{n}x^{n}+a_{2n}x^{2n}+\ldots +a_{\lambda n}x^{\lambda n}$$ $w$ being any root of $x^n=1$(except x= 1), and $\lambda n$ the greatest multiple of n contained in $k$. Show there is a similar formula for $a_{\mu }+a_{\mu +n}x^{n}+a_{\mu+2n}x^{2n}+\dots,$ where $0 <  \mu < n$. Now i know the question presents a result in the first statement and I am supposed to show the result in the second statement but as a challenge or for the fun of it i was hoping to prove both, Although i did n't get much far. Starting with the LHS of the first result  $$\dfrac {1} {n}\left\{ f\left( x\right) +f\left( wx\right) +\ldots +f\left( w^{n-1}x\right) \right\} $$ I thought of substituting values for those functions and then combining the terms $$\dfrac {1} {n}\left\{(a_{0}+a_{1}x+\ldots +a_{k}x^{k}) + (a_{0}+a_{1}wx+\ldots +a_{k}w^{k}x^{k} )+ \dots+ (a_{0}+a_{1}w^{n-1}x+\ldots +a_{k}w^{k(n-1)}x^{k}) \right\} $$ $$=\dfrac {1} {n}\{na_{0}+a_{1}x\left( 1+w +\ldots +w^{n-1}\right) +\dots+a_{k}x^{k}\left( 1+w^{k}+\ldots +w^{k(n-1)}\right) $$ Now I know that $\left( 1+w +\ldots +w^{n-1}\right) = 0$ and from my scratch work proof i highly suspect that $\left( 1+w^{p}+\ldots +w^{p(n-1)}\right) =0 $ provided p is not divisible by n (p mod n > 0) as well. Although when p is a multiple of n  we have positive but undefined sum. $$1^{p}+w^{p}+w^{2p}+\ldots +w^{p\left( n-1\right) } = \dfrac {1} {2}+\dfrac {\sin\left( 2p\pi -\dfrac {p\pi } {n}\right) +i\cos \dfrac {p\pi } {n} -i\cos \left( 2p\pi -\dfrac {p\pi } {n}\right) } {2\sin \dfrac {p\pi } {n}}$$ Hence we are left with expected terms but what about the multiple n ? Where am i going wrong here. Also any help with the second part of the question would be much appreciated.","I am trying to show that, If $$f\left( x\right) =a_{0}+a_{1}x+\ldots +a_{k}x^{k}$$ then $$\dfrac {1} {n}\left\{ f\left( x\right) +f\left( wx\right) +\ldots +f\left( w^{n-1}x\right) \right\} =a_{0}+a_{n}x^{n}+a_{2n}x^{2n}+\ldots +a_{\lambda n}x^{\lambda n}$$ $w$ being any root of $x^n=1$(except x= 1), and $\lambda n$ the greatest multiple of n contained in $k$. Show there is a similar formula for $a_{\mu }+a_{\mu +n}x^{n}+a_{\mu+2n}x^{2n}+\dots,$ where $0 <  \mu < n$. Now i know the question presents a result in the first statement and I am supposed to show the result in the second statement but as a challenge or for the fun of it i was hoping to prove both, Although i did n't get much far. Starting with the LHS of the first result  $$\dfrac {1} {n}\left\{ f\left( x\right) +f\left( wx\right) +\ldots +f\left( w^{n-1}x\right) \right\} $$ I thought of substituting values for those functions and then combining the terms $$\dfrac {1} {n}\left\{(a_{0}+a_{1}x+\ldots +a_{k}x^{k}) + (a_{0}+a_{1}wx+\ldots +a_{k}w^{k}x^{k} )+ \dots+ (a_{0}+a_{1}w^{n-1}x+\ldots +a_{k}w^{k(n-1)}x^{k}) \right\} $$ $$=\dfrac {1} {n}\{na_{0}+a_{1}x\left( 1+w +\ldots +w^{n-1}\right) +\dots+a_{k}x^{k}\left( 1+w^{k}+\ldots +w^{k(n-1)}\right) $$ Now I know that $\left( 1+w +\ldots +w^{n-1}\right) = 0$ and from my scratch work proof i highly suspect that $\left( 1+w^{p}+\ldots +w^{p(n-1)}\right) =0 $ provided p is not divisible by n (p mod n > 0) as well. Although when p is a multiple of n  we have positive but undefined sum. $$1^{p}+w^{p}+w^{2p}+\ldots +w^{p\left( n-1\right) } = \dfrac {1} {2}+\dfrac {\sin\left( 2p\pi -\dfrac {p\pi } {n}\right) +i\cos \dfrac {p\pi } {n} -i\cos \left( 2p\pi -\dfrac {p\pi } {n}\right) } {2\sin \dfrac {p\pi } {n}}$$ Hence we are left with expected terms but what about the multiple n ? Where am i going wrong here. Also any help with the second part of the question would be much appreciated.",,"['real-analysis', 'elementary-number-theory', 'complex-numbers']"
54,Riemann zeta function and uniform convergence,Riemann zeta function and uniform convergence,,"A question in a past paper says prove that this series converges pointwise but not uniformly $$\xi(x):= \sum_{n=1}^\infty \frac{1}{n^x} .$$ But I thought that it did converge uniformly to some function $\xi(x):(1,\infty) \to \mathbb{R}$. Here's why; if you work on $(1+\delta,\infty)$ for $\delta >0$, then clearly we have $$\left\|\frac{1}{n^x}\right\|_\infty = \frac{1}{n^{1+\delta}}$$ and clearly $\sum \frac{1}{n^{1+\delta}}$ converges so by the Weierstrass M-test $\sum_{n=1}^\infty \frac{1}{n^x}$ converges uniformly on $(1+\delta,\infty)$ so letting $\delta \to 0$ gives $\sum_{n=1}^\infty \frac{1}{n^x}$ converging uniformly on $(1,\infty)$?","A question in a past paper says prove that this series converges pointwise but not uniformly $$\xi(x):= \sum_{n=1}^\infty \frac{1}{n^x} .$$ But I thought that it did converge uniformly to some function $\xi(x):(1,\infty) \to \mathbb{R}$. Here's why; if you work on $(1+\delta,\infty)$ for $\delta >0$, then clearly we have $$\left\|\frac{1}{n^x}\right\|_\infty = \frac{1}{n^{1+\delta}}$$ and clearly $\sum \frac{1}{n^{1+\delta}}$ converges so by the Weierstrass M-test $\sum_{n=1}^\infty \frac{1}{n^x}$ converges uniformly on $(1+\delta,\infty)$ so letting $\delta \to 0$ gives $\sum_{n=1}^\infty \frac{1}{n^x}$ converging uniformly on $(1,\infty)$?",,['real-analysis']
55,"Show $\int_0^\infty x^{-\alpha}\sin x dx$ exists for $\alpha \in (0,2)$.",Show  exists for .,"\int_0^\infty x^{-\alpha}\sin x dx \alpha \in (0,2)","Show that $\int_0^\infty x^{-\alpha}\sin x dx$ exists for $\alpha \in (0,2)$. This is a real analysis class question.  I am not quite sure how to show this.  I tried a whole bunch of things like integration by parts, bounding sine, ... and nothing worked. Any hints, ideas, or solutions would be appreciated.","Show that $\int_0^\infty x^{-\alpha}\sin x dx$ exists for $\alpha \in (0,2)$. This is a real analysis class question.  I am not quite sure how to show this.  I tried a whole bunch of things like integration by parts, bounding sine, ... and nothing worked. Any hints, ideas, or solutions would be appreciated.",,['real-analysis']
56,Equicontinuity and modulus of continuity,Equicontinuity and modulus of continuity,,"A modulus of continuity for a function $f$ is a continuous increasing function $\alpha$ such that $\alpha(0) = 0$ and $|f(x) - f(y)| < \alpha(|x-y|)$ for all $x$ and $y$. I am trying to prove that an equicontinuous family $\mathcal F$ of functions has a common modulus of continuity. This seems intuitively obvious, but I am having difficulty proving continuity. So far, I have defined $\alpha(\delta) = \sup\{|f(x) - f(y)| :  d(x,y) \leq \delta, f\in \mathcal F\} $. I want to show that this function is continuous in $\delta$. Any suggestions?","A modulus of continuity for a function $f$ is a continuous increasing function $\alpha$ such that $\alpha(0) = 0$ and $|f(x) - f(y)| < \alpha(|x-y|)$ for all $x$ and $y$. I am trying to prove that an equicontinuous family $\mathcal F$ of functions has a common modulus of continuity. This seems intuitively obvious, but I am having difficulty proving continuity. So far, I have defined $\alpha(\delta) = \sup\{|f(x) - f(y)| :  d(x,y) \leq \delta, f\in \mathcal F\} $. I want to show that this function is continuous in $\delta$. Any suggestions?",,['real-analysis']
57,Continuous non-decreasing image of a measure zero set,Continuous non-decreasing image of a measure zero set,,"I am looking at an old qualifying exam to study for my finals; it asks the following true/false question: Let $f$ be a continuous, non-decreasing function defined on $[0,1]$, and let $E$ be a set of Lebesgue measure zero. Then, $f(E)$ is a set of Lebesgue measure zero. I suspect this is false, but am not sure. Can anyone think of a solution without using the notion of absolute continuity? The reason for this is because AC won't be covered on the final.","I am looking at an old qualifying exam to study for my finals; it asks the following true/false question: Let $f$ be a continuous, non-decreasing function defined on $[0,1]$, and let $E$ be a set of Lebesgue measure zero. Then, $f(E)$ is a set of Lebesgue measure zero. I suspect this is false, but am not sure. Can anyone think of a solution without using the notion of absolute continuity? The reason for this is because AC won't be covered on the final.",,"['real-analysis', 'measure-theory']"
58,Is $f(x+iy)=x^2-y^2 + i\sqrt{|xy|}$ complex differentiable?,Is  complex differentiable?,f(x+iy)=x^2-y^2 + i\sqrt{|xy|},"I was solving a problem yesterday and it has bugged me for the whole night, im not sure whether if I got it correct or not. First I was asked if $f(x+iy)=x^2-y^2 + i\sqrt{|xy|}$ satisfies the C-R equations at $0$. So I found $\frac{\partial u}{\partial x}$ $=$ $\frac{\partial v}{\partial y}$ & $-\frac{\partial u}{\partial y}$ $=$ $\frac{\partial v}{\partial x}$ $u=x^2-y^2$ and $v=\sqrt{|xy|}$ So I found $\frac{\partial u}{\partial x} = 2x$, $-\frac{\partial u}{\partial y}=2y$, $\frac{\partial v}{\partial x}=\frac{1}{2}\frac{\sqrt{y}}{\sqrt{x}}$ and $\frac{\partial v}{\partial y}=\frac{1}{2}\frac{\sqrt{x}}{\sqrt{y}}$. So obviously it does not satisfy the C-R equations. I was wondering if I have to do anything else because it asks if it satisfies the C-R equations at $0$? The next part of the question asks me if $f$ is differentiable at $0$. And it hints that I should consider $\mathrm{lim}_{r\rightarrow 0} \frac{f(re^{i\theta})}{re^{i\theta}}$. I assume they are meaning $f$ is complex differentiable? (not real differentiable?). How would I determine if f is differentiable at $0$? Because it says on wikipedia that the sole existence of partial derivatives satisfying the Cauchy-Riemann equations is not enough to ensure complex differentiability at that point. It is necessary to make sure that u and v are real differentiable, which is a stronger condition than the existence of the partial derivatives but it is not necessary to require continuity of these partial derivatives. Then f = u + iv is complex-differentiable at that point if and only if the partial derivatives of u and v satisfy the Cauchy–Riemann equations at that point. The question I have is, since $f$ does not satisfy the C-R equations (see my calculations above), is there any need to do anything further (in question 2)? Can I just say its not differentiable at $0$? Why did they ask me to consider that limit? Thanks alot, Im really stuck and slightly confused with all this..","I was solving a problem yesterday and it has bugged me for the whole night, im not sure whether if I got it correct or not. First I was asked if $f(x+iy)=x^2-y^2 + i\sqrt{|xy|}$ satisfies the C-R equations at $0$. So I found $\frac{\partial u}{\partial x}$ $=$ $\frac{\partial v}{\partial y}$ & $-\frac{\partial u}{\partial y}$ $=$ $\frac{\partial v}{\partial x}$ $u=x^2-y^2$ and $v=\sqrt{|xy|}$ So I found $\frac{\partial u}{\partial x} = 2x$, $-\frac{\partial u}{\partial y}=2y$, $\frac{\partial v}{\partial x}=\frac{1}{2}\frac{\sqrt{y}}{\sqrt{x}}$ and $\frac{\partial v}{\partial y}=\frac{1}{2}\frac{\sqrt{x}}{\sqrt{y}}$. So obviously it does not satisfy the C-R equations. I was wondering if I have to do anything else because it asks if it satisfies the C-R equations at $0$? The next part of the question asks me if $f$ is differentiable at $0$. And it hints that I should consider $\mathrm{lim}_{r\rightarrow 0} \frac{f(re^{i\theta})}{re^{i\theta}}$. I assume they are meaning $f$ is complex differentiable? (not real differentiable?). How would I determine if f is differentiable at $0$? Because it says on wikipedia that the sole existence of partial derivatives satisfying the Cauchy-Riemann equations is not enough to ensure complex differentiability at that point. It is necessary to make sure that u and v are real differentiable, which is a stronger condition than the existence of the partial derivatives but it is not necessary to require continuity of these partial derivatives. Then f = u + iv is complex-differentiable at that point if and only if the partial derivatives of u and v satisfy the Cauchy–Riemann equations at that point. The question I have is, since $f$ does not satisfy the C-R equations (see my calculations above), is there any need to do anything further (in question 2)? Can I just say its not differentiable at $0$? Why did they ask me to consider that limit? Thanks alot, Im really stuck and slightly confused with all this..",,"['calculus', 'real-analysis', 'analysis', 'complex-analysis']"
59,Convergence of a bounded sequence with bounded L2 variation,Convergence of a bounded sequence with bounded L2 variation,,"I have the following question, that a friend of mine asked me yesterday: Let $H$ be a Hilbert space, with norm $\|\cdot\|$. Let $(x_k)_{k \ge 0}$ be some sequence in $H$. Assume that $x_k$ is bounded (so that $\|x_k\| \le C$ for all $k$), and that $(x_k)$ has bounded L2 variance , i.e., $$\sum_{k \ge 0} \|x_{k+1}-x_k\|^2 < +\infty.$$ Does it then follow that the sequence $(x_k)$ converges? If not for all Hilbert spaces, then Does it at least follow that $(x_k)$ converges for $H=\mathbb{R}^n$?","I have the following question, that a friend of mine asked me yesterday: Let $H$ be a Hilbert space, with norm $\|\cdot\|$. Let $(x_k)_{k \ge 0}$ be some sequence in $H$. Assume that $x_k$ is bounded (so that $\|x_k\| \le C$ for all $k$), and that $(x_k)$ has bounded L2 variance , i.e., $$\sum_{k \ge 0} \|x_{k+1}-x_k\|^2 < +\infty.$$ Does it then follow that the sequence $(x_k)$ converges? If not for all Hilbert spaces, then Does it at least follow that $(x_k)$ converges for $H=\mathbb{R}^n$?",,"['real-analysis', 'functional-analysis']"
60,Smoothness of harmonic functions,Smoothness of harmonic functions,,In the book on PDEs I'm reading there is a section on harmonic functions. To prove that these functions are in the class $C^\infty$ the author use standard mollifiers which I am not comfortable with. If there another proof of the $C^\infty(U)$ for the functions $u$ such that $\Delta u = 0$ on $U$?,In the book on PDEs I'm reading there is a section on harmonic functions. To prove that these functions are in the class $C^\infty$ the author use standard mollifiers which I am not comfortable with. If there another proof of the $C^\infty(U)$ for the functions $u$ such that $\Delta u = 0$ on $U$?,,"['real-analysis', 'partial-differential-equations']"
61,How can I show that these two integrals are equal?,How can I show that these two integrals are equal?,,"How can I show that there is an equality $$ \int_{-\infty}^\infty\int_{-\infty}^\infty e^{-x^2-y^2}dxdy=\left(\int_{-\infty}^\infty e^{-x^2}dx\right)\left(\int_{-\infty}^\infty e^{-x^2}dx\right)? $$","How can I show that there is an equality $$ \int_{-\infty}^\infty\int_{-\infty}^\infty e^{-x^2-y^2}dxdy=\left(\int_{-\infty}^\infty e^{-x^2}dx\right)\left(\int_{-\infty}^\infty e^{-x^2}dx\right)? $$",,"['real-analysis', 'analysis', 'integration']"
62,"Conditional convergence, Mertens theorem","Conditional convergence, Mertens theorem",,"If $\sum a_n$ and $\sum b_n$ both converge and one of them absolutely then the Cauchy product $\sum c_n$ converges to $\sum a_n \sum b_n$. ($c_n = \sum_{k = 0}^n a_k b_{n - k}$), by Mertens Theorem. Now, if both converge conditionally then the product does not have to converge as $a_n = b_n = (-1)^n/n$ shows. My question now is: What if $\sum a_n$ and $\sum b_n$ both converge conditionally and $\sum c_n$ converges, then is it always true that $\sum c_n$ converges to the product? By the way, this is not homework, I'm already past the real analysis part.","If $\sum a_n$ and $\sum b_n$ both converge and one of them absolutely then the Cauchy product $\sum c_n$ converges to $\sum a_n \sum b_n$. ($c_n = \sum_{k = 0}^n a_k b_{n - k}$), by Mertens Theorem. Now, if both converge conditionally then the product does not have to converge as $a_n = b_n = (-1)^n/n$ shows. My question now is: What if $\sum a_n$ and $\sum b_n$ both converge conditionally and $\sum c_n$ converges, then is it always true that $\sum c_n$ converges to the product? By the way, this is not homework, I'm already past the real analysis part.",,['real-analysis']
63,Function $y=e^{1/\ln(x)}$ has a singularity at $x=1$ which breaks it into two continuous parts. What is the minimum distance between these parts?,Function  has a singularity at  which breaks it into two continuous parts. What is the minimum distance between these parts?,y=e^{1/\ln(x)} x=1,"The function $y = e^{1/\ln(x)}$ has a singularity at $x = 1$ that partitions it into two separate continuous sections. Notice that it has reflective symmetry about a $y=x$ axis, because it can be re-written as $\ln(x)\ln(y) = 1$ . When plotted, the 'left curve', (i.e. where $x < 1$ ), has a discreet length, is continuous between the points $(0,1)$ and $(1,0).$ The 'right curve', (i.e. where $x > 1$ ), is of infinite length. It has asymptotes at $x = 1$ , and $y=1$ . I am trying to find the minimum distance between the left and right curves, so slopes must be calculated. The derivative of the function is $$-\frac{y}{x\ln^2(x)}$$ The left curve has a centre point at $(\frac{1}{e}, \frac{1}{e})$ , and the tangent at this point is $-1$ . The normal there has slope $1$ , and intersects the right curve at $(e, e)$ . The tangent at that intersection is also $-1$ , so the normal there has a slope of $1$ . The straight line between $\left(\frac{1}{e}, \frac{1}{e}\right)$ and $(e, e)$ therefore seems to represent the minimum distance between the left and right curves. It equates to $\sqrt{2}\left(e - \frac{1}{e}\right)$ . But could there be another pair of points, (one in each section) where the distance between them is shorter than this? Clearly they would have to have identical slopes. I think the answer is no but I have not been able to prove it. It would be nice if a method could be found that could be applied to piecewise-continuous functions in general. Maybe the Lagrangian method could be used, but I've only seen that applied to two distinct functions, and this problem is about a single function that has two separate subdomains.","The function has a singularity at that partitions it into two separate continuous sections. Notice that it has reflective symmetry about a axis, because it can be re-written as . When plotted, the 'left curve', (i.e. where ), has a discreet length, is continuous between the points and The 'right curve', (i.e. where ), is of infinite length. It has asymptotes at , and . I am trying to find the minimum distance between the left and right curves, so slopes must be calculated. The derivative of the function is The left curve has a centre point at , and the tangent at this point is . The normal there has slope , and intersects the right curve at . The tangent at that intersection is also , so the normal there has a slope of . The straight line between and therefore seems to represent the minimum distance between the left and right curves. It equates to . But could there be another pair of points, (one in each section) where the distance between them is shorter than this? Clearly they would have to have identical slopes. I think the answer is no but I have not been able to prove it. It would be nice if a method could be found that could be applied to piecewise-continuous functions in general. Maybe the Lagrangian method could be used, but I've only seen that applied to two distinct functions, and this problem is about a single function that has two separate subdomains.","y = e^{1/\ln(x)} x = 1 y=x \ln(x)\ln(y) = 1 x < 1 (0,1) (1,0). x > 1 x = 1 y=1 -\frac{y}{x\ln^2(x)} (\frac{1}{e}, \frac{1}{e}) -1 1 (e, e) -1 1 \left(\frac{1}{e}, \frac{1}{e}\right) (e, e) \sqrt{2}\left(e - \frac{1}{e}\right)","['real-analysis', 'functions']"
64,"Find all positive sequneces $(s_n)_n$ such that: $\forall n\geq 1, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2$",Find all positive sequneces  such that:,"(s_n)_n \forall n\geq 1, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2","The goal is to find all positive sequneces $(s_n)_n$ such that: $$ \forall n\geq 1, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2. $$ My attempt is the following: Let us consider a sequence $(s_n)_n$ satisfying: $$ s_n>0, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2, $$ for all $n\geq 1$ . From the last equality we can remark: $$ \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^{n-1} s_k+ s_n\right)^2= \left(\sum_{k=1}^{n-1} s_k\right)^2+s_n^2 + 2s_n \sum_{k=1}^{n-1} s_k. $$ This implies that $$ \sum_{k=1}^n s_k^3 =\underbrace{\left(\sum_{k=1}^{n-1} s_k\right)^2}_{\sum_{k=1}^{n-1} s_k^3} +s_n^2 + 2s_n \sum_{k=1}^{n-1} s_k. $$ So we get that $$ s_n^3 - s_n^2 - 2s_n \sum_{k=1}^{n-1} s_k= 0 \implies s_n^2 - s_n -2 \sum_{k=1}^{n-1} s_k = 0, $$ since $s_n>0$ . However I cannot find the sequences $(s_n)_n$ that satisfy the above equality.",The goal is to find all positive sequneces such that: My attempt is the following: Let us consider a sequence satisfying: for all . From the last equality we can remark: This implies that So we get that since . However I cannot find the sequences that satisfy the above equality.,"(s_n)_n 
\forall n\geq 1, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2.
 (s_n)_n 
s_n>0, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2,
 n\geq 1 
\sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^{n-1} s_k+ s_n\right)^2= \left(\sum_{k=1}^{n-1} s_k\right)^2+s_n^2 + 2s_n \sum_{k=1}^{n-1} s_k.
 
\sum_{k=1}^n s_k^3 =\underbrace{\left(\sum_{k=1}^{n-1} s_k\right)^2}_{\sum_{k=1}^{n-1} s_k^3} +s_n^2 + 2s_n \sum_{k=1}^{n-1} s_k.
 
s_n^3 - s_n^2 - 2s_n \sum_{k=1}^{n-1} s_k= 0 \implies s_n^2 - s_n -2 \sum_{k=1}^{n-1} s_k = 0,
 s_n>0 (s_n)_n","['real-analysis', 'calculus', 'sequences-and-series']"
65,Tough integrals: $\int_0^1\frac{\log x\arctan\left(\frac{\log x}{2\pi}\right)}{1+x^2}dx$,Tough integrals:,\int_0^1\frac{\log x\arctan\left(\frac{\log x}{2\pi}\right)}{1+x^2}dx,"Here are two integrals: $$1:\hspace{2cm} \int_0^1\frac{\log x\arctan\left(\frac{\log x}{2\pi}\right)}{1+x^2}dx=\frac{\pi^2}{4}\log\left(\frac{192\pi^4}{\Gamma\left(\frac14\right)^8}\right)+\frac{\pi C}{2}  $$ $$2:\hspace{1.7cm}\int_0^1\frac{\log x\arctan\left(\frac{\pi}{2\log x}\right)}{1+x^2}dx=\frac{\pi C}{2}+\frac{\pi^2}{8}-\frac{\pi^2}{24}\log\left(\frac{A^{36}}{2\pi^3}\right) $$ where $C$ is Catalan's constant and $A$ is Glaisher's constant. My proof for both of them uses Feynman's trick on $$G(a)=\int_0^1\frac{\log x\arctan\left(\frac{a}{\log x}\right)}{1+x^2}dx $$ but it requires to know some values of $\psi^{-2}(x)$ , which I had to look up on Wikipedia . So, given the ""nice"" closed forms, I hope there is a way to evaluate them without using Feynman technique. For instance in $(1)$ I tried to expand the $\arctan$ as a series, and then use the fact that $$\int_0^1\frac{\log^{2n}(x)}{1+x^2}dx=\frac{(-1)^n}{2}\left(\frac{\pi}{2}\right)^{2n+1}E_{2n} $$ to get a series with Euler numbers. But this is wrong, since $\left|\frac{\log x}{2\pi}\right|>1$ for $0<x<1$ , and so the series is not convergent. In contrast, if one tries the same approach on $(2)$ , where the series is convergent due to $\left|\frac{\pi}{2\log x}\right|<1$ , one gets integrals of the form $$\int_0^1\frac{\log^{-2n}(x)}{1+x^2}dx$$ which are not convergent at all. So how would you do it?","Here are two integrals: where is Catalan's constant and is Glaisher's constant. My proof for both of them uses Feynman's trick on but it requires to know some values of , which I had to look up on Wikipedia . So, given the ""nice"" closed forms, I hope there is a way to evaluate them without using Feynman technique. For instance in I tried to expand the as a series, and then use the fact that to get a series with Euler numbers. But this is wrong, since for , and so the series is not convergent. In contrast, if one tries the same approach on , where the series is convergent due to , one gets integrals of the form which are not convergent at all. So how would you do it?",1:\hspace{2cm} \int_0^1\frac{\log x\arctan\left(\frac{\log x}{2\pi}\right)}{1+x^2}dx=\frac{\pi^2}{4}\log\left(\frac{192\pi^4}{\Gamma\left(\frac14\right)^8}\right)+\frac{\pi C}{2}   2:\hspace{1.7cm}\int_0^1\frac{\log x\arctan\left(\frac{\pi}{2\log x}\right)}{1+x^2}dx=\frac{\pi C}{2}+\frac{\pi^2}{8}-\frac{\pi^2}{24}\log\left(\frac{A^{36}}{2\pi^3}\right)  C A G(a)=\int_0^1\frac{\log x\arctan\left(\frac{a}{\log x}\right)}{1+x^2}dx  \psi^{-2}(x) (1) \arctan \int_0^1\frac{\log^{2n}(x)}{1+x^2}dx=\frac{(-1)^n}{2}\left(\frac{\pi}{2}\right)^{2n+1}E_{2n}  \left|\frac{\log x}{2\pi}\right|>1 0<x<1 (2) \left|\frac{\pi}{2\log x}\right|<1 \int_0^1\frac{\log^{-2n}(x)}{1+x^2}dx,"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
66,Convergence of $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$,Convergence of,\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}},"There is an integral $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$ . Prove that it converges only conditionally. It is absolutely divergent because when $x \to +\infty$ one would have $\frac{\sin(x)}{\sqrt{x}} \to 0$ , so $\left|\sin\left( \frac{\sin(x)}{\sqrt{x}} \right)\right| \geq \frac{1}{2} \left| \frac{\sin(x)}{\sqrt{x}} \right| $ , at least when $x > x_0$ for some $x_0 > 1$ . So for $x > x_0$ one would have $$ \left| \sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{1}{\sqrt{x}}\right| \geq \frac{1}{2}\frac{|\sin(x)|}{x} $$ It is known that $\int_1^{+\infty} \frac{|\sin(x)|}{x} dx$ diverges. But how to prove that $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$ converges? Any help would be appreciated. UPD. After reading the comments I came to a solution. Please check it out. We have $$ \sin\left(\frac{\sin(x)}{\sqrt{x}}\right) = \frac{\sin(x)}{\sqrt{x}} + g(x) $$ where $g(x) = o\left(\frac{\sin^2(x)}{x}\right)$ when $x \to +\infty$ . So $$ \sin\left(\frac{\sin(x)}{\sqrt{x}}\right)\frac{1}{\sqrt{x}} = \frac{\sin(x)}{x} + h(x) $$ where $h(x) = o\left(\frac{\sin^2(x)}{x^{\frac{3}{2}}}\right)$ when $x \to +\infty$ . The integral $\int_1^{+\infty } \frac{\sin^2(x)}{x^{\frac{3}{2}}} dx$ converges absolutely, so $\int_1^{+\infty } h(x) dx$ converges absolutely and the convergence of $\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}}$ depends only on the convergence of $\int_1^{+\infty}\frac{\sin(x)dx}{x}$ . This integral is convergent, so the integral under consideration converges.","There is an integral . Prove that it converges only conditionally. It is absolutely divergent because when one would have , so , at least when for some . So for one would have It is known that diverges. But how to prove that converges? Any help would be appreciated. UPD. After reading the comments I came to a solution. Please check it out. We have where when . So where when . The integral converges absolutely, so converges absolutely and the convergence of depends only on the convergence of . This integral is convergent, so the integral under consideration converges.","\int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}} x \to +\infty \frac{\sin(x)}{\sqrt{x}} \to 0 \left|\sin\left( \frac{\sin(x)}{\sqrt{x}} \right)\right| \geq \frac{1}{2} \left| \frac{\sin(x)}{\sqrt{x}} \right|  x > x_0 x_0 > 1 x > x_0 
\left| \sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{1}{\sqrt{x}}\right| \geq \frac{1}{2}\frac{|\sin(x)|}{x}
 \int_1^{+\infty} \frac{|\sin(x)|}{x} dx \int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}} 
\sin\left(\frac{\sin(x)}{\sqrt{x}}\right) = \frac{\sin(x)}{\sqrt{x}} + g(x)
 g(x) = o\left(\frac{\sin^2(x)}{x}\right) x \to +\infty 
\sin\left(\frac{\sin(x)}{\sqrt{x}}\right)\frac{1}{\sqrt{x}} = \frac{\sin(x)}{x} + h(x)
 h(x) = o\left(\frac{\sin^2(x)}{x^{\frac{3}{2}}}\right) x \to +\infty \int_1^{+\infty } \frac{\sin^2(x)}{x^{\frac{3}{2}}} dx \int_1^{+\infty } h(x) dx \int_1^{+\infty}\sin\left( \frac{\sin(x)}{\sqrt{x}} \right) \frac{dx}{\sqrt{x}} \int_1^{+\infty}\frac{\sin(x)dx}{x}","['real-analysis', 'integration', 'solution-verification', 'improper-integrals']"
67,Integral inequality implies majorization by solution of ODE,Integral inequality implies majorization by solution of ODE,,"Let $f:[0, \infty)\to [0, \infty)$ be non-decreasing (and not necessarily differentiable nor continuous) and satisfy $$f(t)\leq f(0)+C\int_{0}^{t}f(s)^{2}ds,$$ where $C>0$ . Is it true then that $$f(t)\leq g(t)\quad \text{for all}~t<t_{*},$$ where $t_{*}>0$ and $g$ is a differentiable function on the interval $[0,t_*)$ such that $$g'(t)=Cg(t)^{2}\quad\forall\,0<t<t_*, \quad g(0)=f(0)?$$ Obviously if we would have equality in the above integral inequality and differentiability of $f$ , we would have $f=g$ , but is it still true that $f\leq g$ under these weaker assumptions? My idea is to discretize it (by the monotonicity of $f$ ), so that we have $$f_{n}\leq f_{0}+C\sum_{k=1}^{n}{f^{2}_{k}}$$ and by the assumption on $g$ , $$g_{n}= g_{0}+C\sum_{k=1}^{n}{g^{2}_{k}}, \quad g_{0}=f_{0},$$ and show that $f_{k}\leq g_{k}$ for all $1\leq k\leq n$ but the inductive step does not seem to be working.","Let be non-decreasing (and not necessarily differentiable nor continuous) and satisfy where . Is it true then that where and is a differentiable function on the interval such that Obviously if we would have equality in the above integral inequality and differentiability of , we would have , but is it still true that under these weaker assumptions? My idea is to discretize it (by the monotonicity of ), so that we have and by the assumption on , and show that for all but the inductive step does not seem to be working.","f:[0, \infty)\to [0, \infty) f(t)\leq f(0)+C\int_{0}^{t}f(s)^{2}ds, C>0 f(t)\leq g(t)\quad \text{for all}~t<t_{*}, t_{*}>0 g [0,t_*) g'(t)=Cg(t)^{2}\quad\forall\,0<t<t_*, \quad g(0)=f(0)? f f=g f\leq g f f_{n}\leq f_{0}+C\sum_{k=1}^{n}{f^{2}_{k}} g g_{n}= g_{0}+C\sum_{k=1}^{n}{g^{2}_{k}}, \quad g_{0}=f_{0}, f_{k}\leq g_{k} 1\leq k\leq n","['real-analysis', 'integration', 'ordinary-differential-equations']"
68,"$f^{-1}(0)=g^{-1}(0)$, $f^{-1}(1)=g^{-1}(1)$, then $f=g$",", , then",f^{-1}(0)=g^{-1}(0) f^{-1}(1)=g^{-1}(1) f=g,"$f(x)$ and $g(x)$ are two polynomials in $\mathbb{C}[x]$ , and $$f^{-1}(a)=\{z\in \mathbb{C}\mid f(z)=a\}$$ if $$f^{-1}(0)=g^{-1}(0),\;f^{-1}(1)=g^{-1}(1)$$ then $f=g$ holds. I know $f^{-1}(0)=g^{-1}(0)$ implies that the two polynomials have the same set of zero points, but I dont know how to use the property of $f^{-1}(1)=g^{-1}(1)$ .","and are two polynomials in , and if then holds. I know implies that the two polynomials have the same set of zero points, but I dont know how to use the property of .","f(x) g(x) \mathbb{C}[x] f^{-1}(a)=\{z\in \mathbb{C}\mid f(z)=a\} f^{-1}(0)=g^{-1}(0),\;f^{-1}(1)=g^{-1}(1) f=g f^{-1}(0)=g^{-1}(0) f^{-1}(1)=g^{-1}(1)","['real-analysis', 'linear-algebra', 'analysis']"
69,"show that if $\lim (f(x_1)+\cdots +f(x_n))/n$ exists whenever $\lim (x_1+\cdots +x_n)/n$ exists, then $f$ is continuous","show that if  exists whenever  exists, then  is continuous",\lim (f(x_1)+\cdots +f(x_n))/n \lim (x_1+\cdots +x_n)/n f,"Show that if $f:[0,1]\to \mathbb{R}$ and $$\lim \limits _{n\to \infty}\frac{f(x_1)+\cdots + f(x_n)}{n}$$ exists whenever $$\lim \limits _{n\to \infty}\frac{x_1+\cdots + x_n}{n}$$ exists, where $(x_n)$ is a sequence of real numbers, then $f$ is continuous. It suffices to show that if $x_n\to x$ , then $f(x_n)\to f(x)$ . It might be useful to show this via a contradiction; suppose there is a sequence $x_n$ so that $x_n\to x\in \mathbb{R}$ but $f(x_n)\not \to f(x)$ . We know that $\dfrac{f(x_1)+\cdots +f(x_n)}n$ exists and equals $L$ , say. By definition, there exists $\varepsilon >0$ so that for all $N$ , $\exists n\geq N$ such that $|f(x_n)-f(x)|\geq \varepsilon$ . But I'm not sure how to get a contradiction from here.","Show that if and exists whenever exists, where is a sequence of real numbers, then is continuous. It suffices to show that if , then . It might be useful to show this via a contradiction; suppose there is a sequence so that but . We know that exists and equals , say. By definition, there exists so that for all , such that . But I'm not sure how to get a contradiction from here.","f:[0,1]\to \mathbb{R} \lim \limits _{n\to \infty}\frac{f(x_1)+\cdots + f(x_n)}{n} \lim \limits _{n\to \infty}\frac{x_1+\cdots + x_n}{n} (x_n) f x_n\to x f(x_n)\to f(x) x_n x_n\to x\in \mathbb{R} f(x_n)\not \to f(x) \dfrac{f(x_1)+\cdots +f(x_n)}n L \varepsilon >0 N \exists n\geq N |f(x_n)-f(x)|\geq \varepsilon","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
70,Does $z$ depend on $x$ in the proof of Theorem 6.20 in Folland?,Does  depend on  in the proof of Theorem 6.20 in Folland?,z x,"The theorem states: Let $K$ be a Lebesgue measurable function on $(0, \infty) \times (0, \infty)$ such that $K(\lambda x, \lambda y) = \lambda^{-1}K(x,y)$ for all $\lambda > 0$ and $\int_0^\infty |K(x,1)|x^{-1/p}\,dx = C < \infty$ for some $p \in [1, \infty]$ , and let $q$ be the conjugate exponent to $p$ . For $f \in L^p$ and $g \in L^q$ , let $$Tf(y) = \int_0^\infty K(x,y)f(x)\,dx, \quad Sg(x) = \int_0^\infty K(x,y)g(y)\,dy.$$ Then $Tf$ and $Sg$ are defind a.e., and $\|Tf\|_p \leq C\|f\|_p$ and $\|Sg\|_q \leq C\|g\|_q$ . The proof proceeds as follows: Setting $z = x/y$ , we have $$\int_0^\infty |K(x,y) f(x)|\,dx = \int_0^\infty |K(yz,y) f(yz)|y\,dz = \int_0^\infty |K(z,1) f_z(y)|\,dz$$ where $f_z(u) = f(yz)$ ; moreover $$\|f_z\|_p = \Bigg[\int_0^\infty |f(yz)|^p\,dy\Bigg]^{1/p} = \Bigg[\int_0^\infty |f(x)|^pz^{-1}\,dx\Bigg]^{1/p} = z^{-1/p}\|f\|_p.$$ The doubt I have is rather elementary. In the very last equality, how can we pull out $z$ if $z$ itself depends on $x$ ? The proof then proceeds: Therefore, by the Minkowski inequality for integrals $Tf$ exists a.e. and $$\|Tf\|_p \leq \int_0^\infty |K(z,1)\|f_z\|_p\,dz = \|f\|_p \int_0^\infty |K(z,1)|z^{-1/p}\,dz = C\|f\|_p.$$ How does the Minkowski inequality for integrals imply the existence of $Tf$ ? I understand the rest of the proof so I have omitted it.","The theorem states: Let be a Lebesgue measurable function on such that for all and for some , and let be the conjugate exponent to . For and , let Then and are defind a.e., and and . The proof proceeds as follows: Setting , we have where ; moreover The doubt I have is rather elementary. In the very last equality, how can we pull out if itself depends on ? The proof then proceeds: Therefore, by the Minkowski inequality for integrals exists a.e. and How does the Minkowski inequality for integrals imply the existence of ? I understand the rest of the proof so I have omitted it.","K (0, \infty) \times (0, \infty) K(\lambda x, \lambda y) = \lambda^{-1}K(x,y) \lambda > 0 \int_0^\infty |K(x,1)|x^{-1/p}\,dx = C < \infty p \in [1, \infty] q p f \in L^p g \in L^q Tf(y) = \int_0^\infty K(x,y)f(x)\,dx, \quad Sg(x) = \int_0^\infty K(x,y)g(y)\,dy. Tf Sg \|Tf\|_p \leq C\|f\|_p \|Sg\|_q \leq C\|g\|_q z = x/y \int_0^\infty |K(x,y) f(x)|\,dx = \int_0^\infty |K(yz,y) f(yz)|y\,dz = \int_0^\infty |K(z,1) f_z(y)|\,dz f_z(u) = f(yz) \|f_z\|_p = \Bigg[\int_0^\infty |f(yz)|^p\,dy\Bigg]^{1/p} = \Bigg[\int_0^\infty |f(x)|^pz^{-1}\,dx\Bigg]^{1/p} = z^{-1/p}\|f\|_p. z z x Tf \|Tf\|_p \leq \int_0^\infty |K(z,1)\|f_z\|_p\,dz = \|f\|_p \int_0^\infty |K(z,1)|z^{-1/p}\,dz = C\|f\|_p. Tf","['real-analysis', 'integration', 'lebesgue-integral', 'lp-spaces']"
71,"Does there exists a sequence $b_n$, s.t. $\lim b_n = 0$ and for every divergent series $\sum a_n$, The series $\sum a_n b_n$ also diverges?","Does there exists a sequence , s.t.  and for every divergent series , The series  also diverges?",b_n \lim b_n = 0 \sum a_n \sum a_n b_n,"I am trying to detemine whether the following statement is true or false: Does there exists a positive sequence $b_n$ , s.t. $\lim b_n = 0$ and for any positive divergent series $\sum a_n$ , The series $\sum a_n b_n$ also diverges? At first I believed the statement is false, Because it does make sense to me that we can always find a divergent series $\sum a_n$ such that $a_n$ will need a ""little enough push"" so that the series will do converge. when looking at some very slow growing sequences such as $b_n =1/ \ln\ln\ln\ln\ln(n)$ , I was not able to find such $a_n$ . So the question is, Is it possible to find such $b_n$ which converge to $0$ slow enough? Any hints will be appericiated.","I am trying to detemine whether the following statement is true or false: Does there exists a positive sequence , s.t. and for any positive divergent series , The series also diverges? At first I believed the statement is false, Because it does make sense to me that we can always find a divergent series such that will need a ""little enough push"" so that the series will do converge. when looking at some very slow growing sequences such as , I was not able to find such . So the question is, Is it possible to find such which converge to slow enough? Any hints will be appericiated.",b_n \lim b_n = 0 \sum a_n \sum a_n b_n \sum a_n a_n b_n =1/ \ln\ln\ln\ln\ln(n) a_n b_n 0,"['real-analysis', 'calculus', 'sequences-and-series']"
72,How does one show that $\left|\operatorname{sgn}(a)|a|^{1/2}-\operatorname{sgn}(b)|b|^{1/2}\right|^2\le2|a-b|$ using more elementary techniques?,How does one show that  using more elementary techniques?,\left|\operatorname{sgn}(a)|a|^{1/2}-\operatorname{sgn}(b)|b|^{1/2}\right|^2\le2|a-b|,"$\newcommand{\sgn}{\operatorname{sgn}}$ It was left as an exercise to show that: $\forall a,b\in\Bbb R,$ $$\left|\sgn(a)|a|^{1/2}-\sgn(b)|b|^{1/2}\right|^2\le2\cdot|a-b|$$ The only techniques I have seen used to solve these problems (they seem to occur often in the functional analysis of $\mathcal{L}^p$ spaces - I've seen this method used in the proofs of Banach-Saks and some other theorems) is to do a division and show that the resulting rational function is bounded and positive in the region of interest by taking limits. My solution given below works, but it felt like more work than should have been necessary for this problem - I am asking if there are simpler approaches. I then am reducing this problem to showing that: $$\frac{|\sgn(a)|a|^{1/2}-\sgn(b)|b|^{1/2}|^2 }{|a-b|}\le2$$ Always. Since the expression is symmetric in $a,b$ , assume wlog that $a\ge b$ . Note of course that the expression is always positive, so we need only show that it is bounded from above. Consider then the above as a function $f(a)$ defined on regions where $a\ge b$ . If $b=0$ the expression is clearly $1\le 2$ for all $a$ , so instead take $b\gt 0$ as a first case. For $a\ge b\gt 0$ : $$f(a)=\frac{a+b-2\sqrt{ab}}{a-b}=\frac{(a^{1/2}-b^{1/2})^2}{(a^{1/2}-b^{1/2})(a^{1/2}+b^{1/2})}=\frac{a^{1/2}-b^{1/2}}{a^{1/2}+b^{1/2}}$$ As $a\to b^+$ the limit is $0\le 2$ ; now take its derivative: $$f'(a\ge b\gt0)=\frac{\sqrt{b/a}}{a+b+2\sqrt{ab}}\gt0$$ So as $f'$ is continuous we have that $f$ is a strictly increasing function, beginning (in the range $a\ge b\gt0$ ) at $0$ and has its upper bound at $\lim_{a\to\infty}f(a)=1\lt2$ . Therefore $0\le f\lt 2$ when $a\ge b\ge0$ . Now suppose $b\lt0$ . There are two cases; $a\gt0,b\le a\lt 0$ . $a=0$ is excluded as the function is clearly $1$ in this case also. First take $a\gt0$ : $$f(a)=\frac{a-b+2\sqrt{a|b|}}{a-b}\ge1$$ And: $$f'(a)=\frac{(1+\sqrt{|b|/a})(a-b)-a+b-2\sqrt{a|b|}}{(a-b)^2}=\frac{-b\sqrt{|b|/a}-\sqrt{a|b|}}{(a-b)^2}$$ Let the numerator be $h(a)$ . $h'(a)=-\frac{b}{2a}\sqrt{\frac{|b|}{a}}-\frac{1}{2}\sqrt{\frac{|b|}{a}}=0\iff a=-b$ , and as $a$ exceeds this point $h'(a)\lt 0$ clearly, so this is a maximum of $h$ , and $f(-b)=2$ and thus when $a\gt0,\,f\le 2$ . Now suppose $b\le a\lt 0$ : $$f(a)=\frac{a+b+2\sqrt{|a||b|}}{b-a}$$ And: $$f'(a)=\frac{(1-\sqrt{|b|/|a|})(b-a)+a+b+2\sqrt{|a||b|}}{(b-a)^2}=\frac{2b+\sqrt{|a||b|}-b\sqrt{|b|/|a|}}{(b-a)^2}$$ Call the numerator $g(a)$ . $g'(a)=-\frac{1}{2}\sqrt{|b|/|a|}-\frac{b}{2|a|}\sqrt{|b|/|a|}\gt0$ since $|b|\gt|a|$ . Therefore $g$ is increasing; take $\lim_{a\to b^+}g(a)=0$ , which implies that $g$ increases beyond $0$ and is always positive. Thus $f'$ is positive and $f$ increases. As $a\to 0^-$ , $f\to 1$ , and this implies that $f\le 1$ in the region $b\le a\le0$ . Having considered all cases for $a\ge b$ wlog, this concludes that $f\le 2$ . This was tedious work for me; I made many mistakes with absolute value derivatives along the way... was there a more elementary way of seeing this inequality? Royden leaves it as an exercise and I'm not great with inequalities - I presume one did not need to take this calculus approach. Note: it is left as another exercise to show that: $$\left|\sgn(a)\cdot|a|^2-\sgn(b)\cdot|b|^2\right|\le2\cdot|a-b|(|a|+|b|)$$ Which definitely suggests these problems have a simpler solution, since calculating the above using my calculus method would be very tedious.","It was left as an exercise to show that: The only techniques I have seen used to solve these problems (they seem to occur often in the functional analysis of spaces - I've seen this method used in the proofs of Banach-Saks and some other theorems) is to do a division and show that the resulting rational function is bounded and positive in the region of interest by taking limits. My solution given below works, but it felt like more work than should have been necessary for this problem - I am asking if there are simpler approaches. I then am reducing this problem to showing that: Always. Since the expression is symmetric in , assume wlog that . Note of course that the expression is always positive, so we need only show that it is bounded from above. Consider then the above as a function defined on regions where . If the expression is clearly for all , so instead take as a first case. For : As the limit is ; now take its derivative: So as is continuous we have that is a strictly increasing function, beginning (in the range ) at and has its upper bound at . Therefore when . Now suppose . There are two cases; . is excluded as the function is clearly in this case also. First take : And: Let the numerator be . , and as exceeds this point clearly, so this is a maximum of , and and thus when . Now suppose : And: Call the numerator . since . Therefore is increasing; take , which implies that increases beyond and is always positive. Thus is positive and increases. As , , and this implies that in the region . Having considered all cases for wlog, this concludes that . This was tedious work for me; I made many mistakes with absolute value derivatives along the way... was there a more elementary way of seeing this inequality? Royden leaves it as an exercise and I'm not great with inequalities - I presume one did not need to take this calculus approach. Note: it is left as another exercise to show that: Which definitely suggests these problems have a simpler solution, since calculating the above using my calculus method would be very tedious.","\newcommand{\sgn}{\operatorname{sgn}} \forall a,b\in\Bbb R, \left|\sgn(a)|a|^{1/2}-\sgn(b)|b|^{1/2}\right|^2\le2\cdot|a-b| \mathcal{L}^p \frac{|\sgn(a)|a|^{1/2}-\sgn(b)|b|^{1/2}|^2
}{|a-b|}\le2 a,b a\ge b f(a) a\ge b b=0 1\le 2 a b\gt 0 a\ge b\gt 0 f(a)=\frac{a+b-2\sqrt{ab}}{a-b}=\frac{(a^{1/2}-b^{1/2})^2}{(a^{1/2}-b^{1/2})(a^{1/2}+b^{1/2})}=\frac{a^{1/2}-b^{1/2}}{a^{1/2}+b^{1/2}} a\to b^+ 0\le 2 f'(a\ge b\gt0)=\frac{\sqrt{b/a}}{a+b+2\sqrt{ab}}\gt0 f' f a\ge b\gt0 0 \lim_{a\to\infty}f(a)=1\lt2 0\le f\lt 2 a\ge b\ge0 b\lt0 a\gt0,b\le a\lt 0 a=0 1 a\gt0 f(a)=\frac{a-b+2\sqrt{a|b|}}{a-b}\ge1 f'(a)=\frac{(1+\sqrt{|b|/a})(a-b)-a+b-2\sqrt{a|b|}}{(a-b)^2}=\frac{-b\sqrt{|b|/a}-\sqrt{a|b|}}{(a-b)^2} h(a) h'(a)=-\frac{b}{2a}\sqrt{\frac{|b|}{a}}-\frac{1}{2}\sqrt{\frac{|b|}{a}}=0\iff a=-b a h'(a)\lt 0 h f(-b)=2 a\gt0,\,f\le 2 b\le a\lt 0 f(a)=\frac{a+b+2\sqrt{|a||b|}}{b-a} f'(a)=\frac{(1-\sqrt{|b|/|a|})(b-a)+a+b+2\sqrt{|a||b|}}{(b-a)^2}=\frac{2b+\sqrt{|a||b|}-b\sqrt{|b|/|a|}}{(b-a)^2} g(a) g'(a)=-\frac{1}{2}\sqrt{|b|/|a|}-\frac{b}{2|a|}\sqrt{|b|/|a|}\gt0 |b|\gt|a| g \lim_{a\to b^+}g(a)=0 g 0 f' f a\to 0^- f\to 1 f\le 1 b\le a\le0 a\ge b f\le 2 \left|\sgn(a)\cdot|a|^2-\sgn(b)\cdot|b|^2\right|\le2\cdot|a-b|(|a|+|b|)","['real-analysis', 'inequality', 'solution-verification', 'absolute-value']"
73,"An order that lacks ""infinite"" transitivity?","An order that lacks ""infinite"" transitivity?",,"The ordering on the real line "" $<$ "" possesses the property that if we have a sequence $(a_n)_{n=1}^{\infty}$ such that $a_n < a_{n+1}$ and if we have a finite limit $a$ , then $a_1 < a_2 < a_3 < \dots < a$ and we can conclude that $a_1 < a$ - even though transitivity of "" $<$ "" is a statement of finitely many elements. Can we have an ordering, not necessarily on the reals, where ""infinite"" transitivity is not necessarily followed? We'd keep all the other properties: anti-symmetric and transitive; but we could have an $""a""$ such that $a_0$ and $a$ are incomparable but there exists an infinite chain  such that $a_0 < a_1 < \dots < a$ . Have these orderings been discussed already? Attempt: An attempt I've thought about is taking the extended reals ( $\mathbb{R} \cup{\{\pm \infty\}}$ ) and defining the normal order $<$ on the finite numbers but every finite number is ""greater than"" $\infty$ , $-\infty > \infty$ , and every finite number is ""less than"" $-\infty$ . So if we had a sequence diverging to $\infty$ , we'd simultaneously arrive at the smallest element while getting larger; which wouldn't be infinite transitive. However, I'm curious about a case where our limit $a$ is still larger than at least one element in our sequence. Motivation: Went on a wikipedia rabbit hole after reviewing Zorn's lemma.","The ordering on the real line "" "" possesses the property that if we have a sequence such that and if we have a finite limit , then and we can conclude that - even though transitivity of "" "" is a statement of finitely many elements. Can we have an ordering, not necessarily on the reals, where ""infinite"" transitivity is not necessarily followed? We'd keep all the other properties: anti-symmetric and transitive; but we could have an such that and are incomparable but there exists an infinite chain  such that . Have these orderings been discussed already? Attempt: An attempt I've thought about is taking the extended reals ( ) and defining the normal order on the finite numbers but every finite number is ""greater than"" , , and every finite number is ""less than"" . So if we had a sequence diverging to , we'd simultaneously arrive at the smallest element while getting larger; which wouldn't be infinite transitive. However, I'm curious about a case where our limit is still larger than at least one element in our sequence. Motivation: Went on a wikipedia rabbit hole after reviewing Zorn's lemma.","< (a_n)_{n=1}^{\infty} a_n < a_{n+1} a a_1 < a_2 < a_3 < \dots < a a_1 < a < ""a"" a_0 a a_0 < a_1 < \dots < a \mathbb{R} \cup{\{\pm \infty\}} < \infty -\infty > \infty -\infty \infty a","['real-analysis', 'sequences-and-series', 'relations', 'order-theory']"
74,Any Well-Ordering of $\mathbb{R}$ has no Corresponding Metric,Any Well-Ordering of  has no Corresponding Metric,\mathbb{R},"Background I have been doing some reading over winter break so far and found the idea that $\mathbb{R}$ has a well-ordering strange. So I have been thinking about it a little and was wondering if my proof for the following property is sound. If not, is it true? Proof critics are always encouraged! Thanks! Statement First, let me define a term Definition: An ordering $\leq$ has a corresponding metric $d(x,y)$ if: $$x < y < z \Rightarrow d(x,y) < d(x,z)\ \land\ d(y,z) < d(x,z)$$ The property I have tried to prove is Theorem: Any well-ordering of $\mathbb{R}$ has no corresponding metric Proof Because of the well-ordered property, we can define $L_n$ to be the set containing the $n$ least elements in $\mathbb{R}$ . $\forall x \in \mathbb{R}\setminus L_2$ , there exists an $r_1 \in \mathbb{R}$ such that the open ball $B_{r_1}(x)$ has two unique least elements not equal to $x$ . Call the smaller of the two $l$ . Because $B_{r_1}(x)$ is open, there exists some $\epsilon_1 > 0$ such that $B_{\epsilon_1}(l) \subset B_{r_1}(x)$ . Now define $r_2 := d(x, l)$ . By definition of open ball, $l \not\in B_{r_2}(x)$ . Because of well-order, $B_{r_2}(x)$ has some least element $l'$ . There exists some $\epsilon_2 > 0$ such that $B_{\epsilon_2}(l') \subset B_{r_2}(x)$ . So we have $l < l'$ , $l \not\in B_{\epsilon_2}(l')$ and so $\forall y \in B_{\epsilon_2}(l)$ , $y < l' \Rightarrow y \not\in B_{r_2}(x)$ . Thus, $B_{\epsilon_2}(l) \cap B_{r_2}(x) = \emptyset$ . Now, let $\epsilon := \min(\epsilon_1, \epsilon_2)$ . $B_\epsilon(l) = \{l\}$ . It contains no elements less than $l$ , as otherwise $l$ is not the min element of $B_r(x)$ ; it has no elements larger than $l$ , as otherwise they would be in $B_{r_2}(x)$ . Thus, for any open ball $B$ with min element $l$ , there exists $\epsilon > 0$ such that $B_\epsilon(l) = \{l\}$ . Let $G$ be a set of all minimum elements of open balls. There is a clear injection from $G$ to $\mathbb{Q}$ by selecting any rational number in a small enough ball (using the regular metric) centered around elements of $G$ . Now notice that every open bounded interval is an open ball, and for any $x \in \mathbb{R}$ , there exists some open interval $(x, b)$ where $b > x$ . The smallest element in $(x, b)$ is the next element above $x$ . Thus, for every $x \in \mathbb{R}$ , the preceeding element of $x$ is in $G$ (and is unique with respect to $x$ ). This implies that since $\mathbb{R}$ is uncountable, so is $G$ . This is our contradiction, and so a corresponding metric $d$ can not exist. QED .","Background I have been doing some reading over winter break so far and found the idea that has a well-ordering strange. So I have been thinking about it a little and was wondering if my proof for the following property is sound. If not, is it true? Proof critics are always encouraged! Thanks! Statement First, let me define a term Definition: An ordering has a corresponding metric if: The property I have tried to prove is Theorem: Any well-ordering of has no corresponding metric Proof Because of the well-ordered property, we can define to be the set containing the least elements in . , there exists an such that the open ball has two unique least elements not equal to . Call the smaller of the two . Because is open, there exists some such that . Now define . By definition of open ball, . Because of well-order, has some least element . There exists some such that . So we have , and so , . Thus, . Now, let . . It contains no elements less than , as otherwise is not the min element of ; it has no elements larger than , as otherwise they would be in . Thus, for any open ball with min element , there exists such that . Let be a set of all minimum elements of open balls. There is a clear injection from to by selecting any rational number in a small enough ball (using the regular metric) centered around elements of . Now notice that every open bounded interval is an open ball, and for any , there exists some open interval where . The smallest element in is the next element above . Thus, for every , the preceeding element of is in (and is unique with respect to ). This implies that since is uncountable, so is . This is our contradiction, and so a corresponding metric can not exist. QED .","\mathbb{R} \leq d(x,y) x < y < z \Rightarrow d(x,y) < d(x,z)\ \land\ d(y,z) < d(x,z) \mathbb{R} L_n n \mathbb{R} \forall x \in \mathbb{R}\setminus L_2 r_1 \in \mathbb{R} B_{r_1}(x) x l B_{r_1}(x) \epsilon_1 > 0 B_{\epsilon_1}(l) \subset B_{r_1}(x) r_2 := d(x, l) l \not\in B_{r_2}(x) B_{r_2}(x) l' \epsilon_2 > 0 B_{\epsilon_2}(l') \subset B_{r_2}(x) l < l' l \not\in B_{\epsilon_2}(l') \forall y \in B_{\epsilon_2}(l) y < l' \Rightarrow y \not\in B_{r_2}(x) B_{\epsilon_2}(l) \cap B_{r_2}(x) = \emptyset \epsilon := \min(\epsilon_1, \epsilon_2) B_\epsilon(l) = \{l\} l l B_r(x) l B_{r_2}(x) B l \epsilon > 0 B_\epsilon(l) = \{l\} G G \mathbb{Q} G x \in \mathbb{R} (x, b) b > x (x, b) x x \in \mathbb{R} x G x \mathbb{R} G d","['real-analysis', 'set-theory', 'order-theory', 'well-orders']"
75,Epsilon Delta definition of a Derivative,Epsilon Delta definition of a Derivative,,"The derivative at a specific point $c$ is represented as a limit by: $$ f'(c) = \lim_{x\to c} \frac{f(x) - f(c)}{x - c} $$ It's clear to me that the epsilon delta definition of a derivative at a point $c$ would be: $$ \forall \epsilon > 0 ~\exists \delta > 0 \forall x: \\ 0 < |x-c| < \delta \rightarrow |\frac{f(x)-f(c)}{x-c} - L| < \epsilon $$ What's unclear to me is how to formally represent the derivative as a function of $x$ , rather than only at point $c$ .  Basically, how would we represent this limit formally (the $\Delta x$ is the part tripping me up): $$ f'(x) = \lim_{\Delta x\to0} \frac{f(x + \Delta x) - f(x)}{\Delta x} $$","The derivative at a specific point is represented as a limit by: It's clear to me that the epsilon delta definition of a derivative at a point would be: What's unclear to me is how to formally represent the derivative as a function of , rather than only at point .  Basically, how would we represent this limit formally (the is the part tripping me up):","c  f'(c) = \lim_{x\to c} \frac{f(x) - f(c)}{x - c}  c 
\forall \epsilon > 0 ~\exists \delta > 0 \forall x: \\
0 < |x-c| < \delta \rightarrow |\frac{f(x)-f(c)}{x-c} - L| < \epsilon
 x c \Delta x  f'(x) = \lim_{\Delta x\to0} \frac{f(x + \Delta x) - f(x)}{\Delta x} ","['real-analysis', 'calculus', 'limits', 'derivatives', 'epsilon-delta']"
76,"Maximum of $ F(f)=\int_0^1 |f(x)|^2\; dx-\left(\int_0^1 f(x)\; dx\right)^2 $ over a subset of continuous functions on $[0,1]$",Maximum of  over a subset of continuous functions on," F(f)=\int_0^1 |f(x)|^2\; dx-\left(\int_0^1 f(x)\; dx\right)^2  [0,1]","Let $X$ be a subset of $C([0,1])$ with $$ X=\big\{f\in C([0,1]): 0\le f(x)\le x,\text{$f$ is a polynomial}\big\} $$ where $C([0,1])$ denotes the space of continuous real-valued functions on $[0,1]$ . Define a nonlinear functional $F$ on $C([0,1])$ as $$ F(f)=\int_0^1 |f(x)|^2\; dx-\left(\int_0^1 f(x)\; dx\right)^2 $$ Can $F$ achieve a maximum in the set $X$ ? Remarks. If one restricts the set to the linear functions $f$ of the form $f(x)=kx$ with $0\le k\le 1$ , then this reduces to a min/max problem for values of $k$ : $$ F(f) = \int_0^1 (kx)^2\;dx-\left(\int_0^1kx\;dx\right)^2=\frac{k^2}{3}-\frac{k^2}{4} = \frac{k^2}{12} $$ So the maximum is achieved at $f(x)=x$ for the restriction. One could also restrict the functions to be parabolas of the form $f(x)=kx^2$ with $k\in[0,1]$ , and get $$ F(f)=\frac{k^2}{5}-\frac{k^2}{9}=\frac{4k^2}{45}, $$ where the maximum is achieved at $f(x)=x^2$ . If one allows one more parameter and consider $f(x)=kx^m$ , then $$ F(f)=k^2\cdot \left(\frac{1}{2m+1}-\frac{1}{(m+1)^2}\right)=\frac{k^2m^2}{(2m+1)(m+1)^2}\;. $$ Then one can study the maximum of $g(m)=\frac{m^2}{(2m+1)(m+1)^2}$ over positive integers $m$ . But this is far from analyzing all the polynomials in $X$ . Another observation is that the functional can be written as $F(f)=\|f\|_{L^2}^2-\|f\|_{L^1}^2$ . But this seems not helpful at all. This question was motivated by this unanswered question on the site. I was curious and attempted to solve that one but I didn't get anything. So I restrict the attention to the set of polynomials here.","Let be a subset of with where denotes the space of continuous real-valued functions on . Define a nonlinear functional on as Can achieve a maximum in the set ? Remarks. If one restricts the set to the linear functions of the form with , then this reduces to a min/max problem for values of : So the maximum is achieved at for the restriction. One could also restrict the functions to be parabolas of the form with , and get where the maximum is achieved at . If one allows one more parameter and consider , then Then one can study the maximum of over positive integers . But this is far from analyzing all the polynomials in . Another observation is that the functional can be written as . But this seems not helpful at all. This question was motivated by this unanswered question on the site. I was curious and attempted to solve that one but I didn't get anything. So I restrict the attention to the set of polynomials here.","X C([0,1]) 
X=\big\{f\in C([0,1]): 0\le f(x)\le x,\text{f is a polynomial}\big\}
 C([0,1]) [0,1] F C([0,1]) 
F(f)=\int_0^1 |f(x)|^2\; dx-\left(\int_0^1 f(x)\; dx\right)^2
 F X f f(x)=kx 0\le k\le 1 k 
F(f) = \int_0^1 (kx)^2\;dx-\left(\int_0^1kx\;dx\right)^2=\frac{k^2}{3}-\frac{k^2}{4} = \frac{k^2}{12}
 f(x)=x f(x)=kx^2 k\in[0,1] 
F(f)=\frac{k^2}{5}-\frac{k^2}{9}=\frac{4k^2}{45},
 f(x)=x^2 f(x)=kx^m 
F(f)=k^2\cdot \left(\frac{1}{2m+1}-\frac{1}{(m+1)^2}\right)=\frac{k^2m^2}{(2m+1)(m+1)^2}\;.
 g(m)=\frac{m^2}{(2m+1)(m+1)^2} m X F(f)=\|f\|_{L^2}^2-\|f\|_{L^1}^2","['real-analysis', 'analysis']"
77,Does a continuous function map Cauchy sequences to Cauchy sequences?,Does a continuous function map Cauchy sequences to Cauchy sequences?,,"I am self-learning Real Analysis from the text Understanding Analysis by Stephen Abbott. I'd like someone to verify if my proofs/counterexamples to below exercise are rigorous and correct. [Abbott, 4.4.6] Give an example of each of the following, or state that such a request is impossible. For any that are impossible, supply a short explanation of why this is the case. (a) A continuous function $\displaystyle f:( 0,1)\rightarrow \mathbf{R}$ and a Cauchy sequence $\displaystyle ( x_{n})$ such that $\displaystyle f( x_{n})$ is not a Cauchy sequence. (b) A uniformly continuous function $\displaystyle f:( 0,1)\rightarrow \mathbf{R}$ and a Cauchy sequence $\displaystyle ( x_{n})$ such that $\displaystyle f( x_{n})$ is not Cauchy. (c) A continuous function $\displaystyle f:[ 0,\infty )\rightarrow \mathbf{R}$ and a Cauchy sequence $\displaystyle ( x_{n})$ such that $\displaystyle f( x_{n})$ is not a Cauchy sequence. Proof. (a) Consider $\displaystyle f( x) =\frac{1}{x}$ defined and continuous on $\displaystyle ( 0,1)$ . Let $\displaystyle ( x_{n}) =\frac{1}{n}$ be a Cauchy sequence in $\displaystyle ( 0,1)$ , where $\displaystyle ( x_{n})\rightarrow 0$ . $\displaystyle f( x_{n})$ is an unbounded sequence, and so it's not Cauchy. (b) This request is impossible. Suppose $\displaystyle ( x_{n})$ is a Cauchy sequence in $\displaystyle ( 0,1)$ . Then, for all $\displaystyle \delta  >0$ , there exists $\displaystyle N\in \mathbf{N}$ , $\displaystyle | x_{n} -x_{m}| < \delta $ for all $\displaystyle n >m\geq N$ . Since $\displaystyle f$ is uniformly continuous, for all $\displaystyle \epsilon  >0$ , there exists $\displaystyle \delta  >0$ , such that for all points $\displaystyle x,y$ satisfying $\displaystyle | x-y| < \delta $ , we have $\displaystyle | f( x) -f( y)| < \epsilon $ . Consequently, for all $\displaystyle \epsilon  >0$ , there exists $\displaystyle N\in \mathbf{N}$ , such that $\displaystyle | f( x_{n}) -f( x_{m})| < \epsilon $ for all $\displaystyle n >m\geq N$ . So, $\displaystyle f( x_{n})$ is Cauchy. (c) This request is impossible. $\displaystyle [ 0,\infty )$ is a closed set. For all Cauchy sequences $\displaystyle ( x_{n}) \subseteq [ 0,\infty )$ such that $\displaystyle ( x_{n})\rightarrow c$ , the image sequence $\displaystyle f( x_{n})\rightarrow f( c)$ . So, $\displaystyle f( x_{n})$ is Cauchy.","I am self-learning Real Analysis from the text Understanding Analysis by Stephen Abbott. I'd like someone to verify if my proofs/counterexamples to below exercise are rigorous and correct. [Abbott, 4.4.6] Give an example of each of the following, or state that such a request is impossible. For any that are impossible, supply a short explanation of why this is the case. (a) A continuous function and a Cauchy sequence such that is not a Cauchy sequence. (b) A uniformly continuous function and a Cauchy sequence such that is not Cauchy. (c) A continuous function and a Cauchy sequence such that is not a Cauchy sequence. Proof. (a) Consider defined and continuous on . Let be a Cauchy sequence in , where . is an unbounded sequence, and so it's not Cauchy. (b) This request is impossible. Suppose is a Cauchy sequence in . Then, for all , there exists , for all . Since is uniformly continuous, for all , there exists , such that for all points satisfying , we have . Consequently, for all , there exists , such that for all . So, is Cauchy. (c) This request is impossible. is a closed set. For all Cauchy sequences such that , the image sequence . So, is Cauchy.","\displaystyle f:( 0,1)\rightarrow \mathbf{R} \displaystyle ( x_{n}) \displaystyle f( x_{n}) \displaystyle f:( 0,1)\rightarrow \mathbf{R} \displaystyle ( x_{n}) \displaystyle f( x_{n}) \displaystyle f:[ 0,\infty )\rightarrow \mathbf{R} \displaystyle ( x_{n}) \displaystyle f( x_{n}) \displaystyle f( x) =\frac{1}{x} \displaystyle ( 0,1) \displaystyle ( x_{n}) =\frac{1}{n} \displaystyle ( 0,1) \displaystyle ( x_{n})\rightarrow 0 \displaystyle f( x_{n}) \displaystyle ( x_{n}) \displaystyle ( 0,1) \displaystyle \delta  >0 \displaystyle N\in \mathbf{N} \displaystyle | x_{n} -x_{m}| < \delta  \displaystyle n >m\geq N \displaystyle f \displaystyle \epsilon  >0 \displaystyle \delta  >0 \displaystyle x,y \displaystyle | x-y| < \delta  \displaystyle | f( x) -f( y)| < \epsilon  \displaystyle \epsilon  >0 \displaystyle N\in \mathbf{N} \displaystyle | f( x_{n}) -f( x_{m})| < \epsilon  \displaystyle n >m\geq N \displaystyle f( x_{n}) \displaystyle [ 0,\infty ) \displaystyle ( x_{n}) \subseteq [ 0,\infty ) \displaystyle ( x_{n})\rightarrow c \displaystyle f( x_{n})\rightarrow f( c) \displaystyle f( x_{n})","['real-analysis', 'continuity', 'solution-verification', 'cauchy-sequences', 'uniform-continuity']"
78,Showing a series is finite a.e. given an $L^p$ function.,Showing a series is finite a.e. given an  function.,L^p,"Q: Suppose $f \in L^2 (\mathbb{R})$ . Show that $\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}$ is finite a.e. $x \in \mathbb{R}$ . It seems that we have to show the sum is in $L^p (\mathbb{R})$ for some $p \geq 1$ . Let's try $p = 2$ , applying Cauchy-Schwarz inequality, we have $$\int \left|\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}\right|^2 dx \leq \left(\sum_{n = 1}^\infty \frac{1}{n^{3/2}}\right) \int \left(\sum_{n = 1}^\infty (f(x + n))^2\right) dx = C \sum_{n = 1}^\infty \|f\|_2^2 = \infty .$$ It seems that this estimate is too large. For $p = 1$ , it seems that a similar situation occurs when CS-inequality is applied. I am thinking if we can show $$\int \left|\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}\right| dx \leq \sum_{n = 1}^\infty \int \frac{[f(x + n)]^2}{n^{3/2}} dx,$$ then we are done. However I have no idea how to get this (or a similar) estimate.","Q: Suppose . Show that is finite a.e. . It seems that we have to show the sum is in for some . Let's try , applying Cauchy-Schwarz inequality, we have It seems that this estimate is too large. For , it seems that a similar situation occurs when CS-inequality is applied. I am thinking if we can show then we are done. However I have no idea how to get this (or a similar) estimate.","f \in L^2 (\mathbb{R}) \sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}} x \in \mathbb{R} L^p (\mathbb{R}) p \geq 1 p = 2 \int \left|\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}\right|^2 dx \leq \left(\sum_{n = 1}^\infty \frac{1}{n^{3/2}}\right) \int \left(\sum_{n = 1}^\infty (f(x + n))^2\right) dx = C \sum_{n = 1}^\infty \|f\|_2^2 = \infty . p = 1 \int \left|\sum_{n = 1}^\infty \frac{f(x + n)}{n^{3/4}}\right| dx \leq \sum_{n = 1}^\infty \int \frac{[f(x + n)]^2}{n^{3/2}} dx,","['real-analysis', 'measure-theory']"
79,Upper semicontinuity in real analysis,Upper semicontinuity in real analysis,,Exercise from book: Let $\left(f_{k}\right)_{k=1}^{\infty}$ be a sequence of functions and suppose that they are all upper semi-continuous at $x_{0}$ . Define the function $g$ by $g(x)=\inf _{1 \leq k<\infty} f_{k}(x)$ . Show that $g$ is upper semi-continuous at $x_{0}$ . My attempt: By definition of upper semi continuity we have $\lvert x-x_0\rvert \Longrightarrow f_{k}(x) < f_{k}(x_0)+\varepsilon$ $ \inf f_{k}(x) < f_{k}(x_0)+\varepsilon$ I think it completes the proof. Is it right or I missing something????? I am not familiar with topology. so I am interested in only real analysis terms,Exercise from book: Let be a sequence of functions and suppose that they are all upper semi-continuous at . Define the function by . Show that is upper semi-continuous at . My attempt: By definition of upper semi continuity we have I think it completes the proof. Is it right or I missing something????? I am not familiar with topology. so I am interested in only real analysis terms,\left(f_{k}\right)_{k=1}^{\infty} x_{0} g g(x)=\inf _{1 \leq k<\infty} f_{k}(x) g x_{0} \lvert x-x_0\rvert \Longrightarrow f_{k}(x) < f_{k}(x_0)+\varepsilon  \inf f_{k}(x) < f_{k}(x_0)+\varepsilon,"['real-analysis', 'functions', 'semicontinuous-functions']"
80,A rigorous proof that $\nabla \cdot E = \frac{\rho}{\epsilon_0}$,A rigorous proof that,\nabla \cdot E = \frac{\rho}{\epsilon_0},"Suppose that $\rho: \mathbb R^3 \to \mathbb R$ is a function that tells us the electric charge density at each point in space. According to Coulomb's law, the electric field at a point $x \in \mathbb R^3$ is $$ E(x) = \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \frac{x - y}{\|x - y \|^3} \, dy. $$ Question: How do you prove rigorously that, for the function $E$ defined above, we have $$ \tag{1} \nabla \cdot E = \frac{\rho}{\epsilon_0} . $$ Comments: It's not easy to find a rigorous proof of this because physics textbooks tend to give non-rigorous arguments, whereas math books tend not to discuss this at all because it's about physics. Here's a typical physics derivation of this fact: \begin{align} \nabla \cdot E(x) &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \, \nabla \cdot \left(\frac{x - y}{\|x - y \|^3} \right) \, dy \\ &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \, 4 \pi \,\delta(x - y) \, dy \\ &= \frac{\rho(x)}{\epsilon_0}. \end{align} This non-rigorous argument is based on the assertion that the divergence of the function $x \mapsto \frac{x}{\|x\|^3}$ is $4 \pi \delta$ , where $\delta$ is the delta function on $\mathbb R^3$ . That claim is typically justified by non-rigorous arguments. Bonus question: Where would I look to find a rigorous proof of this or similar physics equations? Is there a book on electromagnetism written for mathematicians? A similar question was asked on the physics stackexchange, but it hasn't received a good answer. Since the question is about providing a rigorous proof I think it's a better fit for math.stackexchange. Edit: I think a key insight I was missing is that proving (1) is equivalent to showing that $$ \tag{2} \text{if} \quad \varphi(x) = -\frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \| x - y \|^{-1} \, dy \quad \text{then}\quad   \nabla^2 \varphi = \frac{\rho}{\epsilon_0}. $$ The function $-\varphi$ is called the electric potential in electrostatics. This fact (2) is fundamental to the study of the Laplace equation and Poisson's equation, and it is of course proved rigorously in PDEs textbooks such as Evans. So, it is not that there is a gap in the mathematical literature -- rather, if you want to find a rigorous proof of (1), you just need to recognize that it's equivalent to (2). Why does (2) imply (1)? First notice that if $y \in \mathbb R^3$ and $h(x) = -\|x - y \|^{-1}$ then $\nabla h(x) = \frac{x - y}{\|x - y \|^3}$ for all $x \in \mathbb R^3, x \neq y$ . It follows that \begin{align} \tag{3} \nabla \varphi(x) &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \nabla \left( -\| x - y \|^{-1} \right) \, dy \\ &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \frac{x - y}{\|x - y \|^3} \, dy \\ &= E(x). \end{align} So, $$ \nabla^2 \varphi(x) = \nabla \cdot \nabla \varphi(x) = \nabla \cdot E(x). $$ Thus, if we can show that $\nabla^2 \varphi = \frac{\rho}{\epsilon_0}$ , then we will have shown that $\nabla \cdot E = \frac{\rho}{\epsilon_0}$ . By the way, it is still not clear to me how to prove (3) rigorously. I think we need to use the dominated convergence theorem or one of the theorems from measure theory which imply that, under certain conditions, the limit of the integral is the integral of the limit.","Suppose that is a function that tells us the electric charge density at each point in space. According to Coulomb's law, the electric field at a point is Question: How do you prove rigorously that, for the function defined above, we have Comments: It's not easy to find a rigorous proof of this because physics textbooks tend to give non-rigorous arguments, whereas math books tend not to discuss this at all because it's about physics. Here's a typical physics derivation of this fact: This non-rigorous argument is based on the assertion that the divergence of the function is , where is the delta function on . That claim is typically justified by non-rigorous arguments. Bonus question: Where would I look to find a rigorous proof of this or similar physics equations? Is there a book on electromagnetism written for mathematicians? A similar question was asked on the physics stackexchange, but it hasn't received a good answer. Since the question is about providing a rigorous proof I think it's a better fit for math.stackexchange. Edit: I think a key insight I was missing is that proving (1) is equivalent to showing that The function is called the electric potential in electrostatics. This fact (2) is fundamental to the study of the Laplace equation and Poisson's equation, and it is of course proved rigorously in PDEs textbooks such as Evans. So, it is not that there is a gap in the mathematical literature -- rather, if you want to find a rigorous proof of (1), you just need to recognize that it's equivalent to (2). Why does (2) imply (1)? First notice that if and then for all . It follows that So, Thus, if we can show that , then we will have shown that . By the way, it is still not clear to me how to prove (3) rigorously. I think we need to use the dominated convergence theorem or one of the theorems from measure theory which imply that, under certain conditions, the limit of the integral is the integral of the limit.","\rho: \mathbb R^3 \to \mathbb R x \in \mathbb R^3 
E(x) = \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \frac{x - y}{\|x - y \|^3} \, dy.
 E 
\tag{1} \nabla \cdot E = \frac{\rho}{\epsilon_0} .
 \begin{align}
\nabla \cdot E(x) &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \, \nabla \cdot \left(\frac{x - y}{\|x - y \|^3} \right) \, dy \\
&= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \, 4 \pi \,\delta(x - y) \, dy \\
&= \frac{\rho(x)}{\epsilon_0}.
\end{align} x \mapsto \frac{x}{\|x\|^3} 4 \pi \delta \delta \mathbb R^3 
\tag{2} \text{if} \quad \varphi(x) = -\frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \| x - y \|^{-1} \, dy \quad \text{then}\quad  
\nabla^2 \varphi = \frac{\rho}{\epsilon_0}.
 -\varphi y \in \mathbb R^3 h(x) = -\|x - y \|^{-1} \nabla h(x) = \frac{x - y}{\|x - y \|^3} x \in \mathbb R^3, x \neq y \begin{align}
\tag{3} \nabla \varphi(x) &= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \nabla \left( -\| x - y \|^{-1} \right) \, dy \\
&= \frac{1}{4 \pi \epsilon_0} \int_{\mathbb R^3} \rho(y) \frac{x - y}{\|x - y \|^3} \, dy \\
&= E(x).
\end{align} 
\nabla^2 \varphi(x) = \nabla \cdot \nabla \varphi(x) = \nabla \cdot E(x).
 \nabla^2 \varphi = \frac{\rho}{\epsilon_0} \nabla \cdot E = \frac{\rho}{\epsilon_0}","['real-analysis', 'electromagnetism']"
81,What is the motivation behind the steps in this 'simple' proof that $\pi$ is irrational?,What is the motivation behind the steps in this 'simple' proof that  is irrational?,\pi,"In $1947$ , Ivan Niven published A Simple Proof that $\pi$ is irrational , which only requires knowledge of elementary calculus to understand. Since then, many variations of this proof have been published, one of which I will briefly outline here: Assume, for the sake of contradiction, that $\pi$ can be expressed in the form $a/b$ , where $a$ and $b$ are integers. Let $f$ be the polynomial function defined by $$ f(x)=\frac{x^n(a-bx)^n}{n!} \, , $$ where $n$ is a positive integer. Consider the integral $I=\int_{0}^{\pi} f(x)\sin x \, dx$ . Since $f$ and $\sin$ are positive on the interval $(0,\pi)$ , we have $0<I$ . Moreover, if $n$ is large enough, then $I<1$ . This is because $$ f(x)\sin x =\frac{x^n(a-bx)^n}{n!}\sin x<\frac{x^n\pi^n}{n!} $$ and $$\int_{0}^{\pi}\frac{x^n\pi^n}{n!} \, dx <1$$ for sufficiently large $n$ . On the other hand, integration by parts tells us that \begin{align} \int f(x) \sin x \, dx = &-f(x)\cos x +f'(x)\sin x + f''(x) \cos x - f'''(x)\sin x \\ &-f''''(x)\cos x +\ldots\pm f^{(2n)}(x)\cos x \end{align} Every function $f^{(k)}$ (with $0 \leq k\leq 2n$ ) takes integer values when $x=0$ and $x=\pi$ . The same is true for $\sin$ and $\cos$ . Hence, $I$ must be an integer. But earlier we showed that if $n$ is sufficiently large, $0<I<1$ , thereby reaching a contradiction. $\blacksquare$ Although the steps in the proof are not too difficult to follow, it still leaves me scratching my head thinking why on earth someone would try to prove $\pi$ is irrational by defining a bizzarre-looking function and then proceeding to integrate it. The proof seems entirely unmotivated. Is there any insight into how someone could have thought of such a proof? Is there something about the function $f$ and the trigonometric functions that provide us with information about the irrationality of $\pi$ ?","In , Ivan Niven published A Simple Proof that is irrational , which only requires knowledge of elementary calculus to understand. Since then, many variations of this proof have been published, one of which I will briefly outline here: Assume, for the sake of contradiction, that can be expressed in the form , where and are integers. Let be the polynomial function defined by where is a positive integer. Consider the integral . Since and are positive on the interval , we have . Moreover, if is large enough, then . This is because and for sufficiently large . On the other hand, integration by parts tells us that Every function (with ) takes integer values when and . The same is true for and . Hence, must be an integer. But earlier we showed that if is sufficiently large, , thereby reaching a contradiction. Although the steps in the proof are not too difficult to follow, it still leaves me scratching my head thinking why on earth someone would try to prove is irrational by defining a bizzarre-looking function and then proceeding to integrate it. The proof seems entirely unmotivated. Is there any insight into how someone could have thought of such a proof? Is there something about the function and the trigonometric functions that provide us with information about the irrationality of ?","1947 \pi \pi a/b a b f 
f(x)=\frac{x^n(a-bx)^n}{n!} \, ,
 n I=\int_{0}^{\pi} f(x)\sin x \, dx f \sin (0,\pi) 0<I n I<1 
f(x)\sin x =\frac{x^n(a-bx)^n}{n!}\sin x<\frac{x^n\pi^n}{n!}
 \int_{0}^{\pi}\frac{x^n\pi^n}{n!} \, dx <1 n \begin{align}
\int f(x) \sin x \, dx = &-f(x)\cos x +f'(x)\sin x + f''(x) \cos x - f'''(x)\sin x \\ &-f''''(x)\cos x +\ldots\pm f^{(2n)}(x)\cos x
\end{align} f^{(k)} 0 \leq k\leq 2n x=0 x=\pi \sin \cos I n 0<I<1 \blacksquare \pi f \pi","['real-analysis', 'calculus', 'proof-explanation', 'irrational-numbers', 'pi']"
82,Number of roots of the equation $f(xf(x)) = \sqrt{9 - x^2f^2(x)}$,Number of roots of the equation,f(xf(x)) = \sqrt{9 - x^2f^2(x)},"Given the following graph of a $5$ th degree polynomial $y = f(x)$ : Find the number of solutions of the equation: $$f(xf(x)) = \sqrt{9 - x^2f^2(x)}$$ My attempt was to figure out what $f(x)$ is, by noticing the roots of its derivative based on the graph to know $f'(x)$ , then find $\int f'(x)$ , but the function I found did not match the original graph. (Even if I founded, I think it would be very messy to plug it in the equation) One other idea is that let $t = xf(x)$ , then the equation become: $$f(t) = \sqrt{9-t^2}$$ But I can't process from here. Is my idea correct? If yes, then how do I continue? Or is there a better way to solve this?","Given the following graph of a th degree polynomial : Find the number of solutions of the equation: My attempt was to figure out what is, by noticing the roots of its derivative based on the graph to know , then find , but the function I found did not match the original graph. (Even if I founded, I think it would be very messy to plug it in the equation) One other idea is that let , then the equation become: But I can't process from here. Is my idea correct? If yes, then how do I continue? Or is there a better way to solve this?",5 y = f(x) f(xf(x)) = \sqrt{9 - x^2f^2(x)} f(x) f'(x) \int f'(x) t = xf(x) f(t) = \sqrt{9-t^2},"['real-analysis', 'calculus', 'functions']"
83,The real function $f$ such that $\log \cdots \log (f)$ is strictly convex on its domain for any number of $\log$'s,The real function  such that  is strictly convex on its domain for any number of 's,f \log \cdots \log (f) \log,"Does there exist a function $f: (a,b) \to \mathbb R$ ( $a,b$ are allowed to be infinity) such that $\log \cdots \log (f)$ is strictly convex on its whole domain of definition for an arbitrary number (though finitely many) of $\log$ 's? If such function exists, it must be very very convex (""more convex"" than any $\exp \dots \exp (x) $ , which will become concave after a finitely many $\log$ 's applied on) The form of $f$ doesn't have to be concrete. It can be infinite series or one can even show its existence or nonexistence.","Does there exist a function ( are allowed to be infinity) such that is strictly convex on its whole domain of definition for an arbitrary number (though finitely many) of 's? If such function exists, it must be very very convex (""more convex"" than any , which will become concave after a finitely many 's applied on) The form of doesn't have to be concrete. It can be infinite series or one can even show its existence or nonexistence.","f: (a,b) \to \mathbb R a,b \log \cdots \log (f) \log \exp \dots \exp (x)  \log f","['real-analysis', 'functions', 'logarithms', 'convex-analysis', 'iterated-function-system']"
84,Is there $k\in\mathbb{R}-\mathbb{Z}$ such that $2^k\in\mathbb{N}$ and $3^k\in\mathbb{N}?$,Is there  such that  and,k\in\mathbb{R}-\mathbb{Z} 2^k\in\mathbb{N} 3^k\in\mathbb{N}?,Is there $k\in\mathbb{R}-\mathbb{Z}$ such that $2^k\in\mathbb{N}$ and $3^k\in\mathbb{N}?$ My guess is no such $k$ exists but seems hard to prove. Any ideas? Thanks in advance,Is there such that and My guess is no such exists but seems hard to prove. Any ideas? Thanks in advance,k\in\mathbb{R}-\mathbb{Z} 2^k\in\mathbb{N} 3^k\in\mathbb{N}? k,"['real-analysis', 'logarithms']"
85,Epsilon-delta proof for $\lim_{x \rightarrow a} \frac{x^n - a^n}{x -a}$,Epsilon-delta proof for,\lim_{x \rightarrow a} \frac{x^n - a^n}{x -a},"I'm having trouble finding the right way to approach this limit. $$ \left| \frac{x^n - a^n}{x-a} - na^{n-1} \right| < \varepsilon, \text{ given } |x-a| < \delta $$ I've tried rewriting $\frac{x^n - a^n}{x-a}$ as $\sum_{k=0}^n x^ka^{n-k}$ , but that made it hard to reintroduce a $\delta$ into the inequality. I've also tried assuming $\delta < 1$ and rewritting the numerator as $(a+1)^n - a^n$ , but similarly ran into problems with a disappearing delta. Can I please have a nudge in the right direction?","I'm having trouble finding the right way to approach this limit. I've tried rewriting as , but that made it hard to reintroduce a into the inequality. I've also tried assuming and rewritting the numerator as , but similarly ran into problems with a disappearing delta. Can I please have a nudge in the right direction?"," \left| \frac{x^n - a^n}{x-a} - na^{n-1} \right| < \varepsilon, \text{ given } |x-a| < \delta  \frac{x^n - a^n}{x-a} \sum_{k=0}^n x^ka^{n-k} \delta \delta < 1 (a+1)^n - a^n","['real-analysis', 'limits']"
86,"Is there a bijective function $f:[0,1] \to [0,1]$ such that the graph of $f$ in $\mathbb{R}^2$ is a dense subset of $[0,1] \times [0,1]$?",Is there a bijective function  such that the graph of  in  is a dense subset of ?,"f:[0,1] \to [0,1] f \mathbb{R}^2 [0,1] \times [0,1]","Is there a bijective function $f:[0,1] \to [0,1]$ such that the graph of $f$ in $\mathbb{R}^2$ is a dense subset of $[0,1] \times [0,1]$ ? (Exact same as title). I think the question is not affected much if we ask the same question but for a function $f:(0,1) \to [0,1]$ or $f:[0,1) \to (0,1]$ etc, as opposed to $f:[0,1] \to [0,1]$ , which was in the original question. All that really matters is that the domain and range are bounded, connected subsets of $\mathbb{R}^2$ . I suspect the answer to the question is yes, but I don't know how to construct such a function. The first thing to note is that, if such a function exists it must be nowhere continuous, else the graph of f would not be dense throughout all of $[0,1] \times [0,1]$ . However, it is not clear if the graph of our function would be a totally disconnected subset of $[0,1] \times [0,1]$ . Can a nowhere continuous function have a connected graph? I have actually not read the answers to the above question in any detail, and anyway, it may not be relevant to answer the question here (although it might). My attempt: Let $f_{ Conway_{(0,1)} }:(0,1) \to \mathbb{R} $ be the Conway base-13 function , but with domain restricted to $(0,1)$ . Now define $f_{Conway_{(0,1)}bounded}(x) = \frac{1}{\pi} \arctan(f_{ Conway_{(0,1)} }(x)) + \frac{1}{2}$ with domain $(0,1)$ and range $(0,1)$ . Then the function is well-defined, and the graph of $f_{Conway_{(0,1)}bounded}:(0,1) \to (0,1)$ is a dense subset of $[0,1] \times [0,1]$ . Now we can easily modify our function $f_{Conway_{(0,1)}bounded}$ so that it has domain $[0,1]$ and range $[0,1]$ , and I will assume the reader can do this and leave the details for brevity. But the point is, these missing two points in the domain, $0$ and $1$ , are not a problem. The problem is that our function is not injective. Note that we cannot answer the question by only removing points from the graph of $f_{Conway_{(0,1)}bounded}$ , for then you would be removing lots of points from the domain, and so this would not be a function with domain $(0,1)$ . So maybe doing something clever to $f_{Conway_{(0,1)}bounded}$ , or perhaps coming up with an entirely different way to construct a function to answer the question is necessary.","Is there a bijective function such that the graph of in is a dense subset of ? (Exact same as title). I think the question is not affected much if we ask the same question but for a function or etc, as opposed to , which was in the original question. All that really matters is that the domain and range are bounded, connected subsets of . I suspect the answer to the question is yes, but I don't know how to construct such a function. The first thing to note is that, if such a function exists it must be nowhere continuous, else the graph of f would not be dense throughout all of . However, it is not clear if the graph of our function would be a totally disconnected subset of . Can a nowhere continuous function have a connected graph? I have actually not read the answers to the above question in any detail, and anyway, it may not be relevant to answer the question here (although it might). My attempt: Let be the Conway base-13 function , but with domain restricted to . Now define with domain and range . Then the function is well-defined, and the graph of is a dense subset of . Now we can easily modify our function so that it has domain and range , and I will assume the reader can do this and leave the details for brevity. But the point is, these missing two points in the domain, and , are not a problem. The problem is that our function is not injective. Note that we cannot answer the question by only removing points from the graph of , for then you would be removing lots of points from the domain, and so this would not be a function with domain . So maybe doing something clever to , or perhaps coming up with an entirely different way to construct a function to answer the question is necessary.","f:[0,1] \to [0,1] f \mathbb{R}^2 [0,1] \times [0,1] f:(0,1) \to [0,1] f:[0,1) \to (0,1] f:[0,1] \to [0,1] \mathbb{R}^2 [0,1] \times [0,1] [0,1] \times [0,1] f_{ Conway_{(0,1)} }:(0,1) \to \mathbb{R}  (0,1) f_{Conway_{(0,1)}bounded}(x) = \frac{1}{\pi} \arctan(f_{ Conway_{(0,1)} }(x)) + \frac{1}{2} (0,1) (0,1) f_{Conway_{(0,1)}bounded}:(0,1) \to (0,1) [0,1] \times [0,1] f_{Conway_{(0,1)}bounded} [0,1] [0,1] 0 1 f_{Conway_{(0,1)}bounded} (0,1) f_{Conway_{(0,1)}bounded}","['real-analysis', 'general-topology']"
87,Computing $\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}$,Computing,\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}},"I managed to find $$\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)-8\underbrace{\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx}_{I}\tag1$$ but I could not finish $I$ : My attempt: In the book, Almost Impossible Integrals, Sums and series , page $243$ ,  Eq $(3.281)$ we have $$\tan x\ln(\sin x)=-\sum_{n=1}^\infty\left(\psi\left(\frac{n+1}{2}\right)-\psi\left(\frac{n}{2}\right)-\frac1n\right)\sin(2nx)$$ $$=-\sum_{n=1}^\infty\left(\int_0^1\frac{1-t}{1+t}t^{n-1}dt\right)\sin(2nx),\quad 0<x<\frac{\pi}{2}$$ So $$I=-\sum_{n=1}^\infty\left(\int_0^1\frac{1-t}{1+t}t^{n-1}dt\right)\underbrace{\left(\int_0^{\pi/2}x^2\sin(2nx)\ln(\sin x)dx\right)}_{J}$$ For $J$ , we use the Fourier series of $\ln(\sin x)=-\ln(2)-\sum_{k=1}^\infty\frac{\cos(2kx)}{k}$ $$J=-\ln(2)\underbrace{\int_0^{\pi/2}x^2\sin(2nx)dx}_{J_1}-\sum_{k=1}^\infty \frac1k\underbrace{\int_0^{\pi/2}x^2\sin(2nx)\cos(2kx)dx}_{J_2}$$ $$J_1=\frac{\cos(n\pi)}{4n^3}-\frac{1}{4n^3}-\frac{3\zeta(2)\cos(n\pi)}{4n}+\frac{\pi\sin(n\pi)}{4n^2}$$ $$=\frac{(-1)^n}{4n^3}-\frac{1}{4n^3}-\frac{3\zeta(2)(-1)^n}{4n}$$ The last result follows from $\cos(n\pi)=(-1)^n$ and $\sin(n\pi)=0$ for $n=1,2,3,..$ $$J_2=\frac18\left(\frac{1}{(k-n)^3}-\frac{\pi\sin(\pi(k-n))}{(k-n)^3}-\frac{(1-3(k-n)^2\zeta(2))\cos(\pi(k-n))}{(k-n)^3}\right)$$ $$-\frac18\left(\frac{1}{(k+n)^3}-\frac{\pi\sin(\pi(k+n))}{(k+n)^3}-\frac{(1-3(k+n)^2\zeta(2))\cos(\pi(k+n))}{(k+n)^3}\right)$$ I stopped here as I can not simplify $J_2$ . But I think it can be simplified considering $n,k\in\mathbb{Z}>0$ . Any idea? Also do you have a different path? Thank you. Addendum I also found that $$\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=\int_0^1\frac{\text{Li}_4(x)-\ln(1-x)\text{Li}_3(x)-\frac12\text{Li}_2^2(x)}{x\sqrt{1-x}}dx$$ $$=\int_0^{\pi/2}\frac{2\text{Li}_4(\sin^2x)-4\ln(\cos x)\text{Li}_3(\sin^2x)-\text{Li}_2^2(\sin^2x)}{\sin x}dx$$ Proof of $(1)$ : Differentiating both sides of $\int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n}$ with respect to $n$ gives $$\int_0^1 x^{n-1}\ln x\ln(1-x)dx=\frac{H_n}{n^2}+\frac{H_n^{(2)}}{n}-\frac{\zeta(2)}{n}$$ multiply both sides by $\frac{4^n}{n^2{2n\choose n}}$ then $\sum_{n=1}^\infty$ we get $$\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(2)}}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(2)4^n}{n^3{2n\choose n}}=\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\sum_{n=1}^\infty\frac{(4x)^n}{n^2{2n\choose n}}\right)dx$$ $$=\int_0^1\frac{\ln x\ln(1-x)}{x}\left(2\arcsin^2(\sqrt{x})\right)dx$$ $$\overset{\sqrt{x}=\sin\theta}{=}16\int_0^{\pi/2}x^2\cot x\ln(\sin x)\ln(\cos x)dx\tag1$$ Next, differentiate both sides of $\int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n}$ with respect to $n$ twice we get $$-\frac12\int_0^1 x^{n-1}\ln^2x\ln(1-x)dx=\frac{H_n}{n^3}+\frac{H_n^{(2)}}{n^2}+\frac{H_n^{(3)}}{n}-\frac{\zeta(2)}{n^2}-\frac{\zeta(3)}{n}$$ multiply both sides by $\frac{4^n}{n{2n\choose n}}$ then $\sum_{n=1}^\infty$ we have $$\sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(2)}}{n^3{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(2)4^n}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(3)4^n}{n^2{2n\choose n}}$$ $$=-\frac12\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\sum_{n=1}^\infty\frac{(4x)^n}{n{2n\choose n}}\right)dx$$ $$=-\frac12\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\frac{2\sqrt{x}\arcsin(\sqrt{x})}{\sqrt{1-x}}\right)dx$$ $$\overset{\sqrt{x}=\sin\theta}{=}-16\int_0^{\pi/2}x\ln(\cos x)\ln^2(\sin x)dx$$ $$\overset{IBP}{=}16\int_0^{\pi/2}x^2\cot x\ln(\sin x)\ln(\cos x)dx-8\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx\tag2$$ Subtracting $(2)$ from $(1)$ and using $\sum_{n=1}^\infty\frac{\zeta(3)4^n}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)$ yields $$\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)-8\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx$$","I managed to find but I could not finish : My attempt: In the book, Almost Impossible Integrals, Sums and series , page ,  Eq we have So For , we use the Fourier series of The last result follows from and for I stopped here as I can not simplify . But I think it can be simplified considering . Any idea? Also do you have a different path? Thank you. Addendum I also found that Proof of : Differentiating both sides of with respect to gives multiply both sides by then we get Next, differentiate both sides of with respect to twice we get multiply both sides by then we have Subtracting from and using yields","\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)-8\underbrace{\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx}_{I}\tag1 I 243 (3.281) \tan x\ln(\sin x)=-\sum_{n=1}^\infty\left(\psi\left(\frac{n+1}{2}\right)-\psi\left(\frac{n}{2}\right)-\frac1n\right)\sin(2nx) =-\sum_{n=1}^\infty\left(\int_0^1\frac{1-t}{1+t}t^{n-1}dt\right)\sin(2nx),\quad 0<x<\frac{\pi}{2} I=-\sum_{n=1}^\infty\left(\int_0^1\frac{1-t}{1+t}t^{n-1}dt\right)\underbrace{\left(\int_0^{\pi/2}x^2\sin(2nx)\ln(\sin x)dx\right)}_{J} J \ln(\sin x)=-\ln(2)-\sum_{k=1}^\infty\frac{\cos(2kx)}{k} J=-\ln(2)\underbrace{\int_0^{\pi/2}x^2\sin(2nx)dx}_{J_1}-\sum_{k=1}^\infty \frac1k\underbrace{\int_0^{\pi/2}x^2\sin(2nx)\cos(2kx)dx}_{J_2} J_1=\frac{\cos(n\pi)}{4n^3}-\frac{1}{4n^3}-\frac{3\zeta(2)\cos(n\pi)}{4n}+\frac{\pi\sin(n\pi)}{4n^2} =\frac{(-1)^n}{4n^3}-\frac{1}{4n^3}-\frac{3\zeta(2)(-1)^n}{4n} \cos(n\pi)=(-1)^n \sin(n\pi)=0 n=1,2,3,.. J_2=\frac18\left(\frac{1}{(k-n)^3}-\frac{\pi\sin(\pi(k-n))}{(k-n)^3}-\frac{(1-3(k-n)^2\zeta(2))\cos(\pi(k-n))}{(k-n)^3}\right) -\frac18\left(\frac{1}{(k+n)^3}-\frac{\pi\sin(\pi(k+n))}{(k+n)^3}-\frac{(1-3(k+n)^2\zeta(2))\cos(\pi(k+n))}{(k+n)^3}\right) J_2 n,k\in\mathbb{Z}>0 \sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=\int_0^1\frac{\text{Li}_4(x)-\ln(1-x)\text{Li}_3(x)-\frac12\text{Li}_2^2(x)}{x\sqrt{1-x}}dx =\int_0^{\pi/2}\frac{2\text{Li}_4(\sin^2x)-4\ln(\cos x)\text{Li}_3(\sin^2x)-\text{Li}_2^2(\sin^2x)}{\sin x}dx (1) \int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n} n \int_0^1 x^{n-1}\ln x\ln(1-x)dx=\frac{H_n}{n^2}+\frac{H_n^{(2)}}{n}-\frac{\zeta(2)}{n} \frac{4^n}{n^2{2n\choose n}} \sum_{n=1}^\infty \sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(2)}}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(2)4^n}{n^3{2n\choose n}}=\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\sum_{n=1}^\infty\frac{(4x)^n}{n^2{2n\choose n}}\right)dx =\int_0^1\frac{\ln x\ln(1-x)}{x}\left(2\arcsin^2(\sqrt{x})\right)dx \overset{\sqrt{x}=\sin\theta}{=}16\int_0^{\pi/2}x^2\cot x\ln(\sin x)\ln(\cos x)dx\tag1 \int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n} n -\frac12\int_0^1 x^{n-1}\ln^2x\ln(1-x)dx=\frac{H_n}{n^3}+\frac{H_n^{(2)}}{n^2}+\frac{H_n^{(3)}}{n}-\frac{\zeta(2)}{n^2}-\frac{\zeta(3)}{n} \frac{4^n}{n{2n\choose n}} \sum_{n=1}^\infty \sum_{n=1}^\infty\frac{4^nH_n}{n^4{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(2)}}{n^3{2n\choose n}}+\sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(2)4^n}{n^3{2n\choose n}}-\sum_{n=1}^\infty\frac{\zeta(3)4^n}{n^2{2n\choose n}} =-\frac12\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\sum_{n=1}^\infty\frac{(4x)^n}{n{2n\choose n}}\right)dx =-\frac12\int_0^1\frac{\ln x\ln(1-x)}{x}\left(\frac{2\sqrt{x}\arcsin(\sqrt{x})}{\sqrt{1-x}}\right)dx \overset{\sqrt{x}=\sin\theta}{=}-16\int_0^{\pi/2}x\ln(\cos x)\ln^2(\sin x)dx \overset{IBP}{=}16\int_0^{\pi/2}x^2\cot x\ln(\sin x)\ln(\cos x)dx-8\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx\tag2 (2) (1) \sum_{n=1}^\infty\frac{\zeta(3)4^n}{n^2{2n\choose n}}=3\zeta(2)\zeta(3) \sum_{n=1}^\infty\frac{4^nH_n^{(3)}}{n^2{2n\choose n}}=3\zeta(2)\zeta(3)-8\int_0^{\pi/2}x^2\tan x\ln^2(\sin x)dx","['real-analysis', 'integration', 'sequences-and-series', 'binomial-coefficients', 'harmonic-numbers']"
88,Is the supremum of a continuous function bounded?,Is the supremum of a continuous function bounded?,,"Let $f:\mathbb{R}\times[a,b]\to \mathbb{R}$ be a continuous function with $a<b$ , such that for every $y\in [a,b]$ we have that $$\sup_x f(x,y)\in \mathbb{R}$$ Does that mean that $$\sup_{y\in [a,b]}\sup_{x\in \mathbb{R}}f(x,y)\in \mathbb{R}$$ I believe it is not true, but I don't know how to prove it.","Let be a continuous function with , such that for every we have that Does that mean that I believe it is not true, but I don't know how to prove it.","f:\mathbb{R}\times[a,b]\to \mathbb{R} a<b y\in [a,b] \sup_x f(x,y)\in \mathbb{R} \sup_{y\in [a,b]}\sup_{x\in \mathbb{R}}f(x,y)\in \mathbb{R}","['real-analysis', 'calculus', 'multivariable-calculus', 'continuity', 'supremum-and-infimum']"
89,Mean value theorem for Improper Integrals,Mean value theorem for Improper Integrals,,"If $f$ is continuous on $[a,\infty)$ and $\phi(x)\geq0$ and be integrable in $[a,\infty) $ then there exists some $c\in(a,\infty)$ such that $$\int_{a}^{\infty} f(x)\phi(x) dx = f(c)\int_{a}^{\infty}\phi(x) dx$$ I tried $\int_{a}^{\infty} f(x)\phi(x) dx = \lim_{n\to\infty}   \int_{a}^{n} f(x)\phi(x)dx$ . Can we apply mean value theorem for proper integrals to the latter integral and then take limit?",If is continuous on and and be integrable in then there exists some such that I tried . Can we apply mean value theorem for proper integrals to the latter integral and then take limit?,"f [a,\infty) \phi(x)\geq0 [a,\infty)  c\in(a,\infty) \int_{a}^{\infty} f(x)\phi(x) dx = f(c)\int_{a}^{\infty}\phi(x) dx \int_{a}^{\infty} f(x)\phi(x) dx = \lim_{n\to\infty}   \int_{a}^{n} f(x)\phi(x)dx","['real-analysis', 'calculus']"
90,"If $f$ is uniformly continuous on two open sets with a non-empty intersection, then $f$ is uniformly continuous on their union","If  is uniformly continuous on two open sets with a non-empty intersection, then  is uniformly continuous on their union",f f,"There is a problem when I am solving this question:- Suppose $a<b<c<d$ . Prove that if $f$ is uniformly continuous on $(a,b)$ and on $(c,d)$ then $f$ is uniformly continuous on $(a,b)\cup(c,d)$ . I solve the question like this: $\forall \epsilon>0$ . As $f$ is uniformly continuous on A, then $\exists\delta_1>0\ni\forall x,y\in (a,b),|x-y|<\delta_1\implies|f(x)-f(y)|<\epsilon$ Also, $f$ is uniformly continuous on (c,d), then $\exists\delta_2>0\ni\forall x,y\in (c,d),|x-y|<\delta_2\implies|f(x)-f(y)|<\epsilon$ Take $\delta$ = $\min(\delta_1,\delta_2),\forall x,y\in(a,b)\cup(c,d),|x-y|<\delta\implies |f(x)-f(y)|<\epsilon$ . But when I see the solution, it is given as Take $\delta$ = $\min(\delta_1,\delta_2, c-b)$ .   Then $\forall x,y \in (a,b)\cup(c,d)$ , $|x-y|<\delta \implies x,y \in (a,b) \text{ or } x,y \in (c,d),\text{ and } |x-y|<\delta_1\text{ and }\delta_2$ $\implies|f(x)-f(y)|<\epsilon$ $\therefore f$ is uniformly continuous on $(a,b)\cup (c,d)$ . But why they take $c-b$ in the expression of $\delta$ ? And how does it guarantee that $x,y\in(a,b)$ or $(c,d)$ not something like $x\in(a,b)$ and $y \in (c,d)$ or vice versa? Why we can't take $x\in(a,b)$ and $y\in(c,d)$ to prove uniform continuity on $(a,b)\cup(c,d)$ ?","There is a problem when I am solving this question:- Suppose . Prove that if is uniformly continuous on and on then is uniformly continuous on . I solve the question like this: . As is uniformly continuous on A, then Also, is uniformly continuous on (c,d), then Take = . But when I see the solution, it is given as Take = .   Then , is uniformly continuous on . But why they take in the expression of ? And how does it guarantee that or not something like and or vice versa? Why we can't take and to prove uniform continuity on ?","a<b<c<d f (a,b) (c,d) f (a,b)\cup(c,d) \forall \epsilon>0 f \exists\delta_1>0\ni\forall x,y\in (a,b),|x-y|<\delta_1\implies|f(x)-f(y)|<\epsilon f \exists\delta_2>0\ni\forall x,y\in (c,d),|x-y|<\delta_2\implies|f(x)-f(y)|<\epsilon \delta \min(\delta_1,\delta_2),\forall
x,y\in(a,b)\cup(c,d),|x-y|<\delta\implies |f(x)-f(y)|<\epsilon \delta \min(\delta_1,\delta_2, c-b) \forall x,y \in (a,b)\cup(c,d) |x-y|<\delta \implies x,y \in (a,b) \text{ or } x,y \in (c,d),\text{ and } |x-y|<\delta_1\text{ and }\delta_2 \implies|f(x)-f(y)|<\epsilon \therefore f (a,b)\cup (c,d) c-b \delta x,y\in(a,b) (c,d) x\in(a,b) y \in (c,d) x\in(a,b) y\in(c,d) (a,b)\cup(c,d)","['real-analysis', 'proof-explanation', 'uniform-continuity']"
91,Proving the inequality $\prod_{n=1}^\infty \left( 1+\frac1{n^2+\ln n} \right) < \frac72$,Proving the inequality,\prod_{n=1}^\infty \left( 1+\frac1{n^2+\ln n} \right) < \frac72,"I am struggling to prove $$\prod_{n=1}^k \left( 1+\frac1{n^2+\ln n} \right) < \frac72$$ For all $k \geq 1$ . Clearly, this is true for $k=1$ , so it suffices to show $$\prod_{n=1}^\infty \left( 1+\frac1{n^2+\ln n} \right) < \frac72$$ The obvious way to show this would be to evaluate the product directly, though I don't think that is feasible. I am not sure how else to approach this product. This would be a ( much ) tighter upper bound than $$\exp\left ({\sum_{n=1}^k} \frac1{n^2+\ln n} \right )$$ Which comes from the monotone convergence theorem.","I am struggling to prove For all . Clearly, this is true for , so it suffices to show The obvious way to show this would be to evaluate the product directly, though I don't think that is feasible. I am not sure how else to approach this product. This would be a ( much ) tighter upper bound than Which comes from the monotone convergence theorem.",\prod_{n=1}^k \left( 1+\frac1{n^2+\ln n} \right) < \frac72 k \geq 1 k=1 \prod_{n=1}^\infty \left( 1+\frac1{n^2+\ln n} \right) < \frac72 \exp\left ({\sum_{n=1}^k} \frac1{n^2+\ln n} \right ),"['real-analysis', 'sequences-and-series', 'inequality', 'convergence-divergence', 'infinite-product']"
92,Prove that $A\cup B$ is connected. [duplicate],Prove that  is connected. [duplicate],A\cup B,"This question already has an answer here : If $\overline{A} \cap B \neq \emptyset, E = A \cup B$, prove that E is connected (1 answer) Closed 2 years ago . Let $A$ , $B$ $\subseteq \mathbb{R^n}$ be connected sets and suppose that $\overline{A} \cap B \ne \emptyset$ . Prove that $A\cup B$ is   connected. My attempt: I've tried a proof by contradiction. Suppose that $A\cup B$ is disconnected, e.g. $A\cup B = X\cup Y$ , where $X, Y$ are disjoint, nonempty, and open in $A\cup B$ . Besides that, we have $A\cap X$ and $Y\cap A$ open in $A$ , and that they cover A. But since A is connected by hypotesis, then we can suppose that $A\cap X= \emptyset$ . And that implies that $A \subseteq Y$ . So far, so good... but where do I get the contradiction? Where do I use that $\overline{A} \cap B \ne \emptyset$ ? Any help to finish this proof would be appreciated!","This question already has an answer here : If $\overline{A} \cap B \neq \emptyset, E = A \cup B$, prove that E is connected (1 answer) Closed 2 years ago . Let , be connected sets and suppose that . Prove that is   connected. My attempt: I've tried a proof by contradiction. Suppose that is disconnected, e.g. , where are disjoint, nonempty, and open in . Besides that, we have and open in , and that they cover A. But since A is connected by hypotesis, then we can suppose that . And that implies that . So far, so good... but where do I get the contradiction? Where do I use that ? Any help to finish this proof would be appreciated!","A B \subseteq \mathbb{R^n} \overline{A} \cap B \ne \emptyset A\cup B A\cup B A\cup B = X\cup Y X, Y A\cup B A\cap X Y\cap A A A\cap X= \emptyset A \subseteq Y \overline{A} \cap B \ne \emptyset","['real-analysis', 'general-topology', 'connectedness']"
93,Measurable functions with arbitrarily small periods are constant,Measurable functions with arbitrarily small periods are constant,,"Let $f:\mathbb{R}\to \mathbb{R}$ be a measurable function that has arbitrarily small periods, that is, for any $\varepsilon>0$ there is $0<T<\varepsilon$ such that $f(x+T)=f(x)$ for all $x\in\mathbb{R}$ . I need to show that $f$ is constant, at least almost everywhere. Can someone find an elementary proof of this ? One proof : I found a proof in which I used distribution theory. First replace $f$ by $f_A:= f\mathbf{1}_{\vert f\vert\leq A}$ for $A>0$ to get a bounded function. Therefore, $f$ belongs to $L_{loc}^1(\mathbb{R})$ and can be seen as an element of $\mathcal{D}'(\mathbb{R})$ . Take $T$ a period of $f$ and denote by $\tau_Tf:= f(\cdot-T)$ the translation operator by $T$ . Take $\varphi$ as a test function, and compute \begin{aligned} \left<f, \varphi \right> &=\left<\tau_T f, \varphi \right> \\ &= \left< f,\tau_{-T} \varphi \right> \end{aligned} Thus, for $T$ arbitrarily small $$\left<f,\tau_T \varphi - \varphi \right> = 0.$$ Then, one uses the fact that if $\underset{n\to\infty}{\lim}T_n =0$ , then $$\underset{T_n\to0}{\lim} \frac{\tau_{T_n}\varphi - \varphi}{T_n} = \varphi'\quad \text{in}\ \mathcal{D(\mathbb{R})}.$$ Passing to the limit in the above inequality one gets that $$\left<f', \varphi \right> = 0 $$ for any test function $\varphi$ . Thus $$f' = 0\quad\text{in}\ \mathcal{D'}(\mathbb{R}).$$ By a classical result, this implies that $f$ is constant almost everywhere. Motivation: In a paper called ""On the approximation of Lebesgue integrals by Riemann sums"", Jessen proves the following theorem. Let $f$ be a $L^1(\mathbf{T})$ function. Denote its Riemann sum $$f_n:= \frac{1}{n}\sum_{k=1}^n f(x+\frac{k}{n}).$$ Then $$\underset{n\to\infty}{\lim} f_{2^n}(x) = \int_0^1 f\quad a.e.x $$ Jessen considers $$\phi(x):=\overline{\lim} f_{2^n}(x)$$ which is a $2^{-n}$ periodic function for all $n$ , hence an almost everywhere constant function. The proof reduces to show that this constant is $\int_0^1 f$ .","Let be a measurable function that has arbitrarily small periods, that is, for any there is such that for all . I need to show that is constant, at least almost everywhere. Can someone find an elementary proof of this ? One proof : I found a proof in which I used distribution theory. First replace by for to get a bounded function. Therefore, belongs to and can be seen as an element of . Take a period of and denote by the translation operator by . Take as a test function, and compute Thus, for arbitrarily small Then, one uses the fact that if , then Passing to the limit in the above inequality one gets that for any test function . Thus By a classical result, this implies that is constant almost everywhere. Motivation: In a paper called ""On the approximation of Lebesgue integrals by Riemann sums"", Jessen proves the following theorem. Let be a function. Denote its Riemann sum Then Jessen considers which is a periodic function for all , hence an almost everywhere constant function. The proof reduces to show that this constant is .","f:\mathbb{R}\to \mathbb{R} \varepsilon>0 0<T<\varepsilon f(x+T)=f(x) x\in\mathbb{R} f f f_A:= f\mathbf{1}_{\vert f\vert\leq A} A>0 f L_{loc}^1(\mathbb{R}) \mathcal{D}'(\mathbb{R}) T f \tau_Tf:= f(\cdot-T) T \varphi \begin{aligned}
\left<f, \varphi \right> &=\left<\tau_T f, \varphi \right> \\ &= \left< f,\tau_{-T} \varphi \right>
\end{aligned} T \left<f,\tau_T \varphi - \varphi \right> = 0. \underset{n\to\infty}{\lim}T_n =0 \underset{T_n\to0}{\lim} \frac{\tau_{T_n}\varphi - \varphi}{T_n} = \varphi'\quad \text{in}\ \mathcal{D(\mathbb{R})}. \left<f', \varphi \right> = 0  \varphi f' = 0\quad\text{in}\ \mathcal{D'}(\mathbb{R}). f f L^1(\mathbf{T}) f_n:= \frac{1}{n}\sum_{k=1}^n f(x+\frac{k}{n}). \underset{n\to\infty}{\lim} f_{2^n}(x) = \int_0^1 f\quad a.e.x  \phi(x):=\overline{\lim} f_{2^n}(x) 2^{-n} n \int_0^1 f","['real-analysis', 'distribution-theory', 'periodic-functions', 'measurable-functions']"
94,Why $ \frac{n+2}{n-2}<(n+2)^{2/n}$ for $n\geq 7$.,Why  for ., \frac{n+2}{n-2}<(n+2)^{2/n} n\geq 7,"In some paper, the authors mentioned the following statement: One can easily check that for $n\geq  7$ , $$ \frac{n+2}{n-2}<(n+2)^{2/n}.$$ This statement is correct, and their objective was to find an upper bound of $\frac{n+2}{n-2}$ , eventually starting from some integer. Now my question is how we can see that $(n+2)^{2/n}$ is an upper bound for $\frac{n+2}{n-2}$ starting from some integer ( here it is $7$ ). Thank you.","In some paper, the authors mentioned the following statement: One can easily check that for , This statement is correct, and their objective was to find an upper bound of , eventually starting from some integer. Now my question is how we can see that is an upper bound for starting from some integer ( here it is ). Thank you.",n\geq  7  \frac{n+2}{n-2}<(n+2)^{2/n}. \frac{n+2}{n-2} (n+2)^{2/n} \frac{n+2}{n-2} 7,"['real-analysis', 'inequality', 'exponential-function']"
95,Does a convex function preserve convexity of sets?,Does a convex function preserve convexity of sets?,,"A convex function is defined in a way that’s a bit weird to me, as it’s defined as a function whose area above the graph of a convex set is always convex. Is it also true that if $f:S\to T$ is convex, then any convex set in $U\subseteq S$ maps to a convex set $f(U)\subseteq T$ ? This would mean a convex function is a “structure preserving map for structures on which convexity is defined”.","A convex function is defined in a way that’s a bit weird to me, as it’s defined as a function whose area above the graph of a convex set is always convex. Is it also true that if is convex, then any convex set in maps to a convex set ? This would mean a convex function is a “structure preserving map for structures on which convexity is defined”.",f:S\to T U\subseteq S f(U)\subseteq T,"['real-analysis', 'convex-analysis']"
96,Interesting Difference between Lebesgue and Riemann Integral,Interesting Difference between Lebesgue and Riemann Integral,,"The Riemann integral makes it so that if we have $|f| \leq |g|$ on $[0,1]$ , then integrability of $g$ does not necessarily imply the integrability of $f$ . For example, let $f = \chi_\mathbb{Q}$ , $g = 1$ . Then $g$ is integrable but $f$ is not. The idea here seems to be that (at least on spaces with finite measure), the Lebesgue integral does a better job at dealing with a lack of regularity that does not occur due to ""explosions"". Almost always (pun not intended), the Lebesgue integral fails to converge due to a blow-up (i.e $1/x$ ) or a function having tails that are too large ( $1/\sqrt{x}$ , away from the origin). What exactly is it about the Lebesgue integral that prevents singular behavior on finite measure spaces? Holder's inequality tells us that on finite measure spaces, essential boundedness is enough to guarantee us the existence of an integral, but this is in no way true for the Riemann integral. It seems that the catastrophic failure of the Riemann/Darboux integral is this idea that both upper and lower sums need to converge as the partition mesh goes to $0$ . In the case of the rationals, for any finite partition, the upper and lower sums are always $0,1$ respectively, meaning convergence does not happen. Does the Lebesgue integral avoid this by only considering a supremum (say over an increasing simple function approximation?) Edit: The more I think about this, the more I realize that the issue rests with measurability. The Riemann integral for the characteristic of the rationals does not converges because there is no coherent way to assign a Jordan content (strictly speaking it is not a measure) to this set. Specifically, the above monotonicty for converge (RHS converging implies LHS converging) occurs only when both $f,g$ are Lebesgue measurable functions . Thus, I feel that the monotonicity would be valid for the Riemann integral if we were told that both $f,g$ are ""Jordan measurable"" (whatever that might mean).","The Riemann integral makes it so that if we have on , then integrability of does not necessarily imply the integrability of . For example, let , . Then is integrable but is not. The idea here seems to be that (at least on spaces with finite measure), the Lebesgue integral does a better job at dealing with a lack of regularity that does not occur due to ""explosions"". Almost always (pun not intended), the Lebesgue integral fails to converge due to a blow-up (i.e ) or a function having tails that are too large ( , away from the origin). What exactly is it about the Lebesgue integral that prevents singular behavior on finite measure spaces? Holder's inequality tells us that on finite measure spaces, essential boundedness is enough to guarantee us the existence of an integral, but this is in no way true for the Riemann integral. It seems that the catastrophic failure of the Riemann/Darboux integral is this idea that both upper and lower sums need to converge as the partition mesh goes to . In the case of the rationals, for any finite partition, the upper and lower sums are always respectively, meaning convergence does not happen. Does the Lebesgue integral avoid this by only considering a supremum (say over an increasing simple function approximation?) Edit: The more I think about this, the more I realize that the issue rests with measurability. The Riemann integral for the characteristic of the rationals does not converges because there is no coherent way to assign a Jordan content (strictly speaking it is not a measure) to this set. Specifically, the above monotonicty for converge (RHS converging implies LHS converging) occurs only when both are Lebesgue measurable functions . Thus, I feel that the monotonicity would be valid for the Riemann integral if we were told that both are ""Jordan measurable"" (whatever that might mean).","|f| \leq |g| [0,1] g f f = \chi_\mathbb{Q} g = 1 g f 1/x 1/\sqrt{x} 0 0,1 f,g f,g","['real-analysis', 'lebesgue-integral', 'riemann-integration']"
97,Find all continuous functions in $0$ that $2f(2x) = f(x) + x $,Find all continuous functions in  that,0 2f(2x) = f(x) + x ,"I need to find all functions that they are continuous in zero and $$ 2f(2x) = f(x) + x $$ About I know that there are many examples and that forum but I don't understand one thing   in it and I need additional explanation. (Nowhere I see similar problem :( ) My try I take $ y= 2x$ then $$f(y) = \frac{1}{2}f\left(\frac{1}{2}y\right) + \frac{1}{4}$$ after induction I get: $$f(y) = \frac{1}{2^n}f\left(\frac{1}{2^n}y\right) + y\left(\frac{1}{2^2} + \frac{1}{2^4} + ... + \frac{1}{2^{2n}}  \right)$$ I take $\lim_{n\rightarrow \infty} $ $$ \lim_{n\rightarrow \infty}f(y) = f(y) = \lim_{n\rightarrow \infty} \frac{1}{2^n}f\left(\frac{1}{2^n}y\right) + y\cdot \lim_{n\rightarrow \infty} \left(\frac{1}{2^2} + \frac{1}{2^4} + ... + \frac{1}{2^{2n}}  \right)$$ $$f(y) = \lim_{n\rightarrow \infty} \frac{1}{2^n} \cdot f\left( \lim_{n\rightarrow \infty} \frac{1}{2^n}y \right) + \frac{1}{3}y$$ Ok, there I have question - what I should there after? How do I know that $$f(0) = 0 $$ ? I think that it can be related with "" continuous functions in $0$ "" but function is continous in $0$ when $$ \lim_{y\rightarrow 0^+}f(y)=f(0)=\lim_{y\rightarrow 0^-}f(y)$$ And I don't see a reason why $f(0)=0$ edit Ok, I know why $f(0) =0$ but why I need informations about ""Continuity at a point $0$ "" ? It comes to $$\lim_{n\rightarrow \infty}f\left(\frac{1}{2^n}y\right) = f\left( \lim_{n\rightarrow \infty} \frac{1}{2^n}y \right)$$ ?","I need to find all functions that they are continuous in zero and About I know that there are many examples and that forum but I don't understand one thing   in it and I need additional explanation. (Nowhere I see similar problem :( ) My try I take then after induction I get: I take Ok, there I have question - what I should there after? How do I know that ? I think that it can be related with "" continuous functions in "" but function is continous in when And I don't see a reason why edit Ok, I know why but why I need informations about ""Continuity at a point "" ? It comes to ?", 2f(2x) = f(x) + x   y= 2x f(y) = \frac{1}{2}f\left(\frac{1}{2}y\right) + \frac{1}{4} f(y) = \frac{1}{2^n}f\left(\frac{1}{2^n}y\right) + y\left(\frac{1}{2^2} + \frac{1}{2^4} + ... + \frac{1}{2^{2n}}  \right) \lim_{n\rightarrow \infty}   \lim_{n\rightarrow \infty}f(y) = f(y) = \lim_{n\rightarrow \infty} \frac{1}{2^n}f\left(\frac{1}{2^n}y\right) + y\cdot \lim_{n\rightarrow \infty} \left(\frac{1}{2^2} + \frac{1}{2^4} + ... + \frac{1}{2^{2n}}  \right) f(y) = \lim_{n\rightarrow \infty} \frac{1}{2^n} \cdot f\left( \lim_{n\rightarrow \infty} \frac{1}{2^n}y \right) + \frac{1}{3}y f(0) = 0  0 0  \lim_{y\rightarrow 0^+}f(y)=f(0)=\lim_{y\rightarrow 0^-}f(y) f(0)=0 f(0) =0 0 \lim_{n\rightarrow \infty}f\left(\frac{1}{2^n}y\right) = f\left( \lim_{n\rightarrow \infty} \frac{1}{2^n}y \right),['real-analysis']
98,Open balls in $\mathbb{R}^d$ are Jordan Measurable,Open balls in  are Jordan Measurable,\mathbb{R}^d,"I'm trying to solve the following question from Terrence Tao's An Introduction to Measure Theory. Show that an open Euclidean   ball $B(x, r) := \{y \in \mathbb{R}^d : |y − x| < r\}$ in $\mathbb{R}^d$ is Jordan measurable, with Jordan   measure $c_d r^d$ for some constant $c_d > 0$ depending only on $d$ . Is there an elementary way to approach this problem?","I'm trying to solve the following question from Terrence Tao's An Introduction to Measure Theory. Show that an open Euclidean   ball in is Jordan measurable, with Jordan   measure for some constant depending only on . Is there an elementary way to approach this problem?","B(x, r) := \{y \in \mathbb{R}^d
: |y − x| < r\} \mathbb{R}^d c_d r^d c_d > 0 d",['real-analysis']
99,Seeking Methods to solve $\int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx $,Seeking Methods to solve,\int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx ,"As an extension of a question I posed earlier , I thought it would be best to try and final the result for a more general form: \begin{equation} I_n = \int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx  \end{equation} with $n \in \mathbb{R}, n > 1$ As with the previous question, I'm interested in finding alternative ways of solving this that does not rely on complex analysis. My Method: I employ the exact same method as with my earlier question. Here first let \begin{equation} J_n(t) = \int_{0}^{\infty} \frac{e^{-tx^n}}{x^n + 1}\:dx  \end{equation} We see that $I_n = J_n(1)$ and that $J_n(0) = \frac{1}{n}\Gamma\left(1 - \frac{1}{n}\right)\Gamma\left(\frac{1}{n}\right)$ (This is shown here ) Now, take the derivative with respect to ' $t$ ' to achieve \begin{align}  J_n'(t) &= \int_{0}^{\infty} \frac{-x^ne^{-tx^n}}{x^n + 1}\:dx = -\int_{0}^{\infty} \frac{\left(x^n + 1 - 1\right)e^{-tx^n}}{x^n + 1}\:dx \\ &= -\left[\int_{0}^{\infty}e^{-tx^n}\:dx - \int_{0}^{\infty}\frac{e^{-tx^n}}{x^n + 1}\:dx   \right] \\ &= -\left[ \frac{t^{-\frac{1}{n}}}{n}\Gamma\left(\frac{1}{n}\right) -J_n(t)\right] \end{align} Which yields the differential equation: \begin{equation}  J_n'(t) - J_n(t) = -\frac{t^{-\frac{1}{n}}}{n}\Gamma\left(\frac{1}{n}\right) \end{equation} Which yields the solution: \begin{equation}  J_n(t) = \frac{1}{n}\Gamma\left(1 - \frac{1}{n}, t\right)\Gamma\left(\frac{1}{n}\right)e^t \end{equation} And finally: \begin{equation}  I_n = J_n(1) = \int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx = \frac{e}{n}\Gamma\left(1 - \frac{1}{n}, 1\right)\Gamma\left(\frac{1}{n}\right) \end{equation} Which for me, is a nice result. Fascinated to see other methods! Edit - Thanks to spaceisdarkgreen for the pickup on my mistyping of the Incomplete Gamma Function.","As an extension of a question I posed earlier , I thought it would be best to try and final the result for a more general form: with As with the previous question, I'm interested in finding alternative ways of solving this that does not rely on complex analysis. My Method: I employ the exact same method as with my earlier question. Here first let We see that and that (This is shown here ) Now, take the derivative with respect to ' ' to achieve Which yields the differential equation: Which yields the solution: And finally: Which for me, is a nice result. Fascinated to see other methods! Edit - Thanks to spaceisdarkgreen for the pickup on my mistyping of the Incomplete Gamma Function.","\begin{equation}
I_n = \int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx 
\end{equation} n \in \mathbb{R}, n > 1 \begin{equation}
J_n(t) = \int_{0}^{\infty} \frac{e^{-tx^n}}{x^n + 1}\:dx 
\end{equation} I_n = J_n(1) J_n(0) = \frac{1}{n}\Gamma\left(1 - \frac{1}{n}\right)\Gamma\left(\frac{1}{n}\right) t \begin{align}
 J_n'(t) &= \int_{0}^{\infty} \frac{-x^ne^{-tx^n}}{x^n + 1}\:dx = -\int_{0}^{\infty} \frac{\left(x^n + 1 - 1\right)e^{-tx^n}}{x^n + 1}\:dx \\
&= -\left[\int_{0}^{\infty}e^{-tx^n}\:dx - \int_{0}^{\infty}\frac{e^{-tx^n}}{x^n + 1}\:dx   \right] \\
&= -\left[ \frac{t^{-\frac{1}{n}}}{n}\Gamma\left(\frac{1}{n}\right) -J_n(t)\right]
\end{align} \begin{equation}
 J_n'(t) - J_n(t) = -\frac{t^{-\frac{1}{n}}}{n}\Gamma\left(\frac{1}{n}\right)
\end{equation} \begin{equation}
 J_n(t) = \frac{1}{n}\Gamma\left(1 - \frac{1}{n}, t\right)\Gamma\left(\frac{1}{n}\right)e^t
\end{equation} \begin{equation}
 I_n = J_n(1) = \int_{0}^{\infty} \frac{e^{-x^n}}{x^n + 1}\:dx = \frac{e}{n}\Gamma\left(1 - \frac{1}{n}, 1\right)\Gamma\left(\frac{1}{n}\right)
\end{equation}","['real-analysis', 'integration']"
