,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Magnetic field of ideal toroidal solenoid,Magnetic field of ideal toroidal solenoid,,"A torus of major radius $b$ and minor radius $a$ can be parametrized by $\mathbf{r}:[0,2\pi]^2\to\mathbb{R}^3$, $\mathbf{r}(u,v)$ $=(b+a\cos v)\cos u\mathbf{i}+(b+a\cos v)\sin u\mathbf{j}+a\sin v\mathbf{k}$. Given a point $\mathbf{x}$ outside the surface of the torus, I would like to calculate the following integral: $$\mathbf{B}(\mathbf{x})=\frac{\mu_0}{2\pi} \int_{0}^{2\pi}\int_0^{2\pi}\frac{IN \partial_v\mathbf{r}(u,v) \times(\mathbf{x}-\mathbf{r}(u,v) )}{2\pi\|\mathbf{x}-\mathbf{r}(u,v)\|^3}dudv $$where $\mu_0, I$ and $N$ are constant. A physical interpretation of the expression is as the magnetic field generated by a toroidal solenoid of $N$ loops flown through by a current of intensity $I$ but reduced to an ideal solenoid with a surface distribution of current. We can chose $\mathbf{x}=x\mathbf{i}+z\mathbf{k}$ without loss of generality. I calculate $$ \partial_v\mathbf{r}(u,v) \times(\mathbf{x}-\mathbf{r}(u,v))=(-az\sin v\sin u+a^2\sin u+ab\cos v\sin u)\mathbf{i}$$$$+(ax\cos v-ab\cos v\cos u-a^2\cos u+az\sin v\cos u)\mathbf{j}+ax\sin v \cos u\mathbf{k}$$and $$\|\mathbf{x}-\mathbf{r}(u,v)\|^2=x^2-2x(b+a\cos v)\cos u+(b+a\cos v)^2+z^2-2az\sin v+a^2\sin v.$$Please correct me if my calculation are wrong. By reasoning on the $u$ symmetry, I would say that the $x$ and $z$ component of the integral are zero, while I calculate the $y$ component $B_2(\mathbf{x})$ as$$\frac{\mu_0 IN}{8\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{ax\cos v-ab\cos u\cos v-a^2\cos u+az\cos u\sin v}{(x^2-2x(b+a\cos v)\cos u+(b+a\cos v)^2+z^2-2az\sin v+a^2\sin v)^{3/2}}dudv.$$ Is everything correct until this point? If it is, how can we calculate this integral? I suspect that complex analysis, of which I have some little elements, may help, but I am not sure how... I heartily thank you for any answer! P.S.: By using intuitive arguments and Ampère's circuital law, which I have never seen proved for surface currents, my textbook of physics shows that, for $x\in(b-a,b+a)$, $\mathbf{B}(x\mathbf{i})=-\frac{\mu_0 NI}{2\pi x}\mathbf{j}$, while the field is assumed to be zero outside the torus. I would not be amazed if the integral gave us these same values...","A torus of major radius $b$ and minor radius $a$ can be parametrized by $\mathbf{r}:[0,2\pi]^2\to\mathbb{R}^3$, $\mathbf{r}(u,v)$ $=(b+a\cos v)\cos u\mathbf{i}+(b+a\cos v)\sin u\mathbf{j}+a\sin v\mathbf{k}$. Given a point $\mathbf{x}$ outside the surface of the torus, I would like to calculate the following integral: $$\mathbf{B}(\mathbf{x})=\frac{\mu_0}{2\pi} \int_{0}^{2\pi}\int_0^{2\pi}\frac{IN \partial_v\mathbf{r}(u,v) \times(\mathbf{x}-\mathbf{r}(u,v) )}{2\pi\|\mathbf{x}-\mathbf{r}(u,v)\|^3}dudv $$where $\mu_0, I$ and $N$ are constant. A physical interpretation of the expression is as the magnetic field generated by a toroidal solenoid of $N$ loops flown through by a current of intensity $I$ but reduced to an ideal solenoid with a surface distribution of current. We can chose $\mathbf{x}=x\mathbf{i}+z\mathbf{k}$ without loss of generality. I calculate $$ \partial_v\mathbf{r}(u,v) \times(\mathbf{x}-\mathbf{r}(u,v))=(-az\sin v\sin u+a^2\sin u+ab\cos v\sin u)\mathbf{i}$$$$+(ax\cos v-ab\cos v\cos u-a^2\cos u+az\sin v\cos u)\mathbf{j}+ax\sin v \cos u\mathbf{k}$$and $$\|\mathbf{x}-\mathbf{r}(u,v)\|^2=x^2-2x(b+a\cos v)\cos u+(b+a\cos v)^2+z^2-2az\sin v+a^2\sin v.$$Please correct me if my calculation are wrong. By reasoning on the $u$ symmetry, I would say that the $x$ and $z$ component of the integral are zero, while I calculate the $y$ component $B_2(\mathbf{x})$ as$$\frac{\mu_0 IN}{8\pi^2}\int_0^{2\pi}\int_0^{2\pi}\frac{ax\cos v-ab\cos u\cos v-a^2\cos u+az\cos u\sin v}{(x^2-2x(b+a\cos v)\cos u+(b+a\cos v)^2+z^2-2az\sin v+a^2\sin v)^{3/2}}dudv.$$ Is everything correct until this point? If it is, how can we calculate this integral? I suspect that complex analysis, of which I have some little elements, may help, but I am not sure how... I heartily thank you for any answer! P.S.: By using intuitive arguments and Ampère's circuital law, which I have never seen proved for surface currents, my textbook of physics shows that, for $x\in(b-a,b+a)$, $\mathbf{B}(x\mathbf{i})=-\frac{\mu_0 NI}{2\pi x}\mathbf{j}$, while the field is assumed to be zero outside the torus. I would not be amazed if the integral gave us these same values...",,"['complex-analysis', 'multivariable-calculus', 'definite-integrals', 'physics', 'vector-analysis']"
1,What happens when a partial derivative can't be evaluated at a given point because of an indeterminacy? Is there any way to get around it?,What happens when a partial derivative can't be evaluated at a given point because of an indeterminacy? Is there any way to get around it?,,"For example, I have the following function $$f(x,y,z) = \begin{cases} \sqrt{|xyz|} & \text{if $(x,y,z) \ne (0,0,0)$} \\ 0 & \text{if $(x,y,z)=(0,0,0)$} \end{cases}$$ whose partial derivative with respect to x, if I followed the differentiation rules correctly, is $$\frac{\partial f}{\partial x}(x,y,z) = \frac{xy^{2}z^{2}}{2|xyz|^{3/2}}\ $$ Now, I'm asked to evaluate the partial derivative at the origin (0,0,0); but as you can probably see, it's undefined at that point! So what is there to be done? It's not the first exercise that leads me to this situation. Many thanks!","For example, I have the following function $$f(x,y,z) = \begin{cases} \sqrt{|xyz|} & \text{if $(x,y,z) \ne (0,0,0)$} \\ 0 & \text{if $(x,y,z)=(0,0,0)$} \end{cases}$$ whose partial derivative with respect to x, if I followed the differentiation rules correctly, is $$\frac{\partial f}{\partial x}(x,y,z) = \frac{xy^{2}z^{2}}{2|xyz|^{3/2}}\ $$ Now, I'm asked to evaluate the partial derivative at the origin (0,0,0); but as you can probably see, it's undefined at that point! So what is there to be done? It's not the first exercise that leads me to this situation. Many thanks!",,"['multivariable-calculus', 'derivatives', 'partial-derivative', 'indeterminate-forms']"
2,Implicit differentiation with partial derivatives?!,Implicit differentiation with partial derivatives?!,,"I really don't have idea if I am correct, but I would like to check if what is making sense to me, is indeed correct. If I have a surface in $R^3$ that corresponds to the equation below: $$ z = 6 - x - x² - 2y² $$ When I'm asked to implicit differentiate that, for example, in terms of $\frac{\partial z}{\partial y}$, why should I treat $x$ as a constant?!?! The reason for why I should do that, in my head, is because we just want to check what a tiny change in $z$ is going to change our $y$, so it's just like taking a derivative of a curve in a plane that intersects my surface, and that plane is described by a fixed $x$ value. Because we're fixing $x$ to get that intersection, we treat it as a constant?! Am I right? Since: $$ \frac{\partial z}{\partial y} = -4y $$ Can assume that $-4y$ is going to be the slope of the tangent line for any point of that intersection?! For example, the point $\alpha = (0,1,4)$ is on the plane $x = 0$, and satisfies the equation for the surface. Now, if want to get the tangent line on that point, I just need to get another point that is going to follow the slope of $-4$ because $-4y : y = 1 \Rightarrow -4$, and is on the plane. That point can be, for example $\gamma = (0,2,0)$. Now I can get a director vetor $\vec{t}$, and get a parametric equation for that tangent line: $$ \vec{t} = \alpha - \gamma = (0,-1,4) $$ $$ x = 0 + \beta \cdot 0\\ y = 1 + \beta \cdot -1\\ z = 4 + \beta \cdot 4\\ $$ Is everything correct? If it isn't, please try to explain what have I done wrong. Thanks!","I really don't have idea if I am correct, but I would like to check if what is making sense to me, is indeed correct. If I have a surface in $R^3$ that corresponds to the equation below: $$ z = 6 - x - x² - 2y² $$ When I'm asked to implicit differentiate that, for example, in terms of $\frac{\partial z}{\partial y}$, why should I treat $x$ as a constant?!?! The reason for why I should do that, in my head, is because we just want to check what a tiny change in $z$ is going to change our $y$, so it's just like taking a derivative of a curve in a plane that intersects my surface, and that plane is described by a fixed $x$ value. Because we're fixing $x$ to get that intersection, we treat it as a constant?! Am I right? Since: $$ \frac{\partial z}{\partial y} = -4y $$ Can assume that $-4y$ is going to be the slope of the tangent line for any point of that intersection?! For example, the point $\alpha = (0,1,4)$ is on the plane $x = 0$, and satisfies the equation for the surface. Now, if want to get the tangent line on that point, I just need to get another point that is going to follow the slope of $-4$ because $-4y : y = 1 \Rightarrow -4$, and is on the plane. That point can be, for example $\gamma = (0,2,0)$. Now I can get a director vetor $\vec{t}$, and get a parametric equation for that tangent line: $$ \vec{t} = \alpha - \gamma = (0,-1,4) $$ $$ x = 0 + \beta \cdot 0\\ y = 1 + \beta \cdot -1\\ z = 4 + \beta \cdot 4\\ $$ Is everything correct? If it isn't, please try to explain what have I done wrong. Thanks!",,"['multivariable-calculus', 'implicit-differentiation', 'tangent-line']"
3,Can $C^k(\overline{\Omega})$ functions extend to $C^k(\mathbb{R}^n)$?,Can  functions extend to ?,C^k(\overline{\Omega}) C^k(\mathbb{R}^n),"Consider the following definition. Let the open set $\Omega\subset{\mathbb R}^n$, and $k$ be a positive integer. $C^k(\Omega)$ will denote the space of functions possessing continuous derivatives up to order $k$ on $\Omega$, and $C^k(\overline{\Omega})$ will denote the space of all $u\in C^k(\Omega)$ such that $\partial^{\alpha}u$ extends continuously to the closure $\overline{\Omega}$ for $0\leq|\alpha|\leq k$. Here is my question : If $u\in C^k(\overline{\Omega})$, can $\partial^\alpha u$ extend continuously to $\mathbb{R}^n$ for $0\leq|\alpha|\leq k$? This question is motivated by the following ones: Relationship among the function spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ What does the notation $C(\bar U)$ mean for $U\subset\Bbb{R}^d$ open? Reference request: $C^k(\overline\Omega)$ as restriction of $C^{k}(\mathbb{R}^d)$ functions on $\Omega$","Consider the following definition. Let the open set $\Omega\subset{\mathbb R}^n$, and $k$ be a positive integer. $C^k(\Omega)$ will denote the space of functions possessing continuous derivatives up to order $k$ on $\Omega$, and $C^k(\overline{\Omega})$ will denote the space of all $u\in C^k(\Omega)$ such that $\partial^{\alpha}u$ extends continuously to the closure $\overline{\Omega}$ for $0\leq|\alpha|\leq k$. Here is my question : If $u\in C^k(\overline{\Omega})$, can $\partial^\alpha u$ extend continuously to $\mathbb{R}^n$ for $0\leq|\alpha|\leq k$? This question is motivated by the following ones: Relationship among the function spaces $C_c^\infty(\Omega)$, $C_c^\infty(\overline{\Omega})$ and $C_c^\infty(\Bbb{R}^d)$ What does the notation $C(\bar U)$ mean for $U\subset\Bbb{R}^d$ open? Reference request: $C^k(\overline\Omega)$ as restriction of $C^{k}(\mathbb{R}^d)$ functions on $\Omega$",,"['real-analysis', 'multivariable-calculus']"
4,An integral: magnetic fied of infinite wire,An integral: magnetic fied of infinite wire,,"Let $\boldsymbol{l}:\mathbb{R}\to\mathbb{R}^3$ be the piecewise smooth parametrization of an infinitely long curve $\gamma$. Let us define$$\boldsymbol{B}(\boldsymbol{x})=\int_\gamma\frac{d\boldsymbol{l}\times(\boldsymbol{x}-\boldsymbol{l})}{\|\boldsymbol{x}-\boldsymbol{l}\|^3}=\int_{-\infty}^{+\infty}\frac{\boldsymbol{l}'(t)\times(\boldsymbol{x}-\boldsymbol{l}(t))}{\|\boldsymbol{x}-\boldsymbol{l}(t)\|^3}dt.$$A physical interpretation of the integral is that $\boldsymbol{B}$ represents the magnetic field generated by an infinitely long wire $\gamma$ carrying a current $I$ such that $\mu_0 I=4\pi$ (where $\mu_0$ is vacuum permeabilty). Can we be sure that the integral converges and, if we can, how can it be proved? I think that the best way to approach the problem is verifying whether the integral converges as a Lebesgue integral, which is equivalent to verifying whether the integral of the absolute value of the integrand converges, and I notice that every component of the integral is the difference of two terms having the form $l_i'(t)(x_j-l_i(t))\|\boldsymbol{x}-\boldsymbol{l}(t)\|^{-3}$. I see that $|l_i'(t)(x_j-l_i(t))|\|\boldsymbol{x}-\boldsymbol{l}(t)\|^{-3}$ $\le |l_i'(t)||x_j-l_i(t)|^{-2}$, but the absolute value does not allow me to use the rule $l_i'(t)dt=dl_i$...","Let $\boldsymbol{l}:\mathbb{R}\to\mathbb{R}^3$ be the piecewise smooth parametrization of an infinitely long curve $\gamma$. Let us define$$\boldsymbol{B}(\boldsymbol{x})=\int_\gamma\frac{d\boldsymbol{l}\times(\boldsymbol{x}-\boldsymbol{l})}{\|\boldsymbol{x}-\boldsymbol{l}\|^3}=\int_{-\infty}^{+\infty}\frac{\boldsymbol{l}'(t)\times(\boldsymbol{x}-\boldsymbol{l}(t))}{\|\boldsymbol{x}-\boldsymbol{l}(t)\|^3}dt.$$A physical interpretation of the integral is that $\boldsymbol{B}$ represents the magnetic field generated by an infinitely long wire $\gamma$ carrying a current $I$ such that $\mu_0 I=4\pi$ (where $\mu_0$ is vacuum permeabilty). Can we be sure that the integral converges and, if we can, how can it be proved? I think that the best way to approach the problem is verifying whether the integral converges as a Lebesgue integral, which is equivalent to verifying whether the integral of the absolute value of the integrand converges, and I notice that every component of the integral is the difference of two terms having the form $l_i'(t)(x_j-l_i(t))\|\boldsymbol{x}-\boldsymbol{l}(t)\|^{-3}$. I see that $|l_i'(t)(x_j-l_i(t))|\|\boldsymbol{x}-\boldsymbol{l}(t)\|^{-3}$ $\le |l_i'(t)||x_j-l_i(t)|^{-2}$, but the absolute value does not allow me to use the rule $l_i'(t)dt=dl_i$...",,"['multivariable-calculus', 'improper-integrals', 'lebesgue-integral', 'physics']"
5,Change of variables between spaces with different dimensionality in using the Riemann integral,Change of variables between spaces with different dimensionality in using the Riemann integral,,"On wikipedia ( https://en.wikipedia.org/wiki/Integration_by_substitution ) I find that, \begin{equation} \int_{\phi(U)} f(v)dv = \int_U f(\phi(u))|\det(D\phi(u))|du \end{equation} for the Riemann integral under a number of conditions. I'm looking for a more general formulation, where $f:R^m\rightarrow R$, $M$ is a mapping $M:V \rightarrow U$, where $V\subset R^n$ is open and $U \subset R^m$ is some n dimensional surface, so that: \begin{equation} \int_{V} f(M(v))dv = \int_U f(u) \cdot \text{something} \cdot du \end{equation} Already in the same wikipedia article, conditions for doing this using Lebesgue integration is given, but I want to know when and how this can be done using the Riemann integral, because I'm writing a short text that should be easy to understand, so I don't want to make it too abstract. A literature reference would be appreciated.","On wikipedia ( https://en.wikipedia.org/wiki/Integration_by_substitution ) I find that, \begin{equation} \int_{\phi(U)} f(v)dv = \int_U f(\phi(u))|\det(D\phi(u))|du \end{equation} for the Riemann integral under a number of conditions. I'm looking for a more general formulation, where $f:R^m\rightarrow R$, $M$ is a mapping $M:V \rightarrow U$, where $V\subset R^n$ is open and $U \subset R^m$ is some n dimensional surface, so that: \begin{equation} \int_{V} f(M(v))dv = \int_U f(u) \cdot \text{something} \cdot du \end{equation} Already in the same wikipedia article, conditions for doing this using Lebesgue integration is given, but I want to know when and how this can be done using the Riemann integral, because I'm writing a short text that should be easy to understand, so I don't want to make it too abstract. A literature reference would be appreciated.",,"['integration', 'multivariable-calculus', 'differential-geometry', 'riemann-integration', 'change-of-basis']"
6,Change of variables problem: why not to take all possible regions to be integrated?,Change of variables problem: why not to take all possible regions to be integrated?,,"I was working out this example in Jon Rogawski multivariable calculus book: My question is: why didn't he take all possible regions that satisfy the conditions: $$1\le xy\le4,1\le y/x\le4$$ which include the region in the third quarter ?? Also, if we choose only the region in the first quadrant to be integrated, then after transformation of coordinate, there are two areas satisfy transformation: $$[1,2]\times [1,2]$$ as the example, and also: $$[-1,-2]\times [-1,-2]$$ So why don't we include it in integration ?","I was working out this example in Jon Rogawski multivariable calculus book: My question is: why didn't he take all possible regions that satisfy the conditions: $$1\le xy\le4,1\le y/x\le4$$ which include the region in the third quarter ?? Also, if we choose only the region in the first quadrant to be integrated, then after transformation of coordinate, there are two areas satisfy transformation: $$[1,2]\times [1,2]$$ as the example, and also: $$[-1,-2]\times [-1,-2]$$ So why don't we include it in integration ?",,['multivariable-calculus']
7,$f$ is uniformly continuous only if $g$ is constant,is uniformly continuous only if  is constant,f g,"Let $g:\mathbb R\to\mathbb R$ be continuous and define $f:\mathbb R^2\to\mathbb R$ by $f(x_1,x_2)=g(x_1x_2)$. Show that $f$ is uniformly continuous only if $g$ is a constant function. I'm not sure whether the following is right. I don't use the continuity of $g$, so I guess something in the argument must be wrong.$\newcommand{\norm}[1]{\lVert#1\rVert}$ Proof . Suppose that $g$ is not constant. Then there exist $\gamma_0,\gamma_1\in\mathbb R$ such that $g(\gamma_0)\neq g(\gamma_1)$. Let $\{a_n:=(n,\gamma_0/n)\}_{n\in\mathbb N}$ and $\{b_n:=(n,\gamma_1/n)\}_{n\in\mathbb N}$ be sequences on $\mathbb R^2$. Then     $$\norm{a_n-b_n}=\norm{\left(0,\frac{\gamma_0-\gamma_1}n\right)}\to\norm{(0,0)}=0$$     as $n\to\infty$, but     $$\norm{f(a_n)-f(b_n)}=\norm{g(\gamma_0)-g(\gamma_1)}$$     is constant and non-zero. Therefore $f$ cannot be uniformly continuous.","Let $g:\mathbb R\to\mathbb R$ be continuous and define $f:\mathbb R^2\to\mathbb R$ by $f(x_1,x_2)=g(x_1x_2)$. Show that $f$ is uniformly continuous only if $g$ is a constant function. I'm not sure whether the following is right. I don't use the continuity of $g$, so I guess something in the argument must be wrong.$\newcommand{\norm}[1]{\lVert#1\rVert}$ Proof . Suppose that $g$ is not constant. Then there exist $\gamma_0,\gamma_1\in\mathbb R$ such that $g(\gamma_0)\neq g(\gamma_1)$. Let $\{a_n:=(n,\gamma_0/n)\}_{n\in\mathbb N}$ and $\{b_n:=(n,\gamma_1/n)\}_{n\in\mathbb N}$ be sequences on $\mathbb R^2$. Then     $$\norm{a_n-b_n}=\norm{\left(0,\frac{\gamma_0-\gamma_1}n\right)}\to\norm{(0,0)}=0$$     as $n\to\infty$, but     $$\norm{f(a_n)-f(b_n)}=\norm{g(\gamma_0)-g(\gamma_1)}$$     is constant and non-zero. Therefore $f$ cannot be uniformly continuous.",,"['limits', 'multivariable-calculus', 'proof-verification', 'uniform-continuity']"
8,Integral involving the von Mises-Fisher distribution,Integral involving the von Mises-Fisher distribution,,"I'm going quickly through the VonMises-Fisher distribution $M$ on $\mathbb S^{d-1}$ and its properties. Its probability density function is: $$f(x; \kappa,\mu)= c(\kappa)\exp(\kappa x^T\mu)$$ where $\kappa \times \mu \in[0,\infty)\times \mathbb S^{d-1}$ are fixed parameters and $c(\kappa)$ is a normalizing constant so the whole thing will integrate $1$. Specifically, $$c(\kappa)=\left(\frac\kappa2\right)^{d/2-1}\frac1{\Gamma(d/2)I_{d/2-1}(\kappa)}$$ where $\Gamma$ is the gamma function and $I_\nu$ is the modified Bessel function of the first kind and order $\nu$ is $$I_\nu(\kappa) = \frac{(\kappa/2)^\nu}{\Gamma(\nu +\frac12)\Gamma(\frac12)}\int_{-1}^1e^{\kappa t}(1-t^2)^{\nu -\frac12}dt.$$ The paper I'm reading mentions the following equality without further explanation $$\int_{\mathbb{S}^{d-1}_{\geq\frac1r}}f_\kappa(\xi) \, d\xi = c(\kappa) \frac{s_{d-2}}{s_{d-1}} \int_0^{\arccos(-\log(rc(\kappa))/\kappa)} e^{\kappa\cos\theta} \sin^{d-2} \theta \, d\theta$$   where $s_{d-1}=\displaystyle \frac{2\pi^{\frac d2}}{\Gamma(\frac d2)}$ and    $$\mathbb S_{\geq\frac1r}^{d-1} =\{x\in\mathbb S^{d-1}:f(x;\kappa,\mu)\geq r\}$$ for a fixed $r>0$ I'm a bit rusty on multivariable integration. I thought on spherical coordinates, but I'm not sure how does it work when I'm working on $d-1$ dimensions. How can I get the highlighted equality?","I'm going quickly through the VonMises-Fisher distribution $M$ on $\mathbb S^{d-1}$ and its properties. Its probability density function is: $$f(x; \kappa,\mu)= c(\kappa)\exp(\kappa x^T\mu)$$ where $\kappa \times \mu \in[0,\infty)\times \mathbb S^{d-1}$ are fixed parameters and $c(\kappa)$ is a normalizing constant so the whole thing will integrate $1$. Specifically, $$c(\kappa)=\left(\frac\kappa2\right)^{d/2-1}\frac1{\Gamma(d/2)I_{d/2-1}(\kappa)}$$ where $\Gamma$ is the gamma function and $I_\nu$ is the modified Bessel function of the first kind and order $\nu$ is $$I_\nu(\kappa) = \frac{(\kappa/2)^\nu}{\Gamma(\nu +\frac12)\Gamma(\frac12)}\int_{-1}^1e^{\kappa t}(1-t^2)^{\nu -\frac12}dt.$$ The paper I'm reading mentions the following equality without further explanation $$\int_{\mathbb{S}^{d-1}_{\geq\frac1r}}f_\kappa(\xi) \, d\xi = c(\kappa) \frac{s_{d-2}}{s_{d-1}} \int_0^{\arccos(-\log(rc(\kappa))/\kappa)} e^{\kappa\cos\theta} \sin^{d-2} \theta \, d\theta$$   where $s_{d-1}=\displaystyle \frac{2\pi^{\frac d2}}{\Gamma(\frac d2)}$ and    $$\mathbb S_{\geq\frac1r}^{d-1} =\{x\in\mathbb S^{d-1}:f(x;\kappa,\mu)\geq r\}$$ for a fixed $r>0$ I'm a bit rusty on multivariable integration. I thought on spherical coordinates, but I'm not sure how does it work when I'm working on $d-1$ dimensions. How can I get the highlighted equality?",,"['integration', 'multivariable-calculus', 'probability-distributions', 'definite-integrals']"
9,What is the index of a vector field with positive divergence?,What is the index of a vector field with positive divergence?,,"Let $v:\Bbb R^n\to\Bbb R^n$ be a smooth vector field with an isolated zero at $z\in\Bbb R^n$. Suppose that $(\operatorname{div}v)(z)>0$. Can we say anything about the index of $v$ at $z$? I know that if a vector field is a ""source"" in the sense that the field always points away from the zero, then its index will be $1$. But as far as I know this understanding of positive divergence is a heuristic, and in general one can not find a sphere $S_\epsilon$ centered at $z$ such that $v$ points out of this sphere. However, if positive divergence indeed guarantees the existence of such an $S_\epsilon$, then the vector field on $S_\epsilon$ is smoothly homotopic to the Gauss mapping, and so has index $1$.","Let $v:\Bbb R^n\to\Bbb R^n$ be a smooth vector field with an isolated zero at $z\in\Bbb R^n$. Suppose that $(\operatorname{div}v)(z)>0$. Can we say anything about the index of $v$ at $z$? I know that if a vector field is a ""source"" in the sense that the field always points away from the zero, then its index will be $1$. But as far as I know this understanding of positive divergence is a heuristic, and in general one can not find a sphere $S_\epsilon$ centered at $z$ such that $v$ points out of this sphere. However, if positive divergence indeed guarantees the existence of such an $S_\epsilon$, then the vector field on $S_\epsilon$ is smoothly homotopic to the Gauss mapping, and so has index $1$.",,"['multivariable-calculus', 'differential-geometry', 'differential-topology']"
10,How to handle this integral? [closed],How to handle this integral? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to deal with this integral ? $$ \int_{a}^{b}\cdots\int_{a}^{b} \frac{\mathrm{d}x_{1}\,\mathrm{d}x_{2}\cdots\mathrm{d}x_{n}} {1 + x_{1}^{2} + x_{1}^{2}x_{2}^{2} + x_{1}^{2}x_{2}^{2}x_{3}^{2} + \cdots + x_{1}^{2}x_{2}^{2}\cdots x_{n}^{2}} $$ where $b > a > 0$. Is there a proper way to bring the denominator to numerator with some transformation and then do the integral ?.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How to deal with this integral ? $$ \int_{a}^{b}\cdots\int_{a}^{b} \frac{\mathrm{d}x_{1}\,\mathrm{d}x_{2}\cdots\mathrm{d}x_{n}} {1 + x_{1}^{2} + x_{1}^{2}x_{2}^{2} + x_{1}^{2}x_{2}^{2}x_{3}^{2} + \cdots + x_{1}^{2}x_{2}^{2}\cdots x_{n}^{2}} $$ where $b > a > 0$. Is there a proper way to bring the denominator to numerator with some transformation and then do the integral ?.",,"['multivariable-calculus', 'definite-integrals']"
11,Jacobian matrix of the parametrization of (part of) a ball,Jacobian matrix of the parametrization of (part of) a ball,,"I read (in E. Sernesi, Geometria 2) that the function $\varphi:\left(-\frac{\pi}{2},\frac{\pi}{2}\right)\times\mathbb{R}^{n}\to\mathbb{R}^{n+1}$ defined by $$\varphi(\theta_1,\ldots,\theta_n,r)=\left(r\prod_{k=1}^n\cos\theta_k,r\sin\theta_n\prod_{k=1}^{n-1}\cos\theta_k, r\sin\theta_{n-1}\prod_{k=1}^{n-2}\cos\theta_k,\ldots ,r\sin\theta_1\right)$$has an invertible Jacobian matrix.  How can it be proved? I  have calculated such matrix as $$\begin{pmatrix}\prod_k\cos\theta_k & -r\sin\theta_1\prod_{k\ne 1}\cos\theta_k &\ldots& -r\sin\theta_n\prod_{k\ne n}\cos\theta_k\\\sin\theta_n\prod_{k<n}\cos\theta_k& -r\sin\theta_1\sin\theta_n\prod_{k<n,k\ne 1}\cos\theta_k&\ldots&-\cos\theta_n\prod_{k<n,k\ne n}\cos\theta_k\\\vdots& \vdots &\ddots&\vdots \\ \sin\theta_1&r\cos\theta_1 &\ldots&0\end{pmatrix}$$I have tried to use induction to prove it, and showed that it holds for $n=2$, and $n=1$, by calculating the determinant, but I do not know how to prove it for the general case. Using the determinant in the general case by using the Laplace expansion does not seem applicable to me... verifying the independence of the columns or rows may be an option, but I cannot do so. I thank any answerer very much!","I read (in E. Sernesi, Geometria 2) that the function $\varphi:\left(-\frac{\pi}{2},\frac{\pi}{2}\right)\times\mathbb{R}^{n}\to\mathbb{R}^{n+1}$ defined by $$\varphi(\theta_1,\ldots,\theta_n,r)=\left(r\prod_{k=1}^n\cos\theta_k,r\sin\theta_n\prod_{k=1}^{n-1}\cos\theta_k, r\sin\theta_{n-1}\prod_{k=1}^{n-2}\cos\theta_k,\ldots ,r\sin\theta_1\right)$$has an invertible Jacobian matrix.  How can it be proved? I  have calculated such matrix as $$\begin{pmatrix}\prod_k\cos\theta_k & -r\sin\theta_1\prod_{k\ne 1}\cos\theta_k &\ldots& -r\sin\theta_n\prod_{k\ne n}\cos\theta_k\\\sin\theta_n\prod_{k<n}\cos\theta_k& -r\sin\theta_1\sin\theta_n\prod_{k<n,k\ne 1}\cos\theta_k&\ldots&-\cos\theta_n\prod_{k<n,k\ne n}\cos\theta_k\\\vdots& \vdots &\ddots&\vdots \\ \sin\theta_1&r\cos\theta_1 &\ldots&0\end{pmatrix}$$I have tried to use induction to prove it, and showed that it holds for $n=2$, and $n=1$, by calculating the determinant, but I do not know how to prove it for the general case. Using the determinant in the general case by using the Laplace expansion does not seem applicable to me... verifying the independence of the columns or rows may be an option, but I cannot do so. I thank any answerer very much!",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'vector-analysis']"
12,Inverse Function Theorem Question.,Inverse Function Theorem Question.,,"The inverse function theorem is a general statement about finding local open sets and an inverse on those sets. The standard proof by Spivak doesn't tell you about the sizes of the sets. A different proof using the contraction mapping theorem does using the norm of the differential, - is it possible to recover this estimate within Spivaks proof? Or is the result strictly stronger? For clarity, I mean set containment with reference to balls or cubes. For example if we have a C^1 transformation F with nonzero jacobian where the differential is approximately equal to the identity on some set S, we can say that for a circle C contained in S, that F(C) contains a circle slightly smaller than C and is contained in a circle slightly larger than F(C).","The inverse function theorem is a general statement about finding local open sets and an inverse on those sets. The standard proof by Spivak doesn't tell you about the sizes of the sets. A different proof using the contraction mapping theorem does using the norm of the differential, - is it possible to recover this estimate within Spivaks proof? Or is the result strictly stronger? For clarity, I mean set containment with reference to balls or cubes. For example if we have a C^1 transformation F with nonzero jacobian where the differential is approximately equal to the identity on some set S, we can say that for a circle C contained in S, that F(C) contains a circle slightly smaller than C and is contained in a circle slightly larger than F(C).",,"['real-analysis', 'multivariable-calculus']"
13,Question about Gauss divergence theorem,Question about Gauss divergence theorem,,"The Divergence Theorem says that for $\Omega \subset  \mathbb{R}^n$ $$\int_\Omega \nabla \cdot F = \int_{\partial\Omega} F \cdot n $$ where $n$ is the outward normal. Assuming that $F$ is a smooth vector field, does the Divergence Theorem require $\partial \Omega$ to have $n$ dimensional Lebesgue measure zero?","The Divergence Theorem says that for $\Omega \subset  \mathbb{R}^n$ $$\int_\Omega \nabla \cdot F = \int_{\partial\Omega} F \cdot n $$ where $n$ is the outward normal. Assuming that $F$ is a smooth vector field, does the Divergence Theorem require $\partial \Omega$ to have $n$ dimensional Lebesgue measure zero?",,"['real-analysis', 'multivariable-calculus']"
14,Prove that the line integral of a vector valued function does not depend on the particular path,Prove that the line integral of a vector valued function does not depend on the particular path,,"Let C denote the path from $\alpha$ to $\beta$.   If $\textbf{F}$ is a gradient vector, that is, there exists a differentiable function $f$ such that $$\nabla f=F,$$ then \begin{eqnarray*} \int_{C}\textbf{F}\; ds &=& \int_{\alpha}^{\beta} \textbf{F}(\vec{c}(t)).\vec{c'}(t)\; dt \\ &=&\int_{\alpha}^{\beta} \nabla f(\vec{c}(t)).\vec{c'}(t)\; dt \\ &=&\int_{\alpha}^{\beta}  \frac{\partial  f(\vec{c}(t))}{\partial t}\; dt \\ &=&f(\vec{c}(\beta))- f(\vec{c}( \alpha)) \end{eqnarray*} That is, the integral of $\textbf{F}$ over $C$ depends on the values of the end points $c(\beta)$ and $c(\alpha)$ and is thus independent of the path between them. This proof is true if and only if  $\textbf{F}$ is a gradient vector, what if not ?","Let C denote the path from $\alpha$ to $\beta$.   If $\textbf{F}$ is a gradient vector, that is, there exists a differentiable function $f$ such that $$\nabla f=F,$$ then \begin{eqnarray*} \int_{C}\textbf{F}\; ds &=& \int_{\alpha}^{\beta} \textbf{F}(\vec{c}(t)).\vec{c'}(t)\; dt \\ &=&\int_{\alpha}^{\beta} \nabla f(\vec{c}(t)).\vec{c'}(t)\; dt \\ &=&\int_{\alpha}^{\beta}  \frac{\partial  f(\vec{c}(t))}{\partial t}\; dt \\ &=&f(\vec{c}(\beta))- f(\vec{c}( \alpha)) \end{eqnarray*} That is, the integral of $\textbf{F}$ over $C$ depends on the values of the end points $c(\beta)$ and $c(\alpha)$ and is thus independent of the path between them. This proof is true if and only if  $\textbf{F}$ is a gradient vector, what if not ?",,['multivariable-calculus']
15,"Showing that $\|.\|$ is a norm of the space of 1-forms $\Omega^1(U)$, where $U\subset\mathbb{R}^n$.","Showing that  is a norm of the space of 1-forms , where .",\|.\| \Omega^1(U) U\subset\mathbb{R}^n,"Let $U\subset\mathbb{R}^n$ and let $\Omega^p(U)$ denote the vector space of $p$-forms ($p\in\mathbb{N}$). Define the isomorphism $\Phi:\Omega^{1}(U)\to\Omega^{n-1}(U)$ as $$\Phi\left(\omega=\sum_{i=1}^nf_idx_i\right)=\sum_{i=1}^n(-1)^{i-1}f_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_n$$   It can be easily shown that $\Phi$ is an isomorphism. Now define for $\omega\in\Omega^{1}(U)$, $$\|\omega\|=\int_U\omega\:\Phi(\omega)$$ We want to show that $\|.\|$ is actually a norm on $\Omega^1(U)$. So I started by the following. I want to show that $\|\omega\|\geq 0$. Notice that $$\int_U\omega\:\Phi(\omega)=\int_U\sum_{i=1}^n\sum_{j=1}^n(-1)^{j-1}(f_if_j)dx_idx_1\ldots dx_{j-1}dx_{j+1}\ldots dx_n$$ Now notice that the term $(-1)^{j-1}(f_if_j)dx_idx_1\ldots dx_{j-1}dx_{j+1}\ldots dx_n\not=0$ only if $i=j$. If not, there will be two $dx_i$'s and term will be $0$. So the sum can be rewritten  $$\int_U\omega\:\Phi(\omega)=\int_U\sum_{i=1}^n(-1)^{i-1}f_i^2dx_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_n$$. Now in order to rearrange $dx_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_{n}$ to $dx_1\ldots dx_n$ we need to do $i-1$ swaps and we get \begin{align*}\int_U\omega\:\Phi(\omega)&=\int_U\sum_{i=1}^n(-1)^{i-1}\cdot f_i^2\cdot (-1)^{i-1}dx_1\ldots dx_n\\&=\int_U\sum_{i=1}^n f_i^2 dx_1\ldots dx_n\\&=\sum_{i=1}^n\int_Uf_i^2dx_1\ldots dx_n\geq 0\end{align*} Also is easy to show that $\|\omega\|=0$ if and only $\omega=0$. I got stuck when showing the triangle inequality and scaling. Here my try :to show the triangle inequality, let $\omega,\eta\in\Omega^1(U)$ and using the fact that $\Phi$ is a homomorphism, \begin{align*}\|\omega+\eta\|&=\int_U(\omega+\eta)\Phi(\omega+\eta)=\int_U(\omega+\eta)\left(\Phi(\omega)+\Phi(\eta)\right)\\&=\int_U\omega\Phi(\omega)+\eta\Phi(\omega)+\omega\Phi(\eta)+\eta\Phi(\eta)\\&=\|\omega\|+\|\eta\|+\int_U\eta\Phi(\omega)+\omega\Phi(\eta)\end{align*} I got suck here. Can anyone help with the triangle inequality and scaling? EDIT : If we redefine $\|\omega\|=\sqrt{\int_U\omega\:\Phi(\omega)}$ then $\|\omega\|$ is still greater than 0, equal if and only if $\omega=0$. And for scaling we get  $$\|c\omega\|=\sqrt{\int_U c\omega\:\Phi(c\omega)}=\sqrt{c^2\int_U \omega\:\Phi(\omega)}=|c|\sqrt{\int_U \omega\:\Phi(\omega)}=|c|\|\omega\|$$ and what remains to show is the triangle inequality. Any help?","Let $U\subset\mathbb{R}^n$ and let $\Omega^p(U)$ denote the vector space of $p$-forms ($p\in\mathbb{N}$). Define the isomorphism $\Phi:\Omega^{1}(U)\to\Omega^{n-1}(U)$ as $$\Phi\left(\omega=\sum_{i=1}^nf_idx_i\right)=\sum_{i=1}^n(-1)^{i-1}f_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_n$$   It can be easily shown that $\Phi$ is an isomorphism. Now define for $\omega\in\Omega^{1}(U)$, $$\|\omega\|=\int_U\omega\:\Phi(\omega)$$ We want to show that $\|.\|$ is actually a norm on $\Omega^1(U)$. So I started by the following. I want to show that $\|\omega\|\geq 0$. Notice that $$\int_U\omega\:\Phi(\omega)=\int_U\sum_{i=1}^n\sum_{j=1}^n(-1)^{j-1}(f_if_j)dx_idx_1\ldots dx_{j-1}dx_{j+1}\ldots dx_n$$ Now notice that the term $(-1)^{j-1}(f_if_j)dx_idx_1\ldots dx_{j-1}dx_{j+1}\ldots dx_n\not=0$ only if $i=j$. If not, there will be two $dx_i$'s and term will be $0$. So the sum can be rewritten  $$\int_U\omega\:\Phi(\omega)=\int_U\sum_{i=1}^n(-1)^{i-1}f_i^2dx_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_n$$. Now in order to rearrange $dx_idx_1\ldots dx_{i-1}dx_{i+1}\ldots dx_{n}$ to $dx_1\ldots dx_n$ we need to do $i-1$ swaps and we get \begin{align*}\int_U\omega\:\Phi(\omega)&=\int_U\sum_{i=1}^n(-1)^{i-1}\cdot f_i^2\cdot (-1)^{i-1}dx_1\ldots dx_n\\&=\int_U\sum_{i=1}^n f_i^2 dx_1\ldots dx_n\\&=\sum_{i=1}^n\int_Uf_i^2dx_1\ldots dx_n\geq 0\end{align*} Also is easy to show that $\|\omega\|=0$ if and only $\omega=0$. I got stuck when showing the triangle inequality and scaling. Here my try :to show the triangle inequality, let $\omega,\eta\in\Omega^1(U)$ and using the fact that $\Phi$ is a homomorphism, \begin{align*}\|\omega+\eta\|&=\int_U(\omega+\eta)\Phi(\omega+\eta)=\int_U(\omega+\eta)\left(\Phi(\omega)+\Phi(\eta)\right)\\&=\int_U\omega\Phi(\omega)+\eta\Phi(\omega)+\omega\Phi(\eta)+\eta\Phi(\eta)\\&=\|\omega\|+\|\eta\|+\int_U\eta\Phi(\omega)+\omega\Phi(\eta)\end{align*} I got suck here. Can anyone help with the triangle inequality and scaling? EDIT : If we redefine $\|\omega\|=\sqrt{\int_U\omega\:\Phi(\omega)}$ then $\|\omega\|$ is still greater than 0, equal if and only if $\omega=0$. And for scaling we get  $$\|c\omega\|=\sqrt{\int_U c\omega\:\Phi(c\omega)}=\sqrt{c^2\int_U \omega\:\Phi(\omega)}=|c|\sqrt{\int_U \omega\:\Phi(\omega)}=|c|\|\omega\|$$ and what remains to show is the triangle inequality. Any help?",,"['linear-algebra', 'multivariable-calculus', 'differential-forms']"
16,Calculating the lipschitz constant of this function?,Calculating the lipschitz constant of this function?,,"So I have a function $f:\mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}$ given by $$f(x,y) = \sum_{i=1}^m y_i A_ix$$ where $A_i$ are $n$ by $n$ real symmetric positive definite matrices (not that it really matters in this case) for $i=1, \dots m$ . I would like to calculate the Lipschitz constant of $\nabla_x f(x,y)$ on the set that $\|x \| \leq 1$ . When I fix $x \in \mathbb{R}^n\cap\{x: \|x\| \leq 1\}$ and take $y^1,y^2 \in \mathbb{R}^m$ I have $$ \|\nabla_x f(x,y^1) - \nabla_x f(x,y^2)\| = \|\sum_{i=1}^m (y^1_i - y^2_i) A_ix\| $$ but I'm having trouble majorizing this for $L\|y^1 - y^2\|$ . How can I calculate such an $L$ so that the inequality works?",So I have a function given by where are by real symmetric positive definite matrices (not that it really matters in this case) for . I would like to calculate the Lipschitz constant of on the set that . When I fix and take I have but I'm having trouble majorizing this for . How can I calculate such an so that the inequality works?,"f:\mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R} f(x,y) = \sum_{i=1}^m y_i A_ix A_i n n i=1, \dots m \nabla_x f(x,y) \|x \| \leq 1 x \in \mathbb{R}^n\cap\{x: \|x\| \leq 1\} y^1,y^2 \in \mathbb{R}^m  \|\nabla_x f(x,y^1) - \nabla_x f(x,y^2)\| = \|\sum_{i=1}^m (y^1_i - y^2_i) A_ix\|  L\|y^1 - y^2\| L","['real-analysis', 'multivariable-calculus', 'functions', 'lipschitz-functions']"
17,Double integrals to find area?,Double integrals to find area?,,"In my math textbook it's written that double integrals is used to find area. Is it true? I thought I'd used to find volume. If so, how is it used to find area?","In my math textbook it's written that double integrals is used to find area. Is it true? I thought I'd used to find volume. If so, how is it used to find area?",,"['integration', 'multivariable-calculus']"
18,Higher Order Derivative Tests in Multiple Dimensions,Higher Order Derivative Tests in Multiple Dimensions,,"To evaluate the minima, maxima, and saddle points of a real function of 2 variables, we use the second derivative test after evaluating the critical points to identify the type of extrema they are. Recall that given a function $f(x,y)$ and the Hessian matrix function $H(f)$, the second derivative test tells us If $det(H)(x_0,y_0)> 0$, and $f_{xx}(x_0, y_0) < 0$, then $(x_0,y_0)$ is a local maximum. If $det(H)(x_0,y_0)> 0$, and $f_{xx}(x_0, y_0) > 0$, then $(x_0,y_0)$ is a local minimum. If $det(H)(x_0,y_0)< 0$, then $(x_0,y_0)$ is a saddle point. If $det(H)(x_0,y_0)=0$, then the test is inconclusive. What I am most interested in is the last of these bullets. What is there left to do besides look at the graph in case the second derivative test yields inconclusive results? For single-variable functions, you would use a higher order derivative. However, a higher order derivative would have $2^o$ derivatives (for order $o$), so what calculation would be necessary to identify the type of critical point in this case?","To evaluate the minima, maxima, and saddle points of a real function of 2 variables, we use the second derivative test after evaluating the critical points to identify the type of extrema they are. Recall that given a function $f(x,y)$ and the Hessian matrix function $H(f)$, the second derivative test tells us If $det(H)(x_0,y_0)> 0$, and $f_{xx}(x_0, y_0) < 0$, then $(x_0,y_0)$ is a local maximum. If $det(H)(x_0,y_0)> 0$, and $f_{xx}(x_0, y_0) > 0$, then $(x_0,y_0)$ is a local minimum. If $det(H)(x_0,y_0)< 0$, then $(x_0,y_0)$ is a saddle point. If $det(H)(x_0,y_0)=0$, then the test is inconclusive. What I am most interested in is the last of these bullets. What is there left to do besides look at the graph in case the second derivative test yields inconclusive results? For single-variable functions, you would use a higher order derivative. However, a higher order derivative would have $2^o$ derivatives (for order $o$), so what calculation would be necessary to identify the type of critical point in this case?",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
19,Graphing the surface $z = xy$,Graphing the surface,z = xy,"Let the surface $S \subset \mathbb{R}^3$ be the graph of the function $f:\mathbb{R}^2 \to \mathbb{R}, f (x, y) = xy$. Let $U$ be the portion of $S$ for which $x^2 + y^2 ≤ 2$ and let $C$ be the boundary curve of $U$. Sketch $S, U $ & $ C$. I've tried to use slices to draw $S$. So $z = xy$. Letting $x = 1$, then S in the $zy$-plane is the whole plain. Letting $x = 1$ then the $zy$-plane $S$ is the whole plane again; and letting $z = 1$ then $xy$-plane is occupied a series of hyperbolas. But how do I graph these together? Also, could someone please explain the concept of a boundary to me? Guessing - is it the set of inequalities that bound a graph? EDIT: Does it look like this?","Let the surface $S \subset \mathbb{R}^3$ be the graph of the function $f:\mathbb{R}^2 \to \mathbb{R}, f (x, y) = xy$. Let $U$ be the portion of $S$ for which $x^2 + y^2 ≤ 2$ and let $C$ be the boundary curve of $U$. Sketch $S, U $ & $ C$. I've tried to use slices to draw $S$. So $z = xy$. Letting $x = 1$, then S in the $zy$-plane is the whole plain. Letting $x = 1$ then the $zy$-plane $S$ is the whole plane again; and letting $z = 1$ then $xy$-plane is occupied a series of hyperbolas. But how do I graph these together? Also, could someone please explain the concept of a boundary to me? Guessing - is it the set of inequalities that bound a graph? EDIT: Does it look like this?",,"['calculus', 'multivariable-calculus', 'solid-geometry']"
20,Sketching the surface $x^2+y^2+4z^2 = 1$,Sketching the surface,x^2+y^2+4z^2 = 1,"Let the surface $S \subset \mathbb{R}^3$ be the solutions of the equation $g(x, y, z)$ $ = 1$ where $g(x,y,z)=x^2 +y^2 +4z^2$. Let $U$ be the finite region of S satisfying $z > 0$ and let $C$ be the boundary of $U$. Sketch the surface $S$, the region $U$ and boundary $C$. So I've trouble visualising this. I know I've to sketch $x^2+y^2+4z^2 = 1$, so I thought I should let $y=0$. Then $x^2+4z^2 = 1$ is an ellipse, which I can draw. Similarly, $y^2+4z^2 = 1$ is an ellipse. Letting $z = 0$ then $x^2+y^2 = 1$a circle of radius $1$. However, I'm unable to put this together in 3-D. Also, what's the definition of boundary? The set of inequalities that bound that part of the surface? EDIT: If sketch all three in 3D axis, I get this: The two ellipses are inside the circle, which is wrong. EDIT 2: EDIT 3: Should it look like this? But in that case I would like to ask what happened to the circle in the $xy$ plane?","Let the surface $S \subset \mathbb{R}^3$ be the solutions of the equation $g(x, y, z)$ $ = 1$ where $g(x,y,z)=x^2 +y^2 +4z^2$. Let $U$ be the finite region of S satisfying $z > 0$ and let $C$ be the boundary of $U$. Sketch the surface $S$, the region $U$ and boundary $C$. So I've trouble visualising this. I know I've to sketch $x^2+y^2+4z^2 = 1$, so I thought I should let $y=0$. Then $x^2+4z^2 = 1$ is an ellipse, which I can draw. Similarly, $y^2+4z^2 = 1$ is an ellipse. Letting $z = 0$ then $x^2+y^2 = 1$a circle of radius $1$. However, I'm unable to put this together in 3-D. Also, what's the definition of boundary? The set of inequalities that bound that part of the surface? EDIT: If sketch all three in 3D axis, I get this: The two ellipses are inside the circle, which is wrong. EDIT 2: EDIT 3: Should it look like this? But in that case I would like to ask what happened to the circle in the $xy$ plane?",,"['calculus', 'multivariable-calculus', 'solid-geometry']"
21,Calculation of Variational Derivative,Calculation of Variational Derivative,,"Following is from Olver's book on Lie groups and Differential Equations: Define the variational problem: \begin{eqnarray*}   \mathcal{L}= \int_\Omega L(x,u^{(n)})   \end{eqnarray*} where $u^{(n)}$ represents partial derivatives upto n'th order. L is smooth. Given $f,\eta: \mathbb{R}^m \rightarrow \mathbb{R}^q$. $\eta$ be a function with compact support in $\Omega$. We want to calculate the formula for variational derivative:  \begin{eqnarray*}   \frac{d}{d\epsilon}\Big|_{\epsilon=0}\mathcal{L}[f + \epsilon\eta] &=& \int_{\Omega} \frac{d}{d\epsilon} \Big|_{\epsilon = 0} L(x,pr^{(n)}(f+\epsilon\eta)(x))dx \\    &=& \int_{\Omega} \Big\{ \sum_{\alpha,J}\frac{\partial L}{\partial u^{\alpha}_{J}}(x,pr^{(n)}f(x)) . \partial_{J}\eta^{\alpha}(x)\Big\}  dx    \end{eqnarray*} where $\frac{\partial L}{\partial u^{\alpha}_J}$ is derivative of $L$ with respect to $u^{\alpha}_J$, $J$ being the multi-index representing the ``$J^{th}$'' derivative of $u^{\alpha}, 1 \leq \alpha \leq q, |J| \leq n$ and $pr^{(n)}$ is used for prolongation. Now this is the part I don't understand: Since $\eta$ has compact support, we can use the divergence theorem to integrate the latter expression by parts, with the boundary terms on $\partial\Omega$ vanishing. Each partial derivative $\partial/\partial x^j$, when applied to the derivatives $\partial L/\partial U^{\alpha}_{J}$ of the Lagrangian, becomes the total derivative $D_j$ since L depends on $x$ through the function $u=f(x)$ \begin{eqnarray*}   \frac{d}{d\epsilon}\Big|_{\epsilon=0}\mathcal{L}[f + \epsilon\eta] &=& \int_{\Omega}\Big\{\sum_{\alpha}\Big[\sum_{J}(-D)_J\frac{\partial L}{\partial u^{\alpha}_J}(x,pr^{(n)}f(x))\Big]\eta^{\alpha}(x) \Big\}dx   \end{eqnarray*} where, for $J=(j_1,...,j_k)$, \begin{equation*}   (-D)_{J} = (-1)^kD_{J} = (-D_{j_1})(-D_{j_2})...(-D_{j_k})   \end{equation*} I understand that the integration by parts formula for higher dimension https://en.wikipedia.org/wiki/Green's_identities has to be used, but unable to fit this equation into the formula to get the desired result. Need help filling this gap","Following is from Olver's book on Lie groups and Differential Equations: Define the variational problem: \begin{eqnarray*}   \mathcal{L}= \int_\Omega L(x,u^{(n)})   \end{eqnarray*} where $u^{(n)}$ represents partial derivatives upto n'th order. L is smooth. Given $f,\eta: \mathbb{R}^m \rightarrow \mathbb{R}^q$. $\eta$ be a function with compact support in $\Omega$. We want to calculate the formula for variational derivative:  \begin{eqnarray*}   \frac{d}{d\epsilon}\Big|_{\epsilon=0}\mathcal{L}[f + \epsilon\eta] &=& \int_{\Omega} \frac{d}{d\epsilon} \Big|_{\epsilon = 0} L(x,pr^{(n)}(f+\epsilon\eta)(x))dx \\    &=& \int_{\Omega} \Big\{ \sum_{\alpha,J}\frac{\partial L}{\partial u^{\alpha}_{J}}(x,pr^{(n)}f(x)) . \partial_{J}\eta^{\alpha}(x)\Big\}  dx    \end{eqnarray*} where $\frac{\partial L}{\partial u^{\alpha}_J}$ is derivative of $L$ with respect to $u^{\alpha}_J$, $J$ being the multi-index representing the ``$J^{th}$'' derivative of $u^{\alpha}, 1 \leq \alpha \leq q, |J| \leq n$ and $pr^{(n)}$ is used for prolongation. Now this is the part I don't understand: Since $\eta$ has compact support, we can use the divergence theorem to integrate the latter expression by parts, with the boundary terms on $\partial\Omega$ vanishing. Each partial derivative $\partial/\partial x^j$, when applied to the derivatives $\partial L/\partial U^{\alpha}_{J}$ of the Lagrangian, becomes the total derivative $D_j$ since L depends on $x$ through the function $u=f(x)$ \begin{eqnarray*}   \frac{d}{d\epsilon}\Big|_{\epsilon=0}\mathcal{L}[f + \epsilon\eta] &=& \int_{\Omega}\Big\{\sum_{\alpha}\Big[\sum_{J}(-D)_J\frac{\partial L}{\partial u^{\alpha}_J}(x,pr^{(n)}f(x))\Big]\eta^{\alpha}(x) \Big\}dx   \end{eqnarray*} where, for $J=(j_1,...,j_k)$, \begin{equation*}   (-D)_{J} = (-1)^kD_{J} = (-D_{j_1})(-D_{j_2})...(-D_{j_k})   \end{equation*} I understand that the integration by parts formula for higher dimension https://en.wikipedia.org/wiki/Green's_identities has to be used, but unable to fit this equation into the formula to get the desired result. Need help filling this gap",,"['multivariable-calculus', 'vector-analysis', 'calculus-of-variations']"
22,Confused on surface integral problem,Confused on surface integral problem,,"I am asked to evaluate $$\iint_{S} [ \nabla \phi \times \nabla \psi] \bullet n dS$$ where $\phi=(x+y+z)^2$ and $\psi=x^2-y^2+z^2$ where S is the curved surface of the hemisphere $x^2+y^2+z^2=1$ , $z \ge 0$. My attempts: I tried using that if$ \bar A= \phi \nabla \psi$ then $\nabla \times \bar A = \nabla \phi \times \nabla \psi$ So I calculated $$\phi \nabla \psi=(2x(x+y+z)^2,-2y(x+y+z)^2,2z(x+y+z)^2)$$ So essentially according to above this is what I need to integrate over S. For my normal I thought this would be obtained by writing $$G(x,y,z)=1-x^2-y^2-z^2$$ and solving for the gradient and dividing by its norm. Doing this i get that $$\hat n=\frac{(x)i+(y)j+(z)k}{x^2+y^2+z^2}$$ For the dS, I thought if it is of the form $z=f(x,y)$ then we can obtain the dS as $\sqrt{1+f_{x}^2+f_{y}^2}$ ie $dS=\frac{1}{\sqrt{x^2+y^2}}$ But I have no clue if I am on the right path. I feel like I am not confident on it. Can anyone help? Any advice would be appreciated, I have been trying for several hours with no progress","I am asked to evaluate $$\iint_{S} [ \nabla \phi \times \nabla \psi] \bullet n dS$$ where $\phi=(x+y+z)^2$ and $\psi=x^2-y^2+z^2$ where S is the curved surface of the hemisphere $x^2+y^2+z^2=1$ , $z \ge 0$. My attempts: I tried using that if$ \bar A= \phi \nabla \psi$ then $\nabla \times \bar A = \nabla \phi \times \nabla \psi$ So I calculated $$\phi \nabla \psi=(2x(x+y+z)^2,-2y(x+y+z)^2,2z(x+y+z)^2)$$ So essentially according to above this is what I need to integrate over S. For my normal I thought this would be obtained by writing $$G(x,y,z)=1-x^2-y^2-z^2$$ and solving for the gradient and dividing by its norm. Doing this i get that $$\hat n=\frac{(x)i+(y)j+(z)k}{x^2+y^2+z^2}$$ For the dS, I thought if it is of the form $z=f(x,y)$ then we can obtain the dS as $\sqrt{1+f_{x}^2+f_{y}^2}$ ie $dS=\frac{1}{\sqrt{x^2+y^2}}$ But I have no clue if I am on the right path. I feel like I am not confident on it. Can anyone help? Any advice would be appreciated, I have been trying for several hours with no progress",,"['calculus', 'multivariable-calculus', 'vector-analysis', 'surface-integrals']"
23,Show that a function has bounded support,Show that a function has bounded support,,"Definition in my book: A function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ has bounded support if there exists a closed interval $I$ in $\mathbb{R}^n$ such that $f(x)=0$ if $x \notin I$. Now I have to show that if $f$ and $g$ have bounded support (in $\mathbb{R}^n$) and $c \in \mathbb{R}$, then $f+g$ and $cf$ have bounded support too. This is what I did: Since $f$ and $g$ have bounded support, there exist $I_1$ and $I_2$ such that $supp(f)=I_1$ and $supp(g)=I_2$. Then I can state $supp(f+g) \subseteq I_1 \cup I_2$ and $supp(cf) \subseteq I_1$, but that doesn't prove that $f+g$ and $cf$ have bounded support, does it? I believe it only shows that if a bounded support exists, it would a subset of $I_1 \cup I_2$ or more important it would be finite. Any thoughts on the matter? Can the bounded support be an empty set?","Definition in my book: A function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ has bounded support if there exists a closed interval $I$ in $\mathbb{R}^n$ such that $f(x)=0$ if $x \notin I$. Now I have to show that if $f$ and $g$ have bounded support (in $\mathbb{R}^n$) and $c \in \mathbb{R}$, then $f+g$ and $cf$ have bounded support too. This is what I did: Since $f$ and $g$ have bounded support, there exist $I_1$ and $I_2$ such that $supp(f)=I_1$ and $supp(g)=I_2$. Then I can state $supp(f+g) \subseteq I_1 \cup I_2$ and $supp(cf) \subseteq I_1$, but that doesn't prove that $f+g$ and $cf$ have bounded support, does it? I believe it only shows that if a bounded support exists, it would a subset of $I_1 \cup I_2$ or more important it would be finite. Any thoughts on the matter? Can the bounded support be an empty set?",,['multivariable-calculus']
24,Find the volume of the tetrahedron using triple integrals,Find the volume of the tetrahedron using triple integrals,,"Find the volume of the tetrahedron with vertices $(0,0,0), (0,0,1), (1,0,1), (0,1,1)$, equation $$x+y=z$$ bounded by $x=0$, $y=0$ and $z=1$. Then $x+y=1$. My integral: is this correct? $$\int_0^1 \int_0^{z} \int_0^{z-y}  \,dx\,dy\,dz$$","Find the volume of the tetrahedron with vertices $(0,0,0), (0,0,1), (1,0,1), (0,1,1)$, equation $$x+y=z$$ bounded by $x=0$, $y=0$ and $z=1$. Then $x+y=1$. My integral: is this correct? $$\int_0^1 \int_0^{z} \int_0^{z-y}  \,dx\,dy\,dz$$",,"['multivariable-calculus', 'volume', 'multiple-integral']"
25,Shortest path between points on intersecting planes,Shortest path between points on intersecting planes,,"I have two intersecting planes and two arbitrary points $\mathbf{p}_1$ and $ \mathbf{p}_2$, one on each plane. I would like to calculate the minimum distance of a path from one point to the other with the path constrained to the planes. This is my current method for calculating the minimum distance: Find an equation for the intersection of the two planes, let's call it $\mathbf{\ell}(t)$. Then by minimizing the following distance function I should get the minimum distance: $\| \mathbf{\ell}(s)-\mathbf{p}_1 \| + \|\mathbf{\ell}(s)-\mathbf{p}_2 \|$. We differentiate to get $\frac{(\mathbf{\ell}(s) - \mathbf{p}_1) \cdot \nabla \mathbf{\ell}}{\| \mathbf{\ell}(s) - \mathbf{p}_1 \|}+\frac{(\mathbf{\ell}(s) - \mathbf{p}_2) \cdot \nabla \mathbf{\ell}}{\| \mathbf{\ell}(s) - \mathbf{p}_2 \|}=0$ I can solve this equation; however, the formula is rather unruly. I was wondering if there is a simple solution to this or a better approach. For instance is the shortest path the projection of the straight line (in $\mathbb{R}^3$) between $\mathbf{p}_1$ and $\mathbf{p}_2$ onto the two planes? Thanks in advance.","I have two intersecting planes and two arbitrary points $\mathbf{p}_1$ and $ \mathbf{p}_2$, one on each plane. I would like to calculate the minimum distance of a path from one point to the other with the path constrained to the planes. This is my current method for calculating the minimum distance: Find an equation for the intersection of the two planes, let's call it $\mathbf{\ell}(t)$. Then by minimizing the following distance function I should get the minimum distance: $\| \mathbf{\ell}(s)-\mathbf{p}_1 \| + \|\mathbf{\ell}(s)-\mathbf{p}_2 \|$. We differentiate to get $\frac{(\mathbf{\ell}(s) - \mathbf{p}_1) \cdot \nabla \mathbf{\ell}}{\| \mathbf{\ell}(s) - \mathbf{p}_1 \|}+\frac{(\mathbf{\ell}(s) - \mathbf{p}_2) \cdot \nabla \mathbf{\ell}}{\| \mathbf{\ell}(s) - \mathbf{p}_2 \|}=0$ I can solve this equation; however, the formula is rather unruly. I was wondering if there is a simple solution to this or a better approach. For instance is the shortest path the projection of the straight line (in $\mathbb{R}^3$) between $\mathbf{p}_1$ and $\mathbf{p}_2$ onto the two planes? Thanks in advance.",,['multivariable-calculus']
26,Proving $f$ is differentiable,Proving  is differentiable,f,"Let $f(x,y) = \begin{cases} 3^k,  & \text{if $2^{k-1}<||(x,y)||\le2^k$} \\ 0, & \text{if (x,y) = (0,0) } \end{cases}$ Prove $f$ is differentiable at $(0,0)$ I've tried proving that partial derivatives are defined with no success, and I don't see how do I use the definition of differentiability here. Thanks","Let $f(x,y) = \begin{cases} 3^k,  & \text{if $2^{k-1}<||(x,y)||\le2^k$} \\ 0, & \text{if (x,y) = (0,0) } \end{cases}$ Prove $f$ is differentiable at $(0,0)$ I've tried proving that partial derivatives are defined with no success, and I don't see how do I use the definition of differentiability here. Thanks",,"['calculus', 'real-analysis', 'multivariable-calculus']"
27,"Derivative of $\frac{x^py^q}{x^2+y^2};\; p,q\in\mathbb{N}$",Derivative of,"\frac{x^py^q}{x^2+y^2};\; p,q\in\mathbb{N}","Let $$ f(x,y) = \left\{\begin{array}{ll} \frac{x^py^q}{x^2+y^2}&\quad \text{for } (x,y)\neq (0,0)\\ 0 &\quad \text{for } (x,y) = (0,0) \end{array}\right.. $$ Find all $p,q \in \mathbb{N}$ so that $\partial f/ \partial x$ and $\partial f/ \partial y$ exist. $\partial f/ \partial x$ and $\partial f/ \partial y$ are continuous. $f$ is differentiable. I've only been able to work out continuity. On the path $(x,x)$, $f(x,x) = x^{p+q}/(2x^2)$. So for $f$ to be continuous, $p+q \ge 3$. If $p+q \ge 3$, $p$ or $q\ge 2$, so WLOG assume $q \ge 2$ and $p\ge 1$. Then $$ |f(x,y)| = \left|\frac{x^py^q}{x^2+y^2}\right| \le \frac{|x^p||y^2||y^{q-2}|}{|y^2|} \to 0 \text{ as } (x,y) \to (0,0). $$ The partial derivatives on $\mathbb{R}^2\setminus (0,0)$ are $$ \partial f/\partial x = \frac{(p-2)x^{p+1}y^q + px^{p-1}y^{q+2}}{(x^2+y^2)^2} $$ $$ \partial f/\partial y = \frac{(q-2)x^{p}y^{q+1} + qx^{p+2}y^{q-1}}{(x^2+y^2)^2} $$ My guess for continuity of the partials is $p+q\ge 4$ but I can't confirm it. Please help! Is there a general strategy about investigating the derivatives of functions with parameters $p,q$ like this? Thank you!","Let $$ f(x,y) = \left\{\begin{array}{ll} \frac{x^py^q}{x^2+y^2}&\quad \text{for } (x,y)\neq (0,0)\\ 0 &\quad \text{for } (x,y) = (0,0) \end{array}\right.. $$ Find all $p,q \in \mathbb{N}$ so that $\partial f/ \partial x$ and $\partial f/ \partial y$ exist. $\partial f/ \partial x$ and $\partial f/ \partial y$ are continuous. $f$ is differentiable. I've only been able to work out continuity. On the path $(x,x)$, $f(x,x) = x^{p+q}/(2x^2)$. So for $f$ to be continuous, $p+q \ge 3$. If $p+q \ge 3$, $p$ or $q\ge 2$, so WLOG assume $q \ge 2$ and $p\ge 1$. Then $$ |f(x,y)| = \left|\frac{x^py^q}{x^2+y^2}\right| \le \frac{|x^p||y^2||y^{q-2}|}{|y^2|} \to 0 \text{ as } (x,y) \to (0,0). $$ The partial derivatives on $\mathbb{R}^2\setminus (0,0)$ are $$ \partial f/\partial x = \frac{(p-2)x^{p+1}y^q + px^{p-1}y^{q+2}}{(x^2+y^2)^2} $$ $$ \partial f/\partial y = \frac{(q-2)x^{p}y^{q+1} + qx^{p+2}y^{q-1}}{(x^2+y^2)^2} $$ My guess for continuity of the partials is $p+q\ge 4$ but I can't confirm it. Please help! Is there a general strategy about investigating the derivatives of functions with parameters $p,q$ like this? Thank you!",,"['calculus', 'real-analysis', 'multivariable-calculus']"
28,Continuity of two variable function,Continuity of two variable function,,"I've got the following exercise and I need some help trying to figure out what's a proper way of proving it: Take the function $$f(x,y)= \begin{cases} \sqrt{1-x^2-y^2},  & \text{if $x^2+y^2<1$} \\ 0, & \text{if $x^2+y^2\ge1$} \end{cases} $$ And the question, is prove that the function is continuous on $ R^2 $ Is it enough to say that if you plug $x^2+y^2=1$ (which are the only set points where discontinuity could happen) on both equations and see that they both equal to zero, it means that the function will be continuous at those points because the limit is always zero no matter where you approach it from?","I've got the following exercise and I need some help trying to figure out what's a proper way of proving it: Take the function $$f(x,y)= \begin{cases} \sqrt{1-x^2-y^2},  & \text{if $x^2+y^2<1$} \\ 0, & \text{if $x^2+y^2\ge1$} \end{cases} $$ And the question, is prove that the function is continuous on $ R^2 $ Is it enough to say that if you plug $x^2+y^2=1$ (which are the only set points where discontinuity could happen) on both equations and see that they both equal to zero, it means that the function will be continuous at those points because the limit is always zero no matter where you approach it from?",,"['limits', 'multivariable-calculus', 'continuity']"
29,Can't understand partial derivatives identity,Can't understand partial derivatives identity,,"I'm studying multivariable calculus, and can't understand this. Could someone derive/explain this identity? $$\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x} = -1$$ Edit: $x$, $y$ , $z$ are real-valued functions with multiple variables $$x = x(y,z)$$ $$y = y(x,z)$$ $$z = z(x,y)$$","I'm studying multivariable calculus, and can't understand this. Could someone derive/explain this identity? $$\frac{\partial x}{\partial y}\frac{\partial y}{\partial z}\frac{\partial z}{\partial x} = -1$$ Edit: $x$, $y$ , $z$ are real-valued functions with multiple variables $$x = x(y,z)$$ $$y = y(x,z)$$ $$z = z(x,y)$$",,"['real-analysis', 'multivariable-calculus']"
30,A $k+1$ Manifold whose boundary is the solution set to the equation $f(\vec{x})=\vec{0}$,A  Manifold whose boundary is the solution set to the equation,k+1 f(\vec{x})=\vec{0},"Let $f: \mathbb{R}^{n+k} \to \mathbb{R}^n$ be of class $C^r$. Let $f_1, ..., f_n$ be components of $f$. Define $$M=\{\vec{x} | f(\vec{x})=\vec{0}\},$$ $$N=\{\vec{x} | f_1(\vec{x})=0, ..., f_{n-1}(\vec{x})=0\, f_n(\vec{x})\geq 0\}.$$ Assume that (1). the Jacobian matrix $Df(\vec{x})$ has rank $n$ for each $\vec{x}\in M$ and that (2). the matrix $$\frac{\partial (f_1, ..., f_{n-1})}{\partial (\vec{x})}$$ has rank $n-1$ for each $\vec{x}\in N$. Prove that $N$ is a $k+1$-manifold whose boundary is $M$. Attempt I've proved that $M$ is a $k$-manifold. Define $F: \mathbb{R}^{n+k} \to \mathbb{R}^{n+k}$ by $$F(x_1, ..., x_k, x_{k+1}, ..., x_{n+k})=(x_1, ..., x_k, f_1(\vec{x}), ..., f_n(\vec{x})).$$ The given $\vec{x}\in M$, we know that $DF(\vec{x})$ has rank $n+k$, hence is non-singular, since $Df(\vec{x})$ has rank $n$. So $F$ is a diffeomorphism of class $C^r$ (Inverse Function Theorem) of a neighborhood $A$ of $\vec{x}$ in $\mathbb{R}^{n+k}$ with an open set $B$ in $\mathbb{R}^{n+k}$. $F$ carries the open set $V=A\cap M$ of $M$ onto the open set $B\cap (\mathbb{R}^k \times {0}^n)$ of $\mathbb{R}^k \times {0}^n$. Let $\pi: \mathbb{R}^k \times {0}^n \to \mathbb{R}^k$ be the projection from $\mathbb{R}^k \times {0}^n$ onto $\mathbb{R}^k$. Then $\pi$ is an open map, hence carries $B\cap (\mathbb{R}^k \times {0}^n)$ onto an open set $U$ in $\mathbb{R}^{k}$. Now it is easy to verify that $\alpha: \mathbb{R}^k \to \mathbb{R}^{n+k}$ defined by $\alpha=(\pi \circ F)^{-1}=F^{-1}\circ \pi^{-1}$ is a bijection of class $C^r$ whose inverse is continuous on $V$. Moreover, $D\alpha (\vec{x})$ has rank $k$ for $\vec{x}\in M$. Therefore $\alpha$ is a coordinate patch, so $M$ is a $k$-manifold. However, after manipulation with the definition of $N$, I cannot find a coordinate patch $g: \mathbb{R}^{k+1} \to \mathbb{R}^{n+k}$ on $N$. Can anyone suggest how to prove that $N$ is a $k+1$-manifold? Thank you!","Let $f: \mathbb{R}^{n+k} \to \mathbb{R}^n$ be of class $C^r$. Let $f_1, ..., f_n$ be components of $f$. Define $$M=\{\vec{x} | f(\vec{x})=\vec{0}\},$$ $$N=\{\vec{x} | f_1(\vec{x})=0, ..., f_{n-1}(\vec{x})=0\, f_n(\vec{x})\geq 0\}.$$ Assume that (1). the Jacobian matrix $Df(\vec{x})$ has rank $n$ for each $\vec{x}\in M$ and that (2). the matrix $$\frac{\partial (f_1, ..., f_{n-1})}{\partial (\vec{x})}$$ has rank $n-1$ for each $\vec{x}\in N$. Prove that $N$ is a $k+1$-manifold whose boundary is $M$. Attempt I've proved that $M$ is a $k$-manifold. Define $F: \mathbb{R}^{n+k} \to \mathbb{R}^{n+k}$ by $$F(x_1, ..., x_k, x_{k+1}, ..., x_{n+k})=(x_1, ..., x_k, f_1(\vec{x}), ..., f_n(\vec{x})).$$ The given $\vec{x}\in M$, we know that $DF(\vec{x})$ has rank $n+k$, hence is non-singular, since $Df(\vec{x})$ has rank $n$. So $F$ is a diffeomorphism of class $C^r$ (Inverse Function Theorem) of a neighborhood $A$ of $\vec{x}$ in $\mathbb{R}^{n+k}$ with an open set $B$ in $\mathbb{R}^{n+k}$. $F$ carries the open set $V=A\cap M$ of $M$ onto the open set $B\cap (\mathbb{R}^k \times {0}^n)$ of $\mathbb{R}^k \times {0}^n$. Let $\pi: \mathbb{R}^k \times {0}^n \to \mathbb{R}^k$ be the projection from $\mathbb{R}^k \times {0}^n$ onto $\mathbb{R}^k$. Then $\pi$ is an open map, hence carries $B\cap (\mathbb{R}^k \times {0}^n)$ onto an open set $U$ in $\mathbb{R}^{k}$. Now it is easy to verify that $\alpha: \mathbb{R}^k \to \mathbb{R}^{n+k}$ defined by $\alpha=(\pi \circ F)^{-1}=F^{-1}\circ \pi^{-1}$ is a bijection of class $C^r$ whose inverse is continuous on $V$. Moreover, $D\alpha (\vec{x})$ has rank $k$ for $\vec{x}\in M$. Therefore $\alpha$ is a coordinate patch, so $M$ is a $k$-manifold. However, after manipulation with the definition of $N$, I cannot find a coordinate patch $g: \mathbb{R}^{k+1} \to \mathbb{R}^{n+k}$ on $N$. Can anyone suggest how to prove that $N$ is a $k+1$-manifold? Thank you!",,"['real-analysis', 'multivariable-calculus', 'manifolds', 'manifolds-with-boundary']"
31,How does Green's theorem apply here?,How does Green's theorem apply here?,,"Let $D$ be the region delimited by  $$\partial D: \begin{cases} C_1: x^2 + y^2 = 5^2\\ C_2:(x-2)^2+y^2= 1\\ C_3:(x+2)^2+y^2 = 1\\ C_4: x^2+(y-2)^2= 1\\ C_5: x^2+(y+2)^2= 1 \end{cases} $$ I've sketched this and the region looks like this (the inside of the big circle, intersection the outside of the small circles should be shaded): So the boundary of the region is not a closed curve... but I'm asked to verify Green's theorem for this region anyway. I don't understand how the theorem applies, could someone explain this? E: From the comments it looks like I should do $$\iint_D (Q_x-P_y)dA=\oint_{C_1} Fds-\sum_{i\ge 2} \oint_{C_i}Fds$$ Is this correct? I don't see how this follows from Green's theorem's statement (using wikipedia for reference).","Let $D$ be the region delimited by  $$\partial D: \begin{cases} C_1: x^2 + y^2 = 5^2\\ C_2:(x-2)^2+y^2= 1\\ C_3:(x+2)^2+y^2 = 1\\ C_4: x^2+(y-2)^2= 1\\ C_5: x^2+(y+2)^2= 1 \end{cases} $$ I've sketched this and the region looks like this (the inside of the big circle, intersection the outside of the small circles should be shaded): So the boundary of the region is not a closed curve... but I'm asked to verify Green's theorem for this region anyway. I don't understand how the theorem applies, could someone explain this? E: From the comments it looks like I should do $$\iint_D (Q_x-P_y)dA=\oint_{C_1} Fds-\sum_{i\ge 2} \oint_{C_i}Fds$$ Is this correct? I don't see how this follows from Green's theorem's statement (using wikipedia for reference).",,"['integration', 'multivariable-calculus', 'greens-theorem']"
32,"Show that the vector field $\vec F=(xf(u),xg(u))$ is not conservative",Show that the vector field  is not conservative,"\vec F=(xf(u),xg(u))","I'm trying to prove that the vector field  $\vec F=(xf(u),xg(u))$ with $u=xy$ is not conservative. I suppose that there is a function $\phi$ so that $\nabla \phi= \vec F$. So I need to satisfy that: $$\frac {\partial \phi}{\partial x}= yf(u)$$ $$\frac {\partial \phi}{\partial y}= xg(u)$$ Calculating the mixed partial for each I get that: $$\frac {\partial^2 \phi}{\partial x \partial y}= f(u)+y \frac{\partial f(u)}{\partial y}$$ $$\frac {\partial^2 \phi}{\partial y \partial x}= g(u)+x \frac{\partial g(u)}{\partial x}$$ Since mixed partials are equal I have that: $$f(u)+y \frac{\partial f(u)}{\partial y}=g(u)+x \frac{\partial g(u)}{\partial x}$$ and $\frac {\partial f(u)}{\partial y}=\frac {df}{du}\frac {\partial u}{\partial y}= x\frac{df}{du}$, applying this to both sides I end up with: $$f(u)+u \frac{df}{du}=g(u)+u \frac{dg}{du}$$ This is where I'm stuck, I'm not quite sure how to get a contradiction out of this or how I can show that this statement cannot hold. Should I have taken a different approach to this problem?","I'm trying to prove that the vector field  $\vec F=(xf(u),xg(u))$ with $u=xy$ is not conservative. I suppose that there is a function $\phi$ so that $\nabla \phi= \vec F$. So I need to satisfy that: $$\frac {\partial \phi}{\partial x}= yf(u)$$ $$\frac {\partial \phi}{\partial y}= xg(u)$$ Calculating the mixed partial for each I get that: $$\frac {\partial^2 \phi}{\partial x \partial y}= f(u)+y \frac{\partial f(u)}{\partial y}$$ $$\frac {\partial^2 \phi}{\partial y \partial x}= g(u)+x \frac{\partial g(u)}{\partial x}$$ Since mixed partials are equal I have that: $$f(u)+y \frac{\partial f(u)}{\partial y}=g(u)+x \frac{\partial g(u)}{\partial x}$$ and $\frac {\partial f(u)}{\partial y}=\frac {df}{du}\frac {\partial u}{\partial y}= x\frac{df}{du}$, applying this to both sides I end up with: $$f(u)+u \frac{df}{du}=g(u)+u \frac{dg}{du}$$ This is where I'm stuck, I'm not quite sure how to get a contradiction out of this or how I can show that this statement cannot hold. Should I have taken a different approach to this problem?",,"['multivariable-calculus', 'vector-fields', 'vector-analysis']"
33,Limit of a multivariable piecewise function,Limit of a multivariable piecewise function,,"Let $f:\mathbb{R^2}\to\mathbb{R}$ given by: $f(x,y) =   \begin{cases}        x-y+1     & \text{if }xy\geq 0 \\       y-x-1  & \text{if } xy<0 \\   \end{cases}$ I need to compute $\lim_{(x,y)\to(0,1)}f(x,y)$, but I'm not sure about the following reasoning: The problem here is that $f$, close to $(0,1)$, corresponds to different formulas. If $D_1=\{(x,y)\in\mathbb{R^2}: xy\geq0\}$ and $D_2=(D_1)^C$, we have $\lim_{(x,y)\to(0,1),(x,y)\in D_1}f(x,y)=0$ and the same for $(x,y)\in D_2$, because $f$ is continuous on $D_1$ and $D_2$. So, can I conclude that the limit is $0$? My english isn't very good; if you find any errors, please correct them.","Let $f:\mathbb{R^2}\to\mathbb{R}$ given by: $f(x,y) =   \begin{cases}        x-y+1     & \text{if }xy\geq 0 \\       y-x-1  & \text{if } xy<0 \\   \end{cases}$ I need to compute $\lim_{(x,y)\to(0,1)}f(x,y)$, but I'm not sure about the following reasoning: The problem here is that $f$, close to $(0,1)$, corresponds to different formulas. If $D_1=\{(x,y)\in\mathbb{R^2}: xy\geq0\}$ and $D_2=(D_1)^C$, we have $\lim_{(x,y)\to(0,1),(x,y)\in D_1}f(x,y)=0$ and the same for $(x,y)\in D_2$, because $f$ is continuous on $D_1$ and $D_2$. So, can I conclude that the limit is $0$? My english isn't very good; if you find any errors, please correct them.",,"['calculus', 'limits', 'multivariable-calculus']"
34,There are infinitely many projections,There are infinitely many projections,,Can anyone explain please why such projections are infinitely many?,Can anyone explain please why such projections are infinitely many?,,"['multivariable-calculus', 'operator-theory']"
35,Volume enclosed by $(x^2+y^2+z^2)^2=x$,Volume enclosed by,(x^2+y^2+z^2)^2=x,"I need to calculate the volume of solid enclosed by the surface $(x^2+y^2+z^2)^2=x$, using only spherical coordinates. My attempt: by changing coordinates to spherical: $x=r\sin\phi\cos\theta~,~y=r\sin\phi\sin\theta~,~z=r\cos\phi$ we obtain the Jacobian $J=r^2\sin\phi$. When $\phi$ and $\theta$ are fixed, $r$ varies from $0$ to $\sqrt[3]{\sin\phi\cos\theta}$ (because $r^4=r\sin\phi\cos\theta$). Keeping $\theta$ fixed, we let $\phi$ vary from $0$ to $\pi$. Thus the volume equals: $$V=\int\limits_{0}^{\pi}\int\limits_{0}^{\pi}\int\limits_{0}^{\sqrt[3]{\sin\phi\cos\theta}}r^2\sin\phi ~dr ~d\phi ~d\theta=0$$ Which is obviously wrong. What am I doing wrong?","I need to calculate the volume of solid enclosed by the surface $(x^2+y^2+z^2)^2=x$, using only spherical coordinates. My attempt: by changing coordinates to spherical: $x=r\sin\phi\cos\theta~,~y=r\sin\phi\sin\theta~,~z=r\cos\phi$ we obtain the Jacobian $J=r^2\sin\phi$. When $\phi$ and $\theta$ are fixed, $r$ varies from $0$ to $\sqrt[3]{\sin\phi\cos\theta}$ (because $r^4=r\sin\phi\cos\theta$). Keeping $\theta$ fixed, we let $\phi$ vary from $0$ to $\pi$. Thus the volume equals: $$V=\int\limits_{0}^{\pi}\int\limits_{0}^{\pi}\int\limits_{0}^{\sqrt[3]{\sin\phi\cos\theta}}r^2\sin\phi ~dr ~d\phi ~d\theta=0$$ Which is obviously wrong. What am I doing wrong?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
36,Uniqueness of derivative in $\mathbb{R}^n$ [closed],Uniqueness of derivative in  [closed],\mathbb{R}^n,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question There is one confusing moment. How did Rudin get (16)? My though is the following: Since $\dfrac{|B\mathbf{h}|}{|\mathbf{h}|}\to 0$ as $\mathbf{h}\to \mathbf{0}$. Fixing $\mathbf{h}\neq\mathbf{0}$ if $t\to 0$ then $t\mathbf{h}\to 0$. Hence $\dfrac{|B(t\mathbf{h})|}{|t\mathbf{h}|}\to 0$ by above. Am I right? Any comment is greatly appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question There is one confusing moment. How did Rudin get (16)? My though is the following: Since $\dfrac{|B\mathbf{h}|}{|\mathbf{h}|}\to 0$ as $\mathbf{h}\to \mathbf{0}$. Fixing $\mathbf{h}\neq\mathbf{0}$ if $t\to 0$ then $t\mathbf{h}\to 0$. Hence $\dfrac{|B(t\mathbf{h})|}{|t\mathbf{h}|}\to 0$ by above. Am I right? Any comment is greatly appreciated.",,['multivariable-calculus']
37,"Why is $F=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j}, (x,y)\neq 0$ not conservative?",Why is  not conservative?,"F=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j}, (x,y)\neq 0","My book says that $$F=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j}, (x,y)\neq 0$$ is not conservative (besides $curl(F)$ being $0$), so I cannot use the theorem that $$\int_\gamma \vec{F} \cdot d\vec{r} = \int_a^b \nabla\phi(g(t))g'(t)dt =  \phi(b)-\phi(a)$$ but I can find $\phi$ such that $\nabla\phi = F$, so why this vector field cannot be conservative?","My book says that $$F=\frac{-y}{x^2+y^2}\vec{i} + \frac{x}{x^2+y^2}\vec{j}, (x,y)\neq 0$$ is not conservative (besides $curl(F)$ being $0$), so I cannot use the theorem that $$\int_\gamma \vec{F} \cdot d\vec{r} = \int_a^b \nabla\phi(g(t))g'(t)dt =  \phi(b)-\phi(a)$$ but I can find $\phi$ such that $\nabla\phi = F$, so why this vector field cannot be conservative?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
38,"Gauss´s law proof ""details""","Gauss´s law proof ""details""",,"I know that this question has already been asked multiple times but I´m still not getting on the mathematical details behind the answers... So I hope that this question doesn´t get closed; also I already published it in https://physics.stackexchange.com/ but unfortunately nobody answered me and I´m guessing because this is more a mathematical question rather than a physics question. First I will consider the case of discrete charges  (I suppose that there are $M$ charges inside the volume and $N-M$ charges outside the volume; hence a total of $N$ charges): $$\begin{align} \oint_{\partial\Omega}\vec{E}\cdot\vec{dA} & =\int_{\Omega}\nabla\cdot \vec{E} dV \\ & = {1\over 4\pi\epsilon_0}\sum_{k=1}^{N}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV \\ & = {1\over 4\pi\epsilon_0}\sum_{k=1}^{M}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV +  {1\over 4\pi\epsilon_0}\sum_{k=M+1}^{N}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV \end{align}$$ Note that in the second term of the last equality is zero because we are just integrating over $\Omega$ and in this term the charges are outside $\Omega$ hence: $\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV=0$ Know we can compute the first term of the last equality using the fact that: $\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})=4\pi\delta(\vec{r}-\vec{r_k})$ $${1\over 4\pi\epsilon_0}\sum_{k=1}^{M}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV={1\over \epsilon_0}\sum_{K=1}^{M}q_k\int_{\Omega}\delta(\vec{r}-\vec{r_k})dV={1\over \epsilon_0}\sum_{k=1}^{M}q_k$$ (In the last equality I´m not sure that I can use the fact that $\int_{\Omega}\delta(\vec{r}-\vec{r_k})dV=1$ because I´m not integrating over all the space) Now if we want to derive the differential form: $$\int_{\Omega}\nabla\cdot \vec{E}dV={1\over \epsilon_0}\sum_{k=1}^{M}q_k={1\over \epsilon_0}\sum_{k=1}^{M}q_k\int_{\Omega}\delta(\vec{r}-\vec{r_k})d^3\vec{r}={1\over \epsilon_0}\int_{\Omega}\rho(\vec{r})d^3\vec{r}$$ And then we conclude that $$\nabla\cdot E={1\over \epsilon_0}\rho$$ (becuase the equality of integrals is valid for all region $\Omega$) So if everthing is right then that is gauss law for discrete charges; Now what happens if a have a continuous charge that is inside $\Omega$? (for example a solid charged cone inside a sphere) this is the part where I´m having trouble: The electric field of a continuous dsitribution is given by $$E(\vec r)={1\over 4\pi\epsilon_0}\int_{\Omega´}\rho(\vec s)({\vec{r}-\vec{s}\over |\vec{r}-\vec{s}|^3})d^3\vec{s}$$ where $\Omega´$ is the charged region in the space we have that $$\begin{align} \nabla\cdot{E(\vec{r})} & = {1\over 4\pi\epsilon_0}\int_{\Omega´}\rho(\vec s)\nabla \cdot({\vec{r}-\vec{s}\over |\vec{r}-\vec{s}|^3})d^3\vec{s} (\text{differentiation under the integral sign})\\ & = {1\over \epsilon_0}\int_{\Omega´}\rho(\vec{s})\delta(\vec{r}-\vec{s})d^3\vec{s} \\ & = {1\over \epsilon_0}\rho(\vec{r}) (\text{shifting property of delta function}) \end{align}$$ (I have a big issue in this part becuase the density $\rho$ is just defined in the charged solid region $\Omega´$ so it doesn´t make sense to talk about the density in all the space;also i´m not sure if the last equality is valid because I´m not integrating over all the space) Know assuming that it makes sense to talk about $\rho$ in all the space then if we want to obtain the integral form we use the divergence theorem:  $$\oint_{\partial \Omega}E\cdot \vec{dS}=\int_{\Omega}\nabla\cdot\vec{E}dV={1\over \epsilon_0}\int_{\Omega}\rho(\vec{r})dV={Q_{int}\over \epsilon_0}$$ I would really appreciate if you can help me with the ""details"" of the proof, also any comments or suggestion would be highly appreciated.","I know that this question has already been asked multiple times but I´m still not getting on the mathematical details behind the answers... So I hope that this question doesn´t get closed; also I already published it in https://physics.stackexchange.com/ but unfortunately nobody answered me and I´m guessing because this is more a mathematical question rather than a physics question. First I will consider the case of discrete charges  (I suppose that there are $M$ charges inside the volume and $N-M$ charges outside the volume; hence a total of $N$ charges): $$\begin{align} \oint_{\partial\Omega}\vec{E}\cdot\vec{dA} & =\int_{\Omega}\nabla\cdot \vec{E} dV \\ & = {1\over 4\pi\epsilon_0}\sum_{k=1}^{N}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV \\ & = {1\over 4\pi\epsilon_0}\sum_{k=1}^{M}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV +  {1\over 4\pi\epsilon_0}\sum_{k=M+1}^{N}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV \end{align}$$ Note that in the second term of the last equality is zero because we are just integrating over $\Omega$ and in this term the charges are outside $\Omega$ hence: $\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV=0$ Know we can compute the first term of the last equality using the fact that: $\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})=4\pi\delta(\vec{r}-\vec{r_k})$ $${1\over 4\pi\epsilon_0}\sum_{k=1}^{M}q_k\int_{\Omega}\nabla\cdot({\vec{r}-\vec{r_k}\over |\vec{r}-\vec{r_k}|^3})dV={1\over \epsilon_0}\sum_{K=1}^{M}q_k\int_{\Omega}\delta(\vec{r}-\vec{r_k})dV={1\over \epsilon_0}\sum_{k=1}^{M}q_k$$ (In the last equality I´m not sure that I can use the fact that $\int_{\Omega}\delta(\vec{r}-\vec{r_k})dV=1$ because I´m not integrating over all the space) Now if we want to derive the differential form: $$\int_{\Omega}\nabla\cdot \vec{E}dV={1\over \epsilon_0}\sum_{k=1}^{M}q_k={1\over \epsilon_0}\sum_{k=1}^{M}q_k\int_{\Omega}\delta(\vec{r}-\vec{r_k})d^3\vec{r}={1\over \epsilon_0}\int_{\Omega}\rho(\vec{r})d^3\vec{r}$$ And then we conclude that $$\nabla\cdot E={1\over \epsilon_0}\rho$$ (becuase the equality of integrals is valid for all region $\Omega$) So if everthing is right then that is gauss law for discrete charges; Now what happens if a have a continuous charge that is inside $\Omega$? (for example a solid charged cone inside a sphere) this is the part where I´m having trouble: The electric field of a continuous dsitribution is given by $$E(\vec r)={1\over 4\pi\epsilon_0}\int_{\Omega´}\rho(\vec s)({\vec{r}-\vec{s}\over |\vec{r}-\vec{s}|^3})d^3\vec{s}$$ where $\Omega´$ is the charged region in the space we have that $$\begin{align} \nabla\cdot{E(\vec{r})} & = {1\over 4\pi\epsilon_0}\int_{\Omega´}\rho(\vec s)\nabla \cdot({\vec{r}-\vec{s}\over |\vec{r}-\vec{s}|^3})d^3\vec{s} (\text{differentiation under the integral sign})\\ & = {1\over \epsilon_0}\int_{\Omega´}\rho(\vec{s})\delta(\vec{r}-\vec{s})d^3\vec{s} \\ & = {1\over \epsilon_0}\rho(\vec{r}) (\text{shifting property of delta function}) \end{align}$$ (I have a big issue in this part becuase the density $\rho$ is just defined in the charged solid region $\Omega´$ so it doesn´t make sense to talk about the density in all the space;also i´m not sure if the last equality is valid because I´m not integrating over all the space) Know assuming that it makes sense to talk about $\rho$ in all the space then if we want to obtain the integral form we use the divergence theorem:  $$\oint_{\partial \Omega}E\cdot \vec{dS}=\int_{\Omega}\nabla\cdot\vec{E}dV={1\over \epsilon_0}\int_{\Omega}\rho(\vec{r})dV={Q_{int}\over \epsilon_0}$$ I would really appreciate if you can help me with the ""details"" of the proof, also any comments or suggestion would be highly appreciated.",,"['multivariable-calculus', 'proof-verification', 'physics', 'vector-analysis', 'mathematical-physics']"
39,$\nabla^2 u = 0 $ and integral $u$ around $\partial B_\rho$,and integral  around,\nabla^2 u = 0  u \partial B_\rho,"I'm doing exercises from book about vector calculus. And there is problem which I'm not sure what to do. Let's see the hypothesis.  Let, $D \subseteq \mathbb{R}^2$ an open set. Let, $u:\overline{D} \to \mathbb{R} \in C^2$. Suposse that, $\vec{p} \in D$, and, $0<\rho \leq R$. Consider, a ball $B_R:= B(\vec{p},R)$. If $\nabla^2u = 0 $ at $D$ then :  $$\frac{1}{2\pi R}\int_{\partial B_R} u \, ds  = u(\vec{p})$$ So far, I know two facts, which the author suggested to readers :  $$ \lim_{\rho \to 0} \frac{1}{\rho}\int_{\partial B_\rho}u \,ds = 2\pi u(\vec{p}) $$ $$ \frac{d}{d\rho} \left ( \frac{1}{\rho}\int_{\partial B_\rho}u \,ds   \right) =  \frac{1}{\rho}\iint_{B_\rho}\nabla^2 u \,ds = 0  $$ That's means : $$\frac{1}{\rho}\int_{\partial B_\rho}u \,ds = constant$$ Therefore, the expression above, should be equal to $2\pi u(\vec{p})$. No matter what values $\rho$ takes in particular if I take $\rho = R$. Then we are done. But I'm not really sure if the argument above works or I'm ignoring something. Any help is appreciated","I'm doing exercises from book about vector calculus. And there is problem which I'm not sure what to do. Let's see the hypothesis.  Let, $D \subseteq \mathbb{R}^2$ an open set. Let, $u:\overline{D} \to \mathbb{R} \in C^2$. Suposse that, $\vec{p} \in D$, and, $0<\rho \leq R$. Consider, a ball $B_R:= B(\vec{p},R)$. If $\nabla^2u = 0 $ at $D$ then :  $$\frac{1}{2\pi R}\int_{\partial B_R} u \, ds  = u(\vec{p})$$ So far, I know two facts, which the author suggested to readers :  $$ \lim_{\rho \to 0} \frac{1}{\rho}\int_{\partial B_\rho}u \,ds = 2\pi u(\vec{p}) $$ $$ \frac{d}{d\rho} \left ( \frac{1}{\rho}\int_{\partial B_\rho}u \,ds   \right) =  \frac{1}{\rho}\iint_{B_\rho}\nabla^2 u \,ds = 0  $$ That's means : $$\frac{1}{\rho}\int_{\partial B_\rho}u \,ds = constant$$ Therefore, the expression above, should be equal to $2\pi u(\vec{p})$. No matter what values $\rho$ takes in particular if I take $\rho = R$. Then we are done. But I'm not really sure if the argument above works or I'm ignoring something. Any help is appreciated",,"['calculus', 'multivariable-calculus', 'vector-analysis', 'harmonic-functions']"
40,Green's Theorem on Line Integral,Green's Theorem on Line Integral,,"I am asked to find the line integral for the following field: $$F = (e^{y^2}-2y)i + (2xye^{y^2}+\sin(y^2))j$$ On the line segment with points $(0,0),(1,2)$ and $(3,0)$. I have to do it with Greens theorem. This is the setup I have so far. $$\int_C F \cdot dr = \iint_D \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\, dA$$ $$\frac{\partial Q}{\partial x} = ye^{y^2} \qquad \frac{\partial P}{\partial y}=2ye^{y^2}-2$$ Line from $(0,0)$ to $(1,2)$ is $y = 2x$. Line from $(1,2)$ to $(3,0)$ is $y = -x + 3$ $$\int_0^1\int_0^{2x}ye^{y^2}-2ye^{y^2}-2 + \int_1^3\int_0^{-x+3}ye^{y^2}-2ye^{y^2}-2 $$ $$\int_0^1\int_0^{2x}-ye^{y^2}-2 \, dy\,dx+ \int_1^3\int_0^{-x+3}-ye^{y^2}-2 \,dy\,dx$$ $$\int_0^1\left[\frac{e^{y^2}}{2} - 2y\right]_0^{2x} + \int_1^3\left[-\frac{e^{y^2}}{2} - 2y\right]_0^{-x+3}$$ $$\int_0^1\frac{e^{4x^2}}{2}-\frac{1}{2}-4x \,dx+ \int_1^3-\frac{e^{(-x+3)^2}}{2}-2(-x+3)+\frac{1}{2}\,dx$$ Let $u = -x + 3$ and $du = -1$. $$\int_0^1\frac{e^{4x^2}}{2}-\frac{1}{2}-4x \,dx+ \int_1^3\frac{e^{(u)^2}}{2}+2(u)-\frac{1}{2}\,du$$ $$\left[\frac{e^{4x^2}}{8x}-\frac{x}{2}-2x^2 \right]_0^1+ \left[\frac{e^{(u)^2}}{4u}+u^2+\frac{u}{2}\right]_1^3$$ $$\left[\frac{e^{4x^2}}{8x}-\frac{x}{2}-2x^2 \right]_0^1+ \left[\frac{e^{(-x+3)^2}}{4(-x+3)}+(-x+3)^2+\frac{-x+3}{2}\right]_1^3$$ $$\frac{e^{4}}{8}-\frac{1}{2}-2 \,-\frac{e^{4}}{8}-4-1$$ $$\frac{1}{2}-2 \,-5$$ But the answer key says the answer is $-3$. Where have I gone wrong?","I am asked to find the line integral for the following field: $$F = (e^{y^2}-2y)i + (2xye^{y^2}+\sin(y^2))j$$ On the line segment with points $(0,0),(1,2)$ and $(3,0)$. I have to do it with Greens theorem. This is the setup I have so far. $$\int_C F \cdot dr = \iint_D \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\, dA$$ $$\frac{\partial Q}{\partial x} = ye^{y^2} \qquad \frac{\partial P}{\partial y}=2ye^{y^2}-2$$ Line from $(0,0)$ to $(1,2)$ is $y = 2x$. Line from $(1,2)$ to $(3,0)$ is $y = -x + 3$ $$\int_0^1\int_0^{2x}ye^{y^2}-2ye^{y^2}-2 + \int_1^3\int_0^{-x+3}ye^{y^2}-2ye^{y^2}-2 $$ $$\int_0^1\int_0^{2x}-ye^{y^2}-2 \, dy\,dx+ \int_1^3\int_0^{-x+3}-ye^{y^2}-2 \,dy\,dx$$ $$\int_0^1\left[\frac{e^{y^2}}{2} - 2y\right]_0^{2x} + \int_1^3\left[-\frac{e^{y^2}}{2} - 2y\right]_0^{-x+3}$$ $$\int_0^1\frac{e^{4x^2}}{2}-\frac{1}{2}-4x \,dx+ \int_1^3-\frac{e^{(-x+3)^2}}{2}-2(-x+3)+\frac{1}{2}\,dx$$ Let $u = -x + 3$ and $du = -1$. $$\int_0^1\frac{e^{4x^2}}{2}-\frac{1}{2}-4x \,dx+ \int_1^3\frac{e^{(u)^2}}{2}+2(u)-\frac{1}{2}\,du$$ $$\left[\frac{e^{4x^2}}{8x}-\frac{x}{2}-2x^2 \right]_0^1+ \left[\frac{e^{(u)^2}}{4u}+u^2+\frac{u}{2}\right]_1^3$$ $$\left[\frac{e^{4x^2}}{8x}-\frac{x}{2}-2x^2 \right]_0^1+ \left[\frac{e^{(-x+3)^2}}{4(-x+3)}+(-x+3)^2+\frac{-x+3}{2}\right]_1^3$$ $$\frac{e^{4}}{8}-\frac{1}{2}-2 \,-\frac{e^{4}}{8}-4-1$$ $$\frac{1}{2}-2 \,-5$$ But the answer key says the answer is $-3$. Where have I gone wrong?",,"['integration', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
41,Computing a pullback difficulties,Computing a pullback difficulties,,"From this example: How to calculate the pullback of a $k$-form explicitly The answer said to think of $\alpha$ as a map, and I understand the working he is doing, however I am not sure how this conincides with my definition of a pullback. I was given: For a smooth function $f:\mathbb{R^n} \to \mathbb{R^m}$ smooth with $p \in \mathbb{R^n}$ and $w$ a $k$-form on $\mathbb{R^n}$ we have that: $f^\star w(p)((v_1)_p,...,(v_k)_p) = w(f(p))(Df(p)(v_1),...,Df(p)(v_k))$ now I can see why this definition makes sense, however I cannot see how I can use this definition to get the solution of say the example in the link given i.e. how can I use this definition to find $\alpha^\star w$ where $w = xy dx + 2z dy -y dz$ and $\alpha(u,v) = (uv,u^2,3u+v)$","From this example: How to calculate the pullback of a $k$-form explicitly The answer said to think of $\alpha$ as a map, and I understand the working he is doing, however I am not sure how this conincides with my definition of a pullback. I was given: For a smooth function $f:\mathbb{R^n} \to \mathbb{R^m}$ smooth with $p \in \mathbb{R^n}$ and $w$ a $k$-form on $\mathbb{R^n}$ we have that: $f^\star w(p)((v_1)_p,...,(v_k)_p) = w(f(p))(Df(p)(v_1),...,Df(p)(v_k))$ now I can see why this definition makes sense, however I cannot see how I can use this definition to get the solution of say the example in the link given i.e. how can I use this definition to find $\alpha^\star w$ where $w = xy dx + 2z dy -y dz$ and $\alpha(u,v) = (uv,u^2,3u+v)$",,"['real-analysis', 'multivariable-calculus', 'differential-geometry']"
42,"Is $h(\mathbf{a},b)=\int_\Omega f(\mathbf{x})e^{-g(\mathbf{x})}\mathrm{d}\mathbf{x}$ differentiable?",Is  differentiable?,"h(\mathbf{a},b)=\int_\Omega f(\mathbf{x})e^{-g(\mathbf{x})}\mathrm{d}\mathbf{x}","Let $f\colon\Bbb{R}^n\to\Bbb{R}$ be an affine function and $g\colon\Bbb{R}^n\to\Bbb{R}$ be a non-negative function. We define $h\colon\Bbb{R}^n\times\Bbb{R}\to\Bbb{R}$ as follows $$ h(\mathbf{a},b)=\int_\Omega f(\mathbf{x})e^{-g(\mathbf{x})}\mathrm{d}\mathbf{x}, $$ where $\Omega=\{\mathbf{x}\in\mathbb{R}^n\colon\mathbf{a}^\top\mathbf{x}+b\geq0\}$ is the ""positive"" halfspace defined by the hyperplane $\mathbf{a}^\top\mathbf{x}+b=0$, where $\mathbf{a}\in\Bbb{R}$, $b\in\Bbb{R}$ are arbitrary. Can we state that $h$ is differentiable in $\Bbb{R}^n\times\Bbb{R}$? How could we prove that? EDITS: I have evaluated the above integral with specific choices of $f$, $g$ and the resulting function $h$ seems to be differentiable (as a sum of differentiable functions). However, I would like to show differentiability in the more abstract form given above. I have shown that $h$ is convex over $\Bbb{R}^n\times\Bbb{R}$, but I think this requires to prove differentiability first.","Let $f\colon\Bbb{R}^n\to\Bbb{R}$ be an affine function and $g\colon\Bbb{R}^n\to\Bbb{R}$ be a non-negative function. We define $h\colon\Bbb{R}^n\times\Bbb{R}\to\Bbb{R}$ as follows $$ h(\mathbf{a},b)=\int_\Omega f(\mathbf{x})e^{-g(\mathbf{x})}\mathrm{d}\mathbf{x}, $$ where $\Omega=\{\mathbf{x}\in\mathbb{R}^n\colon\mathbf{a}^\top\mathbf{x}+b\geq0\}$ is the ""positive"" halfspace defined by the hyperplane $\mathbf{a}^\top\mathbf{x}+b=0$, where $\mathbf{a}\in\Bbb{R}$, $b\in\Bbb{R}$ are arbitrary. Can we state that $h$ is differentiable in $\Bbb{R}^n\times\Bbb{R}$? How could we prove that? EDITS: I have evaluated the above integral with specific choices of $f$, $g$ and the resulting function $h$ seems to be differentiable (as a sum of differentiable functions). However, I would like to show differentiability in the more abstract form given above. I have shown that $h$ is convex over $\Bbb{R}^n\times\Bbb{R}$, but I think this requires to prove differentiability first.",,"['real-analysis', 'integration', 'multivariable-calculus', 'derivatives']"
43,Green's Theorem with respect to a given polar region.,Green's Theorem with respect to a given polar region.,,"Using Green's Theorem, compute the counterclockwise circulation $I$ of   $\vec{F}=\langle-\sqrt{x^2+y^2},\sqrt{x^2+y^2}\rangle$ around the   region defined by the polar coordinate inequalities $7 \leq r \leq 8$   and $0 \leq \theta \leq \pi$. My inclination is to approach as follows: $$ \\ \displaystyle \\ Q =  \sqrt{x^2+y^2} \implies Q_x = \dfrac{ x}{\sqrt{x^2+y^2}} \\ P = -\sqrt{x^2+y^2} \implies P_y = \dfrac{-y}{\sqrt{x^2+y^2}} \\  \\  \\ x = r\cos{\theta} \\ y = r\sin{\theta} \\ \implies \vec{F} = \langle -r, r \rangle \\ \implies I = \int\int_R \left(Q_x-P_y\right) \mathrm{d}A = \int_0^\pi \int_7^8 \frac{r^2(\cos{\theta}+\sin{\theta})}{r}\mathrm{d}r\mathrm{d}\theta \\ \implies I = \int_0^\pi (\cos{\theta} + \sin{\theta}) \ \mathrm{d}\theta \cdot \int_7^8 r \ \mathrm{d}r $$ Evaluate and find $I = -15$. Is this effective, valid, both, or neither?","Using Green's Theorem, compute the counterclockwise circulation $I$ of   $\vec{F}=\langle-\sqrt{x^2+y^2},\sqrt{x^2+y^2}\rangle$ around the   region defined by the polar coordinate inequalities $7 \leq r \leq 8$   and $0 \leq \theta \leq \pi$. My inclination is to approach as follows: $$ \\ \displaystyle \\ Q =  \sqrt{x^2+y^2} \implies Q_x = \dfrac{ x}{\sqrt{x^2+y^2}} \\ P = -\sqrt{x^2+y^2} \implies P_y = \dfrac{-y}{\sqrt{x^2+y^2}} \\  \\  \\ x = r\cos{\theta} \\ y = r\sin{\theta} \\ \implies \vec{F} = \langle -r, r \rangle \\ \implies I = \int\int_R \left(Q_x-P_y\right) \mathrm{d}A = \int_0^\pi \int_7^8 \frac{r^2(\cos{\theta}+\sin{\theta})}{r}\mathrm{d}r\mathrm{d}\theta \\ \implies I = \int_0^\pi (\cos{\theta} + \sin{\theta}) \ \mathrm{d}\theta \cdot \int_7^8 r \ \mathrm{d}r $$ Evaluate and find $I = -15$. Is this effective, valid, both, or neither?",,"['calculus', 'integration', 'multivariable-calculus', 'problem-solving', 'polar-coordinates']"
44,How to find the differential with respect to the supremum norm and $L^1$ norm,How to find the differential with respect to the supremum norm and  norm,L^1,"I'm given a function $F:C([0,1])\rightarrow C([0,1])$, $F(f)=f^2$ (where $C([0,1])$ is given the supremum norm) and I want to find $D_g F(f)$ for any $f,g\in C([0,1])$. I find that $D_g F(f)= 2fg$, but I did not use anywhere the supremum norm. In another question, I'm asked to find the differential using the $L^1$ norm so surely the process uses the norm somewhere? Edit: To clarify what I want to know, since $f$ under the $L^1$ norm is not even continuous, then surely I can't find $DF(f)$ (the Frechet derivative). I want to find the Gateaux derivative when $C([0,1])$ is endowed the $L^1$ norm, which is defined to be $$D_g F(f)=\lim_{t\rightarrow 0} \frac{F(f+tg)-F(f)}{t}$$ which has no mention of the norm. Am I misunderstanding something here?","I'm given a function $F:C([0,1])\rightarrow C([0,1])$, $F(f)=f^2$ (where $C([0,1])$ is given the supremum norm) and I want to find $D_g F(f)$ for any $f,g\in C([0,1])$. I find that $D_g F(f)= 2fg$, but I did not use anywhere the supremum norm. In another question, I'm asked to find the differential using the $L^1$ norm so surely the process uses the norm somewhere? Edit: To clarify what I want to know, since $f$ under the $L^1$ norm is not even continuous, then surely I can't find $DF(f)$ (the Frechet derivative). I want to find the Gateaux derivative when $C([0,1])$ is endowed the $L^1$ norm, which is defined to be $$D_g F(f)=\lim_{t\rightarrow 0} \frac{F(f+tg)-F(f)}{t}$$ which has no mention of the norm. Am I misunderstanding something here?",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'differential-operators']"
45,"Probability Density Function Equation, Multivariable Calculus","Probability Density Function Equation, Multivariable Calculus",,"I have the following problem: The formula for the normal distribution has a π in it. In this simplified version of the normal probability density function, solve for C. The correct answer has π in it. $$ 1 = C\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2+y^2)}dydx $$ Can anybody tell me how to solve this problem? Any help is appreciated!","I have the following problem: The formula for the normal distribution has a π in it. In this simplified version of the normal probability density function, solve for C. The correct answer has π in it. $$ 1 = C\int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2+y^2)}dydx $$ Can anybody tell me how to solve this problem? Any help is appreciated!",,"['probability', 'integration', 'multivariable-calculus', 'improper-integrals', 'density-function']"
46,"Finding critical points and determining local maxima and minima for $f(x,y)=-x^3+4xy-2y^2+1$",Finding critical points and determining local maxima and minima for,"f(x,y)=-x^3+4xy-2y^2+1","Finding critical points and determining local maxima and minima for $f(x,y)=-x^3+4xy-2y^2+1$ First I took the following derivatives: $f_x=-3x^2+4y$ $f_{xx}=-6x$ $f_y=4x-4y$ $f_{yy}=-4$ $f_{xy}=4$ I then set $f_x=0$ and $f_y=0$ and solved for the solution to the system of equations.  The answer I got is $x=\frac{4}{3}$ and $y=\frac{4}{3}$ I know I now have to classify my critical points to determine if they are local maxima or minima.  So using the following formula: $D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-[f_{xy}(x,y)]^2$ I get: $-6x(-4)-(4)^2 = 24x-16$ For the next step I'm supposed to plug in $D(x,y)$ but I have no $y$ variable.  Any hints on where I'm going wrong here?","Finding critical points and determining local maxima and minima for $f(x,y)=-x^3+4xy-2y^2+1$ First I took the following derivatives: $f_x=-3x^2+4y$ $f_{xx}=-6x$ $f_y=4x-4y$ $f_{yy}=-4$ $f_{xy}=4$ I then set $f_x=0$ and $f_y=0$ and solved for the solution to the system of equations.  The answer I got is $x=\frac{4}{3}$ and $y=\frac{4}{3}$ I know I now have to classify my critical points to determine if they are local maxima or minima.  So using the following formula: $D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-[f_{xy}(x,y)]^2$ I get: $-6x(-4)-(4)^2 = 24x-16$ For the next step I'm supposed to plug in $D(x,y)$ but I have no $y$ variable.  Any hints on where I'm going wrong here?",,"['calculus', 'multivariable-calculus', 'optimization']"
47,Second order Taylor expansion of vector-valued function,Second order Taylor expansion of vector-valued function,,"I am wondering what is the second order Taylor expansion of a vector-valued function $f(x):\mathbb{R}^M\rightarrow \mathbb{R}^N$. I know that the gradient of a vector-valued function is a Jacobian matrix $\nabla f(x)\in \mathbb{R}^{M\times N}$, and using this we have the first order Taylor expansion $f(x) = f(y)+\nabla f(y)^\top (x-y) + \text{higher order terms}$. My question is: what is the second-order expansion? It seems that the ""Hessian"" of $f(x)$ is a $M\times N \times M$ matrix (tensor).  Do I need some tensor operations to form the expansion?","I am wondering what is the second order Taylor expansion of a vector-valued function $f(x):\mathbb{R}^M\rightarrow \mathbb{R}^N$. I know that the gradient of a vector-valued function is a Jacobian matrix $\nabla f(x)\in \mathbb{R}^{M\times N}$, and using this we have the first order Taylor expansion $f(x) = f(y)+\nabla f(y)^\top (x-y) + \text{higher order terms}$. My question is: what is the second-order expansion? It seems that the ""Hessian"" of $f(x)$ is a $M\times N \times M$ matrix (tensor).  Do I need some tensor operations to form the expansion?",,"['calculus', 'multivariable-calculus', 'taylor-expansion']"
48,Volume of Sliced Parallelpiped - Triple Integrals (HW Problem),Volume of Sliced Parallelpiped - Triple Integrals (HW Problem),,"I'm trying to use a triple integral to compute the volume of a parallelepiped, which is basically a parallelogram in 3-d. The question goes as follows: Use a triple integral to compute the volume of the following region. The larger of two solids formed when the parallelepiped (slanted box) with vertices $(0,0,0), (2,0,0), (0,2,0), (2,2,0), (0,1,1), (2,1,1), (0,3,1), (2,3,1)$ is sliced by the plane $y=2$. Obviously, the quantity to be integrated is just $1 dzdydx$ but I am unsure of what to put for the integrand. I guess the bounds for x would be 0 and 2 and the z is bounded by z=3 and z=0, and y is bounded by the line y=2 and the point $(0,0,0)$ right? Would the integral then be $\int_0^2 {\int_0^2 {\int_0^3 {1dzdydx}}}$ ? But it can't be... that would be the volume of a rectangular prism...and it seems to me that the shape is that of a trapezoidal prism. These are just my thoughts so far, sorry if it seemed a bit scattered and random. By the way, this problem comes from the section titled Triple Integrals in Rectangular Coordinates. Thanks for any help in advance.","I'm trying to use a triple integral to compute the volume of a parallelepiped, which is basically a parallelogram in 3-d. The question goes as follows: Use a triple integral to compute the volume of the following region. The larger of two solids formed when the parallelepiped (slanted box) with vertices $(0,0,0), (2,0,0), (0,2,0), (2,2,0), (0,1,1), (2,1,1), (0,3,1), (2,3,1)$ is sliced by the plane $y=2$. Obviously, the quantity to be integrated is just $1 dzdydx$ but I am unsure of what to put for the integrand. I guess the bounds for x would be 0 and 2 and the z is bounded by z=3 and z=0, and y is bounded by the line y=2 and the point $(0,0,0)$ right? Would the integral then be $\int_0^2 {\int_0^2 {\int_0^3 {1dzdydx}}}$ ? But it can't be... that would be the volume of a rectangular prism...and it seems to me that the shape is that of a trapezoidal prism. These are just my thoughts so far, sorry if it seemed a bit scattered and random. By the way, this problem comes from the section titled Triple Integrals in Rectangular Coordinates. Thanks for any help in advance.",,"['integration', 'multivariable-calculus']"
49,Finding an equation of a plane perpindicular to xy plane that intersects with a surface and has a directional derivate of zero at this point.,Finding an equation of a plane perpindicular to xy plane that intersects with a surface and has a directional derivate of zero at this point.,,"I'm a bit new to 3D space and haven't had much practice with it. One question I'm working on says: A plane perpendicular to the x-y plane contains the point (3, 2, 2) on   the paraboloid $36z=4x^2+9y^2$. The cross-section of the paraboloid   created by this plane has slope 0 at this point. Find an equation of   the plane. What I did was I solved for z explicitly, giving me: $z=\frac{4}{36}x^2 + \frac{9}{36}y^2$ Then I found the partials with respect to $x$ and $y$ giving me: $F_x = \frac{8}{36}x$ $F_y = \frac{18}{36}y$ Evaluating the partials at the point $(3,2,2)$ gives me: $F_x(3) = \frac{24}{34}$ $F_y(2) = 1$ Since it's given that the directional derivative at this point is zero, and that the plane is perpendicular to the x-y plane, I get the equation of the plane to be: $0=\frac{24}{34}(x-3) + (y-2)$ But the answer in the textbook says: $0=2(x-2)+3(y-2)$ Any help would be greatly appreciated. Thanks","I'm a bit new to 3D space and haven't had much practice with it. One question I'm working on says: A plane perpendicular to the x-y plane contains the point (3, 2, 2) on   the paraboloid $36z=4x^2+9y^2$. The cross-section of the paraboloid   created by this plane has slope 0 at this point. Find an equation of   the plane. What I did was I solved for z explicitly, giving me: $z=\frac{4}{36}x^2 + \frac{9}{36}y^2$ Then I found the partials with respect to $x$ and $y$ giving me: $F_x = \frac{8}{36}x$ $F_y = \frac{18}{36}y$ Evaluating the partials at the point $(3,2,2)$ gives me: $F_x(3) = \frac{24}{34}$ $F_y(2) = 1$ Since it's given that the directional derivative at this point is zero, and that the plane is perpendicular to the x-y plane, I get the equation of the plane to be: $0=\frac{24}{34}(x-3) + (y-2)$ But the answer in the textbook says: $0=2(x-2)+3(y-2)$ Any help would be greatly appreciated. Thanks",,"['multivariable-calculus', 'partial-derivative']"
50,Using Fubini's Theorem in Stochastic Calculus,Using Fubini's Theorem in Stochastic Calculus,,"In basic calculus: 'Fubini's theorem' allows us to switch order of integration in double integrals without changing the bounds provided we are integrating over a rectangle. From here : If the area is not a rectangle, we will very likely have to change the bounds. In stochastic calculus: Let $T > 0$. Given a stochastic process $\{ X_t \}_{t \in [0,T]}$ on $(\Omega, \mathscr{F}, \mathbb{P})$, we can say by 'Fubini's theorem' that $$\mathbb{E} [\int_0^T X_t^2 dt] = \int_0^T \mathbb{E}[X_t^2] dt$$ or, if I'm not mistaken $$\int_{\Omega} \int_0^T X_t^2 dt d\mathbb{P} = \int_0^T \int_{\Omega} X_t^2 d\mathbb{P} dt$$ The left hand side of the first equation comes from here: Itô isometry , so all the assumptions apply. Question : Why is it that the bounds do not change? Well obviously we can just look at the link earlier , but how can this be put in terms of 'Fubini's theorem' in basic calculus? I'm looking for an explanation for a beginning stochastic calculus student who knows only some basics of measure theory. As far as a beginning stochastic Calculus student knows, 'Fubini's theorem' is that proposition from basic Calculus which is for Riemann integrals and assumes rectangular area of integration. So how is that applicable here? Is such an area $\Omega \times [0, T]$ rectangular?","In basic calculus: 'Fubini's theorem' allows us to switch order of integration in double integrals without changing the bounds provided we are integrating over a rectangle. From here : If the area is not a rectangle, we will very likely have to change the bounds. In stochastic calculus: Let $T > 0$. Given a stochastic process $\{ X_t \}_{t \in [0,T]}$ on $(\Omega, \mathscr{F}, \mathbb{P})$, we can say by 'Fubini's theorem' that $$\mathbb{E} [\int_0^T X_t^2 dt] = \int_0^T \mathbb{E}[X_t^2] dt$$ or, if I'm not mistaken $$\int_{\Omega} \int_0^T X_t^2 dt d\mathbb{P} = \int_0^T \int_{\Omega} X_t^2 d\mathbb{P} dt$$ The left hand side of the first equation comes from here: Itô isometry , so all the assumptions apply. Question : Why is it that the bounds do not change? Well obviously we can just look at the link earlier , but how can this be put in terms of 'Fubini's theorem' in basic calculus? I'm looking for an explanation for a beginning stochastic calculus student who knows only some basics of measure theory. As far as a beginning stochastic Calculus student knows, 'Fubini's theorem' is that proposition from basic Calculus which is for Riemann integrals and assumes rectangular area of integration. So how is that applicable here? Is such an area $\Omega \times [0, T]$ rectangular?",,"['calculus', 'multivariable-calculus', 'stochastic-processes', 'stochastic-calculus', 'isometry']"
51,"Suggestion for reference book for differential forms, differentiable manifolds and other topics","Suggestion for reference book for differential forms, differentiable manifolds and other topics",,"I am currently taking a course on multivariable calculus and our professor is following the book by Do Carmo: Differential forms and applications. I feel the text is too rigorous, which I really appreciate and it appeals to me as well, but it is making things quite difficult for me to understand.Definitions and theorems are becoming increasingly complex. Could anyone suggest a book I could use together with Do Carmo's book so that I am able to understand the text better. My goal is to master the material presented in Do Carmo's book.As of now we have completed the first three chapters from the text which are- Differential forms, Line integrals and Differentiable manifolds.  Thanks.","I am currently taking a course on multivariable calculus and our professor is following the book by Do Carmo: Differential forms and applications. I feel the text is too rigorous, which I really appreciate and it appeals to me as well, but it is making things quite difficult for me to understand.Definitions and theorems are becoming increasingly complex. Could anyone suggest a book I could use together with Do Carmo's book so that I am able to understand the text better. My goal is to master the material presented in Do Carmo's book.As of now we have completed the first three chapters from the text which are- Differential forms, Line integrals and Differentiable manifolds.  Thanks.",,"['multivariable-calculus', 'reference-request', 'manifolds', 'book-recommendation', 'differential-forms']"
52,Differentiation under the integral sign for an electrostatic field,Differentiation under the integral sign for an electrostatic field,,"Let $\rho\in C(\bar{D})$ be a continuous function on the compact set $\bar{D}$ and let us define $$\mathbf{E}(\mathbf{x}_0):=k\int_D\frac{\rho(\mathbf{x})}{\|\mathbf{x}_0-\mathbf{x}\|^3}(\mathbf{x}_0-\mathbf{x})d\mathbf{x}$$where I have used the short notation $d\mathbf{x}$ for $dxdydz$, with $\mathbf{x}=(x,y,z)$, and where $k$ is a constant, Coulomb's constant if we intend $\rho$ to be an electric charge density and $\mathbf{E}$ the electrostatic field. Clearly $$\int_D\frac{\rho(\mathbf{x})}{\|\mathbf{x}_0-\mathbf{x}\|^3}(\mathbf{x}_0-\mathbf{x})d\mathbf{x}=-\int_{D-\mathbf{x_0}}\frac{\rho(\mathbf{x}+\mathbf{x}_0)}{\|\mathbf{x}\|^3}\mathbf{x}d\mathbf{x}$$therefore I think that imposing conditions upon $\rho$ would allow us to have a finite $\mathbf{E}$ variously subject to desired conditions of regularity. Although this problem arises in a physics context, I would like to find a mathematical proof of how to guarantee the usual conditions of regularity assumed in physics for $\mathbf{E}$. For example if $\rho\in C^k(A)$ where $A$ is open and contains $\bar{D}$, can it be guaranteed that $\mathbf{E}$ is of class $C^k$? If it can, how is it proved? The fact that the domain of integration $D-\mathbf{x}_0$ depends upon the variable(s) $\mathbf{x}_0$ does not allow me to use standard results of differentiation under the integral sign like the fact that if $V\subset\mathbb{R}^3$ is compact and $f:V\times[a,b]\to\mathbb{R}$ has a continuous partial derivative $\frac{\partial f}{\partial t}\in C(V\times[a,b])$ then for all $t\in[a,b]$ $$\frac{d}{dt}\int_V f(x,y,z, t)dxdydz=\int_V\frac{\partial}{\partial t} f(x,y,z, t)dxdydz.$$ I heartily thank you for any answer!","Let $\rho\in C(\bar{D})$ be a continuous function on the compact set $\bar{D}$ and let us define $$\mathbf{E}(\mathbf{x}_0):=k\int_D\frac{\rho(\mathbf{x})}{\|\mathbf{x}_0-\mathbf{x}\|^3}(\mathbf{x}_0-\mathbf{x})d\mathbf{x}$$where I have used the short notation $d\mathbf{x}$ for $dxdydz$, with $\mathbf{x}=(x,y,z)$, and where $k$ is a constant, Coulomb's constant if we intend $\rho$ to be an electric charge density and $\mathbf{E}$ the electrostatic field. Clearly $$\int_D\frac{\rho(\mathbf{x})}{\|\mathbf{x}_0-\mathbf{x}\|^3}(\mathbf{x}_0-\mathbf{x})d\mathbf{x}=-\int_{D-\mathbf{x_0}}\frac{\rho(\mathbf{x}+\mathbf{x}_0)}{\|\mathbf{x}\|^3}\mathbf{x}d\mathbf{x}$$therefore I think that imposing conditions upon $\rho$ would allow us to have a finite $\mathbf{E}$ variously subject to desired conditions of regularity. Although this problem arises in a physics context, I would like to find a mathematical proof of how to guarantee the usual conditions of regularity assumed in physics for $\mathbf{E}$. For example if $\rho\in C^k(A)$ where $A$ is open and contains $\bar{D}$, can it be guaranteed that $\mathbf{E}$ is of class $C^k$? If it can, how is it proved? The fact that the domain of integration $D-\mathbf{x}_0$ depends upon the variable(s) $\mathbf{x}_0$ does not allow me to use standard results of differentiation under the integral sign like the fact that if $V\subset\mathbb{R}^3$ is compact and $f:V\times[a,b]\to\mathbb{R}$ has a continuous partial derivative $\frac{\partial f}{\partial t}\in C(V\times[a,b])$ then for all $t\in[a,b]$ $$\frac{d}{dt}\int_V f(x,y,z, t)dxdydz=\int_V\frac{\partial}{\partial t} f(x,y,z, t)dxdydz.$$ I heartily thank you for any answer!",,"['integration', 'multivariable-calculus', 'derivatives', 'physics']"
53,p - norms inequality,p - norms inequality,,"For $v \in \mathbb{R}^n$ denote by $||v||_p$ its p-norm. That is $(\sum_{i=1}^nv_i^p)^{\frac{1}{p}}$ where $v_i$ are the componenets of $v$. I'm looking for a way to bound the following expression: $$M_v=\frac{||v||_2^2||v||_4^4}{||v||_3^6}$$ Using Holder's inequality, and the fact that $||v||_2 \leq ||v||_3$ One can show that $M_v \leq n^{\frac{1}{3}}$. I'm wondering if that's the best possible bound. If, for example, $v_i \sim \frac{1}{\sqrt{i}}$ then it's easy to verify that $M_v$ scales like $\ln(n)$. I'm unable to find an example for which $M_v$ is much larger than that. Intuitively, I would like to say that $M_v$ is bound by some logarithm, but I can't find a suitable proof or a counterexample. Thanks a lot.","For $v \in \mathbb{R}^n$ denote by $||v||_p$ its p-norm. That is $(\sum_{i=1}^nv_i^p)^{\frac{1}{p}}$ where $v_i$ are the componenets of $v$. I'm looking for a way to bound the following expression: $$M_v=\frac{||v||_2^2||v||_4^4}{||v||_3^6}$$ Using Holder's inequality, and the fact that $||v||_2 \leq ||v||_3$ One can show that $M_v \leq n^{\frac{1}{3}}$. I'm wondering if that's the best possible bound. If, for example, $v_i \sim \frac{1}{\sqrt{i}}$ then it's easy to verify that $M_v$ scales like $\ln(n)$. I'm unable to find an example for which $M_v$ is much larger than that. Intuitively, I would like to say that $M_v$ is bound by some logarithm, but I can't find a suitable proof or a counterexample. Thanks a lot.",,"['multivariable-calculus', 'inequality', 'normed-spaces', 'examples-counterexamples']"
54,Extremal points of a sum of trigonometric functions,Extremal points of a sum of trigonometric functions,,"Show that the sum of trigonometric functions $$ f(x,y,z)=\cos(x+y+\alpha_1)+\cos(x-y+\alpha_2)+\cos(y+z+\alpha_3)\\+\cos(y-z+\alpha_4)+\cos(x+z+\alpha_5)+ \cos(x-z+\alpha_6) $$ where the $\alpha_i$ are arbitrary angles, does not have any local-but-not-global maximum or minimum. The same result seems true for the more general case $\sum_{i=1}^n\cos(\vec{v}_i.\vec{x}+\alpha_i)$ where $\vec{x}=(x_1,x_2,\ldots)$ are the variables and $\vec{v}_i$ are vectors with entries equal to $0$, $1$ or $-1$.","Show that the sum of trigonometric functions $$ f(x,y,z)=\cos(x+y+\alpha_1)+\cos(x-y+\alpha_2)+\cos(y+z+\alpha_3)\\+\cos(y-z+\alpha_4)+\cos(x+z+\alpha_5)+ \cos(x-z+\alpha_6) $$ where the $\alpha_i$ are arbitrary angles, does not have any local-but-not-global maximum or minimum. The same result seems true for the more general case $\sum_{i=1}^n\cos(\vec{v}_i.\vec{x}+\alpha_i)$ where $\vec{x}=(x_1,x_2,\ldots)$ are the variables and $\vec{v}_i$ are vectors with entries equal to $0$, $1$ or $-1$.",,"['multivariable-calculus', 'optimization', 'trigonometric-series']"
55,A function with total differential 0 suffices $|f(p)-f(q)|\leq M\|p-q\|^2$,A function with total differential 0 suffices,|f(p)-f(q)|\leq M\|p-q\|^2,"Let $f$ be of class $C^2$ in the plane, and let $S$ be a closed and bounded set such that $f_1(p) = f_2(p) = 0$ for all $p\in S$. Show that there is a constant $M$ such that $|f(p)-f(q)|\leq M\|p-q\|^2$ for all points $p,q\in S$. So I get the function is locally constant on each point, given its differential is 0, so if $S$ is connected any $M$ works, but if it's made of more than 1 connected set, I'm only aware the $M$ I can find depends on both $\sup\{|f(p)-f(q)|:p,q\in S\}$ and the diameter of the set, but I still don't get how to get into it by using either second derivatives or chain rule (this problem comes from the chain rule section in my textbook).","Let $f$ be of class $C^2$ in the plane, and let $S$ be a closed and bounded set such that $f_1(p) = f_2(p) = 0$ for all $p\in S$. Show that there is a constant $M$ such that $|f(p)-f(q)|\leq M\|p-q\|^2$ for all points $p,q\in S$. So I get the function is locally constant on each point, given its differential is 0, so if $S$ is connected any $M$ works, but if it's made of more than 1 connected set, I'm only aware the $M$ I can find depends on both $\sup\{|f(p)-f(q)|:p,q\in S\}$ and the diameter of the set, but I still don't get how to get into it by using either second derivatives or chain rule (this problem comes from the chain rule section in my textbook).",,['multivariable-calculus']
56,How to understand dot product is the angle's cosine?,How to understand dot product is the angle's cosine?,,"How can one see that a dot product gives the angle's cosine between two vectors. (assuming they are normalized) Thinking about how to prove this in the most intuitive way resulted in proving a trigonometric identity: $\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$. But even after proving this successfully, the connection between and cosine and dot product does not immediately stick out and instead I rely on remembering that this is valid while taking comfort in the fact that I've seen the proof in the past. My questions are: How do you see this connection? How do you extend the notion of dot product vs. angle to higher dimensions - 4 and higher?","How can one see that a dot product gives the angle's cosine between two vectors. (assuming they are normalized) Thinking about how to prove this in the most intuitive way resulted in proving a trigonometric identity: $\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$. But even after proving this successfully, the connection between and cosine and dot product does not immediately stick out and instead I rely on remembering that this is valid while taking comfort in the fact that I've seen the proof in the past. My questions are: How do you see this connection? How do you extend the notion of dot product vs. angle to higher dimensions - 4 and higher?",,"['geometry', 'vector-spaces', 'inner-products']"
57,What happens to tangential gradient when flattening a surface,What happens to tangential gradient when flattening a surface,,"The tangential gradient $\nabla_\tau f$ associated to a surface $S$ is defined as the projection of a suitable extension $\nabla f$ to the tangent plane to that surface. It seems reasonable to think that when changing coordinates  in order to flatten the surface, the tangential gradient is somehow related to the actual gradient. Having no experience with this notion, I cannot imagine precisely what happens when making such a transformation. Suppose that the surface $S$ is parametrized by $x(s,t)$ where $s,t$ lie in a plane region $T$. We know the surface integral formula $$ \iint_S fdS = \iint_T f(x(s,t)) \left \| \frac{\partial x}{\partial s} \times \frac{\partial x}{\partial t}\right\|dsdt. $$ I am interested in what happens when we have to integrate a function depending on the tangential gradient, i.e. I would like to see a formula for  $$ \iint_S |\nabla_\tau f|^2 dS = ... $$ My guess is that we'll have a gradient term and a Jacobian factor, but I don't have a precise idea on how to find the formula or prove its corectedness. The questions are: What is the completion of the above formula? Do you have any useful references treating this kind of integrals? (if possible, not too advanced in differential geometry notions...)","The tangential gradient $\nabla_\tau f$ associated to a surface $S$ is defined as the projection of a suitable extension $\nabla f$ to the tangent plane to that surface. It seems reasonable to think that when changing coordinates  in order to flatten the surface, the tangential gradient is somehow related to the actual gradient. Having no experience with this notion, I cannot imagine precisely what happens when making such a transformation. Suppose that the surface $S$ is parametrized by $x(s,t)$ where $s,t$ lie in a plane region $T$. We know the surface integral formula $$ \iint_S fdS = \iint_T f(x(s,t)) \left \| \frac{\partial x}{\partial s} \times \frac{\partial x}{\partial t}\right\|dsdt. $$ I am interested in what happens when we have to integrate a function depending on the tangential gradient, i.e. I would like to see a formula for  $$ \iint_S |\nabla_\tau f|^2 dS = ... $$ My guess is that we'll have a gradient term and a Jacobian factor, but I don't have a precise idea on how to find the formula or prove its corectedness. The questions are: What is the completion of the above formula? Do you have any useful references treating this kind of integrals? (if possible, not too advanced in differential geometry notions...)",,"['calculus', 'integration', 'multivariable-calculus', 'surface-integrals']"
58,Finding slope $\frac{dy}{dx}$ of tangent line to a curve defined in polar coordinates,Finding slope  of tangent line to a curve defined in polar coordinates,\frac{dy}{dx},"Problem: Let the curve $f$ be defined by $r = e^{\theta}$ . Compute the slope $\frac{dy}{dx}$ of the tangent line to $f$ . Then use your result to define a function $g(x,\theta)$ that is a tangent line to $f$ for every $\theta$ . Find the angle $\zeta$ between $0$ and $\frac{\pi}{2}$ where the tangent line to $f(\zeta)$ intersects the $x$ -axis in the point $x = 3$ . Attempt at solution: We have $$ \frac{dy}{dx} = \frac{r \cos(\theta) + \frac{dr}{d \theta} \sin(\theta)}{ -r \sin(\theta) + \frac{dr}{d \theta}\cos(\theta)} = \bigg(\frac{dy/d\theta}{dx /d \theta} \bigg) $$ hence \begin{align*} \frac{dy}{dx} = \frac{e^{\theta} \cos(\theta) + e^{\theta} \sin(\theta) } {-e^{\theta} \sin(\theta) + e^{\theta} \cos(\theta)} = m \end{align*} Now, I defined my function $g$ as follows: $$ g(x, \theta) = m(x - \theta) + e^{\theta} $$ But I'm not sure if the last part is correct, i.e. the $e^{\theta}$ . If the tangent line has to go through the point $x=3$ aswell, then we must have \begin{align*} g(3,\theta) = m(3 - \theta) + e^{\theta}. \end{align*} Then, should I try solving the equation \begin{align*} m(3- \zeta) + e^{\zeta} = 0 \end{align*} for $\zeta$ with Maple, searching for solutions around the interval $\left[0, \frac{\pi}{2}\right]$ ? If I do this, I get the numerical value $1.25$ , which lies between $0$ and $\frac{\pi}{2}$ . So is my reasoning correct? Help would be appreciated!","Problem: Let the curve be defined by . Compute the slope of the tangent line to . Then use your result to define a function that is a tangent line to for every . Find the angle between and where the tangent line to intersects the -axis in the point . Attempt at solution: We have hence Now, I defined my function as follows: But I'm not sure if the last part is correct, i.e. the . If the tangent line has to go through the point aswell, then we must have Then, should I try solving the equation for with Maple, searching for solutions around the interval ? If I do this, I get the numerical value , which lies between and . So is my reasoning correct? Help would be appreciated!","f r = e^{\theta} \frac{dy}{dx} f g(x,\theta) f \theta \zeta 0 \frac{\pi}{2} f(\zeta) x x = 3  \frac{dy}{dx} = \frac{r \cos(\theta) + \frac{dr}{d \theta} \sin(\theta)}{ -r \sin(\theta) + \frac{dr}{d \theta}\cos(\theta)} = \bigg(\frac{dy/d\theta}{dx /d \theta} \bigg)  \begin{align*} \frac{dy}{dx} = \frac{e^{\theta} \cos(\theta) + e^{\theta} \sin(\theta) } {-e^{\theta} \sin(\theta) + e^{\theta} \cos(\theta)} = m
\end{align*} g  g(x, \theta) = m(x - \theta) + e^{\theta}  e^{\theta} x=3 \begin{align*}
g(3,\theta)
= m(3 - \theta) + e^{\theta}.
\end{align*} \begin{align*}
m(3- \zeta) + e^{\zeta}
= 0
\end{align*} \zeta \left[0, \frac{\pi}{2}\right] 1.25 0 \frac{\pi}{2}","['calculus', 'real-analysis', 'multivariable-calculus']"
59,"Manifold, exist smooth nonnegative function with regular value at 0?","Manifold, exist smooth nonnegative function with regular value at 0?",,"If $X$ is any manifold with boundary, then does there exist a smooth nonnegative function $f$ on $X$, with a regular value at $0$, such that $\partial X = f^{-1}(0)$?","If $X$ is any manifold with boundary, then does there exist a smooth nonnegative function $f$ on $X$, with a regular value at $0$, such that $\partial X = f^{-1}(0)$?",,"['general-topology', 'multivariable-calculus']"
60,closed form is exact in euclidean space,closed form is exact in euclidean space,,"Question is to show that $d(f)=0$ for a $0$ form on $\mathbb{R}^n$ then $f$ is a constant function. See that $$0=df=\sum_i\frac{\partial f}{\partial x_i}dx_i$$ implies that $\frac{\partial f}{\partial x_i}=0$ for all $i$. Let $a\in \mathbb{R}^n$. For this $a$ we have $$\frac{\partial f}{\partial x_1}(a)=\lim_{h\rightarrow 0}\frac{f(a_1+h,a_2,\cdots,a_n)-f(a_1,\cdots,a_n)}{h}=0$$ Seeing this as single variable function and using mean value theorem we see that $f(a_1+p,a_2,\cdots,a_n)=f(a_1,a_2,\cdots,a_n)$ for all $p\in\mathbb{R}$ For $p=-a_1$ we have $f(0,a_2,\cdots,a_n)=f(a_1,a_2,\cdots,a_n)$ Now, do the same thin for $f(0,x_2,c_3,\cdots,x_n)$ and conclude that  $f(0,0,\cdots,a_n)=f(0,a_2,\cdots,a_n)$. Combining with previous result we have  $f(0,0,\cdots,a_n)=f(0,a_2,\cdots,a_n)=f(a_1,a_2,\cdots,a_n)$. Repeating this, we get $f(0,0,\cdots,0)=f(a_1,a_2,\cdots,a_n)$. Thus, $f$  is constant. Now, i am not sure if i can really use this type of approach and succeed in case of $1$ forms. Suppose $f$ is a $1$  form and $df=0$ i.e., $f=g_1dx_1+g_2dx_2+\cdots g_ndx_n$ and $df=0$ implies $$0=df=\sum_i d(g_i)\wedge dx_i=\sum_{i,j}\frac{\partial g_i}{\partial x_j}dx_j\wedge dx_i$$ So, we have $$\frac{\partial g_i}{\partial x_j} - \frac{\partial g_j}{\partial x_i}=0$$for all $i,j$.. I am not sure how to use  this... Note : Credits to above correction goes to user muaddib Please suggest some hints...","Question is to show that $d(f)=0$ for a $0$ form on $\mathbb{R}^n$ then $f$ is a constant function. See that $$0=df=\sum_i\frac{\partial f}{\partial x_i}dx_i$$ implies that $\frac{\partial f}{\partial x_i}=0$ for all $i$. Let $a\in \mathbb{R}^n$. For this $a$ we have $$\frac{\partial f}{\partial x_1}(a)=\lim_{h\rightarrow 0}\frac{f(a_1+h,a_2,\cdots,a_n)-f(a_1,\cdots,a_n)}{h}=0$$ Seeing this as single variable function and using mean value theorem we see that $f(a_1+p,a_2,\cdots,a_n)=f(a_1,a_2,\cdots,a_n)$ for all $p\in\mathbb{R}$ For $p=-a_1$ we have $f(0,a_2,\cdots,a_n)=f(a_1,a_2,\cdots,a_n)$ Now, do the same thin for $f(0,x_2,c_3,\cdots,x_n)$ and conclude that  $f(0,0,\cdots,a_n)=f(0,a_2,\cdots,a_n)$. Combining with previous result we have  $f(0,0,\cdots,a_n)=f(0,a_2,\cdots,a_n)=f(a_1,a_2,\cdots,a_n)$. Repeating this, we get $f(0,0,\cdots,0)=f(a_1,a_2,\cdots,a_n)$. Thus, $f$  is constant. Now, i am not sure if i can really use this type of approach and succeed in case of $1$ forms. Suppose $f$ is a $1$  form and $df=0$ i.e., $f=g_1dx_1+g_2dx_2+\cdots g_ndx_n$ and $df=0$ implies $$0=df=\sum_i d(g_i)\wedge dx_i=\sum_{i,j}\frac{\partial g_i}{\partial x_j}dx_j\wedge dx_i$$ So, we have $$\frac{\partial g_i}{\partial x_j} - \frac{\partial g_j}{\partial x_i}=0$$for all $i,j$.. I am not sure how to use  this... Note : Credits to above correction goes to user muaddib Please suggest some hints...",,['multivariable-calculus']
61,Diffeomorphism between Euclidean space,Diffeomorphism between Euclidean space,,"How does one show that if $f:U\rightarrow V$ is a diffeomorphism between open sets $U\subset\mathbb{R}^m$ and $V\subset\mathbb{R}^n$ then $m=n$? Here is some working: For $u\in U$ let $v=f(u)\in V$.  The Jacobi matrices $J_f(u):\mathbb{R}^m\rightarrow\mathbb{R}^n$ and $J_{f^{-1}}(v):\mathbb{R}^n\rightarrow\mathbb{R}^m$.  We have $f\circ f^{-1}=\text{id}_V$ and $f^{-1}\circ f=\text{id}_U$.  The chain rule yields $I_n=J_{\text{id}_V}(v)=J_f(u)\circ J_{f^{-1}}(v):\mathbb{R}^n\rightarrow\mathbb{R}^n$, the identity, and $I_m=J_{\text{id}_U}(u)=J_{f^{-1}}(v)\circ J_f(u):\mathbb{R}^m\rightarrow\mathbb{R}^m$, again the identity. By definition this means that $J_f(u)$ is invertible with inverse $J_{f^{-1}}(v)$, so we must have $m=n$.","How does one show that if $f:U\rightarrow V$ is a diffeomorphism between open sets $U\subset\mathbb{R}^m$ and $V\subset\mathbb{R}^n$ then $m=n$? Here is some working: For $u\in U$ let $v=f(u)\in V$.  The Jacobi matrices $J_f(u):\mathbb{R}^m\rightarrow\mathbb{R}^n$ and $J_{f^{-1}}(v):\mathbb{R}^n\rightarrow\mathbb{R}^m$.  We have $f\circ f^{-1}=\text{id}_V$ and $f^{-1}\circ f=\text{id}_U$.  The chain rule yields $I_n=J_{\text{id}_V}(v)=J_f(u)\circ J_{f^{-1}}(v):\mathbb{R}^n\rightarrow\mathbb{R}^n$, the identity, and $I_m=J_{\text{id}_U}(u)=J_{f^{-1}}(v)\circ J_f(u):\mathbb{R}^m\rightarrow\mathbb{R}^m$, again the identity. By definition this means that $J_f(u)$ is invertible with inverse $J_{f^{-1}}(v)$, so we must have $m=n$.",,"['multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
62,Evaluating line integral technique.,Evaluating line integral technique.,,I'm trying to do a few questions set by a lecturer on line integrals. I was struggling with a few of them and decided to look at the solutions: Often when $$\int_C \vec{F}\cdot d\vec{r}$$ is tricky to evaluate he will say $$\int_C~ \vec{F}\cdot d\vec{r}=\int_C ~(\vec{F}-\nabla f)\cdot d\vec{r}$$ I have never seen this method before and don't understand what he is doing how the two are equal or even where the function $f$ comes from. Could anyone explain in the simplest way possible or give me something to research so I know what is going on. Thanks.,I'm trying to do a few questions set by a lecturer on line integrals. I was struggling with a few of them and decided to look at the solutions: Often when $$\int_C \vec{F}\cdot d\vec{r}$$ is tricky to evaluate he will say $$\int_C~ \vec{F}\cdot d\vec{r}=\int_C ~(\vec{F}-\nabla f)\cdot d\vec{r}$$ I have never seen this method before and don't understand what he is doing how the two are equal or even where the function $f$ comes from. Could anyone explain in the simplest way possible or give me something to research so I know what is going on. Thanks.,,['multivariable-calculus']
63,Solving an Iterated Integral,Solving an Iterated Integral,,"Given the iterated integral: $$\int^{\sqrt{2}}_{-\sqrt{2}}\int^{\sqrt{2-x^2}}_{-\sqrt{2-x^2}}\int^{\sqrt{4-x^2-y^2}}_{\sqrt{x^2+y^2}}{\left(x^2+y^2+z^2\right)^{3/2}}dzdydx$$ Now, my question is, what are the two quadric surfaces that bound the region from above and below, and what are their equations? Also, how do I integrate this? I need help integrating this. Can someone help me?","Given the iterated integral: $$\int^{\sqrt{2}}_{-\sqrt{2}}\int^{\sqrt{2-x^2}}_{-\sqrt{2-x^2}}\int^{\sqrt{4-x^2-y^2}}_{\sqrt{x^2+y^2}}{\left(x^2+y^2+z^2\right)^{3/2}}dzdydx$$ Now, my question is, what are the two quadric surfaces that bound the region from above and below, and what are their equations? Also, how do I integrate this? I need help integrating this. Can someone help me?",,"['integration', 'multivariable-calculus', 'definite-integrals']"
64,Finding Critical Points and Local Maxima/Minima or Saddle Point,Finding Critical Points and Local Maxima/Minima or Saddle Point,,"I need help to find critical points of the function: $$f(x,y)=\frac{-x^3}{3}+x-y^2$$ Then I have to classify these critical points as local maxima/minima or saddle points. I thought that to find the critical points, I have to find the 1st derivative and to find local max/min or saddle, I have to use the second derivative test. I am having a little trouble both in finding first and second derivatives and how to use it to find the given above. Can someone help me? Edit: I found the critical points to be $(1,0)$ and $(-1,0)$. Can someone verify this as well?","I need help to find critical points of the function: $$f(x,y)=\frac{-x^3}{3}+x-y^2$$ Then I have to classify these critical points as local maxima/minima or saddle points. I thought that to find the critical points, I have to find the 1st derivative and to find local max/min or saddle, I have to use the second derivative test. I am having a little trouble both in finding first and second derivatives and how to use it to find the given above. Can someone help me? Edit: I found the critical points to be $(1,0)$ and $(-1,0)$. Can someone verify this as well?",,"['calculus', 'multivariable-calculus']"
65,Proving linearity of derivative,Proving linearity of derivative,,"The derivative for a function $f : \mathbb{R}^n \to \mathbb{R}^m$ defined in some open set containing $x$ at $x$ is defined (at least in Rudin and other references), if it exists, to be the linear function $A : \mathbb{R}^n \to \mathbb{R}^m$ such that \begin{equation} \lim_{t \to 0} \left| \frac{f(x + v t) - f(x)}{t} - Av\right| = 0. \end{equation} Suppose we drop the bolded assumption that $A$ is linear; can we then prove, from the remaining definition, that if $f$ is differentiable at $x$ its derivative must be linear? I suspect the answer is no (although I certainly understand why the derivative only makes intuitive sense for $A$ linear). It is certainly possible to prove under stricter assumptions (for example continuity of derivative viewed as a map $\mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m)$), but I am looking for something given only the above and other conclusions (for example the uniqueness of $A$ if it exists) that can be made without assuming $A$ is linear. If it is impossible, please provide a reason or better yet some sort of specific counter-example. Thank you in advance.","The derivative for a function $f : \mathbb{R}^n \to \mathbb{R}^m$ defined in some open set containing $x$ at $x$ is defined (at least in Rudin and other references), if it exists, to be the linear function $A : \mathbb{R}^n \to \mathbb{R}^m$ such that \begin{equation} \lim_{t \to 0} \left| \frac{f(x + v t) - f(x)}{t} - Av\right| = 0. \end{equation} Suppose we drop the bolded assumption that $A$ is linear; can we then prove, from the remaining definition, that if $f$ is differentiable at $x$ its derivative must be linear? I suspect the answer is no (although I certainly understand why the derivative only makes intuitive sense for $A$ linear). It is certainly possible to prove under stricter assumptions (for example continuity of derivative viewed as a map $\mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m)$), but I am looking for something given only the above and other conclusions (for example the uniqueness of $A$ if it exists) that can be made without assuming $A$ is linear. If it is impossible, please provide a reason or better yet some sort of specific counter-example. Thank you in advance.",,"['real-analysis', 'multivariable-calculus']"
66,Show that such an $f$ cannot exist,Show that such an  cannot exist,f,"Suppose $f:\mathbb R^n\to\mathbb R$ is a scalar field, such that for a given vector $a\in\mathbb R^n$ and any $y\in\mathbb R^n-\{0\}$ we have, $f'(a;y)>0$ . Show that such a function $f$ cannot exist. The exercise is taken from Apostol Calculus Vol. 2. I am a beginner in the subject so I want to ensure that my arguments and understanding are absolutely clear. Here is my solution: By definition, $$f'(a;y)=\lim_{h\to0}\dfrac{f(a+hy)-f(a)}{h}$$ and by assumption, as $f'(a;y)>0$ , we have $$\lim_{h\to0}\dfrac{f(a+hy)-f(a)}{h}>0$$ Taking $-y$ instead of $y$ we have $f'(a;-y)>0$ by assumption, hence, $$\lim_{h\to0}\dfrac{f(a-hy)-f(a)}{h}>0$$ . Taking $t=-h$ and noting that $h\to0$ implies $t\to0$ , we can say that $$\lim_{t\to0}\dfrac{f(a+ty)-f(a)}{-t}>0\implies \lim_{t\to0}\dfrac{f(a+ty)-f(a)}{t}<0\implies f'(a;y)<0$$ which is an obvious contradiction. Hence no such $f$ exists.","Suppose is a scalar field, such that for a given vector and any we have, . Show that such a function cannot exist. The exercise is taken from Apostol Calculus Vol. 2. I am a beginner in the subject so I want to ensure that my arguments and understanding are absolutely clear. Here is my solution: By definition, and by assumption, as , we have Taking instead of we have by assumption, hence, . Taking and noting that implies , we can say that which is an obvious contradiction. Hence no such exists.",f:\mathbb R^n\to\mathbb R a\in\mathbb R^n y\in\mathbb R^n-\{0\} f'(a;y)>0 f f'(a;y)=\lim_{h\to0}\dfrac{f(a+hy)-f(a)}{h} f'(a;y)>0 \lim_{h\to0}\dfrac{f(a+hy)-f(a)}{h}>0 -y y f'(a;-y)>0 \lim_{h\to0}\dfrac{f(a-hy)-f(a)}{h}>0 t=-h h\to0 t\to0 \lim_{t\to0}\dfrac{f(a+ty)-f(a)}{-t}>0\implies \lim_{t\to0}\dfrac{f(a+ty)-f(a)}{t}<0\implies f'(a;y)<0 f,"['multivariable-calculus', 'derivatives', 'solution-verification']"
67,Proving injectivity of a multivariable function,Proving injectivity of a multivariable function,,"Let I denote the interval $(0,\infty)$, we define the function $f:I^2\to I^2$ by, $$f(x,y)=\left({\Gamma(4x+y)\Gamma(y)\over {\Gamma(2x+y)}^2},{\Gamma(4x+y)\Gamma(2x+y)\over {\Gamma(3x+y)}^2}\right)$$ where, $\Gamma()$ is the gamma function. It has been observed numerically that this function is injective in its domain, but how can we prove that?","Let I denote the interval $(0,\infty)$, we define the function $f:I^2\to I^2$ by, $$f(x,y)=\left({\Gamma(4x+y)\Gamma(y)\over {\Gamma(2x+y)}^2},{\Gamma(4x+y)\Gamma(2x+y)\over {\Gamma(3x+y)}^2}\right)$$ where, $\Gamma()$ is the gamma function. It has been observed numerically that this function is injective in its domain, but how can we prove that?",,['multivariable-calculus']
68,integral of product of curve with itself in three dimensional,integral of product of curve with itself in three dimensional,,"I have the next problem: Let $\gamma:[0,1]\to R^3$ differentiable curve piecewise, and let $\Delta_r=\{(s,t)\in [0,1]^2| |\gamma(s)-\gamma(t)|<r\}$, i want to know if: $$\int_{\gamma\times\gamma\cap \Delta_r}\gamma'(s)\gamma'(t)dsdt\geq 0 $$ I have the easy case when $r>diam(\gamma)$. Thanks!","I have the next problem: Let $\gamma:[0,1]\to R^3$ differentiable curve piecewise, and let $\Delta_r=\{(s,t)\in [0,1]^2| |\gamma(s)-\gamma(t)|<r\}$, i want to know if: $$\int_{\gamma\times\gamma\cap \Delta_r}\gamma'(s)\gamma'(t)dsdt\geq 0 $$ I have the easy case when $r>diam(\gamma)$. Thanks!",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
69,Any unit speed reparametrization $\beta=\alpha(h)$ of $\alpha$ is reparametrized by an arc length function.,Any unit speed reparametrization  of  is reparametrized by an arc length function.,\beta=\alpha(h) \alpha,"These are definitions given from Barret O'neill's Elementary Differential Geometry. Definition $1$. A curve in $\mathbb{R}^3$ is a differentiable function $\alpha: I\to \mathbb{R}^3$ from an open interval $I$ into $\mathbb{R}^3$. Definition $2$. Let $\alpha: I\to \mathbb{R}^3$ be a curve. If $h: J\to I$ is a differentiable function on an open interval $J$, then the composite function $$\beta=\alpha (h):J\to \mathbb{R}^3$$ is a curve called a reparametrization of $\alpha$ by $h$. My question is, how can I prove that any two unit speed reparametrization, $\alpha(h)$ is essentially a reparametrization by the arc length function $s(t)=\int_{t_i}^t ||\alpha '(u)||du$, with only different base points, say $t_1$ and $t_2$. To solve this problem, it's enough to show that any unit speed reparametrization $\beta=\alpha(h)$ of $\alpha$ is reparametrized by an arc length function at some base point $t_0\in I$. However, the only fact I have is $||\beta'(w)||=||\alpha '(h(w))|||h'(w)|=1$. I need to show that $h$ is an inverse function of the arc length $s$ of $\alpha$. But I have no idea how to progress from here. I would greatly appreciate it if anyone could explain this to me, I'm having a hard time grasping this.","These are definitions given from Barret O'neill's Elementary Differential Geometry. Definition $1$. A curve in $\mathbb{R}^3$ is a differentiable function $\alpha: I\to \mathbb{R}^3$ from an open interval $I$ into $\mathbb{R}^3$. Definition $2$. Let $\alpha: I\to \mathbb{R}^3$ be a curve. If $h: J\to I$ is a differentiable function on an open interval $J$, then the composite function $$\beta=\alpha (h):J\to \mathbb{R}^3$$ is a curve called a reparametrization of $\alpha$ by $h$. My question is, how can I prove that any two unit speed reparametrization, $\alpha(h)$ is essentially a reparametrization by the arc length function $s(t)=\int_{t_i}^t ||\alpha '(u)||du$, with only different base points, say $t_1$ and $t_2$. To solve this problem, it's enough to show that any unit speed reparametrization $\beta=\alpha(h)$ of $\alpha$ is reparametrized by an arc length function at some base point $t_0\in I$. However, the only fact I have is $||\beta'(w)||=||\alpha '(h(w))|||h'(w)|=1$. I need to show that $h$ is an inverse function of the arc length $s$ of $\alpha$. But I have no idea how to progress from here. I would greatly appreciate it if anyone could explain this to me, I'm having a hard time grasping this.",,"['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
70,Geodesic equation for surface of sphere,Geodesic equation for surface of sphere,,"One of the standard problems of calculus of variations is showing that geodesics on the surface of the sphere are great circles. But I don't understand the equation. The equation for great circle path is derived to be: $$ a\cos(\phi-\phi_0) = \cot(\theta)$$ Where $\phi_0$ and $a$ are constants of integration, and everything else has a standard meaning of spherical geometry. This equation should describe a great circle path, so lets choose one in equatorial plane $\theta=\pi/2$, gives gives: $$a\cos(\phi-\phi_0)=0$$ But this is no equation of the circle, this is just set of 2 points where $\phi-\phi_0=\pi/2, 3\pi/2$.","One of the standard problems of calculus of variations is showing that geodesics on the surface of the sphere are great circles. But I don't understand the equation. The equation for great circle path is derived to be: $$ a\cos(\phi-\phi_0) = \cot(\theta)$$ Where $\phi_0$ and $a$ are constants of integration, and everything else has a standard meaning of spherical geometry. This equation should describe a great circle path, so lets choose one in equatorial plane $\theta=\pi/2$, gives gives: $$a\cos(\phi-\phi_0)=0$$ But this is no equation of the circle, this is just set of 2 points where $\phi-\phi_0=\pi/2, 3\pi/2$.",,"['multivariable-calculus', 'differential-geometry', 'calculus-of-variations']"
71,Calculate all of the second order partial derivatives of a function?,Calculate all of the second order partial derivatives of a function?,,My attempt: $g_x$ = 3$x^2$-6x $g_y$ = 3$y^2$-12y $g_{xx}$ = 6x-6 $g_{yy}$ = 6y-12 $g_{xy}$ = 0 $g_{yx}$ = 0 Are these correct? Also would I be correct in saying that $g_{xy}$ = $g_{yx}$ for all functions?,My attempt: $g_x$ = 3$x^2$-6x $g_y$ = 3$y^2$-12y $g_{xx}$ = 6x-6 $g_{yy}$ = 6y-12 $g_{xy}$ = 0 $g_{yx}$ = 0 Are these correct? Also would I be correct in saying that $g_{xy}$ = $g_{yx}$ for all functions?,,"['multivariable-calculus', 'partial-derivative']"
72,Using spherical coordinates to find volume of a region,Using spherical coordinates to find volume of a region,,"Use spherical coordinates to find the volume of the region lying above $z = \sqrt{3x^2+3y^2}$ and within the $x^2+y^2+z^2=2az$, $a>0$. So far I know that the first graph is a cone and the second one is some kind of sphere. I have completed the square so that the new equation is: $$x^2+y^2+(z-a)^2=a^2$$ I know how to convert to spherical coordinates but the $a$ is throwing me a bit.","Use spherical coordinates to find the volume of the region lying above $z = \sqrt{3x^2+3y^2}$ and within the $x^2+y^2+z^2=2az$, $a>0$. So far I know that the first graph is a cone and the second one is some kind of sphere. I have completed the square so that the new equation is: $$x^2+y^2+(z-a)^2=a^2$$ I know how to convert to spherical coordinates but the $a$ is throwing me a bit.",,"['calculus', 'multivariable-calculus']"
73,"Prove that $g_x(x,y,z)+g_y(x,y,z)+g_z(x,y,z)=0$",Prove that,"g_x(x,y,z)+g_y(x,y,z)+g_z(x,y,z)=0","I am having some trouble proving the following: Prove that if $f$ is a differentiable function of $3$ variables and $g(x,y,z)=f(x-y,y-z,z-x)$, then $g_x(x,y,z)+g_y(x,y,z)+g_z(x,y,z)=0$ I tried setting $u=x-y,v=y-z,w=z-x$, so that $g(x,y,z)=f(u(x,y,z),v(x,y,z),w(x,y,z)).$ If this is right, can someone help me finish this?","I am having some trouble proving the following: Prove that if $f$ is a differentiable function of $3$ variables and $g(x,y,z)=f(x-y,y-z,z-x)$, then $g_x(x,y,z)+g_y(x,y,z)+g_z(x,y,z)=0$ I tried setting $u=x-y,v=y-z,w=z-x$, so that $g(x,y,z)=f(u(x,y,z),v(x,y,z),w(x,y,z)).$ If this is right, can someone help me finish this?",,['multivariable-calculus']
74,Finding points on surface with specified tangent planes,Finding points on surface with specified tangent planes,,"Suppose that $ f(x,y,z) = \frac{3}{2-x} + \frac{1}{y-z} $ and let $S$ be the surface given by the equation $f(x,y,z) = 1$ Are there any points on $S$ where the tangent plane to $S$ is parallel to the $xy$, $xz$ or $yz$–planes? If such points exist find them. If no such points exist, explain why. Do we find the gradient vectors of both $S$ and the plane we are checking. And find the values of $x,y,x$ such that one gradient vector is a scalar multiple of the other? Would this be how we approach the question?","Suppose that $ f(x,y,z) = \frac{3}{2-x} + \frac{1}{y-z} $ and let $S$ be the surface given by the equation $f(x,y,z) = 1$ Are there any points on $S$ where the tangent plane to $S$ is parallel to the $xy$, $xz$ or $yz$–planes? If such points exist find them. If no such points exist, explain why. Do we find the gradient vectors of both $S$ and the plane we are checking. And find the values of $x,y,x$ such that one gradient vector is a scalar multiple of the other? Would this be how we approach the question?",,['multivariable-calculus']
75,Finding the parameterization of a curve for a line integral problem,Finding the parameterization of a curve for a line integral problem,,"I have to calculate the work of a particle that travel along a curve, given the following vector field: $F(x, y, z) = (2z-1, 0, 2y)$ and where the curve is the intersection between: $s1: z = x^2 + y^2$ and $s2: 4x^2 + 4y^2 + 1 = 4x + 4y$ using the definition of line integrals. What complicates me of this exercise is parameterize the curve, any help?","I have to calculate the work of a particle that travel along a curve, given the following vector field: $F(x, y, z) = (2z-1, 0, 2y)$ and where the curve is the intersection between: $s1: z = x^2 + y^2$ and $s2: 4x^2 + 4y^2 + 1 = 4x + 4y$ using the definition of line integrals. What complicates me of this exercise is parameterize the curve, any help?",,"['calculus', 'multivariable-calculus', 'vector-fields', 'line-integrals']"
76,Analogs to vectors -- *unoriented* line segments,Analogs to vectors -- *unoriented* line segments,,"A real vector can be thought of as an oriented line segment.  Linear algebra and multivariable calculus can be taken pretty far just by considering these types of objects (obviously there are generalizations to other vector spaces, but let's not worry about those). Using these oriented line segments, we can do several important things.  We can describe oriented subspaces of $\Bbb R^n$ build a calculus on them such that derivatives of paths tell you not only the tangent space but also ""which way a particle would move on this path"" in integration, it allows us to find oriented areas/ volumes/ etc Lots and lots more examples exist To me, though, the idea of orientation is just something extra and possibly unnecessary.  $\Bbb R^n$ isn't naturally endowed with an orientation -- we have to add one.  So could we devise a similar set of objects without the orientation?  So you'd just have a line segment, the derivative of a path would just give you the tangent space at a point, and integrals would just give you areas/ volumes/ etc. As I understand it, Descartes described geometry with just line segments.  And later Hermann Grassmann's father thought that a natural product of two line segments is just an area segment (like a bivector but without the orientation). This question partly comes from me thinking it would be nice if we could take integrals and not have to worry about the part of of the function above or beneath the $x$-axis -- we'd just get an area.  And it'd be nice if we didn't have to add an orientation onto a space to use differential area/ volume/ etc elements. And it'd be nice if we could use elementary calculus methods to integrate on non-orientable spaces. So my questions are: Could we develop a consistent system with these properties?  Would it be at all useful (maybe it'd be more complicated and completely subsumed by vector math)?  Has it already been done and I just haven't heard about it (if so, could you provide a source)?","A real vector can be thought of as an oriented line segment.  Linear algebra and multivariable calculus can be taken pretty far just by considering these types of objects (obviously there are generalizations to other vector spaces, but let's not worry about those). Using these oriented line segments, we can do several important things.  We can describe oriented subspaces of $\Bbb R^n$ build a calculus on them such that derivatives of paths tell you not only the tangent space but also ""which way a particle would move on this path"" in integration, it allows us to find oriented areas/ volumes/ etc Lots and lots more examples exist To me, though, the idea of orientation is just something extra and possibly unnecessary.  $\Bbb R^n$ isn't naturally endowed with an orientation -- we have to add one.  So could we devise a similar set of objects without the orientation?  So you'd just have a line segment, the derivative of a path would just give you the tangent space at a point, and integrals would just give you areas/ volumes/ etc. As I understand it, Descartes described geometry with just line segments.  And later Hermann Grassmann's father thought that a natural product of two line segments is just an area segment (like a bivector but without the orientation). This question partly comes from me thinking it would be nice if we could take integrals and not have to worry about the part of of the function above or beneath the $x$-axis -- we'd just get an area.  And it'd be nice if we didn't have to add an orientation onto a space to use differential area/ volume/ etc elements. And it'd be nice if we could use elementary calculus methods to integrate on non-orientable spaces. So my questions are: Could we develop a consistent system with these properties?  Would it be at all useful (maybe it'd be more complicated and completely subsumed by vector math)?  Has it already been done and I just haven't heard about it (if so, could you provide a source)?",,"['linear-algebra', 'multivariable-calculus', 'vectors']"
77,"Find a vector field G such that F = curl(G), Given F = ...","Find a vector field G such that F = curl(G), Given F = ...",,"Given $F = (xe^y, -x \cos z, -ze^y)$, I find that $\operatorname{div}(F) = 0$ thus there should exist a vector field $G$ such that $\operatorname{curl}(G) = F$. I run into issues finding the solution. To start out I write out $G = (G_1, G_2, G_3)$, $$\implies xe^y = \frac{\partial G_3}{ \partial y} - \frac{\partial G_2 }{\partial z},$$ $$-x \cos z = -\left( \frac{\partial G_3}{\partial x} - \frac{\partial G_1}{\partial z} \right),$$ and $$-z e^y = \frac{\partial G_2}{ \partial x} - \frac{\partial G_1}{\partial y}$$ I have a hard time making assumptions at this point since I know that there may be multiple solutions for the vector field $G$. I was hoping I could be pointed in the right direction on what step I should take next.","Given $F = (xe^y, -x \cos z, -ze^y)$, I find that $\operatorname{div}(F) = 0$ thus there should exist a vector field $G$ such that $\operatorname{curl}(G) = F$. I run into issues finding the solution. To start out I write out $G = (G_1, G_2, G_3)$, $$\implies xe^y = \frac{\partial G_3}{ \partial y} - \frac{\partial G_2 }{\partial z},$$ $$-x \cos z = -\left( \frac{\partial G_3}{\partial x} - \frac{\partial G_1}{\partial z} \right),$$ and $$-z e^y = \frac{\partial G_2}{ \partial x} - \frac{\partial G_1}{\partial y}$$ I have a hard time making assumptions at this point since I know that there may be multiple solutions for the vector field $G$. I was hoping I could be pointed in the right direction on what step I should take next.",,"['multivariable-calculus', 'vector-analysis']"
78,Metric tensors and linear (differential) operators defined on a manifold and its osculating sphere,Metric tensors and linear (differential) operators defined on a manifold and its osculating sphere,,"Given a 1D Riemannian manifold  $\Gamma$  embedded in 2D Euclidean space  (e.g. a parametric curve on a plane $\mathbb{R}^{2}$ ), and  point $x_{0}\in \Gamma$, we denote  $S^{1}(x_{0})$  the circle osculating $\Gamma$ at $ x_0 $. Denote also $ g = \lbrace g_{ij}\rbrace $ and $ h = \lbrace h_{ij}\rbrace  $ the induced metric tensors of $\Gamma$ and  $S^{1}(x_{0})$ respectively, with $ g^{ij} $ and $ h^{ij} $ their inverse matrices and $ G $, $ H $ their determinants. My questions are: Are the values of induced metric tensors $ \lbrace g_{ij} \rbrace$ and $  \lbrace h_{ij} \rbrace $ equal at the point $x_0\in \Gamma$? If yes, does it imply that for any  linear (differential)   operator defined on a manifold via metric tensor we will have the   equality again? In particular, the Laplace-Beltrami operator: $ > \forall f \in \mathcal{C}^{\infty}(\mathbb{R}^{2}) $ $$ \Delta_{\Gamma} \,f :=  \dfrac{1}{\sqrt{G}} \partial_{i} \Big(  \sqrt{G} g^{ij} \partial_{i} \,f \Big) \bigg|_{x=x_{0}}  \color{red}{\stackrel{(?)}{=}} \dfrac{1}{\sqrt{H}} \partial_{i} \Big(  \sqrt{H} h^{ij} \partial_{i} \, f \Big) \bigg|_{x=x_{0}}  =: \Delta_{S^{1}(x_0)}\, f. $$ If the answer for the previous question is yes , would it be possible to estimate the discrepancy of the values of these operators   within  the $ \varepsilon$-neighborhood of $ x_{0} $? Is it possible to have similar conclusions for the manifolds of higher dimensions? Attempted solutions: Yes.  Since the both $ g $ and $ h $ are defined as restriction Euclidean metric from $ \mathbb{R}^{2} $ to the tangent spaces of $\Gamma$ and  $S^{1}(x_{0})$, we conclude that $g\big|_{x=x_0} = h\big|_{x=x_0}$ if and only if the tangent spaces of $\Gamma$ and  $S^{1}(x_0)$ coincide at $ x_0 $.  It follows from the definition of an osculating circle that the unit normal vectors of $\Gamma$ and  $S^{1}(x_{0})$ at $ x_0 $ are equal, so as the tangent vectors.  Does the equity of tangent vectors imply the equity of tangent planes though? Yes, by definition?  Or do we need to establish the proximity of operators not only at the point $ x_0 $, but also in some neighborhood? Consider a point $ x_1 \in \Gamma$ such that $ \big|x_0 - x_1\big|<\varepsilon $ for some $ \varepsilon>0 $.  Given a linear (differential) operator $ \mathcal{F}_{\Omega} $ defined on a manifold $ \Omega $ in terms of its metric tensor, we want to estimate the norm of difference of operators $ \mathcal{F}_{\Gamma} $ and $ \mathcal{F}_{S^{1}(x_{0})} $ at the point $ x_1 $: $$ \bigg\| \mathcal{F}_{\Gamma}\big|_{x_1}  -  \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \bigg\| =  \sup_{ 0 \not\equiv f \in \mathcal{C}^{\infty}(\mathbb{R}^{2})}  \Bigg\lbrace  	\dfrac{\Big\|  \big[\mathcal{F}_{\Gamma}  -  \mathcal{F}_{S^{1}(x_{0})}  \big]\big|_{x_1} f \Big\|}{\| f\|} \Bigg\rbrace , $$ where $ \big[\mathcal{F}_{\Gamma}  -  \mathcal{F}_{S^{1}(x_{0})}  \big] \big|_{x_1} f = \big[\mathcal{F}_{\Gamma} \big] \big|_{x_1} f - \big[ \mathcal{F}_{S^{1}(x_{0})}  \big] \big|_{x_1} f $. Here $ \big[\mathcal{F}_{\Gamma} \big] \big|_{x_1} $ and $ \big[ \mathcal{F}_{S^{1}(x_{0})}  \big] f \big|_{x_1} $ denote the operator $ \mathcal{F} $ defined on $ \Gamma $ and  $S^{1}(x_{0}) $ respectively,   estimated at the point $ x_1 $ applied to a function $ f $. $$ \bigg\| \mathcal{F}_{\Gamma}\big|_{x_1}  -  \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \bigg\| \leq \Big\| \mathcal{F}_{\Gamma}\big|_{x_1}\Big\| + \Big\| \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \Big\| $$ If we assume that the operator $ \mathcal{F} $ is defined as a linear combination of the entries of metric tensor matrix, i.e. $ \mathcal{F} : = a^{ij}g _{ij} $, then the norm $$ \Big\| \mathcal{F}_{\Gamma}\big|_{x_1} \Big\| \leq  |a^{ij}| \cdot  \big\|g _{ij}({x_1})\big\| = \big| a^{ij}g_{ij}(x_1) \big|,  \quad i,j = \overline{1,2}. $$ Since we are talking about Riemannian manifolds, we know that the metric tensor has to vary smoothly from point to point. If $ \Gamma $ can be parametrized in terms of its arclength $ s $ with $ s_0, s_1 $ corresponding to $ x_0,x_1 $ respectively, then we can apply Taylor expansion to every entry of the metric tensor: $$  g_{ij}(s_1) = g_{ij}(s_0)  +  (s_1 - s_0) \cdot  \partial_{s} g_{ij}\big|_{s=s_1} + \frac{1}{2} (s_1 - s_0)^{2} \cdot  \partial_{s}^{2} g_{ij}\big|_{s=s_1} + \dots   $$ If there exists  a constant $ C>0 $ such that $ |  \partial_{s} g_{ij} | \leq C $ $ \forall i,j = 1,2 $, we get $$ \big\| g_{ij}(s_1)  - g_{ij}(s_0)\big\| =  C \varepsilon  + \frac{1}{2} C^{2} \varepsilon^{2} + \dots  \approx \mathcal{O}(C\varepsilon). $$ If, in addition, we assume the existance of a constant $ A $ s.t. $ a^{ij}\leq A $, then $$ \Big\| \mathcal{F}_{\Gamma}\big|_{x_1} \Big\| \leq  4 A C\varepsilon, \quad  \Big\| \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \Big\| \leq 4 A C\varepsilon, $$ so that  $$ \boxed{ \Big\| \mathcal{F}_{\Gamma}\big|_{x_1}  -  \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \Big\| \leq 8 A C \varepsilon } $$ Is there a way to obtain similar estimates for a wider class of linear operators $ \mathcal{F} $, and/or with less assumptions on the metric tensor? As for the manifolds of higher dimensions, I assume that we would have to reformulate the problem itself. For example, in order to generalize the problem for case of 2D surface in 3D Euclidean space we will have to use two principle curvatures and construct something like osculating ellipsoid (perhabs, an oblate spheroid ) with its major and minor axis inverse proportional to the principle curvatures of the manifold.  Similarly we can go over higher number of dimensions by osculating the manifold with an $ n $-dimensional ball stretched along the principle axis of the manifold.","Given a 1D Riemannian manifold  $\Gamma$  embedded in 2D Euclidean space  (e.g. a parametric curve on a plane $\mathbb{R}^{2}$ ), and  point $x_{0}\in \Gamma$, we denote  $S^{1}(x_{0})$  the circle osculating $\Gamma$ at $ x_0 $. Denote also $ g = \lbrace g_{ij}\rbrace $ and $ h = \lbrace h_{ij}\rbrace  $ the induced metric tensors of $\Gamma$ and  $S^{1}(x_{0})$ respectively, with $ g^{ij} $ and $ h^{ij} $ their inverse matrices and $ G $, $ H $ their determinants. My questions are: Are the values of induced metric tensors $ \lbrace g_{ij} \rbrace$ and $  \lbrace h_{ij} \rbrace $ equal at the point $x_0\in \Gamma$? If yes, does it imply that for any  linear (differential)   operator defined on a manifold via metric tensor we will have the   equality again? In particular, the Laplace-Beltrami operator: $ > \forall f \in \mathcal{C}^{\infty}(\mathbb{R}^{2}) $ $$ \Delta_{\Gamma} \,f :=  \dfrac{1}{\sqrt{G}} \partial_{i} \Big(  \sqrt{G} g^{ij} \partial_{i} \,f \Big) \bigg|_{x=x_{0}}  \color{red}{\stackrel{(?)}{=}} \dfrac{1}{\sqrt{H}} \partial_{i} \Big(  \sqrt{H} h^{ij} \partial_{i} \, f \Big) \bigg|_{x=x_{0}}  =: \Delta_{S^{1}(x_0)}\, f. $$ If the answer for the previous question is yes , would it be possible to estimate the discrepancy of the values of these operators   within  the $ \varepsilon$-neighborhood of $ x_{0} $? Is it possible to have similar conclusions for the manifolds of higher dimensions? Attempted solutions: Yes.  Since the both $ g $ and $ h $ are defined as restriction Euclidean metric from $ \mathbb{R}^{2} $ to the tangent spaces of $\Gamma$ and  $S^{1}(x_{0})$, we conclude that $g\big|_{x=x_0} = h\big|_{x=x_0}$ if and only if the tangent spaces of $\Gamma$ and  $S^{1}(x_0)$ coincide at $ x_0 $.  It follows from the definition of an osculating circle that the unit normal vectors of $\Gamma$ and  $S^{1}(x_{0})$ at $ x_0 $ are equal, so as the tangent vectors.  Does the equity of tangent vectors imply the equity of tangent planes though? Yes, by definition?  Or do we need to establish the proximity of operators not only at the point $ x_0 $, but also in some neighborhood? Consider a point $ x_1 \in \Gamma$ such that $ \big|x_0 - x_1\big|<\varepsilon $ for some $ \varepsilon>0 $.  Given a linear (differential) operator $ \mathcal{F}_{\Omega} $ defined on a manifold $ \Omega $ in terms of its metric tensor, we want to estimate the norm of difference of operators $ \mathcal{F}_{\Gamma} $ and $ \mathcal{F}_{S^{1}(x_{0})} $ at the point $ x_1 $: $$ \bigg\| \mathcal{F}_{\Gamma}\big|_{x_1}  -  \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \bigg\| =  \sup_{ 0 \not\equiv f \in \mathcal{C}^{\infty}(\mathbb{R}^{2})}  \Bigg\lbrace  	\dfrac{\Big\|  \big[\mathcal{F}_{\Gamma}  -  \mathcal{F}_{S^{1}(x_{0})}  \big]\big|_{x_1} f \Big\|}{\| f\|} \Bigg\rbrace , $$ where $ \big[\mathcal{F}_{\Gamma}  -  \mathcal{F}_{S^{1}(x_{0})}  \big] \big|_{x_1} f = \big[\mathcal{F}_{\Gamma} \big] \big|_{x_1} f - \big[ \mathcal{F}_{S^{1}(x_{0})}  \big] \big|_{x_1} f $. Here $ \big[\mathcal{F}_{\Gamma} \big] \big|_{x_1} $ and $ \big[ \mathcal{F}_{S^{1}(x_{0})}  \big] f \big|_{x_1} $ denote the operator $ \mathcal{F} $ defined on $ \Gamma $ and  $S^{1}(x_{0}) $ respectively,   estimated at the point $ x_1 $ applied to a function $ f $. $$ \bigg\| \mathcal{F}_{\Gamma}\big|_{x_1}  -  \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \bigg\| \leq \Big\| \mathcal{F}_{\Gamma}\big|_{x_1}\Big\| + \Big\| \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \Big\| $$ If we assume that the operator $ \mathcal{F} $ is defined as a linear combination of the entries of metric tensor matrix, i.e. $ \mathcal{F} : = a^{ij}g _{ij} $, then the norm $$ \Big\| \mathcal{F}_{\Gamma}\big|_{x_1} \Big\| \leq  |a^{ij}| \cdot  \big\|g _{ij}({x_1})\big\| = \big| a^{ij}g_{ij}(x_1) \big|,  \quad i,j = \overline{1,2}. $$ Since we are talking about Riemannian manifolds, we know that the metric tensor has to vary smoothly from point to point. If $ \Gamma $ can be parametrized in terms of its arclength $ s $ with $ s_0, s_1 $ corresponding to $ x_0,x_1 $ respectively, then we can apply Taylor expansion to every entry of the metric tensor: $$  g_{ij}(s_1) = g_{ij}(s_0)  +  (s_1 - s_0) \cdot  \partial_{s} g_{ij}\big|_{s=s_1} + \frac{1}{2} (s_1 - s_0)^{2} \cdot  \partial_{s}^{2} g_{ij}\big|_{s=s_1} + \dots   $$ If there exists  a constant $ C>0 $ such that $ |  \partial_{s} g_{ij} | \leq C $ $ \forall i,j = 1,2 $, we get $$ \big\| g_{ij}(s_1)  - g_{ij}(s_0)\big\| =  C \varepsilon  + \frac{1}{2} C^{2} \varepsilon^{2} + \dots  \approx \mathcal{O}(C\varepsilon). $$ If, in addition, we assume the existance of a constant $ A $ s.t. $ a^{ij}\leq A $, then $$ \Big\| \mathcal{F}_{\Gamma}\big|_{x_1} \Big\| \leq  4 A C\varepsilon, \quad  \Big\| \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \Big\| \leq 4 A C\varepsilon, $$ so that  $$ \boxed{ \Big\| \mathcal{F}_{\Gamma}\big|_{x_1}  -  \mathcal{F}_{S^{1}(x_{0})}\big|_{x_1} \Big\| \leq 8 A C \varepsilon } $$ Is there a way to obtain similar estimates for a wider class of linear operators $ \mathcal{F} $, and/or with less assumptions on the metric tensor? As for the manifolds of higher dimensions, I assume that we would have to reformulate the problem itself. For example, in order to generalize the problem for case of 2D surface in 3D Euclidean space we will have to use two principle curvatures and construct something like osculating ellipsoid (perhabs, an oblate spheroid ) with its major and minor axis inverse proportional to the principle curvatures of the manifold.  Similarly we can go over higher number of dimensions by osculating the manifold with an $ n $-dimensional ball stretched along the principle axis of the manifold.",,"['functional-analysis', 'multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'differential-operators']"
79,"Tangent line of a lemniscate at (0,0)","Tangent line of a lemniscate at (0,0)",,"I need to find the tangent line of the function $y=g(x)$ implicitly defined by $(x^2+y^2)^2-2a^2(x^2-y^2)=0$ at $(0,0)$, but I don't know how. I can't use implicit differentiation and evaluate at $(0,0)$, because when $y=0$ I can't use the Implicit Function Theorem to calculate the derivative and, therefore, the slope of the tangent line. I'd appreciate your help. Thanks.","I need to find the tangent line of the function $y=g(x)$ implicitly defined by $(x^2+y^2)^2-2a^2(x^2-y^2)=0$ at $(0,0)$, but I don't know how. I can't use implicit differentiation and evaluate at $(0,0)$, because when $y=0$ I can't use the Implicit Function Theorem to calculate the derivative and, therefore, the slope of the tangent line. I'd appreciate your help. Thanks.",,"['calculus', 'multivariable-calculus', 'implicit-differentiation', 'implicit-function-theorem']"
80,Working out the area of Australia through Calculus? [closed],Working out the area of Australia through Calculus? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I was wondering if it would be possible, and if so how, to calculate the area of an abstract shape on a sphere using surface integrals and Parametric surfaces and such. I am looking in to this as part of my assignment and I am not sure how to approach this problem.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 9 years ago . Improve this question I was wondering if it would be possible, and if so how, to calculate the area of an abstract shape on a sphere using surface integrals and Parametric surfaces and such. I am looking in to this as part of my assignment and I am not sure how to approach this problem.",,"['calculus', 'multivariable-calculus', 'parametric', 'surface-integrals']"
81,Seeking Recommendation on Theoretical Multivariable Calculus textbooks,Seeking Recommendation on Theoretical Multivariable Calculus textbooks,,"I am a college sophomore with double majors in mathematics and microbiology. I wrote this email to seek your advice on selecting a theoretical, proof-based textbook on the multivariable calculus. I will be taking a multivariable calculus on this Summer but it unfortunately is a computational one with little theories. I would like one that comprehensively covers the theories of multivariable calculus and perhaps including sections on the applications too (but not necessary). Couple of textbooks I have in my mind are ones written by Serge Lang, Apostol, Marsden, Hubbard, and Fleming. Which one is good for self-learning? Sincerely, PK","I am a college sophomore with double majors in mathematics and microbiology. I wrote this email to seek your advice on selecting a theoretical, proof-based textbook on the multivariable calculus. I will be taking a multivariable calculus on this Summer but it unfortunately is a computational one with little theories. I would like one that comprehensively covers the theories of multivariable calculus and perhaps including sections on the applications too (but not necessary). Couple of textbooks I have in my mind are ones written by Serge Lang, Apostol, Marsden, Hubbard, and Fleming. Which one is good for self-learning? Sincerely, PK",,"['calculus', 'multivariable-calculus']"
82,Lagrange Multipliers Dilemma,Lagrange Multipliers Dilemma,,"In the problem $f(x,y) = xy$ and $g(x, y) = x^2 + 9y^2 = 18$ I get $y = 2λx$, $x = 18λy$ and $x^2 + 9y^2 = 18$ (the constraint). All is fine, but I feel like I'll get two different answers depending on what my first step is. I could do $y = 2λ(18λy) $ or I could do $x = 18λ(2λx)$ Almost identical. But... $y = 36λ^2y$, $y - 36λ^2y = 0$, $y(1-36λ^2) y = 0$ or $λ = \pm1/6$. or $x = 36λ^2x$, $x - 36λ^2x = 0$, $x(1-36λ^2) = 0$ where $x = 0$ or $λ =\pm 1/6$. Which component is zero flips! What am I doing wrong?","In the problem $f(x,y) = xy$ and $g(x, y) = x^2 + 9y^2 = 18$ I get $y = 2λx$, $x = 18λy$ and $x^2 + 9y^2 = 18$ (the constraint). All is fine, but I feel like I'll get two different answers depending on what my first step is. I could do $y = 2λ(18λy) $ or I could do $x = 18λ(2λx)$ Almost identical. But... $y = 36λ^2y$, $y - 36λ^2y = 0$, $y(1-36λ^2) y = 0$ or $λ = \pm1/6$. or $x = 36λ^2x$, $x - 36λ^2x = 0$, $x(1-36λ^2) = 0$ where $x = 0$ or $λ =\pm 1/6$. Which component is zero flips! What am I doing wrong?",,"['multivariable-calculus', 'lagrange-multiplier']"
83,Visualising surface integrals,Visualising surface integrals,,"For a current problem I am working on, I have run into angular surface integrals, i.e. the differential solid angle $\text{d}\Omega$. Specifically the surface integrals are defined by  \begin{equation} A_{\mu\nu} = \int\int_{\rm S} \text{d}\Omega~ \frac{x_\mu x_\nu}{r^2}, \end{equation} where $x_\mu=\{x,y,z\}$ for $\mu=1,2,3$ and $r = \sqrt{x_1^2+x_2^2+x_3^2}$ and the differential solid angle is the usual definition $\text{d}\Omega=\sin(\theta)d\theta d\phi$. It is easy to show, by evaluating the integrals analytically that  \begin{equation} A_{\mu\nu} = \frac{4\pi}{3}\delta_{\mu\nu}. \end{equation} Here $\delta_{\mu\nu}$ is the Kronecker delta. What if I wanted to visualise these surface integrals? I tried plotting the angular functions which are being integrated i.e. for $A_{22}$ \begin{equation} A_{22} = \int\int_{\rm S} \text{d}\Omega~ \frac{x_2 x_2}{r^2} = \int\int_{\rm S} \text{d}\Omega~ \left(\sin(\theta)\sin(\phi)\right)^2 \end{equation} I plotted the angular function $f(\theta,\phi)=\left(\sin(\theta)\sin(\phi)\right)^2$, and get the figure below Now for $A_{23}$, which equals zero, I get the following  \begin{equation} A_{23} = \int\int_{\rm S} \text{d}\Omega~ \frac{x_2 x_3}{r^2} = \int\int_{\rm S} \text{d}\Omega~ \left(\sin(\theta)\sin(\phi)\cos(\theta)\right) \end{equation} which when plotted looks like I can obviously see the symmetries and anti-symmetries for two different surfaces although I can't figure out why the second one has a surface integral of zero. To me it should be the first, as they cancel. Am I plotting these surface integrals correctly? Can someone explain the difference between the two surfaces, and why the second is equal to zero and not the first? Thanks.","For a current problem I am working on, I have run into angular surface integrals, i.e. the differential solid angle $\text{d}\Omega$. Specifically the surface integrals are defined by  \begin{equation} A_{\mu\nu} = \int\int_{\rm S} \text{d}\Omega~ \frac{x_\mu x_\nu}{r^2}, \end{equation} where $x_\mu=\{x,y,z\}$ for $\mu=1,2,3$ and $r = \sqrt{x_1^2+x_2^2+x_3^2}$ and the differential solid angle is the usual definition $\text{d}\Omega=\sin(\theta)d\theta d\phi$. It is easy to show, by evaluating the integrals analytically that  \begin{equation} A_{\mu\nu} = \frac{4\pi}{3}\delta_{\mu\nu}. \end{equation} Here $\delta_{\mu\nu}$ is the Kronecker delta. What if I wanted to visualise these surface integrals? I tried plotting the angular functions which are being integrated i.e. for $A_{22}$ \begin{equation} A_{22} = \int\int_{\rm S} \text{d}\Omega~ \frac{x_2 x_2}{r^2} = \int\int_{\rm S} \text{d}\Omega~ \left(\sin(\theta)\sin(\phi)\right)^2 \end{equation} I plotted the angular function $f(\theta,\phi)=\left(\sin(\theta)\sin(\phi)\right)^2$, and get the figure below Now for $A_{23}$, which equals zero, I get the following  \begin{equation} A_{23} = \int\int_{\rm S} \text{d}\Omega~ \frac{x_2 x_3}{r^2} = \int\int_{\rm S} \text{d}\Omega~ \left(\sin(\theta)\sin(\phi)\cos(\theta)\right) \end{equation} which when plotted looks like I can obviously see the symmetries and anti-symmetries for two different surfaces although I can't figure out why the second one has a surface integral of zero. To me it should be the first, as they cancel. Am I plotting these surface integrals correctly? Can someone explain the difference between the two surfaces, and why the second is equal to zero and not the first? Thanks.",,"['calculus', 'integration', 'multivariable-calculus', 'graphing-functions', 'visualization']"
84,Lipschitz constant of the convex function $f(x) - \frac{a}{2} |x|^2$,Lipschitz constant of the convex function,f(x) - \frac{a}{2} |x|^2,"I was going through this blog post https://blogs.princeton.edu/imabandit/2013/04/04/orf523-strong-convexity/ It has been mentioned without proof that for a function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ that is Lipschitz continuous with Lipschitz constant $\beta$ and strongly convex with coefficient $\alpha$, the function $g:\mathbb{R}^n\rightarrow\mathbb{R}$ defined as \begin{equation}g(\mathbf{x})\triangleq f(\mathbf{x}) - \frac{\alpha}{2}\|\mathbf{x}\|^2\end{equation} is Lipschitz continuous with coefficient $\beta-\alpha$. I tried proving this as follows \begin{align} \nabla g(\mathbf{x}) =& \nabla f(\mathbf{x}) - \alpha \mathbf{x} \\ \|\nabla g(\mathbf{x}) - \nabla g(\mathbf{y})\|^2 = & \|\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})\|^2 + \alpha^2 \|\mathbf{x} -\mathbf{y}\|^2 - 2 \alpha (\nabla f(\mathbf{x}) - \nabla f(\mathbf{y}))^T(\mathbf{x}-\mathbf{y}) \\ \leq & \beta^2 \|\mathbf{x} -\mathbf{y}\|^2 + \alpha^2 \|\mathbf{x} -\mathbf{y}\|^2 - 2 \alpha (\nabla f(\mathbf{x}) - \nabla f(\mathbf{y}))^T(\mathbf{x}-\mathbf{y}) \end{align} where the last inequality was from the definition of Lipschitz continuity of $f$. I am stuck at the last inequality and am not sure on how to proceed. My hunch is that the last term should become $2\alpha\beta\|\mathbf{x} - \mathbf{y}\|^2$ but am not able to prove that. The problem seems fairly straightforward though. Any help would be greatly appreciated.","I was going through this blog post https://blogs.princeton.edu/imabandit/2013/04/04/orf523-strong-convexity/ It has been mentioned without proof that for a function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ that is Lipschitz continuous with Lipschitz constant $\beta$ and strongly convex with coefficient $\alpha$, the function $g:\mathbb{R}^n\rightarrow\mathbb{R}$ defined as \begin{equation}g(\mathbf{x})\triangleq f(\mathbf{x}) - \frac{\alpha}{2}\|\mathbf{x}\|^2\end{equation} is Lipschitz continuous with coefficient $\beta-\alpha$. I tried proving this as follows \begin{align} \nabla g(\mathbf{x}) =& \nabla f(\mathbf{x}) - \alpha \mathbf{x} \\ \|\nabla g(\mathbf{x}) - \nabla g(\mathbf{y})\|^2 = & \|\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})\|^2 + \alpha^2 \|\mathbf{x} -\mathbf{y}\|^2 - 2 \alpha (\nabla f(\mathbf{x}) - \nabla f(\mathbf{y}))^T(\mathbf{x}-\mathbf{y}) \\ \leq & \beta^2 \|\mathbf{x} -\mathbf{y}\|^2 + \alpha^2 \|\mathbf{x} -\mathbf{y}\|^2 - 2 \alpha (\nabla f(\mathbf{x}) - \nabla f(\mathbf{y}))^T(\mathbf{x}-\mathbf{y}) \end{align} where the last inequality was from the definition of Lipschitz continuity of $f$. I am stuck at the last inequality and am not sure on how to proceed. My hunch is that the last term should become $2\alpha\beta\|\mathbf{x} - \mathbf{y}\|^2$ but am not able to prove that. The problem seems fairly straightforward though. Any help would be greatly appreciated.",,"['multivariable-calculus', 'convex-analysis']"
85,"Compute the limit of $\frac{\log \left(|x| + e^{|y|}\right)}{\sqrt{x^2 + y^2}}$ when $(x,y)\to (0,0)$",Compute the limit of  when,"\frac{\log \left(|x| + e^{|y|}\right)}{\sqrt{x^2 + y^2}} (x,y)\to (0,0)","$$\lim_{(x,y)\to (0,0)} \frac{\log \left(|x| + e^{|y|}\right)}{\sqrt{x^2 + y^2}} = ?$$ Assuming that $\log \triangleq \ln$, then I tried the following: 1. Sandwich rule Saying that $\log \left(|x| + e^{|y|}\right) < |x| + e^{|y|}$: $$\begin{align} \lim_{(x,y)\to (0,0)} \frac{\log \left(|x| + e^{|y|}\right)}{\sqrt{x^2 + y^2}} &< \\  & \lim_{(x,y)\to (0,0)} \frac{|x| + e^{|y|}}{\sqrt{x^2 + y^2}} = \\  &= \lim_{r \to 0} \frac{|r\cos \theta| + e^{|r\sin\theta|}}{r} \\  &= \lim_{r \to 0} |\cos\theta| + \frac{e^{|r\sin\theta|}}{r} \end{align}$$ From here it seems that the limit doesn't exist, so it doesn't indicate anything on the given function. 2. Polar coordinates Tried expressing $x=r\cos\theta, y=r\sin\theta$, though got stuck right at the $\log$ function. Also tried using it in the Sandwich rule above, to no avail. 3. Single variable assignment Another technique is to replace an expression of $x$ and $y$ with a single variable $t$, but for this case it is not helpful. The $\sqrt{x^2 + y^2}$ strongly indicates on Polar, though I can't work through that $\log$ and $e$. It seems that I'm missing an important logarithmic identity, though I've seen many identities at Wiki and none is useful.","$$\lim_{(x,y)\to (0,0)} \frac{\log \left(|x| + e^{|y|}\right)}{\sqrt{x^2 + y^2}} = ?$$ Assuming that $\log \triangleq \ln$, then I tried the following: 1. Sandwich rule Saying that $\log \left(|x| + e^{|y|}\right) < |x| + e^{|y|}$: $$\begin{align} \lim_{(x,y)\to (0,0)} \frac{\log \left(|x| + e^{|y|}\right)}{\sqrt{x^2 + y^2}} &< \\  & \lim_{(x,y)\to (0,0)} \frac{|x| + e^{|y|}}{\sqrt{x^2 + y^2}} = \\  &= \lim_{r \to 0} \frac{|r\cos \theta| + e^{|r\sin\theta|}}{r} \\  &= \lim_{r \to 0} |\cos\theta| + \frac{e^{|r\sin\theta|}}{r} \end{align}$$ From here it seems that the limit doesn't exist, so it doesn't indicate anything on the given function. 2. Polar coordinates Tried expressing $x=r\cos\theta, y=r\sin\theta$, though got stuck right at the $\log$ function. Also tried using it in the Sandwich rule above, to no avail. 3. Single variable assignment Another technique is to replace an expression of $x$ and $y$ with a single variable $t$, but for this case it is not helpful. The $\sqrt{x^2 + y^2}$ strongly indicates on Polar, though I can't work through that $\log$ and $e$. It seems that I'm missing an important logarithmic identity, though I've seen many identities at Wiki and none is useful.",,"['limits', 'multivariable-calculus']"
86,Show a multivariable function is differentiable at a point,Show a multivariable function is differentiable at a point,,"Define $F(x,y)=(x^2, xy + y^2)$.  How do I show that $F(x,y)$ is differentiable at the point $A=(a,b)$ without using any known theorems? Am I correct by showing that the limit exists, i.e. $lim_{h \to 0} (FA+h)-F(h)/h$? I broke it up into two parts, first, holding 'b' constant and then having $a + h \to h$ as $h \to 0$.  Then I held 'a' constant and did $b + h \to b$ as $h \to 0$.  This got me all 4 partial derivatives, which I know are correct. Is this the correct approach, or am I missing any other important steps?","Define $F(x,y)=(x^2, xy + y^2)$.  How do I show that $F(x,y)$ is differentiable at the point $A=(a,b)$ without using any known theorems? Am I correct by showing that the limit exists, i.e. $lim_{h \to 0} (FA+h)-F(h)/h$? I broke it up into two parts, first, holding 'b' constant and then having $a + h \to h$ as $h \to 0$.  Then I held 'a' constant and did $b + h \to b$ as $h \to 0$.  This got me all 4 partial derivatives, which I know are correct. Is this the correct approach, or am I missing any other important steps?",,"['real-analysis', 'multivariable-calculus']"
87,How to recover complex function on $\mathbb C$ from integral equation?,How to recover complex function on  from integral equation?,\mathbb C,"Let $f:\mathbb C \to \mathbb C$ be a continuous function with the form $f(z)= z\tilde{f}(|z|)$ for all $z\in \mathbb C,$ where $\tilde{f}$ is a real function defined on $(0, \infty).$ We define $V:\mathbb C \to \mathbb R$ as follows: $$V(z):=V(|z|), \ \forall z \in \mathbb C;$$ where $V(x)=2\int_{0}^{x} y\tilde{f}(y) dy$ for all $x\geq 0.$ My Question : How to show: $\frac{\partial V (z)}{\partial \bar{z}}= f(z)$ ? (In other words: problem states we can recover $f$ from $V$ by taking partial derivatives with respect to $\bar{z}$ )","Let $f:\mathbb C \to \mathbb C$ be a continuous function with the form $f(z)= z\tilde{f}(|z|)$ for all $z\in \mathbb C,$ where $\tilde{f}$ is a real function defined on $(0, \infty).$ We define $V:\mathbb C \to \mathbb R$ as follows: $$V(z):=V(|z|), \ \forall z \in \mathbb C;$$ where $V(x)=2\int_{0}^{x} y\tilde{f}(y) dy$ for all $x\geq 0.$ My Question : How to show: $\frac{\partial V (z)}{\partial \bar{z}}= f(z)$ ? (In other words: problem states we can recover $f$ from $V$ by taking partial derivatives with respect to $\bar{z}$ )",,"['real-analysis', 'integration', 'complex-analysis', 'multivariable-calculus', 'proof-verification']"
88,A Formal proof of Green Theorem,A Formal proof of Green Theorem,,"I want to go through the formal proof of Green theorem on a regular, simple and closed curve oriented counterclockwise and the vector space $F$ is a continuously differentiable  vector field such that $F: R^2->R^2$ defined on some open set containing C. I want to proof it by refinement in small rectangles. I proved that the cancelation does work, that means that if I have two adjacent rectangles I don't need to care about the edge in the middle. But I have trouble to use $\epsilon$ and $\delta$ to prove that the error is negligible. That means I want to proof that the for the compact set C, there is a compact set C' contained in C, such that C' is at most $\epsilon $ distance away from C. And my refinement can cover C' but contained in C. Could any one give me some hint on it? I cannot find any information on how to deal with the refinement in the proof of Green theorem but only general idea. I really want to go through the detail about it. Thank you so much for your help","I want to go through the formal proof of Green theorem on a regular, simple and closed curve oriented counterclockwise and the vector space $F$ is a continuously differentiable  vector field such that $F: R^2->R^2$ defined on some open set containing C. I want to proof it by refinement in small rectangles. I proved that the cancelation does work, that means that if I have two adjacent rectangles I don't need to care about the edge in the middle. But I have trouble to use $\epsilon$ and $\delta$ to prove that the error is negligible. That means I want to proof that the for the compact set C, there is a compact set C' contained in C, such that C' is at most $\epsilon $ distance away from C. And my refinement can cover C' but contained in C. Could any one give me some hint on it? I cannot find any information on how to deal with the refinement in the proof of Green theorem but only general idea. I really want to go through the detail about it. Thank you so much for your help",,"['real-analysis', 'multivariable-calculus', 'vector-spaces', 'vector-analysis']"
89,problem book on Functions of Two Real Variables,problem book on Functions of Two Real Variables,,"please refer a problem book on Functions of Two Real Variables : limit, continuity, partial derivatives, differentiability , maxima and minima. Method of Lagrange multipliers, Homogeneous functions including Euler’s theorem. i have already done this text from Calculus by Thomas Finney .But i need one problem book to solve in order to be confident to sit for my exam. I dont need a proof oriented problem book,my focus is to solve problems which are applications of theorems. please provide your suggestions.thank you","please refer a problem book on Functions of Two Real Variables : limit, continuity, partial derivatives, differentiability , maxima and minima. Method of Lagrange multipliers, Homogeneous functions including Euler’s theorem. i have already done this text from Calculus by Thomas Finney .But i need one problem book to solve in order to be confident to sit for my exam. I dont need a proof oriented problem book,my focus is to solve problems which are applications of theorems. please provide your suggestions.thank you",,['multivariable-calculus']
90,Bounded integral operator,Bounded integral operator,,"So if $k:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\sup_{x}\int |k(x,y)|dy<\infty \ \  \ \sup_{y}\int|k(x,y)|dx<\infty$$ How do you show in this case, $Kf(x)=\int k(x,y)f(y)dy$ is a bounded from $L^2$ to $L^2$.  Any hints? Clearly $k$ cannot be $L^2$.","So if $k:\mathbb{R}^2\rightarrow\mathbb{R}$ such that $$\sup_{x}\int |k(x,y)|dy<\infty \ \  \ \sup_{y}\int|k(x,y)|dx<\infty$$ How do you show in this case, $Kf(x)=\int k(x,y)f(y)dy$ is a bounded from $L^2$ to $L^2$.  Any hints? Clearly $k$ cannot be $L^2$.",,"['real-analysis', 'functional-analysis', 'multivariable-calculus']"
91,"Prove $\int_{[a,b]}f=\int_{[a,c]}f+\int_{[c,b]}f$",Prove,"\int_{[a,b]}f=\int_{[a,c]}f+\int_{[c,b]}f","Let $a,b\in\mathbb C$ and $c\in[a,b]$. Let $f$ be continuous on $[a,b]$. Use the definition to show that \begin{equation} \int_{[a,b]}f=\int_{[a,c]}f+\int_{[c,b]}f \end{equation} Note: You should stick to the definition, which gives, e.g., $\int_{[a,b]}f=\int_0^1 f(a+t(b-a))\cdot (b-a)dt$. Ok. My first step is noting that $[a,b]$ is actually the line segment connecting the complex numbers $a$ and $b$, which is the image of the curve \begin{equation} \gamma(t)=a+(b-a)t, \quad 0 \le t \le 1 \end{equation} which explains where the definition of $\int_{[a,b]}$ comes from, using  \begin{equation} \int_\gamma f = \int_a^b f(\gamma(t))\gamma'(t)dt \end{equation} Now here is the start of my proof: \begin{align*} \int_{[a,c]}f+\int_{[c,b]}f&=\int_0^1 f(a+t(c-a))\cdot (c-a)dt+\int_0^1 f(c+t(b-c))\cdot (b-c)dt \\ &=(c-a)\int_0^1 f(a+t(c-a))dt+(b-c)\int_0^1 f(c+t(b-c))dt \end{align*} I'm not great with manipulating integrals, so I appreciate any help you can offer. Thanks!","Let $a,b\in\mathbb C$ and $c\in[a,b]$. Let $f$ be continuous on $[a,b]$. Use the definition to show that \begin{equation} \int_{[a,b]}f=\int_{[a,c]}f+\int_{[c,b]}f \end{equation} Note: You should stick to the definition, which gives, e.g., $\int_{[a,b]}f=\int_0^1 f(a+t(b-a))\cdot (b-a)dt$. Ok. My first step is noting that $[a,b]$ is actually the line segment connecting the complex numbers $a$ and $b$, which is the image of the curve \begin{equation} \gamma(t)=a+(b-a)t, \quad 0 \le t \le 1 \end{equation} which explains where the definition of $\int_{[a,b]}$ comes from, using  \begin{equation} \int_\gamma f = \int_a^b f(\gamma(t))\gamma'(t)dt \end{equation} Now here is the start of my proof: \begin{align*} \int_{[a,c]}f+\int_{[c,b]}f&=\int_0^1 f(a+t(c-a))\cdot (c-a)dt+\int_0^1 f(c+t(b-c))\cdot (b-c)dt \\ &=(c-a)\int_0^1 f(a+t(c-a))dt+(b-c)\int_0^1 f(c+t(b-c))dt \end{align*} I'm not great with manipulating integrals, so I appreciate any help you can offer. Thanks!",,"['complex-analysis', 'multivariable-calculus', 'definite-integrals', 'contour-integration', 'plane-curves']"
92,Intersection between a sphere and a plane and parametrization,Intersection between a sphere and a plane and parametrization,,"I am given that a circle is formed when the unit sphere $x^2+y^2+z^2=1$ intersects the plane $x+y+z=0$. I would like to find the equation of that circle using cylindrical coordinates so that I later can parametrize the equation of the circle. Here is the illustration: $$\left\{ \begin{array}{l} {x^2} + {y^2} + {z^2} = 1\\ x + y + z = 0 \end{array} \right.$$ 1. Converting to cylindrical coordinates Since I am only familiar with cylindrical coordinates (not the way equations should be expressed), I thought that the natural thing would be to simply substitute $x=r\cos(\theta)$ and $y=r\sin(\theta)$, $z=z$ in both equations. Then, I got the following: $$ \left\{ \begin{array}{l} {r^2} + {z^2} = 1\\ r(\cos \theta  + \sin \theta )+z = 0 \end{array} \right.$$ 2. Finding the parametrization As I know, there are infinitly many parametrizations, so there is no ""right one"". So, since I have a representation of the system in cylindrical coordinates, I simply substitute $z=-r(\cos \theta  + \sin \theta ) $ into the first eq. so that I get: $$ \begin{array}{l} {r^2} + {( - r(\cos \theta  + \sin \theta ))^2} = 1\\ {r^2} + {r^2}({\cos ^2}\theta  + {\sin ^2}\theta  + 2\cos \theta \sin \theta ) = 1\\ {r^2}(1 + 1 + \sin 2\theta ) = 1\\ r = \sqrt {\frac{1}{{1 + 1 + \sin 2\theta }}}  \end{array}$$ In a similar way, I would find $z$. Now the thing that worries me is the square root sign. I now that sometimes, having a square root sign involved will cause paramatrization of only parts of the curve. My question is, am I correct in my reasoning and do I have the parametrization for the entire surface?","I am given that a circle is formed when the unit sphere $x^2+y^2+z^2=1$ intersects the plane $x+y+z=0$. I would like to find the equation of that circle using cylindrical coordinates so that I later can parametrize the equation of the circle. Here is the illustration: $$\left\{ \begin{array}{l} {x^2} + {y^2} + {z^2} = 1\\ x + y + z = 0 \end{array} \right.$$ 1. Converting to cylindrical coordinates Since I am only familiar with cylindrical coordinates (not the way equations should be expressed), I thought that the natural thing would be to simply substitute $x=r\cos(\theta)$ and $y=r\sin(\theta)$, $z=z$ in both equations. Then, I got the following: $$ \left\{ \begin{array}{l} {r^2} + {z^2} = 1\\ r(\cos \theta  + \sin \theta )+z = 0 \end{array} \right.$$ 2. Finding the parametrization As I know, there are infinitly many parametrizations, so there is no ""right one"". So, since I have a representation of the system in cylindrical coordinates, I simply substitute $z=-r(\cos \theta  + \sin \theta ) $ into the first eq. so that I get: $$ \begin{array}{l} {r^2} + {( - r(\cos \theta  + \sin \theta ))^2} = 1\\ {r^2} + {r^2}({\cos ^2}\theta  + {\sin ^2}\theta  + 2\cos \theta \sin \theta ) = 1\\ {r^2}(1 + 1 + \sin 2\theta ) = 1\\ r = \sqrt {\frac{1}{{1 + 1 + \sin 2\theta }}}  \end{array}$$ In a similar way, I would find $z$. Now the thing that worries me is the square root sign. I now that sometimes, having a square root sign involved will cause paramatrization of only parts of the curve. My question is, am I correct in my reasoning and do I have the parametrization for the entire surface?",,"['multivariable-calculus', 'algebraic-geometry']"
93,Function of class C(1) except at a point,Function of class C(1) except at a point,,"Let $f:\Omega\to\mathbb R$, $\Omega\subseteq\mathbb R^n$, a continuous function and of class $C^{(1)}(\Omega\setminus \{ x_0\})$, $x_0\in\Omega$. Suppose that the limits $$l_i=lim_{x\to x_0}\frac{\partial f}{\partial x_i}(x), i=1,...,n$$ exist. Show that $$l_i = \frac{\partial f}{\partial x_i}(x_0)$$ and therefore $f\in C^{(1)}(\Omega)$. Hint: Use the mean-value theorem. I don't even know how to start here... I'd be very thankful if someone could tell me how to approach this exercise. Thanks!","Let $f:\Omega\to\mathbb R$, $\Omega\subseteq\mathbb R^n$, a continuous function and of class $C^{(1)}(\Omega\setminus \{ x_0\})$, $x_0\in\Omega$. Suppose that the limits $$l_i=lim_{x\to x_0}\frac{\partial f}{\partial x_i}(x), i=1,...,n$$ exist. Show that $$l_i = \frac{\partial f}{\partial x_i}(x_0)$$ and therefore $f\in C^{(1)}(\Omega)$. Hint: Use the mean-value theorem. I don't even know how to start here... I'd be very thankful if someone could tell me how to approach this exercise. Thanks!",,['multivariable-calculus']
94,"Multi-variable function is undefined at every point, then limit still may exist","Multi-variable function is undefined at every point, then limit still may exist",,"The following question was posed; If a multi-variable function $f(x,y)$ was undefined at every point on a curve, then could a limit exist of a point $(x_0, y_0)$ on this curve for this function? i.e Does $\lim_{(x,y)\rightarrow (x,_0, y_0)}f(x,y)$ exist? To me, the whole point of limits is to find how functions behave when the function gets arbitrarily close to points where it is not defined. So, yes, I would think that regardless if a function $f(x,y)$ is undefined at a point, then a limit still could exist. However, I can't think of an example of a function on a curve that is undefined for every point. So I'm beginning to think that if this question was stated that if $f(x,y)$ was undefined only at a certain point, then a limit could exist, which could be trivially proven by the definition. But because it says that every point on the curve is not defined, then no, the limit cannot exist for an arbitrary point $(x_0, y_0)$. Am I correct in thinking this?","The following question was posed; If a multi-variable function $f(x,y)$ was undefined at every point on a curve, then could a limit exist of a point $(x_0, y_0)$ on this curve for this function? i.e Does $\lim_{(x,y)\rightarrow (x,_0, y_0)}f(x,y)$ exist? To me, the whole point of limits is to find how functions behave when the function gets arbitrarily close to points where it is not defined. So, yes, I would think that regardless if a function $f(x,y)$ is undefined at a point, then a limit still could exist. However, I can't think of an example of a function on a curve that is undefined for every point. So I'm beginning to think that if this question was stated that if $f(x,y)$ was undefined only at a certain point, then a limit could exist, which could be trivially proven by the definition. But because it says that every point on the curve is not defined, then no, the limit cannot exist for an arbitrary point $(x_0, y_0)$. Am I correct in thinking this?",,"['calculus', 'limits', 'multivariable-calculus']"
95,Understanding the Jacobian past calculus,Understanding the Jacobian past calculus,,"What's taught in calculus: In the calculus of multiple variables I learned that the Jacobian $$\textbf J=\frac{\partial(x_1,\ldots,x_n)}{\partial(t_1,\ldots,t_n)}=\left(\begin{array}{ccc}\frac{\partial x_1}{\partial t_1}&\cdots&\frac{\partial x_1}{\partial t_n}\\\vdots&\ddots&\vdots\\\frac{\partial x_n}{\partial t_1}&\ldots&\frac{\partial x_n}{\partial t_n}\end{array}\right)$$ gives me a means of obtaining differentials $dt_1,\ldots,dt_n$ from $dx_1,\ldots,dx_n$. Geometric intuition is provided: The collection $dt$ is the volume/area element of an infinitesimal portion to be integrated. The equation for each member of $dx$ expressed as a vector is $$d\textbf x_k=\sum_{i=0}^n\frac{\partial x_k}{\partial t_i}d\textbf t_i$$ and in a two-dimensional system, we have $$d\textbf x_k=\left<\frac{\partial x_k}{\partial t_1}dt_1,\frac{\partial x_k}{\partial t_2}dt_2,0\right>$$ Then we find the area element with $dx$ corresponding to $dt$ $$|d\textbf x_1\times d\textbf x_2|=\left|\frac{\partial x_1}{\partial t_1}\frac{\partial x_2}{\partial t_2}-\frac{\partial x_1}{\partial t_2}\frac{\partial x_2}{\partial t_1}\right|dt_1dt_2$$ which is the same as $|\det\textbf J|=\left|\det\frac{\partial(x_1,x_2)}{\partial(t_1,t_2)}\right|$, which concludes the motivation for the Jacobian in basic calculus. I'm confident the same exercise as above could be performed in 3D, except with $d\textbf x_1\cdot(d\textbf x_2\times d\textbf x_3)$ for my volume element, but that's where it ends, since there's no notion of cross products above 3D in calculus. My questions: Cross products don't extend to above three dimensions. So far I've had 3D Jacobians explained. What about $n$-dimensional Jacobians? What about coordinate transformations with $T:(t_1,\ldots,t_m)\to(x_1,\ldots,x_n)$ and $n\neq m$? Is the motivation given in calculus at all rigorous? I'm still not certain exactly what a differential is, and why I can manipulate them as above.","What's taught in calculus: In the calculus of multiple variables I learned that the Jacobian $$\textbf J=\frac{\partial(x_1,\ldots,x_n)}{\partial(t_1,\ldots,t_n)}=\left(\begin{array}{ccc}\frac{\partial x_1}{\partial t_1}&\cdots&\frac{\partial x_1}{\partial t_n}\\\vdots&\ddots&\vdots\\\frac{\partial x_n}{\partial t_1}&\ldots&\frac{\partial x_n}{\partial t_n}\end{array}\right)$$ gives me a means of obtaining differentials $dt_1,\ldots,dt_n$ from $dx_1,\ldots,dx_n$. Geometric intuition is provided: The collection $dt$ is the volume/area element of an infinitesimal portion to be integrated. The equation for each member of $dx$ expressed as a vector is $$d\textbf x_k=\sum_{i=0}^n\frac{\partial x_k}{\partial t_i}d\textbf t_i$$ and in a two-dimensional system, we have $$d\textbf x_k=\left<\frac{\partial x_k}{\partial t_1}dt_1,\frac{\partial x_k}{\partial t_2}dt_2,0\right>$$ Then we find the area element with $dx$ corresponding to $dt$ $$|d\textbf x_1\times d\textbf x_2|=\left|\frac{\partial x_1}{\partial t_1}\frac{\partial x_2}{\partial t_2}-\frac{\partial x_1}{\partial t_2}\frac{\partial x_2}{\partial t_1}\right|dt_1dt_2$$ which is the same as $|\det\textbf J|=\left|\det\frac{\partial(x_1,x_2)}{\partial(t_1,t_2)}\right|$, which concludes the motivation for the Jacobian in basic calculus. I'm confident the same exercise as above could be performed in 3D, except with $d\textbf x_1\cdot(d\textbf x_2\times d\textbf x_3)$ for my volume element, but that's where it ends, since there's no notion of cross products above 3D in calculus. My questions: Cross products don't extend to above three dimensions. So far I've had 3D Jacobians explained. What about $n$-dimensional Jacobians? What about coordinate transformations with $T:(t_1,\ldots,t_m)\to(x_1,\ldots,x_n)$ and $n\neq m$? Is the motivation given in calculus at all rigorous? I'm still not certain exactly what a differential is, and why I can manipulate them as above.",,"['multivariable-calculus', 'soft-question', 'vectors', 'differential']"
96,"Differentiating Under the Integral: derivative of $	f(x) = \int_0 ^{x^2} e^{x y^2} \,dy$",Differentiating Under the Integral: derivative of,"	f(x) = \int_0 ^{x^2} e^{x y^2} \,dy","This problem has been giving me trouble-perhaps you can help.  I met this problem in the context of an exam, so it may be that I am on the right track and what I have done so far is as far as a grader would like to see the problem taken, but there's still a chance I'm simply not seeing how to evaluate this integral.  The problem: Compute the derivative of the function  $$ 	f(x) = \int_{0} ^{x^2} e^{x y^2} \,dy $$ The general rule for differentiating under the integral states that  $$ 	\frac{d}{dx} \left( \int_{a(x)} ^{b(x)} f(x,y) \,dy   \right) = \int_{a(x)} ^{b(x)} \frac{\partial }{\partial x} ( f(x,y)) \,dy + f(x,b(x)) b^\prime (x) - f(x, a(x)) a^\prime (x) $$ In our particular case, $a(x) = 0$, $b(x) = x^2$, and $f(x,y) = x y^2$.  Therefore,  \begin{align*} f^\prime (x) &= \int_{0} ^{x^2} \frac{\partial }{\partial x} (e^{x y^2}) \,dy + f(x , x^2) (2x) - f(x, 0)(0) \\  &= \int_{0} ^{x^2} y^2 e^{x y^2} \,dy + 2x e^{x^5} \\ \end{align*} I don't see a clear way to evaluate the integral in this expression.  Do you?  Any help will be greatly appreciated!","This problem has been giving me trouble-perhaps you can help.  I met this problem in the context of an exam, so it may be that I am on the right track and what I have done so far is as far as a grader would like to see the problem taken, but there's still a chance I'm simply not seeing how to evaluate this integral.  The problem: Compute the derivative of the function  $$ 	f(x) = \int_{0} ^{x^2} e^{x y^2} \,dy $$ The general rule for differentiating under the integral states that  $$ 	\frac{d}{dx} \left( \int_{a(x)} ^{b(x)} f(x,y) \,dy   \right) = \int_{a(x)} ^{b(x)} \frac{\partial }{\partial x} ( f(x,y)) \,dy + f(x,b(x)) b^\prime (x) - f(x, a(x)) a^\prime (x) $$ In our particular case, $a(x) = 0$, $b(x) = x^2$, and $f(x,y) = x y^2$.  Therefore,  \begin{align*} f^\prime (x) &= \int_{0} ^{x^2} \frac{\partial }{\partial x} (e^{x y^2}) \,dy + f(x , x^2) (2x) - f(x, 0)(0) \\  &= \int_{0} ^{x^2} y^2 e^{x y^2} \,dy + 2x e^{x^5} \\ \end{align*} I don't see a clear way to evaluate the integral in this expression.  Do you?  Any help will be greatly appreciated!",,"['integration', 'multivariable-calculus', 'derivatives']"
97,Laplacian of scalar field,Laplacian of scalar field,,"I have to find the Laplacian of the following scalar field, but I can't figure out how to convert it to Cartesian/spherical coordinates. $$ U(r)=A e^{-j\left(\vec{k} \cdot \vec{r}\right)}, $$ where $\vec{k}$ and $\vec{r}$ are vectors. Is there a way to solve this?","I have to find the Laplacian of the following scalar field, but I can't figure out how to convert it to Cartesian/spherical coordinates. $$ U(r)=A e^{-j\left(\vec{k} \cdot \vec{r}\right)}, $$ where $\vec{k}$ and $\vec{r}$ are vectors. Is there a way to solve this?",,"['multivariable-calculus', 'laplacian', 'scalar-fields']"
98,Question about Angle-Preserving Operators,Question about Angle-Preserving Operators,,"This an exercise out of Spivak's "" Calculus on Manifolds "". Edit:  There was a typo in the exercise as is noted below in the answers.  The statement has been edited to reflect this. Given $x,y\in\mathbb{R}^{n}$, the angle between $x$ and $y$ is defined by $$\angle(x,y) = \arccos\left(\frac{\langle x,y \rangle}{|x|\cdot |y|}\right),$$ where $\langle x,y \rangle$ denotes the standard Euclidean inner product. A linear operator $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is said to be angle-preserving if $\angle(T(x),T(y)) = \angle(x,y)$ for every $x,y\in\mathbb{R}^{n}$. The exercise as stated: Let $\{x_{1},\dots, x_{n}\}$ be a basis for $\mathbb{R}^{n}$.  Then suppose that $\lambda_{1}, \dots, \lambda_{n}\in \mathbb{R}$ are such that $Tx_{j} = \lambda_{j}x_{j}$ for each $j = 1,\dots, n$. Then $T$ is angle-preserving only if (not if and only if!)$|\lambda_{i}| = |\lambda_{j}|$ for every $1\leq i\leq j\leq n$. I'm having problems with the $(\Rightarrow)$ direction. My best attempt (which seems to lead nowhere) is to suppose that $|\lambda_{j}|\neq |\lambda_{k}|$.  Then by assumption, \begin{align*} \angle(Tx_{j},Tx_{k}) & = \arccos\left(\frac{\langle Tx_{j},Tx_{k} \rangle}{|Tx_{j}|\cdot |Tx_{k}|}\right)\\ & = \arccos\left(\frac{\langle \lambda_{j}{x_{j}},\lambda_{k}{x_{k}} \rangle}{|\lambda_{j}{x_{j}}|\cdot |\lambda_{k}{x_{k}}|}\right)\\ & = \arccos\left(\frac{\lambda_{j}\lambda_{k}\langle {x_{j}},{x_{k}} \rangle}{|\lambda_{j}|\cdot|\lambda_{k}|\cdot|{x_{j}}|\cdot |{x_{k}}|}\right)\\ & = \arccos\left(\text{sign}(\lambda_{j})\text{sign}(\lambda_{k})\frac{\langle {x_{j}},{x_{k}} \rangle }{|{x_{j}}|\cdot |{x_{k}}|}\right)\\ \end{align*} may also be calculated as \begin{align*} \angle(Tx_{j},Tx_{k}) & =  \angle(x_{j},x_{k})\\ & = \arccos\left(\frac{\langle x_{j},x_{k} \rangle}{|x_{j}|\cdot |x_{k}|}\right). \end{align*} Then since $\arccos$ is injective, I believe I can make the jump that $\text{sign}(\lambda_{j})\text{sign}(\lambda_{k}) = 1$, which does not resemble the conclusion that I should arrive at. Note:  I wasn't sure what tag to put this under, so anyone who knows better please feel free to adjust. Thanks for any help you can give.","This an exercise out of Spivak's "" Calculus on Manifolds "". Edit:  There was a typo in the exercise as is noted below in the answers.  The statement has been edited to reflect this. Given $x,y\in\mathbb{R}^{n}$, the angle between $x$ and $y$ is defined by $$\angle(x,y) = \arccos\left(\frac{\langle x,y \rangle}{|x|\cdot |y|}\right),$$ where $\langle x,y \rangle$ denotes the standard Euclidean inner product. A linear operator $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is said to be angle-preserving if $\angle(T(x),T(y)) = \angle(x,y)$ for every $x,y\in\mathbb{R}^{n}$. The exercise as stated: Let $\{x_{1},\dots, x_{n}\}$ be a basis for $\mathbb{R}^{n}$.  Then suppose that $\lambda_{1}, \dots, \lambda_{n}\in \mathbb{R}$ are such that $Tx_{j} = \lambda_{j}x_{j}$ for each $j = 1,\dots, n$. Then $T$ is angle-preserving only if (not if and only if!)$|\lambda_{i}| = |\lambda_{j}|$ for every $1\leq i\leq j\leq n$. I'm having problems with the $(\Rightarrow)$ direction. My best attempt (which seems to lead nowhere) is to suppose that $|\lambda_{j}|\neq |\lambda_{k}|$.  Then by assumption, \begin{align*} \angle(Tx_{j},Tx_{k}) & = \arccos\left(\frac{\langle Tx_{j},Tx_{k} \rangle}{|Tx_{j}|\cdot |Tx_{k}|}\right)\\ & = \arccos\left(\frac{\langle \lambda_{j}{x_{j}},\lambda_{k}{x_{k}} \rangle}{|\lambda_{j}{x_{j}}|\cdot |\lambda_{k}{x_{k}}|}\right)\\ & = \arccos\left(\frac{\lambda_{j}\lambda_{k}\langle {x_{j}},{x_{k}} \rangle}{|\lambda_{j}|\cdot|\lambda_{k}|\cdot|{x_{j}}|\cdot |{x_{k}}|}\right)\\ & = \arccos\left(\text{sign}(\lambda_{j})\text{sign}(\lambda_{k})\frac{\langle {x_{j}},{x_{k}} \rangle }{|{x_{j}}|\cdot |{x_{k}}|}\right)\\ \end{align*} may also be calculated as \begin{align*} \angle(Tx_{j},Tx_{k}) & =  \angle(x_{j},x_{k})\\ & = \arccos\left(\frac{\langle x_{j},x_{k} \rangle}{|x_{j}|\cdot |x_{k}|}\right). \end{align*} Then since $\arccos$ is injective, I believe I can make the jump that $\text{sign}(\lambda_{j})\text{sign}(\lambda_{k}) = 1$, which does not resemble the conclusion that I should arrive at. Note:  I wasn't sure what tag to put this under, so anyone who knows better please feel free to adjust. Thanks for any help you can give.",,"['differential-geometry', 'operator-theory']"
99,Calculating the equation of a multivariable surface of revolution,Calculating the equation of a multivariable surface of revolution,,"I'm stucked with a surface equation problem so I would be very thankful if someone could help me with it. What the excercise says: Find the equation of the revolution surface that is spanned when the curve $xy=10$, $z=0$ is spinned around the axis $y=x$, $z=0$ My approach: I know that the axis direction is described by the vector $d=(1,1,0)$, so when I let Q(u,v,w) be a point in the curve and P(x,y,z) be a point of the surface spanned by the rotation of Q around the given axis, I have 2 equations: \begin{equation} u+v=t \end{equation} \begin{equation} x+y=t \end{equation} I deduced that from the fact that P and Q are in the same plane that is normal to the axis. I also have the equations: \begin{equation} uv=10 \end{equation} \begin{equation} w=0 \end{equation} Of course that is inmediate because Q(u,v,w) is a point in the curve. I tried using the third equation so I could have $v=\frac{10}{u}$, then because of that and equation 3 we would have $$u^2-tu+10=0$$ By solving that I obtain $$u=\displaystyle\frac{t+-\sqrt{t^2-40}}{2}$$ And there is my great dilema, how do I know which of the two possibles $u$ must I choose? I would be very, very thankful if someone could answer me this. Greetings!","I'm stucked with a surface equation problem so I would be very thankful if someone could help me with it. What the excercise says: Find the equation of the revolution surface that is spanned when the curve $xy=10$, $z=0$ is spinned around the axis $y=x$, $z=0$ My approach: I know that the axis direction is described by the vector $d=(1,1,0)$, so when I let Q(u,v,w) be a point in the curve and P(x,y,z) be a point of the surface spanned by the rotation of Q around the given axis, I have 2 equations: \begin{equation} u+v=t \end{equation} \begin{equation} x+y=t \end{equation} I deduced that from the fact that P and Q are in the same plane that is normal to the axis. I also have the equations: \begin{equation} uv=10 \end{equation} \begin{equation} w=0 \end{equation} Of course that is inmediate because Q(u,v,w) is a point in the curve. I tried using the third equation so I could have $v=\frac{10}{u}$, then because of that and equation 3 we would have $$u^2-tu+10=0$$ By solving that I obtain $$u=\displaystyle\frac{t+-\sqrt{t^2-40}}{2}$$ And there is my great dilema, how do I know which of the two possibles $u$ must I choose? I would be very, very thankful if someone could answer me this. Greetings!",,"['multivariable-calculus', 'surfaces']"
