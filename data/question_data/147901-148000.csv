,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Intuition behind proof in the Rudin book that there is no largest/smallest real number. [duplicate],Intuition behind proof in the Rudin book that there is no largest/smallest real number. [duplicate],,"This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 8 years ago . In Rudin's Principles of Mathematical Analysis (3rd ed), he proves (at the very beginning: example 1.1) that the set $A$ of all positive rationals $p$ such that $p^2<2$ contains no largest number and the set $B$ of all positive rationals $p$ such that $p^2>2$ contains no smallest number. To show this, he starts by associating with each rational $p>0$, the number $q=p-(p^2-2)/(p+2) = (2p+2)/(p+2)$. And thus $q^2-2=2(p^2-2)/(p+2)^2$. My question is actually simple: from where does he get the expression for $q$ and how does one get the intuition to come up with the form of the number $q$?","This question already has answers here : Choice of $q$ in Baby Rudin's Example 1.1 (16 answers) Closed 8 years ago . In Rudin's Principles of Mathematical Analysis (3rd ed), he proves (at the very beginning: example 1.1) that the set $A$ of all positive rationals $p$ such that $p^2<2$ contains no largest number and the set $B$ of all positive rationals $p$ such that $p^2>2$ contains no smallest number. To show this, he starts by associating with each rational $p>0$, the number $q=p-(p^2-2)/(p+2) = (2p+2)/(p+2)$. And thus $q^2-2=2(p^2-2)/(p+2)^2$. My question is actually simple: from where does he get the expression for $q$ and how does one get the intuition to come up with the form of the number $q$?",,"['analysis', 'inequality', 'numerical-methods', 'radicals', 'diophantine-approximation']"
1,"Prove that $\{ar+b:a,b\in Z\}$, where $r$ is an irrational number is dense in $R$ by using the following lemma","Prove that , where  is an irrational number is dense in  by using the following lemma","\{ar+b:a,b\in Z\} r R","Prove that $\{ar+b:a,b\in Z\}$ , where $r$ is an irrational number is dense in $R$ by using the following lemma: If $x$ is irrational, there are infinitely many rational numbers $h/k$ with $k\gt 0$ such that $|x-h/k|\lt 1/k^2$ . My work: Say $S=\{ar+b:a,b\in Z\}$ . Given any real number $x$ and $\delta \gt 0$ , by the above lemma, we can find some integer $k\gt 1/\delta$ such that $|kr-h|\lt \delta$ . Now, if we let $|kr-h|=L$ , then $L\in S$ , and I think since this number is less than $\delta$ , we can multiply it by some integer $z$ to make it come inside the interval $(x-\delta, x+\delta)$ . However, I cannot show this rigorously. How can I write this down? I would greatly appreciate any help.","Prove that , where is an irrational number is dense in by using the following lemma: If is irrational, there are infinitely many rational numbers with such that . My work: Say . Given any real number and , by the above lemma, we can find some integer such that . Now, if we let , then , and I think since this number is less than , we can multiply it by some integer to make it come inside the interval . However, I cannot show this rigorously. How can I write this down? I would greatly appreciate any help.","\{ar+b:a,b\in Z\} r R x h/k k\gt 0 |x-h/k|\lt 1/k^2 S=\{ar+b:a,b\in Z\} x \delta \gt 0 k\gt 1/\delta |kr-h|\lt \delta |kr-h|=L L\in S \delta z (x-\delta, x+\delta)","['real-analysis', 'analysis', 'solution-verification', 'real-numbers']"
2,"What the terms ""basis functions"" and ""orthogonal"" denote in the case of signals?","What the terms ""basis functions"" and ""orthogonal"" denote in the case of signals?",,"I am a beginer. I have read that any given signal whether it is simple or complex one,can be represented as summation of orthogonal basis functions. Here, what the terms Orthogonal and Basis function denote in case of signals?","I am a beginer. I have read that any given signal whether it is simple or complex one,can be represented as summation of orthogonal basis functions. Here, what the terms Orthogonal and Basis function denote in case of signals?",,"['real-analysis', 'analysis', 'functions', 'harmonic-analysis', 'signal-processing']"
3,"A function $f$ twice differentiable, such that $f$ and $f''$ are bounded imply $f'$ bounded?","A function  twice differentiable, such that  and  are bounded imply  bounded?",f f f'' f',"Let $f:(0,\infty) \to \mathbb{R} $ twice differentiable. Suppose $A$, $C$ in $(0,\infty)$ such that for each $x>0$ we have $|f(x)|<A$ and $|f''(x)|<C$. Prove that for each $x>0$ and each $h>0$ we have $|f'(x)| \le (A/h)$+$Ch$ Using Mean Value Theorem $f'(x)-f'(y) = f''(c) (x-y)$ for some $c$. Since $f''$ is bounded by $C$ $f'(x)-f'(y) \le  C (x-y)$. Letting $h=x-y$ we got $f'(x) \le  Ch + f'(y)$ How can I bound $f'(y)$ by $A/h$ to obtain the desired bound??","Let $f:(0,\infty) \to \mathbb{R} $ twice differentiable. Suppose $A$, $C$ in $(0,\infty)$ such that for each $x>0$ we have $|f(x)|<A$ and $|f''(x)|<C$. Prove that for each $x>0$ and each $h>0$ we have $|f'(x)| \le (A/h)$+$Ch$ Using Mean Value Theorem $f'(x)-f'(y) = f''(c) (x-y)$ for some $c$. Since $f''$ is bounded by $C$ $f'(x)-f'(y) \le  C (x-y)$. Letting $h=x-y$ we got $f'(x) \le  Ch + f'(y)$ How can I bound $f'(y)$ by $A/h$ to obtain the desired bound??",,"['real-analysis', 'analysis']"
4,"Convolution with Gaussian, without distribution theory, part 1","Convolution with Gaussian, without distribution theory, part 1",,"I only know basic $L^p$ theory (nothing about distributions) and am trying to prove the following: Let $t>0$, $f\in L^{p}(\mathbb{R}^n,m)$, $\Gamma(t,x)=(4\pi t)^{-n/2}e^{-|x|^2/4t}$ and $$ u(t,x)=\int_{\mathbb{R}^n}\Gamma(t,x-y)f(y)dy. $$ If $1\leq p<\infty$, then $u(t,\cdot)\to f$ in $L^p$ as $t\to 0$. (EDIT: I've asked the original parts 2 and 3 as separate questions.) I've shown that $||u(t,\cdot)||_p\leq ||f||_p$ for $1\leq p\le\infty$ but haven't gotten anywhere from here. Could someone please give some hints or suggest a reference (these seem to be very well-known and widely applicable results).","I only know basic $L^p$ theory (nothing about distributions) and am trying to prove the following: Let $t>0$, $f\in L^{p}(\mathbb{R}^n,m)$, $\Gamma(t,x)=(4\pi t)^{-n/2}e^{-|x|^2/4t}$ and $$ u(t,x)=\int_{\mathbb{R}^n}\Gamma(t,x-y)f(y)dy. $$ If $1\leq p<\infty$, then $u(t,\cdot)\to f$ in $L^p$ as $t\to 0$. (EDIT: I've asked the original parts 2 and 3 as separate questions.) I've shown that $||u(t,\cdot)||_p\leq ||f||_p$ for $1\leq p\le\infty$ but haven't gotten anywhere from here. Could someone please give some hints or suggest a reference (these seem to be very well-known and widely applicable results).",,"['real-analysis', 'analysis', 'normal-distribution', 'lp-spaces', 'gaussian-integral']"
5,Composition of relations. Both relations are functional and mutually inverse mappings. Zorich - MAI p22,Composition of relations. Both relations are functional and mutually inverse mappings. Zorich - MAI p22,,"$\def\R{\mathcal{R}}$ The composition $\mathcal{R}_2 \circ \mathcal{R}_1$ of the relations $\mathcal{R}_1$ and $\mathcal{R}_2$ is defined as follows: $$\mathcal{R}_2 \circ \mathcal{R}_1 := \{(x,z)|\exists y(x\mathcal{R}_1 y \wedge y \mathcal{R}_2 z\}$$ Let $\Delta_X$ be the diagonal of $X^2$ and $\Delta_Y$ the diagonal of $Y^2$ . Show that if the relations $\mathcal{R}_1\subset X\times Y$ and $\mathcal{R}_2 \subset Y \times X$ are such that $(\mathcal{R}_2 \circ \mathcal{R}_1=\Delta_X)\wedge(\mathcal{R}_1 \circ \mathcal{R}_2 = \Delta_Y)$ , then both relations are functional and define mutually inverse mappings of $X$ and $Y$ . I will try to solve this below, but first I must note, this is from Zorich - Mathematical Analysis I, page 22, Exercise 1.3.5, Q1a). Furthermore, I do not condone such long questions. My attempt: What does it mean for a relation to be functional? $$(x \R y_1) \wedge (x \R y_2 ) \implies ( y_1 = y_2)$$ This qualifies the relation as a function. What does it mean for two relations to define mutually inverse mappings of $X$ and $Y$ . Since the relations are functions(if the above does hold), then to satisfy this we need: $$f:X\to Y, G: Y\to X \text{ to have: }$$ $$g \circ f = e_X, f \circ g = e_Y$$ Where $e_X$ and $e_Y$ are the identity mappings on $X$ and $Y$ respectively.","The composition of the relations and is defined as follows: Let be the diagonal of and the diagonal of . Show that if the relations and are such that , then both relations are functional and define mutually inverse mappings of and . I will try to solve this below, but first I must note, this is from Zorich - Mathematical Analysis I, page 22, Exercise 1.3.5, Q1a). Furthermore, I do not condone such long questions. My attempt: What does it mean for a relation to be functional? This qualifies the relation as a function. What does it mean for two relations to define mutually inverse mappings of and . Since the relations are functions(if the above does hold), then to satisfy this we need: Where and are the identity mappings on and respectively.","\def\R{\mathcal{R}} \mathcal{R}_2 \circ \mathcal{R}_1 \mathcal{R}_1 \mathcal{R}_2 \mathcal{R}_2 \circ \mathcal{R}_1 := \{(x,z)|\exists y(x\mathcal{R}_1 y \wedge y \mathcal{R}_2 z\} \Delta_X X^2 \Delta_Y Y^2 \mathcal{R}_1\subset X\times Y \mathcal{R}_2 \subset Y \times X (\mathcal{R}_2 \circ \mathcal{R}_1=\Delta_X)\wedge(\mathcal{R}_1 \circ \mathcal{R}_2 = \Delta_Y) X Y (x \R y_1) \wedge (x \R y_2 ) \implies ( y_1 = y_2) X Y f:X\to Y, G: Y\to X \text{ to have: } g \circ f = e_X, f \circ g = e_Y e_X e_Y X Y",['analysis']
6,$\frac{\partial^2 u}{\partial t^2}-c^2\frac{\partial^2u}{\partial x^2}=0\text{ implies } \frac{\partial^2 u}{\partial z \partial y}=0$,,\frac{\partial^2 u}{\partial t^2}-c^2\frac{\partial^2u}{\partial x^2}=0\text{ implies } \frac{\partial^2 u}{\partial z \partial y}=0,"I have the following question: Show that if $z=x+ct$ and $y=x-ct$ then: $$\frac{\partial^2 u}{\partial t^2}-c^2\frac{\partial^2u}{\partial x^2}=0\text{ implies } \frac{\partial^2 u}{\partial z \partial y}=0$$ If I had $u$ it would follow trivially I imagine, but I don't have $u$. It would seem that $u$ is linear in $t,x$ and since $t,x$ are linear in $y,z$ it seems logically consistent, but I am not sure what to do. Perhaps this is related to harmonic functions. What should I first do to get this started? Perhaps I notice that $c=\frac{z-x}{t} \text{ and } c=\frac{x-y}{t}$ and thus $\frac{z-x}{t} =\frac{x-y}{t}\implies x-y=z-x\implies 2x=y+z$","I have the following question: Show that if $z=x+ct$ and $y=x-ct$ then: $$\frac{\partial^2 u}{\partial t^2}-c^2\frac{\partial^2u}{\partial x^2}=0\text{ implies } \frac{\partial^2 u}{\partial z \partial y}=0$$ If I had $u$ it would follow trivially I imagine, but I don't have $u$. It would seem that $u$ is linear in $t,x$ and since $t,x$ are linear in $y,z$ it seems logically consistent, but I am not sure what to do. Perhaps this is related to harmonic functions. What should I first do to get this started? Perhaps I notice that $c=\frac{z-x}{t} \text{ and } c=\frac{x-y}{t}$ and thus $\frac{z-x}{t} =\frac{x-y}{t}\implies x-y=z-x\implies 2x=y+z$",,"['analysis', 'partial-differential-equations', 'partial-derivative', 'wave-equation']"
7,Path Connectedness and fixed points,Path Connectedness and fixed points,,"We have the following given to us, Let $α, β \colon [0, 1] \to [0, 1]$ be (not necessarily continuous) functions such that $α(x) ≤ β(x)$, for all $x ∈ [0, 1]$. The set $K = \{\,(x, y); α(x) ≤ y ≤ β(x)\,\}$ is closed in $\mathbb R^2$ . Show that there exists $t ∈ [0, 1]$ such that $(t, t) ∈ K$. We think that if $K$ is closed, then it must be path-connected. In which case, we have a continuous function from $[0,1]$ to $[0,1]$ that is contained in $K$. Therefore can use the intermediate value theorem to show that this function must have a fixed point, which would conclude our argument. Our difficulty is that we are unable to prove that $K$ is actually path-connected, even though every example of $α$ and $β$ that we can think of satisfying the required properties make $K$ path connected.","We have the following given to us, Let $α, β \colon [0, 1] \to [0, 1]$ be (not necessarily continuous) functions such that $α(x) ≤ β(x)$, for all $x ∈ [0, 1]$. The set $K = \{\,(x, y); α(x) ≤ y ≤ β(x)\,\}$ is closed in $\mathbb R^2$ . Show that there exists $t ∈ [0, 1]$ such that $(t, t) ∈ K$. We think that if $K$ is closed, then it must be path-connected. In which case, we have a continuous function from $[0,1]$ to $[0,1]$ that is contained in $K$. Therefore can use the intermediate value theorem to show that this function must have a fixed point, which would conclude our argument. Our difficulty is that we are unable to prove that $K$ is actually path-connected, even though every example of $α$ and $β$ that we can think of satisfying the required properties make $K$ path connected.",,"['analysis', 'connectedness', 'fixed-point-theorems']"
8,"Find this maximum of this $\frac{\int_{0}^{\pi}f(x) \, dx}{\int_{0}^{\pi} f(x)\sin x\,dx}$",Find this maximum of this,"\frac{\int_{0}^{\pi}f(x) \, dx}{\int_{0}^{\pi} f(x)\sin x\,dx}","Question: Assmue that $\int_0^\pi f(x)\,dx$ and $\int_0^\pi f(x)\sin x\,dx$ is convergence,and $f(x)>0,\forall x\in(0,\pi)$   Find this maximum as possible for all function $f$   $$I=\dfrac{\int_0^\pi f(x)\,dx}{\int_0^\pi f(x)\sin{x}\,dx}$$ show that： $$I\le\dfrac{4}{\pi}?$$ I think this problem is interesting,But I can't.","Question: Assmue that $\int_0^\pi f(x)\,dx$ and $\int_0^\pi f(x)\sin x\,dx$ is convergence,and $f(x)>0,\forall x\in(0,\pi)$   Find this maximum as possible for all function $f$   $$I=\dfrac{\int_0^\pi f(x)\,dx}{\int_0^\pi f(x)\sin{x}\,dx}$$ show that： $$I\le\dfrac{4}{\pi}?$$ I think this problem is interesting,But I can't.",,['analysis']
9,approximate a Borel set by a continuous,approximate a Borel set by a continuous,,"I wonder if it is possible to approximate a Borel set by a continuous function i.e. Let $B$ a Borel set in $(X,d)$ (compact separable metric space) I wonder if there continuous functions $f_n:X\rightarrow \mathbb{R}$ such that $f_n\rightarrow\chi_B$ ? It is possible that  $f_n\rightarrow\chi_B$ uniformly ? Note: $\chi_B$ is the characteristic function of B. Any suggestion is welcome, thanks.","I wonder if it is possible to approximate a Borel set by a continuous function i.e. Let $B$ a Borel set in $(X,d)$ (compact separable metric space) I wonder if there continuous functions $f_n:X\rightarrow \mathbb{R}$ such that $f_n\rightarrow\chi_B$ ? It is possible that  $f_n\rightarrow\chi_B$ uniformly ? Note: $\chi_B$ is the characteristic function of B. Any suggestion is welcome, thanks.",,"['real-analysis', 'analysis', 'convergence-divergence', 'continuity']"
10,Is $f(x)=\sum_{k\in\mathbb N}\frac1k\sin\frac x{2^k}$ bounded?,Is  bounded?,f(x)=\sum_{k\in\mathbb N}\frac1k\sin\frac x{2^k},"$$f(x)=\sum_{k\in\mathbb N}\frac1k\sin\frac x{2^k}$$Is this function bounded? So obviously this converges because $|\frac1k\sin\frac x{2^k}|<|\frac x{2^k}|$ and $\sum\frac x{2^k}$ converges by the integral test. Now I need to show that there exists a $N$ for all $y\in\text{range}\,f$ such that $|y|<N$. So I think: Let $a_k=\frac1k$ and $b_k=\sin\frac x{2^k}$. From the Schwarz inequality we get $|\sum a_kb_k|\leq\sqrt{\sum|a_k|^2\sum|b_k|^2}$. Since $a_k>0$ for all $k$ we have $a_k=|a_k|$ and thus $|a_k|^2=\frac1{k^2}$. $\sum\frac1{k^2}=\frac{\pi^2}6$ from the Reimann-Zeta. Now all we have left to prove is that $\sum|b_k|^2$ is bounded as well.","$$f(x)=\sum_{k\in\mathbb N}\frac1k\sin\frac x{2^k}$$Is this function bounded? So obviously this converges because $|\frac1k\sin\frac x{2^k}|<|\frac x{2^k}|$ and $\sum\frac x{2^k}$ converges by the integral test. Now I need to show that there exists a $N$ for all $y\in\text{range}\,f$ such that $|y|<N$. So I think: Let $a_k=\frac1k$ and $b_k=\sin\frac x{2^k}$. From the Schwarz inequality we get $|\sum a_kb_k|\leq\sqrt{\sum|a_k|^2\sum|b_k|^2}$. Since $a_k>0$ for all $k$ we have $a_k=|a_k|$ and thus $|a_k|^2=\frac1{k^2}$. $\sum\frac1{k^2}=\frac{\pi^2}6$ from the Reimann-Zeta. Now all we have left to prove is that $\sum|b_k|^2$ is bounded as well.",,"['calculus', 'real-analysis', 'analysis', 'summation']"
11,triangular series and inequality [duplicate],triangular series and inequality [duplicate],,"This question already has answers here : how can I show this question : function (2 answers) Closed 5 years ago . Let $a_1,a_2,\ldots,a_n\in \mathbb R$ and $f(x)=a_1\sin x+a_2\sin 2x+\ldots+a_n\sin nx$  such that $|f(x)|\leq|\sin x|$ for every $x\in \mathbb R$. Prove that $|a_1+2a_2+3a_3+\ldots+na_n|\leq1$. Solve: I solved it very simple. $$|a_1+2a_2+\ldots+na_n|=\lim_{x\to 0}\frac{|a_1\sin x+a_2\sin 2x+\ldots+a_n\sin nx|}{|\sin x|}=\lim_{x\to 0}\frac{|f(x)|}{|\sin x|}\leq1.$$","This question already has answers here : how can I show this question : function (2 answers) Closed 5 years ago . Let $a_1,a_2,\ldots,a_n\in \mathbb R$ and $f(x)=a_1\sin x+a_2\sin 2x+\ldots+a_n\sin nx$  such that $|f(x)|\leq|\sin x|$ for every $x\in \mathbb R$. Prove that $|a_1+2a_2+3a_3+\ldots+na_n|\leq1$. Solve: I solved it very simple. $$|a_1+2a_2+\ldots+na_n|=\lim_{x\to 0}\frac{|a_1\sin x+a_2\sin 2x+\ldots+a_n\sin nx|}{|\sin x|}=\lim_{x\to 0}\frac{|f(x)|}{|\sin x|}\leq1.$$",,"['calculus', 'analysis']"
12,Is there a standard $L^2$ norm for multi-valued function $f:\mathbb R^n \to \mathbb R^n$?,Is there a standard  norm for multi-valued function ?,L^2 f:\mathbb R^n \to \mathbb R^n,"Equipping $\mathbb R^n$ with the usual product Lebesgue measure, what is the standard $L^2$ norm for the function $f :\mathbb R^n \to \mathbb R^n$ define by \begin{align} f(x) &=\left(f_1(x), f_2(x), \ldots, f_n(x) \right), \end{align} where $x =\left(x_1,x_2, \ldots, x_n\right)$ Combining standard Euclidean norm and usual $L^2$ norm, I would say $$ \| f \| _ 2 = \sqrt{ \sum_{i=1}^n \int \left| f_i(x) \right|^2 dx }. \tag{1} $$ But I couldn't find any reference confirming my intuition, so I'm wondering if there is another canonical way to define (1)","Equipping $\mathbb R^n$ with the usual product Lebesgue measure, what is the standard $L^2$ norm for the function $f :\mathbb R^n \to \mathbb R^n$ define by \begin{align} f(x) &=\left(f_1(x), f_2(x), \ldots, f_n(x) \right), \end{align} where $x =\left(x_1,x_2, \ldots, x_n\right)$ Combining standard Euclidean norm and usual $L^2$ norm, I would say $$ \| f \| _ 2 = \sqrt{ \sum_{i=1}^n \int \left| f_i(x) \right|^2 dx }. \tag{1} $$ But I couldn't find any reference confirming my intuition, so I'm wondering if there is another canonical way to define (1)",,['real-analysis']
13,right continuous continuous function is measurable,right continuous continuous function is measurable,,"Let $f: S \times [0, \infty)\rightarrow \mathbb{R}$ satisfy $f(x, t)$ is continuous in $x$ for each $t$ and right continuous in $t$ for each $x \in S$.  Here $S$ is a metric space.  Why is $f$ Borel measurable? (In the joint sense)  I am vaguely familiar with the idea of proof I did a long time ago for if $S$ is instead a Euclidean space, and we have full continuity instead of only right.  Then one interpolated.  But you can't do that here anyway, since $S$ is not Euclidean, and nevermind the fact that you only have right continuity anyway.  I also tried to write $f$ as a limit of joint measurable functions using the right continuity, but failed.","Let $f: S \times [0, \infty)\rightarrow \mathbb{R}$ satisfy $f(x, t)$ is continuous in $x$ for each $t$ and right continuous in $t$ for each $x \in S$.  Here $S$ is a metric space.  Why is $f$ Borel measurable? (In the joint sense)  I am vaguely familiar with the idea of proof I did a long time ago for if $S$ is instead a Euclidean space, and we have full continuity instead of only right.  Then one interpolated.  But you can't do that here anyway, since $S$ is not Euclidean, and nevermind the fact that you only have right continuity anyway.  I also tried to write $f$ as a limit of joint measurable functions using the right continuity, but failed.",,"['real-analysis', 'analysis', 'measure-theory']"
14,Compact $C \subset$ Jordan-measurable $A$ such that $\int_{A-C} 1 < \epsilon$.,Compact  Jordan-measurable  such that .,C \subset A \int_{A-C} 1 < \epsilon,"This question is (3-22) in M. Spivak's Calculus on Manifolds. If $A$ is a Jordan-measurable set and $\epsilon > 0$ show that there is a compact Jordan-measurable set $C \subset A$ such that $\int_{A-C}1 < \epsilon$. Intuitively, I can see why this would be true. Without considering pathological examples immediately, I think: If $A$ is Jordan-measurable then it must be bounded. We could take a closed set $C \subset A$ and for each point $x$ on the boundary of $C$, reduce the distance between $x$ and the boundary of $A$ until it is sufficiently small at each point. In other words, ""expand"" $C$ while staying inside of $A$ until $C$ is ""big enough"" for our claim to hold. I am troubled, however, trying to prove the statement rigourously. Any help with this will be appreciated.","This question is (3-22) in M. Spivak's Calculus on Manifolds. If $A$ is a Jordan-measurable set and $\epsilon > 0$ show that there is a compact Jordan-measurable set $C \subset A$ such that $\int_{A-C}1 < \epsilon$. Intuitively, I can see why this would be true. Without considering pathological examples immediately, I think: If $A$ is Jordan-measurable then it must be bounded. We could take a closed set $C \subset A$ and for each point $x$ on the boundary of $C$, reduce the distance between $x$ and the boundary of $A$ until it is sufficiently small at each point. In other words, ""expand"" $C$ while staying inside of $A$ until $C$ is ""big enough"" for our claim to hold. I am troubled, however, trying to prove the statement rigourously. Any help with this will be appreciated.",,"['real-analysis', 'analysis']"
15,Subset of Cantor set that isn't compact,Subset of Cantor set that isn't compact,,"How to prove that the Cantor set has a subset that is not compact? Actually, I want to prove that every infinite set $X\subset\mathbb{R}^n$ has a subset $Y$ that is not compact. If $X$ isn't bounded, then $X$ has a unbounded subset $Y$ that is not compact. If $X$ is bounded and includes some ball, then $X$ includes a open ball $Y$ that is not compact. But if $X$ is bounded and includes no ball, like the Cantor set, I don't know. I think can be easier start by Cantor set, but I'm not sure. Can you help me? Thanks.","How to prove that the Cantor set has a subset that is not compact? Actually, I want to prove that every infinite set $X\subset\mathbb{R}^n$ has a subset $Y$ that is not compact. If $X$ isn't bounded, then $X$ has a unbounded subset $Y$ that is not compact. If $X$ is bounded and includes some ball, then $X$ includes a open ball $Y$ that is not compact. But if $X$ is bounded and includes no ball, like the Cantor set, I don't know. I think can be easier start by Cantor set, but I'm not sure. Can you help me? Thanks.",,"['real-analysis', 'analysis']"
16,A question on limsup,A question on limsup,,"Let $a_n>0$. Prove that $$\varlimsup_{n\to\infty}n\left(\frac{1+a_{n+1}}{a_n}-1\right)\geq 1.$$ I argue by contradiction. If it is not ture, then $$\exists\ N,\ \forall\ n\geq N, n\left(\frac{1+a_{n+1}}{a_n}-1\right)<1\Rightarrow 1+a_{n+1}-a_n<\frac{a_n}{n}.$$ What contradiction shall I get then....","Let $a_n>0$. Prove that $$\varlimsup_{n\to\infty}n\left(\frac{1+a_{n+1}}{a_n}-1\right)\geq 1.$$ I argue by contradiction. If it is not ture, then $$\exists\ N,\ \forall\ n\geq N, n\left(\frac{1+a_{n+1}}{a_n}-1\right)<1\Rightarrow 1+a_{n+1}-a_n<\frac{a_n}{n}.$$ What contradiction shall I get then....",,"['analysis', 'limsup-and-liminf']"
17,A Fourier series exercise,A Fourier series exercise,,"Can anyone give me a hand with this exercise about Fourier series? Let $f(x)=-\log|2\sin(\frac{x}{2})|\,\,\,$ $0\lt|x|\leq\pi$ 1) Prove that f is integrable in $[-\pi,\pi]$. 2) Calculate the Fourier coefficients of $f$. 3)The Fourier series converge to $f$? $------------------$ What I know: About part 2, as f is even, then it would be enough to calculate the ""$a_n$"" coefficients of the series. That is, the $\int_{-\pi}^{\pi}f(x)cos(nx)$. This integrals can be done integrating by parts, I think. (am I right?) My problems are part 1 and 3, I don't see how to prove them. Thanks for any help. EDIT : Also, i met a problem at part 2. Calculating $a_n$, i arrive at a point where i need to find the value of $\int_{0}^{\pi}cotag(\frac{x}{2})sin(nx)$. I know (checked it numerically) that the value of this integral is $\pi$ for any natural $n$. But I can't find a way to prove this ""by hand"", as integration by parts doesn't seem to work here... Any ideas?","Can anyone give me a hand with this exercise about Fourier series? Let $f(x)=-\log|2\sin(\frac{x}{2})|\,\,\,$ $0\lt|x|\leq\pi$ 1) Prove that f is integrable in $[-\pi,\pi]$. 2) Calculate the Fourier coefficients of $f$. 3)The Fourier series converge to $f$? $------------------$ What I know: About part 2, as f is even, then it would be enough to calculate the ""$a_n$"" coefficients of the series. That is, the $\int_{-\pi}^{\pi}f(x)cos(nx)$. This integrals can be done integrating by parts, I think. (am I right?) My problems are part 1 and 3, I don't see how to prove them. Thanks for any help. EDIT : Also, i met a problem at part 2. Calculating $a_n$, i arrive at a point where i need to find the value of $\int_{0}^{\pi}cotag(\frac{x}{2})sin(nx)$. I know (checked it numerically) that the value of this integral is $\pi$ for any natural $n$. But I can't find a way to prove this ""by hand"", as integration by parts doesn't seem to work here... Any ideas?",,"['analysis', 'fourier-analysis', 'fourier-series']"
18,Show there exists a sequence of positive real numbers s.t. ...,Show there exists a sequence of positive real numbers s.t. ...,,"Let $f_n$  be a sequence of measurable functions on $[0,1]$ with $|f_n(x)|\lt\infty$  a.e. Show there exists a sequence $c_n$  of positive real numbers s.t. $f_n(x)/c_n\to0$  for almost every $x$ in $[0,1]$. Hint: use the borel-cantelli lemma- pick the sequence $c_n$  so that $$m[x{\rm\ in\ }[0,1]:|f_n(x)|/c_n\gt1/n]\lt2^{-n}$$ where $m$ is the measure.","Let $f_n$  be a sequence of measurable functions on $[0,1]$ with $|f_n(x)|\lt\infty$  a.e. Show there exists a sequence $c_n$  of positive real numbers s.t. $f_n(x)/c_n\to0$  for almost every $x$ in $[0,1]$. Hint: use the borel-cantelli lemma- pick the sequence $c_n$  so that $$m[x{\rm\ in\ }[0,1]:|f_n(x)|/c_n\gt1/n]\lt2^{-n}$$ where $m$ is the measure.",,"['real-analysis', 'analysis']"
19,Maximum Principle for Poisson Equation,Maximum Principle for Poisson Equation,,"For a smooth $u(x)$, $x \in \mathbb{R}^n$, satisfying: $\Delta u = -f$ for $||x||<1$ ,    $u=g$ on $||x||=1$ I want to show that there exists a constant $C$ such that: $$\max\{|u|:||x||\leq 1\} \leq C(\max\{|g|:||x||=1\}+\max\{|f|:||x||<1\}).$$","For a smooth $u(x)$, $x \in \mathbb{R}^n$, satisfying: $\Delta u = -f$ for $||x||<1$ ,    $u=g$ on $||x||=1$ I want to show that there exists a constant $C$ such that: $$\max\{|u|:||x||\leq 1\} \leq C(\max\{|g|:||x||=1\}+\max\{|f|:||x||<1\}).$$",,"['calculus', 'analysis', 'partial-differential-equations', 'maximum-principle']"
20,calculate $\lim_{x \rightarrow 0}\left ( x^{-6}\cdot (1-\cos(x)^{\sin(x)})^2 \right )$,calculate,\lim_{x \rightarrow 0}\left ( x^{-6}\cdot (1-\cos(x)^{\sin(x)})^2 \right ),"as in topic, my task is to calculate $$\lim_{x \rightarrow 0}\left ( x^{-6}\cdot (1-\cos x^{\sin x})^2 \right )$$ I do the following: (assuming that de'Hospital is pointless here, as it seems to be) I write Taylor series for $\sin x, \cos x$ $$\sin x=x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+o(x^7)$$ $$\cos x=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+o(x^6)$$ I notice that denominator tells me to use sixth degree something and i get $$\frac{\cos x^{2 \cdot \sin x}- 2 \cdot (\cos x)^{\sin x} + 1}{x^6}$$ and my idea here was to use Bernuolli to $\cos x^{\sin x}$ so that $(1+x)^n\geq1+nx$ but it is going to be nasty and the expression $(...)\geq(...)$ does not tell anything about that limit. Any hints? Thank you in advance.","as in topic, my task is to calculate $$\lim_{x \rightarrow 0}\left ( x^{-6}\cdot (1-\cos x^{\sin x})^2 \right )$$ I do the following: (assuming that de'Hospital is pointless here, as it seems to be) I write Taylor series for $\sin x, \cos x$ $$\sin x=x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+o(x^7)$$ $$\cos x=1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+o(x^6)$$ I notice that denominator tells me to use sixth degree something and i get $$\frac{\cos x^{2 \cdot \sin x}- 2 \cdot (\cos x)^{\sin x} + 1}{x^6}$$ and my idea here was to use Bernuolli to $\cos x^{\sin x}$ so that $(1+x)^n\geq1+nx$ but it is going to be nasty and the expression $(...)\geq(...)$ does not tell anything about that limit. Any hints? Thank you in advance.",,['analysis']
21,on integral equations,on integral equations,,"Consider the equation  $$ \int_{0}^{\infty}\sin(\omega\tau)f(\tau)d\tau =\omega\int_{0}^{\infty}\cos(\omega\tau)f(\tau)d\tau\;, $$ where $\omega >0$ and $f$ is a probability density function. I would like to know whether this equation always has solutions $\omega$.","Consider the equation  $$ \int_{0}^{\infty}\sin(\omega\tau)f(\tau)d\tau =\omega\int_{0}^{\infty}\cos(\omega\tau)f(\tau)d\tau\;, $$ where $\omega >0$ and $f$ is a probability density function. I would like to know whether this equation always has solutions $\omega$.",,"['real-analysis', 'analysis']"
22,Multiple Choice question about a continuous function,Multiple Choice question about a continuous function,,"If $f \colon \mathbb R\to\mathbb R$ is a continuous function, which of the following statements implies that $f(0)=0$? (A) $\int_0^1 f(x)^n \,dx\to 0$ as $n\to\infty$ (B) $\int_0^1 f\left(\frac xn\right) \,dx\to 0$ as $n\to\infty$ (C) $\int_0^1 f(nx) \,dx\to 0$ as $n\to\infty$ (D) $\int_0^1 f(x+n) \,dx\to 0$ as $n\to\infty$ I came across the above problem and do not know how to tackle it. Can someone point me in the right direction? Thanks in advance for your time.","If $f \colon \mathbb R\to\mathbb R$ is a continuous function, which of the following statements implies that $f(0)=0$? (A) $\int_0^1 f(x)^n \,dx\to 0$ as $n\to\infty$ (B) $\int_0^1 f\left(\frac xn\right) \,dx\to 0$ as $n\to\infty$ (C) $\int_0^1 f(nx) \,dx\to 0$ as $n\to\infty$ (D) $\int_0^1 f(x+n) \,dx\to 0$ as $n\to\infty$ I came across the above problem and do not know how to tackle it. Can someone point me in the right direction? Thanks in advance for your time.",,['real-analysis']
23,Prove that $h'(t)=\int_{a}^{b}\frac{\partial\phi}{\partial t}ds$.,Prove that .,h'(t)=\int_{a}^{b}\frac{\partial\phi}{\partial t}ds,"Let $\phi:[a,b]\times[c,b]\to \mathbb{R}$ be continuous. Define $h:[c,d]\to \mathbb{R},h(t)=\int_{a}^{b}\phi(s,t)ds$. Assume that $\frac{\partial\phi}{\partial t}$ exists and is continuous on $[a, b]\times[c, b]$. Prove that $h'(t)=\int_{a}^{b}\frac{\partial\phi}{\partial t}ds$. I have tried to prove it by first differentiating $h(t)$ w.r.t $t$ but it shouldn't be that easy. Is it okay to just differentiate on $h(t)$ w.r.t $t$  directly?  Or how to prove it?","Let $\phi:[a,b]\times[c,b]\to \mathbb{R}$ be continuous. Define $h:[c,d]\to \mathbb{R},h(t)=\int_{a}^{b}\phi(s,t)ds$. Assume that $\frac{\partial\phi}{\partial t}$ exists and is continuous on $[a, b]\times[c, b]$. Prove that $h'(t)=\int_{a}^{b}\frac{\partial\phi}{\partial t}ds$. I have tried to prove it by first differentiating $h(t)$ w.r.t $t$ but it shouldn't be that easy. Is it okay to just differentiate on $h(t)$ w.r.t $t$  directly?  Or how to prove it?",,['analysis']
24,How to prove that such a function is linear?,How to prove that such a function is linear?,,"Let $f:(a.b)\rightarrow \mathbb R$ be a continuous function. How to prove that if  for $\varepsilon >0$ there is a $\delta >0$ such that for $x\in (a,b)$, $|h|< \delta$ such that $x+2h \in (a,b)$ $$\left|\frac{f(x+2h)-2f(x+h)+f(x)}{h^2}\right|<\epsilon, $$ then $f$ is of the form $f(x)=\alpha x+\beta$, where $\alpha,\beta$ are constants ?","Let $f:(a.b)\rightarrow \mathbb R$ be a continuous function. How to prove that if  for $\varepsilon >0$ there is a $\delta >0$ such that for $x\in (a,b)$, $|h|< \delta$ such that $x+2h \in (a,b)$ $$\left|\frac{f(x+2h)-2f(x+h)+f(x)}{h^2}\right|<\epsilon, $$ then $f$ is of the form $f(x)=\alpha x+\beta$, where $\alpha,\beta$ are constants ?",,"['real-analysis', 'analysis']"
25,convergence of infimum,convergence of infimum,,"I have a question that I encountered during my internship: Consider a convergent sequence of continuous, convex functions $\{f_n(x)\}_n$ defined in $\mathbb{R}^M$. These functions are uniformly Lipschitz continuous, that is, $\exists C\in\mathbb{R}$ such that: $$\forall x,y \in \mathbb{R^M},\forall n\ge1\quad |f_n(x)-f_n(y)|\le C|x-y|.$$ Furthermore, each function $f_n(x)$ has a minimizer. The properties of simple convergence and uniform Lipschitz continuity allow us to prove that the convergence is uniform in any compact of $\Bbb R^M$. My question is: Can we demonstrate  that $\inf_{\Bbb R^M}f_n(x)$ converges to $\inf_{\Bbb R^M}f_{\infty}(x)$ as $n\rightarrow\infty$, where $f_{\infty}(x)$ is the limit of $f_n(x)$ and it is supposed that $\inf_{\mathbb{R^M}}f(x)$ is finite? Thanks a lot!","I have a question that I encountered during my internship: Consider a convergent sequence of continuous, convex functions $\{f_n(x)\}_n$ defined in $\mathbb{R}^M$. These functions are uniformly Lipschitz continuous, that is, $\exists C\in\mathbb{R}$ such that: $$\forall x,y \in \mathbb{R^M},\forall n\ge1\quad |f_n(x)-f_n(y)|\le C|x-y|.$$ Furthermore, each function $f_n(x)$ has a minimizer. The properties of simple convergence and uniform Lipschitz continuity allow us to prove that the convergence is uniform in any compact of $\Bbb R^M$. My question is: Can we demonstrate  that $\inf_{\Bbb R^M}f_n(x)$ converges to $\inf_{\Bbb R^M}f_{\infty}(x)$ as $n\rightarrow\infty$, where $f_{\infty}(x)$ is the limit of $f_n(x)$ and it is supposed that $\inf_{\mathbb{R^M}}f(x)$ is finite? Thanks a lot!",,"['real-analysis', 'analysis', 'convex-analysis']"
26,Showing that $\sum \frac{\log n}{n^x}$ converges for $x>1$,Showing that  converges for,\sum \frac{\log n}{n^x} x>1,"I'm trying to show that $\sum \frac{\log n}{n^x}$ converges for $x>1$ by the ratio test. Here's what I've got so far $$\frac{a_{n+1}}{a_n} = \frac{\log (n+1) n^x}{(n+1)^x \log n}$$ $$=\left(\frac{n}{n+1}\right)^x \frac{\log (n+1)}{\log n}$$ but I can't see how to manipulate the $\frac{\log (n+1)}{\log n}$ term to make this congerge to a limit less than 1, can anyone help?","I'm trying to show that $\sum \frac{\log n}{n^x}$ converges for $x>1$ by the ratio test. Here's what I've got so far $$\frac{a_{n+1}}{a_n} = \frac{\log (n+1) n^x}{(n+1)^x \log n}$$ $$=\left(\frac{n}{n+1}\right)^x \frac{\log (n+1)}{\log n}$$ but I can't see how to manipulate the $\frac{\log (n+1)}{\log n}$ term to make this congerge to a limit less than 1, can anyone help?",,[]
27,Limit involving the laplacian,Limit involving the laplacian,,"I'm trying to prove that if $\Omega$ is an open subset of $\mathbb{R}^n$ and $u$ a $C^2$ function then $$\lim_{r\to 0}\frac{2n}{r^2}\left(u(x)-\frac{1}{|\partial B_r(x)|}\int_{\partial B_r(x)}u(y)d\sigma_y\right)=-\Delta u(x).$$ I tried to use the representation formula on balls, but nothing came out of it..","I'm trying to prove that if $\Omega$ is an open subset of $\mathbb{R}^n$ and $u$ a $C^2$ function then $$\lim_{r\to 0}\frac{2n}{r^2}\left(u(x)-\frac{1}{|\partial B_r(x)|}\int_{\partial B_r(x)}u(y)d\sigma_y\right)=-\Delta u(x).$$ I tried to use the representation formula on balls, but nothing came out of it..",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
28,How do I show that this metric space is not convex?,How do I show that this metric space is not convex?,,"Denote $X$, the space of all sequences $\in$ $\mathbb R$. I have a metric $$d(x,y):=\sum_{n=1}^\infty 2^{-n}\frac{| x_n-y_n|}{1+| x_n-y_n|}$$ and $(X,d)$ is a metric space. How would I show that the closed (metric) ball $B[0,\frac{1}{3}]$ with centre $0$ and radius $1/3$ is not convex? I tried working from the definition that a set $A$ in a vector space $X$ is convex if for all $x,y \in A$, and all $t$ in the interval $[0,1]$, $(1 − t ) x + t y \in A,$ but I cant seem to show its not convex. I was looking up on this, and I read somewhere it might have something to do with summable sequences like $x=(1/2,1/2,1/2,...)?$ Im not sure though, I might be wrong.","Denote $X$, the space of all sequences $\in$ $\mathbb R$. I have a metric $$d(x,y):=\sum_{n=1}^\infty 2^{-n}\frac{| x_n-y_n|}{1+| x_n-y_n|}$$ and $(X,d)$ is a metric space. How would I show that the closed (metric) ball $B[0,\frac{1}{3}]$ with centre $0$ and radius $1/3$ is not convex? I tried working from the definition that a set $A$ in a vector space $X$ is convex if for all $x,y \in A$, and all $t$ in the interval $[0,1]$, $(1 − t ) x + t y \in A,$ but I cant seem to show its not convex. I was looking up on this, and I read somewhere it might have something to do with summable sequences like $x=(1/2,1/2,1/2,...)?$ Im not sure though, I might be wrong.",,"['analysis', 'metric-spaces']"
29,Limit for gamma function,Limit for gamma function,,"How can I prove that $$\displaystyle \Gamma(z)=\lim_{n \to \infty} \displaystyle \int_0^n \left( 1-\frac{t}{n}\right)^n  t^{z-1}\ \text{d} t\;=\displaystyle \int_0^{\infty} e^{-t}  t^{z-1}\ \text{d} t\;$$ Issue is how can I prove that the order of the limit and the integral can be changed. I know about the dominated convergence theorem and the monotone convergence theorem, but the additional problem here is that the integration limit itself depends on n.","How can I prove that $$\displaystyle \Gamma(z)=\lim_{n \to \infty} \displaystyle \int_0^n \left( 1-\frac{t}{n}\right)^n  t^{z-1}\ \text{d} t\;=\displaystyle \int_0^{\infty} e^{-t}  t^{z-1}\ \text{d} t\;$$ Issue is how can I prove that the order of the limit and the integral can be changed. I know about the dominated convergence theorem and the monotone convergence theorem, but the additional problem here is that the integration limit itself depends on n.",,"['analysis', 'special-functions', 'gamma-function']"
30,Characterization of normed vector spaces of finite dimension,Characterization of normed vector spaces of finite dimension,,"I have this problem: Let $E$ be a normed vector space. $S=\{x\in E : ||x||=1\}$. Show that if $S$ is compact then $\dim E$ is finite. This follows directly from the Riesz's lemma, but in the notes of the course, the hint for this exercise is: ""The set $\{x\in E: a\leq ||x||\leq b\}$ is homeomorphic to the set$[a,b]\times S$, for $b>a>0$."" How can I use this to solve the problem? Here a homeomorphism is a function $f$ between metric spaces $E$ and $E'$, bijective, and such that $f$ and $f^{-1}$ are continuous. And then, $E$ is homeomorphic to $E'$ if there exist a homeomorphism $f:E\to E'$.","I have this problem: Let $E$ be a normed vector space. $S=\{x\in E : ||x||=1\}$. Show that if $S$ is compact then $\dim E$ is finite. This follows directly from the Riesz's lemma, but in the notes of the course, the hint for this exercise is: ""The set $\{x\in E: a\leq ||x||\leq b\}$ is homeomorphic to the set$[a,b]\times S$, for $b>a>0$."" How can I use this to solve the problem? Here a homeomorphism is a function $f$ between metric spaces $E$ and $E'$, bijective, and such that $f$ and $f^{-1}$ are continuous. And then, $E$ is homeomorphic to $E'$ if there exist a homeomorphism $f:E\to E'$.",,"['analysis', 'metric-spaces', 'normed-spaces']"
31,How to prove the implicit function theorem fails,How to prove the implicit function theorem fails,,"Define $$F(x,y,u,v)= 3x^2-y^2+u^2+4uv+v^2$$ $$G(x,y,u,v)=x^2-y^2+2uv$$ Show that there is no open set in the $(u,v)$ plane such that $(F,G)=(0,0)$ defines $x$ and $y$ in terms of $u$ and $v$. If (F,G) is equal to say (9,-3) you can just apply the Implicit function theorem and show that in a neighborhood of (1,1) $x$ and $y$ are defined in terms of $u$ and $v$. But this question seems to imply that some part of the assumptions must be necessary for such functions to exist? I believe that since the partials exist and are continuous the determinant of $$\pmatrix{ \frac{\partial F}{\partial x}&\frac{\partial F}{\partial y}\cr \frac{\partial G}{\partial x}&\frac{\partial G}{\partial y} }$$ must be non-zero in order for x and y to be implicitly defined on an open set near any point (u,v) but since the above conditions require x=y=0 the determinant of the above matrix is =0. I have not found this in an analysis text but this paper http://www.u.arizona.edu/~nlazzati/Courses/Math519/Notes/Note%203.pdf claims it is necessary.","Define $$F(x,y,u,v)= 3x^2-y^2+u^2+4uv+v^2$$ $$G(x,y,u,v)=x^2-y^2+2uv$$ Show that there is no open set in the $(u,v)$ plane such that $(F,G)=(0,0)$ defines $x$ and $y$ in terms of $u$ and $v$. If (F,G) is equal to say (9,-3) you can just apply the Implicit function theorem and show that in a neighborhood of (1,1) $x$ and $y$ are defined in terms of $u$ and $v$. But this question seems to imply that some part of the assumptions must be necessary for such functions to exist? I believe that since the partials exist and are continuous the determinant of $$\pmatrix{ \frac{\partial F}{\partial x}&\frac{\partial F}{\partial y}\cr \frac{\partial G}{\partial x}&\frac{\partial G}{\partial y} }$$ must be non-zero in order for x and y to be implicitly defined on an open set near any point (u,v) but since the above conditions require x=y=0 the determinant of the above matrix is =0. I have not found this in an analysis text but this paper http://www.u.arizona.edu/~nlazzati/Courses/Math519/Notes/Note%203.pdf claims it is necessary.",,['analysis']
32,n-th derivative condition to become polynomial for real function,n-th derivative condition to become polynomial for real function,,"I've seen a similar result in complex analysis, when an entire complex function satisfies $f(z)f^{(n)}(z)=0$ for all $z \in \mathbb{C}$ implies that $f(z)$ is polynomial. What if when $f: \mathbb{R} \rightarrow \mathbb{R}$ is a $n$ times differentiable function such that $f(x)f^{(n)}(x)=0$ for all $x\in\mathbb{R}$, does it follow that $f$ is polynomial? thanks. what i have tried so far:  I tried induction: for $n=1$ we have $f(x)f'(x)=0$ which means $f(x)^2=c$ hence $f'(x)=0$ for all $x$. Supposing the statement is true for $n−1$, I want to prove that $f^{(n)}(x)f(x)=0$ will implies $f^{(n)}(x)f'(x)=0$ which implies $f^{(n)}(x)=0$ (by induction hypothesis) .. my idea is to define function $g$ such that i could get $f^{(n)}(x)f'(x)=0$.. i'm stuck here.. I have another try, by analysing $\bigcup A_j$, where $A_j$ is the open interval on which $f(x)$ is not zero (it can be proved it is indeed interval).  I also have tried taylor theorem.","I've seen a similar result in complex analysis, when an entire complex function satisfies $f(z)f^{(n)}(z)=0$ for all $z \in \mathbb{C}$ implies that $f(z)$ is polynomial. What if when $f: \mathbb{R} \rightarrow \mathbb{R}$ is a $n$ times differentiable function such that $f(x)f^{(n)}(x)=0$ for all $x\in\mathbb{R}$, does it follow that $f$ is polynomial? thanks. what i have tried so far:  I tried induction: for $n=1$ we have $f(x)f'(x)=0$ which means $f(x)^2=c$ hence $f'(x)=0$ for all $x$. Supposing the statement is true for $n−1$, I want to prove that $f^{(n)}(x)f(x)=0$ will implies $f^{(n)}(x)f'(x)=0$ which implies $f^{(n)}(x)=0$ (by induction hypothesis) .. my idea is to define function $g$ such that i could get $f^{(n)}(x)f'(x)=0$.. i'm stuck here.. I have another try, by analysing $\bigcup A_j$, where $A_j$ is the open interval on which $f(x)$ is not zero (it can be proved it is indeed interval).  I also have tried taylor theorem.",,['analysis']
33,Constructing a continuous function whose graph seems 'special' [duplicate],Constructing a continuous function whose graph seems 'special' [duplicate],,"This question already has answers here : Universal Chord Theorem (4 answers) Closed 5 years ago . I've been reading through Zorich's ""Analysis I"" book recently, and I came across this nice little exercise. Let $f: [0,1]\to \mathbb R$ be a continuous function such that $f(0)=f(1)$. Show that for any $n\in \mathbb N$ there exists a horizontal closed interval of length $\frac 1n$ with endpoints on the graph of this function; if the number $\ell$ is not of the form $\frac 1n$ there exists a function of this form on whose graph one cannot inscribe a horizontal chord of length $\ell$. The first part can be proven like this: Consider $g: [0,(n-1)/n] \to \mathbb R$ given by $g(x) = f(x) - f(x+1/n)$. Then $$\sum_{k=0}^{n-1} g(k/n) = 0$$ and therefore either all of these points are zero or there exists both a point where $g$ is positive and a point where $g$ is negative. By continuity, there must then also be a point where $g = 0$. So we are done. Now, the second statement seemed rather counterintuitive, and I have given it some time now, but don't see a counterexample for $\ell < 1/2$. (For $\ell > 1/2$ the function $f(x) = \sin(2\pi x)$ will do.) Can anyone help me out? Cheers,","This question already has answers here : Universal Chord Theorem (4 answers) Closed 5 years ago . I've been reading through Zorich's ""Analysis I"" book recently, and I came across this nice little exercise. Let $f: [0,1]\to \mathbb R$ be a continuous function such that $f(0)=f(1)$. Show that for any $n\in \mathbb N$ there exists a horizontal closed interval of length $\frac 1n$ with endpoints on the graph of this function; if the number $\ell$ is not of the form $\frac 1n$ there exists a function of this form on whose graph one cannot inscribe a horizontal chord of length $\ell$. The first part can be proven like this: Consider $g: [0,(n-1)/n] \to \mathbb R$ given by $g(x) = f(x) - f(x+1/n)$. Then $$\sum_{k=0}^{n-1} g(k/n) = 0$$ and therefore either all of these points are zero or there exists both a point where $g$ is positive and a point where $g$ is negative. By continuity, there must then also be a point where $g = 0$. So we are done. Now, the second statement seemed rather counterintuitive, and I have given it some time now, but don't see a counterexample for $\ell < 1/2$. (For $\ell > 1/2$ the function $f(x) = \sin(2\pi x)$ will do.) Can anyone help me out? Cheers,",,['analysis']
34,A limit related to the Gibbs phenomenon,A limit related to the Gibbs phenomenon,,"Let $$D_N(x)=\frac{\sin [(N+(1/2))t]}{\sin (t/2)}$$ be the Dirichlet kernel. Let $x(N)$ be the number in $0<x<\pi/N$ such that $D_N(x)=1$. Is $$\left|\int_{x(N)}^{\pi/N} D_N(t)\mathrm dt \right|=O\left(\frac1{N}\right)$$ true? This question arises from my attempt to give a rigorous proof of Gibbs phenomenon. In fact, I only need that the limit is 0 as $N\to\infty$.","Let $$D_N(x)=\frac{\sin [(N+(1/2))t]}{\sin (t/2)}$$ be the Dirichlet kernel. Let $x(N)$ be the number in $0<x<\pi/N$ such that $D_N(x)=1$. Is $$\left|\int_{x(N)}^{\pi/N} D_N(t)\mathrm dt \right|=O\left(\frac1{N}\right)$$ true? This question arises from my attempt to give a rigorous proof of Gibbs phenomenon. In fact, I only need that the limit is 0 as $N\to\infty$.",,"['analysis', 'fourier-analysis', 'fourier-series']"
35,"Let $f$ be a continuous but nowhere differentiable function. Is $f$ convolved with mollifier, a smooth function?","Let  be a continuous but nowhere differentiable function. Is  convolved with mollifier, a smooth function?",f f,"Let $f$ be a continuous but nowhere differentiable function. Is $f$ convolved with mollifier, a smooth function?","Let $f$ be a continuous but nowhere differentiable function. Is $f$ convolved with mollifier, a smooth function?",,"['analysis', 'convolution']"
36,Understanding the proof of the Brezis-Lieb lemma,Understanding the proof of the Brezis-Lieb lemma,,"I am trying to understand the proof of the Brezis-Lieb lemma from the Wikipedia page: https://en.wikipedia.org/wiki/Brezis%E2%80%93Lieb_lemma Here is the statement of said lemma: Let $(X, \mu)$ be a measure space and let $(f_n)_n$ be a sequence of measurable complex-valued functions on $X$ which converge almost everywhere to a function $f$ . The limiting function $f$ is automatically measurable. The Brezis–Lieb lemma asserts that if $p$ is a positive number, then $$\lim _{n\to \infty }\int _{X}{\Big |}|f|^{p}-|f_{n}|^{p}+|f-f_{n}|^{p}{\Big |}\,d\mu =0$$ provided that the sequence $(f_n)_n$ is uniformly bounded in $L^p(X, \mu)$ . So I am uncertain about this last line of this proof. Specifically, why is the supremum finite? This is essentially saying that for all $n$ , the integral is finite correct? But that would imply that $f-f_n$ is in $L^p$ which would imply $f$ is in $L^p$ which is not true in general since all we know is that $f_n$ converges pointwise to $f$ .","I am trying to understand the proof of the Brezis-Lieb lemma from the Wikipedia page: https://en.wikipedia.org/wiki/Brezis%E2%80%93Lieb_lemma Here is the statement of said lemma: Let be a measure space and let be a sequence of measurable complex-valued functions on which converge almost everywhere to a function . The limiting function is automatically measurable. The Brezis–Lieb lemma asserts that if is a positive number, then provided that the sequence is uniformly bounded in . So I am uncertain about this last line of this proof. Specifically, why is the supremum finite? This is essentially saying that for all , the integral is finite correct? But that would imply that is in which would imply is in which is not true in general since all we know is that converges pointwise to .","(X, \mu) (f_n)_n X f f p \lim _{n\to \infty }\int _{X}{\Big |}|f|^{p}-|f_{n}|^{p}+|f-f_{n}|^{p}{\Big |}\,d\mu =0 (f_n)_n L^p(X, \mu) n f-f_n L^p f L^p f_n f","['real-analysis', 'analysis', 'measure-theory', 'proof-explanation']"
37,"$n$th derivative is non-negative for $n=0,1,2,...$, then Taylor series converge, and is exactly $f(x)$ everywhere","th derivative is non-negative for , then Taylor series converge, and is exactly  everywhere","n n=0,1,2,... f(x)","Try to prove, if $$f^{(n)}(x)\geq 0,\;\forall x\in \mathbb{R},\;\forall n=0,1,2,...$$ then the Taylor series at the point $0$ converges everywhere, and it's exactly $f(x)$ , i.e. $$f(x)=\sum_{n=0}^{+\infty}\frac{f^{(n)}(0)}{n!}x^n,\;\forall x\in \mathbb{R}.$$ I know it is sufficient, and necessary, to prove, $$ \lim\limits_{n\to+\infty}{R_n(f,x)}=0\iff\lim\limits_{n\to+\infty}{\frac{f^{(n+1)}(\xi)}{(n+1)!}x^n}=0$$ but I dont know any methods to estimate the value of the $n$ th derivative. Can you help me? Thanks! Update: Notice: The Taylor series converges everywhere (proved in another similar problem: Taylor series of Infinitely differentiable function with nonnegative derivatives ), but it's interesting that it is equal to $f(x)$ . It's possible to prove the formula where $x<0$ : consider $$\lim\limits_{n\to+\infty}{\left|\frac{f^{(n+1)}(\xi)}{(n+1)!}x^{n+1}\right|}\leq\lim\limits_{n\to+\infty}{\left|\frac{f^{(n+1)}(0)}{(n+1)!}\right||x|^{n+1}}=0.$$ ...And I have an idea ! For $x_0>0:$ consider $$g(x)=\sum_{k=0}^{n}{\frac{f^{(k)}(x_0)(x-x_0)^k}{k!}-f(x)}$$ and $g(2x_0)$ .","Try to prove, if then the Taylor series at the point converges everywhere, and it's exactly , i.e. I know it is sufficient, and necessary, to prove, but I dont know any methods to estimate the value of the th derivative. Can you help me? Thanks! Update: Notice: The Taylor series converges everywhere (proved in another similar problem: Taylor series of Infinitely differentiable function with nonnegative derivatives ), but it's interesting that it is equal to . It's possible to prove the formula where : consider ...And I have an idea ! For consider and .","f^{(n)}(x)\geq 0,\;\forall x\in \mathbb{R},\;\forall n=0,1,2,... 0 f(x) f(x)=\sum_{n=0}^{+\infty}\frac{f^{(n)}(0)}{n!}x^n,\;\forall x\in \mathbb{R}. 
\lim\limits_{n\to+\infty}{R_n(f,x)}=0\iff\lim\limits_{n\to+\infty}{\frac{f^{(n+1)}(\xi)}{(n+1)!}x^n}=0 n f(x) x<0 \lim\limits_{n\to+\infty}{\left|\frac{f^{(n+1)}(\xi)}{(n+1)!}x^{n+1}\right|}\leq\lim\limits_{n\to+\infty}{\left|\frac{f^{(n+1)}(0)}{(n+1)!}\right||x|^{n+1}}=0. x_0>0: g(x)=\sum_{k=0}^{n}{\frac{f^{(k)}(x_0)(x-x_0)^k}{k!}-f(x)} g(2x_0)","['real-analysis', 'calculus', 'analysis']"
38,Find example: $\lim_{h\to0}\frac{f(x_0+\alpha h)-f(x_0-\beta h)}{(\alpha+\beta)h}\ \ \text{exists }\ \ \not\!\!\!\implies\ f'(x_0)\ \ \text{exists}$,Find example:,\lim_{h\to0}\frac{f(x_0+\alpha h)-f(x_0-\beta h)}{(\alpha+\beta)h}\ \ \text{exists }\ \ \not\!\!\!\implies\ f'(x_0)\ \ \text{exists},"Assume that $f:[a,b]\to\mathbb R$ is a continuous function. For $x_0\in (a,b)$ and $\alpha,\beta>0$ , we define the (asymetric) difference quotient $$(\Delta_{\alpha,\beta,h}f)(x_0):=\frac{f(x_0+\alpha h)-f(x_0-\beta h)}{(\alpha+\beta)h},\qquad h\neq 0.$$ Given $\alpha,\beta>0$ with $\alpha\neq \beta$ , I want to find an example of $f$ such that $$\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \not\!\!\!\implies\ f'(x_0)\ \ \ \text{exists}.$$ This is a problem that I came up with while I'm preparing my recitation class as a teaching assistant in Analysis (I) course. If $f'(x_0)$ exists, we can prove that $\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)$ by Taylor's theorem: It follows from $f(x_0+\alpha h)=f(x_0)+\alpha f'(x_0)h+o(h)$ and $f(x_0-\beta h)=f(x_0)-\beta f'(x_0)h+o(h)$ that $(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)+o(1)$ as $h\to0$ , hence $\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)$ . Now, a natural question appears: What about the reverse problem? If $\alpha=\beta$ , then for $f(x)=|x|$ and $x_0=0$ we have $(\Delta_{\alpha,\beta,h}f)(0)=0$ for all $h\neq0$ , but $f'(0)$ doesn't exist. However, for $\alpha\neq \beta$ , I failed to find an example. I found that if $\alpha\neq \beta$ , and if $f_-'(x_0)$ , $f_+'(x_0)$ both exist, then we must have $$\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \implies\ f'(x_0)\ \ \ \text{exists}.$$ So, for any example in which $\alpha\neq \beta$ and $\lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \not\!\!\!\implies\ f'(x_0)\ \ \ \text{exists}$ , there must be one of $f_-'(x_0)$ and $f_+'(x_0)$ that doesn't exist. (Note that $f$ should be continuous in this post.) Any help would be appreciated!","Assume that is a continuous function. For and , we define the (asymetric) difference quotient Given with , I want to find an example of such that This is a problem that I came up with while I'm preparing my recitation class as a teaching assistant in Analysis (I) course. If exists, we can prove that by Taylor's theorem: It follows from and that as , hence . Now, a natural question appears: What about the reverse problem? If , then for and we have for all , but doesn't exist. However, for , I failed to find an example. I found that if , and if , both exist, then we must have So, for any example in which and , there must be one of and that doesn't exist. (Note that should be continuous in this post.) Any help would be appreciated!","f:[a,b]\to\mathbb R x_0\in (a,b) \alpha,\beta>0 (\Delta_{\alpha,\beta,h}f)(x_0):=\frac{f(x_0+\alpha h)-f(x_0-\beta h)}{(\alpha+\beta)h},\qquad h\neq 0. \alpha,\beta>0 \alpha\neq \beta f \lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \not\!\!\!\implies\ f'(x_0)\ \ \ \text{exists}. f'(x_0) \lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0) f(x_0+\alpha h)=f(x_0)+\alpha f'(x_0)h+o(h) f(x_0-\beta h)=f(x_0)-\beta f'(x_0)h+o(h) (\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0)+o(1) h\to0 \lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)=f'(x_0) \alpha=\beta f(x)=|x| x_0=0 (\Delta_{\alpha,\beta,h}f)(0)=0 h\neq0 f'(0) \alpha\neq \beta \alpha\neq \beta f_-'(x_0) f_+'(x_0) \lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \implies\ f'(x_0)\ \ \ \text{exists}. \alpha\neq \beta \lim_{h\to0}(\Delta_{\alpha,\beta,h}f)(x_0)\ \ \text{exists }\ \  \not\!\!\!\implies\ f'(x_0)\ \ \ \text{exists} f_-'(x_0) f_+'(x_0) f","['real-analysis', 'calculus', 'analysis', 'derivatives']"
39,Understanding Young's Convolution Inequality and its relation to Convex Bodies,Understanding Young's Convolution Inequality and its relation to Convex Bodies,,"On Pg. 34 of this reference , I encountered Young's Convolution Inequality . The author states the inequality and manipulates it into various forms. I write this post to better understand the manipulations of Young's Convolution Inequality into different forms, and what the author is trying to achieve by doing so. I shall reproduce the text from the reference below (as quotes), and ask my questions inline. It would be greatly appreciated even if you can help answer some of the following questions, if not all. Thank you! If $f$ and $g:\mathbb R\to \mathbb R$ are bounded, integrable functions, the convolution $f * g$ of $f$ and $g$ is defined by $$f*g(x) = \int_\mathbb R f(y) g(x-y) dy$$ Young's Convolution Inequality: If $f \in L_p$ , $g\in L_q$ , and $$\frac 1p + \frac1q  = 1 + \frac1s$$ then $$\|f * g\|_s\le \|f\|_p \|g\|_q$$ Once we have Young’s inequality, we can give a meaning to convolutions of functions that are not both integrable and bounded, provided that they lie in the correct $L_p$ spaces. Q1. What is meant by ""correct"" $L^p$ spaces here? Young’s inequality holds for convolution on any locally compact group, for example the circle. On compact groups it is sharp: there is equality for constant functions. But on $\mathbb R$ , where constant functions are not integrable, the inequality can be improved (for most values of $p$ and $q$ ). Q2. What is meant by compact groups and locally compact groups ? Wikipedia says:  (i) a compact (topological) group is a topological group whose topology is compact and (ii) a locally compact group is a topological group G for which the underlying topology is locally compact and Hausdorff. However, I am able to see which groups we are talking about in this context. It was shown by Beckner [1975] and Brascamp and Lieb [1976a] that the correct constant in Young’s inequality is attained if $f$ and $g$ are appropriate Gaussian densities: that is, for some positive $a$ and $b$ , $f(t) = e^{−at^2}$ and $g(t) = e^{−bt^2}$ . (The appropriate choices of $a$ and $b$ and the value of the best constant for each $p$ and $q$ will not be stated here. Later we shall see that they can be avoided.) Q3. What is meant by the correct constant in Young's inequality? First of all, where is the constant? Is it present in some other form of the inequality, that has not been stated here? Pretty confused about this. How are convolutions related to convex bodies? To answer this question we need to rewrite Young’s inequality slightly. If $1/r+1/s = 1$ , the $L_s$ norm $\|f ∗g\|_s$ can be realized as $$\int_\mathbb R (f*g)(x)h(x)$$ for some function with $\|h\|_r = 1$ . Q4. How was the inequality rewritten into the above format? It'd be great if someone could provide details because I'm unable to see how and what just happened. So, the inequality says that if $1/p + 1/q + 1/r = 2$ , then $$\int \int f(y) g(x-y) h(x) dy dx\le \|f\|_p \|g\|_q \|h\|_r$$ We may rewrite the inequality again with $h(-x)$ in place of $h(x)$ , since this doesn't affect $\|h\|_r$ . $$\int \int f(y) g(x-y) h(-x) dy dx\le \|f\|_p \|g\|_q \|h\|_r$$ Q5. What is the point of replacing $x$ by $-x$ ? What are we gaining? This can be written in a yet more symmetric form, with the help of the map $\phi: \mathbb R^2 \to \mathbb R^3$ such that $\phi: (x,y) \to (y,x-y,-x) := (u,v,w)$ . The range of $\phi$ is the subspace $H$ of $\mathbb R^3$ , given by $$H = \{(u,v,w): u+v+w = 0\}$$ Besides the factor coming from the Jacobian, the integral can be written as $$\int_H f(u) g(v) h(w)$$ where the integral is with respect to the two-dimensional measure on the subspace $H$ . Q6. What are we gaining by rewriting the integral in the form (except that it looks nicer)? Also, does the Jacobian contribute to the constant factor that is being talked about in several places? Q7. Which measure on $H$ are we talking about, explicitly? So Young’s inequality and its sharp forms estimate the integral of a product function on $\mathbb R^3$ over a subspace. What is the simplest product function? If $f, g$ , and $h$ are each the characteristic function of the interval $[−1, 1]$ , the function $F$ given by $$F(u,v,w) = f(u)g(v)h(w)$$ is the characteristic function of the cube $[−1, 1]^3 ⊂ \mathbb R^3$ . The integral of $F$ over a subspace of $\mathbb R^3$ is thus the area of a slice of the cube: the area of a certain convex body. So there is some hope that we might use a convolution inequality to estimate volumes. Q8. What is the sharp form of Young's inequality that the author talks about here? How is it sharp ?","On Pg. 34 of this reference , I encountered Young's Convolution Inequality . The author states the inequality and manipulates it into various forms. I write this post to better understand the manipulations of Young's Convolution Inequality into different forms, and what the author is trying to achieve by doing so. I shall reproduce the text from the reference below (as quotes), and ask my questions inline. It would be greatly appreciated even if you can help answer some of the following questions, if not all. Thank you! If and are bounded, integrable functions, the convolution of and is defined by Young's Convolution Inequality: If , , and then Once we have Young’s inequality, we can give a meaning to convolutions of functions that are not both integrable and bounded, provided that they lie in the correct spaces. Q1. What is meant by ""correct"" spaces here? Young’s inequality holds for convolution on any locally compact group, for example the circle. On compact groups it is sharp: there is equality for constant functions. But on , where constant functions are not integrable, the inequality can be improved (for most values of and ). Q2. What is meant by compact groups and locally compact groups ? Wikipedia says:  (i) a compact (topological) group is a topological group whose topology is compact and (ii) a locally compact group is a topological group G for which the underlying topology is locally compact and Hausdorff. However, I am able to see which groups we are talking about in this context. It was shown by Beckner [1975] and Brascamp and Lieb [1976a] that the correct constant in Young’s inequality is attained if and are appropriate Gaussian densities: that is, for some positive and , and . (The appropriate choices of and and the value of the best constant for each and will not be stated here. Later we shall see that they can be avoided.) Q3. What is meant by the correct constant in Young's inequality? First of all, where is the constant? Is it present in some other form of the inequality, that has not been stated here? Pretty confused about this. How are convolutions related to convex bodies? To answer this question we need to rewrite Young’s inequality slightly. If , the norm can be realized as for some function with . Q4. How was the inequality rewritten into the above format? It'd be great if someone could provide details because I'm unable to see how and what just happened. So, the inequality says that if , then We may rewrite the inequality again with in place of , since this doesn't affect . Q5. What is the point of replacing by ? What are we gaining? This can be written in a yet more symmetric form, with the help of the map such that . The range of is the subspace of , given by Besides the factor coming from the Jacobian, the integral can be written as where the integral is with respect to the two-dimensional measure on the subspace . Q6. What are we gaining by rewriting the integral in the form (except that it looks nicer)? Also, does the Jacobian contribute to the constant factor that is being talked about in several places? Q7. Which measure on are we talking about, explicitly? So Young’s inequality and its sharp forms estimate the integral of a product function on over a subspace. What is the simplest product function? If , and are each the characteristic function of the interval , the function given by is the characteristic function of the cube . The integral of over a subspace of is thus the area of a slice of the cube: the area of a certain convex body. So there is some hope that we might use a convolution inequality to estimate volumes. Q8. What is the sharp form of Young's inequality that the author talks about here? How is it sharp ?","f g:\mathbb R\to \mathbb R f * g f g f*g(x) = \int_\mathbb R f(y) g(x-y) dy f \in L_p g\in L_q \frac 1p + \frac1q  = 1 + \frac1s \|f * g\|_s\le \|f\|_p \|g\|_q L_p L^p \mathbb R p q f g a b f(t) = e^{−at^2} g(t) = e^{−bt^2} a b p q 1/r+1/s = 1 L_s \|f ∗g\|_s \int_\mathbb R (f*g)(x)h(x) \|h\|_r = 1 1/p + 1/q + 1/r = 2 \int \int f(y) g(x-y) h(x) dy dx\le \|f\|_p \|g\|_q \|h\|_r h(-x) h(x) \|h\|_r \int \int f(y) g(x-y) h(-x) dy dx\le \|f\|_p \|g\|_q \|h\|_r x -x \phi: \mathbb R^2 \to \mathbb R^3 \phi: (x,y) \to (y,x-y,-x) := (u,v,w) \phi H \mathbb R^3 H = \{(u,v,w): u+v+w = 0\} \int_H f(u) g(v) h(w) H H \mathbb R^3 f, g h [−1, 1] F F(u,v,w) = f(u)g(v)h(w) [−1, 1]^3 ⊂ \mathbb R^3 F \mathbb R^3","['analysis', 'measure-theory', 'convex-analysis', 'harmonic-analysis', 'convex-geometry']"
40,"Prob. 18, Chap. 2, in Royden's REAL ANALYSIS: If $E$ has finite outer measure, then there is an $F_\sigma$-set $F$ and a $G_\delta$-set $G$ with ...","Prob. 18, Chap. 2, in Royden's REAL ANALYSIS: If  has finite outer measure, then there is an -set  and a -set  with ...",E F_\sigma F G_\delta G,"Here is Prob. 18, Chap. 2, in the book Real Analysis by H. L. Royden and P. M. Fitzpatrick, 4th edition: Let $E$ have finite outer measure. Show that there is an $F_\sigma$ set $F$ and a $G_\delta$ set $G$ such that $F \subseteq E \subseteq G$ and $m^*(F) = m^*(E) = m^*(G)$ . My Attempt: Case 1.  If set $E$ is a (Lebesgue) measurable set of real numbers, then by Theorem 11 (iv), Chap. 2, in Royden, there is an $F_\sigma$ set $F$ with $F \subseteq E$ and $m^*(E \setminus F) = 0$ , and by Theorem 11 (ii), there is a $G_\delta$ set $G$ such that $E \subseteq G$ and $m^*(G \setminus E) = 0$ . Note that the sets $F$ and $G$ both are measurable. Now since $E$ is measurable with finite outer measure and since $F \subseteq E$ , therefore $F$ also has finite outer measure. Thus we have $F \subseteq E \subseteq G$ and $m^*(E \setminus F) = 0$ and $m^*(G \setminus E) = 0$ , which by the excision property of measurable sets (i.e. sets $F$ and $E$ ) with finite outer measure yield $m^*(E) - m^*(F) = 0$ and $m^*(G) - m^*(E) = 0$ , and thus $$ m^*(F) = m^*(E) = m^*(G), $$ as required. Is what I have done correct and accurate in each and every detail? If so, then how to tackle the case when $E$ has finite outer measure but is not measurable? PS: Case 2. Now suppose that set $E$ has finite outer measure but is not measurable. Then by Theorem 11 (ii) and (iv) in Royden, for any $F_\sigma$ set $F$ and any $G_\delta$ set $G$ with $F \subseteq E \subseteq G$ , we must have $m^*(E \setminus F) \neq 0$ and $m^*(G \setminus E) \neq 0$ , that is, $m^*(E \setminus F) >  0$ and $m^*(G \setminus E) > 0$ , which imply $E \setminus F \neq \emptyset$ and $G \setminus E \neq \emptyset$ ; in fact the sets $E \setminus F$ and $G \setminus E$ cannot even be countable. Now as $E = F \cup (E \setminus F)$ , so we have $$ m^*(E) = m^* \big( F \cup (E \setminus F) \big) \leq m^*(F) + m^*( E \setminus F), $$ which implies $$ m^*(E) - m^*(F) \leq m^*( E \setminus F), $$ but since $F \subseteq E$ , by the monotonicity of the outer measure we have $$ 0 \leq m^*(E) - m^*(F) \leq m^*( E \setminus F). \tag{1}  $$ And, as $G = E \cup (G \setminus E)$ , so we have $$ m^*(G) = m^* \big( E \cup (G \setminus E) \big) \leq m^*(E) + m^*( G \setminus E ),  $$ which implies $$ m^*(G) - m^*(E) \leq m^*(G \setminus E), $$ but since $E \subset G$ , by the monotonicity of the outer measure we have $$ 0 \leq m^*(G) - m^*(E) \leq m^*(G \setminus E). \tag{2}  $$ The relations (1) and (2) hold for any $F_\sigma$ set $F$ and for any $G_\delta$ set $G$ such that $F \subseteq E \subseteq G$ . What next? Where can we get from here? PS (Based on the comments by Tab1e): As set $E$ has finite outer measure, so by the definition of the outer measure, for each positive integer $n$ , we can find a countable collection $\left\{ I_{n, k} \right\}_{k=1}^\infty$ of non-empty bounded open intervals covering set $E$ for which $$ \sum_{k=1}^\infty l \left( I_{n, k} \right) < m^*(E) + \frac{1}{n},  $$ Let us put $$  G_n := \bigcup_{k=1}^\infty I_{n, k}. $$ Then by our choice of the $I_{n, k}$ , we have $E \subset G_n$ , and also $$ m^*\left( G_n \right) = m^* \left( \bigcup_{k=1}^\infty I_{n, k} \right) \leq \sum_{k=1}^\infty m^* \left( I_{n, k} \right) = \sum_{k=1}^\infty l \left( I_{n, k} \right) < m^*(E) + \frac{1}{n}, $$ and hence we also have $$ m^* \left( G_n \right) < m^*(E) + \frac{1}{n}. $$ Let us now put $$ G := \bigcap_{n=1}^\infty G_n. $$ This set $G$ is of course a $G_\delta$ set, and simce for each positive integer $n$ we have $G \subset G_n$ and $E \subset G_n$ , we can conclude that $E \subset G$ also and therefore $$ m^*(E) \leq m^* (G) \leq m^* \left( G_n \right) < m^*(E) + \frac{1}{n}, $$ which in turn implies that $$ m^*(E) \leq m^*(G) < m^*(E) + \frac1n  $$ for every positive integer $n$ , which upon taking the limit as $n \to \infty$ yields $$ m^*(E) = m^*(G).  $$ Is this part of my post correct? If so, then how to give the proof for the $F_\sigma$ set?","Here is Prob. 18, Chap. 2, in the book Real Analysis by H. L. Royden and P. M. Fitzpatrick, 4th edition: Let have finite outer measure. Show that there is an set and a set such that and . My Attempt: Case 1.  If set is a (Lebesgue) measurable set of real numbers, then by Theorem 11 (iv), Chap. 2, in Royden, there is an set with and , and by Theorem 11 (ii), there is a set such that and . Note that the sets and both are measurable. Now since is measurable with finite outer measure and since , therefore also has finite outer measure. Thus we have and and , which by the excision property of measurable sets (i.e. sets and ) with finite outer measure yield and , and thus as required. Is what I have done correct and accurate in each and every detail? If so, then how to tackle the case when has finite outer measure but is not measurable? PS: Case 2. Now suppose that set has finite outer measure but is not measurable. Then by Theorem 11 (ii) and (iv) in Royden, for any set and any set with , we must have and , that is, and , which imply and ; in fact the sets and cannot even be countable. Now as , so we have which implies but since , by the monotonicity of the outer measure we have And, as , so we have which implies but since , by the monotonicity of the outer measure we have The relations (1) and (2) hold for any set and for any set such that . What next? Where can we get from here? PS (Based on the comments by Tab1e): As set has finite outer measure, so by the definition of the outer measure, for each positive integer , we can find a countable collection of non-empty bounded open intervals covering set for which Let us put Then by our choice of the , we have , and also and hence we also have Let us now put This set is of course a set, and simce for each positive integer we have and , we can conclude that also and therefore which in turn implies that for every positive integer , which upon taking the limit as yields Is this part of my post correct? If so, then how to give the proof for the set?","E F_\sigma F G_\delta G F \subseteq E \subseteq G m^*(F) = m^*(E) = m^*(G) E F_\sigma F F \subseteq E m^*(E \setminus F) = 0 G_\delta G E \subseteq G m^*(G \setminus E) = 0 F G E F \subseteq E F F \subseteq E \subseteq G m^*(E \setminus F) = 0 m^*(G \setminus E) = 0 F E m^*(E) - m^*(F) = 0 m^*(G) - m^*(E) = 0 
m^*(F) = m^*(E) = m^*(G),
 E E F_\sigma F G_\delta G F \subseteq E \subseteq G m^*(E \setminus F) \neq 0 m^*(G \setminus E) \neq 0 m^*(E \setminus F) >  0 m^*(G \setminus E) > 0 E \setminus F \neq \emptyset G \setminus E \neq \emptyset E \setminus F G \setminus E E = F \cup (E \setminus F) 
m^*(E) = m^* \big( F \cup (E \setminus F) \big) \leq m^*(F) + m^*( E \setminus F),
 
m^*(E) - m^*(F) \leq m^*( E \setminus F),
 F \subseteq E 
0 \leq m^*(E) - m^*(F) \leq m^*( E \setminus F). \tag{1} 
 G = E \cup (G \setminus E) 
m^*(G) = m^* \big( E \cup (G \setminus E) \big) \leq m^*(E) + m^*( G \setminus E ), 
 
m^*(G) - m^*(E) \leq m^*(G \setminus E),
 E \subset G 
0 \leq m^*(G) - m^*(E) \leq m^*(G \setminus E). \tag{2} 
 F_\sigma F G_\delta G F \subseteq E \subseteq G E n \left\{ I_{n, k} \right\}_{k=1}^\infty E 
\sum_{k=1}^\infty l \left( I_{n, k} \right) < m^*(E) + \frac{1}{n}, 
 
 G_n := \bigcup_{k=1}^\infty I_{n, k}.
 I_{n, k} E \subset G_n  m^*\left( G_n \right) = m^* \left( \bigcup_{k=1}^\infty I_{n, k} \right) \leq \sum_{k=1}^\infty m^* \left( I_{n, k} \right) = \sum_{k=1}^\infty l \left( I_{n, k} \right) < m^*(E) + \frac{1}{n},  
m^* \left( G_n \right) < m^*(E) + \frac{1}{n}.
 
G := \bigcap_{n=1}^\infty G_n.
 G G_\delta n G \subset G_n E \subset G_n E \subset G 
m^*(E) \leq m^* (G) \leq m^* \left( G_n \right) < m^*(E) + \frac{1}{n},
 
m^*(E) \leq m^*(G) < m^*(E) + \frac1n 
 n n \to \infty 
m^*(E) = m^*(G). 
 F_\sigma","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'outer-measure']"
41,Are continuous convex functions subharmonic?,Are continuous convex functions subharmonic?,,"We say that a continuous function $u:\mathbb{R}^d\to \mathbb{R}$ is subharmonic if it satisfies the mean value property $$u(x)\leq \frac{1}{|\partial  B_r(x)|}\int_{\partial B_r(x)}u(y)\,\mathrm{d}y \qquad (\star)$$ for any ball $B_r(x)\subset \mathbb{R}^d$ . Let $u:\mathbb{R}^d\to \mathbb{R}$ be a convex function (hence, continuous). Is $u$ subharmonic? If $u\in C^2(\mathbb{R}^d)$ , this is  true. Using a second-order Taylor expansion we have \begin{align*}\int_{\partial B_r(x)}(u(y)-u(x))\,\mathrm{d}y&=\int_{\partial B_r(x)}\left(\nabla u(x)\cdot(y-x)+\frac{1}{2}(y-x)D^2u(\xi)(y-x)^t\right)\,\mathrm{d}y.\end{align*} The first term in the above integral vanishes by symmetry, the second is non-negative because $D^2u(\xi)$ is a positive semi-definite matrix. Therefore, ( $\star$ ) is proven. If $d=1$ , the statement is true when $u$ is continuous, in general. Indeed since balls reduce to intervals, ( $\star$ ) is easily shown to be equivalent to $u$ being midpoint-convex. I'm not sure how to attack the problem in higher dimensions. Of course $(\star)$ is true for affine functions in any dimension, and I'd like to use the fact that the graph of a convex function lies below that of an affine function, loosely speaking. However, to close the estimate I would need $u$ to be equal to the affine function at the boundary of the ball, and this is not necessarily possible.","We say that a continuous function is subharmonic if it satisfies the mean value property for any ball . Let be a convex function (hence, continuous). Is subharmonic? If , this is  true. Using a second-order Taylor expansion we have The first term in the above integral vanishes by symmetry, the second is non-negative because is a positive semi-definite matrix. Therefore, ( ) is proven. If , the statement is true when is continuous, in general. Indeed since balls reduce to intervals, ( ) is easily shown to be equivalent to being midpoint-convex. I'm not sure how to attack the problem in higher dimensions. Of course is true for affine functions in any dimension, and I'd like to use the fact that the graph of a convex function lies below that of an affine function, loosely speaking. However, to close the estimate I would need to be equal to the affine function at the boundary of the ball, and this is not necessarily possible.","u:\mathbb{R}^d\to \mathbb{R} u(x)\leq \frac{1}{|\partial
 B_r(x)|}\int_{\partial B_r(x)}u(y)\,\mathrm{d}y \qquad (\star) B_r(x)\subset \mathbb{R}^d u:\mathbb{R}^d\to \mathbb{R} u u\in C^2(\mathbb{R}^d) \begin{align*}\int_{\partial B_r(x)}(u(y)-u(x))\,\mathrm{d}y&=\int_{\partial B_r(x)}\left(\nabla u(x)\cdot(y-x)+\frac{1}{2}(y-x)D^2u(\xi)(y-x)^t\right)\,\mathrm{d}y.\end{align*} D^2u(\xi) \star d=1 u \star u (\star) u","['analysis', 'convex-analysis']"
42,"Prove that $(K,\delta)$ is a compact metric space.",Prove that  is a compact metric space.,"(K,\delta)","Let $(X, d)$ be a compact metric space. For $x ∈ X $ and $\epsilon > 0$ , define { $B_{\epsilon}(x) := {y ∈ X | d(x, y) < \epsilon}$ }. For $C ⊆ X$ and $\epsilon > 0$ , define $B_{\epsilon}(C) := ∪_{x∈C}B_{\epsilon}(x)$ . Let K be the set of non-empty compact subsets of X. For $C, C_0 ∈ K$ , define $δ(C, C_0) = $ inf{ $\epsilon| C ⊆ B_{\epsilon}(C_0)$ and $C_0 ⊆ B_{\epsilon}(C)$ }. Assuming that it forms a metric space show that $(K, δ) $ is a compact metric space. My attempt: The metric space $(X,d)$ is compact so it is totally bounded. Let $B=$ { $C_k$ } be a sequence of compact metric spaces in $K$ . We need to show that the infinite set has a limit point. $(X, d)$ is totally bounded so let $A_1$ ={ $x_{1,1},x_{2, 1},....,x_{k_1,1}$ } such that ${\cup {B_d(x_i,1) }} $ covers X for $i= (1,1),..,(k_1,1)$ and ${\cup {B_d(x_i,\frac{1}{2}) }} $ covers X for $i= (1,2),..,(k_2,2)$ ,so $A_2= x_{1,2},x_{2,2},...,x_{k_2,2}$ So we can generate a new sequence of compact metric space $A_1,A_2,...,A_n$ as mentioned above.Now,let { $\cup {(A_i)}$ }= $A$ .We take the closure $cl(A)$ which is the closed subset of $(X,d)$ .Hence it is compact so it is in $(K,\delta)$ . $(B_{\delta}(cl(A) \cap B)  /cl(A))\ne \phi$ for all $\epsilon>0$ is what we have to show to prove that $cl(A)$ is the limit point of the sequence { ${C_k}$ } to show that the sequence has bolzano weistrass property. We pick an $\epsilon >0$ and proceed as , Now, $C_i \subset B_{\frac{1}{N_1}}(cl(A))$ where $\frac{1}{N_1} < \frac{\epsilon}{2}$ and $C_i \subset B_{\epsilon}(cl(A))$ where $C_i$ is any compact subset in the sequence { $C_k$ } and $C_i$ can be chosen to be different from $cl(A)$ . Now, let $x \in Cl(A)$ then $x \in X$ and $x \in \cup B_d(x_i,\frac{1}{N_1})$ where $i=(1,N_1),(2,N_1),(3,N_1),...,(k_{N_1},N_1)$ then $d(x,x_i) < \frac{1}{N_1}$ and let $x_n \in C_i$ then $d(x_i,x_n) < \frac{1}{N_1}$ so $d(x,x_n) < \epsilon$ and the other case should also follow when $ x = x_i $ where $i=(1,N_1),(2,N_1),(3,N_1),...,(k_{N_1},N_1)$ . So $cl(A) \subset B_{\epsilon}(C_i)$ . Then $\delta(C_i,cl(A)) < \epsilon$ which allows me to conclude that $cl(A)$ is the limit point of the above mentioned seqeunce (as $\epsilon$ can be chosen arbitrarily).So by bolzano property I can claim the compactness. Is my attempt ok? I have tried hard so that the notations are understandable. Thanks in advance.","Let be a compact metric space. For and , define { }. For and , define . Let K be the set of non-empty compact subsets of X. For , define inf{ and }. Assuming that it forms a metric space show that is a compact metric space. My attempt: The metric space is compact so it is totally bounded. Let { } be a sequence of compact metric spaces in . We need to show that the infinite set has a limit point. is totally bounded so let ={ } such that covers X for and covers X for ,so So we can generate a new sequence of compact metric space as mentioned above.Now,let { }= .We take the closure which is the closed subset of .Hence it is compact so it is in . for all is what we have to show to prove that is the limit point of the sequence { } to show that the sequence has bolzano weistrass property. We pick an and proceed as , Now, where and where is any compact subset in the sequence { } and can be chosen to be different from . Now, let then and where then and let then so and the other case should also follow when where . So . Then which allows me to conclude that is the limit point of the above mentioned seqeunce (as can be chosen arbitrarily).So by bolzano property I can claim the compactness. Is my attempt ok? I have tried hard so that the notations are understandable. Thanks in advance.","(X, d) x ∈ X  \epsilon > 0 B_{\epsilon}(x) := {y ∈
X | d(x, y) < \epsilon} C ⊆ X \epsilon > 0 B_{\epsilon}(C) := ∪_{x∈C}B_{\epsilon}(x) C, C_0 ∈ K δ(C, C_0) =  \epsilon| C ⊆ B_{\epsilon}(C_0) C_0 ⊆ B_{\epsilon}(C) (K, δ)  (X,d) B= C_k K (X, d) A_1 x_{1,1},x_{2, 1},....,x_{k_1,1} {\cup {B_d(x_i,1) }}  i= (1,1),..,(k_1,1) {\cup {B_d(x_i,\frac{1}{2}) }}  i= (1,2),..,(k_2,2) A_2= x_{1,2},x_{2,2},...,x_{k_2,2} A_1,A_2,...,A_n \cup {(A_i)} A cl(A) (X,d) (K,\delta) (B_{\delta}(cl(A) \cap B)  /cl(A))\ne \phi \epsilon>0 cl(A) {C_k} \epsilon >0 C_i \subset B_{\frac{1}{N_1}}(cl(A)) \frac{1}{N_1} < \frac{\epsilon}{2} C_i \subset B_{\epsilon}(cl(A)) C_i C_k C_i cl(A) x \in Cl(A) x \in X x \in \cup B_d(x_i,\frac{1}{N_1}) i=(1,N_1),(2,N_1),(3,N_1),...,(k_{N_1},N_1) d(x,x_i) < \frac{1}{N_1} x_n \in C_i d(x_i,x_n) < \frac{1}{N_1} d(x,x_n) < \epsilon  x = x_i  i=(1,N_1),(2,N_1),(3,N_1),...,(k_{N_1},N_1) cl(A) \subset B_{\epsilon}(C_i) \delta(C_i,cl(A)) < \epsilon cl(A) \epsilon","['analysis', 'metric-spaces', 'compactness']"
43,"Is there a numeral system for real numbers that is always unique, but still has the usual convenient properties?","Is there a numeral system for real numbers that is always unique, but still has the usual convenient properties?",,"For each integer $b\ge 2,$ we know that representations of real numbers are usually unique in the base- $b$ positional notation . The only time that uniqueness fails is if the form ends in a tail of $0$ 's or a tail of $(b-1)$ 's, in which case it is easy to convert between these dual representations. However, the fact that multiple representations are ever possible forces the mathematician to be additionally careful in writing some proofs. For example, in the standard application of Cantor's diagonal argument to show that the continuum is uncountable, one has to be careful to mention that we  are constructing the rows using only terminating forms when there are dual representations and that the (anti-)diagonal element constructed is not somehow a dual form of one of the those terminating forms. Question: Can a numeral system be constructed which represents all real numbers uniquely and only real numbers while still admitting some or all of the following convenient properties of the ordinary positional notation, and perhaps additional nice properties of its own: Being exponentially more efficient than unary , meaning the number of distinct integers represented by at most a certain number of digits is something like the number of distinct symbols in the system to the power of the number of digits. Admitting convenient pen-and-paper and computer algorithms for performing the arithmetic operations of addition, subtraction, multiplication, division and exponentiation, at least when integers or rationals are involved. Allowing for the existence of some convenient divisibility rules of integers, though not necessarily the same ones as those admitted by base- $b.$ Having predictable (eg. periodic/cyclic) patterns in the representations of some large classes of real numbers, like the rationals. If these properties are not possible to fulfill, I would still be interested in a system where there is uniqueness at the cost of losing these features. References to non-standard numeral systems that aim for such a goal (or perhaps other goals of convenience) would be appreciated.","For each integer we know that representations of real numbers are usually unique in the base- positional notation . The only time that uniqueness fails is if the form ends in a tail of 's or a tail of 's, in which case it is easy to convert between these dual representations. However, the fact that multiple representations are ever possible forces the mathematician to be additionally careful in writing some proofs. For example, in the standard application of Cantor's diagonal argument to show that the continuum is uncountable, one has to be careful to mention that we  are constructing the rows using only terminating forms when there are dual representations and that the (anti-)diagonal element constructed is not somehow a dual form of one of the those terminating forms. Question: Can a numeral system be constructed which represents all real numbers uniquely and only real numbers while still admitting some or all of the following convenient properties of the ordinary positional notation, and perhaps additional nice properties of its own: Being exponentially more efficient than unary , meaning the number of distinct integers represented by at most a certain number of digits is something like the number of distinct symbols in the system to the power of the number of digits. Admitting convenient pen-and-paper and computer algorithms for performing the arithmetic operations of addition, subtraction, multiplication, division and exponentiation, at least when integers or rationals are involved. Allowing for the existence of some convenient divisibility rules of integers, though not necessarily the same ones as those admitted by base- Having predictable (eg. periodic/cyclic) patterns in the representations of some large classes of real numbers, like the rationals. If these properties are not possible to fulfill, I would still be interested in a system where there is uniqueness at the cost of losing these features. References to non-standard numeral systems that aim for such a goal (or perhaps other goals of convenience) would be appreciated.","b\ge 2, b 0 (b-1) b.","['number-theory', 'analysis', 'decimal-expansion', 'number-systems']"
44,"What is the most ""efficient"" closed-form expression for approximating a transcendental number?","What is the most ""efficient"" closed-form expression for approximating a transcendental number?",,"$\frac{9}{5}+\sqrt{\frac{9}{5}}=3.1416...$ is a closed-form approximation of $\pi$ with a precision of 3 digits. After reading the wiki page on approximations of pi , I read about many clever methods for finding closed-form approximations of $\pi$ . I was curious how much ""information"" you can pack in a closed-form expression to approximate some transcendental number. I wouldn't know how to measure the ""efficiency"" of a closed-form expression. For instance, the number of integers in the expression divided by the the number of digits of precision. That way, you get information divided by precision. Is there a limit to how much you can pack in an expression?","is a closed-form approximation of with a precision of 3 digits. After reading the wiki page on approximations of pi , I read about many clever methods for finding closed-form approximations of . I was curious how much ""information"" you can pack in a closed-form expression to approximate some transcendental number. I wouldn't know how to measure the ""efficiency"" of a closed-form expression. For instance, the number of integers in the expression divided by the the number of digits of precision. That way, you get information divided by precision. Is there a limit to how much you can pack in an expression?",\frac{9}{5}+\sqrt{\frac{9}{5}}=3.1416... \pi \pi,"['number-theory', 'analysis', 'transcendental-numbers']"
45,Correcting the set in my proof.,Correcting the set in my proof.,,"The question was: Find $\int_{[0, \pi/2]} f$ if $$f(x) = \begin{cases}        \sin{x}, & if  \cos(x) \in \mathbb{Q}, \\       \sin^2{x}, &  if \cos(x) \not\in \mathbb{Q}.    \end{cases}$$ My answer was: Assume that $0 \leq x \leq \pi/2$ , then taking the cosine of this, we get $\cos 0 \geq \cos x \geq \cos (\pi/2),$ so, $1 \geq \cos (x) \geq 0$ (because $\cos (x)$ is a decreasing function in this interval.)\ Now, by monotonicity of measure $$m\{x \in [0, \pi/2] \mid \cos(x)\in \mathbb Q\} \subseteq m\{[0, \pi/2] \cap  \mathbb{Q}\}.$$ But, $m\{[0, \pi/2] \cap  \mathbb{Q}\} = 0.$ \ This is because $\mathbb{Q}$ is countable and hence its measure is 0 and $\{[0, \pi/2] \cap  \mathbb{Q}\} \subset \mathbb{Q}$ , then by monotonicity of measure $m\{[0, \pi/2] \cap  \mathbb{Q}\} = 0.$ \ And since the integral of  each integrable function $f$ on a set of measure equal to $0$ is $0$ , we have:\ $\int_{[0, \pi/2]} f = \int_{[0, \pi/2] \cap  \mathbb{Q}} f + \int_{[0, \pi/2] \cap  \mathbb{Q}^c} f = 0 + \int_{[0, \pi/2] \cap  \mathbb{Q}^c} f = \int_{[0, \pi/2] \cap  \mathbb{Q}^c}  \sin^2{x}  = \int_{[0, \pi/2] \cap  \mathbb{Q}}  \sin^2{x} + \int_{[0, \pi/2] \cap  \mathbb{Q}^c}  \sin^2{x} dx = \int_{[0, \pi/2]}  \sin^2{x} dx,$ \ Where in the last equality we have changed the Lebesgue integral over $[0, \pi/2]$ into Riemann integral over $[0, \pi/2]$ because our function $\sin^2{x}$ is Riemann integrable and bounded by $[0,1]$ and the domain of integration is closed and bounded interval then by \textbf{ Theorem 3, on page 73} the Lebesgue integral is the Riemann integral.\ Now we can compute this integral:\ $$\int_0^{\pi/2}f(x)\,\mathrm d x=\int_0^{\pi/2}\sin^2(x)\,\mathrm d x = \int_0^{\pi/2} \{ \frac{1 - \cos{2x}}{2} \} d x = \frac{\pi}{4} - ( \frac{1}{4} \times 0) = \frac{\pi}{4}. $$ But it turns out that: My justification in this step: $$m\{x \in [0, \pi/2] \mid \cos(x)\in \mathbb Q\} \subseteq m\{[0, \pi/2] \cap  \mathbb{Q}\}.$$ was wrong, could anyone help me correct this step please?","The question was: Find if My answer was: Assume that , then taking the cosine of this, we get so, (because is a decreasing function in this interval.)\ Now, by monotonicity of measure But, \ This is because is countable and hence its measure is 0 and , then by monotonicity of measure \ And since the integral of  each integrable function on a set of measure equal to is , we have:\ \ Where in the last equality we have changed the Lebesgue integral over into Riemann integral over because our function is Riemann integrable and bounded by and the domain of integration is closed and bounded interval then by \textbf{ Theorem 3, on page 73} the Lebesgue integral is the Riemann integral.\ Now we can compute this integral:\ But it turns out that: My justification in this step: was wrong, could anyone help me correct this step please?","\int_{[0, \pi/2]} f f(x) = \begin{cases} 
      \sin{x}, & if  \cos(x) \in \mathbb{Q}, \\
      \sin^2{x}, &  if \cos(x) \not\in \mathbb{Q}.
   \end{cases} 0 \leq x \leq \pi/2 \cos 0 \geq \cos x \geq \cos (\pi/2), 1 \geq \cos (x) \geq 0 \cos (x) m\{x \in [0, \pi/2] \mid \cos(x)\in \mathbb Q\} \subseteq m\{[0, \pi/2] \cap  \mathbb{Q}\}. m\{[0, \pi/2] \cap  \mathbb{Q}\} = 0. \mathbb{Q} \{[0, \pi/2] \cap  \mathbb{Q}\} \subset \mathbb{Q} m\{[0, \pi/2] \cap  \mathbb{Q}\} = 0. f 0 0 \int_{[0, \pi/2]} f = \int_{[0, \pi/2] \cap  \mathbb{Q}} f + \int_{[0, \pi/2] \cap  \mathbb{Q}^c} f = 0 + \int_{[0, \pi/2] \cap  \mathbb{Q}^c} f = \int_{[0, \pi/2] \cap  \mathbb{Q}^c}  \sin^2{x}  = \int_{[0, \pi/2] \cap  \mathbb{Q}}  \sin^2{x} + \int_{[0, \pi/2] \cap  \mathbb{Q}^c}  \sin^2{x} dx = \int_{[0, \pi/2]}  \sin^2{x} dx, [0, \pi/2] [0, \pi/2] \sin^2{x} [0,1] \int_0^{\pi/2}f(x)\,\mathrm d x=\int_0^{\pi/2}\sin^2(x)\,\mathrm d x = \int_0^{\pi/2} \{ \frac{1 - \cos{2x}}{2} \} d x = \frac{\pi}{4} - ( \frac{1}{4} \times 0) = \frac{\pi}{4}.  m\{x \in [0, \pi/2] \mid \cos(x)\in \mathbb Q\} \subseteq m\{[0, \pi/2] \cap  \mathbb{Q}\}.","['real-analysis', 'analysis']"
46,"How to prove that $X$ is a completion of $Y$, for some metric spcaes $(X,d)$ and $(Y,d)$?","How to prove that  is a completion of , for some metric spcaes  and ?","X Y (X,d) (Y,d)","In our Real Analysis class we did a proof for existence of completion of metric space. However, I do not understand how do we use it practically. i.e How do I prove that $X$ is a completion of $Y$ , for some metric spcaes $(X,d)$ and $(Y,d)$ ? For example : How do I prove that $(\mathbb{R},d_1)$ is a completeion of $(\mathbb{Q},d_1)$ ? Here, $d_1$ is the standard Euclidean metric $\underline{\text{My attempt}}$ - I believe we should prove that $\mathbb{Q}$ is subset of $\mathbb{R}$ and $\mathbb{R}$ is complete. This would mean that all Cauchy sequence in $\mathbb{Q}$ converges in $\mathbb{R}$ . However, consider this case - Let $(W,d)$ be an incomplete Metric space and $W \subset Y \subset X$ and $X,Y$ are complete (with same metric). Now, by my argument both $X,Y$ can be the completion of $W$ . Is this   alright? I read somewhere in Stack Exchange that completion is unique(I didn't really understand that answer) $\underline{\text{Approach to resolve this}}$ - We choose the smallest, complete set which contains $W$ as its completion. We can prove that, since $X$ is complete any of it's closed subset is complete. We can also prove that $\overline{W}$ is the smallest closed set that contains $W$ . Hence, $\overline{W}$ is the completion of $W$ (this would explain the notation in the completion proof). Thus in the title - I just have to check if $Y$ is dense in $X$ to prove that $X$ is a completion of $Y$ . This would also solve my example question - We know that $\mathbb{Q}$ is dense in $\mathbb{R}$ and $\mathbb{R}$ is complete. Thus $\mathbb{R}$ is completion of $\mathbb{Q}$ . I am fairly confident that this is correct. However, I am a Physics Master's student this is my first ""Real"" Math course ;) So I would appreciate if someone let's me know if this right and if not the corrections.","In our Real Analysis class we did a proof for existence of completion of metric space. However, I do not understand how do we use it practically. i.e How do I prove that is a completion of , for some metric spcaes and ? For example : How do I prove that is a completeion of ? Here, is the standard Euclidean metric - I believe we should prove that is subset of and is complete. This would mean that all Cauchy sequence in converges in . However, consider this case - Let be an incomplete Metric space and and are complete (with same metric). Now, by my argument both can be the completion of . Is this   alright? I read somewhere in Stack Exchange that completion is unique(I didn't really understand that answer) - We choose the smallest, complete set which contains as its completion. We can prove that, since is complete any of it's closed subset is complete. We can also prove that is the smallest closed set that contains . Hence, is the completion of (this would explain the notation in the completion proof). Thus in the title - I just have to check if is dense in to prove that is a completion of . This would also solve my example question - We know that is dense in and is complete. Thus is completion of . I am fairly confident that this is correct. However, I am a Physics Master's student this is my first ""Real"" Math course ;) So I would appreciate if someone let's me know if this right and if not the corrections.","X Y (X,d) (Y,d) (\mathbb{R},d_1) (\mathbb{Q},d_1) d_1 \underline{\text{My attempt}} \mathbb{Q} \mathbb{R} \mathbb{R} \mathbb{Q} \mathbb{R} (W,d) W \subset Y \subset X X,Y X,Y W \underline{\text{Approach to resolve this}} W X \overline{W} W \overline{W} W Y X X Y \mathbb{Q} \mathbb{R} \mathbb{R} \mathbb{R} \mathbb{Q}","['real-analysis', 'analysis', 'metric-spaces', 'complete-spaces']"
47,"For $0\lt\theta\lt1$, $\frac 1\theta\notin\mathbb Z$, there exists $f\in C[0, 1]$ such that $f(0)=f(1)$ and $f(x+\theta)-f(x)\ne0$","For , , there exists  such that  and","0\lt\theta\lt1 \frac 1\theta\notin\mathbb Z f\in C[0, 1] f(0)=f(1) f(x+\theta)-f(x)\ne0","Prove that for each $0\lt\theta\lt1, \dfrac{1}{\theta} $ isn't an integer, there exists $f \in C[0, 1]$ such that $f(0)=f(1)$ , and $ \forall x\in[0,1-\theta]  , f(x+\theta)-f(x)\ne0 $ (If $\theta\gt\frac12, $ it is obvious that such an $f$ exists.)","Prove that for each isn't an integer, there exists such that , and (If it is obvious that such an exists.)","0\lt\theta\lt1, \dfrac{1}{\theta}  f \in C[0, 1] f(0)=f(1)  \forall x\in[0,1-\theta]  , f(x+\theta)-f(x)\ne0  \theta\gt\frac12,  f","['real-analysis', 'calculus', 'analysis', 'functions']"
48,A property of the x-ray transform,A property of the x-ray transform,,"Problem: Let $P_{\theta}f(x) = \int_{\mathbb{R}}f(x + s \theta) ds$ be defined as the x-ray transform, where $\theta \in S^{n-1}$, and $x$ belongs to $\Theta^{\perp}$, the hyperplane that passes through the origin and is orthogonal to $\theta$. Now, why is it that $\widehat{P_{\theta}f}(\xi) = 0$ for $\xi \in \Theta^{\perp}$ implies that $P_{\theta}f = 0$ ($\hat{f}$ means Fourier transform of $f$)? This result occured in the paper ""Practical and Mathematical Aspects of the Problem of Reconstructing Objects from Radiographs"" (Theorem 4.2 proof), the author did not give any explanation for it. Attempt at an explanation: I was originally thinking maybe the injectivity of the Fourier transform was playing a role here, but I can't see how it can actually be true on a hyperplane instead of $\mathbb{R}^n$. Using the Fourier inversion formula also doesn't seem to give me $P_{\theta}f = 0$, since $\widehat{P_{\theta}f}(\xi)$ only vanishes on $\Theta^{\perp}$ instead of on all of $\mathbb{R}^n$. Am I missing something very obvious here? Any help is appreciated!","Problem: Let $P_{\theta}f(x) = \int_{\mathbb{R}}f(x + s \theta) ds$ be defined as the x-ray transform, where $\theta \in S^{n-1}$, and $x$ belongs to $\Theta^{\perp}$, the hyperplane that passes through the origin and is orthogonal to $\theta$. Now, why is it that $\widehat{P_{\theta}f}(\xi) = 0$ for $\xi \in \Theta^{\perp}$ implies that $P_{\theta}f = 0$ ($\hat{f}$ means Fourier transform of $f$)? This result occured in the paper ""Practical and Mathematical Aspects of the Problem of Reconstructing Objects from Radiographs"" (Theorem 4.2 proof), the author did not give any explanation for it. Attempt at an explanation: I was originally thinking maybe the injectivity of the Fourier transform was playing a role here, but I can't see how it can actually be true on a hyperplane instead of $\mathbb{R}^n$. Using the Fourier inversion formula also doesn't seem to give me $P_{\theta}f = 0$, since $\widehat{P_{\theta}f}(\xi)$ only vanishes on $\Theta^{\perp}$ instead of on all of $\mathbb{R}^n$. Am I missing something very obvious here? Any help is appreciated!",,"['real-analysis', 'analysis', 'fourier-analysis', 'image-processing']"
49,Can we remove the absolute value when doing Diophantine Approximation?,Can we remove the absolute value when doing Diophantine Approximation?,,"This is a very general question. For Diophantine Approximation propositions, the statements always include the absolute value sign, but I think if we can remove the absolute value sign, the approximation will become more useful. For example, we have Hurwitz's theorem: For any irrational $\zeta$ there are infinitely many pairs of integers $p,q$ such that $|\zeta - p/q|<\frac{1}{\sqrt{5}q^2}$ However, when we remove the absolute value, is the following statement true or not? For any irrational $\zeta$ there are infinitely many pairs of integers $p,q$ such that $0<\zeta - p/q<\frac{1}{\sqrt{5}q^2}$ This is just example. My main question is: Is there any theory about it? Where can I get the reference on this topic?","This is a very general question. For Diophantine Approximation propositions, the statements always include the absolute value sign, but I think if we can remove the absolute value sign, the approximation will become more useful. For example, we have Hurwitz's theorem: For any irrational $\zeta$ there are infinitely many pairs of integers $p,q$ such that $|\zeta - p/q|<\frac{1}{\sqrt{5}q^2}$ However, when we remove the absolute value, is the following statement true or not? For any irrational $\zeta$ there are infinitely many pairs of integers $p,q$ such that $0<\zeta - p/q<\frac{1}{\sqrt{5}q^2}$ This is just example. My main question is: Is there any theory about it? Where can I get the reference on this topic?",,"['number-theory', 'analysis', 'diophantine-approximation']"
50,"Surjective, but not Injective Linear map between C[0,1]","Surjective, but not Injective Linear map between C[0,1]",,"Given the Banach space $C[0,1]$={$f$:$[0,1]$$\longrightarrow$$\mathbb{C}$, continuous} equipped with sup norm, I would like to find a bounded linear operator $T$ : $C[0,1]$$\longrightarrow$$C[0,1]$ that is surjective but not injective. And all I can think of is a differential operator, but not all continuous functions are differentiable. So a differential operator wouldn't work unless the domain is restricted. I have come up with $T(f(x))=f(\frac{x}{2})-f(1-\frac{x}{2})$ T is not injective, but I am not convinced if T is even surjective. So any help would be appreciated. Thank you.","Given the Banach space $C[0,1]$={$f$:$[0,1]$$\longrightarrow$$\mathbb{C}$, continuous} equipped with sup norm, I would like to find a bounded linear operator $T$ : $C[0,1]$$\longrightarrow$$C[0,1]$ that is surjective but not injective. And all I can think of is a differential operator, but not all continuous functions are differentiable. So a differential operator wouldn't work unless the domain is restricted. I have come up with $T(f(x))=f(\frac{x}{2})-f(1-\frac{x}{2})$ T is not injective, but I am not convinced if T is even surjective. So any help would be appreciated. Thank you.",,"['analysis', 'banach-spaces', 'differential-operators']"
51,is the sup of functions measurable function?,is the sup of functions measurable function?,,"Good day, remembering the definition: The function $f(x)$ defined on the set $E$ is said to be measurable if  the bounded set $E$ is measurable and if the set $E\cap \left \{ x:f(x)> a \right \}$ is measurable for all $a$. Prove that the least upper bound of a finite or denumerable set of measurable functions is a  measurable function. Can I use this result? If $f(x)$ is a measurable function defined on the set $E$, then the sets $E\cap \left \{ x:f(x) \geq a \right \}$, $E\cap \left \{ x:f(x)= a \right \}$, $E\cap \left \{ x:f(x) \leq a \right \}$ are measurables for all $a$.","Good day, remembering the definition: The function $f(x)$ defined on the set $E$ is said to be measurable if  the bounded set $E$ is measurable and if the set $E\cap \left \{ x:f(x)> a \right \}$ is measurable for all $a$. Prove that the least upper bound of a finite or denumerable set of measurable functions is a  measurable function. Can I use this result? If $f(x)$ is a measurable function defined on the set $E$, then the sets $E\cap \left \{ x:f(x) \geq a \right \}$, $E\cap \left \{ x:f(x)= a \right \}$, $E\cap \left \{ x:f(x) \leq a \right \}$ are measurables for all $a$.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'supremum-and-infimum']"
52,"Does $\sum _{ n=1 }^\infty \frac {(-1) ^n} n \sin (nx) $ converge uniformly on $[0, \pi )$?",Does  converge uniformly on ?,"\sum _{ n=1 }^\infty \frac {(-1) ^n} n \sin (nx)  [0, \pi )","i tried Werierstrass M test, but >$$\left| \frac{(-1)^n}n \sin(nx)\right| \leq \frac 1 n.$$ is there any other way to do it?","i tried Werierstrass M test, but >$$\left| \frac{(-1)^n}n \sin(nx)\right| \leq \frac 1 n.$$ is there any other way to do it?",,"['calculus', 'real-analysis', 'analysis']"
53,What happens when the product of Sigma Algebras is taken over an uncountable set??,What happens when the product of Sigma Algebras is taken over an uncountable set??,,"Let $\{ X_{\alpha} \}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod_{\alpha \in A}X_{\alpha}$, and $\pi_{\alpha}: X \rightarrow X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the $\textbf{product $\sigma$-algebra}$ on $X$ is the $\sigma$-algebra generated by : $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \}.$$ Then we have these next two propositions: 1.If $A$ is countable, then $\textbf{product $\sigma$-algebra}$ is generated by $\{\prod_{\alpha \in A} E_{\alpha}:E_{\alpha} \in \mathcal{M}_{\alpha}  \}.$ Suppose $\mathcal{M}_{\alpha}$ is generated by $\mu_{\alpha},\alpha \in A$. Then $\textbf{product $\sigma$-algebra}$ is generated by $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mu_{\alpha}, \alpha \in A \}.$$ Furthermore, if $A$ is countable and $X_{\alpha} \in \mu_{\alpha}, \alpha \in A$, then $\textbf{product $\sigma$-algebra}$ is generated by $\{\prod_{\alpha \in A} E_{\alpha}:E_{\alpha} \in \mu_{\alpha}  \}.$ The proof is easy. The first bit hinges on the observation that  $$\prod_{\alpha \in A} E_{\alpha}=\cap_{\alpha \in A}\pi^{-1}_{\alpha}(E_{\alpha})$$ and if $E \subset \sigma(F)$(The sigma algebra generated by F), then $\sigma(E) \subset \sigma(F)$. The first part of second bit follows by constructing a set with the required property and the lemma (if $E \subset \sigma(F)$(The sigma algebra generated by F), then $\sigma(E) \subset \sigma(F))$, The second bit follows from the first bit along with the use of lemma. Well I was wondering what happens when $A$ is uncountable. Clearly the proof of $1$ won't go through. I was not able to get any counterexample though. I was thinking of taking $A=\mathbb{R}$ and $X_{\alpha}=[0,1]$ for each $\alpha$. But couldn't succeed. Similarly the proof of $2$ won't go through as well. Can I have a counterexample for this as well?? Also what happens when $X_{\alpha} \not\in \mu_{\alpha}$?? This is certainly used in the proof. Thanks for the help!!","Let $\{ X_{\alpha} \}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod_{\alpha \in A}X_{\alpha}$, and $\pi_{\alpha}: X \rightarrow X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the $\textbf{product $\sigma$-algebra}$ on $X$ is the $\sigma$-algebra generated by : $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \}.$$ Then we have these next two propositions: 1.If $A$ is countable, then $\textbf{product $\sigma$-algebra}$ is generated by $\{\prod_{\alpha \in A} E_{\alpha}:E_{\alpha} \in \mathcal{M}_{\alpha}  \}.$ Suppose $\mathcal{M}_{\alpha}$ is generated by $\mu_{\alpha},\alpha \in A$. Then $\textbf{product $\sigma$-algebra}$ is generated by $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mu_{\alpha}, \alpha \in A \}.$$ Furthermore, if $A$ is countable and $X_{\alpha} \in \mu_{\alpha}, \alpha \in A$, then $\textbf{product $\sigma$-algebra}$ is generated by $\{\prod_{\alpha \in A} E_{\alpha}:E_{\alpha} \in \mu_{\alpha}  \}.$ The proof is easy. The first bit hinges on the observation that  $$\prod_{\alpha \in A} E_{\alpha}=\cap_{\alpha \in A}\pi^{-1}_{\alpha}(E_{\alpha})$$ and if $E \subset \sigma(F)$(The sigma algebra generated by F), then $\sigma(E) \subset \sigma(F)$. The first part of second bit follows by constructing a set with the required property and the lemma (if $E \subset \sigma(F)$(The sigma algebra generated by F), then $\sigma(E) \subset \sigma(F))$, The second bit follows from the first bit along with the use of lemma. Well I was wondering what happens when $A$ is uncountable. Clearly the proof of $1$ won't go through. I was not able to get any counterexample though. I was thinking of taking $A=\mathbb{R}$ and $X_{\alpha}=[0,1]$ for each $\alpha$. But couldn't succeed. Similarly the proof of $2$ won't go through as well. Can I have a counterexample for this as well?? Also what happens when $X_{\alpha} \not\in \mu_{\alpha}$?? This is certainly used in the proof. Thanks for the help!!",,"['real-analysis', 'analysis', 'measure-theory', 'products']"
54,Proof of the Arzelà–Ascoli Theorem,Proof of the Arzelà–Ascoli Theorem,,"I'm stuck on a particular line of the proof of The Arzelà–Ascoli Theorem. In lectures, we have: $1.$ Defined equicontinuous as: Let $X$ be a metric space, $C(X) = \{f: X \rightarrow \mathbb{R}\text{ continuous} \}$ the space of continuous functions, $S \subset C(X)$ . Let $x \in X$ be a point. Then $S$ is equicontinuous at $x$ if $\forall \varepsilon > 0$ , $\exists \delta > 0$ such that $y \in B(x, \delta)$ , $f \in S$ $\implies$ $|f(x) - f(y)| < \varepsilon$ . And obviously $S$ is equicontinuous if it is equicontinuous at all points of $X$ . $2.$ Stated The Arzelà–Ascoli Theorem as: Suppose that $X$ is a compact metric space and $S \subset C(X)$ is a subspace. Then, $S$ is compact $\iff$ $S$ is closed, bounded, and equicontinuous. In the proof of the forward direction: Closed and bounded are clear, so it remains to show that $S$ is equicontinuous. We already know that $S$ is totally bounded, so let $\varepsilon > 0$ and fix $x \in X$ . Then $\exists F \subset S$ finite such that $S \subset \bigcup_{f \in F}B(f, \frac{\varepsilon}{3})$ . Since $F$ is equicontinuous... This is the line that I get stuck at, why is $F$ equicontinuous?","I'm stuck on a particular line of the proof of The Arzelà–Ascoli Theorem. In lectures, we have: Defined equicontinuous as: Let be a metric space, the space of continuous functions, . Let be a point. Then is equicontinuous at if , such that , . And obviously is equicontinuous if it is equicontinuous at all points of . Stated The Arzelà–Ascoli Theorem as: Suppose that is a compact metric space and is a subspace. Then, is compact is closed, bounded, and equicontinuous. In the proof of the forward direction: Closed and bounded are clear, so it remains to show that is equicontinuous. We already know that is totally bounded, so let and fix . Then finite such that . Since is equicontinuous... This is the line that I get stuck at, why is equicontinuous?","1. X C(X) = \{f: X \rightarrow \mathbb{R}\text{ continuous} \} S \subset C(X) x \in X S x \forall \varepsilon > 0 \exists \delta > 0 y \in B(x, \delta) f \in S \implies |f(x) - f(y)| < \varepsilon S X 2. X S \subset C(X) S \iff S S S \varepsilon > 0 x \in X \exists F \subset S S \subset \bigcup_{f \in F}B(f, \frac{\varepsilon}{3}) F F","['analysis', 'metric-spaces', 'compactness', 'proof-explanation']"
55,Does the Cauchy Schwarz inequality hold on the L1 and L infinity norm? [closed],Does the Cauchy Schwarz inequality hold on the L1 and L infinity norm? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question So i am wondering if the Cauchy Schwarz inequality holds for all p-norms, not just when p=2, which is the euclidean space. Thank you.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question So i am wondering if the Cauchy Schwarz inequality holds for all p-norms, not just when p=2, which is the euclidean space. Thank you.",,"['real-analysis', 'analysis']"
56,"Can we, in a certain way, quantify the measure of non-differentiability of functions that are continuous everywhere but differentiable nowhere?","Can we, in a certain way, quantify the measure of non-differentiability of functions that are continuous everywhere but differentiable nowhere?",,"I am not sure how to ask this question because it seems to me that my thoughts on this topic are not clear enough, but I will give it a try. What, really, do I want to know? Well, I would like to know is there any ""measure"" on how ""far"" is such a function from being differentiable at some point? In other words, and hopefully, more precise ones: Suppose that $f$ is some function that is continuous at every point of its domain and differentiable nowhere. Is there a way to assign to every point of such a function a number which measures ""how far is function from being differentiable"" at that point, or, ""how non-differentiable"" the function at that point is? I like to believe that some of you really understand the spirit of the question and what it is all about, although I at the moment do not know how to state it more correctly and more rigorously.","I am not sure how to ask this question because it seems to me that my thoughts on this topic are not clear enough, but I will give it a try. What, really, do I want to know? Well, I would like to know is there any ""measure"" on how ""far"" is such a function from being differentiable at some point? In other words, and hopefully, more precise ones: Suppose that $f$ is some function that is continuous at every point of its domain and differentiable nowhere. Is there a way to assign to every point of such a function a number which measures ""how far is function from being differentiable"" at that point, or, ""how non-differentiable"" the function at that point is? I like to believe that some of you really understand the spirit of the question and what it is all about, although I at the moment do not know how to state it more correctly and more rigorously.",,"['analysis', 'soft-question']"
57,"The polynomial $p(x)=a_0+a_1x+a_2x^2+\cdots +a_nx^n$ has a zero in $[0,1]$ under a given condition",The polynomial  has a zero in  under a given condition,"p(x)=a_0+a_1x+a_2x^2+\cdots +a_nx^n [0,1]","Show that the polynomial $p(x)=a_0+a_1x+a_2x^2+\cdots +a_nx^n$ has a zero in $[0,1]$ when it is given that, $$ \frac{a_0}{1\cdot 2}+\frac{a_1}{2\cdot 3}+\cdots+\frac{a_n}{(n+1)(n+2)}=0.$$ First we consider the function, $\displaystyle f(x)=\frac{a_0}{1\cdot 2}x^2+\frac{a_1}{2\cdot 3}x^3+\cdots+\frac{a_n}{(n+1)(n+2)}x^{n+2}$ . Then, $f(0)=0$ and $f(1)=0$ . Then by Rolle's theorem, $\exists$ $y\in (0,1)$ such that $f'(y)=0$ which gives, $\displaystyle a_0y+\frac{a_1}{2}y^2+\cdots +\frac{a_n}{n+1}y^{n+1}=0$ . Next consider, $\displaystyle g(z)=a_0z+\frac{a_1}{2}z^2+\cdots +\frac{a_n}{n+1}z^{n+1}$ . Then, $g(0)=0$ and $g(y)=0$ . So by Rolle's theorem, $\exists$ $w\in (0,y)\subset (0,1)$ such that $g'(w)=p(w)=0.$ Hence the proof is complete. Is this proof is correct? I think it is correct. But I am looking for an another proof so that I can avoid two times consideration of functions such as $f$ and $g$ .","Show that the polynomial has a zero in when it is given that, First we consider the function, . Then, and . Then by Rolle's theorem, such that which gives, . Next consider, . Then, and . So by Rolle's theorem, such that Hence the proof is complete. Is this proof is correct? I think it is correct. But I am looking for an another proof so that I can avoid two times consideration of functions such as and .","p(x)=a_0+a_1x+a_2x^2+\cdots +a_nx^n [0,1]  \frac{a_0}{1\cdot 2}+\frac{a_1}{2\cdot 3}+\cdots+\frac{a_n}{(n+1)(n+2)}=0. \displaystyle f(x)=\frac{a_0}{1\cdot 2}x^2+\frac{a_1}{2\cdot 3}x^3+\cdots+\frac{a_n}{(n+1)(n+2)}x^{n+2} f(0)=0 f(1)=0 \exists y\in (0,1) f'(y)=0 \displaystyle a_0y+\frac{a_1}{2}y^2+\cdots +\frac{a_n}{n+1}y^{n+1}=0 \displaystyle g(z)=a_0z+\frac{a_1}{2}z^2+\cdots +\frac{a_n}{n+1}z^{n+1} g(0)=0 g(y)=0 \exists w\in (0,y)\subset (0,1) g'(w)=p(w)=0. f g","['real-analysis', 'analysis', 'continuity']"
58,Does every bounded Jordan measurable set have porous boundary?,Does every bounded Jordan measurable set have porous boundary?,,Let $A\subset \mathbb{R}^n$ be a bounded Jordan measurable set. I wonder if its boundary $\partial A$ is necessarily porous. I know that $\partial A$ has Lebesgue measure zero. I also think that one can construct an example of a null Lebesgue set that is not porous. But is it possible to construct such a set in a way that it is also the boundary of a Jordan measurable set?,Let $A\subset \mathbb{R}^n$ be a bounded Jordan measurable set. I wonder if its boundary $\partial A$ is necessarily porous. I know that $\partial A$ has Lebesgue measure zero. I also think that one can construct an example of a null Lebesgue set that is not porous. But is it possible to construct such a set in a way that it is also the boundary of a Jordan measurable set?,,"['real-analysis', 'analysis', 'measure-theory', 'geometric-measure-theory']"
59,"Looking for an example of an increasing function $f:[a,b] \to [a,b]$ which is discontinuous at infinitely many points",Looking for an example of an increasing function  which is discontinuous at infinitely many points,"f:[a,b] \to [a,b]","I am looking for an example of an increasing    function $f:[a,b] \to [a,b]$ which is discontinuous at infinitely many points ; please help , thanks in advance .","I am looking for an example of an increasing    function $f:[a,b] \to [a,b]$ which is discontinuous at infinitely many points ; please help , thanks in advance .",,"['real-analysis', 'analysis']"
60,"$\||f|^{2}f\|_{H^{s}}\leq C \|f\|^{2}_{L^{\infty}} \|f\|_{H^{s}}$ for $s>0, f\in H^{s}(\mathbb R)$?",for ?,"\||f|^{2}f\|_{H^{s}}\leq C \|f\|^{2}_{L^{\infty}} \|f\|_{H^{s}} s>0, f\in H^{s}(\mathbb R)","We let $H^{s}(\mathbb R^{n}), (s\in \mathbb R)$  the Sobolev spaces . It is well known that: the space $H^{s}(\mathbb R^{n})$ is an algebra with respect to pointwise multiplications, for $s>n/2.$ My Question : Can we expect $\||f|^{2}f\|_{H^{s}}\leq C \|f\|^{2}_{L^{\infty}} \|f\|_{H^{s}}$ for $s>0, f\in H^{s}(\mathbb R)$  (where $C$ is some constant)? If Yes, how to prove it? Thanks,","We let $H^{s}(\mathbb R^{n}), (s\in \mathbb R)$  the Sobolev spaces . It is well known that: the space $H^{s}(\mathbb R^{n})$ is an algebra with respect to pointwise multiplications, for $s>n/2.$ My Question : Can we expect $\||f|^{2}f\|_{H^{s}}\leq C \|f\|^{2}_{L^{\infty}} \|f\|_{H^{s}}$ for $s>0, f\in H^{s}(\mathbb R)$  (where $C$ is some constant)? If Yes, how to prove it? Thanks,",,"['analysis', 'sobolev-spaces', 'lp-spaces']"
61,Showing this limit as $x\to 0$,Showing this limit as,x\to 0,"I'm very close to the result and must be missing something basic: $f$ is absolutely continuous on $[\epsilon,1]$ for each $\epsilon \in (0,1)$, $1<p<2$, and, for $0\leq x\leq y\leq 1$, we have (by Hölder's inequality) $$ \int_x^y |f'(t)|\,dt  \le \left(\int_x^y t|f'(t)|^p\,dt\right)^{1/p} \left(\int_x^y t^{1/(1-p ) }\,dt\right)^{1-1/p}, $$ where $\left(\int_0^1 t|f'(t)|^p\,dt\right)^{1/p}=C<\infty$ (by assumption). By letting $x,y$ approach $0$, I'm trying to show that  $$ \frac{f(x)}{x^{1-2/p}}\to 0\qquad \text{as}\ x\to 0. $$ Could someone please explain how this can be obtained rigorously? (I know that by the mean value theorem,  $\int_x^y t^{1/(1-p ) }\,dt =(y-x)s^{1/(1-p )}$ for some $s\in (x,y).$ Of course, this integral behaves like $x^{1+1/(1-p)} = x^{(2-p)/(1-p)}$ which is then raised to the power of $1-1/p$, producing $x^{(p-2)/p}$. But still I haven't proved the result rigorously.) (Perhaps Hölder's inequality does not actually suffice to exploit the assumption that $\left(\int_0^1 t|f'(t)|^p\,dt\right)^{1/p}=C<\infty$, and I need to use some other inequality?)","I'm very close to the result and must be missing something basic: $f$ is absolutely continuous on $[\epsilon,1]$ for each $\epsilon \in (0,1)$, $1<p<2$, and, for $0\leq x\leq y\leq 1$, we have (by Hölder's inequality) $$ \int_x^y |f'(t)|\,dt  \le \left(\int_x^y t|f'(t)|^p\,dt\right)^{1/p} \left(\int_x^y t^{1/(1-p ) }\,dt\right)^{1-1/p}, $$ where $\left(\int_0^1 t|f'(t)|^p\,dt\right)^{1/p}=C<\infty$ (by assumption). By letting $x,y$ approach $0$, I'm trying to show that  $$ \frac{f(x)}{x^{1-2/p}}\to 0\qquad \text{as}\ x\to 0. $$ Could someone please explain how this can be obtained rigorously? (I know that by the mean value theorem,  $\int_x^y t^{1/(1-p ) }\,dt =(y-x)s^{1/(1-p )}$ for some $s\in (x,y).$ Of course, this integral behaves like $x^{1+1/(1-p)} = x^{(2-p)/(1-p)}$ which is then raised to the power of $1-1/p$, producing $x^{(p-2)/p}$. But still I haven't proved the result rigorously.) (Perhaps Hölder's inequality does not actually suffice to exploit the assumption that $\left(\int_0^1 t|f'(t)|^p\,dt\right)^{1/p}=C<\infty$, and I need to use some other inequality?)",,"['real-analysis', 'analysis', 'inequality', 'lp-spaces']"
62,How derivative relates to roots of original function,How derivative relates to roots of original function,,"Assume $f$ is differentiable on $\mathbb{R}$. Show that for any $ k \in \mathbb{R}$, $f' + kf$ has a root between any two distinct roots of $f$. I am completely stumped on this. What are some good ways to show existence of zeros? Thanks.","Assume $f$ is differentiable on $\mathbb{R}$. Show that for any $ k \in \mathbb{R}$, $f' + kf$ has a root between any two distinct roots of $f$. I am completely stumped on this. What are some good ways to show existence of zeros? Thanks.",,"['analysis', 'derivatives']"
63,Remember the Christoffel symbols,Remember the Christoffel symbols,,"This might be a little bit different from what is asked normally on this page, but does anybody here know a good way to remember the definition of the Christoffel symbol? \[ \Gamma^k_{ij} = \frac 12 \sum_{l=1}^2 g^{kl} \left(\frac{\partial g_{il}}{\partial u^j} + \frac{\partial g_{jl}}{\partial u^i} - \frac{\partial g_{ij}}{\partial u^l}\right) \] I have to use this quite often recently, but I always have to look all the indices up, because I have no intuition for this beast.","This might be a little bit different from what is asked normally on this page, but does anybody here know a good way to remember the definition of the Christoffel symbol? \[ \Gamma^k_{ij} = \frac 12 \sum_{l=1}^2 g^{kl} \left(\frac{\partial g_{il}}{\partial u^j} + \frac{\partial g_{jl}}{\partial u^i} - \frac{\partial g_{ij}}{\partial u^l}\right) \] I have to use this quite often recently, but I always have to look all the indices up, because I have no intuition for this beast.",,"['calculus', 'real-analysis']"
64,"How prove there exsit $\xi\in (0,1)$ such $|f(\xi)|\le|f'(\xi)|$",How prove there exsit  such,"\xi\in (0,1) |f(\xi)|\le|f'(\xi)|","Let $f:[0,1]\to \mathbb{R}$ be a differentiable function such that $f(1)=0$ , Prove that there is $\xi\in(0,1)$ , such that $$|f(\xi)|\le|f'(\xi)|.$$ My idea: I think we can prove there exsit $\xi\in (0,1)$ such $$(f(\xi)-f'(\xi))(f(\xi)+f'(\xi))\le 0?$$ maybe we can consider function $$F(x)=e^{\pm x}f(x)$$ But following can't works.","Let be a differentiable function such that , Prove that there is , such that My idea: I think we can prove there exsit such maybe we can consider function But following can't works.","f:[0,1]\to \mathbb{R} f(1)=0 \xi\in(0,1) |f(\xi)|\le|f'(\xi)|. \xi\in (0,1) (f(\xi)-f'(\xi))(f(\xi)+f'(\xi))\le 0? F(x)=e^{\pm x}f(x)",['analysis']
65,"Let $f_n:\mathbb{R}\rightarrow [0, 1]$ be functions such that $\sup_{x \in \mathbb{R}}f_n(x) = 1/n$ and $||f||_1 = 1$.",Let  be functions such that  and .,"f_n:\mathbb{R}\rightarrow [0, 1] \sup_{x \in \mathbb{R}}f_n(x) = 1/n ||f||_1 = 1","Let $f_n:\mathbb{R}\rightarrow [0, 1]$ be functions such that $\sup_{x \in \mathbb{R}}f_n(x) = 1/n$ and $||f_n||_1 = 1$.  Set $F(x) = \sup_{n \in \mathbb{N}}f_n(x)$. Prove that $\int_\mathbb{R}F(x)dx = \infty.$ I am not really sure what tools to use in this question.  My guess is its just some trick I am not seeing and isn't all that complex.  Any suggestions?  Thanks.","Let $f_n:\mathbb{R}\rightarrow [0, 1]$ be functions such that $\sup_{x \in \mathbb{R}}f_n(x) = 1/n$ and $||f_n||_1 = 1$.  Set $F(x) = \sup_{n \in \mathbb{N}}f_n(x)$. Prove that $\int_\mathbb{R}F(x)dx = \infty.$ I am not really sure what tools to use in this question.  My guess is its just some trick I am not seeing and isn't all that complex.  Any suggestions?  Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
66,Show that f is measurable,Show that f is measurable,,"Let $U$ be a open Set of $\mathbb{R} \times [0,\infty]$ and let f be defined as $$f: \mathbb{R}\mapsto [0,\infty], \quad f(x) := \max\{0,\sup\{y| (x,y) \in U\}\} $$ How can I show that $f$ is measurable?","Let $U$ be a open Set of $\mathbb{R} \times [0,\infty]$ and let f be defined as $$f: \mathbb{R}\mapsto [0,\infty], \quad f(x) := \max\{0,\sup\{y| (x,y) \in U\}\} $$ How can I show that $f$ is measurable?",,"['analysis', 'measure-theory']"
67,Product of Hölder and Sobolev functions,Product of Hölder and Sobolev functions,,"Here $C^{\kappa , \lambda} ( \overline{\Omega} ) = \left\{ h|_{\overline{\Omega}} :h \in C^{\kappa , \lambda} ( \mathbb{R}^{n} ) \text{  and } h \text{  has compact support} \right\}$ denotes $\kappa$ times Hölder continuously differentiable functions in $\overline{\Omega}$ and $W^{s,p}(\Omega)$ is the usual Sobolev space with possibly non-integer exponent $s\geq0$. Proposition: Let $\Omega \subset \mathbb{R}^{n}$ be open, bounded and Lipschitz. Let $s   \geq 0 ,1 \leq p< \infty$ and $v \in W^{s,p} ( \Omega ) ,h \in C^{\kappa   , \lambda}_{c} ( \overline{\Omega} )$ where $\kappa \in \mathbb{N}_{0} ,   \lambda \in ( 0,1 ]$ and $\kappa + \lambda \geqslant s$ if $s \in   \mathbb{Z}$ and $\kappa + \lambda >s$ otherwise. Then the product $hu \in   W^{s,p} ( \Omega )$ and there exists a constant such that   $$ \| hu \|_{W^{s,p} ( \Omega )} \leqslant C \| u \|_{W^{s,p} ( \Omega )}. $$ Attempt at proof: The case $s \in \mathbb{N}_0$ is easy since the Sobolev norm doesn't involve the Slobodecijk seminorm and I can use that all partial derivatives of $h$ are uniformly bounded. However, for non integer $s=\lfloor s \rfloor+t, t \in (0,1)$ this seminorm does appear: $$ \| u \|_{W^{s,p} ( \Omega )} := \sum_{| \alpha | \leqslant \lfloor s \rfloor} \| D^{\alpha}  u \|^{p}_{L^{p} ( \Omega )} + \sum_{| \alpha | = \lfloor s \rfloor} \underset{\Omega \times \Omega}{\int \int} \frac{| D^{\alpha}  u ( x ) -D^{\alpha}  u ( y ) |}{| x-y |^{n+tp}} d x d y. $$ And I don't know how to handle the double integrals. Even in the simplest situation where $\lfloor s \rfloor=0$, I don't know how to bound above the integral $$ \underset{\Omega \times \Omega}{\int \int} \frac{| h ( x ) u ( x ) -h ( y ) u ( y ) |}{| x-y |^{n+tp}} d x d y. $$ I fear this might be some trivial inequality, but I just can't see it. Any help would be greatly welcome.","Here $C^{\kappa , \lambda} ( \overline{\Omega} ) = \left\{ h|_{\overline{\Omega}} :h \in C^{\kappa , \lambda} ( \mathbb{R}^{n} ) \text{  and } h \text{  has compact support} \right\}$ denotes $\kappa$ times Hölder continuously differentiable functions in $\overline{\Omega}$ and $W^{s,p}(\Omega)$ is the usual Sobolev space with possibly non-integer exponent $s\geq0$. Proposition: Let $\Omega \subset \mathbb{R}^{n}$ be open, bounded and Lipschitz. Let $s   \geq 0 ,1 \leq p< \infty$ and $v \in W^{s,p} ( \Omega ) ,h \in C^{\kappa   , \lambda}_{c} ( \overline{\Omega} )$ where $\kappa \in \mathbb{N}_{0} ,   \lambda \in ( 0,1 ]$ and $\kappa + \lambda \geqslant s$ if $s \in   \mathbb{Z}$ and $\kappa + \lambda >s$ otherwise. Then the product $hu \in   W^{s,p} ( \Omega )$ and there exists a constant such that   $$ \| hu \|_{W^{s,p} ( \Omega )} \leqslant C \| u \|_{W^{s,p} ( \Omega )}. $$ Attempt at proof: The case $s \in \mathbb{N}_0$ is easy since the Sobolev norm doesn't involve the Slobodecijk seminorm and I can use that all partial derivatives of $h$ are uniformly bounded. However, for non integer $s=\lfloor s \rfloor+t, t \in (0,1)$ this seminorm does appear: $$ \| u \|_{W^{s,p} ( \Omega )} := \sum_{| \alpha | \leqslant \lfloor s \rfloor} \| D^{\alpha}  u \|^{p}_{L^{p} ( \Omega )} + \sum_{| \alpha | = \lfloor s \rfloor} \underset{\Omega \times \Omega}{\int \int} \frac{| D^{\alpha}  u ( x ) -D^{\alpha}  u ( y ) |}{| x-y |^{n+tp}} d x d y. $$ And I don't know how to handle the double integrals. Even in the simplest situation where $\lfloor s \rfloor=0$, I don't know how to bound above the integral $$ \underset{\Omega \times \Omega}{\int \int} \frac{| h ( x ) u ( x ) -h ( y ) u ( y ) |}{| x-y |^{n+tp}} d x d y. $$ I fear this might be some trivial inequality, but I just can't see it. Any help would be greatly welcome.",,"['analysis', 'sobolev-spaces', 'holder-spaces']"
68,When can we use Fubini's Theorem?,When can we use Fubini's Theorem?,,"I am using Munkres' Analysis on Manifolds textbook. Munkres defines Fubini's Theorem on rectangles and on simple regions (at least till the point that I have read). Now, according to the book, we cannot use Fubini's Theorem all the time because it is quite possible that Integral over a region exists but the iterated integral does not (because of problems with either of the single integrals), or the iterated integral exists but the function cannot be integrated over the region. From the exercises of the chapters, it seems that I need to apply Fubini's Theorem on rectifiable sets as opposed to simple regions and rectangles. My question is that if we know with certainty that the Integral exists over the region and we know that each of the single integrals in the iterated integral exist, then is it always true that the integral over the region will equal the iterated integral?","I am using Munkres' Analysis on Manifolds textbook. Munkres defines Fubini's Theorem on rectangles and on simple regions (at least till the point that I have read). Now, according to the book, we cannot use Fubini's Theorem all the time because it is quite possible that Integral over a region exists but the iterated integral does not (because of problems with either of the single integrals), or the iterated integral exists but the function cannot be integrated over the region. From the exercises of the chapters, it seems that I need to apply Fubini's Theorem on rectifiable sets as opposed to simple regions and rectangles. My question is that if we know with certainty that the Integral exists over the region and we know that each of the single integrals in the iterated integral exist, then is it always true that the integral over the region will equal the iterated integral?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
69,"Raabe's test, logarithm test, Bertrand test","Raabe's test, logarithm test, Bertrand test",,"Raabe's test, logarithm test and  Bertrand test are the  most commonly used criterion in calculus. The relationship between them is quite interesting. Here is how: $\sum\limits_{n=1}^\infty a_n$,  $a_n\gt0$, 1). If $\lim\limits_{n\to\infty}n(\dfrac{a_n}{a_{n+1}}-1)=\alpha$, then $$\lim_{n\to\infty} \frac{\ln\frac1{a_n}}{\ln n}=\alpha $$ 2). If $\lim\limits_{n\to\infty}\ln n\left[n\left(\dfrac{a_n}{a_{n+1}}-1\right)-1\right]=\beta$, then $$\lim_{n\to\infty} \frac{\ln\frac1{na_n}}{\ln\ln n}=\beta $$ I have no clue about it. I don't know how to prove it. Could anyone help me? Thanks a lot","Raabe's test, logarithm test and  Bertrand test are the  most commonly used criterion in calculus. The relationship between them is quite interesting. Here is how: $\sum\limits_{n=1}^\infty a_n$,  $a_n\gt0$, 1). If $\lim\limits_{n\to\infty}n(\dfrac{a_n}{a_{n+1}}-1)=\alpha$, then $$\lim_{n\to\infty} \frac{\ln\frac1{a_n}}{\ln n}=\alpha $$ 2). If $\lim\limits_{n\to\infty}\ln n\left[n\left(\dfrac{a_n}{a_{n+1}}-1\right)-1\right]=\beta$, then $$\lim_{n\to\infty} \frac{\ln\frac1{na_n}}{\ln\ln n}=\beta $$ I have no clue about it. I don't know how to prove it. Could anyone help me? Thanks a lot",,"['calculus', 'real-analysis', 'analysis']"
70,"Uniform continuity on $[a,b]$ and $ [b,c]$ $\implies$ uniform continuity on $[a,c]$.",Uniform continuity on  and   uniform continuity on .,"[a,b]  [b,c] \implies [a,c]","Let $f:\mathbb R \to \mathbb R$. Prove that if $f$ is uniformly continuous on $[a,b]$ and $[b,c]$, then $f$ is uniformly continuous on $[a,c]$. My attempt at a solution: I've came up with a solution but I am having doubts if it is correct, so I would like to check that: Let $\epsilon>0$, we know that there exist $\delta_1, \delta_2$ such that  if $x,y \in [a,b]  |x-y|<\delta_1 \implies |f(x)-f(y)|<\dfrac{\epsilon}{2}$ and if $x,y \in [b,c], |x-y|<\delta_2 \implies |f(x)-f(y)|<\dfrac{\epsilon}{2}$ Let $\delta=\min\{2\delta_1,2\delta_2\}$, if $|x-y|=|x-b+b-y|\leq |x-b|+|b-y|<\delta \implies |f(x)-f(y)|\leq |f(x)-f(b)|+|f(b)-f(y)|\leq \dfrac{\epsilon}{2}+\dfrac{\epsilon}{2}=\epsilon$. This proves that $f$ is uniformly continuous. I would appreciate if anyone could tell me if my proof is ok or if I've made any mistakes. (S.A. Understanding Analysis. pp 119 question 4.4.7)","Let $f:\mathbb R \to \mathbb R$. Prove that if $f$ is uniformly continuous on $[a,b]$ and $[b,c]$, then $f$ is uniformly continuous on $[a,c]$. My attempt at a solution: I've came up with a solution but I am having doubts if it is correct, so I would like to check that: Let $\epsilon>0$, we know that there exist $\delta_1, \delta_2$ such that  if $x,y \in [a,b]  |x-y|<\delta_1 \implies |f(x)-f(y)|<\dfrac{\epsilon}{2}$ and if $x,y \in [b,c], |x-y|<\delta_2 \implies |f(x)-f(y)|<\dfrac{\epsilon}{2}$ Let $\delta=\min\{2\delta_1,2\delta_2\}$, if $|x-y|=|x-b+b-y|\leq |x-b|+|b-y|<\delta \implies |f(x)-f(y)|\leq |f(x)-f(b)|+|f(b)-f(y)|\leq \dfrac{\epsilon}{2}+\dfrac{\epsilon}{2}=\epsilon$. This proves that $f$ is uniformly continuous. I would appreciate if anyone could tell me if my proof is ok or if I've made any mistakes. (S.A. Understanding Analysis. pp 119 question 4.4.7)",,"['analysis', 'uniform-continuity']"
71,equation involving the integral of the modular function of a topological group,equation involving the integral of the modular function of a topological group,,"Let $G$ be a locally compact topological group and $H$ a closed subgroup.  Choose a left Haar measure $d\zeta$ for $H$, and let $d\mu$ be any measure for $G$. Also let $f$ and $g$ be continuous compactly supported real functions on $G$. I'm stumped by a step in a proof where the following equality is asserted : $$\int_H \Delta_H(\zeta^{-1}) \Bigg[\int_G f(x\zeta^{-1})g(x)d\mu(x)\Bigg]d\zeta = \int_G g(x)\Bigg[ \int_H f(x\zeta) d\zeta\Bigg] d\mu (x).$$ I can only make the left hand side look like $$\int_G g(x) \Bigg[\int_H \Delta_H(\zeta^{-1})f(x\zeta^{-1})d\zeta \Bigg]d\mu (x)$$ Clearly I'm supposed to use the properties of the modular function $\Delta_H$, but I don't know what to do when the argument for $\Delta_H$ is the variable I'm integrating over.   Any help would be greatly appreciated. Thanks!","Let $G$ be a locally compact topological group and $H$ a closed subgroup.  Choose a left Haar measure $d\zeta$ for $H$, and let $d\mu$ be any measure for $G$. Also let $f$ and $g$ be continuous compactly supported real functions on $G$. I'm stumped by a step in a proof where the following equality is asserted : $$\int_H \Delta_H(\zeta^{-1}) \Bigg[\int_G f(x\zeta^{-1})g(x)d\mu(x)\Bigg]d\zeta = \int_G g(x)\Bigg[ \int_H f(x\zeta) d\zeta\Bigg] d\mu (x).$$ I can only make the left hand side look like $$\int_G g(x) \Bigg[\int_H \Delta_H(\zeta^{-1})f(x\zeta^{-1})d\zeta \Bigg]d\mu (x)$$ Clearly I'm supposed to use the properties of the modular function $\Delta_H$, but I don't know what to do when the argument for $\Delta_H$ is the variable I'm integrating over.   Any help would be greatly appreciated. Thanks!",,"['analysis', 'lie-groups', 'topological-groups', 'harmonic-analysis']"
72,"French metro metric: difficulty to prove that $d(x, y) = 0\iff x = y$.",French metro metric: difficulty to prove that .,"d(x, y) = 0\iff x = y","I think that it is related to the special definition of the metric in my book: $$d(x, y) = \begin{cases}||x - y||,\mbox{ if }\exists \alpha\in\mathbb{R}: \alpha x + (1-\alpha) y = 0;\\ ||x|| + ||y||, \mbox{ otherwise.}\end{cases}$$ This way, for $x = y$, we have $\alpha x + (1 - \alpha) x = 0$, which is true only if $x = 0$, so we fall into the second case: $d(x, x) = ||x|| + ||x|| = ||x||^2 \neq 0$ if $x\neq 0$. Seems like it doesn't satisfy the axioms of a metric. The case described in Woflram MathWorld is simpler, because the condition for the first case is: $x = \alpha y$. This way, for $d(x, x)$, we have $\alpha = 1$, and everything works fine! Am I missing something or is there an error in the problem statement? Thanks in advance!","I think that it is related to the special definition of the metric in my book: $$d(x, y) = \begin{cases}||x - y||,\mbox{ if }\exists \alpha\in\mathbb{R}: \alpha x + (1-\alpha) y = 0;\\ ||x|| + ||y||, \mbox{ otherwise.}\end{cases}$$ This way, for $x = y$, we have $\alpha x + (1 - \alpha) x = 0$, which is true only if $x = 0$, so we fall into the second case: $d(x, x) = ||x|| + ||x|| = ||x||^2 \neq 0$ if $x\neq 0$. Seems like it doesn't satisfy the axioms of a metric. The case described in Woflram MathWorld is simpler, because the condition for the first case is: $x = \alpha y$. This way, for $d(x, x)$, we have $\alpha = 1$, and everything works fine! Am I missing something or is there an error in the problem statement? Thanks in advance!",,['analysis']
73,"Updated: Constructing a bijection between $\left(-\frac{\pi}{2}, \frac{\pi}{2}\right]$ and $\mathbb{R}$",Updated: Constructing a bijection between  and,"\left(-\frac{\pi}{2}, \frac{\pi}{2}\right] \mathbb{R}","I am supposed to construct a bijective function for the interval: \begin{align} I_2=\left(-\frac{\pi}{2} ,\frac{\pi}{2} \right] \longrightarrow \mathbb{R} \tag{Problem} \end{align} I first tried the easier case, i.e. \begin{align}f_1:I_1=\left(-\frac{\pi}{2} ,\frac{\pi}{2} \right) &\longrightarrow \mathbb{R} \\ x& \longmapsto \tan(x) \end{align} which is a bijection. Now I know that the composition of bijective functions is still a bijection. Which means that it should be possible to 'make room' for the missing point $\pi/2$. The following function: \begin{align}\phi : \mathbb{R} &\longrightarrow \mathbb{R \setminus}\lbrace 0 \rbrace \\ x & \longmapsto \begin{cases}x+1  \ \text{if} \ x \in \mathbb{N}_0 \\x \ \text{otherwise} \end{cases}\end{align}  would be bijective, such that the composition $\phi \circ f_1: I_1 \rightarrow \mathbb{R}\setminus \lbrace 0 \rbrace  $ is bijective, and as desired it now has room for a point that I can map to. At this point I am not sure if my approach is correct because I can't find a function that would do the trick. Would I need to come up with another composition or is it enough to define a function that maps to the functions introduced above? Update (in consideration of the answers given) If I understand things correctly I can define: \begin{align}f_2: I_2 &\longrightarrow \mathbb{R} \\ x& \longmapsto \begin{cases}\phi(x) \ \text{for}  \ x \in I_2 \\0 \ \text{for}  \ x=\frac{\pi}{2} \end{cases} \end{align} Update 2 (Clarification required) . Define a new function to be equal to $\phi f_1$ over $I_1$ and have it map $π/2$ to $0$. As suggested (and upvoted) by @TBrendle. If I do understand this correctly, then I need to map $x=\frac{\pi}{2}$ to $0$. However in this case it would make no sense to me to include $I_1$ in the domain, because $\pi/2$ is not in the domain, hence I don't see why I should include it in the codomain, however if I define: \begin{align}w: I_2 &\longrightarrow \mathbb{R} \\ (\phi\circ f_1)(x) & \longmapsto \begin{cases} (\phi \circ f_1) \ \text{if} \ x \in I_2 \\ 0 \ \text{if} \ x= \frac{\pi}{2} \end{cases} \end{align} This doesn't even look like a legitimate function to me anymore, since at $x=0$ the function evaluates to both $0$ and $1$.","I am supposed to construct a bijective function for the interval: \begin{align} I_2=\left(-\frac{\pi}{2} ,\frac{\pi}{2} \right] \longrightarrow \mathbb{R} \tag{Problem} \end{align} I first tried the easier case, i.e. \begin{align}f_1:I_1=\left(-\frac{\pi}{2} ,\frac{\pi}{2} \right) &\longrightarrow \mathbb{R} \\ x& \longmapsto \tan(x) \end{align} which is a bijection. Now I know that the composition of bijective functions is still a bijection. Which means that it should be possible to 'make room' for the missing point $\pi/2$. The following function: \begin{align}\phi : \mathbb{R} &\longrightarrow \mathbb{R \setminus}\lbrace 0 \rbrace \\ x & \longmapsto \begin{cases}x+1  \ \text{if} \ x \in \mathbb{N}_0 \\x \ \text{otherwise} \end{cases}\end{align}  would be bijective, such that the composition $\phi \circ f_1: I_1 \rightarrow \mathbb{R}\setminus \lbrace 0 \rbrace  $ is bijective, and as desired it now has room for a point that I can map to. At this point I am not sure if my approach is correct because I can't find a function that would do the trick. Would I need to come up with another composition or is it enough to define a function that maps to the functions introduced above? Update (in consideration of the answers given) If I understand things correctly I can define: \begin{align}f_2: I_2 &\longrightarrow \mathbb{R} \\ x& \longmapsto \begin{cases}\phi(x) \ \text{for}  \ x \in I_2 \\0 \ \text{for}  \ x=\frac{\pi}{2} \end{cases} \end{align} Update 2 (Clarification required) . Define a new function to be equal to $\phi f_1$ over $I_1$ and have it map $π/2$ to $0$. As suggested (and upvoted) by @TBrendle. If I do understand this correctly, then I need to map $x=\frac{\pi}{2}$ to $0$. However in this case it would make no sense to me to include $I_1$ in the domain, because $\pi/2$ is not in the domain, hence I don't see why I should include it in the codomain, however if I define: \begin{align}w: I_2 &\longrightarrow \mathbb{R} \\ (\phi\circ f_1)(x) & \longmapsto \begin{cases} (\phi \circ f_1) \ \text{if} \ x \in I_2 \\ 0 \ \text{if} \ x= \frac{\pi}{2} \end{cases} \end{align} This doesn't even look like a legitimate function to me anymore, since at $x=0$ the function evaluates to both $0$ and $1$.",,['analysis']
74,"Prove that if $N$ is a null set in $\mathbb{R}^n$, then there exists a Borel null set $N'$ such that $N\subset N'$.","Prove that if  is a null set in , then there exists a Borel null set  such that .",N \mathbb{R}^n N' N\subset N',"Prove that if $N$ is a null set in $\mathbb{R}^n$, then there exists a Borel null set $N'$ such that $N\subset N'$. In fact, prove that $N'$ may be chosen to be a $G_{\delta}$, a countable intersection of open sets. So we know $\lambda(N)=0$ by definition of null set ($\lambda$ is Lebesgue measure). I think this theorem might be helpful: Suppose $A$ is a measurable set in $\mathbb{R}^n$. Then $A$ can be decomposed in the following manner: $A=E\cup N$, $E$ and $N$ are disjoint, $E$ is a Borel set, $N$ is a null set. Thank you.","Prove that if $N$ is a null set in $\mathbb{R}^n$, then there exists a Borel null set $N'$ such that $N\subset N'$. In fact, prove that $N'$ may be chosen to be a $G_{\delta}$, a countable intersection of open sets. So we know $\lambda(N)=0$ by definition of null set ($\lambda$ is Lebesgue measure). I think this theorem might be helpful: Suppose $A$ is a measurable set in $\mathbb{R}^n$. Then $A$ can be decomposed in the following manner: $A=E\cup N$, $E$ and $N$ are disjoint, $E$ is a Borel set, $N$ is a null set. Thank you.",,"['real-analysis', 'analysis', 'measure-theory', 'elementary-set-theory']"
75,Gauss hypergeometric function at z=-1,Gauss hypergeometric function at z=-1,,"is there anything like a special value case of the hypergeometric function if $z=-1$ such that one can evaluate $_2F_1(\alpha,\beta; \gamma; -1)$? I mean there is a nice representation for the case that we have $z=1$, but $z=-1$ seems to be a little bit problematic.","is there anything like a special value case of the hypergeometric function if $z=-1$ such that one can evaluate $_2F_1(\alpha,\beta; \gamma; -1)$? I mean there is a nice representation for the case that we have $z=1$, but $z=-1$ seems to be a little bit problematic.",,"['calculus', 'real-analysis']"
76,"How to show that every bounded variation function on $[a,b]$ is differentiable a.e?",How to show that every bounded variation function on  is differentiable a.e?,"[a,b]","I'm struggling with this question for over a week now. I know the proposition is true, but haven't managed to prove it yet. any suggestions anyone? ($f$ is BV on $I$ if $$\sup\left\{\sum|f(b_k)-f(a_k)| :a_{k+1}\gt b_{k}\gt a_{k} ; a_k,b_k\in I\right\}\lt \infty)$$","I'm struggling with this question for over a week now. I know the proposition is true, but haven't managed to prove it yet. any suggestions anyone? ($f$ is BV on $I$ if $$\sup\left\{\sum|f(b_k)-f(a_k)| :a_{k+1}\gt b_{k}\gt a_{k} ; a_k,b_k\in I\right\}\lt \infty)$$",,"['real-analysis', 'analysis', 'measure-theory', 'bounded-variation']"
77,Schwartz kernel theorem for induced distributions...,Schwartz kernel theorem for induced distributions...,,"I'm studying periodic pseudo-differential operators on torus and I have a question concearning the Schwartz kernel theorem: If $A:C^\infty(\mathbb T^n)\rightarrow \mathcal{D}^{'}(\mathbb T^n)$ is a continuous linear functional then by the Schwartz kernel theorem there is an unique $K_A\in \mathcal{D}^{'}(\mathbb T^{2n})$ such that, $$\langle A\varphi, \psi\rangle=\langle K_A, \psi\otimes \varphi\rangle.$$ Here $D^{'}(\mathbb T^n)$ is the set of all distributions on $\mathbb T^n$ (linear and continuous functionals on $\mathbb T^n$). In the case the distribution $Af$ is induced by $Af$ (as a mapping) in the standard way, $$\psi\mapsto \int_{\mathbb T^n}Af(x)\psi(x)\ dx,$$ then the distribution $K_A$ is also induced by a function?","I'm studying periodic pseudo-differential operators on torus and I have a question concearning the Schwartz kernel theorem: If $A:C^\infty(\mathbb T^n)\rightarrow \mathcal{D}^{'}(\mathbb T^n)$ is a continuous linear functional then by the Schwartz kernel theorem there is an unique $K_A\in \mathcal{D}^{'}(\mathbb T^{2n})$ such that, $$\langle A\varphi, \psi\rangle=\langle K_A, \psi\otimes \varphi\rangle.$$ Here $D^{'}(\mathbb T^n)$ is the set of all distributions on $\mathbb T^n$ (linear and continuous functionals on $\mathbb T^n$). In the case the distribution $Af$ is induced by $Af$ (as a mapping) in the standard way, $$\psi\mapsto \int_{\mathbb T^n}Af(x)\psi(x)\ dx,$$ then the distribution $K_A$ is also induced by a function?",,"['analysis', 'partial-differential-equations', 'fourier-analysis']"
78,When does an analytic function grow faster than a polynomial?,When does an analytic function grow faster than a polynomial?,,"Suppose $f$ is an analytic function with power series expansion $f(z)=\sum_{n=0}^{\infty} a_nz^n$, and $p = \sum_{n=0}^{d}b_nz^n$ is a polynomial. If $f$ is a polynomial of degree larger than $d$, then $|f|$ grows faster than $|p|$, but the situation is not so clear when the expansion of $f$ has infinitely many nonzero coefficients. I would expect the growth of the function $f$ then to be faster than that of $p$, as with the function $e^z = \sum_{n=0}^{\infty}\frac{z^n}{n!}$. However the function $\frac{1}{1-z} = \sum_{n=0}^{\infty}z^n$ also has infinitely many nonzero coefficients and grows slower than any polynomial (as $|z|\to\infty$). I realize this is related to the failure of the power series to converge outside a disk of radius $1$. Also, $log(z)$ grows slower than any polynomial, but any power series representation cannot converge on an infinite radius (The function itself cannot be well-defined everywhere in the complex plane simultaneously). Under what conditions can we say that a power series with infinitely many nonzero coefficients represents a function that grows faster than any polynomial? Is this true for any power series with infinite radius of convergence? Are there such power series which grow at the rate $z^\alpha$, for any $\alpha\in(0,\infty)$? I have in mind the case where $f$ is complex-analytic, but I would also be interested to hear about the case where $f$ is real-analytic, if the cases differ.","Suppose $f$ is an analytic function with power series expansion $f(z)=\sum_{n=0}^{\infty} a_nz^n$, and $p = \sum_{n=0}^{d}b_nz^n$ is a polynomial. If $f$ is a polynomial of degree larger than $d$, then $|f|$ grows faster than $|p|$, but the situation is not so clear when the expansion of $f$ has infinitely many nonzero coefficients. I would expect the growth of the function $f$ then to be faster than that of $p$, as with the function $e^z = \sum_{n=0}^{\infty}\frac{z^n}{n!}$. However the function $\frac{1}{1-z} = \sum_{n=0}^{\infty}z^n$ also has infinitely many nonzero coefficients and grows slower than any polynomial (as $|z|\to\infty$). I realize this is related to the failure of the power series to converge outside a disk of radius $1$. Also, $log(z)$ grows slower than any polynomial, but any power series representation cannot converge on an infinite radius (The function itself cannot be well-defined everywhere in the complex plane simultaneously). Under what conditions can we say that a power series with infinitely many nonzero coefficients represents a function that grows faster than any polynomial? Is this true for any power series with infinite radius of convergence? Are there such power series which grow at the rate $z^\alpha$, for any $\alpha\in(0,\infty)$? I have in mind the case where $f$ is complex-analytic, but I would also be interested to hear about the case where $f$ is real-analytic, if the cases differ.",,"['analysis', 'polynomials', 'power-series']"
79,why $\frac{d}{dy}$ can pass through integral w.r.t. $x$?,why  can pass through integral w.r.t. ?,\frac{d}{dy} x,"When I calculate integration of multivariables, many books use the following step without proofing. I want to know that why it is true: $$\frac{d}{dy}\left[\int^a_b f(x,y)dx\right]_{y=k}=\int^a_b \frac{\partial}{\partial y} \left[f(x,y)\right]_{y=k}dx$$ I also wonder that whether it is true when the integral or differentiation become indefinite. Which is : $$\frac{d}{dy}\int f(x,y)dx=\int\frac{\partial}{\partial y} f(x,y)dx$$ $$\frac{d}{dy}\int^a_b f(x,y)dx=\int^a_b \frac{\partial}{\partial y} f(x,y)dx$$ $$\frac{d}{dy}\left[\int f(x,y)dx\right]_{y=k}=\int \frac{\partial}{\partial y} \left[f(x,y)\right]_{y=k}dx$$","When I calculate integration of multivariables, many books use the following step without proofing. I want to know that why it is true: $$\frac{d}{dy}\left[\int^a_b f(x,y)dx\right]_{y=k}=\int^a_b \frac{\partial}{\partial y} \left[f(x,y)\right]_{y=k}dx$$ I also wonder that whether it is true when the integral or differentiation become indefinite. Which is : $$\frac{d}{dy}\int f(x,y)dx=\int\frac{\partial}{\partial y} f(x,y)dx$$ $$\frac{d}{dy}\int^a_b f(x,y)dx=\int^a_b \frac{\partial}{\partial y} f(x,y)dx$$ $$\frac{d}{dy}\left[\int f(x,y)dx\right]_{y=k}=\int \frac{\partial}{\partial y} \left[f(x,y)\right]_{y=k}dx$$",,"['calculus', 'analysis', 'multivariable-calculus']"
80,"Second derivative Dirac delta distribution times $(x-a)^2$, intepretation","Second derivative Dirac delta distribution times , intepretation",(x-a)^2,"I'm not sure if this calculation is correct and if I interpret it correctly (from old exam). Show that $ (x-a)^2 \delta ''_a = 2 \delta _a $. We have for distributions $f$ and test functions $\varphi$ that $\langle\delta_a,\varphi\rangle = \varphi(a)$ and $\langle f', \varphi\rangle = -\langle f, \varphi'\rangle$. My calculations are as follows: Let $y = x-a$ and note that $y'=1$, then it follows that $$\begin{align} \langle y^2\delta_a'',\varphi\rangle & = \langle \delta_a'',y^2\varphi\rangle \\ & =-\langle\delta_a',(y^2\varphi)'\rangle \\ & = -\langle\delta_a',2 y\varphi+y^2\varphi' \rangle \\ & =\langle\delta_a,(2 y\varphi+y^2\varphi')'\rangle \\ & = \langle\delta_a,2\varphi+2y\varphi' +2y\varphi'+y^2\varphi''\rangle \\ & =2\varphi(a) \\ & =2\langle\delta_a,\varphi\rangle \end{align}$$ Any comments on the calculations? So I guess that this shows that given the function $y=x-a$, a new distribution can be formed by $y^2 \delta_a''$ which maps test functions $\varphi$ to $2\varphi(a)$, and the distribution $y^2 \delta_a''$ is equal to $2\delta_a$ since they map test functions to the same value.","I'm not sure if this calculation is correct and if I interpret it correctly (from old exam). Show that $ (x-a)^2 \delta ''_a = 2 \delta _a $. We have for distributions $f$ and test functions $\varphi$ that $\langle\delta_a,\varphi\rangle = \varphi(a)$ and $\langle f', \varphi\rangle = -\langle f, \varphi'\rangle$. My calculations are as follows: Let $y = x-a$ and note that $y'=1$, then it follows that $$\begin{align} \langle y^2\delta_a'',\varphi\rangle & = \langle \delta_a'',y^2\varphi\rangle \\ & =-\langle\delta_a',(y^2\varphi)'\rangle \\ & = -\langle\delta_a',2 y\varphi+y^2\varphi' \rangle \\ & =\langle\delta_a,(2 y\varphi+y^2\varphi')'\rangle \\ & = \langle\delta_a,2\varphi+2y\varphi' +2y\varphi'+y^2\varphi''\rangle \\ & =2\varphi(a) \\ & =2\langle\delta_a,\varphi\rangle \end{align}$$ Any comments on the calculations? So I guess that this shows that given the function $y=x-a$, a new distribution can be formed by $y^2 \delta_a''$ which maps test functions $\varphi$ to $2\varphi(a)$, and the distribution $y^2 \delta_a''$ is equal to $2\delta_a$ since they map test functions to the same value.",,"['analysis', 'distribution-theory', 'weak-derivatives']"
81,Prove that $\lim_{N\rightarrow\infty}(1/N)\sum_{n=1}^N f(nx)=\int_{0}^1f(t)dt$,Prove that,\lim_{N\rightarrow\infty}(1/N)\sum_{n=1}^N f(nx)=\int_{0}^1f(t)dt,"Suppose $f$ is continuous and periodic on the reals with period 1. Prove that if $x\in[0,1]$ is an irrational number, then $$\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^N f(nx)=\int_{0}^1f(t)dt$$ Suggestion: First consider $f(t) = e^{2\pi(ikt)}$ where k is an integer. I can see that this is a limit of a weighted average, but the suggestion throws me off. I've seen the suggestion in fourier transforms but it's not clicking at the moment. Any help would be welcome.","Suppose $f$ is continuous and periodic on the reals with period 1. Prove that if $x\in[0,1]$ is an irrational number, then $$\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^N f(nx)=\int_{0}^1f(t)dt$$ Suggestion: First consider $f(t) = e^{2\pi(ikt)}$ where k is an integer. I can see that this is a limit of a weighted average, but the suggestion throws me off. I've seen the suggestion in fourier transforms but it's not clicking at the moment. Any help would be welcome.",,"['real-analysis', 'analysis', 'equidistribution']"
82,Class $C^{- \infty }$ functions?,Class  functions?,C^{- \infty },"If my understanding is correct, a class $C^{-1}$ function (in terms of smoothness, of course) can be thought of as a function which integrates to a class $C^{0}$ function. And when we differentiate (in the appropriate sense, of course) it, we can construct a class $C^{-2}$ function. An example would be these three functions, ordered from highest to lowest class: $$f(x) = |x|$$ $$f(x) = \theta (x)$$ $$f(x) = \delta (x)$$ (those are Heaviside step and Dirac delta function) . A natural question that comes to my mind is, is there such a thing as a $C^{- \infty } - smooth$ function? What about discontinuous-everywhere fuctions? I have some ideas, but I'm not sure what to think of them, so I'd appreciate some constructive answers if they really do exist.","If my understanding is correct, a class $C^{-1}$ function (in terms of smoothness, of course) can be thought of as a function which integrates to a class $C^{0}$ function. And when we differentiate (in the appropriate sense, of course) it, we can construct a class $C^{-2}$ function. An example would be these three functions, ordered from highest to lowest class: $$f(x) = |x|$$ $$f(x) = \theta (x)$$ $$f(x) = \delta (x)$$ (those are Heaviside step and Dirac delta function) . A natural question that comes to my mind is, is there such a thing as a $C^{- \infty } - smooth$ function? What about discontinuous-everywhere fuctions? I have some ideas, but I'm not sure what to think of them, so I'd appreciate some constructive answers if they really do exist.",,['analysis']
83,Weak solution of a non-linear problem with Lipschitz functions,Weak solution of a non-linear problem with Lipschitz functions,,"I'm trying to solve the problem 9.5 in Evans PDE book. The statement goes as follows: Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a $\Lambda$-lipschitz bounded function with $f(0)=0$ and $f'(0)>\lambda_1$, where $\lambda_1$ is the first eigenvalue of $-\Delta$ on $H_0^1(\Omega)$. We consider the problem $-\Delta u=f(u)$ in $\Omega$, $u=0$ on $\partial\Omega$ and $u>0$ in $\Omega$.  I have to use the method of sub-supersolutions to solve the problem (find a weak solution). My idea is taking as a supersolution the function vanishing on the boundary such that $-\Delta u=M$, where $M$ is a sufficiently big constant. Using the maximum principle this function is positive. The subsolution should be something like the first eigenfunction, which is positive. The problem is that I can't show that it's a subsolution. It's evident that $\lambda_1<\Lambda$, but the inequalities go in the wrong way in order to show that the first eigenfunction is a subsolution. I can't use Taylor expansion with this Lipschitz function, I only know that $|f(z)|\leq\Lambda|z|$. Any ideas will be appreciated. Thanks","I'm trying to solve the problem 9.5 in Evans PDE book. The statement goes as follows: Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a $\Lambda$-lipschitz bounded function with $f(0)=0$ and $f'(0)>\lambda_1$, where $\lambda_1$ is the first eigenvalue of $-\Delta$ on $H_0^1(\Omega)$. We consider the problem $-\Delta u=f(u)$ in $\Omega$, $u=0$ on $\partial\Omega$ and $u>0$ in $\Omega$.  I have to use the method of sub-supersolutions to solve the problem (find a weak solution). My idea is taking as a supersolution the function vanishing on the boundary such that $-\Delta u=M$, where $M$ is a sufficiently big constant. Using the maximum principle this function is positive. The subsolution should be something like the first eigenfunction, which is positive. The problem is that I can't show that it's a subsolution. It's evident that $\lambda_1<\Lambda$, but the inequalities go in the wrong way in order to show that the first eigenfunction is a subsolution. I can't use Taylor expansion with this Lipschitz function, I only know that $|f(z)|\leq\Lambda|z|$. Any ideas will be appreciated. Thanks",,"['analysis', 'partial-differential-equations']"
84,Find all continuous functions (did I do this correctly?),Find all continuous functions (did I do this correctly?),,"Find all the continuous functions $f:\mathbb{R}\to\mathbb{R}$ satisfying: $f(x+y)=f(x)+f(y)+f(x)f(y)$ for all $x,y\in\mathbb{R}$ Solution attempt: $$\begin{align*} f(x+y) + 1 &= f(x) + f(y) + f(x)f(y) + 1\\ f(x+y) + 1 &= (f(x) + 1)(f(y) + 1) \end{align*}$$ $g(x) = f(x) + 1$, and $g(y) = f(y) + 1$ $$\begin{align*} g(x+y) &= g(x)g(y)\\ (x,y) &= (x,x)\\ g(2x) &= g^2(x)\\ g(x) &= g^2(x/2)\\ g(x) &= c^x,\;\text{ where }c = e^a \end{align*}$$ So $f(x) = c^x - 1$ for all x","Find all the continuous functions $f:\mathbb{R}\to\mathbb{R}$ satisfying: $f(x+y)=f(x)+f(y)+f(x)f(y)$ for all $x,y\in\mathbb{R}$ Solution attempt: $$\begin{align*} f(x+y) + 1 &= f(x) + f(y) + f(x)f(y) + 1\\ f(x+y) + 1 &= (f(x) + 1)(f(y) + 1) \end{align*}$$ $g(x) = f(x) + 1$, and $g(y) = f(y) + 1$ $$\begin{align*} g(x+y) &= g(x)g(y)\\ (x,y) &= (x,x)\\ g(2x) &= g^2(x)\\ g(x) &= g^2(x/2)\\ g(x) &= c^x,\;\text{ where }c = e^a \end{align*}$$ So $f(x) = c^x - 1$ for all x",,"['real-analysis', 'analysis']"
85,Definition of Analytic functions,Definition of Analytic functions,,"The Analytic functions are defined as $f$ is analytic on the domain $D$ if for any $x_0$ in $D$, one can write $$f(x)=\sum_{n=0}^{\infty}a_n(x-x_0)^n$$ and the series is convergent to $f(x)$ for $x$ in the neighbourhood of $x_0$. Equivalently, it's Taylor Series: $$f(x)=\sum_{n=0}^{\infty}\frac{f^n(x_0)}{n!}(x-x_0)^n$$ converges to $f(x)$ for $x$ in the neighbourhood of $x_0$ (in the mean square sense). This is more of copy from Wikipedia . I have a few trivial questions Q. First is it different from saying, a function is analytic if it can be well approximated with Taylor Series (I mean is not for some $x$ in the neighborhood of $x_0$ redundant)? Q. Second, what does the note in bracket ""(in the mean square sense)"" mean? Is mean square sense=reference to euclidean metric? Q. From this definition how can I conclude that $e^{(-1/z)},z\in \mathbb{C}$ is not analytic? Edit Let's define $z \neq 0$ . Then is it analytic or not?","The Analytic functions are defined as $f$ is analytic on the domain $D$ if for any $x_0$ in $D$, one can write $$f(x)=\sum_{n=0}^{\infty}a_n(x-x_0)^n$$ and the series is convergent to $f(x)$ for $x$ in the neighbourhood of $x_0$. Equivalently, it's Taylor Series: $$f(x)=\sum_{n=0}^{\infty}\frac{f^n(x_0)}{n!}(x-x_0)^n$$ converges to $f(x)$ for $x$ in the neighbourhood of $x_0$ (in the mean square sense). This is more of copy from Wikipedia . I have a few trivial questions Q. First is it different from saying, a function is analytic if it can be well approximated with Taylor Series (I mean is not for some $x$ in the neighborhood of $x_0$ redundant)? Q. Second, what does the note in bracket ""(in the mean square sense)"" mean? Is mean square sense=reference to euclidean metric? Q. From this definition how can I conclude that $e^{(-1/z)},z\in \mathbb{C}$ is not analytic? Edit Let's define $z \neq 0$ . Then is it analytic or not?",,"['real-analysis', 'analysis']"
86,Application of the Intermediate Value Theorem,Application of the Intermediate Value Theorem,,"Suppose $f: [1,2] \to [5,7]$ is continuous.  Show that $f(c)=2c+3$ for some $c \in [1,2]$. First note $f(1)=5$ and $f(2)=7$.  By the IVT, all values $c \in [1,2]$ are hit.  I'm just wondering how to put all of these facts together to arrive at $f(c)=2c+3$ for all $c \in [1,2]$.","Suppose $f: [1,2] \to [5,7]$ is continuous.  Show that $f(c)=2c+3$ for some $c \in [1,2]$. First note $f(1)=5$ and $f(2)=7$.  By the IVT, all values $c \in [1,2]$ are hit.  I'm just wondering how to put all of these facts together to arrive at $f(c)=2c+3$ for all $c \in [1,2]$.",,['analysis']
87,How to prove $r^2=2$ ? (Dedekind's cut),How to prove  ? (Dedekind's cut),r^2=2,"Let a (Dedekind) cut $r=\{p \in \mathbb{Q} :p^2<2 \text{ or } p<0\}$ and a cut $2^*=\{t\in \mathbb{Q} : t<2\}$. I want to prove $r^2=2^*$. I could show that $r^2 \subset 2^*$ easily, but I couldn't show that $2^* \subset r^2$. How to show that there is $p$, $p' \in r$ such that $t \leq  pp' <2$ or $t \leq p^2<2$?","Let a (Dedekind) cut $r=\{p \in \mathbb{Q} :p^2<2 \text{ or } p<0\}$ and a cut $2^*=\{t\in \mathbb{Q} : t<2\}$. I want to prove $r^2=2^*$. I could show that $r^2 \subset 2^*$ easily, but I couldn't show that $2^* \subset r^2$. How to show that there is $p$, $p' \in r$ such that $t \leq  pp' <2$ or $t \leq p^2<2$?",,"['real-analysis', 'analysis']"
88,Show that exists a constant $c>0$ such that: $\|f(x)\|\geq c\|x\| \forall x \in \mathbb{R^n}$,Show that exists a constant  such that:,c>0 \|f(x)\|\geq c\|x\| \forall x \in \mathbb{R^n},"So , this is the exercise: Let $f$ be a linear transformation and injective in $\mathbb {R^n}\rightarrow\mathbb{R^m}$. For abuse , let's denote by $\|.\|$ the norm in both sides.Show that exists a constant $c>0$ such that: $$\|f(x)\|\geq c\|x\| \forall x \in \mathbb{R^n}$$ Someone explainded to me how to do this one, but honestly I didn't understand a thing...If anyone can help me how to get starded , Much appreciated!","So , this is the exercise: Let $f$ be a linear transformation and injective in $\mathbb {R^n}\rightarrow\mathbb{R^m}$. For abuse , let's denote by $\|.\|$ the norm in both sides.Show that exists a constant $c>0$ such that: $$\|f(x)\|\geq c\|x\| \forall x \in \mathbb{R^n}$$ Someone explainded to me how to do this one, but honestly I didn't understand a thing...If anyone can help me how to get starded , Much appreciated!",,['analysis']
89,A sufficient condition for a function to be of class $C^2$ in the weak sense.,A sufficient condition for a function to be of class  in the weak sense.,C^2,Let $f\colon\mathbb{R}\to\mathbb{R}$ be a continuous function with weak derivative (i.e. the derivative in the sense of distribution) in $C^1(\mathbb{R})$. Does this condition imply that $f$ is two times continuously differentiable (i.e. $f\in C^2(\mathbb{R}))$?,Let $f\colon\mathbb{R}\to\mathbb{R}$ be a continuous function with weak derivative (i.e. the derivative in the sense of distribution) in $C^1(\mathbb{R})$. Does this condition imply that $f$ is two times continuously differentiable (i.e. $f\in C^2(\mathbb{R}))$?,,"['real-analysis', 'analysis', 'distribution-theory']"
90,There exist a degree $n+1$ polynomial of best approximation if there exist a degree $n$ polynomial of best approximation,There exist a degree  polynomial of best approximation if there exist a degree  polynomial of best approximation,n+1 n,"This is a problem from Mathematical Analysis by Zorich, from the chapter on continuous functions. Definition: Let $P_n$ be a polynomial of degree $n$. For a function $f:[a,b]\to\mathbb{R}$, Let $\Delta(P_n) = \sup_{x\in[a,b]} |f(x)-P_n(x)|$. and $E_n(f) = \inf_{P_n} \Delta(P_n)$. A polynomial $P_n$ is the best approximation of degree $n$ of $f$ is $\Delta(P_n) = E_n(f)$. I have already proved the following: There exist a polynomial $P_0(x) = a$ of best approximation of degree 0. If $Q_\lambda(x) = \lambda P_n(x)$ for some fixed polynomial $P_n$. Then there exist a polynomial $Q_{\lambda_0}$ such that $\Delta(Q_{\lambda_0}) = \min_{\lambda\in \mathbb{R}} \Delta(Q_\lambda)$ I'm stuck on proving the following: If there exists a polynomial of best approximation of degree $n$, there also exists a polynomial of best approximation of degree $n+1$. My intuition is to prove $\lambda_0 x^{n+1}+P_n$ is the $n+1$ best approximation for $f$. Where $\Delta(\lambda_0 x^{n+1}) =  \min_{\lambda\in \mathbb{R}} \Delta(\lambda x^{n+1})$ and $\Delta(P_n) = E(f(x) - \lambda_0 x^{n+1})$. I don't know if this approach is right. I'm stuck on how to proceed.","This is a problem from Mathematical Analysis by Zorich, from the chapter on continuous functions. Definition: Let $P_n$ be a polynomial of degree $n$. For a function $f:[a,b]\to\mathbb{R}$, Let $\Delta(P_n) = \sup_{x\in[a,b]} |f(x)-P_n(x)|$. and $E_n(f) = \inf_{P_n} \Delta(P_n)$. A polynomial $P_n$ is the best approximation of degree $n$ of $f$ is $\Delta(P_n) = E_n(f)$. I have already proved the following: There exist a polynomial $P_0(x) = a$ of best approximation of degree 0. If $Q_\lambda(x) = \lambda P_n(x)$ for some fixed polynomial $P_n$. Then there exist a polynomial $Q_{\lambda_0}$ such that $\Delta(Q_{\lambda_0}) = \min_{\lambda\in \mathbb{R}} \Delta(Q_\lambda)$ I'm stuck on proving the following: If there exists a polynomial of best approximation of degree $n$, there also exists a polynomial of best approximation of degree $n+1$. My intuition is to prove $\lambda_0 x^{n+1}+P_n$ is the $n+1$ best approximation for $f$. Where $\Delta(\lambda_0 x^{n+1}) =  \min_{\lambda\in \mathbb{R}} \Delta(\lambda x^{n+1})$ and $\Delta(P_n) = E(f(x) - \lambda_0 x^{n+1})$. I don't know if this approach is right. I'm stuck on how to proceed.",,"['analysis', 'polynomials']"
91,A hypothesis: Integral of $f^n(x)$ is monotonic as $n$ reaches infinity.,A hypothesis: Integral of  is monotonic as  reaches infinity.,f^n(x) n,"When I'm solving some analysis questions I discovered some interesting phenomenons. I can't prove it or give any counterexamples so I'm seeking help. If $f(x)$ is a continuous, monotonic, non-negative function on $[a,b]$ , then sequence $$\left\{\int_a^bf^n(x)\mathrm{d}x\right\}_{n=1}^{\infty}$$ is monotonic, or at least starting from some $N\in\mathbb{N}$ it is. If someone can prove it or give a counterexample. Thanks! ( $f^n(x)$ means $[f(x)]^n$ .)","When I'm solving some analysis questions I discovered some interesting phenomenons. I can't prove it or give any counterexamples so I'm seeking help. If is a continuous, monotonic, non-negative function on , then sequence is monotonic, or at least starting from some it is. If someone can prove it or give a counterexample. Thanks! ( means .)","f(x) [a,b] \left\{\int_a^bf^n(x)\mathrm{d}x\right\}_{n=1}^{\infty} N\in\mathbb{N} f^n(x) [f(x)]^n","['real-analysis', 'calculus', 'analysis']"
92,Herstein's proof of transcendence of $e$,Herstein's proof of transcendence of,e,"This question concerns Herstein's proof (in Topics in Algebra ) that $e$ is transcendental. If you have the book, it's Theorem 5.2.1, page 218, or there's a transcription here: https://sites.math.washington.edu/~palmieri/Courses/2005/Math403/transcendental.pdf . In the final step, he goes from $$\epsilon_i = \frac{-ie^{i\left(1-\theta_i\right)}\left(i\theta_i\right)^{p-1}\left(1-i\theta_i\right)^p \dotsm \left(n-i\theta_i\right)^p}{(p-1)!}$$ to $$\lvert\epsilon_i\rvert \leq \frac{e^n n^p \left(n!\right)^p}{(p-1)!}$$ I can see where most of this estimate comes from, but am having difficulty with the $\left(n!\right)^p$ part. This seems to assume that $$\left\lvert\left(1-i\theta_i\right) \dotsm \left(n-i\theta_i\right)\right\rvert \leq n! \qquad (*)$$ but some of the terms inside the absolute value will be negative (the $\theta_i$ are between 0 and 1, but $i$ can range from $1$ to $n$ ), so we can't just multiply the inequalities $1-i\theta_i < 1$ , $2-i\theta_i < 2$ , etc. So can anyone clarify how the inequality $(*)$ arises? Thanks very much in advance for assistance.","This question concerns Herstein's proof (in Topics in Algebra ) that is transcendental. If you have the book, it's Theorem 5.2.1, page 218, or there's a transcription here: https://sites.math.washington.edu/~palmieri/Courses/2005/Math403/transcendental.pdf . In the final step, he goes from to I can see where most of this estimate comes from, but am having difficulty with the part. This seems to assume that but some of the terms inside the absolute value will be negative (the are between 0 and 1, but can range from to ), so we can't just multiply the inequalities , , etc. So can anyone clarify how the inequality arises? Thanks very much in advance for assistance.",e \epsilon_i = \frac{-ie^{i\left(1-\theta_i\right)}\left(i\theta_i\right)^{p-1}\left(1-i\theta_i\right)^p \dotsm \left(n-i\theta_i\right)^p}{(p-1)!} \lvert\epsilon_i\rvert \leq \frac{e^n n^p \left(n!\right)^p}{(p-1)!} \left(n!\right)^p \left\lvert\left(1-i\theta_i\right) \dotsm \left(n-i\theta_i\right)\right\rvert \leq n! \qquad (*) \theta_i i 1 n 1-i\theta_i < 1 2-i\theta_i < 2 (*),"['real-analysis', 'analysis', 'number-theory', 'inequality', 'transcendental-numbers']"
93,Are the solutions of a system of real polynomial equations continuous in the coefficients?,Are the solutions of a system of real polynomial equations continuous in the coefficients?,,"Let $f_1(x,c_1),\ldots,f_n(x,c_n)$ be $n$ real polynomials in $n$ variables $x=(x_1,\ldots,x_n)$ of degree at most $d$ with coefficients $c=(c_1,\ldots,c_n)$ . Thus, for each $i=1,\ldots,n$ , we have $c_i\in\mathbb{R}^{{{n+d+1}\choose{d}}} $ . Let $$ \Gamma(c)=\{x\in\mathbb{R}^n\mid f_1(x,c_1)=0,\ldots,f_n(x,c_n)=0\} $$ denote the set of real solutions of the given system of polynomial equations with coefficients $c=(c_1,\ldots,c_n)$ . Moreover, define $$ C=\left\{c\in\mathbb{R}^{n\times{{n+d+1}\choose{d}}} \ \middle|\ \Gamma(c)\neq \emptyset\right\} $$ to be the set of coefficients for which there exists a real solution to the associated system of equations. Let us understand the solution set $\Gamma(c)$ as a set-valued function $\Gamma: C \to 2^{\mathbb{R}^n}$ over $C$ . A (single-valued) continuous function $\gamma:C \to\mathbb{R^n}$ is said to be a continuous selection of the set-valued function $\Gamma: C \to 2^{\mathbb{R}^n}$ if $\gamma(c)\in\Gamma(c)$ for all $c\in C$ . Question. Suppose $C$ is ``nice'', say open and connected (if more topological structure is needed, please feel free to assume so). Does $\Gamma$ admit a continuous selection? (A reference is much appreciated.) Thoughts: Of course, if there is the same number of real solutions over $C$ , then this follows from an implicit function theorem. However, if solution paths intersect or the polynomials are not in general position, then I am not sure how to formally proceed. Related questions are here and here , but they only concern a single polynomial and the question is not quite the same.","Let be real polynomials in variables of degree at most with coefficients . Thus, for each , we have . Let denote the set of real solutions of the given system of polynomial equations with coefficients . Moreover, define to be the set of coefficients for which there exists a real solution to the associated system of equations. Let us understand the solution set as a set-valued function over . A (single-valued) continuous function is said to be a continuous selection of the set-valued function if for all . Question. Suppose is ``nice'', say open and connected (if more topological structure is needed, please feel free to assume so). Does admit a continuous selection? (A reference is much appreciated.) Thoughts: Of course, if there is the same number of real solutions over , then this follows from an implicit function theorem. However, if solution paths intersect or the polynomials are not in general position, then I am not sure how to formally proceed. Related questions are here and here , but they only concern a single polynomial and the question is not quite the same.","f_1(x,c_1),\ldots,f_n(x,c_n) n n x=(x_1,\ldots,x_n) d c=(c_1,\ldots,c_n) i=1,\ldots,n c_i\in\mathbb{R}^{{{n+d+1}\choose{d}}}  
\Gamma(c)=\{x\in\mathbb{R}^n\mid f_1(x,c_1)=0,\ldots,f_n(x,c_n)=0\}
 c=(c_1,\ldots,c_n) 
C=\left\{c\in\mathbb{R}^{n\times{{n+d+1}\choose{d}}} \ \middle|\ \Gamma(c)\neq \emptyset\right\}
 \Gamma(c) \Gamma: C \to 2^{\mathbb{R}^n} C \gamma:C \to\mathbb{R^n} \Gamma: C \to 2^{\mathbb{R}^n} \gamma(c)\in\Gamma(c) c\in C C \Gamma C","['analysis', 'algebraic-geometry', 'polynomials', 'systems-of-equations']"
94,Show that a set $A \subset \mathbb{R}^2$ of positive Lebesgue measure contains the vertices of an equilateral triangle,Show that a set  of positive Lebesgue measure contains the vertices of an equilateral triangle,A \subset \mathbb{R}^2,"I have the following problem for which I'm trying to figure out a solution, and I'm a bit stuck. Any hints or insights would be appreciated. Suppose $A \subseteq \mathbb{R}^2$ is a Lebesgue measurable set of positive Lebesgue measures. Show that it contains the vertices of an equilateral triangle. Here is what I've done so far. First, we can write $$\displaystyle A =\bigcup_{i = 1}^\infty \left(A \cap ((B(0, i) \backslash B(0, i-1))\right)$$ where this is a disjoint union. If, for all $i$ , $$m\left(A \cap ((B(0, i) \backslash  B(0, i-1))\right) = 0$$ then $m(A) = 0$ , which is a contradiction. Hence, we can take $A$ to be bounded. Furthermore, since, by the regularity of measure, we can approximate $A$ by a closed subset $F$ such that $m(A \backslash F) < \epsilon$ for any $\epsilon > 0$ , it suffices to show that $F$ contains the vertices of an equilateral triangle. Hence, we can take our set to be compact. The above so far I believe to be correct. What follows is the rest of my thought process, which I don't believe is correct, but I wonder if anyone has any idea how to fix the gap in it. Since $F$ is compact, we can cover it with finitely many balls of radius $\epsilon$ , so we can write $$F = \bigcup_{i = 1}^n (F \cap B(x_i, \epsilon))$$ Hence, by the countable subadditivity of measure, we have $$m(F) \leq \sum_{i = 1}^n m(F \cap B(x_i, \epsilon))$$ Dividing by the measure of a ball of radius $\epsilon$ , we have $$\frac{m(F)}{\pi \epsilon^2} \leq \sum_{i = 1}^n \frac{m(F \cap B(x_i, \epsilon)}{m(B(x_i, \epsilon))}$$ Now, by Lebesgue's density theorem, taking the limit as $\epsilon \to 0$ , the right hand side is finite, which implies that $m(F) = 0$ , a contradiction. Now, one problem with this is that $n$ actually depends on $\epsilon$ , so we can't actually move the limit inside the finite sum. I do think solving this problem is supposed to make use of Lebesgue's density theorem though. Any hints or insights would be appreciated! Thank you!","I have the following problem for which I'm trying to figure out a solution, and I'm a bit stuck. Any hints or insights would be appreciated. Suppose is a Lebesgue measurable set of positive Lebesgue measures. Show that it contains the vertices of an equilateral triangle. Here is what I've done so far. First, we can write where this is a disjoint union. If, for all , then , which is a contradiction. Hence, we can take to be bounded. Furthermore, since, by the regularity of measure, we can approximate by a closed subset such that for any , it suffices to show that contains the vertices of an equilateral triangle. Hence, we can take our set to be compact. The above so far I believe to be correct. What follows is the rest of my thought process, which I don't believe is correct, but I wonder if anyone has any idea how to fix the gap in it. Since is compact, we can cover it with finitely many balls of radius , so we can write Hence, by the countable subadditivity of measure, we have Dividing by the measure of a ball of radius , we have Now, by Lebesgue's density theorem, taking the limit as , the right hand side is finite, which implies that , a contradiction. Now, one problem with this is that actually depends on , so we can't actually move the limit inside the finite sum. I do think solving this problem is supposed to make use of Lebesgue's density theorem though. Any hints or insights would be appreciated! Thank you!","A \subseteq \mathbb{R}^2 \displaystyle A =\bigcup_{i = 1}^\infty \left(A \cap ((B(0, i) \backslash B(0, i-1))\right) i m\left(A \cap ((B(0, i) \backslash  B(0, i-1))\right) = 0 m(A) = 0 A A F m(A \backslash F) < \epsilon \epsilon > 0 F F \epsilon F = \bigcup_{i = 1}^n (F \cap B(x_i, \epsilon)) m(F) \leq \sum_{i = 1}^n m(F \cap B(x_i, \epsilon)) \epsilon \frac{m(F)}{\pi \epsilon^2} \leq \sum_{i = 1}^n \frac{m(F \cap B(x_i, \epsilon)}{m(B(x_i, \epsilon))} \epsilon \to 0 m(F) = 0 n \epsilon","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
95,Prove that function is in $L^1$,Prove that function is in,L^1,"Let $E \subset \mathbb{R}$ be a measurable subset. Assume that $\int_{E} |x|^{1/4} |f(x)|^2 dx < \infty$ and $\int_{E} x^4 |f(x)|^3 dx < \infty$ , then I want to prove $f \in L^1 (E)$ . Morally the first inequality says that $f(x)$ behaves nicely at $0$ and the second inequality says that $f(x)$ behaves nicely at $\pm \infty$ but I'm struggling to prove the statement rigorously. A possible idea is to use Holder's inequality: we know that $f(x) x^{1/8} \in L^2 (E)$ and $f(x) x^{4/3} \in L^3 (E)$ and probably it can be used somehow but I don't know how. Anyways, any ideas are greatly appreciated!","Let be a measurable subset. Assume that and , then I want to prove . Morally the first inequality says that behaves nicely at and the second inequality says that behaves nicely at but I'm struggling to prove the statement rigorously. A possible idea is to use Holder's inequality: we know that and and probably it can be used somehow but I don't know how. Anyways, any ideas are greatly appreciated!",E \subset \mathbb{R} \int_{E} |x|^{1/4} |f(x)|^2 dx < \infty \int_{E} x^4 |f(x)|^3 dx < \infty f \in L^1 (E) f(x) 0 f(x) \pm \infty f(x) x^{1/8} \in L^2 (E) f(x) x^{4/3} \in L^3 (E),"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
96,$\delta$- hausdorff measure of open ball,- hausdorff measure of open ball,\delta,"Let U open ball in $\mathbb{R}^n$ , $n \ge 2$ , such that diameter $d(U)=\delta$ . Let $ 0 \le s \le 1$ , we need to prove that $H_{\delta}^s(U)= H_{\delta}^s(\partial U)= H_{\delta}^s(\bar{U}) $ . Here $H_{\delta}^s(A)= \inf \{ \sum_{i} d(E_i)^s : A \subset \cup E_i, d(E_i) \le \delta \}, $ and $d(B)$ is just the diameter of $B$ . I am thinking since $\partial{U} \subset \bar{U} $ and $ U \subset \bar{U}$ , we have $H_{\delta}^s(\partial U) \le H_{\delta}^s(\bar{U}) $ and $H_{\delta}^s(U) \le H_{\delta}^s(\bar{U}) $ . I also know that $H_{\delta}^s(U) \le \delta^s$ . Any hint how to continue?","Let U open ball in , , such that diameter . Let , we need to prove that . Here and is just the diameter of . I am thinking since and , we have and . I also know that . Any hint how to continue?","\mathbb{R}^n n \ge 2 d(U)=\delta  0 \le s \le 1 H_{\delta}^s(U)= H_{\delta}^s(\partial U)= H_{\delta}^s(\bar{U})  H_{\delta}^s(A)= \inf \{ \sum_{i} d(E_i)^s : A \subset \cup E_i, d(E_i) \le \delta \},  d(B) B \partial{U} \subset \bar{U}   U \subset \bar{U} H_{\delta}^s(\partial U) \le H_{\delta}^s(\bar{U})  H_{\delta}^s(U) \le H_{\delta}^s(\bar{U})  H_{\delta}^s(U) \le \delta^s","['analysis', 'measure-theory']"
97,Solutions of the equation $x^x=\frac{1}{256}$,Solutions of the equation,x^x=\frac{1}{256},"Find the solutions of the equation $$x^x=\frac{1}{256}$$ I know that the function $f(x)=x^x$ is defined for $x>0$ , so the solutions, if they exists, must be $>0$ ; as far as I know, this is because to maintain the formal properties of power, we must impose that $x>0$ or it is easy to get contradictions like $-1=(-1)^{\frac{2}{2}}=[(-1)^2]^{\frac{1}{2}}=1^{\frac{1}{2}}=1$ . So, since the function $x^x$ has an absolute minimum at $x=\frac{1}{e}$ and it is $\left(\frac{1}{e}\right)^{\frac{1}{e}}>\frac{1}{256}$ , it follows that there isn't an $x>0$ such that $x^x=\frac{1}{256}$ and it follows that the equation hasn't real solutions. However, Wolfram|Alpha says that the equation has the integer solution $x=-4$ and it is indeed a solution, as one can check by substitution. However, for what I said before, the function $x^x$ doesn't exist for $x<0$ and so it can't be evaluated for that value. Why is this happening? Has it something to do with complex numbers? Or finding a solution of that equation is different to consider the function $x^x$ involved? Thank you.","Find the solutions of the equation I know that the function is defined for , so the solutions, if they exists, must be ; as far as I know, this is because to maintain the formal properties of power, we must impose that or it is easy to get contradictions like . So, since the function has an absolute minimum at and it is , it follows that there isn't an such that and it follows that the equation hasn't real solutions. However, Wolfram|Alpha says that the equation has the integer solution and it is indeed a solution, as one can check by substitution. However, for what I said before, the function doesn't exist for and so it can't be evaluated for that value. Why is this happening? Has it something to do with complex numbers? Or finding a solution of that equation is different to consider the function involved? Thank you.",x^x=\frac{1}{256} f(x)=x^x x>0 >0 x>0 -1=(-1)^{\frac{2}{2}}=[(-1)^2]^{\frac{1}{2}}=1^{\frac{1}{2}}=1 x^x x=\frac{1}{e} \left(\frac{1}{e}\right)^{\frac{1}{e}}>\frac{1}{256} x>0 x^x=\frac{1}{256} x=-4 x^x x<0 x^x,"['calculus', 'analysis', 'functions']"
98,"The properties of convex function on the closed unit interval $[0,1]$.",The properties of convex function on the closed unit interval .,"[0,1]","Consider a continuous and convex function $F(x):[0,1]\longrightarrow\mathbb{R}$ . I am wondering if $F(x)$ is continuously differentiable in $[0,1]$ $F(x)$ is of bounded variation in $[0,1]$ $F(x)$ is absolute continuous in $[0,1]$ . The second one is correct, due to this post Proving a convex function is of bounded variation . However, the remaining two became mysterious to me. Royden's chapter 6 answers them if we have an open interval. Corollary 17: Let $\varphi$ be a convex function on $(a,b)$ . Then $\varphi$ is Lipschitz, and therefore absolutely continuous on each closed, bounded subinterval $[c,d]$ and $(a,b)$ Theorem 18: Let $\varphi$ be a convex function on $(a,b)$ . Then $\varphi$ is differentiable except at a countable number of points. By the Theorem 18, it is hard to believe that $F(x)$ will become differentiable in $[0,1]$ . But I cannot find a counterexample. That is, a convex function that is continuous on $[0,1]$ but is not differentiable. The Corollary 17 gives us pretty nice result, but seems like it does not apply to the closed interval. Is it possible to say that if we have $F(x)$ on $[0,1]$ is convex, then it will be convex on $(-\epsilon, 1+\epsilon)$ ? and then we can use Corollary 17 to conclude that it is absolutely continuous on $[0,1]\subset (-\epsilon, 1+\epsilon)$ . Thank you!","Consider a continuous and convex function . I am wondering if is continuously differentiable in is of bounded variation in is absolute continuous in . The second one is correct, due to this post Proving a convex function is of bounded variation . However, the remaining two became mysterious to me. Royden's chapter 6 answers them if we have an open interval. Corollary 17: Let be a convex function on . Then is Lipschitz, and therefore absolutely continuous on each closed, bounded subinterval and Theorem 18: Let be a convex function on . Then is differentiable except at a countable number of points. By the Theorem 18, it is hard to believe that will become differentiable in . But I cannot find a counterexample. That is, a convex function that is continuous on but is not differentiable. The Corollary 17 gives us pretty nice result, but seems like it does not apply to the closed interval. Is it possible to say that if we have on is convex, then it will be convex on ? and then we can use Corollary 17 to conclude that it is absolutely continuous on . Thank you!","F(x):[0,1]\longrightarrow\mathbb{R} F(x) [0,1] F(x) [0,1] F(x) [0,1] \varphi (a,b) \varphi [c,d] (a,b) \varphi (a,b) \varphi F(x) [0,1] [0,1] F(x) [0,1] (-\epsilon, 1+\epsilon) [0,1]\subset (-\epsilon, 1+\epsilon)","['real-analysis', 'analysis', 'convex-analysis', 'absolute-continuity']"
99,Generalize the binomial equation,Generalize the binomial equation,,"I am wondering if generalizing the case $p=2$ we have for $1<p<\infty$ that there is a constant $c(p,l)$ such that for all $(z_j)$ with $z_j \in \mathbb C$ that $$\left\lvert \left\lvert \sum_{j=1}^l z_j \right\rvert^p - \sum_{j=1}^l \left\lvert z_j \right\rvert^p \right\rvert \le c(p,l) \sum_{j \neq k} \vert z_j \vert \vert z_k \vert^{p-1}$$ in the case $p=2$ I can clearly see it holds, but here, I don't know.","I am wondering if generalizing the case we have for that there is a constant such that for all with that in the case I can clearly see it holds, but here, I don't know.","p=2 1<p<\infty c(p,l) (z_j) z_j \in \mathbb C \left\lvert \left\lvert \sum_{j=1}^l z_j \right\rvert^p - \sum_{j=1}^l \left\lvert z_j \right\rvert^p \right\rvert \le c(p,l) \sum_{j \neq k} \vert z_j \vert \vert z_k \vert^{p-1} p=2","['real-analysis', 'analysis', 'inequality']"
