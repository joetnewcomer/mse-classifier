,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Question about using continuity of a seminorm,Question about using continuity of a seminorm,,"I almost have a homework problem solved but I've used a claim that might be dubious. The setting is this:  Let $(V,Q)$ be a locally convex space ($Q$ is the family of seminorms inducing the topology on $V$).  And let $q\in Q$. Claim: For any neighborhood $U$ of $0$ in $V$.  There exists a sufficiently small $\epsilon > 0$ such that $q^{-1}([0,\epsilon))\subset U$. The reason this claim helps me is that I need to prove something about all neighborhoods of $0$ in $V$, and it greatly simplifies things if I can simplify my situation to sets of the form $q^{-1}([0,\epsilon))$.","I almost have a homework problem solved but I've used a claim that might be dubious. The setting is this:  Let $(V,Q)$ be a locally convex space ($Q$ is the family of seminorms inducing the topology on $V$).  And let $q\in Q$. Claim: For any neighborhood $U$ of $0$ in $V$.  There exists a sufficiently small $\epsilon > 0$ such that $q^{-1}([0,\epsilon))\subset U$. The reason this claim helps me is that I need to prove something about all neighborhoods of $0$ in $V$, and it greatly simplifies things if I can simplify my situation to sets of the form $q^{-1}([0,\epsilon))$.",,"['functional-analysis', 'topological-vector-spaces']"
1,change of spectrum under diagonalisation,change of spectrum under diagonalisation,,"I have the following question. Let $T\colon \mathcal{H} \to \mathcal{H}$ be a bounded, self-adjoint operator on a Hilbert-space $\mathcal{H}$. By spectral theorem we know that there exists a measure space $(\Omega, \Sigma, \lambda)$ and a bounded measurable function $f : \Omega \rightarrow \mathbb{R}$ and a unitary operator $U \colon \mathcal{H} \to L^{2}(\Omega)$ such that $(UTU^{-1}) (\varphi) = f\cdot \varphi$. My question is: Is the spectrum of $T$ and the spectrum of $UTU^{-1}$ then the same, i.e. $\sigma(T) = \sigma(UTU^{-1})$ ? Hope this question is not too trivial? Thanks in advance. mika","I have the following question. Let $T\colon \mathcal{H} \to \mathcal{H}$ be a bounded, self-adjoint operator on a Hilbert-space $\mathcal{H}$. By spectral theorem we know that there exists a measure space $(\Omega, \Sigma, \lambda)$ and a bounded measurable function $f : \Omega \rightarrow \mathbb{R}$ and a unitary operator $U \colon \mathcal{H} \to L^{2}(\Omega)$ such that $(UTU^{-1}) (\varphi) = f\cdot \varphi$. My question is: Is the spectrum of $T$ and the spectrum of $UTU^{-1}$ then the same, i.e. $\sigma(T) = \sigma(UTU^{-1})$ ? Hope this question is not too trivial? Thanks in advance. mika",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
2,Well-posedness of the Poisson problem with mixed boundary conditions,Well-posedness of the Poisson problem with mixed boundary conditions,,"Let $\Omega \subset \mathbb R^n$ be a subdomain with Lipschitz boundary, i.e. locally any part of the boundary looks like the graph of a Lipschitz continuous function, after some affine coordinate transformation. Suppose we are given a ""partition"" $\Gamma_D$ and $\Gamma_N$ of the boundary, s.t. these sets are submanifolds of $\mathbb R^n$ with Lipschitz boundary by themselves, and their intersection has measure zero. Let us be given $g \in L^2(\Gamma_D)$ and $h \in L^2(\Gamma_N)$ and some function $f \in L^2(\Omega)$. We want to solve Poisson's equation with mixed boundary conditions $\operatorname{div}\operatorname{grad}  u = f$ over $\Omega$ $u_{|\Gamma_D} = g$ over $\Gamma_D$ $\operatorname{grad} u_{|\Gamma_N} \cdot n = h$ over $\Gamma_N$ It is standard to prove well-posedness of these problems if either $\Gamma_D$ or $\Gamma_N$ is the empty set. I have not found a rigorous proof of well-posedness for general mixed boundary conditions in the standard books like, say, Gilbarg-Trudinger. On the other hand, certain papers suggest the boundary parts are required to meet at an angle that is not 180° in the case of Lipschitz boundaries, so the boundary is necessarily non-smooth. These influences appear confusing to me. I do not know how to learn more about this. Could please give a reference where to learn more about the Poisson problem with mixed boundary conditions? EDIT: In order to motivate why this is interesting and why it confuses me, I would like to point to the paper Ott, Brown: The mixed problem for the Laplacian in Lipschitz domains and R.M. Brown. The mixed problem for Laplace’s equation in a class of Lipschitz domains . On the other hand, in numerical analysis lectures that I attend, this question is usually swept under the rug and one deals freely with mixed boundary conditions. So either I don't know the well-posedness results for simplicial domains, or the numerical examples all belong to the well-posed case.","Let $\Omega \subset \mathbb R^n$ be a subdomain with Lipschitz boundary, i.e. locally any part of the boundary looks like the graph of a Lipschitz continuous function, after some affine coordinate transformation. Suppose we are given a ""partition"" $\Gamma_D$ and $\Gamma_N$ of the boundary, s.t. these sets are submanifolds of $\mathbb R^n$ with Lipschitz boundary by themselves, and their intersection has measure zero. Let us be given $g \in L^2(\Gamma_D)$ and $h \in L^2(\Gamma_N)$ and some function $f \in L^2(\Omega)$. We want to solve Poisson's equation with mixed boundary conditions $\operatorname{div}\operatorname{grad}  u = f$ over $\Omega$ $u_{|\Gamma_D} = g$ over $\Gamma_D$ $\operatorname{grad} u_{|\Gamma_N} \cdot n = h$ over $\Gamma_N$ It is standard to prove well-posedness of these problems if either $\Gamma_D$ or $\Gamma_N$ is the empty set. I have not found a rigorous proof of well-posedness for general mixed boundary conditions in the standard books like, say, Gilbarg-Trudinger. On the other hand, certain papers suggest the boundary parts are required to meet at an angle that is not 180° in the case of Lipschitz boundaries, so the boundary is necessarily non-smooth. These influences appear confusing to me. I do not know how to learn more about this. Could please give a reference where to learn more about the Poisson problem with mixed boundary conditions? EDIT: In order to motivate why this is interesting and why it confuses me, I would like to point to the paper Ott, Brown: The mixed problem for the Laplacian in Lipschitz domains and R.M. Brown. The mixed problem for Laplace’s equation in a class of Lipschitz domains . On the other hand, in numerical analysis lectures that I attend, this question is usually swept under the rug and one deals freely with mixed boundary conditions. So either I don't know the well-posedness results for simplicial domains, or the numerical examples all belong to the well-posed case.",,"['functional-analysis', 'partial-differential-equations']"
3,The dual of $C_0^k(X)$,The dual of,C_0^k(X),"Let $X$ be a compact ball in $\mathbb{R}^n$. Let $C_0^k(X)$ be the space of a $k$ times continuously differentiable complex, valued smooth functions, which vansih outside of $X$. The norm on $C_0^k(X)$ is given by $$ ||f|| = \sum_{| \alpha | \leq k} || \partial^\alpha f ||_\infty $$ How does the Banach space dual space of $C_0^k(X)$ look?","Let $X$ be a compact ball in $\mathbb{R}^n$. Let $C_0^k(X)$ be the space of a $k$ times continuously differentiable complex, valued smooth functions, which vansih outside of $X$. The norm on $C_0^k(X)$ is given by $$ ||f|| = \sum_{| \alpha | \leq k} || \partial^\alpha f ||_\infty $$ How does the Banach space dual space of $C_0^k(X)$ look?",,"['functional-analysis', 'vector-spaces']"
4,Left regular representation of $L^1(G)$ for a locally compact group $G$,Left regular representation of  for a locally compact group,L^1(G) G,"Let $G$ be a locally compact group (not discrete) and let $L$ be the left regular representation of $A = L^1(G)$ on itself i.e. $L: A \to \mathcal{B}(A)$ where $L(f): A \to A$, $L(f)(g) = f*g$. I want to show that $\forall f\in A$,  $||L(f) - I|| \geq \frac{1}{2}$ where $I$ is the identity operator on A. Using the fact that $L^1(G)$ has an approximate identity, one can show that $||L(f) - I|| \geq | ||f||_1 - 1|$ and so the problem is reduced to $f \in A$ such that $\frac{1}{2}< ||f||_1 < \frac{3}{2}$. I'm not entirely sure how useful this is, but it's the only thing I've been able to come up with so far. Any hints or pointers in the right direction would be much appreciated. Edit: I'm still quite lost on this problem. I've tried simply considering the case where $G = \mathbb{R}$. In this case I've managed to show that it's true if $f$ is an indicator function on an interval, but the method I've been using falls apart when considering finite linear combinations of such functions.","Let $G$ be a locally compact group (not discrete) and let $L$ be the left regular representation of $A = L^1(G)$ on itself i.e. $L: A \to \mathcal{B}(A)$ where $L(f): A \to A$, $L(f)(g) = f*g$. I want to show that $\forall f\in A$,  $||L(f) - I|| \geq \frac{1}{2}$ where $I$ is the identity operator on A. Using the fact that $L^1(G)$ has an approximate identity, one can show that $||L(f) - I|| \geq | ||f||_1 - 1|$ and so the problem is reduced to $f \in A$ such that $\frac{1}{2}< ||f||_1 < \frac{3}{2}$. I'm not entirely sure how useful this is, but it's the only thing I've been able to come up with so far. Any hints or pointers in the right direction would be much appreciated. Edit: I'm still quite lost on this problem. I've tried simply considering the case where $G = \mathbb{R}$. In this case I've managed to show that it's true if $f$ is an indicator function on an interval, but the method I've been using falls apart when considering finite linear combinations of such functions.",,"['functional-analysis', 'operator-theory', 'harmonic-analysis', 'locally-compact-groups']"
5,The topology of $L_\mathrm{loc}^2 (\mathbb{R})$,The topology of,L_\mathrm{loc}^2 (\mathbb{R}),"In fact I am reading the book of Ohsawa, Analysis of Several Complex Variables , and I came across this line on page 13, ... $L^{2}_\mathrm{loc}(\Omega)$ with respect to the topology induced by the $L^2$ convergence on compact sets. Yet I am not clear how to induce a topology from. I have a guess. In the following I am considering the case $\Omega=\mathbb{R}$. Define $||f||_n=\int_{-n}^n |f|$ for $f\in L^2_\mathrm{loc}(\mathbb{R}), n=1,2,3,\ldots$. Put $$||f||=\sum\limits _{n=1}^\infty\frac{||f||_n}{1+||f||_n} \frac{1}{2^n}.$$  Then $||\cdot||$ is a metric for $L_\mathrm{loc}^2 (\mathbb{R}) $. Is it this topology? Or something else? Would someone be kind enough to give me some hints on this? Thank you very much.","In fact I am reading the book of Ohsawa, Analysis of Several Complex Variables , and I came across this line on page 13, ... $L^{2}_\mathrm{loc}(\Omega)$ with respect to the topology induced by the $L^2$ convergence on compact sets. Yet I am not clear how to induce a topology from. I have a guess. In the following I am considering the case $\Omega=\mathbb{R}$. Define $||f||_n=\int_{-n}^n |f|$ for $f\in L^2_\mathrm{loc}(\mathbb{R}), n=1,2,3,\ldots$. Put $$||f||=\sum\limits _{n=1}^\infty\frac{||f||_n}{1+||f||_n} \frac{1}{2^n}.$$  Then $||\cdot||$ is a metric for $L_\mathrm{loc}^2 (\mathbb{R}) $. Is it this topology? Or something else? Would someone be kind enough to give me some hints on this? Thank you very much.",,"['functional-analysis', 'topological-vector-spaces']"
6,How to make sense of this integral?,How to make sense of this integral?,,"Let $L$ be the Ornstein-Uhlenbeck operator on $L^2(\gamma)$ where $\gamma$ is the Gaussian measure on $\mathbf R^d$. Hille-Yosida or Lumer-Phillips can be used to prove that $L$ generates a strongly continuous semigroup $e^{tA}$. Now I have the following integral: $$\int_0^\infty (t^2 L)^{N + 1} e^{\beta t^2 L} u \, \frac{\text{d}t}{t},$$ where $u \in L^2$, $\beta > 0$. This doesn't look like a normal Bochner integral. I guess I need some kind of functional calculus. I would like to know how I could have this integral make sense.","Let $L$ be the Ornstein-Uhlenbeck operator on $L^2(\gamma)$ where $\gamma$ is the Gaussian measure on $\mathbf R^d$. Hille-Yosida or Lumer-Phillips can be used to prove that $L$ generates a strongly continuous semigroup $e^{tA}$. Now I have the following integral: $$\int_0^\infty (t^2 L)^{N + 1} e^{\beta t^2 L} u \, \frac{\text{d}t}{t},$$ where $u \in L^2$, $\beta > 0$. This doesn't look like a normal Bochner integral. I guess I need some kind of functional calculus. I would like to know how I could have this integral make sense.",,['functional-analysis']
7,Intuition behind Sobolev norm,Intuition behind Sobolev norm,,"This morning I was thinking at the following (simple) fact. Let us consider $[0, 1] \to \mathbb{R}$ functions and define a linear functional $$F(u)=u(1)-u(0).$$ $F$ is not continuous on $L^2(0, 1)$ (in fact, it is not even defined everywhere), but it is continuous on $H^1(0, 1)$: $$\lvert F(u) \rvert \le \int_0^1\lvert u'(x)\rvert\, dx\le \lVert u \rVert_{H^1}.$$ How would you give an intuitive explanation of this phenomenon? In what sense is Sobolev norm more restricting, so that a ""bad"" $L^2$ functional turns out to be a ""good"" one on $H^1$?","This morning I was thinking at the following (simple) fact. Let us consider $[0, 1] \to \mathbb{R}$ functions and define a linear functional $$F(u)=u(1)-u(0).$$ $F$ is not continuous on $L^2(0, 1)$ (in fact, it is not even defined everywhere), but it is continuous on $H^1(0, 1)$: $$\lvert F(u) \rvert \le \int_0^1\lvert u'(x)\rvert\, dx\le \lVert u \rVert_{H^1}.$$ How would you give an intuitive explanation of this phenomenon? In what sense is Sobolev norm more restricting, so that a ""bad"" $L^2$ functional turns out to be a ""good"" one on $H^1$?",,"['functional-analysis', 'functions', 'intuition', 'sobolev-spaces']"
8,Constructing a subset of $\ell_2$ with dense linear span and infinite complement,Constructing a subset of  with dense linear span and infinite complement,\ell_2,"the problem I'm stuck on is the following: Suppose that S is a countably infinite subset of $\ell_2$ with the property that the linear span of S′ is dense in $\ell_2$ whenever S\S′ is finite. Show that there is some S′ whose linear span is dense in $\ell_2$ and for which S\S′ is infinite. I have tried repeatedly to solve this in somewhat of a 'bang my head against a wall' manner, by constructing a series of subsets of some arbitrary S, such that the complement is finite and of increasing size, but I haven't had any success. I haven't actually used the fact that we're working in $\ell_2$ here, so it's quite likely that I'm meant to use some property of Hilbert spaces - however, I'm not sure what. Could anyone please help? Thankyou very much; Stephen.","the problem I'm stuck on is the following: Suppose that S is a countably infinite subset of $\ell_2$ with the property that the linear span of S′ is dense in $\ell_2$ whenever S\S′ is finite. Show that there is some S′ whose linear span is dense in $\ell_2$ and for which S\S′ is infinite. I have tried repeatedly to solve this in somewhat of a 'bang my head against a wall' manner, by constructing a series of subsets of some arbitrary S, such that the complement is finite and of increasing size, but I haven't had any success. I haven't actually used the fact that we're working in $\ell_2$ here, so it's quite likely that I'm meant to use some property of Hilbert spaces - however, I'm not sure what. Could anyone please help? Thankyou very much; Stephen.",,[]
9,Isomorphism in Banach Spaces,Isomorphism in Banach Spaces,,"Let $E$ and $F$ be Banach spaces. Let $T: E \rightarrow F$ be an isomorphism (i.e., a continuous vector space isomorphism with a continuous inverse). Let $J_E$ and $J_F$ be the canonical injections of $E$ and $F$ , respectively. Define $S: E^{\star\star}/J_E(E)$ $\rightarrow$ $F^{\star \star}/J_F(F)$ , $\quad$$[\phi] \mapsto\left[T^{**} \phi\right]$ a) Prove that $S$ is well-defined. b) Prove that $S$ is a vector space isomorphism. c) Prove that $S$ is continuous. Is the inverse continuous? d) Prove that if $T$ is an isometry, then $S$ is also. I'm trying to do this exercise but I have no idea how to prove that S is an isomorphism. I searched this forum and couldn't find anything like it. And for question C) my answer would be yes as a corollary of the open mapping theorem. For D) I have no idea. I have attached the full exercise for more context.","Let and be Banach spaces. Let be an isomorphism (i.e., a continuous vector space isomorphism with a continuous inverse). Let and be the canonical injections of and , respectively. Define , a) Prove that is well-defined. b) Prove that is a vector space isomorphism. c) Prove that is continuous. Is the inverse continuous? d) Prove that if is an isometry, then is also. I'm trying to do this exercise but I have no idea how to prove that S is an isomorphism. I searched this forum and couldn't find anything like it. And for question C) my answer would be yes as a corollary of the open mapping theorem. For D) I have no idea. I have attached the full exercise for more context.",E F T: E \rightarrow F J_E J_F E F S: E^{\star\star}/J_E(E) \rightarrow F^{\star \star}/J_F(F) \quad[\phi] \mapsto\left[T^{**} \phi\right] S S S T S,"['real-analysis', 'functional-analysis', 'analysis', 'functional-calculus']"
10,Papa Rudin Theorem $7.15$.,Papa Rudin Theorem .,7.15,"There is the definition of $(D\mu)(x)$ : There is the theorem: If $\mu$ is a Borel measure on $R^k$ and $\mu \ \bot \ m$ , then $$(D\mu)(x) = \infty \ a.e. [\mu]. $$ (Denote this equality by $(1)$ .) There is the proof: There is a Borel set $S \subset R^k$ with $m(S) = 0 $ and $\mu(R^k - s) = 0 $ , and there are open sets $V_j \supset S$ with $m(V_j) \lt 1/j$ , for $j = 1,2,3$ ,... For $N = 1,2,3,...,$ let $E_N$ be the set of all $x \in S$ to which correspond radii $r_i = r_i(x)$ , with $\lim r_i = 0$ , such that $$\mu(B(x,r_i)) \lt Nm(B(x,r_i)).$$ ( Denote this inequality by $(2)$ .) Then $(1)$ holds for every $x \in S - \bigcup_{N} E_N$ . Fix $N$ and $j$ , for the moment. Every $x \in E_N$ is then the center of a ball $B_x \subset V_j$ that satisfies $(2)$ . I don't understand how do we conclude that Every $x \in E_N$ is then the center of a ball $B_x \subset V_j$ that satisfies $(2)$ . Any help would be appreciated.","There is the definition of : There is the theorem: If is a Borel measure on and , then (Denote this equality by .) There is the proof: There is a Borel set with and , and there are open sets with , for ,... For let be the set of all to which correspond radii , with , such that ( Denote this inequality by .) Then holds for every . Fix and , for the moment. Every is then the center of a ball that satisfies . I don't understand how do we conclude that Every is then the center of a ball that satisfies . Any help would be appreciated.","(D\mu)(x) \mu R^k \mu \ \bot \ m (D\mu)(x) = \infty \ a.e. [\mu].  (1) S \subset R^k m(S) = 0  \mu(R^k - s) = 0  V_j \supset S m(V_j) \lt 1/j j = 1,2,3 N = 1,2,3,..., E_N x \in S r_i = r_i(x) \lim r_i = 0 \mu(B(x,r_i)) \lt Nm(B(x,r_i)). (2) (1) x \in S - \bigcup_{N} E_N N j x \in E_N B_x \subset V_j (2) x \in E_N B_x \subset V_j (2)","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
11,Reversed Fubini's,Reversed Fubini's,,"Assume we have a real valued function $F:\mathbb{R}^{n} \times (0, \infty) \to \mathbb{R}$ . And assume that we have the function $ g: \mathbb{R}^{n} \to \mathbb{R} $ given by $$ g(x) = \int_{0}^{\infty} F(x,r) dr  $$ Is Lebesgue measurable. can we conclude that $ F $ is also (n+1)-Lebesgue measurable as a function of $\mathbb{R}_{+}^{n+1}$ ? If it helps, we have that $g \in L^{r}(\mathbb{R}^{n})$ for some $ r \in (0,\infty)$ .","Assume we have a real valued function . And assume that we have the function given by Is Lebesgue measurable. can we conclude that is also (n+1)-Lebesgue measurable as a function of ? If it helps, we have that for some .","F:\mathbb{R}^{n} \times (0, \infty) \to \mathbb{R}  g: \mathbb{R}^{n} \to \mathbb{R}  
g(x) = \int_{0}^{\infty} F(x,r) dr 
  F  \mathbb{R}_{+}^{n+1} g \in L^{r}(\mathbb{R}^{n})  r \in (0,\infty)","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'fubini-tonelli-theorems']"
12,"Analysis of an expression involving a function on $\mathbb R^n$. Related to limits, supremums and translations.","Analysis of an expression involving a function on . Related to limits, supremums and translations.",\mathbb R^n,"Let $n \in \mathbb N, \, 0 < \lambda < n$ and $1 \leqslant p < \infty$ , consider the usual Lebesgue measure on $\mathbb R^n$ and define the function $f \colon \mathbb R^n \to \mathbb R$ by $$ f(x) := |x|^{\frac{\lambda - n}{p}}, $$ for every $x \in \mathbb R^n \setminus \{0\}$ ( $f$ is defined arbitrarily at $x = 0$ ). Question. Prove that $$ \lim_{\xi \to 0} \, \, \sup_{x \in \mathbb R^n, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \neq 0 $$ My attempt. Clearly, to prove the result it suffices to find a constant $c > 0$ such that $$ \sup_{x \in \mathbb R^n, \, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \geqslant c, $$ for every $\xi \in \mathbb R^n \setminus \{0\}$ . This is an important observation because it allows us to work with any values of $x$ and $r$ that we find most adequate. I have tried the following: $$ \sup_{x \in \mathbb R^n, \, r > 0}r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \geqslant \int_{B(0,1)} |f(y-\xi) - f(y)|^p \, dy, $$ where I have simply used the definition of supremum and took $x = 0$ and $r=1$ . Furthermore, applying a simple inequality, it follows that $$ \int_{B(0,1)} |f(y-\xi) - f(y)|^p \, dy \geqslant 2^{-p} \int_{B(0,1)} |f(y-\xi)|^p \, dy - \int_{B(0,1)} |f(y)|^p \, dy. $$ Now let us compute the integrals of the right-hand side. For the latter one, using the theory of integration of radial functions, we have that $$ \int_{B(0,1)} |f(y)|^p \, dy = \int_{B(0,1)} |y|^{\frac{\lambda - n}{p}} \, dy = \int_0^1 t^{\lambda -1} \, dt = \frac{1}{\lambda}. $$ Now, I am having a hard time computing the first integral. Does anyone have an idea on how to proceed? Note that the final lower bound should be strictly positive! Thanks for any help in advance.","Let and , consider the usual Lebesgue measure on and define the function by for every ( is defined arbitrarily at ). Question. Prove that My attempt. Clearly, to prove the result it suffices to find a constant such that for every . This is an important observation because it allows us to work with any values of and that we find most adequate. I have tried the following: where I have simply used the definition of supremum and took and . Furthermore, applying a simple inequality, it follows that Now let us compute the integrals of the right-hand side. For the latter one, using the theory of integration of radial functions, we have that Now, I am having a hard time computing the first integral. Does anyone have an idea on how to proceed? Note that the final lower bound should be strictly positive! Thanks for any help in advance.","n \in \mathbb N, \, 0 < \lambda < n 1 \leqslant p < \infty \mathbb R^n f \colon \mathbb R^n \to \mathbb R  f(x) := |x|^{\frac{\lambda - n}{p}},  x \in \mathbb R^n \setminus \{0\} f x = 0  \lim_{\xi \to 0} \, \, \sup_{x \in \mathbb R^n, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \neq 0  c > 0  \sup_{x \in \mathbb R^n, \, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \geqslant c,  \xi \in \mathbb R^n \setminus \{0\} x r  \sup_{x \in \mathbb R^n, \, r > 0}r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy \geqslant \int_{B(0,1)} |f(y-\xi) - f(y)|^p \, dy,  x = 0 r=1  \int_{B(0,1)} |f(y-\xi) - f(y)|^p \, dy \geqslant 2^{-p} \int_{B(0,1)} |f(y-\xi)|^p \, dy - \int_{B(0,1)} |f(y)|^p \, dy.   \int_{B(0,1)} |f(y)|^p \, dy = \int_{B(0,1)} |y|^{\frac{\lambda - n}{p}} \, dy = \int_0^1 t^{\lambda -1} \, dt = \frac{1}{\lambda}. ","['real-analysis', 'functional-analysis', 'limits', 'lebesgue-integral', 'lebesgue-measure']"
13,Hilbert space map with closed image respects closed subspaces?,Hilbert space map with closed image respects closed subspaces?,,"Suppose $X$ and $Y$ are Hilbert spaces, and $T:X \rightarrow Y$ is a bounded linear operator with closed image. Is it true that for every closed subspace $M \subseteq X$ , that $T(M)$ will be closed in $Y$ ? I’m struggling to prove this is true, but I’m also failing to find a counter example. The best idea I have had in trying to prove the result is to use the following lemma: Lemma: Let $L: A \rightarrow B$ be a bounded linear map of Hilbert spaces. Then $\text{im}(L) \subseteq B$ is closed if and only if there is a real constant $c > 0$ such that $c ||a|| \leq ||La||$ for all $a \in (\ker L)^\perp$ . Viewing $M$ as a Hilbert space, we can try to prove that the restriction $T \rvert_M: M \rightarrow Y$ has closed image using the lemma. The issue I’m having with this approach is I don’t see any guarantee that the orthogonal complement of $\ker(T_M)$ (inside of of $M$ ) is contained in the orthogonal complement of $T$ (inside of $X$ ). Any help would be appreciated! And if there is a counter example, is there a nice characterization of maps that send closed subspaces to closed subspaces?","Suppose and are Hilbert spaces, and is a bounded linear operator with closed image. Is it true that for every closed subspace , that will be closed in ? I’m struggling to prove this is true, but I’m also failing to find a counter example. The best idea I have had in trying to prove the result is to use the following lemma: Lemma: Let be a bounded linear map of Hilbert spaces. Then is closed if and only if there is a real constant such that for all . Viewing as a Hilbert space, we can try to prove that the restriction has closed image using the lemma. The issue I’m having with this approach is I don’t see any guarantee that the orthogonal complement of (inside of of ) is contained in the orthogonal complement of (inside of ). Any help would be appreciated! And if there is a counter example, is there a nice characterization of maps that send closed subspaces to closed subspaces?",X Y T:X \rightarrow Y M \subseteq X T(M) Y L: A \rightarrow B \text{im}(L) \subseteq B c > 0 c ||a|| \leq ||La|| a \in (\ker L)^\perp M T \rvert_M: M \rightarrow Y \ker(T_M) M T X,"['linear-algebra', 'functional-analysis', 'hilbert-spaces']"
14,Are Strong- and weak Operator topologies on separable Hilbert spaces sequential?,Are Strong- and weak Operator topologies on separable Hilbert spaces sequential?,,"If I am not mistaken, the norm operator topology should make the set of bounded operators into a sequential space, since the norm defines a metric. I was wondering if the Weak and Strong Operator topologies also turn the bounded operators into a sequential space, i.e. is a sequentially closed set in those topologies automatically closed? If that makes things easier, I am only interested in bounded operators on a separable Hilbert space. Best Lev","If I am not mistaken, the norm operator topology should make the set of bounded operators into a sequential space, since the norm defines a metric. I was wondering if the Weak and Strong Operator topologies also turn the bounded operators into a sequential space, i.e. is a sequentially closed set in those topologies automatically closed? If that makes things easier, I am only interested in bounded operators on a separable Hilbert space. Best Lev",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
15,"How do we know the space of distributions is ""big""?","How do we know the space of distributions is ""big""?",,"Consider the set of compactly supported smooth functions $C^\infty_c(\mathbb{R})$ . The dual of this space, $(C^\infty_c(\mathbb{R}))'$ is often referred to as a very large space. Since every $f \in C^\infty_c(\mathbb{R})$ can be mapped to an element in the dual space by integration we know that $C^\infty_c(\mathbb{R}) \subset (C^\infty_c(\mathbb{R}))'$ and that this inclusion is strict since non-regular distributions are known to exist. But how do we know that the dual space is much larger than the original space? That is, how do we know the non-regular distributions are not rare? Is there a way to quantify how much bigger the dual space is compared to the original space?","Consider the set of compactly supported smooth functions . The dual of this space, is often referred to as a very large space. Since every can be mapped to an element in the dual space by integration we know that and that this inclusion is strict since non-regular distributions are known to exist. But how do we know that the dual space is much larger than the original space? That is, how do we know the non-regular distributions are not rare? Is there a way to quantify how much bigger the dual space is compared to the original space?",C^\infty_c(\mathbb{R}) (C^\infty_c(\mathbb{R}))' f \in C^\infty_c(\mathbb{R}) C^\infty_c(\mathbb{R}) \subset (C^\infty_c(\mathbb{R}))',"['functional-analysis', 'distribution-theory', 'dual-spaces']"
16,$L^p_{loc}(\Omega)$ is completely metrizable,is completely metrizable,L^p_{loc}(\Omega),"Let $\Omega \subset \mathbb{R}^n$ be a (not necessarily bounded) domain and $1 \leq p \leq \infty$ . Then define $L^p_{loc}(\Omega)$ to be the set of functions $f: \Omega \rightarrow \mathbb{R}$ such that $$\Big(\int_K |f|^p\Big)^{1/p} < \infty, \quad \forall K \text{ compact.}$$ I have read that this space is a completely metrizable space but cannot find a proof so I would like to prove it myself but I am having some trouble. My approach is to find a metric such that convergence with respect to that metric agrees with convergence in $L^p_{loc}$ . I am not sure if this is the right approach since constructing such a metric is not obvious. Is there a better way to prove this?",Let be a (not necessarily bounded) domain and . Then define to be the set of functions such that I have read that this space is a completely metrizable space but cannot find a proof so I would like to prove it myself but I am having some trouble. My approach is to find a metric such that convergence with respect to that metric agrees with convergence in . I am not sure if this is the right approach since constructing such a metric is not obvious. Is there a better way to prove this?,"\Omega \subset \mathbb{R}^n 1 \leq p \leq \infty L^p_{loc}(\Omega) f: \Omega \rightarrow \mathbb{R} \Big(\int_K |f|^p\Big)^{1/p} < \infty, \quad \forall K \text{ compact.} L^p_{loc}","['functional-analysis', 'analysis', 'lp-spaces', 'metrizability']"
17,Am I Using Rellich Compactness Theorem Correctly to extract a convergent subsequence,Am I Using Rellich Compactness Theorem Correctly to extract a convergent subsequence,,"I'm trying to work through how Rellich compactness theorem is used at the start of this proof to extract the subsequence Given an sequence $\{y_n\}^{\infty} _{n=1} \in H_A(D^*)/ \mathbb{R}$ that is bounded in the energy norm over $D^*$ , you can extract a subsequence that converges in $H^1(D)$ to an element of $H_A(D)/ \mathbb{R}$ . The start of the proof was given as: Use Poincare Inequality along with Rellich compactness theorem to extract a convergent subsequence in $L^2(D^*)$ . Some notes on terminology: $D \subset D^* \subset \mathbb{R}^n$ (for $n=2,3$ ), the energy norm is $||x||^2 _{E(D)} = \int_{D} A \nabla x \cdot    \nabla x \mbox{ } d \vec{x}$ , $A$ is a $n \times n$ symmetric positive definite matrix, $H_A(D)$ is the space of functions in $H^1(D)$ that are A harmonic on $D$ My attempt so far is: Consider a sequence $\{y_n\}^{\infty} _{n=1} \in H_A(D^*)/ \mathbb{R}$ that is bounded in the energy norm over $D^*$ . This means there exists some positive M such that $||y_n||_{E(D^*)} =\int_{D^*} A \nabla y_n \cdot \nabla y_n \leq M$ . I was thinking that if I can show that the sequence is bounded in $H_0 ^1( D^*)$ then I can use Rellich compactness Theorem to extract a convergent subsequence in $L^2(D^*)$ (?). I tried to follow what I saw here first but I'm not sure this is correct because I think I need to use the quotient space. Next I tried to follow what I found here in the second answer. I took $y_0 \in H_A(D^*)/ \mathbb{R} \subset H^1(D^*)$ to be the average value of $y$ in $D^*$ . This, I think, means that $y_n - y_0 \in H^1 _0(D^*)$ (?). Then $||y_n - y_0||^2_{H^1 _0(D^*)}$ $ = \int_{D^*} (y_n - y_0)^2 d \vec{x} + \int_{D^*} \nabla(y_n - y_0)^2 d \vec{x}$ $\leq C \int_{D^*} \nabla(y_n)^2 \mbox{ } d \vec{x} +\int_{D^*} \nabla(y_n - y_0)^2 \mbox{ } d \vec{x} $ (last inequality is by alternate version of Poincare Inequality from this source) $= (C+1) \int_{D^*} \nabla y_n \cdot \nabla y_n \mbox{ } d \vec{x} -\int_{D^*} 2 \nabla y_0 \cdot \nabla y_n + \nabla y_0 \cdot \nabla y_0 \mbox{ } d \vec{x}$ But here is where I get confused. I was hoping to be able to use the boundedness of the energy norm in this line of reasoning but without the $A$ matrix involved in the integrals I'm not sure how I can. Once the above bounded argument is fixed, next we can say that by Rellich compactness theorem there exists a subsequence $\{u_{n_k}\} = \{Ty_{n_k}\}$ that converges in $L^2(D^*)$ (I think?). Then since the subsequence converges it is Cauchy in $L^2(D^*)$ . Is this the correct way to use Rellich compactness theorem in the start of this proof? Any input is greatly appreciated. Thank you greatly in advance! If you're interested, I'm referencing this paper.","I'm trying to work through how Rellich compactness theorem is used at the start of this proof to extract the subsequence Given an sequence that is bounded in the energy norm over , you can extract a subsequence that converges in to an element of . The start of the proof was given as: Use Poincare Inequality along with Rellich compactness theorem to extract a convergent subsequence in . Some notes on terminology: (for ), the energy norm is , is a symmetric positive definite matrix, is the space of functions in that are A harmonic on My attempt so far is: Consider a sequence that is bounded in the energy norm over . This means there exists some positive M such that . I was thinking that if I can show that the sequence is bounded in then I can use Rellich compactness Theorem to extract a convergent subsequence in (?). I tried to follow what I saw here first but I'm not sure this is correct because I think I need to use the quotient space. Next I tried to follow what I found here in the second answer. I took to be the average value of in . This, I think, means that (?). Then (last inequality is by alternate version of Poincare Inequality from this source) But here is where I get confused. I was hoping to be able to use the boundedness of the energy norm in this line of reasoning but without the matrix involved in the integrals I'm not sure how I can. Once the above bounded argument is fixed, next we can say that by Rellich compactness theorem there exists a subsequence that converges in (I think?). Then since the subsequence converges it is Cauchy in . Is this the correct way to use Rellich compactness theorem in the start of this proof? Any input is greatly appreciated. Thank you greatly in advance! If you're interested, I'm referencing this paper.","\{y_n\}^{\infty} _{n=1} \in H_A(D^*)/ \mathbb{R} D^* H^1(D) H_A(D)/ \mathbb{R} L^2(D^*) D \subset D^* \subset \mathbb{R}^n n=2,3 ||x||^2 _{E(D)} = \int_{D} A \nabla x \cdot
   \nabla x \mbox{ } d \vec{x} A n \times n H_A(D) H^1(D) D \{y_n\}^{\infty} _{n=1} \in H_A(D^*)/ \mathbb{R} D^* ||y_n||_{E(D^*)} =\int_{D^*} A \nabla y_n \cdot \nabla y_n \leq M H_0 ^1( D^*) L^2(D^*) y_0 \in H_A(D^*)/ \mathbb{R} \subset H^1(D^*) y D^* y_n - y_0 \in H^1 _0(D^*) ||y_n - y_0||^2_{H^1 _0(D^*)}  = \int_{D^*} (y_n - y_0)^2 d \vec{x} + \int_{D^*} \nabla(y_n - y_0)^2 d \vec{x} \leq C \int_{D^*} \nabla(y_n)^2 \mbox{ } d \vec{x} +\int_{D^*} \nabla(y_n - y_0)^2 \mbox{ } d \vec{x}  = (C+1) \int_{D^*} \nabla y_n \cdot \nabla y_n \mbox{ } d \vec{x} -\int_{D^*} 2 \nabla y_0 \cdot \nabla y_n + \nabla y_0 \cdot \nabla y_0 \mbox{ } d \vec{x} A \{u_{n_k}\} = \{Ty_{n_k}\} L^2(D^*) L^2(D^*)","['real-analysis', 'functional-analysis', 'solution-verification', 'normed-spaces', 'compactness']"
18,$f(S)=S^{-1}$ is continuous where $S:E\rightarrow E$ is an isomorphism between Banach spaces.,is continuous where  is an isomorphism between Banach spaces.,f(S)=S^{-1} S:E\rightarrow E,"Let $E$ be a Banach Space and $f:Iso(E,E)\rightarrow Iso(E,E)$ be given by $f(S)=S^{-1}$ . Prove that $f$ is continuos. Hint: $f(I-T)=\sum_{n=0}^\infty T^n$ if $\lVert T\rVert<1$ . I was able to solve this without the hint as follows: $$\lVert S^{-1}-S_o^{-1}\rVert=\lVert S^{-1}S_oS_o^{-1}-S_o^{-1} \rVert\leq \lVert S^{-1}S_o-I \rVert \lVert S_o^{-1} \rVert \leq  \lVert S^{-1}S_o-S^{-1} S \rVert \lVert S_o^{-1} \rVert\Rightarrow $$ $$\lVert  S^{-1}-S_o^{-1}\rVert\leq \lVert S^{-1}\rVert\lVert S_o^{-1}\rVert\lVert S-S_o\rVert$$ So we need to find a nice bound for $\lVert S^{-1} \rVert$ . At first I noticed: $$\lVert S^{-1} \rVert=\sup_{x\not=0}\frac{\lVert S^{-1}(x)\rVert}{\lVert x\rVert}=\sup_{v\not=0}\frac{\lVert S^{-1}(S(v))\rVert}{\lVert S(v)\rVert}=\sup_{v\not=0}\frac{\lVert v\rVert}{\lVert S(v)\rVert}=$$ $$\left(\inf_{v\not=0}\frac{\lVert S(v)\rVert}{\lVert v\rVert}\right)^{-1}=\frac{1}{\inf_{\lVert v\rVert =1}\lVert S(v)\rVert}$$ It is straightforward to see that for $\lVert v \rVert=1$ : $$\lVert S (v) \rVert\geq \lVert S_o(v)-(S_o-S) (v) \rVert \geq\lVert S_o(v) \rVert-\lVert S-S_o\rVert\Rightarrow$$ $$ \inf_{\lVert v\rVert=1}\lVert S (v) \rVert \geq \inf_{\lVert v\rVert=1}\lVert S_o (v) \rVert - \lVert S-S_o\rVert\Rightarrow$$ $$ \frac{1}{\lVert S^{-1} \rVert} \geq \frac{1}{\lVert S_o^{-1}\rVert} - \lVert S-S_o\rVert\Rightarrow\lVert S^{-1}\rVert\leq \frac{\lVert S_{o}^{-1}\rVert}{1-\lVert S_o-S \rVert\lVert S_o^{-1}\rVert}$$ Combining this bound for $S$ with our initial inequality yields: $$\rVert S^{-1}- S_o^{-1} \rVert \leq  \lVert S_o- S \rVert\lVert  S^{-1}\rVert \lVert S_o^{-1} \rVert\leq \frac{\lVert S_{o}^{-1}\rVert^2\lVert S-S_o \rVert}{1-\lVert S_o-S \rVert\lVert S_o^{-1}\rVert}$$ And from this it is clear that if $S\in Iso(E,E)$ and $\rVert S- S_o \rVert\leq \delta(\varepsilon)\Rightarrow \rVert S^{-1}- S_o^{-1} \rVert\leq\varepsilon$ . I wonder how the hint comes into play and if my proof avoiding the hint is correct.",Let be a Banach Space and be given by . Prove that is continuos. Hint: if . I was able to solve this without the hint as follows: So we need to find a nice bound for . At first I noticed: It is straightforward to see that for : Combining this bound for with our initial inequality yields: And from this it is clear that if and . I wonder how the hint comes into play and if my proof avoiding the hint is correct.,"E f:Iso(E,E)\rightarrow Iso(E,E) f(S)=S^{-1} f f(I-T)=\sum_{n=0}^\infty T^n \lVert T\rVert<1 \lVert S^{-1}-S_o^{-1}\rVert=\lVert S^{-1}S_oS_o^{-1}-S_o^{-1} \rVert\leq \lVert S^{-1}S_o-I \rVert \lVert S_o^{-1} \rVert \leq  \lVert S^{-1}S_o-S^{-1} S \rVert \lVert S_o^{-1} \rVert\Rightarrow  \lVert  S^{-1}-S_o^{-1}\rVert\leq \lVert S^{-1}\rVert\lVert S_o^{-1}\rVert\lVert S-S_o\rVert \lVert S^{-1} \rVert \lVert S^{-1} \rVert=\sup_{x\not=0}\frac{\lVert S^{-1}(x)\rVert}{\lVert x\rVert}=\sup_{v\not=0}\frac{\lVert S^{-1}(S(v))\rVert}{\lVert S(v)\rVert}=\sup_{v\not=0}\frac{\lVert v\rVert}{\lVert S(v)\rVert}= \left(\inf_{v\not=0}\frac{\lVert S(v)\rVert}{\lVert v\rVert}\right)^{-1}=\frac{1}{\inf_{\lVert v\rVert =1}\lVert S(v)\rVert} \lVert v \rVert=1 \lVert S (v) \rVert\geq \lVert S_o(v)-(S_o-S) (v) \rVert \geq\lVert S_o(v) \rVert-\lVert S-S_o\rVert\Rightarrow  \inf_{\lVert v\rVert=1}\lVert S (v) \rVert \geq \inf_{\lVert v\rVert=1}\lVert S_o (v) \rVert - \lVert S-S_o\rVert\Rightarrow  \frac{1}{\lVert S^{-1} \rVert} \geq \frac{1}{\lVert S_o^{-1}\rVert} - \lVert S-S_o\rVert\Rightarrow\lVert S^{-1}\rVert\leq \frac{\lVert S_{o}^{-1}\rVert}{1-\lVert S_o-S \rVert\lVert S_o^{-1}\rVert} S \rVert S^{-1}- S_o^{-1} \rVert \leq  \lVert S_o- S \rVert\lVert  S^{-1}\rVert \lVert S_o^{-1} \rVert\leq \frac{\lVert S_{o}^{-1}\rVert^2\lVert S-S_o \rVert}{1-\lVert S_o-S \rVert\lVert S_o^{-1}\rVert} S\in Iso(E,E) \rVert S- S_o \rVert\leq \delta(\varepsilon)\Rightarrow \rVert S^{-1}- S_o^{-1} \rVert\leq\varepsilon","['functional-analysis', 'solution-verification', 'alternative-proof']"
19,Prove that $f(x)$ is almost everywhere equal to a characteristic function,Prove that  is almost everywhere equal to a characteristic function,f(x),"Suppose $f$ be a non-negative function in $L^{1}[0,1]$ . Suppose that for every integer $n=1,2,3..$ , we have $$ \int_{[0,1]} f(x)^{n} dm= \int_{[0,1]} f(x) dm $$ Then prove that $f(x)$ must be almost everywhere equal to the characteristic function $\chi_{E}$ of a measurable set. My attempt: Since $f \in L^{1}[0,1], \int_{[0,1]} f dm < \infty$ . I am partitioning the sample space such into $$A_{1} = \{   |f| > 1 \}$$ $$A_{2} = \{   |f| = 1 \}$$ $$A_{3} = \{ 0 <  |f| < 1 \}$$ Applying Fatou's lemma to $g_{n}=f(x)^{n}$ , we get that $$\int_{[0,1]} \liminf_{n \to \infty} g_{n} \leq \liminf_{n \to \infty} \int g_n= \int_{[0,1]}f(x) dm $$ $$\int_{[0,1]} \liminf_{n \to \infty} f(x)^n \leq  \int_{[0,1]}f(x) dm $$ Now, I multiply both the sides with $\chi_{A_1}(x)$ ( it is measurable as f is measurable). $$\int_{[0,1]} \liminf_{n \to \infty} f(x)^n \chi_{A_1}(x) \leq  \int_{[0,1]}f(x) \chi_{A_1}(x) dm $$ $$\int_{[0,1] \cap A_1} \liminf_{n \to \infty} f(x)^n  \leq  \int_{[0,1]\cap A_1}f(x)  dm $$ Thus, $m(A_1)=0$ , else we get a contradiction. For $A_{3}$ , I multiply the original equation with $\chi_{A_3}$ on both the sides, $$ \int_{[0,1] \cap A_3} f(x)^{n} dm= \int_{[0,1]\cap A_3} f(x) > \int_{[0,1] \cap A_3}  dm > 0$$ But, this is true for all $n$ and as we increase $n$ to infinity, we get $0$ on the Left Hand side, hence $m(A_3)=0$ as well. For values of $x$ in $A_2$ , $f(x)^{n}=f(x)$ for all $n$ . Hence $f(x)=1$ almost everywhere. The measurable set $E= \{|f|=1\}$ . Can someone verify or give alternative ideas?","Suppose be a non-negative function in . Suppose that for every integer , we have Then prove that must be almost everywhere equal to the characteristic function of a measurable set. My attempt: Since . I am partitioning the sample space such into Applying Fatou's lemma to , we get that Now, I multiply both the sides with ( it is measurable as f is measurable). Thus, , else we get a contradiction. For , I multiply the original equation with on both the sides, But, this is true for all and as we increase to infinity, we get on the Left Hand side, hence as well. For values of in , for all . Hence almost everywhere. The measurable set . Can someone verify or give alternative ideas?","f L^{1}[0,1] n=1,2,3..  \int_{[0,1]} f(x)^{n} dm= \int_{[0,1]} f(x) dm  f(x) \chi_{E} f \in L^{1}[0,1], \int_{[0,1]} f dm < \infty A_{1} = \{   |f| > 1 \} A_{2} = \{   |f| = 1 \} A_{3} = \{ 0 <  |f| < 1 \} g_{n}=f(x)^{n} \int_{[0,1]} \liminf_{n \to \infty} g_{n} \leq \liminf_{n \to \infty} \int g_n= \int_{[0,1]}f(x) dm  \int_{[0,1]} \liminf_{n \to \infty} f(x)^n \leq  \int_{[0,1]}f(x) dm  \chi_{A_1}(x) \int_{[0,1]} \liminf_{n \to \infty} f(x)^n \chi_{A_1}(x) \leq  \int_{[0,1]}f(x) \chi_{A_1}(x) dm  \int_{[0,1] \cap A_1} \liminf_{n \to \infty} f(x)^n  \leq  \int_{[0,1]\cap A_1}f(x)  dm  m(A_1)=0 A_{3} \chi_{A_3}  \int_{[0,1] \cap A_3} f(x)^{n} dm= \int_{[0,1]\cap A_3} f(x) > \int_{[0,1] \cap A_3}  dm > 0 n n 0 m(A_3)=0 x A_2 f(x)^{n}=f(x) n f(x)=1 E= \{|f|=1\}","['functional-analysis', 'measure-theory']"
20,On the nonempty intersection of almost convex sets,On the nonempty intersection of almost convex sets,,"Let $(X,\|\cdot\|)$ be a Banach space and $U_{X}$ its closed unit ball. According to  Himmelber, Fixed points of compact multifunctions , a subset $B \subset X$ is said to be almost convex if given $\varepsilon >0$ if given $\{x_{1},\ldots x_{n}\}\subset B$ there are $\{z_{1},\ldots, z_{n}\}\subset B$ such that $\|x_{i}-z_{i}\|\leq \varepsilon$ and $\mathrm{co}(z_{1},\ldots, z_{n})\subset B$ (""co"" denotes the convex hull). Of course, a convex set is almost convex. It is not very hard to show (for instance, in the Euclidean plane) that in general, the nonempty intersection of almost convex sets is not an almost convex set. I am looking for an example of a bounded and almost convex (but not convex) subset of $X$ (infinite dimensional), say $B$ , such that if $C$ is any family of almost convex subsets de $B$ with nonempty intersection, then $\cap_{A\in C}A\neq \emptyset$ is an almost set. I have tried with the following example. Example : Let $X$ be the Banach space, endowed its usual supremum norm, of real continuous functions defined in $[0,1]$ ,  and $P[0,1]$ the Berstein polynomials of the functions belonging to $U_{X}$ , recall $$ P[0,1]:=\big\{ \sum_{i=0}^{n} \binom{n}{i}  x\big( \frac{i}{n} \big) t^{i}(1-t)^{n-i}: x\in U_{X}, n\in\mathbb{N}  \big\} , $$ which is a dense (and convex) subset of $U_{X}$ . It  Also, consider the sets $E[0,1]:=\{\exp(-nt):n\in\mathbb{N}\} $ and $B:=P[0,1]\cup E[0,1]$ . Then, it is easy to check that $B$ is almost convex but not $B$ convex. Now, let $C\subset B$ be any family of almost convex sets such that $A^{*}:=\cap_{A\in C}A\neq \emptyset$ . We consider 3 cases: Case 1:  There is $P_{0}[0,1]\subset P[0,1]$ such that $P_{0}[0,1]\subset A$ , $P_{0}[0,1]$ convex and dense in $A$ for each $A\in C$ . Then, $P_{0}[0,1]\subset A^{*}$ and therefore $A^{*}$ is almost convex. Case 2: $  A_{0}\subset E[0,1] \setminus P[0,1]$ for some $A_{0}\in C$ . Then, it is easy to check that $A_{0}$ is almost convex if, and only if, $A_{0}=\{x_{0}\}$ for some $x_{0}\in E[0,1]$ . As we are assuming that $A^{*}\neq\emptyset$ , we have that $A=\{x_{0}\}$ for each $A\in C$ and then, trivially, $A^{*}$ is almost convex. Case 3: There are $P_{0}\subset P[0,1]$ and $E_{0}[0,1]\subset E[0,1]$ such that $A_{0} = P_{0}[0,1]\cup E_{0}[0,1]$ for some $A_{0}\in C$ . I don't know how to prove (if it is true) that in Case 3 $A^{*}$ is almost convex. We can ""relax"" the assumptions by taking $S[0,1]:=\sin(2\pi t)$ (or any other non-polynomial function) ¿Some body can help? Somebody know an example of such set $B$ ? Many thanks in advance for your comments.","Let be a Banach space and its closed unit ball. According to  Himmelber, Fixed points of compact multifunctions , a subset is said to be almost convex if given if given there are such that and (""co"" denotes the convex hull). Of course, a convex set is almost convex. It is not very hard to show (for instance, in the Euclidean plane) that in general, the nonempty intersection of almost convex sets is not an almost convex set. I am looking for an example of a bounded and almost convex (but not convex) subset of (infinite dimensional), say , such that if is any family of almost convex subsets de with nonempty intersection, then is an almost set. I have tried with the following example. Example : Let be the Banach space, endowed its usual supremum norm, of real continuous functions defined in ,  and the Berstein polynomials of the functions belonging to , recall which is a dense (and convex) subset of . It  Also, consider the sets and . Then, it is easy to check that is almost convex but not convex. Now, let be any family of almost convex sets such that . We consider 3 cases: Case 1:  There is such that , convex and dense in for each . Then, and therefore is almost convex. Case 2: for some . Then, it is easy to check that is almost convex if, and only if, for some . As we are assuming that , we have that for each and then, trivially, is almost convex. Case 3: There are and such that for some . I don't know how to prove (if it is true) that in Case 3 is almost convex. We can ""relax"" the assumptions by taking (or any other non-polynomial function) ¿Some body can help? Somebody know an example of such set ? Many thanks in advance for your comments.","(X,\|\cdot\|) U_{X} B \subset X \varepsilon >0 \{x_{1},\ldots x_{n}\}\subset B \{z_{1},\ldots, z_{n}\}\subset B \|x_{i}-z_{i}\|\leq \varepsilon \mathrm{co}(z_{1},\ldots, z_{n})\subset B X B C B \cap_{A\in C}A\neq \emptyset X [0,1] P[0,1] U_{X} 
P[0,1]:=\big\{ \sum_{i=0}^{n} \binom{n}{i}  x\big( \frac{i}{n} \big) t^{i}(1-t)^{n-i}: x\in U_{X}, n\in\mathbb{N}  \big\} ,
 U_{X} E[0,1]:=\{\exp(-nt):n\in\mathbb{N}\}  B:=P[0,1]\cup E[0,1] B B C\subset B A^{*}:=\cap_{A\in C}A\neq \emptyset P_{0}[0,1]\subset P[0,1] P_{0}[0,1]\subset A P_{0}[0,1] A A\in C P_{0}[0,1]\subset A^{*} A^{*}   A_{0}\subset E[0,1] \setminus P[0,1] A_{0}\in C A_{0} A_{0}=\{x_{0}\} x_{0}\in E[0,1] A^{*}\neq\emptyset A=\{x_{0}\} A\in C A^{*} P_{0}\subset P[0,1] E_{0}[0,1]\subset E[0,1] A_{0} = P_{0}[0,1]\cup E_{0}[0,1] A_{0}\in C A^{*} S[0,1]:=\sin(2\pi t) B","['functional-analysis', 'banach-spaces', 'convex-geometry']"
21,"Would We have the preimage of at least one interval taken out from $[0;1]$, inside the any ball taken out from $X$?","Would We have the preimage of at least one interval taken out from , inside the any ball taken out from ?",[0;1] X,"This is the theorem: Let $X$ be separable metric space endowed with non-atomic Borel measure such that $\mu X = 1$ . Using this theorem We can establish isomorphism between $X$ and $[0;1]$ . Denote this mapping by $f$ . I want to show that for at least one positive-measured interval $I \subset [0;1], \: \:$$f^{-1}(I\setminus I’) \subset B$ , where $I’$ is the subset of $I$ , of which measure is equal to $0$ , i.e. $m(I’) = 0$ , where $B$ is any ball in $X$ with positive measure. To sum it up I have to show that Inside every positive-measured $B \subset X$ , We  would have the preimage of at least one subset of positive-measured interval taken out from $[0;1]$ , of which Lebesgue measure would be equal to the measure of taken interval. Is it possible to construct such isomorphic mapping or at least a measure-preserving mapping which would satisfy these conditions? Any help would be appreciated.","This is the theorem: Let be separable metric space endowed with non-atomic Borel measure such that . Using this theorem We can establish isomorphism between and . Denote this mapping by . I want to show that for at least one positive-measured interval , where is the subset of , of which measure is equal to , i.e. , where is any ball in with positive measure. To sum it up I have to show that Inside every positive-measured , We  would have the preimage of at least one subset of positive-measured interval taken out from , of which Lebesgue measure would be equal to the measure of taken interval. Is it possible to construct such isomorphic mapping or at least a measure-preserving mapping which would satisfy these conditions? Any help would be appreciated.","X \mu X = 1 X [0;1] f I \subset [0;1], \: \:f^{-1}(I\setminus I’) \subset B I’ I 0 m(I’) = 0 B X B \subset X [0;1]","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
22,"Show that the set $C := \left\{\left.\sum_{j=1}^\infty\lambda_j e_j\right|\, \lambda_j \in \mathbb{K},\,|\lambda_j| \leq a_j\right\}$ is compact in H.",Show that the set  is compact in H.,"C := \left\{\left.\sum_{j=1}^\infty\lambda_j e_j\right|\, \lambda_j \in \mathbb{K},\,|\lambda_j| \leq a_j\right\}","I am trying to work out question 8.17 from the book 'Functional Analysis: an elementary introduction' by Haase. The question is formulated as follows. Let $(e_j)_{j\geq 1}$ be an orthonormal system in a Hilbert space $H$ , and let $a_j \geq 0$ be scalars with $\sum_{j=1}^\infty a_j^2 < \infty$ . Show that the set \begin{align}     C := \left\{\left.\sum_{j=1}^\infty\lambda_j e_j\right|\, \lambda_j \in \mathbb{K},\,|\lambda_j| \leq a_j\right\} \end{align} is compact in $H$ . This is my attempted solution: I would go with the definition of sequential compactness here. My plan was either to construct a pairwise orthogonal sequence. Then, showing that $ \sum_{j=1}^\infty \|f_n\|^2 < \infty$ would imply that $\sum_j f_n$ converges in H. I thought about using Bessel's inequality, which turns out here to be an equality , since for some $f\in C$ we have \begin{align}     \langle f, e_k\rangle = \langle\sum_{j=1}^\infty \lambda_j e_j, e_k\rangle = \sum_{j=1}^\infty \lambda_j \langle e_j, e_k\rangle = \lambda_k. \end{align} Thus, we can write $f$ as follows \begin{align}     f = \sum_{j=1}^\infty\lambda_j e_j = \sum_{j=1}^\infty\langle f, e_j\rangle e_j = Pf, \end{align} the standard abstract Fourier series. From this, we can deduce that \begin{align}     \|f\|^2 = \sum_{j=1}^\infty |\langle f,e_j\rangle |^2 = \sum_{j=1}^\infty |\lambda_j|^2 \left(\leq \sum_{j=1}^\infty a_j^2 < \infty\right), \end{align} using Pythagoras. Now,  To show that $C$ is compact we have to show that every sequence in $C$ has a converging subsequence. Let $(f_n)_{n\geq 1}$ be a sequence in $C$ . Let $(f_{n_k})_{k\geq 1}$ be a subsequence, we need to show that $(f_{n_k})_{k\geq 1}$ converges to some $f$ in $C$ . Each $f_n$ is of the form \begin{align}     f_n = \sum_{j=1}^\infty \lambda_{n,j} e_{n, j}, \end{align} the coefficients, $\lambda_{n, \cdot}$ , depend on $n$ , and so does the 'positions', $e_{n, \cdot}$ , where we place these coefficients. The sequence $(f_n)_{n\geq 1}$ is not pairwise orthogonal. \begin{align}     \langle f_n, f_m\rangle = \left\langle \sum_{j=1}^\infty \lambda_{n, j}e_{n,j}, \sum_{k=1}^\infty \lambda_{m, k}e_{m,k}\right\rangle = \sum_{j=1}^\infty\sum_{k=1}^\infty \lambda_{n,j}\lambda_{m, k} \langle e_{n,j}, e_{m,k}\rangle \end{align} we can say nothing about the term $\langle e_{n,j}, e_{m,k}\rangle$ . We could also try to show that this set $C$ is closed in some other compact set $D$ . But then we would have to construct some set $D$ , and I do not really get far with this approach either. Any help is much appreciated!","I am trying to work out question 8.17 from the book 'Functional Analysis: an elementary introduction' by Haase. The question is formulated as follows. Let be an orthonormal system in a Hilbert space , and let be scalars with . Show that the set is compact in . This is my attempted solution: I would go with the definition of sequential compactness here. My plan was either to construct a pairwise orthogonal sequence. Then, showing that would imply that converges in H. I thought about using Bessel's inequality, which turns out here to be an equality , since for some we have Thus, we can write as follows the standard abstract Fourier series. From this, we can deduce that using Pythagoras. Now,  To show that is compact we have to show that every sequence in has a converging subsequence. Let be a sequence in . Let be a subsequence, we need to show that converges to some in . Each is of the form the coefficients, , depend on , and so does the 'positions', , where we place these coefficients. The sequence is not pairwise orthogonal. we can say nothing about the term . We could also try to show that this set is closed in some other compact set . But then we would have to construct some set , and I do not really get far with this approach either. Any help is much appreciated!","(e_j)_{j\geq 1} H a_j \geq 0 \sum_{j=1}^\infty a_j^2 < \infty \begin{align}
    C := \left\{\left.\sum_{j=1}^\infty\lambda_j e_j\right|\, \lambda_j \in \mathbb{K},\,|\lambda_j| \leq a_j\right\}
\end{align} H 
\sum_{j=1}^\infty \|f_n\|^2 < \infty \sum_j f_n f\in C \begin{align}
    \langle f, e_k\rangle = \langle\sum_{j=1}^\infty \lambda_j e_j, e_k\rangle = \sum_{j=1}^\infty \lambda_j \langle e_j, e_k\rangle = \lambda_k.
\end{align} f \begin{align}
    f = \sum_{j=1}^\infty\lambda_j e_j = \sum_{j=1}^\infty\langle f, e_j\rangle e_j = Pf,
\end{align} \begin{align}
    \|f\|^2 = \sum_{j=1}^\infty |\langle f,e_j\rangle |^2 = \sum_{j=1}^\infty |\lambda_j|^2 \left(\leq \sum_{j=1}^\infty a_j^2 < \infty\right),
\end{align} C C (f_n)_{n\geq 1} C (f_{n_k})_{k\geq 1} (f_{n_k})_{k\geq 1} f C f_n \begin{align}
    f_n = \sum_{j=1}^\infty \lambda_{n,j} e_{n, j},
\end{align} \lambda_{n, \cdot} n e_{n, \cdot} (f_n)_{n\geq 1} \begin{align}
    \langle f_n, f_m\rangle = \left\langle \sum_{j=1}^\infty \lambda_{n, j}e_{n,j}, \sum_{k=1}^\infty \lambda_{m, k}e_{m,k}\right\rangle = \sum_{j=1}^\infty\sum_{k=1}^\infty \lambda_{n,j}\lambda_{m, k} \langle e_{n,j}, e_{m,k}\rangle
\end{align} \langle e_{n,j}, e_{m,k}\rangle C D D","['functional-analysis', 'hilbert-spaces', 'compactness']"
23,If $f$ is compactly supported and belongs to $L^{p}$ does it also belong to $L^{q}$?,If  is compactly supported and belongs to  does it also belong to ?,f L^{p} L^{q},"I got to this result myself, just working with my understanding of $L^{p}$ spaces. I wanted to check with you if this is in fact true, or if I am making some mistake. Suppose that, for some $p \in [1,\infty]$ , $f \in L^{p}(\mathbb{R}^{d})$ is compactly supported (let us assume that its support lies within a compact set $K \subset \mathbb{R}^{d})$ . Is it true that $f \in L^{q}(\mathbb{R}^{d})$ for every $q< p$ ? It seems true by a simple Hölder's inequality argument: $$\|f\|_{q} = \bigg{(}\int dx |f(x)|^{q}\bigg{)}^{\frac{1}{q}} = \bigg{(}\int_{K}dx |f(x)|^{q}\bigg{)}^{\frac{1}{q}} = \bigg{(}\int_{K} dx 1^{\frac{p}{p-q}}\bigg{)}^{\frac{p}{p-q}}\bigg{(}\int_{K}dx|f(x)|^{q\frac{p}{q}}\bigg{)}^{\frac{q}{p}}$$ and the latter is finite because: $$\int_{K}dx 1^{\frac{p}{p-q}} = m(K) < \infty$$ where $m$ is the Lebesgue measure on $\mathbb{R}^{d}$ , which is finite because $K$ is compact and the other integral is finite because $f \in L^{p}(\mathbb{R}^{d})$ . Is this correct?","I got to this result myself, just working with my understanding of spaces. I wanted to check with you if this is in fact true, or if I am making some mistake. Suppose that, for some , is compactly supported (let us assume that its support lies within a compact set . Is it true that for every ? It seems true by a simple Hölder's inequality argument: and the latter is finite because: where is the Lebesgue measure on , which is finite because is compact and the other integral is finite because . Is this correct?","L^{p} p \in [1,\infty] f \in L^{p}(\mathbb{R}^{d}) K \subset \mathbb{R}^{d}) f \in L^{q}(\mathbb{R}^{d}) q< p \|f\|_{q} = \bigg{(}\int dx |f(x)|^{q}\bigg{)}^{\frac{1}{q}} = \bigg{(}\int_{K}dx |f(x)|^{q}\bigg{)}^{\frac{1}{q}} = \bigg{(}\int_{K} dx 1^{\frac{p}{p-q}}\bigg{)}^{\frac{p}{p-q}}\bigg{(}\int_{K}dx|f(x)|^{q\frac{p}{q}}\bigg{)}^{\frac{q}{p}} \int_{K}dx 1^{\frac{p}{p-q}} = m(K) < \infty m \mathbb{R}^{d} K f \in L^{p}(\mathbb{R}^{d})","['functional-analysis', 'analysis', 'lp-spaces']"
24,How to prove the resonate theorem using Gelfand Lemma.,How to prove the resonate theorem using Gelfand Lemma.,,"Here is the Gelfand lemma :Suppose $X$ is a Banach space, $p:X\to\mathbb{R}$ satisfies : $p(x)\ge 0$ ; $p(\lambda x)=\lambda p(x),\quad \forall \lambda>0$ ; $p(x_1+x_2)\le p(x_1)+p(x_2)$ ; $\liminf\limits_{n\to\infty}p(x_n)\ge p(x)$ where $x_n\to x$ . Then there exists $M>0$ such that $p(x)\le M||x||$ . I have proved it and try to use it to prove the uniform boundness principle : Let $X,Y$ be Banach spaces and $\{A_\lambda:\lambda\in\Lambda\}$ be a family of bounded linear operators from $X$ to $Y$ . If $\{A_\lambda:\lambda\in\Lambda\}$ is pointwise bounded on X, then $\{A_\lambda:\lambda\in\Lambda\}$ is uniformly bounded, i.e., $\sup∥T_λ∥<\infty$ . I considered to show that $\sup\limits_{\lambda\in\Lambda}||A_\lambda x||$ is the $p(x)$ in Gelfand Lemma but I failed.","Here is the Gelfand lemma :Suppose is a Banach space, satisfies : ; ; ; where . Then there exists such that . I have proved it and try to use it to prove the uniform boundness principle : Let be Banach spaces and be a family of bounded linear operators from to . If is pointwise bounded on X, then is uniformly bounded, i.e., . I considered to show that is the in Gelfand Lemma but I failed.","X p:X\to\mathbb{R} p(x)\ge 0 p(\lambda x)=\lambda p(x),\quad \forall \lambda>0 p(x_1+x_2)\le p(x_1)+p(x_2) \liminf\limits_{n\to\infty}p(x_n)\ge p(x) x_n\to x M>0 p(x)\le M||x|| X,Y \{A_\lambda:\lambda\in\Lambda\} X Y \{A_\lambda:\lambda\in\Lambda\} \{A_\lambda:\lambda\in\Lambda\} \sup∥T_λ∥<\infty \sup\limits_{\lambda\in\Lambda}||A_\lambda x|| p(x)","['functional-analysis', 'bilinear-operator']"
25,Uniqueness of Sturm-Liouville like problem,Uniqueness of Sturm-Liouville like problem,,"the following is an exercise taken from the written exam of the functional analysis course that I am following Let $f \, : \, [0,1] \times \mathbb{R} \to \mathbb{R}$ be a $C^1$ function that satisfies $$ \lim_{|\xi|\to \infty}{\frac{f(x,\xi)}{\xi}} = 0 $$ uniformly with respect to $x \in [0,1]$ . Prove that the problem $$ \begin{cases} &-u^{''} = f(x,u(x)) \\ &u(0) = u(1) = 0\end{cases} $$ has a unique classical solution I managed to prove the existence of the solution but I didn't managed to prove the uniqueness of the solution. I would like to demonstrate uniqueness by not using tools external to the course, whcih are : Sturm-Liouville theory, a bit of Sobolev spaces, Nemitski operators, Fourier transform, Functional calculus and spectral theorem, Lax-Milgram theorem, Schauder-Fixed point theorem, Implicit function theorem for Banach spaces, Inversion theorem for Banach spaces and Lagrange multipliers for Banach spaces. This is how I proved the existence. PROOF OF EXISTENCE : Consider the operator $\tilde{T} : L^2([0,1]) \to H^1_0{([0,1])}$ that maps $g \in L^2([0,1])$ in the unique solution $\tilde{T}(g) = v$ of the Sturm-Liouville problem $$\begin{cases} &-v^{''} = g \\&v(0) = v(1) = 0\end{cases}$$ one can prove that $\tilde{T}$ is continuos because multiplying by $\tilde{T}(g)$ , integrating and using integration by parts we find $$ \int_{0}^{1}{ g \cdot (\tilde{T}g) dx} = -\int_{0}^{1}{(\tilde{T}g)^{''}\cdot(\tilde{T}g)dx} = \int_{0}^{1}{ (\tilde{T}g)^2 dx}$$ Combining this with the Cauchy-Schwarz inequality we find ( from now on I will omit $([0,1])$ in the functional spaces for simplicity ) $$ ||(\tilde{T}g)' ||^2_{L^2} \leq ||(\tilde{T}g)' ||_{L^2}||g ||_{L^2}$$ dividing we find $$ ||(\tilde{T}g)' ||_{L^2} \leq ||g ||_{L^2}$$ which proves the continuity by the Poincarè inequality. Now let $i \; : \; H^1_0 \to L^2$ be the inclusion operator, we know it is compact thus $T = i \circ \tilde{T}$ is compact Consider the Nemitski non-linear operator $L^2 \ni u \mapsto Gf = f(x,u(x)) \in L^2$ ( I won't prove $Gf$ is in $L^2$ since we did it in class ) Now let $$\begin{split} F \; : \; L^2([0,1]) &\to L^2([0,1])\\ L^2 \ni u &\mapsto F(u) := T(G(u)) \in L^2 \end{split}$$ it's easy to show that $u$ solves the problem if and only if $F(u) = u$ . I show an important inequality for $G$ , let $N > 0$ , let $$M = M(N) := \max_{\substack{%  			0 \leq x \leq 1\\  			|y| \leq N}}{|f(x,y)|} \hspace{0.5cm} \epsilon = \epsilon(N) := \sup_{\substack{%  			0 \leq x \leq 1\\  			|y| \geq N}}{\frac{|f(x,\xi)|}{|\xi|}}$$ clearly $M = M(N)$ is finite for all $N > 0$ and $\lim_{N \to \infty}{\epsilon(N)} = 0$ Then we find $$||G(u)||^2_{L^2} = \int_{0}^{1}{|f(x,u(x))|^2 dx} = \int_{ |u(x)| \leq N }{ |f(x,u(x)|^2 dx} + \int_{|u(x)| > N}{ |f(x,u(x)|^2 dx} \leq \int_{ |u(x)| \leq N }{ M^2 dx} + \int_{ |u(x)| > N }{ \epsilon^2 \cdot |u(x)|^2 dx} \leq  M^2 + \epsilon^2 ||u||^2_{L^2}$$ this shows that $G$ sends bounded sets in bounded sets, therefore $F = T \circ G$ is compact since $T$ is compact. Now let $N >> 0$ and $R >> 0$ ( $R$ possibly depending on $N$ ) be such that $||T||^2(M^2 + \epsilon^2 R^2) \leq R^2$ then since $||F(u)||_{L^2} \leq ||T||\cdot||G(u)||_{L^2}$ I have that the set $$D := \{ u \in L^2([0,1]) \; : \; ||u||_{L^2} \leq R \}$$ is mapped into itself by $F$ , meaning $||u||_{L^2} \leq R \implies ||F(u)||_{L^2} \leq R$ , therefore if I restrict $F$ to $D$ by Schauder fixed point theorem there exists $u \in L^2$ such that $F(u) = u$ , which proves the existence I didn't managed to make any progress from here","the following is an exercise taken from the written exam of the functional analysis course that I am following Let be a function that satisfies uniformly with respect to . Prove that the problem has a unique classical solution I managed to prove the existence of the solution but I didn't managed to prove the uniqueness of the solution. I would like to demonstrate uniqueness by not using tools external to the course, whcih are : Sturm-Liouville theory, a bit of Sobolev spaces, Nemitski operators, Fourier transform, Functional calculus and spectral theorem, Lax-Milgram theorem, Schauder-Fixed point theorem, Implicit function theorem for Banach spaces, Inversion theorem for Banach spaces and Lagrange multipliers for Banach spaces. This is how I proved the existence. PROOF OF EXISTENCE : Consider the operator that maps in the unique solution of the Sturm-Liouville problem one can prove that is continuos because multiplying by , integrating and using integration by parts we find Combining this with the Cauchy-Schwarz inequality we find ( from now on I will omit in the functional spaces for simplicity ) dividing we find which proves the continuity by the Poincarè inequality. Now let be the inclusion operator, we know it is compact thus is compact Consider the Nemitski non-linear operator ( I won't prove is in since we did it in class ) Now let it's easy to show that solves the problem if and only if . I show an important inequality for , let , let clearly is finite for all and Then we find this shows that sends bounded sets in bounded sets, therefore is compact since is compact. Now let and ( possibly depending on ) be such that then since I have that the set is mapped into itself by , meaning , therefore if I restrict to by Schauder fixed point theorem there exists such that , which proves the existence I didn't managed to make any progress from here","f \, : \, [0,1] \times \mathbb{R} \to \mathbb{R} C^1  \lim_{|\xi|\to \infty}{\frac{f(x,\xi)}{\xi}} = 0  x \in [0,1]  \begin{cases} &-u^{''} = f(x,u(x)) \\ &u(0) = u(1) = 0\end{cases}  \tilde{T} : L^2([0,1]) \to H^1_0{([0,1])} g \in L^2([0,1]) \tilde{T}(g) = v \begin{cases} &-v^{''} = g \\&v(0) = v(1) = 0\end{cases} \tilde{T} \tilde{T}(g)  \int_{0}^{1}{ g \cdot (\tilde{T}g) dx} = -\int_{0}^{1}{(\tilde{T}g)^{''}\cdot(\tilde{T}g)dx} = \int_{0}^{1}{ (\tilde{T}g)^2 dx} ([0,1])  ||(\tilde{T}g)' ||^2_{L^2} \leq ||(\tilde{T}g)' ||_{L^2}||g ||_{L^2}  ||(\tilde{T}g)' ||_{L^2} \leq ||g ||_{L^2} i \; : \; H^1_0 \to L^2 T = i \circ \tilde{T} L^2 \ni u \mapsto Gf = f(x,u(x)) \in L^2 Gf L^2 \begin{split} F \; : \; L^2([0,1]) &\to L^2([0,1])\\ L^2 \ni u &\mapsto F(u) := T(G(u)) \in L^2 \end{split} u F(u) = u G N > 0 M = M(N) := \max_{\substack{%
 			0 \leq x \leq 1\\
 			|y| \leq N}}{|f(x,y)|} \hspace{0.5cm} \epsilon = \epsilon(N) := \sup_{\substack{%
 			0 \leq x \leq 1\\
 			|y| \geq N}}{\frac{|f(x,\xi)|}{|\xi|}} M = M(N) N > 0 \lim_{N \to \infty}{\epsilon(N)} = 0 ||G(u)||^2_{L^2} = \int_{0}^{1}{|f(x,u(x))|^2 dx} = \int_{ |u(x)| \leq N }{ |f(x,u(x)|^2 dx} + \int_{|u(x)| > N}{ |f(x,u(x)|^2 dx} \leq \int_{ |u(x)| \leq N }{ M^2 dx} + \int_{ |u(x)| > N }{ \epsilon^2 \cdot |u(x)|^2 dx} \leq  M^2 + \epsilon^2 ||u||^2_{L^2} G F = T \circ G T N >> 0 R >> 0 R N ||T||^2(M^2 + \epsilon^2 R^2) \leq R^2 ||F(u)||_{L^2} \leq ||T||\cdot||G(u)||_{L^2} D := \{ u \in L^2([0,1]) \; : \; ||u||_{L^2} \leq R \} F ||u||_{L^2} \leq R \implies ||F(u)||_{L^2} \leq R F D u \in L^2 F(u) = u","['functional-analysis', 'fixed-point-theorems', 'weak-derivatives', 'sturm-liouville']"
26,Proving that $(H_0^1)^\perp = H^\infty$,Proving that,(H_0^1)^\perp = H^\infty,"The question is clear. Note that $H_0^1 =\{ g \in H^1 : g(0)=0\}$ , and $f \in H^1$ if $\sup \frac{1}{2\pi} \int \lvert f(re^{i \theta}) d \theta < \infty$ , and $H^\infty$ contains all bounded analytic functions. What I found that might work for this is the following: If $Y=H_0^1$ , by the definition, $Y^\perp = \{ f \in L^\infty : \langle g, f\rangle =0, \forall g \in H_0^1\}$ . Note that $z^n=e^{i n \theta}$ belongs to $H_0^1$ when $n>0$ . So integrating against it, it follows that any function in the dual must have all coefficients $a_m=0$ , for $m<0$ . However, a function in $L^{\infty}$ with that property is analytic so it is in $H^{\infty}$ , and if $f \in H^\infty$ , with the same reasoning it lies in $Y^\perp$ and hence, $Y^\perp =H^\infty$ . However, it doesn't seem to be clear to myself and I still don't see the part where we conclude the functions are analytic and hence lie in $H_0^1$ .","The question is clear. Note that , and if , and contains all bounded analytic functions. What I found that might work for this is the following: If , by the definition, . Note that belongs to when . So integrating against it, it follows that any function in the dual must have all coefficients , for . However, a function in with that property is analytic so it is in , and if , with the same reasoning it lies in and hence, . However, it doesn't seem to be clear to myself and I still don't see the part where we conclude the functions are analytic and hence lie in .","H_0^1 =\{ g \in H^1 : g(0)=0\} f \in H^1 \sup \frac{1}{2\pi} \int \lvert f(re^{i \theta}) d \theta < \infty H^\infty Y=H_0^1 Y^\perp = \{ f \in L^\infty : \langle g, f\rangle =0, \forall g \in H_0^1\} z^n=e^{i n \theta} H_0^1 n>0 a_m=0 m<0 L^{\infty} H^{\infty} f \in H^\infty Y^\perp Y^\perp =H^\infty H_0^1","['functional-analysis', 'complex-analysis']"
27,Is every weighted shift on $\ell^2$ unitarily equivalent to a weighted shift with positive weights?,Is every weighted shift on  unitarily equivalent to a weighted shift with positive weights?,\ell^2,"Consider $\mathbf t = \{t_n\}_{n=1}^\infty \subset \Bbb C$ , and define the left weighted shift operator $W_{\mathbf t}: \ell^2 \to \ell^2$ by $$W_{\mathbf t}(x_1,x_2,\ldots) = (t_1x_2,t_2x_3,\ldots)$$ Is $W_{\mathbf t}$ unitarily equivalent to some left weighted shift $W_{\mathbf p}$ where $\mathbf p = \{p_n\}_{n=1}^\infty \subset \Bbb R_{>0}$ ? The notation $\Bbb R_{>0}$ is used for positive real numbers. Firstly, I wrote $t_j = r_j e^{i\theta_j}$ for every $j\ge 1$ , i.e., converted to polar coordinates. My natural guess is that $W_{\mathbf t}$ and $W_{\mathbf r}$ should be unitarily equivalent, where $\mathbf r = \{r_n\}_{n=1}^\infty \subset \Bbb R_{\ge 0}$ . The $r_j$ 's can be zero, so even after establishing this equivalence, we must find an appropriate candidate for $\mathbf p$ . Thanks for any help! I'd appreciate hints or solutions.","Consider , and define the left weighted shift operator by Is unitarily equivalent to some left weighted shift where ? The notation is used for positive real numbers. Firstly, I wrote for every , i.e., converted to polar coordinates. My natural guess is that and should be unitarily equivalent, where . The 's can be zero, so even after establishing this equivalence, we must find an appropriate candidate for . Thanks for any help! I'd appreciate hints or solutions.","\mathbf t = \{t_n\}_{n=1}^\infty \subset \Bbb C W_{\mathbf t}: \ell^2 \to \ell^2 W_{\mathbf t}(x_1,x_2,\ldots) = (t_1x_2,t_2x_3,\ldots) W_{\mathbf t} W_{\mathbf p} \mathbf p = \{p_n\}_{n=1}^\infty \subset \Bbb R_{>0} \Bbb R_{>0} t_j = r_j e^{i\theta_j} j\ge 1 W_{\mathbf t} W_{\mathbf r} \mathbf r = \{r_n\}_{n=1}^\infty \subset \Bbb R_{\ge 0} r_j \mathbf p","['functional-analysis', 'operator-theory']"
28,$A$ is dense in $A''$ in the strong topology (Murphy's book),is dense in  in the strong topology (Murphy's book),A A'',"In Lemma 4.1.4. Murphy is doing something strange that I don't seem to understand. for each $x\in H$ we find $v_n(x)$ such that it converges to $u(x)$ . we wish to prove that is holds for EVERY $x$ . Murphy goes on to make this construction on $H^n$ so that in the end he can take $W$ (neighbourhood of $u$ in the strong topology) and show that there there is at least some $v_n(x)$ so it is indeed the sot closure. What I don't understand is why he uses this $x_1,...x_n$ what is he trying to say? surely not that $W$ is a sot neighbourhood of $u$ if $|(t-u)x_j|<\epsilon$ for finitely many $x_j$ ?! Can someone illuminate this argument, please?","In Lemma 4.1.4. Murphy is doing something strange that I don't seem to understand. for each we find such that it converges to . we wish to prove that is holds for EVERY . Murphy goes on to make this construction on so that in the end he can take (neighbourhood of in the strong topology) and show that there there is at least some so it is indeed the sot closure. What I don't understand is why he uses this what is he trying to say? surely not that is a sot neighbourhood of if for finitely many ?! Can someone illuminate this argument, please?","x\in H v_n(x) u(x) x H^n W u v_n(x) x_1,...x_n W u |(t-u)x_j|<\epsilon x_j","['functional-analysis', 'operator-theory', 'c-star-algebras', 'von-neumann-algebras']"
29,An $\ell^qL^p$ inequality: $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}\leq \|{\varphi}\|_{L^2}$.,An  inequality: .,\ell^qL^p \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}\leq \|{\varphi}\|_{L^2},"An $\ell^qL^p$ inequality: $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}\leq \|{\varphi}\|_{L^2}$ . The dimensions I care about are $d=2$ and $d=3$ . We define $\ell^qL^p$ in the following way. For $\alpha\in \mathbb Z^d$ , let $\square_\alpha$ be the unit cube in $\mathbb R^d$ with centre at $\alpha$ and let $\chi_\alpha$ be its characteristic function. Define $\ell^q(L^p)$ to be the set of functions for which $$\|{f}\|_{\ell^qL^p}:=\left({\sum_\alpha\|{f\chi_\alpha}\|^q_{L^p}}\right)^{\frac{1}{q}}=\left({\sum_\alpha\left({\int_{\square_\alpha}|{f(x)}|^p\mathrm{d}x}\right)^{\frac{p}{q}}}\right)^{\frac{1}{q}}$$ is finite. The question says to use the Hölder inequality and the Sobolev inequality. Idea of proof: Using $L^p$ interpolation, $\frac{1}{4}=\frac{\frac{1}{2}}{2}+\frac{\frac{1}{2}}{\infty}$ , so \begin{align*}\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}= \left({\sum_\alpha\|{\nabla\langle\nabla\rangle^{-2}\varphi\chi_\alpha}\|^2_{L^4}}\right)^{\frac{1}{2}}&\leq\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}^{\frac{1}{2}}\left({\sum_\alpha\|{\nabla\langle\nabla\rangle^{-2}\varphi\chi_\alpha}\|^2_{L^2}}\right)^{\frac{1}{2}}\\ &= \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}^{\frac{1}{2}}\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^2}^{\frac{1}{2}}.\end{align*} Now $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^2}\leq\|\varphi\|_{L^2}$ which is easily seen on the Fourier side. I'm not sure how to prove $\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}$ is bounded by $\|\varphi\|_{L^2}$ (I'm not even sure if this is true).","An inequality: . The dimensions I care about are and . We define in the following way. For , let be the unit cube in with centre at and let be its characteristic function. Define to be the set of functions for which is finite. The question says to use the Hölder inequality and the Sobolev inequality. Idea of proof: Using interpolation, , so Now which is easily seen on the Fourier side. I'm not sure how to prove is bounded by (I'm not even sure if this is true).","\ell^qL^p \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}\leq \|{\varphi}\|_{L^2} d=2 d=3 \ell^qL^p \alpha\in \mathbb Z^d \square_\alpha \mathbb R^d \alpha \chi_\alpha \ell^q(L^p) \|{f}\|_{\ell^qL^p}:=\left({\sum_\alpha\|{f\chi_\alpha}\|^q_{L^p}}\right)^{\frac{1}{q}}=\left({\sum_\alpha\left({\int_{\square_\alpha}|{f(x)}|^p\mathrm{d}x}\right)^{\frac{p}{q}}}\right)^{\frac{1}{q}} L^p \frac{1}{4}=\frac{\frac{1}{2}}{2}+\frac{\frac{1}{2}}{\infty} \begin{align*}\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{\ell^2L^4}= \left({\sum_\alpha\|{\nabla\langle\nabla\rangle^{-2}\varphi\chi_\alpha}\|^2_{L^4}}\right)^{\frac{1}{2}}&\leq\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}^{\frac{1}{2}}\left({\sum_\alpha\|{\nabla\langle\nabla\rangle^{-2}\varphi\chi_\alpha}\|^2_{L^2}}\right)^{\frac{1}{2}}\\
&= \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty}^{\frac{1}{2}}\|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^2}^{\frac{1}{2}}.\end{align*} \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^2}\leq\|\varphi\|_{L^2} \|\nabla\langle\nabla\rangle^{-2}\varphi\|_{L^\infty} \|\varphi\|_{L^2}","['linear-algebra', 'functional-analysis', 'analysis', 'partial-differential-equations', 'quantum-mechanics']"
30,Are $L^2$-closed subspaces of $C^\infty$ finite-dimensional?,Are -closed subspaces of  finite-dimensional?,L^2 C^\infty,"I have the following question: Given a closed Riemannian manifold $M$ , let $V\subseteq C^\infty(M)$ be a linear subspace that is closed with respect to the $L^2$ -norm. The question now is whether $V$ is finite-dimensional. My idea is to use Rellich's embedding theorem together with the open mapping theorem: Denote by $\iota : H^m(M) \rightarrow L^2(M)$ , $m \in \mathbb{N}$ the canonical inclusion, by Rellich this is a compact injective map. The restriction of $\iota$ to $V$ gives a bijection $\iota\lvert_V: (V, \lvert\lvert\cdot\rvert \rvert_{H^m})\rightarrow (V,  \lvert\lvert\cdot\rvert \rvert_{L^2})$ . If I now can show that $V$ is also closed with respect to the $H^m$ -norm, I can use the open mapping theorem to get that the inverse $(\iota\lvert_V)^{-1}:(V,  \lvert\lvert\cdot\rvert \rvert_{L^2})\rightarrow (V, \lvert\lvert\cdot\rvert \rvert_{H^m})$ is continous. In this case, the identity of $V$ factors through $H^m$ , i.e. $id_V = \iota\lvert_V \circ (\iota\lvert_V)^{-1}$ and hence it is a compact map. This then implies that $V$ is finite-dimensional. The missing puzzle piece is therefore to show that $V$ is closed as a subspace of $H^m$ (doesn't matter for which $m$ ). I'm grateful for any hint on how to prove this, less grateful for a counterexample, but at least then I can stop thinking about this ;) Thanks!","I have the following question: Given a closed Riemannian manifold , let be a linear subspace that is closed with respect to the -norm. The question now is whether is finite-dimensional. My idea is to use Rellich's embedding theorem together with the open mapping theorem: Denote by , the canonical inclusion, by Rellich this is a compact injective map. The restriction of to gives a bijection . If I now can show that is also closed with respect to the -norm, I can use the open mapping theorem to get that the inverse is continous. In this case, the identity of factors through , i.e. and hence it is a compact map. This then implies that is finite-dimensional. The missing puzzle piece is therefore to show that is closed as a subspace of (doesn't matter for which ). I'm grateful for any hint on how to prove this, less grateful for a counterexample, but at least then I can stop thinking about this ;) Thanks!","M V\subseteq C^\infty(M) L^2 V \iota : H^m(M) \rightarrow L^2(M) m \in \mathbb{N} \iota V \iota\lvert_V: (V, \lvert\lvert\cdot\rvert \rvert_{H^m})\rightarrow (V,  \lvert\lvert\cdot\rvert \rvert_{L^2}) V H^m (\iota\lvert_V)^{-1}:(V,  \lvert\lvert\cdot\rvert \rvert_{L^2})\rightarrow (V, \lvert\lvert\cdot\rvert \rvert_{H^m}) V H^m id_V = \iota\lvert_V \circ (\iota\lvert_V)^{-1} V V H^m m","['functional-analysis', 'partial-differential-equations', 'global-analysis']"
31,"Does it always happen $\|ax+by\|=1 \implies |a+b|\leq 1?$ for $\|x\|=\|y\|=1$ (where $a, b\in \mathbb{R}$)",Does it always happen  for  (where ),"\|ax+by\|=1 \implies |a+b|\leq 1? \|x\|=\|y\|=1 a, b\in \mathbb{R}","I am new in functional analysis. I was reading a theory related to unit ball in Banach space. While studying the norm attainment set of points I met with a confusion: For a finite dimensional real Banach space $\mathbb{X}$ and for $\|x\|=\|y\|=1$ , do always $\|ax+by\|=1 \implies |a+b|\leq 1? $ (where $a, b\in \mathbb{R}$ ) I think the answer is yes. Also I have a doubt on whether the statement is true in any normed linear space. I have tried with my limited knowledge to prove it by contradiction i.e., by considering $|a+b|>1$ trying to find two distinct elements $x, y$ with $\|x\|=\|y\|=1$ such that $\|ax+by\|\ne 1$ but I could not able to proceed. Please help me. Thank you in advance. Thank you all for the quick suggestions but all the examples are in Hilbert space. What will happen if $\mathbb{X}$ is not a Hilbert space? Is the statement still wrong?","I am new in functional analysis. I was reading a theory related to unit ball in Banach space. While studying the norm attainment set of points I met with a confusion: For a finite dimensional real Banach space and for , do always (where ) I think the answer is yes. Also I have a doubt on whether the statement is true in any normed linear space. I have tried with my limited knowledge to prove it by contradiction i.e., by considering trying to find two distinct elements with such that but I could not able to proceed. Please help me. Thank you in advance. Thank you all for the quick suggestions but all the examples are in Hilbert space. What will happen if is not a Hilbert space? Is the statement still wrong?","\mathbb{X} \|x\|=\|y\|=1 \|ax+by\|=1 \implies |a+b|\leq 1?  a, b\in \mathbb{R} |a+b|>1 x, y \|x\|=\|y\|=1 \|ax+by\|\ne 1 \mathbb{X}","['linear-algebra', 'functional-analysis', 'normed-spaces', 'banach-spaces']"
32,Trying to see that operator space injective norm is dominated by tmax norm.,Trying to see that operator space injective norm is dominated by tmax norm.,,"A ternary ring of operator (TRO) between two complex Hilbert spaces $H$ and $K$ is defined to be a norm closed subspace $V$ of $B(H, K)$ and satisfies $xy^*z \in V$ for all $x, y, z \in V$ . Let $V$ and $W$ be TROs. A linear map $\pi: V \to W$ is called TRO-homomorphism provided $\pi(xy^*z) =\pi(x) \pi(y) ^*\pi(z) $ for all $x, y, z \in V$ . Motivated by $C^{\ast}$ -algebras, in section $5$ of Kaur and Ruan - Local Properties of Ternary Rings of Operators and Their Linking $C^*$ -Algebras , tmax norm $\|.\|_{\text{tmax}}$ on $V \otimes W$ is defined as follows. For $u= \sum_{i=1}^r v_i \otimes w_i \in V \otimes W$ , the tmax norm $\lVert u\rVert_{\text{tmax}}$ is the supremum of the norm $\| \pi.\sigma(u)\|_{B(H)}=\lVert\sum_{i=1}^{r} \pi(v_i) \sigma(w_{i})\rVert_{B(H)}$ over all pairs of TRO-homomorphism $\pi: V \to B(H)$ , $\sigma: W \to B(H)$ satisfying following commuting conditions $ \pi(v) \sigma(w)=\sigma(w) \pi(v)$ and $ \pi(v) \sigma(w)^*=\sigma(w)^* \pi(v)$ . In the paper Section $5$ , equation $5.2$ following is stated without proof $\| u\|_{\vee} \leq \|u\|_{\text{tmax}}\leq \|u\|_{\wedge}$ where $\|.\|_{\vee}$ and $\|.\|_{\wedge}$ denotes the operator space injective and projective norm respectively. In the paper it is not explained that why $\| u\|_{\vee} \leq \|u\|_{\text{tmax}}$ . I am unable to see this. Can someone please explain this to me? P. S: This question has been posted on mathoverflow also and can be found here .","A ternary ring of operator (TRO) between two complex Hilbert spaces and is defined to be a norm closed subspace of and satisfies for all . Let and be TROs. A linear map is called TRO-homomorphism provided for all . Motivated by -algebras, in section of Kaur and Ruan - Local Properties of Ternary Rings of Operators and Their Linking -Algebras , tmax norm on is defined as follows. For , the tmax norm is the supremum of the norm over all pairs of TRO-homomorphism , satisfying following commuting conditions and . In the paper Section , equation following is stated without proof where and denotes the operator space injective and projective norm respectively. In the paper it is not explained that why . I am unable to see this. Can someone please explain this to me? P. S: This question has been posted on mathoverflow also and can be found here .","H K V B(H, K) xy^*z \in V x, y, z \in V V W \pi: V \to W \pi(xy^*z) =\pi(x) \pi(y) ^*\pi(z)  x, y, z \in V C^{\ast} 5 C^* \|.\|_{\text{tmax}} V \otimes W u= \sum_{i=1}^r v_i \otimes w_i \in V \otimes W \lVert u\rVert_{\text{tmax}} \| \pi.\sigma(u)\|_{B(H)}=\lVert\sum_{i=1}^{r} \pi(v_i) \sigma(w_{i})\rVert_{B(H)} \pi: V \to B(H) \sigma: W \to B(H)  \pi(v) \sigma(w)=\sigma(w) \pi(v)  \pi(v) \sigma(w)^*=\sigma(w)^* \pi(v) 5 5.2 \| u\|_{\vee} \leq \|u\|_{\text{tmax}}\leq \|u\|_{\wedge} \|.\|_{\vee} \|.\|_{\wedge} \| u\|_{\vee} \leq \|u\|_{\text{tmax}}","['functional-analysis', 'operator-theory', 'operator-algebras']"
33,Find adjoint to integral operator from $H^1$ to $L_2$,Find adjoint to integral operator from  to,H^1 L_2,"Let $k(x, y): \mathbb{R}^2 \to \mathbb{R}$ be a kernel and $T: H^1(a, b) \to L_2(c, d)$ $$ T u(x) = \int\limits_{a}^{b} k(x, s) u(s) ds$$ Find the adjoint operator $T^*$ . It is easy to see the if $T: L_2(a, b) \to L_2(c, d)$ , then $T^*$ can be expressed as $$ T^*v(s) = \int\limits_{c}^{d} k(x, s) v(x) dx$$ However, here we need $$ (Tu, v)_{L_2(c, d)} = (u, T^*v)_{H^1(a, b)} = (u, T^*v)_{L_2(a, b)} + (u', (T^*v)')_{L_2(a, b)}$$ I have tried to use the identity $$ \int\limits_{a}^{b} uv dx = \int\limits_{a}^{b} u'v dx + \int\limits_{a}^{b} uv' dx $$ Then $$ \int\limits_{c}^{d} \int\limits_{a}^{b} k(x, s) u(s) v(x) ds dx =  \int\limits_{a}^{b} u(s) \left[ \int\limits_{c}^{d} k'_s(x, s) v(x) dx \right] ds + \int\limits_{a}^{b} u'(s) \left[ \int\limits_{c}^{d} k(x, s) v(x) dx \right] ds $$ And it does not look like a form of adjoint operator... Thank you in advance for any help!","Let be a kernel and Find the adjoint operator . It is easy to see the if , then can be expressed as However, here we need I have tried to use the identity Then And it does not look like a form of adjoint operator... Thank you in advance for any help!","k(x, y): \mathbb{R}^2 \to \mathbb{R} T: H^1(a, b) \to L_2(c, d)  T u(x) = \int\limits_{a}^{b} k(x, s) u(s) ds T^* T: L_2(a, b) \to L_2(c, d) T^*  T^*v(s) = \int\limits_{c}^{d} k(x, s) v(x) dx  (Tu, v)_{L_2(c, d)} = (u, T^*v)_{H^1(a, b)} = (u, T^*v)_{L_2(a, b)} + (u', (T^*v)')_{L_2(a, b)}  \int\limits_{a}^{b} uv dx = \int\limits_{a}^{b} u'v dx + \int\limits_{a}^{b} uv' dx   \int\limits_{c}^{d} \int\limits_{a}^{b} k(x, s) u(s) v(x) ds dx = 
\int\limits_{a}^{b} u(s) \left[ \int\limits_{c}^{d} k'_s(x, s) v(x) dx \right] ds +
\int\limits_{a}^{b} u'(s) \left[ \int\limits_{c}^{d} k(x, s) v(x) dx \right] ds ","['functional-analysis', 'adjoint-operators', 'integral-operators']"
34,"Showing that a sequence is Cauchy on $C[0,1]$.",Showing that a sequence is Cauchy on .,"C[0,1]","Let $$f_k(t) = \begin{cases}0, \text{ if } t \in [0,1/2]\\ 1, \text{ if } t \in [1/2 + 1/k, 1].\end{cases}$$ We will prove the sequence $(f_k)_{k \in \mathbb{N}}$ is Cauchy on $(C[0,1],\|\cdot\|_p)$ for $1 \leq p < \infty$ . (this can also be done for $\infty$ but for the sake of the exercise we shall omit that case). Notice that \begin{align} \|f_n-f_n\|_p & = \left(\int_0^1|f_n(t)-f_m(t)|^pdt\right)^{1/p}\\ & \leq 2\left(\int_{1/2+1/n}^1|f_n(t)|^pdt - \int_{1/2 + 1/m}^1|f_m(t)|^pdt\right)^{1/p}\\ & \leq 2\left(\int_{1/2+1/n}^1dt - \int_{1/2+1/m}^1dt\right)^{1/p}\\ & \leq 2\left(1-1/2-1/n - 1 + 1/2 + 1/m\right)^{1/p}\\ & = 2\left(\frac{1}{m} - \frac{1}{n}\right)^{1/p}. \end{align} And when $n,m \rightarrow \infty$ we have $\|f_n-f_m\| \rightarrow 0$ . Rigorously, we could pick $N > 0$ so that when choosing $n>m\geq N$ we have $1/m-1/n < \epsilon$ . Can anyone verify if my reasoning is correct? Thanks in advance!!","Let We will prove the sequence is Cauchy on for . (this can also be done for but for the sake of the exercise we shall omit that case). Notice that And when we have . Rigorously, we could pick so that when choosing we have . Can anyone verify if my reasoning is correct? Thanks in advance!!","f_k(t) = \begin{cases}0, \text{ if } t \in [0,1/2]\\
1, \text{ if } t \in [1/2 + 1/k, 1].\end{cases} (f_k)_{k \in \mathbb{N}} (C[0,1],\|\cdot\|_p) 1 \leq p < \infty \infty \begin{align}
\|f_n-f_n\|_p & = \left(\int_0^1|f_n(t)-f_m(t)|^pdt\right)^{1/p}\\
& \leq 2\left(\int_{1/2+1/n}^1|f_n(t)|^pdt - \int_{1/2 + 1/m}^1|f_m(t)|^pdt\right)^{1/p}\\
& \leq 2\left(\int_{1/2+1/n}^1dt - \int_{1/2+1/m}^1dt\right)^{1/p}\\
& \leq 2\left(1-1/2-1/n - 1 + 1/2 + 1/m\right)^{1/p}\\
& = 2\left(\frac{1}{m} - \frac{1}{n}\right)^{1/p}.
\end{align} n,m \rightarrow \infty \|f_n-f_m\| \rightarrow 0 N > 0 n>m\geq N 1/m-1/n < \epsilon","['functional-analysis', 'complete-spaces']"
35,Inequality needed in molecular dynamics,Inequality needed in molecular dynamics,,"$\newcommand{\f}[2]{\frac{#1}{#2}}$ $\newcommand{\nor}[2]{ \left| \! \left| #1 \right| \! \right|_{#2} }$ $\newcommand{\ab}[1]{\left|#1\right|}$ $\newcommand{\R}{\mathbb{R}}$ $\newcommand{\pa}[1]{\left( #1 \right)}$ $\def\1{1}$ Consider the norm \begin{align*} 	\nor{V}{L^{\f{3}{2}}(\R^3) + L^\infty(\R^3)} := {\underset{\substack{V_{3/2} \in L^{\f{3}{2}  }(\R^3) \\ V_{\infty} \in L^\infty(\R^3) \\ V = V_{3/2} + V_\infty}}{\text{inf}}\quad} \pa{\nor{V_{3/2}}{L^{\f{3}{2}}} + \nor{V_\infty}{L^\infty(\R^3)}} \end{align*} which defines a Banach space. It enables to treat local singularities since $\ab{\cdot}^{-p} \in L^{\f{3}{2}} + L^\infty$ for any $p < 2$ , by decomposing $\ab{\cdot}^{-p} = \ab{\cdot}^{-p} \1_{\ab{\cdot} \le c}  + \ab{\cdot}^{-p}\1_{\ab{\cdot} \ge c}$ (the first function is in $L^{\f{3}{2}}$ and the second in $L^{\infty}$ ) but $\ab{\cdot}^{-p} \notin L^{\f{3}{2}}$ . But $\ab{\cdot}^{-2} \notin L^{\f{3}{2}} + L^\infty$ . My question is, do we have \begin{align*} 	\nor{ \ab{r-\cdot}^{-1} - \ab{\cdot}^{-1}  }{L^{\f{3}{2}} + L^\infty} \le c \ab{r}  \end{align*} for any $r \in \R^3$ , where $c$ does not depend on $r$ ? Of course it's when $r$ is small that there is an issue. At first sight, it is natural to do \begin{align}\label{eqq} 	\ab{\ab{r-x}^{-1} - \ab{x}^{-1}} &= \ab{\ab{r-x} - \ab{x}}\ab{r-x}^{-1}\ab{x}^{-1} \nonumber \\ 						 & \le \ab{r} \ab{r-x}^{-1}\ab{x}^{-1} \end{align} but then $\nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty}$ is not bounded in $r$ when $r$ is small, since \begin{align*} \nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} \rightarrow +\infty \end{align*} when $\ab{r} \rightarrow 0$ . But for any $\varepsilon > 0$ small, \begin{align*} \ab{r}^\varepsilon \nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} \rightarrow 0 \end{align*} So is there an approach, which is finer and which yields the result ? Or what is the lowest singularity, that we cannot remove ?","Consider the norm which defines a Banach space. It enables to treat local singularities since for any , by decomposing (the first function is in and the second in ) but . But . My question is, do we have for any , where does not depend on ? Of course it's when is small that there is an issue. At first sight, it is natural to do but then is not bounded in when is small, since when . But for any small, So is there an approach, which is finer and which yields the result ? Or what is the lowest singularity, that we cannot remove ?","\newcommand{\f}[2]{\frac{#1}{#2}} \newcommand{\nor}[2]{ \left| \! \left| #1 \right| \! \right|_{#2} } \newcommand{\ab}[1]{\left|#1\right|} \newcommand{\R}{\mathbb{R}} \newcommand{\pa}[1]{\left( #1 \right)} \def\1{1} \begin{align*}
	\nor{V}{L^{\f{3}{2}}(\R^3) + L^\infty(\R^3)} := {\underset{\substack{V_{3/2} \in L^{\f{3}{2}  }(\R^3) \\ V_{\infty} \in L^\infty(\R^3) \\ V = V_{3/2} + V_\infty}}{\text{inf}}\quad} \pa{\nor{V_{3/2}}{L^{\f{3}{2}}} + \nor{V_\infty}{L^\infty(\R^3)}}
\end{align*} \ab{\cdot}^{-p} \in L^{\f{3}{2}} + L^\infty p < 2 \ab{\cdot}^{-p} = \ab{\cdot}^{-p} \1_{\ab{\cdot} \le c}  + \ab{\cdot}^{-p}\1_{\ab{\cdot} \ge c} L^{\f{3}{2}} L^{\infty} \ab{\cdot}^{-p} \notin L^{\f{3}{2}} \ab{\cdot}^{-2} \notin L^{\f{3}{2}} + L^\infty \begin{align*}
	\nor{ \ab{r-\cdot}^{-1} - \ab{\cdot}^{-1}  }{L^{\f{3}{2}} + L^\infty} \le c \ab{r} 
\end{align*} r \in \R^3 c r r \begin{align}\label{eqq}
	\ab{\ab{r-x}^{-1} - \ab{x}^{-1}} &= \ab{\ab{r-x} - \ab{x}}\ab{r-x}^{-1}\ab{x}^{-1} \nonumber \\
						 & \le \ab{r} \ab{r-x}^{-1}\ab{x}^{-1}
\end{align} \nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} r r \begin{align*}
\nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} \rightarrow +\infty
\end{align*} \ab{r} \rightarrow 0 \varepsilon > 0 \begin{align*}
\ab{r}^\varepsilon \nor{\ab{r-\cdot}^{-1}\ab{\cdot}^{-1}}{L^{\f{3}{2}} + L^\infty} \rightarrow 0
\end{align*}","['functional-analysis', 'inequality', 'lipschitz-functions']"
36,$\ast$-homomorphism and two-sided ideals of unitarization $C^\ast$-algebra,-homomorphism and two-sided ideals of unitarization -algebra,\ast C^\ast,"We work with the unitization of $C^\ast$ -algebra. I.e. $C^\ast$ -algebra $\mathcal{A}$ with norm $\|\cdot\|$ . Let $\tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C}$ as a vector space. We endow it with multiplication and involution, $$(a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu)$$ and $$(a,\lambda)^\ast:=(a^\ast,\bar{\lambda})$$ I managed to prove that this is a $\ast$ -algebra. Now I have some questions because I want to understand this example. We denote $\omega:\mathcal{A}\rightarrow\tilde{\mathcal{A}}$ by the map $\omega(a):=(a,0)$ . I want to show that this is a $\ast$ -homomorphism, i.e. it is linear and so on. The example does not show anything regarding $\ast$ -homomorphism, but my attempt: Let $a,b\in\mathcal{A}$ then $\omega(a+b)=(a+b,0)=(a,0)+(b,0)=\omega(a)+\omega(b)$ . Is it true? I readed about two side ideals in my book. Is it true that $\omega(\mathcal{A})$ is a two-sided ideal in $\tilde{\mathcal{A}}$ ? If so, how come? Definition: A (two-sided) ideal $J$ in a Banach algebra $\mathscr{A}$ is a subspace of $A$ with the property that $A \in \mathscr{A}$ and $S \in J$ implies $AS \in J$ and $SA \in J$ . Is this definition the right one to use? If so, then I will try to do the part "" $AS \in J$ "" if anyone can give an example of the other part $SA \in J$ .","We work with the unitization of -algebra. I.e. -algebra with norm . Let as a vector space. We endow it with multiplication and involution, and I managed to prove that this is a -algebra. Now I have some questions because I want to understand this example. We denote by the map . I want to show that this is a -homomorphism, i.e. it is linear and so on. The example does not show anything regarding -homomorphism, but my attempt: Let then . Is it true? I readed about two side ideals in my book. Is it true that is a two-sided ideal in ? If so, how come? Definition: A (two-sided) ideal in a Banach algebra is a subspace of with the property that and implies and . Is this definition the right one to use? If so, then I will try to do the part "" "" if anyone can give an example of the other part .","C^\ast C^\ast \mathcal{A} \|\cdot\| \tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C} (a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu) (a,\lambda)^\ast:=(a^\ast,\bar{\lambda}) \ast \omega:\mathcal{A}\rightarrow\tilde{\mathcal{A}} \omega(a):=(a,0) \ast \ast a,b\in\mathcal{A} \omega(a+b)=(a+b,0)=(a,0)+(b,0)=\omega(a)+\omega(b) \omega(\mathcal{A}) \tilde{\mathcal{A}} J \mathscr{A} A A \in \mathscr{A} S \in J AS \in J SA \in J AS \in J SA \in J","['functional-analysis', 'operator-theory', 'ideals', 'operator-algebras']"
37,An inequality regarding $L^2$ and $H^{-1}$ norms,An inequality regarding  and  norms,L^2 H^{-1},"Necas theorem gives that $$\|p\|_{L^2(\Omega)}\lesssim\|p\|_{H^{-1}(\Omega)}+\sum_{i=1}^n\|\frac{\partial p}{\partial x_i}\|_{H^{-1}(\Omega)}\tag{$*$},$$ where $\Omega$ is connected Lipschitz domain and $p\in L^2(\Omega)$ . Now I need to prove that $(*)$ is equavalent to $$\|p\|_{L^2(\Omega)}\lesssim\sum_{i=1}^n\|\frac{\partial p}{\partial x_i}\|_{H^{-1}(\Omega)},\tag{$**$}$$ where $p\in L_0^2(\Omega)=\{p\in L^2(\Omega):\int_{\Omega}p=0\}$ . I already know how to prove $(*)\Rightarrow(**)$ . In order to prove $(**)\Rightarrow(*)$ , I think $p\in L^2$ can be decomposed to $p=\bar{p} + p_0$ , where $\bar{p}=\frac{1}{|\Omega|}\int_{\Omega}p$ and $p_0=p-\bar{p} \in L_0^2(\Omega)$ . Hence, we only need to prove that $\|\bar{p}\|_{L^2}\lesssim\|p\|_{H^{-1}}\quad(***)$ . $\|p\|_{H^{-1}}=sup\frac{\int_\Omega pv}{\|v\|_{H^1}}$ where the supremum is taken for all $v\in H_0^1$ . If I could take v as a constant, I could have easily proved $(***)$ . I wonder if the inequality $(***)$ holds, and how to prove it if it holds. Maybe I can take $v \in H_0^1$ which is a constant in a large area?","Necas theorem gives that where is connected Lipschitz domain and . Now I need to prove that is equavalent to where . I already know how to prove . In order to prove , I think can be decomposed to , where and . Hence, we only need to prove that . where the supremum is taken for all . If I could take v as a constant, I could have easily proved . I wonder if the inequality holds, and how to prove it if it holds. Maybe I can take which is a constant in a large area?","\|p\|_{L^2(\Omega)}\lesssim\|p\|_{H^{-1}(\Omega)}+\sum_{i=1}^n\|\frac{\partial p}{\partial x_i}\|_{H^{-1}(\Omega)}\tag{*}, \Omega p\in L^2(\Omega) (*) \|p\|_{L^2(\Omega)}\lesssim\sum_{i=1}^n\|\frac{\partial p}{\partial x_i}\|_{H^{-1}(\Omega)},\tag{**} p\in L_0^2(\Omega)=\{p\in L^2(\Omega):\int_{\Omega}p=0\} (*)\Rightarrow(**) (**)\Rightarrow(*) p\in L^2 p=\bar{p} + p_0 \bar{p}=\frac{1}{|\Omega|}\int_{\Omega}p p_0=p-\bar{p} \in L_0^2(\Omega) \|\bar{p}\|_{L^2}\lesssim\|p\|_{H^{-1}}\quad(***) \|p\|_{H^{-1}}=sup\frac{\int_\Omega pv}{\|v\|_{H^1}} v\in H_0^1 (***) (***) v \in H_0^1","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
38,Proving boundedness of operator,Proving boundedness of operator,,"Let $E$ be a finite dimensional normed space with norm $||\cdot||$ . Show that $T : (E,||\cdot||) \rightarrow (\mathbb{K}^n,||\cdot||_\infty),$ given by $x = \sum_{I=1}^nx_ie_i \mapsto (x_1,\dots,x_n)$ is continuous. My attempt: Let $B = \{e_1,\dots,e_n\}$ be a base for $E$ . Then for any $x = \sum_{i=1}^nx_ie_i$ where $(x_i)_{I=1}^n \in \mathbb{K}^n$ we have the following: \begin{align*}||Tx||_\infty & = \max_{1 \leq i \leq n}|x_i|\\ & \leq \sum_{I=1}^n|x_i|\frac{||e_i||}{||e_i||}\\ & = \frac{1}{\sum_{I=1}^n||e_i||}\sum_{i=1}^n||x_ie_i||.\end{align*} But now I am stuck because I can't go from $\sum_{i=1}^n||x_ie_i||$ to $||\sum_{I=1}^nx_ie_i|| = ||x||$ . Ps .: I am not able to use the famous auxiliary lemma: that gives the following estimate $$c|\sum_{I=1}x_i| \leq ||x_1e_1 + \dots x_ne_n|| = ||x||.$$ Thank you.",Let be a finite dimensional normed space with norm . Show that given by is continuous. My attempt: Let be a base for . Then for any where we have the following: But now I am stuck because I can't go from to . Ps .: I am not able to use the famous auxiliary lemma: that gives the following estimate Thank you.,"E ||\cdot|| T : (E,||\cdot||) \rightarrow (\mathbb{K}^n,||\cdot||_\infty), x = \sum_{I=1}^nx_ie_i \mapsto (x_1,\dots,x_n) B = \{e_1,\dots,e_n\} E x = \sum_{i=1}^nx_ie_i (x_i)_{I=1}^n \in \mathbb{K}^n \begin{align*}||Tx||_\infty & = \max_{1 \leq i \leq n}|x_i|\\
& \leq \sum_{I=1}^n|x_i|\frac{||e_i||}{||e_i||}\\
& = \frac{1}{\sum_{I=1}^n||e_i||}\sum_{i=1}^n||x_ie_i||.\end{align*} \sum_{i=1}^n||x_ie_i|| ||\sum_{I=1}^nx_ie_i|| = ||x|| c|\sum_{I=1}x_i| \leq ||x_1e_1 + \dots x_ne_n|| = ||x||.","['functional-analysis', 'solution-verification']"
39,Justifying term by term differentiation of spherical harmonics expansion,Justifying term by term differentiation of spherical harmonics expansion,,"I saw in many physics texts term by term differentiation of spherical harmonics expansion, but since they're physics texts they're without rigourous proof. Take for example the following from Wikipedia Given the multipole expansion of a scalar field ${\displaystyle \phi =\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell  }\phi _{\ell m}(r)Y_{\ell m}(\theta ,\phi ),}$ we can express its gradient in terms of the VSH as ${\displaystyle \nabla \phi =\sum _{\ell =0}^{\infty }\sum _{m=-\ell  }^{\ell }\left({\frac {d\phi _{\ell m}}{dr}}\mathbf {Y} _{\ell  m}+{\frac {\phi _{\ell m}}{r}}\mathbf {\Psi } _{\ell m}\right).}$ I know the spherical harmonics $Y_{\ell m}$ forms an orthonormal basis for $L^2$ functions, so that any $L^2$ function $\phi$ can be written as an infinite series expansion of spherical harmonics in the above way. If $\phi$ is $H^1$ function, then we expect its derivative to also have an spherical harmonic expansion. However, how do we know rigourosuly that it can be obtained by term by term differentiation of the series for $\phi$ in the above manner? In other words, how do we know the coefficients for the spherical harmonic expansion of $\nabla\phi$ is related to the coefficients for the spherical harmonic expansion of $\phi$ via term by term differentiation? Since the convergence of the series that I know of is only in $L^2$ , not uniform, I can't quite see how to properly justify it.. Could one justify it if we have more regularity like $C^k$ for some large $k$ ?","I saw in many physics texts term by term differentiation of spherical harmonics expansion, but since they're physics texts they're without rigourous proof. Take for example the following from Wikipedia Given the multipole expansion of a scalar field we can express its gradient in terms of the VSH as I know the spherical harmonics forms an orthonormal basis for functions, so that any function can be written as an infinite series expansion of spherical harmonics in the above way. If is function, then we expect its derivative to also have an spherical harmonic expansion. However, how do we know rigourosuly that it can be obtained by term by term differentiation of the series for in the above manner? In other words, how do we know the coefficients for the spherical harmonic expansion of is related to the coefficients for the spherical harmonic expansion of via term by term differentiation? Since the convergence of the series that I know of is only in , not uniform, I can't quite see how to properly justify it.. Could one justify it if we have more regularity like for some large ?","{\displaystyle \phi =\sum _{\ell =0}^{\infty }\sum _{m=-\ell }^{\ell
 }\phi _{\ell m}(r)Y_{\ell m}(\theta ,\phi ),} {\displaystyle \nabla \phi =\sum _{\ell =0}^{\infty }\sum _{m=-\ell
 }^{\ell }\left({\frac {d\phi _{\ell m}}{dr}}\mathbf {Y} _{\ell
 m}+{\frac {\phi _{\ell m}}{r}}\mathbf {\Psi } _{\ell m}\right).} Y_{\ell m} L^2 L^2 \phi \phi H^1 \phi \nabla\phi \phi L^2 C^k k","['sequences-and-series', 'functional-analysis', 'analysis', 'derivatives', 'spherical-harmonics']"
40,"Asymptotics on root $x = x_n \in [2\pi n, 2\pi n + \pi/2]$ of $\sin{x}=(\log{x})^{-1}$ [duplicate]",Asymptotics on root  of  [duplicate],"x = x_n \in [2\pi n, 2\pi n + \pi/2] \sin{x}=(\log{x})^{-1}","This question already has answers here : Asymptotic behavior of the solutions to $\sin x = (\log x)^{-1}$ (2 answers) Closed 1 year ago . Problem Description I am currently self-studying Buijn's book on asymptotic analysis ""Asymptotic Methods in Analysis"". I am stuck at the first exercise of chapter 2: I have completed the first part of the question . My problem is with proving the specific form of the $x_n$ root. My Attempt In said chapter of the book, three methods are demonstrated to tackle implicit-function problems in asymptotic analysis. Firstly , the method where one converts the equation to the form $z/f(z) = w$ with $w$ being $z$ -independent and $f(0) \ne 0$ . Then close enough to $z=0$ and for small enough $w$ , $z$ can be expressed as a power series in $w$ which satisfies the given equation. Secondly , the method where one derives a relation which can be used to get better and better approximations of the solution iteratively. Thirdly , a special case of the previous method where the relation is produced using Newton's method. ie: $$x_n = x_{n-1} - f(x_{n-1})/f'(x_{n-1})$$ The first thing I noticed is that for $x \rightarrow \infty$ , $(\ln{x})^{-1} \rightarrow 0$ therefore $\sin{x} \rightarrow 0$ . This means that $x_n \rightarrow 2 \pi n$ since $x_n \in [2 \pi n, 2 \pi n + \pi/2]$ . Setting $$x_n = 2 \pi n + z$$ I attempted to convert $\sin{x}=(\ln{x})^{-1}$ in the form of the first method descirbed above. This failed as I arrive at $$ (\sin z)^{-1} = \log{(2\pi n)} +  \log {\Big(1 + \frac{z}{2\pi n}\Big)}$$ I am not able to construct a $w$ which decreases as $n$ increases and is z-independant, while keeping $f(z)$ $n$ -independant (is this necessary?). I got a bit further by writting $$ \log {\Big(1 + \frac{z}{2\pi n}\Big)} = O(\frac{z}{2\pi n}) = O(\frac{1}{2\pi n}), \quad (n \rightarrow \infty, z \rightarrow 0) $$ and then $$(\sin z)^{-1} = \log (2\pi n) + O(\frac{1}{2\pi n})$$ I then tried applying the first method to $(\sin z)^{-1} = \log (2\pi n)$ but unfortunately the $f(0) \ne 0$ assumption is broken. The next thing I attempted was the third method. Using Newton's method for $f(x) = \log{x}\sin{x} - 1$ , we arrive at the following iterative formula for recursive approximations $\phi_k(n)$ for $x$ : $$\phi_{k+1} = \phi_k - \frac{\log{\phi_k}\sin{\phi_k}-1}{\frac{\sin{\phi_k}}{\phi_k}+\log{\phi_k}\cos{\phi_k}}$$ If we start with $\phi_0(n) = 2\pi n$ , $$\phi_1 = 2\pi n + (\log {2 \pi n})^{-1}$$ which looks promising. But then this nice pattern breaks and we do not get the result we are looking for. So, my final attempt was to use $\phi_1$ by writting $x = \phi_1 + z$ and trying to show that $$z = O((\log{2\pi n})^{-3}), \quad (n \rightarrow \infty)$$ by using $\sin{x} \log{x} = 1$ . However this also failed Any kind of help is very much appreciated!","This question already has answers here : Asymptotic behavior of the solutions to $\sin x = (\log x)^{-1}$ (2 answers) Closed 1 year ago . Problem Description I am currently self-studying Buijn's book on asymptotic analysis ""Asymptotic Methods in Analysis"". I am stuck at the first exercise of chapter 2: I have completed the first part of the question . My problem is with proving the specific form of the root. My Attempt In said chapter of the book, three methods are demonstrated to tackle implicit-function problems in asymptotic analysis. Firstly , the method where one converts the equation to the form with being -independent and . Then close enough to and for small enough , can be expressed as a power series in which satisfies the given equation. Secondly , the method where one derives a relation which can be used to get better and better approximations of the solution iteratively. Thirdly , a special case of the previous method where the relation is produced using Newton's method. ie: The first thing I noticed is that for , therefore . This means that since . Setting I attempted to convert in the form of the first method descirbed above. This failed as I arrive at I am not able to construct a which decreases as increases and is z-independant, while keeping -independant (is this necessary?). I got a bit further by writting and then I then tried applying the first method to but unfortunately the assumption is broken. The next thing I attempted was the third method. Using Newton's method for , we arrive at the following iterative formula for recursive approximations for : If we start with , which looks promising. But then this nice pattern breaks and we do not get the result we are looking for. So, my final attempt was to use by writting and trying to show that by using . However this also failed Any kind of help is very much appreciated!","x_n z/f(z) = w w z f(0) \ne 0 z=0 w z w x_n = x_{n-1} - f(x_{n-1})/f'(x_{n-1}) x \rightarrow \infty (\ln{x})^{-1} \rightarrow 0 \sin{x} \rightarrow 0 x_n \rightarrow 2 \pi n x_n \in [2 \pi n, 2 \pi n + \pi/2] x_n = 2 \pi n + z \sin{x}=(\ln{x})^{-1}  (\sin z)^{-1} = \log{(2\pi n)} +  \log {\Big(1 + \frac{z}{2\pi n}\Big)} w n f(z) n  \log {\Big(1 + \frac{z}{2\pi n}\Big)} = O(\frac{z}{2\pi n}) = O(\frac{1}{2\pi n}), \quad (n \rightarrow \infty, z \rightarrow 0)  (\sin z)^{-1} = \log (2\pi n) + O(\frac{1}{2\pi n}) (\sin z)^{-1} = \log (2\pi n) f(0) \ne 0 f(x) = \log{x}\sin{x} - 1 \phi_k(n) x \phi_{k+1} = \phi_k - \frac{\log{\phi_k}\sin{\phi_k}-1}{\frac{\sin{\phi_k}}{\phi_k}+\log{\phi_k}\cos{\phi_k}} \phi_0(n) = 2\pi n \phi_1 = 2\pi n + (\log {2 \pi n})^{-1} \phi_1 x = \phi_1 + z z = O((\log{2\pi n})^{-3}), \quad (n \rightarrow \infty) \sin{x} \log{x} = 1","['functional-analysis', 'numerical-methods', 'asymptotics']"
41,"Embedding of $W^{1,\infty}$ in $C^{0,1}$",Embedding of  in,"W^{1,\infty} C^{0,1}","Theorem 5 of pag 283 of the text ""Partial Differential Equations - Second Edition (Lawrence C. Evans)"" states Let $U$ be a bounded, open subset of $\mathbb{R}^n$ , and suppose $\partial U$ is $C^1$ . Assume $n < p \leq \infty$ and $u \in W^{1,p}(U)$ . Then $u$ has a version $u^* \in C^{0,\gamma}(\overline{U})$ , for $\gamma := 1 - \frac{n}{p}$ , with the estimate \begin{equation} ||u^*||_{C^{0,\gamma}(\overline{U})} \leq C||u||_{W^{1,p}(U)}\end{equation} The costant $C$ depends only on $p,n$ and $U$ Where $u^*$ is a version of $u$ means that $u^*(x) = u(x)$ almost everywhere for $x \in U$ , and \begin{equation}||u^*||_{C^{0,\gamma}(\overline{U})} := sup_{x \in \overline{U}}{|u^*(x)|} + sup_{\substack{x,y \in U \\ x \neq y}}{\frac{|u^*(x)-u^*(y)|}{|x - y|^\gamma}}\end{equation} The book proves the theorem in the case $p < \infty$ and says ""The case $p = \infty$ is easy to prove directly"". I'm only interested in the case $p = \infty$ . I only managed to prove that there is a costant $C$ and that $u$ has a version $u^*$ such that $$\sup_{x \in \overline{U}}{|u^*(x)|} \leq C||u||_{W^{1,\infty}(U)}$$ I don't understand how to prove that $$ sup_{\substack{x,y \in \overline{U} \\ x \neq y}}{\frac{|u^*(x)-u^*(y)|}{|x - y|}} \leq C||u||_{W^{1,\infty}(U)}$$ Note that there is no hypothesis about the connection of $U$ . My question is : How can I prove the teorem in an easy way in the case $p = \infty$ ?","Theorem 5 of pag 283 of the text ""Partial Differential Equations - Second Edition (Lawrence C. Evans)"" states Let be a bounded, open subset of , and suppose is . Assume and . Then has a version , for , with the estimate The costant depends only on and Where is a version of means that almost everywhere for , and The book proves the theorem in the case and says ""The case is easy to prove directly"". I'm only interested in the case . I only managed to prove that there is a costant and that has a version such that I don't understand how to prove that Note that there is no hypothesis about the connection of . My question is : How can I prove the teorem in an easy way in the case ?","U \mathbb{R}^n \partial U C^1 n < p \leq \infty u \in W^{1,p}(U) u u^* \in C^{0,\gamma}(\overline{U}) \gamma := 1 - \frac{n}{p} \begin{equation} ||u^*||_{C^{0,\gamma}(\overline{U})} \leq C||u||_{W^{1,p}(U)}\end{equation} C p,n U u^* u u^*(x) = u(x) x \in U \begin{equation}||u^*||_{C^{0,\gamma}(\overline{U})} := sup_{x \in \overline{U}}{|u^*(x)|} + sup_{\substack{x,y \in U \\ x \neq y}}{\frac{|u^*(x)-u^*(y)|}{|x - y|^\gamma}}\end{equation} p < \infty p = \infty p = \infty C u u^* \sup_{x \in \overline{U}}{|u^*(x)|} \leq C||u||_{W^{1,\infty}(U)}  sup_{\substack{x,y \in \overline{U} \\ x \neq y}}{\frac{|u^*(x)-u^*(y)|}{|x - y|}} \leq C||u||_{W^{1,\infty}(U)} U p = \infty","['functional-analysis', 'sobolev-spaces', 'lipschitz-functions', 'holder-spaces']"
42,Metric defined on k tensors,Metric defined on k tensors,,"If $g$ is a riemannian metric on $M$ and $f$ is a real valued smooth function on $M$ , Then, why does $g(\nabla^kf,\nabla^kf)$ make sense? How does one interpret this expression? Can one view $\nabla^kf$ as a a smooth vector field on $M$ ? For answers, try to avoid coordinates. I am interested in how one views $g$ in this case. $g$ is a map $\mathfrak{X}(M)\times \mathfrak{X}(M)\rightarrow C^{\infty}(M)$ , whereas $\nabla^kf$ is a smooth linear map $\bigoplus_{i=1}^k \mathfrak{X}(M)\rightarrow C^{\infty}(M)$ For reference: Look at the answer to the following question: Understanding iterated covariant derivatives to define Sobolev spaces on manifolds","If is a riemannian metric on and is a real valued smooth function on , Then, why does make sense? How does one interpret this expression? Can one view as a a smooth vector field on ? For answers, try to avoid coordinates. I am interested in how one views in this case. is a map , whereas is a smooth linear map For reference: Look at the answer to the following question: Understanding iterated covariant derivatives to define Sobolev spaces on manifolds","g M f M g(\nabla^kf,\nabla^kf) \nabla^kf M g g \mathfrak{X}(M)\times \mathfrak{X}(M)\rightarrow C^{\infty}(M) \nabla^kf \bigoplus_{i=1}^k \mathfrak{X}(M)\rightarrow C^{\infty}(M)",['functional-analysis']
43,Differential Operator Invertible,Differential Operator Invertible,,"Good evening, I have a little problem with the invertibility of an operator $A : W^{1,2}(S^1,\mathbb{R}^{2n})\longrightarrow L^2(S^1,\mathbb{R^{2n}})$ defined as $$(A\zeta)(t) = \dot{\zeta}(t)-\dot{\Psi}(t)\cdot\Psi^{-1}(t)\cdot\zeta(t),$$ where $\Psi(t)\in \mathbb{R}^{2n\times 2n}$ is a path of symplectic matrices with $\Psi(0)=Id$ . The claim then is that $A$ is invertible if and only if $1$ is not an eigenvalue of $\Psi(t)$ for any $t$ . In the paper, it sounds like this is obvious, so I started to show that $\ker A = \{0\}$ : If $A\zeta = 0$ (i.e. $(A\zeta)(t) = 0$ a.e.), we have $$\dot{\Psi}(t)\cdot\underbrace{\Psi^{-1}(t)\cdot\zeta(t)}_{=:\alpha(t)} =\dot{\zeta}(t)  .$$ So setting $\alpha (t) := \Psi^{-1}(t)\cdot\zeta(t)$ , this simplifies to \begin{align} \dot{\Psi}(t)\cdot\alpha(t) &=\frac{d}{dt}(\Psi(t)\cdot\alpha(t)) \\ \Leftrightarrow \dot{\Psi}(t)\cdot\alpha(t) &=\dot{\Psi}(t)\cdot\alpha(t)+\Psi(t)\dot{\alpha}(t) \\ \Leftrightarrow \Psi(t)\cdot\dot{\alpha}(t) &= 0 \end{align} Since $\Psi(t)$ is invertible (as a symplectic matrix), this means $\dot{\alpha}(t) = 0$ , so $\alpha(t)$ is a constant vector and so by definition of $\alpha$ we get that $$ \zeta(t)=\Psi(t)\cdot\alpha $$ is actually an element of the kernel. This seems be the place where the claim with $1$ (not) being an eigenvalue of $\Psi(t)$ should come into play, however I don't really see how. Of course, maybe my steps have an error somewhere, so I would really like to hear other people's opinions on it.","Good evening, I have a little problem with the invertibility of an operator defined as where is a path of symplectic matrices with . The claim then is that is invertible if and only if is not an eigenvalue of for any . In the paper, it sounds like this is obvious, so I started to show that : If (i.e. a.e.), we have So setting , this simplifies to Since is invertible (as a symplectic matrix), this means , so is a constant vector and so by definition of we get that is actually an element of the kernel. This seems be the place where the claim with (not) being an eigenvalue of should come into play, however I don't really see how. Of course, maybe my steps have an error somewhere, so I would really like to hear other people's opinions on it.","A : W^{1,2}(S^1,\mathbb{R}^{2n})\longrightarrow L^2(S^1,\mathbb{R^{2n}}) (A\zeta)(t) = \dot{\zeta}(t)-\dot{\Psi}(t)\cdot\Psi^{-1}(t)\cdot\zeta(t), \Psi(t)\in \mathbb{R}^{2n\times 2n} \Psi(0)=Id A 1 \Psi(t) t \ker A = \{0\} A\zeta = 0 (A\zeta)(t) = 0 \dot{\Psi}(t)\cdot\underbrace{\Psi^{-1}(t)\cdot\zeta(t)}_{=:\alpha(t)} =\dot{\zeta}(t)  . \alpha (t) := \Psi^{-1}(t)\cdot\zeta(t) \begin{align}
\dot{\Psi}(t)\cdot\alpha(t) &=\frac{d}{dt}(\Psi(t)\cdot\alpha(t)) \\ \Leftrightarrow \dot{\Psi}(t)\cdot\alpha(t) &=\dot{\Psi}(t)\cdot\alpha(t)+\Psi(t)\dot{\alpha}(t) \\ \Leftrightarrow \Psi(t)\cdot\dot{\alpha}(t) &= 0
\end{align} \Psi(t) \dot{\alpha}(t) = 0 \alpha(t) \alpha  \zeta(t)=\Psi(t)\cdot\alpha  1 \Psi(t)","['functional-analysis', 'ordinary-differential-equations', 'operator-theory', 'symplectic-linear-algebra']"
44,"The functionals $\|\cdot\|_{L^{p,q}}$ do not satisfy the triangle inequality.",The functionals  do not satisfy the triangle inequality.,"\|\cdot\|_{L^{p,q}}","Given a measurable function $f$ on a measure space $(X,\mu)$ and $0<p,q\leq \infty$ , define $$\|f\|_{L^{p,q}}=\left\{ \begin{array}{ll} \displaystyle{\left(\int_{0}^\infty\left(t^{1/p}f^*(t)\right)^q\,\frac{dt}{t}\right)^{1/q}}, & \mbox{si } q<\infty,  \\ \sup_{t>0}t^{1/p}f^*(t), & \mbox{si } q=\infty.   \end{array} \right.$$ Consider the functions $f(t)=t\quad$ and $\quad g(t)=1-t$ defined on $[0,1]$ . My question is: How can I find $f^*$ and $g^*$ ? where \begin{align*} f^*: [0,\infty)&\longrightarrow [0,\infty)\\ t&\longmapsto f^*(t)=\inf\{s>0: d_f(s)\leq t\}\end{align*} and $$d_f(s)=\mu\left(\{x: |f(x)|>s\}\right),\ \ s>0$$ denotes the distribution function. The Loukas Grafakos-Classical Fourier Analysis book suggests that $f^*(\alpha)=g^*(\alpha)=(1-\alpha)\mathcal{X}_{[0,1]}(\alpha)$ . Here $\mathcal{X}$ denotes the characteristic function.","Given a measurable function on a measure space and , define Consider the functions and defined on . My question is: How can I find and ? where and denotes the distribution function. The Loukas Grafakos-Classical Fourier Analysis book suggests that . Here denotes the characteristic function.","f (X,\mu) 0<p,q\leq \infty \|f\|_{L^{p,q}}=\left\{
\begin{array}{ll}
\displaystyle{\left(\int_{0}^\infty\left(t^{1/p}f^*(t)\right)^q\,\frac{dt}{t}\right)^{1/q}}, & \mbox{si } q<\infty,  \\
\sup_{t>0}t^{1/p}f^*(t), & \mbox{si } q=\infty.  
\end{array}
\right. f(t)=t\quad \quad g(t)=1-t [0,1] f^* g^* \begin{align*} f^*: [0,\infty)&\longrightarrow [0,\infty)\\ t&\longmapsto f^*(t)=\inf\{s>0: d_f(s)\leq t\}\end{align*} d_f(s)=\mu\left(\{x: |f(x)|>s\}\right),\ \ s>0 f^*(\alpha)=g^*(\alpha)=(1-\alpha)\mathcal{X}_{[0,1]}(\alpha) \mathcal{X}","['real-analysis', 'functional-analysis', 'fourier-analysis', 'lp-spaces', 'decreasing-rearrangements']"
45,Proof of a theorem about generators of uniformly continuous semigroup,Proof of a theorem about generators of uniformly continuous semigroup,,"I´m reading Vrabie and I'm have some problems understanding this proof. We have that $\{S(t); t\geq 0\}$ is uniformly continuous, so $\displaystyle\lim_{t\downarrow 0} S(t)=I$ . Question 1: This implies that $\exists\rho >0$ such that $||\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt -I||_{\mathcal{L}(X)} <1$ . Because of the limit we have that $\exists\rho >0$ such that $|| S(t)  -I||_{\mathcal{L}(X)} <1$ . Now $$||\displaystyle\int_0^\rho S(t) dt -\displaystyle\int_0^\rho I dt||_{\mathcal{L}(X)}\leq \displaystyle\int_0^\rho ||S(t)  - I|| dt<\displaystyle\int_0^\rho  1 dt$$ this is equivalent to: $$||\displaystyle\int_0^\rho S(t) dt -\rho I ||_{\mathcal{L}(X)}<\rho$$ and just dividing by $\rho$ we have the inequality. I assume that $\displaystyle\int_0^\rho I dt=\rho I$ , but I don't know if it is true. Is it? We notice that the integral here is a Riemann integral of a continuous function $S:[0,\rho]\to\mathcal{L}(X)$ , which is defined by a simple analogy with its scalar counterpart . Question 2: About the sentence in bold, how do we define it? We define it as a operator given by $$x\to \displaystyle\frac{1}{x}\displaystyle\int_0^xS(t) dt$$ A bit lost here. Consecuently, the operator $\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt$ is invertible and accordingly $\displaystyle\int_0^\rho S(t) dt$ has the same property. Question 3: The phrase in bold just stunned me. We know that $S(t)$ is invertible as it belongs to a uniformly continuous semigroup by how can we deduce that the operator $\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt$ is invertible? For the following part: Question 4: we can change the letter $\rho$ highlighted in blue by the letter $h$ highlighted in blue, just because of question 2. Is jut notation. Question 5: If we take limit when $h\to 0$ in $$\displaystyle\frac{1}{h}\displaystyle\int_h^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt$$ why do we get $S(\rho)-I$ ?. Lets do a little of work, $$\displaystyle\frac{1}{h}\displaystyle\int_h^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt=\displaystyle\frac{1}{h}\displaystyle\int_0^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt$$ , so it's clear that $$\displaystyle\frac{1}{h}\displaystyle\int_0^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt=S(\rho)\;\text{when}\; h\to 0$$ . But where where does the $I$ come from? It comes from $\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt$ ?","I´m reading Vrabie and I'm have some problems understanding this proof. We have that is uniformly continuous, so . Question 1: This implies that such that . Because of the limit we have that such that . Now this is equivalent to: and just dividing by we have the inequality. I assume that , but I don't know if it is true. Is it? We notice that the integral here is a Riemann integral of a continuous function , which is defined by a simple analogy with its scalar counterpart . Question 2: About the sentence in bold, how do we define it? We define it as a operator given by A bit lost here. Consecuently, the operator is invertible and accordingly has the same property. Question 3: The phrase in bold just stunned me. We know that is invertible as it belongs to a uniformly continuous semigroup by how can we deduce that the operator is invertible? For the following part: Question 4: we can change the letter highlighted in blue by the letter highlighted in blue, just because of question 2. Is jut notation. Question 5: If we take limit when in why do we get ?. Lets do a little of work, , so it's clear that . But where where does the come from? It comes from ?","\{S(t); t\geq 0\} \displaystyle\lim_{t\downarrow 0} S(t)=I \exists\rho >0 ||\displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt -I||_{\mathcal{L}(X)} <1 \exists\rho >0 || S(t)  -I||_{\mathcal{L}(X)} <1 ||\displaystyle\int_0^\rho S(t) dt -\displaystyle\int_0^\rho I dt||_{\mathcal{L}(X)}\leq \displaystyle\int_0^\rho ||S(t)  - I|| dt<\displaystyle\int_0^\rho  1 dt ||\displaystyle\int_0^\rho S(t) dt -\rho I ||_{\mathcal{L}(X)}<\rho \rho \displaystyle\int_0^\rho I dt=\rho I S:[0,\rho]\to\mathcal{L}(X) x\to \displaystyle\frac{1}{x}\displaystyle\int_0^xS(t) dt \displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt \displaystyle\int_0^\rho S(t) dt S(t) \displaystyle\frac{1}{\rho}\displaystyle\int_0^\rho S(t) dt \rho h h\to 0 \displaystyle\frac{1}{h}\displaystyle\int_h^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt S(\rho)-I \displaystyle\frac{1}{h}\displaystyle\int_h^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt=\displaystyle\frac{1}{h}\displaystyle\int_0^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt \displaystyle\frac{1}{h}\displaystyle\int_0^{\rho+h} S(t) dt-\displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt=S(\rho)\;\text{when}\; h\to 0 I \displaystyle\frac{1}{h}\displaystyle\int_0^h S(t) dt","['functional-analysis', 'analysis', 'semigroups', 'semigroup-of-operators']"
46,An identity for holomorphic functional calculus,An identity for holomorphic functional calculus,,"Let $A$ be a $C^*$ -algebra, $p\in A$ a projection, and $a\in A$ self-adjoint. Let $\Gamma$ be the circle of radius $1/2$ with center $1$ . It is claimed that $$\frac{1}{2\pi i}\int_\Gamma(z-p)^{-1}a(z-p)^{-1}\,dz=pa(1-p)+(1-p)ap.$$ I'm looking for a hint to prove this. Comment: I understand that $$\frac{1}{2\pi i}\int_\Gamma(z-p)^{-1}\,dz=p,$$ hence $pa(1-p)=\frac{1}{2\pi i}\int_\Gamma\frac{a(1-p)}{z-p}\,dz$ and $(1-p)ap=\frac{1}{2\pi i}\int_\Gamma\frac{(1-p)a}{z-p}\,dz$ . So $$pa(1-p)+(1-p)ap=\frac{1}{2\pi i}\int_\Gamma\frac{a(1-p)(z-p)+(1-p)a(z-p)}{(z-p)^2}\,dz.$$ But I'm not sure if this is the way.","Let be a -algebra, a projection, and self-adjoint. Let be the circle of radius with center . It is claimed that I'm looking for a hint to prove this. Comment: I understand that hence and . So But I'm not sure if this is the way.","A C^* p\in A a\in A \Gamma 1/2 1 \frac{1}{2\pi i}\int_\Gamma(z-p)^{-1}a(z-p)^{-1}\,dz=pa(1-p)+(1-p)ap. \frac{1}{2\pi i}\int_\Gamma(z-p)^{-1}\,dz=p, pa(1-p)=\frac{1}{2\pi i}\int_\Gamma\frac{a(1-p)}{z-p}\,dz (1-p)ap=\frac{1}{2\pi i}\int_\Gamma\frac{(1-p)a}{z-p}\,dz pa(1-p)+(1-p)ap=\frac{1}{2\pi i}\int_\Gamma\frac{a(1-p)(z-p)+(1-p)a(z-p)}{(z-p)^2}\,dz.","['functional-analysis', 'operator-algebras', 'c-star-algebras']"
47,Bounding the norms of the powers of a $2\times 2$ matrix,Bounding the norms of the powers of a  matrix,2\times 2,"Let $\|.\|_2$ denote the matrix norm induced from the Euclidian vectornorm and let \begin{align} M=\left(\begin{array}{cc} a+b & -b \\ 1 & 0 \end{array}\right). \end{align} I need to bound $\|M^n\|_2$ for any $n\in \mathbb{N}$ . It is assumed that $a\in[0,1]$ and $b\in(0,1)$ . I tried to use Gelfand's formula from which we obtain: $$\|M^n\|_2\leq (\rho(M)+o(1))^n,$$ where $\rho(M)$ denotes the spectral radius of $M$ . It is easy to see why $\rho(M)\leq \max\left\{1-\frac{1-a}{2},b\right\}$ . Therefore $\|M^n\|_2<c$ for any n, if $\, a\leq q<1$ . In the case $a=1$ we see that: \begin{align*} \|M^n\|_2=\left\|\frac{1}{b-1}\left(\begin{array}{cc} b^{n+1}-1 & -b-b^{n+1} \\ b^{n}-1 & -b-b^{n} \end{array}\right)\right\|_2\leq c. \end{align*} But I have no idea how to bound $\|M^n\|_2$ when $a \rightarrow 1$ , since I haven't found any results on the convergence of Gelfand's formula. Any suggestions how to prove this? Also according to many plots, it should hold that \begin{align*} \|M^n\|_2\leq\left\|\frac{1}{b-1}\left(\begin{array}{cc} b^{n+1}-1 & -b-b^{n+1} \\ b^{n}-1 & -b-b^{n} \end{array}\right)\right\|_2\leq c, \end{align*} for any $a\in[0,1]$ . Any idea how to prove the last inequality?","Let denote the matrix norm induced from the Euclidian vectornorm and let I need to bound for any . It is assumed that and . I tried to use Gelfand's formula from which we obtain: where denotes the spectral radius of . It is easy to see why . Therefore for any n, if . In the case we see that: But I have no idea how to bound when , since I haven't found any results on the convergence of Gelfand's formula. Any suggestions how to prove this? Also according to many plots, it should hold that for any . Any idea how to prove the last inequality?","\|.\|_2 \begin{align}
M=\left(\begin{array}{cc}
a+b & -b \\
1 & 0
\end{array}\right).
\end{align} \|M^n\|_2 n\in \mathbb{N} a\in[0,1] b\in(0,1) \|M^n\|_2\leq (\rho(M)+o(1))^n, \rho(M) M \rho(M)\leq \max\left\{1-\frac{1-a}{2},b\right\} \|M^n\|_2<c \, a\leq q<1 a=1 \begin{align*}
\|M^n\|_2=\left\|\frac{1}{b-1}\left(\begin{array}{cc}
b^{n+1}-1 & -b-b^{n+1} \\
b^{n}-1 & -b-b^{n}
\end{array}\right)\right\|_2\leq c.
\end{align*} \|M^n\|_2 a \rightarrow 1 \begin{align*}
\|M^n\|_2\leq\left\|\frac{1}{b-1}\left(\begin{array}{cc}
b^{n+1}-1 & -b-b^{n+1} \\
b^{n}-1 & -b-b^{n}
\end{array}\right)\right\|_2\leq c,
\end{align*} a\in[0,1]","['linear-algebra', 'sequences-and-series', 'functional-analysis', 'matrix-norms', 'spectral-radius']"
48,How to interpret the constant of integration/endpoints of the sum of derivatives linear operator,How to interpret the constant of integration/endpoints of the sum of derivatives linear operator,,"Consider the linear operator expression $$L[f] = f + f' + f'' + f''' + f'''' + ... $$ This doesn't always converge but for some expressions it does converge (such as polynomials or linear combinations of exponentials $e^{ax}$ where $|a| < 1$ ) A natural question to ask is if this operator has a closed form. Using a technique almost identical to the proof of the geometric series combined with integration factors we can see that $$ L[f] - f = \frac{d}{dx}[L[f]] $$ $$ \frac{d}{dx}[L[f]] - L[f] = -f $$ $$  e^{-x} \frac{d}{dx}[L[f]] - e^{-x} L[f] = -e^{-x} f $$ $$ e^{-x} L[f] = - \int e^{-x} f dx $$ $$ L[f] = -e^x \int e^{-x} f dx $$ So this ""closed form"" is not exactly well defined, theres an indefinite integral in there which means it varies by an arbitrary constant. Now if you take a nice polynomial (ex: $x^2$ ) And apply this formula without even understanding it you can end up with a correct answer: $$ L[x^2] = x^2 + 2x + 2 $$ $$ -e^x \int e^{-x} x^2 dx  = -e^x \left( e^{-x} \left( x^2 + 2x + 2 \right) + C \right)$$ And now we just ""set C to 0"" and voila we have perfect agreement. And so while it possible to ""experimentally"" check this formula works, setting C to 0 makes absolutely no sense in the general context of functions. How do you know what C is supposed to be 0? In this case setting $C$ to is 0 is equivalent to saying $f(0)=2$ and how on earth did you know that $f(0)=2$ and not $3$ or something else in general.  You need some initial conditions/ideas on where your bounds start/end. Is there a way to make this formula into something more explicit / rigorous by either defining what the indefinite integral's bounds should be OR by describing a procedure (maybe its not guaranteed to halt in general) that can tell you what value of C is the right value? A possible strategy: If $f(x)$ has an asymptote as $x \rightarrow \pm \infty$ then perhaps the correct $C$ value is one that lets one of these asymptotic values be $0$ . But this is such a frankenstein arbitrary kind of rule. It can be codified as, select $c$ such that $$ \lim_{x \rightarrow \infty} \int_{c}^{x} e^{-x}f(x) dx  = 0$$ Then that is your integration bound OR if you prefer the language of constants then your additive constant $C = -e^{-c}f(c)$","Consider the linear operator expression This doesn't always converge but for some expressions it does converge (such as polynomials or linear combinations of exponentials where ) A natural question to ask is if this operator has a closed form. Using a technique almost identical to the proof of the geometric series combined with integration factors we can see that So this ""closed form"" is not exactly well defined, theres an indefinite integral in there which means it varies by an arbitrary constant. Now if you take a nice polynomial (ex: ) And apply this formula without even understanding it you can end up with a correct answer: And now we just ""set C to 0"" and voila we have perfect agreement. And so while it possible to ""experimentally"" check this formula works, setting C to 0 makes absolutely no sense in the general context of functions. How do you know what C is supposed to be 0? In this case setting to is 0 is equivalent to saying and how on earth did you know that and not or something else in general.  You need some initial conditions/ideas on where your bounds start/end. Is there a way to make this formula into something more explicit / rigorous by either defining what the indefinite integral's bounds should be OR by describing a procedure (maybe its not guaranteed to halt in general) that can tell you what value of C is the right value? A possible strategy: If has an asymptote as then perhaps the correct value is one that lets one of these asymptotic values be . But this is such a frankenstein arbitrary kind of rule. It can be codified as, select such that Then that is your integration bound OR if you prefer the language of constants then your additive constant",L[f] = f + f' + f'' + f''' + f'''' + ...  e^{ax} |a| < 1  L[f] - f = \frac{d}{dx}[L[f]]   \frac{d}{dx}[L[f]] - L[f] = -f    e^{-x} \frac{d}{dx}[L[f]] - e^{-x} L[f] = -e^{-x} f   e^{-x} L[f] = - \int e^{-x} f dx   L[f] = -e^x \int e^{-x} f dx  x^2  L[x^2] = x^2 + 2x + 2   -e^x \int e^{-x} x^2 dx  = -e^x \left( e^{-x} \left( x^2 + 2x + 2 \right) + C \right) C f(0)=2 f(0)=2 3 f(x) x \rightarrow \pm \infty C 0 c  \lim_{x \rightarrow \infty} \int_{c}^{x} e^{-x}f(x) dx  = 0 C = -e^{-c}f(c),"['functional-analysis', 'ordinary-differential-equations', 'operator-theory', 'indefinite-integrals']"
49,What assumptions are needed for compactness and self-adjointness?,What assumptions are needed for compactness and self-adjointness?,,"I am studying functional analysis and got stuck with a textbook exercise. I greatly appreciate some hints/help or a push in the right direction! Define for $g\in C^0[-1,1]$ the integral operator $T_g:L^2\to L^2$ , by $$T_g(f)(s):=\int_0^1g(s-t)f(t)dt$$ Now I need to oppose conditions on $g$ such that $T_g$ becomes a compact self-adjoint operator. My progress: First for we look at when $T_g$ is self-adjoint. So we want some conditions such that: $\langle T_g(f)(s),h(s)\rangle=\langle f(s), T_g(h)(s)\rangle$ . For the left hand side we get the following: $$\langle T_g(f)(s),h(s)\rangle=\int_0^1T_g(f)(s)\overline{h(s)}ds=\int_0^1\int_0^1g(s-t)f(t)\overline{h(s)}dtds$$ And for the right hand side we get the following: $$\langle f(s), T_g(h)(s)\rangle=\int_0^1f(s)\overline{T_g(h)(s)}ds=\int_0^1 \int_0^1f(s)\overline{g(s-t)} \overline{h(t)}dsdt$$ Well now I am not to sure I thought that $\overline g=g$ is the only condition for $T_g$ to be self-adjoint. Now for compactness: I wanted to pick a bounded subset $M \subset L^2$ , and show that $T_g(M) $ is compact. For that I want to show that $T_g(M)$ is closed, bounded and equicontiuous. I have no good idea/intuition on how to show these properties with the given information I have. Can someone help me solve this exercise I think it is a exercise from which I can learn a lot and get a better grasp! :)","I am studying functional analysis and got stuck with a textbook exercise. I greatly appreciate some hints/help or a push in the right direction! Define for the integral operator , by Now I need to oppose conditions on such that becomes a compact self-adjoint operator. My progress: First for we look at when is self-adjoint. So we want some conditions such that: . For the left hand side we get the following: And for the right hand side we get the following: Well now I am not to sure I thought that is the only condition for to be self-adjoint. Now for compactness: I wanted to pick a bounded subset , and show that is compact. For that I want to show that is closed, bounded and equicontiuous. I have no good idea/intuition on how to show these properties with the given information I have. Can someone help me solve this exercise I think it is a exercise from which I can learn a lot and get a better grasp! :)","g\in C^0[-1,1] T_g:L^2\to L^2 T_g(f)(s):=\int_0^1g(s-t)f(t)dt g T_g T_g \langle T_g(f)(s),h(s)\rangle=\langle f(s), T_g(h)(s)\rangle \langle T_g(f)(s),h(s)\rangle=\int_0^1T_g(f)(s)\overline{h(s)}ds=\int_0^1\int_0^1g(s-t)f(t)\overline{h(s)}dtds \langle f(s), T_g(h)(s)\rangle=\int_0^1f(s)\overline{T_g(h)(s)}ds=\int_0^1 \int_0^1f(s)\overline{g(s-t)} \overline{h(t)}dsdt \overline g=g T_g M \subset L^2 T_g(M)  T_g(M)","['functional-analysis', 'operator-theory']"
50,Hölder and Slobodeckij spaces,Hölder and Slobodeckij spaces,,"For a bounded domain $\Omega\subset\mathbb{R}^d$ , $s\in(0,1)$ , and $p\in(1,\infty)$ , the Sobolev-Slobodeckij seminorm is given by $$ [f]_{W^{s,p}}^p=\int_\Omega\int_\Omega\frac{|f(x)-f(y)|^p}{|x-y|^{d+sp}}dxdy. $$ A) Is it true and B) if it is, is there a direct proof (without going into fractional Laplacians, interpolation spaces, etc.) that $s$ -Hölder continuous functions have finite $[f]_{W^{s,p}}$ seminorm? Naively bounding $|f(x)-f(y)|$ by the Hölder norm times $|x-y|^s$ just barely fails.","For a bounded domain , , and , the Sobolev-Slobodeckij seminorm is given by A) Is it true and B) if it is, is there a direct proof (without going into fractional Laplacians, interpolation spaces, etc.) that -Hölder continuous functions have finite seminorm? Naively bounding by the Hölder norm times just barely fails.","\Omega\subset\mathbb{R}^d s\in(0,1) p\in(1,\infty) 
[f]_{W^{s,p}}^p=\int_\Omega\int_\Omega\frac{|f(x)-f(y)|^p}{|x-y|^{d+sp}}dxdy.
 s [f]_{W^{s,p}} |f(x)-f(y)| |x-y|^s","['real-analysis', 'functional-analysis']"
51,An operator on the space of compactly supported sequence does not satisfy a given property.,An operator on the space of compactly supported sequence does not satisfy a given property.,,"Let $\ell_0(\mathbb{N})$ denote the space of compactly sequences: $$\ell_0(\mathbb{N})=\left\{\sum_{k=1}^\infty a_k \,e_k:\#\{j:a_j\neq0<\infty\}\right\}$$ Show that the map $\lambda:\ell_0(\mathbb{N})\to\mathbb{C}$ : $\sum_{j=1}^N a_j \,e_j\mapsto\sum_{j=1}^N a_j$ does not satisfy the following property: There exists $f\in\ell_2(\mathbb{N})$ such that for all $\tau\in \ell_0(\mathbb{N})$ , $\lambda(\tau)=\sum_{j=1}^\infty\tau(j)\overline{f(j)}$ . I don't quite understand the notation $\sum_{k=1}^\infty a_k \,e_k$ to represent a sequence. In particular, could you clarify the difference between $\sum_{j=1}^N a_j e_j$ and $\sum_{j=1}^N a_j$ ? Also can I please get a hint on how to solve this problem.","Let denote the space of compactly sequences: Show that the map : does not satisfy the following property: There exists such that for all , . I don't quite understand the notation to represent a sequence. In particular, could you clarify the difference between and ? Also can I please get a hint on how to solve this problem.","\ell_0(\mathbb{N}) \ell_0(\mathbb{N})=\left\{\sum_{k=1}^\infty a_k \,e_k:\#\{j:a_j\neq0<\infty\}\right\} \lambda:\ell_0(\mathbb{N})\to\mathbb{C} \sum_{j=1}^N a_j \,e_j\mapsto\sum_{j=1}^N a_j f\in\ell_2(\mathbb{N}) \tau\in \ell_0(\mathbb{N}) \lambda(\tau)=\sum_{j=1}^\infty\tau(j)\overline{f(j)} \sum_{k=1}^\infty a_k \,e_k \sum_{j=1}^N a_j e_j \sum_{j=1}^N a_j","['real-analysis', 'sequences-and-series', 'functional-analysis', 'lp-spaces']"
52,Obtaining the integral kernel of an operator,Obtaining the integral kernel of an operator,,"Let's say I have a (bounded for now) linear operator $A$ on $L^2(\mathbb R^n)$ say, and I would like to find its kernel (which I'll suppose exists), i.e. a function $K(x, y)$ such that $Af(x) = \int K(x, y)f(y)dy$ . Is it possible to find an expression for $K(x, y)$ if I know how $A$ acts on every function? For instance, I know $\langle Af, g \rangle$ and some of my initial thoughts is that if I can let $f = \delta_y$ and $g = \delta_x$ then $A\delta_y(z) = \int K(z, u)\delta_y(u)du = K(z, y)$ and therefore $$\langle Af, g \rangle = \int A\delta_y(z)\overline{\delta_x(z)}dz = \int K(z, y) \delta_x(z)dz = K(x, y).$$ But I'm not sure if this method is permitted. I can't think of any counterexamples but I know the delta does not belong to $L^2$ and can't be approximated by $L^2$ functions. What's the usual technique to get the kernel of an integral operator?","Let's say I have a (bounded for now) linear operator on say, and I would like to find its kernel (which I'll suppose exists), i.e. a function such that . Is it possible to find an expression for if I know how acts on every function? For instance, I know and some of my initial thoughts is that if I can let and then and therefore But I'm not sure if this method is permitted. I can't think of any counterexamples but I know the delta does not belong to and can't be approximated by functions. What's the usual technique to get the kernel of an integral operator?","A L^2(\mathbb R^n) K(x, y) Af(x) = \int K(x, y)f(y)dy K(x, y) A \langle Af, g \rangle f = \delta_y g = \delta_x A\delta_y(z) = \int K(z, u)\delta_y(u)du = K(z, y) \langle Af, g \rangle = \int A\delta_y(z)\overline{\delta_x(z)}dz = \int K(z, y) \delta_x(z)dz = K(x, y). L^2 L^2","['real-analysis', 'functional-analysis']"
53,What do conjugate points mean in lagrangian mechanics?,What do conjugate points mean in lagrangian mechanics?,,"Good evening, my mathematical physics professor explained conjugate points to us in a way that just didn't make much sense to me, so I looked for a deeper more geometric definition online. I found some, but I'm having trouble understanding the connection between the ""geometric"" definition of conjugate points and the ""lagrangian"" one. I'll start by sharing my understanding of it in the geometric sense (which may well be wrong): Take a point $p$ on a riemannian manifold and a smooth family $\gamma_s(t)$ of geodesics passing through it, say at $t=0$ . Let $\gamma = \gamma_0.$ We define the Jacobi field associated to $\gamma$ as $$ J(t) = \left.\frac{d}{ds}\gamma_s(t)\right|_{s=0} $$ which intuitively describes the separation between the points on $\gamma$ and an infinitely close other geodesic in the family at time $t$ . From what I've read, an equivalent definition is to say that a Jacobi field is any solution of the equation $$ \frac{D^2}{dt^2}J + R(J, \dot \gamma)\dot \gamma = 0 \ \ \ (\star_1)$$ where $\frac{D^2}{dt^2}$ denotes the derivative along $\gamma$ and $R$ is the Riemann curvature tensor. A point $q = \gamma(\tau)$ is said to be a conjugate of $p$ if $J(\tau) = 0$ , meaning that our family ""merges"" into this point (at least in the limit). For example, any two opposite points of $S^2$ are conjugate. Now for the lagrangian definition: the setting is 1-d lagrangian mechanics, so we have some functions $\gamma, \eta: \Bbb{R} \to \Bbb{R}$ , an action functional $$ \mathcal{A}_\mathcal{L}(\gamma) \triangleq \int_{t_0}^{t_1} \mathcal{L}(\gamma(t), \dot\gamma(t), t)dt, $$ its first variation (let's call $\phi(\lambda) = \mathcal{A}_\mathcal{L}(\gamma+\lambda\eta) $ ) $$ \delta\mathcal{A}_\mathcal{L}(\gamma; \eta) \triangleq \phi'(0)$$ and its second variation $$ \delta^2\mathcal{A}_\mathcal{L}(\gamma; \eta) \triangleq \phi''(0).$$ It turns out that if we freeze $\gamma$ we can write $\delta^2\mathcal{A}_\mathcal{L}(\gamma; \eta)$ as an action functional $\mathcal{A}_\mathcal{Q}(\eta)$ ( $\mathcal{Q}$ is called ""auxiliary lagrangian""), so we can consider the Euler-Lagrange equation associated with it. Any solution to this equation is called a Jacobi field, and if we couple it with the initial conditions $$ \eta(\tau) = 0, \dot\eta(\tau) = 1$$ for some time $\tau$ , we have a unique solution $\tilde\eta(t)$ . Finally, for every solution $\tau_\star$ of $\tilde\eta(\tau_\star) = 0$ we say that $ (\tau, \gamma(\tau)) $ and $(\tau_\star, \gamma(\tau_\star))$ are conjugate points. For completeness, the mentioned Euler-Lagrange equation turns out to be $$ -\left(a \dot \eta\right)' + (c-\dot b)\eta=0 \ \ \ (\star_2)$$ where $$ a(t) = \mathcal{L}_{\dot\gamma\dot\gamma}(\gamma(t), \dot\gamma(t), t), b(t) = \mathcal{L}_{\gamma\dot\gamma}(\gamma(t), \dot\gamma(t), t), c(t) = \mathcal{L}_{\gamma\gamma}(\gamma(t), \dot\gamma(t), t). $$ Now, several questions: Do $\star_1$ and $\star_2$ correspond in some way? They do look sort of similar but I can't really figure out what $J$ and $R$ would be in the lagrangian formulation. It seems like $J$ would correspond to $\eta$ , but the substitution doesn't quite turn out right. What manifold am I considering in the lagrangian setting? What is the ""hidden geometry"" of this context? A while ago we defined the Jacobi metric in the mechanical context (where $L = T-V$ and $E = T+V$ is conserved) as $$ ds^2 = (E - V(q))\sum_{h, k}A_{hk}(q)dq_hdq_k $$ where $q_i$ are the lagrangian coordinates and $A$ is the kinetic matrix, but I can't really see the connection with what I described until now. I guess the manifold we are considering now, since we are in the 1-dimensional case, would be $(t, \gamma(t)) \in \Bbb{R} \times \Bbb{R}$ with the euclidean metric on the time coordinate and some metric dependent on the lagrangian on the space coordinate, but I can't really pull everything together. What do conjugate points represent in the mechanical context? Really, I can't figure this out. This auxiliary lagrangian doesn't seem to have some neat physical or geometric meaning to me, so I'm just a bit confused about this. The physics lesson looked like a nonsensical gibberish of derivatives to me, so any help is appreciated.","Good evening, my mathematical physics professor explained conjugate points to us in a way that just didn't make much sense to me, so I looked for a deeper more geometric definition online. I found some, but I'm having trouble understanding the connection between the ""geometric"" definition of conjugate points and the ""lagrangian"" one. I'll start by sharing my understanding of it in the geometric sense (which may well be wrong): Take a point on a riemannian manifold and a smooth family of geodesics passing through it, say at . Let We define the Jacobi field associated to as which intuitively describes the separation between the points on and an infinitely close other geodesic in the family at time . From what I've read, an equivalent definition is to say that a Jacobi field is any solution of the equation where denotes the derivative along and is the Riemann curvature tensor. A point is said to be a conjugate of if , meaning that our family ""merges"" into this point (at least in the limit). For example, any two opposite points of are conjugate. Now for the lagrangian definition: the setting is 1-d lagrangian mechanics, so we have some functions , an action functional its first variation (let's call ) and its second variation It turns out that if we freeze we can write as an action functional ( is called ""auxiliary lagrangian""), so we can consider the Euler-Lagrange equation associated with it. Any solution to this equation is called a Jacobi field, and if we couple it with the initial conditions for some time , we have a unique solution . Finally, for every solution of we say that and are conjugate points. For completeness, the mentioned Euler-Lagrange equation turns out to be where Now, several questions: Do and correspond in some way? They do look sort of similar but I can't really figure out what and would be in the lagrangian formulation. It seems like would correspond to , but the substitution doesn't quite turn out right. What manifold am I considering in the lagrangian setting? What is the ""hidden geometry"" of this context? A while ago we defined the Jacobi metric in the mechanical context (where and is conserved) as where are the lagrangian coordinates and is the kinetic matrix, but I can't really see the connection with what I described until now. I guess the manifold we are considering now, since we are in the 1-dimensional case, would be with the euclidean metric on the time coordinate and some metric dependent on the lagrangian on the space coordinate, but I can't really pull everything together. What do conjugate points represent in the mechanical context? Really, I can't figure this out. This auxiliary lagrangian doesn't seem to have some neat physical or geometric meaning to me, so I'm just a bit confused about this. The physics lesson looked like a nonsensical gibberish of derivatives to me, so any help is appreciated.","p \gamma_s(t) t=0 \gamma = \gamma_0. \gamma  J(t) = \left.\frac{d}{ds}\gamma_s(t)\right|_{s=0}  \gamma t  \frac{D^2}{dt^2}J + R(J, \dot \gamma)\dot \gamma = 0 \ \ \ (\star_1) \frac{D^2}{dt^2} \gamma R q = \gamma(\tau) p J(\tau) = 0 S^2 \gamma, \eta: \Bbb{R} \to \Bbb{R}  \mathcal{A}_\mathcal{L}(\gamma) \triangleq \int_{t_0}^{t_1} \mathcal{L}(\gamma(t), \dot\gamma(t), t)dt,  \phi(\lambda) = \mathcal{A}_\mathcal{L}(\gamma+\lambda\eta)   \delta\mathcal{A}_\mathcal{L}(\gamma; \eta) \triangleq \phi'(0)  \delta^2\mathcal{A}_\mathcal{L}(\gamma; \eta) \triangleq \phi''(0). \gamma \delta^2\mathcal{A}_\mathcal{L}(\gamma; \eta) \mathcal{A}_\mathcal{Q}(\eta) \mathcal{Q}  \eta(\tau) = 0, \dot\eta(\tau) = 1 \tau \tilde\eta(t) \tau_\star \tilde\eta(\tau_\star) = 0  (\tau, \gamma(\tau))  (\tau_\star, \gamma(\tau_\star))  -\left(a \dot \eta\right)' + (c-\dot b)\eta=0 \ \ \ (\star_2)  a(t) = \mathcal{L}_{\dot\gamma\dot\gamma}(\gamma(t), \dot\gamma(t), t), b(t) = \mathcal{L}_{\gamma\dot\gamma}(\gamma(t), \dot\gamma(t), t), c(t) = \mathcal{L}_{\gamma\gamma}(\gamma(t), \dot\gamma(t), t).  \star_1 \star_2 J R J \eta L = T-V E = T+V  ds^2 = (E - V(q))\sum_{h, k}A_{hk}(q)dq_hdq_k  q_i A (t, \gamma(t)) \in \Bbb{R} \times \Bbb{R}","['functional-analysis', 'differential-geometry', 'riemannian-geometry', 'mathematical-physics', 'euler-lagrange-equation']"
54,functional derivative and dualspace,functional derivative and dualspace,,"Consider the function space $F=\{ f : \mathbb{R}^m \rightarrow \mathbb{R}^n\}$ and the empirical scalarproduct: $$ \langle f,g\rangle:=1/n\sum^n_{i=1}f(x_i)^Tg(x_i), $$ for a a finite dataset $x_1, \ldots, x_n \in \mathbb{R}^m$ . The Dualspace is defined as $F^*=\{\langle d,\cdot\rangle:d\in F\}$ . The cost functional $C$ (for example: $$C(f)=1/n\sum_{i=1}^n\|f(x_i)-f*(x_i)\|^2$$ ) only depends on the values of $f \in F$ at the data points. As a result it was said that, the (functional) derivative of the $\operatorname{cost} C$ at point $f_0 \in F$ can be viewed as an element of $F^{*}$ , which we write $\left.\partial_{f} C\right|_{f_0}$ , such that $\left.\partial_f C\right|_{f_0}=\langle d|_{f_0}, \cdot\rangle$ for some $d|_{f_0} \in F$ . Can someone explain why the last equation holds? Also im not familiar with the functional derivative, but according to the definition of wiki: https://en.wikipedia.org/wiki/Functional_derivative shouldn't the derivative at $f_0$ also be a function from $\mathbb{R}^m$ to $\mathbb{R}^n$ ?","Consider the function space and the empirical scalarproduct: for a a finite dataset . The Dualspace is defined as . The cost functional (for example: ) only depends on the values of at the data points. As a result it was said that, the (functional) derivative of the at point can be viewed as an element of , which we write , such that for some . Can someone explain why the last equation holds? Also im not familiar with the functional derivative, but according to the definition of wiki: https://en.wikipedia.org/wiki/Functional_derivative shouldn't the derivative at also be a function from to ?","F=\{ f : \mathbb{R}^m \rightarrow \mathbb{R}^n\} 
\langle f,g\rangle:=1/n\sum^n_{i=1}f(x_i)^Tg(x_i),
 x_1, \ldots, x_n \in \mathbb{R}^m F^*=\{\langle d,\cdot\rangle:d\in F\} C C(f)=1/n\sum_{i=1}^n\|f(x_i)-f*(x_i)\|^2 f \in F \operatorname{cost} C f_0 \in F F^{*} \left.\partial_{f} C\right|_{f_0} \left.\partial_f C\right|_{f_0}=\langle d|_{f_0}, \cdot\rangle d|_{f_0} \in F f_0 \mathbb{R}^m \mathbb{R}^n","['functional-analysis', 'statistics', 'dual-spaces', 'frechet-derivative']"
55,"Assume $f,g, fg\in L^1(\mathbb{R}^n)$ and $\widehat{f}\in L^1(\mathbb{R}^n)$. Prove that $\widehat{fg}=(2\pi)^{-n}\widehat{f}*\widehat{g}$",Assume  and . Prove that,"f,g, fg\in L^1(\mathbb{R}^n) \widehat{f}\in L^1(\mathbb{R}^n) \widehat{fg}=(2\pi)^{-n}\widehat{f}*\widehat{g}","To demonstrate this exercise we will use the following theorem: Theorem: Let $f\in L^1(\mathbb{R}^n)$ and assume that $\widehat{f}\in L^1(\mathbb{R}^n)$ . Then $f$ is equivalent to a continuous function. Therefore, we assume with no loss of generality that $f$ is continuous. Then for $x\in\mathbb{R}^n$ , $$f(x)=(2\pi)^{-n}\displaystyle{\int_{\mathbb{R}^n}\widehat{f}(\xi)e^{ix\cdot\xi}\,d\xi}.$$ by the previous theorem and by definition of Fourier transform \begin{align*} \widehat{fg}(\xi)&=\int_{\mathbb{R}^n}f(x)g(x)e^{-ix\cdot\xi}\,dx\\ &= \int_{\mathbb{R}^n}\left[(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)e^{ix\cdot\eta}\,d\eta\right]g(x)e^{-ix\cdot\xi}\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}g(x)e^{-ix\cdot\xi}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)e^{ix\cdot\eta}\,d\eta\right]\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)g(x)e^{-ix(\xi-\eta)}\,d\eta\right]\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)g(x)e^{-ix\cdot(\xi-\eta)}\,dx\right]\,d\eta\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)\left[\int_{\mathbb{R}^n}g(x)e^{-ix\cdot(\xi-\eta)}\,dx\right]\,d\eta\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)\widehat{g}(\xi-\eta)\,d\eta\\&=(2\pi)^{-n}\left(\widehat{f}*\widehat{g}\right)(\xi). \end{align*} Remark: Note that Fubini's theorem has been applied to the fifth line of the chain of equalities above, which is possible since $\widehat{f}(\eta)g(x)\in L^1(\mathbb{R}^n\times\mathbb{R}^n)$ which implies that $\displaystyle{\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g}\hspace{.2cm}$ exists. Indeed: \begin{equation} \left|\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g\right|\leq\int_{\mathbb{R}^n\times\mathbb{R}^n}|\widehat{f}g|=\|\widehat{f}g\|_{L^1}\leq\|\widehat{f}\|_{L^1}\|g\|_{L^1}<\infty\qquad (1). \end{equation} My concerns about the above proof are: The chain of equalities immediately above is correct to justify that $\displaystyle{\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g}$ exists? Also I'm not sure if the last inequality in $(1)$ is true. Did I apply Fubbini's theorem correctly?","To demonstrate this exercise we will use the following theorem: Theorem: Let and assume that . Then is equivalent to a continuous function. Therefore, we assume with no loss of generality that is continuous. Then for , by the previous theorem and by definition of Fourier transform Remark: Note that Fubini's theorem has been applied to the fifth line of the chain of equalities above, which is possible since which implies that exists. Indeed: My concerns about the above proof are: The chain of equalities immediately above is correct to justify that exists? Also I'm not sure if the last inequality in is true. Did I apply Fubbini's theorem correctly?","f\in L^1(\mathbb{R}^n) \widehat{f}\in L^1(\mathbb{R}^n) f f x\in\mathbb{R}^n f(x)=(2\pi)^{-n}\displaystyle{\int_{\mathbb{R}^n}\widehat{f}(\xi)e^{ix\cdot\xi}\,d\xi}. \begin{align*}
\widehat{fg}(\xi)&=\int_{\mathbb{R}^n}f(x)g(x)e^{-ix\cdot\xi}\,dx\\ &= \int_{\mathbb{R}^n}\left[(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)e^{ix\cdot\eta}\,d\eta\right]g(x)e^{-ix\cdot\xi}\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}g(x)e^{-ix\cdot\xi}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)e^{ix\cdot\eta}\,d\eta\right]\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)g(x)e^{-ix(\xi-\eta)}\,d\eta\right]\,dx\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\left[\int_{\mathbb{R}^n}\widehat{f}(\eta)g(x)e^{-ix\cdot(\xi-\eta)}\,dx\right]\,d\eta\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)\left[\int_{\mathbb{R}^n}g(x)e^{-ix\cdot(\xi-\eta)}\,dx\right]\,d\eta\\&=(2\pi)^{-n}\int_{\mathbb{R}^n}\widehat{f}(\eta)\widehat{g}(\xi-\eta)\,d\eta\\&=(2\pi)^{-n}\left(\widehat{f}*\widehat{g}\right)(\xi).
\end{align*} \widehat{f}(\eta)g(x)\in L^1(\mathbb{R}^n\times\mathbb{R}^n) \displaystyle{\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g}\hspace{.2cm} \begin{equation}
\left|\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g\right|\leq\int_{\mathbb{R}^n\times\mathbb{R}^n}|\widehat{f}g|=\|\widehat{f}g\|_{L^1}\leq\|\widehat{f}\|_{L^1}\|g\|_{L^1}<\infty\qquad (1).
\end{equation} \displaystyle{\int_{\mathbb{R}^n\times\mathbb{R}^n}\widehat{f}g} (1)","['real-analysis', 'functional-analysis', 'fourier-analysis', 'lp-spaces', 'fourier-transform']"
56,Show that in $\mathbb R^3$ the function $F(x)=\frac{-1}{4\pi|x|}e^{-|x|}$ is a fundamental solution of the operator $\Delta-I$,Show that in  the function  is a fundamental solution of the operator,\mathbb R^3 F(x)=\frac{-1}{4\pi|x|}e^{-|x|} \Delta-I,"Show that in $\mathbb R^3$ the function $F(x)=\frac{-1}{4\pi|x|}e^{-|x|}$ is a fundamental solution of the operator $\Delta-I$ By the way,the function $F$ is the Yukawa potential.What I try is that if $F$ is any fundamental solution, taking Fourier transform about $(\Delta-I)F=\delta$ plus noting that the characteristic polynomial of the operator is $-(1+4\pi^2|\xi|^2)$ I can get $$F=\int _{\mathbb R^d}\frac{1}{-(1+4\pi^2|\xi|^2)}e^{2\pi ix\xi}d\xi=-\frac{1}{2}e^{-|x|}$$ in the sense of distribution.So in $\mathbb R^3$ I think $-\frac{1}{2}e^{-|x|}$ may be a fundamental solution which is different from the given function $\frac{-1}{4\pi|x|}e^{-|x|}$ (Is what I calculate right?this incidentally show that in general the fundamental solution is not unique?).But how to show $\frac{-1}{4\pi|x|}e^{-|x|}$ is also a fundamental solution?The hint says to use two identity $$\int_{|\xi|=1}e^{2\pi i\xi x}d\sigma(\xi)=\frac{2\sin(2\pi|x|)}{|x|},\hat{Q_y}(\xi)=e^{-2\pi y|\xi|}\frac{sign(\xi)}{i}$$ where $Q$ is the conjugate Possion kernel.But I can't apply them to solve the problem.Can anyone give me a helping hand,thank you","Show that in the function is a fundamental solution of the operator By the way,the function is the Yukawa potential.What I try is that if is any fundamental solution, taking Fourier transform about plus noting that the characteristic polynomial of the operator is I can get in the sense of distribution.So in I think may be a fundamental solution which is different from the given function (Is what I calculate right?this incidentally show that in general the fundamental solution is not unique?).But how to show is also a fundamental solution?The hint says to use two identity where is the conjugate Possion kernel.But I can't apply them to solve the problem.Can anyone give me a helping hand,thank you","\mathbb R^3 F(x)=\frac{-1}{4\pi|x|}e^{-|x|} \Delta-I F F (\Delta-I)F=\delta -(1+4\pi^2|\xi|^2) F=\int _{\mathbb R^d}\frac{1}{-(1+4\pi^2|\xi|^2)}e^{2\pi ix\xi}d\xi=-\frac{1}{2}e^{-|x|} \mathbb R^3 -\frac{1}{2}e^{-|x|} \frac{-1}{4\pi|x|}e^{-|x|} \frac{-1}{4\pi|x|}e^{-|x|} \int_{|\xi|=1}e^{2\pi i\xi x}d\sigma(\xi)=\frac{2\sin(2\pi|x|)}{|x|},\hat{Q_y}(\xi)=e^{-2\pi y|\xi|}\frac{sign(\xi)}{i} Q","['functional-analysis', 'distribution-theory']"
57,"Analytic K-Homology (Higson, Roe) - Exercise 8.8.8","Analytic K-Homology (Higson, Roe) - Exercise 8.8.8",,"I am trying to understand the proof of Proposition 8.3.16 (***) in Higson and Roe's Analytic K-Homology book. They rely on Exercise 8.8.8 (which is the only argument I don't fully understand). It goes as follows: Let $A$ be a $C^*$ -algebra, $\rho:A\rightarrow \mathbb{B}(H)$ a representation of $A$ on a (separable, multigraded) Hilbert space $H$ . Define $$ \mathcal{D}_\rho(A)=\{T\in\mathbb{B}(H)\;:\;[T,\rho(a)]\sim0,\;\;\;\forall a\in A\},$$ and $$\mathcal{D}_{\rho}(A//A)=\{T\in \mathcal{D}_{\rho}(A)\;:\;T\rho(a)\sim0\sim\rho(a)T,\;\;\;\forall a\in A\}, $$ where $T\sim S$ if and only if $T-S\in\mathbb{K}(H)$ . Exercise 8.8.8: Suposse $P\in\mathcal{D}_{\rho}(A)$ is self-adjoint and that $\rho(a)P\rho(a^*)$ is positive modulo compact operators for every $a\in A$ . Prove that $P$ is positive modulo $\mathcal{D}_{\rho}(A//A)$ . The book provides a hint: since $P$ is self-adjoint, we can write $P=P_+-P_-$ , where $P_{\pm}$ are positive and $P_+P_-=P_-P+=0$ . Following this line of reasoning, the idea would be to show that $P_-\in\mathcal{D}_{\rho}(A//A)$ . I have tried computing $$ \langle \rho(a)P_-x,\rho(a)P_-x\rangle,$$ for some $x\in H$ , with the hope that I could prove this vanishes modulo compacts, but to no avail. (***)- Disclaimer: the reason why I also explicted this result is because in that setting one might use that the representation $\rho$ is non-degenerate (Lemma 8.3.8), meaning that $\overline{\rho(A)}H$ is dense in $H$ , although Exercise 8.8.8 appears to be true in the more general picture.","I am trying to understand the proof of Proposition 8.3.16 (***) in Higson and Roe's Analytic K-Homology book. They rely on Exercise 8.8.8 (which is the only argument I don't fully understand). It goes as follows: Let be a -algebra, a representation of on a (separable, multigraded) Hilbert space . Define and where if and only if . Exercise 8.8.8: Suposse is self-adjoint and that is positive modulo compact operators for every . Prove that is positive modulo . The book provides a hint: since is self-adjoint, we can write , where are positive and . Following this line of reasoning, the idea would be to show that . I have tried computing for some , with the hope that I could prove this vanishes modulo compacts, but to no avail. (***)- Disclaimer: the reason why I also explicted this result is because in that setting one might use that the representation is non-degenerate (Lemma 8.3.8), meaning that is dense in , although Exercise 8.8.8 appears to be true in the more general picture.","A C^* \rho:A\rightarrow \mathbb{B}(H) A H  \mathcal{D}_\rho(A)=\{T\in\mathbb{B}(H)\;:\;[T,\rho(a)]\sim0,\;\;\;\forall a\in A\}, \mathcal{D}_{\rho}(A//A)=\{T\in \mathcal{D}_{\rho}(A)\;:\;T\rho(a)\sim0\sim\rho(a)T,\;\;\;\forall a\in A\},  T\sim S T-S\in\mathbb{K}(H) P\in\mathcal{D}_{\rho}(A) \rho(a)P\rho(a^*) a\in A P \mathcal{D}_{\rho}(A//A) P P=P_+-P_- P_{\pm} P_+P_-=P_-P+=0 P_-\in\mathcal{D}_{\rho}(A//A)  \langle \rho(a)P_-x,\rho(a)P_-x\rangle, x\in H \rho \overline{\rho(A)}H H","['functional-analysis', 'analysis', 'hilbert-spaces', 'c-star-algebras', 'k-theory']"
58,Is the integral operator bounded?,Is the integral operator bounded?,,"I have the following problem. Suppose $\mu$ - measure on $T$ , $K(s,t) \geq 0$ and measurable function on $T \times T$ with respect to $\mu \times \mu$ . U is the integral operator: $$(Uf)(s) = \int\limits_T K(s,t)f(t) dt.$$ I know that $Uf \in L^2(\mu)$ if $f \in L^\infty(\mu)$ and the following estimate: $$\int\limits_T K(s,t) K(s,x) ds \leq C (K(t,x) + K(x,t)),$$ where $C>0$ . I want to prove that $U$ is bounded operator from $L^2(\mu)$ to itself. I observe that \begin{align} (Uf, Uf) &= \int\limits_T \left( \int\limits_T K(s, t) f(t) dt \int\limits_T K(s, \tau) f(\tau) d\tau\right) ds\\ & = \int\limits_T \int\limits_T \int\limits_T K(s, t) K(s, \tau)ds f(\tau) f(t) dt d\tau \\ & \leq \int\limits_T \int\limits_T f(\tau) f(t) C(K(t, \tau) + K(\tau, t)) dt d\tau \\ &\leq 2C(Uf, f) \end{align} for every $f \geq 0$ and $(\cdot,\cdot)$ means inner product in $L^2(\mu)$ . But I don't is it useful.","I have the following problem. Suppose - measure on , and measurable function on with respect to . U is the integral operator: I know that if and the following estimate: where . I want to prove that is bounded operator from to itself. I observe that for every and means inner product in . But I don't is it useful.","\mu T K(s,t) \geq 0 T \times T \mu \times \mu (Uf)(s) = \int\limits_T K(s,t)f(t) dt. Uf \in L^2(\mu) f \in L^\infty(\mu) \int\limits_T K(s,t) K(s,x) ds \leq C (K(t,x) + K(x,t)), C>0 U L^2(\mu) \begin{align}
(Uf, Uf) &= \int\limits_T \left( \int\limits_T K(s, t) f(t) dt \int\limits_T K(s, \tau) f(\tau) d\tau\right) ds\\
& = \int\limits_T \int\limits_T \int\limits_T K(s, t) K(s, \tau)ds f(\tau) f(t) dt d\tau \\
& \leq \int\limits_T \int\limits_T f(\tau) f(t) C(K(t, \tau) + K(\tau, t)) dt d\tau \\
&\leq 2C(Uf, f)
\end{align} f \geq 0 (\cdot,\cdot) L^2(\mu)","['functional-analysis', 'operator-theory']"
59,"Show that there exists a unique bounded linear operator T such that $\langle T_n x,y\rangle \to \langle Tx,y\rangle$",Show that there exists a unique bounded linear operator T such that,"\langle T_n x,y\rangle \to \langle Tx,y\rangle","Let H be a Hilbert space. Assuming $\{Tn\}_n$ is a sequence of bounded linear operators from H to H such that for all x, y $\in$ H we have $\langle T_n x, y\rangle$ converges as $n \to \infty$ .Show that there exists a unique bounded linear operator T such that $\langle T_n x,y\rangle \to \langle Tx,y\rangle$ . I tried to use a result of extension which states that we can extend in a unique way a bounded linear operator over a dense subset to an operator over the whole space. But I don't see how to use it and if it's a good idea.","Let H be a Hilbert space. Assuming is a sequence of bounded linear operators from H to H such that for all x, y H we have converges as .Show that there exists a unique bounded linear operator T such that . I tried to use a result of extension which states that we can extend in a unique way a bounded linear operator over a dense subset to an operator over the whole space. But I don't see how to use it and if it's a good idea.","\{Tn\}_n \in \langle T_n x, y\rangle n \to \infty \langle T_n x,y\rangle \to \langle Tx,y\rangle","['functional-analysis', 'hilbert-spaces']"
60,alternative for Schwarz inequality,alternative for Schwarz inequality,,"I wish to show that $$\sum_{n=1}^{\infty}|\langle e_n, x\rangle||\langle e_n,y \rangle| \leq \|x\|\|y\|$$ where $\{e_n\}$ is an orthonormal system. But my professor wants me to prove it without using Cauchy-Schwarz (or Hölder) inequality. His sugestion is to use the obvious inequality $(a-b)^2 \geq 0$ for the sequences $a_n = |\langle e_n, x\rangle|$ and $b_n = |\langle e_n, y\rangle| $ . It follows that, in general, $$ \sum_{n=1}^{\infty}\{(a_n - b_n)^2\} \geq 0 \implies \sum_{n=1}^{\infty}\{a_n^2 + b_n^2 - 2a_nb_n\}\geq 0$$ is this right? if so, by Bessel's inequality (i'm allowed to use it lol), $\sum a_n^2 = \sum|\langle e_n, x\rangle|^2 \leq \|x\|$ and $\sum b_n^2 =\sum |\langle e_n, y\rangle|^2 \leq \|y\|^2$ . With this, I get to the inequality: $$ 2\sum_{n=1}^{\infty}|\langle e_n, x\rangle||\langle e_n,y \rangle| \leq  \|x\|^2 + \|y\|^2$$ and... this is not quite what I want. Did i do something wrong or i'm forgetting something? how can I proceeed or restart it? any help will be very appreciated! thanks in advance.","I wish to show that where is an orthonormal system. But my professor wants me to prove it without using Cauchy-Schwarz (or Hölder) inequality. His sugestion is to use the obvious inequality for the sequences and . It follows that, in general, is this right? if so, by Bessel's inequality (i'm allowed to use it lol), and . With this, I get to the inequality: and... this is not quite what I want. Did i do something wrong or i'm forgetting something? how can I proceeed or restart it? any help will be very appreciated! thanks in advance.","\sum_{n=1}^{\infty}|\langle e_n, x\rangle||\langle e_n,y \rangle| \leq \|x\|\|y\| \{e_n\} (a-b)^2 \geq 0 a_n = |\langle e_n, x\rangle| b_n = |\langle e_n, y\rangle|   \sum_{n=1}^{\infty}\{(a_n - b_n)^2\} \geq 0 \implies \sum_{n=1}^{\infty}\{a_n^2 + b_n^2 - 2a_nb_n\}\geq 0 \sum a_n^2 = \sum|\langle e_n, x\rangle|^2 \leq \|x\| \sum b_n^2 =\sum |\langle e_n, y\rangle|^2 \leq \|y\|^2  2\sum_{n=1}^{\infty}|\langle e_n, x\rangle||\langle e_n,y \rangle| \leq  \|x\|^2 + \|y\|^2","['linear-algebra', 'functional-analysis', 'cauchy-schwarz-inequality', 'orthonormal']"
61,How did nuclear spaces come about?,How did nuclear spaces come about?,,"I researched a lot what the point of nuclear spaces is. From what I understand they were invented by Grothendieck to make a more general statement for the Kernel Theorem by Schwartz. He figured out that the Theorem holds more generally if the respective spaces are nuclear. Now a nuclear space can be defined by saying that the projective tensor product coincides with the injective tensor product. I wanted to figure out where exactly in the proof or which part of it really depends on the nuclearity. More generally I wanted to understand how Grothendieck came up with the idea that the spaces have to be nuclear. I also can't really find the proof of the theorem, I looked in Grothendiecks doctoral thesis but it is french so I am not sure which part it is. I hope there is someone here that could shed some light on this. Thanks you!","I researched a lot what the point of nuclear spaces is. From what I understand they were invented by Grothendieck to make a more general statement for the Kernel Theorem by Schwartz. He figured out that the Theorem holds more generally if the respective spaces are nuclear. Now a nuclear space can be defined by saying that the projective tensor product coincides with the injective tensor product. I wanted to figure out where exactly in the proof or which part of it really depends on the nuclearity. More generally I wanted to understand how Grothendieck came up with the idea that the spaces have to be nuclear. I also can't really find the proof of the theorem, I looked in Grothendiecks doctoral thesis but it is french so I am not sure which part it is. I hope there is someone here that could shed some light on this. Thanks you!",,"['functional-analysis', 'distribution-theory', 'topological-vector-spaces']"
62,"Proof that the trigonometric functions form a basis for $L^2[0, 2\pi]$",Proof that the trigonometric functions form a basis for,"L^2[0, 2\pi]","My math teacher has recently talked about the Fourier series; any periodic function can be written as a sum of trigonometric functions. That's cool and stuff, but he didn't prove it. We only derived a ""formula"" for the coefficients and without proof, I will feel unsatisfied. I started to dig around the internet and found stuff about Hilbert spaces, Schauder bases and $L^2$ spaces (which is a Hilbert space). I then read that the set $\{\sin(nx), \cos(nx)\}_{n \in \mathbb{N}}$ is a Schauder basis for the space $L^p[0, 2 \pi]$ and, if I have not misunderstood, will prove the thing that I want to prove. I also read about orthonormal spaces and their connection to Hilbert spaces. So in this case I want to see proof that the set of trigonometric functions form an orthonormal basis for $L^2[0, 2\pi]$ . The orthogonal part is quite easy and I have searched for proof of the rest for a while and I can't find anything. Maybe some kind person will give me proof of this? However, I don't know how complicated that proof actually is and considering that I have almost no knowledge in this field, a handwavy argument would satisfy me. If not that, then at least guidance. For example, what I need to learn to understand the proof etc.","My math teacher has recently talked about the Fourier series; any periodic function can be written as a sum of trigonometric functions. That's cool and stuff, but he didn't prove it. We only derived a ""formula"" for the coefficients and without proof, I will feel unsatisfied. I started to dig around the internet and found stuff about Hilbert spaces, Schauder bases and spaces (which is a Hilbert space). I then read that the set is a Schauder basis for the space and, if I have not misunderstood, will prove the thing that I want to prove. I also read about orthonormal spaces and their connection to Hilbert spaces. So in this case I want to see proof that the set of trigonometric functions form an orthonormal basis for . The orthogonal part is quite easy and I have searched for proof of the rest for a while and I can't find anything. Maybe some kind person will give me proof of this? However, I don't know how complicated that proof actually is and considering that I have almost no knowledge in this field, a handwavy argument would satisfy me. If not that, then at least guidance. For example, what I need to learn to understand the proof etc.","L^2 \{\sin(nx), \cos(nx)\}_{n \in \mathbb{N}} L^p[0, 2 \pi] L^2[0, 2\pi]","['functional-analysis', 'trigonometry', 'fourier-series']"
63,Function such that $f(x)+f(1-x)=1$,Function such that,f(x)+f(1-x)=1,"Given that the function $f$ is continuous and has the property $f(f(x))=1-x$ for all $x\in[0,1]$ . Find $J=\int_{0}^{1}f(x)dx$ . My try: I did this problem without finding the function $f(x)$ , but I am interested in finding the function $f(x)$ . So what I did is I replaced $x$ with $f^{-1}(x)$ in the given equation (I assumed $f^{-1}$ exists.): $$\tag{1} f(x)=1-f^{-1}(x).$$ Also, by applying $f^{-1}$ on both sides of given equation $$f(x)=f^{-1}(1-x).$$ Replacing $x$ with $1-x$ , $$f(1-x)=f^{-1}(x).$$ Substituting in $(1)$ I got $$f(x)+f(1-x)=1.$$ This actually seemed to be a easier equation but I couldn't solve it. Particularly I think that it might be a piecewise function.","Given that the function is continuous and has the property for all . Find . My try: I did this problem without finding the function , but I am interested in finding the function . So what I did is I replaced with in the given equation (I assumed exists.): Also, by applying on both sides of given equation Replacing with , Substituting in I got This actually seemed to be a easier equation but I couldn't solve it. Particularly I think that it might be a piecewise function.","f f(f(x))=1-x x\in[0,1] J=\int_{0}^{1}f(x)dx f(x) f(x) x f^{-1}(x) f^{-1} \tag{1} f(x)=1-f^{-1}(x). f^{-1} f(x)=f^{-1}(1-x). x 1-x f(1-x)=f^{-1}(x). (1) f(x)+f(1-x)=1.","['calculus', 'integration', 'functional-analysis', 'definite-integrals']"
64,Example of a rank-one operator to satisfy conditions about the spectrum,Example of a rank-one operator to satisfy conditions about the spectrum,,"$U$ is defined to be a bilateral shift operator such that $Ue_n= u_ne_{n+1}$ for $n\in \mathbb{Z}$ , where $u_n \ne 1$ . I am looking for an example of $U$ and a rank-one operator $T \in \mathcal{B}(\ell^2(\mathbb{Z}))$ so that $\sigma (U) = \{ z \in \mathbb{C} : |z| = 1 \}$ and $\sigma(U+T)=  \{ z \in \mathbb{C} : |z| \le 1 \}$ . Some thoughts: I showed that for any bilateral shift $U$ with $Ue_n=u_ne_{n+1}$ for $n \in \mathbb{Z}$ such that $|u_n|=1$ , we have $ \sigma (U) = \{ z \in \mathbb{C} : |z| = 1 \}$ . Also, I saw it from here The spectrum of the operators $\sigma(U+T) = \sigma(U) \cup \sigma(T)  $ . So I wonder if I could find some rank one operator to satisfy the condition based on that. But other suggestions will also be great. Thank you.","is defined to be a bilateral shift operator such that for , where . I am looking for an example of and a rank-one operator so that and . Some thoughts: I showed that for any bilateral shift with for such that , we have . Also, I saw it from here The spectrum of the operators . So I wonder if I could find some rank one operator to satisfy the condition based on that. But other suggestions will also be great. Thank you.",U Ue_n= u_ne_{n+1} n\in \mathbb{Z} u_n \ne 1 U T \in \mathcal{B}(\ell^2(\mathbb{Z})) \sigma (U) = \{ z \in \mathbb{C} : |z| = 1 \} \sigma(U+T)=  \{ z \in \mathbb{C} : |z| \le 1 \} U Ue_n=u_ne_{n+1} n \in \mathbb{Z} |u_n|=1  \sigma (U) = \{ z \in \mathbb{C} : |z| = 1 \} \sigma(U+T) = \sigma(U) \cup \sigma(T)  ,"['functional-analysis', 'operator-theory', 'operator-algebras']"
65,Show that operator is diagonalizable,Show that operator is diagonalizable,,"Let $T$ be the self-adjoint operator defined by $$(T x)_{n} = x_{n + 1} + x_{n - 1} \text{ for } n \in \mathbb{Z}, (x_{n})_{n \in \mathbb{Z}}$$ on $\ell^{2}(\mathbb{Z})$ . Show that $T$ is diagonalizable by giving an explicit unitary operator $B : L^{2}(\mathbb{T}, \frac{d \theta}{2 \pi}) \rightarrow \ell^{2}(\mathbb{Z})$ such that $B M_{f} = TB$ , where $f : \mathbb{T} \rightarrow \mathbb{R}$ is $f(e^{i \theta}) = 2 \cos(\theta)$ . Here $M_f$ is multiplication by $f$ such that $(M_f g)(\xi) = f(\xi)g(\xi)$ . Note also that $\mathbb{T}$ here means the unit circle. I am not sure what $B$ should be. Usually, to show diagonalizability I use the Spectral Theorem which states that every normal operator (acting on a separable Hilbert space) is diagonalizable. But here one asks to explicitly give a unitary operator.","Let be the self-adjoint operator defined by on . Show that is diagonalizable by giving an explicit unitary operator such that , where is . Here is multiplication by such that . Note also that here means the unit circle. I am not sure what should be. Usually, to show diagonalizability I use the Spectral Theorem which states that every normal operator (acting on a separable Hilbert space) is diagonalizable. But here one asks to explicitly give a unitary operator.","T (T x)_{n} = x_{n + 1} + x_{n - 1} \text{ for } n \in \mathbb{Z}, (x_{n})_{n \in \mathbb{Z}} \ell^{2}(\mathbb{Z}) T B : L^{2}(\mathbb{T}, \frac{d \theta}{2 \pi}) \rightarrow \ell^{2}(\mathbb{Z}) B M_{f} = TB f : \mathbb{T} \rightarrow \mathbb{R} f(e^{i \theta}) = 2 \cos(\theta) M_f f (M_f g)(\xi) = f(\xi)g(\xi) \mathbb{T} B","['functional-analysis', 'analysis']"
66,"Can $(X, \left \| \cdot \right \|_{T})$ being a Banach space imply $T$ as a closed operator?",Can  being a Banach space imply  as a closed operator?,"(X, \left \| \cdot \right \|_{T}) T","Let $X$ and $Y$ be Banach spaces and let $T:X \rightarrow Y$ be a linear operator. For each element $x \in X$ , define a norm (more specifically, the graph norm ) $\left \| \cdot \right \|_{T}$ on $X$ by : $$\left \| x \right \|_{T} = \left \| x \right \| + \left \|  Tx \right \|$$ for $x \in X$ . I managed to prove that if $T$ is a closed operator, then $(X, \left \| \cdot \right \|_{T})$ is a Banach space, using Cauchy sequences. I was wondering if it's possible to prove the other way around, respectively if $(X, \left \| \cdot \right \|_{T})$ is a Banach space, then $T$ is a closed operator and if so, how I could achieve this.","Let and be Banach spaces and let be a linear operator. For each element , define a norm (more specifically, the graph norm ) on by : for . I managed to prove that if is a closed operator, then is a Banach space, using Cauchy sequences. I was wondering if it's possible to prove the other way around, respectively if is a Banach space, then is a closed operator and if so, how I could achieve this.","X Y T:X \rightarrow Y x \in X \left \| \cdot \right \|_{T} X \left \| x \right \|_{T} = \left \| x \right \| + \left \|
 Tx \right \| x \in X T (X, \left \| \cdot \right \|_{T}) (X, \left \| \cdot \right \|_{T}) T","['functional-analysis', 'banach-spaces']"
67,Positive elements in Hermitian * Banach Algebras,Positive elements in Hermitian * Banach Algebras,,"Let $A$ be a commutative Hermitian * Banach algebra, that is, a commutative Banach algebra with involution such that every self-adjoint element has real spectrum. It is known that if $x\in A$ , then $y := x^\ast x \geq 0$ , that is, $y^\ast = y$ and $\sigma(y)\subset[0,\infty)$ , see ""Bonsall and Duncan, Complete normed algebras, 41.Th 5. My question is the following: let $y\in A$ be a positive element as before. Can we assume the existence of $x\in A$ such that $y = x^\ast x$ ? I'm particularly interested in the case $A = L^1 (\mathbb{R})$ , the set of integrable functions over the real line, with convolution as multiplication. Note that in this case, $A$ is a $A^\star$ -Banach algebra, that is, it is continuously embedded in a $C^\star$ -algebra. It is well known that the answer to this question is affirmative if $A$ is a $C^\star$ -algebra, but I haven't been unable to find its answer to this more general case. Edit: It seems that the answer to this question is positive if we add the condition that $0 \notin \sigma(y)$ , i.e. $\sigma(y) \subset (0,\infty)$ , see Theorem 11.20 in ""W. Rudin, Functional analysis"". However, I am still interested in the case $0 \in \sigma(y)$ , so any help will be well received.","Let be a commutative Hermitian * Banach algebra, that is, a commutative Banach algebra with involution such that every self-adjoint element has real spectrum. It is known that if , then , that is, and , see ""Bonsall and Duncan, Complete normed algebras, 41.Th 5. My question is the following: let be a positive element as before. Can we assume the existence of such that ? I'm particularly interested in the case , the set of integrable functions over the real line, with convolution as multiplication. Note that in this case, is a -Banach algebra, that is, it is continuously embedded in a -algebra. It is well known that the answer to this question is affirmative if is a -algebra, but I haven't been unable to find its answer to this more general case. Edit: It seems that the answer to this question is positive if we add the condition that , i.e. , see Theorem 11.20 in ""W. Rudin, Functional analysis"". However, I am still interested in the case , so any help will be well received.","A x\in A y := x^\ast x \geq 0 y^\ast = y \sigma(y)\subset[0,\infty) y\in A x\in A y = x^\ast x A = L^1 (\mathbb{R}) A A^\star C^\star A C^\star 0 \notin \sigma(y) \sigma(y) \subset (0,\infty) 0 \in \sigma(y)","['functional-analysis', 'operator-theory', 'c-star-algebras', 'banach-algebras']"
68,"The function $\phi\colon t \mapsto \int_{0}^{1}f(x,t)\, dx$ is Borel",The function  is Borel,"\phi\colon t \mapsto \int_{0}^{1}f(x,t)\, dx","Suppose $f\colon[0,1]\times [0,1] \rightarrow \mathbb{R} $ . Functions $x \mapsto f(x,t)$ are integrable, functions $t \mapsto f(x,t)$ are continuous. I need to prove that the following function is Borel: $$\phi\colon t \mapsto \int_{0}^{1}f(x,t)\,dx.$$ I tried to use monotone class theorem for $E=\{B \in \mathcal{B}(\mathbb{R})\mid \phi^{-1}(B) \in \mathcal{B}([0,1]) \} \subset \mathcal{B}(\mathbb{R})$ . I need to show that $E=\mathcal{B}(\mathbb{R})$ . The only thing I managed to prove is that $E$ is an algebra. So, the idea seems wrong. Please give me any ideas how to solve the problem. Thank you for any hints!","Suppose . Functions are integrable, functions are continuous. I need to prove that the following function is Borel: I tried to use monotone class theorem for . I need to show that . The only thing I managed to prove is that is an algebra. So, the idea seems wrong. Please give me any ideas how to solve the problem. Thank you for any hints!","f\colon[0,1]\times [0,1] \rightarrow \mathbb{R}  x \mapsto f(x,t) t \mapsto f(x,t) \phi\colon t \mapsto \int_{0}^{1}f(x,t)\,dx. E=\{B \in \mathcal{B}(\mathbb{R})\mid \phi^{-1}(B) \in \mathcal{B}([0,1]) \} \subset \mathcal{B}(\mathbb{R}) E=\mathcal{B}(\mathbb{R}) E","['integration', 'functional-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
69,Is this generalization of the Brier score strictly proper?,Is this generalization of the Brier score strictly proper?,,"Let $\Omega$ be a set, and let $\mathcal P$ be the set of finitely additive probability measures defined on $2^\Omega$ . If $\Omega$ is finite with $|\Omega| = N$ , then the Brier score , defined by $$B(P, \omega) = \frac{1}{N}\sum_{A \subseteq 2^\Omega} (P(A) - 1_A(\omega))^2,\tag{1}$$ for all $P \in \mathcal P$ and $\omega \in \Omega$ , is strictly proper , that is: $$\sum_\omega B(P, \omega) P(\omega) \leq \sum_\omega B(Q,\omega)P(\omega)\tag{2}$$ for all $Q \in \mathcal P$ , and the inequality is strict unless $Q = P$ . The proof involves nothing more than basic calculus. I'm wondering if this result can be generalized to arbitrary $\Omega$ . Here's the idea. First, we replace the summation in (1) with some bounded linear functional. In particular, Let $X$ be the linear space of real-valued bounded functions on $2^\Omega$ , and let $f_{P, \omega}$ be the function in $X$ defined by $f_{P, \omega}(A) = (P(A) - 1_A(\omega))^2$ . Let $\ell$ be a positive bounded linear functional on $X$ . Now generalize the definition in (1) by defining $$B(P, \omega) = \ell(f_{P, \omega}).\tag{3}$$ Next, we replace the sums in (2) with integrals. Since $B(Q, \cdot)$ is non-negative for all $Q \in \mathcal P$ , we can define its integral with respect to $P$ as the supremum of the $P$ -integrals of simple functions that are dominated by $B(Q,\cdot)$ , where the finitely additive integral of a simple function is defined in the usual way. (See this , for example.) My question, then, is Is it true that, with $B$ defined by (3), $$\int B(P, \omega)P(d\omega) \leq \int B(Q, \omega)P(d\omega)$$ holds for all $P,Q \in \mathcal P$ , with strict inequality unless $Q = P$ ? One idea I had for approaching the problem is to fix $P \in \mathcal P$ and consider the function $g_P: Q \mapsto \int B(Q,\omega)P(d\omega)$ . I want to know whether $g_P$ achieves a minimum uniquely at $P$ . Now, by Jensen's inequality $f_{\lambda P_1 + (1-\lambda) P_2, \omega} \leq \lambda f_{P_1, \omega} + (1-\lambda) f_{P_2, \omega}$ holds for all $P_1,P_2 \in P$ , $\omega \in \Omega$ , and $\lambda \in [0,1]$ . Thus, because $\ell$ and the $P$ -integral are both order-preserving, $g_P$ is a convex function. So my question is essentially a convex optimization problem. I'm uncertain how to proceed from here.","Let be a set, and let be the set of finitely additive probability measures defined on . If is finite with , then the Brier score , defined by for all and , is strictly proper , that is: for all , and the inequality is strict unless . The proof involves nothing more than basic calculus. I'm wondering if this result can be generalized to arbitrary . Here's the idea. First, we replace the summation in (1) with some bounded linear functional. In particular, Let be the linear space of real-valued bounded functions on , and let be the function in defined by . Let be a positive bounded linear functional on . Now generalize the definition in (1) by defining Next, we replace the sums in (2) with integrals. Since is non-negative for all , we can define its integral with respect to as the supremum of the -integrals of simple functions that are dominated by , where the finitely additive integral of a simple function is defined in the usual way. (See this , for example.) My question, then, is Is it true that, with defined by (3), holds for all , with strict inequality unless ? One idea I had for approaching the problem is to fix and consider the function . I want to know whether achieves a minimum uniquely at . Now, by Jensen's inequality holds for all , , and . Thus, because and the -integral are both order-preserving, is a convex function. So my question is essentially a convex optimization problem. I'm uncertain how to proceed from here.","\Omega \mathcal P 2^\Omega \Omega |\Omega| = N B(P, \omega) = \frac{1}{N}\sum_{A \subseteq 2^\Omega} (P(A) - 1_A(\omega))^2,\tag{1} P \in \mathcal P \omega \in \Omega \sum_\omega B(P, \omega) P(\omega) \leq \sum_\omega B(Q,\omega)P(\omega)\tag{2} Q \in \mathcal P Q = P \Omega X 2^\Omega f_{P, \omega} X f_{P, \omega}(A) = (P(A) - 1_A(\omega))^2 \ell X B(P, \omega) = \ell(f_{P, \omega}).\tag{3} B(Q, \cdot) Q \in \mathcal P P P B(Q,\cdot) B \int B(P, \omega)P(d\omega) \leq \int B(Q, \omega)P(d\omega) P,Q \in \mathcal P Q = P P \in \mathcal P g_P: Q \mapsto \int B(Q,\omega)P(d\omega) g_P P f_{\lambda P_1 + (1-\lambda) P_2, \omega} \leq \lambda f_{P_1, \omega} + (1-\lambda) f_{P_2, \omega} P_1,P_2 \in P \omega \in \Omega \lambda \in [0,1] \ell P g_P","['functional-analysis', 'probability-theory', 'measure-theory', 'statistics', 'convex-analysis']"
70,spectrum of two operators on Banach space,spectrum of two operators on Banach space,,"Question: Let $E$ be a Banach space, $T\in \mathcal{B}(E)$ . Prove that: for all $\epsilon > 0$ , there exist $\delta > 0$ , such that for all $S\in \mathcal{B}(E)$ , if $\|T-S\| < \delta$ , then $$\sigma(S)\subset \lbrace \lambda \in \mathbb{K}: d(\lambda, \sigma(T))< \epsilon \rbrace$$ where $\sigma(T)$ and $\sigma(S)$ is the spectrum set of $T$ and $S$ . My attempt: Let $\lambda \in \sigma (S)$ , we can suppose that $\lambda \notin \sigma(T)$ . For all $\mu \in \sigma(T)$ , I know that $\|S\| \geq |\lambda|$ , $\|T\| \geq |\mu|$ , and $$|\lambda- \mu|=\|\lambda-\mu\|=\|\lambda - S + S - T + T -\mu\|\le \|\lambda-T\|+\|S-T\|+\|T- \mu\|$$ or $$|\lambda- \mu|=\|\lambda-\mu\|=\|\lambda - S + S - T + T -\mu\|\le \|\lambda-S\|+\|S-T\|+\|S- \mu\|$$ I don't know how to use the $\|S\| \geq |\lambda|$ , $\|T\| \geq |\mu|$ to compute the $\|\lambda-S\|, \|S- \mu\|$ . I had seen two related questions Spectrum in Banach Algebra and An exercise about the spectrum of an element in Banach algebra. . But i still have no idea about my question. Thanks in advance.","Question: Let be a Banach space, . Prove that: for all , there exist , such that for all , if , then where and is the spectrum set of and . My attempt: Let , we can suppose that . For all , I know that , , and or I don't know how to use the , to compute the . I had seen two related questions Spectrum in Banach Algebra and An exercise about the spectrum of an element in Banach algebra. . But i still have no idea about my question. Thanks in advance.","E T\in \mathcal{B}(E) \epsilon > 0 \delta > 0 S\in \mathcal{B}(E) \|T-S\| < \delta \sigma(S)\subset \lbrace \lambda \in \mathbb{K}: d(\lambda, \sigma(T))< \epsilon \rbrace \sigma(T) \sigma(S) T S \lambda \in \sigma (S) \lambda \notin \sigma(T) \mu \in \sigma(T) \|S\| \geq |\lambda| \|T\| \geq |\mu| |\lambda- \mu|=\|\lambda-\mu\|=\|\lambda - S + S - T + T -\mu\|\le \|\lambda-T\|+\|S-T\|+\|T- \mu\| |\lambda- \mu|=\|\lambda-\mu\|=\|\lambda - S + S - T + T -\mu\|\le \|\lambda-S\|+\|S-T\|+\|S- \mu\| \|S\| \geq |\lambda| \|T\| \geq |\mu| \|\lambda-S\|, \|S- \mu\|","['functional-analysis', 'operator-algebras']"
71,Existence of a `Partial Weak Limit' in $L^1$,Existence of a `Partial Weak Limit' in,L^1,"Suppose that I have a sequence of functions $f_n\in L^1(\mathbb{R}^d)$ for which $\lim_{n\rightarrow \infty} \int_{\mathbb{R}^d} f_n(x)g(x)dx$ exists for all $g\in S\subset L^\infty(\mathbb{R}^d)$ , where $S$ is a closed subspace of $L^\infty(\mathbb{R}^d)$ . Is it possible (and if not, under which conditions) to conclude the existence of a `partial weak limit' $f$ which satisfies $\|f\|_{L^1}\leq \lim\inf_{n\rightarrow \infty} \|f_n\|_{L^1}$ and \begin{equation} \int_{\mathbb{R}^d} f(x)g(x)dx = \lim_{n\rightarrow \infty} \int_{\mathbb{R}^d} f_n(x)g(x)dx, \end{equation} for all $g\in S$ ?","Suppose that I have a sequence of functions for which exists for all , where is a closed subspace of . Is it possible (and if not, under which conditions) to conclude the existence of a `partial weak limit' which satisfies and for all ?","f_n\in L^1(\mathbb{R}^d) \lim_{n\rightarrow \infty} \int_{\mathbb{R}^d} f_n(x)g(x)dx g\in S\subset L^\infty(\mathbb{R}^d) S L^\infty(\mathbb{R}^d) f \|f\|_{L^1}\leq \lim\inf_{n\rightarrow \infty} \|f_n\|_{L^1} \begin{equation}
\int_{\mathbb{R}^d} f(x)g(x)dx = \lim_{n\rightarrow \infty} \int_{\mathbb{R}^d} f_n(x)g(x)dx,
\end{equation} g\in S",['functional-analysis']
72,Two versions of the spectral Theorem?,Two versions of the spectral Theorem?,,"I'm studying the spectral Theorem (for bounded self-adjoint operators) by myself and I'm following Nik Weaver's nice book. Let me first introduce some notations first. Notations: If $\mathcal{H}$ is a Hilbert space, $\mathcal{B}(\mathcal{H})$ is the (Banach space) of all bounded linear operators $A: \mathcal{H} \to \mathcal{H}$ . If $A \in \mathcal{B}(\mathcal{H})$ , $\mbox{sp}(A)$ is the spectrum of $A$ . Now, let $(X, \mathcal{F},\mu)$ be a $\sigma$ -finite measure space. A measurable Hilbert bundle over $X$ is a disjoint union: $$\mathcal{X} = \bigcup_{n\in \mathbb{N}}(X_{n}\times \mathcal{H}_{n}) $$ where $\{X_{n}\}_{n\in \mathbb{N}}$ is a measurable partition of $X$ and, for each $0 \le n \le \infty$ , $\mathcal{H}_{n}$ is a Hilbert space with dimension $n$ . Finally, $f: X \to \mathcal{H}$ is weakly-measurable if the function $x \mapsto \langle f(x),v\rangle$ is measurable for every $v \in \mathcal{H}$ . We denote $L^{2}(X;\mathcal{H})$ the set of all weakly measurable functions $f: X \to \mathcal{H}$ such that: $$||f|| := \int_{x}||f(x)||^{2}d\mu(x) < +\infty $$ modulo functions which are zero almost everywhere. This is a Hibert space with inner product: $$\langle f,g\rangle := \int_{x}\langle f(x),g(x)\rangle d\mu(x) $$ If $f \in L^{2}(X;\mathcal{H})$ , $M_{f}$ is the operator multiplication by $f$ . Also, $L^{2}(X;\mathcal{X}) := \bigoplus_{n\in \mathbb{N}}L^{2}(\mathcal{X}_{n};\mathcal{H}_{n})$ . Now, the statement of the spectral Theorem in this reference is as follows. Theorem: Let $\mathcal{B}(\mathcal{H})$ be self-adjoint. Then there exits a probability measure $\mu$ on $\mbox{sp}(A)$ , a measurable Hilbert bundle $\mathcal{X}$ over $\mbox{sp}(A)$ and an isometric isomorphism $U: L^{2}(\mbox{sp}(A);\mathcal{X}) \to \mathcal{H}$ such that $A = UM_{x}U^{-1}$ . However, I'm more interested in another version of this Theorem, which is stated in Dimock's book and goes like (with adapted notation) Theorem: Let $A \in \mathcal{B}(\mathcal{H})$ be self-adjoint. Then, there exists a measure space $(\mathcal{M},\mathcal{\Omega},\mu)$ , a bounded measurable function $\tau: \mathcal{M}\to \mathbb{R}$ and a unitary operator $U: \mathcal{H}\to L^{2}(\mathcal{M},\mu)$ such that $A = UM_{\tau}U^{-1}$ . Question: How can I obtain Dimock's version of the spectral Theorem from Weaver's version of it?","I'm studying the spectral Theorem (for bounded self-adjoint operators) by myself and I'm following Nik Weaver's nice book. Let me first introduce some notations first. Notations: If is a Hilbert space, is the (Banach space) of all bounded linear operators . If , is the spectrum of . Now, let be a -finite measure space. A measurable Hilbert bundle over is a disjoint union: where is a measurable partition of and, for each , is a Hilbert space with dimension . Finally, is weakly-measurable if the function is measurable for every . We denote the set of all weakly measurable functions such that: modulo functions which are zero almost everywhere. This is a Hibert space with inner product: If , is the operator multiplication by . Also, . Now, the statement of the spectral Theorem in this reference is as follows. Theorem: Let be self-adjoint. Then there exits a probability measure on , a measurable Hilbert bundle over and an isometric isomorphism such that . However, I'm more interested in another version of this Theorem, which is stated in Dimock's book and goes like (with adapted notation) Theorem: Let be self-adjoint. Then, there exists a measure space , a bounded measurable function and a unitary operator such that . Question: How can I obtain Dimock's version of the spectral Theorem from Weaver's version of it?","\mathcal{H} \mathcal{B}(\mathcal{H}) A: \mathcal{H} \to \mathcal{H} A \in \mathcal{B}(\mathcal{H}) \mbox{sp}(A) A (X, \mathcal{F},\mu) \sigma X \mathcal{X} = \bigcup_{n\in \mathbb{N}}(X_{n}\times \mathcal{H}_{n})  \{X_{n}\}_{n\in \mathbb{N}} X 0 \le n \le \infty \mathcal{H}_{n} n f: X \to \mathcal{H} x \mapsto \langle f(x),v\rangle v \in \mathcal{H} L^{2}(X;\mathcal{H}) f: X \to \mathcal{H} ||f|| := \int_{x}||f(x)||^{2}d\mu(x) < +\infty  \langle f,g\rangle := \int_{x}\langle f(x),g(x)\rangle d\mu(x)  f \in L^{2}(X;\mathcal{H}) M_{f} f L^{2}(X;\mathcal{X}) := \bigoplus_{n\in \mathbb{N}}L^{2}(\mathcal{X}_{n};\mathcal{H}_{n}) \mathcal{B}(\mathcal{H}) \mu \mbox{sp}(A) \mathcal{X} \mbox{sp}(A) U: L^{2}(\mbox{sp}(A);\mathcal{X}) \to \mathcal{H} A = UM_{x}U^{-1} A \in \mathcal{B}(\mathcal{H}) (\mathcal{M},\mathcal{\Omega},\mu) \tau: \mathcal{M}\to \mathbb{R} U: \mathcal{H}\to L^{2}(\mathcal{M},\mu) A = UM_{\tau}U^{-1}","['functional-analysis', 'hilbert-spaces', 'spectral-theory']"
73,What's the right definitition of continuously differentiable?,What's the right definitition of continuously differentiable?,,"Suppose $V$ and $W$ are Banach spaces, $U\subset V$ is open, and $F:U\to W$ is a differentiable function. Then the derivative of $F$ is the map $$ DF:U\to B(V;W) $$ where $B(V;W)$ is the Banach space of continuous linear maps $V\to W$ . We say that $F$ is of class $\mathcal{C}^1$ at a point $x_0\in U$ if the mapping $$ U\ni x\mapsto DF(x) \in B(V;W) $$ is continuous at $x_0$ ; we say that $F$ is of class $\mathcal{C}^1$ on $U$ if $F$ is of class $\mathcal{C}^1$ at each point in $U$ . If $X$ is an arbitrary subset of the Banach space $V$ and $f:X\to W$ is a map, then we say that $f$ is of class $\mathcal{C}^1$ on $X$ if there exists an open subset $U$ of $V$ where $X\subset U$ and a function $F:U\to W$ of class $\mathcal{C}^1$ on $U$ where $F|_X=f$ . (Informally, we can extend $f$ to an open set on which it is of class $\mathcal{C}^1$ .) See this answer for a function $f$ which is continuously differentiable at only a single point. Namely, if $g(t)=t^2\sin(1/t)$ for $t\in\mathbb{R}$ then the function $$ f(t) = \sum_{n\geq 1} \frac{g(t-1/n)}{2^n} $$ is continuously differentiable at $t=0$ . However, $f$ has discontinuities arbitrarily close to the origin so $f$ cannot be of class $\mathcal{C}^1$ on any open set containing $0$ . That is, $f$ is a function which is of class $\mathcal{C}^1$ at $0$ , but $f$ is not $\mathcal{C}^1$ on $\{0\}$ . This does not seem right to me. Of course, it is not ""typical"" for a function we encounter to behave this way. However, this example still bothers me. What can we do? Can we slightly modify the above definitions so that this does not happen? Is the answer I referenced somehow incorrect? (I couldn't prove the results he stated...)","Suppose and are Banach spaces, is open, and is a differentiable function. Then the derivative of is the map where is the Banach space of continuous linear maps . We say that is of class at a point if the mapping is continuous at ; we say that is of class on if is of class at each point in . If is an arbitrary subset of the Banach space and is a map, then we say that is of class on if there exists an open subset of where and a function of class on where . (Informally, we can extend to an open set on which it is of class .) See this answer for a function which is continuously differentiable at only a single point. Namely, if for then the function is continuously differentiable at . However, has discontinuities arbitrarily close to the origin so cannot be of class on any open set containing . That is, is a function which is of class at , but is not on . This does not seem right to me. Of course, it is not ""typical"" for a function we encounter to behave this way. However, this example still bothers me. What can we do? Can we slightly modify the above definitions so that this does not happen? Is the answer I referenced somehow incorrect? (I couldn't prove the results he stated...)",V W U\subset V F:U\to W F  DF:U\to B(V;W)  B(V;W) V\to W F \mathcal{C}^1 x_0\in U  U\ni x\mapsto DF(x) \in B(V;W)  x_0 F \mathcal{C}^1 U F \mathcal{C}^1 U X V f:X\to W f \mathcal{C}^1 X U V X\subset U F:U\to W \mathcal{C}^1 U F|_X=f f \mathcal{C}^1 f g(t)=t^2\sin(1/t) t\in\mathbb{R}  f(t) = \sum_{n\geq 1} \frac{g(t-1/n)}{2^n}  t=0 f f \mathcal{C}^1 0 f \mathcal{C}^1 0 f \mathcal{C}^1 \{0\},"['functional-analysis', 'derivatives', 'continuity', 'banach-spaces', 'frechet-derivative']"
74,Weak $L^p$ convergence for passing to the limit in piecewise linear approximation of sign function?,Weak  convergence for passing to the limit in piecewise linear approximation of sign function?,L^p,"Consider $$ S_\epsilon(\xi) =  \begin{cases} 1 & \text{ if } \xi > \epsilon \\  \xi/\epsilon &\text{ if }  |\xi| < \epsilon \\ -1 &\text{ if }  \xi < - \epsilon  \end{cases}$$ which is a smoothed version of the $\mathrm{sign}$ function. Suppose that $u_n \to u$ weakly in $L^p([0,1])$ for all $p \in [1,\infty]$ as $n \to \infty$ . Is it true that $S_\epsilon(u_n-1) \to S_\epsilon(u-1)$ weakly in some $L^p$ ?",Consider which is a smoothed version of the function. Suppose that weakly in for all as . Is it true that weakly in some ?," S_\epsilon(\xi) = 
\begin{cases}
1 & \text{ if } \xi > \epsilon \\ 
\xi/\epsilon &\text{ if }  |\xi| < \epsilon \\
-1 &\text{ if }  \xi < - \epsilon 
\end{cases} \mathrm{sign} u_n \to u L^p([0,1]) p \in [1,\infty] n \to \infty S_\epsilon(u_n-1) \to S_\epsilon(u-1) L^p","['real-analysis', 'calculus', 'functional-analysis', 'measure-theory', 'lp-spaces']"
75,"Weyl theorem, compact and relatively compact operators","Weyl theorem, compact and relatively compact operators",,"Weyl's Theorem says that if $A$ is a $T$ -compact operator on a Banach space $X$ , then $T$ and $T+A$ have the same essential spectrum. To be $T$ -compact, $A$ must satisfy two conditions (1) $D(T)\subset D(A)$ (2) For any bounded sequence $\{u_n\}\subset D(T)$ such that $\{Tu_n\}$ is bounded then $\{Au_n\}$ contains a convergent sub-sequence. I have a few questions: (1) From what I understand, the definition of being compact is that for any bounded sequence $\{u_n\}\subset X$ then $\{Au_n\}$ contains a convergent sub-sequence. Thus it implies that the domain of a compact operator be all of $X$ . Is that right? (2) If $A$ is compact, it is automatically $T$ -compact, thus satisfies the condition of the theorem. Is that right? (3) If I have an operator $T+A$ with both $T$ and $A$ having each a ""natural"" domain such that the condition $D(T)\subset D(A)$ be not satisfied. However, $D(T+A)=D(T\cap A)$ . Then one could reduce the domain $T$ and $A$ to $D(T+A)$ . Then the condition (1) of relative compactness would automatically be satisfied and only condition (2) is needed to be checked. Then, in the end, one just need to check Condition (2) on $D(T+A)$ to apply Weyl's theorem. Is that right? (4) Finally, I am in the following situation: I can check that for any bounded sequence $\{u_n\}\subset D(A)$ , then $\{Au_n\}$ has a converging subsequence. So, "" $A$ is compact on its domain"". Does such a notion exist?","Weyl's Theorem says that if is a -compact operator on a Banach space , then and have the same essential spectrum. To be -compact, must satisfy two conditions (1) (2) For any bounded sequence such that is bounded then contains a convergent sub-sequence. I have a few questions: (1) From what I understand, the definition of being compact is that for any bounded sequence then contains a convergent sub-sequence. Thus it implies that the domain of a compact operator be all of . Is that right? (2) If is compact, it is automatically -compact, thus satisfies the condition of the theorem. Is that right? (3) If I have an operator with both and having each a ""natural"" domain such that the condition be not satisfied. However, . Then one could reduce the domain and to . Then the condition (1) of relative compactness would automatically be satisfied and only condition (2) is needed to be checked. Then, in the end, one just need to check Condition (2) on to apply Weyl's theorem. Is that right? (4) Finally, I am in the following situation: I can check that for any bounded sequence , then has a converging subsequence. So, "" is compact on its domain"". Does such a notion exist?",A T X T T+A T A D(T)\subset D(A) \{u_n\}\subset D(T) \{Tu_n\} \{Au_n\} \{u_n\}\subset X \{Au_n\} X A T T+A T A D(T)\subset D(A) D(T+A)=D(T\cap A) T A D(T+A) D(T+A) \{u_n\}\subset D(A) \{Au_n\} A,"['functional-analysis', 'hilbert-spaces', 'banach-spaces', 'spectral-theory', 'compact-operators']"
76,generalization of cubic spline and thin plate spline,generalization of cubic spline and thin plate spline,,"Let us consider some interpolation problems: $\renewcommand\phi\varphi$ We have some points $n$ points $x_i \in \mathbb R^d$ along with corresponding values $y_i \in \mathbb R$ . We'd like to find a function $f: \mathbb R^d \to \mathbb R$ subject to the constraints $f(x_i) = y_i \forall i=1, \ldots, n$ . For $d=1$ if we additionally require $f$ to minimize the bending energy $E = \int \left(\frac{d^2f}{dx^2} \right) dx$ (the energy of a thin rod bent in a way such that it goes through $(x_i, y_i)$ ), then it turns out $f$ must be the cubic spline. We can represent cubic splines using the RBF (radial basis function) $\phi(r) = \vert r \vert^3$ . For $d=2$ we can repeat the same, this time instead of a rod, we use a thin idealized piece of e.g. sheet metal, where the bending energy is $E = \int \left(\frac{\partial^2 f}{\partial x^2}\right)^2 + 2\left(\frac{\partial^2 f}{\partial x\partial y}\right)^2+\left(\frac{\partial^2 f}{\partial y^2}\right)^2 dxdy$ . We can then represent  the function $f$ minimizing $E$ using thin plate splines which can be represented using the RBF $\phi(x) = |r|^2 \log(|r|)$ . Are there any $RBFs$ known for the solutions of when we generalize this problem to $d=3$ and $d>3$ ? I'm not sure but I assume in the general case the energy would be $$E = \int \sum\limits_{i_1 + i_2 + \ldots+  i_n = 2} \binom{2}{i_1,i_2,\ldots,i_n}\left( \frac{\partial^2f}{\prod_j \partial x_{j}^{i_j}} \right)^2 d\pmb x.$$","Let us consider some interpolation problems: We have some points points along with corresponding values . We'd like to find a function subject to the constraints . For if we additionally require to minimize the bending energy (the energy of a thin rod bent in a way such that it goes through ), then it turns out must be the cubic spline. We can represent cubic splines using the RBF (radial basis function) . For we can repeat the same, this time instead of a rod, we use a thin idealized piece of e.g. sheet metal, where the bending energy is . We can then represent  the function minimizing using thin plate splines which can be represented using the RBF . Are there any known for the solutions of when we generalize this problem to and ? I'm not sure but I assume in the general case the energy would be","\renewcommand\phi\varphi n x_i \in \mathbb R^d y_i \in \mathbb R f: \mathbb R^d \to \mathbb R f(x_i) = y_i \forall i=1, \ldots, n d=1 f E = \int \left(\frac{d^2f}{dx^2} \right) dx (x_i, y_i) f \phi(r) = \vert r \vert^3 d=2 E = \int \left(\frac{\partial^2 f}{\partial x^2}\right)^2 + 2\left(\frac{\partial^2 f}{\partial x\partial y}\right)^2+\left(\frac{\partial^2 f}{\partial y^2}\right)^2 dxdy f E \phi(x) = |r|^2 \log(|r|) RBFs d=3 d>3 E = \int \sum\limits_{i_1 + i_2 + \ldots+  i_n = 2} \binom{2}{i_1,i_2,\ldots,i_n}\left( \frac{\partial^2f}{\prod_j \partial x_{j}^{i_j}} \right)^2 d\pmb x.","['functional-analysis', 'numerical-methods', 'spline', 'functional-calculus']"
77,$L^p$ compactness for product of two sequences of functions,compactness for product of two sequences of functions,L^p,"Let $f_n:[a,b] \to \mathbb R$ , $n \in \mathbb N$ , be a sequence of $L^p$ functions for some $p \in (1,\infty)$ . For every fixed $m\in \mathbb N^*$ , suppose that the sequence of functions $$\{f_{n}\psi_m(f_n)\}_{n \in \mathbb N}$$ has a strongly convergent subsequence in $L^p([a,b])$ . Here $\psi_m$ is a smooth function such that $$\psi_m(f) =  \begin{cases} 1 \qquad \text{ if } |f|\ge 1/m \\ 0 \qquad \text{ if } |f|\le 1/(2m) \end{cases} $$ and $0 \le \psi_m \le 1$ . Is it true that $\{f_n\}_{n\in \mathbb N}$ also has a strongly convergent subsequence in $L^p([a,b])$ ? I wanted to apply a diagonal argument: [1] , but I can't make it work properly.","Let , , be a sequence of functions for some . For every fixed , suppose that the sequence of functions has a strongly convergent subsequence in . Here is a smooth function such that and . Is it true that also has a strongly convergent subsequence in ? I wanted to apply a diagonal argument: [1] , but I can't make it work properly.","f_n:[a,b] \to \mathbb R n \in \mathbb N L^p p \in (1,\infty) m\in \mathbb N^* \{f_{n}\psi_m(f_n)\}_{n \in \mathbb N} L^p([a,b]) \psi_m \psi_m(f) = 
\begin{cases}
1 \qquad \text{ if } |f|\ge 1/m \\
0 \qquad \text{ if } |f|\le 1/(2m)
\end{cases}
 0 \le \psi_m \le 1 \{f_n\}_{n\in \mathbb N} L^p([a,b])","['real-analysis', 'functional-analysis', 'lp-spaces']"
78,Question about product of ideals in a $C^*$-algebra,Question about product of ideals in a -algebra,C^*,"Consider the following fragments from Murphy's book ' $C^*$ -algebras and operator theory' I'm trying to understand why $B \cap I = BIB$ . Attempt: The inclusion $BIB \subseteq B\cap I$ is trivial since $B$ is hereditary and $I$ is an ideal. To show the other inclusion, it suffices to show that $(B\cap I)^+ \subseteq BIB$ since the positive elements of the $C^*$ -algebra $B \cap I$ (this is a $C^*$ -subalgebra because $B \cap I$ is a closed ideal of $B$ ) linearly span $B\cap I$ . Fix $a \in B \cap I $ . Then $a^{1/2} \in B \cap I$ . Let $(u_\lambda)$ be an approximate unit for $B$ . Then $$a = \lim_\lambda u_\lambda a = \lim_\lambda {u_\lambda} a^{1/2}a^{1/2} \in BIB$$ Is this correct?","Consider the following fragments from Murphy's book ' -algebras and operator theory' I'm trying to understand why . Attempt: The inclusion is trivial since is hereditary and is an ideal. To show the other inclusion, it suffices to show that since the positive elements of the -algebra (this is a -subalgebra because is a closed ideal of ) linearly span . Fix . Then . Let be an approximate unit for . Then Is this correct?",C^* B \cap I = BIB BIB \subseteq B\cap I B I (B\cap I)^+ \subseteq BIB C^* B \cap I C^* B \cap I B B\cap I a \in B \cap I  a^{1/2} \in B \cap I (u_\lambda) B a = \lim_\lambda u_\lambda a = \lim_\lambda {u_\lambda} a^{1/2}a^{1/2} \in BIB,['functional-analysis']
79,"Properties of the linear operator $T(f)=\chi_{[0,\frac{1}{2}]}f(2x)$ where $T:L^1[0,1] \to L^1[0,1]$",Properties of the linear operator  where,"T(f)=\chi_{[0,\frac{1}{2}]}f(2x) T:L^1[0,1] \to L^1[0,1]","Properties of the linear operator $T(f)=\chi_{[0,\frac{1}{2}]}f(2x)$ where $T:L^1[0,1] \to L^1[0,1]$ I am supposed to find the operator norm, check whether it is one to one or onto and if it is compact. I think that the norm is $1/2$ as $\int_{0}^{1/2}f(2x)=1/2\int_{0}^{1}f(x)$ . It is $1-1$ as $T(f)=0$ iff $f(x)=0$ for all $x$ . It is clearly not surjective. I also think that it cannot be compact. I think that the range is closed and not finite dimensional and so $T$ is not compact. IS that true?It is closed beceasue passing into subsequence we get a function that is $0$ on $[1/2,1]$","Properties of the linear operator where I am supposed to find the operator norm, check whether it is one to one or onto and if it is compact. I think that the norm is as . It is as iff for all . It is clearly not surjective. I also think that it cannot be compact. I think that the range is closed and not finite dimensional and so is not compact. IS that true?It is closed beceasue passing into subsequence we get a function that is on","T(f)=\chi_{[0,\frac{1}{2}]}f(2x) T:L^1[0,1] \to L^1[0,1] 1/2 \int_{0}^{1/2}f(2x)=1/2\int_{0}^{1}f(x) 1-1 T(f)=0 f(x)=0 x T 0 [1/2,1]","['functional-analysis', 'compact-operators']"
80,Mollifiers and Banach space valued functions,Mollifiers and Banach space valued functions,,"In Section 5.9.2 of Evans PDE, it gives some properties of Sobolev space involving time. Suppose $u\in L^2(0,T;H_0^1(U))$ . Extend $u$ to be $0$ on $(-\infty,0)$ and $(T,\infty)$ and then set $u^\varepsilon=\eta_\varepsilon\ast u$ , $\eta_\varepsilon$ denoting the usual mollifier on $\mathbb{R}^1$ . In the proof of Theorem 3, it says that ""Fix any point $s\in (0,T)$ for which $$ u^\varepsilon(s)\rightarrow u(s) \text{ in }L^2(U)."" $$ My questions are: why can we find such $s$ ? Since $u^\varepsilon\rightarrow u$ in $L^2(0,T;H_0^1(U))$ , it seems that we can only get a subsequence of $u^\varepsilon$ converging to $u$ almost everywhere? Why consider convergence in $L^2(U)$ not in $H_0^1(U)$ ?","In Section 5.9.2 of Evans PDE, it gives some properties of Sobolev space involving time. Suppose . Extend to be on and and then set , denoting the usual mollifier on . In the proof of Theorem 3, it says that ""Fix any point for which My questions are: why can we find such ? Since in , it seems that we can only get a subsequence of converging to almost everywhere? Why consider convergence in not in ?","u\in L^2(0,T;H_0^1(U)) u 0 (-\infty,0) (T,\infty) u^\varepsilon=\eta_\varepsilon\ast u \eta_\varepsilon \mathbb{R}^1 s\in (0,T) 
u^\varepsilon(s)\rightarrow u(s) \text{ in }L^2(U).""
 s u^\varepsilon\rightarrow u L^2(0,T;H_0^1(U)) u^\varepsilon u L^2(U) H_0^1(U)","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
81,Is it possible to construct a Haar measure on a locally compact group assuming the existence of such a measure on compact groups?,Is it possible to construct a Haar measure on a locally compact group assuming the existence of such a measure on compact groups?,,"At first I thought that, once one proves the existence of a Haar measure on compact groups, it should be relatively straightforward to construct it on locally compact groups, by adequately piecing together Haar measures on compact neighbourhoods of points in a locally compact group. However, I then realized that these neighbourhoods need not have a group structure, which means that I can't make direct use of the existence of a Haar measure on compact groups. So, I was wondering if there is a way around this that doesn't require a completely different approach. In other words, is there a way to use the existence of a Haar measure on compact groups to prove the existence of a Haar measure on locally compact groups? If I think of the special case of $\mathbb{R}$ there seems to be no obvious way of doing this. Moreover, since no additive subgroup of $\mathbb{R}$ is compact, it doesn't seem possible to construct a Haar measure in this way.","At first I thought that, once one proves the existence of a Haar measure on compact groups, it should be relatively straightforward to construct it on locally compact groups, by adequately piecing together Haar measures on compact neighbourhoods of points in a locally compact group. However, I then realized that these neighbourhoods need not have a group structure, which means that I can't make direct use of the existence of a Haar measure on compact groups. So, I was wondering if there is a way around this that doesn't require a completely different approach. In other words, is there a way to use the existence of a Haar measure on compact groups to prove the existence of a Haar measure on locally compact groups? If I think of the special case of there seems to be no obvious way of doing this. Moreover, since no additive subgroup of is compact, it doesn't seem possible to construct a Haar measure in this way.",\mathbb{R} \mathbb{R},"['functional-analysis', 'measure-theory']"
82,Turning a definite integral equation into a differential one.,Turning a definite integral equation into a differential one.,,"Consider the following equation where $p(u)$ is a probability distribution and where $g$ is the unknown : \begin{equation}  g = \int_{-\infty}^\infty p(u)f(u,g)  \mathrm{d} u \end{equation} Using the definition of a functional derivative : $$ \frac{\delta F[f(x)]}{\delta f(y)}=\lim _{\epsilon \rightarrow 0} \frac{F[f(x)+\epsilon \delta(x-y)]-F[f(x)]}{\epsilon}$$ Am I allowed to say that the first equation is equivalent to : $$ \frac{\delta g[p(u)]}{\delta p(u_0)} = f(u_0,g)$$ If not, where is my mistake ? I am trying to understand how to invert definite integral equations. Any reference or advice is always welcome. Thank you.","Consider the following equation where is a probability distribution and where is the unknown : Using the definition of a functional derivative : Am I allowed to say that the first equation is equivalent to : If not, where is my mistake ? I am trying to understand how to invert definite integral equations. Any reference or advice is always welcome. Thank you.","p(u) g \begin{equation}
 g = \int_{-\infty}^\infty p(u)f(u,g)  \mathrm{d} u
\end{equation}  \frac{\delta F[f(x)]}{\delta f(y)}=\lim _{\epsilon \rightarrow 0} \frac{F[f(x)+\epsilon \delta(x-y)]-F[f(x)]}{\epsilon}  \frac{\delta g[p(u)]}{\delta p(u_0)} = f(u_0,g)","['functional-analysis', 'integral-equations']"
83,A noncomplete inner product space may have a nonempty closed convex subset which does not have a unique element of minimal norm,A noncomplete inner product space may have a nonempty closed convex subset which does not have a unique element of minimal norm,,"It is well known that if $H$ is a Hilbert space and $E$ is a nonempty closed convex subset of $H$ , then there is a unique element in $E$ of minimal norm, i.e., a unique element $x_0\in E$ such that $\|x_0\|=\min _{x\in E} \|x\|$ . (cf. Rudin's Real and Complex Analysis, Theorem 4.10) Its proof crucially uses completeness of $H$ . I'm wondering if this fails if $H$ is not complete, but equipped with an inner product. A counterexample when $H$ is a Banach sapce, is given in Counterexamples to a theorem in Rudin's book on elements of smallest norm in a closed convex sets in a Hilbert space . But in this counterexample, $C[0,1]$ is not an inner product space. Is there a counterexample for a noncomplete innerproduct space?","It is well known that if is a Hilbert space and is a nonempty closed convex subset of , then there is a unique element in of minimal norm, i.e., a unique element such that . (cf. Rudin's Real and Complex Analysis, Theorem 4.10) Its proof crucially uses completeness of . I'm wondering if this fails if is not complete, but equipped with an inner product. A counterexample when is a Banach sapce, is given in Counterexamples to a theorem in Rudin's book on elements of smallest norm in a closed convex sets in a Hilbert space . But in this counterexample, is not an inner product space. Is there a counterexample for a noncomplete innerproduct space?","H E H E x_0\in E \|x_0\|=\min _{x\in E} \|x\| H H H C[0,1]","['real-analysis', 'linear-algebra', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
84,"Is a continuous function bounded on a set $\mathbb{R}\times [a,b]$?",Is a continuous function bounded on a set ?,"\mathbb{R}\times [a,b]","Let's consider $$f:\mathbb{R}\times [a,b]\to\mathbb{R}$$ a $C^1$ function such that, for every $y_0\in [a,b]$ we have that $f(x,y_0)\in H^1(\mathbb{R})$ , would that mean that $$\sup_{(x,y)\in \mathbb{R}\times [a,b]}|f(x,y)|$$ is bounded? I believe it's true, but I don't know how to prove it, any hint would be appreciated.","Let's consider a function such that, for every we have that , would that mean that is bounded? I believe it's true, but I don't know how to prove it, any hint would be appreciated.","f:\mathbb{R}\times [a,b]\to\mathbb{R} C^1 y_0\in [a,b] f(x,y_0)\in H^1(\mathbb{R}) \sup_{(x,y)\in \mathbb{R}\times [a,b]}|f(x,y)|","['real-analysis', 'calculus', 'functional-analysis', 'sobolev-spaces', 'supremum-and-infimum']"
85,Spectral description of the Kronecker factor,Spectral description of the Kronecker factor,,"$\newcommand{\set}[1]{\{#1\}}$ $\newcommand{\mc}{\mathcal}$ $\newcommand{\Z}{\mathbb Z}$ $\newcommand{\C}{\mathbb C}$ $\newcommand{\R}{\mathbb R}$ Definitions Let $(X, \mc X, \mu)$ be probability space and $T:X\to X$ be an invertible measure preserving transformation. Let us write $L^2$ to mean $L^2(X, \mc X, \mu)$ . Let $U_T$ be the associated Koopman operator on $L^2$ . We may write $Tf$ in place of $U_Tf$ . We say that $\lambda\in \C$ is an eigenvalue of the measure preserving system $(X, T)$ if there is a nonzero function $f\in L^2$ such that $Tf=\lambda f$ . Given an eigenvalue $\lambda$ , we say that $f\in L^2$ is an eigenfunction corresponding to $\lambda$ if $Tf=\lambda f$ . Let $\mc X_1$ denote the $\sigma$ -algebra generated by the set of all the eigenfunctions. Let $H_{pp}$ be the closure of the span of all the eigenfunctions. We say that $f\in L^2$ is almost periodic if the closure of $\set{T^nf:\ n\in \Z}$ is compact in $L^2$ . It is shows in Proposition 2 of this blog post of Tao , assuming ergodicity of $T$ , that $f$ is almost periodic if and only if $f$ is measurable with respect to $\mc X_1$ . In other words, $f$ is almost periodic if and only if $f\in L^2(X, \mc X_1, \mu)$ . Question Exercise 5 in this blog post of Tao asks to show the following. Exercise. Assume $T$ is $\mu$ -ergodic and $f\in L^2$ be given. Then $f\in L^2(X, \mc X_1, \mu)$ if and only if $f$ is in $H_{pp}$ . (I do not think ergodicty is required but right now I am content with the ergodic case.) The hint given is that first one may use the fact that $f\in L^2(X, \mc X_1, \mu)$ if and only if $f$ if almost periodic and also use the fact that the product of two eigenfunctions is also an eigenfunction. I am unable to see how this hint helps solve the question at hand. Independent of the hint, I thought of using the spectral theorem to push the information to $\mathbb T=\R/\Z$ . The almost periodicity of $f$ in $L^2$ gives that the constant function $1$ is almost periodic in $L^2(\mathbb T, \nu)$ , where $\nu$ is the spectral measure corresponding to $f$ . However, I couldn't make any progress using this.","Definitions Let be probability space and be an invertible measure preserving transformation. Let us write to mean . Let be the associated Koopman operator on . We may write in place of . We say that is an eigenvalue of the measure preserving system if there is a nonzero function such that . Given an eigenvalue , we say that is an eigenfunction corresponding to if . Let denote the -algebra generated by the set of all the eigenfunctions. Let be the closure of the span of all the eigenfunctions. We say that is almost periodic if the closure of is compact in . It is shows in Proposition 2 of this blog post of Tao , assuming ergodicity of , that is almost periodic if and only if is measurable with respect to . In other words, is almost periodic if and only if . Question Exercise 5 in this blog post of Tao asks to show the following. Exercise. Assume is -ergodic and be given. Then if and only if is in . (I do not think ergodicty is required but right now I am content with the ergodic case.) The hint given is that first one may use the fact that if and only if if almost periodic and also use the fact that the product of two eigenfunctions is also an eigenfunction. I am unable to see how this hint helps solve the question at hand. Independent of the hint, I thought of using the spectral theorem to push the information to . The almost periodicity of in gives that the constant function is almost periodic in , where is the spectral measure corresponding to . However, I couldn't make any progress using this.","\newcommand{\set}[1]{\{#1\}} \newcommand{\mc}{\mathcal} \newcommand{\Z}{\mathbb Z} \newcommand{\C}{\mathbb C} \newcommand{\R}{\mathbb R} (X, \mc X, \mu) T:X\to X L^2 L^2(X, \mc X, \mu) U_T L^2 Tf U_Tf \lambda\in \C (X, T) f\in L^2 Tf=\lambda f \lambda f\in L^2 \lambda Tf=\lambda f \mc X_1 \sigma H_{pp} f\in L^2 \set{T^nf:\ n\in \Z} L^2 T f f \mc X_1 f f\in L^2(X, \mc X_1, \mu) T \mu f\in L^2 f\in L^2(X, \mc X_1, \mu) f H_{pp} f\in L^2(X, \mc X_1, \mu) f \mathbb T=\R/\Z f L^2 1 L^2(\mathbb T, \nu) \nu f","['functional-analysis', 'measure-theory', 'ergodic-theory']"
86,There exist infinitely many subsequences of $(f_{m})_{m \geq 1}$ which converge at every point of $E$,There exist infinitely many subsequences of  which converge at every point of,(f_{m})_{m \geq 1} E,"Let $E=\{ \frac{1}{n} | n \in \mathbb{N}\}$ . For each $m \in \mathbb{N}$ define $f_{m} : E \to \mathbb{R} $ by $$ f_{m}(x) =  \begin{cases} \cos{(m x)} & \text{if }\,x \geq \frac{1}{m}\\ 0 & \text{if }\,\frac{1}{m+10}<x<\frac{1}{m}\\  x&\text{if } x \le \frac{1}{m+10}\\ \end{cases} $$ Then which of the following statements is true? $(1)$ No subsequence of $(f_{m})_{m \geq 1}$ converges at every point of $E.$ $(2)$ Every subsequence of $(f_{m})_{m \geq 1}$ converges at every point of $E.$ $(3)$ There exist infinitely many subsequences of $(f_{m})_{m \geq 1}$ which converge at every point of $E.$ $(4)$ There exist a subsequence of $(f_{m})_{m \geq 1}$ which converges to $0$ at every point of $E.$ Here is what I tried : Let $\frac{1}{k} \in E$ .Then $\forall n \geq k$ , we have $f_{n}(\frac{1}{k})= \cos{(\frac{n}{k})}$ . I don't understand how to approach further. Any help would be appreciated. Thanks in advance.","Let . For each define by Then which of the following statements is true? No subsequence of converges at every point of Every subsequence of converges at every point of There exist infinitely many subsequences of which converge at every point of There exist a subsequence of which converges to at every point of Here is what I tried : Let .Then , we have . I don't understand how to approach further. Any help would be appreciated. Thanks in advance.","E=\{ \frac{1}{n} | n \in \mathbb{N}\} m \in \mathbb{N} f_{m} : E \to \mathbb{R}  
f_{m}(x) = 
\begin{cases}
\cos{(m x)} & \text{if }\,x \geq \frac{1}{m}\\
0 & \text{if }\,\frac{1}{m+10}<x<\frac{1}{m}\\ 
x&\text{if } x \le \frac{1}{m+10}\\
\end{cases}
 (1) (f_{m})_{m \geq 1} E. (2) (f_{m})_{m \geq 1} E. (3) (f_{m})_{m \geq 1} E. (4) (f_{m})_{m \geq 1} 0 E. \frac{1}{k} \in E \forall n \geq k f_{n}(\frac{1}{k})= \cos{(\frac{n}{k})}","['real-analysis', 'functional-analysis', 'analysis', 'uniform-convergence']"
87,Showing that: $A$ maximal monotone $\Longleftrightarrow A^*$ monotone.,Showing that:  maximal monotone  monotone.,A \Longleftrightarrow A^*,"For the purposes of my thesis, I am interested in proving the following: Let $A: D\left(A\right) \subset H \to H$ be an operator, where $H$ is a Hilbert space with $H^* = H$ . Then, the following hold: \begin{align*} A \; \text{maximal monotone} \; &\Longleftrightarrow A^*  \; \text{maximal monotone} \\ &\Longleftrightarrow A \; \text{is closed, D(A) is dense and} \; A,A^* \; \text{are monotone.} \end{align*} Edit: My question evolved around proving the equivalence: $$A \; \text{maximal monotone} \Longleftrightarrow A \; \text{is closed, D(A) is dense and} \; A,A^* \; \text{are monotone.}$$ Specifically, I was interested in the $(\Leftarrow)$ direction, which seem to trouble me a lot, whereas $(\Rightarrow)$ was pretty straightforward. After a lot of research, I found out that the proof was a rather long and hard result by Brezis and Browder. In the answer section, I provide a sketch of the proof.","For the purposes of my thesis, I am interested in proving the following: Let be an operator, where is a Hilbert space with . Then, the following hold: Edit: My question evolved around proving the equivalence: Specifically, I was interested in the direction, which seem to trouble me a lot, whereas was pretty straightforward. After a lot of research, I found out that the proof was a rather long and hard result by Brezis and Browder. In the answer section, I provide a sketch of the proof.","A: D\left(A\right) \subset H \to H H H^* = H \begin{align*} A \; \text{maximal monotone} \; &\Longleftrightarrow A^*  \; \text{maximal monotone} \\ &\Longleftrightarrow A \; \text{is closed, D(A) is dense and} \; A,A^* \; \text{are monotone.} \end{align*} A \; \text{maximal monotone} \Longleftrightarrow A \; \text{is closed, D(A) is dense and} \; A,A^* \; \text{are monotone.} (\Leftarrow) (\Rightarrow)","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'spectral-theory', 'monotone-functions']"
88,Show a $C^*$-algebra admits an approximate kind of identity.,Show a -algebra admits an approximate kind of identity.,C^*,"I'm reading the book ""An invitation to $C^*$ -algebra's"" by William Arveson and I'm focused on the proof of proposition 1.3.1, which says: Let $A$ be a $C^*$ -algebra and $J$ be a closed two-sided ideal of $A$ .   Then for every $x \in J$ there is a sequence $(e_n)_n$ of   self-adjoint elements of $J$ with $$Sp_A(e_n) \subseteq [0,1], \quad  \lim_n \Vert xe_n - x \Vert = 0$$ The author provides a proof in the case that $A$ has a unit and concludes by saying that the case where $A$ has no unit follows by adjoining a unit, leaving the details to the reader. So, suppose $A$ has no unit. Then consider the unitalisation $A_I:=A \oplus \mathbb{C}$ . Maybe I can show that $J$ is a two-sided ideal in $A_I$ ? So, let $a + \lambda 1 \in A_I$ and $x \in J$ ( $a \in A, \lambda \in \mathbb{C}$ ). Then $$(a+\lambda1)x = ax + \lambda x$$ and if I can show that $\lambda x \in J$ , I can show that $J$ is also a two-sided ideal in $A_I$ . However, I don't see why this should be true.","I'm reading the book ""An invitation to -algebra's"" by William Arveson and I'm focused on the proof of proposition 1.3.1, which says: Let be a -algebra and be a closed two-sided ideal of .   Then for every there is a sequence of   self-adjoint elements of with The author provides a proof in the case that has a unit and concludes by saying that the case where has no unit follows by adjoining a unit, leaving the details to the reader. So, suppose has no unit. Then consider the unitalisation . Maybe I can show that is a two-sided ideal in ? So, let and ( ). Then and if I can show that , I can show that is also a two-sided ideal in . However, I don't see why this should be true.","C^* A C^* J A x \in J (e_n)_n J Sp_A(e_n) \subseteq [0,1], \quad  \lim_n \Vert xe_n - x \Vert = 0 A A A A_I:=A \oplus \mathbb{C} J A_I a + \lambda 1 \in A_I x \in J a \in A, \lambda \in \mathbb{C} (a+\lambda1)x = ax + \lambda x \lambda x \in J J A_I",['functional-analysis']
89,The Functional-Calculus Version of the Spectral Theorem,The Functional-Calculus Version of the Spectral Theorem,,"In the book Analysis Now by Pedersen, the Spectral Theorem is that, for a normal operator $T$ acting on a Hilbert space $H$ , there is an isometric star-isomorphism between $C(\text{sp}(T))$ and the $C^*$ -algebra that is generated by $I$ and $T$ . This star-isomorphism is called the continous functional calculus for $T$ . I am under the impression that this is the first -- or at least an early -- version of the Spectral Theorem (for the infinite-dimensional setting). First, what does this tell us, that is, why would one care about a functional calculus? Second, how does this relate to the more common multiplication-version of the the Spectral Theorem?","In the book Analysis Now by Pedersen, the Spectral Theorem is that, for a normal operator acting on a Hilbert space , there is an isometric star-isomorphism between and the -algebra that is generated by and . This star-isomorphism is called the continous functional calculus for . I am under the impression that this is the first -- or at least an early -- version of the Spectral Theorem (for the infinite-dimensional setting). First, what does this tell us, that is, why would one care about a functional calculus? Second, how does this relate to the more common multiplication-version of the the Spectral Theorem?",T H C(\text{sp}(T)) C^* I T T,"['functional-analysis', 'analysis', 'spectral-theory']"
90,Proof verification of von Neumann's ergodic theorem by spectral theorem,Proof verification of von Neumann's ergodic theorem by spectral theorem,,"I would like to make sure the following proof is correct. Spectral theorem, simplest form I will be using- Let $H$ be a separable Hilbert space and $T:H \to H$ normal, then we may wlog assume $H = L^2 (X,u)$ where $X$ is a second countable locally compact Hausdorff (these properties are easy since $X$ will be a countable union of compact subsets of $C$ ), and $u$ is a complete probability regular measure (the Riesz-Markov gives that it's complete so let's take that for fun), and $T$ acts by $M_f$ . Von Neumann's ergodic theorem- Suppose $T : L^2 (Y,v) \to L^2 (Y,v)$ is unitary, let $U$ denote the closed fixed subspace of $T$ , and $P_U$ the projection. Then for any $h \in L^2(Y,v)$ , $ \frac{1}{n} \sum_{i=1}^n T^i(h) \to P_U(h)$ . Proof - Since unitary is normal, we use the spectral theorem to transfer our world to that of $H=L^2(X,u)$ , $T = M_f$ . Since $T$ is unitary, the spectrum lies in $S^1$ (this is because it is contained in the disk of radius $1$ , and $T^*(T-r) = 1-rT^*$ , and the right is invertible when $|r|<1$ by the geometric series). This lets us assume wlog that $f$ takes only values on $S^1$ (because where it doesn't, is of measure $0$ , since the support of the pushforward of the measure to $\mathbb{C}$ is the spectrum). Now, $U$ is identified as the functions with a representative supported in $f=1$ , and multiplying by the indicator of $f=1$ is thus the projection. Now for any fixed function $g \in L^2 (X,u)$ $\frac{1}{n} \sum_{i=1}^n f^ig \to P_U(g)$ pointwise, by DCT (dominated by max( $10g^2,10)$ ) this gives the desired convergence.","I would like to make sure the following proof is correct. Spectral theorem, simplest form I will be using- Let be a separable Hilbert space and normal, then we may wlog assume where is a second countable locally compact Hausdorff (these properties are easy since will be a countable union of compact subsets of ), and is a complete probability regular measure (the Riesz-Markov gives that it's complete so let's take that for fun), and acts by . Von Neumann's ergodic theorem- Suppose is unitary, let denote the closed fixed subspace of , and the projection. Then for any , . Proof - Since unitary is normal, we use the spectral theorem to transfer our world to that of , . Since is unitary, the spectrum lies in (this is because it is contained in the disk of radius , and , and the right is invertible when by the geometric series). This lets us assume wlog that takes only values on (because where it doesn't, is of measure , since the support of the pushforward of the measure to is the spectrum). Now, is identified as the functions with a representative supported in , and multiplying by the indicator of is thus the projection. Now for any fixed function pointwise, by DCT (dominated by max( ) this gives the desired convergence.","H T:H \to H H = L^2 (X,u) X X C u T M_f T : L^2 (Y,v) \to L^2 (Y,v) U T P_U h \in L^2(Y,v)  \frac{1}{n} \sum_{i=1}^n T^i(h) \to P_U(h) H=L^2(X,u) T = M_f T S^1 1 T^*(T-r) = 1-rT^* |r|<1 f S^1 0 \mathbb{C} U f=1 f=1 g \in L^2 (X,u) \frac{1}{n} \sum_{i=1}^n f^ig \to P_U(g) 10g^2,10)","['functional-analysis', 'solution-verification', 'ergodic-theory']"
91,Lax-Milgram as a corollary of Stampacchia theorem [Brezis book],Lax-Milgram as a corollary of Stampacchia theorem [Brezis book],,"I'm reading Brezis functional analysis,sobolev spaces and pdes book, where at page 140 of the 2010 Springer edition there is the following corollary 5.8 , that is a corollary of Stampacchia theorem (theorem 5.6.) : Assume $a(u,u)$ is a coercive bilinear form on $H$ (where $H$ is a Hilbert space over $\mathbb{R}$ ); then for every $\phi \in H^*$ there exists an element $u$ such that $a(u,v)=\langle \phi,v \rangle,$ for every $v \in H.$ Moreover, if $a$ is symmetric, $u$ is characterized by the property $\mathcal{P}:$ $$u\in H \ \ \text{and} \ \  \frac{1}{2}a(u,u)-\langle \phi,u \rangle = \text{min}_{v \in H}\big\{\frac{1}{2} a(v,v)-\langle \phi,v \rangle \big\}$$ Unfortunately, Brezis gives a very sketchy proof of the corollary, saying that one should just apply the reasoning of a previous corollary (5.4) , which say that if $M$ is a closed linear subspace of $H.$ For $x\in H,$ $y=P_Kx$ is   characterized by the property that for all $m \in M$ $$ y\in M \ \text{and} \ \langle x-y, m \rangle =0$$ question Can you provide me a more detailed proof of this fact? Either a proof given as an answer or a reference to a detailed proof in some other book is good. Plese read The only request is that I would like to follow the approach of Brezis of deriving it from Stampacchia theorem.","I'm reading Brezis functional analysis,sobolev spaces and pdes book, where at page 140 of the 2010 Springer edition there is the following corollary 5.8 , that is a corollary of Stampacchia theorem (theorem 5.6.) : Assume is a coercive bilinear form on (where is a Hilbert space over ); then for every there exists an element such that for every Moreover, if is symmetric, is characterized by the property Unfortunately, Brezis gives a very sketchy proof of the corollary, saying that one should just apply the reasoning of a previous corollary (5.4) , which say that if is a closed linear subspace of For is   characterized by the property that for all question Can you provide me a more detailed proof of this fact? Either a proof given as an answer or a reference to a detailed proof in some other book is good. Plese read The only request is that I would like to follow the approach of Brezis of deriving it from Stampacchia theorem.","a(u,u) H H \mathbb{R} \phi \in H^* u a(u,v)=\langle \phi,v \rangle, v \in H. a u \mathcal{P}: u\in H \ \ \text{and} \ \  \frac{1}{2}a(u,u)-\langle \phi,u \rangle = \text{min}_{v \in H}\big\{\frac{1}{2} a(v,v)-\langle \phi,v \rangle \big\} M H. x\in H, y=P_Kx m \in M  y\in M \ \text{and} \ \langle x-y, m \rangle =0","['real-analysis', 'functional-analysis']"
92,Differentiating term by term in a Banach space: how to justify it?,Differentiating term by term in a Banach space: how to justify it?,,"After looking at this question , I am now wondering if the theorem proven in the first answer below can be generalized to a Banach space. See here for my attempt. But before doing that, I have the following problem: NOTE: I use the notations in the first answer below that question. I don't know how to justify why the series $\sum a_kf_k$ can be differentiated term by term, i.e. why $\partial^\alpha (\sum a_kf_k)=\sum a_k(\partial ^\alpha f_k) $ , and why the convergence of $\sum a_k\partial ^\alpha f_k$ implies the existence of $\partial^\alpha (\sum a_kf_k)$ In Banach space, the multi-index notation does not mean anything, so I should write $\sum a_kD^nf_k$ .","After looking at this question , I am now wondering if the theorem proven in the first answer below can be generalized to a Banach space. See here for my attempt. But before doing that, I have the following problem: NOTE: I use the notations in the first answer below that question. I don't know how to justify why the series can be differentiated term by term, i.e. why , and why the convergence of implies the existence of In Banach space, the multi-index notation does not mean anything, so I should write .",\sum a_kf_k \partial^\alpha (\sum a_kf_k)=\sum a_k(\partial ^\alpha f_k)  \sum a_k\partial ^\alpha f_k \partial^\alpha (\sum a_kf_k) \sum a_kD^nf_k,"['functional-analysis', 'banach-spaces', 'separable-spaces', 'frechet-derivative']"
93,Trouble in understanding why function is constant in a given domain,Trouble in understanding why function is constant in a given domain,,"Definition The group of unitary operators $u(\theta)$ on $L^2(\mathbb R^3)$ given by $(u(\theta)\Phi)(r) =\Phi(\theta) \equiv e^{\frac{3\theta}{2}}\Phi(e^\theta r)$ is called the group of dilation operators on $\mathbb R^3.$ Let $H$ be a compact operator and $R(z) \equiv(H-z)^{-1}$ the resolvent. Define $R(z,\theta)$ by $R(z,\theta) \equiv u(\theta)R(z)u(\theta)^{-1}$ . Let us define also $\mathcal O \equiv \{\theta \in \mathbb C: (u(\theta)\Phi)(r)$ for $\theta \in \mathbb R$ has an analytic continuation $ \}$ In this article A Class of Analytic Perturbations for One-body Schrδdinger Hamiltonians they were able to show that the function $\Psi_z(\theta)=(\phi(\theta) ,R(z,\theta)\phi(\theta))$ is meromorphic in $z$ for $z \in \mathcal C^{++} \equiv \{z \in \mathbb C : Im \ z > 0 ,\ Re \ z > 0 \} $ and $\theta \in \mathcal O^\epsilon \equiv \{\theta \in \mathcal O: Im \ \theta > \epsilon \}$ Now since for $\theta \in \mathbb R,\ u(\theta)$ is unitary we have that $\Psi_z \equiv (\Phi,R(z)\Phi)=\Psi_z(\theta)$ from this they claim that $\Psi_z(\theta)$ that for fixed $z \in \mathcal C^{++}$ and $\theta \in \mathcal O^\epsilon$ the function $\Psi_z(\theta)$ is constant in $\theta$ . My question is why is $\Psi_z(\theta)$ constant for fixed $z \in \mathcal C^{++}$ and $\theta \in \mathcal O^\epsilon$ ?",Definition The group of unitary operators on given by is called the group of dilation operators on Let be a compact operator and the resolvent. Define by . Let us define also for has an analytic continuation In this article A Class of Analytic Perturbations for One-body Schrδdinger Hamiltonians they were able to show that the function is meromorphic in for and Now since for is unitary we have that from this they claim that that for fixed and the function is constant in . My question is why is constant for fixed and ?,"u(\theta) L^2(\mathbb R^3) (u(\theta)\Phi)(r) =\Phi(\theta) \equiv e^{\frac{3\theta}{2}}\Phi(e^\theta r) \mathbb R^3. H R(z) \equiv(H-z)^{-1} R(z,\theta) R(z,\theta) \equiv u(\theta)R(z)u(\theta)^{-1} \mathcal O \equiv \{\theta \in \mathbb C: (u(\theta)\Phi)(r) \theta \in \mathbb R  \} \Psi_z(\theta)=(\phi(\theta)
,R(z,\theta)\phi(\theta)) z z \in \mathcal C^{++} \equiv \{z \in \mathbb C : Im \ z > 0 ,\ Re \ z > 0 \}  \theta \in \mathcal O^\epsilon \equiv \{\theta \in \mathcal O: Im \ \theta > \epsilon \} \theta \in \mathbb R,\ u(\theta) \Psi_z \equiv (\Phi,R(z)\Phi)=\Psi_z(\theta) \Psi_z(\theta) z \in \mathcal C^{++} \theta \in \mathcal O^\epsilon \Psi_z(\theta) \theta \Psi_z(\theta) z \in \mathcal C^{++} \theta \in \mathcal O^\epsilon","['complex-analysis', 'functional-analysis', 'operator-theory', 'mathematical-physics']"
94,"If $A$ is a Borel set and $\overline A\setminus A$ is a null set, can we embed $L^p(A)$ into $L^p(\overline A)$?","If  is a Borel set and  is a null set, can we embed  into ?",A \overline A\setminus A L^p(A) L^p(\overline A),"Let $p\ge1$ and $A\in\mathcal B(\mathbb R)$ and assume that $N:=\overline A\setminus A$ is a Leesgue null set. Can any function $f\in\mathcal L^p(A)$ be extended to a function $g\in\mathcal L^p(\overline A)$ ? Since $N\in\mathcal B(\mathbb R)$ , $$g(x):=\left.\begin{cases}f(x)&\text{, if }x\in A\\0&\text{, otherwise}\end{cases}\right\}\;\;\;\text{for }x\in\overline A$$ should be piecewie Borel measurable, hence Borel measurable. And since $N$ is a Lebesgue null set, $\left\|g\right\|_{L^p(\overline A)}=\left\|f\right\|_{L^p(A)}$ . Am I missing something? Remark : The question came to my mind as I've noticed that a function $f:[a,b]\to\mathbb C$ is called absolutely continuous if there is a $h\in L^1((a,b))$ with $f(x)=f(a)+\int_0^xh(y)\:{\rm d}y$ for all $x\in[a,b]$ and it seems akward to me why one is explicitly assuming that $h$ is defined on the open interval $(a,b)$ and not $h\in L^1([a,b])$ . Is there anything I'm missing? Even uniqueness (in an ""almost everywhere"" sense) shouldn't be a problem, since $\{a,b\}$ is a null set.","Let and and assume that is a Leesgue null set. Can any function be extended to a function ? Since , should be piecewie Borel measurable, hence Borel measurable. And since is a Lebesgue null set, . Am I missing something? Remark : The question came to my mind as I've noticed that a function is called absolutely continuous if there is a with for all and it seems akward to me why one is explicitly assuming that is defined on the open interval and not . Is there anything I'm missing? Even uniqueness (in an ""almost everywhere"" sense) shouldn't be a problem, since is a null set.","p\ge1 A\in\mathcal B(\mathbb R) N:=\overline A\setminus A f\in\mathcal L^p(A) g\in\mathcal L^p(\overline A) N\in\mathcal B(\mathbb R) g(x):=\left.\begin{cases}f(x)&\text{, if }x\in A\\0&\text{, otherwise}\end{cases}\right\}\;\;\;\text{for }x\in\overline A N \left\|g\right\|_{L^p(\overline A)}=\left\|f\right\|_{L^p(A)} f:[a,b]\to\mathbb C h\in L^1((a,b)) f(x)=f(a)+\int_0^xh(y)\:{\rm d}y x\in[a,b] h (a,b) h\in L^1([a,b]) \{a,b\}","['functional-analysis', 'measure-theory', 'lp-spaces']"
95,Gaussian measure on $\mathcal{S}(\mathbb{R})$?,Gaussian measure on ?,\mathcal{S}(\mathbb{R}),"Let $v: \mathbb{R}^{d} \to \mathbb{C}$ be positive-definite and write $$ \langle \delta, v\delta \rangle := \int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\delta(x)v(x-y)\delta(y)dxdy$$ where $\delta$ stands for the Dirac delta distribution. I'm trying to prove that the following equality holds: $$e^{-\frac{1}{2}\langle \delta, v \delta \rangle} = \int_{\mathcal{S}(\mathbb{R}^{d})}e^{i\delta(\phi)}d\mu_{v}(\phi) \hspace{1cm} (1)$$ where $\mu_{v}(\phi)$ is a gaussian measure on $\mathcal{S}(\mathbb{R}^{d})$ which is characterized by $v$ . Note that (1) is a ""functional version"" of the ""Fourier transform of a gaussian is a gaussian"" result in $\mathbb{R}^{n}$ . My question Is there any representation theorem analogous to Bochner's that allows me to prove (1)? The Minlos-Bochner theorem is a functional version of Bochner's theorem on $\mathbb{R}^{n}$ but the measure given by it is defined on $\mathcal{S}'(\mathbb{R}^{d})$ rather than $\mathcal{S}(\mathbb{R}^{d})$ . In addition, is there a simpler way to prove (1) ?","Let be positive-definite and write where stands for the Dirac delta distribution. I'm trying to prove that the following equality holds: where is a gaussian measure on which is characterized by . Note that (1) is a ""functional version"" of the ""Fourier transform of a gaussian is a gaussian"" result in . My question Is there any representation theorem analogous to Bochner's that allows me to prove (1)? The Minlos-Bochner theorem is a functional version of Bochner's theorem on but the measure given by it is defined on rather than . In addition, is there a simpler way to prove (1) ?","v: \mathbb{R}^{d} \to \mathbb{C}  \langle \delta, v\delta \rangle := \int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{d}}\delta(x)v(x-y)\delta(y)dxdy \delta e^{-\frac{1}{2}\langle \delta, v \delta \rangle} = \int_{\mathcal{S}(\mathbb{R}^{d})}e^{i\delta(\phi)}d\mu_{v}(\phi) \hspace{1cm} (1) \mu_{v}(\phi) \mathcal{S}(\mathbb{R}^{d}) v \mathbb{R}^{n} \mathbb{R}^{n} \mathcal{S}'(\mathbb{R}^{d}) \mathcal{S}(\mathbb{R}^{d})","['functional-analysis', 'analysis', 'measure-theory']"
96,Equivalent Formulations of the Open Mapping Theorem,Equivalent Formulations of the Open Mapping Theorem,,"Different textbooks give different, but equivalent, formulations of the open mapping theorem, and I want to understand how all of these rigorously relate (and how they're all equivalent). I intuitively understand why they're equivalent, but want a deeper formal understanding. So, here are the various formulations: Rudin : Let $X,Y$ be Banach spaces, and $T \in \mathcal{L}(X,Y)$ (set of all bounded linear maps from $X$ to $Y$ ) be a surjective map. Then $T$ is an open map. Note that Rudin's formulation doesn't include the converse of this statement, although it can easily be checked. Royden : Let $X,Y$ be Banach Spaces and $T \in \mathcal{L}(X,Y)$ . Then $\text{Im}(T)$ is closed if and only if the operator $T$ is open. Brezis : Let $X,Y$ be Banach Spaces and $T \in \mathcal{L}(X,Y)$ . Then there is $r > 0$ such that $B_{r}^{Y}(0) \subset T(B^{X}_{1}(0)$ ), where $B^{E}_{r}(x)$ denotes the open ball of radius $r$ about $x$ in space $E$ . Intuitively, I can see how these formulations are equivalent. If the image if closed and the operator is open, the image is an open and closed space in $Y$ , which means that it's $Y$ itself (i.e. the map is surjective). Similarly, in the third formulation, if the image of unit ball contains a neighborhood of the origin, then by linearity all open sets will be mapped to open sets. I do, however, struggle with formalizing these ideas, and would appreciate if someone could (fairly rigorously) explain the equivalence of these notions.","Different textbooks give different, but equivalent, formulations of the open mapping theorem, and I want to understand how all of these rigorously relate (and how they're all equivalent). I intuitively understand why they're equivalent, but want a deeper formal understanding. So, here are the various formulations: Rudin : Let be Banach spaces, and (set of all bounded linear maps from to ) be a surjective map. Then is an open map. Note that Rudin's formulation doesn't include the converse of this statement, although it can easily be checked. Royden : Let be Banach Spaces and . Then is closed if and only if the operator is open. Brezis : Let be Banach Spaces and . Then there is such that ), where denotes the open ball of radius about in space . Intuitively, I can see how these formulations are equivalent. If the image if closed and the operator is open, the image is an open and closed space in , which means that it's itself (i.e. the map is surjective). Similarly, in the third formulation, if the image of unit ball contains a neighborhood of the origin, then by linearity all open sets will be mapped to open sets. I do, however, struggle with formalizing these ideas, and would appreciate if someone could (fairly rigorously) explain the equivalence of these notions.","X,Y T \in \mathcal{L}(X,Y) X Y T X,Y T \in \mathcal{L}(X,Y) \text{Im}(T) T X,Y T \in \mathcal{L}(X,Y) r > 0 B_{r}^{Y}(0) \subset T(B^{X}_{1}(0) B^{E}_{r}(x) r x E Y Y","['functional-analysis', 'banach-spaces']"
97,"If $f_n \to f$ a.e. , and $f_n$ bounded sequence ,then $f_n \to f$ weakly in $L^p(\mathbb{R})$ .","If  a.e. , and  bounded sequence ,then  weakly in  .",f_n \to f f_n f_n \to f L^p(\mathbb{R}),"Let $1<p<\infty$ and $\{f_n\}\in L^p(\mathbb{R})$ be a uniformly bounded sequence, i.e. $\|f_n\|_p\le M, \forall n$ , for some $M>0$ . If $f_n \to f$ a.e. , prove that $f_n \to f$ weakly in $L^p(\mathbb{R})$ . My attempt : Let $A$ be any measurable subset $A \subset\mathbb{R}$ , by lemma 2 we have $\lim \int_A f_n = \int_A f$ and using lemma 1 we have $f_n \to f$ weakly in $L^p(\mathbb{R})$ . lemma 1: if $1\le p<\infty$ and $\{f_n\}\in L^p(\mathbb{R})$ be a bounded sequence, then $f_n \to f$ weakly in $L^p(\mathbb{R})$ iff for every measurable subset $A \subset\mathbb{R}$ , $$\lim \int_A f_n = \int_A f$$ proof: WLOG that $m(A)<\infty$ , let $g_0 \in L^q(\mathbb{R})$ , $1<q\le\infty$ be any function and denote $g= \chi_{A}$ ; \begin{align} \int_{\mathbb{R}} g_0f_n - \int_{\mathbb{R}} g_0f  & = \int_{\mathbb{R}} (g_0 -g)(f_n-f) + \int_{\mathbb{R}} g(f_n-f) \\ & \le \|g_0-g\|_q\|f_n-f\|_p + \frac{\epsilon}{2} \\ & \le \frac{\epsilon}{2}+\frac{\epsilon}{2} =\epsilon \implies  f_n \rightharpoonup  f  \end{align} where I used boundedness in the last inequality above. lemma 2: if $A$ is measurable and $\{f_n\}\in L^p(\mathbb{R})$ be a bounded sequence, then $\{f_n\}$ is uniformly integrable over $A$ .  (proof in royden ch.7)","Let and be a uniformly bounded sequence, i.e. , for some . If a.e. , prove that weakly in . My attempt : Let be any measurable subset , by lemma 2 we have and using lemma 1 we have weakly in . lemma 1: if and be a bounded sequence, then weakly in iff for every measurable subset , proof: WLOG that , let , be any function and denote ; where I used boundedness in the last inequality above. lemma 2: if is measurable and be a bounded sequence, then is uniformly integrable over .  (proof in royden ch.7)","1<p<\infty \{f_n\}\in L^p(\mathbb{R}) \|f_n\|_p\le M, \forall n M>0 f_n \to f f_n \to f L^p(\mathbb{R}) A A \subset\mathbb{R} \lim \int_A f_n = \int_A f f_n \to f L^p(\mathbb{R}) 1\le p<\infty \{f_n\}\in L^p(\mathbb{R}) f_n \to f L^p(\mathbb{R}) A \subset\mathbb{R} \lim \int_A f_n = \int_A f m(A)<\infty g_0 \in L^q(\mathbb{R}) 1<q\le\infty g= \chi_{A} \begin{align}
\int_{\mathbb{R}} g_0f_n - \int_{\mathbb{R}} g_0f 
& = \int_{\mathbb{R}} (g_0 -g)(f_n-f) + \int_{\mathbb{R}} g(f_n-f) \\
& \le \|g_0-g\|_q\|f_n-f\|_p + \frac{\epsilon}{2} \\
& \le \frac{\epsilon}{2}+\frac{\epsilon}{2} =\epsilon \implies  f_n \rightharpoonup  f 
\end{align} A \{f_n\}\in L^p(\mathbb{R}) \{f_n\} A","['real-analysis', 'functional-analysis', 'proof-verification', 'lp-spaces', 'weak-convergence']"
98,"Closure of a subspace of $\textsf{C}(J)$, where $J = [t_0-\beta,t_0+\beta]$","Closure of a subspace of , where","\textsf{C}(J) J = [t_0-\beta,t_0+\beta]","Question: Let $\textsf{C}(J)$ be the metric space of all real-valued continuous functions on the interval $J = [t_0-\beta,t_0+\beta]$ with the metric $d$ defined by $$d(x,y)=\max_{t\in J}|x(t)-y(t)|.$$ Let $K$ be a subspace of $\textsf{C}(J)$ consisting of all those functions $x\in\textsf{C}(J)$ that satisfy $$|x(t)-x_0|\leq c\beta$$ for a fixed $c$ . Show that $K$ is closed in $\textsf{C}(J)$ . Here is my approach. Let $x\in\overline{K}$ . Then there exists a sequence $(x_n)\subset K$ such that $x_n$ converges to $x$ . Thus we have for any $\varepsilon>0$ , there exists $N\in\mathbb{N}$ such that whenever $n>N$ , we have $$|x(t)-x_n(t)|<\varepsilon$$ To show $K$ is closed, it suffices to show that $x\in K$ . Thus, $$\begin{align} |x(t)-x_0| &= |x(t)-x_n(t)+x_n(t)-x_0| \\ &\leq |x(t)-x_n(t)|+|x_n(t)-x_0| \\ &< \varepsilon+c\beta \end{align}$$ for which $\varepsilon\rightarrow 0$ as $n\rightarrow \infty.$ Thus, $|x(t)-x_0|<c\beta$ , which implies that $x\in K$ . Am I correct with this proof?","Question: Let be the metric space of all real-valued continuous functions on the interval with the metric defined by Let be a subspace of consisting of all those functions that satisfy for a fixed . Show that is closed in . Here is my approach. Let . Then there exists a sequence such that converges to . Thus we have for any , there exists such that whenever , we have To show is closed, it suffices to show that . Thus, for which as Thus, , which implies that . Am I correct with this proof?","\textsf{C}(J) J = [t_0-\beta,t_0+\beta] d d(x,y)=\max_{t\in J}|x(t)-y(t)|. K \textsf{C}(J) x\in\textsf{C}(J) |x(t)-x_0|\leq c\beta c K \textsf{C}(J) x\in\overline{K} (x_n)\subset K x_n x \varepsilon>0 N\in\mathbb{N} n>N |x(t)-x_n(t)|<\varepsilon K x\in K \begin{align}
|x(t)-x_0| &= |x(t)-x_n(t)+x_n(t)-x_0| \\
&\leq |x(t)-x_n(t)|+|x_n(t)-x_0| \\
&< \varepsilon+c\beta
\end{align} \varepsilon\rightarrow 0 n\rightarrow \infty. |x(t)-x_0|<c\beta x\in K","['real-analysis', 'functional-analysis', 'proof-verification', 'metric-spaces']"
99,Convergence in measure metrizable?,Convergence in measure metrizable?,,"I am trying to show for a $\sigma$ -finite measure space $f_n\rightarrow f$ $\mu$ -stochastically iff $\lim_{n\rightarrow\infty} d(f_n,f)=0$ where $$d(f,g):=\sum^\infty_{k=1}\frac{2^{-k}}{\mu(\Omega_k)}\int I_{\Omega_k}\frac{|f_n-f|}{1+|f_n-f|}d\mu$$ but I'm having trouble proving either direction. For the forward direction I say from the definition (w.l.o.g. replacing $f_n$ by $f_n-f$ ): $$\forall A\in\mathcal{A},\epsilon>0:\mu(A\cap\{f_n>\epsilon\})\rightarrow 0 (n\rightarrow\infty),$$ it follows that $$\forall A\in\mathcal{A},\epsilon>0:\mu(A\cap\{\frac{f_n}{1+f_n}>\frac{\epsilon}{1+\epsilon}\})\rightarrow 0 (n\rightarrow\infty).$$ Then by choosing a constant some $C=1/(\epsilon+1)>0$ argue as follows $$d(f_n,0):= \sum^\infty_{k=1}\frac{2^{-k}}{\mu(\Omega_k)}\int I_{\Omega_k}\frac{|f_n|}{1+|f_n|}d\mu$$ $$\leq\sum c_k \int I_{\Omega\cap\{|f_n|/(1+|f_n|)>C\}}\leq \sum c_k \mu(A\cap\{\frac{|f_n|}{1+|f_n|}>C\})\rightarrow 0(n\rightarrow\infty).$$ Is this the right approach? Is there such a constant? For the other direction I argue for a non-empty measurable set $A$ with finite measure and $\epsilon,\delta>0$ we can choose $A$ to be one of the partitioning sets and it follows that for some $N\in\mathbb{N}$ and all $n\geq N$ : $$\delta>d(f_n,0)\geq \int_{A}\frac{|f_n|}{2\mu(A)(1+|f_n|)}\geq \mu(A\cap\{\frac{|f_n|}{(1+|f_n|)}>\epsilon\})/2\mu(A).$$ And now the direction follows with $\delta\rightarrow 0$ . Is this legal? Any help would be great! Sorry for the sloppy tex.",I am trying to show for a -finite measure space -stochastically iff where but I'm having trouble proving either direction. For the forward direction I say from the definition (w.l.o.g. replacing by ): it follows that Then by choosing a constant some argue as follows Is this the right approach? Is there such a constant? For the other direction I argue for a non-empty measurable set with finite measure and we can choose to be one of the partitioning sets and it follows that for some and all : And now the direction follows with . Is this legal? Any help would be great! Sorry for the sloppy tex.,"\sigma f_n\rightarrow f \mu \lim_{n\rightarrow\infty} d(f_n,f)=0 d(f,g):=\sum^\infty_{k=1}\frac{2^{-k}}{\mu(\Omega_k)}\int I_{\Omega_k}\frac{|f_n-f|}{1+|f_n-f|}d\mu f_n f_n-f \forall A\in\mathcal{A},\epsilon>0:\mu(A\cap\{f_n>\epsilon\})\rightarrow 0 (n\rightarrow\infty), \forall A\in\mathcal{A},\epsilon>0:\mu(A\cap\{\frac{f_n}{1+f_n}>\frac{\epsilon}{1+\epsilon}\})\rightarrow 0 (n\rightarrow\infty). C=1/(\epsilon+1)>0 d(f_n,0):= \sum^\infty_{k=1}\frac{2^{-k}}{\mu(\Omega_k)}\int I_{\Omega_k}\frac{|f_n|}{1+|f_n|}d\mu \leq\sum c_k \int I_{\Omega\cap\{|f_n|/(1+|f_n|)>C\}}\leq \sum c_k \mu(A\cap\{\frac{|f_n|}{1+|f_n|}>C\})\rightarrow 0(n\rightarrow\infty). A \epsilon,\delta>0 A N\in\mathbb{N} n\geq N \delta>d(f_n,0)\geq \int_{A}\frac{|f_n|}{2\mu(A)(1+|f_n|)}\geq \mu(A\cap\{\frac{|f_n|}{(1+|f_n|)}>\epsilon\})/2\mu(A). \delta\rightarrow 0","['integration', 'functional-analysis', 'measure-theory', 'convergence-divergence', 'metric-spaces']"
