,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding the coefficient of an ODE,Finding the coefficient of an ODE,,"The real constants $ùëé_3,ùëé_2,ùëé_1,ùëé_0$ are such that the ODE $$ùë¶^{(4)}+ùëé_3ùë¶^{(3)}+ùëé_2ùë¶''+ùëé_1ùë¶'+ùëé_0ùë¶=0$$ has $3ùë°ùëí^{‚àíùë°}+ùëí^{‚àí4ùë°}\sin(ùë°)$ as a solution. What is $ùëé_0$ ? I would really like some help in solving this problem. The hint was to figure out the characteristic polynomial first, which is $$r^4 + a_3r^3 + a_2r^2 + a_1r + a_0 = 0.$$ While I could plug in the given solution in, that'd be too complicated. I've tried to see if I could simplify the expression $3ùë°ùëí^{‚àíùë°}+ùëí^{‚àí4ùë°}\sin(ùë°)$ further into one term, but so far haven't really found a way to do so. Any help would really be appreciated!","The real constants are such that the ODE has as a solution. What is ? I would really like some help in solving this problem. The hint was to figure out the characteristic polynomial first, which is While I could plug in the given solution in, that'd be too complicated. I've tried to see if I could simplify the expression further into one term, but so far haven't really found a way to do so. Any help would really be appreciated!","ùëé_3,ùëé_2,ùëé_1,ùëé_0 ùë¶^{(4)}+ùëé_3ùë¶^{(3)}+ùëé_2ùë¶''+ùëé_1ùë¶'+ùëé_0ùë¶=0 3ùë°ùëí^{‚àíùë°}+ùëí^{‚àí4ùë°}\sin(ùë°) ùëé_0 r^4 + a_3r^3 + a_2r^2 + a_1r + a_0 = 0. 3ùë°ùëí^{‚àíùë°}+ùëí^{‚àí4ùë°}\sin(ùë°)","['linear-algebra', 'ordinary-differential-equations', 'roots', 'characteristic-functions']"
1,Dynamical systems proof that $ f(t)$ is less than or equal to $g(t)$,Dynamical systems proof that  is less than or equal to, f(t) g(t),"Suppose that $f'(t) \le F(f(t),t)$ where $F$ is continuously differentiable. If $g(t)$ is a solution to the equation $g'(t) = F(g(t),t)$ and $g(a) = f(a)$ , then prove that $f(t) \le g(t)$ for all $t \ge a$","Suppose that where is continuously differentiable. If is a solution to the equation and , then prove that for all","f'(t) \le F(f(t),t) F g(t) g'(t) = F(g(t),t) g(a) = f(a) f(t) \le g(t) t \ge a","['ordinary-differential-equations', 'dynamical-systems', 'gronwall-type-inequality']"
2,"Understanding what is meant by ""Integrability"" with regards to the Euler Top","Understanding what is meant by ""Integrability"" with regards to the Euler Top",,"I am reading Discrete Systems and Integrability by F.W. Nijhoff, J. Hietarinta, and N. Joshi. Currently, I am investigating Chapter 6 and in particular the Euler Top. The book says the following: There are several properties associated with integrability of maps,   for example, the existence of a sifficient number of conserved   quantities, symmetries, Lax pair and the behavior around   singularities. The book then defines a type of Integrability A system with $2N$ -dimension phase space is called Louiville   Integrable if there are $N$ conserved quantities The Euler Top is  given by $$ \begin{cases} \overset{\cdot}{x}_1 = \alpha_1 x_2 x_3, \\ \overset{\cdot}{x}_2 = \alpha_2 x_{3}x_{1}, \\ \overset{\cdot}{x}_3 = \alpha_3 x_{1}x_{2}. \end{cases} $$ where $\alpha_{1,2,3}$ are real parameters. This is one of the most famous integrable systems of classicaly mechanics. The function $H(x) = \gamma_1 x_1^2 + \gamma_2 x_2^2 + \gamma_3 x_3^2$ is an integral (conserved quantity?) for the above system $\iff \gamma \perp \alpha$ . In particular, there are $3$ integrals of motion (conserved quantities) of the system where only two of them are independent: $$H_1 = \alpha_2 x_3^2 - \alpha_3 x_2^2, \; H_2 = \alpha_3 x_1^2 - \alpha_1 x_3^2 , \; H_3 = \alpha_1 x_2^2 - \alpha_2 x_1^2$$ So, here is what I know. The dimension of the Euler Top is $3$ because there are three independent variables $x_1, x_2, x_3$ . We have found $2$ independent conserved quantities. Since the dimension of the system is $3$ , then this doesn't fit the definition of the Louiville Integrability. So, by what definition is the Euler Top called Integrable?","I am reading Discrete Systems and Integrability by F.W. Nijhoff, J. Hietarinta, and N. Joshi. Currently, I am investigating Chapter 6 and in particular the Euler Top. The book says the following: There are several properties associated with integrability of maps,   for example, the existence of a sifficient number of conserved   quantities, symmetries, Lax pair and the behavior around   singularities. The book then defines a type of Integrability A system with -dimension phase space is called Louiville   Integrable if there are conserved quantities The Euler Top is  given by where are real parameters. This is one of the most famous integrable systems of classicaly mechanics. The function is an integral (conserved quantity?) for the above system . In particular, there are integrals of motion (conserved quantities) of the system where only two of them are independent: So, here is what I know. The dimension of the Euler Top is because there are three independent variables . We have found independent conserved quantities. Since the dimension of the system is , then this doesn't fit the definition of the Louiville Integrability. So, by what definition is the Euler Top called Integrable?","2N N 
\begin{cases}
\overset{\cdot}{x}_1 = \alpha_1 x_2 x_3, \\
\overset{\cdot}{x}_2 = \alpha_2 x_{3}x_{1}, \\
\overset{\cdot}{x}_3 = \alpha_3 x_{1}x_{2}.
\end{cases}
 \alpha_{1,2,3} H(x) = \gamma_1 x_1^2 + \gamma_2 x_2^2 + \gamma_3 x_3^2 \iff \gamma \perp \alpha 3 H_1 = \alpha_2 x_3^2 - \alpha_3 x_2^2, \; H_2 = \alpha_3 x_1^2 - \alpha_1 x_3^2 , \; H_3 = \alpha_1 x_2^2 - \alpha_2 x_1^2 3 x_1, x_2, x_3 2 3","['ordinary-differential-equations', 'dynamical-systems', 'integrable-systems']"
3,How to solve the system of differential equations?,How to solve the system of differential equations?,,"Let $x_i=x_i(t,s),i=0,1,2$ be  functions of $t,s$ and consider the system of differential equations \begin{cases} \frac{\partial x_0}{\partial t}=0,\\ \frac{\partial x_1}{\partial t}=x_0,\\ \frac{\partial x_2}{\partial t}=2x_1,\\ \frac{\partial x_0}{\partial s}=2x_1,\\ \frac{\partial x_1}{\partial s}=x_2,\\ \frac{\partial x_2}{\partial s}=0, \end{cases} How to solve it? My attemt. First of all  I have solved the first 3 equations and get \begin{align}  &x_{{0}}=C_1 \left( s \right) ,\\ &x_{{1}}  =C_1 \left( s \right) t+C_2(s),\\ &x_{{2}}  =C_1 \left( s  \right) {t}^{2}+2\,C_2 \left( s \right) t+C_3 \left( s  \right),   \end{align} and substitute into the last 3 equation $$ \begin{cases} \displaystyle {\frac { d}{{ d}s}}C_1 \left( s  \right) =2\,C_1 \left( s \right) t+2\,C_2 \left( s  \right),\\ \\  \displaystyle t  {\frac { d}{{ d}s}}C_1 \left( s \right)+{\frac { d}{{ d}s}}C_2 \left( s \right) =C_1 \left( s \right) {t}^{2}+2\,C_2 \left( s \right) t+C_3 \left( s \right) ,\\ \\  \displaystyle {t}^{2}\, {\frac { d}{{ d}s}}C_1  \left( s \right)  +2\,t  {\frac { d}{{ d}s} }C_2 \left( s \right) +{\frac { d}{{ d}s}}C_3 \left( s \right) =0, \end{cases} $$ or normalize it: $$ \begin{cases} \displaystyle{\frac { d}{{ d}s}}C_1 \left( s  \right)=2\,C_1 \left( s \right) t+2\,C_2 \left( s \right) ,\\ \\ \displaystyle{\frac { d}{{ d}s}}C_2 \left( s  \right)=-C_1 \left( s \right) {t}^{2}+C_2 \left( s \right) ,\\ \\ \displaystyle{\frac { d}{{ d}s}}C_3 \left( s  \right)=-2\,C_2 \left( s \right) {t}^{ 2}-2\,C_2 \left( s \right) t  \end{cases} $$ Then I solve the system for $C_1(s),C_2(s),C_3(s)$ and get \begin{align*} &C_1 \left( s \right) =-{\frac {2\,C_1'\,{s}^{2}{t}^{2}+4\,C_2'\,s{t}^{2}+2\,C_1'\,st+4\,C_3'\,{ t}^{2}+2\,C_2'\,t+C_1'}{4{t}^{3}}},\\ &C_2 \left( s  \right) =\frac{1}{2}\,C_1'\,{s}^{2}+C_2'\,s+C_3', \\ &C_3  \left( s \right) =-\,{\frac {2\,C_1'\,{s}^{2}{t}^{2}+4\,C_2\,s{t}^{2}-2\,C_1'\,st+4\,C_3'\,{t}^{2}-2\,C_2'\,t+C_1'}{4t}}. \end{align*} Here $C_1',C_2',C_3'$ are constants. Then I substitute it  into the above expression for $x_0,x_1,x_2$ and get \begin{align*} &x_{{0}} \left( t,s \right) ={\frac { \left( -2\,C_1'\,{s}^{2}-4\,C_2'\,s-4\,C_3' \right) {t}^{2}+ \left(  -2\,C_1'\,s-2\,C_2' \right) t-C_1'}{4{t}^{3}}}, \\&x_{{1} } \left( t,s \right) ={\frac { \left( -2\,C_1'\,s-2\,{C_2'} \right) t-C_1'}{4{t}^{2}}},\\ &x_{{2}} \left( t,s \right) =- \,{\frac {C_1'}{2t}} \end{align*} But obviously it is not solutions of the initial system! Where is my mistake? Note, the system have solution, for example $x_1^2-x_0 x_2$ is its first integral.","Let be  functions of and consider the system of differential equations How to solve it? My attemt. First of all  I have solved the first 3 equations and get and substitute into the last 3 equation or normalize it: Then I solve the system for and get Here are constants. Then I substitute it  into the above expression for and get But obviously it is not solutions of the initial system! Where is my mistake? Note, the system have solution, for example is its first integral.","x_i=x_i(t,s),i=0,1,2 t,s \begin{cases}
\frac{\partial x_0}{\partial t}=0,\\
\frac{\partial x_1}{\partial t}=x_0,\\
\frac{\partial x_2}{\partial t}=2x_1,\\
\frac{\partial x_0}{\partial s}=2x_1,\\
\frac{\partial x_1}{\partial s}=x_2,\\
\frac{\partial x_2}{\partial s}=0,
\end{cases} \begin{align}
 &x_{{0}}=C_1 \left( s \right) ,\\
&x_{{1}}  =C_1 \left( s \right) t+C_2(s),\\
&x_{{2}}  =C_1 \left( s
 \right) {t}^{2}+2\,C_2 \left( s \right) t+C_3 \left( s
 \right),  
\end{align} 
\begin{cases}
\displaystyle {\frac { d}{{ d}s}}C_1 \left( s
 \right) =2\,C_1 \left( s \right) t+2\,C_2 \left( s
 \right),\\
\\
 \displaystyle t  {\frac { d}{{ d}s}}C_1 \left( s \right)+{\frac { d}{{ d}s}}C_2 \left( s \right) =C_1 \left( s \right) {t}^{2}+2\,C_2 \left( s \right) t+C_3 \left( s \right) ,\\ \\
 \displaystyle {t}^{2}\, {\frac { d}{{ d}s}}C_1
 \left( s \right)  +2\,t  {\frac { d}{{ d}s}
}C_2 \left( s \right) +{\frac { d}{{ d}s}}C_3 \left( s \right) =0,
\end{cases}
 
\begin{cases}
\displaystyle{\frac { d}{{ d}s}}C_1 \left( s
 \right)=2\,C_1 \left( s \right) t+2\,C_2 \left( s \right) ,\\ \\
\displaystyle{\frac { d}{{ d}s}}C_2 \left( s
 \right)=-C_1 \left( s \right) {t}^{2}+C_2 \left( s \right) ,\\ \\
\displaystyle{\frac { d}{{ d}s}}C_3 \left( s
 \right)=-2\,C_2 \left( s \right) {t}^{
2}-2\,C_2 \left( s \right) t 
\end{cases}
 C_1(s),C_2(s),C_3(s) \begin{align*}
&C_1 \left( s \right) =-{\frac {2\,C_1'\,{s}^{2}{t}^{2}+4\,C_2'\,s{t}^{2}+2\,C_1'\,st+4\,C_3'\,{
t}^{2}+2\,C_2'\,t+C_1'}{4{t}^{3}}},\\
&C_2 \left( s
 \right) =\frac{1}{2}\,C_1'\,{s}^{2}+C_2'\,s+C_3', \\
&C_3
 \left( s \right) =-\,{\frac {2\,C_1'\,{s}^{2}{t}^{2}+4\,C_2\,s{t}^{2}-2\,C_1'\,st+4\,C_3'\,{t}^{2}-2\,C_2'\,t+C_1'}{4t}}.
\end{align*} C_1',C_2',C_3' x_0,x_1,x_2 \begin{align*}
&x_{{0}} \left( t,s \right) ={\frac { \left( -2\,C_1'\,{s}^{2}-4\,C_2'\,s-4\,C_3' \right) {t}^{2}+ \left( 
-2\,C_1'\,s-2\,C_2' \right) t-C_1'}{4{t}^{3}}}, \\&x_{{1}
} \left( t,s \right) ={\frac { \left( -2\,C_1'\,s-2\,{C_2'} \right) t-C_1'}{4{t}^{2}}},\\ &x_{{2}} \left( t,s \right) =-
\,{\frac {C_1'}{2t}}
\end{align*} x_1^2-x_0 x_2","['ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations']"
4,When does $\int \frac{dx}{x} = \ln|x|$ and when $\int \frac{dx}{x} = \ln(x)$?,When does  and when ?,\int \frac{dx}{x} = \ln|x| \int \frac{dx}{x} = \ln(x),"Sorry for the provocative question but I am often see a solution where the absolute value is neglect for example: $$ \begin{cases} xuu_x+yuu_y=u^2-1,  x>0\\ u(x,x^2)=x^3\\ \end{cases} $$ We look at: $$\frac{dx}{xu}=\frac{dy}{yu}=\frac{du}{u^2-1}$$ We solve: $$ \begin{cases} \frac{dx}{xu}=\frac{dy}{yu}\\ \frac{dx}{xu}=\frac{du}{u^2-1}\\ \end{cases}$$ for $$\frac{dx}{xu}=\frac{dy}{yu}$$ we can multiple by $u\neq 0$ : $$\frac{dx}{x}=\frac{dy}{y}$$ which is: $$\ln(x)=\ln|y|+c^*_1\rightarrow \ln|y|-\ln(x)=c^*_1\rightarrow \ln(\frac{|y|}{x})=c^*_1\rightarrow \frac{|y|}{x}=c_1$$ but on the recitation answer the solved: $$\frac{dx}{x}=\frac{dy}{y}\rightarrow \ln(x)=\ln(y)+c^*_1$$ Why can this be done? What am I missing?",Sorry for the provocative question but I am often see a solution where the absolute value is neglect for example: We look at: We solve: for we can multiple by : which is: but on the recitation answer the solved: Why can this be done? What am I missing?,"
\begin{cases}
xuu_x+yuu_y=u^2-1,  x>0\\
u(x,x^2)=x^3\\
\end{cases}
 \frac{dx}{xu}=\frac{dy}{yu}=\frac{du}{u^2-1} 
\begin{cases}
\frac{dx}{xu}=\frac{dy}{yu}\\
\frac{dx}{xu}=\frac{du}{u^2-1}\\
\end{cases} \frac{dx}{xu}=\frac{dy}{yu} u\neq 0 \frac{dx}{x}=\frac{dy}{y} \ln(x)=\ln|y|+c^*_1\rightarrow \ln|y|-\ln(x)=c^*_1\rightarrow \ln(\frac{|y|}{x})=c^*_1\rightarrow \frac{|y|}{x}=c_1 \frac{dx}{x}=\frac{dy}{y}\rightarrow \ln(x)=\ln(y)+c^*_1","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
5,Looking for a book on a 1st differential equations course [duplicate],Looking for a book on a 1st differential equations course [duplicate],,"This question already has answers here : Could you recommend some classic textbooks on ordinary/partial differential equation? (6 answers) Closed 4 years ago . I'm currently taking differential equations but the guide book (Elementary differential equations and boundary value problems) is aimed for engineers (much like the calculus one from Stewart). I'm looking for a book with the classic mathematical format (definition, theorem, corollary...) and not too much talk or applications. Thanks","This question already has answers here : Could you recommend some classic textbooks on ordinary/partial differential equation? (6 answers) Closed 4 years ago . I'm currently taking differential equations but the guide book (Elementary differential equations and boundary value problems) is aimed for engineers (much like the calculus one from Stewart). I'm looking for a book with the classic mathematical format (definition, theorem, corollary...) and not too much talk or applications. Thanks",,"['ordinary-differential-equations', 'reference-request', 'book-recommendation']"
6,Why do solutions to first order autonomous ODEs tend towards equllibria?,Why do solutions to first order autonomous ODEs tend towards equllibria?,,"Consider a first order autonomous ODE: $$\frac{dy}{dt}=f(y)$$ The solutions to this ODE cannot cross each other due to the Picard-Lindel√∂f theorem (lets say this ODE satisfies it). Also note that, because the sign of $f(y)$ can only change at the zeros and the function can't cross the zeros since they are (equilibrium) solutions, $f(y)$ is monotone. These two reasons put together mean that any solution to a 1st order autonomous ODE are bounded by the equilibrium solutions that surround them: p and q are equilibrium solutions, i.e. f(p)=f(q)=0. This is what my introductory textbook on differential equations and all the online resources I could find say about the subject. What I don't get is how they go further and say that the limit of these bound solutions approach the equilibria as $t\to\infty$ or $-\infty$ depending on if $f(y)$ is decreasing or increasing or, if there is no bounding equllibrium solution, $f(y)$ approaches $\pm\infty$ . I mean I believe them, and it certainly seems intuitive, yet I don't see how monotonicity and not being able to cross over guarantee this alone. Couldn't you have a solution with a horizontal asymptote slightly lower/higher than the next highest/lowest equilibria? Such a solution would still be monotone, and still be bounded by the equilibrium solutions, yet doesn't have to get close to them: So why must these solutions approach equilibrium points/explode to infinity if there are no available equilibrium points? Edit: looking at the slope field of my proposed solution, I see that the answer to my question probably has to do with the time invarience of these solutions as the slope field can't change over time to make my proposed solution possible without the asymptote also being an equilibrium solution. But how do I make this precise instead of a graphical intuition?","Consider a first order autonomous ODE: The solutions to this ODE cannot cross each other due to the Picard-Lindel√∂f theorem (lets say this ODE satisfies it). Also note that, because the sign of can only change at the zeros and the function can't cross the zeros since they are (equilibrium) solutions, is monotone. These two reasons put together mean that any solution to a 1st order autonomous ODE are bounded by the equilibrium solutions that surround them: p and q are equilibrium solutions, i.e. f(p)=f(q)=0. This is what my introductory textbook on differential equations and all the online resources I could find say about the subject. What I don't get is how they go further and say that the limit of these bound solutions approach the equilibria as or depending on if is decreasing or increasing or, if there is no bounding equllibrium solution, approaches . I mean I believe them, and it certainly seems intuitive, yet I don't see how monotonicity and not being able to cross over guarantee this alone. Couldn't you have a solution with a horizontal asymptote slightly lower/higher than the next highest/lowest equilibria? Such a solution would still be monotone, and still be bounded by the equilibrium solutions, yet doesn't have to get close to them: So why must these solutions approach equilibrium points/explode to infinity if there are no available equilibrium points? Edit: looking at the slope field of my proposed solution, I see that the answer to my question probably has to do with the time invarience of these solutions as the slope field can't change over time to make my proposed solution possible without the asymptote also being an equilibrium solution. But how do I make this precise instead of a graphical intuition?",\frac{dy}{dt}=f(y) f(y) f(y) t\to\infty -\infty f(y) f(y) \pm\infty,['ordinary-differential-equations']
7,Differential equation $y' +y^2 = \frac{15}{4x^2}$,Differential equation,y' +y^2 = \frac{15}{4x^2},Could you please assist me with the method of this question? I haven't been able to find similar examples. The information in the question is: Knowing that $u(x) = \frac52\cdot x^{-1}$ is the particular solution of $y' +y^2 = \frac{15}{4x^2}$ find the solution of $y=y(x)$ that satisfies $y(1) = 0$ . The answer is $$\frac{15\cdot x^4-15}{6\cdot x^5+10\cdot x}$$,Could you please assist me with the method of this question? I haven't been able to find similar examples. The information in the question is: Knowing that is the particular solution of find the solution of that satisfies . The answer is,u(x) = \frac52\cdot x^{-1} y' +y^2 = \frac{15}{4x^2} y=y(x) y(1) = 0 \frac{15\cdot x^4-15}{6\cdot x^5+10\cdot x},['ordinary-differential-equations']
8,"Why Wronskian works for DEs, but in general it doesn't show independence","Why Wronskian works for DEs, but in general it doesn't show independence",,"For a set of functions $f_1,f_2,...f_n$ , if their Wronskian determinant is identically zero $W(f_1,...f_n)(x) = 0$ for all $x$ in some interval $I$ we can't conlcude that these functions are linearly dependent. But in case of differential equations, if $f_1,f_2,...f_n$ are solutions of some linear DE and the Wronskian is zero on some interval we say that the functions $f_1,f_2,...f_n$ are linearly dependent. Is there some deep meaning why this is so?","For a set of functions , if their Wronskian determinant is identically zero for all in some interval we can't conlcude that these functions are linearly dependent. But in case of differential equations, if are solutions of some linear DE and the Wronskian is zero on some interval we say that the functions are linearly dependent. Is there some deep meaning why this is so?","f_1,f_2,...f_n W(f_1,...f_n)(x) = 0 x I f_1,f_2,...f_n f_1,f_2,...f_n","['ordinary-differential-equations', 'linear-transformations', 'wronskian']"
9,Sum of exponential growth and decay,Sum of exponential growth and decay,,"Suppose we have the equation: $$y(t_i) = C_0 + C_1 e^{-\lambda_1 t_i} + C_2 e^{\lambda_2 t_i}$$ where $C_0$ , $C_1$ , $\lambda_1$ , $C_2$ , and $\lambda_2 \ge 0$ This is equivalent to the summation of an exponential decay curve with that of exponential growth. Thus, one can see that $C_0$ is equivalent to the sum of the asymptotic value for the decay part as $t \rightarrow \infty$ and the asymptotic values for the gowth part as $t \rightarrow -\infty$ $y(0) = C_0 + C_1 + C_2$ If I have a sample of n observations, what are some good starting approximations for the estimates of $C_1$ , $\lambda_1$ , $C_2$ , and $\lambda_2$ which I can then pass into a non-linear optimization algorithm.  I've explored ODE solutions to these types of equations, but cannot find a good ""least squares"" type function of the observed data. I am considering partitioning the observations close to the position of the inflection point in the data, and estimating the $\lambda_1, \lambda_2$ from the least squares solution to the log transform of the data points to the left and the right, and then solving for $C_0, C_1,$ and $C_2$ after substituting $\hat{\lambda_1}$ and $\hat{\lambda_2}$ into the above equation. Any alternative solutions would be greatly appreciated. UPDATE: @JJacquelin gave an excellent answer.  I worked out the solution myself to confirm that the equations hold. Here is an illustration: However, there is at least one instances where the approach breaks down. The expression for $p \ or \ q = \frac{B \pm \sqrt{B^2 + 4A}}{2}$ can't be calculated when $B^2 + 4A < 0$ .","Suppose we have the equation: where , , , , and This is equivalent to the summation of an exponential decay curve with that of exponential growth. Thus, one can see that is equivalent to the sum of the asymptotic value for the decay part as and the asymptotic values for the gowth part as If I have a sample of n observations, what are some good starting approximations for the estimates of , , , and which I can then pass into a non-linear optimization algorithm.  I've explored ODE solutions to these types of equations, but cannot find a good ""least squares"" type function of the observed data. I am considering partitioning the observations close to the position of the inflection point in the data, and estimating the from the least squares solution to the log transform of the data points to the left and the right, and then solving for and after substituting and into the above equation. Any alternative solutions would be greatly appreciated. UPDATE: @JJacquelin gave an excellent answer.  I worked out the solution myself to confirm that the equations hold. Here is an illustration: However, there is at least one instances where the approach breaks down. The expression for can't be calculated when .","y(t_i) = C_0 + C_1 e^{-\lambda_1 t_i} + C_2 e^{\lambda_2 t_i} C_0 C_1 \lambda_1 C_2 \lambda_2 \ge 0 C_0 t \rightarrow \infty t \rightarrow -\infty y(0) = C_0 + C_1 + C_2 C_1 \lambda_1 C_2 \lambda_2 \lambda_1, \lambda_2 C_0, C_1, C_2 \hat{\lambda_1} \hat{\lambda_2} p \ or \ q = \frac{B \pm \sqrt{B^2 + 4A}}{2} B^2 + 4A < 0","['ordinary-differential-equations', 'exponential-function', 'least-squares', 'parameter-estimation', 'exponential-sum']"
10,Transforming a nonlinear system to a new system that has an equilibrium point at the origin,Transforming a nonlinear system to a new system that has an equilibrium point at the origin,,"Take a look at the following system $$ \begin{align} \dot{x}_1 &= x_2\\ \dot{x}_2 &= -x_1 + x^3_1 - x_2 \end{align} $$ which has three equilibrium points (0,0),(1,0), and (-1,0). In the book I'm reading, the author asks to define a new system that has equilibrium point as the origin for (1,0) and (-1,0). The procedure in the book is as follows:  let $y=x-x_e$ where $x_e$ is an equilibrium point and let's perform the transformation for $x_e = (1,0)$ , hence: $$ \begin{align} y_1 &= x_1 - (1) = x_1 - 1 \implies \dot{y}_1 = \dot{x}_1 \\ y_2 &= x_2 - (0) = x_2  \implies \dot{y}_2 = \dot{x}_2 \end{align} $$ The new system is now $$ \begin{align} \dot{y}_1 &= y_2 \\ \dot{y}_2 &= -(y_1+1) + (y_1+1)^3 - y_2 \end{align} $$ Now we need to show that the new system $\dot{y}=g(y)$ has an equilibrium point at the origin (i.e. $\dot{y}=g(y) \implies 0 = g(y_e)$ ), we get $$ \begin{align} 0 &= y_2 \\ 0 &= -(y_1+1) + (y_1+1)^3 + y_2 \end{align} $$ $y_2=0, -(y_1+1) + (y_1+1)^3 = 0 \implies y_1=0,-2$ . The new system has two equilibrium points and one of them are not at the origin. How to justify this issue. The actual question in the book states:","Take a look at the following system which has three equilibrium points (0,0),(1,0), and (-1,0). In the book I'm reading, the author asks to define a new system that has equilibrium point as the origin for (1,0) and (-1,0). The procedure in the book is as follows:  let where is an equilibrium point and let's perform the transformation for , hence: The new system is now Now we need to show that the new system has an equilibrium point at the origin (i.e. ), we get . The new system has two equilibrium points and one of them are not at the origin. How to justify this issue. The actual question in the book states:","
\begin{align}
\dot{x}_1 &= x_2\\
\dot{x}_2 &= -x_1 + x^3_1 - x_2
\end{align}
 y=x-x_e x_e x_e = (1,0) 
\begin{align}
y_1 &= x_1 - (1) = x_1 - 1 \implies \dot{y}_1 = \dot{x}_1 \\
y_2 &= x_2 - (0) = x_2  \implies \dot{y}_2 = \dot{x}_2
\end{align}
 
\begin{align}
\dot{y}_1 &= y_2 \\
\dot{y}_2 &= -(y_1+1) + (y_1+1)^3 - y_2
\end{align}
 \dot{y}=g(y) \dot{y}=g(y) \implies 0 = g(y_e) 
\begin{align}
0 &= y_2 \\
0 &= -(y_1+1) + (y_1+1)^3 + y_2
\end{align}
 y_2=0, -(y_1+1) + (y_1+1)^3 = 0 \implies y_1=0,-2","['ordinary-differential-equations', 'transformation', 'nonlinear-system']"
11,Why do we find the homogeneous solution of inhomogeneous Differential Equations?,Why do we find the homogeneous solution of inhomogeneous Differential Equations?,,"We all know that if we have inhomogeneous differential equation, we must solve for homogeneous solution and the inhomogeneous solution. And, in the end, we add them together for the complete solution. Suppose i have simple ODE : $y''+2y'+y=x$ And i got : $y_h=\left(C_1+xC_2\right)e^{-x}\\ y_p=x-2\\$ $  \begin{aligned} \therefore y &= y_h+y_p\\ &=\left(C_1+xC_2\right)e^{-x}+x-2 \end{aligned}$ But i know that the inhomogeneous solution ( $y_p$ ) is satisfied enough for that ODE, so why don't we use the $y_p$ only? Lately, This question appear in my mind and sometimes it's annoying cz i still can't answer it.","We all know that if we have inhomogeneous differential equation, we must solve for homogeneous solution and the inhomogeneous solution. And, in the end, we add them together for the complete solution. Suppose i have simple ODE : And i got : But i know that the inhomogeneous solution ( ) is satisfied enough for that ODE, so why don't we use the only? Lately, This question appear in my mind and sometimes it's annoying cz i still can't answer it.","y''+2y'+y=x y_h=\left(C_1+xC_2\right)e^{-x}\\
y_p=x-2\\ 
 \begin{aligned} \therefore y &= y_h+y_p\\
&=\left(C_1+xC_2\right)e^{-x}+x-2
\end{aligned} y_p y_p","['ordinary-differential-equations', 'differential']"
12,Curl of implicit vector field?,Curl of implicit vector field?,,"I know I can check whether a given (continous-differentiable) vector field (with simple connected domain) is conservative by checking if its curl is zero. In the 2d case for example $$ \begin{bmatrix} z_1 \\ z_2 \end{bmatrix} = \begin{bmatrix} F_1(x, y) \\ F_2(x, y) \end{bmatrix} $$ so if the curl $$ \frac{\partial }{\partial x} F_2(x,y) - \frac{\partial }{\partial y} F_1(x,y) = 0 $$ then the vector field is conservative. But what can be done if the vector field is not available as a set of explicit functions but instead only implicitly (and it is not possible to isolate the expressions)? I.e. like this: $$ \begin{bmatrix} G_1(x, y, z_1) \\ G_2(x, y, z_2) \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \tag{1} $$ Or even like this: $$ \begin{bmatrix} H_1(x, y, z_1, z_2) \\ H_2(x, y, z_1, z_2) \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \tag{2} $$ Question : Can I still somehow check if the implicit vector field $(1)$ or even $(2)$ is conservative? Example : I will give an explicit example: $$ \begin{align} z_1 &= G_1(x, y, z_1) = x^2 + y^2 + z_1^3 = 0 \\ z_2 &= G_2(x, y, z_2) = (x + y + z_2)^3 = 0  \end{align} $$ Implicit differentiation: $$ \begin{align} 0 &= 2 y + 3 z_1^2  \frac{\partial z_1}{ \partial y} \\ 0 &= 3(x + y + z_2)^2 \Big(1 + \frac{\partial z_2}{\partial x} \Big) \end{align} $$ Solve for partial derivatives: $$ \begin{align} \frac{\partial z_2}{\partial x} &= -1 \\ \frac{\partial z_1}{\partial y} &= -\frac{2 y}{3 z_1^2} \end{align} $$ Then the curl should be $$ \frac{\partial z_2}{\partial x} - \frac{\partial z_1}{\partial y} = \frac{2 y}{3 z_1^2} - 1 \neq 0 $$ So in this case, the vector field should be not a conservative vector field. Is this conclusion and the derivation correct like this?","I know I can check whether a given (continous-differentiable) vector field (with simple connected domain) is conservative by checking if its curl is zero. In the 2d case for example so if the curl then the vector field is conservative. But what can be done if the vector field is not available as a set of explicit functions but instead only implicitly (and it is not possible to isolate the expressions)? I.e. like this: Or even like this: Question : Can I still somehow check if the implicit vector field or even is conservative? Example : I will give an explicit example: Implicit differentiation: Solve for partial derivatives: Then the curl should be So in this case, the vector field should be not a conservative vector field. Is this conclusion and the derivation correct like this?","
\begin{bmatrix}
z_1 \\
z_2
\end{bmatrix} = \begin{bmatrix}
F_1(x, y) \\
F_2(x, y)
\end{bmatrix}
 
\frac{\partial }{\partial x} F_2(x,y) - \frac{\partial }{\partial y} F_1(x,y) = 0
 
\begin{bmatrix}
G_1(x, y, z_1) \\
G_2(x, y, z_2)
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix} \tag{1}
 
\begin{bmatrix}
H_1(x, y, z_1, z_2) \\
H_2(x, y, z_1, z_2)
\end{bmatrix} = \begin{bmatrix}
0 \\
0
\end{bmatrix} \tag{2}
 (1) (2) 
\begin{align}
z_1 &= G_1(x, y, z_1) = x^2 + y^2 + z_1^3 = 0 \\
z_2 &= G_2(x, y, z_2) = (x + y + z_2)^3 = 0 
\end{align}
 
\begin{align}
0 &= 2 y + 3 z_1^2  \frac{\partial z_1}{ \partial y} \\
0 &= 3(x + y + z_2)^2 \Big(1 + \frac{\partial z_2}{\partial x} \Big)
\end{align}
 
\begin{align}
\frac{\partial z_2}{\partial x} &= -1 \\
\frac{\partial z_1}{\partial y} &= -\frac{2 y}{3 z_1^2}
\end{align}
 
\frac{\partial z_2}{\partial x} - \frac{\partial z_1}{\partial y} = \frac{2 y}{3 z_1^2} - 1 \neq 0
","['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'vectors', 'vector-analysis']"
13,Differential equation $y'' - y + 2\sin(x)=0$,Differential equation,y'' - y + 2\sin(x)=0,I need help and explanation with this differential equation. Actually I really don't know how to solve just this type of equations. So the problem: $$y'' - y + 2\sin(x)=0$$ In my opinion first of all we solve homogeneous equation $y''-y=0$ and the solution of this is $y=c_1e^x+c_2e^{-x}$ . And after that to solve it with $2\sin(x)$ . From this point I need help.,I need help and explanation with this differential equation. Actually I really don't know how to solve just this type of equations. So the problem: In my opinion first of all we solve homogeneous equation and the solution of this is . And after that to solve it with . From this point I need help.,y'' - y + 2\sin(x)=0 y''-y=0 y=c_1e^x+c_2e^{-x} 2\sin(x),"['ordinary-differential-equations', 'fundamental-solution']"
14,How to prove $\lim_{n\to\infty}\sup A(t)\le\frac{a}{b}$?,How to prove ?,\lim_{n\to\infty}\sup A(t)\le\frac{a}{b},"Suppose $A(t)>0(t\ge 0)$ , $a, b>0$ , let $$ A'(t)\le aA-bA^2. $$ Prove $\lim_{n\to\infty}\sup A(t)\le\frac{a}{b}$ . Using Taylor formula $$ A(0)=A(t)-tA'(t)+o(t)\ge (1-ta)A(t) +tbA^2(t)+o(t). $$ then $$ \frac{A(0)+o(t)-A(t)}{tA(t)}\ge -a+bA(t). $$ therefore, I only need to prove $$ \lim_{t\to\infty}\sup \frac{A(0)+o(t)-A(t)}{tA(t)}=0. $$ but I have no idea about the above formula. Could you please give me any hints? Thanks in advance!","Suppose , , let Prove . Using Taylor formula then therefore, I only need to prove but I have no idea about the above formula. Could you please give me any hints? Thanks in advance!","A(t)>0(t\ge 0) a, b>0 
A'(t)\le aA-bA^2.
 \lim_{n\to\infty}\sup A(t)\le\frac{a}{b} 
A(0)=A(t)-tA'(t)+o(t)\ge (1-ta)A(t) +tbA^2(t)+o(t).
 
\frac{A(0)+o(t)-A(t)}{tA(t)}\ge -a+bA(t).
 
\lim_{t\to\infty}\sup \frac{A(0)+o(t)-A(t)}{tA(t)}=0.
","['calculus', 'ordinary-differential-equations', 'inequality']"
15,"Does this type of ODE satisfies unicity? ( right hand side f(t,y) = g(t) y, with g in L1 )","Does this type of ODE satisfies unicity? ( right hand side f(t,y) = g(t) y, with g in L1 )",,"I have the following ordinary differential equation on $(0,1)$ $$ \dot y(t) = g(t) y(t) \quad \text{ almost everywhere in } (0,1) $$ Where $g(t)$ only satisfies being integrable ( $\int_0^1 |g(x)|dx < \infty $ ) and we are looking for absolutely continuous solutions $y(t)$ on $[0,1]$ . (so, as a consequence, the solutions we look for are bounded). The standard Picard's existence and unicity theorem requires $g(t)$ to be continuous, which is not the case in here. Can I ensure uniqueness of solution? It is easy to see that the zero function is a solution, similarly, if I have a solution and an interval $[t_0,t]$ such that $y(s) \neq 0$ , has constant sign for all $s \in [t_0,t]$ , then it is also easy to see that $$ y(s) = y(t_0) e^{\int_{t_0}^s g(x)dx }.  $$ With this formula, I can extend such solution over the whole interval $(0,1)$ and notice that it would never touch the $0$ , since, for that to happen, the integral above the exponential has to value $-\infty$ ; which doesn't happens because $g$ is integrable. But I'm unsure on how to claim unicity of solution, as in, for some misterious $g$ , a wild solution could emerge from 0.","I have the following ordinary differential equation on Where only satisfies being integrable ( ) and we are looking for absolutely continuous solutions on . (so, as a consequence, the solutions we look for are bounded). The standard Picard's existence and unicity theorem requires to be continuous, which is not the case in here. Can I ensure uniqueness of solution? It is easy to see that the zero function is a solution, similarly, if I have a solution and an interval such that , has constant sign for all , then it is also easy to see that With this formula, I can extend such solution over the whole interval and notice that it would never touch the , since, for that to happen, the integral above the exponential has to value ; which doesn't happens because is integrable. But I'm unsure on how to claim unicity of solution, as in, for some misterious , a wild solution could emerge from 0.","(0,1) 
\dot y(t) = g(t) y(t) \quad \text{ almost everywhere in } (0,1)
 g(t) \int_0^1 |g(x)|dx < \infty  y(t) [0,1] g(t) [t_0,t] y(s) \neq 0 s \in [t_0,t] 
y(s) = y(t_0) e^{\int_{t_0}^s g(x)dx }. 
 (0,1) 0 -\infty g g",['ordinary-differential-equations']
16,"The IVP, $x'(t)=x^{2/3};x(0)=0$ in an interval arount $t=0$ has a","The IVP,  in an interval arount  has a",x'(t)=x^{2/3};x(0)=0 t=0,"The IVP, $\dot x(t)=x^{2/3};x(0)=0$ in an interval arount $t=0$ has a (a)No solution (b)Unique solution (c)Finitely many solutions (d)Infinitely many solutions. Solution:- Applying Picard's -Lindelof Uniqueness and existence theorem. $f(x,t)=3x^{3/2}$ , Will it be continuous at $(0,0)$ ? When $x<0$ , $f(x,t)$ no more real valued. So, Discontinuous at $(0,0)$ . So, I can not judge from the theorem. I solved using variable separable form. I got the solution, $3x(t)^{1/3}=t+c$ . When I apply initial condition, I get $3x(t)^{1/3}=t$ . A unique solution. But in the answer key it is written that equation has infinitely many solutions. How it is possible? Please explain.","The IVP, in an interval arount has a (a)No solution (b)Unique solution (c)Finitely many solutions (d)Infinitely many solutions. Solution:- Applying Picard's -Lindelof Uniqueness and existence theorem. , Will it be continuous at ? When , no more real valued. So, Discontinuous at . So, I can not judge from the theorem. I solved using variable separable form. I got the solution, . When I apply initial condition, I get . A unique solution. But in the answer key it is written that equation has infinitely many solutions. How it is possible? Please explain.","\dot x(t)=x^{2/3};x(0)=0 t=0 f(x,t)=3x^{3/2} (0,0) x<0 f(x,t) (0,0) 3x(t)^{1/3}=t+c 3x(t)^{1/3}=t",[]
17,show solutions of $y'' + p(t)y' + y = 0$ tend to 0,show solutions of  tend to 0,y'' + p(t)y' + y = 0,"I'm trying to show that, if $p$ is a continuous periodic function (with period 1, but I don't think it matters), then all solutions to $y'' + p(t)y' + y = 0$ go to $0$ at $t=\infty$ . I can show that $y' \to 0$ , but I'm not sure if this is helpful. I know that, in general, a vanishing derivative does not imply a convergent antiderivative, but I'm hoping that is the case in the context of this particular ODE.","I'm trying to show that, if is a continuous periodic function (with period 1, but I don't think it matters), then all solutions to go to at . I can show that , but I'm not sure if this is helpful. I know that, in general, a vanishing derivative does not imply a convergent antiderivative, but I'm hoping that is the case in the context of this particular ODE.",p y'' + p(t)y' + y = 0 0 t=\infty y' \to 0,"['calculus', 'ordinary-differential-equations']"
18,What does constant along characteristic mean,What does constant along characteristic mean,,"When we have linear or quasilinear first order pde $$ a(x,y,u) u_x + b(x,y,u) u_y = c(x,y,u) $$ And suppose we have found characteristics with prescribed Cauchy data $u |_{\text{curve}} = \phi$ I always see on my notes that the solution $u$ is always constant on the characteristics curves. What do they really mean by that? What is the geometric intuition ..?",When we have linear or quasilinear first order pde And suppose we have found characteristics with prescribed Cauchy data I always see on my notes that the solution is always constant on the characteristics curves. What do they really mean by that? What is the geometric intuition ..?," a(x,y,u) u_x + b(x,y,u) u_y = c(x,y,u)  u |_{\text{curve}} = \phi u","['ordinary-differential-equations', 'partial-differential-equations', 'linear-pde', 'parabolic-pde', 'pseudo-differential-operators']"
19,"Can you have solution for the equation $|\nabla f|^2=f\,\Delta f$, for a homogeneous polynomial $f$ with $\deg(f)>2$?","Can you have solution for the equation , for a homogeneous polynomial  with ?","|\nabla f|^2=f\,\Delta f f \deg(f)>2","Consider the following equation \begin{equation} |\nabla f|^2=f\,\Delta f, \end{equation} where $\nabla f$ is the gradient of $f$ and $\Delta f$ is the Laplacian of $f$ . Does this equation have a solution $f\colon \mathbb{R}^n\to \mathbb{R}$ such that $f$ is a homogenous polynomial with $\deg(f)>2$ and $n>1$ ?",Consider the following equation where is the gradient of and is the Laplacian of . Does this equation have a solution such that is a homogenous polynomial with and ?,"\begin{equation}
|\nabla f|^2=f\,\Delta f,
\end{equation} \nabla f f \Delta f f f\colon \mathbb{R}^n\to \mathbb{R} f \deg(f)>2 n>1","['ordinary-differential-equations', 'partial-differential-equations']"
20,Computing the stable manifold for a fixed point,Computing the stable manifold for a fixed point,,"Compute the stable manifold of $(0,0)$ for the system $$\dot{x}=x-y^3$$ $$\dot{y}=-y$$ Here in my proposed solution My first step isn't necessary for the solution. Although, it is helpful in seeing whether or not the x-axis and y-axis are stable. The Jacobian is $J(x,y)=\begin{pmatrix} \frac{\partial{f}}{\partial{x}} & \frac{\partial{f}}{\partial{y}} \\ \frac{\partial{g}}{\partial{x}} & \frac{\partial{g}}{\partial{y}} \end{pmatrix}=\begin{pmatrix} 1 & -3y^2 \\ 0 & -1 \end{pmatrix}$ . Therefore, $\begin{pmatrix} \frac{dx}{dt} \\ \frac{dy}{dt}  \end{pmatrix}=J(x_0,y_0)\begin{pmatrix} x-x_0 \\ y-y_0  \end{pmatrix}=\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix}=\begin{pmatrix} x \\ -y \end{pmatrix}$ . So, the x-axis is unstable while the y-axis is stable. To compute the stable manifold, we need to apply the stable manifold theorem. By the definition of $\dot{x}$ and $\dot{y}$ , $$\frac{dy}{dx}=\frac{-y}{x-y^3}$$ Therefore, $$ydx + (x-y^3)dy=0$$ We can now apply the theory of integrating factors which states that $M(x,y)dx +N(x,y)dy= 0$ . We are given that $M(x,y)=y$ and $N(x,y)=x-y^3$ . If we follow the first example in this pdf , we see that the equations are exact since $$M_y(x,y)=1=N_x(x,y)$$ So, we will therefore find a general solution such that $f(x,y)=C$ . Hence, $$f = \int Mdx + g(y)= \int ydx + g(y)= xy + g(y)$$ But, we know that $$f_y=N=x-y^3=x+g'(y)$$ which implies that $$g'(y)=-y^3$$ and so $$g(y) = \int -y^3dy =\frac{-y^4}{4}+C$$ Hence, $$f(x,y)=xy+\frac{-y^4}{4}=C$$ is the general solution. To avoid singularities in the orbits of $y(x)$ which pass through $(0,0)$ , we should set $C=0$ . Then, $$xy=\frac{y^4}{4}$$ $$4x=y^3$$ $$\sqrt[3]{4x}=y$$ Therefore, $y(x)=\sqrt[3]{4x}$ is the stable manifold. I'm not sure if there is a shorter approach to compute the stable manifold. The solution inside this question doesn't appear to work for this problem. Is this approach correct? Please let me know if there are any better alternatives.","Compute the stable manifold of for the system Here in my proposed solution My first step isn't necessary for the solution. Although, it is helpful in seeing whether or not the x-axis and y-axis are stable. The Jacobian is . Therefore, . So, the x-axis is unstable while the y-axis is stable. To compute the stable manifold, we need to apply the stable manifold theorem. By the definition of and , Therefore, We can now apply the theory of integrating factors which states that . We are given that and . If we follow the first example in this pdf , we see that the equations are exact since So, we will therefore find a general solution such that . Hence, But, we know that which implies that and so Hence, is the general solution. To avoid singularities in the orbits of which pass through , we should set . Then, Therefore, is the stable manifold. I'm not sure if there is a shorter approach to compute the stable manifold. The solution inside this question doesn't appear to work for this problem. Is this approach correct? Please let me know if there are any better alternatives.","(0,0) \dot{x}=x-y^3 \dot{y}=-y J(x,y)=\begin{pmatrix} \frac{\partial{f}}{\partial{x}} & \frac{\partial{f}}{\partial{y}} \\ \frac{\partial{g}}{\partial{x}} & \frac{\partial{g}}{\partial{y}} \end{pmatrix}=\begin{pmatrix} 1 & -3y^2 \\ 0 & -1 \end{pmatrix} \begin{pmatrix} \frac{dx}{dt} \\ \frac{dy}{dt}  \end{pmatrix}=J(x_0,y_0)\begin{pmatrix} x-x_0 \\ y-y_0  \end{pmatrix}=\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}\begin{pmatrix} x \\ y \end{pmatrix}=\begin{pmatrix} x \\ -y \end{pmatrix} \dot{x} \dot{y} \frac{dy}{dx}=\frac{-y}{x-y^3} ydx + (x-y^3)dy=0 M(x,y)dx
+N(x,y)dy= 0 M(x,y)=y N(x,y)=x-y^3 M_y(x,y)=1=N_x(x,y) f(x,y)=C f = \int Mdx + g(y)= \int ydx + g(y)= xy + g(y) f_y=N=x-y^3=x+g'(y) g'(y)=-y^3 g(y) = \int -y^3dy =\frac{-y^4}{4}+C f(x,y)=xy+\frac{-y^4}{4}=C y(x) (0,0) C=0 xy=\frac{y^4}{4} 4x=y^3 \sqrt[3]{4x}=y y(x)=\sqrt[3]{4x}","['ordinary-differential-equations', 'proof-verification', 'manifolds']"
21,Convert in a Sturm-Liouville form $y''+R(x)y'+(Q(x)+\lambda p(x))y=0$,Convert in a Sturm-Liouville form,y''+R(x)y'+(Q(x)+\lambda p(x))y=0,"Convert in a Sturm-Liouville form $$y''+R(x)y'+(Q(x)+\lambda P(x))y=0\tag1$$ My attempt: The form of Sturm-Liouville is: $$\frac{d}{dx}[r(x) \frac{dy}{dx}]+(q(x)+\lambda p(x))y=0\tag2$$ For obtain this, we need multiply for $\mu(x)$ the ODE $(1)$ . This result in: $$\mu y''+\mu R(x)y'+(\mu Q(x)+\lambda \mu P(x))y=0 \tag3$$ I need rewrite the first term of $(3)$ . Here, i'm stuck. I don't have a clear idea of how rewrite and then proceed for solve the exercise. Can someone help me?","Convert in a Sturm-Liouville form My attempt: The form of Sturm-Liouville is: For obtain this, we need multiply for the ODE . This result in: I need rewrite the first term of . Here, i'm stuck. I don't have a clear idea of how rewrite and then proceed for solve the exercise. Can someone help me?",y''+R(x)y'+(Q(x)+\lambda P(x))y=0\tag1 \frac{d}{dx}[r(x) \frac{dy}{dx}]+(q(x)+\lambda p(x))y=0\tag2 \mu(x) (1) \mu y''+\mu R(x)y'+(\mu Q(x)+\lambda \mu P(x))y=0 \tag3 (3),"['ordinary-differential-equations', 'sturm-liouville']"
22,Application of Gronwall Inequality to existence of solutions,Application of Gronwall Inequality to existence of solutions,,"Consider the $N$ -dimensional autonomous system of ODEs $$\dot{x}= f(x),$$ where $f(x)$ is defined for any $x \in \mathbb{R}^N$ , and satisfies $||f(x)|| \leq \alpha||x||$ , where $\alpha$ is a positive scalar constant, and the norm $||x||$ is the usual quadratic norm (the sum of squared components of a vector under the square root). Using Gronwall‚Äôs inequality, show that the solution emerging from any point $x_0\in\mathbb{R}^N$ exists for any finite time. Here is my proposed solution. We can first write $f(x)$ as an integral equation, $$x(t) = x_0 + \int_{t_0}^{t} f(x(s)) ds$$ where the integration constant is chosen such that $x(t_0)=x_0$ . WLOG, assume that $t_0=0$ . Then, \begin{equation} \begin{split} ||x(t)|| & = ||x_0 + \int_{0}^{t} f(x(s)) ds|| \\  & \leq ||x_0|| + ||\int_{0}^{t} f(x(s)) ds|| \\  & \leq ||x_0|| + \alpha\int_{0}^{t} ||x(s)|| ds  \end{split} \end{equation} Therefore, by the integral form of Gronwall's inequality , we see that \begin{equation} \begin{split} ||x(t)|| & \leq ||x_0|| + \alpha\int_{0}^{t} ||x(s)|| ds \\  & \leq  ||x_0||e^{\alpha(t)} \end{split} \end{equation} So, if we let $M = ||x_0||$ , then $||x(t)||\leq{{M}e^{\alpha(t)}}$ . Therefore, the solution is uniformly bounded on $[0,t]$ for $t>0$ . As $t>0$ was arbitrary, the solution is defined for all positive values of $t$ . We can then analyze what happens for negative values of $t$ by reversing time and applying the same argument to $[-t,0]$ . Once again assume that $t_0=0$ . Then, $$x(t) = x_0 + \int_{-t}^{0} f(x(s)) ds$$ Therefore, \begin{equation} \begin{split} ||x(t)|| & = ||x_0 + \int_{-t}^{0} f(x(s)) ds|| \\  & \leq ||x_0|| + ||\int_{-t}^{0} f(x(s)) ds|| \\  & \leq ||x_0|| + \alpha\int_{-t}^{0} ||x(s)|| ds \\  & \leq ||x_0||e^{\alpha(0+t)} \\  & = {M}e^{\alpha(t)} \end{split} \end{equation} So, the solution is uniformly bounded on $[-t,0]$ for $t<0$ . Combining these two bounds, we see that the solution emerging from any point $x_0\in\mathbb{R}^N$ exists for any finite time. Is this approach correct? Please let me know if there are any better alternatives.","Consider the -dimensional autonomous system of ODEs where is defined for any , and satisfies , where is a positive scalar constant, and the norm is the usual quadratic norm (the sum of squared components of a vector under the square root). Using Gronwall‚Äôs inequality, show that the solution emerging from any point exists for any finite time. Here is my proposed solution. We can first write as an integral equation, where the integration constant is chosen such that . WLOG, assume that . Then, Therefore, by the integral form of Gronwall's inequality , we see that So, if we let , then . Therefore, the solution is uniformly bounded on for . As was arbitrary, the solution is defined for all positive values of . We can then analyze what happens for negative values of by reversing time and applying the same argument to . Once again assume that . Then, Therefore, So, the solution is uniformly bounded on for . Combining these two bounds, we see that the solution emerging from any point exists for any finite time. Is this approach correct? Please let me know if there are any better alternatives.","N \dot{x}= f(x), f(x) x \in \mathbb{R}^N ||f(x)|| \leq \alpha||x|| \alpha ||x|| x_0\in\mathbb{R}^N f(x) x(t) = x_0 + \int_{t_0}^{t} f(x(s)) ds x(t_0)=x_0 t_0=0 \begin{equation}
\begin{split}
||x(t)|| & = ||x_0 + \int_{0}^{t} f(x(s)) ds|| \\
 & \leq ||x_0|| + ||\int_{0}^{t} f(x(s)) ds|| \\
 & \leq ||x_0|| + \alpha\int_{0}^{t} ||x(s)|| ds 
\end{split}
\end{equation} \begin{equation}
\begin{split}
||x(t)|| & \leq ||x_0|| + \alpha\int_{0}^{t} ||x(s)|| ds \\
 & \leq  ||x_0||e^{\alpha(t)}
\end{split}
\end{equation} M = ||x_0|| ||x(t)||\leq{{M}e^{\alpha(t)}} [0,t] t>0 t>0 t t [-t,0] t_0=0 x(t) = x_0 + \int_{-t}^{0} f(x(s)) ds \begin{equation}
\begin{split}
||x(t)|| & = ||x_0 + \int_{-t}^{0} f(x(s)) ds|| \\
 & \leq ||x_0|| + ||\int_{-t}^{0} f(x(s)) ds|| \\
 & \leq ||x_0|| + \alpha\int_{-t}^{0} ||x(s)|| ds \\
 & \leq ||x_0||e^{\alpha(0+t)} \\
 & = {M}e^{\alpha(t)}
\end{split}
\end{equation} [-t,0] t<0 x_0\in\mathbb{R}^N","['ordinary-differential-equations', 'proof-verification', 'integral-inequality']"
23,Different way of solving $\frac{d^2u}{dx^2}=1$?,Different way of solving ?,\frac{d^2u}{dx^2}=1,"I'm a third year physics student, not a math student. The conventional way to solve this would be: $$\frac{d^2u}{dx^2}=\frac{d}{dx}(\frac{du}{dx})=1 \Rightarrow d(\frac{du}{dx})= dx\Rightarrow \int d(\frac{du}{dx})=\int dx\Rightarrow \frac{du}{dx}=x+A \Rightarrow  du=(x+A)dx \Rightarrow u=\frac{x^2}{2}+Ax+B$$ So I'm wondering if what follows is correct: $$\frac{d^2u}{dx^2}=1 \Rightarrow d^2u=(dx)^2 \Rightarrow d(du)=dxdx \Rightarrow \int d(du)=\int dxdx \Rightarrow du=\int dxdx$$ At this point, I don't know if there are other ways to proceed, but I use integration by parts $$\int vdw=vw-\int wdv$$ by considering $v=dx$ and $dw=dx$ , so that $w=x$ and $dv=d(dx)$ : $$du=\int dxdx=xdx-\int x¬∑d(dx)$$ If I assume that $-\int x¬∑d(dx)=C$ is a constant, we get $$du=xdx+C \Rightarrow u=\frac{x^2}{2}+Cx+D$$ which is what we had originally. However, how would I go about proving that $\int x¬∑d(dx)=constant$ ? Is it because $d(dx)=0$ ? And how do I prove that? Are there any other interesting ways to solve this differential equation?","I'm a third year physics student, not a math student. The conventional way to solve this would be: So I'm wondering if what follows is correct: At this point, I don't know if there are other ways to proceed, but I use integration by parts by considering and , so that and : If I assume that is a constant, we get which is what we had originally. However, how would I go about proving that ? Is it because ? And how do I prove that? Are there any other interesting ways to solve this differential equation?","\frac{d^2u}{dx^2}=\frac{d}{dx}(\frac{du}{dx})=1 \Rightarrow d(\frac{du}{dx})= dx\Rightarrow \int d(\frac{du}{dx})=\int dx\Rightarrow \frac{du}{dx}=x+A \Rightarrow 
du=(x+A)dx \Rightarrow u=\frac{x^2}{2}+Ax+B \frac{d^2u}{dx^2}=1 \Rightarrow d^2u=(dx)^2 \Rightarrow d(du)=dxdx \Rightarrow \int d(du)=\int dxdx \Rightarrow du=\int dxdx \int vdw=vw-\int wdv v=dx dw=dx w=x dv=d(dx) du=\int dxdx=xdx-\int x¬∑d(dx) -\int x¬∑d(dx)=C du=xdx+C \Rightarrow u=\frac{x^2}{2}+Cx+D \int x¬∑d(dx)=constant d(dx)=0","['calculus', 'integration', 'ordinary-differential-equations', 'indefinite-integrals']"
24,Existence of Periodic Orbit,Existence of Periodic Orbit,,Consider the planar system $\dot x_1 = x_2 - x_1^3$ $\dot x_2 = -x_1$ Prove that there exists no periodic orbit in this system. I tried to use the Bendixson criteria. The divergence is equal to $-3x_1^2$ which is equal to zero on the $x_2$ axis and therefore we cannot use the Bendixson criteria.,Consider the planar system Prove that there exists no periodic orbit in this system. I tried to use the Bendixson criteria. The divergence is equal to which is equal to zero on the axis and therefore we cannot use the Bendixson criteria.,\dot x_1 = x_2 - x_1^3 \dot x_2 = -x_1 -3x_1^2 x_2,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
25,Inconsistency in units in differential equations for 2d projectile trajectory,Inconsistency in units in differential equations for 2d projectile trajectory,,"I am to solve the following two coupled second order differential equations involving the motion of a projectile. For the $y''(t)$ differential equation, I do not understand why the "" $g/m$ "" term is not  just "" $g$ "". The units do not match. I emailed my professor, and she responded that the term is supposed to be "" $g/m$ "" and not "" $g$ "", but she did not explain why.","I am to solve the following two coupled second order differential equations involving the motion of a projectile. For the differential equation, I do not understand why the "" "" term is not  just "" "". The units do not match. I emailed my professor, and she responded that the term is supposed to be "" "" and not "" "", but she did not explain why.",y''(t) g/m g g/m g,"['ordinary-differential-equations', 'physics', 'dimensional-analysis']"
26,Analyzing Zeeman's heartbeat equations. Proving that there is one and only limit cycle,Analyzing Zeeman's heartbeat equations. Proving that there is one and only limit cycle,,"From Poincare Bendixon Theorem we know that a non empty compact limit set of a continuously differentiable dynamical system in the plane which contains no equilibrium point is a closed orbit. I'm trying to analyse Zeeman's heartbeat equations, \begin{align}œµ\,\frac{dx}{dt}&=‚àí(x^3‚àíTx+b),\;T>0\\ \frac{db}{dt}&=(x‚àíx_0),\end{align} and I have to prove that there exist a closed orbit (limit cycle) and that it's the only one. So I need to use the above mentioned theorem to prove that there exist a limit cycle. I've already found out that the origin of my system is totally unstable (which is (0,0) for $\epsilon = 0.2$ , $T=1$ and $x_0=0$ ). Also all the trajectories flow away from the origin. Generally the critical point of my system is $$(x,b)=(x_0,Tx_0-x_0^3).$$ Do I consider this point to be a unique equilibrium point? Theorem says that there must be no equilibrium point, so what if there exists totally unstable critical point? Is there any other method for proving that there exist one and only limit cycle?","From Poincare Bendixon Theorem we know that a non empty compact limit set of a continuously differentiable dynamical system in the plane which contains no equilibrium point is a closed orbit. I'm trying to analyse Zeeman's heartbeat equations, and I have to prove that there exist a closed orbit (limit cycle) and that it's the only one. So I need to use the above mentioned theorem to prove that there exist a limit cycle. I've already found out that the origin of my system is totally unstable (which is (0,0) for , and ). Also all the trajectories flow away from the origin. Generally the critical point of my system is Do I consider this point to be a unique equilibrium point? Theorem says that there must be no equilibrium point, so what if there exists totally unstable critical point? Is there any other method for proving that there exist one and only limit cycle?","\begin{align}œµ\,\frac{dx}{dt}&=‚àí(x^3‚àíTx+b),\;T>0\\ \frac{db}{dt}&=(x‚àíx_0),\end{align} \epsilon = 0.2 T=1 x_0=0 (x,b)=(x_0,Tx_0-x_0^3).","['ordinary-differential-equations', 'analysis', 'bifurcation']"
27,Finding an integrating factor and solving: $(2x \sin(x + y) + \cos(x + y))dx + \cos(x + y)dy = 0$,Finding an integrating factor and solving:,(2x \sin(x + y) + \cos(x + y))dx + \cos(x + y)dy = 0,"I am trying to find an integrating factor and solve the following differential equation: $$(2x \sin(x + y) + \cos(x + y))dx + \cos(x + y)dy = 0$$ These are my steps: $$(2x \sin(x + y) + \cos(x + y)) + \cos(x + y)dy/dx = 0$$ I check if the equation is exact: \begin{equation} \partial U_{xy} = \partial U_{xy} \end{equation} \begin{equation} 2x\cos \left(x+y\right)-\sin \left(x+y\right) \neq -\sin \left(x+y\right), \end{equation} Its not so I need to find an integrating factor such that $$\frac{d}{dy} \left( Œº(x)2x\cos \left(x+y\right)-\sin \left(x+y\right) \right)= \frac{d}{dx} \left( Œº(x)-\sin \left(x+y\right) \right)$$ And at this point I simply get stuck. Any help or advice would be appreciated.",I am trying to find an integrating factor and solve the following differential equation: These are my steps: I check if the equation is exact: Its not so I need to find an integrating factor such that And at this point I simply get stuck. Any help or advice would be appreciated.,"(2x \sin(x + y) + \cos(x + y))dx + \cos(x + y)dy = 0 (2x \sin(x + y) + \cos(x + y)) + \cos(x + y)dy/dx = 0 \begin{equation}
\partial U_{xy} = \partial U_{xy}
\end{equation} \begin{equation}
2x\cos \left(x+y\right)-\sin \left(x+y\right) \neq -\sin \left(x+y\right),
\end{equation} \frac{d}{dy} \left( Œº(x)2x\cos \left(x+y\right)-\sin \left(x+y\right) \right)= \frac{d}{dx} \left( Œº(x)-\sin \left(x+y\right) \right)","['integration', 'ordinary-differential-equations']"
28,Concentration of $Z$ in the chemical reaction $6Z+B\rightleftharpoons 2Z+A$,Concentration of  in the chemical reaction,Z 6Z+B\rightleftharpoons 2Z+A,"Find the differential equation for $z$ the concentration of $Z$ in the chemistry equation $$6Z+B\rightleftharpoons^{k_1}_{k_{-1}} 2Z+A$$ My idea: Let $[Z]=z,\ [A]=a,\ [B]=b$ . Then, $$\frac{dz}{dt}=-4k_1z^6b+4k_{-1}z^2a$$ Now, my question is, if the term '' $4k_{-1}z^2a$ '' is well written (the one in the left direction)? How to completely finish the exercise?","Find the differential equation for the concentration of in the chemistry equation My idea: Let . Then, Now, my question is, if the term '' '' is well written (the one in the left direction)? How to completely finish the exercise?","z Z 6Z+B\rightleftharpoons^{k_1}_{k_{-1}} 2Z+A [Z]=z,\ [A]=a,\ [B]=b \frac{dz}{dt}=-4k_1z^6b+4k_{-1}z^2a 4k_{-1}z^2a",['ordinary-differential-equations']
29,"If omega limit set contains only one point, $x^*$, then $\lim_{t\to\infty}\phi(t;x_0)=x^*$","If omega limit set contains only one point, , then",x^* \lim_{t\to\infty}\phi(t;x_0)=x^*,"I'm trying to proof the following, and I'm looking for a verification of my proof. If it is incorrect, I'm looking for some help towards a right proof. If the omega limit set is $\omega(x_0)=\{x^*\}$ , then $\lim_{t\to\infty}\phi(t;x_0)=x^*$ Suppose $\lim_{t\to\infty}\phi(t;x_0)$ exists and equals $x'$ . Then $\omega(x_0)=\{x'\}$ , so it suffices to proof that the limit exists. Now assume for the sake of contradiction that $\lim_{t\to\infty}\phi(t;x_0)$ does not exist. Because $x^*\in\omega(x_0)$ , there exists a sequence $(t_n)_{n\in\mathbb{N}}$ such that $\lim_{n\to\infty}\phi(t_n;x_0)=x^*$ . Because $\lim_{t\to\infty}\phi(t;x_0)$ does not exist, there exists a $\delta>0$ such that there is no $T$ such that $\phi(t;x_0)\in B_{\delta}(x^*)$ for all $t\geq T$ (otherwise the limit would exist). Fix this $\delta$ . From continuity of the flow and existence of a sequence $(t_n)_{n\in\mathbb{N}}$ satisfying $\lim_{n\to\infty}t_n=\infty$ and $\lim_{n\to\infty}\phi(t_n,x_0)=x^*$ , it follows that for an arbitrary $\epsilon>\delta$ , there exists a sequence $(s_n)_{n\in\mathbb{N}}$ satisfying $\lim_{n\to\infty}s_n=\infty$ such that for all $n\in\mathbb{N}$ , $\phi(s_n;x_0)\in\overline{B_{\epsilon}(x^*)}\setminus B_{\delta}(x^*)$ , which is in particular a closed set. Now it follows from Bolzano-Weierstrass that $\left(\phi(s_n;x_0)\right)_{n\in\mathbb{N}}$ has a convergent subsequence $\left(\phi(s_{n_k};x_0)\right)_{k\in\mathbb{N}}$ , belonging to $\overline{B_{\epsilon}(x^*)}\setminus B_{\delta}(x^*)$ , because this set is closed, and thus the limit value is unequal to $x^*$ ; thus $\omega(x_0)\neq\{x^*\}$ , which gives the desired contradiction.","I'm trying to proof the following, and I'm looking for a verification of my proof. If it is incorrect, I'm looking for some help towards a right proof. If the omega limit set is , then Suppose exists and equals . Then , so it suffices to proof that the limit exists. Now assume for the sake of contradiction that does not exist. Because , there exists a sequence such that . Because does not exist, there exists a such that there is no such that for all (otherwise the limit would exist). Fix this . From continuity of the flow and existence of a sequence satisfying and , it follows that for an arbitrary , there exists a sequence satisfying such that for all , , which is in particular a closed set. Now it follows from Bolzano-Weierstrass that has a convergent subsequence , belonging to , because this set is closed, and thus the limit value is unequal to ; thus , which gives the desired contradiction.","\omega(x_0)=\{x^*\} \lim_{t\to\infty}\phi(t;x_0)=x^* \lim_{t\to\infty}\phi(t;x_0) x' \omega(x_0)=\{x'\} \lim_{t\to\infty}\phi(t;x_0) x^*\in\omega(x_0) (t_n)_{n\in\mathbb{N}} \lim_{n\to\infty}\phi(t_n;x_0)=x^* \lim_{t\to\infty}\phi(t;x_0) \delta>0 T \phi(t;x_0)\in B_{\delta}(x^*) t\geq T \delta (t_n)_{n\in\mathbb{N}} \lim_{n\to\infty}t_n=\infty \lim_{n\to\infty}\phi(t_n,x_0)=x^* \epsilon>\delta (s_n)_{n\in\mathbb{N}} \lim_{n\to\infty}s_n=\infty n\in\mathbb{N} \phi(s_n;x_0)\in\overline{B_{\epsilon}(x^*)}\setminus B_{\delta}(x^*) \left(\phi(s_n;x_0)\right)_{n\in\mathbb{N}} \left(\phi(s_{n_k};x_0)\right)_{k\in\mathbb{N}} \overline{B_{\epsilon}(x^*)}\setminus B_{\delta}(x^*) x^* \omega(x_0)\neq\{x^*\}","['real-analysis', 'ordinary-differential-equations', 'proof-verification']"
30,Let $f:\mathbb{R}\to \mathbb{R}$ be a $C^1$ function such that $f(n)=0$ for all $n \in \mathbb{Z}$. Solutions of $x'=f(x)$.,Let  be a  function such that  for all . Solutions of .,f:\mathbb{R}\to \mathbb{R} C^1 f(n)=0 n \in \mathbb{Z} x'=f(x),I'm trying to do this exercise: Let $f:\mathbb{R}\to \mathbb{R}$ be a $C^1$ function such that $f(n)=0$ for all $n \in \mathbb{Z}$ . Prove that the maximal solutions of $x'(t)=f(x(t))$ are defined and bounded for all $t \in\mathbb{R}$ . I really don't know how to attack this problem. I'd appreciate any hint. Thanks for your time.,I'm trying to do this exercise: Let be a function such that for all . Prove that the maximal solutions of are defined and bounded for all . I really don't know how to attack this problem. I'd appreciate any hint. Thanks for your time.,f:\mathbb{R}\to \mathbb{R} C^1 f(n)=0 n \in \mathbb{Z} x'(t)=f(x(t)) t \in\mathbb{R},"['calculus', 'ordinary-differential-equations']"
31,Determining Bifurcation of a Function,Determining Bifurcation of a Function,,"Hello I am trying to find analyze the bifurcation behavior of $\dot{N} = N(N - e^{\alpha N}) , N \geq 0, \alpha > 0$ as $\alpha$ is varied and find their stability. Playing around with different values of $\alpha$ , I see that when $\alpha$ is not small(e.g. $\alpha \geq 1$ would be considered ""not small"" in regards to this particular system) I find that for positive values of $N$ the only fixed point will be at $N = 0$ when considering only the non negative values of $N$ . This fixed point is stable because all positive values of $N$ are decreasing towards it and more rapidly so the larger $N$ is relative to $\alpha$ . On the other hand an interesting thing seems to occur for a small enough value, say suppose $\alpha = .1$ , assigning random nonnegative but small such as $N = 1$ $\dot{N}$ will be negative since $\dot{N}(1) = 1 - e^.1 < 0$ , positive when $N$ is ""just right in terms of being small or large"" e.g. $N = 10$ causes $\dot{N}$ to be positive since $\dot{N}(10) = 10(10 - e^1) > 0$ but then negative again after $N$ is large enough relative to $\alpha = .1$ since $\dot{N}(100) = 100(100 - e^{10}) < 0$ . Based on this quick qualitative analysis of the simple example of the value I assigned of $\alpha = .1$ , it seems there will now be 3 fixed points, 0 will still be stable but the second fixed point occurring immediately after $N$ starts to be positive will be unstable since values greater than this fixed point will increase away from it. The third fixed point occurring when $\dot{N} = 0$ for when the function then starts to decrease again will also be stable since values before it are increasing but decreasing afterwards. The main difficulty I am having in further studying the bifurcation behavior of this system is determining what is the exact bifurcation point and what type of bifurcation it would be classified as. Currently I hypothesize that the particular bifurcation point would be considered a saddle node bifurcation since the two fixed points appear and vanish before and after alpha is small enough relative to the system and that a logarithmic scale analysis would be helpful when trying to represent the bifurcation. I want to know how to solve for the exact bifurcation value in a quantitative manner, any advice would be much appreciated.","Hello I am trying to find analyze the bifurcation behavior of as is varied and find their stability. Playing around with different values of , I see that when is not small(e.g. would be considered ""not small"" in regards to this particular system) I find that for positive values of the only fixed point will be at when considering only the non negative values of . This fixed point is stable because all positive values of are decreasing towards it and more rapidly so the larger is relative to . On the other hand an interesting thing seems to occur for a small enough value, say suppose , assigning random nonnegative but small such as will be negative since , positive when is ""just right in terms of being small or large"" e.g. causes to be positive since but then negative again after is large enough relative to since . Based on this quick qualitative analysis of the simple example of the value I assigned of , it seems there will now be 3 fixed points, 0 will still be stable but the second fixed point occurring immediately after starts to be positive will be unstable since values greater than this fixed point will increase away from it. The third fixed point occurring when for when the function then starts to decrease again will also be stable since values before it are increasing but decreasing afterwards. The main difficulty I am having in further studying the bifurcation behavior of this system is determining what is the exact bifurcation point and what type of bifurcation it would be classified as. Currently I hypothesize that the particular bifurcation point would be considered a saddle node bifurcation since the two fixed points appear and vanish before and after alpha is small enough relative to the system and that a logarithmic scale analysis would be helpful when trying to represent the bifurcation. I want to know how to solve for the exact bifurcation value in a quantitative manner, any advice would be much appreciated.","\dot{N} = N(N - e^{\alpha N}) , N \geq 0, \alpha > 0 \alpha \alpha \alpha \alpha \geq 1 N N = 0 N N N \alpha \alpha = .1 N = 1 \dot{N} \dot{N}(1) = 1 - e^.1 < 0 N N = 10 \dot{N} \dot{N}(10) = 10(10 - e^1) > 0 N \alpha = .1 \dot{N}(100) = 100(100 - e^{10}) < 0 \alpha = .1 N \dot{N} = 0","['ordinary-differential-equations', 'stability-theory', 'bifurcation']"
32,Solving a certain system of differential equations,Solving a certain system of differential equations,,"Let us consider the system of differential equations: $$x'(t)= y(t)^2, y'(t) = x(t)^2, x(0) = a, y(0)=b, a, b \in \mathbb{R}.$$ How would one go about solving this system of differential equations?",Let us consider the system of differential equations: How would one go about solving this system of differential equations?,"x'(t)= y(t)^2, y'(t) = x(t)^2, x(0) = a, y(0)=b, a, b \in \mathbb{R}.",['ordinary-differential-equations']
33,Non-linear second order ODE,Non-linear second order ODE,,"I have to solve $$ y''(x)+(y'(x))^2=y'(x). $$ Using $ y'(x)=z $ , I can write $$\int \frac{1}{z-z^2}dz=\int dx $$ So: $$\frac{1}{z(1-z)}=\frac{A}{z}+\frac{B}{1-z}$$ leads to $$ \int \frac{1}{z(1-z)}dz=\int \frac{1}{z}dz+\int \frac{1}{1-z}dz= \ln(z)-\ln(1-z)$$ $$\Rightarrow \ln\left(\frac{z}{1-z}\right)=x+c $$ $$\Rightarrow z=\frac{e^{x+c}}{1+e^{x+c}}=y'$$ $$\Rightarrow y=\int \frac{e^{x+c}}{1+e^{x+c}}dx $$ Now, calling $e^{x+c}=t$ : $$y=\int\frac{t}{1+t}\cdot\frac{dt}{t}=\ln(1+t)\Rightarrow \ln(1+e^{x+c})$$ I checked the calculations and i thought was right but WolframAlpha says that the result is $\ln(c_1+e^x)+c_2$ . What am I doing wrong? Thanks for any help!","I have to solve Using , I can write So: leads to Now, calling : I checked the calculations and i thought was right but WolframAlpha says that the result is . What am I doing wrong? Thanks for any help!", y''(x)+(y'(x))^2=y'(x).   y'(x)=z  \int \frac{1}{z-z^2}dz=\int dx  \frac{1}{z(1-z)}=\frac{A}{z}+\frac{B}{1-z}  \int \frac{1}{z(1-z)}dz=\int \frac{1}{z}dz+\int \frac{1}{1-z}dz= \ln(z)-\ln(1-z) \Rightarrow \ln\left(\frac{z}{1-z}\right)=x+c  \Rightarrow z=\frac{e^{x+c}}{1+e^{x+c}}=y' \Rightarrow y=\int \frac{e^{x+c}}{1+e^{x+c}}dx  e^{x+c}=t y=\int\frac{t}{1+t}\cdot\frac{dt}{t}=\ln(1+t)\Rightarrow \ln(1+e^{x+c}) \ln(c_1+e^x)+c_2,"['ordinary-differential-equations', 'logarithms', 'exponential-function', 'indefinite-integrals', 'substitution']"
34,If the exponential of matrices equals the identity. Could they fail to commute?,If the exponential of matrices equals the identity. Could they fail to commute?,,"I am reading Vladimir Arnold's book on ordinary differential equations, and there is a problem on the Sample Examination Problems' section that is giving me a lot of troubles. The question is: Can the operators $A$ and $B$ fail to commute if $e^{A}=e^{B}=e^{A+B}= E$ , where $E$ is the identity. So far I can deduce that the eigenvalues of $A$ and $B$ most be imaginary numbers of the form $i 2\pi k$, where $k$ in an integer. Because if we write $A$ in its canonical form $A=Q^{-1}\Delta Q$, where $\Delta$ is the canonical form of $A$, using the properties of exponentials $e^{A}=e^{Q^{-1}\Delta Q}= Q^{-1} e^{\Delta} Q=E$. Then $e^{\Delta}=Q (Q^{-1} e^{\Delta} Q)Q^{-1}=Q (E) Q^{-1}=E$. A similar conclusion holds for $B$ and $A+B$. Since $\Delta$ is the canonical form $e^{t\Delta}$ it has polynomials $\frac{t^{m}}{m!}$ outside the diagonal if there is a Jordan-Block, but by evaluating on $t=1$ we find that they must be zero, so the matix $\Delta$ must be a diagonal one. And using the complex exponential we conclude that the eigenvalues of $A$ are of the form $i 2\pi k$. And that is as far as I go, I have tried some matrices but they all appear to commute. I try using Zassenhaus' formula, but no luck. Thanks.","I am reading Vladimir Arnold's book on ordinary differential equations, and there is a problem on the Sample Examination Problems' section that is giving me a lot of troubles. The question is: Can the operators $A$ and $B$ fail to commute if $e^{A}=e^{B}=e^{A+B}= E$ , where $E$ is the identity. So far I can deduce that the eigenvalues of $A$ and $B$ most be imaginary numbers of the form $i 2\pi k$, where $k$ in an integer. Because if we write $A$ in its canonical form $A=Q^{-1}\Delta Q$, where $\Delta$ is the canonical form of $A$, using the properties of exponentials $e^{A}=e^{Q^{-1}\Delta Q}= Q^{-1} e^{\Delta} Q=E$. Then $e^{\Delta}=Q (Q^{-1} e^{\Delta} Q)Q^{-1}=Q (E) Q^{-1}=E$. A similar conclusion holds for $B$ and $A+B$. Since $\Delta$ is the canonical form $e^{t\Delta}$ it has polynomials $\frac{t^{m}}{m!}$ outside the diagonal if there is a Jordan-Block, but by evaluating on $t=1$ we find that they must be zero, so the matix $\Delta$ must be a diagonal one. And using the complex exponential we conclude that the eigenvalues of $A$ are of the form $i 2\pi k$. And that is as far as I go, I have tried some matrices but they all appear to commute. I try using Zassenhaus' formula, but no luck. Thanks.",,"['linear-algebra', 'ordinary-differential-equations', 'matrix-exponential']"
35,Solving $(x^2 + 1) dy + 4 xy dx = x dx$ using separable variable method and integrating factor method,Solving  using separable variable method and integrating factor method,(x^2 + 1) dy + 4 xy dx = x dx,"Solving $(x^2 + 1) dy + 4 xy dx = x dx$ using separable variable method and integrating factor method By integrating factor method - I put it into the form of $ \frac{dy}{dx} + \frac{4x}{x^2 + 1} \cdot y = \frac{x}{x^2 + 1} $ My integrating factor is $I = (x^2 + 1)^2 $ By formula, $ y (x^2 +1)^2 = \int{ (x^3 + x)} dx $ And thus my general solution is $ y = \frac{x^4 + 2x^2 + C}{4(x^2 + 1)^2} $ By separable method is where I got problems with... I put it into the form of $\int{ (\frac{x}{x^2 + 1} )} dx = \int{ (\frac{1}{(1-4y)}} dy $ Getting $\frac{1}{2} \ln (x^2 + 1) + C = \frac{1}{4} \ln (1-4y) $ $2 \ln (x^2 +1) + C = \ln (1-4y) $ $1-4y = e^{\ln (x^2 + 1)^2} + e^{C} = (x^2 +1)^2 + C $ $ y = \frac{ (x^2+1)^2 + C - 1}{4} $ Why is my general solution from separable method very different from the integrating factor method ? I suspect that I have went wrong in the separable method but cannot identify it ...","Solving $(x^2 + 1) dy + 4 xy dx = x dx$ using separable variable method and integrating factor method By integrating factor method - I put it into the form of $ \frac{dy}{dx} + \frac{4x}{x^2 + 1} \cdot y = \frac{x}{x^2 + 1} $ My integrating factor is $I = (x^2 + 1)^2 $ By formula, $ y (x^2 +1)^2 = \int{ (x^3 + x)} dx $ And thus my general solution is $ y = \frac{x^4 + 2x^2 + C}{4(x^2 + 1)^2} $ By separable method is where I got problems with... I put it into the form of $\int{ (\frac{x}{x^2 + 1} )} dx = \int{ (\frac{1}{(1-4y)}} dy $ Getting $\frac{1}{2} \ln (x^2 + 1) + C = \frac{1}{4} \ln (1-4y) $ $2 \ln (x^2 +1) + C = \ln (1-4y) $ $1-4y = e^{\ln (x^2 + 1)^2} + e^{C} = (x^2 +1)^2 + C $ $ y = \frac{ (x^2+1)^2 + C - 1}{4} $ Why is my general solution from separable method very different from the integrating factor method ? I suspect that I have went wrong in the separable method but cannot identify it ...",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
36,Query on converting a differential equation to Sturm-Liouville Form,Query on converting a differential equation to Sturm-Liouville Form,,"I have a query on Sturm-Liouville operators written in a textbook that I am currently using for my course on Mathematical Methods in Physics. In the book, I do agree that Sturm-Liouville equations take the form of $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ whereby  $r(x)=\frac{dp(x)}{dx}$ However, in the portion written in the book, it says that any $2^{\text{nd}}$ order differential equations in the form $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ can be converted into Sturm-Liouville form by multiplying through by a suitable integrating factor, which is given by $F(x)=\exp\int^{x}{\frac{r(u)-p'(u)}{p(u)}du}$ to give the Sturm-Liouville (S-L) form $[F(x)p(x)y]'+F(x)q(x)y+\lambda F(x)\rho(x)y=0$ with a different but still non-negative weight function $F(x)\rho(x)$. My question now here is, why a different weight factor? If so, what is the point of writing $\rho(x)$ in the non-(S-L) form anyway? To my knowledge, I think that when we are solving an eigenvalue equation associated with a differential operator, it is always in the form of $Ly=\lambda y$, where $L$ refers to the differential operator. So I believe that the starting equation here should not be $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ but rather, $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda y = 0$ that is equivalent to solving an eigenvalue equation $Ly=\lambda y$ (apart from the negative sign in $\lambda$) Hence, the integrating factor $F(x)$ to convert a non-(S-L) differential operator $L$ in $Ly=\lambda y$ is the weight function as I feel that $F(x)Ly=\lambda F(x)y$ => $L'y=\lambda F(x)y$, where $L'$ is the new differential operator in S-L form. Indeed, if we compare with the initial form $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ equivalent to $Ly=\lambda \rho(x) y$, we should deduce that $\rho(x) = F(x)$, i.e. $F(x)$ is the weight function in this case right? Or am I misinterpreting the proposed form from the book that I am referring to? Should there always be a $\rho(x)$ in a general $2^{\text{nd}}$ order ODE when we are trying to solve the eigenvalues in the initial step?  Thank you. PS: Sorry for the long question! FYI: The book I am referring to is ""Mathematical Methods for Physics and Engineering"" by Riley, Hobson and Bence.","I have a query on Sturm-Liouville operators written in a textbook that I am currently using for my course on Mathematical Methods in Physics. In the book, I do agree that Sturm-Liouville equations take the form of $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ whereby  $r(x)=\frac{dp(x)}{dx}$ However, in the portion written in the book, it says that any $2^{\text{nd}}$ order differential equations in the form $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ can be converted into Sturm-Liouville form by multiplying through by a suitable integrating factor, which is given by $F(x)=\exp\int^{x}{\frac{r(u)-p'(u)}{p(u)}du}$ to give the Sturm-Liouville (S-L) form $[F(x)p(x)y]'+F(x)q(x)y+\lambda F(x)\rho(x)y=0$ with a different but still non-negative weight function $F(x)\rho(x)$. My question now here is, why a different weight factor? If so, what is the point of writing $\rho(x)$ in the non-(S-L) form anyway? To my knowledge, I think that when we are solving an eigenvalue equation associated with a differential operator, it is always in the form of $Ly=\lambda y$, where $L$ refers to the differential operator. So I believe that the starting equation here should not be $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ but rather, $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda y = 0$ that is equivalent to solving an eigenvalue equation $Ly=\lambda y$ (apart from the negative sign in $\lambda$) Hence, the integrating factor $F(x)$ to convert a non-(S-L) differential operator $L$ in $Ly=\lambda y$ is the weight function as I feel that $F(x)Ly=\lambda F(x)y$ => $L'y=\lambda F(x)y$, where $L'$ is the new differential operator in S-L form. Indeed, if we compare with the initial form $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ equivalent to $Ly=\lambda \rho(x) y$, we should deduce that $\rho(x) = F(x)$, i.e. $F(x)$ is the weight function in this case right? Or am I misinterpreting the proposed form from the book that I am referring to? Should there always be a $\rho(x)$ in a general $2^{\text{nd}}$ order ODE when we are trying to solve the eigenvalues in the initial step?  Thank you. PS: Sorry for the long question! FYI: The book I am referring to is ""Mathematical Methods for Physics and Engineering"" by Riley, Hobson and Bence.",,"['ordinary-differential-equations', 'hilbert-spaces', 'mathematical-physics', 'sturm-liouville']"
37,"ODE $\ x(y+4)+\frac{dy}{dx}=0 $, with conditions leading to a log of negative number?","ODE , with conditions leading to a log of negative number?",\ x(y+4)+\frac{dy}{dx}=0 ,"Trivial ODE and trivial question: $$\ x(y+4)+\frac{dy}{dx}=0 $$ with initial conditions $y=-5, x=0$ After we separate variables we get: $$\ -\frac{dy}{y+4}=x\,dx $$ Integrate left and write parts: $$-\ln(y+4)=\frac{1}{2}x^2 + C $$ Here we see that if $y=-5 $ we have a log of negative number.  Trying not to think about it, I proceed as follows: Exponentiation of both parts: $$y = e^{-1/2x^2-C}  - 4$$  And then: $$y = e^{-1/2x^2}e^{-C} - 4$$  $$y = e^{-1/2x^2}C - 4$$  (this new $C $ to $e^{-C}$ and cannot be negative) $$-5 = e^{0}C - 4$$ $$C=-1$$ (but we see that it is in fact negative under given initial conditions) which gives us the correct answer: $$y = -e^{-1/2x^2} - 4$$ Now I feel that I've cheated somewhere. My only guess that I was able to deal with a log of a negative number and get negative $C $ (which shouldn't be negative) is because I have involved complex numbers between the lines. Is it so? If not, how can I make this solution rigorous? Thanks!","Trivial ODE and trivial question: $$\ x(y+4)+\frac{dy}{dx}=0 $$ with initial conditions $y=-5, x=0$ After we separate variables we get: $$\ -\frac{dy}{y+4}=x\,dx $$ Integrate left and write parts: $$-\ln(y+4)=\frac{1}{2}x^2 + C $$ Here we see that if $y=-5 $ we have a log of negative number.  Trying not to think about it, I proceed as follows: Exponentiation of both parts: $$y = e^{-1/2x^2-C}  - 4$$  And then: $$y = e^{-1/2x^2}e^{-C} - 4$$  $$y = e^{-1/2x^2}C - 4$$  (this new $C $ to $e^{-C}$ and cannot be negative) $$-5 = e^{0}C - 4$$ $$C=-1$$ (but we see that it is in fact negative under given initial conditions) which gives us the correct answer: $$y = -e^{-1/2x^2} - 4$$ Now I feel that I've cheated somewhere. My only guess that I was able to deal with a log of a negative number and get negative $C $ (which shouldn't be negative) is because I have involved complex numbers between the lines. Is it so? If not, how can I make this solution rigorous? Thanks!",,['ordinary-differential-equations']
38,How can I know if the general solution of this ODE is convex?,How can I know if the general solution of this ODE is convex?,,I would like to know if the set of solutions of the following third order linear ODE is convex. $$y''' + y'' -2y' + y = e^x $$ Can I do that by solving it? Can I infer that from the fact that $y(x) = e^x$ is a particular solution and is a convex function?,I would like to know if the set of solutions of the following third order linear ODE is convex. $$y''' + y'' -2y' + y = e^x $$ Can I do that by solving it? Can I infer that from the fact that $y(x) = e^x$ is a particular solution and is a convex function?,,"['ordinary-differential-equations', 'convex-analysis']"
39,Finding Lyapunov function for a stable equilibruim of a non linear system,Finding Lyapunov function for a stable equilibruim of a non linear system,,"Given the following system: $$ \left\{  \begin{array}{c} \dot x=y-x^2-x \\  \dot y=3x-x^2-y  \end{array} \right.  $$ I need to find the equilibrium points, and if stable, to find a Lyapunov function. I have found two equilibrium points: $(0,0), (1,2)$. By linearization I'v found $(0,0)$ to be unstable, and $(1,2)$ to be stable. I tried to find a Lyapunov function for $(1,2)$ with no success. How can I find such a function?","Given the following system: $$ \left\{  \begin{array}{c} \dot x=y-x^2-x \\  \dot y=3x-x^2-y  \end{array} \right.  $$ I need to find the equilibrium points, and if stable, to find a Lyapunov function. I have found two equilibrium points: $(0,0), (1,2)$. By linearization I'v found $(0,0)$ to be unstable, and $(1,2)$ to be stable. I tried to find a Lyapunov function for $(1,2)$ with no success. How can I find such a function?",,"['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
40,Solve Green function of an annulus to calculate the shape of a clamped elastic sheet,Solve Green function of an annulus to calculate the shape of a clamped elastic sheet,,"I am trying to solve the shape of an elastic sheet clamped at $r=1$ and $r=b<1$.  $$\left\{ \begin{array}{c l}	      \Delta u = \rho(r,\phi)  \quad (a<r<1)\\      u(a)=0\\      u(1)=1 \end{array}\right.$$ I have solved the solution for a case that has the rotational symmetry ($\partial u /\partial \phi = 0$): $$\left\{ \begin{array}{c l}	      \Delta u = g  \quad (a<r<1)\\      u(a)=0\\      u(1)=1 \end{array}\right.$$ where $g$ is a constant. With the polar form of the Laplacian $\Delta = \frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial u}{\partial r}\right)+\frac{1}{r^2}\frac{\partial^2 u}{\partial \phi ^2}=\frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial u}{\partial r}\right)$, the solution is $$u(r,\phi)=\frac{g}{4} r^2 + C_1 \log{r} + C_2$$ where $C_1, C_2$ are determined by the two boundary conditions. My question is how can I find the Green function $G(\mathbf{r,r'})$ of this problem to reproduce this result ($\rho (r,\phi)= g$) so that I can apply it to the original problem with general $\rho(r,\phi)$? I am particularly interested in $\rho(r,\phi)=\delta(r-b)\delta(\phi)$ where $a<b<1$, which represents a point charge at $r=b$. I really appreciate your attention! Following Dylan's solution and choosing $\phi_0=0$, $A_n(r)$ can be calculated as \begin{equation} \begin{aligned} A_n(r)&=\frac{1}{n} \left[\Theta(r-r_0)\cdot \sinh{(n\log{(\frac{r}{r_0})})}+\frac{\sinh{(n\log{(\frac{a}{r})})}\sinh{(n\log{r_0})}}{\sinh{(n\log{a})}}\right]\\ &=\frac{1}{n} \left[\Theta (r-r_0) \cdot \left((\frac{r}{r_0})^n-(\frac{r}{r_0})^{-n}\right)+\frac{\left((\frac{a}{r})^n-(\frac{a}{r})^{-n}\right)\left(r_0^n-r_0^{-n}\right)}{a^n-a^{-n}}\right] \end{aligned} \end{equation} Since the source is even in the $\phi$ component, $B_n(r)=0$. Here is a plot of the numerical result (cross section) for a problem with actual numbers. $u_c(r)$ uses the first 30 terms of the series. $u_h(r)$ is the homogeneous solution. $u_m(r)$ is the part corresponding to the uniform loading in the in-homogeneous solution and $u_c(r)$ is the part corresponding to the point load.","I am trying to solve the shape of an elastic sheet clamped at $r=1$ and $r=b<1$.  $$\left\{ \begin{array}{c l}	      \Delta u = \rho(r,\phi)  \quad (a<r<1)\\      u(a)=0\\      u(1)=1 \end{array}\right.$$ I have solved the solution for a case that has the rotational symmetry ($\partial u /\partial \phi = 0$): $$\left\{ \begin{array}{c l}	      \Delta u = g  \quad (a<r<1)\\      u(a)=0\\      u(1)=1 \end{array}\right.$$ where $g$ is a constant. With the polar form of the Laplacian $\Delta = \frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial u}{\partial r}\right)+\frac{1}{r^2}\frac{\partial^2 u}{\partial \phi ^2}=\frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial u}{\partial r}\right)$, the solution is $$u(r,\phi)=\frac{g}{4} r^2 + C_1 \log{r} + C_2$$ where $C_1, C_2$ are determined by the two boundary conditions. My question is how can I find the Green function $G(\mathbf{r,r'})$ of this problem to reproduce this result ($\rho (r,\phi)= g$) so that I can apply it to the original problem with general $\rho(r,\phi)$? I am particularly interested in $\rho(r,\phi)=\delta(r-b)\delta(\phi)$ where $a<b<1$, which represents a point charge at $r=b$. I really appreciate your attention! Following Dylan's solution and choosing $\phi_0=0$, $A_n(r)$ can be calculated as \begin{equation} \begin{aligned} A_n(r)&=\frac{1}{n} \left[\Theta(r-r_0)\cdot \sinh{(n\log{(\frac{r}{r_0})})}+\frac{\sinh{(n\log{(\frac{a}{r})})}\sinh{(n\log{r_0})}}{\sinh{(n\log{a})}}\right]\\ &=\frac{1}{n} \left[\Theta (r-r_0) \cdot \left((\frac{r}{r_0})^n-(\frac{r}{r_0})^{-n}\right)+\frac{\left((\frac{a}{r})^n-(\frac{a}{r})^{-n}\right)\left(r_0^n-r_0^{-n}\right)}{a^n-a^{-n}}\right] \end{aligned} \end{equation} Since the source is even in the $\phi$ component, $B_n(r)=0$. Here is a plot of the numerical result (cross section) for a problem with actual numbers. $u_c(r)$ uses the first 30 terms of the series. $u_h(r)$ is the homogeneous solution. $u_m(r)$ is the part corresponding to the uniform loading in the in-homogeneous solution and $u_c(r)$ is the part corresponding to the point load.",,"['ordinary-differential-equations', 'partial-differential-equations', 'physics', 'greens-function', 'poissons-equation']"
41,"Geometric Brownian motion, product ansatz rationale","Geometric Brownian motion, product ansatz rationale",,"My question is why does the subsequent product ansatz for the geometric Brownian motion work? Suppose we have the gBm $$dS_t=\mu S_tdt+\sigma S_tdB_t,\ S(0)=S_0$$ We assume the solution is given by the product $V_t\cdot U_t$ where $V_t$ is the solution of the ODE $$dV_t=\mu V_tdt,\ V(0)=S_0$$ given by $$V_t=S_0e^{\mu t}$$ and $U_t$ is the solution of the SDE $$dUt=\sigma U_tdB_t,\ U(0)=1$$ given by (Ito's lemma) $$U_t=e^{-\tfrac{1}{2}\sigma^2t+bB_t}.$$ Inserting them in the product ansatz we indeed get the solution $$S_t=V_tU_t=S_0e^{(a-\tfrac{1}{2}\sigma^2)t+bB_t}$$ of the gBm. My background in differential equations is limited, therefore I would appreciate some information regarding the rationale, which makes this work.","My question is why does the subsequent product ansatz for the geometric Brownian motion work? Suppose we have the gBm $$dS_t=\mu S_tdt+\sigma S_tdB_t,\ S(0)=S_0$$ We assume the solution is given by the product $V_t\cdot U_t$ where $V_t$ is the solution of the ODE $$dV_t=\mu V_tdt,\ V(0)=S_0$$ given by $$V_t=S_0e^{\mu t}$$ and $U_t$ is the solution of the SDE $$dUt=\sigma U_tdB_t,\ U(0)=1$$ given by (Ito's lemma) $$U_t=e^{-\tfrac{1}{2}\sigma^2t+bB_t}.$$ Inserting them in the product ansatz we indeed get the solution $$S_t=V_tU_t=S_0e^{(a-\tfrac{1}{2}\sigma^2)t+bB_t}$$ of the gBm. My background in differential equations is limited, therefore I would appreciate some information regarding the rationale, which makes this work.",,"['ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-differential-equations']"
42,Linearization method or Lyapunov function - example,Linearization method or Lyapunov function - example,,"I have to find a proper Lyapunov function or use the linearization method, for the following system: $\left\{ \begin{array}{ll} x'=-x-y+xy \\ y'=x-y+x^2+y^2 \\ \end{array} \right.$ So first of all I tried a linearization. I've found out that $(x,y)=(0,0)$ is a solution of our system. I build a Jacobian matrix, substitute with above point, and find the characteristic polynomial which is $\lambda^2+2\lambda+2=0$, hence $\lambda_1=-1-i$ and $\lambda_2=-1+i$. We have that $Re \lambda <0$, so the zero solution is asymptotically stable. Am I right? I've also tried traditionally the Lyapunov function $V(x,y)=x^2+y^2$ ... but isn't the above enough? Phase portrait: Please check my solution, and if anything is wrong - correct me. I would also need help with Lyapunov function in this case","I have to find a proper Lyapunov function or use the linearization method, for the following system: $\left\{ \begin{array}{ll} x'=-x-y+xy \\ y'=x-y+x^2+y^2 \\ \end{array} \right.$ So first of all I tried a linearization. I've found out that $(x,y)=(0,0)$ is a solution of our system. I build a Jacobian matrix, substitute with above point, and find the characteristic polynomial which is $\lambda^2+2\lambda+2=0$, hence $\lambda_1=-1-i$ and $\lambda_2=-1+i$. We have that $Re \lambda <0$, so the zero solution is asymptotically stable. Am I right? I've also tried traditionally the Lyapunov function $V(x,y)=x^2+y^2$ ... but isn't the above enough? Phase portrait: Please check my solution, and if anything is wrong - correct me. I would also need help with Lyapunov function in this case",,"['ordinary-differential-equations', 'analysis', 'derivatives', 'stability-in-odes', 'lyapunov-functions']"
43,Differential equation $ x y y' = x^2 + ( 1+ a ) y^2 $,Differential equation, x y y' = x^2 + ( 1+ a ) y^2 ,"I have to solve this equation, where $ a \in \mathbb R $: $$ x y y' = x^2 + ( 1+ a ) y^2 $$ And I firstly thought about Bernouilli's method : we set $ y^2 = z$ then the equation : $$ x \frac {z'} z = x^2 + (1+a) z $$ so then the homogenous equation gives : $$\frac{ z' } { z^2 } = \frac{1+a}{x} $$ so $$ \frac 1 z = \ln ( \frac{1}{ x^{1+a}   })+ cst  $$ It seems already false comparing to the answer I'm suppose to have. Can you tell me where is my mistake and if the substitution recommanded by Bernouilli's is a good idea?","I have to solve this equation, where $ a \in \mathbb R $: $$ x y y' = x^2 + ( 1+ a ) y^2 $$ And I firstly thought about Bernouilli's method : we set $ y^2 = z$ then the equation : $$ x \frac {z'} z = x^2 + (1+a) z $$ so then the homogenous equation gives : $$\frac{ z' } { z^2 } = \frac{1+a}{x} $$ so $$ \frac 1 z = \ln ( \frac{1}{ x^{1+a}   })+ cst  $$ It seems already false comparing to the answer I'm suppose to have. Can you tell me where is my mistake and if the substitution recommanded by Bernouilli's is a good idea?",,['ordinary-differential-equations']
44,First order non-linear ordinary differential equation containing $e^x$,First order non-linear ordinary differential equation containing,e^x,$$(t+e^y) + \left(\frac{t^2}{2} + 2te^y\right)y' = 0$$ How can i solve this? I've never seen a differential equation like that.,$$(t+e^y) + \left(\frac{t^2}{2} + 2te^y\right)y' = 0$$ How can i solve this? I've never seen a differential equation like that.,,['ordinary-differential-equations']
45,Laplace Equation on Rectangle & Fourier Series,Laplace Equation on Rectangle & Fourier Series,,"I am trying to solve the Laplace Equation on the rectangle. Namely, I trying to solve the following PDE problem: Solve the equation $\Delta u = 0$ on the rectangle $R = \{(x, y): 0\leq x \leq a, \hspace{2mm} 0\leq y \leq b\}$ subject to $u(x,0) = f(x)$, $u(x, b) = 0$, $u(0, y) = 0$ and $u(a, y) = 0$. I have separated variables and used the first three boundary conditions to obtain $$u(x, y) = \sum_{n= 1}^{\infty} c_n \sin\left(\frac{n\pi x}{a}\right)\sinh\left(\frac{n\pi (b-y)}{a}\right)$$ Now it is the final boundary condition that seems strange. I am reading how it has been dealt with Olver's introduction book to PDEs. His argument goes as follows: We want $$u(x, 0) =:f(x) =  \sum_{n= 1}^{\infty} c_n \sinh\left(\frac{n\pi b}{a}\right)\sin\left(\frac{n\pi x}{a}\right)$$ and this looks like the fourier sine series for $f(x)$. So let $$b_n = \frac{2}{a} \int_0^a f(x) \sin\left(\frac{n\pi x}{a}\right)dx$$ then $c_n = \frac{b_n}{\sinh\left(\frac{n\pi b}{a}\right)}$. But the issue I have here is that does the fact that $f$ has a fourier sine series not assume/imply that the function $f$ is odd? How is this possible - was the choice of $f$ not arbitrary? So the PDE problem could have been posed with a specific function for $f$ where $f$ is instead even and then this does not make sense anymore?","I am trying to solve the Laplace Equation on the rectangle. Namely, I trying to solve the following PDE problem: Solve the equation $\Delta u = 0$ on the rectangle $R = \{(x, y): 0\leq x \leq a, \hspace{2mm} 0\leq y \leq b\}$ subject to $u(x,0) = f(x)$, $u(x, b) = 0$, $u(0, y) = 0$ and $u(a, y) = 0$. I have separated variables and used the first three boundary conditions to obtain $$u(x, y) = \sum_{n= 1}^{\infty} c_n \sin\left(\frac{n\pi x}{a}\right)\sinh\left(\frac{n\pi (b-y)}{a}\right)$$ Now it is the final boundary condition that seems strange. I am reading how it has been dealt with Olver's introduction book to PDEs. His argument goes as follows: We want $$u(x, 0) =:f(x) =  \sum_{n= 1}^{\infty} c_n \sinh\left(\frac{n\pi b}{a}\right)\sin\left(\frac{n\pi x}{a}\right)$$ and this looks like the fourier sine series for $f(x)$. So let $$b_n = \frac{2}{a} \int_0^a f(x) \sin\left(\frac{n\pi x}{a}\right)dx$$ then $c_n = \frac{b_n}{\sinh\left(\frac{n\pi b}{a}\right)}$. But the issue I have here is that does the fact that $f$ has a fourier sine series not assume/imply that the function $f$ is odd? How is this possible - was the choice of $f$ not arbitrary? So the PDE problem could have been posed with a specific function for $f$ where $f$ is instead even and then this does not make sense anymore?",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'fourier-series', 'harmonic-functions']"
46,Non-homogeneous wave equation,Non-homogeneous wave equation,,"We have the following problem:  $$v_{tt}=c^2v_{xx}+f(x,t), \ \ 0<x<\ell , \  t>0 \\ v(x,0)=\phi (x) , \ \ 0\leq x\leq \ell \\ v_t(x,0)=\psi (x) , \ \ 0\leq x\leq \ell \\ v_x(0,t)=v_x(\ell , t)=0 , \ \ t\geq 0$$ The general solution is of the form $$v(x,t)=u(x,t)+w(x,t)$$ where $u(x,t)$ is the solution of teh homogeneous differential equation $u_{tt}=c^2u_{xx}$. So, we have to solve the following two problems: The first one is:  $$u_{tt}=c^2u_{xx}, \ \ 0<x<\ell , \  t>0 \\ u(x,0)=\phi (x)-w(x,0) , \ \ 0\leq x\leq \ell \\ u_t(x,0)=\psi (x)-w_t(x,0) , \ \ 0\leq x\leq \ell \\ u_x(0,t)=u_x(\ell , t)=0 , \ \ t\geq 0$$  and the second problem is:  $$w_{tt}=c^2w_{xx}+f(x,t), \ \ 0<x<\ell , \  t>0 \\ w(x,0)=0 , \ \ 0\leq x\leq \ell \\ w_t(x,0)=0, \ \ 0\leq x\leq \ell \\ w_x(0,t)=w_x(\ell , t)=0 , \ \ t\geq 0$$ Right? Or do we have to do something else? Is everything is correct so far, the solution of the first problem can we found by d'Alembert's formula, or not? How can we solve the second problem?","We have the following problem:  $$v_{tt}=c^2v_{xx}+f(x,t), \ \ 0<x<\ell , \  t>0 \\ v(x,0)=\phi (x) , \ \ 0\leq x\leq \ell \\ v_t(x,0)=\psi (x) , \ \ 0\leq x\leq \ell \\ v_x(0,t)=v_x(\ell , t)=0 , \ \ t\geq 0$$ The general solution is of the form $$v(x,t)=u(x,t)+w(x,t)$$ where $u(x,t)$ is the solution of teh homogeneous differential equation $u_{tt}=c^2u_{xx}$. So, we have to solve the following two problems: The first one is:  $$u_{tt}=c^2u_{xx}, \ \ 0<x<\ell , \  t>0 \\ u(x,0)=\phi (x)-w(x,0) , \ \ 0\leq x\leq \ell \\ u_t(x,0)=\psi (x)-w_t(x,0) , \ \ 0\leq x\leq \ell \\ u_x(0,t)=u_x(\ell , t)=0 , \ \ t\geq 0$$  and the second problem is:  $$w_{tt}=c^2w_{xx}+f(x,t), \ \ 0<x<\ell , \  t>0 \\ w(x,0)=0 , \ \ 0\leq x\leq \ell \\ w_t(x,0)=0, \ \ 0\leq x\leq \ell \\ w_x(0,t)=w_x(\ell , t)=0 , \ \ t\geq 0$$ Right? Or do we have to do something else? Is everything is correct so far, the solution of the first problem can we found by d'Alembert's formula, or not? How can we solve the second problem?",,"['ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
47,Prove that the origin is Liapunov Stable for the given system,Prove that the origin is Liapunov Stable for the given system,,"Consider the system $$\dot{x} = y \\ \dot{y} = -4x  $$ ($\dot{x}$ means $\displaystyle \frac{dx}{dt}$ and $\dot{y}$ means $\displaystyle \frac{dy}{dt})$ I need to prove that the fixed point $\mathbf{x^{*} = 0}$ is Liapunov stable. For reference, according to my textbook (Strogatz), a fixed point $\mathbf{x^{*}}$ is said to be Liapunov stable if $\forall \epsilon > 0$, $\exists \delta >0$ such that $|\mathbf{x(t)} - \mathbf{x^{*}}| < \epsilon$ whenever $t \geq 0$ and $|\mathbf{x(0)}- \mathbf{x^{*}}|< \delta$ Now, if we let $x(0)=x_{0}$, $y(0)=y_{0}$, I was able to solve this system using eigenvalues and eigenvectors to have the following general solution: $$ x(t) = x_{0} \cos (2t) + \frac{y_{0}}{2}\sin(2t) \\ y(t) = -2x_{0}\sin(2t) + y_{0}\cos(2t)$$ According to my solutions manual, ""it can be observed from the equations in the solution that $\delta < |x_{0}|$. This implies that $|x(t),y(t)|<2|x_{0}|$, where $2|x_{0}|$ is the radius $\epsilon$. Thus, it can be concluded that $|x(t),y(t)|<\epsilon$."" However, I don't understand how they were able to show this. We never really learned how to do these kinds of $\epsilon-\delta$ proofs for systems in my class, and this solution doesn't give a very good (meaning ""detailed"") explanation as to how they went about doing it. Could someone please explain to me step-by-step how they were able to surmise all of this? I would like to be able to apply this to other problems going forward, but unless I see one done out in detail, I am afraid I will not be able to. Thank you for your time and patience.","Consider the system $$\dot{x} = y \\ \dot{y} = -4x  $$ ($\dot{x}$ means $\displaystyle \frac{dx}{dt}$ and $\dot{y}$ means $\displaystyle \frac{dy}{dt})$ I need to prove that the fixed point $\mathbf{x^{*} = 0}$ is Liapunov stable. For reference, according to my textbook (Strogatz), a fixed point $\mathbf{x^{*}}$ is said to be Liapunov stable if $\forall \epsilon > 0$, $\exists \delta >0$ such that $|\mathbf{x(t)} - \mathbf{x^{*}}| < \epsilon$ whenever $t \geq 0$ and $|\mathbf{x(0)}- \mathbf{x^{*}}|< \delta$ Now, if we let $x(0)=x_{0}$, $y(0)=y_{0}$, I was able to solve this system using eigenvalues and eigenvectors to have the following general solution: $$ x(t) = x_{0} \cos (2t) + \frac{y_{0}}{2}\sin(2t) \\ y(t) = -2x_{0}\sin(2t) + y_{0}\cos(2t)$$ According to my solutions manual, ""it can be observed from the equations in the solution that $\delta < |x_{0}|$. This implies that $|x(t),y(t)|<2|x_{0}|$, where $2|x_{0}|$ is the radius $\epsilon$. Thus, it can be concluded that $|x(t),y(t)|<\epsilon$."" However, I don't understand how they were able to show this. We never really learned how to do these kinds of $\epsilon-\delta$ proofs for systems in my class, and this solution doesn't give a very good (meaning ""detailed"") explanation as to how they went about doing it. Could someone please explain to me step-by-step how they were able to surmise all of this? I would like to be able to apply this to other problems going forward, but unless I see one done out in detail, I am afraid I will not be able to. Thank you for your time and patience.",,"['ordinary-differential-equations', 'dynamical-systems']"
48,"Number of equilibrium points, first order ODE","Number of equilibrium points, first order ODE",,"Question: Determine the number and location of the equilibrium points for all values of $c$ of $\dot{x}=x^2+2x+c+2$, $(1)$ $x \in \mathbb{R}$, where $c \in \mathbb{R}$, constant, is a control parameter for the system. My Answer: $\dot{x}=0$ for stationary points. So, $x^2+2x+c+2=0$. Using the quadratic formula we get, $x=\frac{-2¬± \sqrt{2^2-4(c+2)}}{2}=-1¬±\sqrt{-1-c}$ . If $c=-1$: then $x=-1$ is a stationary point of $(1)$. If $c<-1$: then $x=-1¬±\sqrt{\alpha}$ where $\alpha=-1-c>0$ are stationary points of $(1)$. If $c>-1$: then $x=-1¬±\sqrt{\beta}i$ where $\beta=1+c>0$ are stationary points of $(1)$. I am not sure if what I have done is correct as I was not sure how to deal with $c$ being unknown. Is this all I needed to do or have I missed some points?","Question: Determine the number and location of the equilibrium points for all values of $c$ of $\dot{x}=x^2+2x+c+2$, $(1)$ $x \in \mathbb{R}$, where $c \in \mathbb{R}$, constant, is a control parameter for the system. My Answer: $\dot{x}=0$ for stationary points. So, $x^2+2x+c+2=0$. Using the quadratic formula we get, $x=\frac{-2¬± \sqrt{2^2-4(c+2)}}{2}=-1¬±\sqrt{-1-c}$ . If $c=-1$: then $x=-1$ is a stationary point of $(1)$. If $c<-1$: then $x=-1¬±\sqrt{\alpha}$ where $\alpha=-1-c>0$ are stationary points of $(1)$. If $c>-1$: then $x=-1¬±\sqrt{\beta}i$ where $\beta=1+c>0$ are stationary points of $(1)$. I am not sure if what I have done is correct as I was not sure how to deal with $c$ being unknown. Is this all I needed to do or have I missed some points?",,"['ordinary-differential-equations', 'stationary-point']"
49,Almost done solving IVP using Laplace transform .. Need advice/guidance,Almost done solving IVP using Laplace transform .. Need advice/guidance,,"I have the following IVP: $$\begin{cases}y''-2y'+5y = -8e^{-t},\\ \ \\  y(0) = 2\\ \ \\ y'(0) = 12\end{cases}$$ I will show my steps so far. After taking the transform of each term I get: $$s^2Y(s) - 2sY(s) + 5Y(s) - 2s - 8 = \frac {-8}{s+1}$$ I simplified the L.H.S and got: $$ Y(s)(s^2-2s+5) = \frac {-8}{s+1} + 2s+ 8 $$ Then the R.H.S becomes: $$ Y(s)(s^2-2s+5) = \frac {2s^2+10s}{s+1}$$ I then get: $$Y(s) =  \frac {2s^2+10s}{(s+1)(s^2-2s+5)}$$ I then used partial fraction decomposition and got $A = -1, B = 3$, and $C = 5$ I then have the following: $$Y(s) = \frac {-1}{s+1} + \frac {3s+5}{s^2-2s+5}$$ I am now stuck finding the inverse Laplace transform of the second term. Please can someone check my work and let me know if I did this right and, if so, what is that inverse Laplace transform I mentioned at the last step? Help is greatly appreciated. Thank you.","I have the following IVP: $$\begin{cases}y''-2y'+5y = -8e^{-t},\\ \ \\  y(0) = 2\\ \ \\ y'(0) = 12\end{cases}$$ I will show my steps so far. After taking the transform of each term I get: $$s^2Y(s) - 2sY(s) + 5Y(s) - 2s - 8 = \frac {-8}{s+1}$$ I simplified the L.H.S and got: $$ Y(s)(s^2-2s+5) = \frac {-8}{s+1} + 2s+ 8 $$ Then the R.H.S becomes: $$ Y(s)(s^2-2s+5) = \frac {2s^2+10s}{s+1}$$ I then get: $$Y(s) =  \frac {2s^2+10s}{(s+1)(s^2-2s+5)}$$ I then used partial fraction decomposition and got $A = -1, B = 3$, and $C = 5$ I then have the following: $$Y(s) = \frac {-1}{s+1} + \frac {3s+5}{s^2-2s+5}$$ I am now stuck finding the inverse Laplace transform of the second term. Please can someone check my work and let me know if I did this right and, if so, what is that inverse Laplace transform I mentioned at the last step? Help is greatly appreciated. Thank you.",,"['ordinary-differential-equations', 'laplace-transform']"
50,"Non linear Differential equation, sketching nullclines","Non linear Differential equation, sketching nullclines",,"Question Consider the system of ordinary differential equations \begin{equation}   \begin{aligned} \dot{x} &= 1 + y - \exp(-x) \\ \dot{y} &= x^3 -y   \end{aligned} \end{equation} Find and classify the fixed point(s) of this system Sketch the nullclines of the system and sketch a plausible phase portrait I have done the following : established that the only fixed point is (0,0) evaluated the Jacobian of the system at this fixed point So at (0,0) I have $$ \begin{bmatrix}   1   & 1 \\   0  & -1 \end{bmatrix} $$ And the eigenvalues of this are $$ \lambda_1 = 1 , \lambda_2 = -1 $$ The eigenvectors for this are $$ v_1 = (1, 0)^T, v_2 = (1, -2)^T $$ From this point I'm unsure how to consider plotting. The plot that I have done is as follows : The solution is here : I don't understand how the green lines are formed there, and why they aren't converging to the x axis. For my plot I have found the eigenvectors from the Jacobian at the fixed point, and used those for the flow. My question is - how to know the direction of the green flow lines for a system like this. I reviewed the following posts and was unable to answer my question : Plotting phase portrait of saddle node using Nullclines , this question seemed as though it might have been similar, but there are no sketches and the info is a bit sparse (for me). Help interpreting behaviour of a simple system of differential equations using  nullclines and direction fields ,  this question seems similar in nature, but the system is larger and the use of  XPPAUT complicates things (for me).","Question Consider the system of ordinary differential equations \begin{equation}   \begin{aligned} \dot{x} &= 1 + y - \exp(-x) \\ \dot{y} &= x^3 -y   \end{aligned} \end{equation} Find and classify the fixed point(s) of this system Sketch the nullclines of the system and sketch a plausible phase portrait I have done the following : established that the only fixed point is (0,0) evaluated the Jacobian of the system at this fixed point So at (0,0) I have $$ \begin{bmatrix}   1   & 1 \\   0  & -1 \end{bmatrix} $$ And the eigenvalues of this are $$ \lambda_1 = 1 , \lambda_2 = -1 $$ The eigenvectors for this are $$ v_1 = (1, 0)^T, v_2 = (1, -2)^T $$ From this point I'm unsure how to consider plotting. The plot that I have done is as follows : The solution is here : I don't understand how the green lines are formed there, and why they aren't converging to the x axis. For my plot I have found the eigenvectors from the Jacobian at the fixed point, and used those for the flow. My question is - how to know the direction of the green flow lines for a system like this. I reviewed the following posts and was unable to answer my question : Plotting phase portrait of saddle node using Nullclines , this question seemed as though it might have been similar, but there are no sketches and the info is a bit sparse (for me). Help interpreting behaviour of a simple system of differential equations using  nullclines and direction fields ,  this question seems similar in nature, but the system is larger and the use of  XPPAUT complicates things (for me).",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'stability-in-odes']"
51,Recognize this power series?,Recognize this power series?,,"I've been doing some research in ergodic theory (on Perron-Frobenius operators on Banach spaces of analytic functions) and have been led to some power series that may have been studied in other contexts. The power series are $$ \Phi_k(z)= \sum_{m\text{ even}}\binom mk z^{m/2}, $$ where the sum is taken over those $m$'s such that $m\ge k$. I have played around with this and written it as a difference of two rational expressions in $\sqrt z$ (even though, of course $\Phi_k(z)$ is actually an analytic function of $z$ in the unit disk - the odd powers of $\sqrt z$ cancel). Are these functions known in some existing context? Are there other well-known examples (possibly series solutions of differential equations?) where one is led to a difference of power series in $\sqrt z$, where the half-integer powers all cancel?","I've been doing some research in ergodic theory (on Perron-Frobenius operators on Banach spaces of analytic functions) and have been led to some power series that may have been studied in other contexts. The power series are $$ \Phi_k(z)= \sum_{m\text{ even}}\binom mk z^{m/2}, $$ where the sum is taken over those $m$'s such that $m\ge k$. I have played around with this and written it as a difference of two rational expressions in $\sqrt z$ (even though, of course $\Phi_k(z)$ is actually an analytic function of $z$ in the unit disk - the odd powers of $\sqrt z$ cancel). Are these functions known in some existing context? Are there other well-known examples (possibly series solutions of differential equations?) where one is led to a difference of power series in $\sqrt z$, where the half-integer powers all cancel?",,"['ordinary-differential-equations', 'power-series']"
52,"Solve nonlinear ODE's System $x_1'(t)=-x_1(t)x_3(t)$, $x_2'(t)=-x_2(t)x_3(t)$, $x_3'(t)=x_1^2(t)+x_2^2(t)$","Solve nonlinear ODE's System , ,",x_1'(t)=-x_1(t)x_3(t) x_2'(t)=-x_2(t)x_3(t) x_3'(t)=x_1^2(t)+x_2^2(t),"I need to study the flow generated by the vector field $X(x,y,z)=(-xz,-yz,x^2+y^2)$. Therefore, I need to solve the system: $$ \left\{  \begin{array}{ccc}  x_1'(t)&=&-x_1(t)x_3(t)\\ x_2'(t) &=& -x_2(t)x_3(t)\\ x_3'(t) &=&x_1^2(t)+x_2^2(t) \end{array}\right. $$ I don't know how to solve this. I would appretiate any reference or help to solve this kind of equations.","I need to study the flow generated by the vector field $X(x,y,z)=(-xz,-yz,x^2+y^2)$. Therefore, I need to solve the system: $$ \left\{  \begin{array}{ccc}  x_1'(t)&=&-x_1(t)x_3(t)\\ x_2'(t) &=& -x_2(t)x_3(t)\\ x_3'(t) &=&x_1^2(t)+x_2^2(t) \end{array}\right. $$ I don't know how to solve this. I would appretiate any reference or help to solve this kind of equations.",,"['ordinary-differential-equations', 'dynamical-systems']"
53,Prove a dynamical system has at least one periodic orbit,Prove a dynamical system has at least one periodic orbit,,"Question : Prove that the dynamical system    $$ \dot{x}=x(1-x^2-y^2)-y+\frac{1}{2}xy, \\ \dot{y}=y(1-x^2-y^2)+x+y^2+2x^2, $$   where $(x,y) \in \mathbb{R}^2$, has at least one periodic orbit. My Answer : I used $x=r\cos\theta$ and $y=r\sin\theta$ to write the system in polar coordinates. I got,  $$ \dot{r}=r(1-r^2)+r^2\sin^3\theta+\frac{5}{2}r^2\cos^2\theta \sin\theta, \\ \dot{\theta}=1+\frac{1}{2}r\cos\theta \sin^2\theta +2r\cos^3\theta. $$ In previous examples we have looked at where $\dot{r}$ and $\dot{\theta}$ are greater than or less than $0$. But I am not sure how the $sine$ and $cosine$ components will affect $\dot{r}$ and $\dot{\theta}$. As $sine$ and $cosine$ oscillate do I only need to look at $\dot{r}=r(1-r^2)$, and $\dot{\theta}=1$ ?","Question : Prove that the dynamical system    $$ \dot{x}=x(1-x^2-y^2)-y+\frac{1}{2}xy, \\ \dot{y}=y(1-x^2-y^2)+x+y^2+2x^2, $$   where $(x,y) \in \mathbb{R}^2$, has at least one periodic orbit. My Answer : I used $x=r\cos\theta$ and $y=r\sin\theta$ to write the system in polar coordinates. I got,  $$ \dot{r}=r(1-r^2)+r^2\sin^3\theta+\frac{5}{2}r^2\cos^2\theta \sin\theta, \\ \dot{\theta}=1+\frac{1}{2}r\cos\theta \sin^2\theta +2r\cos^3\theta. $$ In previous examples we have looked at where $\dot{r}$ and $\dot{\theta}$ are greater than or less than $0$. But I am not sure how the $sine$ and $cosine$ components will affect $\dot{r}$ and $\dot{\theta}$. As $sine$ and $cosine$ oscillate do I only need to look at $\dot{r}=r(1-r^2)$, and $\dot{\theta}=1$ ?",,"['ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates']"
54,Prove $x(t)\geq x(0)e^{\int_{0}^{t} y(s) ds}$? [closed],Prove ? [closed],x(t)\geq x(0)e^{\int_{0}^{t} y(s) ds},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Given conditions on $x(t)$ and $y(t)$ $x(t)$ and $y(t)$ are both infinitely many times differentiable. $x(t)\geq0$ and $y(t)\geq0$ for all $t\geq0$. $x(t)\geq x(0) + $$\int_{0}^{t} y(s)x(s) ds$ Then I need to prove that     $x(t)\geq x(0)e^{\int_{0}^{t} y(s) ds}$ I am just confused how to start, anyone give me some ideas so that I could understand how to begin?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Given conditions on $x(t)$ and $y(t)$ $x(t)$ and $y(t)$ are both infinitely many times differentiable. $x(t)\geq0$ and $y(t)\geq0$ for all $t\geq0$. $x(t)\geq x(0) + $$\int_{0}^{t} y(s)x(s) ds$ Then I need to prove that     $x(t)\geq x(0)e^{\int_{0}^{t} y(s) ds}$ I am just confused how to start, anyone give me some ideas so that I could understand how to begin?",,"['calculus', 'ordinary-differential-equations']"
55,What connects Fibonacci and Lucas numbers?,What connects Fibonacci and Lucas numbers?,,"The Fibonacci numbers $F_n$ and Lucas numbers $L_n$ both satisfy the recurrence relation $x_n=x_{n-1}+x_{n-2}$. The Fibonacci numbers start $0,1$ while the Lucas numbers start $2,1$. I would like to know about the theory underpinning them, in particular, the analogies with differential equations ($\sin$ and $\cos$) and how it works for for higher order (linear, homogeneous, constant-coefficient) difference equations. I have many questions Are the two sequences linearly independent? Orthogonal? Given $F_0=0$ and $F_1=1$ does this define $L_n$? How? $\sin$ and $\cos$ have $0,1$ and $1,0$ as $f(0)$ and $f'(0)$, respectively. Can we view $F_n$ and $L_n$ similarly? On the page: http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/lucasNbs.html they say that (2,1) is chosen because they are the two ""simplest"" (positive) numbers which don't produce the Fibonacci numbers (or shifted versions of them). Also, they seem to exclude (0,2) which produces the Fibonacci numbers. (I need to to read the following page http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/fibGen.html#gstart to work out what ""simplest"" really means and why the choice (0,1) and (2,1) give rise to so many nice properties.)","The Fibonacci numbers $F_n$ and Lucas numbers $L_n$ both satisfy the recurrence relation $x_n=x_{n-1}+x_{n-2}$. The Fibonacci numbers start $0,1$ while the Lucas numbers start $2,1$. I would like to know about the theory underpinning them, in particular, the analogies with differential equations ($\sin$ and $\cos$) and how it works for for higher order (linear, homogeneous, constant-coefficient) difference equations. I have many questions Are the two sequences linearly independent? Orthogonal? Given $F_0=0$ and $F_1=1$ does this define $L_n$? How? $\sin$ and $\cos$ have $0,1$ and $1,0$ as $f(0)$ and $f'(0)$, respectively. Can we view $F_n$ and $L_n$ similarly? On the page: http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/lucasNbs.html they say that (2,1) is chosen because they are the two ""simplest"" (positive) numbers which don't produce the Fibonacci numbers (or shifted versions of them). Also, they seem to exclude (0,2) which produces the Fibonacci numbers. (I need to to read the following page http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/fibGen.html#gstart to work out what ""simplest"" really means and why the choice (0,1) and (2,1) give rise to so many nice properties.)",,"['ordinary-differential-equations', 'recurrence-relations', 'fibonacci-numbers']"
56,What does the notation $xf(x)$ and $f_{xxx}$ or $f_{xxy}$ mean?,What does the notation  and  or  mean?,xf(x) f_{xxx} f_{xxy},"Hi I'm wondering about notations I encountered when studying differential equations and oscillation. $V''' = \frac{3\pi}{4b}(f_{xxx} + f_{xyy} +g_{xxy}+g_{yyy})+\frac{3\pi}{4b^2}[f_{xy}(f_{xx}+f_{yy})+g_{xy}(g_{xx}+g_{yy})+f_{xx}g_{xx}-f_{yy}g_{yy}]$ This is a Calculation of Stability of the limit Cycle in Hopf bifurcation, and I don't know what $f_{xxx}$ etc., means. A differential equation is given by $\frac{dx}{dt}=xf(x,y)$ What does the $xf$ stand for? Thanks Josi","Hi I'm wondering about notations I encountered when studying differential equations and oscillation. $V''' = \frac{3\pi}{4b}(f_{xxx} + f_{xyy} +g_{xxy}+g_{yyy})+\frac{3\pi}{4b^2}[f_{xy}(f_{xx}+f_{yy})+g_{xy}(g_{xx}+g_{yy})+f_{xx}g_{xx}-f_{yy}g_{yy}]$ This is a Calculation of Stability of the limit Cycle in Hopf bifurcation, and I don't know what $f_{xxx}$ etc., means. A differential equation is given by $\frac{dx}{dt}=xf(x,y)$ What does the $xf$ stand for? Thanks Josi",,"['ordinary-differential-equations', 'notation']"
57,"prove that $x(t) \in ]0,\pi[$",prove that,"x(t) \in ]0,\pi[","Given the Cauchy Problem: $ \left\{ \begin{array}{@{}l} x'(t) = \sin (x(t)),\ t\in\mathbb{R}\\  x(0)=x_0 \in ]0,\pi[ \end{array} \right. $ I try to prove that $x(t) \in ]0,\pi[ $ What I did: Using Cauchy-Lipschitz theorem, I proved that the Cauchy Problem has a unique solution on $\mathbb{R}$ I also proved that the solution cannot be constant since no $k \in \mathbb{Z}$ verifies $x_0=k\pi$. I don't know to follow-up from there. any help much appreciated.","Given the Cauchy Problem: $ \left\{ \begin{array}{@{}l} x'(t) = \sin (x(t)),\ t\in\mathbb{R}\\  x(0)=x_0 \in ]0,\pi[ \end{array} \right. $ I try to prove that $x(t) \in ]0,\pi[ $ What I did: Using Cauchy-Lipschitz theorem, I proved that the Cauchy Problem has a unique solution on $\mathbb{R}$ I also proved that the solution cannot be constant since no $k \in \mathbb{Z}$ verifies $x_0=k\pi$. I don't know to follow-up from there. any help much appreciated.",,"['real-analysis', 'ordinary-differential-equations']"
58,Solving following System of ordinary differential equations.,Solving following System of ordinary differential equations.,,How to solve system of ODEs which contains independent variable like t in equation as in this particular case :  $$\frac{dx}{dt}= -x + ty $$ $$ \frac{dy}{dt} = tx-y$$ can we solve them or additional information may be needed (may be about t) ?,How to solve system of ODEs which contains independent variable like t in equation as in this particular case :  $$\frac{dx}{dt}= -x + ty $$ $$ \frac{dy}{dt} = tx-y$$ can we solve them or additional information may be needed (may be about t) ?,,['ordinary-differential-equations']
59,Hamiltonfunction under a Duffing Oscillator,Hamiltonfunction under a Duffing Oscillator,,"Given is the following oscillator $$\ddot{x} + \lambda\dot{x}=x-x^3$$ I've already rewritten this as a system of first order equations $$\begin{cases} \dot{x} = y \\ \dot{y}=x-x^3-\lambda y \end{cases}$$ Now the question is how the Hamiltonfunction $H(x,y)=\frac{1}{2}y^2+U(x)$ changes in time under our given oscillator with $\lambda \in \mathbb{R}$. We know that for $\lambda=0$, $\dot{y}=-U'(x)$. If I calculate $\dot{H}$ I find $$\dot{H}=\frac{\partial H}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial H}{\partial y}\frac{\partial y}{\partial t}=U'(x)\cdot y + y(x-x^3-\lambda y)$$ The problem I face is that I don't know if $U(x)$ depends on $\lambda$. Does anyone have an idea to get me going?","Given is the following oscillator $$\ddot{x} + \lambda\dot{x}=x-x^3$$ I've already rewritten this as a system of first order equations $$\begin{cases} \dot{x} = y \\ \dot{y}=x-x^3-\lambda y \end{cases}$$ Now the question is how the Hamiltonfunction $H(x,y)=\frac{1}{2}y^2+U(x)$ changes in time under our given oscillator with $\lambda \in \mathbb{R}$. We know that for $\lambda=0$, $\dot{y}=-U'(x)$. If I calculate $\dot{H}$ I find $$\dot{H}=\frac{\partial H}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial H}{\partial y}\frac{\partial y}{\partial t}=U'(x)\cdot y + y(x-x^3-\lambda y)$$ The problem I face is that I don't know if $U(x)$ depends on $\lambda$. Does anyone have an idea to get me going?",,['ordinary-differential-equations']
60,Significance of an interval for an ODE,Significance of an interval for an ODE,,"Question Find all the solutions for the equation  $$y'(x)+y(x)=\int_0^x y(t) dt$$ defined on $[0,1]$. Attempt I have calculated the complete solution as  $$y(x)=Ae^{\frac{-1+\sqrt5}{2}x}+Be^{\frac{-1-\sqrt5}{2}x}$$ However, I cannot figure out what to do with the provided interval since the equation make sense for all values of $x$ in $(-\infty,\infty)$. Can somebody explain what the question intends me to do with the interval? Edit As seen by answers below I have realised the interval acts provides the initial. Proceeding under the provided information, I got the answer as follows $$y'(0)=-y(0)$$ $$y(0)= Ae^{0}+Be^{0}=A+B$$ $$y'(0)= \frac{-1+\sqrt5}{2}Ae^{0}+\frac{-1-\sqrt5}{2}Be^{0}= \frac{-1+\sqrt5}{2}A+\frac{-1-\sqrt5}{2}B$$ $$-\left( \frac{-1+\sqrt5}{2}A+\frac{-1-\sqrt5}{2}B \right)=A+B$$ $$\frac{1-\sqrt5}{2}A+\frac{1+\sqrt5}{2}B =A+B$$ $$\frac{-1-\sqrt5}{2}A=\frac{1-\sqrt5}{2}B $$ $$\frac{1+\sqrt5}{2}A=\frac{-1+\sqrt5}{2}B $$ $$A=\frac{-1+\sqrt5}{1+\sqrt5}B $$ Inputting this in the final equation $$y(x)=\frac{-1+\sqrt5}{1+\sqrt5}Be^{\frac{-1+\sqrt5}{2}x}+Be^{\frac{-1-\sqrt5}{2}x} = B\left(\frac{-1+\sqrt5}{1+\sqrt5}e^{\frac{-1+\sqrt5}{2}x}+e^{\frac{-1-\sqrt5}{2}x} \right)$$ Is this correct?","Question Find all the solutions for the equation  $$y'(x)+y(x)=\int_0^x y(t) dt$$ defined on $[0,1]$. Attempt I have calculated the complete solution as  $$y(x)=Ae^{\frac{-1+\sqrt5}{2}x}+Be^{\frac{-1-\sqrt5}{2}x}$$ However, I cannot figure out what to do with the provided interval since the equation make sense for all values of $x$ in $(-\infty,\infty)$. Can somebody explain what the question intends me to do with the interval? Edit As seen by answers below I have realised the interval acts provides the initial. Proceeding under the provided information, I got the answer as follows $$y'(0)=-y(0)$$ $$y(0)= Ae^{0}+Be^{0}=A+B$$ $$y'(0)= \frac{-1+\sqrt5}{2}Ae^{0}+\frac{-1-\sqrt5}{2}Be^{0}= \frac{-1+\sqrt5}{2}A+\frac{-1-\sqrt5}{2}B$$ $$-\left( \frac{-1+\sqrt5}{2}A+\frac{-1-\sqrt5}{2}B \right)=A+B$$ $$\frac{1-\sqrt5}{2}A+\frac{1+\sqrt5}{2}B =A+B$$ $$\frac{-1-\sqrt5}{2}A=\frac{1-\sqrt5}{2}B $$ $$\frac{1+\sqrt5}{2}A=\frac{-1+\sqrt5}{2}B $$ $$A=\frac{-1+\sqrt5}{1+\sqrt5}B $$ Inputting this in the final equation $$y(x)=\frac{-1+\sqrt5}{1+\sqrt5}Be^{\frac{-1+\sqrt5}{2}x}+Be^{\frac{-1-\sqrt5}{2}x} = B\left(\frac{-1+\sqrt5}{1+\sqrt5}e^{\frac{-1+\sqrt5}{2}x}+e^{\frac{-1-\sqrt5}{2}x} \right)$$ Is this correct?",,['ordinary-differential-equations']
61,Differential Equation-Separation of variables,Differential Equation-Separation of variables,,"1) Solve the following differential equation by separation of variables (or otherwise) $$\frac{dy}{dx}-1=e^{x-y}$$. What I tried :- Suppose $$z^2=e^{x-y}$$         $$\Rightarrow 2z\frac{dz}{dx}=e^{x-y}(1-\frac{dy}{dx})$$ $$\Rightarrow 2z\frac{dz}{dx}=z^2(1-\frac{dy}{dx})$$ $$\Rightarrow 2\frac{dz}{dx}=z(1-\frac{dy}{dx})$$ $$\Rightarrow \frac{dy}{dx}=1-\frac{2dz}{zdx} \tag1$$ Again, $$\frac{dy}{dx}-1=e^{x-y}$$ $$\Rightarrow \frac{dy}{dx}=1+z^2 \tag2$$ Equating $(1)$ and $(2)$, $$1+z^2=1-\frac{2dz}{zdx}$$ $$\Rightarrow z^3dx=-2dz$$ $$\Rightarrow z^{-3}dz=-\frac{1}{2}dx$$ $$\Rightarrow \int z^{-3}dz=\int -\frac{1}{2}dx$$ $$\Rightarrow \frac{z^{-2}}{-2}=-\frac{1}{2} x+\frac{c}{2}$$ $$\Rightarrow -z^{-2}=-x+c$$ $$\Rightarrow z^{-2}-x+c=0$$ Am I correct ?","1) Solve the following differential equation by separation of variables (or otherwise) $$\frac{dy}{dx}-1=e^{x-y}$$. What I tried :- Suppose $$z^2=e^{x-y}$$         $$\Rightarrow 2z\frac{dz}{dx}=e^{x-y}(1-\frac{dy}{dx})$$ $$\Rightarrow 2z\frac{dz}{dx}=z^2(1-\frac{dy}{dx})$$ $$\Rightarrow 2\frac{dz}{dx}=z(1-\frac{dy}{dx})$$ $$\Rightarrow \frac{dy}{dx}=1-\frac{2dz}{zdx} \tag1$$ Again, $$\frac{dy}{dx}-1=e^{x-y}$$ $$\Rightarrow \frac{dy}{dx}=1+z^2 \tag2$$ Equating $(1)$ and $(2)$, $$1+z^2=1-\frac{2dz}{zdx}$$ $$\Rightarrow z^3dx=-2dz$$ $$\Rightarrow z^{-3}dz=-\frac{1}{2}dx$$ $$\Rightarrow \int z^{-3}dz=\int -\frac{1}{2}dx$$ $$\Rightarrow \frac{z^{-2}}{-2}=-\frac{1}{2} x+\frac{c}{2}$$ $$\Rightarrow -z^{-2}=-x+c$$ $$\Rightarrow z^{-2}-x+c=0$$ Am I correct ?",,['ordinary-differential-equations']
62,Proving integral identity for solution to Bessel equation,Proving integral identity for solution to Bessel equation,,"I read in a book that $$\int_0^1x[J_n(\alpha x)]^2dx = \frac{1}{2}[J_n'(\alpha)]^2$$ where $\alpha$ is a zero of $J_n$ and where $J_n$ is a solution to Bessels equation. The book gives a hint telling me to use the substitution $z=\alpha x$ and then integrating by parts and then use the fact that $J_n$ solves Bessels equation. I tried to use the hint to prove this, but end up going in circles when Integrating by parts.","I read in a book that $$\int_0^1x[J_n(\alpha x)]^2dx = \frac{1}{2}[J_n'(\alpha)]^2$$ where $\alpha$ is a zero of $J_n$ and where $J_n$ is a solution to Bessels equation. The book gives a hint telling me to use the substitution $z=\alpha x$ and then integrating by parts and then use the fact that $J_n$ solves Bessels equation. I tried to use the hint to prove this, but end up going in circles when Integrating by parts.",,"['ordinary-differential-equations', 'bessel-functions']"
63,"If $y=\mathrm{e}^x\big(a\sin x+b\cos x\big)$, then express $y^{(n)}$ in terms of $y$ and $y'$.","If , then express  in terms of  and .",y=\mathrm{e}^x\big(a\sin x+b\cos x\big) y^{(n)} y y',"Let $y=e^x(a\sin x+b\cos x)$.  Show $y''=py'+ qy$ for some constants $p$ and $q$; and express all higher derivatives as linear combinations of $y'$ and $y$. I got to $y''=2y'-2y$, but I'm not sure how to do the linear combinations part, I don't know how to reduce the $n$th derivative to just $y'$ and $y$.","Let $y=e^x(a\sin x+b\cos x)$.  Show $y''=py'+ qy$ for some constants $p$ and $q$; and express all higher derivatives as linear combinations of $y'$ and $y$. I got to $y''=2y'-2y$, but I'm not sure how to do the linear combinations part, I don't know how to reduce the $n$th derivative to just $y'$ and $y$.",,"['real-analysis', 'sequences-and-series', 'ordinary-differential-equations', 'recurrence-relations', 'recursion']"
64,How to find out the critical angle of a ball separating the circle?,How to find out the critical angle of a ball separating the circle?,,"Question: Let function $x:\mathbf R_+\to[0,2\pi]$ satisfying the second-order nonlinear ODE   \begin{equation} \left\{ \begin{aligned} \ddot x(t) &= \sin x(t)-\cos x(t)+{\dot x(t)}^2, \quad t>0; \\ x(0) &= 0, \\ \dot x(0) &= \epsilon, \end{aligned}\right. \end{equation}   where $0<\epsilon\ll1$ is a given constant. Suppose that there exist a $t_0\in\mathbf R_+$ such that ${\dot x(t_0)}^2=\cos x(t_0)$. Then how to find out the value $x(t_0)$? Actually the question arises from the following physical phenomenon: A small ball starts from the north pole of a large vertical circle with a sufficient small initial velocity, and then falls outside. For simplicity, we normalize all the coefficients like the gravitational acceleration, the radius of the circle and the friction factor, and neglect the size of the ball. Then it's easy to derive that the angle of the vertical line with the line connecting ball and center of circle satisfies the kinematic equation presented above, and the time $t_0$ is exactly the moment that the ball separates from the circle with critical angle $x(t_0)$. If the friction is not involved, then the equation of the angle degenerates into $$\ddot x = \sin x,$$ and the question can be trivially solved by applying the first integral in the theory of ODEs. But how to solve the case that the friction is involved? That is, how to solve the general question described above analytically? The key difficulty is that the ODE in Question seems not to possess a first integral. I have no idea then... Any comments or hints will be appreciated. TIA!","Question: Let function $x:\mathbf R_+\to[0,2\pi]$ satisfying the second-order nonlinear ODE   \begin{equation} \left\{ \begin{aligned} \ddot x(t) &= \sin x(t)-\cos x(t)+{\dot x(t)}^2, \quad t>0; \\ x(0) &= 0, \\ \dot x(0) &= \epsilon, \end{aligned}\right. \end{equation}   where $0<\epsilon\ll1$ is a given constant. Suppose that there exist a $t_0\in\mathbf R_+$ such that ${\dot x(t_0)}^2=\cos x(t_0)$. Then how to find out the value $x(t_0)$? Actually the question arises from the following physical phenomenon: A small ball starts from the north pole of a large vertical circle with a sufficient small initial velocity, and then falls outside. For simplicity, we normalize all the coefficients like the gravitational acceleration, the radius of the circle and the friction factor, and neglect the size of the ball. Then it's easy to derive that the angle of the vertical line with the line connecting ball and center of circle satisfies the kinematic equation presented above, and the time $t_0$ is exactly the moment that the ball separates from the circle with critical angle $x(t_0)$. If the friction is not involved, then the equation of the angle degenerates into $$\ddot x = \sin x,$$ and the question can be trivially solved by applying the first integral in the theory of ODEs. But how to solve the case that the friction is involved? That is, how to solve the general question described above analytically? The key difficulty is that the ODE in Question seems not to possess a first integral. I have no idea then... Any comments or hints will be appreciated. TIA!",,"['ordinary-differential-equations', 'dynamical-systems', 'physics', 'mathematical-physics', 'nonlinear-dynamics']"
65,Does the laplacian operator work on time as well as spacial variables?,Does the laplacian operator work on time as well as spacial variables?,,"I am currently working on a diffusion problem involving partial differential equations and am a bit confused about how the Laplacian (Laplace?) operator works on different independent variables such as time and space. Here is a picture of the question: PDE Problem involving space and time . Now what I initially tried to do was expand out the laplace term into a second derivative in x and a second derivative in time. However, in the solutions which are provided in the following picture, they do not expand out a second derivative in time and I am confused as to why they would only do it in x. Solution to PDE . Does the laplace operator only work on spacial variables then? Thank you for your help. Note the method being used to solve the problem is a separation of variables.","I am currently working on a diffusion problem involving partial differential equations and am a bit confused about how the Laplacian (Laplace?) operator works on different independent variables such as time and space. Here is a picture of the question: PDE Problem involving space and time . Now what I initially tried to do was expand out the laplace term into a second derivative in x and a second derivative in time. However, in the solutions which are provided in the following picture, they do not expand out a second derivative in time and I am confused as to why they would only do it in x. Solution to PDE . Does the laplace operator only work on spacial variables then? Thank you for your help. Note the method being used to solve the problem is a separation of variables.",,"['ordinary-differential-equations', 'partial-differential-equations', 'laplacian']"
66,Differential equation: find fundamental set of solutions,Differential equation: find fundamental set of solutions,,"We have $y' = Ay$ with $A = \begin{pmatrix} 2 & 1 \\ 0 & 1 \end{pmatrix} $ First I found out the eigenvalues which are $e_1 =2$ and $e_2 = 1$. Both of them are $Œ± = 1$ which means they both have the algebraic multiplicity $1$. Then I found out the geometric multiplicity. For both eigenvalues it is $Œ≥ =1$. Now I got troubles to set up the fundamental set of solution. I tried to use the following rule: Every funtion(=column of the fundamental matrix) of the fundametal set of solutions is a linear combination of the following functions:  $x^ve^{e_jx}$ with $1 \leq j \le 2$, $0 \le v \le Œ±_j-Œ≥_j$ The theorem works when I have only one eigenvalue but this time I have two different eigenvalues and it is not working. I dont see where my mistake is. I get: $$œÜ = \begin{pmatrix} a_1e^{2x} \\ a_2e^x \end{pmatrix} $$ afterwards I would usually do a compare of coefficients of $AœÜ$ and $œÜ'$ to find $a_1$ and $a_2$ but the  $œÜ$ seems to be wrong. Alternatively I could solve this differential equation with ""Jordan-Matrix way"" but I want to know where my mistake is.","We have $y' = Ay$ with $A = \begin{pmatrix} 2 & 1 \\ 0 & 1 \end{pmatrix} $ First I found out the eigenvalues which are $e_1 =2$ and $e_2 = 1$. Both of them are $Œ± = 1$ which means they both have the algebraic multiplicity $1$. Then I found out the geometric multiplicity. For both eigenvalues it is $Œ≥ =1$. Now I got troubles to set up the fundamental set of solution. I tried to use the following rule: Every funtion(=column of the fundamental matrix) of the fundametal set of solutions is a linear combination of the following functions:  $x^ve^{e_jx}$ with $1 \leq j \le 2$, $0 \le v \le Œ±_j-Œ≥_j$ The theorem works when I have only one eigenvalue but this time I have two different eigenvalues and it is not working. I dont see where my mistake is. I get: $$œÜ = \begin{pmatrix} a_1e^{2x} \\ a_2e^x \end{pmatrix} $$ afterwards I would usually do a compare of coefficients of $AœÜ$ and $œÜ'$ to find $a_1$ and $a_2$ but the  $œÜ$ seems to be wrong. Alternatively I could solve this differential equation with ""Jordan-Matrix way"" but I want to know where my mistake is.",,['real-analysis']
67,Region of attraction and stability via Liapunov's function,Region of attraction and stability via Liapunov's function,,"EXERCISE: Estimate the region of stability for the stationary point $O(0,0)$ given the differential system:   $$x'=y$$   $$y'=x^7-2\cdot x-y$$   using the liapunov's funcion $V(x,y)=\dfrac{1}{2}\cdot x^2+\dfrac{1}{2}\cdot(x+y)^2$ Attempt : We have that $V'=V_x \cdot x'+V_y \cdot y'$ So,$V'=(2x+y) \cdot y +(x+y)\cdot (x^7-2x -y)=x^8-2x^2+x^7y-xy $ So,how can i find the sign of V'.I think that i have to find it $V'<0$ everywhere outside the origin and the stationary point $O(0,0)$ will be asymptotic stable! After,that how can i find the region of attraction?","EXERCISE: Estimate the region of stability for the stationary point $O(0,0)$ given the differential system:   $$x'=y$$   $$y'=x^7-2\cdot x-y$$   using the liapunov's funcion $V(x,y)=\dfrac{1}{2}\cdot x^2+\dfrac{1}{2}\cdot(x+y)^2$ Attempt : We have that $V'=V_x \cdot x'+V_y \cdot y'$ So,$V'=(2x+y) \cdot y +(x+y)\cdot (x^7-2x -y)=x^8-2x^2+x^7y-xy $ So,how can i find the sign of V'.I think that i have to find it $V'<0$ everywhere outside the origin and the stationary point $O(0,0)$ will be asymptotic stable! After,that how can i find the region of attraction?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory', 'basins-of-attraction']"
68,Solve $(y+ x^3y + 2x^2)dx + (x + 4xy^4+ 8y^3)dy = 0$,Solve,(y+ x^3y + 2x^2)dx + (x + 4xy^4+ 8y^3)dy = 0,Basically I tried to group the terms ydx + xdy is a perfect differential but I couldn't think a method for  ($x^3$y)dx + (4x$y^4$)dy . Also dividing by xy didn't help me.,Basically I tried to group the terms ydx + xdy is a perfect differential but I couldn't think a method for  ($x^3$y)dx + (4x$y^4$)dy . Also dividing by xy didn't help me.,,['ordinary-differential-equations']
69,Eigenvectors and their relationship to (first order) linear differential equations,Eigenvectors and their relationship to (first order) linear differential equations,,"Ok, so in my differential equations class we've been doing problems which more or less amount to solving equations of the form: $$\frac{dY}{dt} = AY$$ Where $A$ is just some $2\times2$ linear transformation and $Y$ is a parametric vector function defined more specifically as $$Y(t) = \begin{bmatrix} x(t) \\ y(t) \end{bmatrix}$$ The end result, assuming that there exists $\lambda_1, \lambda_2 \ne 0; \lambda_1 \ne \lambda_2$ which define the eigen values for A, is a definition for $Y(t)$ of the form, $$Y(t) = k_1e^{\lambda_1t}\vec{V_1} + k_2e^{\lambda_2t}\vec{V_2}$$ Where $\vec{V_1}, \vec{V_2}$ are the corresponding eigen vectors to their respective eigen values and $k_1, k_2$ are just some constants. For solutions which involve either $k_1 = 0$ or $k_2 = 0$ , the end result is a straight-line solution. The rest are exponential curves within the vector space defined by the eigen vectors. My understanding of eigen vectors, from a linear algebra class I took a year ago, so far is as follows (roughly): geometrically speaking, an eigenvector is any vector whose direction after transformation by some matrix $A$ remains the same. It's only scaled and/or negated. Every eigenvector for some matrix $A$ composes a subspace which in turn defines the eigen space for $A$ 's vector basis. therefore, the eigenvectors which $span(A)$ are linearly independent and define a coordinate space which also exists within $A$ . Regardless of whether or not the above is correct (if there's a mistake, any clarification/correction would be appreciated), what is it about eigenvectors specifically which allows for them to be used to solve these forms of differential equations?","Ok, so in my differential equations class we've been doing problems which more or less amount to solving equations of the form: Where is just some linear transformation and is a parametric vector function defined more specifically as The end result, assuming that there exists which define the eigen values for A, is a definition for of the form, Where are the corresponding eigen vectors to their respective eigen values and are just some constants. For solutions which involve either or , the end result is a straight-line solution. The rest are exponential curves within the vector space defined by the eigen vectors. My understanding of eigen vectors, from a linear algebra class I took a year ago, so far is as follows (roughly): geometrically speaking, an eigenvector is any vector whose direction after transformation by some matrix remains the same. It's only scaled and/or negated. Every eigenvector for some matrix composes a subspace which in turn defines the eigen space for 's vector basis. therefore, the eigenvectors which are linearly independent and define a coordinate space which also exists within . Regardless of whether or not the above is correct (if there's a mistake, any clarification/correction would be appreciated), what is it about eigenvectors specifically which allows for them to be used to solve these forms of differential equations?","\frac{dY}{dt} = AY A 2\times2 Y Y(t) = \begin{bmatrix}
x(t) \\
y(t)
\end{bmatrix} \lambda_1, \lambda_2 \ne 0; \lambda_1 \ne \lambda_2 Y(t) Y(t) = k_1e^{\lambda_1t}\vec{V_1} + k_2e^{\lambda_2t}\vec{V_2} \vec{V_1}, \vec{V_2} k_1, k_2 k_1 = 0 k_2 = 0 A A A span(A) A","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
70,"Region of attraction of : $x'=-y-x^3,y'=x-y^3$ via Lyapunov Function",Region of attraction of :  via Lyapunov Function,"x'=-y-x^3,y'=x-y^3","PROBLEM : $1)$ Show that the stationary point $O(0,0)$ is asymptotic stable $2)$ Find a region of attraction for the system : $$x'=-y-x^3$$ $$y'=x-y^3$$ given the Lyapunov's function: $$V=x^2+y^2$$ First, I differentiate my Lyapunov's function and i take: $$ \dot{V}=-2 \cdot (x^4+y^4)<0 $$ So, the stationary point $$O(0,0)$$ is asymptotic stable as $$\dot{V}<0$$ everywhere outside the origin. For the second question of my problem, I believe that the region of attraction is the circle : $$x^2+y^2=c $$ Is this right?How  can I find $c$ and the boundary  of the estimation of the region of attraction? If i am right with the circle can anyone help me to write it in a good mathematical way? Last days I am trying to understand how regions of attraction work so , I would really appreciate a thorough solution and explanation about how to find this region of attraction. Thanks in advance!","PROBLEM : Show that the stationary point is asymptotic stable Find a region of attraction for the system : given the Lyapunov's function: First, I differentiate my Lyapunov's function and i take: So, the stationary point is asymptotic stable as everywhere outside the origin. For the second question of my problem, I believe that the region of attraction is the circle : Is this right?How  can I find and the boundary  of the estimation of the region of attraction? If i am right with the circle can anyone help me to write it in a good mathematical way? Last days I am trying to understand how regions of attraction work so , I would really appreciate a thorough solution and explanation about how to find this region of attraction. Thanks in advance!","1) O(0,0) 2) x'=-y-x^3 y'=x-y^3 V=x^2+y^2  \dot{V}=-2 \cdot (x^4+y^4)<0  O(0,0) \dot{V}<0 x^2+y^2=c  c","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory', 'basins-of-attraction']"
71,Floquet multipliers of $x'=f(t)A_ox$,Floquet multipliers of,x'=f(t)A_ox,I am trying to find the Floquet multipliers of $x'=f(t)A_ox$ where f(t) is a scalar T-periodic function and $A_o$ is a constant matrix with real distinct eigenvalues. I know that the floquet multipliers are the eigenvalues of L where the fundamental matrix $X(t)=e^{Lt}$. I guess my first problem is that I don't know how to find the fundamental matrix of $x'=f(t)A_ox$. It doesn't fit any of the standard forms I have learned. Thanks for any help you can give.,I am trying to find the Floquet multipliers of $x'=f(t)A_ox$ where f(t) is a scalar T-periodic function and $A_o$ is a constant matrix with real distinct eigenvalues. I know that the floquet multipliers are the eigenvalues of L where the fundamental matrix $X(t)=e^{Lt}$. I guess my first problem is that I don't know how to find the fundamental matrix of $x'=f(t)A_ox$. It doesn't fit any of the standard forms I have learned. Thanks for any help you can give.,,"['ordinary-differential-equations', 'periodic-functions']"
72,How do I transform $(\sin^2y + x \arctan y)y' = 1 $ to apply Lagrange's method?,How do I transform  to apply Lagrange's method?,(\sin^2y + x \arctan y)y' = 1 ,"Orignal equation is: $$(\sin^2y + x \arctan y)y' = 1 $$ It is very well seen that the equation is first-order non-linear one, so I wanted to use Lagrange's method here, but I do not know how to transform it to $$y'+p(x)\cdot y = q(x)$$ which is standard form of the equation","Orignal equation is: $$(\sin^2y + x \arctan y)y' = 1 $$ It is very well seen that the equation is first-order non-linear one, so I wanted to use Lagrange's method here, but I do not know how to transform it to $$y'+p(x)\cdot y = q(x)$$ which is standard form of the equation",,['ordinary-differential-equations']
73,How to solve this linear ODE with time-varying coefficients?,How to solve this linear ODE with time-varying coefficients?,,"I have a problem with the following ODE of a time-varying system: $$y''+\frac{4}{t} y' +\frac{2}{t^2} y=u(t), \qquad t>0 $$ The exercises indicates: Find a representation of the form $x'(t)=A(t)x(t)+B(t)u(t)$ Find a fundamental solution set and construct a fundamental solutions matrix $X(t)$ Determine state transition matrix $\phi (t,\tau)$ So, I tried to solve it, my procedure is in these two pictures (text in spanish): My Procedure part 1 My procedure part 2 I guess it's a total disaster, but I'm confused and I would like any guidance for a better understanding and find the correct procedures. Thanks for your attention.","I have a problem with the following ODE of a time-varying system: The exercises indicates: Find a representation of the form Find a fundamental solution set and construct a fundamental solutions matrix Determine state transition matrix So, I tried to solve it, my procedure is in these two pictures (text in spanish): My Procedure part 1 My procedure part 2 I guess it's a total disaster, but I'm confused and I would like any guidance for a better understanding and find the correct procedures. Thanks for your attention.","y''+\frac{4}{t} y' +\frac{2}{t^2} y=u(t), \qquad t>0  x'(t)=A(t)x(t)+B(t)u(t) X(t) \phi (t,\tau)","['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
74,Existence and uniqueness - $y'' + p(x)y' + q(x)y = f(x)$,Existence and uniqueness -,y'' + p(x)y' + q(x)y = f(x),"Exercise : Study the existence and the uniqueness of the solutions of the initial value problem : $$y'' + p(x)y' + q(x)y = f(x)$$ $$y(x_0) = y_0, y'(x_0) = y_1$$ where $p(x),q(x),f(x)$ continuous functions in some domain $a<x<b$ with $x_0 \in (a,b)$. Now, I do not know how to handle this. I know that for a simple case : $y' = f(x,y)$, you can study and determine the existence of a solution in a domain $D = \{(x,y) \in \mathbb R^2:|y-y_0|\leq Œµ,|x-x_0| = \delta\}$ if the function $f(x,y)$ is continuous over $D$, while for the uniqueness, you just check if the function is bounded/Lipschitz . But on this case, that it involves multiple function that we do not know if they're $\neq 0$ and also a second-derivative, how should I approach ? What exactly shall I do to answer the question thoroughly ? Thanks in advance !","Exercise : Study the existence and the uniqueness of the solutions of the initial value problem : $$y'' + p(x)y' + q(x)y = f(x)$$ $$y(x_0) = y_0, y'(x_0) = y_1$$ where $p(x),q(x),f(x)$ continuous functions in some domain $a<x<b$ with $x_0 \in (a,b)$. Now, I do not know how to handle this. I know that for a simple case : $y' = f(x,y)$, you can study and determine the existence of a solution in a domain $D = \{(x,y) \in \mathbb R^2:|y-y_0|\leq Œµ,|x-x_0| = \delta\}$ if the function $f(x,y)$ is continuous over $D$, while for the uniqueness, you just check if the function is bounded/Lipschitz . But on this case, that it involves multiple function that we do not know if they're $\neq 0$ and also a second-derivative, how should I approach ? What exactly shall I do to answer the question thoroughly ? Thanks in advance !",,"['ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
75,Looking for error in my calculations,Looking for error in my calculations,,Solve the differential equation $ xy' - y = y^2 $ I'm trying to do it using Bernoulli substitution: $$ z = y^{-1} ;y = z^{-1} ; y^2 = z^{-2} ; y' = -z^{-2}z' $$ $$ - xz' - z = 1 $$ $$ - xz' - z = 0 $$ $$ z' = - \frac{z}{x} $$ $$ \int \frac{1}{z} dz = - \int \frac{1}{x}dx $$ $$ z = \frac{1}{xC(x)} $$ $$z' = \frac{-xC(x) +xC'(x)}{x^2 C^2(x)} $$ Substituting to $ - xz' - z = 1 $ $$ -x \frac{-C(x) - xC'(x)}{x^2C^2(x)} - \frac{1}{xC(x)} = 1 $$ $$ \frac{C'(x)}{C^2(x)} = 1 $$ $$ \frac{-C'(x)}{C^2(x)} = -1 $$ $$ (\frac{1}{C(x)})' = -1 $$ $$ \frac{1}{C(x)} = -\int1dx$$ $$ \frac{1}{C(x)} = -x+c$$ $$C(x) = \frac{1}{-x+c}$$ $$z=\frac{c-x}{x}$$ $$ y = \frac{x}{c-x}$$ than when I type it to Wolfram I get $ y(x) = \frac{-(e^{c_1} x)}{(e^{c_1} x - 1)} $ I don't know where I made a mistake.,Solve the differential equation $ xy' - y = y^2 $ I'm trying to do it using Bernoulli substitution: $$ z = y^{-1} ;y = z^{-1} ; y^2 = z^{-2} ; y' = -z^{-2}z' $$ $$ - xz' - z = 1 $$ $$ - xz' - z = 0 $$ $$ z' = - \frac{z}{x} $$ $$ \int \frac{1}{z} dz = - \int \frac{1}{x}dx $$ $$ z = \frac{1}{xC(x)} $$ $$z' = \frac{-xC(x) +xC'(x)}{x^2 C^2(x)} $$ Substituting to $ - xz' - z = 1 $ $$ -x \frac{-C(x) - xC'(x)}{x^2C^2(x)} - \frac{1}{xC(x)} = 1 $$ $$ \frac{C'(x)}{C^2(x)} = 1 $$ $$ \frac{-C'(x)}{C^2(x)} = -1 $$ $$ (\frac{1}{C(x)})' = -1 $$ $$ \frac{1}{C(x)} = -\int1dx$$ $$ \frac{1}{C(x)} = -x+c$$ $$C(x) = \frac{1}{-x+c}$$ $$z=\frac{c-x}{x}$$ $$ y = \frac{x}{c-x}$$ than when I type it to Wolfram I get $ y(x) = \frac{-(e^{c_1} x)}{(e^{c_1} x - 1)} $ I don't know where I made a mistake.,,['ordinary-differential-equations']
76,"Stability of the origin for $x'=y-x^3,y'=-x^3$",Stability of the origin for,"x'=y-x^3,y'=-x^3","The system $$\begin{align} x'&=y-x^3, \\ y'&=-x^3,  \end{align}$$ has a fixed point in the origin. The function  $$V(x,y):=x^4+2y^2 $$ is a Lyapunov function, since  $$\dot{V}=\frac{\mathrm{d}}{\mathrm{d} t} V(x(t),y(t))=-4x^6(t) \leq 0 .$$ It follows that solutions of the system travel through contour lines of $V$ (which resemble ellipses) in such a way that the value of $V$ is non-increasing. This proves that the origin is stable in the sense of Lyapunov. My question is: is it also asymptotically stable? I can't deduce it immediately since $\dot{V}=0$ on the $y$-axis (that is $\dot V$ is not negative definite). Thank you!","The system $$\begin{align} x'&=y-x^3, \\ y'&=-x^3,  \end{align}$$ has a fixed point in the origin. The function  $$V(x,y):=x^4+2y^2 $$ is a Lyapunov function, since  $$\dot{V}=\frac{\mathrm{d}}{\mathrm{d} t} V(x(t),y(t))=-4x^6(t) \leq 0 .$$ It follows that solutions of the system travel through contour lines of $V$ (which resemble ellipses) in such a way that the value of $V$ is non-increasing. This proves that the origin is stable in the sense of Lyapunov. My question is: is it also asymptotically stable? I can't deduce it immediately since $\dot{V}=0$ on the $y$-axis (that is $\dot V$ is not negative definite). Thank you!",,"['ordinary-differential-equations', 'nonlinear-system', 'stability-in-odes', 'stability-theory']"
77,Simple Sturm-Liouville problem without square-integrable solutions.,Simple Sturm-Liouville problem without square-integrable solutions.,,"I know how to solve basic Sturm-Liouville problems of the form $$ -(p f')' + q f = \lambda w f , $$ but the following singular SP-Problem, $$ - f''(x) + \log^2(x) f(x) = \frac{1}{x} f(x) \ , $$ with $p=1$, $q = \log^2$, $\lambda=1$ and $w=\frac 1 x$, defined on the positive half-line $\mathbb{R}_+$, with boundary values $ f(0) = f(\infty) = 0 $, seems to evade my knowledge. I have found the solution $$ f_1(x) = c_1 e^{-x\log(x) + x}, $$ without plugging in the boundary values just yet, and I know that there's a second linear independent one which may be found by the Spectral Parameter Power Series Method as $$ f_2(x) = c_2 f_1(x) \int_0^x \frac{1}{p(t)(f_1(t))^2}\ dt $$ but - contrary to Sturm-Liouville theory as I understand it - neither one is square integrable with respect to the measure $w dx = \frac{dx}{x}$, that is, $$ \int_{0}^\infty (-f''(x) + \log^2(x) f(x)) \overline{f(x)}\ dx =\int_{0}^\infty |f(x)|^2 \frac{dx}{x} $$  diverges, no matter the constants $c_1$, $c_2$. What's going on here - am I missing something fundamental? Aren't there any non-trivial solutions? Note that I also realize the parallels to the LHS of a Schr√∂dinger equation for a single particle in the potential $V = \log^2$, although the RHS of the equation is not a constant and thus cannot be interpreted as the energy of a system, but this does not contribute to my understanding.","I know how to solve basic Sturm-Liouville problems of the form $$ -(p f')' + q f = \lambda w f , $$ but the following singular SP-Problem, $$ - f''(x) + \log^2(x) f(x) = \frac{1}{x} f(x) \ , $$ with $p=1$, $q = \log^2$, $\lambda=1$ and $w=\frac 1 x$, defined on the positive half-line $\mathbb{R}_+$, with boundary values $ f(0) = f(\infty) = 0 $, seems to evade my knowledge. I have found the solution $$ f_1(x) = c_1 e^{-x\log(x) + x}, $$ without plugging in the boundary values just yet, and I know that there's a second linear independent one which may be found by the Spectral Parameter Power Series Method as $$ f_2(x) = c_2 f_1(x) \int_0^x \frac{1}{p(t)(f_1(t))^2}\ dt $$ but - contrary to Sturm-Liouville theory as I understand it - neither one is square integrable with respect to the measure $w dx = \frac{dx}{x}$, that is, $$ \int_{0}^\infty (-f''(x) + \log^2(x) f(x)) \overline{f(x)}\ dx =\int_{0}^\infty |f(x)|^2 \frac{dx}{x} $$  diverges, no matter the constants $c_1$, $c_2$. What's going on here - am I missing something fundamental? Aren't there any non-trivial solutions? Note that I also realize the parallels to the LHS of a Schr√∂dinger equation for a single particle in the potential $V = \log^2$, although the RHS of the equation is not a constant and thus cannot be interpreted as the energy of a system, but this does not contribute to my understanding.",,"['linear-algebra', 'ordinary-differential-equations', 'hilbert-spaces', 'sturm-liouville']"
78,Derivation of forward/backward/central difference methods from taylor series,Derivation of forward/backward/central difference methods from taylor series,,"I'm trying to learn more about finite difference methods here. Theorically using the taylor series $u(x)=\sum_{n=0}^{\infty}\frac{(x-x_i)^n}{n!}(\frac{d^nu}{dx^n})_i $ you can get the forward/backward/central difference methods, which are: forward: $(\frac{du}{dx})_i\approx\frac{u_{i+1}-u_i}{\Delta x}$ backward: $(\frac{du}{dx})_i\approx\frac{u_{i}-u_{i-1}}{\Delta x}$ central: $(\frac{du}{dx})_i\approx\frac{u_{i+1}-u_{i-1}}{2\Delta x}$ But I'm having a rough time trying to understand how the above taylor series is being expanded to obtain the difference methods. The fact of not having very clear how taylor works and that subindex notation is confusing me. In the lecture says that $u_i \approx\ {u(x_i)}$ and $x_i=i\Delta x$ Also, the lecture I'm following introduces FDM saying that: $\frac{\delta u}{\delta x}(x)=\lim_{x\to 0} \frac{u(x+\Delta x) - u(x)}{\Delta x} = \lim_{x\to 0} \frac{u(x)-u(x-\Delta x)}{\Delta x} = \lim_{x\to 0} \frac{u(x+\Delta x)-u(x-\Delta x)}{2\Delta x} $ But how can i prove those equations are equals, any idea?","I'm trying to learn more about finite difference methods here. Theorically using the taylor series $u(x)=\sum_{n=0}^{\infty}\frac{(x-x_i)^n}{n!}(\frac{d^nu}{dx^n})_i $ you can get the forward/backward/central difference methods, which are: forward: $(\frac{du}{dx})_i\approx\frac{u_{i+1}-u_i}{\Delta x}$ backward: $(\frac{du}{dx})_i\approx\frac{u_{i}-u_{i-1}}{\Delta x}$ central: $(\frac{du}{dx})_i\approx\frac{u_{i+1}-u_{i-1}}{2\Delta x}$ But I'm having a rough time trying to understand how the above taylor series is being expanded to obtain the difference methods. The fact of not having very clear how taylor works and that subindex notation is confusing me. In the lecture says that $u_i \approx\ {u(x_i)}$ and $x_i=i\Delta x$ Also, the lecture I'm following introduces FDM saying that: $\frac{\delta u}{\delta x}(x)=\lim_{x\to 0} \frac{u(x+\Delta x) - u(x)}{\Delta x} = \lim_{x\to 0} \frac{u(x)-u(x-\Delta x)}{\Delta x} = \lim_{x\to 0} \frac{u(x+\Delta x)-u(x-\Delta x)}{2\Delta x} $ But how can i prove those equations are equals, any idea?",,"['ordinary-differential-equations', 'numerical-methods', 'taylor-expansion']"
79,What is general solution of the PDE $(x^2+y^2)u_x+2xyu_y=-u^2$?,What is general solution of the PDE ?,(x^2+y^2)u_x+2xyu_y=-u^2,"What is the general solution of the following PDE? $$(x^2+y^2)u_x+2xyu_y=-u^2.$$ If we write the characteristic equation $$\frac{dx}{x^2+y^2}=\frac{dy}{2xy}=\frac{-du}{u^2}$$ then, we find $\frac{d(x+y)}{(x+y)^2}=\frac{-du}{u^2}$ and we have $c_1=1/u+1/(x+y)$. And then? Best regards.","What is the general solution of the following PDE? $$(x^2+y^2)u_x+2xyu_y=-u^2.$$ If we write the characteristic equation $$\frac{dx}{x^2+y^2}=\frac{dy}{2xy}=\frac{-du}{u^2}$$ then, we find $\frac{d(x+y)}{(x+y)^2}=\frac{-du}{u^2}$ and we have $c_1=1/u+1/(x+y)$. And then? Best regards.",,"['ordinary-differential-equations', 'partial-differential-equations']"
80,Definition of fixed points?,Definition of fixed points?,,"I encountered fixed points in various theories like in case of real analysis while studying about fixed points, suppose $f(x)$ is a real-valued function then fixed points of $f$ are given by $x$ satisfying $f(x)  = x$. While considering Dynamical systems for 'flows' where say for one-dimensional case, the governing equation is $\dot{x} = f(x)$, in that case the fixed points are given by $\dot{x} = 0$ or $f(x) = 0$. Are the two definitions equivalent in some way? If so how are they related?","I encountered fixed points in various theories like in case of real analysis while studying about fixed points, suppose $f(x)$ is a real-valued function then fixed points of $f$ are given by $x$ satisfying $f(x)  = x$. While considering Dynamical systems for 'flows' where say for one-dimensional case, the governing equation is $\dot{x} = f(x)$, in that case the fixed points are given by $\dot{x} = 0$ or $f(x) = 0$. Are the two definitions equivalent in some way? If so how are they related?",,"['real-analysis', 'ordinary-differential-equations', 'definition', 'dynamical-systems', 'fixed-points']"
81,When does the Picard iteration give a power series?,When does the Picard iteration give a power series?,,"I been trying to figure out when or if it is possible to see that the Picard i.e  iteration $x_{k+1}=x_{0}+\int f(t,x_{k}(t))dt$ leads to a power series expansion of the solution $x$. In simple and concrete cases I can see it. But it would be nice to be able to think about this iteration as a general way of obtaning a power series solution theoretically.","I been trying to figure out when or if it is possible to see that the Picard i.e  iteration $x_{k+1}=x_{0}+\int f(t,x_{k}(t))dt$ leads to a power series expansion of the solution $x$. In simple and concrete cases I can see it. But it would be nice to be able to think about this iteration as a general way of obtaning a power series solution theoretically.",,['ordinary-differential-equations']
82,"Proving that $F(x)=\int_{a}^{x}f(x,y)dy$ is continuously differentiable",Proving that  is continuously differentiable,"F(x)=\int_{a}^{x}f(x,y)dy","I know that $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is continuous, $f$ is differentiable in $x$ with $D_{1}f:\mathbb{R}^{2}\rightarrow \mathbb{R}$ continuous and I know that:    $F(x)=\int_{a}^{x}f(x,y)dy, x\in \mathbb{R}$ . I now want to prove that $F$ is continously differentiable (thus that $F$ is differentiable and that $F'(x)$ is continuous) I also need to show that: $$F'(x)=f(x,x)+\int_{a}^{x}\frac{\partial f(x,y)}{\partial x}dy, x \in \mathbb{R}$$. I know that there exists a $G$ such that: $\int_{a}^{x}f(x,y)dy=G(x)-G(a)$ with $G'(x)=f(x,y)$,  and that $\frac{d}{dx}\int_{a}^{b}f(x,y)dy=\int_{a}^{b}\frac{\partial f(x,y)}{\partial x}dy$, that's the furthest I got with this problem can anyone help me solve this?","I know that $f:\mathbb{R}^2\rightarrow\mathbb{R}$ is continuous, $f$ is differentiable in $x$ with $D_{1}f:\mathbb{R}^{2}\rightarrow \mathbb{R}$ continuous and I know that:    $F(x)=\int_{a}^{x}f(x,y)dy, x\in \mathbb{R}$ . I now want to prove that $F$ is continously differentiable (thus that $F$ is differentiable and that $F'(x)$ is continuous) I also need to show that: $$F'(x)=f(x,x)+\int_{a}^{x}\frac{\partial f(x,y)}{\partial x}dy, x \in \mathbb{R}$$. I know that there exists a $G$ such that: $\int_{a}^{x}f(x,y)dy=G(x)-G(a)$ with $G'(x)=f(x,y)$,  and that $\frac{d}{dx}\int_{a}^{b}f(x,y)dy=\int_{a}^{b}\frac{\partial f(x,y)}{\partial x}dy$, that's the furthest I got with this problem can anyone help me solve this?",,"['integration', 'ordinary-differential-equations']"
83,solution of a ODE $dy/dx = ((y+a)(y+b) + x)/(y+b)^2$,solution of a ODE,dy/dx = ((y+a)(y+b) + x)/(y+b)^2,"In my work, I derived an ODE of the following form $\frac{dy}{dx} = \frac{(y+a)(y+b) + x}{(y+b)^2}$ where $a > 0$ and $b > 0$ are constants. I wonder if there is an analytical solution of this equation? Anyone can give a clue on how to solve it?","In my work, I derived an ODE of the following form $\frac{dy}{dx} = \frac{(y+a)(y+b) + x}{(y+b)^2}$ where $a > 0$ and $b > 0$ are constants. I wonder if there is an analytical solution of this equation? Anyone can give a clue on how to solve it?",,['ordinary-differential-equations']
84,Determining rates in an ODE,Determining rates in an ODE,,"I am building a model with a system of ODEs for the interaction of bacteria with human cells. I want to determine the rate parameters in the model and in the literature I found that the maximum number of bacteria that can get attached to a single cell is¬†20 and this maximum number will be reached within 4¬†hours.  I want to know how I can use these information to come up with the rates that I want. In the model I have a rate with which unattached bacteria gets attached. As it takes 4¬†hours for 20¬†bacteria to attach, can I take the rate with which a bacterium moves to the attached state to be 4/20? Then I have another rate, namely the rate with which cells move into the infected cell stage. This happens when a cell can no longer attach any more bacteria (having reached its maximum capacity for attachment) is reached. So, can I take it to be 1/4¬†hours? My model is somewhat similar to the model found here (the rate with which cells become infected in my model is modelled similarly to the rate $\omega$; $P$¬†is the bacteria, $E_A$¬†is the uninfected cells, $E_U$ is the infected cells):","I am building a model with a system of ODEs for the interaction of bacteria with human cells. I want to determine the rate parameters in the model and in the literature I found that the maximum number of bacteria that can get attached to a single cell is¬†20 and this maximum number will be reached within 4¬†hours.  I want to know how I can use these information to come up with the rates that I want. In the model I have a rate with which unattached bacteria gets attached. As it takes 4¬†hours for 20¬†bacteria to attach, can I take the rate with which a bacterium moves to the attached state to be 4/20? Then I have another rate, namely the rate with which cells move into the infected cell stage. This happens when a cell can no longer attach any more bacteria (having reached its maximum capacity for attachment) is reached. So, can I take it to be 1/4¬†hours? My model is somewhat similar to the model found here (the rate with which cells become infected in my model is modelled similarly to the rate $\omega$; $P$¬†is the bacteria, $E_A$¬†is the uninfected cells, $E_U$ is the infected cells):",,"['ordinary-differential-equations', 'dynamical-systems', 'mathematical-modeling', 'biology']"
85,Integrate an Ordinary Differential Equation [closed],Integrate an Ordinary Differential Equation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I found this from a derivation from a Dimensional Analysis book. It starts from the following equation: $$\frac{d^2(F(x))^2)}{dx^2}+\frac{x}{3}\frac{dF(x)}{dx}+\frac{F(x)}{3}=0$$ where $F(x)$ is a function of $x$ This ODE is then integrated with respect to $x$, and it got $$\frac{dF(x)^2)}{dx}+\frac{x\cdot F}{3}= \text{const.}$$ where $const.$ should be some terms which the book chooses not to show I stared at this derivation for a long time, but I could not figure out how this integration makes sense. Please forgive my stupidity if it turns out to be something simple.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I found this from a derivation from a Dimensional Analysis book. It starts from the following equation: $$\frac{d^2(F(x))^2)}{dx^2}+\frac{x}{3}\frac{dF(x)}{dx}+\frac{F(x)}{3}=0$$ where $F(x)$ is a function of $x$ This ODE is then integrated with respect to $x$, and it got $$\frac{dF(x)^2)}{dx}+\frac{x\cdot F}{3}= \text{const.}$$ where $const.$ should be some terms which the book chooses not to show I stared at this derivation for a long time, but I could not figure out how this integration makes sense. Please forgive my stupidity if it turns out to be something simple.",,"['calculus', 'ordinary-differential-equations']"
86,First order ODE - Solve $dy/dx=\sin(x+2y)+\cos(x+2y)$,First order ODE - Solve,dy/dx=\sin(x+2y)+\cos(x+2y),"I am taking an online course on IIT's MOOC site https://onlinecourses.nptel.ac.in/noc17_ma11/ on Ordinary Differential equations. One of the questions in the assignment is the following : Solve the differential equation $$\frac{dy}{dx}=\sin(x+2y)+\cos(x+2y)$$ I am having some trouble attempting this question. Here are my quick thoughts: This is not in the separable form. Substituting $y=ux$ or $x=vy$ does not yield a homogenous function. The coefficient of $dx$ is not linear. This is not an exact differential equation. $\partial{P}/\partial(y)=2\cos(x+2y)-2\sin(x+2y)$ and $\partial{Q}/\partial(x)=0$ Obviously, this must have an integrating factor then. Are my initial thoughts correct? Is that the correct way to proceed? I tried googling, but didn't find much luck. Any hints in the right direction would be great.","I am taking an online course on IIT's MOOC site https://onlinecourses.nptel.ac.in/noc17_ma11/ on Ordinary Differential equations. One of the questions in the assignment is the following : Solve the differential equation I am having some trouble attempting this question. Here are my quick thoughts: This is not in the separable form. Substituting or does not yield a homogenous function. The coefficient of is not linear. This is not an exact differential equation. and Obviously, this must have an integrating factor then. Are my initial thoughts correct? Is that the correct way to proceed? I tried googling, but didn't find much luck. Any hints in the right direction would be great.",\frac{dy}{dx}=\sin(x+2y)+\cos(x+2y) y=ux x=vy dx \partial{P}/\partial(y)=2\cos(x+2y)-2\sin(x+2y) \partial{Q}/\partial(x)=0,['ordinary-differential-equations']
87,"Conversion of BVP $y'' + y' + y = 0 ,y(0) = 0, y(1) = 1$ to Integral equation.",Conversion of BVP  to Integral equation.,"y'' + y' + y = 0 ,y(0) = 0, y(1) = 1","I was trying to convert this Boundary Value problem to an integral equation(Fredholm Integral Equation) $y'' + y' + y = 0 ,y(0) = 0, y(1) = 1       \rightarrow (*)$ I tried but got stuck,here  Integrating the above ODE we get $\int_{0}^{x}y''(x) dx + \int_{0}^{x}y'(x) dx + \int_{0}^{x}y dx = c$ $y'(x) - y'(0)+y(x)- y(0) + \int_{0}^{x}y dx = c$ at $x = 1$ $y'(1) - y'(0) + y(1) - y(0) + \int_{0}^{1}y dx = c$ So $c = y'(1) - y'(0) + 1 + \int_{0}^{1}y dx$ Next substituting this value of $c$ in $(*)$,we get $y(x) = -\int_{0}^{x}y dx - \int_{0}^{x}(x-t)y(t)dt + xy'(1) + x + x\int_{0}^{1}y dx$ I think I am making mistake somewhere as the form is $y = \int_{0}^{1} K(x,t)y(t)dt$, any help!","I was trying to convert this Boundary Value problem to an integral equation(Fredholm Integral Equation) $y'' + y' + y = 0 ,y(0) = 0, y(1) = 1       \rightarrow (*)$ I tried but got stuck,here  Integrating the above ODE we get $\int_{0}^{x}y''(x) dx + \int_{0}^{x}y'(x) dx + \int_{0}^{x}y dx = c$ $y'(x) - y'(0)+y(x)- y(0) + \int_{0}^{x}y dx = c$ at $x = 1$ $y'(1) - y'(0) + y(1) - y(0) + \int_{0}^{1}y dx = c$ So $c = y'(1) - y'(0) + 1 + \int_{0}^{1}y dx$ Next substituting this value of $c$ in $(*)$,we get $y(x) = -\int_{0}^{x}y dx - \int_{0}^{x}(x-t)y(t)dt + xy'(1) + x + x\int_{0}^{1}y dx$ I think I am making mistake somewhere as the form is $y = \int_{0}^{1} K(x,t)y(t)dt$, any help!",,"['ordinary-differential-equations', 'boundary-value-problem', 'integral-equations']"
88,Using Jacobi Elliptic Functions for the solution of an ODE,Using Jacobi Elliptic Functions for the solution of an ODE,,"I need to solve the following ODE: $$ Ay''-By^3+Cy=0 $$ where $A,B,C \in \mathbb{R}_{+}$ (i.e.- positive constants). After reading a little bit about the properties of the Jacobi sn function, I think the solution should contain sn in it, but have no idea how to arrive to it. Will you please help me understand how to formally obtain the solution for this ode? In addition, is there any book that contains solved examples for odes with Jacobi functions as solutions? Thanks !","I need to solve the following ODE: $$ Ay''-By^3+Cy=0 $$ where $A,B,C \in \mathbb{R}_{+}$ (i.e.- positive constants). After reading a little bit about the properties of the Jacobi sn function, I think the solution should contain sn in it, but have no idea how to arrive to it. Will you please help me understand how to formally obtain the solution for this ode? In addition, is there any book that contains solved examples for odes with Jacobi functions as solutions? Thanks !",,['ordinary-differential-equations']
89,Does global Lyapunov stability imply unique equilibrium?,Does global Lyapunov stability imply unique equilibrium?,,"Recall that given a time-invariant dynamical system $$\dot x = f(x)$$ We say that an equilibrium point at the origin, $x_e \in \mathbb{R}^n$, of the above system is stable (in the sense of Lyapunov) if: $$\forall \epsilon > 0, \exists \delta > 0, \text{ s.t. } \|x(t_0) - x_e\| < \delta \implies \|x(t) - x_e\|< \epsilon, \forall t \geq t_0$$ Suppose that the above condition holds for all $x_0 = x(t_0) \in \mathbb{R}^n$, then we can say that the equilibrium point is globally stable. Suppose that $x_e$ is globally stable, then is it the unique equilibrium of $\dot x = f(x)$? Note that global asymptotic stability implies uniqueness because every trajectory has to converge to that point.","Recall that given a time-invariant dynamical system $$\dot x = f(x)$$ We say that an equilibrium point at the origin, $x_e \in \mathbb{R}^n$, of the above system is stable (in the sense of Lyapunov) if: $$\forall \epsilon > 0, \exists \delta > 0, \text{ s.t. } \|x(t_0) - x_e\| < \delta \implies \|x(t) - x_e\|< \epsilon, \forall t \geq t_0$$ Suppose that the above condition holds for all $x_0 = x(t_0) \in \mathbb{R}^n$, then we can say that the equilibrium point is globally stable. Suppose that $x_e$ is globally stable, then is it the unique equilibrium of $\dot x = f(x)$? Note that global asymptotic stability implies uniqueness because every trajectory has to converge to that point.",,"['ordinary-differential-equations', 'definition', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
90,$\lim_{t \to \infty} \vert \phi(t) \vert = \lim_{t \to \infty} \vert \ \ddot \phi(t) \vert =0$; does $\vert \dot \phi(t) \vert \to 0$ as well?,; does  as well?,\lim_{t \to \infty} \vert \phi(t) \vert = \lim_{t \to \infty} \vert \ \ddot \phi(t) \vert =0 \vert \dot \phi(t) \vert \to 0,"This question has repeatedly appeared to me whilst studying certain linear differential equations with time-dependent coefficients. Let $\phi(t) \in C^2(\Bbb R, \Bbb C)$; that is, $\phi(t)$ is a twice continuously differentialble complex valued function on the real line $\Bbb R$.  Assume that $\lim_{t \to \infty} \vert \phi(t) \vert = 0, \tag{1}$ and $\lim_{t \to \infty} \vert \ddot \phi(t) \vert \to 0 \tag{2}$ as well.  Then must we also have $\lim_{t \to \infty} \vert \dot \phi(t) \vert \to 0? \tag{3}$ An answer to the above question as such would be most appreciated.  There are, however, two more restricted questions the answers to which would suffice for my purposes: I.) Suppose instead of (2) we assume the existence of a globally bounded, non-negative real function $b: \Bbb R \to \Bbb R_{\ge 0}$ such that $\vert \ddot \phi(t) \vert \le b(t) \vert \phi(t) \vert \tag{4}$ for sufficiently large $t$.  Is this hypothesis, in concert with (1), sufficient to force (3)?  Readers should feel free to add various hypotheses on $b(t)$ if they desire, such as $b(t) \in C^k(\Bbb R, \Bbb R_{\ge 0})$ for some $k\ge 0$, or that $b(t)$ exhibits some specific functional behavior, e.g. $b(t) = e^{-t}$ for sufficiently large $t$. I am particularly interested in the case $b(t) = B > 0$ a constant, so that $\vert \ddot \phi(t) \vert \le B \vert \phi(t) \vert. \tag{5}$ 2.) Suppose $c(t) \in C^k(\Bbb R, \Bbb C)$, $\Vert c(t) \Vert_k < \infty$, and $\ddot \phi(t) + c(t)\phi(t) = 0; \tag{6}$ then the hypothesis $\lim_{t \to \infty} \vert \phi(t) \vert \to 0$ clearly implies (2); can we now show $\lim_{t \to \infty} \vert \dot \phi(t) \vert \to 0$? Of particular interest to me is the case $k = 0$, that is, $c(t):\Bbb R \to \Bbb C$ is a bounded, continuous function. It is clear that these questions are, more or less, in order of decreasing generality: (2) is a case of (1) , itself a case of most widely scoped question stated at the beginning. My own efforts on this problem focused primarily on case (2.) and equation (6).  I looked a several things; for instance, (6) implies $\dot \phi \ddot \phi(t) + c(t)\phi(t) \dot \phi(t) = 0, \tag{7}$ or $\dfrac{1}{2} \dfrac{d(\dot \phi(t))^2}{dt} + c(t) \dfrac{1}{2} \dfrac{d (\phi(t))^2}{dt} = 0 \tag{8}$ or $\dfrac{d(\dot \phi(t))^2}{dt} + c(t)\dfrac{d (\phi(t))^2}{dt} = 0, \tag{9}$ which leads to an integral relationship $( (\dot \phi(t))^2 - (\dot \phi(t_0))^2 + \displaystyle \int_{t_0}^t c(s)\dfrac{d (\phi(s))^2}{ds} = E, \tag{10}$ a constant.  I think perhaps (10) might be used to show $\dot \phi(t)$ becomes small for large $t$, since $\phi(t)$ does; but I haven't found a conclusive argument along these lines as of this writing. I also tried looking at (1) and (2) directly, hoping to show that if $\ddot \phi(t)$ became very small, so that $\dot \phi(t)$ couldn't change too much, it ($\dot \phi$) would have to remain relatively small, lest $\phi(t)$ itself grow in a way not permitted by (1); but these are at the present more intuitive speculations rather than rigorous results. I'm hoping someone can help me fill in the gaps . . . any insights offered will be seriously considered and appreciated, whether or not they provide a complete solution.","This question has repeatedly appeared to me whilst studying certain linear differential equations with time-dependent coefficients. Let $\phi(t) \in C^2(\Bbb R, \Bbb C)$; that is, $\phi(t)$ is a twice continuously differentialble complex valued function on the real line $\Bbb R$.  Assume that $\lim_{t \to \infty} \vert \phi(t) \vert = 0, \tag{1}$ and $\lim_{t \to \infty} \vert \ddot \phi(t) \vert \to 0 \tag{2}$ as well.  Then must we also have $\lim_{t \to \infty} \vert \dot \phi(t) \vert \to 0? \tag{3}$ An answer to the above question as such would be most appreciated.  There are, however, two more restricted questions the answers to which would suffice for my purposes: I.) Suppose instead of (2) we assume the existence of a globally bounded, non-negative real function $b: \Bbb R \to \Bbb R_{\ge 0}$ such that $\vert \ddot \phi(t) \vert \le b(t) \vert \phi(t) \vert \tag{4}$ for sufficiently large $t$.  Is this hypothesis, in concert with (1), sufficient to force (3)?  Readers should feel free to add various hypotheses on $b(t)$ if they desire, such as $b(t) \in C^k(\Bbb R, \Bbb R_{\ge 0})$ for some $k\ge 0$, or that $b(t)$ exhibits some specific functional behavior, e.g. $b(t) = e^{-t}$ for sufficiently large $t$. I am particularly interested in the case $b(t) = B > 0$ a constant, so that $\vert \ddot \phi(t) \vert \le B \vert \phi(t) \vert. \tag{5}$ 2.) Suppose $c(t) \in C^k(\Bbb R, \Bbb C)$, $\Vert c(t) \Vert_k < \infty$, and $\ddot \phi(t) + c(t)\phi(t) = 0; \tag{6}$ then the hypothesis $\lim_{t \to \infty} \vert \phi(t) \vert \to 0$ clearly implies (2); can we now show $\lim_{t \to \infty} \vert \dot \phi(t) \vert \to 0$? Of particular interest to me is the case $k = 0$, that is, $c(t):\Bbb R \to \Bbb C$ is a bounded, continuous function. It is clear that these questions are, more or less, in order of decreasing generality: (2) is a case of (1) , itself a case of most widely scoped question stated at the beginning. My own efforts on this problem focused primarily on case (2.) and equation (6).  I looked a several things; for instance, (6) implies $\dot \phi \ddot \phi(t) + c(t)\phi(t) \dot \phi(t) = 0, \tag{7}$ or $\dfrac{1}{2} \dfrac{d(\dot \phi(t))^2}{dt} + c(t) \dfrac{1}{2} \dfrac{d (\phi(t))^2}{dt} = 0 \tag{8}$ or $\dfrac{d(\dot \phi(t))^2}{dt} + c(t)\dfrac{d (\phi(t))^2}{dt} = 0, \tag{9}$ which leads to an integral relationship $( (\dot \phi(t))^2 - (\dot \phi(t_0))^2 + \displaystyle \int_{t_0}^t c(s)\dfrac{d (\phi(s))^2}{ds} = E, \tag{10}$ a constant.  I think perhaps (10) might be used to show $\dot \phi(t)$ becomes small for large $t$, since $\phi(t)$ does; but I haven't found a conclusive argument along these lines as of this writing. I also tried looking at (1) and (2) directly, hoping to show that if $\ddot \phi(t)$ became very small, so that $\dot \phi(t)$ couldn't change too much, it ($\dot \phi$) would have to remain relatively small, lest $\phi(t)$ itself grow in a way not permitted by (1); but these are at the present more intuitive speculations rather than rigorous results. I'm hoping someone can help me fill in the gaps . . . any insights offered will be seriously considered and appreciated, whether or not they provide a complete solution.",,"['real-analysis', 'ordinary-differential-equations']"
91,Linear vs Non-Linear,Linear vs Non-Linear,,I am attempting some practice problems and I am not sure how to tell if a differential equation is linear or non linear. The first problem: $$5\left(\frac{dx}{dt}\right)+5x^2+3=0$$ This is non Linear. Could someone explain that to me? The second was $$y^3\left(\frac{d^2x}{dy^2}\right)+3x -\left(\frac{8}{y-1}\right)=0$$ This one is linear. Could someone explain that to me? I thought the $\frac{8}{y-1}$ would make it non-linear.,I am attempting some practice problems and I am not sure how to tell if a differential equation is linear or non linear. The first problem: $$5\left(\frac{dx}{dt}\right)+5x^2+3=0$$ This is non Linear. Could someone explain that to me? The second was $$y^3\left(\frac{d^2x}{dy^2}\right)+3x -\left(\frac{8}{y-1}\right)=0$$ This one is linear. Could someone explain that to me? I thought the $\frac{8}{y-1}$ would make it non-linear.,,['ordinary-differential-equations']
92,Solution to second order second degree equation $ x^2 (d^2y/dx^2)^2=(dy/dx)^2+1$.,Solution to second order second degree equation ., x^2 (d^2y/dx^2)^2=(dy/dx)^2+1,I am unawae of any method that is there to solve such an equation. IT is totaly different from the usual linear differential equation of degree 2. Any Hint as to how to solve.,I am unawae of any method that is there to solve such an equation. IT is totaly different from the usual linear differential equation of degree 2. Any Hint as to how to solve.,,['ordinary-differential-equations']
93,Check Solution: $x^2y''+xy'-y=x^2e^x$ via Variation of Parameters,Check Solution:  via Variation of Parameters,x^2y''+xy'-y=x^2e^x,"I was revisiting differential equations and I came across the following question: By method of variation of parameters, the particular solution of the equation $$x^2y''+xy'-y=x^2e^x$$is: $(a) \ e^x+\frac1x$ $(b)\ e^x- \frac{e^x}x$ $(c)\ e^x+\frac{e^x}x$ $(d)\ e^x-\frac1x$ My attempt: Let $x=e^z$. Then the differential equation becomes: $$\frac{d^2y}{dz^2}-y=e^{2z}e^{e^z}$$   Solution of the homogenous equation is :$y_c=c_1 e^z + c_2 e^{-z}$ where $c_1$ and $c_2$ are arbitrary constants. Thus, solution of original homogenous equation is $y_c=c_1x+c_2\frac1x$. I calculated the Wronskian which came out to be $\frac{-2}{x}$. Therefore, particular solution is    $$y_p=-x\int \frac{x^2e^x}{-2}+\frac1x \int \frac{x^4e^x}{-2}=\frac x2 \int x^2e^x-\frac{1}{2x} \int x^4 e^x $$ On simplifying : $$-\frac32 x^2 e^x-\frac{11}2 xe^x +12e^x\left(1-\frac1x\right)$$ Where have I gone wrong? I realized that even if I made mistake right in the end while simplifying there will be a term involving $x^2$ and none of the four options have any such term. The answer given is $(b)$.","I was revisiting differential equations and I came across the following question: By method of variation of parameters, the particular solution of the equation $$x^2y''+xy'-y=x^2e^x$$is: $(a) \ e^x+\frac1x$ $(b)\ e^x- \frac{e^x}x$ $(c)\ e^x+\frac{e^x}x$ $(d)\ e^x-\frac1x$ My attempt: Let $x=e^z$. Then the differential equation becomes: $$\frac{d^2y}{dz^2}-y=e^{2z}e^{e^z}$$   Solution of the homogenous equation is :$y_c=c_1 e^z + c_2 e^{-z}$ where $c_1$ and $c_2$ are arbitrary constants. Thus, solution of original homogenous equation is $y_c=c_1x+c_2\frac1x$. I calculated the Wronskian which came out to be $\frac{-2}{x}$. Therefore, particular solution is    $$y_p=-x\int \frac{x^2e^x}{-2}+\frac1x \int \frac{x^4e^x}{-2}=\frac x2 \int x^2e^x-\frac{1}{2x} \int x^4 e^x $$ On simplifying : $$-\frac32 x^2 e^x-\frac{11}2 xe^x +12e^x\left(1-\frac1x\right)$$ Where have I gone wrong? I realized that even if I made mistake right in the end while simplifying there will be a term involving $x^2$ and none of the four options have any such term. The answer given is $(b)$.",,"['ordinary-differential-equations', 'proof-verification']"
94,Finding the invariant lines of a vector field,Finding the invariant lines of a vector field,,"How do I find the invariant lines of the following system: $x' = x(1-x+y)$ $y' = y(1-3x-y)$ The fixed points of the system are $\{(0,0),(0,1),(1,0),(0.5,-0.5)\}$ I know there are 4 lines. The invariant lines $x = 0$ and $y = 0$ are trivial. How do I find the remaining 2?","How do I find the invariant lines of the following system: $x' = x(1-x+y)$ $y' = y(1-3x-y)$ The fixed points of the system are $\{(0,0),(0,1),(1,0),(0.5,-0.5)\}$ I know there are 4 lines. The invariant lines $x = 0$ and $y = 0$ are trivial. How do I find the remaining 2?",,"['ordinary-differential-equations', 'dynamical-systems']"
95,How to solve: $z''=-c/z^4$,How to solve:,z''=-c/z^4,"I would like tho solve the following nonlinear second order differential equation $\frac{d^2{z}}{d{t^{2}}}=-\frac{c}{z^4}$ Where $C$ is a constant. I have try to use this steps http://www.sosmath.com/diffeq/second/nonlineareq/nonlineareq.html with no success. Also, I've tryed to use the solver in Matlab, and they give me an error. I don't think that this ODE is impossible to solve analytically. Thank you in advance","I would like tho solve the following nonlinear second order differential equation $\frac{d^2{z}}{d{t^{2}}}=-\frac{c}{z^4}$ Where $C$ is a constant. I have try to use this steps http://www.sosmath.com/diffeq/second/nonlineareq/nonlineareq.html with no success. Also, I've tryed to use the solver in Matlab, and they give me an error. I don't think that this ODE is impossible to solve analytically. Thank you in advance",,['ordinary-differential-equations']
96,Meaning of a bifurcation? Meaning of bifurcation diagrams?,Meaning of a bifurcation? Meaning of bifurcation diagrams?,,"So it seems like in the literature there are two separate meanings for what a bifurcation is. One meaning is that a bifurcation is the point at which an equilibrium changes behavior. The other meaning is that a bifurcation is when an equilibrium point splits into two. Now here is my confusion. I am trying to study the dimensionless generalized version of Chua Circuit Equations $$\dot{x}=a(y-\phi(x))$$$$\dot{y}=x-y+z$$ $$\dot{z}=-by$$ $$\phi(x) = \frac{1}{16}x^3 - \frac{1}{6}$$ and I'm supposed to study bifurcations in this context using $a$ as bifurcation parameter. Now the equilibria in this case are not dependent on the bifurcation parameter $a$. So in this context the equilibria might change behavior but they will not change position or split as a function of $a$. So what is a bifurcation exactly? Is it a splitting, moving of equilibria or it is a change in behaviour of equilibrium points? Or both? Also with regards to bifurcation diagrams, I came across the bifurcation diagram in this paper: https://pdfs.semanticscholar.org/5a3c/fa3a66da70f9b6af8510ec7abdeca443fdbb.pdf This is a bifurcation diagram of Chua Equations using the parameter $a$. I don't understand why the equilibrium points are moving and splitting since they aren't a function of $a$? I don't really understand what is going on in this diagram.","So it seems like in the literature there are two separate meanings for what a bifurcation is. One meaning is that a bifurcation is the point at which an equilibrium changes behavior. The other meaning is that a bifurcation is when an equilibrium point splits into two. Now here is my confusion. I am trying to study the dimensionless generalized version of Chua Circuit Equations $$\dot{x}=a(y-\phi(x))$$$$\dot{y}=x-y+z$$ $$\dot{z}=-by$$ $$\phi(x) = \frac{1}{16}x^3 - \frac{1}{6}$$ and I'm supposed to study bifurcations in this context using $a$ as bifurcation parameter. Now the equilibria in this case are not dependent on the bifurcation parameter $a$. So in this context the equilibria might change behavior but they will not change position or split as a function of $a$. So what is a bifurcation exactly? Is it a splitting, moving of equilibria or it is a change in behaviour of equilibrium points? Or both? Also with regards to bifurcation diagrams, I came across the bifurcation diagram in this paper: https://pdfs.semanticscholar.org/5a3c/fa3a66da70f9b6af8510ec7abdeca443fdbb.pdf This is a bifurcation diagram of Chua Equations using the parameter $a$. I don't understand why the equilibrium points are moving and splitting since they aren't a function of $a$? I don't really understand what is going on in this diagram.",,"['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
97,Performing Automatic Differentiation by way of Simple Example,Performing Automatic Differentiation by way of Simple Example,,"I understand the theory of symbolic and numerical differentiation pretty well, however am struggling on understanding automatic differentiation. I have read the following example in documentation and struggle with how certain steps are performed and require clarity. By way of example, say we want to find the solution (using automatic differentiation) of $f'(3)$ where $f(x)$ is equal to: $$f(x) = \frac{(x+1)(x-2)}{x+3}$$ My understanding is to use automatic differentiation method, prior to solving for $f'(x)$, we must understand two things: 1) We must work with the concept of ""value pairs"" when evaluating expressions i.e. $$\vec{u} = (u, u')$$ where u denotes the value of the function at a point $x_{0}$ 2) For the purposes of this simple example, understand the two symbolic differentiation rules: $$\vec{x} = (x, 1)$$  $$\vec{c} = (c, 0)$$ Now using the rules above we evaluate as follow's: Example In that example, how did they get the value of 5 in the expression: $\frac{(4, 5)}{(6, 1)}$ ? and how did they get a result of 13/18?","I understand the theory of symbolic and numerical differentiation pretty well, however am struggling on understanding automatic differentiation. I have read the following example in documentation and struggle with how certain steps are performed and require clarity. By way of example, say we want to find the solution (using automatic differentiation) of $f'(3)$ where $f(x)$ is equal to: $$f(x) = \frac{(x+1)(x-2)}{x+3}$$ My understanding is to use automatic differentiation method, prior to solving for $f'(x)$, we must understand two things: 1) We must work with the concept of ""value pairs"" when evaluating expressions i.e. $$\vec{u} = (u, u')$$ where u denotes the value of the function at a point $x_{0}$ 2) For the purposes of this simple example, understand the two symbolic differentiation rules: $$\vec{x} = (x, 1)$$  $$\vec{c} = (c, 0)$$ Now using the rules above we evaluate as follow's: Example In that example, how did they get the value of 5 in the expression: $\frac{(4, 5)}{(6, 1)}$ ? and how did they get a result of 13/18?",,"['ordinary-differential-equations', 'derivatives']"
98,Absolute convergence of Dyson series for evolution operator,Absolute convergence of Dyson series for evolution operator,,"In this paper https://arxiv.org/abs/math-ph/9901018 the author (on page 4) mentions : ""...a standard argument for the absolute convergence of the Dyson series for [the evolution operator] $\hat{U}_{\tau}$ ..."" . Could you please give me a hint of what is a standard way of proving the absolute convergence of the Dyson series for the evolution operator? References are welcome. (I am not a mathematician, but a physicist)","In this paper https://arxiv.org/abs/math-ph/9901018 the author (on page 4) mentions : ""...a standard argument for the absolute convergence of the Dyson series for [the evolution operator] $\hat{U}_{\tau}$ ..."" . Could you please give me a hint of what is a standard way of proving the absolute convergence of the Dyson series for the evolution operator? References are welcome. (I am not a mathematician, but a physicist)",,"['ordinary-differential-equations', 'mathematical-physics', 'quantum-mechanics']"
99,Diferential Equation by Laplace: $y'=-\frac{y}{ay+b}+S$,Diferential Equation by Laplace:,y'=-\frac{y}{ay+b}+S,"Hello and thanks for reading. I wish to check the stability of the ODE $$ y'=-\frac{y}{ay+b}+S $$ by applying the limit theorem of Laplace, where $a$, $b$ and $S$ are real. After some algebra I have: $$ ay'y+by'-aSy-bS+y=0. $$ When I do Laplace transformation then I have to $L(y'y)$ and can't go on. I need the Laplace expression of $y$. Thanks for your time.","Hello and thanks for reading. I wish to check the stability of the ODE $$ y'=-\frac{y}{ay+b}+S $$ by applying the limit theorem of Laplace, where $a$, $b$ and $S$ are real. After some algebra I have: $$ ay'y+by'-aSy-bS+y=0. $$ When I do Laplace transformation then I have to $L(y'y)$ and can't go on. I need the Laplace expression of $y$. Thanks for your time.",,"['ordinary-differential-equations', 'stability-in-odes']"
