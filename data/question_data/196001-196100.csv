,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is $\frac{\partial \overline{z}}{\partial z}$?,What is ?,\frac{\partial \overline{z}}{\partial z},We can write $$x=\frac{z+\overline{z}}{2}$$ where $z$ is a complex number. Here $z=x+iy$. What is $\frac{\partial x}{\partial z}$? My book says that it is $\frac{1}{2}$. But is $\frac{\partial \overline{z}}{\partial z}=0$?,We can write $$x=\frac{z+\overline{z}}{2}$$ where $z$ is a complex number. Here $z=x+iy$. What is $\frac{\partial x}{\partial z}$? My book says that it is $\frac{1}{2}$. But is $\frac{\partial \overline{z}}{\partial z}=0$?,,[]
1,Complex derivatives of complex functions,Complex derivatives of complex functions,,"I have a question about how to calculate a complex derivative. For example, I don't understand the following equations: $\frac{\partial}{\partial z}(\frac{1-z\bar{z}}{(e^{it}-z)(e^{-it}-\bar{z})})=\frac{1}{e^{-it}-\bar{z}}\frac{\partial}{\partial z}(\frac{1-z\bar{z}}{e^{it}-z})=\frac{1}{e^{-it}-\bar{z}}\frac{e^{it}(e^{-it}-\bar{z})}{(e^{it}-z)^2}$ . Can someone explain me please those two equations? I actually thought that $\bar{z}$ is not differentiable but that seems not correct, I don't know.","I have a question about how to calculate a complex derivative. For example, I don't understand the following equations: . Can someone explain me please those two equations? I actually thought that is not differentiable but that seems not correct, I don't know.",\frac{\partial}{\partial z}(\frac{1-z\bar{z}}{(e^{it}-z)(e^{-it}-\bar{z})})=\frac{1}{e^{-it}-\bar{z}}\frac{\partial}{\partial z}(\frac{1-z\bar{z}}{e^{it}-z})=\frac{1}{e^{-it}-\bar{z}}\frac{e^{it}(e^{-it}-\bar{z})}{(e^{it}-z)^2} \bar{z},"['complex-analysis', 'derivatives']"
2,There exists a positive and differentiable function $ f(x) $ such that $ f(x) \cdot (e^x + \left|x\right|) $ is differentiable in $ \mathbb{R} $,There exists a positive and differentiable function  such that  is differentiable in, f(x)   f(x) \cdot (e^x + \left|x\right|)   \mathbb{R} ,"I need to prove or disprove the following statement: There exists a positive and differentiable function $ f(x) $ such that $ f(x) \cdot (e^x + \left|x\right|) $ is differentiable in $ \mathbb{R} $ . Is my proof correct? The statement is false. Suppose. to the contrary, that there exists a positive and differentiable function $ f(x) $ on $ \mathbb{R} $ such that $ f(x) * (e^x + \left|x\right|) $ is differentiable in $ \mathbb{R} $ . $ \forall x, f(x) > 0 $ , which means $ \frac{1}{f(x)} $ is well-defined and differentiable. Therefore, as the product of two differentiable functions, $\frac{f(x)\cdot (e^x + \left|x\right|)}{f(x)} \Rightarrow (e^x + |x|)$ is differentiable at $ x = 0 \Rightarrow e^x + |x| - e^x $ is differentiable at $ 0 \Rightarrow |x| $ is differentiable at $ 0 $ - contradiction.","I need to prove or disprove the following statement: There exists a positive and differentiable function such that is differentiable in . Is my proof correct? The statement is false. Suppose. to the contrary, that there exists a positive and differentiable function on such that is differentiable in . , which means is well-defined and differentiable. Therefore, as the product of two differentiable functions, is differentiable at is differentiable at is differentiable at - contradiction."," f(x)   f(x) \cdot (e^x + \left|x\right|)   \mathbb{R}   f(x)   \mathbb{R}   f(x) * (e^x + \left|x\right|)   \mathbb{R}   \forall x, f(x) > 0   \frac{1}{f(x)}  \frac{f(x)\cdot (e^x + \left|x\right|)}{f(x)} \Rightarrow (e^x + |x|)  x = 0 \Rightarrow e^x + |x| - e^x   0 \Rightarrow |x|   0 ","['real-analysis', 'calculus', 'derivatives']"
3,Calculus operators in Laplace space?,Calculus operators in Laplace space?,,"I was playing with Laplace transforms and found something curious. Suppose we have an expression $4x+3$ that we want to take the derivative of or the integral of. If we instead took its Laplace transform, we have $$\mathcal{L} [4x+3](p)= \frac{3p+4}{p^2}$$ Now, let's multiply this and divide this by $p$ , and then invert the transform. We can see that $$\frac{3p+4}{p^2}\cdot p = \frac{3p+4}{p}$$ $$\mathcal{L}^{-1}\left[\frac{3p+4}{p}\right](x) = 3\delta(x)+4$$ $$\frac{3p+4}{p^2}\cdot \frac1p = \frac{3p+4}{p^3}$$ $$\mathcal{L}^{-1}\left[\frac{3p+4}{p^3}\right](x) = 2x^2+3x$$ I noticed an odd thing. The results are quite similar to taking the integral in the divide case, and taking a derivative in the multiplying case. There's an extra dirac delta function which gives me doubts however, so I tried with something different. Using $\sin(2x)+3$ gives me a similar result. If we take its Laplace transform, multiply/divide by $p$ , and then invert the transform, we get $3\delta(x)+2\cos(2x)$ from multiplying $p$ , and $3x-\frac12\cos(2x)+\frac12$ from dividing by $p$ , which in the vein of integration can be said to be the same as the indefinite integral since we just collect constants. Clearly, there seems to be some relationship between dividing in Laplace space and integrating in real space, and vice versa, but it doesn't seem it's exactly the same either, since there's extra constants and Dirac delta functions. I'm guessing I'm supposed to account for some type of initial condition? but i'm not sure. Any ideas on what's going on?","I was playing with Laplace transforms and found something curious. Suppose we have an expression that we want to take the derivative of or the integral of. If we instead took its Laplace transform, we have Now, let's multiply this and divide this by , and then invert the transform. We can see that I noticed an odd thing. The results are quite similar to taking the integral in the divide case, and taking a derivative in the multiplying case. There's an extra dirac delta function which gives me doubts however, so I tried with something different. Using gives me a similar result. If we take its Laplace transform, multiply/divide by , and then invert the transform, we get from multiplying , and from dividing by , which in the vein of integration can be said to be the same as the indefinite integral since we just collect constants. Clearly, there seems to be some relationship between dividing in Laplace space and integrating in real space, and vice versa, but it doesn't seem it's exactly the same either, since there's extra constants and Dirac delta functions. I'm guessing I'm supposed to account for some type of initial condition? but i'm not sure. Any ideas on what's going on?",4x+3 \mathcal{L} [4x+3](p)= \frac{3p+4}{p^2} p \frac{3p+4}{p^2}\cdot p = \frac{3p+4}{p} \mathcal{L}^{-1}\left[\frac{3p+4}{p}\right](x) = 3\delta(x)+4 \frac{3p+4}{p^2}\cdot \frac1p = \frac{3p+4}{p^3} \mathcal{L}^{-1}\left[\frac{3p+4}{p^3}\right](x) = 2x^2+3x \sin(2x)+3 p 3\delta(x)+2\cos(2x) p 3x-\frac12\cos(2x)+\frac12 p,"['calculus', 'integration', 'derivatives', 'laplace-transform']"
4,"If $f$ is a smooth real valued function on real line such that $f'(0)=1$ and $|f^{(n)} (x)|$ is uniformly bounded by $1$ , then $f(x)=\sin x$?","If  is a smooth real valued function on real line such that  and  is uniformly bounded by  , then ?",f f'(0)=1 |f^{(n)} (x)| 1 f(x)=\sin x,"Let $f : \mathbb R \to \mathbb R$ be a smooth ( infinitely differentiable everywhere ) function such that $f '(0)=1$ and $|f^{(n)} (x)| \le 1 , \forall x \in \mathbb R , \forall n \ge 0$ ( as usual denoting $f^{(0)}(x):=f(x)$) ; then is it true that $f(x)=\sin x , \forall x \in \mathbb R$ ?","Let $f : \mathbb R \to \mathbb R$ be a smooth ( infinitely differentiable everywhere ) function such that $f '(0)=1$ and $|f^{(n)} (x)| \le 1 , \forall x \in \mathbb R , \forall n \ge 0$ ( as usual denoting $f^{(0)}(x):=f(x)$) ; then is it true that $f(x)=\sin x , \forall x \in \mathbb R$ ?",,"['calculus', 'real-analysis']"
5,Possibility of computing antiderivative using dual numbers,Possibility of computing antiderivative using dual numbers,,"It is known that, given a function $f(x)$ , plugging in the dual number $x+\varepsilon$ , where $\varepsilon^2=0$ , yields $f(x) + f'(x)\varepsilon$ . For example: $$f(x) = x^3\\f(x+\varepsilon)=(x+\varepsilon)(x+\varepsilon)(x+\varepsilon)\\f(x+\varepsilon) = (x^2+2x\varepsilon)(x+\varepsilon)\\f(x+\varepsilon) = x^3 + 3x^2\varepsilon$$ We observe that $x^3$ is the value of $f(x)$ and $3x^2$ is the derivative of $f(x)$ . I would like to know if the process is reversible in such a way that, given a few data points demonstrating the derivative of some function $g(x)$ , such as $$g(1+\varepsilon) = a_1 + \varepsilon\\g(2+\varepsilon) = a_2 + 4\varepsilon\\g(3+\varepsilon) = a_3 + 9\varepsilon\\g(4+\varepsilon) = a_4+16\varepsilon,$$ could we algorithmically recover $g(x)$ ? We as humans can see that the pattern among the dual parts is the series of squares, so $g'(x)=x^2$ , telling us that $g(x)$ must be $\int{g'(x)}dx$ , which would be $\frac{1}{3}x^3+C$ . I don't have much hope for it due to the significant hurdles in the way, such as the data-fitting aspect of trying to decide what function is being represented by the dual parts and the fact that we are losing information when we multiply $\varepsilon$ by itself. It would be extremely convenient to have a quick and simple way of computing antiderivatives though.","It is known that, given a function , plugging in the dual number , where , yields . For example: We observe that is the value of and is the derivative of . I would like to know if the process is reversible in such a way that, given a few data points demonstrating the derivative of some function , such as could we algorithmically recover ? We as humans can see that the pattern among the dual parts is the series of squares, so , telling us that must be , which would be . I don't have much hope for it due to the significant hurdles in the way, such as the data-fitting aspect of trying to decide what function is being represented by the dual parts and the fact that we are losing information when we multiply by itself. It would be extremely convenient to have a quick and simple way of computing antiderivatives though.","f(x) x+\varepsilon \varepsilon^2=0 f(x) + f'(x)\varepsilon f(x) = x^3\\f(x+\varepsilon)=(x+\varepsilon)(x+\varepsilon)(x+\varepsilon)\\f(x+\varepsilon) = (x^2+2x\varepsilon)(x+\varepsilon)\\f(x+\varepsilon) = x^3 + 3x^2\varepsilon x^3 f(x) 3x^2 f(x) g(x) g(1+\varepsilon) = a_1 + \varepsilon\\g(2+\varepsilon) = a_2 + 4\varepsilon\\g(3+\varepsilon) = a_3 + 9\varepsilon\\g(4+\varepsilon) = a_4+16\varepsilon, g(x) g'(x)=x^2 g(x) \int{g'(x)}dx \frac{1}{3}x^3+C \varepsilon","['integration', 'derivatives', 'dual-numbers']"
6,Evaluating $\lim\limits_{x\rightarrow0}\frac{e^{-1/x^2}}{x}$,Evaluating,\lim\limits_{x\rightarrow0}\frac{e^{-1/x^2}}{x},"Today in my analysis class, we were preparing for the final and this question came up: Evaluate $$\lim_{x\to0}\frac{e^{-1/x^2}}{x}$$ We tried taking the $\log$, using L'Hopitals and some other tricks but couldn't figure it out. I thought maybe viewing this limits as a sequence in the following way might help; $$\lim_{x\rightarrow0}\frac{e^{-1/x^2}}{x} = \lim_{n\rightarrow \infty}\frac{e^{-n^2}}{1/n} = \lim_{n\rightarrow \infty}\frac{n}{e^{n^2}}$$ But from them I'm not sure. Thank you.","Today in my analysis class, we were preparing for the final and this question came up: Evaluate $$\lim_{x\to0}\frac{e^{-1/x^2}}{x}$$ We tried taking the $\log$, using L'Hopitals and some other tricks but couldn't figure it out. I thought maybe viewing this limits as a sequence in the following way might help; $$\lim_{x\rightarrow0}\frac{e^{-1/x^2}}{x} = \lim_{n\rightarrow \infty}\frac{e^{-n^2}}{1/n} = \lim_{n\rightarrow \infty}\frac{n}{e^{n^2}}$$ But from them I'm not sure. Thank you.",,"['real-analysis', 'limits']"
7,Does a connection exist between the roots of $\mathrm{Si}\left(\frac{z}{2}\right)$ and the roots of its derivative?,Does a connection exist between the roots of  and the roots of its derivative?,\mathrm{Si}\left(\frac{z}{2}\right),"From the answer to this question about the complex roots of $\mathrm{Si}\left(\frac{z}{2}\right)$ , we now know that the asymptotic of the $k$ -th complex root $z_k = x_k + i y_k $ is: \begin{align} x_k &\approx 4\pi k - \frac{{\log k}}{{\pi k}} \\ \\ y_k &\approx 2\log x_k + 2\log (\pi /2) \end{align} The Hadamard product for $\mathrm{Si}\left(\frac{z}{2}\right)$ is: $$\mathrm{Si}\left(\frac{z}{2}\right) = \frac{z}{2}\,\prod_{z_k} \left(1-\frac{z}{z_k} \right) \tag{1}$$ with $z_k$ taken as quadruples $z_k, -z_k, \overline{z_k}, -\overline{z_k}$ . The logarithmic derivative of the RHS of (1) then becomes: $$\frac{\mathrm{Si}'}{\mathrm{Si}}\left(\frac{z}{2}\right) = \sum_{z_k} \frac{1}{z-z_k} + \frac{1}{z}\tag{2}$$ hence, $$\mathrm{Si}'\left(\frac{z}{2}\right) = \frac{\sin\left(\frac{z}{2}\right)}{z} =\mathrm{Si}\left(\frac{z}{2}\right)\,\left( \sum_{z_k} \frac{1}{z-z_k} + \frac{1}{z}\right)\tag{3}$$ So, the regular real roots of $\displaystyle \frac{\sin\left(\frac{z}{2}\right)}{z}$ , i.e. $\mu_m = \pm 2\pi m$ with $m \in \mathbb{N}$ , will occur when: $$\sum_{z_k} \frac{1}{z-z_k} = -\frac{1}{z} \tag{4}$$ Question: Suppose we only know the asymptotic for the complex roots $z_k$ of $\mathrm{Si}\left(\frac{z}{2}\right)$ , could we then derive, e.g. using (4), any information/constraints about the location of the real roots $\mu_m$ of its derivative $\mathrm{Si}'\left(\frac{z}{2}\right)$ ? Potentially related questions here , here and here .","From the answer to this question about the complex roots of , we now know that the asymptotic of the -th complex root is: The Hadamard product for is: with taken as quadruples . The logarithmic derivative of the RHS of (1) then becomes: hence, So, the regular real roots of , i.e. with , will occur when: Question: Suppose we only know the asymptotic for the complex roots of , could we then derive, e.g. using (4), any information/constraints about the location of the real roots of its derivative ? Potentially related questions here , here and here .","\mathrm{Si}\left(\frac{z}{2}\right) k z_k = x_k + i y_k  \begin{align}
x_k &\approx 4\pi k - \frac{{\log k}}{{\pi k}} \\
\\
y_k &\approx 2\log x_k + 2\log (\pi /2)
\end{align} \mathrm{Si}\left(\frac{z}{2}\right) \mathrm{Si}\left(\frac{z}{2}\right) = \frac{z}{2}\,\prod_{z_k} \left(1-\frac{z}{z_k} \right) \tag{1} z_k z_k, -z_k, \overline{z_k}, -\overline{z_k} \frac{\mathrm{Si}'}{\mathrm{Si}}\left(\frac{z}{2}\right) = \sum_{z_k} \frac{1}{z-z_k} + \frac{1}{z}\tag{2} \mathrm{Si}'\left(\frac{z}{2}\right) = \frac{\sin\left(\frac{z}{2}\right)}{z} =\mathrm{Si}\left(\frac{z}{2}\right)\,\left( \sum_{z_k} \frac{1}{z-z_k} + \frac{1}{z}\right)\tag{3} \displaystyle \frac{\sin\left(\frac{z}{2}\right)}{z} \mu_m = \pm 2\pi m m \in \mathbb{N} \sum_{z_k} \frac{1}{z-z_k} = -\frac{1}{z} \tag{4} z_k \mathrm{Si}\left(\frac{z}{2}\right) \mu_m \mathrm{Si}'\left(\frac{z}{2}\right)","['integration', 'complex-analysis', 'derivatives', 'asymptotics', 'roots']"
8,"Prove that if $f''(x)≤f(x)$ , then $ f'(x)<\sqrt{2}f(x)$ for every $x\in\mathbb{R}$ [closed]","Prove that if  , then  for every  [closed]",f''(x)≤f(x)  f'(x)<\sqrt{2}f(x) x\in\mathbb{R},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I need help with this problem, ""Let $f\in\mathcal{C}^{2}(\mathbb{R})$ such that $f(x)$ , $f'(x)$ and $f''(x)$ are all strictly positive for every $x\in\mathbb{R}$ . Show that if $f''(x)\leq f(x)$ for every $x\in\mathbb{R}$ , then $f'(x)<\sqrt{2}f(x)$ for every $x\in\mathbb{R}$ "" I apologize if it seems like I'm asking you to do my work, but I honestly don't know where to begin with this...","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I need help with this problem, ""Let such that , and are all strictly positive for every . Show that if for every , then for every "" I apologize if it seems like I'm asking you to do my work, but I honestly don't know where to begin with this...",f\in\mathcal{C}^{2}(\mathbb{R}) f(x) f'(x) f''(x) x\in\mathbb{R} f''(x)\leq f(x) x\in\mathbb{R} f'(x)<\sqrt{2}f(x) x\in\mathbb{R},"['real-analysis', 'linear-algebra', 'derivatives', 'inequality']"
9,Does convexity on a set implies hessian PSD on the boundary?,Does convexity on a set implies hessian PSD on the boundary?,,"Let $C \subset \mathbb{R}^N$ be a closed convex set with nonempty interior. Let $f : \mathbb{R}^N \to \mathbb{R}$ be twice differentiable on $C$ . We also assume that it is convex on $C$ : $$ (\forall x,y \in C) (\forall \alpha \in [0,1]) \quad f((1- \alpha)x + \alpha y) \leq (1- \alpha)f(x) + \alpha f(y).$$ My main question is : can we say that the Hessian of $f$ is positive semidefinite on $C$ ? Some comments: The basic result I know holds when $C$ is an open set ; so applying such result on the interior of $C$ , we immediately get that for all $x \in {\rm{int}}~C$ , $\nabla^2 f(x) \succeq 0$ . So my question is about what happens at the boundary. What is also clear is that if I assume further that $f$ is of class $C^2$ on $C$ (and not only twice differentiable), we could pass to the limit from the interior to the boundary of $C$ to conclude that yes, $\nabla^2 f(x) \succeq 0$ for all $x \in C$ . I have been trying to find a counterexample, but every example of ""twice differentiable but not $C^2$ "" function I find is highly nonconvex, so I start to wonder if convexity and twice differentiable implies $C^2$ ? After all, it is known that: convex functions are always continuous in the interior of their domain the subdifferential of a convex function has a closed graph ; so differentiable convex functions have a continuous gradient So my secondary question is : is the hessian of a twice differentiable convex function always continous?","Let be a closed convex set with nonempty interior. Let be twice differentiable on . We also assume that it is convex on : My main question is : can we say that the Hessian of is positive semidefinite on ? Some comments: The basic result I know holds when is an open set ; so applying such result on the interior of , we immediately get that for all , . So my question is about what happens at the boundary. What is also clear is that if I assume further that is of class on (and not only twice differentiable), we could pass to the limit from the interior to the boundary of to conclude that yes, for all . I have been trying to find a counterexample, but every example of ""twice differentiable but not "" function I find is highly nonconvex, so I start to wonder if convexity and twice differentiable implies ? After all, it is known that: convex functions are always continuous in the interior of their domain the subdifferential of a convex function has a closed graph ; so differentiable convex functions have a continuous gradient So my secondary question is : is the hessian of a twice differentiable convex function always continous?","C \subset \mathbb{R}^N f : \mathbb{R}^N \to \mathbb{R} C C  (\forall x,y \in C) (\forall \alpha \in [0,1]) \quad f((1- \alpha)x + \alpha y) \leq (1- \alpha)f(x) + \alpha f(y). f C C C x \in {\rm{int}}~C \nabla^2 f(x) \succeq 0 f C^2 C C \nabla^2 f(x) \succeq 0 x \in C C^2 C^2","['derivatives', 'continuity', 'convex-analysis', 'hessian-matrix']"
10,Finding all the minima and maxima within a range [closed],Finding all the minima and maxima within a range [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I'm not sure how to find all the maximas and minimas where the range is $1≤x≤18$ and the function is: $$20\sin \left(\fracπ6x-\frac {2\pi}3\right)+22$$ I already found the first derivative which is: $$\frac{10\pi}3\cos\left(\frac{\pi}{6}x-\frac{2\pi}{3}\right)=0$$ where $x$ is $7$ and using $f''(x)$ and subbing in my $x$ , I get $-5.48$ which is a maximum.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question I'm not sure how to find all the maximas and minimas where the range is and the function is: I already found the first derivative which is: where is and using and subbing in my , I get which is a maximum.",1≤x≤18 20\sin \left(\fracπ6x-\frac {2\pi}3\right)+22 \frac{10\pi}3\cos\left(\frac{\pi}{6}x-\frac{2\pi}{3}\right)=0 x 7 f''(x) x -5.48,"['derivatives', 'maxima-minima']"
11,$n$th derivative of $\sin(\cos(x))$,th derivative of,n \sin(\cos(x)),"$$f_n(x)=\frac{d^n}{dx^n}\sin(\cos(x))$$ Prove there are an infinite number of $n$ such that the global maximum of $f_n(x)$ is an integer. I looked this up online and it seems like this is related to exponential generating functions, but I don't know enough about them. How does this work?","Prove there are an infinite number of such that the global maximum of is an integer. I looked this up online and it seems like this is related to exponential generating functions, but I don't know enough about them. How does this work?",f_n(x)=\frac{d^n}{dx^n}\sin(\cos(x)) n f_n(x),"['calculus', 'derivatives']"
12,When can we interchange partial and total derivates?,When can we interchange partial and total derivates?,,"I have a question regarding a math proof, which can be found on 17 (Lemma 4) on this document . A screenshot is provided below. I am a bit unsure how equation 29 is derived. They seem to be using some sort of interchange of partial derivatives, but I don't understand the justification of switching partial derivative for the total derivative.","I have a question regarding a math proof, which can be found on 17 (Lemma 4) on this document . A screenshot is provided below. I am a bit unsure how equation 29 is derived. They seem to be using some sort of interchange of partial derivatives, but I don't understand the justification of switching partial derivative for the total derivative.",,"['calculus', 'derivatives', 'partial-derivative']"
13,Why does partial derivative of the joint equation of two straight lines give another two straight lines belonging to the same family?,Why does partial derivative of the joint equation of two straight lines give another two straight lines belonging to the same family?,,"If we are to find the point of intersection of the lines whose joint equation is $2x^2+3xy-2y^2-9x+7y-5=0$ then we can EITHER consider it as a quadratic in $x$ and then apply quadratic formula and get the equations of two straight lines and then their point of intersection. OR, we can differentiate the given equation first w.r.t $x$ and then w.r.t. $y$ and obtain two equations and then their point of intersection. We get the same point both ways. While the equations obtained in both ways are different. Looks like, partial derivative is giving the equations from the same family as the point of intersection is same. Why is this so? What exactly is partial derivative doing here?","If we are to find the point of intersection of the lines whose joint equation is then we can EITHER consider it as a quadratic in and then apply quadratic formula and get the equations of two straight lines and then their point of intersection. OR, we can differentiate the given equation first w.r.t and then w.r.t. and obtain two equations and then their point of intersection. We get the same point both ways. While the equations obtained in both ways are different. Looks like, partial derivative is giving the equations from the same family as the point of intersection is same. Why is this so? What exactly is partial derivative doing here?",2x^2+3xy-2y^2-9x+7y-5=0 x x y,"['geometry', 'derivatives', 'contest-math', 'partial-derivative', 'coordinate-systems']"
14,Differentiation of Composite Functions (Chain Rule Related),Differentiation of Composite Functions (Chain Rule Related),,"Suppose I have the arbitrary functions $f, g$ , and $h: \mathbb{R}^2 \longrightarrow \mathbb{R}$ . The question I have is, what is the derivative with respect to $x$ of: \begin{equation} f(g(x,y), h(x, y)) \end{equation} It is important that the functional relationship between g(x,y) and h(x,y) remains arbitrary. I do understand the chain rule if it were to be applied to a simple case such as $f(g(x,y))$ , in which case this would be equal to $f_x(g(x,y))g_x(x,y)$ , where $f_x$ and $g_x$ are the partial derivatives with respect to $x$ . Thank you.","Suppose I have the arbitrary functions , and . The question I have is, what is the derivative with respect to of: It is important that the functional relationship between g(x,y) and h(x,y) remains arbitrary. I do understand the chain rule if it were to be applied to a simple case such as , in which case this would be equal to , where and are the partial derivatives with respect to . Thank you.","f, g h: \mathbb{R}^2 \longrightarrow \mathbb{R} x \begin{equation}
f(g(x,y), h(x, y))
\end{equation} f(g(x,y)) f_x(g(x,y))g_x(x,y) f_x g_x x","['calculus', 'derivatives', 'chain-rule']"
15,"How to calculate the first variation of $\,y \rightarrow \int_{a}^{b} y(x)\,\mathrm dx\,$ with $\,V=C^{0}([a,b])$",How to calculate the first variation of  with,"\,y \rightarrow \int_{a}^{b} y(x)\,\mathrm dx\, \,V=C^{0}([a,b])","So I was solving this problem and I wanted to know if I did everything right. The problem is: I have to calculate the first variation of the functional $F:V\rightarrow \mathbb{R}\quad$ defined by $\displaystyle\;y\rightarrow\!\int_{a}^{b} \!y(x)\,\mathrm dx\;$ with $\;V=C^{0}([a,b])\,.$ So the formula I used is this: $\;\delta F[y]=\dfrac{\partial F}{\partial y}\cdot v$ First, we need to calculate the functional derivative $\partial F/\partial y$ $\begin{align} \dfrac{\partial F}{\partial y}&=\lim\limits_{\varepsilon\rightarrow 0} \dfrac{(F[y+\varepsilon v]-F[y])}{\varepsilon}\\ &=\lim\limits_{\varepsilon\rightarrow 0}\dfrac{\int_{a}^{b}[y(x)+\varepsilon v(x)]\,\mathrm dx-\int_{a}^{b}y(x)\,\mathrm dx}{\varepsilon}\\ &=\lim\limits_{\varepsilon\rightarrow 0}\dfrac{\int_{a}^{b}\varepsilon v(x)\,\mathrm dx}{\varepsilon}=\int_{a}^{b}v(x)\,\mathrm dx\,. \end{align}$ So at the end, with $\;\delta F[y]=\partial F/\partial y\cdot v\,,\,$ I have : $\displaystyle\delta F[y] =\int_{a}^{b}v(x)\,\mathrm dx\cdot v$ Am I right or am I missing something?","So I was solving this problem and I wanted to know if I did everything right. The problem is: I have to calculate the first variation of the functional defined by with So the formula I used is this: First, we need to calculate the functional derivative So at the end, with I have : Am I right or am I missing something?","F:V\rightarrow \mathbb{R}\quad \displaystyle\;y\rightarrow\!\int_{a}^{b} \!y(x)\,\mathrm dx\; \;V=C^{0}([a,b])\,. \;\delta F[y]=\dfrac{\partial F}{\partial y}\cdot v \partial F/\partial y \begin{align}
\dfrac{\partial F}{\partial y}&=\lim\limits_{\varepsilon\rightarrow 0} \dfrac{(F[y+\varepsilon v]-F[y])}{\varepsilon}\\
&=\lim\limits_{\varepsilon\rightarrow 0}\dfrac{\int_{a}^{b}[y(x)+\varepsilon v(x)]\,\mathrm dx-\int_{a}^{b}y(x)\,\mathrm dx}{\varepsilon}\\
&=\lim\limits_{\varepsilon\rightarrow 0}\dfrac{\int_{a}^{b}\varepsilon v(x)\,\mathrm dx}{\varepsilon}=\int_{a}^{b}v(x)\,\mathrm dx\,.
\end{align} \;\delta F[y]=\partial F/\partial y\cdot v\,,\, \displaystyle\delta F[y] =\int_{a}^{b}v(x)\,\mathrm dx\cdot v","['integration', 'limits', 'derivatives', 'optimization', 'calculus-of-variations']"
16,Smooth family of metrics,Smooth family of metrics,,"Let $M$ be a smooth manifold and $(g(t))_{t\in[a,b]}$ be a family of complete Riemannian metrics (on $M$ ) that smoothly depend on $t$ . If $x$ and $y$ are two arbitrary points on $M$ , we can define a distance function $f(t):=d_{g(t)}(x,y)$ that varies through time. The distance $d_{g(t)}$ is the usual distance induced by the Riemannian  metric $g(t)$ . What I am interested in is the regularity of the function $f$ , and specifically I want to use the Rademacher's theorem which guarantees me that $f$ is almost everywhere differentiable on $M$ if I can prove that $f$ is Lipschitz continuous or at least locally Lipschitz continuous. A first idea is to use the completeness of $M$ to deduce that there is a minimising unit-speed geodesic $\gamma_t$ between $x$ and $y$ at any time, so we write : $$d_{g(t)}(x,y)=L_{g(t)}(\gamma_t)$$ But I can't prove why the family of minimising geodesic $(\gamma_t)$ would also depend smoothly on time. Moreover, I want to also prove an explicit formula for the derivative (when $f$ is differentiable ) which is : $$\frac{d}{dt}|_{t_0}d_{g(t)}(x,y)=L_{g(t_0)}(\gamma_{t_0})$$ Any help is appreciated.","Let be a smooth manifold and be a family of complete Riemannian metrics (on ) that smoothly depend on . If and are two arbitrary points on , we can define a distance function that varies through time. The distance is the usual distance induced by the Riemannian  metric . What I am interested in is the regularity of the function , and specifically I want to use the Rademacher's theorem which guarantees me that is almost everywhere differentiable on if I can prove that is Lipschitz continuous or at least locally Lipschitz continuous. A first idea is to use the completeness of to deduce that there is a minimising unit-speed geodesic between and at any time, so we write : But I can't prove why the family of minimising geodesic would also depend smoothly on time. Moreover, I want to also prove an explicit formula for the derivative (when is differentiable ) which is : Any help is appreciated.","M (g(t))_{t\in[a,b]} M t x y M f(t):=d_{g(t)}(x,y) d_{g(t)} g(t) f f M f M \gamma_t x y d_{g(t)}(x,y)=L_{g(t)}(\gamma_t) (\gamma_t) f \frac{d}{dt}|_{t_0}d_{g(t)}(x,y)=L_{g(t_0)}(\gamma_{t_0})","['derivatives', 'differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'lipschitz-functions']"
17,Definition of a Derivative,Definition of a Derivative,,"So, I recently decided to major in mathematics, and I take calculus next semester, but I bought a calculus textbook and have been learning a little bit of calculus. I was wondering if I understand this correctly, but the derivative is not the instantaneous rate of change but rather the best constant approximation for the rate of change. Instantaneous means ""no lapse in time"". To me that makes the definition of a derivative nonsensical because you can't measure a rate of change when there is no lapse in time.","So, I recently decided to major in mathematics, and I take calculus next semester, but I bought a calculus textbook and have been learning a little bit of calculus. I was wondering if I understand this correctly, but the derivative is not the instantaneous rate of change but rather the best constant approximation for the rate of change. Instantaneous means ""no lapse in time"". To me that makes the definition of a derivative nonsensical because you can't measure a rate of change when there is no lapse in time.",,"['calculus', 'derivatives', 'definition']"
18,A characterization of absolutely continuous functions,A characterization of absolutely continuous functions,,"Let $\lambda$ be the Lebesgue measure on $[a, b]$ . I'm trying to prove below result mentioned as Theorem 5.2 in my lecture note of gradient flows. Theorem A map $F:[a, b] \to \mathbb R$ is absolutely continuous if and only if there is a $\lambda$ -integrable $f:[a, b] \to \mathbb R$ such that $$ \int_a^x f \mathrm d \lambda = F(x)-F(a) \quad \forall x \in [a, b]. $$ In that case, $F$ is differentiable $\lambda$ -a.e. and $F'=f$ $\lambda$ -a.e. Could you confirm if my below attempt is fine? Proof Let $F$ be absolutely continuous. Then $F$ is of bounded variation. Then $F$ is differentiable $\lambda$ -a.e. and its derivative $F'$ is $\lambda$ -integrable . Let $G(x) := \int_a^x F' \mathrm d \lambda$ . Then $G$ is absolutely continuous. So $G-F$ is absolutely continuous. By Lebesgue differentiation theorem , $G$ is differentiable $\lambda$ -a.e. and $G' = F'$ $\lambda$ -a.e. It follows that $(G-F)' = 0$ $\lambda$ -a.e. Then $G-F$ is a constant. On the other hand, $G(a)=F(a)=0$ . So $G=F$ . Let $f:[a, b] \to \mathbb R$ be $\lambda$ -integrable such that $$ \int_a^x f \mathrm d \lambda = F(x)-F(a) \quad \forall x \in [a, b]. $$ Let $G(x) := \int_a^x f \mathrm d \lambda$ . Then $G$ is absolutely continuous. Then $F =G + F(a)$ is absolutely continuous. By Lebesgue differentiation theorem , $G$ is differentiable $\lambda$ -a.e. and $G' = f$ $\lambda$ -a.e. On the other hand, $F'=G'$ $\lambda$ -a.e.","Let be the Lebesgue measure on . I'm trying to prove below result mentioned as Theorem 5.2 in my lecture note of gradient flows. Theorem A map is absolutely continuous if and only if there is a -integrable such that In that case, is differentiable -a.e. and -a.e. Could you confirm if my below attempt is fine? Proof Let be absolutely continuous. Then is of bounded variation. Then is differentiable -a.e. and its derivative is -integrable . Let . Then is absolutely continuous. So is absolutely continuous. By Lebesgue differentiation theorem , is differentiable -a.e. and -a.e. It follows that -a.e. Then is a constant. On the other hand, . So . Let be -integrable such that Let . Then is absolutely continuous. Then is absolutely continuous. By Lebesgue differentiation theorem , is differentiable -a.e. and -a.e. On the other hand, -a.e.","\lambda [a, b] F:[a, b] \to \mathbb R \lambda f:[a, b] \to \mathbb R 
\int_a^x f \mathrm d \lambda = F(x)-F(a) \quad \forall x \in [a, b].
 F \lambda F'=f \lambda F F F \lambda F' \lambda G(x) := \int_a^x F' \mathrm d \lambda G G-F G \lambda G' = F' \lambda (G-F)' = 0 \lambda G-F G(a)=F(a)=0 G=F f:[a, b] \to \mathbb R \lambda 
\int_a^x f \mathrm d \lambda = F(x)-F(a) \quad \forall x \in [a, b].
 G(x) := \int_a^x f \mathrm d \lambda G F =G + F(a) G \lambda G' = f \lambda F'=G' \lambda","['real-analysis', 'derivatives', 'lebesgue-measure', 'absolute-continuity']"
19,The $\lambda$-integrability of the derivative of a function of bounded variation,The -integrability of the derivative of a function of bounded variation,\lambda,"Let $f:[a, b] \to \mathbb{R}$ and $\operatorname{Var}_{[a, b]} f$ its total variation . Let $\lambda$ be the Lebesgue measure on $[a, b]$ . I would like to prove below properties for functions of bounded variation, i.e., Theorem If $f$ is of bounded variation, then $f$ is differentiable $\lambda$ -a.e., The function $$ g(x) := \begin{cases} f^{\prime}(x) & \text{if } f \text { differentiable at } x \in (a, b),\\ 0 & \text {otherwise}. \end{cases} $$ is $\lambda$ -integrable such that $\int_a^b |g| \mathrm d \lambda \leq \operatorname{Var}_{[a, b]} f$ . Could you confirm if my below attempt is fine? Proof By this lemma, there exist increasing functions $f_1, f_2:[a, b] \rightarrow \mathbb{R}$ such that $f=f_1-$ $f_2$ and $$ \operatorname{Var}_{[a, b]} f = f_1(b)-f_1(a)+f_2(b)-f_2(a). $$ By this lemma, $f_1, f_2$ are differentiable $\lambda$ -a.e. Claim (1.) then follows. Let $$ g_1(x) := \begin{cases} f_1^{\prime}(x) & \text{if } f_1 \text { differentiable at } x \in (a, b),\\ 0 & \text {otherwise}. \end{cases} $$ and $$ g_2(x) := \begin{cases} f_2^{\prime}(x) & \text{if } f_2 \text { differentiable at } x \in (a, b),\\ 0 & \text {otherwise}. \end{cases} $$ I proved that $g, g_1, g_2$ are Borel measurable. So $g, g_1, g_2$ are $\lambda$ -measurable . I proved that $g_1, g_2 \ge 0$ and $$ \int_a^b g_1 \mathrm d \lambda \le f_1(b)-f_1(a) \quad \text{and} \quad \int_a^b g_2 \mathrm d \lambda \le f_2(b)-f_2(a). $$ Notice that $g=g_1-g_2$ $\lambda$ -a.e., so $$ \int_a^b |g| \mathrm d \lambda \le \int_a^b |g_1| \mathrm d \lambda + \int_a^b |g_2| \mathrm d \lambda = (f_1(b)-f_1(a))+(f_2(b)-f_2(a)) = \operatorname{Var}_{[a, b]} f < \infty. $$ This completes the proof.","Let and its total variation . Let be the Lebesgue measure on . I would like to prove below properties for functions of bounded variation, i.e., Theorem If is of bounded variation, then is differentiable -a.e., The function is -integrable such that . Could you confirm if my below attempt is fine? Proof By this lemma, there exist increasing functions such that and By this lemma, are differentiable -a.e. Claim (1.) then follows. Let and I proved that are Borel measurable. So are -measurable . I proved that and Notice that -a.e., so This completes the proof.","f:[a, b] \to \mathbb{R} \operatorname{Var}_{[a, b]} f \lambda [a, b] f f \lambda 
g(x) :=
\begin{cases}
f^{\prime}(x) & \text{if } f \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
 \lambda \int_a^b |g| \mathrm d \lambda \leq \operatorname{Var}_{[a, b]} f f_1, f_2:[a, b] \rightarrow \mathbb{R} f=f_1- f_2 
\operatorname{Var}_{[a, b]} f = f_1(b)-f_1(a)+f_2(b)-f_2(a).
 f_1, f_2 \lambda 
g_1(x) :=
\begin{cases}
f_1^{\prime}(x) & \text{if } f_1 \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
 
g_2(x) :=
\begin{cases}
f_2^{\prime}(x) & \text{if } f_2 \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
 g, g_1, g_2 g, g_1, g_2 \lambda g_1, g_2 \ge 0 
\int_a^b g_1 \mathrm d \lambda \le f_1(b)-f_1(a)
\quad \text{and} \quad
\int_a^b g_2 \mathrm d \lambda \le f_2(b)-f_2(a).
 g=g_1-g_2 \lambda 
\int_a^b |g| \mathrm d \lambda \le \int_a^b |g_1| \mathrm d \lambda + \int_a^b |g_2| \mathrm d \lambda = (f_1(b)-f_1(a))+(f_2(b)-f_2(a)) = \operatorname{Var}_{[a, b]} f < \infty.
","['real-analysis', 'derivatives', 'lebesgue-measure', 'bounded-variation']"
20,"Why isn't there any method to solve integral, like differentiation has?","Why isn't there any method to solve integral, like differentiation has?",,"Differentiation of function has a method to solve, by limits $$ \frac{d(f(x))}{dx} = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ Is there any method by which we can solve integral without using antiderivative, like differentiation does?","Differentiation of function has a method to solve, by limits Is there any method by which we can solve integral without using antiderivative, like differentiation does?", \frac{d(f(x))}{dx} = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h},"['integration', 'limits']"
21,Does the chain rule imply the product rule?,Does the chain rule imply the product rule?,,"Let $\mathbb{F}$ be a field, and consider $\mathbb{F}^\mathbb{F}$ as an algebra over $\mathbb{F}$ with the standard function multiplication. Let $D$ be a linear transformation on a subalgebra of $\mathbb{F}^\mathbb{F}$ closed under function composition that satisfies the chain rule.  Does $D$ necessarily satisfy the product rule for arbitrary $\mathbb{F}$ ? (Inspired by a comment on this question. ) What if the subalgebra must be unital?","Let be a field, and consider as an algebra over with the standard function multiplication. Let be a linear transformation on a subalgebra of closed under function composition that satisfies the chain rule.  Does necessarily satisfy the product rule for arbitrary ? (Inspired by a comment on this question. ) What if the subalgebra must be unital?",\mathbb{F} \mathbb{F}^\mathbb{F} \mathbb{F} D \mathbb{F}^\mathbb{F} D \mathbb{F},"['linear-algebra', 'abstract-algebra', 'derivatives', 'vector-spaces', 'field-theory']"
22,"Solution verification: Partials of $f(x,y) = y/x, x \neq 0$ at $(0,0)$",Solution verification: Partials of  at,"f(x,y) = y/x, x \neq 0 (0,0)","This is a question from Chapter 2 of Vector Calculus Study Guide Solutions Manual (Karen Pao, Frederick Soon) . Let $$f(x,y) =\begin{cases} y/x, & x\neq 0 \\ 0 & x = 0.  \end{cases}$$ at $(0,0)$ Compute $f_x(0,0)$ and $f_y(0,0)$ if they exist. I used the limit definition and I obtained: $$f_x(0,0) = \lim_{h \to 0} \dfrac{f(h,0) - f(0,0)}{h} = \lim_{h \to 0} \dfrac{0/h - 0}{h}= 0 $$ and $$f_y(0,0) = \lim_{h \to 0} \dfrac{f(0,h) - f(0,0)}{h} = \lim_{h \to 0} \dfrac{0-0}{h} = 0 .$$ However the given answer is that $f_x$ does not exist and $f_y = 0$ . Can someone please let me know how my calculation for $f_x(0,0)$ is wrong? Thank you!","This is a question from Chapter 2 of Vector Calculus Study Guide Solutions Manual (Karen Pao, Frederick Soon) . Let at Compute and if they exist. I used the limit definition and I obtained: and However the given answer is that does not exist and . Can someone please let me know how my calculation for is wrong? Thank you!","f(x,y) =\begin{cases} y/x, & x\neq 0 \\ 0 & x = 0.  \end{cases} (0,0) f_x(0,0) f_y(0,0) f_x(0,0) = \lim_{h \to 0} \dfrac{f(h,0) - f(0,0)}{h} = \lim_{h \to 0} \dfrac{0/h - 0}{h}= 0  f_y(0,0) = \lim_{h \to 0} \dfrac{f(0,h) - f(0,0)}{h} = \lim_{h \to 0} \dfrac{0-0}{h} = 0 . f_x f_y = 0 f_x(0,0)","['limits', 'derivatives', 'solution-verification', 'partial-derivative']"
23,Function defined differently on rationals and irrationals with derivative given by classic rules,Function defined differently on rationals and irrationals with derivative given by classic rules,,"Today, one of my students was given the function $$f(x) = \begin{cases} x^3+1 \quad \text{ if }x\in \Bbb Q\\ x^3+x \quad \text{ if }x\in \Bbb R \backslash \Bbb Q \end{cases} $$ He had some questions on the derivative and he wrote $$ f'(x)=\begin{cases} 3x^2 \quad \text{ if }x\in \Bbb Q\\ 3x^2+1 \quad \text{ if }x\in \Bbb R \backslash \Bbb Q \end{cases}$$ Which is not true of course, but I was wondering if there exists some function where it could be the case i.e. a function defined differently on rationals and irrationals with derivative given by the classic rules of derivation like my student wanted to do. The main problem I see is that the function will not be continuous at almost every point so not differentiable almost everywhere, right ? Or can someone maybe find such a function ?","Today, one of my students was given the function He had some questions on the derivative and he wrote Which is not true of course, but I was wondering if there exists some function where it could be the case i.e. a function defined differently on rationals and irrationals with derivative given by the classic rules of derivation like my student wanted to do. The main problem I see is that the function will not be continuous at almost every point so not differentiable almost everywhere, right ? Or can someone maybe find such a function ?",f(x) = \begin{cases} x^3+1 \quad \text{ if }x\in \Bbb Q\\ x^3+x \quad \text{ if }x\in \Bbb R \backslash \Bbb Q \end{cases}   f'(x)=\begin{cases} 3x^2 \quad \text{ if }x\in \Bbb Q\\ 3x^2+1 \quad \text{ if }x\in \Bbb R \backslash \Bbb Q \end{cases},"['real-analysis', 'derivatives', 'examples-counterexamples']"
24,Differentiating $(\frac{\sqrt{x+1}}{\sqrt{x}})^3$ using chain rule,Differentiating  using chain rule,(\frac{\sqrt{x+1}}{\sqrt{x}})^3,"We have, $$y = \left(\frac{\sqrt{x+1}}{\sqrt{x}}\right)^3$$ $$ \implies y = \left(1 + \frac{1}{x}\right)^{3/2}$$ Differentiating both sides w.r.t. $x$ (Using chain rule) $$\implies \frac{dy}{dx} = \frac32\left(1 + \frac{1}{x}\right)^{3/2 -1 }\cdot \dfrac{d}{dx}\left(1 + \frac{1}{x}\right)$$ $$ \implies \frac{dy}{dx} = \frac32\left(1 + \frac{1}{x}\right)^{1/2}\cdot \frac{-1}{x^2}$$ $$ \implies \boxed{\frac{dy}{dx} = \frac{-3}{2x^2}\sqrt{1 + \frac{1}{x}}}$$ What is the mistake in my work? Graphs of $f'(x)$ and $\frac{-3}{2x^2}\sqrt{1 + \frac{1}{x}}$ are not same .","We have, Differentiating both sides w.r.t. (Using chain rule) What is the mistake in my work? Graphs of and are not same .",y = \left(\frac{\sqrt{x+1}}{\sqrt{x}}\right)^3  \implies y = \left(1 + \frac{1}{x}\right)^{3/2} x \implies \frac{dy}{dx} = \frac32\left(1 + \frac{1}{x}\right)^{3/2 -1 }\cdot \dfrac{d}{dx}\left(1 + \frac{1}{x}\right)  \implies \frac{dy}{dx} = \frac32\left(1 + \frac{1}{x}\right)^{1/2}\cdot \frac{-1}{x^2}  \implies \boxed{\frac{dy}{dx} = \frac{-3}{2x^2}\sqrt{1 + \frac{1}{x}}} f'(x) \frac{-3}{2x^2}\sqrt{1 + \frac{1}{x}},"['calculus', 'derivatives', 'chain-rule']"
25,Is it possible to show that an absolutely continuous function $I\to \mathbb{R}$ is differentiable a.e. without invoking Lebesgue theory?,Is it possible to show that an absolutely continuous function  is differentiable a.e. without invoking Lebesgue theory?,I\to \mathbb{R},"Theorem: let $f:I\to \mathbb{R}$ be absolutely continuous , then $f'$ exists a.e. As mentioned here , Rudin's Real and Complex Analysis proves the theorem using a fair bit of measure theory. The theorem, however, is very easily stated with little measure theory; one would only need said theory to define the meaning of ""a.e."", which could be done as follows: Definition: for an arbitrary $x\in\mathbb{R}$ , let $P(x)$ be a statement about $x$ (e.g. "" $x$ is rational"", "" $x$ is larger than 2"", etc.). We write $$¬P:=\{x\in\mathbb{R} : P(x) \text{ is false}\}$$ We say that $P$ holds almost everywhere iff $$\inf\left\{\sum_{k=1}^{\infty}(b_k-a_k) : ¬P\subseteq \bigcup_{k=1}^{\infty}[a_k,b_k] \right\} = 0.$$ Considering that the above is the only bit of measure theory we need to state the theorem, is it possible to prove the theorem without invoking any more measure theoretical machinery?","Theorem: let be absolutely continuous , then exists a.e. As mentioned here , Rudin's Real and Complex Analysis proves the theorem using a fair bit of measure theory. The theorem, however, is very easily stated with little measure theory; one would only need said theory to define the meaning of ""a.e."", which could be done as follows: Definition: for an arbitrary , let be a statement about (e.g. "" is rational"", "" is larger than 2"", etc.). We write We say that holds almost everywhere iff Considering that the above is the only bit of measure theory we need to state the theorem, is it possible to prove the theorem without invoking any more measure theoretical machinery?","f:I\to \mathbb{R} f' x\in\mathbb{R} P(x) x x x ¬P:=\{x\in\mathbb{R} : P(x) \text{ is false}\} P \inf\left\{\sum_{k=1}^{\infty}(b_k-a_k) : ¬P\subseteq \bigcup_{k=1}^{\infty}[a_k,b_k] \right\} = 0.","['real-analysis', 'analysis', 'derivatives', 'alternative-proof', 'absolute-continuity']"
26,"How do I find A in $y = Ax^2 + x + 7000$ (differential calculus, Leibniz's notation)","How do I find A in  (differential calculus, Leibniz's notation)",y = Ax^2 + x + 7000,"So the problem I'm trying to solve is prefaced with this: Earlier we mentioned that NASA claims that the Vomit Comet can make passengers experience weightlessness for about 25 seconds. Let’s check on that claim. To simulate weightlessness (neutral buoyancy) the pilot must execute a parabolic flight path: $y = Ax^2 + Bx + C.$ In Problem #88 you should have found that B and C were 1 and 7000, respectively, so the flight path is $y = Ax^2 + x + 7000$ with A yet to be determined. The pilot will climb at an angle of 45◦ to an altitude of about 7000 meters and then follow this parabolic path to produce a vertical acceleration of d $d^2y/dt^2 = −9.8 m/s^2$ (matching the acceleration due to gravity) and horizontal acceleration of d 2x dt 2 = 0. This will provide neutral buoyancy inside the plane. On the way back down the pilot pulls out of this dive when the altitude returns to 7000 meters. For training purposes this is repeated 40 times. And this is the problem itself: To determine A we need one more fact. At the beginning of the maneuver, the initial airspeed is about 180 meters/second (approximately 400 mph). Use this to determine dx/dt and in turn use this and the fact that $d^2y/dt^2$ = −9.8 to determine A. The solution is $A = -9.8 / 180^2$ . I think I am confused about some of the concepts involved in solving this problem, which are detailed here along with things I know: I'm not sure how to achieve the ""speed"" of things in calculus. I am used to velocity, which you find by taking the derivative of whatever you're working with, and acceleration is found with the second derivative. So, how is speed calculated? I understand that airspeed is the sum of windspeed and groundspeed. In order to visualize this, I drew a right triangle with  the airspeed being the diagonal (which in this case I believe is curved, so I guess it's technically not a right triangle, but for simplicity's sake this is what I used), the bottom being the groundspeed ( $dx/dt$ ) , and the side being the windspeed ( $dy/dt$ ). I am guessing in order to solve this I would need to utilize Pythagorean's theorem, $a^2+b^2=c^2$ , which in this problem is $180^2 = a^2 + b^2$ . What I don't understand about this is how is this relevant to finding A? From what I understand, A is supposed to represent how wide the parabola is. This is what I did to determine $dx/dt$ : $dy = 2Axdx + dx$ $dx = dy/2Ax+1$ $dx/dt = 1/(2Ax+1)$ I don't understand how I would use this and the fact that $d^2y/dt^2 = −9.8 m/s^2$ to solve for A.","So the problem I'm trying to solve is prefaced with this: Earlier we mentioned that NASA claims that the Vomit Comet can make passengers experience weightlessness for about 25 seconds. Let’s check on that claim. To simulate weightlessness (neutral buoyancy) the pilot must execute a parabolic flight path: In Problem #88 you should have found that B and C were 1 and 7000, respectively, so the flight path is with A yet to be determined. The pilot will climb at an angle of 45◦ to an altitude of about 7000 meters and then follow this parabolic path to produce a vertical acceleration of d (matching the acceleration due to gravity) and horizontal acceleration of d 2x dt 2 = 0. This will provide neutral buoyancy inside the plane. On the way back down the pilot pulls out of this dive when the altitude returns to 7000 meters. For training purposes this is repeated 40 times. And this is the problem itself: To determine A we need one more fact. At the beginning of the maneuver, the initial airspeed is about 180 meters/second (approximately 400 mph). Use this to determine dx/dt and in turn use this and the fact that = −9.8 to determine A. The solution is . I think I am confused about some of the concepts involved in solving this problem, which are detailed here along with things I know: I'm not sure how to achieve the ""speed"" of things in calculus. I am used to velocity, which you find by taking the derivative of whatever you're working with, and acceleration is found with the second derivative. So, how is speed calculated? I understand that airspeed is the sum of windspeed and groundspeed. In order to visualize this, I drew a right triangle with  the airspeed being the diagonal (which in this case I believe is curved, so I guess it's technically not a right triangle, but for simplicity's sake this is what I used), the bottom being the groundspeed ( ) , and the side being the windspeed ( ). I am guessing in order to solve this I would need to utilize Pythagorean's theorem, , which in this problem is . What I don't understand about this is how is this relevant to finding A? From what I understand, A is supposed to represent how wide the parabola is. This is what I did to determine : I don't understand how I would use this and the fact that to solve for A.",y = Ax^2 + Bx + C. y = Ax^2 + x + 7000 d^2y/dt^2 = −9.8 m/s^2 d^2y/dt^2 A = -9.8 / 180^2 dx/dt dy/dt a^2+b^2=c^2 180^2 = a^2 + b^2 dx/dt dy = 2Axdx + dx dx = dy/2Ax+1 dx/dt = 1/(2Ax+1) d^2y/dt^2 = −9.8 m/s^2,"['calculus', 'derivatives', 'physics', 'differential']"
27,Find real intervals where a function is nearly constant,Find real intervals where a function is nearly constant,,"A differentiable function $f$ is said to be nearly constant over $I \subset \mathbb{R}$ if $f$ does not vary much over $I$ . For example, one can ask for 𝜖>0 small enough such that $\displaystyle \int_{I} (f(x)-\lambda_I)^2 \, \mathrm{d}\mu(x) < \epsilon$ with $\lambda_I = \frac{1}{\mu(I)} \int_{I} f(x)\,d\mu(x)$ . Does anyone know if there is an article or page that discusses how to find these $I$ intervals for a given $\epsilon$ threshold? More generally I am looking for a (numerical) analysis method that would allow me to know if sometimes a function $f$ is almost constant. Thanks in advance!","A differentiable function is said to be nearly constant over if does not vary much over . For example, one can ask for 𝜖>0 small enough such that with . Does anyone know if there is an article or page that discusses how to find these intervals for a given threshold? More generally I am looking for a (numerical) analysis method that would allow me to know if sometimes a function is almost constant. Thanks in advance!","f I \subset \mathbb{R} f I \displaystyle \int_{I} (f(x)-\lambda_I)^2 \, \mathrm{d}\mu(x) < \epsilon \lambda_I = \frac{1}{\mu(I)} \int_{I} f(x)\,d\mu(x) I \epsilon f","['real-analysis', 'integration', 'derivatives', 'numerical-methods', 'algorithms']"
28,Differentiability of a piecewise function involving $\sin$,Differentiability of a piecewise function involving,\sin,"$ f(x)= \begin{cases}  x^2\sin\left(\frac{\pi}{x}\right)\;,\quad x <  0\\  A\;,\qquad\qquad     x = 0 \\       ax^2+b\;,\qquad x > 0 \end{cases} $ I need to find $A$ , $a$ and $b$ knowing that $f$ is differentiable. I am doing this by using the definition of the derivative and showing that the two one-sided limits at 0 from the negative and positive side exist, and they must be equal to each other. I guess they must also be equal to the derivative of $f(x)$ at $0$ , which would be 0 since any constant function is differentiable. For the negative side I got the limit as $x$ tends to negative $0$ of $x\sin\left(\frac{\pi}{x}\right)$ - $\frac{A}{x}$ . For the positive side I got the limit as $x$ tends to positive $0$ of $ax + \frac{b}{x} - \frac{A}{x}$ . I set these two limits equal to $0$ and each other. I know for the first limit that the limit as $x$ tends to negative $0$ of $x\sin\left(\frac{\pi}{x}\right)=0$ by the sandwich theorem if it is defined as $0$ at $x=0$ . For $\frac{-A}{x}$ to tend to $0$ , A must be 0. Similarly, I ended up with $b$ = $0$ and $a$ any real number. I'm unsure on my method and would like some help with it. Cheers. Would it be best to first use the fact that $f(x)$ must be continuous at $0$ for it to be differentiable at $0$ ? That way I know $x^2\sin\left(\frac{\pi}{x}\right)$ would be $0$ as $x$ tends to $0$ from the left, and that should equal $b$ , meaning $b=0$ . Since the two limits must equal $f(0)$ , that would also give $A = 0$ . I could then deduce that $a$ can be any real number using the differentiability fact ?","I need to find , and knowing that is differentiable. I am doing this by using the definition of the derivative and showing that the two one-sided limits at 0 from the negative and positive side exist, and they must be equal to each other. I guess they must also be equal to the derivative of at , which would be 0 since any constant function is differentiable. For the negative side I got the limit as tends to negative of - . For the positive side I got the limit as tends to positive of . I set these two limits equal to and each other. I know for the first limit that the limit as tends to negative of by the sandwich theorem if it is defined as at . For to tend to , A must be 0. Similarly, I ended up with = and any real number. I'm unsure on my method and would like some help with it. Cheers. Would it be best to first use the fact that must be continuous at for it to be differentiable at ? That way I know would be as tends to from the left, and that should equal , meaning . Since the two limits must equal , that would also give . I could then deduce that can be any real number using the differentiability fact ?","
f(x)=
\begin{cases}
 x^2\sin\left(\frac{\pi}{x}\right)\;,\quad x <  0\\
 A\;,\qquad\qquad     x = 0 \\
      ax^2+b\;,\qquad x > 0
\end{cases}
 A a b f f(x) 0 x 0 x\sin\left(\frac{\pi}{x}\right) \frac{A}{x} x 0 ax + \frac{b}{x} - \frac{A}{x} 0 x 0 x\sin\left(\frac{\pi}{x}\right)=0 0 x=0 \frac{-A}{x} 0 b 0 a f(x) 0 0 x^2\sin\left(\frac{\pi}{x}\right) 0 x 0 b b=0 f(0) A = 0 a","['calculus', 'derivatives']"
29,What are the applications of a more general matrix exponential and its derivative?,What are the applications of a more general matrix exponential and its derivative?,,"The standard, decoupled matrix exponential $\exp(At)$ for a real matrix $A$ and a continuous variable $t$ occurs in the solution to systems of linear ordinary differential equations. But, this wikipedia article has a slightly more generalized structure https://en.wikipedia.org/wiki/Derivative_of_the_exponential_map Instead of $\exp(At),$ we have $\exp(A(t))$ , where $A(t)$ is a general function of the variable $t$ in matrix form. What are the applications of this slightly more generalized matrix form, and its derivative?","The standard, decoupled matrix exponential for a real matrix and a continuous variable occurs in the solution to systems of linear ordinary differential equations. But, this wikipedia article has a slightly more generalized structure https://en.wikipedia.org/wiki/Derivative_of_the_exponential_map Instead of we have , where is a general function of the variable in matrix form. What are the applications of this slightly more generalized matrix form, and its derivative?","\exp(At) A t \exp(At), \exp(A(t)) A(t) t","['matrices', 'derivatives', 'operator-theory']"
30,Prove $f(x+y)=f(x) + y' \nabla f(x) + \frac{1}{2} y' ( \int_0^1 ( \int_0^t \nabla^2 f(x+ \tau y) d \tau)dt)y$ when f twice differentiable over sphere,Prove  when f twice differentiable over sphere,f(x+y)=f(x) + y' \nabla f(x) + \frac{1}{2} y' ( \int_0^1 ( \int_0^t \nabla^2 f(x+ \tau y) d \tau)dt)y,"In the book http://www.athenasc.com/nonlinbook.html by Dimitri P. Bertsekas, there is a proposition A.23(a) in Appendix A without proof that : I am able to prove the other two statements (b) and (c) which use Taylor series but unable to prove this one. How can this statement be proved?","In the book http://www.athenasc.com/nonlinbook.html by Dimitri P. Bertsekas, there is a proposition A.23(a) in Appendix A without proof that : I am able to prove the other two statements (b) and (c) which use Taylor series but unable to prove this one. How can this statement be proved?",,"['derivatives', 'taylor-expansion']"
31,"Show that $f''(x)=e^xf(x)$ with $f(a)=f(b)=0$ makes $f\equiv 0$ $\forall x \in [a,b]$",Show that  with  makes,"f''(x)=e^xf(x) f(a)=f(b)=0 f\equiv 0 \forall x \in [a,b]","Define $f \in C^{2}\left[a,b\right]$ satisfying $f''(x)=e^xf(x)$ . Show that $f''(x)=e^xf(x)$ with $f(a)=f(b)=0$ makes $f\equiv 0$ $\forall x\in [a,b]$ . Actually, I figure out a solution as follows: (just taking about the idea) We can prove a general conclusion：if $f \in C^{2}\left[a,b\right]$ satisfying $f''(x)=g(x)f(x)$ where $g(x) \in C^{0}\left[a,b\right]$ satisfying $g(x)>0$ , and $f(a)=f(b)=0$ , we have $f\equiv 0$ $\forall x \in [a,b]$ . The idea is to prove that if there exists $x_0\in (a,b)$ such that $f(x_0)\ne0$ (let's assume that $f(x_0)>0$ ), we can prove that there exists $x_1\in (a,b)$ such that $f(x_1)>0$ , $f'(x_1)>0$ and $f''(x_1)>0$ . And through this conclusion we can easily get that $f(x)$ will be strictly monotonically increasing in the interval $\left[x_1,b\right]$ . So my questions are: Is there any other solution of this problem? Can we solve this differential equation problem?","Define satisfying . Show that with makes . Actually, I figure out a solution as follows: (just taking about the idea) We can prove a general conclusion：if satisfying where satisfying , and , we have . The idea is to prove that if there exists such that (let's assume that ), we can prove that there exists such that , and . And through this conclusion we can easily get that will be strictly monotonically increasing in the interval . So my questions are: Is there any other solution of this problem? Can we solve this differential equation problem?","f \in C^{2}\left[a,b\right] f''(x)=e^xf(x) f''(x)=e^xf(x) f(a)=f(b)=0 f\equiv 0 \forall x\in [a,b] f \in C^{2}\left[a,b\right] f''(x)=g(x)f(x) g(x) \in C^{0}\left[a,b\right] g(x)>0 f(a)=f(b)=0 f\equiv 0 \forall x \in [a,b] x_0\in (a,b) f(x_0)\ne0 f(x_0)>0 x_1\in (a,b) f(x_1)>0 f'(x_1)>0 f''(x_1)>0 f(x) \left[x_1,b\right]","['real-analysis', 'ordinary-differential-equations']"
32,Distributions of the form $u_f$ are dense in $\mathcal{D}'(\mathbb{R}^n)$,Distributions of the form  are dense in,u_f \mathcal{D}'(\mathbb{R}^n),"Let $f \in L^1_{loc}$ , and define $u_f \in \mathcal{D}'(\mathbb{R}^n)$ to be any distribution of the form $$\langle u_f, \phi\rangle = \int f\phi.$$ I would like to show that the collection of all such $u_f$ is dense in the space of distributions $\mathcal{D}'(\mathbb{R}^n)$ . The motivation for this question came form this Wiki on distributions , which provides a proof of the formula for the differential operator acting on distributions. What bothers me is that the proof relies on representing the distribution as an integral, but clearly not every distribution can be represented this way (take the Dirac- $\Delta$ function for example). The article hints that collection of $u_f$ mentioned above is dense in the space of distributions, so it suffices to consider this case and from there I assume one can make a limiting argument. How would one go about proving this?","Let , and define to be any distribution of the form I would like to show that the collection of all such is dense in the space of distributions . The motivation for this question came form this Wiki on distributions , which provides a proof of the formula for the differential operator acting on distributions. What bothers me is that the proof relies on representing the distribution as an integral, but clearly not every distribution can be represented this way (take the Dirac- function for example). The article hints that collection of mentioned above is dense in the space of distributions, so it suffices to consider this case and from there I assume one can make a limiting argument. How would one go about proving this?","f \in L^1_{loc} u_f \in \mathcal{D}'(\mathbb{R}^n) \langle u_f, \phi\rangle = \int f\phi. u_f \mathcal{D}'(\mathbb{R}^n) \Delta u_f","['functional-analysis', 'derivatives', 'operator-theory', 'distribution-theory']"
33,"Spivak: what precisely is ""bounds on the error"" in Taylor theorem?","Spivak: what precisely is ""bounds on the error"" in Taylor theorem?",,"The following problem is from Chapter 20 of Spivak's Calculus (a) In Problem 11-41 you showed that the equation $x^2=\cos{x}$ has precisely two solutions. Use the third degree Taylor polynomial of $\cos$ to show that the solutions are approximately $\pm \sqrt{2/3}$ , and find bounds on the error. Then use the fifth degree Taylor polynomial to get a better approximation. What precisely does ""bounds on the error"" mean here? Consider first the third degree Taylor polynomial and the use of Taylor's theorem $$\cos{x}=P_{3,0}(x)+R_{3,0}(x)\tag{1}$$ where $P_{3,0}$ is the third degree Taylor Polynomial at $0$ . Then $$P_{3,0}(x)=1-\frac{x^2}{2}$$ $$R_{3,0}(x)=\frac{\cos{t}}{4!}x^4, t\in (0,x)$$ $$\implies |R_{3,0}(x)|\leq \frac{x^4}{4!}$$ If we equate the polynomial $P_{3,0}(x)$ to $x^2$ we obtain $x=\pm\sqrt{\frac{2}{3}}$ . At these particular values of $x$ , we know that the remainder is less than $\frac{1}{54}$ . What this means is that the value of $\cos{\left (\pm\sqrt{\frac{2}{3}}\right )}$ is within $\frac{1}{54}$ of $\frac{2}{3}$ . However, this does not tell us much about how far off we are from the actual $x$ at which $\cos{(x)}=x^2$ . Note that the problem statement makes reference to a previous problem 11-41. In that problem we showed that the two solutions to $x^2=\cos{x}$ are such that $|x|<1$ . Thus $R_{3,0}(x)<\frac{1}{4!}$ . If we equate $(1)$ to $x^2$ , however, and use $R(x)$ to denote $R_{3,0}(x)$ we obtain $$1-\frac{x^2}{2}+R(x)=x^2$$ $$|3x^2-2|\leq 2|R(x)|\leq 2\frac{x^4}{4!}\leq \frac{1}{12}$$ $$\implies \frac{\sqrt{23}}{6}\leq |x|\leq \frac{5}{6}$$ So, based on bounds on the remainder, we were able to determine bounds on $x$ . My questions are what is the ""bounds on the error"" in this case with a third degree Taylor polynomial? in general, how do we know that the value of $x$ we found by equating the Taylor polynomial to $x^2$ is within the intervals we found wherein $f(x)=x^2$ ? In the case above, it happened to be true, ie $\frac{\sqrt{23}}{6}\leq |x|=\left |\pm\sqrt{\frac{2}{3}}\right | \leq \frac{5}{6}$","The following problem is from Chapter 20 of Spivak's Calculus (a) In Problem 11-41 you showed that the equation has precisely two solutions. Use the third degree Taylor polynomial of to show that the solutions are approximately , and find bounds on the error. Then use the fifth degree Taylor polynomial to get a better approximation. What precisely does ""bounds on the error"" mean here? Consider first the third degree Taylor polynomial and the use of Taylor's theorem where is the third degree Taylor Polynomial at . Then If we equate the polynomial to we obtain . At these particular values of , we know that the remainder is less than . What this means is that the value of is within of . However, this does not tell us much about how far off we are from the actual at which . Note that the problem statement makes reference to a previous problem 11-41. In that problem we showed that the two solutions to are such that . Thus . If we equate to , however, and use to denote we obtain So, based on bounds on the remainder, we were able to determine bounds on . My questions are what is the ""bounds on the error"" in this case with a third degree Taylor polynomial? in general, how do we know that the value of we found by equating the Taylor polynomial to is within the intervals we found wherein ? In the case above, it happened to be true, ie","x^2=\cos{x} \cos \pm \sqrt{2/3} \cos{x}=P_{3,0}(x)+R_{3,0}(x)\tag{1} P_{3,0} 0 P_{3,0}(x)=1-\frac{x^2}{2} R_{3,0}(x)=\frac{\cos{t}}{4!}x^4, t\in (0,x) \implies |R_{3,0}(x)|\leq \frac{x^4}{4!} P_{3,0}(x) x^2 x=\pm\sqrt{\frac{2}{3}} x \frac{1}{54} \cos{\left (\pm\sqrt{\frac{2}{3}}\right )} \frac{1}{54} \frac{2}{3} x \cos{(x)}=x^2 x^2=\cos{x} |x|<1 R_{3,0}(x)<\frac{1}{4!} (1) x^2 R(x) R_{3,0}(x) 1-\frac{x^2}{2}+R(x)=x^2 |3x^2-2|\leq 2|R(x)|\leq 2\frac{x^4}{4!}\leq \frac{1}{12} \implies \frac{\sqrt{23}}{6}\leq |x|\leq \frac{5}{6} x x x^2 f(x)=x^2 \frac{\sqrt{23}}{6}\leq |x|=\left |\pm\sqrt{\frac{2}{3}}\right | \leq \frac{5}{6}","['calculus', 'integration', 'derivatives', 'taylor-expansion']"
34,Derivative of AX'B with respect to X,Derivative of AX'B with respect to X,,"Since $\frac{\partial \left( AXB \right)}{\partial X}={{B}^{T}}\otimes A$ , What is $\frac{\partial \left( A{{X}^{T}}B \right)}{\partial X}$ ? Thank you for your help.","Since , What is ? Thank you for your help.",\frac{\partial \left( AXB \right)}{\partial X}={{B}^{T}}\otimes A \frac{\partial \left( A{{X}^{T}}B \right)}{\partial X},"['linear-algebra', 'matrices', 'derivatives']"
35,Compute the limit $\lim_{x\to 0}\left(\frac{(1+x)^\frac{1}{x}}{e}\right)^\frac{1}{x}$ [closed],Compute the limit  [closed],\lim_{x\to 0}\left(\frac{(1+x)^\frac{1}{x}}{e}\right)^\frac{1}{x},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question $\lim_{x\to 0}\left(\frac{(1+x)^\frac{1}{x}}{e}\right)^\frac{1}{x}$. I tried adding $1$ and subtracting $1$ so I can use $1^\infty$ case.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question $\lim_{x\to 0}\left(\frac{(1+x)^\frac{1}{x}}{e}\right)^\frac{1}{x}$. I tried adding $1$ and subtracting $1$ so I can use $1^\infty$ case.",,"['calculus', 'limits']"
36,Spivak: Understanding computation of an upper bound for remainder of Taylor polynomial approximation of $\log{(1+x)}$.,Spivak: Understanding computation of an upper bound for remainder of Taylor polynomial approximation of .,\log{(1+x)},"There is a particular calculation in Spivak's Calculus that I had to think about a lot. I think I finally understood it but I am not sure about why it was done this way. The calculation is related to another recent question that I asked about computing the remainder term in a Taylor polynomial approximation to $\log{(1+x)}$ . The conclusion of that question was how to compute the remainder term in the following expression, for $x\geq 0$ $$\log{(1+x)}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+...+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}, t\in (0,x)$$ where $$|R_{n,0}(x)|=\left | \frac{(-1)^n}{n+1}t^{n+1} \right |\leq \frac{x^{n+1}}{n+1} \tag{1}$$ and $R_{n,0}(x)$ is the remainder term. Some work was involved in coming up with the remainder above (which is obtained from the integral form of the remainder), instead of just applying the formula for the Lagrange remainder (which I will do below). Then Spivak says and there is a slightly more complicated estimate when $-1<x<0$ (Problem 16). For this function the remainder term can be made as small as desired by choosing $n$ sufficiently large, provided that $-1<x\leq 1$ . The ""slightly more complicated estimate when $-1<x<0$ turns out to be $$\left | \frac{(-1)^n}{n+1}t^{n+1} \right |\leq \frac{x^{n+1}}{(1+x)(n+1)}\tag{2}$$ Let's see what happens if we use the Lagrange form of the remainder. We have $$R_{n,0}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-0)^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n \cdot n!}{(1+t)^{n+1}}\cdot \frac{1}{(n+1)!}\cdot x^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n x^{n+1}}{(1+t)^{n+1}(n+1)}, t\in (0,x)\tag{3}$$ the magnitude of which is smaller than $\frac{|x|^{n+1}}{n+1}$ , and we can see this also can be made as small as desired. Why didn't Spivak just use this remainder to make the points he made, namely that for $-1<x\leq 1$ the remainder is decreasing in $n$ ? He could have, correct? Now, the reason we need to come up with an upper bound for the remainder is so that we have an expression that is dependent on quantities we know ( $x$ and $n$ ), and not quantities we don't know ( $t$ ), correct? Here is how we obtain $(2)$ : For $-1<x\leq t\leq 0$ we have $$0<1+x\leq 1+t\leq 1$$ $$0\leq \frac{1}{1+t}\leq \frac{1}{1+x}$$ $$\left | \int_0^x\frac{u^n}{1+u}du\right |\leq \int_0^x \frac{|u|^n}{1+x}du\leq \frac{|x|^{n+1}}{(1+x)(n+1)}$$ But is the following calculation also correct? EDIT: no, the following is incorrect. The integral form of the remainder is $\int_0^x (-1)^n \frac{u^n}{1+u}du$ Assume $-1<x\leq 0$ , $n$ is odd and let $f(x)=-\frac{x^{n+1}}{n+1}$ . Then $$f(0)=0>\int_x^0 \frac{u^n}{1+u}du>\int_x^0 u^n du = -\frac{x^{n+1}}{n+1}=f(x)\tag{4}$$ And the Intermediate Value Theorem tells us that there exists some $t\in (x,0)$ such that $-\frac{t^{n+1}}{n+1}=\int_x^0 \frac{u^n}{1+u}du$ . Thus, the remainder term is $$R_{n,0}=(-1)^{n} \int_0^x \frac{u^n}{1+u}du=(-1)\cdot \frac{t^{n+1}}{n+1}<0$$ $$-\frac{x^{n+1}}{n+1}<-\frac{t^{n+1}}{n+1}=R_{n,0}(x)<0$$ Hence $$|R_{n,0}(x)|<\frac{|x|^{n+1}}{n+1}$$ Now assume $n$ is even. Then $$f(0)=0<\int_x^0\frac{u^n}{1+u}du<\int_x^0 u^ndu=-\frac{x^{n+1}}{n+1}=f(x)$$ Again, the Intermediate Value Theorem tells us that there exists some $t\in (x,0)$ such that $-\frac{t^{n+1}}{n+1}=\int_x^0 \frac{u^n}{1+u}du$ . Therefore, $$R_{n,0}=(-1)^{n} \int_0^x \frac{u^n}{1+u}du= \frac{t^{n+1}}{n+1}<0$$ $$\frac{x^{n+1}}{n+1}<\frac{t^{n+1}}{n+1}=R_{n,0}(x)<0$$ Hence $$|R_{n,0}(x)|<\frac{|x|^{n+1}}{n+1}$$ EDIT: as per Ted Shifrin's comment, these calculations don't, in fact, work. The reason is that the integrals are all for $|x|<1$ , and so $\frac{1}{1+u}>1$ . Thus, inequalities like in $(4)$ don't work.","There is a particular calculation in Spivak's Calculus that I had to think about a lot. I think I finally understood it but I am not sure about why it was done this way. The calculation is related to another recent question that I asked about computing the remainder term in a Taylor polynomial approximation to . The conclusion of that question was how to compute the remainder term in the following expression, for where and is the remainder term. Some work was involved in coming up with the remainder above (which is obtained from the integral form of the remainder), instead of just applying the formula for the Lagrange remainder (which I will do below). Then Spivak says and there is a slightly more complicated estimate when (Problem 16). For this function the remainder term can be made as small as desired by choosing sufficiently large, provided that . The ""slightly more complicated estimate when turns out to be Let's see what happens if we use the Lagrange form of the remainder. We have the magnitude of which is smaller than , and we can see this also can be made as small as desired. Why didn't Spivak just use this remainder to make the points he made, namely that for the remainder is decreasing in ? He could have, correct? Now, the reason we need to come up with an upper bound for the remainder is so that we have an expression that is dependent on quantities we know ( and ), and not quantities we don't know ( ), correct? Here is how we obtain : For we have But is the following calculation also correct? EDIT: no, the following is incorrect. The integral form of the remainder is Assume , is odd and let . Then And the Intermediate Value Theorem tells us that there exists some such that . Thus, the remainder term is Hence Now assume is even. Then Again, the Intermediate Value Theorem tells us that there exists some such that . Therefore, Hence EDIT: as per Ted Shifrin's comment, these calculations don't, in fact, work. The reason is that the integrals are all for , and so . Thus, inequalities like in don't work.","\log{(1+x)} x\geq 0 \log{(1+x)}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+...+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}, t\in (0,x) |R_{n,0}(x)|=\left | \frac{(-1)^n}{n+1}t^{n+1} \right |\leq \frac{x^{n+1}}{n+1} \tag{1} R_{n,0}(x) -1<x<0 n -1<x\leq 1 -1<x<0 \left | \frac{(-1)^n}{n+1}t^{n+1} \right |\leq \frac{x^{n+1}}{(1+x)(n+1)}\tag{2} R_{n,0}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-0)^{n+1}, t\in (0,x) =\frac{(-1)^n \cdot n!}{(1+t)^{n+1}}\cdot \frac{1}{(n+1)!}\cdot x^{n+1}, t\in (0,x) =\frac{(-1)^n x^{n+1}}{(1+t)^{n+1}(n+1)}, t\in (0,x)\tag{3} \frac{|x|^{n+1}}{n+1} -1<x\leq 1 n x n t (2) -1<x\leq t\leq 0 0<1+x\leq 1+t\leq 1 0\leq \frac{1}{1+t}\leq \frac{1}{1+x} \left | \int_0^x\frac{u^n}{1+u}du\right |\leq \int_0^x \frac{|u|^n}{1+x}du\leq \frac{|x|^{n+1}}{(1+x)(n+1)} \int_0^x (-1)^n \frac{u^n}{1+u}du -1<x\leq 0 n f(x)=-\frac{x^{n+1}}{n+1} f(0)=0>\int_x^0 \frac{u^n}{1+u}du>\int_x^0 u^n du = -\frac{x^{n+1}}{n+1}=f(x)\tag{4} t\in (x,0) -\frac{t^{n+1}}{n+1}=\int_x^0 \frac{u^n}{1+u}du R_{n,0}=(-1)^{n} \int_0^x \frac{u^n}{1+u}du=(-1)\cdot \frac{t^{n+1}}{n+1}<0 -\frac{x^{n+1}}{n+1}<-\frac{t^{n+1}}{n+1}=R_{n,0}(x)<0 |R_{n,0}(x)|<\frac{|x|^{n+1}}{n+1} n f(0)=0<\int_x^0\frac{u^n}{1+u}du<\int_x^0 u^ndu=-\frac{x^{n+1}}{n+1}=f(x) t\in (x,0) -\frac{t^{n+1}}{n+1}=\int_x^0 \frac{u^n}{1+u}du R_{n,0}=(-1)^{n} \int_0^x \frac{u^n}{1+u}du= \frac{t^{n+1}}{n+1}<0 \frac{x^{n+1}}{n+1}<\frac{t^{n+1}}{n+1}=R_{n,0}(x)<0 |R_{n,0}(x)|<\frac{|x|^{n+1}}{n+1} |x|<1 \frac{1}{1+u}>1 (4)","['calculus', 'integration', 'derivatives', 'taylor-expansion']"
37,Finding the pattern of Lagrange Inversion Formula for $x+\sin x$,Finding the pattern of Lagrange Inversion Formula for,x+\sin x,"Today I read about the Lagrange Inversion Theorem, which to restate: $$f\text{ is analytic at point }a, f'(a) \neq 0 \\ \implies f^{-1}(x)=a+\sum_{n=1}^\infty g_n\frac{(x-f(a))^n}{n!}, \\ g_n=\lim_{t\rightarrow a}\bigg(\frac{d^{n-1}}{dt^{n-1}}\bigg[\Big(\frac{t-a}{f(t)-f(a)}\Big)^n\bigg]\bigg)$$ To test it, I chose $f(x)=x+\sin x$ and $a=0$ . The formula then became: $$f^{-1}(x)=\sum_{n=1}^\infty g_n\frac{x^n}{n!},g_n=\lim_{t\rightarrow 0}\bigg(\frac{d^{n-1}}{dt^{n-1}}\bigg[\Big(\frac{t}{t+\sin t}\Big)^n\bigg]\bigg)$$ I computed $g_n$ for small $n$ first. The first odd ones include $$g_1=\frac{1}{2},g_3=\frac{1}{16},g_5=\frac{1}{16},g_7=\frac{43}{256},g_9=\frac{223}{256},g_{11}=\frac{60623}{8192},g_{13}=\frac{764783}{8192};$$ { $g_2,g_4,\dots,g_{14}$ }  have all shown to be zero. The only noticeable pattern I can see in $g_n$ for odd n is that for the denominators, which I'll call $d_n$ , is that $d_{n+4}=2^{(n+11)/4}d_n$ ; I cannot see a pattern for the numerators. How do I find the formula for the coefficients? Or does no closed formula (other than the limit above) exist?","Today I read about the Lagrange Inversion Theorem, which to restate: To test it, I chose and . The formula then became: I computed for small first. The first odd ones include { }  have all shown to be zero. The only noticeable pattern I can see in for odd n is that for the denominators, which I'll call , is that ; I cannot see a pattern for the numerators. How do I find the formula for the coefficients? Or does no closed formula (other than the limit above) exist?","f\text{ is analytic at point }a, f'(a) \neq 0 \\ \implies f^{-1}(x)=a+\sum_{n=1}^\infty g_n\frac{(x-f(a))^n}{n!}, \\ g_n=\lim_{t\rightarrow a}\bigg(\frac{d^{n-1}}{dt^{n-1}}\bigg[\Big(\frac{t-a}{f(t)-f(a)}\Big)^n\bigg]\bigg) f(x)=x+\sin x a=0 f^{-1}(x)=\sum_{n=1}^\infty g_n\frac{x^n}{n!},g_n=\lim_{t\rightarrow 0}\bigg(\frac{d^{n-1}}{dt^{n-1}}\bigg[\Big(\frac{t}{t+\sin t}\Big)^n\bigg]\bigg) g_n n g_1=\frac{1}{2},g_3=\frac{1}{16},g_5=\frac{1}{16},g_7=\frac{43}{256},g_9=\frac{223}{256},g_{11}=\frac{60623}{8192},g_{13}=\frac{764783}{8192}; g_2,g_4,\dots,g_{14} g_n d_n d_{n+4}=2^{(n+11)/4}d_n","['sequences-and-series', 'limits', 'derivatives', 'lagrange-inversion']"
38,Formula for the $n^{\text{th}}$ Derivative of the Reciprocal of a Polynomial,Formula for the  Derivative of the Reciprocal of a Polynomial,n^{\text{th}},"Is there a formula for the $n^{\text{th}}$ derivative of the reciprocal of a polynomial $f(x) = \sum_{k=0}^{K} a_k x^k$ ? Question 1: In particular, does there exist a formula for $$g_n(x) = \frac{d^n}{dx^n} \left(\frac{1}{\sum_{k=0}^{K} a_k x^k}\right)$$ In my particular case a partial fraction decomposition cannot be found for the denominator. If anyone knows of the answer to the first question, I am also curious about an equation of the following form. Question 2: Does there also exist a formula (in terms of $n$ ) for the following derivative? $$\hat{g}_n(x) = \frac{d^n}{dx^n} \left(\frac{x}{\sum_{k=0}^{K} a_k x^k}\right)$$","Is there a formula for the derivative of the reciprocal of a polynomial ? Question 1: In particular, does there exist a formula for In my particular case a partial fraction decomposition cannot be found for the denominator. If anyone knows of the answer to the first question, I am also curious about an equation of the following form. Question 2: Does there also exist a formula (in terms of ) for the following derivative?",n^{\text{th}} f(x) = \sum_{k=0}^{K} a_k x^k g_n(x) = \frac{d^n}{dx^n} \left(\frac{1}{\sum_{k=0}^{K} a_k x^k}\right) n \hat{g}_n(x) = \frac{d^n}{dx^n} \left(\frac{x}{\sum_{k=0}^{K} a_k x^k}\right),"['derivatives', 'induction']"
39,Does the shortest distance between two curves exist along common normal?,Does the shortest distance between two curves exist along common normal?,,"I read here that the shortest distance between two differentiable non-intersecting curves is along their common normal. But if we consider $y^2=x+1$ , $y^2 = x$ , their common normal is actually the largest possible distance: Maybe this is because the minimum distance between these two curves never exists in the first place... Is this a condition we should add in the statement? Thanks","I read here that the shortest distance between two differentiable non-intersecting curves is along their common normal. But if we consider , , their common normal is actually the largest possible distance: Maybe this is because the minimum distance between these two curves never exists in the first place... Is this a condition we should add in the statement? Thanks",y^2=x+1 y^2 = x,"['calculus', 'derivatives', 'optimization', 'analytic-geometry']"
40,What justifies interchanging partial derivatives here?,What justifies interchanging partial derivatives here?,,"I am following a text on turbulence by Frisch. Using the summation convention, he writes the Navier-Stokes equations as $$\partial_t v_i + v_j\partial_j v_i = - \partial_i p + \nu \partial_{jj}v_i\\ \partial_iv_i = 0.$$ where $\nu$ is a constant. He claims that taking the divergence above results in a Poisson equation. Taking the divergence results in : $$\partial_i \partial_t v_i + \partial_i v_j\partial_j v_i = -\partial_i \partial_i p + \partial_i \nu \partial_{jj}v_i.$$ Assuming we may interchange partial derivatives, we have $$\partial_t \partial_i v_i + \partial_i v_j\partial_j v_i = -\partial_i \partial_i p +  \nu \partial_{jj}\partial_iv_i.$$ By the divergence free condition the first and last terms vanish, leaving $$\partial_i v_j\partial_j v_i = -\partial_i \partial_i p \\ \implies\partial_{ij}(v_iv_j) = -\partial_{ii}p.$$ The question I have now is what justifies interchanging the partial derivatives as we did? Also as an aside, I am still grasping the summation notation. How can we think of the term $\partial_{ij}(v_iv_j)$ ? Since the indices are not repeated adjacently, is it not summed?","I am following a text on turbulence by Frisch. Using the summation convention, he writes the Navier-Stokes equations as where is a constant. He claims that taking the divergence above results in a Poisson equation. Taking the divergence results in : Assuming we may interchange partial derivatives, we have By the divergence free condition the first and last terms vanish, leaving The question I have now is what justifies interchanging the partial derivatives as we did? Also as an aside, I am still grasping the summation notation. How can we think of the term ? Since the indices are not repeated adjacently, is it not summed?","\partial_t v_i + v_j\partial_j v_i = - \partial_i p + \nu \partial_{jj}v_i\\
\partial_iv_i = 0. \nu \partial_i \partial_t v_i + \partial_i v_j\partial_j v_i = -\partial_i \partial_i p + \partial_i \nu \partial_{jj}v_i. \partial_t \partial_i v_i + \partial_i v_j\partial_j v_i = -\partial_i \partial_i p +  \nu \partial_{jj}\partial_iv_i. \partial_i v_j\partial_j v_i = -\partial_i \partial_i p \\ \implies\partial_{ij}(v_iv_j) = -\partial_{ii}p. \partial_{ij}(v_iv_j)","['real-analysis', 'derivatives', 'partial-differential-equations', 'partial-derivative', 'fluid-dynamics']"
41,$n$-th derivative of inverse of a function wrt the function.,-th derivative of inverse of a function wrt the function.,n,"I wanted to write the taylor series of the inverse of a bijective function around a point $y_0$ with the coefficients being in terms of the $\underline{\text{ derivatives of the function itself  }}$ at the point $f(y=y_0)=x_0$ , i.e: $$f^{-1}(y)=\sum_{n\geq 0}\left.\frac{d^nf^{-1}(y)}{dy^n}\right|_{y=y_0}\frac{(y-y_0)^n}{n!}\\ =\sum_{n\geq 0}o_n\left(\frac{dy}{df^{-1}(y)},\ldots,\frac{d^ny}{d(f^{-1}(y))^n}\right)_{f^{-1}(y=y_0)=x_0}\frac{(y-y_0)^n}{n!}$$ This helps me find the coefficients of the taylor series of an inverse function directly if I know the coefficients of the taylor series itself. And thus, I can invert a function easily. Moving forward, I will use $y$ for the function and $x$ for its inverse and let $y_n$ be $\frac{d^ny}{dx^n}$ and the same for $x_n$ . We know: $$y_1=\frac{1}{x_1}$$ Differentiating both sides wrt $y$ : $$\frac{dy_1}{dy}=-\frac{x_2}{x_1^2}$$ Then, multiplying both sides by $y_1$ and using the chain rule: $$\frac{dy_1}{dy}y_1=y_2=-\frac{x_2}{x_1^3}$$ You can imagine how deducing the $n$ -th derivative this way will get easily chaotic. So, instead I decided to take the $n$ -th derivative of the equation $y_1x_1=1$ wrt $y$ then look for a pattern. Taking the first derivative this way goes as follows: $$y_1x_1=1\\ (y_1)_yx_1+y_1x_2=0\\ y_2x_1+y_1^2x_2=0$$ The first four derivatives go as follows: $$y_1x_1=1\\ y_2x_1+y_1^2x_2=0\\ y_3x_1+3y_1y_2x_2+y_1^3x_3=0\\ y_4x_1+4y_1y_3x_2+3y_2^2x_2+3y_1^2y_2x_3+y_1^4x_4=0$$ I have this expression for the pattern that is easy to prove by induction: $$\frac{d^n}{dy^n}(x_1y_1)=\sum_{o_1p_1+o_2p_2=n}\left(n-\left|o_1-o_2\right|\right)y_{o_1}^{p_1}y_{o_2}^{p_2}x_{p_1+p_2}$$ However, I don't know how to use this to deduce the $n$ -th derivative in terms of $x_k$ 's only.","I wanted to write the taylor series of the inverse of a bijective function around a point with the coefficients being in terms of the at the point , i.e: This helps me find the coefficients of the taylor series of an inverse function directly if I know the coefficients of the taylor series itself. And thus, I can invert a function easily. Moving forward, I will use for the function and for its inverse and let be and the same for . We know: Differentiating both sides wrt : Then, multiplying both sides by and using the chain rule: You can imagine how deducing the -th derivative this way will get easily chaotic. So, instead I decided to take the -th derivative of the equation wrt then look for a pattern. Taking the first derivative this way goes as follows: The first four derivatives go as follows: I have this expression for the pattern that is easy to prove by induction: However, I don't know how to use this to deduce the -th derivative in terms of 's only.","y_0 \underline{\text{ derivatives of the function itself  }} f(y=y_0)=x_0 f^{-1}(y)=\sum_{n\geq 0}\left.\frac{d^nf^{-1}(y)}{dy^n}\right|_{y=y_0}\frac{(y-y_0)^n}{n!}\\
=\sum_{n\geq 0}o_n\left(\frac{dy}{df^{-1}(y)},\ldots,\frac{d^ny}{d(f^{-1}(y))^n}\right)_{f^{-1}(y=y_0)=x_0}\frac{(y-y_0)^n}{n!} y x y_n \frac{d^ny}{dx^n} x_n y_1=\frac{1}{x_1} y \frac{dy_1}{dy}=-\frac{x_2}{x_1^2} y_1 \frac{dy_1}{dy}y_1=y_2=-\frac{x_2}{x_1^3} n n y_1x_1=1 y y_1x_1=1\\
(y_1)_yx_1+y_1x_2=0\\
y_2x_1+y_1^2x_2=0 y_1x_1=1\\
y_2x_1+y_1^2x_2=0\\
y_3x_1+3y_1y_2x_2+y_1^3x_3=0\\
y_4x_1+4y_1y_3x_2+3y_2^2x_2+3y_1^2y_2x_3+y_1^4x_4=0 \frac{d^n}{dy^n}(x_1y_1)=\sum_{o_1p_1+o_2p_2=n}\left(n-\left|o_1-o_2\right|\right)y_{o_1}^{p_1}y_{o_2}^{p_2}x_{p_1+p_2} n x_k","['derivatives', 'taylor-expansion']"
42,Trivial question on derivative of quadratic form of vector-valued function,Trivial question on derivative of quadratic form of vector-valued function,,"This seems like a trivial question but I am currently stuck and cannot see what I am doing wrong. So let us consider a function $f(x) : \mathbb{R}^d \rightarrow \mathbb{R}^d$ . I want to compute the derivative w.r.t. $x \in \mathbb{R}^d$ of an expression that contains a quadratic form of $f(x)$ $$I = f(x)^{\top} C f(x) . $$ Here $C$ is a $d\times d$ matrix. By taking the derivative w.r.t to the vector $x$ we have $$ \frac{\partial I}{\partial x} = 2C f(x) \cdot \nabla f(x),  $$ where $\nabla f(x)$ denotes the Jacobian of $f$ which will be a $d \times d$ matrix. Now  my problem is that the dimensions of the matrices in the last expression do not match: We have $C: d\times d$ , $f(x): d\times 1$ , and $\nabla f(x): d \times d$ . So the last two dimensions do not add up. What I am doing wrong? Is the correct derivative $$ \frac{\partial I}{\partial x} = \nabla f(x) 2 C f(x)  ,  $$ or $$ \frac{\partial I}{\partial x} = ( 2 C f(x) )^{\top} \cdot \nabla f(x)  $$","This seems like a trivial question but I am currently stuck and cannot see what I am doing wrong. So let us consider a function . I want to compute the derivative w.r.t. of an expression that contains a quadratic form of Here is a matrix. By taking the derivative w.r.t to the vector we have where denotes the Jacobian of which will be a matrix. Now  my problem is that the dimensions of the matrices in the last expression do not match: We have , , and . So the last two dimensions do not add up. What I am doing wrong? Is the correct derivative or","f(x) : \mathbb{R}^d \rightarrow \mathbb{R}^d x \in \mathbb{R}^d f(x) I = f(x)^{\top} C f(x) .  C d\times d x  \frac{\partial I}{\partial x} = 2C f(x) \cdot \nabla f(x),   \nabla f(x) f d \times d C: d\times d f(x): d\times 1 \nabla f(x): d \times d  \frac{\partial I}{\partial x} = \nabla f(x) 2 C f(x)  ,    \frac{\partial I}{\partial x} = ( 2 C f(x) )^{\top} \cdot \nabla f(x)  ","['linear-algebra', 'matrices', 'derivatives', 'partial-derivative', 'matrix-calculus']"
43,Does tangent exist at $x=1$ to $y(x) =\ln\left(\frac{1+\sqrt{1-x^2}}x\right)-\sqrt{1-x^2}$? [closed],Does tangent exist at  to ? [closed],x=1 y(x) =\ln\left(\frac{1+\sqrt{1-x^2}}x\right)-\sqrt{1-x^2},"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question $y(x) =\ln\left(\frac{1+\sqrt{1-x^2}}x\right)-\sqrt{1-x^2}$ is defined on $(0,1]$ . Does there exist it's tangent at the end point $x=1$ ? Left hand derivative of $y(x)$ exists at the point at $x=1$ and it is continuous on $(0,1]$ . But I can not understand if tangent to $y(x)$ at $x=1$ exists.I think any straight line which passes through $(1,0)$ will be a tangent to $x=1$ to $y(x)$ because it just touches at that point. Can anyone please help me ?",Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question is defined on . Does there exist it's tangent at the end point ? Left hand derivative of exists at the point at and it is continuous on . But I can not understand if tangent to at exists.I think any straight line which passes through will be a tangent to to because it just touches at that point. Can anyone please help me ?,"y(x) =\ln\left(\frac{1+\sqrt{1-x^2}}x\right)-\sqrt{1-x^2} (0,1] x=1 y(x) x=1 (0,1] y(x) x=1 (1,0) x=1 y(x)","['real-analysis', 'calculus', 'derivatives']"
44,What is the geometric meaning that the directional derivative is positive?,What is the geometric meaning that the directional derivative is positive?,,"We have the function $$f(x,y)=x^2y+xy^2+\frac{1}{3}y^3-4y$$ I have calculated the gradient $$\nabla f=\binom{2xy+y^2}{x^2+2xy+y^2-4}$$ The directional derivative of $f$ at the point $(1,1)$ at the direction $v=\binom{2}{1}$ is equal to $\frac{6}{\sqrt{5}}>0$ . What is the geometric meaning that the directional derivative is positive?",We have the function I have calculated the gradient The directional derivative of at the point at the direction is equal to . What is the geometric meaning that the directional derivative is positive?,"f(x,y)=x^2y+xy^2+\frac{1}{3}y^3-4y \nabla f=\binom{2xy+y^2}{x^2+2xy+y^2-4} f (1,1) v=\binom{2}{1} \frac{6}{\sqrt{5}}>0","['calculus', 'derivatives', 'partial-derivative', 'vector-analysis']"
45,Second derivative of a matrix function,Second derivative of a matrix function,,"I am trying to find the $\textit{second}$ derivative of $X \mapsto (F \circ G) (X)$ , where $G(X)=\frac{AXA}{\operatorname{tr}(AXA)}$ and $F(X)=\operatorname{tr}\left((CXC)^\frac{1}{2}\right)$ are functions of the positive matrix X. The matrices A and C are constant and also positive. I computed the first derivative and got $\frac{\partial (F \circ G)}{\partial X}=\frac{1}{2}\operatorname{tr}\left( AXA\right)^{-1}\left(AC(CG(X)C)^{-\frac{1}{2}}CA -\operatorname{tr} \left((CG(X)C)^\frac{1}{2} \right)A^2\right)$ notably thanks to: Derivative of $\mbox{Tr} \left(\left(AXA\right)^{\frac{1}{2}}\right)$ with respect to $X$ Derivative of $X \mapsto \frac{AXA}{\operatorname{Tr} \left( AXA \right )}$ However, to derive again, I am stuck with the inverse square root $(CG(X)C)^{-\frac{1}{2}}$ . Would you know how to deal with it?","I am trying to find the derivative of , where and are functions of the positive matrix X. The matrices A and C are constant and also positive. I computed the first derivative and got notably thanks to: Derivative of $\mbox{Tr} \left(\left(AXA\right)^{\frac{1}{2}}\right)$ with respect to $X$ Derivative of $X \mapsto \frac{AXA}{\operatorname{Tr} \left( AXA \right )}$ However, to derive again, I am stuck with the inverse square root . Would you know how to deal with it?",\textit{second} X \mapsto (F \circ G) (X) G(X)=\frac{AXA}{\operatorname{tr}(AXA)} F(X)=\operatorname{tr}\left((CXC)^\frac{1}{2}\right) \frac{\partial (F \circ G)}{\partial X}=\frac{1}{2}\operatorname{tr}\left( AXA\right)^{-1}\left(AC(CG(X)C)^{-\frac{1}{2}}CA -\operatorname{tr} \left((CG(X)C)^\frac{1}{2} \right)A^2\right) (CG(X)C)^{-\frac{1}{2}},"['matrices', 'derivatives', 'matrix-calculus']"
46,Suggested books on Leibniz notation (differentials)?,Suggested books on Leibniz notation (differentials)?,,"I'm a freshman in Computer Engineering (but will probably switch to Math very soon) and I've followed some course in Analysis 1 and 2. I found the concept of differentials really interesting, but I've never really grasped the real meaning of the Leibniz notation, such as in: $$\int {x\ dx} \ \ \ \ \ \ \ \ \ \frac{dy}{dx}$$ In integrals I think of it as a multiplication between a quantity $x$ and another one $dx$ . This machine is building small rectangles that will add up to an area. I've studied the integral definitions and I know that there is definitely more behind it (particularly sums), but this is a useful representation that I have in my mind that allow me to conceptualize problems, especially in phisics. In derivatives I see $\frac{dy}{dx}$ as a division, between something that I want to calculate ( $dy$ ), that is getting smaller and smaller, and an inifnitesimal distance $dx$ . But, especially for derivates, I don't really get how is it possible for the ratio of two infinitesimals to output a real number. I know that this is involved with limits, and limits can do these magical things, but it's just not intuitive for me. Not considering the last doubt, I think that there is much more about the concept of differential that I do not really know. Thus, considering our beautiful math sections we have in our university library, I'm searching for book suggestions about this awesome topic to learn something more about it! Thanks.","I'm a freshman in Computer Engineering (but will probably switch to Math very soon) and I've followed some course in Analysis 1 and 2. I found the concept of differentials really interesting, but I've never really grasped the real meaning of the Leibniz notation, such as in: In integrals I think of it as a multiplication between a quantity and another one . This machine is building small rectangles that will add up to an area. I've studied the integral definitions and I know that there is definitely more behind it (particularly sums), but this is a useful representation that I have in my mind that allow me to conceptualize problems, especially in phisics. In derivatives I see as a division, between something that I want to calculate ( ), that is getting smaller and smaller, and an inifnitesimal distance . But, especially for derivates, I don't really get how is it possible for the ratio of two infinitesimals to output a real number. I know that this is involved with limits, and limits can do these magical things, but it's just not intuitive for me. Not considering the last doubt, I think that there is much more about the concept of differential that I do not really know. Thus, considering our beautiful math sections we have in our university library, I'm searching for book suggestions about this awesome topic to learn something more about it! Thanks.",\int {x\ dx} \ \ \ \ \ \ \ \ \ \frac{dy}{dx} x dx \frac{dy}{dx} dy dx,"['calculus', 'integration', 'derivatives', 'book-recommendation', 'differential']"
47,Is there a way to find/closely estimate the value that the Mean Value Theorem promises?,Is there a way to find/closely estimate the value that the Mean Value Theorem promises?,,"For background, the Mean Value Theorem says that if $f\in C^1([a,b])$ , then there exists $c\in(a,b)$ such that $$f'(c) = \frac{f(b) - f(a)}{b-a}.$$ My question is, how does $c$ vary as a function of properties of $f$ , $a$ , and $b$ ? For some $f$ , we have an easy closed form for $c$ .  Take $f(x) = x^3$ , $a = 0$ , and $b$ an arbitrary $x$ (wlog, assume $x > 0$ ), for example.  Then we have the equation $$\frac{x^3}{x} = 3c^2,$$ so that $c = \frac{x}{\sqrt{3}}$ . But what if the function is more complicated? Obviously we could just solve numerically, but is there a theorem that can tell us anything interesting about $c$ ?  For example, a theorem that tells us when $c$ is a polynomial function of $x$ ?  Or an algebraic function?  Or how close to $x / 2$ we can expect $c$ to be?  How good an estimate $c = x/2$ is?  How higher derivatives affect whether $c$ is closer to one or the other endpoint?  Etc. Running list of insights We may assume $a = 0$ and $f(0) = 0$ without loss of generality because derivatives are well-defined up to addition by a constant and phase shifts do not affect the underlying problem.","For background, the Mean Value Theorem says that if , then there exists such that My question is, how does vary as a function of properties of , , and ? For some , we have an easy closed form for .  Take , , and an arbitrary (wlog, assume ), for example.  Then we have the equation so that . But what if the function is more complicated? Obviously we could just solve numerically, but is there a theorem that can tell us anything interesting about ?  For example, a theorem that tells us when is a polynomial function of ?  Or an algebraic function?  Or how close to we can expect to be?  How good an estimate is?  How higher derivatives affect whether is closer to one or the other endpoint?  Etc. Running list of insights We may assume and without loss of generality because derivatives are well-defined up to addition by a constant and phase shifts do not affect the underlying problem.","f\in C^1([a,b]) c\in(a,b) f'(c) = \frac{f(b) - f(a)}{b-a}. c f a b f c f(x) = x^3 a = 0 b x x > 0 \frac{x^3}{x} = 3c^2, c = \frac{x}{\sqrt{3}} c c x x / 2 c c = x/2 c a = 0 f(0) = 0","['calculus', 'derivatives', 'taylor-expansion', 'mean-value-theorem']"
48,Where does the constant term $+1$ go when simplifying this integral in eq (2)?,Where does the constant term  go when simplifying this integral in eq (2)?,+1,"I've been looking at this breakdown that deals with taking the gradient of an expectation w.r.t the distribution the expectation is sampling from. On equation 2 is where I get stuck. Where does the $+1$ on the RHS of eq (2) go? I was/am expecting it to be: $\int{ (\log p_\theta(y) - \log q_\phi(x) +1 )  \nabla_{\phi}  q_\phi(x)  dz }$ . I've my answer and should add that both $p(x,z)$ and $q_\phi (z|x)$ are probabilities. By integrating over one (to get 1), then taking the gradient (to get 0), my $+1$ dissapears. $$\begin{align}  \nabla_{\phi} E_{q_\phi(z|x)} \left [ \log p(x, z) - \log q_\phi(z|x) \right] &= \nabla_{\phi} \int{\log p(x, z)  q_\phi(z|x) dz} -\nabla_{\phi} \int{q_\phi(z|x) \log q_\phi(z|x) dz } \tag{1}\\ &=\int{\log p(x, z) \nabla_{\phi} q_\phi(z|x) dz} - \int{(\log q_\phi(z|x) +1) \nabla_{\phi}  q_\phi(z|x)  dz } \tag{2}\\ &= \int{ (\log p(x, z) - \log q_\phi(z|x) )  \nabla_{\phi}  q_\phi(z|x)  dz } \tag{3} \end{align}$$","I've been looking at this breakdown that deals with taking the gradient of an expectation w.r.t the distribution the expectation is sampling from. On equation 2 is where I get stuck. Where does the on the RHS of eq (2) go? I was/am expecting it to be: . I've my answer and should add that both and are probabilities. By integrating over one (to get 1), then taking the gradient (to get 0), my dissapears.","+1 \int{ (\log p_\theta(y) - \log q_\phi(x) +1 )  \nabla_{\phi}  q_\phi(x)  dz } p(x,z) q_\phi (z|x) +1 \begin{align} 
\nabla_{\phi} E_{q_\phi(z|x)} \left [ \log p(x, z) - \log q_\phi(z|x) \right] &= \nabla_{\phi} \int{\log p(x, z)  q_\phi(z|x) dz} -\nabla_{\phi} \int{q_\phi(z|x) \log q_\phi(z|x) dz } \tag{1}\\
&=\int{\log p(x, z) \nabla_{\phi} q_\phi(z|x) dz} - \int{(\log q_\phi(z|x) +1) \nabla_{\phi} 
q_\phi(z|x)  dz } \tag{2}\\
&= \int{ (\log p(x, z) - \log q_\phi(z|x) )  \nabla_{\phi}  q_\phi(z|x)  dz } \tag{3}
\end{align}","['integration', 'derivatives', 'expected-value', 'density-function']"
49,Conditions for chain rule for Gateaux derivatives,Conditions for chain rule for Gateaux derivatives,,"Let $X,Y,Z$ be locally convex topological vector spaces over $\mathbb R$ (not necessarily Banach), $D_X \subseteq X$ , $D_Y \subseteq Y$ and let $f \colon D_X \to D_Y$ , $g \colon D_Y \to Z$ . Let us assume that $f$ is Gateaux-differentiable in some $x \in D_X$ and $g$ is Gateaux-differentiable in $f(x)$ . Which are the least well-known additional conditions for the chain rule to hold? By the chain rule I mean the assertion that $g \circ f$ is Gateaux-differentiable and $\mathrm d(g \circ f)(x;\cdot) = \mathrm d g(f(x);\mathrm d f(x;\cdot))$ . Wikipedia says that continuity of the derivatives of $f$ and $g$ is required, but it does not specify whether this means that only $\mathrm df \colon D_X \times X \to Y$ should be continuous or, more than that, $x \mapsto \mathrm df(x,\cdot) \in L(X,Y)$ should be continuous (and, of course, the same for $g$ ). And is it generally assumed that $D_X$ and $D_Y$ are open or is it enough to know that they are neighborhoods of $x$ and $f(x)$ ? Please also help me to find a citable reference. I looked into some books, but have not found this chain rule. Since this question got no answers after a few days, I also posted it on mathoverflow .","Let be locally convex topological vector spaces over (not necessarily Banach), , and let , . Let us assume that is Gateaux-differentiable in some and is Gateaux-differentiable in . Which are the least well-known additional conditions for the chain rule to hold? By the chain rule I mean the assertion that is Gateaux-differentiable and . Wikipedia says that continuity of the derivatives of and is required, but it does not specify whether this means that only should be continuous or, more than that, should be continuous (and, of course, the same for ). And is it generally assumed that and are open or is it enough to know that they are neighborhoods of and ? Please also help me to find a citable reference. I looked into some books, but have not found this chain rule. Since this question got no answers after a few days, I also posted it on mathoverflow .","X,Y,Z \mathbb R D_X \subseteq X D_Y \subseteq Y f \colon D_X \to D_Y g \colon D_Y \to Z f x \in D_X g f(x) g \circ f \mathrm d(g \circ f)(x;\cdot) = \mathrm d g(f(x);\mathrm d f(x;\cdot)) f g \mathrm df \colon D_X \times X \to Y x \mapsto \mathrm df(x,\cdot) \in L(X,Y) g D_X D_Y x f(x)","['functional-analysis', 'derivatives', 'reference-request', 'chain-rule', 'gateaux-derivative']"
50,Derivative of Integral with respect to a function,Derivative of Integral with respect to a function,,"I have the following function $$ f(\tilde{x}) = \int^\tilde{x}_0 x dH(x) $$ and I want to find $f'(\tilde{x})$ . Here, $H(x)$ denotes a cumulative density function. So far, it is clear to me that I can use the Second Fundamental Theorem of Calculus, i.e., if a function $g(x)$ is continuous and $G$ defined for a all $x \in [a,b]$ as the integral $$ G(x) = \int^x_a g(t) dt, $$ then $G$ is differentiable on $(a,b)$ and $$ G'(x) = \frac{d}{dx} G(x) = \frac{d}{dx} \int^x_a g(t) dt = (G(x)-G(a))'=G'(x)=g(x). $$ In the problem above I do not integrate with respect to a variable but to a function. Let $F(\tilde{x})$ be the integral of $f(\tilde{x})$ , then I get $$ f'(\tilde{x}) = \frac{d}{dx} \int^\tilde{x}_0 x dH(x) = \left[f(H(\tilde{x}))-f(H(0))\right]' =  $$ There I am somewhat stuck and not sure how to proceed. Any help would be appreciated!","I have the following function and I want to find . Here, denotes a cumulative density function. So far, it is clear to me that I can use the Second Fundamental Theorem of Calculus, i.e., if a function is continuous and defined for a all as the integral then is differentiable on and In the problem above I do not integrate with respect to a variable but to a function. Let be the integral of , then I get There I am somewhat stuck and not sure how to proceed. Any help would be appreciated!","
f(\tilde{x}) = \int^\tilde{x}_0 x dH(x)
 f'(\tilde{x}) H(x) g(x) G x \in [a,b] 
G(x) = \int^x_a g(t) dt,
 G (a,b) 
G'(x) = \frac{d}{dx} G(x) = \frac{d}{dx} \int^x_a g(t) dt = (G(x)-G(a))'=G'(x)=g(x).
 F(\tilde{x}) f(\tilde{x}) 
f'(\tilde{x}) = \frac{d}{dx} \int^\tilde{x}_0 x dH(x) = \left[f(H(\tilde{x}))-f(H(0))\right]' = 
","['integration', 'derivatives']"
51,Using gradient descent with cross entropy loss [closed],Using gradient descent with cross entropy loss [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question In the logistic regression, for an intercept $$\beta_0\in R$$ ,parameter vector $$\beta=\left(\beta_0,\beta_1,...,\beta_p\right)\in R^p$$ , target $$y_i\in\left\{0,1\right\}$$ , and feature vector $$x_i=\left(x_{i1},x_{i2},...,x_{ip}\right)^T\in R^p$$ for i = 1, . . . , n, the (l2-regularized) log-loss that we will work with is: $$L\left(\beta_{0,}\beta\right)=\frac{1}{2}\beta^T\beta+\frac{\lambda}{n}\Sigma\left[y_i\ln\left(\frac{1}{\sigma\left(\beta_0+\beta^Tx_i\right)}\right)+\left(1-y_i\right)\ln\left(\frac{1}{1-\sigma\left(\beta_0+\beta^Tx_i\right)}\right)\right]$$ where $$\sigma\left(z\right)=\left(1+e^{-z}\right)^{-1}$$ How to calculate the gradient?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question In the logistic regression, for an intercept ,parameter vector , target , and feature vector for i = 1, . . . , n, the (l2-regularized) log-loss that we will work with is: where How to calculate the gradient?","\beta_0\in R \beta=\left(\beta_0,\beta_1,...,\beta_p\right)\in R^p y_i\in\left\{0,1\right\} x_i=\left(x_{i1},x_{i2},...,x_{ip}\right)^T\in R^p L\left(\beta_{0,}\beta\right)=\frac{1}{2}\beta^T\beta+\frac{\lambda}{n}\Sigma\left[y_i\ln\left(\frac{1}{\sigma\left(\beta_0+\beta^Tx_i\right)}\right)+\left(1-y_i\right)\ln\left(\frac{1}{1-\sigma\left(\beta_0+\beta^Tx_i\right)}\right)\right] \sigma\left(z\right)=\left(1+e^{-z}\right)^{-1}",['derivatives']
52,How to integrate $\int x'(t)\ dx(t)$ and similar?,How to integrate  and similar?,\int x'(t)\ dx(t),"I just wanted to ask how to integrate the one in the title and also another one which is very similar. Also, I don't know why but WolframAlpha just treats the derivatives as contants. Integrals: $$\int x'(t) dx(t)\ ;$$ $$\int x'(t)^2 dx(t)\ .$$ I don't really know but using $x'(t)=\frac{dx}{dt}(t)$ and isolating differentials may help.","I just wanted to ask how to integrate the one in the title and also another one which is very similar. Also, I don't know why but WolframAlpha just treats the derivatives as contants. Integrals: I don't really know but using and isolating differentials may help.",\int x'(t) dx(t)\ ; \int x'(t)^2 dx(t)\ . x'(t)=\frac{dx}{dt}(t),"['calculus', 'integration', 'derivatives']"
53,(complex)Let $f: D \subset \mathbb R^2 \to \mathbb R^2$ be a function defined on an open subset $D \subset \mathbb R^2$...,(complex)Let  be a function defined on an open subset ...,f: D \subset \mathbb R^2 \to \mathbb R^2 D \subset \mathbb R^2,"Let $f: D \subset \mathbb R^2 \to \mathbb R^2$ be a function defined on an open subset $D \subset \mathbb R^2$ . Remember that $f$ is said to be differentiable (in the Frechét sense) at (x_0, y_0) \in D if there is a linear map $Df(x_0, y_0) : \mathbb R^2 \to \mathbb R^2$ such what $$\lim_{(h,k) \to (0,0)} \frac{f((x_0,y_0) + (h,k)) – f(x_0, y_0) – Df(x_0, y_0)( h,k)}{\vert\vert(h,k)\vert\vert} = 0$$ Show that $f$ is differentiable in the complex sense at $z_0 = x_0 + iy_0$ if and only if $f$ is differentiable in the Frechét sense and $Df(x_0, y_0)$ defines a linear $\mathbb C$ mapping of $\mathbb C$ into $\mathbb C$ i.e. $Df(x_0, y_0)(z + \lambda z' ) = Df(x_0, y_0)z + \lambda Df(x_0,y_0) z', \forall z, z', \lambda \in \mathbb C$ . $f$ is differentiable in $z_0$ if there is $f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h}$ . $\lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} - f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0) - f'(z_0)h}{h}$ . $f'(z_0)(z + \lambda z') = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} (z + \lambda z')=  \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h}(z + \lambda z') = \lim_{h \to 0} \frac{f(z_0 + h)z +f(z_0 + h)\lambda Z'}{h} - \frac{f(z_0)z + f(z_0)\lambda z'}{h} = \lim_{h \to 0}\frac{f(z_0 + h)z - f(z_0)z}{h} + \frac{f(z_0 + h)\lambda Z'-\lambda z'}{h} = f'(z_0)(z) + f'(z_0)(\lambda z')$ That's what I managed to come up with, is it acceptable? Grateful for the attention.","Let be a function defined on an open subset . Remember that is said to be differentiable (in the Frechét sense) at (x_0, y_0) \in D if there is a linear map such what Show that is differentiable in the complex sense at if and only if is differentiable in the Frechét sense and defines a linear mapping of into i.e. . is differentiable in if there is . . That's what I managed to come up with, is it acceptable? Grateful for the attention.","f: D \subset \mathbb R^2 \to \mathbb R^2 D \subset \mathbb R^2 f Df(x_0, y_0) : \mathbb R^2 \to \mathbb R^2 \lim_{(h,k) \to (0,0)} \frac{f((x_0,y_0) + (h,k)) – f(x_0, y_0) – Df(x_0, y_0)( h,k)}{\vert\vert(h,k)\vert\vert} = 0 f z_0 = x_0 + iy_0 f Df(x_0, y_0) \mathbb C \mathbb C \mathbb C Df(x_0, y_0)(z + \lambda z' ) = Df(x_0, y_0)z + \lambda Df(x_0,y_0) z', \forall z, z', \lambda \in \mathbb C f z_0 f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} - f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0) - f'(z_0)h}{h} f'(z_0)(z + \lambda z') = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} (z + \lambda z')=  \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h}(z + \lambda z') = \lim_{h \to 0} \frac{f(z_0 + h)z +f(z_0 + h)\lambda Z'}{h} - \frac{f(z_0)z + f(z_0)\lambda z'}{h} = \lim_{h \to 0}\frac{f(z_0 + h)z - f(z_0)z}{h} + \frac{f(z_0 + h)\lambda Z'-\lambda z'}{h} = f'(z_0)(z) + f'(z_0)(\lambda z')","['limits', 'derivatives', 'complex-numbers', 'solution-verification', 'problem-solving']"
54,Wrong example of derivative of algebra in Sadri Hassani's mathematical physics book?,Wrong example of derivative of algebra in Sadri Hassani's mathematical physics book?,,"The definition of the derivative of an algebra: Definition 3.4.1 A vector space endomorphism $\mathbf D$ : $\mathcal{A} \rightarrow \mathcal{A}$ is called a derivation on $\mathcal{A}$ if it has the additional property $$ \mathbf{D}(\mathbf{a b})=\lceil\mathbf{D}(\mathbf{a})\rceil \mathbf{b}+\mathbf{a}[\mathbf{D}(\mathbf{b})\rceil $$ And he gave an example that all the algebra $C^r(a,b)$ , i.e., all the function in interval $(a,b)$ have derivatives up to order $r$ with multiplication defined as $(f g)(t) \equiv f(t) g(t) \quad \forall t \in(a, b)$ . And the  ordinary differentiation is a derivative of $C^r(a,b)$ . My question is , differentiation will map a function $f$ to $f^\prime$ while might have derivative up to order $r-1$ instead of $r$ , hence the map is not endomorphism, then how can we say that this map is an example of the derivative of an algebra?","The definition of the derivative of an algebra: Definition 3.4.1 A vector space endomorphism : is called a derivation on if it has the additional property And he gave an example that all the algebra , i.e., all the function in interval have derivatives up to order with multiplication defined as . And the  ordinary differentiation is a derivative of . My question is , differentiation will map a function to while might have derivative up to order instead of , hence the map is not endomorphism, then how can we say that this map is an example of the derivative of an algebra?","\mathbf D \mathcal{A} \rightarrow \mathcal{A} \mathcal{A} 
\mathbf{D}(\mathbf{a b})=\lceil\mathbf{D}(\mathbf{a})\rceil \mathbf{b}+\mathbf{a}[\mathbf{D}(\mathbf{b})\rceil
 C^r(a,b) (a,b) r (f g)(t) \equiv f(t) g(t) \quad \forall t \in(a, b) C^r(a,b) f f^\prime r-1 r","['abstract-algebra', 'derivatives']"
55,Is there a darboux fuction that doesn't have a primitive?,Is there a darboux fuction that doesn't have a primitive?,,"We know that if f is a differentiable fuction, then f' is a Darboux fuction due to the Darboux theorem. However, the Darboux theorem isn't know as the ""characterization of fuctions with primitive theorem"" so I guess there have to be at least one fuction that is Darboux that doesn't have a primitive. I'm trying to find an example but I don't get anything. Is this an still open problem? Is there any property in between ""darboux"" and continuous that can caracterizate if a fuction has a primitive?","We know that if f is a differentiable fuction, then f' is a Darboux fuction due to the Darboux theorem. However, the Darboux theorem isn't know as the ""characterization of fuctions with primitive theorem"" so I guess there have to be at least one fuction that is Darboux that doesn't have a primitive. I'm trying to find an example but I don't get anything. Is this an still open problem? Is there any property in between ""darboux"" and continuous that can caracterizate if a fuction has a primitive?",,"['calculus', 'integration', 'derivatives']"
56,log - derivative transformation.,log - derivative transformation.,,"That problem actually came out from Advanced Macroeconomics exercise, where one needs to transform an elasticity of substitution between capital and effective labor in the gross production function. Formulating problem mathematically, there is a function $F = F(x,y)$ . Then one constructs a derivative/function: $$\frac{\partial(x/y)}{\partial(F_x/F_y)}\cdot\frac{F_x/F_y}{x/y} = \frac{\partial \log(x/y)}{\partial \log(F_x/F_y)},$$ and ""transform"" this derivative into: $$\frac{\partial \log(x/F)}{\partial \log(F_x)}. $$ Here we assume that the function has all the needed partial derivatives and they are continuous in a point of interest. So the question is how one may prove this transformation, and, probably, which fact one may use to prove this transition? I would appreciate any idea on how to proceed. corrections: Function $F$ should be homothetic .","That problem actually came out from Advanced Macroeconomics exercise, where one needs to transform an elasticity of substitution between capital and effective labor in the gross production function. Formulating problem mathematically, there is a function . Then one constructs a derivative/function: and ""transform"" this derivative into: Here we assume that the function has all the needed partial derivatives and they are continuous in a point of interest. So the question is how one may prove this transformation, and, probably, which fact one may use to prove this transition? I would appreciate any idea on how to proceed. corrections: Function should be homothetic .","F = F(x,y) \frac{\partial(x/y)}{\partial(F_x/F_y)}\cdot\frac{F_x/F_y}{x/y} = \frac{\partial \log(x/y)}{\partial \log(F_x/F_y)}, \frac{\partial \log(x/F)}{\partial \log(F_x)}.
 F","['real-analysis', 'analysis', 'derivatives']"
57,"Reference Request: Does there exist a definition for a ""total finite difference"" operator?","Reference Request: Does there exist a definition for a ""total finite difference"" operator?",,"I was just wondering if there is a total finite difference operator defined in a way that is analogous to the total derivative operator. As we know, for a function of one variable, we might define the finite difference by $$\Delta f(x_n) = f(x_{n + 1}) - f(x_n),$$ and we could conceivably treat this as a derivative operation if we consider an equally spaced grid so that $$\frac{\Delta_h f(x)}{\Delta_h\,x} = \frac{f(x + h) - f(x)}{(x + h) - x}.$$ My question is if anyone has extended this to a ""Total Finite Difference"" operator in the same way that we have a total derivative for $f(x,y)$ given by something like $$D_{\Delta} f(x_n,y_m) = f_{\Delta x} \Delta x_n + f_{\Delta y}\Delta y_m.$$ where $f_{\Delta x} = f(x_{n + 1}, y) - f(x_n)$ and $\Delta x_n = x_{n + 1} - x_n$ (or something like this). If this does exist, does anyone know if this type of operation plays well with summation by parts? Any help is appreciated. Thank you!","I was just wondering if there is a total finite difference operator defined in a way that is analogous to the total derivative operator. As we know, for a function of one variable, we might define the finite difference by and we could conceivably treat this as a derivative operation if we consider an equally spaced grid so that My question is if anyone has extended this to a ""Total Finite Difference"" operator in the same way that we have a total derivative for given by something like where and (or something like this). If this does exist, does anyone know if this type of operation plays well with summation by parts? Any help is appreciated. Thank you!","\Delta f(x_n) = f(x_{n + 1}) - f(x_n), \frac{\Delta_h f(x)}{\Delta_h\,x} = \frac{f(x + h) - f(x)}{(x + h) - x}. f(x,y) D_{\Delta} f(x_n,y_m) = f_{\Delta x} \Delta x_n + f_{\Delta y}\Delta y_m. f_{\Delta x} = f(x_{n + 1}, y) - f(x_n) \Delta x_n = x_{n + 1} - x_n","['discrete-mathematics', 'derivatives', 'finite-differences', 'discrete-calculus']"
58,Non-integrable derivative of increasing function,Non-integrable derivative of increasing function,,"Is there an example of an increasing and differentiable function $f$ on an interval $[a,b]$ such that its derivative is not Riemann integrable on the same interval?",Is there an example of an increasing and differentiable function on an interval such that its derivative is not Riemann integrable on the same interval?,"f [a,b]","['real-analysis', 'integration', 'derivatives', 'examples-counterexamples']"
59,"Spivak Calculus, Ch 11, Problem 11a): Proof that if $f$ weakly convex, then [$f$ convex $\iff$ graph of $f$ contains no straight line segments].","Spivak Calculus, Ch 11, Problem 11a): Proof that if  weakly convex, then [ convex  graph of  contains no straight line segments].",f f \iff f,"Call a function $f$ weakly convex on an interval if for $a<b<c$ in this interval we have $$\frac{f(x)-f(a)}{x-a} \leq \frac{f(b)-f(a)}{b-a}$$ (a) Show that a weakly convex function is convex if and only if its graph contains no straight line segments. (Sometimes a weakly convex function is simply called ""convex"", while convex functions in our sense are called ""strictly convex""). The proof I came up with differs from the solution manual, and I would like to verify if it is correct. Proof By assumption, $f$ is weakly convex. Assume $f$ is also (strictly) convex. Assume the graph of $f$ contains a straight line segment. Let $a<b<c$ be points on the line segment portion of $f$ 's graph. Then, $$\frac{f(b)-f(a)}{b-a}=\frac{f(c)-f(a)}{c-a}$$ But this contradicts our assumption that $f$ is strictly convex, which implies that $$\frac{f(b)-f(a)}{b-a}<\frac{f(c)-f(a)}{c-a}$$ Therefore, $f$ convex $\implies$ graph of $f$ contains no straight line segments. Now assume, for proof by contradiction, that the graph of $f$ contains no straight line segments. For any $a<b<c$ , we know that $\frac{f(b)-f(a)}{b-a} \leq \frac{f(c)-f(a)}{c-a}$ Assume that for some $a,b,$ and $c$ such that $a<b<c$ we have $\frac{f(b)-f(a)}{b-a}=\frac{f(c)-f(a)}{c-a}=\alpha$ . Let $x_2 \in (b,c)$ . Then by weak convexity $$\frac{f(x_2)-f(b)}{x_2-b}\leq \alpha\tag{1}$$ Let $x_1 \in (a,b)$ . Assume for proof by contradiction that $$\frac{f(x_1)-f(a)}{x_1-a}<\alpha\tag{2}$$ Then, $\alpha<\frac{f(b)-f(x_1)}{b-x_1} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \leq \frac{f(x_2)-f(b)}{x_2-b}$ . But this contradicts $(1)$ . Therefore, by proof by contradiction, $$\forall x, x \in (a,b) \implies \frac{f(x_1)-f(a)}{x_1-a}=\alpha$$ But then the graph of $f$ contains a straight line segment between $a$ and $b$ , which contradicts our initial assumption of no straight lines in the graph of $f$ . Therefore, by proof by contradiction, $$\frac{f(b)-f(a)}{b-a}<\frac{f(c)-f(a)}{c-a}$$ That is, $f$ is convex. Therefore, graph of $f$ contains no straight line segments $\implies f$ convex.","Call a function weakly convex on an interval if for in this interval we have (a) Show that a weakly convex function is convex if and only if its graph contains no straight line segments. (Sometimes a weakly convex function is simply called ""convex"", while convex functions in our sense are called ""strictly convex""). The proof I came up with differs from the solution manual, and I would like to verify if it is correct. Proof By assumption, is weakly convex. Assume is also (strictly) convex. Assume the graph of contains a straight line segment. Let be points on the line segment portion of 's graph. Then, But this contradicts our assumption that is strictly convex, which implies that Therefore, convex graph of contains no straight line segments. Now assume, for proof by contradiction, that the graph of contains no straight line segments. For any , we know that Assume that for some and such that we have . Let . Then by weak convexity Let . Assume for proof by contradiction that Then, . But this contradicts . Therefore, by proof by contradiction, But then the graph of contains a straight line segment between and , which contradicts our initial assumption of no straight lines in the graph of . Therefore, by proof by contradiction, That is, is convex. Therefore, graph of contains no straight line segments convex.","f a<b<c \frac{f(x)-f(a)}{x-a} \leq \frac{f(b)-f(a)}{b-a} f f f a<b<c f \frac{f(b)-f(a)}{b-a}=\frac{f(c)-f(a)}{c-a} f \frac{f(b)-f(a)}{b-a}<\frac{f(c)-f(a)}{c-a} f \implies f f a<b<c \frac{f(b)-f(a)}{b-a} \leq \frac{f(c)-f(a)}{c-a} a,b, c a<b<c \frac{f(b)-f(a)}{b-a}=\frac{f(c)-f(a)}{c-a}=\alpha x_2 \in (b,c) \frac{f(x_2)-f(b)}{x_2-b}\leq \alpha\tag{1} x_1 \in (a,b) \frac{f(x_1)-f(a)}{x_1-a}<\alpha\tag{2} \alpha<\frac{f(b)-f(x_1)}{b-x_1} \leq \frac{f(x_2)-f(x_1)}{x_2-x_1} \leq \frac{f(x_2)-f(b)}{x_2-b} (1) \forall x, x \in (a,b) \implies \frac{f(x_1)-f(a)}{x_1-a}=\alpha f a b f \frac{f(b)-f(a)}{b-a}<\frac{f(c)-f(a)}{c-a} f f \implies f","['calculus', 'derivatives', 'solution-verification']"
60,"Maclaurin series, find the tenth derivative","Maclaurin series, find the tenth derivative",,"The problem is as follows: Find the Maclaurin series of $$\begin{cases} \frac{\sin(x)}{x},& x \neq 0 \\ 1,& x=0 \end{cases}$$ and then find $f^{10}(0)$ . I figured out the series, if $x\neq 0$ then it is $$\sum_{k=0}^{\infty} (-1)^k \frac{x^{2k}}{(2k+1)!}$$ but I have a question about the tenth derivative. I know how to find it in the typical Maclaurin series, but because the definition of the function says that if $x=0, f(x)=1$ . Does that mean that $f^{(10)}(0)=1$ too? If no, can I have a quick clue, what should I do?","The problem is as follows: Find the Maclaurin series of and then find . I figured out the series, if then it is but I have a question about the tenth derivative. I know how to find it in the typical Maclaurin series, but because the definition of the function says that if . Does that mean that too? If no, can I have a quick clue, what should I do?","\begin{cases} \frac{\sin(x)}{x},& x \neq 0 \\ 1,& x=0 \end{cases} f^{10}(0) x\neq 0 \sum_{k=0}^{\infty} (-1)^k \frac{x^{2k}}{(2k+1)!} x=0, f(x)=1 f^{(10)}(0)=1","['sequences-and-series', 'derivatives', 'taylor-expansion']"
61,Doubt regarding finding the gradient of of a scalar field,Doubt regarding finding the gradient of of a scalar field,,"I am new to vector calculus. I watched few you tube videos and came to the conclusion that directional derivative is something like slope with direction and its value is scalar while gradient of a scalar is the vector which has  magnitude and direction as that of directional derivative.Is my understanding correct.Please correct me if i am wrong.It would be great if you can point me some wonderful simple resource (any text book or video resource ) on which i can better understand the concept Now i stumbled upon this exercise in one of my college text books I am uploading the photo here I hardly understand this question what does $||\nabla f(x,y,z) || $ mean ..how come k becomes $\frac{5}{3}$ ..Which formula of is used here.Any help and suggestion will be highly appreciated .Thanks and regards in advance. Now",I am new to vector calculus. I watched few you tube videos and came to the conclusion that directional derivative is something like slope with direction and its value is scalar while gradient of a scalar is the vector which has  magnitude and direction as that of directional derivative.Is my understanding correct.Please correct me if i am wrong.It would be great if you can point me some wonderful simple resource (any text book or video resource ) on which i can better understand the concept Now i stumbled upon this exercise in one of my college text books I am uploading the photo here I hardly understand this question what does mean ..how come k becomes ..Which formula of is used here.Any help and suggestion will be highly appreciated .Thanks and regards in advance. Now,"||\nabla f(x,y,z) ||  \frac{5}{3}","['calculus', 'derivatives', 'vector-analysis', 'vector-fields', 'slope']"
62,"Spivak's Calculus, Ch. 11, **68: $f(x)=\alpha x+x^2\sin{1/x}$ for $x \neq 0$, $f(0)=0$. Prove $f$ is not increasing in an interval around $0$.","Spivak's Calculus, Ch. 11, **68:  for , . Prove  is not increasing in an interval around .",f(x)=\alpha x+x^2\sin{1/x} x \neq 0 f(0)=0 f 0,"Two asterisks on a problem in Spivak's Calculus signal a potentially very tricky problem. I solved the following two asterisk problem from chapter 11, ""Significance of the Derivative"". I am wondering about the correctness of the steps to solve item c and d. This will be a long question, hopefully someone has patience :). **68. Let $f(x)=\alpha x+x^2\sin{1/x}$ for $x \neq 0$ , and let $f(0)=0$ . In order to find the sign of $f'(x)$ when $\alpha \geq 1$ it is necessary to decide if $2x\sin{1/x}-\cos{1/x}$ is $<-1$ for any numbers $x$ close to $0$ . It is a little more convenient to consider the function $g(y)=\frac{2\sin{y}}{y}-\cos{y}$ for $y \neq 0$ ; we want to know if $g(y)<-1$ for large $y$ . This question is quite delicate; the most significant part of $g(y)$ is $-\cos{y}$ , which does reach the value $-1$ , but this happens only when $\sin{y}=0$ , and it is not at all clear whether $g$ itself can have values $<-1$ . The obvious approach to this problem is to find the local minimum values of $g$ . Unfortunately, it is impossible to solve the equation $g'(y)=0$ explicitly, so more ingenuity is required. (a) Show that if $g'(y)=0$ , then $$\cos{(y)}=\sin{(y)}\frac{2-y^2}{2y}$$ and conclude that $$g(y)=\sin{(y)}\frac{2+y^2}{2y}$$ (b) Now show that if $g'(y)=0$ , then $$\sin^2{(y)}=\frac{4y^2}{4+y^4}$$ and conclude that $$|g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}$$ (c) Using the fact that $\frac{2+y^2}{\sqrt{4+y^4}}>1$ , show that if $\alpha=1$ , then $f$ is not increasing in an interval around $0$ . (d) Using the fact that $\lim\limits_{y \to \infty}  \frac{2+y^2}{\sqrt{4+y^4}}=1$ , show that if $\alpha>1$ , then $f$ is increasing in some interval around $0$ . Here is my solution a) We know that $$g(y)=\frac{2\sin{y}}{y}-\cos{(y)}, \text{ for } y \neq 0\tag{1}$$ $$g(y)=$$ $$g'(y)=\frac{2\cos{y}\cdot y -2\sin{y}}{y^2}+\sin{y}=0$$ $$\implies \cos{(y)}=\sin{(y)}\frac{2-y^2}{2y}\tag{2}$$ Now we substitute this back into $(1)$ to obtain $$g(y)=\sin{(y)}\frac{2+y^2}{2y}$$ b) Starting from $(2)$ , we square both sides to obtain $$\sin^2{(y)}=\frac{4y^2}{4+y^4}\tag{3}$$ Then $$|g(y)|=\left | \sin{(y)}\frac{2+y^2}{2y} \right |=|\sin{(y)}| \left | \frac{2+y^2}{2y} \right |$$ And after we sub in $(3)$ we obtain $$\implies |g(y)|=\frac{2+y^2}{\sqrt{4+y^4}}$$ c) My question is about this part. The problem statement claims that $\frac{2+y^2}{\sqrt{4+y^4}}>1$ . Proof $$|2+y^2|=2+y^2= \sqrt{(2+y^2)^2}$$ $$=\sqrt{4+4y^2+y^4}>\sqrt{4+y^2}, \text{ for } y \neq 0$$ $$\implies \frac{2+y^2}{\sqrt{4+y^2}}>1$$ $$\blacksquare$$ Assume $\alpha=1$ . $$x \neq 0 \implies f(x)=x+x^2\sin{(1/x)}$$ $$f'(x)=1+2x\sin{(1/x)}-\cos{(1/x)}$$ $$=1+g(1/x)$$ Since we know that $f'>0$ for some point in any interval around $0$ (we can conclude this based on a previous problem), if we can show that $f'<0$ for some point in any interval around $0$ then we will have shown that $f$ is not increasing in any interval around $0$ . Let $y=-2\pi k, k=1,2,...$ . Then $g(-2\pi k)=-\cos{(2\pi k)}=-1$ . Recall that $g'(y)=\frac{2\cos{y}\cdot y -2\sin{y}}{y^2}+\sin{y}$ . Therefore $$g'(-2\pi k)=-\frac{1}{\pi k}<0$$ Hence there is some $c \in (-2\pi k-\delta, -2\pi k) \land g(y)<g(-2\pi k)=-1$ Finally, this means that $$\exists c, c \in (-\frac{1}{2 \pi k}, -\frac{1}{2\pi k+\delta}) \land g(1/x)<-1$$ Hence $f'(c)<-1$ . d) $\lim\limits_{y \to \infty} \frac{2+y^2}{\sqrt{4+y^4}}=1$ means $$\forall \epsilon>0\ \exists N>0\ \forall y\ y>N\implies \left | \frac{2+y^2}{\sqrt{4+y^4}} -1\right | < \epsilon$$ $$\implies 1-\epsilon<|g(y)|<1+\epsilon$$ Assume $\alpha>1$ . $$x \neq 0 \implies f(x)=\alpha x+x^2\sin{(1/x)}$$ $$f'(x)=\alpha+2x\sin{(1/x)}-\cos{(1/x)}$$ $$=\alpha+g(1/x)>\alpha-|g(1/x)|$$ Let $\epsilon=\frac{\alpha-1}{2}$ . Then $$1-\frac{\alpha-1}{2}<|g(1/x)|<1+\frac{\alpha-1}{2}$$ $$\frac{3-\alpha}{2}<|g(1/x)|<\frac{1+\alpha}{2}$$ Therefore $$f'(x)>\alpha-\frac{1+\alpha}{2}=\frac{\alpha-1}{2}>0$$ That is $$\exists N>0\ \forall y\ y>N \implies f'(x)>0$$ $$\exists N>0\ \forall x\ x<\frac{1}{N} \implies f'(x)>0$$ Therefore, $f$ is increasing on $(0, \frac{1}{N})$ . Similarly, because $\lim\limits_{y \to -\infty} \frac{2+y^2}{\sqrt{4+y^4}}=1$ we can show with very similar steps that $$\exists N<0\ \forall x\ x>\frac{1}{N} \implies f'(x)>0$$ Also, $f'(0)=\alpha$ , so we can say that $$\exists N>0\ \forall x\ |x|<\frac{1}{N} \implies f'(x)>0$$ Therefore $f$ is increasing on $(-\frac{1}{N}, \frac{1}{N})$ .","Two asterisks on a problem in Spivak's Calculus signal a potentially very tricky problem. I solved the following two asterisk problem from chapter 11, ""Significance of the Derivative"". I am wondering about the correctness of the steps to solve item c and d. This will be a long question, hopefully someone has patience :). **68. Let for , and let . In order to find the sign of when it is necessary to decide if is for any numbers close to . It is a little more convenient to consider the function for ; we want to know if for large . This question is quite delicate; the most significant part of is , which does reach the value , but this happens only when , and it is not at all clear whether itself can have values . The obvious approach to this problem is to find the local minimum values of . Unfortunately, it is impossible to solve the equation explicitly, so more ingenuity is required. (a) Show that if , then and conclude that (b) Now show that if , then and conclude that (c) Using the fact that , show that if , then is not increasing in an interval around . (d) Using the fact that , show that if , then is increasing in some interval around . Here is my solution a) We know that Now we substitute this back into to obtain b) Starting from , we square both sides to obtain Then And after we sub in we obtain c) My question is about this part. The problem statement claims that . Proof Assume . Since we know that for some point in any interval around (we can conclude this based on a previous problem), if we can show that for some point in any interval around then we will have shown that is not increasing in any interval around . Let . Then . Recall that . Therefore Hence there is some Finally, this means that Hence . d) means Assume . Let . Then Therefore That is Therefore, is increasing on . Similarly, because we can show with very similar steps that Also, , so we can say that Therefore is increasing on .","f(x)=\alpha x+x^2\sin{1/x} x \neq 0 f(0)=0 f'(x) \alpha \geq 1 2x\sin{1/x}-\cos{1/x} <-1 x 0 g(y)=\frac{2\sin{y}}{y}-\cos{y} y \neq 0 g(y)<-1 y g(y) -\cos{y} -1 \sin{y}=0 g <-1 g g'(y)=0 g'(y)=0 \cos{(y)}=\sin{(y)}\frac{2-y^2}{2y} g(y)=\sin{(y)}\frac{2+y^2}{2y} g'(y)=0 \sin^2{(y)}=\frac{4y^2}{4+y^4} |g(y)|=\frac{2+y^2}{\sqrt{4+y^4}} \frac{2+y^2}{\sqrt{4+y^4}}>1 \alpha=1 f 0 \lim\limits_{y \to \infty}
 \frac{2+y^2}{\sqrt{4+y^4}}=1 \alpha>1 f 0 g(y)=\frac{2\sin{y}}{y}-\cos{(y)}, \text{ for } y \neq 0\tag{1} g(y)= g'(y)=\frac{2\cos{y}\cdot y -2\sin{y}}{y^2}+\sin{y}=0 \implies \cos{(y)}=\sin{(y)}\frac{2-y^2}{2y}\tag{2} (1) g(y)=\sin{(y)}\frac{2+y^2}{2y} (2) \sin^2{(y)}=\frac{4y^2}{4+y^4}\tag{3} |g(y)|=\left | \sin{(y)}\frac{2+y^2}{2y} \right |=|\sin{(y)}| \left | \frac{2+y^2}{2y} \right | (3) \implies |g(y)|=\frac{2+y^2}{\sqrt{4+y^4}} \frac{2+y^2}{\sqrt{4+y^4}}>1 |2+y^2|=2+y^2= \sqrt{(2+y^2)^2} =\sqrt{4+4y^2+y^4}>\sqrt{4+y^2}, \text{ for } y \neq 0 \implies \frac{2+y^2}{\sqrt{4+y^2}}>1 \blacksquare \alpha=1 x \neq 0 \implies f(x)=x+x^2\sin{(1/x)} f'(x)=1+2x\sin{(1/x)}-\cos{(1/x)} =1+g(1/x) f'>0 0 f'<0 0 f 0 y=-2\pi k, k=1,2,... g(-2\pi k)=-\cos{(2\pi k)}=-1 g'(y)=\frac{2\cos{y}\cdot y -2\sin{y}}{y^2}+\sin{y} g'(-2\pi k)=-\frac{1}{\pi k}<0 c \in (-2\pi k-\delta, -2\pi k) \land g(y)<g(-2\pi k)=-1 \exists c, c \in (-\frac{1}{2 \pi k}, -\frac{1}{2\pi k+\delta}) \land g(1/x)<-1 f'(c)<-1 \lim\limits_{y \to \infty} \frac{2+y^2}{\sqrt{4+y^4}}=1 \forall \epsilon>0\ \exists N>0\ \forall y\ y>N\implies \left | \frac{2+y^2}{\sqrt{4+y^4}} -1\right | < \epsilon \implies 1-\epsilon<|g(y)|<1+\epsilon \alpha>1 x \neq 0 \implies f(x)=\alpha x+x^2\sin{(1/x)} f'(x)=\alpha+2x\sin{(1/x)}-\cos{(1/x)} =\alpha+g(1/x)>\alpha-|g(1/x)| \epsilon=\frac{\alpha-1}{2} 1-\frac{\alpha-1}{2}<|g(1/x)|<1+\frac{\alpha-1}{2} \frac{3-\alpha}{2}<|g(1/x)|<\frac{1+\alpha}{2} f'(x)>\alpha-\frac{1+\alpha}{2}=\frac{\alpha-1}{2}>0 \exists N>0\ \forall y\ y>N \implies f'(x)>0 \exists N>0\ \forall x\ x<\frac{1}{N} \implies f'(x)>0 f (0, \frac{1}{N}) \lim\limits_{y \to -\infty} \frac{2+y^2}{\sqrt{4+y^4}}=1 \exists N<0\ \forall x\ x>\frac{1}{N} \implies f'(x)>0 f'(0)=\alpha \exists N>0\ \forall x\ |x|<\frac{1}{N} \implies f'(x)>0 f (-\frac{1}{N}, \frac{1}{N})","['calculus', 'derivatives', 'proof-explanation']"
63,Differentiability of Wiener integral with respect to a parameter,Differentiability of Wiener integral with respect to a parameter,,"Let $f:[0,T]\times\Theta\to\mathbb R$ and let $\{B_t\}_{t\in [0,T]}$ be a Brownian motion. Consider the Wiener integral $$\int_0^T f(t,\theta)dB_t.$$ I am looking for conditions that ensure that $$\frac{d}{d\theta}\int_0^T f(t,\theta)dB_t=\int_0^T \partial_{\theta}f(t,\theta)dB_t$$ In this and this article conditions are set in the case in which $f$ is not deterministic. Thanks in advance.",Let and let be a Brownian motion. Consider the Wiener integral I am looking for conditions that ensure that In this and this article conditions are set in the case in which is not deterministic. Thanks in advance.,"f:[0,T]\times\Theta\to\mathbb R \{B_t\}_{t\in [0,T]} \int_0^T f(t,\theta)dB_t. \frac{d}{d\theta}\int_0^T f(t,\theta)dB_t=\int_0^T \partial_{\theta}f(t,\theta)dB_t f","['derivatives', 'stochastic-calculus', 'stochastic-integrals']"
64,Derivative of vector-valued function's norm-squared,Derivative of vector-valued function's norm-squared,,I'm trying to understand the partial chain rule and want to check my understanding. Let $$g(\mathbf{x})=\Vert{\mathbf{f}(\mathbf{x})}\Vert^2=\mathbf{f}(\mathbf{x})^T\mathbf{f}(\mathbf{x})$$ Then am I right in saying $$\frac{\text{d}g}{\text{d}\mathbf{x}}=2(\mathbf{J}(\mathbf{f})(\mathbf{x}))^T\mathbf{f}(\mathbf{x})$$,I'm trying to understand the partial chain rule and want to check my understanding. Let Then am I right in saying,g(\mathbf{x})=\Vert{\mathbf{f}(\mathbf{x})}\Vert^2=\mathbf{f}(\mathbf{x})^T\mathbf{f}(\mathbf{x}) \frac{\text{d}g}{\text{d}\mathbf{x}}=2(\mathbf{J}(\mathbf{f})(\mathbf{x}))^T\mathbf{f}(\mathbf{x}),"['multivariable-calculus', 'derivatives', 'jacobian']"
65,"Generating exercises about extrema of $f(x,y)$",Generating exercises about extrema of,"f(x,y)","I can generate many examples of functions $f(x,y)$ for which finding local extrema is a good exercise for students, for example $f(x,y)=x^3+y^3-3xy$ , $f(x, y) = x^3 + 3xy^2 − 51x − 24y$ , $f (x, y) = xy + \ln y + x^2$ . The general idea of solving them is to obtain a polynomial of a small degree, for which the roots are easy to obtain, because, e.g., some of them are integer or rational. However, I do not know how to obtain nontrivial, i.e., not of the type $f(x,y)=(x-a)^2+(y-b)^2$ , examples of functions with previously known extrema. A long time ago I found a simple rule for memorizing the values of $\sin x$ for $x\in\{\pi/6,\pi/4,\pi/3,\pi/2\}$ . I did not believe that it is not generally known. Indeed, many years later I found this observation in a written source. In this case, I also believe that the mathematical society knows the general rules of generating many examples of easy but not trivial exercises, only I do not know them. Are such rules known to you?","I can generate many examples of functions for which finding local extrema is a good exercise for students, for example , , . The general idea of solving them is to obtain a polynomial of a small degree, for which the roots are easy to obtain, because, e.g., some of them are integer or rational. However, I do not know how to obtain nontrivial, i.e., not of the type , examples of functions with previously known extrema. A long time ago I found a simple rule for memorizing the values of for . I did not believe that it is not generally known. Indeed, many years later I found this observation in a written source. In this case, I also believe that the mathematical society knows the general rules of generating many examples of easy but not trivial exercises, only I do not know them. Are such rules known to you?","f(x,y) f(x,y)=x^3+y^3-3xy f(x, y) = x^3 + 3xy^2 − 51x − 24y f (x, y) = xy + \ln y + x^2 f(x,y)=(x-a)^2+(y-b)^2 \sin x x\in\{\pi/6,\pi/4,\pi/3,\pi/2\}","['calculus', 'derivatives', 'polynomials']"
66,General formula for the derivative of a pentation?,General formula for the derivative of a pentation?,,"Now, many people probably know of the first three hyperoperations, such as addition, multiplication, and exponentiation. However, many don't know of the fourth, tetration, and even less then know about the fifth hyperoperation, pentation. Much how like multiplication is repeated addition, and exponentiation is repeated multiplication. Tetration is repeated exponentiation. E.g. $^2x$ = $x^x$ and $^3x$ = $x^{x^x}$ I was wondering if there was a general formula for the derivative of tetrations, turns out my friend and I found one for all n $\ge$ 2 where n $\in$$\mathbb{Z}$ , this being. $\frac{d}{dx}$ [ $^nx$ ] = $\displaystyle\sum_{k=1}^{n-2}$ $\left(_nx_{n-k}^{k-1}\right)\frac{1}{x}$ + $_nx$$_2^{n-2}$ + $_nx$$_2^{n-1}$ Take note of the notation im using here: $_ax$$_z$ = $^ax$ * $^bx$ * … $^zx$ $x^{k-1}$ = x * ln $^{k-1}(x)$ This will return the correct expression, but now I began to wonder if there was a formula for the derivative of pentated equation. Pentation being: $_2x$ = $^xx$ Which means x exponentiated, x many times. I have no clue where to even began differentiating this, the formula evidently wouldn't spit out a good answer and I've scoured the stack exchange and internet looking for an answer but all I've come across is a nice formula which I also found earlier with no prior knowledge of the post. That being: $\frac{d}{dx}$ [ $^nx$ ] = $^nx$ $\left(\frac{^{n-1}x}{x}+ \frac{d}{dx}[^{n-1}x]ln(x)\right)$ Here's the link to the question: $n^{th}$ derivative of a tetration function Despite this, it still doesn't help. I'm lost as to what to do so any help would be amazing. Alas, I'm just some random internet stranger not really knowing what he's talking about. So if I'm missing something painfully obvious feel free to point it out.","Now, many people probably know of the first three hyperoperations, such as addition, multiplication, and exponentiation. However, many don't know of the fourth, tetration, and even less then know about the fifth hyperoperation, pentation. Much how like multiplication is repeated addition, and exponentiation is repeated multiplication. Tetration is repeated exponentiation. E.g. = and = I was wondering if there was a general formula for the derivative of tetrations, turns out my friend and I found one for all n 2 where n , this being. [ ] = + + Take note of the notation im using here: = * * … = x * ln This will return the correct expression, but now I began to wonder if there was a formula for the derivative of pentated equation. Pentation being: = Which means x exponentiated, x many times. I have no clue where to even began differentiating this, the formula evidently wouldn't spit out a good answer and I've scoured the stack exchange and internet looking for an answer but all I've come across is a nice formula which I also found earlier with no prior knowledge of the post. That being: [ ] = Here's the link to the question: $n^{th}$ derivative of a tetration function Despite this, it still doesn't help. I'm lost as to what to do so any help would be amazing. Alas, I'm just some random internet stranger not really knowing what he's talking about. So if I'm missing something painfully obvious feel free to point it out.",^2x x^x ^3x x^{x^x} \ge \in\mathbb{Z} \frac{d}{dx} ^nx \displaystyle\sum_{k=1}^{n-2} \left(_nx_{n-k}^{k-1}\right)\frac{1}{x} _nx_2^{n-2} _nx_2^{n-1} _ax_z ^ax ^bx ^zx x^{k-1} ^{k-1}(x) _2x ^xx \frac{d}{dx} ^nx ^nx \left(\frac{^{n-1}x}{x}+ \frac{d}{dx}[^{n-1}x]ln(x)\right),"['calculus', 'derivatives', 'tetration']"
67,"Partial derivative of a function of the form $g(x)=\int_V f(x,y)~dy$",Partial derivative of a function of the form,"g(x)=\int_V f(x,y)~dy","Suppose $U,V$ are open boxes in $\Bbb R^n$ , $\Bbb R^m$ , respectively, and $f:U\times V\to \Bbb R$ is continuously differentiable. For $g:U\to \Bbb R$ defined by $g(x)=\int_V f(x,y)~dy$ , I am trying to compute $\partial g/\partial x_i$ $(i=1,\dots,n)$ . Note that $$ \dfrac{\partial g}{\partial x_i} (x)=\lim_{h\to 0}\dfrac{g(x+he_i)-g(x)}{h}=\lim_{h\to0} \frac{1}{h} \int_V f(x+he_i,y)-f(x,y)~dy , $$ so if we can interchange the integral and the limit, we get $\partial g(x)/\partial x_i= \int_V \partial_if (x,y)~dy$ . But can we interchange the integral and the limit in this case?","Suppose are open boxes in , , respectively, and is continuously differentiable. For defined by , I am trying to compute . Note that so if we can interchange the integral and the limit, we get . But can we interchange the integral and the limit in this case?","U,V \Bbb R^n \Bbb R^m f:U\times V\to \Bbb R g:U\to \Bbb R g(x)=\int_V f(x,y)~dy \partial g/\partial x_i (i=1,\dots,n)  \dfrac{\partial g}{\partial x_i} (x)=\lim_{h\to 0}\dfrac{g(x+he_i)-g(x)}{h}=\lim_{h\to0} \frac{1}{h} \int_V f(x+he_i,y)-f(x,y)~dy ,  \partial g(x)/\partial x_i= \int_V \partial_if (x,y)~dy","['real-analysis', 'integration', 'limits', 'multivariable-calculus', 'derivatives']"
68,I have a little problem trying to understand differentials,I have a little problem trying to understand differentials,,"I have just learned differentials and I have a few things unclear. For example I found a contradiction trying to differentiate the function $f(f(x))$ , where $f:\mathbb{R} \to \mathbb{R}$ . Using the chain rule we get $$(f(f(x)))'=f'(f(x))f'(x)$$ However, if we use the differentials we get that $$\frac{d(f(f(x)))}{dx}=\frac{df}{df}\frac{df}{dx}=\frac{df}{dx}$$ This is true if $f(x)$ is a constant or $f(f(x))=f(x)$ . Why doesn't this work for the general case?","I have just learned differentials and I have a few things unclear. For example I found a contradiction trying to differentiate the function , where . Using the chain rule we get However, if we use the differentials we get that This is true if is a constant or . Why doesn't this work for the general case?",f(f(x)) f:\mathbb{R} \to \mathbb{R} (f(f(x)))'=f'(f(x))f'(x) \frac{d(f(f(x)))}{dx}=\frac{df}{df}\frac{df}{dx}=\frac{df}{dx} f(x) f(f(x))=f(x),"['real-analysis', 'calculus', 'analysis', 'derivatives', 'chain-rule']"
69,Obtaining the higher-order differentials of a function of one variable,Obtaining the higher-order differentials of a function of one variable,,"I'm considering the differential of a function of one variable $f$ to be defined as a function of two variables $df$ for which the following holds: $$df(x, h) = f'(x)h$$ All good so far. The extension to higher-order differentials, however, is what puzzles me. This is a passage from Courant: (...) it may be pointed out for the sake of completeness that we may also form second and higher differentials. For if we think of $h$ as chosen in any manner, but always the same for every value of $x$ , then $dy=hf'(x)$ is a function of $x$ , of which we can again form the differential. The result will be called the second differential of y , and will be denoted by the symbol $d^2y=d^2f(x)$ . The increment of $hf'(x)$ being $h\{f'(x+h)-f'(x)\}$ , the second differential is obtained by replacing the quantity in brackets by its linear part $hf''(x)$ , so that $d^2y=h^2f''(x)$ . We may naturally proceed further along the same lines, obtaining third, fourth, ... differentials of y, etc., which can be defined by the expressions $h^3f'''(x), h^4f^{iv}(x)$ , and so on. Two questions arise from this for me: The author writes of the second differential of a function. To arrive at it they had to arbitrarily fix some value of $h$ in order to define a new single-variable function $dy$ such that $dy(x) = hf'(x)$ for the given $h$ , and form the differential of this new function. Doesn't this procedure give rise to an infinite number of different functions, each associated with a different value of $h$ , which would, in turn, lead to an infinite number of different functions, all of which satisfy the definition of the second differential? Given that the first differential is a function of two variables, unlike the single-variable function from which it was formed, speaking of the second (and higher-order) differential as the differential of the differential doesn't seem accurate. Is there a procedure that circumvents the ""fix a value of $h$ "" argument and still leads to a rigorous definition in this context? Why is it simply assumed that the $h$ 's that multiply the higher-order derivatives in the definition of higher-order differentials are all the same ( ""third, fourth, ... differentials of y, etc., which can be defined by the expressions $h^3f'''(x), h^4f^{iv}(x)$ , and so on"" )? Shouldn't each $h$ be independent of the other, such that a more accurate description of the higher-order differentials should be $h_1h_2h_3f'''(x)$ , $h_1h_2h_3h_4f^{(4)}(x)$ ? Thanks.","I'm considering the differential of a function of one variable to be defined as a function of two variables for which the following holds: All good so far. The extension to higher-order differentials, however, is what puzzles me. This is a passage from Courant: (...) it may be pointed out for the sake of completeness that we may also form second and higher differentials. For if we think of as chosen in any manner, but always the same for every value of , then is a function of , of which we can again form the differential. The result will be called the second differential of y , and will be denoted by the symbol . The increment of being , the second differential is obtained by replacing the quantity in brackets by its linear part , so that . We may naturally proceed further along the same lines, obtaining third, fourth, ... differentials of y, etc., which can be defined by the expressions , and so on. Two questions arise from this for me: The author writes of the second differential of a function. To arrive at it they had to arbitrarily fix some value of in order to define a new single-variable function such that for the given , and form the differential of this new function. Doesn't this procedure give rise to an infinite number of different functions, each associated with a different value of , which would, in turn, lead to an infinite number of different functions, all of which satisfy the definition of the second differential? Given that the first differential is a function of two variables, unlike the single-variable function from which it was formed, speaking of the second (and higher-order) differential as the differential of the differential doesn't seem accurate. Is there a procedure that circumvents the ""fix a value of "" argument and still leads to a rigorous definition in this context? Why is it simply assumed that the 's that multiply the higher-order derivatives in the definition of higher-order differentials are all the same ( ""third, fourth, ... differentials of y, etc., which can be defined by the expressions , and so on"" )? Shouldn't each be independent of the other, such that a more accurate description of the higher-order differentials should be , ? Thanks.","f df df(x, h) = f'(x)h h x dy=hf'(x) x d^2y=d^2f(x) hf'(x) h\{f'(x+h)-f'(x)\} hf''(x) d^2y=h^2f''(x) h^3f'''(x), h^4f^{iv}(x) h dy dy(x) = hf'(x) h h h h h^3f'''(x), h^4f^{iv}(x) h h_1h_2h_3f'''(x) h_1h_2h_3h_4f^{(4)}(x)","['calculus', 'derivatives']"
70,Is there any easier proof for derivative of $x^e$,Is there any easier proof for derivative of,x^e,I am trying to prove that $\frac{d}{dx}x^e=ex^{e-1}$ . $\displaystyle \frac{d}{dx}x^e=\lim_{h\to 0}\dfrac{(x+h)^e-x^e}{h}=\lim_{h\to 0}x^e \cdot \dfrac {e^{e  \ln \left({1 + \frac h x} \right)} - 1} {e  \ln {\left(1 + \dfrac h x\right)} } \cdot \dfrac {e  \ln {\left(1 + \dfrac h x\right)}} {\dfrac h x} \cdot \dfrac 1 x = e x^{e - 1}$ since $\displaystyle \lim_{x \mathop \to 0} \frac {e^ x - 1} x = 1$ and $\displaystyle \lim_{x \mathop \to 0} \frac { \ln {(1 + x)} } x = 1$ . Is there any way which is simpler? Thanks a lot in advance!,I am trying to prove that . since and . Is there any way which is simpler? Thanks a lot in advance!,\frac{d}{dx}x^e=ex^{e-1} \displaystyle \frac{d}{dx}x^e=\lim_{h\to 0}\dfrac{(x+h)^e-x^e}{h}=\lim_{h\to 0}x^e \cdot \dfrac {e^{e  \ln \left({1 + \frac h x} \right)} - 1} {e  \ln {\left(1 + \dfrac h x\right)} } \cdot \dfrac {e  \ln {\left(1 + \dfrac h x\right)}} {\dfrac h x} \cdot \dfrac 1 x = e x^{e - 1} \displaystyle \lim_{x \mathop \to 0} \frac {e^ x - 1} x = 1 \displaystyle \lim_{x \mathop \to 0} \frac { \ln {(1 + x)} } x = 1,"['calculus', 'derivatives']"
71,"Definition of certain Gateaux differentials of the norm in E. R. Lorch: ""A Curvature Study of Convex Bodies in Banach Spaces""","Definition of certain Gateaux differentials of the norm in E. R. Lorch: ""A Curvature Study of Convex Bodies in Banach Spaces""",,"In the paper E. R. Lorch: A Curvature Study of Convex Bodies in Banach Spaces from 1953, the following assumptions and definitions are stated in section II (p. 107-108): Let $(B, \| \cdot \|)$ be a real Banach space and $B^*$ its dual space . Let $r > 1$ and $G(x) := \frac{1}{r} \| x \|^r$ and $\phi(\alpha, \beta) := G(x + \alpha y + \beta z)$ , where $x, y, z \in B$ are fixed and $\alpha, \beta \in \mathbb{R}$ . Assuming that $\phi$ is twice continuously differentiable , the derivative of $\phi$ with respect to $\alpha$ at $\alpha = \beta = 0$ will be denoted the $G_y(x)$ . [...] It is clear that $G_x(x) = r G(x)$ . [...] The second derivatives of $\phi$ at $\alpha = \beta = 0$ will be denoted by $G_{y, y}(x)$ , $G_{y, z}(x)$ and $G_{z, z}(x)$ . [...] It is clear that $G_{y, z}(x) = G_{z, y}(x)$ . I am trying to understand the functions $G_y$ , $G_{y, y}$ , $G_{y, z}$ and $G_{z, z}$ . Firstly, we have $G \colon B \to \mathbb{R}$ and $\phi \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ . I would write $$ G_y(x) = \frac{\partial}{\partial \alpha}\bigg|_{\alpha = \beta = 0} \phi(\alpha, \beta) = \lim_{\gamma \to 0} \frac{\phi(\gamma, 0) - \phi(0, 0)}{\gamma} = \lim_{\gamma \to 0} \frac{G(x + \gamma y) - G(x)}{\gamma} = \text{d}G(x; y), $$ where the last expression is the Gateaux derivative of $G$ at $x$ in direction $y$ . This would make sense since we can then verify $G_x(x) = r G(x)$ (which is stated as ""clear"" above): for $x \in B$ we have $$ G_x(x) = \lim_{\gamma \to 0} \frac{G(x + \gamma x) - G(x)}{\gamma} = G(x) \lim_{\gamma \to 0} \frac{(1 + \gamma)^r - 1}{\gamma} = r G(x). $$ Furthermore, $y \mapsto G_y(x)$ is a linear bounded map $B \to \mathbb R$ , which seems to be what is stated in theorem 1 (""If $x$ is any fixed element in $B$ , then $G_y(x)$ represents a bounded linear functional ""). How do the corresponding expressions for $G_{y, y}(x)$ , $G_{y, z}(x)$ and $G_{z, z}(x)$ look like? In particular, are the second derivatives both taken with respect to $\alpha$ or are they mixed? Wikipedia gives the following as one definition of the second order Gateaux derivative of $G$ : $$ D^2 G(x)\{y ,z \} := \lim_{\gamma \to 0} \frac{\text{d}G(x + \gamma y; z) - \text{d}G(x; z)}{\gamma} = \frac{\partial^2}{\partial \alpha \partial \beta} \bigg|_{\alpha = \beta= 0} \phi(\alpha, \beta). $$ Is this expression $G_{y, z}(x)$ ? If so, we would presumably have $$ G_{y, y}(x) = \frac{\partial^2}{\partial \alpha^2} \phi(\alpha, \beta) \bigg|_{\alpha = \beta = 0} = \frac{\partial^2}{\partial \alpha^2} G(x + \alpha y + \beta z) \bigg|_{\alpha = \beta = 0} = \lim_{\gamma \to 0} \frac{\text{d}G(x + \gamma y; y) - \text{d}G(x; y)}{\gamma} $$ and analogously $$ G_{z, z}(x) = \frac{\partial^2}{\partial \beta^2} \phi(\alpha, \beta) \bigg|_{\alpha = \beta = 0} = \frac{\partial^2}{\partial \beta^2} G(x + \alpha y + \beta z) \bigg|_{\alpha = \beta = 0}. $$ According to Theorem 2 in that paper, we have $G_{y, z}(x) \in B^*$ for $y \in B$ and $G_y(x) \in B^*$ for $x \in B$ . Do my above guesses about $G_{y, y}$ , ... fulfill those requirements?","In the paper E. R. Lorch: A Curvature Study of Convex Bodies in Banach Spaces from 1953, the following assumptions and definitions are stated in section II (p. 107-108): Let be a real Banach space and its dual space . Let and and , where are fixed and . Assuming that is twice continuously differentiable , the derivative of with respect to at will be denoted the . [...] It is clear that . [...] The second derivatives of at will be denoted by , and . [...] It is clear that . I am trying to understand the functions , , and . Firstly, we have and . I would write where the last expression is the Gateaux derivative of at in direction . This would make sense since we can then verify (which is stated as ""clear"" above): for we have Furthermore, is a linear bounded map , which seems to be what is stated in theorem 1 (""If is any fixed element in , then represents a bounded linear functional ""). How do the corresponding expressions for , and look like? In particular, are the second derivatives both taken with respect to or are they mixed? Wikipedia gives the following as one definition of the second order Gateaux derivative of : Is this expression ? If so, we would presumably have and analogously According to Theorem 2 in that paper, we have for and for . Do my above guesses about , ... fulfill those requirements?","(B, \| \cdot \|) B^* r > 1 G(x) := \frac{1}{r} \| x \|^r \phi(\alpha, \beta) := G(x + \alpha y + \beta z) x, y, z \in B \alpha, \beta \in \mathbb{R} \phi \phi \alpha \alpha = \beta = 0 G_y(x) G_x(x) = r G(x) \phi \alpha = \beta = 0 G_{y, y}(x) G_{y, z}(x) G_{z, z}(x) G_{y, z}(x) = G_{z, y}(x) G_y G_{y, y} G_{y, z} G_{z, z} G \colon B \to \mathbb{R} \phi \colon \mathbb{R} \times \mathbb{R} \to \mathbb{R} 
G_y(x)
= \frac{\partial}{\partial \alpha}\bigg|_{\alpha = \beta = 0} \phi(\alpha, \beta)
= \lim_{\gamma \to 0} \frac{\phi(\gamma, 0) - \phi(0, 0)}{\gamma}
= \lim_{\gamma \to 0} \frac{G(x + \gamma y) - G(x)}{\gamma}
= \text{d}G(x; y),
 G x y G_x(x) = r G(x) x \in B 
G_x(x)
= \lim_{\gamma \to 0} \frac{G(x + \gamma x) - G(x)}{\gamma}
= G(x) \lim_{\gamma \to 0} \frac{(1 + \gamma)^r - 1}{\gamma}
= r G(x).
 y \mapsto G_y(x) B \to \mathbb R x B G_y(x) G_{y, y}(x) G_{y, z}(x) G_{z, z}(x) \alpha G 
D^2 G(x)\{y ,z \}
:= \lim_{\gamma \to 0} \frac{\text{d}G(x + \gamma y; z) - \text{d}G(x; z)}{\gamma}
= \frac{\partial^2}{\partial \alpha \partial \beta} \bigg|_{\alpha = \beta= 0} \phi(\alpha, \beta).
 G_{y, z}(x) 
G_{y, y}(x)
= \frac{\partial^2}{\partial \alpha^2} \phi(\alpha, \beta) \bigg|_{\alpha = \beta = 0}
= \frac{\partial^2}{\partial \alpha^2} G(x + \alpha y + \beta z) \bigg|_{\alpha = \beta = 0}
= \lim_{\gamma \to 0} \frac{\text{d}G(x + \gamma y; y) - \text{d}G(x; y)}{\gamma}
 
G_{z, z}(x)
= \frac{\partial^2}{\partial \beta^2} \phi(\alpha, \beta) \bigg|_{\alpha = \beta = 0}
= \frac{\partial^2}{\partial \beta^2} G(x + \alpha y + \beta z) \bigg|_{\alpha = \beta = 0}.
 G_{y, z}(x) \in B^* y \in B G_y(x) \in B^* x \in B G_{y, y}","['derivatives', 'normed-spaces', 'banach-spaces', 'dual-spaces', 'gateaux-derivative']"
72,Proving that the derivative at a local extremum is 0,Proving that the derivative at a local extremum is 0,,"We wish to prove that for differentiable $f:[a,b] \to \mathbb{R}$ , if $f$ has a local extremum $c$ , then $f'(c)=0$ . Proof: Consider the function $f$ having a local maximum (otherwise consider $-f$ ). Supposing $c$ is a local extremum, we must have that $f(c)\ge f(x)$ $\forall x \in(c-\delta,c+\delta), \delta>0$ . We know then that $f(x)-f(c)\le 0$ . We also have that either $x-c <0$ or $x-c >0$ , as we are assuming $x$ and $c$ are distinct. Then, if we have $x-c>0$ then taking the limit as $x$ approaches $c$ of $\frac{f(x)-f(c)}{x-c}$ , we get that $f'(c) \le 0$ . On the other hand, if $x-c<0$ then taking the limit as $x$ approaches $c$ of $\frac{f(x)-f(c)}{x-c}$ , we get that $f'(c) \ge 0$ . For both conditions to hold, we must have $f'(c) =0$ . My lecturer gave a different proof in the notes for our module, but is this way correct?","We wish to prove that for differentiable , if has a local extremum , then . Proof: Consider the function having a local maximum (otherwise consider ). Supposing is a local extremum, we must have that . We know then that . We also have that either or , as we are assuming and are distinct. Then, if we have then taking the limit as approaches of , we get that . On the other hand, if then taking the limit as approaches of , we get that . For both conditions to hold, we must have . My lecturer gave a different proof in the notes for our module, but is this way correct?","f:[a,b] \to \mathbb{R} f c f'(c)=0 f -f c f(c)\ge f(x) \forall x \in(c-\delta,c+\delta), \delta>0 f(x)-f(c)\le 0 x-c <0 x-c >0 x c x-c>0 x c \frac{f(x)-f(c)}{x-c} f'(c) \le 0 x-c<0 x c \frac{f(x)-f(c)}{x-c} f'(c) \ge 0 f'(c) =0",['real-analysis']
73,"Prove that $\sup_{(a, b)} f’_{-} (x) \geq 0 \geq \inf_{(a, b)} f’_{-}(x) $",Prove that,"\sup_{(a, b)} f’_{-} (x) \geq 0 \geq \inf_{(a, b)} f’_{-}(x) ","Function $f$ is continuous on $[a, b]$ and has a $f’_{-}$ on $(a, b)$ . $f(a) = f(b)$ , then show that $$\sup_{(a, b)} f’_{-}(x) \geq 0 \geq \inf_{(a, b)} f’_{-}(x) $$ Here’s my attempt. Consider 2 cases: $f$ is constant, then the statement is obvious. $f$ is not a constant. Since $f$ is continuous, there is a minimum $m$ and maximum $M$ on $[a, b]$ . Then $m = f(x_1)$ , $M = f(x_2)$ . If both $x$ s are in $(a, b)$ , the statement is proven, because $f’_{-}(x_1) \leq 0, f’_{-}(x_2) \geq 0$ . I don’t know what to do if only one of the $x$ s lies in $(a, b)$ . Do you have any ideas?","Function is continuous on and has a on . , then show that Here’s my attempt. Consider 2 cases: is constant, then the statement is obvious. is not a constant. Since is continuous, there is a minimum and maximum on . Then , . If both s are in , the statement is proven, because . I don’t know what to do if only one of the s lies in . Do you have any ideas?","f [a, b] f’_{-} (a, b) f(a) = f(b) \sup_{(a, b)} f’_{-}(x) \geq 0 \geq \inf_{(a, b)} f’_{-}(x)  f f f m M [a, b] m = f(x_1) M = f(x_2) x (a, b) f’_{-}(x_1) \leq 0, f’_{-}(x_2) \geq 0 x (a, b)","['derivatives', 'proof-writing']"
74,Confusion when using the product rule (differentiation),Confusion when using the product rule (differentiation),,"I'm quite confused by something related to the product rule that should be easy. Let $\phi:\mathbb{R}^p \to \mathbb{R}$ a diferentiable function. Define $$\varphi:\mathbb{R}^p \to \mathbb{R}, \quad \varphi(t) = e^{\phi(t)}, \quad t \in \mathbb{R}^p$$ The derivative of $\varphi$ , using the chain rule, is given by: $$\varphi'(t)=e^{\phi(t)} \phi'(t) = \varphi(t)\phi'(t)  \in \mathbb{R}^p$$ I am trying to use the product rule to find $\varphi''(t)$ : $$\varphi''(t)= \varphi'(t)\phi'(t) +\varphi(t)\phi''(t)$$ But note that the respective dimensions are not compatible: $$\underbrace{\varphi''(t)}_{p\times p}= \underbrace{\varphi'(t)}_{p\times 1} \, \,\underbrace{\phi'(t)}_{p\times 1} +\underbrace{\varphi(t)}_{1\times 1} \, \,\underbrace{\phi''(t)}_{p\times p}$$ I'm committing some bullshit because it doesn't make sense. help!","I'm quite confused by something related to the product rule that should be easy. Let a diferentiable function. Define The derivative of , using the chain rule, is given by: I am trying to use the product rule to find : But note that the respective dimensions are not compatible: I'm committing some bullshit because it doesn't make sense. help!","\phi:\mathbb{R}^p \to \mathbb{R} \varphi:\mathbb{R}^p \to \mathbb{R}, \quad \varphi(t) = e^{\phi(t)}, \quad t \in \mathbb{R}^p \varphi \varphi'(t)=e^{\phi(t)} \phi'(t) = \varphi(t)\phi'(t)  \in \mathbb{R}^p \varphi''(t) \varphi''(t)= \varphi'(t)\phi'(t) +\varphi(t)\phi''(t) \underbrace{\varphi''(t)}_{p\times p}= \underbrace{\varphi'(t)}_{p\times 1} \, \,\underbrace{\phi'(t)}_{p\times 1} +\underbrace{\varphi(t)}_{1\times 1} \, \,\underbrace{\phi''(t)}_{p\times p}","['calculus', 'derivatives']"
75,What is the tensor power series of a vector?,What is the tensor power series of a vector?,,"In this Wikipedia article https://en.m.wikipedia.org/wiki/Jet_(mathematics) there’s Taylor’s theorem for functions $f: \mathbb{R}^m \to \mathbb{R}^n$ where $f(x)=\sum_{n\in \mathbb{N}} D^n f(x_0) (x-x_0)^{\otimes n} /n!$ . What does this tensor product mean here and where can I learn more about it? If it’s just the normal krownecker/tensor  product, then how do they sum to a vector?","In this Wikipedia article https://en.m.wikipedia.org/wiki/Jet_(mathematics) there’s Taylor’s theorem for functions where . What does this tensor product mean here and where can I learn more about it? If it’s just the normal krownecker/tensor  product, then how do they sum to a vector?",f: \mathbb{R}^m \to \mathbb{R}^n f(x)=\sum_{n\in \mathbb{N}} D^n f(x_0) (x-x_0)^{\otimes n} /n!,"['derivatives', 'taylor-expansion', 'tensor-products', 'online-resources', 'jet-bundles']"
76,"Directional derivatives, continuity at $0$ and differentiability of $f(x,y)=\begin{cases}\frac{xy^6}{x^4+y^8},(x,y)\neq (0,0)\\0,(x,y)=0\\\end{cases}$","Directional derivatives, continuity at  and differentiability of","0 f(x,y)=\begin{cases}\frac{xy^6}{x^4+y^8},(x,y)\neq (0,0)\\0,(x,y)=0\\\end{cases}","I have solved the following problem and I would appreciate some feedback on my solution, thanks: Let $f(x,y)=\begin{cases}\frac{xy^6}{x^4+y^8},(x,y)\neq (0,0)\\0,(x,y)=0\\\end{cases}$ . (a) Find all the directional derivatives of $f$ at $(0,0)$ . (b) Is f continuous at $(0,0)$ ? (c) Is f differentiable at $(0,0)$ ? My solution: (a) $$D_{\mathbf{v}}f=\lim\limits_{t\to 0}\frac{f(\mathbf{0}+t\mathbf{v})-f(\mathbf{0})}{t}=\lim\limits_{t\to 0}\frac{tv_1t^6v_2^2}{t^4v_1^4+t^8v_2^8}\frac{1}{t}=\lim\limits_{t\to 0} t^2\frac{v_1v_2^6}{v_1^4+t^4v_2^8}=0,$$ where $\mathbf{v}=(v_1,v_2)\neq (0,0).$ (b) $f(x,mx)=x^3\frac{m^6}{1+m^8x^4}\xrightarrow{(x,y)\to (0,0)}0$ but $f(x,\sqrt{x})=\frac{1}{2}$ so $\lim\limits_{(x,y)\to (0,0)}f(x,y)$ does not exist and $f$ is thus not continuous at $(0,0).$ (c) $f$ cannot be differentiable at $(0,0)$ since it is not continuous at $(0,0).$","I have solved the following problem and I would appreciate some feedback on my solution, thanks: Let . (a) Find all the directional derivatives of at . (b) Is f continuous at ? (c) Is f differentiable at ? My solution: (a) where (b) but so does not exist and is thus not continuous at (c) cannot be differentiable at since it is not continuous at","f(x,y)=\begin{cases}\frac{xy^6}{x^4+y^8},(x,y)\neq (0,0)\\0,(x,y)=0\\\end{cases} f (0,0) (0,0) (0,0) D_{\mathbf{v}}f=\lim\limits_{t\to 0}\frac{f(\mathbf{0}+t\mathbf{v})-f(\mathbf{0})}{t}=\lim\limits_{t\to 0}\frac{tv_1t^6v_2^2}{t^4v_1^4+t^8v_2^8}\frac{1}{t}=\lim\limits_{t\to 0} t^2\frac{v_1v_2^6}{v_1^4+t^4v_2^8}=0, \mathbf{v}=(v_1,v_2)\neq (0,0). f(x,mx)=x^3\frac{m^6}{1+m^8x^4}\xrightarrow{(x,y)\to (0,0)}0 f(x,\sqrt{x})=\frac{1}{2} \lim\limits_{(x,y)\to (0,0)}f(x,y) f (0,0). f (0,0) (0,0).","['multivariable-calculus', 'derivatives', 'continuity']"
77,Average rate of water dripping from a cylindrical bucket,Average rate of water dripping from a cylindrical bucket,,"Water drips out of the bottom of a cylindrical bucket that is initially full. The rate of dripping is proportional to the height of water column in the bucket. If the rate of dripping at half height is R, then the average rate of dripping until the bucket becomes almost empty is: 1.greater than R 2.R 3. Between R/2 and R 4. Less than R/2 How to find the average rate of dripping, please help. Thanks in advance. My attempt (Modified as per my trivial understanding of the comments) Let $$V=\pi r^2 h$$ The rate of water dripping from the bucket is: $$-\frac{dV}{dt}=\pi r^2 \frac{dh}{dt}$$ Now we have $-\frac{dV}{dt}=kh$ for some constant of proportional $k$ . Also for half height we have $R=kH/2$ which gives $k=2R/H$ . Substituting back we get: $-\frac{dV}{dt}= 2Rh/H$ When the bucket becomes almost empty, the instantaneous rate is: $-\frac{dV}{dt}= 2R$ But I need to find the average rate of dripping. Is it the right way to do it? What should I do next, please suggest.","Water drips out of the bottom of a cylindrical bucket that is initially full. The rate of dripping is proportional to the height of water column in the bucket. If the rate of dripping at half height is R, then the average rate of dripping until the bucket becomes almost empty is: 1.greater than R 2.R 3. Between R/2 and R 4. Less than R/2 How to find the average rate of dripping, please help. Thanks in advance. My attempt (Modified as per my trivial understanding of the comments) Let The rate of water dripping from the bucket is: Now we have for some constant of proportional . Also for half height we have which gives . Substituting back we get: When the bucket becomes almost empty, the instantaneous rate is: But I need to find the average rate of dripping. Is it the right way to do it? What should I do next, please suggest.",V=\pi r^2 h -\frac{dV}{dt}=\pi r^2 \frac{dh}{dt} -\frac{dV}{dt}=kh k R=kH/2 k=2R/H -\frac{dV}{dt}= 2Rh/H -\frac{dV}{dt}= 2R,"['calculus', 'derivatives', 'volume', 'related-rates']"
78,Motion in plane,Motion in plane,,"A particle moves in the plane with an acceleration that is parallel to the y-axis and proportional to the distance from the x-axis. If the acceleration is attractive (that is, directed toward the x-axis), then show that the equation of the path may be written in the form $y = A\cos(mx + B)$ . If the acceleration is repulsive (directed away from the x-axis), on the other hand, then show that the path will be given by an equation of the form $ y = Ae^{mx} + Be^{-mx}$ .","A particle moves in the plane with an acceleration that is parallel to the y-axis and proportional to the distance from the x-axis. If the acceleration is attractive (that is, directed toward the x-axis), then show that the equation of the path may be written in the form . If the acceleration is repulsive (directed away from the x-axis), on the other hand, then show that the path will be given by an equation of the form .",y = A\cos(mx + B)  y = Ae^{mx} + Be^{-mx},"['calculus', 'integration', 'analysis', 'derivatives', 'definite-integrals']"
79,Derivative of $\pi: G \times \mathfrak{g} \rightarrow G \times_K \mathfrak{g}$,Derivative of,\pi: G \times \mathfrak{g} \rightarrow G \times_K \mathfrak{g},"Let $G$ be a compact Lie group with Lie algebra $\mathfrak{g}.$ Let $K$ be a closed subgroup of $G$ with Lie algebra $\mathfrak{k}.$ We define the manifold $$\mathcal{E}:= G \times_K \mathfrak{g}$$ to be the quotient of the $K$ action on $G \times \mathfrak{g}$ , where $K$ acts on $G$ by right multiplication, and on $\mathfrak{g}$ by the adjoint action. Let $$ \pi: G \times \mathfrak{g} \rightarrow G \times_K \mathfrak{g}$$ be the projection map; which associates to $(g,X) $ its equivalence class $[g,X] $ in $G \times_K \mathfrak{g}$ . Let $(g,X) \in G \times \mathfrak{g}$ , if we identify $T_{(g,X)} (G \times \mathfrak{g})$ with $\mathfrak{g} \times \mathfrak{g}$ and identify $T_{\pi(g,X)}(G \times_K \mathfrak{g})$ with $\mathfrak{g}/\mathfrak{k} \times \mathfrak{g}$ , can we give an explicit expression for the derivative of $\pi$ at $(g,X) ?$ What I've tried is to take $Z_1= \alpha'(t)$ , $Z_2= \beta'(t)$ in $\mathfrak{g} \times \mathfrak{g}$ and $$d\pi_{(g,X)}(Z_1,Z_2)= d\pi_{(g,X)}(Z_1,0)+ d\pi_{(g,X)}(0,Z_2)= \frac{d}{dt} \Bigg|_{t=0} \pi(\alpha(t),X) +\frac{d}{dt} \Bigg|_{t=0} \pi(g,\beta(t)),$$ but then I don't how to continue?","Let be a compact Lie group with Lie algebra Let be a closed subgroup of with Lie algebra We define the manifold to be the quotient of the action on , where acts on by right multiplication, and on by the adjoint action. Let be the projection map; which associates to its equivalence class in . Let , if we identify with and identify with , can we give an explicit expression for the derivative of at What I've tried is to take , in and but then I don't how to continue?","G \mathfrak{g}. K G \mathfrak{k}. \mathcal{E}:= G \times_K \mathfrak{g} K G \times \mathfrak{g} K G \mathfrak{g}  \pi: G \times \mathfrak{g} \rightarrow G \times_K \mathfrak{g} (g,X)  [g,X]  G \times_K \mathfrak{g} (g,X) \in G \times \mathfrak{g} T_{(g,X)} (G \times \mathfrak{g}) \mathfrak{g} \times \mathfrak{g} T_{\pi(g,X)}(G \times_K \mathfrak{g}) \mathfrak{g}/\mathfrak{k} \times \mathfrak{g} \pi (g,X) ? Z_1= \alpha'(t) Z_2= \beta'(t) \mathfrak{g} \times \mathfrak{g} d\pi_{(g,X)}(Z_1,Z_2)= d\pi_{(g,X)}(Z_1,0)+ d\pi_{(g,X)}(0,Z_2)= \frac{d}{dt} \Bigg|_{t=0} \pi(\alpha(t),X) +\frac{d}{dt} \Bigg|_{t=0} \pi(g,\beta(t)),","['derivatives', 'differential-geometry', 'lie-groups', 'lie-algebras']"
80,How to solve for the total derivative with respect to a parameter for a system of two embedded functions?,How to solve for the total derivative with respect to a parameter for a system of two embedded functions?,,"My problem has two embedded functions: $$E = F(x, P)$$ $$P = f(x, E)$$ where $x$ is a parameter. I want to sign the total derivative of $E$ wrt to $x$ , i.e. $dE/dx$ . Can this be solved? If so, is it correct to simply proceed as follows? $$dE = F_1dx + F_2dP$$ $$dP = f_1dx +f_2dE $$ where $F_1$ is the partial derivative of $F$ wrt the first argument $x$ , and $F_2$ is the partial derivative wrt a change in $P$ . Similarly for $f_1$ and $f_2$ . Substituting: $$dE = F_1dx + F_2(f_1dx +f_2dE)$$ $$dE(1 - F_2f2) = (F_1 + F_2f_1)dx $$ $$dE/dx =  (F_1 + F_2f_2)/(1 - F_2f_1)$$ I have priors on all the partial derivatives: $F_2 < 0$ and $f_1 > 0$ so the denominator is positive. The numerator is negative, so if above calculations are legit $dE/dx < 0$ . Thanks.","My problem has two embedded functions: where is a parameter. I want to sign the total derivative of wrt to , i.e. . Can this be solved? If so, is it correct to simply proceed as follows? where is the partial derivative of wrt the first argument , and is the partial derivative wrt a change in . Similarly for and . Substituting: I have priors on all the partial derivatives: and so the denominator is positive. The numerator is negative, so if above calculations are legit . Thanks.","E = F(x, P) P = f(x, E) x E x dE/dx dE = F_1dx + F_2dP dP = f_1dx +f_2dE  F_1 F x F_2 P f_1 f_2 dE = F_1dx + F_2(f_1dx +f_2dE) dE(1 - F_2f2) = (F_1 + F_2f_1)dx  dE/dx =  (F_1 + F_2f_2)/(1 - F_2f_1) F_2 < 0 f_1 > 0 dE/dx < 0","['calculus', 'derivatives']"
81,Finding roots in higher order polynomials using binary search,Finding roots in higher order polynomials using binary search,,"I could successfully implement a root finder, by recursively: finding the zeroes of the nth derivative which gives the extrema of the (n-1)th derivative which provides alternating sign values to be used to find the zeroes of the (n-1)th derivative using binary search. What is the name of this method? The wikipedia article has a paragraph on ""Newton's method (and similar derivative-based methods)"" that doesn't mention it. Can it be expanded to support multivariate polynomials? I couldn't find a way to do it by myself, and I couldn't search for it because I couldn't find the name of this method.","I could successfully implement a root finder, by recursively: finding the zeroes of the nth derivative which gives the extrema of the (n-1)th derivative which provides alternating sign values to be used to find the zeroes of the (n-1)th derivative using binary search. What is the name of this method? The wikipedia article has a paragraph on ""Newton's method (and similar derivative-based methods)"" that doesn't mention it. Can it be expanded to support multivariate polynomials? I couldn't find a way to do it by myself, and I couldn't search for it because I couldn't find the name of this method.",,"['multivariable-calculus', 'derivatives', 'roots']"
82,What correlation exists between the assumptions $a)$ and $b)$?,What correlation exists between the assumptions  and ?,a) b),"Let $F\in C^1(\mathbb{R}\setminus\{0\})$ and let $\varepsilon>0$ small. Consider the two assumptions: $$a)\quad F(s)\ge \frac{1}{s^2}\quad\mbox{ for $0<s<\varepsilon$};$$ $$b)\quad sF^{\prime}(s)\ge \frac{1}{s}\quad\mbox{ for $0<s<\varepsilon$}.$$ As an exercise, I need to understand which is stronger between them; I mean, if $a)\implies b)$ or $b)\implies a)$ . Or, in general, what kind of relation exists between them. I tried this: conditions $b)$ is equivalent to $F^{\prime}(s)\ge \frac{1}{s^2}$ for $0<s<\varepsilon$ . Thus $$\int_s^{\varepsilon} F^{\prime}(v) dv \ge\int_s^{\varepsilon}\frac{1}{v^2} dv\iff F(\varepsilon)-F(s)\ge \frac{1}{s} -\frac{1}{\varepsilon}\iff F(s)\le C-\frac{1}{s},$$ for a suitable constant $C>0$ . Apparently, I do not see any relation beteween them. Could someone please help me understand what I am doing wrong? Or please give some hints? Thank you in advance!","Let and let small. Consider the two assumptions: As an exercise, I need to understand which is stronger between them; I mean, if or . Or, in general, what kind of relation exists between them. I tried this: conditions is equivalent to for . Thus for a suitable constant . Apparently, I do not see any relation beteween them. Could someone please help me understand what I am doing wrong? Or please give some hints? Thank you in advance!","F\in C^1(\mathbb{R}\setminus\{0\}) \varepsilon>0 a)\quad F(s)\ge \frac{1}{s^2}\quad\mbox{ for 0<s<\varepsilon}; b)\quad sF^{\prime}(s)\ge \frac{1}{s}\quad\mbox{ for 0<s<\varepsilon}. a)\implies b) b)\implies a) b) F^{\prime}(s)\ge \frac{1}{s^2} 0<s<\varepsilon \int_s^{\varepsilon} F^{\prime}(v) dv \ge\int_s^{\varepsilon}\frac{1}{v^2} dv\iff F(\varepsilon)-F(s)\ge \frac{1}{s} -\frac{1}{\varepsilon}\iff F(s)\le C-\frac{1}{s}, C>0","['real-analysis', 'calculus', 'integration', 'functional-analysis', 'derivatives']"
83,Heuristic reason for Rademacher Theorem,Heuristic reason for Rademacher Theorem,,Loosely speaking Rademacher Theorem states that Lipschitz functions are almost everywhere differentiable. Is there a heuristic justification for this?,Loosely speaking Rademacher Theorem states that Lipschitz functions are almost everywhere differentiable. Is there a heuristic justification for this?,,"['real-analysis', 'derivatives', 'continuity', 'lipschitz-functions']"
84,Derivatives. Getting things into and out of d(),Derivatives. Getting things into and out of d(),,In a textbook I encountered this: $$\frac{H}{\omega_s}2\omega_r\frac{d\omega_r}{dt} = (P_m-P_e)\frac{d\delta}{dt}$$ The left-hand side of this equation can be rewritten to give $$\frac{H}{\omega_s}\frac{d\left(\omega^2_r\right)}{dt} = (P_m-P_e)\frac{d\delta}{dt}$$ But to me it seems that $\frac{d(w_r^2)}{dt}$ is the same as $\frac{d}{dt}w_r^2$ which is just $2w_r$ and not $2w_r\frac{dw_r}{dt}$ What am I missing here? EDIT: Got it! If it were $\frac{d(t^2)}{dt}$ then it would be $2t$ But $w_r$ is a function of $t$ so you need to use the chain rule!,In a textbook I encountered this: The left-hand side of this equation can be rewritten to give But to me it seems that is the same as which is just and not What am I missing here? EDIT: Got it! If it were then it would be But is a function of so you need to use the chain rule!,\frac{H}{\omega_s}2\omega_r\frac{d\omega_r}{dt} = (P_m-P_e)\frac{d\delta}{dt} \frac{H}{\omega_s}\frac{d\left(\omega^2_r\right)}{dt} = (P_m-P_e)\frac{d\delta}{dt} \frac{d(w_r^2)}{dt} \frac{d}{dt}w_r^2 2w_r 2w_r\frac{dw_r}{dt} \frac{d(t^2)}{dt} 2t w_r t,"['calculus', 'derivatives', 'notation']"
85,Converting one form of the Gompertz equation into another,Converting one form of the Gompertz equation into another,,"Consider the Gompertz equation that models the dynamics of the population of a single-species $$\frac{dN}{dt}=r_0e^{-\alpha t}N$$ and convert it to the following form $$\frac{dN}{dt}=\alpha N\ln\left(\frac{K}{N}\right)$$ So here's my attempt, the problem doesn't explicitly say that the constant of integration is $0$ or not, so I consider two cases when solving and derived, $$ N(t) = N_0 \exp \left( \frac{r_0}{\alpha}\left(1-e^{-\alpha t}\right) \right) $$ where I took $N(0)=N_0$ , and, $N(t)=e^{-\frac{r_0}{\alpha}e^{-\alpha t}}$ , for the case where the constant is $0$ . So, taking the limit we arrive at the carrying capacity to get, $$ K=\lim N=\lim e^{-\frac{r_0}{\alpha}e^{-\alpha t}}=e^{-\frac{r_0}{\alpha}}\iff\ln K=-\frac{r_0}{\alpha}\iff r_0=-\alpha\ln K $$ Now, observe $$ N(t)=e^{-\frac{r_0}{\alpha}e^{-\alpha t}}\iff\ln N=-\frac{r_0}{\alpha}e^{-\alpha t}\Rightarrow\ln N=\ln Ke^{-\alpha t} $$ so that $\frac{\ln N}{\ln K}=e^{-\alpha t}$ . Plugging all these in to the initial Gompertz equation should get you to arrive at the conclusion, I would believe. Now here's where I'm running into trouble, the fact that I have $\frac{\ln N}{\ln K}$ and not $\ln(\frac{K}{N})$ . Not sure if it's here where my mistake lies, but if you can provide some guidance that would be useful. I'll attempt derivation using the first form of the solution I got without a constant of integration equaling to zero in the mean time.","Consider the Gompertz equation that models the dynamics of the population of a single-species and convert it to the following form So here's my attempt, the problem doesn't explicitly say that the constant of integration is or not, so I consider two cases when solving and derived, where I took , and, , for the case where the constant is . So, taking the limit we arrive at the carrying capacity to get, Now, observe so that . Plugging all these in to the initial Gompertz equation should get you to arrive at the conclusion, I would believe. Now here's where I'm running into trouble, the fact that I have and not . Not sure if it's here where my mistake lies, but if you can provide some guidance that would be useful. I'll attempt derivation using the first form of the solution I got without a constant of integration equaling to zero in the mean time.",\frac{dN}{dt}=r_0e^{-\alpha t}N \frac{dN}{dt}=\alpha N\ln\left(\frac{K}{N}\right) 0  N(t) = N_0 \exp \left( \frac{r_0}{\alpha}\left(1-e^{-\alpha t}\right) \right)  N(0)=N_0 N(t)=e^{-\frac{r_0}{\alpha}e^{-\alpha t}} 0  K=\lim N=\lim e^{-\frac{r_0}{\alpha}e^{-\alpha t}}=e^{-\frac{r_0}{\alpha}}\iff\ln K=-\frac{r_0}{\alpha}\iff r_0=-\alpha\ln K   N(t)=e^{-\frac{r_0}{\alpha}e^{-\alpha t}}\iff\ln N=-\frac{r_0}{\alpha}e^{-\alpha t}\Rightarrow\ln N=\ln Ke^{-\alpha t}  \frac{\ln N}{\ln K}=e^{-\alpha t} \frac{\ln N}{\ln K} \ln(\frac{K}{N}),"['calculus', 'ordinary-differential-equations', 'derivatives', 'biology']"
86,"Whether the partial derivatives are bounded in any neighbourhood of $(0,0)$ or not.",Whether the partial derivatives are bounded in any neighbourhood of  or not.,"(0,0)","The function is given as $f(x,y)=\begin{cases}(x^{2}+y^{2})\sin \left(\frac{1}{x^{2}+y^{2}}\right)& (x,y)\ne (0,0)\\0 &(x,y)=(0,0)\end{cases}$ . The options are (a) The partial derivatives $\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}$ exist at $(0,0)$ but are unbounded in any neighbourhood of $(0,0)$ . (b) $f$ is continuous but not differential at $(0,0)$ . (c) $f$ is differential at $(0,0)$ . (d) $f$ is continuous but not differrential at $(0,0)$ I computed the partial derivative as, $$\begin{aligned}\frac{\partial f}{\partial x}\Big|_{(0,0)}&=\lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h}\\&=\lim_{h\to 0} \frac{f(h,0)-f(0,0)}{h}\\&=\lim_{h\to 0} \frac{h^{2}\sin \frac{1}{h^{2}}-0}{h}\\&=\lim_{h\to 0} h\sin \frac{1}{h^{2}}\\&=0\end{aligned}$$ Similarly, $\frac{\partial f}{\partial y}\Big|_{(0,0)}$ can be shown equal to 0. Since the partial derivatives both exits and equals zero. So are continuous, hence $f$ is differential at $(0,0)$ and hence continuous too. But how to check whether the partial derivatives are bounded in any neighbourhood of zero or not? Thanks in advance!","The function is given as . The options are (a) The partial derivatives exist at but are unbounded in any neighbourhood of . (b) is continuous but not differential at . (c) is differential at . (d) is continuous but not differrential at I computed the partial derivative as, Similarly, can be shown equal to 0. Since the partial derivatives both exits and equals zero. So are continuous, hence is differential at and hence continuous too. But how to check whether the partial derivatives are bounded in any neighbourhood of zero or not? Thanks in advance!","f(x,y)=\begin{cases}(x^{2}+y^{2})\sin \left(\frac{1}{x^{2}+y^{2}}\right)& (x,y)\ne (0,0)\\0 &(x,y)=(0,0)\end{cases} \frac{\partial f}{\partial x},\frac{\partial f}{\partial y} (0,0) (0,0) f (0,0) f (0,0) f (0,0) \begin{aligned}\frac{\partial f}{\partial x}\Big|_{(0,0)}&=\lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h}\\&=\lim_{h\to 0} \frac{f(h,0)-f(0,0)}{h}\\&=\lim_{h\to 0} \frac{h^{2}\sin \frac{1}{h^{2}}-0}{h}\\&=\lim_{h\to 0} h\sin \frac{1}{h^{2}}\\&=0\end{aligned} \frac{\partial f}{\partial y}\Big|_{(0,0)} f (0,0)","['derivatives', 'continuity', 'partial-derivative']"
87,How do I determine further solutions of the equation using Rolle's theorem?,How do I determine further solutions of the equation using Rolle's theorem?,,"I gave this equation $2^x=1+x^2$ with the $1$ st zero is $x_1=0$ and the $2$ nd zero is $x_2=1$ . (easy reading) Now I want to calculate more zeros using Rolle's theorem, and I've rearranged the function for this: $$f(x)=1+x^2-2^x$$ and formed the first two derivatives: $f'(x)=2x-\ln(2)*2^x$ and $f''(x)=2-\ln(2)^2*2^x$ So and from here it fails now. I know what Rolle's theorem says. Between two zeros of the function there is a zero of the derivative, and if $f$ is twice differentiable, then between three zeros of the function there are two zeros of the first derivative and one zero of the second derivative. Also, I saw that $f(4) > 0$ and $f(5) < 0$ . Therefore, in the interval (4,5) there is at least one more zero of $f$ according to the intermediate value theorem. But how do I determine furthermore precise real solutions of the equation?","I gave this equation with the st zero is and the nd zero is . (easy reading) Now I want to calculate more zeros using Rolle's theorem, and I've rearranged the function for this: and formed the first two derivatives: and So and from here it fails now. I know what Rolle's theorem says. Between two zeros of the function there is a zero of the derivative, and if is twice differentiable, then between three zeros of the function there are two zeros of the first derivative and one zero of the second derivative. Also, I saw that and . Therefore, in the interval (4,5) there is at least one more zero of according to the intermediate value theorem. But how do I determine furthermore precise real solutions of the equation?",2^x=1+x^2 1 x_1=0 2 x_2=1 f(x)=1+x^2-2^x f'(x)=2x-\ln(2)*2^x f''(x)=2-\ln(2)^2*2^x f f(4) > 0 f(5) < 0 f,"['analysis', 'derivatives', 'roots', 'rolles-theorem']"
88,Uniform convergence of a sequence of functions to a function (g) but their derivatives converge pointwise to a function which is not (g'),Uniform convergence of a sequence of functions to a function (g) but their derivatives converge pointwise to a function which is not (g'),,"Question is that find a sequence ( $f_n$ ) of continuously differentiable real functions defined on $[0,1]$ converges uniformly to a differentiable function ( $g$ ) and ( $f_n'$ ) converge pointwise to a function that is not ( $g'$ ). I am trying to find this function. I think that $\frac{x}{1+x^2n^2}$ converges uniformly to zero on [0,1] and derivate $\frac{1-n^2x^2}{(1+x^2n^2)^2}$ converges pointwise to 1 at zero and 0 for all other points. Is my logic correct? Any help will be appreciated.Thanks","Question is that find a sequence ( ) of continuously differentiable real functions defined on converges uniformly to a differentiable function ( ) and ( ) converge pointwise to a function that is not ( ). I am trying to find this function. I think that converges uniformly to zero on [0,1] and derivate converges pointwise to 1 at zero and 0 for all other points. Is my logic correct? Any help will be appreciated.Thanks","f_n [0,1] g f_n' g' \frac{x}{1+x^2n^2} \frac{1-n^2x^2}{(1+x^2n^2)^2}",['real-analysis']
89,Prove or disprove that this paramater-dependent integral is continously differentiable,Prove or disprove that this paramater-dependent integral is continously differentiable,,"How to prove (or perhaps disprove) that the following statement is correct: If $f: (-1,1) \to \mathbb{R}$ is continously differentiable, then $F(x) = \int_0^{\sqrt[3]{x}}t^3f(xt)dt$ is also continously differentiable on $(-1,1)$ . I know that if the upper bound of the integral was contionously differentiable, then $F$ would also be. But this is not the case here. I've tried proving this statement by definition of derivatives and finding a simple counterexample but without any luck.","How to prove (or perhaps disprove) that the following statement is correct: If is continously differentiable, then is also continously differentiable on . I know that if the upper bound of the integral was contionously differentiable, then would also be. But this is not the case here. I've tried proving this statement by definition of derivatives and finding a simple counterexample but without any luck.","f: (-1,1) \to \mathbb{R} F(x) = \int_0^{\sqrt[3]{x}}t^3f(xt)dt (-1,1) F","['real-analysis', 'integration', 'multivariable-calculus', 'derivatives', 'continuity']"
90,Prove that a function is $\mathcal{C}^n$ using the reccurence,Prove that a function is  using the reccurence,\mathcal{C}^n,"Let $n \in \mathbb{N}^{*}$ and $f:\left[0,1\left[\rightarrow \mathbb{R}\right.\right.$ a $\mathrm{C}^{n} $ function. Let $g$ the application $\left[0,1\left[\rightarrow \mathbb{R}\right.\right.$ such that $g(x)=\frac{f(x)-f(0)}{x}$ for each $x \neq 0$ and $g(0)=f^{\prime}(0)$ . Let $\mathrm{t}:\left[0,1\left[\rightarrow \mathbb{R}\right.\right.$ defined by $\mathrm{t}(x)=f(x)-\sum_{\ell=0}^{n} \frac{f^{(\ell)}(0)}{\ell !} x^{\ell}$ , and $h$ defined by $h(x)=\frac{\mathrm{t}(x)}{x}$ if $x \neq 0$ and $h(0)=0 .$ Using L'Hôpital's rule , prove that $h$ is a $\mathrm{C}^{n-1}$ function  on $[0,1[$ , and $\forall 0 \leq k \leq n-1$ we have $h^{(k)}(0)=0$ . In $]0,1[$ , it's okay. I still have a problem with $0$ , I tried using L'Hôpital's rule, and the recurrence, but I didn't get the result. Any help! I appreciate it.","Let and a function. Let the application such that for each and . Let defined by , and defined by if and Using L'Hôpital's rule , prove that is a function  on , and we have . In , it's okay. I still have a problem with , I tried using L'Hôpital's rule, and the recurrence, but I didn't get the result. Any help! I appreciate it.","n \in \mathbb{N}^{*} f:\left[0,1\left[\rightarrow \mathbb{R}\right.\right. \mathrm{C}^{n}  g \left[0,1\left[\rightarrow \mathbb{R}\right.\right. g(x)=\frac{f(x)-f(0)}{x} x \neq 0 g(0)=f^{\prime}(0) \mathrm{t}:\left[0,1\left[\rightarrow \mathbb{R}\right.\right. \mathrm{t}(x)=f(x)-\sum_{\ell=0}^{n} \frac{f^{(\ell)}(0)}{\ell !} x^{\ell} h h(x)=\frac{\mathrm{t}(x)}{x} x \neq 0 h(0)=0 . h \mathrm{C}^{n-1} [0,1[ \forall 0 \leq k \leq n-1 h^{(k)}(0)=0 ]0,1[ 0","['real-analysis', 'derivatives']"
91,Does a bounded function converge if its derivative tends to zero?,Does a bounded function converge if its derivative tends to zero?,,Suppose we have function $f:\Bbb R^+\to\Bbb R$ that is bounded $|f(t)|<M$ and differentiable such that $\lim\limits_{t\to\infty}f'(t) = 0$ . Does this imply that $f(t)$ converges?,Suppose we have function that is bounded and differentiable such that . Does this imply that converges?,f:\Bbb R^+\to\Bbb R |f(t)|<M \lim\limits_{t\to\infty}f'(t) = 0 f(t),"['real-analysis', 'derivatives']"
92,Trying to find an example of a real-valued functions that shares two or more values with its derivative.,Trying to find an example of a real-valued functions that shares two or more values with its derivative.,,"I'm trying to find an example of a real-valued functions that shares two or more values with its derivative, but f' doesn't equal f. By sharing two values a and b, I mean f = a whenever f' = a, and f = b whenever f' = b. I know it's impossible for 2 meromorphic complex entire functions (that's what my paper's about) so I need a function that's differentiable once, and not analytic on C.","I'm trying to find an example of a real-valued functions that shares two or more values with its derivative, but f' doesn't equal f. By sharing two values a and b, I mean f = a whenever f' = a, and f = b whenever f' = b. I know it's impossible for 2 meromorphic complex entire functions (that's what my paper's about) so I need a function that's differentiable once, and not analytic on C.",,"['real-analysis', 'complex-analysis', 'derivatives']"
93,Proving an interesting identity,Proving an interesting identity,,"Let $Q(z)=(z-\alpha_1)\cdots(z-\alpha_n)$ be a polynomial of degree $>1$ with distinct roots outside the real line. We have $$\sum_{j=1}^n \frac{1}{Q'(\alpha_j)}=0.$$ I know an interesting but indirect proof using the continuity of the Fourier transform, but I want to know whether there is a proof relying on more rudimentary techniques.","Let $Q(z)=(z-\alpha_1)\cdots(z-\alpha_n)$ be a polynomial of degree $>1$ with distinct roots outside the real line. We have $$\sum_{j=1}^n \frac{1}{Q'(\alpha_j)}=0.$$ I know an interesting but indirect proof using the continuity of the Fourier transform, but I want to know whether there is a proof relying on more rudimentary techniques.",,['complex-numbers']
94,How to interpret Carathéodory's theorem for a twice differentiable function?,How to interpret Carathéodory's theorem for a twice differentiable function?,,"Let $ f(x) = |x|^3 $ , compute $f'(0),f''(0),f'''(0)$ . I was trying to use Carathéodory's theorem to prove that the function is differentiable. Finding $f'(0)$ We know that the function is differentiable at $x=0$ if there exists a $f_1(x)$ continuous at $x=0$ such that $$f(x) - f(0) = f_1(x)(x-0) \cdots (*)$$ Here $f(0) = 0$ . Also we know that $f(x) = x^3, x \ge 0 $ and $f(x) = -x^3,x < 0 $ Then $f_1(x) = x^2 ; x \ge 0 $ and $f(x) = -x^2; x < 0$ . This function $f_1(x)$ is continuous at $x = 0$ and also satisfies $(*)$ Hence $f'(0) = f_1(0) = 0$ Finding $f''(0)$ This is where I am confused. To show that $f''(0)$ exist using the Carathéodory's theorem should I show that there is a function $f_2(x)$ continuous at $x= 0 $ such that $f_1(x) - f_1(0) = f_2(x)(x-0)$ or should I show that the function $f_2(x)$ satisfies $f'(x) - f'(0) = f_2(x)(x-0) ?$","Let , compute . I was trying to use Carathéodory's theorem to prove that the function is differentiable. Finding We know that the function is differentiable at if there exists a continuous at such that Here . Also we know that and Then and . This function is continuous at and also satisfies Hence Finding This is where I am confused. To show that exist using the Carathéodory's theorem should I show that there is a function continuous at such that or should I show that the function satisfies"," f(x) = |x|^3  f'(0),f''(0),f'''(0) f'(0) x=0 f_1(x) x=0 f(x) - f(0) = f_1(x)(x-0) \cdots (*) f(0) = 0 f(x) = x^3, x \ge 0  f(x) = -x^3,x < 0  f_1(x) = x^2 ; x \ge 0  f(x) = -x^2; x < 0 f_1(x) x = 0 (*) f'(0) = f_1(0) = 0 f''(0) f''(0) f_2(x) x= 0  f_1(x) - f_1(0) = f_2(x)(x-0) f_2(x) f'(x) - f'(0) = f_2(x)(x-0) ?","['real-analysis', 'derivatives']"
95,"Continuity and differentiability of f(x,y) at the origin [closed]","Continuity and differentiability of f(x,y) at the origin [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $f$ be a function defined by $$f(x,y) = \begin{cases} \dfrac{\sin x - \sin y}{x-y} & x \neq y \\ \cos x & x = y \end{cases}$$ Study the continuity and differentiability of $f$ in the origin.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let be a function defined by Study the continuity and differentiability of in the origin.","f f(x,y) = \begin{cases} \dfrac{\sin x - \sin y}{x-y} & x \neq y \\ \cos x & x = y \end{cases} f","['real-analysis', 'analysis', 'derivatives', 'continuity']"
96,Taylor expansion of a non-Taylor series,Taylor expansion of a non-Taylor series,,"Show that the following function is arbitrarily differentiable and find its Taylor expansion zero-centered. $$g:\mathbb{R}\to\mathbb{R}, \hspace{5mm} g(x):= \sum_{n=0}^{\infty} \frac{\cos(n^2x)}{2^n}.$$ Solution: At the moment, I don't know (with formal proof) if the symbols $\frac{\mathrm{d}}{\mathrm{d}x}$ , $\sum_{n=0}^{\infty}$ switch. I proceed with one of the formal definition of the derivative: $$\frac{\mathrm{d}}{\mathrm{d}x}(g) = \lim_{x \to x_0} \frac{g(x)-g(x_0)}{x-x_0} = \lim_{x \to x_0} \left(\sum_{n=0}^{\infty} \frac{\cos(n^2x)-\cos(n^2x_0)}{2^n(x-x_0)}\right) \hspace{5mm} \text{if the limit exists.}$$ I'm stuck at this point. Also I don't know if the symbols $\lim_{x\to x_0}$ and $\sum_{n=0}^{\infty}$ switch. By the way, I tried with the following expansion: $$\cos:\mathbb{R}\to\mathbb{R}, \hspace{5mm} \cos(x):= \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k)!} x^{2k}.$$ But I don't see any advantage of use this expansion. I would be grateful for any help/hint.","Show that the following function is arbitrarily differentiable and find its Taylor expansion zero-centered. Solution: At the moment, I don't know (with formal proof) if the symbols , switch. I proceed with one of the formal definition of the derivative: I'm stuck at this point. Also I don't know if the symbols and switch. By the way, I tried with the following expansion: But I don't see any advantage of use this expansion. I would be grateful for any help/hint.","g:\mathbb{R}\to\mathbb{R}, \hspace{5mm} g(x):= \sum_{n=0}^{\infty} \frac{\cos(n^2x)}{2^n}. \frac{\mathrm{d}}{\mathrm{d}x} \sum_{n=0}^{\infty} \frac{\mathrm{d}}{\mathrm{d}x}(g) = \lim_{x \to x_0} \frac{g(x)-g(x_0)}{x-x_0} = \lim_{x \to x_0} \left(\sum_{n=0}^{\infty} \frac{\cos(n^2x)-\cos(n^2x_0)}{2^n(x-x_0)}\right) \hspace{5mm} \text{if the limit exists.} \lim_{x\to x_0} \sum_{n=0}^{\infty} \cos:\mathbb{R}\to\mathbb{R}, \hspace{5mm} \cos(x):= \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k)!} x^{2k}.","['real-analysis', 'derivatives', 'power-series', 'taylor-expansion']"
97,Intuition on the higher order tangent bundles,Intuition on the higher order tangent bundles,,"The first order tangent bundle $TM$ can be thought of as the set of velocities at each point on the manifold. It can be formally defined in one of two ways: it can be the set of derivations on the manifold or the set of equivalence classes of tangent curves. I’ve read that the tangent bundle is itself a smooth manifold so you can then take the tangent bundle of that. My question would be how that would work. When I think of the tangent bundle, I think of a bunch of tangent spaces and I can’t grasp how they make a smooth manifold. Given the smooth structure, how would a higher order tangent space be defined and what would it mean? I’ve seen $T^2 M$ be defined as the set of second order derivations, as equivalence classes of curves that agree to to their second derivative, and recursively as $T^k M \equiv T(T^{k-1} M)$ . How are they related, what would the dimensionality of these tangent bundles be, and what is the geometric intuition behind it? How is the tangent bundle a smooth manifold? What is the definition of higher order tangent bundles? What is the geometric interpretation of them? What math can be done with them? Bonus: Could you explain the same for the cotangent bundle and its higher analogues along with mixed bundles like $T^6 T^{3*} M$ ?","The first order tangent bundle can be thought of as the set of velocities at each point on the manifold. It can be formally defined in one of two ways: it can be the set of derivations on the manifold or the set of equivalence classes of tangent curves. I’ve read that the tangent bundle is itself a smooth manifold so you can then take the tangent bundle of that. My question would be how that would work. When I think of the tangent bundle, I think of a bunch of tangent spaces and I can’t grasp how they make a smooth manifold. Given the smooth structure, how would a higher order tangent space be defined and what would it mean? I’ve seen be defined as the set of second order derivations, as equivalence classes of curves that agree to to their second derivative, and recursively as . How are they related, what would the dimensionality of these tangent bundles be, and what is the geometric intuition behind it? How is the tangent bundle a smooth manifold? What is the definition of higher order tangent bundles? What is the geometric interpretation of them? What math can be done with them? Bonus: Could you explain the same for the cotangent bundle and its higher analogues along with mixed bundles like ?",TM T^2 M T^k M \equiv T(T^{k-1} M) T^6 T^{3*} M,"['derivatives', 'differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'tangent-bundle']"
98,Sufficient and necessary conditions for a smooth function to have a Fourier transform with rapid decay,Sufficient and necessary conditions for a smooth function to have a Fourier transform with rapid decay,,Let $f \in C^\infty(\mathbb R) \cap L^1(\mathbb R)$ . I want to learn necessary and sufficient conditions for the Fourier transform $$\hat f (y) := \int_{\mathbb R} f(x) \exp(-2 \pi i xy) dx$$ to have rapid decay. Meaning that we have $$\lim_{|y| \to \infty} y^k \hat f(y) = 0$$ for all $k \in \mathbb N_0$ . Using the Riemann–Lebesgue lemma and the fact $$\hat{f^{(k)}}(y) = (2 \pi i y)^k \hat f(y)$$ in case $f^{(k)} \in L^1$ we get $\hat f$ has rapid decay if all derivatives of $f$ are in $L^1$ . So this condition is sufficient . Is it also necessary ? Which other sufficient and necessary conditions are there?,Let . I want to learn necessary and sufficient conditions for the Fourier transform to have rapid decay. Meaning that we have for all . Using the Riemann–Lebesgue lemma and the fact in case we get has rapid decay if all derivatives of are in . So this condition is sufficient . Is it also necessary ? Which other sufficient and necessary conditions are there?,f \in C^\infty(\mathbb R) \cap L^1(\mathbb R) \hat f (y) := \int_{\mathbb R} f(x) \exp(-2 \pi i xy) dx \lim_{|y| \to \infty} y^k \hat f(y) = 0 k \in \mathbb N_0 \hat{f^{(k)}}(y) = (2 \pi i y)^k \hat f(y) f^{(k)} \in L^1 \hat f f L^1,"['real-analysis', 'derivatives', 'fourier-analysis', 'fourier-transform']"
99,Chain rule substitution differentials,Chain rule substitution differentials,,"I'm reading through Bamberg and Sternberg, and I'm on Chapter 5. It has the attached passage. I understand it up until it says we can substitute our $dy=15x^2dx$ into the $d(y^2)=2y\space dy$ equation. It states that $dy$ replaces $h$ as our displacement, since before it was made clear that we could write $d\alpha_y(h)=\alpha'(y)h,$ so this is merely a change in notation. However, all we did in the bottom was define a function $y(x)=5x^3+1$ whose differential $dy$ so happeend to have the same name as what we replaced $h$ with. So, how come we can just substitute this in to result in an example of the chain rule? To be clear, I mean: if the $dy$ in $d\alpha=\alpha'dy$ is just a real number and if the $dy$ in $dy=15x^2dx$ is instead a function of $x,$ how can we substitute the second instance of $dy$ into the first instance of $dy$ , just because we named them the same?","I'm reading through Bamberg and Sternberg, and I'm on Chapter 5. It has the attached passage. I understand it up until it says we can substitute our into the equation. It states that replaces as our displacement, since before it was made clear that we could write so this is merely a change in notation. However, all we did in the bottom was define a function whose differential so happeend to have the same name as what we replaced with. So, how come we can just substitute this in to result in an example of the chain rule? To be clear, I mean: if the in is just a real number and if the in is instead a function of how can we substitute the second instance of into the first instance of , just because we named them the same?","dy=15x^2dx d(y^2)=2y\space dy dy h d\alpha_y(h)=\alpha'(y)h, y(x)=5x^3+1 dy h dy d\alpha=\alpha'dy dy dy=15x^2dx x, dy dy","['calculus', 'derivatives', 'differential-forms', 'chain-rule']"
