,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What does it mean to converge to a point if it is not clear what the point even is?,What does it mean to converge to a point if it is not clear what the point even is?,,"What does it mean for a sequence to converge to an element if the limit might not necessarily be defined or known, or not necessarily in the universe under consideration (whatever this means)? I am not just talking about real numbers; it can be more general. The definition of a sequence $(x_n)$ converging to $x$ seems to say: for any $\epsilon > 0$ , there is $N \in \mathbb{N}$ such that $|x_n - x| < \epsilon$ for all $n > N$ . But doesn’t this assume the existence of a point $x$ under consideration? For example, when we show that $\{p \in \mathbb{Q}: p^2 < 2\}$ “approaches” a number, e.g. by considering $1.4, 1.41, 1.414, ...$ , what does that even mean, if we have not yet constructed the real numbers? What is meant by “number” in this case? Does it even make sense to say that? Although we might not ""know"" what the limiting point $x$ is for $\{p \in \mathbb{Q}: p^2 < 2\}$ , it seems to still make sense to speak of a limit. (Again, I'm assuming we do not as yet know what real numbers might be, or what completeness is, etc.). In such a case, how should we define convergence, if we cannot have a point to explicitly refer to? In general, couldn’t we converge to “something”, but it is not at all clear what that “something” should or might be? If it isn’t clear what that something should be, then how can we even speak of converging to that something? Is this just a logical/semantic/notational thing in the definition of convergence?","What does it mean for a sequence to converge to an element if the limit might not necessarily be defined or known, or not necessarily in the universe under consideration (whatever this means)? I am not just talking about real numbers; it can be more general. The definition of a sequence converging to seems to say: for any , there is such that for all . But doesn’t this assume the existence of a point under consideration? For example, when we show that “approaches” a number, e.g. by considering , what does that even mean, if we have not yet constructed the real numbers? What is meant by “number” in this case? Does it even make sense to say that? Although we might not ""know"" what the limiting point is for , it seems to still make sense to speak of a limit. (Again, I'm assuming we do not as yet know what real numbers might be, or what completeness is, etc.). In such a case, how should we define convergence, if we cannot have a point to explicitly refer to? In general, couldn’t we converge to “something”, but it is not at all clear what that “something” should or might be? If it isn’t clear what that something should be, then how can we even speak of converging to that something? Is this just a logical/semantic/notational thing in the definition of convergence?","(x_n) x \epsilon > 0 N \in \mathbb{N} |x_n - x| < \epsilon n > N x \{p \in \mathbb{Q}: p^2 < 2\} 1.4, 1.41, 1.414, ... x \{p \in \mathbb{Q}: p^2 < 2\}","['real-analysis', 'calculus', 'limits']"
1,Approximation in the integral to calculate the age of the universe,Approximation in the integral to calculate the age of the universe,,"I'm computing the following integral: $$T=\frac{1}{H_{0}} \int^{1}_{0} \frac{da}{\sqrt{\frac{\Omega_{M}(t_{0})}{a}}\left( \sqrt{1+\frac{\Omega_{R}(t_0)}{\Omega_{M}(t_0) \,a}+\frac{\Omega_{\Lambda}(t_0) \,a^3}{\Omega_{M}(t_0) }}{} \right)}$$ with: $\Omega_{\Lambda}(t_0)= 0.73,\Omega_{M}(t_0)= 0.27,\Omega_{R}(t_0)=8.51 \cdot 10^{-5},H_0=2.26\cdot 10^{-18}s^{-1}$ At this passage I want to do: $$T=\frac{1}{H_{0}} \int^{1}_{0} \frac{da}{\sqrt{\frac{\Omega_{M}(t_{0})}{a}}\left( \sqrt{1+\frac{\Omega_{R}(t_0)}{\Omega_{M}(t_0) \,a}+\frac{\Omega_{\Lambda}(t_0) \,a^3}{\Omega_{M}(t_0) }}{} \right)} \simeq\frac{1}{H_{0}} \int^{1}_{0} \frac{da}{\sqrt{\frac{\Omega_{M}(t_{0})}{a}}\left( \sqrt{1+\frac{\Omega_{\Lambda}(t_0) \,a^3}{\Omega_{M}(t_0) }}{} \right)} $$ I can't see why this approximation makes sense.",I'm computing the following integral: with: At this passage I want to do: I can't see why this approximation makes sense.,"T=\frac{1}{H_{0}} \int^{1}_{0} \frac{da}{\sqrt{\frac{\Omega_{M}(t_{0})}{a}}\left( \sqrt{1+\frac{\Omega_{R}(t_0)}{\Omega_{M}(t_0) \,a}+\frac{\Omega_{\Lambda}(t_0) \,a^3}{\Omega_{M}(t_0) }}{} \right)} \Omega_{\Lambda}(t_0)= 0.73,\Omega_{M}(t_0)= 0.27,\Omega_{R}(t_0)=8.51 \cdot 10^{-5},H_0=2.26\cdot 10^{-18}s^{-1} T=\frac{1}{H_{0}} \int^{1}_{0} \frac{da}{\sqrt{\frac{\Omega_{M}(t_{0})}{a}}\left( \sqrt{1+\frac{\Omega_{R}(t_0)}{\Omega_{M}(t_0) \,a}+\frac{\Omega_{\Lambda}(t_0) \,a^3}{\Omega_{M}(t_0) }}{} \right)} \simeq\frac{1}{H_{0}} \int^{1}_{0} \frac{da}{\sqrt{\frac{\Omega_{M}(t_{0})}{a}}\left( \sqrt{1+\frac{\Omega_{\Lambda}(t_0) \,a^3}{\Omega_{M}(t_0) }}{} \right)} ","['calculus', 'limits', 'definite-integrals']"
2,Whether $\lim_{n\to \infty} \frac{2}{\mathsf{e}}(\sum_{k=0}^{\lfloor n/2\rfloor} \binom{n}{k}(1-\frac{2k}{n})^{n-1})^{-1/n}$ exists,Whether  exists,\lim_{n\to \infty} \frac{2}{\mathsf{e}}(\sum_{k=0}^{\lfloor n/2\rfloor} \binom{n}{k}(1-\frac{2k}{n})^{n-1})^{-1/n},"Problem : Decide whether or not $\lim_{n\to \infty} \frac{2}{\mathsf{e}}\left(\sum_{k=0}^{\lfloor n/2\rfloor} \binom{n}{k} \left(1-\frac{2k}{n}\right)^{n-1}\right)^{-1/n}$ exists. Background Information : I encountered this problem, when I tried to answer the following question Interval of convergence of Lagrange's infinite series In my answer there, I $\color{blue}{\textrm{GUESS}}$ that the limit exists and equals to the Laplace limit $0.66274 34193 49181 58097 47420 97109 25290...$ which is the solution of the equation $x \mathrm{e}^{\sqrt{1+x^2}} = 1 + \sqrt{1+x^2}$ . (For Laplace limit and more information, see Ref. [1]-[4].) Let $B_n \triangleq \frac{2}{\mathsf{e}}\left(\sum_{k=0}^{\lfloor n/2\rfloor} \binom{n}{k} \left(1-\frac{2k}{n}\right)^{n-1}\right)^{-1/n}$ . Some numerical experiments show that $B_n$ is non-increasing. I tried to prove it, but have not yet succeeded. When $n=1000$ (Maple can not easily evaluate $B_n$ for larger $n$ ), $B_{1000} = 0.6627434531...$ Any comments and solutions are welcome. Reference [1] https://en.wikipedia.org/wiki/Laplace_limit [2] http://www.mygeodesy.id.au/documents/Solutions%20of%20Keplers%20Equation.pdf [3] https://arxiv.org/pdf/1305.3438.pdf [4] ""Orbital Mechanics for Engineering Students"", http://www.nssc.ac.cn/wxzygx/weixin/201607/P020160718380095698873.pdf","Problem : Decide whether or not exists. Background Information : I encountered this problem, when I tried to answer the following question Interval of convergence of Lagrange's infinite series In my answer there, I that the limit exists and equals to the Laplace limit which is the solution of the equation . (For Laplace limit and more information, see Ref. [1]-[4].) Let . Some numerical experiments show that is non-increasing. I tried to prove it, but have not yet succeeded. When (Maple can not easily evaluate for larger ), Any comments and solutions are welcome. Reference [1] https://en.wikipedia.org/wiki/Laplace_limit [2] http://www.mygeodesy.id.au/documents/Solutions%20of%20Keplers%20Equation.pdf [3] https://arxiv.org/pdf/1305.3438.pdf [4] ""Orbital Mechanics for Engineering Students"", http://www.nssc.ac.cn/wxzygx/weixin/201607/P020160718380095698873.pdf",\lim_{n\to \infty} \frac{2}{\mathsf{e}}\left(\sum_{k=0}^{\lfloor n/2\rfloor} \binom{n}{k} \left(1-\frac{2k}{n}\right)^{n-1}\right)^{-1/n} \color{blue}{\textrm{GUESS}} 0.66274 34193 49181 58097 47420 97109 25290... x \mathrm{e}^{\sqrt{1+x^2}} = 1 + \sqrt{1+x^2} B_n \triangleq \frac{2}{\mathsf{e}}\left(\sum_{k=0}^{\lfloor n/2\rfloor} \binom{n}{k} \left(1-\frac{2k}{n}\right)^{n-1}\right)^{-1/n} B_n n=1000 B_n n B_{1000} = 0.6627434531...,"['real-analysis', 'limits', 'convergence-divergence']"
3,Can this function be defined in a way to make it continuous at $x=0$?,Can this function be defined in a way to make it continuous at ?,x=0,"We have $$f=\frac{x}{\vert x-1 \vert - \vert x +1 \vert}$$ If we want to ""define"" this function to be continuous at $x=0$ , it's limit at $0$ must equal $f(0)$ . So we should find this limit and assign it to be equal to $f(0)$ , then the function is continuous at $0$ . Since we are looking at the function when $x\to 0$ , $x\neq 0$ . Lets divide both sides by $x$ . $$f=\frac{x}{\vert x-1 \vert - \vert x +1 \vert}=\frac{1}{\frac{\vert x-1 \vert}{x}-\frac{\vert x+1\vert}{x}}=\frac{1}{\vert 1-\frac{1}{x}\vert - \vert 1+ \frac{1}{x}\vert }$$ We can use $\lim \phi(x)^{-1}=\frac{1}{\lim \phi(x)}$ here ( the limit $\neq$ 0, by hypothesis ). The inverse of the limit of $\phi(x)=\vert 1 - \frac{1}{x} \vert-\vert 1+\frac{1}{x}\vert$ , when $x\to 0$ . If $x<1$ , we have that $$\frac{1}{x}>1\implies0>1-\frac{1}{x}\implies \Bigg\vert 1-\frac{1}{x}\Bigg\vert=-\Big(1-\frac{1}{x}\Big)$$ Now if $x>0$ , we have that $$\Bigg\vert 1 - \frac{1}{x} \Bigg\vert-\Bigg\vert 1+\frac{1}{x}\Bigg\vert=-2$$ and if $x<0$ , then $$\Bigg\vert 1 - \frac{1}{x} \Bigg\vert-\Bigg\vert 1+\frac{1}{x}\Bigg\vert=1-\frac{1}{x}-1-\frac{1}{x}=\frac{(-2)}{x}$$ The limit of $f$ when $x\to 0$ appears to be $\frac{-1}{2}$ . Could anyone tell me what errors I made in the limit finding process?","We have If we want to ""define"" this function to be continuous at , it's limit at must equal . So we should find this limit and assign it to be equal to , then the function is continuous at . Since we are looking at the function when , . Lets divide both sides by . We can use here ( the limit 0, by hypothesis ). The inverse of the limit of , when . If , we have that Now if , we have that and if , then The limit of when appears to be . Could anyone tell me what errors I made in the limit finding process?",f=\frac{x}{\vert x-1 \vert - \vert x +1 \vert} x=0 0 f(0) f(0) 0 x\to 0 x\neq 0 x f=\frac{x}{\vert x-1 \vert - \vert x +1 \vert}=\frac{1}{\frac{\vert x-1 \vert}{x}-\frac{\vert x+1\vert}{x}}=\frac{1}{\vert 1-\frac{1}{x}\vert - \vert 1+ \frac{1}{x}\vert } \lim \phi(x)^{-1}=\frac{1}{\lim \phi(x)} \neq \phi(x)=\vert 1 - \frac{1}{x} \vert-\vert 1+\frac{1}{x}\vert x\to 0 x<1 \frac{1}{x}>1\implies0>1-\frac{1}{x}\implies \Bigg\vert 1-\frac{1}{x}\Bigg\vert=-\Big(1-\frac{1}{x}\Big) x>0 \Bigg\vert 1 - \frac{1}{x} \Bigg\vert-\Bigg\vert 1+\frac{1}{x}\Bigg\vert=-2 x<0 \Bigg\vert 1 - \frac{1}{x} \Bigg\vert-\Bigg\vert 1+\frac{1}{x}\Bigg\vert=1-\frac{1}{x}-1-\frac{1}{x}=\frac{(-2)}{x} f x\to 0 \frac{-1}{2},"['real-analysis', 'limits', 'continuity']"
4,Using integral comparison to estimate how fast the partial sums of $\sum_{k=1}^N\frac1{k^3}$ converge,Using integral comparison to estimate how fast the partial sums of  converge,\sum_{k=1}^N\frac1{k^3},"Consider the following infinite sum and its partial sums. $$ S = \sum_{k=1}^{\infty } \frac{1}{k^3} $$ $$ S_N = \sum _{k=1}^{N} \frac{1}{k^3} $$ Use integral comparison to estimate how fast $S_N$ tends to $S$ . That is, show that $0<S−S_N<CN^{−q}$ giving the best values you can for the constants $C$ and $q$ . My work is as follows: The lower Riemann sum is given by $\sum_{k=2}^{\infty} \frac{1}{k^3}$ . The upper Riemann sum is $\sum_{k=1}^{\infty} \frac{1}{k^3}$ . Since $ S = \sum_{k=1}^{\infty } \frac{1}{k^3} $ , the the lower Riemman Sum is $S-1 = \sum_{k=2}^{\infty} \frac{1}{k^3}$ . Thus, $S-1 < \int_{1}^{\infty} \frac{dx}{x^3} < S$ . We could evaluate the improper integral if it converges, however it shows to be inconclusive. It happens to be equal to $\zeta(3)$ , however that is out of scope of the class I am taking and it is not accepted. I don't know where to go from there. Also, what can I do with $ S_N = \sum _{k=1}^{N} \frac{1}{k^3} $ , I am stuck on how to proceed.","Consider the following infinite sum and its partial sums. Use integral comparison to estimate how fast tends to . That is, show that giving the best values you can for the constants and . My work is as follows: The lower Riemann sum is given by . The upper Riemann sum is . Since , the the lower Riemman Sum is . Thus, . We could evaluate the improper integral if it converges, however it shows to be inconclusive. It happens to be equal to , however that is out of scope of the class I am taking and it is not accepted. I don't know where to go from there. Also, what can I do with , I am stuck on how to proceed.", S = \sum_{k=1}^{\infty } \frac{1}{k^3}   S_N = \sum _{k=1}^{N} \frac{1}{k^3}  S_N S 0<S−S_N<CN^{−q} C q \sum_{k=2}^{\infty} \frac{1}{k^3} \sum_{k=1}^{\infty} \frac{1}{k^3}  S = \sum_{k=1}^{\infty } \frac{1}{k^3}  S-1 = \sum_{k=2}^{\infty} \frac{1}{k^3} S-1 < \int_{1}^{\infty} \frac{dx}{x^3} < S \zeta(3)  S_N = \sum _{k=1}^{N} \frac{1}{k^3} ,"['calculus', 'integration']"
5,Find without L'Hospital's rule: $\lim\limits_{x \to 2} \frac{\sqrt{17 - 2x^{2}}\sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} - 9}{(x - 2)^{2}}$,Find without L'Hospital's rule:,\lim\limits_{x \to 2} \frac{\sqrt{17 - 2x^{2}}\sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} - 9}{(x - 2)^{2}},$A = \sqrt{17 - 2x^{2}} \sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} \\  \Rightarrow \lim\limits_{x \to 2} \frac{\sqrt{17 - 2x^{2}}\sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} - 9}{(x - 2)^{2}} \\  =  \lim\limits_{x \to 2} \frac{A^{6} - 9^{6}}{(x - 2)^{2}(A^{5} + 9A^{4} + ... + 9^{5})} \\  =  \lim\limits_{x \to 2} \frac{(x - 2)^{2}(-72x^{10} - 192x^{9} + 940x^{8} + 2576x^{7} + 874x^{6} + 1992x^{5} - 24543x^{4} - 73908x^{3} - 82540x^{2} - 200414x - 102154)}{(x - 2)^{2}(A^{5} + 9A^{4} + ... + 9^{5})} \\  =  \frac{-2.11.3^{10}}{6.9^{5}} = \frac{-11}{3}.$ Am I right? Is there a simple way?,Am I right? Is there a simple way?,"A = \sqrt{17 - 2x^{2}} \sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} \\ 
\Rightarrow \lim\limits_{x \to 2} \frac{\sqrt{17 - 2x^{2}}\sqrt[3]{3x^{3} - 2x^{2} + 8x - 5} - 9}{(x - 2)^{2}} \\ 
=  \lim\limits_{x \to 2} \frac{A^{6} - 9^{6}}{(x - 2)^{2}(A^{5} + 9A^{4} + ... + 9^{5})} \\ 
=  \lim\limits_{x \to 2} \frac{(x - 2)^{2}(-72x^{10} - 192x^{9} + 940x^{8} + 2576x^{7} + 874x^{6} + 1992x^{5} - 24543x^{4} - 73908x^{3} - 82540x^{2} - 200414x - 102154)}{(x - 2)^{2}(A^{5} + 9A^{4} + ... + 9^{5})} \\ 
=  \frac{-2.11.3^{10}}{6.9^{5}} = \frac{-11}{3}.","['limits', 'functions', 'limits-without-lhopital']"
6,On the ratio $\frac{F_n}{B_n}$,On the ratio,\frac{F_n}{B_n},"One of the interesting limits that I came up with is: $$\lim_{n\to\infty} \frac{F_{n}}{B_{n}}\;\;\;\;\;\;\;\;\;\; \left( n \in \mathbb N^+\right)$$ Where $F_n$ is the nth Fibonacci number and $B_n$ is the nth Bell number . If $n$ is a natural odd number then it can be written as $n=2k-1$ , where $k\in \mathbb N^+$ , Using Stirling's approximation for the double factorial denoted $n!!=\left(2k-1\right)!!$ and the relation $B_{n}\ge n!!$ we have: $$0<\frac{F_{n}}{B_{n}} <\frac{F_{n}}{n!!}\sim  \frac{\left(\frac{1+\sqrt{5}}{2}\right)^{2k-1}-\left(\frac{1-\sqrt{5}}{2}\right)^{2k-1}}{\sqrt{5}}\cdot\frac{2^{k}\sqrt{2\pi k}\left(\frac{k}{e}\right)^{k}}{\sqrt{4\pi k}\left(\frac{2k}{e}\right)^{2k}}<\frac{2\cdot2^{k}}{k^{k}}$$ Taking the limit as $k \to \infty$ and using squeeze theorem follows: $$\lim_{n\to\infty} \frac{F_{n}}{B_{n}}=0$$ Which means as $n$ gets larger,the fraction with the numerator counting  the number of ways to tile a board of size $1×n$ with squares and dominoes of size $1×1$ and $1×2$ respectively and the dinominator counting all possible partitions of a set with cardinality $n+1$ gets smaller. The same can be done for $n$ even. For more information refer to this link . Note : I've already proved that for all $k \in \mathbb N$ the relation $B_k\ge F_k$ holds, using this we conclude that: $$0<\frac{F_{n}}{B_{n}}\le1$$ The question is that: does there exist a more elegant way to prove this convergence?","One of the interesting limits that I came up with is: Where is the nth Fibonacci number and is the nth Bell number . If is a natural odd number then it can be written as , where , Using Stirling's approximation for the double factorial denoted and the relation we have: Taking the limit as and using squeeze theorem follows: Which means as gets larger,the fraction with the numerator counting  the number of ways to tile a board of size with squares and dominoes of size and respectively and the dinominator counting all possible partitions of a set with cardinality gets smaller. The same can be done for even. For more information refer to this link . Note : I've already proved that for all the relation holds, using this we conclude that: The question is that: does there exist a more elegant way to prove this convergence?",\lim_{n\to\infty} \frac{F_{n}}{B_{n}}\;\;\;\;\;\;\;\;\;\; \left( n \in \mathbb N^+\right) F_n B_n n n=2k-1 k\in \mathbb N^+ n!!=\left(2k-1\right)!! B_{n}\ge n!! 0<\frac{F_{n}}{B_{n}} <\frac{F_{n}}{n!!}\sim  \frac{\left(\frac{1+\sqrt{5}}{2}\right)^{2k-1}-\left(\frac{1-\sqrt{5}}{2}\right)^{2k-1}}{\sqrt{5}}\cdot\frac{2^{k}\sqrt{2\pi k}\left(\frac{k}{e}\right)^{k}}{\sqrt{4\pi k}\left(\frac{2k}{e}\right)^{2k}}<\frac{2\cdot2^{k}}{k^{k}} k \to \infty \lim_{n\to\infty} \frac{F_{n}}{B_{n}}=0 n 1×n 1×1 1×2 n+1 n k \in \mathbb N B_k\ge F_k 0<\frac{F_{n}}{B_{n}}\le1,['limits']
7,Find $\lim\limits_{x \to 1} \dfrac{f_{n+1}(x) - f_n(x)}{(1-x)^{n+1}}$ where $f_n(x) =x^{x^{...^{x}}}$.,Find  where .,\lim\limits_{x \to 1} \dfrac{f_{n+1}(x) - f_n(x)}{(1-x)^{n+1}} f_n(x) =x^{x^{...^{x}}},"For $n \ge 1$ and $x \in (0, \infty)$ , consider the function: $$f_n(x) = x^{x^{...^{x}}}$$ where $n$ represents the number of $x$ 's in $f$ . For example, we'd have $f_1(x) = x$ , $f_2(x)=x^x$ , $f_3(x) = x^{x^x}$ and so on. I have to find the limit: $$\lim\limits_{x \to 1} \dfrac{f_{n + 1}(x) - f_{n}(x)}{(1 - x)^{n + 1}}$$ I don't know how to solve this. I made the observation that: $$f_{n + 1}(x) = (f_n(x)) ^ x$$ but it didn't help me all that much. However I look at it, it seems like I have to apply L'Hospital, but I don't see any way of finding the derivative of $f_n(x)$ . Is there some other approach that I should use?","For and , consider the function: where represents the number of 's in . For example, we'd have , , and so on. I have to find the limit: I don't know how to solve this. I made the observation that: but it didn't help me all that much. However I look at it, it seems like I have to apply L'Hospital, but I don't see any way of finding the derivative of . Is there some other approach that I should use?","n \ge 1 x \in (0, \infty) f_n(x) = x^{x^{...^{x}}} n x f f_1(x) = x f_2(x)=x^x f_3(x) = x^{x^x} \lim\limits_{x \to 1} \dfrac{f_{n + 1}(x) - f_{n}(x)}{(1 - x)^{n + 1}} f_{n + 1}(x) = (f_n(x)) ^ x f_n(x)",['calculus']
8,"If $\lim_{x\to 0}{f(x)} = \lim_{x\to 0}{g(x)}=0$, then is $\lim_{x\to 0}\frac{\sin f(x)}{g(x)}= \lim_{x\to 0}\frac {f(x)} {g(x)}$?","If , then is ?",\lim_{x\to 0}{f(x)} = \lim_{x\to 0}{g(x)}=0 \lim_{x\to 0}\frac{\sin f(x)}{g(x)}= \lim_{x\to 0}\frac {f(x)} {g(x)},I have been playing around with this identity for a while: $$\lim_{x\to 0}\frac{\sin ax}{bx} = \frac {a} {b}$$ What I realized is that I can generalize this as such: $$\lim_{x\to 0}\frac{\sin f(x)}{g(x)}= \lim_{x\to 0}\frac {f(x)} {g(x)}$$ where $$\lim_{x\to 0}{f(x)} = \lim_{x\to 0}{g(x)}=0$$ Can you help me figure out whether this is true or not? I have literally tried all functions I could think of and couldn't find any counterexamples. I tried using L'Hôpital's rule but I couldn't prove it that way. I think an epsilon-delta sytle proof could be nice but I don't know how to do it. I even asked a few math teachers about this in my high school but none could make sense of it. I would really appreciate some help here.,I have been playing around with this identity for a while: What I realized is that I can generalize this as such: where Can you help me figure out whether this is true or not? I have literally tried all functions I could think of and couldn't find any counterexamples. I tried using L'Hôpital's rule but I couldn't prove it that way. I think an epsilon-delta sytle proof could be nice but I don't know how to do it. I even asked a few math teachers about this in my high school but none could make sense of it. I would really appreciate some help here.,\lim_{x\to 0}\frac{\sin ax}{bx} = \frac {a} {b} \lim_{x\to 0}\frac{\sin f(x)}{g(x)}= \lim_{x\to 0}\frac {f(x)} {g(x)} \lim_{x\to 0}{f(x)} = \lim_{x\to 0}{g(x)}=0,"['real-analysis', 'calculus', 'limits', 'trigonometry', 'limits-without-lhopital']"
9,Limit involving prime and composite numbers,Limit involving prime and composite numbers,,Can someone help me to figure out what $$\lim_{n\to\infty} \frac {c_n}n-\frac{c_n}{p_n}-\frac {c_n}{n^2}$$ is equal to? I am pretty sure its $1$ and i tried many different things but i couldnt figure it out. $c_n$ is the nth composite number excluding $1$ and $p_n$ is the nth prime number. This limit is equal to $$\lim_{n\to\infty} c_n\frac {\gamma(n)}{n^2}$$ where $\gamma(x)$ is equal to how many numbers less or equal to $x$ are composite. Its kinda the inverse function of $c_n$,Can someone help me to figure out what is equal to? I am pretty sure its and i tried many different things but i couldnt figure it out. is the nth composite number excluding and is the nth prime number. This limit is equal to where is equal to how many numbers less or equal to are composite. Its kinda the inverse function of,\lim_{n\to\infty} \frac {c_n}n-\frac{c_n}{p_n}-\frac {c_n}{n^2} 1 c_n 1 p_n \lim_{n\to\infty} c_n\frac {\gamma(n)}{n^2} \gamma(x) x c_n,"['limits', 'prime-numbers']"
10,What Can We Say About the Continuity of $y=\frac{x}{x}$ at $x=0$?,What Can We Say About the Continuity of  at ?,y=\frac{x}{x} x=0,"If we can't divide by $0$ , should $\frac{x}{x}$ be discontinuous and undefined at $x=0$ or is it continuous with value $1$ ? Most online graph calculators plot a continuous curve. If it's continuous at $x=0$ with $y=1$ , then we should be able to say that $\frac{(a-b)}{(a-b)} = 1$ at $a=b$ . Or $q^2*\frac{p}{q}$ is $0$ at $q = 0$ . And that whole proof of $2=1$ would hold true. The graphs online for say $\frac{x^2-4}{x-2}$ at $x=2$ are puzzling me.","If we can't divide by , should be discontinuous and undefined at or is it continuous with value ? Most online graph calculators plot a continuous curve. If it's continuous at with , then we should be able to say that at . Or is at . And that whole proof of would hold true. The graphs online for say at are puzzling me.",0 \frac{x}{x} x=0 1 x=0 y=1 \frac{(a-b)}{(a-b)} = 1 a=b q^2*\frac{p}{q} 0 q = 0 2=1 \frac{x^2-4}{x-2} x=2,"['limits', 'continuity', 'calculator']"
11,Calculate $\lim\limits_{x\rightarrow 0^+} \int\limits_0^1 \ln(1+\sin(tx))dt$,Calculate,\lim\limits_{x\rightarrow 0^+} \int\limits_0^1 \ln(1+\sin(tx))dt,"Calculate $$\lim_{x\rightarrow 0^+}\int_0^1\ln(1+\sin(tx))dt$$ My try: $$\lim_{x\rightarrow 0^+} \int_0^1 \ln (1+ \sin (tx)) dt=\lim_{x\rightarrow 0^+} ([t \ln (1+\sin (tx))]^1_0 - \int_0^1 \frac{t \cos (tx) x}{1+ \sin (tx)} dt)$$ Then I want to use: $$u=1+\sin (tx), du=\cos (tx) x$$ But then I have: $$\lim_{x\rightarrow 0^+}([t \ln (1+\sin (tx))]^1_0 - \int_1^{1+\sin x} \frac{\arcsin(u-1)}{ux} du)$$ So I think my idea about $u$ is not helpfull and I need other idea. Can you help me?",Calculate My try: Then I want to use: But then I have: So I think my idea about is not helpfull and I need other idea. Can you help me?,"\lim_{x\rightarrow 0^+}\int_0^1\ln(1+\sin(tx))dt \lim_{x\rightarrow 0^+} \int_0^1 \ln (1+ \sin (tx)) dt=\lim_{x\rightarrow 0^+} ([t \ln (1+\sin (tx))]^1_0 - \int_0^1 \frac{t \cos (tx) x}{1+ \sin (tx)} dt) u=1+\sin (tx), du=\cos (tx) x \lim_{x\rightarrow 0^+}([t \ln (1+\sin (tx))]^1_0 - \int_1^{1+\sin x} \frac{\arcsin(u-1)}{ux} du) u","['real-analysis', 'integration', 'limits', 'definite-integrals']"
12,Find the limit as $x \to 0$ with integral from $0$ to $x$ of $\cos(t^3)/(t+x)$,Find the limit as  with integral from  to  of,x \to 0 0 x \cos(t^3)/(t+x),Find the limit $$\lim_{x\to 0}\int_0^x \frac{\cos(t^3)}{t+x} dt$$ Can we use that $\frac{\cos(t^3)}{t+x}$ ~ $\frac{1}{t+x}$ at $0$ and take this integral to be $\ln(2x) - \ln(x)  = \ln 2$ ? The given answer is $\ln 2$,Find the limit Can we use that ~ at and take this integral to be ? The given answer is,\lim_{x\to 0}\int_0^x \frac{\cos(t^3)}{t+x} dt \frac{\cos(t^3)}{t+x} \frac{1}{t+x} 0 \ln(2x) - \ln(x)  = \ln 2 \ln 2,"['calculus', 'limits']"
13,Finding $\lim\limits_{x\to1^-}\Bigl(\prod\limits_{n=0}^{\infty}\Bigl(\frac{1+x^{n+1}}{1+x^n}\Bigr)^{x^n}\Bigr)$,Finding,\lim\limits_{x\to1^-}\Bigl(\prod\limits_{n=0}^{\infty}\Bigl(\frac{1+x^{n+1}}{1+x^n}\Bigr)^{x^n}\Bigr),Compute : $$L=\lim_{x\to 1^-} \left(\prod_{n=0}^{\infty} \left(\frac {1+x^{n+1}}{1+x^n}\right)^{x^n}\right)$$ My try: Writing out the first few terms I noticed that the limit can be expressed as $$L=\lim_{x\to 1^-} \frac 12\left(\prod_{n=1}^{\infty} (1+x^n)^{x^{n-1} -x^n}\right)$$ $$L=\frac 12\lim_{x\to 1^-} \left(\prod_{n=1}^{\infty} (1+x^n)^{x^{n-1}}\right)^{1-x}$$ Converting to exponential form I get $$L=\frac 12 \exp {\left(\lim_{x\to 1^-} (1-x)\sum_{n=1}^{\infty} \left(x^{n-1}\ln(1+x^n)\right)\right)}$$ And got stuck here. I did try to use approximation $\ln x\sim x$ hereon to get final answer as $\frac {\sqrt e}{2}$ but I think it was improper to use approximation and hence I believe that the answer I got is flawed. I also noticed that the form I obtained is quite similar to be used for a Riemann Sum but I didn't get a way to tackle the problem using that way. Can someone please help me with this problem....,Compute : My try: Writing out the first few terms I noticed that the limit can be expressed as Converting to exponential form I get And got stuck here. I did try to use approximation hereon to get final answer as but I think it was improper to use approximation and hence I believe that the answer I got is flawed. I also noticed that the form I obtained is quite similar to be used for a Riemann Sum but I didn't get a way to tackle the problem using that way. Can someone please help me with this problem....,L=\lim_{x\to 1^-} \left(\prod_{n=0}^{\infty} \left(\frac {1+x^{n+1}}{1+x^n}\right)^{x^n}\right) L=\lim_{x\to 1^-} \frac 12\left(\prod_{n=1}^{\infty} (1+x^n)^{x^{n-1} -x^n}\right) L=\frac 12\lim_{x\to 1^-} \left(\prod_{n=1}^{\infty} (1+x^n)^{x^{n-1}}\right)^{1-x} L=\frac 12 \exp {\left(\lim_{x\to 1^-} (1-x)\sum_{n=1}^{\infty} \left(x^{n-1}\ln(1+x^n)\right)\right)} \ln x\sim x \frac {\sqrt e}{2},"['real-analysis', 'limits', 'infinite-product', 'riemann-sum']"
14,Prove that $\lim_{x\to\infty}\sum_{n=1}^{\infty}\frac x{n^2+x^2}$ exists and is positive,Prove that  exists and is positive,\lim_{x\to\infty}\sum_{n=1}^{\infty}\frac x{n^2+x^2},"Show That $$\sum_{n=1}^\infty{1\over x ^2+n^2} \sim \frac1x$$ as $x\to \infty.$ It is enough to show that $\lim_{x\to\infty}\sum_{n=1}^{\infty}\frac x{n^2+x^2}$ exists and is positive $$=\lim_{x\to\infty}\lim_{k\to\infty}{x\over x^2}\sum_{n=1}^{kx}{1\over 1+(n/x)^2}$$ $$=\lim_{k\to\infty}\lim_{x\to\infty}{1\over x}\sum_{n=1}^{kx}{1\over 1+(n/x)^2}$$ $$=\lim_{k\to\infty}\int_0^k{1\over1+n^2}dn$$ $$=\int_0^\infty{1\over1+n^2}dn=\pi/2$$ According to Desmos, my calculations are correct but I'm worried about the rigour of the proof. How Do I justify my limit swaps and conversion to Riemann sum","Show That as It is enough to show that exists and is positive According to Desmos, my calculations are correct but I'm worried about the rigour of the proof. How Do I justify my limit swaps and conversion to Riemann sum",\sum_{n=1}^\infty{1\over x ^2+n^2} \sim \frac1x x\to \infty. \lim_{x\to\infty}\sum_{n=1}^{\infty}\frac x{n^2+x^2} =\lim_{x\to\infty}\lim_{k\to\infty}{x\over x^2}\sum_{n=1}^{kx}{1\over 1+(n/x)^2} =\lim_{k\to\infty}\lim_{x\to\infty}{1\over x}\sum_{n=1}^{kx}{1\over 1+(n/x)^2} =\lim_{k\to\infty}\int_0^k{1\over1+n^2}dn =\int_0^\infty{1\over1+n^2}dn=\pi/2,"['real-analysis', 'limits', 'proof-verification']"
15,Prove a sequence with bounded variation converges.,Prove a sequence with bounded variation converges.,,"A sequence is said to have bounded variation if: $$ \exists M \in\Bbb R: \sigma_n = |x_2 - x_1| + |x_3 - x_2| + \cdots + |x_{n+1} - x_n| \le M,\ \forall n\in\Bbb N $$ Prove that boundedness of variation implies convergence of $\{x_n\}$ This question is based on my previous question here , where I needed to prove 'convergence implies boundedness of variation'. Now I want to do the opposite. First, note that $\sigma_n \ge 0,\ \forall n\in \Bbb N$ . The sequence is also convergent by monotone convergence theorem, because $\sigma_n$ is monotonically increasing: $$ \sigma_n \le M,\ \sigma_{n+1} \ge \sigma_n \implies \exists \lim_{n\to\infty}\sigma_n = L $$ Then $\sigma_n$ satisfy Cauchy's criteria, thus we may fix any $p \in\Bbb N$ , such that: $$ \lim_{n\to\infty}(\sigma_{n+p} - \sigma_n) = 0 $$ Consider the difference: $$ \sigma_{n+p} - \sigma_n = \sum_{k=n+1}^{n+p}|x_k - x_{k-1}| $$ Writing the limit for both sides: $$ \lim_{n\to\infty}(\sigma_{n+p} - \sigma_n) = \lim_{n\to\infty}\sum_{k=n+1}^{n+p}|x_k - x_{k-1}| = 0 $$ And that is only possible in case every term is the sum tends to 0 no matter what $p$ we choose, which means: $$ \exists \lim_{n\to\infty} |x_{n+p} - x_{n}| = 0 $$ Therefore $x_n$ is Cauchy, hence convergent. I would like to ask for verification of my proof. If the above is invalid, what would be a proper proof?","A sequence is said to have bounded variation if: Prove that boundedness of variation implies convergence of This question is based on my previous question here , where I needed to prove 'convergence implies boundedness of variation'. Now I want to do the opposite. First, note that . The sequence is also convergent by monotone convergence theorem, because is monotonically increasing: Then satisfy Cauchy's criteria, thus we may fix any , such that: Consider the difference: Writing the limit for both sides: And that is only possible in case every term is the sum tends to 0 no matter what we choose, which means: Therefore is Cauchy, hence convergent. I would like to ask for verification of my proof. If the above is invalid, what would be a proper proof?","
\exists M \in\Bbb R: \sigma_n = |x_2 - x_1| + |x_3 - x_2| + \cdots + |x_{n+1} - x_n| \le M,\ \forall n\in\Bbb N
 \{x_n\} \sigma_n \ge 0,\ \forall n\in \Bbb N \sigma_n 
\sigma_n \le M,\ \sigma_{n+1} \ge \sigma_n \implies \exists \lim_{n\to\infty}\sigma_n = L
 \sigma_n p \in\Bbb N 
\lim_{n\to\infty}(\sigma_{n+p} - \sigma_n) = 0
 
\sigma_{n+p} - \sigma_n = \sum_{k=n+1}^{n+p}|x_k - x_{k-1}|
 
\lim_{n\to\infty}(\sigma_{n+p} - \sigma_n) = \lim_{n\to\infty}\sum_{k=n+1}^{n+p}|x_k - x_{k-1}| = 0
 p 
\exists \lim_{n\to\infty} |x_{n+p} - x_{n}| = 0
 x_n","['real-analysis', 'limits', 'proof-verification']"
16,"Disprove or prove using delta-epsilon definition of limit that $\lim_{(x,y) \to (0,0)}{\frac{x^3-y^3}{x^2-y^2}} = 0$ [duplicate]",Disprove or prove using delta-epsilon definition of limit that  [duplicate],"\lim_{(x,y) \to (0,0)}{\frac{x^3-y^3}{x^2-y^2}} = 0","This question already has answers here : What is $\lim_{(x,y)\rightarrow(0,0)} \frac{x^3-y^3}{x^2-y^2}$? (2 answers) Find $\lim \limits_{(x,y) \rightarrow (0,0)}\frac{x^3 y^3 }{x^2+y^2}$ (epsilon delta proof) (1 answer) Closed 5 years ago . I want to prove if the following limit exists, using epsilon-delta definition, or prove it doesn't exist: $$\lim_{(x,y) \to (0,0)}{\frac{x^3-y^3}{x^2-y^2}} = 0$$ My attempt:  First I proved some directional limits, like for $y=mx$ , and $y=ax^n$ , and for all of them I got 0. So I conjectured that this limit exists and it's 0. Then I have to prove: $$\forall \delta \gt 0 : \exists \epsilon \gt 0 : \|(x,y)\| \lt \epsilon \rightarrow \left| \frac{x^3-y^3}{x^2-y^2}\right| \lt \delta$$ First I noted that $\frac{x^3-y^3}{x^2-y^2} = \frac{(x^2+xy+y^2)(x-y)}{(x+y)(x-y)} = \frac{x^2+xy+y^2}{x+y}$ . Then I did $\left|\frac{x^2+xy+y^2}{x+y}\right| \leq \left|\frac{x(x+y)}{x+y}\right|+\frac{y^2}{\vert x+y\vert} = \vert x \vert + \frac{y^2}{\vert x+y\vert}$ Using $\|(x,y)\| = \vert x\vert + \vert y\vert$ and assuming $\|(x,y)\| \lt \epsilon$ $\vert x\vert + \vert y\vert  = \vert x \vert + \frac{ y^2}{\vert y \vert} \geq \vert x \vert+\frac{y^2}{\vert y\vert+\vert x\vert}$ but I can't continue from that since $\vert x + y \vert \leq \vert x \vert + \vert y \vert$ . I don't know what else to try.","This question already has answers here : What is $\lim_{(x,y)\rightarrow(0,0)} \frac{x^3-y^3}{x^2-y^2}$? (2 answers) Find $\lim \limits_{(x,y) \rightarrow (0,0)}\frac{x^3 y^3 }{x^2+y^2}$ (epsilon delta proof) (1 answer) Closed 5 years ago . I want to prove if the following limit exists, using epsilon-delta definition, or prove it doesn't exist: My attempt:  First I proved some directional limits, like for , and , and for all of them I got 0. So I conjectured that this limit exists and it's 0. Then I have to prove: First I noted that . Then I did Using and assuming but I can't continue from that since . I don't know what else to try.","\lim_{(x,y) \to (0,0)}{\frac{x^3-y^3}{x^2-y^2}} = 0 y=mx y=ax^n \forall \delta \gt 0 : \exists \epsilon \gt 0 : \|(x,y)\| \lt \epsilon \rightarrow \left| \frac{x^3-y^3}{x^2-y^2}\right| \lt \delta \frac{x^3-y^3}{x^2-y^2} = \frac{(x^2+xy+y^2)(x-y)}{(x+y)(x-y)} = \frac{x^2+xy+y^2}{x+y} \left|\frac{x^2+xy+y^2}{x+y}\right| \leq \left|\frac{x(x+y)}{x+y}\right|+\frac{y^2}{\vert x+y\vert} = \vert x \vert + \frac{y^2}{\vert x+y\vert} \|(x,y)\| = \vert x\vert + \vert y\vert \|(x,y)\| \lt \epsilon \vert x\vert + \vert y\vert  = \vert x \vert + \frac{ y^2}{\vert y \vert} \geq \vert x \vert+\frac{y^2}{\vert y\vert+\vert x\vert} \vert x + y \vert \leq \vert x \vert + \vert y \vert","['real-analysis', 'limits', 'multivariable-calculus']"
17,Nested powers of $\sqrt 2$ has a solution different from its limit. What does this mean?,Nested powers of  has a solution different from its limit. What does this mean?,\sqrt 2,"The infinitely nested power expression below has a limit of $2$ : $$x=\sqrt2^{\sqrt2^{\sqrt2^{...}}}$$ In finding this limit, we may use: $$x=(\sqrt2)^x$$ But this expression has two solutions, $2$ and $4$ . We know that $2$ is the right answer by evaluating some finite truncations, but this $4$ is bothering me. What does $4$ mean in this expression? Is it significant in some way?","The infinitely nested power expression below has a limit of : In finding this limit, we may use: But this expression has two solutions, and . We know that is the right answer by evaluating some finite truncations, but this is bothering me. What does mean in this expression? Is it significant in some way?",2 x=\sqrt2^{\sqrt2^{\sqrt2^{...}}} x=(\sqrt2)^x 2 4 2 4 4,"['limits', 'power-towers']"
18,"If $\lim\limits_{x\to\alpha}\frac{x-2}{x^3-2x+m}=-\infty$, then what are the possible values for $\alpha$ and $m$?","If , then what are the possible values for  and ?",\lim\limits_{x\to\alpha}\frac{x-2}{x^3-2x+m}=-\infty \alpha m,"If $\lim\limits_{x\to\alpha}\dfrac{x-2}{x^3-2x+m}=-\infty$ , then what are the possible values for $\alpha$ and $m$ ? A student I'm tutoring came to me with this problem. I believe the limit is not one-sided, so that the expression approaches $-\infty$ as $x\to\alpha$ from either direction. I believe this would require $x=\alpha$ to be a zero of the denominator, so that $$x^3-2x+m=(x-\alpha)(x^2+\alpha x+\alpha^2-2)+\alpha^3-2\alpha+m$$ The remainder term should vanish, so $$\alpha^3-2\alpha+m=0$$ But in order for the limit to diverge to the ""same"" $-\infty$ from either side of $x=\alpha$ , I'm under the impression that $x=\alpha$ should actually be a zero of multiplicity $2$ . (I'm picturing the behavior of $-\dfrac1{x^2}$ around $x=0$ .) Then we'd have $$x^2+\alpha x+\alpha^2-2=(x-\alpha)(x+2\alpha)+3\alpha^2-2$$ Again the remainder should be $0$ , so that $$3\alpha^2-2=0\implies\alpha=\pm\sqrt{\frac23}\implies m=\pm\frac43\sqrt{\frac23}$$ When I check the limits for either pair of $(\alpha,m)$ , I find $$\lim_{x\to-\sqrt{\frac23}}\frac{x-2}{x^3-2x-\frac43\sqrt{\frac23}}=\color{red}+\infty$$ $$\lim_{x\to\sqrt{\frac23}}\frac{x-2}{x^3-2x+\frac43\sqrt{\frac23}}=-\infty$$ It seems that the sign of $\dfrac{x-2}{x+2\alpha}$ dictates whether the limit diverges to positive or negative infinity. This explanation seems a bit too hand-wavy and perhaps too verbose for a high-school-level calculus student. Is there a more straightforward or concise argument that can be made to show that $\alpha=\sqrt{\dfrac23}$ and $m=\dfrac43\sqrt{\dfrac23}$ is the answer?","If , then what are the possible values for and ? A student I'm tutoring came to me with this problem. I believe the limit is not one-sided, so that the expression approaches as from either direction. I believe this would require to be a zero of the denominator, so that The remainder term should vanish, so But in order for the limit to diverge to the ""same"" from either side of , I'm under the impression that should actually be a zero of multiplicity . (I'm picturing the behavior of around .) Then we'd have Again the remainder should be , so that When I check the limits for either pair of , I find It seems that the sign of dictates whether the limit diverges to positive or negative infinity. This explanation seems a bit too hand-wavy and perhaps too verbose for a high-school-level calculus student. Is there a more straightforward or concise argument that can be made to show that and is the answer?","\lim\limits_{x\to\alpha}\dfrac{x-2}{x^3-2x+m}=-\infty \alpha m -\infty x\to\alpha x=\alpha x^3-2x+m=(x-\alpha)(x^2+\alpha x+\alpha^2-2)+\alpha^3-2\alpha+m \alpha^3-2\alpha+m=0 -\infty x=\alpha x=\alpha 2 -\dfrac1{x^2} x=0 x^2+\alpha x+\alpha^2-2=(x-\alpha)(x+2\alpha)+3\alpha^2-2 0 3\alpha^2-2=0\implies\alpha=\pm\sqrt{\frac23}\implies m=\pm\frac43\sqrt{\frac23} (\alpha,m) \lim_{x\to-\sqrt{\frac23}}\frac{x-2}{x^3-2x-\frac43\sqrt{\frac23}}=\color{red}+\infty \lim_{x\to\sqrt{\frac23}}\frac{x-2}{x^3-2x+\frac43\sqrt{\frac23}}=-\infty \dfrac{x-2}{x+2\alpha} \alpha=\sqrt{\dfrac23} m=\dfrac43\sqrt{\dfrac23}","['calculus', 'limits', 'polynomials']"
19,"Limit, Riemann Sum, Integration, Natural logarithm","Limit, Riemann Sum, Integration, Natural logarithm",,"For any natural number $m$ , $\lim_{n\rightarrow \infty }\left ( \frac{1}{n+1}+\frac{1}{n+2}+\frac{1}{n+3}+\cdots +\frac{1}{mn} \right )=\ln (m)$ . I tried to prove the statement in the following way. Proof: $$\lim_{n\rightarrow \infty }\left ( \frac{1}{n+1}+\frac{1}{n+2}+\frac{1}{n+3}+\cdots +\frac{1}{mn} \right )=\lim_{n\rightarrow \infty }\sum_{r=1}^{(m-1)n}\frac{1}{n+r}$$ Dividing the numerator and the denominator of $\frac{1}{n+r}$ by $n$ , we get $\frac{1/n}{1+r/n}$ . Therefore, $$\lim_{n\rightarrow \infty }\sum_{r=1}^{(m-1)n}\frac{1}{n+r}=\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{r=1}^{(m-1)n}\frac{1}{1+r/n}$$ this is a Riemann sum, so replacing $\frac{1}{n}$ with $dx$ , $\frac{r}{n}$ with $x$ , and integrating between the limits $x=0$ and $x=m-1$ we get $ $$\int_{0}^{m-1}\frac{dx}{1+x}=\ln(1+x)|_{0}^{m-1}=\ln(m)-\ln(1)=\ln(m)\blacksquare$$ Is this a valid way?","For any natural number , . I tried to prove the statement in the following way. Proof: Dividing the numerator and the denominator of by , we get . Therefore, this is a Riemann sum, so replacing with , with , and integrating between the limits and we get $ Is this a valid way?",m \lim_{n\rightarrow \infty }\left ( \frac{1}{n+1}+\frac{1}{n+2}+\frac{1}{n+3}+\cdots +\frac{1}{mn} \right )=\ln (m) \lim_{n\rightarrow \infty }\left ( \frac{1}{n+1}+\frac{1}{n+2}+\frac{1}{n+3}+\cdots +\frac{1}{mn} \right )=\lim_{n\rightarrow \infty }\sum_{r=1}^{(m-1)n}\frac{1}{n+r} \frac{1}{n+r} n \frac{1/n}{1+r/n} \lim_{n\rightarrow \infty }\sum_{r=1}^{(m-1)n}\frac{1}{n+r}=\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{r=1}^{(m-1)n}\frac{1}{1+r/n} \frac{1}{n} dx \frac{r}{n} x x=0 x=m-1 \int_{0}^{m-1}\frac{dx}{1+x}=\ln(1+x)|_{0}^{m-1}=\ln(m)-\ln(1)=\ln(m)\blacksquare,"['integration', 'limits', 'logarithms', 'riemann-integration', 'riemann-sum']"
20,Prove that $ { \lim_{n\to\infty} \left( 1-\frac{1}{n} \right)^n = e^{-1} } $.,Prove that ., { \lim_{n\to\infty} \left( 1-\frac{1}{n} \right)^n = e^{-1} } ,"I want to prove that $$  { \lim_{n\to\infty} \left( 1-\frac{1}{n} \right)^n =   e^{-1} } $$ . I came up with a proof, but want to make sure that it is correct. Here is my proof: $$  { \lim_{n\to\infty} \left( 1-\frac{1}{n} \right)^n = \lim_{n\to\infty} \left(\frac{n-1}{n} \right)^n = \lim_{n\to\infty} \left(\frac{n}{n-1} \right)^{-n} = \\ \lim_{n\to\infty} \left((1+\frac{1}{n-1} )^n\right)^{-1} = \lim_{n-1\to\infty} \left((1+\frac{1}{n-1} )^{n-1}(1+\frac{1}{n-1})\right)^{-1} = \\ \lim_{n-1\to\infty} \left(e(1+\frac{1}{n-1})\right)^{-1} = e^{-1}}  $$ Is it valid proof? I know there are other proofs, but I want to know about this one. In particular can we substitude $e$ instead of $(1+\frac{1}{n-1} )^{n-1}$ and $1$ instead of $(1+\frac{1}{n-1})$ in the limit? Thank you for your answers.","I want to prove that . I came up with a proof, but want to make sure that it is correct. Here is my proof: Is it valid proof? I know there are other proofs, but I want to know about this one. In particular can we substitude instead of and instead of in the limit? Thank you for your answers.","  { \lim_{n\to\infty} \left( 1-\frac{1}{n} \right)^n = 
 e^{-1} }    { \lim_{n\to\infty} \left( 1-\frac{1}{n} \right)^n = \lim_{n\to\infty} \left(\frac{n-1}{n} \right)^n = \lim_{n\to\infty} \left(\frac{n}{n-1} \right)^{-n} = \\
\lim_{n\to\infty} \left((1+\frac{1}{n-1} )^n\right)^{-1} = \lim_{n-1\to\infty} \left((1+\frac{1}{n-1} )^{n-1}(1+\frac{1}{n-1})\right)^{-1} = \\
\lim_{n-1\to\infty} \left(e(1+\frac{1}{n-1})\right)^{-1} = e^{-1}}   e (1+\frac{1}{n-1} )^{n-1} 1 (1+\frac{1}{n-1})","['limits', 'proof-verification']"
21,How can a doubly improper integral become a singly improper integral after substitution?,How can a doubly improper integral become a singly improper integral after substitution?,,How can a doubly improper integral become a singly improper integral after substitution? If we take $x=\sin^2{t}$ then: $$\int_0^1\frac{1}{x\sqrt{1-x}}dx=2\int_0^{\pi/2}\csc{t}dt$$ On the lefthand side the integral is doubly improper on both the lower ( $0$ ) and upper ( $1$ ) bound. But after substitution the new integral is only improper at the lower bound ( $0$ ). How come? What happened to the improper upper bound?,How can a doubly improper integral become a singly improper integral after substitution? If we take then: On the lefthand side the integral is doubly improper on both the lower ( ) and upper ( ) bound. But after substitution the new integral is only improper at the lower bound ( ). How come? What happened to the improper upper bound?,x=\sin^2{t} \int_0^1\frac{1}{x\sqrt{1-x}}dx=2\int_0^{\pi/2}\csc{t}dt 0 1 0,"['calculus', 'limits', 'improper-integrals', 'substitution']"
22,"WolframAlpha and I don't agree on $( xy\sin y )/(3x^2+y^2)$ as $(x,y)\to(0,0)$",WolframAlpha and I don't agree on  as,"( xy\sin y )/(3x^2+y^2) (x,y)\to(0,0)","WolframAlpha claims that the following limit does not exist: $$ \lim_{(x,y)\to(0,0)} \frac{xy\sin{y}}{3x^2+y^2} $$ https://www.wolframalpha.com/input/?i=lim+(xysiny)%2F(3x%5E2%2By%5E2)+as+%7Bx-%3E0,+y-%3E0%7D However, I just proved that its limit is zero. Here is how: $2|(\sqrt{3} x)(y)| \leq 3x^2 + y^2 $ by AM-GM. Therefore, $ |xy \sin{y}| \leq \frac{1}{2\sqrt3}(3x^2+y^2)|\sin{y}|$ And so, $ 0 \leq |\frac{xy\sin{y}}{3x^2+y^2}| \leq \frac{1}{2\sqrt{3}}|\sin{y}|$ The one on the left and the right both go to zero as $(x,y)\to(0,0)$ ; therefore our expression in the middle must also converge to zero. Finally, because $|\frac{xy\sin{y}}{3x^2+y^2}|$ converges to zero, $\frac{xy\sin{y}}{3x^2+y^2}$ converges to zero as well. I am absolutely sure my proof is logical and correct, especially because WolframAlpha says '0' to this: https://www.wolframalpha.com/input/?i=lim+(xy%5E2)%2F(3x%5E2%2By%5E2)+as+%7Bx-%3E0,+y-%3E0%7D . However, my textbook also claims it to be nonexistent. What is the limit? If it really is zero, then why does WolframAlpha claim it to be nonexistent? If it isn't zero, where did I go wrong?","WolframAlpha claims that the following limit does not exist: https://www.wolframalpha.com/input/?i=lim+(xysiny)%2F(3x%5E2%2By%5E2)+as+%7Bx-%3E0,+y-%3E0%7D However, I just proved that its limit is zero. Here is how: by AM-GM. Therefore, And so, The one on the left and the right both go to zero as ; therefore our expression in the middle must also converge to zero. Finally, because converges to zero, converges to zero as well. I am absolutely sure my proof is logical and correct, especially because WolframAlpha says '0' to this: https://www.wolframalpha.com/input/?i=lim+(xy%5E2)%2F(3x%5E2%2By%5E2)+as+%7Bx-%3E0,+y-%3E0%7D . However, my textbook also claims it to be nonexistent. What is the limit? If it really is zero, then why does WolframAlpha claim it to be nonexistent? If it isn't zero, where did I go wrong?"," \lim_{(x,y)\to(0,0)} \frac{xy\sin{y}}{3x^2+y^2}  2|(\sqrt{3} x)(y)| \leq 3x^2 + y^2   |xy \sin{y}| \leq \frac{1}{2\sqrt3}(3x^2+y^2)|\sin{y}|  0 \leq |\frac{xy\sin{y}}{3x^2+y^2}| \leq \frac{1}{2\sqrt{3}}|\sin{y}| (x,y)\to(0,0) |\frac{xy\sin{y}}{3x^2+y^2}| \frac{xy\sin{y}}{3x^2+y^2}","['limits', 'multivariable-calculus', 'wolfram-alpha']"
23,"Limits, derivatives, and dividing by zero. [Contradiction in derivative defintions?]","Limits, derivatives, and dividing by zero. [Contradiction in derivative defintions?]",,"In the limit definition where the denominator is $x - a$ , and we take the limit as $x$ approaches $a$ , we assume that this denominator is not equal to zero. Where (besides the fact that it is necessary to avoid division by zero) does this assumption come from? Why is this not in direct contradiction with the definition of continuity, stating that when a function is continuous the limit as $x$ approaches $a$ will equal $f(a)$ so long as the range around $f(a)$ is not infinite. If $f(x) = x$ , and the limit as $x$ approaches $a$ equals $f(a)$ , why in the limit definitions are we allowed to make the assumption that the limit of term $x$ as $x$ approaches $a$ will not simply lead to $a - a$ , or even more simply the limit as $h$ approaches $0$ will not simply equal $0$ ? How is computing the limits of the $x$ or $h$ term in the derivative formulas different than computing a limit of a linear function?","In the limit definition where the denominator is , and we take the limit as approaches , we assume that this denominator is not equal to zero. Where (besides the fact that it is necessary to avoid division by zero) does this assumption come from? Why is this not in direct contradiction with the definition of continuity, stating that when a function is continuous the limit as approaches will equal so long as the range around is not infinite. If , and the limit as approaches equals , why in the limit definitions are we allowed to make the assumption that the limit of term as approaches will not simply lead to , or even more simply the limit as approaches will not simply equal ? How is computing the limits of the or term in the derivative formulas different than computing a limit of a linear function?",x - a x a x a f(a) f(a) f(x) = x x a f(a) x x a a - a h 0 0 x h,"['limits', 'derivatives', 'indeterminate-forms']"
24,Differentiability of $f(x)={x^2\sin{(\dfrac{1}{x})}}$ at $x=0$ [duplicate],Differentiability of  at  [duplicate],f(x)={x^2\sin{(\dfrac{1}{x})}} x=0,"This question already has answers here : Show that the function $g(x) = x^2 \sin(\frac{1}{x}) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$ (2 answers) Closed 5 years ago . I'm a bit confused here. When we take the derivative of  $$f(x)=\begin{cases}   x^2\sin{\biggl(\dfrac{1}{x}\biggr)}&x\neq0\\   0 &x=0\\ \end{cases} $$ It's derivative is not defined at $x=0$ as  $f(x)=2x\sin{\biggl(\dfrac{1}{x}\biggr)}-\cos{\biggl(\dfrac{1}{x}\biggr)}$. However, approaching this problem through the limit form of the derivative gives the value of the derivative at $x=0$: $$f'(x)=\lim_{h\to 0}\dfrac{f(x+h)-f(x)}{h}$$ $$\implies f'(0)=\lim_{h\to0}\dfrac{h^2\sin{\biggl(\dfrac{1}{x}\biggr)}-0}{h}=0$$ What I'm curious about here is why differentiating $f(x)$ first and then entering $x=0$ is failing in contrast to finding the derivative through limits?","This question already has answers here : Show that the function $g(x) = x^2 \sin(\frac{1}{x}) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$ (2 answers) Closed 5 years ago . I'm a bit confused here. When we take the derivative of  $$f(x)=\begin{cases}   x^2\sin{\biggl(\dfrac{1}{x}\biggr)}&x\neq0\\   0 &x=0\\ \end{cases} $$ It's derivative is not defined at $x=0$ as  $f(x)=2x\sin{\biggl(\dfrac{1}{x}\biggr)}-\cos{\biggl(\dfrac{1}{x}\biggr)}$. However, approaching this problem through the limit form of the derivative gives the value of the derivative at $x=0$: $$f'(x)=\lim_{h\to 0}\dfrac{f(x+h)-f(x)}{h}$$ $$\implies f'(0)=\lim_{h\to0}\dfrac{h^2\sin{\biggl(\dfrac{1}{x}\biggr)}-0}{h}=0$$ What I'm curious about here is why differentiating $f(x)$ first and then entering $x=0$ is failing in contrast to finding the derivative through limits?",,"['limits', 'derivatives']"
25,Find limit $x\rightarrow0$ of $f(x)=x^2\cdot\left({\sin{\frac 1 x}}\right)^2$,Find limit  of,x\rightarrow0 f(x)=x^2\cdot\left({\sin{\frac 1 x}}\right)^2,"I have following function: $$f(x)=x^2\cdot\left({\sin{\frac 1 x}}\right)^2$$ I want to find the limit of the function for $x\rightarrow0^\pm$. First I analyze $\frac 1 x$: $\frac {1}{x}\rightarrow +\infty$ for $x\rightarrow0^+$ but the $\sin$ of infinity does not exist. Then I use the comparison theorem (I don't know how it's called in English) and conclude that, because $$\left|{x^2\left({\sin{\frac 1 x}}\right)}^2 \right| \le \frac{1}{x^2}\rightarrow0^+$$ therefore the initial function tends to $0$. Is this reasoning correct? Are there better ways?","I have following function: $$f(x)=x^2\cdot\left({\sin{\frac 1 x}}\right)^2$$ I want to find the limit of the function for $x\rightarrow0^\pm$. First I analyze $\frac 1 x$: $\frac {1}{x}\rightarrow +\infty$ for $x\rightarrow0^+$ but the $\sin$ of infinity does not exist. Then I use the comparison theorem (I don't know how it's called in English) and conclude that, because $$\left|{x^2\left({\sin{\frac 1 x}}\right)}^2 \right| \le \frac{1}{x^2}\rightarrow0^+$$ therefore the initial function tends to $0$. Is this reasoning correct? Are there better ways?",,['limits']
26,Find the oblique asymptote of $\sqrt{x^2+3x}$ for $x\rightarrow-\infty$,Find the oblique asymptote of  for,\sqrt{x^2+3x} x\rightarrow-\infty,I want to find the asymptote oblique of the following function for $x\rightarrow\pm\infty$ $$f(x)=\sqrt{x^2+3x}=\sqrt{x^2\left(1+\frac{3x}{x^2}\right)}\sim\sqrt{x^2}=|x|$$ For $x\rightarrow+\infty$ we have: $$\frac{f(x)}{x}\sim\frac{|x|}{x}=\frac{x}{x}=1$$ which means that the function grows linearly. $$f(x)-mx=\sqrt{x^2+3x}-x=x\left(\sqrt{\frac{x^2}{x^2}+\frac{3x}{x^2}}-1\right)\sim x\left(\frac {1}{2}\cdot\frac{3}{x}\right)=\frac{3}{2}$$ The oblique asymptote is $y=x+\frac 3 2$ which is correct. For $x\rightarrow-\infty$ we have: $$\frac{f(x)}{x}=\frac{|x|}{x}=\frac{-x}{x}=-1$$ This means that $$f(x)-mx=\sqrt{x^2+3x}+x=x\left(\sqrt{\frac{x^2}{x^2}+\frac{3x}{x^2}}+1\right)=x\left(\sqrt{1+\frac{3}{x}}+1\right)\sim x\cdot2\rightarrow -\infty$$ Which is not what my textbook reports ($-\frac{3}{2}$). Any hints on what I did wrong to find the $q$ for $x\rightarrow-\infty$?,I want to find the asymptote oblique of the following function for $x\rightarrow\pm\infty$ $$f(x)=\sqrt{x^2+3x}=\sqrt{x^2\left(1+\frac{3x}{x^2}\right)}\sim\sqrt{x^2}=|x|$$ For $x\rightarrow+\infty$ we have: $$\frac{f(x)}{x}\sim\frac{|x|}{x}=\frac{x}{x}=1$$ which means that the function grows linearly. $$f(x)-mx=\sqrt{x^2+3x}-x=x\left(\sqrt{\frac{x^2}{x^2}+\frac{3x}{x^2}}-1\right)\sim x\left(\frac {1}{2}\cdot\frac{3}{x}\right)=\frac{3}{2}$$ The oblique asymptote is $y=x+\frac 3 2$ which is correct. For $x\rightarrow-\infty$ we have: $$\frac{f(x)}{x}=\frac{|x|}{x}=\frac{-x}{x}=-1$$ This means that $$f(x)-mx=\sqrt{x^2+3x}+x=x\left(\sqrt{\frac{x^2}{x^2}+\frac{3x}{x^2}}+1\right)=x\left(\sqrt{1+\frac{3}{x}}+1\right)\sim x\cdot2\rightarrow -\infty$$ Which is not what my textbook reports ($-\frac{3}{2}$). Any hints on what I did wrong to find the $q$ for $x\rightarrow-\infty$?,,"['limits', 'asymptotics']"
27,Finding $\lim_{x\to 0} \frac{1 + 1/x}{1 + 1/x^2}$ using definition of a limit,Finding  using definition of a limit,\lim_{x\to 0} \frac{1 + 1/x}{1 + 1/x^2},"I need to either compute $$\lim_{x\to 0} \dfrac{1 + 1/x}{1 + 1/x^2}$$ using definition of a limit or prove it doesn't exist. My attempt: Given $\epsilon > 0$, we wish to find $N$ such that $\forall x \geq N$, $$\left|\dfrac{1 + 1/x}{1 + 1/x^2} - 0\right| < \epsilon.$$ We have $$\left|\dfrac{1 + 1/x}{1 + 1/x^2} - 0\right| = \left|\dfrac{x^2 + x}{x^2 + 1}\right| \leq \dfrac{x^2 + x}{x} = x + 1 < \epsilon. $$ Thus, we can choose $N \geq \epsilon - 1$ and then whenever $x > N$, we have $\left|f(x)\right| < \epsilon \implies \lim_{x\to 0 } f(x) = 0$","I need to either compute $$\lim_{x\to 0} \dfrac{1 + 1/x}{1 + 1/x^2}$$ using definition of a limit or prove it doesn't exist. My attempt: Given $\epsilon > 0$, we wish to find $N$ such that $\forall x \geq N$, $$\left|\dfrac{1 + 1/x}{1 + 1/x^2} - 0\right| < \epsilon.$$ We have $$\left|\dfrac{1 + 1/x}{1 + 1/x^2} - 0\right| = \left|\dfrac{x^2 + x}{x^2 + 1}\right| \leq \dfrac{x^2 + x}{x} = x + 1 < \epsilon. $$ Thus, we can choose $N \geq \epsilon - 1$ and then whenever $x > N$, we have $\left|f(x)\right| < \epsilon \implies \lim_{x\to 0 } f(x) = 0$",,['real-analysis']
28,Does existence of $\lim_{x \to 0} f(x)$ imply $\lim_{x \to 0} x f'(x) = 0$?,Does existence of  imply ?,\lim_{x \to 0} f(x) \lim_{x \to 0} x f'(x) = 0,"Suppose we have a function $f : \mathbb{R}^+ \to \mathbb{R}$.  It seems intuitive to me that if $\lim_{x \to 0} f(x)$ exists, then $\lim_{x \to 0} x f'(x) = 0$.  I suspect that for real functions, there may be pathological counterexamples to this, but at least for analytic functions (where $0$ may be on the boundary of the analytic disk) then it should be true. In the analytic case, I can give an argument for this that would convince a typical physicist like myself.  $f(x)$ cannot have an essential singularity at $0$, because  $\lim_{x \to 0} f(x)$ would not exist.  So as $x \to 0$, it scales like $f(x) = c + O( x^\alpha )$ for some constant $c$ and $\alpha > 0$, since the limit exists.  Therefore, $x f'(x) = O( x^\alpha ) \to 0$. Two questions: Is this true, and if so, how generally valid is it? Is there a simpler proof that does not rely on scaling arguments?  This seems like the kind of problem that one would typically address with a fairly general theorem like L'Hospital's rule.  Another strategy is to write $xf'=(xf)'-f$, but this just shifts the burden of the problem to showing that $\lim_{x \to 0}[xf(x)]'=\lim_{x \to 0} f(x)$.  I am guessing that the solution is totally obvious and I am just not seeing it.","Suppose we have a function $f : \mathbb{R}^+ \to \mathbb{R}$.  It seems intuitive to me that if $\lim_{x \to 0} f(x)$ exists, then $\lim_{x \to 0} x f'(x) = 0$.  I suspect that for real functions, there may be pathological counterexamples to this, but at least for analytic functions (where $0$ may be on the boundary of the analytic disk) then it should be true. In the analytic case, I can give an argument for this that would convince a typical physicist like myself.  $f(x)$ cannot have an essential singularity at $0$, because  $\lim_{x \to 0} f(x)$ would not exist.  So as $x \to 0$, it scales like $f(x) = c + O( x^\alpha )$ for some constant $c$ and $\alpha > 0$, since the limit exists.  Therefore, $x f'(x) = O( x^\alpha ) \to 0$. Two questions: Is this true, and if so, how generally valid is it? Is there a simpler proof that does not rely on scaling arguments?  This seems like the kind of problem that one would typically address with a fairly general theorem like L'Hospital's rule.  Another strategy is to write $xf'=(xf)'-f$, but this just shifts the burden of the problem to showing that $\lim_{x \to 0}[xf(x)]'=\lim_{x \to 0} f(x)$.  I am guessing that the solution is totally obvious and I am just not seeing it.",,"['real-analysis', 'limits']"
29,Prove that a function is affine,Prove that a function is affine,,"Let $f,g:\mathbb R\to \mathbb R$ be functions such that    $$f(x+h)=f(x)+g(x)h+\alpha(x,h)$$ for all $x,h\in \mathbb R$, where   $|\alpha(x,h)|\le Ch^3$. Show that $f(x)=ax+b$ for some $a,b\in  \mathbb R$. I think the way I'm supposed to prove this is by showing that $f'(x)$ is identically constant. The derivative of $f$ at $x$ is $\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$. I can write the given condition as $$\left|\frac{f(x+h)-f(x)}{h}\right|\le |g(x)| + Ch^2,$$and as $h\to 0$, we get $|f'(x)|\le |g(x)|$. Am I on the right track? Any further hints?","Let $f,g:\mathbb R\to \mathbb R$ be functions such that    $$f(x+h)=f(x)+g(x)h+\alpha(x,h)$$ for all $x,h\in \mathbb R$, where   $|\alpha(x,h)|\le Ch^3$. Show that $f(x)=ax+b$ for some $a,b\in  \mathbb R$. I think the way I'm supposed to prove this is by showing that $f'(x)$ is identically constant. The derivative of $f$ at $x$ is $\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$. I can write the given condition as $$\left|\frac{f(x+h)-f(x)}{h}\right|\le |g(x)| + Ch^2,$$and as $h\to 0$, we get $|f'(x)|\le |g(x)|$. Am I on the right track? Any further hints?",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
30,Finding limit of multivariable function.,Finding limit of multivariable function.,,"I have a question that might be silly but I need to understand these kind of problems. So i have this limit:$$\lim_{(x,y)\to(0,0)} \frac{x^3 + y^3}{x^2 + y^2}$$ To solve it I am going to use poolar coordinates, so the limit would be like this: $$\lim_{r\to0} \frac{r^3\cos^3\theta + r^3\sin^3\theta}{r^2}=$$ $$\lim_{r\to0} {r(\cos^3\theta + \sin^3\theta)}$$ Now it is clear to me that r tends to $r\to0$ but, can I actually say something about $\cos^3\theta + \sin^3\theta?$ In case I could, then I would say that it is bounded, thus the limit would be 0. But it is not clear to me, could anyone help me with this?","I have a question that might be silly but I need to understand these kind of problems. So i have this limit:$$\lim_{(x,y)\to(0,0)} \frac{x^3 + y^3}{x^2 + y^2}$$ To solve it I am going to use poolar coordinates, so the limit would be like this: $$\lim_{r\to0} \frac{r^3\cos^3\theta + r^3\sin^3\theta}{r^2}=$$ $$\lim_{r\to0} {r(\cos^3\theta + \sin^3\theta)}$$ Now it is clear to me that r tends to $r\to0$ but, can I actually say something about $\cos^3\theta + \sin^3\theta?$ In case I could, then I would say that it is bounded, thus the limit would be 0. But it is not clear to me, could anyone help me with this?",,"['limits', 'multivariable-calculus', 'polar-coordinates']"
31,Summation $\sum_{n=1}^\infty \frac{1}{2^n+1}$,Summation,\sum_{n=1}^\infty \frac{1}{2^n+1},So I was was messing around with infinite series and I came across one that is deceptively similar to the familiar $\sum_{n=1}^\infty \frac{1}{2^n} = 1 $ Simply add $1$ to each denominator of each term of the series. This is $\sum_{n=1}^\infty \frac{1}{2^n+1} \approx 0.76449...$ I have used all my weaponry on trying to crack it and the best I've come up with is the equivalent series $\sum_{n=1}^\infty \frac{(-1)^n}{2^n-1} $ Could somebody help me retrieve some sort of compact value for this expression ( e.g. $\sum_{n=1}^\infty \frac{1}{n^2} $ can be rewritten as $\frac{\pi^2}{6}$) or is this pretty much impossible? Thanks!,So I was was messing around with infinite series and I came across one that is deceptively similar to the familiar $\sum_{n=1}^\infty \frac{1}{2^n} = 1 $ Simply add $1$ to each denominator of each term of the series. This is $\sum_{n=1}^\infty \frac{1}{2^n+1} \approx 0.76449...$ I have used all my weaponry on trying to crack it and the best I've come up with is the equivalent series $\sum_{n=1}^\infty \frac{(-1)^n}{2^n-1} $ Could somebody help me retrieve some sort of compact value for this expression ( e.g. $\sum_{n=1}^\infty \frac{1}{n^2} $ can be rewritten as $\frac{\pi^2}{6}$) or is this pretty much impossible? Thanks!,,"['limits', 'summation', 'exponential-function']"
32,Proof that $\lim_{x\to \infty} (1+(x/n))^n = e^x$ Using Monotone Convergence Theorem,Proof that  Using Monotone Convergence Theorem,\lim_{x\to \infty} (1+(x/n))^n = e^x,"Use monotone convergence theorem to prove that $e(x):=\lim_{n\to\infty}(1+x/n)^n$ exists for all real $x$.  Then show that $e(-x):=\lim_{n\to\infty}(1-x/n)^n=1/e(x)$. I'm struggling to use monotone convergence theorem with this; obviously $e(x)$ is monotone when $x>0$, but it's not when $-n<x<0$.  So now what? Further, what makes the second part so difficult is that we can't use any derivatives, logarithms, prior knowledge of $e^x$, etc.  We have to do this using just the Bernoulli inequality and finite geometric series, or the regular definition of convergence.  I'm just not making any progress and I don't understand how I'm supposed to do this. How can you show both of these things?","Use monotone convergence theorem to prove that $e(x):=\lim_{n\to\infty}(1+x/n)^n$ exists for all real $x$.  Then show that $e(-x):=\lim_{n\to\infty}(1-x/n)^n=1/e(x)$. I'm struggling to use monotone convergence theorem with this; obviously $e(x)$ is monotone when $x>0$, but it's not when $-n<x<0$.  So now what? Further, what makes the second part so difficult is that we can't use any derivatives, logarithms, prior knowledge of $e^x$, etc.  We have to do this using just the Bernoulli inequality and finite geometric series, or the regular definition of convergence.  I'm just not making any progress and I don't understand how I'm supposed to do this. How can you show both of these things?",,"['calculus', 'real-analysis', 'limits', 'functions', 'limits-without-lhopital']"
33,A quirky proof for a limit of an integral,A quirky proof for a limit of an integral,,"I am stuck on this result, which the professor wrote as ""trivial"", but I don't find a way out. I have the function $$f_{\alpha}(t) = \frac{1}{2\pi} \sum_{k = 1}^{+\infty} \frac{1}{k}\int_0^{\pi} (\alpha(p))^k \sin^{2k}(\epsilon(p) t)\ dp$$ and he told use that for $t\to +\infty$ we have: $$f_{\alpha}(t) = \frac{1}{2\pi} \sum_{k = 1}^{+\infty} \frac{1}{4^k k}\binom{2k}{k}\int_0^{\pi} (\alpha(p))^k\ dp$$ Now, it's all about the sine since it's the only term with a dependance on $t$. Yet I cannot find a way to send $$\sin^{2k}(\epsilon(p) t)$$ into $$\frac{1}{4^k}\binom{2k}{k}$$ Any help? Thank you so much. More Details $$\epsilon(p)$$ Is a positive, limited and continuous function. The ""true"" starting point was $$f_{\alpha}(t) = -\frac{1}{2\pi}\int_0^{\pi} \log\left(1 - \alpha(p)\sin^2(\epsilon(p)t)\right)\ dp$$ Then I thought I could have expanded the logarithm in series. Maybe I shouldn't had to...","I am stuck on this result, which the professor wrote as ""trivial"", but I don't find a way out. I have the function $$f_{\alpha}(t) = \frac{1}{2\pi} \sum_{k = 1}^{+\infty} \frac{1}{k}\int_0^{\pi} (\alpha(p))^k \sin^{2k}(\epsilon(p) t)\ dp$$ and he told use that for $t\to +\infty$ we have: $$f_{\alpha}(t) = \frac{1}{2\pi} \sum_{k = 1}^{+\infty} \frac{1}{4^k k}\binom{2k}{k}\int_0^{\pi} (\alpha(p))^k\ dp$$ Now, it's all about the sine since it's the only term with a dependance on $t$. Yet I cannot find a way to send $$\sin^{2k}(\epsilon(p) t)$$ into $$\frac{1}{4^k}\binom{2k}{k}$$ Any help? Thank you so much. More Details $$\epsilon(p)$$ Is a positive, limited and continuous function. The ""true"" starting point was $$f_{\alpha}(t) = -\frac{1}{2\pi}\int_0^{\pi} \log\left(1 - \alpha(p)\sin^2(\epsilon(p)t)\right)\ dp$$ Then I thought I could have expanded the logarithm in series. Maybe I shouldn't had to...",,"['integration', 'limits', 'trigonometry', 'asymptotics', 'power-series']"
34,Exponential and factorial that grow at exactly the same rate,Exponential and factorial that grow at exactly the same rate,,"I want to find a relation of the form $$ n^{n^a} = \Theta (n!) $$ I know and can reason fairly easily that $n^n$, where $a=1$, grows faster than $n!$, and $n^{\sqrt{n}}$, where $a=\frac{1}{2}$, grows more slowly than $n!$, so we can state the bound: $$\frac{1}{2} < a < 1$$  However, there appears to be no value in the bound, even in the limit as $a$ approaches $1$, that can make the exponential grow at exactly the same rate as the factorial.  Is there no way to satisfy the above expression (in which case, why?), or have I missed something fairly obvious?","I want to find a relation of the form $$ n^{n^a} = \Theta (n!) $$ I know and can reason fairly easily that $n^n$, where $a=1$, grows faster than $n!$, and $n^{\sqrt{n}}$, where $a=\frac{1}{2}$, grows more slowly than $n!$, so we can state the bound: $$\frac{1}{2} < a < 1$$  However, there appears to be no value in the bound, even in the limit as $a$ approaches $1$, that can make the exponential grow at exactly the same rate as the factorial.  Is there no way to satisfy the above expression (in which case, why?), or have I missed something fairly obvious?",,"['limits', 'factorial', 'computational-complexity']"
35,Compute; $\lim_{x\to\infty}\frac{(1-y^{x-1}\frac{x-1}{2})xy^{x-1}}{1-xy^{x-1}}$ with $0 \leq y <1$.,Compute;  with .,\lim_{x\to\infty}\frac{(1-y^{x-1}\frac{x-1}{2})xy^{x-1}}{1-xy^{x-1}} 0 \leq y <1,"Prove that $$\lim_{x\to\infty}\frac{(1-y^{x-2}\frac{x-1}{2})xy^{x-1}}{1-xy^{x-1}}=1$$ where $x \in \mathbb{N}$ ($x \neq 0$) and $0 \leq y <1$. I have managed to to multiply by $\frac{2}{2}$ to get $$\lim_{x\to\infty}\frac{[2-y^{x-1}(x-1)]xy^{x-1}}{2-2xy^{x-1}}$$ And then, through expanding and rearranging : $$\lim_{x\to\infty}\frac{(2-2xy^{x-1})xy^{x-1}+x^{2}y^{2x-2}+xy^{2x-2}}{2-2xy^{x-1}}$$ Which obviously results in : $$\lim_{x\to\infty}\bigg(\frac{xy^{x-1}}{2-2xy^{x-1}}+\frac{x^{2}y^{2x-2}+xy^{2x-2}}{2-2xy^{x-1}}\bigg)$$ However I don’t know how to go on from here.","Prove that $$\lim_{x\to\infty}\frac{(1-y^{x-2}\frac{x-1}{2})xy^{x-1}}{1-xy^{x-1}}=1$$ where $x \in \mathbb{N}$ ($x \neq 0$) and $0 \leq y <1$. I have managed to to multiply by $\frac{2}{2}$ to get $$\lim_{x\to\infty}\frac{[2-y^{x-1}(x-1)]xy^{x-1}}{2-2xy^{x-1}}$$ And then, through expanding and rearranging : $$\lim_{x\to\infty}\frac{(2-2xy^{x-1})xy^{x-1}+x^{2}y^{2x-2}+xy^{2x-2}}{2-2xy^{x-1}}$$ Which obviously results in : $$\lim_{x\to\infty}\bigg(\frac{xy^{x-1}}{2-2xy^{x-1}}+\frac{x^{2}y^{2x-2}+xy^{2x-2}}{2-2xy^{x-1}}\bigg)$$ However I don’t know how to go on from here.",,"['calculus', 'real-analysis', 'analysis', 'limits', 'multivariable-calculus']"
36,"Question on ""Spivak's"" Theorem","Question on ""Spivak's"" Theorem",,"I stumbled upon this Theorem here on pages 91 and 92 in the text. Check the complete Theorem there but it's basically this one. I am having trouble understanding two parts in the final step of the proof: $(1)$ Now, as $x \rightarrow a$ every point in the interval $[a,x]$ gets arbitrarily close to $x$ , so $$\lim_{x\to a}{c_x}=x$$ Wouldn't it be $\lim_{x\to a}{c_x}=a$ instead? $(2)$ Thus, $$f'_R(a)=\lim_{x\to a^{+}}\frac{f(x)-f(a)}{x-a}=\bbox[yellow]{\lim_{x\to a^{+}}f'(c_x)=\lim_{x\to a^{+}}f'(x)}=L$$ I'm not quite sure I follow that highlighted part, can someone enlighten me?","I stumbled upon this Theorem here on pages 91 and 92 in the text. Check the complete Theorem there but it's basically this one. I am having trouble understanding two parts in the final step of the proof: Now, as every point in the interval gets arbitrarily close to , so Wouldn't it be instead? Thus, I'm not quite sure I follow that highlighted part, can someone enlighten me?","(1) x \rightarrow a [a,x] x \lim_{x\to a}{c_x}=x \lim_{x\to a}{c_x}=a (2) f'_R(a)=\lim_{x\to a^{+}}\frac{f(x)-f(a)}{x-a}=\bbox[yellow]{\lim_{x\to a^{+}}f'(c_x)=\lim_{x\to a^{+}}f'(x)}=L",['real-analysis']
37,Show that $\lim_{x \to 1} [2x + 1] \ne 10$ using the epsilon delta definition of a limit not existing,Show that  using the epsilon delta definition of a limit not existing,\lim_{x \to 1} [2x + 1] \ne 10,"I started with: => $|2x + 1 - 10| \ge \epsilon$ => $|2x - 9| \ge \epsilon $ => $ |2(x - 1) - 7| \ge \epsilon $ $ 0 < |x - 1| < \delta $ must be satisfied. I'm not sure how to proceed after this. In the epsilon-delta proof of a limit we set $\delta$ to a value which satisfies $|f(x) - L| < \epsilon $ for all values of $\epsilon$. However I'm not sure if I should be setting a value of $\epsilon$ for the negation of a limit rather than a $\delta$, and if I do I'm not sure how to proceed with it to show that every $\delta$ value satisfies the $\epsilon$ value that I would have selected.","I started with: => $|2x + 1 - 10| \ge \epsilon$ => $|2x - 9| \ge \epsilon $ => $ |2(x - 1) - 7| \ge \epsilon $ $ 0 < |x - 1| < \delta $ must be satisfied. I'm not sure how to proceed after this. In the epsilon-delta proof of a limit we set $\delta$ to a value which satisfies $|f(x) - L| < \epsilon $ for all values of $\epsilon$. However I'm not sure if I should be setting a value of $\epsilon$ for the negation of a limit rather than a $\delta$, and if I do I'm not sure how to proceed with it to show that every $\delta$ value satisfies the $\epsilon$ value that I would have selected.",,"['calculus', 'limits', 'epsilon-delta']"
38,"Using the definition of $\lim_{x\to a} f(x) = \infty$ , prove that $\lim_{x\to 0} \frac{4x+\sqrt{5}}{2x^3+x^2} = \infty$","Using the definition of  , prove that",\lim_{x\to a} f(x) = \infty \lim_{x\to 0} \frac{4x+\sqrt{5}}{2x^3+x^2} = \infty,"This is my working so far: $$\lim_{x\to a} f(x) = \infty$$ if, $\forall K > 0, \, \exists \delta >0$ s.t. $|f(x)|>K$ when $0<|x-a|<\delta$ & $x\in \text{Dom}(f)$ Proof: Given $K>0$, find $\delta>0$ s.t. $|f(x)| > K$ when $0<|x-a|<\delta$ i.e. that, $$\left|\frac{4x+\sqrt{5}}{2x^3+x^2}\right| > K\quad \text{ when }|x|<\delta$$ My problem begins here in that I haven's a clue how to simplify $\left|\frac{4x+\sqrt{5}}{2x^3+x^2}\right| > K$ and get something in terms of $x$ or $|x|$ to then determine my $\delta$ accordingly. Thank you for taking the time and $\textbf{hints only please!}$","This is my working so far: $$\lim_{x\to a} f(x) = \infty$$ if, $\forall K > 0, \, \exists \delta >0$ s.t. $|f(x)|>K$ when $0<|x-a|<\delta$ & $x\in \text{Dom}(f)$ Proof: Given $K>0$, find $\delta>0$ s.t. $|f(x)| > K$ when $0<|x-a|<\delta$ i.e. that, $$\left|\frac{4x+\sqrt{5}}{2x^3+x^2}\right| > K\quad \text{ when }|x|<\delta$$ My problem begins here in that I haven's a clue how to simplify $\left|\frac{4x+\sqrt{5}}{2x^3+x^2}\right| > K$ and get something in terms of $x$ or $|x|$ to then determine my $\delta$ accordingly. Thank you for taking the time and $\textbf{hints only please!}$",,['limits']
39,Tangent at $0$ of $\sqrt{|x|}$,Tangent at  of,0 \sqrt{|x|},"This is a question regarding the solution to a question from Adams' book on Calculus. The question asks whether the function $f(x) = \sqrt{|x|}$ has a tangent line at $x = 0$. The answer is no. But when looking through the solution manual, Adams reasons like this. He writes: Since $\lim\limits_{h \to 0} \dfrac{\sqrt{|0+h|}-0}{h} = \lim\limits_{h \to 0} \dfrac{1}{|h|\operatorname{sgn}{(h)}}$ does not exist… My question is about this equality. The limit expressions are not equal, even close to $0$, for example at $h=\frac{1}{2}$. Is this a typo? Is this some property of limits I don't know about? My own calculations led me to the first limit  equalling $\frac{1}{\sqrt{|h|}}\text{sgn}{(h)}$ which also shows that the limit doesn't exist (right?). But it doesn't explain Adams' answer. And specifically, is there some situation where I can ""remove roots"" in this way? What if the limit did exist? Thank you in advance.","This is a question regarding the solution to a question from Adams' book on Calculus. The question asks whether the function $f(x) = \sqrt{|x|}$ has a tangent line at $x = 0$. The answer is no. But when looking through the solution manual, Adams reasons like this. He writes: Since $\lim\limits_{h \to 0} \dfrac{\sqrt{|0+h|}-0}{h} = \lim\limits_{h \to 0} \dfrac{1}{|h|\operatorname{sgn}{(h)}}$ does not exist… My question is about this equality. The limit expressions are not equal, even close to $0$, for example at $h=\frac{1}{2}$. Is this a typo? Is this some property of limits I don't know about? My own calculations led me to the first limit  equalling $\frac{1}{\sqrt{|h|}}\text{sgn}{(h)}$ which also shows that the limit doesn't exist (right?). But it doesn't explain Adams' answer. And specifically, is there some situation where I can ""remove roots"" in this way? What if the limit did exist? Thank you in advance.",,"['calculus', 'limits']"
40,Bounding or evaluating an integral limit,Bounding or evaluating an integral limit,,"I don't know if this integral was in the literature, then refers it please. This question is a curiosity after I did some experiments with Wolfram Alpha online calculator, and I hope that has mathematical meaning, that is, that I believe that it is well-defined. Consider the sequence of integrals with first term $$\int_1^{\infty}x^{-x}dx,$$  and second  $$\int_1^{\infty}\int_1^{\infty}x^{-y}y^{-x}dxdy.$$ And now the third term of  this sequence being a triple integral, with factors in this new integrand $x^{-y}, y^{-x}$ and $z^{-x}, z^{-y}$, and also $z^{-x}$ and $z^{-y}$. Thus I am saying this integral $$\int_1^{\infty}\int_1^{\infty}\int_1^{\infty}x^{-y-z}y^{-x-z}z^{-x-y}dxdydz.$$ And the next integral of our infinite sequence of integrals, with a similar pattern than previous. Question. Is it known what is the value of the limit integral? If you know the literature please refers it. If it isn't in the literature, can you then calculate an approximation of such limit integral? Or well a good upper bound. Many thanks.","I don't know if this integral was in the literature, then refers it please. This question is a curiosity after I did some experiments with Wolfram Alpha online calculator, and I hope that has mathematical meaning, that is, that I believe that it is well-defined. Consider the sequence of integrals with first term $$\int_1^{\infty}x^{-x}dx,$$  and second  $$\int_1^{\infty}\int_1^{\infty}x^{-y}y^{-x}dxdy.$$ And now the third term of  this sequence being a triple integral, with factors in this new integrand $x^{-y}, y^{-x}$ and $z^{-x}, z^{-y}$, and also $z^{-x}$ and $z^{-y}$. Thus I am saying this integral $$\int_1^{\infty}\int_1^{\infty}\int_1^{\infty}x^{-y-z}y^{-x-z}z^{-x-y}dxdydz.$$ And the next integral of our infinite sequence of integrals, with a similar pattern than previous. Question. Is it known what is the value of the limit integral? If you know the literature please refers it. If it isn't in the literature, can you then calculate an approximation of such limit integral? Or well a good upper bound. Many thanks.",,"['integration', 'limits']"
41,"Prove existence of $\lim_{(x,y) \rightarrow (0,2)} \frac{x (y-2)^3}{3x - 5(y-2)^4}$",Prove existence of,"\lim_{(x,y) \rightarrow (0,2)} \frac{x (y-2)^3}{3x - 5(y-2)^4}","I've been trying to prove the existence of this limit... trying with a bunch of different curves I'm always getting $0$ (I don't know if I'm using the suggestion correctly) but when I try to make an epsilon-delta proof I get stucked because I don't know how to properly bound the denominator. $$\lim_{(x,y) \rightarrow (0,2)} \frac{x (y-2)^3}{3x - 5(y-2)^4}$$ (Consider curves close to $3x-5(y-2)^4=0$ ) My question is: if indeed this limit does exist -and it's $0$ - how can I prove it? As I wrote above I don't know how to work around the denominator. I need something smaller than $|3x-5(y-2)^4|$ ... Or maybe the limit does not exist, and then my question would be what curve can you think of to prove it. Thank you for your time!","I've been trying to prove the existence of this limit... trying with a bunch of different curves I'm always getting (I don't know if I'm using the suggestion correctly) but when I try to make an epsilon-delta proof I get stucked because I don't know how to properly bound the denominator. (Consider curves close to ) My question is: if indeed this limit does exist -and it's - how can I prove it? As I wrote above I don't know how to work around the denominator. I need something smaller than ... Or maybe the limit does not exist, and then my question would be what curve can you think of to prove it. Thank you for your time!","0 \lim_{(x,y) \rightarrow (0,2)} \frac{x (y-2)^3}{3x - 5(y-2)^4} 3x-5(y-2)^4=0 0 |3x-5(y-2)^4|","['analysis', 'limits', 'multivariable-calculus', 'proof-writing', 'epsilon-delta']"
42,$\lim _{x\to \infty }\left(1+\sqrt{x}\sin \frac{1}{x}\right)^{\sqrt{x}}$,,\lim _{x\to \infty }\left(1+\sqrt{x}\sin \frac{1}{x}\right)^{\sqrt{x}},"How do I find the given limit? It seems that the limit without the $()^{\sqrt{x}}$ is 1, but I don't know how to solve it with the $()^{\sqrt{x}}$, i.e. $\lim _\limits{{x\to \infty }}\left(1+\sqrt{x}\sin \frac{1}{x}\right)^{\sqrt{x}}$.","How do I find the given limit? It seems that the limit without the $()^{\sqrt{x}}$ is 1, but I don't know how to solve it with the $()^{\sqrt{x}}$, i.e. $\lim _\limits{{x\to \infty }}\left(1+\sqrt{x}\sin \frac{1}{x}\right)^{\sqrt{x}}$.",,"['calculus', 'limits', 'limits-without-lhopital']"
43,How to show a power series is defined?,How to show a power series is defined?,,"One way I can think of is using radius of convergence. Since inside the interval of convergence, the series converges, i.e. not diverges to $\pm\infty$ For example I have $\cos x=\sum^{\infty}_{n=0}\frac{(-1)^n}{(2n)!}x^{2n}$, so $a_{2k}=\frac{(-1)^k}{(2k)!}$ when $n$ is even, then $R=\lim|\frac{a_{n}}{a_{n+1}}|=\lim (2n+2)(2n+1)=\infty$, so $\cos x$ is convergent for all real numbers and therefore defined everywhere. Is my logic correct?","One way I can think of is using radius of convergence. Since inside the interval of convergence, the series converges, i.e. not diverges to $\pm\infty$ For example I have $\cos x=\sum^{\infty}_{n=0}\frac{(-1)^n}{(2n)!}x^{2n}$, so $a_{2k}=\frac{(-1)^k}{(2k)!}$ when $n$ is even, then $R=\lim|\frac{a_{n}}{a_{n+1}}|=\lim (2n+2)(2n+1)=\infty$, so $\cos x$ is convergent for all real numbers and therefore defined everywhere. Is my logic correct?",,"['limits', 'convergence-divergence', 'power-series']"
44,"$\sqrt{1},\sqrt{2},\sqrt{3},\dotsc \sqrt{n}$. Calculate $\lim_{n\to\infty}\mu/\sigma$",. Calculate,"\sqrt{1},\sqrt{2},\sqrt{3},\dotsc \sqrt{n} \lim_{n\to\infty}\mu/\sigma","The question is: There is a collection of numbers: $\sqrt{1},\sqrt{2},\sqrt{3},\dotsc \sqrt{n}$.  $\mu$ is its arithmetic mean or average; $\sigma$ is its standard deviation. Calculate $$\lim_{n\to\infty}\frac{\mu}{\sigma} = \ ?$$ I think of the Squeeze Theorem, but I don't know how to do it, and maybe there is a more proper way to solve it. Can you help me, please?","The question is: There is a collection of numbers: $\sqrt{1},\sqrt{2},\sqrt{3},\dotsc \sqrt{n}$.  $\mu$ is its arithmetic mean or average; $\sigma$ is its standard deviation. Calculate $$\lim_{n\to\infty}\frac{\mu}{\sigma} = \ ?$$ I think of the Squeeze Theorem, but I don't know how to do it, and maybe there is a more proper way to solve it. Can you help me, please?",,"['limits', 'standard-deviation', 'means']"
45,How do we prove in general that $\lim_{t \to x} {t^a} = x^a$,How do we prove in general that,\lim_{t \to x} {t^a} = x^a,"How do we prove that for $a \in \mathbb{R}$,  $\lim_{t \to x} {t^a} = x^a$ in general using the epsilon/delta definition? My friend and I just spent an hour showing it for $t^{1/3}$, and the proof was very reliant on specific factoring and bounding a polynomial which I can see becoming highly nontrivial.","How do we prove that for $a \in \mathbb{R}$,  $\lim_{t \to x} {t^a} = x^a$ in general using the epsilon/delta definition? My friend and I just spent an hour showing it for $t^{1/3}$, and the proof was very reliant on specific factoring and bounding a polynomial which I can see becoming highly nontrivial.",,"['real-analysis', 'limits']"
46,Approaching to a number and limit,Approaching to a number and limit,,"Consider $f(x)$ function . We want to calculate $\lim_{x \to 3}f(x)$. So for left limit , we approach to $3$ and then compute $f(2.9) , f(2.99) , f(2.999)$ and so on . Now there is a weird thing . It is obvious that $2.9999.... = 3$ and also when we are talking about limit , point isn't important . In this case we don't take care about $f(3)$ but when we approach to $3$ infinitely , we get $3$ as $2.9999.... = 3$ ! . I'm very confused about these two concepts .","Consider $f(x)$ function . We want to calculate $\lim_{x \to 3}f(x)$. So for left limit , we approach to $3$ and then compute $f(2.9) , f(2.99) , f(2.999)$ and so on . Now there is a weird thing . It is obvious that $2.9999.... = 3$ and also when we are talking about limit , point isn't important . In this case we don't take care about $f(3)$ but when we approach to $3$ infinitely , we get $3$ as $2.9999.... = 3$ ! . I'm very confused about these two concepts .",,['limits']
47,When to use rationalization vs. substitution when finding limits?,When to use rationalization vs. substitution when finding limits?,,The function I'm looking at is this: $f(x)=\frac{\sqrt {x^2+2x+6}-3}{x-1}$ I'm not entirely sure when to use rationalization vs. taking values close to $x=a$ when finding limits. I realize I can make $x$ equal values close to $1.$ I can also rationalize the function to $f(x)=\frac{x-1}{\sqrt{x^2+2x+6}+3}$ by multiplying the denominator and numerator by the conjugate of the numerator and use $x=1$ exactly. When do I use each method? Thank you!,The function I'm looking at is this: $f(x)=\frac{\sqrt {x^2+2x+6}-3}{x-1}$ I'm not entirely sure when to use rationalization vs. taking values close to $x=a$ when finding limits. I realize I can make $x$ equal values close to $1.$ I can also rationalize the function to $f(x)=\frac{x-1}{\sqrt{x^2+2x+6}+3}$ by multiplying the denominator and numerator by the conjugate of the numerator and use $x=1$ exactly. When do I use each method? Thank you!,,"['calculus', 'real-analysis', 'limits', 'soft-question', 'rational-functions']"
48,Computing the integral $\int \limits_{1}^{\infty}\left(\frac{1}{\lfloor{x}\rfloor}-\frac{1}{x}\right)$ ....,Computing the integral  ....,\int \limits_{1}^{\infty}\left(\frac{1}{\lfloor{x}\rfloor}-\frac{1}{x}\right),"Prove that $$\large\int \limits_{1}^{\infty}\Bigg(\dfrac{1}{\lfloor{x}\rfloor}-\dfrac{1}{x}\Bigg)dx=\lim \limits_{n \to \infty} \Bigg(-\ln(n) + \sum \limits_{k=1}^n\dfrac{1}{k}\Bigg)$$ I was reading an article on Euler–Mascheroni constant$\Big(\gamma\approx0.577215664901532\Big)$, when I read that it is defined as the limiting difference between the harmonic series and the natural logarithm : $$\gamma=\lim \limits_{n \to \infty} \Bigg(-\ln(n) + \sum \limits_{k=1}^n\dfrac{1}{k}\Bigg)$$ That is completely fine, but in the next ""step"" this limit is equated to a definite integral of reciprocal of $x$ subtracted from the reciprocal of floor of $x$. It can not really understand this transition from limit to a definite integral. Also, I could not find even a single proof of this equivalence anywhere. A geometrical explanation (or even an algebraic one) will surely help me understand this. Thanks in Advance ! :-)","Prove that $$\large\int \limits_{1}^{\infty}\Bigg(\dfrac{1}{\lfloor{x}\rfloor}-\dfrac{1}{x}\Bigg)dx=\lim \limits_{n \to \infty} \Bigg(-\ln(n) + \sum \limits_{k=1}^n\dfrac{1}{k}\Bigg)$$ I was reading an article on Euler–Mascheroni constant$\Big(\gamma\approx0.577215664901532\Big)$, when I read that it is defined as the limiting difference between the harmonic series and the natural logarithm : $$\gamma=\lim \limits_{n \to \infty} \Bigg(-\ln(n) + \sum \limits_{k=1}^n\dfrac{1}{k}\Bigg)$$ That is completely fine, but in the next ""step"" this limit is equated to a definite integral of reciprocal of $x$ subtracted from the reciprocal of floor of $x$. It can not really understand this transition from limit to a definite integral. Also, I could not find even a single proof of this equivalence anywhere. A geometrical explanation (or even an algebraic one) will surely help me understand this. Thanks in Advance ! :-)",,"['limits', 'definite-integrals']"
49,"If $f(s) = (1+s)^{(1+s)^{(1+s)/s}/s}$, show that $\lim_{s \to \infty} f(s)/s = 1$.","If , show that .",f(s) = (1+s)^{(1+s)^{(1+s)/s}/s} \lim_{s \to \infty} f(s)/s = 1,"If $f(s) = (1+s)^{(1+s)^{(1+s)/s}/s}$, show that $\lim_{s \to \infty} f(s)/s = 1$. This function comes up in the parameterization of the solutions to $x^y = y^x$. See for example, here: Are there real solutions to $x^y = y^x = 3$ where $y \neq x$? If we write $y = rx$, we get $x = r^{1/(r-1)}$ and $y=r^{r/(r-1)}$. Then $x^y =(r^{1/(r-1)})^{r^{r/(r-1)}} =r^{r^{r/(r-1)}/(r-1)} $. Finally, if we write $r = 1+s$, this is $f(s) = (1+s)^{(1+s)^{1+1/s}/s} $. Wolfy says that, around $s=0$, $$f(s) =e^e +  \dfrac{e^{1 + e} s^2}{24}  - \dfrac{e^{1 + e} s^3}{24}  + \dfrac{e^{1 + e} (219 + 5 e) s^4}{5760} + O(s^5) $$ and, around $s = \infty$, $$f(s) =s + (\log^2(1/s) - \log(1/s) + 1)  + \dfrac{\log^4(1/s) - 3 \log^3(1/s) + 5 \log^2(1/s) - 6 \log(1/s) + 2}{2 s} + O((1/s)^2), $$ calling this a generalized Puiseux series. My question is: How to show that that expansion at $s = \infty$ is correct. I would be satisfied with a proof that, as the title says, $\lim_{s \to \infty} \dfrac{f(s)}{s} = 1 $. It might help to note that $\dfrac{y}{x} = r$ and $\dfrac{r}{s} = \dfrac{1+s}{s} = 1+\dfrac1{s} $.","If $f(s) = (1+s)^{(1+s)^{(1+s)/s}/s}$, show that $\lim_{s \to \infty} f(s)/s = 1$. This function comes up in the parameterization of the solutions to $x^y = y^x$. See for example, here: Are there real solutions to $x^y = y^x = 3$ where $y \neq x$? If we write $y = rx$, we get $x = r^{1/(r-1)}$ and $y=r^{r/(r-1)}$. Then $x^y =(r^{1/(r-1)})^{r^{r/(r-1)}} =r^{r^{r/(r-1)}/(r-1)} $. Finally, if we write $r = 1+s$, this is $f(s) = (1+s)^{(1+s)^{1+1/s}/s} $. Wolfy says that, around $s=0$, $$f(s) =e^e +  \dfrac{e^{1 + e} s^2}{24}  - \dfrac{e^{1 + e} s^3}{24}  + \dfrac{e^{1 + e} (219 + 5 e) s^4}{5760} + O(s^5) $$ and, around $s = \infty$, $$f(s) =s + (\log^2(1/s) - \log(1/s) + 1)  + \dfrac{\log^4(1/s) - 3 \log^3(1/s) + 5 \log^2(1/s) - 6 \log(1/s) + 2}{2 s} + O((1/s)^2), $$ calling this a generalized Puiseux series. My question is: How to show that that expansion at $s = \infty$ is correct. I would be satisfied with a proof that, as the title says, $\lim_{s \to \infty} \dfrac{f(s)}{s} = 1 $. It might help to note that $\dfrac{y}{x} = r$ and $\dfrac{r}{s} = \dfrac{1+s}{s} = 1+\dfrac1{s} $.",,['limits']
50,finding limit with $\cos$ function occur $n$ times,finding limit with  function occur  times,\cos n,"Finding $\displaystyle \lim_{x\rightarrow 0}\frac{1-\cos(1-\cos(1-\cos(1-\cdots \cdots (1-\cos x))))}{x^{2^n}}$ where number of $\cos$ is $n$ times when $x\rightarrow 0$ then $\displaystyle 1-\cos x = 2\sin^2 \frac{x}{2} \rightarrow 2\frac{x}{2} = x$ so $1-\cos (1-\cos x) = 1-\cos x$ some help me., thanks","Finding $\displaystyle \lim_{x\rightarrow 0}\frac{1-\cos(1-\cos(1-\cos(1-\cdots \cdots (1-\cos x))))}{x^{2^n}}$ where number of $\cos$ is $n$ times when $x\rightarrow 0$ then $\displaystyle 1-\cos x = 2\sin^2 \frac{x}{2} \rightarrow 2\frac{x}{2} = x$ so $1-\cos (1-\cos x) = 1-\cos x$ some help me., thanks",,['limits']
51,$\lim _{x\to 0}\left(\frac{\tan\left(x\right)-x}{x-\sin\left(x\right)}\right)$ without L'Hopital's Rule,without L'Hopital's Rule,\lim _{x\to 0}\left(\frac{\tan\left(x\right)-x}{x-\sin\left(x\right)}\right),"Here are the functions: a) $\displaystyle\lim _{x\to 0}\left(\frac{\tan\left(x\right)-x}{x-\sin\left(x\right)}\right)$ If I used L'Hopital's rule the limit is $2$ b) $\displaystyle\lim _{x\to 0}\:\frac{e^x\cdot \:\sin\left(x\right)-x\cdot \left(1+x\right)}{x^3}$ here $\dfrac{1}{3}$ c) $\displaystyle\lim _{x\to 0}\left(\frac{\ln\left(\sin\left(3 x\right)\right)}{\ln\left(\sin\left(7x\right)\right)}\right)$ and here $1$ but the problem is that I am not allowed to use L'Hopital's rule, can you give me ideas for another type of approaches? UPDATE: I apologize, I see there is some discussion and confusion among people, which obviously goes beyond my functions, but still I wanted to explain that I have been missing a lots of lectures recently due to illness and last week I got $0$ points for using L'hopital because we have not learnt it, so my guess was that we are not allowed this time either, but  I just talked to my tutor and he told me that just in the last lecture, they introduced L'hopital rule to us so I am free to use it. I'm very sorry.","Here are the functions: a) $\displaystyle\lim _{x\to 0}\left(\frac{\tan\left(x\right)-x}{x-\sin\left(x\right)}\right)$ If I used L'Hopital's rule the limit is $2$ b) $\displaystyle\lim _{x\to 0}\:\frac{e^x\cdot \:\sin\left(x\right)-x\cdot \left(1+x\right)}{x^3}$ here $\dfrac{1}{3}$ c) $\displaystyle\lim _{x\to 0}\left(\frac{\ln\left(\sin\left(3 x\right)\right)}{\ln\left(\sin\left(7x\right)\right)}\right)$ and here $1$ but the problem is that I am not allowed to use L'Hopital's rule, can you give me ideas for another type of approaches? UPDATE: I apologize, I see there is some discussion and confusion among people, which obviously goes beyond my functions, but still I wanted to explain that I have been missing a lots of lectures recently due to illness and last week I got $0$ points for using L'hopital because we have not learnt it, so my guess was that we are not allowed this time either, but  I just talked to my tutor and he told me that just in the last lecture, they introduced L'hopital rule to us so I am free to use it. I'm very sorry.",,"['limits', 'limits-without-lhopital']"
52,Evaluation of $\lim_{n\rightarrow \infty}\frac{n!\cdot e^n}{\sqrt{n}n^n}$ without stirling approximation,Evaluation of  without stirling approximation,\lim_{n\rightarrow \infty}\frac{n!\cdot e^n}{\sqrt{n}n^n},"Evaluation of $\displaystyle \lim_{n\rightarrow \infty}\frac{n!\cdot e^n}{\sqrt{n}\cdot n^n}$ $\bf{My\; Try::}$we can write it as $$l=\lim_{n\rightarrow \infty}\frac{e^n}{\sqrt{n}}\cdot \left(\frac{1}{n}\cdot \frac{2}{n}\cdot \frac{3}{n}\cdots \cdots \frac{n}{n}\right)$$ $$\ln (l) = \lim_{n\rightarrow \infty}\bigg[n-\frac{1}{2}n+\sum^{n}_{r=1}\ln\left(\frac{r}{n}\right)\bigg]$$ Now how can i solve it, Help required, Thanks","Evaluation of $\displaystyle \lim_{n\rightarrow \infty}\frac{n!\cdot e^n}{\sqrt{n}\cdot n^n}$ $\bf{My\; Try::}$we can write it as $$l=\lim_{n\rightarrow \infty}\frac{e^n}{\sqrt{n}}\cdot \left(\frac{1}{n}\cdot \frac{2}{n}\cdot \frac{3}{n}\cdots \cdots \frac{n}{n}\right)$$ $$\ln (l) = \lim_{n\rightarrow \infty}\bigg[n-\frac{1}{2}n+\sum^{n}_{r=1}\ln\left(\frac{r}{n}\right)\bigg]$$ Now how can i solve it, Help required, Thanks",,['limits']
53,A limit of indeterminate,A limit of indeterminate,,"$$\lim _{x\to \infty }\frac{\left(e^x+x\right)^{n+1}-e^{\left(n+1\right)x}}{xe^{nx}}\:= \:\:?$$ I tried to get it to a simpler form like this: $$\lim _{x\to \infty }\frac{\left(e^x+x\right)^{n+1}-e^{\left(n+1\right)x}}{xe^{nx}}\:=\:\lim _{x\to \infty }\frac{e^{\left(n+1\right)x}\left(1+\frac{x}{e^x}\right)^{n+1}-e^{\left(n+1\right)x}}{xe^{nx}}=\lim _{x\to \infty }\frac{e^x}{x}\left(\left(1+\frac{x}{e^x}\right)^{n+1}-1\right)$$ Then i noted $\frac{e^x}{x}$ with t which tends to infinity also. Then i applyed the formula for $a^n-b^n$ (I've considered $1$ as $1^\left(n+1\right)$) and i got 1, but the answer is $n+1$. What have i missed ?","$$\lim _{x\to \infty }\frac{\left(e^x+x\right)^{n+1}-e^{\left(n+1\right)x}}{xe^{nx}}\:= \:\:?$$ I tried to get it to a simpler form like this: $$\lim _{x\to \infty }\frac{\left(e^x+x\right)^{n+1}-e^{\left(n+1\right)x}}{xe^{nx}}\:=\:\lim _{x\to \infty }\frac{e^{\left(n+1\right)x}\left(1+\frac{x}{e^x}\right)^{n+1}-e^{\left(n+1\right)x}}{xe^{nx}}=\lim _{x\to \infty }\frac{e^x}{x}\left(\left(1+\frac{x}{e^x}\right)^{n+1}-1\right)$$ Then i noted $\frac{e^x}{x}$ with t which tends to infinity also. Then i applyed the formula for $a^n-b^n$ (I've considered $1$ as $1^\left(n+1\right)$) and i got 1, but the answer is $n+1$. What have i missed ?",,['limits']
54,clarification on showing $\lim_{x\to 0} \frac{1}{x}$ does not exist.,clarification on showing  does not exist.,\lim_{x\to 0} \frac{1}{x},"So this has to do with the classic problem from here: Proof that the limit of $\frac{1}{x}$ as $x$ approaches $0$ does not exist I only had a question of how to properly come up with the needed contradiction. So if I am proving this via contradiction I would be assuming that $\left|\frac{1}{x} - l\right| < \epsilon$,  my issue is, how do I arrive at the necessary $x$ because to obtain that $$\left|\frac{1}{x}\right| > \epsilon + |l|$$ I would need to use the condition $$\left|\frac{1}{x} - l\right| > \epsilon$$ Doing the manipulations with my current assumption is not leading me to the right answer. What would I have to do to arrive at the contradiction?","So this has to do with the classic problem from here: Proof that the limit of $\frac{1}{x}$ as $x$ approaches $0$ does not exist I only had a question of how to properly come up with the needed contradiction. So if I am proving this via contradiction I would be assuming that $\left|\frac{1}{x} - l\right| < \epsilon$,  my issue is, how do I arrive at the necessary $x$ because to obtain that $$\left|\frac{1}{x}\right| > \epsilon + |l|$$ I would need to use the condition $$\left|\frac{1}{x} - l\right| > \epsilon$$ Doing the manipulations with my current assumption is not leading me to the right answer. What would I have to do to arrive at the contradiction?",,"['calculus', 'real-analysis', 'limits']"
55,How to find f'(0) when a certain limit and condition are given.,How to find f'(0) when a certain limit and condition are given.,,I have this Calculus 1 question as a homework and I was wondering how it is possible to prove that f'(0)=0 as the solution says. I just can't seem to understand how to deal with this problem. Thank you everyone!,I have this Calculus 1 question as a homework and I was wondering how it is possible to prove that f'(0)=0 as the solution says. I just can't seem to understand how to deal with this problem. Thank you everyone!,,"['calculus', 'limits', 'derivatives']"
56,$a_{n+1}=a_n+\frac{2a_{n-1}}{n+1}$ implies $a_n/n^2$ converges? [duplicate],implies  converges? [duplicate],a_{n+1}=a_n+\frac{2a_{n-1}}{n+1} a_n/n^2,"This question already has an answer here : How to calculate the limit of $\frac{a_n}{n^2}$ for the sequence $a_{n+1}=a_n+\frac{2 a_{n-1}}{n+1}$? (1 answer) Closed 7 years ago . Let $a_0=\pi$, $a_1=\pi^2$, $a_{n+1}=a_n+\frac{2a_{n-1}}{n+1}$ How can we prove that $a_n/n^2$ converges? It is easy to see that $$a_n-a_1=\sum_{k=1}^{n-1}\frac{2a_{k-1}}{k+1}\geq 2a_0\sum_{k=1}^{n-1}\frac{1}{k+1}\to\infty.$$ Then how to do?","This question already has an answer here : How to calculate the limit of $\frac{a_n}{n^2}$ for the sequence $a_{n+1}=a_n+\frac{2 a_{n-1}}{n+1}$? (1 answer) Closed 7 years ago . Let $a_0=\pi$, $a_1=\pi^2$, $a_{n+1}=a_n+\frac{2a_{n-1}}{n+1}$ How can we prove that $a_n/n^2$ converges? It is easy to see that $$a_n-a_1=\sum_{k=1}^{n-1}\frac{2a_{k-1}}{k+1}\geq 2a_0\sum_{k=1}^{n-1}\frac{1}{k+1}\to\infty.$$ Then how to do?",,['limits']
57,Limit involving $(1+x)^x$ term,Limit involving  term,(1+x)^x,"I don't know how to solve the following limit without using series expansion. $$\lim_{x\to 0} \frac{(1+x)^x -1 -x^2}{x^3} $$ I have tried use L'Hopital's rule and finding bounds to use squeeze theorem but to no avail. Please give me hints on how to compute the limit, instead of posting full answers. Thanks in advance :) Edit: I have just found an answer, though it is not elegant at all. It is done simply by applying L'Hopital's rule three times and there is not much to say about it. Still, other answers are welcome.","I don't know how to solve the following limit without using series expansion. $$\lim_{x\to 0} \frac{(1+x)^x -1 -x^2}{x^3} $$ I have tried use L'Hopital's rule and finding bounds to use squeeze theorem but to no avail. Please give me hints on how to compute the limit, instead of posting full answers. Thanks in advance :) Edit: I have just found an answer, though it is not elegant at all. It is done simply by applying L'Hopital's rule three times and there is not much to say about it. Still, other answers are welcome.",,"['calculus', 'real-analysis', 'limits']"
58,Proving the limit of a given cubic function from the epsilon-delta definition of a limit,Proving the limit of a given cubic function from the epsilon-delta definition of a limit,,"Would anyone know how to prove the limit of this cubic equation using the epsilon delta definition? $\lim_{x \rightarrow 2} x^3 +2x^2 -x -1 = 13  $ I really don't know where to start other than inputting the values of $a$, $L$ and $f(x)$ for this example into the definition of a limit: $0 < |x - 2| < d $ implies $|(x^3 +2x^2 -x -1) - 13| < \epsilon$","Would anyone know how to prove the limit of this cubic equation using the epsilon delta definition? $\lim_{x \rightarrow 2} x^3 +2x^2 -x -1 = 13  $ I really don't know where to start other than inputting the values of $a$, $L$ and $f(x)$ for this example into the definition of a limit: $0 < |x - 2| < d $ implies $|(x^3 +2x^2 -x -1) - 13| < \epsilon$",,"['calculus', 'limits', 'polynomials', 'epsilon-delta']"
59,Ratio of Modified Bessel Function of First Kind with its derivative (of positive order),Ratio of Modified Bessel Function of First Kind with its derivative (of positive order),,"I am trying to find the limit: $lim_{x \rightarrow 0} x\frac{I_{\nu}'(x)}{I_{\nu}(x)}$ for real order $\nu > 0$. Wolfram Alpha gives the result of $\nu$ for real order $\nu > 0$, but I can't see why. The recurrence relations regarding derivatives $2 I'_{\nu}(x) = I_{\nu-1}(x) + I_{\nu+1}(x)$ doesn't seem the right thing to use. Any suggestions?","I am trying to find the limit: $lim_{x \rightarrow 0} x\frac{I_{\nu}'(x)}{I_{\nu}(x)}$ for real order $\nu > 0$. Wolfram Alpha gives the result of $\nu$ for real order $\nu > 0$, but I can't see why. The recurrence relations regarding derivatives $2 I'_{\nu}(x) = I_{\nu-1}(x) + I_{\nu+1}(x)$ doesn't seem the right thing to use. Any suggestions?",,"['limits', 'bessel-functions']"
60,Proof that lim sup of union equals union of lim sup,Proof that lim sup of union equals union of lim sup,,"My home work is: let $ A_{n},B_{n}$ be subsets of the sample space. Prove that $$ \limsup_{n\to\infty} (A_{n}\cup B_{n}) = \limsup_{n\to\infty} A_{n}\cup\limsup_{n\to\infty} B_{n} $$ I managed to get to this: $$ \bigcap_{1}^{n}\bigcup_{n\geq m}^{ } A_{m}\cup \bigcap_{1}^{n}\bigcup_{n\geq m}^{ } B_{m}\ = \bigcap_{1}^{n}\bigcup_{n\geq m}^{ } A_{m}\cup B_{m} $$ Really appreciate if anyone can help me with this","My home work is: let $ A_{n},B_{n}$ be subsets of the sample space. Prove that $$ \limsup_{n\to\infty} (A_{n}\cup B_{n}) = \limsup_{n\to\infty} A_{n}\cup\limsup_{n\to\infty} B_{n} $$ I managed to get to this: $$ \bigcap_{1}^{n}\bigcup_{n\geq m}^{ } A_{m}\cup \bigcap_{1}^{n}\bigcup_{n\geq m}^{ } B_{m}\ = \bigcap_{1}^{n}\bigcup_{n\geq m}^{ } A_{m}\cup B_{m} $$ Really appreciate if anyone can help me with this",,"['limits', 'limsup-and-liminf']"
61,Derivation of Dirac delta function,Derivation of Dirac delta function,,Is there anyone could give me a hint how to find the distributional derivative of the delta function $\delta$? I don't know how to deal with the infinite point.,Is there anyone could give me a hint how to find the distributional derivative of the delta function $\delta$? I don't know how to deal with the infinite point.,,"['real-analysis', 'limits']"
62,Finding $\lim_{x \to 0}\frac{\sqrt[3]{1+2x}-1}{x}$,Finding,\lim_{x \to 0}\frac{\sqrt[3]{1+2x}-1}{x},"Good morning everyone, first time posting here. For my calculus class, we are asked to find  $$\lim_{x \to 0}\frac{\sqrt[3]{1+2x}-1}{x}$$ We are given the hint that $$a^3-b^3=(a-b)(a^2+ab+b^2)$$ I have tried several step by step online solvers, but they all use derivatives and something called the 'hospital' rule, which we haven't learned yet. So in conclusion, if someone could set me on a path to solving this that uses the hint I included, I'd be very grateful.","Good morning everyone, first time posting here. For my calculus class, we are asked to find  $$\lim_{x \to 0}\frac{\sqrt[3]{1+2x}-1}{x}$$ We are given the hint that $$a^3-b^3=(a-b)(a^2+ab+b^2)$$ I have tried several step by step online solvers, but they all use derivatives and something called the 'hospital' rule, which we haven't learned yet. So in conclusion, if someone could set me on a path to solving this that uses the hint I included, I'd be very grateful.",,"['calculus', 'limits']"
63,For a positive real sequence $(a_n)$,For a positive real sequence,(a_n),"For a positive real sequence $(a_n)$ such that $$\lim_{n\to \infty} \frac {a_1+a_2+\cdots+a_n}{n}=L$$ and $$\lim_{n\to \infty} \frac {a_n}{n}=0$$ Then show that $$\lim_{n\to \infty} \frac {a_1^2+a_2^2+\cdots+a_n^2}{n^2}=0$$ I have tried to create an inequality using Cauchy-Schwarz and using the Squeeze Theorem, but it didn't work, and I have thought of using Stolz-Cesàro's Theorem, but it also didn't work. Can someone help me?","For a positive real sequence $(a_n)$ such that $$\lim_{n\to \infty} \frac {a_1+a_2+\cdots+a_n}{n}=L$$ and $$\lim_{n\to \infty} \frac {a_n}{n}=0$$ Then show that $$\lim_{n\to \infty} \frac {a_1^2+a_2^2+\cdots+a_n^2}{n^2}=0$$ I have tried to create an inequality using Cauchy-Schwarz and using the Squeeze Theorem, but it didn't work, and I have thought of using Stolz-Cesàro's Theorem, but it also didn't work. Can someone help me?",,"['real-analysis', 'limits']"
64,Problem 560 of Demidovich's book of problems.,Problem 560 of Demidovich's book of problems.,,"Need a solution for a problem 560 from Demidovich's book of problems. $$ \lim_{x\rightarrow a}\frac{a^{a^x}-a^{x^a}}{a^x-x^a} $$ I know an answer: $$ a^{a^a}\ln(a); $$ I had tired some replacements and some decomposition, but it's not worked. P.S. Level of problem mean a solution without l'Hospital.","Need a solution for a problem 560 from Demidovich's book of problems. $$ \lim_{x\rightarrow a}\frac{a^{a^x}-a^{x^a}}{a^x-x^a} $$ I know an answer: $$ a^{a^a}\ln(a); $$ I had tired some replacements and some decomposition, but it's not worked. P.S. Level of problem mean a solution without l'Hospital.",,"['calculus', 'limits']"
65,Hint for $\lim_{n\rightarrow\infty} \sqrt[n]{\prod_{i=1}^n\frac{1}{\cos\frac{1}{i}}}$.,Hint for .,\lim_{n\rightarrow\infty} \sqrt[n]{\prod_{i=1}^n\frac{1}{\cos\frac{1}{i}}},How to calculate the following limit: $$\lim_{n\rightarrow\infty}\sqrt[n]{\prod_{i=1}^n\frac{1}{\cos\frac{1}{i}}}$$ thanks.,How to calculate the following limit: $$\lim_{n\rightarrow\infty}\sqrt[n]{\prod_{i=1}^n\frac{1}{\cos\frac{1}{i}}}$$ thanks.,,"['calculus', 'limits']"
66,What is the value of $\lim _{x\to 0}\left\lfloor\frac{\tan x \sin x}{x^2}\right\rfloor$,What is the value of,\lim _{x\to 0}\left\lfloor\frac{\tan x \sin x}{x^2}\right\rfloor,How do I evaluate the limit $$\lim _{x\to 0}\left\lfloor\frac{\tan x \sin x}{x^2}\right\rfloor$$  where $\lfloor\cdot\rfloor$ denotes greatest integer function. I know that $x>\sin x$ and $x < \tan x$ but how do I use these results here? $\tan x \over x$ tends to $1+$ whereas $\sin x \over x$ tends to $1-$,How do I evaluate the limit $$\lim _{x\to 0}\left\lfloor\frac{\tan x \sin x}{x^2}\right\rfloor$$  where $\lfloor\cdot\rfloor$ denotes greatest integer function. I know that $x>\sin x$ and $x < \tan x$ but how do I use these results here? $\tan x \over x$ tends to $1+$ whereas $\sin x \over x$ tends to $1-$,,"['calculus', 'limits', 'trigonometry', 'ceiling-and-floor-functions']"
67,What is the serie convergence of $ \sum_{n=1}^\infty \frac{(3n)!}{n^{3n}} $,What is the serie convergence of, \sum_{n=1}^\infty \frac{(3n)!}{n^{3n}} ,I am stucked with this example: $$  \sum_{n=1}^\infty \frac{(3n)!}{n^{3n}} $$ Im proving this using d'alembert principle So I have $$ \lim_{x\to \infty} \frac{\frac{(3(n+1))!}{(n+1)^{3(n+1)}}}{\frac{(3n)!}{n^{3n}}} $$ So after some calculations i get $$ \lim_{x\to \infty} \frac{(3n)!(3n+3)(3n+2)(3n+1)(n^{3n})}{(3n)!(n+1)^{3(n+1)}} $$ $$ \lim_{x\to \infty} \frac{(3n+3)(3n+2)(3n+1)(n^{3n})}{(n+1)^{3n+3}} $$ $$ \lim_{x\to \infty} \frac{(3n+3)(3n+2)(3n+1)(n^{3n})}{n^{3n+3}+1} $$ And I don't know how to cut the powers $n^{3n}$ $$ \frac {n^{3n}}{ n^{3n} * n^3 +1} $$ I will be very thankful for every help and explanation. I hope I named this problem corectly in english cause it isnt my mother tongue.,I am stucked with this example: $$  \sum_{n=1}^\infty \frac{(3n)!}{n^{3n}} $$ Im proving this using d'alembert principle So I have $$ \lim_{x\to \infty} \frac{\frac{(3(n+1))!}{(n+1)^{3(n+1)}}}{\frac{(3n)!}{n^{3n}}} $$ So after some calculations i get $$ \lim_{x\to \infty} \frac{(3n)!(3n+3)(3n+2)(3n+1)(n^{3n})}{(3n)!(n+1)^{3(n+1)}} $$ $$ \lim_{x\to \infty} \frac{(3n+3)(3n+2)(3n+1)(n^{3n})}{(n+1)^{3n+3}} $$ $$ \lim_{x\to \infty} \frac{(3n+3)(3n+2)(3n+1)(n^{3n})}{n^{3n+3}+1} $$ And I don't know how to cut the powers $n^{3n}$ $$ \frac {n^{3n}}{ n^{3n} * n^3 +1} $$ I will be very thankful for every help and explanation. I hope I named this problem corectly in english cause it isnt my mother tongue.,,"['limits', 'divergent-series']"
68,Finding the limit by doing the natural log of the numerator and denominator,Finding the limit by doing the natural log of the numerator and denominator,,"I want to show more concretely that $$\lim_{x\to\infty} \frac{e^\sqrt{x}}{x^{a}} = \infty$$ To do this I did the natural log of the numerator and denominator and then did l'Hospital's rule. My question is: it allowed to do the natural log of the numerator and denominator in order to put it in a clear form before evaluating the limit? Cause I know $6/3=2$, but $\ln\left(6\right)/\ln\left(3\right)$ is not. If it is not allowed what is a way to concretely evaluate this limit.","I want to show more concretely that $$\lim_{x\to\infty} \frac{e^\sqrt{x}}{x^{a}} = \infty$$ To do this I did the natural log of the numerator and denominator and then did l'Hospital's rule. My question is: it allowed to do the natural log of the numerator and denominator in order to put it in a clear form before evaluating the limit? Cause I know $6/3=2$, but $\ln\left(6\right)/\ln\left(3\right)$ is not. If it is not allowed what is a way to concretely evaluate this limit.",,"['limits', 'logarithms', 'limits-without-lhopital']"
69,Showing the existence of $\lim\limits_{x\to 1^-} \frac{1-x}{1-f(x)}$,Showing the existence of,\lim\limits_{x\to 1^-} \frac{1-x}{1-f(x)},"Let $f:[0,1]\to[0,1]$ be continuous, differentiable and with $f(1)=1$ and such that $f(x)\le x$ for all $x$. Consider the limit $$ \lim_{x\to 1^-} \frac{1-x}{1-f(x)}. $$ Does the limit exist? If it exists, is it possible to say that it must be finite?","Let $f:[0,1]\to[0,1]$ be continuous, differentiable and with $f(1)=1$ and such that $f(x)\le x$ for all $x$. Consider the limit $$ \lim_{x\to 1^-} \frac{1-x}{1-f(x)}. $$ Does the limit exist? If it exists, is it possible to say that it must be finite?",,"['real-analysis', 'limits']"
70,Using a definite integral find the value of $\lim_{n\rightarrow \infty }(\frac{1}{n}+\frac{1}{n+1}+...+\frac{1}{2n})$ [duplicate],Using a definite integral find the value of  [duplicate],\lim_{n\rightarrow \infty }(\frac{1}{n}+\frac{1}{n+1}+...+\frac{1}{2n}),"This question already has answers here : The limit of truncated sums of harmonic series, $\lim\limits_{k\to\infty}\sum_{n=k+1}^{2k}{\frac{1}{n}}$ (12 answers) Closed 8 years ago . Task: Using a definite integral find the value of: $$\lim_{n\rightarrow \infty }(\frac{1}{n}+\frac{1}{n+1}+...+\frac{1}{2n})$$ My Attempt: I began by writing out the sequence as a summation, where I afterwards isolated the $n$ sub-intervals multiplication: $$ \lim_{n\rightarrow \infty }\sum_{i=0}^{n}{\frac{1}{n+i}} = \lim_{n\rightarrow \infty }\sum_{i=0}^{n}{\frac{n}{n+i}\frac{1}{n}} $$ Here I encountered an unfamiliar situation with $i$ in the denominator and not in the numerator. Further investigation lead me to harmonic numbers, which is something I haven't covered yet and shouldn't be required. In attempting to solve this task I have found the following resource on the limit definition of a definite integral .","This question already has answers here : The limit of truncated sums of harmonic series, $\lim\limits_{k\to\infty}\sum_{n=k+1}^{2k}{\frac{1}{n}}$ (12 answers) Closed 8 years ago . Task: Using a definite integral find the value of: $$\lim_{n\rightarrow \infty }(\frac{1}{n}+\frac{1}{n+1}+...+\frac{1}{2n})$$ My Attempt: I began by writing out the sequence as a summation, where I afterwards isolated the $n$ sub-intervals multiplication: $$ \lim_{n\rightarrow \infty }\sum_{i=0}^{n}{\frac{1}{n+i}} = \lim_{n\rightarrow \infty }\sum_{i=0}^{n}{\frac{n}{n+i}\frac{1}{n}} $$ Here I encountered an unfamiliar situation with $i$ in the denominator and not in the numerator. Further investigation lead me to harmonic numbers, which is something I haven't covered yet and shouldn't be required. In attempting to solve this task I have found the following resource on the limit definition of a definite integral .",,"['limits', 'definite-integrals', 'riemann-integration']"
71,Find $\lim\limits_{n \to \infty} \frac{1!+3!+\ldots+(2n-1)!}{2!+4!+\ldots+(2n)!}$,Find,\lim\limits_{n \to \infty} \frac{1!+3!+\ldots+(2n-1)!}{2!+4!+\ldots+(2n)!},"$$\lim\limits_{n \to \infty} \frac{1!+3!+\ldots+(2n-1)!}{2!+4!+\ldots+(2n)!}$$ I have tried some standard approaches like dividing by $(2n)!$ and comparing consecutive terms. Hint, please :) Thanks in advance.","$$\lim\limits_{n \to \infty} \frac{1!+3!+\ldots+(2n-1)!}{2!+4!+\ldots+(2n)!}$$ I have tried some standard approaches like dividing by $(2n)!$ and comparing consecutive terms. Hint, please :) Thanks in advance.",,"['limits', 'factorial']"
72,"If a sequence $a_n$ has exactly three partial limits, and a sequence $b_n$ has exactly two partial limits. could the sequence $c_n=a_n+b_n$ converge?","If a sequence  has exactly three partial limits, and a sequence  has exactly two partial limits. could the sequence  converge?",a_n b_n c_n=a_n+b_n,"If a sequence $a_n$ has exactly three partial limits, and a sequence $b_n$ has exactly two partial limits. is it possible for the sequence $c_n=a_n+b_n$ to converge? I think this question doesn't demand a proof for every two sequences that have the same features, i think there is some examples for sequences that work and some that don't. for now I could think of examples for sequences that only disprove this allotment. For example $a_n= 1,2,3,1,2,3,1,2,3,...$ and $b_n=-1,1,-1,1,-1,1,-1,..$ then $c_n=0,3,2,4,1,4,0,3,2...$ do you gues have an example for sequences that might work?","If a sequence $a_n$ has exactly three partial limits, and a sequence $b_n$ has exactly two partial limits. is it possible for the sequence $c_n=a_n+b_n$ to converge? I think this question doesn't demand a proof for every two sequences that have the same features, i think there is some examples for sequences that work and some that don't. for now I could think of examples for sequences that only disprove this allotment. For example $a_n= 1,2,3,1,2,3,1,2,3,...$ and $b_n=-1,1,-1,1,-1,1,-1,..$ then $c_n=0,3,2,4,1,4,0,3,2...$ do you gues have an example for sequences that might work?",,"['calculus', 'limits', 'convergence-divergence']"
73,Why are there limits which do not exist?,Why are there limits which do not exist?,,"The limit of a real function can be of one of three types. A real limit would be a limit where the value is a real number: $$\lim_{x\to0}x^2=0$$ An infinite limit would be a limit where the value is equal to $\pm\infty$: $$\lim_{x\to-\infty}{1\over e^x}=\infty$$ And an undefined/non-existing limit would be one where it is not possible to assign a value of any kind to the limit: $$\lim_{x\to\infty}\sin x$$ My question is, what other undefined limits are out there? I understand that $\lim_{x\to\infty}\sin x$ is undefined because $\sin x$ is a periodic function whose value is in $[-1,1]$, and we can't find which, if any, single value in that interval it will achieve as $x$ tends to infinity. I also know I could get a similar undefined limit using any other trigonometric function. Are there any other limits composed of elementary functions which are undefined?  Are such undefined limits something exclusive to periodic functions or can they be constructed using other kinds of functions?","The limit of a real function can be of one of three types. A real limit would be a limit where the value is a real number: $$\lim_{x\to0}x^2=0$$ An infinite limit would be a limit where the value is equal to $\pm\infty$: $$\lim_{x\to-\infty}{1\over e^x}=\infty$$ And an undefined/non-existing limit would be one where it is not possible to assign a value of any kind to the limit: $$\lim_{x\to\infty}\sin x$$ My question is, what other undefined limits are out there? I understand that $\lim_{x\to\infty}\sin x$ is undefined because $\sin x$ is a periodic function whose value is in $[-1,1]$, and we can't find which, if any, single value in that interval it will achieve as $x$ tends to infinity. I also know I could get a similar undefined limit using any other trigonometric function. Are there any other limits composed of elementary functions which are undefined?  Are such undefined limits something exclusive to periodic functions or can they be constructed using other kinds of functions?",,"['real-analysis', 'limits']"
74,Find the limit $\lim_\limits{x\to +\infty}{\left( \left( e+1\right) ^{\ln \left( e^x+1\right)} - \left( e+1\right) ^x\right)} $,Find the limit,\lim_\limits{x\to +\infty}{\left( \left( e+1\right) ^{\ln \left( e^x+1\right)} - \left( e+1\right) ^x\right)} ,"Find without using De L'Hospital's rule the following limit: $$\lim_\limits{x\to +\infty}{\left( \left( e+1\right) ^{\ln \left( e^x+1\right)} - \left( e+1\right) ^x\right)} $$ I have tried to factorize it but I always seem to end up with an indeterminate form... How can I do it with using DLH? Please don't use approximations because I haven't ""officially"" learnt them yet...","Find without using De L'Hospital's rule the following limit: I have tried to factorize it but I always seem to end up with an indeterminate form... How can I do it with using DLH? Please don't use approximations because I haven't ""officially"" learnt them yet...",\lim_\limits{x\to +\infty}{\left( \left( e+1\right) ^{\ln \left( e^x+1\right)} - \left( e+1\right) ^x\right)} ,['limits']
75,What is the limit $\lim_{q\to 1} \vartheta_{2}^{2}(q)(1-q)$,What is the limit,\lim_{q\to 1} \vartheta_{2}^{2}(q)(1-q),"If $\vartheta_{2}(q)$ is jacobi's theta function, what is the limit $$\lim_{q\to 1} \vartheta_{2}^{2}(q)(1-q)$$ for the nome $q$. I would like to know whether the limit exists or not. If it does, please let me know and provide it's evaluation","If $\vartheta_{2}(q)$ is jacobi's theta function, what is the limit $$\lim_{q\to 1} \vartheta_{2}^{2}(q)(1-q)$$ for the nome $q$. I would like to know whether the limit exists or not. If it does, please let me know and provide it's evaluation",,"['calculus', 'limits', 'special-functions', 'modular-forms', 'theta-functions']"
76,"Show that this difference goes to zero,","Show that this difference goes to zero,",,"$$\frac{1+\sqrt{2} + ... + \sqrt{N}}{N} - \frac{2}{3}\sqrt{N} \to  0.$$ The hint given in the question is this: choose appropriate Riemann sums and estimate the approximation error. My current work: $$\frac{1+\sqrt{2} + ... + \sqrt{N}}{N} - \frac{2}{3}\sqrt{N}$$ $$=: A_n =(\sum_{k=1}^N \sqrt{k}\frac{1}{N}) - \frac{2}{3}\sqrt{N}$$ The first term is in the form of a Riemann sum, so letting N go to infinity, we see that mesh(p) goes to zero, for some partition p, which gives the (improper) Riemann integral, over the interval [1,N]: $$\lim_{N->\infty}\int_1^N \sqrt{x}dx$$ Evaluation of the integral, without evaluating the limit, gives: $$\frac{2}{3}N^{\frac{3}{2}} - \frac{2}{3}$$ Then $$A_n = \frac{2}{3}N^{\frac{3}{2}} - \frac{2}{3} - \frac{2}{3}\sqrt{N}$$ And this is where I am currently stuck.  The above equation is a little suspect, because I let N go to infinity to get the improper integral, while I did nothing with the $\frac{2}{3}\sqrt{N}$ term -- and just included this term into the equation, since I feel it gets me a little closer to do some kind of approximation. Any hints would be greatly appreciated. Thanks,","$$\frac{1+\sqrt{2} + ... + \sqrt{N}}{N} - \frac{2}{3}\sqrt{N} \to  0.$$ The hint given in the question is this: choose appropriate Riemann sums and estimate the approximation error. My current work: $$\frac{1+\sqrt{2} + ... + \sqrt{N}}{N} - \frac{2}{3}\sqrt{N}$$ $$=: A_n =(\sum_{k=1}^N \sqrt{k}\frac{1}{N}) - \frac{2}{3}\sqrt{N}$$ The first term is in the form of a Riemann sum, so letting N go to infinity, we see that mesh(p) goes to zero, for some partition p, which gives the (improper) Riemann integral, over the interval [1,N]: $$\lim_{N->\infty}\int_1^N \sqrt{x}dx$$ Evaluation of the integral, without evaluating the limit, gives: $$\frac{2}{3}N^{\frac{3}{2}} - \frac{2}{3}$$ Then $$A_n = \frac{2}{3}N^{\frac{3}{2}} - \frac{2}{3} - \frac{2}{3}\sqrt{N}$$ And this is where I am currently stuck.  The above equation is a little suspect, because I let N go to infinity to get the improper integral, while I did nothing with the $\frac{2}{3}\sqrt{N}$ term -- and just included this term into the equation, since I feel it gets me a little closer to do some kind of approximation. Any hints would be greatly appreciated. Thanks,",,"['calculus', 'real-analysis', 'integration', 'limits', 'riemann-sum']"
77,limit question - $\lim_{x\rightarrow a}g(f(x))=c$,limit question -,\lim_{x\rightarrow a}g(f(x))=c,Can i say that if  $\lim_{x\rightarrow a}f(x)=b$ and $\lim_{x\rightarrow b}g(x)=c$ then $\lim_{x\rightarrow a}g(f(x))=c$ ? I don't think so but don't know how to prove it. Thanks.,Can i say that if  $\lim_{x\rightarrow a}f(x)=b$ and $\lim_{x\rightarrow b}g(x)=c$ then $\lim_{x\rightarrow a}g(f(x))=c$ ? I don't think so but don't know how to prove it. Thanks.,,"['calculus', 'limits']"
78,What is the best way to explain setting a restriction on $\delta$ in $\epsilon$-$\delta$ proofs?,What is the best way to explain setting a restriction on  in - proofs?,\delta \epsilon \delta,"I'm trying to prepare a somewhat informal lesson striving to provide an intuitive understanding of why for some limit proofs, we have to set an upper bound on $\delta$. For example, here's part of the preliminary analysis of the proof I'm walking through: Problem: Prove that $\displaystyle \lim_{x\to3} 9x^2+6x+1=100$.   $$\begin{align*} \left|9x^2+6x+1-100\right|&=\left|9x^2+6x-99\right|\\ &=3\left|3x^2+2x-33\right|\\ &=3|(3x+11)(x-3)|\\ &=3|3x+11||x-3| \end{align*}$$   Now if we let $\delta\le1$, we have   $$|x-3|<1\implies-1<x-3<1\implies17<3x+11<23\implies|3x+11|<23$$   $$\begin{align*} \left|9x^2+6x+1-100\right|&=3|3x+11||x-3|\\ &<69|x-3|\\ &<\epsilon \end{align*}$$   which means $\delta=\min\left\{1,\dfrac{\epsilon}{69}\right\}$ is sufficient. What's the best way to illustrate why we set $\delta\le1$ in an intuitive way, and why it is sometimes necessary to use a different upper bound?","I'm trying to prepare a somewhat informal lesson striving to provide an intuitive understanding of why for some limit proofs, we have to set an upper bound on $\delta$. For example, here's part of the preliminary analysis of the proof I'm walking through: Problem: Prove that $\displaystyle \lim_{x\to3} 9x^2+6x+1=100$.   $$\begin{align*} \left|9x^2+6x+1-100\right|&=\left|9x^2+6x-99\right|\\ &=3\left|3x^2+2x-33\right|\\ &=3|(3x+11)(x-3)|\\ &=3|3x+11||x-3| \end{align*}$$   Now if we let $\delta\le1$, we have   $$|x-3|<1\implies-1<x-3<1\implies17<3x+11<23\implies|3x+11|<23$$   $$\begin{align*} \left|9x^2+6x+1-100\right|&=3|3x+11||x-3|\\ &<69|x-3|\\ &<\epsilon \end{align*}$$   which means $\delta=\min\left\{1,\dfrac{\epsilon}{69}\right\}$ is sufficient. What's the best way to illustrate why we set $\delta\le1$ in an intuitive way, and why it is sometimes necessary to use a different upper bound?",,"['limits', 'intuition', 'epsilon-delta', 'proof-explanation']"
79,Find the Derivative $ 7^{\ln x} $ using first principle,Find the Derivative  using first principle, 7^{\ln x} ,"I still can't figure this out, Question is Find the Derivative $ 7^{\ln(x)} $ using first principle This is where I got $$\lim_{h \to 0} \frac{7^{\ln(x+h)} -  7^{\ln(x)}}{h}  $$ then What should I do?","I still can't figure this out, Question is Find the Derivative $ 7^{\ln(x)} $ using first principle This is where I got $$\lim_{h \to 0} \frac{7^{\ln(x+h)} -  7^{\ln(x)}}{h}  $$ then What should I do?",,"['calculus', 'limits']"
80,"Regarding $\lim \limits_{(x,y,z)\to (0,0,0)}\left(\frac{x^2z}{x^2+y^2+16z^2}\right)$--is WolframAlpha incorrect?",Regarding --is WolframAlpha incorrect?,"\lim \limits_{(x,y,z)\to (0,0,0)}\left(\frac{x^2z}{x^2+y^2+16z^2}\right)","$$      \lim_{x,y,z\to 0}         {zx^2\over x^2+y^2+16z^2}$$ So I am trying to evaluate this limit.. To me, by using the squeeze theorem, it seems that the answer must be zero. I trying using the spherical coordinates, which also gives in the same result. However, WolframAlpha says the limit does not exist . Could I know whether I am missing something or WolframAlpha is incorrect?(as it happens occasionally)","$$      \lim_{x,y,z\to 0}         {zx^2\over x^2+y^2+16z^2}$$ So I am trying to evaluate this limit.. To me, by using the squeeze theorem, it seems that the answer must be zero. I trying using the spherical coordinates, which also gives in the same result. However, WolframAlpha says the limit does not exist . Could I know whether I am missing something or WolframAlpha is incorrect?(as it happens occasionally)",,"['limits', 'multivariable-calculus', 'wolfram-alpha']"
81,Prove $\lim_{k \rightarrow \infty} \int_{0}^{k} x^n(1-k^{-1}x)^{k}dx = n!$,Prove,\lim_{k \rightarrow \infty} \int_{0}^{k} x^n(1-k^{-1}x)^{k}dx = n!,I need to show the following limit $\lim_{k \rightarrow \infty} \int_{0}^{k} x^n(1-k^{-1}x)^{k}dx = n!$ I tried using the binomial expansion of $(1-x)^k$ but its not leading me anywhere !  maybe I need to use DCT .. Please help !,I need to show the following limit $\lim_{k \rightarrow \infty} \int_{0}^{k} x^n(1-k^{-1}x)^{k}dx = n!$ I tried using the binomial expansion of $(1-x)^k$ but its not leading me anywhere !  maybe I need to use DCT .. Please help !,,"['limits', 'measure-theory']"
82,Real Analysis question about proving limits using boundedness and monotony,Real Analysis question about proving limits using boundedness and monotony,,"I am trying to get a hang of solving problems that ask to ""prove the limit"". One of the questions on our previous homework was: The solution starts with . How do we know that sn is bounded below and not above? I believe the next step is to plug in the formula for Sn+1 into Sn ^2 - x >= 0 but I am not sure why. I think I will understand the rest of the solution if you can help me understand this. Thank you.","I am trying to get a hang of solving problems that ask to ""prove the limit"". One of the questions on our previous homework was: The solution starts with . How do we know that sn is bounded below and not above? I believe the next step is to plug in the formula for Sn+1 into Sn ^2 - x >= 0 but I am not sure why. I think I will understand the rest of the solution if you can help me understand this. Thank you.",,"['calculus', 'real-analysis', 'limits']"
83,"Computing $\lim_{(x,y)\to (0,0)}\frac{\sin(x+y)}{x+y}$",Computing,"\lim_{(x,y)\to (0,0)}\frac{\sin(x+y)}{x+y}","I'm trying to compute the following limits and the textbook that I'm looking at suggested the following method. $$\lim_{(x,y)\to (0,0)}\frac{\sin(x+y)}{x+y}$$ $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}$$ Each of these limits are equal to $\lim_{t\to 0}\frac{\sin(t)}{t}=1$. However, I'm curious how to analytically prove that such change in variables yield the correct value of the original limit. That is, when are we justified in changing a vector $(x,y)$ to a single variable $t$? In the single variable case, I'm familiar with such manipulations, but I have a bit of uneasiness in seeing such manipulations for the first time in multivariable case. I'd appreciate it if anyone could explain to me why and when such transformations are justified in the multivariate case.","I'm trying to compute the following limits and the textbook that I'm looking at suggested the following method. $$\lim_{(x,y)\to (0,0)}\frac{\sin(x+y)}{x+y}$$ $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy)}{xy}$$ Each of these limits are equal to $\lim_{t\to 0}\frac{\sin(t)}{t}=1$. However, I'm curious how to analytically prove that such change in variables yield the correct value of the original limit. That is, when are we justified in changing a vector $(x,y)$ to a single variable $t$? In the single variable case, I'm familiar with such manipulations, but I have a bit of uneasiness in seeing such manipulations for the first time in multivariable case. I'd appreciate it if anyone could explain to me why and when such transformations are justified in the multivariate case.",,"['real-analysis', 'analysis', 'limits', 'multivariable-calculus', 'continuity']"
84,About a limit with Euler $\Gamma$ function.,About a limit with Euler  function.,\Gamma,"For all $x \in \Bbb R_+^*$, we put:  $$f(x)=\frac{1}{\Gamma(x)}\int_x^{+\infty}t^{x-1}e^{-t}dt.$$ Can we compute the limit : $\displaystyle\lim_{x \to +\infty} f(x) $?","For all $x \in \Bbb R_+^*$, we put:  $$f(x)=\frac{1}{\Gamma(x)}\int_x^{+\infty}t^{x-1}e^{-t}dt.$$ Can we compute the limit : $\displaystyle\lim_{x \to +\infty} f(x) $?",,['limits']
85,Limit-Fundamental Concept?,Limit-Fundamental Concept?,,"Can anyone clear me this fundamental concept about this.  I am confused for many months over this. It is said that in $\lim_{x\to a} f(x)^{g(x)}$. $f(x)$ should be greater then $0$. Can anyone explain the reason? If $f(x)$ is $-2$ and $g(x)$ is $\frac{1}{2}$,then it is not possible. But if $f(x)$ is $-2$ and $g(x)$ is $2$,then why it is not possible? (It is true as $(-2)^2 = 4$)","Can anyone clear me this fundamental concept about this.  I am confused for many months over this. It is said that in $\lim_{x\to a} f(x)^{g(x)}$. $f(x)$ should be greater then $0$. Can anyone explain the reason? If $f(x)$ is $-2$ and $g(x)$ is $\frac{1}{2}$,then it is not possible. But if $f(x)$ is $-2$ and $g(x)$ is $2$,then why it is not possible? (It is true as $(-2)^2 = 4$)",,"['calculus', 'limits']"
86,Find $ \lim_{x\rightarrow 0}\frac{\tan^2 x+2x}{x+x^2} $ without L'Hôpital's Rule,Find  without L'Hôpital's Rule, \lim_{x\rightarrow 0}\frac{\tan^2 x+2x}{x+x^2} ,I need to find $$\lim_{x\to 0}\frac{\tan^2 x+2x}{x+x^2}$$ This is what I did: Let $f(x)=\frac{\tan^2(x)+2x}{x+x^2}=\frac{\frac{\sin^2(x)}{\cos^2(x)}+2x}{x+x^2}=\frac{\sin^2(x)+2x\cos^2(x)}{x(x+1)\cos^2(x)}=\frac{\sin^2(x)}{x(x+1)\cos^2(x)}+\frac{2}{x+1}=\frac{\sin (x)}{x}\cdot\frac{\sin(x)}{(x+1)\cos^2(x)}+\frac{2}{x+1}$ $\lim_{x\to0}\left(\frac{\sin(x)}{x}\cdot\frac{\sin(x)}{(x+1)\cos^2(x)}+\frac{2}{x+1}\right)=\underbrace{\left(\lim_{x\to0}\frac{\sin(x)}{x}\right)}_{=1}\cdot\left(\lim_{x\to0}\frac{\sin(x)}{(x+1)\cos^2(x)}\right)+\underbrace{\lim_{x\to0}\left(\frac{2}{x+1}\right)}_{=2}=\lim_{x\to0}\frac{\sin(x)}{(x+1)\cos^2(x)}+2=\frac{\lim_{x\to0}\sin(x)}{\underbrace{\left(\lim_{x\to0}x+1\right)}_{=1}\cdot\underbrace{\left(\lim_{x\to0}\cos^2(x)\right)}_{=1^2=1}}+2=\lim_{x\to0}\sin(x)+2=0+2=2$ Is this correct? I feel like I have assumed the some limits rather than proving it. We are allowed to use the common result $\frac{\sin x}{x}$ is $1$ as $x$ tends to $0$.,I need to find $$\lim_{x\to 0}\frac{\tan^2 x+2x}{x+x^2}$$ This is what I did: Let $f(x)=\frac{\tan^2(x)+2x}{x+x^2}=\frac{\frac{\sin^2(x)}{\cos^2(x)}+2x}{x+x^2}=\frac{\sin^2(x)+2x\cos^2(x)}{x(x+1)\cos^2(x)}=\frac{\sin^2(x)}{x(x+1)\cos^2(x)}+\frac{2}{x+1}=\frac{\sin (x)}{x}\cdot\frac{\sin(x)}{(x+1)\cos^2(x)}+\frac{2}{x+1}$ $\lim_{x\to0}\left(\frac{\sin(x)}{x}\cdot\frac{\sin(x)}{(x+1)\cos^2(x)}+\frac{2}{x+1}\right)=\underbrace{\left(\lim_{x\to0}\frac{\sin(x)}{x}\right)}_{=1}\cdot\left(\lim_{x\to0}\frac{\sin(x)}{(x+1)\cos^2(x)}\right)+\underbrace{\lim_{x\to0}\left(\frac{2}{x+1}\right)}_{=2}=\lim_{x\to0}\frac{\sin(x)}{(x+1)\cos^2(x)}+2=\frac{\lim_{x\to0}\sin(x)}{\underbrace{\left(\lim_{x\to0}x+1\right)}_{=1}\cdot\underbrace{\left(\lim_{x\to0}\cos^2(x)\right)}_{=1^2=1}}+2=\lim_{x\to0}\sin(x)+2=0+2=2$ Is this correct? I feel like I have assumed the some limits rather than proving it. We are allowed to use the common result $\frac{\sin x}{x}$ is $1$ as $x$ tends to $0$.,,"['calculus', 'limits', 'limits-without-lhopital']"
87,Using $\lim\limits_{x \to 0} \frac{\sin x}{x} = 1$ evaluate the limit,Using  evaluate the limit,\lim\limits_{x \to 0} \frac{\sin x}{x} = 1,Use $$\lim_{x\to 0} \frac{\sin x}{x} = 1$$ to evaluate the limit $$\lim_{x\to 0} \frac{x\tan (x^2)}{\cos(3x)\sin^3(2x)}$$ I'm really not sure how to go about this apart from trig identities and using the limit $=1$ to simplify it. Can anyone even get me started on it please?,Use $$\lim_{x\to 0} \frac{\sin x}{x} = 1$$ to evaluate the limit $$\lim_{x\to 0} \frac{x\tan (x^2)}{\cos(3x)\sin^3(2x)}$$ I'm really not sure how to go about this apart from trig identities and using the limit $=1$ to simplify it. Can anyone even get me started on it please?,,"['calculus', 'limits', 'limits-without-lhopital']"
88,Evaluating a limit by Riemann sums,Evaluating a limit by Riemann sums,,"I'm having some issues with understanding exactly how to see that a given limit/sum is a Riemann sum. For example: $\lim_{n \to \infty} \frac{1}{\sqrt{n}} \sum_{k=1}^n \frac{1}{\sqrt{k}}$. I recognize that $\frac{1}{\sqrt{n}}$ is the length of the partitions, and the other fraction is $f(c_k)$, where $c_k$ is a value picked in the k'th interval of the n'th partition, but to me it looks like $f(x) = \frac{1}{\sqrt{x}}$ and so $c_k$ should be just $k$, but $k$ is a lot bigger than most of the partitions' length. Another type of problem I see is where $c_k$ depends on $n$, and again it seems to me that there isn't a $c_k$ for each k'th interval. For example this problem: $\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n sin(\frac{k\pi}{2n})$. Here the length of each partition is $\frac{1}{n}$, but $c_k = \frac{k\pi}{2} \frac{1}{n}$, and since the ""multiplier"" is greater than one, it looks to me like $c_k$ is bound to skip some intervals eventually, and so there isn't a $c_k$ to each interval, and then it's no longer a Riemann sum. But, obviously it is, I just don't understand it.. can anyone explain this to me? My textbook has a few problems like this, but no explanations. Thanks a lot in advance.","I'm having some issues with understanding exactly how to see that a given limit/sum is a Riemann sum. For example: $\lim_{n \to \infty} \frac{1}{\sqrt{n}} \sum_{k=1}^n \frac{1}{\sqrt{k}}$. I recognize that $\frac{1}{\sqrt{n}}$ is the length of the partitions, and the other fraction is $f(c_k)$, where $c_k$ is a value picked in the k'th interval of the n'th partition, but to me it looks like $f(x) = \frac{1}{\sqrt{x}}$ and so $c_k$ should be just $k$, but $k$ is a lot bigger than most of the partitions' length. Another type of problem I see is where $c_k$ depends on $n$, and again it seems to me that there isn't a $c_k$ for each k'th interval. For example this problem: $\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n sin(\frac{k\pi}{2n})$. Here the length of each partition is $\frac{1}{n}$, but $c_k = \frac{k\pi}{2} \frac{1}{n}$, and since the ""multiplier"" is greater than one, it looks to me like $c_k$ is bound to skip some intervals eventually, and so there isn't a $c_k$ to each interval, and then it's no longer a Riemann sum. But, obviously it is, I just don't understand it.. can anyone explain this to me? My textbook has a few problems like this, but no explanations. Thanks a lot in advance.",,"['calculus', 'limits', 'riemann-sum']"
89,"Find the result of $ \sum_{x = 1}^{\infty}\frac{1}{x}\,\log\left(\frac{kx}{\left\lfloor kx\right\rfloor}_{o}\right)$",Find the result of," \sum_{x = 1}^{\infty}\frac{1}{x}\,\log\left(\frac{kx}{\left\lfloor kx\right\rfloor}_{o}\right)","I would like to calculate the sum $$ \sum_{x = 1}^{\infty}\frac{1}{x}\, \log\left(\frac{kx}{\left\lfloor kx \right\rfloor}_{o}\right) $$ where $k = \sqrt{\, 2\,} + 1$, $x$ is an odd integer and $\left\lfloor z \right\rfloor_o$ indicates the greatest odd integer $\leq z$. Considering that the odd greatest integer function satisfies $\left\lfloor z \right\rfloor_{o} =  2\left\lfloor\frac{z + 1}{2}\right\rfloor - 1$, and setting $x = 2n - 1$ to obtain a summation over all positive integers, the sum can also be written as $$ \sum_{n = 1}^{\infty}\frac{1}{2n - 1}\, \log\left(\frac{k\left[2n - 1\right]} {2\left\lfloor kn - k/2 + 1/2\right\rfloor - 1}\right) $$ The numerical value of the sum converges to $0.952842\ldots$ ( these first six decimal digits are stable after summing the first $10^{8}$ terms ). I tried several approaches to determine an explicit expression for this limit, including transformations of floor function by Fourier series or Laplace transforms, but I was not able to get it. Even if  a closed expression could maybe not exist, I would be very interested in obtaining a simpler expression for this infinite sum.","I would like to calculate the sum $$ \sum_{x = 1}^{\infty}\frac{1}{x}\, \log\left(\frac{kx}{\left\lfloor kx \right\rfloor}_{o}\right) $$ where $k = \sqrt{\, 2\,} + 1$, $x$ is an odd integer and $\left\lfloor z \right\rfloor_o$ indicates the greatest odd integer $\leq z$. Considering that the odd greatest integer function satisfies $\left\lfloor z \right\rfloor_{o} =  2\left\lfloor\frac{z + 1}{2}\right\rfloor - 1$, and setting $x = 2n - 1$ to obtain a summation over all positive integers, the sum can also be written as $$ \sum_{n = 1}^{\infty}\frac{1}{2n - 1}\, \log\left(\frac{k\left[2n - 1\right]} {2\left\lfloor kn - k/2 + 1/2\right\rfloor - 1}\right) $$ The numerical value of the sum converges to $0.952842\ldots$ ( these first six decimal digits are stable after summing the first $10^{8}$ terms ). I tried several approaches to determine an explicit expression for this limit, including transformations of floor function by Fourier series or Laplace transforms, but I was not able to get it. Even if  a closed expression could maybe not exist, I would be very interested in obtaining a simpler expression for this infinite sum.",,"['number-theory', 'limits', 'summation', 'ceiling-and-floor-functions']"
90,Example of limit of a function,Example of limit of a function,,"I'm self-studying from the book Understanding Analysis by Stephen Abbott, and I don't understand example 4.2.2 on page 105. The aim of the example is to show that: $$ \lim_{x \to 2} g(x) = 4 $$  where $g(x) = x^2$. The author starts by writing: $$ |g(x) - 4| = |x^2 - 4 | = |x+2||x-2| $$ which I understand. But then the following paragraph makes no sense to me: We can make $|x-2|$ as small as we like, but we need an upper bound on $|x+2|$ in order to know how small to choose $\delta$. The presence of the variable $x$ causes some initial confusion, but keep in mind that we are discussing the limit as $x$ approaches $2$. If we  agree that our $\delta$-neighborhood around $c = 2$ must have radius no bigger than $\delta = 1$, then we get the upper bound $|x + 2| \leq |3 + 2| = 5$ for all $x \in V_\delta(c)$. By saying we are only considering the $\delta$-neighborhood around $c=2$ with a radius smaller than or equal to $1$, I would think we get: $$ V_\delta (c) = \{ x \in \mathbb{R} \mid |x-2| < 1 \} = (1,3) $$ so I don't understand where ""$|x + 2| \leq |3 + 2| = 5$ for all $x \in V_\delta(c)$"" comes from? Any help is much appreciated.","I'm self-studying from the book Understanding Analysis by Stephen Abbott, and I don't understand example 4.2.2 on page 105. The aim of the example is to show that: $$ \lim_{x \to 2} g(x) = 4 $$  where $g(x) = x^2$. The author starts by writing: $$ |g(x) - 4| = |x^2 - 4 | = |x+2||x-2| $$ which I understand. But then the following paragraph makes no sense to me: We can make $|x-2|$ as small as we like, but we need an upper bound on $|x+2|$ in order to know how small to choose $\delta$. The presence of the variable $x$ causes some initial confusion, but keep in mind that we are discussing the limit as $x$ approaches $2$. If we  agree that our $\delta$-neighborhood around $c = 2$ must have radius no bigger than $\delta = 1$, then we get the upper bound $|x + 2| \leq |3 + 2| = 5$ for all $x \in V_\delta(c)$. By saying we are only considering the $\delta$-neighborhood around $c=2$ with a radius smaller than or equal to $1$, I would think we get: $$ V_\delta (c) = \{ x \in \mathbb{R} \mid |x-2| < 1 \} = (1,3) $$ so I don't understand where ""$|x + 2| \leq |3 + 2| = 5$ for all $x \in V_\delta(c)$"" comes from? Any help is much appreciated.",,"['real-analysis', 'limits', 'functions', 'epsilon-delta']"
91,Calculus - prove that limit doesn't exists using esplion and delta,Calculus - prove that limit doesn't exists using esplion and delta,,"Using epsilon and delta proof this : $$\lim_{x \to 3} \sqrt{x+1}\neq1$$ Exist $\varepsilon>0$ All $\delta>0$ so for some $x$ that appiles $0<|x-3|<\delta$ and however $|\sqrt{x+1}-1|\geq\varepsilon$ I don't really know how to approach this, any help will be appreciated.","Using epsilon and delta proof this : $$\lim_{x \to 3} \sqrt{x+1}\neq1$$ Exist $\varepsilon>0$ All $\delta>0$ so for some $x$ that appiles $0<|x-3|<\delta$ and however $|\sqrt{x+1}-1|\geq\varepsilon$ I don't really know how to approach this, any help will be appreciated.",,"['calculus', 'limits', 'epsilon-delta']"
92,The Weibull as the limiting distribution of the Burr distribution,The Weibull as the limiting distribution of the Burr distribution,,"I often deal with ""payout patterns"" which are vectors of the cumulative percentage of a loss that has been paid over time. For example, for $t \in [0, 1, 2, 3, 4, 5]$ I may have $p_t = (5\%, 15\%, 60\%, 85\%, 98\%, 100\%)$ . As these vectors have to be between $0$ and $1$ , I tend to used smoothed versions obtained by finding the ""closest"" parametric cumulative distribution function, almost always minimizing the sum squared distance between the empirical value and the CDF at each observations (similar to finding the distribution with the minimum Cramer-von Mises criterion). Two of the distributions I often use are the Burr and the Weibull. I use the common North American actuarial parametrization as brought in Klugman, Panjer, and Wilmott (1998). For clarity, in terms of the distribution functions, they are: $$ \begin{align} \Large F(x)_\textrm{Weibull} &= \LARGE 1 - e^{-\left(\frac{x}{\theta}\right)^\tau}\\ \Large F(x)_\textrm{Burr} &= \Large 1 - \left(\frac{1}{1 + \left(\frac{x}{\theta}\right)^\gamma}\right)^\alpha \end{align} $$ What I have seen many times when solving for the minimum distance, is that if the Burr's $\theta$ and $\alpha$ diverge to $\infty$ , the Burr CDF approaches the Weibull and the Burr $\gamma$ is the Weibull $\tau$ . I have sketched out a framework for a proof, but I a do not believe it is rigorous. What I would appreciate is: Corrections, comments, or any constructive criticism on whether or not this relationship can be formally proven, and Whether there is any way to estimate the Weibull $\theta$ from the diverging Burr (which I doubt for reasons brought below). Proof attempt We have the two distributions brought above. The question can be stated as proving: $$ \lim_{\alpha, \theta \rightarrow \infty} \left(\frac{1}{1 + \left(\frac{x}{\theta}\right)^\gamma}\right)^\alpha \to \Large e^{-\left(\frac{x}{\theta}\right)^\tau} $$ Firstly, re-write the left side as: $$ \lim_{\alpha, \theta \rightarrow \infty}\left({1 + \frac{x^\gamma}{\theta^\gamma}}\right)^{-\alpha} $$ Now let $\xi = \theta^\gamma$ . We now have $$ \lim_{\alpha, \xi \rightarrow \infty}\left({1 + \frac{x^\gamma}{\xi}}\right)^{-\alpha} $$ Now here is the weak part. As both $\alpha$ and $\xi$ are approaching $\infty$ , replace both with $n$ . Firstly, I'm not sure that is necessarily mathematically legal, and secondly, this is where question 2 probably fails, since notwithstanding that both $\alpha$ and $\xi (\theta^\gamma)$ diverge, they do so at different rates. That being said, we now have $$ \lim_{n \rightarrow \infty}\left({1 + \frac{x^\gamma}{n}}\right)^{-n} $$ Taking the log and re-arranging, we get a L'Hopital condition of $$ \lim_{n \rightarrow \infty} \frac{\log\left[1 + \frac{x^\gamma}{n}\right]}{-\frac{1}{n}} $$ as both numerator and denominator approach 0. Taking the individual derivatives of the numerator and denominator we get $$ \lim_{n \rightarrow \infty} \Large \frac{\left[\frac{1}{1 + \frac{x^\gamma}{n}}\right]\cdot\left({-x}^\gamma\right) n^{-2}}{n^{-2}} $$ The powers of $n$ cancel leaving us with $$ \lim_{n \rightarrow \infty} \Large \left[\frac{1}{1 + \frac{x^\gamma}{n}}\right]\cdot\left({-x}^\gamma\right) $$ which converges to $-x^\gamma$ . Since we originally logged the formula, we actually have $$ \lim_{\alpha, \theta \rightarrow \infty} \left(\frac{1}{1 + \left(\frac{x}{\theta}\right)^\gamma}\right)^\alpha \to \Large e^{-x^\gamma} $$ which is very close to what we actually want. What is missing is the $\theta^\gamma$ in the denominator, realizing that this $\theta$ has nothing to do with that found in the Burr. So, I would appreciate help in making this more rigorous, if possible, understanding any limitations, and any other opportunities to learn. Thank you.","I often deal with ""payout patterns"" which are vectors of the cumulative percentage of a loss that has been paid over time. For example, for I may have . As these vectors have to be between and , I tend to used smoothed versions obtained by finding the ""closest"" parametric cumulative distribution function, almost always minimizing the sum squared distance between the empirical value and the CDF at each observations (similar to finding the distribution with the minimum Cramer-von Mises criterion). Two of the distributions I often use are the Burr and the Weibull. I use the common North American actuarial parametrization as brought in Klugman, Panjer, and Wilmott (1998). For clarity, in terms of the distribution functions, they are: What I have seen many times when solving for the minimum distance, is that if the Burr's and diverge to , the Burr CDF approaches the Weibull and the Burr is the Weibull . I have sketched out a framework for a proof, but I a do not believe it is rigorous. What I would appreciate is: Corrections, comments, or any constructive criticism on whether or not this relationship can be formally proven, and Whether there is any way to estimate the Weibull from the diverging Burr (which I doubt for reasons brought below). Proof attempt We have the two distributions brought above. The question can be stated as proving: Firstly, re-write the left side as: Now let . We now have Now here is the weak part. As both and are approaching , replace both with . Firstly, I'm not sure that is necessarily mathematically legal, and secondly, this is where question 2 probably fails, since notwithstanding that both and diverge, they do so at different rates. That being said, we now have Taking the log and re-arranging, we get a L'Hopital condition of as both numerator and denominator approach 0. Taking the individual derivatives of the numerator and denominator we get The powers of cancel leaving us with which converges to . Since we originally logged the formula, we actually have which is very close to what we actually want. What is missing is the in the denominator, realizing that this has nothing to do with that found in the Burr. So, I would appreciate help in making this more rigorous, if possible, understanding any limitations, and any other opportunities to learn. Thank you.","t \in [0, 1, 2, 3, 4, 5] p_t = (5\%, 15\%, 60\%, 85\%, 98\%, 100\%) 0 1 
\begin{align}
\Large F(x)_\textrm{Weibull} &= \LARGE 1 - e^{-\left(\frac{x}{\theta}\right)^\tau}\\
\Large F(x)_\textrm{Burr} &= \Large 1 - \left(\frac{1}{1 + \left(\frac{x}{\theta}\right)^\gamma}\right)^\alpha
\end{align}
 \theta \alpha \infty \gamma \tau \theta 
\lim_{\alpha, \theta \rightarrow \infty} \left(\frac{1}{1 + \left(\frac{x}{\theta}\right)^\gamma}\right)^\alpha \to \Large e^{-\left(\frac{x}{\theta}\right)^\tau}
 
\lim_{\alpha, \theta \rightarrow \infty}\left({1 + \frac{x^\gamma}{\theta^\gamma}}\right)^{-\alpha}
 \xi = \theta^\gamma 
\lim_{\alpha, \xi \rightarrow \infty}\left({1 + \frac{x^\gamma}{\xi}}\right)^{-\alpha}
 \alpha \xi \infty n \alpha \xi (\theta^\gamma) 
\lim_{n \rightarrow \infty}\left({1 + \frac{x^\gamma}{n}}\right)^{-n}
 
\lim_{n \rightarrow \infty} \frac{\log\left[1 + \frac{x^\gamma}{n}\right]}{-\frac{1}{n}}
 
\lim_{n \rightarrow \infty} \Large \frac{\left[\frac{1}{1 + \frac{x^\gamma}{n}}\right]\cdot\left({-x}^\gamma\right) n^{-2}}{n^{-2}}
 n 
\lim_{n \rightarrow \infty} \Large \left[\frac{1}{1 + \frac{x^\gamma}{n}}\right]\cdot\left({-x}^\gamma\right)
 -x^\gamma 
\lim_{\alpha, \theta \rightarrow \infty} \left(\frac{1}{1 + \left(\frac{x}{\theta}\right)^\gamma}\right)^\alpha \to \Large e^{-x^\gamma}
 \theta^\gamma \theta","['limits', 'statistics', 'probability-distributions', 'solution-verification']"
93,Is this a legal transformation?,Is this a legal transformation?,,"To be found: $$\lim \left(1+\frac{2}{n}\right)^n$$ Presuppose $~~\lim \left(1+\frac{1}{n}\right)^n=e~~$ is already shown. Expanding the first equation: $$\lim \left(1+\frac{1}{\frac{n}{2}}\right)^{2\cdot\frac{n}{2}}=\lim \left(1+\frac{1}{\frac{n}{2}}\right)^{\frac{n}{2}}\cdot\lim \left(1+\frac{1}{\frac{n}{2}}\right)^{\frac{n}{2}}$$ I'd say that those two factors converge against $e$, just not as fast. My explanation would be, that the sets $\mathbb N$ and $\{\frac{n}{2}:n\in \mathbb N\}$ have infinitely many elements in common. So is it enough to just (state that and) add $$=\lim \left(1+\frac{1}{n}\right)^n\cdot \lim \left(1+\frac{1}{n}\right)^n=e\cdot e=e^2$$?","To be found: $$\lim \left(1+\frac{2}{n}\right)^n$$ Presuppose $~~\lim \left(1+\frac{1}{n}\right)^n=e~~$ is already shown. Expanding the first equation: $$\lim \left(1+\frac{1}{\frac{n}{2}}\right)^{2\cdot\frac{n}{2}}=\lim \left(1+\frac{1}{\frac{n}{2}}\right)^{\frac{n}{2}}\cdot\lim \left(1+\frac{1}{\frac{n}{2}}\right)^{\frac{n}{2}}$$ I'd say that those two factors converge against $e$, just not as fast. My explanation would be, that the sets $\mathbb N$ and $\{\frac{n}{2}:n\in \mathbb N\}$ have infinitely many elements in common. So is it enough to just (state that and) add $$=\lim \left(1+\frac{1}{n}\right)^n\cdot \lim \left(1+\frac{1}{n}\right)^n=e\cdot e=e^2$$?",,['limits']
94,Evaluating a limit (involving derivative),Evaluating a limit (involving derivative),,"Evaluate the following limit:  $$\mathop {\lim }\limits_{x \to 1} {\left( {{{f(x)} \over {f(1)}}} \right)^{{1 \over {\log x}}}}$$ My work: $$\eqalign{   & \mathop {\lim }\limits_{x \to 1} f(x) = \mathop {\lim }\limits_{x \to 1} {{f(x) - f(1)} \over {x - 1}} \cdot (x - 1) + f(1) = \mathop {\lim }\limits_{x \to 1} f'(1) \cdot (x - 1) + f(1)  \cr    & \mathop {\lim }\limits_{x \to 1} {\left( {{{f'(1) \cdot (x - 1) + f(1)} \over {f(1)}}} \right)^{{1 \over {\log x}}}} = \exp \left( {\ln \left( {{{f'(1) \cdot (x - 1) + f(1)} \over {f(1)}}} \right) \cdot {1 \over {\log x}}} \right) \cr} $$ Now, I have an expreesion which is essentially $0\cdot \infty$ which is problamatic. I think I'm on the right way, but missing something here. Can you help me? Update: $f$ is differentiable at $x=1$ and $f(1) > 0$","Evaluate the following limit:  $$\mathop {\lim }\limits_{x \to 1} {\left( {{{f(x)} \over {f(1)}}} \right)^{{1 \over {\log x}}}}$$ My work: $$\eqalign{   & \mathop {\lim }\limits_{x \to 1} f(x) = \mathop {\lim }\limits_{x \to 1} {{f(x) - f(1)} \over {x - 1}} \cdot (x - 1) + f(1) = \mathop {\lim }\limits_{x \to 1} f'(1) \cdot (x - 1) + f(1)  \cr    & \mathop {\lim }\limits_{x \to 1} {\left( {{{f'(1) \cdot (x - 1) + f(1)} \over {f(1)}}} \right)^{{1 \over {\log x}}}} = \exp \left( {\ln \left( {{{f'(1) \cdot (x - 1) + f(1)} \over {f(1)}}} \right) \cdot {1 \over {\log x}}} \right) \cr} $$ Now, I have an expreesion which is essentially $0\cdot \infty$ which is problamatic. I think I'm on the right way, but missing something here. Can you help me? Update: $f$ is differentiable at $x=1$ and $f(1) > 0$",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
95,"Understanding an ""obvious"" deduction in analysis","Understanding an ""obvious"" deduction in analysis",,"In Elementary topology , page $365$ reads: Since $\varphi(s)<s$ for $s\in (0,1)$ it follows that the sequence $\varphi^n(s)$ is monotonically decreasing, and we easily see that it tends to $0$ for each $s\in (0,1)$ $($ here $\varphi^n (s)$ denotes $\varphi(\varphi(\cdots \varphi(s)\cdots ),\;n$ times, where $\varphi $ is continuous $(0,1)\to(0,1))$ I don't understand why it is ""easy"" to see that $\varphi^n(s)\to 0$ . What prevents it from converging to some $0<\lambda <1?$ I am guessing continuity will come into play, but I don't see how.","In Elementary topology , page reads: Since for it follows that the sequence is monotonically decreasing, and we easily see that it tends to for each here denotes times, where is continuous I don't understand why it is ""easy"" to see that . What prevents it from converging to some I am guessing continuity will come into play, but I don't see how.","365 \varphi(s)<s s\in (0,1) \varphi^n(s) 0 s\in (0,1) ( \varphi^n (s) \varphi(\varphi(\cdots \varphi(s)\cdots ),\;n \varphi  (0,1)\to(0,1)) \varphi^n(s)\to 0 0<\lambda <1?","['real-analysis', 'limits']"
96,If $(x_n)$ is an unbounded increasing sequence then $\sum (1-x_n/x_{n+1})$ diverges [duplicate],If  is an unbounded increasing sequence then  diverges [duplicate],(x_n) \sum (1-x_n/x_{n+1}),"This question already has answers here : If the positive series $\sum a_n$ diverges and $s_n=\sum\limits_{k\leqslant n}a_k$ then $\sum \frac{a_n}{s_n}$ diverges as well (2 answers) Closed 8 years ago . Let {$x_n$} be monotone increasing sequence of positive real numbers. Show that if {$x_n$} is unbounded, then $\sum_{n=1}^{\infty}(1-\frac{x_n}{x_{n+1}})$ diverges.","This question already has answers here : If the positive series $\sum a_n$ diverges and $s_n=\sum\limits_{k\leqslant n}a_k$ then $\sum \frac{a_n}{s_n}$ diverges as well (2 answers) Closed 8 years ago . Let {$x_n$} be monotone increasing sequence of positive real numbers. Show that if {$x_n$} is unbounded, then $\sum_{n=1}^{\infty}(1-\frac{x_n}{x_{n+1}})$ diverges.",,"['real-analysis', 'limits', 'convergence-divergence']"
97,monotone convergence question,monotone convergence question,,"I am trying to show that  $$\lim_{n \rightarrow \infty} \int^{n^2}_{0}{e^{-x^2}n \sin\frac{x}{n}dx} = \frac{1}{2}.$$ I have tried by using the monotone convergence theorem, but if I take  $f_n = e^{-x^2}n \sin\frac{x}{n}$ on $(0,n^2)$ and $0$ otherwise I can show neither of 1) $f_n \leq f_{n+1}$ a.e 2) $\sup_n \int f_n <\infty$ Is this the right approach or am I missing something?","I am trying to show that  $$\lim_{n \rightarrow \infty} \int^{n^2}_{0}{e^{-x^2}n \sin\frac{x}{n}dx} = \frac{1}{2}.$$ I have tried by using the monotone convergence theorem, but if I take  $f_n = e^{-x^2}n \sin\frac{x}{n}$ on $(0,n^2)$ and $0$ otherwise I can show neither of 1) $f_n \leq f_{n+1}$ a.e 2) $\sup_n \int f_n <\infty$ Is this the right approach or am I missing something?",,"['integration', 'limits', 'lebesgue-integral']"
98,Simple proof of Chain Rule through $\frac{\Delta y}{\Delta x} = \frac{dy}{dx}\biggr|_{x=x_1} + k$,Simple proof of Chain Rule through,\frac{\Delta y}{\Delta x} = \frac{dy}{dx}\biggr|_{x=x_1} + k,"In an online lecture ( link to Youtube ), the professor proves the Chain Rule using the following statement: $$\frac{\Delta y}{\Delta x} = \frac{dy}{dx}\biggr|_{x=x_1} + k$$ $$\Delta y = \frac{dy}{dx}\biggr|_{x=x_1}  \Delta x+ k\Delta x$$ where $$\lim_{\Delta x \to0}k=0.\\[0.4in]$$ Then he says that $x$ and $y$ are functions of a third variable $t$, so: $$\begin{align*} \frac{\Delta y}{\Delta t} &= \frac{dy}{dx}\biggr|_{x=x_1}  \frac{\Delta x}{\Delta t}+ k\frac{\Delta x}{\Delta t}\\[0.2in] \lim_{\Delta t \to 0}\frac{\Delta y}{\Delta t} &=\lim_{\Delta t \to 0} \frac{dy}{dx}\biggr|_{x=x_1} \lim_{\Delta t \to 0}\frac{\Delta x}{\Delta t}+ \lim_{\Delta t \to 0}k \lim_{\Delta t \to 0}\frac{\Delta x}{\Delta t}\\[0.2in] \frac{dy}{dt} &= \frac{dy}{dx} \frac{dx}{dt}+ 0\cdot\frac{dx}{dt} \end{align*}$$ The part I don't understand is, when he says that the last term in the last line, is not $0$ because of $k$ , but also because as $\Delta t \to 0$, $dx$ approaches $0$ as well. But why? I don't understand how $dx$ approaches zero as $dt$ approaches $0$? Shouldn't it be the other way around? That $dx/dt$ should increase without bound as $dt$ approaches $0$? Also, another question... how are we allowed to simply define $\lim_{\Delta x \to0}k=0$? Although it makes sense graphically, but since $k$ is a number, doesn't that violate the fact that $\lim_{\Delta x \to0}c=c$? is it something to do with $\Delta x$?","In an online lecture ( link to Youtube ), the professor proves the Chain Rule using the following statement: $$\frac{\Delta y}{\Delta x} = \frac{dy}{dx}\biggr|_{x=x_1} + k$$ $$\Delta y = \frac{dy}{dx}\biggr|_{x=x_1}  \Delta x+ k\Delta x$$ where $$\lim_{\Delta x \to0}k=0.\\[0.4in]$$ Then he says that $x$ and $y$ are functions of a third variable $t$, so: $$\begin{align*} \frac{\Delta y}{\Delta t} &= \frac{dy}{dx}\biggr|_{x=x_1}  \frac{\Delta x}{\Delta t}+ k\frac{\Delta x}{\Delta t}\\[0.2in] \lim_{\Delta t \to 0}\frac{\Delta y}{\Delta t} &=\lim_{\Delta t \to 0} \frac{dy}{dx}\biggr|_{x=x_1} \lim_{\Delta t \to 0}\frac{\Delta x}{\Delta t}+ \lim_{\Delta t \to 0}k \lim_{\Delta t \to 0}\frac{\Delta x}{\Delta t}\\[0.2in] \frac{dy}{dt} &= \frac{dy}{dx} \frac{dx}{dt}+ 0\cdot\frac{dx}{dt} \end{align*}$$ The part I don't understand is, when he says that the last term in the last line, is not $0$ because of $k$ , but also because as $\Delta t \to 0$, $dx$ approaches $0$ as well. But why? I don't understand how $dx$ approaches zero as $dt$ approaches $0$? Shouldn't it be the other way around? That $dx/dt$ should increase without bound as $dt$ approaches $0$? Also, another question... how are we allowed to simply define $\lim_{\Delta x \to0}k=0$? Although it makes sense graphically, but since $k$ is a number, doesn't that violate the fact that $\lim_{\Delta x \to0}c=c$? is it something to do with $\Delta x$?",,"['calculus', 'limits', 'differential-operators']"
99,How do we find the limit of periodic function as x approaches to infinity $\left(\cos x^{\frac{1}{\sin x}}\right)$,How do we find the limit of periodic function as x approaches to infinity,\left(\cos x^{\frac{1}{\sin x}}\right),"Stuck with a periodic function limit problem: $$\lim_{ x\to \infty}\cos x^{\frac{1}{\sin x}}$$ That's how I tried to solve it: $$\lim_{ x\to \infty}\cos x^{\frac{1}{\sin x}}=\exp \left(\lim_{x \to \infty } \left(\frac{1}{\sin x}\cdot \ln\left(\cos x\right)\right)\right)$$ Now I have to get rid of the fact that x $\to \infty$. So let $u=\frac{1}{x}, u\to 0:$ $$\exp \left(\lim_{u \to 0 } \left(\frac{1}{\sin\frac{1}{u}}\cdot \ln\left(\cos\frac{1}{u}\right)\right)\right)$$ But $\frac{1}{u}=\infty, $ as $ u \to 0$ and we have $sin(\infty)$. And I don't see the way to solve this problem. Any help/hint will be appreciated","Stuck with a periodic function limit problem: $$\lim_{ x\to \infty}\cos x^{\frac{1}{\sin x}}$$ That's how I tried to solve it: $$\lim_{ x\to \infty}\cos x^{\frac{1}{\sin x}}=\exp \left(\lim_{x \to \infty } \left(\frac{1}{\sin x}\cdot \ln\left(\cos x\right)\right)\right)$$ Now I have to get rid of the fact that x $\to \infty$. So let $u=\frac{1}{x}, u\to 0:$ $$\exp \left(\lim_{u \to 0 } \left(\frac{1}{\sin\frac{1}{u}}\cdot \ln\left(\cos\frac{1}{u}\right)\right)\right)$$ But $\frac{1}{u}=\infty, $ as $ u \to 0$ and we have $sin(\infty)$. And I don't see the way to solve this problem. Any help/hint will be appreciated",,"['calculus', 'real-analysis', 'limits', 'trigonometry']"
