,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Determinant of a matrix shifted by m,Determinant of a matrix shifted by m,,"Let $A$ be an $n\times n$ matrix and $Z$ be the $n\times n$ matrix, whose entries are all $m$.  Let $S$ be the sum of all the adjoints of $A$. Then my conjecture is $\det(A+Z)=\det(A)+Sm$ , in particular $\det(A+Z)=\det(A)$ holds if and only if  $S=0$. If the conjecture is true, how can it be proven ? For invertible $A$, sylvester's theorem can be used. $\det(A+Z)=\det(A)\det(I+A^{-1}Z)$  The matrix $A^{-1}Z$ is the product of the row vector containing the row sums of $A^{-1}$  and the column vector $(m,...,m)$. Sylvester's theorem states that the order of the  vectors can be exchanged. The scalar product of the vectors is $0$ if and only if   the sum of the row sums of $A^{-1}$ is $0$ and this is the same as the sum of the   adjoints of $A$ divided by the determinant of $A$. So, for invertible matrices, my  conjecture should hold. But how can I manage the case when $A$ is singular ?","Let $A$ be an $n\times n$ matrix and $Z$ be the $n\times n$ matrix, whose entries are all $m$.  Let $S$ be the sum of all the adjoints of $A$. Then my conjecture is $\det(A+Z)=\det(A)+Sm$ , in particular $\det(A+Z)=\det(A)$ holds if and only if  $S=0$. If the conjecture is true, how can it be proven ? For invertible $A$, sylvester's theorem can be used. $\det(A+Z)=\det(A)\det(I+A^{-1}Z)$  The matrix $A^{-1}Z$ is the product of the row vector containing the row sums of $A^{-1}$  and the column vector $(m,...,m)$. Sylvester's theorem states that the order of the  vectors can be exchanged. The scalar product of the vectors is $0$ if and only if   the sum of the row sums of $A^{-1}$ is $0$ and this is the same as the sum of the   adjoints of $A$ divided by the determinant of $A$. So, for invertible matrices, my  conjecture should hold. But how can I manage the case when $A$ is singular ?",,"['linear-algebra', 'matrices', 'determinant']"
1,$3 \times 3$ real matrix: relation with determinants,real matrix: relation with determinants,3 \times 3,"$A$ is a $3 \times 3$ matrix with real entries such that $\operatorname{det}(A+I_3)=\operatorname{det}(A+2I_3)$. Then is $2\operatorname{det}(A+I_3)+\operatorname{det}(A-I_3)+ 6   =3 \operatorname{det}A$? So, if the first holds, then does the second also hold? That $6$ in the sum makes it somehow ""different"". Any idea / solution?","$A$ is a $3 \times 3$ matrix with real entries such that $\operatorname{det}(A+I_3)=\operatorname{det}(A+2I_3)$. Then is $2\operatorname{det}(A+I_3)+\operatorname{det}(A-I_3)+ 6   =3 \operatorname{det}A$? So, if the first holds, then does the second also hold? That $6$ in the sum makes it somehow ""different"". Any idea / solution?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
2,diagonalization of a matrix over finite fields,diagonalization of a matrix over finite fields,,"I'm having a problem with determine whether a matrix is diagonalizable over $\mathbb F_{2}$, over $\mathbb F_{3}$, etc. for example, for the following matrix: $$        \begin{bmatrix}         1 & 1  & 0 \\          1 & 1  & 0 \\          0 & 0  & 1          \end{bmatrix}$$ the characteristic polynomial is $P(x) = -x^{3}+3x^{2}+2x$  I understand that I need to ""convert"" it to the correct field in some way- but that part I didn't get yet. I'd be happy to get some help with this...","I'm having a problem with determine whether a matrix is diagonalizable over $\mathbb F_{2}$, over $\mathbb F_{3}$, etc. for example, for the following matrix: $$        \begin{bmatrix}         1 & 1  & 0 \\          1 & 1  & 0 \\          0 & 0  & 1          \end{bmatrix}$$ the characteristic polynomial is $P(x) = -x^{3}+3x^{2}+2x$  I understand that I need to ""convert"" it to the correct field in some way- but that part I didn't get yet. I'd be happy to get some help with this...",,"['matrices', 'finite-fields', 'diagonalization']"
3,Matrix with all eigenvalues $0$ but not triangular?,Matrix with all eigenvalues  but not triangular?,0,Is the situation described in the title achievable? I am looking for a $3\times 3$ case specifically.,Is the situation described in the title achievable? I am looking for a $3\times 3$ case specifically.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
4,find an elementary matrix $E$ such that $EA=B$,find an elementary matrix  such that,E EA=B,Let matrix $$A = \begin{pmatrix} 1 & 2 & 0 \\ -3 & 1 & 1 \\ 0 & 4 & 2 \end{pmatrix}  $$ and $$B = \begin{pmatrix} 1 & 2 & 0 \\ 0 & 7 & 1 \\ 0 & 4 & 2 \end{pmatrix}  $$ Find an elementary matrix $E$ such that $EA= B$. I try with many $$E = \begin{pmatrix} -1 & -2 & 0 \\ 1 & 2 & 0 \\ 3 & -1 & 1 \end{pmatrix}  $$ but not correct,Let matrix $$A = \begin{pmatrix} 1 & 2 & 0 \\ -3 & 1 & 1 \\ 0 & 4 & 2 \end{pmatrix}  $$ and $$B = \begin{pmatrix} 1 & 2 & 0 \\ 0 & 7 & 1 \\ 0 & 4 & 2 \end{pmatrix}  $$ Find an elementary matrix $E$ such that $EA= B$. I try with many $$E = \begin{pmatrix} -1 & -2 & 0 \\ 1 & 2 & 0 \\ 3 & -1 & 1 \end{pmatrix}  $$ but not correct,,"['matrices', 'matrix-equations']"
5,"If $Ax = b$ has more than one solution so does $Ax = 0$, where $A$ is $m\times n$ real matrix.","If  has more than one solution so does , where  is  real matrix.",Ax = b Ax = 0 A m\times n,"Problem: If $Ax = b$ has more than one solution so does $Ax = 0$, where $A$ is $m\times n$ real matrix. In the explanation part it is written that when $Ax = b$ is consistent the solution sets of non-homogeneous equation and the homogeneous equation are translates of each other. So, in this case, the two equations have the same number of solutions. I am not able to understand what exactly above explanation says? However, I was thinking that if $Ax = b$ has more than one solution then it must be infinite. Hence there must be at least one free variable. Thus corresponding homogeneous system must be having infinite solutions. Am I thinking correctly? Thanks for the help.","Problem: If $Ax = b$ has more than one solution so does $Ax = 0$, where $A$ is $m\times n$ real matrix. In the explanation part it is written that when $Ax = b$ is consistent the solution sets of non-homogeneous equation and the homogeneous equation are translates of each other. So, in this case, the two equations have the same number of solutions. I am not able to understand what exactly above explanation says? However, I was thinking that if $Ax = b$ has more than one solution then it must be infinite. Hence there must be at least one free variable. Thus corresponding homogeneous system must be having infinite solutions. Am I thinking correctly? Thanks for the help.",,"['linear-algebra', 'matrices']"
6,"If $A$, $B$, $A-B$ and $I+A$ are invertible $n×n$ matrices then prove the following","If , ,  and  are invertible  matrices then prove the following",A B A-B I+A n×n,i. $(A-B)^{-1} = A^{-1} + A^{-1}(B^{-1} - A^{-1})^{-1}$ ii. $(I+A)^{-1} = I-(A^{-1} + I)^{-1}$ iii. $tr((I+A)^{-1}) + tr((A^{-1} + I)^{-1}) = n$ I'm stuck on these. So far I only thought about taking the second half of each equation and try to find the first half but I had no success.,i. $(A-B)^{-1} = A^{-1} + A^{-1}(B^{-1} - A^{-1})^{-1}$ ii. $(I+A)^{-1} = I-(A^{-1} + I)^{-1}$ iii. $tr((I+A)^{-1}) + tr((A^{-1} + I)^{-1}) = n$ I'm stuck on these. So far I only thought about taking the second half of each equation and try to find the first half but I had no success.,,"['linear-algebra', 'matrices']"
7,Confused about a linear equation,Confused about a linear equation,,"So I am working through some notes on Linear Algebra and I cant seem to follow this one part. The question asks to  Solve: $x+y-z+2w=-20$ $2x-y+z+w=11$ $3x-2y+z-2w+27$ I don't have a problem with putting the equation into matrix form and even reducing it. The way the notes explains it is what I don't understand at all. So first once its in echelon, $$ \begin{bmatrix}  1 & 1 & -1 & 2 & -20\\  0 & 1 & -1 & 1 & -17\\  0 & 0 & 1 & 3 &-2\\  \end{bmatrix}  $$the notes states: ""The solution is therefore $$ X=\begin{pmatrix} -w-3\\ -4w-19\\ -3w-2\\ 2\\ \end{pmatrix} $$ (This is the first place I am confused? Why are there $4$ rows now? And how did they get these numbers?). Then, we can reduce and have  $$ \begin{bmatrix} 1&0&0&1&-3\\ 0&1&0&4&-19\\ 0&0&1&3&-2 \end{bmatrix} $$ Now for the second confusing part, it then states: ""Note, the solution is $$ X=\begin{pmatrix} -3\\ -19\\ -2\\ 0\\ \end{pmatrix} +w\begin{pmatrix} -1\\ -4\\ -3\\ 1\\ \end{pmatrix} $$ Note that the rank is $3$ and there is $1$ parameter. Thanks a lot for the help guys. Hopefully I can understand it.","So I am working through some notes on Linear Algebra and I cant seem to follow this one part. The question asks to  Solve: $x+y-z+2w=-20$ $2x-y+z+w=11$ $3x-2y+z-2w+27$ I don't have a problem with putting the equation into matrix form and even reducing it. The way the notes explains it is what I don't understand at all. So first once its in echelon, $$ \begin{bmatrix}  1 & 1 & -1 & 2 & -20\\  0 & 1 & -1 & 1 & -17\\  0 & 0 & 1 & 3 &-2\\  \end{bmatrix}  $$the notes states: ""The solution is therefore $$ X=\begin{pmatrix} -w-3\\ -4w-19\\ -3w-2\\ 2\\ \end{pmatrix} $$ (This is the first place I am confused? Why are there $4$ rows now? And how did they get these numbers?). Then, we can reduce and have  $$ \begin{bmatrix} 1&0&0&1&-3\\ 0&1&0&4&-19\\ 0&0&1&3&-2 \end{bmatrix} $$ Now for the second confusing part, it then states: ""Note, the solution is $$ X=\begin{pmatrix} -3\\ -19\\ -2\\ 0\\ \end{pmatrix} +w\begin{pmatrix} -1\\ -4\\ -3\\ 1\\ \end{pmatrix} $$ Note that the rank is $3$ and there is $1$ parameter. Thanks a lot for the help guys. Hopefully I can understand it.",,"['linear-algebra', 'matrices', 'matrix-equations']"
8,How to find matrix $A$ given $Ax=b$. Also $det(A)$ & $sum(A)$ are known. [duplicate],How to find matrix  given . Also  &  are known. [duplicate],A Ax=b det(A) sum(A),"This question already has an answer here : Solving for $A$ in $Ax = b$ (1 answer) Closed 10 years ago . Here is an example: $A = \begin{bmatrix} 2 & 3 \\ 5 & 1 \end{bmatrix}$ $x = \begin{bmatrix} 3 \\ 8 \end{bmatrix}$ $b = Ax$ so $b = \begin{bmatrix} 30 \\ 23 \end{bmatrix}$ Now i want to find $A$ using $x$ and $b$ matrices. How can i do that ? Some says there is a way to invert a non-square matrix (something like right and left) so i can find $A$, but i dunno how to do that. Additional : What if i had $det(A)$ and sum of entries in $A$ (which is $-13$ & $11$ in this case).","This question already has an answer here : Solving for $A$ in $Ax = b$ (1 answer) Closed 10 years ago . Here is an example: $A = \begin{bmatrix} 2 & 3 \\ 5 & 1 \end{bmatrix}$ $x = \begin{bmatrix} 3 \\ 8 \end{bmatrix}$ $b = Ax$ so $b = \begin{bmatrix} 30 \\ 23 \end{bmatrix}$ Now i want to find $A$ using $x$ and $b$ matrices. How can i do that ? Some says there is a way to invert a non-square matrix (something like right and left) so i can find $A$, but i dunno how to do that. Additional : What if i had $det(A)$ and sum of entries in $A$ (which is $-13$ & $11$ in this case).",,['matrices']
9,find all values of k for which A is invertible,find all values of k for which A is invertible,,$\begin{bmatrix} k &k  &0 \\  k^2 &2  &k \\   0& k & k \end{bmatrix}$ what I did is find the det first:  $$\det= k(2k-k^2)-k(k^3-0)-0(k^3 -0)=2k^2-k^3-k^4$$ when $det = 0$  the matrix isn't invertible $$2k^2-k^3-k^4=0$$ $$k^2(k^2 +k-2)=0$$ $$k^2+k-2=0$$ $$(k+2)(k-1)=0$$ $k = -2$ or $k = 1$. I am lost here how to find the value for k when the matrix is invertible.,$\begin{bmatrix} k &k  &0 \\  k^2 &2  &k \\   0& k & k \end{bmatrix}$ what I did is find the det first:  $$\det= k(2k-k^2)-k(k^3-0)-0(k^3 -0)=2k^2-k^3-k^4$$ when $det = 0$  the matrix isn't invertible $$2k^2-k^3-k^4=0$$ $$k^2(k^2 +k-2)=0$$ $$k^2+k-2=0$$ $$(k+2)(k-1)=0$$ $k = -2$ or $k = 1$. I am lost here how to find the value for k when the matrix is invertible.,,"['linear-algebra', 'matrices']"
10,Diagonalisability…without the characteristic polynomial,Diagonalisability…without the characteristic polynomial,,Let us consider an $n\times n$ matrix  $A$  defined as follows $$ A=\begin{pmatrix} 1+a&1&\cdots &1\\ 1&1+a&\ddots&\vdots\\ \vdots&\ddots&\ddots&1\\ 1&\cdots&1&1+a \end{pmatrix} $$ is diagonalizable without using the characteristic polynomial of $A$. My matrix contains $1+a$ ($a$ is a real number) along the diagonal and $1$ elsewhere. I just noticed that $a$ is   eigenvalue but I don't know how to conclude with that.. Thanks.,Let us consider an $n\times n$ matrix  $A$  defined as follows $$ A=\begin{pmatrix} 1+a&1&\cdots &1\\ 1&1+a&\ddots&\vdots\\ \vdots&\ddots&\ddots&1\\ 1&\cdots&1&1+a \end{pmatrix} $$ is diagonalizable without using the characteristic polynomial of $A$. My matrix contains $1+a$ ($a$ is a real number) along the diagonal and $1$ elsewhere. I just noticed that $a$ is   eigenvalue but I don't know how to conclude with that.. Thanks.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
11,How to estimate the lower bound of a given Toeplitz matrix's eigenvalue?,How to estimate the lower bound of a given Toeplitz matrix's eigenvalue?,,"Given the Toeplitz matrix $$\begin{pmatrix}   1 & a & a^2 & \cdots & a^n  \\     a &1 &a & \cdots & a^{n-1} \\     a^2&a & 1 & \cdots& a^{n-2} \\     \vdots &   \vdots  &   \vdots & \ddots &   \vdots  \\     a^n & a^{n-1} & a^{n-2} & \cdots & 1\\   \end{pmatrix}$$ where $a \in (0,1)$ , can one find the eigenvalues of the matrix? If not, can one find a lower bound? Any links or reference materials? Thanks.","Given the Toeplitz matrix where , can one find the eigenvalues of the matrix? If not, can one find a lower bound? Any links or reference materials? Thanks.","\begin{pmatrix}
  1 & a & a^2 & \cdots & a^n  \\
    a &1 &a & \cdots & a^{n-1} \\
    a^2&a & 1 & \cdots& a^{n-2} \\
    \vdots &   \vdots  &   \vdots & \ddots &   \vdots  \\
    a^n & a^{n-1} & a^{n-2} & \cdots & 1\\
  \end{pmatrix} a \in (0,1)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'estimation', 'toeplitz-matrices']"
12,Number of 3x3 matrices with determinant $1$ and coefficients in $\mathbb{Z}_5$,Number of 3x3 matrices with determinant  and coefficients in,1 \mathbb{Z}_5,"Let $M=(m_{ij}), m_{ij} \in \mathbb{Z}_5$. $det(M) \in \{0,1,2,3,4\}$. There are equal number of matrices with determinants $1,2,3$ and $4$, because determinant is multiplied by $2$ when one of rows is doubled, and is multiplied by $-1$ when two rows are swapped. The only question is the number of singular matrices. I have written a Python script and found that: There are 465125 3x3 matrices in GF(5) with det=0 There are 372000 3x3 matrices in GF(5) with det=1 There are 372000 3x3 matrices in GF(5) with det=2 There are 372000 3x3 matrices in GF(5) with det=3 There are 372000 3x3 matrices in GF(5) with det=4 Can you explain why the number of singular matrices is $465125$?","Let $M=(m_{ij}), m_{ij} \in \mathbb{Z}_5$. $det(M) \in \{0,1,2,3,4\}$. There are equal number of matrices with determinants $1,2,3$ and $4$, because determinant is multiplied by $2$ when one of rows is doubled, and is multiplied by $-1$ when two rows are swapped. The only question is the number of singular matrices. I have written a Python script and found that: There are 465125 3x3 matrices in GF(5) with det=0 There are 372000 3x3 matrices in GF(5) with det=1 There are 372000 3x3 matrices in GF(5) with det=2 There are 372000 3x3 matrices in GF(5) with det=3 There are 372000 3x3 matrices in GF(5) with det=4 Can you explain why the number of singular matrices is $465125$?",,"['linear-algebra', 'matrices', 'elementary-number-theory']"
13,Gauss-Newton method -- is this matrix product invertible?,Gauss-Newton method -- is this matrix product invertible?,,"In the Gauss-Newton method for solving overdetermined systems of equations,  the iteration matrix is of the form $(J^t   J)^{-1}   J^t$, for a $m \times n$ Jacobian matrix $J$ with $m > n$.   I was under the impression that if J had full column rank, then the product $J^t   J$ would always be invertible.   Is this incorrect?","In the Gauss-Newton method for solving overdetermined systems of equations,  the iteration matrix is of the form $(J^t   J)^{-1}   J^t$, for a $m \times n$ Jacobian matrix $J$ with $m > n$.   I was under the impression that if J had full column rank, then the product $J^t   J$ would always be invertible.   Is this incorrect?",,"['matrices', 'numerical-methods']"
14,inverse of an infinite matrix,inverse of an infinite matrix,,How to find inverse of an infinite lower triangular matrix all of whose diagonal entries are 1 and the entries of each column are given by coefficients of some power series rings?,How to find inverse of an infinite lower triangular matrix all of whose diagonal entries are 1 and the entries of each column are given by coefficients of some power series rings?,,"['linear-algebra', 'matrices', 'matrix-calculus']"
15,How to get an eigenvector of a $3\times 3$ matrix that has first column and a row of zeros,How to get an eigenvector of a  matrix that has first column and a row of zeros,3\times 3,"I have the following matrix $$ \begin{bmatrix} 1& 0& 0\\ 0& 1& 1\\ 0& 1& 1 \end{bmatrix} $$ First I got the eigenvalues which are $0$, $1$, $2$. I tried to get the eigenvectors associated with the above eigenvalues  but I cannot in case of the eigenvalue $1$ as I got the following matrix $$ \begin{bmatrix}     0& 0& 0\\     0& 0& 1\\     0& 1& 0 \end{bmatrix} $$ So, how can I get an eigenvector for this matrix?","I have the following matrix $$ \begin{bmatrix} 1& 0& 0\\ 0& 1& 1\\ 0& 1& 1 \end{bmatrix} $$ First I got the eigenvalues which are $0$, $1$, $2$. I tried to get the eigenvectors associated with the above eigenvalues  but I cannot in case of the eigenvalue $1$ as I got the following matrix $$ \begin{bmatrix}     0& 0& 0\\     0& 0& 1\\     0& 1& 0 \end{bmatrix} $$ So, how can I get an eigenvector for this matrix?",,"['matrices', 'eigenvalues-eigenvectors']"
16,How are specific linear maps defined?,How are specific linear maps defined?,,"I'm revising for exams and a question that often crops up is: given a linear map $\mathcal{T}:\;\mathbb{R}^n\to\mathbb{R}^m$, describe how to represent $\mathcal{T}$ as a matrix relative to bases $\mathfrak{B}_n,\mathfrak{B}_m$ of $\mathbb{R}^n,\mathbb{R}^m$. What confuses me is how $\mathcal{T}$ can be defined in the first place without referring to bases. Is it implied that $\mathcal{T}$ is defined under the standard bases? Or at least under some bases? (if so then I know how to answer the question)","I'm revising for exams and a question that often crops up is: given a linear map $\mathcal{T}:\;\mathbb{R}^n\to\mathbb{R}^m$, describe how to represent $\mathcal{T}$ as a matrix relative to bases $\mathfrak{B}_n,\mathfrak{B}_m$ of $\mathbb{R}^n,\mathbb{R}^m$. What confuses me is how $\mathcal{T}$ can be defined in the first place without referring to bases. Is it implied that $\mathcal{T}$ is defined under the standard bases? Or at least under some bases? (if so then I know how to answer the question)",,"['linear-algebra', 'matrices']"
17,Cholesky Decomposition for positive semidefinite separation,Cholesky Decomposition for positive semidefinite separation,,"Cholesky decomposition is a common way to test positive semi definiteness of a symmetric matrix $A$. If the algorithm ""goes wrong"" trying to take a square root of a negative number, I know the matrix isn't PD. How can I use this algorithm (any of the ways to represent it) to find a vector $x$ such that $x^TAx < 0$?","Cholesky decomposition is a common way to test positive semi definiteness of a symmetric matrix $A$. If the algorithm ""goes wrong"" trying to take a square root of a negative number, I know the matrix isn't PD. How can I use this algorithm (any of the ways to represent it) to find a vector $x$ such that $x^TAx < 0$?",,"['matrices', 'numerical-linear-algebra']"
18,Why do we define addition of matrices only when they have the same size,Why do we define addition of matrices only when they have the same size,,What happens if we define $$ \begin{pmatrix}         1 & 2 \\ 	1 & 2 \\         1 & 2          \end{pmatrix} + \begin{pmatrix}         1 & 2 & 3 \\ 	1 & 2 & 3 \\         1 & 2 & 3         \end{pmatrix} = \begin{pmatrix}         2 & 4 & 3 \\ 	2 & 4 & 3 \\         2 & 4 & 3         \end{pmatrix} $$ I think computers do this to refresh a part of the screen. But why can't we do it by matrices by filling the remaining rows/columns with zero? Is it related to the definition of vector spaces?,What happens if we define $$ \begin{pmatrix}         1 & 2 \\ 	1 & 2 \\         1 & 2          \end{pmatrix} + \begin{pmatrix}         1 & 2 & 3 \\ 	1 & 2 & 3 \\         1 & 2 & 3         \end{pmatrix} = \begin{pmatrix}         2 & 4 & 3 \\ 	2 & 4 & 3 \\         2 & 4 & 3         \end{pmatrix} $$ I think computers do this to refresh a part of the screen. But why can't we do it by matrices by filling the remaining rows/columns with zero? Is it related to the definition of vector spaces?,,"['linear-algebra', 'matrices']"
19,Nilpotent operator of index $n$,Nilpotent operator of index,n,"Let $T: \mathbb R^n \to \mathbb R^n$ be a linear operator such that $T^{n-1} \neq 0$ but $T^n = 0$. Prove that $\text{rank}(T)=n-1$ and give an example of such operator. PS. This was on a homework, I searched a lot but couldn't find the solution/hint. The point is the problem can be solved in an elementary way (i.e. no use of characteristic polynomials, eigenvalues, etc.). I tried $\text{rank}(T) + \text{nullity}(T) = n$ which gives $\text{nullity}(T)=1$, but no results...","Let $T: \mathbb R^n \to \mathbb R^n$ be a linear operator such that $T^{n-1} \neq 0$ but $T^n = 0$. Prove that $\text{rank}(T)=n-1$ and give an example of such operator. PS. This was on a homework, I searched a lot but couldn't find the solution/hint. The point is the problem can be solved in an elementary way (i.e. no use of characteristic polynomials, eigenvalues, etc.). I tried $\text{rank}(T) + \text{nullity}(T) = n$ which gives $\text{nullity}(T)=1$, but no results...",,"['linear-algebra', 'matrices']"
20,Jordan form example clarification,Jordan form example clarification,,"I am practicing Jordan forms and came across the following example: $$A=\left(\begin{array}{rrr} 1&1&1\\-1&-1&-1\\1&1&1\end{array}\right).$$  Jordan canonical form is $J=\begin{pmatrix}0&0&0\\0&0&0\\0&0&1\end{pmatrix}$. My question is shouldn't $J=\begin{pmatrix}0&1&0\\0&0&0\\0&0&1\end{pmatrix}$ be the Jordan form? I computed the eigenvalues which are 0 (multiplicity 2) and 1 (multiplicity 1). Now because of multiplicity 1 the block containing eigenvalue 1 is $1\times1$ matrix. With 0 we find the eigenvectors to be $E_1=\{\begin{pmatrix}-1\\1\\0\end{pmatrix}, \begin{pmatrix}-1\\0\\1\end{pmatrix}\}$. Since the dimension is 2 my Jordan block will have $2\times2$ block with diagonal entries 0 and thus we get the second form. Where am I making a mistake?","I am practicing Jordan forms and came across the following example: $$A=\left(\begin{array}{rrr} 1&1&1\\-1&-1&-1\\1&1&1\end{array}\right).$$  Jordan canonical form is $J=\begin{pmatrix}0&0&0\\0&0&0\\0&0&1\end{pmatrix}$. My question is shouldn't $J=\begin{pmatrix}0&1&0\\0&0&0\\0&0&1\end{pmatrix}$ be the Jordan form? I computed the eigenvalues which are 0 (multiplicity 2) and 1 (multiplicity 1). Now because of multiplicity 1 the block containing eigenvalue 1 is $1\times1$ matrix. With 0 we find the eigenvectors to be $E_1=\{\begin{pmatrix}-1\\1\\0\end{pmatrix}, \begin{pmatrix}-1\\0\\1\end{pmatrix}\}$. Since the dimension is 2 my Jordan block will have $2\times2$ block with diagonal entries 0 and thus we get the second form. Where am I making a mistake?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
21,"If $F\subseteq\mathrm{Mat}_n(\mathbb{Q})$, then $[F:\mathbb Q]\leq n$?","If , then ?",F\subseteq\mathrm{Mat}_n(\mathbb{Q}) [F:\mathbb Q]\leq n,Let $F$ be a field contained in the ring of $n\times n$ matrices over $\mathbb Q$. Prove that $[F:\mathbb Q]\leq n$. I have an idea to consider a degree $n$ extension $K$ of $\mathbb Q$ and left multiplication of elements of $K$ by themselves is a $\mathbb Q$-linear transform so we get an isomorphic copy of $K$ and hence has degree $n$ over $\mathbb Q$ in matrix ring... but why don't we have larger subfields?,Let $F$ be a field contained in the ring of $n\times n$ matrices over $\mathbb Q$. Prove that $[F:\mathbb Q]\leq n$. I have an idea to consider a degree $n$ extension $K$ of $\mathbb Q$ and left multiplication of elements of $K$ by themselves is a $\mathbb Q$-linear transform so we get an isomorphic copy of $K$ and hence has degree $n$ over $\mathbb Q$ in matrix ring... but why don't we have larger subfields?,,"['abstract-algebra', 'matrices', 'field-theory']"
22,A is Mn×n(C) with rank r and m(t) is the minimal polynomial of A. Prove deg $m(t) \leq r+1$,A is Mn×n(C) with rank r and m(t) is the minimal polynomial of A. Prove deg,m(t) \leq r+1,"$A$ is a matrix of $M_{n \times n}(\mathbb{C})$ with rank $r$ and $m(t)$ is the minimal polynomial of A. I need to prove that : deg $m(t) \leq r+1$ I need to find a condition of the matrix A, in which deg $m(t) = r+1$ Can anyone help me ? The solution involves the primary decomposition theorem for matrices and Jordan form..","$A$ is a matrix of $M_{n \times n}(\mathbb{C})$ with rank $r$ and $m(t)$ is the minimal polynomial of A. I need to prove that : deg $m(t) \leq r+1$ I need to find a condition of the matrix A, in which deg $m(t) = r+1$ Can anyone help me ? The solution involves the primary decomposition theorem for matrices and Jordan form..",,"['linear-algebra', 'matrices', 'polynomials']"
23,Unexpected Lower Triangular Matrix given by Matlab,Unexpected Lower Triangular Matrix given by Matlab,,"I was trying to find the lower triangular matrix L and upper triangular matrix U of an A matrix using the [L, U]= lu(A) command. For some reason, matlab won't give me a proper L matrix. Below are my original A matrix as well as the L matrix and U matrix produced by matlab A = [4 2 -5 1;-8 0 9 7;-32 -4 43 18;24 4 -22 -8] L = [-0.125 1 0 0;0.25 0.6 -0.2 1;1 0 0 0;-0.75 0.6 1 0] U = [-32 -4 43 18;0 1.5 0.375 3.25;0 0 10 3.33;0 0 0 1] What exactly is going on here?","I was trying to find the lower triangular matrix L and upper triangular matrix U of an A matrix using the [L, U]= lu(A) command. For some reason, matlab won't give me a proper L matrix. Below are my original A matrix as well as the L matrix and U matrix produced by matlab A = [4 2 -5 1;-8 0 9 7;-32 -4 43 18;24 4 -22 -8] L = [-0.125 1 0 0;0.25 0.6 -0.2 1;1 0 0 0;-0.75 0.6 1 0] U = [-32 -4 43 18;0 1.5 0.375 3.25;0 0 10 3.33;0 0 0 1] What exactly is going on here?",,"['matrices', 'matlab']"
24,"Given two symmetric, real, positive definite matrices, $A,B$ such that $AB=BA$, prove that $AB$ is positive definite.","Given two symmetric, real, positive definite matrices,  such that , prove that  is positive definite.","A,B AB=BA AB","So far: The commutativity of the matrices gives me simultaneous diagonalization, the fact that they're symmetric tells me that they're simultaneously diagonizable by a orthogonal matrix. The fact that they're positive definite means that their eigenvalues are all positive, so that means that the eigenvalues of AB are also positive definite. Am I going in the right direction? What am I missing here? How do I formally prove this? Thanks for your time.","So far: The commutativity of the matrices gives me simultaneous diagonalization, the fact that they're symmetric tells me that they're simultaneously diagonizable by a orthogonal matrix. The fact that they're positive definite means that their eigenvalues are all positive, so that means that the eigenvalues of AB are also positive definite. Am I going in the right direction? What am I missing here? How do I formally prove this? Thanks for your time.",,"['linear-algebra', 'matrices']"
25,Is this determinant identity correct?,Is this determinant identity correct?,,"For complex valued matrices $A,B$ where $B$ is invertible, does $$\det(I+B^{-1}AA^*)=\det(I+AA^*B^{-1})=\det(I+AB^{-1}A^*)=\det(I+A^*B^{-1}A)?$$ Here $A^*$ is the conjugate transform. I guess $\det(I+B^{-1}AA^*)=\det(I+AA^*B^{-1})$ holds by Sylvester's identity . Correction $$\det(I+B^{-1}AA^*)=\det(I+AA^*B^{-1})=\det(I+A^*B^{-1}A)?$$","For complex valued matrices $A,B$ where $B$ is invertible, does $$\det(I+B^{-1}AA^*)=\det(I+AA^*B^{-1})=\det(I+AB^{-1}A^*)=\det(I+A^*B^{-1}A)?$$ Here $A^*$ is the conjugate transform. I guess $\det(I+B^{-1}AA^*)=\det(I+AA^*B^{-1})$ holds by Sylvester's identity . Correction $$\det(I+B^{-1}AA^*)=\det(I+AA^*B^{-1})=\det(I+A^*B^{-1}A)?$$",,"['matrices', 'determinant']"
26,Multiplication of 3 matrices - Index vs. Matrix notation,Multiplication of 3 matrices - Index vs. Matrix notation,,"I'm having a problem multiplicating 3 matrices in index notation. I know this should be trivial but I just can't figure it out. Is there any formula like $$\ A'_{\mu\nu} =  M_{\mu}^{\ \rho}(M^{-1})_{\nu}^{\ \theta}A_{\rho\theta }$$ $\rightarrow$ $A'=MAM  $ (M is diagonal if this changes anything and summation over identical indices is assumed (Einstein summation convention).) For converting between matrix and index notation?, As far as i know the following relation holds $\ A'_{\mu \nu} =  M_{\mu}^{\ \rho}M_{\nu}^{\ \theta}A_{\rho\theta }$ $\rightarrow$ $A'=MAM^{-1}  $ as physicists use it in special relativity, but I can't derive this formula either because I just can't figure out where the inverse matrix comes into play. I of course already wrote down specific examples of the sum on the left hand side and the matrix multiplication on the right hand side, but it always seems to work without the inverse. Any help, tip or link where the equivalence is shown explicitly will be much much appreciated because I'm stuck with this problem for quite a while.","I'm having a problem multiplicating 3 matrices in index notation. I know this should be trivial but I just can't figure it out. Is there any formula like (M is diagonal if this changes anything and summation over identical indices is assumed (Einstein summation convention).) For converting between matrix and index notation?, As far as i know the following relation holds as physicists use it in special relativity, but I can't derive this formula either because I just can't figure out where the inverse matrix comes into play. I of course already wrote down specific examples of the sum on the left hand side and the matrix multiplication on the right hand side, but it always seems to work without the inverse. Any help, tip or link where the equivalence is shown explicitly will be much much appreciated because I'm stuck with this problem for quite a while.",\ A'_{\mu\nu} =  M_{\mu}^{\ \rho}(M^{-1})_{\nu}^{\ \theta}A_{\rho\theta } \rightarrow A'=MAM   \ A'_{\mu \nu} =  M_{\mu}^{\ \rho}M_{\nu}^{\ \theta}A_{\rho\theta } \rightarrow A'=MAM^{-1}  ,"['linear-algebra', 'matrices', 'tensors']"
27,Matrix Exponent,Matrix Exponent,,"I know that the exponential function for an $n\times n$ non-degenerate square matrix $A$ is: $$e^{At}=S \operatorname{diag}(e^{\lambda_1t},\dots,e^{\lambda_nt}) \space S^{-1}, $$ where $S$ is the eigenvector matrix and $\lambda_i$ are its eigenvalues. What is $e^{A}$? Is it $$e^{A}=S \operatorname{diag}(e^{\lambda_1},\dots,e^{\lambda_n}) \space S^{-1}?$$ Also, what is $\cos(A)$?","I know that the exponential function for an $n\times n$ non-degenerate square matrix $A$ is: $$e^{At}=S \operatorname{diag}(e^{\lambda_1t},\dots,e^{\lambda_nt}) \space S^{-1}, $$ where $S$ is the eigenvector matrix and $\lambda_i$ are its eigenvalues. What is $e^{A}$? Is it $$e^{A}=S \operatorname{diag}(e^{\lambda_1},\dots,e^{\lambda_n}) \space S^{-1}?$$ Also, what is $\cos(A)$?",,"['linear-algebra', 'matrices']"
28,Suppose $Df(z)^TDf(z) = \lambda(z)I$. Show $f(z)$ is holomorphic or $\overline{f(z)}$ is holomorphic,Suppose . Show  is holomorphic or  is holomorphic,Df(z)^TDf(z) = \lambda(z)I f(z) \overline{f(z)},"From an old qualifier: Let $z=x+iy$, $f=f(z)=u+iv$. Assume $\Omega$ is an open connected   domain in $\mathbb{C}$, $f\in C^2(\Omega)$. Denote $$Df = \left[   \begin{matrix} \frac{\partial u}{\partial x} & \frac{\partial  u}{\partial y} \\ \frac{\partial v}{\partial x} & \frac{\partial  v}{\partial y} \end{matrix} \right].$$ Suppose for every $z\in  \Omega$, $$Df(z)^TDf(z) = \lambda(z)I$$ for some $\lambda(z)$, where   $I$ is the 2x2 identity matrix. Then show that either $f(z)$ is   holomorphic or $\overline{f(z)}$ is holomorphic. Ideas: We get $$\left( \frac{\partial u}{\partial x} \right)^2 + \left( \frac{\partial v}{\partial x} \right)^2 = \left( \frac{\partial u}{\partial y} \right)^2 + \left( \frac{\partial v}{\partial y} \right)^2 \quad \text{and}\tag{1}$$ $$\frac{\partial u}{\partial x}\frac{\partial u}{\partial y} + \frac{\partial v}{\partial x}\frac{\partial v}{\partial y} =0\tag{2}.$$ What I've been doing is differentiating (1) and (2) with respect to $x$ and $y$ and trying to cancel. One thing I ended up with is $$\Delta u \left(\frac{\partial u}{\partial x} - \frac{\partial u}{\partial y}\right) + \Delta v \left(\frac{\partial v}{\partial x} - \frac{\partial v}{\partial y}\right)=0.$$ But I need to show relations between $\partial u/\partial x$ and $\partial v/\partial y$ etc.","From an old qualifier: Let $z=x+iy$, $f=f(z)=u+iv$. Assume $\Omega$ is an open connected   domain in $\mathbb{C}$, $f\in C^2(\Omega)$. Denote $$Df = \left[   \begin{matrix} \frac{\partial u}{\partial x} & \frac{\partial  u}{\partial y} \\ \frac{\partial v}{\partial x} & \frac{\partial  v}{\partial y} \end{matrix} \right].$$ Suppose for every $z\in  \Omega$, $$Df(z)^TDf(z) = \lambda(z)I$$ for some $\lambda(z)$, where   $I$ is the 2x2 identity matrix. Then show that either $f(z)$ is   holomorphic or $\overline{f(z)}$ is holomorphic. Ideas: We get $$\left( \frac{\partial u}{\partial x} \right)^2 + \left( \frac{\partial v}{\partial x} \right)^2 = \left( \frac{\partial u}{\partial y} \right)^2 + \left( \frac{\partial v}{\partial y} \right)^2 \quad \text{and}\tag{1}$$ $$\frac{\partial u}{\partial x}\frac{\partial u}{\partial y} + \frac{\partial v}{\partial x}\frac{\partial v}{\partial y} =0\tag{2}.$$ What I've been doing is differentiating (1) and (2) with respect to $x$ and $y$ and trying to cancel. One thing I ended up with is $$\Delta u \left(\frac{\partial u}{\partial x} - \frac{\partial u}{\partial y}\right) + \Delta v \left(\frac{\partial v}{\partial x} - \frac{\partial v}{\partial y}\right)=0.$$ But I need to show relations between $\partial u/\partial x$ and $\partial v/\partial y$ etc.",,"['complex-analysis', 'matrices']"
29,Partition of a complex matrix,Partition of a complex matrix,,"Given $A\in \Bbb M(n,\Bbb C)$,I was asked to show that there exist two polynomial $g$ and $h$ with both constant term vanished,such that $g(A)$ is diagonalizable ,$h(A)$ is nilpotent and $A=g(A)+h(A)$ Well...I got totally confused,especially with requirement that constant term vanish.Why do they have to be so?Is $g$,$h$ related to the characteristic polynomial of $A$?","Given $A\in \Bbb M(n,\Bbb C)$,I was asked to show that there exist two polynomial $g$ and $h$ with both constant term vanished,such that $g(A)$ is diagonalizable ,$h(A)$ is nilpotent and $A=g(A)+h(A)$ Well...I got totally confused,especially with requirement that constant term vanish.Why do they have to be so?Is $g$,$h$ related to the characteristic polynomial of $A$?",,"['linear-algebra', 'matrices']"
30,Linear transformation whose $n$th power is identity,Linear transformation whose th power is identity,n,"Let $V$ be a vector space over field $F$ with $\dim_FV=2$. Suppose $T:V\longrightarrow V$ is a linear transformation with $T^n=Id$ for some positive integer $n$ (the finite $n$ is the order of $T$). (i) If $F=\mathbb{Q}$, prove that $n\leq 6$. (ii) For any integer $N$, prove that there exists a finite field $F$ with a non-diagonalizable $T$ of order larger than $N$. I have totally no way to solve this question. Do not know where to start...","Let $V$ be a vector space over field $F$ with $\dim_FV=2$. Suppose $T:V\longrightarrow V$ is a linear transformation with $T^n=Id$ for some positive integer $n$ (the finite $n$ is the order of $T$). (i) If $F=\mathbb{Q}$, prove that $n\leq 6$. (ii) For any integer $N$, prove that there exists a finite field $F$ with a non-diagonalizable $T$ of order larger than $N$. I have totally no way to solve this question. Do not know where to start...",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'matrices', 'field-theory']"
31,"Interesting determinant: Let $A$ be an $n$ by $n$ matrix with entries $a_{i,j}$ given that $a_{i,j}=2$ if $i=j$",Interesting determinant: Let  be an  by  matrix with entries  given that  if,"A n n a_{i,j} a_{i,j}=2 i=j","Let $A$ be an $n$ by $n$ matrix with entries $a_{i,j}$ given that $a_{i,j}=2$ if $i=j$, $a_{i,j}=1$ if $i-j\equiv\pm2\pmod n$, and $a_{i,j}=0$ otherwise. Find $\det A$. It seems that the determinant is 4 for all odd $n$. What about even $n$?","Let $A$ be an $n$ by $n$ matrix with entries $a_{i,j}$ given that $a_{i,j}=2$ if $i=j$, $a_{i,j}=1$ if $i-j\equiv\pm2\pmod n$, and $a_{i,j}=0$ otherwise. Find $\det A$. It seems that the determinant is 4 for all odd $n$. What about even $n$?",,"['linear-algebra', 'matrices', 'contest-math', 'determinant', 'numerical-linear-algebra']"
32,Are the eigenvectors of a power matrix A^k the eigenvectors of the matrix A?,Are the eigenvectors of a power matrix A^k the eigenvectors of the matrix A?,,"If $x\in\mathbb C^n$ is an eigenvector of $B\in\Bbb C^{n\times n}$ and $B=A^k$ for a certain $k\in\Bbb N$, is $x$ an eigenvector of $A\in\Bbb C^{n\times n}$?","If $x\in\mathbb C^n$ is an eigenvector of $B\in\Bbb C^{n\times n}$ and $B=A^k$ for a certain $k\in\Bbb N$, is $x$ an eigenvector of $A\in\Bbb C^{n\times n}$?",,"['matrices', 'eigenvalues-eigenvectors']"
33,Are there matrices with with non-real elements?,Are there matrices with with non-real elements?,,"I know that the definition of a matrix is a rectangular arrangement of elements, which are real numbers. But does there exist such a thing as a rectangular arrangement of complex numbers? How are these matrices called, if they exist?","I know that the definition of a matrix is a rectangular arrangement of elements, which are real numbers. But does there exist such a thing as a rectangular arrangement of complex numbers? How are these matrices called, if they exist?",,"['matrices', 'complex-numbers']"
34,On $n\times n$ matrices $A$ with trace of the powers equal to $0$,On  matrices  with trace of the powers equal to,n\times n A 0,Let $R$ be a commutative ring with identity and let $A \in M_n(R)$ be such that $$\mbox{tr}A = \mbox{tr}A^2 = \cdots = \mbox{tr}A^n = 0 .$$ I want to show that $n!A^n= 0$. Any suggestion or reference would be helpful. P.S.: When $R$ is a field of characteristic zero I can prove that $A^n=0$ but I have no idea for the general case.,Let $R$ be a commutative ring with identity and let $A \in M_n(R)$ be such that $$\mbox{tr}A = \mbox{tr}A^2 = \cdots = \mbox{tr}A^n = 0 .$$ I want to show that $n!A^n= 0$. Any suggestion or reference would be helpful. P.S.: When $R$ is a field of characteristic zero I can prove that $A^n=0$ but I have no idea for the general case.,,"['linear-algebra', 'abstract-algebra', 'matrices']"
35,Plot of ||X||infinity norm,Plot of ||X||infinity norm,,"Can anybody tell me why the plot of $\|X\|_{\infty}$ in $\mathbb{R}^2$ comes out to be square? Since $\|(x_1,x_2)\|_{\infty} = \max\{|x_1|,|x_2|\}$, then let us say $|x_1|$ is max. Why the plot is square?","Can anybody tell me why the plot of $\|X\|_{\infty}$ in $\mathbb{R}^2$ comes out to be square? Since $\|(x_1,x_2)\|_{\infty} = \max\{|x_1|,|x_2|\}$, then let us say $|x_1|$ is max. Why the plot is square?",,"['matrices', 'normed-spaces']"
36,Adjoint of a Matrix Definition,Adjoint of a Matrix Definition,,"Tom M. Apostol in his book ""calculus Vol. 2"" page 122 (see image below) defines adjoint of a matrix as the transpose of the conjugate of the matrix. Is this definition always correct ? Does it agree with the adjoint defined here , i.e. transpose of the cofactor matrix?","Tom M. Apostol in his book ""calculus Vol. 2"" page 122 (see image below) defines adjoint of a matrix as the transpose of the conjugate of the matrix. Is this definition always correct ? Does it agree with the adjoint defined here , i.e. transpose of the cofactor matrix?",,"['linear-algebra', 'matrices', 'terminology', 'definition']"
37,Determinant of block matrices with non-square blocks,Determinant of block matrices with non-square blocks,,"Let $A$ be $m \times n$ matrix, and $B$ be $n \times m$ matrix. Then Show that $$\det\begin{bmatrix}I_{n} & B\\ A & I_{m} \end{bmatrix}=\det\begin{bmatrix}I_{m} & A\\ B & I_{n}\end{bmatrix}$$ Show that $$\det(I_{m}-AB)=\det(I_{n}-BA)$$ Is it true that $I_{m}-AB$ and $I_{n}-BA$ have the same rank? Someone please show me a way to start proving. I tried to use formulas for calculating block matrix with square matrices, but it didn't help me on this one. I really don't know where to start.","Let be matrix, and be matrix. Then Show that Show that Is it true that and have the same rank? Someone please show me a way to start proving. I tried to use formulas for calculating block matrix with square matrices, but it didn't help me on this one. I really don't know where to start.",A m \times n B n \times m \det\begin{bmatrix}I_{n} & B\\ A & I_{m} \end{bmatrix}=\det\begin{bmatrix}I_{m} & A\\ B & I_{n}\end{bmatrix} \det(I_{m}-AB)=\det(I_{n}-BA) I_{m}-AB I_{n}-BA,"['linear-algebra', 'matrices', 'determinant', 'matrix-rank', 'block-matrices']"
38,Multivariate normal distribution from invertable covariance matrix,Multivariate normal distribution from invertable covariance matrix,,"I want to generate a random vector with $\mathcal{N}(0, C)$ distribution, i.e. normal distribution with $0$ mean and given covariance matrix $C$. $C$ is not invertible (singular). Here it's written: The covariance matrix is allowed to be singular (in which case the corresponding distribution has no density). This case arises frequently in statistics (...) So, how can I do it without inverting $C$?","I want to generate a random vector with $\mathcal{N}(0, C)$ distribution, i.e. normal distribution with $0$ mean and given covariance matrix $C$. $C$ is not invertible (singular). Here it's written: The covariance matrix is allowed to be singular (in which case the corresponding distribution has no density). This case arises frequently in statistics (...) So, how can I do it without inverting $C$?",,"['matrices', 'normal-distribution']"
39,A basic doubt over diagonalization of a matrix,A basic doubt over diagonalization of a matrix,,"Suppose, we have a matrix $A$ which is diagonalizable i.e. there exist an invertible matrix $B$ such that $B^{-1}AB = D$ where $D$ is a diagonal matrix. Is $B$ unique ? I don't think so.","Suppose, we have a matrix $A$ which is diagonalizable i.e. there exist an invertible matrix $B$ such that $B^{-1}AB = D$ where $D$ is a diagonal matrix. Is $B$ unique ? I don't think so.",,"['linear-algebra', 'matrices']"
40,Low-rank matrix approximation in terms of entry-wise $L_1$ norm,Low-rank matrix approximation in terms of entry-wise  norm,L_1,"According to the Eckart–Young theorem, the low-rank matrix approximation problem $$\min_{\tilde{A}} \quad \| A - \tilde{A} \|_F \quad \text{s.t.} \quad \text{rank}(\tilde{A}) \le r$$ is given by the SVD of $A$, where $\| \cdot \|_F$ denotes the Frobenius norm. I think if the the Frobenius norm is replaced by the nuclear norm, the solution will not change, but I wonder whether there is an analytic solution when the Frobenius norm (entry-wise $L_2$ norm) is replaced by the entry-wise $L_1$ norm. Note that the entry-wise $L_1$ norm is NOT the nuclear norm.","According to the Eckart–Young theorem, the low-rank matrix approximation problem $$\min_{\tilde{A}} \quad \| A - \tilde{A} \|_F \quad \text{s.t.} \quad \text{rank}(\tilde{A}) \le r$$ is given by the SVD of $A$, where $\| \cdot \|_F$ denotes the Frobenius norm. I think if the the Frobenius norm is replaced by the nuclear norm, the solution will not change, but I wonder whether there is an analytic solution when the Frobenius norm (entry-wise $L_2$ norm) is replaced by the entry-wise $L_1$ norm. Note that the entry-wise $L_1$ norm is NOT the nuclear norm.",,"['linear-algebra', 'matrices', 'optimization', 'normed-spaces', 'svd']"
41,Writing u as a linear combination of the vectors in S.,Writing u as a linear combination of the vectors in S.,,"Write vector  u = $$\left[\begin{array}{ccc|c}2 \\10 \\1\end{array}\right]$$ as a linear combination of the vectors in S. Use elementary row operations on an augmented matrix to find the necessary coefficients. S = { $v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   v2\left[\begin{matrix}5\\4\\1\end{matrix}\right] $ }. If it is not possible, explain why? This is what i have so far: S = { $v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   v3\left[\begin{matrix}5\\4\\1\end{matrix}\right]. v4\left[\begin{matrix}2\\10\\1\end{matrix}\right] $ } $c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   c3\left[\begin{matrix}5\\4\\1\end{matrix}\right]. c4\left[\begin{matrix}2\\10\\1\end{matrix}\right] =\left[\begin{matrix}0\\0\\0\end{matrix}\right]$ $c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   c3\left[\begin{matrix}5\\4\\1\end{matrix}\right]. c4\left[\begin{matrix}2\\10\\1\end{matrix}\right]$ $ \begin{bmatrix} 1 & 4 & 5 & 2\\ 2 & 2 & 4 & 10\\ 2 & 1 & 1 & 1\\ \end{bmatrix} $ Now i don't know how to do this. Help will greatly be appreciated. Thanks","Write vector  u = $$\left[\begin{array}{ccc|c}2 \\10 \\1\end{array}\right]$$ as a linear combination of the vectors in S. Use elementary row operations on an augmented matrix to find the necessary coefficients. S = { $v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   v2\left[\begin{matrix}5\\4\\1\end{matrix}\right] $ }. If it is not possible, explain why? This is what i have so far: S = { $v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   v3\left[\begin{matrix}5\\4\\1\end{matrix}\right]. v4\left[\begin{matrix}2\\10\\1\end{matrix}\right] $ } $c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   c3\left[\begin{matrix}5\\4\\1\end{matrix}\right]. c4\left[\begin{matrix}2\\10\\1\end{matrix}\right] =\left[\begin{matrix}0\\0\\0\end{matrix}\right]$ $c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],   c3\left[\begin{matrix}5\\4\\1\end{matrix}\right]. c4\left[\begin{matrix}2\\10\\1\end{matrix}\right]$ $ \begin{bmatrix} 1 & 4 & 5 & 2\\ 2 & 2 & 4 & 10\\ 2 & 1 & 1 & 1\\ \end{bmatrix} $ Now i don't know how to do this. Help will greatly be appreciated. Thanks",,"['linear-algebra', 'matrices', 'vector-spaces']"
42,Square root of a diagonal matrix $\lambda I$,Square root of a diagonal matrix,\lambda I,"Could you help me prove that if $M \in \mathcal{M}_{2 \times 2}(\mathbb{R})$ satisfies $X^2=\lambda I$, $ \ \ \ \lambda \in \mathbb{R}, \ \  \lambda <0$, then there exist $y,z \in \mathbb{R}, \ \ yz \le \lambda$ and $X=\left[\begin{array}{ccc}\sqrt{\lambda -yz}&y\\z&-\sqrt{\lambda - yz}\end{array}\right]$ or $X=\left[\begin{array}{ccc}-\sqrt{\lambda -yz}&y\\z&\sqrt{\lambda - yz}\end{array}\right]$? I know that if $\lambda \ge 0 \ \ X=\lambda I$ or $X=-\lambda I$ Let $X=\left[\begin{array}{ccc}a&b\\c&d\end{array}\right]$, $X^2=\left[\begin{array}{ccc}a^2+bc&ab+bd\\ac+cd&bc+d^2\end{array}\right]$. So it appears I need to solve $\begin{cases} a^2+bc=\lambda & \\ab+bd=0 & \\ ac+cd=0 & \\ bc+d^2= \lambda & \end{cases}$. I know that $det(X^2) = det(X)^2$, so $(ad-bc)^2= \lambda$. But I don't know how to use it.","Could you help me prove that if $M \in \mathcal{M}_{2 \times 2}(\mathbb{R})$ satisfies $X^2=\lambda I$, $ \ \ \ \lambda \in \mathbb{R}, \ \  \lambda <0$, then there exist $y,z \in \mathbb{R}, \ \ yz \le \lambda$ and $X=\left[\begin{array}{ccc}\sqrt{\lambda -yz}&y\\z&-\sqrt{\lambda - yz}\end{array}\right]$ or $X=\left[\begin{array}{ccc}-\sqrt{\lambda -yz}&y\\z&\sqrt{\lambda - yz}\end{array}\right]$? I know that if $\lambda \ge 0 \ \ X=\lambda I$ or $X=-\lambda I$ Let $X=\left[\begin{array}{ccc}a&b\\c&d\end{array}\right]$, $X^2=\left[\begin{array}{ccc}a^2+bc&ab+bd\\ac+cd&bc+d^2\end{array}\right]$. So it appears I need to solve $\begin{cases} a^2+bc=\lambda & \\ab+bd=0 & \\ ac+cd=0 & \\ bc+d^2= \lambda & \end{cases}$. I know that $det(X^2) = det(X)^2$, so $(ad-bc)^2= \lambda$. But I don't know how to use it.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
43,Change of basis (Gram-Schmidt),Change of basis (Gram-Schmidt),,"I was wondering whether it is possible to write down explicitely the matrix that represents the change of basis from a basis $\{v_1,....,v_n\}$  to a basis $\{e_1,...,e_n\}$, where $e_i$ is the basis constructed from $\{v_1,....,v_n\}$ by the Gram-Schmidt process. Or if the inverse map would be easier, I would also be interested in this matrix.","I was wondering whether it is possible to write down explicitely the matrix that represents the change of basis from a basis $\{v_1,....,v_n\}$  to a basis $\{e_1,...,e_n\}$, where $e_i$ is the basis constructed from $\{v_1,....,v_n\}$ by the Gram-Schmidt process. Or if the inverse map would be easier, I would also be interested in this matrix.",,['linear-algebra']
44,What's this theorem?,What's this theorem?,,"While reading an old book, I came across this theorem: Neither name nor proof was given, can somebody provide some further information about this throrem? Thanks.","While reading an old book, I came across this theorem: Neither name nor proof was given, can somebody provide some further information about this throrem? Thanks.",,['matrices']
45,How to show that multiplication of matrices is associative and distributive over addition,How to show that multiplication of matrices is associative and distributive over addition,,"How to show that multiplication of matrices, when defined, is associative and distributive over addition. Let $R$ be a ring, and the set of all $n \times m$ matrices over $R$.","How to show that multiplication of matrices, when defined, is associative and distributive over addition. Let $R$ be a ring, and the set of all $n \times m$ matrices over $R$.",,['linear-algebra']
46,Identity makes every matrix invertible?,Identity makes every matrix invertible?,,"I have found this in a proof and do not understand where this comes from: If A is singular, then there exists $\delta \in \mathbb{R}_{>0} \forall \epsilon\in (0,\delta): \epsilon \operatorname{Id}+A $ is nonsingular. It sounds similar to some corollary from the definition of continuous functions but I do not see how to proof this.","I have found this in a proof and do not understand where this comes from: If A is singular, then there exists $\delta \in \mathbb{R}_{>0} \forall \epsilon\in (0,\delta): \epsilon \operatorname{Id}+A $ is nonsingular. It sounds similar to some corollary from the definition of continuous functions but I do not see how to proof this.",,"['real-analysis', 'linear-algebra']"
47,Matrix determinant using Laplace method,Matrix determinant using Laplace method,,"I have the following matrix of order four for which I have calculated the determinant using Laplace's method. $$ \begin{bmatrix}  2 & 1 &  3 &  1 \\  4 & 3 &  1 &  4 \\ -1 & 5 & -2 &  1 \\  1 & 3 & -2 & -1 \\ \end{bmatrix} $$ Finding the determinant gives me $-726$. Now if I check the result at Wolfram Alpha, it says the result is $-180$ (Because there are no zeros in the matrix, expand with respect to row one) so it uses only the first row to calculate the determinant of the matrix. My question is: Why it uses only the first row to find the determinant?","I have the following matrix of order four for which I have calculated the determinant using Laplace's method. $$ \begin{bmatrix}  2 & 1 &  3 &  1 \\  4 & 3 &  1 &  4 \\ -1 & 5 & -2 &  1 \\  1 & 3 & -2 & -1 \\ \end{bmatrix} $$ Finding the determinant gives me $-726$. Now if I check the result at Wolfram Alpha, it says the result is $-180$ (Because there are no zeros in the matrix, expand with respect to row one) so it uses only the first row to calculate the determinant of the matrix. My question is: Why it uses only the first row to find the determinant?",,"['matrices', 'determinant']"
48,Cramer's Rule for Inverse,Cramer's Rule for Inverse,,"I am trying to find the inverse of the following matrix using Cramer's rule:$\begin{bmatrix}2&0&0&0\\ 1&-2&1&-1\\ 0&2&2&3 \\1&0&1&1  \end{bmatrix}$. If this was a $3\times3$, I would know the process but I am completely stuck since this is a $4 \times 4$. My question is, how do you even begin to calculate $C_{1,1}, ...., C_{m,n}$? I tried searching for a tutorial with no luck and my book does not mention 4x4. My idea was to compute:$\begin{bmatrix} -2&1&-1\\ 2&2&3 \\0&1&1\end{bmatrix}$ for $C_{1,1}$ and take this long process all the way up to $C_{m,n}$. But if this is correct, do I solve the determinant of the 3x3 matrix now and that is my co factor or....? Any help would be appreciated. Also since this is a very long process, I was wondering if there is any algorithm for paper-pencil process or any easier way to compute it?","I am trying to find the inverse of the following matrix using Cramer's rule:$\begin{bmatrix}2&0&0&0\\ 1&-2&1&-1\\ 0&2&2&3 \\1&0&1&1  \end{bmatrix}$. If this was a $3\times3$, I would know the process but I am completely stuck since this is a $4 \times 4$. My question is, how do you even begin to calculate $C_{1,1}, ...., C_{m,n}$? I tried searching for a tutorial with no luck and my book does not mention 4x4. My idea was to compute:$\begin{bmatrix} -2&1&-1\\ 2&2&3 \\0&1&1\end{bmatrix}$ for $C_{1,1}$ and take this long process all the way up to $C_{m,n}$. But if this is correct, do I solve the determinant of the 3x3 matrix now and that is my co factor or....? Any help would be appreciated. Also since this is a very long process, I was wondering if there is any algorithm for paper-pencil process or any easier way to compute it?",,"['linear-algebra', 'matrices']"
49,Define the linear transformation T: P2 -> R2 by T(p) = [p(0) p(0)] Find a basis for the kernel of T.,Define the linear transformation T: P2 -> R2 by T(p) = [p(0) p(0)] Find a basis for the kernel of T.,,"Pretty lost on how to answer this question. Define the linear transformation $T:P_2 \rightarrow \Bbb{R}^2$ by $$ T(p) =\left[\begin{array}{c}p(0)\\p(0)\end{array}\right] $$ Find a basis for the kernel of $T$. So a $P_2$ polynomial has the form $ax + bx + cx^2$. So $T(p)$ will always have he form $[a\; a]^\intercal$. That would mean the kernel of $T$ is $[a\; a]^\intercal$ where $a = 0$, correct? It was my understanding the kernel of a transformation is all $u$ such that $T(u)= 0$. If this is correct,which I'm sure it probably isn't, how do I find the basis? A basis has to be linearly independent and would have to span the kernel, so would it be a polynomial of the form $a + bx + cx^2$?","Pretty lost on how to answer this question. Define the linear transformation $T:P_2 \rightarrow \Bbb{R}^2$ by $$ T(p) =\left[\begin{array}{c}p(0)\\p(0)\end{array}\right] $$ Find a basis for the kernel of $T$. So a $P_2$ polynomial has the form $ax + bx + cx^2$. So $T(p)$ will always have he form $[a\; a]^\intercal$. That would mean the kernel of $T$ is $[a\; a]^\intercal$ where $a = 0$, correct? It was my understanding the kernel of a transformation is all $u$ such that $T(u)= 0$. If this is correct,which I'm sure it probably isn't, how do I find the basis? A basis has to be linearly independent and would have to span the kernel, so would it be a polynomial of the form $a + bx + cx^2$?",,"['linear-algebra', 'matrices']"
50,The Effect of a Transpose on a Matrix Inequality,The Effect of a Transpose on a Matrix Inequality,,"In the solution to an exercise I came across the following: $y^TA_N \geq c_N^T \rightarrow A_N^Ty \geq c_N$. Now I was wondering, is it in general true that an inequality remains valid when 'taking transposes on both sides'? If so, what is the proof for this?","In the solution to an exercise I came across the following: $y^TA_N \geq c_N^T \rightarrow A_N^Ty \geq c_N$. Now I was wondering, is it in general true that an inequality remains valid when 'taking transposes on both sides'? If so, what is the proof for this?",,['matrices']
51,trace of product of positive definite matrix,trace of product of positive definite matrix,,"For positive definite matrices $A$ and $C$, positive semidefinite matrices $B$ and $D$, I want to know whether $tr\{ABCD\}=0$ implies that $tr\{BD\}=0$.","For positive definite matrices $A$ and $C$, positive semidefinite matrices $B$ and $D$, I want to know whether $tr\{ABCD\}=0$ implies that $tr\{BD\}=0$.",,"['linear-algebra', 'matrices', 'trace']"
52,Solving elementary row operations,Solving elementary row operations,,"So I am faced with the following: $$ \begin{align} x_1 + 4x_2 - 2x_3 +8x_4 &=12\\ x_2 - 7x_3 +2x_4 &=-4\\ 5x_3 -x_4 &=7\\ x_3 +3x_4 &=-5 \end{align}$$ How should I approach this problem? In other words, what is the next elementary row operation that I should attempt in order to solve it? I know how to do with 3 equations by using the augmented method but this got me a little confused.","So I am faced with the following: $$ \begin{align} x_1 + 4x_2 - 2x_3 +8x_4 &=12\\ x_2 - 7x_3 +2x_4 &=-4\\ 5x_3 -x_4 &=7\\ x_3 +3x_4 &=-5 \end{align}$$ How should I approach this problem? In other words, what is the next elementary row operation that I should attempt in order to solve it? I know how to do with 3 equations by using the augmented method but this got me a little confused.",,"['linear-algebra', 'matrices']"
53,Why is $\operatorname{rank}(A^* A)=\operatorname{rank}(A) \Leftrightarrow$ $(A^* Ax=0 \Leftrightarrow Ax=0)$?,Why is  ?,\operatorname{rank}(A^* A)=\operatorname{rank}(A) \Leftrightarrow (A^* Ax=0 \Leftrightarrow Ax=0),Let $A \in M_{m\times n}(F)$ and $x \in F^n$. $A^*$ is the adjoint of $A$. Why is $\operatorname{rank}(A^* A)=\operatorname{rank}(A)$ equivalent to $A^* Ax=0$ if and only if $Ax=0$?,Let $A \in M_{m\times n}(F)$ and $x \in F^n$. $A^*$ is the adjoint of $A$. Why is $\operatorname{rank}(A^* A)=\operatorname{rank}(A)$ equivalent to $A^* Ax=0$ if and only if $Ax=0$?,,"['linear-algebra', 'matrices', 'vector-spaces']"
54,"Matrix Equation, Solving for Variables.","Matrix Equation, Solving for Variables.",,"I'm going through my exercises, and came across a problem that wasn't covered in our lectures. Here's the question: $ \begin{align} \begin{bmatrix} a-b & b+c\\ 3d+c & 2a-4d \end{bmatrix} \end{align} = $ $ \begin{align} \begin{bmatrix} 8 & 1\\ 7 & 6 \end{bmatrix} \end{align} $ What I have done so far is: $ \begin{align} \begin{bmatrix} a-b-8 & b+c-1\\ 3d+c-7 & 2a-4d-6 \end{bmatrix} \end{align} = $ $ \begin{align} \begin{bmatrix} 0 & 0\\ 0 & 0 \end{bmatrix} \end{align} $ And the solving for variables, $$ a-b-8 = 0 $$ $$ a-b = 8 $$ $$ a = \frac{8}{-b} $$ $$ b+c-1=0 $$ $$ b+c=1 $$ $$ b=\frac{1}{c} $$ $$ 3d+c-7=0 $$ $$ 3d+c=7 $$ $$ 3d=\frac{7}{c} $$ $$ d=\frac{7}{3c} $$ $$ 2a-4d-6=0 $$ $$ 2a-4d=6 $$ $$ \frac{16}{-2b}-\frac{28}{12c}=6 $$ Am I going about this correctly? Or am I just doing this completely incorrect?","I'm going through my exercises, and came across a problem that wasn't covered in our lectures. Here's the question: $ \begin{align} \begin{bmatrix} a-b & b+c\\ 3d+c & 2a-4d \end{bmatrix} \end{align} = $ $ \begin{align} \begin{bmatrix} 8 & 1\\ 7 & 6 \end{bmatrix} \end{align} $ What I have done so far is: $ \begin{align} \begin{bmatrix} a-b-8 & b+c-1\\ 3d+c-7 & 2a-4d-6 \end{bmatrix} \end{align} = $ $ \begin{align} \begin{bmatrix} 0 & 0\\ 0 & 0 \end{bmatrix} \end{align} $ And the solving for variables, $$ a-b-8 = 0 $$ $$ a-b = 8 $$ $$ a = \frac{8}{-b} $$ $$ b+c-1=0 $$ $$ b+c=1 $$ $$ b=\frac{1}{c} $$ $$ 3d+c-7=0 $$ $$ 3d+c=7 $$ $$ 3d=\frac{7}{c} $$ $$ d=\frac{7}{3c} $$ $$ 2a-4d-6=0 $$ $$ 2a-4d=6 $$ $$ \frac{16}{-2b}-\frac{28}{12c}=6 $$ Am I going about this correctly? Or am I just doing this completely incorrect?",,"['linear-algebra', 'matrices', 'systems-of-equations']"
55,Silliness: $\exists~X~\text{s.t.}~AX=B \iff B\in R(L_A)$,Silliness:,\exists~X~\text{s.t.}~AX=B \iff B\in R(L_A),"So, I am asked to prove that the system of linear equations $AX=B$ has $\color{black}{a~solution}$ if and only if $B\in R(L_A)$. $R$ denotes the ""range of"" and $L_A$ is left multiplication by $A$. If the question says ""a solution"" does that mean that I can rely on the fact that it's invertible? Friedberg's 1989 edition of Linear Algebra says the above are equivalent, namely that $\exists~X~\text{s.t.}~AX=B \iff B\in R(L_A)$: How am I to ""prove"" this?","So, I am asked to prove that the system of linear equations $AX=B$ has $\color{black}{a~solution}$ if and only if $B\in R(L_A)$. $R$ denotes the ""range of"" and $L_A$ is left multiplication by $A$. If the question says ""a solution"" does that mean that I can rely on the fact that it's invertible? Friedberg's 1989 edition of Linear Algebra says the above are equivalent, namely that $\exists~X~\text{s.t.}~AX=B \iff B\in R(L_A)$: How am I to ""prove"" this?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'matrices', 'vector-spaces']"
56,Find the rank of the following matrix.,Find the rank of the following matrix.,,"$A= \left[ \begin{array}{ccc} 3 & -1 & 2 \\ -6 & 2 & 4 \\ -3 & 1 & 2 \end{array} \right]$ Applying, $R_{3}-\frac{1}{2}R_{2}$ ~ $A= \left[ \begin{array}{ccc} 3 & -1 & 2 \\ -6 & 2 & 4 \\ 0 & 0 & 0 \end{array} \right]$ Applying, $R_{2}+2R_{1}$ ~ $A= \left[ \begin{array}{ccc} 3 & -1 & 2 \\ 0 & 0 & 8 \\ 0 & 0 & 0 \end{array} \right]$","$A= \left[ \begin{array}{ccc} 3 & -1 & 2 \\ -6 & 2 & 4 \\ -3 & 1 & 2 \end{array} \right]$ Applying, $R_{3}-\frac{1}{2}R_{2}$ ~ $A= \left[ \begin{array}{ccc} 3 & -1 & 2 \\ -6 & 2 & 4 \\ 0 & 0 & 0 \end{array} \right]$ Applying, $R_{2}+2R_{1}$ ~ $A= \left[ \begin{array}{ccc} 3 & -1 & 2 \\ 0 & 0 & 8 \\ 0 & 0 & 0 \end{array} \right]$",,"['linear-algebra', 'matrices', 'matrix-rank']"
57,"Solving systems of linear equations using matrices, 3 equations, 4 variables","Solving systems of linear equations using matrices, 3 equations, 4 variables",,"I understand how to solve systems of linear equations when they have the same number of variables as equations. But what about when there are only three equations and 4 variables?  For example, when i was looking through an exam paper, i came across this question- w + x + y + z = 1  2w + x + 3y + z =7 2w + 2x + y + 2z =7 The question does not implicitly ask for us to solve using matrices, but it is in a question about matrices... Any help would be appreciated!","I understand how to solve systems of linear equations when they have the same number of variables as equations. But what about when there are only three equations and 4 variables?  For example, when i was looking through an exam paper, i came across this question- w + x + y + z = 1  2w + x + 3y + z =7 2w + 2x + y + 2z =7 The question does not implicitly ask for us to solve using matrices, but it is in a question about matrices... Any help would be appreciated!",,"['linear-algebra', 'matrices']"
58,Gradient of $Z \mapsto \left( Z^T \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)Z\right)$,Gradient of,Z \mapsto \left( Z^T \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)Z\right),"I know that $$ \nabla_{Z} \operatorname{Tr} \left( Z^T A Z \right) = 2 A Z $$ and I would like to compute the gradient $$ \nabla_{Z} \operatorname{Tr} \left( Z^T \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)Z\right)$$ I would like to see how this would be done, given my basic matrix calculus skills. All the entries are real and both $A$ and $\operatorname{diag}[(ZZ^T\mathbf{1}) - ZZ^T]$ are symmetric and $Z$ is a tall, rectangular matrix (more rows than columns), while $\operatorname{diag}(\cdot)$ denotes a diagonal matrix, whose diagonal elements are specified by the placeholder $\cdot$ and I think that $\left(\operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)$ is positive semidefinite (PSD), as it seems like it is in the form of a Laplacian matrix.","I know that and I would like to compute the gradient I would like to see how this would be done, given my basic matrix calculus skills. All the entries are real and both and are symmetric and is a tall, rectangular matrix (more rows than columns), while denotes a diagonal matrix, whose diagonal elements are specified by the placeholder and I think that is positive semidefinite (PSD), as it seems like it is in the form of a Laplacian matrix.", \nabla_{Z} \operatorname{Tr} \left( Z^T A Z \right) = 2 A Z   \nabla_{Z} \operatorname{Tr} \left( Z^T \left( \operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right)Z\right) A \operatorname{diag}[(ZZ^T\mathbf{1}) - ZZ^T] Z \operatorname{diag}(\cdot) \cdot \left(\operatorname{diag}(ZZ^T\mathbf{1}) - ZZ^T\right),"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'scalar-fields']"
59,Orthogonal Decomposition,Orthogonal Decomposition,,[Ciarlet 1.2-2] Let $O$ be an orthogonal matrix. Show that there exists an orthogonal matrix $Q$ such that $$Q^{-1}OQ\ =\ \left(\begin{array}{rrrrrrrrrrr} 1 &        &   &    &        &    &               &              &        &   & \\   & \ddots &   &    &        &    &               &              &        &   & \\   &        & 1 &    &        &    &               &              &        &   & \\   &        &   & -1 &        &    &               &              &        &   & \\   &        &   &    & \ddots &    &               &              &        &   & \\   &        &   &    &        & -1 &               &              &        &   & \\   &        &   &    &        &    &  \cos\theta_1 & \sin\theta_1 &        &   & \\   &        &   &    &        &    & -\sin\theta_1 & \cos\theta_1 &        &   & \\   &        &   &    &        &    &               &              & \ddots &   & \\   &        &   &    &        &    &               &              &        &  \cos\theta_r & \sin\theta_r\\   &        &   &    &        &    &               &              &        & -\sin\theta_r & \cos\theta_r \end{array}\right).$$,[Ciarlet 1.2-2] Let $O$ be an orthogonal matrix. Show that there exists an orthogonal matrix $Q$ such that $$Q^{-1}OQ\ =\ \left(\begin{array}{rrrrrrrrrrr} 1 &        &   &    &        &    &               &              &        &   & \\   & \ddots &   &    &        &    &               &              &        &   & \\   &        & 1 &    &        &    &               &              &        &   & \\   &        &   & -1 &        &    &               &              &        &   & \\   &        &   &    & \ddots &    &               &              &        &   & \\   &        &   &    &        & -1 &               &              &        &   & \\   &        &   &    &        &    &  \cos\theta_1 & \sin\theta_1 &        &   & \\   &        &   &    &        &    & -\sin\theta_1 & \cos\theta_1 &        &   & \\   &        &   &    &        &    &               &              & \ddots &   & \\   &        &   &    &        &    &               &              &        &  \cos\theta_r & \sin\theta_r\\   &        &   &    &        &    &               &              &        & -\sin\theta_r & \cos\theta_r \end{array}\right).$$,,"['linear-algebra', 'matrices', 'inner-products']"
60,Solve for $X$ in a simple $2\times2$ equation system.,Solve for  in a simple  equation system.,X 2\times2,I posted a similar question recently but I still have problem with this problem and would appreciate any help! $$\left[ \begin{array}{cc} 9 & -3\\ 5 & -5\end{array} \right] - X \left[ \begin{array}{cc} -9 & -2\\ 8 & 5\end{array} \right] = E$$ With $E$ i pressume they mean the identity matrix $\left[ \begin{array}{cc} 1 & 0\\ 0 & 1\end{array} \right]$. How should I go on and solve this for the $2\times2$ matrix $X$? a full development so I can follow your solution would be very much appreciated! Thank you kindly for you help!,I posted a similar question recently but I still have problem with this problem and would appreciate any help! $$\left[ \begin{array}{cc} 9 & -3\\ 5 & -5\end{array} \right] - X \left[ \begin{array}{cc} -9 & -2\\ 8 & 5\end{array} \right] = E$$ With $E$ i pressume they mean the identity matrix $\left[ \begin{array}{cc} 1 & 0\\ 0 & 1\end{array} \right]$. How should I go on and solve this for the $2\times2$ matrix $X$? a full development so I can follow your solution would be very much appreciated! Thank you kindly for you help!,,"['linear-algebra', 'matrices']"
61,"$p$-Sylow subgroups of $\operatorname{GL}(n, \mathbb{F}_{p^k})$",-Sylow subgroups of,"p \operatorname{GL}(n, \mathbb{F}_{p^k})","Is there a way to classify all $p$-Sylow subgroups of $GL(n,\, \mathbb{F}_{p^k})$ where $|\mathbb{F}_{p^k}|=p^k$? Clearly the prime $p$ (that is the characteristic of the base field) is a very privileged number; for example we can say that a $p$-Sylow of $GL(n,\, \mathbb{F}_{p^k})$ has order $p^{k(1+2+\ldots+(n-1))}$. But what about the structure of these $p$-Sylows can we identify them as normalizers/centralizers of flags in the projective geometry?","Is there a way to classify all $p$-Sylow subgroups of $GL(n,\, \mathbb{F}_{p^k})$ where $|\mathbb{F}_{p^k}|=p^k$? Clearly the prime $p$ (that is the characteristic of the base field) is a very privileged number; for example we can say that a $p$-Sylow of $GL(n,\, \mathbb{F}_{p^k})$ has order $p^{k(1+2+\ldots+(n-1))}$. But what about the structure of these $p$-Sylows can we identify them as normalizers/centralizers of flags in the projective geometry?",,"['matrices', 'group-theory', 'finite-groups', 'sylow-theory', 'linear-groups']"
62,"Find all $A,B$ such that $AB-BA=0$.",Find all  such that .,"A,B AB-BA=0","Can someone give me a hint how to find all $n\times n$ matrices $A,B$ over an arbitrary field, such that they commute, i.e. such that $AB=BA$ ? I found this problem in some lectures notes where the trace of a matrix was discussed, so my guess is, that I should use it - but I have no idea how (although I know that $tr(AB)=tr(BA)$). Manually writing this out and trying to solve a huge system just doesn't seem like a good strategy.","Can someone give me a hint how to find all $n\times n$ matrices $A,B$ over an arbitrary field, such that they commute, i.e. such that $AB=BA$ ? I found this problem in some lectures notes where the trace of a matrix was discussed, so my guess is, that I should use it - but I have no idea how (although I know that $tr(AB)=tr(BA)$). Manually writing this out and trying to solve a huge system just doesn't seem like a good strategy.",,['linear-algebra']
63,Deducing properties of a transformation from its matrix,Deducing properties of a transformation from its matrix,,"Give the rank of the matrix $$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$$ Is the corresponding linear mapping injective, surjective, bijective? Answer: the rank is three. Thus, the corresponding linear mapping is neither injective, nor surjective or bijective. It is clear that the matrix has a rank of 3, since there are only three linearly independent columns in it. However, where do we get the properties of linear mapping and where is the mapping defined anyway? (don't see any corresponding notations) I'm rather new to this, so any readings are of great value. Thanks in advance.","Give the rank of the matrix $$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}$$ Is the corresponding linear mapping injective, surjective, bijective? Answer: the rank is three. Thus, the corresponding linear mapping is neither injective, nor surjective or bijective. It is clear that the matrix has a rank of 3, since there are only three linearly independent columns in it. However, where do we get the properties of linear mapping and where is the mapping defined anyway? (don't see any corresponding notations) I'm rather new to this, so any readings are of great value. Thanks in advance.",,"['linear-algebra', 'matrices']"
64,Matrix trace based formulation of least-squares,Matrix trace based formulation of least-squares,,"How can the following function be represented in a matrix form using matrix trace? $||y-X\beta||^2 + \lambda \beta^T S \beta$ Note that $y, \beta$ are real vectors and $\lambda$ is a real scalar while $X$ is a rectangular matrix and $S$ is a square matrix with both matrices again having real entries.","How can the following function be represented in a matrix form using matrix trace? $||y-X\beta||^2 + \lambda \beta^T S \beta$ Note that $y, \beta$ are real vectors and $\lambda$ is a real scalar while $X$ is a rectangular matrix and $S$ is a square matrix with both matrices again having real entries.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'trace']"
65,How to construct non-square isometry matrix,How to construct non-square isometry matrix,,"How can we construct a non-square isometry matrix $U\in \mathcal{M_{n,m}}$; that is, all columns of $U$ are orthonormal and $U U^T=I_{n,n}$?","How can we construct a non-square isometry matrix $U\in \mathcal{M_{n,m}}$; that is, all columns of $U$ are orthonormal and $U U^T=I_{n,n}$?",,"['linear-algebra', 'matrices']"
66,An inequality involving the norms of symmetric positive definite matrices,An inequality involving the norms of symmetric positive definite matrices,,"Given A and B two real symmetric positive definite matrices is it true that, for some norm $\|.\|$, this inequality holds $$ \|AB-I\| \leq \|A^2B^2-I\| \qquad ? $$","Given A and B two real symmetric positive definite matrices is it true that, for some norm $\|.\|$, this inequality holds $$ \|AB-I\| \leq \|A^2B^2-I\| \qquad ? $$",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
67,Commutation matrix proof,Commutation matrix proof,,Prove that each commutation matrix $K$ is invertible and that $K^{-1} = K^{T}$ We found that $K$ is a square matrix and because we assume that $K$ only has distinct elements it has the maximal rank and is therefore an invertible square matrix. We don't know how to prove the last part. Could someone please help?,Prove that each commutation matrix $K$ is invertible and that $K^{-1} = K^{T}$ We found that $K$ is a square matrix and because we assume that $K$ only has distinct elements it has the maximal rank and is therefore an invertible square matrix. We don't know how to prove the last part. Could someone please help?,,['matrices']
68,Matrix Pseudo-Inverse using LU Decomposition?,Matrix Pseudo-Inverse using LU Decomposition?,,"What is the step by step numerical approach to calculate the pseudo-inverse of a matrix with M rows and N columns, using LU decomposition? So far, I have found this , but it uses singular value decomposition.","What is the step by step numerical approach to calculate the pseudo-inverse of a matrix with M rows and N columns, using LU decomposition? So far, I have found this , but it uses singular value decomposition.",,"['matrices', 'numerical-linear-algebra', 'inverse']"
69,Find the spanning set of the range of the linear transformation $T(x)=Ax$.,Find the spanning set of the range of the linear transformation .,T(x)=Ax,Let $$         A=         \begin{bmatrix}         -4 & -4 & 12 & 0 \\         -4 & -4 & 12 & 0 \\         4 & -2 & 0 &-6 \\         1 &-4 &7 &-5 \\          \end{bmatrix} $$ Find the spanning set of the range of the linear transformation $T(x)=Ax$. I have found the row reduced echelon form of A. $$         RREF(A)=         \begin{bmatrix}         1 & 0 & -1 & -1 \\         0 & 1 & -2 & 1 \\         0 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 \\          \end{bmatrix} $$ I don't know what to do with it after.,Let $$         A=         \begin{bmatrix}         -4 & -4 & 12 & 0 \\         -4 & -4 & 12 & 0 \\         4 & -2 & 0 &-6 \\         1 &-4 &7 &-5 \\          \end{bmatrix} $$ Find the spanning set of the range of the linear transformation $T(x)=Ax$. I have found the row reduced echelon form of A. $$         RREF(A)=         \begin{bmatrix}         1 & 0 & -1 & -1 \\         0 & 1 & -2 & 1 \\         0 & 0 & 0 & 0 \\         0 & 0 & 0 & 0 \\          \end{bmatrix} $$ I don't know what to do with it after.,,"['linear-algebra', 'matrices', 'transformation']"
70,Orthogonal Projection of matrix onto subspace,Orthogonal Projection of matrix onto subspace,,"Let's say I have the subspace $$S=\{(X_1,X_2,X_3,X_4)\mid~~6X_1 - 2X_2 + 4X_3-10X_4 = 0\}$$ How do I go about finding the matrix which is the orthogonal projection onto this subspace?","Let's say I have the subspace $$S=\{(X_1,X_2,X_3,X_4)\mid~~6X_1 - 2X_2 + 4X_3-10X_4 = 0\}$$ How do I go about finding the matrix which is the orthogonal projection onto this subspace?",,"['linear-algebra', 'matrices', 'vector-spaces']"
71,Showing that $(A_{ij})=\left(\frac1{1-x_ix_j}\right)$ is positive semidefinite,Showing that  is positive semidefinite,(A_{ij})=\left(\frac1{1-x_ix_j}\right),"Consider the matrix $A$ whose elements are $A_{ij} = \frac{1}{1-x_i x_j} $ where we have  $ -1 < x_i < 1$ and $ -1 < x_j < 1$ for $ i,j=1,2,...n$.  For example, when $n=3$ the matrix becomes \begin{pmatrix}        \frac{1}{1-x_1^2} & \frac{1}{1-x_1x_2} & \frac{1}{1-x_1x_3}          \\        \frac{1}{1-x_1x_2} &   \frac{1}{1-x_2^2}        & \frac{1}{1-x_2x_3}  \\        \frac{1}{1-x_1x_3}            & \frac{1}{1-x_2x_3}  & \frac{1}{1-x_3^2}      \end{pmatrix} For $n=2,3$ I considered the principal minors and concluded that this matrix is positive semidefinite. However, I could not generalize it for $n \geq 3$. How can I prove that this matrix is positive semidefinite for arbitrary $n$ ?","Consider the matrix $A$ whose elements are $A_{ij} = \frac{1}{1-x_i x_j} $ where we have  $ -1 < x_i < 1$ and $ -1 < x_j < 1$ for $ i,j=1,2,...n$.  For example, when $n=3$ the matrix becomes \begin{pmatrix}        \frac{1}{1-x_1^2} & \frac{1}{1-x_1x_2} & \frac{1}{1-x_1x_3}          \\        \frac{1}{1-x_1x_2} &   \frac{1}{1-x_2^2}        & \frac{1}{1-x_2x_3}  \\        \frac{1}{1-x_1x_3}            & \frac{1}{1-x_2x_3}  & \frac{1}{1-x_3^2}      \end{pmatrix} For $n=2,3$ I considered the principal minors and concluded that this matrix is positive semidefinite. However, I could not generalize it for $n \geq 3$. How can I prove that this matrix is positive semidefinite for arbitrary $n$ ?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
72,Symmetric positive definite matrix inequality,Symmetric positive definite matrix inequality,,"Hi could you help me with the following: Show that for a symmetric positive definite matrix $B$, $$b_{ij} + b_{jk} + b_{ki} \leqslant b_{ii} + b_{jj} + b_{kk}$$ holds for any $1 \leqslant i,j,k \leqslant n$ with $b_{ij}$ being the entry at $(i,j)$ of matrix $B$. Thanks a lot.","Hi could you help me with the following: Show that for a symmetric positive definite matrix $B$, $$b_{ij} + b_{jk} + b_{ki} \leqslant b_{ii} + b_{jj} + b_{kk}$$ holds for any $1 \leqslant i,j,k \leqslant n$ with $b_{ij}$ being the entry at $(i,j)$ of matrix $B$. Thanks a lot.",,"['linear-algebra', 'matrices', 'inequality']"
73,computing with unitary matrices,computing with unitary matrices,,"I am currently working on a problem and I am stuck with the following issue. For $A \in GL(n)$ and $B \in U(n)$ I am hoping that it is true that $$ A(B-A)^{-1}B = B(B-A)^{-1}A $$ My question is whether this is indeed the case and if so what I need to look into to understand why. ( I just assume for the moment that $B - A$ has an inverse ) PS: Just to give some context I am stuck with this issue because I am playing around with the kernel that I have computed for the operator $$ D := i \;I\frac{d}{dx} + B(x) : C^\infty_T ([0,1],\mathbb{C^m}) \to C^\infty_T ([0,1],\mathbb{C^m})$$ with boundary condition $f(1) = T f(0)$ for $T \in U(n)$ and $B$ Hermitian. I am currently trying to show that $L_T \;f(1) = T L_T \; f(0)$ where $L_T$ is the integral operator given by the kernel that I have computed for $D$. In order for things to work out I have to show that $$ p(1)(T-p(1))^{-1}T = T(T-p(1))^{-1}p(1)$$ where $p$ is assumed to conjugate $D$ to $-ip^{-1}Dp=D^p = \frac{d}{dx}I$","I am currently working on a problem and I am stuck with the following issue. For $A \in GL(n)$ and $B \in U(n)$ I am hoping that it is true that $$ A(B-A)^{-1}B = B(B-A)^{-1}A $$ My question is whether this is indeed the case and if so what I need to look into to understand why. ( I just assume for the moment that $B - A$ has an inverse ) PS: Just to give some context I am stuck with this issue because I am playing around with the kernel that I have computed for the operator $$ D := i \;I\frac{d}{dx} + B(x) : C^\infty_T ([0,1],\mathbb{C^m}) \to C^\infty_T ([0,1],\mathbb{C^m})$$ with boundary condition $f(1) = T f(0)$ for $T \in U(n)$ and $B$ Hermitian. I am currently trying to show that $L_T \;f(1) = T L_T \; f(0)$ where $L_T$ is the integral operator given by the kernel that I have computed for $D$. In order for things to work out I have to show that $$ p(1)(T-p(1))^{-1}T = T(T-p(1))^{-1}p(1)$$ where $p$ is assumed to conjugate $D$ to $-ip^{-1}Dp=D^p = \frac{d}{dx}I$",,"['linear-algebra', 'matrices']"
74,Consider a matrix $A$ with integer entries such that $a_{ij}=0$ for $i>j$ and $a_{ii}=1$ . then which of the followings are true?,Consider a matrix  with integer entries such that  for  and  . then which of the followings are true?,A a_{ij}=0 i>j a_{ii}=1,"Consider a matrix $A=(a_{ij})_{ n ×n }$ with integer entries such that $a_{ij}=0$ for $i>j$ and $a_{ii}=1$ for $i=1,…,n$. then which of the followings are true? $A^{-1}$ exists and it has integer entries. $A^{-1}$ exists and it has some entries that are not integer. $A^{-1}$ is a polynomial of $A$ with integer coefficients. $A^{-1}$ is not a power of $A$ unless $A$ is the identity matrix. By the given conditions $A$ is the upper triangular matrix with diagonal elements $1$.so eigenvalues are $1$.so their product=determinant of $A =1.$ So 1 is true. Inverse of the identity matrix is itself with has all integer entries so 2 is false. But I have no idea about (3) and (4) can anyone help me please.","Consider a matrix $A=(a_{ij})_{ n ×n }$ with integer entries such that $a_{ij}=0$ for $i>j$ and $a_{ii}=1$ for $i=1,…,n$. then which of the followings are true? $A^{-1}$ exists and it has integer entries. $A^{-1}$ exists and it has some entries that are not integer. $A^{-1}$ is a polynomial of $A$ with integer coefficients. $A^{-1}$ is not a power of $A$ unless $A$ is the identity matrix. By the given conditions $A$ is the upper triangular matrix with diagonal elements $1$.so eigenvalues are $1$.so their product=determinant of $A =1.$ So 1 is true. Inverse of the identity matrix is itself with has all integer entries so 2 is false. But I have no idea about (3) and (4) can anyone help me please.",,"['linear-algebra', 'matrices']"
75,Quadratic function values through iterative updates,Quadratic function values through iterative updates,,"Suppose a function $$f(x)=\frac{1}{2}x^TAx-b^Tx$$ is given, for some symmetric $A\in\mathbb{R}^{n\times n}$ for which all off-diagonal entries of A are nonpositive, and every diagonal entry of A is positive and is the sum of absolute values of all other entries in the row. Hence, given that $D$ is a diagonal matrix containing the elements of $A$, the Gershgorin theorem states that the spectrum of $D^{-1}A$ is in $[0, 2]$. Given an initial $x_0$, I'm interested what happens with $f(x_0)$ after $x_0$ has been updated as in Jacobi iteration to the system $Ax=b$. According to https://scicomp.stackexchange.com/questions/1478/jacobi-iteration-to-reduce-the-quadratic-function (the accepted answer) the function values always decrease, but I'm not sure about the spectra bound used. Can the value of $f(x)$ increase after Jacobi iteration has been applied to any $x_0$?","Suppose a function $$f(x)=\frac{1}{2}x^TAx-b^Tx$$ is given, for some symmetric $A\in\mathbb{R}^{n\times n}$ for which all off-diagonal entries of A are nonpositive, and every diagonal entry of A is positive and is the sum of absolute values of all other entries in the row. Hence, given that $D$ is a diagonal matrix containing the elements of $A$, the Gershgorin theorem states that the spectrum of $D^{-1}A$ is in $[0, 2]$. Given an initial $x_0$, I'm interested what happens with $f(x_0)$ after $x_0$ has been updated as in Jacobi iteration to the system $Ax=b$. According to https://scicomp.stackexchange.com/questions/1478/jacobi-iteration-to-reduce-the-quadratic-function (the accepted answer) the function values always decrease, but I'm not sure about the spectra bound used. Can the value of $f(x)$ increase after Jacobi iteration has been applied to any $x_0$?",,"['linear-algebra', 'matrices', 'optimization']"
76,What is the Matrix corresponding to a Linear Transformation,What is the Matrix corresponding to a Linear Transformation,,Given $T: P_2 \rightarrow P_3$ defined by: $T(at^2 + bt +c) = (a-b+c)t^3 + (-a + 3b - 2c)t^2 +(-a-b)t +(2b-c)$ What is the corresponding Matrix of $T$? This is what I have: First I rewrite the transformation as follows: $T_0 = 2b - c$ $T_1 = -a - b$ $T_2 = -a + 3b -2c$ $T_3 = a-b+c$ And I know $T_i = \sum \limits_{j=1}^n \mu_{ji} b_i$ where $b_i$ is the basis vector. So my matrix $\mu$ is $\pmatrix{0&-1&-1&1\\                                 2&-1&3&-1\\                                 -1&0&-2&1}$ Is this correct? Is my rewriting of $T$ correct?,Given $T: P_2 \rightarrow P_3$ defined by: $T(at^2 + bt +c) = (a-b+c)t^3 + (-a + 3b - 2c)t^2 +(-a-b)t +(2b-c)$ What is the corresponding Matrix of $T$? This is what I have: First I rewrite the transformation as follows: $T_0 = 2b - c$ $T_1 = -a - b$ $T_2 = -a + 3b -2c$ $T_3 = a-b+c$ And I know $T_i = \sum \limits_{j=1}^n \mu_{ji} b_i$ where $b_i$ is the basis vector. So my matrix $\mu$ is $\pmatrix{0&-1&-1&1\\                                 2&-1&3&-1\\                                 -1&0&-2&1}$ Is this correct? Is my rewriting of $T$ correct?,,"['linear-algebra', 'matrices', 'transformation']"
77,How to prove positive definiteness?,How to prove positive definiteness?,,"$B_{(n+1)(n+1)}$ = $       \begin{bmatrix}         A & u \\         u^T & 1 \\         \end{bmatrix}$ is given, and $A$ is a positive definite matrix where its Cholesky factorization is given by $A=L*L^T$ formula. $A$ is $n\times n$ matrix and $u$ is a n_vector. Now assuming $\|L\|^{-1}\leq 1$, I need to show that $B$ is a positive definite for all $\|u\|<1$. Thanks","$B_{(n+1)(n+1)}$ = $       \begin{bmatrix}         A & u \\         u^T & 1 \\         \end{bmatrix}$ is given, and $A$ is a positive definite matrix where its Cholesky factorization is given by $A=L*L^T$ formula. $A$ is $n\times n$ matrix and $u$ is a n_vector. Now assuming $\|L\|^{-1}\leq 1$, I need to show that $B$ is a positive definite for all $\|u\|<1$. Thanks",,"['matrices', 'normed-spaces']"
78,Is this matrix positive semi-definite?,Is this matrix positive semi-definite?,,"$$A=\left[\frac{1}{i+j}\right]=\left(\begin{matrix}\frac{1}{1+1}&\frac{1}{1+2}&\cdots&\frac{1}{1+n}&\\\frac{1}{2+1}&\frac{1}{2+2}&\cdots&\frac{1}{2+n}\\\vdots&\vdots&\ddots&\vdots\\\frac{1}{n+1}&\frac{1}{n+2}&\cdots&\frac{1}{n+n}\end{matrix}\right)$$ My textbook exercise says yes, but I can't prove it.","$$A=\left[\frac{1}{i+j}\right]=\left(\begin{matrix}\frac{1}{1+1}&\frac{1}{1+2}&\cdots&\frac{1}{1+n}&\\\frac{1}{2+1}&\frac{1}{2+2}&\cdots&\frac{1}{2+n}\\\vdots&\vdots&\ddots&\vdots\\\frac{1}{n+1}&\frac{1}{n+2}&\cdots&\frac{1}{n+n}\end{matrix}\right)$$ My textbook exercise says yes, but I can't prove it.",,['matrices']
79,Matrix norms and spectral radius,Matrix norms and spectral radius,,"Recently I started exploring convergence of some iterative methods and spotted the equivalent of the spectral radius and a matrix norm. For instance, http://www.scribd.com/doc/37323755/36/Richardson-Iteration states in Example 1.28 that 2-norm of a matrix is its spectral radius. On the other hand, What is the difference between the Frobenius norm and the 2-norm of a matrix? states a difference. What is the difference between the Frobenius and 2-norm of a matrix? Is the class of symmetric matrices for which the equality of 2-norm and the spectral radius holds?","Recently I started exploring convergence of some iterative methods and spotted the equivalent of the spectral radius and a matrix norm. For instance, http://www.scribd.com/doc/37323755/36/Richardson-Iteration states in Example 1.28 that 2-norm of a matrix is its spectral radius. On the other hand, What is the difference between the Frobenius norm and the 2-norm of a matrix? states a difference. What is the difference between the Frobenius and 2-norm of a matrix? Is the class of symmetric matrices for which the equality of 2-norm and the spectral radius holds?",,"['linear-algebra', 'matrices', 'normed-spaces', 'spectral-radius']"
80,Invertibility of Hermitian matrix,Invertibility of Hermitian matrix,,"Suppose $G$ is a Hermitian $n \times n$ matrix and $A$ is some $n \times n$ matrix over complex numbers.  such that $G-A^H G A$ is positive-definite. Then can we show that $G$ is invertible? Also, can we conclude anything about the eigenvalues of the matrix $A$ in terms of the eigenvalues of $G$ (for instance, relate the number of positive eigenvalues of $G$ with the eigenvalues of $A$ of norm less than 1)?","Suppose $G$ is a Hermitian $n \times n$ matrix and $A$ is some $n \times n$ matrix over complex numbers.  such that $G-A^H G A$ is positive-definite. Then can we show that $G$ is invertible? Also, can we conclude anything about the eigenvalues of the matrix $A$ in terms of the eigenvalues of $G$ (for instance, relate the number of positive eigenvalues of $G$ with the eigenvalues of $A$ of norm less than 1)?",,"['linear-algebra', 'matrices']"
81,Matrices whose condition number is $1$,Matrices whose condition number is,1,"The condition number of the identity matrix $I$ always equals $1$ . Are there any other matrices that have a condition number equal to $1$ , but are neither the identity matrix nor $\lambda I$ (for any scalar $\lambda$ )? (because if $A$ is a matrix, then $\mbox{cond}(\lambda A) = \mbox{cond}(A)$ )","The condition number of the identity matrix always equals . Are there any other matrices that have a condition number equal to , but are neither the identity matrix nor (for any scalar )? (because if is a matrix, then )",I 1 1 \lambda I \lambda A \mbox{cond}(\lambda A) = \mbox{cond}(A),"['linear-algebra', 'matrices', 'singular-values', 'condition-number']"
82,Is there any known formula for $A^n \mathbf{x}^n + A^{n-1} \mathbf{x}^{n-1} + \ldots +A \mathbf{x} + I$?,Is there any known formula for ?,A^n \mathbf{x}^n + A^{n-1} \mathbf{x}^{n-1} + \ldots +A \mathbf{x} + I,"Good afternoon. My question is, if there is any known formula for the expression $$A^n \mathbf{x}^n + A^{n-1} \mathbf{x}^{n-1} + \ldots+ A \mathbf{x} + I,$$ where $A$ is a matrix and $\mathbf{x}$ is a column vector. By $\mathbf{x}^n$, I denote the element-wise $n$-th power of the column vector $\mathbf{x}$. What I am looking for is something like a closed-form formula. It seems to be likely that something like that exists, since the expression resembles the geometric sum. Thanks a lot in advance.","Good afternoon. My question is, if there is any known formula for the expression $$A^n \mathbf{x}^n + A^{n-1} \mathbf{x}^{n-1} + \ldots+ A \mathbf{x} + I,$$ where $A$ is a matrix and $\mathbf{x}$ is a column vector. By $\mathbf{x}^n$, I denote the element-wise $n$-th power of the column vector $\mathbf{x}$. What I am looking for is something like a closed-form formula. It seems to be likely that something like that exists, since the expression resembles the geometric sum. Thanks a lot in advance.",,"['linear-algebra', 'matrices']"
83,A basic question related with the positive definite matrix,A basic question related with the positive definite matrix,,I have a one doubt related with positive definite matrices. Suppose that we have an arbitrary non zero matrix $A$ . Can we find such matrix $B$ which may depend on $A $such that product $AB$ is always a positive definite matrix irrespective of the nature of matrix $A$?  I need help with this. Thanks,I have a one doubt related with positive definite matrices. Suppose that we have an arbitrary non zero matrix $A$ . Can we find such matrix $B$ which may depend on $A $such that product $AB$ is always a positive definite matrix irrespective of the nature of matrix $A$?  I need help with this. Thanks,,"['linear-algebra', 'matrices']"
84,How can I get matrices for practicing Jordan normal form?,How can I get matrices for practicing Jordan normal form?,,"I would like to practice the algorithm for the transformation from a matrix to its jordan normal form (with change of basis). To do so, I wrote this script that generates random $n \times n$ matrices, with $n \in \{2,3,4,5\}$ import random, numpy  n = random.randint(2,5)  matrix = []  for i in xrange(n):     line = []     for j in xrange(n):         line.append(random.randint(-10, 10))     matrix.append(line)  A = numpy.matrix(matrix)  print(""Here is your %i x %i Matrix"" % (n, n)) print(A) This way of generating random matrices isn't very good for the following reasons: The numbers can get really ugly ( example ) Sometimes it is not possible to calculate the decomposition of the matrix ( example ) Do you know either pages with many examples of ""good"" matrices up to $5 \times 5$ or do you know how to change my script?","I would like to practice the algorithm for the transformation from a matrix to its jordan normal form (with change of basis). To do so, I wrote this script that generates random $n \times n$ matrices, with $n \in \{2,3,4,5\}$ import random, numpy  n = random.randint(2,5)  matrix = []  for i in xrange(n):     line = []     for j in xrange(n):         line.append(random.randint(-10, 10))     matrix.append(line)  A = numpy.matrix(matrix)  print(""Here is your %i x %i Matrix"" % (n, n)) print(A) This way of generating random matrices isn't very good for the following reasons: The numbers can get really ugly ( example ) Sometimes it is not possible to calculate the decomposition of the matrix ( example ) Do you know either pages with many examples of ""good"" matrices up to $5 \times 5$ or do you know how to change my script?",,"['linear-algebra', 'matrices']"
85,Is this vector derivative correct?,Is this vector derivative correct?,,"I want to comprehend the derivative of the cost function in linear regression involving Ridge regularization, the equation is: $$L^{\text{Ridge}}(\beta) = \sum_{i=1}^n (y_i - \phi(x_i)^T\beta)^2 + \lambda \sum_{j=1}^k \beta_j^2$$ Where the sum of squares can be rewritten as: $$L^{}(\beta) = ||y-X\beta||^2 + \lambda \sum_{j=1}^k \beta_j^2$$ For finding the optimum its derivative is set to zero, which leads to this solution: $$\beta^{\text{Ridge}} = (X^TX + \lambda I)^{-1} X^T y$$ Now I would like to understand this and try to derive it myself, heres what I got: Since $||x||^2 = x^Tx$ and  $\frac{\partial}{\partial x} [x^Tx] = 2x^T$ this can be applied by using the chain rule: \begin{align*} \frac{\partial}{\partial \beta} L^{\text{Ridge}}(\beta) = 0^T  &= -2(y - X \beta)^TX + 2 \lambda I\\ 0 &= -2(y - X \beta) X^T + 2 \lambda I\\ 0  &= -2X^Ty + 2X^TX\beta + 2 \lambda I\\ 0  &= -X^Ty + X^TX\beta + 2 \lambda I\\   &= X^TX\beta + 2 \lambda I\\ (X^TX + \lambda I)^{-1} X^Ty &= \beta \end{align*} Where I strugle is the next-to-last equation, I multiply it with $(X^TX + \lambda I)^{-1}$ and I don't think that leads to a correct equation. What have I done wrong?","I want to comprehend the derivative of the cost function in linear regression involving Ridge regularization, the equation is: $$L^{\text{Ridge}}(\beta) = \sum_{i=1}^n (y_i - \phi(x_i)^T\beta)^2 + \lambda \sum_{j=1}^k \beta_j^2$$ Where the sum of squares can be rewritten as: $$L^{}(\beta) = ||y-X\beta||^2 + \lambda \sum_{j=1}^k \beta_j^2$$ For finding the optimum its derivative is set to zero, which leads to this solution: $$\beta^{\text{Ridge}} = (X^TX + \lambda I)^{-1} X^T y$$ Now I would like to understand this and try to derive it myself, heres what I got: Since $||x||^2 = x^Tx$ and  $\frac{\partial}{\partial x} [x^Tx] = 2x^T$ this can be applied by using the chain rule: \begin{align*} \frac{\partial}{\partial \beta} L^{\text{Ridge}}(\beta) = 0^T  &= -2(y - X \beta)^TX + 2 \lambda I\\ 0 &= -2(y - X \beta) X^T + 2 \lambda I\\ 0  &= -2X^Ty + 2X^TX\beta + 2 \lambda I\\ 0  &= -X^Ty + X^TX\beta + 2 \lambda I\\   &= X^TX\beta + 2 \lambda I\\ (X^TX + \lambda I)^{-1} X^Ty &= \beta \end{align*} Where I strugle is the next-to-last equation, I multiply it with $(X^TX + \lambda I)^{-1}$ and I don't think that leads to a correct equation. What have I done wrong?",,"['matrices', 'derivatives', 'vector-analysis']"
86,Characteristic equation for 2-nd order ODE,Characteristic equation for 2-nd order ODE,,"Given a differential equation $\dot x = Ax$, $x \in \mathbb{R}^n$ we define its characteristic equation as $\chi(\lambda) = \det (\lambda I - A)$. Consider now the second order ODE $$   \ddot x + A x + B \dot x = 0, \;\;\; x \in \mathbb{R}^n $$ With substitution $u = x$, $v = \dot x$ we can rewrite this ODE as a system $$   \left\{\begin{array}{rcl}   \dot u & = & v \\   \dot v & = & -Au - Bv.   \end{array}\right. $$ This is an ODE with matrix $$   \begin{bmatrix}      0 & I \\      -A & -B   \end{bmatrix} $$ and hence with characteristic equation $$    \chi(\lambda) = \det \begin{bmatrix} \lambda I & - I \\ A & \lambda  I+B\end{bmatrix} . $$ I know that there is a representation of such determinant as a determinant of $n \times n$ matrix with $a_{ii} + \lambda^2$ on the diagonal. How to obtain such representation? How to find off-diagonal items explicitly?","Given a differential equation $\dot x = Ax$, $x \in \mathbb{R}^n$ we define its characteristic equation as $\chi(\lambda) = \det (\lambda I - A)$. Consider now the second order ODE $$   \ddot x + A x + B \dot x = 0, \;\;\; x \in \mathbb{R}^n $$ With substitution $u = x$, $v = \dot x$ we can rewrite this ODE as a system $$   \left\{\begin{array}{rcl}   \dot u & = & v \\   \dot v & = & -Au - Bv.   \end{array}\right. $$ This is an ODE with matrix $$   \begin{bmatrix}      0 & I \\      -A & -B   \end{bmatrix} $$ and hence with characteristic equation $$    \chi(\lambda) = \det \begin{bmatrix} \lambda I & - I \\ A & \lambda  I+B\end{bmatrix} . $$ I know that there is a representation of such determinant as a determinant of $n \times n$ matrix with $a_{ii} + \lambda^2$ on the diagonal. How to obtain such representation? How to find off-diagonal items explicitly?",,"['matrices', 'ordinary-differential-equations', 'determinant']"
87,Matrix transformation mapped onto itself,Matrix transformation mapped onto itself,,"Question: $$A= \begin{pmatrix} k & -2 \\ 1-k & k \end{pmatrix}\text{, where k is a constant}$$ $$\text {A transformation } T : \mathbb{R}^2  \rightarrow  \mathbb{R}^2 \text{ is  represented by the matrix A.}$$ $$\text {Find the value of k for which the line } y = 2x \text{ is mapped onto itself under T.}$$ Working: $$\begin{pmatrix} k & -2 \\ 1-k & k \end{pmatrix}\cdot\begin{pmatrix} x \\ 2x \end{pmatrix}=\begin{pmatrix} x \\ 2x \end{pmatrix}$$ $$\begin{pmatrix} x(k-4) \\ x(1+k)  \end{pmatrix}=\begin{pmatrix} x \\ 2x \end{pmatrix}$$ $$x(k-4)=x$$ $$x(1+k)=2x$$ Leaving me with $k=5$ and $k=1$, However the answer is $k = 9$ why?","Question: $$A= \begin{pmatrix} k & -2 \\ 1-k & k \end{pmatrix}\text{, where k is a constant}$$ $$\text {A transformation } T : \mathbb{R}^2  \rightarrow  \mathbb{R}^2 \text{ is  represented by the matrix A.}$$ $$\text {Find the value of k for which the line } y = 2x \text{ is mapped onto itself under T.}$$ Working: $$\begin{pmatrix} k & -2 \\ 1-k & k \end{pmatrix}\cdot\begin{pmatrix} x \\ 2x \end{pmatrix}=\begin{pmatrix} x \\ 2x \end{pmatrix}$$ $$\begin{pmatrix} x(k-4) \\ x(1+k)  \end{pmatrix}=\begin{pmatrix} x \\ 2x \end{pmatrix}$$ $$x(k-4)=x$$ $$x(1+k)=2x$$ Leaving me with $k=5$ and $k=1$, However the answer is $k = 9$ why?",,['matrices']
88,Matrices of Trace $0$,Matrices of Trace,0,The set of all $n$-square matrices with trace $0$ is a subspace of the set of all $n$-square matrices. Is there a standard notation and/or name for this subspace?,The set of all $n$-square matrices with trace $0$ is a subspace of the set of all $n$-square matrices. Is there a standard notation and/or name for this subspace?,,"['linear-algebra', 'matrices', 'notation', 'terminology']"
89,Strassen Multiplication?,Strassen Multiplication?,,"How are the values of the 7 new matrices derived? I'm referring to the values that reduce matrix multiplication to 7 multiplications per level: $M_1 = \left(A_{1,1} + A_{2,2}\right)\left(B_{1,1} + B_{2,2}\right)$ ... $M_7 = \left(A_{1,2} - A_{2,2}\right)\left(B_{2,1} + B_{2,2}\right)$ Could someone show how these values were arrived at?","How are the values of the 7 new matrices derived? I'm referring to the values that reduce matrix multiplication to 7 multiplications per level: $M_1 = \left(A_{1,1} + A_{2,2}\right)\left(B_{1,1} + B_{2,2}\right)$ ... $M_7 = \left(A_{1,2} - A_{2,2}\right)\left(B_{2,1} + B_{2,2}\right)$ Could someone show how these values were arrived at?",,[]
90,How to find out the dimension of a given vector space?,How to find out the dimension of a given vector space?,,What will be the dimension of a vector space $ V =\{ a_{ij}\in \mathbb{C_{n\times n}} : a_{ij}=-a_{ji} \}$ over field $\mathbb{R}$  and over field $\mathbb{C}$?,What will be the dimension of a vector space $ V =\{ a_{ij}\in \mathbb{C_{n\times n}} : a_{ij}=-a_{ji} \}$ over field $\mathbb{R}$  and over field $\mathbb{C}$?,,"['linear-algebra', 'matrices']"
91,Projection matrix and Eigenvalue,Projection matrix and Eigenvalue,,Would like to have some guidance. $P$ is projection matrix on $U$ and $0\notin v\notin \mathbb{R}^2$ I need to show that if $v$ is element of $U$ than $v$ is Eigenvector of $P$ with Eigenvalue 1. I know that for projection matrix Eigenvalue is $1$ or $0$... but why in this case only $1$?,Would like to have some guidance. $P$ is projection matrix on $U$ and $0\notin v\notin \mathbb{R}^2$ I need to show that if $v$ is element of $U$ than $v$ is Eigenvector of $P$ with Eigenvalue 1. I know that for projection matrix Eigenvalue is $1$ or $0$... but why in this case only $1$?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
92,Characterizing matrices over a division ring as a product of a simple module?,Characterizing matrices over a division ring as a product of a simple module?,,"I was reading about semisimple modules, and there's a fact relating to simple modules that I don't recall. Suppose you have a ring $R=M_n(D)$, by which I mean the $n\times n$ ring of matrices with entries in a division ring $D$. Then actually, $R\cong M^n$ for some simple module $M$, and this $M$ is unique up to isomorphism. I'm hoping to understand this before proceeding, but I don't see this isomorphism, nor why such a module is unique up to isomorphism. Can someone please explain? A kind thanks all.","I was reading about semisimple modules, and there's a fact relating to simple modules that I don't recall. Suppose you have a ring $R=M_n(D)$, by which I mean the $n\times n$ ring of matrices with entries in a division ring $D$. Then actually, $R\cong M^n$ for some simple module $M$, and this $M$ is unique up to isomorphism. I'm hoping to understand this before proceeding, but I don't see this isomorphism, nor why such a module is unique up to isomorphism. Can someone please explain? A kind thanks all.",,"['abstract-algebra', 'matrices', 'modules']"
93,Natural number matrix solutions to $\sigma_i\sigma_j+\sigma_j\sigma_i = I\delta_{ij}$,Natural number matrix solutions to,\sigma_i\sigma_j+\sigma_j\sigma_i = I\delta_{ij},"Given the two matrices: $\sigma_i$ and $\sigma_j$ we can construct a Clifford algebra based on the anti commutator rule: $$\{\sigma_i,\sigma_j\}=\delta_{ij}1$$ where $\delta_{ij}$ is the Kronecker symbol. The question is: if the matrices are $(N\times N)$ and their elements are Natural numbers, how many matrices vs. $N$ can I find satisfying the anti commutator equation?  I would appreciate any suggestion.","Given the two matrices: $\sigma_i$ and $\sigma_j$ we can construct a Clifford algebra based on the anti commutator rule: $$\{\sigma_i,\sigma_j\}=\delta_{ij}1$$ where $\delta_{ij}$ is the Kronecker symbol. The question is: if the matrices are $(N\times N)$ and their elements are Natural numbers, how many matrices vs. $N$ can I find satisfying the anti commutator equation?  I would appreciate any suggestion.",,"['matrices', 'clifford-algebras']"
94,Differentiating a function with respect to a vector,Differentiating a function with respect to a vector,,"I need to differentiate the function $u$ shown below with respect to a vector $\psi$: ($a, c$ and $f$ are constants) $$u(\psi) =\left[\begin{array}{cccc} a & f & 0 & 0\\ c & a & f & 0\\ 0 & c & a & f\\ 0 & 0 & c & a \end{array}\right]\left[\begin{array}{c} \psi^{1}\\ \psi^{2}\\ \psi^{3}\\ \psi^{4} \end{array}\right]$$ I'm thinking that the answer would be:  $\left[\begin{array}{cccc} a & f & 0 & 0\\ c & a & f & 0\\ 0 & c & a & f\\ 0 & 0 & c & a \end{array}\right]$ by working out the derivative of each term of the matrix with respect to its corresponding $\psi$. But I was hesitant that the function could be written as: $u=\left[\begin{array}{c} a\psi^{1}+f\psi^{2}\\ c\psi^{1}+a\psi^{2}+f\psi^{3}\\ c\psi^{2}+a\psi^{3}+f\psi^{4}\\ c\psi^{3}+a\psi^{4} \end{array}\right]$ and hence its derivative will just be: $\left[\begin{array}{c} a\\ a\\ a\\ a \end{array}\right]$, if the rows were differentiated with respect to only the corresponding $\psi$. Please let me know which is correct. I think the first one, but need to be sure.","I need to differentiate the function $u$ shown below with respect to a vector $\psi$: ($a, c$ and $f$ are constants) $$u(\psi) =\left[\begin{array}{cccc} a & f & 0 & 0\\ c & a & f & 0\\ 0 & c & a & f\\ 0 & 0 & c & a \end{array}\right]\left[\begin{array}{c} \psi^{1}\\ \psi^{2}\\ \psi^{3}\\ \psi^{4} \end{array}\right]$$ I'm thinking that the answer would be:  $\left[\begin{array}{cccc} a & f & 0 & 0\\ c & a & f & 0\\ 0 & c & a & f\\ 0 & 0 & c & a \end{array}\right]$ by working out the derivative of each term of the matrix with respect to its corresponding $\psi$. But I was hesitant that the function could be written as: $u=\left[\begin{array}{c} a\psi^{1}+f\psi^{2}\\ c\psi^{1}+a\psi^{2}+f\psi^{3}\\ c\psi^{2}+a\psi^{3}+f\psi^{4}\\ c\psi^{3}+a\psi^{4} \end{array}\right]$ and hence its derivative will just be: $\left[\begin{array}{c} a\\ a\\ a\\ a \end{array}\right]$, if the rows were differentiated with respect to only the corresponding $\psi$. Please let me know which is correct. I think the first one, but need to be sure.",,"['calculus', 'matrices', 'derivatives', 'vector-analysis']"
95,Determinant of a special kind of block matrix,Determinant of a special kind of block matrix,,I have a $2\times2$ block matrix $M$ defined as follows: $$\begin{pmatrix}X+|X| & X-|X| \\  Y-|Y| & Y+|Y|\end{pmatrix}$$ where $X$ and $Y$ are $n\times n$ matrices and $|X|$ denotes the modulus of the entire matrix $X$ that essentially comprises modulus of individual elements of $X$. How may I find the determinant of the matrix $M$ in terms of $X$ and $Y$? Looking for a simplified solution?,I have a $2\times2$ block matrix $M$ defined as follows: $$\begin{pmatrix}X+|X| & X-|X| \\  Y-|Y| & Y+|Y|\end{pmatrix}$$ where $X$ and $Y$ are $n\times n$ matrices and $|X|$ denotes the modulus of the entire matrix $X$ that essentially comprises modulus of individual elements of $X$. How may I find the determinant of the matrix $M$ in terms of $X$ and $Y$? Looking for a simplified solution?,,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
96,How can I compare two Markov processes?,How can I compare two Markov processes?,,"There is a discrete-time irreductible Markov process with $r$ possible states.    $k$ observations were performed. At each observation a state of process was determined. $T_0 = \lbrace 0,1,\dots ,k-1\rbrace$ $T_1 = \lbrace 1,2,\dots ,k-1\rbrace$ $n_i(t) = 1$ means that process was in state $i$ at time $t$, ($t \in T_0$), $n_i(t) = 0$ otherwise, $v_{ij}(t) = 1$ means that process changed from state $i$ to state $j$ between time $t-1$ and $t$, ($t \in T_1$),  $v_{ij}(t) = 0$ otherwise. I estimate probability of transition from state $i$ to state $j$ as $\hat{p}_{ij} = \frac{\displaystyle\sum_{t \in T_1}v_{ij}(t)}{\displaystyle\sum\limits_{t \in T_1}n_i(t-1)}$ First question: How many observations do I need to properly estimate transition matrix? If $p_{ij}$ is an ideal estimation, I don't want relative error between $\hat{p}_{ij}$ and $p_{ij}$ ( $\frac{|\hat{p}_{ij}-p_{ij}|}{p_{ij}}$ ) to be more than $\varepsilon = 0.01$ with probability $P \ge 0.99$. I need this in form $k=k(r,\varepsilon, P)$. I know that the  number will be large but this does not matter. I just want to know the number. For my purposes $r$ will be between 7 and 15, $\forall_i p_{ii} \neq 1$. If there would be a state $i$ that was never entered or was entered at the last step (what could be written as $\sum\limits_{t \in T_1}n_i(t-1)=0$), I will just simply exclude that state from further calculations. Second question: I have first transition matrix $A$. That matrix describes ""model"" process and was estimated with sufficient number of observations. Then, second process was observed and new matrix $B$ was estimated (with sufficient number of observations). How can I decide if second process matches ""model"" process? I want this method to have an ""error factor"" (I don't know how to name it) witch changeable value $\varphi$. I need 3 or 4 of those methods for comparison.","There is a discrete-time irreductible Markov process with $r$ possible states.    $k$ observations were performed. At each observation a state of process was determined. $T_0 = \lbrace 0,1,\dots ,k-1\rbrace$ $T_1 = \lbrace 1,2,\dots ,k-1\rbrace$ $n_i(t) = 1$ means that process was in state $i$ at time $t$, ($t \in T_0$), $n_i(t) = 0$ otherwise, $v_{ij}(t) = 1$ means that process changed from state $i$ to state $j$ between time $t-1$ and $t$, ($t \in T_1$),  $v_{ij}(t) = 0$ otherwise. I estimate probability of transition from state $i$ to state $j$ as $\hat{p}_{ij} = \frac{\displaystyle\sum_{t \in T_1}v_{ij}(t)}{\displaystyle\sum\limits_{t \in T_1}n_i(t-1)}$ First question: How many observations do I need to properly estimate transition matrix? If $p_{ij}$ is an ideal estimation, I don't want relative error between $\hat{p}_{ij}$ and $p_{ij}$ ( $\frac{|\hat{p}_{ij}-p_{ij}|}{p_{ij}}$ ) to be more than $\varepsilon = 0.01$ with probability $P \ge 0.99$. I need this in form $k=k(r,\varepsilon, P)$. I know that the  number will be large but this does not matter. I just want to know the number. For my purposes $r$ will be between 7 and 15, $\forall_i p_{ii} \neq 1$. If there would be a state $i$ that was never entered or was entered at the last step (what could be written as $\sum\limits_{t \in T_1}n_i(t-1)=0$), I will just simply exclude that state from further calculations. Second question: I have first transition matrix $A$. That matrix describes ""model"" process and was estimated with sufficient number of observations. Then, second process was observed and new matrix $B$ was estimated (with sufficient number of observations). How can I decide if second process matches ""model"" process? I want this method to have an ""error factor"" (I don't know how to name it) witch changeable value $\varphi$. I need 3 or 4 of those methods for comparison.",,"['matrices', 'markov-chains', 'parameter-estimation']"
97,Elementary matrices,Elementary matrices,,Is there a way to visualize the action of elementary matrices? (Or perhaps matrices in general). Perhaps someone could give an intuitive view of the effects of elementary row operations. Actually I am trying to intuitively understand why these elementary matrices general $GL_n$. Thanks.,Is there a way to visualize the action of elementary matrices? (Or perhaps matrices in general). Perhaps someone could give an intuitive view of the effects of elementary row operations. Actually I am trying to intuitively understand why these elementary matrices general $GL_n$. Thanks.,,"['linear-algebra', 'matrices', 'transformation']"
98,Finding the column space from nullspace and particular solution,Finding the column space from nullspace and particular solution,,"Let $A$ be a $3 \times 4$ matrix and it has a particular solution of $\begin{bmatrix} x_1\\  x_2\\  x_3\\  x_4 \end{bmatrix}= \begin{bmatrix} 1\\  0\\  -1\\  0 \end{bmatrix}$ The nullspace of $A$ is $\begin{bmatrix} x_1\\  x_2\\  x_3\\  x_4 \end{bmatrix}= t\begin{bmatrix} 1\\  1\\  0\\  1 \end{bmatrix}+ s\begin{bmatrix} -2\\  1\\  1\\  0 \end{bmatrix}$, where $t,s \in \mathbb{R}$. From this, do I have enough information to find the column space of $A$? I am thinking that I could because I can find out the reduced row echelon form of $A$ by working backwards from the nullspace. And I could get the $rref(A)=\begin{bmatrix} 1 & 0 & 2 & -1\\  0 &  1& -1 & -1\\  0 & 0 & 0 & 0 \end{bmatrix}$. From here, isn't the column space of $A$ just the $span(\begin{bmatrix} 1\\  0\\  0 \end{bmatrix}, \begin{bmatrix} 0\\  1\\  0 \end{bmatrix})$? We could also determine that the left-nullspace is going to be of rank 1. But from the book that I am reading, it says that I should not have enough information to find the column space of $A$. Why is it so? Isn't the span I wrote above the column space of $A$ already?","Let $A$ be a $3 \times 4$ matrix and it has a particular solution of $\begin{bmatrix} x_1\\  x_2\\  x_3\\  x_4 \end{bmatrix}= \begin{bmatrix} 1\\  0\\  -1\\  0 \end{bmatrix}$ The nullspace of $A$ is $\begin{bmatrix} x_1\\  x_2\\  x_3\\  x_4 \end{bmatrix}= t\begin{bmatrix} 1\\  1\\  0\\  1 \end{bmatrix}+ s\begin{bmatrix} -2\\  1\\  1\\  0 \end{bmatrix}$, where $t,s \in \mathbb{R}$. From this, do I have enough information to find the column space of $A$? I am thinking that I could because I can find out the reduced row echelon form of $A$ by working backwards from the nullspace. And I could get the $rref(A)=\begin{bmatrix} 1 & 0 & 2 & -1\\  0 &  1& -1 & -1\\  0 & 0 & 0 & 0 \end{bmatrix}$. From here, isn't the column space of $A$ just the $span(\begin{bmatrix} 1\\  0\\  0 \end{bmatrix}, \begin{bmatrix} 0\\  1\\  0 \end{bmatrix})$? We could also determine that the left-nullspace is going to be of rank 1. But from the book that I am reading, it says that I should not have enough information to find the column space of $A$. Why is it so? Isn't the span I wrote above the column space of $A$ already?",,"['linear-algebra', 'matrices']"
99,Counting paths of a variable length on a directed graph,Counting paths of a variable length on a directed graph,,"If I've been given a directed Graph $G = (V,E)$ and its adjacency matrix $A$. How do I calculate how many (directed) paths do exist from one specific node to another (specific) one? In general $a_{v,w}^{(n)}$ which is the n -th power of the entry $a_{v,w}$ of $A$ is the answer I am looking for (for the amount of pathes of the length $n$ from node $v$ to node $w$). But how do I determine an explicit formula for a variable n? Thank you in advance!","If I've been given a directed Graph $G = (V,E)$ and its adjacency matrix $A$. How do I calculate how many (directed) paths do exist from one specific node to another (specific) one? In general $a_{v,w}^{(n)}$ which is the n -th power of the entry $a_{v,w}$ of $A$ is the answer I am looking for (for the amount of pathes of the length $n$ from node $v$ to node $w$). But how do I determine an explicit formula for a variable n? Thank you in advance!",,"['matrices', 'graph-theory']"
