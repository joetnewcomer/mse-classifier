,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Book recommendation on the history of PDE/ODE?,Book recommendation on the history of PDE/ODE?,,I would like to know something like what's the first PDE etc. Could you recommend book on the history of PDE/ODE? thanks.,I would like to know something like what's the first PDE etc. Could you recommend book on the history of PDE/ODE? thanks.,,"['reference-request', 'math-history', 'ordinary-differential-equations']"
1,Deriving the equation of a parabola?,Deriving the equation of a parabola?,,"Imagine you wanted to derive the equation of a parabola by imposing that all parallel rays that bounce on the parabola, end up in the foci of the curve. By using geometry and vectors, I arrived to something like: $$ F = (a, b) \\ -\sqrt{(a - x)^2 + (b - y(x))^2} = (a - x)y'(x) + y(x) + b \Rightarrow \\ (a - x)^2 y'(x)^2 + 2(a - x)y'(x)y(x) + 2(a - x)y'(x)b = a^2 + x^2 +2ax $$ I've just imposing that the $\cos{\theta}$ has to be equal when the ray bounces (using dot products): This is far from what I was expecting: $$ y''(x) = k $$ For $a = b = 0$ we have the following: $$ xy'(x)^2 - 2y'(x)y(x) = x $$ This is a pretty intimidating equation. My question is: Is this line of reasoning correct? Is this strange looking equation right? Is there any other way of doing this more efficiently?","Imagine you wanted to derive the equation of a parabola by imposing that all parallel rays that bounce on the parabola, end up in the foci of the curve. By using geometry and vectors, I arrived to something like: I've just imposing that the has to be equal when the ray bounces (using dot products): This is far from what I was expecting: For we have the following: This is a pretty intimidating equation. My question is: Is this line of reasoning correct? Is this strange looking equation right? Is there any other way of doing this more efficiently?","
F = (a, b) \\
-\sqrt{(a - x)^2 + (b - y(x))^2} = (a - x)y'(x) + y(x) + b \Rightarrow \\
(a - x)^2 y'(x)^2 + 2(a - x)y'(x)y(x) + 2(a - x)y'(x)b = a^2 + x^2 +2ax
 \cos{\theta} 
y''(x) = k
 a = b = 0 
xy'(x)^2 - 2y'(x)y(x) = x
",['ordinary-differential-equations']
2,Transform ODE system of first order into ODE of $n$-th order,Transform ODE system of first order into ODE of -th order,n,"I know that you can always transform a higher order ODE into a system of first order ODEs. In our lecture the professor made the statement that the reverse, namely transforming a given system of first order ODEs into one ODE of a higher order, is not always possible. I was a bit confused by this statement because as far as I have understood the reverse is only possible in the very rare case where \begin{align} y_0'&=y_1\\ y_1'&=y_2\\ &\;\vdots\\ y_{n-1}'&=y_n\\ y_n'&=f(x,y_0,y_1, \cdots, y_n). \end{align} If the given system doesn't have this form you can't transform it into an ODE of $n$ -th order. Is this correct? Did I miss something?","I know that you can always transform a higher order ODE into a system of first order ODEs. In our lecture the professor made the statement that the reverse, namely transforming a given system of first order ODEs into one ODE of a higher order, is not always possible. I was a bit confused by this statement because as far as I have understood the reverse is only possible in the very rare case where If the given system doesn't have this form you can't transform it into an ODE of -th order. Is this correct? Did I miss something?","\begin{align}
y_0'&=y_1\\
y_1'&=y_2\\
&\;\vdots\\
y_{n-1}'&=y_n\\
y_n'&=f(x,y_0,y_1, \cdots, y_n).
\end{align} n","['real-analysis', 'ordinary-differential-equations']"
3,Second order linear ODE with nonconstant coefficients: Why can we not use the ansatz $e^{\lambda x}$?,Second order linear ODE with nonconstant coefficients: Why can we not use the ansatz ?,e^{\lambda x},"For a constant-coefficient case, it is guaranteed that the solutions we get from taking $e^{\lambda x}$ to be the ansatz and finding out the value of $\lambda$ are valid. Generally, for a homogeneous equation $$ay''+by'+cy=0$$ it is equivalent to taking the linear differential operator as $$\mathcal{L}=aD^2 + bD + c$$ where $D = d/dx$ , and saying that $$\mathcal{L}y=0$$ So I would say that $e^{\lambda x}$ is a basis for the eigenspace of $\mathcal{L}$ which corresponds to the eigenvalue of $$\Lambda=a\lambda^2 + b\lambda +c$$ henceforth making the equation $\mathcal{L}y=\Lambda y$ . However, this method does not work for a nonconstant-coefficient case. But I don't see why it doesn't work. Instead of talking about general cases, I will use a specific example from now on. Consider the equation $$y''+xy'+y=0$$ I found that it is entirely valid up to carrying out the characteristic equation. $$\lambda^2 e^{\lambda x} + x \lambda e^{\lambda x} + e^{\lambda x}=0$$ $$(\lambda^2+x\lambda+1)e^{\lambda x}=0$$ Since $e^{\lambda x}>0$ , $$\lambda^2+x\lambda+1 = 0$$ Then we can find $\lambda$ using the quadratic formula ( admitting that $\lambda$ is a function of $x$ ): $$\lambda_1(x) = \frac{-x + \sqrt{x^2-4}}{2},\ \lambda_2(x) = \frac{-x - \sqrt{x^2-4}}{2}$$ Since the equation is linear, we can guarantee that the span of $\{e^{\lambda_1 x}, e^{\lambda_2 x}\}$ is a solution. Thus, $$y(x) = e^{-x/2} \times [Ae^{+\frac{\sqrt{x^2-4}}{2}x} + Be^{-\frac{\sqrt{x^2-4}}{2}x}]$$ Except for the fact that the eigenvalue $\lambda$ is a function of $x$ there is nothing peculiar about it. I thought it is fine, because what we are taking as ""vectors"" is the solution function $y$ , not the independent variable $x$ underlying at the back of this function.","For a constant-coefficient case, it is guaranteed that the solutions we get from taking to be the ansatz and finding out the value of are valid. Generally, for a homogeneous equation it is equivalent to taking the linear differential operator as where , and saying that So I would say that is a basis for the eigenspace of which corresponds to the eigenvalue of henceforth making the equation . However, this method does not work for a nonconstant-coefficient case. But I don't see why it doesn't work. Instead of talking about general cases, I will use a specific example from now on. Consider the equation I found that it is entirely valid up to carrying out the characteristic equation. Since , Then we can find using the quadratic formula ( admitting that is a function of ): Since the equation is linear, we can guarantee that the span of is a solution. Thus, Except for the fact that the eigenvalue is a function of there is nothing peculiar about it. I thought it is fine, because what we are taking as ""vectors"" is the solution function , not the independent variable underlying at the back of this function.","e^{\lambda x} \lambda ay''+by'+cy=0 \mathcal{L}=aD^2 + bD + c D = d/dx \mathcal{L}y=0 e^{\lambda x} \mathcal{L} \Lambda=a\lambda^2 + b\lambda +c \mathcal{L}y=\Lambda y y''+xy'+y=0 \lambda^2 e^{\lambda x} + x \lambda e^{\lambda x} + e^{\lambda x}=0 (\lambda^2+x\lambda+1)e^{\lambda x}=0 e^{\lambda x}>0 \lambda^2+x\lambda+1 = 0 \lambda \lambda x \lambda_1(x) = \frac{-x + \sqrt{x^2-4}}{2},\ \lambda_2(x) = \frac{-x - \sqrt{x^2-4}}{2} \{e^{\lambda_1 x}, e^{\lambda_2 x}\} y(x) = e^{-x/2} \times [Ae^{+\frac{\sqrt{x^2-4}}{2}x} + Be^{-\frac{\sqrt{x^2-4}}{2}x}] \lambda x y x","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'solution-verification', 'eigenfunctions']"
4,Euler method fails,Euler method fails,,"Consider the differential equation $$y'=y-2e^{-x},$$ with initial value $y(0)=1$ . It's not hard to see that $y=e^{-x}$ is the solution. I am trying to solve this numerically via the forward Euler method. However, no matter how I shrink my step size $h$ , the error of my numerical solution always diverges: in fact, the error at $x$ seems to be growing like $e^x$ . My question is why does the numerical solution error always diverge? The numerical solution from the Euler method can be expressed in the form $$y_n=A(1+h)^n+Be^{-hn},$$ for appropriate constants $A,B$ . I have a feeling that divergence has something to do with the fact that $|1+h|>1$ , but I'm not sure how to formalize this. As a follow-up, which numerical methods will solve this DE without the error blowing up?","Consider the differential equation with initial value . It's not hard to see that is the solution. I am trying to solve this numerically via the forward Euler method. However, no matter how I shrink my step size , the error of my numerical solution always diverges: in fact, the error at seems to be growing like . My question is why does the numerical solution error always diverge? The numerical solution from the Euler method can be expressed in the form for appropriate constants . I have a feeling that divergence has something to do with the fact that , but I'm not sure how to formalize this. As a follow-up, which numerical methods will solve this DE without the error blowing up?","y'=y-2e^{-x}, y(0)=1 y=e^{-x} h x e^x y_n=A(1+h)^n+Be^{-hn}, A,B |1+h|>1","['ordinary-differential-equations', 'numerical-methods']"
5,Characterization of Plane Curves *via* Curvature $\kappa(s)$ or Equal Curvature Curves are Congruent,Characterization of Plane Curves *via* Curvature  or Equal Curvature Curves are Congruent,\kappa(s),"My engagement with this topic was piqued by this question , in which the OP MathMan was seeking help in applying the principle that two plane curves with identical curvature function (I will make this more precise in what follows) are themselves identical ""except for probably their position in $\Bbb R^2$ "" ( sic ).  As I attempted to answer MathMan's concerns, I became more and more aware that the underlying concept was worthy of address in and of itself.  Specifically, I began to wonder just how it might be proved.  In particular, I wanted, and still want, an analysis/proof of the assertion that two curves the curvature functions of which are the same are ""congruent"" in the sense that one may be made pointwise identical to the other by a rigid motion of $\Bbb R^2$ .  In formulating a precise statement of this result, I searched math.stackexchange.com in the usual manner for related questions, but found nothing which seemed exactly on point, so I am proceeding to ask it here. Having said these things, I turn to my Question: Let $I \subseteq \Bbb R \tag 1$ be an open interval, not necesarily bounded, and let $\alpha, \beta: I \to \Bbb R^2 \tag 2$ be regular, arc-length parametrized curves with curvatures $\kappa_\alpha, \kappa_\beta: I \to \Bbb R^+ = \{r \in \Bbb R, \; r > 0 \}, \tag 3$ as defined in the Frenet-Serret equations , $\dot T_\alpha(s) = \kappa_\alpha(s) N_\alpha(s), \; \dot T_\beta(s) = \kappa_\beta(s) N_\beta(s), \tag 4$ where $N_\alpha(s)$ and $N_\beta(s)$ are the unit normal fields to $\alpha(s)$ and $\beta(s)$ , respectively.  Then if $\kappa_\alpha(s) = \kappa_\beta(s), \; \forall s \in I, \tag 5$ it follows that there is an orthogonal transformation $O$ of $\Bbb R^2$ and a vector $\vec v \in \Bbb R^2 \tag 6$ such that $\alpha(s) = O\beta(s) + \vec v, \; \forall s \in I. \tag 7$","My engagement with this topic was piqued by this question , in which the OP MathMan was seeking help in applying the principle that two plane curves with identical curvature function (I will make this more precise in what follows) are themselves identical ""except for probably their position in "" ( sic ).  As I attempted to answer MathMan's concerns, I became more and more aware that the underlying concept was worthy of address in and of itself.  Specifically, I began to wonder just how it might be proved.  In particular, I wanted, and still want, an analysis/proof of the assertion that two curves the curvature functions of which are the same are ""congruent"" in the sense that one may be made pointwise identical to the other by a rigid motion of .  In formulating a precise statement of this result, I searched math.stackexchange.com in the usual manner for related questions, but found nothing which seemed exactly on point, so I am proceeding to ask it here. Having said these things, I turn to my Question: Let be an open interval, not necesarily bounded, and let be regular, arc-length parametrized curves with curvatures as defined in the Frenet-Serret equations , where and are the unit normal fields to and , respectively.  Then if it follows that there is an orthogonal transformation of and a vector such that","\Bbb R^2 \Bbb R^2 I \subseteq \Bbb R \tag 1 \alpha, \beta: I \to \Bbb R^2 \tag 2 \kappa_\alpha, \kappa_\beta: I \to \Bbb R^+ = \{r \in \Bbb R, \; r > 0 \}, \tag 3 \dot T_\alpha(s) = \kappa_\alpha(s) N_\alpha(s), \; \dot T_\beta(s) = \kappa_\beta(s) N_\beta(s), \tag 4 N_\alpha(s) N_\beta(s) \alpha(s) \beta(s) \kappa_\alpha(s) = \kappa_\beta(s), \; \forall s \in I, \tag 5 O \Bbb R^2 \vec v \in \Bbb R^2 \tag 6 \alpha(s) = O\beta(s) + \vec v, \; \forall s \in I. \tag 7","['ordinary-differential-equations', 'differential-geometry', 'curves', 'curvature']"
6,Why are exact differential equations called so?,Why are exact differential equations called so?,,"As the question says, why are exact differential equations called so? From Wikipedia, I got ""The nomenclature of ""exact differential equation"" refers to the exact differential of a function"". That leads me to ask why is an exact differential called so? Usually, the term 'exact' in the context of English refers to some quantity that is not approximated in any way. How does that definition fit here?","As the question says, why are exact differential equations called so? From Wikipedia, I got ""The nomenclature of ""exact differential equation"" refers to the exact differential of a function"". That leads me to ask why is an exact differential called so? Usually, the term 'exact' in the context of English refers to some quantity that is not approximated in any way. How does that definition fit here?",,"['ordinary-differential-equations', 'terminology', 'math-history']"
7,Separation of variables -- what to do about the case where we might be dividing by zero?,Separation of variables -- what to do about the case where we might be dividing by zero?,,"In not a few textbooks, we have some version of this example of a falling body: Problem. Solve $\frac{dv}{dt} = a - bv$ with the initial condition $(t,v)=(0,0)$ . Solution. Rearrange to get $\frac{dt}{dv} \overset 1= \frac{1}{a - bv}.$ Integrating, $t=-\frac{1}{b}\ln|a-bv|+C_1$ . Rearranging, $e^{b(C_1-t)}=|a-bv|$ or $a-bv=\pm e^{b(C_1-t)}=C_2 e^{-bt}$ . Plug in the initial condition to get $C_2=a$ and $a-bv=ae^{-bt}$ . Now rearrange to get the solution: $v=\frac{a}{b}(1-e^{-bt}).$ My question is: At $\overset 1=$ , we assume $a-bv\neq0$ ---so, how do we handle the case where $a-bv=0$ ? (Do we for example simply assert that this never happens? How do we justify this assertion? Or is there some other more careful/precise way of dealing with $a-bv=0$ ?)","In not a few textbooks, we have some version of this example of a falling body: Problem. Solve with the initial condition . Solution. Rearrange to get Integrating, . Rearranging, or . Plug in the initial condition to get and . Now rearrange to get the solution: My question is: At , we assume ---so, how do we handle the case where ? (Do we for example simply assert that this never happens? How do we justify this assertion? Or is there some other more careful/precise way of dealing with ?)","\frac{dv}{dt} = a - bv (t,v)=(0,0) \frac{dt}{dv} \overset 1= \frac{1}{a - bv}. t=-\frac{1}{b}\ln|a-bv|+C_1 e^{b(C_1-t)}=|a-bv| a-bv=\pm e^{b(C_1-t)}=C_2 e^{-bt} C_2=a a-bv=ae^{-bt} v=\frac{a}{b}(1-e^{-bt}). \overset 1= a-bv\neq0 a-bv=0 a-bv=0",[]
8,"Solving the PDE $x_{1}\dfrac{\partial f}{\partial x_{1}}+x_{2}\dfrac{\partial f}{\partial x_{2}}=e^{f(x_{1},x_{2})}-\alpha.$",Solving the PDE,"x_{1}\dfrac{\partial f}{\partial x_{1}}+x_{2}\dfrac{\partial f}{\partial x_{2}}=e^{f(x_{1},x_{2})}-\alpha.","This post is closely related to Dirichlet to Neumann operator in the unit ball with Fourier Analysis . I have transformed the exercise in the post above into a problem of finding solution of PDE: $$x_1\frac{\partial f}{\partial x_1} + x_2 \frac{\partial f}{\partial x_2} = e^{f(x_1,x_2)}-\alpha \text{ for } \alpha>0.$$ This is an exercise in Fourier analysis, so I am not prepared too many knowledge in differential equation. There are two hints: Use the Fourier Expansion; 2. Separate the argument into $\alpha\in\mathbb{N}$ and $\alpha\notin\mathbb{N}$ . However... I don't have any idea about how to solve this.. Any idea? Thank you! Edit: In the link above,  it suggested that if $\alpha\notin\mathbb{N}$ , then $f=\log\alpha$ , and asked the reader to further discover what happens if $\alpha\in\mathbb{N}$ . So if $\alpha\notin\mathbb{N}$ , we have $$x_1\frac{\partial}{\partial x_1}+x_2\frac{\partial f}{\partial x_2}=0 \text{??}$$ Edit 2: Below is how I convert the exercise in the link above to this PDE: Within the context of this exercise, we have the coinciding solution of the Dirichlet problem $$\Delta u=0 \text{ on } B_1$$ $$u=f \text{ on } \partial B_1 =\mathbb{S}^1$$ and of the Neumann problem $$\Delta u=0 \text{ on } B_1$$ $$\frac{\partial u}{\partial\nu}=e^f-\alpha \text{ on } \partial B_1=\mathbb{S}^1,$$ where $\dfrac{\partial u}{\partial\nu}= \nabla u\cdot \nu$ is the normal derivative of $u$ at the boundary with respect to the unit outer normal direction $\nu$ . Now, note that for a point $(x_1,x_2)\in\partial B_1 = \mathbb{S}^1$ , we always have $\nu=(x_1,x_2)$ . Also, by the solution of the Dirichlet problem, we know that $u=f$ on $\partial B_1=\mathbb{S}^1$ , and thus on the boundary we have $$\frac{\partial u}{\partial\nu} = \nabla u\cdot \nu=x_1\frac{\partial u}{\partial x_1} + x_2 \frac{\partial u}{\partial x_2} = x_1 \frac{\partial f}{\partial x_1} + x_2\frac{\partial f}{\partial x_2},$$ but the boundary condition of Neumann problem is $$\dfrac{\partial u}{\partial\nu}=e^{f}-\alpha,$$ and thus we have $$x_1\frac{\partial f}{\partial x_1} + x_2\frac{\partial f}{\partial x_2} = e^{f(x_1,x_2)}-\alpha.$$ Edit 3: (initial value) As ""Ninad"" pointed out, we need to an initial value to decide what is $C(t).$ And I believe perhaps the initial value is related to whether $\alpha$ is natural or not. The exercise does not give what happens if $\theta=0$ . However, I missed one conditiona that $f\in C^\infty(\mathbb{S}^1)$ , a infinity smooth $2\pi-$ periodic function. I don't know if this helps to provide us the initial value.","This post is closely related to Dirichlet to Neumann operator in the unit ball with Fourier Analysis . I have transformed the exercise in the post above into a problem of finding solution of PDE: This is an exercise in Fourier analysis, so I am not prepared too many knowledge in differential equation. There are two hints: Use the Fourier Expansion; 2. Separate the argument into and . However... I don't have any idea about how to solve this.. Any idea? Thank you! Edit: In the link above,  it suggested that if , then , and asked the reader to further discover what happens if . So if , we have Edit 2: Below is how I convert the exercise in the link above to this PDE: Within the context of this exercise, we have the coinciding solution of the Dirichlet problem and of the Neumann problem where is the normal derivative of at the boundary with respect to the unit outer normal direction . Now, note that for a point , we always have . Also, by the solution of the Dirichlet problem, we know that on , and thus on the boundary we have but the boundary condition of Neumann problem is and thus we have Edit 3: (initial value) As ""Ninad"" pointed out, we need to an initial value to decide what is And I believe perhaps the initial value is related to whether is natural or not. The exercise does not give what happens if . However, I missed one conditiona that , a infinity smooth periodic function. I don't know if this helps to provide us the initial value.","x_1\frac{\partial f}{\partial x_1} + x_2 \frac{\partial f}{\partial x_2} = e^{f(x_1,x_2)}-\alpha \text{ for } \alpha>0. \alpha\in\mathbb{N} \alpha\notin\mathbb{N} \alpha\notin\mathbb{N} f=\log\alpha \alpha\in\mathbb{N} \alpha\notin\mathbb{N} x_1\frac{\partial}{\partial x_1}+x_2\frac{\partial f}{\partial x_2}=0 \text{??} \Delta u=0 \text{ on } B_1 u=f \text{ on } \partial B_1 =\mathbb{S}^1 \Delta u=0 \text{ on } B_1 \frac{\partial u}{\partial\nu}=e^f-\alpha \text{ on } \partial B_1=\mathbb{S}^1, \dfrac{\partial u}{\partial\nu}= \nabla u\cdot \nu u \nu (x_1,x_2)\in\partial B_1 = \mathbb{S}^1 \nu=(x_1,x_2) u=f \partial B_1=\mathbb{S}^1 \frac{\partial u}{\partial\nu} = \nabla u\cdot \nu=x_1\frac{\partial u}{\partial x_1} + x_2 \frac{\partial u}{\partial x_2} = x_1 \frac{\partial f}{\partial x_1} + x_2\frac{\partial f}{\partial x_2}, \dfrac{\partial u}{\partial\nu}=e^{f}-\alpha, x_1\frac{\partial f}{\partial x_1} + x_2\frac{\partial f}{\partial x_2} = e^{f(x_1,x_2)}-\alpha. C(t). \alpha \theta=0 f\in C^\infty(\mathbb{S}^1) 2\pi-","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'fourier-analysis', 'harmonic-analysis']"
9,particular solution guessing of a Riccati equation,particular solution guessing of a Riccati equation,,I can construct the general solution of a Riccati equation if a particular solution is known. Is there any guessing trick do exist for example: $$\text{For }y'+y^2=\frac{2}{x^2} \text{ seek for a P.S. in the form }y=\frac{c}{x}$$ $$\text{For }x^3y'+x^2y-y^2=2x^4 \text{ seek for a P.S. in the form }y=cx^2$$ $\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\vdots$ Why seeking for a particular solution $(\text{P.S.})$ in that form $?$ And how the knew that will be work $?$ I really appreciated any kind of help.,I can construct the general solution of a Riccati equation if a particular solution is known. Is there any guessing trick do exist for example: Why seeking for a particular solution in that form And how the knew that will be work I really appreciated any kind of help.,\text{For }y'+y^2=\frac{2}{x^2} \text{ seek for a P.S. in the form }y=\frac{c}{x} \text{For }x^3y'+x^2y-y^2=2x^4 \text{ seek for a P.S. in the form }y=cx^2 \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\vdots (\text{P.S.}) ? ?,"['ordinary-differential-equations', 'analysis']"
10,Energy function of pendulum,Energy function of pendulum,,"Im following G.Teschl book for ODE and dynamical systems, and in problem 6.26 Im asked to find the energy function of the mathematical pendulum $\ddot x = -\mu \dot x - sin(x) $ . The book shows a standar(i guess) way to find an energy function, so I tried to emulate it as follows, We amplify $\ddot x = -\mu \dot x - sin(x)$ by $\dot x$ and we get $\ddot x \dot x= -\mu \dot x^{2} - sin(x)\dot x$ which can be writen as $\frac{d}{dt} \frac{{\dot x}^{2}}{2} = -\mu \dot x^{2} - sin(x)\dot x  $ . Now we will call $- \dot U(x)=-\mu \dot x - sin(x)$ , then, $\frac{d}{dt} \frac{{\dot x}^{2}}{2} = -\dot U(x) \dot x$ which leads us to $\frac{d}{dt} (\frac{{\dot x}^{2}}{2} + U(x))=0$ . And this should represent the derivative of the energy function of the double pendulum. Now Im asked to show that the energy function could be used as a Liapunov function to prove stability of $(0,0)$ but I havent been able to do the last thing, I havent been able to solve it explicitly, so any help would be really appreciated. Thanks so much guys <3","Im following G.Teschl book for ODE and dynamical systems, and in problem 6.26 Im asked to find the energy function of the mathematical pendulum . The book shows a standar(i guess) way to find an energy function, so I tried to emulate it as follows, We amplify by and we get which can be writen as . Now we will call , then, which leads us to . And this should represent the derivative of the energy function of the double pendulum. Now Im asked to show that the energy function could be used as a Liapunov function to prove stability of but I havent been able to do the last thing, I havent been able to solve it explicitly, so any help would be really appreciated. Thanks so much guys <3","\ddot x = -\mu \dot x - sin(x)  \ddot x = -\mu \dot x - sin(x) \dot x \ddot x \dot x= -\mu \dot x^{2} - sin(x)\dot x \frac{d}{dt} \frac{{\dot x}^{2}}{2} = -\mu \dot x^{2} - sin(x)\dot x   - \dot U(x)=-\mu \dot x - sin(x) \frac{d}{dt} \frac{{\dot x}^{2}}{2} = -\dot U(x) \dot x \frac{d}{dt} (\frac{{\dot x}^{2}}{2} + U(x))=0 (0,0)","['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
11,Proving concavity of derivative,Proving concavity of derivative,,"Let $f(x)$ be  defined and continuous and derivable for $x>-1$ , $f(0)=1$ , $f’(0)=0$ and $$f''(x) = \frac {1+x}{1+f(x)}.$$ Prove that $f’(x)$ is concave up for all $x>-1$ . My attempt:  I tried to integrate by multiplying both sides by $dy/dx$ but could not proceed further.","Let be  defined and continuous and derivable for , , and Prove that is concave up for all . My attempt:  I tried to integrate by multiplying both sides by but could not proceed further.",f(x) x>-1 f(0)=1 f’(0)=0 f''(x) = \frac {1+x}{1+f(x)}. f’(x) x>-1 dy/dx,"['ordinary-differential-equations', 'derivatives', 'monotone-functions']"
12,Stability of $y = \cos(2t)$ for the ode $y'' + 4y = 0$,Stability of  for the ode,y = \cos(2t) y'' + 4y = 0,"I want to examine the stability of the solution $\phi = \cos(2t)$ of the ode $y'' + 4y = 0$ . I know the general solution of this ode is $y = c_1 \cos(2t) + c_2 \sin(2t)$ . So to examine the stability of my solution, I need to see if other solutions stay close to it starting at $t = 0$ . It'll be asymptotically stable if other solutions converge to it. So, when $t = 0$ , $\phi(0) = \cos(0) = 1$ . and $\phi'(0) = -\sin(0) =0$ . But.. how do I pick $y(t)$ so that $y(0)$ starts out close to $1$ and $y'(0)$ is close to $0$ ? Then what? Well, $y(t) = 1$ when $t = 0$ and when $t = 0$ , $y'(0) = 1$ . I'm not sure how to proceed.","I want to examine the stability of the solution of the ode . I know the general solution of this ode is . So to examine the stability of my solution, I need to see if other solutions stay close to it starting at . It'll be asymptotically stable if other solutions converge to it. So, when , . and . But.. how do I pick so that starts out close to and is close to ? Then what? Well, when and when , . I'm not sure how to proceed.",\phi = \cos(2t) y'' + 4y = 0 y = c_1 \cos(2t) + c_2 \sin(2t) t = 0 t = 0 \phi(0) = \cos(0) = 1 \phi'(0) = -\sin(0) =0 y(t) y(0) 1 y'(0) 0 y(t) = 1 t = 0 t = 0 y'(0) = 1,['ordinary-differential-equations']
13,How to calculate the geodesics in polar coordinates?,How to calculate the geodesics in polar coordinates?,,"I know that: $$ \ddot{r}^2 = r\dot{\phi}^2 $$ and $$ \ddot{\phi}=-\frac{2}{r}\dot{r}\dot{\phi} $$ and I know that using this I should be able to get the geodesic equations, but for my life I just can't, I keep trying but I end up with things I cannot solve. I tried getting $r$ from the first equation and replacing it in the second, getting $\dot{\phi}$ from the second and replacing it in the first, I even found that: $$ r^2\dot{\phi} = constant $$ but that didn't help at all, I am completely stuck and I can't find any derivation of the geodesic equations anywhere in the internet, please help I want to understand","I know that: and and I know that using this I should be able to get the geodesic equations, but for my life I just can't, I keep trying but I end up with things I cannot solve. I tried getting from the first equation and replacing it in the second, getting from the second and replacing it in the first, I even found that: but that didn't help at all, I am completely stuck and I can't find any derivation of the geodesic equations anywhere in the internet, please help I want to understand","
\ddot{r}^2 = r\dot{\phi}^2
 
\ddot{\phi}=-\frac{2}{r}\dot{r}\dot{\phi}
 r \dot{\phi} 
r^2\dot{\phi} = constant
","['ordinary-differential-equations', 'differential-geometry', 'general-relativity']"
14,Does this PDE have a solution?,Does this PDE have a solution?,,"Let $U_0 \subset \mathbb{R}^3$ be an open neighborhood of $0$ and $X:U_0 \to\mathbb{R}^3$ a smooth vector field, such that $$X(x,y,z) = (X_1(x,y,z),1,0). $$ where $X_1:U_0 \to\mathbb{R}$ , satisfies the following hypotheses: $X_1(x,0,z) =0,$ $\forall$ $(x,0,z)$ $\in$ $U_0$ . $\frac{\partial}{\partial x}X_1(x,0,z)=\frac{\partial}{\partial y}X_1(x,0,z) =0$ , $\forall$ $(x,0,z) \in U_0$ $\frac{\partial}{\partial y}X_1(x,0,z)\neq 0$ $\forall (x,0,z) \in U_0$ $X_1(0,0,0)=(0,0,0)$ . I need to find a change of coordinates $\varphi: W_0\subset \mathbb{R}^3 \to V_0\subset \mathbb{R}^3$ ( $\varphi(0)=0$ ), such that $$Z(x,y,z) = \text{d}\varphi_{\varphi^{-1}(x,y,z)}X(\varphi^{-1}(x,y,z)) = (y,1,0) $$ or $$Z(x,y,z) = \text{d}\varphi_{\varphi^{-1}(x,y,z)}X(\varphi^{-1}(x,y,z)) = (-y,1,0). $$ The problem that I am facing does not allow me to mess up the z and y-axes in my coordinate system. A natural way to solve this problem is trying to find this changing of coordinates in the following form $$\varphi(x,y,z) = (f(x,y,z),y,z).$$ (with $\frac {\partial f}{\partial x} (0) \neq 0$ and $f(0,0,0)=0$ ). If this coordinate change works, we would get $$Z(\varphi(x,y,z)) = \text{d}\varphi(x,y,z) \cdot X(x,y,z)$$ $$(\pm y,1,0) = \left( \frac{\partial f}{\partial x}(x,y,z) X_1(x,y,z) + \frac{\partial f}{\partial y} (x,y,z) ,1,0 \right) .$$ And then my question appears, does anyone know if this PDE has a solution? $$   \frac{\partial f}{\partial x}(x,y,z) X_1(x,y,z) + \frac{\partial  f}{\partial y} (x,y,z)= \pm y,$$ $$f(0,0,0)=0,$$ $$\frac{\partial f}{\partial x}(0,0,0) \neq 0. $$","Let be an open neighborhood of and a smooth vector field, such that where , satisfies the following hypotheses: . , . I need to find a change of coordinates ( ), such that or The problem that I am facing does not allow me to mess up the z and y-axes in my coordinate system. A natural way to solve this problem is trying to find this changing of coordinates in the following form (with and ). If this coordinate change works, we would get And then my question appears, does anyone know if this PDE has a solution?","U_0 \subset \mathbb{R}^3 0 X:U_0 \to\mathbb{R}^3 X(x,y,z) = (X_1(x,y,z),1,0).  X_1:U_0 \to\mathbb{R} X_1(x,0,z) =0, \forall (x,0,z) \in U_0 \frac{\partial}{\partial x}X_1(x,0,z)=\frac{\partial}{\partial y}X_1(x,0,z) =0 \forall (x,0,z) \in U_0 \frac{\partial}{\partial y}X_1(x,0,z)\neq 0 \forall (x,0,z) \in U_0 X_1(0,0,0)=(0,0,0) \varphi: W_0\subset \mathbb{R}^3 \to V_0\subset \mathbb{R}^3 \varphi(0)=0 Z(x,y,z) = \text{d}\varphi_{\varphi^{-1}(x,y,z)}X(\varphi^{-1}(x,y,z)) = (y,1,0)  Z(x,y,z) = \text{d}\varphi_{\varphi^{-1}(x,y,z)}X(\varphi^{-1}(x,y,z)) = (-y,1,0).  \varphi(x,y,z) = (f(x,y,z),y,z). \frac {\partial f}{\partial x} (0) \neq 0 f(0,0,0)=0 Z(\varphi(x,y,z)) = \text{d}\varphi(x,y,z) \cdot X(x,y,z) (\pm y,1,0) = \left( \frac{\partial f}{\partial x}(x,y,z) X_1(x,y,z) + \frac{\partial f}{\partial y} (x,y,z) ,1,0 \right) .    \frac{\partial f}{\partial x}(x,y,z) X_1(x,y,z) + \frac{\partial
 f}{\partial y} (x,y,z)= \pm y, f(0,0,0)=0, \frac{\partial f}{\partial x}(0,0,0) \neq 0. ","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'dynamical-systems']"
15,Problem book for differential equations?? Are there?,Problem book for differential equations?? Are there?,,"Is there a problem book which gathers the most relevant exercises in differential equations (historical problems, eventually providing counter-examples for some theorems and properties if some conditions are neglected)? I'm interested in both ODE and PDE. Thanks a lot!","Is there a problem book which gathers the most relevant exercises in differential equations (historical problems, eventually providing counter-examples for some theorems and properties if some conditions are neglected)? I'm interested in both ODE and PDE. Thanks a lot!",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'reference-request', 'book-recommendation']"
16,The Z transform: why $z^{-n}$ and not $z^n$?,The Z transform: why  and not ?,z^{-n} z^n,"The Z-transform for a discrete signal is defined as $X(z) = \sum_{n = -\infty}^{\infty} x[n]z^{-n}$. I was wondering why we invert the exponent of $z$, rather than define it as $X(z) = \sum_{n = -\infty}^{\infty} x[n]z^{n}$, which to my mind seems like a more natural definition. I've not been able to find an answer for this anywhere. Both definitions seem to give pretty much the same theory (but different domains of convergence in applications).","The Z-transform for a discrete signal is defined as $X(z) = \sum_{n = -\infty}^{\infty} x[n]z^{-n}$. I was wondering why we invert the exponent of $z$, rather than define it as $X(z) = \sum_{n = -\infty}^{\infty} x[n]z^{n}$, which to my mind seems like a more natural definition. I've not been able to find an answer for this anywhere. Both definitions seem to give pretty much the same theory (but different domains of convergence in applications).",,"['ordinary-differential-equations', 'recurrence-relations', 'signal-processing']"
17,Solving first order differential equation with power series,Solving first order differential equation with power series,,"So, I was told solve the equation $y' - y = x^2$ using power series. Normal methods tell me that the solution is $y = c_{0}e^{x}-x^{2}-2x-2$, and this can be verified by plugging it back in. However, I am stuck on trying to solve this with power series. We assume $y = \Sigma_{n=0}^{\infty}a_{n}x^{n}$. Thus, $y' = \Sigma_{n=0}^{\infty}a_{n+1}(n+1)x^{n}$. I plug these into the original equation. $$\Sigma_{n=0}^{\infty}a_{n+1}(n+1)x^{n}-\Sigma_{n=0}^{\infty}a_{n}x^{n}=x^{2} \\ a_{1}x^{0}+2a_{2}x^{1}+3a_{3}x^{2}+4a_{4}x^{3}+\dots-a_{0}x^{0}-a_{1}x^{1}-a_{2}x^{2}-a_{3}x^{3}-\dots = x^{2}$$ By equating powers of $x$, I find the following relations $$a_{1}-a_{0}=0 \\ 2a_{2}-a_{1}=0 \\ 3a_{3}-a_{2}=1 \\ 4a_{4}-a_{3}=0 \\ \vdots \\ na_{n}-a_{n-1}=0 $$ So, I write the coefficients as $$ a_{1}=a_{0}\\ a_{2}=\frac{a_{1}}{2}=\frac{a_{0}}{2}\\ a_{3}=\frac{1}{3}+\frac{a_{2}}{3}=\frac{a_{0}}{6}+\frac{1}{3}\\ a_{4}=\frac{a_{0}}{24}+\frac{1}{12}\\ \vdots\\ a_{n}=\frac{a_{n}}{n!}+\frac{2}{n!} $$ Combining these to form $y$, I get $$y=\Sigma_{n=0}^{\infty}\frac{a_{0}}{n!}x^{n}+\Sigma_{n=3}^{\infty}\frac{2}{n!}x^{n} $$ The first bit gives me $a_{0}e^{x}$ as expected, but I don't see how to extract $-x-2x-2$ from the second half. Is there something wrong in my approach that lead to an incorrect answer, or am I missing something in the manipulation of power series?","So, I was told solve the equation $y' - y = x^2$ using power series. Normal methods tell me that the solution is $y = c_{0}e^{x}-x^{2}-2x-2$, and this can be verified by plugging it back in. However, I am stuck on trying to solve this with power series. We assume $y = \Sigma_{n=0}^{\infty}a_{n}x^{n}$. Thus, $y' = \Sigma_{n=0}^{\infty}a_{n+1}(n+1)x^{n}$. I plug these into the original equation. $$\Sigma_{n=0}^{\infty}a_{n+1}(n+1)x^{n}-\Sigma_{n=0}^{\infty}a_{n}x^{n}=x^{2} \\ a_{1}x^{0}+2a_{2}x^{1}+3a_{3}x^{2}+4a_{4}x^{3}+\dots-a_{0}x^{0}-a_{1}x^{1}-a_{2}x^{2}-a_{3}x^{3}-\dots = x^{2}$$ By equating powers of $x$, I find the following relations $$a_{1}-a_{0}=0 \\ 2a_{2}-a_{1}=0 \\ 3a_{3}-a_{2}=1 \\ 4a_{4}-a_{3}=0 \\ \vdots \\ na_{n}-a_{n-1}=0 $$ So, I write the coefficients as $$ a_{1}=a_{0}\\ a_{2}=\frac{a_{1}}{2}=\frac{a_{0}}{2}\\ a_{3}=\frac{1}{3}+\frac{a_{2}}{3}=\frac{a_{0}}{6}+\frac{1}{3}\\ a_{4}=\frac{a_{0}}{24}+\frac{1}{12}\\ \vdots\\ a_{n}=\frac{a_{n}}{n!}+\frac{2}{n!} $$ Combining these to form $y$, I get $$y=\Sigma_{n=0}^{\infty}\frac{a_{0}}{n!}x^{n}+\Sigma_{n=3}^{\infty}\frac{2}{n!}x^{n} $$ The first bit gives me $a_{0}e^{x}$ as expected, but I don't see how to extract $-x-2x-2$ from the second half. Is there something wrong in my approach that lead to an incorrect answer, or am I missing something in the manipulation of power series?",,"['ordinary-differential-equations', 'power-series']"
18,Maximal solution of a differential equation belongs to manifold $M$.,Maximal solution of a differential equation belongs to manifold .,M,"I'm trying to prove the following: Let $U\subseteq\mathbb R^n$ be an open set and $g\colon U\to\mathbb R^n$ be a (continuous) vector field.   Suppose that for every $p\in U$ the equation   $$x' = g(x), \quad x\in U,$$   admits a unique maximal solution $x(\cdot)$ satisfying $x(0)=p$.   Now, let $M\subseteq U$ be a (at least) $C^1$ manifold closed in $U$.   If $g$ is tangent to $M$, that is, if $g(x) \in T_xM$ for every $x\in M$, then every maximal solution of $x' = g(x)$ that intercepts $M$ ""lives"" entirely in $M$. I have no clue on how to attack this problem. The first thing I've thought is to use that $M$ is locally a level set of a $C^1$ function $f\colon U\to\mathbb R^m$, say $f^{-1}(0)$, but working ""locally"" is not compatible with ""maximal solution"", so I'm stuck. I really don't know how to proceed. Any help would be appreciated. Thanks in advance.","I'm trying to prove the following: Let $U\subseteq\mathbb R^n$ be an open set and $g\colon U\to\mathbb R^n$ be a (continuous) vector field.   Suppose that for every $p\in U$ the equation   $$x' = g(x), \quad x\in U,$$   admits a unique maximal solution $x(\cdot)$ satisfying $x(0)=p$.   Now, let $M\subseteq U$ be a (at least) $C^1$ manifold closed in $U$.   If $g$ is tangent to $M$, that is, if $g(x) \in T_xM$ for every $x\in M$, then every maximal solution of $x' = g(x)$ that intercepts $M$ ""lives"" entirely in $M$. I have no clue on how to attack this problem. The first thing I've thought is to use that $M$ is locally a level set of a $C^1$ function $f\colon U\to\mathbb R^m$, say $f^{-1}(0)$, but working ""locally"" is not compatible with ""maximal solution"", so I'm stuck. I really don't know how to proceed. Any help would be appreciated. Thanks in advance.",,"['ordinary-differential-equations', 'analysis', 'smooth-manifolds']"
19,Solving system of differential equations to evaluate $x(t)$,Solving system of differential equations to evaluate,x(t),"I have to solve the following system of equations and find $x(t)$ $$\frac{dx}{dt} =a\cos(y)-b$$ $$x\frac{dy}{dt}=-a\sin(y)$$ Initial conditions: $t=0,x=x_o, y=\frac{\pi}{2}$ By dividing equations and integration, i obtained $x(y)$: $$x=x_o \csc(y) \left|\csc(y)-\cot(y)\right|^{\frac{b}{a}}$$ Now I cannot proceed to find $x(t)$.","I have to solve the following system of equations and find $x(t)$ $$\frac{dx}{dt} =a\cos(y)-b$$ $$x\frac{dy}{dt}=-a\sin(y)$$ Initial conditions: $t=0,x=x_o, y=\frac{\pi}{2}$ By dividing equations and integration, i obtained $x(y)$: $$x=x_o \csc(y) \left|\csc(y)-\cot(y)\right|^{\frac{b}{a}}$$ Now I cannot proceed to find $x(t)$.",,"['calculus', 'ordinary-differential-equations', 'trigonometry', 'trigonometric-integrals']"
20,How to solve $y''+y=\cos x$?,How to solve ?,y''+y=\cos x,Solve $y''+y=\cos x$. After first solving the homogeneous equation we know that the solution to it is $y=c_1\cos x+c_2\sin x$. We can guess that the private solution to non-homogeneous equation will be of form: $y_p=x(A_1\cos x+A_2\sin x)$. Then: $$ y_p'=A_1\cos x-A_1x\sin x+A_2\sin x+A_2x\cos x\\ y_p''=-2A_1\sin x-A_1x\cos x+2A_2\cos x-A_2x\sin x $$ If we plug these into the original equation we get: $$ \cos x(A_1+A_2x-A_1x+2A_2)+\sin x(A_2-A_1-2A_2-A_2x)=\cos x \quad\ast $$ We can try to solve the system: $$ \begin{cases} x(A_2-A_1)+A_1+2A_2=1\\ x(-A_1-A_2)+A_2-2A_1=0 \end{cases} $$ But there're 3 unknowns in the system so I don't see how to find out the values of $A_1$ and $A_2$. The solution says that we get $2(A_2\cos x-A_1\sin x)=\cos x$ from which is follows that $A_1=0$ and $A_2=0.5$. But how do we get to this conclusion? Is there some trick I missed? I checked my calculations in Wolfram Alpha and they match .,Solve $y''+y=\cos x$. After first solving the homogeneous equation we know that the solution to it is $y=c_1\cos x+c_2\sin x$. We can guess that the private solution to non-homogeneous equation will be of form: $y_p=x(A_1\cos x+A_2\sin x)$. Then: $$ y_p'=A_1\cos x-A_1x\sin x+A_2\sin x+A_2x\cos x\\ y_p''=-2A_1\sin x-A_1x\cos x+2A_2\cos x-A_2x\sin x $$ If we plug these into the original equation we get: $$ \cos x(A_1+A_2x-A_1x+2A_2)+\sin x(A_2-A_1-2A_2-A_2x)=\cos x \quad\ast $$ We can try to solve the system: $$ \begin{cases} x(A_2-A_1)+A_1+2A_2=1\\ x(-A_1-A_2)+A_2-2A_1=0 \end{cases} $$ But there're 3 unknowns in the system so I don't see how to find out the values of $A_1$ and $A_2$. The solution says that we get $2(A_2\cos x-A_1\sin x)=\cos x$ from which is follows that $A_1=0$ and $A_2=0.5$. But how do we get to this conclusion? Is there some trick I missed? I checked my calculations in Wolfram Alpha and they match .,,"['ordinary-differential-equations', 'multivariable-calculus']"
21,Differential equation via Laplace transform,Differential equation via Laplace transform,,"$$xy''+(2x+3)y'+(x+3)y=3e^{-x}$$ $$L[xy'']+L[(2x+3)y']+L[(x+3)y]=L[3e^{-x}]$$ $$-\frac{d}{dp}(p^2Y)-\frac{d}{dp}(2pY)-\frac{dY}{dp}=\frac{3}{p+1}$$ $$-\frac{dY}{dp}(p^2)-\frac{dY}{dp}(2p)-\frac{dY}{dp}=\frac{3}{p+1}$$ $$-\frac{dY}{dp}(p^2+2p+1)=\frac{3}{p+1}$$ $$-dY=-\frac{3dp}{p^3+3p^2+3p+1}$$ $$\int-dY=\int\frac{3dp}{p^3+3p^2+3p+1}$$ $$-Y=\frac{-3}{2(x+1)^2}+C$$ $$Y=\frac{3}{2(x+1)^2}+C$$ This is the answer which I got, however, at the end of the book it says that the answer is $$Y=xe^{-x}$$ Can someone please tell me if my answer is correct, and if so how I can make it look like $Y=xe^{-x} \text{?}$","$$xy''+(2x+3)y'+(x+3)y=3e^{-x}$$ $$L[xy'']+L[(2x+3)y']+L[(x+3)y]=L[3e^{-x}]$$ $$-\frac{d}{dp}(p^2Y)-\frac{d}{dp}(2pY)-\frac{dY}{dp}=\frac{3}{p+1}$$ $$-\frac{dY}{dp}(p^2)-\frac{dY}{dp}(2p)-\frac{dY}{dp}=\frac{3}{p+1}$$ $$-\frac{dY}{dp}(p^2+2p+1)=\frac{3}{p+1}$$ $$-dY=-\frac{3dp}{p^3+3p^2+3p+1}$$ $$\int-dY=\int\frac{3dp}{p^3+3p^2+3p+1}$$ $$-Y=\frac{-3}{2(x+1)^2}+C$$ $$Y=\frac{3}{2(x+1)^2}+C$$ This is the answer which I got, however, at the end of the book it says that the answer is $$Y=xe^{-x}$$ Can someone please tell me if my answer is correct, and if so how I can make it look like $Y=xe^{-x} \text{?}$",,"['ordinary-differential-equations', 'laplace-transform']"
22,geodesic equations for spheres,geodesic equations for spheres,,"I get an ordinary differential equation when i'm checking the geodesics of spheres are great circles using stereographic projection (i know there're better ways to get the geodesics directly). I just wanna know the property of this equation $\lambda ''(1+\lambda ^2)=\lambda \lambda '^2$ with initial value $\lambda(0)=0$, such as if it is solvable, and if $\lambda'$ is increasing.","I get an ordinary differential equation when i'm checking the geodesics of spheres are great circles using stereographic projection (i know there're better ways to get the geodesics directly). I just wanna know the property of this equation $\lambda ''(1+\lambda ^2)=\lambda \lambda '^2$ with initial value $\lambda(0)=0$, such as if it is solvable, and if $\lambda'$ is increasing.",,"['ordinary-differential-equations', 'differential-geometry']"
23,Unbounded solution of a ODE,Unbounded solution of a ODE,,"Let $f,g:[0,\infty)\to \mathbb{R}$ be two continuous functions such that $\lim\limits_{x\to\infty}f(x)=1$ and $\int_0^\infty|g(x)|dx<\infty$. Consider the ODE $$\begin{pmatrix} y_1'\\ y'_2\end{pmatrix}=\begin{pmatrix} 0 & f(x)\\ g(x) & 0\end{pmatrix}\begin{pmatrix} y_1\\ y_2\end{pmatrix}.$$ Suppose that $\Phi(x)=\begin{pmatrix} \phi_1(x)\\ \phi_2(x)\end{pmatrix}$ is a solution of the above ODE such that $\phi_1$ is bounded. Prove that $$\lim\limits_{x\to\infty}\phi_2(x)=0.$$ Deduce that the above ODE has an unbounded solution. I really do not know where to start. Any hint?","Let $f,g:[0,\infty)\to \mathbb{R}$ be two continuous functions such that $\lim\limits_{x\to\infty}f(x)=1$ and $\int_0^\infty|g(x)|dx<\infty$. Consider the ODE $$\begin{pmatrix} y_1'\\ y'_2\end{pmatrix}=\begin{pmatrix} 0 & f(x)\\ g(x) & 0\end{pmatrix}\begin{pmatrix} y_1\\ y_2\end{pmatrix}.$$ Suppose that $\Phi(x)=\begin{pmatrix} \phi_1(x)\\ \phi_2(x)\end{pmatrix}$ is a solution of the above ODE such that $\phi_1$ is bounded. Prove that $$\lim\limits_{x\to\infty}\phi_2(x)=0.$$ Deduce that the above ODE has an unbounded solution. I really do not know where to start. Any hint?",,['ordinary-differential-equations']
24,"Orthogonal Trajectories Using Polar Coordinates. Correct Calculations, Two Different Answers?","Orthogonal Trajectories Using Polar Coordinates. Correct Calculations, Two Different Answers?",,"My textbook, George F. Simmons' Differential Equations with Applications and Historical Notes, asks to find the orthogonal trajectory of the family of curves $r = 2Ccos(\theta)$ where C is a parameter. The original equation of the family of curves was $x^2 + y^2 = 2Cx$ , but it led to an equation that was as yet unsolvable using the methods taught by the textbook up to that moment. $^*$ To compensate, the textbook switched to polar coordinates and started solving it that way, which was what I have shown in the calculations below. $*$ For further elaboration, the authors got $\dfrac{dy}{dx} = \dfrac{2xy}{x^2 - y^2}$ from $x^2 + y^2 = 2Cx$ . They then said, ""Unfortunately, the variables cannot be separated, so without additional techniques for solving differential equations we can go no further in this direction. However, if we use polar coordinates, the equation of the family can be written as $r = 2Ccos(\theta)$ "". And then they continue with their calculations, as stated below. The solution that I am getting is different from that of the textbook. All of the (similar) previous problems that I have completed have been correct, so if there are errors in my understanding of this concept, I cannot detect them. My Solution $r = 2Ccos(\theta)$ $\dfrac{dr}{d\theta} = -2Csin(\theta)$ We need to eliminate the arbitrary constant $C$ because we don't just want the orthogonal trajectory for a single curve -- we want the orthogonal trajectories for the entire family of curves; therefore, we want $\dfrac{dr}{d\theta}$ in terms of $r$ and $\theta$ . $\dfrac{r}{2\cos(\theta)} = C$ $\therefore \dfrac{dr}{d\theta} = -2\left( \dfrac{r}{2cos(\theta)} \right) sin(\theta)$ $= \dfrac{-rsin(\theta)}{cos(\theta)}$ The orthogonal trajectories will have a slope which is the negative reciprocal of the slopes of the family of curves: $\therefore \dfrac{-d\theta}{dr} = \dfrac{-rsin(\theta)}{cos(\theta)}$ $\implies \dfrac{d\theta}{dr} = \dfrac{rsin(\theta)}{cos(\theta)}$ $\implies \dfrac{dr}{d\theta} = \dfrac{cos(\theta)}{rsin(\theta)}$ $\implies dr(r) = \dfrac{cos(\theta)}{sin(\theta)} (d\theta)$ And we can now proceed with separation of variables... Textbook's Solution $r = 2Ccos(\theta)$ $\dfrac{dr}{d\theta} = -2Csin(\theta)$ After eliminating C we arrive at $\dfrac{rd\theta}{dr} = \dfrac{-cos(\theta)}{sin(\theta)}$ as the differential equation of the given family. Accordingly, $\dfrac{rd\theta}{dr} = \dfrac{sin(\theta)}{cos(\theta)}$ is the differential equation of the orthogonal trajectories. In this case, the variables can be separated, yielding $\dfrac{dr}{r} = \dfrac{cos(\theta) d\theta}{sin(\theta)}$ And it then proceeds with integration ... I'm wondering if both solutions (mine and the textbook) are correct? Or have I made an error? If I've made an error, I would appreciate it if people could please take the time to carefully explain the reasoning behind it. I have only just begun studying differential equations (chapter 1), so any explanation would have to be very elementary.","My textbook, George F. Simmons' Differential Equations with Applications and Historical Notes, asks to find the orthogonal trajectory of the family of curves where C is a parameter. The original equation of the family of curves was , but it led to an equation that was as yet unsolvable using the methods taught by the textbook up to that moment. To compensate, the textbook switched to polar coordinates and started solving it that way, which was what I have shown in the calculations below. For further elaboration, the authors got from . They then said, ""Unfortunately, the variables cannot be separated, so without additional techniques for solving differential equations we can go no further in this direction. However, if we use polar coordinates, the equation of the family can be written as "". And then they continue with their calculations, as stated below. The solution that I am getting is different from that of the textbook. All of the (similar) previous problems that I have completed have been correct, so if there are errors in my understanding of this concept, I cannot detect them. My Solution We need to eliminate the arbitrary constant because we don't just want the orthogonal trajectory for a single curve -- we want the orthogonal trajectories for the entire family of curves; therefore, we want in terms of and . The orthogonal trajectories will have a slope which is the negative reciprocal of the slopes of the family of curves: And we can now proceed with separation of variables... Textbook's Solution After eliminating C we arrive at as the differential equation of the given family. Accordingly, is the differential equation of the orthogonal trajectories. In this case, the variables can be separated, yielding And it then proceeds with integration ... I'm wondering if both solutions (mine and the textbook) are correct? Or have I made an error? If I've made an error, I would appreciate it if people could please take the time to carefully explain the reasoning behind it. I have only just begun studying differential equations (chapter 1), so any explanation would have to be very elementary.",r = 2Ccos(\theta) x^2 + y^2 = 2Cx ^* * \dfrac{dy}{dx} = \dfrac{2xy}{x^2 - y^2} x^2 + y^2 = 2Cx r = 2Ccos(\theta) r = 2Ccos(\theta) \dfrac{dr}{d\theta} = -2Csin(\theta) C \dfrac{dr}{d\theta} r \theta \dfrac{r}{2\cos(\theta)} = C \therefore \dfrac{dr}{d\theta} = -2\left( \dfrac{r}{2cos(\theta)} \right) sin(\theta) = \dfrac{-rsin(\theta)}{cos(\theta)} \therefore \dfrac{-d\theta}{dr} = \dfrac{-rsin(\theta)}{cos(\theta)} \implies \dfrac{d\theta}{dr} = \dfrac{rsin(\theta)}{cos(\theta)} \implies \dfrac{dr}{d\theta} = \dfrac{cos(\theta)}{rsin(\theta)} \implies dr(r) = \dfrac{cos(\theta)}{sin(\theta)} (d\theta) r = 2Ccos(\theta) \dfrac{dr}{d\theta} = -2Csin(\theta) \dfrac{rd\theta}{dr} = \dfrac{-cos(\theta)}{sin(\theta)} \dfrac{rd\theta}{dr} = \dfrac{sin(\theta)}{cos(\theta)} \dfrac{dr}{r} = \dfrac{cos(\theta) d\theta}{sin(\theta)},"['ordinary-differential-equations', 'polar-coordinates']"
25,Using the Dirac Delta function in PDE's,Using the Dirac Delta function in PDE's,,"Solve the diffusion equation on the positive half-line $\frac{∂u}{∂t}−a^2\frac{∂^2u}{∂x^2}=0,0≤x<∞$ subject to the initial and the boundary condition $u(x,0)=Qδ(x−x_0),u_x(0,t)=0.$ Where $Q≠0$ and $x_0>0$ are given constants, and $δ(⋅)$ is the Dirac delta-function. I think that The equation is invariant with respect to $x → −x$ and $v → −v$, and the initial condition is odd in $x$, hence the solution is even in $x$. Would I be correct in saying this? Also I am stuck at this point and unsure where to go from here.","Solve the diffusion equation on the positive half-line $\frac{∂u}{∂t}−a^2\frac{∂^2u}{∂x^2}=0,0≤x<∞$ subject to the initial and the boundary condition $u(x,0)=Qδ(x−x_0),u_x(0,t)=0.$ Where $Q≠0$ and $x_0>0$ are given constants, and $δ(⋅)$ is the Dirac delta-function. I think that The equation is invariant with respect to $x → −x$ and $v → −v$, and the initial condition is odd in $x$, hence the solution is even in $x$. Would I be correct in saying this? Also I am stuck at this point and unsure where to go from here.",,"['ordinary-differential-equations', 'partial-differential-equations', 'dirac-delta']"
26,Solving differential equation $x y^2 y' = x+1$,Solving differential equation,x y^2 y' = x+1,I am asked to solve the following differential equation: $$ x y^2 y' = x+1 $$ My process was $$ \begin{align*} x y^2 y' &= x+1\\ xy^2 \frac{dy}{dx} &= x+1\\ y^2 dy &= \frac{x+1}{x} dx\\ \int y^2 dy &= \int \frac{x+1}{x} dx\\ \int y^2 dy &= \int dx + \int \frac{1}{x} dx\\ \frac{y^3}{3} &= x + \ln |x| + C\\ y &= \sqrt[3]{3 \left( x + \ln |x| + C \right)} \end{align*} $$ but when I was checking my result on Wolfram I noticed that it was given in a different way. Is my result incorrect? What caused the results to be different? Is it the absolute value sign of the $\ln$? Thank you.,I am asked to solve the following differential equation: $$ x y^2 y' = x+1 $$ My process was $$ \begin{align*} x y^2 y' &= x+1\\ xy^2 \frac{dy}{dx} &= x+1\\ y^2 dy &= \frac{x+1}{x} dx\\ \int y^2 dy &= \int \frac{x+1}{x} dx\\ \int y^2 dy &= \int dx + \int \frac{1}{x} dx\\ \frac{y^3}{3} &= x + \ln |x| + C\\ y &= \sqrt[3]{3 \left( x + \ln |x| + C \right)} \end{align*} $$ but when I was checking my result on Wolfram I noticed that it was given in a different way. Is my result incorrect? What caused the results to be different? Is it the absolute value sign of the $\ln$? Thank you.,,"['calculus', 'ordinary-differential-equations']"
27,Integration of $\frac{1}{dx}$ in physical applications,Integration of  in physical applications,\frac{1}{dx},$$\frac{1}{dc} = \frac{dx}{εx}$$ That's what I was left with when I was finding the capacitance of a parallel plate capacitor with non-uniform dielectric in it. I just needed to find $c$ (capacitance) for this problem where the total distance ($D$) was also given. But I don't think this integration which is $$∫\frac{1}{dc} = ∫\frac{dx}{εx}$$ [$ε$ is constant] can be solved. Can it be? Or am I doing something wrong?,$$\frac{1}{dc} = \frac{dx}{εx}$$ That's what I was left with when I was finding the capacitance of a parallel plate capacitor with non-uniform dielectric in it. I just needed to find $c$ (capacitance) for this problem where the total distance ($D$) was also given. But I don't think this integration which is $$∫\frac{1}{dc} = ∫\frac{dx}{εx}$$ [$ε$ is constant] can be solved. Can it be? Or am I doing something wrong?,,"['ordinary-differential-equations', 'physics']"
28,"Who's the ""Author"" of the integrating factor method?","Who's the ""Author"" of the integrating factor method?",,"I've always been interested in how someone discovered this method, it felt pretty magical when I first learnt it, and I've been wondering who discovered/how was it derived for the first time . Does anyone here happen to know this?","I've always been interested in how someone discovered this method, it felt pretty magical when I first learnt it, and I've been wondering who discovered/how was it derived for the first time . Does anyone here happen to know this?",,"['ordinary-differential-equations', 'math-history']"
29,$ y' = x^2 + y^2 $ asymptote,asymptote, y' = x^2 + y^2 ,For the D.E.  $$ y' = x^2 + y^2 $$ show that the solution with $y(0) = 0$ has a vertical asymptote at some point $x_0$. Try to find upper and lower bounds for $x_0$. I have also been given the hint that a crude estimate for the bounds is  $$ \sqrt{\frac{\pi}{2}} < x_0 < 2\sqrt{\frac{\pi}{2}} $$ however I am unsure about how to obtain these bounds. My approach so far has been to compare the differential equation above to the equation: $$y' = 1 + y^2 $$ which we know has the solution of $ y = \tan(x + c) $ and asymptotes at $x = \pm \frac{\pi}{2}$. But apart from that I am stuck.,For the D.E.  $$ y' = x^2 + y^2 $$ show that the solution with $y(0) = 0$ has a vertical asymptote at some point $x_0$. Try to find upper and lower bounds for $x_0$. I have also been given the hint that a crude estimate for the bounds is  $$ \sqrt{\frac{\pi}{2}} < x_0 < 2\sqrt{\frac{\pi}{2}} $$ however I am unsure about how to obtain these bounds. My approach so far has been to compare the differential equation above to the equation: $$y' = 1 + y^2 $$ which we know has the solution of $ y = \tan(x + c) $ and asymptotes at $x = \pm \frac{\pi}{2}$. But apart from that I am stuck.,,"['ordinary-differential-equations', 'bessel-functions']"
30,Is there an English edition of Jorge Sotomayor's book on differential equations?,Is there an English edition of Jorge Sotomayor's book on differential equations?,,"I am currently using Jorge Sotomayor's ""Lições de equações diferenciais ordinárias"", in Portuguese. However, Portuguese is not my best language by a long shot, and I struggle a little. Is there an English edition of this book? I have tried looking in different sites, with no luck. The only possible mention is in another thread: Good book to study ODEs throgh geometric ideas It's mentioned in the first comment with an English title: Jorge Sotomayor, Lessons of Ordinary Differential Equations, IMPA, 1979. But I haven't been able to find it anywhere. The reason I need to use this book is because my college course follows it's material and it's presented a bit differently from the other books I have tried.","I am currently using Jorge Sotomayor's ""Lições de equações diferenciais ordinárias"", in Portuguese. However, Portuguese is not my best language by a long shot, and I struggle a little. Is there an English edition of this book? I have tried looking in different sites, with no luck. The only possible mention is in another thread: Good book to study ODEs throgh geometric ideas It's mentioned in the first comment with an English title: Jorge Sotomayor, Lessons of Ordinary Differential Equations, IMPA, 1979. But I haven't been able to find it anywhere. The reason I need to use this book is because my college course follows it's material and it's presented a bit differently from the other books I have tried.",,"['ordinary-differential-equations', 'reference-request']"
31,Solution of a special case of Abel's differential equation of the second kind,Solution of a special case of Abel's differential equation of the second kind,,"My teacher mentioned to the class the following case of Abel's equation: $$y\cdot\frac{dy}{dx}-y=Ax+B$$ where $A, B\in \Bbb R $. I have thoroughly searched the Web, but I haven't found a certain sufficient way to solve it. Any ideas?","My teacher mentioned to the class the following case of Abel's equation: $$y\cdot\frac{dy}{dx}-y=Ax+B$$ where $A, B\in \Bbb R $. I have thoroughly searched the Web, but I haven't found a certain sufficient way to solve it. Any ideas?",,[]
32,Solution of $ f \circ f=f'$,Solution of, f \circ f=f',"Let $f:\mathbb R \to \mathbb R $ be a function such that $f \circ  f=f'$ and $f(0)=0$ , I proved that $f$ is the null function. Can we prove that the same result holds if we change $f \circ f=f'$ by $f \circ f \circ f \circ f=f'$? Thank you for your help.","Let $f:\mathbb R \to \mathbb R $ be a function such that $f \circ  f=f'$ and $f(0)=0$ , I proved that $f$ is the null function. Can we prove that the same result holds if we change $f \circ f=f'$ by $f \circ f \circ f \circ f=f'$? Thank you for your help.",,"['ordinary-differential-equations', 'functions']"
33,Writing nonautonomous systems as autonomous systems,Writing nonautonomous systems as autonomous systems,,Apparently any mth order nonautonomous system is equivalent to a first order autonomous system in higher dimensional space. How does this work in practice? I would you write $\displaystyle \frac{d^3x}{dt^3}=sin(t) \frac{x\ddot{x}}{\dot{x}^2}$ as a first order autonomous system on $\mathbb{R^4}$. I would like hints on how to start this question and no full solutions please.,Apparently any mth order nonautonomous system is equivalent to a first order autonomous system in higher dimensional space. How does this work in practice? I would you write $\displaystyle \frac{d^3x}{dt^3}=sin(t) \frac{x\ddot{x}}{\dot{x}^2}$ as a first order autonomous system on $\mathbb{R^4}$. I would like hints on how to start this question and no full solutions please.,,"['ordinary-differential-equations', 'manifolds']"
34,"What do mathematicians mean by ""analytical solution of an equation""?","What do mathematicians mean by ""analytical solution of an equation""?",,"Given a PDE equations of the form: $\dfrac{\partial}{\partial t} u(t,x) = \left(\hat{L}+\hat{N_u}\right)u(t,x)    \;\;\;\;\;\;\hspace{10mm}(**)$ where $\hat{L}$ is a linear operator  and  $\hat{N_u}$ is a non linear operator what does mathematicians mean by: 1) ""eq ($**$) has a general exact solution""; 2) ""eq ($**$) has not a general exact solution, some particular solutions are exact other has to be found numerically"" 3) ""eq ($**$) has no exact solution, it can by solved in an approximated way using numerical methods"" Some examples: case 1) the KdV equation $\dfrac{\partial u}{\partial t} +6 u \dfrac{\partial u}{\partial x} + \dfrac{\partial^3 u}{\partial x^3}=0$ admits a general exact solution via the inverse scattering transform . (And in this case I have not understood why there is a substantial literature about KdV numerical solution if it admits an exact general solution). case 2) The three body problem is famous for not admitting an exact general solution but here http://www.physics.buffalo.edu/phy410-505-2009/topic3/lec-3-4.pdf I have read that Chenciner and Montgomery proved in 2000 that a numerical solution found by Moore with the three bodies chasing one another around a ﬁgure of eight path was exact. case 3)  The Schroedinger equation of the Helium Atom.","Given a PDE equations of the form: $\dfrac{\partial}{\partial t} u(t,x) = \left(\hat{L}+\hat{N_u}\right)u(t,x)    \;\;\;\;\;\;\hspace{10mm}(**)$ where $\hat{L}$ is a linear operator  and  $\hat{N_u}$ is a non linear operator what does mathematicians mean by: 1) ""eq ($**$) has a general exact solution""; 2) ""eq ($**$) has not a general exact solution, some particular solutions are exact other has to be found numerically"" 3) ""eq ($**$) has no exact solution, it can by solved in an approximated way using numerical methods"" Some examples: case 1) the KdV equation $\dfrac{\partial u}{\partial t} +6 u \dfrac{\partial u}{\partial x} + \dfrac{\partial^3 u}{\partial x^3}=0$ admits a general exact solution via the inverse scattering transform . (And in this case I have not understood why there is a substantial literature about KdV numerical solution if it admits an exact general solution). case 2) The three body problem is famous for not admitting an exact general solution but here http://www.physics.buffalo.edu/phy410-505-2009/topic3/lec-3-4.pdf I have read that Chenciner and Montgomery proved in 2000 that a numerical solution found by Moore with the three bodies chasing one another around a ﬁgure of eight path was exact. case 3)  The Schroedinger equation of the Helium Atom.",,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods']"
35,"How to prove: $f(x)$ is differentiable on $(0,+\infty)$",How to prove:  is differentiable on,"f(x) (0,+\infty)","The function $f(x)$ is defined on    $(0,+\infty)$. We know $f'(1)$ exists and we have that $$\forall x,y \in(0,+\infty), \quad  f(xy)=yf(x)+xf(y)$$ How to prove:$f(x)$ is differentiable on $(0,+\infty)$ and$$f'(x)=\frac{f(x)}{x}+f'(1)?$$ I've got $f(1)=0$, but have no idea to prove other things.","The function $f(x)$ is defined on    $(0,+\infty)$. We know $f'(1)$ exists and we have that $$\forall x,y \in(0,+\infty), \quad  f(xy)=yf(x)+xf(y)$$ How to prove:$f(x)$ is differentiable on $(0,+\infty)$ and$$f'(x)=\frac{f(x)}{x}+f'(1)?$$ I've got $f(1)=0$, but have no idea to prove other things.",,"['calculus', 'ordinary-differential-equations', 'functions']"
36,"Elliptic PDE, uniqueness of solution","Elliptic PDE, uniqueness of solution",,"I'm considering a partial differential equation of the form $$\nabla^2 u + \mathbf{a}\cdot\nabla u = 0$$ with Dirichlet boundary conditions, where $\mathbf{a}$ is a (smooth, nonconstant) vector field. Can anybody give me a hint how to find sufficient conditions on $\mathbf{a}$ so that the solution is unique?  Thank you.","I'm considering a partial differential equation of the form $$\nabla^2 u + \mathbf{a}\cdot\nabla u = 0$$ with Dirichlet boundary conditions, where $\mathbf{a}$ is a (smooth, nonconstant) vector field. Can anybody give me a hint how to find sufficient conditions on $\mathbf{a}$ so that the solution is unique?  Thank you.",,"['ordinary-differential-equations', 'partial-differential-equations']"
37,Transition time in a Lotka-Volterra system,Transition time in a Lotka-Volterra system,,"I am working with a set of real-valued ordinary differential equations based on the Lotka-Volterra competition equations : $$\begin{align} \dot{a_1} & = a_1 \left( 1 - a_1 - 2 a_2 \right) \\ \dot{a_2} & = a_2 \left( 1 - a_2 - (1-1/\nu) a_1 \right) \end{align}$$ where $a_{1,2} \in [0,1]$ and $\nu \ge 1$. I would like to obtain a closed form (or analytical) solution for the time, $\tau$, it takes for this system to transit between two regions in state space. Specifically, I would like to solve for $\tau$ given $a_1(0) = \delta$ and $a_2(\tau) = \delta$, where $1/2 \lt \delta \lt 1$. This is along the manifold between two fixed points: the unstable manifold of a saddle at $(a_1,a_2) = (1,0)$ and a stable equilibrium at $(a_1,a_2) = (0,1)$. There is a solution for $\nu = 1$ ($\tau = -2 \text{ln}(1/\delta-1)$), but I have been unable to find a solution for the more general case in terms of $\nu$ (or even for any particular value of $\nu \gt 1$, including the limiting case of $\nu \rightarrow \infty$). This paper seems to imply that the equations above are not fully integrable except when $\nu = 1$ (see Eq. 25'), i.e, when the second equation is not a function of $a_1$. However, I'm not actually interested in solving this system for all time over the full state space. Question: Are there any methods to solve or obtain a reliable approximation for $\tau = f(\nu, \delta)$ for this system just within my region of interest? Attempts: In addition to paper and pencil, I have used Matlab's dsolve and Mathematica's DSolve along with assumptions to try to solve the ODEs for the specified boundary conditions. I was unable solve the system using these, but might there be ways to transform or break up the system that would facilitate a solution? I have tried using low-order power series, e.g., this paper , about each of the equilibria to obtain functions of time that are then inverted to solve for $\tau$. This was far from accurate as only a few series terms can be used. I have tried schemes based on simulating the ODE and fitting the transit times to a function of $\nu$. This requires finding initial conditions that lie on the manifold. How can that be done reliably as a function of $\nu$ and $\delta$? And can fitting methods be adapted to more general forms of the equation above (i.e., reusing the same fitting coefficients and without fitting high dimensional surfaces)? I am more interested in non-fitting-based solutions, but if you can demonstrate something that works well, I would be happy to look at it. Update 1 – Mar. 17, 2014 : The nullclines of the system and the Jacobian determinant (related to curvature) can be used to obtain estimated initial and final conditions. Solving for the roots of $\det(J)=0$ as a function of $a_1$ and scaling appropriately, one obtains $$ a_2(t) = \tfrac{1}{2} \left( 2 - a_1(t) - \sqrt{a_1(t) \left( \tfrac{2}{\nu} \left( a_1(t)-1 \right) + 2 - a_1(t) \right)} \right)$$ For $a_1(0) = 1-\delta$, this expression appears to provide a good approximation for $a_2(0)$ on the stable manifold (attracting contour) in question. It is less reliable for obtaining an estimate for $a_1(\tau)$. Perhaps this is due to the Jacobian evaluated at $(a_1,a_2) = (0,1)$ being a defective matrix with only one eigenvector. Update 2 – Mar. 25, 2014 – (post bounty): The equation from @JJacquelin's answer immediately after the transformation to polar coordinates can be simplified and integrated definitely (I used Mathematica 9) with respect to the bounds at times $0$ and $\tau$: $$\begin{align} \int_0^\tau{dt} & = \int_{\theta_0}^{\theta_{\tau}}{\frac{\sin^3\theta + \cos^3\theta + \left(2\cos\theta + \left(1-\tfrac{1}{\nu}\right)\sin\theta\right)\sin\theta\cos\theta}{\left(\tfrac{1}{\nu} \cos \theta + \sin \theta\right)\sin\theta\cos \theta}d\theta} + \int_{\rho_0}^{\rho_{\tau}}{\frac{1}{\rho}d\rho} \\ \tau & = \int_{\theta_0}^{\theta_{\tau}}{\frac{1-\tfrac{1}{\nu}+\left(2+\cot\theta\right)\cot\theta+\tan\theta}{1+\tfrac{1}{\nu}\cot\theta}d\theta} + \ln\left(\frac{\rho_{\tau}}{\rho_0}\right) \\ & = \frac{\nu^2}{1+\nu^2}\left(\ln\left(\frac{\cos(\theta_0)}{\cos(\theta_{\tau})}\right) +2\ln\left(\frac{\cos\theta_{\tau}+\nu\sin\theta_{\tau}}{cos\theta_0+\nu\sin\theta_0}\right)+\frac{1}{\nu^2}\ln\left(\frac{\cos\theta_{\tau}\left(1+\nu\tan\theta_{\tau}\right)^2}{\cos\theta_0\left(1+\nu\tan\theta_0\right)^2}\right)\right) + \nu\ln\left(\frac{\nu+\cot\theta_0}{\nu+\cot\theta_{\tau}}\right) + \ln\left(\frac{\rho_{\tau}}{\rho_0}\right) \end{align}$$ Transforming back from polar to Cartesian coordinates further simplifies the expression: $$\tau = \ln\left(\frac{a_1(0)}{a_1(\tau)}\right) + 2\ln\left(\frac{a_1(\tau)+\nu a_2(\tau)}{a_1(0)+\nu a_2(0)}\right) + \nu\ln\left(\frac{a_2(\tau)\left(a_1(0)+\nu a_2(0)\right)}{a_2(0)\left(a_1(\tau)+\nu a_2(\tau)\right)}\right)$$ Here is some ""simple"" Matlab code that demonstrates this. Also in the code is an ODE-based method that uses ode45 's event detection . This accurately finds the time $\tau$ by integrating along the manifold in question and terminating when the solution satisfies a condition. It's simple and fast for this basic case. However, recall that this system is a simplified version of a more general one in which the time scaling of the vector field can vary and even be different in each of the two dimensions. This could lead to long integration times if the scaling cannot be factored. I'm hoping that the analytical solution given here can be generalized. The analytic solution-based part of my Matlab code underestimates $\tau$ (except for $\nu$ close to $1$ when it slightly overestimates it). The source of almost all of the error is due to the estimate of $a_1(\tau)$ ( af1 in the code). Something else I'm looking into.","I am working with a set of real-valued ordinary differential equations based on the Lotka-Volterra competition equations : $$\begin{align} \dot{a_1} & = a_1 \left( 1 - a_1 - 2 a_2 \right) \\ \dot{a_2} & = a_2 \left( 1 - a_2 - (1-1/\nu) a_1 \right) \end{align}$$ where $a_{1,2} \in [0,1]$ and $\nu \ge 1$. I would like to obtain a closed form (or analytical) solution for the time, $\tau$, it takes for this system to transit between two regions in state space. Specifically, I would like to solve for $\tau$ given $a_1(0) = \delta$ and $a_2(\tau) = \delta$, where $1/2 \lt \delta \lt 1$. This is along the manifold between two fixed points: the unstable manifold of a saddle at $(a_1,a_2) = (1,0)$ and a stable equilibrium at $(a_1,a_2) = (0,1)$. There is a solution for $\nu = 1$ ($\tau = -2 \text{ln}(1/\delta-1)$), but I have been unable to find a solution for the more general case in terms of $\nu$ (or even for any particular value of $\nu \gt 1$, including the limiting case of $\nu \rightarrow \infty$). This paper seems to imply that the equations above are not fully integrable except when $\nu = 1$ (see Eq. 25'), i.e, when the second equation is not a function of $a_1$. However, I'm not actually interested in solving this system for all time over the full state space. Question: Are there any methods to solve or obtain a reliable approximation for $\tau = f(\nu, \delta)$ for this system just within my region of interest? Attempts: In addition to paper and pencil, I have used Matlab's dsolve and Mathematica's DSolve along with assumptions to try to solve the ODEs for the specified boundary conditions. I was unable solve the system using these, but might there be ways to transform or break up the system that would facilitate a solution? I have tried using low-order power series, e.g., this paper , about each of the equilibria to obtain functions of time that are then inverted to solve for $\tau$. This was far from accurate as only a few series terms can be used. I have tried schemes based on simulating the ODE and fitting the transit times to a function of $\nu$. This requires finding initial conditions that lie on the manifold. How can that be done reliably as a function of $\nu$ and $\delta$? And can fitting methods be adapted to more general forms of the equation above (i.e., reusing the same fitting coefficients and without fitting high dimensional surfaces)? I am more interested in non-fitting-based solutions, but if you can demonstrate something that works well, I would be happy to look at it. Update 1 – Mar. 17, 2014 : The nullclines of the system and the Jacobian determinant (related to curvature) can be used to obtain estimated initial and final conditions. Solving for the roots of $\det(J)=0$ as a function of $a_1$ and scaling appropriately, one obtains $$ a_2(t) = \tfrac{1}{2} \left( 2 - a_1(t) - \sqrt{a_1(t) \left( \tfrac{2}{\nu} \left( a_1(t)-1 \right) + 2 - a_1(t) \right)} \right)$$ For $a_1(0) = 1-\delta$, this expression appears to provide a good approximation for $a_2(0)$ on the stable manifold (attracting contour) in question. It is less reliable for obtaining an estimate for $a_1(\tau)$. Perhaps this is due to the Jacobian evaluated at $(a_1,a_2) = (0,1)$ being a defective matrix with only one eigenvector. Update 2 – Mar. 25, 2014 – (post bounty): The equation from @JJacquelin's answer immediately after the transformation to polar coordinates can be simplified and integrated definitely (I used Mathematica 9) with respect to the bounds at times $0$ and $\tau$: $$\begin{align} \int_0^\tau{dt} & = \int_{\theta_0}^{\theta_{\tau}}{\frac{\sin^3\theta + \cos^3\theta + \left(2\cos\theta + \left(1-\tfrac{1}{\nu}\right)\sin\theta\right)\sin\theta\cos\theta}{\left(\tfrac{1}{\nu} \cos \theta + \sin \theta\right)\sin\theta\cos \theta}d\theta} + \int_{\rho_0}^{\rho_{\tau}}{\frac{1}{\rho}d\rho} \\ \tau & = \int_{\theta_0}^{\theta_{\tau}}{\frac{1-\tfrac{1}{\nu}+\left(2+\cot\theta\right)\cot\theta+\tan\theta}{1+\tfrac{1}{\nu}\cot\theta}d\theta} + \ln\left(\frac{\rho_{\tau}}{\rho_0}\right) \\ & = \frac{\nu^2}{1+\nu^2}\left(\ln\left(\frac{\cos(\theta_0)}{\cos(\theta_{\tau})}\right) +2\ln\left(\frac{\cos\theta_{\tau}+\nu\sin\theta_{\tau}}{cos\theta_0+\nu\sin\theta_0}\right)+\frac{1}{\nu^2}\ln\left(\frac{\cos\theta_{\tau}\left(1+\nu\tan\theta_{\tau}\right)^2}{\cos\theta_0\left(1+\nu\tan\theta_0\right)^2}\right)\right) + \nu\ln\left(\frac{\nu+\cot\theta_0}{\nu+\cot\theta_{\tau}}\right) + \ln\left(\frac{\rho_{\tau}}{\rho_0}\right) \end{align}$$ Transforming back from polar to Cartesian coordinates further simplifies the expression: $$\tau = \ln\left(\frac{a_1(0)}{a_1(\tau)}\right) + 2\ln\left(\frac{a_1(\tau)+\nu a_2(\tau)}{a_1(0)+\nu a_2(0)}\right) + \nu\ln\left(\frac{a_2(\tau)\left(a_1(0)+\nu a_2(0)\right)}{a_2(0)\left(a_1(\tau)+\nu a_2(\tau)\right)}\right)$$ Here is some ""simple"" Matlab code that demonstrates this. Also in the code is an ODE-based method that uses ode45 's event detection . This accurately finds the time $\tau$ by integrating along the manifold in question and terminating when the solution satisfies a condition. It's simple and fast for this basic case. However, recall that this system is a simplified version of a more general one in which the time scaling of the vector field can vary and even be different in each of the two dimensions. This could lead to long integration times if the scaling cannot be factored. I'm hoping that the analytical solution given here can be generalized. The analytic solution-based part of my Matlab code underestimates $\tau$ (except for $\nu$ close to $1$ when it slightly overestimates it). The source of almost all of the error is due to the estimate of $a_1(\tau)$ ( af1 in the code). Something else I'm looking into.",,"['ordinary-differential-equations', 'dynamical-systems', 'closed-form', 'systems-of-equations']"
38,Solving the Weierstrass differential equation for $g_2^3=27g_3^2$,Solving the Weierstrass differential equation for,g_2^3=27g_3^2,"I am asked to solve the Weierstrass Differential Equation: $$(y')^2=4y^3-g_2y-g_3.$$ in the case that $g_2^3=27g_3^2$. This seems easy, but I can't quite figure it out. Any ideas?","I am asked to solve the Weierstrass Differential Equation: $$(y')^2=4y^3-g_2y-g_3.$$ in the case that $g_2^3=27g_3^2$. This seems easy, but I can't quite figure it out. Any ideas?",,['ordinary-differential-equations']
39,Converting Second Order Linear Equations to First Order Linear Equations,Converting Second Order Linear Equations to First Order Linear Equations,,$\color{green}{\text{Question}}$: How can the following $\color{blue}{\text{second-order linear equation}}$ be converted into a $\color{blue}{\text{first-order linear equation}}$? This is our second-order linear Equation :   $${y}''-2y'+2y=e^{2t}\sin t$$ $\color{green}{\text{I think}\ldots}$ This equation contains the dependent variable and the independent variables . Equation is not complete. Equation is not homogeneous. How can I convert it to a first-order linear equation? Thank you for any hint.,$\color{green}{\text{Question}}$: How can the following $\color{blue}{\text{second-order linear equation}}$ be converted into a $\color{blue}{\text{first-order linear equation}}$? This is our second-order linear Equation :   $${y}''-2y'+2y=e^{2t}\sin t$$ $\color{green}{\text{I think}\ldots}$ This equation contains the dependent variable and the independent variables . Equation is not complete. Equation is not homogeneous. How can I convert it to a first-order linear equation? Thank you for any hint.,,['ordinary-differential-equations']
40,How to solve $u'' + k u + \epsilon u^3 = 0$?,How to solve ?,u'' + k u + \epsilon u^3 = 0,"I am looking at the project of my ODE class, there is one problem saying we have to solve $u'' + k u + \epsilon u^3 = 0$. The problem gives us some values of $k$, $\epsilon$ and says you should experiment with different initial values with Euler's method. I have solved the equation using Euler's method. But now I am confused, how can I know I get the right solution, in previous exercises the book gave the answer for the function and we can compare it with the Euler answer, the project page says nothing about the answer. I tried letting $u = e^{rt}$ like in the class to get the characteristics equation if both constants are 1: $$ r^2 + r + e^{2rt}=0 $$ but I can't solve this. Thanks alot. The initial values are $u(0)=0$, $u'(0)=1$, and $k=\epsilon =1$.","I am looking at the project of my ODE class, there is one problem saying we have to solve $u'' + k u + \epsilon u^3 = 0$. The problem gives us some values of $k$, $\epsilon$ and says you should experiment with different initial values with Euler's method. I have solved the equation using Euler's method. But now I am confused, how can I know I get the right solution, in previous exercises the book gave the answer for the function and we can compare it with the Euler answer, the project page says nothing about the answer. I tried letting $u = e^{rt}$ like in the class to get the characteristics equation if both constants are 1: $$ r^2 + r + e^{2rt}=0 $$ but I can't solve this. Thanks alot. The initial values are $u(0)=0$, $u'(0)=1$, and $k=\epsilon =1$.",,['ordinary-differential-equations']
41,Find a Solution using Green's Function,Find a Solution using Green's Function,,"Find a solution to the given initial-value problem using Green's functions $$y""+3y'+2y= \frac{1}{1+e^x}; y(0)=0,y'(0)=1$$ So I have figured out that $$y_c=e^{-x}-e^{-2x}$$ Now I am having issues figuring out what $y_p$ is. This is what I have so far: $$y_1=e^{-x}, y_2=e^{-2x},$$ so $$W(y_1, y_2)=-e^{-3x}.$$ $$G(x,t)=\frac{e^{-t}e^{-2x}-e^{-x}e^{-2t}}{-e^{-3x}} = -e^{-t}e^{x}+e^{-2t}e^{2x}$$ $$y_p=\int^x_0 G(x,t)f(t)dt$$ $$y_p=-e^x \int^x_0 \frac{e^{-t}}{1+e^t}dt+e^{2x} \int^x_0 \frac{e^{-2t}}{1+e^t}dt$$ So $$y_p=-\ln(e^{-x}+1)(e^x+e^{2x})-\frac{e^{2x}}{2}+\frac{1}{2}+(e^{x}+e^{2x})\ln(2)$$ Is this correct?","Find a solution to the given initial-value problem using Green's functions $$y""+3y'+2y= \frac{1}{1+e^x}; y(0)=0,y'(0)=1$$ So I have figured out that $$y_c=e^{-x}-e^{-2x}$$ Now I am having issues figuring out what $y_p$ is. This is what I have so far: $$y_1=e^{-x}, y_2=e^{-2x},$$ so $$W(y_1, y_2)=-e^{-3x}.$$ $$G(x,t)=\frac{e^{-t}e^{-2x}-e^{-x}e^{-2t}}{-e^{-3x}} = -e^{-t}e^{x}+e^{-2t}e^{2x}$$ $$y_p=\int^x_0 G(x,t)f(t)dt$$ $$y_p=-e^x \int^x_0 \frac{e^{-t}}{1+e^t}dt+e^{2x} \int^x_0 \frac{e^{-2t}}{1+e^t}dt$$ So $$y_p=-\ln(e^{-x}+1)(e^x+e^{2x})-\frac{e^{2x}}{2}+\frac{1}{2}+(e^{x}+e^{2x})\ln(2)$$ Is this correct?",,['ordinary-differential-equations']
42,"Differential equation, perturbation method","Differential equation, perturbation method",,"Consider the one dimensional ODE $\frac{dy}{dx}=\epsilon y^2 +x$, where $y=y(x,\epsilon)$. Discuss the effect of changes in the values of parameter $\epsilon$ in the solution (hint: assume initially $\epsilon_{0} =0$ and initial condition $y(0,0)=0$, then see how different $y(x,0)$ is from the $y(x,\epsilon)$. I first tried to solve the ODE, but I couldn't. How can I apply the perturbation method here?","Consider the one dimensional ODE $\frac{dy}{dx}=\epsilon y^2 +x$, where $y=y(x,\epsilon)$. Discuss the effect of changes in the values of parameter $\epsilon$ in the solution (hint: assume initially $\epsilon_{0} =0$ and initial condition $y(0,0)=0$, then see how different $y(x,0)$ is from the $y(x,\epsilon)$. I first tried to solve the ODE, but I couldn't. How can I apply the perturbation method here?",,['ordinary-differential-equations']
43,Examples of systems conforming the Lorenz Attractor,Examples of systems conforming the Lorenz Attractor,,"Might sound like a trivial question but would you please show me some examples of real systems conforming the Lorenz Attractor? It can be any kind of system, just a little list. It can be a system arising from Economical, Social, Biological or Physical matters.","Might sound like a trivial question but would you please show me some examples of real systems conforming the Lorenz Attractor? It can be any kind of system, just a little list. It can be a system arising from Economical, Social, Biological or Physical matters.",,"['ordinary-differential-equations', 'dynamical-systems', 'chaos-theory']"
44,Green's function for a particular operator,Green's function for a particular operator,,"Lately, I've been trying to solve differential equations of the form $$f''+k^2 f =g~~,$$ and $g$ is a continuous function on $[0,2\pi]$. A friend mentioned that I check out Green's function. Unfortunately, I found it hard understanding most the materials I found online. I'm hoping someone here will find time to make me understand better. Let  $$D=\frac{\mathrm{d}^2}{\mathrm{d}x^2}+k^2~~~~k\in \mathbf R,$$ how can I find the Green's  function for $D$?","Lately, I've been trying to solve differential equations of the form $$f''+k^2 f =g~~,$$ and $g$ is a continuous function on $[0,2\pi]$. A friend mentioned that I check out Green's function. Unfortunately, I found it hard understanding most the materials I found online. I'm hoping someone here will find time to make me understand better. Let  $$D=\frac{\mathrm{d}^2}{\mathrm{d}x^2}+k^2~~~~k\in \mathbf R,$$ how can I find the Green's  function for $D$?",,['ordinary-differential-equations']
45,General solution of constant-coefficient second-order linear ODE,General solution of constant-coefficient second-order linear ODE,,"I am trying to look a bit deeper into the mathematics the equation of motion used in physics and engineering. I have some specific questions at the end, but please correct me if I make a mistake in  my statement. I have an ordinary, second-order, linear, homogeneous differential equation: $$ M \ddot{y} + C \dot{y} + K y  = 0 $$ where $M$, $C$ and $K$ are real valued. I am familiar with an Ad-hoc method of solving this equation. Namely, I assume a solution  takes form of $$ y_k(t) = Y_k e^{\omega_k t} $$ substitute the form into the ODE, $$ \left( M \omega_k^2 + C \omega_k + K\right) y_k  = 0, $$ and solve for $\omega_k$ such that $y_k \neq 0$. If $y$ is in dimension $N$,  we generally have $2N$ solutions, where for each $k$, $\omega_k$ and its complex conjugate $\tilde{\omega}_k$ are solutions.  Since our ODE is linear, the general solution is a combination of the individual solutions $$ y(t) = \sum_{k = 1}^N Y_k e^{\omega_k t} + \tilde{Y}_k e^{\tilde{\omega}_k t} $$ The values of $Y_k$ and $\tilde{Y}_k$ are determined from initial values, i.e. $y(0) = y_0$,  and further restrictions common in physics, such as $y(t)$ must be real for all $t \geq 0$. Now here are my questions: 1 - If we select the form $y_k(t) = Y_k e^{ \mathbf{i}\omega_k t}$, am I correct to say that the roots $\omega_k$ and $\tilde{\omega}_k$ are no longer complex conjugates, but $-1$ times their complex conjugates? 2 - How do we know that the general solution cannot contain terms which are not representable by the exponential form? In other words, is our general solution truly general?","I am trying to look a bit deeper into the mathematics the equation of motion used in physics and engineering. I have some specific questions at the end, but please correct me if I make a mistake in  my statement. I have an ordinary, second-order, linear, homogeneous differential equation: $$ M \ddot{y} + C \dot{y} + K y  = 0 $$ where $M$, $C$ and $K$ are real valued. I am familiar with an Ad-hoc method of solving this equation. Namely, I assume a solution  takes form of $$ y_k(t) = Y_k e^{\omega_k t} $$ substitute the form into the ODE, $$ \left( M \omega_k^2 + C \omega_k + K\right) y_k  = 0, $$ and solve for $\omega_k$ such that $y_k \neq 0$. If $y$ is in dimension $N$,  we generally have $2N$ solutions, where for each $k$, $\omega_k$ and its complex conjugate $\tilde{\omega}_k$ are solutions.  Since our ODE is linear, the general solution is a combination of the individual solutions $$ y(t) = \sum_{k = 1}^N Y_k e^{\omega_k t} + \tilde{Y}_k e^{\tilde{\omega}_k t} $$ The values of $Y_k$ and $\tilde{Y}_k$ are determined from initial values, i.e. $y(0) = y_0$,  and further restrictions common in physics, such as $y(t)$ must be real for all $t \geq 0$. Now here are my questions: 1 - If we select the form $y_k(t) = Y_k e^{ \mathbf{i}\omega_k t}$, am I correct to say that the roots $\omega_k$ and $\tilde{\omega}_k$ are no longer complex conjugates, but $-1$ times their complex conjugates? 2 - How do we know that the general solution cannot contain terms which are not representable by the exponential form? In other words, is our general solution truly general?",,['ordinary-differential-equations']
46,Simple simple Euler Lagrange Equation,Simple simple Euler Lagrange Equation,,"Just starting a course on Lagrangian Mechanics and I'm just wondering what about the Euler-Lagrange equation, and more specifically what I'm meant to be trying to do .. One of the questions from my textbook reads : Solve the Euler-Lagrange equation for the following function \begin{align*} f(y,y') = y^2+y'^2 \end{align*} Looks simple enough... But where should I be headed to start? The Euler-Lagrange Equation as we have it is \begin{align*} \frac{\partial f}{\partial y} = \frac{d}{dx} \frac{\partial f}{\partial y'} \end{align*} Do I just find the partial derivatives of f treating y and y' as independent variables as follows: $$\frac{\partial f}{\partial y} = 2y,$$ $$\frac{\partial f}{\partial y'} = 2y$$ so then we have : $$2y = \frac{d}{dx}(2y')$$ $$y = \frac{d}{dx}(y')$$ $$yx+c = y'$$ $$\frac{yx^2}{2}+cx+d = y$$ And then simplify with the y isolated on the right so it looks nice .. or at least thats what I thought.. Any comments ? right track, wrong track?  Also, what is this euler equation actually solving for? Is it just a nice way of solving differential equations in this form ? When we did this in class, we did a monster proof showing it minimizes the path between 2 points or finds the extrema for the function $I(x) = \int^a_b \ F(y(x),y'(x),x) dx,$ which is all great but I'm not too sure how this relates to the equation we solve in the question .. Thanks for all","Just starting a course on Lagrangian Mechanics and I'm just wondering what about the Euler-Lagrange equation, and more specifically what I'm meant to be trying to do .. One of the questions from my textbook reads : Solve the Euler-Lagrange equation for the following function \begin{align*} f(y,y') = y^2+y'^2 \end{align*} Looks simple enough... But where should I be headed to start? The Euler-Lagrange Equation as we have it is \begin{align*} \frac{\partial f}{\partial y} = \frac{d}{dx} \frac{\partial f}{\partial y'} \end{align*} Do I just find the partial derivatives of f treating y and y' as independent variables as follows: $$\frac{\partial f}{\partial y} = 2y,$$ $$\frac{\partial f}{\partial y'} = 2y$$ so then we have : $$2y = \frac{d}{dx}(2y')$$ $$y = \frac{d}{dx}(y')$$ $$yx+c = y'$$ $$\frac{yx^2}{2}+cx+d = y$$ And then simplify with the y isolated on the right so it looks nice .. or at least thats what I thought.. Any comments ? right track, wrong track?  Also, what is this euler equation actually solving for? Is it just a nice way of solving differential equations in this form ? When we did this in class, we did a monster proof showing it minimizes the path between 2 points or finds the extrema for the function $I(x) = \int^a_b \ F(y(x),y'(x),x) dx,$ which is all great but I'm not too sure how this relates to the equation we solve in the question .. Thanks for all",,"['ordinary-differential-equations', 'physics', 'calculus-of-variations']"
47,Model of spread of a rumor,Model of spread of a rumor,,"From Stewart 7e pg 614 # 9 ""One model for the spread of a rumor is that the rate of spread is proportional to the product of the fraction y of the population who have heard th eremor and the fraction who have not hear the rumor. a) Write a differential equation that is satisfied by y. b) Solve the differential equation c) A small town has 1000 inhabitants. At 8 am 80 people have heard a rumor. By noon half the town has heard it. At what will 90 percent of the population have heard the rumor? "" The wording of this is very ambiguous to me and I can't really make sense of it. They mention a product, so I know that something is being multiplied and that y is a fraction which belongs to the population who have seen it so I think that ""have not"" heard is a constant, and that y is a fraction taht represents who have. I tried to set this up and it is the wrong answer. I am not sure what they want from that, the English usage is too ambiguous to make sense of it. The complete lack of punctuation is what really does it.","From Stewart 7e pg 614 # 9 ""One model for the spread of a rumor is that the rate of spread is proportional to the product of the fraction y of the population who have heard th eremor and the fraction who have not hear the rumor. a) Write a differential equation that is satisfied by y. b) Solve the differential equation c) A small town has 1000 inhabitants. At 8 am 80 people have heard a rumor. By noon half the town has heard it. At what will 90 percent of the population have heard the rumor? "" The wording of this is very ambiguous to me and I can't really make sense of it. They mention a product, so I know that something is being multiplied and that y is a fraction which belongs to the population who have seen it so I think that ""have not"" heard is a constant, and that y is a fraction taht represents who have. I tried to set this up and it is the wrong answer. I am not sure what they want from that, the English usage is too ambiguous to make sense of it. The complete lack of punctuation is what really does it.",,['calculus']
48,Differential equations notation,Differential equations notation,,"I've always wondered why does the differential equation notation for linear equations differ from the standard terminology of vector spaces. We all know that the equation $y'' + p(x) y' + q(x)y = g(x)$ for some function $g$ is called linear and that the associated equation $y'' + p(x)y' + q(x) y = 0$ is called homogeneous . But why is that? WHY should mathematicians explicitly cause confusion with the rest of the theory of vector spaces? What I mean by that is : Why not call the equation $y'' + p(x)y' + q(x)y = g(x)$ an affine equation and call $y' + p(x) y' + q(x) y = 0$ a linear equation? Because linear equations (in the sense of differential equations) are not linear in the sense of vector spaces unless they're homogeneous ; and linear equations (in the sense of differential equations) remind me more of a linear system of the form $Ax = b$ (which is called an affine equation in vector space theory) than of a linear equation at all. Just so that I made myself clear ; I perfectly know the difference between linear equations in linear algebra and linear equations in differential equations theory ; I'm asking for some reason of ""why the name"". Thanks in advance,","I've always wondered why does the differential equation notation for linear equations differ from the standard terminology of vector spaces. We all know that the equation $y'' + p(x) y' + q(x)y = g(x)$ for some function $g$ is called linear and that the associated equation $y'' + p(x)y' + q(x) y = 0$ is called homogeneous . But why is that? WHY should mathematicians explicitly cause confusion with the rest of the theory of vector spaces? What I mean by that is : Why not call the equation $y'' + p(x)y' + q(x)y = g(x)$ an affine equation and call $y' + p(x) y' + q(x) y = 0$ a linear equation? Because linear equations (in the sense of differential equations) are not linear in the sense of vector spaces unless they're homogeneous ; and linear equations (in the sense of differential equations) remind me more of a linear system of the form $Ax = b$ (which is called an affine equation in vector space theory) than of a linear equation at all. Just so that I made myself clear ; I perfectly know the difference between linear equations in linear algebra and linear equations in differential equations theory ; I'm asking for some reason of ""why the name"". Thanks in advance,",,"['ordinary-differential-equations', 'notation']"
49,General solution to $\frac{df}{dx}=3f$,General solution to,\frac{df}{dx}=3f,I am learning how to do calculus and was presented with an example I am struggling a bit to understand. Why does $\frac{df}{dx}=3f$ have the general solution of $f(x)=Ce^{3x}$?,I am learning how to do calculus and was presented with an example I am struggling a bit to understand. Why does $\frac{df}{dx}=3f$ have the general solution of $f(x)=Ce^{3x}$?,,"['calculus', 'ordinary-differential-equations']"
50,Understanding a numerical integration method,Understanding a numerical integration method,,"I am working on a project that uses numerical integration to solve some differential equations. Not coming from a solid math background, I have a problem understanding some integration methods. Specifically, I have the following equation: $$ \frac{d y}{d t} = \frac{u - y}{v} \,, $$ where $u$ and $v$ are constants and $y(0)$ is known. I solved this initially with a simple Euler integration and then with some other higher-order methods (Runge-Kutta 4). Then, I found another numerical integrator that did: $$ y_{n+1} = y_{n} + (u - y_{n})(1 - e^{\frac{-\Delta t}{v}}) \,. $$ My question is: what is this integration method? I checked and the results are basically the same, but I had no idea about this technique and I would like to know more about it.","I am working on a project that uses numerical integration to solve some differential equations. Not coming from a solid math background, I have a problem understanding some integration methods. Specifically, I have the following equation: $$ \frac{d y}{d t} = \frac{u - y}{v} \,, $$ where $u$ and $v$ are constants and $y(0)$ is known. I solved this initially with a simple Euler integration and then with some other higher-order methods (Runge-Kutta 4). Then, I found another numerical integrator that did: $$ y_{n+1} = y_{n} + (u - y_{n})(1 - e^{\frac{-\Delta t}{v}}) \,. $$ My question is: what is this integration method? I checked and the results are basically the same, but I had no idea about this technique and I would like to know more about it.",,"['ordinary-differential-equations', 'numerical-methods']"
51,Differential Equations Flow-chart/genealogical diagram?,Differential Equations Flow-chart/genealogical diagram?,,"There are many methods to solve differential equations. There are many kinds of equations, different orders, linear, non-linear, homogeneous, exact, the other kind of homogeneous etc. I would like to know of any diagrams that organize equations in to families by the best suited solution method. Or maybe a genealogical diagram, or even a flow chart. Something that gives the ""big picture"" on all these methods and types of equations. Is there any such digram? If not how would you organize it? (the goal is for the diagram to assist one in choosing a method for solving, while at the same time clarifying the similarities and difference between equations. Something like the diagram that shows the real, imaginary, natural  and complex numbers, perhaps: Of course it would be more complex... and it might fill a wall, but I'm OK with that.","There are many methods to solve differential equations. There are many kinds of equations, different orders, linear, non-linear, homogeneous, exact, the other kind of homogeneous etc. I would like to know of any diagrams that organize equations in to families by the best suited solution method. Or maybe a genealogical diagram, or even a flow chart. Something that gives the ""big picture"" on all these methods and types of equations. Is there any such digram? If not how would you organize it? (the goal is for the diagram to assist one in choosing a method for solving, while at the same time clarifying the similarities and difference between equations. Something like the diagram that shows the real, imaginary, natural  and complex numbers, perhaps: Of course it would be more complex... and it might fill a wall, but I'm OK with that.",,"['ordinary-differential-equations', 'partial-differential-equations']"
52,Deriving a differential equation for momentum from the integral form,Deriving a differential equation for momentum from the integral form,,"The Lugiato-Lefever equation can be written in the form: $$ \frac{\partial \psi}{\partial \tau}  = -(1 + i\alpha)\psi - i\frac{\beta}{2}\frac{\partial ^2 \psi}{\partial \theta^2} + i|\psi|^2 \psi + F_0 \exp[i\delta_m \sin\theta]$$ The solutions to this partial differential equation can have soliton solutions. In a paper that I am reading, I found an equation for the ""momentum"" of these solitons: $$ P = -\frac{i}{2}\int_{-\pi}^{\pi} d\theta \left ( \psi^*\frac{\partial \psi}{\partial \theta} - \psi \frac{\partial \psi^*}{\partial \theta}   \right )$$ The authors then proceed to find the time derivative of the above equation and state without showing any calculations that it is given by: $$ \frac{dP}{d \tau} = -2P - i\int_{-\pi}^{\pi} d\theta \left ( \psi^*\frac{\partial F}{\partial \theta} - \psi \frac{\partial F^*}{\partial \theta}   \right )  $$ where $F(\theta,\tau) = F_0 \exp[i\delta_m \sin\theta]$ . I am struggling to derive the third equation from the second. I attempted to take the derivative of the second equation with respect to $\tau$ , but then I got stuck dealing with too many parameters and I feel like there is an easy way to show this.","The Lugiato-Lefever equation can be written in the form: The solutions to this partial differential equation can have soliton solutions. In a paper that I am reading, I found an equation for the ""momentum"" of these solitons: The authors then proceed to find the time derivative of the above equation and state without showing any calculations that it is given by: where . I am struggling to derive the third equation from the second. I attempted to take the derivative of the second equation with respect to , but then I got stuck dealing with too many parameters and I feel like there is an easy way to show this."," \frac{\partial \psi}{\partial \tau}  = -(1 + i\alpha)\psi - i\frac{\beta}{2}\frac{\partial ^2 \psi}{\partial \theta^2} + i|\psi|^2 \psi + F_0 \exp[i\delta_m \sin\theta]  P = -\frac{i}{2}\int_{-\pi}^{\pi} d\theta \left ( \psi^*\frac{\partial \psi}{\partial \theta} - \psi \frac{\partial \psi^*}{\partial \theta}   \right )  \frac{dP}{d \tau} = -2P - i\int_{-\pi}^{\pi} d\theta \left ( \psi^*\frac{\partial F}{\partial \theta} - \psi \frac{\partial F^*}{\partial \theta}   \right )   F(\theta,\tau) = F_0 \exp[i\delta_m \sin\theta] \tau","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
53,Solving an inexact ODE,Solving an inexact ODE,,"Given the ODE: $y'=\frac{2x^2y-xy^2+0.5y^2}{x^2-xy+y-1}$ Given that $M(x,y) = -2x^2y+xy^2-0.5y^2$ and $N(x,y)= x^2-xy+y-1$ , $N_x \neq M_y$ , I've been trying to look for an integration factor to make this ODE an exact one. The Integration factor I've found is: $ \mu(x) = \frac {e^{-2x+2}}{(x-1)^2}$ But that dosen't make the ODE exact. I've triple checked my way of solving and I can't find any reason to that error. I did try finding an integration factor for the variable $y$ but that doesn't solve it either. My way of finding the integration factor: $ \frac{M_y-N_x}{N(x,y)} = \frac{-2x^2+2xy-y-(2x-y)}{x^2-xy+y-1} =\frac{-2x^2+2xy-2x}{x^2-xy+y-1} = \frac{-2x(x-y+1)}{(x-1)(x-y+1)} = \frac{-2x}{x-1}$ $ \mu(x)  = e^{\int\frac{-2x}{x-1}dx} $ $=  e^{-2x+2-2\ln(x-1)}  = e^{-2x+2} \cdot \frac{1}{(x-1)^2}  $","Given the ODE: Given that and , , I've been trying to look for an integration factor to make this ODE an exact one. The Integration factor I've found is: But that dosen't make the ODE exact. I've triple checked my way of solving and I can't find any reason to that error. I did try finding an integration factor for the variable but that doesn't solve it either. My way of finding the integration factor:","y'=\frac{2x^2y-xy^2+0.5y^2}{x^2-xy+y-1} M(x,y) = -2x^2y+xy^2-0.5y^2 N(x,y)= x^2-xy+y-1 N_x \neq M_y  \mu(x) = \frac {e^{-2x+2}}{(x-1)^2} y  \frac{M_y-N_x}{N(x,y)} = \frac{-2x^2+2xy-y-(2x-y)}{x^2-xy+y-1} =\frac{-2x^2+2xy-2x}{x^2-xy+y-1} = \frac{-2x(x-y+1)}{(x-1)(x-y+1)} = \frac{-2x}{x-1}  \mu(x)  = e^{\int\frac{-2x}{x-1}dx}  =  e^{-2x+2-2\ln(x-1)}  = e^{-2x+2} \cdot \frac{1}{(x-1)^2}  ",['ordinary-differential-equations']
54,Reviewing a 2016 IITJEE problem,Reviewing a 2016 IITJEE problem,,"IITJEE has been asking novel and interesting problems at ancillary level for engineering entrance in India. A shortcoming occurs rather rarely. In 2016, they asked a MCQ (Multiple choice question) with one or more than one choices correct: Let $f:(0,\infty)\to R$ be a differentiable function such that $f'(x)=2-\frac{f(x)}{x}, \forall x \in (0,\infty)$ and $$f(1)\ne 1.....(*).$$ Then (A) $~ \lim_{x\to 0^+} f'(1/x)=1$ , $\quad \text{(B)}~ \lim_{x \to 0^+} xf(1/x)=2, \quad \text{(C)} \lim_{x \to 0^+} x^2 f'(x) =0,\quad$ $\text{(D)}\quad |f(x)|\le 2~ \forall x \in (0,2)$ We solve the ODE $$f'(x)=2-\frac{f(x)}{x} \implies xf'(x)+f(x)=2x \implies \frac{d}{dx} xf(x)=2x$$ By integrating we get $$xf(x)=x^2+C \implies f(x)=x+\frac{C}{x}.....(**)$$ Where, usually $C$ determined by a given value of $f(a)$ at some $x=a$ . However, here a peculiar condition ( $*$ ) has been given. Using ( $**$ ), we find that the option (A) is correct both with and without the condition ( $*$ ) given by them. Also, (A) option is the official solution. Here, the question is whether the stated problem  is all right. Your comments are welcome. EDIT : There was a typo in option (A),this particular option has  been corrected now. You may also see here for the original source.","IITJEE has been asking novel and interesting problems at ancillary level for engineering entrance in India. A shortcoming occurs rather rarely. In 2016, they asked a MCQ (Multiple choice question) with one or more than one choices correct: Let be a differentiable function such that and Then (A) , We solve the ODE By integrating we get Where, usually determined by a given value of at some . However, here a peculiar condition ( ) has been given. Using ( ), we find that the option (A) is correct both with and without the condition ( ) given by them. Also, (A) option is the official solution. Here, the question is whether the stated problem  is all right. Your comments are welcome. EDIT : There was a typo in option (A),this particular option has  been corrected now. You may also see here for the original source.","f:(0,\infty)\to R f'(x)=2-\frac{f(x)}{x}, \forall x \in (0,\infty) f(1)\ne 1.....(*). ~ \lim_{x\to 0^+} f'(1/x)=1 \quad \text{(B)}~ \lim_{x \to 0^+} xf(1/x)=2, \quad \text{(C)} \lim_{x \to 0^+} x^2 f'(x) =0,\quad \text{(D)}\quad |f(x)|\le 2~ \forall x \in (0,2) f'(x)=2-\frac{f(x)}{x} \implies xf'(x)+f(x)=2x \implies \frac{d}{dx} xf(x)=2x xf(x)=x^2+C \implies f(x)=x+\frac{C}{x}.....(**) C f(a) x=a * ** *",['ordinary-differential-equations']
55,"Making the ""Two-Timing"" method rigorous (Perturbation theory)","Making the ""Two-Timing"" method rigorous (Perturbation theory)",,"I'm reading about the method of two-timing in section 7.6 of Nonlinear Dynamics and Chaos by Strogatz, and I have some questions about how to make this concept rigorous. In this section the book considers equations of the form $$ \hspace{4.5cm} \ddot{x} + x + \epsilon h(x,\dot{x}) = 0  \hspace{4.5cm} (1) $$ where $0 \leq \epsilon \ll 1$ , $x: \mathbb{R} \to \mathbb{R}$ , and $h: \mathbb{R}^2 \to \mathbb{R}$ is an arbitrary smooth function. The section first covers regular perturbation theory, which I feel okay with. But then author gets to the method of two-timing for ODEs that exhibit multiple time scales, and this is where the explanation starts to feel a little hand-wavey. Here is Strogatz's description of the method: To apply two-timing to (1), let $\tau = t$ denote the fast $O(1)$ time, and let $T = \epsilon t$ denote the slow time. We'll treat these two times as if they were independent variables. In particular, functions of the slow time $T$ will be regarded as constants on the fast time scale $\tau$ . It's hard to justify this idea rigorously, but it works. [Arg! Show me the rigor!]...Next we turn to the mechanics of the method. We expand the solution of (1) as a series $$ \hspace{3cm} x(t,\epsilon) = x_0(\tau, T) + \epsilon x_1(\tau, T) + O(\epsilon^2). \hspace{3cm} (2) $$ The time derivatives in (1) are transformed according to the chain rule: \begin{align*}    \hspace{3cm} \dot{x} = \frac{dx}{dt} = \frac{\partial x}{\partial \tau} + \frac{\partial x}{\partial T}    \frac{\partial T}{\partial t} = \frac{\partial x}{\partial \tau} + \epsilon \frac{\partial x}{\partial T}   \hspace{3cm} (3) \end{align*} A subscript notation for differentiation is more compact; thus we write (3) as $$ \hspace{5.1cm} \dot{x} = \partial_{\tau} x + \epsilon \partial_T x. \hspace{5.1cm} (4) $$ After substituting (2) into (4) and collecting powers of $\epsilon$ , we find $$ \hspace{3.3cm} \dot{x} = \partial_{\tau} x_0 + \epsilon (\partial_T x_0 + \partial_{\tau} x_1) + O(\epsilon^2). \hspace{3.3cm} (5) $$ Similarly, $$ \hspace{3cm} \ddot{x} = \partial_{\tau \tau} x_0 + \epsilon(\partial_{\tau \tau} x_1 + 2 \partial_{T \tau} x_0) + O(\epsilon^2). \hspace{3cm} (6) $$ My main question : How can we make the above ideas rigorous? My attempt is as follows:  Given the ODE (1), we first assume that the solution $x(t;\epsilon)$ can be expressed as $$ x(t;\epsilon) = X(t,\epsilon t)$$ for some function $X: \mathbb{R}^2 \to \mathbb{R}$ . Now let $\tau, T: \mathbb{R} \to \mathbb{R}$ be functions defined by $\tau(t) := t$ and $T(t;\epsilon) := \epsilon t$ for all $t \in \mathbb{R}, \epsilon > 0$ . We then have $X(\tau(t),T(t)) = x(t;\epsilon)$ for all $t \in \mathbb{R}, \epsilon > 0$ . The chain rule computation is then straightforward: \begin{align*}    \dot{X} := \frac{dX}{dt} &= \frac{\partial X}{\partial \tau} \frac{d\tau}{dt} + \frac{\partial X}{\partial T} \frac{dT}{dt} =  \frac{\partial X}{\partial \tau} + \frac{\partial X}{\partial T} \epsilon. \end{align*} The subsequent equations (4)-(6) should now be the same, just with $x$ replaced by $X$ . (Is my interpretation correct so far?) The Taylor expansion part is where I'm a bit perplexed: It seems that (2) is a Taylor expansion of the function $\epsilon \mapsto x(t; \epsilon)$ , or equivalently, $\epsilon \mapsto X(\tau(t), T(t))$ . Now if we had some arbitrary smooth function $f: \mathbb{R}^2 \to \mathbb{R}$ indexed by some parameter $\epsilon$ , then I agree that the map $\epsilon \mapsto f(x,y; \epsilon)$ has some expansion $$ f(x,y; \epsilon) = f_0(x,y) + f_1(x,y) \epsilon + f_2(x,y) \epsilon^2 + \cdots, $$ (provided that it is smooth), where the coefficient functions $f_0, f_1,\ldots$ do not depend on $\epsilon$ . But my problem with (2) is that the ""coefficients"" of the Taylor series, namely $x_0(\tau,T), x_1(\tau,T),\ldots$ are functions of $\epsilon$ , since $T$ depends on $\epsilon$ . How then is this a legitimate Taylor expansion? Also, an additional question: Is there a way to know a prior if the solution $x = x(t;\epsilon)$ to some ODE $\dot{x} = f(x)$ can be expressed in the form $x(t;\epsilon) = X(t, \epsilon t)$ ? Or do we generally make such an assumption based on intuition? In one of the examples that Strogatz uses, the ODE has the analytic solution $x(t) = (1-\epsilon^2)^{-1/2} e^{-\epsilon t} \cos((1-\epsilon^2)^{1/2} t)$ , so clearly $X(t_1, t_2) := (1-\epsilon^2)^{-1/2} e^{-t_2} \cos((1-\epsilon^2)^{1/2} t_1)$ satisfies the requirement. But is there a way to see this a priori (i.e., if we couldn't analytically solve the ODE)? Any insights would be greatly appreciated. This two-timing concept is rather confusing to me, and I think seeing the details spelled out very rigorously would help my understanding.","I'm reading about the method of two-timing in section 7.6 of Nonlinear Dynamics and Chaos by Strogatz, and I have some questions about how to make this concept rigorous. In this section the book considers equations of the form where , , and is an arbitrary smooth function. The section first covers regular perturbation theory, which I feel okay with. But then author gets to the method of two-timing for ODEs that exhibit multiple time scales, and this is where the explanation starts to feel a little hand-wavey. Here is Strogatz's description of the method: To apply two-timing to (1), let denote the fast time, and let denote the slow time. We'll treat these two times as if they were independent variables. In particular, functions of the slow time will be regarded as constants on the fast time scale . It's hard to justify this idea rigorously, but it works. [Arg! Show me the rigor!]...Next we turn to the mechanics of the method. We expand the solution of (1) as a series The time derivatives in (1) are transformed according to the chain rule: A subscript notation for differentiation is more compact; thus we write (3) as After substituting (2) into (4) and collecting powers of , we find Similarly, My main question : How can we make the above ideas rigorous? My attempt is as follows:  Given the ODE (1), we first assume that the solution can be expressed as for some function . Now let be functions defined by and for all . We then have for all . The chain rule computation is then straightforward: The subsequent equations (4)-(6) should now be the same, just with replaced by . (Is my interpretation correct so far?) The Taylor expansion part is where I'm a bit perplexed: It seems that (2) is a Taylor expansion of the function , or equivalently, . Now if we had some arbitrary smooth function indexed by some parameter , then I agree that the map has some expansion (provided that it is smooth), where the coefficient functions do not depend on . But my problem with (2) is that the ""coefficients"" of the Taylor series, namely are functions of , since depends on . How then is this a legitimate Taylor expansion? Also, an additional question: Is there a way to know a prior if the solution to some ODE can be expressed in the form ? Or do we generally make such an assumption based on intuition? In one of the examples that Strogatz uses, the ODE has the analytic solution , so clearly satisfies the requirement. But is there a way to see this a priori (i.e., if we couldn't analytically solve the ODE)? Any insights would be greatly appreciated. This two-timing concept is rather confusing to me, and I think seeing the details spelled out very rigorously would help my understanding."," \hspace{4.5cm} \ddot{x} + x + \epsilon h(x,\dot{x}) = 0  \hspace{4.5cm} (1)  0 \leq \epsilon \ll 1 x: \mathbb{R} \to \mathbb{R} h: \mathbb{R}^2 \to \mathbb{R} \tau = t O(1) T = \epsilon t T \tau  \hspace{3cm} x(t,\epsilon) = x_0(\tau, T) + \epsilon x_1(\tau, T) + O(\epsilon^2). \hspace{3cm} (2)  \begin{align*}
   \hspace{3cm} \dot{x} = \frac{dx}{dt} = \frac{\partial x}{\partial \tau} + \frac{\partial x}{\partial T}
   \frac{\partial T}{\partial t} = \frac{\partial x}{\partial \tau} + \epsilon \frac{\partial x}{\partial T}   \hspace{3cm} (3)
\end{align*}  \hspace{5.1cm} \dot{x} = \partial_{\tau} x + \epsilon \partial_T x. \hspace{5.1cm} (4)  \epsilon  \hspace{3.3cm} \dot{x} = \partial_{\tau} x_0 + \epsilon (\partial_T x_0 + \partial_{\tau} x_1) + O(\epsilon^2). \hspace{3.3cm} (5)   \hspace{3cm} \ddot{x} = \partial_{\tau \tau} x_0 + \epsilon(\partial_{\tau \tau} x_1 + 2 \partial_{T \tau} x_0) + O(\epsilon^2). \hspace{3cm} (6)  x(t;\epsilon)  x(t;\epsilon) = X(t,\epsilon t) X: \mathbb{R}^2 \to \mathbb{R} \tau, T: \mathbb{R} \to \mathbb{R} \tau(t) := t T(t;\epsilon) := \epsilon t t \in \mathbb{R}, \epsilon > 0 X(\tau(t),T(t)) = x(t;\epsilon) t \in \mathbb{R}, \epsilon > 0 \begin{align*}
   \dot{X} := \frac{dX}{dt} &= \frac{\partial X}{\partial \tau} \frac{d\tau}{dt} + \frac{\partial X}{\partial T} \frac{dT}{dt} =  \frac{\partial X}{\partial \tau} + \frac{\partial X}{\partial T} \epsilon.
\end{align*} x X \epsilon \mapsto x(t; \epsilon) \epsilon \mapsto X(\tau(t), T(t)) f: \mathbb{R}^2 \to \mathbb{R} \epsilon \epsilon \mapsto f(x,y; \epsilon)  f(x,y; \epsilon) = f_0(x,y) + f_1(x,y) \epsilon + f_2(x,y) \epsilon^2 + \cdots,  f_0, f_1,\ldots \epsilon x_0(\tau,T), x_1(\tau,T),\ldots \epsilon T \epsilon x = x(t;\epsilon) \dot{x} = f(x) x(t;\epsilon) = X(t, \epsilon t) x(t) = (1-\epsilon^2)^{-1/2} e^{-\epsilon t} \cos((1-\epsilon^2)^{1/2} t) X(t_1, t_2) := (1-\epsilon^2)^{-1/2} e^{-t_2} \cos((1-\epsilon^2)^{1/2} t_1)","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'perturbation-theory']"
56,Proving $S_1-S_2$ is non-positive,Proving  is non-positive,S_1-S_2,"I have system number (1) as: $S_1’=-bS_1I_1+(1-c)aI_1$ $I_1’=bS_1I_1-aI_1$ And on the other hand I have system number (2) as: $S_2’=-bS_2I_2+aI_2$ $I_2’=bS_2I_2-aI_2$ All parameters $a,b,c>0$ . Both systems have the same initial conditions as in $S_1(0)=S_2(0)=s_0>0$ and $I_1(0)=I_2(0)=i_0>0$ . I want to prove that $S_1-S_2$ is non-positive. What I did so far: I found $S_2$ explicitly though not sure it is useful as $S_1$ can’t be found explicitly unfortunately. Using the initial conditions then I have $S_1=  \frac{(1-c)a}{b}+(s_0- \frac{(1-c)a}{b}$$ )e^{-\int_{0}^{t} bI_1(t) \ dt}$ . Similarly, I obtained $S_2=  \frac{a}{b}+(s_0- \frac{a}{b}$$ )e^{-\int_{0}^{t} bI_2(t) \ dt}$ . So from 2 and 3 it is deduced that $(S_1-S_2)(0)=0$ and $$\lim_{t\rightarrow \infty}(S_1-S_2)(t)=-ca/b.$$ By manipulating the equation for $S_1$ then we have the relationship $e^{-\int_{0}^{t} bI_1(t) \ dt}= \frac{|bS_1(t)-(1-c)a|}{|bs_0-(1-c)a|} $ Despite all that I can’t seem to prove that $(S_1-S_2)(t)$ is negative for $t>0$ . I checked this via simulations and indeed it is the case but I’m trying by contradiction and can’t find something that can lead to a contradiction in anything. Help is appreciated!","I have system number (1) as: And on the other hand I have system number (2) as: All parameters . Both systems have the same initial conditions as in and . I want to prove that is non-positive. What I did so far: I found explicitly though not sure it is useful as can’t be found explicitly unfortunately. Using the initial conditions then I have . Similarly, I obtained . So from 2 and 3 it is deduced that and By manipulating the equation for then we have the relationship Despite all that I can’t seem to prove that is negative for . I checked this via simulations and indeed it is the case but I’m trying by contradiction and can’t find something that can lead to a contradiction in anything. Help is appreciated!","S_1’=-bS_1I_1+(1-c)aI_1 I_1’=bS_1I_1-aI_1 S_2’=-bS_2I_2+aI_2 I_2’=bS_2I_2-aI_2 a,b,c>0 S_1(0)=S_2(0)=s_0>0 I_1(0)=I_2(0)=i_0>0 S_1-S_2 S_2 S_1 S_1=  \frac{(1-c)a}{b}+(s_0- \frac{(1-c)a}{b} )e^{-\int_{0}^{t} bI_1(t) \ dt} S_2=  \frac{a}{b}+(s_0- \frac{a}{b} )e^{-\int_{0}^{t} bI_2(t) \ dt} (S_1-S_2)(0)=0 \lim_{t\rightarrow \infty}(S_1-S_2)(t)=-ca/b. S_1 e^{-\int_{0}^{t} bI_1(t) \ dt}= \frac{|bS_1(t)-(1-c)a|}{|bs_0-(1-c)a|}  (S_1-S_2)(t) t>0","['real-analysis', 'ordinary-differential-equations']"
57,Critically damped system,Critically damped system,,"Coming from engineering background, my math might not be as rigour here. Please enlighten me. By definition, critically damped 2nd order system ( $\zeta=1$ ) does not have overshoot. Is the statement above (my assumption) false? For context, this is the system: $$ Q\ddot{x}+2\zeta \omega_n\dot{x}+\omega_n^2x=\omega_n^2r $$ which is also in the form: $$ Q\ddot{x}+a\dot{x}+bx=br $$ where $r$ is some constant. I know that the response is in the form: $$ x(t)=r\left[1−(k_1+k_2t)e^{\frac{−\omega_n}{Q}t}\right] $$ Where $k_1$ , $k_2$ is some constant depend on initial condition. For $x(0) = 0$ , We know that $k_1=0$ , but how to determine $k_2$ (which depends on $x'(0)$ )? I know that this might be very trivial for some of you, but I'm really confused. For context, I asked a similar question in the electrical engineering site . If you are interested, the problem is the 2nd order system that I set ""gains"" for (gains correspond to changing the coefficient in the ode (a & b)), behave critically damped. However, when I set up a the original first order system with the input of the controlled gains, it had an overshoot. If you know why kindly please enlighten me as well. This is the unstable first order system trying to be controlled: $$ p\dot{x}-lx=r $$ Letting $$ r= K_p\left(c-x\right) + K_i\int _0^tc-x\ dt\ + K_d\left(\dot{c} - \dot{x}\right) $$ This is a PID control for 1st order system. Making the sytem to become 2nd order. and track the value $c$","Coming from engineering background, my math might not be as rigour here. Please enlighten me. By definition, critically damped 2nd order system ( ) does not have overshoot. Is the statement above (my assumption) false? For context, this is the system: which is also in the form: where is some constant. I know that the response is in the form: Where , is some constant depend on initial condition. For , We know that , but how to determine (which depends on )? I know that this might be very trivial for some of you, but I'm really confused. For context, I asked a similar question in the electrical engineering site . If you are interested, the problem is the 2nd order system that I set ""gains"" for (gains correspond to changing the coefficient in the ode (a & b)), behave critically damped. However, when I set up a the original first order system with the input of the controlled gains, it had an overshoot. If you know why kindly please enlighten me as well. This is the unstable first order system trying to be controlled: Letting This is a PID control for 1st order system. Making the sytem to become 2nd order. and track the value","\zeta=1 
Q\ddot{x}+2\zeta \omega_n\dot{x}+\omega_n^2x=\omega_n^2r
 
Q\ddot{x}+a\dot{x}+bx=br
 r 
x(t)=r\left[1−(k_1+k_2t)e^{\frac{−\omega_n}{Q}t}\right]
 k_1 k_2 x(0) = 0 k_1=0 k_2 x'(0) 
p\dot{x}-lx=r
 
r= K_p\left(c-x\right) + K_i\int _0^tc-x\ dt\ + K_d\left(\dot{c} - \dot{x}\right)
 c","['ordinary-differential-equations', 'control-theory']"
58,From discrete dynamical system to continuous one,From discrete dynamical system to continuous one,,"To me it seems that is not always the case that a discrete dynamical system of the form $$ x_{k+1} = f(x_k) $$ can be seen as the discretization of a continuous one like $$ \dot{x}(t) = f(x), $$ while we can always go in the opposite direction. My question is: is there some keyword I can search in the literature to understand better this connection or do you have any reference to suggest? I would like to understand if there are some sufficient conditions guaranteeing the possibility of doing this transition, I don't even need a constructive way to build this continuous version, ""just"" existence would already be a great thing to understand. To be clear, I would appreciate even results on this sort of ""continuous prolongation"" based on increasing the dimension of the phase space, i.e. for example for a discrete system of $\mathbb{R}^2$ if there is a dimension $d\geq 2$ such that on it there is a well defined continuous system whose discretization at certain time instants correspond to the discrete one. This question should be easier since there is not the constraint in the dimension. I know it is quite a broad question, but I need at least some terminology to look for. Up to now I have read about embeddability of homeomorphisms into flows and this seems to answer to my question in some cases, so I just wanted to know if the embeddability into flows is the right thing to study or if there are other perspectives. P.S. About getting an approximation of them I am finding very interesting this question Links between difference and differential equations? at the moment.","To me it seems that is not always the case that a discrete dynamical system of the form can be seen as the discretization of a continuous one like while we can always go in the opposite direction. My question is: is there some keyword I can search in the literature to understand better this connection or do you have any reference to suggest? I would like to understand if there are some sufficient conditions guaranteeing the possibility of doing this transition, I don't even need a constructive way to build this continuous version, ""just"" existence would already be a great thing to understand. To be clear, I would appreciate even results on this sort of ""continuous prolongation"" based on increasing the dimension of the phase space, i.e. for example for a discrete system of if there is a dimension such that on it there is a well defined continuous system whose discretization at certain time instants correspond to the discrete one. This question should be easier since there is not the constraint in the dimension. I know it is quite a broad question, but I need at least some terminology to look for. Up to now I have read about embeddability of homeomorphisms into flows and this seems to answer to my question in some cases, so I just wanted to know if the embeddability into flows is the right thing to study or if there are other perspectives. P.S. About getting an approximation of them I am finding very interesting this question Links between difference and differential equations? at the moment."," x_{k+1} = f(x_k)   \dot{x}(t) = f(x),  \mathbb{R}^2 d\geq 2","['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'vector-fields']"
59,How to solve this ODE with the Laplace transform?,How to solve this ODE with the Laplace transform?,,"I want to solve this ODE $$ y'''-y''-y'+y= -10 \cos (2t-1)+5 \sin(2t-1) $$ with $y( \frac12)= 1 $ , $ y'( \frac12 )=2 $ , $y''( \frac12 )=1 $ $ t \in [ \frac12 , + \infty [ $ using the Laplace-Transformation. usually I use the differential-approach: $$ ( L( f^{(k)}))(s)=s^k(Lf)(s)- \sum_{j=0}^{k-1} s^j f^{(k-1-j)} (0) $$ so setting $Y(s)= L \{ y(t) \} $ I get $s^3 Y(s)-s^0y''(0)-s^1y'(0)-s^2y(0)-s^2Y(s)+s^0 y'(0)+s^1y(0)-sy(s)+s^0y(0)+Y(s)... $ I can't continue here, because of the initital values. How can I solve this ODE? Do I need to shift it somehow? Or do $ y''(0) etc..$ vanish, because $ t \in [ \frac12, \infty[ $ ? Appreciate any help ! EDIT Thank you for the help so far ! after applying the shift theorem I solve $$z'''-z''-z'+z=-10 \cos (2t) +5 \sin(2t) $$ with $ z(0)=1,z'(0)=2,z''(0)=1 $ Using the differential approach it comes to $$s^3 Z(s)-1-2s-s^2-s^2Z(s)+2+s-sZ(s)+1+Z(s)= \frac{-10s+10}{(s^2+4} $$ $$ \Leftrightarrow Z(s)(s^3-s^2-s+1)= \frac{-10s+10}{(s^2+4)} -2+s^2+s $$ $$ \Leftrightarrow Z(s)= \frac{(-10s+10)-(s^2+4)(2+s^2+s)}{(s^3-s^2-s+1)(s^2+4)} $$ $$ \Leftrightarrow Z(s)= - \frac32 \frac{1}{s-1}- 2 \frac{1}{(s-1)^2} + \frac12 \frac{1}{s+1} + \frac{2}{s^2+4} $$ looking up the inverse it comes to $z(t)= - \frac32 e^t - 2t e^t+ \frac12 e^{-t}+ \sin(2t) $ but $ z'(t)= \cos(t)-2te^t- \frac{7 e^t}{2}- \frac{e^{-t}}{2} $ and so $ z'(0)=-2 \neq 2 $ where is my Mistake?","I want to solve this ODE with , , using the Laplace-Transformation. usually I use the differential-approach: so setting I get I can't continue here, because of the initital values. How can I solve this ODE? Do I need to shift it somehow? Or do vanish, because ? Appreciate any help ! EDIT Thank you for the help so far ! after applying the shift theorem I solve with Using the differential approach it comes to looking up the inverse it comes to but and so where is my Mistake?"," y'''-y''-y'+y= -10 \cos (2t-1)+5 \sin(2t-1)  y( \frac12)= 1   y'( \frac12 )=2  y''( \frac12 )=1   t \in [ \frac12 , + \infty [   ( L( f^{(k)}))(s)=s^k(Lf)(s)- \sum_{j=0}^{k-1} s^j f^{(k-1-j)} (0)  Y(s)= L \{ y(t) \}  s^3 Y(s)-s^0y''(0)-s^1y'(0)-s^2y(0)-s^2Y(s)+s^0 y'(0)+s^1y(0)-sy(s)+s^0y(0)+Y(s)...   y''(0) etc..  t \in [ \frac12, \infty[  z'''-z''-z'+z=-10 \cos (2t) +5 \sin(2t)   z(0)=1,z'(0)=2,z''(0)=1  s^3 Z(s)-1-2s-s^2-s^2Z(s)+2+s-sZ(s)+1+Z(s)= \frac{-10s+10}{(s^2+4}   \Leftrightarrow Z(s)(s^3-s^2-s+1)= \frac{-10s+10}{(s^2+4)} -2+s^2+s   \Leftrightarrow Z(s)= \frac{(-10s+10)-(s^2+4)(2+s^2+s)}{(s^3-s^2-s+1)(s^2+4)}   \Leftrightarrow Z(s)= - \frac32 \frac{1}{s-1}- 2 \frac{1}{(s-1)^2} + \frac12 \frac{1}{s+1} + \frac{2}{s^2+4}  z(t)= - \frac32 e^t - 2t e^t+ \frac12 e^{-t}+ \sin(2t)   z'(t)= \cos(t)-2te^t- \frac{7 e^t}{2}- \frac{e^{-t}}{2}   z'(0)=-2 \neq 2 ","['ordinary-differential-equations', 'derivatives', 'laplace-transform']"
60,Dynamic Systems: Adjustment time to perturbations,Dynamic Systems: Adjustment time to perturbations,,"I am new to the field of dynamic systems and have what I feel is a pretty basic question. If I have a simple dynamic system $\dot{x}=-kx$ with one stable equilbrium point, and I move my solution away from the equilibrium point by a value $\Delta x$ , how long will it take for my solution to converge back to the equilibrium point? Let's call this adjustmnet time ( $\tau$ ). What happens if this is a nonlinear dynamic system, for instance $\dot{x}=-kx^3$ ? Then how long does it take? Can someone: Point me to another questions if it has already been asked Point me to a reference where I can figure this out on my own Walk me through a solution","I am new to the field of dynamic systems and have what I feel is a pretty basic question. If I have a simple dynamic system with one stable equilbrium point, and I move my solution away from the equilibrium point by a value , how long will it take for my solution to converge back to the equilibrium point? Let's call this adjustmnet time ( ). What happens if this is a nonlinear dynamic system, for instance ? Then how long does it take? Can someone: Point me to another questions if it has already been asked Point me to a reference where I can figure this out on my own Walk me through a solution",\dot{x}=-kx \Delta x \tau \dot{x}=-kx^3,"['ordinary-differential-equations', 'dynamical-systems', 'perturbation-theory']"
61,A weird differential equation with some unknown trick?,A weird differential equation with some unknown trick?,,"Here is the differential equation I need to solve: $$y=xy'+k\cdot \frac{y'}{y'-1}$$ (Here $k$ is a given constant). What I have thought initially is to rewrite $\frac{y'}{y'-1}$ as $\left(1+\frac{1}{y'-1}\right)$ . But I don't think it helps that much. Hope someone could give me a hint. Also, in the question, we are asked to consider $k>0$ and $k<0$ separately. There is definitely a reason why behind this, but I have not figured that out. Thanks :))","Here is the differential equation I need to solve: (Here is a given constant). What I have thought initially is to rewrite as . But I don't think it helps that much. Hope someone could give me a hint. Also, in the question, we are asked to consider and separately. There is definitely a reason why behind this, but I have not figured that out. Thanks :))",y=xy'+k\cdot \frac{y'}{y'-1} k \frac{y'}{y'-1} \left(1+\frac{1}{y'-1}\right) k>0 k<0,['ordinary-differential-equations']
62,Solve a nonlinear differential equation of the first order,Solve a nonlinear differential equation of the first order,,"I am trying to solve a nonlinear differential equation of the first order that comes from a geometric problem ; $$x(2x-1)y'^2-(2x-1)(2y-1)y'+y(2y-1)=0.$$ edit1 I am looking for human methods to solve the equation edit2  the geometric problem was discussed on this french forum http://www.les-mathematiques.net/phorum/read.php?8,1779080,1779080 We can see the differential equation here http://www.les-mathematiques.net/phorum/read.php?8,1779080,1780782#msg-1780782 edit 3 I do not trust formal computer programs: look at Wolfram's answer when asked to calculate the cubic root of -1 https://www.wolframalpha.com/input/?i=%7B%5Csqrt%5B3%5D%7B-1%7D%7D%29+ .","I am trying to solve a nonlinear differential equation of the first order that comes from a geometric problem ; edit1 I am looking for human methods to solve the equation edit2  the geometric problem was discussed on this french forum http://www.les-mathematiques.net/phorum/read.php?8,1779080,1779080 We can see the differential equation here http://www.les-mathematiques.net/phorum/read.php?8,1779080,1780782#msg-1780782 edit 3 I do not trust formal computer programs: look at Wolfram's answer when asked to calculate the cubic root of -1 https://www.wolframalpha.com/input/?i=%7B%5Csqrt%5B3%5D%7B-1%7D%7D%29+ .",x(2x-1)y'^2-(2x-1)(2y-1)y'+y(2y-1)=0.,"['calculus', 'ordinary-differential-equations']"
63,Problem regarding the Wronskian formula,Problem regarding the Wronskian formula,,"I'm studying from G.Teschl ODE & Dynamical system and I got stucked at problem 3.20, which is: Find a formula for the Wronskian $W(x,y)=\dot xy-\dot yx$ of the autonomous system $\ddot x + c_{1}\dot x + c_{0}x=0$ . I know I can find the characteristic polynomial $\lambda^{2}+c_{1}\lambda+c_{0}=0$ where the can be calculated and that way try to find explicits solutions. But I don't think that's what is wanted where. Thanks so much for your answers!.","I'm studying from G.Teschl ODE & Dynamical system and I got stucked at problem 3.20, which is: Find a formula for the Wronskian of the autonomous system . I know I can find the characteristic polynomial where the can be calculated and that way try to find explicits solutions. But I don't think that's what is wanted where. Thanks so much for your answers!.","W(x,y)=\dot xy-\dot yx \ddot x + c_{1}\dot x + c_{0}x=0 \lambda^{2}+c_{1}\lambda+c_{0}=0","['calculus', 'ordinary-differential-equations']"
64,Schrödinger equation involving the Dirac-Delta,Schrödinger equation involving the Dirac-Delta,,"I am taking a course on quantum mechanics and I try to understand the time-independent Schrödinger-equation with the Delta-potential: $$\frac{-\hslash^2}{2m}\psi''(x)-V_0\delta(x)\psi(x)=E\psi(x)$$ where $V_0,m,\hslash>0$ and $\psi$ and $E$ are unknown. I can somehow mimic the procedure for solving the equation but I do not understand what I am doing there. The main problem is that I do not understand what $\delta(x)$ exactly means. I know that $\delta$ is a distribution which can for example be defined as being a functional or as being a measure. However, I don't understand why $\delta$ can take $x$ as an argument. I guess it is some abuse of notation, right? So, here are my questions: What does $\delta(x)$ in the above equation mean? Or more general: What does $\delta(x)$ mean if it occurs in an ODE? How is a solution for such an ODE defined? If $\delta(x)$ is abuse of notation, what would be the correct notation for such an equation? Of course, I know the ""physicist's explanation"" of $\delta(x)$ (i.e.: it can be used to describe a potential $V(x)=0$ for $x \neq 0$ and $V(0)=-\infty$ ). Unfortunately, this does not help me. I am looking for a mathematical precise explanation. A reference to a textbook would also be great (a mathematics book - not a physics book). I've asked a related question on physics stackexchange. You can find the link in the comments.","I am taking a course on quantum mechanics and I try to understand the time-independent Schrödinger-equation with the Delta-potential: where and and are unknown. I can somehow mimic the procedure for solving the equation but I do not understand what I am doing there. The main problem is that I do not understand what exactly means. I know that is a distribution which can for example be defined as being a functional or as being a measure. However, I don't understand why can take as an argument. I guess it is some abuse of notation, right? So, here are my questions: What does in the above equation mean? Or more general: What does mean if it occurs in an ODE? How is a solution for such an ODE defined? If is abuse of notation, what would be the correct notation for such an equation? Of course, I know the ""physicist's explanation"" of (i.e.: it can be used to describe a potential for and ). Unfortunately, this does not help me. I am looking for a mathematical precise explanation. A reference to a textbook would also be great (a mathematics book - not a physics book). I've asked a related question on physics stackexchange. You can find the link in the comments.","\frac{-\hslash^2}{2m}\psi''(x)-V_0\delta(x)\psi(x)=E\psi(x) V_0,m,\hslash>0 \psi E \delta(x) \delta \delta x \delta(x) \delta(x) \delta(x) \delta(x) V(x)=0 x \neq 0 V(0)=-\infty","['ordinary-differential-equations', 'reference-request', 'definition', 'dirac-delta', 'quantum-mechanics']"
65,What's wrong with this method of solving linear ODE systems with time varying coefficients?,What's wrong with this method of solving linear ODE systems with time varying coefficients?,,"The problem is stated: ""What is wrong with the following calculation for an arbitrary continuous matrix $A(t)$ ? $$\frac{d}{dt}\big[\exp \int_{t_0}^t A(s) ds\big] = A(t) \exp \big[ \int_{t_0}^t A(s) ds \big] $$ so that $\exp (\int_{t_0}^t A(s) ds)$ is a fundamental matrix of $\mathbf{y}' = A(t)\mathbf{y}$ for an arbitrary continuous matrix $A(t)$ ."" This seems like a straightforward application of the chain rule and the fundamental theorem of calculus, so I'm not sure what the question-writer is looking for.","The problem is stated: ""What is wrong with the following calculation for an arbitrary continuous matrix ? so that is a fundamental matrix of for an arbitrary continuous matrix ."" This seems like a straightforward application of the chain rule and the fundamental theorem of calculus, so I'm not sure what the question-writer is looking for.",A(t) \frac{d}{dt}\big[\exp \int_{t_0}^t A(s) ds\big] = A(t) \exp \big[ \int_{t_0}^t A(s) ds \big]  \exp (\int_{t_0}^t A(s) ds) \mathbf{y}' = A(t)\mathbf{y} A(t),"['calculus', 'ordinary-differential-equations']"
66,Solve a PDE defining a limit cycle of a nonlinear DE,Solve a PDE defining a limit cycle of a nonlinear DE,,"When trying to identify a limit cycle of a 2nd order nonlinear DE $$\begin{cases}\dot{y}_1=y_2\\\dot{y}_2=-ky_2-\frac{b(a)sin(y_1)}{1+c_1(a)cos(y_1)+c_2(a)sin(y_1)}\end{cases},$$ where $b(a)$ , $c_1(a)$ , and $c_2(a)$ are some coefficients depending on the parameter $a$ , I arrived at the following 1st order PDE: $$\frac{\partial V}{\partial y_1}y_2-\frac{\partial V}{\partial y_2}ky_2-\frac{\partial V}{\partial y_2}f(y_1)=0.$$ I wish to find a solution to this equation in the form $V(y_1,y_2)=0$ such that the following conditions are satisfied: $V(y_1+2\pi,y_2)=V(y_1,y_2)$ ; $\frac{dy_2}{dy_1}\bigg|_{y_1=0}=0$ and $\frac{d^2y_2}{dy_1^2}\bigg|_{y_1=0}>0$ ; $\frac{dy_2}{dy_1}\bigg|_{y_1=\pi}=0$ and $\frac{d^2y_2}{dy_1^2}\bigg|_{y_1=\pi}<0$ . Item 1 comes from the fact that the DE is defined on a cylinder, while Items 2 and 3 say that the plot $y_2=y_2(y_1)$ of $V(y_1,y_2)=0$ has a minimum at $y_1=0$ and a maximum at $y_1=\pi$ . The latter conditions were obtained from numerical simulations. Furthermore, we know that $f(y_1+2\pi)=f(y_1)$ and $f(0)=0$ . Question: I wonder whether this problem is well defined and if ""yes"", how to solve it? Indeed, I want to find the limit set $V(y_1,y_2)=0$ since the problem -- as it is stated -- can have only one solution that corresponds to the limit cycle. My attempts. Let $y_1=0$ . If $\frac{\partial V}{\partial y_2}\bigg|_{y_1=0}\neq 0$ , Item 2 along with $f(0)=0$ implies that $y_2=0$ which contradicts the fact that the point $(0,0)$ is an isolated equilibrium. So, I conclude that $$\frac{\partial V}{\partial y_2}\bigg|_{y_1=0}= 0 \mbox{ and } \frac{\partial V}{\partial y_1}\bigg|_{y_1=0}= 0.$$ Following the same logic I conclude that the same holds for $y_1=\pi$ : $$\frac{\partial V}{\partial y_2}\bigg|_{y_1=\pi}= 0 \mbox{ and } \frac{\partial V}{\partial y_1}\bigg|_{y_1=\pi}= 0.$$ So, basically, that's it. I tried several candidates for $V(y_1,y_2)$ , but couldn't get any meaningful result. This plot makes me to believe that there is a limit cycle. The red trajectory (approximately) corresponds to the unstable solution of the sadle, two others are just computed for different initial values of $y_2$ .","When trying to identify a limit cycle of a 2nd order nonlinear DE where , , and are some coefficients depending on the parameter , I arrived at the following 1st order PDE: I wish to find a solution to this equation in the form such that the following conditions are satisfied: ; and ; and . Item 1 comes from the fact that the DE is defined on a cylinder, while Items 2 and 3 say that the plot of has a minimum at and a maximum at . The latter conditions were obtained from numerical simulations. Furthermore, we know that and . Question: I wonder whether this problem is well defined and if ""yes"", how to solve it? Indeed, I want to find the limit set since the problem -- as it is stated -- can have only one solution that corresponds to the limit cycle. My attempts. Let . If , Item 2 along with implies that which contradicts the fact that the point is an isolated equilibrium. So, I conclude that Following the same logic I conclude that the same holds for : So, basically, that's it. I tried several candidates for , but couldn't get any meaningful result. This plot makes me to believe that there is a limit cycle. The red trajectory (approximately) corresponds to the unstable solution of the sadle, two others are just computed for different initial values of .","\begin{cases}\dot{y}_1=y_2\\\dot{y}_2=-ky_2-\frac{b(a)sin(y_1)}{1+c_1(a)cos(y_1)+c_2(a)sin(y_1)}\end{cases}, b(a) c_1(a) c_2(a) a \frac{\partial V}{\partial y_1}y_2-\frac{\partial V}{\partial y_2}ky_2-\frac{\partial V}{\partial y_2}f(y_1)=0. V(y_1,y_2)=0 V(y_1+2\pi,y_2)=V(y_1,y_2) \frac{dy_2}{dy_1}\bigg|_{y_1=0}=0 \frac{d^2y_2}{dy_1^2}\bigg|_{y_1=0}>0 \frac{dy_2}{dy_1}\bigg|_{y_1=\pi}=0 \frac{d^2y_2}{dy_1^2}\bigg|_{y_1=\pi}<0 y_2=y_2(y_1) V(y_1,y_2)=0 y_1=0 y_1=\pi f(y_1+2\pi)=f(y_1) f(0)=0 V(y_1,y_2)=0 y_1=0 \frac{\partial V}{\partial y_2}\bigg|_{y_1=0}\neq 0 f(0)=0 y_2=0 (0,0) \frac{\partial V}{\partial y_2}\bigg|_{y_1=0}= 0 \mbox{ and } \frac{\partial V}{\partial y_1}\bigg|_{y_1=0}= 0. y_1=\pi \frac{\partial V}{\partial y_2}\bigg|_{y_1=\pi}= 0 \mbox{ and } \frac{\partial V}{\partial y_1}\bigg|_{y_1=\pi}= 0. V(y_1,y_2) y_2","['ordinary-differential-equations', 'partial-differential-equations', 'dynamical-systems']"
67,Why are differential equations satisfied/defined only on open domains?,Why are differential equations satisfied/defined only on open domains?,,"I am going through FEM lectures by University of Michigan, and in one of the lectures, the professor writes an ODE: $$\frac{d^2u}{dx^2}+f(x)= 0, \ \ \ x \in (0,L)$$ Why are these differential equations only satisfied in open intervals, not in closed domains, when boundary conditions are specified? For example, one of the set of boundary conditions could be $$u(0) = 0 \ \ and \ \ \frac{du}{dx}(x=L) = 0$$","I am going through FEM lectures by University of Michigan, and in one of the lectures, the professor writes an ODE: Why are these differential equations only satisfied in open intervals, not in closed domains, when boundary conditions are specified? For example, one of the set of boundary conditions could be","\frac{d^2u}{dx^2}+f(x)= 0, \ \ \ x \in (0,L) u(0) = 0 \ \ and \ \ \frac{du}{dx}(x=L) = 0","['ordinary-differential-equations', 'finite-element-method']"
68,Uniqueness of solutions of $y''+y=0$,Uniqueness of solutions of,y''+y=0,Recently I was asked about the uniqueness of the solutions of the equation $y'=y$. It could be obtained by multiply $e^{-x}$. This time I was wondering about the same question regarding a different equation. We know that the solutions of $y''+y=0$ are of the form $a\sin(x)+b\cos(x) $. How can we explain that these are the ONLY solutions?,Recently I was asked about the uniqueness of the solutions of the equation $y'=y$. It could be obtained by multiply $e^{-x}$. This time I was wondering about the same question regarding a different equation. We know that the solutions of $y''+y=0$ are of the form $a\sin(x)+b\cos(x) $. How can we explain that these are the ONLY solutions?,,"['calculus', 'ordinary-differential-equations']"
69,Solving this equation reducing it to Bessel Equation,Solving this equation reducing it to Bessel Equation,,Question : Solve the differential equation  $ 4 \frac{d^2y}{dx^2}+9xy=0 $ by reducing it to Bessel equation. What Have I tried so far? I know that a standard Bessel equation is $$x^2 \frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x^2-n^2)y=0$$ I tried using   $ t^2-n^2=9x$   to proceed with the problem but am reaching nowhere. Kindly tell me how to approach this problem?,Question : Solve the differential equation  $ 4 \frac{d^2y}{dx^2}+9xy=0 $ by reducing it to Bessel equation. What Have I tried so far? I know that a standard Bessel equation is $$x^2 \frac{d^2y}{dx^2}+x\frac{dy}{dx}+(x^2-n^2)y=0$$ I tried using   $ t^2-n^2=9x$   to proceed with the problem but am reaching nowhere. Kindly tell me how to approach this problem?,,"['ordinary-differential-equations', 'bessel-functions']"
70,Cauchy differential equation,Cauchy differential equation,,"I'm trying to resolve this cauchy problem: $ y'=2y+1$ such as $y(0)=1$ the general integral for the differential equation is $\frac{1}{2}(e^{2x+2c_1}-1)$ for $y(0)=1$ : $y(0)=\frac{1}{2}(e^{2c_1}-1)=1$ my doubt is about the fact that i don't know how to ""get down"" that $c_1$ and i can't resolve the problem, can you help me with this one? Thank you for your time.","I'm trying to resolve this cauchy problem: $ y'=2y+1$ such as $y(0)=1$ the general integral for the differential equation is $\frac{1}{2}(e^{2x+2c_1}-1)$ for $y(0)=1$ : $y(0)=\frac{1}{2}(e^{2c_1}-1)=1$ my doubt is about the fact that i don't know how to ""get down"" that $c_1$ and i can't resolve the problem, can you help me with this one? Thank you for your time.",,"['ordinary-differential-equations', 'cauchy-problem']"
71,"Existence and uniqueness of solution of : $y'(x) = (x-y(x))^{4/5}, \space y(3)=3$",Existence and uniqueness of solution of :,"y'(x) = (x-y(x))^{4/5}, \space y(3)=3","Study the Existence and Uniqueness of the IVP :   $$y'(x) = (x-y(x))^{4/5}, \space y(3)=3$$ Regarding existence, I'd first assume $f$ the function such : $$f(x,y)=(x-y)^{4/5}$$ We observe that $f(x,y)$ is not continuous in $ \mathbb R^2$ since we need $x-y\geq0$. Letting $D$ be a domain such : $$D=\{(x,y)\in \mathbb R^2: |x-3| \leq ε_1, \space |y-3|\leq ε_2 \}$$ we can say that there exists $ε_1,ε_2 \in \mathbb R$ such that $f$ is continuous in $D$, which means that the IVP has a solution in $D$. Question : Is the above statement correct ? Shall I go into more details explaining why $f$ can be continuous in such a domain ? Finally, regarding uniqueness, I know that showing that $f(x,y)=(x-y)^{4/5}$ is a Lipschitz function is enough. : $$|f(x,y_2) -f(x,y_1)|=|(x-y_2)^{4/5}-(x-y_1)^{4/5}|$$ The function $g(y)=(x-y)^{4/5}$ is continuous in the interval $[y_1,y_2]$ with $y_1,y_2 \in D_f$ and $g$ is also differentiable in $(y_1,y_2)$ , which means that we can apply the Mean Value Theorem and yield : $$g'(ξ)=\frac{g(y_2)-g(y_1)}{y_2-y_1}\Rightarrow \bigg|\frac{4}{5(x-ξ)^{1/5}}\bigg||y_2-y_1|=|(x-y_2)^{4/5}-(x-y_1)^{4/5}|$$ Here, we can see that there isn't a bound for the expression $\bigg|\frac{4}{5(x-ξ)^{1/5}}\bigg|$, thus meaning that the solution is not unique .","Study the Existence and Uniqueness of the IVP :   $$y'(x) = (x-y(x))^{4/5}, \space y(3)=3$$ Regarding existence, I'd first assume $f$ the function such : $$f(x,y)=(x-y)^{4/5}$$ We observe that $f(x,y)$ is not continuous in $ \mathbb R^2$ since we need $x-y\geq0$. Letting $D$ be a domain such : $$D=\{(x,y)\in \mathbb R^2: |x-3| \leq ε_1, \space |y-3|\leq ε_2 \}$$ we can say that there exists $ε_1,ε_2 \in \mathbb R$ such that $f$ is continuous in $D$, which means that the IVP has a solution in $D$. Question : Is the above statement correct ? Shall I go into more details explaining why $f$ can be continuous in such a domain ? Finally, regarding uniqueness, I know that showing that $f(x,y)=(x-y)^{4/5}$ is a Lipschitz function is enough. : $$|f(x,y_2) -f(x,y_1)|=|(x-y_2)^{4/5}-(x-y_1)^{4/5}|$$ The function $g(y)=(x-y)^{4/5}$ is continuous in the interval $[y_1,y_2]$ with $y_1,y_2 \in D_f$ and $g$ is also differentiable in $(y_1,y_2)$ , which means that we can apply the Mean Value Theorem and yield : $$g'(ξ)=\frac{g(y_2)-g(y_1)}{y_2-y_1}\Rightarrow \bigg|\frac{4}{5(x-ξ)^{1/5}}\bigg||y_2-y_1|=|(x-y_2)^{4/5}-(x-y_1)^{4/5}|$$ Here, we can see that there isn't a bound for the expression $\bigg|\frac{4}{5(x-ξ)^{1/5}}\bigg|$, thus meaning that the solution is not unique .",,"['ordinary-differential-equations', 'dynamical-systems', 'lipschitz-functions', 'initial-value-problems']"
72,Stability analysis using the Jacobian,Stability analysis using the Jacobian,,"I must find the steady states, the Jacobian, and the stability of each point. $x' =x^2 - y^2$ and $y' = x(1-y)$ Simply solving for when these equations equal zero, I found that $y=0,1$, after getting $y-yy=0$, as must x, as $x^2=y^2$. Thus I believe the steady states to be $(0,0)$ and $(1,1)$ but I don't understand the Jacobian matrix. I think it is: $   \left[ {\begin{array}{cc}    2x-y^2 & x^2-2y \\    1-y & x-1 \\   \end{array} } \right] $ So for the state (0,0): $   \left[ {\begin{array}{cc}    0 & 0 \\    1 & 1 \\   \end{array} } \right] $ So, the trace of the matrix is $1$ and the determinant is $0$. Is this neutral center? For (1,1): $   \left[ {\begin{array}{cc}    0 & 0 \\    0 & 0\\   \end{array} } \right] $ So the trace and the determinant are both $0$ (how do we characterize this) Any help would be appreciated, as would work checking, am new to differential equations.","I must find the steady states, the Jacobian, and the stability of each point. $x' =x^2 - y^2$ and $y' = x(1-y)$ Simply solving for when these equations equal zero, I found that $y=0,1$, after getting $y-yy=0$, as must x, as $x^2=y^2$. Thus I believe the steady states to be $(0,0)$ and $(1,1)$ but I don't understand the Jacobian matrix. I think it is: $   \left[ {\begin{array}{cc}    2x-y^2 & x^2-2y \\    1-y & x-1 \\   \end{array} } \right] $ So for the state (0,0): $   \left[ {\begin{array}{cc}    0 & 0 \\    1 & 1 \\   \end{array} } \right] $ So, the trace of the matrix is $1$ and the determinant is $0$. Is this neutral center? For (1,1): $   \left[ {\begin{array}{cc}    0 & 0 \\    0 & 0\\   \end{array} } \right] $ So the trace and the determinant are both $0$ (how do we characterize this) Any help would be appreciated, as would work checking, am new to differential equations.",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
73,Flow of constant vector field on $\mathbb{S}^n$ (Jost: Exercise 1.16),Flow of constant vector field on  (Jost: Exercise 1.16),\mathbb{S}^n,"This is a problem that comes from Jost's Riemannian Geometry and Geometric Analysis (Chapter 1, Exercise 16). We consider the constant vector field $X(x)=a$ for all $x\in\mathbb{R}^{n+1}$. We obtain a vector field $\tilde{X}(x)$ on $\mathbb{S}^n$ by projecting $X(x)$ onto $T_x\mathbb{S}^n$ for $x\in\mathbb{S}^n$. Determine the corresponding flow on $\mathbb{S}^n$. I've been stuck on this for a while, and haven't been sure exactly what to try. This is the partial work that I have so far: Let $\mathbf{1}_{n+1}$ be the $(n+1)\times(n+1)$ identity matrix. Note that the projection onto the tangent space $T_x\mathbb{S}^n$ is simply given by $$P_x:=\mathbf{1}_{n+1}-xx^\top:\mathbb{R}^{n+1}\to T_x\mathbb{S}^n.$$ Now we see that  $$\tilde{X}(x)=P_x(X(x))=P_x(a)=(\mathbf{1}_{n+1} - xx^\top)a = a - xx^\top a = a - \langle{a,x}\rangle x\in T_x\mathbb{S}^n.$$ So for any $p\in\mathbb{S}^n$ we are looking for a solution to the following ODE $$\begin{cases}\dot\gamma(t)=\tilde{X}(\gamma(t))=a-\langle{a,\gamma(t)}\rangle\gamma(t),\\\gamma(0)=p.\end{cases}$$ We have two fixed point solutions given by $\gamma_{\pm}(t)=\pm\frac{a}{\|a\|}.$ I'm pretty sure (although I haven't shown this yet) that if we start from any point that isn't a fixed point that the flow will tend to $\frac{a}{\|a\|}$. At first I thought that the flow might be related to the projection of the flow of $X$ in $\mathbb{R}^{n+1}$, i.e. take $\Gamma:\mathbb{R}\to\mathbb{R}^{n+1}$ given by $\Gamma(t)=p + ta$, and then see if $\gamma(t):=\frac{\Gamma(t)}{\|\Gamma(t)\|}$ solves the ODE. Unfortunately, it doesn't solve it on $\mathbb{S}^1\subseteq\mathbb{R}^2$ (although a reparameterization would probably do the trick in this case since $\mathbb{S}^1$ is $1$-dimensional and the direction of the flow seems to be correct), and in any higher dimensions it doesn't even flow in the right direction. So I'm pretty sure this idea is incorrect, and I wasn't able to get it to work at all. The second idea I had was to consider the geodesic starting at $p$ and in the direction $\dot\gamma(0)=a-\langle{a,p}\rangle p\in T_p\mathbb{S}^n$, and maybe reparameterize it if need be. Unfortunately, this also didn't work and I wasn't able to figure out whether the reparameterization idea would work here. Other than that, I've been pretty stuck and I'm not sure if I'm supposed to be able to just be able to solve this ODE directly or if there is some geometric insight that I can use to determine this flow. Any hints and explanations would be extremely appreciated! I've been working on this for a long time, and this is the only question in this chapter I haven't been able to figure out.","This is a problem that comes from Jost's Riemannian Geometry and Geometric Analysis (Chapter 1, Exercise 16). We consider the constant vector field $X(x)=a$ for all $x\in\mathbb{R}^{n+1}$. We obtain a vector field $\tilde{X}(x)$ on $\mathbb{S}^n$ by projecting $X(x)$ onto $T_x\mathbb{S}^n$ for $x\in\mathbb{S}^n$. Determine the corresponding flow on $\mathbb{S}^n$. I've been stuck on this for a while, and haven't been sure exactly what to try. This is the partial work that I have so far: Let $\mathbf{1}_{n+1}$ be the $(n+1)\times(n+1)$ identity matrix. Note that the projection onto the tangent space $T_x\mathbb{S}^n$ is simply given by $$P_x:=\mathbf{1}_{n+1}-xx^\top:\mathbb{R}^{n+1}\to T_x\mathbb{S}^n.$$ Now we see that  $$\tilde{X}(x)=P_x(X(x))=P_x(a)=(\mathbf{1}_{n+1} - xx^\top)a = a - xx^\top a = a - \langle{a,x}\rangle x\in T_x\mathbb{S}^n.$$ So for any $p\in\mathbb{S}^n$ we are looking for a solution to the following ODE $$\begin{cases}\dot\gamma(t)=\tilde{X}(\gamma(t))=a-\langle{a,\gamma(t)}\rangle\gamma(t),\\\gamma(0)=p.\end{cases}$$ We have two fixed point solutions given by $\gamma_{\pm}(t)=\pm\frac{a}{\|a\|}.$ I'm pretty sure (although I haven't shown this yet) that if we start from any point that isn't a fixed point that the flow will tend to $\frac{a}{\|a\|}$. At first I thought that the flow might be related to the projection of the flow of $X$ in $\mathbb{R}^{n+1}$, i.e. take $\Gamma:\mathbb{R}\to\mathbb{R}^{n+1}$ given by $\Gamma(t)=p + ta$, and then see if $\gamma(t):=\frac{\Gamma(t)}{\|\Gamma(t)\|}$ solves the ODE. Unfortunately, it doesn't solve it on $\mathbb{S}^1\subseteq\mathbb{R}^2$ (although a reparameterization would probably do the trick in this case since $\mathbb{S}^1$ is $1$-dimensional and the direction of the flow seems to be correct), and in any higher dimensions it doesn't even flow in the right direction. So I'm pretty sure this idea is incorrect, and I wasn't able to get it to work at all. The second idea I had was to consider the geodesic starting at $p$ and in the direction $\dot\gamma(0)=a-\langle{a,p}\rangle p\in T_p\mathbb{S}^n$, and maybe reparameterize it if need be. Unfortunately, this also didn't work and I wasn't able to figure out whether the reparameterization idea would work here. Other than that, I've been pretty stuck and I'm not sure if I'm supposed to be able to just be able to solve this ODE directly or if there is some geometric insight that I can use to determine this flow. Any hints and explanations would be extremely appreciated! I've been working on this for a long time, and this is the only question in this chapter I haven't been able to figure out.",,"['ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry']"
74,Proving existence of solutions for second-order differential equation $u''(t)=-w^2\sin(u(t))$,Proving existence of solutions for second-order differential equation,u''(t)=-w^2\sin(u(t)),"Let $w>0$ and $$u''(t)=-w^2\sin(u(t))$$ for $t\geq 0$.  I'd like to see that the solutions to the system exist on the whole $\mathbb{R}_{+}$ and if an initial value is given, they are supposed to be unique. Usually I'd translate a second-order system to a first-order one, by renaming $v(t):= u''(t)$ and then rewriting it into a first-order matrix differential equation, which would be linear and therefore I'd easily be able to draw conclusions about the existence of solutions, but this does not work here, because of the $\sin()$ function. Another approach would be to immediately try to solve by seperating variables, i.e. $$ \int_0^t \frac{u''(s)}{\sin(u(s))}\,ds = \int_0^t -w^2 \, ds$$ but here the second order ruins the substitution ( or can you somehow integrate that? ). I'm not looking for a solution, but a hint on how to approach these second order autonomous systems!","Let $w>0$ and $$u''(t)=-w^2\sin(u(t))$$ for $t\geq 0$.  I'd like to see that the solutions to the system exist on the whole $\mathbb{R}_{+}$ and if an initial value is given, they are supposed to be unique. Usually I'd translate a second-order system to a first-order one, by renaming $v(t):= u''(t)$ and then rewriting it into a first-order matrix differential equation, which would be linear and therefore I'd easily be able to draw conclusions about the existence of solutions, but this does not work here, because of the $\sin()$ function. Another approach would be to immediately try to solve by seperating variables, i.e. $$ \int_0^t \frac{u''(s)}{\sin(u(s))}\,ds = \int_0^t -w^2 \, ds$$ but here the second order ruins the substitution ( or can you somehow integrate that? ). I'm not looking for a solution, but a hint on how to approach these second order autonomous systems!",,"['real-analysis', 'ordinary-differential-equations']"
75,Number of linearly independent solutions for a second order linear inhomogeneous ODE and PDE,Number of linearly independent solutions for a second order linear inhomogeneous ODE and PDE,,"A second order, linear, homogeneous ordinary differential equation (ODE) has two linearly independent solutions. $\bullet$ Is it also true for a second order, linear, inhomogeneous ODE? $\bullet$ How many linearly independent solutions do a second order partial differential equation (PDE) have?","A second order, linear, homogeneous ordinary differential equation (ODE) has two linearly independent solutions. $\bullet$ Is it also true for a second order, linear, inhomogeneous ODE? $\bullet$ How many linearly independent solutions do a second order partial differential equation (PDE) have?",,"['ordinary-differential-equations', 'partial-derivative']"
76,"Functional equation $f(xy) = f'(x)f(y)$, am I doing it right?","Functional equation , am I doing it right?",f(xy) = f'(x)f(y),"So I want to find all functions $f : \mathbb R \to \mathbb R$ so that: $$f(xy) = f'(x)f(y), \forall x,y \in \mathbb R$$ My try : $y=1$ gives: $f(x) = f'(x)f(1)$ or $f'(x) = \frac{f(x)}{f(1)}$ $$f(x) = e^{\frac{x}{f(1)}}$$ where $f(1)$ must fulfil $$f(1) = e^{\frac 1 {f(1)}}$$ We can conclude $f(1)>0$ and since $\cases{1/x & \text{is monotonic decaying}\\ e^x &\text{is monotonic increasing}}$ Therefore there must exist one and only such $f(1) \in \mathbb R$. Does this make sense?","So I want to find all functions $f : \mathbb R \to \mathbb R$ so that: $$f(xy) = f'(x)f(y), \forall x,y \in \mathbb R$$ My try : $y=1$ gives: $f(x) = f'(x)f(1)$ or $f'(x) = \frac{f(x)}{f(1)}$ $$f(x) = e^{\frac{x}{f(1)}}$$ where $f(1)$ must fulfil $$f(1) = e^{\frac 1 {f(1)}}$$ We can conclude $f(1)>0$ and since $\cases{1/x & \text{is monotonic decaying}\\ e^x &\text{is monotonic increasing}}$ Therefore there must exist one and only such $f(1) \in \mathbb R$. Does this make sense?",,"['calculus', 'ordinary-differential-equations', 'functional-equations']"
77,Existence of a second order linear differential equation,Existence of a second order linear differential equation,,"I have stumbled upon this problem in my differential equations homework: Does there exist a second order linear homogeneous differential equation of the form $$y''+ p(x)y' + q(x)y= 0$$ such that $ p(x) $ and $ q(x) $ are continuous on the entire real line that has both $ f(x) = \cos(x) $ and $ g(x) = e^{x^2} $ as solutions? I have the general solution of $ y = c_1\cos(x) + c_2e^x $ and the $ y' $ and $ y'' $ of this function. I am guessing I need to combine the $ y, y', $ and $ y'' $ but I am having trouble canceling out the resulting constants. Any suggested paths or methods?","I have stumbled upon this problem in my differential equations homework: Does there exist a second order linear homogeneous differential equation of the form $$y''+ p(x)y' + q(x)y= 0$$ such that $ p(x) $ and $ q(x) $ are continuous on the entire real line that has both $ f(x) = \cos(x) $ and $ g(x) = e^{x^2} $ as solutions? I have the general solution of $ y = c_1\cos(x) + c_2e^x $ and the $ y' $ and $ y'' $ of this function. I am guessing I need to combine the $ y, y', $ and $ y'' $ but I am having trouble canceling out the resulting constants. Any suggested paths or methods?",,['ordinary-differential-equations']
78,Solve: $y''+y=cos3x$,Solve:,y''+y=cos3x,"$$y''+y=cos3x$$ $$\lambda^2+\lambda=0\Rightarrow \lambda_{1,2}=\pm i$$ $$y_{h}=c_{1}cosx+c_{2}sinx$$ Because the homogeneous solutions is $ c_{1}cosx   $ and not $c_{1}cos3x$ we can the particular solution to be: $$y_{p}=Acos3x+Bsin3x$$ Substituting the particular solution to the ODE: $$y'_{p}=-3Asin3x+3Bcos3x$$ $$y''_{p}=-9Acos3x-9Bsin3x$$ $$-9Acos3x-9Bsin3x+Acos3x+Bsin3x=cos3x$$ $$-8Acos3x-8Bsin3x=cos3x$$ Is it correct that there is no sin3x on the RHS of the equation? usually it is because the particular solution was incorrect","$$y''+y=cos3x$$ $$\lambda^2+\lambda=0\Rightarrow \lambda_{1,2}=\pm i$$ $$y_{h}=c_{1}cosx+c_{2}sinx$$ Because the homogeneous solutions is $ c_{1}cosx   $ and not $c_{1}cos3x$ we can the particular solution to be: $$y_{p}=Acos3x+Bsin3x$$ Substituting the particular solution to the ODE: $$y'_{p}=-3Asin3x+3Bcos3x$$ $$y''_{p}=-9Acos3x-9Bsin3x$$ $$-9Acos3x-9Bsin3x+Acos3x+Bsin3x=cos3x$$ $$-8Acos3x-8Bsin3x=cos3x$$ Is it correct that there is no sin3x on the RHS of the equation? usually it is because the particular solution was incorrect",,['ordinary-differential-equations']
79,Solve Boundary Value Problem for $y''+ y' + e^xy = f(x)$,Solve Boundary Value Problem for,y''+ y' + e^xy = f(x),Consider to solve Boundary Value Problem : $y''+ y' + e^xy = f(x)$ with $0 < x < 1$ and $y(0)=y(1)=0$ with exact solution $y(x) = \sin \pi x$ $f(x)=(e^x- \pi^2)\sin \pi x + \pi \cos \pi x$. How to plot two curves for exact solution and numerical solution by MATLAB?,Consider to solve Boundary Value Problem : $y''+ y' + e^xy = f(x)$ with $0 < x < 1$ and $y(0)=y(1)=0$ with exact solution $y(x) = \sin \pi x$ $f(x)=(e^x- \pi^2)\sin \pi x + \pi \cos \pi x$. How to plot two curves for exact solution and numerical solution by MATLAB?,,"['ordinary-differential-equations', 'numerical-methods', 'matlab', 'boundary-value-problem']"
80,Differential equation $f''(x)+2 x f(x)f'(x) = 0$,Differential equation,f''(x)+2 x f(x)f'(x) = 0,"I am trying to solve, $ f''(x)+2 x f(x)f'(x) = 0$ with boundary conditions $f(-\infty)=1$ and $f(\infty)=0$. I have found that for instance $f(x) = 3/2 x^{-2}$ but obviously it does not satisfy the proper boundary conditions. Any ideas for a solution?","I am trying to solve, $ f''(x)+2 x f(x)f'(x) = 0$ with boundary conditions $f(-\infty)=1$ and $f(\infty)=0$. I have found that for instance $f(x) = 3/2 x^{-2}$ but obviously it does not satisfy the proper boundary conditions. Any ideas for a solution?",,"['ordinary-differential-equations', 'nonlinear-system']"
81,Local Truncation Error of Implicit Euler,Local Truncation Error of Implicit Euler,,"The LTE of an implicit Euler method is $O(h^2)$ because the method has order $O(h)$, but I'm not sure where to get started in proving this arithmetically. Any help would be appreciated. Thank you!","The LTE of an implicit Euler method is $O(h^2)$ because the method has order $O(h)$, but I'm not sure where to get started in proving this arithmetically. Any help would be appreciated. Thank you!",,"['ordinary-differential-equations', 'numerical-methods', 'fixed-point-iteration']"
82,Difficult Inverse Laplace Transform,Difficult Inverse Laplace Transform,,"I've had this question in my exam, which most of my batch mates couldn't solve it.The question by the way is the Laplace Transform inverse of $$\frac{\ln s}{(s+1)^2}$$ A Hint was also given, which includes the Laplace Transform of ln t.","I've had this question in my exam, which most of my batch mates couldn't solve it.The question by the way is the Laplace Transform inverse of $$\frac{\ln s}{(s+1)^2}$$ A Hint was also given, which includes the Laplace Transform of ln t.",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
83,Is there a way to solve explicitly the following functional equation?,Is there a way to solve explicitly the following functional equation?,,"I want to find an unknown function (actually CDF) $F(p)$ which solves $1 - \lambda F(\frac{q_L}{q_H}p) - (1-\lambda)F(p-[q_H-q_L]) - \frac{K}{p-c_H} = 0$, where $0<\lambda<1$, $q_H > q_L > 0$, $q_H > c_H > 0$, $K>0$, and $p \in (c_H, q_H]$. Unfortunately, I don't really have an idea how to proceed, apart from randomly guessing functional forms (I'm note even sure about which tags to choose for this problem). So any suggestions would be greatly appreciated. Thanks!","I want to find an unknown function (actually CDF) $F(p)$ which solves $1 - \lambda F(\frac{q_L}{q_H}p) - (1-\lambda)F(p-[q_H-q_L]) - \frac{K}{p-c_H} = 0$, where $0<\lambda<1$, $q_H > q_L > 0$, $q_H > c_H > 0$, $K>0$, and $p \in (c_H, q_H]$. Unfortunately, I don't really have an idea how to proceed, apart from randomly guessing functional forms (I'm note even sure about which tags to choose for this problem). So any suggestions would be greatly appreciated. Thanks!",,"['ordinary-differential-equations', 'functions', 'recurrence-relations']"
84,Hermite Polynomials: Rodrigues to Integral Representation,Hermite Polynomials: Rodrigues to Integral Representation,,"I would like to go from this representation of the Hermite polynomials: $$H_n(z)=(-1)^ne^{z^2}\frac{d^n}{dz^n}e^{-z^2} \tag{1}$$ to this representation $$H_n(z)=\frac{2^n}{\sqrt{\pi}}\int_{-\infty}^{\infty}(z+is)^n\,e^{-s^2}\,ds \tag{2}$$ I have tried to use Cauchy's theorem to give the $n^{\textrm{th}}$ derivative term in terms of a contour integral, and then contort the integrand and contour so that we're left with an integral over the real line, but I just can't find a way to do it. I've also read this question , but all of answers go through a different representation/route than the one I desire. Could you help me out? EDIT: To shed some light on what exactly I desire, I will present the first few steps of my attempted path. For an analytic function $f(z)$ , we can express its $n^{\textrm{th}}$ derivative by a contour integral. $$\frac{d^n}{dz^n}f(z)=\frac{n!}{2\pi i}\oint_{\Gamma} \frac{f(w)}{(w-z)^{n+1}} dw $$ Where $\Gamma$ is a simple closed curve that encircles the point $z$ in the complex plane. Using this we can rewrite $(1)$ as $$H_n(z)=(-1)^ne^{z^2}\frac{n!}{2\pi i}\oint_{\Gamma} \frac{e^{-w^2}}{(w-z)^{n+1}} dw$$ I don't know where to go from here. If I make the substitution $s=w-z$ , I end up going in a circle showing that $H_n(z)=H_n(z)$ . Moreover, I can't find a simple way to reduce that integral to an integral over $\mathbb{R}$ (due to the gaussian term in the integral).","I would like to go from this representation of the Hermite polynomials: to this representation I have tried to use Cauchy's theorem to give the derivative term in terms of a contour integral, and then contort the integrand and contour so that we're left with an integral over the real line, but I just can't find a way to do it. I've also read this question , but all of answers go through a different representation/route than the one I desire. Could you help me out? EDIT: To shed some light on what exactly I desire, I will present the first few steps of my attempted path. For an analytic function , we can express its derivative by a contour integral. Where is a simple closed curve that encircles the point in the complex plane. Using this we can rewrite as I don't know where to go from here. If I make the substitution , I end up going in a circle showing that . Moreover, I can't find a simple way to reduce that integral to an integral over (due to the gaussian term in the integral).","H_n(z)=(-1)^ne^{z^2}\frac{d^n}{dz^n}e^{-z^2} \tag{1} H_n(z)=\frac{2^n}{\sqrt{\pi}}\int_{-\infty}^{\infty}(z+is)^n\,e^{-s^2}\,ds \tag{2} n^{\textrm{th}} f(z) n^{\textrm{th}} \frac{d^n}{dz^n}f(z)=\frac{n!}{2\pi i}\oint_{\Gamma} \frac{f(w)}{(w-z)^{n+1}} dw  \Gamma z (1) H_n(z)=(-1)^ne^{z^2}\frac{n!}{2\pi i}\oint_{\Gamma} \frac{e^{-w^2}}{(w-z)^{n+1}} dw s=w-z H_n(z)=H_n(z) \mathbb{R}","['ordinary-differential-equations', 'special-functions', 'complex-integration']"
85,What is the degree of the differential equation $\left|\frac{dy}{dx}\right| + \left|y\right| = 0$?,What is the degree of the differential equation ?,\left|\frac{dy}{dx}\right| + \left|y\right| = 0,"Consider the differential equation $$\left|\frac{dy}{dx}\right| + \left|y\right| = 0$$  where $\left|\cdot\right|$ means the absolute value function. I have to find the degree of the above differential equation. Can I say the degree of this differential equation is not defined as it is not a polynomial in $y'$? If we further solve it, we get $$\left|\frac{dy}{dx}\right| = -\left|y\right|$$ Then taking square we get  $ \left(\frac{dy}{dx}\right)^{2}- y^{2} =0$ which has degree $2$. Now can I say that the two differential equation are not same? So the degree of the first one is not defined. Am I right?","Consider the differential equation $$\left|\frac{dy}{dx}\right| + \left|y\right| = 0$$  where $\left|\cdot\right|$ means the absolute value function. I have to find the degree of the above differential equation. Can I say the degree of this differential equation is not defined as it is not a polynomial in $y'$? If we further solve it, we get $$\left|\frac{dy}{dx}\right| = -\left|y\right|$$ Then taking square we get  $ \left(\frac{dy}{dx}\right)^{2}- y^{2} =0$ which has degree $2$. Now can I say that the two differential equation are not same? So the degree of the first one is not defined. Am I right?",,['ordinary-differential-equations']
86,Ordinary Differential Equations used in Cosmology,Ordinary Differential Equations used in Cosmology,,I'm just reading over some Cosmology notes and there is a little ODE solve that I am not quite understanding. I have an equation of the form: $$ \ddot{R}=-\frac{GM}{R^{2}} $$ Integrating gives: $$ \dot{R}^{2}=+\frac{2GM}{R}+C $$ The notes are essentially saying that this can be solved with a parameter $\theta$: Could anyone run through the method for solving ODE's such as this. In terms of density this can be written as: $$ \dot{R}^{2}=\frac{4\pi{G}}{3}\rho_{0}R $$,I'm just reading over some Cosmology notes and there is a little ODE solve that I am not quite understanding. I have an equation of the form: $$ \ddot{R}=-\frac{GM}{R^{2}} $$ Integrating gives: $$ \dot{R}^{2}=+\frac{2GM}{R}+C $$ The notes are essentially saying that this can be solved with a parameter $\theta$: Could anyone run through the method for solving ODE's such as this. In terms of density this can be written as: $$ \dot{R}^{2}=\frac{4\pi{G}}{3}\rho_{0}R $$,,['ordinary-differential-equations']
87,Exact Differential Equations $(axy^2+by)dx+(bx^2y+ax)dy=0$.,Exact Differential Equations .,(axy^2+by)dx+(bx^2y+ax)dy=0,"$M(x,y)dx + N(x,y)dy=0$ is said to be a perfect differential when $$\frac{\partial (M(x,y))}{\partial y}=\frac{\partial (N(x,y))}{\partial x}$$ Let $M_y=\frac{\partial (M(x,y))}{\partial y}$ and $N_x=\frac{\partial (N(x,y))}{\partial x}$. In case if: $\frac{M_y-N_x}{N(x,y)}$ is only a function of x only (say $f(x)$) then it has a integrating factor $e^{\int f(x)dx}$. But if $\frac{M_y-N_x}{N(x,y)}$ is a constant then what will be the integrating factor? Say for this problem:   $(axy^2+by)dx+(bx^2y+ax)dy=0$ Is there any other method to solve this differential equation?","$M(x,y)dx + N(x,y)dy=0$ is said to be a perfect differential when $$\frac{\partial (M(x,y))}{\partial y}=\frac{\partial (N(x,y))}{\partial x}$$ Let $M_y=\frac{\partial (M(x,y))}{\partial y}$ and $N_x=\frac{\partial (N(x,y))}{\partial x}$. In case if: $\frac{M_y-N_x}{N(x,y)}$ is only a function of x only (say $f(x)$) then it has a integrating factor $e^{\int f(x)dx}$. But if $\frac{M_y-N_x}{N(x,y)}$ is a constant then what will be the integrating factor? Say for this problem:   $(axy^2+by)dx+(bx^2y+ax)dy=0$ Is there any other method to solve this differential equation?",,['ordinary-differential-equations']
88,Prove that there is no solution in terms of elementary functions of $y'=x^2+y^2$,Prove that there is no solution in terms of elementary functions of,y'=x^2+y^2,"I have seen before that $$y'=x^2+y^2$$ Has no solution in terms of elementary functions. I typed it in on Wolfram Alpha ( here ) and got a giant solution with a bunch of Bessel functions that I did not understand. This equation is almost like a Ricatti equation, but not exactly, so there may be some insight to be gained there. Again, my question is: How do you prove that this differential equation has no solution in terms of elementary functions?","I have seen before that $$y'=x^2+y^2$$ Has no solution in terms of elementary functions. I typed it in on Wolfram Alpha ( here ) and got a giant solution with a bunch of Bessel functions that I did not understand. This equation is almost like a Ricatti equation, but not exactly, so there may be some insight to be gained there. Again, my question is: How do you prove that this differential equation has no solution in terms of elementary functions?",,['ordinary-differential-equations']
89,Solving differential equation $x''(t)=x^6$.,Solving differential equation .,x''(t)=x^6,Solve the following differential equation $$x''(t)=x^6(t)$$ If I had $x'(t)$ instead of $x''(t)$ the exercise would have been easier for me. I would appreciate some help with this problem. Thank you very much.,Solve the following differential equation $$x''(t)=x^6(t)$$ If I had $x'(t)$ instead of $x''(t)$ the exercise would have been easier for me. I would appreciate some help with this problem. Thank you very much.,,['ordinary-differential-equations']
90,"Differential Equations: Stable, Semi-Stable, and Unstable","Differential Equations: Stable, Semi-Stable, and Unstable",,"I am trying to identify the stable, unstable, and semistable critical points for the following differential equation: $\dfrac{dy}{dt} = 4y^2 (4 - y^2)$. If I understand the definition of stable and unstable critical points, then it seems to me that the semi-stable point is at $y = 0$ since in a neighborhood around $y=0$ we have the slope as positive. It also seems that $y = -2,2$ is a stable critical point since points around its neighborhood are negative for decreasing values and positive for increasing values. So it appears there are no unstable critical points. Am I on the right track here? I also need to find $k$ for $y(t) \rightarrow k$ given $y(0) = 1.4$ and $k$ for $y(t) \rightarrow k$ given $y(0) = -3.2$. Any help anyone can provide is appreciated.","I am trying to identify the stable, unstable, and semistable critical points for the following differential equation: $\dfrac{dy}{dt} = 4y^2 (4 - y^2)$. If I understand the definition of stable and unstable critical points, then it seems to me that the semi-stable point is at $y = 0$ since in a neighborhood around $y=0$ we have the slope as positive. It also seems that $y = -2,2$ is a stable critical point since points around its neighborhood are negative for decreasing values and positive for increasing values. So it appears there are no unstable critical points. Am I on the right track here? I also need to find $k$ for $y(t) \rightarrow k$ given $y(0) = 1.4$ and $k$ for $y(t) \rightarrow k$ given $y(0) = -3.2$. Any help anyone can provide is appreciated.",,['ordinary-differential-equations']
91,Is there a way to know if the solution curve to a differential equation is an even or odd function?,Is there a way to know if the solution curve to a differential equation is an even or odd function?,,"Suppose you are given a differential equation and a set of initial conditions (or boundary conditions) pointing to a unique solution.  Is there any way to know off-hand if the solution will be an even function, an odd function, or neither? This is, I suppose, tricky, because the fundamental set of solutions could include both even and odd functions (say, sine and cosine).  The trick is knowing a priori that, for a given set of conditions, the constant coefficient for all of the odd solutions is zero and the constant coefficient for all (or some?) of the even functions are non-zero values.  Or vice versa. Can this be done?","Suppose you are given a differential equation and a set of initial conditions (or boundary conditions) pointing to a unique solution.  Is there any way to know off-hand if the solution will be an even function, an odd function, or neither? This is, I suppose, tricky, because the fundamental set of solutions could include both even and odd functions (say, sine and cosine).  The trick is knowing a priori that, for a given set of conditions, the constant coefficient for all of the odd solutions is zero and the constant coefficient for all (or some?) of the even functions are non-zero values.  Or vice versa. Can this be done?",,['ordinary-differential-equations']
92,"Phase curve of $\ddot{x}=-x,\ddot{y}=-y$",Phase curve of,"\ddot{x}=-x,\ddot{y}=-y","Rewriting the ODE in the title, we get $$\dot{x_1}=x_2,\dot{x_2}=-x_1, \dot{x_3}=x_4, \dot{x_4}=-x_3.$$ It is easy to show that $$x_1=A\cos(t)+B\sin(t),x_2=B\cos(t)-A\sin(t),\\x_3=C\cos(t)+D\sin(t),x_4=D\cos(t)-C\sin(t).$$ In Ordinary Differential Equations by V. I. Arnold , there are several problems on this equation. For example, it can be shown that each phase curve is on a 3-sphere, and is a great circle of that. The next problem is more difficult: show that the phase curves on a given 3-sphere form a 2-sphere . I attempt to find out which 3-dimensional subspace the 2-sphere lies in, but in vain. And the last question is about the linking number . Since 3-sphere can be regarded as $\mathbb R^3\cup \{\infty\}$, a partition of 3-sphere into circles determines a partition of $\mathbb R^3$ into circles and nonclosed circles. Show that any two of the circles of this partition are linked with linking number 1 . I am not sure how to visualize this partition, and what the circles stand for. Can anyone give some advice? Thanks in advance.","Rewriting the ODE in the title, we get $$\dot{x_1}=x_2,\dot{x_2}=-x_1, \dot{x_3}=x_4, \dot{x_4}=-x_3.$$ It is easy to show that $$x_1=A\cos(t)+B\sin(t),x_2=B\cos(t)-A\sin(t),\\x_3=C\cos(t)+D\sin(t),x_4=D\cos(t)-C\sin(t).$$ In Ordinary Differential Equations by V. I. Arnold , there are several problems on this equation. For example, it can be shown that each phase curve is on a 3-sphere, and is a great circle of that. The next problem is more difficult: show that the phase curves on a given 3-sphere form a 2-sphere . I attempt to find out which 3-dimensional subspace the 2-sphere lies in, but in vain. And the last question is about the linking number . Since 3-sphere can be regarded as $\mathbb R^3\cup \{\infty\}$, a partition of 3-sphere into circles determines a partition of $\mathbb R^3$ into circles and nonclosed circles. Show that any two of the circles of this partition are linked with linking number 1 . I am not sure how to visualize this partition, and what the circles stand for. Can anyone give some advice? Thanks in advance.",,['ordinary-differential-equations']
93,Is there an Analytical solution for Blasius equation?,Is there an Analytical solution for Blasius equation?,,"Blasius Equation was introduced to me during my University time ...and I would like to have a solution for it $$y''' + yy'' = 0$$ the $y$ here makes the equation from a linear simple to solve to a non-linear almost impossible to solve , one solution is to use the numerical method but I would like to have the exact analytic solution .","Blasius Equation was introduced to me during my University time ...and I would like to have a solution for it $$y''' + yy'' = 0$$ the $y$ here makes the equation from a linear simple to solve to a non-linear almost impossible to solve , one solution is to use the numerical method but I would like to have the exact analytic solution .",,['ordinary-differential-equations']
94,"Total no. of solutions of $dy/dx + |y| =0 , \ y (0)=0$",Total no. of solutions of,"dy/dx + |y| =0 , \ y (0)=0","I have to find total number solutions  of $dy/dx + |y| =0 , \ y (0)=0$ , should I form two diferential equations by considering y positive and negative , i don't seem to be confident. Hints for this? Thanks","I have to find total number solutions  of $dy/dx + |y| =0 , \ y (0)=0$ , should I form two diferential equations by considering y positive and negative , i don't seem to be confident. Hints for this? Thanks",,['ordinary-differential-equations']
95,Solving $t^2=x(t) x'(t) t^3+x'(t)$,Solving,t^2=x(t) x'(t) t^3+x'(t),Solve the following differential equation $$t^2=x(t) x'(t) t^3+x'(t)$$ I would appreciate some help with this problem. Thank you very much.,Solve the following differential equation $$t^2=x(t) x'(t) t^3+x'(t)$$ I would appreciate some help with this problem. Thank you very much.,,['ordinary-differential-equations']
96,Solve $y'=1+x-(1+2x)y+xy^2$,Solve,y'=1+x-(1+2x)y+xy^2,Some observations : 1) Not Bernoulli/homogeneous 2) Not exact How to solve this and in general how to attempt equations that don't have a standard method ? Appreciate any help,Some observations : 1) Not Bernoulli/homogeneous 2) Not exact How to solve this and in general how to attempt equations that don't have a standard method ? Appreciate any help,,['ordinary-differential-equations']
97,Nonlineary differential equation,Nonlineary differential equation,,"I have following problem: let's assume this nonlinear differential equation $$y''(x)+ay(x)+by'(x)^2=0$$ for unknown function $y(x)$ of one variable $x$ and $a,b \in \mathbb{R}$ are real parameters. Is there any method to find general solution? I can only find solution for $b=0$ as it is then linear ode. P.S. Does this equation have a special name?","I have following problem: let's assume this nonlinear differential equation $$y''(x)+ay(x)+by'(x)^2=0$$ for unknown function $y(x)$ of one variable $x$ and $a,b \in \mathbb{R}$ are real parameters. Is there any method to find general solution? I can only find solution for $b=0$ as it is then linear ode. P.S. Does this equation have a special name?",,['ordinary-differential-equations']
98,Solve $xy(1+xy^2)\frac {dy}{dx}=1$,Solve,xy(1+xy^2)\frac {dy}{dx}=1,"Solve $xy(1+xy^2)\frac {dy}{dx}=1$ Tried to solve it as an exact ODE, but it didnt work.","Solve $xy(1+xy^2)\frac {dy}{dx}=1$ Tried to solve it as an exact ODE, but it didnt work.",,['ordinary-differential-equations']
99,Differential Equations; A prerequisite to Differential Geometry?,Differential Equations; A prerequisite to Differential Geometry?,,"Is thorough knowledge in ODEs/PDEs and solution techniques to be considered a prerequisite to the study of Differential Geometry (Specifically Riemannian Geometry)? If not, how would one describe the relation between the subjects?","Is thorough knowledge in ODEs/PDEs and solution techniques to be considered a prerequisite to the study of Differential Geometry (Specifically Riemannian Geometry)? If not, how would one describe the relation between the subjects?",,"['ordinary-differential-equations', 'differential-geometry']"
