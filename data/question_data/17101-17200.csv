,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to prove that this function will converge to $1$?,How to prove that this function will converge to ?,1,I have an assignment for tomorrow that ask me to prove that the sequence/function $f(x) =\begin{cases}\dfrac x2\quad\quad\quad\text{if } x\text{ is even}\\3x+1\quad\;\text{if }x\text{ is odd}\end{cases}$ (where $x$ is a natural number) will converge to $1$ . I have tried by hand and it seems to work but I have no idea where to start. Thanks a lot.,I have an assignment for tomorrow that ask me to prove that the sequence/function (where is a natural number) will converge to . I have tried by hand and it seems to work but I have no idea where to start. Thanks a lot.,f(x) =\begin{cases}\dfrac x2\quad\quad\quad\text{if } x\text{ is even}\\3x+1\quad\;\text{if }x\text{ is odd}\end{cases} x 1,['calculus']
1,Efficient free alternative to *Mathematica*,Efficient free alternative to *Mathematica*,,"I am searching for a free alternative to Mathematica . By efficient, I mean that it should have every (or at least almost every) function that you can find in Mathematica , including for example Number Recognition . Also, if you know such software, can you post a reference manual for usage here as well?","I am searching for a free alternative to Mathematica . By efficient, I mean that it should have every (or at least almost every) function that you can find in Mathematica , including for example Number Recognition . Also, if you know such software, can you post a reference manual for usage here as well?",,"['calculus', 'soft-question', 'math-software', 'mathematical-modeling', 'computational-mathematics']"
2,Find the value of : $\lim_{n\to\infty} \left( \left(\sum_{k=n+1}^{2n}2\sqrt[2k]{2k}-\sqrt[k]{k}\right)-n\right)$,Find the value of :,\lim_{n\to\infty} \left( \left(\sum_{k=n+1}^{2n}2\sqrt[2k]{2k}-\sqrt[k]{k}\right)-n\right),"Find $$\lim_{n\to\infty} \left( \left(\sum_{k=n+1}^{2n}2\sqrt[2k]{2k}-\sqrt[k]{k}\right)-n\right).$$ I have tried rewriting the sum in a clever way, applying the Mean Value Theorem or Stolz-Cesaro Lemma somehow but haven't found anything fruitful. Can someone please share a hint/trick to evaluate this? Thank you.","Find $$\lim_{n\to\infty} \left( \left(\sum_{k=n+1}^{2n}2\sqrt[2k]{2k}-\sqrt[k]{k}\right)-n\right).$$ I have tried rewriting the sum in a clever way, applying the Mean Value Theorem or Stolz-Cesaro Lemma somehow but haven't found anything fruitful. Can someone please share a hint/trick to evaluate this? Thank you.",,"['calculus', 'sequences-and-series']"
3,Rate Of Change Of Shadow,Rate Of Change Of Shadow,,"A spotlight on the ground shines on a building $12m$ away. If a man $2m$ tall walks from the spotlight towards the building at a speed of $1.6m/s$, how fast is the length of his shadow on the building decreasing when he is $4m$ from the building? This is my diagram of the scenario: Please have a look at my solution and confirm if correct and if not, give me a hint as to what I'm doing wrong. $H =$ height of the shadow on the building $h = $height of the man $X = $distance from building to the man $x = $distance from spotlight to the man From the diagram $x + X = 12, $ and $h = 2$ $x(t) = 1.6t$, with $t$ in $seconds$. $\frac{dx}{dt} = 1.6$ By similar triangles: $\frac{h}{H} = \frac{x}{x+X} \Leftrightarrow H = \frac{(x+X)h}{x} = \frac{(12)(2)}{x} = \frac{24}{x}$ $\Rightarrow \frac{dH}{dt} = \frac{d}{dt}(\frac{24}{x})  = -\frac{24}{x^2}\frac{dx}{dt}$ When the man is $4m$ from the building, $x = 12 - 4=8 \Rightarrow \frac{dH}{dt}  = -\frac{24}{8^2}(1.6) = -\frac{3}{5}m/s$","A spotlight on the ground shines on a building $12m$ away. If a man $2m$ tall walks from the spotlight towards the building at a speed of $1.6m/s$, how fast is the length of his shadow on the building decreasing when he is $4m$ from the building? This is my diagram of the scenario: Please have a look at my solution and confirm if correct and if not, give me a hint as to what I'm doing wrong. $H =$ height of the shadow on the building $h = $height of the man $X = $distance from building to the man $x = $distance from spotlight to the man From the diagram $x + X = 12, $ and $h = 2$ $x(t) = 1.6t$, with $t$ in $seconds$. $\frac{dx}{dt} = 1.6$ By similar triangles: $\frac{h}{H} = \frac{x}{x+X} \Leftrightarrow H = \frac{(x+X)h}{x} = \frac{(12)(2)}{x} = \frac{24}{x}$ $\Rightarrow \frac{dH}{dt} = \frac{d}{dt}(\frac{24}{x})  = -\frac{24}{x^2}\frac{dx}{dt}$ When the man is $4m$ from the building, $x = 12 - 4=8 \Rightarrow \frac{dH}{dt}  = -\frac{24}{8^2}(1.6) = -\frac{3}{5}m/s$",,['calculus']
4,Solution verification: Find the orthogonal trajectories of the family of curves for $x^2 + 2y^2 = k^2$,Solution verification: Find the orthogonal trajectories of the family of curves for,x^2 + 2y^2 = k^2,"I need help with the following question: Find the orthogonal trajectories of the family of curves for $x^2 + 2y^2 = k^2$ I have taken the following steps, are they correct? From what I understand, I have to take the following steps First differentiate to find the differential equation Then write the differential equation in this $$\frac{dy}{dx} = -\frac{1}{f(x,y)}$$ Then solve the new equation. So, this is what I have: The first step is to differentiate with respect to $x$ so find $\frac{dy}{dx}$. \begin{align} \frac{dy}{dx} x^2 + \frac{dy}{dx} 2y^2 &= \frac{dy}{dx}k^2 \\ 2x + \frac{dy}{dx}\cdot 4y &= 0 \\ \frac{dy}{dx} &= -\frac{x}{2y} \end{align} Now, the second step is to do the negative recipricol. $$\frac{dy}{dx} = \frac{2y}{x}$$ The third step is to solve the newly formed differential equation. \begin{align} dy \frac{1}{2y} &= \frac{1}{x} dx \\ \int \frac{1}{2y} dy &= \int \frac{1}{x} dx \\ \frac{\ln{y}}{2} &= \ln{x} + C \\ \ln y &= 2 \ln x + C\\ \ln y &= \ln x^2 + C \\ y &= Ax^2 \quad \end{align} In this case, $A = e^C$. Are my steps correct? Thanks a bunch for your help! EDIT: I have a found a minor error in the last few steps when multiplying the two. It should be: \begin{align} \ln y &= 2(\ln x + C) \\ \ln y &= 2\ln x + 2C \\ e^{\ln y} &= e^{\ln x^2 + 2C} \\ y &= x^2 \cdot e^{2C} \\ \end{align} Here, we let $A = e^{2C}$ and say the final answer is $$y = Ax^2$$","I need help with the following question: Find the orthogonal trajectories of the family of curves for $x^2 + 2y^2 = k^2$ I have taken the following steps, are they correct? From what I understand, I have to take the following steps First differentiate to find the differential equation Then write the differential equation in this $$\frac{dy}{dx} = -\frac{1}{f(x,y)}$$ Then solve the new equation. So, this is what I have: The first step is to differentiate with respect to $x$ so find $\frac{dy}{dx}$. \begin{align} \frac{dy}{dx} x^2 + \frac{dy}{dx} 2y^2 &= \frac{dy}{dx}k^2 \\ 2x + \frac{dy}{dx}\cdot 4y &= 0 \\ \frac{dy}{dx} &= -\frac{x}{2y} \end{align} Now, the second step is to do the negative recipricol. $$\frac{dy}{dx} = \frac{2y}{x}$$ The third step is to solve the newly formed differential equation. \begin{align} dy \frac{1}{2y} &= \frac{1}{x} dx \\ \int \frac{1}{2y} dy &= \int \frac{1}{x} dx \\ \frac{\ln{y}}{2} &= \ln{x} + C \\ \ln y &= 2 \ln x + C\\ \ln y &= \ln x^2 + C \\ y &= Ax^2 \quad \end{align} In this case, $A = e^C$. Are my steps correct? Thanks a bunch for your help! EDIT: I have a found a minor error in the last few steps when multiplying the two. It should be: \begin{align} \ln y &= 2(\ln x + C) \\ \ln y &= 2\ln x + 2C \\ e^{\ln y} &= e^{\ln x^2 + 2C} \\ y &= x^2 \cdot e^{2C} \\ \end{align} Here, we let $A = e^{2C}$ and say the final answer is $$y = Ax^2$$",,"['calculus', 'ordinary-differential-equations', 'solution-verification']"
5,Halmos on Definability and Luzin on Division by 0,Halmos on Definability and Luzin on Division by 0,,"For a successful introduction of a new symbol (e.g. ' $\emptyset$ ') into a mathematical discourse it is necessary and sufficient that the symbol refer to something (e.g. Existence + Specification in ZF) and to nothing else (e.g. Extensionality). I learned of this notion of definability from Halmos (1960). Luzin (1961) has an introductory section (§8) explaining why division by $0$ isn't allowed. His argument seems new to me so I just want to ask whether I understood it properly. He doesn't say this explicitly, but it seems that he relies on the notion of definability described above. Consider: $$a\over 0 \tag 1$$ We know that $(1)$ denotes the unique number $b$ s.t. $b \cdot 0 = a$ . Now, either $a=0$ or $a \ne 0$ . If $a = 0$ , then according to that definition, $(1)$ denotes the unique number $b$ that satisfies the equation $b \cdot 0 = 0$ . But all numbers satisfy that equation, so while there is such a number $b$ , there is no unique root of that equation, so by the definition of definability, $(1)$ fails to denote a number when $a =0$ . If $a \ne 0$ , then according to that same definition, $(1)$ denotes the unique number $b$ that satisfies the equation $b \cdot0 =a$ , where $a$ by hypothesis differs from $0$ . But no $b$ different from $0$ satisfies that equation, so by the definition of definability, $(1)$ fails to denote a number when $a \ne 0$ . Since in both cases $(1)$ fails to denote a unique number, $(1)$ is said to be ill-defined. That's my reconstruction of Luzin's argument - is it entirely correct? References Halmos, P. (1960) Naive Set Theory . Luzin, N.N. (1961) Differential Calculus .","For a successful introduction of a new symbol (e.g. ' ') into a mathematical discourse it is necessary and sufficient that the symbol refer to something (e.g. Existence + Specification in ZF) and to nothing else (e.g. Extensionality). I learned of this notion of definability from Halmos (1960). Luzin (1961) has an introductory section (§8) explaining why division by isn't allowed. His argument seems new to me so I just want to ask whether I understood it properly. He doesn't say this explicitly, but it seems that he relies on the notion of definability described above. Consider: We know that denotes the unique number s.t. . Now, either or . If , then according to that definition, denotes the unique number that satisfies the equation . But all numbers satisfy that equation, so while there is such a number , there is no unique root of that equation, so by the definition of definability, fails to denote a number when . If , then according to that same definition, denotes the unique number that satisfies the equation , where by hypothesis differs from . But no different from satisfies that equation, so by the definition of definability, fails to denote a number when . Since in both cases fails to denote a unique number, is said to be ill-defined. That's my reconstruction of Luzin's argument - is it entirely correct? References Halmos, P. (1960) Naive Set Theory . Luzin, N.N. (1961) Differential Calculus .",\emptyset 0 a\over 0 \tag 1 (1) b b \cdot 0 = a a=0 a \ne 0 a = 0 (1) b b \cdot 0 = 0 b (1) a =0 a \ne 0 (1) b b \cdot0 =a a 0 b 0 (1) a \ne 0 (1) (1),"['calculus', 'algebra-precalculus', 'elementary-set-theory', 'arithmetic', 'definition']"
6,A counterexample,A counterexample,,"Let $f:\mathbb{R}\to(0,\infty)$ a locally integrable function. I want to compare these two conditions $$\limsup_{r\to + \infty}\frac{r}{\int_{-r}^r f(x)dx}<+\infty. \tag{1}\label{1}$$ and $$\limsup_{r\to + \infty}\frac{r^{\frac{1}{p}}\left(\int_{-r}^r f(x)^qdx\right)^\frac{1}{q}}{\int_{-r}^r f(x)dx}<+\infty, \tag{2}\label{2}$$ with $p,q>1$ and $\frac{1}{p}+\frac{1}{q}=1.$ We can see that if $f$ is bounded then $\eqref{1}$ is stronger than $\eqref{2}$, because we would have $$r^{\frac{1}{p}}\left(\int_{-r}^r f(x)^qdx\right)^\frac{1}{q}\leq M 2^\frac{1}{q}r.$$ Now if $\inf_{s\in\mathbb{R}}f(s)=c>0$, we have $\eqref{2}$ stronger than $\eqref{1}$. Because $$c\ 2^\frac{1}{q} \ r\leq r^{\frac{1}{p}}\left(\int_{-r}^r f(x)^qdx\right)^\frac{1}{q}.$$ Of course if we have $c\leq f(s)\leq M$ with $c>0$, then $\eqref{1}$ and $\eqref{2}$ are equivalent. I found that for example $f(x)=e^x$, $f$ satisfies $\eqref{1}$ but not $\eqref{2}$. I don't know if I can find a function $f$ which satisfies $\eqref{2}$ but not $\eqref{1}$. To search it, $f$ must necessarily satisfy $\inf_{s\in\mathbb{R}}f(s)=0$.","Let $f:\mathbb{R}\to(0,\infty)$ a locally integrable function. I want to compare these two conditions $$\limsup_{r\to + \infty}\frac{r}{\int_{-r}^r f(x)dx}<+\infty. \tag{1}\label{1}$$ and $$\limsup_{r\to + \infty}\frac{r^{\frac{1}{p}}\left(\int_{-r}^r f(x)^qdx\right)^\frac{1}{q}}{\int_{-r}^r f(x)dx}<+\infty, \tag{2}\label{2}$$ with $p,q>1$ and $\frac{1}{p}+\frac{1}{q}=1.$ We can see that if $f$ is bounded then $\eqref{1}$ is stronger than $\eqref{2}$, because we would have $$r^{\frac{1}{p}}\left(\int_{-r}^r f(x)^qdx\right)^\frac{1}{q}\leq M 2^\frac{1}{q}r.$$ Now if $\inf_{s\in\mathbb{R}}f(s)=c>0$, we have $\eqref{2}$ stronger than $\eqref{1}$. Because $$c\ 2^\frac{1}{q} \ r\leq r^{\frac{1}{p}}\left(\int_{-r}^r f(x)^qdx\right)^\frac{1}{q}.$$ Of course if we have $c\leq f(s)\leq M$ with $c>0$, then $\eqref{1}$ and $\eqref{2}$ are equivalent. I found that for example $f(x)=e^x$, $f$ satisfies $\eqref{1}$ but not $\eqref{2}$. I don't know if I can find a function $f$ which satisfies $\eqref{2}$ but not $\eqref{1}$. To search it, $f$ must necessarily satisfy $\inf_{s\in\mathbb{R}}f(s)=0$.",,"['calculus', 'integration', 'functions', 'examples-counterexamples']"
7,Center of Mass and Centroid,Center of Mass and Centroid,,"Find the centroid of the region lying between the graphs of the functions $y=\sin x$ and $y=\cos x$ over the interval $[0,\frac\pi4]$. I approached the question like this: Find the $M$ $$M = \int_0^{\tfrac\pi4}(\sin x-\cos x)\,dx = 1-\sqrt2$$ Find the $M$ of $y$ $$M_y = \int_0^{\tfrac\pi4}x(\sin x-\cos x)\,dx = 1-\frac{\pi}{2\sqrt2}$$ Find the $M$ of $x$ $$M_x = \int_0^{\tfrac\pi4}\frac12(\sin x-\cos x)^2\,dx = \frac18(\pi-2)$$ The center of mass at $y = M_x/M$ and the center of mass at $x = M_y/M$ $$y = \frac{\dfrac18(\pi-2)}{1-\sqrt2},x = \frac{1-\dfrac{\pi}{2\sqrt2}}{1-\sqrt2}$$ I appreciate the help! Thank you for the comments.","Find the centroid of the region lying between the graphs of the functions $y=\sin x$ and $y=\cos x$ over the interval $[0,\frac\pi4]$. I approached the question like this: Find the $M$ $$M = \int_0^{\tfrac\pi4}(\sin x-\cos x)\,dx = 1-\sqrt2$$ Find the $M$ of $y$ $$M_y = \int_0^{\tfrac\pi4}x(\sin x-\cos x)\,dx = 1-\frac{\pi}{2\sqrt2}$$ Find the $M$ of $x$ $$M_x = \int_0^{\tfrac\pi4}\frac12(\sin x-\cos x)^2\,dx = \frac18(\pi-2)$$ The center of mass at $y = M_x/M$ and the center of mass at $x = M_y/M$ $$y = \frac{\dfrac18(\pi-2)}{1-\sqrt2},x = \frac{1-\dfrac{\pi}{2\sqrt2}}{1-\sqrt2}$$ I appreciate the help! Thank you for the comments.",,"['calculus', 'integration', 'definite-integrals']"
8,Maclaurin Series for a natural logarithm,Maclaurin Series for a natural logarithm,,"Can anyone please help me with this question? Find the Maclaurin series and the interval of convergence for $f(x) = \ln(1-7x^9)$ I thought the answer was $$\sum_{n=1}^{\infty} (-1)^n \frac{7x^{9n}}{n} $$ but it seems that my homework assignment website will not accept that answer. I also am not sure how to find the interval of convergence. I know that $\ln|1-x|$ converges for $|x| < 1$, but I cannot figure out the interval of convergence for my current problem. Any help or insight is greatly appreciated! :)","Can anyone please help me with this question? Find the Maclaurin series and the interval of convergence for $f(x) = \ln(1-7x^9)$ I thought the answer was $$\sum_{n=1}^{\infty} (-1)^n \frac{7x^{9n}}{n} $$ but it seems that my homework assignment website will not accept that answer. I also am not sure how to find the interval of convergence. I know that $\ln|1-x|$ converges for $|x| < 1$, but I cannot figure out the interval of convergence for my current problem. Any help or insight is greatly appreciated! :)",,"['calculus', 'sequences-and-series', 'taylor-expansion']"
9,Convergence of $\sum_{k=1}^{\infty} \left (\sum_{j=1}^{k}\frac 1 j\right)^{-k}$,Convergence of,\sum_{k=1}^{\infty} \left (\sum_{j=1}^{k}\frac 1 j\right)^{-k},"Does $\displaystyle\sum_{k=1}^{\infty} \left (\sum_{j=1}^{k}\frac 1 j\right)^{-k}$ converges ? Let's call the inner sum $a_k$ such that $\displaystyle\sum_{k=1}^{\infty} (a_k)^{-k}$, applying root test we get: $(a_k)^{-1}= \left(\sum_{j=1}^{k}\frac 1 j\right)^{-1} = \frac {1}{1/1+1/2+...1/k}<1$ So the given sum converges. Is that all right ?","Does $\displaystyle\sum_{k=1}^{\infty} \left (\sum_{j=1}^{k}\frac 1 j\right)^{-k}$ converges ? Let's call the inner sum $a_k$ such that $\displaystyle\sum_{k=1}^{\infty} (a_k)^{-k}$, applying root test we get: $(a_k)^{-1}= \left(\sum_{j=1}^{k}\frac 1 j\right)^{-1} = \frac {1}{1/1+1/2+...1/k}<1$ So the given sum converges. Is that all right ?",,"['calculus', 'sequences-and-series', 'convergence-divergence']"
10,Involutive fourier transform,Involutive fourier transform,,"The writer here states I am introducing a viewpoint  (the involutive convention)  which makes the Fourier transform its own inverse  (i.e., the Fourier transform so defined is an  involution). If I am reading the notation correctly, the definition given is: $$F(f)(s) = \int_{-\infty}^{\infty}\exp(2\pi is x)\overline{f(x)}dx.$$ Under this convention, $F$ fails to be a linear operator; but, I don't think this is too big of a deal, since $F$ ends up being conjugate-linear . In any event, I have never seen this definition before. My question is, firstly, does it have any subtle issues that make it a bad idea? If not, a thoughtful discussion of the benefits of this definition would be appreciated.","The writer here states I am introducing a viewpoint  (the involutive convention)  which makes the Fourier transform its own inverse  (i.e., the Fourier transform so defined is an  involution). If I am reading the notation correctly, the definition given is: $$F(f)(s) = \int_{-\infty}^{\infty}\exp(2\pi is x)\overline{f(x)}dx.$$ Under this convention, $F$ fails to be a linear operator; but, I don't think this is too big of a deal, since $F$ ends up being conjugate-linear . In any event, I have never seen this definition before. My question is, firstly, does it have any subtle issues that make it a bad idea? If not, a thoughtful discussion of the benefits of this definition would be appreciated.",,"['calculus', 'functional-analysis', 'reference-request', 'fourier-analysis', 'integral-transforms']"
11,How find the integral $I=\int_{-R}^{R}\frac{\sqrt{R^2-x^2}}{(a-x)\sqrt{R^2+a^2-2ax}}dx$,How find the integral,I=\int_{-R}^{R}\frac{\sqrt{R^2-x^2}}{(a-x)\sqrt{R^2+a^2-2ax}}dx,"Find the integral: $$I=\int_{-R}^{R}\dfrac{\sqrt{R^2-x^2}}{(a-x)\sqrt{R^2+a^2-2ax}}\;\mathrm dx$$ My try: Let $x=R\sin{t},\;t\in\left[-\dfrac{\pi}{2},\dfrac{\pi}{2}\right]$   then,   $$I=\int_{-\pi/2}^{\pi/2}\dfrac{R\cos{t}}{(a-R\sin{t})\sqrt{R^2+a^2-2aR\sin{t}}}\cdot R\cos{t}\;\mathrm dt$$   so,   $$I=R^2\int_{-\pi/2}^{\pi/2}\dfrac{\cos^2{t}}{(a-R\sin{t})\sqrt{R^2+a^2-2aR\sin{t}}}\;\mathrm dt$$ Maybe following can use Gamma function? But I can't find it. Thank you someone can help me.","Find the integral: $$I=\int_{-R}^{R}\dfrac{\sqrt{R^2-x^2}}{(a-x)\sqrt{R^2+a^2-2ax}}\;\mathrm dx$$ My try: Let $x=R\sin{t},\;t\in\left[-\dfrac{\pi}{2},\dfrac{\pi}{2}\right]$   then,   $$I=\int_{-\pi/2}^{\pi/2}\dfrac{R\cos{t}}{(a-R\sin{t})\sqrt{R^2+a^2-2aR\sin{t}}}\cdot R\cos{t}\;\mathrm dt$$   so,   $$I=R^2\int_{-\pi/2}^{\pi/2}\dfrac{\cos^2{t}}{(a-R\sin{t})\sqrt{R^2+a^2-2aR\sin{t}}}\;\mathrm dt$$ Maybe following can use Gamma function? But I can't find it. Thank you someone can help me.",,"['calculus', 'integration', 'definite-integrals']"
12,Derivative $\Delta x$ and $dx$ difference,Derivative  and  difference,\Delta x dx,"This may seems like a dummy question but I need to ask it. Consider the definition of derivative: $$\frac{d}{dx}F(x) = \lim_{\Delta x->0}\frac{F(x+\Delta x) - F(x)}{\Delta x} = f(x)$$ Also: $$f(x)\Delta x = F(x+\Delta x) - F(x) \tag{When $\Delta x$ gets closer to $0$}$$ I can also say that: $$\frac{d}{dx}F(x) = f(x)$$ So: $$dF(x) = f(x)dx$$ but $dF(x)$ can also be seen as $F(x+\Delta x) - F(x) \tag{When $\Delta x$ gets closer to $0$}$ So should $dx$ be considered $\Delta x \tag{When $\Delta x$ gets closer to $0$}$? I think this is wrong because it's the same as saying $\lim_{\Delta x \to 0} \Delta x =dx$ when in true $\lim_{\Delta x \to 0} \Delta x =0$. Or maybe $\Delta x$ already means a change in $x$, so the limit of this change, aproaching infinity is gonna be $dx$. In this case, no problem, but and in cases that people use $h$ instead $\Delta x$? I think i'm consufing it a lot. Sorry...","This may seems like a dummy question but I need to ask it. Consider the definition of derivative: $$\frac{d}{dx}F(x) = \lim_{\Delta x->0}\frac{F(x+\Delta x) - F(x)}{\Delta x} = f(x)$$ Also: $$f(x)\Delta x = F(x+\Delta x) - F(x) \tag{When $\Delta x$ gets closer to $0$}$$ I can also say that: $$\frac{d}{dx}F(x) = f(x)$$ So: $$dF(x) = f(x)dx$$ but $dF(x)$ can also be seen as $F(x+\Delta x) - F(x) \tag{When $\Delta x$ gets closer to $0$}$ So should $dx$ be considered $\Delta x \tag{When $\Delta x$ gets closer to $0$}$? I think this is wrong because it's the same as saying $\lim_{\Delta x \to 0} \Delta x =dx$ when in true $\lim_{\Delta x \to 0} \Delta x =0$. Or maybe $\Delta x$ already means a change in $x$, so the limit of this change, aproaching infinity is gonna be $dx$. In this case, no problem, but and in cases that people use $h$ instead $\Delta x$? I think i'm consufing it a lot. Sorry...",,"['calculus', 'integration', 'notation']"
13,Do there exist solutions for this equation?,Do there exist solutions for this equation?,,"We know that solutions exist for equations of the following variety: $$ye^y=x \iff y=W(x)$$ Where W is the Lambert W function.  We can augment the problem slightly, and ask if there exist solutions for equations of the following form: $y^2e^y=x$  Taking the square root and dividing by 2 on both sides allows us to obtain a form in which we can use Lambert W once more. However, do there exist any solutions for equations of the following form:  $$(y^2+\epsilon)e^y=x$$ Where $\epsilon$ is some constant. Scott and Man have authored a paper on equations of the form $$(y-a)(y-b)+e^y=0$$ and so this may be of some help. Is anyone aware of a paper that shows solutions to this problem?","We know that solutions exist for equations of the following variety: $$ye^y=x \iff y=W(x)$$ Where W is the Lambert W function.  We can augment the problem slightly, and ask if there exist solutions for equations of the following form: $y^2e^y=x$  Taking the square root and dividing by 2 on both sides allows us to obtain a form in which we can use Lambert W once more. However, do there exist any solutions for equations of the following form:  $$(y^2+\epsilon)e^y=x$$ Where $\epsilon$ is some constant. Scott and Man have authored a paper on equations of the form $$(y-a)(y-b)+e^y=0$$ and so this may be of some help. Is anyone aware of a paper that shows solutions to this problem?",,"['calculus', 'analysis', 'numerical-methods']"
14,Riemann sum error and the integral,Riemann sum error and the integral,,"It is a well known, that we have the following approximation error: $$  \left|\int_{a}^{b}f(t)dt-\sum_{i=0}^{n}f\left(\xi_{i}\right)s_{n}\right|<\frac{b-a}{2}s_{n}\cdot\text{max}_{x\in\left[a,b\right]}\left|f'\left(x\right)\right|,$$ where $s_{n}$ is the length of the equidistant decomposition of the interval $\left[a,b\right]$ and $f\in{C^{1}}\left(\left[a,b\right]\right)$. My quesstions are: 1.) How this error estimate can be improved, if $f$ and $f'$ are both Lipschitz continuous? 2.) How such estimates look like, if $f$ is a bivariate function? Best regards Lucas","It is a well known, that we have the following approximation error: $$  \left|\int_{a}^{b}f(t)dt-\sum_{i=0}^{n}f\left(\xi_{i}\right)s_{n}\right|<\frac{b-a}{2}s_{n}\cdot\text{max}_{x\in\left[a,b\right]}\left|f'\left(x\right)\right|,$$ where $s_{n}$ is the length of the equidistant decomposition of the interval $\left[a,b\right]$ and $f\in{C^{1}}\left(\left[a,b\right]\right)$. My quesstions are: 1.) How this error estimate can be improved, if $f$ and $f'$ are both Lipschitz continuous? 2.) How such estimates look like, if $f$ is a bivariate function? Best regards Lucas",,"['calculus', 'integration', 'numerical-methods']"
15,Can a curve be an asymptote?,Can a curve be an asymptote?,,"$f(x)=x^3+\frac{3}{x-1}$ This was the question given to me. I replied that $f(x)$ will have only a single vertical asymptote of $x=1$. My teacher told that there'll be be two asymptotes. One is the vertical one($x=1$) and another is the curve $y=x^3$. I checked on the Internet and except for Mathworld , which includes curve in its definition of asymptotes, every other site defines asymptote as a line. Can a curve be an asymptote?","$f(x)=x^3+\frac{3}{x-1}$ This was the question given to me. I replied that $f(x)$ will have only a single vertical asymptote of $x=1$. My teacher told that there'll be be two asymptotes. One is the vertical one($x=1$) and another is the curve $y=x^3$. I checked on the Internet and except for Mathworld , which includes curve in its definition of asymptotes, every other site defines asymptote as a line. Can a curve be an asymptote?",,"['calculus', 'limits', 'asymptotics', 'infinity']"
16,"Existence of solution of ODE $y^{\prime}=f(y,t)$ where $f(y,t)$ is not defined in initial value.",Existence of solution of ODE  where  is not defined in initial value.,"y^{\prime}=f(y,t) f(y,t)","Consider a differential separable equation $$y^{\prime}=f(y,t)$$ with initial solution $y(t_0)=y_0$. Suppose that $f(y_0,t_0)$ is not defined. Is there a theorem which can be used to prove the existence and the uniqueness of the solution of this Differential Equation? The trouble is because $f(y_0,t_0)$ is not defined (much worse than discontinuous where we can still use Carathéodory's existence theorem) For example a separable differential equation $y^{\prime}=\frac{1}{y-1}+2$ with initial solution $y(0)=1$.","Consider a differential separable equation $$y^{\prime}=f(y,t)$$ with initial solution $y(t_0)=y_0$. Suppose that $f(y_0,t_0)$ is not defined. Is there a theorem which can be used to prove the existence and the uniqueness of the solution of this Differential Equation? The trouble is because $f(y_0,t_0)$ is not defined (much worse than discontinuous where we can still use Carathéodory's existence theorem) For example a separable differential equation $y^{\prime}=\frac{1}{y-1}+2$ with initial solution $y(0)=1$.",,"['calculus', 'analysis', 'ordinary-differential-equations']"
17,What is the relation of $\int f dx^1\wedge dx^2\wedge ...\wedge dx^n=\int f dx^1...dx^n$,What is the relation of,\int f dx^1\wedge dx^2\wedge ...\wedge dx^n=\int f dx^1...dx^n,"In a book ""calculus on manifolds"" it is defined that $\int f dx^1\wedge dx^2\wedge ...\wedge dx^n=\int f dx^1...dx^n$ but how it is possible the relate the integrand of a multilinear function (n-differential form) with the remann integral. when i am learning remann integration, i considered $dx$ as an infinitestimal distance although the lecturer didn't explain isolately what $dx$ is. I don't quite see the relationship between remann integral and tensors","In a book ""calculus on manifolds"" it is defined that $\int f dx^1\wedge dx^2\wedge ...\wedge dx^n=\int f dx^1...dx^n$ but how it is possible the relate the integrand of a multilinear function (n-differential form) with the remann integral. when i am learning remann integration, i considered $dx$ as an infinitestimal distance although the lecturer didn't explain isolately what $dx$ is. I don't quite see the relationship between remann integral and tensors",,"['calculus', 'analysis', 'integration', 'differential-forms']"
18,A question regarding representation of a function as a power series,A question regarding representation of a function as a power series,,"I'm trying to help my brother with a calculus problem related to the representation of a function as a power series. The task is to find what power series is represented by the following function, and what is its interval of convergence: $$f(x)= \frac{4}{4 + x^2} \text{, where the center of the series is }c=0 \text{.} $$ $\textbf{My attempt at a solution:} $ I write $\frac{4}{4+ x^2}= \frac{1}{1+ x^2/4}$. Now I know that $\sum_{n=0}^\infty ar^n= \frac{a}{1-r}$, so in our case I believe we have $a=1, r=-\frac{x^2}{4}$. From this I conclude that $$\frac{4}{4+ x^2}= \sum_{n=0}^\infty \left(-\frac{x^2}{4}\right)^n \text{.}$$ This series converges for $|-\frac{x^2}{4}| < 1$, implying that $|x^2|<4$, and thus $|x|<2$, right? If this is correct, then that means the radius of convergence for this series is $R= 2$. I checked that this series diverges for $x= -2,2$. Therefore, we get that the interval of convergence for this series is the open interval $(-2,2)$. Was this the correct approach to be using? Please let me know if there are any mistakes, thanks.","I'm trying to help my brother with a calculus problem related to the representation of a function as a power series. The task is to find what power series is represented by the following function, and what is its interval of convergence: $$f(x)= \frac{4}{4 + x^2} \text{, where the center of the series is }c=0 \text{.} $$ $\textbf{My attempt at a solution:} $ I write $\frac{4}{4+ x^2}= \frac{1}{1+ x^2/4}$. Now I know that $\sum_{n=0}^\infty ar^n= \frac{a}{1-r}$, so in our case I believe we have $a=1, r=-\frac{x^2}{4}$. From this I conclude that $$\frac{4}{4+ x^2}= \sum_{n=0}^\infty \left(-\frac{x^2}{4}\right)^n \text{.}$$ This series converges for $|-\frac{x^2}{4}| < 1$, implying that $|x^2|<4$, and thus $|x|<2$, right? If this is correct, then that means the radius of convergence for this series is $R= 2$. I checked that this series diverges for $x= -2,2$. Therefore, we get that the interval of convergence for this series is the open interval $(-2,2)$. Was this the correct approach to be using? Please let me know if there are any mistakes, thanks.",,"['calculus', 'power-series']"
19,integral of the product of a trigonometric and an exponential function,integral of the product of a trigonometric and an exponential function,,"Since tan has an odd power I would normally aim to sub $u=\sec(x)$, but I cant get rid of the $2^x$. $$\int 2^x \tan^9(x^2)\sec(x^2)dx$$ I also tried integrating by parts but it got more complicated.","Since tan has an odd power I would normally aim to sub $u=\sec(x)$, but I cant get rid of the $2^x$. $$\int 2^x \tan^9(x^2)\sec(x^2)dx$$ I also tried integrating by parts but it got more complicated.",,"['calculus', 'integration', 'trigonometry', 'exponential-function', 'indefinite-integrals']"
20,Riemann Sum Optimization,Riemann Sum Optimization,,"At the moment, I am taking a calculus course at my high school (we are starting to learn about integrals) and thought of an interesting problem while learning about Riemann Sums. Once I develop a method on how to solve this problem for any general curve, I hope to write a script in python. Problem Consider the following parameters: $y = f(x) = (x - 1.5)^{1/3} + 2$ Domain: $[-2, 6]$ $n = 4$ The width of each subdivision, $\Delta w_k$, does not have to be equal. Using left-hand endpoints, right-hand endpoints, midpoints, or a combination of all three, how could you orient the rectangles to cover the greatest area underneath the curve? Ideally, the intended python script I want to write would involve an algorithm that can be applied to any particular curve, any specific domain interval, and any number of subdivisions. How would you even approach this problem? What steps would you outline? Graph of $y = f(x) = (x - 1.5)^{1/3} + 2$ Same Graph with Varying Widths This is an example that involves the midpoint approximation with varying widths ($\Delta w_k$).","At the moment, I am taking a calculus course at my high school (we are starting to learn about integrals) and thought of an interesting problem while learning about Riemann Sums. Once I develop a method on how to solve this problem for any general curve, I hope to write a script in python. Problem Consider the following parameters: $y = f(x) = (x - 1.5)^{1/3} + 2$ Domain: $[-2, 6]$ $n = 4$ The width of each subdivision, $\Delta w_k$, does not have to be equal. Using left-hand endpoints, right-hand endpoints, midpoints, or a combination of all three, how could you orient the rectangles to cover the greatest area underneath the curve? Ideally, the intended python script I want to write would involve an algorithm that can be applied to any particular curve, any specific domain interval, and any number of subdivisions. How would you even approach this problem? What steps would you outline? Graph of $y = f(x) = (x - 1.5)^{1/3} + 2$ Same Graph with Varying Widths This is an example that involves the midpoint approximation with varying widths ($\Delta w_k$).",,"['calculus', 'optimization']"
21,Is this possible,Is this possible,,"If $r_1, r_2, t_1,$ and $t_2$ are real numbers and if $\left|r_{1}\right|<\left|r_{2}\right|$, $\left|t_{1}\right|<\left|t_{2}\right|$ and  $$ \left|\left(r_{1}-r_{2}\right)\left(t_{1}-t_{2}\right)\right|>\left|\left(r_{1}+r_{2}\right)\left(t_{1}+t_{2}\right)-2\left(r_{1}r_{2}+t_{1}t_{2}\right)\right|, $$ does it hold that  $ \left|\left(r_{1}+r_{2}\right)-\left(t_{1}+t_{2}\right)\right|\ge\left|\left|r_{1}\right|-\left|t_{1}\right|\right|+\left|\left|r_{2}\right|-\left|t_{2}\right|\right|? $ Thanks in advance.","If $r_1, r_2, t_1,$ and $t_2$ are real numbers and if $\left|r_{1}\right|<\left|r_{2}\right|$, $\left|t_{1}\right|<\left|t_{2}\right|$ and  $$ \left|\left(r_{1}-r_{2}\right)\left(t_{1}-t_{2}\right)\right|>\left|\left(r_{1}+r_{2}\right)\left(t_{1}+t_{2}\right)-2\left(r_{1}r_{2}+t_{1}t_{2}\right)\right|, $$ does it hold that  $ \left|\left(r_{1}+r_{2}\right)-\left(t_{1}+t_{2}\right)\right|\ge\left|\left|r_{1}\right|-\left|t_{1}\right|\right|+\left|\left|r_{2}\right|-\left|t_{2}\right|\right|? $ Thanks in advance.",,['calculus']
22,Minimizing the maximum of some functions,Minimizing the maximum of some functions,,"I have a function defined as $f(x, y)_i = \frac{\sqrt{(x_i - x)^2 + (y_i - y)^2}}{S_i}$ Let's say I have $n$ such functions $f(x, y)_1, f(x, y)_2$ ... $f(x, y)_n$ . I have to minimize the maximum of these functions by choosing appropriate values of $x$ and $y$ . How to do this?",I have a function defined as Let's say I have such functions ... . I have to minimize the maximum of these functions by choosing appropriate values of and . How to do this?,"f(x, y)_i = \frac{\sqrt{(x_i - x)^2 + (y_i - y)^2}}{S_i} n f(x, y)_1, f(x, y)_2 f(x, y)_n x y","['calculus', 'optimization']"
23,Generators of an ideal in a ring of real valued functions,Generators of an ideal in a ring of real valued functions,,"It is part of an exercise in the book Basic Homological Algebra, Chapter 2 Exercise 16. Suppose $R$ is the subring of $C^\infty(\mathbb{R})$ of all functions with period $2\pi$, and let $I$ be the maximal ideal of $R$ consisting of all functions of $R$ taking $0$ to $0$. How can I prove that $I$ is generated by $\sin(x)$ and any (one) function in $I$ which take nonzero value at $\pi$?","It is part of an exercise in the book Basic Homological Algebra, Chapter 2 Exercise 16. Suppose $R$ is the subring of $C^\infty(\mathbb{R})$ of all functions with period $2\pi$, and let $I$ be the maximal ideal of $R$ consisting of all functions of $R$ taking $0$ to $0$. How can I prove that $I$ is generated by $\sin(x)$ and any (one) function in $I$ which take nonzero value at $\pi$?",,"['calculus', 'ring-theory']"
24,Proving that a particular kind of multiple integral yields a real number.,Proving that a particular kind of multiple integral yields a real number.,,"I encountered the following problem on a practice exam: Suppose $a_1,...,a_n>0$ are such that $$\sum_{k=1}^n\frac{1}{a_k}<1.$$ Then $$\int_1^\infty\cdots\int_1^\infty\frac{1}{x_1^{a_1}+\cdots+x_n^{a_n}}\,dx_1\cdots dx_n<\infty.$$ Now, this is not tricky for $n=1$, as $\frac{1}{a_1}<1$ iff $a_1>1$ in this circumstance, and so convergence is guaranteed by the convergence of $\sum_{k=1}^\infty k^{-a_1},$ but I'm not sure how to proceed for other $n$. I have a few thoughts: (1) Actually calculate the antiderivative at each stage, and apply methods of improper integrals where necessary. This seems like it would be more hectic than it's worth. (2) Find an upper bounding integrand that is more friendly, and show that that integral evaluates to a real number. This one seems like it could be doable, if I can find such a bounding integrand. My attempts so far have either failed to evaluate to a real number or failed to be a bounding integrand. (3) Generalize the integral test and the result that $\sum_{k=1}^\infty k^{-p}$ is a real number for $p>1$ in some fashion. This seems like it could be my best bet, but I worry that if I don't phrase them carefully, I may be trying to prove results that are incompatible or not both true. My question is this : Does anyone have any recommendations for how I might proceed (whether it's a hint for one of the approaches I listed above or the seed of an entirely different approach)? Thanks for any help you can give me!","I encountered the following problem on a practice exam: Suppose $a_1,...,a_n>0$ are such that $$\sum_{k=1}^n\frac{1}{a_k}<1.$$ Then $$\int_1^\infty\cdots\int_1^\infty\frac{1}{x_1^{a_1}+\cdots+x_n^{a_n}}\,dx_1\cdots dx_n<\infty.$$ Now, this is not tricky for $n=1$, as $\frac{1}{a_1}<1$ iff $a_1>1$ in this circumstance, and so convergence is guaranteed by the convergence of $\sum_{k=1}^\infty k^{-a_1},$ but I'm not sure how to proceed for other $n$. I have a few thoughts: (1) Actually calculate the antiderivative at each stage, and apply methods of improper integrals where necessary. This seems like it would be more hectic than it's worth. (2) Find an upper bounding integrand that is more friendly, and show that that integral evaluates to a real number. This one seems like it could be doable, if I can find such a bounding integrand. My attempts so far have either failed to evaluate to a real number or failed to be a bounding integrand. (3) Generalize the integral test and the result that $\sum_{k=1}^\infty k^{-p}$ is a real number for $p>1$ in some fashion. This seems like it could be my best bet, but I worry that if I don't phrase them carefully, I may be trying to prove results that are incompatible or not both true. My question is this : Does anyone have any recommendations for how I might proceed (whether it's a hint for one of the approaches I listed above or the seed of an entirely different approach)? Thanks for any help you can give me!",,"['calculus', 'integration']"
25,A trigonometric-integral inequality,A trigonometric-integral inequality,,"This problem comes from a discussion with one of my friends: Prove that: $$\displaystyle \lim_{n \to \infty}\int_{1}^{n}{\sin (x)\sin(x^2)}\,{\mathrm dx}< \lim_{n \to \infty}\int_{1}^{n}{\sin(x^2)}\,{\mathrm dx}$$ I wonder if there is a reasonable way to somehow solve such a problem. Thanks.","This problem comes from a discussion with one of my friends: Prove that: $$\displaystyle \lim_{n \to \infty}\int_{1}^{n}{\sin (x)\sin(x^2)}\,{\mathrm dx}< \lim_{n \to \infty}\int_{1}^{n}{\sin(x^2)}\,{\mathrm dx}$$ I wonder if there is a reasonable way to somehow solve such a problem. Thanks.",,"['calculus', 'limits', 'inequality', 'integral-inequality']"
26,"If $f$ and $g$  are continuous and  for every $q\in \mathbb{Q}$ we have $f(q)=g(q)$, then $f(x)=g(x)$ for every $x\in \mathbb{R}$ [duplicate]","If  and   are continuous and  for every  we have , then  for every  [duplicate]",f g q\in \mathbb{Q} f(q)=g(q) f(x)=g(x) x\in \mathbb{R},"This question already has answers here : Closed 13 years ago . Possible Duplicate: Can there be two distinct, continuous functions that are equal at all rationals? Hello guys, Let $f$ and $g$ be continuous functions, $f,g:\mathbb{R} \to \mathbb{R}$, such that for every $q\in \mathbb{Q}$ we have $f(q)=g(q)$. I need to prove that $f(x)=g(x)$ for every $x\in \mathbb{R}$. I think I should prove that with sequences. We can choose a $x\in \mathbb{R}$, and we know that there is a sequence of rational numbers whose limit is $x$. Let's call it $X_{n}$, so $\lim f(X_{n})=\lim g(X_{n})$, when $n \to \infty $, and we get what we want. Is it correct? What do you think?","This question already has answers here : Closed 13 years ago . Possible Duplicate: Can there be two distinct, continuous functions that are equal at all rationals? Hello guys, Let $f$ and $g$ be continuous functions, $f,g:\mathbb{R} \to \mathbb{R}$, such that for every $q\in \mathbb{Q}$ we have $f(q)=g(q)$. I need to prove that $f(x)=g(x)$ for every $x\in \mathbb{R}$. I think I should prove that with sequences. We can choose a $x\in \mathbb{R}$, and we know that there is a sequence of rational numbers whose limit is $x$. Let's call it $X_{n}$, so $\lim f(X_{n})=\lim g(X_{n})$, when $n \to \infty $, and we get what we want. Is it correct? What do you think?",,[]
27,On deriving the arclength of a hyperbola,On deriving the arclength of a hyperbola,,"In my attempts to derive the closed form for the arclength of the hyperbola, I wound up with the following integral: $$\int\frac{\sqrt{1-m\;\sin^2 u}}{\sin^2 u}\mathrm{d}u$$ I am aware that such integrals are expressible as combinations of elliptic integrals and trigonometric functions, and Mathematica does return a result containing elliptic integrals. However, the result I got from Mathematica seems to suggest that the original integrand was split like so: $$\frac{\sqrt{1-m\;\sin^2 u}}{\sin^2 u}=\frac{1-m}{\sqrt{1-m\;\sin^2 u}}-\sqrt{1-m\;\sin^2 u}+\frac{\csc^2u-m\;\sin^2 u}{\sqrt{1-m\;\sin^2 u}}$$ and then integrated to yield the expression containing the elliptic integrals. What I'm stuck with is how this splitting was thought of so that the final integral is expressible in terms of the Legendre elliptic integrals. A possible clue lay in the fact that through an appropriate substitution, the original integral can be expressed as $$\int\mathrm{ds}^2(v|m)\mathrm{d}v$$ where $\mathrm{ds}(v|m)=\frac{\mathrm{dn}(v|m)}{\mathrm{sn}(v|m)}$ is a Jacobian elliptic function, from which this formula applies. However, I don't know how the formula listed in the DLMF was derived, and I suspect that the answer to my question will also explain that formula. I would appreciate any help in understanding why the integrand was split in that manner. Update : Trivially, $$\mathrm{ds}^2(v|m)=\mathrm{ns}^2(v|m)-m=1-m+\mathrm{cs}^2(v|m)$$ The elliptic integral of the second kind is supposed to arise due to the fact that $$\int\mathrm{dn}^2(v|m)\mathrm{d}v$$ $$=\int\mathrm{dn}(v|m)\mathrm{d}(\mathrm{am}(v|m))$$ $$=\int\sqrt{1-m\;\mathrm{sn}^2(v|m)}\mathrm{d}(\mathrm{am}(v|m))$$ $$=\int\sqrt{1-m\;\sin^2(\mathrm{am}(v|m))}\mathrm{d}(\mathrm{am}(v|m))$$ $$=E(\mathrm{am}(v|m)|m)$$ so I suppose figuring out how to integrate the squares of any of $\mathrm{cs}$, $\mathrm{ns}$, or $\mathrm{ds}$ would do the trick. I suspect something like integration by parts or tricks similar to those used for integrating ratios of trigonometric functions would work here, but I can't seem to find the proper strategy. Yet another way would be to show that $$-\int\left(\mathrm{cs}^2(v|m)+\mathrm{dn}^2(v|m)\right)\mathrm{d}v=\mathrm{cs}(v|m)\mathrm{dn}(v|m)$$ Differentiating the RHS gives the integrand, but supposing I did not know this expression, how would this integral be evaluated?","In my attempts to derive the closed form for the arclength of the hyperbola, I wound up with the following integral: $$\int\frac{\sqrt{1-m\;\sin^2 u}}{\sin^2 u}\mathrm{d}u$$ I am aware that such integrals are expressible as combinations of elliptic integrals and trigonometric functions, and Mathematica does return a result containing elliptic integrals. However, the result I got from Mathematica seems to suggest that the original integrand was split like so: $$\frac{\sqrt{1-m\;\sin^2 u}}{\sin^2 u}=\frac{1-m}{\sqrt{1-m\;\sin^2 u}}-\sqrt{1-m\;\sin^2 u}+\frac{\csc^2u-m\;\sin^2 u}{\sqrt{1-m\;\sin^2 u}}$$ and then integrated to yield the expression containing the elliptic integrals. What I'm stuck with is how this splitting was thought of so that the final integral is expressible in terms of the Legendre elliptic integrals. A possible clue lay in the fact that through an appropriate substitution, the original integral can be expressed as $$\int\mathrm{ds}^2(v|m)\mathrm{d}v$$ where $\mathrm{ds}(v|m)=\frac{\mathrm{dn}(v|m)}{\mathrm{sn}(v|m)}$ is a Jacobian elliptic function, from which this formula applies. However, I don't know how the formula listed in the DLMF was derived, and I suspect that the answer to my question will also explain that formula. I would appreciate any help in understanding why the integrand was split in that manner. Update : Trivially, $$\mathrm{ds}^2(v|m)=\mathrm{ns}^2(v|m)-m=1-m+\mathrm{cs}^2(v|m)$$ The elliptic integral of the second kind is supposed to arise due to the fact that $$\int\mathrm{dn}^2(v|m)\mathrm{d}v$$ $$=\int\mathrm{dn}(v|m)\mathrm{d}(\mathrm{am}(v|m))$$ $$=\int\sqrt{1-m\;\mathrm{sn}^2(v|m)}\mathrm{d}(\mathrm{am}(v|m))$$ $$=\int\sqrt{1-m\;\sin^2(\mathrm{am}(v|m))}\mathrm{d}(\mathrm{am}(v|m))$$ $$=E(\mathrm{am}(v|m)|m)$$ so I suppose figuring out how to integrate the squares of any of $\mathrm{cs}$, $\mathrm{ns}$, or $\mathrm{ds}$ would do the trick. I suspect something like integration by parts or tricks similar to those used for integrating ratios of trigonometric functions would work here, but I can't seem to find the proper strategy. Yet another way would be to show that $$-\int\left(\mathrm{cs}^2(v|m)+\mathrm{dn}^2(v|m)\right)\mathrm{d}v=\mathrm{cs}(v|m)\mathrm{dn}(v|m)$$ Differentiating the RHS gives the integrand, but supposing I did not know this expression, how would this integral be evaluated?",,"['calculus', 'special-functions', 'plane-curves', 'conic-sections']"
28,"Please help me identify any errors in my solution to the following DE: $xf(x)-f'(x)=0$, $f(0)=1$","Please help me identify any errors in my solution to the following DE: ,",xf(x)-f'(x)=0 f(0)=1,"Context/background: I am self-studying series, first in the context of generating functions and now in the context of functional/differential equations. As such, I like to set myself practise problems, which is what I bring you today. Of course, being self-studying, it can be hard to find anyone to give feedback on my problems and solutions, which is why I bring this one to you today. Thanks for your time. Problem statement: Given the differential equation $xf(x)-f'(x)=0$ and $f(0)=1$ , find $f(x)$ in power series form. Bonus questions: find the closed form of $f(x)$ (if it exists) and the radius of convergence of the power series. Solution: Let $f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4+...$ Then: $xf(x)=\sum_{n=0}^{\infty}a_nx^{n+1}=a_0x+a_1x^2+a_2x^3+a_3x^4+...$ And: $f'(x)=\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n=a_1+2a_2x+3a_3x^2+4a_4x^3+5a_5x^4+...$ Furthermore: $xf(x)-f'(x)=-a_1+(a_0-2a_2)x+(a_1-3a_3)x^2+(a_2-4a_4)x^3+(a_3-5a_5)x^4+...$ Given that $f(0)=1$ , we must have that $a_0=1$ . Moreover, since the coefficients of $xf(x)-f'(x)$ must all be zero, we have the following (infinite) system of equations: $$-a_1=0\Longrightarrow a_1=0$$ $$a_0-2a_2=0\Longrightarrow a_2=\frac{a_0}{2}$$ $$a_1-3a_3=0\Longrightarrow a_3=0$$ $$a_2-4a_4=0\Longrightarrow a_4=\frac{a_2}{4}=\frac{a_0}{8}$$ $$a_3-5a_5=0\Longrightarrow a_5=0$$ $$a_4-6a_6=0\Longrightarrow a_6=\frac{a_4}{6}=\frac{a_0}{48}$$ The pattern continues, with every odd coefficient equal to $0$ and every odd coefficient equal to $a_0$ divided by the product of every even number up to and including $n$ . As such, it makes sense to define a new sequence of coefficients $c_n$ such that $c_n=a_{2n}$ ; then $c_n=\frac{a_0}{(2n)!!}=\frac{1}{(2n)!!}$ and the power series solution is $f(x)=\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!!}=1+\frac{x^2}{2}+\frac{x^4}{8}+\frac{x^6}{48}+...$ Radius of convergence Consider $\lim_{n\to\infty}|\frac{c_{n+1}}{c_n}|$ . This is equal to $\lim_{n\to\infty}|\frac{\frac{1}{(2n+2)!!}}{\frac{1}{(2n)!!}}|=\lim_{n\to\infty}|\frac{(2n)!!}{(2n+2)!!}|=\lim_{n\to\infty}|\frac{2n(2n-2)(2n-4)...2}{(2n+2)(2n)(2n-2)(2n-4)...2}|=\lim_{n\to\infty}|\frac{1}{2n+2}|=0$ Since the limit of the ratio of consecutive coefficients is $0$ , the radius of convergence for this series is infinite; i.e. it converges for all $x$ . Closed form solution I'll admit, this one stumped me for a bit. Then I realised that $(2n)!!=2^nn!$ , so we have $\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!!}=\sum_{n=0}^{\infty}\frac{x^{2n}}{2^nn!}$ . Setting $t=\frac{x^2}{2}$ , we have $\sum_{n=0}^{\infty}\frac{x^{2n}}{2^nn!}=\sum_{n=0}^{\infty}\frac{t^n}{n!}$ . This last series has the very well-known closed form of $e^t$ , so the closed form for our series solution (in $x$ ) is $f(x)=e^{\frac{x^2}{2}}$ . To check said solution: $xf(x)=xe^{\frac{x^2}{2}}$ , and $f'(x)=xe^{\frac{x^2}{2}}$ , so $xf(x)-f'(x)=xe^{\frac{x^2}{2}}-xe^{\frac{x^2}{2}}=0$ . My question: As I mentioned earlier, I'm a self-studier, so it can be hard to find (qualified) feedback. So if there's any you could offer me in regards to my solution — in terms of rigour, flow, leaps of logic, whatever you can think of — I'd really appreciate it. I want to improve!","Context/background: I am self-studying series, first in the context of generating functions and now in the context of functional/differential equations. As such, I like to set myself practise problems, which is what I bring you today. Of course, being self-studying, it can be hard to find anyone to give feedback on my problems and solutions, which is why I bring this one to you today. Thanks for your time. Problem statement: Given the differential equation and , find in power series form. Bonus questions: find the closed form of (if it exists) and the radius of convergence of the power series. Solution: Let Then: And: Furthermore: Given that , we must have that . Moreover, since the coefficients of must all be zero, we have the following (infinite) system of equations: The pattern continues, with every odd coefficient equal to and every odd coefficient equal to divided by the product of every even number up to and including . As such, it makes sense to define a new sequence of coefficients such that ; then and the power series solution is Radius of convergence Consider . This is equal to Since the limit of the ratio of consecutive coefficients is , the radius of convergence for this series is infinite; i.e. it converges for all . Closed form solution I'll admit, this one stumped me for a bit. Then I realised that , so we have . Setting , we have . This last series has the very well-known closed form of , so the closed form for our series solution (in ) is . To check said solution: , and , so . My question: As I mentioned earlier, I'm a self-studier, so it can be hard to find (qualified) feedback. So if there's any you could offer me in regards to my solution — in terms of rigour, flow, leaps of logic, whatever you can think of — I'd really appreciate it. I want to improve!",xf(x)-f'(x)=0 f(0)=1 f(x) f(x) f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4+... xf(x)=\sum_{n=0}^{\infty}a_nx^{n+1}=a_0x+a_1x^2+a_2x^3+a_3x^4+... f'(x)=\sum_{n=0}^{\infty}(n+1)a_{n+1}x^n=a_1+2a_2x+3a_3x^2+4a_4x^3+5a_5x^4+... xf(x)-f'(x)=-a_1+(a_0-2a_2)x+(a_1-3a_3)x^2+(a_2-4a_4)x^3+(a_3-5a_5)x^4+... f(0)=1 a_0=1 xf(x)-f'(x) -a_1=0\Longrightarrow a_1=0 a_0-2a_2=0\Longrightarrow a_2=\frac{a_0}{2} a_1-3a_3=0\Longrightarrow a_3=0 a_2-4a_4=0\Longrightarrow a_4=\frac{a_2}{4}=\frac{a_0}{8} a_3-5a_5=0\Longrightarrow a_5=0 a_4-6a_6=0\Longrightarrow a_6=\frac{a_4}{6}=\frac{a_0}{48} 0 a_0 n c_n c_n=a_{2n} c_n=\frac{a_0}{(2n)!!}=\frac{1}{(2n)!!} f(x)=\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!!}=1+\frac{x^2}{2}+\frac{x^4}{8}+\frac{x^6}{48}+... \lim_{n\to\infty}|\frac{c_{n+1}}{c_n}| \lim_{n\to\infty}|\frac{\frac{1}{(2n+2)!!}}{\frac{1}{(2n)!!}}|=\lim_{n\to\infty}|\frac{(2n)!!}{(2n+2)!!}|=\lim_{n\to\infty}|\frac{2n(2n-2)(2n-4)...2}{(2n+2)(2n)(2n-2)(2n-4)...2}|=\lim_{n\to\infty}|\frac{1}{2n+2}|=0 0 x (2n)!!=2^nn! \sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!!}=\sum_{n=0}^{\infty}\frac{x^{2n}}{2^nn!} t=\frac{x^2}{2} \sum_{n=0}^{\infty}\frac{x^{2n}}{2^nn!}=\sum_{n=0}^{\infty}\frac{t^n}{n!} e^t x f(x)=e^{\frac{x^2}{2}} xf(x)=xe^{\frac{x^2}{2}} f'(x)=xe^{\frac{x^2}{2}} xf(x)-f'(x)=xe^{\frac{x^2}{2}}-xe^{\frac{x^2}{2}}=0,"['calculus', 'power-series', 'closed-form']"
29,Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$,Evaluating,\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!},"I'm supposed to calculate: $$\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!}$$ By using WolframAlpha, I might guess that the limit is $\frac{1}{2}$ , which is a pretty interesting and nice result. I wonder in which ways we may approach it.","I'm supposed to calculate: By using WolframAlpha, I might guess that the limit is , which is a pretty interesting and nice result. I wonder in which ways we may approach it.",\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} \frac{1}{2},"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
30,Is There Any Meaning To This Operator? [closed],Is There Any Meaning To This Operator? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question A while ago I was thinking about how in physics there is velocity $\frac{d}{dt}x(t)$ being the gradient of a displacement-time function $x(t)$ , and  angular velocity $\frac{d\theta}{dt}$ being the gradient of an angular displacement-time function $\theta(t)$ , and how in mathematics no one seems to define the gradient of the angle of a tangent line of some function at some point $x$ . Perhaps this is for a reason as it might just not be that interesting, but I went ahead and came up with the following: $$\frac{dy}{dx}\mid_{x=x} = \tan\theta(x)$$ We define the operator $\hat{\theta}$ as $$\hat{\theta}[f] = \frac{d}{dx}\arctan\left(\frac{df}{dx}\mid_{x=x}\right)$$ An interesting result is for example, $$\hat{\theta}[\ln x] = \frac{d}{dx}\text{arccot}\,x$$ Which can be thought meaningfully as the rate of change with respect to $x$ of the angle of the tangent at a point of $\ln x$ is the gradient of $\text{arccot}\,x$ . This is interesting but quite obvious if one were to calculate it. Another one is that if $f'' = 1$ then $$\text{arc length} = \int_a^b\frac{dx}{\sqrt{\hat{\theta}[f]}}$$ There are also interesting differential equations one could explore with $$\hat{\theta}[y] = \frac{y''}{(y')^2+1}$$ Looking at invariants like $$\hat{\theta}[y] = y$$ or perhaps $$\hat{\theta}[y] = y'$$ These are for the most part the only interesting things I've been able to find regarding $\hat{\theta}$ , and so I'm wondering if anyone can come up with anything interesting , or provide insights, or perhaps even reference a paper or something where this is used. Or perhaps if it is completely useless then one could say that too if they wanted. Edit: To clarify, I know that there is a difference between a derivative and a gradient. The first section was just to explain where the idea came from","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question A while ago I was thinking about how in physics there is velocity being the gradient of a displacement-time function , and  angular velocity being the gradient of an angular displacement-time function , and how in mathematics no one seems to define the gradient of the angle of a tangent line of some function at some point . Perhaps this is for a reason as it might just not be that interesting, but I went ahead and came up with the following: We define the operator as An interesting result is for example, Which can be thought meaningfully as the rate of change with respect to of the angle of the tangent at a point of is the gradient of . This is interesting but quite obvious if one were to calculate it. Another one is that if then There are also interesting differential equations one could explore with Looking at invariants like or perhaps These are for the most part the only interesting things I've been able to find regarding , and so I'm wondering if anyone can come up with anything interesting , or provide insights, or perhaps even reference a paper or something where this is used. Or perhaps if it is completely useless then one could say that too if they wanted. Edit: To clarify, I know that there is a difference between a derivative and a gradient. The first section was just to explain where the idea came from","\frac{d}{dt}x(t) x(t) \frac{d\theta}{dt} \theta(t) x \frac{dy}{dx}\mid_{x=x} = \tan\theta(x) \hat{\theta} \hat{\theta}[f] = \frac{d}{dx}\arctan\left(\frac{df}{dx}\mid_{x=x}\right) \hat{\theta}[\ln x] = \frac{d}{dx}\text{arccot}\,x x \ln x \text{arccot}\,x f'' = 1 \text{arc length} = \int_a^b\frac{dx}{\sqrt{\hat{\theta}[f]}} \hat{\theta}[y] = \frac{y''}{(y')^2+1} \hat{\theta}[y] = y \hat{\theta}[y] = y' \hat{\theta}","['calculus', 'ordinary-differential-equations', 'functions', 'derivatives']"
31,Taylor series of $\displaystyle x!_{(\infty)}=\prod_{j=1}^{\infty} j^{\text{sinc}(x-j)}$ - generalization of $\psi(z)$,Taylor series of  - generalization of,\displaystyle x!_{(\infty)}=\prod_{j=1}^{\infty} j^{\text{sinc}(x-j)} \psi(z),"1. Premise I would like to find the closed form of this function: I have the infinite-multifactorial function $f(x)=x!_{(\infty)}$ and I want to calculate its Taylor series. $$f(x)=\prod_{j=1}^{\infty} j^{\text{sinc}(x-j)}\qquad\text{ where}\quad\text{sinc}(z)=\begin{cases}\frac{\sin(\pi z)}{\pi z}&z\neq 0\\ 1&z=0\end{cases}$$ After various calculations that I am not here to write (they are very long and repetitive), I have arrived at this formulation: $$f'(x)=f(x)\cdot\underbrace{\sum_{j=1}^{\infty}\ln(j)\text{sinc}(x-j)}_{=: g(x)}$$ and applying the Leibnitz rule $$f^{(n)}(x)=\frac{\mathrm{d}^{n-1}}{\mathrm{d}x^{n-1}}f(x)g(x)=\sum_{k=0}^{n-1}\binom{n-1}{k}f^{(k)}(x)g^{(n-k-1)}(x)$$ I get that: $$f^{(n)}(0)=\sum_{k=0}^{n-1}\binom{n-1}{k}f^{(k)}(0)g^{(n-k-1)}(0)$$ $f^{(n)}$ is obtained iteratively, while $$g^{(n)}(0)=\sum_{j=1}^{\infty}\ln(j)\text{sinc}^{(n)}(-j)=\sum_{k=0}^{\left\lfloor\frac{n-1}{2}\right\rfloor}\frac{(-1)^k\pi^{2k}}{2k+1}\binom{n}{2k} b_{n-2k}$$ $$g^{(n)}(0)=b_n+\sum_{k=1}^{\left\lfloor\frac{n-1}{2}\right\rfloor}\frac{(-1)^k\pi^{2k}}{2k+1}\binom{n}{2k} b_{n-2k}$$ Where: $$b_n=-n!\cdot \eta'(n)=\begin{cases} \dfrac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma&n=1\\ -n!\cdot \dfrac{(2^{n-1}-1)\zeta'\left(n\right)+\zeta\left(n\right)\ln\left(2\right)}{2^{n-1}}&n\geq 2 \end{cases}$$ Where $\eta(z)$ is the Dirichlet Eta function By writing the series up to grade 5 I obtained this sum: $$\begin{align} f(x)\approx& 1+\frac{b_{1}}{1!}x+\frac{b_{1}^{2}+b_{2}}{2!}x^{2}+\frac{b_{1}^{3}+3b_{1}b_{2}+b_{3}-\pi^{2}b_{1}}{3!}x^{3}\\ +&\frac{\left(b_{1}^{4}+6b_{1}^{2}b_{2}+4b_{1}b_{3}+3b_{2}^{2}+b_{4}\right)-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}\\ +&\frac{{\left(b_{1}^{5}+10b_{1}^{3}b_{2}+15b_{1}b_{2}^{2}+10b_{1}^{2}b_{3}+5b_{1}b_{4}+10b_{2}b_{3}+b_{5}\right)+\left(\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}\right)}}{5!}x^{5} \end{align}$$ It may seem complicated but the part of the coefficients not containing powers of $\pi$ can be written as complete Bell polynomials : $$\begin{align} f(x)\approx&B_0(\{b_i\})+\frac{B_1(\{b_i\})}{1!}x+\frac{B_2(\{b_i\})}{2!}x^{2}+\frac{B_3(\{b_i\})-\pi^{2}b_{1}}{3!}x^{3}\\ +&\frac{B_4(\{b_i\})-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}\\ +&\frac{B_5(\{b_i\})+\left(\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}\right)}{5!}x^{5} \end{align}$$ Where: $B_n(\{b_i\})=B_n(b_1,...,b_n)$ is the $n$ -th complete Bell Polynomial So I separated the first part from the rest: $$f(x)=\sum_{k=0}^{\infty}\frac{B_k(\{b_i\})}{k!}x^k+\frac{-\pi^{2}b_{1}}{3!}x^{3}+\frac{-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}+\frac{\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}}{5!}x^{5}+...$$ My problem now is that I can't understand what the closed formulation of the remaining piece of coefficients could be, can anyone help me? 2. Motivation of interest Using the formula $$\sum _{n=0}^{\infty }{B_{n}(b_{1},\dots ,b_{n}) \over n!}x^{n}=\exp \left(\sum _{i=1}^{\infty }{b_{i} \over i!}x^{i}\right)$$ We have that the part of $f$ in which the first series appears can be written as: $$\sum _{n=0}^{\infty }{B_{n}(\{b_i\}) \over n!}x^{n}=\exp\left[\left(\dfrac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma\right)x-\sum_{n=2}^{\infty} \dfrac{(2^{n-1}-1)\zeta'\left(n\right)+\zeta\left(n\right)\ln\left(2\right)}{2^{n-1}}x^n\right]$$ $$=\exp\left[\left(\frac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma\right)x+\sum_{n=2}^{\infty}\frac{\zeta'\left(n\right)}{2^{n-1}}x^{n}-\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n}-\ln\left(2\right)\sum_{n=2}^{\infty}\frac{\zeta\left(n\right)}{2^{n-1}}x^{n}\right]$$ $$=\exp\left[\frac{\ln\left(2\right)^{2}}{2}x+2\sum_{n=2}^{\infty}\zeta'\left(n\right)\left(\frac{x}{2}\right)^{n}-\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n}+\ln\left(2\right)x\psi\left(1-\frac{x}{2}\right)\right]$$ Here a series involving the derivative of the zeta function appears twice. I would like to find some way to express the series in this way $$\sum_{n=2}^{\infty}\zeta'(n)x^{n-1}=K-\ln(2)f(1-x)$$ so the sum becomes $$\sum _{n=0}^{\infty }{B_{n}(\{b_i\}) \over n!}x^{n}=\exp\left[\ln\left(2\right)x\left(\frac{\ln\left(2\right)}{2}+f\left(1-\frac{x}{2}\right)-f\left(1-x\right)+\psi\left(1-\frac{x}{2}\right)\right)\right]$$ I think there may be connections with the gamma function being $$\sum_{n=2}^{\infty}\zeta(n)x^{n-1}=-\gamma-\psi(1-x)$$ I think it could be interesting that the infinte-multifactorial could be connected with some function deriving from the gamma function. 3. Update I realized I can write the infinite-multifactor function as $$x!_{(\infty)}=\exp\left(\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)x}{x-n}\right)^{\text{sinc}(x)}$$ In this way I only study the function inside the exponential so as not to have that uncomfortable presence of $\pi$ $$\begin{align}\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)x}{x-n}=&\sum_{s=1}^{\infty}x^{s}\left(-\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)}{n^{s}}\right)\\ =&\sum_{s=1}^{\infty}x^{s}\left(\frac{\mathrm{d}}{\mathrm{d}s}\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}}{n^{s}}\right)\\ =&-\sum_{s=1}^{\infty}\eta'(s)x^s\\ =&-\sum_{s=1}^{\infty}\frac{(2^{s-1}-1)\zeta'(s)+\ln(2)\zeta(s)}{2^{s-1}}x^s \end{align}$$ So there is a strong correlation between the infinite-multifactorial and the series $$\begin{align}x!_{(\infty)}=&\exp\left(-\sum_{s=1}^{\infty}\eta'(s)x^s\right)^{\text{sinc}(x)}\\ =&\exp\left(-\sum_{s=1}^{\infty}\dfrac{\ln(2)\zeta(s)+(2^{s-1}-1)\zeta'(s)}{2^{s-1}}x^s\right)^{\text{sinc}(x)}\\ =&\exp\left(\ln(2)\psi\left(1-\frac{x}{2}\right)x+\sum_{s=1}^{\infty}(2^{1-s}-1)\zeta'(s)x^{s}\right)^{\text{sinc}(x)}\end{align}$$ We considered the regularized zeta where $\zeta(1)=\gamma$ and $\eta'(1)=\gamma\ln(2)-\dfrac{\ln(2)^2}{2}$","1. Premise I would like to find the closed form of this function: I have the infinite-multifactorial function and I want to calculate its Taylor series. After various calculations that I am not here to write (they are very long and repetitive), I have arrived at this formulation: and applying the Leibnitz rule I get that: is obtained iteratively, while Where: Where is the Dirichlet Eta function By writing the series up to grade 5 I obtained this sum: It may seem complicated but the part of the coefficients not containing powers of can be written as complete Bell polynomials : Where: is the -th complete Bell Polynomial So I separated the first part from the rest: My problem now is that I can't understand what the closed formulation of the remaining piece of coefficients could be, can anyone help me? 2. Motivation of interest Using the formula We have that the part of in which the first series appears can be written as: Here a series involving the derivative of the zeta function appears twice. I would like to find some way to express the series in this way so the sum becomes I think there may be connections with the gamma function being I think it could be interesting that the infinte-multifactorial could be connected with some function deriving from the gamma function. 3. Update I realized I can write the infinite-multifactor function as In this way I only study the function inside the exponential so as not to have that uncomfortable presence of So there is a strong correlation between the infinite-multifactorial and the series We considered the regularized zeta where and","f(x)=x!_{(\infty)} f(x)=\prod_{j=1}^{\infty} j^{\text{sinc}(x-j)}\qquad\text{ where}\quad\text{sinc}(z)=\begin{cases}\frac{\sin(\pi z)}{\pi z}&z\neq 0\\
1&z=0\end{cases} f'(x)=f(x)\cdot\underbrace{\sum_{j=1}^{\infty}\ln(j)\text{sinc}(x-j)}_{=: g(x)} f^{(n)}(x)=\frac{\mathrm{d}^{n-1}}{\mathrm{d}x^{n-1}}f(x)g(x)=\sum_{k=0}^{n-1}\binom{n-1}{k}f^{(k)}(x)g^{(n-k-1)}(x) f^{(n)}(0)=\sum_{k=0}^{n-1}\binom{n-1}{k}f^{(k)}(0)g^{(n-k-1)}(0) f^{(n)} g^{(n)}(0)=\sum_{j=1}^{\infty}\ln(j)\text{sinc}^{(n)}(-j)=\sum_{k=0}^{\left\lfloor\frac{n-1}{2}\right\rfloor}\frac{(-1)^k\pi^{2k}}{2k+1}\binom{n}{2k} b_{n-2k} g^{(n)}(0)=b_n+\sum_{k=1}^{\left\lfloor\frac{n-1}{2}\right\rfloor}\frac{(-1)^k\pi^{2k}}{2k+1}\binom{n}{2k} b_{n-2k} b_n=-n!\cdot \eta'(n)=\begin{cases}
\dfrac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma&n=1\\
-n!\cdot \dfrac{(2^{n-1}-1)\zeta'\left(n\right)+\zeta\left(n\right)\ln\left(2\right)}{2^{n-1}}&n\geq 2
\end{cases} \eta(z) \begin{align}
f(x)\approx& 1+\frac{b_{1}}{1!}x+\frac{b_{1}^{2}+b_{2}}{2!}x^{2}+\frac{b_{1}^{3}+3b_{1}b_{2}+b_{3}-\pi^{2}b_{1}}{3!}x^{3}\\ +&\frac{\left(b_{1}^{4}+6b_{1}^{2}b_{2}+4b_{1}b_{3}+3b_{2}^{2}+b_{4}\right)-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}\\ +&\frac{{\left(b_{1}^{5}+10b_{1}^{3}b_{2}+15b_{1}b_{2}^{2}+10b_{1}^{2}b_{3}+5b_{1}b_{4}+10b_{2}b_{3}+b_{5}\right)+\left(\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}\right)}}{5!}x^{5}
\end{align} \pi \begin{align}
f(x)\approx&B_0(\{b_i\})+\frac{B_1(\{b_i\})}{1!}x+\frac{B_2(\{b_i\})}{2!}x^{2}+\frac{B_3(\{b_i\})-\pi^{2}b_{1}}{3!}x^{3}\\ +&\frac{B_4(\{b_i\})-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}\\ +&\frac{B_5(\{b_i\})+\left(\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}\right)}{5!}x^{5}
\end{align} B_n(\{b_i\})=B_n(b_1,...,b_n) n f(x)=\sum_{k=0}^{\infty}\frac{B_k(\{b_i\})}{k!}x^k+\frac{-\pi^{2}b_{1}}{3!}x^{3}+\frac{-\pi^{2}\left(4b_{1}^{2}+2b_{2}\right)}{4!}x^{4}+\frac{\pi^{4}b_{1}-20b_{1}b_{2}\pi^{2}-10b_{1}^{3}\pi^{2}-20\pi^{2}\frac{b_{3}}{6}}{5!}x^{5}+... \sum _{n=0}^{\infty }{B_{n}(b_{1},\dots ,b_{n}) \over n!}x^{n}=\exp \left(\sum _{i=1}^{\infty }{b_{i} \over i!}x^{i}\right) f \sum _{n=0}^{\infty }{B_{n}(\{b_i\}) \over n!}x^{n}=\exp\left[\left(\dfrac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma\right)x-\sum_{n=2}^{\infty} \dfrac{(2^{n-1}-1)\zeta'\left(n\right)+\zeta\left(n\right)\ln\left(2\right)}{2^{n-1}}x^n\right] =\exp\left[\left(\frac{\ln\left(2\right)^{2}}{2}-\ln\left(2\right)\gamma\right)x+\sum_{n=2}^{\infty}\frac{\zeta'\left(n\right)}{2^{n-1}}x^{n}-\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n}-\ln\left(2\right)\sum_{n=2}^{\infty}\frac{\zeta\left(n\right)}{2^{n-1}}x^{n}\right] =\exp\left[\frac{\ln\left(2\right)^{2}}{2}x+2\sum_{n=2}^{\infty}\zeta'\left(n\right)\left(\frac{x}{2}\right)^{n}-\sum_{n=2}^{\infty}\zeta'\left(n\right)x^{n}+\ln\left(2\right)x\psi\left(1-\frac{x}{2}\right)\right] \sum_{n=2}^{\infty}\zeta'(n)x^{n-1}=K-\ln(2)f(1-x) \sum _{n=0}^{\infty }{B_{n}(\{b_i\}) \over n!}x^{n}=\exp\left[\ln\left(2\right)x\left(\frac{\ln\left(2\right)}{2}+f\left(1-\frac{x}{2}\right)-f\left(1-x\right)+\psi\left(1-\frac{x}{2}\right)\right)\right] \sum_{n=2}^{\infty}\zeta(n)x^{n-1}=-\gamma-\psi(1-x) x!_{(\infty)}=\exp\left(\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)x}{x-n}\right)^{\text{sinc}(x)} \pi \begin{align}\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)x}{x-n}=&\sum_{s=1}^{\infty}x^{s}\left(-\sum_{n=1}^{\infty}\left(-1\right)^{n}\frac{\ln\left(n\right)}{n^{s}}\right)\\
=&\sum_{s=1}^{\infty}x^{s}\left(\frac{\mathrm{d}}{\mathrm{d}s}\sum_{n=1}^{\infty}\frac{\left(-1\right)^{n}}{n^{s}}\right)\\
=&-\sum_{s=1}^{\infty}\eta'(s)x^s\\
=&-\sum_{s=1}^{\infty}\frac{(2^{s-1}-1)\zeta'(s)+\ln(2)\zeta(s)}{2^{s-1}}x^s
\end{align} \begin{align}x!_{(\infty)}=&\exp\left(-\sum_{s=1}^{\infty}\eta'(s)x^s\right)^{\text{sinc}(x)}\\
=&\exp\left(-\sum_{s=1}^{\infty}\dfrac{\ln(2)\zeta(s)+(2^{s-1}-1)\zeta'(s)}{2^{s-1}}x^s\right)^{\text{sinc}(x)}\\
=&\exp\left(\ln(2)\psi\left(1-\frac{x}{2}\right)x+\sum_{s=1}^{\infty}(2^{1-s}-1)\zeta'(s)x^{s}\right)^{\text{sinc}(x)}\end{align} \zeta(1)=\gamma \eta'(1)=\gamma\ln(2)-\dfrac{\ln(2)^2}{2}","['calculus', 'sequences-and-series', 'polynomials', 'recurrence-relations', 'taylor-expansion']"
32,Closed form of the integral of $r \ln (r^2 + \rho^2 - 2r \rho \cos (\theta - t))$,Closed form of the integral of,r \ln (r^2 + \rho^2 - 2r \rho \cos (\theta - t)),I have the following integral that needs to be evaluated. $$ \int_{0}^{2\pi} \int_0^{\frac 1 {\sqrt{4 \cos^2 \theta  + 9 \sin^2 \theta}}} \frac r 2 \ln (r^2 + \rho^2 - 2r \rho \cos (\theta - \phi)) dr d\theta $$ I tried using Mathematica but it gives the following output -((\[Rho] (-2 E^(2 I \[Theta]) Sqrt[         26 - 5 E^(-2 I \[Theta]) -           5 E^(2 I \[Theta])] + (5 - 26 E^(2 I \[Theta]) +            5 E^(4 I \[Theta])) \[Rho] ArcTan[          Cot[\[Theta] - \[Phi]]] Cos[          2 \[Theta] - 2 \[Phi]] Csc[\[Theta] - \[Phi]] - (5 -            26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) \[Rho] ArcTan[          Cot[\[Theta] - \[Phi]] - (           Sqrt[2] Csc[\[Theta] - \[Phi]])/(\[Rho] Sqrt[            13 - 5 Cos[2 \[Theta]]])] Cos[          2 \[Theta] - 2 \[Phi]] Csc[\[Theta] - \[Phi]] -         10 \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] +         52 E^(2 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] \ - 10 E^(4 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] +         5 \[Rho] Cos[\[Theta] - \[Phi]] Log[(           5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 -             2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(           5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (           4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[           26 - 5 E^(-2 I \[Theta]) - 5 E^(2 I \[Theta])]] -         26 E^(2 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[(           5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 -             2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(           5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (           4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[           26 - 5 E^(-2 I \[Theta]) - 5 E^(2 I \[Theta])]] +         5 E^(4 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[(           5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 -             2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(           5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (           4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[           26 - 5 E^(-2 I \[Theta]) -             5 E^(2 I \[Theta])]]) Sin[\[Theta] - \[Phi]])/(5 -       26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta]))) Does closed form of the above integral exists? What approaches might I try to solve this integral? Is this possible to express this integral in terms of special functions?,I have the following integral that needs to be evaluated. I tried using Mathematica but it gives the following output -((\[Rho] (-2 E^(2 I \[Theta]) Sqrt[         26 - 5 E^(-2 I \[Theta]) -           5 E^(2 I \[Theta])] + (5 - 26 E^(2 I \[Theta]) +            5 E^(4 I \[Theta])) \[Rho] ArcTan[          Cot[\[Theta] - \[Phi]]] Cos[          2 \[Theta] - 2 \[Phi]] Csc[\[Theta] - \[Phi]] - (5 -            26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) \[Rho] ArcTan[          Cot[\[Theta] - \[Phi]] - (           Sqrt[2] Csc[\[Theta] - \[Phi]])/(\[Rho] Sqrt[            13 - 5 Cos[2 \[Theta]]])] Cos[          2 \[Theta] - 2 \[Phi]] Csc[\[Theta] - \[Phi]] -         10 \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] +         52 E^(2 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] \ - 10 E^(4 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[\[Rho]] +         5 \[Rho] Cos[\[Theta] - \[Phi]] Log[(           5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 -             2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(           5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (           4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[           26 - 5 E^(-2 I \[Theta]) - 5 E^(2 I \[Theta])]] -         26 E^(2 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[(           5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 -             2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(           5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (           4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[           26 - 5 E^(-2 I \[Theta]) - 5 E^(2 I \[Theta])]] +         5 E^(4 I \[Theta]) \[Rho] Cos[\[Theta] - \[Phi]] Log[(           5 \[Rho]^2 + 5 E^(4 I \[Theta]) \[Rho]^2 -             2 E^(2 I \[Theta]) (2 + 13 \[Rho]^2))/(           5 - 26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta])) - (           4 \[Rho] Cos[\[Theta] - \[Phi]])/Sqrt[           26 - 5 E^(-2 I \[Theta]) -             5 E^(2 I \[Theta])]]) Sin[\[Theta] - \[Phi]])/(5 -       26 E^(2 I \[Theta]) + 5 E^(4 I \[Theta]))) Does closed form of the above integral exists? What approaches might I try to solve this integral? Is this possible to express this integral in terms of special functions?,"
\int_{0}^{2\pi} \int_0^{\frac 1 {\sqrt{4 \cos^2 \theta  + 9 \sin^2 \theta}}} \frac r 2 \ln (r^2 + \rho^2 - 2r \rho \cos (\theta - \phi)) dr d\theta
","['calculus', 'integration', 'definite-integrals', 'polar-coordinates']"
33,How to evaluate $\int[(\frac{x}{2})^x + (\frac{2}{x})^x]\log_2x\mathrm dx$?,How to evaluate ?,\int[(\frac{x}{2})^x + (\frac{2}{x})^x]\log_2x\mathrm dx,"This integral appeared in my exam but I couldn't solve it within the time limits (~2 minutes) $$I = \int\left[\left(\frac{x}{2}\right)^x + \left(\frac{2}{x}\right)^x\right]\log_2x \mathrm dx$$ A) $\left(\frac{x}{2}\right)^x + \left(\frac{2}{x}\right)^x$ B) $\left(\frac{x}{2}\right)^x -\left(\frac{2}{x}\right)^x$ C) $\left(\frac{x}{2}\right)^x\log_2x$ D) $\left(\frac{x}{2}\right)^x\log_x2$ E) $\left(\frac{2}{x}\right)^x\log_x2$ F) $\left(\frac{2}{x}\right)^x\log_2x$ I observed that the first two terms in addition are reciprocal. However, I got failed by trying substitution techniques ( like $t = (\frac{x}{2})^x$ ) and integration by parts is making it more messy. Can you give a hint/solution (like a proper substitution or method) to solve this ? Thanks !","This integral appeared in my exam but I couldn't solve it within the time limits (~2 minutes) A) B) C) D) E) F) I observed that the first two terms in addition are reciprocal. However, I got failed by trying substitution techniques ( like ) and integration by parts is making it more messy. Can you give a hint/solution (like a proper substitution or method) to solve this ? Thanks !",I = \int\left[\left(\frac{x}{2}\right)^x + \left(\frac{2}{x}\right)^x\right]\log_2x \mathrm dx \left(\frac{x}{2}\right)^x + \left(\frac{2}{x}\right)^x \left(\frac{x}{2}\right)^x -\left(\frac{2}{x}\right)^x \left(\frac{x}{2}\right)^x\log_2x \left(\frac{x}{2}\right)^x\log_x2 \left(\frac{2}{x}\right)^x\log_x2 \left(\frac{2}{x}\right)^x\log_2x t = (\frac{x}{2})^x,"['calculus', 'integration', 'indefinite-integrals']"
34,How to integrate $\frac{\tan x}{x+\sin x}$?,How to integrate ?,\frac{\tan x}{x+\sin x},"How to calculate the integral $$\int\frac{\tan x}{x+\sin x}\mathrm dx\;\;?$$ I use a website called integral of the day, all the previous ones have answers, but today I cannot find an answer for it and no online calculators can do it either. It comes from this website: https://www.sammserver.com/iotd/ , but as it is the next day it has changed now.","How to calculate the integral I use a website called integral of the day, all the previous ones have answers, but today I cannot find an answer for it and no online calculators can do it either. It comes from this website: https://www.sammserver.com/iotd/ , but as it is the next day it has changed now.",\int\frac{\tan x}{x+\sin x}\mathrm dx\;\;?,"['calculus', 'integration']"
35,Prove the closed-form of $\int_{0}^{1}\frac{K^\prime\left (\frac{1-x^2}{2}\right)}{\sqrt{1+x^2}}\text{d}x$,Prove the closed-form of,\int_{0}^{1}\frac{K^\prime\left (\frac{1-x^2}{2}\right)}{\sqrt{1+x^2}}\text{d}x,"Question : Prove that $$ \int_{0}^{1} \frac{K^{\prime}\left ( \frac{1-x^2}{2}  \right ) }{\sqrt{1+x^2} } \text{d} x =\frac{\Gamma\left ( \frac{1}{24}  \right )  \Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{48\pi\sqrt{3}}, $$ where $K^\prime(x)=K\left(\sqrt{1-x^2}\right)$ and $K(x)$ is known as a complete elliptic integral of the first kind with the elliptic modulus $x$ . $\Gamma(z)=\int_{0}^{\infty}t^{z-1}e^{-t}\text{d}t$ is the Euler Gamma function. The equality's right hand side is probably given by an elliptic integral singular value $K(k_6)$ . Here are my thoughts. We can determine a similar one $$ \int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{ \sqrt{1-x^2} }\text{d}x=\frac{1}{\pi\sqrt{2}}\int_{0}^{\pi} \int_{0}^{\pi} \int_{0}^{\pi}  \frac{1}{3-\cos x-\cos y-\cos z}\text{d}x\text{d}y\text{d}z\\ =\frac{\Gamma\left ( \frac{1}{24}  \right )  \Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{32\pi\sqrt{3}}. $$ The direct calculation shows the first equality. The second one is refered to the third of Watson's triple integrals . So we only need to show $$ \int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{ \sqrt{1-x^2} }\text{d}x =\frac32\int_{0}^{1} \frac{K^\prime\left ( \frac{1-x^2}{2}  \right ) }{\sqrt{1+x^2} } \text{d}x. $$ So far, however, I can't see any connections between two integrals. Some subtle transformations and substitutions may be helpful. But I am not able to come up with them by myself. By integrating the function $f(x)=\frac{K\left ( \frac{1+x^2}{2}  \right ) }{ \sqrt{1-x^2} }$ along the real axis, we deduce $$ \int_{0}^{\sqrt{2} } \frac{K^\prime\left (1-x^2\right ) }{ \sqrt{1+x^2} }\text{d}x =\frac{\sqrt{2}}{2}\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{ \sqrt{1-x^2} }\text{d}x\\ =\frac{\sqrt{6}\,\Gamma\left ( \frac{1}{24}  \right )  \Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{192\pi}. $$ The desired integral is actually represented as follows $$ \int_{-1}^{1} \int_{1}^{\infty} \int_{1}^{\infty}  \frac{1}{\sqrt{x^2-1}\sqrt{y^2-1} \sqrt{1-z^2}  } \frac{1}{3+x+y+z} \text{d}x\text{d}y\text{d}z $$ which is equivalent to $\int_{0}^{\infty}e^{-3z}K_0(z)^2I_0(z)\text{d}z=\frac{\pi^2}3\int_{0}^{\infty}e^{-3z}I_0(z)^3\text{d}z$ after some complex integral technique. Detailed explanations are writing now.","Question : Prove that where and is known as a complete elliptic integral of the first kind with the elliptic modulus . is the Euler Gamma function. The equality's right hand side is probably given by an elliptic integral singular value . Here are my thoughts. We can determine a similar one The direct calculation shows the first equality. The second one is refered to the third of Watson's triple integrals . So we only need to show So far, however, I can't see any connections between two integrals. Some subtle transformations and substitutions may be helpful. But I am not able to come up with them by myself. By integrating the function along the real axis, we deduce The desired integral is actually represented as follows which is equivalent to after some complex integral technique. Detailed explanations are writing now.","
\int_{0}^{1} \frac{K^{\prime}\left ( \frac{1-x^2}{2}  \right ) }{\sqrt{1+x^2} }
\text{d} x
=\frac{\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{48\pi\sqrt{3}},
 K^\prime(x)=K\left(\sqrt{1-x^2}\right) K(x) x \Gamma(z)=\int_{0}^{\infty}t^{z-1}e^{-t}\text{d}t K(k_6) 
\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }\text{d}x=\frac{1}{\pi\sqrt{2}}\int_{0}^{\pi} \int_{0}^{\pi} \int_{0}^{\pi} 
\frac{1}{3-\cos x-\cos y-\cos z}\text{d}x\text{d}y\text{d}z\\
=\frac{\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{32\pi\sqrt{3}}.
 
\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }\text{d}x
=\frac32\int_{0}^{1} \frac{K^\prime\left ( \frac{1-x^2}{2}  \right ) }{\sqrt{1+x^2} }
\text{d}x.
 f(x)=\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} } 
\int_{0}^{\sqrt{2} } \frac{K^\prime\left (1-x^2\right ) }{
\sqrt{1+x^2} }\text{d}x
=\frac{\sqrt{2}}{2}\int_{0}^{1}\frac{K\left ( \frac{1+x^2}{2}  \right ) }{
\sqrt{1-x^2} }\text{d}x\\
=\frac{\sqrt{6}\,\Gamma\left ( \frac{1}{24}  \right ) 
\Gamma\left ( \frac{5}{24}  \right)\Gamma\left ( \frac{7}{24}  \right ) \Gamma\left (  \frac{11}{24}\right )  }{192\pi}.
 
\int_{-1}^{1} \int_{1}^{\infty} \int_{1}^{\infty} 
\frac{1}{\sqrt{x^2-1}\sqrt{y^2-1} \sqrt{1-z^2}  }
\frac{1}{3+x+y+z} \text{d}x\text{d}y\text{d}z
 \int_{0}^{\infty}e^{-3z}K_0(z)^2I_0(z)\text{d}z=\frac{\pi^2}3\int_{0}^{\infty}e^{-3z}I_0(z)^3\text{d}z","['calculus', 'integration', 'definite-integrals', 'gamma-function', 'elliptic-integrals']"
36,Is there a way to infer periodicity of functions purely from their Taylor series?,Is there a way to infer periodicity of functions purely from their Taylor series?,,"I have been working with Taylor series for a while and I am interested in the following. Suppose I have the Maclaurin (for simplicity) series: $f \left(x \right) = \sum_{n = 0}^{\infty}{a_{n}x^{n}}$ where the formula $a_{n} = g \left(n \right)$ is known. Furthermore, assume that the above series converges everywhere. Under these conditions, is it possible to deduce from $g \left(n \right)$ that $f \left(x \right)$ is periodic? For instance, for $f \left(x \right) = \cos{\left(x \right)}$ , $a_{2n} = \frac{\left(-1 \right)^{n}}{\left(2n \right)!}$ and $a_{2n + 1} = 0$ for $n \ge 0$ . Does something about the ""form"" (for lack of a better word) of these coefficients imply the periodicity of $\cos{ \left(x \right)}$ ? I have read in other questions on this site that if $g \left(n \right)$ is not known a priori, this is an undecidable problem: is this still the case when $g \left(n \right)$ is known? As an aside, here is what I have tried: if $f \left(x \right)$ is periodic then one has $f \left(x \right) = f \left(x + P \right)$ , where $P$ is the period and $P > 0$ . Then, one has: $f \left(x + P \right) = \sum_{n = 0}^{\infty}{a_{n} \left(x + P \right)^{n}}$ In particular, we have $f \left(0 \right) = f \left(P \right)$ , giving: $a_{0} = a_{0} + \sum_{n = 1}^{\infty}{a_{n}P^{n}}$ Giving $\sum_{n = 1}^{\infty}{a_{n}P^{n}} = 0$ One can also differentiate $f \left(x \right)$ , which yields: $f' \left(x \right) = \sum_{n = 1}^{\infty}{n a_{n} x^{n - 1}}$ Requiring that $f' \left(x \right) = f' \left(x + P \right)$ and setting $x = 0$ , one then has: $a_{1} = a_{1} + \sum_{n = 2}^{\infty}{n a_{n} P^{n - 1}}$ That is: $\sum_{n = 2}^{\infty}{n a_{n} P^{n - 1}} = 0$ The above procedure can be repeated ad infinitum for higher derivatives, which seems to give an infinite number of conditions which must be satisfied by the various $a_{n}$ 's. However, it seems really complicated to deduce periodicity from these conditons. Any help or insight would be greatly appreciated.","I have been working with Taylor series for a while and I am interested in the following. Suppose I have the Maclaurin (for simplicity) series: where the formula is known. Furthermore, assume that the above series converges everywhere. Under these conditions, is it possible to deduce from that is periodic? For instance, for , and for . Does something about the ""form"" (for lack of a better word) of these coefficients imply the periodicity of ? I have read in other questions on this site that if is not known a priori, this is an undecidable problem: is this still the case when is known? As an aside, here is what I have tried: if is periodic then one has , where is the period and . Then, one has: In particular, we have , giving: Giving One can also differentiate , which yields: Requiring that and setting , one then has: That is: The above procedure can be repeated ad infinitum for higher derivatives, which seems to give an infinite number of conditions which must be satisfied by the various 's. However, it seems really complicated to deduce periodicity from these conditons. Any help or insight would be greatly appreciated.",f \left(x \right) = \sum_{n = 0}^{\infty}{a_{n}x^{n}} a_{n} = g \left(n \right) g \left(n \right) f \left(x \right) f \left(x \right) = \cos{\left(x \right)} a_{2n} = \frac{\left(-1 \right)^{n}}{\left(2n \right)!} a_{2n + 1} = 0 n \ge 0 \cos{ \left(x \right)} g \left(n \right) g \left(n \right) f \left(x \right) f \left(x \right) = f \left(x + P \right) P P > 0 f \left(x + P \right) = \sum_{n = 0}^{\infty}{a_{n} \left(x + P \right)^{n}} f \left(0 \right) = f \left(P \right) a_{0} = a_{0} + \sum_{n = 1}^{\infty}{a_{n}P^{n}} \sum_{n = 1}^{\infty}{a_{n}P^{n}} = 0 f \left(x \right) f' \left(x \right) = \sum_{n = 1}^{\infty}{n a_{n} x^{n - 1}} f' \left(x \right) = f' \left(x + P \right) x = 0 a_{1} = a_{1} + \sum_{n = 2}^{\infty}{n a_{n} P^{n - 1}} \sum_{n = 2}^{\infty}{n a_{n} P^{n - 1}} = 0 a_{n},"['calculus', 'power-series']"
37,Analytical solution for length of star trails?,Analytical solution for length of star trails?,,"Assuming there is an (infinitely) large sphere (the sky) that rotates counterclockwise as seen from above around its axis of rotation (which goes through its center point) at a fixed rate ω_sky, with a point (star) on its inner surface at 0° ≤ φ_star < 360°, -90° ≤ θ_star ≤ 90°. In the center, a vector (a startracker) is aligned with the axis of rotation toward the north pole, but misaligned by a degree (say, θ_tracker = 90°- 𝛿, 𝛿=1°, φ_tracker=0°) and rotates at the same rate and in the same direction as the sky. Finally, a vector (camera) is fixed to the rotation of that vector, but is initally pointed to the star. Over a duration (full rotation of both sky + (tracker+camera)), from the perspective of the camera, the point/star describes a closed path. Viewed from the outside of the sphere, fixed on the changing point on the sphere the camera points to (red dot): From the inside, showing how the star/fixed point on the sphere changes position from the perspective of the camera: Which when stacked in a long exposure can look like this: Typical exposures are much shorter, so their resulting trail is just a very short segment of this closed path. Is it possible to calculate the length of this path (star trail) in degrees over a certain range of (exposure) time analytically, for a given star position and misalignment 𝛿?","Assuming there is an (infinitely) large sphere (the sky) that rotates counterclockwise as seen from above around its axis of rotation (which goes through its center point) at a fixed rate ω_sky, with a point (star) on its inner surface at 0° ≤ φ_star < 360°, -90° ≤ θ_star ≤ 90°. In the center, a vector (a startracker) is aligned with the axis of rotation toward the north pole, but misaligned by a degree (say, θ_tracker = 90°- 𝛿, 𝛿=1°, φ_tracker=0°) and rotates at the same rate and in the same direction as the sky. Finally, a vector (camera) is fixed to the rotation of that vector, but is initally pointed to the star. Over a duration (full rotation of both sky + (tracker+camera)), from the perspective of the camera, the point/star describes a closed path. Viewed from the outside of the sphere, fixed on the changing point on the sphere the camera points to (red dot): From the inside, showing how the star/fixed point on the sphere changes position from the perspective of the camera: Which when stacked in a long exposure can look like this: Typical exposures are much shorter, so their resulting trail is just a very short segment of this closed path. Is it possible to calculate the length of this path (star trail) in degrees over a certain range of (exposure) time analytically, for a given star position and misalignment 𝛿?",,"['calculus', 'integration', 'angle', 'spheres']"
38,Interesting closed form for $\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta$,Interesting closed form for,\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta,"Some time ago I used a formal approach to derive the following identity: $$\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta=\frac{3^{\frac{1}{12}}\pi\sqrt{2}}{AGM(1+\sqrt{3},\sqrt{8})}\tag{1}$$ where $AGM$ is the arithmetic-geometric mean . Wolfram Alpha does not tell me whether this is correct, but it does appear to be accurate to many decimal places. I have three questions: Can anyone verify whether $(1)$ is in fact correct? Is there a way of generalizing $(1)$ to integrals of the form $\int_0^{\frac{\pi}{2}}\left(a+\sin^2{\theta}\right)^{-\frac{1}{3}}\;d\theta$ or is this integral more special? My derivation (see below) appears to only work for $a=\frac{1}{3}$. There is a superficial similarity between $(1)$ and elliptic integrals (e.g. the $AGM$ evaluation ); is there a way to transform this integral into an elliptic integral that I have missed, or is it merely a coincidence that an integral of this form is the reciprocal of an $AGM$? Derivation : I have put this here in case it helps to see where I am coming from; I apologize for its length. I began by using a multiple integration trick of squaring the integral and converting to polar coordinates to evaluate $\int_0^\infty e^{-x^6}dx=\frac{1}{6}\Gamma(\frac{1}{6})$ as follows: $$\left[\int_0^\infty e^{-x^6}\;dx\right]^2=\int_0^\infty\int_0^{\frac{\pi}{2}}re^{-r^6(\cos^6\theta\;+\;\sin^6\theta)}\;d\theta\;dx={\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{3r^6\cos^2\theta\sin^2\theta}\;d\theta\;dx}$$ $$=\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{\frac{3r^6}{4}\sin^22\theta}\;d\theta\;dx={\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{\frac{3r^6}{4}\cos^2\theta}\;d\theta\;dx}$$ I then made use of the following formula (see here ): $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}e^{4x\cos^2\theta}\;d\theta\tag{2}$$ Using $(2)$ and formally interchanging integration and summation we get: $$\frac{\Gamma(\frac{1}{6})^2}{36}=\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{4\left(\frac{3r^6}{16}\right)\cos^2\theta}\;d\theta\;dx=\frac{\pi}{2}\int_0^\infty re^{-r^6}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3r^6}{16}\right)^n\;dx$$ $$=\frac{\pi}{2}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3}{16}\right)^n \int_0^\infty r^{6n+1}e^{-r^6}\;dx=\frac{\pi}{12}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3}{16}\right)^n \Gamma\left(n+\frac{1}{3}\right)$$ I then used Laplace transform identities and $(2)$, freely interchanging integrals and sums, to write: $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\frac{\Gamma\left(n+\frac{1}{3}\right)}{s^{n+\frac{1}{3}}}=L\left[\sum_{n=0}^\infty \frac{(2n)!}{(n!)^3}t^{n-\frac{2}{3}}\right](s)={\frac{2}{\pi}L\left[t^{-\frac{2}{3}}\int_0^\frac{\pi}{2}e^{4t\cos^2\theta}\;d\theta\right](s)}={\frac{2}{\pi}\int_0^\frac{\pi}{2}L\left[t^{-\frac{2}{3}}e^{4t\cos^2\theta}\right](s)\;d\theta}={\frac{2}{\pi}\int_0^\frac{\pi}{2}\frac{\Gamma(\frac{1}{3})}{(s-4\cos^2\theta)^{\frac{1}{3}}}\;d\theta}$$ Accordingly, since $\frac{4}{3}-\cos^2\theta=\frac{1}{3}+\sin^2{\theta}$ we can deduce that: $$\frac{\Gamma(\frac{1}{6})^2}{36}=\frac{\Gamma(\frac{1}{3})}{6}\left(\frac{4}{3}\right)^\frac{1}{3}\int_0^\frac{\pi}{2}\frac{1}{(\frac{1}{3}+\sin^2\theta)^{\frac{1}{3}}}\;d\theta$$ Reflection and duplication give $\Gamma(\frac{1}{6})=2^{-\frac{1}{3}}\sqrt{\frac{3}{\pi}}\Gamma(\frac{1}{3})^2$ and hence we have the following identity: $$\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta=\frac{3^\frac{1}{3}\Gamma(\frac{1}{3})^3}{2^\frac{7}{3}\pi}\tag{3}$$ while $(1)$ may be obtained by using the following identity (see here ): $$\Gamma\left(\frac{1}{6}\right)=\frac{2^\frac{14}{9}3^\frac{1}{3}\pi^\frac{5}{6}}{AGM(1+\sqrt{3},\sqrt{8})^\frac{2}{3}}$$ This completes the derivation; I cannot see how a method like this (especially with the conversion to polar coordinates) could be used to give results more general than $(1)$ and $(3)$.","Some time ago I used a formal approach to derive the following identity: $$\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta=\frac{3^{\frac{1}{12}}\pi\sqrt{2}}{AGM(1+\sqrt{3},\sqrt{8})}\tag{1}$$ where $AGM$ is the arithmetic-geometric mean . Wolfram Alpha does not tell me whether this is correct, but it does appear to be accurate to many decimal places. I have three questions: Can anyone verify whether $(1)$ is in fact correct? Is there a way of generalizing $(1)$ to integrals of the form $\int_0^{\frac{\pi}{2}}\left(a+\sin^2{\theta}\right)^{-\frac{1}{3}}\;d\theta$ or is this integral more special? My derivation (see below) appears to only work for $a=\frac{1}{3}$. There is a superficial similarity between $(1)$ and elliptic integrals (e.g. the $AGM$ evaluation ); is there a way to transform this integral into an elliptic integral that I have missed, or is it merely a coincidence that an integral of this form is the reciprocal of an $AGM$? Derivation : I have put this here in case it helps to see where I am coming from; I apologize for its length. I began by using a multiple integration trick of squaring the integral and converting to polar coordinates to evaluate $\int_0^\infty e^{-x^6}dx=\frac{1}{6}\Gamma(\frac{1}{6})$ as follows: $$\left[\int_0^\infty e^{-x^6}\;dx\right]^2=\int_0^\infty\int_0^{\frac{\pi}{2}}re^{-r^6(\cos^6\theta\;+\;\sin^6\theta)}\;d\theta\;dx={\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{3r^6\cos^2\theta\sin^2\theta}\;d\theta\;dx}$$ $$=\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{\frac{3r^6}{4}\sin^22\theta}\;d\theta\;dx={\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{\frac{3r^6}{4}\cos^2\theta}\;d\theta\;dx}$$ I then made use of the following formula (see here ): $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}x^n=\frac{2}{\pi}\int_0^{\frac{\pi}{2}}e^{4x\cos^2\theta}\;d\theta\tag{2}$$ Using $(2)$ and formally interchanging integration and summation we get: $$\frac{\Gamma(\frac{1}{6})^2}{36}=\int_0^\infty re^{-r^6}\int_0^{\frac{\pi}{2}}e^{4\left(\frac{3r^6}{16}\right)\cos^2\theta}\;d\theta\;dx=\frac{\pi}{2}\int_0^\infty re^{-r^6}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3r^6}{16}\right)^n\;dx$$ $$=\frac{\pi}{2}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3}{16}\right)^n \int_0^\infty r^{6n+1}e^{-r^6}\;dx=\frac{\pi}{12}\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\left(\frac{3}{16}\right)^n \Gamma\left(n+\frac{1}{3}\right)$$ I then used Laplace transform identities and $(2)$, freely interchanging integrals and sums, to write: $$\sum_{n=0}^{\infty}\frac{(2n)!}{(n!)^3}\frac{\Gamma\left(n+\frac{1}{3}\right)}{s^{n+\frac{1}{3}}}=L\left[\sum_{n=0}^\infty \frac{(2n)!}{(n!)^3}t^{n-\frac{2}{3}}\right](s)={\frac{2}{\pi}L\left[t^{-\frac{2}{3}}\int_0^\frac{\pi}{2}e^{4t\cos^2\theta}\;d\theta\right](s)}={\frac{2}{\pi}\int_0^\frac{\pi}{2}L\left[t^{-\frac{2}{3}}e^{4t\cos^2\theta}\right](s)\;d\theta}={\frac{2}{\pi}\int_0^\frac{\pi}{2}\frac{\Gamma(\frac{1}{3})}{(s-4\cos^2\theta)^{\frac{1}{3}}}\;d\theta}$$ Accordingly, since $\frac{4}{3}-\cos^2\theta=\frac{1}{3}+\sin^2{\theta}$ we can deduce that: $$\frac{\Gamma(\frac{1}{6})^2}{36}=\frac{\Gamma(\frac{1}{3})}{6}\left(\frac{4}{3}\right)^\frac{1}{3}\int_0^\frac{\pi}{2}\frac{1}{(\frac{1}{3}+\sin^2\theta)^{\frac{1}{3}}}\;d\theta$$ Reflection and duplication give $\Gamma(\frac{1}{6})=2^{-\frac{1}{3}}\sqrt{\frac{3}{\pi}}\Gamma(\frac{1}{3})^2$ and hence we have the following identity: $$\int_0^{\frac{\pi}{2}}\frac{1}{\left(\frac{1}{3}+\sin^2{\theta}\right)^{\frac{1}{3}}}\;d\theta=\frac{3^\frac{1}{3}\Gamma(\frac{1}{3})^3}{2^\frac{7}{3}\pi}\tag{3}$$ while $(1)$ may be obtained by using the following identity (see here ): $$\Gamma\left(\frac{1}{6}\right)=\frac{2^\frac{14}{9}3^\frac{1}{3}\pi^\frac{5}{6}}{AGM(1+\sqrt{3},\sqrt{8})^\frac{2}{3}}$$ This completes the derivation; I cannot see how a method like this (especially with the conversion to polar coordinates) could be used to give results more general than $(1)$ and $(3)$.",,"['calculus', 'definite-integrals', 'closed-form', 'gamma-function', 'elliptic-integrals']"
39,Is Robinson’s non standard analysis accepted by the mathematical community?,Is Robinson’s non standard analysis accepted by the mathematical community?,,"Up until now I had known that calculus had been initially formulated by Leibniz and Newton in terms of infinitesimals, but that approach led to inconsistencies, and so Cauchy and others reformulated calculus rigorously using the epsilon-delta method. Recently I stumbled across “Elementary Calculus : An Infinitesimal Approach” by H. J. Keisler. In his preface Keisler says that the infinitesimal approach to calculus had been placed on rigorous footing in the 1960s by Abraham Robinson. And here is my question : is it universally accepted now that Robinson’s calculus is rigorous/not logically inconsistent? In essence, what’s the common consensus in the math community regarding Robinson’s approach to infinitesimal calculus?","Up until now I had known that calculus had been initially formulated by Leibniz and Newton in terms of infinitesimals, but that approach led to inconsistencies, and so Cauchy and others reformulated calculus rigorously using the epsilon-delta method. Recently I stumbled across “Elementary Calculus : An Infinitesimal Approach” by H. J. Keisler. In his preface Keisler says that the infinitesimal approach to calculus had been placed on rigorous footing in the 1960s by Abraham Robinson. And here is my question : is it universally accepted now that Robinson’s calculus is rigorous/not logically inconsistent? In essence, what’s the common consensus in the math community regarding Robinson’s approach to infinitesimal calculus?",,"['calculus', 'infinitesimals']"
40,The length arc of parabola,The length arc of parabola,,"Find the length of the arc of the parabola $y^2=4x$ from $x=0$ to $x=4$ . In the manual solution is $2\sqrt{5}+\ln(2+\sqrt{5}).$ My answer is $\displaystyle 2\int_0^4 \! \sqrt{1+\frac{\mathrm{d}x}{\mathrm{d}y}} \, \mathrm{d}x$ $\displaystyle 2\int_0^4 \! \sqrt{1+\frac{y^2}{4}} \, \mathrm{d}x$ $\displaystyle 2\int_0^4 \! \sqrt{4+y^2} \, \mathrm{d}x= 4 \sqrt{5}+2\ln(2+\sqrt{5})$ But my answer is $4 \sqrt{5}+2\ln(2+\sqrt{5})$ As I doubled the length, Do we need to double the arc length?","Find the length of the arc of the parabola from to . In the manual solution is My answer is But my answer is As I doubled the length, Do we need to double the arc length?","y^2=4x x=0 x=4 2\sqrt{5}+\ln(2+\sqrt{5}). \displaystyle 2\int_0^4 \! \sqrt{1+\frac{\mathrm{d}x}{\mathrm{d}y}} \, \mathrm{d}x \displaystyle 2\int_0^4 \! \sqrt{1+\frac{y^2}{4}} \, \mathrm{d}x \displaystyle 2\int_0^4 \! \sqrt{4+y^2} \, \mathrm{d}x= 4 \sqrt{5}+2\ln(2+\sqrt{5}) 4 \sqrt{5}+2\ln(2+\sqrt{5})","['calculus', 'integration', 'conic-sections']"
41,How to solve this integral $\int_0^{1}(2x^2+1)\sqrt{\frac{1-x^2}{1-x^8}}\ dx$,How to solve this integral,\int_0^{1}(2x^2+1)\sqrt{\frac{1-x^2}{1-x^8}}\ dx,$$\int_0^{1}(2x^2+1)\sqrt{\frac{1-x^2}{1-x^8}}\ dx$$ substituting $x^4=\sin u; dx=\frac{\cos u}{4(\sin u)^{3/4}}$ $$\frac1{4}\int_0^{\pi/2}\frac{\left(2\sqrt{\sin u}+1\right){\sqrt{1-\sqrt{\sin u}}}}{(\sin u)^{3/4}}\ du$$ $$\frac 1{2}\int_0^{\pi/2}\frac{\sqrt{1-\sqrt{\sin u}}}{(\sin u)^{1/4}}du+\frac1{4}\int_0^{\pi/2}\frac{\sqrt{1-\sqrt{\sin u}}}{(\sin u)^{3/4}}\ du$$,substituting,\int_0^{1}(2x^2+1)\sqrt{\frac{1-x^2}{1-x^8}}\ dx x^4=\sin u; dx=\frac{\cos u}{4(\sin u)^{3/4}} \frac1{4}\int_0^{\pi/2}\frac{\left(2\sqrt{\sin u}+1\right){\sqrt{1-\sqrt{\sin u}}}}{(\sin u)^{3/4}}\ du \frac 1{2}\int_0^{\pi/2}\frac{\sqrt{1-\sqrt{\sin u}}}{(\sin u)^{1/4}}du+\frac1{4}\int_0^{\pi/2}\frac{\sqrt{1-\sqrt{\sin u}}}{(\sin u)^{3/4}}\ du,"['calculus', 'integration', 'definite-integrals']"
42,"Evaluate two integrals involving $\operatorname{Li}_3,\operatorname{Li}_4$",Evaluate two integrals involving,"\operatorname{Li}_3,\operatorname{Li}_4","I need to evaluate $$\int_{1}^{\infty}  \frac{\displaystyle{\operatorname{Re}\left (  \operatorname{Li}_3\left ( \frac{1+x}{2}  \right )  \right ) \ln^2\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x.$$ $\operatorname{Li}_n(.)$ denotes the "" Polylogarithms "". Numerical tests derived $$\int_{1}^{\infty}  \frac{\displaystyle{\operatorname{Re}\left (  \operatorname{Li}_3\left ( \frac{1+x}{2}  \right )  \right ) \ln^2\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x =\operatorname{Im}\left[\operatorname{Li}_3\left ( \frac{1+i}{2}  \right ) \right]^2+\frac{1911}{4096}\zeta(3)^2 +\frac{413}{1536}\zeta(3)\ln^32 -\frac{721}{6144}\pi^2\zeta(3)\ln2-\frac{41}{1536}\pi^2\ln^42 +\frac{77}{12288}\pi^4\ln^22+\frac{7}{256}\ln^62.$$ I checked this $$\int_{1}^{\infty}  \frac{\displaystyle{\operatorname{Re}\left (  \operatorname{Li}_4\left ( \frac{1+x}{2}  \right )  \right ) \ln^3\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x =3\operatorname{Im} \left [ \operatorname{Li}_4\left ( \frac{1+i}{2}  \right ) \right ]^2 -\frac{343\pi^4\operatorname{Li}_4\left ( \frac{1}{2}  \right ) }{49152} +\frac{693\operatorname{Li}_4\left ( \frac{1}{2}  \right )^2}{256} -\frac{5\operatorname{Li}_4\left ( \frac{1}{2}  \right )}{256}\ln^42 +\frac{25\pi^2\operatorname{Li}_4\left ( \frac{1}{2}  \right )}{2048}\ln^22 -\frac{117649\pi^8}{2831155200}-\frac{\ln^82}{3072} +\frac{5\pi^2\ln^62}{12288}-\frac{1061\pi^4\ln^42}{2949120}  +\frac{343\pi^6\ln^22}{2359296}.$$","I need to evaluate denotes the "" Polylogarithms "". Numerical tests derived I checked this","\int_{1}^{\infty} 
\frac{\displaystyle{\operatorname{Re}\left ( 
\operatorname{Li}_3\left ( \frac{1+x}{2}  \right )  \right )
\ln^2\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x. \operatorname{Li}_n(.) \int_{1}^{\infty} 
\frac{\displaystyle{\operatorname{Re}\left ( 
\operatorname{Li}_3\left ( \frac{1+x}{2}  \right )  \right )
\ln^2\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x
=\operatorname{Im}\left[\operatorname{Li}_3\left ( \frac{1+i}{2}  \right )
\right]^2+\frac{1911}{4096}\zeta(3)^2 +\frac{413}{1536}\zeta(3)\ln^32
-\frac{721}{6144}\pi^2\zeta(3)\ln2-\frac{41}{1536}\pi^2\ln^42
+\frac{77}{12288}\pi^4\ln^22+\frac{7}{256}\ln^62. \int_{1}^{\infty} 
\frac{\displaystyle{\operatorname{Re}\left ( 
\operatorname{Li}_4\left ( \frac{1+x}{2}  \right )  \right )
\ln^3\left ( \frac{1+x}{2}  \right )  }}{x(1+x^2)} \text{d}x
=3\operatorname{Im}
\left [ \operatorname{Li}_4\left ( \frac{1+i}{2}  \right ) \right ]^2
-\frac{343\pi^4\operatorname{Li}_4\left ( \frac{1}{2}  \right ) }{49152}
+\frac{693\operatorname{Li}_4\left ( \frac{1}{2}  \right )^2}{256}
-\frac{5\operatorname{Li}_4\left ( \frac{1}{2}  \right )}{256}\ln^42
+\frac{25\pi^2\operatorname{Li}_4\left ( \frac{1}{2}  \right )}{2048}\ln^22
-\frac{117649\pi^8}{2831155200}-\frac{\ln^82}{3072}
+\frac{5\pi^2\ln^62}{12288}-\frac{1061\pi^4\ln^42}{2949120} 
+\frac{343\pi^6\ln^22}{2359296}.","['calculus', 'integration', 'improper-integrals', 'contour-integration', 'polylogarithm']"
43,Integral of $(1+t^2)^{\frac{1}{2}} \cdot (1+t^4)^\frac{1}{4}$,Integral of,(1+t^2)^{\frac{1}{2}} \cdot (1+t^4)^\frac{1}{4},"I was solving the following problem of finding the General Solution of a given ODE given below: $$ \frac{\mathrm dy}{\mathrm dt} + \frac{ty}{1+t^2} = 1 - \frac{t^3y}{1+t^4}$$ I ended up with $$y\cdot(1+t^2)^{1/2}\cdot(1+t^4)^{(1/4)} = \int (1+t^2)^{1/2}\cdot (1+t^4)^{1/4}\,\mathrm dt$$ How do I Integrate the RHS. Even online integral calculators are showing that they are unable to do it. Can someone tell me if it is even integrable or not. Or is there any other idea of solving the ODE. Anyways I wish to learn why the integration on the RHS is not happenning. Is it non integrable? I looked at the parabola formed by $$(1+t^2)^{\frac{1}{2}} \cdot (1+t^4)^\frac{1}{4}$$ on an online graph plotter and it looked like the area under was infinitely much as it was upward opening and a bit narrow in shape(not very).",I was solving the following problem of finding the General Solution of a given ODE given below: I ended up with How do I Integrate the RHS. Even online integral calculators are showing that they are unable to do it. Can someone tell me if it is even integrable or not. Or is there any other idea of solving the ODE. Anyways I wish to learn why the integration on the RHS is not happenning. Is it non integrable? I looked at the parabola formed by on an online graph plotter and it looked like the area under was infinitely much as it was upward opening and a bit narrow in shape(not very).," \frac{\mathrm dy}{\mathrm dt} + \frac{ty}{1+t^2} = 1 - \frac{t^3y}{1+t^4} y\cdot(1+t^2)^{1/2}\cdot(1+t^4)^{(1/4)} = \int (1+t^2)^{1/2}\cdot (1+t^4)^{1/4}\,\mathrm dt (1+t^2)^{\frac{1}{2}} \cdot (1+t^4)^\frac{1}{4}","['calculus', 'integration', 'ordinary-differential-equations']"
44,$\int_0^{\infty} \frac{e^{-\pi x^2}+e^{-\pi /x^2}}{e^{\pi x}+e^{\pi /x}} dx$ Sum of Coefficients in Cubic Polynomial,Sum of Coefficients in Cubic Polynomial,\int_0^{\infty} \frac{e^{-\pi x^2}+e^{-\pi /x^2}}{e^{\pi x}+e^{\pi /x}} dx,"I have been trying to solve this problem and I am stuck half way. My Working: I have solved the integrals and noticed that the value of $\alpha$ comes out to be $$\alpha\approx 94.05$$ So, the cubic equation becomes $$94.05^3A+94.05^2B+94.05C+D=0$$ I am wondering if the question is correct or not. Because I think we cannot solve for four unknowns $$-(A+B+C+D)= ?$$ using 1 equation only, I think we need three more equations. There may be many possible answers for this equation as coefficients are not defined. If it is still solvable please let me know, $\color{red}{\text{I am not asking answer I just want to know is it solvable or not ?}}$ Thanks Edit : $\color{blue}{\text{The original problem has been deleted now}}$ on the site maybe due to wrong question statement and not properly stated or something is missing. Maybe they feel it is not possible to find the value of $A+B+C+D$ However if you still want to try these lovely integrals then you can. Thank you for the support.","I have been trying to solve this problem and I am stuck half way. My Working: I have solved the integrals and noticed that the value of comes out to be So, the cubic equation becomes I am wondering if the question is correct or not. Because I think we cannot solve for four unknowns using 1 equation only, I think we need three more equations. There may be many possible answers for this equation as coefficients are not defined. If it is still solvable please let me know, Thanks Edit : on the site maybe due to wrong question statement and not properly stated or something is missing. Maybe they feel it is not possible to find the value of However if you still want to try these lovely integrals then you can. Thank you for the support.",\alpha \alpha\approx 94.05 94.05^3A+94.05^2B+94.05C+D=0 -(A+B+C+D)= ? \color{red}{\text{I am not asking answer I just want to know is it solvable or not ?}} \color{blue}{\text{The original problem has been deleted now}} A+B+C+D,"['calculus', 'integration', 'solution-verification']"
45,"Troubles calculating $\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx$",Troubles calculating,"\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx","I want to evaluate the integral $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx$$ with the Cauchy principal value $\mathrm{p.v}$ . This integral converges, but just to be sure I also looked at the much simpler integral $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx<\mathrm{p.v.}\int_0^1\frac{x}{\pi}\frac{2}{1-2x}=-\frac{1}{\pi},$$ this confirms the initial integral to be finite (in the Cauchy sense). To find the antiderivative I tried using Leibniz's formula on $$\int\tan(ax)\,dx=-\frac{\ln\lvert\cos(ax)\rvert}{a}$$ to arrive at $$\int\tan(\pi x)x\,dx=\frac{d}{da}\left.\int\tan(ax)\,dx\right|_{a=\pi}=\frac{\ln\lvert \cos(\pi x)\rvert}{\pi^2} + \frac{x \tan(\pi x))}{\pi}.$$ Using the definition of the Cauchy principal value, namely \begin{align} \mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx &=\lim_{\varepsilon\to0^+}\left[\int_0^{\frac{1}{2}-\varepsilon}\tan(\pi x)x\,dx+\int_{\frac{1}{2}+\varepsilon}^1\tan(\pi x)x\,dx\right]\\[5pt] &=\lim_{\varepsilon\to0^+}\left[\frac{\ln\cos\left\lvert\pi (\frac{1}{2}-\varepsilon)\right\rvert}{\pi^2} + \frac{(\frac{1}{2}-\varepsilon) \tan\left(\pi (\frac{1}{2}-\varepsilon)\right)}{\pi}\right.\\[5pt] &\phantom{=\lim_{\varepsilon\to0^+}\left[\right.}\left.-\frac{\ln\cos\left\lvert\pi (\frac{1}{2}+\varepsilon)\right\rvert}{\pi^2} - \frac{(\frac{1}{2}+\varepsilon) \tan\left(\pi (\frac{1}{2}+\varepsilon)\right)}{\pi}\right] \end{align} the limit diverges for $\varepsilon\to0^+$ . I then also tried to manipulate the integral to become $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx=\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx\approx-0.22063560.$$ The value seems to be right, as I tried to evaluate the integral graphically, but I'd like to have the exact result. The other problem I encountered was that the antiderivative of the right integral also diverges for the upper limit: $$\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx=\left[2\left(\frac{\ln\lvert \cos(\pi x)\rvert}{\pi^2} + \frac{x \tan(\pi x))}{\pi}\right)+\frac{\ln\lvert \cos(\pi x)\rvert}{\pi} \right]_0^{\frac{1}{2}}\to\infty.$$ I really don't understand where things went wrong and any help is highly appreciated. Edit: I forgot to differentiate $\tan(a x)$ -.-, so the antiderivative isn't correct. Therefore my next question would be, if $$\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx$$ has a nice result. Edit2: After consulting computer algebra systems I got a result for the integral: $$\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx=\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx=-\frac{\ln(2)}{\pi}.$$ I don't know how it did the evaluation, but it seems consistent with the numerics. Edit3: Ok I got it. If we start from $$\int_0^z\pi x\tan(\pi x)\,dx,$$ after recognizing that $\pi\tan(\pi x)=-\frac{d}{dx}\ln\cos(\pi x)$ , we can do integration by parts to arrive at $$-z\ln\cos(\pi z)+\int_0^z\ln\cos(\pi x)\,dx.$$ After some manipulations, we can recognize the Clausen function: \begin{align}&-z\ln(2\cos(\pi z))+\int_0^z\ln(2\cos(\pi x))\,dx\\=&-z\ln(2\cos(\pi z))+\frac{1}{2\pi}\int_0^{2\pi z}\ln\left(2\cos\Big(\frac{x}{2}\Big)\right)\,dx\\=&-z\ln(2\cos(\pi z))+\frac{1}{2\pi}\mathrm{Cl}_2(\pi-2\pi z).\end{align} Now going back to the original integral, we have \begin{align}\int_0^{z}\tan(\pi x)(2x-1)\,dx&=2\int_0^{z}\tan(\pi x)x\,dx-\int_0^{z}\tan(\pi x)\,dx\\&=-\frac{2z}{\pi}\ln(2\cos(\pi z))+\frac{1}{\pi^2}\mathrm{Cl}_2(\pi-2\pi z)+\frac{\ln\cos(\pi z)}{\pi},\end{align} which in the limit $z\to\frac{1}{2}$ gives $-\frac{\ln(2)}{\pi}$ .","I want to evaluate the integral with the Cauchy principal value . This integral converges, but just to be sure I also looked at the much simpler integral this confirms the initial integral to be finite (in the Cauchy sense). To find the antiderivative I tried using Leibniz's formula on to arrive at Using the definition of the Cauchy principal value, namely the limit diverges for . I then also tried to manipulate the integral to become The value seems to be right, as I tried to evaluate the integral graphically, but I'd like to have the exact result. The other problem I encountered was that the antiderivative of the right integral also diverges for the upper limit: I really don't understand where things went wrong and any help is highly appreciated. Edit: I forgot to differentiate -.-, so the antiderivative isn't correct. Therefore my next question would be, if has a nice result. Edit2: After consulting computer algebra systems I got a result for the integral: I don't know how it did the evaluation, but it seems consistent with the numerics. Edit3: Ok I got it. If we start from after recognizing that , we can do integration by parts to arrive at After some manipulations, we can recognize the Clausen function: Now going back to the original integral, we have which in the limit gives .","\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx \mathrm{p.v} \mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx<\mathrm{p.v.}\int_0^1\frac{x}{\pi}\frac{2}{1-2x}=-\frac{1}{\pi}, \int\tan(ax)\,dx=-\frac{\ln\lvert\cos(ax)\rvert}{a} \int\tan(\pi x)x\,dx=\frac{d}{da}\left.\int\tan(ax)\,dx\right|_{a=\pi}=\frac{\ln\lvert \cos(\pi x)\rvert}{\pi^2} + \frac{x \tan(\pi x))}{\pi}. \begin{align}
\mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx &=\lim_{\varepsilon\to0^+}\left[\int_0^{\frac{1}{2}-\varepsilon}\tan(\pi x)x\,dx+\int_{\frac{1}{2}+\varepsilon}^1\tan(\pi x)x\,dx\right]\\[5pt]
&=\lim_{\varepsilon\to0^+}\left[\frac{\ln\cos\left\lvert\pi (\frac{1}{2}-\varepsilon)\right\rvert}{\pi^2} + \frac{(\frac{1}{2}-\varepsilon) \tan\left(\pi (\frac{1}{2}-\varepsilon)\right)}{\pi}\right.\\[5pt]
&\phantom{=\lim_{\varepsilon\to0^+}\left[\right.}\left.-\frac{\ln\cos\left\lvert\pi (\frac{1}{2}+\varepsilon)\right\rvert}{\pi^2} - \frac{(\frac{1}{2}+\varepsilon) \tan\left(\pi (\frac{1}{2}+\varepsilon)\right)}{\pi}\right]
\end{align} \varepsilon\to0^+ \mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx=\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx\approx-0.22063560. \int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx=\left[2\left(\frac{\ln\lvert \cos(\pi x)\rvert}{\pi^2} + \frac{x \tan(\pi x))}{\pi}\right)+\frac{\ln\lvert \cos(\pi x)\rvert}{\pi} \right]_0^{\frac{1}{2}}\to\infty. \tan(a x) \int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx \mathrm{p.v.}\int_0^1\tan(\pi x)x\,dx=\int_0^{\frac{1}{2}}\tan(\pi x)(2x-1)\,dx=-\frac{\ln(2)}{\pi}. \int_0^z\pi x\tan(\pi x)\,dx, \pi\tan(\pi x)=-\frac{d}{dx}\ln\cos(\pi x) -z\ln\cos(\pi z)+\int_0^z\ln\cos(\pi x)\,dx. \begin{align}&-z\ln(2\cos(\pi z))+\int_0^z\ln(2\cos(\pi x))\,dx\\=&-z\ln(2\cos(\pi z))+\frac{1}{2\pi}\int_0^{2\pi z}\ln\left(2\cos\Big(\frac{x}{2}\Big)\right)\,dx\\=&-z\ln(2\cos(\pi z))+\frac{1}{2\pi}\mathrm{Cl}_2(\pi-2\pi z).\end{align} \begin{align}\int_0^{z}\tan(\pi x)(2x-1)\,dx&=2\int_0^{z}\tan(\pi x)x\,dx-\int_0^{z}\tan(\pi x)\,dx\\&=-\frac{2z}{\pi}\ln(2\cos(\pi z))+\frac{1}{\pi^2}\mathrm{Cl}_2(\pi-2\pi z)+\frac{\ln\cos(\pi z)}{\pi},\end{align} z\to\frac{1}{2} -\frac{\ln(2)}{\pi}","['calculus', 'integration', 'definite-integrals', 'cauchy-principal-value']"
46,Hard integral - does it have a closed form?,Hard integral - does it have a closed form?,,"$$ \int_{-1}^0 \sqrt{ x + \ln  \left( \frac{1}{x+1} \right ) }  ~~~dx$$ All I could do is this: $$ f(x) = \sqrt{ x + \ln  \left( \frac{1}{x+1} \right ) }  = \sqrt{ x - \ln  \left( {x+1} \right ) }$$ Then: $$ u = x+ 1 \\ x = u - 1 \\ \Rightarrow du = dx $$ $$\sqrt{ u-1 - \ln  \left(u \right ) } ~~ du$$ But this is a deadend, because I don't know how to continue from this type of function.. Any help would be appreciated!","All I could do is this: Then: But this is a deadend, because I don't know how to continue from this type of function.. Any help would be appreciated!", \int_{-1}^0 \sqrt{ x + \ln  \left( \frac{1}{x+1} \right ) }  ~~~dx  f(x) = \sqrt{ x + \ln  \left( \frac{1}{x+1} \right ) }  = \sqrt{ x - \ln  \left( {x+1} \right ) }  u = x+ 1 \\ x = u - 1 \\ \Rightarrow du = dx  \sqrt{ u-1 - \ln  \left(u \right ) } ~~ du,['calculus']
47,"Spivak Chapter 10, Exercise 19 solution verification. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists.","Spivak Chapter 10, Exercise 19 solution verification. Prove that if  and  both exist, then  exists.",f^{(n)}(g(a)) g^{(n)}(a) (f\circ g)^{(n)}(a),"This question is asked here and here . I'm going to try to answer it for my own edification, and for the next poor soul that comes across it and has trouble :) I'm using Spivak's Answer Book solution, but adding notes to hopefully clarify things. Here goes: From Calculus by Michael Spivak 3rd Edition, Chapter 10, Exercise 19. 10-19. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. A little experimentation should convince you that it is unwise to seek a formula for $(f\circ g)^{(n)}(a)$ . In order to prove that $(f\circ g)^{(n)}(a)$ exists you will therefore have to devise a reasonable assertion about $(f\circ g)^{(n)}(a)$ which can be proved by induction. Try something like: "" $(f\circ g)^{(n)}(a)$ exists and is a sum of terms each of which is a product of terms of the form..."" First, let's look at the first few derivatives of $(f\circ g)$ . (These are calculated via straightforward application of the Chain Rule): $$(f\circ g)^{\prime}(x) = f^{\prime}(g(x)) \cdot g^{\prime}(x)$$ $$(f\circ g)^{\prime\prime}(x) = f^{\prime\prime}(g(x)) \cdot g^{\prime}(x)^2 + f^{\prime}(g(x)) \cdot g^{\prime\prime}(x)$$ $$(f\circ g)^{\prime\prime\prime}(x) = f^{\prime\prime\prime}(g(x)) \cdot g^{\prime}(x)^3 + 3f^{\prime\prime}(g(x)) \cdot g^{\prime}(x)g^{\prime\prime}(x) + f^{\prime}(g(x)) \cdot g^{\prime\prime\prime}(x)$$ These suggest a general form for derivatives of $(f\circ g)$ : the $n$ -th derivative of $f\circ g$ seems to be made up of a sum of terms that are each the product of some constant, times some derivative of $f$ at $g(x)$ , times some derivatives of $g$ at $x$ , with these derivatives of $g$ perhaps raised to some power. None of the derivatives (of $f$ or $g$ ) are of order higher than $n$ . Conjecture: If $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ for some number $c$ , nonnegative integers $m_1,\dots,m_n$ and a natural number $k \leq n$ . (Aside: The conjecture concerns the existence of the derivative at a single point , $a$ . It says, ""if $g$ is $n$ -times differentiable at $a$ and $f$ is $n$ -times differentiable at $g(a)$ , then $f\circ g$ is $n$ -times differentiable at $a$ . If there's a second point $b$ such that $g$ is $n$ -times differentiable at $b$ and $f$ is $n$ -times differentiable at $g(b)$ , then $f\circ g$ is $n$ -times differentiable at $b$ , and $(f\circ g)^{(n)}(b)$ is the sum of terms of the form $$c\cdot[g^{\prime}(b)]^{m_1}\cdot[g^{\prime\prime}(b)]^{m_2}\cdots[g^{(n)}(b)]^{m_n}\cdot f^{(k)}(g(b))$$ where $c$ , $m_1, \dots, m_n$ , $k$ in each term are all identical to the corresponding term for $(f\circ g)^{(n)}(a)$ .) Proof: Let's restate the conjecture we're trying to prove. Conjecture: If $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ Proof of the conjecture is by induction on $n$ Case for $n = 1$ If $f^{\prime}(g(a))$ and $g^{\prime}(a)$ both exist, then the Chain rule states that $(f\circ g)^{\prime}(a)$ exists and is equal to $f^{\prime}(g(a)) \cdot g^{\prime}(a)$ . Thus our conjecture is true for $n = 1$ (with $c = m_1 = k = 1$ ). Case for $n + 1$ We will assume the conjecture is true for $n$ , that is, if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ Note that we are not assuming that $(f\circ g)^{(n)}(a)$ exists. Our assumption is that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. (This distinction is important and sets this proof apart from inductive arguments I've previously encountered. Our assumption isn't simply ""thing is true"". Our assumption is that "" if this condition is met, thing is true."" This is in agreement with the theorem we're trying to prove. The theorem doesn't say the $n$ -th derivative of $f\circ g$ exists. Only that, if certain conditions are met, the derivative exists.) Now we need to prove the conjecture for the $n + 1$ case. We need to show that if $f^{(n+1)}(g(a))$ and $g^{(n+1)}(a)$ both exist, then $(f\circ g)^{(n+1)}(a)$ exists, and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdots[g^{(n+1)}(a)]^{m_{n+1}}\cdot f^{(k)}(g(a))$$ with $k \leq n+1$ . Suppose $f^{(n+1)}(g(a))$ and $g^{(n+1)}(a)$ both exist. This means that $f^{(n)}(y)$ exists for all $y$ in some interval around $g(a)$ , and likewise $g^{(n)}(x)$ exists for all $x$ in some interval around $a$ . Why? Because, by definition the $(n+1)$ derivatives are $$f^{(n+1)}(g(a)) = \lim_{y \to g(a)}\frac{f^{(n)}(y) - f^{(n)}(g(a))}{y-g(a)}$$ $$g^{(n+1)}(a) = \lim_{x \to a}\frac{g^{(n)}(x) - g^{(n)}(a)}{x-a}$$ and in order for these limits to exist, the $n$ -order derivatives must exist in some intervals around these points. Furthermore, $f^{(k)}$ and $g^{(k)}$ must also exist in these intervals for all $1\leq k < n$ . Because $f^{(n)}$ exists at and around $g(a)$ , there is some $\varepsilon_f > 0$ such that for all $y$ if $|y - g(a)| < \varepsilon_f$ , $f^{(n)}(y)$ exists. Likewise, since $g^{(n)}$ exists at and around $a$ there is some $\delta_g >0$ such that for all $x$ if $|x-a| < \delta_g$ , $g^{(n)}(x)$ exists. We know that $g$ is continuous at $a$ (it's differentiable). As such there exists some $\delta_f > 0$ such that for all $x$ if $|x-a| < \delta_f$ , then $|g(x) - g(a)| < \varepsilon_f$ . If we use $\delta_{min} = \min(\delta_g,\delta_f)$ then for all $x$ if $|x-a| < \delta_{min}$ , both $g^{(n)}(x)$ and $f^{(n)}(g(x))$ will exist. In this interval both $g^{(n)}(x)$ and $f^{(n)}(g(x))$ exist, so the $n$ -case assumption tells us $(f\circ g)^{(n)}(x)$ exists and is a sum of terms of the form $$c\cdot[g^{\prime}(x)]^{m_1}\cdot[g^{\prime\prime}(x)]^{m_2}\cdots[g^{(n)}(x)]^{m_n}\cdot f^{(k)}(g(x))$$ Worth noting: we're no longer only talking about the value of $(f\circ g)^{(n)}$ at a single point $a$ . $(f\circ g)^{(n)}(x)$ exists for all $x$ in this interval around $a$ . $(f\circ g)^{(n)}$ is a function that's defined at and around $a$ . Furthermore, $(f\circ g)^{(n)}(x)$ is the sum of products of constants and derivatives of $f$ and $g$ and these derivatives are all themselves differentiable at $a$ Therefore, the derivative of $(f\circ g)^{(n)}$ at $a$ exists and can be calculated using the standard, sum, product and chain rules for derivatives. Doing so will result in terms that look like either this $$c\cdot[g^{\prime}(a)]^{m_1}\cdots m_{\alpha}[g^{(\alpha)}(a)]^{m_{\alpha}-1}\cdot[g^{(\alpha+1)}(a)]^{m_{\alpha+1}+1}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a))$$ this $$c\cdot[g^{\prime}(a)]^{m_1}\cdots m_n[g^{(n)}(a)]^{m_n-1}\cdot[g^{(n+1)}(a)]\cdot f^{(k)}(g(a))$$ or this $$c\cdot[g^{\prime}(a)]^{m_1+1}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k+1)}(g(a))$$ all of which fit the required form for terms of the $n+1$ case. Therefore, if $f^{(n+1)}(g(a))$ and $g^{(n+1)}(a)$ both exist, then $((f\circ g)^{(n)})^{\prime}(a) = (f\circ g)^{(n+1)}(a)$ exists, and is a sum of terms of the form $$c\cdot[g^{\prime}(a)]^{m_1}\cdots[g^{(n+1)}(a)]^{m_{n+1}}\cdot f^{(k)}(g(a))$$ with $k \leq n+1$ . This completes the proof. I hope.","This question is asked here and here . I'm going to try to answer it for my own edification, and for the next poor soul that comes across it and has trouble :) I'm using Spivak's Answer Book solution, but adding notes to hopefully clarify things. Here goes: From Calculus by Michael Spivak 3rd Edition, Chapter 10, Exercise 19. 10-19. Prove that if and both exist, then exists. A little experimentation should convince you that it is unwise to seek a formula for . In order to prove that exists you will therefore have to devise a reasonable assertion about which can be proved by induction. Try something like: "" exists and is a sum of terms each of which is a product of terms of the form..."" First, let's look at the first few derivatives of . (These are calculated via straightforward application of the Chain Rule): These suggest a general form for derivatives of : the -th derivative of seems to be made up of a sum of terms that are each the product of some constant, times some derivative of at , times some derivatives of at , with these derivatives of perhaps raised to some power. None of the derivatives (of or ) are of order higher than . Conjecture: If and both exist, then exists and is a sum of terms of the form for some number , nonnegative integers and a natural number . (Aside: The conjecture concerns the existence of the derivative at a single point , . It says, ""if is -times differentiable at and is -times differentiable at , then is -times differentiable at . If there's a second point such that is -times differentiable at and is -times differentiable at , then is -times differentiable at , and is the sum of terms of the form where , , in each term are all identical to the corresponding term for .) Proof: Let's restate the conjecture we're trying to prove. Conjecture: If and both exist, then exists and is a sum of terms of the form Proof of the conjecture is by induction on Case for If and both exist, then the Chain rule states that exists and is equal to . Thus our conjecture is true for (with ). Case for We will assume the conjecture is true for , that is, if and both exist, then exists and is a sum of terms of the form Note that we are not assuming that exists. Our assumption is that if and both exist, then exists. (This distinction is important and sets this proof apart from inductive arguments I've previously encountered. Our assumption isn't simply ""thing is true"". Our assumption is that "" if this condition is met, thing is true."" This is in agreement with the theorem we're trying to prove. The theorem doesn't say the -th derivative of exists. Only that, if certain conditions are met, the derivative exists.) Now we need to prove the conjecture for the case. We need to show that if and both exist, then exists, and is a sum of terms of the form with . Suppose and both exist. This means that exists for all in some interval around , and likewise exists for all in some interval around . Why? Because, by definition the derivatives are and in order for these limits to exist, the -order derivatives must exist in some intervals around these points. Furthermore, and must also exist in these intervals for all . Because exists at and around , there is some such that for all if , exists. Likewise, since exists at and around there is some such that for all if , exists. We know that is continuous at (it's differentiable). As such there exists some such that for all if , then . If we use then for all if , both and will exist. In this interval both and exist, so the -case assumption tells us exists and is a sum of terms of the form Worth noting: we're no longer only talking about the value of at a single point . exists for all in this interval around . is a function that's defined at and around . Furthermore, is the sum of products of constants and derivatives of and and these derivatives are all themselves differentiable at Therefore, the derivative of at exists and can be calculated using the standard, sum, product and chain rules for derivatives. Doing so will result in terms that look like either this this or this all of which fit the required form for terms of the case. Therefore, if and both exist, then exists, and is a sum of terms of the form with . This completes the proof. I hope.","f^{(n)}(g(a)) g^{(n)}(a) (f\circ g)^{(n)}(a) (f\circ g)^{(n)}(a) (f\circ g)^{(n)}(a) (f\circ g)^{(n)}(a) (f\circ g)^{(n)}(a) (f\circ g) (f\circ g)^{\prime}(x) = f^{\prime}(g(x)) \cdot g^{\prime}(x) (f\circ g)^{\prime\prime}(x) = f^{\prime\prime}(g(x)) \cdot g^{\prime}(x)^2 + f^{\prime}(g(x)) \cdot g^{\prime\prime}(x) (f\circ g)^{\prime\prime\prime}(x) = f^{\prime\prime\prime}(g(x)) \cdot g^{\prime}(x)^3 + 3f^{\prime\prime}(g(x)) \cdot g^{\prime}(x)g^{\prime\prime}(x) + f^{\prime}(g(x)) \cdot g^{\prime\prime\prime}(x) (f\circ g) n f\circ g f g(x) g x g f g n f^{(n)}(g(a)) g^{(n)}(a) (f\circ g)^{(n)}(a) c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a)) c m_1,\dots,m_n k \leq n a g n a f n g(a) f\circ g n a b g n b f n g(b) f\circ g n b (f\circ g)^{(n)}(b) c\cdot[g^{\prime}(b)]^{m_1}\cdot[g^{\prime\prime}(b)]^{m_2}\cdots[g^{(n)}(b)]^{m_n}\cdot f^{(k)}(g(b)) c m_1, \dots, m_n k (f\circ g)^{(n)}(a) f^{(n)}(g(a)) g^{(n)}(a) (f\circ g)^{(n)}(a) c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a)) n n = 1 f^{\prime}(g(a)) g^{\prime}(a) (f\circ g)^{\prime}(a) f^{\prime}(g(a)) \cdot g^{\prime}(a) n = 1 c = m_1 = k = 1 n + 1 n f^{(n)}(g(a)) g^{(n)}(a) (f\circ g)^{(n)}(a) c\cdot[g^{\prime}(a)]^{m_1}\cdot[g^{\prime\prime}(a)]^{m_2}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a)) (f\circ g)^{(n)}(a) f^{(n)}(g(a)) g^{(n)}(a) (f\circ g)^{(n)}(a) n f\circ g n + 1 f^{(n+1)}(g(a)) g^{(n+1)}(a) (f\circ g)^{(n+1)}(a) c\cdot[g^{\prime}(a)]^{m_1}\cdots[g^{(n+1)}(a)]^{m_{n+1}}\cdot f^{(k)}(g(a)) k \leq n+1 f^{(n+1)}(g(a)) g^{(n+1)}(a) f^{(n)}(y) y g(a) g^{(n)}(x) x a (n+1) f^{(n+1)}(g(a)) = \lim_{y \to g(a)}\frac{f^{(n)}(y) - f^{(n)}(g(a))}{y-g(a)} g^{(n+1)}(a) = \lim_{x \to a}\frac{g^{(n)}(x) - g^{(n)}(a)}{x-a} n f^{(k)} g^{(k)} 1\leq k < n f^{(n)} g(a) \varepsilon_f > 0 y |y - g(a)| < \varepsilon_f f^{(n)}(y) g^{(n)} a \delta_g >0 x |x-a| < \delta_g g^{(n)}(x) g a \delta_f > 0 x |x-a| < \delta_f |g(x) - g(a)| < \varepsilon_f \delta_{min} = \min(\delta_g,\delta_f) x |x-a| < \delta_{min} g^{(n)}(x) f^{(n)}(g(x)) g^{(n)}(x) f^{(n)}(g(x)) n (f\circ g)^{(n)}(x) c\cdot[g^{\prime}(x)]^{m_1}\cdot[g^{\prime\prime}(x)]^{m_2}\cdots[g^{(n)}(x)]^{m_n}\cdot f^{(k)}(g(x)) (f\circ g)^{(n)} a (f\circ g)^{(n)}(x) x a (f\circ g)^{(n)} a (f\circ g)^{(n)}(x) f g a (f\circ g)^{(n)} a c\cdot[g^{\prime}(a)]^{m_1}\cdots m_{\alpha}[g^{(\alpha)}(a)]^{m_{\alpha}-1}\cdot[g^{(\alpha+1)}(a)]^{m_{\alpha+1}+1}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k)}(g(a)) c\cdot[g^{\prime}(a)]^{m_1}\cdots m_n[g^{(n)}(a)]^{m_n-1}\cdot[g^{(n+1)}(a)]\cdot f^{(k)}(g(a)) c\cdot[g^{\prime}(a)]^{m_1+1}\cdots[g^{(n)}(a)]^{m_n}\cdot f^{(k+1)}(g(a)) n+1 f^{(n+1)}(g(a)) g^{(n+1)}(a) ((f\circ g)^{(n)})^{\prime}(a) = (f\circ g)^{(n+1)}(a) c\cdot[g^{\prime}(a)]^{m_1}\cdots[g^{(n+1)}(a)]^{m_{n+1}}\cdot f^{(k)}(g(a)) k \leq n+1","['calculus', 'derivatives', 'solution-verification', 'induction', 'chain-rule']"
48,Prove that $\int_0^b x^3 dx = \frac{b^4}{4}$,Prove that,\int_0^b x^3 dx = \frac{b^4}{4},"The problem is as below: $\textbf{Problem 1}$ Prove that $\int_0^b x^3 dx = \dfrac{b^4}{4}$ by considering partitions into $n$ equal subintervals, using the  formula for $\sum_{i=1}^n i^3$ which was found in problem 2-6.    This problem requries only a straightforwad imitation of calculations in the text, but you should write it as a formal proof to make certain that all the fine points of the argument are clear. I'm using the 4th Ed. of Calculus by Michael Spivak. I'm looking for some criticism on my proof-writing. Here is my solution: We consider the interval $[0,b]$ of the function $f(x) = x^3$ . We use lower and upper sums in our proof. First, let $P = \{t_0, \dots, t_n\}$ be a partition of $[0,b]$ . Suppose that these partitions divide $[0,b]$ into $n$ equal subintervals. Then, the length of each subinterval is $\dfrac{b}{n}$ . Specifically, we have that $t_0 = 0, t_1 = \dfrac{b}{n}, t_2 = \dfrac{2b}{n}$ . So, each partition $t_i = \dfrac{ib}{n}$ . So, we define $$ 	m_i = t_{i-1}^3 \quad M_i = t_i^3 $$ so we have that $$ 	L(f, P) = \sum_{i=1}^n t_{i-1}^3 \dfrac{b}{n} $$ and specifically, $$ 	L(f, P) = \sum_{i=1}^n \dfrac{(i-1)^3b^3}{n^3} \cdot \dfrac{b}{n} $$ Moreover, $$ 	L(f, P) = \sum_{j=0}^{n-1} j^3 \cdot \dfrac{b^4}{n^4} $$ Which simplifies to: $$ 	L(f, P) = (\dfrac{n(n-1)}{2})^2 \cdot \dfrac{b^4}{n^4} $$ The last steps: \begin{align*} 	L(f, P) &= (\dfrac{n^2(n-1)^2}{4}) \cdot \dfrac{b^4}{n^4} 	\\&= \dfrac{(n-1)^2}{4} \cdot \dfrac{b^4}{n^2} 	\\&= \dfrac{(n-1)^2}{n^2} \cdot \dfrac{b^4}{4} \end{align*} The process for the upper sum is similar, we take the same partition: \begin{align*} 	U(f, P) &= \sum_{i=1}^n (\dfrac{ib}{n})^3 \cdot \dfrac{b}{n} 		\\&= \sum_i^n i^3 \cdot \dfrac{b^4}{n^4} 		\\&= \dfrac{n^2(n+1)^2}{4} \cdot \dfrac{b^4}{n^4} 		\\&= \dfrac{(n+1)^2}{n^2} \cdot \dfrac{b^4}{4} \end{align*} Now, it is clear that $L(f, P) \leq \dfrac{b^4}{4} \leq U(f, P)$ for all $P$ of equal partition width. By choosing $n$ sufficiently large, we can make $U(f, P_n) - L(f, P_n)$ as small as desired where $n$ is the number of sub-intervals. Taking the limits of both sides to infinity, by squeeze theorem, we see that $\lim_{n \to \infty} L(f, P_n) = \lim_{n \to \infty} U(f, P_n)$ and so the integral is $\dfrac{b^4}{4}$ for the function $f(x) = x^3$ .","The problem is as below: Prove that by considering partitions into equal subintervals, using the  formula for which was found in problem 2-6.    This problem requries only a straightforwad imitation of calculations in the text, but you should write it as a formal proof to make certain that all the fine points of the argument are clear. I'm using the 4th Ed. of Calculus by Michael Spivak. I'm looking for some criticism on my proof-writing. Here is my solution: We consider the interval of the function . We use lower and upper sums in our proof. First, let be a partition of . Suppose that these partitions divide into equal subintervals. Then, the length of each subinterval is . Specifically, we have that . So, each partition . So, we define so we have that and specifically, Moreover, Which simplifies to: The last steps: The process for the upper sum is similar, we take the same partition: Now, it is clear that for all of equal partition width. By choosing sufficiently large, we can make as small as desired where is the number of sub-intervals. Taking the limits of both sides to infinity, by squeeze theorem, we see that and so the integral is for the function .","\textbf{Problem 1} \int_0^b x^3 dx = \dfrac{b^4}{4} n \sum_{i=1}^n i^3 [0,b] f(x) = x^3 P = \{t_0, \dots, t_n\} [0,b] [0,b] n \dfrac{b}{n} t_0 = 0, t_1 = \dfrac{b}{n}, t_2 = \dfrac{2b}{n} t_i = \dfrac{ib}{n} 
	m_i = t_{i-1}^3 \quad M_i = t_i^3
 
	L(f, P) = \sum_{i=1}^n t_{i-1}^3 \dfrac{b}{n}
 
	L(f, P) = \sum_{i=1}^n \dfrac{(i-1)^3b^3}{n^3} \cdot \dfrac{b}{n}
 
	L(f, P) = \sum_{j=0}^{n-1} j^3 \cdot \dfrac{b^4}{n^4}
 
	L(f, P) = (\dfrac{n(n-1)}{2})^2 \cdot \dfrac{b^4}{n^4}
 \begin{align*}
	L(f, P) &= (\dfrac{n^2(n-1)^2}{4}) \cdot \dfrac{b^4}{n^4}
	\\&= \dfrac{(n-1)^2}{4} \cdot \dfrac{b^4}{n^2}
	\\&= \dfrac{(n-1)^2}{n^2} \cdot \dfrac{b^4}{4}
\end{align*} \begin{align*}
	U(f, P) &= \sum_{i=1}^n (\dfrac{ib}{n})^3 \cdot \dfrac{b}{n}
		\\&= \sum_i^n i^3 \cdot \dfrac{b^4}{n^4}
		\\&= \dfrac{n^2(n+1)^2}{4} \cdot \dfrac{b^4}{n^4}
		\\&= \dfrac{(n+1)^2}{n^2} \cdot \dfrac{b^4}{4}
\end{align*} L(f, P) \leq \dfrac{b^4}{4} \leq U(f, P) P n U(f, P_n) - L(f, P_n) n \lim_{n \to \infty} L(f, P_n) = \lim_{n \to \infty} U(f, P_n) \dfrac{b^4}{4} f(x) = x^3","['calculus', 'solution-verification']"
49,Use Green's Theorem to Prove Change of Variable for Double Integral,Use Green's Theorem to Prove Change of Variable for Double Integral,,"Question: Use Green's Theorem to prove: $$\iint_{R}dxdy=\iint_{S}\begin{vmatrix}\frac{\partial(x,y)}{\partial(u,v)}\end{vmatrix}dudv$$ $\Big(\frac{\partial(x,y)}{\partial(u,v)} $ is the Jacobian of the transformation $\Big)$ I found this question from the Stewart's Calculus Book. The question only states that $x=g(u,v)$ and $y=h(u,v)$ are the transformation (I suppose it is one-to-one, because in order to apply this, the transformation has to be one-to-one). Here comes the problem. Let C be a positively-oriented boundary curve of R (simple closed), and $Q=x ,P=0$ so that $\frac{\partial{Q}}{\partial{x}}-\frac{\partial{P}}{\partial{y}}=1$ and $S$ be the region of transformed R, then: \begin{align} \iint_{R}dxdy&=\oint_CP \hspace{0.1cm}dx+Q\hspace{0.1cm}dy\\ &=\oint_Cx\hspace{0.1cm}dy\\ &=\oint_Cg(u,v)\,(h_udu+h_vdv)\\ &=\iint_Sg_uh_v+gh_{vu}-g_vh_u-gh_{uv}\;dA \end{align} The result can be obtained if $h_{vu}=h_{uv}$ . However, it is not obvious to me that these are equal. Also, doing this way provides no information for the 'absolute' of the Jacobian. Can someone please point out my problems? Thank you. ADD ON: There has been no answer for three days..., I can't really think out of a solution on my own, I will appreciate a lot if someone answer me, thanks.","Question: Use Green's Theorem to prove: is the Jacobian of the transformation I found this question from the Stewart's Calculus Book. The question only states that and are the transformation (I suppose it is one-to-one, because in order to apply this, the transformation has to be one-to-one). Here comes the problem. Let C be a positively-oriented boundary curve of R (simple closed), and so that and be the region of transformed R, then: The result can be obtained if . However, it is not obvious to me that these are equal. Also, doing this way provides no information for the 'absolute' of the Jacobian. Can someone please point out my problems? Thank you. ADD ON: There has been no answer for three days..., I can't really think out of a solution on my own, I will appreciate a lot if someone answer me, thanks.","\iint_{R}dxdy=\iint_{S}\begin{vmatrix}\frac{\partial(x,y)}{\partial(u,v)}\end{vmatrix}dudv \Big(\frac{\partial(x,y)}{\partial(u,v)}  \Big) x=g(u,v) y=h(u,v) Q=x ,P=0 \frac{\partial{Q}}{\partial{x}}-\frac{\partial{P}}{\partial{y}}=1 S \begin{align}
\iint_{R}dxdy&=\oint_CP \hspace{0.1cm}dx+Q\hspace{0.1cm}dy\\
&=\oint_Cx\hspace{0.1cm}dy\\
&=\oint_Cg(u,v)\,(h_udu+h_vdv)\\
&=\iint_Sg_uh_v+gh_{vu}-g_vh_u-gh_{uv}\;dA
\end{align} h_{vu}=h_{uv}","['calculus', 'change-of-variable', 'greens-theorem']"
50,Is this a valid way of deriving the area of a circle?,Is this a valid way of deriving the area of a circle?,,"On the Wikipedia article about deriving the area of a circle, it mentions that the formula $$ \text{area} = \pi r^2 $$ can be derived by evaluating the integral $$ 2 \int_{-r}^{r} \sqrt{r^2-x^2} \, dx \, . $$ However, the article also seems to suggest that this can result in a circular argument if it is not done carefully: This particular proof may appear to beg the question, if the sine and cosine functions involved in the trigonometric substitution are regarded as being defined in relation to circles. However, as noted earlier, it is possible to define sine, cosine, and $\pi$ in a way that is totally independent of trigonometry, in which case the proof is valid by the change of variables formula and Fubini's theorem, assuming the basic properties of sine and cosine (which can also be proved without assuming anything about their relation to circles). My question is: can the following argument be used to prove that the area of a circle is $\pi r^2$ ? Let $$ I=\int \sqrt{r^2-x^2} \, dx = \int \sqrt{r^2\left(1-\left(\frac{x}{r}\right)^2\right)} \, dx \, = r\int \sqrt{1-\left(\frac{x}{r}\right)^2} \, dx . $$ Then let $\theta = \arcsin(x/r)$ so that $x=r\sin\theta$ . Since $dx = r\cos\theta \, d\theta$ , the integral is transformed to $$ r^2\int\sqrt{1-\sin^2\theta}\, \cos \theta  \, d\theta = r^2\int \cos^2\theta \, d\theta \, . $$ Finally, we may use the identity $$ \cos^2\theta \equiv \frac{\cos(2\theta)+1}{2} $$ to obtain $$ r^2 \int \frac{\cos(2\theta)+1}{2} \, d\theta = r^2 \int \frac{\cos(2\theta)+1}{2} \, d\theta = r^2\left(\frac{\sin(2\theta)}{4}+\frac{\theta}{2}\right) \, . $$ Note that when $x=r$ , $\theta = \arcsin(1) = \pi/2$ ; when $x=-r$ , $\theta = - \pi/2$ . So the area of a circle is $$ 2 \int_{-r}^{r} \sqrt{r^2-x^2} \, dx \, = 2 \left[r^2\left(\frac{\sin(2\theta)}{4}+\frac{\theta}{2}\right)\right]_{-\pi/2}^{\pi/2} = \pi r^2 \, . $$ Admittedly, this was not the most elegant of proofs, but I'm wondering if it is mathematically rigorous.","On the Wikipedia article about deriving the area of a circle, it mentions that the formula can be derived by evaluating the integral However, the article also seems to suggest that this can result in a circular argument if it is not done carefully: This particular proof may appear to beg the question, if the sine and cosine functions involved in the trigonometric substitution are regarded as being defined in relation to circles. However, as noted earlier, it is possible to define sine, cosine, and in a way that is totally independent of trigonometry, in which case the proof is valid by the change of variables formula and Fubini's theorem, assuming the basic properties of sine and cosine (which can also be proved without assuming anything about their relation to circles). My question is: can the following argument be used to prove that the area of a circle is ? Let Then let so that . Since , the integral is transformed to Finally, we may use the identity to obtain Note that when , ; when , . So the area of a circle is Admittedly, this was not the most elegant of proofs, but I'm wondering if it is mathematically rigorous.","
\text{area} = \pi r^2
 
2 \int_{-r}^{r} \sqrt{r^2-x^2} \, dx \, .
 \pi \pi r^2 
I=\int \sqrt{r^2-x^2} \, dx = \int \sqrt{r^2\left(1-\left(\frac{x}{r}\right)^2\right)} \, dx \, = r\int \sqrt{1-\left(\frac{x}{r}\right)^2} \, dx .
 \theta = \arcsin(x/r) x=r\sin\theta dx = r\cos\theta \, d\theta 
r^2\int\sqrt{1-\sin^2\theta}\, \cos \theta  \, d\theta = r^2\int \cos^2\theta \, d\theta \, .
 
\cos^2\theta \equiv \frac{\cos(2\theta)+1}{2}
 
r^2 \int \frac{\cos(2\theta)+1}{2} \, d\theta = r^2 \int \frac{\cos(2\theta)+1}{2} \, d\theta = r^2\left(\frac{\sin(2\theta)}{4}+\frac{\theta}{2}\right) \, .
 x=r \theta = \arcsin(1) = \pi/2 x=-r \theta = - \pi/2 
2 \int_{-r}^{r} \sqrt{r^2-x^2} \, dx \, = 2 \left[r^2\left(\frac{\sin(2\theta)}{4}+\frac{\theta}{2}\right)\right]_{-\pi/2}^{\pi/2} = \pi r^2 \, .
","['calculus', 'integration', 'definite-integrals', 'circles', 'area']"
51,"If $ f_n\to f $ pointwise, and $f_n $ are increasing continuous functions, and $ f $ is continuous, then $f_n\to f $ uniformly [duplicate]","If  pointwise, and  are increasing continuous functions, and  is continuous, then  uniformly [duplicate]", f_n\to f  f_n   f  f_n\to f ,"This question already has answers here : Sequence of monotone functions converging to a continuous limit, is the convergence uniform? (2 answers) Closed 3 years ago . let $ f_{n}:[a,b]\to\mathbb{R} $ be a sequence of increasing and continuous functions (each function is continuous and increasing, it dosent mean that the sequence is increasing). And assume that exists $ f:[a,b]\to\mathbb{R} $ such that $ f $ is continuous and $ f_{n}\to f $ pointwise. I have to prove that the convergence is also uniformly. I tried to  assume by contradiction that there is no uniformly convergence, but I got stuck. Now I have a new intuitio, since $ f_n $ and $ f $ are continious in $ [a,b] $ they are uniformly continuous. so actually for any $ x $ I can write: $ |f_{n}\left(x\right)-f\left(x\right)|\leq|f_{n}\left(x\right)-f_{n}\left(x+\delta\right)|+|f\left(x\right)-f\left(x+\delta\right)|+|f_{n}\left(x+\delta\right)-f_{n}\left(x+\delta\right)| $ Where $ \delta $ would fit to some $ \varepsilon $ from the unifromly continuous definition, where the first two terms can be small as I want, I can continue with it untill $ x+k\delta $ ""is close enough"" to $ b $ where $ k \in \mathbb{R} $ . But again, Im not sure how to do it since $ \delta $ would have to depend on $ n $ . Any hints would help. Thanks in advance","This question already has answers here : Sequence of monotone functions converging to a continuous limit, is the convergence uniform? (2 answers) Closed 3 years ago . let be a sequence of increasing and continuous functions (each function is continuous and increasing, it dosent mean that the sequence is increasing). And assume that exists such that is continuous and pointwise. I have to prove that the convergence is also uniformly. I tried to  assume by contradiction that there is no uniformly convergence, but I got stuck. Now I have a new intuitio, since and are continious in they are uniformly continuous. so actually for any I can write: Where would fit to some from the unifromly continuous definition, where the first two terms can be small as I want, I can continue with it untill ""is close enough"" to where . But again, Im not sure how to do it since would have to depend on . Any hints would help. Thanks in advance"," f_{n}:[a,b]\to\mathbb{R}   f:[a,b]\to\mathbb{R}   f   f_{n}\to f   f_n   f   [a,b]   x   |f_{n}\left(x\right)-f\left(x\right)|\leq|f_{n}\left(x\right)-f_{n}\left(x+\delta\right)|+|f\left(x\right)-f\left(x+\delta\right)|+|f_{n}\left(x+\delta\right)-f_{n}\left(x+\delta\right)|   \delta   \varepsilon   x+k\delta   b   k \in \mathbb{R}   \delta   n ","['calculus', 'sequence-of-function']"
52,"If $f(x)=\sum_{n=1}^{50}(x-n)^{51n-n^2},$ then find $\frac{f(51)}{f'(51)}.$",If  then find,"f(x)=\sum_{n=1}^{50}(x-n)^{51n-n^2}, \frac{f(51)}{f'(51)}.","My work so far follows: We have: $$\ln f(x)=\ln\left[\sum_{n=1}^{50} (x-n)^{51n-n^2}\right].$$ Now differentiating both sides w.r.t. $x,$ we obtain: $$\frac{f'(x)}{f(x)}=\frac{\displaystyle\sum_{n=1}^{50}(51n-n^2)(x-n)^{51n-n^2-1}}{\displaystyle\sum_{n=1}^{50}(x-n)^{51n-n^2}}.$$ Plugging in $x=51,$ we have: $$\frac{f'(51)}{f(51)}=\frac{\displaystyle\sum_{n=1}^{50}(51n-n^2)(51-n)^{51n-n^2-1}}{\displaystyle\sum_{n=1}^{50}(51-n)^{51n-n^2}}\\=\frac{\displaystyle\sum_{n=1}^{50}n(51-n)^{51n-n^2}}{\displaystyle\sum_{n=1}^{50}(51-n)^{51n-n^2}}.$$ How to handle this messy fraction? Please suggest..",My work so far follows: We have: Now differentiating both sides w.r.t. we obtain: Plugging in we have: How to handle this messy fraction? Please suggest..,"\ln f(x)=\ln\left[\sum_{n=1}^{50} (x-n)^{51n-n^2}\right]. x, \frac{f'(x)}{f(x)}=\frac{\displaystyle\sum_{n=1}^{50}(51n-n^2)(x-n)^{51n-n^2-1}}{\displaystyle\sum_{n=1}^{50}(x-n)^{51n-n^2}}. x=51, \frac{f'(51)}{f(51)}=\frac{\displaystyle\sum_{n=1}^{50}(51n-n^2)(51-n)^{51n-n^2-1}}{\displaystyle\sum_{n=1}^{50}(51-n)^{51n-n^2}}\\=\frac{\displaystyle\sum_{n=1}^{50}n(51-n)^{51n-n^2}}{\displaystyle\sum_{n=1}^{50}(51-n)^{51n-n^2}}.","['calculus', 'sequences-and-series', 'derivatives']"
53,Solve: $f''(x) = f^{-1}(x)$,Solve:,f''(x) = f^{-1}(x),I found this differential equation $$ f''(x) = f^{-1}(x) $$ which is interesting to me. I wonder if there is a calculus oriented way to solve this differential equation. Is there some method to to solve differential equations containing inverse of a function ?,I found this differential equation which is interesting to me. I wonder if there is a calculus oriented way to solve this differential equation. Is there some method to to solve differential equations containing inverse of a function ?, f''(x) = f^{-1}(x) ,"['calculus', 'ordinary-differential-equations', 'polynomials', 'inverse-function', 'golden-ratio']"
54,Inequality involving $-x\log(x)$,Inequality involving,-x\log(x),"Suppose that $$\exp(-a + b - \log(c)) \leq x \leq \exp(a + b -\log(c)).$$ Moreover, suppose that $0 \leq x \leq \frac{1}{e}$ . I would like to conclude that $$-x\log(x) \leq \frac{-\exp(-a +b)}{c}(a + b - \log(c)).$$ First, observe that $$x \geq \exp(-a + b - \log(c)) \implies-x \leq -\exp(-a + b - \log(c)).$$ Furthermore, since $\log(x)$ is an increasing function, we have that $$\log(x) \leq a + b -\log(c).$$ If we didn't need to care about signs, we could conclude our result.  I think I need to use the fact that $f(x) = -x\log(x)$ is increasing in $(0, 1/e]$ and that $\log(x) < 0$ in $(0, 1/e]$ .","Suppose that Moreover, suppose that . I would like to conclude that First, observe that Furthermore, since is an increasing function, we have that If we didn't need to care about signs, we could conclude our result.  I think I need to use the fact that is increasing in and that in .","\exp(-a + b - \log(c)) \leq x \leq \exp(a + b -\log(c)). 0 \leq x \leq \frac{1}{e} -x\log(x) \leq \frac{-\exp(-a +b)}{c}(a + b - \log(c)). x \geq \exp(-a + b - \log(c)) \implies-x \leq -\exp(-a + b - \log(c)). \log(x) \log(x) \leq a + b -\log(c). f(x) = -x\log(x) (0, 1/e] \log(x) < 0 (0, 1/e]","['calculus', 'algebra-precalculus', 'inequality', 'logarithms']"
55,"If $f''(x)+f(x)>0$ and $f(x)>0$ $\forall x\in(a,b)$; $f(a)=f(b)=0$; prove that $b-a>\pi$.",If  and  ; ; prove that .,"f''(x)+f(x)>0 f(x)>0 \forall x\in(a,b) f(a)=f(b)=0 b-a>\pi","Please help me to solve this question: Suppose $f:[a,b] \to \Bbb R$ satisfies: $f''(x)+f(x)>0$ and $f(x)>0$ for all $x\in(a ,b)$ ; $f(a)=f(b)=0$ . Prove that $b-a>\pi$ . Thanks in advance.",Please help me to solve this question: Suppose satisfies: and for all ; . Prove that . Thanks in advance.,"f:[a,b] \to \Bbb R f''(x)+f(x)>0 f(x)>0 x\in(a ,b) f(a)=f(b)=0 b-a>\pi","['calculus', 'analysis', 'ordinary-differential-equations', 'contest-math']"
56,Find the sum $\sum_{n=1}^{\infty} \frac{\ln n}{n(n- \ln n)}$,Find the sum,\sum_{n=1}^{\infty} \frac{\ln n}{n(n- \ln n)},"Find the following sum : $\sum_{n=1}^{\infty} \frac{\ln n }{n(n- \ln n)}$ I couldn't find any way to solve. I just want a hint, how to initiate?","Find the following sum : I couldn't find any way to solve. I just want a hint, how to initiate?",\sum_{n=1}^{\infty} \frac{\ln n }{n(n- \ln n)},['sequences-and-series']
57,"Given $f(x) = x^n e^{-x}$, show that $\int_0^1 f(x)\, dx$ is equal to a given expression.","Given , show that  is equal to a given expression.","f(x) = x^n e^{-x} \int_0^1 f(x)\, dx",Consider the function: $$f : \mathbb{R} \rightarrow \mathbb{R} \hspace{2cm} f(x) = x^n e^{-x}$$ I have to show the following: $$\int_0^1 f(x) dx = n! \bigg [ 1 - \dfrac{1}{e} \bigg ( 1 + \dfrac{1}{1!} + \dfrac{1}{2!} + \dfrac{1}{3!} + ... + \dfrac{1}{n!} \bigg ) \bigg ]$$ I used the notation: $$I_n = \int_0^1 f(x)dx$$ And by integrating by parts I got the recurrence formula: $$I_n = - \dfrac{1}{e} + n \cdot I_{n - 1}$$ But I don't have any idea as to how I could show what is asked.,Consider the function: I have to show the following: I used the notation: And by integrating by parts I got the recurrence formula: But I don't have any idea as to how I could show what is asked.,f : \mathbb{R} \rightarrow \mathbb{R} \hspace{2cm} f(x) = x^n e^{-x} \int_0^1 f(x) dx = n! \bigg [ 1 - \dfrac{1}{e} \bigg ( 1 + \dfrac{1}{1!} + \dfrac{1}{2!} + \dfrac{1}{3!} + ... + \dfrac{1}{n!} \bigg ) \bigg ] I_n = \int_0^1 f(x)dx I_n = - \dfrac{1}{e} + n \cdot I_{n - 1},['calculus']
58,Cases for only one mixed partial derivative is continuous,Cases for only one mixed partial derivative is continuous,,"During studying Clairaut's theorem(In the case of 2 variable functions), I started wondering why the condition of the theorem is written as 'both mixed partial derivative (i.e. $f_{xy}$ and $f_{yx}$ ) are continuous'. There can be two possibilities. If $f_{xy}$ is continuous, then $f_{yx}$ has to be continuous. There exist a function $f$ such that $f_{xy}$ is continuous but $f_{yx}$ doesn't. However, I couldn't prove the first statement, nor found the counterexample, so if you know what is the right statement, please help us. Thank you in advance","During studying Clairaut's theorem(In the case of 2 variable functions), I started wondering why the condition of the theorem is written as 'both mixed partial derivative (i.e. and ) are continuous'. There can be two possibilities. If is continuous, then has to be continuous. There exist a function such that is continuous but doesn't. However, I couldn't prove the first statement, nor found the counterexample, so if you know what is the right statement, please help us. Thank you in advance",f_{xy} f_{yx} f_{xy} f_{yx} f f_{xy} f_{yx},"['calculus', 'multivariable-calculus']"
59,Find the limit of: $\lim_{n\to\infty} \frac{1}{\sqrt[n+1]{(n+1)!} - \sqrt[n]{(n)!}}$ [duplicate],Find the limit of:  [duplicate],\lim_{n\to\infty} \frac{1}{\sqrt[n+1]{(n+1)!} - \sqrt[n]{(n)!}},"This question already has an answer here : Limit of the sequence $a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}$ (1 answer) Closed 4 years ago . Could be the following limit computed without using Stirling's approximation formula? $$\lim_{n\to\infty} \frac{1}{\sqrt[n+1]{(n+1)!} - \sqrt[n]{(n)!}}$$ I know that the limit is $e$, but I'm looking for some alternative ways that doesn't require to resort to the use of Stirling's approximation. I really appreciate any support at this limit. Thanks.","This question already has an answer here : Limit of the sequence $a_n=\sqrt[n+1]{(n+1)!}-\sqrt[n]{n!}$ (1 answer) Closed 4 years ago . Could be the following limit computed without using Stirling's approximation formula? $$\lim_{n\to\infty} \frac{1}{\sqrt[n+1]{(n+1)!} - \sqrt[n]{(n)!}}$$ I know that the limit is $e$, but I'm looking for some alternative ways that doesn't require to resort to the use of Stirling's approximation. I really appreciate any support at this limit. Thanks.",,"['real-analysis', 'limits', 'exponential-function', 'radicals']"
60,Which books to use for self-studying calculus and linear algebra? [closed],Which books to use for self-studying calculus and linear algebra? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question After years of divorce with mathematics I decided to come back to it. I studied computer science, so had quite a lot of maths in university, but though passing the exams, I didn't feel I fully grasped it. Now I'd like to come back to this material and go through it in detail to really understand it. My reasons are: For fun I plan to also study some university physics It'll surely sharpen some of the skills used at my work as a programmer I was recommended to start with linear algebra and calculus. I'm planning to study both of them more or less concurrently. I want to self-study and am deciding what books to use for that. I like to dig deep into things and relly understand theory behind them, so probably I'd like books with more theoretical approach. But I wouldn't like to totally skip any real-world applications of those things, so ideally the book would also cover some of that. I'd also like something challenging and rigorous. Recommended positions I've done some research and see people recommending those positions: Calculus: ""Calculus"" by Michael Spivak ""Calculus"" by Tom Apostol (2 volumes) Linear algebra: ""Linear Algebra"" by by Stephen H. Friedberg, Lawrence E. Spence,, Arnold J. Insel ""Linear Algebra"" by Kenneth Hoffman ""Linear Algebra Done Right"" by Sheldon Axler ""Linear Algebra"" by Georgi E. Shilov Questions I've read Tom Apostol's book covers some linear algebra. Does it mean that I would be able to skip linear algebra book at all and learn both just doing his ""Calculus""? Spivak's ""Calculus"" covers only single-variable. If I choose to follow his book, what would be good choice of a book for multi-variable calculus? Which of those books on linear algebra cover both theory and applications? Or maybe it'd be wise to complete one more theoretical and then read about applications alone in another text?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question After years of divorce with mathematics I decided to come back to it. I studied computer science, so had quite a lot of maths in university, but though passing the exams, I didn't feel I fully grasped it. Now I'd like to come back to this material and go through it in detail to really understand it. My reasons are: For fun I plan to also study some university physics It'll surely sharpen some of the skills used at my work as a programmer I was recommended to start with linear algebra and calculus. I'm planning to study both of them more or less concurrently. I want to self-study and am deciding what books to use for that. I like to dig deep into things and relly understand theory behind them, so probably I'd like books with more theoretical approach. But I wouldn't like to totally skip any real-world applications of those things, so ideally the book would also cover some of that. I'd also like something challenging and rigorous. Recommended positions I've done some research and see people recommending those positions: Calculus: ""Calculus"" by Michael Spivak ""Calculus"" by Tom Apostol (2 volumes) Linear algebra: ""Linear Algebra"" by by Stephen H. Friedberg, Lawrence E. Spence,, Arnold J. Insel ""Linear Algebra"" by Kenneth Hoffman ""Linear Algebra Done Right"" by Sheldon Axler ""Linear Algebra"" by Georgi E. Shilov Questions I've read Tom Apostol's book covers some linear algebra. Does it mean that I would be able to skip linear algebra book at all and learn both just doing his ""Calculus""? Spivak's ""Calculus"" covers only single-variable. If I choose to follow his book, what would be good choice of a book for multi-variable calculus? Which of those books on linear algebra cover both theory and applications? Or maybe it'd be wise to complete one more theoretical and then read about applications alone in another text?",,"['calculus', 'linear-algebra', 'reference-request', 'self-learning', 'book-recommendation']"
61,"Are these upper and lower bounds for $\frac{x!}{\left\lfloor{x}\right\rfloor!}$ useful? If so, are they already known?","Are these upper and lower bounds for  useful? If so, are they already known?",\frac{x!}{\left\lfloor{x}\right\rfloor!},"Truncating the infinite series for the derivative of the Digamma function $$ \psi'(x) = \sum_{n=0}^\infty\frac{1}{(x + n)^2} $$ after $m-1$ terms, where $m$ is a positive integer (the case $m=2$ answered the question How do we prove that $(x-1)!\leq{(\frac{x}{2})^{x-1}}$ ? ), finding upper and lower bounds for the remainder, and integrating twice between the limits $2$ and $2+x$ (at least, I think that's what I did, but it was a long slog, and my notation has changed several times since then), one arrives at the inequalities $$ \left(\frac{m+1+x}{m+1}\right)^{m+1+x} \!\!\! < \frac{e^x(m+x)!}{e^{(H_m-\gamma)x}m!} < \left(\frac{m+x}{m}\right)^{m+x} \quad (x > 0;\ m = 1, 2, 3, \ldots). $$ This seems most useful (if useful at all!) for smallish $x$ . Replacing $m+x$ by $x$ and $m$ by $\left\lfloor{x}\right\rfloor$ , we get $$ \left(\frac{x+1}{\left\lfloor{x}\right\rfloor+1}\right)^{x+1} \!\!\! < \frac{e^{x-\left\lfloor{x}\right\rfloor}}{e^{(H_m-\gamma)(x-\left\lfloor{x}\right\rfloor)}} \cdot \frac{x!}{\left\lfloor{x}\right\rfloor!} < \left(\frac{x}{\left\lfloor{x}\right\rfloor}\right)^x \quad(x > 1,\ x \notin \mathbb{N}). $$ This seems to give sharper bounds than the following simple exact form of Stirling's approximation : $$ \sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n} \leqslant n! \leqslant en^{n+\frac{1}{2}}e^{-n}. $$ On the other hand, it seems to be generally inferior to the full version of Robbins's bounds: $$ \sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n+1}} < n! < \sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n}}. $$ For small values of $x - \left\lfloor{x}\right\rfloor$ , my formula does sometimes give better results. For example, $7.04! \bumpeq 5463.7647$ , and in this case my formula gives the strict bounds $(5463.7292, 5463.8071)$ , whereas Robbins's formula gives $(5463.0514, 5463.8080)$ , and the simplified version of his formula gives the distinctly worse estimates $(5399.5135, 5855.4353)$ . Might my horrid formula therefore have some actual use? If so, has it been published already? Does it have a less nasty proof than the one I've sketched?","Truncating the infinite series for the derivative of the Digamma function after terms, where is a positive integer (the case answered the question How do we prove that ? ), finding upper and lower bounds for the remainder, and integrating twice between the limits and (at least, I think that's what I did, but it was a long slog, and my notation has changed several times since then), one arrives at the inequalities This seems most useful (if useful at all!) for smallish . Replacing by and by , we get This seems to give sharper bounds than the following simple exact form of Stirling's approximation : On the other hand, it seems to be generally inferior to the full version of Robbins's bounds: For small values of , my formula does sometimes give better results. For example, , and in this case my formula gives the strict bounds , whereas Robbins's formula gives , and the simplified version of his formula gives the distinctly worse estimates . Might my horrid formula therefore have some actual use? If so, has it been published already? Does it have a less nasty proof than the one I've sketched?","
\psi'(x) = \sum_{n=0}^\infty\frac{1}{(x + n)^2}
 m-1 m m=2 (x-1)!\leq{(\frac{x}{2})^{x-1}} 2 2+x 
\left(\frac{m+1+x}{m+1}\right)^{m+1+x}
\!\!\! < \frac{e^x(m+x)!}{e^{(H_m-\gamma)x}m!} <
\left(\frac{m+x}{m}\right)^{m+x}
\quad (x > 0;\ m = 1, 2, 3, \ldots).
 x m+x x m \left\lfloor{x}\right\rfloor 
\left(\frac{x+1}{\left\lfloor{x}\right\rfloor+1}\right)^{x+1}
\!\!\! <
\frac{e^{x-\left\lfloor{x}\right\rfloor}}{e^{(H_m-\gamma)(x-\left\lfloor{x}\right\rfloor)}}
\cdot \frac{x!}{\left\lfloor{x}\right\rfloor!}
<
\left(\frac{x}{\left\lfloor{x}\right\rfloor}\right)^x
\quad(x > 1,\ x \notin \mathbb{N}).
 
\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n} \leqslant n! \leqslant en^{n+\frac{1}{2}}e^{-n}.
 
\sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n+1}} < n! < \sqrt{2\pi}n^{n+\frac{1}{2}}e^{-n}e^{\frac{1}{12n}}.
 x - \left\lfloor{x}\right\rfloor 7.04! \bumpeq 5463.7647 (5463.7292, 5463.8071) (5463.0514, 5463.8080) (5399.5135, 5855.4353)","['calculus', 'inequality', 'reference-request', 'alternative-proof']"
62,Evaluating the nested radical $ \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots}}} $. [closed],Evaluating the nested radical . [closed], \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots}}} ,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Improve this question How does one prove the following limit? $$   \lim_{n \to \infty}   \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots \sqrt{1 + (n - 1) \sqrt{1 + n}}}}} = 3. $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . The community reviewed whether to reopen this question last month and left it closed: Original close reason(s) were not resolved Improve this question How does one prove the following limit? $$   \lim_{n \to \infty}   \sqrt{1 + 2 \sqrt{1 + 3 \sqrt{1 + \cdots \sqrt{1 + (n - 1) \sqrt{1 + n}}}}} = 3. $$",,"['real-analysis', 'calculus']"
63,"How to prove the real value function $f$ is convex on $[a,d]$ if $f$ is convex on both $[a,c]$ and $[b,d]$? ($a<b<c<d$).",How to prove the real value function  is convex on  if  is convex on both  and ? ().,"f [a,d] f [a,c] [b,d] a<b<c<d","How to prove the real value function $f$ is convex on $[a,d]$ if $f$ is convex on both $[a,c]$ and $[b,d]$? ($a<b<c<d$). I try to show that: for arbitrary $x\in(a,b), z\in(c,d)$, and for all $y\in (x,z)$ the inequality $\dfrac{f(y)-f(x)}{y-x}\leqslant \dfrac{f(z)-f(y)}{z-y}$ holds. Then $f$ is convex on $[a,d]$. But it seems to need some inequality tricks, I can't finish it.","How to prove the real value function $f$ is convex on $[a,d]$ if $f$ is convex on both $[a,c]$ and $[b,d]$? ($a<b<c<d$). I try to show that: for arbitrary $x\in(a,b), z\in(c,d)$, and for all $y\in (x,z)$ the inequality $\dfrac{f(y)-f(x)}{y-x}\leqslant \dfrac{f(z)-f(y)}{z-y}$ holds. Then $f$ is convex on $[a,d]$. But it seems to need some inequality tricks, I can't finish it.",,['calculus']
64,A question on the limit $\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$,A question on the limit,\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2},"I stumbled upon this question from a Calculus exam: Let $f \in C^1(\mathbb{R})$ be monotonically decreasing such that $\lim \limits_{x \rightarrow \infty} f(x) = 0$. Prove that the limit $$\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$$ exists and is finite. Now obviously the solution the writers of this problem had intended was to look at the integral $$\int \limits_{0}^{1} \frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx = \int \limits_{1}^{\infty} f(x)\cos x \, dx$$ which converges by the Dirichlet criterion and then the Riemann sums attributed to the partitions $\Pi_n = \{0, \frac{1}{n}, \frac{2}{n}, ..., 1  \}$ are $S_n=n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$ which if would imply  $$\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}=\int \limits_{0}^{1} \frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx < \infty$$ if the function $\frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}$ was Riemann-integrable in $[0,1]$. The problem with that approach however, is that the integral  $$\int \limits_{0}^{1}\frac{cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx$$ is not a Riemann-integral but an improper-integral so one cannot conclude that $$\lim \limits_{n \rightarrow \infty} S_n = \int \limits_{0}^{1}\frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx$$ After many hours of thinking, trying to resolve this issue I realized that the question may be false as it makes no sense that the limit exists because that would sort of mean that the integral is a proper Riemann-integral which would imply the function is bounded (which is not necessarily true depending on the choice of $f$). To test this, I used Wolfram Alpha to calculate numerical estimates with the function $f(x)=\frac{1}{x}$, which confirmed my speculations, though I have not been able to rigorously prove it. My question is can you find an example that disproves the claim the question makes? Or was I wrong and the claim can be proven?","I stumbled upon this question from a Calculus exam: Let $f \in C^1(\mathbb{R})$ be monotonically decreasing such that $\lim \limits_{x \rightarrow \infty} f(x) = 0$. Prove that the limit $$\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$$ exists and is finite. Now obviously the solution the writers of this problem had intended was to look at the integral $$\int \limits_{0}^{1} \frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx = \int \limits_{1}^{\infty} f(x)\cos x \, dx$$ which converges by the Dirichlet criterion and then the Riemann sums attributed to the partitions $\Pi_n = \{0, \frac{1}{n}, \frac{2}{n}, ..., 1  \}$ are $S_n=n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$ which if would imply  $$\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}=\int \limits_{0}^{1} \frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx < \infty$$ if the function $\frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}$ was Riemann-integrable in $[0,1]$. The problem with that approach however, is that the integral  $$\int \limits_{0}^{1}\frac{cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx$$ is not a Riemann-integral but an improper-integral so one cannot conclude that $$\lim \limits_{n \rightarrow \infty} S_n = \int \limits_{0}^{1}\frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx$$ After many hours of thinking, trying to resolve this issue I realized that the question may be false as it makes no sense that the limit exists because that would sort of mean that the integral is a proper Riemann-integral which would imply the function is bounded (which is not necessarily true depending on the choice of $f$). To test this, I used Wolfram Alpha to calculate numerical estimates with the function $f(x)=\frac{1}{x}$, which confirmed my speculations, though I have not been able to rigorously prove it. My question is can you find an example that disproves the claim the question makes? Or was I wrong and the claim can be proven?",,['calculus']
65,Substitution in complex integration,Substitution in complex integration,,"Let $f$ and $\phi$ be continuously differentiable functions on the closed unit disc $D=\{\lambda\in \Bbb C; |\lambda|\leq1\}$ and suppose that there exists $u:[0,2\pi]\to[u_0, u_0+2\pi]$ continuously differentiable such that $$e^{iu(t)}=\phi(e^{it}), t\in[0,2\pi].$$ Let $$A=\int_0^{2\pi}|f(\phi(e^{it}))|^2dt.$$ The author of the book I am reading say that, taking te substitution $u=u(t)$ we conclude that $$A=\int_{u_0}^{u_0+2\pi}|f(e^{iu})|^2\frac{1}{|\phi'(e^{it(u)})|}du.$$ I want to conclude this equality, but I didn't undestand why we should put a modulus on that factor... If we differenciate $e^{iu(t)}=\phi(e^{it})$ in $t$ we have $$ie^{iu(t)}\frac{du}{dt}=ie^{it}\phi'(e^{it}), t\in[0,2\pi].$$ If we could take modulus in this equality I could conclude what I want... but why could I do this? The formula for integration by substitution is $$\int_{\varphi(a)}^{\varphi(b)}f(x)dx=\int_a^bf(\varphi(t))\varphi'(t)dt.$$","Let $f$ and $\phi$ be continuously differentiable functions on the closed unit disc $D=\{\lambda\in \Bbb C; |\lambda|\leq1\}$ and suppose that there exists $u:[0,2\pi]\to[u_0, u_0+2\pi]$ continuously differentiable such that $$e^{iu(t)}=\phi(e^{it}), t\in[0,2\pi].$$ Let $$A=\int_0^{2\pi}|f(\phi(e^{it}))|^2dt.$$ The author of the book I am reading say that, taking te substitution $u=u(t)$ we conclude that $$A=\int_{u_0}^{u_0+2\pi}|f(e^{iu})|^2\frac{1}{|\phi'(e^{it(u)})|}du.$$ I want to conclude this equality, but I didn't undestand why we should put a modulus on that factor... If we differenciate $e^{iu(t)}=\phi(e^{it})$ in $t$ we have $$ie^{iu(t)}\frac{du}{dt}=ie^{it}\phi'(e^{it}), t\in[0,2\pi].$$ If we could take modulus in this equality I could conclude what I want... but why could I do this? The formula for integration by substitution is $$\int_{\varphi(a)}^{\varphi(b)}f(x)dx=\int_a^bf(\varphi(t))\varphi'(t)dt.$$",,"['calculus', 'integration', 'complex-analysis', 'definite-integrals', 'substitution']"
66,Analytic proof of $\sqrt{n}$ irrationality or integrality,Analytic proof of  irrationality or integrality,\sqrt{n},"Basic arithmetics (uniqueness of prime factorization) tells us that, for $n\in \mathbb N$, either $\alpha:=\sqrt{n}$ is an integer or is an irrational. While designing undergraduate exams, I wondered if there was some elementary, calculus-based proof of that fact. First I tried an approach along the lines of Niven's proof of the irrationality of $\pi$. The proof studies the family of polynomials $P_k(x):=\frac{1}{k!}(qx(p-qx))^k$ for $p,q\in \mathbb N$. Assuming $\pi=\frac{p}{q}$ leads to a contradiction, since the sequence of integrals $\int_0^{p/q}P_k(t)\sin(t)~dt$ is both positive, integral and tending to $0$. But this works out nicely because of the differential relations satisfied by sin (that come in an essential way in integrating by parts). This argument works as well for any function with nice, linear differential relations with integers coefficients ( e.g. exp). Unfortunately, this is not so easy with, say, $x^2-2$. Or I missed something obvious. In the same spirit I tried to study various sequences of integrals related to $x^2 -2$, but to no avail. Or, again, I missed something obvious :) Another way I explored is to study $f(x):=x^\alpha$ from the point of view of functional equations. Assuming $\alpha=\frac{p}{q}$ is rational, we both have $$\cases{f\circ f(x)=x^n\\qf'(x)=pf(x)}$$ If an $f$ satisifies these constraints then, using Faà Di'Bruno formula, we can painstakingly prove the claim by observing that after differentiating $k$ times we have a relationship where $f^{(k)}$ appears alone only if $k$ is a square. One can arrange to prove that this condition is exactly the sought one: if $\alpha^2=n$ then this must be read at the $k=n$-th step. Although the latter idea works well, it is way from being elementary! (In particular, the exam talked about something else entirely ;) ) So I ask the question here: does anybody know a more elementary analytic proof?","Basic arithmetics (uniqueness of prime factorization) tells us that, for $n\in \mathbb N$, either $\alpha:=\sqrt{n}$ is an integer or is an irrational. While designing undergraduate exams, I wondered if there was some elementary, calculus-based proof of that fact. First I tried an approach along the lines of Niven's proof of the irrationality of $\pi$. The proof studies the family of polynomials $P_k(x):=\frac{1}{k!}(qx(p-qx))^k$ for $p,q\in \mathbb N$. Assuming $\pi=\frac{p}{q}$ leads to a contradiction, since the sequence of integrals $\int_0^{p/q}P_k(t)\sin(t)~dt$ is both positive, integral and tending to $0$. But this works out nicely because of the differential relations satisfied by sin (that come in an essential way in integrating by parts). This argument works as well for any function with nice, linear differential relations with integers coefficients ( e.g. exp). Unfortunately, this is not so easy with, say, $x^2-2$. Or I missed something obvious. In the same spirit I tried to study various sequences of integrals related to $x^2 -2$, but to no avail. Or, again, I missed something obvious :) Another way I explored is to study $f(x):=x^\alpha$ from the point of view of functional equations. Assuming $\alpha=\frac{p}{q}$ is rational, we both have $$\cases{f\circ f(x)=x^n\\qf'(x)=pf(x)}$$ If an $f$ satisifies these constraints then, using Faà Di'Bruno formula, we can painstakingly prove the claim by observing that after differentiating $k$ times we have a relationship where $f^{(k)}$ appears alone only if $k$ is a square. One can arrange to prove that this condition is exactly the sought one: if $\alpha^2=n$ then this must be read at the $k=n$-th step. Although the latter idea works well, it is way from being elementary! (In particular, the exam talked about something else entirely ;) ) So I ask the question here: does anybody know a more elementary analytic proof?",,['calculus']
67,"Evaluate this indefinite inegral $\int \sin^{e}x \,dx$",Evaluate this indefinite inegral,"\int \sin^{e}x \,dx","How to evaluate this  indefinite integral $$\int \sin^{e}x\, dx$$ I evaluate from wolfram aplha but i didn't get it I have no idea from where I should start. Please give me hint.","How to evaluate this  indefinite integral $$\int \sin^{e}x\, dx$$ I evaluate from wolfram aplha but i didn't get it I have no idea from where I should start. Please give me hint.",,"['calculus', 'integration', 'indefinite-integrals']"
68,Integrate $\int\sqrt{25\sin^2(5x)+49\cos^2(7x)}dx$,Integrate,\int\sqrt{25\sin^2(5x)+49\cos^2(7x)}dx,I have tried solving the following integral using Calc I&II methods and couldn't find an answer. I would like to know the techniques for solving such integral. $$\int\sqrt{25\sin^2(5x)+49\cos^2(7x)}dx$$,I have tried solving the following integral using Calc I&II methods and couldn't find an answer. I would like to know the techniques for solving such integral. $$\int\sqrt{25\sin^2(5x)+49\cos^2(7x)}dx$$,,"['calculus', 'integration']"
69,Prove that exists $\lim_{x\rightarrow \infty }\frac{f(x)}{x}$ and determine its value [duplicate],Prove that exists  and determine its value [duplicate],\lim_{x\rightarrow \infty }\frac{f(x)}{x},"This question already has answers here : Show that if $\int_0^x f(y)dy \sim Ax^\alpha$ then $f(x)\sim \alpha Ax^{\alpha -1}$ (2 answers) Closed 6 years ago . Let $f:[0,\infty )\rightarrow \mathbb{R}$ be an increasing function, such that $\lim_{x\rightarrow \infty }\frac{1}{x^{2}}\cdot \int_{0}^{x}f(t)dt=1$. Prove that exists $\lim_{x\rightarrow \infty }\frac{f(x)}{x}$ and calculate this limit. If $f$ would be continuous, we'd have $1=\lim_{x\rightarrow \infty }\frac{1}{x^{2}}\cdot \int_{0}^{x}f(t)dt=\lim_{x\rightarrow \infty }\frac{f(x)}{2x}$, therefore $\lim_{x\rightarrow \infty }\frac{f(x)}{x}=2.$ But we don't know if $f$ is continuous or not, and I wasn't able to find any other idea.","This question already has answers here : Show that if $\int_0^x f(y)dy \sim Ax^\alpha$ then $f(x)\sim \alpha Ax^{\alpha -1}$ (2 answers) Closed 6 years ago . Let $f:[0,\infty )\rightarrow \mathbb{R}$ be an increasing function, such that $\lim_{x\rightarrow \infty }\frac{1}{x^{2}}\cdot \int_{0}^{x}f(t)dt=1$. Prove that exists $\lim_{x\rightarrow \infty }\frac{f(x)}{x}$ and calculate this limit. If $f$ would be continuous, we'd have $1=\lim_{x\rightarrow \infty }\frac{1}{x^{2}}\cdot \int_{0}^{x}f(t)dt=\lim_{x\rightarrow \infty }\frac{f(x)}{2x}$, therefore $\lim_{x\rightarrow \infty }\frac{f(x)}{x}=2.$ But we don't know if $f$ is continuous or not, and I wasn't able to find any other idea.",,"['calculus', 'real-analysis', 'limits', 'definite-integrals']"
70,convergent or divergent $\int_{-4}^{1} \frac{dz}{(z + 3)^3}$,convergent or divergent,\int_{-4}^{1} \frac{dz}{(z + 3)^3},"Question Determine whether convergent or divergent. $$ \int_{-4}^{1} \frac{dz}{(z + 3)^3} $$ Thinking I'm not sure how best to go about this, whether I'm justified in my result. Basically I'm saying that as I can't find the first limit, the integral is divergent. I'm not sure if I should be, in some way, trying to combine the two limits (and using L'Hopitals), or if as soon as I've established that one doesn't exist the whole thing can be determined to be divergent (I think this is correct). If of one of the two limits is divergent, can I conclude that the integral is divergent? Definition If $f$ is continuous at all $x$ in the interval $[a, b]$, except maybe at $c$ , where $a < c < b$ , and if $\lim_{x \to c} |f(x)| = + \infty$ , then $$   \int_{a}^{b} f(x) \mathop{dx} =   \lim_{t \to c^- }  \int_{a}^{t}  f(x) \mathop{dx}   +    \lim_{s \to c^+ }  \int_{s}^{b}  f(x) \mathop{dx} $$ if this limit exists, otherwise it is divergent. Working The improper integral is $$ \int \frac{dz}{(z + 3)^3} = - \frac{1}{2(z + 3)^2} + C $$ There's a discontinuity at $z = -3$ , so splitting the integral up as \begin{equation*}   \begin{aligned}     \int_{-4}^{1} \frac{dz}{(z + 3)^3}     &    =     \lim_{a \to -3^-}     \int_{-4}^{a} \frac{dz}{(z + 3)^3}     +     \lim_{b \to -3^+}     \int_{b}^{1} \frac{dz}{(z + 3)^3}  \\     &=     - \frac{1}{2}     \left(       \lim_{a \to -3^-}       \left[         \frac{1}{(z + 3)^2}       \right]_{-4}^{a}       +       \lim_{b \to -3^+}       \left[         \frac{1}{(z + 3)^2}       \right]_{b}^{1}     \right)   \end{aligned} \end{equation*} The first limit, $\lim_{a \to -3^-} \left[\frac{1}{(z + 3)^2} \right]_{-4}^{a}$, is found as \begin{equation*}   \begin{aligned}     \lim_{a \to -3^-} \left[\frac{1}{(z + 3)^2} \right]_{-4}^{a} &=     \lim_{a \to -3^-} \left[\frac{1}{(a + 3)^2} - \frac{1}{(-1)^2}  \right] \\     &= \lim_{a \to -3^-} \left[\frac{1}{(a + 3)^2} - 1   \right] \\     &= \lim_{a \to -3^-} \left[\frac{1 - (a + 3)^2}{(a + 3)^2}   \right] \\   \end{aligned} \end{equation*}","Question Determine whether convergent or divergent. $$ \int_{-4}^{1} \frac{dz}{(z + 3)^3} $$ Thinking I'm not sure how best to go about this, whether I'm justified in my result. Basically I'm saying that as I can't find the first limit, the integral is divergent. I'm not sure if I should be, in some way, trying to combine the two limits (and using L'Hopitals), or if as soon as I've established that one doesn't exist the whole thing can be determined to be divergent (I think this is correct). If of one of the two limits is divergent, can I conclude that the integral is divergent? Definition If $f$ is continuous at all $x$ in the interval $[a, b]$, except maybe at $c$ , where $a < c < b$ , and if $\lim_{x \to c} |f(x)| = + \infty$ , then $$   \int_{a}^{b} f(x) \mathop{dx} =   \lim_{t \to c^- }  \int_{a}^{t}  f(x) \mathop{dx}   +    \lim_{s \to c^+ }  \int_{s}^{b}  f(x) \mathop{dx} $$ if this limit exists, otherwise it is divergent. Working The improper integral is $$ \int \frac{dz}{(z + 3)^3} = - \frac{1}{2(z + 3)^2} + C $$ There's a discontinuity at $z = -3$ , so splitting the integral up as \begin{equation*}   \begin{aligned}     \int_{-4}^{1} \frac{dz}{(z + 3)^3}     &    =     \lim_{a \to -3^-}     \int_{-4}^{a} \frac{dz}{(z + 3)^3}     +     \lim_{b \to -3^+}     \int_{b}^{1} \frac{dz}{(z + 3)^3}  \\     &=     - \frac{1}{2}     \left(       \lim_{a \to -3^-}       \left[         \frac{1}{(z + 3)^2}       \right]_{-4}^{a}       +       \lim_{b \to -3^+}       \left[         \frac{1}{(z + 3)^2}       \right]_{b}^{1}     \right)   \end{aligned} \end{equation*} The first limit, $\lim_{a \to -3^-} \left[\frac{1}{(z + 3)^2} \right]_{-4}^{a}$, is found as \begin{equation*}   \begin{aligned}     \lim_{a \to -3^-} \left[\frac{1}{(z + 3)^2} \right]_{-4}^{a} &=     \lim_{a \to -3^-} \left[\frac{1}{(a + 3)^2} - \frac{1}{(-1)^2}  \right] \\     &= \lim_{a \to -3^-} \left[\frac{1}{(a + 3)^2} - 1   \right] \\     &= \lim_{a \to -3^-} \left[\frac{1 - (a + 3)^2}{(a + 3)^2}   \right] \\   \end{aligned} \end{equation*}",,"['calculus', 'real-analysis', 'integration', 'convergence-divergence']"
71,multiple loans multiple payers - how to snowball fairly,multiple loans multiple payers - how to snowball fairly,,"My brother and I both have a large sum of student loan debt.  I have more than he does and my interest rates are slightly larger as well.  We are both attempting to snowball our debt separately.  It occurred to me that we may be able to accelerate paying off our student loan debt if we snowballed together.  We could both tackle one loan together and then focus on the next and as a result, both end up with less interest to pay in the long run. However, I'm stuck on how to implement this idea in a fair way.  We both pay different amounts each month. Is there any way we can take into account our total balances, interest rates, and the amount paid to keep track of what percentage of payments are going towards each other's loans? As an example let's say I have 4 loans at \$10,000 at 10% APR (A, B, C, D) and he has 4 loans at \$5,000 at 5% APR (E, F, G, H). So we have a total of 60,000 due, $40,000 of which is mine. Does anyone have an equation for this or can anyone better explain why this will or will not work? Do we have to pay proportionally to our debt at all times in order to make this work? If we pay equal amounts, at a certain point he should stop paying because he's done with his portion and I should continue; how will we know when that point is? This should help cut the amount of interest we pay.  Will it actually do that or will it only benefit the person with the higher interest rates?","My brother and I both have a large sum of student loan debt.  I have more than he does and my interest rates are slightly larger as well.  We are both attempting to snowball our debt separately.  It occurred to me that we may be able to accelerate paying off our student loan debt if we snowballed together.  We could both tackle one loan together and then focus on the next and as a result, both end up with less interest to pay in the long run. However, I'm stuck on how to implement this idea in a fair way.  We both pay different amounts each month. Is there any way we can take into account our total balances, interest rates, and the amount paid to keep track of what percentage of payments are going towards each other's loans? As an example let's say I have 4 loans at \$10,000 at 10% APR (A, B, C, D) and he has 4 loans at \$5,000 at 5% APR (E, F, G, H). So we have a total of 60,000 due, $40,000 of which is mine. Does anyone have an equation for this or can anyone better explain why this will or will not work? Do we have to pay proportionally to our debt at all times in order to make this work? If we pay equal amounts, at a certain point he should stop paying because he's done with his portion and I should continue; how will we know when that point is? This should help cut the amount of interest we pay.  Will it actually do that or will it only benefit the person with the higher interest rates?",,['calculus']
72,What can be said about the sequence $(\left\Vert A^{n}\left( x\right) \right\Vert ^{\frac{1}{n}})_{n}$,What can be said about the sequence,(\left\Vert A^{n}\left( x\right) \right\Vert ^{\frac{1}{n}})_{n},Let $H$ be a separable infinite dimensional Hilbert space and let $A\in B(H)$ be a bounded operator. For any $x\in H$ is $(\left\Vert A^{n}\left( x\right) \right\Vert ^{\frac{1}{n}})_{n}$ a decreasing and/or a convergent sequence ? Recall that $\lim \left\Vert A^{n} \right\Vert ^{\frac{1}{n}}$ is the spectral radius of $A$.,Let $H$ be a separable infinite dimensional Hilbert space and let $A\in B(H)$ be a bounded operator. For any $x\in H$ is $(\left\Vert A^{n}\left( x\right) \right\Vert ^{\frac{1}{n}})_{n}$ a decreasing and/or a convergent sequence ? Recall that $\lim \left\Vert A^{n} \right\Vert ^{\frac{1}{n}}$ is the spectral radius of $A$.,,"['calculus', 'linear-algebra', 'operator-theory', 'linear-transformations', 'spectral-theory']"
73,Is this closed form correct $\int_{0}^{\pi/2}\cos^2\left(\cos^2\left({x\over \pi}\right)\right)\mathrm dx=\gamma?$,Is this closed form correct,\int_{0}^{\pi/2}\cos^2\left(\cos^2\left({x\over \pi}\right)\right)\mathrm dx=\gamma?,"Where $\gamma $ is Euler's constant, given that: $$\int_{0}^{\pi/2}\cos^2\left(\cos^2\left({x\over \pi}\right)\right)\mathrm dx=\gamma\tag1$$ Using $$2\cos^2(A)=1+\cos(2A)$$ $(1)\implies $ $${\pi\over 4}+{1\over 2}\int_{0}^{\pi/2}\cos\left(2\cos^2\left({x\over \pi}\right)\right)\mathrm dx\tag2$$ $(2)\implies $ $${\pi\over 4}+{1\over 2}\int_{0}^{\pi/2}\cos\left(1+\cos\left({2x\over \pi}\right)\right)\mathrm dx\tag3$$  Using $\cos(A+B)=\cos(A)\cos(B)-\sin(A)\sin(B)$ $(3)\implies $ $${\pi\over 4}+{1\over 2}\cos(1)\int_{0}^{\pi/2}\cos\left(\cos\left({2x\over \pi}\right)\right)\mathrm dx-{1\over 2}\sin(1)\int_{0}^{\pi/2}\sin\left(\cos\left({2x\over \pi}\right)\right)\mathrm dx\tag4$$ $$I=\int_{0}^{\pi/2}\cos\left(\cos\left({2x\over \pi}\right)\right)\mathrm dx\tag5$$ Let $u=\cos\left({2x\over \pi}\right)\implies dx=-{\pi\over 2}{1\over \sin(2x/\pi)}$ $u^2=1-\sin^2(2x/\pi)$ $$I={\pi\over 2}\int_{\cos(1)}^{1}{\cos(u)\over \sqrt{1-u^2}}\mathrm du\tag6$$ Recall $$(1-x)^{-1/2}=\sum_{k=0}^{\infty}{(2k-1)!!\over (2k)!!}x^k$$ $(6)\implies$ $$I={\pi\over 2}\sum_{k=0}^{\infty}{(2k-1)!!\over (2k)!!}\int_{\cos(1)}^{1}u^{2k}\cos(u)\mathrm du\tag7$$","Where $\gamma $ is Euler's constant, given that: $$\int_{0}^{\pi/2}\cos^2\left(\cos^2\left({x\over \pi}\right)\right)\mathrm dx=\gamma\tag1$$ Using $$2\cos^2(A)=1+\cos(2A)$$ $(1)\implies $ $${\pi\over 4}+{1\over 2}\int_{0}^{\pi/2}\cos\left(2\cos^2\left({x\over \pi}\right)\right)\mathrm dx\tag2$$ $(2)\implies $ $${\pi\over 4}+{1\over 2}\int_{0}^{\pi/2}\cos\left(1+\cos\left({2x\over \pi}\right)\right)\mathrm dx\tag3$$  Using $\cos(A+B)=\cos(A)\cos(B)-\sin(A)\sin(B)$ $(3)\implies $ $${\pi\over 4}+{1\over 2}\cos(1)\int_{0}^{\pi/2}\cos\left(\cos\left({2x\over \pi}\right)\right)\mathrm dx-{1\over 2}\sin(1)\int_{0}^{\pi/2}\sin\left(\cos\left({2x\over \pi}\right)\right)\mathrm dx\tag4$$ $$I=\int_{0}^{\pi/2}\cos\left(\cos\left({2x\over \pi}\right)\right)\mathrm dx\tag5$$ Let $u=\cos\left({2x\over \pi}\right)\implies dx=-{\pi\over 2}{1\over \sin(2x/\pi)}$ $u^2=1-\sin^2(2x/\pi)$ $$I={\pi\over 2}\int_{\cos(1)}^{1}{\cos(u)\over \sqrt{1-u^2}}\mathrm du\tag6$$ Recall $$(1-x)^{-1/2}=\sum_{k=0}^{\infty}{(2k-1)!!\over (2k)!!}x^k$$ $(6)\implies$ $$I={\pi\over 2}\sum_{k=0}^{\infty}{(2k-1)!!\over (2k)!!}\int_{\cos(1)}^{1}u^{2k}\cos(u)\mathrm du\tag7$$",,['calculus']
74,"Finding the area between curves, calculus, how does my answer look?","Finding the area between curves, calculus, how does my answer look?",,"i got a test tomorrow and before i step in I wanna make sure i understand this. How does my answer look? The prof didnt provide any solutions, just the worksheets, so im worried everything im doing is wrong.","i got a test tomorrow and before i step in I wanna make sure i understand this. How does my answer look? The prof didnt provide any solutions, just the worksheets, so im worried everything im doing is wrong.",,['calculus']
75,"Find the maximum of a functional over all $f$ on $[0,1]$, where $0 \leq f(x) \leq x$ (difficult) [closed]","Find the maximum of a functional over all  on , where  (difficult) [closed]","f [0,1] 0 \leq f(x) \leq x","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $X$ be the space of all continuous functions $f : [0,1] \rightarrow \mathbb{R}$  with $0 \leq f(x) \leq x$ for every $x\in [0,1]$. Consider the functional  $$F(f)=\int_{0}^1 f(x)^2 dx - \left( \int_0^1 f (x) dx \right)^2, \quad f\in X.$$ Does $F$ admit a maximum in $X$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $X$ be the space of all continuous functions $f : [0,1] \rightarrow \mathbb{R}$  with $0 \leq f(x) \leq x$ for every $x\in [0,1]$. Consider the functional  $$F(f)=\int_{0}^1 f(x)^2 dx - \left( \int_0^1 f (x) dx \right)^2, \quad f\in X.$$ Does $F$ admit a maximum in $X$?",,"['calculus', 'real-analysis', 'probability', 'definite-integrals']"
76,Integral of a modulus function vs absolute value of definite integral,Integral of a modulus function vs absolute value of definite integral,,"Please refer to the diagram below. The diagram shows a curve with equation $y = cos {x\over 2} cos x$, for 0 $\le x$ $\le$ $\pi$, along with the $x$ and $y$-intercepts of the graph. Question: By first finding $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ , explain why $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$ is smaller than $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$. You may refer to the graph provided for assistance. I have found $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ and have done so as shown: I then start to calculate $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$ as shown below. Now, I realise I would also end up calculating $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ eventually which implies that $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ = $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$? But the question has already stated that $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$ will be smaller than $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$, so I'm not really sure how to proceed from here...","Please refer to the diagram below. The diagram shows a curve with equation $y = cos {x\over 2} cos x$, for 0 $\le x$ $\le$ $\pi$, along with the $x$ and $y$-intercepts of the graph. Question: By first finding $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ , explain why $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$ is smaller than $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$. You may refer to the graph provided for assistance. I have found $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ and have done so as shown: I then start to calculate $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$ as shown below. Now, I realise I would also end up calculating $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ eventually which implies that $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$ = $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$? But the question has already stated that $\vert \int_{\pi\over 3}^{2\pi\over 3} cos {x\over 2} cos x \,dx \vert$ will be smaller than $\int_{\pi\over 3}^{2\pi\over 3} \vert cos {x\over 2} cos x \vert\,dx$, so I'm not really sure how to proceed from here...",,"['calculus', 'integration', 'definite-integrals']"
77,Application of the Dominated convergence theorem for series,Application of the Dominated convergence theorem for series,,"The following exercise is taken from a Calculus I course exam: Let $k\in \mathbb N$. Prove the existence of $$x = \lim_{k\to \infty}\sum_{n=1}^{\infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right)$$   and calculate $x$. This is an obvious application of the Dominated convergence theorem for infinite series. I think I found a solution, however I am not sure if it is entirely correct: Step 1: We have to show that for all $n\in \mathbb N$ the limit $\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right)$ exists. This holds since, for all $n\in \mathbb N$: $$\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \lim_{k\to \infty}\exp(-n) \cdot \lim_{k \to \infty}\exp\left(\frac{k}{n}e^{-\frac{k}{n}}\right) =  \exp(-n),$$ having used that the exponential function grows faster than every polynomial. Step2: We have to show the existence of a majorant sequence. Since $\frac{k}{n}e^{-\frac{k}{n}}$ converges towards $0$ for all $n\in \mathbb N$ we can choose $k\in \mathbb N$ so big that $\frac{k}{n}e^{-\frac{k}{n}} \leq \frac{1}{2}$ (or any other finite number). It thereby follows that $$\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \exp(-n) \cdot \exp\left(\frac{k}{n}e^{-\frac{k}{n}}\right) \leq \exp(-n) \cdot \exp\left(\frac{1}{2}\right) =: b_n$$  and $\sum b_n$ obviously converges (geometric series). Now we can apply the Dominated convergence theorem yielding the existence of $x$. Furthermore, we get $$x = \lim_{k\to \infty}\sum_{n=1}^{\infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \sum_{n=1}^{\infty}\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \sum_{n=1}^{\infty}\exp(-n) \quad\quad\quad= \frac{1}{e}\sum_{n=0}^{\infty}\exp(-n) = \frac{1}{e-1}$$ Is my approach correct or does it have flaws? Step 2 is the one I am not happy with but don't know how to do it better. Thanks in advance for your feedback!","The following exercise is taken from a Calculus I course exam: Let $k\in \mathbb N$. Prove the existence of $$x = \lim_{k\to \infty}\sum_{n=1}^{\infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right)$$   and calculate $x$. This is an obvious application of the Dominated convergence theorem for infinite series. I think I found a solution, however I am not sure if it is entirely correct: Step 1: We have to show that for all $n\in \mathbb N$ the limit $\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right)$ exists. This holds since, for all $n\in \mathbb N$: $$\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \lim_{k\to \infty}\exp(-n) \cdot \lim_{k \to \infty}\exp\left(\frac{k}{n}e^{-\frac{k}{n}}\right) =  \exp(-n),$$ having used that the exponential function grows faster than every polynomial. Step2: We have to show the existence of a majorant sequence. Since $\frac{k}{n}e^{-\frac{k}{n}}$ converges towards $0$ for all $n\in \mathbb N$ we can choose $k\in \mathbb N$ so big that $\frac{k}{n}e^{-\frac{k}{n}} \leq \frac{1}{2}$ (or any other finite number). It thereby follows that $$\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \exp(-n) \cdot \exp\left(\frac{k}{n}e^{-\frac{k}{n}}\right) \leq \exp(-n) \cdot \exp\left(\frac{1}{2}\right) =: b_n$$  and $\sum b_n$ obviously converges (geometric series). Now we can apply the Dominated convergence theorem yielding the existence of $x$. Furthermore, we get $$x = \lim_{k\to \infty}\sum_{n=1}^{\infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \sum_{n=1}^{\infty}\lim_{k\to \infty}\exp\left(-n+\frac{k}{n}e^{-\frac{k}{n}}\right) = \sum_{n=1}^{\infty}\exp(-n) \quad\quad\quad= \frac{1}{e}\sum_{n=0}^{\infty}\exp(-n) = \frac{1}{e-1}$$ Is my approach correct or does it have flaws? Step 2 is the one I am not happy with but don't know how to do it better. Thanks in advance for your feedback!",,"['calculus', 'real-analysis', 'analysis', 'convergence-divergence', 'exponential-function']"
78,Find the area of the part of the paraboloid $z = x^2 + y^2$ that lies under the plane $z=4-x$,Find the area of the part of the paraboloid  that lies under the plane,z = x^2 + y^2 z=4-x,"So the first thing I did was to try and parametrize the parabloid as: $$r(\theta,z)=\sqrt{z}\cos(\theta)i+\sqrt{z}\sin(\theta)j+zk$$ Then I found $||r_\theta\times r_z||=\frac{1}{2}\sqrt{4z+1}$. Hence the surface area is $\int\int _SdS=\int \int_D \frac{1}{2}\sqrt{4z+1}dA$. Here, $D$ is the region with $0\leq \theta \leq 2\pi$ and $0 \leq z\leq 4-x=4-\sqrt{z}\cos(\theta)$. But Here is my problem, I can't find the upper limit in of z in this integral, the lower limit is 0. But I can't find an upper limit $g(\theta)$. How do I get rid of the $z$ from the limit? Any help would be appreciated I also tried as suggested below to change to a new parametrization but I always go back to the same integral, any hints here?","So the first thing I did was to try and parametrize the parabloid as: $$r(\theta,z)=\sqrt{z}\cos(\theta)i+\sqrt{z}\sin(\theta)j+zk$$ Then I found $||r_\theta\times r_z||=\frac{1}{2}\sqrt{4z+1}$. Hence the surface area is $\int\int _SdS=\int \int_D \frac{1}{2}\sqrt{4z+1}dA$. Here, $D$ is the region with $0\leq \theta \leq 2\pi$ and $0 \leq z\leq 4-x=4-\sqrt{z}\cos(\theta)$. But Here is my problem, I can't find the upper limit in of z in this integral, the lower limit is 0. But I can't find an upper limit $g(\theta)$. How do I get rid of the $z$ from the limit? Any help would be appreciated I also tried as suggested below to change to a new parametrization but I always go back to the same integral, any hints here?",,"['calculus', 'integration', 'multivariable-calculus', 'surfaces', 'parametric']"
79,Accurate $\zeta(s)$ integral identities for $\sum_\limits{n=2}^{\infty}\frac{1}{n^{s}\sqrt{\ln{n}}}$,Accurate  integral identities for,\zeta(s) \sum_\limits{n=2}^{\infty}\frac{1}{n^{s}\sqrt{\ln{n}}},"Some time ago while doing formal symbolic manipulations for fun (without worrying about convergence or getting into analysis ) to see where I would get, I did the following manipulation: Starting with the following formula (which I think is quite well known and follows from 3.325 in this book by substitution changing the variable of integration to $\sqrt{x}$): $$\int_{0}^{\infty}x^{-\frac{1}{2}}e^{-ax-\frac{ab}{x}}dx=\sqrt{\frac{\pi}{a}}e^{-2\sqrt{ab}}$$ I let $a=\ln{n}$ and took a sum from $n=2$ to $\infty$ and then interchanged integration and summation on the left and used the definition of the zeta function to get: $$\int_{0}^{\infty}x^{-\frac{1}{2}}\left[\zeta\left(x+\frac{b}{x}\right)-1\right]dx=\sqrt{\pi}\sum_{n=2}^{\infty}\frac{1}{n^{2\sqrt{b}}\sqrt{\ln{n}}}$$ I then let $s=2\sqrt{b}$ and performed the substitution $x\rightarrow u^2$ in the integral to arrive at the following interesting formula: $$\sum_{n=2}^{\infty}\frac{1}{n^{s}\sqrt{\ln{n}}}=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}\zeta\left(x^{2}+\frac{s^2}{4 x^2}\right)-1 \;dx \tag{1}$$ At a different time, I took $\int_{0}^{\infty}g(t)\sum_\limits{n=2}^{\infty}\frac{1}{n^{st}}dt$ for arbitrary $g(t)$ and rearranged the summation and integration and used the definition of the Laplace transform to turn this into $\sum_\limits{n=2}^{\infty}L[g(t)](s\ln{n})$. I then substituted in $g(t)=H(t-a)f(t-a)$ where $H(t)$ is the Heaviside function and $f(t)$ is arbitrary and used some Laplace transform identities and the definition of the zeta function to get: $$\int_{0}^{\infty}\left(\zeta(st)-1\right)H(t-a)f(t-a)dt=\sum_{n=2}^{\infty}e^{-as\ln{n}}L[f(t)](s\ln{n})$$ $$=\int_{a}^{\infty}\left(\zeta(st)-1\right) f(t-a)dt$$ Then I set $f(t)=\frac{1}{\pi\sqrt{t}}$ and evaluated the Laplace transform to get: $$\sum_{n=2}^{\infty}\frac{1}{n^{as}\sqrt{s\ln{n}}}=\int_{a}^{\infty}\frac{\zeta(st)-1}{\sqrt{\pi(t-a)}}dt$$ I then made the substitution $t\rightarrow a+\frac{x^2}{s}$ and set $a=1$ to finally get: $$\sum_{n=2}^{\infty}\frac{1}{n^{s}\sqrt{\ln{n}}}=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}\zeta(x^{2}+s)-1 \; dx \tag{2}$$ I think that equations $(1)$ and $(2)$ are quite beautiful, but I do not know whether they are true or not because of the formal kind of way I derived them which can give spurious results as I've found . Although the 2 integrals look similar, I am not sure if they are actually directly related (since I cannot think of a substitution that would turn one into the other) or if the apparent similarity is just a coincidence. When I tested a few special values of $s$ with Wolfram Alpha, e.g. with $s=\pi$ here , here and here I got that the difference between the two integrals was on the order of $10^{-14}$ and the error in each of the two identities was around $10^{-12}$. Not being familiar with the error in Wolfram Alpha infinite calculations, I am not sure whether that implies that my representations are wrong or not, and I know there can be some very accurate near identities . Other very accurate but not exact approximations I have derived have had errors on the order of $10^{-16}$, which makes me inclined to doubt the expressions' accuracy. I have not been able to find either of these purported identities anywhere. My question : does anyone know whether identities $(1)$ and $(2)$ are actually true or not, and if they are not does anyone know what the error terms are through a more accurate approach?","Some time ago while doing formal symbolic manipulations for fun (without worrying about convergence or getting into analysis ) to see where I would get, I did the following manipulation: Starting with the following formula (which I think is quite well known and follows from 3.325 in this book by substitution changing the variable of integration to $\sqrt{x}$): $$\int_{0}^{\infty}x^{-\frac{1}{2}}e^{-ax-\frac{ab}{x}}dx=\sqrt{\frac{\pi}{a}}e^{-2\sqrt{ab}}$$ I let $a=\ln{n}$ and took a sum from $n=2$ to $\infty$ and then interchanged integration and summation on the left and used the definition of the zeta function to get: $$\int_{0}^{\infty}x^{-\frac{1}{2}}\left[\zeta\left(x+\frac{b}{x}\right)-1\right]dx=\sqrt{\pi}\sum_{n=2}^{\infty}\frac{1}{n^{2\sqrt{b}}\sqrt{\ln{n}}}$$ I then let $s=2\sqrt{b}$ and performed the substitution $x\rightarrow u^2$ in the integral to arrive at the following interesting formula: $$\sum_{n=2}^{\infty}\frac{1}{n^{s}\sqrt{\ln{n}}}=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}\zeta\left(x^{2}+\frac{s^2}{4 x^2}\right)-1 \;dx \tag{1}$$ At a different time, I took $\int_{0}^{\infty}g(t)\sum_\limits{n=2}^{\infty}\frac{1}{n^{st}}dt$ for arbitrary $g(t)$ and rearranged the summation and integration and used the definition of the Laplace transform to turn this into $\sum_\limits{n=2}^{\infty}L[g(t)](s\ln{n})$. I then substituted in $g(t)=H(t-a)f(t-a)$ where $H(t)$ is the Heaviside function and $f(t)$ is arbitrary and used some Laplace transform identities and the definition of the zeta function to get: $$\int_{0}^{\infty}\left(\zeta(st)-1\right)H(t-a)f(t-a)dt=\sum_{n=2}^{\infty}e^{-as\ln{n}}L[f(t)](s\ln{n})$$ $$=\int_{a}^{\infty}\left(\zeta(st)-1\right) f(t-a)dt$$ Then I set $f(t)=\frac{1}{\pi\sqrt{t}}$ and evaluated the Laplace transform to get: $$\sum_{n=2}^{\infty}\frac{1}{n^{as}\sqrt{s\ln{n}}}=\int_{a}^{\infty}\frac{\zeta(st)-1}{\sqrt{\pi(t-a)}}dt$$ I then made the substitution $t\rightarrow a+\frac{x^2}{s}$ and set $a=1$ to finally get: $$\sum_{n=2}^{\infty}\frac{1}{n^{s}\sqrt{\ln{n}}}=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}\zeta(x^{2}+s)-1 \; dx \tag{2}$$ I think that equations $(1)$ and $(2)$ are quite beautiful, but I do not know whether they are true or not because of the formal kind of way I derived them which can give spurious results as I've found . Although the 2 integrals look similar, I am not sure if they are actually directly related (since I cannot think of a substitution that would turn one into the other) or if the apparent similarity is just a coincidence. When I tested a few special values of $s$ with Wolfram Alpha, e.g. with $s=\pi$ here , here and here I got that the difference between the two integrals was on the order of $10^{-14}$ and the error in each of the two identities was around $10^{-12}$. Not being familiar with the error in Wolfram Alpha infinite calculations, I am not sure whether that implies that my representations are wrong or not, and I know there can be some very accurate near identities . Other very accurate but not exact approximations I have derived have had errors on the order of $10^{-16}$, which makes me inclined to doubt the expressions' accuracy. I have not been able to find either of these purported identities anywhere. My question : does anyone know whether identities $(1)$ and $(2)$ are actually true or not, and if they are not does anyone know what the error terms are through a more accurate approach?",,"['calculus', 'real-analysis', 'sequences-and-series', 'laplace-transform', 'riemann-zeta']"
80,"Definite integral $\int_{-1}^{1} e^{\frac{1}{x^2-1}}\cos{ax}\,dx$",Definite integral,"\int_{-1}^{1} e^{\frac{1}{x^2-1}}\cos{ax}\,dx","So I've been attempting to solve this definite integral in my spare time for a couple of weeks, now, and I think I've used my entire bag of tricks. $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx$$ I think this function should be differentiable within the range of $(-1,1)$. I already found the following two topics which have helped greatly: Evaluate integral $\int_{-1}^{1} x^2 \exp(\frac{1}{x^2-1}) dx$ Evaluate definite integral $\int_{-1}^1 \exp(1/(x^2-1)) \, dx$ However, the addition of the $\cos{ax}$ term is confounding. It is an even function, so $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx = 2 \int_{0}^{1} e^{\frac{1}{x^2-1}}\cos{ax}\,dx.$$ I have also managed to express it as something like looks like a Fourier transform, but I got stuck at the end: $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx.$$ Following Euler's Formula, $e^{i\theta} = \cos\theta + i\sin\theta$, $$\cos\theta = e^{i\theta} - i\sin\theta$$ $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx$$ $$=\int_{-1}^{1} e^\frac{1}{x^2-1}\left(e^{iax} - i\sin{ax}\right)\,dx$$ $$=\int_{-1}^{1} e^\frac{1}{x^2-1} e^{iax}\,dx - i\int_{-1}^{1} e^{\frac{1}{x^2-1}} \sin{ax}\,dx.$$ Since the $\sin{ax}$ part of the equation is odd, it goes to zero (it also goes to zero since it's imaginary, and our original function is real): $$=\int_{-1}^{1} e^\frac{1}{x^2-1} e^{iax}\,dx.$$ By substituting $x\rightarrow t$ and $a\rightarrow \omega$, we get something that is essentially a Fourier transform, $$=\int_{-1}^{1} e^\frac{1}{t^2-1} e^{i\omega t}\,dt,$$ or $$=\int_{-\infty}^{\infty} f(t)\, e^{i\omega t}\,dt,$$ where $$f(t) =  \begin{cases}  e^\frac{1}{t^2-1} & -1 < t < 1 \\ 0 & \text{otherwise} \end{cases}.$$ While I can't seem to solve the example of $\mathcal{F}[f(t)]$, it occurs to me that I may be able to solve the example of $\mathcal{F}[\ln{f(t)}]$: $$\mathcal{F}[\ln{f(t)}] = \mathcal{F}\left[\frac{1}{t^2-1}\right],$$ which Wolfram Alpha gives as, $$=-\sqrt{\frac{\pi}{2}}\operatorname{sgn}\omega\sin\omega.$$ However, that doesn't seem to help. I can get this far: $$\frac{d}{dt}\ln f(t) = \frac{f'(t)}{f(t)},$$ and $$\mathcal{F}\left[\frac{d}{dt}f(t)\right] = i\omega\mathcal{F}[f(t)],$$ therefore $$\mathcal{F}\left[\frac{d}{dt}\ln f(t)\right] = i\omega\mathcal{F}[\ln f(t)] = \mathcal{F}\left[\frac{f'(t)}{f(t)}\right],$$ however, I don't think that gets me any closer to solving for $\mathcal{F}[f(t)].$ I also looked at this short discussion about determining $\mathcal{F}\left[e^{f(t)}\right]$ in terms of $\mathcal{F}[f(t)]$, but I didn't really get anywhere. Anyone have any good ideas?","So I've been attempting to solve this definite integral in my spare time for a couple of weeks, now, and I think I've used my entire bag of tricks. $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx$$ I think this function should be differentiable within the range of $(-1,1)$. I already found the following two topics which have helped greatly: Evaluate integral $\int_{-1}^{1} x^2 \exp(\frac{1}{x^2-1}) dx$ Evaluate definite integral $\int_{-1}^1 \exp(1/(x^2-1)) \, dx$ However, the addition of the $\cos{ax}$ term is confounding. It is an even function, so $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx = 2 \int_{0}^{1} e^{\frac{1}{x^2-1}}\cos{ax}\,dx.$$ I have also managed to express it as something like looks like a Fourier transform, but I got stuck at the end: $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx.$$ Following Euler's Formula, $e^{i\theta} = \cos\theta + i\sin\theta$, $$\cos\theta = e^{i\theta} - i\sin\theta$$ $$\int_{-1}^{1} e^\frac{1}{x^2-1}\cos{ax}\,dx$$ $$=\int_{-1}^{1} e^\frac{1}{x^2-1}\left(e^{iax} - i\sin{ax}\right)\,dx$$ $$=\int_{-1}^{1} e^\frac{1}{x^2-1} e^{iax}\,dx - i\int_{-1}^{1} e^{\frac{1}{x^2-1}} \sin{ax}\,dx.$$ Since the $\sin{ax}$ part of the equation is odd, it goes to zero (it also goes to zero since it's imaginary, and our original function is real): $$=\int_{-1}^{1} e^\frac{1}{x^2-1} e^{iax}\,dx.$$ By substituting $x\rightarrow t$ and $a\rightarrow \omega$, we get something that is essentially a Fourier transform, $$=\int_{-1}^{1} e^\frac{1}{t^2-1} e^{i\omega t}\,dt,$$ or $$=\int_{-\infty}^{\infty} f(t)\, e^{i\omega t}\,dt,$$ where $$f(t) =  \begin{cases}  e^\frac{1}{t^2-1} & -1 < t < 1 \\ 0 & \text{otherwise} \end{cases}.$$ While I can't seem to solve the example of $\mathcal{F}[f(t)]$, it occurs to me that I may be able to solve the example of $\mathcal{F}[\ln{f(t)}]$: $$\mathcal{F}[\ln{f(t)}] = \mathcal{F}\left[\frac{1}{t^2-1}\right],$$ which Wolfram Alpha gives as, $$=-\sqrt{\frac{\pi}{2}}\operatorname{sgn}\omega\sin\omega.$$ However, that doesn't seem to help. I can get this far: $$\frac{d}{dt}\ln f(t) = \frac{f'(t)}{f(t)},$$ and $$\mathcal{F}\left[\frac{d}{dt}f(t)\right] = i\omega\mathcal{F}[f(t)],$$ therefore $$\mathcal{F}\left[\frac{d}{dt}\ln f(t)\right] = i\omega\mathcal{F}[\ln f(t)] = \mathcal{F}\left[\frac{f'(t)}{f(t)}\right],$$ however, I don't think that gets me any closer to solving for $\mathcal{F}[f(t)].$ I also looked at this short discussion about determining $\mathcal{F}\left[e^{f(t)}\right]$ in terms of $\mathcal{F}[f(t)]$, but I didn't really get anywhere. Anyone have any good ideas?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'fourier-transform']"
81,A conjectured closed form of $\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx$,A conjectured closed form of,\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx,"Consider the following integral: $$\mathcal{I}=\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx.$$ I tried to evaluate $\mathcal{I}$ in a closed form (both manually and using Mathematica ), but without success. However, if WolframAlpha is provided with a numerical approximation $\,\mathcal{I}\approx 3.2694067500684...$, it returns a possible closed form:  $$\mathcal{I}\stackrel?=\frac\pi{2\,\ln^2 2}.$$ Further numeric caclulations show that this value is correct up to at least $10^3$ decimal digits. So, I conjecture that this is the exact value of $\mathcal{I}$. Question: Is this conjecture correct?","Consider the following integral: $$\mathcal{I}=\int\limits_0^\infty\frac{x-1}{\sqrt{2^x-1}\ \ln\left(2^x-1\right)}dx.$$ I tried to evaluate $\mathcal{I}$ in a closed form (both manually and using Mathematica ), but without success. However, if WolframAlpha is provided with a numerical approximation $\,\mathcal{I}\approx 3.2694067500684...$, it returns a possible closed form:  $$\mathcal{I}\stackrel?=\frac\pi{2\,\ln^2 2}.$$ Further numeric caclulations show that this value is correct up to at least $10^3$ decimal digits. So, I conjecture that this is the exact value of $\mathcal{I}$. Question: Is this conjecture correct?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'closed-form']"
82,Prove or disprove the limit of a definite integral,Prove or disprove the limit of a definite integral,,"I am trying to reproduce the work of a published paper where I need to evaluate a limit of a definite integral $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}\frac{2y^3-3y^2}{{(1-y)}^2}\right]\mathrm{d}y\,.$$ The author of the paper argues that because $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}\frac{2y^3-3y^2}{{(1-y)}^2}\right]\mathrm{d}y=\lim_{\xi\to\infty}\xi\int_0^1\exp\left[-\frac{\xi^2}{4}\sum_{n=3}^\infty ny^{n-1}\right]\mathrm{d}y\,,$$ the contribution from higher orders of $y$ is negligible when $\xi\to\infty$, so $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}\frac{2y^3-3y^2}{{(1-y)}^2}\right]\mathrm{d}y=\lim_{\xi\to\infty}\xi\int_0^1\exp\left[-\frac{\xi^2}{4}3y^2\right]\mathrm{d}y=\sqrt{\frac{\pi}{3}}\lim_{\xi\to\infty}\mathrm{erf}\left(\frac{\sqrt{3}\xi}{2}\right)=\sqrt{\frac{\pi}{3}}\,.$$ I am not sure if it is OK just to throw away all the higher order terms of $y$, but numerical evaluation shows the limit is correct. Is there some better way to obtain this limit? It looks that if we can find a function $f(y)$ such that $$f(y)\leq\frac{2y^3-3y^2}{{(1-y)}^2}\leq-3y^2$$ and $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}f(y)\right]\mathrm{d}y=\sqrt{\frac{\pi}{3}}\,,$$ then from the squeeze rule the limit is correct. I tried to find an appropriate $f(y)$ but did not make much progress.","I am trying to reproduce the work of a published paper where I need to evaluate a limit of a definite integral $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}\frac{2y^3-3y^2}{{(1-y)}^2}\right]\mathrm{d}y\,.$$ The author of the paper argues that because $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}\frac{2y^3-3y^2}{{(1-y)}^2}\right]\mathrm{d}y=\lim_{\xi\to\infty}\xi\int_0^1\exp\left[-\frac{\xi^2}{4}\sum_{n=3}^\infty ny^{n-1}\right]\mathrm{d}y\,,$$ the contribution from higher orders of $y$ is negligible when $\xi\to\infty$, so $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}\frac{2y^3-3y^2}{{(1-y)}^2}\right]\mathrm{d}y=\lim_{\xi\to\infty}\xi\int_0^1\exp\left[-\frac{\xi^2}{4}3y^2\right]\mathrm{d}y=\sqrt{\frac{\pi}{3}}\lim_{\xi\to\infty}\mathrm{erf}\left(\frac{\sqrt{3}\xi}{2}\right)=\sqrt{\frac{\pi}{3}}\,.$$ I am not sure if it is OK just to throw away all the higher order terms of $y$, but numerical evaluation shows the limit is correct. Is there some better way to obtain this limit? It looks that if we can find a function $f(y)$ such that $$f(y)\leq\frac{2y^3-3y^2}{{(1-y)}^2}\leq-3y^2$$ and $$\lim_{\xi\to\infty}\xi\int_0^1\exp\left[\frac{\xi^2}{4}f(y)\right]\mathrm{d}y=\sqrt{\frac{\pi}{3}}\,,$$ then from the squeeze rule the limit is correct. I tried to find an appropriate $f(y)$ but did not make much progress.",,"['calculus', 'limits', 'definite-integrals']"
83,Fourier transform of integral related to zeta function,Fourier transform of integral related to zeta function,,"In this MO question here , I asked about the Fourier transform of the zeta function. The second answer lists the following as a representation for $\zeta(s)$, with $E(x)$ as the floor function: \begin{multline} \zeta(\frac 1 2 +it)=-it\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx -\frac 1 2\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx\\-\frac{1+2it}{1-2it}, \tag{$\ast$} \end{multline} The idea is that by getting the Fourier transform of the expression $\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx$, we can essentially get the whole thing. To get this Fourier transform, we need to solve the double integral $F(\omega) = \int_{-\infty}^{\infty} \left[\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx \right] e^{-i\omega t} dt$ The answer then goes onto say: With the above formula, it is easy to find an explicit expression for the Fourier transform: in fact, we need only to calculate the Fourier transform of $t\mapsto e^{-it \ln x}$, which is $\delta_0(\tau+\frac{\ln x}{2π})$... As a result, the Fourier transform of the second term in $(\ast)$ is given by   $$ \int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 }\delta_0(\tau+\frac{\ln x}{2π})dx, $$ I am somewhat at a loss for how to arrive at this result. How do you go from the double integral to this? It doesn't seem like you can flip the order of the integrals here since there's that $e^{-i\omega t}$ on the outside.","In this MO question here , I asked about the Fourier transform of the zeta function. The second answer lists the following as a representation for $\zeta(s)$, with $E(x)$ as the floor function: \begin{multline} \zeta(\frac 1 2 +it)=-it\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx -\frac 1 2\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx\\-\frac{1+2it}{1-2it}, \tag{$\ast$} \end{multline} The idea is that by getting the Fourier transform of the expression $\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx$, we can essentially get the whole thing. To get this Fourier transform, we need to solve the double integral $F(\omega) = \int_{-\infty}^{\infty} \left[\int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 -it }dx \right] e^{-i\omega t} dt$ The answer then goes onto say: With the above formula, it is easy to find an explicit expression for the Fourier transform: in fact, we need only to calculate the Fourier transform of $t\mapsto e^{-it \ln x}$, which is $\delta_0(\tau+\frac{\ln x}{2π})$... As a result, the Fourier transform of the second term in $(\ast)$ is given by   $$ \int_{1}^{+\infty}\bigl(x-E(x)\bigr)x^{-\frac3 2 }\delta_0(\tau+\frac{\ln x}{2π})dx, $$ I am somewhat at a loss for how to arrive at this result. How do you go from the double integral to this? It doesn't seem like you can flip the order of the integrals here since there's that $e^{-i\omega t}$ on the outside.",,"['calculus', 'integration', 'fourier-analysis', 'fourier-transform', 'multiple-integral']"
84,"Infinite tetration, convergence radius","Infinite tetration, convergence radius",,"I got this problem from my teacher as a optional challenge. I am open about this being a given problem, however it is not homework. The problem is stated as follows. Assume we have an infinite tetration as follows $$x^{x^{x^{.^{.^.}}}} \, = \, a$$ With a given $a$ find $x$ . The next part of the problem was to discuss the convergence radius of a. If a is too big or too small the tetration does not converge. Below is my humble stab at the problem. My friend said you would have to treat the tetration as a infinite series, and therefore could not perform algebraic manipulations on it before it is know whether it converges or diverges. However my attempt is to first do some algebraic steps, then discuss the convergence radius. I) Initial discussion At the start it is obvious that the tetration converges when $a=1$ (just set $x=1$ ) Now after some computer hardwork it seems that the tetration fails to converge when a is roughly larger than 3. II) Algebraic manipulation $$ x^{x^{x^{.^{.^.}}}} \, = \, a$$ This is the same as $$ x^a \, = \, a$$ $$ \log_a(x^a) \, = \, \log_a(a)$$ $$ \log_a(x) \, = \, \frac{1}{a}$$ $$ x \, = \, a^{\frac{1}{a}}$$ Now, if we let $a=2$ then $x = \sqrt{2}$ . After some more computational work, this seems to be correct, which makes me believe this formula is correct. III) Discsussion about convergence By looking at the derivative of $ \displaystyle \large a^{\frac{1}{a}} $ we see that the maxima occurs when $a=e$ , which also seems to correspond to the inital computational work. Now I think, that the minima of $\displaystyle \large a^{1/a}$ is zero by looking at its graph, studying its derivative and the endpoints. So that my ""guess"" or work shows that it converges when $$ a \in [0 \, , \,  1/e] $$ VI) My questions Can my algebraic manipulations be justified? They seem rather sketchy taking the a`th logarithm and so on . (Although they seem to ""magically"" give out the right answer) By looking at Wikipedia it seems that the tetration converge when $$ a \in \left[ 1/e \, , \, e \right] $$ This is almost what I have, why is my lower bound wrong? How can  I find the correct lower bound?","I got this problem from my teacher as a optional challenge. I am open about this being a given problem, however it is not homework. The problem is stated as follows. Assume we have an infinite tetration as follows With a given find . The next part of the problem was to discuss the convergence radius of a. If a is too big or too small the tetration does not converge. Below is my humble stab at the problem. My friend said you would have to treat the tetration as a infinite series, and therefore could not perform algebraic manipulations on it before it is know whether it converges or diverges. However my attempt is to first do some algebraic steps, then discuss the convergence radius. I) Initial discussion At the start it is obvious that the tetration converges when (just set ) Now after some computer hardwork it seems that the tetration fails to converge when a is roughly larger than 3. II) Algebraic manipulation This is the same as Now, if we let then . After some more computational work, this seems to be correct, which makes me believe this formula is correct. III) Discsussion about convergence By looking at the derivative of we see that the maxima occurs when , which also seems to correspond to the inital computational work. Now I think, that the minima of is zero by looking at its graph, studying its derivative and the endpoints. So that my ""guess"" or work shows that it converges when VI) My questions Can my algebraic manipulations be justified? They seem rather sketchy taking the a`th logarithm and so on . (Although they seem to ""magically"" give out the right answer) By looking at Wikipedia it seems that the tetration converge when This is almost what I have, why is my lower bound wrong? How can  I find the correct lower bound?","x^{x^{x^{.^{.^.}}}} \, = \, a a x a=1 x=1  x^{x^{x^{.^{.^.}}}} \, = \, a  x^a \, = \, a  \log_a(x^a) \, = \, \log_a(a)  \log_a(x) \, = \, \frac{1}{a}  x \, = \, a^{\frac{1}{a}} a=2 x = \sqrt{2}  \displaystyle \large a^{\frac{1}{a}}  a=e \displaystyle \large a^{1/a}  a \in [0 \, , \,  1/e]   a \in \left[ 1/e \, , \, e \right] ","['real-analysis', 'convergence-divergence', 'exponentiation', 'tetration']"
85,Flawed AP Calc question? Inflection points.,Flawed AP Calc question? Inflection points.,,"The following question was presented to me by a tutoring student in AP Calculus. It's supposedly from a practice test - not sure if it's official. Here's the issue. Below I've reproduced the complete graph of some continuous function $f(x)$. The question asks us to identify what $x$ are inflection points and to offer an explanation as to why. My student's teacher provided her class with an answer that $x=2$ is an inflection point. I disagree. $f'(x)$ is discontinuous at $x=2$ and $f''(x)$ does not exist! Furthermore, there is no tangent at $x=2$. I see no transition from concavity to convexity anywhere here. From the definitions of an inflection point provided in Stewart's and Thomas's textbooks, no inflection points exist . Can anyone chime in here? Is anyone aware of an alternate definition of inflection point - especially one that is taught in US high schools - that could have been intended here?","The following question was presented to me by a tutoring student in AP Calculus. It's supposedly from a practice test - not sure if it's official. Here's the issue. Below I've reproduced the complete graph of some continuous function $f(x)$. The question asks us to identify what $x$ are inflection points and to offer an explanation as to why. My student's teacher provided her class with an answer that $x=2$ is an inflection point. I disagree. $f'(x)$ is discontinuous at $x=2$ and $f''(x)$ does not exist! Furthermore, there is no tangent at $x=2$. I see no transition from concavity to convexity anywhere here. From the definitions of an inflection point provided in Stewart's and Thomas's textbooks, no inflection points exist . Can anyone chime in here? Is anyone aware of an alternate definition of inflection point - especially one that is taught in US high schools - that could have been intended here?",,['calculus']
86,Swapping limit at infinity with limit at 0,Swapping limit at infinity with limit at 0,,I am trying to calculate: $$\lim_{x\rightarrow\infty}x^2\sin\left(\frac{1}{x^2}\right) $$ I am pretty sure that this is equivalent to calculating: $$\lim_{k\rightarrow0}\frac{\sin(k)}{k}=1 $$ Since $k=\dfrac{1}{x^2}$ and $\lim\limits_{x\rightarrow\infty}k=0$. Is there any way I can make this formal?,I am trying to calculate: $$\lim_{x\rightarrow\infty}x^2\sin\left(\frac{1}{x^2}\right) $$ I am pretty sure that this is equivalent to calculating: $$\lim_{k\rightarrow0}\frac{\sin(k)}{k}=1 $$ Since $k=\dfrac{1}{x^2}$ and $\lim\limits_{x\rightarrow\infty}k=0$. Is there any way I can make this formal?,,"['calculus', 'limits']"
87,limit of a region of integration in $\mathbb{R}^2$ approaches a line,limit of a region of integration in  approaches a line,\mathbb{R}^2,"I am trying to follow the derivation of derivatives in a paper published in some japanese journal but there seems to be a mistake in the proof. I will present the problem in 2D and in 2 variables so that it's easier to visualize. Let $c_1=(x_1,y_1)$ and $c_2 = (x_2,y_2)$ be two points in $\mathbb{R^2}$ which I will denote as centers. The domain will be a unit square with vertices at $(0,0),(1,1),$ etc. Define $V_1 = \{z=(x,y):\|z-c_1\| \le ||z-c_2||\}$ and $V_2 = \{z=(x,y): ||z-c_2|| \le ||z-c_1||\}$. Define the function $$G(c_1,c_2) = \displaystyle \int_{V_1}2f'(||z-c_1||^2)(x_1-z_1)\phi(z)\,dz$$ where $z = (z_1,z_2)$. For ease of notation, I will denote $h(z,c_1)=2f'(\|z-c_1\|^2)(x_1-z_1)\phi(z).$ My goal is to compute $$\frac{\partial G}{\partial{x_2}}.$$ Notice that the integrand does not depend on $x_2$ but rather $V_1$ does. So a first step in this derivation is to consider $$\Delta G=G(c_1,c_2+he_1)-G(c_1,c_2)$$ where $e_1 = (1,0)$ and $h \in \mathbb{R}$ and subsequently compute the divided difference as $h \rightarrow 0$. If $V_1' = \{z=(x,y): \|z-c_1\| \le \|z - (c_2+he_1)\|\}$ and $V_2' = \{z - (x,y): \|z-(c_2+he_1)\| \le \|z-c_1\|\}$ denote the modified regions about the shifted centers, we have that $$\Delta G = \displaystyle \int_{V_1'\cap V_2}h(z,c_1)\, dz-\int_{V_1 \cap V_2'} h(z,c_1) \,dz $$ as can be seen from, for example: .  In the picture, the red region corresponds to $V_1' \cap V_2$ while the yellow region corresponds to $V_2' \cap V_1$. The derivative, as is presented in other sources (legitimate) but without proof is: $$\frac{\partial G}{\partial x_2} = \displaystyle \int_W \frac{x_2-z_2}{\|c_1-c_2\|}h(z,c_1) \,dz$$ where $W$ is the hyperplane $V_1 \cap V_2 = \partial V_1 \cap \partial V_2$, i.e. the common line segment of the regions $V_1,V_2.$ I am trying to do reverse engineering, i.e. trying to make sense of the result to see how to proceed with the derivation. But what throws me off is the presence of a ""unit-like normal vector"" in the form of $\frac{x_2 - z_2}{\|c_1-c_2\|}$. If it helps, it's a standard property that the vector $c_1-c_2$ is orthogonal to $\partial V_1 \cap \partial V_2$. Because of this, I've been thinking that maybe the divergence theorem could be invoked in some way but I just don't see it. I am also aware that as $h \rightarrow 0, (V_1'\cap V_2) \cup (V_2'\cap V_2) = \partial V_1 \cap \partial V_2$. Any ideas how to proceed?","I am trying to follow the derivation of derivatives in a paper published in some japanese journal but there seems to be a mistake in the proof. I will present the problem in 2D and in 2 variables so that it's easier to visualize. Let $c_1=(x_1,y_1)$ and $c_2 = (x_2,y_2)$ be two points in $\mathbb{R^2}$ which I will denote as centers. The domain will be a unit square with vertices at $(0,0),(1,1),$ etc. Define $V_1 = \{z=(x,y):\|z-c_1\| \le ||z-c_2||\}$ and $V_2 = \{z=(x,y): ||z-c_2|| \le ||z-c_1||\}$. Define the function $$G(c_1,c_2) = \displaystyle \int_{V_1}2f'(||z-c_1||^2)(x_1-z_1)\phi(z)\,dz$$ where $z = (z_1,z_2)$. For ease of notation, I will denote $h(z,c_1)=2f'(\|z-c_1\|^2)(x_1-z_1)\phi(z).$ My goal is to compute $$\frac{\partial G}{\partial{x_2}}.$$ Notice that the integrand does not depend on $x_2$ but rather $V_1$ does. So a first step in this derivation is to consider $$\Delta G=G(c_1,c_2+he_1)-G(c_1,c_2)$$ where $e_1 = (1,0)$ and $h \in \mathbb{R}$ and subsequently compute the divided difference as $h \rightarrow 0$. If $V_1' = \{z=(x,y): \|z-c_1\| \le \|z - (c_2+he_1)\|\}$ and $V_2' = \{z - (x,y): \|z-(c_2+he_1)\| \le \|z-c_1\|\}$ denote the modified regions about the shifted centers, we have that $$\Delta G = \displaystyle \int_{V_1'\cap V_2}h(z,c_1)\, dz-\int_{V_1 \cap V_2'} h(z,c_1) \,dz $$ as can be seen from, for example: .  In the picture, the red region corresponds to $V_1' \cap V_2$ while the yellow region corresponds to $V_2' \cap V_1$. The derivative, as is presented in other sources (legitimate) but without proof is: $$\frac{\partial G}{\partial x_2} = \displaystyle \int_W \frac{x_2-z_2}{\|c_1-c_2\|}h(z,c_1) \,dz$$ where $W$ is the hyperplane $V_1 \cap V_2 = \partial V_1 \cap \partial V_2$, i.e. the common line segment of the regions $V_1,V_2.$ I am trying to do reverse engineering, i.e. trying to make sense of the result to see how to proceed with the derivation. But what throws me off is the presence of a ""unit-like normal vector"" in the form of $\frac{x_2 - z_2}{\|c_1-c_2\|}$. If it helps, it's a standard property that the vector $c_1-c_2$ is orthogonal to $\partial V_1 \cap \partial V_2$. Because of this, I've been thinking that maybe the divergence theorem could be invoked in some way but I just don't see it. I am also aware that as $h \rightarrow 0, (V_1'\cap V_2) \cup (V_2'\cap V_2) = \partial V_1 \cap \partial V_2$. Any ideas how to proceed?",,"['calculus', 'integration', 'derivatives']"
88,How to classify/ solve this PDE?,How to classify/ solve this PDE?,,"I am searching how to solve the PDE below but I can not seem to find a decent example online. My major did not focus much in solving PDEs so I feel very deficient. I know how to solve for the steady state - but I am wondering if I can solve the following analytically, not numerically. \begin{gather*} \frac{\partial F(x, t)}{\partial t} = -g(x)F(x, t) -A\frac{\partial F(x, t)}{\partial x}-B\frac{\partial^{2}F(x, t)}{\partial x^{2}} \\ \\ IC: F(x, 0) = 1 \end{gather*} I thought about doing a Fourier transform but I end up with a convolution in freq space - and that would be hard to handle. I also know that this is an advection - diffusion - convection PDE but its not specific enough to return any nice Google searches. Any suggestion or advice would be greatly appreciated. Thank you in advance! Follow up: Consider Case 1, where function g(t) is only a function of time. \begin{gather*} g(t) :  = \frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta } \\ \\ \frac{\partial F(x, t)}{\partial t} = -\left(\frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta }\right)F(x, t)  -A\frac{\partial F(x, t)}{\partial x}-B\frac{\partial^{2}F(x, t)}{\partial x^{2}} \\ \\ \frac{\partial\tilde{F}(\omega , t)}{\partial t} = -\left(\frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta }\right)\tilde{F}(\omega , t)  -A(i2\pi \omega )\tilde{F}(\omega , t)+B(i2\pi \omega )^{2}\tilde{F}(\omega , t) \\ \\ \frac{\partial\tilde{F}(\omega , t)}{\partial t} = \left[-\left(\frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta }\right)-A(i2\pi \omega )+B(i2\pi \omega )^{2}\right]\tilde{F}(\omega , t) \\ \\ \mbox{The DE is of the form:} \\ y^{\prime}+ay = 0\mbox{ so when I solve I get: }\ \\ \\ \tilde{F}(\omega , t) =   Ce^{\left(\frac{t}{\beta }\right)^{\alpha }+(A(i2\pi \omega )-B(i2\pi \omega )^{2})t} \\ \\ \mbox{-At this point I don't know how to apply the IC but if I assume C=1,} \\ \mbox{and take the inverse Fourier - I get the solution below - which looks good.} \\ \\ F(x, t) =   \frac{e^{\left(\frac{t}{\beta }\right)^{\alpha }-\frac{(x+At)^{2}}{4Bt}}}{2\sqrt{\pi }\sqrt{Bt}} \end{gather*} When I plug in fixed values for all variables - I get the same values but opposite signs. This is somewhat good news but still I am a little confused. Additional evidence Here is the Mathematica ourput for the two derivative terms. Note the second derivative becomes positive. A-Term: In[2]:= FourierTransform[-A*D[f[t, x], x], x, w, FourierParameters -> {0, -2 Pi}]  Out[2]= -2 i A Pi w FourierTransform[f[t, x], x, w] B-Term: In[1]:= FourierTransform[-B*D[f[t, x], x, x], x, w, FourierParameters -> {0, -2 Pi}]  Out[1]= 4 B Pi^2 w^2 FourierTransform[f[t, x], x, w] I kept everything consistent with Mathematica; therefore, I am confident my signs should be right. I really hope someone can shed some light into this!!! I am almost depressed about it.","I am searching how to solve the PDE below but I can not seem to find a decent example online. My major did not focus much in solving PDEs so I feel very deficient. I know how to solve for the steady state - but I am wondering if I can solve the following analytically, not numerically. \begin{gather*} \frac{\partial F(x, t)}{\partial t} = -g(x)F(x, t) -A\frac{\partial F(x, t)}{\partial x}-B\frac{\partial^{2}F(x, t)}{\partial x^{2}} \\ \\ IC: F(x, 0) = 1 \end{gather*} I thought about doing a Fourier transform but I end up with a convolution in freq space - and that would be hard to handle. I also know that this is an advection - diffusion - convection PDE but its not specific enough to return any nice Google searches. Any suggestion or advice would be greatly appreciated. Thank you in advance! Follow up: Consider Case 1, where function g(t) is only a function of time. \begin{gather*} g(t) :  = \frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta } \\ \\ \frac{\partial F(x, t)}{\partial t} = -\left(\frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta }\right)F(x, t)  -A\frac{\partial F(x, t)}{\partial x}-B\frac{\partial^{2}F(x, t)}{\partial x^{2}} \\ \\ \frac{\partial\tilde{F}(\omega , t)}{\partial t} = -\left(\frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta }\right)\tilde{F}(\omega , t)  -A(i2\pi \omega )\tilde{F}(\omega , t)+B(i2\pi \omega )^{2}\tilde{F}(\omega , t) \\ \\ \frac{\partial\tilde{F}(\omega , t)}{\partial t} = \left[-\left(\frac{\alpha \left(\frac{t}{\beta }\right)^{\alpha -1}}{\beta }\right)-A(i2\pi \omega )+B(i2\pi \omega )^{2}\right]\tilde{F}(\omega , t) \\ \\ \mbox{The DE is of the form:} \\ y^{\prime}+ay = 0\mbox{ so when I solve I get: }\ \\ \\ \tilde{F}(\omega , t) =   Ce^{\left(\frac{t}{\beta }\right)^{\alpha }+(A(i2\pi \omega )-B(i2\pi \omega )^{2})t} \\ \\ \mbox{-At this point I don't know how to apply the IC but if I assume C=1,} \\ \mbox{and take the inverse Fourier - I get the solution below - which looks good.} \\ \\ F(x, t) =   \frac{e^{\left(\frac{t}{\beta }\right)^{\alpha }-\frac{(x+At)^{2}}{4Bt}}}{2\sqrt{\pi }\sqrt{Bt}} \end{gather*} When I plug in fixed values for all variables - I get the same values but opposite signs. This is somewhat good news but still I am a little confused. Additional evidence Here is the Mathematica ourput for the two derivative terms. Note the second derivative becomes positive. A-Term: In[2]:= FourierTransform[-A*D[f[t, x], x], x, w, FourierParameters -> {0, -2 Pi}]  Out[2]= -2 i A Pi w FourierTransform[f[t, x], x, w] B-Term: In[1]:= FourierTransform[-B*D[f[t, x], x, x], x, w, FourierParameters -> {0, -2 Pi}]  Out[1]= 4 B Pi^2 w^2 FourierTransform[f[t, x], x, w] I kept everything consistent with Mathematica; therefore, I am confident my signs should be right. I really hope someone can shed some light into this!!! I am almost depressed about it.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis']"
89,Differential equation $f'(x)=\alpha\cdot f(x-1)^\beta$,Differential equation,f'(x)=\alpha\cdot f(x-1)^\beta,"Is there a way to solve $$f'(x)=\alpha\cdot f(x-1)^\beta,$$ where $\alpha>0,\beta\neq0.$ I know that if the arguments matched, I could use separation of variables to get $\frac{1}{1-\beta}f(x)^{1-\beta}=ax+C$, unless $\beta=1$, in which case I get a log. Here though, I don't even know how to start because the derivative is in terms of $x$ while the function itself is in terms of $x-1$.","Is there a way to solve $$f'(x)=\alpha\cdot f(x-1)^\beta,$$ where $\alpha>0,\beta\neq0.$ I know that if the arguments matched, I could use separation of variables to get $\frac{1}{1-\beta}f(x)^{1-\beta}=ax+C$, unless $\beta=1$, in which case I get a log. Here though, I don't even know how to start because the derivative is in terms of $x$ while the function itself is in terms of $x-1$.",,"['calculus', 'ordinary-differential-equations', 'delay-differential-equations']"
90,"How would I complete my proof that $\int_a^bf(g(x))\,dx = \int_{g(a)}^{g(b)}f(x)\frac{d}{dx}(g^{-1}(x))\,dx$?",How would I complete my proof that ?,"\int_a^bf(g(x))\,dx = \int_{g(a)}^{g(b)}f(x)\frac{d}{dx}(g^{-1}(x))\,dx","Around two years ago during a second semester calculus class, my professor remarked that $\int\sin(x^2)\,dx$ could not be integrated. Being a bit defiant, I tried (in vain) to prove him wrong.  While my efforts (obviously) were not successful, I did notice that: $$\int_a^bf(g(x))\,dx = \int_{g(a)}^{g(b)}f(x)\frac{d}{dx}(g^{-1}(x))\,dx$$ Will hold if $g(x)$ is strictly increasing or strictly decreasing on the interval from $a$ to $b$. I worked this out via intuition initially, but I recently decided I would try to prove it formally.  What I have so far is below. Let $a \in \mathbb{R}$, $b \in \mathbb{R}$ with $a < b$. Let $f: \mathbb{R} \to \mathbb{R}$ and $g: \mathbb{R} \to \mathbb{R}$ be functions such that $g'(x) > 0$ for $a < x < b$. Let $P_n = \{[x_0,x_1],[x_1,x_2],...,[x_{n-1},x_n]\}$ be a partition of $[a,b]$, such that $\lim_{n\to\infty} x_{i} - x_{i-1} = 0$ for any $i \in \mathbb{N}$. Then: $$\lim_{n\to\infty} \sum_{i=1}^n (f(g(x_i)))(x_i-x_{i-1}) = \int_a^b f(g(x))\,dx$$ Let $S_n = \{[g(x_0),g(x_1)],[g(x_1),g(x_2)],...,[g(x_{n-1}),g(x_n)]\}$ be a partition of $[g(a),g(b)]$. At this point I planned to show: $$(f(g(x_i))\frac{d}{dx}g^{-1}(g(x_i)))(g(x_i)-g(x_{i-1})) = (f(g(x_i)))(x_i-x_{i-1})$$ I have intuition that this would work, but I can't see how to actually do it. I feel like I am missing something obvious, I just can't tell what that is. With that result I planned to finish the proof by showing that \begin{align}\int_{g(a)}^{g(b)}f(x)\frac{d}{dx}(g^{-1}(x))\,dx =& \lim_{n\to\infty} \sum_{i=1}^n (f(g(x_i))\frac{d}{dx}g^{-1}(g(x_i)))(g(x_i)-g(x_{i-1}))\\ =& \lim_{n\to\infty} \sum_{i=1}^n (f(g(x_i)))(x_i-x_{i-1})\\ =&\int_a^bf(g(x))\,dx\end{align} I have a pretty good idea of how I would do these last steps, it is just showing that one equality that is tripping me up. So my question is how would I complete this proof or modify it so that it can be completed easily? Or if the original assertion is flawed please provide a counterexample. I know you could probably get this result through substitution, but I wanted to try to get it with Riemann sums for practice.","Around two years ago during a second semester calculus class, my professor remarked that $\int\sin(x^2)\,dx$ could not be integrated. Being a bit defiant, I tried (in vain) to prove him wrong.  While my efforts (obviously) were not successful, I did notice that: $$\int_a^bf(g(x))\,dx = \int_{g(a)}^{g(b)}f(x)\frac{d}{dx}(g^{-1}(x))\,dx$$ Will hold if $g(x)$ is strictly increasing or strictly decreasing on the interval from $a$ to $b$. I worked this out via intuition initially, but I recently decided I would try to prove it formally.  What I have so far is below. Let $a \in \mathbb{R}$, $b \in \mathbb{R}$ with $a < b$. Let $f: \mathbb{R} \to \mathbb{R}$ and $g: \mathbb{R} \to \mathbb{R}$ be functions such that $g'(x) > 0$ for $a < x < b$. Let $P_n = \{[x_0,x_1],[x_1,x_2],...,[x_{n-1},x_n]\}$ be a partition of $[a,b]$, such that $\lim_{n\to\infty} x_{i} - x_{i-1} = 0$ for any $i \in \mathbb{N}$. Then: $$\lim_{n\to\infty} \sum_{i=1}^n (f(g(x_i)))(x_i-x_{i-1}) = \int_a^b f(g(x))\,dx$$ Let $S_n = \{[g(x_0),g(x_1)],[g(x_1),g(x_2)],...,[g(x_{n-1}),g(x_n)]\}$ be a partition of $[g(a),g(b)]$. At this point I planned to show: $$(f(g(x_i))\frac{d}{dx}g^{-1}(g(x_i)))(g(x_i)-g(x_{i-1})) = (f(g(x_i)))(x_i-x_{i-1})$$ I have intuition that this would work, but I can't see how to actually do it. I feel like I am missing something obvious, I just can't tell what that is. With that result I planned to finish the proof by showing that \begin{align}\int_{g(a)}^{g(b)}f(x)\frac{d}{dx}(g^{-1}(x))\,dx =& \lim_{n\to\infty} \sum_{i=1}^n (f(g(x_i))\frac{d}{dx}g^{-1}(g(x_i)))(g(x_i)-g(x_{i-1}))\\ =& \lim_{n\to\infty} \sum_{i=1}^n (f(g(x_i)))(x_i-x_{i-1})\\ =&\int_a^bf(g(x))\,dx\end{align} I have a pretty good idea of how I would do these last steps, it is just showing that one equality that is tripping me up. So my question is how would I complete this proof or modify it so that it can be completed easily? Or if the original assertion is flawed please provide a counterexample. I know you could probably get this result through substitution, but I wanted to try to get it with Riemann sums for practice.",,"['calculus', 'integration']"
91,Sequence involving floor function - limit and bounds,Sequence involving floor function - limit and bounds,,"My first time here, so please excuse any breaches of etiquette. For a given  $p \in \mathbb N$  and irrational $\alpha$, let $\varepsilon_n=\alpha n-\lfloor \alpha n \rfloor,$ and $S_n=\frac{1}{n+1}\sum_{i=0}^{n} p^{-\varepsilon_i}$ (where $n=0 , 1, ...$). I suspect that $\lim_{n \to \infty} S_n=\frac{p-1}{p \ln p}$, but have no idea how to prove (or disprove) it. [Edit: Many thanks to Daniel Fischer for explaining this]. I'd like to find some well-behaved bounding functions for the sequence $\{ S_n \}$, i.e., relatively simple continuous functions $f$ and $g$ such that $f(n) \le S_n \le g(n)$: I think $f(x)=\frac{p-1}{p \ln p}$ and $g(x)=f(x)+\frac {1}{x+1}$ will do, but I would like some suggestions for proving/disproving this (or functions that are a tighter fit). Can anyone please help with part 2, or point me towards some basic resources that might shed light on it?","My first time here, so please excuse any breaches of etiquette. For a given  $p \in \mathbb N$  and irrational $\alpha$, let $\varepsilon_n=\alpha n-\lfloor \alpha n \rfloor,$ and $S_n=\frac{1}{n+1}\sum_{i=0}^{n} p^{-\varepsilon_i}$ (where $n=0 , 1, ...$). I suspect that $\lim_{n \to \infty} S_n=\frac{p-1}{p \ln p}$, but have no idea how to prove (or disprove) it. [Edit: Many thanks to Daniel Fischer for explaining this]. I'd like to find some well-behaved bounding functions for the sequence $\{ S_n \}$, i.e., relatively simple continuous functions $f$ and $g$ such that $f(n) \le S_n \le g(n)$: I think $f(x)=\frac{p-1}{p \ln p}$ and $g(x)=f(x)+\frac {1}{x+1}$ will do, but I would like some suggestions for proving/disproving this (or functions that are a tighter fit). Can anyone please help with part 2, or point me towards some basic resources that might shed light on it?",,"['calculus', 'sequences-and-series', 'elementary-number-theory']"
92,Related Rates - possible textbook error,Related Rates - possible textbook error,,"Recently I've been asked to do some exercises from a textbook but I cannot understand how the author derived the correct answer. Here is the exercise in question: At $8$:$00$ boat $A$ is located $25\,km$ south of boat $B$. If boat $A$ is traveling west at $16\,km/h$ and boat $B$ south at $20\,km/h$, at what rate is the distance between the boats changing at $8$:$30$? Textbook answer: $25.6\,km/h$ My answer: $-10.12\,km/h$ I have likely made a mistake somewhere. Could someone help me determine which answer is correct?","Recently I've been asked to do some exercises from a textbook but I cannot understand how the author derived the correct answer. Here is the exercise in question: At $8$:$00$ boat $A$ is located $25\,km$ south of boat $B$. If boat $A$ is traveling west at $16\,km/h$ and boat $B$ south at $20\,km/h$, at what rate is the distance between the boats changing at $8$:$30$? Textbook answer: $25.6\,km/h$ My answer: $-10.12\,km/h$ I have likely made a mistake somewhere. Could someone help me determine which answer is correct?",,"['calculus', 'geometry']"
93,Bound on first derivative $\max \left(\frac{|f'(x)|^2}{f(x)} \right) \le 2 \max |f''(x)|$,Bound on first derivative,\max \left(\frac{|f'(x)|^2}{f(x)} \right) \le 2 \max |f''(x)|,"I want to show that for a function $f \in C_c^2((a,b))$ non-negative, the inequality  $$\sup \left(\frac{|f'(x)|^2}{f(x)} \right) \le 2 \sup |f''(x)|$$ holds. I noticed that the left term is equal to  $2 |\sqrt{f(x)}'|^2.$ So the question is equivalent to: Can I bound this term just by the second derivative? Currently, I don't see how this could work. The problem I am also having with the exercise is that we get problems if $f(x)=0$ cause then we may divide zero by zero on the left-hand side and it is not immediate to me that the limit is finite. If anything is unclear, please let me know.","I want to show that for a function $f \in C_c^2((a,b))$ non-negative, the inequality  $$\sup \left(\frac{|f'(x)|^2}{f(x)} \right) \le 2 \sup |f''(x)|$$ holds. I noticed that the left term is equal to  $2 |\sqrt{f(x)}'|^2.$ So the question is equivalent to: Can I bound this term just by the second derivative? Currently, I don't see how this could work. The problem I am also having with the exercise is that we get problems if $f(x)=0$ cause then we may divide zero by zero on the left-hand side and it is not immediate to me that the limit is finite. If anything is unclear, please let me know.",,"['calculus', 'real-analysis', 'analysis']"
94,How do I evaluate $\int_{1/3}^3 \frac{\arctan x}{x^2 - x + 1} \; dx$?,How do I evaluate ?,\int_{1/3}^3 \frac{\arctan x}{x^2 - x + 1} \; dx,"I need to calculate the following definite integral: $$\int_{1/3}^3 \frac{\arctan x}{x^2 - x + 1} \; dx.$$ The only thing that I've found is: $$\int_{1/3}^3 \frac{\arctan x}{x^2 - x + 1} \; dx = \int_{1/3}^3 \frac{\arctan \frac{1}{x}}{x^2 - x + 1} \; dx,$$ but it doesn't seem useful.",I need to calculate the following definite integral: The only thing that I've found is: but it doesn't seem useful.,"\int_{1/3}^3 \frac{\arctan x}{x^2 - x + 1} \; dx. \int_{1/3}^3 \frac{\arctan x}{x^2 - x + 1} \; dx = \int_{1/3}^3 \frac{\arctan \frac{1}{x}}{x^2 - x + 1} \; dx,",['calculus']
95,Integration of $\sec(x+2) \tan(x+2)/\sqrt{x+2} $,Integration of,\sec(x+2) \tan(x+2)/\sqrt{x+2} ,"What is $\int\frac{\sec(x+2)\tan(x+2)}{\sqrt{x+2}}dx$? I tried the $u=\sqrt{x+2}$ but just get $\int 2\sec(u^2)\tan(u^2)du$, which I am stuck on. Also tried $w=x+2$ which gives a similar problem, $\int\frac{\sec w\tan w}{\sqrt{w}}dw$. So anyone who can help me with this, please do. Thanks in advance","What is $\int\frac{\sec(x+2)\tan(x+2)}{\sqrt{x+2}}dx$? I tried the $u=\sqrt{x+2}$ but just get $\int 2\sec(u^2)\tan(u^2)du$, which I am stuck on. Also tried $w=x+2$ which gives a similar problem, $\int\frac{\sec w\tan w}{\sqrt{w}}dw$. So anyone who can help me with this, please do. Thanks in advance",,['calculus']
96,"How to prove an extremum existence in problems, regarding calculus of variations","How to prove an extremum existence in problems, regarding calculus of variations",,"Let's consider a functional $S(y)=\int_{a}^{b}{f(x, y, y') \cdot dx}$. It's known that if the function that attains minumum or maximum to $y(x)$ does exists, then it can be got from the Euler-Lagrange equation. The problem occurs, when we are not sure enough in which class of the functions we are going to look for the solution. The typical example is the Dido's problem:  we are looking for a function $f: [-1, 1] \longrightarrow \mathbb{R}_{+}$, graph of which encloses the maximum area between $x$ axis and itself under a fixed length. The right answer is $y=\sqrt{1-x^{2}} \in C[-1, 1]$ But we could not recieve this particular result, if we were considering the problem, for example, in class $C^{1}[-1, 1]$ of all continously differentiable functions. The question is: 1) Does the Euler-Lagrange equation requires twice differentiability from the solutions? 2) How to prove that the Dido's problem has a solution in $C[-1, 1]$ using  methods of the calculus of variations or functional analysis? I was told that it would be reasonable to consider the closure of $C^{1}[-1, 1]$, which should be compact in $C[-1, 1]$, but there are some troubles with proving this and it's not clear enough,  which result this fact may lead to. Any help would be much appreciated.","Let's consider a functional $S(y)=\int_{a}^{b}{f(x, y, y') \cdot dx}$. It's known that if the function that attains minumum or maximum to $y(x)$ does exists, then it can be got from the Euler-Lagrange equation. The problem occurs, when we are not sure enough in which class of the functions we are going to look for the solution. The typical example is the Dido's problem:  we are looking for a function $f: [-1, 1] \longrightarrow \mathbb{R}_{+}$, graph of which encloses the maximum area between $x$ axis and itself under a fixed length. The right answer is $y=\sqrt{1-x^{2}} \in C[-1, 1]$ But we could not recieve this particular result, if we were considering the problem, for example, in class $C^{1}[-1, 1]$ of all continously differentiable functions. The question is: 1) Does the Euler-Lagrange equation requires twice differentiability from the solutions? 2) How to prove that the Dido's problem has a solution in $C[-1, 1]$ using  methods of the calculus of variations or functional analysis? I was told that it would be reasonable to consider the closure of $C^{1}[-1, 1]$, which should be compact in $C[-1, 1]$, but there are some troubles with proving this and it's not clear enough,  which result this fact may lead to. Any help would be much appreciated.",,"['calculus', 'real-analysis', 'functional-analysis', 'calculus-of-variations']"
97,Spivak Calculus Chapter II Exercise 22 :: Arithmetic mean and geometric mean,Spivak Calculus Chapter II Exercise 22 :: Arithmetic mean and geometric mean,,"the exercise says: If $a_1,\ldots,a_n\ge0$ then the A-M is $A_n=\frac{a_1+\cdots+a_n}{n}$ and G-M is $G_n=\sqrt[n]{a_1a_2\cdots a_n}$ we would like to show that $G_n\le A_n$ (1) suppose that $a_1<A_n$ then some $a_i>A_n$, for convenience say $a_2>A_n$ Let $\overline{a}_1=A_n$ and $\overline{a}_2=a_1+a_2-\overline{a}_1$ show that $$\overline{a}_1\overline{a}_2\ge a_1a_2$$ I can't understand how he concludes $G_n\le \overline{G}_n$, maybe he says suppose that $a_3<A_n$ then  for convenience say $a_4>A_n$, and so $$\overline{a}_1\overline{a}_2\ge a_1a_2\\ \overline{a}_3\overline{a}_4\ge a_3a_4\\ \vdots \\ \overline{a}_{n-1}\overline{a}_n\ge a_{n-1}a_n$$ and multiply each to get the result but what if $a_3=A_n$? is there a  more rigorous proof? also i can't understand the rest of what he says if anyone can clarify it thanks in advance","the exercise says: If $a_1,\ldots,a_n\ge0$ then the A-M is $A_n=\frac{a_1+\cdots+a_n}{n}$ and G-M is $G_n=\sqrt[n]{a_1a_2\cdots a_n}$ we would like to show that $G_n\le A_n$ (1) suppose that $a_1<A_n$ then some $a_i>A_n$, for convenience say $a_2>A_n$ Let $\overline{a}_1=A_n$ and $\overline{a}_2=a_1+a_2-\overline{a}_1$ show that $$\overline{a}_1\overline{a}_2\ge a_1a_2$$ I can't understand how he concludes $G_n\le \overline{G}_n$, maybe he says suppose that $a_3<A_n$ then  for convenience say $a_4>A_n$, and so $$\overline{a}_1\overline{a}_2\ge a_1a_2\\ \overline{a}_3\overline{a}_4\ge a_3a_4\\ \vdots \\ \overline{a}_{n-1}\overline{a}_n\ge a_{n-1}a_n$$ and multiply each to get the result but what if $a_3=A_n$? is there a  more rigorous proof? also i can't understand the rest of what he says if anyone can clarify it thanks in advance",,"['calculus', 'inequality']"
98,Does such a series exist?,Does such a series exist?,,"This question came up as some puzzle. Does there exist a sequence of real numbers ${c_j}$ such that $\sum{c_j^m} = m$ for all positive integers $m$? I argue no.  Suppose there exists such a sequence.  Then, we must have $\sum{c_j}^2 = 2$.  Thus, we need $|c_j| \leq \sqrt{2}$ for all $j$.  Similarly, we would need $\sum c_j^4 = 4$, and so $|c_j| \leq (4)^{\frac{1}{4}}$, for all $j$.  Continuing this procedure for all even $m$, we get that $|c_j| \leq 1$ for every $j$ from the limit. But now, if that is the case, we have that $c_j^2 \geq c_j^4$ for every $j$, and so $2 = \sum c_j^2 \geq \sum c_j^4 = 4$, a contradiction.  Thus, no such sequence can exist. Does this seem correct?  It seems strange that I only care about the even integers.  If correct, can you think of any other ways to solve this?","This question came up as some puzzle. Does there exist a sequence of real numbers ${c_j}$ such that $\sum{c_j^m} = m$ for all positive integers $m$? I argue no.  Suppose there exists such a sequence.  Then, we must have $\sum{c_j}^2 = 2$.  Thus, we need $|c_j| \leq \sqrt{2}$ for all $j$.  Similarly, we would need $\sum c_j^4 = 4$, and so $|c_j| \leq (4)^{\frac{1}{4}}$, for all $j$.  Continuing this procedure for all even $m$, we get that $|c_j| \leq 1$ for every $j$ from the limit. But now, if that is the case, we have that $c_j^2 \geq c_j^4$ for every $j$, and so $2 = \sum c_j^2 \geq \sum c_j^4 = 4$, a contradiction.  Thus, no such sequence can exist. Does this seem correct?  It seems strange that I only care about the even integers.  If correct, can you think of any other ways to solve this?",,"['calculus', 'sequences-and-series']"
99,How to solve this equation in $ \mathbb{C} $?,How to solve this equation in ?, \mathbb{C} ,"From a small simple calculation , we get the following formulas: $ \begin{cases} e^x = \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} \Big) + \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} \Big) + \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} \Big) = A_0 (x) + A_1 (x) + A_2 (x) \\ e^{jx} = \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} \Big) + j \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} \Big) + j^2 \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} \Big) = A_{0} (x) + j A_{1} (x) + j^2 A_2 (x)  \\ e^{j^{2} x} = \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} \Big) + j^2 \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} \Big) + j \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} \Big) = A_{0} (x) + j^2 A_{1} (x) + j A_2 (x) \end{cases} $ with : $ j = e^{ i \dfrac{2 \pi}{3} } $. That means : $ \begin{cases} A_0 (x) = \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} = \frac{1}{3} ( e^{x} + e^{jx} + e^{j^{2} x } ) \\ A_1 (x) = \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} = \frac{1}{3} ( e^{x} + j^2 e^{jx} + j e^{j^{2} x } ) \\ A_2 (x) = \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} = \frac{1}{3} ( e^{x} + j e^{jx} + j^2 e^{j^{2} x } ) \end{cases} $ My question is the following: Could you tell me a method to find $ a $ and $ b $ in $ \mathbb {C} $ according to $ x $ such that : $ e^{ jx } = ( A_0 (a) + j A_1 ( a) ) ( A_0 (b) + j A_1 ( b) ) $ Thank you very much for your help.","From a small simple calculation , we get the following formulas: $ \begin{cases} e^x = \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} \Big) + \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} \Big) + \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} \Big) = A_0 (x) + A_1 (x) + A_2 (x) \\ e^{jx} = \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} \Big) + j \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} \Big) + j^2 \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} \Big) = A_{0} (x) + j A_{1} (x) + j^2 A_2 (x)  \\ e^{j^{2} x} = \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} \Big) + j^2 \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} \Big) + j \Big( \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} \Big) = A_{0} (x) + j^2 A_{1} (x) + j A_2 (x) \end{cases} $ with : $ j = e^{ i \dfrac{2 \pi}{3} } $. That means : $ \begin{cases} A_0 (x) = \displaystyle \sum_{n \geq 0} \frac{x^{3n}}{(3n)!} = \frac{1}{3} ( e^{x} + e^{jx} + e^{j^{2} x } ) \\ A_1 (x) = \displaystyle \sum_{n \geq 0} \frac{x^{3n+1}}{(3n+1)!} = \frac{1}{3} ( e^{x} + j^2 e^{jx} + j e^{j^{2} x } ) \\ A_2 (x) = \displaystyle \sum_{n \geq 0} \frac{x^{3n+2}}{(3n+2)!} = \frac{1}{3} ( e^{x} + j e^{jx} + j^2 e^{j^{2} x } ) \end{cases} $ My question is the following: Could you tell me a method to find $ a $ and $ b $ in $ \mathbb {C} $ according to $ x $ such that : $ e^{ jx } = ( A_0 (a) + j A_1 ( a) ) ( A_0 (b) + j A_1 ( b) ) $ Thank you very much for your help.",,"['calculus', 'trigonometry', 'complex-numbers', 'power-series']"
