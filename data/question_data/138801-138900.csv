,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Matricial differential equation and eigenvalues,Matricial differential equation and eigenvalues,,"I have the matricial differential equation: $$\frac{\text{d}}{\text{d}t}\vec{x}(t)=A \cdot \vec{x}(t)$$ where: $$\vec{x}(t) \in \mathbb{R}^3, \quad A=\begin{pmatrix} a & 0 & b \newline 0 & c & 0 \newline d & 0 & a \end{pmatrix}, \quad a,b,c,d \in \mathbb{R}$$ I need to find $a,b,c,d \in \mathbb{R}$ such that: $$\lim_{t \to +\infty} \vec{x}(t)=\vec{0}$$ for every initial condition $\vec{x}(0) \in \mathbb{R}^3$ . The very short solution provided by the text is that the eigenvalues of the matrix $A$ must have negative real part. I can't undetstand why. What I have done so far: I know that the solution can be written in the form: $$\vec{x}(t)=e^{tA} \cdot \vec{x}(0)$$ and I can easily find the eigenvalues of $A$ : $$c, \quad a-\sqrt{bd}, \quad a+\sqrt{bd}$$ If the real part must be negative then we must have: $c<0$ ; $a<0 \quad \text{if} \quad bd<0$ ; $a<-\sqrt{bd} \quad \text{if} \quad bd>0$ .",I have the matricial differential equation: where: I need to find such that: for every initial condition . The very short solution provided by the text is that the eigenvalues of the matrix must have negative real part. I can't undetstand why. What I have done so far: I know that the solution can be written in the form: and I can easily find the eigenvalues of : If the real part must be negative then we must have: ; ; .,"\frac{\text{d}}{\text{d}t}\vec{x}(t)=A \cdot \vec{x}(t) \vec{x}(t) \in \mathbb{R}^3, \quad A=\begin{pmatrix} a & 0 & b \newline 0 & c & 0 \newline d & 0 & a \end{pmatrix}, \quad a,b,c,d \in \mathbb{R} a,b,c,d \in \mathbb{R} \lim_{t \to +\infty} \vec{x}(t)=\vec{0} \vec{x}(0) \in \mathbb{R}^3 A \vec{x}(t)=e^{tA} \cdot \vec{x}(0) A c, \quad a-\sqrt{bd}, \quad a+\sqrt{bd} c<0 a<0 \quad \text{if} \quad bd<0 a<-\sqrt{bd} \quad \text{if} \quad bd>0","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
1,I need an example of a function such that the derivative minus x is the same as the original function [closed],I need an example of a function such that the derivative minus x is the same as the original function [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question I'm curious if there is a function F(x) whose derivative f(x) is the same as F(x) + x. I tried to make one myself by adding things to e^x with no success. the function was mentioned in Question 21 of the academic assessment conducted in Korea in March 2021. It didn't mention the specific function, and the question was solvable without it. But I'm curious what that function would be. The test was for high schoolers, so I think the function will not wander outside the context of Korean High School Math curriculum.(The only Transcendental function taught in Korea's high school math curriculum is logarithmic function, trigonometric functions, and exponential function)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question I'm curious if there is a function F(x) whose derivative f(x) is the same as F(x) + x. I tried to make one myself by adding things to e^x with no success. the function was mentioned in Question 21 of the academic assessment conducted in Korea in March 2021. It didn't mention the specific function, and the question was solvable without it. But I'm curious what that function would be. The test was for high schoolers, so I think the function will not wander outside the context of Korean High School Math curriculum.(The only Transcendental function taught in Korea's high school math curriculum is logarithmic function, trigonometric functions, and exponential function)",,"['calculus', 'integration', 'ordinary-differential-equations']"
2,"How to find a continuous and positive $f$, such that the ODE $x''(t)=f(x(t))$ doesn't have unique solution given some initial value?","How to find a continuous and positive , such that the ODE  doesn't have unique solution given some initial value?",f x''(t)=f(x(t)),"As the title . I want to find a continuous $f$ , such that the ODE $x''(t)=f(x(t))$ has more than one solutions when the initial value $x(0), x'(0)$ are given. I hope $f(x)\neq0$ for every $x\in\mathbb{R}$ . So assume $f$ is always positive. Is it possible to find one?","As the title . I want to find a continuous , such that the ODE has more than one solutions when the initial value are given. I hope for every . So assume is always positive. Is it possible to find one?","f x''(t)=f(x(t)) x(0), x'(0) f(x)\neq0 x\in\mathbb{R} f",['ordinary-differential-equations']
3,Asymptotic solution of ODE,Asymptotic solution of ODE,,"I have the following system of ODEs $$ -4 (u a(u)+1) f'(u)=2 f(u) \left(u a'(u)+a(u)\right)+\left(u \left(3 a(u)^2+\frac{1}{4}\right) a'(u)+\left(a(u)^3+\frac{a(u)}{4}\right)\right),$$ $$a'(u)=\frac{1}{2} u f'(u),$$ $$a(0)=-1/2, f(0)=0.$$ I need to find the behaviour of solutions at $u\rightarrow\infty$ . Numerical experiments indicate that $f(u)\approx 3/8$ and $a(u)\approx -1/u$ . But how can it be rigorously shown? I would be even happy to be able to show that $\lim_{u\rightarrow\infty}a(u)=0.$ How to achieve that? These equations have some relevance for Hubbard model of strongly correlated system (2-sites case). $-1/2\le a(u)\le 1/2$ implies that some approximate method fulfils the Pauli exclusion principle.",I have the following system of ODEs I need to find the behaviour of solutions at . Numerical experiments indicate that and . But how can it be rigorously shown? I would be even happy to be able to show that How to achieve that? These equations have some relevance for Hubbard model of strongly correlated system (2-sites case). implies that some approximate method fulfils the Pauli exclusion principle.,"
-4 (u a(u)+1) f'(u)=2 f(u) \left(u a'(u)+a(u)\right)+\left(u \left(3 a(u)^2+\frac{1}{4}\right) a'(u)+\left(a(u)^3+\frac{a(u)}{4}\right)\right), a'(u)=\frac{1}{2} u f'(u), a(0)=-1/2,
f(0)=0. u\rightarrow\infty f(u)\approx 3/8 a(u)\approx -1/u \lim_{u\rightarrow\infty}a(u)=0. -1/2\le a(u)\le 1/2","['ordinary-differential-equations', 'asymptotics']"
4,What are the rules for solving differential inequalities using Laplace Transforms?,What are the rules for solving differential inequalities using Laplace Transforms?,,"I am looking to solve a non-autonomous differential inequality of the form $$\frac{dV}{d\tau}-\epsilon V-\frac{E^2}{2\epsilon}\leq\frac{A^2}{2\epsilon}\tau^2+\frac{2AE\tau}{\epsilon},$$ where $V:\mathbb{R}_{\geq0}\to\mathbb{R}_{\geq0}$ , and $\epsilon$ , $E$ , and $A$ are positive (non zero) scalar constants. When I take the Laplace transform, and after some simple algebraic manipulation I obtain $$(s-\epsilon)\mathcal{V}(s)\leq\frac{A^2}{\epsilon s^3}+\frac{2AE}{\epsilon s^2}+\frac{E^2}{2\epsilon s}+V(0).$$ Now, the question is about how to properly dived both sides by $(s-\epsilon )$ since this, as far as I can tell, is sign indefinite. Now, if I assume that that there is no sign change when I do this operation, followed by partial fraction decomposition and then taking the inverse Laplace transform, I obtain $$V(\tau)\leq \left(\frac{A^{2}}{\epsilon^{4}}+\frac{E^{2}}{2\epsilon^{2}}+V(0)+\frac{2AE}{\epsilon^{3}}\right)\text{e}^{\epsilon\tau}-\left(\frac{A^{2}}{\epsilon^{3}}+\frac{2AE}{\epsilon^{2}}\right)\tau-\frac{E^{2}}{4\epsilon^{2}}\tau^{2}-\frac{E^{2}}{2\epsilon^{2}}-\frac{A^{2}}{\epsilon^{4}}-\frac{2AE}{\epsilon^{3}}.$$ This seems fine on initial inspection, since $V(\tau)=V(0)$ when $\tau=0$ , however, things get odd when you change the value of $\epsilon$ . For example, let $A=0.1$ , $E=0.2$ , $V(0)=0$ , then when $\epsilon =0.1$ the value of $V(\tau)<0$ for $\tau\in(1,9)$ (rough bounds). This is a problem since $V$ is a positive definite function (time derivative is positive definite and initial condition here is zero). There is clearly an issue here, and my suspicion is in what I am doing while I am working in the Laplace domain. Now, I see that I can divide equation two by $(s-\epsilon)$ and take it in piecwise form such that $$\mathcal{V}_{u}(s)\leq\begin{cases} \frac{1}{s-\epsilon}\left(\frac{A^{2}}{\epsilon}\frac{1}{s^{3}}+\frac{2AE}{\epsilon}\frac{1}{s^{2}}+\frac{E^{2}}{\epsilon2}\frac{1}{s}+V_{u}(t_{0})\right), & s-\epsilon\geq0\\ -\frac{1}{s-\epsilon}\left(\frac{A^{2}}{\epsilon}\frac{1}{s^{3}}+\frac{2AE}{\epsilon}\frac{1}{s^{2}}+\frac{E^{2}}{\epsilon2}\frac{1}{s}+V_{u}(t_{0})\right), & s-\epsilon<0 \end{cases},$$ but how would one take the inverse Laplace transform of this while taking the conditions of the piecewise function into account? Would I do the following $$\mathcal{L}^{-1}\{s-\epsilon\}$$ and start to look at the time derivatives of the Dirac-delta function? That seems very bizarre to me, so any help on this issue would be greatly appreciated. perhaps I shouldn't use Laplace transforms and I should solve this differential inequality by a different method? Suggestions are welcome! Thanks, Sage","I am looking to solve a non-autonomous differential inequality of the form where , and , , and are positive (non zero) scalar constants. When I take the Laplace transform, and after some simple algebraic manipulation I obtain Now, the question is about how to properly dived both sides by since this, as far as I can tell, is sign indefinite. Now, if I assume that that there is no sign change when I do this operation, followed by partial fraction decomposition and then taking the inverse Laplace transform, I obtain This seems fine on initial inspection, since when , however, things get odd when you change the value of . For example, let , , , then when the value of for (rough bounds). This is a problem since is a positive definite function (time derivative is positive definite and initial condition here is zero). There is clearly an issue here, and my suspicion is in what I am doing while I am working in the Laplace domain. Now, I see that I can divide equation two by and take it in piecwise form such that but how would one take the inverse Laplace transform of this while taking the conditions of the piecewise function into account? Would I do the following and start to look at the time derivatives of the Dirac-delta function? That seems very bizarre to me, so any help on this issue would be greatly appreciated. perhaps I shouldn't use Laplace transforms and I should solve this differential inequality by a different method? Suggestions are welcome! Thanks, Sage","\frac{dV}{d\tau}-\epsilon V-\frac{E^2}{2\epsilon}\leq\frac{A^2}{2\epsilon}\tau^2+\frac{2AE\tau}{\epsilon}, V:\mathbb{R}_{\geq0}\to\mathbb{R}_{\geq0} \epsilon E A (s-\epsilon)\mathcal{V}(s)\leq\frac{A^2}{\epsilon s^3}+\frac{2AE}{\epsilon s^2}+\frac{E^2}{2\epsilon s}+V(0). (s-\epsilon ) V(\tau)\leq \left(\frac{A^{2}}{\epsilon^{4}}+\frac{E^{2}}{2\epsilon^{2}}+V(0)+\frac{2AE}{\epsilon^{3}}\right)\text{e}^{\epsilon\tau}-\left(\frac{A^{2}}{\epsilon^{3}}+\frac{2AE}{\epsilon^{2}}\right)\tau-\frac{E^{2}}{4\epsilon^{2}}\tau^{2}-\frac{E^{2}}{2\epsilon^{2}}-\frac{A^{2}}{\epsilon^{4}}-\frac{2AE}{\epsilon^{3}}. V(\tau)=V(0) \tau=0 \epsilon A=0.1 E=0.2 V(0)=0 \epsilon =0.1 V(\tau)<0 \tau\in(1,9) V (s-\epsilon) \mathcal{V}_{u}(s)\leq\begin{cases}
\frac{1}{s-\epsilon}\left(\frac{A^{2}}{\epsilon}\frac{1}{s^{3}}+\frac{2AE}{\epsilon}\frac{1}{s^{2}}+\frac{E^{2}}{\epsilon2}\frac{1}{s}+V_{u}(t_{0})\right), & s-\epsilon\geq0\\
-\frac{1}{s-\epsilon}\left(\frac{A^{2}}{\epsilon}\frac{1}{s^{3}}+\frac{2AE}{\epsilon}\frac{1}{s^{2}}+\frac{E^{2}}{\epsilon2}\frac{1}{s}+V_{u}(t_{0})\right), & s-\epsilon<0
\end{cases}, \mathcal{L}^{-1}\{s-\epsilon\}","['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'laplace-transform']"
5,Show one solution of ODE system is bounded above and below,Show one solution of ODE system is bounded above and below,,"Defined on $\mathbb{R}$ , there're three $C^1$ functions $u_{1}(x),u_{2}(x),u_{3}(x)$ . Given the ODE system \begin{cases} u_{1}'(x)&=u_{2}(x)u_{3}(x) \\ u_{2}'(x)&=-u_{1}(x)u_{3}(x) \\ u_{3}'(x)&=-k^2u_{1}(x)u_{2}(x) \\ \end{cases} Here $0<k<1$ . The initial condition is $u_{1}(0)=0,u_{2}(0)=u_{3}(0)=1$ . Show that $\sqrt{1-k^2}\leq u_{3}(x)\leq1$ . I have known $u_{1}^2(x)+u_{2}^2(x)=1$ by combining the first two equations. However, I haven't found how to continue the boundeness of $u_{3}$ . By differentiating $u_{3}$ I got $u_{3}''=k^2(u_{1}^2-u_{2}^2)u_{3}$ but it seems no use. By $u_{1}^2+u_{2}^2=1$ we may know $1\geq 2u_{1}u_{2}$ .","Defined on , there're three functions . Given the ODE system Here . The initial condition is . Show that . I have known by combining the first two equations. However, I haven't found how to continue the boundeness of . By differentiating I got but it seems no use. By we may know .","\mathbb{R} C^1 u_{1}(x),u_{2}(x),u_{3}(x) \begin{cases}
u_{1}'(x)&=u_{2}(x)u_{3}(x) \\
u_{2}'(x)&=-u_{1}(x)u_{3}(x) \\
u_{3}'(x)&=-k^2u_{1}(x)u_{2}(x) \\
\end{cases} 0<k<1 u_{1}(0)=0,u_{2}(0)=u_{3}(0)=1 \sqrt{1-k^2}\leq u_{3}(x)\leq1 u_{1}^2(x)+u_{2}^2(x)=1 u_{3} u_{3} u_{3}''=k^2(u_{1}^2-u_{2}^2)u_{3} u_{1}^2+u_{2}^2=1 1\geq 2u_{1}u_{2}",['ordinary-differential-equations']
6,Use power series to solve the differential equation,Use power series to solve the differential equation,,"Consider the differential equation \begin{align} t\frac{dy}{dt} - t^2y + 5 &= 0 \\ y(0) &= 0 \end{align} I want to solve this using power series, so consider $$y(t)=a_{0}+a_{1}t+a_{2}t^2+\cdots=\sum_{n=0}^{\infty}a_{n}t^{n}$$ and $$\frac{dy}{dt}=a_{1}+2a_{2}t+3a_{3}t^2+\cdots=\sum_{n=1}^{\infty}na_{n}t^{n-1}$$ Note that \begin{align*}     0&=t\frac{dy}{dt}-t^2y+5 \\     &=t\left(\sum_{n=1}^{\infty}na_{n}t^{n-1} \right)-t^2\left(\sum_{n=0}^{\infty}a_{n}t^{n} \right)+5 \hspace{0.4cm} (\ast) \end{align*} The idea here would be to arrive at an expression of the form $\sum_{k=0}^{\infty} z k^n=0$ and then $k=0$ where can I establish a recurrence relation knowing that $a_{0}=y(0)=1$ but my problem occurs when simplifying $(\ast)$ , any suggestions on how to solve this? I appreciate it!","Consider the differential equation I want to solve this using power series, so consider and Note that The idea here would be to arrive at an expression of the form and then where can I establish a recurrence relation knowing that but my problem occurs when simplifying , any suggestions on how to solve this? I appreciate it!","\begin{align}
t\frac{dy}{dt} - t^2y + 5 &= 0 \\
y(0) &= 0
\end{align} y(t)=a_{0}+a_{1}t+a_{2}t^2+\cdots=\sum_{n=0}^{\infty}a_{n}t^{n} \frac{dy}{dt}=a_{1}+2a_{2}t+3a_{3}t^2+\cdots=\sum_{n=1}^{\infty}na_{n}t^{n-1} \begin{align*}
    0&=t\frac{dy}{dt}-t^2y+5 \\
    &=t\left(\sum_{n=1}^{\infty}na_{n}t^{n-1} \right)-t^2\left(\sum_{n=0}^{\infty}a_{n}t^{n} \right)+5 \hspace{0.4cm} (\ast)
\end{align*} \sum_{k=0}^{\infty} z k^n=0 k=0 a_{0}=y(0)=1 (\ast)","['real-analysis', 'ordinary-differential-equations']"
7,How should I find the nontrivial solution to this differential equation using the method of undetermined coefficients?,How should I find the nontrivial solution to this differential equation using the method of undetermined coefficients?,,"Find the nontrivial solution of this differential equation $x^2\frac{d^2y}{dx^2}+3x\frac{dy}{dx}+5y=8x, y(1)=2, y(exp(\pi/4))=2sinh(\pi/4).$ Here's my work: Consider the differential equation $x^2\frac{d^2y}{dx^2}+3x\frac{dy}{dx}+5y=8x, y(1)=2, y(exp(\pi/4))=2sinh(\pi/4).$ The corresponding homogeneous equation is $x^2\frac{d^2y}{dx^2}+3x\frac{dy}{dx}+5y=0.$ Its characteristic equation is $r(r-1)+3r+5=0,$ so $r=-1\pm 2i.$ Now we have $y_{h}(x)=e^{-x}(c_{1}cos(2x)+c_{2}sin(2x)).$ Note that $y_{p}=Ax+B, y_{p}'=A, y_{p}''=0.$ Observe that $x^{2}y_{p}''+3xy_{p}'+5y_{p}=8x$ $x^2(0)+3x(A)+5(Ax+B)=8x$ $3Ax+5Ax+5B=8x$ $8Ax+5B=8x.$ Since $A=1, B=0,$ it follows that $y_{p}=x.$ Thus $y(x)=y_{h}(x)+y_{p}=e^{-x}(c_{1}cos(2x)+c_{2}sin(2x))+x.$ By our boundary conditions, we have that $y(1)=2\implies 2=e^{-1}(c_{1}cos(2)+c_{2}sin(2))+1,$ so $c_{1}cos(2)+c_{2}sin(2)=e.$ Then, $y(e^{\pi/4})=2sinh(\pi/4)\implies 2sinh(\pi/4)=e^{-e^{\pi/4}}(c_{1}cos(2e^{\pi/4})+c_{2}sin(2e^{\pi/4}))+e^{\pi/4}.$ Now I'm stuck. How should I proceed from here and find $c_{1}, c_{2}?$ What's the correct answer?","Find the nontrivial solution of this differential equation Here's my work: Consider the differential equation The corresponding homogeneous equation is Its characteristic equation is so Now we have Note that Observe that Since it follows that Thus By our boundary conditions, we have that so Then, Now I'm stuck. How should I proceed from here and find What's the correct answer?","x^2\frac{d^2y}{dx^2}+3x\frac{dy}{dx}+5y=8x, y(1)=2, y(exp(\pi/4))=2sinh(\pi/4). x^2\frac{d^2y}{dx^2}+3x\frac{dy}{dx}+5y=8x, y(1)=2, y(exp(\pi/4))=2sinh(\pi/4). x^2\frac{d^2y}{dx^2}+3x\frac{dy}{dx}+5y=0. r(r-1)+3r+5=0, r=-1\pm 2i. y_{h}(x)=e^{-x}(c_{1}cos(2x)+c_{2}sin(2x)). y_{p}=Ax+B, y_{p}'=A, y_{p}''=0. x^{2}y_{p}''+3xy_{p}'+5y_{p}=8x x^2(0)+3x(A)+5(Ax+B)=8x 3Ax+5Ax+5B=8x 8Ax+5B=8x. A=1, B=0, y_{p}=x. y(x)=y_{h}(x)+y_{p}=e^{-x}(c_{1}cos(2x)+c_{2}sin(2x))+x. y(1)=2\implies 2=e^{-1}(c_{1}cos(2)+c_{2}sin(2))+1, c_{1}cos(2)+c_{2}sin(2)=e. y(e^{\pi/4})=2sinh(\pi/4)\implies 2sinh(\pi/4)=e^{-e^{\pi/4}}(c_{1}cos(2e^{\pi/4})+c_{2}sin(2e^{\pi/4}))+e^{\pi/4}. c_{1}, c_{2}?","['ordinary-differential-equations', 'solution-verification']"
8,Finding solution of non-homogeneous ODE when we know the solutions of the homogeneous counterpart,Finding solution of non-homogeneous ODE when we know the solutions of the homogeneous counterpart,,"Given ODE is: $$y''+Py'+Qy=0$$ It has two solutions $f(x)$ and $xf(x)$ . I need to find the solution to the following non-homogeneous ODE: $$y''+Py'+Qy=f(x)$$ where $f(x)$ on the RHS is the solution of the first ODE. I need to find its solution. I tried to apply variation of parameters method, but failed to proceed and also found this question similar. But being a beginner who's self-learning ODE, I am stuck. An elaboration of its solution would really help me to understand the working of such questions. EDIT I reconsidered the variation of parameter method and seemed to find a legit solution! $$y=f(x)\left(\int \frac{-xf(x).f(x) dx}{f(x)^2} +c_1\right) + xf(x)\left( \int \frac{f(x). f(x) dx}{f(x)^2} +c_2\right)$$ P.S. The denominator is the wronskian  of $f(x)$ and $xf(x)$ which gives $f(x)^2$ On solving, it gives: $$y=f(x) \cdot \left(\frac {x^2}{2}+c_2 x+ c_1\right)$$ Thanks for the valuable comment!","Given ODE is: It has two solutions and . I need to find the solution to the following non-homogeneous ODE: where on the RHS is the solution of the first ODE. I need to find its solution. I tried to apply variation of parameters method, but failed to proceed and also found this question similar. But being a beginner who's self-learning ODE, I am stuck. An elaboration of its solution would really help me to understand the working of such questions. EDIT I reconsidered the variation of parameter method and seemed to find a legit solution! P.S. The denominator is the wronskian  of and which gives On solving, it gives: Thanks for the valuable comment!",y''+Py'+Qy=0 f(x) xf(x) y''+Py'+Qy=f(x) f(x) y=f(x)\left(\int \frac{-xf(x).f(x) dx}{f(x)^2} +c_1\right) + xf(x)\left( \int \frac{f(x). f(x) dx}{f(x)^2} +c_2\right) f(x) xf(x) f(x)^2 y=f(x) \cdot \left(\frac {x^2}{2}+c_2 x+ c_1\right),"['ordinary-differential-equations', 'fundamental-solution']"
9,Solution of ODE with discontinuous coefficients,Solution of ODE with discontinuous coefficients,,"Can anyone please point to any references regarding solutions of ODE of the form $$(a+1_{x=0})f'(x)-f(x)=h(x)-\int_0^\infty h(t)e^{-t}dt \tag{1}$$ with $f(0)=0$ , where $h$ is a given function, a is some constant and $1_{x=0}$ is the indicator function. I only need the solution for $x\geq 0$ .  I see that when the coefficient is some continuous function of $x$ , say $b(x)$ , that is $$b(x)f'(x)-f(x)=h(x)-\int_0^\infty h(t)e^{-t}dt$$ the solution is given by $$-e^{-\int_0^x\frac{1}{b(u)}du}\int_{x}^\infty\frac{1}{b(y)}\left(h(y)-\int_0^\infty h(t)e^{-t}dt\right)e^{\int_0^y\frac{1}{b(u)}du}dy$$ I am not sure how to proceed when the coefficient is not continuous.","Can anyone please point to any references regarding solutions of ODE of the form with , where is a given function, a is some constant and is the indicator function. I only need the solution for .  I see that when the coefficient is some continuous function of , say , that is the solution is given by I am not sure how to proceed when the coefficient is not continuous.",(a+1_{x=0})f'(x)-f(x)=h(x)-\int_0^\infty h(t)e^{-t}dt \tag{1} f(0)=0 h 1_{x=0} x\geq 0 x b(x) b(x)f'(x)-f(x)=h(x)-\int_0^\infty h(t)e^{-t}dt -e^{-\int_0^x\frac{1}{b(u)}du}\int_{x}^\infty\frac{1}{b(y)}\left(h(y)-\int_0^\infty h(t)e^{-t}dt\right)e^{\int_0^y\frac{1}{b(u)}du}dy,"['real-analysis', 'calculus', 'ordinary-differential-equations']"
10,Is it possible to solve this first order differential equation analytically?,Is it possible to solve this first order differential equation analytically?,,"I've been trying to solve the equations for the path of a projectile moving under constant gravity and affected by air resistance and I come up against this nasty FODE. Is there a means by which it could be solved analytically for $y(x)$ or $x(y)$ ? $$\frac{\mathrm dy}{\mathrm dx}\sqrt{1+\left(\frac{\mathrm dy}{\mathrm dx}\right)^2} + \operatorname{arsinh}\left(\frac{\mathrm dy}{\mathrm dx}\right)=K-\frac{a}{x^{2}},$$ where $K$ and $a$ are real, positive constants.","I've been trying to solve the equations for the path of a projectile moving under constant gravity and affected by air resistance and I come up against this nasty FODE. Is there a means by which it could be solved analytically for or ? where and are real, positive constants.","y(x) x(y) \frac{\mathrm dy}{\mathrm dx}\sqrt{1+\left(\frac{\mathrm dy}{\mathrm dx}\right)^2} + \operatorname{arsinh}\left(\frac{\mathrm dy}{\mathrm dx}\right)=K-\frac{a}{x^{2}}, K a","['calculus', 'ordinary-differential-equations']"
11,"Solutions behavior of i.v.p $y'(x)+e^x(2+\sin x)y(x)=\frac{1}{x^2+1},\ y(0)=a,\ x\geq 0 $",Solutions behavior of i.v.p,"y'(x)+e^x(2+\sin x)y(x)=\frac{1}{x^2+1},\ y(0)=a,\ x\geq 0 ","Consider the initial value problem (i.v.p) $$y'(x)+e^x(2+\sin x)y(x)=\frac{1}{x^2+1},\ y(0)=a,\ x\geq 0\ \ (a\in\mathbb{R}). $$ The unique solution of this problem has the form $$y(x)=e^{-\int_0^x e^s(2+\sin s)ds}\left(a+\int_0^x \frac {1}{1+t^2}e^{\int_0^t e^s(2+\sin s)ds}dt\right),\ x\geq 0 $$ Firstly, I proved that for every $a\in\mathbb{R}$ it holds that $\displaystyle\lim_{x\to+\infty} y(x)=0$ (note that $e^x(2+\sin x)\geq 1,\forall\ x\geq 0$ and $\frac{1}{x^2+1}\to 0$ , as $x\to +\infty$ ). However, I have a problem with those two questions: $\bullet$ Can I tell with certainty, that there exist a solution $y$ of the i.v.p such that $y(x)<0,\forall x\geq x_0$ (for some $x_0\geq 0$ )? $\bullet$ Can I tell with certainty, that there exist an oscillative solution $y$ (i.e solution that has infinite countable roots) of the i.v.p? Is it possible the answer lies hidden at the asymptotic behavior of those integrals? Hint: Calculation of integrals isn't necessary. Thanks for your help!! I appreciate it","Consider the initial value problem (i.v.p) The unique solution of this problem has the form Firstly, I proved that for every it holds that (note that and , as ). However, I have a problem with those two questions: Can I tell with certainty, that there exist a solution of the i.v.p such that (for some )? Can I tell with certainty, that there exist an oscillative solution (i.e solution that has infinite countable roots) of the i.v.p? Is it possible the answer lies hidden at the asymptotic behavior of those integrals? Hint: Calculation of integrals isn't necessary. Thanks for your help!! I appreciate it","y'(x)+e^x(2+\sin x)y(x)=\frac{1}{x^2+1},\ y(0)=a,\ x\geq 0\ \ (a\in\mathbb{R}).  y(x)=e^{-\int_0^x e^s(2+\sin s)ds}\left(a+\int_0^x \frac {1}{1+t^2}e^{\int_0^t e^s(2+\sin s)ds}dt\right),\ x\geq 0  a\in\mathbb{R} \displaystyle\lim_{x\to+\infty} y(x)=0 e^x(2+\sin x)\geq 1,\forall\ x\geq 0 \frac{1}{x^2+1}\to 0 x\to +\infty \bullet y y(x)<0,\forall x\geq x_0 x_0\geq 0 \bullet y","['calculus', 'ordinary-differential-equations']"
12,Solve a coupled ODE system $x'(t)$ and $y'(t)$ including time $t$,Solve a coupled ODE system  and  including time,x'(t) y'(t) t,"I met this ODE system of functions $x(t), y(t)$ , with initial values $x(0)=\frac{\sqrt{2}}{2}, y(0)=\frac{\sqrt{2}}{2}$ . \begin{equation} \frac{\mathrm{d}x}{\mathrm{d}t}=-y+x(x^2+y^2)\sin\frac{\pi}{\sqrt{x^2+y^2}} \end{equation} \begin{equation} \frac{\mathrm{d}y}{\mathrm{d}t}=x+y(x^2+y^2)\sin\frac{\pi}{\sqrt{x^2+y^2}} \end{equation} I tried to eliminate the second term including function $\sin$ \begin{equation} \frac{\frac{\mathrm{d}x}{\mathrm{d}t}+y}{\frac{\mathrm{d}y}{\mathrm{d}t}-x}=\frac{x}{y} \end{equation} After substituting $\frac{x}{y}$ with $u$ , the above equation becomes \begin{equation} \mathrm{d}u=-(1+u^2)\Rightarrow \arctan u=-u+C \end{equation} Use initial values, $\arctan 1=-1+C$ meaning $C=\frac{\pi}{4}+1$ . But it seems more difficult when I tried to substitute $u$ back to the ODE system. Can we have a better solution?","I met this ODE system of functions , with initial values . I tried to eliminate the second term including function After substituting with , the above equation becomes Use initial values, meaning . But it seems more difficult when I tried to substitute back to the ODE system. Can we have a better solution?","x(t), y(t) x(0)=\frac{\sqrt{2}}{2}, y(0)=\frac{\sqrt{2}}{2} \begin{equation}
\frac{\mathrm{d}x}{\mathrm{d}t}=-y+x(x^2+y^2)\sin\frac{\pi}{\sqrt{x^2+y^2}}
\end{equation} \begin{equation}
\frac{\mathrm{d}y}{\mathrm{d}t}=x+y(x^2+y^2)\sin\frac{\pi}{\sqrt{x^2+y^2}}
\end{equation} \sin \begin{equation}
\frac{\frac{\mathrm{d}x}{\mathrm{d}t}+y}{\frac{\mathrm{d}y}{\mathrm{d}t}-x}=\frac{x}{y}
\end{equation} \frac{x}{y} u \begin{equation}
\mathrm{d}u=-(1+u^2)\Rightarrow \arctan u=-u+C
\end{equation} \arctan 1=-1+C C=\frac{\pi}{4}+1 u",['ordinary-differential-equations']
13,Can I use Frobenius method when there is no regular singular point under the condition?,Can I use Frobenius method when there is no regular singular point under the condition?,,"Problem is solve the differential equation $$2x^2y''-xy'+(1-x)y=0,~x>0$$ and I've only learned Frobenius method to solve differential equation with this kind of form. But $$p(x)=2x^2$$ which means when $$x>0$$ there is no regular singular point so I'm not sure if I can apply Frobenius method here. Can I use it?",Problem is solve the differential equation and I've only learned Frobenius method to solve differential equation with this kind of form. But which means when there is no regular singular point so I'm not sure if I can apply Frobenius method here. Can I use it?,"2x^2y''-xy'+(1-x)y=0,~x>0 p(x)=2x^2 x>0",['ordinary-differential-equations']
14,Topological invariant of manifolds equipped with vector field,Topological invariant of manifolds equipped with vector field,,"Question Given a manifold $M$ equipped with a vector field $\xi$ which has a description in local coordinates. Is it possible to construct a topological invariant which can be used to tell if another manifold equipped with a vector field is equivalent up to coordinate transformation. This question can be equivalently formulated through differential $\alpha_i$ forms where $\alpha_i = \ker \xi$ Motivation Given a ordinary differential equation $\mathcal{E}$ it can be considered as a manifold equipped with the cartan distribution \begin{equation} D = \partial_{x} + p_1 \partial_{p_0} + \ldots + p_{k-1}\partial_{p_{k-2}} + F(x,p_0,\ldots,p_{k-1})\partial_{p_{k-1}} \end{equation} Having found equivalence classes of ordinary differential equations of a fixed order and its solution. I want to see if its possible to determine if a given ODE is equivalent to my ODE.",Question Given a manifold equipped with a vector field which has a description in local coordinates. Is it possible to construct a topological invariant which can be used to tell if another manifold equipped with a vector field is equivalent up to coordinate transformation. This question can be equivalently formulated through differential forms where Motivation Given a ordinary differential equation it can be considered as a manifold equipped with the cartan distribution Having found equivalence classes of ordinary differential equations of a fixed order and its solution. I want to see if its possible to determine if a given ODE is equivalent to my ODE.,"M \xi \alpha_i \alpha_i = \ker \xi \mathcal{E} \begin{equation}
D = \partial_{x} + p_1 \partial_{p_0} + \ldots + p_{k-1}\partial_{p_{k-2}} + F(x,p_0,\ldots,p_{k-1})\partial_{p_{k-1}}
\end{equation}","['ordinary-differential-equations', 'differential-geometry', 'differential-forms', 'vector-fields']"
15,A few questions concerning the Poincare-Bendixson theorem,A few questions concerning the Poincare-Bendixson theorem,,"I have a few questions concerning the Poincare-Bendixson result:( $G\subset \mathbb{R}^2$ open, $f\in C_{loc}^{1-}(G,\mathbb{R}^2)$ ) Let $u$ be a solution of $u'=f(u)$ so that $\overline{u(\mathbb{R^+})}\subset G$ compact. Assume $\omega(u)\cap E=\varnothing$ , where $E$ is the set of equilibria. Then $\omega(u)=u(\mathbb{R}^+)$ , i.e. $u$ is a periodic solution or $\omega(u)$ is a periodic orbit. Question According to https://sites.me.ucsb.edu/~moehlis/APC591/tutorials/tutorial3/node2.html a periodic orbit is a solution for which there exists $0<T<\infty$ so that $u(x+T)=u(x)$ for all $x$ . So if I have a periodic orbit why is this not a periodic solution? By Poincare-Bendixson we either have $\omega(u)=u(\mathbb{R}^+)$ or $\omega(u)=u([0,T])$ where $T$ is the period. But if I understand correctly if we have a periodic orbit it already holds $u([0,T])=u(\mathbb{R}^+)$ . It seems that this is false, is there a good example or picture to see this? Moreover I don't really get the point of homoclines: I'm imagining it as a loop which I run through once (then the period is infinity) or more than ones and then I have the corresponding period, is this the right picture? For me homoclinic orbits are periodic solutions. Depending on how often I run through the loop the omega limit set could contain only one point but also the complete loop, is this correct?","I have a few questions concerning the Poincare-Bendixson result:( open, ) Let be a solution of so that compact. Assume , where is the set of equilibria. Then , i.e. is a periodic solution or is a periodic orbit. Question According to https://sites.me.ucsb.edu/~moehlis/APC591/tutorials/tutorial3/node2.html a periodic orbit is a solution for which there exists so that for all . So if I have a periodic orbit why is this not a periodic solution? By Poincare-Bendixson we either have or where is the period. But if I understand correctly if we have a periodic orbit it already holds . It seems that this is false, is there a good example or picture to see this? Moreover I don't really get the point of homoclines: I'm imagining it as a loop which I run through once (then the period is infinity) or more than ones and then I have the corresponding period, is this the right picture? For me homoclinic orbits are periodic solutions. Depending on how often I run through the loop the omega limit set could contain only one point but also the complete loop, is this correct?","G\subset \mathbb{R}^2 f\in C_{loc}^{1-}(G,\mathbb{R}^2) u u'=f(u) \overline{u(\mathbb{R^+})}\subset G \omega(u)\cap E=\varnothing E \omega(u)=u(\mathbb{R}^+) u \omega(u) 0<T<\infty u(x+T)=u(x) x \omega(u)=u(\mathbb{R}^+) \omega(u)=u([0,T]) T u([0,T])=u(\mathbb{R}^+)","['ordinary-differential-equations', 'dynamical-systems', 'periodic-functions']"
16,Solving a modified matrix equation using solutions of the original equation,Solving a modified matrix equation using solutions of the original equation,,"I am trying to find analytically the eigenmodes of the following equation for a damped system with $N$ degrees of freedom: $$ \mathbf{M}\ddot{\mathbf{u}} + \mathbf{C} \dot{\mathbf{u}} + \mathbf{K}\mathbf{u} = 0 $$ in terms of the eigenmodes of the undamped system that I have already found computationally: $$ \mathbf{M}\ddot{\mathbf{u}} + \mathbf{K}\mathbf{u} = 0 $$ These eigenmodes are (say) $ \mathbf{u}_n(t) = \mathbf{u}_n e^{i\omega_n t} $ where $$ \mathbf{M}^{-1}\mathbf{K}\mathbf{u}_n = \omega_n^2 \mathbf{u}_n $$ $ \mathbf{M}$ is the diagonal mass matrix and $ \mathbf{C}$ is the diagonal matrix of damping factors for each dof. $ \mathbf{K}$ is the symmetric spring constant matrix coupling different degrees of freedom. My attempt so far: Assuming the time dependence as $\mathbf{u}(t) = \mathbf{u}e^{i\omega t}$ , the damped equation can be written as: $$  (\mathbf{M}^{-1}\mathbf{K} + i\omega\mathbf{M}^{-1}\mathbf{C})\mathbf{u} = \omega^2 \mathbf{u} $$ Now since $\mathbf{C}$ is not proportional to the Identity matrix, it does not commute with the ""dynamical"" matrix $\mathbf{M}^{-1}\mathbf{K}$ and so the previously found normal modes are not the eigenmodes for the new system. But is it possible to maybe rescale some of the degrees of freedom or transform the equation in another way and find the new normal modes using the old ones $\mathbf{u}_n$ ? Also, note that I wish to look at the solutions for low frequencies, so treating the damping as a small perturbation does not help.","I am trying to find analytically the eigenmodes of the following equation for a damped system with degrees of freedom: in terms of the eigenmodes of the undamped system that I have already found computationally: These eigenmodes are (say) where is the diagonal mass matrix and is the diagonal matrix of damping factors for each dof. is the symmetric spring constant matrix coupling different degrees of freedom. My attempt so far: Assuming the time dependence as , the damped equation can be written as: Now since is not proportional to the Identity matrix, it does not commute with the ""dynamical"" matrix and so the previously found normal modes are not the eigenmodes for the new system. But is it possible to maybe rescale some of the degrees of freedom or transform the equation in another way and find the new normal modes using the old ones ? Also, note that I wish to look at the solutions for low frequencies, so treating the damping as a small perturbation does not help.",N  \mathbf{M}\ddot{\mathbf{u}} + \mathbf{C} \dot{\mathbf{u}} + \mathbf{K}\mathbf{u} = 0   \mathbf{M}\ddot{\mathbf{u}} + \mathbf{K}\mathbf{u} = 0   \mathbf{u}_n(t) = \mathbf{u}_n e^{i\omega_n t}   \mathbf{M}^{-1}\mathbf{K}\mathbf{u}_n = \omega_n^2 \mathbf{u}_n   \mathbf{M}  \mathbf{C}  \mathbf{K} \mathbf{u}(t) = \mathbf{u}e^{i\omega t}   (\mathbf{M}^{-1}\mathbf{K} + i\omega\mathbf{M}^{-1}\mathbf{C})\mathbf{u} = \omega^2 \mathbf{u}  \mathbf{C} \mathbf{M}^{-1}\mathbf{K} \mathbf{u}_n,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
17,Explicit Runge-Kutta method for solving ODE,Explicit Runge-Kutta method for solving ODE,,"I would like to understand in some detail why this Runge-Kutta method is explicit if and only if $c_1=0$ . Why in this case the r.h.s. doesn't contain $y_{n+1}$ ? EDIT $c_1=0$ EDIT 2 I would also like to know if this implicit Euler method $$\begin{array}{c|c}1&1\\\hline&1\end{array}$$ is implicit in $k_1$ or also in $y_{n+1}$ . In other words, is it possible to obtain all these 4 possibilities ? Like being $(i)$ implicit for $k_1$ and explicit for $y_1$ , $(ii)$ implicit for $k_1$ and implicit for $y_1$ , $(iii)$ explicit for $k_1$ and explicit for $y_1$ , $(iv)$ explicit for $k_1$ and implicit for $y_1$ all of this for some suitable $s\geq 1$ stage method or perhaps with other index $i$ than $i=1$ ? In other words do we/can we distinguish already in the definition in which variable namely whether $k_i$ or $y_i$ is the method implicit and in which it is explicit ? Still, in your formula in the comment $y_{n+1}=y_n+hk_1$ the r.h.s. doesn't contain $y_{n+1}$ .","I would like to understand in some detail why this Runge-Kutta method is explicit if and only if . Why in this case the r.h.s. doesn't contain ? EDIT EDIT 2 I would also like to know if this implicit Euler method is implicit in or also in . In other words, is it possible to obtain all these 4 possibilities ? Like being implicit for and explicit for , implicit for and implicit for , explicit for and explicit for , explicit for and implicit for all of this for some suitable stage method or perhaps with other index than ? In other words do we/can we distinguish already in the definition in which variable namely whether or is the method implicit and in which it is explicit ? Still, in your formula in the comment the r.h.s. doesn't contain .",c_1=0 y_{n+1} c_1=0 \begin{array}{c|c}1&1\\\hline&1\end{array} k_1 y_{n+1} (i) k_1 y_1 (ii) k_1 y_1 (iii) k_1 y_1 (iv) k_1 y_1 s\geq 1 i i=1 k_i y_i y_{n+1}=y_n+hk_1 y_{n+1},"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
18,"Solution to $y'=y^2, y(1)=0$",Solution to,"y'=y^2, y(1)=0","Solution to $y'=y^2$ is $y = \frac{1}{C-x}$ . If we have extra condition that $y(1)=0$ than we get $0 = \frac{1}{C-1}$ . C then has no solutions. My text book says that the solution to this is $y=0$ , but why?","Solution to is . If we have extra condition that than we get . C then has no solutions. My text book says that the solution to this is , but why?",y'=y^2 y = \frac{1}{C-x} y(1)=0 0 = \frac{1}{C-1} y=0,"['ordinary-differential-equations', 'initial-value-problems']"
19,Solve the differential equation $(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0.$,Solve the differential equation,(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0.,"Solve the differential equation $(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0.$ I tried solving the problem like this: We assume $M=(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)$ and $N=(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x}).$ Now, we observe, $\frac{\partial  M}{\partial y}=12x^2+4xy-12y^2+2e^{2x}-e^y$ and $\frac{\partial N}{\partial x}=6x^2y+12x^2-12y^2-e^y+2e^{2x}.$ We find, that if we considered $M=12x^2y+4x^3-4y^3+2ye^{2x}-e^y$ and $N=4x^3-12xy^2+3y^2-xe^y+e^{2x}$ then $\frac{\partial  M}{\partial y}=12x^2-12y^2+2e^{2x}-e^y=\frac{\partial  N}{\partial x}.$ So, we consider or rather write the given equation, as $(12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+2xy^2dx+2x^3ydy=0.$ Now, we consider the differential equation, $(12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy=0.$ This equation is an exact differential equation. The solution is of this differential equation is $$\int (12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+\int 3y^2dy=c_1\implies 4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3=c_1.$$ Next we try to find the solution of the differential equation, $2xy^2dx+2x^3ydy=0.$ We write this differential equation as $\frac{dy}{dx}=\frac{2xy^2}{2x^3y}=\frac{y}{x^2}\implies \frac{dy}{y}=\frac{dx}{x^2}.$ On integrating, both sides of $\frac{dy}{y}=\frac{dx}{x^2},$ we obtain, $\int\frac{dy}{y}=\int\frac{dx}{x^2}\implies \log y=-\frac 1x+c_2\implies \log y+\frac 1x=c_2.$ Now, we add the solutions, of both of the differential equation, $(12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy=0$ and $2xy^2dx+2x^3ydy=0,$ to get the solution of the given differential equation, $$(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0$$ i.e $$4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3+\log y+\frac 1x=c_1+c_2=c.$$ So, the solution of the given differential equation is $4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3+\log y+\frac 1x=c.$ It seems, that when I approach the problem like this, we get a solution of the given equation. But is this a legit way? I don't understand, this this way of solving differential equations is justified. Finally, I want to know whether the solution, I calculated is valid or not?","Solve the differential equation I tried solving the problem like this: We assume and Now, we observe, and We find, that if we considered and then So, we consider or rather write the given equation, as Now, we consider the differential equation, This equation is an exact differential equation. The solution is of this differential equation is Next we try to find the solution of the differential equation, We write this differential equation as On integrating, both sides of we obtain, Now, we add the solutions, of both of the differential equation, and to get the solution of the given differential equation, i.e So, the solution of the given differential equation is It seems, that when I approach the problem like this, we get a solution of the given equation. But is this a legit way? I don't understand, this this way of solving differential equations is justified. Finally, I want to know whether the solution, I calculated is valid or not?","(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0. M=(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y) N=(2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x}). \frac{\partial  M}{\partial y}=12x^2+4xy-12y^2+2e^{2x}-e^y \frac{\partial N}{\partial x}=6x^2y+12x^2-12y^2-e^y+2e^{2x}. M=12x^2y+4x^3-4y^3+2ye^{2x}-e^y N=4x^3-12xy^2+3y^2-xe^y+e^{2x} \frac{\partial  M}{\partial y}=12x^2-12y^2+2e^{2x}-e^y=\frac{\partial  N}{\partial x}. (12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+2xy^2dx+2x^3ydy=0. (12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy=0. \int (12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+\int 3y^2dy=c_1\implies 4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3=c_1. 2xy^2dx+2x^3ydy=0. \frac{dy}{dx}=\frac{2xy^2}{2x^3y}=\frac{y}{x^2}\implies \frac{dy}{y}=\frac{dx}{x^2}. \frac{dy}{y}=\frac{dx}{x^2}, \int\frac{dy}{y}=\int\frac{dx}{x^2}\implies \log y=-\frac 1x+c_2\implies \log y+\frac 1x=c_2. (12x^2y+4x^3-4y^3+2ye^{2x}-e^y)dx+(4x^3-12xy^2+3y^2-xe^y+e^{2x})dy=0 2xy^2dx+2x^3ydy=0, (2x^3y+4x^3-12xy^2+3y^2-xe^y+e^{2x})dy+(12x^2y+2xy^2+4x^3-4y^3+2ye^{2x}-e^y)dx=0 4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3+\log y+\frac 1x=c_1+c_2=c. 4x^3y+x^4-4y^3x+ye^{2x}-e^yx+y^3+\log y+\frac 1x=c.","['ordinary-differential-equations', 'solution-verification', 'alternative-proof']"
20,Solve the following differential equation: $\frac{d^2y}{dx^2}-y=2+5x$ in the mentioned method.,Solve the following differential equation:  in the mentioned method.,\frac{d^2y}{dx^2}-y=2+5x,"I was reading a book called ""Introductory course in Differential Equations "" by D.A Murray. A problem solving approach was presented in the book, which goes as follows: Solve $(D^3+3D^2+2D)y = x^2$ The solution given was : The roots of $f(D)$ are $0,-2, -1$ ; and hence the complementary function is $c_1 + c_2e^{-2x}+ c_3e^{-x}.$ The particular integral $$\frac{1}{D^3+3D^2+2D}x^2=\frac{1}{2D}(1+\frac 32D+\frac {D^2} {2})^{-1}(x^2)=\frac{1}{2D}(1+\frac 32D+\frac {7D^2} {4}+...)(x^2)=\frac{1}{2D}(x^2-3x+\frac 72)=\frac{x}{12}(2x^2-9x+21),$$ and $\frac 1D x$ being merely $\int xdx.$ The complete solution is $$y=c_1 + c_2e^{-2x}+ c_3e^{-x}+\frac{x}{12}(2x^2-9x+21).$$ Then, they asked to: Solve $$\frac{d^2y}{dx^2}-y=2+5x$$ by this method ( method described above). I tried solving it,like this: Given, $$\frac{d^2y}{dx^2}-y=2+5x.$$ This implies $(D^2-1)y=2+5x.$ Now, we wish to calculate solution of the equation $(D^2-1)y=0,$ i.e the complementary function. The roots of the function $f(x)=x^2-1=0,$ is $x=\pm 1.$ So, the complementary function is $c_1e^{1}+c_2e^{-1}.$ Now, we proceed to calculate the particular integral of $$\frac{d^2y}{dx^2}-y=2+5x.$$ This is given by $y=\frac{1}{D^2-1}(2+5x).$ We can write this as $$y=\frac{1}{D^2-1}(2+5x)\implies \frac{1}{D}(D-\frac 1D)^{-1}(2+5x)=\frac 1D(1+D-\frac 1D-1)^{-1}(2+5x)=\frac 1D[1-(D-\frac 1D-1)+(D-\frac 1D-1)^2](2+5x)=\frac 1D(1-D+\frac 1D+1+D^2+\frac{1}{D^2}+1-2-2D+2\frac 1D)(2+5x)=\frac 1D[(2+5x)-D(2+5x)+\frac 1D(...$$ But the problem lies here. I dont have to worry about the terms of the form $D^n$ where $n\geq 2$ as then $D^n(2+5x)=0$ but the problem lies for the terms of the form $\frac{1}{D^n},n\in\Bbb Z,$ in the infinite series. How should I go on evaluating  this? I am not quite getting it...","I was reading a book called ""Introductory course in Differential Equations "" by D.A Murray. A problem solving approach was presented in the book, which goes as follows: Solve The solution given was : The roots of are ; and hence the complementary function is The particular integral and being merely The complete solution is Then, they asked to: Solve by this method ( method described above). I tried solving it,like this: Given, This implies Now, we wish to calculate solution of the equation i.e the complementary function. The roots of the function is So, the complementary function is Now, we proceed to calculate the particular integral of This is given by We can write this as But the problem lies here. I dont have to worry about the terms of the form where as then but the problem lies for the terms of the form in the infinite series. How should I go on evaluating  this? I am not quite getting it...","(D^3+3D^2+2D)y = x^2 f(D) 0,-2, -1 c_1 + c_2e^{-2x}+ c_3e^{-x}. \frac{1}{D^3+3D^2+2D}x^2=\frac{1}{2D}(1+\frac 32D+\frac {D^2} {2})^{-1}(x^2)=\frac{1}{2D}(1+\frac 32D+\frac {7D^2} {4}+...)(x^2)=\frac{1}{2D}(x^2-3x+\frac 72)=\frac{x}{12}(2x^2-9x+21), \frac 1D x \int xdx. y=c_1 + c_2e^{-2x}+ c_3e^{-x}+\frac{x}{12}(2x^2-9x+21). \frac{d^2y}{dx^2}-y=2+5x \frac{d^2y}{dx^2}-y=2+5x. (D^2-1)y=2+5x. (D^2-1)y=0, f(x)=x^2-1=0, x=\pm 1. c_1e^{1}+c_2e^{-1}. \frac{d^2y}{dx^2}-y=2+5x. y=\frac{1}{D^2-1}(2+5x). y=\frac{1}{D^2-1}(2+5x)\implies \frac{1}{D}(D-\frac 1D)^{-1}(2+5x)=\frac 1D(1+D-\frac 1D-1)^{-1}(2+5x)=\frac 1D[1-(D-\frac 1D-1)+(D-\frac 1D-1)^2](2+5x)=\frac 1D(1-D+\frac 1D+1+D^2+\frac{1}{D^2}+1-2-2D+2\frac 1D)(2+5x)=\frac 1D[(2+5x)-D(2+5x)+\frac 1D(... D^n n\geq 2 D^n(2+5x)=0 \frac{1}{D^n},n\in\Bbb Z,","['calculus', 'ordinary-differential-equations']"
21,whether the difference of two ode solutions monotone,whether the difference of two ode solutions monotone,,"This is a problem from my ODE course: Let $f(x,y)$ and $F(x,y)$ be continuously differentiable functions defined on an open region $\Omega$ in the plane, and for any $(x,y) \in \Omega$ , we have $f(x,y) \leq F(x,y)$ . Let the solutions to the initial value problem $y'=f(x,y)$ and $y'=F(x,y)$ with the same initial condition $y(x_0)=y_0$ be $y=\varphi(x)$ and $y=\Phi(x)$ , respectively, where $(x_0,y_0) \in \Omega$ . Let $d(x)=\Phi(x)-\varphi(x)$ . Is $d(x)$ monotonically increasing? This is not always true. I use MATLAB and find $F(x,y)=f(x,y)+1=x-y^2+1$ is a counterexample. But I need some examples that are easy to prove.","This is a problem from my ODE course: Let and be continuously differentiable functions defined on an open region in the plane, and for any , we have . Let the solutions to the initial value problem and with the same initial condition be and , respectively, where . Let . Is monotonically increasing? This is not always true. I use MATLAB and find is a counterexample. But I need some examples that are easy to prove.","f(x,y) F(x,y) \Omega (x,y) \in \Omega f(x,y) \leq F(x,y) y'=f(x,y) y'=F(x,y) y(x_0)=y_0 y=\varphi(x) y=\Phi(x) (x_0,y_0) \in \Omega d(x)=\Phi(x)-\varphi(x) d(x) F(x,y)=f(x,y)+1=x-y^2+1",['ordinary-differential-equations']
22,"Finding a formula for the $n^{\text{th}}$ term in a series solution to $y''+y'+e^{-x}y=0,y(0)=3,y'(0)=5$",Finding a formula for the  term in a series solution to,"n^{\text{th}} y''+y'+e^{-x}y=0,y(0)=3,y'(0)=5","I am tasked to find a formula for the $n^{\text{th}}$ term in a series solution to $y''+y'+e^{-x}y=0$ subject to initial conditions $y(0)=3,y'(0)=5$ . If $y$ is a solution to this DE and $D=\frac{d}{dx}$ it isn't too hard to see by taking successive derivatives that $$\Big[D^{n+2}+D^{n+1}+e^{-x}(D-1)^n\Big]y=0 \text{ for all } n\geq 0$$ Since $(D-1)^n=\sum_{k=0}^n(-1)^{n-k}{n\choose k}D^k$ can write this as $$\Bigg[D^{n+2}+D^{n+1}+e^{-x}\sum_{k=0}^n(-1)^{n-k}{n\choose k}D^k\Bigg]y=0$$ If we evaluate this as $x=0$ and rearrange terms we can say $$y^{(n+2)}(0)=-\Bigg[y^{(n+1)}(0)+\sum_{k=0}^n(-1)^{n-k}{n \choose k}y^{(k)}(0)\Bigg]$$ Is there any chance of finding a general formula for $y^{(n)}(0)$ in terms of $n$ ? I also considered writing $y$ as $\sum_{n=0}^{\infty}a_nt^n$ and $e^{-t}$ as $\sum_{n=0}^{\infty}\frac{(-t)^n}{n!}$ and solving for the coefficients this way, but I wasn't about to spend a majority of my day collecting coefficients. Thoughts? Thanks.","I am tasked to find a formula for the term in a series solution to subject to initial conditions . If is a solution to this DE and it isn't too hard to see by taking successive derivatives that Since can write this as If we evaluate this as and rearrange terms we can say Is there any chance of finding a general formula for in terms of ? I also considered writing as and as and solving for the coefficients this way, but I wasn't about to spend a majority of my day collecting coefficients. Thoughts? Thanks.","n^{\text{th}} y''+y'+e^{-x}y=0 y(0)=3,y'(0)=5 y D=\frac{d}{dx} \Big[D^{n+2}+D^{n+1}+e^{-x}(D-1)^n\Big]y=0 \text{ for all } n\geq 0 (D-1)^n=\sum_{k=0}^n(-1)^{n-k}{n\choose k}D^k \Bigg[D^{n+2}+D^{n+1}+e^{-x}\sum_{k=0}^n(-1)^{n-k}{n\choose k}D^k\Bigg]y=0 x=0 y^{(n+2)}(0)=-\Bigg[y^{(n+1)}(0)+\sum_{k=0}^n(-1)^{n-k}{n \choose k}y^{(k)}(0)\Bigg] y^{(n)}(0) n y \sum_{n=0}^{\infty}a_nt^n e^{-t} \sum_{n=0}^{\infty}\frac{(-t)^n}{n!}",['sequences-and-series']
23,Kernel and Differential Operator,Kernel and Differential Operator,,"I hope your day is great so far. I have a question. I am given a Cauchy-Euler ODE $$\big((x+2)^3\mathbf{D}^3+4(x+2)^2\mathbf{D}^2+3(x+2)\mathbf{D}+1\big)y=\frac{1}{x+2},$$ where $\mathbf{D}^n=d^n/dx^n$ . Letting $e^t=x+2$ transforms this ODE, such that the coefficients are constants, giving a new ODE $$(\mathsf{D}^3+\mathsf{D}^2+\mathsf{D}+1)y=e^{-t},$$ where $\mathsf{D}^n=d^n/dt^n$ . I can't use the formula $$\frac{1}{p(\mathsf{D})}[e^{at}]=\frac{1}{p(a)}e^{at},p(a)\neq0,$$ since $p(a)=0$ . How can I find the particular solution then? I actually find that we can use $$y_p=\frac{1}{p_{n-k}(a)}\bigg(\frac{1}{k!}t^k+\text{ker}\,\mathsf{D}^k\bigg)e^{at},$$ for this case. However, I don't understand this formula and was hoping someone could explain that. Thanks in advance!","I hope your day is great so far. I have a question. I am given a Cauchy-Euler ODE where . Letting transforms this ODE, such that the coefficients are constants, giving a new ODE where . I can't use the formula since . How can I find the particular solution then? I actually find that we can use for this case. However, I don't understand this formula and was hoping someone could explain that. Thanks in advance!","\big((x+2)^3\mathbf{D}^3+4(x+2)^2\mathbf{D}^2+3(x+2)\mathbf{D}+1\big)y=\frac{1}{x+2}, \mathbf{D}^n=d^n/dx^n e^t=x+2 (\mathsf{D}^3+\mathsf{D}^2+\mathsf{D}+1)y=e^{-t}, \mathsf{D}^n=d^n/dt^n \frac{1}{p(\mathsf{D})}[e^{at}]=\frac{1}{p(a)}e^{at},p(a)\neq0, p(a)=0 y_p=\frac{1}{p_{n-k}(a)}\bigg(\frac{1}{k!}t^k+\text{ker}\,\mathsf{D}^k\bigg)e^{at},","['linear-algebra', 'ordinary-differential-equations', 'differential-operators']"
24,Solve the following differential equation using power series :$(x-1)y''+xy'+\frac yx=0.$,Solve the following differential equation using power series :,(x-1)y''+xy'+\frac yx=0.,"I was learning to solve differetial equations, using power series. There was a problem given as: Solve the following differential equation using power series : $$(x-1)y''+xy'+\frac yx=0.$$ I tried solving the problem in the following way: Assuming, $y=\Sigma_{n=0}^\infty c_nx^n$ to be a solution of $(x-1)y""+xy'+\frac yx=0.$ Now, $y'=\Sigma_{n=1}^\infty n c_nx^{n-1}$ and $y''=\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}.$ Substituting these in the given equation, we get, $$(x-1)\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+x\Sigma_{n=1}^\infty n c_nx^{n-1}+\frac 1x\Sigma_{n=0}^\infty c_nx^n=0\implies x\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}-\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+x\Sigma_{n=1}^\infty n c_nx^{n-1}+\frac 1x\Sigma_{n=0}^\infty c_nx^n=0\implies \Sigma_{n=2}^\infty n(n-1) c_nx^{n-1}-\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+\Sigma_{n=1}^\infty n c_nx^{n}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0.$$ Reindexing the summations we get, $$\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n+1}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=0}^\infty (n+1) c_{n+1}x^{n+1}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0.$$ Again, $$\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n+1}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=0}^\infty (n+1) c_{n+1}x^{n+1}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0\implies \Sigma_{n=1}^\infty (n+1)(n) c_{n+1}x^{n}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=1}^\infty (n) c_{n}x^{n}+ \Sigma_{n=-1}^\infty c_{n+1}x^{n}=0\implies \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}x^{n}- (n+2)(n+1) c_{n+2}x^{n}+(n) c_{n}x^{n}+ c_{n+1}x^{n}]+2c_2+\frac {c_0}{x}+c_1=0.$$ Now, the right-hand side of the equation is free of $x^{-1}$ term due to which, $c_0=0.$ So, $ \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}x^{n}- (n+2)(n+1) c_{n+2}x^{n}+(n) c_{n}x^{n}+ c_{n+1}x^{n}]+2c_2+\frac {c_0}{x}+c_1=0\implies  \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}]x^n+2c_2+c_1=0.$ Now, comparing LHS and RHS of the equation, we can say that $2c_2+c_1=0$ and $ (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0.$ Thus, $c_2=-\frac{c_1}{2}.$ For $n\geq 1,$ we have, $ (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0\implies (n^2+n+1) c_{n+1}- (n+2)(n+1) c_{n+2}+n c_{n}=0\implies -(n^2+n+1) c_{n+1}+(n+2)(n+1) c_{n+2}-n c_{n}=0\implies (n+2)(n+1) c_{n+2}-(n^2+n+1) c_{n+1}-n c_{n}=0.$ I don't know how to proceed further since, the equation $ (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0,$ is a recurrence relation though, but how to find the coefficient $c_n$ here ? I am not getting, how to solve this recurrence relation in order to find $c_n$ ? (I am just a beginner and I need an elementary level explanation of this.) I am skeptical , whether at all a power series solution exists or not. (I found this problem in a handout, so I don't know about the source. If anyone does know about it, please do let me know.) There might be several posts, concerning the same topic, but I can't seem to find it either. My attempt is modified after Elliot Yu's beautiful suggestion","I was learning to solve differetial equations, using power series. There was a problem given as: Solve the following differential equation using power series : I tried solving the problem in the following way: Assuming, to be a solution of Now, and Substituting these in the given equation, we get, Reindexing the summations we get, Again, Now, the right-hand side of the equation is free of term due to which, So, Now, comparing LHS and RHS of the equation, we can say that and Thus, For we have, I don't know how to proceed further since, the equation is a recurrence relation though, but how to find the coefficient here ? I am not getting, how to solve this recurrence relation in order to find ? (I am just a beginner and I need an elementary level explanation of this.) I am skeptical , whether at all a power series solution exists or not. (I found this problem in a handout, so I don't know about the source. If anyone does know about it, please do let me know.) There might be several posts, concerning the same topic, but I can't seem to find it either. My attempt is modified after Elliot Yu's beautiful suggestion","(x-1)y''+xy'+\frac yx=0. y=\Sigma_{n=0}^\infty c_nx^n (x-1)y""+xy'+\frac yx=0. y'=\Sigma_{n=1}^\infty n c_nx^{n-1} y''=\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}. (x-1)\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+x\Sigma_{n=1}^\infty n c_nx^{n-1}+\frac 1x\Sigma_{n=0}^\infty c_nx^n=0\implies x\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}-\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+x\Sigma_{n=1}^\infty n c_nx^{n-1}+\frac 1x\Sigma_{n=0}^\infty c_nx^n=0\implies \Sigma_{n=2}^\infty n(n-1) c_nx^{n-1}-\Sigma_{n=2}^\infty n(n-1) c_nx^{n-2}+\Sigma_{n=1}^\infty n c_nx^{n}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0. \Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n+1}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=0}^\infty (n+1) c_{n+1}x^{n+1}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0. \Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n+1}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=0}^\infty (n+1) c_{n+1}x^{n+1}+ \Sigma_{n=0}^\infty c_nx^{n-1}=0\implies \Sigma_{n=1}^\infty (n+1)(n) c_{n+1}x^{n}-\Sigma_{n=0}^\infty (n+2)(n+1) c_{n+2}x^{n}+\Sigma_{n=1}^\infty (n) c_{n}x^{n}+ \Sigma_{n=-1}^\infty c_{n+1}x^{n}=0\implies \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}x^{n}- (n+2)(n+1) c_{n+2}x^{n}+(n) c_{n}x^{n}+ c_{n+1}x^{n}]+2c_2+\frac {c_0}{x}+c_1=0. x^{-1} c_0=0.  \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}x^{n}- (n+2)(n+1) c_{n+2}x^{n}+(n) c_{n}x^{n}+ c_{n+1}x^{n}]+2c_2+\frac {c_0}{x}+c_1=0\implies  \Sigma_{n=1}^\infty [(n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}]x^n+2c_2+c_1=0. 2c_2+c_1=0  (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0. c_2=-\frac{c_1}{2}. n\geq 1,  (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0\implies (n^2+n+1) c_{n+1}- (n+2)(n+1) c_{n+2}+n c_{n}=0\implies -(n^2+n+1) c_{n+1}+(n+2)(n+1) c_{n+2}-n c_{n}=0\implies (n+2)(n+1) c_{n+2}-(n^2+n+1) c_{n+1}-n c_{n}=0.  (n+1)(n) c_{n+1}- (n+2)(n+1) c_{n+2}+(n) c_{n}+ c_{n+1}=0, c_n c_n","['ordinary-differential-equations', 'power-series', 'problem-solving']"
25,Check a problem solution $au + (b + i\lambda)u_{xx} = 0$ with boundary conditions.,Check a problem solution  with boundary conditions.,au + (b + i\lambda)u_{xx} = 0,"Let $a, b >0 $ and $\lambda \in \mathbb{R}$ such that $\lambda \neq 0$ . Consider $u \in C^{1}([\alpha, \beta])$ such that $u(\alpha) = u_{x}(\alpha) = u_{x}(\beta)=0$ and the problem $$ au + (b + i\lambda)u_{xx} = 0, \ \  \text{in} \ \ (\alpha, \beta) $$ with $0 < \alpha < \beta < \infty$ . I would like to know if the above problem has a solution. I know that the problem $au + (b + \lambda)u_{xx} = 0$ with the same conditions has a solution and is null. I think in: Let $V = (u,u_{x})^{T}$ , then system can be written as the following $$ V_{x} = BV, \ \ \text{with} \ \ V(\alpha) = 0 $$ with $$ B = \left[ \begin{array}{cccc} 0 & 1 \\ -\frac{a}{ (b+i\lambda)}& 0 \\ \end{array} \right] $$ The solution of the differential Equation is given by $$ V(x) = e^{B(x - \alpha)}V(\alpha) $$ Thus, the fact that $V(\alpha) = 0$ , we get $V = 0$ in $(\alpha, \beta)$ I didn't use the fact that $u_{x}(\beta) = 0$ . Am I right?","Let and such that . Consider such that and the problem with . I would like to know if the above problem has a solution. I know that the problem with the same conditions has a solution and is null. I think in: Let , then system can be written as the following with The solution of the differential Equation is given by Thus, the fact that , we get in I didn't use the fact that . Am I right?","a, b >0  \lambda \in \mathbb{R} \lambda \neq 0 u \in C^{1}([\alpha, \beta]) u(\alpha) = u_{x}(\alpha) = u_{x}(\beta)=0 
au + (b + i\lambda)u_{xx} = 0, \ \  \text{in} \ \ (\alpha, \beta)
 0 < \alpha < \beta < \infty au + (b + \lambda)u_{xx} = 0 V = (u,u_{x})^{T} 
V_{x} = BV, \ \ \text{with} \ \ V(\alpha) = 0
  B = \left[
\begin{array}{cccc}
0 & 1 \\
-\frac{a}{ (b+i\lambda)}& 0 \\
\end{array}
\right]  
V(x) = e^{B(x - \alpha)}V(\alpha)
 V(\alpha) = 0 V = 0 (\alpha, \beta) u_{x}(\beta) = 0",['ordinary-differential-equations']
26,Local Truncation Error of Adam Bashforth 3-step,Local Truncation Error of Adam Bashforth 3-step,,"I want to get the local truncation error of this Adam Bashforth 3-step method $$y_{n+1} = y_n + \frac{h}{12} (23 f_{n+2}  16 f_{n+1} + 5 f_n)$$ I started by remembering the following Taylor development $$ y(x_{n+1}) = y(x_n) +hy'(x_n) +\frac{h^2}{2}y''(x_n)+\frac{h^3}{6}y'''(x_n)+\frac{h^4}{24 }y^{(4)}(x_n)$$ After, I do the subtraction of these two expressions and I get the following $$y(x_{n+1})- y_{n+1} = y(x_n)-y_n + hy'(x_n) - \frac{h}{12} (23 f_{n+2}) +\frac{h^2}{2}y''(x_n)+ 16\frac{h}{12} (f_{n+1}) +\frac{h^3}{6}y'''(x_n)- 5\frac{h}{12} (f_n) +\frac{h^4}{24 }y^{(4)}(x_n)$$ But from here I don't know how to continue since I try to simplify some terms to finally arrive at $$LTE=\frac{h^4} {8} y^{(4)}(\eta).$$ I'm struggling to catch the error in the calculation but I'm finally stuck","I want to get the local truncation error of this Adam Bashforth 3-step method I started by remembering the following Taylor development After, I do the subtraction of these two expressions and I get the following But from here I don't know how to continue since I try to simplify some terms to finally arrive at I'm struggling to catch the error in the calculation but I'm finally stuck","y_{n+1} = y_n + \frac{h}{12} (23 f_{n+2}  16 f_{n+1} + 5 f_n)  y(x_{n+1}) = y(x_n) +hy'(x_n) +\frac{h^2}{2}y''(x_n)+\frac{h^3}{6}y'''(x_n)+\frac{h^4}{24 }y^{(4)}(x_n) y(x_{n+1})- y_{n+1} = y(x_n)-y_n + hy'(x_n) - \frac{h}{12} (23 f_{n+2}) +\frac{h^2}{2}y''(x_n)+ 16\frac{h}{12} (f_{n+1}) +\frac{h^3}{6}y'''(x_n)- 5\frac{h}{12} (f_n) +\frac{h^4}{24 }y^{(4)}(x_n) LTE=\frac{h^4}
{8} y^{(4)}(\eta).","['ordinary-differential-equations', 'numerical-methods']"
27,Rules for Choosing Bounds and Initial Conditions when Using 2nd Order Runge Kutta Methods,Rules for Choosing Bounds and Initial Conditions when Using 2nd Order Runge Kutta Methods,,"I have a question regarding 2nd order Runge-Kutta methods, specifically where it regards the bounds of the solution. Let's say I have to solve a 1st order ODE $\frac{dy}{dx}=f(x,y)$ numerically using Heun's method: $$y_{i+1}=y_i+\frac{1}{2}h(k_1+k_2)$$ $$k_1=f(x_i,y_i), k_2=f(x_i+h,y_i+k_1h)$$ The ODE can be solved within a range let's say $a<x<b$ . My question is whether the initial condition needs to coincide with $x=a$ or whether it only needs between the bounds. Here is an example I was messing around with when I noticed the potential issue. Solve the following ODE using Heun's method within the bounds $a<x<b$ with a step size of $h=0.2$ . $$xy^\prime+y=x\sin(x), y(\pi)=2$$ If the range is $1<x<3$ where $x_0=\pi$ is outside of the range the numerical error is very high giving the following plot after calculating in MATLAB. I noticed the shape of the approximation was similar to the exact solution. If I change the range to $3<x<5$ so that $x_0=\pi$ falls within the range, the error is significantly smaller. Is this because $a<x_0<b$ or is it because $\pi\approx3$ and I should have started at $x_0=3$ ?","I have a question regarding 2nd order Runge-Kutta methods, specifically where it regards the bounds of the solution. Let's say I have to solve a 1st order ODE numerically using Heun's method: The ODE can be solved within a range let's say . My question is whether the initial condition needs to coincide with or whether it only needs between the bounds. Here is an example I was messing around with when I noticed the potential issue. Solve the following ODE using Heun's method within the bounds with a step size of . If the range is where is outside of the range the numerical error is very high giving the following plot after calculating in MATLAB. I noticed the shape of the approximation was similar to the exact solution. If I change the range to so that falls within the range, the error is significantly smaller. Is this because or is it because and I should have started at ?","\frac{dy}{dx}=f(x,y) y_{i+1}=y_i+\frac{1}{2}h(k_1+k_2) k_1=f(x_i,y_i), k_2=f(x_i+h,y_i+k_1h) a<x<b x=a a<x<b h=0.2 xy^\prime+y=x\sin(x), y(\pi)=2 1<x<3 x_0=\pi 3<x<5 x_0=\pi a<x_0<b \pi\approx3 x_0=3","['ordinary-differential-equations', 'numerical-methods', 'recursive-algorithms', 'runge-kutta-methods']"
28,Reduce System of 3 ODEs to 2 ODEs,Reduce System of 3 ODEs to 2 ODEs,,"I'm currently self-studying the book ""Nonlinear Dynamics and Chaos"". I got the following system $$ \dot{x} = -kxy \\ \dot{y} = kxy - ly \\ \dot{z} = ly $$ According to a comment on one of Strogatzs' videos on Youtube one is able to reduce this system to 2 equations. I never did something along the lines so I'm not quite sure how to do this. $\dot{y}$ looks like a combination of the other two. Can I simply say something along the lines that $\dot{y}$ is equal to $-(\dot{x} - \dot{z})$ and simply apply the methods I already know to to $\dot{x}$ and $\dot{z}$ ? In the book is an exercise where they combine the system to a single ODE, but I don't really get how as well. I assume combining it to two ODEs must be simpler. I would appreciate some help or some resource I can check out to understand the process better.","I'm currently self-studying the book ""Nonlinear Dynamics and Chaos"". I got the following system According to a comment on one of Strogatzs' videos on Youtube one is able to reduce this system to 2 equations. I never did something along the lines so I'm not quite sure how to do this. looks like a combination of the other two. Can I simply say something along the lines that is equal to and simply apply the methods I already know to to and ? In the book is an exercise where they combine the system to a single ODE, but I don't really get how as well. I assume combining it to two ODEs must be simpler. I would appreciate some help or some resource I can check out to understand the process better.","
\dot{x} = -kxy \\
\dot{y} = kxy - ly \\
\dot{z} = ly
 \dot{y} \dot{y} -(\dot{x} - \dot{z}) \dot{x} \dot{z}","['ordinary-differential-equations', 'analysis', 'nonlinear-dynamics']"
29,First order ODE - appropriate method,First order ODE - appropriate method,,"I'm trying to solve the following ODE $y'(x)=2y -x$ I tried to solve it with the method of variation of constants and it worked. I considered the homogenous equation without the last term ( $-x$ ),and I found the solution of the homogenous equation $y_h=C e^{2x}$ . Then, I considered the function $y(x)=C(x)e^{2x}$ , and $y^{'}(x)=C^{'}(x)e^{2x}+2C(x)e^{2x}$ . By plugging these 2 equations in the initial ODE, I got $C^{'}(x)=\frac{-x}{e^{2x}}$ Then, I integrated $C'(x)$ , and the particular solution , using $C(x)$ , is given by $y_p=\frac{1}{4} (2x+1)$ Then, the general solution is given by $y(x)=y_h(x)+y_p(x) =\frac{1}{4}(2x+1) +Ce^{2x}$ The question is: why am I allowed to use the method of variation of constants? I understood how to use it, but when I can use it?","I'm trying to solve the following ODE I tried to solve it with the method of variation of constants and it worked. I considered the homogenous equation without the last term ( ),and I found the solution of the homogenous equation . Then, I considered the function , and . By plugging these 2 equations in the initial ODE, I got Then, I integrated , and the particular solution , using , is given by Then, the general solution is given by The question is: why am I allowed to use the method of variation of constants? I understood how to use it, but when I can use it?",y'(x)=2y -x -x y_h=C e^{2x} y(x)=C(x)e^{2x} y^{'}(x)=C^{'}(x)e^{2x}+2C(x)e^{2x} C^{'}(x)=\frac{-x}{e^{2x}} C'(x) C(x) y_p=\frac{1}{4} (2x+1) y(x)=y_h(x)+y_p(x) =\frac{1}{4}(2x+1) +Ce^{2x},"['calculus', 'ordinary-differential-equations']"
30,Integrate $\ddot{u}(x)=(a+bx)u(x)$,Integrate,\ddot{u}(x)=(a+bx)u(x),"I wish to integrate the following differential equation $$\ddot{u}(x)=(a+bx)u(x)$$ to solve for $u(x)$ . I tried to characterize it as a Sturm-Liouville problem and introduce an integrating factor, but I was unable to make much headway. I would like to try integrating it without appealing to a power series and recurrence relationship. (I was already capable of producing a power series solution, but it did not elucidate me.) If the differential equation was first order, I would have immediately used separability. I also tried the ansatz $u(x) = e^{\pm \sqrt a x} w(x)$ which produced the equation $$ \ddot w(x) + 2\sqrt {a} \dot w(x) - b x w(x) = 0 $$","I wish to integrate the following differential equation to solve for . I tried to characterize it as a Sturm-Liouville problem and introduce an integrating factor, but I was unable to make much headway. I would like to try integrating it without appealing to a power series and recurrence relationship. (I was already capable of producing a power series solution, but it did not elucidate me.) If the differential equation was first order, I would have immediately used separability. I also tried the ansatz which produced the equation","\ddot{u}(x)=(a+bx)u(x) u(x) u(x) = e^{\pm \sqrt a x} w(x) 
\ddot w(x) + 2\sqrt {a} \dot w(x) - b x w(x) = 0
","['ordinary-differential-equations', 'sturm-liouville']"
31,$\omega$-limit set in $\mathbb S^2$,-limit set in,\omega \mathbb S^2,"Let $X(x,y,z)=(-y+xz^2,x+yz^2,-zx^2-zy^2)$ be a vectorial field in $\mathbb S^2$ , computationally it is easy to see that the $\omega-$ limit set of any point other than the poles is the equator, but I don't know how to prove this mathematically, any ideas please?","Let be a vectorial field in , computationally it is easy to see that the limit set of any point other than the poles is the equator, but I don't know how to prove this mathematically, any ideas please?","X(x,y,z)=(-y+xz^2,x+yz^2,-zx^2-zy^2) \mathbb S^2 \omega-","['ordinary-differential-equations', 'dynamical-systems']"
32,Is every vector field on real line complete?,Is every vector field on real line complete?,,"This question was left as an exercise in my course of Differential Geometry. I am not able to make much progress on it. Question: Is every vector field on the real line complete? Attempt:A vector field X along a curve $\sigma : [a,b] \to M$ is a mapping $X: [a,b]\to T(M)$ which lifts $\sigma$ ; ie $\pi \circ X=\sigma$ . A smooth vector field X on M is called complete if $D_t =m$ where is defined on Page 37 of Frank Warner's book as follows: But I am not able to think in the case of $D_t=\mathbb{R}$ , that under what conditions the equality should hold. So, can you please help me with this? Thanks!","This question was left as an exercise in my course of Differential Geometry. I am not able to make much progress on it. Question: Is every vector field on the real line complete? Attempt:A vector field X along a curve is a mapping which lifts ; ie . A smooth vector field X on M is called complete if where is defined on Page 37 of Frank Warner's book as follows: But I am not able to think in the case of , that under what conditions the equality should hold. So, can you please help me with this? Thanks!","\sigma : [a,b] \to M X: [a,b]\to T(M) \sigma \pi \circ X=\sigma D_t =m D_t=\mathbb{R}",['ordinary-differential-equations']
33,Converting a differential equation to a hypergeometric equation,Converting a differential equation to a hypergeometric equation,,"I would like  to transform the following differential equation into a hypergeometric equation: ${z^2}\frac{{{d^2}W}}{{d{z^2}}} + z\frac{{{d^{}}W}}{{d{z^{}}}} + \left( {{A^2} - {B^2}\left( {1 - {C_{}}z + D\frac{{z\,}}{{{{\left( {1 - z} \right)}^{}}}}} \right) - \frac{F}{{{{\left( {1 - z} \right)}^2}}}} \right)W = 0.$ Here A, B, C, D and F are constants. I tried by letting I tried by letting $W = {z^p}{\left( {1 - z} \right)^q}y.$ After replacing it into the above equation, I  ran into trouble trying to ""fit"" the values of p and q so that the y  term in the resulting equation would be independent of z.  I thought it was a  straightforward thing to do.  However, the values that I got for p and q did not reduce the equation to a hypergeometric one. Obviously, I am missing something simple here. Any advice will be most appreciated.","I would like  to transform the following differential equation into a hypergeometric equation: Here A, B, C, D and F are constants. I tried by letting I tried by letting After replacing it into the above equation, I  ran into trouble trying to ""fit"" the values of p and q so that the y  term in the resulting equation would be independent of z.  I thought it was a  straightforward thing to do.  However, the values that I got for p and q did not reduce the equation to a hypergeometric one. Obviously, I am missing something simple here. Any advice will be most appreciated.","{z^2}\frac{{{d^2}W}}{{d{z^2}}} + z\frac{{{d^{}}W}}{{d{z^{}}}} + \left( {{A^2} - {B^2}\left( {1 - {C_{}}z + D\frac{{z\,}}{{{{\left( {1 - z} \right)}^{}}}}} \right) - \frac{F}{{{{\left( {1 - z} \right)}^2}}}} \right)W = 0. W = {z^p}{\left( {1 - z} \right)^q}y.","['ordinary-differential-equations', 'hypergeometric-function']"
34,Envelope of x-t graph in Damped harmonic oscillations,Envelope of x-t graph in Damped harmonic oscillations,,"In our lecture, we learnt that the $x-t$ graph of an underdamped harmonic oscillator is basically a sinusoidal curve with a fixed frequency, bounded by an envelope which is an exponentially decaying curve, like this: Now, we learned to solve the damped ODE $\ddot x+2\beta \dot x+\omega_0^2x=0$ to get the general non-trivial solution $x(t)=e^{-\beta t}\bigg(c_1\exp(i\omega t)+c_2\exp(-i\omega t)\bigg)$ where $\omega=\sqrt{\omega_0^2-\beta^2}$ . The envelope is supposed to be $y=e^{-\beta t}$ . The problem is : I want to prove that all the extremas of $y=\exp(-ax)\sin(bx)$ lie on the envelope $y=\exp(-ax)$ and its mirror image about the x-axis. So, first, for getting the extremas, $\dot y=0$ which gives us $$\exp(-ax)\bigg(-a\sin bx+b\cos bx\bigg)=0$$ or $$\tan bx =\dfrac ba$$ . Now suppose that $x=x_0$ satisfies the last condition. Then how do we show that the point $\bigg(x_0, \exp(-ax_0)\sin(bx_0) \bigg)$ must lie on the curve $y=\pm \exp(-ax)$ , or $y^2-\exp(-2ax)=0$ ?","In our lecture, we learnt that the graph of an underdamped harmonic oscillator is basically a sinusoidal curve with a fixed frequency, bounded by an envelope which is an exponentially decaying curve, like this: Now, we learned to solve the damped ODE to get the general non-trivial solution where . The envelope is supposed to be . The problem is : I want to prove that all the extremas of lie on the envelope and its mirror image about the x-axis. So, first, for getting the extremas, which gives us or . Now suppose that satisfies the last condition. Then how do we show that the point must lie on the curve , or ?","x-t \ddot x+2\beta \dot x+\omega_0^2x=0 x(t)=e^{-\beta t}\bigg(c_1\exp(i\omega t)+c_2\exp(-i\omega t)\bigg) \omega=\sqrt{\omega_0^2-\beta^2} y=e^{-\beta t} y=\exp(-ax)\sin(bx) y=\exp(-ax) \dot y=0 \exp(-ax)\bigg(-a\sin bx+b\cos bx\bigg)=0 \tan bx =\dfrac ba x=x_0 \bigg(x_0, \exp(-ax_0)\sin(bx_0) \bigg) y=\pm \exp(-ax) y^2-\exp(-2ax)=0",['ordinary-differential-equations']
35,Does $f(t) \leq - \int_0^t f(s) ds \implies f \leq 0$?,Does ?,f(t) \leq - \int_0^t f(s) ds \implies f \leq 0,"I don't know if $f(t) \leq - \int_0^t f(s) ds \implies  f \leq 0$ is true or not (f is a continuous function with $f(0) = 0$ ). The range of $t$ is $[0,\infty[$ I can show that it implies that $\int_0^t f(s) ds \leq 0$ (by multiplying by $e^{t}$ and integrating) but there are function f that are not negative everywhere such that $\int_0^t f(s) ds \leq 0$ (really negative next to $0$ and then positive but small). For the context, I am trying to get information about $g \geq 0$ when I know that $g(t) + \int_0^t g(t) \leq t$ . I would like to obtain a kind of gronwall bound, with a ""small"" contant (not exponential) as: $g(t)\leq C t$ or better something with not depend on $t$ .","I don't know if is true or not (f is a continuous function with ). The range of is I can show that it implies that (by multiplying by and integrating) but there are function f that are not negative everywhere such that (really negative next to and then positive but small). For the context, I am trying to get information about when I know that . I would like to obtain a kind of gronwall bound, with a ""small"" contant (not exponential) as: or better something with not depend on .","f(t) \leq - \int_0^t f(s) ds \implies  f \leq 0 f(0) = 0 t [0,\infty[ \int_0^t f(s) ds \leq 0 e^{t} \int_0^t f(s) ds \leq 0 0 g \geq 0 g(t) + \int_0^t g(t) \leq t g(t)\leq C t t","['real-analysis', 'ordinary-differential-equations']"
36,Maximum of $y''$ for BVP with $y''''\leq0$.,Maximum of  for BVP with .,y'' y''''\leq0,"Consider the following boundary value problem for some $L>0$ and $w(x)\geq 0$ : $$\frac{d^4y}{dx^4}=-w(x)\,;\,\,\,y(0)=y(L)=0,\,y'(0)=y'(L)=0.$$ Here $w$ can be pathological: a step-function or an impulse function $w(x)=w_0\,\delta(x-a)$ : anything that ensures that $y''$ is continuous (but not necessarily differentiable). From context (below) you can understand the kind of things that are allowed here. Is it always the case that the maximum of $|y''|$ is at $x=0$ or $x=L$ ? The fact that the second derivative of $y''$ is negative ensures that the maximum of $|y''|$ occurs at $x=0$ , $x=L$ , or where $\dfrac{d}{dx}(y''(x))=0$ . In context this is related to the maximum bending moment of fixed end beam. To that end I find with a point load at the midpoint, $w(x)=w_0\,\delta(x-L/2)$ we have: $$|y''(0)|=|y''(L/2)|=|y''(L)|=\frac{w_0L}{8}.$$ But can we have $|y''(x_m)|>|y''(0)|,|y''(L)|$ for $0<x_m<L$ . I feel like I could ask this of an engineer (and have done so here ), but would be interested in a more mathematical answer.","Consider the following boundary value problem for some and : Here can be pathological: a step-function or an impulse function : anything that ensures that is continuous (but not necessarily differentiable). From context (below) you can understand the kind of things that are allowed here. Is it always the case that the maximum of is at or ? The fact that the second derivative of is negative ensures that the maximum of occurs at , , or where . In context this is related to the maximum bending moment of fixed end beam. To that end I find with a point load at the midpoint, we have: But can we have for . I feel like I could ask this of an engineer (and have done so here ), but would be interested in a more mathematical answer.","L>0 w(x)\geq 0 \frac{d^4y}{dx^4}=-w(x)\,;\,\,\,y(0)=y(L)=0,\,y'(0)=y'(L)=0. w w(x)=w_0\,\delta(x-a) y'' |y''| x=0 x=L y'' |y''| x=0 x=L \dfrac{d}{dx}(y''(x))=0 w(x)=w_0\,\delta(x-L/2) |y''(0)|=|y''(L/2)|=|y''(L)|=\frac{w_0L}{8}. |y''(x_m)|>|y''(0)|,|y''(L)| 0<x_m<L","['ordinary-differential-equations', 'optimization', 'maxima-minima', 'boundary-value-problem', 'step-function']"
37,Differential Equations Solving for Critical Points,Differential Equations Solving for Critical Points,,"$x'' +20x - 5x^3 = 0$ Did a quick substitution and found the critical points to be $(2,0), (-2,0)$ , and $(0,0)$ . However, when solving for the eigen values of the corresponding matrix found that the eigen values were both $0$ . Unclear what to do from now. My matrix was $\big\{\{0,1\},\{5x^2-20,0\}\big\}$ .","Did a quick substitution and found the critical points to be , and . However, when solving for the eigen values of the corresponding matrix found that the eigen values were both . Unclear what to do from now. My matrix was .","x'' +20x - 5x^3 = 0 (2,0), (-2,0) (0,0) 0 \big\{\{0,1\},\{5x^2-20,0\}\big\}",['ordinary-differential-equations']
38,Is central manifold theorem applicable on linear dynamical systems?,Is central manifold theorem applicable on linear dynamical systems?,,"I wondered if we could apply the central manifold theorem to a linear dynamical system. I tried to solve the most general case. Having a system such as: $$ \dot{x} =  -x + f(x,y) \\ \dot{y} =  -y + g(x,y) \\ $$ Let's apply the central manifold theorem: $$ y = h(x) = \sum_{n>2}h_n x^n \\ \partial_x h(x) = n \sum_{n>2}h_n x^{n-1} \\ \dot{y} = \partial_x h(x)\dot{x} = (n \sum_{n>2}h_n x^{n-1})(-x + f(x,y)) \\ = n\sum_{n>2}h_n x^n + f(x,y) n \sum_{n>2}h_n x^{n-1}  $$ On the other hand, we know that $$ \dot{y} = - \sum_{n>2}h_n x^n + g(x, y) $$ Now let's apply this to the linear system of choice: $$ \dot{x} = -x + y \\ \dot{y} = -y +  x $$ where $f(x,y) = y$ and $g(x,y) = x$ replacing $f(x, y) = y = h(x)= \sum_{n>2}h_n x^n$ would gives us $$ n\sum_{n>2}h_n x^n + n (\sum_{n>2}h_n x^n)(\sum_{n>2}h_n x^{n-1}) =   -\sum_{n>2}h_n x^n + x^1 $$ This equality is not met since there's a $x^1$ term on the right-hand side, which means that it's not possible to have a central manifold on a lower dimension (1d). I also tried it with an example of a system that has $wx^1$ so I can set $w=0$ to overcome this problem. I ended up having all the coefficients equal zero in the end: $h_i=0, i >0 $ I don't know how general is my conclusion, can we apply the central manifold theorem to Linear systems?","I wondered if we could apply the central manifold theorem to a linear dynamical system. I tried to solve the most general case. Having a system such as: Let's apply the central manifold theorem: On the other hand, we know that Now let's apply this to the linear system of choice: where and replacing would gives us This equality is not met since there's a term on the right-hand side, which means that it's not possible to have a central manifold on a lower dimension (1d). I also tried it with an example of a system that has so I can set to overcome this problem. I ended up having all the coefficients equal zero in the end: I don't know how general is my conclusion, can we apply the central manifold theorem to Linear systems?","
\dot{x} =  -x + f(x,y) \\
\dot{y} =  -y + g(x,y) \\
 
y = h(x) = \sum_{n>2}h_n x^n \\
\partial_x h(x) = n \sum_{n>2}h_n x^{n-1} \\
\dot{y} = \partial_x h(x)\dot{x} = (n \sum_{n>2}h_n x^{n-1})(-x + f(x,y)) \\
= n\sum_{n>2}h_n x^n + f(x,y) n \sum_{n>2}h_n x^{n-1} 
 
\dot{y} = - \sum_{n>2}h_n x^n + g(x, y)
 
\dot{x} = -x + y \\
\dot{y} = -y +  x
 f(x,y) = y g(x,y) = x f(x, y) = y = h(x)= \sum_{n>2}h_n x^n 
n\sum_{n>2}h_n x^n + n (\sum_{n>2}h_n x^n)(\sum_{n>2}h_n x^{n-1}) = 
 -\sum_{n>2}h_n x^n + x^1
 x^1 wx^1 w=0 h_i=0, i >0 ","['ordinary-differential-equations', 'differential-geometry', 'manifolds', 'dynamical-systems']"
39,Characterization of weak solutions to a(n) (simple) ODE,Characterization of weak solutions to a(n) (simple) ODE,,"Fix $f \in L^1_{loc}(a,b)$ and define the operator $L_f: C_c^\infty(a,b) \to \mathbb{R}$ given by $$L_f(\phi) = -\int_a^b f\phi'.$$ It is well-known that $f$ is a weak solution to the differential equation $u' = 0$ . Moreover, in this case $f$ is constant a.e. I am sort of new to this idea of weak derivatives, but how could we formulate a similar operator so that $f$ is a weak solution to say for example the differential equation $u'' = 0$ ? In this case how would one find all solutions?","Fix and define the operator given by It is well-known that is a weak solution to the differential equation . Moreover, in this case is constant a.e. I am sort of new to this idea of weak derivatives, but how could we formulate a similar operator so that is a weak solution to say for example the differential equation ? In this case how would one find all solutions?","f \in L^1_{loc}(a,b) L_f: C_c^\infty(a,b) \to \mathbb{R} L_f(\phi) = -\int_a^b f\phi'. f u' = 0 f f u'' = 0","['real-analysis', 'ordinary-differential-equations']"
40,Struggles solving : $\frac23xyy'=\sqrt{x^6-y^4}+y^2.$,Struggles solving :,\frac23xyy'=\sqrt{x^6-y^4}+y^2.,"Solve $\frac23xyy'=\sqrt{x^6-y^4}+y^2.$ (Hint: the equation can be boiled down to a homogeneous one.) My attempt: I didn't know how to obtain a homogeneous equation, so I'm going to show what I did instead. We can write $\frac23xyy'=|x|^3\sqrt{1-\frac{y^4}{x^6}}+y^2.$ Let $z=\frac{y^2}{x^3}=y^2x^{-3}.$ Then $$\begin{aligned}z'&=2yy'x^{-3}-3y^2x^{-4}\\ \implies x^4z'&=2xyy'-3y^2\\ \implies\frac23xyy'&=\frac{x^4z'+3y^2}3=\frac{x^4z'}3+y^2\\ \implies\frac{x^4z'}3+y^2&=|x|^3\sqrt{1-z^2}+y^2\\ \implies\frac{|x|z'}3&=\sqrt{1-z^2}\\ \implies\frac{dz}{\sqrt{1-z^2}}&=3\frac{dx}{|x|}\\ \implies\arcsin z&=3\ln|x|+C, C\in\Bbb R\\ \implies z&=\sin\ln(C|x|^3), C>0\\ \implies y^2&=x^3\sin\ln(C|x|^3)C>0.\end{aligned}$$ I'm not sure my answer is correct. I didn't get far plugging the result into the equation. How do we solve the given ODE? Or should I better ask, how do we obtain a homogeneous ODE from it?","Solve (Hint: the equation can be boiled down to a homogeneous one.) My attempt: I didn't know how to obtain a homogeneous equation, so I'm going to show what I did instead. We can write Let Then I'm not sure my answer is correct. I didn't get far plugging the result into the equation. How do we solve the given ODE? Or should I better ask, how do we obtain a homogeneous ODE from it?","\frac23xyy'=\sqrt{x^6-y^4}+y^2. \frac23xyy'=|x|^3\sqrt{1-\frac{y^4}{x^6}}+y^2. z=\frac{y^2}{x^3}=y^2x^{-3}. \begin{aligned}z'&=2yy'x^{-3}-3y^2x^{-4}\\ \implies x^4z'&=2xyy'-3y^2\\ \implies\frac23xyy'&=\frac{x^4z'+3y^2}3=\frac{x^4z'}3+y^2\\ \implies\frac{x^4z'}3+y^2&=|x|^3\sqrt{1-z^2}+y^2\\ \implies\frac{|x|z'}3&=\sqrt{1-z^2}\\ \implies\frac{dz}{\sqrt{1-z^2}}&=3\frac{dx}{|x|}\\ \implies\arcsin z&=3\ln|x|+C, C\in\Bbb R\\ \implies z&=\sin\ln(C|x|^3), C>0\\ \implies y^2&=x^3\sin\ln(C|x|^3)C>0.\end{aligned}",['ordinary-differential-equations']
41,solutions of Initial value problem of system of differential equations,solutions of Initial value problem of system of differential equations,,"I have problems when reading this theorem from the book. I am confused that why here $$ u(\tau)=u^{\prime}(\tau)=\ldots=u^{(n-2)}(\tau)=0, \quad u^{(n-1)}(\tau)=1 $$ but $$y\left(t_0\right)=y^{\prime}\left(t_0\right)=\ldots=y^{(n-1)}\left(t_0\right)=0 $$ And why in the example below the theorem, $y(1)=y'(1)=0$ changed to $y(1)=2,y'(1)=5$ later? Could someone please explain to me! Thanks in advance. \\ THEOREM 6. Let $L(t, \lambda)=\lambda^n+a_{n-1}(t) \lambda^{n-1}+\ldots+a_1(t) \lambda+a_0(t) \quad$ and $\quad D=\frac{d}{d t}$ . Denote by $E(t, \tau)$ the uniquely determined solution $u(t)$ of the initial value problem (16) $\quad L(t, D) u=0$ $$ u(\tau)=u^{\prime}(\tau)=\ldots=u^{(n-2)}(\tau)=0, \quad u^{(n-1)}(\tau)=1 $$ Then (17) $$ y(t)=\int_{t_0}^t E(t, \tau) g(\tau) d \tau $$ is the solution of the problem $$ \begin{aligned} &L(t, D) y=g(t) \\ &y\left(t_0\right)=y^{\prime}\left(t_0\right)=\ldots=y^{(n-1)}\left(t_0\right)=0 \end{aligned} $$ Example 6. Find the solution of the differential equation (18) $t^2 y^{\prime \prime}-2 t y^{\prime}+2 y=t^2 \sin t^4, \quad t>0$ , which satisfies $y(1)=2, y^{\prime}(1)=5$ . Solution: The homogeneous equation $t^2 y^{\prime \prime}-2 t y^{\prime}+2 y=0$ is an Euler equation. The indicial equation is $\lambda(\lambda-1)-2 \lambda+2=0$ , with roots $\lambda=1, \lambda=2$ . The general solution of the homogeneous equation is thus $$ y(t)=C t+D t^2 \quad(C, D \text { constants }) . $$ We now find the fundamental solution. By (16), this must satisfy the conditions $y(\tau)=0, y^{\prime}(\tau)=1$ . We obtain the following system of equations for $C$ and $D$ : $$ \left\{\begin{array} { l }  { C \tau + D \tau ^ { 2 } = 0 } \\ { C + 2 D \tau = 1 } \end{array} \quad \Longleftrightarrow \quad \left\{\begin{array}{l} C=-1 \\ D=1 / \tau \end{array}\right.\right. $$ It follows that $$ E(t, \tau)=-t+\frac{t^2}{\tau}, \quad t, \tau>0 $$ By theorem 6 $$ \bar{y}(t)=\int_1^t\left(-t+\frac{t^2}{\tau}\right) \sin \tau^4 d \tau, \quad t>0 $$ is that solution of equation (18) which satisfies $y(1)=y^{\prime}(1)=0$ . (We have divided (18) by $t^2$ to make the coefficient of $y^{\prime \prime}$ equal to 1 .) The general solution of $(18)$ is obtained by adding the general solution of the homogeneous equation, i.e., $$ y(t)=\bar{y}(t)+C t+D t^2 $$ The conditions $y(1)=2, y^{\prime}(1)=5$ lead to $C+D=2, C+2 D=5$ , resulting in $C=-1, D=3$ . Hence the required solution is $$ y(t)=\int_1^t\left(-t+\frac{t^2}{\tau}\right) \sin \tau^4 d \tau-t+3 t^2, \quad t>0 $$","I have problems when reading this theorem from the book. I am confused that why here but And why in the example below the theorem, changed to later? Could someone please explain to me! Thanks in advance. \\ THEOREM 6. Let and . Denote by the uniquely determined solution of the initial value problem (16) Then (17) is the solution of the problem Example 6. Find the solution of the differential equation (18) , which satisfies . Solution: The homogeneous equation is an Euler equation. The indicial equation is , with roots . The general solution of the homogeneous equation is thus We now find the fundamental solution. By (16), this must satisfy the conditions . We obtain the following system of equations for and : It follows that By theorem 6 is that solution of equation (18) which satisfies . (We have divided (18) by to make the coefficient of equal to 1 .) The general solution of is obtained by adding the general solution of the homogeneous equation, i.e., The conditions lead to , resulting in . Hence the required solution is","
u(\tau)=u^{\prime}(\tau)=\ldots=u^{(n-2)}(\tau)=0, \quad u^{(n-1)}(\tau)=1
 y\left(t_0\right)=y^{\prime}\left(t_0\right)=\ldots=y^{(n-1)}\left(t_0\right)=0
 y(1)=y'(1)=0 y(1)=2,y'(1)=5 L(t, \lambda)=\lambda^n+a_{n-1}(t) \lambda^{n-1}+\ldots+a_1(t) \lambda+a_0(t) \quad \quad D=\frac{d}{d t} E(t, \tau) u(t) \quad L(t, D) u=0 
u(\tau)=u^{\prime}(\tau)=\ldots=u^{(n-2)}(\tau)=0, \quad u^{(n-1)}(\tau)=1
 
y(t)=\int_{t_0}^t E(t, \tau) g(\tau) d \tau
 
\begin{aligned}
&L(t, D) y=g(t) \\
&y\left(t_0\right)=y^{\prime}\left(t_0\right)=\ldots=y^{(n-1)}\left(t_0\right)=0
\end{aligned}
 t^2 y^{\prime \prime}-2 t y^{\prime}+2 y=t^2 \sin t^4, \quad t>0 y(1)=2, y^{\prime}(1)=5 t^2 y^{\prime \prime}-2 t y^{\prime}+2 y=0 \lambda(\lambda-1)-2 \lambda+2=0 \lambda=1, \lambda=2 
y(t)=C t+D t^2 \quad(C, D \text { constants }) .
 y(\tau)=0, y^{\prime}(\tau)=1 C D 
\left\{\begin{array} { l } 
{ C \tau + D \tau ^ { 2 } = 0 } \\
{ C + 2 D \tau = 1 }
\end{array} \quad \Longleftrightarrow \quad \left\{\begin{array}{l}
C=-1 \\
D=1 / \tau
\end{array}\right.\right.
 
E(t, \tau)=-t+\frac{t^2}{\tau}, \quad t, \tau>0
 
\bar{y}(t)=\int_1^t\left(-t+\frac{t^2}{\tau}\right) \sin \tau^4 d \tau, \quad t>0
 y(1)=y^{\prime}(1)=0 t^2 y^{\prime \prime} (18) 
y(t)=\bar{y}(t)+C t+D t^2
 y(1)=2, y^{\prime}(1)=5 C+D=2, C+2 D=5 C=-1, D=3 
y(t)=\int_1^t\left(-t+\frac{t^2}{\tau}\right) \sin \tau^4 d \tau-t+3 t^2, \quad t>0
","['ordinary-differential-equations', 'systems-of-equations', 'initial-value-problems']"
42,Equation of motion given by $\ddot x = -\mbox{sgn}(x)$,Equation of motion given by,\ddot x = -\mbox{sgn}(x),I need to solve the following differential equation $$\ddot x = -\mbox{sgn}(x)$$ where $x \mapsto \mbox{sgn} (x)$ is the sign function. I can see that it would behave like constant acceleration but it also flips and oscillates when it crosses the origin.,I need to solve the following differential equation where is the sign function. I can see that it would behave like constant acceleration but it also flips and oscillates when it crosses the origin.,\ddot x = -\mbox{sgn}(x) x \mapsto \mbox{sgn} (x),['ordinary-differential-equations']
43,Trick of selecting fundamental matrix in nonhomogeneous differential equations to simplify calculations,Trick of selecting fundamental matrix in nonhomogeneous differential equations to simplify calculations,,"Problem: I came across the following differential equation set: $$ \begin{cases} 	\dot{x}&=3x-2y+t\\ 	\dot{y}&=4x-y+t^2\\ \end{cases} $$ My solution I followed normal practices and found the general solution to the homogeneous equation $$ \left[ \begin{array}{c} 	x\\ 	y\\ \end{array} \right] '=\left[ \begin{matrix} 	3&		-2\\ 	4&		-1\\ \end{matrix} \right] \cdot \left[ \begin{array}{c} 	x\\ 	y\\ \end{array} \right] \Rightarrow \left[ \begin{array}{c} 	x\\ 	y\\ \end{array} \right] =\left[ \begin{array}{c} 	e^t\left( c_1\cos 2t+c_2\sin 2t \right)\\ 	e^t\left( \left( c_1-c_2 \right) \cos 2t+\left( c_1+c_2 \right) \sin 2t \right)\\ \end{array} \right]  $$ I selected $(c_1, c_2)=(1,0)$ and $(0,1)$ to form a fundamental matrix $X=\left[ \begin{matrix} 	e^t\cos 2t&		e^t\sin 2t\\ 	e^t\left( \cos 2t+\sin 2t \right)&		e^t\left( \sin 2t-\cos 2t \right)\\ \end{matrix} \right] $ , and plugged it into the variation of constants formula to get a particular solution $y(t)=X(t)\int_{0}^{t}\left(X^{-1}(\tau)\cdot \left[ \begin{array}{c} 	\tau\\ 	\tau ^2\\ \end{array} \right]\right)\mathrm{d}\tau $ , and after a one-page long calculation and the help of Wolframalpha, I got the following solution of $y$ : Question I wonder what's wrong with my selection since the what wolfram gave me if I typed the original equation set would be like this: So I guess there must be something in my selection of the fundamental matrix that makes the calculations really complex, but how can I improve it? Also, what's your favourite way of solving this kind of nonhomogeneous linear differential equation sets?","Problem: I came across the following differential equation set: My solution I followed normal practices and found the general solution to the homogeneous equation I selected and to form a fundamental matrix , and plugged it into the variation of constants formula to get a particular solution , and after a one-page long calculation and the help of Wolframalpha, I got the following solution of : Question I wonder what's wrong with my selection since the what wolfram gave me if I typed the original equation set would be like this: So I guess there must be something in my selection of the fundamental matrix that makes the calculations really complex, but how can I improve it? Also, what's your favourite way of solving this kind of nonhomogeneous linear differential equation sets?","
\begin{cases}
	\dot{x}&=3x-2y+t\\
	\dot{y}&=4x-y+t^2\\
\end{cases}
 
\left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] '=\left[ \begin{matrix}
	3&		-2\\
	4&		-1\\
\end{matrix} \right] \cdot \left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] \Rightarrow \left[ \begin{array}{c}
	x\\
	y\\
\end{array} \right] =\left[ \begin{array}{c}
	e^t\left( c_1\cos 2t+c_2\sin 2t \right)\\
	e^t\left( \left( c_1-c_2 \right) \cos 2t+\left( c_1+c_2 \right) \sin 2t \right)\\
\end{array} \right] 
 (c_1, c_2)=(1,0) (0,1) X=\left[ \begin{matrix}
	e^t\cos 2t&		e^t\sin 2t\\
	e^t\left( \cos 2t+\sin 2t \right)&		e^t\left( \sin 2t-\cos 2t \right)\\
\end{matrix} \right]  y(t)=X(t)\int_{0}^{t}\left(X^{-1}(\tau)\cdot \left[ \begin{array}{c}
	\tau\\
	\tau ^2\\
\end{array} \right]\right)\mathrm{d}\tau  y","['linear-algebra', 'ordinary-differential-equations', 'computational-complexity']"
44,Find an explicit solution of this equation,Find an explicit solution of this equation,,"Let $\alpha \in (0,1)$ . Is it possible to solve  the equation $$\cos{x}=\frac{\alpha}{\pi}\int_{0}^{\infty}\frac{\cos{(xy)}}{y^{1-\alpha}(y^{2\alpha}+1)}dy\quad (1).$$ It is very easy to compute $$\int_{0}^{\infty}\frac{1}{y^{1-\alpha}(y^{2\alpha}+1)}dy=\frac{\pi}{2\alpha}.$$ Simply change variables $y^{\alpha}\rightarrow y$ . This shows that (1) has a solution, since the modulus of the right side is bounded by $\frac{1}{2}$ . In fact the equation has infinitely many solutions by the periodicity of the cosine function. But, I don't know how to solve it.","Let . Is it possible to solve  the equation It is very easy to compute Simply change variables . This shows that (1) has a solution, since the modulus of the right side is bounded by . In fact the equation has infinitely many solutions by the periodicity of the cosine function. But, I don't know how to solve it.","\alpha \in (0,1) \cos{x}=\frac{\alpha}{\pi}\int_{0}^{\infty}\frac{\cos{(xy)}}{y^{1-\alpha}(y^{2\alpha}+1)}dy\quad (1). \int_{0}^{\infty}\frac{1}{y^{1-\alpha}(y^{2\alpha}+1)}dy=\frac{\pi}{2\alpha}. y^{\alpha}\rightarrow y \frac{1}{2}","['real-analysis', 'integration', 'complex-analysis', 'ordinary-differential-equations', 'contour-integration']"
45,Solve differential equation $(x^2 + y^2 + x)dx + ydy = 0$ by making a variable substitution or by turning it into a total differential equation,Solve differential equation  by making a variable substitution or by turning it into a total differential equation,(x^2 + y^2 + x)dx + ydy = 0,"Solve differential equation $(x^2 + y^2 + x)dx + ydy = 0$ by making a variable substitution or by turning it into a total differential equation. I tried various substitutions. For example this equation is equivalent to $(x^2+y^2)dx + d(xy) = 0$ so I tried substituting with $(x,y) \to (x, u=xy)$ so the equation becomes $(x^2 + \dfrac{u^2}{x^2})dx + d(u) = 0$ . If I rewrite this as $u' + \dfrac{u^2}{x^2} = -x^2$ then it is a Ricatti differential equation, though I can't find any solution for this equation. For example, it seemed logical trying for $u = x^\alpha$ , but $\alpha x^{\alpha-1} + x^{2\alpha-2}=-x^2$ doesn't say much to me. Any suggestions?","Solve differential equation by making a variable substitution or by turning it into a total differential equation. I tried various substitutions. For example this equation is equivalent to so I tried substituting with so the equation becomes . If I rewrite this as then it is a Ricatti differential equation, though I can't find any solution for this equation. For example, it seemed logical trying for , but doesn't say much to me. Any suggestions?","(x^2 + y^2 + x)dx + ydy = 0 (x^2+y^2)dx + d(xy) = 0 (x,y) \to (x, u=xy) (x^2 + \dfrac{u^2}{x^2})dx + d(u) = 0 u' + \dfrac{u^2}{x^2} = -x^2 u = x^\alpha \alpha x^{\alpha-1} + x^{2\alpha-2}=-x^2",['ordinary-differential-equations']
46,Deriving a second-order system of differential equations that describes the motion of a planet (in cartesian coordinates),Deriving a second-order system of differential equations that describes the motion of a planet (in cartesian coordinates),,"I got a question thats probably very basic but I just cant figure it out. I want to derive a system of second order differential equations that describes the motion of a planet in (x,y)-coordinates (cartesian coordinates). Ive seen ways of doing this by switching to polar coordinates but I want to know how to derive it without switching to polar coordinates. I dont know where to start. If we have the following equations: $$ F = \frac{-GMm}{r^2} \text{ and } F = ma $$ We can combine these two and then write it as $$ a = \frac{-GM}{r^2} $$ Where $r = \sqrt{x^2+y^2}$ which will give us $$ a = -\frac{GM}{x^2+y^2} $$ But this doesnt seem right. If anyone could help me figure out how to derive this so that I get a system of second order differential equations I would be very grateful. Thanks!","I got a question thats probably very basic but I just cant figure it out. I want to derive a system of second order differential equations that describes the motion of a planet in (x,y)-coordinates (cartesian coordinates). Ive seen ways of doing this by switching to polar coordinates but I want to know how to derive it without switching to polar coordinates. I dont know where to start. If we have the following equations: We can combine these two and then write it as Where which will give us But this doesnt seem right. If anyone could help me figure out how to derive this so that I get a system of second order differential equations I would be very grateful. Thanks!","
F = \frac{-GMm}{r^2} \text{ and } F = ma
 
a = \frac{-GM}{r^2}
 r = \sqrt{x^2+y^2} 
a = -\frac{GM}{x^2+y^2}
","['ordinary-differential-equations', 'celestial-mechanics']"
47,Find solution set of $\mathbb{C}$-valued equation $y^{(4)}=2y^{(3)}-y''$,Find solution set of -valued equation,\mathbb{C} y^{(4)}=2y^{(3)}-y'',"Problem: $\text{Find the solution set of this } \mathbb{C}\text{-valued equation }$ $$y^{(4)}=2y^{(3)}-y''$$ $\text{So we have } y^4=2y^3-y^2 \implies y=1\lor y=0$ . As the solution said, we have $\phi_1(x)=1, \phi_2(x)=x, \phi_3(x)=e^x, \phi_4(x)=xe^x$ and they are linearly independent . $\mathscr{H}=\{c_1\phi_1+c_2\phi_2+c_3\phi_3+c_4\phi_4|c_1,c_2,c_3,c_4\in\mathbb{C}\}$ I have no idea where those $\phi$ come from, it'll be great, if some one could explain it. Thanks in advance!","Problem: . As the solution said, we have and they are linearly independent . I have no idea where those come from, it'll be great, if some one could explain it. Thanks in advance!","\text{Find the solution set of this } \mathbb{C}\text{-valued equation } y^{(4)}=2y^{(3)}-y'' \text{So we have } y^4=2y^3-y^2 \implies y=1\lor y=0 \phi_1(x)=1, \phi_2(x)=x, \phi_3(x)=e^x, \phi_4(x)=xe^x \mathscr{H}=\{c_1\phi_1+c_2\phi_2+c_3\phi_3+c_4\phi_4|c_1,c_2,c_3,c_4\in\mathbb{C}\} \phi",['ordinary-differential-equations']
48,"Solving the false ""theorem"" $\int\frac1{f(x)}dx=\frac1{\int f(x)dx}$","Solving the false ""theorem""",\int\frac1{f(x)}dx=\frac1{\int f(x)dx},"For fun, I am trying to solve the false ""theorem"" $$\int\frac1{f(x)}dx=\frac1{\int f(x)dx}$$ for all functions $f$ that satisfy the above equation. I got a solution but it seems to be wrong, and I was wondering where it went wrong. Here's how I did it: I first started by differentiating both sides, using Chain Rule on the RHS: $$\frac1{f(x)}=-\frac{f(x)}{(\int f(x)dx)^2}$$ Cross multiplying, I get: $$-(f(x))^2=\left(\int f(x)dx\right)^2$$ So: $$\int f(x)dx=\sqrt{-(f(x))^2}$$ Then, differentiating both sides again: $$f(x)=\frac12(-(f(x))^2)^{-1/2}\cdot(-2f(x))\cdot f'(x)$$ Which simplifies to (I switch from $f(x)$ to $y$ from here to make it easier to keep track of the variables): $$y=-(-y^2)^{-1/2}\cdot\frac{dy}{dx}$$ Separating the variables: $$dx=\frac{-(-y^2)^{-1/2}}ydy$$ At this point I'm not sure what else to do so I introduce the imaginary numbers into the equation (this is where it starts getting iffy for me, because I never mixed imaginary numbers with integration before): $$dx=\frac{-(-1)^{-1/2}\cdot(y^2)^{-1/2}}ydy$$ $$dx=\frac{-1/i}{y^2}dy$$ $$\int dx=-\frac1i\int\frac1{y^2}dy$$ $$x+C_1=-\frac1i\cdot\left(-\frac1y+C_2\right)$$ $$x+C_1=\frac1{yi}+C_2$$ $$x+C_3=\frac1{yi}$$ $$yi=\frac1{x+C_3}$$ $$\boxed{f(x)=\frac1{ix+C}}$$ where $C$ is an arbitrary complex number. But the problem with this solution is when I try to plug this back into the original equation, it doesn't seem to make the equation true. For example, let's say I choose $C=0$ , so that $f(x)=\frac1{ix}$ . Then the LHS becomes: $$\int\frac1{f(x)}dx=\int ixdx=i\frac{x^2}2+C_4$$ But the RHS becomes: $$\frac1{\int f(x)dx}=\frac1{\int\frac1{ix}dx}=\frac1{\frac1i\ln|x|+C_5}$$ which obviously don't match each other at all.","For fun, I am trying to solve the false ""theorem"" for all functions that satisfy the above equation. I got a solution but it seems to be wrong, and I was wondering where it went wrong. Here's how I did it: I first started by differentiating both sides, using Chain Rule on the RHS: Cross multiplying, I get: So: Then, differentiating both sides again: Which simplifies to (I switch from to from here to make it easier to keep track of the variables): Separating the variables: At this point I'm not sure what else to do so I introduce the imaginary numbers into the equation (this is where it starts getting iffy for me, because I never mixed imaginary numbers with integration before): where is an arbitrary complex number. But the problem with this solution is when I try to plug this back into the original equation, it doesn't seem to make the equation true. For example, let's say I choose , so that . Then the LHS becomes: But the RHS becomes: which obviously don't match each other at all.",\int\frac1{f(x)}dx=\frac1{\int f(x)dx} f \frac1{f(x)}=-\frac{f(x)}{(\int f(x)dx)^2} -(f(x))^2=\left(\int f(x)dx\right)^2 \int f(x)dx=\sqrt{-(f(x))^2} f(x)=\frac12(-(f(x))^2)^{-1/2}\cdot(-2f(x))\cdot f'(x) f(x) y y=-(-y^2)^{-1/2}\cdot\frac{dy}{dx} dx=\frac{-(-y^2)^{-1/2}}ydy dx=\frac{-(-1)^{-1/2}\cdot(y^2)^{-1/2}}ydy dx=\frac{-1/i}{y^2}dy \int dx=-\frac1i\int\frac1{y^2}dy x+C_1=-\frac1i\cdot\left(-\frac1y+C_2\right) x+C_1=\frac1{yi}+C_2 x+C_3=\frac1{yi} yi=\frac1{x+C_3} \boxed{f(x)=\frac1{ix+C}} C C=0 f(x)=\frac1{ix} \int\frac1{f(x)}dx=\int ixdx=i\frac{x^2}2+C_4 \frac1{\int f(x)dx}=\frac1{\int\frac1{ix}dx}=\frac1{\frac1i\ln|x|+C_5},"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'complex-numbers']"
49,Is it possible that $y'(x) = x^2 - y^2$ has a second solution?,Is it possible that  has a second solution?,y'(x) = x^2 - y^2,"While testing Physics-informed neural networks I discovered that IVP $$ y'(x) = x^2 - y^2 $$ $$ y(0) = 2.5 $$ possibly has a second solution. I'm quite sure that my model once found a very good approximation (I discarded it because I wasn't looking for it) of this ""new"" solution. But I cannot recreate it since, so I started wondering whether this IVP actually has a second solution. My best attempt so far at finding the second solution is shown on this graph as a blue curve. Curves on the graph are: Blue curve - best neural network solution Red curve - known solution found with a RK45 numerical method Green curve - actual derivative of NN solution Orange curve - value of $f(x, y) = x^2 - y^2$ computed from the NN solution Is it possible to check if that potential second solution actually exists or does not exist, without finding it? I know that this IVP is a form of Riccati equation and its known solution has a (not simple) analytical form . So if the potential second solution actually exists, I am wondering if it has maybe been already found or the existing analytical solution can be adapted to fit the potential ""second"" solution.","While testing Physics-informed neural networks I discovered that IVP possibly has a second solution. I'm quite sure that my model once found a very good approximation (I discarded it because I wasn't looking for it) of this ""new"" solution. But I cannot recreate it since, so I started wondering whether this IVP actually has a second solution. My best attempt so far at finding the second solution is shown on this graph as a blue curve. Curves on the graph are: Blue curve - best neural network solution Red curve - known solution found with a RK45 numerical method Green curve - actual derivative of NN solution Orange curve - value of computed from the NN solution Is it possible to check if that potential second solution actually exists or does not exist, without finding it? I know that this IVP is a form of Riccati equation and its known solution has a (not simple) analytical form . So if the potential second solution actually exists, I am wondering if it has maybe been already found or the existing analytical solution can be adapted to fit the potential ""second"" solution."," y'(x) = x^2 - y^2   y(0) = 2.5  f(x, y) = x^2 - y^2","['ordinary-differential-equations', 'numerical-methods', 'initial-value-problems']"
50,Getting derivative by dividing partial derivatives (in ODE linear stability analysis),Getting derivative by dividing partial derivatives (in ODE linear stability analysis),,"In short: I'm trying to understand how dividing two partial derivatives gives a ""non-partial"" derivative. Let me give the context, since there might be notational peculiarities specific to that domain. Given the following system of ordinary differential equations: $$\frac{du}{dt} = f(u,v)$$ $$\frac{dv}{dt} = g(u,v)$$ and let $(u_0, v_0)$ be a steady solution: $f(u_0, v_0) = g(u_0, v_0) = 0$ . Murray ( pp. 226-227 ) draws the picture of some possible reaction null clines for $f=0$ , $g=0$ at a steady state $(u_0, v_0)$ (see picture below) and states that at $(u_0, v_0)$ , the ""gradient on $g=0$ "" fulfils $$ \frac{dv}{du} \Bigg]_{g=0} = - \frac{\frac{\partial g}{\partial u}}{\frac{\partial g}{\partial v}} $$ . How does one get this identity? Why is there a negative sign?","In short: I'm trying to understand how dividing two partial derivatives gives a ""non-partial"" derivative. Let me give the context, since there might be notational peculiarities specific to that domain. Given the following system of ordinary differential equations: and let be a steady solution: . Murray ( pp. 226-227 ) draws the picture of some possible reaction null clines for , at a steady state (see picture below) and states that at , the ""gradient on "" fulfils . How does one get this identity? Why is there a negative sign?","\frac{du}{dt} = f(u,v) \frac{dv}{dt} = g(u,v) (u_0, v_0) f(u_0, v_0) = g(u_0, v_0) = 0 f=0 g=0 (u_0, v_0) (u_0, v_0) g=0  \frac{dv}{du} \Bigg]_{g=0} = - \frac{\frac{\partial g}{\partial u}}{\frac{\partial g}{\partial v}} ","['ordinary-differential-equations', 'partial-derivative', 'stability-in-odes', 'stability-theory']"
51,How to find a system of ODEs that give a specific function,How to find a system of ODEs that give a specific function,,"I have the given system of equations: \begin{equation} x^2 + y^2 - z = C_1 \\ x y z=C_2 \end{equation} and I want to find  a system of ODEs that yield the equation above as a solution. The ODEs must be in the form: \begin{equation} \frac{dx}{dt}=f_1(x,y,z)\\ \frac{dy}{dt}=f_2(x,y,z)\\ \frac{dz}{dt}=f_3(x,y,z)\\ \end{equation} First I would sum the two at the top to one and get $x^2+y^2-z+xyz=C_1+C_2$ Then it appears that the system of ODEs is really the gradient of the function f with respect to $t$ . So I would just integrate this with respect to t for each function and divide them all by three, so their sum is equal to the equation at the top and get: \begin{equation} \frac{dx}{dt}=x^2+xyz-C_1\\ \frac{dy}{dt}=y^2-C_2\\ \frac{dz}{dt}=-z \end{equation} Integrating I get: \begin{equation} x=t(x^2+xyz-C_1)\\ y=t(y^2-C_2)\\ z=-tz\\ \end{equation} But something tells me this is not right. Any ideas? Thanks","I have the given system of equations: and I want to find  a system of ODEs that yield the equation above as a solution. The ODEs must be in the form: First I would sum the two at the top to one and get Then it appears that the system of ODEs is really the gradient of the function f with respect to . So I would just integrate this with respect to t for each function and divide them all by three, so their sum is equal to the equation at the top and get: Integrating I get: But something tells me this is not right. Any ideas? Thanks","\begin{equation}
x^2 + y^2 - z = C_1 \\
x y z=C_2
\end{equation} \begin{equation}
\frac{dx}{dt}=f_1(x,y,z)\\
\frac{dy}{dt}=f_2(x,y,z)\\
\frac{dz}{dt}=f_3(x,y,z)\\
\end{equation} x^2+y^2-z+xyz=C_1+C_2 t \begin{equation}
\frac{dx}{dt}=x^2+xyz-C_1\\
\frac{dy}{dt}=y^2-C_2\\
\frac{dz}{dt}=-z
\end{equation} \begin{equation}
x=t(x^2+xyz-C_1)\\
y=t(y^2-C_2)\\
z=-tz\\
\end{equation}",['ordinary-differential-equations']
52,"Find all solutions to the equation $\frac{dx}{dt}=\sqrt[3]{x}, x(0)=0$",Find all solutions to the equation,"\frac{dx}{dt}=\sqrt[3]{x}, x(0)=0","Find all solutions to the equation $$\frac{dx}{dt}=\sqrt[3]{x}, x(0)=0$$ My solution for $x\neq 0$ : $$\frac{dx}{dt}=\sqrt[3]{x}$$ $$\frac{\frac{dx}{dt}}{\sqrt[3]{x}}dt=\int 1 dt$$ $$\frac{3}{2}x^{\frac 23} = t+C$$ $$x(0)=0 \Rightarrow C=0, \frac{3}{2}x^{\frac 23} = t$$ $$x^{\frac 23}=\frac 23 t$$ $$x=\pm\sqrt{\frac 8{27} t^3}$$ But I don't know what I can do with $x=0$ .",Find all solutions to the equation My solution for : But I don't know what I can do with .,"\frac{dx}{dt}=\sqrt[3]{x}, x(0)=0 x\neq 0 \frac{dx}{dt}=\sqrt[3]{x} \frac{\frac{dx}{dt}}{\sqrt[3]{x}}dt=\int 1 dt \frac{3}{2}x^{\frac 23} = t+C x(0)=0 \Rightarrow C=0, \frac{3}{2}x^{\frac 23} = t x^{\frac 23}=\frac 23 t x=\pm\sqrt{\frac 8{27} t^3} x=0",['ordinary-differential-equations']
53,"An ODE that is solved ""apparently"" correctly, but gives the wrong result","An ODE that is solved ""apparently"" correctly, but gives the wrong result",,"I wanted to solve \begin{equation} y''+\frac{1}{x}y'=0 \end{equation} and thought immediately about subsitution $y''=w'$ , \begin{equation} w'+\frac{1}{x}w=0\\ \frac{w'}{w}=-\frac{1}{x}\\ \ln|w|=\frac{1}{x^2} \\ w=e^{\frac{1}{x^2}}  \\ \text{make the back-substitution } w=y'\\ y'=Ce^{\frac{1}{x^2}} \\ y=Ce^{\frac{1}{x^2}}x-\sqrt{\pi}\operatorname{erf}\frac{1}{x}+D \end{equation} Although this procedure is correct, the answer is wrong. Still, if I  do substitution but use integrating factor, I also get the wrong answer: \begin{equation} w'+\frac{1}{x}w=0\\ \text{integrating factor}: e^{\int\frac{1}{x}dx} \\ \bigg(w e^{e^{-\frac{1}{r^2}}}\bigg)'=0\\ w e^{e^{-\frac{1}{r^2}}}=C\\\text{make the back-substitution } w''=y'\\ y'=Ce^{\frac{1}{x^2}}\\ y=Ce^{\frac{1}{x^2}}x-\sqrt{\pi}\operatorname{erf}\frac{1}{x}+D \end{equation} So two right procedures, and two wrong answers. Any hints ? Thanks","I wanted to solve and thought immediately about subsitution , Although this procedure is correct, the answer is wrong. Still, if I  do substitution but use integrating factor, I also get the wrong answer: So two right procedures, and two wrong answers. Any hints ? Thanks","\begin{equation}
y''+\frac{1}{x}y'=0
\end{equation} y''=w' \begin{equation}
w'+\frac{1}{x}w=0\\
\frac{w'}{w}=-\frac{1}{x}\\
\ln|w|=\frac{1}{x^2} \\
w=e^{\frac{1}{x^2}} 
\\ \text{make the back-substitution } w=y'\\
y'=Ce^{\frac{1}{x^2}} \\
y=Ce^{\frac{1}{x^2}}x-\sqrt{\pi}\operatorname{erf}\frac{1}{x}+D
\end{equation} \begin{equation}
w'+\frac{1}{x}w=0\\ \text{integrating factor}: e^{\int\frac{1}{x}dx} \\
\bigg(w e^{e^{-\frac{1}{r^2}}}\bigg)'=0\\
w e^{e^{-\frac{1}{r^2}}}=C\\\text{make the back-substitution } w''=y'\\
y'=Ce^{\frac{1}{x^2}}\\
y=Ce^{\frac{1}{x^2}}x-\sqrt{\pi}\operatorname{erf}\frac{1}{x}+D
\end{equation}","['ordinary-differential-equations', 'substitution']"
54,Proof of ODE solution,Proof of ODE solution,,It is known that if you have the following differential equation: $$y+P(x)y'=Q(x)$$ Then $y$ is $$\frac{1}{I(x)}\int I(x)Q(x)dx$$ Where $$I(x)=e^{\int P(x) dx}$$ is the integrating factor. So what is the proof of this solution (other than just substituting in for $y$ )? I have no idea where to start.,It is known that if you have the following differential equation: Then is Where is the integrating factor. So what is the proof of this solution (other than just substituting in for )? I have no idea where to start.,y+P(x)y'=Q(x) y \frac{1}{I(x)}\int I(x)Q(x)dx I(x)=e^{\int P(x) dx} y,"['calculus', 'ordinary-differential-equations', 'derivatives']"
55,solve $f'(t) = f(t) (a-bf(t))$ [duplicate],solve  [duplicate],f'(t) = f(t) (a-bf(t)),"This question already has an answer here : Transform a logistic equation into a linear ODE using substitution (1 answer) Closed 1 year ago . Solve $f'(t) = f(t) (a-bf(t)), f(t_0) = y_0$ , where $a,b, t_0\in\mathbb{R}$ and $f$ is a real-valued function taking real-values. Apparently the solution is $$f(t) = \dfrac{a y_0 e^{a(t-t_0)}}{by_0 e^{a(t-t_0)} + (a-y_0 b)},$$ but I don't know how to derive this. I know the standard approach for solving first order linear differential equations, but that's clearly not applicable in this case. I also tried doing something like multiplying both sides by some sort of integrating factor like $e^{-at}$ . Doing so yields $$(e^{-at}f(t))' = -e^{-at}b f(t)^2.$$ Therefore $$\dfrac{(e^{-at} f(t))'}{f(t)^2} = -e^{-at}b.$$ It doesn't seem like integrating both sides is useful in this case. Perhaps some sort of substitution might be useful?","This question already has an answer here : Transform a logistic equation into a linear ODE using substitution (1 answer) Closed 1 year ago . Solve , where and is a real-valued function taking real-values. Apparently the solution is but I don't know how to derive this. I know the standard approach for solving first order linear differential equations, but that's clearly not applicable in this case. I also tried doing something like multiplying both sides by some sort of integrating factor like . Doing so yields Therefore It doesn't seem like integrating both sides is useful in this case. Perhaps some sort of substitution might be useful?","f'(t) = f(t) (a-bf(t)), f(t_0) = y_0 a,b, t_0\in\mathbb{R} f f(t) = \dfrac{a y_0 e^{a(t-t_0)}}{by_0 e^{a(t-t_0)} + (a-y_0 b)}, e^{-at} (e^{-at}f(t))' = -e^{-at}b f(t)^2. \dfrac{(e^{-at} f(t))'}{f(t)^2} = -e^{-at}b.","['calculus', 'ordinary-differential-equations']"
56,show that the ODE $(\sqrt x + \sqrt y)\sqrt y dx = xdy$ has no solution $y(x)$ such that $\lim_{x\to \infty}\frac{y(x)}{x}=L$ when $ L>0 \in \Bbb R$,show that the ODE  has no solution  such that  when,(\sqrt x + \sqrt y)\sqrt y dx = xdy y(x) \lim_{x\to \infty}\frac{y(x)}{x}=L  L>0 \in \Bbb R,I need to show that the ODE $(\sqrt x + \sqrt y)\sqrt y dx = xdy$ has no solution $y(x)$ such that $\lim_{x\to \infty}\frac{y(x)}{x}=L$ when $ L>0 \in \Bbb R$ I solved it by finding the general solution $y(x) = \frac{1}{4} (4 c_1^2 x + 4 c_1 x \log(x) + x \log^2(x))$ and then its easy to see that $\lim_{x\to \infty}\frac{y(x)}{x}=\infty$ and we finished. I'm asking if there is a more elegant proof to this problem instead of the straight forward solution,I need to show that the ODE has no solution such that when I solved it by finding the general solution and then its easy to see that and we finished. I'm asking if there is a more elegant proof to this problem instead of the straight forward solution,(\sqrt x + \sqrt y)\sqrt y dx = xdy y(x) \lim_{x\to \infty}\frac{y(x)}{x}=L  L>0 \in \Bbb R y(x) = \frac{1}{4} (4 c_1^2 x + 4 c_1 x \log(x) + x \log^2(x)) \lim_{x\to \infty}\frac{y(x)}{x}=\infty,"['calculus', 'ordinary-differential-equations']"
57,Solving $y'' + 2y' + 2y = 2\delta' + 2\delta$ without Laplace transform,Solving  without Laplace transform,y'' + 2y' + 2y = 2\delta' + 2\delta,Im trying to solve the following differential equation: $$y'' + 2y' + 2y = 2\delta' + 2\delta$$ I did this by first setting $ y(t) = z(t)\theta(t)$ and finding the causal solution to the problem. From this i got the following solution: $$y(t) = 2e^{-t}\cos(t)\theta(t) $$ However this is only the solution to the homogenous equation and when it comes to finding a particular solution im stuck. I'm somewhat aware that it can be solved with Laplace transform but since i haven't studied it yet i can't use it.,Im trying to solve the following differential equation: I did this by first setting and finding the causal solution to the problem. From this i got the following solution: However this is only the solution to the homogenous equation and when it comes to finding a particular solution im stuck. I'm somewhat aware that it can be solved with Laplace transform but since i haven't studied it yet i can't use it.,y'' + 2y' + 2y = 2\delta' + 2\delta  y(t) = z(t)\theta(t) y(t) = 2e^{-t}\cos(t)\theta(t) ,"['ordinary-differential-equations', 'dirac-delta', 'step-function']"
58,"Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$?",Does there exists two differentiable functions  on  such that  on  and  on ?,"f, g I W(f, g) (x) >0 A W(f, g) <0 I\setminus A","Let $I=(0, 1) $ and $A=\mathcal{C}\cap (0, 1) $ where $\mathcal{C}$ denote Cantor set. $\color{red}{Question}$ : Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$ ? Where $W(f,g)(x) =\begin{vmatrix}f(x) &g(x) \\f'(x)&g'(x)\end{vmatrix}$ denote the Wronskian of $f, g$ at $x\in I$ Previous Question : Does there exists two functions $f, g\in C^1(I)$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$? Let me summarize $W(f, g) (x) \neq 0$ for some $x\in I$ implies $\{f, g\}$ linearly independent. If two functions are solutions of a differential equation $y""+p(x) y'+q(x) y=0$ on $I$ where $p, q\in C(I) $ then by Abel's identity we have $$W(f, g) (x) =W(f, g) (x_o) e^{-\int_{x_0}^{x} p(t) dt}$$ Then $W(f, g) (x_0) \neq 0$ for some $x_0\in I$ implies $W(f, g) \neq 0$ on $I$ Moreover $W(f,g)$ different from zero with the same sign at every point ${\displaystyle x} \in {\displaystyle I}$ If $f, g \in C^1(I) $ then $w(x) =W(f, g) (x) =f(x) g'(x) -f'(x) g(x) $ is a continuous map on $I$ . Then $S=\{x\in I : W(f, g) (x) >0\}$ is an open set. Further if $W(f, g) $ attains both positive and negative values then by Darboux property $W(f, g) (x) =0$ for some $x\in I$ . Hence if $f, g$ are solution of the same differential equation and $f, g$ has continuous derivative. Then $ W(f, g) (x) =0$ on $I$ . Hence to get two functions $f, g$ with the mentioned properties, we need to consider the following : $f, g$ can't be the solution of  a Linear Homogenous Differential Equation. $f, g$ can't be continuously differentiable on $I$ . $f, g$ can't be linearly dependent on $I$ . Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$ ?","Let and where denote Cantor set. : Does there exists two differentiable functions on such that on and on ? Where denote the Wronskian of at Previous Question : Does there exists two functions $f, g\in C^1(I)$ for which $W(f, g) (x) >0$ for some $x$ and $W(f, g) (x) <0$ for some $x$? Let me summarize for some implies linearly independent. If two functions are solutions of a differential equation on where then by Abel's identity we have Then for some implies on Moreover different from zero with the same sign at every point If then is a continuous map on . Then is an open set. Further if attains both positive and negative values then by Darboux property for some . Hence if are solution of the same differential equation and has continuous derivative. Then on . Hence to get two functions with the mentioned properties, we need to consider the following : can't be the solution of  a Linear Homogenous Differential Equation. can't be continuously differentiable on . can't be linearly dependent on . Does there exists two differentiable functions on such that on and on ?","I=(0, 1)  A=\mathcal{C}\cap (0, 1)  \mathcal{C} \color{red}{Question} f, g I W(f, g) (x) >0 A W(f, g) <0 I\setminus A W(f,g)(x) =\begin{vmatrix}f(x) &g(x) \\f'(x)&g'(x)\end{vmatrix} f, g x\in I W(f, g) (x) \neq 0 x\in I \{f, g\} y""+p(x) y'+q(x) y=0 I p, q\in C(I)  W(f, g) (x) =W(f, g) (x_o) e^{-\int_{x_0}^{x} p(t) dt} W(f, g) (x_0) \neq 0 x_0\in I W(f, g) \neq 0 I W(f,g) {\displaystyle x} \in {\displaystyle I} f, g \in C^1(I)  w(x) =W(f, g) (x) =f(x) g'(x) -f'(x) g(x)  I S=\{x\in I : W(f, g) (x) >0\} W(f, g)  W(f, g) (x) =0 x\in I f, g f, g  W(f, g) (x) =0 I f, g f, g f, g I f, g I f, g I W(f, g) (x) >0 A W(f, g) <0 I\setminus A","['ordinary-differential-equations', 'analysis', 'determinant', 'applications', 'wronskian']"
59,Confusion about Finite Differences Method Notation,Confusion about Finite Differences Method Notation,,"I would need a confirmation that I am understanding one of the steps in the lecture notes about numerical methods right. We have an ODE with $y\in C^1(t)$ : \begin{align} y'(t) &= f(t, y(t))\\ y(t_0) &= y_0 \end{align} and we choose the integration interval $I = [t_0, T]$ , which will be divided with the step length $h = (T-t_0)/N_h$ , with $N_h$ being the number of the steps. $u_j$ is the approximation of the exact solution at $t_j$ , the exact solution is denoted by $y_j := y(t_j)$ . Also let $f_j$ be defined as $f_j = f(t_j, u_j)$ and $u_0 = y_0$ . Then the text goes on as: 'When using a simple one step method we can use finite difference method: \begin{align} y'(t_n) \approx \frac{y(t_{n+1})-y(t_n)}{h} \end{align} and with using $y'(t_n) = f_n$ we get the Euler's forward method: $u_{n+1} = u_n + hf_n$ .' The problem I am having stems from the fact that we choose the notation $y_n$ for the exact solutions and I am confused as far as how I can set $y(t_n) = f_n$ since $f_n$ is a function of the approximated solutions. It is clear to me, that when I put the values of $u_n$ into the finite difference method, I get the desired form of the Euler's formula, but is it how it is meant (and the finite difference method is just a general formula with a little unfortunate notation)?","I would need a confirmation that I am understanding one of the steps in the lecture notes about numerical methods right. We have an ODE with : and we choose the integration interval , which will be divided with the step length , with being the number of the steps. is the approximation of the exact solution at , the exact solution is denoted by . Also let be defined as and . Then the text goes on as: 'When using a simple one step method we can use finite difference method: and with using we get the Euler's forward method: .' The problem I am having stems from the fact that we choose the notation for the exact solutions and I am confused as far as how I can set since is a function of the approximated solutions. It is clear to me, that when I put the values of into the finite difference method, I get the desired form of the Euler's formula, but is it how it is meant (and the finite difference method is just a general formula with a little unfortunate notation)?","y\in C^1(t) \begin{align}
y'(t) &= f(t, y(t))\\
y(t_0) &= y_0
\end{align} I = [t_0, T] h = (T-t_0)/N_h N_h u_j t_j y_j := y(t_j) f_j f_j = f(t_j, u_j) u_0 = y_0 \begin{align}
y'(t_n) \approx \frac{y(t_{n+1})-y(t_n)}{h}
\end{align} y'(t_n) = f_n u_{n+1} = u_n + hf_n y_n y(t_n) = f_n f_n u_n","['ordinary-differential-equations', 'numerical-methods', 'finite-differences', 'cauchy-problem', 'eulers-method']"
60,How to find the radius of convergence of series solution of an ODE?,How to find the radius of convergence of series solution of an ODE?,,"Recently, I watched a video https://www.youtube.com/watch?v=8lHAMZWDQHI which taught me how to find the radius of convergence of power series solution of an ODE. For example, if we want to find the radius of convergence of power series solution $\displaystyle y=\sum \limits _{i=0}^\infty c_n(x-1)^n$ of ODE $(x^2+2x+5)y''+xy'+y=0$ , just finding the roots of $x^2+2x+5$ then computing the minimum distance between $1$ and one of the roots. Generally, we have an ODE as follows: $$y^{(n)}+p_{n-1}(x)y^{(n-1)}+\cdots +p_1(x)y'+p_0(x)y=0,$$ where $p_i(x)$ is analytic in the complex plane except for some poles, and $\rho _1,\rho _2,\ldots ,\rho _k$ are regular singular points. Assume $\displaystyle y(x)=\sum c_kx^n$ is a nonzero formal power series solution. Is it true that the radius of convergence of $y(x)$ equals $\displaystyle d=\min \limits _{1\leq i\leq k}\rho _i$ ? Is it possible that there exists $|z_0|>d$ making $y(z_0)$ converge? Thanks very much for your precious help.","Recently, I watched a video https://www.youtube.com/watch?v=8lHAMZWDQHI which taught me how to find the radius of convergence of power series solution of an ODE. For example, if we want to find the radius of convergence of power series solution of ODE , just finding the roots of then computing the minimum distance between and one of the roots. Generally, we have an ODE as follows: where is analytic in the complex plane except for some poles, and are regular singular points. Assume is a nonzero formal power series solution. Is it true that the radius of convergence of equals ? Is it possible that there exists making converge? Thanks very much for your precious help.","\displaystyle y=\sum \limits _{i=0}^\infty c_n(x-1)^n (x^2+2x+5)y''+xy'+y=0 x^2+2x+5 1 y^{(n)}+p_{n-1}(x)y^{(n-1)}+\cdots +p_1(x)y'+p_0(x)y=0, p_i(x) \rho _1,\rho _2,\ldots ,\rho _k \displaystyle y(x)=\sum c_kx^n y(x) \displaystyle d=\min \limits _{1\leq i\leq k}\rho _i |z_0|>d y(z_0)","['complex-analysis', 'ordinary-differential-equations', 'power-series']"
61,Stability considerations for a special type of LTV systems,Stability considerations for a special type of LTV systems,,"I have been considering the stability of an (nonautonomous) LTV system $\dot{x} = A(t)x$ of the form $\begin{bmatrix}\dot{x}_{1}\\ \dot{x}_{2} \end{bmatrix} = \begin{bmatrix}-a(t) & -b(t)\\ 1 & 0 \end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}$ , where we know that $a(t), b(t) > 0$ and both $a(t), b(t)$ are both continuous, differentiable, and bounded. By simulating such a system, it clearly seems to be stable, but I do not quite see how to prove that this is true and which restrictions that apply. By computing the eigenvalues we get $\lambda_{1,2}(t) = \frac{-a(t) \pm \sqrt{a(t)^{2} - 4b(t)}}{2} < 0\; \forall\; t $ , with the restrictions we placed on $a(t), b(t)$ . However, as emphasized by, e.g., Khalil (Nonlinear Systems (2002), Section 4.6), the eigenvalues of $A$ cannot be used to assert stability of such a system. If we instead fix $a, b$ , then, by the same eigenvalues, the (autonomous) LTI system $\dot{x} = Ax$ is stable as long as $a, b > 0$ . I know that asserting the stability of general LTV systems is difficult, and from what I can see, most of the literature refer to numerical integration to find the transition matrix of the system. But is there really no other way to discuss the system shown above? The most ""obvious"" alternative that comes to mind is to find an appropriate Lyapunov function candidate, but I do not seem to be able to come up with a good candidate (?)","I have been considering the stability of an (nonautonomous) LTV system of the form , where we know that and both are both continuous, differentiable, and bounded. By simulating such a system, it clearly seems to be stable, but I do not quite see how to prove that this is true and which restrictions that apply. By computing the eigenvalues we get , with the restrictions we placed on . However, as emphasized by, e.g., Khalil (Nonlinear Systems (2002), Section 4.6), the eigenvalues of cannot be used to assert stability of such a system. If we instead fix , then, by the same eigenvalues, the (autonomous) LTI system is stable as long as . I know that asserting the stability of general LTV systems is difficult, and from what I can see, most of the literature refer to numerical integration to find the transition matrix of the system. But is there really no other way to discuss the system shown above? The most ""obvious"" alternative that comes to mind is to find an appropriate Lyapunov function candidate, but I do not seem to be able to come up with a good candidate (?)","\dot{x} = A(t)x \begin{bmatrix}\dot{x}_{1}\\ \dot{x}_{2} \end{bmatrix} = \begin{bmatrix}-a(t) & -b(t)\\ 1 & 0 \end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} a(t), b(t) > 0 a(t), b(t) \lambda_{1,2}(t) = \frac{-a(t) \pm \sqrt{a(t)^{2} - 4b(t)}}{2} < 0\; \forall\; t  a(t), b(t) A a, b \dot{x} = Ax a, b > 0","['linear-algebra', 'ordinary-differential-equations', 'control-theory', 'linear-control']"
62,"Periodic solutions of $x'' = Ax$, with matrix $A$. [closed]","Periodic solutions of , with matrix . [closed]",x'' = Ax A,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $x'' = Ax$ . Let $A = \begin{pmatrix} 1 & 1\\ 0 & \epsilon \end{pmatrix}$ . Find $\epsilon$ such that it has periodic solution. I don't know what are the conditions that the matrix has to meet?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question . Let . Find such that it has periodic solution. I don't know what are the conditions that the matrix has to meet?","x'' = Ax A = \begin{pmatrix}
1 & 1\\
0 & \epsilon
\end{pmatrix} \epsilon","['matrices', 'ordinary-differential-equations']"
63,Triple bar meaning in proof of the Principle of Superposition,Triple bar meaning in proof of the Principle of Superposition,,"What does the triple bar mean in this context? ""Thus $y(t)0,...$ ""","What does the triple bar mean in this context? ""Thus ""","y(t)0,...","['ordinary-differential-equations', 'proof-explanation']"
64,Can someone help verify the first part of my solution for this DDE?,Can someone help verify the first part of my solution for this DDE?,,"I'm trying to solve this delayed differential equation that I saw in a paper I'm studying $$y(t)=ay(t)ay(t\tau)$$ with $a$ as a positive constant, $y(t\tau)=0$ for $t<\tau$ and the initial condition as $y(0)=y_0$ . (I posted about this a couple of days ago and was making a terrible mistake so I deleted that one. I'm doing it again now, but I compare my solution to the one that someone else got and it's slightly different, even though I think my working is correct mostly? I can't see their working so I'm not sure where/if I'm going wrong, so I just wanted to confirm if I'm making a mistake or not.) I'll outline my working below: On $[0,\tau]$ we have $$y(t)ay(t)=0$$ which gives us $$y(t)=y_0e^{at}$$ if we use an integrating factor of $e^{at}$ . For $[\tau,2\tau]$ : $$y(t)ay(t)=ay_0e^{a(t-\tau)}$$ which gives me $$y(t) = y_0e^{at}[1-ae^{-a\tau}(t-\tau)]$$ Then, for $t \in [2\tau, 3\tau]$ , I can just continue using the method of steps, but I get a super long expression that does not,,, particularly seem like it would be very generalizable, so I'm not sure if I can get a closed form solution or not (also why I think I might be making a mistake). (Edit: Adding it here anyway) For $t \in [2\tau, 3\tau]$ : $$y(t) = y_0 e^{at}\bigg[1-at+2a\tau + ae^{-a\tau}\bigg(a\bigg(\frac{t^2}{2} - 2\tau\bigg) + \tau - 2\tau^2\bigg) \bigg]$$ Could someone please confirm if I got the first bits that I've written up there correct though? I'd appreciate it a lot, as I do tend to make many many basic errors sometimes. EDIT 2: Got the answer by continuing to use the method of steps and changing how I wrote my solution for $t \in [2\tau, 3\tau]$ to $$y(t) = y_0\bigg[e^{at}-a(t-\tau)e^{a(t-\tau)} + \frac{a^2}{2}(t-2\tau)^2e^{a(t-2\tau)}\bigg]$$ instead of what it was before, which gave me a much nicer way to look at it. Then I generalized this to pretty much the exact thing user DinosaurEgg wrote below except I realized it after looking at the answers fully once I was done lol. But yeah, so I get the sum $$y(t) = y_0 \sum_{k=0}^{n} \frac{(-a)^k}{k!} (t-k\tau)^ke^{a(t-k\tau)}$$ which is again, the exact same as the answer below. Thanks for all the help though! I do appreciate it! :)","I'm trying to solve this delayed differential equation that I saw in a paper I'm studying with as a positive constant, for and the initial condition as . (I posted about this a couple of days ago and was making a terrible mistake so I deleted that one. I'm doing it again now, but I compare my solution to the one that someone else got and it's slightly different, even though I think my working is correct mostly? I can't see their working so I'm not sure where/if I'm going wrong, so I just wanted to confirm if I'm making a mistake or not.) I'll outline my working below: On we have which gives us if we use an integrating factor of . For : which gives me Then, for , I can just continue using the method of steps, but I get a super long expression that does not,,, particularly seem like it would be very generalizable, so I'm not sure if I can get a closed form solution or not (also why I think I might be making a mistake). (Edit: Adding it here anyway) For : Could someone please confirm if I got the first bits that I've written up there correct though? I'd appreciate it a lot, as I do tend to make many many basic errors sometimes. EDIT 2: Got the answer by continuing to use the method of steps and changing how I wrote my solution for to instead of what it was before, which gave me a much nicer way to look at it. Then I generalized this to pretty much the exact thing user DinosaurEgg wrote below except I realized it after looking at the answers fully once I was done lol. But yeah, so I get the sum which is again, the exact same as the answer below. Thanks for all the help though! I do appreciate it! :)","y(t)=ay(t)ay(t\tau) a y(t\tau)=0 t<\tau y(0)=y_0 [0,\tau] y(t)ay(t)=0 y(t)=y_0e^{at} e^{at} [\tau,2\tau] y(t)ay(t)=ay_0e^{a(t-\tau)} y(t) = y_0e^{at}[1-ae^{-a\tau}(t-\tau)] t \in [2\tau, 3\tau] t \in [2\tau, 3\tau] y(t) = y_0 e^{at}\bigg[1-at+2a\tau +
ae^{-a\tau}\bigg(a\bigg(\frac{t^2}{2} - 2\tau\bigg) + \tau - 2\tau^2\bigg) \bigg] t \in [2\tau, 3\tau] y(t) = y_0\bigg[e^{at}-a(t-\tau)e^{a(t-\tau)} + \frac{a^2}{2}(t-2\tau)^2e^{a(t-2\tau)}\bigg] y(t) = y_0 \sum_{k=0}^{n} \frac{(-a)^k}{k!} (t-k\tau)^ke^{a(t-k\tau)}","['ordinary-differential-equations', 'delay-differential-equations']"
65,First Order Differential Equation -Variation of Parameters Method,First Order Differential Equation -Variation of Parameters Method,,"I'm trying to solve arbitrary current equation for neuron membrane from Gerstner's book. Here it begins: Question: Assuming that before a given time t0 the current is null and the membrane potential is at rest, derive the general solution to Eq. (1) for arbitrary I(t). My Solution: Variation of Parameters method is used to solve differential equation: $$\tau\frac{du}{dt} = \ - (u(t) - u_{\text{rest}}) + RI(t)\quad \tag1 $$ $$\tau\frac{du}{dt} +u(t) =0 \implies \tau\frac{du}{dt}=-u(t)\implies \int \frac{du}{u}=-\int\frac{dt}{\tau}$$ $$u(t)= e^{-t/{\tau}}\cdot C \implies u(t)=C(t)\cdot e^{-t/{\tau}}$$ $$ u^{'}(t)=C^{'}(t)\cdot {e^{-t/{\tau}}-\frac{1}{\tau}e^{-t/\tau}}\cdot C(t) $$ Inserting into  Eq. $(1)$ : $$ \tau\left(C^{'}(t)\cdot {e^{-t/{\tau}} - \frac{1}{\tau}e^{-t/\tau}}\cdot C(t)\right)= -C(t)e^{-t/\tau} +u_{\text{rest}}+RI(t) $$ $$ \tau\cdot C^{'}(t)\cdot e^{-t/{\tau}} =u_{\text{rest}}+RI(t) $$ $$ \int C^{'}(t)=\int \frac{1}{\tau} \left(u_{\text{rest}}+RI(t)\right)\cdot e^{t/{\tau}}dt \implies C(t)= C_{1}+ \frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt $$ $$ u(t)= e^{-t/{\tau}}\left(C_{1} +\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt \right) $$ In solution, it says before a given time $t_{0}$ the current is zero and the $ut_{0} = u_{\text{rest}}$ . With this information I can't apply it to find $C_{1}$ . In some resources, they say applying initial condition results that integration part of equation equals to zero. So I tried that: $$ u(t)= e^{-t/{\tau}}\left(C_{1} +\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt \right) $$ Initial condition $ut_{0} = u_{0}$ $$u_{0} =e^{-t0/{\tau}}(C_{1} + \text{ (they specified integration equals to zero)) } \implies C_{1}=u_{0}\cdot e^{t0/{\tau}}   $$ $$u(t)=e^{(t0-t)/{\tau}}\cdot u_{0} + \frac{e^{-t/{\tau}}}{\tau}\int_{t_{0}}^t(u_{\text{rest}}+RI(t))e^{s/{\tau}}ds $$ I couldn't proceed from that point. Book's Solution: $$\tau\frac{du}{dt} +u(t) =0 \iff \frac{du/dt}{u}=-\frac{1}{\tau} \implies u(t)=ke^{-t/{\tau}} $$ A particular solution can be obtained by the ""variation of parameters"" method: we write $u(t)=k(t)e^{-t/{\tau}}$ and replace it in Eq. (1). $$\tau\left(\frac{dk(t)}{dt}-\frac{1}{\tau}k(t)\right)e^{-t/{\tau}} + k(t)e^{-t/{\tau}} =u_{\text{rest}}+RI(t)  $$ $$\frac{dk(t)}{dt}=\frac{1}{\tau}(u_{\text{rest}}+RI(t))e^{t/{\tau}} $$ Integrating, we find $k(t) = k_{2}+\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(s))e^{s/{\tau}}ds $ where k2 is a new integration constant. Denoting the initial condition by $u_{0} = u_{to}$ , we obtain: $$u(t) = u_{\text{rest}} + (u_{0}-u_{\text{rest}})\cdot e^{-(t-t0)/{\tau}} +\frac{1}{\tau}\int_{t_{0}}^t RI(s)e^{-(t-s)/{\tau}}ds $$ Using the particular initial condition $u(t_{0}) = u_{\text{rest}}$ , the equation simplifies to: $$ \bbox[yellow]{ u(t) = u_{\text{rest}} + \frac{1}{\tau}\int_{t_{0}}^t RI(s)e^{-(t-s)/{\tau}}ds \implies \text{The real solution}}  $$ As you can realize, my solution is wrong. How can I achieve the correct solution. Also some sources say, these equations can be solved by laplas and convolution methods. I searched this, but this method is used solve second degree diff equations. If someone wish to solve by laplas, I will be appreciated. Pls try to help me, I am very desperate right now. Best regards.","I'm trying to solve arbitrary current equation for neuron membrane from Gerstner's book. Here it begins: Question: Assuming that before a given time t0 the current is null and the membrane potential is at rest, derive the general solution to Eq. (1) for arbitrary I(t). My Solution: Variation of Parameters method is used to solve differential equation: Inserting into  Eq. : In solution, it says before a given time the current is zero and the . With this information I can't apply it to find . In some resources, they say applying initial condition results that integration part of equation equals to zero. So I tried that: Initial condition I couldn't proceed from that point. Book's Solution: A particular solution can be obtained by the ""variation of parameters"" method: we write and replace it in Eq. (1). Integrating, we find where k2 is a new integration constant. Denoting the initial condition by , we obtain: Using the particular initial condition , the equation simplifies to: As you can realize, my solution is wrong. How can I achieve the correct solution. Also some sources say, these equations can be solved by laplas and convolution methods. I searched this, but this method is used solve second degree diff equations. If someone wish to solve by laplas, I will be appreciated. Pls try to help me, I am very desperate right now. Best regards.",\tau\frac{du}{dt} = \ - (u(t) - u_{\text{rest}}) + RI(t)\quad \tag1  \tau\frac{du}{dt} +u(t) =0 \implies \tau\frac{du}{dt}=-u(t)\implies \int \frac{du}{u}=-\int\frac{dt}{\tau} u(t)= e^{-t/{\tau}}\cdot C \implies u(t)=C(t)\cdot e^{-t/{\tau}}  u^{'}(t)=C^{'}(t)\cdot {e^{-t/{\tau}}-\frac{1}{\tau}e^{-t/\tau}}\cdot C(t)  (1)  \tau\left(C^{'}(t)\cdot {e^{-t/{\tau}} - \frac{1}{\tau}e^{-t/\tau}}\cdot C(t)\right)= -C(t)e^{-t/\tau} +u_{\text{rest}}+RI(t)   \tau\cdot C^{'}(t)\cdot e^{-t/{\tau}} =u_{\text{rest}}+RI(t)   \int C^{'}(t)=\int \frac{1}{\tau} \left(u_{\text{rest}}+RI(t)\right)\cdot e^{t/{\tau}}dt \implies C(t)= C_{1}+ \frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt   u(t)= e^{-t/{\tau}}\left(C_{1} +\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt \right)  t_{0} ut_{0} = u_{\text{rest}} C_{1}  u(t)= e^{-t/{\tau}}\left(C_{1} +\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(t))e^{t/{\tau}}dt \right)  ut_{0} = u_{0} u_{0} =e^{-t0/{\tau}}(C_{1} + \text{ (they specified integration equals to zero)) } \implies C_{1}=u_{0}\cdot e^{t0/{\tau}}    u(t)=e^{(t0-t)/{\tau}}\cdot u_{0} + \frac{e^{-t/{\tau}}}{\tau}\int_{t_{0}}^t(u_{\text{rest}}+RI(t))e^{s/{\tau}}ds  \tau\frac{du}{dt} +u(t) =0 \iff \frac{du/dt}{u}=-\frac{1}{\tau} \implies u(t)=ke^{-t/{\tau}}  u(t)=k(t)e^{-t/{\tau}} \tau\left(\frac{dk(t)}{dt}-\frac{1}{\tau}k(t)\right)e^{-t/{\tau}} + k(t)e^{-t/{\tau}} =u_{\text{rest}}+RI(t)   \frac{dk(t)}{dt}=\frac{1}{\tau}(u_{\text{rest}}+RI(t))e^{t/{\tau}}  k(t) = k_{2}+\frac{1}{\tau}\int_{t_{0}}^t (u_{\text{rest}}+RI(s))e^{s/{\tau}}ds  u_{0} = u_{to} u(t) = u_{\text{rest}} + (u_{0}-u_{\text{rest}})\cdot e^{-(t-t0)/{\tau}} +\frac{1}{\tau}\int_{t_{0}}^t RI(s)e^{-(t-s)/{\tau}}ds  u(t_{0}) = u_{\text{rest}}  \bbox[yellow]{ u(t) = u_{\text{rest}} + \frac{1}{\tau}\int_{t_{0}}^t RI(s)e^{-(t-s)/{\tau}}ds \implies \text{The real solution}}  ,"['ordinary-differential-equations', 'laplace-transform']"
66,Adaptive step size for Euler Method - How to create?,Adaptive step size for Euler Method - How to create?,,"I think Euler's Method is a great way to simulate ODE:s. It's not the most accurate, but it's the fastest and simplest. Euler's Method is usually used with fixed step size, where $k$ is the step size larger than $0$ and $\dot x = f(x,u)$ is our ODE function. To simulate forward Euler, just iterate this equation: $$x_{i+1} = x_i + k f(x_i, u)$$ To improve stability for Euler's method, then the step size $k$ needs to be adaptive. The smaller choice of the step $k$ , the more stability is guaranteed, but the less accurate the simulation will become. Instead, the adaptive method for the step size is often used to find the best $k$ . Question: I can I improve stability for Euler's method by implementing adaptive step size?","I think Euler's Method is a great way to simulate ODE:s. It's not the most accurate, but it's the fastest and simplest. Euler's Method is usually used with fixed step size, where is the step size larger than and is our ODE function. To simulate forward Euler, just iterate this equation: To improve stability for Euler's method, then the step size needs to be adaptive. The smaller choice of the step , the more stability is guaranteed, but the less accurate the simulation will become. Instead, the adaptive method for the step size is often used to find the best . Question: I can I improve stability for Euler's method by implementing adaptive step size?","k 0 \dot x = f(x,u) x_{i+1} = x_i + k f(x_i, u) k k k","['ordinary-differential-equations', 'numerical-methods', 'nonlinear-dynamics', 'eulers-method']"
67,"When $A$ is skew-symmetric, $Q = e^{A t}$ is orthogonal, i.e. $Q^T Q = I$.","When  is skew-symmetric,  is orthogonal, i.e. .",A Q = e^{A t} Q^T Q = I,"When $A$ is skew-symmetric, then $Q = e^{A t}$ is orthogonal or $Q^T Q = I$ . (For a linear autonomous system $\dot{x} = A x$ , the fundamental matrix of the system is $\Phi(t) = e^{A t}$ .) To prove this identity, I make use of the result $$ \left( e^{P} \right)^T = e^{P^T} \tag{1} $$ for any matrix $P$ . Since $e^P$ can be expressed as the uniformly convergent series, $$ e^P = \sum\limits_{k = 0}^\infty \ {P^k \over k!}, $$ it follows that $$ \left( e^P \right)^T = \left( \sum\limits_{k = 0}^\infty \ {P^k \over k!} \right)^T = \sum\limits_{k = 0}^\infty \ {(P^k)^T \over k!}  = \sum\limits_{k = 0}^\infty \ {(P^T)^k \over k!} = e^{P^T} $$ Assume that $A$ is skew-symmetric. Then $$ A^T = -A $$ Define $$ Q = e^{A t} $$ Then we have $$ Q^T = e^{( A t)^T} = e^{A^T t} = e^{- A t} = \left( e^{A t} \right)^{-1} = Q^{-1} $$ Thus, it follows that $$ Q^T Q = Q^{-1} Q = I $$ showing that $Q$ is orthogonal. I hope that this proof is OK!","When is skew-symmetric, then is orthogonal or . (For a linear autonomous system , the fundamental matrix of the system is .) To prove this identity, I make use of the result for any matrix . Since can be expressed as the uniformly convergent series, it follows that Assume that is skew-symmetric. Then Define Then we have Thus, it follows that showing that is orthogonal. I hope that this proof is OK!","A Q = e^{A t} Q^T Q = I \dot{x} = A x \Phi(t) = e^{A t} 
\left( e^{P} \right)^T = e^{P^T} \tag{1}
 P e^P 
e^P = \sum\limits_{k = 0}^\infty \ {P^k \over k!},
 
\left( e^P \right)^T = \left( \sum\limits_{k = 0}^\infty \ {P^k \over k!} \right)^T
= \sum\limits_{k = 0}^\infty \ {(P^k)^T \over k!}
 = \sum\limits_{k = 0}^\infty \ {(P^T)^k \over k!} = e^{P^T}
 A 
A^T = -A
 
Q = e^{A t}
 
Q^T = e^{( A t)^T} = e^{A^T t} = e^{- A t} = \left( e^{A t} \right)^{-1} = Q^{-1}
 
Q^T Q = Q^{-1} Q = I
 Q","['linear-algebra', 'ordinary-differential-equations']"
68,Solving simple ODE,Solving simple ODE,,"I'm trying to solve this ODE: $$y'(x)=\cos^2(y) \\ y(1)=$$ $$\int \dfrac{1}{\cos^2(y)} \, dy =\int \, dx $$ $\tan(y) = x +C.$ And $y(1)=,$ so $\tan()=1+CC=-1.$ Now if I solve for $y(x),$ $y(x)=\arctan(\tan(y))=\arctan(x-1)$ , which is wrong because $y(1)=.$ What do I miss when I solve for $y(x)$ in the last step? The correct answer is $y(x)=\arctan(x-1)+.$","I'm trying to solve this ODE: And so Now if I solve for , which is wrong because What do I miss when I solve for in the last step? The correct answer is","y'(x)=\cos^2(y) \\ y(1)= \int \dfrac{1}{\cos^2(y)} \, dy =\int \, dx  \tan(y) = x +C. y(1)=, \tan()=1+CC=-1. y(x), y(x)=\arctan(\tan(y))=\arctan(x-1) y(1)=. y(x) y(x)=\arctan(x-1)+.","['calculus', 'ordinary-differential-equations']"
69,Lyapunov function to prove globally asymptotically stable,Lyapunov function to prove globally asymptotically stable,,"I have the system $x'=-x^3+2y^3$ and $y'=-2xy^2$ . I need to prove that the point $(0,0)$ is asymptotically globally stable. Here's what I did: if we have a Lyapunov function $v(x,y)=ax^2+bxy+cy^2$ , then \begin{align} \dot{v}(x,y)&=(2ax+by)(-x^3+2y^3)+(2cy+bx)(-2xy^2)\\ &= 2a(-x^4+2xy^3)+b(-yx^3+2y^4-2x^2y^2)+2c(-2xy^3) \end{align} Now, I can see that if we let $a=c=1$ and $b=0$ , then $v(x,y)=x^2+y^2$ is positive definite and $\dot{v}(x,y)=-2x^4$ is negative semidefinite. However, this shows $(0,0)$ is stable and not asymptotically stable. How do I prove this stronger result? I know it's true because my textbook says the origin is ""at least"" stable. The only idea I had was to write $\dot{v}(x,y)=-(x+y)^4+...$ but this hasn't gotten me very far.","I have the system and . I need to prove that the point is asymptotically globally stable. Here's what I did: if we have a Lyapunov function , then Now, I can see that if we let and , then is positive definite and is negative semidefinite. However, this shows is stable and not asymptotically stable. How do I prove this stronger result? I know it's true because my textbook says the origin is ""at least"" stable. The only idea I had was to write but this hasn't gotten me very far.","x'=-x^3+2y^3 y'=-2xy^2 (0,0) v(x,y)=ax^2+bxy+cy^2 \begin{align}
\dot{v}(x,y)&=(2ax+by)(-x^3+2y^3)+(2cy+bx)(-2xy^2)\\
&= 2a(-x^4+2xy^3)+b(-yx^3+2y^4-2x^2y^2)+2c(-2xy^3)
\end{align} a=c=1 b=0 v(x,y)=x^2+y^2 \dot{v}(x,y)=-2x^4 (0,0) \dot{v}(x,y)=-(x+y)^4+...","['ordinary-differential-equations', 'stability-theory', 'lyapunov-functions']"
70,"Why is $y'=\frac{x^2}{y^2}$ a Bernoulli equation and not a linear one, but $y'=-\frac{2y}{x}$ is Linear and not Bernoulli?","Why is  a Bernoulli equation and not a linear one, but  is Linear and not Bernoulli?",y'=\frac{x^2}{y^2} y'=-\frac{2y}{x},"Im going through this book on differential equations: Differential Equations (Schaum's Outlines) 4th Edition In chapter 3, Supplementary Problems I have to determine whether an equation is homogeneous and/or linear, and if not linear, whether it is a Bernoulli equation. Definition of a Linear equation: $y' + p(x)y=q(x)$ Definition of a Bernoulli equation: $y' + p(x)y=q(x)y^n$ I understand why: $y'=\frac{x^2}{y^2}$ is a Bernoulli equation. It cannot be a Linear one because when we rearrange everything we get: $y' + 0 = x^2 \cdot y^{-2}$ So we get: $ n=-2 \\ p(x)=0 \\ q(x)=x^2 $ Because we have two $y$ 's (although with a - sign in their powers, it doesn't matter), so we cannot formulate a Linear equation. But here: $y'=-\frac{2y}{x}$ is not a Bernoulli equation but a Linear one. I can see why it is a Linear equation, if we rearrange things we get: $y' + 2\frac{1}{x}\cdot y = 0$ So we get: $ p(x)=2\frac{1}{x} \\ q(x)=0 $ But why cant it also be a Bernoulli equation with $q(x)=0$ ?","Im going through this book on differential equations: Differential Equations (Schaum's Outlines) 4th Edition In chapter 3, Supplementary Problems I have to determine whether an equation is homogeneous and/or linear, and if not linear, whether it is a Bernoulli equation. Definition of a Linear equation: Definition of a Bernoulli equation: I understand why: is a Bernoulli equation. It cannot be a Linear one because when we rearrange everything we get: So we get: Because we have two 's (although with a - sign in their powers, it doesn't matter), so we cannot formulate a Linear equation. But here: is not a Bernoulli equation but a Linear one. I can see why it is a Linear equation, if we rearrange things we get: So we get: But why cant it also be a Bernoulli equation with ?","y' + p(x)y=q(x) y' + p(x)y=q(x)y^n y'=\frac{x^2}{y^2} y' + 0 = x^2 \cdot y^{-2} 
n=-2 \\
p(x)=0 \\
q(x)=x^2
 y y'=-\frac{2y}{x} y' + 2\frac{1}{x}\cdot y = 0 
p(x)=2\frac{1}{x} \\
q(x)=0
 q(x)=0",['ordinary-differential-equations']
71,Solving the ODE $\frac{dy}{dx}=-\frac{2x+3y+y^2}{x+2xy}$,Solving the ODE,\frac{dy}{dx}=-\frac{2x+3y+y^2}{x+2xy},"In my ODE course, I've stumbled upon the following ODE whose RHS is a rational function. $$\frac{dy}{dx}=-\frac{2x+3y+y^2}{x+2xy}.$$ It seems that the question is insinuating that we can solve using elementary tools. The first thing I thought is seeing if we get an exact form. Letting $N=x+2xy$ and $M=-(2x+3y+y^2)$ we have that $$M_y=-3-2y=-2-(1+2y)=-2-N_x$$ So the form $Mdx+Ndy$ is not exact. If I want to make it exact by multiplying by an integrating factor $\mu$ , then I must solve the PDE $$\mu_yM+\mu M_y=\mu_xN+\mu N_x=\mu_xN+\mu(-M_y-2)$$ But it doesn't seem I can get much from this. Am I missing some obvious trick? Only hints, no full answers please! I realized I made a mistake with the problem in fact the equation is $$\frac{dy}{dx}=-\frac{\color{red}{3}x+\color{red}{2}y+y^2}{x+2xy}.$$ This means that $$M_y=-2-2y=-2(1+y)=-2N_x.$$ But still that doesn't help. In any case, I will be interested in an answer to any of these equations.","In my ODE course, I've stumbled upon the following ODE whose RHS is a rational function. It seems that the question is insinuating that we can solve using elementary tools. The first thing I thought is seeing if we get an exact form. Letting and we have that So the form is not exact. If I want to make it exact by multiplying by an integrating factor , then I must solve the PDE But it doesn't seem I can get much from this. Am I missing some obvious trick? Only hints, no full answers please! I realized I made a mistake with the problem in fact the equation is This means that But still that doesn't help. In any case, I will be interested in an answer to any of these equations.",\frac{dy}{dx}=-\frac{2x+3y+y^2}{x+2xy}. N=x+2xy M=-(2x+3y+y^2) M_y=-3-2y=-2-(1+2y)=-2-N_x Mdx+Ndy \mu \mu_yM+\mu M_y=\mu_xN+\mu N_x=\mu_xN+\mu(-M_y-2) \frac{dy}{dx}=-\frac{\color{red}{3}x+\color{red}{2}y+y^2}{x+2xy}. M_y=-2-2y=-2(1+y)=-2N_x.,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
72,Maximum error bound for ODE,Maximum error bound for ODE,,"For $f\in C([0,T]\mathbb{R})$ , let $|f(t,x)-f(t,y)|\leq L|x-y|$ for a $L\geq0$ . Also suppose we have a differential equation \begin{align} y'(t) &= f(t,y(t))\\ y(0)&=y_0 \end{align} with $y_1,\dots, y_N$ the solutions given by the Forward Euler method. So far I found that a bound for the maximum error is given by $$\max_{0\leq n \leq N} e_n \leq \frac{M}{2L}(e^{LT}-1)h$$ where $M=\max_{0\leq t \leq T}|y''(\tau)|$ for a $\tau\in[0,T]$ , and $h=T/N$ the step size. My question is: how do I find this bound for the following initial value problem: \begin{align} y'(t)&=\arctan(y(t))\\ y(0)&=y_0? \end{align} I am stuck at finding $M$ and $L$ . How could I find the maximum of the double derivative of $y(t)$ if I do not know anything about the function?","For , let for a . Also suppose we have a differential equation with the solutions given by the Forward Euler method. So far I found that a bound for the maximum error is given by where for a , and the step size. My question is: how do I find this bound for the following initial value problem: I am stuck at finding and . How could I find the maximum of the double derivative of if I do not know anything about the function?","f\in C([0,T]\mathbb{R}) |f(t,x)-f(t,y)|\leq L|x-y| L\geq0 \begin{align}
y'(t) &= f(t,y(t))\\
y(0)&=y_0
\end{align} y_1,\dots, y_N \max_{0\leq n \leq N} e_n \leq \frac{M}{2L}(e^{LT}-1)h M=\max_{0\leq t \leq T}|y''(\tau)| \tau\in[0,T] h=T/N \begin{align}
y'(t)&=\arctan(y(t))\\
y(0)&=y_0?
\end{align} M L y(t)","['ordinary-differential-equations', 'numerical-methods', 'error-function']"
73,pure Poisson birth process ordinary differential equations,pure Poisson birth process ordinary differential equations,,"Consider the pure Poisson process defined by \begin{align} P_n'(t) &= -\lambda_n P_n(t) + \lambda_{n-1}P_{n-1}, \quad n \geq 1,\\ P'_0(t) &= -\lambda_0 P_0(t). \end{align} with $P_0(0) = 1$ . Let $\lambda_n > 0$ for all $n$ , prove that for every fixed $n \geq 1$ , the function $P_n(t)$ first increases, then decreases to $0$ . If $t_n$ is the place of the maximum, then $t_1 < t_2< t_3<\dots$ The hint suggests to use induction and differentiate these set of equations, but I'm not sure how that gives arise to the answer.","Consider the pure Poisson process defined by with . Let for all , prove that for every fixed , the function first increases, then decreases to . If is the place of the maximum, then The hint suggests to use induction and differentiate these set of equations, but I'm not sure how that gives arise to the answer.","\begin{align}
P_n'(t) &= -\lambda_n P_n(t) + \lambda_{n-1}P_{n-1}, \quad n \geq 1,\\
P'_0(t) &= -\lambda_0 P_0(t).
\end{align} P_0(0) = 1 \lambda_n > 0 n n \geq 1 P_n(t) 0 t_n t_1 < t_2< t_3<\dots","['probability', 'ordinary-differential-equations', 'poisson-process']"
74,Convergence of a system of ODE's (replicator dynamics),Convergence of a system of ODE's (replicator dynamics),,"I have to find the points of convergence (i.e. $\lim_{t\to\infty} v^k(t)$ ) of the following replicator dynamics, given by a system of $4$ linear ODE's: $$\frac{\dot{v}^k(t)}{v^k(t)}=\alpha[(Av)^k-v^TAv]\quad if\quad v^k(t)>0$$ $$\dot{v}^k(t)=0\qquad if\quad v^k(t)=0$$ given an initial $v^k(0)>0$ for $k=1,2,3,4$ . For what matters $A$ is the following matrix: $$ A=\begin{bmatrix} 4 & 5 & 4 & 3\\ 3 & 4 & 5 & 4\\ 2 & 3 & 4 & 5\\ 1 & 2 & 3 & 4 \end{bmatrix} $$ I have never studied dynamical systems, and I only know how to study those of the very simple form: $$\dot{x}=Ax$$ via the exponential matrix of $A$ . So I am clueless. Any help would be precious, also, I would be very interested in an handy reference about this kind of problems.","I have to find the points of convergence (i.e. ) of the following replicator dynamics, given by a system of linear ODE's: given an initial for . For what matters is the following matrix: I have never studied dynamical systems, and I only know how to study those of the very simple form: via the exponential matrix of . So I am clueless. Any help would be precious, also, I would be very interested in an handy reference about this kind of problems.","\lim_{t\to\infty} v^k(t) 4 \frac{\dot{v}^k(t)}{v^k(t)}=\alpha[(Av)^k-v^TAv]\quad if\quad v^k(t)>0 \dot{v}^k(t)=0\qquad if\quad v^k(t)=0 v^k(0)>0 k=1,2,3,4 A 
A=\begin{bmatrix}
4 & 5 & 4 & 3\\
3 & 4 & 5 & 4\\
2 & 3 & 4 & 5\\
1 & 2 & 3 & 4
\end{bmatrix}
 \dot{x}=Ax A","['real-analysis', 'ordinary-differential-equations', 'reference-request', 'dynamical-systems']"
75,Drag exerted on condensing raindrop,Drag exerted on condensing raindrop,,"Newton's second law of motion $F = ma$ = $m{dv\over dt}$ can be written in the form $F = {d\over dt}(mv)$ in terms of the momentum $mv$ of a particle of mass $m$ and velocity $v$ , and remains valid in this form even if $m$ is not constant, as assumed so far. Suppose a spherical raindrop falls through air saturated with water vapor, and assume that by condensation the mass of the raindrop increases at a rate proportional to its surface area, with $c$ the constant of proportionality. If the initial radius and velocity of the raindrop are both zero, show that the drag exerted by the condensation of the water vapor has the effect of making the raindrop fall with acceleration ${1\over 4}g$ . Hint: Show that ${d\over dr}(r^3v) = ({\delta \over c})r^3g$ , where $r$ is the radius of the raindrop and $\delta$ is its density. This is a question from a math textbook from the chapter integration and differential equations My approach Given that, ${dm\over dt} \propto 4\pi r^2$ ${dm\over dt} = c\cdot 4\pi r^2\cdot \delta \cdot{dr\over dt}$ since mass = density  volume $m = {4\over 3}\pi r^3\cdot\delta$ I can't write the equation for $v$ . Also I am totally stucked at this step. Plus I can't reach the hint given in question. Any kind of help or suggestion or worked out steps would be appreciated.","Newton's second law of motion = can be written in the form in terms of the momentum of a particle of mass and velocity , and remains valid in this form even if is not constant, as assumed so far. Suppose a spherical raindrop falls through air saturated with water vapor, and assume that by condensation the mass of the raindrop increases at a rate proportional to its surface area, with the constant of proportionality. If the initial radius and velocity of the raindrop are both zero, show that the drag exerted by the condensation of the water vapor has the effect of making the raindrop fall with acceleration . Hint: Show that , where is the radius of the raindrop and is its density. This is a question from a math textbook from the chapter integration and differential equations My approach Given that, since mass = density  volume I can't write the equation for . Also I am totally stucked at this step. Plus I can't reach the hint given in question. Any kind of help or suggestion or worked out steps would be appreciated.",F = ma m{dv\over dt} F = {d\over dt}(mv) mv m v m c {1\over 4}g {d\over dr}(r^3v) = ({\delta \over c})r^3g r \delta {dm\over dt} \propto 4\pi r^2 {dm\over dt} = c\cdot 4\pi r^2\cdot \delta \cdot{dr\over dt} m = {4\over 3}\pi r^3\cdot\delta v,"['calculus', 'integration', 'ordinary-differential-equations', 'physics']"
76,Help needed solving logistic differential equation with initial conditions [duplicate],Help needed solving logistic differential equation with initial conditions [duplicate],,"This question already has answers here : How do you solve the Initial value probelm $dp/dt = 10p(1-p),    p(0)=0.1$? (3 answers) Logistic equation involving population (1 answer) Closed 2 years ago . Given information: Solve $$\frac{dP}{dt} = P(10 - 2P)$$ with initial conditions of $P(0) = 1$ . I have separated the variables, integrated, and simplified so I am now at $\ln(p) - \ln(p-5) = 10t + c$ I need help determining firstly, if thats correct thus far, and secondly, where to go from here as everything else I have tried has not worked.","This question already has answers here : How do you solve the Initial value probelm $dp/dt = 10p(1-p),    p(0)=0.1$? (3 answers) Logistic equation involving population (1 answer) Closed 2 years ago . Given information: Solve with initial conditions of . I have separated the variables, integrated, and simplified so I am now at I need help determining firstly, if thats correct thus far, and secondly, where to go from here as everything else I have tried has not worked.",\frac{dP}{dt} = P(10 - 2P) P(0) = 1 \ln(p) - \ln(p-5) = 10t + c,['ordinary-differential-equations']
77,Finding a constant such that the following integral inequality holds,Finding a constant such that the following integral inequality holds,,"I am tasked to find a constant $c>0$ such that for all $C^1$ functions in $(0, 1)$ this variational problem is true: $$cu(0)^2 \leq \int^1_0 u'^2 + u^2 dt$$ My instinct told me to calculate the minimizer of that funcitonal, and I got that it must be of the form $c_1e^t + c_2e^{-t} = \bar{u}$ , where $\bar{u}$ is the minimizer. Then we can compute the minimum quite easily in the general case and get that $c_1(e^{2} - 1)$ is a  lower bound for the functional. But all this allows me to state is that: $$c_1(e^{2} - 1) \leq \int^1_0 u'^2 + u^2 dt$$ I am not sure ow to introduce the $u(0)$ term.","I am tasked to find a constant such that for all functions in this variational problem is true: My instinct told me to calculate the minimizer of that funcitonal, and I got that it must be of the form , where is the minimizer. Then we can compute the minimum quite easily in the general case and get that is a  lower bound for the functional. But all this allows me to state is that: I am not sure ow to introduce the term.","c>0 C^1 (0, 1) cu(0)^2 \leq \int^1_0 u'^2 + u^2 dt c_1e^t + c_2e^{-t} = \bar{u} \bar{u} c_1(e^{2} - 1) c_1(e^{2} - 1) \leq \int^1_0 u'^2 + u^2 dt u(0)","['calculus', 'integration', 'ordinary-differential-equations', 'optimization', 'calculus-of-variations']"
78,Why should I only pick the positive number for $C$ in this IVP (separable differential equation)?,Why should I only pick the positive number for  in this IVP (separable differential equation)?,C,"I'm practicing some problems for an upcoming DEs test. I tried the following initial value problem: $\displaystyle\frac{1}{2}\displaystyle\frac{dy}{dx}=\sqrt{y+1}\cos x,y(\pi)=0$ Here's my work: $\begin{align} \displaystyle\frac{1}{2}\displaystyle\frac{dy}{dx}&=\sqrt{y+1}\cos x \\ \int\displaystyle\frac{dy}{\sqrt{y+1}}&=\int 2\cos x\,dx \\ 2\sqrt{y+1}&=2\sin x+c_1 \\ \sqrt{y+1}&=\sin x + C \\ y+1&=(\sin x + C)^2 \end{align}$ Plugging in the initial value I get: $\begin{align} 0+1&=(\sin\pi+C)^2\\ C^2&=1 \\ C&=\pm 1 \end{align}$ So the final answer I got was $y=(\sin x \pm 1)^2-1$ , but the solution in the textbook was $y=(\sin x +1)^2-1$ . Why did they only pick $+1$ as an answer for $C$ ?","I'm practicing some problems for an upcoming DEs test. I tried the following initial value problem: Here's my work: Plugging in the initial value I get: So the final answer I got was , but the solution in the textbook was . Why did they only pick as an answer for ?","\displaystyle\frac{1}{2}\displaystyle\frac{dy}{dx}=\sqrt{y+1}\cos x,y(\pi)=0 \begin{align}
\displaystyle\frac{1}{2}\displaystyle\frac{dy}{dx}&=\sqrt{y+1}\cos x \\
\int\displaystyle\frac{dy}{\sqrt{y+1}}&=\int 2\cos x\,dx \\
2\sqrt{y+1}&=2\sin x+c_1 \\
\sqrt{y+1}&=\sin x + C \\
y+1&=(\sin x + C)^2
\end{align} \begin{align}
0+1&=(\sin\pi+C)^2\\
C^2&=1 \\
C&=\pm 1
\end{align} y=(\sin x \pm 1)^2-1 y=(\sin x +1)^2-1 +1 C","['ordinary-differential-equations', 'initial-value-problems']"
79,Solve linear system of ODE's,Solve linear system of ODE's,,"It's been years since I formally saw ODE's and I am quite rusty, I don;t remember how to solve linear ODE's. I have a problem and managed to derive the following system: \begin{cases} 	2\ddot{q_1} = 2q_1 + q_2\\ 	2\ddot{q_2} = q_1 \end{cases} where $q_1$ and $q_2$ are scalar functions with parameter $t$ . It seems to me the solution is a trig function but I am not sure how to go about this problem.","It's been years since I formally saw ODE's and I am quite rusty, I don;t remember how to solve linear ODE's. I have a problem and managed to derive the following system: where and are scalar functions with parameter . It seems to me the solution is a trig function but I am not sure how to go about this problem.","\begin{cases}
	2\ddot{q_1} = 2q_1 + q_2\\
	2\ddot{q_2} = q_1
\end{cases} q_1 q_2 t","['calculus', 'ordinary-differential-equations', 'systems-of-equations']"
80,$(1-x^2)y''-xy'+\lambda y=0$ -SturmLiouville,-SturmLiouville,(1-x^2)y''-xy'+\lambda y=0,"I try to get this equation in SturmLiouville and I get stuck. $(1-x^2)y''-xy'+\lambda y=0$ My solution: $\mu(1-x^2)y''-\mu xy'+\mu \lambda y=(py')'+(\lambda r-q)y$ $\mu(1-x^2)y''-\mu xy'+\mu \lambda y=p'y'+py''+(\lambda r-q)y$ $p'=\mu x, p=\mu(1-x^2) \implies \frac{p'}{p}=\frac{x}{1-x^2}\implies ln(p)=-\frac{1}{2}ln|1-x^2|\implies p=\frac{1}{\sqrt{(|1-x^2|)}}$ Then $\mu=\frac{1}{(1-x^2)^3}$ $\frac{1}{(1-x^2)^2}y''-\frac{1}{(1-x^2)^3}xy'+\frac{1}{(1-x^2)^3}\lambda y=p'y'+py''+(\lambda r-q)y$ Here I get stuck ,please help Thanks !","I try to get this equation in SturmLiouville and I get stuck. My solution: Then Here I get stuck ,please help Thanks !","(1-x^2)y''-xy'+\lambda y=0 \mu(1-x^2)y''-\mu xy'+\mu \lambda y=(py')'+(\lambda r-q)y \mu(1-x^2)y''-\mu xy'+\mu \lambda y=p'y'+py''+(\lambda r-q)y p'=\mu x, p=\mu(1-x^2) \implies \frac{p'}{p}=\frac{x}{1-x^2}\implies ln(p)=-\frac{1}{2}ln|1-x^2|\implies p=\frac{1}{\sqrt{(|1-x^2|)}} \mu=\frac{1}{(1-x^2)^3} \frac{1}{(1-x^2)^2}y''-\frac{1}{(1-x^2)^3}xy'+\frac{1}{(1-x^2)^3}\lambda y=p'y'+py''+(\lambda r-q)y",['ordinary-differential-equations']
81,Solving the ODE $\;\;y'^2-yy'+e^x=0$,Solving the ODE,\;\;y'^2-yy'+e^x=0,"Solve, $$y'^2-yy'+e^x=0$$ By seeing it as a quadratic equation in $y'$ , we have $y'=\dfrac{y\pm\sqrt{y^2-4e^x}}{2}$ . But from here I don't know how to continue since both $y^2$ and $e^x$ are under square root. Another approach I've tried is using $y'=\frac1{x'}$ , $$\frac1{x'^2}-\frac{y}{x'}+e^x=0\;\Rightarrow\; e^x x'^2-y\;x'+1=0$$ I'm not sure if it helps.","Solve, By seeing it as a quadratic equation in , we have . But from here I don't know how to continue since both and are under square root. Another approach I've tried is using , I'm not sure if it helps.",y'^2-yy'+e^x=0 y' y'=\dfrac{y\pm\sqrt{y^2-4e^x}}{2} y^2 e^x y'=\frac1{x'} \frac1{x'^2}-\frac{y}{x'}+e^x=0\;\Rightarrow\; e^x x'^2-y\;x'+1=0,['ordinary-differential-equations']
82,"Second Order Differential Equations $ay''+by'+cy=0$, without complex numbers","Second Order Differential Equations , without complex numbers",ay''+by'+cy=0,"How to solve the following equation without using Second Order Differential Equations formulas or Power Series: $$af^{''}(x)+bf^{'}(x)+cf(x)=0$$ where $b^2-4ac<0$ . I know that the soltution is something like this: $$f(x)=A\sin (qx)+B\cos (qx)$$ In any event, I would like to know how to solve this equation without that well-known formula that we find in any Differential Equations Course. I would like an elementary solution to this equation and without complex numbers. For instance, if $b^2-4ac>0$ then we can solve it by using something like that: $$(e^{kx}g(x))^{'}=0$$ . Can't we do something similar as well? The second question I want to ask is if I want to solve the equation with Power Series is it correct the following approach: I prove by induction that $f$ is infinitely derivable and after that I assume: $$f(x)=\sum_{n=0}^\infty a_n x^n$$ I know that not all infinitely derivable can be written in this form, for example: $e^{-\frac{1}{x^2}}$ . So it shoul be wrong. Nonetheless, there are several people who provide solutions to this problem by using this fact which for me seems to be blatantly erroneous.","How to solve the following equation without using Second Order Differential Equations formulas or Power Series: where . I know that the soltution is something like this: In any event, I would like to know how to solve this equation without that well-known formula that we find in any Differential Equations Course. I would like an elementary solution to this equation and without complex numbers. For instance, if then we can solve it by using something like that: . Can't we do something similar as well? The second question I want to ask is if I want to solve the equation with Power Series is it correct the following approach: I prove by induction that is infinitely derivable and after that I assume: I know that not all infinitely derivable can be written in this form, for example: . So it shoul be wrong. Nonetheless, there are several people who provide solutions to this problem by using this fact which for me seems to be blatantly erroneous.",af^{''}(x)+bf^{'}(x)+cf(x)=0 b^2-4ac<0 f(x)=A\sin (qx)+B\cos (qx) b^2-4ac>0 (e^{kx}g(x))^{'}=0 f f(x)=\sum_{n=0}^\infty a_n x^n e^{-\frac{1}{x^2}},"['real-analysis', 'ordinary-differential-equations']"
83,Behaviour of the solutions of a system of differential equations at infinity.,Behaviour of the solutions of a system of differential equations at infinity.,,"Consider the following system of differential equations: $$ \begin{cases} u'=-u+uv\\ v'=-2v-u^2 \end{cases} $$ I'm able to prove that solutions must tend to $0$ if $t\to 0$ by the use of Lyapunov function $L(x,y)=x^2+y^2$ but I'm unable to prove that the function: $$ I(t)=\frac{x^2(t)+2y^2(t)}{x^2(t)+y^2(t)} $$ must admit limit for $t\to+\infty$ Any hint for a solution? Thanks in advance.",Consider the following system of differential equations: I'm able to prove that solutions must tend to if by the use of Lyapunov function but I'm unable to prove that the function: must admit limit for Any hint for a solution? Thanks in advance.,"
\begin{cases}
u'=-u+uv\\
v'=-2v-u^2
\end{cases}
 0 t\to 0 L(x,y)=x^2+y^2 
I(t)=\frac{x^2(t)+2y^2(t)}{x^2(t)+y^2(t)}
 t\to+\infty","['real-analysis', 'ordinary-differential-equations', 'systems-of-equations', 'lyapunov-functions']"
84,Understanding where does the second (stochastic) attractor of the system come from.,Understanding where does the second (stochastic) attractor of the system come from.,,"I am currently reading a paper , studying the population dynamics in 3-dimensional Lotka-Volterra model with the following interaction change descriptive system: $$ \begin{equation}     \begin{cases}         \dot x = ax - 0.06x^2-\Large\frac{xy}{x+10}\normalsize,\\         \dot y = -y + \Large\frac{2xy}{x+10}\normalsize-\Large\frac{0.405\cdot yz}{y+10}\normalsize,\\         \dot z = 0.038z^2 -\Large\frac{z^2}{y+20}\normalsize.\\     \end{cases} \end{equation}\tag{1} $$ Where $a$ is the variable parameter, for different values of which the model's behavior is studied. In the paper, authors claim, that the respective non-degenerate (biologically meaningfull if $a > 0.9158$ ) equlibria for $(1)$ is $M_3(\bar x_3, \bar y_3, \bar z_3)$ , where $\bar x_3 = \frac{a}{0.12}-5+\frac{\sqrt{(a+0.6)^2-1.52}}{0.12},~\bar y_3 = 6.31579,~\bar z_3 = -40.3 + 80.6\frac{\bar x_3}{\bar x_3 + 10}.$ Now, the problem is that further in the paper, authors consider a case, when $(1)$ posesses coexisting limit-1 cycle (red) and two-band chaotic attractor (blue). But the case is, I understnad what the red curve is, that is just basically the set of solutions of the system $(1)$ , plotted not against the time variable, but against each other $(x(t),y(t),z(t))$ . I even managed to get the ""red"" curve by myself, by solving $(1)$ for $a=1.803$ in python, using scipy.integrate.odeint : But then, I completely fail to understand, where does the ""blue"" attractor come from, and there is no any further information in the paper  about where it comes from as well, rather the blue attractor is just called ""attractor of a Feigenbaum's tree"". If anyone would be so kind to explain me, where the ""blue"" attractor here comes from, that would be great! Thank you in advance!","I am currently reading a paper , studying the population dynamics in 3-dimensional Lotka-Volterra model with the following interaction change descriptive system: Where is the variable parameter, for different values of which the model's behavior is studied. In the paper, authors claim, that the respective non-degenerate (biologically meaningfull if ) equlibria for is , where Now, the problem is that further in the paper, authors consider a case, when posesses coexisting limit-1 cycle (red) and two-band chaotic attractor (blue). But the case is, I understnad what the red curve is, that is just basically the set of solutions of the system , plotted not against the time variable, but against each other . I even managed to get the ""red"" curve by myself, by solving for in python, using scipy.integrate.odeint : But then, I completely fail to understand, where does the ""blue"" attractor come from, and there is no any further information in the paper  about where it comes from as well, rather the blue attractor is just called ""attractor of a Feigenbaum's tree"". If anyone would be so kind to explain me, where the ""blue"" attractor here comes from, that would be great! Thank you in advance!","
\begin{equation}
    \begin{cases}
        \dot x = ax - 0.06x^2-\Large\frac{xy}{x+10}\normalsize,\\
        \dot y = -y + \Large\frac{2xy}{x+10}\normalsize-\Large\frac{0.405\cdot yz}{y+10}\normalsize,\\
        \dot z = 0.038z^2 -\Large\frac{z^2}{y+20}\normalsize.\\
    \end{cases}
\end{equation}\tag{1}
 a a > 0.9158 (1) M_3(\bar x_3, \bar y_3, \bar z_3) \bar x_3 = \frac{a}{0.12}-5+\frac{\sqrt{(a+0.6)^2-1.52}}{0.12},~\bar y_3 = 6.31579,~\bar z_3 = -40.3 + 80.6\frac{\bar x_3}{\bar x_3 + 10}. (1) (1) (x(t),y(t),z(t)) (1) a=1.803","['ordinary-differential-equations', 'stability-in-odes', 'bifurcation', 'basins-of-attraction']"
85,How long does it take until the tank contains the same amount of both liquid A and liquid B?,How long does it take until the tank contains the same amount of both liquid A and liquid B?,,"A tank initially contains 400 litres of liquid A. Liquid B is pumped into the tank at the rate of 20 litres per minute. The contents of the tank is pumped out at the same rate of 20 litres per minute. The mixture is stirred continuously and the tank is kept full at 400 litres at all times. Let X (t) denote the amount (in litres) of liquid A in the tank after t minutes. How long does it take until the tank contains the same amount of both liquid A and liquid B? This is a problem I've seen and solved a few times in different forms and while I understand how to solve it mechanically I don't know why certain choices are made. I determine that: $ X(t) = 400(1 - e^{-t/20})$ I know from here to answer the above question I would need to solve for $t$ in the below equation: $200 = 400(1 - e^{-t/20})$ As I understand it $X(t)$ is the mixture of $A$ and $B$ at a point in time. So if $X(t)$ makes up 200L of the 400L then the tank would be 50% $A$ and 50% ( $A$ , $B$ ) Why have we chosen 200 for $X(t)$ ? For how much of this problem is my understanding incorrect?","A tank initially contains 400 litres of liquid A. Liquid B is pumped into the tank at the rate of 20 litres per minute. The contents of the tank is pumped out at the same rate of 20 litres per minute. The mixture is stirred continuously and the tank is kept full at 400 litres at all times. Let X (t) denote the amount (in litres) of liquid A in the tank after t minutes. How long does it take until the tank contains the same amount of both liquid A and liquid B? This is a problem I've seen and solved a few times in different forms and while I understand how to solve it mechanically I don't know why certain choices are made. I determine that: I know from here to answer the above question I would need to solve for in the below equation: As I understand it is the mixture of and at a point in time. So if makes up 200L of the 400L then the tank would be 50% and 50% ( , ) Why have we chosen 200 for ? For how much of this problem is my understanding incorrect?", X(t) = 400(1 - e^{-t/20}) t 200 = 400(1 - e^{-t/20}) X(t) A B X(t) A A B X(t),"['ordinary-differential-equations', 'mathematical-modeling']"
86,Determining whether this equation is linearly independent,Determining whether this equation is linearly independent,,"I am given 2 sets of equations and I want to determine if they're linearly independent or not. $e^{-x}, -2e^{2x}+5e^{-x}$ $x|x|, x^2$ The definition of linear independence: if there are no constants $a, b \not= 0$ such that $ax_1(t)+bx_2(t)=0$ . For the first set of equations, I can tell that this is linearly independent because of the term $-2e^{2x}$ will never go away. For the second set of equations, I'm not so sure. If $x \geq 0$ , then this equation is not linearly independent. But $x \geq 0$ is not stated as a restriction. I just need someone to hopefully confirm my findings and see if my hypothesis is correct.","I am given 2 sets of equations and I want to determine if they're linearly independent or not. The definition of linear independence: if there are no constants such that . For the first set of equations, I can tell that this is linearly independent because of the term will never go away. For the second set of equations, I'm not so sure. If , then this equation is not linearly independent. But is not stated as a restriction. I just need someone to hopefully confirm my findings and see if my hypothesis is correct.","e^{-x}, -2e^{2x}+5e^{-x} x|x|, x^2 a, b \not= 0 ax_1(t)+bx_2(t)=0 -2e^{2x} x \geq 0 x \geq 0","['linear-algebra', 'ordinary-differential-equations']"
87,Exponential populations that depend upon each other,Exponential populations that depend upon each other,,"I have a question about how to solve an exponential problem that involves two populations, each which depends on the other. For example, let's say we have an initial population of $h$ humans that increases by $H$ percent each year. And let's say we have an initial population of $d$ dragons which increases by $D$ percent each year. Each dragon kills $K_h$ humans a year. And every human kills $H_d$ dragons each year. How can I figure out, in this obviously fictional example, how many dragons or humans there are after $t$ years? Please note: I am not particularly advanced in math; I apologize if this question is either trivial or nonsensical. Please bear with me.","I have a question about how to solve an exponential problem that involves two populations, each which depends on the other. For example, let's say we have an initial population of humans that increases by percent each year. And let's say we have an initial population of dragons which increases by percent each year. Each dragon kills humans a year. And every human kills dragons each year. How can I figure out, in this obviously fictional example, how many dragons or humans there are after years? Please note: I am not particularly advanced in math; I apologize if this question is either trivial or nonsensical. Please bear with me.",h H d D K_h H_d t,"['ordinary-differential-equations', 'systems-of-equations', 'exponential-function', 'word-problem']"
88,Dirichlet problem with separation of variables,Dirichlet problem with separation of variables,,"Use separation of variables to find a nonzero solution for the following Dirichlet problem: $\Delta u = 0$ in $\Omega$ with $u=0$ on $\partial \Omega$ , where $\Omega = \{(x,y) \in \mathbb{R}^{2}: 0 < y < \pi\}$ $\Omega \in \mathbb{R}^{2}$ is open and bounded, u is harmonic on $\Omega$ . If we assume that $u \in C^{2}(\Omega) \cap C(\bar{\Omega})$ , then according to the maximum principle for harmonic functions we have: $0 = \min_{y \in \partial \Omega} u(y) \leq u(x) \leq \max_{y \in \partial \Omega} u(y) = 0$ $\hspace{3mm} \forall x \in \Omega \hspace{3mm}$ (since $u = 0$ on $\partial \Omega$ ) Hence u(x) = 0 for all $x \in \Omega$ . I have the feeling that there does not exist a nonzero solution, because of this maximum principle. So how do I know that there does exist a nonzero solution and how can I find this solution. I think I need to assume that there exist a solution $u(x,y) = X(x) \cdot Y(y)$ . Then $0 = \Delta u = u_{xx} + u_{yy} = Y(y) \cdot X''(x) + X(x) \cdot Y''(y)$ . So $X''(x) + aX(x) = 0$ and $Y''(y) + b \cdot Y(y) = 0$ for some constants $a = a(y)$ and $b = b(x)$ . And then solve these ODEs for $X(x)$ and $Y(y)$ . So $u(x,y) = X(x) \cdot Y(y) = (c_{1} \cdot \cos(\sqrt{a}x) + c_{2} \cdot \sin(\sqrt{a} x)) \cdot (d_{1} \cdot \cos(\sqrt{b}y) + d_{2} \cdot \sin(\sqrt{b} y))$ And we know that $u = 0$ on $\partial \Omega$ . So that means that $u(x,0) = 0$ and $u(x,\pi) = 0$ . And with these boundary values, I only find that $d_{1} = 0$ . So how to continue then? And what does this Dirichlet problem tell is then about the maximum principle and what is wrong about my conclusion that $u(x) = 0$ for all $x \in \Omega$ ?","Use separation of variables to find a nonzero solution for the following Dirichlet problem: in with on , where is open and bounded, u is harmonic on . If we assume that , then according to the maximum principle for harmonic functions we have: (since on ) Hence u(x) = 0 for all . I have the feeling that there does not exist a nonzero solution, because of this maximum principle. So how do I know that there does exist a nonzero solution and how can I find this solution. I think I need to assume that there exist a solution . Then . So and for some constants and . And then solve these ODEs for and . So And we know that on . So that means that and . And with these boundary values, I only find that . So how to continue then? And what does this Dirichlet problem tell is then about the maximum principle and what is wrong about my conclusion that for all ?","\Delta u = 0 \Omega u=0 \partial \Omega \Omega = \{(x,y) \in \mathbb{R}^{2}: 0 < y < \pi\} \Omega \in \mathbb{R}^{2} \Omega u \in C^{2}(\Omega) \cap C(\bar{\Omega}) 0 = \min_{y \in \partial \Omega} u(y) \leq u(x) \leq \max_{y \in \partial \Omega} u(y) = 0 \hspace{3mm} \forall x \in \Omega \hspace{3mm} u = 0 \partial \Omega x \in \Omega u(x,y) = X(x) \cdot Y(y) 0 = \Delta u = u_{xx} + u_{yy} = Y(y) \cdot X''(x) + X(x) \cdot Y''(y) X''(x) + aX(x) = 0 Y''(y) + b \cdot Y(y) = 0 a = a(y) b = b(x) X(x) Y(y) u(x,y) = X(x) \cdot Y(y) = (c_{1} \cdot \cos(\sqrt{a}x) + c_{2} \cdot \sin(\sqrt{a} x)) \cdot (d_{1} \cdot \cos(\sqrt{b}y) + d_{2} \cdot \sin(\sqrt{b} y)) u = 0 \partial \Omega u(x,0) = 0 u(x,\pi) = 0 d_{1} = 0 u(x) = 0 x \in \Omega","['ordinary-differential-equations', 'harmonic-functions', 'maximum-principle']"
89,Exact ODE: Why is $du = 0$?,Exact ODE: Why is ?,du = 0,"The following form $$M(x,y)dx + N(x,y)dy$$ can be derived from the total differential of a bivariate function $u(x,y)$ , which is $$du = u_x(x,y)dx + u_y(x,y)dy.$$ Therefore, $du = M(x,y)dx + N(x,y)dy$ . This is perfectly understandable so far. Then we somehow set $u(x,y) = c$ , which makes the differential of $u$ equal to $0$ . I'm confused about this. If $u(x,y) = c$ , then $y$ is no longer an independent variable. The value of $y$ is now dependent on $x$ . Simply put, $u(x,y) = c$ is equal to $u(x,y(x)) = c$ . It's really just an implicit form of a single-variable function. So how is $du$ even defined in this case? Why is $du$ equal to $0$ when there's no tangent plane to approximate the value of $\Delta u$ in the first place?","The following form can be derived from the total differential of a bivariate function , which is Therefore, . This is perfectly understandable so far. Then we somehow set , which makes the differential of equal to . I'm confused about this. If , then is no longer an independent variable. The value of is now dependent on . Simply put, is equal to . It's really just an implicit form of a single-variable function. So how is even defined in this case? Why is equal to when there's no tangent plane to approximate the value of in the first place?","M(x,y)dx + N(x,y)dy u(x,y) du = u_x(x,y)dx + u_y(x,y)dy. du = M(x,y)dx + N(x,y)dy u(x,y) = c u 0 u(x,y) = c y y x u(x,y) = c u(x,y(x)) = c du du 0 \Delta u","['ordinary-differential-equations', 'differential-forms']"
90,"Derivation of Laplace transform of Bessel function, where does initial condition go?","Derivation of Laplace transform of Bessel function, where does initial condition go?",,"I'm trying to understand the derivation of the Laplace transform of $J_0(t)$ given in Spiegel's ""Laplace Transforms"" book, on p. 23 of my copy. One way is to use the power series representation for $J_0(t)$ . The other way is to take the Laplace transform of Bessel's differential equation $tJ_0''(t) + J_0'(t) + t J_0(t) = 0$ , using the initial conditions $J_0(0) = 1$ , $J_0'(0)=0$ , however there's something that's seriously confusing me. When we do the second method, using the properties of the LT, we arrive at $$ -\frac{d}{ds} (s^2 y -s -0) +(sy -1) - \frac{dy}{ds} = 0. $$ Then, doing algebra and solving the separable ODE, we get $y(s) = \frac{c}{\sqrt{s^2+1}}$ , and then use the initial-value theorem to get $c=1$ . The thing I don't understand is that the initial condition $Y'(0) = 0$ is not registered by this method, since it disappears when we take the derivative above, also we don't use it in the initial-value theorem, we use instead $Y(0) = 1$ . But I thought two initial conditions were required to uniquely determine a 2nd order ODE like this? Also, doesn't the power series solution use $Y'(0) = 0$ , and then they agree? I have a feeling that this has something to do with the fact that there are two solutions to this ODE, the solutions of the first and second kind, but I don't really understand well what is happening. Is anyone able to enlighten me? Greg","I'm trying to understand the derivation of the Laplace transform of given in Spiegel's ""Laplace Transforms"" book, on p. 23 of my copy. One way is to use the power series representation for . The other way is to take the Laplace transform of Bessel's differential equation , using the initial conditions , , however there's something that's seriously confusing me. When we do the second method, using the properties of the LT, we arrive at Then, doing algebra and solving the separable ODE, we get , and then use the initial-value theorem to get . The thing I don't understand is that the initial condition is not registered by this method, since it disappears when we take the derivative above, also we don't use it in the initial-value theorem, we use instead . But I thought two initial conditions were required to uniquely determine a 2nd order ODE like this? Also, doesn't the power series solution use , and then they agree? I have a feeling that this has something to do with the fact that there are two solutions to this ODE, the solutions of the first and second kind, but I don't really understand well what is happening. Is anyone able to enlighten me? Greg","J_0(t) J_0(t) tJ_0''(t) + J_0'(t) + t J_0(t) = 0 J_0(0) = 1 J_0'(0)=0 
-\frac{d}{ds} (s^2 y -s -0) +(sy -1) - \frac{dy}{ds} = 0.
 y(s) = \frac{c}{\sqrt{s^2+1}} c=1 Y'(0) = 0 Y(0) = 1 Y'(0) = 0",['ordinary-differential-equations']
91,Question about exact ODE. Why in $h'(y)$ contain $x$?,Question about exact ODE. Why in  contain ?,h'(y) x,"Find the solution ODE $$\left(x^3e^xy+4x^2e^xy+2xe^xy\right)dx+(x^3e^x+x^2e^x)dy=0.$$ Let $M(x,y)=x^3e^xy+4x^2e^xy+2y$ and $N(x,y)=x^3e^x+x^2e^x$ . \begin{align*} 	\dfrac{\partial M}{\partial y}&=\dfrac{\partial}{\partial y}\left(x^3e^xy+4x^2e^xy+2xe^xy\right) \\ 	&=x^3e^x+4x^2e^x+2xe^x.\\ 	\dfrac{\partial N}{\partial x}&=\dfrac{\partial}{\partial x}\left(x^3e^x+x^2e^x\right) \\ 	&=3x^2e^x+x^3e^x+2xe^x+x^2e^x\\ 	&=x^3e^x+4x^2e^x+2xe^x. \end{align*} This is exact ODE since $\dfrac{\partial M}{\partial y}= \dfrac{\partial N}{\partial x}$ . Now, \begin{alignat}{2} 	&&\dfrac{\partial F(x,y)}{\partial x}&=M(x,y)\nonumber\\ 	\Longleftrightarrow\quad 	&&\dfrac{\partial F(x,y)}{\partial x}&=x^3e^xy+4x^2e^xy+2y\nonumber\\ 	\Longleftrightarrow\quad 	&&\int\partial F(x,y)&=\int\left(x^3e^xy+4x^2e^xy+2y\right) \partial x\nonumber\\ 	\Longleftrightarrow\quad 	&&\int\partial F(x,y)&=\int x^3e^xy\partial x+\int4x^2e^xy \partial x+\int2y \partial x.\label{ijoet} \end{alignat} Consider that \begin{align} 	\int x^3e^xy\partial x&=x^3e^xy-\int e^xy 3x^2 \partial x\nonumber\\ 	&=x^3e^xy-\int 3x^2 e^x y \partial x\nonumber\\ 	&=x^3e^xy-\left(3x^2e^x y-\int e^x y 6x \partial x\right)\nonumber\\ 	&=x^3e^xy-3x^2e^x y+\int  6x e^x y\partial x\nonumber\\ 	&=x^3e^xy-3x^2e^x y+6x e^x y-\int e^x y 6\partial x\nonumber\\ 	&=x^3e^xy-3x^2e^x y+6x e^x y- 6 e^x y+h_1(y).\label{meong} \end{align} \begin{align} 	\int 4x^2e^xy\partial x&=4x^2e^xy-\int e^xy 8x \partial x\nonumber\\ 	&=4x^2e^xy-\int 8x e^xy  \partial x\nonumber\\ 	&=4x^2e^xy-\left(8x e^xy-\int e^xy 8 \partial x\right)\nonumber\\ 	&=4x^2e^xy-8x e^xy+\int 8e^xy \partial x\nonumber\\ 	&=4x^2e^xy-8x e^xy+ 8e^xy +h_2(y).\label{meong1} \end{align} \begin{align} 	\int2y \partial x&= 2xy+h_3(y).\label{meong2} \end{align} So, we have \begin{alignat}{2} 	&&\int\partial F(x,y)&=x^3e^xy-3x^2e^x y+6x e^x y- 6 e^x y+h_1(y)\nonumber\\ 	&&&\quad +4x^2e^xy-8x e^xy+ 8e^xy +h_2(y)+2xy+h_3(y)\nonumber\\ 	\Longleftrightarrow\quad 	&&F(x,y)&=x^3e^xy+x^2e^x y-2 x e^x y+2e^x y+2xy +h(y)\nonumber \end{alignat} which $h(y)=h_1(y)+h_2(y)+h_3(y)$ . Next, consider that \begin{alignat*}{2} 	&&\dfrac{\partial F(x,y)}{\partial y}&=N(x,y)\\ 	\Longleftrightarrow\quad 	&&\dfrac{\partial}{\partial y}\left(x^3e^xy+x^2e^x y-2 x e^x y+2e^x y+2xy +h(y)\right)&=x^3e^x+x^2e^x\\ 	\Longleftrightarrow\quad 	&&x^3e^x+x^2e^x -2 x e^x +2e^x +2x +h'(y)&=x^3e^x+x^2e^x\\ 	\Longleftrightarrow\quad 	&&h'(y)&=2 x e^x -2e^x -2x \\ \end{alignat*} When I want to find $h(y)$ , I have found $h'(y)=2 x e^x -2e^x -2x$ . Why in $h'(y)$ contain $x$ ? What my mistake?","Find the solution ODE Let and . This is exact ODE since . Now, Consider that So, we have which . Next, consider that When I want to find , I have found . Why in contain ? What my mistake?","\left(x^3e^xy+4x^2e^xy+2xe^xy\right)dx+(x^3e^x+x^2e^x)dy=0. M(x,y)=x^3e^xy+4x^2e^xy+2y N(x,y)=x^3e^x+x^2e^x \begin{align*}
	\dfrac{\partial M}{\partial y}&=\dfrac{\partial}{\partial y}\left(x^3e^xy+4x^2e^xy+2xe^xy\right) \\
	&=x^3e^x+4x^2e^x+2xe^x.\\
	\dfrac{\partial N}{\partial x}&=\dfrac{\partial}{\partial x}\left(x^3e^x+x^2e^x\right) \\
	&=3x^2e^x+x^3e^x+2xe^x+x^2e^x\\
	&=x^3e^x+4x^2e^x+2xe^x.
\end{align*} \dfrac{\partial M}{\partial y}= \dfrac{\partial N}{\partial x} \begin{alignat}{2}
	&&\dfrac{\partial F(x,y)}{\partial x}&=M(x,y)\nonumber\\
	\Longleftrightarrow\quad
	&&\dfrac{\partial F(x,y)}{\partial x}&=x^3e^xy+4x^2e^xy+2y\nonumber\\
	\Longleftrightarrow\quad
	&&\int\partial F(x,y)&=\int\left(x^3e^xy+4x^2e^xy+2y\right) \partial x\nonumber\\
	\Longleftrightarrow\quad
	&&\int\partial F(x,y)&=\int x^3e^xy\partial x+\int4x^2e^xy \partial x+\int2y \partial x.\label{ijoet}
\end{alignat} \begin{align}
	\int x^3e^xy\partial x&=x^3e^xy-\int e^xy 3x^2 \partial x\nonumber\\
	&=x^3e^xy-\int 3x^2 e^x y \partial x\nonumber\\
	&=x^3e^xy-\left(3x^2e^x y-\int e^x y 6x \partial x\right)\nonumber\\
	&=x^3e^xy-3x^2e^x y+\int  6x e^x y\partial x\nonumber\\
	&=x^3e^xy-3x^2e^x y+6x e^x y-\int e^x y 6\partial x\nonumber\\
	&=x^3e^xy-3x^2e^x y+6x e^x y- 6 e^x y+h_1(y).\label{meong}
\end{align} \begin{align}
	\int 4x^2e^xy\partial x&=4x^2e^xy-\int e^xy 8x \partial x\nonumber\\
	&=4x^2e^xy-\int 8x e^xy  \partial x\nonumber\\
	&=4x^2e^xy-\left(8x e^xy-\int e^xy 8 \partial x\right)\nonumber\\
	&=4x^2e^xy-8x e^xy+\int 8e^xy \partial x\nonumber\\
	&=4x^2e^xy-8x e^xy+ 8e^xy +h_2(y).\label{meong1}
\end{align} \begin{align}
	\int2y \partial x&= 2xy+h_3(y).\label{meong2}
\end{align} \begin{alignat}{2}
	&&\int\partial F(x,y)&=x^3e^xy-3x^2e^x y+6x e^x y- 6 e^x y+h_1(y)\nonumber\\
	&&&\quad +4x^2e^xy-8x e^xy+ 8e^xy +h_2(y)+2xy+h_3(y)\nonumber\\
	\Longleftrightarrow\quad
	&&F(x,y)&=x^3e^xy+x^2e^x y-2 x e^x y+2e^x y+2xy +h(y)\nonumber
\end{alignat} h(y)=h_1(y)+h_2(y)+h_3(y) \begin{alignat*}{2}
	&&\dfrac{\partial F(x,y)}{\partial y}&=N(x,y)\\
	\Longleftrightarrow\quad
	&&\dfrac{\partial}{\partial y}\left(x^3e^xy+x^2e^x y-2 x e^x y+2e^x y+2xy +h(y)\right)&=x^3e^x+x^2e^x\\
	\Longleftrightarrow\quad
	&&x^3e^x+x^2e^x -2 x e^x +2e^x +2x +h'(y)&=x^3e^x+x^2e^x\\
	\Longleftrightarrow\quad
	&&h'(y)&=2 x e^x -2e^x -2x \\
\end{alignat*} h(y) h'(y)=2 x e^x -2e^x -2x h'(y) x",['ordinary-differential-equations']
92,How to solve $\dot{\mathbf{r}}(\theta)^2 + \mathbf{r}(\theta)^2 = 1$?,How to solve ?,\dot{\mathbf{r}}(\theta)^2 + \mathbf{r}(\theta)^2 = 1,"I'm trying to determine all smooth functions $\mathbf{r}(\theta)$ such that the curve $\gamma(\theta) = (\mathbf{r}(\theta)\cos(\theta), \mathbf{r}(\theta)\sin(\theta))$ is unit-speed curve, i.e. that $\|\dot{\gamma}(\theta)\| = 1 \implies $$\dot{\mathbf{r}}(\theta)^2 + \mathbf{r}(\theta)^2 = 1$ . According to my source, which omits intermediate steps, the solution ends up being $\pm\sin(\theta + \alpha)$ for some constant $\alpha$ . But how can I show this myself?","I'm trying to determine all smooth functions such that the curve is unit-speed curve, i.e. that . According to my source, which omits intermediate steps, the solution ends up being for some constant . But how can I show this myself?","\mathbf{r}(\theta) \gamma(\theta) = (\mathbf{r}(\theta)\cos(\theta), \mathbf{r}(\theta)\sin(\theta)) \|\dot{\gamma}(\theta)\| = 1 \implies \dot{\mathbf{r}}(\theta)^2 + \mathbf{r}(\theta)^2 = 1 \pm\sin(\theta + \alpha) \alpha","['ordinary-differential-equations', 'differential-geometry', 'curves']"
93,$5y''+6y'=x^2+5x +3$ What is the $ y_p$ here? How to derive $y_p$?,What is the  here? How to derive ?,5y''+6y'=x^2+5x +3  y_p y_p,"$5y''+6y'=x^2+5x +3$ What is the $ y_p$ here? So I tried the usual. when the right hand side is polynomial degree of 2, my $ y_p = Ax^2 + Bx + C$ . I then differentiate once and twice and sub them back in. But this time it doesn't work. I think it is because the left hand side is missing a constant term. So for example in general, $ay'' + by'= \sum _{i=0}^n\:\beta _i\:x^i$ Can someone help out? I don't want the answer. I just want to know how do you get the functional form of $y_p$ ? How do you derive it? Can someone prove it (ie what y_p) is  or link a proof to me? Also, how do you mathematically express ""some polynomial""?","What is the here? So I tried the usual. when the right hand side is polynomial degree of 2, my . I then differentiate once and twice and sub them back in. But this time it doesn't work. I think it is because the left hand side is missing a constant term. So for example in general, Can someone help out? I don't want the answer. I just want to know how do you get the functional form of ? How do you derive it? Can someone prove it (ie what y_p) is  or link a proof to me? Also, how do you mathematically express ""some polynomial""?",5y''+6y'=x^2+5x +3  y_p  y_p = Ax^2 + Bx + C ay'' + by'= \sum _{i=0}^n\:\beta _i\:x^i y_p,['ordinary-differential-equations']
94,Finding Constant C in ODE with Initial Condition,Finding Constant C in ODE with Initial Condition,,At some point we get: $$te^ty=te^t-e^t+C$$ Given the initial condition: $$y\log 2=1$$ How do we get $C$ from here? The lecture note writes: $$2\log 2 - 2 + C = 2\log 2$$ which I do not understand...,At some point we get: Given the initial condition: How do we get from here? The lecture note writes: which I do not understand...,te^ty=te^t-e^t+C y\log 2=1 C 2\log 2 - 2 + C = 2\log 2,['ordinary-differential-equations']
95,How does this log rule work?,How does this log rule work?,,"In my textbook, I saw the following steps: Step (1): $\ln|v(t)-49| = -\frac{t}{5}+C$ Step (2): $v(t) = 49 + ce^{-t/5}$ How did they go from step 1 to 2. When solving this, I would think to put $v(t) = e^{-t/5+C} +49$ . Can someone explain how the c was brought to the front as a product?","In my textbook, I saw the following steps: Step (1): Step (2): How did they go from step 1 to 2. When solving this, I would think to put . Can someone explain how the c was brought to the front as a product?",\ln|v(t)-49| = -\frac{t}{5}+C v(t) = 49 + ce^{-t/5} v(t) = e^{-t/5+C} +49,"['algebra-precalculus', 'ordinary-differential-equations', 'logarithms']"
96,Soluition of Riccati equation,Soluition of Riccati equation,,"Solution of Riccati equation: $y' -y + y^2 e^x + 5e^{-x} = 0 , y(0) = \pi $ My work: A Riccati equation of the form $y' + py + q y^2 = r $ can be transformed to second order ODE as follows. $u''+\left ( p - \dfrac{q'}{q} \right ) u' -rq u =0 $ Therefore, $p = -1 $ , $q = e^x $ , $r = -5 e^{-x}$ $u''+\left ( -1 - \dfrac{(e^{x})'}{e^x} \right ) u' + 5u =0 $ $u''+\left ( -1 -1 \right ) u' + 5u =0 $ $u''- 2u' + 5u =0 $ The auxillary equation is $ k^2 -2k +5  = 0 $ $k = \dfrac{2 \pm \sqrt{4-20}}{2}$ $k = 1 \pm 2 i$ Therefore, the solution is, $u(x) = e^{x} (C_1 \cos (2x) + C_2 \sin (2x))$ How to find the solution for y? Kindly advise.","Solution of Riccati equation: My work: A Riccati equation of the form can be transformed to second order ODE as follows. Therefore, , , The auxillary equation is Therefore, the solution is, How to find the solution for y? Kindly advise.","y' -y + y^2 e^x + 5e^{-x} = 0 , y(0) = \pi  y' + py + q y^2 = r  u''+\left ( p - \dfrac{q'}{q} \right ) u' -rq u =0  p = -1  q = e^x  r = -5 e^{-x} u''+\left ( -1 - \dfrac{(e^{x})'}{e^x} \right ) u' + 5u =0  u''+\left ( -1 -1 \right ) u' + 5u =0  u''- 2u' + 5u =0   k^2 -2k +5  = 0  k = \dfrac{2 \pm \sqrt{4-20}}{2} k = 1 \pm 2 i u(x) = e^{x} (C_1 \cos (2x) + C_2 \sin (2x))",['ordinary-differential-equations']
97,Under what circumstances is $\ddot{x}=\dot{x}\frac{d\dot{x}}{dx}$,Under what circumstances is,\ddot{x}=\dot{x}\frac{d\dot{x}}{dx},"I was told a trick by my professors for certain integrals. Given that the path of a particle in 1D is $x(t)$ , then the following is true $$ \ddot{x}=\frac{d\dot{x}}{dt}=\frac{d\dot{x}}{dx}\frac{dx}{dt}=\dot{x}\frac{d\dot{x}}{dx} $$ My impression is that this is true for any path, so long as $x$ only depends on $t$ . The reason I ask is for a specific problem in Thornton and Marion. I think the answers I am finding online might be wrong. Specifically this is 2-1; suppose a force is acting on a single particle. If the force has the form $F=F(x_i,t)=f(x_i)g(t)$ , determine if the equation of motion is integrable. The standard answer I am finding copied and pasted is: $$ m\ddot{x}_i=f(x_i)g(t)\\ \text{Since it is the case that } \ddot{x}_i = \frac{d\dot{x}}{dt} \text{, then we have}\\ m\frac{d\dot{x}}{f(x_i)}=g(t)dt $$ Which is then just declared not integrable without further clarification. Suppose that the substitution for $\ddot{x}$ I mentioned is generally true for functions of one variable.Then we can rewrite the EOM as $$ m\ddot{x}_i=f(x_i)g(t)\\ m\dot{x}\frac{d\dot{x}_i}{dx_i}=f(x_i)g(t)\\ \text{Which can be rewritten as } m\dot{x}d\dot{x}=f(x_i)g(t)dx_i $$ To which I would argue that as long as $g(t)$ is not dependent on $x_i$ , then the above should be perfectly integrable. Specifically: $$ \frac{m}{2}\dot{x}^2=g(t)\int f(x_i)dx_i\\ \text{which can be modified to give }\\ \sqrt{\frac{m}{2}}\dot{x}=\sqrt{g(t)}\sqrt{\int f(x_i)dx_i}\\ \text{which just simply gives } \frac{dx_i}{\sqrt{\int f(x_i)dx_i}}=\sqrt{\frac{2g(t)}{m}}dt $$ Which would appear to still be integrable. So have people been copying the wrong solution for years, or am I wrong for some reason I do not see?","I was told a trick by my professors for certain integrals. Given that the path of a particle in 1D is , then the following is true My impression is that this is true for any path, so long as only depends on . The reason I ask is for a specific problem in Thornton and Marion. I think the answers I am finding online might be wrong. Specifically this is 2-1; suppose a force is acting on a single particle. If the force has the form , determine if the equation of motion is integrable. The standard answer I am finding copied and pasted is: Which is then just declared not integrable without further clarification. Suppose that the substitution for I mentioned is generally true for functions of one variable.Then we can rewrite the EOM as To which I would argue that as long as is not dependent on , then the above should be perfectly integrable. Specifically: Which would appear to still be integrable. So have people been copying the wrong solution for years, or am I wrong for some reason I do not see?","x(t) 
\ddot{x}=\frac{d\dot{x}}{dt}=\frac{d\dot{x}}{dx}\frac{dx}{dt}=\dot{x}\frac{d\dot{x}}{dx}
 x t F=F(x_i,t)=f(x_i)g(t) 
m\ddot{x}_i=f(x_i)g(t)\\
\text{Since it is the case that } \ddot{x}_i = \frac{d\dot{x}}{dt} \text{, then we have}\\
m\frac{d\dot{x}}{f(x_i)}=g(t)dt
 \ddot{x} 
m\ddot{x}_i=f(x_i)g(t)\\
m\dot{x}\frac{d\dot{x}_i}{dx_i}=f(x_i)g(t)\\
\text{Which can be rewritten as } m\dot{x}d\dot{x}=f(x_i)g(t)dx_i
 g(t) x_i 
\frac{m}{2}\dot{x}^2=g(t)\int f(x_i)dx_i\\
\text{which can be modified to give }\\ \sqrt{\frac{m}{2}}\dot{x}=\sqrt{g(t)}\sqrt{\int f(x_i)dx_i}\\
\text{which just simply gives } \frac{dx_i}{\sqrt{\int f(x_i)dx_i}}=\sqrt{\frac{2g(t)}{m}}dt
","['ordinary-differential-equations', 'functions', 'derivatives', 'physics', 'classical-mechanics']"
98,"$\frac{dx}{dt} = x-xy,\quad \frac{dy}{dt} = -y+xy.$ Find the range of values of $\ x\ $ and $\ y\ $ for which both variables are increasing.",Find the range of values of  and  for which both variables are increasing.,"\frac{dx}{dt} = x-xy,\quad \frac{dy}{dt} = -y+xy. \ x\  \ y\ ","$$\frac{dx}{dt} = x-xy,\quad \frac{dy}{dt} = -y+xy.$$ Find the range of values of $\ x\ $ and $\ y\ $ for which both variables are increasing. Here, increasing means the derivative is $\ >0.$ My attempt: $x-xy>0,\quad -y+xy>0 \implies x>y.$ We also have: $\ x(1-y)>0\ \implies \text{either}\ (\ x>0\ \text{and}\ y<1\ )\quad \text{or}\ (\ x<0\ \text{and}\ y>1\ ).$ and $y(-1+x)>0\implies\ (\ y>0\ \text{and}\ x>1\ )\quad \text{or}\ (\ y<0\ \text{and}\ x<1\ ),$ But now I don't know where to go from here without getting confused. We can also say that $\ x(-1+x)>y(-1+x)>0,\ $ assuming that $\ ( \ y>0\ $ and $\ x>1),\ $ or $\ ( \ y<0\ $ and $\ x<1).$ and try to make sense of the quadratic inequality in $x,$ but the different cases make this a headache. Note that this is a difficult A Level problem, so should be solvable quite quickly. I'm confident that I'm missing something straightforward.","Find the range of values of and for which both variables are increasing. Here, increasing means the derivative is My attempt: We also have: and But now I don't know where to go from here without getting confused. We can also say that assuming that and or and and try to make sense of the quadratic inequality in but the different cases make this a headache. Note that this is a difficult A Level problem, so should be solvable quite quickly. I'm confident that I'm missing something straightforward.","\frac{dx}{dt} = x-xy,\quad \frac{dy}{dt} = -y+xy. \ x\  \ y\  \ >0. x-xy>0,\quad -y+xy>0 \implies x>y. \ x(1-y)>0\ \implies \text{either}\ (\ x>0\ \text{and}\ y<1\ )\quad \text{or}\ (\ x<0\ \text{and}\ y>1\ ). y(-1+x)>0\implies\ (\ y>0\ \text{and}\ x>1\ )\quad \text{or}\ (\ y<0\ \text{and}\ x<1\ ), \ x(-1+x)>y(-1+x)>0,\  \ ( \ y>0\  \ x>1),\  \ ( \ y<0\  \ x<1). x,","['ordinary-differential-equations', 'derivatives', 'problem-solving']"
99,Is this trick not true when solving an ODE?,Is this trick not true when solving an ODE?,,"I have a difficult first-order differential equation to solve. Here it is $$-y'^2y^2+ay^4+by=-c$$ Where y is a function of time and a,b and c are constants. So in order to find an easier differential equation that I could solve I did the following, we know that $$1=\frac{dt}{dt}=\frac{dt}{dy}\frac{dy}{dt}=\frac{dt}{dy}y'$$ Let's thus rewrite the ODE as, $$-y'^2y^2+ay^4+by=-c\frac{dt}{dy}y'$$ Since I know that $y'$ is never 0, I divide by $y'$ on both sides, $$(-y'y^2+\frac{a}{y'}y^4+\frac{b}{y'}y)dy=-c\times dt$$ So if we integrate both sides with respect to the correct variable we get, $$\int(-y'y^2+\frac{a}{y'}y^4+\frac{b}{y'}y)dy=\int-c\times dt$$ $$-\frac{y'}{3}y^3+\frac{a}{5y'}y^5+\frac{b}{2y'}y^2=-c\times t+p$$ Where p is an integration constant. This puts me in a more favorable place for solving my original problem. Is the trick used in equation 3 valid? Are there any errors?","I have a difficult first-order differential equation to solve. Here it is Where y is a function of time and a,b and c are constants. So in order to find an easier differential equation that I could solve I did the following, we know that Let's thus rewrite the ODE as, Since I know that is never 0, I divide by on both sides, So if we integrate both sides with respect to the correct variable we get, Where p is an integration constant. This puts me in a more favorable place for solving my original problem. Is the trick used in equation 3 valid? Are there any errors?",-y'^2y^2+ay^4+by=-c 1=\frac{dt}{dt}=\frac{dt}{dy}\frac{dy}{dt}=\frac{dt}{dy}y' -y'^2y^2+ay^4+by=-c\frac{dt}{dy}y' y' y' (-y'y^2+\frac{a}{y'}y^4+\frac{b}{y'}y)dy=-c\times dt \int(-y'y^2+\frac{a}{y'}y^4+\frac{b}{y'}y)dy=\int-c\times dt -\frac{y'}{3}y^3+\frac{a}{5y'}y^5+\frac{b}{2y'}y^2=-c\times t+p,"['integration', 'ordinary-differential-equations', 'derivatives']"
