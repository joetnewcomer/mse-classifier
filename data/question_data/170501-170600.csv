,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Understanding the point estimation of the expected value,Understanding the point estimation of the expected value,,"I am trying to understand this problem, however I can't get past some of the definitions used when estimating the expected value. What I would need is to confirm or disprove my conclusions - I read school materials and tried to find the answers on internet. Let's say we measure how much are sea fish toxic and we want to know the expected value of toxicity, when we catch some fish. From my understanding we should do something like this:  If we knew the random variable of toxicity $X$, we could compute the expected value, but we don't know that. If we want to estimate the expected value, the estimation is: $E(\bar x)= u$, where $\bar x$ is sample mean. So we catch, for example, $100$ fish and measure the toxicity - now is the part, where I am lost: In my scripts it says, that this is a random sample, from which we get random vector $(X_1, X_2, X_3, \dots, X_{100})$. How can we get a random vector? Random vector should consist of random variables - does that mean, that each of the fish get's it's own random variable? Or does it mean, that the random vector gives us values of random events and those random events mean : I will catch $100$ of fish from Atlantic is $1$ event and I will catch $100$ fish from Pacific is second event? The second one seems right to me. Ok let's say we have this random vector To compute the estimation: $E(\bar x)=\frac 1n \sum_{i = 1}^{100}x_i$ $N$ = all fish in sea Now does $x_i$ represent the numerical values of toxicity of each fish caught from our random vector $(X_1, X_2, \dots)$? So if we would be able to catch all the fish in sea, wouldn't this be just an average? Maybe the best thing to understand this would be, if someone could estimate the expected value, when for example we caught $10$ fish: toxicity of each fish: $5,2,7,8,9,1,1,1,2,1 - $ from pool of $100$ fish. Thanks for replies!","I am trying to understand this problem, however I can't get past some of the definitions used when estimating the expected value. What I would need is to confirm or disprove my conclusions - I read school materials and tried to find the answers on internet. Let's say we measure how much are sea fish toxic and we want to know the expected value of toxicity, when we catch some fish. From my understanding we should do something like this:  If we knew the random variable of toxicity $X$, we could compute the expected value, but we don't know that. If we want to estimate the expected value, the estimation is: $E(\bar x)= u$, where $\bar x$ is sample mean. So we catch, for example, $100$ fish and measure the toxicity - now is the part, where I am lost: In my scripts it says, that this is a random sample, from which we get random vector $(X_1, X_2, X_3, \dots, X_{100})$. How can we get a random vector? Random vector should consist of random variables - does that mean, that each of the fish get's it's own random variable? Or does it mean, that the random vector gives us values of random events and those random events mean : I will catch $100$ of fish from Atlantic is $1$ event and I will catch $100$ fish from Pacific is second event? The second one seems right to me. Ok let's say we have this random vector To compute the estimation: $E(\bar x)=\frac 1n \sum_{i = 1}^{100}x_i$ $N$ = all fish in sea Now does $x_i$ represent the numerical values of toxicity of each fish caught from our random vector $(X_1, X_2, \dots)$? So if we would be able to catch all the fish in sea, wouldn't this be just an average? Maybe the best thing to understand this would be, if someone could estimate the expected value, when for example we caught $10$ fish: toxicity of each fish: $5,2,7,8,9,1,1,1,2,1 - $ from pool of $100$ fish. Thanks for replies!",,"['probability', 'statistics', 'estimation']"
1,Probability answer cross check,Probability answer cross check,,"A researcher wishes to conduct a study of the color preferences of new car buyers. Suppose that 40% of this population prefers the color red. If 16 buyers are randomly selected, what is the probability that at least 2 buyers would prefer red? Round your answer to four decimal places. My method: o.40 * 16 / 2 = 3.2 Did I get it right? if not, what is the best approach","A researcher wishes to conduct a study of the color preferences of new car buyers. Suppose that 40% of this population prefers the color red. If 16 buyers are randomly selected, what is the probability that at least 2 buyers would prefer red? Round your answer to four decimal places. My method: o.40 * 16 / 2 = 3.2 Did I get it right? if not, what is the best approach",,"['probability', 'statistics']"
2,How to describe a statistical dataset more precisely?,How to describe a statistical dataset more precisely?,,"I am a newbie in stat and I need to be able to do this for a GIS application: Say I have the following dataset of only two possible values $0$ and $1$: $0,0,0,1,1,1,0,1$ The mean would equal $\frac{4}{8}=0.5$ Thus to find out how many $1$'s were in the intial dataset, we would simply multiply the mean by the total number of values and get $4$. Now consider a dataset with $\it three$ distinct values, $0,0.5$ and $1$ $0,0.5,0.5,1,1,1,0,0$ The mean would again equal $\frac{4}{8}=0.5$. In this case, would there be a way of determining how many values of $0.5$ and how many values of $1$ were in the dataset? Perhaps using the standard deviation, sum, median, or range?","I am a newbie in stat and I need to be able to do this for a GIS application: Say I have the following dataset of only two possible values $0$ and $1$: $0,0,0,1,1,1,0,1$ The mean would equal $\frac{4}{8}=0.5$ Thus to find out how many $1$'s were in the intial dataset, we would simply multiply the mean by the total number of values and get $4$. Now consider a dataset with $\it three$ distinct values, $0,0.5$ and $1$ $0,0.5,0.5,1,1,1,0,0$ The mean would again equal $\frac{4}{8}=0.5$. In this case, would there be a way of determining how many values of $0.5$ and how many values of $1$ were in the dataset? Perhaps using the standard deviation, sum, median, or range?",,"['probability', 'statistics', 'data-analysis']"
3,Estimate $m$ using method of maximum likelihood.,Estimate  using method of maximum likelihood.,m,"In the box there are $91$ balls, where $m$ are red, and the rest are blue. To estimate unknown parameter $m$ , at once $19$ balls are drawn, $7$ being red and $12$ being blue. Based on the given sample, estimate $m$ using method of maximum likelihood. I'm aware this is the hypergeometric distribution in question. I'm having trouble finding the maximum using MLE and how would I use these $19$ drawn balls to get the answer?","In the box there are balls, where are red, and the rest are blue. To estimate unknown parameter , at once balls are drawn, being red and being blue. Based on the given sample, estimate using method of maximum likelihood. I'm aware this is the hypergeometric distribution in question. I'm having trouble finding the maximum using MLE and how would I use these drawn balls to get the answer?",91 m m 19 7 12 m 19,"['probability', 'statistics', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
4,Boys and girls statistic problem,Boys and girls statistic problem,,"In a class consisting of $18$ boys and $12$ girls the Professor asks a question. Each boy has a probability $1/3$ to know the answer, while corresponding probability for each girl is $1/2$. What is the expected number of students who can answer correctly ?","In a class consisting of $18$ boys and $12$ girls the Professor asks a question. Each boy has a probability $1/3$ to know the answer, while corresponding probability for each girl is $1/2$. What is the expected number of students who can answer correctly ?",,"['probability', 'statistics']"
5,"If $X,Y$ are positive exchangeable random variables, then $E\left(\frac{X}{Y}\right)\geq 1$.","If  are positive exchangeable random variables, then .","X,Y E\left(\frac{X}{Y}\right)\geq 1","I would like to show that if $X,Y$ are positive exchangeable random variables, then $E\left(\frac{X}{Y}\right)\geq 1$. I have used several approaches, one is iterated condtioning, the other being the Cauchy-Schwarz Inequality, but have not been able to find this bound. Does anyone have any ideas? I know also that a more general condition exists under which the inequality holds as well, which should involve finding concave functions and using Jensen's Inequality. Thanks!","I would like to show that if $X,Y$ are positive exchangeable random variables, then $E\left(\frac{X}{Y}\right)\geq 1$. I have used several approaches, one is iterated condtioning, the other being the Cauchy-Schwarz Inequality, but have not been able to find this bound. Does anyone have any ideas? I know also that a more general condition exists under which the inequality holds as well, which should involve finding concave functions and using Jensen's Inequality. Thanks!",,"['probability', 'statistics']"
6,How to compute V(N|N+S=2) for a given tableau?,How to compute V(N|N+S=2) for a given tableau?,,"The problem state: Let N denote the number of accidents occurring during one month on the   northbound side of a highway and let S denote the number occurring on   the southbound side. Suppose that N and S are jointly distributed as   indicated in the table. N\S            0           1           2       3 or more   0           0.4         0.06       *0.10*     0.04   1           0.10       *0.18*       0.08      0.03   2          *0.12*       0.06        0.05      0.02   3 or more   0.05        0.04        0.02      0.01               0.67        0.34        0.25 I compute the sum of the colum with S=0 is => 0.67  S=1 is => 0.34  S=2 IS => 0.25 Then by def. I have V(N|N+S=2)=E(N^2|N+S=2)-E(N|N+S=2) And, P(N=n|N+S=2)=P(N=n,S=2-n)/P(S=n-2) Then for n=0,1,2 I have: P(N=0|N+S=2)=0.10/0.25=2/5  P(N=1|N+S=2)=0.18/0.34=9/17  P(N=2|N+S=2)=0.12/0.67=12/67 Finaly I can compute: E(N|N+S=2)=0*2/5+1*9/17+2*12/67=(1011/1139)  E(N^2|N+S=2)=0*2/5+1*9/17+4*12/67=1.2458 Variance asked: V(N|N+S=2)=E(N^2|N+S=2)-E(N|N+S=2)=0.3582 My promblem is: That I can't find my error, because the book said the answer is 0.5475 but ny number doesn't much. If someone can't tell me where is my error I will be happy. Thanks!","The problem state: Let N denote the number of accidents occurring during one month on the   northbound side of a highway and let S denote the number occurring on   the southbound side. Suppose that N and S are jointly distributed as   indicated in the table. N\S            0           1           2       3 or more   0           0.4         0.06       *0.10*     0.04   1           0.10       *0.18*       0.08      0.03   2          *0.12*       0.06        0.05      0.02   3 or more   0.05        0.04        0.02      0.01               0.67        0.34        0.25 I compute the sum of the colum with S=0 is => 0.67  S=1 is => 0.34  S=2 IS => 0.25 Then by def. I have V(N|N+S=2)=E(N^2|N+S=2)-E(N|N+S=2) And, P(N=n|N+S=2)=P(N=n,S=2-n)/P(S=n-2) Then for n=0,1,2 I have: P(N=0|N+S=2)=0.10/0.25=2/5  P(N=1|N+S=2)=0.18/0.34=9/17  P(N=2|N+S=2)=0.12/0.67=12/67 Finaly I can compute: E(N|N+S=2)=0*2/5+1*9/17+2*12/67=(1011/1139)  E(N^2|N+S=2)=0*2/5+1*9/17+4*12/67=1.2458 Variance asked: V(N|N+S=2)=E(N^2|N+S=2)-E(N|N+S=2)=0.3582 My promblem is: That I can't find my error, because the book said the answer is 0.5475 but ny number doesn't much. If someone can't tell me where is my error I will be happy. Thanks!",,"['probability', 'statistics']"
7,How to find the inverse distribution function of a mixture random variable?,How to find the inverse distribution function of a mixture random variable?,,"Let $X,Y, Z$ be three random variables such that  $$F_Z(x) = wF_X(x) + (1-w)F_Y(x)$$ Then is it true that $$F_{Z}^{-1}(x) = wF_{X}^{-1}(x) + (1-w)F_{Y}^{-1}(x)?$$ Or, how do we fix the $F_{Z}^{-1}$? Thanks everyone for you cooperation.","Let $X,Y, Z$ be three random variables such that  $$F_Z(x) = wF_X(x) + (1-w)F_Y(x)$$ Then is it true that $$F_{Z}^{-1}(x) = wF_{X}^{-1}(x) + (1-w)F_{Y}^{-1}(x)?$$ Or, how do we fix the $F_{Z}^{-1}$? Thanks everyone for you cooperation.",,"['probability', 'statistics']"
8,How old are the oldest 12% of readers of this paper? The inverse of a Normal.,How old are the oldest 12% of readers of this paper? The inverse of a Normal.,,"The problem said, The ages of subscribers to a certain newspaper are normally   distributed with mean   $35.5$ years and standard deviation $4.8$. (a) What percentage of readers are between $30$ and $40$ years old? I have: mean$=35.5$         std$=4.8$ And $P(30<x<40)=P((30-35.5)/4.8<z<(40-35.5)/4.8)=0.699817 = 69.9817\%...$ (b) How old are the oldest $12\%$ of readers of this paper? $P(X>x)=0.12$   I try to use the mean of $35.5$ and sed $4.8$ in my calculator to obtain the value of the age required, but it is not right due to the answer book said$=41.14$ years. What is the correct way to solve this point (b)?. Thanks!","The problem said, The ages of subscribers to a certain newspaper are normally   distributed with mean   $35.5$ years and standard deviation $4.8$. (a) What percentage of readers are between $30$ and $40$ years old? I have: mean$=35.5$         std$=4.8$ And $P(30<x<40)=P((30-35.5)/4.8<z<(40-35.5)/4.8)=0.699817 = 69.9817\%...$ (b) How old are the oldest $12\%$ of readers of this paper? $P(X>x)=0.12$   I try to use the mean of $35.5$ and sed $4.8$ in my calculator to obtain the value of the age required, but it is not right due to the answer book said$=41.14$ years. What is the correct way to solve this point (b)?. Thanks!",,"['probability', 'statistics']"
9,"Probability in the board game ""Istanbul""","Probability in the board game ""Istanbul""",,"I tried this game yesterday with a couple of friends (really interesting, although I did not win, I would definitely recommend it) here is a small piece of the game: A player first calls out a number between 2-12,(name this variable x) then rolls two dices, (name the result y) if $y \ge x$, then player gets x number of dollars. If $y<x$, then the player only gets 2 dollars (the lowest possible value) and if the player has completed a certain task he will also have the following ability: After rolling two dices, the player can choose either re-roll or replace the number of one dice with the number 4 My question is what should my strategy be before and after I completed the task given that I want to maximize my revenue Before the task, I am thinking... $$E(12)=12*{1\over36}+2*{35\over36}=2.27$$ $$E(11)=11*{3\over36}+2*{33\over36}=2.75$$ $$E(10)=10*{6\over36}+2*{30\over36}=3.33$$ $$E(9)=9*{10\over36}+2*{26\over36}=3.94$$ $$E(8)=8*{15\over36}+2*{21\over36}=4.5$$ $$E(7)=7*{21\over36}+2*{15\over36}=4.92$$ to be honest I think calling 7 is pretty good, you have like 60% chance of getting it, 7 is the most common number for 2 dices, it also has the highest expected value in this case, is it a coincidence? $$E(6)=6*{26\over36}+2*{10\over36}=4.89$$ $$E(5)=5*{30\over36}+2*{6\over36}=4.5$$ After the task, its hard for me to calculate $$E(7)=7*{32\over36}+{4\over36}*(EE(7))=6.768$$ I feel like this is correct, $32\over36$ is the probability of getting at least 1 dice $\ge$ 3 when rolling two dices. $EE(7)=7*{21\over36}+2*{15\over36}=4.92$ which is the expectation for 7 when I don't have the ability to replace/re-roll and if I was right about my calculation E(8)=7.125 and it should be the biggest expectation ? This is pretty neat. I thought even though I completed the task, the expectation distribution should be the same as the previous (7 is still the most likely number? 90+?) now what should my strategy be? completing the task is going to cost me some time...well it raised my lower bound by a lot(previous 2, now 5:4+anything) but if I just call out 8 every time... I am getting 1+ compare to calling 7 every time(even though the expected value is differ by 2.2, still not much) How should I estimate the cost of getting the task done? can someone check my answers? and answer my doubts? great thanks...I need my revenge :)","I tried this game yesterday with a couple of friends (really interesting, although I did not win, I would definitely recommend it) here is a small piece of the game: A player first calls out a number between 2-12,(name this variable x) then rolls two dices, (name the result y) if $y \ge x$, then player gets x number of dollars. If $y<x$, then the player only gets 2 dollars (the lowest possible value) and if the player has completed a certain task he will also have the following ability: After rolling two dices, the player can choose either re-roll or replace the number of one dice with the number 4 My question is what should my strategy be before and after I completed the task given that I want to maximize my revenue Before the task, I am thinking... $$E(12)=12*{1\over36}+2*{35\over36}=2.27$$ $$E(11)=11*{3\over36}+2*{33\over36}=2.75$$ $$E(10)=10*{6\over36}+2*{30\over36}=3.33$$ $$E(9)=9*{10\over36}+2*{26\over36}=3.94$$ $$E(8)=8*{15\over36}+2*{21\over36}=4.5$$ $$E(7)=7*{21\over36}+2*{15\over36}=4.92$$ to be honest I think calling 7 is pretty good, you have like 60% chance of getting it, 7 is the most common number for 2 dices, it also has the highest expected value in this case, is it a coincidence? $$E(6)=6*{26\over36}+2*{10\over36}=4.89$$ $$E(5)=5*{30\over36}+2*{6\over36}=4.5$$ After the task, its hard for me to calculate $$E(7)=7*{32\over36}+{4\over36}*(EE(7))=6.768$$ I feel like this is correct, $32\over36$ is the probability of getting at least 1 dice $\ge$ 3 when rolling two dices. $EE(7)=7*{21\over36}+2*{15\over36}=4.92$ which is the expectation for 7 when I don't have the ability to replace/re-roll and if I was right about my calculation E(8)=7.125 and it should be the biggest expectation ? This is pretty neat. I thought even though I completed the task, the expectation distribution should be the same as the previous (7 is still the most likely number? 90+?) now what should my strategy be? completing the task is going to cost me some time...well it raised my lower bound by a lot(previous 2, now 5:4+anything) but if I just call out 8 every time... I am getting 1+ compare to calling 7 every time(even though the expected value is differ by 2.2, still not much) How should I estimate the cost of getting the task done? can someone check my answers? and answer my doubts? great thanks...I need my revenge :)",,"['probability', 'statistics', 'game-theory']"
10,Determining the population mean,Determining the population mean,,"So I asked 10 random people in the gym some questions and this is how it went: Do you know how to swim? If the person responded no then I was not allowed to ask them any more questions. However, if the person responded yes then I had to ask them their age. In addition, I took note of their gender. Basically, I have a table that looks something like this: Now, my professor wants us to do this question: Estimate the average age of males and females who can swim in San Francisco. So what I did was took the sum (which was $150$) of the age and divided by $10$ and got the average age to be $15$. Now this is where I am confused: My professor asked ""Include your best guess as to what the population mean will be"" but how would I be able to get the population mean from this data if this is a sample? Also, the professor gave a part for correlation and said ""Describe direction, form, and strength of the relationship."" However, don't you need two different variables to create a relationship? In this case, the only variable is age.","So I asked 10 random people in the gym some questions and this is how it went: Do you know how to swim? If the person responded no then I was not allowed to ask them any more questions. However, if the person responded yes then I had to ask them their age. In addition, I took note of their gender. Basically, I have a table that looks something like this: Now, my professor wants us to do this question: Estimate the average age of males and females who can swim in San Francisco. So what I did was took the sum (which was $150$) of the age and divided by $10$ and got the average age to be $15$. Now this is where I am confused: My professor asked ""Include your best guess as to what the population mean will be"" but how would I be able to get the population mean from this data if this is a sample? Also, the professor gave a part for correlation and said ""Describe direction, form, and strength of the relationship."" However, don't you need two different variables to create a relationship? In this case, the only variable is age.",,"['statistics', 'means']"
11,Find Least Squares Regression Line,Find Least Squares Regression Line,,"I have a problem where I need to find the least squares regression line. I have found $\beta_0$ and $\beta_1$ in the following equation $$y = \beta_0 + \beta_1 \cdot x + \epsilon$$ So I have both the vectors $y$ and $x$. I know that $\hat{y}$ the vector predictor of $y$ is $x \cdot \beta$ and that the residual vector is $\epsilon = y - \hat{y}$. I know also that the least squares regression line looks something like this $$\hat{y} = a + b \cdot x$$ and that what I need to find is $a$ and $b$, but I don't know exactly how to do it. Currently I am using Matlab, and I need to do it in Matlab. Any idea how should I proceed, based on the fact that I am using Matlab? Correct me if I did/said something wrong anyway.","I have a problem where I need to find the least squares regression line. I have found $\beta_0$ and $\beta_1$ in the following equation $$y = \beta_0 + \beta_1 \cdot x + \epsilon$$ So I have both the vectors $y$ and $x$. I know that $\hat{y}$ the vector predictor of $y$ is $x \cdot \beta$ and that the residual vector is $\epsilon = y - \hat{y}$. I know also that the least squares regression line looks something like this $$\hat{y} = a + b \cdot x$$ and that what I need to find is $a$ and $b$, but I don't know exactly how to do it. Currently I am using Matlab, and I need to do it in Matlab. Any idea how should I proceed, based on the fact that I am using Matlab? Correct me if I did/said something wrong anyway.",,"['statistics', 'convex-optimization']"
12,"From actuarial exam: Calculate the variance of the retirement package for a new employee, > given that the value is at least 10.","From actuarial exam: Calculate the variance of the retirement package for a new employee, > given that the value is at least 10.",,"The distribution of values of the retirement package offered by a   company to new employees is modeled by the probability density   function, Calculate the variance of the retirement package for a new employee,   given that the value is at least 10. I know that, Var(Y)= E(V(Y|X>10)+ V(E(Y|X>10)), is an equation that show a relationship between both. But I have no idea how to do this exercise. Thanks, any advise it will be apreciated.","The distribution of values of the retirement package offered by a   company to new employees is modeled by the probability density   function, Calculate the variance of the retirement package for a new employee,   given that the value is at least 10. I know that, Var(Y)= E(V(Y|X>10)+ V(E(Y|X>10)), is an equation that show a relationship between both. But I have no idea how to do this exercise. Thanks, any advise it will be apreciated.",,"['probability', 'probability-theory', 'statistics']"
13,"3 Drawers and 2 kind of socks, what is the probability that you get a pair (red or black)?","3 Drawers and 2 kind of socks, what is the probability that you get a pair (red or black)?",,"There are 3 drawers in a dresser`, and you are equally likely to pick   any of the three. In drawer 1, there are 2 black socks and 3 red   socks. In drawer 2, there are 3 black socks and 2 red socks. In drawer   3, there are 3 black socks and 3 red socks. Once you have randomly   selected a drawer, you randomly pull out a sock of that drawer. If you were to randomly choose a drawer and the draw two socks   from that drawer, what is the probability that you get a pair (red   or black)? Where is my mistake: P(R OR B|D1)= 2/5 * 3/4 = 3/10  P(R OR B|D2)= 3/5 * 2/4 = 3/10 P(R OR B|D3)= 1/2 * 1/2 = 1/4  P(R OR B)=P(R OR B|D1) P(D1) + P(R OR B|D2) P(D2) +P(R OR B|D3) P(D3) The correct answer is 0.4, but if I use my work I can not reach that value. Can you help me to know where is my mistake? Thanks, comunity.","There are 3 drawers in a dresser`, and you are equally likely to pick   any of the three. In drawer 1, there are 2 black socks and 3 red   socks. In drawer 2, there are 3 black socks and 2 red socks. In drawer   3, there are 3 black socks and 3 red socks. Once you have randomly   selected a drawer, you randomly pull out a sock of that drawer. If you were to randomly choose a drawer and the draw two socks   from that drawer, what is the probability that you get a pair (red   or black)? Where is my mistake: P(R OR B|D1)= 2/5 * 3/4 = 3/10  P(R OR B|D2)= 3/5 * 2/4 = 3/10 P(R OR B|D3)= 1/2 * 1/2 = 1/4  P(R OR B)=P(R OR B|D1) P(D1) + P(R OR B|D2) P(D2) +P(R OR B|D3) P(D3) The correct answer is 0.4, but if I use my work I can not reach that value. Can you help me to know where is my mistake? Thanks, comunity.",,"['probability', 'probability-theory', 'statistics']"
14,Prove the following using Chebyshev and Markov inequality.,Prove the following using Chebyshev and Markov inequality.,,"Suppose $X$ is a random variable with mean $\mu$ and variance $\sigma^2$ . Show that $$P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2}$$ Question: I know the exercise wants me to use Markov inequality and  Chebyshev inequality, but I can't reach the same answer. If someone can help me, it will be appreciated. Thanks!","Suppose is a random variable with mean and variance . Show that Question: I know the exercise wants me to use Markov inequality and  Chebyshev inequality, but I can't reach the same answer. If someone can help me, it will be appreciated. Thanks!",X \mu \sigma^2 P(|X-\mu| \geq k\sigma) \leq \frac{1}{k^2},"['probability', 'probability-theory', 'statistics']"
15,Sample statistics probability bernoulli trials,Sample statistics probability bernoulli trials,,"Problem There are two restaurants on the campus of a university. Each can feed 120 students. We know that there are 200 students attending the university who will want to eat lunch in one of the restaurants. The restaurant is chosen randomly by the student, for example by tossing a fair coin. What is the probability that there will not be enough meals in one of the restaurants? My Answer Each students choice is a Bernoulli trial with even probabilities on each of the two outcomes that is: $X_i = 1 $ with $ p=1/2$ or $X_i = 0 $ with $ p=1/2$ where $X_i$ is a random variable which denotes student i's choice. So we want $P(\Sigma_1^{200} X_i \gt 120) \bigcup P(\Sigma_1^{200} X_i \lt 80)$ But from here I don't really know where to go with it? Any help would be great.","Problem There are two restaurants on the campus of a university. Each can feed 120 students. We know that there are 200 students attending the university who will want to eat lunch in one of the restaurants. The restaurant is chosen randomly by the student, for example by tossing a fair coin. What is the probability that there will not be enough meals in one of the restaurants? My Answer Each students choice is a Bernoulli trial with even probabilities on each of the two outcomes that is: $X_i = 1 $ with $ p=1/2$ or $X_i = 0 $ with $ p=1/2$ where $X_i$ is a random variable which denotes student i's choice. So we want $P(\Sigma_1^{200} X_i \gt 120) \bigcup P(\Sigma_1^{200} X_i \lt 80)$ But from here I don't really know where to go with it? Any help would be great.",,"['probability', 'probability-theory', 'statistics', 'statistical-inference', 'order-statistics']"
16,What does tilde mean?,What does tilde mean?,,"In statistics books, I have seen expressions like: $\frac{(n-1)S^2}{\sigma^2} \sim \chi{( n-1 )}$ In this context, what does the $\sim$ mean?","In statistics books, I have seen expressions like: $\frac{(n-1)S^2}{\sigma^2} \sim \chi{( n-1 )}$ In this context, what does the $\sim$ mean?",,"['probability', 'statistics', 'probability-distributions']"
17,What is the probability that if three dice are thrown that the maximum number shown is $5$?,What is the probability that if three dice are thrown that the maximum number shown is ?,5,"'You throw $3$ regular $6$-sided dice. What is the probability that the one (or the ones ) that show the maximum amount of ""eyes"" show $5$?' Can't seem to get my head around this one. Unsure on where to start, thankful for any help!","'You throw $3$ regular $6$-sided dice. What is the probability that the one (or the ones ) that show the maximum amount of ""eyes"" show $5$?' Can't seem to get my head around this one. Unsure on where to start, thankful for any help!",,"['probability', 'statistics']"
18,"Find the pdf of $Y = g(X)$, where $X$ is a uniform random variable","Find the pdf of , where  is a uniform random variable",Y = g(X) X,"The question is as follows: Let $X$ be a uniform random variable over $(-1,2)$. Let $g(x) = |x|$. Find the pdf of $Y = g(X)$. And here is my take so far: $$f(x) = \begin{cases} 1/2 & \text{ if } -1 \leq x \leq 2, \\        0   &  \text{ otherwise.} \end{cases}$$ The cdf of $Y$: $F_Y(y) = P(Y \leq y) = P(|X| \leq y)$. and from here I know I have to divide intervals but I don't know how to do that. I assume I have to divide it in $4$ different cases where if $y > 0$, if $y > 2$, or $0 < y <2$, $y > 2$? And find cdf and take derivative to find pdf.","The question is as follows: Let $X$ be a uniform random variable over $(-1,2)$. Let $g(x) = |x|$. Find the pdf of $Y = g(X)$. And here is my take so far: $$f(x) = \begin{cases} 1/2 & \text{ if } -1 \leq x \leq 2, \\        0   &  \text{ otherwise.} \end{cases}$$ The cdf of $Y$: $F_Y(y) = P(Y \leq y) = P(|X| \leq y)$. and from here I know I have to divide intervals but I don't know how to do that. I assume I have to divide it in $4$ different cases where if $y > 0$, if $y > 2$, or $0 < y <2$, $y > 2$? And find cdf and take derivative to find pdf.",,"['statistics', 'probability-distributions', 'random-variables', 'uniform-distribution']"
19,Probability of average people coming to work,Probability of average people coming to work,,"I am working on this: A restaurant employs $9$ people. $2$ bartenders, $3$ waiters and $4$ work in the kitchen. It has been observed that   on any day the probability of an employee to call in sick is $2\%$ for the bartenders, $3\%$ for the waiters and $1\%$ for   the kitchen staff. Calculate the average number of the employees coming to work. I did this: P(no employee missing) = $0.98$^$2$ * $0.97$^$3$ * $0.99$^$4$ = $0.842$ avg number of people coming to work = $9$ *P(no employee is missing) = $9*0.842$ = $7.578$ = $8$ But I have a feeling it might not be correct. Any thoughts?","I am working on this: A restaurant employs $9$ people. $2$ bartenders, $3$ waiters and $4$ work in the kitchen. It has been observed that   on any day the probability of an employee to call in sick is $2\%$ for the bartenders, $3\%$ for the waiters and $1\%$ for   the kitchen staff. Calculate the average number of the employees coming to work. I did this: P(no employee missing) = $0.98$^$2$ * $0.97$^$3$ * $0.99$^$4$ = $0.842$ avg number of people coming to work = $9$ *P(no employee is missing) = $9*0.842$ = $7.578$ = $8$ But I have a feeling it might not be correct. Any thoughts?",,"['probability', 'statistics']"
20,Finding PDF from Multivariate Distribution and integrating out extra Variables,Finding PDF from Multivariate Distribution and integrating out extra Variables,,"Suppose X, Y and Z have the joint pdf $f(x,y,z)=6(1+x+y+z)^{-4}$,  $x>0,y>0,z>0$ for $ U=x+y+z$ find pdf of $U$ so... $v=y+z$,  $2<v<\infty$ $w=y$,   $0<w$ $z=w-v$, $y=w$ so $f_{u,v,w}(u,v,w)=6(1+u-v-w+w+w-v)^{-4}|J|=6(1+u-2v+w)^{-4}|J|$ where J=1 from the jacobian to find the pdf of $U$ we must integrate out $v$ and $w$ which gets me $\int_1^\infty \int_2^\infty 6(1+u-2v+w)^{-4}dvdw=\int_1^\infty (-3+u+w)^{-3}dw$ $=\frac{1}{2}(u-2)^{-2}$ , $3<u<\infty$ so $f_u(u)=\frac{1}{2}(u-2)^{-2}$ , $3<u<\infty$","Suppose X, Y and Z have the joint pdf $f(x,y,z)=6(1+x+y+z)^{-4}$,  $x>0,y>0,z>0$ for $ U=x+y+z$ find pdf of $U$ so... $v=y+z$,  $2<v<\infty$ $w=y$,   $0<w$ $z=w-v$, $y=w$ so $f_{u,v,w}(u,v,w)=6(1+u-v-w+w+w-v)^{-4}|J|=6(1+u-2v+w)^{-4}|J|$ where J=1 from the jacobian to find the pdf of $U$ we must integrate out $v$ and $w$ which gets me $\int_1^\infty \int_2^\infty 6(1+u-2v+w)^{-4}dvdw=\int_1^\infty (-3+u+w)^{-3}dw$ $=\frac{1}{2}(u-2)^{-2}$ , $3<u<\infty$ so $f_u(u)=\frac{1}{2}(u-2)^{-2}$ , $3<u<\infty$",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
21,A Basic Probability Question - I am getting the wrong answer,A Basic Probability Question - I am getting the wrong answer,,"Problem: In a deck of $52$ cards there are $4$ kings. A card is drawn at random from the deck and its face value noted; then the card is returned. This procedure is followed $4$ times. Compute the probability that there are exactly $2$ kings in the $4$ selected cards if it is known that there is at least one king in those selected. Answer: Let $p$ be the probability we seek. Let $p_2$ be the probability that we draw exactly $2$ kings. Let $p_1$ be the probability that we draw at least $1$ king. \begin{eqnarray*} p &=& \frac{p_2}{p_1} \\ p_2 &=& {13 \choose 2}{(\frac{4}{52})^2}{(\frac{48}{52})}^2 \\ {13 \choose 2} &=& \frac{13(12)}{2(1)} = 13(6) \\ p_2 &=& 13(6){(\frac{1}{13})^2}{(\frac{12}{13})}^2 = \frac{6(12)^2}{13^3} \\ \end{eqnarray*} Let $p_0$ be the probability that we draw no kings. \begin{eqnarray*} p_0 &=& (\frac{48}{52})^4 = (\frac{12}{13})^4 \\ p_1 &=& 1 - p_0 = 1 - (\frac{12}{13})^4 = \frac{13^4 - 12^4}{13^4} \\ p &=& \frac{ \frac{6(12)^2}{13^3} }{ \frac{13^4 - 12^4}{13^4} } = \frac{6(13)(12)^2}{13^4 - 12^4} \\ \end{eqnarray*} However, the book gets: \begin{eqnarray*} p = \frac{6(12)^2}{13^4 - 12^4} \\ \end{eqnarray*} I am hoping that somebody can tell me where I went wrong. ~","Problem: In a deck of $52$ cards there are $4$ kings. A card is drawn at random from the deck and its face value noted; then the card is returned. This procedure is followed $4$ times. Compute the probability that there are exactly $2$ kings in the $4$ selected cards if it is known that there is at least one king in those selected. Answer: Let $p$ be the probability we seek. Let $p_2$ be the probability that we draw exactly $2$ kings. Let $p_1$ be the probability that we draw at least $1$ king. \begin{eqnarray*} p &=& \frac{p_2}{p_1} \\ p_2 &=& {13 \choose 2}{(\frac{4}{52})^2}{(\frac{48}{52})}^2 \\ {13 \choose 2} &=& \frac{13(12)}{2(1)} = 13(6) \\ p_2 &=& 13(6){(\frac{1}{13})^2}{(\frac{12}{13})}^2 = \frac{6(12)^2}{13^3} \\ \end{eqnarray*} Let $p_0$ be the probability that we draw no kings. \begin{eqnarray*} p_0 &=& (\frac{48}{52})^4 = (\frac{12}{13})^4 \\ p_1 &=& 1 - p_0 = 1 - (\frac{12}{13})^4 = \frac{13^4 - 12^4}{13^4} \\ p &=& \frac{ \frac{6(12)^2}{13^3} }{ \frac{13^4 - 12^4}{13^4} } = \frac{6(13)(12)^2}{13^4 - 12^4} \\ \end{eqnarray*} However, the book gets: \begin{eqnarray*} p = \frac{6(12)^2}{13^4 - 12^4} \\ \end{eqnarray*} I am hoping that somebody can tell me where I went wrong. ~",,"['probability', 'combinatorics', 'statistics', 'probability-distributions']"
22,What does it mean to raise a distribution to a power?,What does it mean to raise a distribution to a power?,,"What does it mean in statistics if we raise distributions to the powers? Like if $X$ is uniformly distributed on $[0,1]$ then what is the distribution of $X^3$ or the expected value of $X^3$?","What does it mean in statistics if we raise distributions to the powers? Like if $X$ is uniformly distributed on $[0,1]$ then what is the distribution of $X^3$ or the expected value of $X^3$?",,['statistics']
23,Finding integral of $2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx$,Finding integral of,2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx,Finding integral of $$2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx$$ using integral by parts I get that Finding integral of $$2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx = \dfrac{n}{\lambda} \cdot \dfrac{n-1}{\lambda} \int_0^\infty x^{2n-4} \cdot x e^{-\lambda x^2} \ dx$$ so I assume that $$2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx = \dfrac{n!}{\lambda^n}$$. How can I formally prove this?,Finding integral of $$2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx$$ using integral by parts I get that Finding integral of $$2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx = \dfrac{n}{\lambda} \cdot \dfrac{n-1}{\lambda} \int_0^\infty x^{2n-4} \cdot x e^{-\lambda x^2} \ dx$$ so I assume that $$2 \lambda\int_0^{\infty} x^{2n}xe^{-\lambda x^2} \ dx = \dfrac{n!}{\lambda^n}$$. How can I formally prove this?,,"['calculus', 'statistics']"
24,Show that that the following conditional probability equation holds,Show that that the following conditional probability equation holds,,"Let $X$ and $Y$ be random variables with finite means.  Prove that $$ \min E(Y-g(X))^2 =E(Y-E(Y\mid X))^2$$ holds, where $g(x)$ ranges over all functions and the min is of $g(x)$.  I honestly have no idea where to start with this equality.  Can someone perhaps guide me as to what I need to do with the min?  I think that's the biggest trouble right now. Is there some identity that can used to easily solve this?","Let $X$ and $Y$ be random variables with finite means.  Prove that $$ \min E(Y-g(X))^2 =E(Y-E(Y\mid X))^2$$ holds, where $g(x)$ ranges over all functions and the min is of $g(x)$.  I honestly have no idea where to start with this equality.  Can someone perhaps guide me as to what I need to do with the min?  I think that's the biggest trouble right now. Is there some identity that can used to easily solve this?",,"['probability', 'probability-theory', 'statistics']"
25,Probability - A coin is tossed 10 times and comes up heads about 60% of the time. What is ...,Probability - A coin is tossed 10 times and comes up heads about 60% of the time. What is ...,,"Probability - A coin comes up heads about 60% of the time.  If it is tossed 10 times, what is the probability that exactly between 5 and 7 heads occur consecutively? (I received the following clarification: It is the probability of getting exactly 5 consecutively or exactly 6 consecutively or exactly 7 consecutively.) I was thinking of a binomial distribution, but not so sure that will address the precise details or is there a chance this is 10 Bernoulli trials and we need to find the conditional probability that all successes will occur consecutively (ie, no two successes will be separated by any failures) and we need to give the number of successes between 5 and 7. Then I think the denominator will be the binomial distribution summing from k=5 to k=7, but then not sure of the numerator precisely. Please show details so I may understand the correct process.","Probability - A coin comes up heads about 60% of the time.  If it is tossed 10 times, what is the probability that exactly between 5 and 7 heads occur consecutively? (I received the following clarification: It is the probability of getting exactly 5 consecutively or exactly 6 consecutively or exactly 7 consecutively.) I was thinking of a binomial distribution, but not so sure that will address the precise details or is there a chance this is 10 Bernoulli trials and we need to find the conditional probability that all successes will occur consecutively (ie, no two successes will be separated by any failures) and we need to give the number of successes between 5 and 7. Then I think the denominator will be the binomial distribution summing from k=5 to k=7, but then not sure of the numerator precisely. Please show details so I may understand the correct process.",,"['probability', 'statistics']"
26,Efficient methods for drawing random numbers and Monte Carlo for Tsallis q-Gaussians,Efficient methods for drawing random numbers and Monte Carlo for Tsallis q-Gaussians,,"I would like to draw random numbers from the q-Gaussian used in ""Tsallis statistics."" This is specifically the distribution $$ f(x) = {\sqrt{\beta} \over C_q} e_q(-\beta x^2) $$ where $$ e_q(x) = [1+(1-q)x]^{1 \over 1-q} $$ $\beta$ is a free parameter, and $C_q$ is a normalization constant. (The general form of $C_q$ can be easily found so I'm not reproducing it here.)  In the limit that $q \rightarrow 1$ this goes to the ""usual"" Gaussian with exponential tails.  For $q<1$ it has finite support, which is not currently interesting to me.  For $q>1$, it has heavy tails, meaning the tails decay algebraically instead of exponentially. I'm interested in computing integrals using Monte Carlo methods that will give expectation values of various functions under q-Gaussians in cases where $q>1$.  Are there any established, efficient methods for drawing random numbers and/or computing Monte Carlo integrals with such ""heavy tail"" distributions?","I would like to draw random numbers from the q-Gaussian used in ""Tsallis statistics."" This is specifically the distribution $$ f(x) = {\sqrt{\beta} \over C_q} e_q(-\beta x^2) $$ where $$ e_q(x) = [1+(1-q)x]^{1 \over 1-q} $$ $\beta$ is a free parameter, and $C_q$ is a normalization constant. (The general form of $C_q$ can be easily found so I'm not reproducing it here.)  In the limit that $q \rightarrow 1$ this goes to the ""usual"" Gaussian with exponential tails.  For $q<1$ it has finite support, which is not currently interesting to me.  For $q>1$, it has heavy tails, meaning the tails decay algebraically instead of exponentially. I'm interested in computing integrals using Monte Carlo methods that will give expectation values of various functions under q-Gaussians in cases where $q>1$.  Are there any established, efficient methods for drawing random numbers and/or computing Monte Carlo integrals with such ""heavy tail"" distributions?",,"['statistics', 'normal-distribution', 'expectation', 'monte-carlo']"
27,Extrapolate data points from a series of averages,Extrapolate data points from a series of averages,,"Given a series of data points: $$\begin{array}{c|c|c|c|c|c|c|c|}   & \text{Monday} & \text{Tuesday} & \text{Wednesday} & \text{Thursday} & \text{Friday} & \text{Saturday} & \text{Sunday} \\ \hline \text{Data Points} & 34 & 38&52&12&54&22&33 \\ \hline \end{array}$$ I can compute mean averages: $$\begin{array}{c|c|c|c|c|c|c|c|}   & \text{M-W} & \text{T-Th} & \text{W-F} & \text{Th-S} & \text{F-Su} \\ \hline \text{Data Points} & 41.33 & 34.00 & 39.33 & 29.33 & 36.33 \\ \hline \end{array}$$ Now say I only have the series of averages. Two questions: I think it is impossible to extrapolate the individual data points. Is there a proof of this? Is it possible to extrapolate possible ranges for the individual data points? If so, how? Edit: Assume all data points are in the range 0-100.","Given a series of data points: $$\begin{array}{c|c|c|c|c|c|c|c|}   & \text{Monday} & \text{Tuesday} & \text{Wednesday} & \text{Thursday} & \text{Friday} & \text{Saturday} & \text{Sunday} \\ \hline \text{Data Points} & 34 & 38&52&12&54&22&33 \\ \hline \end{array}$$ I can compute mean averages: $$\begin{array}{c|c|c|c|c|c|c|c|}   & \text{M-W} & \text{T-Th} & \text{W-F} & \text{Th-S} & \text{F-Su} \\ \hline \text{Data Points} & 41.33 & 34.00 & 39.33 & 29.33 & 36.33 \\ \hline \end{array}$$ Now say I only have the series of averages. Two questions: I think it is impossible to extrapolate the individual data points. Is there a proof of this? Is it possible to extrapolate possible ranges for the individual data points? If so, how? Edit: Assume all data points are in the range 0-100.",,"['statistics', 'average', 'data-analysis', 'extrapolation']"
28,Joint Distribution function and joint density function,Joint Distribution function and joint density function,,"I am really confused with what the question is asking and where to start. Thanks in advance for help. Suppose $(a,b) $~uniform$([0,1]\times[0,1])$  (Does that mean we are given the join density function?) Determine the cdf $F: R^2 \to [0,1]$ Determine the density $f$ from $F$.","I am really confused with what the question is asking and where to start. Thanks in advance for help. Suppose $(a,b) $~uniform$([0,1]\times[0,1])$  (Does that mean we are given the join density function?) Determine the cdf $F: R^2 \to [0,1]$ Determine the density $f$ from $F$.",,"['probability', 'statistics', 'probability-distributions']"
29,Understanding of Independent Random Variables,Understanding of Independent Random Variables,,"Just wanted to clarify my understanding of independent random variables.  Say, (U,V) are random variables s.t. U=the number of trials needed for the first head and V=the number of trials needed for the second head If I want to explain that its not independent, is the reason that in this case the number of trials for one random variable depends on the other?  What I mean is that if it takes 1 trial to get one head and 2 trials for two heads, then that first head contributes to the latter variable.  Is my understanding correct, or should it be more mathematical instead?","Just wanted to clarify my understanding of independent random variables.  Say, (U,V) are random variables s.t. U=the number of trials needed for the first head and V=the number of trials needed for the second head If I want to explain that its not independent, is the reason that in this case the number of trials for one random variable depends on the other?  What I mean is that if it takes 1 trial to get one head and 2 trials for two heads, then that first head contributes to the latter variable.  Is my understanding correct, or should it be more mathematical instead?",,"['probability', 'probability-theory', 'statistics']"
30,Stochastic process: A bus with random numbers of passengers entering and exiting at each stop?,Stochastic process: A bus with random numbers of passengers entering and exiting at each stop?,,"A bus with infinite capacity runs on an infinite route with stops indexed $n = 1, 2, 3,\dots$ The bus arrives empty at stop $1$. When the bus arrives at stop $n$, each passenger on it gets off with probability $p$, independent of each other. Then all the waiting passengers get on. Let $W(n)$ be the number of passengers waiting at stop $n$, and suppose $\{W(n), n\ge0\}$ are iid Possion$(λ)$ random variables. Let $R(n)$ be the number of passengers on the bus when it leaves stop $n$. What is the distribution of $R(n)$? The answer is given as: $$Poisson(λ/p*(1-(1-p)^n))$$ My attempt: $$R(n)=Binomial(R(n-1),1-p)+Poisson(λ) = Poisson(R(n-1)*(1-p)+λ)$$ I don't know what to do past this, or if I'm thinking about this the right way.","A bus with infinite capacity runs on an infinite route with stops indexed $n = 1, 2, 3,\dots$ The bus arrives empty at stop $1$. When the bus arrives at stop $n$, each passenger on it gets off with probability $p$, independent of each other. Then all the waiting passengers get on. Let $W(n)$ be the number of passengers waiting at stop $n$, and suppose $\{W(n), n\ge0\}$ are iid Possion$(λ)$ random variables. Let $R(n)$ be the number of passengers on the bus when it leaves stop $n$. What is the distribution of $R(n)$? The answer is given as: $$Poisson(λ/p*(1-(1-p)^n))$$ My attempt: $$R(n)=Binomial(R(n-1),1-p)+Poisson(λ) = Poisson(R(n-1)*(1-p)+λ)$$ I don't know what to do past this, or if I'm thinking about this the right way.",,"['statistics', 'stochastic-processes', 'random-variables', 'poisson-distribution', 'binomial-distribution']"
31,"""standard co-deviation""","""standard co-deviation""",,"This is a terminology/notation question.  I swear I've seen covariance matrices written as \begin{bmatrix}   \sigma_x^2 & \sigma_{x,y}^2 \\   \sigma_{y,x}^2 & \sigma_y^2 \end{bmatrix} Given that $\sigma_x^2$ is a variance, then $\sigma_x$ is a standard deviation. Given that $\sigma_{x,y}^2$ is a covariance, what is $\sigma_{x,y}$ called? Does it have a name?","This is a terminology/notation question.  I swear I've seen covariance matrices written as \begin{bmatrix}   \sigma_x^2 & \sigma_{x,y}^2 \\   \sigma_{y,x}^2 & \sigma_y^2 \end{bmatrix} Given that $\sigma_x^2$ is a variance, then $\sigma_x$ is a standard deviation. Given that $\sigma_{x,y}^2$ is a covariance, what is $\sigma_{x,y}$ called? Does it have a name?",,"['probability', 'statistics', 'notation', 'terminology']"
32,Finding probability $P(x + y \le 1)$ given the joint pdf,Finding probability  given the joint pdf,P(x + y \le 1),"So in the problem I was given the joint pdf $f(x,y) = x + y$, $0< x <1$, $0< y <1$, $0$ elsewhere. I am tasked to find $P( x+y \le 1 )$. My intuition was to $\int_0^1\int_{1-x}^1 (x+y) \, dy \, dx $ working that out, i got the answer of 50%. Is my intuition correct?","So in the problem I was given the joint pdf $f(x,y) = x + y$, $0< x <1$, $0< y <1$, $0$ elsewhere. I am tasked to find $P( x+y \le 1 )$. My intuition was to $\int_0^1\int_{1-x}^1 (x+y) \, dy \, dx $ working that out, i got the answer of 50%. Is my intuition correct?",,['statistics']
33,Set theory possible values,Set theory possible values,,"I'm studying for an upcoming exam and a practice question stumped me. It is asking for possible values A), B), and C) could have. $C^c$ means the complement of $C$. Am I right when I reason that A) would just be any number smaller than 0.7, or is it warranted to be more precise? Family of events → [0, 1] $P(A \cup B \cup C) = 0.7$ A)  $P(A \cup B)$ B)  $P((A \cup B) \cap C^c)$ C)  $P(A^c \cap B^c \cap C^c)$","I'm studying for an upcoming exam and a practice question stumped me. It is asking for possible values A), B), and C) could have. $C^c$ means the complement of $C$. Am I right when I reason that A) would just be any number smaller than 0.7, or is it warranted to be more precise? Family of events → [0, 1] $P(A \cup B \cup C) = 0.7$ A)  $P(A \cup B)$ B)  $P((A \cup B) \cap C^c)$ C)  $P(A^c \cap B^c \cap C^c)$",,"['probability', 'measure-theory', 'statistics']"
34,Maintain the variance in a stream,Maintain the variance in a stream,,"I am trying to understand how to compute the variance online.  If I have the value 23 ten times, the value 10 five times and the value 4 three times, what is the variance? We know the mean is $(230+50+12)/18 \approx 16.22$  and the variance is approx $61.17$ . It is easy to maintain the mean online by just storing the sum and the number  of numbers we have seen. However, if the numbers arrived one at a time, how can we maintain the variance as the numbers arrive? That is without just storing all the numbers  and recalculating each time a new number arrives.","I am trying to understand how to compute the variance online.  If I have the value 23 ten times, the value 10 five times and the value 4 three times, what is the variance? We know the mean is $(230+50+12)/18 \approx 16.22$  and the variance is approx $61.17$ . It is easy to maintain the mean online by just storing the sum and the number  of numbers we have seen. However, if the numbers arrived one at a time, how can we maintain the variance as the numbers arrive? That is without just storing all the numbers  and recalculating each time a new number arrives.",,"['statistics', 'means']"
35,Histogram statistical comparison reference recommendation,Histogram statistical comparison reference recommendation,,"I am currently wrestling with several pairs of discrete probability distribution histograms (n = 60,000,000+ for each) and are seeking to determine the significance of any and all dissimilarity between the histograms in each pair. In my research meanderings, I have come across the Bhattarcharyya distance , which is nice and very straight forward to use - but, what is not clear to me is how to measure the statistical significance of the calculated distance. Is there a reference that explains the statistical significance of values determined when using the Bhattarcharyya distance?","I am currently wrestling with several pairs of discrete probability distribution histograms (n = 60,000,000+ for each) and are seeking to determine the significance of any and all dissimilarity between the histograms in each pair. In my research meanderings, I have come across the Bhattarcharyya distance , which is nice and very straight forward to use - but, what is not clear to me is how to measure the statistical significance of the calculated distance. Is there a reference that explains the statistical significance of values determined when using the Bhattarcharyya distance?",,['statistics']
36,Variance of unbiased estimator of a random sample from the uniform distribution?,Variance of unbiased estimator of a random sample from the uniform distribution?,,"Let X1,...,Xn be a random sample from the uniform distribution on the interval from 0 to theta for some theta>0.  I want to find the variance of the unbiased estimator. I know the unbiased estimator is Y= [(n+1)/n]*X(n) where X(n)=Max(X1,...X) but am lost on how to find the variance of Y.  Thanks so much.","Let X1,...,Xn be a random sample from the uniform distribution on the interval from 0 to theta for some theta>0.  I want to find the variance of the unbiased estimator. I know the unbiased estimator is Y= [(n+1)/n]*X(n) where X(n)=Max(X1,...X) but am lost on how to find the variance of Y.  Thanks so much.",,"['probability', 'statistics', 'expectation', 'uniform-distribution']"
37,"When the population variance is unknown, we should use t-distribution.","When the population variance is unknown, we should use t-distribution.",,It is clear that T-distribution should be used when the sample size is small and the population variance is unknown. My question is Why? Why we use t-distribution in this case? Anybody give me specific reasons for it?,It is clear that T-distribution should be used when the sample size is small and the population variance is unknown. My question is Why? Why we use t-distribution in this case? Anybody give me specific reasons for it?,,"['probability', 'statistics', 'probability-distributions']"
38,Z-score from Pvalue,Z-score from Pvalue,,What is the Z-score for a 10% confidence level (i.e. 0.1 pvalue)? I want the standard answer used for including in my thesis write up. I googled and used excel to calculate as well but they are all slightly different. Thank You.,What is the Z-score for a 10% confidence level (i.e. 0.1 pvalue)? I want the standard answer used for including in my thesis write up. I googled and used excel to calculate as well but they are all slightly different. Thank You.,,['statistics']
39,Flipping a fair coin three times: How to find independent/dependent events?,Flipping a fair coin three times: How to find independent/dependent events?,,"Given that I have a fair coin which I toss three times, I have the following sample space: $S=\left\{HHH , HHT , HTH, THH , HTT, TTH, THT , TTT\right\}$ how do I: a) Find three events A , B, and C such that no two are independent but $P(A\cap B\cap  C)=P(A)P(B)P(C)$ ? b) Find three events A , B, and C such that every two are independent but all three are not independent: $P(A\cap  B\cap  C)≠P(A)P(B)P(C)$ ? How would I approach a problem like this ? I started by drawing a binary tree that ended up with 8 leaves but I have no idea how to use it to get the answers. Any help would be appreciated.","Given that I have a fair coin which I toss three times, I have the following sample space: $S=\left\{HHH , HHT , HTH, THH , HTT, TTH, THT , TTT\right\}$ how do I: a) Find three events A , B, and C such that no two are independent but $P(A\cap B\cap  C)=P(A)P(B)P(C)$ ? b) Find three events A , B, and C such that every two are independent but all three are not independent: $P(A\cap  B\cap  C)≠P(A)P(B)P(C)$ ? How would I approach a problem like this ? I started by drawing a binary tree that ended up with 8 leaves but I have no idea how to use it to get the answers. Any help would be appreciated.",,"['probability', 'statistics', 'bayesian', 'independence']"
40,Having an independent event with animals,Having an independent event with animals,,"In a building for 24 apartments. It is known that there is only one dog in 8 apartments and a single cat in 6 apartments. How many apartments must have cat and dog for events ""have dog"" and "" have cat"" to be independent? I know that Events $C$ and $D$ are independent if $$P(C\cap D)=P(C)P(D)\ .$$ probability of cats 8/24 = 1/3 and probability of dogs 6/24 = 1/4 probability of cats or dog =(1/4+1/3) = 7/12? I have to find a number that do the probability be 1/1. Am I riht??","In a building for 24 apartments. It is known that there is only one dog in 8 apartments and a single cat in 6 apartments. How many apartments must have cat and dog for events ""have dog"" and "" have cat"" to be independent? I know that Events $C$ and $D$ are independent if $$P(C\cap D)=P(C)P(D)\ .$$ probability of cats 8/24 = 1/3 and probability of dogs 6/24 = 1/4 probability of cats or dog =(1/4+1/3) = 7/12? I have to find a number that do the probability be 1/1. Am I riht??",,"['probability', 'statistics']"
41,"""Well known properties"" of Poisson distribution","""Well known properties"" of Poisson distribution",,"I'm working with Bradley Efron (2010): Large Scale Inference and my question concerns the proof of Lemma 2.3. Here we have $z_i \sim F_0$ with probability $\pi_0$,  $z_i \sim F_1$ with probability $\pi_1$ and $N \sim Poi(\eta)$. The $z_i$ are independent of each other and $i$ runs from $1$ to $N$. $N_0(\mathcal{Z}) := \vert \{ i: z_i \sim F_0, z_i \in \mathcal{Z} \} \vert $ $N_1(\mathcal{Z}) := \vert \{ i: z_i \sim F_1, z_i \in \mathcal{Z} \} \vert $ $N_+(\mathcal{Z}) := N_0(\mathcal{Z})+ N_1(\mathcal{Z}) $ Due to ""well known properties of the Poisson distribution"" it follows that $N_0(\mathcal{Z}) \sim Poi(\eta \pi_0 F_0(\mathcal{Z}))$ independently of $N_1(\mathcal{Z}) \sim Poi(\eta \pi_1 F_1(\mathcal{Z}))$ as well as $N_0(\mathcal{Z}) \vert N_+(\mathcal{Z})  \sim Bin(N_+(\mathcal{Z}), \pi_0 F_0(\mathcal{Z}) / F(\mathcal{Z})  )$ With $F = \pi_0 F_0 + \pi_1 F_1$ being the ""mixed distribution"". Can someone help me out with those well known properties needed to conclude 1. and 2. ?","I'm working with Bradley Efron (2010): Large Scale Inference and my question concerns the proof of Lemma 2.3. Here we have $z_i \sim F_0$ with probability $\pi_0$,  $z_i \sim F_1$ with probability $\pi_1$ and $N \sim Poi(\eta)$. The $z_i$ are independent of each other and $i$ runs from $1$ to $N$. $N_0(\mathcal{Z}) := \vert \{ i: z_i \sim F_0, z_i \in \mathcal{Z} \} \vert $ $N_1(\mathcal{Z}) := \vert \{ i: z_i \sim F_1, z_i \in \mathcal{Z} \} \vert $ $N_+(\mathcal{Z}) := N_0(\mathcal{Z})+ N_1(\mathcal{Z}) $ Due to ""well known properties of the Poisson distribution"" it follows that $N_0(\mathcal{Z}) \sim Poi(\eta \pi_0 F_0(\mathcal{Z}))$ independently of $N_1(\mathcal{Z}) \sim Poi(\eta \pi_1 F_1(\mathcal{Z}))$ as well as $N_0(\mathcal{Z}) \vert N_+(\mathcal{Z})  \sim Bin(N_+(\mathcal{Z}), \pi_0 F_0(\mathcal{Z}) / F(\mathcal{Z})  )$ With $F = \pi_0 F_0 + \pi_1 F_1$ being the ""mixed distribution"". Can someone help me out with those well known properties needed to conclude 1. and 2. ?",,"['probability', 'statistics']"
42,"Explanation for ""jointly pdf is constant but marginal pdf is not""","Explanation for ""jointly pdf is constant but marginal pdf is not""",,"Consider: $X,Y \sim \text{uniformly distributed in }(0 \leq y \leq x \leq 1)$ From short computation, we know: Jointly pdf: $f_{XY}(x,y) = 2$ Marginal pdf of $x$: $f_{X}(x) =\int_0^x 2\,dy=2x, 0\leq x \leq 1$ We know this is a uniformly distributed random variable, so we expect the pdf is constant. However, the second one is not. If it is not uniformly distributed, then what type of distribution of it? How to explain it in a more intuitive but persuasive way without using computation result ?","Consider: $X,Y \sim \text{uniformly distributed in }(0 \leq y \leq x \leq 1)$ From short computation, we know: Jointly pdf: $f_{XY}(x,y) = 2$ Marginal pdf of $x$: $f_{X}(x) =\int_0^x 2\,dy=2x, 0\leq x \leq 1$ We know this is a uniformly distributed random variable, so we expect the pdf is constant. However, the second one is not. If it is not uniformly distributed, then what type of distribution of it? How to explain it in a more intuitive but persuasive way without using computation result ?",,"['statistics', 'probability-distributions', 'random-variables', 'uniform-distribution']"
43,Small question concerning the proof of the unbiased estimate of the population variance,Small question concerning the proof of the unbiased estimate of the population variance,,"The main purpose of this question is to check my understanding. As in I have an answer that I think is correct, but I'm not sure, since Stats is not my forte. So given a random variable $X$, we take some samples from it and denote by $X_i$ the random variable ""the $i$th observation from each sample"" and denote by $\overline{X}$ the random variable the sample mean. We're trying to show that: $$E\left[\frac{1}{n-1}\sum(X_i-\overline{X})^2\right] = \sigma^2 \tag 1$$ where $\operatorname{Var}(X)=\sigma^2$. Let $Y_i = X_i-\overline{X}$ and since $E[X_i]=E[\overline{X}]$ we have that $E(Y_i)=0$. This means that the LHS of (1) becomes: $$ \begin{align} E\left[\frac{1}{n-1}\sum(Y_i^2)\right] & = \frac{1}{n-1}\sum E[Y_i^2] \\[8pt] & = \frac{1}{n-1}\sum (\operatorname{Var}(Y_i)-(E[Y_i])^2) \\[8pt] & = \frac{1}{n-1}\sum \operatorname{Var}(Y_i) \\[8pt] & =\frac{1}{n-1}\sum \operatorname{Var}(X_i-\overline{X}) \end{align} $$ Here is where I made the mistake of saying that $$\operatorname{Var}(X_i-\overline{X}) = \operatorname{Var}(X_i)+(-1)^2\operatorname{Var}(\overline{X}) = \sigma^2+\frac{\sigma^2}{n} = \sigma^2\frac{n+1}{n}$$ However, the proof I was following gave a longer calculation ending with $Var(Y_i)=\sigma^2\frac{n-1}{n}$ which then lead immediately to the desired result. So I wondered where I had gone wrong. My educated guess is that it is not true that $\operatorname{Var}(X_i-\overline{X})=\operatorname{Var}(X_i)+(-1)^2\operatorname{Var}(\overline{X})$ because $\operatorname{Var}(X+Y) = \operatorname{Var}(x)+\operatorname{Var}(Y)$ only if the random variables are independent and $X_i$ and $\overline{X}$ are not. Have I explained my error correctly or is there another mistake in my calculations? Thank you, Diana","The main purpose of this question is to check my understanding. As in I have an answer that I think is correct, but I'm not sure, since Stats is not my forte. So given a random variable $X$, we take some samples from it and denote by $X_i$ the random variable ""the $i$th observation from each sample"" and denote by $\overline{X}$ the random variable the sample mean. We're trying to show that: $$E\left[\frac{1}{n-1}\sum(X_i-\overline{X})^2\right] = \sigma^2 \tag 1$$ where $\operatorname{Var}(X)=\sigma^2$. Let $Y_i = X_i-\overline{X}$ and since $E[X_i]=E[\overline{X}]$ we have that $E(Y_i)=0$. This means that the LHS of (1) becomes: $$ \begin{align} E\left[\frac{1}{n-1}\sum(Y_i^2)\right] & = \frac{1}{n-1}\sum E[Y_i^2] \\[8pt] & = \frac{1}{n-1}\sum (\operatorname{Var}(Y_i)-(E[Y_i])^2) \\[8pt] & = \frac{1}{n-1}\sum \operatorname{Var}(Y_i) \\[8pt] & =\frac{1}{n-1}\sum \operatorname{Var}(X_i-\overline{X}) \end{align} $$ Here is where I made the mistake of saying that $$\operatorname{Var}(X_i-\overline{X}) = \operatorname{Var}(X_i)+(-1)^2\operatorname{Var}(\overline{X}) = \sigma^2+\frac{\sigma^2}{n} = \sigma^2\frac{n+1}{n}$$ However, the proof I was following gave a longer calculation ending with $Var(Y_i)=\sigma^2\frac{n-1}{n}$ which then lead immediately to the desired result. So I wondered where I had gone wrong. My educated guess is that it is not true that $\operatorname{Var}(X_i-\overline{X})=\operatorname{Var}(X_i)+(-1)^2\operatorname{Var}(\overline{X})$ because $\operatorname{Var}(X+Y) = \operatorname{Var}(x)+\operatorname{Var}(Y)$ only if the random variables are independent and $X_i$ and $\overline{X}$ are not. Have I explained my error correctly or is there another mistake in my calculations? Thank you, Diana",,['statistics']
44,Is there a fundamental way to prove Generalized Mean is a increasing function,Is there a fundamental way to prove Generalized Mean is a increasing function,,"Generalized mean: $$M_k=\left(\sum_{i=1}^n\frac{{x_i}^k}{n}\right)^{\frac 1 k}$$ I try to prove $L=\ln\left(M_k\right)$ is increasing. $$\frac{d\left(L\right)}{dk}=\frac{\sum_{i=1}^n{x_i}^k\ln\left(x_i\right)}{\sum_{i=1}^n{x_i}^k}$$ and I stuck here. I visited wikipedia and it says this can be proved using Jesen's inequality. But I want a simple one(or complicated one, if one think using Jesen's inequality is more easier). Any help is going to be appreciated. Thanks","Generalized mean: $$M_k=\left(\sum_{i=1}^n\frac{{x_i}^k}{n}\right)^{\frac 1 k}$$ I try to prove $L=\ln\left(M_k\right)$ is increasing. $$\frac{d\left(L\right)}{dk}=\frac{\sum_{i=1}^n{x_i}^k\ln\left(x_i\right)}{\sum_{i=1}^n{x_i}^k}$$ and I stuck here. I visited wikipedia and it says this can be proved using Jesen's inequality. But I want a simple one(or complicated one, if one think using Jesen's inequality is more easier). Any help is going to be appreciated. Thanks",,"['calculus', 'real-analysis', 'probability', 'statistics']"
45,a problem regarding conditional probability and binomial distribution.,a problem regarding conditional probability and binomial distribution.,,"Die A has 4 red and 2 white faces whereas die B has 2 red and 4 white faces . A coin (fair) is tossed once . If it falls head , the game is carried on by throwing die A alone. If it falls tail die B is used . Let $X_n =1$ or $0$ according as the nth draw is white or red . and let $S_n = X_1 + X_2 +X _3 +\cdots+X_n$ now, i tried to find the expectation and variance of $S_n$ . and i think $S_n$  it follows Bin(n,1/2). $P(X_n= 1)=P(X_n=1\mid \text{coin turned out to be head})\cdot0.5+P(X_n=1\mid \text{coin turned out to be tail})\cdot0.5= (4/6)\cdot(0.5) +(2/6)\cdot(0.5)=0.5$ so, $S_n $follows Bin (n, 0.5)] and in that case , E($S_n$)=.5n and Var($S_n$)=.25n----(1) but if i solve it in this way : Var($S_n$)=[Var($S_n$|coin turned out to be head)]*0.5+Var[($S_n$|coin turned out to be tail)]*0.5 =$[E(S_n|$coin turned out to be head$)-E(S_n)]^2$*0.5+$[E(S_n|$coin turned out to be tail$)-E(S_n)]^2$*0.5 implies  Var($S_n$) =2n/9+$n^2$/36-----(2) what is the reason of difference between the two results obtained in (1) and (2) ? my book says $S_n$ doesn't follow binomial (n, 0.5) . is it the reason of this error  ? but, why isn't it binomial ?","Die A has 4 red and 2 white faces whereas die B has 2 red and 4 white faces . A coin (fair) is tossed once . If it falls head , the game is carried on by throwing die A alone. If it falls tail die B is used . Let $X_n =1$ or $0$ according as the nth draw is white or red . and let $S_n = X_1 + X_2 +X _3 +\cdots+X_n$ now, i tried to find the expectation and variance of $S_n$ . and i think $S_n$  it follows Bin(n,1/2). $P(X_n= 1)=P(X_n=1\mid \text{coin turned out to be head})\cdot0.5+P(X_n=1\mid \text{coin turned out to be tail})\cdot0.5= (4/6)\cdot(0.5) +(2/6)\cdot(0.5)=0.5$ so, $S_n $follows Bin (n, 0.5)] and in that case , E($S_n$)=.5n and Var($S_n$)=.25n----(1) but if i solve it in this way : Var($S_n$)=[Var($S_n$|coin turned out to be head)]*0.5+Var[($S_n$|coin turned out to be tail)]*0.5 =$[E(S_n|$coin turned out to be head$)-E(S_n)]^2$*0.5+$[E(S_n|$coin turned out to be tail$)-E(S_n)]^2$*0.5 implies  Var($S_n$) =2n/9+$n^2$/36-----(2) what is the reason of difference between the two results obtained in (1) and (2) ? my book says $S_n$ doesn't follow binomial (n, 0.5) . is it the reason of this error  ? but, why isn't it binomial ?",,"['probability', 'statistics', 'probability-distributions', 'stochastic-processes', 'binomial-distribution']"
46,A question about Fisher–Neyman factorization theorem,A question about Fisher–Neyman factorization theorem,,"$f_{\theta}(x)$, then $T$ is sufficient for $\theta$ if and only if nonnegative functions $g$ and $h$ can be found such that $f_{\theta} = h(x)g_{\theta}(T(x)) $ The statement is:  if $F(t)$ is a one to one function and $T$ is a sufficient statistic, then $F(T)$ is a sufficient statistic. I can't see this from the factorization criterion, how is  $F(T)$ a sufficient statistic?","$f_{\theta}(x)$, then $T$ is sufficient for $\theta$ if and only if nonnegative functions $g$ and $h$ can be found such that $f_{\theta} = h(x)g_{\theta}(T(x)) $ The statement is:  if $F(t)$ is a one to one function and $T$ is a sufficient statistic, then $F(T)$ is a sufficient statistic. I can't see this from the factorization criterion, how is  $F(T)$ a sufficient statistic?",,"['statistics', 'self-learning', 'statistical-inference']"
47,Degree of freedom in statistic,Degree of freedom in statistic,,"I am always having difficulty understanding what degree of freedom really is. I know that when I post this question people are going to tell me to do some research myself but I did it and I am still having problem understanding it. I watched youtube videos and read articles ...etc but I still do not understand. To be specific, let me tell you where I am stuck. For example If we have a sample size of 3 {x1,x2,x3} is the set of sample values At the same time, we assume that we know the sample mean is going to be equal to be 2 knowing there are 3 variables, the first 2 variables can be anything. Say, x1 = 1, x2 = 4. With  simple calculation we know x3 must be 1 for the sample mean to become 2. In another word, when first and second variables are determined, the third variable must be specific value(not free to be any value) to meet the fact that sample mean must be 2. Ok, until this point everything makes sense to me. When it comes to calculating the variance, I am totally confused. In each and every videos and articles I watched and read always says the same thing. The reason why degree of freedom for estimating the variance is N-1, which N is the sample size. The reason for that is because we know the sample mean is going to equal to specific value in the first place . This logic sound quite contradicting to me!! The reason why we need a sample mean instead of the population mean is because we do not know the value of population mean in the first place. Now you are telling me that we know the sample mean is going to be specific value as if we already know the population mean? In another word, if we don't know what the population mean is, how are we possibly going to know what value our sample mean is even close to? Does not the entire concept just sound contradicting? Maybe it is just because of my lack of knowledge but if that is the case, can anyone help me out by explaining to me the concept in simple English? To summarise the question: I do not understand why in the first example, we want to assume the sample mean is going to be 2 , since in reality when we draw a sample from a population we don't even know the population mean, so how are we going to know sample mean is going to be 2? @ImATurtle, Thanks for your detailed answer. In response to your answer, I understood that knowing the sample mean is necessary for calculating sample variance. However, my problem is I don't really get how you are going to know the sample mean before you even know all your sample values in the first place. For example, given sample values are {2,3,4,5,6}, we know that the sample mean is 4 only after you draw the sample and do the calculation . The five sample values were freely drawn from the population without doubt. Yet, people keep on saying that we know the sample mean will be 4, therefore after drawing the sample values 2,3,4,5, we know the fifth value is 6. So, what make it possible for us to predict the sample mean is going to be 4 before we even start drawing samples from population?","I am always having difficulty understanding what degree of freedom really is. I know that when I post this question people are going to tell me to do some research myself but I did it and I am still having problem understanding it. I watched youtube videos and read articles ...etc but I still do not understand. To be specific, let me tell you where I am stuck. For example If we have a sample size of 3 {x1,x2,x3} is the set of sample values At the same time, we assume that we know the sample mean is going to be equal to be 2 knowing there are 3 variables, the first 2 variables can be anything. Say, x1 = 1, x2 = 4. With  simple calculation we know x3 must be 1 for the sample mean to become 2. In another word, when first and second variables are determined, the third variable must be specific value(not free to be any value) to meet the fact that sample mean must be 2. Ok, until this point everything makes sense to me. When it comes to calculating the variance, I am totally confused. In each and every videos and articles I watched and read always says the same thing. The reason why degree of freedom for estimating the variance is N-1, which N is the sample size. The reason for that is because we know the sample mean is going to equal to specific value in the first place . This logic sound quite contradicting to me!! The reason why we need a sample mean instead of the population mean is because we do not know the value of population mean in the first place. Now you are telling me that we know the sample mean is going to be specific value as if we already know the population mean? In another word, if we don't know what the population mean is, how are we possibly going to know what value our sample mean is even close to? Does not the entire concept just sound contradicting? Maybe it is just because of my lack of knowledge but if that is the case, can anyone help me out by explaining to me the concept in simple English? To summarise the question: I do not understand why in the first example, we want to assume the sample mean is going to be 2 , since in reality when we draw a sample from a population we don't even know the population mean, so how are we going to know sample mean is going to be 2? @ImATurtle, Thanks for your detailed answer. In response to your answer, I understood that knowing the sample mean is necessary for calculating sample variance. However, my problem is I don't really get how you are going to know the sample mean before you even know all your sample values in the first place. For example, given sample values are {2,3,4,5,6}, we know that the sample mean is 4 only after you draw the sample and do the calculation . The five sample values were freely drawn from the population without doubt. Yet, people keep on saying that we know the sample mean will be 4, therefore after drawing the sample values 2,3,4,5, we know the fifth value is 6. So, what make it possible for us to predict the sample mean is going to be 4 before we even start drawing samples from population?",,['statistics']
48,Normal distribution with sample,Normal distribution with sample,,"I'm trying to figure out the best approach to this problem. I would assume that I can use the Central Limit theorem first and then a binomial cdf: Chocolate is packaged into jars using a computerized system. The volume of one candy that can fill one jar follows a normal approximation with a mean of 300 ml and $\sigma$ = 24ml. In an experiment, 15 random jars were selected. What was the probability that there were at most 2 jars that had less than 275 ml of candy in its jar? $P(\frac{X-300}{24}<\frac{275-300}{24}) = P(Z<-1.04) = .1429$ Then $P(Y \leq 2) = 105(.1429)^{2}(.8571)^{13}+15(.1429)(.8571)^{14}+(.8571)^{15} = 0.635$ Not sure if this is the right approach or not. The answer isn't correct, just wondering where I am off.","I'm trying to figure out the best approach to this problem. I would assume that I can use the Central Limit theorem first and then a binomial cdf: Chocolate is packaged into jars using a computerized system. The volume of one candy that can fill one jar follows a normal approximation with a mean of 300 ml and $\sigma$ = 24ml. In an experiment, 15 random jars were selected. What was the probability that there were at most 2 jars that had less than 275 ml of candy in its jar? $P(\frac{X-300}{24}<\frac{275-300}{24}) = P(Z<-1.04) = .1429$ Then $P(Y \leq 2) = 105(.1429)^{2}(.8571)^{13}+15(.1429)(.8571)^{14}+(.8571)^{15} = 0.635$ Not sure if this is the right approach or not. The answer isn't correct, just wondering where I am off.",,"['probability', 'statistics', 'normal-distribution']"
49,Why this process is nonergodic?,Why this process is nonergodic?,,"I am studying a tutorial on stochastic processes and there's an example in it which I don't understand anything of it. First of all there is this criterion for a mean-ergodic random process: For a WSS random process to be ergodic in the mean, the variance of   the sample mean   $$\operatorname{var} (\hat\mu_N)=\frac{1}{N}\sum_{k=-(N-1)}^{N-1}\left(1-\frac{|k|}{N}\right)(r_X[k]-\mu^2)$$   must converge to zero as $N\to\infty$. 1- What is $r_X[k]$ in the formula and how is it computed? Then there is the following example: Define a random process as $X[n]=A$ where $A=N(0,1)$. The random   process is WSS since $$\mu_X[n]=E[X[n]]=E[A]=0=\mu$$   $$r_X[k]=E[X[n]X[n+k]]=E[A^2]=1$$ However, it should be clear that sample mean will not converge to $\mu$ In addition, it can be shown that var(sample mean)=1 Each realization is not representative of the ensemble of realizations. Assuminig that $A=N(0,1)$ is the standard normal distribution 2- How is it clear that sample mean does not converge to $\mu$? 3- Why var(sample mean)=1?","I am studying a tutorial on stochastic processes and there's an example in it which I don't understand anything of it. First of all there is this criterion for a mean-ergodic random process: For a WSS random process to be ergodic in the mean, the variance of   the sample mean   $$\operatorname{var} (\hat\mu_N)=\frac{1}{N}\sum_{k=-(N-1)}^{N-1}\left(1-\frac{|k|}{N}\right)(r_X[k]-\mu^2)$$   must converge to zero as $N\to\infty$. 1- What is $r_X[k]$ in the formula and how is it computed? Then there is the following example: Define a random process as $X[n]=A$ where $A=N(0,1)$. The random   process is WSS since $$\mu_X[n]=E[X[n]]=E[A]=0=\mu$$   $$r_X[k]=E[X[n]X[n+k]]=E[A^2]=1$$ However, it should be clear that sample mean will not converge to $\mu$ In addition, it can be shown that var(sample mean)=1 Each realization is not representative of the ensemble of realizations. Assuminig that $A=N(0,1)$ is the standard normal distribution 2- How is it clear that sample mean does not converge to $\mu$? 3- Why var(sample mean)=1?",,"['statistics', 'stochastic-processes', 'random-variables', 'intuition', 'average']"
50,How to make statistical sense of this experiment:,How to make statistical sense of this experiment:,,"I have conducted an experiment but I am now unsure of how to say, from a statistics point of view, that the data supports or not that a certain phenomenon has occurred, meaning it could be mere measurement error. This was the experiment: a sample of steel had its ferrite(one common constituent of steels) content measured by a  certain device 10 times, resulting in 10 values(likely hovering around the true value), a mean value  and the standard deviation(is this really what should be being computed here?). Then the sample received a heat treatment and again had its ferrite content measured by the same device 10 times, resulting in 10 values, a mean value and the standard deviation. Let´s assume the values for the mean and standard deviation for the untreated sample are, respectively, 25 and 1.2. And the values for the treated sample are 23 and 1. How can I make the proper statistical treatment here? How to go about computing how certain  one can be when ascertaining the phenomenon did/did not happen? EDIT: Actually, in the experiment, one sample was used to measure the non-heat treated ferrite content. Then four sets of samples, of the very same material/same batch of course, were heat treated for different lengths of time at the same temperature, each of the four sets treated at a different temperature. For each set of samples, all of the samples were heat treated together, then at say, 300 seconds, one sample was taken out of the oven. Then, at 600 seconds another, at 6000 seconds another, and so on. The ferrite content of all samples was measured using the same device. The samples that were heat treated for long times have numbers that show clearly that something happened, even without proper statistical analysis. The problem is dealing with the samples treated for short times, as they showed numbers that are similar to the untreated sample, hence the need to test them for statistical significance.","I have conducted an experiment but I am now unsure of how to say, from a statistics point of view, that the data supports or not that a certain phenomenon has occurred, meaning it could be mere measurement error. This was the experiment: a sample of steel had its ferrite(one common constituent of steels) content measured by a  certain device 10 times, resulting in 10 values(likely hovering around the true value), a mean value  and the standard deviation(is this really what should be being computed here?). Then the sample received a heat treatment and again had its ferrite content measured by the same device 10 times, resulting in 10 values, a mean value and the standard deviation. Let´s assume the values for the mean and standard deviation for the untreated sample are, respectively, 25 and 1.2. And the values for the treated sample are 23 and 1. How can I make the proper statistical treatment here? How to go about computing how certain  one can be when ascertaining the phenomenon did/did not happen? EDIT: Actually, in the experiment, one sample was used to measure the non-heat treated ferrite content. Then four sets of samples, of the very same material/same batch of course, were heat treated for different lengths of time at the same temperature, each of the four sets treated at a different temperature. For each set of samples, all of the samples were heat treated together, then at say, 300 seconds, one sample was taken out of the oven. Then, at 600 seconds another, at 6000 seconds another, and so on. The ferrite content of all samples was measured using the same device. The samples that were heat treated for long times have numbers that show clearly that something happened, even without proper statistical analysis. The problem is dealing with the samples treated for short times, as they showed numbers that are similar to the untreated sample, hence the need to test them for statistical significance.",,['statistics']
51,Trying to understand Bienaymé formula,Trying to understand Bienaymé formula,,"In Bienaymé formula, it states that $var(\bar X) = \large\frac{\sigma^2}{n}$. However, when I was going through the proof here , it says the variances of $X_1,X_2,X_3......X_n$ are the same(assuming they are all independent). Can anyone explain the reason behind it? I am confused about how different random variables can have the same variance.","In Bienaymé formula, it states that $var(\bar X) = \large\frac{\sigma^2}{n}$. However, when I was going through the proof here , it says the variances of $X_1,X_2,X_3......X_n$ are the same(assuming they are all independent). Can anyone explain the reason behind it? I am confused about how different random variables can have the same variance.",,"['probability', 'statistics']"
52,Poisson(Exponential) Distribution question,Poisson(Exponential) Distribution question,,"I am trying to solve the following question: If the number of calls received per hour by an answering service is a Poisson random variable with rate of 6 calls per hour, what is the probability of waiting more than 15 minutes between two successive calls? My understanding of this question is that it's asking what's the probability that there is $0$ call in $15$ min. Therefore, for the Poisson parameter,  $\lambda t = 3/2 $ where $t = 0.25$. Based on the Poisson distribution formula   $p(X=x) = \lambda^x \frac{\Large e^{-\lambda}}{x!} $, the probability that zero call arrives in the next 15 minutes is $p(X = 0) ={(3/2)^0  \frac{\Large e^{-3/2}}{0!}}$ which is ${ e^{-3/2}}$ Is this the correct way of doing it? I have been searching similar questions online and I found someone's answer on reddit using exponential distribution and I dont know if that's correct because the answer is different from mine.","I am trying to solve the following question: If the number of calls received per hour by an answering service is a Poisson random variable with rate of 6 calls per hour, what is the probability of waiting more than 15 minutes between two successive calls? My understanding of this question is that it's asking what's the probability that there is $0$ call in $15$ min. Therefore, for the Poisson parameter,  $\lambda t = 3/2 $ where $t = 0.25$. Based on the Poisson distribution formula   $p(X=x) = \lambda^x \frac{\Large e^{-\lambda}}{x!} $, the probability that zero call arrives in the next 15 minutes is $p(X = 0) ={(3/2)^0  \frac{\Large e^{-3/2}}{0!}}$ which is ${ e^{-3/2}}$ Is this the correct way of doing it? I have been searching similar questions online and I found someone's answer on reddit using exponential distribution and I dont know if that's correct because the answer is different from mine.",,"['probability', 'statistics', 'poisson-distribution']"
53,Urn problem and combinatorics,Urn problem and combinatorics,,"You have $5$ red and $4$ black balls. How many ways there are to distribute all to $3$ different bottles? If I had $9$ red balls, then it would be $\binom{n+k-1}{k}$ = $\binom{3+9-1}{9}$, but I have no idea how to solve it when I have different colours... Example: 1 Red, 1 Black and 2 Bottle (b_x for bottle) If you have 1 Red ball  1 Black ball and 2 Bottles then you  have four options: first option: In Bottle 1 : Red and Black and there no one in Bottle 2. second option: In Bottle 1 : Red and there is Black in Bottle 2. third option: In Bottle 1 : Black and there is Red in Bottle 2. fourth option:  In Bottle 1 : empty  and there is Red and Black in Bottle 2.","You have $5$ red and $4$ black balls. How many ways there are to distribute all to $3$ different bottles? If I had $9$ red balls, then it would be $\binom{n+k-1}{k}$ = $\binom{3+9-1}{9}$, but I have no idea how to solve it when I have different colours... Example: 1 Red, 1 Black and 2 Bottle (b_x for bottle) If you have 1 Red ball  1 Black ball and 2 Bottles then you  have four options: first option: In Bottle 1 : Red and Black and there no one in Bottle 2. second option: In Bottle 1 : Red and there is Black in Bottle 2. third option: In Bottle 1 : Black and there is Red in Bottle 2. fourth option:  In Bottle 1 : empty  and there is Red and Black in Bottle 2.",,"['combinatorics', 'statistics']"
54,Proof of equivalent probabilities in anomaly detection,Proof of equivalent probabilities in anomaly detection,,"In A New Look at Anomaly Detection there is a claim for the proof of probabilistic definition of normal  is as follows, a guess of the probability for event i is $\pi_i$, the true probability is $p_i$ then ""if we make the average value of $-\log \pi_i$ as small as possible then we can prove the estimated probabilities are as close as possible to the underlying $p_i$"" (direct quote) in particular that $\max_{\pi} \sum_i{p_i\log{\pi_i}}=\sum_i{p_i\log{p_i}}$ I don't follow the logic of why this is.","In A New Look at Anomaly Detection there is a claim for the proof of probabilistic definition of normal  is as follows, a guess of the probability for event i is $\pi_i$, the true probability is $p_i$ then ""if we make the average value of $-\log \pi_i$ as small as possible then we can prove the estimated probabilities are as close as possible to the underlying $p_i$"" (direct quote) in particular that $\max_{\pi} \sum_i{p_i\log{\pi_i}}=\sum_i{p_i\log{p_i}}$ I don't follow the logic of why this is.",,"['probability', 'statistics']"
55,the probability density function (PDF) of concatenation of two Gaussian variables,the probability density function (PDF) of concatenation of two Gaussian variables,,"Gaussian variable $x$ follows from $N(u_x,\sigma_x^2)$ and $y$ follows from $N(u_y,\sigma_y^2)$. Assume we have the vector $\bf{z}=[x,y]^T\in R^2$, then it seems that no matter whether $x$ and $y$ are independent or not, we always have that $\bf{z}$ also follows from the Gaussian distribution $N([u_x,u_y]^T, Cov([x,y]^T))$, where $Cov$ means the covariance. The above claim is reformulated from the last eight lines on the left column in page 4 of http://www.cs.bham.ac.uk/~axk/ICML_Flip_2013.pdf . Could anyone show why the distribution of $\bf{z}$ is Gaussian? and how to get the related parameters?","Gaussian variable $x$ follows from $N(u_x,\sigma_x^2)$ and $y$ follows from $N(u_y,\sigma_y^2)$. Assume we have the vector $\bf{z}=[x,y]^T\in R^2$, then it seems that no matter whether $x$ and $y$ are independent or not, we always have that $\bf{z}$ also follows from the Gaussian distribution $N([u_x,u_y]^T, Cov([x,y]^T))$, where $Cov$ means the covariance. The above claim is reformulated from the last eight lines on the left column in page 4 of http://www.cs.bham.ac.uk/~axk/ICML_Flip_2013.pdf . Could anyone show why the distribution of $\bf{z}$ is Gaussian? and how to get the related parameters?",,"['calculus', 'probability', 'statistics', 'probability-distributions', 'random-variables']"
56,"Two types of errors, type-$1$ error and type-$2$ error, can not be minimized simultaneously when the sample size $n$ is already fixed. How?","Two types of errors, type- error and type- error, can not be minimized simultaneously when the sample size  is already fixed. How?",1 2 n,"I read in some of the books that the two types of errors, type-$1$ error and type-$2$ error, can not be minimized simultaneously in Neyman Pearson Theory of testing of hypothesis when the sample size $n$ is already fixed.I am clear up to this that if one tries to reduce the type-$2$ error then the reduction in the number of time committing the type-$2$ error leads to the number of times rejection of the null hypothesis when it is actually false. that is one can say that it leads to the increment of correct rejection. but then how it can be connected to the increment in type-$1$ error, is my problem.","I read in some of the books that the two types of errors, type-$1$ error and type-$2$ error, can not be minimized simultaneously in Neyman Pearson Theory of testing of hypothesis when the sample size $n$ is already fixed.I am clear up to this that if one tries to reduce the type-$2$ error then the reduction in the number of time committing the type-$2$ error leads to the number of times rejection of the null hypothesis when it is actually false. that is one can say that it leads to the increment of correct rejection. but then how it can be connected to the increment in type-$1$ error, is my problem.",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
57,"Erin rolls 4 four-sided dice all at once, then can roll a subset of her choosing a 2nd time. What is the probability of getting all the same number?","Erin rolls 4 four-sided dice all at once, then can roll a subset of her choosing a 2nd time. What is the probability of getting all the same number?",,"Here's what I have so far: All 4 same on first try = (1/4)^4 * 4 3 same, then get 4th on 2nd roll = 4 * (1/4)^3 * (3/4) * (4!/3!) Here's where I'm confused: 2 same = 4 * (1/4)^2 * (3/4)(2/4 :to avoid counting double couplets twice) * (4!/2! :the order they can be arranged) + 4*3/4^4 1 same = ??? (I get easily stuck on combinatorics problems like this, and would be so grateful if you could thoroughly explain a simple strategy for solving these kind of problems with combinatorics; while I know what combinatorics are, I don't think I've fully understood how to apply them in different kinds of contexts.)","Here's what I have so far: All 4 same on first try = (1/4)^4 * 4 3 same, then get 4th on 2nd roll = 4 * (1/4)^3 * (3/4) * (4!/3!) Here's where I'm confused: 2 same = 4 * (1/4)^2 * (3/4)(2/4 :to avoid counting double couplets twice) * (4!/2! :the order they can be arranged) + 4*3/4^4 1 same = ??? (I get easily stuck on combinatorics problems like this, and would be so grateful if you could thoroughly explain a simple strategy for solving these kind of problems with combinatorics; while I know what combinatorics are, I don't think I've fully understood how to apply them in different kinds of contexts.)",,"['probability', 'combinatorics', 'statistics']"
58,Variance of subset vs total variance,Variance of subset vs total variance,,"Is it true that the variance of subset is smaller than variance of the total set? Given each element in the set is a N-dimensional vector, and the distance is defined as Euclidean distance. Variance is defined as in statistics term (sum of square of Euclidean distance to the mean data point). One thing for sure is that if the total variance is 0, then the variance of subset should be 0 as well.","Is it true that the variance of subset is smaller than variance of the total set? Given each element in the set is a N-dimensional vector, and the distance is defined as Euclidean distance. Variance is defined as in statistics term (sum of square of Euclidean distance to the mean data point). One thing for sure is that if the total variance is 0, then the variance of subset should be 0 as well.",,"['linear-algebra', 'statistics']"
59,Underdetermined vs Overdetermined Problem,Underdetermined vs Overdetermined Problem,,"I'm trying to create a model which is of the form $$y = (a_0 + a_1l)[b_0+\sum_{m=1}^M b_m\cos(mx-\alpha_m)] [c_0 +\sum_{n=1}^N c_n\cos(nz-\beta_n)]$$ In the above system, $l$,$x$ and $z$ are independent variables and $y$ is the dependent variable. The $a$, $b$ and $c$ terms are the unknowns. To solve for these unknowns, I have two separate data sets that I can use. Using data set $1$ creates an overdetermined system providing me with more observations than unknowns, while data set $2$ creates an underdetermined system with less observations than unknowns. In such a case, which approach would be better - underdetermined or overdetermined? and Why?","I'm trying to create a model which is of the form $$y = (a_0 + a_1l)[b_0+\sum_{m=1}^M b_m\cos(mx-\alpha_m)] [c_0 +\sum_{n=1}^N c_n\cos(nz-\beta_n)]$$ In the above system, $l$,$x$ and $z$ are independent variables and $y$ is the dependent variable. The $a$, $b$ and $c$ terms are the unknowns. To solve for these unknowns, I have two separate data sets that I can use. Using data set $1$ creates an overdetermined system providing me with more observations than unknowns, while data set $2$ creates an underdetermined system with less observations than unknowns. In such a case, which approach would be better - underdetermined or overdetermined? and Why?",,"['linear-algebra', 'statistics', 'numerical-methods', 'systems-of-equations']"
60,Approach $P(|\frac{x}{64} - p| \le 0.1)$ for a binomial distribution,Approach  for a binomial distribution,P(|\frac{x}{64} - p| \le 0.1),"I have a statistics exercise, with a stochastic variable $X$ that has a $\text{Binomial}(64,p)$ distribution. P is unknown, and the goal is to find it. In one of the sub-exercises, I have to approach the following: $$P\left(\left|\frac{x}{64} - p\right| \le 0.1\right)$$ However, I simply have no idea where to even start. Can anyone attempt to explain how to approach a problem like this?","I have a statistics exercise, with a stochastic variable $X$ that has a $\text{Binomial}(64,p)$ distribution. P is unknown, and the goal is to find it. In one of the sub-exercises, I have to approach the following: $$P\left(\left|\frac{x}{64} - p\right| \le 0.1\right)$$ However, I simply have no idea where to even start. Can anyone attempt to explain how to approach a problem like this?",,"['statistics', 'binomial-distribution']"
61,Find the sample space for a committee of two chosen from Alice,Find the sample space for a committee of two chosen from Alice,,"Find the sample space for a committee of two chosen from Alice (A), Bill (B), Carol (C), and David (D). {(A, B), (A, D), (B, C), (C, D)} {(A, B), (C, D {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)} {A, B, C, D} my choice is --> {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)} Find the sample space if both sexes must be represented. {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)} {(A, B), (C, D)} {A, B, C, D} {(A, B), (A, D), (B, C), (C, D)} my choice is ----> {(A, B), (C, D)} is that correct ???","Find the sample space for a committee of two chosen from Alice (A), Bill (B), Carol (C), and David (D). {(A, B), (A, D), (B, C), (C, D)} {(A, B), (C, D {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)} {A, B, C, D} my choice is --> {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)} Find the sample space if both sexes must be represented. {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)} {(A, B), (C, D)} {A, B, C, D} {(A, B), (A, D), (B, C), (C, D)} my choice is ----> {(A, B), (C, D)} is that correct ???",,"['probability', 'combinatorics', 'statistics']"
62,Probability about confidence interval,Probability about confidence interval,,"Let $X_1,...X_n$ be iid $N(\theta,1)$. A 95% confidence interval for   $\theta$ is $\overline{X}\pm\frac{1.96}{\sqrt{n}}$.Let p denote the   probability that an additional independent observation,$X_{n+1}$, will   fall in this interval.Is p greater than, less than, equal to .95?Prove   your answer. Casella & Berger, Chapter 9 - Interval Estimation First $\overline{X}\approx N(\theta,\frac{1}{n})\Rightarrow Y=\sqrt{n}\overline{X}$~$N(\theta,1)$ I know that $$P(\overline{X}-\frac{1.96}{\sqrt{n}}\leq\theta\leq \overline{X}+\frac{1.96}{\sqrt{n}})=0.95$$ $$P(\theta-\frac{1.96}{\sqrt{n}}\leq\overline{X}\leq\theta+\frac{1.96}{\sqrt{n}})=0.95$$ $$P(\theta-1.96\leq\overline{X}-\theta\leq\theta+1.96)=0.95$$ $$P(\theta-1.96\leq Y\leq\theta+1.96)=0.95$$ That's why I think it's the same, I tried other ways but got nowhere. Someone give me a hint?","Let $X_1,...X_n$ be iid $N(\theta,1)$. A 95% confidence interval for   $\theta$ is $\overline{X}\pm\frac{1.96}{\sqrt{n}}$.Let p denote the   probability that an additional independent observation,$X_{n+1}$, will   fall in this interval.Is p greater than, less than, equal to .95?Prove   your answer. Casella & Berger, Chapter 9 - Interval Estimation First $\overline{X}\approx N(\theta,\frac{1}{n})\Rightarrow Y=\sqrt{n}\overline{X}$~$N(\theta,1)$ I know that $$P(\overline{X}-\frac{1.96}{\sqrt{n}}\leq\theta\leq \overline{X}+\frac{1.96}{\sqrt{n}})=0.95$$ $$P(\theta-\frac{1.96}{\sqrt{n}}\leq\overline{X}\leq\theta+\frac{1.96}{\sqrt{n}})=0.95$$ $$P(\theta-1.96\leq\overline{X}-\theta\leq\theta+1.96)=0.95$$ $$P(\theta-1.96\leq Y\leq\theta+1.96)=0.95$$ That's why I think it's the same, I tried other ways but got nowhere. Someone give me a hint?",,"['probability', 'statistics', 'statistical-inference', 'confidence-interval']"
63,"statistics , two different results related in their standard deviation","statistics , two different results related in their standard deviation",,"I have a question I don't know how to think about it. We have a sample of $25$ people, $16$ of them smoke and $9$ don't. The average capacity of their lungs in smokers is $103$ and in non-smokers is $95$ and the standard deviation of all people in the sample is $10$. Can we be sure with $95\%$ confidence that smokers have statistically more lung capacity? My problem is that I can't understand basically what does that deviation mean and how can it help? The problem seems easy but I am so confused about it. Thank you.","I have a question I don't know how to think about it. We have a sample of $25$ people, $16$ of them smoke and $9$ don't. The average capacity of their lungs in smokers is $103$ and in non-smokers is $95$ and the standard deviation of all people in the sample is $10$. Can we be sure with $95\%$ confidence that smokers have statistically more lung capacity? My problem is that I can't understand basically what does that deviation mean and how can it help? The problem seems easy but I am so confused about it. Thank you.",,['statistics']
64,statistics and biased estimator of normal distributions,statistics and biased estimator of normal distributions,,"Let $X_1,X_2,X_3,X_4$ be independent, identically distributed random variables from a population with mean $\mu =10$ and variance $\sigma ^2=10$ . Let $$\overline Y=\frac{X_1+X_2+X_3+X_4}{4}$$ denote the average of these four random variables (in other words, the sample mean). a) What is the expected value and variance of $\overline Y$ ? b) Now, consider a different estimator of $\mu$ : $$\displaystyle W=\frac{X_1+X_2+X_3+X_4}{8}.$$ This is an example of a weighted average of the $X_i$ .  What is the expected value and  variance of $W$ ? c) Based on your answers on parts (a) and (b), which estimator of $\mu$ do you prefer, $\overline Y$ or $W$ ? Attempt: a) I got $\mathbb{E}(X)=10$ , but variance known formula is $\mathbb{E}(X^2)-\mathbb{E}(X)^2$ but got stuck there. b) $\dfrac{20}{8}+\dfrac{10}{4}+5=\text{mean}$ , and stuck on variance. c) Checking unbiasness?","Let be independent, identically distributed random variables from a population with mean and variance . Let denote the average of these four random variables (in other words, the sample mean). a) What is the expected value and variance of ? b) Now, consider a different estimator of : This is an example of a weighted average of the .  What is the expected value and  variance of ? c) Based on your answers on parts (a) and (b), which estimator of do you prefer, or ? Attempt: a) I got , but variance known formula is but got stuck there. b) , and stuck on variance. c) Checking unbiasness?","X_1,X_2,X_3,X_4 \mu =10 \sigma ^2=10 \overline Y=\frac{X_1+X_2+X_3+X_4}{4} \overline Y \mu \displaystyle W=\frac{X_1+X_2+X_3+X_4}{8}. X_i W \mu \overline Y W \mathbb{E}(X)=10 \mathbb{E}(X^2)-\mathbb{E}(X)^2 \dfrac{20}{8}+\dfrac{10}{4}+5=\text{mean}","['probability', 'statistics', 'normal-distribution']"
65,What is a easy way to draw from distribution $f(x|u) \propto x^{\alpha-1}I(x<u)$?,What is a easy way to draw from distribution ?,f(x|u) \propto x^{\alpha-1}I(x<u),"We know $u$ and want to draw $x$ from the conditional density $f(x|u) \propto x^{\alpha-1}I(x<u), \alpha>0$. One way is that first draw $r$ from uniform(0,1), and then set $x=u e^{\frac{\log(r)}{\alpha}}$. I don't know why. Please help me to explain it. Thanks","We know $u$ and want to draw $x$ from the conditional density $f(x|u) \propto x^{\alpha-1}I(x<u), \alpha>0$. One way is that first draw $r$ from uniform(0,1), and then set $x=u e^{\frac{\log(r)}{\alpha}}$. I don't know why. Please help me to explain it. Thanks",,"['statistics', 'statistical-inference']"
66,Direction of Study Suggestion,Direction of Study Suggestion,,"I'm not sure if this belongs in stats or here, or why stats would be considered different to math. During my self-study of biochemistry and medicine, I notice that a vast range of published studies are flawed and irreproducible, leading to incorrect hypotheses about many things. In particular it seems the statistics in these studies, notably the P value and confidence intervals, are incorrectly determined and derived. I therefore made the focus of my study temporarily on statistics, but I feel like I'm taking things on faith, such as Chi-squared tests etc, without actually understanding the mathematics behind the formula. I understand the law of large numbers, but not why a sample size needs n=30 or greater for this law to hold in some tests, and why not some other arbitrary number. I appreciate the integral of two points in a normal distribution represents the area, which is a reflection of the probability of an event, but this sort of explanation for more advanced concepts seems to be lacking in the intro courses. I don't like accepting such concepts on faith as seems to be the case with all the online introductory stats courses I can find. I realized then that mathematical statistics is what I'm after, however my mathematical education doesn't extend beyond single variable calculus. I'm prepared to devote some time (6 months or longer) to understanding the foundational math necessary to approach an introduction to statistics from a more rigorous perspective. Would anyone be able to guide me on a study plan necessary that would lead up to probability theory and mathematical statistics, with an end goal of being able to fully understand applied statistics in science and the reasoning behind why certain equations and values of n are used. Ideally I'd like to learn both the mathematical theory, and the application together, perhaps in a package such as R, where I can see the applications of calculus and probability theory to stats. On an unrelated note, why are statistics and mathematics treated as two separate categories? I see stats often applies formulae, but not understanding the application of applied formulae from a mathematical grounding is surely a dangerous thing?","I'm not sure if this belongs in stats or here, or why stats would be considered different to math. During my self-study of biochemistry and medicine, I notice that a vast range of published studies are flawed and irreproducible, leading to incorrect hypotheses about many things. In particular it seems the statistics in these studies, notably the P value and confidence intervals, are incorrectly determined and derived. I therefore made the focus of my study temporarily on statistics, but I feel like I'm taking things on faith, such as Chi-squared tests etc, without actually understanding the mathematics behind the formula. I understand the law of large numbers, but not why a sample size needs n=30 or greater for this law to hold in some tests, and why not some other arbitrary number. I appreciate the integral of two points in a normal distribution represents the area, which is a reflection of the probability of an event, but this sort of explanation for more advanced concepts seems to be lacking in the intro courses. I don't like accepting such concepts on faith as seems to be the case with all the online introductory stats courses I can find. I realized then that mathematical statistics is what I'm after, however my mathematical education doesn't extend beyond single variable calculus. I'm prepared to devote some time (6 months or longer) to understanding the foundational math necessary to approach an introduction to statistics from a more rigorous perspective. Would anyone be able to guide me on a study plan necessary that would lead up to probability theory and mathematical statistics, with an end goal of being able to fully understand applied statistics in science and the reasoning behind why certain equations and values of n are used. Ideally I'd like to learn both the mathematical theory, and the application together, perhaps in a package such as R, where I can see the applications of calculus and probability theory to stats. On an unrelated note, why are statistics and mathematics treated as two separate categories? I see stats often applies formulae, but not understanding the application of applied formulae from a mathematical grounding is surely a dangerous thing?",,['statistics']
67,Conditional probability to Conditional expectation,Conditional probability to Conditional expectation,,"My first related question is this link. enter link description here Expected value of geometric distribution is $$c=E(X)$$ $$c=0\times p+(1+c)\times q =q+cq$$ $$c=\frac{q}{p}$$ when $X= number \ of \ failures \ before \ the \ 1st \ success$ I just learned about conditioning through $Statistics \ 110$, lecture in Harvard. So I want to proof that equation using conditioning. $$P(X)=P(X\mid A_1)P(A_1)+P(X\mid A_2)P(A_2)$$ So, $$E(X)=\sum\limits_{x=1}^{\infty} xP(X=x)=\sum\limits_{x=1}^{\infty}xP(X=x\mid A_1)P(A_1)+\sum\limits_{x=1}^{\infty}xP(X=x\mid A_2)P(A_2)$$ $$when \ P(A_1)=P(1st \ success)=p, \ P(A_2)=P(1st \ failure)=q$$ I know $$\sum\limits_{x=1}^{\infty}xP(X=x\mid A_1)P(A_1)=0$$ because $P(X=x\mid A_1)=0 \ ,\ for \ any \ positive \ integer \ x $. And now we have $q\sum\limits_{x=1}^{\infty}xP(X=x\mid A_2)$ only. But I have no idea that covert $\sum\limits_{x=1}^{\infty}xP(X=x\mid A_2)$ into $(1+c)$ algebraicly. I want to know that this process has any error and some idea to complete this proof.","My first related question is this link. enter link description here Expected value of geometric distribution is $$c=E(X)$$ $$c=0\times p+(1+c)\times q =q+cq$$ $$c=\frac{q}{p}$$ when $X= number \ of \ failures \ before \ the \ 1st \ success$ I just learned about conditioning through $Statistics \ 110$, lecture in Harvard. So I want to proof that equation using conditioning. $$P(X)=P(X\mid A_1)P(A_1)+P(X\mid A_2)P(A_2)$$ So, $$E(X)=\sum\limits_{x=1}^{\infty} xP(X=x)=\sum\limits_{x=1}^{\infty}xP(X=x\mid A_1)P(A_1)+\sum\limits_{x=1}^{\infty}xP(X=x\mid A_2)P(A_2)$$ $$when \ P(A_1)=P(1st \ success)=p, \ P(A_2)=P(1st \ failure)=q$$ I know $$\sum\limits_{x=1}^{\infty}xP(X=x\mid A_1)P(A_1)=0$$ because $P(X=x\mid A_1)=0 \ ,\ for \ any \ positive \ integer \ x $. And now we have $q\sum\limits_{x=1}^{\infty}xP(X=x\mid A_2)$ only. But I have no idea that covert $\sum\limits_{x=1}^{\infty}xP(X=x\mid A_2)$ into $(1+c)$ algebraicly. I want to know that this process has any error and some idea to complete this proof.",,"['probability', 'statistics']"
68,How to determine the expectation of the square of a binomial collection,How to determine the expectation of the square of a binomial collection,,"I'm trying to find how to express the expectation of the square of a collection of binomial measurements. If we have a collection: $$A = a_1 + a_2 + \cdots + a_n$$ The expectation of $A$ is the sum of the expectations of each term of $A$. However, for the square of $A$, I'm getting stuck on how $np +(n^2-n)p^2$ is derived. Without any background in number theory, I'm trying to find it as follows, assuming a set of three trials in $A$: $$E(A^2) = E((a_1 + a_2 + a_3)(a_1 + a_2 + a_3))$$ $$E(A^2) = E(a_1a_1 + a_1a_2 + a_1a_3 + a_2a_1 + a_2a_2 + a_2a_3 + a_3a_1 + a_3a_2 + a_3a_3)$$ $$E(A^2) = E(a_1a_1 + a_2a_2 + a_3a_3)+ 2 E(a_1a_2 + a_1a_3 + a_2a_3))$$ ...and from here it grinds to a halt. I am not sure how to evaluate these pairings, and what happens to the probabilities in each. For the final product, I get the idea that in squaring we have $n=3$ that are equal, and the remaining being the entire square of values minus these, which would be $n^2-n = 6$, but then how the probabilities are handled is not making sense. I'm not sure why the $n^2-n$ term has a squared probability when the first one has no square. I'm hoping someone can explain this in basic terms.","I'm trying to find how to express the expectation of the square of a collection of binomial measurements. If we have a collection: $$A = a_1 + a_2 + \cdots + a_n$$ The expectation of $A$ is the sum of the expectations of each term of $A$. However, for the square of $A$, I'm getting stuck on how $np +(n^2-n)p^2$ is derived. Without any background in number theory, I'm trying to find it as follows, assuming a set of three trials in $A$: $$E(A^2) = E((a_1 + a_2 + a_3)(a_1 + a_2 + a_3))$$ $$E(A^2) = E(a_1a_1 + a_1a_2 + a_1a_3 + a_2a_1 + a_2a_2 + a_2a_3 + a_3a_1 + a_3a_2 + a_3a_3)$$ $$E(A^2) = E(a_1a_1 + a_2a_2 + a_3a_3)+ 2 E(a_1a_2 + a_1a_3 + a_2a_3))$$ ...and from here it grinds to a halt. I am not sure how to evaluate these pairings, and what happens to the probabilities in each. For the final product, I get the idea that in squaring we have $n=3$ that are equal, and the remaining being the entire square of values minus these, which would be $n^2-n = 6$, but then how the probabilities are handled is not making sense. I'm not sure why the $n^2-n$ term has a squared probability when the first one has no square. I'm hoping someone can explain this in basic terms.",,"['statistics', 'expectation', 'binomial-distribution']"
69,Intuition behind direction of maximum variance?,Intuition behind direction of maximum variance?,,"I'm trying to understand the phrase ""direction of maximum variance"" which keeps popping up in the context of PCA. For example, in this set of 2D points, it is clear they approximately lie on a line. If I could only choose one dimension on which to represent these points it would be that line -- but why does that make this line the direction of maximum variance?","I'm trying to understand the phrase ""direction of maximum variance"" which keeps popping up in the context of PCA. For example, in this set of 2D points, it is clear they approximately lie on a line. If I could only choose one dimension on which to represent these points it would be that line -- but why does that make this line the direction of maximum variance?",,"['linear-algebra', 'statistics']"
70,Understanding of the probability,Understanding of the probability,,"I have a problem with understanding some of my statistics homework. I hope that some of you could help me understand. In summary the question is as follows: There are 30 people in a group, which are split up in 2 groups of 15. The first group we call group1 and the other group2.  Group1 has been placed in an order from 1 till 15 and group 2 is ordered 16 till 30.  In the question we need to find two persons. We do this by removing each time the 3rd person till we only have 2 people left. So for example: 1 2 3 4 5 6 7 8 9 10 11.. 30; We start with number 4, first number 7 will be removed. Then number 10 will be removed. Next will be number 13. Etc, And after number 28 number 1 will be removed. till there are only 2 people left. The start position is random, so it could be 1 till 30. I need to answer the following questions: What is the probability that there are 0 persons left from group1? What is the probability that there is 1 person left from group1? What is the probability that the two persons, who are still there are both from group1? Someone told me the answers, but I do not understand them. So I hope that someone could explain it to me. I got the following answers: 1) 4/30 2) 22/30 3) 4/30","I have a problem with understanding some of my statistics homework. I hope that some of you could help me understand. In summary the question is as follows: There are 30 people in a group, which are split up in 2 groups of 15. The first group we call group1 and the other group2.  Group1 has been placed in an order from 1 till 15 and group 2 is ordered 16 till 30.  In the question we need to find two persons. We do this by removing each time the 3rd person till we only have 2 people left. So for example: 1 2 3 4 5 6 7 8 9 10 11.. 30; We start with number 4, first number 7 will be removed. Then number 10 will be removed. Next will be number 13. Etc, And after number 28 number 1 will be removed. till there are only 2 people left. The start position is random, so it could be 1 till 30. I need to answer the following questions: What is the probability that there are 0 persons left from group1? What is the probability that there is 1 person left from group1? What is the probability that the two persons, who are still there are both from group1? Someone told me the answers, but I do not understand them. So I hope that someone could explain it to me. I got the following answers: 1) 4/30 2) 22/30 3) 4/30",,"['probability', 'statistics']"
71,Nonparametric Skew of Data,Nonparametric Skew of Data,,"Recently in my studies of statistics, I have come across the second skewness coefficient to determine the skewness of the set of data. The formula is given by: $$ \frac{3(\mu - \nu)}{\sigma}$$ However, as a positive skew is indicated by a larger positive number, and a negative one by a larger negative number, I do not see the need for the factor of 3. Pearson thought it important enough to adopt this new way, but now most people simply drop the factor of 3. The reason on Wikipedia is ""..the difference between the mean and the mode for many distributions is approximately three times the difference between the mean and the median"", which I do not understand. Please can someone explain why the factor of 3 is there?  Thanks for the help","Recently in my studies of statistics, I have come across the second skewness coefficient to determine the skewness of the set of data. The formula is given by: $$ \frac{3(\mu - \nu)}{\sigma}$$ However, as a positive skew is indicated by a larger positive number, and a negative one by a larger negative number, I do not see the need for the factor of 3. Pearson thought it important enough to adopt this new way, but now most people simply drop the factor of 3. The reason on Wikipedia is ""..the difference between the mean and the mode for many distributions is approximately three times the difference between the mean and the median"", which I do not understand. Please can someone explain why the factor of 3 is there?  Thanks for the help",,"['statistics', 'statistical-inference']"
72,Solving the probability of independent evnts without the complement,Solving the probability of independent evnts without the complement,,"Suppose that virus transmision in 500 acts of intercourse are mutually independent events and that the probability of transmission in any one act is $\frac{1}{500}$. What is the probability of infection? So I do know that one way to solve this is to find the probability of complement of the event we are trying to solve. Letting $C_1,C_2,C_3...C_{500}$ denote the events that a virus does not occur during encounters 1,2,....500. The probability of no infection is: $$P(C_1\cap C_2 \cap....\cap C_{500}) = (1 - (\frac{1}{500}))^{500} = 0.37$$ then to find the probability of infection I would just do : $1 - 0.37 = 0.63$ but my question is how would I find the probability not using the complement? I would have thought since the events are independent and each with probability of $\frac{1}{500}$ that if I multiplied each independet event I could obtain the value, but that is not the case. What am I forgetting to consider if I wanted to calculate this way?  I'm asking more so to have a fuller understanding of both sides of the coin. Edit: I think I may have figured out what I'm missing in my thinking. In the case of trying to figure out the probability of infection I have to take into account that infection could occur on the first transmission, or the second, or the third,...etc. Also transmission could occur on every interaction or on a few interactions but not all. So in each of these scenarios I would encounter some sort of combination of probabilities like $(\frac{499}{500})(\frac{499}{500})(\frac{1}{500})(\frac{499}{500})......(\frac{1}{500})$ as an example of one possible combination.","Suppose that virus transmision in 500 acts of intercourse are mutually independent events and that the probability of transmission in any one act is $\frac{1}{500}$. What is the probability of infection? So I do know that one way to solve this is to find the probability of complement of the event we are trying to solve. Letting $C_1,C_2,C_3...C_{500}$ denote the events that a virus does not occur during encounters 1,2,....500. The probability of no infection is: $$P(C_1\cap C_2 \cap....\cap C_{500}) = (1 - (\frac{1}{500}))^{500} = 0.37$$ then to find the probability of infection I would just do : $1 - 0.37 = 0.63$ but my question is how would I find the probability not using the complement? I would have thought since the events are independent and each with probability of $\frac{1}{500}$ that if I multiplied each independet event I could obtain the value, but that is not the case. What am I forgetting to consider if I wanted to calculate this way?  I'm asking more so to have a fuller understanding of both sides of the coin. Edit: I think I may have figured out what I'm missing in my thinking. In the case of trying to figure out the probability of infection I have to take into account that infection could occur on the first transmission, or the second, or the third,...etc. Also transmission could occur on every interaction or on a few interactions but not all. So in each of these scenarios I would encounter some sort of combination of probabilities like $(\frac{499}{500})(\frac{499}{500})(\frac{1}{500})(\frac{499}{500})......(\frac{1}{500})$ as an example of one possible combination.",,"['probability', 'statistics', 'probability-theory']"
73,Covariance and Correlation in Multinormal random variable,Covariance and Correlation in Multinormal random variable,,"Find the covariance and correlation of $N_i$ and $N_j$, where $N_1, N_2, \ldots,N_r$ are multinormal random variable. At the beginning, I think that I have: $$P(N_1=n_1,N_2=n_2,\ldots,N_r=n_r)=\frac{1}{n_1!n_2!\cdots n_r!} p_1^{n_1} p_2^{n_2} \cdots p_r^{n_r}$$ Then, I think I should calculate $P(N_i), P(N_j), P(N_i,N_j)$, but I don't know how.","Find the covariance and correlation of $N_i$ and $N_j$, where $N_1, N_2, \ldots,N_r$ are multinormal random variable. At the beginning, I think that I have: $$P(N_1=n_1,N_2=n_2,\ldots,N_r=n_r)=\frac{1}{n_1!n_2!\cdots n_r!} p_1^{n_1} p_2^{n_2} \cdots p_r^{n_r}$$ Then, I think I should calculate $P(N_i), P(N_j), P(N_i,N_j)$, but I don't know how.",,"['probability', 'statistics']"
74,"Follow-on question to ""Fifty men and thirty woman...""","Follow-on question to ""Fifty men and thirty woman...""",,"This questions relates to this question Fifty men and thirty woman are lined up at random. How do I find the expected number of men who have a woman standing next to them. and the answer given by André Nicolas. (I would normally just comment on that question but don't have enough reputation to do that and I am dying to find out what I am missing) It seems to me like the linearity of expectation would apply to independent random events and it seems like the location of the men relative to the women are not independent in this problem.  As you move from man $1$ to $50$, wouldn't the probability of a man standing next to a woman change depending on the number of men and women already seen? If I understand André's answer correctly, the expected number of men standing next to a woman would be: $$2(1-(i))+48(1-(ii)) \approx 32.1$$ I wrote a simulation and came up with approximately $30.62$ as the answer. What am I missing?","This questions relates to this question Fifty men and thirty woman are lined up at random. How do I find the expected number of men who have a woman standing next to them. and the answer given by André Nicolas. (I would normally just comment on that question but don't have enough reputation to do that and I am dying to find out what I am missing) It seems to me like the linearity of expectation would apply to independent random events and it seems like the location of the men relative to the women are not independent in this problem.  As you move from man $1$ to $50$, wouldn't the probability of a man standing next to a woman change depending on the number of men and women already seen? If I understand André's answer correctly, the expected number of men standing next to a woman would be: $$2(1-(i))+48(1-(ii)) \approx 32.1$$ I wrote a simulation and came up with approximately $30.62$ as the answer. What am I missing?",,"['probability', 'statistics']"
75,How do I weight votes based on number of possible voters?,How do I weight votes based on number of possible voters?,,Scenario: I have a book club that reads a book every month. My website allows the readers to give a 5 star rating to each book. We have 10 members and a rule that a member may only vote on a book they have read. At the end of the year we read a book by the highest rated author. The problem: If only one person votes for a book but the other 9 didn't finish the that one person determines the sole vote for that book. Example: Book - number of votes - Average Vote Catcher in the Rye - 7 - 4.0 Catch 22 - 5 - 3.8 The Hunger Games - 1 - 5.0 In this example hunger games would win the year. But clearly Catcher in the Rye is more favored as most of the group just didn't bother to finish hunger games. Desired Outcome An ideal algorithm would weight the votes based on the number possible votes reducing the multiplier as the percentage of votes decreases. votes < 3 - (33% of total available) Multiply average by .5 3 > votes < 7 - (66% of total available) Multiply average by .75 votes > 7 - Vote stands. Is their a linear equation I can use to implement this type of weighting? or is there a more logical method of calculating the votes? Note: I want to avoid making the average by the number of possible voters (add 0 for each non vote) as this would bottom out some of the longer books that people did not get the time to read. A gradual decrease that only considers the votes cast is much preferred.,Scenario: I have a book club that reads a book every month. My website allows the readers to give a 5 star rating to each book. We have 10 members and a rule that a member may only vote on a book they have read. At the end of the year we read a book by the highest rated author. The problem: If only one person votes for a book but the other 9 didn't finish the that one person determines the sole vote for that book. Example: Book - number of votes - Average Vote Catcher in the Rye - 7 - 4.0 Catch 22 - 5 - 3.8 The Hunger Games - 1 - 5.0 In this example hunger games would win the year. But clearly Catcher in the Rye is more favored as most of the group just didn't bother to finish hunger games. Desired Outcome An ideal algorithm would weight the votes based on the number possible votes reducing the multiplier as the percentage of votes decreases. votes < 3 - (33% of total available) Multiply average by .5 3 > votes < 7 - (66% of total available) Multiply average by .75 votes > 7 - Vote stands. Is their a linear equation I can use to implement this type of weighting? or is there a more logical method of calculating the votes? Note: I want to avoid making the average by the number of possible voters (add 0 for each non vote) as this would bottom out some of the longer books that people did not get the time to read. A gradual decrease that only considers the votes cast is much preferred.,,"['statistics', 'voting-theory']"
76,Expected loss in regards to a question containing a continuous random variable with uniform distribution,Expected loss in regards to a question containing a continuous random variable with uniform distribution,,"I have a general question about a homework assignment that deals with a uniform distribution of a continuous variable. Here is the question (and the parts of the question): Suppose parking rules are enforced from 8AM to 12AM (midnight), say from $t = 8$ to $t = 24$. It is also known that the parking rules are enforced once a day. It is also known that a ticket can be issued anytime during the 16-hour period (ie. any time point is equally likely). Let $T$ denote the time of enforcement. a. Suppose you do not have the right to park your vehicle in the parking lot, but you parked there from 10AM ($t = 10$) to 1PM ($t = 13$). What is the probability of receiving a ticket? b. If a ticket is 160 dollars, what is the expected loss from the illegal parking? c. The parking rate is 2 dollars per hour. What is an appropriate action based on the expected loss. Justify your answer. My answer for a and b: a. First of all $S_t = (8,24)$, and since T is continuous, $f(t) = \frac{1}{24 - 8} = \frac{1}{16}, 8 < t < 24$. To find the probability of getting a ticket between $t = 10$ and $t = 13$, I did this: $P(10 < t < 13) = \int_{10}^{13}\frac{1}{16}dt = \frac{3}{16} = 0.1875$ b. This is where I get a little confused. So far, I made a new variable called $L$ which is loss. And the support is $S_L = (0,160)$, where 0 is no ticket and 160 is the price of getting a ticket. For the expected loss, since I think I'm trying to find $E(160)$, then this would just $= 160$. Is it really that easy? Finally, I'm confused for part c. If the parking rate is 2 dollars per hour, then the expected loss if you pay for parking for 3 hours would be $E(6) = 6$. This would mean that in order to be worth it (I think), $E(pay) > E(160)$. But since this never happens in a 16 hour period, then the appropriate action would be to just purchase parking. Is this correct, or am I not taking something into account?","I have a general question about a homework assignment that deals with a uniform distribution of a continuous variable. Here is the question (and the parts of the question): Suppose parking rules are enforced from 8AM to 12AM (midnight), say from $t = 8$ to $t = 24$. It is also known that the parking rules are enforced once a day. It is also known that a ticket can be issued anytime during the 16-hour period (ie. any time point is equally likely). Let $T$ denote the time of enforcement. a. Suppose you do not have the right to park your vehicle in the parking lot, but you parked there from 10AM ($t = 10$) to 1PM ($t = 13$). What is the probability of receiving a ticket? b. If a ticket is 160 dollars, what is the expected loss from the illegal parking? c. The parking rate is 2 dollars per hour. What is an appropriate action based on the expected loss. Justify your answer. My answer for a and b: a. First of all $S_t = (8,24)$, and since T is continuous, $f(t) = \frac{1}{24 - 8} = \frac{1}{16}, 8 < t < 24$. To find the probability of getting a ticket between $t = 10$ and $t = 13$, I did this: $P(10 < t < 13) = \int_{10}^{13}\frac{1}{16}dt = \frac{3}{16} = 0.1875$ b. This is where I get a little confused. So far, I made a new variable called $L$ which is loss. And the support is $S_L = (0,160)$, where 0 is no ticket and 160 is the price of getting a ticket. For the expected loss, since I think I'm trying to find $E(160)$, then this would just $= 160$. Is it really that easy? Finally, I'm confused for part c. If the parking rate is 2 dollars per hour, then the expected loss if you pay for parking for 3 hours would be $E(6) = 6$. This would mean that in order to be worth it (I think), $E(pay) > E(160)$. But since this never happens in a 16 hour period, then the appropriate action would be to just purchase parking. Is this correct, or am I not taking something into account?",,"['probability', 'statistics', 'expectation', 'uniform-continuity']"
77,What is the expected value of the highest of N independent draws from the unit uniform distribution?,What is the expected value of the highest of N independent draws from the unit uniform distribution?,,"I have a seemingly innocuous problem that I can't seem to wrap my head around. The following is mentioned in passing on one of my lecture slides, but when I try to arrive at the same conclusion I get stuck. Consider N independent draws from a uniform distribution over [0, 1]. On average, what is the highest draw? I know that the answer is: $$ \frac{N}{N+1} $$ but I can't arrive there myself. Honestly, I'm not even sure how to start. Can someone sketch a procedure or show me exactly what needs to be done? Thanks!","I have a seemingly innocuous problem that I can't seem to wrap my head around. The following is mentioned in passing on one of my lecture slides, but when I try to arrive at the same conclusion I get stuck. Consider N independent draws from a uniform distribution over [0, 1]. On average, what is the highest draw? I know that the answer is: $$ \frac{N}{N+1} $$ but I can't arrive there myself. Honestly, I'm not even sure how to start. Can someone sketch a procedure or show me exactly what needs to be done? Thanks!",,"['probability', 'statistics']"
78,Paired and unpaired data-Statistics/Hypothesis testing,Paired and unpaired data-Statistics/Hypothesis testing,,"I'm getting a bit confused about paired and unpaired data, for example in this question I don't understand how this data is paired. If it was the same steel pipes that were left uncoated first in the soil to see the effects of corrosion and then taken out and were coated this time and then left in the soil to see the effects of corrosion then I understand the data would have been paired  because  the definition of paired data that I know is when there is one sample which has been tested twice (repeated measures) but here clearly it is different steel pipes that are left uncoated and coated so how is this data paired. So when deciding that data is paired or not what exactly are we supposed to look at? Also can I quickly check: null hypothesis would be: mean1-mean2=0, alternative hypothesis would be mean1-mean2>0, where mean 1 is for uncoated pipes and mean 2 is for coated pipes and our rejection region would be (using a t-statistic) t>t(alpha) , where alpha is 5%","I'm getting a bit confused about paired and unpaired data, for example in this question I don't understand how this data is paired. If it was the same steel pipes that were left uncoated first in the soil to see the effects of corrosion and then taken out and were coated this time and then left in the soil to see the effects of corrosion then I understand the data would have been paired  because  the definition of paired data that I know is when there is one sample which has been tested twice (repeated measures) but here clearly it is different steel pipes that are left uncoated and coated so how is this data paired. So when deciding that data is paired or not what exactly are we supposed to look at? Also can I quickly check: null hypothesis would be: mean1-mean2=0, alternative hypothesis would be mean1-mean2>0, where mean 1 is for uncoated pipes and mean 2 is for coated pipes and our rejection region would be (using a t-statistic) t>t(alpha) , where alpha is 5%",,"['statistics', 'statistical-inference']"
79,What will be $E(X)$ and $Var(X)$?,What will be  and ?,E(X) Var(X),"Given a discrete random variable $$P(X = k) = \frac{1}{2^k}, \ \ \ k=1,2,...$$ How to calculate the expected value and Variance of $X$?","Given a discrete random variable $$P(X = k) = \frac{1}{2^k}, \ \ \ k=1,2,...$$ How to calculate the expected value and Variance of $X$?",,"['probability', 'statistics']"
80,Convergence of the probability of RV with negative binomial distribution,Convergence of the probability of RV with negative binomial distribution,,"I am seeking an answer to this question. Given the number of tools successfully produced $R$ before the $yth$ failure follows a negative binomial distribution $(y,p)$. $$ P(R=r)=\begin{pmatrix}r+y-1\\ r \end{pmatrix}(1-p)^{y}p^{r} $$ Prove that $P(R=r)$ converges to $0$ as $y$ increases. I can see that I need to prove that the factorial ratio  $ \begin{pmatrix}r+y-1\\ r\end{pmatrix} $ is going to infinity slower than $(1-p)^y$ is going to $0$, as $y$ increases.","I am seeking an answer to this question. Given the number of tools successfully produced $R$ before the $yth$ failure follows a negative binomial distribution $(y,p)$. $$ P(R=r)=\begin{pmatrix}r+y-1\\ r \end{pmatrix}(1-p)^{y}p^{r} $$ Prove that $P(R=r)$ converges to $0$ as $y$ increases. I can see that I need to prove that the factorial ratio  $ \begin{pmatrix}r+y-1\\ r\end{pmatrix} $ is going to infinity slower than $(1-p)^y$ is going to $0$, as $y$ increases.",,"['probability', 'statistics']"
81,Testing hypothesis that means are same using t-test and confidence intervals give different results,Testing hypothesis that means are same using t-test and confidence intervals give different results,,"I have two samples and I want to test the null hypothesis that the means of the two samples are the same at a 95% level of confidence interval. When I use a t-test my p value is 0.023 and so I reject the null hypothesis that the means are the same, and conclude there is a significant difference between the means. However when I calculate the 95% confidence intervals of each sample individually the confidence intervals overlap, which suggests to me that we do not have enough evidence to reject the null hypothesis and conclude that the means are different. Is it possible to get different conclusions using these two methods, and if so which one should I trust more? Or have I done something wrong somewhere? Thanks in advance","I have two samples and I want to test the null hypothesis that the means of the two samples are the same at a 95% level of confidence interval. When I use a t-test my p value is 0.023 and so I reject the null hypothesis that the means are the same, and conclude there is a significant difference between the means. However when I calculate the 95% confidence intervals of each sample individually the confidence intervals overlap, which suggests to me that we do not have enough evidence to reject the null hypothesis and conclude that the means are different. Is it possible to get different conclusions using these two methods, and if so which one should I trust more? Or have I done something wrong somewhere? Thanks in advance",,"['statistics', 'hypothesis-testing']"
82,permutation test for one-way ANOVA?,permutation test for one-way ANOVA?,,"I want to know what steps to do the permutation test (and the R/SAS code) for one-way ANOVA? Would you give me the specific steps? Thanks, Buddies!","I want to know what steps to do the permutation test (and the R/SAS code) for one-way ANOVA? Would you give me the specific steps? Thanks, Buddies!",,['statistics']
83,How to test if two sets of data are closely related?,How to test if two sets of data are closely related?,,As part of my masters thesis i am 'Examining the Reliability of Markov Chains  and The Kalman Filter as Stock Market Forecasters'. I will be using the daily returns from the s&p500 over a 5 year period as a benchmark and compare the results from each model against this benchmark to establish which is more accurate. However i am looking for a formal test which will allow me to establish which data set  is more similar to the s&p500? I considered a simple correlation between data sets but i would prefer something more conclusive. Any ideas? I really would appreciate any help. Thank You,As part of my masters thesis i am 'Examining the Reliability of Markov Chains  and The Kalman Filter as Stock Market Forecasters'. I will be using the daily returns from the s&p500 over a 5 year period as a benchmark and compare the results from each model against this benchmark to establish which is more accurate. However i am looking for a formal test which will allow me to establish which data set  is more similar to the s&p500? I considered a simple correlation between data sets but i would prefer something more conclusive. Any ideas? I really would appreciate any help. Thank You,,"['statistics', 'finance']"
84,Suggestions for a good statistics book and a calculus book?,Suggestions for a good statistics book and a calculus book?,,"I am studying machine learning and I am very interested in going into depth. I feel without good knowledge of differential calculus and statistics, it is very difficult to have perfection. I want to know about very good books on statistics and calculus where topics goes from school level to engineering level. Any suggestion will be appreciated.","I am studying machine learning and I am very interested in going into depth. I feel without good knowledge of differential calculus and statistics, it is very difficult to have perfection. I want to know about very good books on statistics and calculus where topics goes from school level to engineering level. Any suggestion will be appreciated.",,"['calculus', 'statistics', 'multivariable-calculus', 'reference-request', 'book-recommendation']"
85,Standard deviation of phase for a random phasor sum,Standard deviation of phase for a random phasor sum,,"I have a phasor sum $a e^{j \theta} = \frac{1}{\sqrt{N}} \sum_{k=1}^{N} \alpha_k e^{j \phi_k }$ where $\phi_k = [-\pi, \pi]$, the standard deviation $\sigma_{\phi}$ of the phase is known and the mean of the phase is $\mu_\phi = 0$. What is the standard deviation of $\theta$?","I have a phasor sum $a e^{j \theta} = \frac{1}{\sqrt{N}} \sum_{k=1}^{N} \alpha_k e^{j \phi_k }$ where $\phi_k = [-\pi, \pi]$, the standard deviation $\sigma_{\phi}$ of the phase is known and the mean of the phase is $\mu_\phi = 0$. What is the standard deviation of $\theta$?",,"['statistics', 'standard-deviation', 'exponential-sum']"
86,Have there been any attempts to unify statistics and decision theory into a single framework that refrains from estimating probabilities?,Have there been any attempts to unify statistics and decision theory into a single framework that refrains from estimating probabilities?,,"If I understand correctly: statistics, narrowly construed, is all about using data to estimate probabilities. decision theory can then be applied to those probabilities in order to predict which decisions maximize expected utility. I may be completely off the mark here, but I have a hunch that this is really the wrong way of going about it. Perhaps we should look for decision-theoretic methods that skip the estimation of probabilities altogether and make utility-maximizing decisions ""straight from the data,"" so to speak. Imaginably, this could circumvent certain difficulties with the Bayesian paradigm, like finding an uninformative prior distribution . Question. Have there been any attempts to unify statistics and decision theory into a single framework that refrains from estimating probabilities, and instead makes utility-maximizing decisions ""straight from the data""? If not, I would also be interested in answers of the form: ""This probably wouldn't work because..."" Please keep it civilized, thanks.","If I understand correctly: statistics, narrowly construed, is all about using data to estimate probabilities. decision theory can then be applied to those probabilities in order to predict which decisions maximize expected utility. I may be completely off the mark here, but I have a hunch that this is really the wrong way of going about it. Perhaps we should look for decision-theoretic methods that skip the estimation of probabilities altogether and make utility-maximizing decisions ""straight from the data,"" so to speak. Imaginably, this could circumvent certain difficulties with the Bayesian paradigm, like finding an uninformative prior distribution . Question. Have there been any attempts to unify statistics and decision theory into a single framework that refrains from estimating probabilities, and instead makes utility-maximizing decisions ""straight from the data""? If not, I would also be interested in answers of the form: ""This probably wouldn't work because..."" Please keep it civilized, thanks.",,"['statistics', 'soft-question', 'decision-theory']"
87,How spicy are my peppers?,How spicy are my peppers?,,"This came naturally to me when I studied statistics ~10 years ago, but for the life of me I can't remember how to work this out: For a given chilli pepper, there is (let's say exactly) $1/100$ chance that the flavour is exceptionally spicy. In a bag of 30 peppers, if I eat all of them, what is the chance that I encounter an exceptionally spicy chilli? I wanted to say that the chance was $30/100 = 0.3$, but quickly realised I was wrong as I know I'm not guaranteed to have eaten a spicy pepper if I eat 100. How can I work this out please?","This came naturally to me when I studied statistics ~10 years ago, but for the life of me I can't remember how to work this out: For a given chilli pepper, there is (let's say exactly) $1/100$ chance that the flavour is exceptionally spicy. In a bag of 30 peppers, if I eat all of them, what is the chance that I encounter an exceptionally spicy chilli? I wanted to say that the chance was $30/100 = 0.3$, but quickly realised I was wrong as I know I'm not guaranteed to have eaten a spicy pepper if I eat 100. How can I work this out please?",,"['probability', 'statistics']"
88,Probability that less dice thrown will beat more dice thrown?,Probability that less dice thrown will beat more dice thrown?,,"I'm trying to figure out a general equation to determine the probability that the sum of $m$ dice will be greater than the sum of $n$ dice, where $m < n$. For example, if I roll $4$ dice, what's the probability that I will beat a roll of $6$ dice? Assume standard $6$ sided dice. I've been trying to adapt the answer I found here , but I think it's really a completely separate problem.","I'm trying to figure out a general equation to determine the probability that the sum of $m$ dice will be greater than the sum of $n$ dice, where $m < n$. For example, if I roll $4$ dice, what's the probability that I will beat a roll of $6$ dice? Assume standard $6$ sided dice. I've been trying to adapt the answer I found here , but I think it's really a completely separate problem.",,"['probability', 'statistics', 'dice']"
89,What's the conditional probability mass function of a Poisson random variable less than t given that it and another Poisson r.v. equal t?,What's the conditional probability mass function of a Poisson random variable less than t given that it and another Poisson r.v. equal t?,,"The full question I'm working on: Let $X_1$ and $X_2$ be two independent Poisson random variables with   mean parameter $\lambda > 0$. Let $T= T(X_1, X_2) = X_1 + X_2$. [Note:   You may use the fact that the sum of two independent Poisson random   variables has a Poisson distribution.] For $t \ge x_1 \ge 0$, find the conditional probability mass function   Pr$\{X_1 = x_1 | X_1 + X_2 = t\}$. Does this conditional distribution   depend on $\lambda$? Be sure to specify the support of this   conditional marginal distribution. I know Pr$\{X_1 = x_1\} = \frac{\lambda^{x_1}}{x_1 !}e^{-\lambda} $ and that Pr$\{X_1 + X_2 = x\} = \frac{(2\lambda)^{x}}{x !}e^{-2\lambda}$. I imagine the answer is something like it, starting with $t$ and removing $X_2$. Maybe $\frac{(2\lambda)^{x}}{x !}e^{-2\lambda}-\frac{\lambda^{x_2}}{x_2 !}e^{-\lambda}$? My intuition is that there is a better answer. Any hints or suggestions are very appreciated.","The full question I'm working on: Let $X_1$ and $X_2$ be two independent Poisson random variables with   mean parameter $\lambda > 0$. Let $T= T(X_1, X_2) = X_1 + X_2$. [Note:   You may use the fact that the sum of two independent Poisson random   variables has a Poisson distribution.] For $t \ge x_1 \ge 0$, find the conditional probability mass function   Pr$\{X_1 = x_1 | X_1 + X_2 = t\}$. Does this conditional distribution   depend on $\lambda$? Be sure to specify the support of this   conditional marginal distribution. I know Pr$\{X_1 = x_1\} = \frac{\lambda^{x_1}}{x_1 !}e^{-\lambda} $ and that Pr$\{X_1 + X_2 = x\} = \frac{(2\lambda)^{x}}{x !}e^{-2\lambda}$. I imagine the answer is something like it, starting with $t$ and removing $X_2$. Maybe $\frac{(2\lambda)^{x}}{x !}e^{-2\lambda}-\frac{\lambda^{x_2}}{x_2 !}e^{-\lambda}$? My intuition is that there is a better answer. Any hints or suggestions are very appreciated.",,"['statistics', 'probability-distributions']"
90,Find the MLE estimator for $\theta$,Find the MLE estimator for,\theta,"Let $Y_1 ,Y_2 ,\ldots,Y_n$ be a random sample from a distribution with pdf $f(y) =  e^{-(y -\theta) }$ for $y \geq 0 $ and $0$ else a) Find the Method of Moments estimator for $\theta$ b) Find the MLE estimator for $\theta$ I'm pretty sure I found out how to do a) but b) I'm having trouble with. Everytime I take the logarithm and then take the derivative, $\theta$ disappears, any help?","Let be a random sample from a distribution with pdf for and else a) Find the Method of Moments estimator for b) Find the MLE estimator for I'm pretty sure I found out how to do a) but b) I'm having trouble with. Everytime I take the logarithm and then take the derivative, disappears, any help?","Y_1 ,Y_2 ,\ldots,Y_n f(y) =  e^{-(y -\theta) } y \geq 0  0 \theta \theta \theta","['statistics', 'maximum-likelihood', 'parameter-estimation']"
91,Expecatation and Variance question,Expecatation and Variance question,,A clown moves forward in jumps of 1 unit to the right or one unit to the left. He jumps left with P=p and right with $P=1-p$. With independence between jumps. Let $C$ be the position of the clown after n jumps. What is $E[C]$ and $V[C]$. I basically said the clown will be  np jumps to the left and  $n(1-p)$ to the right. So  $E[C]=np-n(1-p)=2np-n$. Where a positive expectation means the clown will be positioned to the left. Does this seem correct? And would variance be  $np(1-p)$?,A clown moves forward in jumps of 1 unit to the right or one unit to the left. He jumps left with P=p and right with $P=1-p$. With independence between jumps. Let $C$ be the position of the clown after n jumps. What is $E[C]$ and $V[C]$. I basically said the clown will be  np jumps to the left and  $n(1-p)$ to the right. So  $E[C]=np-n(1-p)=2np-n$. Where a positive expectation means the clown will be positioned to the left. Does this seem correct? And would variance be  $np(1-p)$?,,"['probability', 'statistics', 'expectation']"
92,iid and correlated order statistics a comparison,iid and correlated order statistics a comparison,,"Consider the two random variables $X_1$ and $X_2$ defined via i.i.d, non-negative random variables $Y_1$ and $Y_2$ as $ X_1=\min(Y_1,Y_2)\\$ $ X_2=\min(Y_1,Y_1)$  -  (maximally correlated) The question is can we say $X_1 \leq X_2$ always? How to prove it?","Consider the two random variables $X_1$ and $X_2$ defined via i.i.d, non-negative random variables $Y_1$ and $Y_2$ as $ X_1=\min(Y_1,Y_2)\\$ $ X_2=\min(Y_1,Y_1)$  -  (maximally correlated) The question is can we say $X_1 \leq X_2$ always? How to prove it?",,"['probability', 'statistics', 'order-statistics']"
93,Intuition for proof of Slepians Inequality,Intuition for proof of Slepians Inequality,,"If z is a centered gaussian random variable and $ x_1 ,x_2 ,..,x_n ,y_1,y_2,..,y_n $ are points in $ \mathbb{R}^{2n} $ satisfying $ |x_i-x_j |_2 \leq |y_i -y_j |_2 \ \ \ \forall i,j \in [n] $ then $ E \ max_{i=1} ^n <x_i, z>  \ \leq \ E \ max_{i=1} ^n <y_i, z> . $ Apparently this is not true for other distributions even for centered ones. The proofs I've found are very long and confusing, does anyone have any intuitive explanation for why the gaussian distribution works? Is it purely coincidence that the gaussian distribution also shows up in the central limit theorem?","If z is a centered gaussian random variable and $ x_1 ,x_2 ,..,x_n ,y_1,y_2,..,y_n $ are points in $ \mathbb{R}^{2n} $ satisfying $ |x_i-x_j |_2 \leq |y_i -y_j |_2 \ \ \ \forall i,j \in [n] $ then $ E \ max_{i=1} ^n <x_i, z>  \ \leq \ E \ max_{i=1} ^n <y_i, z> . $ Apparently this is not true for other distributions even for centered ones. The proofs I've found are very long and confusing, does anyone have any intuitive explanation for why the gaussian distribution works? Is it purely coincidence that the gaussian distribution also shows up in the central limit theorem?",,"['linear-algebra', 'probability', 'statistics', 'probability-distributions', 'normal-distribution']"
94,Comparing sample and population standard deviation,Comparing sample and population standard deviation,,"I want to compute the standard deviation of some data points that I obtain during four series of experiments. For the first three experiments that I have conducted, the number of data points that I obtain is quite limited and I am able to compute the ""classic"" population standard deviation. For the fourth experiment however, I obtain a very large number of data points (more than 2^48) for which I can not compute the population standard deviation. I can nethertheless compute the sample standard deviation on say, the first 2^32 data points. I am then wondering, is it right to compare the population standard deviation of the first three experiments with the sample standard deviation of the fourth experiment ? Thanks","I want to compute the standard deviation of some data points that I obtain during four series of experiments. For the first three experiments that I have conducted, the number of data points that I obtain is quite limited and I am able to compute the ""classic"" population standard deviation. For the fourth experiment however, I obtain a very large number of data points (more than 2^48) for which I can not compute the population standard deviation. I can nethertheless compute the sample standard deviation on say, the first 2^32 data points. I am then wondering, is it right to compare the population standard deviation of the first three experiments with the sample standard deviation of the fourth experiment ? Thanks",,"['statistics', 'standard-deviation']"
95,Integration limits and probability density,Integration limits and probability density,,"So I've got the density function for the $2$-dimensional random variable $(X,Y)$: $$p(x,y) = \frac{4}{3}xe^{-x-y} $$ when $ 0 < y < x$. Otherwise, it's $0$. I am now interested in the density of the random variable $W = X + Y$. This is given by: $$\int_{-\infty}^\infty p(x,w-x)dx$$ Fine enough, but now I run into a problem I always have... what are all the limits concerning this problem? Not just for the integral, but for $W$ as a random variable as well; i.e. when does its density function equal $0$, and when does it equal a function $d(w)$?","So I've got the density function for the $2$-dimensional random variable $(X,Y)$: $$p(x,y) = \frac{4}{3}xe^{-x-y} $$ when $ 0 < y < x$. Otherwise, it's $0$. I am now interested in the density of the random variable $W = X + Y$. This is given by: $$\int_{-\infty}^\infty p(x,w-x)dx$$ Fine enough, but now I run into a problem I always have... what are all the limits concerning this problem? Not just for the integral, but for $W$ as a random variable as well; i.e. when does its density function equal $0$, and when does it equal a function $d(w)$?",,['statistics']
96,Is this estimator biased?,Is this estimator biased?,,"I am struggeling to understand the how an estimator is arrived and whether it can be determined it is biased or not. I have this example Let $X_1 , X_2 ,\ldots, X_7$ denote a random sample from a population having mean $\mu$ and variance $\sigma^2$. Consider the following estimator of $\mu$: $$\hat{\Theta}_1 = \frac{X_1 + X_2 +\cdots+ X_7}{7}$$ $$\hat{\Theta}_2 = \frac{2X_1 − X_6 + X_4}{2}$$ a) Is either Estimator unbiased? So since these are trying to estimate the $\mu$, $E[\hat{\Theta}] = \mu$. So i've done it like this   $E[\hat{\Theta}_1] = \frac{E[X_1] + E[X_2]+\cdots+E[X_7]}{7}$. And since the expected value has been taken from the same population, we would expect it would be the same in all cases therefore it can be rewritten as   $E[\hat{\Theta}_1] = \frac{7E[X]}{7} = E[X]$. which lead to showing that it is unbiased. My confusing occurs when i use the Expected value on an estimator estimating $\mu$, it does not make sense to do that, (and seems incorrect to say so), since it's like finding the expected value of $\mu$, which obviously is $\mu$.  So what am I doing wrong?","I am struggeling to understand the how an estimator is arrived and whether it can be determined it is biased or not. I have this example Let $X_1 , X_2 ,\ldots, X_7$ denote a random sample from a population having mean $\mu$ and variance $\sigma^2$. Consider the following estimator of $\mu$: $$\hat{\Theta}_1 = \frac{X_1 + X_2 +\cdots+ X_7}{7}$$ $$\hat{\Theta}_2 = \frac{2X_1 − X_6 + X_4}{2}$$ a) Is either Estimator unbiased? So since these are trying to estimate the $\mu$, $E[\hat{\Theta}] = \mu$. So i've done it like this   $E[\hat{\Theta}_1] = \frac{E[X_1] + E[X_2]+\cdots+E[X_7]}{7}$. And since the expected value has been taken from the same population, we would expect it would be the same in all cases therefore it can be rewritten as   $E[\hat{\Theta}_1] = \frac{7E[X]}{7} = E[X]$. which lead to showing that it is unbiased. My confusing occurs when i use the Expected value on an estimator estimating $\mu$, it does not make sense to do that, (and seems incorrect to say so), since it's like finding the expected value of $\mu$, which obviously is $\mu$.  So what am I doing wrong?",,"['statistics', 'estimation', 'parameter-estimation', 'means']"
97,Comparing standard deviations of two unknown lists,Comparing standard deviations of two unknown lists,,Consider the following problem: List A contains 15 numbers in the range from 10 to 50. List B contains 15 numbers in the range from -50 to -10. Or any other two lists. Is there some trick to compare the standard deviations of the two lists without even knowing what the actual numbers are?,Consider the following problem: List A contains 15 numbers in the range from 10 to 50. List B contains 15 numbers in the range from -50 to -10. Or any other two lists. Is there some trick to compare the standard deviations of the two lists without even knowing what the actual numbers are?,,"['statistics', 'standard-deviation']"
98,Question on regression,Question on regression,,"So I've been given this formula For regression $R^2=1 - \sum \frac{{(y_i -  \hat{y}_i)}^2}{(y_1-\bar{y})^2}$ Now an obvious question that has come to me is why $R^2$ stays the same in certain scenarios, for example: so the regression formulas are: $$\mathrm{TESCO}= 0.2830 + 0.7160 \,\mathrm{FTSE100} $$ $R^2: 10.6% $ $$\mathrm{FTSE100} = 0.8530 + 0.1475 \, \mathrm{TESCO} $$ $R^2: 10.6% $ then why is the coefficient of determination is the same in both cases Any explanation on this would be great, thank you!","So I've been given this formula For regression $R^2=1 - \sum \frac{{(y_i -  \hat{y}_i)}^2}{(y_1-\bar{y})^2}$ Now an obvious question that has come to me is why $R^2$ stays the same in certain scenarios, for example: so the regression formulas are: $$\mathrm{TESCO}= 0.2830 + 0.7160 \,\mathrm{FTSE100} $$ $R^2: 10.6% $ $$\mathrm{FTSE100} = 0.8530 + 0.1475 \, \mathrm{TESCO} $$ $R^2: 10.6% $ then why is the coefficient of determination is the same in both cases Any explanation on this would be great, thank you!",,"['statistics', 'regression']"
99,Mean of Poisson distribution,Mean of Poisson distribution,,Let $X$ have a Poisson distribution with double mode at $x=1$ and $x=2$. Find $ P(X=0)$.Here is my solution: $$\mu= \frac {p(2) 2!}{p(1)}$$ Then how can find the mean? Thanks.,Let $X$ have a Poisson distribution with double mode at $x=1$ and $x=2$. Find $ P(X=0)$.Here is my solution: $$\mu= \frac {p(2) 2!}{p(1)}$$ Then how can find the mean? Thanks.,,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
