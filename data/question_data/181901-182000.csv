,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is it true that any norm could be taken to be the definition of differentiation on $\mathbb{R}^n$?,Is it true that any norm could be taken to be the definition of differentiation on ?,\mathbb{R}^n,"Let $||\cdot||$ be the 2-norm. Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ be a function. Then we define $f'(x)=A$ if $0=\lim_{h\to 0} \frac{||f(x+h)-f(x)-Ah||}{||h||}$. If i take a different norm other than 2-norm, then would the unique linear transformation $A$ differ? If not, how do i prove it? If so, what is a counterexample?","Let $||\cdot||$ be the 2-norm. Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ be a function. Then we define $f'(x)=A$ if $0=\lim_{h\to 0} \frac{||f(x+h)-f(x)-Ah||}{||h||}$. If i take a different norm other than 2-norm, then would the unique linear transformation $A$ differ? If not, how do i prove it? If so, what is a counterexample?",,"['real-analysis', 'multivariable-calculus']"
1,What is wrong with my calculation about a wave equation here?,What is wrong with my calculation about a wave equation here?,,"Consider the following wave equation in ""negative"" time: $$u_{tt}=\Delta u, \quad x\in {\Bbb R}^3, \ \color{red}{t<0}$$   with initial conditions   $$ u(x,0)=g(x),\quad u_t(x,0)=0,\quad x\in{\Bbb R}^3. $$ For $t>0$, define $v:\Bbb{R}^3\times[0,\infty)\to{\Bbb R}$ as  $$v(x,s):=u(x,-s).$$ Then we have $$ v_{s}(x,s)=u_t(x,-s)\cdot(-1),\quad v_{ss}(x,s)=u_{tt}(x,-s)\cdot (-1)(-1)=u_{tt}(x,-s) $$ and $$ \Delta v(x,s)=\Delta u(x,-s). $$ Since $u_{tt}(x,-s)=\Delta u(x,-s)$ for all $s>0$, we thus have $$v_{tt}=\Delta v, \quad v\in {\Bbb R}^3, \ \color{blue}{t>0}$$   with initial conditions   $$ v(x,0)=g(x),\quad v_t(x,0)=0,\quad x\in{\Bbb R}^3. $$   Then by the Kirchhoof's formula, we have   $$ v(x,t)=\frac{\partial}{\partial t}\left(\frac{1}{4\pi t}\int_{B(x;|t|)}g(y)dS(y) \right), $$   which implies that when $t<0$,   $$ u(x,t)=v(x,-t)=\frac{\partial}{\partial t}\left(\frac{1}{4\pi (-t)}\int_{B(x;|t|)}g(y)dS(y) \right). $$ When $g(x)\equiv 1$, using the formula above, we have $$ u(x,t)=\frac{\partial}{\partial t}\left(\frac{1}{4\pi (-t)}(4\pi t^2) \right)=-1. $$ It follows that  $$ \lim_{t\to 0-}u(x,t)=-1\not= u(x,0). $$ What did I do wrong?","Consider the following wave equation in ""negative"" time: $$u_{tt}=\Delta u, \quad x\in {\Bbb R}^3, \ \color{red}{t<0}$$   with initial conditions   $$ u(x,0)=g(x),\quad u_t(x,0)=0,\quad x\in{\Bbb R}^3. $$ For $t>0$, define $v:\Bbb{R}^3\times[0,\infty)\to{\Bbb R}$ as  $$v(x,s):=u(x,-s).$$ Then we have $$ v_{s}(x,s)=u_t(x,-s)\cdot(-1),\quad v_{ss}(x,s)=u_{tt}(x,-s)\cdot (-1)(-1)=u_{tt}(x,-s) $$ and $$ \Delta v(x,s)=\Delta u(x,-s). $$ Since $u_{tt}(x,-s)=\Delta u(x,-s)$ for all $s>0$, we thus have $$v_{tt}=\Delta v, \quad v\in {\Bbb R}^3, \ \color{blue}{t>0}$$   with initial conditions   $$ v(x,0)=g(x),\quad v_t(x,0)=0,\quad x\in{\Bbb R}^3. $$   Then by the Kirchhoof's formula, we have   $$ v(x,t)=\frac{\partial}{\partial t}\left(\frac{1}{4\pi t}\int_{B(x;|t|)}g(y)dS(y) \right), $$   which implies that when $t<0$,   $$ u(x,t)=v(x,-t)=\frac{\partial}{\partial t}\left(\frac{1}{4\pi (-t)}\int_{B(x;|t|)}g(y)dS(y) \right). $$ When $g(x)\equiv 1$, using the formula above, we have $$ u(x,t)=\frac{\partial}{\partial t}\left(\frac{1}{4\pi (-t)}(4\pi t^2) \right)=-1. $$ It follows that  $$ \lim_{t\to 0-}u(x,t)=-1\not= u(x,0). $$ What did I do wrong?",,['multivariable-calculus']
2,Geometrical Application of Calculus with Speed,Geometrical Application of Calculus with Speed,,"Two vehicles are heading for a crossroad (point $C$) and intend to pass straight through. Vehicle $A$ is $100\,\mathrm{km}$ due North travelling at $80\,\mathrm{km}/\mathrm{hr}$ towards $C$ Vehicle $B$ is $150\,\mathrm{km}$ due East travelling at $90\,\mathrm{km}/\mathrm{hr}$ towards $C$ Show that after time $t$, the distance $D$ between them is given by: $$D^2=100(145t^2-430t+325)$$","Two vehicles are heading for a crossroad (point $C$) and intend to pass straight through. Vehicle $A$ is $100\,\mathrm{km}$ due North travelling at $80\,\mathrm{km}/\mathrm{hr}$ towards $C$ Vehicle $B$ is $150\,\mathrm{km}$ due East travelling at $90\,\mathrm{km}/\mathrm{hr}$ towards $C$ Show that after time $t$, the distance $D$ between them is given by: $$D^2=100(145t^2-430t+325)$$",,"['calculus', 'multivariable-calculus', 'proof-verification']"
3,Coordinate transformation on del operator,Coordinate transformation on del operator,,"If there are two sets of orthogonal bases, $\hat{x}_1, ...,\hat{x_n}$ and $\hat{u}_1, ...,\hat{u_n}$, and a point in space can be expressed as $$\vec{r}=x_1\hat{x}_1+...+x_n\hat{x}_n$$ and $$\vec{r}=u_1\hat{u}_1+...+u_n\hat{u}_n$$ with the coordinate transformation $$x_i=f_i(u_1,...,u_n), i=1,2,...,n$$ So the volume element transformation shall be $$dx_1dx_2...dx_n=||A||du_1du_2...du_2$$ where $A$ is a $n\times n$ matrix with $$A_{ij}=\partial{f_i}/\partial{u_j}$$ Is there a similar relationship with the del operator $\nabla$? Given all $f_i$ and the del operator for the first set of bases, and all the projections $\hat{x}_i \cdot \hat{u}_i$, how do I get the del operator for the second set?","If there are two sets of orthogonal bases, $\hat{x}_1, ...,\hat{x_n}$ and $\hat{u}_1, ...,\hat{u_n}$, and a point in space can be expressed as $$\vec{r}=x_1\hat{x}_1+...+x_n\hat{x}_n$$ and $$\vec{r}=u_1\hat{u}_1+...+u_n\hat{u}_n$$ with the coordinate transformation $$x_i=f_i(u_1,...,u_n), i=1,2,...,n$$ So the volume element transformation shall be $$dx_1dx_2...dx_n=||A||du_1du_2...du_2$$ where $A$ is a $n\times n$ matrix with $$A_{ij}=\partial{f_i}/\partial{u_j}$$ Is there a similar relationship with the del operator $\nabla$? Given all $f_i$ and the del operator for the first set of bases, and all the projections $\hat{x}_i \cdot \hat{u}_i$, how do I get the del operator for the second set?",,"['calculus', 'multivariable-calculus']"
4,Check my answer - Finding the jacobi matrix of a function,Check my answer - Finding the jacobi matrix of a function,,"We are given $f: \mathbb R^n \to \mathbb R^n$ such that: $0 \neq x \in \mathbb R^n$, $f(x)=\frac{x}{|x|}$, where $|x| = \sqrt {x_1^2 +x_2^2+...+x_n^2}$ Find the jacobi matrix (the differential matrix) of $f$. My solution: I realized that if $x= \begin{pmatrix} x_1\\x_2\\ \vdots \\x_n \end{pmatrix}$ and $f_1(x_1)= \frac{x_1}{|x|}$ and $f_2(x_2)=\frac{x_2}{|x|}$ and so on then: where $i\neq j$ $$\frac{df_k(x_i)}{dx_j}=-\frac{x_ix_j}{|x|^3}$$ and  $$\frac{df_k(x_i)}{dx_i}=\frac{|x|^2-x_i^2}{|x|^3}$$ explanation: $$f_k(x_j)=\frac{x_j}{\sqrt{x_1^2+...+x_n^2}}$$, then if we derive by $x_i$ then we get $$\frac{-0.5*2x_i*|x|^{-1}*x_j}{|x|^2} = -\frac{x_ix_j}{|x|^3}$$ and if we derive by $x_j$ then the result will be $\frac{|x|^2-x_j^2}{|x|^3}$ So to sum up, in my opinion the jacobi matrix looks like this: in the diagonal, in entry $(i,i)$ we have: $$\frac{|x|^2-x_i^2}{|x|^3}$$ and not on the diagonal. in entry $(i,j)$ we have $$-\frac{x_ix_j}{|x|^3}$$ Is this correct?","We are given $f: \mathbb R^n \to \mathbb R^n$ such that: $0 \neq x \in \mathbb R^n$, $f(x)=\frac{x}{|x|}$, where $|x| = \sqrt {x_1^2 +x_2^2+...+x_n^2}$ Find the jacobi matrix (the differential matrix) of $f$. My solution: I realized that if $x= \begin{pmatrix} x_1\\x_2\\ \vdots \\x_n \end{pmatrix}$ and $f_1(x_1)= \frac{x_1}{|x|}$ and $f_2(x_2)=\frac{x_2}{|x|}$ and so on then: where $i\neq j$ $$\frac{df_k(x_i)}{dx_j}=-\frac{x_ix_j}{|x|^3}$$ and  $$\frac{df_k(x_i)}{dx_i}=\frac{|x|^2-x_i^2}{|x|^3}$$ explanation: $$f_k(x_j)=\frac{x_j}{\sqrt{x_1^2+...+x_n^2}}$$, then if we derive by $x_i$ then we get $$\frac{-0.5*2x_i*|x|^{-1}*x_j}{|x|^2} = -\frac{x_ix_j}{|x|^3}$$ and if we derive by $x_j$ then the result will be $\frac{|x|^2-x_j^2}{|x|^3}$ So to sum up, in my opinion the jacobi matrix looks like this: in the diagonal, in entry $(i,i)$ we have: $$\frac{|x|^2-x_i^2}{|x|^3}$$ and not on the diagonal. in entry $(i,j)$ we have $$-\frac{x_ix_j}{|x|^3}$$ Is this correct?",,"['calculus', 'multivariable-calculus', 'derivatives', 'normed-spaces']"
5,Open and connected in $R^n$ revised,Open and connected in  revised,R^n,I am trying to understand the following: If we have an open and connected set in $R^n$ then it can be connected with line segments parallel to the axes. I managed to prove this:  If a set $U$ is open and connected in $\mathbb{R}^n$ then we can prove it is polygonally connected(there is a path formed from line segments completely contained in $U$). My question now is how would I modify the path such that the line segments remain in $U$ and they are now parallel to the axes? I would very much appreciate some help. Thank you!,I am trying to understand the following: If we have an open and connected set in $R^n$ then it can be connected with line segments parallel to the axes. I managed to prove this:  If a set $U$ is open and connected in $\mathbb{R}^n$ then we can prove it is polygonally connected(there is a path formed from line segments completely contained in $U$). My question now is how would I modify the path such that the line segments remain in $U$ and they are now parallel to the axes? I would very much appreciate some help. Thank you!,,"['real-analysis', 'analysis', 'multivariable-calculus', 'connectedness']"
6,Calculating a multivariate probability density - how to invert the function?,Calculating a multivariate probability density - how to invert the function?,,"This is an example from a lecture, however it was presented without proof, so I'm trying to find a way to calculate the PDF for the given condition: $X_1$ and $X_2$ are two independent random variables with an uniform distribution on $(0,1)$. For the random variables: $(Z_1, Z_2) := \sqrt{-2 \ln X_1} (\cos 2\pi X_2, \sin 2\pi X_2)$ calculate the PDF. My reasoning is as follows: Use the formula for $Z = Z(X)$: $\;\rho_Z(\mathbf{z}) = \rho_X[\mathbf{x}(\mathbf{z})] \cdot \left| \frac{ \partial \mathbf{x} }{ \partial \mathbf{z} } \right|$ I can calculate the Jacobian: $$\frac{\partial (Z_1, Z_2)}{\partial (X_1, X_2)} = \begin{vmatrix} -\frac{\cos (2 \pi X_2)}{\sqrt{2} X_1 \sqrt{-\ln{X_1}}} & -\frac{\sin (2 \pi X_2)}{\sqrt{2} X_1 \sqrt{-\ln X_1 }} \\ -2 \sqrt{2} \pi \sqrt{-\ln X_1} \sin(2 \pi X_2) & 2 \sqrt{2} \pi \cos (2 \pi X_2) \sqrt{-\ln X_1} \end{vmatrix} $$ and then the inverse: $$\frac{\partial (X_1, X_2)}{\partial (Z_1, Z_2)} = \begin{vmatrix} -\sqrt{2} X_1 \cos (2 \pi X_2) \sqrt{- \ln X_1} & -\frac{\sin (2 \pi X_2)}{2 \sqrt{2} \pi \sqrt{- \ln X_1}} \\ -\sqrt{2} X_1 \sqrt{-\ln{X_1} \sin (2 \pi X_2)} & \frac{\cos (2 \pi X_2)}{2 \sqrt{2} \pi \sqrt{- \ln X_1}} \end{vmatrix} $$ Then multiply the inverse Jacobian by $\rho_{(X_1, X_2)}(x_1,x_2)$ to get: $$ \begin{pmatrix} X_1 \left(1 + \cos (4 \pi X_2) \right) \ln X_1 - \frac{ \sin^2 (2 \pi x_2) }{2 \pi} & \frac{ (1 + 4 \pi X_1 \ln X_1) \sin (4 \pi X_2) }{ 4 \pi} \end{pmatrix} $$ But how can I obtain the relation this inverse relation $(X_1(Z_1, Z_2), X_2(Z_1, Z_2))$? Am I suppose to calculate this PDF by different means? All my calculation are from Mathematica. Thanks for any suggestions!","This is an example from a lecture, however it was presented without proof, so I'm trying to find a way to calculate the PDF for the given condition: $X_1$ and $X_2$ are two independent random variables with an uniform distribution on $(0,1)$. For the random variables: $(Z_1, Z_2) := \sqrt{-2 \ln X_1} (\cos 2\pi X_2, \sin 2\pi X_2)$ calculate the PDF. My reasoning is as follows: Use the formula for $Z = Z(X)$: $\;\rho_Z(\mathbf{z}) = \rho_X[\mathbf{x}(\mathbf{z})] \cdot \left| \frac{ \partial \mathbf{x} }{ \partial \mathbf{z} } \right|$ I can calculate the Jacobian: $$\frac{\partial (Z_1, Z_2)}{\partial (X_1, X_2)} = \begin{vmatrix} -\frac{\cos (2 \pi X_2)}{\sqrt{2} X_1 \sqrt{-\ln{X_1}}} & -\frac{\sin (2 \pi X_2)}{\sqrt{2} X_1 \sqrt{-\ln X_1 }} \\ -2 \sqrt{2} \pi \sqrt{-\ln X_1} \sin(2 \pi X_2) & 2 \sqrt{2} \pi \cos (2 \pi X_2) \sqrt{-\ln X_1} \end{vmatrix} $$ and then the inverse: $$\frac{\partial (X_1, X_2)}{\partial (Z_1, Z_2)} = \begin{vmatrix} -\sqrt{2} X_1 \cos (2 \pi X_2) \sqrt{- \ln X_1} & -\frac{\sin (2 \pi X_2)}{2 \sqrt{2} \pi \sqrt{- \ln X_1}} \\ -\sqrt{2} X_1 \sqrt{-\ln{X_1} \sin (2 \pi X_2)} & \frac{\cos (2 \pi X_2)}{2 \sqrt{2} \pi \sqrt{- \ln X_1}} \end{vmatrix} $$ Then multiply the inverse Jacobian by $\rho_{(X_1, X_2)}(x_1,x_2)$ to get: $$ \begin{pmatrix} X_1 \left(1 + \cos (4 \pi X_2) \right) \ln X_1 - \frac{ \sin^2 (2 \pi x_2) }{2 \pi} & \frac{ (1 + 4 \pi X_1 \ln X_1) \sin (4 \pi X_2) }{ 4 \pi} \end{pmatrix} $$ But how can I obtain the relation this inverse relation $(X_1(Z_1, Z_2), X_2(Z_1, Z_2))$? Am I suppose to calculate this PDF by different means? All my calculation are from Mathematica. Thanks for any suggestions!",,"['probability', 'multivariable-calculus']"
7,SOLVED: Green's theorem result and line integral result are not equal! What am I doing wrong?,SOLVED: Green's theorem result and line integral result are not equal! What am I doing wrong?,,"I have this line integral: $\oint 3ydx+x^2dy$ and the path is a line from $(0, 0)$ to $(1, 0)$ (so this is $y=0$), another line from $(1, 0)$ to $(1, 1)$ (so this is $x=1$) and a curve $y=x^2$ from $(1, 1)$ to $(0, 0)$. Evaluating this integral using the line integral (and anticlockwise = positive): 1) $y=0$ gives $0$ 2) $x=1$ gives $0$ as well Edit: this gives actually $1$ 3) $y=x^2$ $dy=2xdx$ and substituing everything in gives $\oint 3x^2dx+x^2*2xdx=\oint 3x^2+2x^3dx$. The limits are from $1$ to $0$, so $\int_1^0 3x^2+2x^3dx=-1.5$ Adding everything gives $-1.5$ Edit: This becomes actually $-0.5$ Now using Green's theorem: Finding the partial derivatives and substituing these into the Green's formula gives: $\int_0^1\int_0^{x^2}(2x-3)dydx=-0.5$ What am I doing wrong because obviously $-0.5 \neq -1.5$? Edit: $-0.5 = -0.5$ Thanks!","I have this line integral: $\oint 3ydx+x^2dy$ and the path is a line from $(0, 0)$ to $(1, 0)$ (so this is $y=0$), another line from $(1, 0)$ to $(1, 1)$ (so this is $x=1$) and a curve $y=x^2$ from $(1, 1)$ to $(0, 0)$. Evaluating this integral using the line integral (and anticlockwise = positive): 1) $y=0$ gives $0$ 2) $x=1$ gives $0$ as well Edit: this gives actually $1$ 3) $y=x^2$ $dy=2xdx$ and substituing everything in gives $\oint 3x^2dx+x^2*2xdx=\oint 3x^2+2x^3dx$. The limits are from $1$ to $0$, so $\int_1^0 3x^2+2x^3dx=-1.5$ Adding everything gives $-1.5$ Edit: This becomes actually $-0.5$ Now using Green's theorem: Finding the partial derivatives and substituing these into the Green's formula gives: $\int_0^1\int_0^{x^2}(2x-3)dydx=-0.5$ What am I doing wrong because obviously $-0.5 \neq -1.5$? Edit: $-0.5 = -0.5$ Thanks!",,"['integration', 'multivariable-calculus', 'definite-integrals']"
8,Differentiability in $R^n$,Differentiability in,R^n,"I have the definition of the derivative for $f:\mathbb R^n \rightarrow\mathbb  R^m$ at a point $a$ as: $f$ is differentiable at a then there exists a linear map $L:\mathbb  R^n \rightarrow\mathbb  R^m$ such that $\frac{f(a+h)-f(a)-L(h)}{||h||} \rightarrow 0$ as $h \rightarrow 0$ . (0, a, h are vectors in $\mathbb R^n$). I am trying to derive from this definition that the following holds: If we denote by $D(f(a))$ = total derivative of $f$ at point $a$ then it is true that if $f$ is differentiable at $a$: $||f(x) - f(a) -D(f(a))(x-a)|| < \epsilon ||x-a||$ for $x$ close to $a$. I know this seems (or it is) very easy but since the total derivatives is now a matrix of partial derviative I am not sure how to continue a proof. Extending it from $\mathbb R$ seems dangerous because I cannot say $D(f(a))(x-a) = L(x-a)$ or is this true?","I have the definition of the derivative for $f:\mathbb R^n \rightarrow\mathbb  R^m$ at a point $a$ as: $f$ is differentiable at a then there exists a linear map $L:\mathbb  R^n \rightarrow\mathbb  R^m$ such that $\frac{f(a+h)-f(a)-L(h)}{||h||} \rightarrow 0$ as $h \rightarrow 0$ . (0, a, h are vectors in $\mathbb R^n$). I am trying to derive from this definition that the following holds: If we denote by $D(f(a))$ = total derivative of $f$ at point $a$ then it is true that if $f$ is differentiable at $a$: $||f(x) - f(a) -D(f(a))(x-a)|| < \epsilon ||x-a||$ for $x$ close to $a$. I know this seems (or it is) very easy but since the total derivatives is now a matrix of partial derviative I am not sure how to continue a proof. Extending it from $\mathbb R$ seems dangerous because I cannot say $D(f(a))(x-a) = L(x-a)$ or is this true?",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
9,Harmonic map into sphere,Harmonic map into sphere,,"Let $B$ be the unit ball and $S$ the unit sphere in $\mathbb{R}^3$. Consider the map $u: B\rightarrow S$ defined as: \begin{equation} u^j(x)=\frac{x_j}{|x|}\quad\forall \ j =1, 2, 3. \end{equation}I would like to show that for each $j=1, 2, 3,$ we have \begin{equation} -\triangle u^j=|Du|^2u^j\quad \text{in }B\setminus\{0\}. \end{equation} My working is as below but I can't exactly establish the relationship; I am missing a factor of two and I don't know where I am going wrong. Firstly, we have \begin{equation} \frac{\partial u^j}{\partial x_i}=\frac{|x|\delta_{ij}-x_jx_i|x|^{-1}}{|x|^2}=\frac{\delta_{ij}}{|x|}-\frac{x_ix_j}{|x|^3} \end{equation}where $\delta_{ij}=1$ if $i=j$ and zero otherwise. Consequently, \begin{align} |Du|^2=\sum_{j=1}^3\sum_{i=1}^{3}\left(\frac{\partial u^j}{\partial x_i}\right)^2&=\sum_{j=1}^3\sum_{i=1}^{3}\frac{|x|^2\delta_{ij}-2\delta_{ij}x_ix_j+|x|^{-2}x_j^2x_i^2}{|x|^4}\\ &=\frac{3}{|x|^2}-\frac{2}{|x|^2}+\frac{1}{|x|^2}\\ &=\frac{2}{|x|^2}. \end{align} So \begin{equation} |Du|^2u^j=\frac{2x_j}{|x|^3}. \end{equation} For the left hand side, however, I end up with \begin{align} -\sum_{i=1}^3\frac{\partial}{\partial x_i}\left(\frac{\partial u^j}{\partial x_i}\right)&=\sum_{i=1}^3-\frac{\partial}{\partial x_i}\left(\frac{\delta_{ij}}{|x|}\right)+\frac{\partial}{\partial x_i}\left(\frac{x_ix_j}{|x|^3}\right)\\ &=\sum_{i=1}^3\frac{\delta_{ij}x_i}{|x|^3}+\frac{|x|^3x_j-3|x|x_i^2x_j}{|x|^6}\\ &=\frac{x_j}{|x|^3} \end{align} Somewhere I am missing a factor of two and I can't spot it.","Let $B$ be the unit ball and $S$ the unit sphere in $\mathbb{R}^3$. Consider the map $u: B\rightarrow S$ defined as: \begin{equation} u^j(x)=\frac{x_j}{|x|}\quad\forall \ j =1, 2, 3. \end{equation}I would like to show that for each $j=1, 2, 3,$ we have \begin{equation} -\triangle u^j=|Du|^2u^j\quad \text{in }B\setminus\{0\}. \end{equation} My working is as below but I can't exactly establish the relationship; I am missing a factor of two and I don't know where I am going wrong. Firstly, we have \begin{equation} \frac{\partial u^j}{\partial x_i}=\frac{|x|\delta_{ij}-x_jx_i|x|^{-1}}{|x|^2}=\frac{\delta_{ij}}{|x|}-\frac{x_ix_j}{|x|^3} \end{equation}where $\delta_{ij}=1$ if $i=j$ and zero otherwise. Consequently, \begin{align} |Du|^2=\sum_{j=1}^3\sum_{i=1}^{3}\left(\frac{\partial u^j}{\partial x_i}\right)^2&=\sum_{j=1}^3\sum_{i=1}^{3}\frac{|x|^2\delta_{ij}-2\delta_{ij}x_ix_j+|x|^{-2}x_j^2x_i^2}{|x|^4}\\ &=\frac{3}{|x|^2}-\frac{2}{|x|^2}+\frac{1}{|x|^2}\\ &=\frac{2}{|x|^2}. \end{align} So \begin{equation} |Du|^2u^j=\frac{2x_j}{|x|^3}. \end{equation} For the left hand side, however, I end up with \begin{align} -\sum_{i=1}^3\frac{\partial}{\partial x_i}\left(\frac{\partial u^j}{\partial x_i}\right)&=\sum_{i=1}^3-\frac{\partial}{\partial x_i}\left(\frac{\delta_{ij}}{|x|}\right)+\frac{\partial}{\partial x_i}\left(\frac{x_ix_j}{|x|^3}\right)\\ &=\sum_{i=1}^3\frac{\delta_{ij}x_i}{|x|^3}+\frac{|x|^3x_j-3|x|x_i^2x_j}{|x|^6}\\ &=\frac{x_j}{|x|^3} \end{align} Somewhere I am missing a factor of two and I can't spot it.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'partial-differential-equations', 'proof-verification']"
10,At which point the plane tangent to the surface is horizontal?,At which point the plane tangent to the surface is horizontal?,,"I am given the surface $z = x^{2} - 7xy - y^{2} - 46x + 2y$ and have to find the point, among four options, at which the tangent plane to the surface is horizontal. Now my reasoning is this: if we write $F (x, y, z) = x^{2} - 7xy - y^{2} - 46x + 2y - z,$ the tangent plane to the surface at the point $(a, b, c)$ is $$(2a - 7b - 46)(x - a) + (-7a - 2b + 2) (y - b) + (-1) (z - c) = 0,$$ and since horizontal planes have the equation $z = k$, for some constant $k$, we need find the point which makes $2a - 7b - 46$ and $-7a - 2b + 2$ zero. The problem is that two of the possible answers meet this condition: $(2, -6, -52)$ and $(2, -6, -26)$. So my question is, am I doing something wrong?","I am given the surface $z = x^{2} - 7xy - y^{2} - 46x + 2y$ and have to find the point, among four options, at which the tangent plane to the surface is horizontal. Now my reasoning is this: if we write $F (x, y, z) = x^{2} - 7xy - y^{2} - 46x + 2y - z,$ the tangent plane to the surface at the point $(a, b, c)$ is $$(2a - 7b - 46)(x - a) + (-7a - 2b + 2) (y - b) + (-1) (z - c) = 0,$$ and since horizontal planes have the equation $z = k$, for some constant $k$, we need find the point which makes $2a - 7b - 46$ and $-7a - 2b + 2$ zero. The problem is that two of the possible answers meet this condition: $(2, -6, -52)$ and $(2, -6, -26)$. So my question is, am I doing something wrong?",,['multivariable-calculus']
11,Multivariable optimization books,Multivariable optimization books,,"I have some economic data in hand, and I would like to make forecasting out of it (e.g., consumer demand, price elasticity and so on). As far as I understand, these characteristics can be (to some extent) approximated by polynomials. I know about Lagrange interpolation. If there were some data of input variable X and output F(x) I would easily interpolate by Lagrange, and predict F(x) for the future values. But there is a lot of input variables in my data, and I am not sure where to start. How should I analyze this data? Maybe fix some input variables and try to interpolate the rest? Maybe I have not enough data to make these predictions? So I would like to get some directions on where to start. If you could suggest some books or articles on this subject (preferrably focused on practice), that would be perfect. I know basic algebra and calculus, but haven't worked with optimization and prediction on real data. UPD . When I asked this on mathoverflow, it was suggested to ask it here, so I apologize for multiple postings. Folks there recommended Ken Judd's Numerical Methods in Economics book, but as far as I get out of Google Books, it is too theoretical for me, because what I want is to solve a practical problem. Ideally, I would like the examples in the books to be solved with Matlab/Mathematica/Excel. Thank you. UPD2. Ok, answering a clarification, I would be more specific. I have a data of a production and trade company for some period. It is monthly-tabulated and contains money spent for advertisement in that month (in journals and Internet, let's denote A1 and A2 respectively), good price P for that month, number of good units sold for that month S (filled post-factum). In fact, S = S(A1, A2, P) is a multivariable function. In reality, number of input variables is slightly larger (seasonal changes that affect customers' demand, competitors prices that are also tabulated, let's say up to 6 input variables). What I want to do is to predict S(A1, A2, P) for the coming month given A1, A2, P, i.e. to predict sales.","I have some economic data in hand, and I would like to make forecasting out of it (e.g., consumer demand, price elasticity and so on). As far as I understand, these characteristics can be (to some extent) approximated by polynomials. I know about Lagrange interpolation. If there were some data of input variable X and output F(x) I would easily interpolate by Lagrange, and predict F(x) for the future values. But there is a lot of input variables in my data, and I am not sure where to start. How should I analyze this data? Maybe fix some input variables and try to interpolate the rest? Maybe I have not enough data to make these predictions? So I would like to get some directions on where to start. If you could suggest some books or articles on this subject (preferrably focused on practice), that would be perfect. I know basic algebra and calculus, but haven't worked with optimization and prediction on real data. UPD . When I asked this on mathoverflow, it was suggested to ask it here, so I apologize for multiple postings. Folks there recommended Ken Judd's Numerical Methods in Economics book, but as far as I get out of Google Books, it is too theoretical for me, because what I want is to solve a practical problem. Ideally, I would like the examples in the books to be solved with Matlab/Mathematica/Excel. Thank you. UPD2. Ok, answering a clarification, I would be more specific. I have a data of a production and trade company for some period. It is monthly-tabulated and contains money spent for advertisement in that month (in journals and Internet, let's denote A1 and A2 respectively), good price P for that month, number of good units sold for that month S (filled post-factum). In fact, S = S(A1, A2, P) is a multivariable function. In reality, number of input variables is slightly larger (seasonal changes that affect customers' demand, competitors prices that are also tabulated, let's say up to 6 input variables). What I want to do is to predict S(A1, A2, P) for the coming month given A1, A2, P, i.e. to predict sales.",,"['multivariable-calculus', 'optimization', 'economics']"
12,Finding a vector parametric equation given P and Q equations?,Finding a vector parametric equation given P and Q equations?,,"Find a vector parametric equation $r⃗(t)$ for the line through the points $P=(3,5,4)$ and $Q=(1,4,7)$ for each of the given conditions on the parameter $t$. If $r⃗ (0)=(3,5,4)$ and $r⃗ (7)=(1,4,7)$, then $r⃗ (t)= $? I found the vector for $PQ$ and got $(-2,-1,3)$. I then took the original $P$ and made $r(t) = (3,5,4) + t (-2,-1,3)$. Not sure if I am just not really understanding the idea, but does anyone know how to solve this?","Find a vector parametric equation $r⃗(t)$ for the line through the points $P=(3,5,4)$ and $Q=(1,4,7)$ for each of the given conditions on the parameter $t$. If $r⃗ (0)=(3,5,4)$ and $r⃗ (7)=(1,4,7)$, then $r⃗ (t)= $? I found the vector for $PQ$ and got $(-2,-1,3)$. I then took the original $P$ and made $r(t) = (3,5,4) + t (-2,-1,3)$. Not sure if I am just not really understanding the idea, but does anyone know how to solve this?",,"['multivariable-calculus', 'parametric']"
13,Distance between point and plane,Distance between point and plane,,"Find the distance from the Point $A = (1,0,2)$ to the plane passing through the point $(1,-2,1)$ and perpendicular to the line given by the parametric equations: $$ \begin{align} x & = 7, \\  y & = 1 + 2t, \\ z & = -3 + t. \end{align} $$ The answer is $\sqrt{5}$, but I can't seem to get that. I get that the plane equation ends up being $0x + 2y + z + 3 = 0$, but then when I try to compute the distance it turns out to be $\sqrt{3}/\sqrt{5}$ or something along those lines.","Find the distance from the Point $A = (1,0,2)$ to the plane passing through the point $(1,-2,1)$ and perpendicular to the line given by the parametric equations: $$ \begin{align} x & = 7, \\  y & = 1 + 2t, \\ z & = -3 + t. \end{align} $$ The answer is $\sqrt{5}$, but I can't seem to get that. I get that the plane equation ends up being $0x + 2y + z + 3 = 0$, but then when I try to compute the distance it turns out to be $\sqrt{3}/\sqrt{5}$ or something along those lines.",,"['multivariable-calculus', 'vectors']"
14,Area of a sphere bounded by a paraboloid,Area of a sphere bounded by a paraboloid,,"I need to find the area of the surface $x^2+y^2+z^2 = a^2$ for $y^2 \ge a(a+x)$. I know that $A = 4a \int_{-a}^0 dx \int_{\sqrt{a^2+ax}}^{\sqrt{a^2-x^2}} \frac{dy}{\sqrt{a^2-x^2-y^2}}$, but I have trouble finding the limits of integration when converting the integral into polar coordinates: $A = 4a \int_{\frac{\pi}{2}}^{\pi} d \phi \int_{r(\phi)}^{a} \frac{rdr}{\sqrt{a^2-r^2}}$ where $r(\phi)$ solves $r^2 \sin^2 \phi - a \cos \phi r - a^2 =0$. Is there anything I'm doing wrong? Thank you.","I need to find the area of the surface $x^2+y^2+z^2 = a^2$ for $y^2 \ge a(a+x)$. I know that $A = 4a \int_{-a}^0 dx \int_{\sqrt{a^2+ax}}^{\sqrt{a^2-x^2}} \frac{dy}{\sqrt{a^2-x^2-y^2}}$, but I have trouble finding the limits of integration when converting the integral into polar coordinates: $A = 4a \int_{\frac{\pi}{2}}^{\pi} d \phi \int_{r(\phi)}^{a} \frac{rdr}{\sqrt{a^2-r^2}}$ where $r(\phi)$ solves $r^2 \sin^2 \phi - a \cos \phi r - a^2 =0$. Is there anything I'm doing wrong? Thank you.",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
15,"Can we find $t₂∈(0,1)$ such that $f(α+t₂(μ-α),β+t₂(ρ-β))≠0$",Can we find  such that,"t₂∈(0,1) f(α+t₂(μ-α),β+t₂(ρ-β))≠0","Let $f:ℝ²→ℝ$ be an arbitrary non-zero harmonic function. Assume that there exists $t₁∈(0,1)$ such that $f(α+t₁(μ-α),β+t₁(ρ-β))=0$. Then my question is: Can we find $t₂∈(0,1)$ such that $t₂≠t₁$ and $f(α+t₂(μ-α),β+t₂(ρ-β))≠0$. Here $α,μ,β,ρ$ are constants.","Let $f:ℝ²→ℝ$ be an arbitrary non-zero harmonic function. Assume that there exists $t₁∈(0,1)$ such that $f(α+t₁(μ-α),β+t₁(ρ-β))=0$. Then my question is: Can we find $t₂∈(0,1)$ such that $t₂≠t₁$ and $f(α+t₂(μ-α),β+t₂(ρ-β))≠0$. Here $α,μ,β,ρ$ are constants.",,"['real-analysis', 'multivariable-calculus']"
16,Derivative of a Linear Map,Derivative of a Linear Map,,"I'm devastatingly incompetent at linear algebra and multivariable calculus. I just cannot understand it at all. Here's the easiest problem from my homework, and my attempt at solving it, and where I am stuck. Hopefully if I find at least one point of illumination in this problem, I may be able to find more light in this cold dark dungeon. The Problem: Suppose that $f:R^n\to R^m$ is linear. Show that $Df(x)$ exists for each $x\in R^n$ and $Df(x)=f(x)$ My attempt: Take any $x\in R^n$. Consider $$\lim_{h\to 0}\|f(x+h)-f(x)-Th\|/\|h\| = \lim_{h\to 0}\|f(x)+f(h)-f(x)-Th\|/\|h\| = \lim_{h\to 0}\|f(h)-Th\|/\|h\|$$ If there exists a linear map T such that this limit equals 0, then f is differentiable at x and $Df(x)=T$. My issues: I don't know where to go from there. If I assumed that $Th=Df(h)=f(h)$, I know that $$\lim_{h\to 0}\|f(h)-f(h)\|/\|h\|=0$$, but I don't know how to prove it the other way around.","I'm devastatingly incompetent at linear algebra and multivariable calculus. I just cannot understand it at all. Here's the easiest problem from my homework, and my attempt at solving it, and where I am stuck. Hopefully if I find at least one point of illumination in this problem, I may be able to find more light in this cold dark dungeon. The Problem: Suppose that $f:R^n\to R^m$ is linear. Show that $Df(x)$ exists for each $x\in R^n$ and $Df(x)=f(x)$ My attempt: Take any $x\in R^n$. Consider $$\lim_{h\to 0}\|f(x+h)-f(x)-Th\|/\|h\| = \lim_{h\to 0}\|f(x)+f(h)-f(x)-Th\|/\|h\| = \lim_{h\to 0}\|f(h)-Th\|/\|h\|$$ If there exists a linear map T such that this limit equals 0, then f is differentiable at x and $Df(x)=T$. My issues: I don't know where to go from there. If I assumed that $Th=Df(h)=f(h)$, I know that $$\lim_{h\to 0}\|f(h)-f(h)\|/\|h\|=0$$, but I don't know how to prove it the other way around.",,"['linear-algebra', 'ordinary-differential-equations', 'multivariable-calculus', 'derivatives']"
17,Divergence as a surface integral,Divergence as a surface integral,,"I had a shot at the final problem of section 16.4 in 'Calculus a Complete Course' by Adams, I knew full well that I wasn't even going to come close to a correct answer so my question relates to the answer provided in the solution manual. Note: I can't get the surface integral to show up as a closed surface integral, so where ever you see a double integral please assume that i'm referring to the integral over a closed surface. Problem statement: Let $P_{0}$ be a fixed point and for each $\epsilon>0$ let $D_\epsilon$ and $S_\epsilon$ define the the domain and boundary respectively. (such that the conditions of the Divergence Theorem are satisfied) Suppose that the maximum distance from $P_0$ to some other point $P\in D_\epsilon$ approach zero as $\epsilon\rightarrow 0^+$. If $D_\epsilon$ has volume $vol(D_\epsilon)$, show that $$\lim_{\epsilon\rightarrow 0^+} \frac{1}{vol(D_\epsilon)} \iint_{S_\epsilon} \boldsymbol{F\cdot\hat{N}} dS = \nabla\cdot\boldsymbol{F}(P_0)$$ My Problem: So the proof given by Adams starts out by stating the divergence theorem and dividing both sides of the equation by the volume of the domain. Some algebraic manipulation brings me to this point. $$\begin{array}{lcr} \left|\frac{1}{vol(D_\epsilon)} \iint_{S_\epsilon}\boldsymbol{F\cdot\hat{N}}\, dS - \nabla\cdot\boldsymbol{F}(P_0)\right| & \leq &\frac{1}{vol(D_\epsilon)} \iiint_{D_\epsilon}\left|\nabla\cdot\boldsymbol{F}-\nabla\cdot\boldsymbol{F}(P_0)\right|dV \\& \leq & \max\limits_{P\in D_\epsilon} \left|\nabla\cdot\boldsymbol{F}-\nabla\cdot\boldsymbol{F}(P_0)\right|\end{array}$$ Now the first line seems to result from some algebraic manipulation and application of the triangle inequality, but the jump to the second line baffles me. (Hopefully i'm not overlooking something that's painfully obvious) Ps. cannot figure out how to get the expression on the second line to align properly.","I had a shot at the final problem of section 16.4 in 'Calculus a Complete Course' by Adams, I knew full well that I wasn't even going to come close to a correct answer so my question relates to the answer provided in the solution manual. Note: I can't get the surface integral to show up as a closed surface integral, so where ever you see a double integral please assume that i'm referring to the integral over a closed surface. Problem statement: Let $P_{0}$ be a fixed point and for each $\epsilon>0$ let $D_\epsilon$ and $S_\epsilon$ define the the domain and boundary respectively. (such that the conditions of the Divergence Theorem are satisfied) Suppose that the maximum distance from $P_0$ to some other point $P\in D_\epsilon$ approach zero as $\epsilon\rightarrow 0^+$. If $D_\epsilon$ has volume $vol(D_\epsilon)$, show that $$\lim_{\epsilon\rightarrow 0^+} \frac{1}{vol(D_\epsilon)} \iint_{S_\epsilon} \boldsymbol{F\cdot\hat{N}} dS = \nabla\cdot\boldsymbol{F}(P_0)$$ My Problem: So the proof given by Adams starts out by stating the divergence theorem and dividing both sides of the equation by the volume of the domain. Some algebraic manipulation brings me to this point. $$\begin{array}{lcr} \left|\frac{1}{vol(D_\epsilon)} \iint_{S_\epsilon}\boldsymbol{F\cdot\hat{N}}\, dS - \nabla\cdot\boldsymbol{F}(P_0)\right| & \leq &\frac{1}{vol(D_\epsilon)} \iiint_{D_\epsilon}\left|\nabla\cdot\boldsymbol{F}-\nabla\cdot\boldsymbol{F}(P_0)\right|dV \\& \leq & \max\limits_{P\in D_\epsilon} \left|\nabla\cdot\boldsymbol{F}-\nabla\cdot\boldsymbol{F}(P_0)\right|\end{array}$$ Now the first line seems to result from some algebraic manipulation and application of the triangle inequality, but the jump to the second line baffles me. (Hopefully i'm not overlooking something that's painfully obvious) Ps. cannot figure out how to get the expression on the second line to align properly.",,['multivariable-calculus']
18,Extrema with Lagrange Multipliers,Extrema with Lagrange Multipliers,,"I have the following exercise: Define $f:\mathbb{R}^2\to\mathbb{R}$ by $f(x,y)=(x^2y)^{1/3}$.  Is $f$ differentiable at $(0,0)?$. Has $f$ absolute extrema in $D=\{(x,y)\in\mathbb{R}^2:x^2+y^2\leq 1\}$?. The function $f$ is continous and by the compactness of $D$ I know because of Weierstrass theorem that $f$ has absolute maxima and minima in $D$, but how to find them?. I'm considering the possibility of study the existence of critical points inside $D$ (when $x^2+y^2<1$) and for the boundary of $D$ separately, but for the last I'd use Lagrange multipliers and the calculations I'm doing doesn't seem right. I have $\displaystyle\frac{\partial f}{\partial x}= \displaystyle\frac{2y^{1/3}}{3(x^2y)^{2/3}}=\displaystyle\frac{2y^{1/3}}{3x^{1/3}}$ and $\displaystyle\frac{\partial f}{\partial y}= \displaystyle\frac{x^2}{3(x^2y)^{2/3}}=\displaystyle\frac{x^{2/3}}{3y^{2/3}}$. Now define $g(x) = x^2+y^2-1=0$ in order to find extrema in $\partial D$ and for the last one $\displaystyle\frac{\partial g}{\partial x} = 2x$ and $\displaystyle\frac{\partial g}{\partial y}=2y$. Using Lagrange multipliers I have to solve $$\begin{cases} \frac{\partial f}{\partial x}(x,y) = \lambda\frac{\partial g}{\partial x}(x,y) \\ \frac{\partial f}{\partial y}(x,y) = \lambda\frac{\partial g}{\partial y}(x,y)\\ g(x,y)=0\end{cases}$$ which is $$\begin{cases} \frac{2y^{1/3}}{3x^{1/3}} = 2\lambda x & (1)\\ \frac{x^{2/3}}{3y^{2/3}} = 2\lambda y & (2)\\ x^2+y^2=1 & (3) \end{cases}$$ Using $(1)$ I get $y$ in terms of $x,\lambda$ having $y^{1/3}=3\lambda x^{4/3}\implies y= 9\lambda ^3 x^4$; and by $(2)$ I have $x^{2/3}=6\lambda y^{5/3}\implies x= (6\lambda y ^{5/3})^{3/2}=6\sqrt{6}\lambda^{3/2}y^{5/2}$. Then $y=9\lambda ^3 x^4 = 9\lambda^3 (6\sqrt{6}\lambda ^{3/2} y^{5/2})^4=9\cdot6^6\lambda ^9 y ^{10}\implies y^2=\displaystyle\frac{1}{9\lambda^3 (6\sqrt{6}\lambda ^{3/2} y^{5/2})^{4/9}}$. Isn't this too weird as a solution?. I still would have to use it to find possibles $\lambda$ but it seems the former system has -if my calculations are correct- strange solutions; did I had another mistake?.","I have the following exercise: Define $f:\mathbb{R}^2\to\mathbb{R}$ by $f(x,y)=(x^2y)^{1/3}$.  Is $f$ differentiable at $(0,0)?$. Has $f$ absolute extrema in $D=\{(x,y)\in\mathbb{R}^2:x^2+y^2\leq 1\}$?. The function $f$ is continous and by the compactness of $D$ I know because of Weierstrass theorem that $f$ has absolute maxima and minima in $D$, but how to find them?. I'm considering the possibility of study the existence of critical points inside $D$ (when $x^2+y^2<1$) and for the boundary of $D$ separately, but for the last I'd use Lagrange multipliers and the calculations I'm doing doesn't seem right. I have $\displaystyle\frac{\partial f}{\partial x}= \displaystyle\frac{2y^{1/3}}{3(x^2y)^{2/3}}=\displaystyle\frac{2y^{1/3}}{3x^{1/3}}$ and $\displaystyle\frac{\partial f}{\partial y}= \displaystyle\frac{x^2}{3(x^2y)^{2/3}}=\displaystyle\frac{x^{2/3}}{3y^{2/3}}$. Now define $g(x) = x^2+y^2-1=0$ in order to find extrema in $\partial D$ and for the last one $\displaystyle\frac{\partial g}{\partial x} = 2x$ and $\displaystyle\frac{\partial g}{\partial y}=2y$. Using Lagrange multipliers I have to solve $$\begin{cases} \frac{\partial f}{\partial x}(x,y) = \lambda\frac{\partial g}{\partial x}(x,y) \\ \frac{\partial f}{\partial y}(x,y) = \lambda\frac{\partial g}{\partial y}(x,y)\\ g(x,y)=0\end{cases}$$ which is $$\begin{cases} \frac{2y^{1/3}}{3x^{1/3}} = 2\lambda x & (1)\\ \frac{x^{2/3}}{3y^{2/3}} = 2\lambda y & (2)\\ x^2+y^2=1 & (3) \end{cases}$$ Using $(1)$ I get $y$ in terms of $x,\lambda$ having $y^{1/3}=3\lambda x^{4/3}\implies y= 9\lambda ^3 x^4$; and by $(2)$ I have $x^{2/3}=6\lambda y^{5/3}\implies x= (6\lambda y ^{5/3})^{3/2}=6\sqrt{6}\lambda^{3/2}y^{5/2}$. Then $y=9\lambda ^3 x^4 = 9\lambda^3 (6\sqrt{6}\lambda ^{3/2} y^{5/2})^4=9\cdot6^6\lambda ^9 y ^{10}\implies y^2=\displaystyle\frac{1}{9\lambda^3 (6\sqrt{6}\lambda ^{3/2} y^{5/2})^{4/9}}$. Isn't this too weird as a solution?. I still would have to use it to find possibles $\lambda$ but it seems the former system has -if my calculations are correct- strange solutions; did I had another mistake?.",,"['multivariable-calculus', 'optimization']"
19,Triple integral with a cone as a domain,Triple integral with a cone as a domain,,"How can I find $\displaystyle\iiint_D f$ if $f(x,y,z) =\sqrt{x^2+y^2}$ $D$ is what is inside of $z^2=x^2+y^2,z=0,z=1$?. I tried to do it with cylindrical coordinates as follows: $x=\rho\cos\theta, y=\rho\sin\theta,z=z$; considering the condition $z^2=x^2+y^2$ I have that $\rho^2\cos^2\theta+\rho^2\sin^2\theta=z^2\implies\rho^2=z^2$, $\rho$ is always greater than zero, then $0\leq \rho\leq 1$ and it seems that I need to go the full circle then all the conditions are: $$0\leq\rho \leq 1 \\ 0\leq\theta\leq 2\pi \\ 0 \leq z \leq 1$$ Then $$\iiint_Df(x,y,z)\;dxdydz = \int_0^1\int_0^{2\pi}\int_0^1 \sqrt{r^2}r\;drd\theta dz =\frac{1}{3}\int_0^1\int_0^{2\pi}d\theta dz = \frac{2}{3}\pi$$. Is that right?. I've read that this also can be computed considering spherical coordinates, is there a good reason to use that change or just will make things harder?","How can I find $\displaystyle\iiint_D f$ if $f(x,y,z) =\sqrt{x^2+y^2}$ $D$ is what is inside of $z^2=x^2+y^2,z=0,z=1$?. I tried to do it with cylindrical coordinates as follows: $x=\rho\cos\theta, y=\rho\sin\theta,z=z$; considering the condition $z^2=x^2+y^2$ I have that $\rho^2\cos^2\theta+\rho^2\sin^2\theta=z^2\implies\rho^2=z^2$, $\rho$ is always greater than zero, then $0\leq \rho\leq 1$ and it seems that I need to go the full circle then all the conditions are: $$0\leq\rho \leq 1 \\ 0\leq\theta\leq 2\pi \\ 0 \leq z \leq 1$$ Then $$\iiint_Df(x,y,z)\;dxdydz = \int_0^1\int_0^{2\pi}\int_0^1 \sqrt{r^2}r\;drd\theta dz =\frac{1}{3}\int_0^1\int_0^{2\pi}d\theta dz = \frac{2}{3}\pi$$. Is that right?. I've read that this also can be computed considering spherical coordinates, is there a good reason to use that change or just will make things harder?",,"['multivariable-calculus', 'definite-integrals', 'solution-verification']"
20,Arclength does not change with reparametrization,Arclength does not change with reparametrization,,"Recall that the length of a curve $\alpha : [a,b] \rightarrow \mathbb{R}^3$ is given by $L(\alpha) = \int |\alpha'(t)|  dt$. Let $\beta(r): [c,d] \rightarrow \mathbb{R}^3$ be a reparametrization of $\alpha$ defined by taking a map $h: [c,d] \rightarrow [a,b]$ with $h(c) = a, h(d)=b$ and $h'(r) \geq 0$ for all $r \in [c,d]$. Show that the arclength does not change under this type of reparametrization. I believe I have to use the chain rule. I initially set $\beta(r) = \alpha(h(r))$. Then using the chain rule, I get $\beta '(r) = \alpha '(h(r)) \cdot \frac{dh}{ds} (r)$. Does this imply that the magnitudes are the same as well? Because then that would be sufficient to prove that the arclength is the same.","Recall that the length of a curve $\alpha : [a,b] \rightarrow \mathbb{R}^3$ is given by $L(\alpha) = \int |\alpha'(t)|  dt$. Let $\beta(r): [c,d] \rightarrow \mathbb{R}^3$ be a reparametrization of $\alpha$ defined by taking a map $h: [c,d] \rightarrow [a,b]$ with $h(c) = a, h(d)=b$ and $h'(r) \geq 0$ for all $r \in [c,d]$. Show that the arclength does not change under this type of reparametrization. I believe I have to use the chain rule. I initially set $\beta(r) = \alpha(h(r))$. Then using the chain rule, I get $\beta '(r) = \alpha '(h(r)) \cdot \frac{dh}{ds} (r)$. Does this imply that the magnitudes are the same as well? Because then that would be sufficient to prove that the arclength is the same.",,"['multivariable-calculus', 'differential-geometry']"
21,maxima and minima of 2 variable function,maxima and minima of 2 variable function,,"How can I show that $f(x,y)=e^x cos(y)$ doesn't have maxima nor minima in the unit circle? Because $f_x = f_{xx} =0$ when $x=0$ and $y=\frac{\pi}{2} +n\pi, n\in \Bbb{Z}$. and isn't $(0,\frac{\pi}{2},0)$ in the unit circle? if we try to find the maximas or minimas with derivates. $$f_{y}=-\frac{e^{x+iy}-e^{x-iy}}{2i}$$ And when $x,y=0\to f_y(x,y)=0$. isn't $(0,0,0)$ in the circle..?","How can I show that $f(x,y)=e^x cos(y)$ doesn't have maxima nor minima in the unit circle? Because $f_x = f_{xx} =0$ when $x=0$ and $y=\frac{\pi}{2} +n\pi, n\in \Bbb{Z}$. and isn't $(0,\frac{\pi}{2},0)$ in the unit circle? if we try to find the maximas or minimas with derivates. $$f_{y}=-\frac{e^{x+iy}-e^{x-iy}}{2i}$$ And when $x,y=0\to f_y(x,y)=0$. isn't $(0,0,0)$ in the circle..?",,['multivariable-calculus']
22,Find the volume of the solid bounded by $z=x^2+y^2+1$ and $z=2-x^2-y^2$.,Find the volume of the solid bounded by  and .,z=x^2+y^2+1 z=2-x^2-y^2,"Question: Find the volume of the solid bounded by $z=x^2+y^2+1$ and $z=2-x^2-y^2$. Setting the 2 equations equal w.r.t. $z$, $x^2+y^2+1=2-x^2-y^2 \rightarrow x=\pm\sqrt{\frac 12-y^2}$ Therefore the boundary of $y=\pm\frac {1}{\sqrt2}$. So to find the volume of the solid, take the integration by subtracting the volume above and below the boundaries. $\displaystyle V=\int_{-\frac {1}{\sqrt2}}^{+\frac {1}{\sqrt2}}\int_{-\sqrt{\frac 12-y^2}}^{+\sqrt{\frac 12-y^2}}(2-x^2-y^2)dxdy-\int_{-\frac {1}{\sqrt2}}^{+\frac {1}{\sqrt2}}\int_{-\sqrt{\frac 12-y^2}}^{+\sqrt{\frac 12-y^2}}(x^2+y^2+1)dxdy$ This is what I did. Without solving the equation, can someone tell me if it is correct? Thank you!","Question: Find the volume of the solid bounded by $z=x^2+y^2+1$ and $z=2-x^2-y^2$. Setting the 2 equations equal w.r.t. $z$, $x^2+y^2+1=2-x^2-y^2 \rightarrow x=\pm\sqrt{\frac 12-y^2}$ Therefore the boundary of $y=\pm\frac {1}{\sqrt2}$. So to find the volume of the solid, take the integration by subtracting the volume above and below the boundaries. $\displaystyle V=\int_{-\frac {1}{\sqrt2}}^{+\frac {1}{\sqrt2}}\int_{-\sqrt{\frac 12-y^2}}^{+\sqrt{\frac 12-y^2}}(2-x^2-y^2)dxdy-\int_{-\frac {1}{\sqrt2}}^{+\frac {1}{\sqrt2}}\int_{-\sqrt{\frac 12-y^2}}^{+\sqrt{\frac 12-y^2}}(x^2+y^2+1)dxdy$ This is what I did. Without solving the equation, can someone tell me if it is correct? Thank you!",,"['calculus', 'integration', 'multivariable-calculus']"
23,Find all stationary points of multivariable function,Find all stationary points of multivariable function,,"$$f(x,y) = \left(y^2 + y -16\right)\sin(x)$$ Find ALL stationary points of $f$ and classify each as local max, min or saddle point. My working so far is $f_x = \left(y^2 + y -16\right)\cos x$ $f_y = \left(2y + 1\right)\sin x$ $f_{xx} = -\left(y^2 + y - 16\right)\sin x$ $f_{yy}= 2\sin x$ $f_{xy}= \left(2y + 1\right)\cos x$ For stationary points I need $f_x=0$ and $f_y=0$ For $\left(2y+1\right)\sin(x) = 0$ need either $y=-\dfrac{1}{2}$ or $x=0$. Now have I made a mistake somewhere because when I put into the other equation to find stationary points when $x = 0$, $y = \dfrac{-1 \pm \sqrt{65}}{2}$ which is fine but when I use $y=-\dfrac{1}{2}$ there is no $x$ value Thanks in advance!","$$f(x,y) = \left(y^2 + y -16\right)\sin(x)$$ Find ALL stationary points of $f$ and classify each as local max, min or saddle point. My working so far is $f_x = \left(y^2 + y -16\right)\cos x$ $f_y = \left(2y + 1\right)\sin x$ $f_{xx} = -\left(y^2 + y - 16\right)\sin x$ $f_{yy}= 2\sin x$ $f_{xy}= \left(2y + 1\right)\cos x$ For stationary points I need $f_x=0$ and $f_y=0$ For $\left(2y+1\right)\sin(x) = 0$ need either $y=-\dfrac{1}{2}$ or $x=0$. Now have I made a mistake somewhere because when I put into the other equation to find stationary points when $x = 0$, $y = \dfrac{-1 \pm \sqrt{65}}{2}$ which is fine but when I use $y=-\dfrac{1}{2}$ there is no $x$ value Thanks in advance!",,"['multivariable-calculus', 'trigonometry']"
24,There exists a constant arc length parametrization,There exists a constant arc length parametrization,,"I heard that for any curve in the plane that can be given parametrically by $\vec{r}(t)=\langle x(t),y(t)\rangle$ for $a\leq t\leq b$ that there exists a constant arc length parametrization, i.e. another parametrization $\hat{\vec{r}}(t)=\langle \hat{x}(t),\hat{y}(t)\rangle$ that satisfies $\hat{x}'(t)^2+\hat{y}'(t)^2=1$ and $\hat{\vec{r}}\big((a,b)\big)=\vec{r}\big((a,b)\big)$. What is the name of this theorem and who proved it?  I've been looking around the internet and it seems like the Gauss-Bonnet Theorem comes up a lot, but I don't see the connection between that and this?  Maybe I just don't understand it as well as I need to.","I heard that for any curve in the plane that can be given parametrically by $\vec{r}(t)=\langle x(t),y(t)\rangle$ for $a\leq t\leq b$ that there exists a constant arc length parametrization, i.e. another parametrization $\hat{\vec{r}}(t)=\langle \hat{x}(t),\hat{y}(t)\rangle$ that satisfies $\hat{x}'(t)^2+\hat{y}'(t)^2=1$ and $\hat{\vec{r}}\big((a,b)\big)=\vec{r}\big((a,b)\big)$. What is the name of this theorem and who proved it?  I've been looking around the internet and it seems like the Gauss-Bonnet Theorem comes up a lot, but I don't see the connection between that and this?  Maybe I just don't understand it as well as I need to.",,"['multivariable-calculus', 'differential-geometry', 'plane-curves', 'parametric']"
25,"Evaluate $\iint_{x<u,y<v, x^2+y^2<1} dx\,dy$",Evaluate,"\iint_{x<u,y<v, x^2+y^2<1} dx\,dy","How can I evaluate the following double integral: $$\iint\limits_{\substack{x<u,y<v, \\ x^2+y^2<1}} dx\,dy$$ If we didn't have the restrictions $x<u, y<v$ polar coordinates would have worked quite nicely to give $\pi$ , the area of a unit circle, but how do I proceed here? All suggestions are welcome. Thank you in advance.","How can I evaluate the following double integral: If we didn't have the restrictions polar coordinates would have worked quite nicely to give , the area of a unit circle, but how do I proceed here? All suggestions are welcome. Thank you in advance.","\iint\limits_{\substack{x<u,y<v, \\ x^2+y^2<1}} dx\,dy x<u, y<v \pi","['integration', 'multivariable-calculus', 'definite-integrals']"
26,Finding parameterization of a curve,Finding parameterization of a curve,,"Let $z=4-2x-2y$ be a plane having a curve $\gamma$ on it.  The projection of $\gamma$ on $z=0$ is the circle $x^2 + y^2 =1$ . Find a parameterization of $\gamma$ . How can I do it ? I know that the surface is $ x(u,v) = (u,v, 4-2u-2v) $ , and that our curve must be of the form $ \gamma(t) = (u(t), v(t) , 4-2u(t)-2v(t) ) $ .  After taking $ z=0, x^2+y^2=1 $ we get $3-8v+5v^2 =0 $ and I can't understand how it helps ... Will you please help ? Thanks","Let $z=4-2x-2y$ be a plane having a curve $\gamma$ on it.  The projection of $\gamma$ on $z=0$ is the circle $x^2 + y^2 =1$ . Find a parameterization of $\gamma$ . How can I do it ? I know that the surface is $ x(u,v) = (u,v, 4-2u-2v) $ , and that our curve must be of the form $ \gamma(t) = (u(t), v(t) , 4-2u(t)-2v(t) ) $ .  After taking $ z=0, x^2+y^2=1 $ we get $3-8v+5v^2 =0 $ and I can't understand how it helps ... Will you please help ? Thanks",,['multivariable-calculus']
27,"If $|\nabla F| > 1$ and $|F| \le 1$, is there a zero nearby?","If  and , is there a zero nearby?",|\nabla F| > 1 |F| \le 1,"I saw this claim, stated without much explanation, in an article I'm reading: Let $F:\mathbb{R}^n\to\mathbb{R}$ be a $C^1$ function which satisfies $|\nabla F|>1$ everywhere. We know that $|F(0)| \le 1$. Then there is a point $x$ which is at most distance $1$ away from $0$, such that $F(x) = 0$. I'm trying to justify this claim for my own understanding. Here is how I proved it: According to (???), we can find a (unique?) ""flow curve"" $\gamma:\mathbb{R}\to\mathbb{R}^n$ satisfying:   $$\dot \gamma(t) = \nabla F(\gamma(t))$$   $$\gamma(0) = 0$$   Look at $f:\mathbb{R}\to\mathbb{R}:t \mapsto F(\gamma(t))$. It satisfies $\dot f(t) = \nabla F(\gamma(t)) \cdot \dot \gamma(t) = ||\dot \gamma(t)||^2 > 1$, and $|f(0)| \le 1$. By a simple 1-dimensional calculus argument, there is a $t_0 \in \left[-1, 1\right]$ such that $f(t_0) = 0$. The length of the arc $\gamma$ between the origin and our zero is $$\left|\int_0^{t_0} ||\dot \gamma(t)||\,dt\right| \stackrel{\text{Cauchy-Schwarz}}{\le}\sqrt{|t_0| \cdot \left|\int_0^{t_0} ||\dot \gamma(t)||^2\,dt\right|} = \sqrt{|t_0| \cdot \left|\int_0^{t_0} \dot f(t)\,dt\right|} \le \sqrt{|t_0 f(0)|} \le 1$$   so the distance between $x = \gamma(t_0)$ and $0$ is at most $1$. My questions: Is the proof correct? What is the justification for the existence of $\gamma$? Is there a simpler proof, particularly one that doesn't use the theory of ODEs?","I saw this claim, stated without much explanation, in an article I'm reading: Let $F:\mathbb{R}^n\to\mathbb{R}$ be a $C^1$ function which satisfies $|\nabla F|>1$ everywhere. We know that $|F(0)| \le 1$. Then there is a point $x$ which is at most distance $1$ away from $0$, such that $F(x) = 0$. I'm trying to justify this claim for my own understanding. Here is how I proved it: According to (???), we can find a (unique?) ""flow curve"" $\gamma:\mathbb{R}\to\mathbb{R}^n$ satisfying:   $$\dot \gamma(t) = \nabla F(\gamma(t))$$   $$\gamma(0) = 0$$   Look at $f:\mathbb{R}\to\mathbb{R}:t \mapsto F(\gamma(t))$. It satisfies $\dot f(t) = \nabla F(\gamma(t)) \cdot \dot \gamma(t) = ||\dot \gamma(t)||^2 > 1$, and $|f(0)| \le 1$. By a simple 1-dimensional calculus argument, there is a $t_0 \in \left[-1, 1\right]$ such that $f(t_0) = 0$. The length of the arc $\gamma$ between the origin and our zero is $$\left|\int_0^{t_0} ||\dot \gamma(t)||\,dt\right| \stackrel{\text{Cauchy-Schwarz}}{\le}\sqrt{|t_0| \cdot \left|\int_0^{t_0} ||\dot \gamma(t)||^2\,dt\right|} = \sqrt{|t_0| \cdot \left|\int_0^{t_0} \dot f(t)\,dt\right|} \le \sqrt{|t_0 f(0)|} \le 1$$   so the distance between $x = \gamma(t_0)$ and $0$ is at most $1$. My questions: Is the proof correct? What is the justification for the existence of $\gamma$? Is there a simpler proof, particularly one that doesn't use the theory of ODEs?",,"['calculus', 'multivariable-calculus', 'proof-verification', 'alternative-proof', 'gradient-flows']"
28,Composition of linear differential operators is a linear differential operator,Composition of linear differential operators is a linear differential operator,,"I will use multi-index notation: $$ \newcommand{\p}{\partial}  P = (p_1, \dots, p_d), |P| = p_1 + \dots + p_d,  \p^P u = \dfrac{\p ^{|P|} u}{\p x_1 ^{p_1} \dots \p x_d ^{p_d}} .$$ Let $A = \sum_{|P| \leq k} a_P(x) \p^P $ and $B = \sum_{|Q| \leq l} b_Q(x) \p^Q $ be two linear differential operators. I want to show its composition is still a linear differential operator. How can I define $c_Z(x)$ to obtain $C$ a linear differential operator such that $$ C(u) = \sum_{|Z| \leq k+l} c_Z(x) \p ^Z = A(B(u)) \quad ? $$","I will use multi-index notation: $$ \newcommand{\p}{\partial}  P = (p_1, \dots, p_d), |P| = p_1 + \dots + p_d,  \p^P u = \dfrac{\p ^{|P|} u}{\p x_1 ^{p_1} \dots \p x_d ^{p_d}} .$$ Let $A = \sum_{|P| \leq k} a_P(x) \p^P $ and $B = \sum_{|Q| \leq l} b_Q(x) \p^Q $ be two linear differential operators. I want to show its composition is still a linear differential operator. How can I define $c_Z(x)$ to obtain $C$ a linear differential operator such that $$ C(u) = \sum_{|Z| \leq k+l} c_Z(x) \p ^Z = A(B(u)) \quad ? $$",,"['multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
29,Normal vector for a surface: explicit vs implicit formula,Normal vector for a surface: explicit vs implicit formula,,"If I have the surface of a hemisphere $S : x^2 + y^2 + z^2 = 4; z\geq 0$, then using the gradient to calculate the unit normal vector yields $\hat n = <\frac{x}{2}, \frac{y}{2}, \frac{z}{2}>$. But my textbook uses the explicit formula of the hemisphere $z=f(x,y)$ which yields $\hat n = <\frac{x}{z}, \frac{y}{z}, 1>$. Why is there a discrepancy between these two equations, and (in the case of the 2nd equation) how does it make sense physically for the z-component of the normal to always be 1? Also why is it that the normal coming from the explicit equation is undefined for $z=0$ when it shouldn't be? If I use the first normal vector in Stokes' theorem for some vector field $\vec F $, I get a different solution than if I were to use the 2nd one.","If I have the surface of a hemisphere $S : x^2 + y^2 + z^2 = 4; z\geq 0$, then using the gradient to calculate the unit normal vector yields $\hat n = <\frac{x}{2}, \frac{y}{2}, \frac{z}{2}>$. But my textbook uses the explicit formula of the hemisphere $z=f(x,y)$ which yields $\hat n = <\frac{x}{z}, \frac{y}{z}, 1>$. Why is there a discrepancy between these two equations, and (in the case of the 2nd equation) how does it make sense physically for the z-component of the normal to always be 1? Also why is it that the normal coming from the explicit equation is undefined for $z=0$ when it shouldn't be? If I use the first normal vector in Stokes' theorem for some vector field $\vec F $, I get a different solution than if I were to use the 2nd one.",,['multivariable-calculus']
30,Variants of the bump function.,Variants of the bump function.,,"The title of this question isn't really clear because of the 150 char limit. What I actually want to ask is this: If I would have a bump function for $-1 < x < 1$ and I would have some variants of it (a little wider or smaller and higher or lower), how does the bump function have to be designed so I can determine when it is equal or higher than the other function(exponential decay)? This explains a little what a bump function is. The function I'm looking for is does not have to look like that exactly, but at $x = 0$ and $x = 1$, the slope has to be $0$. I've been trying to create some functions that make a bump between $-1$ and $1$, but when I have a sum of some of those, I can't solve for $x$. What I mean by a sum of variants is this. Let the bump represent the energy level of a light flash. So, if $y = e\times \operatorname{exp}\left(-\frac{1}{1-x^2}\right)$, $x$ represents time and $y$ represents the energy level. If you would have some device with a light sensor which has to do something if at some moment the light exceeds a value, it has constantly have to check if its input (sum of energy levels) is more than some value. So a bump function represents a flash of light, but the flashes are not the same, they don't start at the same time, they differ in duration and they might differ in intensity. So if I would have three flashes, the first starts at $4$, its duration is $2.5$, and it is $1.1$ times intense as the standard flash. For the second: start $2$, duration $6$, intensity $0.5$. And for the third: start $5$, duration $2$, intensity $3$. The formulas would then be $$y = 1.1\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-5}{1.25})^2}\right)$$ $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right)$$ $$y = 3\times e\times \operatorname{exp}\left( -\frac{1}{1-(x-6)^2}\right)$$ and the formulas for the sum of those flashes would be $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right)$$ for $2 < x < 4$ and for $7 < x < 8$, $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right) + 1.1\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-5}{1.25})^2}\right)$$ for $4 < x < 5$, $y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right) + 1.1\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-5}{1.25})^2}\right) + 3\times e\times \operatorname{exp}\left( -\frac{1}{1-(x-6)^2}\right)$ for $5 < x < 6.5$, $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right) + 3\times e\times \operatorname{exp}\left( -\frac{1}{1-(x-6)^2}\right)$$ for $6.5 < x < 7$. This is impossible to invert so it can't be solved for $x$. What function which makes a nice bump can I use so It can be inverted if it is summed up like that?","The title of this question isn't really clear because of the 150 char limit. What I actually want to ask is this: If I would have a bump function for $-1 < x < 1$ and I would have some variants of it (a little wider or smaller and higher or lower), how does the bump function have to be designed so I can determine when it is equal or higher than the other function(exponential decay)? This explains a little what a bump function is. The function I'm looking for is does not have to look like that exactly, but at $x = 0$ and $x = 1$, the slope has to be $0$. I've been trying to create some functions that make a bump between $-1$ and $1$, but when I have a sum of some of those, I can't solve for $x$. What I mean by a sum of variants is this. Let the bump represent the energy level of a light flash. So, if $y = e\times \operatorname{exp}\left(-\frac{1}{1-x^2}\right)$, $x$ represents time and $y$ represents the energy level. If you would have some device with a light sensor which has to do something if at some moment the light exceeds a value, it has constantly have to check if its input (sum of energy levels) is more than some value. So a bump function represents a flash of light, but the flashes are not the same, they don't start at the same time, they differ in duration and they might differ in intensity. So if I would have three flashes, the first starts at $4$, its duration is $2.5$, and it is $1.1$ times intense as the standard flash. For the second: start $2$, duration $6$, intensity $0.5$. And for the third: start $5$, duration $2$, intensity $3$. The formulas would then be $$y = 1.1\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-5}{1.25})^2}\right)$$ $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right)$$ $$y = 3\times e\times \operatorname{exp}\left( -\frac{1}{1-(x-6)^2}\right)$$ and the formulas for the sum of those flashes would be $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right)$$ for $2 < x < 4$ and for $7 < x < 8$, $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right) + 1.1\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-5}{1.25})^2}\right)$$ for $4 < x < 5$, $y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right) + 1.1\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-5}{1.25})^2}\right) + 3\times e\times \operatorname{exp}\left( -\frac{1}{1-(x-6)^2}\right)$ for $5 < x < 6.5$, $$y = 0.5\times e\times \operatorname{exp}\left( -\frac{1}{1-(\frac{x-3}{3})^2}\right) + 3\times e\times \operatorname{exp}\left( -\frac{1}{1-(x-6)^2}\right)$$ for $6.5 < x < 7$. This is impossible to invert so it can't be solved for $x$. What function which makes a nice bump can I use so It can be inverted if it is summed up like that?",,"['integration', 'functional-analysis', 'functions', 'multivariable-calculus']"
31,Four equations with three variables!,Four equations with three variables!,,"I have these four equations: $$\left\{\begin{matrix} A_1 = A_c -D \theta cos(\phi)\\  A_2 = A_c -D \theta cos(\phi-\pi/2)\\  A_3 = A_c -D \theta cos(\phi-\pi)\\  A_4 = A_c -D \theta cos(\phi-3\pi/2) \end{matrix}\right.$$ Variables $A_1$ to $A_4$ are given, D is a constant, and Variables $A_c$, $\theta$, and $\phi$ are unknown. I need to find $\frac{d \theta}{d A_1}$ to $\frac{d \theta}{d A_4}$, but I have some problems. What I tried was that I used three of these equations and derived $\theta$, $\phi$, and $A_c$, and then differentiated $\theta$ with respect to $A_i$ separately, but I am not sure if this is correct or not. The thing is that variables $A_i$ are related. This means that if variable $A_1$ changes, at least one of the variables $A_2$ to $A_4$ will also change as there is this relation between the $A_i$ and $A_c$ parameters: $A_1+A_2+A_3+A_4=4 A_c$ I also get different results when I use three other set of these four equations to derive the parameters. So how can I study the changes of variables $\theta$ and $\phi$ due to changes in variables $A_1$ to $A_4$. Edit: If I derive variable $\theta$ from the first three equations listed above, I'll have: $$\theta=\frac{\sqrt{2}\, \sqrt{{\mathrm{A1}}^2 - 2\, \mathrm{A1}\, \mathrm{A2} + 2\, {\mathrm{A2}}^2 - 2\, \mathrm{A2}\, \mathrm{A3} + {\mathrm{A3}}^2}}{2\, R}$$ and if I derive variable $\theta$ from the first, second and fourth equations, I'll have: $$\theta=\frac{\sqrt{2}\, \sqrt{2\, {\mathrm{A1}}^2 - 2\, \mathrm{A1}\, \mathrm{A2} - 2\, \mathrm{A1}\, \mathrm{A4} + {\mathrm{A2}}^2 + {\mathrm{A4}}^2}}{2\, R}$$ ,and finally I use $\theta$ derived by Ross Millikan in the answer below which is: $$\theta=\frac{\sqrt{{\left(\mathrm{A1} - \mathrm{A3}\right)}^2 + {\left(\mathrm{A2} - \mathrm{A4}\right)}^2}}{2\, R}$$ As you notice, the first two solutions do not involve all $A_1$ to $A_4$. All of these solution calculates correct value for $\theta$, but when I differentiate them with respect to $A_1$, you will see different results. This is how I do this: For $\theta=45 \deg$ and $\phi=0$, I first calculated all $A_1$ to $A_4$ parameters using the four equation in the begining. Now if I use these $A_1$ to $A_4$ parameters in the above solutions for $\theta$, they all would calculate correct $\theta$, but when I differentiate above solutions for $\theta$, and then use $A_1$ to $A_4$ parameters, they determines values of -0.1667, -0.3333, and -0.1667, respectively. And if I repeat this procedure for $\theta=45 \deg$ and $\phi=45 \deg$, then the results would be 0.0, -0.2357, and -0.1179. My understanding is that the correct one is the third solution that involves all of the $A_1$ to $A_4$ parameters, but then I don't know what is logically wrong with others or how I can relate the two other solutions to the correct one?","I have these four equations: $$\left\{\begin{matrix} A_1 = A_c -D \theta cos(\phi)\\  A_2 = A_c -D \theta cos(\phi-\pi/2)\\  A_3 = A_c -D \theta cos(\phi-\pi)\\  A_4 = A_c -D \theta cos(\phi-3\pi/2) \end{matrix}\right.$$ Variables $A_1$ to $A_4$ are given, D is a constant, and Variables $A_c$, $\theta$, and $\phi$ are unknown. I need to find $\frac{d \theta}{d A_1}$ to $\frac{d \theta}{d A_4}$, but I have some problems. What I tried was that I used three of these equations and derived $\theta$, $\phi$, and $A_c$, and then differentiated $\theta$ with respect to $A_i$ separately, but I am not sure if this is correct or not. The thing is that variables $A_i$ are related. This means that if variable $A_1$ changes, at least one of the variables $A_2$ to $A_4$ will also change as there is this relation between the $A_i$ and $A_c$ parameters: $A_1+A_2+A_3+A_4=4 A_c$ I also get different results when I use three other set of these four equations to derive the parameters. So how can I study the changes of variables $\theta$ and $\phi$ due to changes in variables $A_1$ to $A_4$. Edit: If I derive variable $\theta$ from the first three equations listed above, I'll have: $$\theta=\frac{\sqrt{2}\, \sqrt{{\mathrm{A1}}^2 - 2\, \mathrm{A1}\, \mathrm{A2} + 2\, {\mathrm{A2}}^2 - 2\, \mathrm{A2}\, \mathrm{A3} + {\mathrm{A3}}^2}}{2\, R}$$ and if I derive variable $\theta$ from the first, second and fourth equations, I'll have: $$\theta=\frac{\sqrt{2}\, \sqrt{2\, {\mathrm{A1}}^2 - 2\, \mathrm{A1}\, \mathrm{A2} - 2\, \mathrm{A1}\, \mathrm{A4} + {\mathrm{A2}}^2 + {\mathrm{A4}}^2}}{2\, R}$$ ,and finally I use $\theta$ derived by Ross Millikan in the answer below which is: $$\theta=\frac{\sqrt{{\left(\mathrm{A1} - \mathrm{A3}\right)}^2 + {\left(\mathrm{A2} - \mathrm{A4}\right)}^2}}{2\, R}$$ As you notice, the first two solutions do not involve all $A_1$ to $A_4$. All of these solution calculates correct value for $\theta$, but when I differentiate them with respect to $A_1$, you will see different results. This is how I do this: For $\theta=45 \deg$ and $\phi=0$, I first calculated all $A_1$ to $A_4$ parameters using the four equation in the begining. Now if I use these $A_1$ to $A_4$ parameters in the above solutions for $\theta$, they all would calculate correct $\theta$, but when I differentiate above solutions for $\theta$, and then use $A_1$ to $A_4$ parameters, they determines values of -0.1667, -0.3333, and -0.1667, respectively. And if I repeat this procedure for $\theta=45 \deg$ and $\phi=45 \deg$, then the results would be 0.0, -0.2357, and -0.1179. My understanding is that the correct one is the third solution that involves all of the $A_1$ to $A_4$ parameters, but then I don't know what is logically wrong with others or how I can relate the two other solutions to the correct one?",,"['ordinary-differential-equations', 'multivariable-calculus']"
32,Change an integral from Polar to Cartesian Form,Change an integral from Polar to Cartesian Form,,"I'm trying to convert the following integral from Polar to Cartesian form: $$\displaystyle\int_{\pi/2}^{\pi} \int_{0}^{\sin\theta}\,r^2\,dr\, d\theta$$ I think the integral should be:  $\int \int \sqrt{x^2+y^2} \, dx\, dy$ I know the region is in the second quadrant but I'm not sure how to determine the integral limits.","I'm trying to convert the following integral from Polar to Cartesian form: $$\displaystyle\int_{\pi/2}^{\pi} \int_{0}^{\sin\theta}\,r^2\,dr\, d\theta$$ I think the integral should be:  $\int \int \sqrt{x^2+y^2} \, dx\, dy$ I know the region is in the second quadrant but I'm not sure how to determine the integral limits.",,"['integration', 'multivariable-calculus']"
33,Triple Integral of region bounded by cylinder $x^2 + 3z^2 = 9$ and the planes $y = 0$ and $x + y = 3$,Triple Integral of region bounded by cylinder  and the planes  and,x^2 + 3z^2 = 9 y = 0 x + y = 3,"Here is the questoin with a diagram. My attempt at solution: $$x^2 + 3z^2 = 9 \Rightarrow 3z^z = 9-x^2 \Rightarrow z^2 = 3 - \frac{x^2}{3} $$ $$\Rightarrow -\sqrt{3 - \frac{x^2}{3}} \leq z \leq \sqrt{3 - \frac{x^2}{3}}  $$ $$x = y - 3, \,\,\,\,\,\,\,\ y = 0$$ From here on I don't know how to proceed and figure out the bounds and I would appreciate any hints. My other question is what are some general techniques to figure out the bounds of areas for evaluating double and triple integrals? My problem is that I don't know what the graphs of the function look like and a lot of solutions I see online are where people are visualizing the graphs and using the graphs to figure out the region. For example here my professor gives a solution for a question where we had to change the order of integration and he's using the facts ""W describes unit sphere"", ""hence we have projection into xy plane"" etc.","Here is the questoin with a diagram. My attempt at solution: $$x^2 + 3z^2 = 9 \Rightarrow 3z^z = 9-x^2 \Rightarrow z^2 = 3 - \frac{x^2}{3} $$ $$\Rightarrow -\sqrt{3 - \frac{x^2}{3}} \leq z \leq \sqrt{3 - \frac{x^2}{3}}  $$ $$x = y - 3, \,\,\,\,\,\,\,\ y = 0$$ From here on I don't know how to proceed and figure out the bounds and I would appreciate any hints. My other question is what are some general techniques to figure out the bounds of areas for evaluating double and triple integrals? My problem is that I don't know what the graphs of the function look like and a lot of solutions I see online are where people are visualizing the graphs and using the graphs to figure out the region. For example here my professor gives a solution for a question where we had to change the order of integration and he's using the facts ""W describes unit sphere"", ""hence we have projection into xy plane"" etc.",,"['multivariable-calculus', 'definite-integrals']"
34,Write the function in the form $y=f(u)$ and $u=g(x)$. Then find $dy/dx$ as a function of $x$,Write the function in the form  and . Then find  as a function of,y=f(u) u=g(x) dy/dx x,"$$y=\left(3x^2-(8/x)-x\right)^9$$ I know that $y = u^9$ and then $u = 3x^2-\dfrac{8}{x}-x$, but then I do not know how to put it together to solve for $dy/dx$.","$$y=\left(3x^2-(8/x)-x\right)^9$$ I know that $y = u^9$ and then $u = 3x^2-\dfrac{8}{x}-x$, but then I do not know how to put it together to solve for $dy/dx$.",,"['calculus', 'multivariable-calculus']"
35,How to find the partial derivative of this function?,How to find the partial derivative of this function?,,"Lets say I have a function:$$\nu=\frac{RT}{P}+B_{p}(T)RT$$ and I am trying to find $\left(\frac{\partial \nu}{\partial T}\right)_{P}$. I understanding that the partial derivative of the first term is just $\frac{R}{P}$ but the second term has two terms that depend on T. Do I use the product rule like regular derivatives? If so, I think the answer would look something like this: $$\left(\frac{\partial \nu}{\partial T}\right)_{P}=\frac{R}{P}+RT \frac{\partial B_{P}(T)}{\partial T}+RB_{P}(T)$$ Am I correct in saying this or does the product rule not apply to partial derivatives like I was thinking.","Lets say I have a function:$$\nu=\frac{RT}{P}+B_{p}(T)RT$$ and I am trying to find $\left(\frac{\partial \nu}{\partial T}\right)_{P}$. I understanding that the partial derivative of the first term is just $\frac{R}{P}$ but the second term has two terms that depend on T. Do I use the product rule like regular derivatives? If so, I think the answer would look something like this: $$\left(\frac{\partial \nu}{\partial T}\right)_{P}=\frac{R}{P}+RT \frac{\partial B_{P}(T)}{\partial T}+RB_{P}(T)$$ Am I correct in saying this or does the product rule not apply to partial derivatives like I was thinking.",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
36,How to find other solutions to this vectorproblem?,How to find other solutions to this vectorproblem?,,"Suppose I have a vector field $\mathbf{A}(x,y,z)$, of which I know: $$ \mathbf{A}(x,y,0)=(1+\alpha x)\hat{z}$$ Thus, I know the value of $\mathbf{A}$ in the $xy$-plane. Say, within $|x|,|y|\leq\frac{1}{2}$. Furthermore, I have the following requirements for $\mathbf{A}$. $$\nabla\cdot\mathbf{A}=0, \\ \nabla\times\mathbf{A}=0,$$ which have to be satisfied in $|x|,|y|,|z|\leq{\frac{1}{2}}$. I want to find the vector field $\mathbf{A}$ that satisfy all of the above conditions, at least for the given boundaries, but for larger (infinite?), domains as possible. I did find the following solution, but, with some rather crude assumptions, so I wonder if there are any other approaches to solve the problem. Assumption 1: There is no $y$-dependency. Assumption 2: $\displaystyle\frac{d\mathbf{A}_x}{dx}=0$. Under these assumptions, one can easily obtain from the curl-requirement, that $$\mathbf{A}=\alpha z\hat{x}+(1+\alpha x)\hat{z}$$ But, is this the only one? I am especially interested in other solutions which do no show $y$-dependency, and, even more interested if there is a solution $\displaystyle\lim_{z\to\infty}\mathbf{A}_x<\infty$. A proof that the solution that I obtained straightforwardly is the only one obviously also counts as an answer.","Suppose I have a vector field $\mathbf{A}(x,y,z)$, of which I know: $$ \mathbf{A}(x,y,0)=(1+\alpha x)\hat{z}$$ Thus, I know the value of $\mathbf{A}$ in the $xy$-plane. Say, within $|x|,|y|\leq\frac{1}{2}$. Furthermore, I have the following requirements for $\mathbf{A}$. $$\nabla\cdot\mathbf{A}=0, \\ \nabla\times\mathbf{A}=0,$$ which have to be satisfied in $|x|,|y|,|z|\leq{\frac{1}{2}}$. I want to find the vector field $\mathbf{A}$ that satisfy all of the above conditions, at least for the given boundaries, but for larger (infinite?), domains as possible. I did find the following solution, but, with some rather crude assumptions, so I wonder if there are any other approaches to solve the problem. Assumption 1: There is no $y$-dependency. Assumption 2: $\displaystyle\frac{d\mathbf{A}_x}{dx}=0$. Under these assumptions, one can easily obtain from the curl-requirement, that $$\mathbf{A}=\alpha z\hat{x}+(1+\alpha x)\hat{z}$$ But, is this the only one? I am especially interested in other solutions which do no show $y$-dependency, and, even more interested if there is a solution $\displaystyle\lim_{z\to\infty}\mathbf{A}_x<\infty$. A proof that the solution that I obtained straightforwardly is the only one obviously also counts as an answer.",,"['ordinary-differential-equations', 'multivariable-calculus']"
37,Differentiating the Spherical Mean w.r.t its radius,Differentiating the Spherical Mean w.r.t its radius,,"this question regards differentiating the spherical mean with respect to its radius. This is my attempt so far: Start with the equivalent form of the spherical mean so that we can pass the partial derivative into the integrand: $$S(v,x,r) = {1 \over d \omega_d} \int_{|\xi|=1)} v(x+r\xi)do(\xi)$$ Then take partial derivative with respect to r: $${\partial \over \partial r}S(v,x,r) = {1 \over d \omega_d} \int_{|\xi|=1} {\partial \over \partial r} v(x+r\xi)do(\xi)$$ What I understand so far is that this turns into the LHS of the equality below, because of the multivariable chain rule, but I don't know how to go from the LHS to the RHS: $${1 \over d \omega_d} \int_{|\xi|=1} \sum_{i=1}^{d} {\partial \over \partial x^i} v(x+r\xi)\xi^ido(\xi)=S(v,x,r) = {1 \over d \omega_d r^{d-1}} \int_{\partial B(x, r)} {\partial \over \partial \nu} v(y)do(y)$$ I tried to write the integrand of RHS using the definition of divergence, and get it to look like: $$\int_{\partial B(x, r)} {\partial \over \partial \nu} v(y)do(y)=\int_{\partial B(x, r)} v(y)  \cdot \nu do(y)$$ But I am not understanding what would happen to $\xi ^i$, and how/when to make the substitution back to the original form of the spherical mean, over $B(x,r)$. Any input/hints are appreciated.","this question regards differentiating the spherical mean with respect to its radius. This is my attempt so far: Start with the equivalent form of the spherical mean so that we can pass the partial derivative into the integrand: $$S(v,x,r) = {1 \over d \omega_d} \int_{|\xi|=1)} v(x+r\xi)do(\xi)$$ Then take partial derivative with respect to r: $${\partial \over \partial r}S(v,x,r) = {1 \over d \omega_d} \int_{|\xi|=1} {\partial \over \partial r} v(x+r\xi)do(\xi)$$ What I understand so far is that this turns into the LHS of the equality below, because of the multivariable chain rule, but I don't know how to go from the LHS to the RHS: $${1 \over d \omega_d} \int_{|\xi|=1} \sum_{i=1}^{d} {\partial \over \partial x^i} v(x+r\xi)\xi^ido(\xi)=S(v,x,r) = {1 \over d \omega_d r^{d-1}} \int_{\partial B(x, r)} {\partial \over \partial \nu} v(y)do(y)$$ I tried to write the integrand of RHS using the definition of divergence, and get it to look like: $$\int_{\partial B(x, r)} {\partial \over \partial \nu} v(y)do(y)=\int_{\partial B(x, r)} v(y)  \cdot \nu do(y)$$ But I am not understanding what would happen to $\xi ^i$, and how/when to make the substitution back to the original form of the spherical mean, over $B(x,r)$. Any input/hints are appreciated.",,['multivariable-calculus']
38,Vector calculus and divergence theorem properties,Vector calculus and divergence theorem properties,,"I cannot seem to figure out why the following is true: $\partial \over \partial r$ $\int_{B(x,r)}\triangle v(z) dz =$ $\int_{\partial B(x,r)}\triangle v(y) do(y)$ I attempted to apply divergence theorem to the LHS, but here's where I get stuck and don't know where to proceed: $\partial \over \partial r$ $\int_{B(x,r)}\triangle v(z) dz = $ $\partial \over \partial r$ $\int_{B(x,r)}div(\nabla v(z)) dz =$ $\partial \over \partial r$ $\int_{\partial B(x,r)}\nabla v(y) \cdot \nu dy$ Where $\nu$ is an exterior normal to $\partial B(x,r)$. I do not know when/how to pass the partial derivative into the integrand to get the result. Thanks in advance,","I cannot seem to figure out why the following is true: $\partial \over \partial r$ $\int_{B(x,r)}\triangle v(z) dz =$ $\int_{\partial B(x,r)}\triangle v(y) do(y)$ I attempted to apply divergence theorem to the LHS, but here's where I get stuck and don't know where to proceed: $\partial \over \partial r$ $\int_{B(x,r)}\triangle v(z) dz = $ $\partial \over \partial r$ $\int_{B(x,r)}div(\nabla v(z)) dz =$ $\partial \over \partial r$ $\int_{\partial B(x,r)}\nabla v(y) \cdot \nu dy$ Where $\nu$ is an exterior normal to $\partial B(x,r)$. I do not know when/how to pass the partial derivative into the integrand to get the result. Thanks in advance,",,['multivariable-calculus']
39,How to minimize $\|v-w\|_2$?,How to minimize ?,\|v-w\|_2,"Let $W\subseteq \mathbb R^m$ be a $n$-dimensional subspace and $F:W\longrightarrow \mathbb R$ given by $$F(w)=\|v-w\|,$$ where $v$ is a fixed vector on $\mathbb R^m$. I need some help for showing $\displaystyle u=\sum_{i=1}^n \langle v, w_i\rangle w_i$ is a minimum for $F$. Here $\{w_1, \ldots, w_n\}$ is an orthonormal basis for $W$. Sketch: I have already found the gradient of $F$: $$\nabla F(w)=\left(\frac{v_1-w_1}{\|v-w\|}, \ldots, \frac{v_m-w_m}{\|v-w\|}\right).$$ I tried to find the critical points of $F$ but it sounds $v=w$ is the only critical point..I don't know how to proceed.. Any help will be valuable.. Thanks","Let $W\subseteq \mathbb R^m$ be a $n$-dimensional subspace and $F:W\longrightarrow \mathbb R$ given by $$F(w)=\|v-w\|,$$ where $v$ is a fixed vector on $\mathbb R^m$. I need some help for showing $\displaystyle u=\sum_{i=1}^n \langle v, w_i\rangle w_i$ is a minimum for $F$. Here $\{w_1, \ldots, w_n\}$ is an orthonormal basis for $W$. Sketch: I have already found the gradient of $F$: $$\nabla F(w)=\left(\frac{v_1-w_1}{\|v-w\|}, \ldots, \frac{v_m-w_m}{\|v-w\|}\right).$$ I tried to find the critical points of $F$ but it sounds $v=w$ is the only critical point..I don't know how to proceed.. Any help will be valuable.. Thanks",,"['calculus', 'multivariable-calculus']"
40,Complex integral over a semi circle,Complex integral over a semi circle,,"Let $f(z) := \frac{\mathbb{e}^{iz}}{z}$ $z \in \mathbb{C}$ where $0 \notin \mathbb{C}$ I need to show that when $C_k$, a semi circle of radius e is traversed in the clockwise direction is traversed in the clockwise direction $ \int_{C_k} f(z) dz \to -i\pi $  as $ k\to 0$ So firstly i set up a parameterization of the semi circle $z(t) = e^{-it}$ where $-\pi\leq t \leq 0 $ (I'm not sure this is totally correct) I then change the variable to get the integral $\int_{C_k} f(z) dz$ = $\int_{-\pi}^0 -ie^{ike^{-it}} dt$ I then try and bound this using the fact that $| \int_{C_k} f(z) dz | \leq \int_{C_k} |f(z)| |dz|$ but i cannot seem to find a bound!","Let $f(z) := \frac{\mathbb{e}^{iz}}{z}$ $z \in \mathbb{C}$ where $0 \notin \mathbb{C}$ I need to show that when $C_k$, a semi circle of radius e is traversed in the clockwise direction is traversed in the clockwise direction $ \int_{C_k} f(z) dz \to -i\pi $  as $ k\to 0$ So firstly i set up a parameterization of the semi circle $z(t) = e^{-it}$ where $-\pi\leq t \leq 0 $ (I'm not sure this is totally correct) I then change the variable to get the integral $\int_{C_k} f(z) dz$ = $\int_{-\pi}^0 -ie^{ike^{-it}} dt$ I then try and bound this using the fact that $| \int_{C_k} f(z) dz | \leq \int_{C_k} |f(z)| |dz|$ but i cannot seem to find a bound!",,"['integration', 'complex-analysis', 'multivariable-calculus']"
41,Cauchy repeated integration formula - different lower limits/ change of variable,Cauchy repeated integration formula - different lower limits/ change of variable,,"Cauchy's repeated integration formula is as follows: Let $f^{(-n)}$ be a continuous function on the real line.  Then the $n^{th}$ repeated integral of $f$ based at $a$, $$f^{(-n)}(x) = \int_a^x \int_a^{\sigma_1} \cdots \int_a^{\sigma_{n-1}} f(\sigma_{n}) \, \mathrm{d}\sigma_{n} \cdots \, \mathrm{d}\sigma_2 \, \mathrm{d}\sigma_1$$ is given by single integration $$f^{(-n)}(x) = \frac{1}{(n-1)!} \int_a^x\left(x-t\right)^{n-1} f(t)\,\mathrm{d}t$$ source: http://en.wikipedia.org/wiki/Cauchy_formula_for_repeated_integration Now, consider something like: $$I(c)= \int_0^c \int_b^2 \int_0^a ({2-x})\, \mathrm{d}x\, \mathrm{d}a\, \mathrm{d}b$$ The lower limits are different ($0,b,0$) and I want to apply Cauchy's formula, what do I do? Is there any generalized variant for this? I tried a change of variables, but was unsuccessful: Set $a=2-t.$ So when $a=b$,we have $t=2-b$, and when $a=2$, we have $t=0.$ Also, $\, \mathrm{d}a = -\, \mathrm{d}t$. Substituting $t$ into the integral and reversing the limits of integration by utilizing the negative sign , I got: $$I(c)= \int_0^c \int_0^{2-b} \int_0^{2-a} ({2-x})\, \mathrm{d}x\, \mathrm{d}a\, \mathrm{d}b$$ Now, I applied Cauchy's formula since all the lower bounds on the integrals are equal. According to the formula, my answer should be: $$I(c)=\frac{1}{2} \int_0^c {(c-t)}^2 ({2-t})\, \mathrm{d}t $$ However, when calculated step-by-step, integrating thrice, $I(c)$ turns out to be different and does not match. Why so? Thank you for your time. EDIT: Besides, is it even possible to find a closed-form for this one? If not, can you suggest why?","Cauchy's repeated integration formula is as follows: Let $f^{(-n)}$ be a continuous function on the real line.  Then the $n^{th}$ repeated integral of $f$ based at $a$, $$f^{(-n)}(x) = \int_a^x \int_a^{\sigma_1} \cdots \int_a^{\sigma_{n-1}} f(\sigma_{n}) \, \mathrm{d}\sigma_{n} \cdots \, \mathrm{d}\sigma_2 \, \mathrm{d}\sigma_1$$ is given by single integration $$f^{(-n)}(x) = \frac{1}{(n-1)!} \int_a^x\left(x-t\right)^{n-1} f(t)\,\mathrm{d}t$$ source: http://en.wikipedia.org/wiki/Cauchy_formula_for_repeated_integration Now, consider something like: $$I(c)= \int_0^c \int_b^2 \int_0^a ({2-x})\, \mathrm{d}x\, \mathrm{d}a\, \mathrm{d}b$$ The lower limits are different ($0,b,0$) and I want to apply Cauchy's formula, what do I do? Is there any generalized variant for this? I tried a change of variables, but was unsuccessful: Set $a=2-t.$ So when $a=b$,we have $t=2-b$, and when $a=2$, we have $t=0.$ Also, $\, \mathrm{d}a = -\, \mathrm{d}t$. Substituting $t$ into the integral and reversing the limits of integration by utilizing the negative sign , I got: $$I(c)= \int_0^c \int_0^{2-b} \int_0^{2-a} ({2-x})\, \mathrm{d}x\, \mathrm{d}a\, \mathrm{d}b$$ Now, I applied Cauchy's formula since all the lower bounds on the integrals are equal. According to the formula, my answer should be: $$I(c)=\frac{1}{2} \int_0^c {(c-t)}^2 ({2-t})\, \mathrm{d}t $$ However, when calculated step-by-step, integrating thrice, $I(c)$ turns out to be different and does not match. Why so? Thank you for your time. EDIT: Besides, is it even possible to find a closed-form for this one? If not, can you suggest why?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals']"
42,"Lagrange Multipliers, two constraints.","Lagrange Multipliers, two constraints.",,Question: . Use Lagrange multipliers to ﬁnd the constrained critical points of f subject to the given constraints. Here is the equation and the here is my solution . I am stuck now and I don't know how to proceed. I got a couple of restrictions but that's about it.,Question: . Use Lagrange multipliers to ﬁnd the constrained critical points of f subject to the given constraints. Here is the equation and the here is my solution . I am stuck now and I don't know how to proceed. I got a couple of restrictions but that's about it.,,['multivariable-calculus']
43,Question about proof of Morse Lemma,Question about proof of Morse Lemma,,"I am working on a problem for my differential geometry course. We are proving the following special case of the Morse lemma: Let $U \subseteq \mathbb{R}^n$ be open and containing the origin $\mathbf{0} \in \mathbb{R}^n$ (denote coordinates on $U$ by $x = (x_1, \dots, x_n)$ ). Suppose we have smooth $f : U \to \mathbb{R}$ , $f(\mathbf{0}) = 0$ , with $\mathbf{0}$ a nondegenerate critical point of $f$ of index $1 \le \lambda \le n$ . That is, we know that: $\frac{\partial f}{\partial x_i}(\mathbf{0}) = 0$ , all $i$ . The Hessian matrix $\left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right]$ is nonsingular. $\left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right]$ has $\lambda$ negative eigenvalues and $n - \lambda$ positive eigenvalues. By transforming coordinates in a neighborhood of $\mathbf{0}$ to $y = (y_1, \dots ,y_n)$ , prove that we can write: $$f(y) = - \sum_1^\lambda (y_i)^2 + \sum_{\lambda + 1}^n (y_i)^2$$ Here is what I have so far. Because $f(\mathbf{0}) = 0$ and $\frac{\partial f}{\partial x_i}(\mathbf{0}) = 0$ , all $i$ , I know from a previous result in class that I am able to write $$f(x) = \frac{1}{2} (x_1, \dots , x_n) \left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] (x_1, \dots, x_n)^T$$ in a sufficiently small spherical neighborhood $B$ of $\mathbf{0}$ ( $B \subseteq U$ ). Then I can take the Hessian $\left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right]$ and rewrite it in it's eigendecomposition $$\left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] = Q^T \Lambda Q.$$ Here, $\Lambda$ is a diagonal matrix with the first $\lambda$ diagonal entries being the negative eigenvalues of the Hessian, and the last $n - \lambda$ diagonal values being the positive eigenvalues. So it seems like I am getting close to finishing the proof, except $Q(x_1, \dots, x_n)^T$ is not quite the coordinate transformation I need. Roughly speaking, I need another transformation to ensure the diagonal elements of $\Lambda$ are $-1, -1, \dots ,1 ,1.$ But I'm not quite sure how to do this. Hints or solutions are greatly appreciated.","I am working on a problem for my differential geometry course. We are proving the following special case of the Morse lemma: Let be open and containing the origin (denote coordinates on by ). Suppose we have smooth , , with a nondegenerate critical point of of index . That is, we know that: , all . The Hessian matrix is nonsingular. has negative eigenvalues and positive eigenvalues. By transforming coordinates in a neighborhood of to , prove that we can write: Here is what I have so far. Because and , all , I know from a previous result in class that I am able to write in a sufficiently small spherical neighborhood of ( ). Then I can take the Hessian and rewrite it in it's eigendecomposition Here, is a diagonal matrix with the first diagonal entries being the negative eigenvalues of the Hessian, and the last diagonal values being the positive eigenvalues. So it seems like I am getting close to finishing the proof, except is not quite the coordinate transformation I need. Roughly speaking, I need another transformation to ensure the diagonal elements of are But I'm not quite sure how to do this. Hints or solutions are greatly appreciated.","U \subseteq \mathbb{R}^n \mathbf{0} \in \mathbb{R}^n U x = (x_1, \dots, x_n) f : U \to \mathbb{R} f(\mathbf{0}) = 0 \mathbf{0} f 1 \le \lambda \le n \frac{\partial f}{\partial x_i}(\mathbf{0}) = 0 i \left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] \left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] \lambda n - \lambda \mathbf{0} y = (y_1, \dots ,y_n) f(y) = - \sum_1^\lambda (y_i)^2 + \sum_{\lambda + 1}^n (y_i)^2 f(\mathbf{0}) = 0 \frac{\partial f}{\partial x_i}(\mathbf{0}) = 0 i f(x) = \frac{1}{2} (x_1, \dots , x_n) \left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] (x_1, \dots, x_n)^T B \mathbf{0} B \subseteq U \left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] \left[ \frac{\partial^2f}{\partial x_i \partial x_j} \left(\mathbf{0}\right)\right] = Q^T \Lambda Q. \Lambda \lambda n - \lambda Q(x_1, \dots, x_n)^T \Lambda -1, -1, \dots ,1 ,1.","['linear-algebra', 'multivariable-calculus', 'differential-geometry']"
44,$\nabla \cdot \hat n$ where $\hat n$ is a unit vector normal to a cylinder of radius $R$ and with a length $L=\infty$,where  is a unit vector normal to a cylinder of radius  and with a length,\nabla \cdot \hat n \hat n R L=\infty,I'd like to calculate $\nabla \cdot  \hat n$ where $\hat n$ is a unit vector  normal  to a  cylinder of radius $R$ and with a length $L=\infty$. What I've thought of is: $\hat n= \hat R $ and using: $\nabla \cdot \vec v = \frac{1}{s} \frac{\partial }{\partial s}(s v_s)+\frac{1}{s} \frac{\partial }{\partial \phi }( v_\phi )  + \frac{\partial }{\partial z}v_z$ giving: I would get: $\nabla \cdot  \hat n=\frac{1}{R} \frac{\partial }{\partial R}(R \cdot 1)=1/R$ Is this a correct way or how should I do it differently?,I'd like to calculate $\nabla \cdot  \hat n$ where $\hat n$ is a unit vector  normal  to a  cylinder of radius $R$ and with a length $L=\infty$. What I've thought of is: $\hat n= \hat R $ and using: $\nabla \cdot \vec v = \frac{1}{s} \frac{\partial }{\partial s}(s v_s)+\frac{1}{s} \frac{\partial }{\partial \phi }( v_\phi )  + \frac{\partial }{\partial z}v_z$ giving: I would get: $\nabla \cdot  \hat n=\frac{1}{R} \frac{\partial }{\partial R}(R \cdot 1)=1/R$ Is this a correct way or how should I do it differently?,,"['calculus', 'multivariable-calculus']"
45,How to convert parametric form to a single algebraic equation?,How to convert parametric form to a single algebraic equation?,,"I'm pretty sure this is impossible to do but here is my attempt. Parametric form: $$x=1+t\\y=2+2t\\z=3+3t$$ Attempt: $$(x,y,z)=(1+t,2+2t,3+3t)$$ That didn't really get me anywhere, so here I tried to put it into symmetric form: $$x-1=\frac{y-2}2=\frac{z-3}3$$ Basically, I'm trying to get it into something like this: $$z^2+xy-2x-y^2=1$$ I am aware that represents a plane but I'm trying to get an equation to represent a line. If this is not possible, what can an equation represent? Only planes? Is there any easy way to tell what an equation represents from just looking at it if it can represent more than one thing? Also, bonus question: I am trying to put the equation $z=2$ into a function. Is $F(x,y,z)=z-2$ correct? My graph of F doesn't match z=2: http://www.wolframalpha.com/input/?i=graph+F%28x%2Cy%2Cz%29%3Dz-2","I'm pretty sure this is impossible to do but here is my attempt. Parametric form: $$x=1+t\\y=2+2t\\z=3+3t$$ Attempt: $$(x,y,z)=(1+t,2+2t,3+3t)$$ That didn't really get me anywhere, so here I tried to put it into symmetric form: $$x-1=\frac{y-2}2=\frac{z-3}3$$ Basically, I'm trying to get it into something like this: $$z^2+xy-2x-y^2=1$$ I am aware that represents a plane but I'm trying to get an equation to represent a line. If this is not possible, what can an equation represent? Only planes? Is there any easy way to tell what an equation represents from just looking at it if it can represent more than one thing? Also, bonus question: I am trying to put the equation $z=2$ into a function. Is $F(x,y,z)=z-2$ correct? My graph of F doesn't match z=2: http://www.wolframalpha.com/input/?i=graph+F%28x%2Cy%2Cz%29%3Dz-2",,"['linear-algebra', 'multivariable-calculus']"
46,Calculating $\Delta_r f(r) $; stuck with: $\nabla \cdot \hat{r}$,Calculating ; stuck with:,\Delta_r f(r)  \nabla \cdot \hat{r},I would like to calculate $\Delta_r f(r) $. This is as far as I got: $\Delta_r f(r) =\nabla \cdot \nabla f(r) =\nabla \cdot \frac{\partial }{\partial r} f(r) \hat{r} = \nabla ( \frac{\partial }{\partial r} f(r) ) \cdot \hat{r} + ( \frac{\partial }{\partial r} f(r) ) \nabla \cdot \hat{r} $ $=\frac{\partial^2 }{\partial r^2 } f(r) \hat{r}\cdot \hat{r}+( \frac{\partial }{\partial r} f(r) ) \nabla \cdot \hat{r}$ $=\frac{\partial^2 }{\partial r^2 } f(r) +( \frac{\partial f}{\partial r}  ) \nabla \cdot \hat{r}$ Now how can I proceed? In other words: how can I calculate $\nabla \cdot \hat{r}$?,I would like to calculate $\Delta_r f(r) $. This is as far as I got: $\Delta_r f(r) =\nabla \cdot \nabla f(r) =\nabla \cdot \frac{\partial }{\partial r} f(r) \hat{r} = \nabla ( \frac{\partial }{\partial r} f(r) ) \cdot \hat{r} + ( \frac{\partial }{\partial r} f(r) ) \nabla \cdot \hat{r} $ $=\frac{\partial^2 }{\partial r^2 } f(r) \hat{r}\cdot \hat{r}+( \frac{\partial }{\partial r} f(r) ) \nabla \cdot \hat{r}$ $=\frac{\partial^2 }{\partial r^2 } f(r) +( \frac{\partial f}{\partial r}  ) \nabla \cdot \hat{r}$ Now how can I proceed? In other words: how can I calculate $\nabla \cdot \hat{r}$?,,"['calculus', 'multivariable-calculus', 'vector-analysis']"
47,How to prove that sequence $(1+1/n)^n$ is convergent and increasing?,How to prove that sequence  is convergent and increasing?,(1+1/n)^n,"For the sequence $(1+1/n)^n$, how does one prove that it is convergent and increasing series? I do know that as $n \to \infty$ it becomes constant $e$.","For the sequence $(1+1/n)^n$, how does one prove that it is convergent and increasing series? I do know that as $n \to \infty$ it becomes constant $e$.",,"['sequences-and-series', 'multivariable-calculus']"
48,Is gradient $\nabla x(\vec{z})$ equal to the product of gradient $\nabla x(\vec{y})$ and Jacobian $J_{\vec{y}}(\vec{z})$?,Is gradient  equal to the product of gradient  and Jacobian ?,\nabla x(\vec{z}) \nabla x(\vec{y}) J_{\vec{y}}(\vec{z}),"I have two functions $x(\vec{y})$ and $\vec{y}(\vec{z})$, and I need the gradient $\nabla x(\vec{z})$. I also have the gradient $\nabla x(\vec{y})$ and the Jacobian $J_{\vec{y}}(\vec{z})$. Can I simply take the product: $$   \nabla x(\vec{z}) = \nabla x(\vec{y})\;J_{\vec{y}}(\vec{z}) ? $$ Background: The first function $x(\vec{y})$ is a scalar-valued multivariate function, the second function $\vec{y}(\vec{z})$ is a vector-valued multivariate function. The gradient is the vector of partial derivatives: $$ \nabla x(\vec{y}) =  \pmatrix{ \frac{\partial x}{\partial y_1}(\vec{y})\\ \vdots\\ \frac{\partial x}{\partial y_n}(\vec{y})\\ } $$ and $$ \nabla x(\vec{z}) =  \pmatrix{ \frac{\partial x}{\partial z_1}(\vec{z})\\ \vdots\\ \frac{\partial x}{\partial z_m}(\vec{z})\\ }. $$ The Jacobian is the matrix of partial derivatives: $$ J_{\vec{y}}(\vec{z}) =  \pmatrix{ \frac{\partial y_1}{\partial z_1}(\vec{z}) & \dots & \frac{\partial y_1}{\partial z_n}(\vec{z}) \\ \vdots & \ddots & \vdots\\ \frac{\partial y_n}{\partial z_1}(\vec{z}) & \dots & \frac{\partial y_n}{\partial z_m}(\vec{z}) }. $$ Of course, I could explicitly form the function $x(\vec{z})$ by replacing each $y_i$ by $y_i(\vec{z})$ and then find the partial derivatives $\partial x/\partial{z_j}$. However, the function $\vec{y}(\vec{z})$ is rather complex (not in the sense of complex numbers, but in the sense that it contains many trigonometric expressions), which would make the process quite cumbersome. On the other hand, the gradient $\nabla x(\vec{y})$ and the Jacobian $J_{\vec{y}}(\vec{z})$ are rather simple. I have the intuition that the proposed product is allowed, and it worked with simple dummy functions, but I'd be interested in a proof, a source or at least the opinion of a mathematician.","I have two functions $x(\vec{y})$ and $\vec{y}(\vec{z})$, and I need the gradient $\nabla x(\vec{z})$. I also have the gradient $\nabla x(\vec{y})$ and the Jacobian $J_{\vec{y}}(\vec{z})$. Can I simply take the product: $$   \nabla x(\vec{z}) = \nabla x(\vec{y})\;J_{\vec{y}}(\vec{z}) ? $$ Background: The first function $x(\vec{y})$ is a scalar-valued multivariate function, the second function $\vec{y}(\vec{z})$ is a vector-valued multivariate function. The gradient is the vector of partial derivatives: $$ \nabla x(\vec{y}) =  \pmatrix{ \frac{\partial x}{\partial y_1}(\vec{y})\\ \vdots\\ \frac{\partial x}{\partial y_n}(\vec{y})\\ } $$ and $$ \nabla x(\vec{z}) =  \pmatrix{ \frac{\partial x}{\partial z_1}(\vec{z})\\ \vdots\\ \frac{\partial x}{\partial z_m}(\vec{z})\\ }. $$ The Jacobian is the matrix of partial derivatives: $$ J_{\vec{y}}(\vec{z}) =  \pmatrix{ \frac{\partial y_1}{\partial z_1}(\vec{z}) & \dots & \frac{\partial y_1}{\partial z_n}(\vec{z}) \\ \vdots & \ddots & \vdots\\ \frac{\partial y_n}{\partial z_1}(\vec{z}) & \dots & \frac{\partial y_n}{\partial z_m}(\vec{z}) }. $$ Of course, I could explicitly form the function $x(\vec{z})$ by replacing each $y_i$ by $y_i(\vec{z})$ and then find the partial derivatives $\partial x/\partial{z_j}$. However, the function $\vec{y}(\vec{z})$ is rather complex (not in the sense of complex numbers, but in the sense that it contains many trigonometric expressions), which would make the process quite cumbersome. On the other hand, the gradient $\nabla x(\vec{y})$ and the Jacobian $J_{\vec{y}}(\vec{z})$ are rather simple. I have the intuition that the proposed product is allowed, and it worked with simple dummy functions, but I'd be interested in a proof, a source or at least the opinion of a mathematician.",,"['multivariable-calculus', 'partial-derivative']"
49,Quesiton about functions,Quesiton about functions,,"I'm embarrassed asking this question. I took a long break.  main question is : cartesian coordinate system in $R^n$ space is shown as $(x_1,x_2 ...x_n)$. Show that for $1\le i\le n$  each $x_i:R^n\mapsto R$ function have partial derivatives with respect to its $k.$ variable and             $$for \quad   1\le k\le n \quad \frac{\partial x_i}{\partial x_k}(p)=\delta_{ik}  $$ Answer in the book was: for $1\le i \le n$ for $1\le i,j \le n$ let $i\neq j$ I dont understand  why is $x_i(p_1,p_2...p_{i-1},p_i+s, p_{i+1, ...p_n})=[p_i+s]$ at 1* . and isn't 2* wrong?","I'm embarrassed asking this question. I took a long break.  main question is : cartesian coordinate system in $R^n$ space is shown as $(x_1,x_2 ...x_n)$. Show that for $1\le i\le n$  each $x_i:R^n\mapsto R$ function have partial derivatives with respect to its $k.$ variable and             $$for \quad   1\le k\le n \quad \frac{\partial x_i}{\partial x_k}(p)=\delta_{ik}  $$ Answer in the book was: for $1\le i \le n$ for $1\le i,j \le n$ let $i\neq j$ I dont understand  why is $x_i(p_1,p_2...p_{i-1},p_i+s, p_{i+1, ...p_n})=[p_i+s]$ at 1* . and isn't 2* wrong?",,"['multivariable-calculus', 'functions', 'differential']"
50,"Find the volume of the intersection of the cylinders $\{(x,y,z)\in \mathbb{R}^3: x^2+z^2\leq 1\} \cap \{(x,y,z)\in \mathbb{R}^3:y^2 + z^2 \leq 1\}$",Find the volume of the intersection of the cylinders,"\{(x,y,z)\in \mathbb{R}^3: x^2+z^2\leq 1\} \cap \{(x,y,z)\in \mathbb{R}^3:y^2 + z^2 \leq 1\}","Find the volume of the intersection of the cylinders $$\{(x,y,z)\in \mathbb{R}^3: x^2+z^2\leq 1\} \cap \{(x,y,z)\in \mathbb{R}^3:x^2 + y^2 \leq 1\}.$$ My first approach led me into contradiction, and my second got me the right answer. I am concerned to find the conceptual errors in my first approach. I start from the set up $$V = 2\iint_{D}\sqrt{1-x^2}dA,$$ where $dA = dx\,dy$ and $D$ is the unit disc. First pass (polar): $$V = 2\iint_{D}\sqrt{1-x^2}dA\\ = 2\int_{0}^{2\pi}\!\!\int_{0}^1\sqrt{1-x^2}r\,dr\,d\theta$$ in the first quadrant, we can parameterize by $x,\theta$ $$= 8\int_{0}^{\pi/2}\!\!\int_{0}^1\sqrt{1-x^2}\frac{x}{\cos{\theta}}\,dr\,d\theta\\= 8\int_{0}^{\pi/2}\!\!\int_{0}^{\cos{\theta}}\sqrt{1-x^2}\frac{x}{\cos^2{\theta}}\,dx\,d\theta\\ = -4\int_{0}^{\pi/2}\sec^2{\theta}\int_{1}^{\sin^2{\theta}}u^{1/2}du\,d\theta\\=4\int_{0}^{\pi/2}\frac{1}{\cos^2{\theta}}\int_{\sin^2{\theta}}^1 u^{1/2}du \,d\theta\\ = 4\int_{0}^{\pi/2}\frac{1}{\cos^2{\theta}} \left( 3/2 - \sin^3{\theta} \right) d\theta,$$ and I notice the first term is not a convergent integral. Why not? Where is my mistake? I know parameterizing by $x,\theta$ is sort of funky, but why is it not working? Second Pass (Cartesian/Green's theorem): $$2\iint_{D}\sqrt{1-x^2}dx\,dy\\ \overset{\text{Green's}}{=} -2\int_{\partial D}y\sqrt{1-x^2}dx\\ = -2\int_{0}^{2\pi}\sin{\theta}|\sin{\theta}|(-\sin{\theta} d\theta)\\ = 4\int_{0}^{\pi}\sin^3{\theta}d\theta = \frac{16}{3}.$$ Thanks for your help as always.","Find the volume of the intersection of the cylinders $$\{(x,y,z)\in \mathbb{R}^3: x^2+z^2\leq 1\} \cap \{(x,y,z)\in \mathbb{R}^3:x^2 + y^2 \leq 1\}.$$ My first approach led me into contradiction, and my second got me the right answer. I am concerned to find the conceptual errors in my first approach. I start from the set up $$V = 2\iint_{D}\sqrt{1-x^2}dA,$$ where $dA = dx\,dy$ and $D$ is the unit disc. First pass (polar): $$V = 2\iint_{D}\sqrt{1-x^2}dA\\ = 2\int_{0}^{2\pi}\!\!\int_{0}^1\sqrt{1-x^2}r\,dr\,d\theta$$ in the first quadrant, we can parameterize by $x,\theta$ $$= 8\int_{0}^{\pi/2}\!\!\int_{0}^1\sqrt{1-x^2}\frac{x}{\cos{\theta}}\,dr\,d\theta\\= 8\int_{0}^{\pi/2}\!\!\int_{0}^{\cos{\theta}}\sqrt{1-x^2}\frac{x}{\cos^2{\theta}}\,dx\,d\theta\\ = -4\int_{0}^{\pi/2}\sec^2{\theta}\int_{1}^{\sin^2{\theta}}u^{1/2}du\,d\theta\\=4\int_{0}^{\pi/2}\frac{1}{\cos^2{\theta}}\int_{\sin^2{\theta}}^1 u^{1/2}du \,d\theta\\ = 4\int_{0}^{\pi/2}\frac{1}{\cos^2{\theta}} \left( 3/2 - \sin^3{\theta} \right) d\theta,$$ and I notice the first term is not a convergent integral. Why not? Where is my mistake? I know parameterizing by $x,\theta$ is sort of funky, but why is it not working? Second Pass (Cartesian/Green's theorem): $$2\iint_{D}\sqrt{1-x^2}dx\,dy\\ \overset{\text{Green's}}{=} -2\int_{\partial D}y\sqrt{1-x^2}dx\\ = -2\int_{0}^{2\pi}\sin{\theta}|\sin{\theta}|(-\sin{\theta} d\theta)\\ = 4\int_{0}^{\pi}\sin^3{\theta}d\theta = \frac{16}{3}.$$ Thanks for your help as always.",,"['geometry', 'integration', 'multivariable-calculus']"
51,Differentiation of a unitary matrix,Differentiation of a unitary matrix,,"Let $\mathbf{U}$ be a unitary matrix ($\mathbf{UU}^\dagger=\mathbf{1}$). What does this implies for $d( \mathbf{ U U }^\dagger)$? Is it mathematically sound to say: \begin{equation} d\mathbf{U} \mathbf{U}^\dagger + \mathbf{U}d \mathbf{U}^\dagger = 0 \implies \mathbf{U} d \mathbf{U}^\dagger = - d \mathbf{U} \mathbf{U}^\dagger\\ \implies d\mathbf{U}^\dagger \mathbf{U} = - \mathbf{U}^\dagger d \mathbf{U} \end{equation} i.e. can I treat $\mathbf{1}$ as a constant in normal differentiation and say that ""$d\mathbf{1}=\mathbf{0}$""","Let $\mathbf{U}$ be a unitary matrix ($\mathbf{UU}^\dagger=\mathbf{1}$). What does this implies for $d( \mathbf{ U U }^\dagger)$? Is it mathematically sound to say: \begin{equation} d\mathbf{U} \mathbf{U}^\dagger + \mathbf{U}d \mathbf{U}^\dagger = 0 \implies \mathbf{U} d \mathbf{U}^\dagger = - d \mathbf{U} \mathbf{U}^\dagger\\ \implies d\mathbf{U}^\dagger \mathbf{U} = - \mathbf{U}^\dagger d \mathbf{U} \end{equation} i.e. can I treat $\mathbf{1}$ as a constant in normal differentiation and say that ""$d\mathbf{1}=\mathbf{0}$""",,"['matrices', 'multivariable-calculus', 'derivatives']"
52,Finding a directional derivative,Finding a directional derivative,,"Find the directional derivative of $f(x,y,z)=3xy+z^2$ at the point $(5,1,−4)$ in the direction of a vector making an angle of $π/3$ with $∇f(5,1,−4)$. $f_\vec u(5,1,−4)=D_\vec uf(5,1,−4)=?$ I know how to do directional derivative questions but I have no idea about this one.  I'm guessing that I'm thinking about the question wrong. $D_\vec uf=f_x\vec u_x+f_y\vec u_y+f_z\vec u_z =\nabla f \cdot \vec u$ where $\|\vec u\|=1$ So $\nabla f= \langle 3y, 3x, 2z \rangle$ and $\nabla f(5, 1, -4)= \langle3,15,-8\rangle$ Then it says $\vec u$ makes a $\pi/3$ angle with $\nabla f(5, 1, -4)$ which would mean $\nabla f(5, 1, -4) \cdot \vec u = \|\nabla f(5, 1, -4)\| * \| \vec u\| * \cos(\pi/3)$ and knowing $\|\vec u\| = 1$ gives $3\vec u_x+15\vec u_y-8\vec u_z=\sqrt {298} * \cos(\pi/3)$ and with $\vec u_x^2+\vec u_y^2+\vec u_z^2=1$ gives and unsolvable system of equations (I think) So... i'm wondering where I went wrong.","Find the directional derivative of $f(x,y,z)=3xy+z^2$ at the point $(5,1,−4)$ in the direction of a vector making an angle of $π/3$ with $∇f(5,1,−4)$. $f_\vec u(5,1,−4)=D_\vec uf(5,1,−4)=?$ I know how to do directional derivative questions but I have no idea about this one.  I'm guessing that I'm thinking about the question wrong. $D_\vec uf=f_x\vec u_x+f_y\vec u_y+f_z\vec u_z =\nabla f \cdot \vec u$ where $\|\vec u\|=1$ So $\nabla f= \langle 3y, 3x, 2z \rangle$ and $\nabla f(5, 1, -4)= \langle3,15,-8\rangle$ Then it says $\vec u$ makes a $\pi/3$ angle with $\nabla f(5, 1, -4)$ which would mean $\nabla f(5, 1, -4) \cdot \vec u = \|\nabla f(5, 1, -4)\| * \| \vec u\| * \cos(\pi/3)$ and knowing $\|\vec u\| = 1$ gives $3\vec u_x+15\vec u_y-8\vec u_z=\sqrt {298} * \cos(\pi/3)$ and with $\vec u_x^2+\vec u_y^2+\vec u_z^2=1$ gives and unsolvable system of equations (I think) So... i'm wondering where I went wrong.",,['multivariable-calculus']
53,Multivariable local maximum proof,Multivariable local maximum proof,,"Suppose we have a twice differentiable function $f: \mathbb{R} ^n \to \mathbb{R}$, a point ${\bf x^0} = (x_1 ^0 , \ldots , x_n ^0)$ and we know that $\nabla f({\bf x}^0) = 0$ $({\bf x - x^0})H({\bf x^0})({\bf x - x^0})^T <0 $, $(H({\bf x^0})$ is the Hessian matrix of $f$) Prove that $f({\bf x^0})$ is the local maximum of $f$. Attempt at a solution Since $\nabla f({\bf x}^0) = 0$ we know that ${\bf x^0}$ is a critical point of the function $f$. Because the Hessian is smaller than $0$ at the point ${\bf x^0}$, $f$ has a local maximum at the point ${\bf x^0}$.$\hspace{0.5em} \square$ Here is my dilemma. My ""proof"" feels very cheap. However, I'm not taking a proof heavy class (in fact proofs of why $f$ has a local maximum at a critical point if the Hessian is smaller than zero are not included in my text!) so I don't know what to add ... I feel like I have said everything that has to be said so I'm hoping any of you can point out to me how I can buff it up. I just want to add that this question isn't really homework, it's from an old final by my teacher which I'm solving in my free time. Thanks. Improved solution We know that $\nabla f({\bf x}^0) = 0$. Then, by definition, ${\bf x}^0$ is a critical point of $f$. Define function $g({\bf x}^0) = ({\bf x - x^0})H({\bf x^0})({\bf x - x^0})^T < 0$. Since $g({\bf x}^0) < 0$, all the eigenvalues of $H({\bf x^0})$ are less than $0$ resulting in $f$ having a local maximum at the point ${\bf x^0}$. All there is left to do is reference where I use known theorems. Is this sufficient?","Suppose we have a twice differentiable function $f: \mathbb{R} ^n \to \mathbb{R}$, a point ${\bf x^0} = (x_1 ^0 , \ldots , x_n ^0)$ and we know that $\nabla f({\bf x}^0) = 0$ $({\bf x - x^0})H({\bf x^0})({\bf x - x^0})^T <0 $, $(H({\bf x^0})$ is the Hessian matrix of $f$) Prove that $f({\bf x^0})$ is the local maximum of $f$. Attempt at a solution Since $\nabla f({\bf x}^0) = 0$ we know that ${\bf x^0}$ is a critical point of the function $f$. Because the Hessian is smaller than $0$ at the point ${\bf x^0}$, $f$ has a local maximum at the point ${\bf x^0}$.$\hspace{0.5em} \square$ Here is my dilemma. My ""proof"" feels very cheap. However, I'm not taking a proof heavy class (in fact proofs of why $f$ has a local maximum at a critical point if the Hessian is smaller than zero are not included in my text!) so I don't know what to add ... I feel like I have said everything that has to be said so I'm hoping any of you can point out to me how I can buff it up. I just want to add that this question isn't really homework, it's from an old final by my teacher which I'm solving in my free time. Thanks. Improved solution We know that $\nabla f({\bf x}^0) = 0$. Then, by definition, ${\bf x}^0$ is a critical point of $f$. Define function $g({\bf x}^0) = ({\bf x - x^0})H({\bf x^0})({\bf x - x^0})^T < 0$. Since $g({\bf x}^0) < 0$, all the eigenvalues of $H({\bf x^0})$ are less than $0$ resulting in $f$ having a local maximum at the point ${\bf x^0}$. All there is left to do is reference where I use known theorems. Is this sufficient?",,"['real-analysis', 'multivariable-calculus', 'optimization', 'quadratic-forms']"
54,Divergence theorem and Green's identities,Divergence theorem and Green's identities,,"Let $V$ be a simply-connected region in $\mathbb{R^3}$ and  $C^1$ functions $f,g:V\to \mathbb{R}$ . Prove that if $\nabla^2f=\nabla^2g=0$ then: $f=g$ $\iff f(x)=g(x)$ for $x\in\partial V$ To prove $\Rightarrow$ is easy. If $f=g$ then for every $x$ in general $f(x)=g(x) $   let alone $x\in\partial V$. I had some difficulties in proving $\Leftarrow$ . I tried to prove it using Green's first identity, but I got stuck.","Let $V$ be a simply-connected region in $\mathbb{R^3}$ and  $C^1$ functions $f,g:V\to \mathbb{R}$ . Prove that if $\nabla^2f=\nabla^2g=0$ then: $f=g$ $\iff f(x)=g(x)$ for $x\in\partial V$ To prove $\Rightarrow$ is easy. If $f=g$ then for every $x$ in general $f(x)=g(x) $   let alone $x\in\partial V$. I had some difficulties in proving $\Leftarrow$ . I tried to prove it using Green's first identity, but I got stuck.",,"['calculus', 'multivariable-calculus']"
55,Zero Eigenvalues for Hessian Matrix,Zero Eigenvalues for Hessian Matrix,,"I need to show that along any line passing through the origin, $$F(x,y) = 3x^4 -4x^2y + y^2$$ has a minimum at $(0,0)$ but that without the restriction, there is no local minimum at $(0,0)$. The first part I can do easily, but for the second part I end up with a critical point at $(0,0)$ and a Hessian matrix which is $$\left(\begin{matrix}  0& 0\\ 0& 2\\ \end{matrix}\right) $$ and thus has a zero determinant (meaning it is a degenerate critical point) and both eigenvalues equal to zero. Does this imply there is no minimum at $(0,0)$ or do I have to do something else? Thanks in advance.","I need to show that along any line passing through the origin, $$F(x,y) = 3x^4 -4x^2y + y^2$$ has a minimum at $(0,0)$ but that without the restriction, there is no local minimum at $(0,0)$. The first part I can do easily, but for the second part I end up with a critical point at $(0,0)$ and a Hessian matrix which is $$\left(\begin{matrix}  0& 0\\ 0& 2\\ \end{matrix}\right) $$ and thus has a zero determinant (meaning it is a degenerate critical point) and both eigenvalues equal to zero. Does this imply there is no minimum at $(0,0)$ or do I have to do something else? Thanks in advance.",,"['multivariable-calculus', 'optimization']"
56,How do I find the general equation of a plane?,How do I find the general equation of a plane?,,"Is anyone able to help me answer the following question? Consider the plane described by the vector equation $$(x, y, z) = (2,−1, 3) + t_1(1, 1, 1) + t_2(−2, 0,−1),\ t_1, t_2 \in R$$ Find a normal vector to this plane and hence find a general equation for a plane that intersects this plane at right angles and passes through the origin.","Is anyone able to help me answer the following question? Consider the plane described by the vector equation $$(x, y, z) = (2,−1, 3) + t_1(1, 1, 1) + t_2(−2, 0,−1),\ t_1, t_2 \in R$$ Find a normal vector to this plane and hence find a general equation for a plane that intersects this plane at right angles and passes through the origin.",,"['calculus', 'multivariable-calculus']"
57,Partial derivatives in multivariable function,Partial derivatives in multivariable function,,"I was doing some practice problems and everything was going great till I saw this question out of the ordinary:  $$ f(x,y) = \sum_{n=0}^\infty(xy)^n \qquad \left|xy\right| < 1 $$ Any thoughts on how to do this? I know that the general formula should be something like this: Differentiate with respect to $x$: $nx^{n-1}y^n$ But this whole summation thing really confuses me as I am not 100% sure what to do.","I was doing some practice problems and everything was going great till I saw this question out of the ordinary:  $$ f(x,y) = \sum_{n=0}^\infty(xy)^n \qquad \left|xy\right| < 1 $$ Any thoughts on how to do this? I know that the general formula should be something like this: Differentiate with respect to $x$: $nx^{n-1}y^n$ But this whole summation thing really confuses me as I am not 100% sure what to do.",,"['multivariable-calculus', 'partial-derivative']"
58,Question about reexpressing the dot product,Question about reexpressing the dot product,,"Suppose that I have two arbitrary 3-dimensional vectors, $\vec{a}$ and $\vec{b}$.  By the definition of the dot product, I can write $$\vec{a} \cdot \vec{b} = \left|\vec{a}\right| \left|\vec{b}\right| \cos \theta$$ I can solve for $\cos \theta$: $$\cos \theta = \frac{\vec{a} \cdot \vec{b}}{\left|\vec{a}\right| \left|\vec{b}\right|}$$ My question is, is it correct to  rewrite $\cos \theta$ in terms of the unit vectors $\hat{a}$ and $\hat{b}$ as follows? $$\cos \theta = \frac{\vec{a} \cdot \vec{b}}{\left|\vec{a}\right| \left|\vec{b}\right|}$$ $$\cos \theta = \frac{\vec{a}}{\left|\vec{a}\right|} \cdot \frac{\vec{b}}{\left|\vec{b}\right|}$$ $$\boxed{\cos \theta = \hat{a} \cdot \hat{b}}$$ where $\hat{a} = \frac{\vec{a}}{\left|\vec{a}\right|}$ and $\hat{b} = \frac{\vec{b}}{\left|\vec{b}\right|}$. In other words, is it correct to reexpress the dot product in that way?  Thanks for your time.","Suppose that I have two arbitrary 3-dimensional vectors, $\vec{a}$ and $\vec{b}$.  By the definition of the dot product, I can write $$\vec{a} \cdot \vec{b} = \left|\vec{a}\right| \left|\vec{b}\right| \cos \theta$$ I can solve for $\cos \theta$: $$\cos \theta = \frac{\vec{a} \cdot \vec{b}}{\left|\vec{a}\right| \left|\vec{b}\right|}$$ My question is, is it correct to  rewrite $\cos \theta$ in terms of the unit vectors $\hat{a}$ and $\hat{b}$ as follows? $$\cos \theta = \frac{\vec{a} \cdot \vec{b}}{\left|\vec{a}\right| \left|\vec{b}\right|}$$ $$\cos \theta = \frac{\vec{a}}{\left|\vec{a}\right|} \cdot \frac{\vec{b}}{\left|\vec{b}\right|}$$ $$\boxed{\cos \theta = \hat{a} \cdot \hat{b}}$$ where $\hat{a} = \frac{\vec{a}}{\left|\vec{a}\right|}$ and $\hat{b} = \frac{\vec{b}}{\left|\vec{b}\right|}$. In other words, is it correct to reexpress the dot product in that way?  Thanks for your time.",,"['calculus', 'multivariable-calculus', '3d']"
59,Properties of some operator on vectors of $\mathbb{R}^2$,Properties of some operator on vectors of,\mathbb{R}^2,"Suppose that $\circ$ is an operation on $\Bbb R^2$ with the following properties: For any $\vec p,\vec q \in \mathbb{R}^2$, and $t \in \mathbb{R}$, $(t \vec p ) \circ \vec q = t(\vec p \circ \vec q)$ holds. For any $\vec p, \vec q, \vec r \in \mathbb{R}^2$, $\vec p \circ (\vec q + \vec r) = \vec p \circ \vec q + \vec p \circ \vec r$ holds. For any $\vec p, \vec q \in \mathbb{R}^2$, $\vec p \circ \vec q = -\vec q \circ \vec p$ holds. For any $\vec p, \vec q, \vec r \in \mathbb{R}^2$, $(\vec p \circ \vec q) \circ \vec r = (\vec p \cdot \vec r)\vec q - (\vec q \cdot \vec r)\vec p$ holds. Why is it then true that $\vec p \circ \vec q = \vec 0$ all the time?","Suppose that $\circ$ is an operation on $\Bbb R^2$ with the following properties: For any $\vec p,\vec q \in \mathbb{R}^2$, and $t \in \mathbb{R}$, $(t \vec p ) \circ \vec q = t(\vec p \circ \vec q)$ holds. For any $\vec p, \vec q, \vec r \in \mathbb{R}^2$, $\vec p \circ (\vec q + \vec r) = \vec p \circ \vec q + \vec p \circ \vec r$ holds. For any $\vec p, \vec q \in \mathbb{R}^2$, $\vec p \circ \vec q = -\vec q \circ \vec p$ holds. For any $\vec p, \vec q, \vec r \in \mathbb{R}^2$, $(\vec p \circ \vec q) \circ \vec r = (\vec p \cdot \vec r)\vec q - (\vec q \cdot \vec r)\vec p$ holds. Why is it then true that $\vec p \circ \vec q = \vec 0$ all the time?",,['multivariable-calculus']
60,What functions on the plane (and on $\mathbb{R}^n$) have projection-valued derivatives?,What functions on the plane (and on ) have projection-valued derivatives?,\mathbb{R}^n,"Thinking about a more general problem I am trying to work out a specific case: If $U\subset \mathbb{R}^2$ is a connected open set what are the differentiable (or $C^r$) functions $f\colon U\to\mathbb{R}^2$ such that $Df(x,y)$ is a rank one projection for every $(x,y)\in U$? In other words what are the solutions of the differential equation $$\begin{cases} \partial_1f_1+\partial_2f_2=1\\ \partial_1f_2\cdot \partial_2f_1=\partial_1f_1\cdot \partial_2f_2 \end{cases}?$$ I managed to solve three special cases. Case 1. If the projections have the same range, say it's not the $y$ axis, then assuming $U$ is convex $f$ is of the form $$f(x,y)=\begin{bmatrix} g(y-mx)+x\\ m\cdot g(y-mx)+mx+c \end{bmatrix}$$ for arbitrary $c,m\in\mathbb{R}$ and differentiable function $g\colon\{y-mx\mid(x,y)\in U\}\to\mathbb{R}$. Hence $$Df(x,y)=\begin{bmatrix} 1-m\cdot g'(y-mx) & g'(y-mx)\\ m-m^2\cdot g'(y-mx) & m\cdot g'(y-mx) \end{bmatrix}.$$ Case 2. If the projections have the same kernel, say it's not the $x$ axis, then assuming $U$ is convex $f$ is of the form $$f(x,y)=\begin{bmatrix} x+my-m\cdot g(x+my)+c\\ g(x+my) \end{bmatrix}$$ for arbitrary $c,m\in\mathbb{R}$ and differentiable function $g\colon\{x+my\mid(x,y)\in U\}\to\mathbb{R}$. Hence $$Df(x,y)=\begin{bmatrix} 1-m\cdot g'(x+my) & m-m^2\cdot g'(x+my)\\ g'(x+my) & m\cdot g'(x+my) \end{bmatrix}.$$ Case 3. If the projections are all orthogonal, in which case $f$ is locally the gradient of some functions $\phi_V\colon U\supset V\to\mathbb{R}$, the only solutions are $$f(x,y)=\begin{bmatrix} mx\pm y\sqrt{m(1-m)}+a\\ (1-m)y\pm x\sqrt{m(1-m)}+b \end{bmatrix}$$ for arbitrary $a,b\in\mathbb{R}$, $m\in[0,1]$. Hence $$Df(x,y)=\begin{bmatrix} m & \pm\sqrt{m(1-m)}\\ \pm\sqrt{m(1-m)} & 1-m \end{bmatrix}.$$ Naturally $f$ maps into a translation of the shared range in Case 1, and is locally constant along the shared kernel in Case 2. But I couldn't find a solution where the two subspaces varied ""independently"". That is: Can the function $$U\to\mathbb{P}^1(\mathbb{R})\times\mathbb{P}^1(\mathbb{R});\quad(x,y)\mapsto(RanDf(x,y),KerDf(x,y))$$ be locally non-constant, and if so what is an example of such a function $f$? I originally asked whether this function can be injective on some open set, but I realize that would be a needlessly strong condition, although I would be interested in an answer to that as well. Ultimately I have the same questions in arbitrary dimension, but for now I just want to solve this on the plane. Have these problems been investigated before and where?","Thinking about a more general problem I am trying to work out a specific case: If $U\subset \mathbb{R}^2$ is a connected open set what are the differentiable (or $C^r$) functions $f\colon U\to\mathbb{R}^2$ such that $Df(x,y)$ is a rank one projection for every $(x,y)\in U$? In other words what are the solutions of the differential equation $$\begin{cases} \partial_1f_1+\partial_2f_2=1\\ \partial_1f_2\cdot \partial_2f_1=\partial_1f_1\cdot \partial_2f_2 \end{cases}?$$ I managed to solve three special cases. Case 1. If the projections have the same range, say it's not the $y$ axis, then assuming $U$ is convex $f$ is of the form $$f(x,y)=\begin{bmatrix} g(y-mx)+x\\ m\cdot g(y-mx)+mx+c \end{bmatrix}$$ for arbitrary $c,m\in\mathbb{R}$ and differentiable function $g\colon\{y-mx\mid(x,y)\in U\}\to\mathbb{R}$. Hence $$Df(x,y)=\begin{bmatrix} 1-m\cdot g'(y-mx) & g'(y-mx)\\ m-m^2\cdot g'(y-mx) & m\cdot g'(y-mx) \end{bmatrix}.$$ Case 2. If the projections have the same kernel, say it's not the $x$ axis, then assuming $U$ is convex $f$ is of the form $$f(x,y)=\begin{bmatrix} x+my-m\cdot g(x+my)+c\\ g(x+my) \end{bmatrix}$$ for arbitrary $c,m\in\mathbb{R}$ and differentiable function $g\colon\{x+my\mid(x,y)\in U\}\to\mathbb{R}$. Hence $$Df(x,y)=\begin{bmatrix} 1-m\cdot g'(x+my) & m-m^2\cdot g'(x+my)\\ g'(x+my) & m\cdot g'(x+my) \end{bmatrix}.$$ Case 3. If the projections are all orthogonal, in which case $f$ is locally the gradient of some functions $\phi_V\colon U\supset V\to\mathbb{R}$, the only solutions are $$f(x,y)=\begin{bmatrix} mx\pm y\sqrt{m(1-m)}+a\\ (1-m)y\pm x\sqrt{m(1-m)}+b \end{bmatrix}$$ for arbitrary $a,b\in\mathbb{R}$, $m\in[0,1]$. Hence $$Df(x,y)=\begin{bmatrix} m & \pm\sqrt{m(1-m)}\\ \pm\sqrt{m(1-m)} & 1-m \end{bmatrix}.$$ Naturally $f$ maps into a translation of the shared range in Case 1, and is locally constant along the shared kernel in Case 2. But I couldn't find a solution where the two subspaces varied ""independently"". That is: Can the function $$U\to\mathbb{P}^1(\mathbb{R})\times\mathbb{P}^1(\mathbb{R});\quad(x,y)\mapsto(RanDf(x,y),KerDf(x,y))$$ be locally non-constant, and if so what is an example of such a function $f$? I originally asked whether this function can be injective on some open set, but I realize that would be a needlessly strong condition, although I would be interested in an answer to that as well. Ultimately I have the same questions in arbitrary dimension, but for now I just want to solve this on the plane. Have these problems been investigated before and where?",,"['reference-request', 'multivariable-calculus', 'differential-geometry']"
61,How to find an equation for the surface that the distance from its point to x-axis is three times the distance from its point to yz-plane,How to find an equation for the surface that the distance from its point to x-axis is three times the distance from its point to yz-plane,,"As title says, how does one find an equation for the surface consisting of all points $P$ for which the distance from $P$ to the x-axis is three times the distance from $P$ to the $yz$-plane?","As title says, how does one find an equation for the surface consisting of all points $P$ for which the distance from $P$ to the x-axis is three times the distance from $P$ to the $yz$-plane?",,"['algebra-precalculus', 'multivariable-calculus']"
62,How to prove mathematically that two planes parallel to a third plane are parallel,How to prove mathematically that two planes parallel to a third plane are parallel,,"Without relying on geometrical intuition and purely using vector calculus, how do we show that two planes parallel to a third plane are parallel? I assume three dimensional space.","Without relying on geometrical intuition and purely using vector calculus, how do we show that two planes parallel to a third plane are parallel? I assume three dimensional space.",,['multivariable-calculus']
63,Contraction mapping proof question,Contraction mapping proof question,,"I'm reading Wendell Fleming's book Functions of Several Variables , and on page 144, he states the following lemma: Suppose $\phi: \mathbb{R}^n \to \mathbb{R}^n$ is continuous on some neighborhood $\Omega$ of $0$, and such that $$\|\phi (t)\| \leq c \|t\| \; \; \; \forall t \in \Omega, \text{ where }0<c<1.$$ Define $\Psi(t) := \sum_{i=0}^\infty \phi^{[i]}(t)$ and $\Psi_r(t) := \sum_{i=0}^r \phi^{[i]}(t)$, where $\phi^{[i]}(t):= \phi \circ \dotsb \circ \phi$ is the i-fold composition of $\phi$. Then $\|\Psi(t)\| \leq \frac{|t|}{1-c}$ $\Psi(t) - \phi[\Psi(t)]=t$. In proving the second part, he states: $$\Psi_r - \phi \circ \Psi_r = \sum_{i=0}^r \phi^{[i]} - \sum_{i=1}^{r+1} \phi^{[i]}= I - \phi^{[r+1]}.$$ I cannot figure out why $\Psi_r - \phi \circ \Psi_r = \sum_{i=0}^r \phi^{[i]} - \sum_{i=1}^{r+1} \phi^{[i]}$ holds. It seems it would be true for $\Psi_r - \Psi_r \circ \phi$. I'm sure I'm just missing something obvious, but I can't see why it is true. I checked to make sure Fleming's notation of function composition is the usual one, and it is. I checked to make sure it wasn't a typo...the rest of the proof proceeds the same way. What am I missing? Here is a scan of the page, if it helps: https://i.sstatic.net/Ufib4.jpg","I'm reading Wendell Fleming's book Functions of Several Variables , and on page 144, he states the following lemma: Suppose $\phi: \mathbb{R}^n \to \mathbb{R}^n$ is continuous on some neighborhood $\Omega$ of $0$, and such that $$\|\phi (t)\| \leq c \|t\| \; \; \; \forall t \in \Omega, \text{ where }0<c<1.$$ Define $\Psi(t) := \sum_{i=0}^\infty \phi^{[i]}(t)$ and $\Psi_r(t) := \sum_{i=0}^r \phi^{[i]}(t)$, where $\phi^{[i]}(t):= \phi \circ \dotsb \circ \phi$ is the i-fold composition of $\phi$. Then $\|\Psi(t)\| \leq \frac{|t|}{1-c}$ $\Psi(t) - \phi[\Psi(t)]=t$. In proving the second part, he states: $$\Psi_r - \phi \circ \Psi_r = \sum_{i=0}^r \phi^{[i]} - \sum_{i=1}^{r+1} \phi^{[i]}= I - \phi^{[r+1]}.$$ I cannot figure out why $\Psi_r - \phi \circ \Psi_r = \sum_{i=0}^r \phi^{[i]} - \sum_{i=1}^{r+1} \phi^{[i]}$ holds. It seems it would be true for $\Psi_r - \Psi_r \circ \phi$. I'm sure I'm just missing something obvious, but I can't see why it is true. I checked to make sure Fleming's notation of function composition is the usual one, and it is. I checked to make sure it wasn't a typo...the rest of the proof proceeds the same way. What am I missing? Here is a scan of the page, if it helps: https://i.sstatic.net/Ufib4.jpg",,['multivariable-calculus']
64,How find this maximum of $f(n)$,How find this maximum of,f(n),"let $x_{i}\in (0,1),i=1,2,\cdots,n,x_{n+1}=x_{1}$,give for any positive integer numbets $n$, find  $$f(n)=\max{\sum_{i=1}^{n}x_{i}(1-x_{i+1})}$$ find the $f(n)$ it is easy find when $n=1$, then  $$f(1)=\max{x_{1}(1-x_{1})}=\dfrac{1}{4}$$ when $n=2$ $$f(2)=\max{\left(x_{1}(1-x_{2}),x_{2}(1-x_{1})\right)}\le\dfrac{1}{4}?$$ $$\cdots\cdots\cdots\cdots\cdots\cdots$$ so I can't any work,Thank you everyone","let $x_{i}\in (0,1),i=1,2,\cdots,n,x_{n+1}=x_{1}$,give for any positive integer numbets $n$, find  $$f(n)=\max{\sum_{i=1}^{n}x_{i}(1-x_{i+1})}$$ find the $f(n)$ it is easy find when $n=1$, then  $$f(1)=\max{x_{1}(1-x_{1})}=\dfrac{1}{4}$$ when $n=2$ $$f(2)=\max{\left(x_{1}(1-x_{2}),x_{2}(1-x_{1})\right)}\le\dfrac{1}{4}?$$ $$\cdots\cdots\cdots\cdots\cdots\cdots$$ so I can't any work,Thank you everyone",,"['multivariable-calculus', 'inequality']"
65,Finding the intersection of two lines. What am I doing wrong?,Finding the intersection of two lines. What am I doing wrong?,,"Question Determine whether the pair of lines are parallel and intersection: $$ r_1 = \langle -1, 0 , 1 \rangle + \lambda \langle 1, 3, 4\rangle$$ $$ r_2 = \langle2, 3, 0\rangle + \mu \langle4, -1, 1\rangle$$ Writing these equations in parametric form gives me: $$r_1 : $$ $$ x = -1 + \lambda$$ $$ y = 3 \lambda$$ $$ z = 1 + 4 \lambda$$ $$r_2 :$$ $$ x = 2 + 4 \mu$$ $$ y = 3 - \mu$$ $$ z = \mu $$ When I solve this equation I get so many solutions that don't make sense. But I don't think these lines are parallel either. Am I doing it wrong? Two lines have vector equations: $$ r = 4 \mathbf i + 5 \mathbf j + 6 \mathbf k + t (\mathbf i + 2 \mathbf j + 2 \mathbf k) $$ and $$ r = -3 \mathbf i + 3 \mathbf j - 8 \mathbf k + t (3 \mathbf i + 2 \mathbf j + 6 \mathbf k) $$ When I move it into parametric form I get: $$ 4 + 2t = -3 + 3t$$ $$ 5 + 2t = 3 + 2t$$ $$ 6 + 2t = -8 + 6t$$ Which is an unsolvable equation. What am I doing so terribly wrong? These are two instances where I cannot seem to solve this type of problem and I wonder what I'm doing wrong because this is a 2nd Year question, but I lost all that wiring in my head.","Question Determine whether the pair of lines are parallel and intersection: $$ r_1 = \langle -1, 0 , 1 \rangle + \lambda \langle 1, 3, 4\rangle$$ $$ r_2 = \langle2, 3, 0\rangle + \mu \langle4, -1, 1\rangle$$ Writing these equations in parametric form gives me: $$r_1 : $$ $$ x = -1 + \lambda$$ $$ y = 3 \lambda$$ $$ z = 1 + 4 \lambda$$ $$r_2 :$$ $$ x = 2 + 4 \mu$$ $$ y = 3 - \mu$$ $$ z = \mu $$ When I solve this equation I get so many solutions that don't make sense. But I don't think these lines are parallel either. Am I doing it wrong? Two lines have vector equations: $$ r = 4 \mathbf i + 5 \mathbf j + 6 \mathbf k + t (\mathbf i + 2 \mathbf j + 2 \mathbf k) $$ and $$ r = -3 \mathbf i + 3 \mathbf j - 8 \mathbf k + t (3 \mathbf i + 2 \mathbf j + 6 \mathbf k) $$ When I move it into parametric form I get: $$ 4 + 2t = -3 + 3t$$ $$ 5 + 2t = 3 + 2t$$ $$ 6 + 2t = -8 + 6t$$ Which is an unsolvable equation. What am I doing so terribly wrong? These are two instances where I cannot seem to solve this type of problem and I wonder what I'm doing wrong because this is a 2nd Year question, but I lost all that wiring in my head.",,"['calculus', 'multivariable-calculus']"
66,Converting an implicit 3-variable equation into explicit function,Converting an implicit 3-variable equation into explicit function,,"I have the equation: $$xz(y+1)+e^y=1$$ and I would like to be able to write $y$ as a function of $x$ and $z$ i.e. $y=f(x,z)$. How would I go about this? I think this might have something to do with partial derivatives. Eventually, I want to determine: $$\lim_{(x,z)\to (0,0)}\frac{f(x,z)}{x^2+z^2}$$ Alternatively, how do I compute this limit if I can't find a formula for $y=f(x,z)$ ?","I have the equation: $$xz(y+1)+e^y=1$$ and I would like to be able to write $y$ as a function of $x$ and $z$ i.e. $y=f(x,z)$. How would I go about this? I think this might have something to do with partial derivatives. Eventually, I want to determine: $$\lim_{(x,z)\to (0,0)}\frac{f(x,z)}{x^2+z^2}$$ Alternatively, how do I compute this limit if I can't find a formula for $y=f(x,z)$ ?",,"['multivariable-calculus', 'partial-derivative', 'implicit-differentiation']"
67,Find a unit vector parallel to both of the planes $8x+y+z = 1$ and $x-y-z=0$,Find a unit vector parallel to both of the planes  and,8x+y+z = 1 x-y-z=0,"Find a unit vector that is parallel to both the plane $8x+y+z = 1$ and the plane $x-y-z=0$. I found the normal vectors to be: $(8,1,1)$ and $(1,-1,-1)$ I took the cross product. $(8,1,1)\times(1,-1,-1) =  \begin{vmatrix} i & j & k \\ 8 & 1 & 1 \\ 1 & -1 & -1 \end{vmatrix}$ = $(2,-7,-9)$ right? I was checking if $(2,-7,-9)$  was orthogonal to my two normal vectors and found it was not orthogonal to $(1,-1,-1)$.","Find a unit vector that is parallel to both the plane $8x+y+z = 1$ and the plane $x-y-z=0$. I found the normal vectors to be: $(8,1,1)$ and $(1,-1,-1)$ I took the cross product. $(8,1,1)\times(1,-1,-1) =  \begin{vmatrix} i & j & k \\ 8 & 1 & 1 \\ 1 & -1 & -1 \end{vmatrix}$ = $(2,-7,-9)$ right? I was checking if $(2,-7,-9)$  was orthogonal to my two normal vectors and found it was not orthogonal to $(1,-1,-1)$.",,['multivariable-calculus']
68,Simplify vector equation,Simplify vector equation,,"I know that $div E=0$ and I know what $ curl E$ is. Further, I know what the vector laplacian of $E$ is. Now I want to simplify $\nabla \times (\nabla \times  f(x,y,z) E(x,y,z))$, where $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ and $E: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ by using these information whenever possible!","I know that $div E=0$ and I know what $ curl E$ is. Further, I know what the vector laplacian of $E$ is. Now I want to simplify $\nabla \times (\nabla \times  f(x,y,z) E(x,y,z))$, where $f:\mathbb{R}^3 \rightarrow \mathbb{R}$ and $E: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ by using these information whenever possible!",,"['calculus', 'real-analysis']"
69,Inverse function theorem: Why is $\frac {\partial \phi }{\partial y} = \frac {-\partial F / \partial y} {\partial F / \partial z}$?,Inverse function theorem: Why is ?,\frac {\partial \phi }{\partial y} = \frac {-\partial F / \partial y} {\partial F / \partial z},"Given $F(x,y,z) = 0$, $\partial F/\partial z \neq 0$ at $p_0$, by the implicit function theorem we can solve for $z=\phi(x,y)$ near $p_0$. I am told that $$\frac {\partial \phi }{\partial y} = \frac {-\partial F / \partial y} {\partial F / \partial z}.$$ I am told that we can come to this conclusion by taking the total derivative $$dF = \frac {\partial F} {\partial x}dx + \frac {\partial F} {\partial y}dy + \frac {\partial F} {\partial z}dz = 0,$$ setting $dx = 0$, and solving for $dz/dx$. I am aware that this has an interpretation in terms of differential forms, but for now it is just an algebraic manipulation to me. Can someone point me to a proof that $$\frac {\partial \phi }{\partial y} = \frac {-\partial F / \partial y} {\partial F / \partial z}?$$","Given $F(x,y,z) = 0$, $\partial F/\partial z \neq 0$ at $p_0$, by the implicit function theorem we can solve for $z=\phi(x,y)$ near $p_0$. I am told that $$\frac {\partial \phi }{\partial y} = \frac {-\partial F / \partial y} {\partial F / \partial z}.$$ I am told that we can come to this conclusion by taking the total derivative $$dF = \frac {\partial F} {\partial x}dx + \frac {\partial F} {\partial y}dy + \frac {\partial F} {\partial z}dz = 0,$$ setting $dx = 0$, and solving for $dz/dx$. I am aware that this has an interpretation in terms of differential forms, but for now it is just an algebraic manipulation to me. Can someone point me to a proof that $$\frac {\partial \phi }{\partial y} = \frac {-\partial F / \partial y} {\partial F / \partial z}?$$",,"['reference-request', 'multivariable-calculus']"
70,"find $\frac{\partial f(u(x(t),y(t)),v(x(t),y(t)))}{\partial t} $",find,"\frac{\partial f(u(x(t),y(t)),v(x(t),y(t)))}{\partial t} ","how to find $$\frac{d z}{d t} $$ where is $z=f(u(x(t),y(t)),v(x(t),y(t)))$ and if some one can give me some advice which make me deal more easy with this subject","how to find $$\frac{d z}{d t} $$ where is $z=f(u(x(t),y(t)),v(x(t),y(t)))$ and if some one can give me some advice which make me deal more easy with this subject",,['multivariable-calculus']
71,Jacobian Determinant of Polar-Coordinate Transformation,Jacobian Determinant of Polar-Coordinate Transformation,,"Consider a Euclidean space of $n\in\mathbb{Z}_+$ ($n\geq3$) dimensions. The coordinates of any vector can be defined in terms of polar coordinates as follows (this example appears in Folland, 1999): \begin{align*} x_1=&\,r\cos\phi_1,\\ x_2=&\,r\sin\phi_1\cos\phi_2,\\ \vdots&\\ x_k=&\,r\left(\prod_{j=1}^{k-1}\sin\phi_j\right)\cos\phi_k,\\ \vdots&\\ x_{n-1}=&\,r\left(\prod_{j=1}^{n-2}\sin\phi_j\right)\cos\theta,\\ x_{n}=&\,r\left(\prod_{j=1}^{n-2}\sin\phi_j\right)\sin\theta, \end{align*} where $(r,(\phi_j)_{j=1}^{n-2},\theta)\in\mathbb{R}^{n}$. Now, the Jacobian determinant corresponding to this transformation is claimed to be given as follows: \begin{align*} \det\left(\frac{\partial (x_j)_{j=1}^n}{\partial(r,(\phi_j)_{j=1}^{n-2},\theta)}\right)=r^{n-1}\prod_{j=1}^{n-2}\sin^{n-j-1}\phi_j. \end{align*} While this result is “standard,” I have had a hard time proving it. Technically, the problem is that the Jacobian matrix of \begin{align*} \frac{\partial (x_j)_{j=1}^n}{\partial(r,(\phi_j)_{j=1}^{n-2},\theta)} \end{align*} is a Hessenberg matrix: that is, it is “almost triangular,” but there is one extra diagonal set of nonzero elements just above the main diagonal. This makes computing the determinant directly extremely tedious. I tried induction and the Laplace expansion, too, to little avail. Are any of you familiar with the canonical proof of this result, or any standard reference? Thank you for your assistance in advance.","Consider a Euclidean space of $n\in\mathbb{Z}_+$ ($n\geq3$) dimensions. The coordinates of any vector can be defined in terms of polar coordinates as follows (this example appears in Folland, 1999): \begin{align*} x_1=&\,r\cos\phi_1,\\ x_2=&\,r\sin\phi_1\cos\phi_2,\\ \vdots&\\ x_k=&\,r\left(\prod_{j=1}^{k-1}\sin\phi_j\right)\cos\phi_k,\\ \vdots&\\ x_{n-1}=&\,r\left(\prod_{j=1}^{n-2}\sin\phi_j\right)\cos\theta,\\ x_{n}=&\,r\left(\prod_{j=1}^{n-2}\sin\phi_j\right)\sin\theta, \end{align*} where $(r,(\phi_j)_{j=1}^{n-2},\theta)\in\mathbb{R}^{n}$. Now, the Jacobian determinant corresponding to this transformation is claimed to be given as follows: \begin{align*} \det\left(\frac{\partial (x_j)_{j=1}^n}{\partial(r,(\phi_j)_{j=1}^{n-2},\theta)}\right)=r^{n-1}\prod_{j=1}^{n-2}\sin^{n-j-1}\phi_j. \end{align*} While this result is “standard,” I have had a hard time proving it. Technically, the problem is that the Jacobian matrix of \begin{align*} \frac{\partial (x_j)_{j=1}^n}{\partial(r,(\phi_j)_{j=1}^{n-2},\theta)} \end{align*} is a Hessenberg matrix: that is, it is “almost triangular,” but there is one extra diagonal set of nonzero elements just above the main diagonal. This makes computing the determinant directly extremely tedious. I tried induction and the Laplace expansion, too, to little avail. Are any of you familiar with the canonical proof of this result, or any standard reference? Thank you for your assistance in advance.",,"['real-analysis', 'multivariable-calculus', 'differential-geometry']"
72,Maximization of function in 3 variables,Maximization of function in 3 variables,,"If $( x,y,z)$ be the lengths of perpendiculars from any interior point P of a triangle $ABC$ on sides $BC,CA$ and $AB$ respectively then find the minimum value of : $$ x^2+ y^2 + z^2 $$ The sides of triangle being $a,b,c$. I thought of using Lagrange's method of multipliers but am not able to find another function in terms of $x,y,z$ and $a,b,c$ Any help will be appreciated. Thanks.","If $( x,y,z)$ be the lengths of perpendiculars from any interior point P of a triangle $ABC$ on sides $BC,CA$ and $AB$ respectively then find the minimum value of : $$ x^2+ y^2 + z^2 $$ The sides of triangle being $a,b,c$. I thought of using Lagrange's method of multipliers but am not able to find another function in terms of $x,y,z$ and $a,b,c$ Any help will be appreciated. Thanks.",,"['multivariable-calculus', 'optimization']"
73,Change of variables in 3 dimensions,Change of variables in 3 dimensions,,"Consider the following integral: $$\int_{|x| = \epsilon} \phi(x) \frac{e^{-m|x|}}{4 \pi |x|^2} d^3x.$$ I wanna show that this integral goes to $\phi(0)$ for $\epsilon \rightarrow 0$. The idea is to swap to spherical coordinates: $$\int_{|x| = \epsilon} \phi(x) \frac{e^{-m|x|}}{4 \pi |x|^2} d^3x = \frac{1}{4 \pi} \int_{S^2} \phi(x) \frac{e^{-m\epsilon}}{\epsilon^2} \epsilon^2 d\Omega = \frac{1}{4 \pi} \int_{S^2} \phi(x) {e^{-m\epsilon}} d\Omega $$ Now we wanna substitute $$y = x/|x|$$ $\frac{1}{4 \pi} \int_{S^2} \phi(x) {e^{-m\epsilon}} d\Omega = \frac{1}{4 \pi} \int_{S^2} \phi(\epsilon y) {e^{-m\epsilon}} d\Omega $ The last step is where I'm not sure what happens. Somehow the infinitesimal variable should change aswell, but I'm not quite sure how. Could somebody make that clear for me? Cheers!","Consider the following integral: $$\int_{|x| = \epsilon} \phi(x) \frac{e^{-m|x|}}{4 \pi |x|^2} d^3x.$$ I wanna show that this integral goes to $\phi(0)$ for $\epsilon \rightarrow 0$. The idea is to swap to spherical coordinates: $$\int_{|x| = \epsilon} \phi(x) \frac{e^{-m|x|}}{4 \pi |x|^2} d^3x = \frac{1}{4 \pi} \int_{S^2} \phi(x) \frac{e^{-m\epsilon}}{\epsilon^2} \epsilon^2 d\Omega = \frac{1}{4 \pi} \int_{S^2} \phi(x) {e^{-m\epsilon}} d\Omega $$ Now we wanna substitute $$y = x/|x|$$ $\frac{1}{4 \pi} \int_{S^2} \phi(x) {e^{-m\epsilon}} d\Omega = \frac{1}{4 \pi} \int_{S^2} \phi(\epsilon y) {e^{-m\epsilon}} d\Omega $ The last step is where I'm not sure what happens. Somehow the infinitesimal variable should change aswell, but I'm not quite sure how. Could somebody make that clear for me? Cheers!",,['calculus']
74,"Show that $ \lim_{(x,y) \to 0} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \alpha/\gamma + \beta/\delta > 1.$",Show that," \lim_{(x,y) \to 0} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \alpha/\gamma + \beta/\delta > 1.","Ted Shifrin on this site posed an interesting problem to me: show that $$ \lim_{(x,y) \to (0,0)} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \frac\alpha\gamma + \frac\beta\delta > 1, \,\,\,\,\text{where } \alpha, \beta, \gamma, \delta >0$$ I think I've got the $(\Leftarrow)$ direction as follows: assume WLOG that $\gamma \leq \delta$. Then switch to polar coordinates and get $$\frac {r^ {\alpha + \beta} (*) } {r^\gamma (1(*) + r^{\delta - \gamma}(*))},$$ where $(*)$ represents some trig stuff that is bounded near zero. Edit: I need to make sure that $(1(*) + r^{\delta - \gamma}(*))$ is bounded below here. The other direction is giving me some trouble. It seems we need a lower bound for the fraction (something to force to zero), and I'm not sure where to find one. In particular, I'm not sure what to do with the denominator. Any ideas?","Ted Shifrin on this site posed an interesting problem to me: show that $$ \lim_{(x,y) \to (0,0)} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \frac\alpha\gamma + \frac\beta\delta > 1, \,\,\,\,\text{where } \alpha, \beta, \gamma, \delta >0$$ I think I've got the $(\Leftarrow)$ direction as follows: assume WLOG that $\gamma \leq \delta$. Then switch to polar coordinates and get $$\frac {r^ {\alpha + \beta} (*) } {r^\gamma (1(*) + r^{\delta - \gamma}(*))},$$ where $(*)$ represents some trig stuff that is bounded near zero. Edit: I need to make sure that $(1(*) + r^{\delta - \gamma}(*))$ is bounded below here. The other direction is giving me some trouble. It seems we need a lower bound for the fraction (something to force to zero), and I'm not sure where to find one. In particular, I'm not sure what to do with the denominator. Any ideas?",,"['calculus', 'limits', 'multivariable-calculus']"
75,A question about differentiability,A question about differentiability,,"So, this is the last subject I have to study for my exam on Friday, and I still can not comprehend how to prove that a function is differentable. $$f(x,y) = \begin{cases} (x^2 + y^2) \cdot \sin \left(\frac{1}{\sqrt{x^2 + y^2}}\right) & \text{ if }(x,y) \neq (0,0),\\ 0& \text{ if }(x,y) = (0,0). \end{cases}$$   Is $f(x,y)$ differentiable at $(x,y) = (0,0)$? I know that I have to calculate the partial derivative according to the definition and then do another limit, but I do not know the formulas for these. I am very, very sorry for asking such a general question, but I am really in trouble and don't seem to understand it. I would appreciate very much a thorough answer.","So, this is the last subject I have to study for my exam on Friday, and I still can not comprehend how to prove that a function is differentable. $$f(x,y) = \begin{cases} (x^2 + y^2) \cdot \sin \left(\frac{1}{\sqrt{x^2 + y^2}}\right) & \text{ if }(x,y) \neq (0,0),\\ 0& \text{ if }(x,y) = (0,0). \end{cases}$$   Is $f(x,y)$ differentiable at $(x,y) = (0,0)$? I know that I have to calculate the partial derivative according to the definition and then do another limit, but I do not know the formulas for these. I am very, very sorry for asking such a general question, but I am really in trouble and don't seem to understand it. I would appreciate very much a thorough answer.",,"['calculus', 'multivariable-calculus', 'derivatives']"
76,Independence of Path and Fundamental Theorem of Calculus?,Independence of Path and Fundamental Theorem of Calculus?,,"a) Show that the given line integral is independent of path. How would you show this? Does this require assigning $C_1$ and $C_2$ two the two legs of the line? b) Then, evaluate the line integral I by finding a potential function f and applying the Fundamental Theorem of Line Integrals.     $$I= \int_{(0, 0)}^{(1, 2)} (x+y dx) + (x-y)dy$$ For b) my final answer was 5/2, but I'm not quite sure. Is this correct? If not, can you please show me how. Thank you in advance!","a) Show that the given line integral is independent of path. How would you show this? Does this require assigning $C_1$ and $C_2$ two the two legs of the line? b) Then, evaluate the line integral I by finding a potential function f and applying the Fundamental Theorem of Line Integrals.     $$I= \int_{(0, 0)}^{(1, 2)} (x+y dx) + (x-y)dy$$ For b) my final answer was 5/2, but I'm not quite sure. Is this correct? If not, can you please show me how. Thank you in advance!",,['multivariable-calculus']
77,Is the divergence of a gradient field the trace of the Hessian?,Is the divergence of a gradient field the trace of the Hessian?,,"Given a $C^2$ multivariate function $f : \mathbb{R}^d \to \mathbb{R}$, the gradient defines a vector field, the divergence of this vector field, then, should be the trace of the Hessian matrix, right?  I'm not entirely sure because the simplified version of divergence is only ever given for $d=3$.","Given a $C^2$ multivariate function $f : \mathbb{R}^d \to \mathbb{R}$, the gradient defines a vector field, the divergence of this vector field, then, should be the trace of the Hessian matrix, right?  I'm not entirely sure because the simplified version of divergence is only ever given for $d=3$.",,['multivariable-calculus']
78,How to check if a normal vector of a plane points towards or away from a certain point,How to check if a normal vector of a plane points towards or away from a certain point,,"The problem is as follows: I have a plane defined by three points. I also have a fourth point. I now want to calculate the normalvector of the plane defined by the first three points, but i want the Normalvector to point towards the side of the plane, where the fourth point is. My idea is to just calculate any normalvector of the plane, then drop a perpenicular from point four to the plane, and then check if the two vectors are parralell or antiparralell. Is the there a better solution without the extra step of droping the perpendicular?","The problem is as follows: I have a plane defined by three points. I also have a fourth point. I now want to calculate the normalvector of the plane defined by the first three points, but i want the Normalvector to point towards the side of the plane, where the fourth point is. My idea is to just calculate any normalvector of the plane, then drop a perpenicular from point four to the plane, and then check if the two vectors are parralell or antiparralell. Is the there a better solution without the extra step of droping the perpendicular?",,"['calculus', 'geometry', 'multivariable-calculus', 'orthonormal']"
79,Finding multiple integral on bounded area.,Finding multiple integral on bounded area.,,"Today I just learn on multiple integral. Somehow this question quite confusing. Find the area of the 1st quadrant region bounded by the curves y=$x^3$, y=2$x^3$ and x=$y^3$, x=4$y^3$ using subsitution method (Jacobbian method). Let y=$u$$x^3$ and x=$v$$y^3$ Could someone please help me some steps on how to do by changing the variables ?","Today I just learn on multiple integral. Somehow this question quite confusing. Find the area of the 1st quadrant region bounded by the curves y=$x^3$, y=2$x^3$ and x=$y^3$, x=4$y^3$ using subsitution method (Jacobbian method). Let y=$u$$x^3$ and x=$v$$y^3$ Could someone please help me some steps on how to do by changing the variables ?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
80,Reference for an integral's convergence on an $n$-ball when $n>2$.,Reference for an integral's convergence on an -ball when .,n n>2,"I was searching for a reference of a standard result from calculus. Unfortunately I couldn't find it. I  think that's mostly due to I am not familiar with any english calculus book. So I am searching for the following: For $$h=(h_1,\ldots,h_n)\in\mathbb{R}^n,$$  the integral $$\int_{\|\boldsymbol h\|<\epsilon} \frac{d\boldsymbol h}{\|\boldsymbol h\|^2}$$ is finite for $n>2$ and does not exist for $n=1,2$, where the norm is arbitrary. Does anyone knows a reference for this result? Can anyone recommend an English calculus book (by calculus I mean differentiation, integration, measure theory, etc)? Thank you very much, Analyst","I was searching for a reference of a standard result from calculus. Unfortunately I couldn't find it. I  think that's mostly due to I am not familiar with any english calculus book. So I am searching for the following: For $$h=(h_1,\ldots,h_n)\in\mathbb{R}^n,$$  the integral $$\int_{\|\boldsymbol h\|<\epsilon} \frac{d\boldsymbol h}{\|\boldsymbol h\|^2}$$ is finite for $n>2$ and does not exist for $n=1,2$, where the norm is arbitrary. Does anyone knows a reference for this result? Can anyone recommend an English calculus book (by calculus I mean differentiation, integration, measure theory, etc)? Thank you very much, Analyst",,"['reference-request', 'multivariable-calculus']"
81,How would one work this out (surface integral of a function),How would one work this out (surface integral of a function),,"Given the definition of the surface integral: $$Area=\iint_{S}{\mathbf{F}\cdot d\mathbf{S}} = \iint_{D}{f(\mathbf{r}(u,v))\cdot \left |\mathbf{r_u} \times \mathbf{r_v}  \right |dA}$$ Where $\mathbf{r}(u,v)$ is a vector function to describe $\mathbf{S}$. (Note the bold typesetting indicates vectors for F, r, S). $\mathbf{r}_u$ means the partial differential of r with respect to u. Now the question given is: Evaluate the surface integral $\iint_{S}{x^2z^2}dS$   S is the part of the cone $z^2 = x^2 + y^2$ that lies between the planes $z=1$ and $z = 2$ Now I tried this and came to a totally wrong solution. Looking up the solution in the solution manual gave me a different to approach the problem. The manual parameterized the function to $x,y$ by describing ""S is the part of the surface $z = \sqrt{x^2 + y^2}$ And then it used the standard approach of $$\iint_{S}{\mathbf{F}\cdot d\mathbf{S}} = \iint_{D}{f(x,y,g(x,y))\sqrt{\left(\frac{\partial z}{\partial x}\right)^2+\left(\frac{\partial z}{\partial x}\right)^2+1}dA}$$ Which results in $\frac{384\sqrt{2}}{3}\pi$ I tried a complete different approach in my work around. Instead of parameterizing the surface by simply stating ""z"" I tried to parameterize it using the fact that z is a cone: $$z = z, 1 < z < 3$$ $$x = \cos(\theta), 0 < \theta < 2\pi$$ $$y = \sin(\theta)$$ $$\mathbf{r}(z,\theta) = \cos(\theta)\mathbf{i}+\sin(\theta)\mathbf{j}+z\mathbf{k}$$ working the above problem out I could find $$\left |\mathbf{r_z} \times \mathbf{r_\theta}  \right | = \sqrt{ z^2\cos^2(\theta) + z^2\cos^2(\theta)} = z$$ $$f(\mathbf{r}(z,\theta)) = \cos^2(\theta)z^2$$ $$A=\int\limits_0^{2\pi}{\int\limits_1^3{\cos^2(\theta)z^3}dz}d\theta= \int\limits_0^{2\pi}{\cos^2(\theta)}d\theta \int\limits_1^3{z^3}dz$$ $$A = 20 \int\limits_0^{2\pi}{\tfrac{1}{2} + \tfrac{1}{2}\cos(2\theta)}d\theta$$ $$A=20\left(\pi+\left[ \sin(4\pi)-\sin(0)\pi\right]\right)=20\pi$$ Which is a total different solution. What did I do wrong, where did I take the wrong decision in this answer?","Given the definition of the surface integral: $$Area=\iint_{S}{\mathbf{F}\cdot d\mathbf{S}} = \iint_{D}{f(\mathbf{r}(u,v))\cdot \left |\mathbf{r_u} \times \mathbf{r_v}  \right |dA}$$ Where $\mathbf{r}(u,v)$ is a vector function to describe $\mathbf{S}$. (Note the bold typesetting indicates vectors for F, r, S). $\mathbf{r}_u$ means the partial differential of r with respect to u. Now the question given is: Evaluate the surface integral $\iint_{S}{x^2z^2}dS$   S is the part of the cone $z^2 = x^2 + y^2$ that lies between the planes $z=1$ and $z = 2$ Now I tried this and came to a totally wrong solution. Looking up the solution in the solution manual gave me a different to approach the problem. The manual parameterized the function to $x,y$ by describing ""S is the part of the surface $z = \sqrt{x^2 + y^2}$ And then it used the standard approach of $$\iint_{S}{\mathbf{F}\cdot d\mathbf{S}} = \iint_{D}{f(x,y,g(x,y))\sqrt{\left(\frac{\partial z}{\partial x}\right)^2+\left(\frac{\partial z}{\partial x}\right)^2+1}dA}$$ Which results in $\frac{384\sqrt{2}}{3}\pi$ I tried a complete different approach in my work around. Instead of parameterizing the surface by simply stating ""z"" I tried to parameterize it using the fact that z is a cone: $$z = z, 1 < z < 3$$ $$x = \cos(\theta), 0 < \theta < 2\pi$$ $$y = \sin(\theta)$$ $$\mathbf{r}(z,\theta) = \cos(\theta)\mathbf{i}+\sin(\theta)\mathbf{j}+z\mathbf{k}$$ working the above problem out I could find $$\left |\mathbf{r_z} \times \mathbf{r_\theta}  \right | = \sqrt{ z^2\cos^2(\theta) + z^2\cos^2(\theta)} = z$$ $$f(\mathbf{r}(z,\theta)) = \cos^2(\theta)z^2$$ $$A=\int\limits_0^{2\pi}{\int\limits_1^3{\cos^2(\theta)z^3}dz}d\theta= \int\limits_0^{2\pi}{\cos^2(\theta)}d\theta \int\limits_1^3{z^3}dz$$ $$A = 20 \int\limits_0^{2\pi}{\tfrac{1}{2} + \tfrac{1}{2}\cos(2\theta)}d\theta$$ $$A=20\left(\pi+\left[ \sin(4\pi)-\sin(0)\pi\right]\right)=20\pi$$ Which is a total different solution. What did I do wrong, where did I take the wrong decision in this answer?",,"['integration', 'multivariable-calculus']"
82,"For given $p$, a regular mapping $F:M \to N$ of surfaces can be diffeomorphism","For given , a regular mapping  of surfaces can be diffeomorphism",p F:M \to N,"Want to show : For given point $p$ of M, a regular mapping $F:M \to N$ of surfaces has a neighborhood $U$ such that $F|_{U}$ is a diffeomorphism of $U$ onto a neighborhood of $F(p)$ in N. I learned that $f$ is diffeomorphism $\iff$ $f^{-1}$ is differentiable That regular mapping $f$: $M$ onto $N$ is one-to-one implies that $f$ is diffeomorphism. I want to show that there exists open set $U$ containing $p$ such that $F|_{U}$ is one-to-one beacuse we know that $F$ is regular. Thanks in advance.","Want to show : For given point $p$ of M, a regular mapping $F:M \to N$ of surfaces has a neighborhood $U$ such that $F|_{U}$ is a diffeomorphism of $U$ onto a neighborhood of $F(p)$ in N. I learned that $f$ is diffeomorphism $\iff$ $f^{-1}$ is differentiable That regular mapping $f$: $M$ onto $N$ is one-to-one implies that $f$ is diffeomorphism. I want to show that there exists open set $U$ containing $p$ such that $F|_{U}$ is one-to-one beacuse we know that $F$ is regular. Thanks in advance.",,"['analysis', 'differential-geometry', 'multivariable-calculus']"
83,Limit of functions with 2 variables,Limit of functions with 2 variables,,"Does the limit of $\frac{x^2+y^2}{|x|+|y|}$ as $(x,y) \to (0,0)$ exists? For me it exists because $\frac{x^2+y^2}{|x|+|y|}$ < $\frac{x^2+y^2+2|xy|}{|x|+|y|}$ = $\frac{(|x|+|y|)^2}{|x|+|y|}$  = $ |x|+|y| $ that goes to $0$. But, according to wolframalpha , this limits does not exists. Where I am wrong? Thanks.","Does the limit of $\frac{x^2+y^2}{|x|+|y|}$ as $(x,y) \to (0,0)$ exists? For me it exists because $\frac{x^2+y^2}{|x|+|y|}$ < $\frac{x^2+y^2+2|xy|}{|x|+|y|}$ = $\frac{(|x|+|y|)^2}{|x|+|y|}$  = $ |x|+|y| $ that goes to $0$. But, according to wolframalpha , this limits does not exists. Where I am wrong? Thanks.",,"['limits', 'multivariable-calculus']"
84,Area of Questionably Generated Manifold,Area of Questionably Generated Manifold,,"I might not possess the language to ask this question, but I'm going to try anyway. Consider a path c ( t ) : $\mathbb{R}\rightarrow \mathbb{R}^n$.  Let c '( t ) denote the tangent vector of the path c ( t ).  Can we ""stitch"" together the tangent vectors given by evaluating c '( t ) at each point along the path such that a smooth manifold is generated?  Does this only work for simply connected closed paths?  Rather, what are the constraints for a path that can potentially serve as a substructure for such a manifold?  Can we meaningfully evaluate the area of this manifold?  Is this manifold 1-dimensional?  Are such manifolds unique to their paths?  Can there exist a manifold for one orientation of the path but not the other?  What properties would need to hold for the reverse orientation to generate a manifold of equivalent area to the standard orientation's manifold?  How can we classify this object?  Are there any applications for this? I will award the answer to whoever covers each question and provides both an interesting example of a path and a counterexample to the idea that any path can have such a manifold. I guess the simplest example of a substructure would be c ( $\theta$ ) : $\mathbb{R}\rightarrow \mathbb{R}^2$ given by c ( $\theta$ ) = < $\cos\theta$, $\sin\theta$ >.  Would the area of the manifold generated be acquired using the integral $\int_{0}^{2\pi}\int_{1}^{\sqrt2} rdrd\theta$  ?  I used intuition about the boundary and basic geometry to obtain this result. I may revise my questions after I get some responses and learn better language to approach this problem with.","I might not possess the language to ask this question, but I'm going to try anyway. Consider a path c ( t ) : $\mathbb{R}\rightarrow \mathbb{R}^n$.  Let c '( t ) denote the tangent vector of the path c ( t ).  Can we ""stitch"" together the tangent vectors given by evaluating c '( t ) at each point along the path such that a smooth manifold is generated?  Does this only work for simply connected closed paths?  Rather, what are the constraints for a path that can potentially serve as a substructure for such a manifold?  Can we meaningfully evaluate the area of this manifold?  Is this manifold 1-dimensional?  Are such manifolds unique to their paths?  Can there exist a manifold for one orientation of the path but not the other?  What properties would need to hold for the reverse orientation to generate a manifold of equivalent area to the standard orientation's manifold?  How can we classify this object?  Are there any applications for this? I will award the answer to whoever covers each question and provides both an interesting example of a path and a counterexample to the idea that any path can have such a manifold. I guess the simplest example of a substructure would be c ( $\theta$ ) : $\mathbb{R}\rightarrow \mathbb{R}^2$ given by c ( $\theta$ ) = < $\cos\theta$, $\sin\theta$ >.  Would the area of the manifold generated be acquired using the integral $\int_{0}^{2\pi}\int_{1}^{\sqrt2} rdrd\theta$  ?  I used intuition about the boundary and basic geometry to obtain this result. I may revise my questions after I get some responses and learn better language to approach this problem with.",,"['calculus', 'real-analysis', 'general-topology', 'geometry', 'multivariable-calculus']"
85,find equation of a plane that is 3d,find equation of a plane that is 3d,,"how to find equation of a plane passing through the point $(5, 6, 8)$ and parallel to the $xz$-plane. I know you can use the equation $a(x-x_0)+ b(y-y_0)+ c(z-z_0)=0$. Do i just plug in and thats it?","how to find equation of a plane passing through the point $(5, 6, 8)$ and parallel to the $xz$-plane. I know you can use the equation $a(x-x_0)+ b(y-y_0)+ c(z-z_0)=0$. Do i just plug in and thats it?",,['multivariable-calculus']
86,Is it possible for a function with range $=R$ have a global max/min without specifying a region?,Is it possible for a function with range  have a global max/min without specifying a region?,=R,"Let $f(x,y,z) = x^2 + y^2 + z^2$ The gradient of $f$ is $\nabla f=(2x, 2y, 2z)$ and if I solve the 3-equations-system, I will find the critical point $P_0=(0,0,0)$ The Hessian matrix of $f$ is $\nabla^2 f= \begin{bmatrix}         2 & 0 & 0 \\         0 & 2 & 0 \\         0 & 0 & 2 \\ \end{bmatrix} $ For $P_0$, $\alpha_1 = det \begin{bmatrix} 2\\ \end{bmatrix} = 2 \gt 0 $, $\alpha_2 = \det \begin{bmatrix} 2 & 0\\ 0 & 2\\ \end{bmatrix} = 4 \gt 0 $, $\alpha_3 = \det \begin{bmatrix} 2 & 0 & 0\\ 0 & 2 & 0\\ 0 & 0 & 2 \end{bmatrix} = 8 \gt 0 $ So $P_0$ is a local minimum and since $f$ is bounded below by $0$, $f(0,0,0) = 0$ is also an global minimum of $f$. Now, let $g(x,y) = 4 + x^2 + y^3 - 3xy$ The gradient of $g$ is $\nabla g=(2x-3y, 3y^2-3x)$ and if I solve the 2-equations-system, I will find the critical points $P_0(0,0) \text{ and } P_1\left(\frac94, \frac32\right)$ The Hessian matrix of $g$ is $\nabla^2 g= \begin{bmatrix}         2 & -3 \\         -3 & 6y \\ \end{bmatrix} $ For $P_0$, $\alpha_1 = det \begin{bmatrix} 2\\ \end{bmatrix} = 2 \gt 0 $, $\alpha_2 = \det \begin{bmatrix} 2 & -3\\ -3 & 0\\ \end{bmatrix} = -9 \lt 0 \mathbf{\text{ saddle point }} $ And for $P_1$, $\alpha_1 = det \begin{bmatrix} 2\\ \end{bmatrix} = 2 \gt 0 $, $\alpha_2 = \det \begin{bmatrix} 2 & -3\\ -3 & 9\\ \end{bmatrix} = 9 \lt 0 \mathbf{\text{ local minimum }} $ I also found $g(0,0) = \mathbf{4}$ and  $g\left(\frac94, \frac32\right) \approx \mathbf{2,313}$. My question is, can I affirm that $g$ does not have a global max/min? And also, will be always the case for a function with range $=R$ when I'm looking for a global max/min without specifying a region? Thanks in advance","Let $f(x,y,z) = x^2 + y^2 + z^2$ The gradient of $f$ is $\nabla f=(2x, 2y, 2z)$ and if I solve the 3-equations-system, I will find the critical point $P_0=(0,0,0)$ The Hessian matrix of $f$ is $\nabla^2 f= \begin{bmatrix}         2 & 0 & 0 \\         0 & 2 & 0 \\         0 & 0 & 2 \\ \end{bmatrix} $ For $P_0$, $\alpha_1 = det \begin{bmatrix} 2\\ \end{bmatrix} = 2 \gt 0 $, $\alpha_2 = \det \begin{bmatrix} 2 & 0\\ 0 & 2\\ \end{bmatrix} = 4 \gt 0 $, $\alpha_3 = \det \begin{bmatrix} 2 & 0 & 0\\ 0 & 2 & 0\\ 0 & 0 & 2 \end{bmatrix} = 8 \gt 0 $ So $P_0$ is a local minimum and since $f$ is bounded below by $0$, $f(0,0,0) = 0$ is also an global minimum of $f$. Now, let $g(x,y) = 4 + x^2 + y^3 - 3xy$ The gradient of $g$ is $\nabla g=(2x-3y, 3y^2-3x)$ and if I solve the 2-equations-system, I will find the critical points $P_0(0,0) \text{ and } P_1\left(\frac94, \frac32\right)$ The Hessian matrix of $g$ is $\nabla^2 g= \begin{bmatrix}         2 & -3 \\         -3 & 6y \\ \end{bmatrix} $ For $P_0$, $\alpha_1 = det \begin{bmatrix} 2\\ \end{bmatrix} = 2 \gt 0 $, $\alpha_2 = \det \begin{bmatrix} 2 & -3\\ -3 & 0\\ \end{bmatrix} = -9 \lt 0 \mathbf{\text{ saddle point }} $ And for $P_1$, $\alpha_1 = det \begin{bmatrix} 2\\ \end{bmatrix} = 2 \gt 0 $, $\alpha_2 = \det \begin{bmatrix} 2 & -3\\ -3 & 9\\ \end{bmatrix} = 9 \lt 0 \mathbf{\text{ local minimum }} $ I also found $g(0,0) = \mathbf{4}$ and  $g\left(\frac94, \frac32\right) \approx \mathbf{2,313}$. My question is, can I affirm that $g$ does not have a global max/min? And also, will be always the case for a function with range $=R$ when I'm looking for a global max/min without specifying a region? Thanks in advance",,"['calculus', 'multivariable-calculus']"
87,"Find the area: $(\frac xa+\frac yb)^2 = \frac xa-\frac yb , y=0 , b>a$",Find the area:,"(\frac xa+\frac yb)^2 = \frac xa-\frac yb , y=0 , b>a","Find the area: $$\left(\frac xa+\frac yb\right)^2 = \frac xa-\frac yb,$$ $ y=0 , b>a$ I work in spherical coordinates.  $x=a\cdot r\cdot \cos(\phi)\;\;,$ $y=b\cdot r\cdot \cos(\phi)$ Then I get the equation and don't know to do with, cause ""a"" and ""b"" are dissapearing. For what are the conditions: $y=0, b>a?$..How to define the limits of integration?","Find the area: $$\left(\frac xa+\frac yb\right)^2 = \frac xa-\frac yb,$$ $ y=0 , b>a$ I work in spherical coordinates.  $x=a\cdot r\cdot \cos(\phi)\;\;,$ $y=b\cdot r\cdot \cos(\phi)$ Then I get the equation and don't know to do with, cause ""a"" and ""b"" are dissapearing. For what are the conditions: $y=0, b>a?$..How to define the limits of integration?",,"['integration', 'multivariable-calculus']"
88,Application of Green's Theorem,Application of Green's Theorem,,"I know this is a really basic question, but I seem to be kind of rusty. $C$ is the boundary of the circle $x^2+y^2=4$ $$\int_C y^3dx-x^3 dy = \int_A -3x^2-3y^2 dA= \int_0^{2 \pi} \int_0^2 -3 r^2 r dr d \theta = -12 \pi$$ Did I make a mistake? My book says it's $-24 \pi$","I know this is a really basic question, but I seem to be kind of rusty. $C$ is the boundary of the circle $x^2+y^2=4$ $$\int_C y^3dx-x^3 dy = \int_A -3x^2-3y^2 dA= \int_0^{2 \pi} \int_0^2 -3 r^2 r dr d \theta = -12 \pi$$ Did I make a mistake? My book says it's $-24 \pi$",,['multivariable-calculus']
89,Surface integration over section of paraboloid below a plane,Surface integration over section of paraboloid below a plane,,"Let S be the finite portion of the surface $z = 4x^2 + y^2$ cut off by the plane  $z = 8x + 4y - 4$. Evaluate the surface integral $(x, y, 3z)\,dS$ over the region $S$ where the normal to $S$ points upwards. I can do this using the divergence theorem, but I don't know what the limits would be for the surface integral. If I parametrise the surface as $r=(\frac{1}{2}r\cos t, r\sin t, r^2)$, then $0\leq t < 2\pi$. But if I substitute the parametrised coordinates into the plane equation then I get $r^2 \leq 4r\cos t + 4r\sin t - 4$. Is this right?","Let S be the finite portion of the surface $z = 4x^2 + y^2$ cut off by the plane  $z = 8x + 4y - 4$. Evaluate the surface integral $(x, y, 3z)\,dS$ over the region $S$ where the normal to $S$ points upwards. I can do this using the divergence theorem, but I don't know what the limits would be for the surface integral. If I parametrise the surface as $r=(\frac{1}{2}r\cos t, r\sin t, r^2)$, then $0\leq t < 2\pi$. But if I substitute the parametrised coordinates into the plane equation then I get $r^2 \leq 4r\cos t + 4r\sin t - 4$. Is this right?",,['multivariable-calculus']
90,Rotating the gradient,Rotating the gradient,,Suppose I have a triangle T in 3dimensional space and i want to rotate it in arbitrary ways. The coordinates for T are given by $f: T_R \in \mathbb{R}^2 \rightarrow T \in \mathbb{R}^3 $ where $T_R$ is a flat reference triangle. The gradient $\nabla f$ is then given by a $3\times 2$-matrix. How can I compute the gradient of the triangle (from the old gradient) after a rotation (lets say a rotation with angle 30 deg around the z-axis)? Thank you!,Suppose I have a triangle T in 3dimensional space and i want to rotate it in arbitrary ways. The coordinates for T are given by $f: T_R \in \mathbb{R}^2 \rightarrow T \in \mathbb{R}^3 $ where $T_R$ is a flat reference triangle. The gradient $\nabla f$ is then given by a $3\times 2$-matrix. How can I compute the gradient of the triangle (from the old gradient) after a rotation (lets say a rotation with angle 30 deg around the z-axis)? Thank you!,,"['matrices', 'multivariable-calculus']"
91,Find gradient of this implicit function,Find gradient of this implicit function,,How to find a gradient of this implicit function? $$ xz+yz^2-3xy-3=0 $$,How to find a gradient of this implicit function? $$ xz+yz^2-3xy-3=0 $$,,"['calculus', 'multivariable-calculus', 'derivatives']"
92,A continuum of critical points in multi-variable calculus,A continuum of critical points in multi-variable calculus,,"In multivariable calculus, we learn to find local extrema by identifying the critical points, and deciding (using the second derivative test, or otherwise) the type of the point - a local max, a local min, or a saddle point. What if we get a continuum of critical points ? say the gradient of $f(x,y)$ vanishes along a curve in the $xy$ plane. Is this situation possible ? what can we say about classifying these points into min/max/saddle ?","In multivariable calculus, we learn to find local extrema by identifying the critical points, and deciding (using the second derivative test, or otherwise) the type of the point - a local max, a local min, or a saddle point. What if we get a continuum of critical points ? say the gradient of $f(x,y)$ vanishes along a curve in the $xy$ plane. Is this situation possible ? what can we say about classifying these points into min/max/saddle ?",,['multivariable-calculus']
93,Evaluating $\iint_s \vec F \cdot \hat n ds $ around the curved surface of cylinder cut by plane at $45^\circ $,Evaluating  around the curved surface of cylinder cut by plane at,\iint_s \vec F \cdot \hat n ds  45^\circ ,"I need to calculate the surface integral of $F(x,y,z) = \hat i x +\hat j y + \hat k z$ on the curved part of surface $x^2+z^2 = 1, x+y=2, $ and $y$ goes from $1$ to $3$ as shown in following figure. How do I evaluate $\displaystyle \iint_S \vec F .\hat n ds$  this surface? EDIT ::I couldn't do it via parametrization, I got the above figure which is incorrect. Using this formula $\iint_s \vec F \cdot \frac{\nabla \phi }{|\nabla \phi|}\sqrt{1 + (z_x)^2 + (z_y)^2} dx dy$ I got the following. Not sure if it's correct.  $$\int_1^3 \;dy \int_{-1}^{2-y} \vec F(x, y , \sqrt{1-x^2})\cdot \frac{x \hat i + \sqrt{1-x^2}\hat k}{\sqrt{1-x^2}} dx  \\ + \int_1^3 \;dy \int_{-1}^{2-y} \vec F(x, y , -\sqrt{1-x^2})\cdot \frac{x \hat i - \sqrt{1-x^2}\hat k}{\sqrt{1-x^2}} dx$$","I need to calculate the surface integral of $F(x,y,z) = \hat i x +\hat j y + \hat k z$ on the curved part of surface $x^2+z^2 = 1, x+y=2, $ and $y$ goes from $1$ to $3$ as shown in following figure. How do I evaluate $\displaystyle \iint_S \vec F .\hat n ds$  this surface? EDIT ::I couldn't do it via parametrization, I got the above figure which is incorrect. Using this formula $\iint_s \vec F \cdot \frac{\nabla \phi }{|\nabla \phi|}\sqrt{1 + (z_x)^2 + (z_y)^2} dx dy$ I got the following. Not sure if it's correct.  $$\int_1^3 \;dy \int_{-1}^{2-y} \vec F(x, y , \sqrt{1-x^2})\cdot \frac{x \hat i + \sqrt{1-x^2}\hat k}{\sqrt{1-x^2}} dx  \\ + \int_1^3 \;dy \int_{-1}^{2-y} \vec F(x, y , -\sqrt{1-x^2})\cdot \frac{x \hat i - \sqrt{1-x^2}\hat k}{\sqrt{1-x^2}} dx$$",,['multivariable-calculus']
94,Sum separable partial differential equation over general domain,Sum separable partial differential equation over general domain,,"First off, thank you for taking your time out and considering my question. I just have a (rather) general question on partial differential equations. So consider the following. Let $(x,y) \mapsto u(x,y)$ be some sufficiently smooth real-valued function over a rather general domain $\mathcal{F} \subset R^2$. And using subscripts to denote partial differentiation, suppose that,  $$ u_x(x,y) = g_1(x) \\ u_y(x,y) = g_2(y)   $$ In particular, note that $g_1$ is only a function of $x$ and $g_2$ is only a function of $y$. Now, if the domain $\mathcal{F}$ were rectangular in the form $[a,b] \times [c,d]$, then using the usual fundamental theorem of calculus, we would obtain $u(x,y) = \int_a^x g_1(\eta)d\eta + \int_c^y g_2(\eta) d\eta$. But what about the case when $\mathcal{F}$ is rather general? That is, it could be connected, simply-connected, etc.? I can still see how $u(x,y)$ would have a similar separably additive form as above, but what about the region of integration? That is, do we have to consider, for each point $(x,y) \in \mathcal{F}$, the $x$-section $\mathcal{F}_y$ and $y$-section $\mathcal{F}_x$ of the domain $\mathcal{F}$ and integrate? Would it have the form like,  $$ u(x,y) = \int_{\mathcal{F}_y} g_1(\eta) d\eta + \int_{\mathcal{F}_x} g_2(\eta) d\eta $$ (and plus some constant of integration if it needs be)? And if it is indeed this form, to recover back the partial derivatives, we can't use the typical fundamental theorem of calculus, but we need to consider the Lebesgue differentiation theorem, correct? Any help would be highly appreciated! Thank you! PS. I'm aware that such sum-separable differential form can, via some change of variables, be rewritten into the wave equation. However, the traditional wave equation in $(t,x)$ where $x \in R^1$ allows for a very general consideration in the spatial coordinate $x$ but usually takes the time coordinate $t$ to be only the positive real line. But in my case above, this is not the case.","First off, thank you for taking your time out and considering my question. I just have a (rather) general question on partial differential equations. So consider the following. Let $(x,y) \mapsto u(x,y)$ be some sufficiently smooth real-valued function over a rather general domain $\mathcal{F} \subset R^2$. And using subscripts to denote partial differentiation, suppose that,  $$ u_x(x,y) = g_1(x) \\ u_y(x,y) = g_2(y)   $$ In particular, note that $g_1$ is only a function of $x$ and $g_2$ is only a function of $y$. Now, if the domain $\mathcal{F}$ were rectangular in the form $[a,b] \times [c,d]$, then using the usual fundamental theorem of calculus, we would obtain $u(x,y) = \int_a^x g_1(\eta)d\eta + \int_c^y g_2(\eta) d\eta$. But what about the case when $\mathcal{F}$ is rather general? That is, it could be connected, simply-connected, etc.? I can still see how $u(x,y)$ would have a similar separably additive form as above, but what about the region of integration? That is, do we have to consider, for each point $(x,y) \in \mathcal{F}$, the $x$-section $\mathcal{F}_y$ and $y$-section $\mathcal{F}_x$ of the domain $\mathcal{F}$ and integrate? Would it have the form like,  $$ u(x,y) = \int_{\mathcal{F}_y} g_1(\eta) d\eta + \int_{\mathcal{F}_x} g_2(\eta) d\eta $$ (and plus some constant of integration if it needs be)? And if it is indeed this form, to recover back the partial derivatives, we can't use the typical fundamental theorem of calculus, but we need to consider the Lebesgue differentiation theorem, correct? Any help would be highly appreciated! Thank you! PS. I'm aware that such sum-separable differential form can, via some change of variables, be rewritten into the wave equation. However, the traditional wave equation in $(t,x)$ where $x \in R^1$ allows for a very general consideration in the spatial coordinate $x$ but usually takes the time coordinate $t$ to be only the positive real line. But in my case above, this is not the case.",,"['multivariable-calculus', 'partial-differential-equations']"
95,"If $f:U\to \mathbb{R}$ is continuous and $(x^2+y^4)f(x,y) + (f(x,y))^3=1$, then $f$ is $C^\infty$","If  is continuous and , then  is","f:U\to \mathbb{R} (x^2+y^4)f(x,y) + (f(x,y))^3=1 f C^\infty","Let $f:U\to \mathbb{R}$ be continuous in $U \subset\mathbb{R}^2$, such that   $$(x^2+y^4)f(x,y) + (f(x,y))^3=1$$   for all $(x,y) \in U$. Prove that $f\in C^{\infty}$. I'm learning the implicit function theorem, I read it, and its proof, and I thought I had understood it. But when I face a problem like this I just don't know how to use it yet. If someone could help me I would appreciate it.","Let $f:U\to \mathbb{R}$ be continuous in $U \subset\mathbb{R}^2$, such that   $$(x^2+y^4)f(x,y) + (f(x,y))^3=1$$   for all $(x,y) \in U$. Prove that $f\in C^{\infty}$. I'm learning the implicit function theorem, I read it, and its proof, and I thought I had understood it. But when I face a problem like this I just don't know how to use it yet. If someone could help me I would appreciate it.",,"['functions', 'multivariable-calculus', 'implicit-differentiation']"
96,"Neumann problem, stuck on a boundary condition.","Neumann problem, stuck on a boundary condition.",,"I am stuck on a problem that I am trying for exam practice and I would very much appreciate a hint to help me out, here is the section where I am stuck: A solution is sought to the Neumann problem for $\nabla^2 u = 0$ in the half plane $z > 0$:   $u = O(|x|^{−a}), \frac{\partial u}{\partial r} = O(|x|^{−a−1}) ~~ \mathrm{as} ~~  |x| → ∞,~~ \frac{\partial u}{\partial z} ~ = p(x, y) ~ on ~ z = 0, \mathrm{where}~ a > 0$. It is assumed that   $\int_{\infty}^{\infty}\int_{-\infty}^{\infty} p(x, y) dx dy = 0$. Explain why this condition is   necessary. My feeling is that this is to do with Green's third identity and that we need the normal derivative in the $x-y$ plane to be integrable in order to find out solution with a Green's function, am I correct? EDIT: the divergence theorem sorts this out.","I am stuck on a problem that I am trying for exam practice and I would very much appreciate a hint to help me out, here is the section where I am stuck: A solution is sought to the Neumann problem for $\nabla^2 u = 0$ in the half plane $z > 0$:   $u = O(|x|^{−a}), \frac{\partial u}{\partial r} = O(|x|^{−a−1}) ~~ \mathrm{as} ~~  |x| → ∞,~~ \frac{\partial u}{\partial z} ~ = p(x, y) ~ on ~ z = 0, \mathrm{where}~ a > 0$. It is assumed that   $\int_{\infty}^{\infty}\int_{-\infty}^{\infty} p(x, y) dx dy = 0$. Explain why this condition is   necessary. My feeling is that this is to do with Green's third identity and that we need the normal derivative in the $x-y$ plane to be integrable in order to find out solution with a Green's function, am I correct? EDIT: the divergence theorem sorts this out.",,['multivariable-calculus']
97,Double solid angle integration with integrand only dependent on relative angle,Double solid angle integration with integrand only dependent on relative angle,,"Suppose one has an integral of the following form, $$ \int \text{d} \Omega_{1} \text{d} \Omega_{2} f(\gamma). $$ Where gamma is the relative angle between $(\theta_1, \phi_1)$ and $(\theta_2, \phi_2)$, $$ \cos \gamma = \cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2 \cos(\phi_1-\phi_2) $$ Based on symmetry arguments one could argue that as the integrand is only dependent the relative angle, we can fix $\Omega_1$, integrate over $\Omega_2$, and multiply the result with $4 \pi$ to compensate for fixing $\Omega_1$. Additionally we can argue that if we fix $\Omega_1$ along the $z$ axis the integration over $\phi_2$ just gives a factor $2 \pi$. So we have, $$ \int \text{d} \Omega_{1} \text{d} \Omega_{2} f(\gamma) = 8 \pi^{2} \int \sin \gamma \, \text{d}\gamma f(\gamma).  $$ However I fail to make an explicit mathematical derivation of the above reasoning. If I try to write the solid angle differentials in terms of $\gamma$ i get quite lengthy and ugly looking expressions.","Suppose one has an integral of the following form, $$ \int \text{d} \Omega_{1} \text{d} \Omega_{2} f(\gamma). $$ Where gamma is the relative angle between $(\theta_1, \phi_1)$ and $(\theta_2, \phi_2)$, $$ \cos \gamma = \cos \theta_1 \cos \theta_2 + \sin \theta_1 \sin \theta_2 \cos(\phi_1-\phi_2) $$ Based on symmetry arguments one could argue that as the integrand is only dependent the relative angle, we can fix $\Omega_1$, integrate over $\Omega_2$, and multiply the result with $4 \pi$ to compensate for fixing $\Omega_1$. Additionally we can argue that if we fix $\Omega_1$ along the $z$ axis the integration over $\phi_2$ just gives a factor $2 \pi$. So we have, $$ \int \text{d} \Omega_{1} \text{d} \Omega_{2} f(\gamma) = 8 \pi^{2} \int \sin \gamma \, \text{d}\gamma f(\gamma).  $$ However I fail to make an explicit mathematical derivation of the above reasoning. If I try to write the solid angle differentials in terms of $\gamma$ i get quite lengthy and ugly looking expressions.",,['multivariable-calculus']
98,How to compute the second derivatives?,How to compute the second derivatives?,,"Motivation: In isogeometric analysis, state variables(e.g. displacement) are defined in the parametric domain, which can be mapped to the physical domain by $\boldsymbol{\xi}\mapsto \boldsymbol{x}$ as shown beneath. However the quantity related to displacment such as stress, strain are spacial derivatives of displacement. The following procedure is commonly used for solving those derivatives. $\blacksquare$ Know \begin{equation} u(\xi,\eta) = \sum_{i} c_iN^i(\xi,\eta)  \end{equation} with  $$x(\xi,\eta) = \sum_{i} x_i N^i(\xi,\eta), \quad y(\xi,\eta) = \sum_{i} y_i N^i(\xi,\eta),$$ where $c_i,x_i,y_i$ are constants, with assumption that $(\xi,\eta)\mapsto(x,y)$ is bijective, i.e. inverse exists, $$J :=[\frac{\partial x_i}{\partial \xi_j}],\: |J| \neq 0\quad (\text{where }x_2 = y,\,\xi_2 = \eta).$$ By chain rule,  $$\frac{\partial u}{\partial \xi_j} = \frac{\partial u}{\partial x_i}\frac{\partial x_i}{\partial \xi_j}$$ or $$ \begin{bmatrix} \frac{\partial u}{\partial \xi}\\ \frac{\partial u}{\partial \eta} \end{bmatrix} = \begin{bmatrix} \frac{\partial x}{\partial \xi} & \frac{\partial y}{\partial \xi}\\ \frac{\partial x}{\partial \eta} & \frac{\partial y}{\partial \eta} \end{bmatrix} \begin{bmatrix} \frac{\partial u}{\partial x}\\ \frac{\partial u}{\partial y} \end{bmatrix}  = J^T \begin{bmatrix} \frac{\partial u}{\partial x}\\ \frac{\partial u}{\partial y} \end{bmatrix}. $$ Hence, $$ \begin{bmatrix} \frac{\partial u}{\partial x}\\ \frac{\partial u}{\partial y} \end{bmatrix} = (J^T)^{-1} \begin{bmatrix} \frac{\partial u}{\partial \xi}\\ \frac{\partial u}{\partial \eta} \end{bmatrix} $$ However, now I need to compute the second derivatives $$\frac{\partial^2 u}{\partial x_i\partial x_j},\quad i \text{ and }j \in \{1,2\}.$$ One may think displacement $\boldsymbol{u} = [u, v]^T$, then the goal is to compute $$\nabla(\nabla\boldsymbol{u})$$ with information as above.","Motivation: In isogeometric analysis, state variables(e.g. displacement) are defined in the parametric domain, which can be mapped to the physical domain by $\boldsymbol{\xi}\mapsto \boldsymbol{x}$ as shown beneath. However the quantity related to displacment such as stress, strain are spacial derivatives of displacement. The following procedure is commonly used for solving those derivatives. $\blacksquare$ Know \begin{equation} u(\xi,\eta) = \sum_{i} c_iN^i(\xi,\eta)  \end{equation} with  $$x(\xi,\eta) = \sum_{i} x_i N^i(\xi,\eta), \quad y(\xi,\eta) = \sum_{i} y_i N^i(\xi,\eta),$$ where $c_i,x_i,y_i$ are constants, with assumption that $(\xi,\eta)\mapsto(x,y)$ is bijective, i.e. inverse exists, $$J :=[\frac{\partial x_i}{\partial \xi_j}],\: |J| \neq 0\quad (\text{where }x_2 = y,\,\xi_2 = \eta).$$ By chain rule,  $$\frac{\partial u}{\partial \xi_j} = \frac{\partial u}{\partial x_i}\frac{\partial x_i}{\partial \xi_j}$$ or $$ \begin{bmatrix} \frac{\partial u}{\partial \xi}\\ \frac{\partial u}{\partial \eta} \end{bmatrix} = \begin{bmatrix} \frac{\partial x}{\partial \xi} & \frac{\partial y}{\partial \xi}\\ \frac{\partial x}{\partial \eta} & \frac{\partial y}{\partial \eta} \end{bmatrix} \begin{bmatrix} \frac{\partial u}{\partial x}\\ \frac{\partial u}{\partial y} \end{bmatrix}  = J^T \begin{bmatrix} \frac{\partial u}{\partial x}\\ \frac{\partial u}{\partial y} \end{bmatrix}. $$ Hence, $$ \begin{bmatrix} \frac{\partial u}{\partial x}\\ \frac{\partial u}{\partial y} \end{bmatrix} = (J^T)^{-1} \begin{bmatrix} \frac{\partial u}{\partial \xi}\\ \frac{\partial u}{\partial \eta} \end{bmatrix} $$ However, now I need to compute the second derivatives $$\frac{\partial^2 u}{\partial x_i\partial x_j},\quad i \text{ and }j \in \{1,2\}.$$ One may think displacement $\boldsymbol{u} = [u, v]^T$, then the goal is to compute $$\nabla(\nabla\boldsymbol{u})$$ with information as above.",,"['multivariable-calculus', 'numerical-methods']"
99,When is it valid for me to just integrate a trig function?,When is it valid for me to just integrate a trig function?,,"I am having a problem identifying when I need to use some kind of integration technique or am I just over complicating things. Could someone please explain to me when I need to or not? Normally, I would just integrate as normal when the problem is like this $$\int \cos(x) dx = \sin(x) + c$$ But when things become like the following, I don't have rules to fall back on and just rely on memory and experience of solving problems. $$\iiint \cos(x+y+z) dz dx dy$$ $$\int \sin(2x) dx$$","I am having a problem identifying when I need to use some kind of integration technique or am I just over complicating things. Could someone please explain to me when I need to or not? Normally, I would just integrate as normal when the problem is like this $$\int \cos(x) dx = \sin(x) + c$$ But when things become like the following, I don't have rules to fall back on and just rely on memory and experience of solving problems. $$\iiint \cos(x+y+z) dz dx dy$$ $$\int \sin(2x) dx$$",,"['multivariable-calculus', 'integration']"
