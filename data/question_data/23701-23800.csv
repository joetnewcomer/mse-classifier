,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Are imaginary numbers also classified into algebraic and non-algebraic numbers, just as real numbers?","Are imaginary numbers also classified into algebraic and non-algebraic numbers, just as real numbers?",,I know that real numbers can be algebraic or non-algebraic (if they cannot be the solution to an algebraic equation). Are imaginary numbers also divided into these two categories?,I know that real numbers can be algebraic or non-algebraic (if they cannot be the solution to an algebraic equation). Are imaginary numbers also divided into these two categories?,,"['linear-algebra', 'complex-numbers', 'terminology', 'algebraic-number-theory']"
1,The set of all regular points of a smooth map is open,The set of all regular points of a smooth map is open,,"Let $M$ be an smooth manifold manifold of dimension $m$ and let $N$ be a smooth manifold of dimension $n$, and let $F:M\rightarrow N$ be a smooth map. Then the set $W=\left\{ p\in M:F\mbox{ has full rank at }p\right\}$    is open. Here is my attempt at the proof. Suppose without loss of generality that $m<n$. Let $\left(U,\varphi\right)$ be a chart at $p$ and let $\left(V,\psi\right)$  be a chart at $F\left(p\right)$ such that $F\left(U\right)\subseteq V$. By identifying linear maps with matrices (with respect to the standard bases for $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$), the map $G:\varphi\left(U\right)\rightarrow M\left(n\times m,\mathbb{R}\right)$ defined by $x\longmapsto d\left(\psi\circ F\circ\varphi^{-1}\right)_{x}$ is continuous. For each $n\times m$ matrix $A$, let $m_A$ be the set of all invertible $m\times m$  submatrices of $A$. It is a standard result in linear algebra that $A$ has full rank if and only if $m_A$ is nonempty. The map $H:M(n\times m,\mathbb{R})\rightarrow\mathbb{R}$ defined    by $A\longmapsto\sum_{S\in m_{A}}\left|\mbox{det}S\right|$ is continuous. It follows that the composition $H\circ G\circ\varphi$ is continuous. Hence, $\left(H\circ G\circ\varphi\right)^{-1}\left(\mathbb{R}-\left\{ 0\right\} \right)$ is an open subset of $U$. Taking the union of all such sets will result in $W$. Therefore $W$ is open. Is it correct?","Let $M$ be an smooth manifold manifold of dimension $m$ and let $N$ be a smooth manifold of dimension $n$, and let $F:M\rightarrow N$ be a smooth map. Then the set $W=\left\{ p\in M:F\mbox{ has full rank at }p\right\}$    is open. Here is my attempt at the proof. Suppose without loss of generality that $m<n$. Let $\left(U,\varphi\right)$ be a chart at $p$ and let $\left(V,\psi\right)$  be a chart at $F\left(p\right)$ such that $F\left(U\right)\subseteq V$. By identifying linear maps with matrices (with respect to the standard bases for $\mathbb{R}^{n}$ and $\mathbb{R}^{m}$), the map $G:\varphi\left(U\right)\rightarrow M\left(n\times m,\mathbb{R}\right)$ defined by $x\longmapsto d\left(\psi\circ F\circ\varphi^{-1}\right)_{x}$ is continuous. For each $n\times m$ matrix $A$, let $m_A$ be the set of all invertible $m\times m$  submatrices of $A$. It is a standard result in linear algebra that $A$ has full rank if and only if $m_A$ is nonempty. The map $H:M(n\times m,\mathbb{R})\rightarrow\mathbb{R}$ defined    by $A\longmapsto\sum_{S\in m_{A}}\left|\mbox{det}S\right|$ is continuous. It follows that the composition $H\circ G\circ\varphi$ is continuous. Hence, $\left(H\circ G\circ\varphi\right)^{-1}\left(\mathbb{R}-\left\{ 0\right\} \right)$ is an open subset of $U$. Taking the union of all such sets will result in $W$. Therefore $W$ is open. Is it correct?",,"['linear-algebra', 'smooth-manifolds']"
2,Mean distance between matrix entries,Mean distance between matrix entries,,"Given a $4$ by $4$ matrix, or in general an $n$ by $n$ square matrix, can we determine the mean euclidean distance (i.e. $\sqrt{\Delta x ^2 + \Delta y^2}$) between entries that are not neighbours? Two matrix entries $(x_1,y_1)$ and $(x_2,y_2)$ are neighbours if $\Delta x=\Delta y=1$ or $\Delta x=1$ and $\Delta y=0$; or lastly $\Delta x=0$ and $\Delta y=1.$ This means for example that the middle entry $(2,2)$ in a $3$ by $3$ matrix has $8$ neighbours, which means that this entry cannot be paired with any other matrix entry to create a non-neighbouring pair. This also means that all entries in a 2 by 2 matrix (below the entry coordinates in the matrix are written) \begin{matrix}   (1,1) & (1,2)  \\   (2,1) & (2,2)    \end{matrix} are considered as neighbours. I understand the first part to solve is: for a given matrix size, how many non-neighbouring pairs of entries are there? For example in a $3$ by $3$ matrix, we have $16$ combinations of entries that are not neighbours. Note we care about combinations of entries because for example the pair $(1,3) \& (3,3)$ and the pair $(3,3) \& (1,3)$ are considered as indistinguishable, i.e., swapping the position of an entry in a pair does not count as a new pair of entries. Once we know the count $C$ of non-neighbouring pairs, then we can safely assign each pair with a probability $1/C,$ in order to solve the mean distance problem.","Given a $4$ by $4$ matrix, or in general an $n$ by $n$ square matrix, can we determine the mean euclidean distance (i.e. $\sqrt{\Delta x ^2 + \Delta y^2}$) between entries that are not neighbours? Two matrix entries $(x_1,y_1)$ and $(x_2,y_2)$ are neighbours if $\Delta x=\Delta y=1$ or $\Delta x=1$ and $\Delta y=0$; or lastly $\Delta x=0$ and $\Delta y=1.$ This means for example that the middle entry $(2,2)$ in a $3$ by $3$ matrix has $8$ neighbours, which means that this entry cannot be paired with any other matrix entry to create a non-neighbouring pair. This also means that all entries in a 2 by 2 matrix (below the entry coordinates in the matrix are written) \begin{matrix}   (1,1) & (1,2)  \\   (2,1) & (2,2)    \end{matrix} are considered as neighbours. I understand the first part to solve is: for a given matrix size, how many non-neighbouring pairs of entries are there? For example in a $3$ by $3$ matrix, we have $16$ combinations of entries that are not neighbours. Note we care about combinations of entries because for example the pair $(1,3) \& (3,3)$ and the pair $(3,3) \& (1,3)$ are considered as indistinguishable, i.e., swapping the position of an entry in a pair does not count as a new pair of entries. Once we know the count $C$ of non-neighbouring pairs, then we can safely assign each pair with a probability $1/C,$ in order to solve the mean distance problem.",,"['linear-algebra', 'matrices', 'probability-theory']"
3,"Affine Maps, Matricies, Invertibility, and Equivalence Relations","Affine Maps, Matricies, Invertibility, and Equivalence Relations",,"A mapping $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is affine if there is an invertible $n$ x $n$ matrix $M$ and a vector $s \in \mathbb{R}$ such that $f(x)=Mx+s$ for every $x \in \mathbb{R}^n$. Show that every affine map $f(x) = Mx+s$ is invertible and its inverse is also affine? Show that the composition of affine maps $\mathbb{R}^n \rightarrow \mathbb{R}^n$ is affine. For two subsets, $S$, $T$ of $\mathbb{R}^n$, write $S \sim T$ if there exists an affine map $f$ such that $f(S) =T$. We then say that $S,T$ are affine-related . Prove that $\sim$ is an equivalence relation on the set of subsets of $\mathbb{R}^n$.","A mapping $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is affine if there is an invertible $n$ x $n$ matrix $M$ and a vector $s \in \mathbb{R}$ such that $f(x)=Mx+s$ for every $x \in \mathbb{R}^n$. Show that every affine map $f(x) = Mx+s$ is invertible and its inverse is also affine? Show that the composition of affine maps $\mathbb{R}^n \rightarrow \mathbb{R}^n$ is affine. For two subsets, $S$, $T$ of $\mathbb{R}^n$, write $S \sim T$ if there exists an affine map $f$ such that $f(S) =T$. We then say that $S,T$ are affine-related . Prove that $\sim$ is an equivalence relation on the set of subsets of $\mathbb{R}^n$.",,"['linear-algebra', 'linear-transformations', 'linear-programming', 'equivalence-relations']"
4,"Integrate $ \int_0^\infty \left( ( x A+I)^{-1} A - \frac{1}{c+x} I \right)\, \mathrm dx $ where $A$ is positive-definite and $c>0$",Integrate  where  is positive-definite and," \int_0^\infty \left( ( x A+I)^{-1} A - \frac{1}{c+x} I \right)\, \mathrm dx  A c>0","$\def\d{\mathrm{d}}$Can someone outline for me have to integrate the following expression: \begin{align} \int_0^\infty \left( ( x A+I)^{-1} A - \frac{1}{c+x} I \right) \,\d x \end{align} where $A$ is a positive definte  matrix and $c>0$. The integration is done element-wise. In the scalar case, this inegral becomes $\log(a)+\log(c)$. One of the responses suggests that the answer is $\log(A)+\log(c)I$. However, I am not very sure how this was shown. Thanks.","$\def\d{\mathrm{d}}$Can someone outline for me have to integrate the following expression: \begin{align} \int_0^\infty \left( ( x A+I)^{-1} A - \frac{1}{c+x} I \right) \,\d x \end{align} where $A$ is a positive definte  matrix and $c>0$. The integration is done element-wise. In the scalar case, this inegral becomes $\log(a)+\log(c)$. One of the responses suggests that the answer is $\log(A)+\log(c)I$. However, I am not very sure how this was shown. Thanks.",,"['linear-algebra', 'integration', 'multivariable-calculus']"
5,How to prevent loss of roots in equations?,How to prevent loss of roots in equations?,,"I was studying the Theory of Equations when I came across a line which was something like this: Cancellation of common factors from both sides of equation leads to a loss of root.          For example, consider an equation    \begin{align*}    x^2-2x&=x-2\\    x(x-2)&=x-2\\    x&=1 \end{align*} They then proceed on to explain that if, instead of factoring out $x-2$, they had simply subtracted $x-2$ and made the RHS zero, they would have two solutions, namely, $x=1$ and $x=2$. I have not provided the entire process. However, as I started solving some problems, I came across a problem like this: $\dfrac{x^2+3x+2}{x^2-6x-7}=0$ Solve the above equation I was stuck in this problem, and didn't cancel out the factors that came upon factorisation of the numerator and denominator, mainly because I was apprehensive about cancelling out the roots.  But in the solution of this problem given with the answers, the solution was like this: Since the domain of the solution set is $\Bbb R - \{7,-1\}$ $\dfrac{(x+1)(x+2)}{(x+1)(x-7)}=0$ $x=-2$ But isn't this process leading to a loss of the solution of the equation $\dfrac1{x-7}=0$? Please help. If you think this question does not meet the standards of this marvellous site, please inform before giving any downvote.","I was studying the Theory of Equations when I came across a line which was something like this: Cancellation of common factors from both sides of equation leads to a loss of root.          For example, consider an equation    \begin{align*}    x^2-2x&=x-2\\    x(x-2)&=x-2\\    x&=1 \end{align*} They then proceed on to explain that if, instead of factoring out $x-2$, they had simply subtracted $x-2$ and made the RHS zero, they would have two solutions, namely, $x=1$ and $x=2$. I have not provided the entire process. However, as I started solving some problems, I came across a problem like this: $\dfrac{x^2+3x+2}{x^2-6x-7}=0$ Solve the above equation I was stuck in this problem, and didn't cancel out the factors that came upon factorisation of the numerator and denominator, mainly because I was apprehensive about cancelling out the roots.  But in the solution of this problem given with the answers, the solution was like this: Since the domain of the solution set is $\Bbb R - \{7,-1\}$ $\dfrac{(x+1)(x+2)}{(x+1)(x-7)}=0$ $x=-2$ But isn't this process leading to a loss of the solution of the equation $\dfrac1{x-7}=0$? Please help. If you think this question does not meet the standards of this marvellous site, please inform before giving any downvote.",,"['linear-algebra', 'quadratics']"
6,bounding the second largest eigenvalue of a regular graph,bounding the second largest eigenvalue of a regular graph,,"I read that the second largest eigenvalue of a graph is always positive except for  specific classes of graphs, i.e the graphs that have second largest eigenvalue smaller than zero are fully characterized. I was not able to figure out this question completely by browsing the web and looking at papers I found. Any reference or full answer would be of great help.","I read that the second largest eigenvalue of a graph is always positive except for  specific classes of graphs, i.e the graphs that have second largest eigenvalue smaller than zero are fully characterized. I was not able to figure out this question completely by browsing the web and looking at papers I found. Any reference or full answer would be of great help.",,"['linear-algebra', 'graph-theory', 'reference-request', 'spectral-graph-theory']"
7,Rank $1$ bilinear form is a product of two linear functionals on a finite dimensional vector space.,Rank  bilinear form is a product of two linear functionals on a finite dimensional vector space.,1,"Let $\mathbb{f}$ be a  non-zero bilinear form on a finite dimensional vector sppace $V.$ Then have to show that  $\mathbb{f}$ can be expressed as a product of two linear functionals i.e.,  $\mathbb{f}(\alpha, \beta)=L_1(\alpha)L_2(\beta)$ for $L_i \in V^*$ iff  $\mathbb{f}$ has rank $1.$ I proved that if  $\mathbb{f}$ is product of two linear functional then its rank is $1$ using left operator and the right operator. I am looking for the proof of the other direction. Help me. Many thanks.","Let $\mathbb{f}$ be a  non-zero bilinear form on a finite dimensional vector sppace $V.$ Then have to show that  $\mathbb{f}$ can be expressed as a product of two linear functionals i.e.,  $\mathbb{f}(\alpha, \beta)=L_1(\alpha)L_2(\beta)$ for $L_i \in V^*$ iff  $\mathbb{f}$ has rank $1.$ I proved that if  $\mathbb{f}$ is product of two linear functional then its rank is $1$ using left operator and the right operator. I am looking for the proof of the other direction. Help me. Many thanks.",,"['linear-algebra', 'matrices', 'multilinear-algebra', 'bilinear-form']"
8,Hermitian Operators and the Spectral Theorem,Hermitian Operators and the Spectral Theorem,,"I understand that in a finite-dimensional vector space $V$, a diagonalizable linear operator $T: V \to V$ decomposes $V$ into a direct sum of its invariant eigenspaces, on each of which it restricts to a scalar multiple of a projection. The Spectral Theorem says that this occurs if $T$ is Hermitian, and I'm trying to piece together a geometric intuition for the proof. Here's my thinking: we know that the eigenspace $E$ of some eigenvector $v$ is $T$-invariant, but we need something extra (presumably involving the Hermiticity of $T$) to show that its orthogonal complement $E^{\perp}$ is also invariant: that is, we know that $Tv$ will stay inside $E$, but Hermiticity is required for $Tw$ (for some $w \notin E$) to stay outside of $E$. Once this is done, we can just restrict to $E^{\perp}$ and play the same game there, so that by induction we separate out $n = \dim{V}$ orthogonal directions to get our eigenbasis. So what is it about Hermiticity that ""stabilizes"" $E^{\perp}$ under $T$? I'm not looking for a rigorous proof, but rather a natural explanation of the geometrical significance of self-adjointness. More generally, why does requiring $T$ to be normal make the Spectral Theorem biconditional?","I understand that in a finite-dimensional vector space $V$, a diagonalizable linear operator $T: V \to V$ decomposes $V$ into a direct sum of its invariant eigenspaces, on each of which it restricts to a scalar multiple of a projection. The Spectral Theorem says that this occurs if $T$ is Hermitian, and I'm trying to piece together a geometric intuition for the proof. Here's my thinking: we know that the eigenspace $E$ of some eigenvector $v$ is $T$-invariant, but we need something extra (presumably involving the Hermiticity of $T$) to show that its orthogonal complement $E^{\perp}$ is also invariant: that is, we know that $Tv$ will stay inside $E$, but Hermiticity is required for $Tw$ (for some $w \notin E$) to stay outside of $E$. Once this is done, we can just restrict to $E^{\perp}$ and play the same game there, so that by induction we separate out $n = \dim{V}$ orthogonal directions to get our eigenbasis. So what is it about Hermiticity that ""stabilizes"" $E^{\perp}$ under $T$? I'm not looking for a rigorous proof, but rather a natural explanation of the geometrical significance of self-adjointness. More generally, why does requiring $T$ to be normal make the Spectral Theorem biconditional?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'spectral-theory', 'adjoint-operators', 'invariant-subspace']"
9,What is the number of all skew symmetric bilinear forms on $m$ dimensional space $V$ over $\mathbb{F}_q$ with rank equal to $2r$?,What is the number of all skew symmetric bilinear forms on  dimensional space  over  with rank equal to ?,m V \mathbb{F}_q 2r,Let $V$ be a vector space of dimension $m$ over the finite field $\mathbb{F}_q.$ Then I want to find the number of all skew symmetric bilinear forms on $V$ with rank equal to $2r$ ($0 \leq 2r \leq m$). I need some help. Thanks in advance.,Let $V$ be a vector space of dimension $m$ over the finite field $\mathbb{F}_q.$ Then I want to find the number of all skew symmetric bilinear forms on $V$ with rank equal to $2r$ ($0 \leq 2r \leq m$). I need some help. Thanks in advance.,,"['linear-algebra', 'matrices']"
10,Finding an $x \in \mathbb R$ so that $\|I - x A\|_2$ becomes minimal,Finding an  so that  becomes minimal,x \in \mathbb R \|I - x A\|_2,"Let $A \in \mathbb R^{n \times n}$, and let $I$ denote the ($n \times n$-)identity matrix. Then I want to find a $x \in \mathbb{R}$, so that: $$\|I - x A\|_2$$ becomes minimal (where $\|\cdot\|_2$ denotes the Euclidean matrix norm). Let's assume $A ≠ 0$ to exclude the trivial case where any $x$ can be chosen. I think it's obvious that such a minimum exists, because $\|I - x A\|$ can be considered a real (continuous) function with respect to $x$ which is of course always positive, and $\lim_{\|x\| \to \infty} \|I - x A \|_2 = \infty$. So there must be (at least one) minimum, and it must somehow depend on $A$. I'm not really sure on how to find it, though. I've tried to write out $\|I - x A\|_2 = \sqrt{\sum_{i=0}^n \sum_{j=0}^n |\delta_{ij}- x a_{ij}|^2}$ and thought about differentiating that thing, but from some of the squares we could pull out $x$ and from others (where $\delta_{ij} = 1$) we can't, so I'm afraid that $\frac d{dx}$ of that thing would quickly become very nasty, let alone setting it $= 0$ and solving it for $x$, so I frankly suspect there's an easier way. Could we somehow bring the eigenvalues, the spectral radius or something like that into the equation, or use one of the properties of the matrix norm?","Let $A \in \mathbb R^{n \times n}$, and let $I$ denote the ($n \times n$-)identity matrix. Then I want to find a $x \in \mathbb{R}$, so that: $$\|I - x A\|_2$$ becomes minimal (where $\|\cdot\|_2$ denotes the Euclidean matrix norm). Let's assume $A ≠ 0$ to exclude the trivial case where any $x$ can be chosen. I think it's obvious that such a minimum exists, because $\|I - x A\|$ can be considered a real (continuous) function with respect to $x$ which is of course always positive, and $\lim_{\|x\| \to \infty} \|I - x A \|_2 = \infty$. So there must be (at least one) minimum, and it must somehow depend on $A$. I'm not really sure on how to find it, though. I've tried to write out $\|I - x A\|_2 = \sqrt{\sum_{i=0}^n \sum_{j=0}^n |\delta_{ij}- x a_{ij}|^2}$ and thought about differentiating that thing, but from some of the squares we could pull out $x$ and from others (where $\delta_{ij} = 1$) we can't, so I'm afraid that $\frac d{dx}$ of that thing would quickly become very nasty, let alone setting it $= 0$ and solving it for $x$, so I frankly suspect there's an easier way. Could we somehow bring the eigenvalues, the spectral radius or something like that into the equation, or use one of the properties of the matrix norm?",,"['linear-algebra', 'matrices', 'normed-spaces']"
11,How is the study of wavelets not just a special case of Fourier analysis?,How is the study of wavelets not just a special case of Fourier analysis?,,"As far as I can tell, ""wavelets"" is just a neologism for certain ""non-smooth"" families of functions which constitute orthonormal bases/families for $L^2[0,1]$. How is wavelet analysis anything new compared to the study of Fourier coefficients or Fourier series or the orthogonal decomposition of $L^2$ functions (i.e. in the most abstract possible function analytic sense, not in the sense of using specifically the orthonormal families of sines/cosines or complex exponentials)? Wavelet transforms just seem like the Fourier transform using a different orthonormal family for $L^2$ besides the complex exponentials, but conceptually this isn't really an achievement. The complex exponentials are a convenient orthonormal family, but at the end of the day aren't they just an orthonormal family?","As far as I can tell, ""wavelets"" is just a neologism for certain ""non-smooth"" families of functions which constitute orthonormal bases/families for $L^2[0,1]$. How is wavelet analysis anything new compared to the study of Fourier coefficients or Fourier series or the orthogonal decomposition of $L^2$ functions (i.e. in the most abstract possible function analytic sense, not in the sense of using specifically the orthonormal families of sines/cosines or complex exponentials)? Wavelet transforms just seem like the Fourier transform using a different orthonormal family for $L^2$ besides the complex exponentials, but conceptually this isn't really an achievement. The complex exponentials are a convenient orthonormal family, but at the end of the day aren't they just an orthonormal family?",,"['linear-algebra', 'fourier-analysis', 'fourier-series', 'orthonormal', 'wavelets']"
12,Significance of symmetric characteristic polynomials?,Significance of symmetric characteristic polynomials?,,"By symmetric characteristic polynomial, I mean for example... the characteristic polynomial of the $3\times3$ identity matrix is: $x^3 - 3x^2 + 3x - 1$ similarly for the $4\times4$ identity matrix it is: $x^4 - 4x^3 + 6x^2 - 4x + 1$ The absolute values of the coefficients are symmetric about the center. Is there some general property of matrices that leads to this symmetry in the characteristic polynomial in cases other than the identity matrices or a scalar times identity? For example, is there something interesting we can say about a $4\times4$ matrix that has this characteristic polynomial? $x^4 - 14x^3 + 26x^2 - 14x + 1$","By symmetric characteristic polynomial, I mean for example... the characteristic polynomial of the $3\times3$ identity matrix is: $x^3 - 3x^2 + 3x - 1$ similarly for the $4\times4$ identity matrix it is: $x^4 - 4x^3 + 6x^2 - 4x + 1$ The absolute values of the coefficients are symmetric about the center. Is there some general property of matrices that leads to this symmetry in the characteristic polynomial in cases other than the identity matrices or a scalar times identity? For example, is there something interesting we can say about a $4\times4$ matrix that has this characteristic polynomial? $x^4 - 14x^3 + 26x^2 - 14x + 1$",,"['linear-algebra', 'matrices']"
13,Eigenvalues and eigenvectors of $I \otimes A \ + \ B^T \otimes I$ (used in Sylvester's equation),Eigenvalues and eigenvectors of  (used in Sylvester's equation),I \otimes A \ + \ B^T \otimes I,"Let $A$ and $B$ be $n \times n$ square matrices, with resp. eigenpairs $(\lambda_i,U_i)$ and $(\mu_j,V_j)$. Let $I_n$ be the order $n$ identity matrix. I have seen a result that says that the $n^2$ eigenvalues of $$M_{AB}:=I_n \otimes A \ + \ B^T \otimes I_n$$ are all the $\lambda_i+\mu_j$. My question is: how is it possible to prove it, and under which conditions is it true ? Another issue: can something be said about the eigenvectors of $M_{AB}$ ? Appendix: The context is that of Sylvester equation : Let notation $M^{vec}$ be associated with the ""vectorialization"" of a matrix $M$, obtained by ""stacking"" its columns like in the following example: \begin{equation} M = \begin{pmatrix}a&c&e\\ b&d&f\end{pmatrix} \ \rightarrow \ M^{vec} = \begin{pmatrix}a\\b\\c\\ d\\e\\f\end{pmatrix}  \end{equation} We will need the fundamental technical property of Kronecker product: $$\underbrace{(ABC)^{vec}}_{vector}=\underbrace{(C^T \otimes A)}_{matrix}.\underbrace{(B)^{vec}}_{vector} \ \ \ (*)$$ Sylvester equation is $$\text{find} \ X \ \ \ \text{such that} \ \ \ AX+XB=C$$ Being evidently equivalent to $$(AX)^{vec} + (XB)^{vec} = C^{vec}$$ it can can be transformed, using property (*) and linearity of Kronecker's product, into: $$(I \otimes A \ + \ B^T \otimes I)X^{vec}=C^{vec}$$ explaining the usefulness of matrix $M_{AB}=I \otimes A \ + \ B^T \otimes I$, its inversibility being a condition for a unique solution to Sylvester's equation, this inversibility being determined by the fact that none of its eigenvalues is zero.","Let $A$ and $B$ be $n \times n$ square matrices, with resp. eigenpairs $(\lambda_i,U_i)$ and $(\mu_j,V_j)$. Let $I_n$ be the order $n$ identity matrix. I have seen a result that says that the $n^2$ eigenvalues of $$M_{AB}:=I_n \otimes A \ + \ B^T \otimes I_n$$ are all the $\lambda_i+\mu_j$. My question is: how is it possible to prove it, and under which conditions is it true ? Another issue: can something be said about the eigenvectors of $M_{AB}$ ? Appendix: The context is that of Sylvester equation : Let notation $M^{vec}$ be associated with the ""vectorialization"" of a matrix $M$, obtained by ""stacking"" its columns like in the following example: \begin{equation} M = \begin{pmatrix}a&c&e\\ b&d&f\end{pmatrix} \ \rightarrow \ M^{vec} = \begin{pmatrix}a\\b\\c\\ d\\e\\f\end{pmatrix}  \end{equation} We will need the fundamental technical property of Kronecker product: $$\underbrace{(ABC)^{vec}}_{vector}=\underbrace{(C^T \otimes A)}_{matrix}.\underbrace{(B)^{vec}}_{vector} \ \ \ (*)$$ Sylvester equation is $$\text{find} \ X \ \ \ \text{such that} \ \ \ AX+XB=C$$ Being evidently equivalent to $$(AX)^{vec} + (XB)^{vec} = C^{vec}$$ it can can be transformed, using property (*) and linearity of Kronecker's product, into: $$(I \otimes A \ + \ B^T \otimes I)X^{vec}=C^{vec}$$ explaining the usefulness of matrix $M_{AB}=I \otimes A \ + \ B^T \otimes I$, its inversibility being a condition for a unique solution to Sylvester's equation, this inversibility being determined by the fact that none of its eigenvalues is zero.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'kronecker-product']"
14,How to prove that $A$ is positive semi-definite if all principal minors are non-negative?,How to prove that  is positive semi-definite if all principal minors are non-negative?,A,"Let $A\in\mathbb C^{n\times n}$ be a Hermitian matrix such that all its principal minors are non-negative (i.e. for $B=\left(a_{l_il_j}\right)_{1≤i,j≤k}$ with $1≤l_1<...<l_k≤n$ we have $\det(B)≥0$). Then how to show that $A$ is positive semi-definite? I thought maybe we could use induction since the condition is also satisfied for every submatrix, but I couldn't find an easy way. Can prove it elegantly?","Let $A\in\mathbb C^{n\times n}$ be a Hermitian matrix such that all its principal minors are non-negative (i.e. for $B=\left(a_{l_il_j}\right)_{1≤i,j≤k}$ with $1≤l_1<...<l_k≤n$ we have $\det(B)≥0$). Then how to show that $A$ is positive semi-definite? I thought maybe we could use induction since the condition is also satisfied for every submatrix, but I couldn't find an easy way. Can prove it elegantly?",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
15,"Is there a commutative ring with a ""generalized determinant""?","Is there a commutative ring with a ""generalized determinant""?",,"Does there exist a commutative ring(-with-a-1) $R$ and positive integer $n$ and function $\hspace{.04 in}f$ from [the set of $n$-by-$n$ matrices over $R$] to $R$ such that $f$ is linear in each row and each column separately and $f$ of the $n$-by-$n$ identity matrix is $1_R$ and for all $n$-by-$n$ matrices $M\hspace{-0.03 in}$, if $M$ is invertible then $\hspace{.04 in}f(M)$ is a unit and $f$ is not the restriction of determinant to $\hspace{.04 in}f\hspace{.02 in}$'s domain ? I'm inspired by this answer .","Does there exist a commutative ring(-with-a-1) $R$ and positive integer $n$ and function $\hspace{.04 in}f$ from [the set of $n$-by-$n$ matrices over $R$] to $R$ such that $f$ is linear in each row and each column separately and $f$ of the $n$-by-$n$ identity matrix is $1_R$ and for all $n$-by-$n$ matrices $M\hspace{-0.03 in}$, if $M$ is invertible then $\hspace{.04 in}f(M)$ is a unit and $f$ is not the restriction of determinant to $\hspace{.04 in}f\hspace{.02 in}$'s domain ? I'm inspired by this answer .",,['linear-algebra']
16,Kernel of a Vandermonde like matrix,Kernel of a Vandermonde like matrix,,"I am wondering how to show that the following matrix has trivial kernel: $$\begin{bmatrix}     1&1&1&1&1&1 \\     s_1&s_2&s_3&s_4&s_5&s_6 \\     s_1^2&s_2^2&s_3^2&s_4^2&s_5^2&s_6^2 \\     s_1^3&s_2^3&s_3^3&s_4^3&s_5^3&s_6^3  \\         e^{s_1}s_1^3&e^{s_2}s_2^3&e^{s_3}s_3^3&e^{s_4}s_4^3&e^{s_5}s_5^3&e^{s_6}s_6^3 \\ e^{s_1}s_1^4&e^{s_2}s_2^4&e^{s_3}s_3^4&e^{s_4}s_4^4&e^{s_5}s_5^4&e^{s_6}s_6^4 \\ e^{s_1}s_1^5&e^{s_2}s_2^5&e^{s_3}s_3^5&e^{s_4}s_4^5&e^{s_5}s_5^5&e^{s_6}s_6^5 \end{bmatrix}$$ where $\zeta,\lambda\in\mathbb{R}\setminus\{0\}$, and $s_i \in \mathbb{C}$ are the roots of the polynomial $\zeta ^2 s^6-s^4+\lambda ^2=0$, which are all non-zero and distinct.","I am wondering how to show that the following matrix has trivial kernel: $$\begin{bmatrix}     1&1&1&1&1&1 \\     s_1&s_2&s_3&s_4&s_5&s_6 \\     s_1^2&s_2^2&s_3^2&s_4^2&s_5^2&s_6^2 \\     s_1^3&s_2^3&s_3^3&s_4^3&s_5^3&s_6^3  \\         e^{s_1}s_1^3&e^{s_2}s_2^3&e^{s_3}s_3^3&e^{s_4}s_4^3&e^{s_5}s_5^3&e^{s_6}s_6^3 \\ e^{s_1}s_1^4&e^{s_2}s_2^4&e^{s_3}s_3^4&e^{s_4}s_4^4&e^{s_5}s_5^4&e^{s_6}s_6^4 \\ e^{s_1}s_1^5&e^{s_2}s_2^5&e^{s_3}s_3^5&e^{s_4}s_4^5&e^{s_5}s_5^5&e^{s_6}s_6^5 \end{bmatrix}$$ where $\zeta,\lambda\in\mathbb{R}\setminus\{0\}$, and $s_i \in \mathbb{C}$ are the roots of the polynomial $\zeta ^2 s^6-s^4+\lambda ^2=0$, which are all non-zero and distinct.",,"['linear-algebra', 'matrices', 'matrix-rank', 'diophantine-approximation']"
17,Inverse of a matrix with uniform off diagonals,Inverse of a matrix with uniform off diagonals,,"Suppose that we have an all positive matrix where the off diagonal elements are all identical. Can one calculate the inverse of the matrix analytically, or more efficiently than the general case? For example: $$ \begin{bmatrix} 1 & .1 & .1 \\ .1 & 2 & .1 \\ .1 & .1 & 3 \\ \end{bmatrix} $$","Suppose that we have an all positive matrix where the off diagonal elements are all identical. Can one calculate the inverse of the matrix analytically, or more efficiently than the general case? For example: $$ \begin{bmatrix} 1 & .1 & .1 \\ .1 & 2 & .1 \\ .1 & .1 & 3 \\ \end{bmatrix} $$",,"['linear-algebra', 'matrices', 'inverse']"
18,"If $A, B$ are positive definite and $\|A+B\| = \|A\|+\|B\|$, they have a common eigenvector","If  are positive definite and , they have a common eigenvector","A, B \|A+B\| = \|A\|+\|B\|","Suppose $A$ and $B$ are two positive definite matrices such that $\|A+B\| = \|A\|+\|B\|$. Show that $A$ and $B$ have a common eigenvector, where $\|A\|$ is the operator norm of $A$. I'm wondering if there is a general strategy to solving problems of this form, I've been struggling with the problems for a while and can't get it. Any help is appreciated, Thank you!","Suppose $A$ and $B$ are two positive definite matrices such that $\|A+B\| = \|A\|+\|B\|$. Show that $A$ and $B$ have a common eigenvector, where $\|A\|$ is the operator norm of $A$. I'm wondering if there is a general strategy to solving problems of this form, I've been struggling with the problems for a while and can't get it. Any help is appreciated, Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'normed-spaces']"
19,Determining whether an orthogonal matrix represents a rotation or reflection,Determining whether an orthogonal matrix represents a rotation or reflection,,"The exercise asks us to determine whether the given orthogonal matrix represents a rotation or a reflection. If it is a rotation, give the angle of rotation; if it is a reflection, give the line of reflection. $$ A = \begin{bmatrix}        -\frac{3}{5} & -\frac{4}{5}\\[0.3em]        -\frac{4}{5} & \frac{3}{5}\\[0.3em]      \end{bmatrix} $$ I know you can check whether it is a reflection or rotation by calculating the determinant. So for example for the matrix above $$det(A) = -\frac{3}{5}\cdot\frac{3}{5}-(-\frac{4}{5})\cdot(-\frac{4}{5}) = -1$$ And so that means that matrix $A$ corresponds to a reflection in $R^2$, but how do you get the line of reflection from this? $$ B = \begin{bmatrix}        -\frac{1}{2} & \frac{\sqrt{3}}{2}\\[0.3em]        -\frac{\sqrt{3}}{2} & -\frac{1}{2}\\[0.3em]      \end{bmatrix} $$ And for a rotation, so for example matrix $B$ given above, can you simply say that it corresponds to the rotation matrix $R$ $$    R =   \begin{bmatrix}        cos(\theta) & -sin(\theta)\\[0.3em]        sin(\theta) & cos(\theta)\\[0.3em]      \end{bmatrix} $$ So this gives $$cos(\theta) = -\frac{1}{2}, \theta = cos^{-1}(-\frac{1}{2}) = 120^{\circ}\\   sin(\theta) = -\frac{\sqrt{3}}{2}, \theta = sin^{-1}(-\frac{\sqrt{3}}{2}) = -60^{\circ} $$ Which means that matrix $B$ corresponds to a counterclockwise rotation of $120^{\circ}$, right?","The exercise asks us to determine whether the given orthogonal matrix represents a rotation or a reflection. If it is a rotation, give the angle of rotation; if it is a reflection, give the line of reflection. $$ A = \begin{bmatrix}        -\frac{3}{5} & -\frac{4}{5}\\[0.3em]        -\frac{4}{5} & \frac{3}{5}\\[0.3em]      \end{bmatrix} $$ I know you can check whether it is a reflection or rotation by calculating the determinant. So for example for the matrix above $$det(A) = -\frac{3}{5}\cdot\frac{3}{5}-(-\frac{4}{5})\cdot(-\frac{4}{5}) = -1$$ And so that means that matrix $A$ corresponds to a reflection in $R^2$, but how do you get the line of reflection from this? $$ B = \begin{bmatrix}        -\frac{1}{2} & \frac{\sqrt{3}}{2}\\[0.3em]        -\frac{\sqrt{3}}{2} & -\frac{1}{2}\\[0.3em]      \end{bmatrix} $$ And for a rotation, so for example matrix $B$ given above, can you simply say that it corresponds to the rotation matrix $R$ $$    R =   \begin{bmatrix}        cos(\theta) & -sin(\theta)\\[0.3em]        sin(\theta) & cos(\theta)\\[0.3em]      \end{bmatrix} $$ So this gives $$cos(\theta) = -\frac{1}{2}, \theta = cos^{-1}(-\frac{1}{2}) = 120^{\circ}\\   sin(\theta) = -\frac{\sqrt{3}}{2}, \theta = sin^{-1}(-\frac{\sqrt{3}}{2}) = -60^{\circ} $$ Which means that matrix $B$ corresponds to a counterclockwise rotation of $120^{\circ}$, right?",,"['linear-algebra', 'rotations', 'orthogonality', 'reflection']"
20,Is it possible to write the Hadamard product of two matrices in tensor notation?,Is it possible to write the Hadamard product of two matrices in tensor notation?,,"Say I have two $4 \times 4$ matrices $(A^{\alpha \beta})$ and $(B^{\mu\nu})$ and want to compute the Hadamard (entry-wise) product. Is there an elegant way of writing this down in the common component, i.e. tensor, notation? Would it be something like $A^{\alpha \beta} B^{\alpha \beta}$ or is that not sufficient? Would this lead to conflicts with Einsteins summation convention?","Say I have two $4 \times 4$ matrices $(A^{\alpha \beta})$ and $(B^{\mu\nu})$ and want to compute the Hadamard (entry-wise) product. Is there an elegant way of writing this down in the common component, i.e. tensor, notation? Would it be something like $A^{\alpha \beta} B^{\alpha \beta}$ or is that not sufficient? Would this lead to conflicts with Einsteins summation convention?",,"['linear-algebra', 'matrices', 'notation', 'tensors']"
21,Properties of 3-vector dot product,Properties of 3-vector dot product,,"I've been playing around with an extension of a dot product to three vectors, as set forth in this question .  Basically, if you have three vectors A, B, and C, then you could compute the following $$TD(A,B,C) = \sum_{i=1}^N a_i b_i c_i$$ where TD means ""triple dot."" I realize this isn't an accepted notation but it is useful for this question. I'm curious if anyone has shown that $$TD(A,B,C)\leqslant \lVert A \rVert \cdot\lVert B \rVert\cdot \lVert C \rVert.$$ I suppose it might involve an extension of the Rearrangement inequality to three vectors.","I've been playing around with an extension of a dot product to three vectors, as set forth in this question .  Basically, if you have three vectors A, B, and C, then you could compute the following $$TD(A,B,C) = \sum_{i=1}^N a_i b_i c_i$$ where TD means ""triple dot."" I realize this isn't an accepted notation but it is useful for this question. I'm curious if anyone has shown that $$TD(A,B,C)\leqslant \lVert A \rVert \cdot\lVert B \rVert\cdot \lVert C \rVert.$$ I suppose it might involve an extension of the Rearrangement inequality to three vectors.",,"['linear-algebra', 'inequality']"
22,Is a symmetric matrix characterized by the diagonal of its resolvent?,Is a symmetric matrix characterized by the diagonal of its resolvent?,,"The resolvent of a square matrix $A$ is defined by $R(s) = (A-sI)^{-1}$ for $s \notin \operatorname{spect}(A)$. Is knowing the diagonal of $R(s)$ for all $s$ sufficient to recover $A$ when $A$ is symmetric? edit: a counter-example of two matrices $A,B$ whose resolvent have the same diagonal has been found by Robert Israel. In the counter example, $A = P B P^T$ for some permutation matrix $P$. Now the question is, it is possible to recover $A$ up to permutations of rows and columns?","The resolvent of a square matrix $A$ is defined by $R(s) = (A-sI)^{-1}$ for $s \notin \operatorname{spect}(A)$. Is knowing the diagonal of $R(s)$ for all $s$ sufficient to recover $A$ when $A$ is symmetric? edit: a counter-example of two matrices $A,B$ whose resolvent have the same diagonal has been found by Robert Israel. In the counter example, $A = P B P^T$ for some permutation matrix $P$. Now the question is, it is possible to recover $A$ up to permutations of rows and columns?",,['linear-algebra']
23,Fundamental Theorem of Linear Algebra over Complex Field,Fundamental Theorem of Linear Algebra over Complex Field,,"How is the fundamental theorem of linear algebra stated when the inducing matrix has elements from the complex field?  For example, does the usual transpose become a Hermitian transpose in a statement like this:   $(nullspace(A)) = (rangespace(A^T))^\perp$","How is the fundamental theorem of linear algebra stated when the inducing matrix has elements from the complex field?  For example, does the usual transpose become a Hermitian transpose in a statement like this:   $(nullspace(A)) = (rangespace(A^T))^\perp$",,"['linear-algebra', 'complex-numbers']"
24,Eigenvalues and eigenvectors of antidiagonal block matrix,Eigenvalues and eigenvectors of antidiagonal block matrix,,"I have $2$ square matrices $A_m$ and $B_m$ which are symmetric and of size $m\times m$ . And the 3rd matrix is $$C = \begin{bmatrix} 0 & A \\ B & 0\end{bmatrix}$$ Now, I would like to calculate the eigenvalues and eigenvectors of matrix $C$ . How can I get it? Or how does it related to the eigenvalues and eigenvectors of $A$ and $B$ ? Thank you very much in advance!","I have square matrices and which are symmetric and of size . And the 3rd matrix is Now, I would like to calculate the eigenvalues and eigenvectors of matrix . How can I get it? Or how does it related to the eigenvalues and eigenvectors of and ? Thank you very much in advance!",2 A_m B_m m\times m C = \begin{bmatrix} 0 & A \\ B & 0\end{bmatrix} C A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
25,Linear independence under weird condition [duplicate],Linear independence under weird condition [duplicate],,"This question already has answers here : Prove that $v_1, \dots v_n$ is a basis of V. (2 answers) Closed 5 years ago . This is a problem from a linear algebra textbook. Given a finite dimensional inner product space $V$ with orthonormal basis $e_1, \ldots, e_n$, show that if a list of vectors $v_1, \ldots, v_n$ satisfies $\|e_j - v_j\| < \frac{1}{\sqrt{n}}$ for all $j$ in $\{1, \ldots, n\}$, then $v_j$'s form a basis of $V$. I have no idea. It's intuitive when I think about $\mathbb{R}^2$, looking at little spheres at the tips of the $e_j$'s. I thought about looking at prefixes, like it should be true that $\|e_j - v_j\| < \frac{1}{\sqrt{i}}$ for all $j$ in $\{1, \ldots, n\}$. So now if $v_i \in \operatorname{span}(v_1, \ldots, v_i)$ then it should violate the inequality. It just looked ugly. Is this even a good direction? Edit: I see it's ridiculous now... Is this something hard (it's Axler's book, 3rd edition, and the problems aren't marked by difficulty, so I don't want to waste time), or am I just being silly?","This question already has answers here : Prove that $v_1, \dots v_n$ is a basis of V. (2 answers) Closed 5 years ago . This is a problem from a linear algebra textbook. Given a finite dimensional inner product space $V$ with orthonormal basis $e_1, \ldots, e_n$, show that if a list of vectors $v_1, \ldots, v_n$ satisfies $\|e_j - v_j\| < \frac{1}{\sqrt{n}}$ for all $j$ in $\{1, \ldots, n\}$, then $v_j$'s form a basis of $V$. I have no idea. It's intuitive when I think about $\mathbb{R}^2$, looking at little spheres at the tips of the $e_j$'s. I thought about looking at prefixes, like it should be true that $\|e_j - v_j\| < \frac{1}{\sqrt{i}}$ for all $j$ in $\{1, \ldots, n\}$. So now if $v_i \in \operatorname{span}(v_1, \ldots, v_i)$ then it should violate the inequality. It just looked ugly. Is this even a good direction? Edit: I see it's ridiculous now... Is this something hard (it's Axler's book, 3rd edition, and the problems aren't marked by difficulty, so I don't want to waste time), or am I just being silly?",,['linear-algebra']
26,Determinant of remainder of a primitive matrix modulo 2,Determinant of remainder of a primitive matrix modulo 2,,"I'm trying to prove the following relation for a matrix $A\in \mathbb{Z}^{m\times m} $, $m\geq 2$. It is assumed that the characteristic polynomial of $A$ is primitive modulo $2$: If $C$ is defined to be the remainder of $A^{2^m-1}\pmod{4}$ , i.e.   $$A^{2^m-1}\equiv I+2C \pmod{4}$$   Then prove   $$\textrm{det}(C)\equiv \textrm{det}(C+I)\equiv 1 \pmod{2}$$ My try: $$2C\equiv A^{2^m-1}-I\pmod{4}\Rightarrow C\equiv \frac{1}{2}(A^{2^m-1}-I)\pmod{2}$$ $$ \textrm{det}(C)\equiv \textrm{det}(\frac{1}{2}(A^{2^m-1}-I)) \pmod{2}$$ Then for $C+I$ $$C+I\equiv \frac{1}{2}(A^{2^m-1}-I)+I \equiv \frac{1}{2}(A^{2^m-1}+I)\pmod{2}$$ Therefore to prove the statement we must have:   $$ \textrm{det}(\frac{1}{2}(A^{2^m-1}-I))\equiv \textrm{det}(\frac{1}{2}(A^{2^m-1}+I)) \equiv 1 \pmod{2} $$ Or $$\frac{1}{2^m}\textrm{det}(A^{2^m-1}-I)\equiv \frac{1}{2^m}\textrm{det}(A^{2^m-1}+I) \equiv 1 \pmod{2}$$ $$\Rightarrow\textrm{det}(A^{2^m-1}-I)\equiv \textrm{det}(A^{2^m-1}+I) \equiv 2^{m} \pmod{2^{m+1}}$$ If we call $P_{A^{2^m-1}}(\lambda)$ the characteristic polynomial of $A^{2^m-1}$ then I know that $\textrm{det}(A^{2^m-1}-\lambda I)\equiv P_{A^{2^m-1}}(\lambda)$, and therefore $$\textrm{det}(A^{2^m-1}-I)= P_{A^{2^m-1}}(1)\,,\textrm{det}(A^{2^m-1}+I)= P_{A^{2^m-1}}(-1)$$ EDIT: I've found out here that if eigenvalues of $A$ are $y_i$ then eigenvalues of $A^n$ will be $y_i^n$, however $y_i$ might be complex. Therefore: $$P_{A}(\lambda)=\prod_{i=0}^m(\lambda-y_i)\Rightarrow P_{A^{2^m-1}}(\lambda)=\prod_{i=0}^m(\lambda-y_i^{2^m-1})$$ Then I tried to compute each coefficient of $P_{A^{2^m-1}}$ in terms of coefficients of  $P_{A}$, which leads to some kind of generalization of Newton identities I asked it here , however it seems it is not possible to compute all of them (Question has received no general answer yet). I couldn't continue it further. Notes: I've tested a few matrices (of degrees 2,3,4,5) with computer and the statement was true for them. Primitive here means that the characteristic polynomial of $A$, $q(t)=\textrm{det}(A-tI)=t^m+a_1t^{m-1}+\ldots+a_m$, be primitive (irreducible) modulo 2.  $$A=\left(\begin{array}{cccc} a_1  & \ldots & a_{m-1}& a_m\\ 1 & \ldots & 0 & 0\\ \vdots  & \ddots & \vdots &\vdots\\ 0 &  \ldots &1 & 0 \end{array}\right)$$ Important Edit : An additional assumption is required for $P_A(x)$  modulo 4. Which is: If $P_A(x)\equiv f(x^2)+xg(x^2)\pmod 2$ then the above claim is not true only if:    $P_A(x)\equiv f(x)^2+xg(x)^2\pmod 4$","I'm trying to prove the following relation for a matrix $A\in \mathbb{Z}^{m\times m} $, $m\geq 2$. It is assumed that the characteristic polynomial of $A$ is primitive modulo $2$: If $C$ is defined to be the remainder of $A^{2^m-1}\pmod{4}$ , i.e.   $$A^{2^m-1}\equiv I+2C \pmod{4}$$   Then prove   $$\textrm{det}(C)\equiv \textrm{det}(C+I)\equiv 1 \pmod{2}$$ My try: $$2C\equiv A^{2^m-1}-I\pmod{4}\Rightarrow C\equiv \frac{1}{2}(A^{2^m-1}-I)\pmod{2}$$ $$ \textrm{det}(C)\equiv \textrm{det}(\frac{1}{2}(A^{2^m-1}-I)) \pmod{2}$$ Then for $C+I$ $$C+I\equiv \frac{1}{2}(A^{2^m-1}-I)+I \equiv \frac{1}{2}(A^{2^m-1}+I)\pmod{2}$$ Therefore to prove the statement we must have:   $$ \textrm{det}(\frac{1}{2}(A^{2^m-1}-I))\equiv \textrm{det}(\frac{1}{2}(A^{2^m-1}+I)) \equiv 1 \pmod{2} $$ Or $$\frac{1}{2^m}\textrm{det}(A^{2^m-1}-I)\equiv \frac{1}{2^m}\textrm{det}(A^{2^m-1}+I) \equiv 1 \pmod{2}$$ $$\Rightarrow\textrm{det}(A^{2^m-1}-I)\equiv \textrm{det}(A^{2^m-1}+I) \equiv 2^{m} \pmod{2^{m+1}}$$ If we call $P_{A^{2^m-1}}(\lambda)$ the characteristic polynomial of $A^{2^m-1}$ then I know that $\textrm{det}(A^{2^m-1}-\lambda I)\equiv P_{A^{2^m-1}}(\lambda)$, and therefore $$\textrm{det}(A^{2^m-1}-I)= P_{A^{2^m-1}}(1)\,,\textrm{det}(A^{2^m-1}+I)= P_{A^{2^m-1}}(-1)$$ EDIT: I've found out here that if eigenvalues of $A$ are $y_i$ then eigenvalues of $A^n$ will be $y_i^n$, however $y_i$ might be complex. Therefore: $$P_{A}(\lambda)=\prod_{i=0}^m(\lambda-y_i)\Rightarrow P_{A^{2^m-1}}(\lambda)=\prod_{i=0}^m(\lambda-y_i^{2^m-1})$$ Then I tried to compute each coefficient of $P_{A^{2^m-1}}$ in terms of coefficients of  $P_{A}$, which leads to some kind of generalization of Newton identities I asked it here , however it seems it is not possible to compute all of them (Question has received no general answer yet). I couldn't continue it further. Notes: I've tested a few matrices (of degrees 2,3,4,5) with computer and the statement was true for them. Primitive here means that the characteristic polynomial of $A$, $q(t)=\textrm{det}(A-tI)=t^m+a_1t^{m-1}+\ldots+a_m$, be primitive (irreducible) modulo 2.  $$A=\left(\begin{array}{cccc} a_1  & \ldots & a_{m-1}& a_m\\ 1 & \ldots & 0 & 0\\ \vdots  & \ddots & \vdots &\vdots\\ 0 &  \ldots &1 & 0 \end{array}\right)$$ Important Edit : An additional assumption is required for $P_A(x)$  modulo 4. Which is: If $P_A(x)\equiv f(x^2)+xg(x^2)\pmod 2$ then the above claim is not true only if:    $P_A(x)\equiv f(x)^2+xg(x)^2\pmod 4$",,"['linear-algebra', 'matrices', 'number-theory', 'modular-arithmetic', 'determinant']"
27,Why is the image of a C*-Algebra complete?,Why is the image of a C*-Algebra complete?,,"I am currently working through the book by Bratteli and Robinson on C* and W* algebras, there is one point at the beginning of chapter 2.3 that is frustrating me. If we take *-morphism to be a function $\pi: U \to B$ between $C^*$-algebras $U,V$ with: $\pi$ is linear and multiplicative $\forall A \in U,$ $\pi(A^*)=\pi(A)^*$ Then a property is of such a map is $||\pi(A)||≤||A||$ (so contractive and as such continuous). The book remarks that the image of $U$ is closed by ""an easy"" application of this continuity property. However I cannot reach the result and I feel like I am failing to do something extremely obvious. Does anybody have any tips? (It is maybe interesting to note that in the erratum they comment that the line ""by an easy"" is to be omitted! One more comment: Since $U$ and $V$ are complete normed vector spaces, I am taking it to be equivalent for a subset to be complete or closed)","I am currently working through the book by Bratteli and Robinson on C* and W* algebras, there is one point at the beginning of chapter 2.3 that is frustrating me. If we take *-morphism to be a function $\pi: U \to B$ between $C^*$-algebras $U,V$ with: $\pi$ is linear and multiplicative $\forall A \in U,$ $\pi(A^*)=\pi(A)^*$ Then a property is of such a map is $||\pi(A)||≤||A||$ (so contractive and as such continuous). The book remarks that the image of $U$ is closed by ""an easy"" application of this continuity property. However I cannot reach the result and I feel like I am failing to do something extremely obvious. Does anybody have any tips? (It is maybe interesting to note that in the erratum they comment that the line ""by an easy"" is to be omitted! One more comment: Since $U$ and $V$ are complete normed vector spaces, I am taking it to be equivalent for a subset to be complete or closed)",,"['linear-algebra', 'functional-analysis', 'c-star-algebras']"
28,Determinant of matrix with unit length rows,Determinant of matrix with unit length rows,,"What can be said about the determinant of a matrix when its rows (or similarly, columns) are unit vectors? Do such determinants have a geometric interpretation? For example, in the two-dimensional case, the determinant of two unit length vectors is the sinus between them.","What can be said about the determinant of a matrix when its rows (or similarly, columns) are unit vectors? Do such determinants have a geometric interpretation? For example, in the two-dimensional case, the determinant of two unit length vectors is the sinus between them.",,"['linear-algebra', 'matrices', 'euclidean-geometry']"
29,Finding $P$ such that $P^TAP$ is a diagonal matrix,Finding  such that  is a diagonal matrix,P P^TAP,"Let $$A = \left(\begin{array}{cc} 2&3 \\ 3&4  \end{array}\right) \in M_2(\mathbb{C})$$ Find $P$ such that $P^TAP = D$ where $D$ is a diagonal matrix. So here's the solution: $$A = \left(\begin{array}{cc|cc} 2&3&1&0\\ 3&4&0&1    \end{array}\right) \sim \left(\begin{array}{cc|cc} 2&0&1&-3/2\\ 0&-1/2&0&1    \end{array}\right)$$ Therefore, $$P = \left(\begin{array}{cc} 1&-3/2\\ 0&1    \end{array}\right) \\ P^TAP = \left(\begin{array}{cc} 2&0\\ 0&-1/2    \end{array}\right) $$ What was done here exactly? I'd be glad elaborate about the process. Thanks.","Let Find such that where is a diagonal matrix. So here's the solution: Therefore, What was done here exactly? I'd be glad elaborate about the process. Thanks.","A = \left(\begin{array}{cc} 2&3 \\ 3&4  \end{array}\right) \in
M_2(\mathbb{C}) P P^TAP = D D A = \left(\begin{array}{cc|cc} 2&3&1&0\\ 3&4&0&1    \end{array}\right) \sim \left(\begin{array}{cc|cc} 2&0&1&-3/2\\ 0&-1/2&0&1    \end{array}\right) P = \left(\begin{array}{cc} 1&-3/2\\ 0&1    \end{array}\right) \\ P^TAP = \left(\begin{array}{cc} 2&0\\ 0&-1/2    \end{array}\right) ","['linear-algebra', 'matrices', 'solution-verification', 'diagonalization']"
30,What's the best way to think about the covariance matrix?,What's the best way to think about the covariance matrix?,,"Let $X$ be a random vector with covariance matrix $\Sigma$. People often describe $\Sigma$ in terms of its components: $\Sigma_{ij}$ is the covariance of the $i$th and $j$th components of $X$. But in linear algebra, thinking about a matrix in terms of its components is often discouraged.  It is often more enlightening to avoid thinking in terms of components. So what is the ""best"" way to think about $\Sigma$, particularly for someone who likes linear algebra? I know that $\Sigma = \mathbb E((X - \mu)(X - \mu)^T)$, where $\mu = \mathbb E(X)$.  But I think I am still missing something, because I'm not sure what to make of that formula.  Does this formula shed light on what $\Sigma$ really is and why we care about it?","Let $X$ be a random vector with covariance matrix $\Sigma$. People often describe $\Sigma$ in terms of its components: $\Sigma_{ij}$ is the covariance of the $i$th and $j$th components of $X$. But in linear algebra, thinking about a matrix in terms of its components is often discouraged.  It is often more enlightening to avoid thinking in terms of components. So what is the ""best"" way to think about $\Sigma$, particularly for someone who likes linear algebra? I know that $\Sigma = \mathbb E((X - \mu)(X - \mu)^T)$, where $\mu = \mathbb E(X)$.  But I think I am still missing something, because I'm not sure what to make of that formula.  Does this formula shed light on what $\Sigma$ really is and why we care about it?",,"['linear-algebra', 'covariance']"
31,Why do $n$ linearly independent vectors span $\mathbb{R}^{n}$?,Why do  linearly independent vectors span ?,n \mathbb{R}^{n},"Suppose we have $n$ linearly independent vectors $\mathbf{v}_{1}\ldots\mathbf{v}_{n}$ in $\mathbb{R}^{n}$. I know that they do span $\mathbb{R}^{n}$, because we can easily specify a non-singular map which sends the $\mathbf{v}_{i}$s to the standard basis, and then to whichever vector in $\mathbb{R}^{n}$ we choose. My question is: do we need all the machinery of linear maps, determinants, etc. or is there a proof which is closer to the definitions? Every time I start writing down a proof I end up wanting to say ""and this set of equations can be solved uniquely because this matrix is non-singular"". Is this necessary?","Suppose we have $n$ linearly independent vectors $\mathbf{v}_{1}\ldots\mathbf{v}_{n}$ in $\mathbb{R}^{n}$. I know that they do span $\mathbb{R}^{n}$, because we can easily specify a non-singular map which sends the $\mathbf{v}_{i}$s to the standard basis, and then to whichever vector in $\mathbb{R}^{n}$ we choose. My question is: do we need all the machinery of linear maps, determinants, etc. or is there a proof which is closer to the definitions? Every time I start writing down a proof I end up wanting to say ""and this set of equations can be solved uniquely because this matrix is non-singular"". Is this necessary?",,"['linear-algebra', 'matrices', 'vectors']"
32,"Drawing a tetrahedron from a parellelepiped to convince myself it is 1/6th the volume,","Drawing a tetrahedron from a parellelepiped to convince myself it is 1/6th the volume,",,"I drew a parallelepiped that is spanned by three vectors, and we know the volume is given by the absolute value of the determinant of the matrix - with the three vectors arranged in rows (or columns, since $detA = detA^T$). How can I draw the tetrahedron from this parallelepiped to convince myself it is indeed 1/6th of the volume, i.e., 1/6 * |detA|? Thanks,","I drew a parallelepiped that is spanned by three vectors, and we know the volume is given by the absolute value of the determinant of the matrix - with the three vectors arranged in rows (or columns, since $detA = detA^T$). How can I draw the tetrahedron from this parallelepiped to convince myself it is indeed 1/6th of the volume, i.e., 1/6 * |detA|? Thanks,",,"['linear-algebra', 'geometry', 'determinant', 'volume']"
33,"Calculating the sum $\frac{1}{2} \sum x^T \Sigma x$ for all $x \in \{0,1\}^n$",Calculating the sum  for all,"\frac{1}{2} \sum x^T \Sigma x x \in \{0,1\}^n","Note: the equation inside the sum is related to Boltzmann Machines / Hopfield Networks, the energy function of these functions are similar. For further info, i.e. on how to derive the maximum likelihood estimator for $\Sigma$ you could take a look at David MacKay's book, section 43 MacKay calls it $W$) http://www.inference.phy.cam.ac.uk/itprnn/ps/521.526.pdf Original Question: I am trying to calculate the sum $$  \frac{1}{2} \sum_{\forall x} x^T \Sigma x   $$ where $\Sigma$ is a symmetric, positive definite matrix, $x$ is a column vector with binary values. I can assume normalization of $\Sigma$ as another step (so that all rows sum to 1), if that will give me convergence. $\Sigma$ is the result of cooccurence calculation $A^TA$ on sample data that contains only binary values, 0 or 1. I ran some numerical experiments on randomly generated samples, from sklearn.preprocessing import normalize import numpy as np np.random.seed(0) A = np.random.randint(0,2,(100,4)) cooc = A.T.dot(A).astype(float) cc = normalize(cooc, norm='l1', axis=1) When I apply the formula, that is, run through all possible values of $x \in \{0,1\}^n$, and calculate the sum, import itertools, numpy.linalg as lin lst = np.array(list(itertools.product([0, 1], repeat=A.shape[1]))) s = 0 for x in lst:      s += np.dot(np.dot(x.T,cc), x) / 2 print s I get 11.15 on data with 4 dimensions. For n=5 I get around 26.6, on any sample I generate with these dimensions. From this I concluded this number is directly tied to the dimension I am working with, and is a constant, so I was wondering if it could be calculated as a limit, somehow. Note: I need this number as a normalization constant, I plan to use $p(x;\Sigma) = \frac{1}{2C} x^T \Sigma x$ as a probability mass function. Here is how I arrived to this question. I was trying to capture frequency of each single variable and dependency between all variables of multivariate binary samples. I used a simple cooccurence calculation on the sample, import numpy as np A = np.array([\ [0.,1.,1.,0], [1.,1.,0, 0], [1.,1.,1.,0], [0, 1.,1.,1.], [0, 0, 1.,0] ]) c = A.T.dot(A).astype(float) print c Result [[ 2.  2.  1.  0.]  [ 2.  4.  3.  1.]  [ 1.  3.  4.  1.]  [ 0.  1.  1.  1.]] Now for any new data point $x$, if I wanted to calculate a ""score"", ex: x = np.array([[0,1,1,0]]) print np.dot(np.dot(x.T,c), x) / 2 would give me 7. The formula basically picks numbers 4,3,3,4 in the middle block of the cooc matrix, and sums them, which was what I wanted because new data point $x=[0,1,1,0]$ has binary variables 2 and 3 turned on (is 1) so I am interested in the dependency between $x_2$ and $x_3$, as well as the frequency of the variables by themselves. Once I had this score, I started wondering if I could turn this function into a PMF, hence the need for normalization constant and the need to integrate the function for all possible values of $x$. I toyed with the idea of dividing the sum by $x^Tx$, thereby causing the equation to look like the Rayleigh Quotient, $$  = \frac{1}{2} \sum_{\forall x} \frac{x^T \Sigma x }{x^Tx}  $$ then, if I assumed ""x=all eigenvalues"" instead of ""x=all possible values"" then perhaps summing all eigenvalues would give me something. But the summation must be for all x. Representing all x's using eigenvectors as basis maybe.. ?","Note: the equation inside the sum is related to Boltzmann Machines / Hopfield Networks, the energy function of these functions are similar. For further info, i.e. on how to derive the maximum likelihood estimator for $\Sigma$ you could take a look at David MacKay's book, section 43 MacKay calls it $W$) http://www.inference.phy.cam.ac.uk/itprnn/ps/521.526.pdf Original Question: I am trying to calculate the sum $$  \frac{1}{2} \sum_{\forall x} x^T \Sigma x   $$ where $\Sigma$ is a symmetric, positive definite matrix, $x$ is a column vector with binary values. I can assume normalization of $\Sigma$ as another step (so that all rows sum to 1), if that will give me convergence. $\Sigma$ is the result of cooccurence calculation $A^TA$ on sample data that contains only binary values, 0 or 1. I ran some numerical experiments on randomly generated samples, from sklearn.preprocessing import normalize import numpy as np np.random.seed(0) A = np.random.randint(0,2,(100,4)) cooc = A.T.dot(A).astype(float) cc = normalize(cooc, norm='l1', axis=1) When I apply the formula, that is, run through all possible values of $x \in \{0,1\}^n$, and calculate the sum, import itertools, numpy.linalg as lin lst = np.array(list(itertools.product([0, 1], repeat=A.shape[1]))) s = 0 for x in lst:      s += np.dot(np.dot(x.T,cc), x) / 2 print s I get 11.15 on data with 4 dimensions. For n=5 I get around 26.6, on any sample I generate with these dimensions. From this I concluded this number is directly tied to the dimension I am working with, and is a constant, so I was wondering if it could be calculated as a limit, somehow. Note: I need this number as a normalization constant, I plan to use $p(x;\Sigma) = \frac{1}{2C} x^T \Sigma x$ as a probability mass function. Here is how I arrived to this question. I was trying to capture frequency of each single variable and dependency between all variables of multivariate binary samples. I used a simple cooccurence calculation on the sample, import numpy as np A = np.array([\ [0.,1.,1.,0], [1.,1.,0, 0], [1.,1.,1.,0], [0, 1.,1.,1.], [0, 0, 1.,0] ]) c = A.T.dot(A).astype(float) print c Result [[ 2.  2.  1.  0.]  [ 2.  4.  3.  1.]  [ 1.  3.  4.  1.]  [ 0.  1.  1.  1.]] Now for any new data point $x$, if I wanted to calculate a ""score"", ex: x = np.array([[0,1,1,0]]) print np.dot(np.dot(x.T,c), x) / 2 would give me 7. The formula basically picks numbers 4,3,3,4 in the middle block of the cooc matrix, and sums them, which was what I wanted because new data point $x=[0,1,1,0]$ has binary variables 2 and 3 turned on (is 1) so I am interested in the dependency between $x_2$ and $x_3$, as well as the frequency of the variables by themselves. Once I had this score, I started wondering if I could turn this function into a PMF, hence the need for normalization constant and the need to integrate the function for all possible values of $x$. I toyed with the idea of dividing the sum by $x^Tx$, thereby causing the equation to look like the Rayleigh Quotient, $$  = \frac{1}{2} \sum_{\forall x} \frac{x^T \Sigma x }{x^Tx}  $$ then, if I assumed ""x=all eigenvalues"" instead of ""x=all possible values"" then perhaps summing all eigenvalues would give me something. But the summation must be for all x. Representing all x's using eigenvectors as basis maybe.. ?",,"['linear-algebra', 'multivariable-calculus']"
34,Find a basis for $U+W$ and $U\cap W$,Find a basis for  and,U+W U\cap W,"Let $$W = \operatorname{span}([2,1,0,1], [0,0,1,0]) \\V = \operatorname{span}([1,2,1,3], [3,1,-1,4])$$ I need to find a basis and the dimension for $U+V$ and $U\cap V$. For $U+V$ I tried: $$U+V = \{u+v|u\in U, v\in V\} = \alpha_1[2,1,0,1]+\alpha_2[0,0,1,0] + \alpha_3[1,2,1,3] + \alpha_4[3,1,-1,4]$$ Therefore I have to find if this set is linearly independent or not. If it is, then it's a basis for $U+V$. By transforming this to a system, we have: $$\begin{cases}2\alpha_1 + 0\alpha_2 + 1\alpha_3 + 3\alpha_4 = 0\\1\alpha_1 + 0\alpha_2 + 2\alpha_3 + 1\alpha_4 = 0\\0\alpha_1 + 1\alpha_2 + 1\alpha_3 -1\alpha_4 = 0\\1\alpha_1 + 0\alpha_2 + 3\alpha_3 + 4\alpha_4 = 0\end{cases}$$ By solving this system, we should get the answer. Is there a easy way to solve it? For the case $U\cap W$ I can't see how to act. Maybe if i find $\beta_1, \beta_2, \beta_3, \beta_4$ such that  $$\beta_1[2,1,0,1]+\beta_2[0,0,1,0] = \beta_3[1,2,1,3] + \beta_4[3,1,-1,4]$$ I should get what the intersection is?","Let $$W = \operatorname{span}([2,1,0,1], [0,0,1,0]) \\V = \operatorname{span}([1,2,1,3], [3,1,-1,4])$$ I need to find a basis and the dimension for $U+V$ and $U\cap V$. For $U+V$ I tried: $$U+V = \{u+v|u\in U, v\in V\} = \alpha_1[2,1,0,1]+\alpha_2[0,0,1,0] + \alpha_3[1,2,1,3] + \alpha_4[3,1,-1,4]$$ Therefore I have to find if this set is linearly independent or not. If it is, then it's a basis for $U+V$. By transforming this to a system, we have: $$\begin{cases}2\alpha_1 + 0\alpha_2 + 1\alpha_3 + 3\alpha_4 = 0\\1\alpha_1 + 0\alpha_2 + 2\alpha_3 + 1\alpha_4 = 0\\0\alpha_1 + 1\alpha_2 + 1\alpha_3 -1\alpha_4 = 0\\1\alpha_1 + 0\alpha_2 + 3\alpha_3 + 4\alpha_4 = 0\end{cases}$$ By solving this system, we should get the answer. Is there a easy way to solve it? For the case $U\cap W$ I can't see how to act. Maybe if i find $\beta_1, \beta_2, \beta_3, \beta_4$ such that  $$\beta_1[2,1,0,1]+\beta_2[0,0,1,0] = \beta_3[1,2,1,3] + \beta_4[3,1,-1,4]$$ I should get what the intersection is?",,"['linear-algebra', 'vector-spaces']"
35,Subspaces of same finite codimension are isomorphic,Subspaces of same finite codimension are isomorphic,,I would like to show that two closed subspaces $Y$ and $Z$ of a normed space $X$ are isomorphic provided $\text{codim } Y = \text{codim } Z <\infty$. I can show that $\text{codim}(Y\cap Z) <\infty$ but I don't know how to use that. Can someone give me a hint.,I would like to show that two closed subspaces $Y$ and $Z$ of a normed space $X$ are isomorphic provided $\text{codim } Y = \text{codim } Z <\infty$. I can show that $\text{codim}(Y\cap Z) <\infty$ but I don't know how to use that. Can someone give me a hint.,,"['linear-algebra', 'functional-analysis', 'normed-spaces']"
36,Center of general linear group [duplicate],Center of general linear group [duplicate],,"This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 9 years ago . Given a (not necessarily finite dimensional) vector space $V$ prove that the center of $\operatorname{GL}(V)$ is the set of all scalar transformations (i.e all transformations of the form $a\operatorname{Id}$)? I know how to prove this for general linear group of degree $n$, please help me solve this for the case of a general linear map.","This question already has answers here : A linear operator commuting with all such operators is a scalar multiple of the identity. (10 answers) Closed 9 years ago . Given a (not necessarily finite dimensional) vector space $V$ prove that the center of $\operatorname{GL}(V)$ is the set of all scalar transformations (i.e all transformations of the form $a\operatorname{Id}$)? I know how to prove this for general linear group of degree $n$, please help me solve this for the case of a general linear map.",,['linear-algebra']
37,Intuitive idea of immersions?,Intuitive idea of immersions?,,"So I understand the definition of immersions and submersions, as well as the motivation for defining such ideas. Not only are they important in understanding properties of mappings of tangent spaces, but they are also one of the few properties that remain invariant under homotopies. However, I am having trouble picturing these types of concepts. For example, how would one know intuitively when a surface can be immersed into the plane $\mathbb{R}^{2}$ without constructing an explicit map and taking the differential to see if it is injective or not? For example, with the punctured tori, how does one see that the construction of it by taking the intersection of two cylinders along a square region is indeed an immersion into the plane? Or that the klein bottle can be immersed in 3-space?","So I understand the definition of immersions and submersions, as well as the motivation for defining such ideas. Not only are they important in understanding properties of mappings of tangent spaces, but they are also one of the few properties that remain invariant under homotopies. However, I am having trouble picturing these types of concepts. For example, how would one know intuitively when a surface can be immersed into the plane $\mathbb{R}^{2}$ without constructing an explicit map and taking the differential to see if it is injective or not? For example, with the punctured tori, how does one see that the construction of it by taking the intersection of two cylinders along a square region is indeed an immersion into the plane? Or that the klein bottle can be immersed in 3-space?",,"['linear-algebra', 'general-topology', 'differential-geometry', 'differential-topology', 'intuition']"
38,Lists versus sets in linear algebra,Lists versus sets in linear algebra,,"I’m currently learning linear algebra from “Linear Algebra Done Right” by Sheldon Axler. The author, in his proofs, makes use of lists of vectors, as opposed to the more conventional usage of sets of vectors. I have some questions concerning this:- Are lists standard in linear algebra? I mean, I have referred to many other books, and all of them seem to use sets rather than lists. If I study linear algebra at a higher level, will my current study using lists prove to be a hindrance? In essence, I mean to ask whether results in higher linear algebra texts make use of lists or rather sets, in their proofs? Aren’t certain results more cumbersome to prove using lists? As an example, consider this statement: If $S$ is a linearly independent set in $V$, and $x \notin span(S)$, then prove that $S \cup \{x\}$ is linearly independent. This is fairly easy to prove considering $S$ as a set. But when it comes to an analogous result for lists, since the order matters, wouldn’t there be many possibilities of adjoining $x$ to the list? And for each of these possibilities, wouldn’t I have to prove that the list is linearly independent? If $x$ is adjoined at the very end of the list, this follows easily from the linear dependence lemma, but what if $x$ is adjoined to the list at some arbitrary position? Wouldn’t statements like this, which involve adjoining (or equally, removal) of vectors be more cumbersome to prove, in the case of lists? Are there other disadvantages of using lists over sets? Frankly, I am in love with Axler’s book and his simple, clean proofs using lists, over other linear algebra texts at this level. But I’m worried that this very simplicity is going to prove troublesome when I decide to study linear algebra at the graduate level. I apologize if this isn’t the right place to post this, but I haven’t seen any discussion regarding this anywhere else, and I thought someone here might be able to give insights.","I’m currently learning linear algebra from “Linear Algebra Done Right” by Sheldon Axler. The author, in his proofs, makes use of lists of vectors, as opposed to the more conventional usage of sets of vectors. I have some questions concerning this:- Are lists standard in linear algebra? I mean, I have referred to many other books, and all of them seem to use sets rather than lists. If I study linear algebra at a higher level, will my current study using lists prove to be a hindrance? In essence, I mean to ask whether results in higher linear algebra texts make use of lists or rather sets, in their proofs? Aren’t certain results more cumbersome to prove using lists? As an example, consider this statement: If $S$ is a linearly independent set in $V$, and $x \notin span(S)$, then prove that $S \cup \{x\}$ is linearly independent. This is fairly easy to prove considering $S$ as a set. But when it comes to an analogous result for lists, since the order matters, wouldn’t there be many possibilities of adjoining $x$ to the list? And for each of these possibilities, wouldn’t I have to prove that the list is linearly independent? If $x$ is adjoined at the very end of the list, this follows easily from the linear dependence lemma, but what if $x$ is adjoined to the list at some arbitrary position? Wouldn’t statements like this, which involve adjoining (or equally, removal) of vectors be more cumbersome to prove, in the case of lists? Are there other disadvantages of using lists over sets? Frankly, I am in love with Axler’s book and his simple, clean proofs using lists, over other linear algebra texts at this level. But I’m worried that this very simplicity is going to prove troublesome when I decide to study linear algebra at the graduate level. I apologize if this isn’t the right place to post this, but I haven’t seen any discussion regarding this anywhere else, and I thought someone here might be able to give insights.",,"['linear-algebra', 'soft-question']"
39,Necessary and sufficient conditions for when spectral radius equals the largest singular value. [duplicate],Necessary and sufficient conditions for when spectral radius equals the largest singular value. [duplicate],,"This question already has answers here : Quick question: matrix with norm equal to spectral radius (4 answers) Closed 8 years ago . One well known fact about matrix norms is the following: If $\lambda_1\geq \dots\geq \lambda_n$ are eigenvalues of a square matrix $A$, then: $$\frac{1}{||A^{-1}||} \leq |\lambda|\leq ||A||$$ If we take our matrix norm to be the matrix 2-norm, and recall that the matrix 2 norm gives us the largest singular value, i.e. $||A||_2=\sigma_1$, then the upper bound implies: $$|\lambda_1|\leq \sigma_1$$ My question is: are there necessary and sufficient conditions for when equality above holds? I vaguely remember something like: $|\lambda_1|=\sigma_1$ iff $\lambda_1$ is non-defective, i.e. its algebraic multiplicity equals its geometric multiplicity. However, I have not been able to find a reference or prove this. Can someone point me to a reference or provide a proof. Thank you in advance for your time.","This question already has answers here : Quick question: matrix with norm equal to spectral radius (4 answers) Closed 8 years ago . One well known fact about matrix norms is the following: If $\lambda_1\geq \dots\geq \lambda_n$ are eigenvalues of a square matrix $A$, then: $$\frac{1}{||A^{-1}||} \leq |\lambda|\leq ||A||$$ If we take our matrix norm to be the matrix 2-norm, and recall that the matrix 2 norm gives us the largest singular value, i.e. $||A||_2=\sigma_1$, then the upper bound implies: $$|\lambda_1|\leq \sigma_1$$ My question is: are there necessary and sufficient conditions for when equality above holds? I vaguely remember something like: $|\lambda_1|=\sigma_1$ iff $\lambda_1$ is non-defective, i.e. its algebraic multiplicity equals its geometric multiplicity. However, I have not been able to find a reference or prove this. Can someone point me to a reference or provide a proof. Thank you in advance for your time.",,"['linear-algebra', 'matrices', 'spectral-theory']"
40,Eigen values of AB and BA,Eigen values of AB and BA,,"let A be a linear transformation from $R^n$ to $R^m$, and B be a linear transformation from $R^m$ to $R^n$, it's easy to show that AB and BA has same eigen-value(except $0$). But my question is  how to show that the multiplicity of eigen-values are the same? can anyone give a proof just from the theory of linear transformation? I mean, without matrix computation.","let A be a linear transformation from $R^n$ to $R^m$, and B be a linear transformation from $R^m$ to $R^n$, it's easy to show that AB and BA has same eigen-value(except $0$). But my question is  how to show that the multiplicity of eigen-values are the same? can anyone give a proof just from the theory of linear transformation? I mean, without matrix computation.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
41,Recurrence with varying coefficient,Recurrence with varying coefficient,,"Problem 1 $$ {\rm f}\left(n\right) = \frac{1}{n}\, \left[{\rm f}\left(n - 1\right)k_{0} + {\rm f}\left(n-2\right)k_{1}\right]\tag{1} $$ ( This can also be written as ${\rm Q}\left(n\right) = k_{0}\frac{{\rm Q}\left(n - 1\right)}{n - 1} +k_{1}\frac{{\rm Q}\left(n - 2\right)}{n - 2}$ provided ${\rm Q}\left(n\right) = n{\rm f}\left(n\right) $ ) where $k_{0},\,k_{1}$ are constants. ${\rm f}\left(0\right)=3,\,{\rm f}\left(1\right)=5$. The variable coefficient $\frac{1}{n}$  is making lot of issues here. It blocks the proper expansion. Question Please help me to find the solution of the recurrence  in terms of $n$ ( implies ${\rm f}\left(n\right)$ )  and also the summation of the recurrence up to infinity ( $ sum = \sum_{n = 0}^{\infty}{\rm f}\left(n\right)$ ). Note:: Partial answers are also welcome. We can discuss that here. This recursion forms a structure of Fibonacci look like tree. I have attached a pic here. You can see Red nodes   to indicate $k_0$ multiplication and green to indicate $k_1$ multiplication. You can see the index of the tree expands like a Fibonacci sequence(starts from 5) Some of the generating function smart manipulations can be looked in to this famous e book for reference link:-generatingfunctionology ! I know a method of using ODE. But I am trying to solve it with out ODE so that I can extent this to higher dimension like matrices in similar structure questions. Please avoid ODE solution or any matrix exponential. A method using series would be more desirable Problem 2 ( extended Matrix version of problem 1) We have a given matrix recurrence , $  (\curlyvee_i,\curlyvee_{i-1})_{1\times2}= (\curlyvee_{i-2},\curlyvee_{i-3})_{1\times2}{\begin{bmatrix}A_{i-1}A_i+B_i & A_{i-1} \\B_{i-1}A_i & B_{i-1} \end{bmatrix}}_{2\times2} \tag 1$ $A_i= r\frac{\Im1}{i},B_i= r^2\frac{\Im2}{i} \tag 2$ $   \Im1=\left( \begin{array}{ccc}     0 & -n_0 & m_0 \\    n_0 & 0 & -l_0 \\   -m_0 & l_0 & 0 \\    \end{array} \right),      \Im2=\left( \begin{array}{ccc}     0 & -(n_1-n_0) & (m_1-m_0) \\    (n_1-n_0) & 0 & -(l_1-l_0) \\    -(m_1-m_0) & (l_1-l_0) & 0 \\    \end{array} \right).\tag3$ $l_1,m_1,n_1,l_0,m_0,n_0,r$ all are constants can't alter at all Given Data about the problem Dimensions of $A_i,B_i,\curlyvee_i$ are $3\times3$ and $A_i,B_i$ are skew symmetric matrices with diagonal zero Our initial condition is given as $(\curlyvee_1,\curlyvee_{0})_{1\times3} $ Properties of  $\Im1,\Im2$ useful for simplification are given below For each matrices  $\Im1 (  a=l_0,b=m_0,c=n_0),  \Im2  (  a=l_1-l_0,b=m_1-m_0,c=n_1-n_0)$,let $p = \sqrt{a^2+b^2+c^2}$. It is true that $\wp^3 = -(a^2+b^2+c^2)\wp = -p^2\wp$ ($\wp$ represents matrices $\Im1,   \Im1$ ,select a,b,c accordingly).  Hence, $\wp^{2m+1} = (-1)^mp^{2m}\wp \tag 4$ and $\wp^{2m} = (-1)^{m-1}p^{2m-2}\wp^2 \tag 5$.  In essence it says both matrices  $\Im1,  \Im2$ satisfies this property. courtesy @JimmyK4542 Question 1. How do we solve this recurrence in terms of n. Is there any way to take out  n from the $3\times3$ matrices in recursion so that we can go for easy simplification. ? NB :: I would like to have a solution with out using ODE or matrix exponential as I can extent to tensors etc. Please avoid such solution. Those were discussed earlier. Another version of the same question I posted as graph problem is given here . I didnt get any response to that,so I made it simplified as matrix recursion here. I haven been trying lot of methods on it none is successful coz of the properties of $A_i$ Note :: Bounty will be awarded to the helpful suggestions which leads to solution. Efforts done so far Tried to evaluate using power series simple method. Result:: failed ,Reason ::   varying n blocks the procedure Tried to make another equivalent series by substitution. Result :: failed,Reason :: it still went back to old issues.  (not sure any nice substitution exists such that we can get form that is solvable..trying) Made it as graph problem .I see a finite structure in Tree and node coloring.. If we go with graph theory method to get an expression for general case n and later add all as series up to infinity , we can find a sequence some thing like GP. We may end up in a finite expression. Result :: Failed ,Reason:: It was tough to get that common expression for a graph based on node contribution Made it as matrix recursion, Result :: failed, could not decompose the matrix recursion component to separate n Tried to compare or extract some properties from Fibonacci tree as it has same structure. Result :: Failed,Reason:: we will still end up in varying coefficient ..","Problem 1 $$ {\rm f}\left(n\right) = \frac{1}{n}\, \left[{\rm f}\left(n - 1\right)k_{0} + {\rm f}\left(n-2\right)k_{1}\right]\tag{1} $$ ( This can also be written as ${\rm Q}\left(n\right) = k_{0}\frac{{\rm Q}\left(n - 1\right)}{n - 1} +k_{1}\frac{{\rm Q}\left(n - 2\right)}{n - 2}$ provided ${\rm Q}\left(n\right) = n{\rm f}\left(n\right) $ ) where $k_{0},\,k_{1}$ are constants. ${\rm f}\left(0\right)=3,\,{\rm f}\left(1\right)=5$. The variable coefficient $\frac{1}{n}$  is making lot of issues here. It blocks the proper expansion. Question Please help me to find the solution of the recurrence  in terms of $n$ ( implies ${\rm f}\left(n\right)$ )  and also the summation of the recurrence up to infinity ( $ sum = \sum_{n = 0}^{\infty}{\rm f}\left(n\right)$ ). Note:: Partial answers are also welcome. We can discuss that here. This recursion forms a structure of Fibonacci look like tree. I have attached a pic here. You can see Red nodes   to indicate $k_0$ multiplication and green to indicate $k_1$ multiplication. You can see the index of the tree expands like a Fibonacci sequence(starts from 5) Some of the generating function smart manipulations can be looked in to this famous e book for reference link:-generatingfunctionology ! I know a method of using ODE. But I am trying to solve it with out ODE so that I can extent this to higher dimension like matrices in similar structure questions. Please avoid ODE solution or any matrix exponential. A method using series would be more desirable Problem 2 ( extended Matrix version of problem 1) We have a given matrix recurrence , $  (\curlyvee_i,\curlyvee_{i-1})_{1\times2}= (\curlyvee_{i-2},\curlyvee_{i-3})_{1\times2}{\begin{bmatrix}A_{i-1}A_i+B_i & A_{i-1} \\B_{i-1}A_i & B_{i-1} \end{bmatrix}}_{2\times2} \tag 1$ $A_i= r\frac{\Im1}{i},B_i= r^2\frac{\Im2}{i} \tag 2$ $   \Im1=\left( \begin{array}{ccc}     0 & -n_0 & m_0 \\    n_0 & 0 & -l_0 \\   -m_0 & l_0 & 0 \\    \end{array} \right),      \Im2=\left( \begin{array}{ccc}     0 & -(n_1-n_0) & (m_1-m_0) \\    (n_1-n_0) & 0 & -(l_1-l_0) \\    -(m_1-m_0) & (l_1-l_0) & 0 \\    \end{array} \right).\tag3$ $l_1,m_1,n_1,l_0,m_0,n_0,r$ all are constants can't alter at all Given Data about the problem Dimensions of $A_i,B_i,\curlyvee_i$ are $3\times3$ and $A_i,B_i$ are skew symmetric matrices with diagonal zero Our initial condition is given as $(\curlyvee_1,\curlyvee_{0})_{1\times3} $ Properties of  $\Im1,\Im2$ useful for simplification are given below For each matrices  $\Im1 (  a=l_0,b=m_0,c=n_0),  \Im2  (  a=l_1-l_0,b=m_1-m_0,c=n_1-n_0)$,let $p = \sqrt{a^2+b^2+c^2}$. It is true that $\wp^3 = -(a^2+b^2+c^2)\wp = -p^2\wp$ ($\wp$ represents matrices $\Im1,   \Im1$ ,select a,b,c accordingly).  Hence, $\wp^{2m+1} = (-1)^mp^{2m}\wp \tag 4$ and $\wp^{2m} = (-1)^{m-1}p^{2m-2}\wp^2 \tag 5$.  In essence it says both matrices  $\Im1,  \Im2$ satisfies this property. courtesy @JimmyK4542 Question 1. How do we solve this recurrence in terms of n. Is there any way to take out  n from the $3\times3$ matrices in recursion so that we can go for easy simplification. ? NB :: I would like to have a solution with out using ODE or matrix exponential as I can extent to tensors etc. Please avoid such solution. Those were discussed earlier. Another version of the same question I posted as graph problem is given here . I didnt get any response to that,so I made it simplified as matrix recursion here. I haven been trying lot of methods on it none is successful coz of the properties of $A_i$ Note :: Bounty will be awarded to the helpful suggestions which leads to solution. Efforts done so far Tried to evaluate using power series simple method. Result:: failed ,Reason ::   varying n blocks the procedure Tried to make another equivalent series by substitution. Result :: failed,Reason :: it still went back to old issues.  (not sure any nice substitution exists such that we can get form that is solvable..trying) Made it as graph problem .I see a finite structure in Tree and node coloring.. If we go with graph theory method to get an expression for general case n and later add all as series up to infinity , we can find a sequence some thing like GP. We may end up in a finite expression. Result :: Failed ,Reason:: It was tough to get that common expression for a graph based on node contribution Made it as matrix recursion, Result :: failed, could not decompose the matrix recursion component to separate n Tried to compare or extract some properties from Fibonacci tree as it has same structure. Result :: Failed,Reason:: we will still end up in varying coefficient ..",,"['linear-algebra', 'discrete-mathematics', 'graph-theory', 'algorithms', 'recursive-algorithms']"
42,What is the name of the matrix that is created by a vector times its transpose.,What is the name of the matrix that is created by a vector times its transpose.,,I am looking for the name of the matrix created by the following operation: $Z = z*z^T$ I know it should create a symmetric matrix with an element $Z_{ij} = z_{i}z_{j}$,I am looking for the name of the matrix created by the following operation: $Z = z*z^T$ I know it should create a symmetric matrix with an element $Z_{ij} = z_{i}z_{j}$,,"['linear-algebra', 'matrices', 'terminology']"
43,Linear Algebra without Matrices,Linear Algebra without Matrices,,"How far could one get in linear algebra without matrices?  It seems like the more I learn, the less I actually use them, but most of the basic theorems and invariants that learned first -- and still use -- were defined via matrices.  Could linear algebra be done without matrices at all?  Is there a book that takes this approach?  I'm just curious how one would go about it.","How far could one get in linear algebra without matrices?  It seems like the more I learn, the less I actually use them, but most of the basic theorems and invariants that learned first -- and still use -- were defined via matrices.  Could linear algebra be done without matrices at all?  Is there a book that takes this approach?  I'm just curious how one would go about it.",,"['linear-algebra', 'soft-question']"
44,Distance of a vector from a subspace - Linear Algebra,Distance of a vector from a subspace - Linear Algebra,,"I want to calculate the distance of the vector $x=(1,1,1,1)$ to the subspace $\{(1,0,2,0) , (0,1,0,2)\}$ I have solved this in 2 ways that I know of but the thing is, the results are different. For instance when I use $||x-Pr(x)||$ I get $\sqrt{2}$, but when I calculate it using the gram determinant (more info here: Distance between a point and a m-dimensional space in n-dimensional space ($m<n$) )  I get $\sqrt{\frac{10}{25}}$ which is weird because both ways should be equivalent. So my question is, what am I missing here? Which one of these result was the correct one? Thank you. Edit: Solution using $||x-Pr(x)||$ : Let $v_1$ be $(1,0,2,0)$ and $v_2$ be $(0,1,0,2)$ Both vectors $v_1$ and $v_2$ are orthogonal meaning the inner product of them is $0$. Now we need to make them orthonormal. After doing so we get $e_1=v_1/\sqrt{3}$ and $e_2=v_2/\sqrt{3}$ Now we calculate $\Pr(x)$. $$\Pr(x)= (x,e_1)e_1+(x,e_2)e_2 =\ldots= (1,1,2,2)$$ Therefore the distance is $d(x,U)= ||x-\Pr(x)||=||(1,1,1,1)-(1,1,2,2)||= ||(0,0,-1,-1)||= \sqrt{2}$","I want to calculate the distance of the vector $x=(1,1,1,1)$ to the subspace $\{(1,0,2,0) , (0,1,0,2)\}$ I have solved this in 2 ways that I know of but the thing is, the results are different. For instance when I use $||x-Pr(x)||$ I get $\sqrt{2}$, but when I calculate it using the gram determinant (more info here: Distance between a point and a m-dimensional space in n-dimensional space ($m<n$) )  I get $\sqrt{\frac{10}{25}}$ which is weird because both ways should be equivalent. So my question is, what am I missing here? Which one of these result was the correct one? Thank you. Edit: Solution using $||x-Pr(x)||$ : Let $v_1$ be $(1,0,2,0)$ and $v_2$ be $(0,1,0,2)$ Both vectors $v_1$ and $v_2$ are orthogonal meaning the inner product of them is $0$. Now we need to make them orthonormal. After doing so we get $e_1=v_1/\sqrt{3}$ and $e_2=v_2/\sqrt{3}$ Now we calculate $\Pr(x)$. $$\Pr(x)= (x,e_1)e_1+(x,e_2)e_2 =\ldots= (1,1,2,2)$$ Therefore the distance is $d(x,U)= ||x-\Pr(x)||=||(1,1,1,1)-(1,1,2,2)||= ||(0,0,-1,-1)||= \sqrt{2}$",,['linear-algebra']
45,"Existence of $p \times p $ matrices $A$ and $B$ over the field $\mathbb F_p$, $p$ prime, such that $AB-BA=I$. [duplicate]","Existence of  matrices  and  over the field ,  prime, such that . [duplicate]",p \times p  A B \mathbb F_p p AB-BA=I,This question already has answers here : Solutions to the matrix equation $\mathbf{AB-BA=I}$ over general fields (4 answers) Closed 9 years ago . Let $p$  be a prime number. Prove or disprove that there exists $p\times p$ matrices $A$ and $B$ over a field $\mathbb F_p$ with $AB-BA = I$. With the aid of MAPLE i was able to find out that this statement is valid for $2$ and $3$. But hadn't been able to prove it theoretically.  This is a homework question and we would receive extra marks if solved. Any hint(s) will be appreciated. Thank you.,This question already has answers here : Solutions to the matrix equation $\mathbf{AB-BA=I}$ over general fields (4 answers) Closed 9 years ago . Let $p$  be a prime number. Prove or disprove that there exists $p\times p$ matrices $A$ and $B$ over a field $\mathbb F_p$ with $AB-BA = I$. With the aid of MAPLE i was able to find out that this statement is valid for $2$ and $3$. But hadn't been able to prove it theoretically.  This is a homework question and we would receive extra marks if solved. Any hint(s) will be appreciated. Thank you.,,"['linear-algebra', 'abstract-algebra', 'matrices']"
46,Show that $V = U^\perp \bigoplus U$,Show that,V = U^\perp \bigoplus U,"If $(V,\langle , \rangle)$ is a Euclidean vector space, $U \subseteq V$ is a subspace of V and $U^\perp := \{v \in V | \langle v,u \rangle = 0, \forall u \in U\}$. Show $V = U^\perp  \bigoplus U$ In the first part I proved that $U^\perp$ is a linear subspace of V already. I need to prove that $V = U^\perp + U$ and $U^\perp \cap U = \{0\}$. For $U^\perp \cap U = \{0\}$ I have: $0 \in U^\perp \cap U$ since $U,U^\perp$ are subspaces of V. Also if $u \in U^\perp \cap U \Rightarrow \langle u,u \rangle = 0$, but by the definition of the inner product it follows that $u=0$. But how can I prove that $V = U^\perp + U$?","If $(V,\langle , \rangle)$ is a Euclidean vector space, $U \subseteq V$ is a subspace of V and $U^\perp := \{v \in V | \langle v,u \rangle = 0, \forall u \in U\}$. Show $V = U^\perp  \bigoplus U$ In the first part I proved that $U^\perp$ is a linear subspace of V already. I need to prove that $V = U^\perp + U$ and $U^\perp \cap U = \{0\}$. For $U^\perp \cap U = \{0\}$ I have: $0 \in U^\perp \cap U$ since $U,U^\perp$ are subspaces of V. Also if $u \in U^\perp \cap U \Rightarrow \langle u,u \rangle = 0$, but by the definition of the inner product it follows that $u=0$. But how can I prove that $V = U^\perp + U$?",,"['linear-algebra', 'inner-products']"
47,orthogonal projection onto orthogonal complement,orthogonal projection onto orthogonal complement,,"If $V=M \oplus M^{\perp}$. For any $v\in V$, the orthogonal projection of $v$ onto $M$ along $M^{\perp}$ is well defined.  Can we take the orthogonal projection of $v$ onto $M^{\perp}$ along $M$?","If $V=M \oplus M^{\perp}$. For any $v\in V$, the orthogonal projection of $v$ onto $M$ along $M^{\perp}$ is well defined.  Can we take the orthogonal projection of $v$ onto $M^{\perp}$ along $M$?",,"['linear-algebra', 'functional-analysis']"
48,What's the Jordan canonical form of this matrix?,What's the Jordan canonical form of this matrix?,,given is the $6 \times 6$-matrix $A$: $A = \begin{pmatrix} 0 & 1 & 0 & -1 & 0 & 0 \\ 0 &0&1&1&-1&0\\ -1&0&0&0&-1&-1 \\ 1 & 0&0&0&1&0 \\ 0&1&0&0&0&1 \\ 0&0&1&1&0&0 \end{pmatrix}$ With only the information that $A$ has exactly two different eigenvalues one eigenvalue is $t_1 = i$ I have to determine the Jordan-matrix. How can I do this with only the information of $t_1 = i$ ?,given is the $6 \times 6$-matrix $A$: $A = \begin{pmatrix} 0 & 1 & 0 & -1 & 0 & 0 \\ 0 &0&1&1&-1&0\\ -1&0&0&0&-1&-1 \\ 1 & 0&0&0&1&0 \\ 0&1&0&0&0&1 \\ 0&0&1&1&0&0 \end{pmatrix}$ With only the information that $A$ has exactly two different eigenvalues one eigenvalue is $t_1 = i$ I have to determine the Jordan-matrix. How can I do this with only the information of $t_1 = i$ ?,,"['linear-algebra', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
49,Eigenvalues of $ADA^T$,Eigenvalues of,ADA^T,"Consider a rectangular matrix $A\in\mathbb{R}^{M\times N}$ and a diagonal matrix $D\in\mathbb{R}^{N\times N}$. What can one say on the eigenvalues and eigenvectors of $ADA^T$? For example, if we denote $\{d_i\}_{i=1..N}$ the diagonal components of $D$ and $A=U\Sigma V^T$ is its singular values decompositions, can you express the eigenvalues and eigenvectors of $ADA^T$ in some simple way using $D,U,\Sigma,V$? Thanks!","Consider a rectangular matrix $A\in\mathbb{R}^{M\times N}$ and a diagonal matrix $D\in\mathbb{R}^{N\times N}$. What can one say on the eigenvalues and eigenvectors of $ADA^T$? For example, if we denote $\{d_i\}_{i=1..N}$ the diagonal components of $D$ and $A=U\Sigma V^T$ is its singular values decompositions, can you express the eigenvalues and eigenvectors of $ADA^T$ in some simple way using $D,U,\Sigma,V$? Thanks!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'svd']"
50,Eigenvalues and multiplication by diagonal matrices,Eigenvalues and multiplication by diagonal matrices,,"I have $n \times n$ real matrices $A$ and $D$. $D$ is diagonal. Let's $v_i(A), \lambda_i(A)$ be a couple of eigenvectors-eigenvalues of $A$. What relationships there exists between $v_i(B), \lambda_i(B)$ and $v_i(A), \lambda_i(A)$ where $B = DA$?","I have $n \times n$ real matrices $A$ and $D$. $D$ is diagonal. Let's $v_i(A), \lambda_i(A)$ be a couple of eigenvectors-eigenvalues of $A$. What relationships there exists between $v_i(B), \lambda_i(B)$ and $v_i(A), \lambda_i(A)$ where $B = DA$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
51,Finding a matrix with respect to a basis,Finding a matrix with respect to a basis,,"Let $T: R^2 \to R^2$ be represented by $\begin{bmatrix}5 & -3\\2 & -2\end{bmatrix}$ with respect to the standard basis. Find the matrix T with respect to the basis B = { $\begin{bmatrix}3 \\1\end{bmatrix}$ , $\begin{bmatrix}1\\2 \end{bmatrix}$ }. I found T$\begin{bmatrix}3 \\1\end{bmatrix}$ and T$\begin{bmatrix}1\\2 \end{bmatrix}$ by: $\begin{bmatrix}5 & -3\\2 & -2\end{bmatrix}$$\begin{bmatrix}3 \\1\end{bmatrix}$ = $\begin{bmatrix}12 \\4\end{bmatrix}$ and $\begin{bmatrix}5 & -3\\2 & -2\end{bmatrix}$$\begin{bmatrix}1 \\2\end{bmatrix}$ = $\begin{bmatrix}-1 \\-2\end{bmatrix}$ so that $[T]_B$ = $\begin{bmatrix}12 & -1\\4 & -2\end{bmatrix}$ but I'm not sure if this is correct. Am I doing the right thing or are my steps wrong?","Let $T: R^2 \to R^2$ be represented by $\begin{bmatrix}5 & -3\\2 & -2\end{bmatrix}$ with respect to the standard basis. Find the matrix T with respect to the basis B = { $\begin{bmatrix}3 \\1\end{bmatrix}$ , $\begin{bmatrix}1\\2 \end{bmatrix}$ }. I found T$\begin{bmatrix}3 \\1\end{bmatrix}$ and T$\begin{bmatrix}1\\2 \end{bmatrix}$ by: $\begin{bmatrix}5 & -3\\2 & -2\end{bmatrix}$$\begin{bmatrix}3 \\1\end{bmatrix}$ = $\begin{bmatrix}12 \\4\end{bmatrix}$ and $\begin{bmatrix}5 & -3\\2 & -2\end{bmatrix}$$\begin{bmatrix}1 \\2\end{bmatrix}$ = $\begin{bmatrix}-1 \\-2\end{bmatrix}$ so that $[T]_B$ = $\begin{bmatrix}12 & -1\\4 & -2\end{bmatrix}$ but I'm not sure if this is correct. Am I doing the right thing or are my steps wrong?",,['linear-algebra']
52,How is b not a linear combination of these vectors?,How is b not a linear combination of these vectors?,,"Determine if $\vec b$ is a linear combination of $\vec a_1,\vec a_2,\vec a_3$. $\vec a_1 = \left[\begin{array}{c} 1     \\ -2   \\ 0    \\ \end{array}\right], \vec a_2 = \left[\begin{array}{c} 0    \\ 1   \\ 2    \\ \end{array}\right], \vec a_3=\left[\begin{array}{c} 5     \\ -6   \\ 8    \\ \end{array}\right], \vec{b} = \left[\begin{array}{c} 2    \\ -1   \\ 6    \\ \end{array}\right]$ Okay, so I made my constants $x_{1}, x_{2}, x_{3}$ for $\vec a_1, \vec a_2,\vec a_3$, respectively. I end up getting the following consistent system: $$\left[\begin{array}{cccc} 1 & 0 & 5 & 2    \\ 0 & 1 & 4 & 3   \\ 0 & 0 & 0 & 0    \\ \end{array}\right]$$ Which has the general solution: $$ \begin{cases} x_{1} = 2 - 5x_{3} \\ x_{2} = 3 - 4x_{3} \\ x_{3} = \text{free}. \end{cases} $$ So $\vec b$ is equal to infinitely  many linear combinations of $\vec a_1,\vec a_2,\vec a_3$, right? Why does my book say that $\vec b$ is not a linear combination of these three vectors? Must the constants be a unique solution?","Determine if $\vec b$ is a linear combination of $\vec a_1,\vec a_2,\vec a_3$. $\vec a_1 = \left[\begin{array}{c} 1     \\ -2   \\ 0    \\ \end{array}\right], \vec a_2 = \left[\begin{array}{c} 0    \\ 1   \\ 2    \\ \end{array}\right], \vec a_3=\left[\begin{array}{c} 5     \\ -6   \\ 8    \\ \end{array}\right], \vec{b} = \left[\begin{array}{c} 2    \\ -1   \\ 6    \\ \end{array}\right]$ Okay, so I made my constants $x_{1}, x_{2}, x_{3}$ for $\vec a_1, \vec a_2,\vec a_3$, respectively. I end up getting the following consistent system: $$\left[\begin{array}{cccc} 1 & 0 & 5 & 2    \\ 0 & 1 & 4 & 3   \\ 0 & 0 & 0 & 0    \\ \end{array}\right]$$ Which has the general solution: $$ \begin{cases} x_{1} = 2 - 5x_{3} \\ x_{2} = 3 - 4x_{3} \\ x_{3} = \text{free}. \end{cases} $$ So $\vec b$ is equal to infinitely  many linear combinations of $\vec a_1,\vec a_2,\vec a_3$, right? Why does my book say that $\vec b$ is not a linear combination of these three vectors? Must the constants be a unique solution?",,['linear-algebra']
53,"Why aren't these two matrices conjugate in SL(2, $\mathbb{R}$)?","Why aren't these two matrices conjugate in SL(2, )?",\mathbb{R},"Can you please give me a ""good"" reason that the following two matrices are not conjugate in $SL(2, \mathbb{R})$?  I'm sure I could prove it with a computation but I'd like to know why they're not conjugate. For instance, I hope there is some sort of conjugate-invariant property of one that is not a property of the other. Thank you. $$\left(\begin{matrix} 1&1\\ 0&1 \end{matrix}\right)$$ $$\left(\begin{matrix} 1&-1\\ 0&1\\ \end{matrix}\right) $$","Can you please give me a ""good"" reason that the following two matrices are not conjugate in $SL(2, \mathbb{R})$?  I'm sure I could prove it with a computation but I'd like to know why they're not conjugate. For instance, I hope there is some sort of conjugate-invariant property of one that is not a property of the other. Thank you. $$\left(\begin{matrix} 1&1\\ 0&1 \end{matrix}\right)$$ $$\left(\begin{matrix} 1&-1\\ 0&1\\ \end{matrix}\right) $$",,"['linear-algebra', 'abstract-algebra']"
54,"What conditions must the constants b1,b2 and b3 satisfy so that the system below has a solution","What conditions must the constants b1,b2 and b3 satisfy so that the system below has a solution",,$$x_1 + 2x_2 + 3x_3 = b_1 \\ 2x_1 + 5x_2 + 3x_3 = b_2 \\ x_2 - 3x_3 = b_3$$ Use Gauss method: $-2p_1+p_2$ to produce $x_2-x_3=-2b_1+b_2$. $p_1+p_3$ to produce $x_1+3x_2=b_1+b_3$ $-p_2+p_1$ to produce $-x_1-3x_2= b_1-b_2$ The above produces the system: the first row $-x_1-3x_2=b_1-b_2$ second row $x_2-x_3=-2b_1+b_2$ third row $x_1+3x_2=b_1+b_3$ Do Gauss method again. $p_1+p_3$ to produce $0=2b_1-b_2+b_3$ Since I got a homogeneous equation from the above. I can just stop and rewrite the system. first row $-x_1-3x_2=b_1-b_2$ second row $x_2-x_3=-2b_1+b_2$ third row $0 = 2b_1-b_2+b_3.$ Write the homogeneous equation in terms of $b_3. b_3 = -2b_1+b_2$ So I can say the system is consistent if and only if $b_3=-2b_1+b_2.$ May I get a verification of my answer?,$$x_1 + 2x_2 + 3x_3 = b_1 \\ 2x_1 + 5x_2 + 3x_3 = b_2 \\ x_2 - 3x_3 = b_3$$ Use Gauss method: $-2p_1+p_2$ to produce $x_2-x_3=-2b_1+b_2$. $p_1+p_3$ to produce $x_1+3x_2=b_1+b_3$ $-p_2+p_1$ to produce $-x_1-3x_2= b_1-b_2$ The above produces the system: the first row $-x_1-3x_2=b_1-b_2$ second row $x_2-x_3=-2b_1+b_2$ third row $x_1+3x_2=b_1+b_3$ Do Gauss method again. $p_1+p_3$ to produce $0=2b_1-b_2+b_3$ Since I got a homogeneous equation from the above. I can just stop and rewrite the system. first row $-x_1-3x_2=b_1-b_2$ second row $x_2-x_3=-2b_1+b_2$ third row $0 = 2b_1-b_2+b_3.$ Write the homogeneous equation in terms of $b_3. b_3 = -2b_1+b_2$ So I can say the system is consistent if and only if $b_3=-2b_1+b_2.$ May I get a verification of my answer?,,['linear-algebra']
55,Orthonormal Basis for Hilbert Spaces,Orthonormal Basis for Hilbert Spaces,,"The following is the definition of orthonormal base that I am using: The notion of an orthonormal basis from linear algebra generalizes over to the case of Hilbert spaces. In a Hilbert space H, an orthonormal basis is a family $\{e_{k}\}_{k} ∈ B$ of elements of $H$ satisfying the conditions: Orthogonality: $\langle e_{k}, e_{j}\rangle = 0$ for all $k,j$ in $B$ with $k \neq j$. Normalization: $||e_{k}|| = 1$ for all $k,j$ in $B$ with $k \neq j$. Completeness: The linear span of the family $e_{k}$ with $k \in B$ is dense in $H$. A system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set. Such a system is always linearly independent. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as: if $\langle v, e_{k}\rangle = 0$ for all $k \in B$ and some $v \in H$ then $v = 0$. This is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if $S$ is any orthonormal set and $v$ is orthogonal to $S$, then $v$ is orthogonal to the closure of the linear span of $S$, which is the whole space. In the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of linear algebra; to distinguish the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique. Questions: 1.How does it follow ""$v$ is orthogonal to the closure of the linear span of $S$"" in the explanation of the restated definition of completeness? 2.In the last paragraph it states that ""The span of the basis vectors is dense in $H$ implies that every vector in the space can be written as the sum of an infinite series. How does this coincide with the usual topological definition of dense being that the closure of a subset gives you the whole space if a subset is dense. Are the two definitions equivalent? Thanks a lot for assistance!","The following is the definition of orthonormal base that I am using: The notion of an orthonormal basis from linear algebra generalizes over to the case of Hilbert spaces. In a Hilbert space H, an orthonormal basis is a family $\{e_{k}\}_{k} ∈ B$ of elements of $H$ satisfying the conditions: Orthogonality: $\langle e_{k}, e_{j}\rangle = 0$ for all $k,j$ in $B$ with $k \neq j$. Normalization: $||e_{k}|| = 1$ for all $k,j$ in $B$ with $k \neq j$. Completeness: The linear span of the family $e_{k}$ with $k \in B$ is dense in $H$. A system of vectors satisfying the first two conditions basis is called an orthonormal system or an orthonormal set. Such a system is always linearly independent. Completeness of an orthonormal system of vectors of a Hilbert space can be equivalently restated as: if $\langle v, e_{k}\rangle = 0$ for all $k \in B$ and some $v \in H$ then $v = 0$. This is related to the fact that the only vector orthogonal to a dense linear subspace is the zero vector, for if $S$ is any orthonormal set and $v$ is orthogonal to $S$, then $v$ is orthogonal to the closure of the linear span of $S$, which is the whole space. In the infinite-dimensional case, an orthonormal basis will not be a basis in the sense of linear algebra; to distinguish the two, the latter basis is also called a Hamel basis. That the span of the basis vectors is dense implies that every vector in the space can be written as the sum of an infinite series, and the orthogonality implies that this decomposition is unique. Questions: 1.How does it follow ""$v$ is orthogonal to the closure of the linear span of $S$"" in the explanation of the restated definition of completeness? 2.In the last paragraph it states that ""The span of the basis vectors is dense in $H$ implies that every vector in the space can be written as the sum of an infinite series. How does this coincide with the usual topological definition of dense being that the closure of a subset gives you the whole space if a subset is dense. Are the two definitions equivalent? Thanks a lot for assistance!",,"['linear-algebra', 'functional-analysis']"
56,How do I find the Jordan normal form of a matrix with complex eigenvalues?,How do I find the Jordan normal form of a matrix with complex eigenvalues?,,"I'm trying to obtain the Jordan normal form and the transformation matrix for the following matrix: $A = \begin{pmatrix} 1 & 0 & 0 & 0 \\\ 1 & 0 & 0 & 1 \\\ 0 & 1 & 0 & 0 \\\ 0 & 0 & 1 & 0 \end{pmatrix}$ I've calculated its characteristic and minimum polynomials as $(λ - 1)^2(λ^2 + λ + 1)$, and thus the eigenvalues are $λ = 1$ (with an algebraic multiplicity of $2$) and $λ = \frac{-1 \pm i\sqrt{3}}{2}$. An eigenvector for $λ = 1$ is $\begin{pmatrix} 0 \\\ 1 \\\ 1 \\\ 1 \end{pmatrix}$. Since the minimum polynomial contains two identical factors, there must be at least a $2 x 2$ Jordan block associated with the eigenvalue $λ = 1$, and so the Jordan normal form must look something like the following: $A = \begin{pmatrix} 1 & 1 & 0 & 0 \\\ 0 & 1 & 0 & 0 \\\ 0 & 0 & \frac{-1 + i\sqrt{3}}{2} & 0 \\\ 0 & 0 & 0 & \frac{-1 - i\sqrt{3}}{2} \end{pmatrix}$ However, I don't know how to derive a transformation matrix $P$ such that $PJ = AP$. How would I go about solving for $P$?","I'm trying to obtain the Jordan normal form and the transformation matrix for the following matrix: $A = \begin{pmatrix} 1 & 0 & 0 & 0 \\\ 1 & 0 & 0 & 1 \\\ 0 & 1 & 0 & 0 \\\ 0 & 0 & 1 & 0 \end{pmatrix}$ I've calculated its characteristic and minimum polynomials as $(λ - 1)^2(λ^2 + λ + 1)$, and thus the eigenvalues are $λ = 1$ (with an algebraic multiplicity of $2$) and $λ = \frac{-1 \pm i\sqrt{3}}{2}$. An eigenvector for $λ = 1$ is $\begin{pmatrix} 0 \\\ 1 \\\ 1 \\\ 1 \end{pmatrix}$. Since the minimum polynomial contains two identical factors, there must be at least a $2 x 2$ Jordan block associated with the eigenvalue $λ = 1$, and so the Jordan normal form must look something like the following: $A = \begin{pmatrix} 1 & 1 & 0 & 0 \\\ 0 & 1 & 0 & 0 \\\ 0 & 0 & \frac{-1 + i\sqrt{3}}{2} & 0 \\\ 0 & 0 & 0 & \frac{-1 - i\sqrt{3}}{2} \end{pmatrix}$ However, I don't know how to derive a transformation matrix $P$ such that $PJ = AP$. How would I go about solving for $P$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
57,A family of commuting endomorphisms is semisimple if each element is semisimple,A family of commuting endomorphisms is semisimple if each element is semisimple,,"If $\phi : V \rightarrow V$ is an endomorphism of a finite-dimensional (say real) vector space, $\phi$ is called ""semisimple"" if any $\phi$-invariant subspace of $V$ has a complimentary $\phi$-invariant subspace ($W \subset V$ is $\phi$-invariant if $\phi (W) \subset W$). We say that a collection $\Phi= \{ \phi_i\}_{i \in I}$ of such endomorphisms is semisimple if any subspace which is invariant under every element of $\Phi$ has a complimentary subspace which is invariant under every element of $\Phi$. How can I prove that "" If $\Phi$ is a commuting family of endomorphisms then $\Phi$ is semisimple if each $\phi_i$ is semisimple ""? Thank you for your help.","If $\phi : V \rightarrow V$ is an endomorphism of a finite-dimensional (say real) vector space, $\phi$ is called ""semisimple"" if any $\phi$-invariant subspace of $V$ has a complimentary $\phi$-invariant subspace ($W \subset V$ is $\phi$-invariant if $\phi (W) \subset W$). We say that a collection $\Phi= \{ \phi_i\}_{i \in I}$ of such endomorphisms is semisimple if any subspace which is invariant under every element of $\Phi$ has a complimentary subspace which is invariant under every element of $\Phi$. How can I prove that "" If $\Phi$ is a commuting family of endomorphisms then $\Phi$ is semisimple if each $\phi_i$ is semisimple ""? Thank you for your help.",,"['linear-algebra', 'abstract-algebra', 'representation-theory']"
58,Questions on positive definite matrices,Questions on positive definite matrices,,"First, in this discussion, I am only considering real matrices.  Second, I have a few questions I am ruminating on related to symmetric matrices.  Some of these questions I need someone to say my logic is correct while others I need you to help provide the logic. I am curious as to what the closure of the set of positive definite matrices is.  I know that this set is an open cone.  Just thinking about it seems to point to the closure being the positive semi-definite matrices.  However, I cannot think of a way to prove this.  Also, is the closure in the set of symmetric matrices, the same as the closure in the set of all matrices.  I believe this to be the same as any sequence of symmetric matrices should be a symmetric matrix. Also, is every symmetric matrix, either positive definite, negative definite or in the boundary of both?  Again this seems somewhat logical but I don't know why. Finally, if the closure of the positive definite matrices is the positive semi-definite ones, do all the matrices in the boundary have determinant zero?  This seems true as I believe these matrices have at least one eigenvalue as $0$ implying $0=det(A-\lambda I)= det (A-0I) =det (A)$.  If I was wrong about the closure is there any way to characterize the matrices in the boundary? Thanks!","First, in this discussion, I am only considering real matrices.  Second, I have a few questions I am ruminating on related to symmetric matrices.  Some of these questions I need someone to say my logic is correct while others I need you to help provide the logic. I am curious as to what the closure of the set of positive definite matrices is.  I know that this set is an open cone.  Just thinking about it seems to point to the closure being the positive semi-definite matrices.  However, I cannot think of a way to prove this.  Also, is the closure in the set of symmetric matrices, the same as the closure in the set of all matrices.  I believe this to be the same as any sequence of symmetric matrices should be a symmetric matrix. Also, is every symmetric matrix, either positive definite, negative definite or in the boundary of both?  Again this seems somewhat logical but I don't know why. Finally, if the closure of the positive definite matrices is the positive semi-definite ones, do all the matrices in the boundary have determinant zero?  This seems true as I believe these matrices have at least one eigenvalue as $0$ implying $0=det(A-\lambda I)= det (A-0I) =det (A)$.  If I was wrong about the closure is there any way to characterize the matrices in the boundary? Thanks!",,"['linear-algebra', 'general-topology', 'matrices', 'determinant']"
59,Representation Theory of the Dihedral Group $D_{2n}$,Representation Theory of the Dihedral Group,D_{2n},"So I'm pretty new into Representation Theory having so far covered only a couple of example sheets. I'm thinking about the following question: Suppose we have the group $D_{2n}$ (for clarity this is the dihedral group of order $2n$, as notation can differ between texts). We can describe this group as follows: $$\langle \sigma, \tau | \sigma ^n=1, \tau ^2=1, \tau \sigma \tau = \sigma ^{-1} \rangle$$ We know this is isomorphic to the symmetries of the regular $n$-gon. Question How can we construct a two-dimensional representation $\psi: D_{2n} \rightarrow \text{GL}_2(\mathbb{R})$? Thoughts Do we simply set up a map that takes $\sigma$ to the standard $2 \times 2$ rotation matrix , with $\theta$ being $\frac{2 \pi}{n}$ and $\tau$ to something like the matrix $\left( \begin{array}{ccc} 0 & 1  \\ 1 & 0  \end{array} \right)$? If this is the case, is there any simple way to show it is two-dimensional?","So I'm pretty new into Representation Theory having so far covered only a couple of example sheets. I'm thinking about the following question: Suppose we have the group $D_{2n}$ (for clarity this is the dihedral group of order $2n$, as notation can differ between texts). We can describe this group as follows: $$\langle \sigma, \tau | \sigma ^n=1, \tau ^2=1, \tau \sigma \tau = \sigma ^{-1} \rangle$$ We know this is isomorphic to the symmetries of the regular $n$-gon. Question How can we construct a two-dimensional representation $\psi: D_{2n} \rightarrow \text{GL}_2(\mathbb{R})$? Thoughts Do we simply set up a map that takes $\sigma$ to the standard $2 \times 2$ rotation matrix , with $\theta$ being $\frac{2 \pi}{n}$ and $\tau$ to something like the matrix $\left( \begin{array}{ccc} 0 & 1  \\ 1 & 0  \end{array} \right)$? If this is the case, is there any simple way to show it is two-dimensional?",,"['linear-algebra', 'group-theory', 'representation-theory', 'rotations']"
60,"Low rank SVD, orthogonal projection onto range of A","Low rank SVD, orthogonal projection onto range of A",,"Say rank(A) = r < n. Then $A = U_{r}S_{r}V_{r}^{T}$. I know the orthogonal projection onto $Ran(A)$ should be $P = U_{r}U_{r}^{T}$ but I'm not sure how to show this. $$ P = A(A^{T}A)^{-1}A^{T} $$ If A had full column rank, then this would be easy since both $V$ and $S$ are invertible. But in this reduced rank case, $S_{r}$ is nonsingular but $V_{r}$ is rectangular and thus, not invertible. So I'm unsure how to simplify the $(A^{T}A)^{-1}$ term... Also, any hint on how to show that the projection onto $N(A^{T})$ is $P_{N} = \tilde{U}_{r}\tilde{U}_{r}^{T}$? (with $U = [U_{r} \tilde{U}_{r}]$). I thought it would just be $P_{N} = I - U_{r}U_{r}^{T}$","Say rank(A) = r < n. Then $A = U_{r}S_{r}V_{r}^{T}$. I know the orthogonal projection onto $Ran(A)$ should be $P = U_{r}U_{r}^{T}$ but I'm not sure how to show this. $$ P = A(A^{T}A)^{-1}A^{T} $$ If A had full column rank, then this would be easy since both $V$ and $S$ are invertible. But in this reduced rank case, $S_{r}$ is nonsingular but $V_{r}$ is rectangular and thus, not invertible. So I'm unsure how to simplify the $(A^{T}A)^{-1}$ term... Also, any hint on how to show that the projection onto $N(A^{T})$ is $P_{N} = \tilde{U}_{r}\tilde{U}_{r}^{T}$? (with $U = [U_{r} \tilde{U}_{r}]$). I thought it would just be $P_{N} = I - U_{r}U_{r}^{T}$",,"['linear-algebra', 'matrices', 'svd']"
61,How to think about the change-of-coordinates matrix $P_{\mathcal{C}\leftarrow\mathcal{B}}$,How to think about the change-of-coordinates matrix,P_{\mathcal{C}\leftarrow\mathcal{B}},"I've taken a linear algebra course in the past, but I feel my understanding of coordinate change is very superficial. For example this exercise (4.7.1 from Lay's ""Linear Algebra and its Applications"" 4. edition): Let $\mathcal{B}={\{ \bf{b_1,b_2} \}}$ and $\mathcal{C}={\{ \bf{c_1,c_2} \}}$ be bases for a vector space V, and suppose $\bf{b_1}=6\bf{c_1}-2\bf{c_2}$ and $\bf{b_2}=9\bf{c_1}-4\bf{c_2}$. a. Find the change-of-coordinates matric from $\mathcal{B}$ to $\mathcal{C}$. b. Find $[\bf{x}]_\mathcal{C} =-3\bf{b_1}+2\bf{b_2}$. I can solve a. because I remember that $P_{\mathcal{C}\leftarrow\mathcal{B}}=\bf{[[b_1]_\mathcal{C}\,\,[b_1]_\mathcal{C}]}$ and that $\bf{[b_1]_\mathcal{C}}$ is the 'weights on the linear combination of $\mathcal{C}$-vectors'. I can also solve b. because I remember that $[\bf{x}]_\mathcal{C}=P_{\mathcal{C}\leftarrow\mathcal{B}}[\bf{x}]_\mathcal{B} $ So I know how to apply the equations, but I feel that if I had a good mental model of what's going on here, it would be obvious that $\left(\begin{array}{rr} 6 & 9 \\ -2 & -4 \end{array}\right)$ is the matrix I'm looking for. In my own experience, usually, if I learn something well I can construct the formulas I don't remember from reasoning, but here I fail, so I've literally spent all day trying to think up some analogy that would let me do the abovementioned exercise without remembering the formulas, here's what I've come up with so far: $[\bf{x}]_\mathcal{B} $ is like a recipe for how much of the vectors of $\mathcal{B}$ you need to get to some point. So, how much of $\mathcal{C}$ do you need to get to $\bf{b_1}$? You need $\left[\begin{array}{rr} 6  \\ -2   \end{array}\right]=[\bf{b_1}]_\mathcal{C}$. How much of $\mathcal{C}$ do you need to get to $\bf{b_2}$? You need $\left[\begin{array}{rr} 9  \\ -4   \end{array}\right]=[\bf{b_2}]_\mathcal{C}$. Thus any ""$\mathcal{B}$-recipe"" $\left[\begin{array}{rr} r_1  \\ r_2   \end{array}\right]$ will in our case become a ""$\mathcal{C}$-recipe"" when we multiply it with    $\left(\begin{array}{rr} 6 & 9 \\ -2 & -4 \end{array}\right)$, because the result will tell us ""how much of $\mathcal{C}$ to get to $\bf{b_1}$ and $\bf{b_2}$, and then how much of these two to get to some new point"". But it's been 'how much to use of $\mathcal{C}$-vectors' all the way this time, so our result has to be some $[\bf{x}]_\mathcal{C}$. My question : What is a nice way to think about change between bases when neither is the standard one? Just looking for any way to think about this at any level of sophistication.","I've taken a linear algebra course in the past, but I feel my understanding of coordinate change is very superficial. For example this exercise (4.7.1 from Lay's ""Linear Algebra and its Applications"" 4. edition): Let $\mathcal{B}={\{ \bf{b_1,b_2} \}}$ and $\mathcal{C}={\{ \bf{c_1,c_2} \}}$ be bases for a vector space V, and suppose $\bf{b_1}=6\bf{c_1}-2\bf{c_2}$ and $\bf{b_2}=9\bf{c_1}-4\bf{c_2}$. a. Find the change-of-coordinates matric from $\mathcal{B}$ to $\mathcal{C}$. b. Find $[\bf{x}]_\mathcal{C} =-3\bf{b_1}+2\bf{b_2}$. I can solve a. because I remember that $P_{\mathcal{C}\leftarrow\mathcal{B}}=\bf{[[b_1]_\mathcal{C}\,\,[b_1]_\mathcal{C}]}$ and that $\bf{[b_1]_\mathcal{C}}$ is the 'weights on the linear combination of $\mathcal{C}$-vectors'. I can also solve b. because I remember that $[\bf{x}]_\mathcal{C}=P_{\mathcal{C}\leftarrow\mathcal{B}}[\bf{x}]_\mathcal{B} $ So I know how to apply the equations, but I feel that if I had a good mental model of what's going on here, it would be obvious that $\left(\begin{array}{rr} 6 & 9 \\ -2 & -4 \end{array}\right)$ is the matrix I'm looking for. In my own experience, usually, if I learn something well I can construct the formulas I don't remember from reasoning, but here I fail, so I've literally spent all day trying to think up some analogy that would let me do the abovementioned exercise without remembering the formulas, here's what I've come up with so far: $[\bf{x}]_\mathcal{B} $ is like a recipe for how much of the vectors of $\mathcal{B}$ you need to get to some point. So, how much of $\mathcal{C}$ do you need to get to $\bf{b_1}$? You need $\left[\begin{array}{rr} 6  \\ -2   \end{array}\right]=[\bf{b_1}]_\mathcal{C}$. How much of $\mathcal{C}$ do you need to get to $\bf{b_2}$? You need $\left[\begin{array}{rr} 9  \\ -4   \end{array}\right]=[\bf{b_2}]_\mathcal{C}$. Thus any ""$\mathcal{B}$-recipe"" $\left[\begin{array}{rr} r_1  \\ r_2   \end{array}\right]$ will in our case become a ""$\mathcal{C}$-recipe"" when we multiply it with    $\left(\begin{array}{rr} 6 & 9 \\ -2 & -4 \end{array}\right)$, because the result will tell us ""how much of $\mathcal{C}$ to get to $\bf{b_1}$ and $\bf{b_2}$, and then how much of these two to get to some new point"". But it's been 'how much to use of $\mathcal{C}$-vectors' all the way this time, so our result has to be some $[\bf{x}]_\mathcal{C}$. My question : What is a nice way to think about change between bases when neither is the standard one? Just looking for any way to think about this at any level of sophistication.",,"['linear-algebra', 'intuition']"
62,"linearly independent commuting $2\times 2$ complex matrices (Hoffman Kunzze, Linear algebra, 6.5.2)","linearly independent commuting  complex matrices (Hoffman Kunzze, Linear algebra, 6.5.2)",2\times 2,"Actual Question is: Let $\mathcal{F}$ be a commuting family of $3\times 3$ complex matrices. How many linearly independent matrices can $\mathcal{F}$ contain? what about the $n\times n$ case? (Hoffman Kunzze, Linear algebra, 6.5.2) I have no idea ow to go directly for $n\times n$.. SO, I thought i would try for $2\times 2$ and $3\times 3$ and then generalize. For $2\times 2$ I know that basis of $\mathcal{M_2}=\{2\times 2 ~complex ~ matrices\}$ is $ \left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right),\left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right),\left( \begin{array}{cccc} 0 & 0  \\ 1 & 0 \end{array} \right) ,\left( \begin{array}{cccc} 0 & 0  \\ 0 & 1 \end{array} \right) $ So, what i was thinking is if i can check which matrices commutes in this connection, this would be the linearly independent commuting sets of $2\times 2$ matrices ( this is my guess not very sure) we have $ \left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)\left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right)= \left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right)$ but, $ \left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)\left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right)=\left( \begin{array}{cccc} 0 & 0  \\ 0 & 0 \end{array} \right)$ i dont want to write all other combinations, but, i would write only commuting matrices $\left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)\left( \begin{array}{cccc} 0 & 0  \\ 0 & 1 \end{array} \right)=\left( \begin{array}{cccc} 0 & 0  \\ 0 & 0 \end{array} \right)$ and $\left( \begin{array}{cccc} 0 & 0  \\ 0 & 1 \end{array} \right)\left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)=\left( \begin{array}{cccc} 0 & 0  \\ 0 & 0 \end{array} \right)$ So, I think these two elements must be linearly independent in the set of commuting $2\times 2$ matrices. I am expecting to do the same for general $n\times n$ but this would be cumbersome.. So, I would be thankful if some one can help me out to solve this in detail... atleast for $2\times 2$ ... Thank You.","Actual Question is: Let $\mathcal{F}$ be a commuting family of $3\times 3$ complex matrices. How many linearly independent matrices can $\mathcal{F}$ contain? what about the $n\times n$ case? (Hoffman Kunzze, Linear algebra, 6.5.2) I have no idea ow to go directly for $n\times n$.. SO, I thought i would try for $2\times 2$ and $3\times 3$ and then generalize. For $2\times 2$ I know that basis of $\mathcal{M_2}=\{2\times 2 ~complex ~ matrices\}$ is $ \left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right),\left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right),\left( \begin{array}{cccc} 0 & 0  \\ 1 & 0 \end{array} \right) ,\left( \begin{array}{cccc} 0 & 0  \\ 0 & 1 \end{array} \right) $ So, what i was thinking is if i can check which matrices commutes in this connection, this would be the linearly independent commuting sets of $2\times 2$ matrices ( this is my guess not very sure) we have $ \left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)\left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right)= \left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right)$ but, $ \left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)\left( \begin{array}{cccc} 0 & 1  \\ 0 & 0 \end{array} \right)=\left( \begin{array}{cccc} 0 & 0  \\ 0 & 0 \end{array} \right)$ i dont want to write all other combinations, but, i would write only commuting matrices $\left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)\left( \begin{array}{cccc} 0 & 0  \\ 0 & 1 \end{array} \right)=\left( \begin{array}{cccc} 0 & 0  \\ 0 & 0 \end{array} \right)$ and $\left( \begin{array}{cccc} 0 & 0  \\ 0 & 1 \end{array} \right)\left( \begin{array}{cccc} 1 & 0  \\ 0 & 0 \end{array} \right)=\left( \begin{array}{cccc} 0 & 0  \\ 0 & 0 \end{array} \right)$ So, I think these two elements must be linearly independent in the set of commuting $2\times 2$ matrices. I am expecting to do the same for general $n\times n$ but this would be cumbersome.. So, I would be thankful if some one can help me out to solve this in detail... atleast for $2\times 2$ ... Thank You.",,['linear-algebra']
63,Matrix Norm Bounds,Matrix Norm Bounds,,"A natural consideration of matrix norms is to compare them, and one of the many standard results on the induced 1, 2, and $\infty$-norms indicate that $$\frac{1}{\sqrt{n}}\|A\|_\infty\leq \|A\|_2\leq \sqrt{m}\|A\|_\infty$$ $$\frac{1}{\sqrt{m}}\|A\|_1\leq \|A\|_2\leq \sqrt{n}\|A\|_1$$ This is proven using Cauchy-Schwarz (among other things). I was recently told that for a square matrix, $$\|A\|_2^2\leq \|A\|_1\|A\|_\infty,$$ which seems more intuitively interesting to me as it's not dependent on the size of your matrix. I tried proving this similarly to the above two by seeing how $\|Ae\|_2$ can get bounded, but wasn't making any headway because when splitting off $\|e\|_2$, it would then be dependent on the matrix size (and not what I'm trying to do). Using simply the definitions, I'm not sure how I could relate the max singular value to the max row and column sums.","A natural consideration of matrix norms is to compare them, and one of the many standard results on the induced 1, 2, and $\infty$-norms indicate that $$\frac{1}{\sqrt{n}}\|A\|_\infty\leq \|A\|_2\leq \sqrt{m}\|A\|_\infty$$ $$\frac{1}{\sqrt{m}}\|A\|_1\leq \|A\|_2\leq \sqrt{n}\|A\|_1$$ This is proven using Cauchy-Schwarz (among other things). I was recently told that for a square matrix, $$\|A\|_2^2\leq \|A\|_1\|A\|_\infty,$$ which seems more intuitively interesting to me as it's not dependent on the size of your matrix. I tried proving this similarly to the above two by seeing how $\|Ae\|_2$ can get bounded, but wasn't making any headway because when splitting off $\|e\|_2$, it would then be dependent on the matrix size (and not what I'm trying to do). Using simply the definitions, I'm not sure how I could relate the max singular value to the max row and column sums.",,"['linear-algebra', 'matrices', 'normed-spaces', 'upper-lower-bounds', 'matrix-norms']"
64,Minimal number of multiplications required to invert a 4x4 matrix,Minimal number of multiplications required to invert a 4x4 matrix,,"Four by four real (well, floating point...) matrices are used in computer graphics to represent projections . Sometimes we need to compute their inverses. How many multiplications are required? Valve's Source SDK implements the ""pen&paper"" algorithm: start with a 4x8 matrix whose left hand side is $A$ (the matrix to invert) and whose right hand side is $I_4$ (the 4x4 identity matrix); apply Gauss moves until the left hand side is $I_4$ and the right hand will be $A^{-1}$. This algorithm requires (by inspecting the code) $4 \cdot 8 + 4 \cdot 4 \cdot 8 = 160$ multiplications. Solving $AX = I$ applying Cramer's rule for each element and computing each 3x3 determinant with Sarrus's rule improves on that: we need $6 \cdot 16$ multiplications for the cofactors, then $4$ more for the determinant of the 4x4 matrix, plus $16$ multiplications for the inverse of that, for a total of $6 \cdot 16 + 4 + 16 = 116$ multiplications. Cleverly precalculating products of two elements as in this answer requires (again inspecting the code) $2\cdot 12 + 6 + 4 \cdot 16 = 94$ multiplications. Can we do better? Can we prove we can't?","Four by four real (well, floating point...) matrices are used in computer graphics to represent projections . Sometimes we need to compute their inverses. How many multiplications are required? Valve's Source SDK implements the ""pen&paper"" algorithm: start with a 4x8 matrix whose left hand side is $A$ (the matrix to invert) and whose right hand side is $I_4$ (the 4x4 identity matrix); apply Gauss moves until the left hand side is $I_4$ and the right hand will be $A^{-1}$. This algorithm requires (by inspecting the code) $4 \cdot 8 + 4 \cdot 4 \cdot 8 = 160$ multiplications. Solving $AX = I$ applying Cramer's rule for each element and computing each 3x3 determinant with Sarrus's rule improves on that: we need $6 \cdot 16$ multiplications for the cofactors, then $4$ more for the determinant of the 4x4 matrix, plus $16$ multiplications for the inverse of that, for a total of $6 \cdot 16 + 4 + 16 = 116$ multiplications. Cleverly precalculating products of two elements as in this answer requires (again inspecting the code) $2\cdot 12 + 6 + 4 \cdot 16 = 94$ multiplications. Can we do better? Can we prove we can't?",,['linear-algebra']
65,Understanding how to find a basis for the row space/column space of some matrix A.,Understanding how to find a basis for the row space/column space of some matrix A.,,"I just need some verification on finding the basis for column spaces and row spaces. If I'm given a matrix A and asked to find a basis for the row space, is the following method correct? -Reduce to row echelon form. The rows with leading 1's will be the basis vectors for the row space. When looking for the basis of the column space (given some matrix A), is the following method correct? -Reduce to row echelon form. The columns with leading 1's corresponding to the original matrix A will be the basis vectors for the column space. When looking for bases of row space/column space, there's no need in taking a transpose of the original matrix, right? I just reduce to row echelon and use the reduced matrix to get my basis vectors for the row space, and use the original matrix to correspond my reduced form columns with leading 1's to get the basis for my column space.","I just need some verification on finding the basis for column spaces and row spaces. If I'm given a matrix A and asked to find a basis for the row space, is the following method correct? -Reduce to row echelon form. The rows with leading 1's will be the basis vectors for the row space. When looking for the basis of the column space (given some matrix A), is the following method correct? -Reduce to row echelon form. The columns with leading 1's corresponding to the original matrix A will be the basis vectors for the column space. When looking for bases of row space/column space, there's no need in taking a transpose of the original matrix, right? I just reduce to row echelon and use the reduced matrix to get my basis vectors for the row space, and use the original matrix to correspond my reduced form columns with leading 1's to get the basis for my column space.",,['linear-algebra']
66,Finding minimal polynomial,Finding minimal polynomial,,"Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix, such that $A$ is not of the form $A=c I_n, c \in \mathbb{R}$ and $(A-2I_n)^3 (A-3I_n)^4=0$. Find the minimal polynomial of $m_A(x)$of $A$. I know that $m_A(x) | (x-2)^3(x-3)^4$, but I am stuck here. Any help appreciated.","Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix, such that $A$ is not of the form $A=c I_n, c \in \mathbb{R}$ and $(A-2I_n)^3 (A-3I_n)^4=0$. Find the minimal polynomial of $m_A(x)$of $A$. I know that $m_A(x) | (x-2)^3(x-3)^4$, but I am stuck here. Any help appreciated.",,['linear-algebra']
67,Congruence of invertible skew symmetric matrices,Congruence of invertible skew symmetric matrices,,"I am asking for some hints to solve this exercise. Given an invertible skew symmetric matrix $A$ , then show that there is an invertible matrix $R$ such that $R^T A R = \begin{pmatrix} 0 & I \\ -I & 0 \end{pmatrix}$ , meaning that this is a block matrix that has the identity matrix in two of the four blocks and the lower one with a negative sign. I am completely stuck!","I am asking for some hints to solve this exercise. Given an invertible skew symmetric matrix , then show that there is an invertible matrix such that , meaning that this is a block matrix that has the identity matrix in two of the four blocks and the lower one with a negative sign. I am completely stuck!",A R R^T A R = \begin{pmatrix} 0 & I \\ -I & 0 \end{pmatrix},['linear-algebra']
68,Matrix with orthogonal columns? [duplicate],Matrix with orthogonal columns? [duplicate],,"This question already has answers here : Name for matrices with orthogonal (not necessarily orthonormal) rows (2 answers) Closed 10 years ago . What do we call a matrix whose columns are orthogonal, such as $\begin{bmatrix}3 & 0 & 0 \\ 0 & 0 & 2 \\ 0 & 1 & 0\end{bmatrix}$? Is there a special name for it? I tried searching to no avail.","This question already has answers here : Name for matrices with orthogonal (not necessarily orthonormal) rows (2 answers) Closed 10 years ago . What do we call a matrix whose columns are orthogonal, such as $\begin{bmatrix}3 & 0 & 0 \\ 0 & 0 & 2 \\ 0 & 1 & 0\end{bmatrix}$? Is there a special name for it? I tried searching to no avail.",,"['linear-algebra', 'matrices']"
69,Can the matrix $A=\begin{bmatrix} 0 & 1\\ 3 & 3 \end{bmatrix}$ be diagonalized over $\mathbb{Z}_5$?,Can the matrix  be diagonalized over ?,A=\begin{bmatrix} 0 & 1\\ 3 & 3 \end{bmatrix} \mathbb{Z}_5,"Im stuck on finding eigenvalues that are in the field please help. Given matrix: $$ A=  \left[\begin{matrix} 0 & 1\\ 3 & 3 \end{matrix}\right] $$ whose entries are from $\mathbb{Z}_5 = \{0, 1, 2, 3, 4\}$, find, if possible, matrices $P$ and $D$ over $\mathbb{Z}_5$ such that $P^{−1} AP = D$. I have found the characteristic polynomial: $x^2-3x-3=0$ Since its over $\mathbb{Z}_5$, $x^2-3x-3=x^2+2x+2=0$. But from there I'm not sure how to find the eigenvalues, once I get the eigenvalues that are in the field it will be easy to find the eigenvectors and create the matrix $P$.","Im stuck on finding eigenvalues that are in the field please help. Given matrix: $$ A=  \left[\begin{matrix} 0 & 1\\ 3 & 3 \end{matrix}\right] $$ whose entries are from $\mathbb{Z}_5 = \{0, 1, 2, 3, 4\}$, find, if possible, matrices $P$ and $D$ over $\mathbb{Z}_5$ such that $P^{−1} AP = D$. I have found the characteristic polynomial: $x^2-3x-3=0$ Since its over $\mathbb{Z}_5$, $x^2-3x-3=x^2+2x+2=0$. But from there I'm not sure how to find the eigenvalues, once I get the eigenvalues that are in the field it will be easy to find the eigenvectors and create the matrix $P$.",,"['linear-algebra', 'matrices']"
70,"Formula relating traces of a linear map, its restriction to an invariant subspace and the induced quotient map?","Formula relating traces of a linear map, its restriction to an invariant subspace and the induced quotient map?",,"Let $V$ be a finite-dimensional vector space, $T\colon V \to V$ a linear map and $W \subset V$ a $T$-invariant subspace (i.e., $T(W) \subset W$). Then there is a well-defined induced quotient map $\overline{T}\colon V/W \to V/W$. Now I recall having seen the following determinant formula: $\det(T) = \det(\overline{T}) \det(T|_W)$, where $T|_W\colon W \to W$ is the restriction of $T$ to the subspace $W$ (though I do not remember the proof anymore). Is there some similar formula relating the traces of $T$, $\overline{T}$ and $T|_W$?","Let $V$ be a finite-dimensional vector space, $T\colon V \to V$ a linear map and $W \subset V$ a $T$-invariant subspace (i.e., $T(W) \subset W$). Then there is a well-defined induced quotient map $\overline{T}\colon V/W \to V/W$. Now I recall having seen the following determinant formula: $\det(T) = \det(\overline{T}) \det(T|_W)$, where $T|_W\colon W \to W$ is the restriction of $T$ to the subspace $W$ (though I do not remember the proof anymore). Is there some similar formula relating the traces of $T$, $\overline{T}$ and $T|_W$?",,"['linear-algebra', 'matrices', 'vector-spaces']"
71,"If $x\perp \mathrm{span}\{r_1,\dots,r_p\}$, can we prove $x\notin\mathrm{span}\{v_1,\dots,v_p\}$?","If , can we prove ?","x\perp \mathrm{span}\{r_1,\dots,r_p\} x\notin\mathrm{span}\{v_1,\dots,v_p\}","Notations : For a scalar $a\in\mathbb{R}$, denote  $$\mathrm{sgn}(a)=\left\{           \begin{array}{l l}             1  & \mbox{if } a>0\\             0  & \mbox{if } a=0\\             -1 & \mbox{if } a<0           \end{array}.\right.$$ For a vector $r\in\mathbb{R}^n$, $\mathrm{sgn}(r)$ is element-wise. Now we have a set of vectors $r_1, \dots, r_p\in\mathbb{R}^n$, $p<n$. The corresponding sign vectors are $v_1,\dots,v_p\in\mathbb{R}^n$ satisfying $$\mathrm{sgn}(r_i)=v_i$$ which implies that the elements of $v_i$ can only be $1$, $-1$ or $0$. Question : Given a nonzero vector $x\in\mathbb{R}^n$ satisfying $x\perp \mathrm{span}\{r_1,\dots,r_p\}$, can we prove $x\notin\mathrm{span}\{v_1,\dots,v_p\}$? If not, can you given any counterexamples?","Notations : For a scalar $a\in\mathbb{R}$, denote  $$\mathrm{sgn}(a)=\left\{           \begin{array}{l l}             1  & \mbox{if } a>0\\             0  & \mbox{if } a=0\\             -1 & \mbox{if } a<0           \end{array}.\right.$$ For a vector $r\in\mathbb{R}^n$, $\mathrm{sgn}(r)$ is element-wise. Now we have a set of vectors $r_1, \dots, r_p\in\mathbb{R}^n$, $p<n$. The corresponding sign vectors are $v_1,\dots,v_p\in\mathbb{R}^n$ satisfying $$\mathrm{sgn}(r_i)=v_i$$ which implies that the elements of $v_i$ can only be $1$, $-1$ or $0$. Question : Given a nonzero vector $x\in\mathbb{R}^n$ satisfying $x\perp \mathrm{span}\{r_1,\dots,r_p\}$, can we prove $x\notin\mathrm{span}\{v_1,\dots,v_p\}$? If not, can you given any counterexamples?",,"['linear-algebra', 'matrices']"
72,Exponential function and matrices,Exponential function and matrices,,How do we prove that the exponential function is a bijection between the set $S_n$ of real symmetric matrices with size $n$ and the set $\Sigma_n$ of real symmetric positive definite matrices with size $n$? Thanks.,How do we prove that the exponential function is a bijection between the set $S_n$ of real symmetric matrices with size $n$ and the set $\Sigma_n$ of real symmetric positive definite matrices with size $n$? Thanks.,,[]
73,How do I write a vector as a linear combination of other vectors.,How do I write a vector as a linear combination of other vectors.,,"Write $\begin{pmatrix} 5  \\ 3 \\15 \end{pmatrix}$ as a linerar combination of the following vectors:  $u=\begin{pmatrix} 1  \\ 2 \\5 \end{pmatrix}$, $v=\begin{pmatrix} 3  \\ -4 \\-1 \end{pmatrix}$, $w=\begin{pmatrix} -1  \\ 1 \\1 \end{pmatrix}$. My attempt: $$\begin{bmatrix} 1 & 3& -1 & 5\\ 2 & -4 & 1& 3\\ 5&-1&1&15\\  \end{bmatrix}\sim\to\begin{bmatrix} 1 & 0& 0 & 3\\ 0 & 1 & 0 & 1\\ 0&0&1&1\\ \end{bmatrix}$$ Obviously I skipped a lot of reduction steps, because it's a pain to type matrices here, but I was wondering if I even did the right thing.","Write $\begin{pmatrix} 5  \\ 3 \\15 \end{pmatrix}$ as a linerar combination of the following vectors:  $u=\begin{pmatrix} 1  \\ 2 \\5 \end{pmatrix}$, $v=\begin{pmatrix} 3  \\ -4 \\-1 \end{pmatrix}$, $w=\begin{pmatrix} -1  \\ 1 \\1 \end{pmatrix}$. My attempt: $$\begin{bmatrix} 1 & 3& -1 & 5\\ 2 & -4 & 1& 3\\ 5&-1&1&15\\  \end{bmatrix}\sim\to\begin{bmatrix} 1 & 0& 0 & 3\\ 0 & 1 & 0 & 1\\ 0&0&1&1\\ \end{bmatrix}$$ Obviously I skipped a lot of reduction steps, because it's a pain to type matrices here, but I was wondering if I even did the right thing.",,"['linear-algebra', 'matrices']"
74,Motivation for the development of the double dual of a vector space [duplicate],Motivation for the development of the double dual of a vector space [duplicate],,This question already has answers here : Motivation to understand double dual space (3 answers) Closed 3 years ago . I was reading on the double dual of a vector space $V$ recently. I was wondering what applications (within mathematics) there are for this concept and/or what was the motivation for the development of this theory.,This question already has answers here : Motivation to understand double dual space (3 answers) Closed 3 years ago . I was reading on the double dual of a vector space $V$ recently. I was wondering what applications (within mathematics) there are for this concept and/or what was the motivation for the development of this theory.,,"['linear-algebra', 'soft-question', 'duality-theorems', 'dual-spaces']"
75,How can I break down a rotation of known amount around a known axis into two rotations of unknown amounts around known axes?,How can I break down a rotation of known amount around a known axis into two rotations of unknown amounts around known axes?,,"I have a vector that's been rotated a known amount about a known axis. I would like to break this rotation down into two separate rotations around known, linearly independent axes where the amounts I need to rotate about each of these axes is unknown and need to be calculated. I'm using matrices for my rotations at the moment but I'm happy to use quaternions if that's an easier way to calculate this. I would like a closed form solution but I think there's a good chance that one does not exist. All this takes place in 3-space. Thanks,","I have a vector that's been rotated a known amount about a known axis. I would like to break this rotation down into two separate rotations around known, linearly independent axes where the amounts I need to rotate about each of these axes is unknown and need to be calculated. I'm using matrices for my rotations at the moment but I'm happy to use quaternions if that's an easier way to calculate this. I would like a closed form solution but I think there's a good chance that one does not exist. All this takes place in 3-space. Thanks,",,"['linear-algebra', 'rotations', 'quaternions']"
76,How to prove that a skew-symmetric matrix with integer entries has a determinant that is a square of some integer? [duplicate],How to prove that a skew-symmetric matrix with integer entries has a determinant that is a square of some integer? [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Determinant of a real skew-symmetric matrix is square of an integer I know that in general, a skew-symmetric matrix with indeterminate elements has a determinant that can be written as a square of some multivariable polynomial. How to prove this? And if I do not know anything about the Pfaffian, can I prove the statement that a skew-symmetric matrix with integer entries has a determinant that is a square of some integer? I mean if someone knows how to prove it without using the Pfaffian?","This question already has answers here : Closed 11 years ago . Possible Duplicate: Determinant of a real skew-symmetric matrix is square of an integer I know that in general, a skew-symmetric matrix with indeterminate elements has a determinant that can be written as a square of some multivariable polynomial. How to prove this? And if I do not know anything about the Pfaffian, can I prove the statement that a skew-symmetric matrix with integer entries has a determinant that is a square of some integer? I mean if someone knows how to prove it without using the Pfaffian?",,"['linear-algebra', 'matrices']"
77,"If $AB=0$, then $A+A^T$ or $B+B^T$ is singular","If , then  or  is singular",AB=0 A+A^T B+B^T,"Define $A$ and $B$ as being square matrices of dimension $2011$. Prove that if $AB=0$, then at least one of matrices $A+A^{T}$ or $B+B^{T}$ have rank below $2011$. -- edit -- Rank of a matrix is a number of linear independent rows. -- edit2 -- dimension instead of rank","Define $A$ and $B$ as being square matrices of dimension $2011$. Prove that if $AB=0$, then at least one of matrices $A+A^{T}$ or $B+B^{T}$ have rank below $2011$. -- edit -- Rank of a matrix is a number of linear independent rows. -- edit2 -- dimension instead of rank",,"['linear-algebra', 'matrices', 'transformation']"
78,A group of linear isomorphisms of $\mathbb C^n$ must have an invariant subspace,A group of linear isomorphisms of  must have an invariant subspace,\mathbb C^n,"Let $G$ be a finite group acting linearly on $\mathbb C^n$, and suppose that $|G| < n^2$. I am trying to show that there is a nonzero invariant subspace $W\subset\mathbb C^n$, i.e. $g(w) \in W$ whenever $w\in W$. I know that if there is an $x\in \mathbb C^n$ with $\sum_{g\in G} g(x) \not=0$, then the linear span of $x$ is the desired subspace. However, I am not sure how to produce such an $x$ or if this is even the right way to go about the problem. Any suggestions?","Let $G$ be a finite group acting linearly on $\mathbb C^n$, and suppose that $|G| < n^2$. I am trying to show that there is a nonzero invariant subspace $W\subset\mathbb C^n$, i.e. $g(w) \in W$ whenever $w\in W$. I know that if there is an $x\in \mathbb C^n$ with $\sum_{g\in G} g(x) \not=0$, then the linear span of $x$ is the desired subspace. However, I am not sure how to produce such an $x$ or if this is even the right way to go about the problem. Any suggestions?",,"['linear-algebra', 'group-theory', 'representation-theory']"
79,A be a $3\times3$ real valued matrix such that $A^{3}=I$ but $A \neq I$ .Then trace(A)=?,A be a  real valued matrix such that  but  .Then trace(A)=?,3\times3 A^{3}=I A \neq I,"I was thinking about the following problem: Let A be a $3\times3$ real valued matrix such that $A^{3}=I$ but $A \neq I$ . Then trace of A must be (a)0, (b)1, (c)-1, (d)3. My attempts: I take A to be $$\begin{pmatrix} x &0  &0 \\   0& x &0 \\   0&0  & x \end{pmatrix}.$$  Now we see $A^{3}=I$ gives $x^{3}=1$ which gives $x=1,w,w^{2}$ where $w$ being the cube root of unity. Thus we see that trace of A is $1+w+w^{2}=0$. So ,i think that (a) is the  right choice.Am i going in the right direction? Please give your valuable opinion. Thanks in advance for your time.","I was thinking about the following problem: Let A be a $3\times3$ real valued matrix such that $A^{3}=I$ but $A \neq I$ . Then trace of A must be (a)0, (b)1, (c)-1, (d)3. My attempts: I take A to be $$\begin{pmatrix} x &0  &0 \\   0& x &0 \\   0&0  & x \end{pmatrix}.$$  Now we see $A^{3}=I$ gives $x^{3}=1$ which gives $x=1,w,w^{2}$ where $w$ being the cube root of unity. Thus we see that trace of A is $1+w+w^{2}=0$. So ,i think that (a) is the  right choice.Am i going in the right direction? Please give your valuable opinion. Thanks in advance for your time.",,['linear-algebra']
80,What is the definition of a generalized eigenvector?,What is the definition of a generalized eigenvector?,,"I am studying Generalized Eigenvectors . It seems that we can define them as $\mathbf{p}_i$ in this equation: $$ (\mathbf{A}-\lambda\mathbf{I})^{k}\mathbf{p}_i = \mathbf{0} $$ in which $k$ is the algebraic multiplicity of $\lambda$ in $ |\mathbf{A}-\lambda\mathbf{I}|=0$. Also it can be defined as: $$ (\mathbf{A}-\lambda\mathbf{I})\mathbf{p}_i = \mathbf{p}_{i-1},~~i=1\ldots,~~\mathbf{p}_{0}=\mathbf{0} $$ or at least this is what I have learned (if sth is wrong, please let me know). Are these definitions equivalent? I mean  $ (\mathbf{A}-\lambda\mathbf{I})^{k}\mathbf{p}_i = \mathbf{0} $ if and only if $ (\mathbf{A}-\lambda\mathbf{I})\mathbf{p}_i = \mathbf{p}_{i-1} $? I can prove the first one, if the second one is true (by multiplying both sides in $ (\mathbf{A}-\lambda\mathbf{I})$ k times), but how can I prove the second one, given the first one? Thanks","I am studying Generalized Eigenvectors . It seems that we can define them as $\mathbf{p}_i$ in this equation: $$ (\mathbf{A}-\lambda\mathbf{I})^{k}\mathbf{p}_i = \mathbf{0} $$ in which $k$ is the algebraic multiplicity of $\lambda$ in $ |\mathbf{A}-\lambda\mathbf{I}|=0$. Also it can be defined as: $$ (\mathbf{A}-\lambda\mathbf{I})\mathbf{p}_i = \mathbf{p}_{i-1},~~i=1\ldots,~~\mathbf{p}_{0}=\mathbf{0} $$ or at least this is what I have learned (if sth is wrong, please let me know). Are these definitions equivalent? I mean  $ (\mathbf{A}-\lambda\mathbf{I})^{k}\mathbf{p}_i = \mathbf{0} $ if and only if $ (\mathbf{A}-\lambda\mathbf{I})\mathbf{p}_i = \mathbf{p}_{i-1} $? I can prove the first one, if the second one is true (by multiplying both sides in $ (\mathbf{A}-\lambda\mathbf{I})$ k times), but how can I prove the second one, given the first one? Thanks",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
81,Translation Matrix and Why non linear?,Translation Matrix and Why non linear?,,"When we translate a point $p_3 = (x,y,z)$ to coordinates $p_4 =(x + t_x , y + t_y ,z + t_z,1)$ we use $4 \times 4$ Translation matrix using homogenous coordinates, hence we add a $1$ to fourth coordinate of $p_4$. But I could not find a matrix $M : R^3 \rightarrow R^4$ for which $M p_3 = p_4$. Note that I am talking about a $1$ on the fourt coordinate what ever vector  $p_3$ be. Why can not we find such a generic translation matrix. Omit the case of matrix addition i.e.  $M$ may only be multiplication of other matrices.","When we translate a point $p_3 = (x,y,z)$ to coordinates $p_4 =(x + t_x , y + t_y ,z + t_z,1)$ we use $4 \times 4$ Translation matrix using homogenous coordinates, hence we add a $1$ to fourth coordinate of $p_4$. But I could not find a matrix $M : R^3 \rightarrow R^4$ for which $M p_3 = p_4$. Note that I am talking about a $1$ on the fourt coordinate what ever vector  $p_3$ be. Why can not we find such a generic translation matrix. Omit the case of matrix addition i.e.  $M$ may only be multiplication of other matrices.",,"['linear-algebra', 'transformation']"
82,Solving for the trace of a matrix,Solving for the trace of a matrix,,"In my research I have commonly had to deal with the following matrix equation (where $\Theta$ is the unknown): \begin{equation} A \Theta + \Theta A^\text{T} = 2 D \end{equation} known as the continuous time Lyapunov equation. All matrices are $n\times n$ with real coefficients, with $\Theta$ and $D$ also being symmetric positive definite. This equation is known not to have a closed form solution and must be solved for each particular problem. However, all I really need is the quantity \begin{equation} F = \text{tr}(A^\text{T} D^{-1} A \Theta) \end{equation} Since all I need is this trace, I was wondering if it was possible to bypass the solution of the Lyapunov equation or, at least, simplify the process in some way. Thus far I have not had any progress. Thank you in advance for your time. Best regards, Gabriel","In my research I have commonly had to deal with the following matrix equation (where $\Theta$ is the unknown): \begin{equation} A \Theta + \Theta A^\text{T} = 2 D \end{equation} known as the continuous time Lyapunov equation. All matrices are $n\times n$ with real coefficients, with $\Theta$ and $D$ also being symmetric positive definite. This equation is known not to have a closed form solution and must be solved for each particular problem. However, all I really need is the quantity \begin{equation} F = \text{tr}(A^\text{T} D^{-1} A \Theta) \end{equation} Since all I need is this trace, I was wondering if it was possible to bypass the solution of the Lyapunov equation or, at least, simplify the process in some way. Thus far I have not had any progress. Thank you in advance for your time. Best regards, Gabriel",,"['linear-algebra', 'trace']"
83,When is linear algebra usually taught,When is linear algebra usually taught,,"Just curious, when should I learn linear algebra in college? And when is linear algebra usually taught? What precedes linear algebra and what usually goes after linear algebra courses?","Just curious, when should I learn linear algebra in college? And when is linear algebra usually taught? What precedes linear algebra and what usually goes after linear algebra courses?",,"['linear-algebra', 'soft-question', 'education']"
84,How to deal with $A^{26}=I$?,How to deal with ?,A^{26}=I,"I got stuck in this problem: Let $A:\mathbb{R}^{6}\rightarrow \mathbb{R}^{6}$ be a linear transformation. Assume $A^{26}=I$, prove that $R^{6}=\oplus_{i=1}^{3} V_{i}$, with $AV_{i}\subset V_{i}$(the explicit condition is $V_{i}$ are 2-dimensional invariant subspaces of $\mathbb{R}^{6}$ under $A$). My thought is $A$ must have a minimal polynomial of degree less or equal to 6. Thus since it divides $x^{26}-1$, the only choices are: $$x-1,x+1,x^{2}-1$$ since the rest term $$(x^{13}-1)/(x-1)*(x^{13}+1)/(x+1)$$ has factors irreducible and degree higher than 6. And the claim is trivial in the case $A=\pm I$. But I do not know how to deal with the case $A^{2}-I=0$ - $A$ can only have eigenvalues $1$ and $-1$, but how this helps to solve the problem? Edit: In the light of did's comments $\sum^{12}_{i=0}x^{i}$ and $\sum^{12}_{i=0}(-1)^{i}x^{i}$ can be reducible over the reals in pairs of 6 quadratics, and the corresponding $A$'s are rotations. But I still feel rather confused as if problem is solved at this stage by suggesting $A$'s minimal polynomial must be a product of $$x-1,x+1,x^{2}-1, x^{2}-\cos[\theta]x+1$$ which are dealt with respectively by $I,-I$, selecting linearly independent vectors and run with $A$, and selecting the rotational invariant subspace. Since obviously cases like $$(x\pm 1)(x^{2}-\cos[\theta]x+1)$$ or even $$(x+1)(x-1)^{2}$$ could happen.","I got stuck in this problem: Let $A:\mathbb{R}^{6}\rightarrow \mathbb{R}^{6}$ be a linear transformation. Assume $A^{26}=I$, prove that $R^{6}=\oplus_{i=1}^{3} V_{i}$, with $AV_{i}\subset V_{i}$(the explicit condition is $V_{i}$ are 2-dimensional invariant subspaces of $\mathbb{R}^{6}$ under $A$). My thought is $A$ must have a minimal polynomial of degree less or equal to 6. Thus since it divides $x^{26}-1$, the only choices are: $$x-1,x+1,x^{2}-1$$ since the rest term $$(x^{13}-1)/(x-1)*(x^{13}+1)/(x+1)$$ has factors irreducible and degree higher than 6. And the claim is trivial in the case $A=\pm I$. But I do not know how to deal with the case $A^{2}-I=0$ - $A$ can only have eigenvalues $1$ and $-1$, but how this helps to solve the problem? Edit: In the light of did's comments $\sum^{12}_{i=0}x^{i}$ and $\sum^{12}_{i=0}(-1)^{i}x^{i}$ can be reducible over the reals in pairs of 6 quadratics, and the corresponding $A$'s are rotations. But I still feel rather confused as if problem is solved at this stage by suggesting $A$'s minimal polynomial must be a product of $$x-1,x+1,x^{2}-1, x^{2}-\cos[\theta]x+1$$ which are dealt with respectively by $I,-I$, selecting linearly independent vectors and run with $A$, and selecting the rotational invariant subspace. Since obviously cases like $$(x\pm 1)(x^{2}-\cos[\theta]x+1)$$ or even $$(x+1)(x-1)^{2}$$ could happen.",,['linear-algebra']
85,What's the solution to $X = AXA$?,What's the solution to ?,X = AXA,"I have given a general $n \times d$ matrix $B$, and need to compute two similarity matrices $R \in \mathbb{R}^{n \times n}$ and $S \in \mathbb{R}^{d \times d}$ as follows: $R = BSB^\top$ and $S = B^\top RB$. Inserting one equation into the other yields $S = B^\top B S B^\top B$. I assume a solution doesn't always exist. In this case, I'm interested in an approximate solution that minimizes $||S - B^\top B S B^\top B||_F$. Is there a name for this kind of problem? And is there an efficient way to solve it (possibly using some iterative algorithm)? Note: The zero matrix would satisfy above equation, but I need similarity matrices, i.e. symmetric matrices for which the diagonal elements must be equal to 1.","I have given a general $n \times d$ matrix $B$, and need to compute two similarity matrices $R \in \mathbb{R}^{n \times n}$ and $S \in \mathbb{R}^{d \times d}$ as follows: $R = BSB^\top$ and $S = B^\top RB$. Inserting one equation into the other yields $S = B^\top B S B^\top B$. I assume a solution doesn't always exist. In this case, I'm interested in an approximate solution that minimizes $||S - B^\top B S B^\top B||_F$. Is there a name for this kind of problem? And is there an efficient way to solve it (possibly using some iterative algorithm)? Note: The zero matrix would satisfy above equation, but I need similarity matrices, i.e. symmetric matrices for which the diagonal elements must be equal to 1.",,"['linear-algebra', 'matrices']"
86,product of hermitian and unitary matrix,product of hermitian and unitary matrix,,"Could anyone tell me how to show that, for any $g\in GL_n(\mathbb{C})$, $\exists$ $R$ a hermitian matrix with positive eigenvalues and $U$ an unitary matrix such that $g=RU$? And (I am not sure) can we use this to prove that $GL_n(\mathbb{C})$ is connected?Is the set of hermitian matrices with positive eigen values are path connected?","Could anyone tell me how to show that, for any $g\in GL_n(\mathbb{C})$, $\exists$ $R$ a hermitian matrix with positive eigenvalues and $U$ an unitary matrix such that $g=RU$? And (I am not sure) can we use this to prove that $GL_n(\mathbb{C})$ is connected?Is the set of hermitian matrices with positive eigen values are path connected?",,"['linear-algebra', 'general-topology', 'matrices', 'metric-spaces']"
87,Dimension of set of commutable matrices,Dimension of set of commutable matrices,,Let $A$ be an $n \times n$ complex matrix and $V = \{B\mid AB=BA\}$ . I've proved that $V$ is a vector space. How can I prove that $\dim V \ge n$ for any $A$ ?,Let be an complex matrix and . I've proved that is a vector space. How can I prove that for any ?,A n \times n V = \{B\mid AB=BA\} V \dim V \ge n A,['linear-algebra']
88,Need help proving blockwise property of matrix multiplication.,Need help proving blockwise property of matrix multiplication.,,"I want to prove the following: If $X$ and $Y$ are $n \times n$ matrices, and $$ X = \left[\begin{matrix} A & B\\ C & D \end{matrix}\right],  Y = \left[\begin{matrix} E & F\\ G & H \end{matrix}\right] $$ where A, B, C, D, E, F, G, and H are $n/2 \times n/2$ submatrices, then   the product $XY$ can be expressed in terms of these blocks: $$ XY = Z = \left[\begin{matrix} AE + BG & AF + BH\\ CE + DG & CF + DH \end{matrix}\right] $$ My initial thought was to use the matrix multiplication definition: $$Z_{ij} = \sum_{k=1}^n X_{ik} Y_{kj}$$ and show that each $Z_{ij}$ equals the element in $Z$ by going case by case. Case1 would be something like: $1 \leq i \leq(n/2), 1 \leq j \leq (n/2)$. So in this case, $X_{ij} = A_{ij}$ and $Y_{ij} = E_{ij}$ I am stuck here and not sure if I am on the right track. Please advise me on how to proceed from here (or suggest an alternative method).","I want to prove the following: If $X$ and $Y$ are $n \times n$ matrices, and $$ X = \left[\begin{matrix} A & B\\ C & D \end{matrix}\right],  Y = \left[\begin{matrix} E & F\\ G & H \end{matrix}\right] $$ where A, B, C, D, E, F, G, and H are $n/2 \times n/2$ submatrices, then   the product $XY$ can be expressed in terms of these blocks: $$ XY = Z = \left[\begin{matrix} AE + BG & AF + BH\\ CE + DG & CF + DH \end{matrix}\right] $$ My initial thought was to use the matrix multiplication definition: $$Z_{ij} = \sum_{k=1}^n X_{ik} Y_{kj}$$ and show that each $Z_{ij}$ equals the element in $Z$ by going case by case. Case1 would be something like: $1 \leq i \leq(n/2), 1 \leq j \leq (n/2)$. So in this case, $X_{ij} = A_{ij}$ and $Y_{ij} = E_{ij}$ I am stuck here and not sure if I am on the right track. Please advise me on how to proceed from here (or suggest an alternative method).",,"['linear-algebra', 'matrices', 'block-matrices']"
89,"Is there a nice way to define the ""maximum"" of two quadratic forms?","Is there a nice way to define the ""maximum"" of two quadratic forms?",,"Suppose I have two quadratic forms on $\mathbb R^n$, represented as symmetric matrices $A$ and $B$ on the usual basis. I am interested in approximating the function $x \mapsto \max(x^TAx, x^TBx)$ while remaining within the space of quadratic forms. Is there a nice way to define a ""maximum"" operation on symmetric matrices, such that $C = \max(A,B)$ if $C$ is, in some sense, the ""smallest"" symmetric matrix satisfying $x^TCx \ge x^TAx$ and $x^TCx \ge x^TBx$ for all vectors $x$? I've purposely left the notion of the ""smallest"" matrix $C$ undefined, as I'll accept any formalization that allows its solution to be elegantly expressed and/or easily computed. One possibility is minimizing the trace of $C$. Another, if we restrict ourselves to positive semidefinite matrices, is minimizing a convenient matrix norm. In any case, $\max$ certainly must be commutative, and must satisfy $\max(A,A) = A$. Also, if $A$ and $B$ share the same eigenvectors, with eigenvalues $\lambda_i$ and $\mu_i$ respectively, then it seems natural that $\max(A,B)$ should also have eigenvectors the same, and eigenvalues $\max(\lambda_i,\mu_i)$. Beyond that, I can't really tell.","Suppose I have two quadratic forms on $\mathbb R^n$, represented as symmetric matrices $A$ and $B$ on the usual basis. I am interested in approximating the function $x \mapsto \max(x^TAx, x^TBx)$ while remaining within the space of quadratic forms. Is there a nice way to define a ""maximum"" operation on symmetric matrices, such that $C = \max(A,B)$ if $C$ is, in some sense, the ""smallest"" symmetric matrix satisfying $x^TCx \ge x^TAx$ and $x^TCx \ge x^TBx$ for all vectors $x$? I've purposely left the notion of the ""smallest"" matrix $C$ undefined, as I'll accept any formalization that allows its solution to be elegantly expressed and/or easily computed. One possibility is minimizing the trace of $C$. Another, if we restrict ourselves to positive semidefinite matrices, is minimizing a convenient matrix norm. In any case, $\max$ certainly must be commutative, and must satisfy $\max(A,A) = A$. Also, if $A$ and $B$ share the same eigenvectors, with eigenvalues $\lambda_i$ and $\mu_i$ respectively, then it seems natural that $\max(A,B)$ should also have eigenvectors the same, and eigenvalues $\max(\lambda_i,\mu_i)$. Beyond that, I can't really tell.",,['linear-algebra']
90,Compact subgroups of the general linear group,Compact subgroups of the general linear group,,"Let $V$ be a finite-dimensional real linear space, and let $K$ be a compact subgroup of $GL(V)$ (with the usual topology); then is there a basis of $V$ such that every $f\in K$ is an orthogonal matrix under this basis?","Let $V$ be a finite-dimensional real linear space, and let $K$ be a compact subgroup of $GL(V)$ (with the usual topology); then is there a basis of $V$ such that every $f\in K$ is an orthogonal matrix under this basis?",,"['linear-algebra', 'topological-groups']"
91,When is a positive semi-definite matrix A positive definite?,When is a positive semi-definite matrix A positive definite?,,"Does it has something to do with the determinant of A? I saw two seperate websites - one which claims that when the determinant of A is zero, and the other claims that when the determinant of A is not zero, then the positive semi-definite matrix is positive definite. Can someone explain to me which is the correct answer, and the reason behind it? Thank you.","Does it has something to do with the determinant of A? I saw two seperate websites - one which claims that when the determinant of A is zero, and the other claims that when the determinant of A is not zero, then the positive semi-definite matrix is positive definite. Can someone explain to me which is the correct answer, and the reason behind it? Thank you.",,['linear-algebra']
92,How to deduce that there are $(n^3+2n-3)/3$ multiplications for the determinant evaluation?,How to deduce that there are  multiplications for the determinant evaluation?,(n^3+2n-3)/3,"In Friedberg's Linear Algebra, the author points out that the evaluation of the determinant of an $n\times n$ matrix by cofactor expansion along any row requires over $n!$ multiplications, whereas evaluating the determinant of an $n\times n$ matrix by elementary row operations can be shown to require only $(n^3+2n-3)/3$ multiplications. I cannot even figure out the case when $n=2$. The key point, I think, is the exact algorithm of evaluating by elementary row operations. But I temporarily have no idea how to go on. Here is my question: How can I deduce the number $(n^3+2n-3)/3$?","In Friedberg's Linear Algebra, the author points out that the evaluation of the determinant of an $n\times n$ matrix by cofactor expansion along any row requires over $n!$ multiplications, whereas evaluating the determinant of an $n\times n$ matrix by elementary row operations can be shown to require only $(n^3+2n-3)/3$ multiplications. I cannot even figure out the case when $n=2$. The key point, I think, is the exact algorithm of evaluating by elementary row operations. But I temporarily have no idea how to go on. Here is my question: How can I deduce the number $(n^3+2n-3)/3$?",,['linear-algebra']
93,Explain what is a linear transformation,Explain what is a linear transformation,,"My textbook says ""unique linear transformations can be defined by a few values, if the given domain vectors form a basis."" However, that is all it says. So can someone explain what a unique linear transformation is? To help make this question more manageable, let me give you some examples from the text: 1) $T(9x+5) = (.1,.2)$ and $T(7x+4) = (.3,.8)$. This is a unique linear b/c $(4a-7b)(9x+5) + (9b-5a)(7x+4)$ will equal ... [do some foiling] ... $= ax +b$ for all $a$, $b$ real. Q: mmm, so what about $(4a-7b)(9x+5) + (9b-5a)(7x+4)$ will equal ... [do some foiling] ... = $ax +b$ for all $a,b$ real' tells me this is a unique linear transformation? Even more fundamentally, just looking at the example problem it seems we have 2 transformations. So if we were going to look for a unique transformation shouldn't we only look at $T(9x+5)$ and/or $T(7x+4)$ individually. Or if we wanted to look at them both should we be asking if a new (unlisted) transformation $T^*$ (which is $T(9x+5+7x +4)$) is a unique linear transformation? Perhaps, I'm just not really clear on what exactly we are finding the linear transformation of. 2) $T(2,1) = 4x +5$; $T(6,3) = 12x +15$. Not unique because $T(a, b) = 2ax + 5b$ works, but so does $T(a,b) = 4bx + (3a - b)$. Q: Again, what specifically are we finding the linear transformation of? Where does $2ax + 5b$ and $4bx + (3a - b)$ comes from? How did the author arrive at it? I'm just not sure on how to interpret these equations? That's it. Yes, I know my questions were probably confused (for I am). But I'm looking for anything to help give me clarity. For I have nothing more to go on that the 2 sentences from the book: ""Unique linear transformations can be defined by a few values, if the given domain vectors form a basis. Otherwise it could fail the uniqueness condition, or worse yet, fail the definition of a function."" Thanks in advance.","My textbook says ""unique linear transformations can be defined by a few values, if the given domain vectors form a basis."" However, that is all it says. So can someone explain what a unique linear transformation is? To help make this question more manageable, let me give you some examples from the text: 1) $T(9x+5) = (.1,.2)$ and $T(7x+4) = (.3,.8)$. This is a unique linear b/c $(4a-7b)(9x+5) + (9b-5a)(7x+4)$ will equal ... [do some foiling] ... $= ax +b$ for all $a$, $b$ real. Q: mmm, so what about $(4a-7b)(9x+5) + (9b-5a)(7x+4)$ will equal ... [do some foiling] ... = $ax +b$ for all $a,b$ real' tells me this is a unique linear transformation? Even more fundamentally, just looking at the example problem it seems we have 2 transformations. So if we were going to look for a unique transformation shouldn't we only look at $T(9x+5)$ and/or $T(7x+4)$ individually. Or if we wanted to look at them both should we be asking if a new (unlisted) transformation $T^*$ (which is $T(9x+5+7x +4)$) is a unique linear transformation? Perhaps, I'm just not really clear on what exactly we are finding the linear transformation of. 2) $T(2,1) = 4x +5$; $T(6,3) = 12x +15$. Not unique because $T(a, b) = 2ax + 5b$ works, but so does $T(a,b) = 4bx + (3a - b)$. Q: Again, what specifically are we finding the linear transformation of? Where does $2ax + 5b$ and $4bx + (3a - b)$ comes from? How did the author arrive at it? I'm just not sure on how to interpret these equations? That's it. Yes, I know my questions were probably confused (for I am). But I'm looking for anything to help give me clarity. For I have nothing more to go on that the 2 sentences from the book: ""Unique linear transformations can be defined by a few values, if the given domain vectors form a basis. Otherwise it could fail the uniqueness condition, or worse yet, fail the definition of a function."" Thanks in advance.",,"['linear-algebra', 'intuition']"
94,"What non-symmetric matrices satisfy $x^TAx>0,\forall x\neq 0$",What non-symmetric matrices satisfy,"x^TAx>0,\forall x\neq 0","Let $A\in R^{n\times n}$ be a matrix. It is positive definite if and only if $A$ is symmetric and $x^TAx>0,\forall x\in R^n$. My question is : if $x^TAx>0,\forall x\in R^n$ but $A$ is not symmetric, what does $A$ look like? I have an example. For a rotation matrix $A$ whose rotation angle is less than 90 degrees, $x^TAx>0,\forall x\in R^n$ but $A$ is not symmetric. Is this the only type of non-symmetric matrices that satisfy $x^TAx>0,\forall x\in R^n$? Can you give any other examples of this kind of matrices? Many thanks.","Let $A\in R^{n\times n}$ be a matrix. It is positive definite if and only if $A$ is symmetric and $x^TAx>0,\forall x\in R^n$. My question is : if $x^TAx>0,\forall x\in R^n$ but $A$ is not symmetric, what does $A$ look like? I have an example. For a rotation matrix $A$ whose rotation angle is less than 90 degrees, $x^TAx>0,\forall x\in R^n$ but $A$ is not symmetric. Is this the only type of non-symmetric matrices that satisfy $x^TAx>0,\forall x\in R^n$? Can you give any other examples of this kind of matrices? Many thanks.",,"['linear-algebra', 'matrices']"
95,A confusion about Ker($A$) and Ker($A^{T}$),A confusion about Ker() and Ker(),A A^{T},"Today we were discussing how for an nxn orthogonal projection matrix from $\mathbb{R^{n}}$ onto a subspace W, Ker($A$)=$(Im$A$)^{\perp}$=$W^{\perp}$ and that Ker($A^{T}$) is also $W^{\perp}$. This prompted the question of what conditions are necessary for a matrix so that the kernels of it and its transpose are equal. It looks like it always works when it's an orthogonal projection, but we struggled to find an example of a square matrix for which the aforementioned identities would not hold, and consequently for which Ker($A$)$\neq$Ker($A^{T}$). It looks like we need to find a transformation whose Kernel would not consist only of things perpendicular to its image, but we were wondering if that was possible or if our reasoning was correct at all. Could you please clarify this issue (I know it's convoluted) and try to point out where we were right and wrong, and when do those identities hold?","Today we were discussing how for an nxn orthogonal projection matrix from $\mathbb{R^{n}}$ onto a subspace W, Ker($A$)=$(Im$A$)^{\perp}$=$W^{\perp}$ and that Ker($A^{T}$) is also $W^{\perp}$. This prompted the question of what conditions are necessary for a matrix so that the kernels of it and its transpose are equal. It looks like it always works when it's an orthogonal projection, but we struggled to find an example of a square matrix for which the aforementioned identities would not hold, and consequently for which Ker($A$)$\neq$Ker($A^{T}$). It looks like we need to find a transformation whose Kernel would not consist only of things perpendicular to its image, but we were wondering if that was possible or if our reasoning was correct at all. Could you please clarify this issue (I know it's convoluted) and try to point out where we were right and wrong, and when do those identities hold?",,"['linear-algebra', 'matrices', 'transpose']"
96,Solving matrix equations of the form $XA = XB$,Solving matrix equations of the form,XA = XB,"I am trying to solve the matrix equation of the form $XA = XB$. $A$, $B$ and the solution sought $X$ are $4 \times 4$ homegeneous matrices which are composed of a rotation matrix and translation vector, like in $$ \begin{bmatrix}  r_1 & r_2 & r_3 & tx\\  r_4 & r_5 & r_6 &ty\\  r_7 & r_8 & r_9 & tz\\ 0 & 0 & 0 & 1  \end{bmatrix}.$$ There are several methods to solve equations of the type AX = XB .  These type of equations occur in domains like robotics, for which solutions have been proposed like the Tsai and Lenz method [IEEE 1987], for example. 1) I feel solving the equation XA = XB is not the same as solving the known form AX = XB . Am I right?  2) Neither can it be solved like the Sylvester equation , because even that requires $AX + XB = C$ form. What i have is $XA = XB$, a redundant set of equations. That is, \begin{align} A_1.X & = X.B_1 \\  A_2.X & = X.B_2 \\ & \vdots \\ A_n.X & = X.B_n \end{align} If i am correct, these could be rewritten into another form like $AX = BX$. Should i rewrite the equation and try to solve this problem using any other existing methods?","I am trying to solve the matrix equation of the form $XA = XB$. $A$, $B$ and the solution sought $X$ are $4 \times 4$ homegeneous matrices which are composed of a rotation matrix and translation vector, like in $$ \begin{bmatrix}  r_1 & r_2 & r_3 & tx\\  r_4 & r_5 & r_6 &ty\\  r_7 & r_8 & r_9 & tz\\ 0 & 0 & 0 & 1  \end{bmatrix}.$$ There are several methods to solve equations of the type AX = XB .  These type of equations occur in domains like robotics, for which solutions have been proposed like the Tsai and Lenz method [IEEE 1987], for example. 1) I feel solving the equation XA = XB is not the same as solving the known form AX = XB . Am I right?  2) Neither can it be solved like the Sylvester equation , because even that requires $AX + XB = C$ form. What i have is $XA = XB$, a redundant set of equations. That is, \begin{align} A_1.X & = X.B_1 \\  A_2.X & = X.B_2 \\ & \vdots \\ A_n.X & = X.B_n \end{align} If i am correct, these could be rewritten into another form like $AX = BX$. Should i rewrite the equation and try to solve this problem using any other existing methods?",,"['linear-algebra', 'matrices']"
97,How to show that you can find two subspaces that don't intersect,How to show that you can find two subspaces that don't intersect,,"Suppose $V, V'$ are subspaces of dimension $d$ of a vector space $X$. Then there is a subspace $W$ of $X$ of codimension $d$ such that $W \cap V = W \cap V' = { 0 }$.  This can be proved by choosing an explicit basis for $X$ which contains a basis for $V$ and a basis for $V'$ and a basis for $V \cap V'$.  On the other hand, there should be a nice way to do this without choosing a basis. Can anyone explain this? Disclosure: This came up when I was doing a homework problem. However, I'm just going to use the non-basis-free approach when I write up my answer.","Suppose $V, V'$ are subspaces of dimension $d$ of a vector space $X$. Then there is a subspace $W$ of $X$ of codimension $d$ such that $W \cap V = W \cap V' = { 0 }$.  This can be proved by choosing an explicit basis for $X$ which contains a basis for $V$ and a basis for $V'$ and a basis for $V \cap V'$.  On the other hand, there should be a nice way to do this without choosing a basis. Can anyone explain this? Disclosure: This came up when I was doing a homework problem. However, I'm just going to use the non-basis-free approach when I write up my answer.",,['linear-algebra']
98,Determinant of a polynomial matrix,Determinant of a polynomial matrix,,"A matrix determinant (naively) can be computed in $O(n!)$ steps, or with a proper LU decomposition $O(n^3)$ steps. This assumes that all the matrix elements are constant. If, however the matrix elements are polynomials (say univariate of max order $p$) at each step of the LU decomposition an element is multiplied by another element producing (on average) ever larger polynomials. Each step therefore takes longer and longer - is the cost perform the decomposition still a polynomial? EDIT : To clarify my reasoning a bit, if polynomial (using FFT as J.M. suggests in the comments) takes $O(m \log m)$, and we must perform $O(n^3)$ operations to get the LU decomposition, each step the polynomial could effectively double in degree at each multiplication*. The running time would look something like $$ \approx O \left ( \sum_{k}^{n^3} (p \ln p)^{2k} \right ) .$$ * (it doesn't quite do that, and this is where I'm stuck)","A matrix determinant (naively) can be computed in $O(n!)$ steps, or with a proper LU decomposition $O(n^3)$ steps. This assumes that all the matrix elements are constant. If, however the matrix elements are polynomials (say univariate of max order $p$) at each step of the LU decomposition an element is multiplied by another element producing (on average) ever larger polynomials. Each step therefore takes longer and longer - is the cost perform the decomposition still a polynomial? EDIT : To clarify my reasoning a bit, if polynomial (using FFT as J.M. suggests in the comments) takes $O(m \log m)$, and we must perform $O(n^3)$ operations to get the LU decomposition, each step the polynomial could effectively double in degree at each multiplication*. The running time would look something like $$ \approx O \left ( \sum_{k}^{n^3} (p \ln p)^{2k} \right ) .$$ * (it doesn't quite do that, and this is where I'm stuck)",,"['linear-algebra', 'matrices', 'asymptotics', 'determinant']"
99,"Let $A$ be a skew-symmetric real matrix, prove that there exists a vector $x\ge0$ such that $Ax\ge0$ and $Ax + x > 0$","Let  be a skew-symmetric real matrix, prove that there exists a vector  such that  and",A x\ge0 Ax\ge0 Ax + x > 0,"This is an assignment that I am struggling with. Let $A$ be a skew-symmetric real matrix, prove that there exists a vector $x\ge0$ such that $Ax\ge0$ and $Ax + x > 0$ . Not sure how to proceed here and would appreciate some pointers/solution. My first thought is to: Show there exists a vector $x\ge0$ so that $Ax\ge0$ Use proof by contradiction by starting with the claim: for any $x\ge0$ , either $Ax<0$ or $Ax+x\le0$ , and derive contradiction, but I did not see a way how. and the second thought is to use Farkas' lemma, which states that for any given matrix A and vector b, one and only one of the following statements is true: (a) There exists a vector $x\ge0$ so that $Ax=b$ ; (b) There exists a vector $y$ s.t. $A^Ty\ge0$ AND $b^Ty<0$ I start by choosing $A+I$ as the ""A"" matrix and some positive vector $b>0$ as the ""b"" vector here. If I can prove that $y$ does not exist under this situation, I would have proven (a) is correct, which means there exists a $x\ge0$ so that $(A+I)x > 0$ , which proves the second half of the case. But unfortunately I cannot find a way to disprove (b). Appreciate your kind advice. Thanks.","This is an assignment that I am struggling with. Let be a skew-symmetric real matrix, prove that there exists a vector such that and . Not sure how to proceed here and would appreciate some pointers/solution. My first thought is to: Show there exists a vector so that Use proof by contradiction by starting with the claim: for any , either or , and derive contradiction, but I did not see a way how. and the second thought is to use Farkas' lemma, which states that for any given matrix A and vector b, one and only one of the following statements is true: (a) There exists a vector so that ; (b) There exists a vector s.t. AND I start by choosing as the ""A"" matrix and some positive vector as the ""b"" vector here. If I can prove that does not exist under this situation, I would have proven (a) is correct, which means there exists a so that , which proves the second half of the case. But unfortunately I cannot find a way to disprove (b). Appreciate your kind advice. Thanks.",A x\ge0 Ax\ge0 Ax + x > 0 x\ge0 Ax\ge0 x\ge0 Ax<0 Ax+x\le0 x\ge0 Ax=b y A^Ty\ge0 b^Ty<0 A+I b>0 y x\ge0 (A+I)x > 0,"['linear-algebra', 'skew-symmetric-matrices']"
