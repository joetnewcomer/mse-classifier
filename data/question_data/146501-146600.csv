,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,improper integral equal to 1,improper integral equal to 1,,For which constants $a$ and $b$ will $$\lim_{p\to\infty}\int_{-p}^p\frac{x^3+ax^2+bx}{x^2+x+1}dx=1$$ Now I set the following: $$\lim_{p\to\infty}\int_{-p}^p\frac{x^3+ax^2+bx}{x^2+x+1}dx=\lim_{p\to\infty}\int_{-p}^0\frac{x^3+ax^2+bx}{x^2+x+1}dx+\lim_{p\to\infty}\int_{0}^p\frac{x^3+ax^2+bx}{x^2+x+1}dx$$ Could I assume that both limits should be equal $\frac{1}{2}$ then try to integrate and try to find the constants? I don't get far by tring that. What else could I try? Thanks!,For which constants $a$ and $b$ will $$\lim_{p\to\infty}\int_{-p}^p\frac{x^3+ax^2+bx}{x^2+x+1}dx=1$$ Now I set the following: $$\lim_{p\to\infty}\int_{-p}^p\frac{x^3+ax^2+bx}{x^2+x+1}dx=\lim_{p\to\infty}\int_{-p}^0\frac{x^3+ax^2+bx}{x^2+x+1}dx+\lim_{p\to\infty}\int_{0}^p\frac{x^3+ax^2+bx}{x^2+x+1}dx$$ Could I assume that both limits should be equal $\frac{1}{2}$ then try to integrate and try to find the constants? I don't get far by tring that. What else could I try? Thanks!,,"['limits', 'improper-integrals']"
1,Find the limit of $(x_n)$ defined by $x_{n+1}=c_nu(x_n)$,Find the limit of  defined by,(x_n) x_{n+1}=c_nu(x_n),Find $\displaystyle \lim_{n\rightarrow \infty }x_n$ : $$\left\{\begin{matrix}x_1=a>0\\ \\ x_{n+1}=\frac{2x_n\cdot \cos\left(\frac{\pi}{2^n+1}\right)}{x_n+1}\end{matrix}\right.$$ I have tried that : Let $a_n=\dfrac{1}{x_n}$ . So : $$a_{n+1}=\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}.a_n+\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}$$ So I tried to have geometric series:  By let : $$a_{n+1}+f(n+1)=\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}(a_n+f(n))$$ So we must find one $f(n)$: $$\frac{f(n)}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}-f(n+1)=\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}$$ As $$f(n)-f(n+1)\cdot 2\cos\frac{\pi}{2^{n+1}}=1$$ Can you give me the way to find one $f(n)$ sastisfied that ; or anyone has nice way to solve this problem,Find $\displaystyle \lim_{n\rightarrow \infty }x_n$ : $$\left\{\begin{matrix}x_1=a>0\\ \\ x_{n+1}=\frac{2x_n\cdot \cos\left(\frac{\pi}{2^n+1}\right)}{x_n+1}\end{matrix}\right.$$ I have tried that : Let $a_n=\dfrac{1}{x_n}$ . So : $$a_{n+1}=\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}.a_n+\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}$$ So I tried to have geometric series:  By let : $$a_{n+1}+f(n+1)=\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}(a_n+f(n))$$ So we must find one $f(n)$: $$\frac{f(n)}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}-f(n+1)=\frac{1}{2\cos\left(\frac{\pi}{2^{n+1}}\right)}$$ As $$f(n)-f(n+1)\cdot 2\cos\frac{\pi}{2^{n+1}}=1$$ Can you give me the way to find one $f(n)$ sastisfied that ; or anyone has nice way to solve this problem,,['limits']
2,How to solve indeterminations of the type $0/0$,How to solve indeterminations of the type,0/0,"I am unable to find these limits: 1) $$ \lim_{x \to 1} \frac{3(1 - x^2) - 2(1 - x^3)}{(1 - x^3)(1 - x^2)} $$ 2) $$ \lim_{x \to 0} \frac{\sqrt{1 - 2x - x^2} - (x + 1)}{x} $$ 3) $$ \lim_{x \to 0} \frac{\sqrt{x + 2} + \sqrt{x + 6} - \sqrt{6} - \sqrt{2}}{x} $$ 4) $$ \lim_{x \to 0} \frac{1 - \sqrt[3]{1 - x}}{1 + \sqrt[3]{3x - 1}} $$ My interest is not in the answers, but in the algebraic manipulations i can use to eliminate the indeterminations of the type $0/0$. My english skills are not so good, i'm sorry for this.","I am unable to find these limits: 1) $$ \lim_{x \to 1} \frac{3(1 - x^2) - 2(1 - x^3)}{(1 - x^3)(1 - x^2)} $$ 2) $$ \lim_{x \to 0} \frac{\sqrt{1 - 2x - x^2} - (x + 1)}{x} $$ 3) $$ \lim_{x \to 0} \frac{\sqrt{x + 2} + \sqrt{x + 6} - \sqrt{6} - \sqrt{2}}{x} $$ 4) $$ \lim_{x \to 0} \frac{1 - \sqrt[3]{1 - x}}{1 + \sqrt[3]{3x - 1}} $$ My interest is not in the answers, but in the algebraic manipulations i can use to eliminate the indeterminations of the type $0/0$. My english skills are not so good, i'm sorry for this.",,"['calculus', 'limits']"
3,"proof of limits in math: if $a_n^3\to a^3$, then $a_n\to a$.","proof of limits in math: if , then .",a_n^3\to a^3 a_n\to a,a(n) is a sequence. I can't find an example :/ can someone check my way to solve it?,a(n) is a sequence. I can't find an example :/ can someone check my way to solve it?,,"['sequences-and-series', 'limits']"
4,Prove that $\frac{2^n}{n!}$ converges 0. [duplicate],Prove that  converges 0. [duplicate],\frac{2^n}{n!},"This question already has answers here : Closed 11 years ago . Possible Duplicate: Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. Prove that $\frac{2^n}{n!}$ converges 0. I can see why, I just don't get how exactly to do convergence proofs. Right now I have: For $n>6$, $|\frac{2^n}{n!}-0|=\frac{2^n}{n!}<\frac{2^n}{3^n}$ and assuming $\frac{2^n}{3^n}<\epsilon$, $n<\frac{\ln\epsilon}{\ln\frac2 3}$ Not sure if the last step is even right... (This was an exam question today)","This question already has answers here : Closed 11 years ago . Possible Duplicate: Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. Prove that $\frac{2^n}{n!}$ converges 0. I can see why, I just don't get how exactly to do convergence proofs. Right now I have: For $n>6$, $|\frac{2^n}{n!}-0|=\frac{2^n}{n!}<\frac{2^n}{3^n}$ and assuming $\frac{2^n}{3^n}<\epsilon$, $n<\frac{\ln\epsilon}{\ln\frac2 3}$ Not sure if the last step is even right... (This was an exam question today)",,"['real-analysis', 'limits']"
5,"""Proof"" that if f(x) ~x, e^f(x) ~ e^x","""Proof"" that if f(x) ~x, e^f(x) ~ e^x",,"While it is not true that $f(x)\sim x \implies e^{f(x)}\sim e^x,$ I can't spot the error in this ""proof"" by induction--or at least I can't articulate it well. Let $f(x)\sim x$ and $x > 1$ P(1): $1 \sim 1, f(x) \sim x,$ and $\lim_{x \to \infty} \frac{1+f(x)}{1+x} = \frac{1}{1+x}+\frac{f(x)}{1+x} = 0 +1 = 1. $ Assume P(k): $$~\lim_{x \to \infty} \frac{\sum_0^{k-1}f(x)^{n}/n!~ + ~f(x)^k/k!}{\sum x^{n}/n!~ +~ x^k/k!} = 1$$ It implies P(k+1): $$~\lim_{x \to \infty} \frac{\sum_0^{k}f(x)^{n}/n!~ + ~f(x)^{k+1}/(k+1)!}{\sum x^{n}/n!~ +~ x^{k+1}/(k+1)!} = 1$$ Since P(k) implies P(k+1), and P(1) is true...? Thanks.","While it is not true that $f(x)\sim x \implies e^{f(x)}\sim e^x,$ I can't spot the error in this ""proof"" by induction--or at least I can't articulate it well. Let $f(x)\sim x$ and $x > 1$ P(1): $1 \sim 1, f(x) \sim x,$ and $\lim_{x \to \infty} \frac{1+f(x)}{1+x} = \frac{1}{1+x}+\frac{f(x)}{1+x} = 0 +1 = 1. $ Assume P(k): $$~\lim_{x \to \infty} \frac{\sum_0^{k-1}f(x)^{n}/n!~ + ~f(x)^k/k!}{\sum x^{n}/n!~ +~ x^k/k!} = 1$$ It implies P(k+1): $$~\lim_{x \to \infty} \frac{\sum_0^{k}f(x)^{n}/n!~ + ~f(x)^{k+1}/(k+1)!}{\sum x^{n}/n!~ +~ x^{k+1}/(k+1)!} = 1$$ Since P(k) implies P(k+1), and P(1) is true...? Thanks.",,"['limits', 'asymptotics', 'induction', 'exponential-function']"
6,"Determine the value of $k$, if any, that makes k continuous everywhere for this piece wise function.","Determine the value of , if any, that makes k continuous everywhere for this piece wise function.",k,"Let   $$ h(x) = \begin{cases} \sin (kx), & \text{if }x\leq  2, \\ x+k^2, & \text{if }x>2, \end{cases} $$   where $k$ is a real constant. Determine the value of $k$, if any, that makes k continuous everywhere. I done my work and here goes. Since $h(x)$ is continuous on $(-\infty,2)$ and on $(2,\infty)$, it suffice to find the value of $k$ such that $h(x)$ is continuous at $x=2$. Suppose $h(x)$ is continuous at $x=0$, then $$\lim_{x\to2^-}h(x)=\lim_{x\to2^-}\sin k(x)=\sin(2k)$$     and $$\lim_{x\to2^+}h(x)=\lim_{x\to2^+}x+k^2=2+k^2$$     Therefore, $\lim_{x\to2^+}h(x)=\lim_{x\to2^-}h(x)$ implies $\sin(2k)=2+k^2$ Ok so now I am stuck. Thanks in advance for helping me out!","Let   $$ h(x) = \begin{cases} \sin (kx), & \text{if }x\leq  2, \\ x+k^2, & \text{if }x>2, \end{cases} $$   where $k$ is a real constant. Determine the value of $k$, if any, that makes k continuous everywhere. I done my work and here goes. Since $h(x)$ is continuous on $(-\infty,2)$ and on $(2,\infty)$, it suffice to find the value of $k$ such that $h(x)$ is continuous at $x=2$. Suppose $h(x)$ is continuous at $x=0$, then $$\lim_{x\to2^-}h(x)=\lim_{x\to2^-}\sin k(x)=\sin(2k)$$     and $$\lim_{x\to2^+}h(x)=\lim_{x\to2^+}x+k^2=2+k^2$$     Therefore, $\lim_{x\to2^+}h(x)=\lim_{x\to2^-}h(x)$ implies $\sin(2k)=2+k^2$ Ok so now I am stuck. Thanks in advance for helping me out!",,"['calculus', 'limits', 'continuity']"
7,Yet another limit to a sequence,Yet another limit to a sequence,,"so, I'm completely stuck with this limit: $$\lim_{n\to\infty}{\left(1+\frac{1}{n^2}\right)^n}$$ Which I can't even grasp how to start with, since I didn't understand the explanation of some method having to do with $e$. However, I believe that it would go something like this: $$\lim_{n\to\infty}{\left(1+\frac{1}{n^2}\right)^{n^2}}$$ $$\lim_{n\to\infty}{\left(\left(1+\frac{1}{n^2}\right)^{n^2}\right)^{1/n^2I}}$$ ... Or something like that... However, as you can see, I'm completely blank. Any suggestions will be appreciated.","so, I'm completely stuck with this limit: $$\lim_{n\to\infty}{\left(1+\frac{1}{n^2}\right)^n}$$ Which I can't even grasp how to start with, since I didn't understand the explanation of some method having to do with $e$. However, I believe that it would go something like this: $$\lim_{n\to\infty}{\left(1+\frac{1}{n^2}\right)^{n^2}}$$ $$\lim_{n\to\infty}{\left(\left(1+\frac{1}{n^2}\right)^{n^2}\right)^{1/n^2I}}$$ ... Or something like that... However, as you can see, I'm completely blank. Any suggestions will be appreciated.",,"['sequences-and-series', 'limits']"
8,The Limit of a Sequence of Paths,The Limit of a Sequence of Paths,,"Given a path connected topological space $X$, consider a sequence $x_1, x_2, x_3, \dotsc$ of points in $X$ converging at some $x \in X$. For each $x_i$ and $x_{i+1}$, there exists some path $p_i$ in $X$ from $x_i$ to $x_{i+1}$. I define the following for all positive integers $i$ and $j$ such that $i + 1 < j$: $$ p_{i \to i+1} = p_i \quad\text{and}\quad p_{i \to j} = p_i \cdot p_{i+1 \to j}, $$ where $\cdot$ denotes path composition; for any paths $f, g : [0, 1] \to X$ such that $f(1) = g(0)$, $(f \cdot g) : [0, 1] \to X$ is a path such that $$ (f \cdot g)(t) = \begin{cases} f(2t) & 0 \leq t \leq {\textstyle\frac{1}{2}}\\ g(2t - 1) & {\textstyle\frac{1}{2}} < t \leq 1. \end{cases} $$ It can be seen that $p_{i \to j}$ is a path from $x_i$ to $x_j$. Consider the sequence $p_{1 \to 2}, p_{1 \to 3}, p_{1 \to 4}, \dotsc$. Visually, the successive terms of the sequence show a path stretching a bit of its end to touch the next $x_k$. Under what conditions can I say that this sequence of paths converges to a path $p$ in $X$ with the following? $$ p(1 - 2^{-k}) = x_{k+1} \quad\text{and}\quad p(1) = x. $$ For instance, does it suffice to require that $X$ has a metric $d$, and $\lim\limits_{i \to \infty} L(p_i) = 0$, where $L(q)$ denotes the length of a path $q$? However, this would impose a metric on $X$, which is a magic bullet I'd rather not use. In fact, does the notion of path length exist? Are there further conditions I must pose on $X$ so that the following limit exists for some path $q : [0, 1] \to X$? $$ L(q) = \lim_{n\to\infty}\sum_{m=1}^n d\left(\textstyle q(\frac{m-1}{n}),q(\frac{m}{n})\right). $$ (The following ""appendix"" may not be fully relevant but it seems interesting) Consider the function $P : [0, 1]^2 \to X$, where $$ P(t,s) = \begin{cases} p_k(2 - 2^k(1 - t)) & t \leq s < 1, k = \lceil-\log_2(1 - t)\rceil\\ p_k\left(2 - 2^k\left(1 - \frac{t + s}{2}\right)\right) & s \leq t < 1, k = \left\lceil-\log_2\left(1 - \frac{t + s}{2}\right)\right\rceil\\ x & t = s = 1 \end{cases} $$ $P$ can be visualized in the graph below: Solid lines contain points in the $ts$-plane which are mapped by $P$ to the same point in $X$. If the sequence $p_{1 \to 2}, p_{1 \to 3}, p_{1 \to 4}, \dotsc$ indeed converges to some path $p$ in $X$, then we have the following: $P : [0, 1] \times [0, 1] \to X$ is a homotopy from $p_1$ to $p$. $P(1 - 2^{2 - k}, t) = p_{1 \to k}(t)$ for any integer $k \geq 2$ and $t \in [0, 1]$. However, does the converse hold? If $P$ is continuous, then does the ""limit path"" $p$ exist?","Given a path connected topological space $X$, consider a sequence $x_1, x_2, x_3, \dotsc$ of points in $X$ converging at some $x \in X$. For each $x_i$ and $x_{i+1}$, there exists some path $p_i$ in $X$ from $x_i$ to $x_{i+1}$. I define the following for all positive integers $i$ and $j$ such that $i + 1 < j$: $$ p_{i \to i+1} = p_i \quad\text{and}\quad p_{i \to j} = p_i \cdot p_{i+1 \to j}, $$ where $\cdot$ denotes path composition; for any paths $f, g : [0, 1] \to X$ such that $f(1) = g(0)$, $(f \cdot g) : [0, 1] \to X$ is a path such that $$ (f \cdot g)(t) = \begin{cases} f(2t) & 0 \leq t \leq {\textstyle\frac{1}{2}}\\ g(2t - 1) & {\textstyle\frac{1}{2}} < t \leq 1. \end{cases} $$ It can be seen that $p_{i \to j}$ is a path from $x_i$ to $x_j$. Consider the sequence $p_{1 \to 2}, p_{1 \to 3}, p_{1 \to 4}, \dotsc$. Visually, the successive terms of the sequence show a path stretching a bit of its end to touch the next $x_k$. Under what conditions can I say that this sequence of paths converges to a path $p$ in $X$ with the following? $$ p(1 - 2^{-k}) = x_{k+1} \quad\text{and}\quad p(1) = x. $$ For instance, does it suffice to require that $X$ has a metric $d$, and $\lim\limits_{i \to \infty} L(p_i) = 0$, where $L(q)$ denotes the length of a path $q$? However, this would impose a metric on $X$, which is a magic bullet I'd rather not use. In fact, does the notion of path length exist? Are there further conditions I must pose on $X$ so that the following limit exists for some path $q : [0, 1] \to X$? $$ L(q) = \lim_{n\to\infty}\sum_{m=1}^n d\left(\textstyle q(\frac{m-1}{n}),q(\frac{m}{n})\right). $$ (The following ""appendix"" may not be fully relevant but it seems interesting) Consider the function $P : [0, 1]^2 \to X$, where $$ P(t,s) = \begin{cases} p_k(2 - 2^k(1 - t)) & t \leq s < 1, k = \lceil-\log_2(1 - t)\rceil\\ p_k\left(2 - 2^k\left(1 - \frac{t + s}{2}\right)\right) & s \leq t < 1, k = \left\lceil-\log_2\left(1 - \frac{t + s}{2}\right)\right\rceil\\ x & t = s = 1 \end{cases} $$ $P$ can be visualized in the graph below: Solid lines contain points in the $ts$-plane which are mapped by $P$ to the same point in $X$. If the sequence $p_{1 \to 2}, p_{1 \to 3}, p_{1 \to 4}, \dotsc$ indeed converges to some path $p$ in $X$, then we have the following: $P : [0, 1] \times [0, 1] \to X$ is a homotopy from $p_1$ to $p$. $P(1 - 2^{2 - k}, t) = p_{1 \to k}(t)$ for any integer $k \geq 2$ and $t \in [0, 1]$. However, does the converse hold? If $P$ is continuous, then does the ""limit path"" $p$ exist?",,"['general-topology', 'sequences-and-series', 'limits', 'metric-spaces']"
9,A well-known sines limit,A well-known sines limit,,"The following question is related to the answer i've found for this limit and i like to know if it's valid. I need to find the following limit: $$\lim_{x\rightarrow0} \frac{\sin(kx)}{x} $$ where k is a fixed positive integer. Proof: Here we'are going to appeal to a very well known inequality: $$ \sin(x) < x < \tan(x),\space 0<x<\frac{\pi}{2}$$ Then we have that: $$ \sin(kx) < kx < \tan(kx),\space 0<x<\frac{\pi}{2k}$$ From the above inequality we get that:  $$\cos(kx) < \frac{\sin(kx)}{kx}< 1$$ After multiplying the inequality by k and taking the limit when x goes to ${0}$ we get that: $$\lim_{x\rightarrow0}\space k\cos(kx) < \lim_{x\rightarrow0}\frac{\sin(kx)}{x}< k$$ By Squeeze Theorem the limit is $k$. For such an answer i received a downvote because in the last inequality i used $""<""$ instead of $""\leq""$. I'd like to know your opinion and if i'm wrong then i want to correct it. Thanks.","The following question is related to the answer i've found for this limit and i like to know if it's valid. I need to find the following limit: $$\lim_{x\rightarrow0} \frac{\sin(kx)}{x} $$ where k is a fixed positive integer. Proof: Here we'are going to appeal to a very well known inequality: $$ \sin(x) < x < \tan(x),\space 0<x<\frac{\pi}{2}$$ Then we have that: $$ \sin(kx) < kx < \tan(kx),\space 0<x<\frac{\pi}{2k}$$ From the above inequality we get that:  $$\cos(kx) < \frac{\sin(kx)}{kx}< 1$$ After multiplying the inequality by k and taking the limit when x goes to ${0}$ we get that: $$\lim_{x\rightarrow0}\space k\cos(kx) < \lim_{x\rightarrow0}\frac{\sin(kx)}{x}< k$$ By Squeeze Theorem the limit is $k$. For such an answer i received a downvote because in the last inequality i used $""<""$ instead of $""\leq""$. I'd like to know your opinion and if i'm wrong then i want to correct it. Thanks.",,"['real-analysis', 'limits']"
10,Elementary Real Analysis - Let me know if I'm on the right track,Elementary Real Analysis - Let me know if I'm on the right track,,"So I'm guessing this is a pretty simple example for this topic but I just want to check myself as I'm new to this analysis area and not sure that what I'm saying is mathematically sound.. The question is show that $\lim\limits_{x \to -\infty}\frac{x^2+1}{x^2-1} =1$. So we must show as $x \to -\infty$, $\left|\tfrac{x^2+1}{x^2-1}  -1\right| < \epsilon$. My attempt is : \begin{align*} \left|\dfrac{x^2+1}{x^2-1}  -1\right| &=\left|\dfrac{x^2+1}{x^2-1}  - \dfrac{x^2-1}{x^2-1} \right| \\\\ &=\left|\dfrac{(x^2+1)-(x^2-1)}{x^2-1} \right| \\\\ &=\left|\dfrac{x^2-x^2+1+1}{x^2-1}\right| \\\\ &=\left|\dfrac{2}{x^2-1}\right| < \left|\dfrac{2}{x^2-4}\right| = \left|\dfrac{2}{(x+2)(x-2)}\right| \\ \end{align*} Now is where I'm not 100 percent sure that what I'm doing is right, can we then say that as $x$ approaches negative infinity, $(x+2)$ and $(x-2)$ become very large and negative, and therefore $2/(x+2)(x-2)$ becomes smaller and smaller and so, for any $\epsilon>0$, $$\epsilon > \left|\dfrac{2}{(x+2)(x-2)}\right| > \left|\dfrac{2}{x^2-1}\right| = \left|\dfrac{x^2+1}{x^2-1}  -1\right|$$   thus proving the original problem ... is this ok/rigorous? please dont be too hard on me I'm really still just trying to grasp the ideas and understand exactly what we are 'allowed' to do .. thanks for any help","So I'm guessing this is a pretty simple example for this topic but I just want to check myself as I'm new to this analysis area and not sure that what I'm saying is mathematically sound.. The question is show that $\lim\limits_{x \to -\infty}\frac{x^2+1}{x^2-1} =1$. So we must show as $x \to -\infty$, $\left|\tfrac{x^2+1}{x^2-1}  -1\right| < \epsilon$. My attempt is : \begin{align*} \left|\dfrac{x^2+1}{x^2-1}  -1\right| &=\left|\dfrac{x^2+1}{x^2-1}  - \dfrac{x^2-1}{x^2-1} \right| \\\\ &=\left|\dfrac{(x^2+1)-(x^2-1)}{x^2-1} \right| \\\\ &=\left|\dfrac{x^2-x^2+1+1}{x^2-1}\right| \\\\ &=\left|\dfrac{2}{x^2-1}\right| < \left|\dfrac{2}{x^2-4}\right| = \left|\dfrac{2}{(x+2)(x-2)}\right| \\ \end{align*} Now is where I'm not 100 percent sure that what I'm doing is right, can we then say that as $x$ approaches negative infinity, $(x+2)$ and $(x-2)$ become very large and negative, and therefore $2/(x+2)(x-2)$ becomes smaller and smaller and so, for any $\epsilon>0$, $$\epsilon > \left|\dfrac{2}{(x+2)(x-2)}\right| > \left|\dfrac{2}{x^2-1}\right| = \left|\dfrac{x^2+1}{x^2-1}  -1\right|$$   thus proving the original problem ... is this ok/rigorous? please dont be too hard on me I'm really still just trying to grasp the ideas and understand exactly what we are 'allowed' to do .. thanks for any help",,"['real-analysis', 'functions', 'limits']"
11,different concepts of abel summation?,different concepts of abel summation?,,"I have a little question concerning abel-summmation. In some books the n-th abel-mean of a sequence $(x_n) \subset \mathbb{K}$ is defined as: $$ A_{n,r}[x_n] = (1 - r) \sum_{k=0}^{n} x_k r^k $$ In other books the abel-means are defined as: $$ A_{n,r}[x_n] = \sum_{k=0}^{n} x_k r^k $$ In both sources it says, that $(x_n)$ converges to $x$ is the sense of abel if $$ \lim_{r \nearrow 1}\lim_{n\to\infty}A_{n,r}[x_n] = x $$ Here is my question: Are these two concepts equivalent? Are the limits (if exist) in both cases the same? With best regards, Mat","I have a little question concerning abel-summmation. In some books the n-th abel-mean of a sequence $(x_n) \subset \mathbb{K}$ is defined as: $$ A_{n,r}[x_n] = (1 - r) \sum_{k=0}^{n} x_k r^k $$ In other books the abel-means are defined as: $$ A_{n,r}[x_n] = \sum_{k=0}^{n} x_k r^k $$ In both sources it says, that $(x_n)$ converges to $x$ is the sense of abel if $$ \lim_{r \nearrow 1}\lim_{n\to\infty}A_{n,r}[x_n] = x $$ Here is my question: Are these two concepts equivalent? Are the limits (if exist) in both cases the same? With best regards, Mat",,"['analysis', 'limits', 'ergodic-theory']"
12,Prove that the order of convergence must be $\geq 1$,Prove that the order of convergence must be,\geq 1,"Given a sequence $\{x_n\}_n$ and real numbers $c > 0$ and $L$, such that $\displaystyle\lim_{n \to \infty}x_n - L = 0$ and $\displaystyle\lim_{n \to \infty} \frac{| x_{n+1} - L |}{|x_n - L|^p} = c$, prove that $p \geq 1$. This is assumed without proof in my textbook and I'd like a rigorous one, but I can't come up with it.","Given a sequence $\{x_n\}_n$ and real numbers $c > 0$ and $L$, such that $\displaystyle\lim_{n \to \infty}x_n - L = 0$ and $\displaystyle\lim_{n \to \infty} \frac{| x_{n+1} - L |}{|x_n - L|^p} = c$, prove that $p \geq 1$. This is assumed without proof in my textbook and I'd like a rigorous one, but I can't come up with it.",,"['calculus', 'limits']"
13,Basic Epsilon-N proof clarification,Basic Epsilon-N proof clarification,,"It's been a couple years since I've done analysis, so I was hoping someone could point out any possible flaws I have in the following proof. Prove $\lim_{n\to\infty}\frac{2^n}{n!} = 0$. For $n > 2$, we have: $\lim_{n\to\infty}\frac{2^n}{n!} = \lim_{n\to\infty}\frac{2^2}{2!}\times\frac{2}{3}\times\frac{2}{4}\times\frac{2}{5}\times\cdots\times\frac{2}{n}$. Need to show that $\frac{2^2}{2!}\times\frac{2}{3}\times\frac{2}{4}\times\frac{2}{5}\times\cdots \to 0$. In other words, need to show that $\lim_{n\to\infty}\frac{2}{n} = 0$: $\forall\epsilon\in\mathbb{N}^+, \exists N\in\mathbb{N} $ such that $\forall n>N$ we have $\frac{2}{n} < \epsilon$. Take $N = \frac{2}{\epsilon}$. Since $n>N$ we have $n > \frac{2}{\epsilon}$. Thus $\frac{2}{n} < \epsilon$. Therefore  $\lim_{n\to\infty}\frac{2}{n} = 0$. And so $\lim_{n\to\infty}\frac{2^2}{2!}\times\frac{2}{3}\times\frac{2}{4}\times\frac{2}{5}\times...\times\frac{2}{n} = 0$. Then $\lim_{n\to\infty}\frac{2^n}{n!} = 0$.","It's been a couple years since I've done analysis, so I was hoping someone could point out any possible flaws I have in the following proof. Prove $\lim_{n\to\infty}\frac{2^n}{n!} = 0$. For $n > 2$, we have: $\lim_{n\to\infty}\frac{2^n}{n!} = \lim_{n\to\infty}\frac{2^2}{2!}\times\frac{2}{3}\times\frac{2}{4}\times\frac{2}{5}\times\cdots\times\frac{2}{n}$. Need to show that $\frac{2^2}{2!}\times\frac{2}{3}\times\frac{2}{4}\times\frac{2}{5}\times\cdots \to 0$. In other words, need to show that $\lim_{n\to\infty}\frac{2}{n} = 0$: $\forall\epsilon\in\mathbb{N}^+, \exists N\in\mathbb{N} $ such that $\forall n>N$ we have $\frac{2}{n} < \epsilon$. Take $N = \frac{2}{\epsilon}$. Since $n>N$ we have $n > \frac{2}{\epsilon}$. Thus $\frac{2}{n} < \epsilon$. Therefore  $\lim_{n\to\infty}\frac{2}{n} = 0$. And so $\lim_{n\to\infty}\frac{2^2}{2!}\times\frac{2}{3}\times\frac{2}{4}\times\frac{2}{5}\times...\times\frac{2}{n} = 0$. Then $\lim_{n\to\infty}\frac{2^n}{n!} = 0$.",,"['calculus', 'sequences-and-series', 'limits']"
14,Continuous function and limit,Continuous function and limit,,"Let's suppose $f$ a continuous function where $$\lim_{x\to+\infty} { f(x+1)- f(x)}=l.$$ How to prove that  $$\lim_{x\to +\infty}\frac{f(x)}{x}=l\,?$$ I've already proove by Cesaro theorem that  $$\lim_{n\to +\infty}\frac{f(n)          }{n}=l,$$ where $n$ is an integer. How to continue it?","Let's suppose $f$ a continuous function where $$\lim_{x\to+\infty} { f(x+1)- f(x)}=l.$$ How to prove that  $$\lim_{x\to +\infty}\frac{f(x)}{x}=l\,?$$ I've already proove by Cesaro theorem that  $$\lim_{n\to +\infty}\frac{f(n)          }{n}=l,$$ where $n$ is an integer. How to continue it?",,"['calculus', 'functions', 'limits']"
15,limit superior of a sequence - showing an alternate definition,limit superior of a sequence - showing an alternate definition,,"I am wondering if anybody can help me with a problem regarding the definition of the limit superior of a sequence - or rather showing an alternate but equivalent defintion holds. The question is: The limit superior of a numerical sequence $\{x_{k}\}$ (presumably this means the sequence is real valued) can be defined as the supremum of the set of limit points of the sequence. Show that this is the same thing as defining $$ \limsup _{n \to \infty} ~ x_n = \bigwedge_{n=1}^{\infty} \bigvee_{k=n}^{\infty} x_k .$$ This question comes from Chapter 4 of ""Probability and Measure"" by Patrick Billingsley. The problem is that Billingsley assumes the reader knows what the symbols $\bigwedge$ and $\bigvee$ are - but I do not! The best I have come up with is that they are the ""meet"" and the ""join"" symbols used in a lattice? Could anybody shed some light on how this problem might be attacked? Billingsley does give some hints to the problem. He says that the following are all equivalent: $x<x_k$ i.o. OR $x<x_k$ for some $k\geq n$, for all $n\geq 1$ OR $x<\bigvee_{k=n}^{\infty} x_k$ for all $n\geq 1$. Using this kind of logic he shows that $$\bigwedge_{n=1}^{\infty}\bigvee_{k=n}^{\infty}x_{k} = \sup\{ x: x<x_n \text{ i.o.}\} .$$ Apparently the supremum of the set above can be seen to be the supremum of the limit points of the sequence - this would prove the result. I think I follow this derivation but I was taking the $\bigvee$ and $\bigwedge$ symbols to simply mean $\bigcup$ and $\bigcap$ for singleton sets $\{x_{k}\}$. I think this is the wrong assumption. I also cannot see the last assertion about the limit points of the sequence. Any help would be much appreciated.","I am wondering if anybody can help me with a problem regarding the definition of the limit superior of a sequence - or rather showing an alternate but equivalent defintion holds. The question is: The limit superior of a numerical sequence $\{x_{k}\}$ (presumably this means the sequence is real valued) can be defined as the supremum of the set of limit points of the sequence. Show that this is the same thing as defining $$ \limsup _{n \to \infty} ~ x_n = \bigwedge_{n=1}^{\infty} \bigvee_{k=n}^{\infty} x_k .$$ This question comes from Chapter 4 of ""Probability and Measure"" by Patrick Billingsley. The problem is that Billingsley assumes the reader knows what the symbols $\bigwedge$ and $\bigvee$ are - but I do not! The best I have come up with is that they are the ""meet"" and the ""join"" symbols used in a lattice? Could anybody shed some light on how this problem might be attacked? Billingsley does give some hints to the problem. He says that the following are all equivalent: $x<x_k$ i.o. OR $x<x_k$ for some $k\geq n$, for all $n\geq 1$ OR $x<\bigvee_{k=n}^{\infty} x_k$ for all $n\geq 1$. Using this kind of logic he shows that $$\bigwedge_{n=1}^{\infty}\bigvee_{k=n}^{\infty}x_{k} = \sup\{ x: x<x_n \text{ i.o.}\} .$$ Apparently the supremum of the set above can be seen to be the supremum of the limit points of the sequence - this would prove the result. I think I follow this derivation but I was taking the $\bigvee$ and $\bigwedge$ symbols to simply mean $\bigcup$ and $\bigcap$ for singleton sets $\{x_{k}\}$. I think this is the wrong assumption. I also cannot see the last assertion about the limit points of the sequence. Any help would be much appreciated.",,"['sequences-and-series', 'limits', 'lattice-orders', 'limsup-and-liminf']"
16,Calculate $ \lim_{n \to \infty} \left( \sum_{k=1}^n \left( \sqrt{n^4 + k} \cdot \sin \left( 2\pi \cdot \frac{k}{n} \right) \right) \right) \ $,Calculate, \lim_{n \to \infty} \left( \sum_{k=1}^n \left( \sqrt{n^4 + k} \cdot \sin \left( 2\pi \cdot \frac{k}{n} \right) \right) \right) \ ,"$$ \mbox{What is the value of this limit ?:}\quad \lim_{n \to \infty}\sum_{k = 1}^{n}\sqrt{n^{4} + k\,}\ \sin\left(2\pi\,\frac{k}{n}\right) $$ I tried looking for sum Riemann sums first, nothing. I tried to bound the sum and then look for Riemann sums, nothing. I rewrote the sum making pairs: term $k$ with term $n - k$ to get the $\sin$ positive. I am stuck. Can someone help me with this limit ?.","I tried looking for sum Riemann sums first, nothing. I tried to bound the sum and then look for Riemann sums, nothing. I rewrote the sum making pairs: term with term to get the positive. I am stuck. Can someone help me with this limit ?.","
\mbox{What is the value of this limit ?:}\quad
\lim_{n \to \infty}\sum_{k = 1}^{n}\sqrt{n^{4} + k\,}\
\sin\left(2\pi\,\frac{k}{n}\right)
 k n - k \sin","['integration', 'sequences-and-series', 'limits', 'trigonometry']"
17,Limit with a geometric interpretation,Limit with a geometric interpretation,,"Let $f:ℝ \to ℝ$ be a $C^∞$ curve. Determine the following limit; $$\lim_{x_1 \to x_2} \dfrac{ \int_{x_1}^{x_2} \sqrt{1+f'(x)^2} dx}{\sqrt{(x_2-x_1)^2+(f(x_2)-f(x_1))^2}}$$ My attempt: I recognized that the numerator represents the arc length of the curve from $P(x_1,f(x_1)$ to $Q(x_2,f(x_2))$ whereas the denominator is the length of the chord $PQ$ . Intuitively, it seems as if when $Q$ tends to $P$ , the numerator and denominator will become nearly equal so I think the limit should tend to $1$ . I quickly verified this in case of a circle and it seems to work. However, I do not know how to prove this for any general function; applying L'hopital's rule once (after letting $x_2=x_1+h$ ) just complicates the limit further although it gets rid of the integral. And, I was looking forward to using this observation about arc length and chord length in my answer but I couldn't. Is there perhaps a better way of doing this using the squeeze theorem? I would appreciate any method which does not use L'hopital's rule and is not complicated computationally. Is my conjecture even correct?","Let be a curve. Determine the following limit; My attempt: I recognized that the numerator represents the arc length of the curve from to whereas the denominator is the length of the chord . Intuitively, it seems as if when tends to , the numerator and denominator will become nearly equal so I think the limit should tend to . I quickly verified this in case of a circle and it seems to work. However, I do not know how to prove this for any general function; applying L'hopital's rule once (after letting ) just complicates the limit further although it gets rid of the integral. And, I was looking forward to using this observation about arc length and chord length in my answer but I couldn't. Is there perhaps a better way of doing this using the squeeze theorem? I would appreciate any method which does not use L'hopital's rule and is not complicated computationally. Is my conjecture even correct?","f:ℝ \to ℝ C^∞ \lim_{x_1 \to x_2} \dfrac{ \int_{x_1}^{x_2} \sqrt{1+f'(x)^2} dx}{\sqrt{(x_2-x_1)^2+(f(x_2)-f(x_1))^2}} P(x_1,f(x_1) Q(x_2,f(x_2)) PQ Q P 1 x_2=x_1+h","['limits', 'definite-integrals', 'limits-without-lhopital', 'arc-length']"
18,"Suppose a sequence $\{a_n\}$, $a_{n+1}=a_n+\frac1{a_n},n\in\mathbb{N}^*$, then $a_n=\sqrt{2n}+\frac{\sqrt2\ln n}{8\sqrt n}+O(\frac{\ln n}{n^{3/2}})$?","Suppose a sequence , , then ?","\{a_n\} a_{n+1}=a_n+\frac1{a_n},n\in\mathbb{N}^* a_n=\sqrt{2n}+\frac{\sqrt2\ln n}{8\sqrt n}+O(\frac{\ln n}{n^{3/2}})","Question Suppose a sequence $\{ a_n \}$ , $a_{n+1}=a_n+\displaystyle\frac1{a_n},a_1=1,n\in\mathbb{N}^*$ , then prove: $a_n=\sqrt{2n}+\displaystyle\frac{\sqrt2\ln n}{8\sqrt n}+O(\displaystyle\frac{\ln n}{n^{3/2}})$ when $n\to\infty$ . My attempt I think this question should be divided into 3 steps. Step 1 Prove: $a_n\sim\sqrt{2n}\ (n\to\infty)$ . To achieve this, first I prove that $a_n\to+\infty$ when $n\to\infty$ , then I use $\text{Stolz theorem}$ to prove that $\displaystyle\lim_{n\to\infty}\frac{a_n^2}{2n}=1.$ Step 2 Prove: $\displaystyle\frac{\sqrt n(a_n-\sqrt{2n})}{\ln n}\to\frac{\sqrt2}8\ (n\to\infty)$ . Because $\displaystyle\frac{\sqrt n(a_n-\sqrt{2n})}{\ln n}=\displaystyle\frac{\sqrt n(a_n^2-2n)}{(a_n+\sqrt{2n})\ln n},$ and $$ \lim_{n\to\infty}\frac{\sqrt n}{a_n+\sqrt{2n}}=\lim_{n\to\infty}\frac1{\frac{a_n}{\sqrt{2n}}+1}\cdot\frac1{\sqrt2}=\frac{\sqrt2}4. $$ $$ \lim_{n\to\infty}\frac{a_n^2-2n}{\ln n}\overset{\text{Stolz}}{=}\lim_{n\to\infty}\frac{a_{n+1}^2-a_n^2-2}{\ln(1+\frac1n)}=\lim_{n\to\infty}n(a_{n+1}^2-a_n^2-2)=\lim_{n\to\infty}n\cdot\frac1{a_n^2}=\frac12.\quad(\ln(1+\frac1n)\sim\frac1n\ (n\to\infty)) $$ Step 3 Prove: $\displaystyle\frac{8\sqrt n(a_n-\sqrt{2n})-\sqrt2\ln n)\cdot n}{8\ln n}=O(1)\ (n\to\infty)$ . This is the part which confuses me a lot, the target seems too tricky for me, I hope someone can help me prove this. Inspiration When I review the progress of the proof, it is a bit like estimating the magnitude of a sequence. It reminds me of the Taylor expansion in functions. So is there a similar expansion in sequences, or where do these terms come from? I think this can give me a deeper insight into this kind of questions. Any sort of help is welcomed!","Question Suppose a sequence , , then prove: when . My attempt I think this question should be divided into 3 steps. Step 1 Prove: . To achieve this, first I prove that when , then I use to prove that Step 2 Prove: . Because and Step 3 Prove: . This is the part which confuses me a lot, the target seems too tricky for me, I hope someone can help me prove this. Inspiration When I review the progress of the proof, it is a bit like estimating the magnitude of a sequence. It reminds me of the Taylor expansion in functions. So is there a similar expansion in sequences, or where do these terms come from? I think this can give me a deeper insight into this kind of questions. Any sort of help is welcomed!","\{ a_n \} a_{n+1}=a_n+\displaystyle\frac1{a_n},a_1=1,n\in\mathbb{N}^* a_n=\sqrt{2n}+\displaystyle\frac{\sqrt2\ln n}{8\sqrt n}+O(\displaystyle\frac{\ln n}{n^{3/2}}) n\to\infty a_n\sim\sqrt{2n}\ (n\to\infty) a_n\to+\infty n\to\infty \text{Stolz theorem} \displaystyle\lim_{n\to\infty}\frac{a_n^2}{2n}=1. \displaystyle\frac{\sqrt n(a_n-\sqrt{2n})}{\ln n}\to\frac{\sqrt2}8\ (n\to\infty) \displaystyle\frac{\sqrt n(a_n-\sqrt{2n})}{\ln n}=\displaystyle\frac{\sqrt n(a_n^2-2n)}{(a_n+\sqrt{2n})\ln n}, 
\lim_{n\to\infty}\frac{\sqrt n}{a_n+\sqrt{2n}}=\lim_{n\to\infty}\frac1{\frac{a_n}{\sqrt{2n}}+1}\cdot\frac1{\sqrt2}=\frac{\sqrt2}4.
 
\lim_{n\to\infty}\frac{a_n^2-2n}{\ln n}\overset{\text{Stolz}}{=}\lim_{n\to\infty}\frac{a_{n+1}^2-a_n^2-2}{\ln(1+\frac1n)}=\lim_{n\to\infty}n(a_{n+1}^2-a_n^2-2)=\lim_{n\to\infty}n\cdot\frac1{a_n^2}=\frac12.\quad(\ln(1+\frac1n)\sim\frac1n\ (n\to\infty))
 \displaystyle\frac{8\sqrt n(a_n-\sqrt{2n})-\sqrt2\ln n)\cdot n}{8\ln n}=O(1)\ (n\to\infty)","['sequences-and-series', 'limits', 'analysis']"
19,Doubt regarding limits on riemann sums,Doubt regarding limits on riemann sums,,"Find $$\lim_{n\to\infty}\frac{1^p+3^p+\ldots+(2n+1)^p}{n^{p+1}}$$ I found a solution here which goes like this: By Riemann sums, for any $p>-1$ : $$ \frac{1}{n}\sum_{k=0}^{n}\left(\frac{2k+1}{n}\right)^p \xrightarrow{n\to +\infty}\int_{0}^{1}(2x)^p\,dx = \color{red}{\frac{2^p}{p+1}}.$$ I don't understand how you can change $\frac{2k+1}{n}$ to $2x$ there, like if there was $\frac{2k}{n}$ instead, I know we can easily change it to $2x$ , but there is an extra $+1$ , how can we igonre that term?","Find I found a solution here which goes like this: By Riemann sums, for any : I don't understand how you can change to there, like if there was instead, I know we can easily change it to , but there is an extra , how can we igonre that term?","\lim_{n\to\infty}\frac{1^p+3^p+\ldots+(2n+1)^p}{n^{p+1}} p>-1  \frac{1}{n}\sum_{k=0}^{n}\left(\frac{2k+1}{n}\right)^p \xrightarrow{n\to +\infty}\int_{0}^{1}(2x)^p\,dx = \color{red}{\frac{2^p}{p+1}}. \frac{2k+1}{n} 2x \frac{2k}{n} 2x +1","['limits', 'riemann-sum']"
20,Defining a custom function,Defining a custom function,,"The Problem Define a function $f:\mathbb{R\times R\rightarrow R}$ which satisfies the following properties: $$\frac{\partial f(x,k)}{\partial x}=0\text{ at }x=0$$ $$\forall k\in\mathbb{R}:f(1,k)=1$$ $$\forall x\in[0,1]:\lim_{k\rightarrow+\infty}f(x,k)=x$$ The function also has to possess $\mathbb{C}^{\infty}$ smoothness , i.e, it should be infinitely differentiable, with respect to the first variable. The Context I figured that anyone with a graphing calculator can go about plotting graphs, if the equation for it is given (at least for most well-behaved equations). However, creating a custom function, given only a sketch of how its plot should look seemed interesting. My target was a function $f(x,k)$ with $k$ being a tunable parameter. Its plot had to look like this: Basically, the function is anchored at the point $(1,1)$ , has a slope of $0$ at $x=0$ , and increasing the value of $k$ makes it look closer to $y=x$ . An Example $$f\left(x,k\right)=\frac{2}{\pi}\cos^{-1}\left(\frac{1}{1+e^{-k}}\cos\left(\frac{\pi x}{2}\right)\right)$$ This was a function that I came up with. Here's the Desmos graph. Finally I am looking for more functions that satisfy the given criteria, hoping that they will lead to more insight into this graph-to-function type of problems. While functions related to the example case are OK of course, I am looking for more ways to approach the question at hand.","The Problem Define a function which satisfies the following properties: The function also has to possess smoothness , i.e, it should be infinitely differentiable, with respect to the first variable. The Context I figured that anyone with a graphing calculator can go about plotting graphs, if the equation for it is given (at least for most well-behaved equations). However, creating a custom function, given only a sketch of how its plot should look seemed interesting. My target was a function with being a tunable parameter. Its plot had to look like this: Basically, the function is anchored at the point , has a slope of at , and increasing the value of makes it look closer to . An Example This was a function that I came up with. Here's the Desmos graph. Finally I am looking for more functions that satisfy the given criteria, hoping that they will lead to more insight into this graph-to-function type of problems. While functions related to the example case are OK of course, I am looking for more ways to approach the question at hand.","f:\mathbb{R\times R\rightarrow R} \frac{\partial f(x,k)}{\partial x}=0\text{ at }x=0 \forall k\in\mathbb{R}:f(1,k)=1 \forall x\in[0,1]:\lim_{k\rightarrow+\infty}f(x,k)=x \mathbb{C}^{\infty} f(x,k) k (1,1) 0 x=0 k y=x f\left(x,k\right)=\frac{2}{\pi}\cos^{-1}\left(\frac{1}{1+e^{-k}}\cos\left(\frac{\pi x}{2}\right)\right)","['limits', 'functions', 'recreational-mathematics', 'graphing-functions', 'smooth-functions']"
21,"Convergence of series $\sum_{n=1}^{\infty}x_n^{\alpha}$ subjected to $x_1\in(0,1),x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$.",Convergence of series  subjected to .,"\sum_{n=1}^{\infty}x_n^{\alpha} x_1\in(0,1),x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}","Suppose $x_1\in(0,1),x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$ , find the values of $\alpha\in\mathbb{R}$ for which the series $\sum_{n=1}^{\infty}x_n^{\alpha}$ is convergent. The following is my solution, please help me to check if there is something which is not right. Any help and comments will welcome. Other methods also welcome! (PS: I also post this in AoPs $x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$ . Solution: It is not difficult to prove that: $\{x_n\}$ is strictly decreasing and $$0<x_n<1,\quad \lim_{n\to\infty }x_n=0.$$ By $x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}}$ , we get $$\frac{1}{x_{n+1}}=\frac1{x_n}+\frac{1}{\sqrt{n}-x_n},$$ then $$\frac{1}{x_{n+1}}-\frac{1}{x_1}=\sum_{k=1}^{n}\frac{1}{\sqrt{k}-x_k}.$$ So $$\frac{1}{x_1}+\sum_{k=1}^{n}\frac{1}{\sqrt{k}}<\frac{1}{x_{n+1}}=\frac{1}{x_1}+\sum_{k=1}^{n}\frac{1}{\sqrt{k}-x_k} <\frac{1}{x_1}+\frac{1}{1-x_1}+\sum_{k=2}^{n}\frac{1}{\sqrt{k}-1}. $$ As we know $$\sum_{k=1}^n\,\frac{1}{\sqrt{k}}\sim2\sqrt{n},\quad n\to\infty,$$ we know that $$x_n\sim\frac{1}{2\sqrt{n}},\quad n\to\infty.$$ Note the limits: by Stolz's theorem, $$\lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}=2 =\lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=2}^{n}\frac{1}{\sqrt{k}-1},$$ this means that $$\lim_{n\to\infty}2\sqrt nx_{n+1}=1,$$ Hence $$x_n\sim\frac1{2\sqrt n},\quad n\to\infty.$$ So $\sum_{n=1}^{\infty}x_n^{\alpha}$ is convergent if and only if $\alpha>2$ . Here also another method for $$x_n\sim\frac1{2\sqrt n},\quad n\to\infty,$$ but  we should know $\lim_{n\to\infty}\sqrt nx_n$ exists first and the limit is positive, if someone can give the proof of the existence and $>0$ . \begin{align*} \lim_{n\to\infty}\sqrt nx_n &=\lim_{n\to\infty}\frac{x_n}{\frac1{\sqrt{n}}}\\ &=\lim_{n\to\infty}\frac{x_n-x_{n+1}}{\frac1{\sqrt{n}}-\frac1{\sqrt{n+1}}}\\ &=\lim_{n\to\infty}\frac{\frac{x_n^2}{\sqrt{n}}}{\frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{n(n+1)}}}\\ &=\lim_{n\to\infty}\sqrt{n+1}(\sqrt{n+1}+\sqrt{n})x_n^2\\ &=\lim_{n\to\infty}\frac{\sqrt{n+1}(\sqrt{n+1}+\sqrt{n})}{n}(\sqrt nx_n)^2\\ &=2\lim_{n\to\infty}(\sqrt nx_n)^2, \end{align*} this implies $$\lim_{n\to\infty}\sqrt nx_n=\frac{1}{2}.$$","Suppose , find the values of for which the series is convergent. The following is my solution, please help me to check if there is something which is not right. Any help and comments will welcome. Other methods also welcome! (PS: I also post this in AoPs . Solution: It is not difficult to prove that: is strictly decreasing and By , we get then So As we know we know that Note the limits: by Stolz's theorem, this means that Hence So is convergent if and only if . Here also another method for but  we should know exists first and the limit is positive, if someone can give the proof of the existence and . this implies","x_1\in(0,1),x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}} \alpha\in\mathbb{R} \sum_{n=1}^{\infty}x_n^{\alpha} x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}} \{x_n\} 0<x_n<1,\quad \lim_{n\to\infty }x_n=0. x_{n+1}=x_n-\frac{x_n^2}{\sqrt{n}} \frac{1}{x_{n+1}}=\frac1{x_n}+\frac{1}{\sqrt{n}-x_n}, \frac{1}{x_{n+1}}-\frac{1}{x_1}=\sum_{k=1}^{n}\frac{1}{\sqrt{k}-x_k}. \frac{1}{x_1}+\sum_{k=1}^{n}\frac{1}{\sqrt{k}}<\frac{1}{x_{n+1}}=\frac{1}{x_1}+\sum_{k=1}^{n}\frac{1}{\sqrt{k}-x_k}
<\frac{1}{x_1}+\frac{1}{1-x_1}+\sum_{k=2}^{n}\frac{1}{\sqrt{k}-1}.
 \sum_{k=1}^n\,\frac{1}{\sqrt{k}}\sim2\sqrt{n},\quad n\to\infty, x_n\sim\frac{1}{2\sqrt{n}},\quad n\to\infty. \lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=1}^{n}\frac{1}{\sqrt{k}}=2
=\lim_{n\to\infty}\frac{1}{\sqrt{n}}\sum_{k=2}^{n}\frac{1}{\sqrt{k}-1}, \lim_{n\to\infty}2\sqrt nx_{n+1}=1, x_n\sim\frac1{2\sqrt n},\quad n\to\infty. \sum_{n=1}^{\infty}x_n^{\alpha} \alpha>2 x_n\sim\frac1{2\sqrt n},\quad n\to\infty, \lim_{n\to\infty}\sqrt nx_n >0 \begin{align*}
\lim_{n\to\infty}\sqrt nx_n
&=\lim_{n\to\infty}\frac{x_n}{\frac1{\sqrt{n}}}\\
&=\lim_{n\to\infty}\frac{x_n-x_{n+1}}{\frac1{\sqrt{n}}-\frac1{\sqrt{n+1}}}\\
&=\lim_{n\to\infty}\frac{\frac{x_n^2}{\sqrt{n}}}{\frac{\sqrt{n+1}-\sqrt{n}}{\sqrt{n(n+1)}}}\\
&=\lim_{n\to\infty}\sqrt{n+1}(\sqrt{n+1}+\sqrt{n})x_n^2\\
&=\lim_{n\to\infty}\frac{\sqrt{n+1}(\sqrt{n+1}+\sqrt{n})}{n}(\sqrt nx_n)^2\\
&=2\lim_{n\to\infty}(\sqrt nx_n)^2,
\end{align*} \lim_{n\to\infty}\sqrt nx_n=\frac{1}{2}.","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
22,Prove that a function doesn't have a horizontal asymptote,Prove that a function doesn't have a horizontal asymptote,,"Suppose that $$ f'(x) = \frac{x^2+8x}{x^2+8} $$ with a horizontal asymptote $y=1$ . Prove that $f(x)$ doesn't have a horizontal asymptote. One of my classmates suggested that if the function $f(x)$ has a horizontal asymptote, then its slope must approach $0$ . But $f'(x) \to 1$ ; thus $f(x)$ doesn't have a horizontal asymptote. But the teacher said that this explanation wouldn't get full points if it were on the test. Any better explanations? (We haven't learned integrals and limits yet. Only the fact that limits are used with the horizontal asymptote).","Suppose that with a horizontal asymptote . Prove that doesn't have a horizontal asymptote. One of my classmates suggested that if the function has a horizontal asymptote, then its slope must approach . But ; thus doesn't have a horizontal asymptote. But the teacher said that this explanation wouldn't get full points if it were on the test. Any better explanations? (We haven't learned integrals and limits yet. Only the fact that limits are used with the horizontal asymptote).","
f'(x) = \frac{x^2+8x}{x^2+8}
 y=1 f(x) f(x) 0 f'(x) \to 1 f(x)","['calculus', 'limits', 'derivatives', 'rational-functions']"
23,Proving that the function $f(x) := |x|^{\frac{\lambda - n}{p}} (1- \psi(x))$ satisfies two specific properties related with limits and supremums.,Proving that the function  satisfies two specific properties related with limits and supremums.,f(x) := |x|^{\frac{\lambda - n}{p}} (1- \psi(x)),"Let $1 \leqslant p < \infty$ and $0 < \lambda < n$ , where $n \in \mathbb N$ is an arbitrary fixed integer that stands for the dimension of the euclidian space $\mathbb R^n$ . In everything that follows, I am dealing with the usual Lebesgue integral on $\mathbb R^n$ . Question. Let $\psi \in C_c^\infty(\mathbb R^n)$ be a function from the class $C^\infty(\mathbb R^n)$ with compact support such that $\chi_{B(0,1)} < \psi < \chi_{B(0,2)}$ . Moreover, in $\mathbb R^n$ define the function $$ f(x) := |x|^{\frac{\lambda - n}{p}}(1-\psi(x)), $$ for every $x \in \mathbb R^n \setminus \{0\}$ . Prove that $f$ satisfies the following properties: $$ \lim_{r \to \infty} \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy \neq 0 $$ and $$ \lim_{\xi \to 0} \, \sup_{x \in \mathbb R^n, \, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy = 0. $$ My attempt. I believe I was able to deal with $1.$ but I don't know how to proceed to prove $2.$ . For $1.$ , I've done the following: Take an arbitrary value $r > 2$ . We have that \begin{align*} \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy &\geqslant r^{-\lambda} \int_{B(0,r)} |y|^{\lambda - n} (1-\psi(y))^p \, dy  \\[.2cm]                                    & > r^{-\lambda}\int_{B(0,r)}|y|^{\lambda - n} (1-\chi_{B(0,2)}(y))^p \, dy \\[.2cm]  &= r^{-\lambda} \int_{B(0,r)} |y|^{\lambda -n} \, dy - r^{-\lambda} \int_{B(0,r)} |y|^{\lambda - n} \chi_{B(0,2)}(y) \, dy. \end{align*} Now let us analyse each of the two last integrals separately. For the first one, we have that $$ \int_{B(0,r)} |y|^{\lambda - n} \, dy = c(n) \int_0^r t^{\lambda - 1} \, dt = c(n) \frac{r^\lambda}{\lambda}. $$ On the other hand, for the second one, noting that $r > 2$ we have that $B(0,r) \cap B(0,2) = B(0,2)$ , from which we obtain $$ \int_{B(0,r)} |y|^{\lambda - n} \chi_{B(0,2)}(y) \, dy = \int_{B(0,2)} |y|^{\lambda - n} \, dy = c(n)\int_0^2 t^{\lambda - 1} = c(n)\frac{2^\lambda}{\lambda}. $$ Consequently, going back to the original calculations, it follows that $$ \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)}|f(y)|^p \, dy \geqslant \frac{c(n)}{\lambda} - c(n)r^{-\lambda} \frac{2^\lambda}{\lambda}.  $$ By taking the limit on both sides as $r \to \infty$ , we conclude that $$ \lim_{r \to \infty} \, \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy \geqslant \frac{c(n)}{\lambda} > 0,$$ which is enough to prove $1.$ Does anyone have any idea on how I can proceed to deal with property $2.$ ? Thanks for any help in advance.","Let and , where is an arbitrary fixed integer that stands for the dimension of the euclidian space . In everything that follows, I am dealing with the usual Lebesgue integral on . Question. Let be a function from the class with compact support such that . Moreover, in define the function for every . Prove that satisfies the following properties: and My attempt. I believe I was able to deal with but I don't know how to proceed to prove . For , I've done the following: Take an arbitrary value . We have that Now let us analyse each of the two last integrals separately. For the first one, we have that On the other hand, for the second one, noting that we have that , from which we obtain Consequently, going back to the original calculations, it follows that By taking the limit on both sides as , we conclude that which is enough to prove Does anyone have any idea on how I can proceed to deal with property ? Thanks for any help in advance.","1 \leqslant p < \infty 0 < \lambda < n n \in \mathbb N \mathbb R^n \mathbb R^n \psi \in C_c^\infty(\mathbb R^n) C^\infty(\mathbb R^n) \chi_{B(0,1)} < \psi < \chi_{B(0,2)} \mathbb R^n  f(x) := |x|^{\frac{\lambda - n}{p}}(1-\psi(x)),  x \in \mathbb R^n \setminus \{0\} f  \lim_{r \to \infty} \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy \neq 0   \lim_{\xi \to 0} \, \sup_{x \in \mathbb R^n, \, r > 0} r^{-\lambda} \int_{B(x,r)} |f(y-\xi) - f(y)|^p \, dy = 0.  1. 2. 1. r > 2 \begin{align*} \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy &\geqslant r^{-\lambda} \int_{B(0,r)} |y|^{\lambda - n} (1-\psi(y))^p \, dy  \\[.2cm]
                                   & > r^{-\lambda}\int_{B(0,r)}|y|^{\lambda - n} (1-\chi_{B(0,2)}(y))^p \, dy \\[.2cm] 
&= r^{-\lambda} \int_{B(0,r)} |y|^{\lambda -n} \, dy - r^{-\lambda} \int_{B(0,r)} |y|^{\lambda - n} \chi_{B(0,2)}(y) \, dy.
\end{align*}  \int_{B(0,r)} |y|^{\lambda - n} \, dy = c(n) \int_0^r t^{\lambda - 1} \, dt = c(n) \frac{r^\lambda}{\lambda}.  r > 2 B(0,r) \cap B(0,2) = B(0,2)  \int_{B(0,r)} |y|^{\lambda - n} \chi_{B(0,2)}(y) \, dy = \int_{B(0,2)} |y|^{\lambda - n} \, dy = c(n)\int_0^2 t^{\lambda - 1} = c(n)\frac{2^\lambda}{\lambda}.   \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)}|f(y)|^p \, dy \geqslant \frac{c(n)}{\lambda} - c(n)r^{-\lambda} \frac{2^\lambda}{\lambda}.   r \to \infty  \lim_{r \to \infty} \, \sup_{x \in \mathbb R^n} r^{-\lambda} \int_{B(x,r)} |f(y)|^p \, dy \geqslant \frac{c(n)}{\lambda} > 0, 1. 2.","['functional-analysis', 'limits', 'solution-verification', 'lebesgue-integral', 'supremum-and-infimum']"
24,Correct method to evaluate the limit $\lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x\sin^2x}}=2/3$?,Correct method to evaluate the limit ?,\lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x\sin^2x}}=2/3,I couldn't solve this question so I looked for hints. One method of solving was to use the Taylor Series expansion of each of the functions. It was a bit long. So another solution used the L'Hospitals Rule instead. $$\lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x\sin^2x}}$$ $$= \lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x^3}}$$ For limit to be finite limit of numerator should be zero. Therefore $$\alpha +\beta =0.$$ But limit of $\sin x$ as $x$ tends to $0$ is taken as $x$ in most cases so the correct equation should be $$\alpha +\beta +\gamma x=0$$ and further equations are obtained similarly. Can the limit of $\sin x$ as $x$ tends to $0$ be taken as either $x$ or $0$ as per convenience? Is using L'Hopitals Rule a correct method of solving this question? OR Is Taylor series the more appropriate method?,I couldn't solve this question so I looked for hints. One method of solving was to use the Taylor Series expansion of each of the functions. It was a bit long. So another solution used the L'Hospitals Rule instead. For limit to be finite limit of numerator should be zero. Therefore But limit of as tends to is taken as in most cases so the correct equation should be and further equations are obtained similarly. Can the limit of as tends to be taken as either or as per convenience? Is using L'Hopitals Rule a correct method of solving this question? OR Is Taylor series the more appropriate method?,\lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x\sin^2x}} = \lim_{x\to 0}{\cfrac{\alpha e^x+\beta e^{-x}+\gamma\sin(x)}{x^3}} \alpha +\beta =0. \sin x x 0 x \alpha +\beta +\gamma x=0 \sin x x 0 x 0,"['calculus', 'limits', 'taylor-expansion']"
25,Evaluate limit of this trig function,Evaluate limit of this trig function,,"I'm going through the first openstax calculus book and i'm struggling to understand something. Let me process this limit as I think it goes and then ask the question about it. Evalulate $$\require{cancel}\lim_{\Theta \to 0}\frac{1-\cos \Theta}{\sin \Theta}$$ \begin{align} \lim_{\Theta \to 0}\frac{1-\cos \Theta}{\sin \Theta} & = \lim_{\Theta \to 0}\frac{1 -\cos \Theta}{\sin \Theta}\cdot\frac{1 + \cos \Theta}{1 + \cos \Theta} \\ & = \lim_{\Theta \to 0} \frac{1 - \cos^2 \Theta}{\sin \Theta(1+ \cos \Theta)} \\ & = \lim_{\Theta \to 0} \frac{\sin^2 \Theta}{\sin \Theta(1 + \cos \Theta)} \\ & = \lim_{\Theta \to 0}\frac{\sin \Theta}{\sin \Theta}\cdot\frac{\sin \Theta}{1 + \cos \Theta} \\ & = \frac{0}{0}\cdot \frac{0}{2} \\ & = undef \end{align} I thought this would have been right, however the book says the answer should be $0$ . All I can think of is if it goes like this, picking up on third-last line. \begin{align} \lim_{\Theta \to 0} \frac{\sin^2 \Theta}{\sin \Theta(1 + \cos \Theta)}  & = \lim_{\Theta \to 0}\frac{\cancel{\sin \Theta}}{\cancel{\sin \Theta}}\cdot\frac{\sin \Theta}{1 + \cos \Theta} \\ & = \frac{0}{2} \\ & = 0 \end{align} But I thought that's not right because for example, $$f(x) = \frac{\cancel{(x+3)}}{\cancel{(x+3)}(x-1)}$$ Still has a discontinuity at $x = -3$ So the cancelling out doesn't remove the discontinuity and I thougt this would be so with the limit above also. Or is there some other way they came to $0$ that I'm not seeing? Thanks.","I'm going through the first openstax calculus book and i'm struggling to understand something. Let me process this limit as I think it goes and then ask the question about it. Evalulate I thought this would have been right, however the book says the answer should be . All I can think of is if it goes like this, picking up on third-last line. But I thought that's not right because for example, Still has a discontinuity at So the cancelling out doesn't remove the discontinuity and I thougt this would be so with the limit above also. Or is there some other way they came to that I'm not seeing? Thanks.","\require{cancel}\lim_{\Theta \to 0}\frac{1-\cos \Theta}{\sin \Theta} \begin{align}
\lim_{\Theta \to 0}\frac{1-\cos \Theta}{\sin \Theta} & = \lim_{\Theta \to 0}\frac{1 -\cos \Theta}{\sin \Theta}\cdot\frac{1 + \cos \Theta}{1 + \cos \Theta} \\
& = \lim_{\Theta \to 0} \frac{1 - \cos^2 \Theta}{\sin \Theta(1+ \cos \Theta)} \\
& = \lim_{\Theta \to 0} \frac{\sin^2 \Theta}{\sin \Theta(1 + \cos \Theta)} \\
& = \lim_{\Theta \to 0}\frac{\sin \Theta}{\sin \Theta}\cdot\frac{\sin \Theta}{1 + \cos \Theta} \\
& = \frac{0}{0}\cdot \frac{0}{2} \\
& = undef
\end{align} 0 \begin{align}
\lim_{\Theta \to 0} \frac{\sin^2 \Theta}{\sin \Theta(1 + \cos \Theta)} 
& = \lim_{\Theta \to 0}\frac{\cancel{\sin \Theta}}{\cancel{\sin \Theta}}\cdot\frac{\sin \Theta}{1 + \cos \Theta} \\
& = \frac{0}{2} \\
& = 0
\end{align} f(x) = \frac{\cancel{(x+3)}}{\cancel{(x+3)}(x-1)} x = -3 0",['limits']
26,L'Hopital's rule with dual numbers,L'Hopital's rule with dual numbers,,"Background: For the dual numbers , we extend the reals with an additional unit vector $\epsilon$ subject to the constraint that $\epsilon^2 = 0$ . We can write dual numbers as $x_0 + x_1 \epsilon$ for $x_0,x_1 \in \mathbb{R}$ . We have a rule for multiplication, $$ (x_0 + x_1 \epsilon)(y_0 + y_1 \epsilon) = x_0 y_0 + x_1 y_0 \epsilon + x_0 y_1 \epsilon + x_1 y_1 \epsilon^2 = x_0 y_0 + (x_1 y_0 + x_0 y_1)\epsilon, $$ as well as division, $$ \frac{x_0 + x_1 \epsilon}{y_0 + y_1 \epsilon} = \frac{x_0 + x_1 \epsilon}{y_0 + y_1 \epsilon} \left(\frac{y_0 - y_1 \epsilon}{y_0 - y_1 \epsilon}\right) = \frac{x_0 y_0 + (x_1 y_0 - x_0 y_1)\epsilon}{y_0^2}. $$ It's clear that if $y_0=0$ , division is undefined for all values of $y_1$ . That is, numbers of the form $x_1 \epsilon$ are zero divisors. Furthermore, we have the exact Taylor series $$ f(x_0 + x_1\epsilon) = f(x_0) + x_1 f'(x_0) \epsilon. $$ My question is: Is L'Hopital's rule $$\lim_{x\rightarrow x_0} \frac{f(x_0)}{g(x_0)} = \lim_{x\rightarrow x_0} \frac{f'(x_0)}{g'(x_0)}$$ when $f(x_0)=g(x_0)=0$ just the addition of the rule $$ \frac{x_1 \epsilon}{y_1 \epsilon} = \frac{x_1}{y_1}? $$ That is, we can write that, if $f(x_0)=g(x_0)=0$ , then L'Hopital's rule can be implemented by making the simplification $$ \frac{f(x_0 + x_1 \epsilon)}{g(x_0 + x_1 \epsilon)} = \frac{x_1 f'(x_0) \epsilon}{x_1 g'(x_0) \epsilon} = \frac{f'(x_0)}{g'(x_0)}? $$ Basically, is L'Hopital's rule equivalent to saying we can ""cancel"" the unit $\epsilon$ when the number is pure imaginary? What are the conditions necessary for making this ""cancellation""? The case where the numerator and denominator are different variables seem to make this less than trivial: if $f(x_0) = g(y_0) = 0$ , then the simplistic version of the rule would say that you should assign the value $$ \frac{f(x_0 + x_1 \epsilon)}{g(y_0 + y_1 \epsilon)} = \frac{x_1 f'(x_0) \epsilon}{y_1 g'(y_0) \epsilon} = \frac{x_1 f'(x_0)}{y_1 g'(y_0)} $$ which does give a well-defined procedure for defining the value of the ratio, but it doesn't necessarily correspond to a limit in $x$ and $y$ (since it's not obvious what order the limits on $x$ and $y$ should be taken). It does, however, correspond to the limit along the path $\big(x(t),y(t)\big) = \big(x_0 + x_1 t, y_0 + y_1 t\big)$ such that $$ \lim_{t\rightarrow 0} \frac{f\big(x(t)\big)}{g\big(y(t)\big)} = \lim_{t\rightarrow 0}\frac{f'\big(x(t)\big)x'(t)}{g'\big(y(t)\big)y'(t)} = \frac{x_1 f'(x_0)}{y_1 g'(y_0)}. $$ In the context of the dual numbers, is there a ""good"" reason for choosing this path? It does seem to have the advantage that it reduces to L'Hopital's rule for $y=x$ .","Background: For the dual numbers , we extend the reals with an additional unit vector subject to the constraint that . We can write dual numbers as for . We have a rule for multiplication, as well as division, It's clear that if , division is undefined for all values of . That is, numbers of the form are zero divisors. Furthermore, we have the exact Taylor series My question is: Is L'Hopital's rule when just the addition of the rule That is, we can write that, if , then L'Hopital's rule can be implemented by making the simplification Basically, is L'Hopital's rule equivalent to saying we can ""cancel"" the unit when the number is pure imaginary? What are the conditions necessary for making this ""cancellation""? The case where the numerator and denominator are different variables seem to make this less than trivial: if , then the simplistic version of the rule would say that you should assign the value which does give a well-defined procedure for defining the value of the ratio, but it doesn't necessarily correspond to a limit in and (since it's not obvious what order the limits on and should be taken). It does, however, correspond to the limit along the path such that In the context of the dual numbers, is there a ""good"" reason for choosing this path? It does seem to have the advantage that it reduces to L'Hopital's rule for .","\epsilon \epsilon^2 = 0 x_0 + x_1 \epsilon x_0,x_1 \in \mathbb{R} 
(x_0 + x_1 \epsilon)(y_0 + y_1 \epsilon) = x_0 y_0 + x_1 y_0 \epsilon + x_0 y_1 \epsilon + x_1 y_1 \epsilon^2 = x_0 y_0 + (x_1 y_0 + x_0 y_1)\epsilon,
 
\frac{x_0 + x_1 \epsilon}{y_0 + y_1 \epsilon} = \frac{x_0 + x_1 \epsilon}{y_0 + y_1 \epsilon} \left(\frac{y_0 - y_1 \epsilon}{y_0 - y_1 \epsilon}\right) = \frac{x_0 y_0 + (x_1 y_0 - x_0 y_1)\epsilon}{y_0^2}.
 y_0=0 y_1 x_1 \epsilon 
f(x_0 + x_1\epsilon) = f(x_0) + x_1 f'(x_0) \epsilon.
 \lim_{x\rightarrow x_0} \frac{f(x_0)}{g(x_0)} = \lim_{x\rightarrow x_0} \frac{f'(x_0)}{g'(x_0)} f(x_0)=g(x_0)=0 
\frac{x_1 \epsilon}{y_1 \epsilon} = \frac{x_1}{y_1}?
 f(x_0)=g(x_0)=0 
\frac{f(x_0 + x_1 \epsilon)}{g(x_0 + x_1 \epsilon)} = \frac{x_1 f'(x_0) \epsilon}{x_1 g'(x_0) \epsilon} = \frac{f'(x_0)}{g'(x_0)}?
 \epsilon f(x_0) = g(y_0) = 0 
\frac{f(x_0 + x_1 \epsilon)}{g(y_0 + y_1 \epsilon)} = \frac{x_1 f'(x_0) \epsilon}{y_1 g'(y_0) \epsilon} = \frac{x_1 f'(x_0)}{y_1 g'(y_0)}
 x y x y \big(x(t),y(t)\big) = \big(x_0 + x_1 t, y_0 + y_1 t\big) 
\lim_{t\rightarrow 0} \frac{f\big(x(t)\big)}{g\big(y(t)\big)} = \lim_{t\rightarrow 0}\frac{f'\big(x(t)\big)x'(t)}{g'\big(y(t)\big)y'(t)} = \frac{x_1 f'(x_0)}{y_1 g'(y_0)}.
 y=x","['abstract-algebra', 'limits', 'analysis', 'exterior-algebra', 'dual-numbers']"
27,"Suppose $|g(x)| \le x^4$, then find $\lim\limits_{x \rightarrow 0}\frac{g(x)}{x}$","Suppose , then find",|g(x)| \le x^4 \lim\limits_{x \rightarrow 0}\frac{g(x)}{x},"Suppose $|g(x)| \le x^4$ ,for all $x$ , then find $\lim\limits_{x \rightarrow 0}\frac{g(x)}{x}$ . I did it as follows: $0 \le |g(x)| \le x^4$ , for all $x$ $0 \le \left|\frac{g(x)}{x}\right| \le |x^3|$ for all $x\neq 0$ Also, $\lim\limits_{x \rightarrow 0}0=0$ and $\lim\limits_{x \rightarrow 0}|x^3|=\lim\limits_{x \rightarrow 0}x^3=0$ . Then by the Squeeze Theorem, we have $\lim\limits_{x \rightarrow 0}\frac{g(x)}{x}=\lim\limits_{x \rightarrow 0}\left|\frac{g(x)}{x}\right|=0$ . I think it's right. I'm just not sure if I can consider $x \neq 0$ in the Squeeze Theorem.","Suppose ,for all , then find . I did it as follows: , for all for all Also, and . Then by the Squeeze Theorem, we have . I think it's right. I'm just not sure if I can consider in the Squeeze Theorem.",|g(x)| \le x^4 x \lim\limits_{x \rightarrow 0}\frac{g(x)}{x} 0 \le |g(x)| \le x^4 x 0 \le \left|\frac{g(x)}{x}\right| \le |x^3| x\neq 0 \lim\limits_{x \rightarrow 0}0=0 \lim\limits_{x \rightarrow 0}|x^3|=\lim\limits_{x \rightarrow 0}x^3=0 \lim\limits_{x \rightarrow 0}\frac{g(x)}{x}=\lim\limits_{x \rightarrow 0}\left|\frac{g(x)}{x}\right|=0 x \neq 0,"['calculus', 'limits']"
28,Limit of a sequence does not converge to zero,Limit of a sequence does not converge to zero,,I want to prove that $$\lim_{n\to \infty}\frac{n}{n+2}\ne 0.$$ I proved it as followed $$|L-a_n|\ge\epsilon$$ $$|0-\frac{n}{n+2}|\ge\epsilon$$ $$\frac{n}{n+2}\ge\epsilon$$ $$n\ge\epsilon n+2\epsilon$$ $$n(1-\epsilon)\ge2\epsilon$$ $$n\ge\frac{2\epsilon}{1-\epsilon}.$$ Is this proof correct? Because I can just switch the sign $\geq$ to $<$ it looks like the series converges to $0$ since $$|L-a_n|\lt\epsilon$$ $$|0-\frac{n}{n+2}|\lt\epsilon$$ $$\frac{n}{n+2}\lt\epsilon$$ $$n\lt\epsilon n+2\epsilon$$ $$n(1-\epsilon)\lt2\epsilon$$ $$n\lt\frac{2\epsilon}{1-\epsilon}.$$,I want to prove that I proved it as followed Is this proof correct? Because I can just switch the sign to it looks like the series converges to since,\lim_{n\to \infty}\frac{n}{n+2}\ne 0. |L-a_n|\ge\epsilon |0-\frac{n}{n+2}|\ge\epsilon \frac{n}{n+2}\ge\epsilon n\ge\epsilon n+2\epsilon n(1-\epsilon)\ge2\epsilon n\ge\frac{2\epsilon}{1-\epsilon}. \geq < 0 |L-a_n|\lt\epsilon |0-\frac{n}{n+2}|\lt\epsilon \frac{n}{n+2}\lt\epsilon n\lt\epsilon n+2\epsilon n(1-\epsilon)\lt2\epsilon n\lt\frac{2\epsilon}{1-\epsilon}.,"['calculus', 'sequences-and-series', 'limits']"
29,Find the derivative of $𝑥𝑒^{x\sin(𝜋/𝑥)}$ at $x = 0$,Find the derivative of  at,𝑥𝑒^{x\sin(𝜋/𝑥)} x = 0,"I am not sure what the correct answer is and I am stuck on a problem. I will list the question below and provide my reasoning. Please let me know where or if there is something incorrect in my process. Here is the question: Consider the function $$     f(x)=  \begin{cases}     xe^{x\sin \big( \frac{\pi}{x} \big)} & \text{if } x\neq 0\\     0              & \text{if } x = 0  \end{cases} $$ Determine $f'(0)$ . Write DNE if the derivative is not defined. (Note: The expression in the exponent is $x\sin(\frac{\pi}{x})$ .) My reasoning To show that $f$ is differentiable at all $x \in \mathbb{R}$ , we must show that $f'(x)$ exists at all $x \in \mathbb{R}$ . Recall that $f$ is differentiable at $x$ if $\lim_{x\to h} \frac{f(x+h) - f(x)}{h}$ exists. The derivative at $x=0$ is given by the limit: $$f'(0) = \lim_{h\to 0} \frac{f(h) - f(0)}{h} = \lim_{h\to 0} \frac{he^{h\sin \big( \frac{\pi}{h} \big)} - 0}{h} = \lim_{h\to 0} e^{h\sin \big( \frac{\pi}{h} \big)}$$ Now the range of sine is also [−1, 1], so $$-1 \leq \sin \Big( \frac{\pi}{h} \Big) \leq 1 $$ Taking $e^{h}$ raised to both sides of an inequality does not change the inequality, so $$(e^{h})^{-1} \leq (e^{h})^{\sin \Big( \frac{\pi}{h} \Big)} \leq (e^{h})^{1} $$ $$e^{-h} \leq e^{h\sin \Big( \frac{\pi}{h} \Big)} \leq e^{h}$$ $$\lim_{h\to 0} e^{-h} \leq \lim_{h\to 0} e^{h\sin \Big( \frac{\pi}{h} \Big)} \leq \lim_{h\to 0} e^{h}$$ So, our original function is bounded by $e^{-h}$ and $e^{h}$ , and since $$ \lim_{h\to 0} e^{-h} = \lim_{h\to 0} e^{h} = 1$$ Then, by squeeze theorem, $$\lim_{h\to 0} e^{h\sin \Big( \frac{\pi}{h} \Big)} = 1. $$ So, $f'(0) = 1$ . When look up the limit on a calculator, I find $\lim_{h\to 0} e^{h\sin \big( \frac{\pi}{h} \big)} = $ DNE. The answer key said $f'(0) = 0$ . Where did I go wrong?","I am not sure what the correct answer is and I am stuck on a problem. I will list the question below and provide my reasoning. Please let me know where or if there is something incorrect in my process. Here is the question: Consider the function Determine . Write DNE if the derivative is not defined. (Note: The expression in the exponent is .) My reasoning To show that is differentiable at all , we must show that exists at all . Recall that is differentiable at if exists. The derivative at is given by the limit: Now the range of sine is also [−1, 1], so Taking raised to both sides of an inequality does not change the inequality, so So, our original function is bounded by and , and since Then, by squeeze theorem, So, . When look up the limit on a calculator, I find DNE. The answer key said . Where did I go wrong?","
    f(x)= 
\begin{cases}
    xe^{x\sin \big( \frac{\pi}{x} \big)} & \text{if } x\neq 0\\
    0              & \text{if } x = 0 
\end{cases}
 f'(0) x\sin(\frac{\pi}{x}) f x \in \mathbb{R} f'(x) x \in \mathbb{R} f x \lim_{x\to h} \frac{f(x+h) - f(x)}{h} x=0 f'(0) = \lim_{h\to 0} \frac{f(h) - f(0)}{h} = \lim_{h\to 0} \frac{he^{h\sin \big( \frac{\pi}{h} \big)} - 0}{h} = \lim_{h\to 0} e^{h\sin \big( \frac{\pi}{h} \big)} -1 \leq \sin \Big( \frac{\pi}{h} \Big) \leq 1  e^{h} (e^{h})^{-1} \leq (e^{h})^{\sin \Big( \frac{\pi}{h} \Big)} \leq (e^{h})^{1}  e^{-h} \leq e^{h\sin \Big( \frac{\pi}{h} \Big)} \leq e^{h} \lim_{h\to 0} e^{-h} \leq \lim_{h\to 0} e^{h\sin \Big( \frac{\pi}{h} \Big)} \leq \lim_{h\to 0} e^{h} e^{-h} e^{h}  \lim_{h\to 0} e^{-h} = \lim_{h\to 0} e^{h} = 1 \lim_{h\to 0} e^{h\sin \Big( \frac{\pi}{h} \Big)} = 1.  f'(0) = 1 \lim_{h\to 0} e^{h\sin \big( \frac{\pi}{h} \big)} =  f'(0) = 0","['calculus', 'limits', 'derivatives']"
30,$\lim\limits_{x\to+\infty}\sum_{k=0}^\infty \left[\lambda f\left(\frac{x}{2^k}\right)-f\left(\frac{x}{3\cdot 2^k}\right)\right]$?,?,\lim\limits_{x\to+\infty}\sum_{k=0}^\infty \left[\lambda f\left(\frac{x}{2^k}\right)-f\left(\frac{x}{3\cdot 2^k}\right)\right],"Some time ago I saw a problem somewhere (I cannot remember where) that goes something like this. Let $f:[0,+\infty)\to\mathbb{R}$ be a real function with $f(x)=f(\lfloor x\rfloor)$ for all $x\geq0$ , $f(0)=0$ and such that $\exists\lim\limits_{x\to+\infty}f(x)\neq0$ . Can the following limit converge (in $\mathbb{R}$ ) for some constant $1\neq\lambda\in\mathbb{R}$ ? $$\boxed{\lim_{x\to+\infty}\sum_{k=0}^\infty \left[\lambda f\left(\frac{x}{2^k}\right)-f\left(\frac{x}{3\cdot 2^k}\right)\right]}$$ Note that the sum on the limit is actually finite as, given $x\geq0$ , there exists $k_0\in\mathbb{N}$ such that $\forall k\geq k_0:f(x/2^k)=f(x/(3\cdot2^k))=f(0)=0$ . I have noticed that, if $f:[0,+\infty)\to\mathbb{R}$ satisfies the above-mentioned hypotheses, then the limit $L = \lim\limits_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x}{2^k}\right)$ cannot exist since $$\Rightarrow L = \lim_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x/2}{2^k}\right) =\lim_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x}{2^{k+1}}\right) =L-\lim_{x\to+\infty}f(x)\neq L$$ But it could so happen that the former limit exists (?). In fact, if we denote $F(x)=\sum_{k=0}^\infty f\left(\frac{x}{2^{k+1}}\right)$ , we can restate the troublesome condition as $\exists\lambda\neq1:\exists\lim\limits_{x\to+\infty}\lambda F(x)-F(x/3)$ . Note that this condition of $F(x)$ alone can't imply the existence of $\lim\limits_{x\to+\infty}F(x)$ (and, thus, it can't lead us to a contradiction by itself) since for example, considering the 3-adic valuation , we have for $\lambda\neq0$ that $F(x)=\lambda^{-\nu_3(\lfloor x\rfloor)}$ (with $F(x):=0$ for $\lfloor x\rfloor = 0$ ) is divergent while $\lambda F(3n)-F(n)=F(n)-F(n)=0$ which I think implies $\lambda F(x)-F(x/3)\to0$ (?). So it could still happen that $\lim\limits_{x\to+\infty}\lambda F(x)-F(x/3)$ exists.","Some time ago I saw a problem somewhere (I cannot remember where) that goes something like this. Let be a real function with for all , and such that . Can the following limit converge (in ) for some constant ? Note that the sum on the limit is actually finite as, given , there exists such that . I have noticed that, if satisfies the above-mentioned hypotheses, then the limit cannot exist since But it could so happen that the former limit exists (?). In fact, if we denote , we can restate the troublesome condition as . Note that this condition of alone can't imply the existence of (and, thus, it can't lead us to a contradiction by itself) since for example, considering the 3-adic valuation , we have for that (with for ) is divergent while which I think implies (?). So it could still happen that exists.","f:[0,+\infty)\to\mathbb{R} f(x)=f(\lfloor x\rfloor) x\geq0 f(0)=0 \exists\lim\limits_{x\to+\infty}f(x)\neq0 \mathbb{R} 1\neq\lambda\in\mathbb{R} \boxed{\lim_{x\to+\infty}\sum_{k=0}^\infty \left[\lambda f\left(\frac{x}{2^k}\right)-f\left(\frac{x}{3\cdot 2^k}\right)\right]} x\geq0 k_0\in\mathbb{N} \forall k\geq k_0:f(x/2^k)=f(x/(3\cdot2^k))=f(0)=0 f:[0,+\infty)\to\mathbb{R} L = \lim\limits_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x}{2^k}\right) \Rightarrow L = \lim_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x/2}{2^k}\right) =\lim_{x\to+\infty}\sum_{k=0}^\infty f\left(\frac{x}{2^{k+1}}\right) =L-\lim_{x\to+\infty}f(x)\neq L F(x)=\sum_{k=0}^\infty f\left(\frac{x}{2^{k+1}}\right) \exists\lambda\neq1:\exists\lim\limits_{x\to+\infty}\lambda F(x)-F(x/3) F(x) \lim\limits_{x\to+\infty}F(x) \lambda\neq0 F(x)=\lambda^{-\nu_3(\lfloor x\rfloor)} F(x):=0 \lfloor x\rfloor = 0 \lambda F(3n)-F(n)=F(n)-F(n)=0 \lambda F(x)-F(x/3)\to0 \lim\limits_{x\to+\infty}\lambda F(x)-F(x/3)","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'analysis']"
31,Can infinite summation be defined without using limits?,Can infinite summation be defined without using limits?,,"Binary addition on natural numbers is defined using simple statements: $$n + 0 = n$$ $$a + succ(b) = succ(a + b)$$ Binary addition of real numbers may be again defined using simple statements (I won't list them here). Could we come up with a list of similar simple statements that would define summation of infinite sequences of real numbers, in a way that does not, even intutitively, rely on the notion of limit or convergence? These are examples of such ""simple, not-limit-using"" statements (though I'm not claiming they are necessarily good candidates): Some base cases: $sum(0, 0, 0, ...) = 0$ $sum(0.5^1, 0.5^2, 0.5^3, ...) = 1$ Respects binary sum: $sum(n_0, n_1, n_2, ...) = r$ implies $r = n_0 + sum(n_1, n_2, ...)$ $sum(n_1, n_2, ...) = r$ implies $n_0 + r = sum(n_0, n_1, n_2, ...)$ Respects binary multiplication: $sum(n_0, n_1, n_2, ...) = r$ implies $sum(n_0 * c, n_1 * c, n_2 * c, ...) = r * c$ Zigzaging rule: $sum(A) = a$ and $sum(B) = b$ and $zigzag(A,B,C)$ implies $sum(C) = a + b$ Helper definitions: For sequences of reals $A$ , $B$ and $C$ , $zigzag(A,B,C)$ holds iff there exists a sequence of naturals $I$ such that $A = (C_{I_0}, C_{I_1}, ...)$ and $B$ equals $C$ with the elements at indices $I$ removed. I'm afraid that if this question does not have an affirmative answer, it would be very hard to give any, because I have not given precise enough description of what is simple and non-limit-using. So I'll be happy to also accept counter-examples that cannot be proven to equal to their limits using my example statements (or other simple statements of the answerer's choosing, if they wish to add some). I'm also aware none of my examples can be used to prove a particular sum diverges. Perhaps we could define summation as a least fixed point? But feel free to restrict yourself to converging sequences, I guess. This question was motivated by this one , where the asker has trouble accepting the conventional definition of infinite summation, which got me thinking about whether one may avoid using limits for defining it.","Binary addition on natural numbers is defined using simple statements: Binary addition of real numbers may be again defined using simple statements (I won't list them here). Could we come up with a list of similar simple statements that would define summation of infinite sequences of real numbers, in a way that does not, even intutitively, rely on the notion of limit or convergence? These are examples of such ""simple, not-limit-using"" statements (though I'm not claiming they are necessarily good candidates): Some base cases: Respects binary sum: implies implies Respects binary multiplication: implies Zigzaging rule: and and implies Helper definitions: For sequences of reals , and , holds iff there exists a sequence of naturals such that and equals with the elements at indices removed. I'm afraid that if this question does not have an affirmative answer, it would be very hard to give any, because I have not given precise enough description of what is simple and non-limit-using. So I'll be happy to also accept counter-examples that cannot be proven to equal to their limits using my example statements (or other simple statements of the answerer's choosing, if they wish to add some). I'm also aware none of my examples can be used to prove a particular sum diverges. Perhaps we could define summation as a least fixed point? But feel free to restrict yourself to converging sequences, I guess. This question was motivated by this one , where the asker has trouble accepting the conventional definition of infinite summation, which got me thinking about whether one may avoid using limits for defining it.","n + 0 = n a + succ(b) = succ(a + b) sum(0, 0, 0, ...) = 0 sum(0.5^1, 0.5^2, 0.5^3, ...) = 1 sum(n_0, n_1, n_2, ...) = r r = n_0 + sum(n_1, n_2, ...) sum(n_1, n_2, ...) = r n_0 + r = sum(n_0, n_1, n_2, ...) sum(n_0, n_1, n_2, ...) = r sum(n_0 * c, n_1 * c, n_2 * c, ...) = r * c sum(A) = a sum(B) = b zigzag(A,B,C) sum(C) = a + b A B C zigzag(A,B,C) I A = (C_{I_0}, C_{I_1}, ...) B C I","['sequences-and-series', 'limits']"
32,"what positive numbers $a,b,c,d $ makes $\frac{x^a y^b}{x^c + y^d}$ bounded in every neighbourhood of $(0,0)$",what positive numbers  makes  bounded in every neighbourhood of,"a,b,c,d  \frac{x^a y^b}{x^c + y^d} (0,0)","I was playing with $\lim\limits_{(x,y) \to (0,0)} \frac{x^a y^b}{x^c + y^d}$ for $a,b,c,d ,x,y >0 $ and realised that $\frac{x^a y^b}{x^c + y^d}$ is bounded in every neighbourhood of $(0,0)$ if $c=2a , d=2b$ because the $AM-GM$ inequality so is it the only case where $z:=\frac{x^a y^b}{x^c + y^d}$ bounded ? it is easy to see that eventually $x,y <1$ so if $z$ is bounded for some $a,b,c,d$ then any $a_1>a$ and $b_1>b$ and $c_1<c$ and $d_1<d$ would also work if $c=2a$ and $d>2b$ then the denominator will be $\frac{x^{a}}{y^b} +\frac{y^{b + \epsilon}}{x^a}$ choose some $0<k<\frac{a}{b+\epsilon }$ $y^{b } = x ^{a-k\epsilon}$ it follows that as $x \to 0$ $ \ z \to \infty $ which means if I replaced one power in the demonstrator with higher value than the $AM-GM$ form then $z$ is unbounded",I was playing with for and realised that is bounded in every neighbourhood of if because the inequality so is it the only case where bounded ? it is easy to see that eventually so if is bounded for some then any and and and would also work if and then the denominator will be choose some it follows that as which means if I replaced one power in the demonstrator with higher value than the form then is unbounded,"\lim\limits_{(x,y) \to (0,0)} \frac{x^a y^b}{x^c + y^d} a,b,c,d ,x,y >0  \frac{x^a y^b}{x^c + y^d} (0,0) c=2a , d=2b AM-GM z:=\frac{x^a y^b}{x^c + y^d} x,y <1 z a,b,c,d a_1>a b_1>b c_1<c d_1<d c=2a d>2b \frac{x^{a}}{y^b} +\frac{y^{b + \epsilon}}{x^a} 0<k<\frac{a}{b+\epsilon } y^{b } = x ^{a-k\epsilon} x \to 0  \ z \to \infty  AM-GM z","['calculus', 'limits', 'multivariable-calculus', 'inequality', 'upper-lower-bounds']"
33,Limit of a Sequence of Spiraling Points,Limit of a Sequence of Spiraling Points,,"The Problem Starting with the vertices $P_1(0,1), P_2(1,1), P_3(1,0), P_4(0,0)$ of a square, we construct further points as shown in the figure: $P_5$ is the midpoint of $P_1P_2$ , $P_6$ is the midpoint of $P_2P_3$ , $P_7$ is the midpoint of $P_3P_4$ , and so on. The polygonal spiral path $P_1P_2P_3P_4P_5P_6P_7\cdots$ approaches a point $P$ inside the square. Find the coordinates of $P$ . [ Calculus by James Stewart, $7^\text{th}$ edition, chapter $11$ , problem plus, problem $18$ ] What I Have Tried First of all, let $P_n=(x_n,y_n)$ . We will first consider only $x$ -coordinates first because the $y$ -coordinate of $P$ can be found similarly. I have proven using induction that $\tfrac12x_n+x_{n+1}+x_{n+2}+x_{n+3}=2$ for all $n\geq1$ as follows. Base Case ( $n=1$ ) Since $x_1=0, x_2=1, x_3=1, x_4=0$ , we have $\tfrac12x_1+x_2+x_3+x_4=\tfrac12\cdot0+1+1+0=2.$ Induction ( $n>1$ ) Assume that $\tfrac12x_n+x_{n+1}+x_{n+2}+x_{n+3}=2$ is true for $n=k$ , then \begin{equation} x_{k+2}+x_{k+3}=2-\tfrac12x_k-x_{k+1}. \end{equation} Also observe that $x_{n+4}=\tfrac12(x_n+x_{n+1})$ . Therefore \begin{equation} \begin{split} \tfrac12x_{k+1}+x_{k+2}+x_{k+3}+x_{k+4}&=\tfrac12x_{k+1}+(2-\tfrac12x_k-x_{k+1})+x_{k+4}\\\\ &=\tfrac12x_{k+1}+2-\tfrac12x_k-x_{k+1}+\tfrac12(x_k+x_{k+1})\\\\  &=2 \end{split} \end{equation} and so it is true for $n=k+1$ . So if $\lim_{n\to\infty}x_n$ exists, we can apply the limit as $n$ approaches $\infty$ to both sides of the equation to get (letting $\lim_{n\to\infty}x_n=L_x$ ) $$\tfrac12L_x+3L_x=2$$ or $$L_x=\tfrac47.$$ My Question The problem arises: how do we prove that $\lim_{n\to\infty}x_n$ does exist? All I've been able to do is prove that $0\leq x_n\leq1$ for all $n\geq1$ . I couldn't find anything on the web that could help. Thanks in advance.","The Problem Starting with the vertices of a square, we construct further points as shown in the figure: is the midpoint of , is the midpoint of , is the midpoint of , and so on. The polygonal spiral path approaches a point inside the square. Find the coordinates of . [ Calculus by James Stewart, edition, chapter , problem plus, problem ] What I Have Tried First of all, let . We will first consider only -coordinates first because the -coordinate of can be found similarly. I have proven using induction that for all as follows. Base Case ( ) Since , we have Induction ( ) Assume that is true for , then Also observe that . Therefore and so it is true for . So if exists, we can apply the limit as approaches to both sides of the equation to get (letting ) or My Question The problem arises: how do we prove that does exist? All I've been able to do is prove that for all . I couldn't find anything on the web that could help. Thanks in advance.","P_1(0,1), P_2(1,1), P_3(1,0), P_4(0,0) P_5 P_1P_2 P_6 P_2P_3 P_7 P_3P_4 P_1P_2P_3P_4P_5P_6P_7\cdots P P 7^\text{th} 11 18 P_n=(x_n,y_n) x y P \tfrac12x_n+x_{n+1}+x_{n+2}+x_{n+3}=2 n\geq1 n=1 x_1=0, x_2=1, x_3=1, x_4=0 \tfrac12x_1+x_2+x_3+x_4=\tfrac12\cdot0+1+1+0=2. n>1 \tfrac12x_n+x_{n+1}+x_{n+2}+x_{n+3}=2 n=k \begin{equation}
x_{k+2}+x_{k+3}=2-\tfrac12x_k-x_{k+1}.
\end{equation} x_{n+4}=\tfrac12(x_n+x_{n+1}) \begin{equation}
\begin{split}
\tfrac12x_{k+1}+x_{k+2}+x_{k+3}+x_{k+4}&=\tfrac12x_{k+1}+(2-\tfrac12x_k-x_{k+1})+x_{k+4}\\\\
&=\tfrac12x_{k+1}+2-\tfrac12x_k-x_{k+1}+\tfrac12(x_k+x_{k+1})\\\\ 
&=2
\end{split}
\end{equation} n=k+1 \lim_{n\to\infty}x_n n \infty \lim_{n\to\infty}x_n=L_x \tfrac12L_x+3L_x=2 L_x=\tfrac47. \lim_{n\to\infty}x_n 0\leq x_n\leq1 n\geq1","['calculus', 'sequences-and-series', 'limits', 'polygons']"
34,"How can we use undefined term ""infinity"" in limits?","How can we use undefined term ""infinity"" in limits?",,"Consider function $f$ such that $f:R \to R$ and $$\displaystyle \lim_{x \to \infty} f(x) $$ Now in standard textbooks of calculus such a limit can be found out if it exist. Like for example $$\displaystyle \lim_{x \to \infty} 1+\dfrac{1}{x^2}=1.$$ But my question is how can limit of $x$ tending to"" $\infty""$ can be found if infinity itself is not even defined? Like i know that we are finding limit when $x$ is ""very large"" but the fact that we are using infinity is not very appealing to me . Instead can we say that $$\displaystyle \lim_{x \to M} f(x) $$ where $M$ is such that $\forall m \in R$ $$M>m.$$ But again we do not know what $M$ is. So can someone clarify this?","Consider function such that and Now in standard textbooks of calculus such a limit can be found out if it exist. Like for example But my question is how can limit of tending to"" can be found if infinity itself is not even defined? Like i know that we are finding limit when is ""very large"" but the fact that we are using infinity is not very appealing to me . Instead can we say that where is such that But again we do not know what is. So can someone clarify this?","f f:R \to R \displaystyle \lim_{x \to \infty} f(x)  \displaystyle \lim_{x \to \infty} 1+\dfrac{1}{x^2}=1. x \infty"" x \displaystyle \lim_{x \to M} f(x)  M \forall m \in R M>m. M","['calculus', 'limits', 'real-numbers']"
35,Does $|\vec x_{n+m}|\leq |\vec x_n+\vec x_m|$ imply the convergence of $\{\vec x_n/n\}$?,Does  imply the convergence of ?,|\vec x_{n+m}|\leq |\vec x_n+\vec x_m| \{\vec x_n/n\},"Let $d\geq 1$ be a positive integer. If $\{\vec x_n\}_{n=1}^\infty$ is a sequence of $d$ -dimensional vectors satisfying $$|\vec x_{n+m}|\leq |\vec x_n+\vec x_m|\qquad \text{for all }n,m\in\mathbb N_{\geq 1},\tag{$*$}$$ where $|\vec x|=[(x^{(1)})^2+\cdots+(x^{(d)})^2]^{1/2}$ for $\vec x=(x^{(1)}, \cdots, x^{(d)})$ . Then does the limit $\lim\limits_{n\to\infty}\frac{\vec x_n}{n}$ exist? Ideas. It follows from $(*)$ and the induction that $|\vec x_n|\leq n|\vec x_1|$ for all $n\in\mathbb N_{\geq 1}$ , hence $\{\vec x_n/n\}_{n\geq 1}$ is a bounded sequence. On the other hand, $(*)$ also implies that $|\vec x_{n+m}|\le |\vec x_n|+|\vec x_m|$ for all $n,m\in\mathbb N_{\geq 1}$ , hence by Fekete's subadditive lemma we get the convergence of $\{|\vec x_n|/n\}_{n\geq 1}$ with $$\lim_{n\to\infty}\frac{|\vec x_n|}n=\inf_{n\geq 1}\frac{|\vec x_n|}n.$$ If $d=1$ , then the answer is yes , which is proved in this post . For $d\geq2$ , I tried to use the same ideas as in that post. If $\lim\limits_{n\to\infty}\frac{|\vec x_n|}n=L$ , then for all $\epsilon>0$ there exists $N\gg L$ such that $\left|\frac{|\vec x_n|}n-L\right|<\varepsilon$ for all $n>N$ . Take $\varepsilon>0$ small enough. Fix $n_0>N$ , then there exists $\vec z_1$ with $|\vec z_1|=1$ such that $\left|\frac{\vec x_{n_0}}{n_0}-L\vec z_1\right|<\varepsilon$ . Let $n$ be the largest integer such that $\left|\frac{\vec x_{k}}{k}-L\vec z_1\right|<\varepsilon$ holds for all $n_0\leq k\leq n$ . Then $\left|\frac{\vec x_{n}}{n}-L\vec z_1\right|<\varepsilon$ and $\left|\frac{\vec x_{n+1}}{n+1}-L\vec z_1\right|\geq\varepsilon$ , hence $\left|\frac{\vec x_{n+1}}{n+1}-L\vec z_2\right|\geq\varepsilon$ for some $\vec z_2$ with $|\vec z_2|=1$ and $\vec z_2$ is far away from $\vec z_1$ . However, they are not as far away as in that post, where $\vec z_1=1, \vec z_2=-1$ for $d=1$ . I'm not sure how to continue. In fact, I suspect that the answer for $d\geq 2$ is negative. But I cannot find a counterexpamle.","Let be a positive integer. If is a sequence of -dimensional vectors satisfying where for . Then does the limit exist? Ideas. It follows from and the induction that for all , hence is a bounded sequence. On the other hand, also implies that for all , hence by Fekete's subadditive lemma we get the convergence of with If , then the answer is yes , which is proved in this post . For , I tried to use the same ideas as in that post. If , then for all there exists such that for all . Take small enough. Fix , then there exists with such that . Let be the largest integer such that holds for all . Then and , hence for some with and is far away from . However, they are not as far away as in that post, where for . I'm not sure how to continue. In fact, I suspect that the answer for is negative. But I cannot find a counterexpamle.","d\geq 1 \{\vec x_n\}_{n=1}^\infty d |\vec x_{n+m}|\leq |\vec x_n+\vec x_m|\qquad \text{for all }n,m\in\mathbb N_{\geq 1},\tag{*} |\vec x|=[(x^{(1)})^2+\cdots+(x^{(d)})^2]^{1/2} \vec x=(x^{(1)}, \cdots, x^{(d)}) \lim\limits_{n\to\infty}\frac{\vec x_n}{n} (*) |\vec x_n|\leq n|\vec x_1| n\in\mathbb N_{\geq 1} \{\vec x_n/n\}_{n\geq 1} (*) |\vec x_{n+m}|\le |\vec x_n|+|\vec x_m| n,m\in\mathbb N_{\geq 1} \{|\vec x_n|/n\}_{n\geq 1} \lim_{n\to\infty}\frac{|\vec x_n|}n=\inf_{n\geq 1}\frac{|\vec x_n|}n. d=1 d\geq2 \lim\limits_{n\to\infty}\frac{|\vec x_n|}n=L \epsilon>0 N\gg L \left|\frac{|\vec x_n|}n-L\right|<\varepsilon n>N \varepsilon>0 n_0>N \vec z_1 |\vec z_1|=1 \left|\frac{\vec x_{n_0}}{n_0}-L\vec z_1\right|<\varepsilon n \left|\frac{\vec x_{k}}{k}-L\vec z_1\right|<\varepsilon n_0\leq k\leq n \left|\frac{\vec x_{n}}{n}-L\vec z_1\right|<\varepsilon \left|\frac{\vec x_{n+1}}{n+1}-L\vec z_1\right|\geq\varepsilon \left|\frac{\vec x_{n+1}}{n+1}-L\vec z_2\right|\geq\varepsilon \vec z_2 |\vec z_2|=1 \vec z_2 \vec z_1 \vec z_1=1, \vec z_2=-1 d=1 d\geq 2","['real-analysis', 'sequences-and-series', 'limits', 'multivariable-calculus']"
36,Riemann sum $\displaystyle F(x)=\lim_{n\to\infty}\frac{1}{\ln(n)}\sum_{k=1}^{n}\text{sinc}(\pi(x+k)) \ln\left(\sin\left(\frac{k\pi}{2n}\right)\right)$,Riemann sum,\displaystyle F(x)=\lim_{n\to\infty}\frac{1}{\ln(n)}\sum_{k=1}^{n}\text{sinc}(\pi(x+k)) \ln\left(\sin\left(\frac{k\pi}{2n}\right)\right),"I need help calculating this limit as a function of $x$ : $$F(x)=\lim_{n\to\infty}\frac{1}{\ln(n)}\sum_{k=1}^{n}\frac{\sin(\pi(x+k))}{\pi(x+k)}\ln\left(\sin\left(\frac{k\pi}{2n}\right)\right)$$ I wanted to use the Riemann sum method in such a way $F(x)$ as to write as an integral, but the logarithm appears and I can't find a way to write it differently. Thanks in advance to anyone who can give me a suggestion I believe that the solution is: $$F(x)=\frac{1}{2\pi}\int_{0}^{\pi}\sin\left(xt\right)\cot\left(\frac{t}{2}\right)\mathrm{d}t+\frac{\sin\left(\pi x\right)}{2\pi x}-\frac{1}{2}$$ (or something like that) but I don't know how to prove it. There could be a correlation since $\dfrac{\mathrm{d}}{\mathrm{d}x}\ln(\sin(x))=\cot(x)$ I think it might also be useful to consider the Riemann–Stieltjes integral: $${\displaystyle \int _{a}^{b}f(x)\,\mathrm {d} g(x)=f(b)g(b)-f(a)g(a)-\int _{a}^{b}g(x)\,\mathrm {d} f(x)}$$","I need help calculating this limit as a function of : I wanted to use the Riemann sum method in such a way as to write as an integral, but the logarithm appears and I can't find a way to write it differently. Thanks in advance to anyone who can give me a suggestion I believe that the solution is: (or something like that) but I don't know how to prove it. There could be a correlation since I think it might also be useful to consider the Riemann–Stieltjes integral:","x F(x)=\lim_{n\to\infty}\frac{1}{\ln(n)}\sum_{k=1}^{n}\frac{\sin(\pi(x+k))}{\pi(x+k)}\ln\left(\sin\left(\frac{k\pi}{2n}\right)\right) F(x) F(x)=\frac{1}{2\pi}\int_{0}^{\pi}\sin\left(xt\right)\cot\left(\frac{t}{2}\right)\mathrm{d}t+\frac{\sin\left(\pi x\right)}{2\pi x}-\frac{1}{2} \dfrac{\mathrm{d}}{\mathrm{d}x}\ln(\sin(x))=\cot(x) {\displaystyle \int _{a}^{b}f(x)\,\mathrm {d} g(x)=f(b)g(b)-f(a)g(a)-\int _{a}^{b}g(x)\,\mathrm {d} f(x)}","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'limits']"
37,"Evaluate $\,\lim\limits_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}$",Evaluate,"\,\lim\limits_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}","How can I evaluate this limit. $$\lim_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}$$ This is what I did: $x\in \big(\frac{-1}{2},0\big)\;$ ; $\;\;\lfloor x\rfloor =-1 $ $$\lim_{x \to 0^{-}}\sin(\lfloor x\rfloor)=\sin(-1)<0 \;\;\text{ and }\;\lim_{x \to 0^{-}} x =0^{-}$$ then $$\lim_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}=+\infty$$ What you think? Any other method ?",How can I evaluate this limit. This is what I did: ; then What you think? Any other method ?,"\lim_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x} x\in \big(\frac{-1}{2},0\big)\; \;\;\lfloor x\rfloor =-1  \lim_{x \to 0^{-}}\sin(\lfloor x\rfloor)=\sin(-1)<0 \;\;\text{ and }\;\lim_{x \to 0^{-}} x =0^{-} \lim_{x \to 0^{-}} \frac{\sin(\lfloor x\rfloor)}{x}=+\infty","['limits', 'trigonometry', 'alternative-proof']"
38,How is the infinite summation under limit that approaches zero converted to an improper integral?,How is the infinite summation under limit that approaches zero converted to an improper integral?,,"I am trying to understand the conversion of Fourier series to the Fourier integral. The book I am referring to is ""Advanced Engineering Mathematics"" by Erwin Kreyszig. The proof contains the following style of conversion: $ \lim_{\Delta \alpha\rightarrow 0} \sum_{n=1}^{\infty} F(\alpha_n)\Delta \alpha \rightarrow \int_{0}^{\infty}F(\alpha)d\alpha  $ How is that summation under zero-approaching limit converted to an improper integral ? What's the essential piece I am missing here to understand it?","I am trying to understand the conversion of Fourier series to the Fourier integral. The book I am referring to is ""Advanced Engineering Mathematics"" by Erwin Kreyszig. The proof contains the following style of conversion: How is that summation under zero-approaching limit converted to an improper integral ? What's the essential piece I am missing here to understand it?","
\lim_{\Delta \alpha\rightarrow 0} \sum_{n=1}^{\infty} F(\alpha_n)\Delta \alpha \rightarrow \int_{0}^{\infty}F(\alpha)d\alpha 
","['limits', 'fourier-transform']"
39,"Find the Fourier series of a function , determining whether it converges pointwise/uniformly","Find the Fourier series of a function , determining whether it converges pointwise/uniformly",,"I've been trying to solve an exercise from a test. I could use your help :) Question Consider the function: $g(x) =  \frac{\pi}{4} $ for $x\in [0,\pi ]$ and $ -\frac{\pi}{4}$ for $x\in (-\pi,0 )  $ . Let $f(n)$ be $g(x)$ 's  periodic continuation. $S_N f(x) = A_0 +\sum_{n=1}^{N}A_n \cos{nx}+i\sum_{n=1}^{N}B_n \sin{nx}$ where $A_0 = \hat{f}(0), A_n = \hat{f}(n)+\hat{f}(-n), B_n = \hat{f}(n)-\hat{f}(-n)$ . Note: The Fourier series of $f$ is $\lim_{N \to \infty } S_Nf$ Find $S_Nf$ . Determine if $S_Nf(x)$ pointwise converges for $x\in \left[-\pi,\pi \right]$ . Determine if $S_Nf(x)$ uniformly converges in $ \left[-\pi,\pi \right]$ . Prove $\lim_{N \to \infty} S_Nf \left( \frac{1}{\pi N}\right) = \int_{0}^{1} \frac{\sin{\left( \frac{t}{\pi}\right)}}{2t} \ dt $ My attempt Sol for 1: $\hat{f}(n)= \frac{1}{2in}$ for $n\in \mathbb{Z}_{odd} $ , else it's $0$ . Hence, $S_N f (x) = \sum_{k=0}^{K}\frac{1}{(2k+1)} \sin{((2k+1)x)}$ where $K=\left\lfloor \frac{N}{2}\right\rfloor$ Sol for 3: $S_N f (x)$ is a continuous function, therefore if $S_Nf$ uniformly converges we know it has to be to $f$ . But $f$ isn't continuous. Therefore we will conclude $S_Nf$ does not uniformly converges. I would appreciate your help in 2 and 4 (if you do please be formal so I can understand better). Thank you!","I've been trying to solve an exercise from a test. I could use your help :) Question Consider the function: for and for . Let be 's  periodic continuation. where . Note: The Fourier series of is Find . Determine if pointwise converges for . Determine if uniformly converges in . Prove My attempt Sol for 1: for , else it's . Hence, where Sol for 3: is a continuous function, therefore if uniformly converges we know it has to be to . But isn't continuous. Therefore we will conclude does not uniformly converges. I would appreciate your help in 2 and 4 (if you do please be formal so I can understand better). Thank you!","g(x) =  \frac{\pi}{4}  x\in [0,\pi ]  -\frac{\pi}{4} x\in (-\pi,0 )   f(n) g(x) S_N f(x) = A_0 +\sum_{n=1}^{N}A_n \cos{nx}+i\sum_{n=1}^{N}B_n \sin{nx} A_0 = \hat{f}(0), A_n = \hat{f}(n)+\hat{f}(-n), B_n = \hat{f}(n)-\hat{f}(-n) f \lim_{N \to \infty } S_Nf S_Nf S_Nf(x) x\in \left[-\pi,\pi \right] S_Nf(x)  \left[-\pi,\pi \right] \lim_{N \to \infty} S_Nf \left( \frac{1}{\pi N}\right) = \int_{0}^{1} \frac{\sin{\left( \frac{t}{\pi}\right)}}{2t} \ dt  \hat{f}(n)= \frac{1}{2in} n\in \mathbb{Z}_{odd}  0 S_N f (x) = \sum_{k=0}^{K}\frac{1}{(2k+1)} \sin{((2k+1)x)} K=\left\lfloor \frac{N}{2}\right\rfloor S_N f (x) S_Nf f f S_Nf","['real-analysis', 'limits', 'fourier-analysis', 'fourier-series']"
40,Double limit and dominated convergence theorem,Double limit and dominated convergence theorem,,"I am interested in the double limit of a particular function $f_m(x,\varepsilon)$ of the form $$f_m(x,\varepsilon) = \frac{1}{m}\sum_{n=0}^{m-1}g(x,\varepsilon).$$ I want to compute $$\lim_{\varepsilon \to 0}\lim_{m\to \infty}\frac{1}{m}\sum_{n=0}^{m-1}g_n(x,\varepsilon).$$ Without the $1/m$ normalisation the limit reduces to $$\lim_{\varepsilon \to 0}\sum_{n=0}^{\infty}g_n(x,\varepsilon)$$ where $|g_n(x,\varepsilon)|\leq h_n(x)$ where $\sum_{n=0}^\infty h_n(x)$ does not necessarily converge for all choices of $x$ . My question is the following. Knowing that $\lim_{m\to \infty} \frac{1}{m}\sum_{n=0}^{m-1}h_n(x)$ converges for almost every $x$ , are we allowed to say that $$\lim_{\varepsilon \to 0}\lim_{m\to \infty}\frac{1}{m}\sum_{n=0}^{m-1}g_n(x,\varepsilon)=\lim_{\varepsilon \to 0}\lim_{m\to \infty}\sum_{n=0}^{\infty}\frac{1}{m}g_n(x,\varepsilon)$$ and apply dominated convergence here by arguing that $|\frac{1}{m}g_n(x,\varepsilon)|\leq \frac{1}{m}h_n(x)$ where now in the dominated convergence theorem we are interested in whether $\lim_{m\to \infty} \frac{1}{m}\sum_{n=0}^{\infty}h_n(x)$ converges which we know it does by assumption? Is my idea nonsense?","I am interested in the double limit of a particular function of the form I want to compute Without the normalisation the limit reduces to where where does not necessarily converge for all choices of . My question is the following. Knowing that converges for almost every , are we allowed to say that and apply dominated convergence here by arguing that where now in the dominated convergence theorem we are interested in whether converges which we know it does by assumption? Is my idea nonsense?","f_m(x,\varepsilon) f_m(x,\varepsilon) = \frac{1}{m}\sum_{n=0}^{m-1}g(x,\varepsilon). \lim_{\varepsilon \to 0}\lim_{m\to \infty}\frac{1}{m}\sum_{n=0}^{m-1}g_n(x,\varepsilon). 1/m \lim_{\varepsilon \to 0}\sum_{n=0}^{\infty}g_n(x,\varepsilon) |g_n(x,\varepsilon)|\leq h_n(x) \sum_{n=0}^\infty h_n(x) x \lim_{m\to \infty} \frac{1}{m}\sum_{n=0}^{m-1}h_n(x) x \lim_{\varepsilon \to 0}\lim_{m\to \infty}\frac{1}{m}\sum_{n=0}^{m-1}g_n(x,\varepsilon)=\lim_{\varepsilon \to 0}\lim_{m\to \infty}\sum_{n=0}^{\infty}\frac{1}{m}g_n(x,\varepsilon) |\frac{1}{m}g_n(x,\varepsilon)|\leq \frac{1}{m}h_n(x) \lim_{m\to \infty} \frac{1}{m}\sum_{n=0}^{\infty}h_n(x)","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'convergence-divergence']"
41,Moore-Penrose pseudoinverse as a limit,Moore-Penrose pseudoinverse as a limit,,"For any matrix $A \in \mathbb{C}^{m \times n}$ , there exists a unique matrix $A^{+}$ such that: $$A^{+} A = \left( A^{+} A \right)^{*}, \qquad A A^{+} = \left( A A^{+} \right)^{*}, \qquad AA^{+}A=A, \qquad A^{+}AA^{+}=A^+$$ I have to prove that, for $\varepsilon>0$ : $$ A^{+} = \lim_{\varepsilon \to 0^{+}} A^{*} \left( A A^{*} + \varepsilon I_{m} \right)^{-1}$$ My idea is to prove that the limit expression satisfies the $4$ conditions. Using the fact that $AA^{*}+\varepsilon I_{m}$ is a hermitian matrix and $(AA^{*}+\varepsilon I_{m})^{-1}=\left((AA^{*}+\varepsilon I_{m})^{-1}\right)^*$ I was able to prove the first two conditions, but for the last two I can't think of how to do it. Any idea how to do it? Pd: When I test $A^{+}A=(A^{+}A)^{*}$ , can I put the limit inside the * operator every time? $$\lim_{\varepsilon\to 0^+}\left(A^{*}(AA^{*}+\varepsilon I_{m})^{-1}A\right)^*=\left(\lim_{\varepsilon\to 0^+}A^{*}(AA^{*}+\varepsilon I_{m})^{-1}\cdot A\right)^{*}$$","For any matrix , there exists a unique matrix such that: I have to prove that, for : My idea is to prove that the limit expression satisfies the conditions. Using the fact that is a hermitian matrix and I was able to prove the first two conditions, but for the last two I can't think of how to do it. Any idea how to do it? Pd: When I test , can I put the limit inside the * operator every time?","A \in \mathbb{C}^{m \times n} A^{+} A^{+} A = \left( A^{+} A \right)^{*}, \qquad A A^{+} = \left( A A^{+} \right)^{*}, \qquad AA^{+}A=A, \qquad A^{+}AA^{+}=A^+ \varepsilon>0  A^{+} = \lim_{\varepsilon \to 0^{+}} A^{*} \left( A A^{*} + \varepsilon I_{m} \right)^{-1} 4 AA^{*}+\varepsilon I_{m} (AA^{*}+\varepsilon I_{m})^{-1}=\left((AA^{*}+\varepsilon I_{m})^{-1}\right)^* A^{+}A=(A^{+}A)^{*} \lim_{\varepsilon\to 0^+}\left(A^{*}(AA^{*}+\varepsilon I_{m})^{-1}A\right)^*=\left(\lim_{\varepsilon\to 0^+}A^{*}(AA^{*}+\varepsilon I_{m})^{-1}\cdot A\right)^{*}","['linear-algebra', 'matrices', 'limits', 'pseudoinverse']"
42,Confusion in the definition of left-hand and right-hand limit,Confusion in the definition of left-hand and right-hand limit,,"In Understanding Analysis, the right-hand limit is defined as Definition 4.6.2. Given a limit point $c$ of a set $A\subseteq \mathbb R$ and a function $F :A \to \mathbb R$ , we write $$\lim_{x\to c^+} f(x) = L$$ if for all $\epsilon > 0$ there exists a $\delta > 0$ such that $|f(x) - L| < \epsilon$ whenever $0 < x-c < \delta$ . I think the requirement that $c$ is a limit point of $A$ is not strong enough for the right-hand limit to be well defined, because if $(c,\infty) \cap A$ is empty and $c$ is a limit point of $A$ (which is possible consider $A = [0,1]$ and $c = 1$ ), then any real number can be the right-hand limit for $f$ at $c$ . Can anyone tell me whether I am correct at this? Maybe we can add definitions for ""left limit points"" and ""right limit point"" to solve this issue.","In Understanding Analysis, the right-hand limit is defined as Definition 4.6.2. Given a limit point of a set and a function , we write if for all there exists a such that whenever . I think the requirement that is a limit point of is not strong enough for the right-hand limit to be well defined, because if is empty and is a limit point of (which is possible consider and ), then any real number can be the right-hand limit for at . Can anyone tell me whether I am correct at this? Maybe we can add definitions for ""left limit points"" and ""right limit point"" to solve this issue.","c A\subseteq \mathbb R F :A \to \mathbb R \lim_{x\to c^+} f(x) = L \epsilon > 0 \delta > 0 |f(x) - L| < \epsilon 0 < x-c < \delta c A (c,\infty) \cap A c A A = [0,1] c = 1 f c","['real-analysis', 'limits', 'analysis']"
43,Does this sequence of unit vectors in a Hilbert space converge?,Does this sequence of unit vectors in a Hilbert space converge?,,"Let $H$ be a Hilbert space, and let $A$ be a densely defined, unbounded operator on $H$ . Then there is a sequence $v_n$ of vectors in Dom $(A)$ such that $\frac{\|Av_n\|}{\|v_n\|}\to \infty$ . My question is essentially whether the obstruction to $\lim_{n\to \infty} Av_n$ being defined is only in norm. More precisely, Let $h\in H\setminus \text{Dom}(A)$ . Since $A$ is densely defined, there exists a Cauchy sequence $v_n$ in $\text{Dom}(A)$ converging to $h$ . Does the sequence $ \frac{Av_n}{\|Av_n\|}$ converge in $H$ to some unit vector? If so, is it independent of which sequence we pick converging to $h$ ? I'm having trouble making any progress since $A$ doesn't play well with the norm. I've tried considering the specific example of $\frac{d}{dx}$ acting on $L^2(\mathbb{R})$ without much success.","Let be a Hilbert space, and let be a densely defined, unbounded operator on . Then there is a sequence of vectors in Dom such that . My question is essentially whether the obstruction to being defined is only in norm. More precisely, Let . Since is densely defined, there exists a Cauchy sequence in converging to . Does the sequence converge in to some unit vector? If so, is it independent of which sequence we pick converging to ? I'm having trouble making any progress since doesn't play well with the norm. I've tried considering the specific example of acting on without much success.",H A H v_n (A) \frac{\|Av_n\|}{\|v_n\|}\to \infty \lim_{n\to \infty} Av_n h\in H\setminus \text{Dom}(A) A v_n \text{Dom}(A) h  \frac{Av_n}{\|Av_n\|} H h A \frac{d}{dx} L^2(\mathbb{R}),"['limits', 'hilbert-spaces', 'unbounded-operators']"
44,Limit right hand side at 0 and derivative,Limit right hand side at 0 and derivative,,"Suppose $f: (0,1) \to \mathbb R$ differentiable such that $$\lim_{x\to0^+}f(x) = A$$ and $$\lim_{x\to0^+}xf'(x)=B.$$ Find the value of B. My attempt: We have $$(xf(x))' = f(x) + xf'(x) \ \ \ \ \  (1)$$ But $(xf(x))'$ , in the limit $x\to0^+$ , is $$\lim_{x\to0^+}\frac{xf(x)}{x} = A$$ So taking limits on both sides of $(1)$ , we have $B = 0$ . Is my approach correct? If it isn't correct, how to solve this?","Suppose differentiable such that and Find the value of B. My attempt: We have But , in the limit , is So taking limits on both sides of , we have . Is my approach correct? If it isn't correct, how to solve this?","f: (0,1) \to \mathbb R \lim_{x\to0^+}f(x) = A \lim_{x\to0^+}xf'(x)=B. (xf(x))' = f(x) + xf'(x) \ \ \ \ \  (1) (xf(x))' x\to0^+ \lim_{x\to0^+}\frac{xf(x)}{x} = A (1) B = 0","['real-analysis', 'calculus', 'limits']"
45,Uniform convergence of this particular limit functional,Uniform convergence of this particular limit functional,,"Let $\{h_n(x)\}_{n\in\mathbb{N}}$ a family of infinitely differentiable functions, defined on the entire real line, vanishing outside $[a,b]$ . Let $F(x)$ be any function defined on $\mathbb{R}$ with the property that $F$ and all its derivatives are $O(|x|^{-N})$ as $|x|\to\infty$ , for every $N$ . By hypothesis, the following limit: $$\lim_{n\to\infty}\int_{x=a}^b h_n(x)F(x)\mathrm{d}x$$ exists for any $F$ of the type specified above. Is it true or false that $\lim_{n\to\infty}\int_{x=a}^b h_n(x)F(x+\tau)\mathrm{d}x$ converge uniformly w.r.t. $\tau\in[\tau_1,\tau_2]\subset\mathbb{R}$ ? I tried with Cauchy criterion, but without success... If the answer is ""no"", with which hypothesis the answer could become ""yes""?","Let a family of infinitely differentiable functions, defined on the entire real line, vanishing outside . Let be any function defined on with the property that and all its derivatives are as , for every . By hypothesis, the following limit: exists for any of the type specified above. Is it true or false that converge uniformly w.r.t. ? I tried with Cauchy criterion, but without success... If the answer is ""no"", with which hypothesis the answer could become ""yes""?","\{h_n(x)\}_{n\in\mathbb{N}} [a,b] F(x) \mathbb{R} F O(|x|^{-N}) |x|\to\infty N \lim_{n\to\infty}\int_{x=a}^b h_n(x)F(x)\mathrm{d}x F \lim_{n\to\infty}\int_{x=a}^b h_n(x)F(x+\tau)\mathrm{d}x \tau\in[\tau_1,\tau_2]\subset\mathbb{R}","['calculus', 'limits', 'uniform-convergence']"
46,Proof of Bertrand test of convergence,Proof of Bertrand test of convergence,,"How to prove the Bertrand convergence test that states: If there exists such limit that $$\lim_{n\to\infty}((n(\frac{a_n}{a_{n+1}}-1)-1)\ln{n})=q$$ then the series $\sum^{+\infty}_{n=1}=a_n$ , where $a_n>0, \forall n\in\mathbb{N}$ , converges if $q>1$ and diverges if $q<1$ . The problem is from the book Drugi, D., Collection of Problems in Mathematical Analysis (in Serbian),  Naša Knjiga D.O,O., 2003, pp. 38, problem 16.","How to prove the Bertrand convergence test that states: If there exists such limit that then the series , where , converges if and diverges if . The problem is from the book Drugi, D., Collection of Problems in Mathematical Analysis (in Serbian),  Naša Knjiga D.O,O., 2003, pp. 38, problem 16.","\lim_{n\to\infty}((n(\frac{a_n}{a_{n+1}}-1)-1)\ln{n})=q \sum^{+\infty}_{n=1}=a_n a_n>0, \forall n\in\mathbb{N} q>1 q<1","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
47,$\lim _{x\to 1}\left(\sqrt{x-1}\right)$,,\lim _{x\to 1}\left(\sqrt{x-1}\right),"I want to be clear with limits of even index root function like this one: $f(x)=\sqrt{x-1}$ . The domain of this function is $[1,\infty)$ . If I'm looking for one-sided limits: $$\lim _{x\to 1^{-}}\left(\sqrt{x-1}\right)=DNE$$ doesn't exist because the function is not defined at the left of $x=1$ . $$\lim _{x\to 1^{+}}\left(\sqrt{x-1}\right)=0$$ does exist because the function is defined at the right of $x=1$ Then, $$\lim _{x\to 1}\left(\sqrt{x-1}\right)=DNE$$ doesn't exist because the one sided limits are not equal. Is my analysis right?","I want to be clear with limits of even index root function like this one: . The domain of this function is . If I'm looking for one-sided limits: doesn't exist because the function is not defined at the left of . does exist because the function is defined at the right of Then, doesn't exist because the one sided limits are not equal. Is my analysis right?","f(x)=\sqrt{x-1} [1,\infty) \lim _{x\to 1^{-}}\left(\sqrt{x-1}\right)=DNE x=1 \lim _{x\to 1^{+}}\left(\sqrt{x-1}\right)=0 x=1 \lim _{x\to 1}\left(\sqrt{x-1}\right)=DNE","['calculus', 'limits']"
48,How could I evaluate $\sum_{r=1}^\infty{\frac{1}{r\sqrt{r+1}}}\;?$,How could I evaluate,\sum_{r=1}^\infty{\frac{1}{r\sqrt{r+1}}}\;?,"How could I evaluate $\displaystyle\,\sum_{r=1}^\infty{\frac{1}{r\sqrt{r+1}}}\;?$ I have tried the substitution $r=k^2-1$ , but this yields a sum with a square root as the starting point ( $\sqrt2$ ). $$\int_1^\infty{\frac{1}{\lfloor x^2 \rfloor}}=\int_1^\sqrt{2}{1}dx+\int_\sqrt{2}^\sqrt{3}{\frac{1}{2}}dx+\int_\sqrt{3}^\sqrt{4}{\frac{1}{3}}dx\,+...$$ I think.","How could I evaluate I have tried the substitution , but this yields a sum with a square root as the starting point ( ). I think.","\displaystyle\,\sum_{r=1}^\infty{\frac{1}{r\sqrt{r+1}}}\;? r=k^2-1 \sqrt2 \int_1^\infty{\frac{1}{\lfloor x^2 \rfloor}}=\int_1^\sqrt{2}{1}dx+\int_\sqrt{2}^\sqrt{3}{\frac{1}{2}}dx+\int_\sqrt{3}^\sqrt{4}{\frac{1}{3}}dx\,+...","['limits', 'summation']"
49,How do I find the limit of this function involving complex numbers as $x → 0$?,How do I find the limit of this function involving complex numbers as ?,x → 0,"I have the function $$ f(x) = \frac{\Im\left(\frac{M^{x + 1} - m^{x + 1}}{M^x - m^x}\right) - \frac{\Im(a)}{2}}{\Re\left(\frac{M^{x + 1} - m^{x + 1}}{M^x - m^x}\right) - \frac{\Re(a)}{2}} $$ where $a$ and $b$ are complex numbers, $M = \frac{a + \sqrt{a^2 + 4 b}}{2}$ , $m = \frac{a - \sqrt{a^2 + 4 b}}{2}$ , and $\Re(z)$ and $\Im(z)$ are the real and imaginary parts of any complex-valued $z$ , respectively. I want to know how to find the limit of this function as $x → 0$ in terms of $a$ , $b$ , $M$ , and/or $m$ . By plotting the function in graphing software I can see that the limit exists, as $\lim_{x → 0^+} f(x) = \lim_{x → 0^-} f(x)$ . However, the presence of the complex numbers is confusing me as to how to proceed. Edit with additional information on implementing Vishu's answer: The graphing software I'm using, GeoGebra, does not allow specifying which branch of the complex logarithm is used, so for some values of $a$ and $b$ Vishu's answer $$ \lim_{x → 0} f(x) = \frac{ \Im\left(\frac{M - m}{\ln\left(\frac{M}{m}\right)}\right) }{ \Re\left(\frac{M - m}{ln\left(\frac{M}{m}\right)}\right) } $$ appears to not work. This can be fixed by instead using the equation $$ \lim_{x → 0} f(x) = \frac{\Im\left(\frac{M - m}{\ln\left(\left|\frac{M}{m}\right|\right) + i \left(\arg\left(\frac{M}{m}\right) + 2 π k\right)}\right)} {\Re\left(\frac{M - m}{\ln\left(\left|\frac{M}{m}\right|\right) + i \left(\arg\left(\frac{M}{m}\right) + 2 π k\right)}\right)} $$ where $k$ is defined as $$ k = \begin{cases} 1 & : 0 < \arg(b) ≤ π ∧ \arg(\sqrt{b}) ≤ \arg(M) ≤ \arg(b) ∨ -π ≤ \arg(b) < 0 ∧ \arg(-\sqrt{b}) ≤ \arg(M) ≤ π \\ −1 & : -π ≤ \arg(b) < 0 ∧ \arg(b) ≤ \arg(M) ≤ \arg(\sqrt{b}) ∨ 0 < \arg(b) ≤ π ∧ -π ≤ \arg(M) ≤ \arg(-\sqrt{b}) \\ 0 & : \text{otherwise} \end{cases} $$","I have the function where and are complex numbers, , , and and are the real and imaginary parts of any complex-valued , respectively. I want to know how to find the limit of this function as in terms of , , , and/or . By plotting the function in graphing software I can see that the limit exists, as . However, the presence of the complex numbers is confusing me as to how to proceed. Edit with additional information on implementing Vishu's answer: The graphing software I'm using, GeoGebra, does not allow specifying which branch of the complex logarithm is used, so for some values of and Vishu's answer appears to not work. This can be fixed by instead using the equation where is defined as","
f(x) = \frac{\Im\left(\frac{M^{x + 1} - m^{x + 1}}{M^x - m^x}\right) - \frac{\Im(a)}{2}}{\Re\left(\frac{M^{x + 1} - m^{x + 1}}{M^x - m^x}\right) - \frac{\Re(a)}{2}}
 a b M = \frac{a + \sqrt{a^2 + 4 b}}{2} m = \frac{a - \sqrt{a^2 + 4 b}}{2} \Re(z) \Im(z) z x → 0 a b M m \lim_{x → 0^+} f(x) = \lim_{x → 0^-} f(x) a b 
\lim_{x → 0} f(x) = \frac{
\Im\left(\frac{M - m}{\ln\left(\frac{M}{m}\right)}\right)
}{
\Re\left(\frac{M - m}{ln\left(\frac{M}{m}\right)}\right)
}
 
\lim_{x → 0} f(x) = \frac{\Im\left(\frac{M - m}{\ln\left(\left|\frac{M}{m}\right|\right) + i \left(\arg\left(\frac{M}{m}\right) + 2 π k\right)}\right)}
{\Re\left(\frac{M - m}{\ln\left(\left|\frac{M}{m}\right|\right) + i \left(\arg\left(\frac{M}{m}\right) + 2 π k\right)}\right)}
 k 
k = \begin{cases}
1 & : 0 < \arg(b) ≤ π ∧ \arg(\sqrt{b}) ≤ \arg(M) ≤ \arg(b) ∨ -π ≤ \arg(b) < 0 ∧ \arg(-\sqrt{b}) ≤ \arg(M) ≤ π \\
−1 & : -π ≤ \arg(b) < 0 ∧ \arg(b) ≤ \arg(M) ≤ \arg(\sqrt{b}) ∨ 0 < \arg(b) ≤ π ∧ -π ≤ \arg(M) ≤ \arg(-\sqrt{b}) \\
0 & : \text{otherwise}
\end{cases}
","['calculus', 'limits', 'complex-numbers']"
50,Which of the following statements is/are correct about a real-valued bounded function in $\mathbb{R}^2?$,Which of the following statements is/are correct about a real-valued bounded function in,\mathbb{R}^2?,"Let $f$ be a real-valued bounded function in $\mathbb{R}^{2}$ such that for all real $t,$ the functions $g_t(y)=f(t,y)$ and $h_t(x)=f(x,t)$ are non-decreasing. Then which of the following is/are correct? $f(x,x)$ is non-decreasing. $f$ can have uncountable number of discontinuities. $\lim\limits_{(x,y)\to(+\infty,+\infty)} f(x,y)$ exists. $\lim\limits_{(x,y)\to(+\infty,-\infty)} f(x,y)$ exists. Option $1$ seems correct to me since $f$ is non-decreasing in each co-ordinate. For 2) let $f(x,y)=\left\{\begin{align} 0, & \mbox{ if }~ x\leq0 \mbox{ or } y\leq0\\ 1, ~& x>0\mbox{ and y>0}\end{align}\right.$ Then $f$ is bounded in $\mathbb{R}^2,$ is non-decreasing and is discontinuous at each point on x-axis as well as on y-axis. So option 2) is also correct. I am thinking that 3) and 4) should also be correct since $f$ is given to be bounded. However, I am not sure.","Let be a real-valued bounded function in such that for all real the functions and are non-decreasing. Then which of the following is/are correct? is non-decreasing. can have uncountable number of discontinuities. exists. exists. Option seems correct to me since is non-decreasing in each co-ordinate. For 2) let Then is bounded in is non-decreasing and is discontinuous at each point on x-axis as well as on y-axis. So option 2) is also correct. I am thinking that 3) and 4) should also be correct since is given to be bounded. However, I am not sure.","f \mathbb{R}^{2} t, g_t(y)=f(t,y) h_t(x)=f(x,t) f(x,x) f \lim\limits_{(x,y)\to(+\infty,+\infty)} f(x,y) \lim\limits_{(x,y)\to(+\infty,-\infty)} f(x,y) 1 f f(x,y)=\left\{\begin{align}
0, & \mbox{ if }~ x\leq0 \mbox{ or } y\leq0\\
1, ~& x>0\mbox{ and y>0}\end{align}\right. f \mathbb{R}^2, f","['real-analysis', 'calculus', 'limits', 'continuity', 'examples-counterexamples']"
51,Limit of limits converges,Limit of limits converges,,"Let $(a_k^\ell)_{k,\ell\in\mathbb N}$ be a double sequence that is Cauchy in the lower coordinate, i.e. $$\sup_{\ell\in\mathbb N}|a_m^\ell-a_n^\ell|<\varepsilon$$ for all $n,m\geq N(\varepsilon)$ . I know that that $a_k:=\lim\limits_{\ell\rightarrow\infty} a_k^\ell$ exists for all $k$ . Let's further assume that $a^\ell:=\lim\limits_{k\rightarrow\infty} a_k^\ell$ exists for all $\ell$ (i.e. we are in $c$ -space). I want to show that $(a^\ell)_{\ell\in\mathbb N}$ converges. My idea is to show it is Cauchy via an $\frac{\varepsilon}{3}$ -argument as follows: $$|a^n-a^m|\leq|a^n-a_k^n|+|a_k^n-a_k^m|+|a_k^m-a^m|<\varepsilon$$ The problem is that the minimal $N(\varepsilon)$ is not well-defined in this situation, because the $k$ is dependent on $n$ and $m$ , and $n$ and $m$ are dependent on $k$ . Can I salvage this or do I have to prove that $(a_k)_{k\in\mathbb N}$ is convergent first?","Let be a double sequence that is Cauchy in the lower coordinate, i.e. for all . I know that that exists for all . Let's further assume that exists for all (i.e. we are in -space). I want to show that converges. My idea is to show it is Cauchy via an -argument as follows: The problem is that the minimal is not well-defined in this situation, because the is dependent on and , and and are dependent on . Can I salvage this or do I have to prove that is convergent first?","(a_k^\ell)_{k,\ell\in\mathbb N} \sup_{\ell\in\mathbb N}|a_m^\ell-a_n^\ell|<\varepsilon n,m\geq N(\varepsilon) a_k:=\lim\limits_{\ell\rightarrow\infty} a_k^\ell k a^\ell:=\lim\limits_{k\rightarrow\infty} a_k^\ell \ell c (a^\ell)_{\ell\in\mathbb N} \frac{\varepsilon}{3} |a^n-a^m|\leq|a^n-a_k^n|+|a_k^n-a_k^m|+|a_k^m-a^m|<\varepsilon N(\varepsilon) k n m n m k (a_k)_{k\in\mathbb N}","['real-analysis', 'functional-analysis', 'limits']"
52,Study the character of recursive sequence,Study the character of recursive sequence,,"I have tried yesterday to ask this question but probably it was not well written so I have decided to show you my attempt step by step. I have to study the character of the following: $$x_{n}=-x_{n-1}^2,\,\, x_1=x\in\mathbb{R}$$ First of all I have observed that for x=0 and x=-1 the sequence is constant so: if $x=0$ then $\lim_{n\to\infty}x_n=0$ , since $x_n=x_{n-1}$ iff $x_{n}=0,1$ . if $x=-1$ . $x_1=-1,\, x_2=-1\,\,x_3=-1....x_n=-1$ then $\lim_{n\to\infty}x_n=-1$ Then $x_{n}-x_{n-1}=-x_{n-1}^2-x_{n-1}$ . So: if $-x_{n-1}^2-x_{n-1}>0$ then $-1<x_{n-1}<0$ . Thus if $-1<x_1<0$ then $-1<x_n<0$ for each $n$ . This means that the sequence is increasing and so it is convergent: $\lim_{n\to\infty}x_{n}=sup\{x_n\}=l$ . $\textbf{l=0}$ since $x_n<0$ . if $-x_{n-1}^2-x_{n-1}<0$ then $x_{n-1}<-1\, \vee x_{n-1}>0$ . Thus if $x_1<-1$ then $x_n<-1$ for each $n$ : $\lim_{n\to\infty}x_{n}=inf\{x_n\}=l$ . Since the possible finite limits are $0,-1$ , they are not acceptable. So $\textbf{l=}$ $-\infty$ . Then?","I have tried yesterday to ask this question but probably it was not well written so I have decided to show you my attempt step by step. I have to study the character of the following: First of all I have observed that for x=0 and x=-1 the sequence is constant so: if then , since iff . if . then Then . So: if then . Thus if then for each . This means that the sequence is increasing and so it is convergent: . since . if then . Thus if then for each : . Since the possible finite limits are , they are not acceptable. So . Then?","x_{n}=-x_{n-1}^2,\,\, x_1=x\in\mathbb{R} x=0 \lim_{n\to\infty}x_n=0 x_n=x_{n-1} x_{n}=0,1 x=-1 x_1=-1,\, x_2=-1\,\,x_3=-1....x_n=-1 \lim_{n\to\infty}x_n=-1 x_{n}-x_{n-1}=-x_{n-1}^2-x_{n-1} -x_{n-1}^2-x_{n-1}>0 -1<x_{n-1}<0 -1<x_1<0 -1<x_n<0 n \lim_{n\to\infty}x_{n}=sup\{x_n\}=l \textbf{l=0} x_n<0 -x_{n-1}^2-x_{n-1}<0 x_{n-1}<-1\, \vee x_{n-1}>0 x_1<-1 x_n<-1 n \lim_{n\to\infty}x_{n}=inf\{x_n\}=l 0,-1 \textbf{l=} -\infty","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification']"
53,A limit involving binomial coefficients and square roots,A limit involving binomial coefficients and square roots,,"I am trying to evaluate the following limit $$\lim_{n\to\infty}\frac{1}{2^n\sqrt{n}}\sum^n_{k=1}\binom{n}{k}\sqrt{k},$$ but to no avail. I experimented with some large values of $n,$ and it seems like the limit is $\frac{1}{\sqrt{2}}.$ In fact, I am able to prove that $$\frac{1}{2^n\sqrt{n}}\sum^n_{k=1}\binom{n}{k}\sqrt{k}\leq\frac{1}{\sqrt{2}}$$ using the fact that the square root function is concave, as follows. We show that $$\sum^n_{k=0}\binom{n}{k}\sqrt{k}\leq \sum^n_{k=0}\binom{n}{k}\sqrt{\frac{n}{2}}=2^n\sqrt{\frac{n}{2}}.$$ Indeed, for $k\leq\frac{n}{2},$ one has $\binom{n}{k}\sqrt{k}\leq\binom{n}{k}\sqrt{\frac{n}{2}}.$ But since square root is concave, we get $\sqrt{\frac{n}{2}}-\sqrt{k}\geq\sqrt{n-k}-\sqrt{\frac{n}{2}}.$ Moreover, $\binom{n}{k}=\binom{n}{n-k},$ so it follows that $$\sum^n_{k=0}\binom{n}{k}\left(\sqrt{k}-\sqrt{\frac{n}{2}}\right)\leq 0,$$ as desired. With this, I suspect that one may obtain a lower bound of the sum and apply squeeze theorem to obtain the final limit. Any ideas on this?","I am trying to evaluate the following limit but to no avail. I experimented with some large values of and it seems like the limit is In fact, I am able to prove that using the fact that the square root function is concave, as follows. We show that Indeed, for one has But since square root is concave, we get Moreover, so it follows that as desired. With this, I suspect that one may obtain a lower bound of the sum and apply squeeze theorem to obtain the final limit. Any ideas on this?","\lim_{n\to\infty}\frac{1}{2^n\sqrt{n}}\sum^n_{k=1}\binom{n}{k}\sqrt{k}, n, \frac{1}{\sqrt{2}}. \frac{1}{2^n\sqrt{n}}\sum^n_{k=1}\binom{n}{k}\sqrt{k}\leq\frac{1}{\sqrt{2}} \sum^n_{k=0}\binom{n}{k}\sqrt{k}\leq \sum^n_{k=0}\binom{n}{k}\sqrt{\frac{n}{2}}=2^n\sqrt{\frac{n}{2}}. k\leq\frac{n}{2}, \binom{n}{k}\sqrt{k}\leq\binom{n}{k}\sqrt{\frac{n}{2}}. \sqrt{\frac{n}{2}}-\sqrt{k}\geq\sqrt{n-k}-\sqrt{\frac{n}{2}}. \binom{n}{k}=\binom{n}{n-k}, \sum^n_{k=0}\binom{n}{k}\left(\sqrt{k}-\sqrt{\frac{n}{2}}\right)\leq 0,","['limits', 'binomial-coefficients']"
54,Finding the pointwise limit of a sequence of elementary functions defined by n * the characteristic function.,Finding the pointwise limit of a sequence of elementary functions defined by n * the characteristic function.,,"I basically want to find the limit of the following sequence: $f_n = nX_{[0, 1/n]}$ $X := 1$ if $x \in [0, 1/n], x = 0$ otherwise. When I look at this function, my first thought is that the limit approaches infinity and therefore doesn't exist, since even though x would have to be closer and closer to 0 for the function to not equal 0, n*1 = n. However, according to the problem description, a limit f should exist. I'm afraid my understanding of this topic is still very basic so if someone could help explain, I'd appreciate it.","I basically want to find the limit of the following sequence: if otherwise. When I look at this function, my first thought is that the limit approaches infinity and therefore doesn't exist, since even though x would have to be closer and closer to 0 for the function to not equal 0, n*1 = n. However, according to the problem description, a limit f should exist. I'm afraid my understanding of this topic is still very basic so if someone could help explain, I'd appreciate it.","f_n = nX_{[0, 1/n]} X := 1 x \in [0, 1/n], x = 0","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'lebesgue-measure']"
55,Is this function differentiable on $\mathbb{R}$?,Is this function differentiable on ?,\mathbb{R},"Here is my prood to see if this function is differentiable over $\mathbb{R}$ . $$f(x) = \vert x\vert x$$ \begin{equation*} \begin{split} \lim_{h\to 0} \dfrac{(x+h)\vert x+h \vert - x\vert x \vert}{h} & = \lim_{h\to 0} \dfrac{x\vert x+h \vert + h \vert x + h\vert - x\vert x \vert }{h} \\\\ & = \lim_{h\to 0} \dfrac{x\vert x \vert + x\vert h\vert + h \vert x \vert + h\vert h \vert - x\vert x \vert}{h} \\\\ & =  \lim_{h\to 0} x \dfrac{\vert h \vert}{h} + \vert x \vert + \vert h \vert \end{split} \end{equation*} Now the last term is zero. The middle term is just $\vert x \vert $ and the first one now depends on the direction: $$\lim_{h\to 0^+} x \dfrac{\vert h \vert}{h} = x$$ $$\lim_{h\to 0^-} x \dfrac{\vert h \vert}{h} = -x$$ Consequently, the limits are different (here $x$ can be whatever number), hence the function exhibits an angular point at every $x$ in the domain. This concludes the function is not differentiable in $\mathbb{R}$ . Now it looks weird to me, maybe I'm right or perhaps I'm wrong. Asking you for help in case!","Here is my prood to see if this function is differentiable over . Now the last term is zero. The middle term is just and the first one now depends on the direction: Consequently, the limits are different (here can be whatever number), hence the function exhibits an angular point at every in the domain. This concludes the function is not differentiable in . Now it looks weird to me, maybe I'm right or perhaps I'm wrong. Asking you for help in case!","\mathbb{R} f(x) = \vert x\vert x \begin{equation*}
\begin{split}
\lim_{h\to 0} \dfrac{(x+h)\vert x+h \vert - x\vert x \vert}{h} & = \lim_{h\to 0} \dfrac{x\vert x+h \vert + h \vert x + h\vert - x\vert x \vert }{h}
\\\\
& = \lim_{h\to 0} \dfrac{x\vert x \vert + x\vert h\vert + h \vert x \vert + h\vert h \vert - x\vert x \vert}{h}
\\\\
& =  \lim_{h\to 0} x \dfrac{\vert h \vert}{h} + \vert x \vert + \vert h \vert
\end{split}
\end{equation*} \vert x \vert  \lim_{h\to 0^+} x \dfrac{\vert h \vert}{h} = x \lim_{h\to 0^-} x \dfrac{\vert h \vert}{h} = -x x x \mathbb{R}","['limits', 'derivatives', 'solution-verification', 'proof-writing', 'absolute-value']"
56,Cardinality of a set including fractions,Cardinality of a set including fractions,,"Attempt: Let $G = \{x > 0 : \lim_{n→∞} \mathsf{frac}((n!)x) = 0\} .$ For each $f ∈ 2^ω$ , define $x_{f} = \sum_{n\ge1} \frac{f(n)}{n!}$ Put $W=\{x_{f} :f∈2^ω\}$ . I don't know how to proceed from here.","Attempt: Let For each , define Put . I don't know how to proceed from here.",G = \{x > 0 : \lim_{n→∞} \mathsf{frac}((n!)x) = 0\} . f ∈ 2^ω x_{f} = \sum_{n\ge1} \frac{f(n)}{n!} W=\{x_{f} :f∈2^ω\},"['limits', 'cardinals']"
57,Limit as $x \to 0^-$ of $x^x$ from the negative side using complex numbers,Limit as  of  from the negative side using complex numbers,x \to 0^- x^x,"We know that $\lim_\limits{x \to 0^+}x^x = 1$ . We also know that the limit as $x \to 0^-$ does not exist, at least for $x \in \mathbb{R}$ . When considering complex numbers, for $z\in \mathbb{C}$ , does the following limit converge? $$\lim_\limits{z \to 0^-}z^z$$ My attempt: Separate $z$ into its real and imaginary components, making this a multivariable limit. Let $\text{Re}(z)=\sigma,$ and $\text{Im}(z) = t$ . Thus, we have $$\lim_\limits{(\sigma, t) \to 0^-}(\sigma + it)^{\sigma+it}$$ But I'm not sure where to go from here.","We know that . We also know that the limit as does not exist, at least for . When considering complex numbers, for , does the following limit converge? My attempt: Separate into its real and imaginary components, making this a multivariable limit. Let and . Thus, we have But I'm not sure where to go from here.","\lim_\limits{x \to 0^+}x^x = 1 x \to 0^- x \in \mathbb{R} z\in \mathbb{C} \lim_\limits{z \to 0^-}z^z z \text{Re}(z)=\sigma, \text{Im}(z) = t \lim_\limits{(\sigma, t) \to 0^-}(\sigma + it)^{\sigma+it}","['limits', 'analysis', 'complex-numbers']"
58,Why is the function $(-2)^{x}$ continuous?,Why is the function  continuous?,(-2)^{x},"If we work over complex mumbers we have $$\lim_{x\to\infty}(-2)^{\frac πx}=(-2)^0=1$$ But, if we work over $\Bbb R$ then the limit $\lim_{x\to\infty}(-2)^{\frac πx}$ should be undefined.  Because, as far as I know $a^x$ is not well defined if $a<0$ and $x\not\in \Bbb Z$ . Thus I couldn't understand. Why is the function $(-2)^{x}$ continuous? What does Wolfram mean?","If we work over complex mumbers we have But, if we work over then the limit should be undefined.  Because, as far as I know is not well defined if and . Thus I couldn't understand. Why is the function continuous? What does Wolfram mean?",\lim_{x\to\infty}(-2)^{\frac πx}=(-2)^0=1 \Bbb R \lim_{x\to\infty}(-2)^{\frac πx} a^x a<0 x\not\in \Bbb Z (-2)^{x},"['calculus', 'algebra-precalculus', 'limits', 'exponential-function']"
59,How can I prove this limit result associating with an infinite nested radical,How can I prove this limit result associating with an infinite nested radical,,"Let $a_n=\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{n}}}}$ I can show that $\lim\limits_{n\to∞}a_n$ converges,let $l=\lim\limits_{n\to∞}a_n$ Now what puzzles me is that how to prove $\lim\limits_{n\to∞}\sqrt{n}\sqrt[n]{l-a_n}=\frac{\sqrt{e}}{2}$ I have already figured out that $\lim\limits_{n\to∞}\sqrt{n}\sqrt[n]{l-a_n}\le\frac{\sqrt{e}}{2}$ How about the other half? P.S. Here's how I work the upper bound out: Since $a_n$ converges, we have $\begin{aligned} l-a_n=& \sum\limits_{i=n}^∞(a_{i+1}-a_i)\\ =& \sum\limits_{i=n}^∞(\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}}-\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i}}}})\\ =&\sum\limits_{i=n}^∞\frac{\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}-\sqrt{2+\sqrt{3+...+\sqrt{i}}}}{\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}}+\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i}}}}}\\ \le&\sum\limits_{i=n}^∞\frac{\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}-\sqrt{2+\sqrt{3+...+\sqrt{i}}}}{2\sqrt{1}}\\ \le&...(\text{repeat the process n times})\\ \le&\sum\limits_{i=n}^∞\frac{\sqrt{i+1}}{2^i\sqrt{i!}}\\ <&\sum_{i=n}^\infty \frac{\sqrt{i+1}}{2^i\sqrt{i!}}\\ \le&\frac{1}{2^n\sqrt{n!}}\sum_{i=n}^\infty\frac{\sqrt{i+1}}{2^{i-n}}\\ =&\frac{1}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{\sqrt{n+i+1}}{2^i} =\frac{\sqrt{n}}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{1}{2^i}+\frac{1}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{\sqrt{n+i+1}-\sqrt{n}}{2^i}\\ =&\frac{2\sqrt{n}}{2^n\sqrt{n!}}+\frac{1}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{1}{2^i}\cdot\frac{i+1}{\sqrt{n+i+1}+\sqrt{n}}<\frac{2\sqrt{n}+C}{2^n\sqrt{n!}}<\frac{2\sqrt{n}+n}{2^n\sqrt{n!}} \end{aligned} $ Now by this estimation of the upper bound, we can show $\lim\limits_{n\to∞}\sqrt{n}\sqrt[n]{l-a_n}\le\frac{\sqrt{e}}{2}$ by calculation(with Stirling's approximation)","Let I can show that converges,let Now what puzzles me is that how to prove I have already figured out that How about the other half? P.S. Here's how I work the upper bound out: Since converges, we have Now by this estimation of the upper bound, we can show by calculation(with Stirling's approximation)","a_n=\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{n}}}} \lim\limits_{n\to∞}a_n l=\lim\limits_{n\to∞}a_n \lim\limits_{n\to∞}\sqrt{n}\sqrt[n]{l-a_n}=\frac{\sqrt{e}}{2} \lim\limits_{n\to∞}\sqrt{n}\sqrt[n]{l-a_n}\le\frac{\sqrt{e}}{2} a_n \begin{aligned}
l-a_n=& \sum\limits_{i=n}^∞(a_{i+1}-a_i)\\
=& \sum\limits_{i=n}^∞(\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}}-\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i}}}})\\
=&\sum\limits_{i=n}^∞\frac{\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}-\sqrt{2+\sqrt{3+...+\sqrt{i}}}}{\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}}+\sqrt{1+\sqrt{2+\sqrt{3+...+\sqrt{i}}}}}\\
\le&\sum\limits_{i=n}^∞\frac{\sqrt{2+\sqrt{3+...+\sqrt{i+1}}}-\sqrt{2+\sqrt{3+...+\sqrt{i}}}}{2\sqrt{1}}\\
\le&...(\text{repeat the process n times})\\
\le&\sum\limits_{i=n}^∞\frac{\sqrt{i+1}}{2^i\sqrt{i!}}\\
<&\sum_{i=n}^\infty \frac{\sqrt{i+1}}{2^i\sqrt{i!}}\\
\le&\frac{1}{2^n\sqrt{n!}}\sum_{i=n}^\infty\frac{\sqrt{i+1}}{2^{i-n}}\\
=&\frac{1}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{\sqrt{n+i+1}}{2^i}
=\frac{\sqrt{n}}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{1}{2^i}+\frac{1}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{\sqrt{n+i+1}-\sqrt{n}}{2^i}\\
=&\frac{2\sqrt{n}}{2^n\sqrt{n!}}+\frac{1}{2^n\sqrt{n!}}\sum_{i=0}^\infty\frac{1}{2^i}\cdot\frac{i+1}{\sqrt{n+i+1}+\sqrt{n}}<\frac{2\sqrt{n}+C}{2^n\sqrt{n!}}<\frac{2\sqrt{n}+n}{2^n\sqrt{n!}}
\end{aligned}
 \lim\limits_{n\to∞}\sqrt{n}\sqrt[n]{l-a_n}\le\frac{\sqrt{e}}{2}","['limits', 'analysis', 'radicals']"
60,For what values of $a$ does $\prod\limits_{k=1}^n a|\sin{k}|\to\infty$ as $n\to\infty$?,For what values of  does  as ?,a \prod\limits_{k=1}^n a|\sin{k}|\to\infty n\to\infty,"For what values of $a$ does $P=\prod\limits_{k=1}^n a|\sin{k}|\to\infty$ as $n\to\infty$ ? Experimenting on desmos, it seemed that if $a>2$ then $P\to\infty$ , but some strange cases like $\prod\limits_{k=1}^{120000} 2.0001|\sin{k}|\approx 4\times10^{-17}$ made me doubt it. Either there exists a critical value for $a$ such that $P\to\infty$ , or $P\not\to\infty$ for all $a$ . Either way, I think it's astounding.","For what values of does as ? Experimenting on desmos, it seemed that if then , but some strange cases like made me doubt it. Either there exists a critical value for such that , or for all . Either way, I think it's astounding.",a P=\prod\limits_{k=1}^n a|\sin{k}|\to\infty n\to\infty a>2 P\to\infty \prod\limits_{k=1}^{120000} 2.0001|\sin{k}|\approx 4\times10^{-17} a P\to\infty P\not\to\infty a,"['limits', 'trigonometry', 'infinite-product']"
61,prove that is big-Oh [closed],prove that is big-Oh [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Show that $2\sqrt n + n^{1/3} \log_2(n) = O(\sqrt n)$ . I need help understanding how to solve this limit: $$ \lim_{n\to \infty}\frac{2\sqrt n + n^{1/3} \log_2(n)}{\sqrt n}$$ thanks","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Show that . I need help understanding how to solve this limit: thanks",2\sqrt n + n^{1/3} \log_2(n) = O(\sqrt n)  \lim_{n\to \infty}\frac{2\sqrt n + n^{1/3} \log_2(n)}{\sqrt n},"['limits', 'asymptotics']"
62,"Does $(a,b,1)$ lie on a line or a plane?",Does  lie on a line or a plane?,"(a,b,1)","The following question is taken from Cengage book by G.Tiwani. This book is used for the preparation of IIT-JEE exam. Question : A function $f:\mathbb R \to \mathbb R$ is defined as $$f(x)=\lim_{n\to\infty}\frac{ax^2+bx+c+e^{nx}}{1+ce^{nx}},$$ where $f$ is continuous on $\mathbb R$ then (A) points $(a,b,c)$ lie on a line in the 3-dimensional coordinate system. (B) points $(a,b)$ represent the 2-dimensional Cartesian plane (C) locus of points $(a,c)$ and $(c,b)$ intersect at one point. (D) points $(a,b,c)$ lie on a plane in the 3-dimensional coordinate system. My Attempt: $$f(x)=\begin{cases} ax^2+bx+c;& x\lt0 \\ \frac{c+1}{1+c};&x=0 \\ \frac1c;&x\gt0\end{cases}$$ Therefore, $c=1, a, b \in \mathbb R$ So, I think the options B,D are correct. But the answer given is A,B,C. How to approach this?","The following question is taken from Cengage book by G.Tiwani. This book is used for the preparation of IIT-JEE exam. Question : A function is defined as where is continuous on then (A) points lie on a line in the 3-dimensional coordinate system. (B) points represent the 2-dimensional Cartesian plane (C) locus of points and intersect at one point. (D) points lie on a plane in the 3-dimensional coordinate system. My Attempt: Therefore, So, I think the options B,D are correct. But the answer given is A,B,C. How to approach this?","f:\mathbb R \to \mathbb R f(x)=\lim_{n\to\infty}\frac{ax^2+bx+c+e^{nx}}{1+ce^{nx}}, f \mathbb R (a,b,c) (a,b) (a,c) (c,b) (a,b,c) f(x)=\begin{cases} ax^2+bx+c;& x\lt0 \\ \frac{c+1}{1+c};&x=0 \\ \frac1c;&x\gt0\end{cases} c=1, a, b \in \mathbb R","['calculus', 'geometry', 'limits', 'contest-math', '3d']"
63,"Uniform convergence of $f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2})$ on $[0,a],a>0$.",Uniform convergence of  on .,"f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}) [0,a],a>0","I want to show the uniform convergence of $f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2})$ on $[0,a],a>0$ . I tried it as follows: $f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}-2n\pi)=n\sin\left(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\right)\sim n\left(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\right)\to \frac{x^2}{4\pi}$ . It follows that $f_n$ converges pointwise to $f:x\mapsto \frac{x^2}{4\pi}$ . To show uniform convergence, I want to show that the sequence $y_n:=\sup_{x\in [0,a]}|f_n(x)-f(x)|$ converges to $0$ . $\begin{align} f_n(x)&=n\left((\sqrt{4\pi^2n^2+x^2}-2n\pi)-(\sqrt{4\pi^2n^2+x^2}-2n\pi)^3 \frac 1{3!}\right)+o(1/n)\\ &=\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)-\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)^3\frac{1}{n^23!}+o(1/n) \end{align}$ $\begin{align}|f_n(x)-f(x)|&\le \left|\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)-\frac {x^2}{4\pi}\right|+\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)^3\frac{1}{n^23!}+o(1/n)\\ &\le a^2\left(\frac 1{4\pi}-\frac 1{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)+o(1/n)\\ &\le a^2\left( \frac 1{4\pi}-\frac 1{\sqrt{4\pi^2+(a/n)^2}+2\pi}\right)+o(1/n)\end{align}$ It follows that $y_n\to 0$ . Is my proof correct? Thanks. Note: $(1): f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}-2n\pi)=n\sin t= n(t-\frac {t^3}{3!}) +nR(t),$ where $t=\sqrt{4\pi^2n^2+x^2}-2n\pi=\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\le \frac {a^2}{4n\pi}$ and $R(t)$ is the remainder term. $(2): |R(t)|\le t^5\sum_{k=1}^\infty \frac{t^{2k-2}}{(2k+3)!}\le \frac{a^{10}}{{4\pi}^{5}n^5}\color{blue}{\sum_{k=1}^\infty \frac{a^{4k-4}}{(4n\pi)^{2k-2}(2k+3)!}}=M\frac{a^{10}}{{4\pi}^{5}n^5}$ . The blue colored series converges and is hence bounded by some $M>0$ . It follows that $|n R(t)|\le M\frac{a^{10}}{{4\pi}^{5}n^4}$ , whence $R(t)=o(1/n)$ .","I want to show the uniform convergence of on . I tried it as follows: . It follows that converges pointwise to . To show uniform convergence, I want to show that the sequence converges to . It follows that . Is my proof correct? Thanks. Note: where and is the remainder term. . The blue colored series converges and is hence bounded by some . It follows that , whence .","f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}) [0,a],a>0 f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}-2n\pi)=n\sin\left(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\right)\sim n\left(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\right)\to \frac{x^2}{4\pi} f_n f:x\mapsto \frac{x^2}{4\pi} y_n:=\sup_{x\in [0,a]}|f_n(x)-f(x)| 0 \begin{align}
f_n(x)&=n\left((\sqrt{4\pi^2n^2+x^2}-2n\pi)-(\sqrt{4\pi^2n^2+x^2}-2n\pi)^3 \frac 1{3!}\right)+o(1/n)\\
&=\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)-\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)^3\frac{1}{n^23!}+o(1/n)
\end{align} \begin{align}|f_n(x)-f(x)|&\le \left|\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)-\frac {x^2}{4\pi}\right|+\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)^3\frac{1}{n^23!}+o(1/n)\\
&\le a^2\left(\frac 1{4\pi}-\frac 1{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)+o(1/n)\\
&\le a^2\left( \frac 1{4\pi}-\frac 1{\sqrt{4\pi^2+(a/n)^2}+2\pi}\right)+o(1/n)\end{align} y_n\to 0 (1): f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}-2n\pi)=n\sin t= n(t-\frac {t^3}{3!}) +nR(t), t=\sqrt{4\pi^2n^2+x^2}-2n\pi=\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\le \frac {a^2}{4n\pi} R(t) (2): |R(t)|\le t^5\sum_{k=1}^\infty \frac{t^{2k-2}}{(2k+3)!}\le \frac{a^{10}}{{4\pi}^{5}n^5}\color{blue}{\sum_{k=1}^\infty \frac{a^{4k-4}}{(4n\pi)^{2k-2}(2k+3)!}}=M\frac{a^{10}}{{4\pi}^{5}n^5} M>0 |n R(t)|\le M\frac{a^{10}}{{4\pi}^{5}n^4} R(t)=o(1/n)","['real-analysis', 'sequences-and-series', 'limits', 'solution-verification', 'uniform-convergence']"
64,Find the limit $\lim _{x\to 0}\frac{\cos x-1+\frac{x}{2}\cdot \sin x}{\ln ^4(x+1)}$,Find the limit,\lim _{x\to 0}\frac{\cos x-1+\frac{x}{2}\cdot \sin x}{\ln ^4(x+1)},"I need to find $\displaystyle \lim _{x\to 0}\frac{\cos x-1+\frac{x}{2}\cdot \sin x}{\ln ^4\left(x+1\right)}$ . I tried using the following: \begin{align*} \ln(1+x)&\approx x,\\ \sin(x)&\approx x-\frac{x^3}{2},\\ \cos(x)&\approx 1-\frac{x^2}{2!}+\frac{x^4}{4!}, \end{align*} I managed to get $\displaystyle \lim _{x\to 0}\frac{\cos x-1+\dfrac{x}{2}\cdot \sin x}{\ln ^4\left(x+1\right)}=\lim _{x\to 0}\frac{\left(\dfrac{\left(-\frac{x^{2}}{2!}+\frac{x^{4}}{4!}\right)}{x^{4}}+\dfrac{x^{2}-\frac{x^{4}}{3!}}{2x^{4}}\right)}{\left(\dfrac{\ln\left(1+x\right)-x}{x}+1\right)^{4}}$ but I can't see how to use  that $\displaystyle \lim _{x\to 0}\frac{\ln\left(1+x\right)-x}{x}=0$ in here. Am I missing something? I didn't learn the little- $\mathcal{O}$ notation yet. Thanks",I need to find . I tried using the following: I managed to get but I can't see how to use  that in here. Am I missing something? I didn't learn the little- notation yet. Thanks,"\displaystyle \lim _{x\to 0}\frac{\cos x-1+\frac{x}{2}\cdot \sin x}{\ln ^4\left(x+1\right)} \begin{align*}
\ln(1+x)&\approx x,\\
\sin(x)&\approx x-\frac{x^3}{2},\\
\cos(x)&\approx 1-\frac{x^2}{2!}+\frac{x^4}{4!},
\end{align*} \displaystyle \lim _{x\to 0}\frac{\cos x-1+\dfrac{x}{2}\cdot \sin x}{\ln ^4\left(x+1\right)}=\lim _{x\to 0}\frac{\left(\dfrac{\left(-\frac{x^{2}}{2!}+\frac{x^{4}}{4!}\right)}{x^{4}}+\dfrac{x^{2}-\frac{x^{4}}{3!}}{2x^{4}}\right)}{\left(\dfrac{\ln\left(1+x\right)-x}{x}+1\right)^{4}} \displaystyle \lim _{x\to 0}\frac{\ln\left(1+x\right)-x}{x}=0 \mathcal{O}","['calculus', 'limits']"
65,Convergence of sum of MLE's over all possible $0/1$ sequences,Convergence of sum of MLE's over all possible  sequences,0/1,"Fix $N\in\mathbb N$ and take $\Theta=\{\frac1{N+1},\ldots,\frac N{N+1}\}$ . For $y^T\in\{0,1\}^T$ , let $\hat\theta(y^T)$ be the number in $\Theta$ that is closest to $\frac1T\sum_{i=1}^Ty^T_i$ . Let $\sum_{i=1}^Ty^T_i=:k(y^T)$ , and let $p_\theta(y^T)=\theta^{k(y^T)}(1-\theta)^{T-k(y^T)}$ . In my lecture notes I read that $$\sum_{y^T\in\{0,1\}^T}p_{\hat{\theta}(y^T)}(y^T)\to N,$$ as $T\to\infty$ , by the law of large numbers. I was not able to figure out why this was the case, exactly. My first problem is that I don't even see how the LLN comes into play here.  Any help on this is much appreciated.","Fix and take . For , let be the number in that is closest to . Let , and let . In my lecture notes I read that as , by the law of large numbers. I was not able to figure out why this was the case, exactly. My first problem is that I don't even see how the LLN comes into play here.  Any help on this is much appreciated.","N\in\mathbb N \Theta=\{\frac1{N+1},\ldots,\frac N{N+1}\} y^T\in\{0,1\}^T \hat\theta(y^T) \Theta \frac1T\sum_{i=1}^Ty^T_i \sum_{i=1}^Ty^T_i=:k(y^T) p_\theta(y^T)=\theta^{k(y^T)}(1-\theta)^{T-k(y^T)} \sum_{y^T\in\{0,1\}^T}p_{\hat{\theta}(y^T)}(y^T)\to N, T\to\infty","['probability', 'limits', 'statistics', 'maximum-likelihood', 'law-of-large-numbers']"
66,Doubts on asymptotic criterion for $\sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}n^{a}\tan^{-1}\bigg(\frac{1}{n^a}\bigg)-e^{1/n}$ with $a>0$.,Doubts on asymptotic criterion for  with .,\sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}n^{a}\tan^{-1}\bigg(\frac{1}{n^a}\bigg)-e^{1/n} a>0,"I have to valuate the character of the following series: $$\sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}n^{a}\tan^{-1}\bigg(\frac{1}{n^a}\bigg)-e^{1/n}$$ with $a>0$ . I have thought that definitely the sequence $a_n$ is made by constant signed terms. So I can apply the asymptotic criterion to study the series. $=n^{a}\tan^{-1}\big(\frac{1}{n^a}\big)-e^{1/n}\\ =n^{a}\big(\frac{1}{n^a}-\frac{1}{3n^{3a}}+o(\frac{1}{n^{3a+1}})\big)-1-\frac{1}{n}+o(\frac{1}{n})\\ =1-\frac{1}{3n^{2a}}+o(\frac{1}{n^{2a+1}})-1-\frac{1}{n}+o(\frac{1}{n})\\ =\frac{1}{3n^{2a}}+o(\frac{1}{n^{2a+1}})-\frac{1}{n}+o(\frac{1}{n})\\ \color{red}{=\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})}$ The red passage is right? I have thought that since $2a+1>1$ then I can put the $o(\frac{1}{n^{2a+1}})$ into $o(\frac{1}{n})$ . Now: if $a\leq 1/2$ then I have $\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})=\frac{1}{3n^{2a}}+o(\frac{1}{n})$ , so the corresponding series diverges and also the original one. if $a>1/2$ then $\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})=-\frac{1}{n}+o(\frac{1}{n})$ and again the series diverges. My overall attempt is right? Edit : I understand there are some problems in the final part with the little $o$ . Can someone help me","I have to valuate the character of the following series: with . I have thought that definitely the sequence is made by constant signed terms. So I can apply the asymptotic criterion to study the series. The red passage is right? I have thought that since then I can put the into . Now: if then I have , so the corresponding series diverges and also the original one. if then and again the series diverges. My overall attempt is right? Edit : I understand there are some problems in the final part with the little . Can someone help me","\sum_{n=1}^{\infty}a_n=\sum_{n=1}^{\infty}n^{a}\tan^{-1}\bigg(\frac{1}{n^a}\bigg)-e^{1/n} a>0 a_n =n^{a}\tan^{-1}\big(\frac{1}{n^a}\big)-e^{1/n}\\
=n^{a}\big(\frac{1}{n^a}-\frac{1}{3n^{3a}}+o(\frac{1}{n^{3a+1}})\big)-1-\frac{1}{n}+o(\frac{1}{n})\\
=1-\frac{1}{3n^{2a}}+o(\frac{1}{n^{2a+1}})-1-\frac{1}{n}+o(\frac{1}{n})\\
=\frac{1}{3n^{2a}}+o(\frac{1}{n^{2a+1}})-\frac{1}{n}+o(\frac{1}{n})\\
\color{red}{=\frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})} 2a+1>1 o(\frac{1}{n^{2a+1}}) o(\frac{1}{n}) a\leq 1/2 \frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})=\frac{1}{3n^{2a}}+o(\frac{1}{n}) a>1/2 \frac{1}{3n^{2a}}-\frac{1}{n}+o(\frac{1}{n})=-\frac{1}{n}+o(\frac{1}{n}) o","['real-analysis', 'sequences-and-series', 'limits', 'asymptotics']"
67,How do we know when direct substitution is the proper approach for a limit?,How do we know when direct substitution is the proper approach for a limit?,,"In the equation $y=\frac{(x+2)(x-2)}{x+2}$ , the limit as $\lim_{x→-2}$ is $-4$ . However, direct substitution would give a result of ""Does not exist"". Given that direct substitution can variously be either appropriate for identifying a limit, or correct in saying that the limit does not exist, or (as in this case) incorrect in saying that the limit does not exist, how do we know if direct substitution should be used? My best guess is that a graphical analysis is the typical first step.","In the equation , the limit as is . However, direct substitution would give a result of ""Does not exist"". Given that direct substitution can variously be either appropriate for identifying a limit, or correct in saying that the limit does not exist, or (as in this case) incorrect in saying that the limit does not exist, how do we know if direct substitution should be used? My best guess is that a graphical analysis is the typical first step.",y=\frac{(x+2)(x-2)}{x+2} \lim_{x→-2} -4,"['calculus', 'limits']"
68,Showing that this particular sequence of functions defined by recursion is point-wise bounded,Showing that this particular sequence of functions defined by recursion is point-wise bounded,,"Let $(s_n)_n$ be a summable sequence. Consider the following sequence of functions $(z_n)_{n\geq 0}$ , $z_n:[0,+\infty)\rightarrow\mathbb{R}$ , defined recursively as \begin{align*} z_0(x)&\equiv 0\\ z_1(x)&=x\\ z_{n+1}(x)&=\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg) \end{align*} It is sufficient for my purposes to show that such sequence is pointwise bounded. Since it is pointwise non-negative (by induction) with an upper bound I would conclude. Notice that I don't need a uniform bound (which in fact does not exist in my opinion), instead I need to show that \begin{equation*} \forall \,x\geq 0\quad \exists\quad L=L(x)>0\quad\text{s.t.} \quad\lim_{n\to+\infty} z_n(x)<L. \end{equation*} Obviously every map of the sequence is non-decreasing, as one can check by induction. First I tried some numerical calculation in order to explore its behavior (I still didn't know at the time if it was bounded or not) and it seems that it admits a pointwise limit to an unbounded function. Here the graph . In this graph on the x-axis we have the integer value $n$ of $z_n$ while on the y-axis are plotted the values of the $z_n(x)$ for several values of $x$ (in this case the values are $1,\dots,10$ ). As you can see each ""orbit"" seems to approach a limit value. The sequence $(s_n)_n$ is actually defined $s_n=2^{-\frac{2}{3}\beta(n+1)}$ for $\beta>0$ (in the graph $\beta=0.7$ ). I want to prove the result for this specific sequence (and for every value of $\beta>0$ ) but I'm confident that it holds for every summable sequence $s_n$ . Of course the value of the limit depends on $s_n$ , and in particular for small values of $\beta$ the convergence is slow, but I still observed it numerically (in this case the limit has a big value). I tried different approaches, e.g. evaluating the difference \begin{align*} |z_{n+1}(x)-z_{n}(x)|=\left|\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)-\sqrt{z_{n-1}(x)}\bigg(\sqrt{z_{n-2}(x)}+s_{n-1}\bigg)\right| \end{align*} but I can't do any progress in this direction. I also tried to take the logarithm and see if some tricks are possible \begin{equation*} \log z_{n+1}=\frac{1}{2} \log z_n(x)+\log(\sqrt{z_{n-1}(x)}+s_n) \end{equation*} but my attempts were unsuccessful. Also computed the ratio, which for $n$ sufficiently large, neglecting $s_n$ , we can approximate as \begin{equation*} \frac{z_{n+1}(x)}{z_n(x)}=\frac{\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)}{\sqrt{z_{n-1}(x)}\bigg(\sqrt{z_{n-2}(x)}+s_{n-1}\bigg)}\approx \sqrt\frac{z_{n}(x)}{z_{n-2}(x)} \end{equation*} but nothing comes to my mind at this point. I need to formalize this result in order to prove the existence of so called self-similar solutions for a dyadic model of turbulence, which is the topic of my master thesis. Thanks to anyone who can give any suggestions or ideas.","Let be a summable sequence. Consider the following sequence of functions , , defined recursively as It is sufficient for my purposes to show that such sequence is pointwise bounded. Since it is pointwise non-negative (by induction) with an upper bound I would conclude. Notice that I don't need a uniform bound (which in fact does not exist in my opinion), instead I need to show that Obviously every map of the sequence is non-decreasing, as one can check by induction. First I tried some numerical calculation in order to explore its behavior (I still didn't know at the time if it was bounded or not) and it seems that it admits a pointwise limit to an unbounded function. Here the graph . In this graph on the x-axis we have the integer value of while on the y-axis are plotted the values of the for several values of (in this case the values are ). As you can see each ""orbit"" seems to approach a limit value. The sequence is actually defined for (in the graph ). I want to prove the result for this specific sequence (and for every value of ) but I'm confident that it holds for every summable sequence . Of course the value of the limit depends on , and in particular for small values of the convergence is slow, but I still observed it numerically (in this case the limit has a big value). I tried different approaches, e.g. evaluating the difference but I can't do any progress in this direction. I also tried to take the logarithm and see if some tricks are possible but my attempts were unsuccessful. Also computed the ratio, which for sufficiently large, neglecting , we can approximate as but nothing comes to my mind at this point. I need to formalize this result in order to prove the existence of so called self-similar solutions for a dyadic model of turbulence, which is the topic of my master thesis. Thanks to anyone who can give any suggestions or ideas.","(s_n)_n (z_n)_{n\geq 0} z_n:[0,+\infty)\rightarrow\mathbb{R} \begin{align*}
z_0(x)&\equiv 0\\
z_1(x)&=x\\
z_{n+1}(x)&=\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)
\end{align*} \begin{equation*}
\forall \,x\geq 0\quad \exists\quad L=L(x)>0\quad\text{s.t.} \quad\lim_{n\to+\infty} z_n(x)<L.
\end{equation*} n z_n z_n(x) x 1,\dots,10 (s_n)_n s_n=2^{-\frac{2}{3}\beta(n+1)} \beta>0 \beta=0.7 \beta>0 s_n s_n \beta \begin{align*}
|z_{n+1}(x)-z_{n}(x)|=\left|\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)-\sqrt{z_{n-1}(x)}\bigg(\sqrt{z_{n-2}(x)}+s_{n-1}\bigg)\right|
\end{align*} \begin{equation*}
\log z_{n+1}=\frac{1}{2} \log z_n(x)+\log(\sqrt{z_{n-1}(x)}+s_n)
\end{equation*} n s_n \begin{equation*}
\frac{z_{n+1}(x)}{z_n(x)}=\frac{\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)}{\sqrt{z_{n-1}(x)}\bigg(\sqrt{z_{n-2}(x)}+s_{n-1}\bigg)}\approx \sqrt\frac{z_{n}(x)}{z_{n-2}(x)}
\end{equation*}","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'sequence-of-function']"
69,Proof that integration is reverse-differentiation,Proof that integration is reverse-differentiation,,"The following section is the start of a proof to show that integration is the inverse of differentiation: Consider an integral $F(x) = \int_{b}^{a} f(x) \ dx$ . Let $F(x) = \int_{a}^{x} f(u) \ du $ . It is apparent that $F(x)$ is a continuous function of $x$ and $F(x+h) = \int_{a}^{x+h} f(u) \ du = \int_{a}^{x} f(u) \ du + \int_{x}^{x+h} f(u) \ du = F(x) + \int_{x}^{x+h} f(u) \ du$ Rearranging and dividing through we get $\frac{F(x+h) - F(x)}{h} = \frac{1}{h}\int_{x}^{x+h} f(u) \ du$ . Finally, taking the limit as $h \rightarrow 0$ the LHS becomes $\frac{dF}{dx}$ and the RHS becomes $f(x)$ . I have several questions about this proof I don't understand: From line one to line two, for what substitution $u$ where $u$ is some function of $x$ is this true? The transformation from [1] to [2] is not obvious to me. How is it apparent $F(x)$ is continuous (intuitively, only the case if $f(x)$ is continuous)? The final line appears to me to hand-wave away the limit as $h \rightarrow 0$ of $\frac{1}{h}$ which should be undefined to my knowledge. In any case, it seems invalid unless $\lim_{h\rightarrow 0} \frac{1}{h} = 1$ which I find unlikely. Thanks!","The following section is the start of a proof to show that integration is the inverse of differentiation: Consider an integral . Let . It is apparent that is a continuous function of and Rearranging and dividing through we get . Finally, taking the limit as the LHS becomes and the RHS becomes . I have several questions about this proof I don't understand: From line one to line two, for what substitution where is some function of is this true? The transformation from [1] to [2] is not obvious to me. How is it apparent is continuous (intuitively, only the case if is continuous)? The final line appears to me to hand-wave away the limit as of which should be undefined to my knowledge. In any case, it seems invalid unless which I find unlikely. Thanks!",F(x) = \int_{b}^{a} f(x) \ dx F(x) = \int_{a}^{x} f(u) \ du  F(x) x F(x+h) = \int_{a}^{x+h} f(u) \ du = \int_{a}^{x} f(u) \ du + \int_{x}^{x+h} f(u) \ du = F(x) + \int_{x}^{x+h} f(u) \ du \frac{F(x+h) - F(x)}{h} = \frac{1}{h}\int_{x}^{x+h} f(u) \ du h \rightarrow 0 \frac{dF}{dx} f(x) u u x F(x) f(x) h \rightarrow 0 \frac{1}{h} \lim_{h\rightarrow 0} \frac{1}{h} = 1,"['calculus', 'limits']"
70,Evaluate the limit : $\lim_{n\to\infty} \frac{f\left(\frac{3}{2}(3+\sqrt{7})^n\right)}{g\left(\frac{1}{2}(2+\sqrt{2})^n\right)}$,Evaluate the limit :,\lim_{n\to\infty} \frac{f\left(\frac{3}{2}(3+\sqrt{7})^n\right)}{g\left(\frac{1}{2}(2+\sqrt{2})^n\right)},Let $f:R\to R$ and $g:R\to R$ be periodic functions with period $\frac{3}{2}$ and $\frac{1}{2}$ respectively such that $$\lim_{x\to 0}\frac{f(x)}{x}=1$$ and $$\lim_{x\to 0}\frac{g(x)}{x}=2$$ then evaluate the limit $$\lim_{n\to\infty} \frac{f\left(\frac{3}{2}(3+\sqrt{7})^n\right)}{g\left(\frac{1}{2}(2+\sqrt{2})^n\right)}$$ My Attempt I assumed $f(x)=\frac{3}{4\pi}\sin\left(\frac{4\pi x}{3}\right)$ and $g(x)=\frac{1}{2\pi}\sin(4\pi x)$ and got answer as $0$ . Could there be a better way to do it,Let and be periodic functions with period and respectively such that and then evaluate the limit My Attempt I assumed and and got answer as . Could there be a better way to do it,f:R\to R g:R\to R \frac{3}{2} \frac{1}{2} \lim_{x\to 0}\frac{f(x)}{x}=1 \lim_{x\to 0}\frac{g(x)}{x}=2 \lim_{n\to\infty} \frac{f\left(\frac{3}{2}(3+\sqrt{7})^n\right)}{g\left(\frac{1}{2}(2+\sqrt{2})^n\right)} f(x)=\frac{3}{4\pi}\sin\left(\frac{4\pi x}{3}\right) g(x)=\frac{1}{2\pi}\sin(4\pi x) 0,"['real-analysis', 'calculus', 'limits', 'binomial-theorem', 'periodic-functions']"
71,"$ x_{n + 1} = \sqrt {c - \sqrt {c + x_n}}$ Find $ c$ such that $ x_0$ in $ (0, c)$, $ (x_n)$ has a finite limit $ \lim x_n$ when $ n\to + \infty$","Find  such that  in ,  has a finite limit  when"," x_{n + 1} = \sqrt {c - \sqrt {c + x_n}}  c  x_0  (0, c)  (x_n)  \lim x_n  n\to + \infty","Given a real number $ c > 0$ , a sequence $ (x_n)$ of real numbers is defined by $ x_{n + 1} = \sqrt {c - \sqrt {c + x_n}}$ for $ n \ge 0$ . Find all values of $ c$ such that for each initial value $ x_0$ in $ (0, c)$ , the sequence $ (x_n)$ is defined for all $ n$ and has a finite limit $ \lim x_n$ when $ n\to + \infty$ . For $f(1)$ to be defined, then $c - \sqrt {c + x_0} \geq 0 \Rightarrow c^2-c \geq x_0$ But $x_0 \in (0;c) \Rightarrow x_0<c$ Thus $c^2-c>c \Rightarrow c(c-2)>0$ . If $c<0 \Rightarrow c - \sqrt {c + x_n} < 0$ , which is a contradiction. Therefore $c \geq 2$ . I think we will induct with $x_{n+1}$ to have $ (x_n)$ is defined for all $ n$ by finding the range of $x_n$ . $\textbf{Definition.}$ The function $f : D \rightarrow D$ is called Lipschitz function on $D$ if there is exist $q \in \mathbb{R}, 0 < q < 1$ such that $|f(x) − f(y)| ≤ q|x − y| \Leftrightarrow |f'(x)| \leq q$ for all $x$ and $y$ in $D$ . $\textbf{Theorem.}$ If $f(x)$ is Lipschitz function on $D$ , then the sequence $\{x_n\}$ defined by $x_0 = a \in D, x_{n+1} = f(x_n)$ converges (has a finite limit $ \lim x_n$ when $ n\to + \infty$ ). Then we can prove $|{f'(x)}| \leq q < 1$ for all $x \in (0;?)$ to have $(x_n)$ has a finite limit $ \lim x_n$ when $ n\to + \infty$ .","Given a real number , a sequence of real numbers is defined by for . Find all values of such that for each initial value in , the sequence is defined for all and has a finite limit when . For to be defined, then But Thus . If , which is a contradiction. Therefore . I think we will induct with to have is defined for all by finding the range of . The function is called Lipschitz function on if there is exist such that for all and in . If is Lipschitz function on , then the sequence defined by converges (has a finite limit when ). Then we can prove for all to have has a finite limit when ."," c > 0  (x_n)  x_{n + 1} = \sqrt {c - \sqrt {c + x_n}}  n \ge 0  c  x_0  (0, c)  (x_n)  n  \lim x_n  n\to + \infty f(1) c - \sqrt {c + x_0} \geq 0 \Rightarrow c^2-c \geq x_0 x_0 \in (0;c) \Rightarrow x_0<c c^2-c>c \Rightarrow c(c-2)>0 c<0 \Rightarrow c - \sqrt {c + x_n} < 0 c \geq 2 x_{n+1}  (x_n)  n x_n \textbf{Definition.} f : D \rightarrow D D q \in \mathbb{R}, 0 < q < 1 |f(x) − f(y)| ≤ q|x − y| \Leftrightarrow |f'(x)| \leq q x y D \textbf{Theorem.} f(x) D \{x_n\} x_0 = a \in D, x_{n+1} = f(x_n)  \lim x_n  n\to + \infty |{f'(x)}| \leq q < 1 x \in (0;?) (x_n)  \lim x_n  n\to + \infty","['calculus', 'sequences-and-series', 'limits', 'contest-math']"
72,Prove a limit by definition given another limit,Prove a limit by definition given another limit,,"I'm trying to prove the following statement: If $\lim_{x\rightarrow a}{f(x)\over x-a}=L$ , for some $L\neq \pm\infty$ , then $\lim_{x\rightarrow a}{f(x)}=0$ My proof goes as follows: Since $\lim_{x\rightarrow a}{f(x)\over x-a}=L$ , by the definition: Given $\varepsilon>0, ~\exists ~\delta>0 ~||x-a|<\delta \implies \left|{f(x)\over x-a}-L\right|<\varepsilon $ Then: $\left|{f(x)\over x-a}-L\right|<\varepsilon$ $\left|{f(x)}-L(x-a)\right|<\varepsilon|x-a|$ $\left|{f(x)}-L(x-a)\right|<\varepsilon|x-a|<\varepsilon\delta$ $\left|{f(x)}-L(x-a)\right|<\varepsilon\delta$ By using the triangle inequality $|u|-|v|\leq |u-v|$ : $|{f(x)}|-|L(x-a)|\leq\left|{f(x)}-L(x-a)\right|<\varepsilon\delta$ $|{f(x)}|-|L(x-a)|<\varepsilon\delta$ $|{f(x)}|<\varepsilon\delta+|L(x-a)|$ $|{f(x)}|<\varepsilon\delta+|L(x-a)|<\varepsilon\delta+|L|\delta$ $|{f(x)}|<\varepsilon\delta+|L|\delta$ $|{f(x)}|<(\varepsilon+|L|)\delta$ Then, given $\varepsilon'>0$ , choose $\delta ={\varepsilon'\over\varepsilon+|L|}$ , we have: $|{f(x)}|<\varepsilon'$ Therefore, since $|x-a|<\delta\implies |f(x)-0|<\varepsilon'$ , then $\lim_{x\rightarrow a}{f(x)}=0$","I'm trying to prove the following statement: If , for some , then My proof goes as follows: Since , by the definition: Given Then: By using the triangle inequality : Then, given , choose , we have: Therefore, since , then","\lim_{x\rightarrow a}{f(x)\over x-a}=L L\neq \pm\infty \lim_{x\rightarrow a}{f(x)}=0 \lim_{x\rightarrow a}{f(x)\over x-a}=L \varepsilon>0, ~\exists ~\delta>0 ~||x-a|<\delta \implies \left|{f(x)\over x-a}-L\right|<\varepsilon  \left|{f(x)\over x-a}-L\right|<\varepsilon \left|{f(x)}-L(x-a)\right|<\varepsilon|x-a| \left|{f(x)}-L(x-a)\right|<\varepsilon|x-a|<\varepsilon\delta \left|{f(x)}-L(x-a)\right|<\varepsilon\delta |u|-|v|\leq |u-v| |{f(x)}|-|L(x-a)|\leq\left|{f(x)}-L(x-a)\right|<\varepsilon\delta |{f(x)}|-|L(x-a)|<\varepsilon\delta |{f(x)}|<\varepsilon\delta+|L(x-a)| |{f(x)}|<\varepsilon\delta+|L(x-a)|<\varepsilon\delta+|L|\delta |{f(x)}|<\varepsilon\delta+|L|\delta |{f(x)}|<(\varepsilon+|L|)\delta \varepsilon'>0 \delta ={\varepsilon'\over\varepsilon+|L|} |{f(x)}|<\varepsilon' |x-a|<\delta\implies |f(x)-0|<\varepsilon' \lim_{x\rightarrow a}{f(x)}=0","['calculus', 'limits']"
73,"Proof verification: if $a_n, b_n>0$ and $\lim\limits_{n \to\infty} \frac{a_n}{b_n}=L_1$ with $L_1>0$, then if $\sum a_n$ converges, so does $\sum b_n$","Proof verification: if  and  with , then if  converges, so does","a_n, b_n>0 \lim\limits_{n \to\infty} \frac{a_n}{b_n}=L_1 L_1>0 \sum a_n \sum b_n","I'm trying a proof technique I'm not used to for limits on fractions, which attempts to avoid an epsilon-delta approach similarly to how the single variable chain rule is proved in baby Rudin, and I was wondering if it works. Any help or tips are very welcome! Statement: If $a_n, b_n>0$ and $\lim \limits_{n \to \infty} \frac{a_n}{b_n} = L_1$ with $L_1 > 0$ , then if $\sum_{n \in \mathbb N} a_n$ converges, so does $\sum_{n \in \Bbb N} b_n$ Proof: Suppose $\sum_{n \in \Bbb N} a_n$ converges to $L_a$ . Since $\lim \limits_{n \to \infty} \frac{a_n}{b_n} = L_1$ , we have $$a_n = b_n(L_1 + \varepsilon(n))$$ with $\varepsilon(n) \to 0$ as $n \to \infty$ . Therefore, $$ \sum_{n \in \Bbb N} a_n = \sum_{n \in \Bbb N} b_n(L_1 + \varepsilon(n)) = L_a, $$ so $$\lim \limits_{n \to \infty} \sum_{i=1}^n b_i(L_1 + \varepsilon(n)) = L_a,$$ and therefore, $$\sum_{i=1}^nb_i=\frac{L_a+\mu(n)}{L_1 + \varepsilon(n) } ,$$ where $\mu(n) \to 0$ as $n \to \infty$ . Therefore, letting $n \to \infty$ results in $\sum_{i=1}^nb_i = \frac{L_a}{L_1}$ , so $\sum_{n \in \mathbb N}b_n$ converges. $\blacksquare$","I'm trying a proof technique I'm not used to for limits on fractions, which attempts to avoid an epsilon-delta approach similarly to how the single variable chain rule is proved in baby Rudin, and I was wondering if it works. Any help or tips are very welcome! Statement: If and with , then if converges, so does Proof: Suppose converges to . Since , we have with as . Therefore, so and therefore, where as . Therefore, letting results in , so converges.","a_n, b_n>0 \lim \limits_{n \to \infty} \frac{a_n}{b_n} = L_1 L_1 > 0 \sum_{n \in \mathbb N} a_n \sum_{n \in \Bbb N} b_n \sum_{n \in \Bbb N} a_n L_a \lim \limits_{n \to \infty} \frac{a_n}{b_n} = L_1 a_n = b_n(L_1 + \varepsilon(n)) \varepsilon(n) \to 0 n \to \infty  \sum_{n \in \Bbb N} a_n = \sum_{n \in \Bbb N} b_n(L_1 + \varepsilon(n)) = L_a,  \lim \limits_{n \to \infty} \sum_{i=1}^n b_i(L_1 + \varepsilon(n)) = L_a, \sum_{i=1}^nb_i=\frac{L_a+\mu(n)}{L_1 + \varepsilon(n) } , \mu(n) \to 0 n \to \infty n \to \infty \sum_{i=1}^nb_i = \frac{L_a}{L_1} \sum_{n \in \mathbb N}b_n \blacksquare","['real-analysis', 'limits', 'summation', 'solution-verification']"
74,"find a nice smooth function $\lim_{x\to\infty} (f(x^2)-f(x))=1$, $\lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1$ and $\lim_{x\to-\infty} (f(x)-f(x^2))=1$?","find a nice smooth function ,  and ?",\lim_{x\to\infty} (f(x^2)-f(x))=1 \lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1 \lim_{x\to-\infty} (f(x)-f(x^2))=1,"can you find a nice real and smooth function where $\lim_{x\to\infty} (f(x^2)-f(x))=1$ , $\lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1$ and $\lim_{x\to-\infty} (f(x)-f(x^2))=1$ ? I've figured a function that follows the first limit. $$\lim_{x\to\infty} (f(x^2)-f(x))=1$$ I first tried logarithms and they didn't work. but it seemed close. $$\ln(x^2)-\ln(x)=\ln(x)$$ So next I tried logarithms of logarithms. and got $\ln(2)$ so I divided my function by $\ln(2)$ . $$\frac{\ln(\ln(x^2))}{\ln(2)}-\frac{\ln(\ln(x))}{\ln(2)}=1$$ because for all x this is true $\frac{\ln(\ln(x))}{\ln(2)}$ is a solution if we ignore the other two limits, the vertical asymptote at 1 and not being defined for all real numbers. $$\lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1$$ The second limit I also found a solution for which is $f(x)=x^n$ where n is any real number because $x^n\times\frac{1}{x}^n=1$ $$\lim_{x\to-\infty} (f(x)-f(x^2))=1$$ for the last one I made sure to use the first on and reordered the terms to make it work and got $-\frac{\ln(\ln(-x))}{\ln(2)}$ . $$-\frac{\ln(\ln(-x))}{\ln(2)}+\frac{\ln(\ln(-x^2))}{\ln(2)}=1$$ even though I could find an example of a function that followed on of the limits, none of them followed all three limits and two of them weren't even defined for all real numbers. So can you find a function that follows all three of these limits, whose domain and range are the real numbers, and are smooth meaning to me continuous and differentiable everywhere.","can you find a nice real and smooth function where , and ? I've figured a function that follows the first limit. I first tried logarithms and they didn't work. but it seemed close. So next I tried logarithms of logarithms. and got so I divided my function by . because for all x this is true is a solution if we ignore the other two limits, the vertical asymptote at 1 and not being defined for all real numbers. The second limit I also found a solution for which is where n is any real number because for the last one I made sure to use the first on and reordered the terms to make it work and got . even though I could find an example of a function that followed on of the limits, none of them followed all three limits and two of them weren't even defined for all real numbers. So can you find a function that follows all three of these limits, whose domain and range are the real numbers, and are smooth meaning to me continuous and differentiable everywhere.",\lim_{x\to\infty} (f(x^2)-f(x))=1 \lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1 \lim_{x\to-\infty} (f(x)-f(x^2))=1 \lim_{x\to\infty} (f(x^2)-f(x))=1 \ln(x^2)-\ln(x)=\ln(x) \ln(2) \ln(2) \frac{\ln(\ln(x^2))}{\ln(2)}-\frac{\ln(\ln(x))}{\ln(2)}=1 \frac{\ln(\ln(x))}{\ln(2)} \lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1 f(x)=x^n x^n\times\frac{1}{x}^n=1 \lim_{x\to-\infty} (f(x)-f(x^2))=1 -\frac{\ln(\ln(-x))}{\ln(2)} -\frac{\ln(\ln(-x))}{\ln(2)}+\frac{\ln(\ln(-x^2))}{\ln(2)}=1,"['real-analysis', 'calculus', 'limits', 'continuity']"
75,Problem of the limits related to a sequence given by relation $a_{n+1}=a_n+a_{n}^{-\frac{1}{k}}$,Problem of the limits related to a sequence given by relation,a_{n+1}=a_n+a_{n}^{-\frac{1}{k}},"Here's the problem: $k>1,k\in\mathbb{N}.$ Given $\ a_0>0,\forall n\in \mathbb{N},a_{n+1}=a_n+a_{n}^{-\frac{1}{k}}.$ Figure out $\lim\limits_{n\to \infty}\frac{(a_n)^{k+1}}{n^k}.$ I have no idea of dealing with the relation.It makes sense to say $a_n$ grows rather slowly,every time by $-\frac{1}{k}$ times of itself.Thus $k+1$ times makes $a_n$ grows like what $n^k$ does.But how to make it to the ground?Are there any techniques to deal with such relations?Anything about it would be highly appreciated.","Here's the problem: Given Figure out I have no idea of dealing with the relation.It makes sense to say grows rather slowly,every time by times of itself.Thus times makes grows like what does.But how to make it to the ground?Are there any techniques to deal with such relations?Anything about it would be highly appreciated.","k>1,k\in\mathbb{N}. \ a_0>0,\forall n\in \mathbb{N},a_{n+1}=a_n+a_{n}^{-\frac{1}{k}}. \lim\limits_{n\to \infty}\frac{(a_n)^{k+1}}{n^k}. a_n -\frac{1}{k} k+1 a_n n^k","['real-analysis', 'sequences-and-series', 'limits', 'analysis']"
76,Ambiguity of an equality of two limits,Ambiguity of an equality of two limits,,"It is often asked to prove an equality about two limits, say, $\lim_{x\to a}f\left(x\right)=\lim_{x\to a}g\left(x\right)$ . But I'm confused by the interpretation of the equality, because it seems that it has two interpretations: $f$ approaches $\lim_{x\to a}g\left(x\right)$ as $x$ approaches $a$ , or $g$ approaches $\lim_{x\to a}f\left(x\right)$ as $x$ approaches $a$ . But many proofs of this kind of equalities seem to only prove either one of them. For instance, if the interpretation (2) is chosen, the proof is like: let $l=\lim_{x\to a}f\left(x\right)$ then prove $l=\lim_{x\to a}g\left(x\right)$ . Why is it possible that the proof can be valid considering only one of the interpretations?","It is often asked to prove an equality about two limits, say, . But I'm confused by the interpretation of the equality, because it seems that it has two interpretations: approaches as approaches , or approaches as approaches . But many proofs of this kind of equalities seem to only prove either one of them. For instance, if the interpretation (2) is chosen, the proof is like: let then prove . Why is it possible that the proof can be valid considering only one of the interpretations?",\lim_{x\to a}f\left(x\right)=\lim_{x\to a}g\left(x\right) f \lim_{x\to a}g\left(x\right) x a g \lim_{x\to a}f\left(x\right) x a l=\lim_{x\to a}f\left(x\right) l=\lim_{x\to a}g\left(x\right),"['calculus', 'limits']"
77,On the matter of removing discontinuities.,On the matter of removing discontinuities.,,"I have been asked to determine whether the function $f(x) = \frac{\sin{x}}{x}$ is continuous on $\mathbb{R}$ . I do understand that the function in continuous everywhere except for the point $x=0$ , where the function is undefined (the $\frac{0}{0}  $ indeterminacy). I do, however, remember what the textbook told me about removable discontinuities, i.e. that whenever a function possesses a definite limit as its argument approaches a value outside of the function’s domain, we are (somehow) given the right to assign the value of that limit to be the value of the function at that point. The question is why in the world such a feint is even plausible? Of course, we can say that the “modified” function with its discontinuity removed is a whole new function, which possesses a domain different from the initial function, that is perfectly fine, but as far as I understand, we regard such functions in their initial form as the ones in which the discontinuities were already removed? I know that it might be too many words for a question to be even comprehensible, so I am going to provide an example using the function I mentioned already: $f(x) = \frac{\sin{x}}{x}$ . We know that it is undefined at $x=0$ , however we can remove the discontinuity at that point (for the limit is equal to $1$ as $x$ approaches $0$ ), and thus make it continuous on $\mathbb{R}$ . Do we still regard the modified function as $f(x) = \frac{\sin{x}}{x}$ , or do we specifically mention that it is a different function, possessing a distinct value at $x = 0$ ?","I have been asked to determine whether the function is continuous on . I do understand that the function in continuous everywhere except for the point , where the function is undefined (the indeterminacy). I do, however, remember what the textbook told me about removable discontinuities, i.e. that whenever a function possesses a definite limit as its argument approaches a value outside of the function’s domain, we are (somehow) given the right to assign the value of that limit to be the value of the function at that point. The question is why in the world such a feint is even plausible? Of course, we can say that the “modified” function with its discontinuity removed is a whole new function, which possesses a domain different from the initial function, that is perfectly fine, but as far as I understand, we regard such functions in their initial form as the ones in which the discontinuities were already removed? I know that it might be too many words for a question to be even comprehensible, so I am going to provide an example using the function I mentioned already: . We know that it is undefined at , however we can remove the discontinuity at that point (for the limit is equal to as approaches ), and thus make it continuous on . Do we still regard the modified function as , or do we specifically mention that it is a different function, possessing a distinct value at ?","f(x) = \frac{\sin{x}}{x} \mathbb{R} x=0 \frac{0}{0} 
 f(x) = \frac{\sin{x}}{x} x=0 1 x 0 \mathbb{R} f(x) = \frac{\sin{x}}{x} x = 0","['real-analysis', 'calculus', 'limits', 'analysis', 'continuity']"
78,$\Bbb P( \lim_{n\to\infty} X_n=\beta )=1 \implies \lim_{n\to\infty}\Bbb P( X_n>\alpha )=1 \quad \forall \alpha < \beta$? (Proof verification),? (Proof verification),\Bbb P( \lim_{n\to\infty} X_n=\beta )=1 \implies \lim_{n\to\infty}\Bbb P( X_n>\alpha )=1 \quad \forall \alpha < \beta,"I believe the following claim is true. I'd be grateful for a check of the proof I provide below. Claim: Let $(X_n)$ be a sequence of random variables on a shared probability space. Then $$ \mathbb P(\lim_{n \to \infty} X_n = \beta) = 1 \implies \lim_{n \to \infty} \mathbb P(X_n > \alpha ) = 1 \quad \forall \alpha < \beta. $$ Note: The interesting thing here is how we can move the limit. Proof: Let $\alpha < \beta$ . Then $$ \begin{align} 1 &= \mathbb P(\lim_{n \to \infty} X_n = \beta) \\ &\leq \mathbb P(\liminf_{n \to \infty} X_n = \beta) \\ &\leq \mathbb P( \liminf_{n \to \infty} X_n \geq \beta) = \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \\ &\leq 1. \end{align} $$ Thus, we have $1 = \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta)$ . Claim *: $\{\lim_{n \to \infty} \inf_{k \geq n} X_k \geq \beta\} \subset \lim_{n \to \infty} \underbrace{\{X_k > \beta - \varepsilon \quad \forall k \geq n \}}_{=: A_n^{\beta - \varepsilon}}$ for all $\varepsilon > 0$ . Proof of Claim *: Let $\varepsilon > 0$ . Let $\omega \in \{\lim_{n \to    \infty} \inf_{k \geq n} X_k \geq \beta\}$ , i.e. $\lim_{n \to   \infty} \inf_{k \geq n} X_k(\omega) \geq \beta$ . Then $\exists N_\varepsilon \in \mathbb N : \forall n \geq   N_\varepsilon : \inf_{k \geq n} X_k(\omega) > \beta - \varepsilon/2$ . In particular, $X_k > \beta - \varepsilon$ for all $k \geq   N_\varepsilon$ , so $\omega \in A_{N_\varepsilon}^{\beta -   \varepsilon}$ . Since $(A_n^{\beta - \varepsilon})$ is in increasing in $n$ , we know $\omega \in \lim_{n \to \infty} A_n^{\beta - \varepsilon}$ . $\square$ We set $\varepsilon := \beta - \alpha > 0$ , so that $\beta - \varepsilon = \alpha$ . Then $$ \begin{align} 1 &= \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \\ &\leq \mathbb P( \lim_{n \to \infty} \underbrace{ \{ X_k > \alpha \quad \forall k \geq n \} }_{=: A_n^\alpha}) &&\text{(by }\textit{Claim*}\text{ with } \varepsilon = \beta - \alpha >0 \text{)}\\ &= \lim_{n \to \infty} \mathbb P(X_k > \alpha \quad \forall k \geq n) &&\text{(since } A_n^\alpha \uparrow \lim_{n \to \infty} A_n^\alpha \text{ and } \mathbb P \text{ monotonically cont.)}\\ &\leq \lim_{n \to \infty} \mathbb P(X_n > \alpha) \\ &\leq 1 \end{align} $$ Therefore, our our chain of inequalities is, in fact, a chain of equalities. In particular, $\lim_{n \to \infty} \mathbb P(X_n > \alpha) = 1$ . $\blacksquare$","I believe the following claim is true. I'd be grateful for a check of the proof I provide below. Claim: Let be a sequence of random variables on a shared probability space. Then Note: The interesting thing here is how we can move the limit. Proof: Let . Then Thus, we have . Claim *: for all . Proof of Claim *: Let . Let , i.e. . Then . In particular, for all , so . Since is in increasing in , we know . We set , so that . Then Therefore, our our chain of inequalities is, in fact, a chain of equalities. In particular, .","(X_n) 
\mathbb P(\lim_{n \to \infty} X_n = \beta) = 1 \implies \lim_{n \to \infty} \mathbb P(X_n > \alpha ) = 1 \quad \forall \alpha < \beta.
 \alpha < \beta 
\begin{align}
1 &= \mathbb P(\lim_{n \to \infty} X_n = \beta) \\
&\leq \mathbb P(\liminf_{n \to \infty} X_n = \beta) \\
&\leq \mathbb P( \liminf_{n \to \infty} X_n \geq \beta) = \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \\
&\leq 1.
\end{align}
 1 = \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \{\lim_{n \to \infty} \inf_{k \geq n} X_k \geq \beta\} \subset \lim_{n \to \infty} \underbrace{\{X_k > \beta - \varepsilon \quad \forall k \geq n \}}_{=: A_n^{\beta - \varepsilon}} \varepsilon > 0 \varepsilon > 0 \omega \in \{\lim_{n \to 
  \infty} \inf_{k \geq n} X_k \geq \beta\} \lim_{n \to
  \infty} \inf_{k \geq n} X_k(\omega) \geq \beta \exists N_\varepsilon \in \mathbb N : \forall n \geq
  N_\varepsilon : \inf_{k \geq n} X_k(\omega) > \beta - \varepsilon/2 X_k > \beta - \varepsilon k \geq
  N_\varepsilon \omega \in A_{N_\varepsilon}^{\beta -
  \varepsilon} (A_n^{\beta - \varepsilon}) n \omega \in \lim_{n \to \infty} A_n^{\beta - \varepsilon} \square \varepsilon := \beta - \alpha > 0 \beta - \varepsilon = \alpha 
\begin{align}
1 &= \mathbb P(\lim_{n \to \infty} \inf_{k \geq n} X_n \geq \beta) \\
&\leq \mathbb P( \lim_{n \to \infty} \underbrace{ \{ X_k > \alpha \quad \forall k \geq n \} }_{=: A_n^\alpha}) &&\text{(by }\textit{Claim*}\text{ with } \varepsilon = \beta - \alpha >0 \text{)}\\
&= \lim_{n \to \infty} \mathbb P(X_k > \alpha \quad \forall k \geq n) &&\text{(since } A_n^\alpha \uparrow \lim_{n \to \infty} A_n^\alpha \text{ and } \mathbb P \text{ monotonically cont.)}\\
&\leq \lim_{n \to \infty} \mathbb P(X_n > \alpha) \\
&\leq 1
\end{align}
 \lim_{n \to \infty} \mathbb P(X_n > \alpha) = 1 \blacksquare","['probability', 'limits', 'solution-verification', 'probability-limit-theorems']"
79,"Let $a_n=(1+\frac{1}{n})^n$, show that $\lim_{n\to\infty}n(e-a_n)\geq \frac e2$.","Let , show that .",a_n=(1+\frac{1}{n})^n \lim_{n\to\infty}n(e-a_n)\geq \frac e2,"Let $a_n=(1+\frac{1}{n})^n$ , show that $$\lim_{n\to\infty}n(e-a_n)\geq \frac e2.$$ We know easily by calculus that $\lim_{n\to\infty}n(e-a_n)=\frac e2$ . However, can we give an easier proof without using functional limit, but only the following prosition. if $a_n>b_n$ for large $n$ , then $\lim a_n\geq \lim b_n$ . What I should is just consider $n(a_{n^2}-a_n)$ , and show it is $\geq \frac e2$ . But is seems not easy to compare $a_{n^2}$ and $a_n$ .","Let , show that We know easily by calculus that . However, can we give an easier proof without using functional limit, but only the following prosition. if for large , then . What I should is just consider , and show it is . But is seems not easy to compare and .",a_n=(1+\frac{1}{n})^n \lim_{n\to\infty}n(e-a_n)\geq \frac e2. \lim_{n\to\infty}n(e-a_n)=\frac e2 a_n>b_n n \lim a_n\geq \lim b_n n(a_{n^2}-a_n) \geq \frac e2 a_{n^2} a_n,"['real-analysis', 'calculus', 'limits', 'exponential-function']"
80,"Constructive proof of: If $\lim_{x \to a}(f \cdot g) (x) = L$ and $\lim_{x \to a}f (x) = \frac{L}{K}$ for $K \neq 0$, then $\lim_{x \to a}g (x)=K$.","Constructive proof of: If  and  for , then .",\lim_{x \to a}(f \cdot g) (x) = L \lim_{x \to a}f (x) = \frac{L}{K} K \neq 0 \lim_{x \to a}g (x)=K,"To improve my comfort with $\delta-\epsilon$ proofs, I aim to constructively prove the following claim: If $\displaystyle \lim_{x \to a}(f \cdot g) (x) = L$ and $\displaystyle \lim_{x \to a}f (x) = \frac{L}{K}$ for $K \neq 0$ , then $\displaystyle \lim_{x \to a}g (x)=K$ . I tried some new tricks, so I just wanted to make sure that everything looked okay. Useful Lemma $\dagger$ : $\left|f(x)g(x)-\frac{L}{K}K \right|=\left| \left[f(x)-\frac{L}{K} \right]\left[g(x)-K\right]+ K\left[f(x)-\frac{L}{K} \right]+ \frac{L}{K}\left[g(x)-K \right] \right|$ By assumption, we know the following statements: $$\forall \varepsilon_{f\cdot g}\gt 0 \ \exists\delta_{f\cdot g} \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta_{f\cdot g} \rightarrow \left|f(x)g(x)-L\right| \lt \varepsilon_{f\cdot g}\right]$$ $$\forall \varepsilon_{f}\gt 0 \ \exists\delta_{f} \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta_{f} \rightarrow \left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f}\right]$$ We want to prove that: $$\forall \varepsilon\gt 0 \ \exists\delta \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta \rightarrow \left|g(x)-K\right| \lt \varepsilon\right]$$ Choose the following error terms: $$\varepsilon_{f\cdot g}=\frac{\varepsilon |L|}{2\cdot2|K|}$$ $$\text{For } \varepsilon_1=\frac{\varepsilon |L|}{2|K|\cdot2|K|} \text {and for } \varepsilon_2=\frac{L}{2|K|} \text{, let }\varepsilon_{f}=\min(\varepsilon_1,\varepsilon_2)$$ For each error term, we have a corresponding $\delta_{f\cdot g}$ and $\delta_{f}$ . Let $\delta = \min(\delta_{f\cdot g},\delta_{f})$ . Let $x$ be an arbitrary element that satisfies $0 \lt |x-a| \lt \delta$ . If the above conditions are satisfied, we can make the below argument: First, note that $L$ can be rewritten as $\frac{L}{K}K$ . By $\dagger$ and our first assumption, we have: \begin{align} \left|f(x)g(x)-\frac{L}{K}K \right|&=\left| \left[f(x)-\frac{L}{K} \right]\left[g(x)-K\right]+ K\left[f(x)-\frac{L}{K} \right]+ \frac{L}{K}\left[g(x)-K \right] \right| \\ &=\left|\left[g(x)-K \right]\cdot\left[\left(f(x)-\frac{L}{K} \right)+\frac{L}{K} \right]+ K\left[f(x)-\frac{L}{K} \right]\right| \\ &=\left|\left[g(x)-K \right]\cdot\left[f(x)\right]+ K\left[f(x)-\frac{L}{K} \right]\right| \lt \varepsilon_{f\cdot g}  \\ \end{align} Applying the reverse triangle inequality, we then have: \begin{align} \left|\left[g(x)-K \right]\cdot\left[f(x)\right]\right|-\left|K\left[f(x)-\frac{L}{K} \right]\right|=|g(x)-K||f(x)|-|K|\left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f\cdot g}\\ \end{align} Then: $$|g(x)-K||f(x)| \lt \varepsilon_{f\cdot g}+ |K|\left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f\cdot g}+|K|\varepsilon_{1}$$ Dividing through by $|f(x)|$ we have: \begin{align} |g(x)-K| \lt \frac{\varepsilon_{f\cdot g}+|K|\varepsilon_{f}}{|f(x)|}&=\frac{1}{|f(x)|}\cdot \left(\frac{\varepsilon |L|}{2\cdot2|K|}+|K|\frac{\varepsilon |L|}{2|K|\cdot2|K|} \right)\\ &=\frac{1}{|f(x)|}\cdot \left ( \frac{|L|}{2|K|}\right) \left( \frac{\varepsilon}{2}+\frac{\varepsilon}{2}\right)\\ &=\frac{\varepsilon\left(\frac{|L|}{2|K|}\right)}{|f(x)|} \end{align} To complete this proof, we need to show that $$\frac{\frac{|L|}{2|K|}}{|f(x)|}\lt 1$$ From our definition of $\varepsilon_2$ , we know that $\left|f(x)-\frac{L}{K} \right|=\left|\frac{L}{K}-f(x) \right| \lt \frac{|L|}{2|K|}$ . Applying the reverse triangle inequality, we have: $$\frac{|L|}{|K|}-|f(x)|\lt\frac{|L|}{2|K|} \implies \frac{|L|}{2|K|} \lt |f(x)| \implies \frac{\frac{|L|}{2|K|}}{|f(x)|}\lt 1 \implies \varepsilon \cdot \frac{\frac{|L|}{2|K|}}{|f(x)|} \lt \varepsilon$$ Therefore, we can conclude that $|g(x)-K| \lt \varepsilon$ so long as $0 \lt |x-a| \lt \delta$ . Any suggestions or easier paths (...with the exception of proving it by contradiction...) are greatly appreciated. Edit : Although it is implied by the construction of the different $\varepsilon$ 's, I see that I should have explicitly stated $L$ must take on non-zero values. Secondly, I should have explicitly demonstrated that division by $|f(x)|$ does not result in division by $0$ . Looking at $\varepsilon_2$ , you can actually show that $0\lt \frac{|L|}{2|K|} \lt |f(x)| \lt \frac{3|L|}{2|K|}$ ...so we have no issue of division by $0$ .","To improve my comfort with proofs, I aim to constructively prove the following claim: If and for , then . I tried some new tricks, so I just wanted to make sure that everything looked okay. Useful Lemma : By assumption, we know the following statements: We want to prove that: Choose the following error terms: For each error term, we have a corresponding and . Let . Let be an arbitrary element that satisfies . If the above conditions are satisfied, we can make the below argument: First, note that can be rewritten as . By and our first assumption, we have: Applying the reverse triangle inequality, we then have: Then: Dividing through by we have: To complete this proof, we need to show that From our definition of , we know that . Applying the reverse triangle inequality, we have: Therefore, we can conclude that so long as . Any suggestions or easier paths (...with the exception of proving it by contradiction...) are greatly appreciated. Edit : Although it is implied by the construction of the different 's, I see that I should have explicitly stated must take on non-zero values. Secondly, I should have explicitly demonstrated that division by does not result in division by . Looking at , you can actually show that ...so we have no issue of division by .","\delta-\epsilon \displaystyle \lim_{x \to a}(f \cdot g) (x) = L \displaystyle \lim_{x \to a}f (x) = \frac{L}{K} K \neq 0 \displaystyle \lim_{x \to a}g (x)=K \dagger \left|f(x)g(x)-\frac{L}{K}K \right|=\left| \left[f(x)-\frac{L}{K} \right]\left[g(x)-K\right]+ K\left[f(x)-\frac{L}{K} \right]+ \frac{L}{K}\left[g(x)-K \right] \right| \forall \varepsilon_{f\cdot g}\gt 0 \ \exists\delta_{f\cdot g} \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta_{f\cdot g} \rightarrow \left|f(x)g(x)-L\right| \lt \varepsilon_{f\cdot g}\right] \forall \varepsilon_{f}\gt 0 \ \exists\delta_{f} \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta_{f} \rightarrow \left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f}\right] \forall \varepsilon\gt 0 \ \exists\delta \gt 0 \text{ s.t. }\forall x\in \mathbb R \left[ 0 \lt |x-a| \lt \delta \rightarrow \left|g(x)-K\right| \lt \varepsilon\right] \varepsilon_{f\cdot g}=\frac{\varepsilon |L|}{2\cdot2|K|} \text{For } \varepsilon_1=\frac{\varepsilon |L|}{2|K|\cdot2|K|} \text {and for } \varepsilon_2=\frac{L}{2|K|} \text{, let }\varepsilon_{f}=\min(\varepsilon_1,\varepsilon_2) \delta_{f\cdot g} \delta_{f} \delta = \min(\delta_{f\cdot g},\delta_{f}) x 0 \lt |x-a| \lt \delta L \frac{L}{K}K \dagger \begin{align}
\left|f(x)g(x)-\frac{L}{K}K \right|&=\left| \left[f(x)-\frac{L}{K} \right]\left[g(x)-K\right]+ K\left[f(x)-\frac{L}{K} \right]+ \frac{L}{K}\left[g(x)-K \right] \right| \\
&=\left|\left[g(x)-K \right]\cdot\left[\left(f(x)-\frac{L}{K} \right)+\frac{L}{K} \right]+ K\left[f(x)-\frac{L}{K} \right]\right| \\
&=\left|\left[g(x)-K \right]\cdot\left[f(x)\right]+ K\left[f(x)-\frac{L}{K} \right]\right| \lt \varepsilon_{f\cdot g}  \\
\end{align} \begin{align}
\left|\left[g(x)-K \right]\cdot\left[f(x)\right]\right|-\left|K\left[f(x)-\frac{L}{K} \right]\right|=|g(x)-K||f(x)|-|K|\left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f\cdot g}\\
\end{align} |g(x)-K||f(x)| \lt \varepsilon_{f\cdot g}+ |K|\left|f(x)-\frac{L}{K}\right| \lt \varepsilon_{f\cdot g}+|K|\varepsilon_{1} |f(x)| \begin{align}
|g(x)-K| \lt \frac{\varepsilon_{f\cdot g}+|K|\varepsilon_{f}}{|f(x)|}&=\frac{1}{|f(x)|}\cdot \left(\frac{\varepsilon |L|}{2\cdot2|K|}+|K|\frac{\varepsilon |L|}{2|K|\cdot2|K|} \right)\\
&=\frac{1}{|f(x)|}\cdot \left ( \frac{|L|}{2|K|}\right) \left( \frac{\varepsilon}{2}+\frac{\varepsilon}{2}\right)\\
&=\frac{\varepsilon\left(\frac{|L|}{2|K|}\right)}{|f(x)|}
\end{align} \frac{\frac{|L|}{2|K|}}{|f(x)|}\lt 1 \varepsilon_2 \left|f(x)-\frac{L}{K} \right|=\left|\frac{L}{K}-f(x) \right| \lt \frac{|L|}{2|K|} \frac{|L|}{|K|}-|f(x)|\lt\frac{|L|}{2|K|} \implies \frac{|L|}{2|K|} \lt |f(x)| \implies \frac{\frac{|L|}{2|K|}}{|f(x)|}\lt 1 \implies \varepsilon \cdot \frac{\frac{|L|}{2|K|}}{|f(x)|} \lt \varepsilon |g(x)-K| \lt \varepsilon 0 \lt |x-a| \lt \delta \varepsilon L |f(x)| 0 \varepsilon_2 0\lt \frac{|L|}{2|K|} \lt |f(x)| \lt \frac{3|L|}{2|K|} 0","['limits', 'solution-verification', 'epsilon-delta']"
81,How to solve this limits question using the properties of modulus and exponents?,How to solve this limits question using the properties of modulus and exponents?,,"Let $f(x)=|x|^{p-2}\sin(\frac{1}{x})+x|\tan x|^{q-3} ,x\neq0$ is differentiable at $x=0$ , given $f(0)=0$ , then what can be the values of $p$ and $q$ ? My attempt: As $x$ approaches $0$ , the $\sin(\frac{1}{x})$ term approaches infinity and so for the limit to be zero $x$ has to be zero. But here I can't understand the inter-relation between value of $p$ and $x$ . If I put the value of $p=3$ and $q=3$ , the overall limit becomes $0$ , but according to the solution given, $p>3$ and $q\ge3$ . But I can't understand how this ranges came. Even tried writing taylor series of sin( $\frac{1}{x}$ ), but that also didn't help. Can anyone please explain the underlying concept in this and how can we solve this question?","Let is differentiable at , given , then what can be the values of and ? My attempt: As approaches , the term approaches infinity and so for the limit to be zero has to be zero. But here I can't understand the inter-relation between value of and . If I put the value of and , the overall limit becomes , but according to the solution given, and . But I can't understand how this ranges came. Even tried writing taylor series of sin( ), but that also didn't help. Can anyone please explain the underlying concept in this and how can we solve this question?","f(x)=|x|^{p-2}\sin(\frac{1}{x})+x|\tan x|^{q-3} ,x\neq0 x=0 f(0)=0 p q x 0 \sin(\frac{1}{x}) x p x p=3 q=3 0 p>3 q\ge3 \frac{1}{x}",['limits']
82,Extreme limit of sum of Faddeeva function,Extreme limit of sum of Faddeeva function,,"I am looking for the proof of $$ \lim_{t \to +0} \left[ \log t + 2\pi \sum_{n = 0}^{\infty} \left\{ \frac{1}{\omega_n}   - \sqrt{\pi} A t ~ \mathrm{W}(i\omega_n At)\right\} \right] = \log \frac{\mathrm{e}^{\gamma/2}}{\pi A}~, $$ with $\omega_n = (2n + 1)\pi~$ and $~A>0$ . Here, $\mathrm{W}(z)$ is the Faddeeva function given by $$ \mathrm{W}(z) = \mathrm{e}^{-z^2} \mathrm{erfc}(-iz)~. $$ The proof of this equation is troubling to me because the equation is necessary for the Eilenberger equation of superconductivity to give the value of the upper critical field at absolute zero. For example, using a function similar to the Fermi distribution function, $$ F(z) = \frac{1}{{\mathrm e}^{z} + 1} , $$ the infinite sum can be rewritten as the sum of the residues of a complex function. After the substitution $i\omega_{n} \to z$ and the analytic continuation that move $z$ on the real axis, the equation can be written as $$ 2\pi \sum_{n = 0}^{\infty} \left\{ \frac{1}{\omega_n}   - \sqrt{\pi} A t ~ \mathrm{W}(i\omega_n At)\right\} = -\int_{-\infty}^{\infty} {\mathrm d}z ~ F(z) \left\{ \frac{1}{z} + i\sqrt{\pi} A t {\mathrm W}(zAt)\right\}. $$ By using this, I was able to verify the limit value numerically. However, even in this form, it is difficult to evaluate the limit value of $t \to +0$ analytically. How can I prove the equation? I am not quite sure how to proceed. Thank you.","I am looking for the proof of with and . Here, is the Faddeeva function given by The proof of this equation is troubling to me because the equation is necessary for the Eilenberger equation of superconductivity to give the value of the upper critical field at absolute zero. For example, using a function similar to the Fermi distribution function, the infinite sum can be rewritten as the sum of the residues of a complex function. After the substitution and the analytic continuation that move on the real axis, the equation can be written as By using this, I was able to verify the limit value numerically. However, even in this form, it is difficult to evaluate the limit value of analytically. How can I prove the equation? I am not quite sure how to proceed. Thank you.","
\lim_{t \to +0} \left[ \log t + 2\pi \sum_{n = 0}^{\infty} \left\{ \frac{1}{\omega_n} 
 - \sqrt{\pi} A t ~ \mathrm{W}(i\omega_n At)\right\} \right] = \log \frac{\mathrm{e}^{\gamma/2}}{\pi A}~,
 \omega_n = (2n + 1)\pi~ ~A>0 \mathrm{W}(z) 
\mathrm{W}(z) = \mathrm{e}^{-z^2} \mathrm{erfc}(-iz)~.
 
F(z) = \frac{1}{{\mathrm e}^{z} + 1} ,
 i\omega_{n} \to z z 
2\pi \sum_{n = 0}^{\infty} \left\{ \frac{1}{\omega_n} 
 - \sqrt{\pi} A t ~ \mathrm{W}(i\omega_n At)\right\} = -\int_{-\infty}^{\infty} {\mathrm d}z ~ F(z) \left\{ \frac{1}{z} + i\sqrt{\pi} A t {\mathrm W}(zAt)\right\}.
 t \to +0","['calculus', 'limits']"
83,Two-variable limit via change of variables,Two-variable limit via change of variables,,"It often happens that by considering polar coordinates we can transform a limit of the form $$\lim_{(x,y) \to (0,0)} f(x,y) \quad \quad \quad (1)$$ into one of the form $$\lim_{(r,\theta),\, r \to 0} F(r)G(\theta). \quad \quad (2)$$ I am assuming that $f(x,y)$ is continuous in a punctured ball centered at the origin of $\mathbb{R}^{2}$ . I have two questions regarding this process: a) Given that $x^{2}+y^{2} = r^{2}$ and $(x,y) \to (0,0)$ iff $r \to 0$ , is it OK to omit the $\theta$ in $(2)$ ? b) In case $F(r)\to 0$ as $r \to 0$ and $G$ is a bounded function, it does follow that $\lim_{(x,y) \to (0,0)} f(x,y)=0$ , right? Can you recommend a calculus text wherein an ""observation"" similar to the one in b is stated and proved in detail? Thanks in advance for you replies...","It often happens that by considering polar coordinates we can transform a limit of the form into one of the form I am assuming that is continuous in a punctured ball centered at the origin of . I have two questions regarding this process: a) Given that and iff , is it OK to omit the in ? b) In case as and is a bounded function, it does follow that , right? Can you recommend a calculus text wherein an ""observation"" similar to the one in b is stated and proved in detail? Thanks in advance for you replies...","\lim_{(x,y) \to (0,0)} f(x,y) \quad \quad \quad (1) \lim_{(r,\theta),\, r \to 0} F(r)G(\theta). \quad \quad (2) f(x,y) \mathbb{R}^{2} x^{2}+y^{2} = r^{2} (x,y) \to (0,0) r \to 0 \theta (2) F(r)\to 0 r \to 0 G \lim_{(x,y) \to (0,0)} f(x,y)=0","['real-analysis', 'calculus', 'limits', 'analysis', 'multivariable-calculus']"
84,"Can we say ""$\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in [t_{k-1},t_{k}]}f(x)=U(f)$""?","Can we say """"?","\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in [t_{k-1},t_{k}]}f(x)=U(f)","Let $f:[0,1]\rightarrow \mathbb{R}$ be bounded and integrable function. Let $R_n=\frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )$ . I need to prove "" $\lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx$ "". I will write down my approach first, so you can clearly see what the question in the title means. My approach: Let partition $P=\{0,\frac{1}{n},\frac{2}{n},...,1\}$ Then note that, $$\inf_{x\in \left [ 0,\frac{1}{n} \right ]}f(x)\leq f\left ( \frac{1}{n} \right )\leq \sup_{x\in \left [ 0,\frac{1}{n} \right ]}f(x)$$ $$\inf_{x\in \left [ \frac{1}{n},\frac{2}{n} \right ]}f(x)\leq f\left ( \frac{2}{n} \right )\leq \sup_{x\in \left [ \frac{1}{n},\frac{2}{n} \right ]}f(x)$$ $$\cdot \cdot \cdot $$ $$\inf_{x\in \left [ \frac{n-1}{n},1 \right ]}f(x)\leq f\left ( \frac{n}{n} \right )=f(1)\leq \sup_{x\in \left [ \frac{n-1}{n},1 \right ]}f(x)$$ So, $$\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)$$ $$\Rightarrow \frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)\; \; \; \cdot \cdot \cdot \bigstar $$ $$\Rightarrow \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)$$ From this step, I wanted to show that $$\lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)=L(f)\; \; \; \text{and} \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)=U(f)$$ so that this could automatically implies $\lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx$ . However, I am not sure where to start to show them. One idea that I have is substracting $\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)$ from each side in step $\bigstar $ because ""f is integrable"" implies "" $\forall \varepsilon > 0,\; \exists  P$ s.t $U(f,P)-L(f,P)<\varepsilon $ "" Can I get some help? Update: As we substract $\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)$ from each side, then we get $$\frac{1}{n}\sum_{k=1}^{n}\left (\sup_{x \in [t_{k-1},t_k]}f(x)-\inf_{x \in [t_{k-1},t_k]}f(x)  \right )=U(f,P)-L(f,P)$$ on the right hand side. Thus, $$\left | \frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right ) \right |\leq U(f,P)-L(f,P) $$ We can take $N$ large enough to make it satisfy the following condition, $$\forall \varepsilon >0\: \: \exists N \: \: s.t\: \:  |U(f,P)-L(f,P)|<\varepsilon \; \; \forall n>N$$ because there always some refinement of $P$ that can make difference smaller. Therefore $\lim [U(f,P)-L(f,P)]=0$ And, thus $$\lim L(f,P) = L(f) = U(f) = \lim U(f,P)$$ This gives us, $$\lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx$$ Does my updated part look right?","Let be bounded and integrable function. Let . I need to prove "" "". I will write down my approach first, so you can clearly see what the question in the title means. My approach: Let partition Then note that, So, From this step, I wanted to show that so that this could automatically implies . However, I am not sure where to start to show them. One idea that I have is substracting from each side in step because ""f is integrable"" implies "" s.t "" Can I get some help? Update: As we substract from each side, then we get on the right hand side. Thus, We can take large enough to make it satisfy the following condition, because there always some refinement of that can make difference smaller. Therefore And, thus This gives us, Does my updated part look right?","f:[0,1]\rightarrow \mathbb{R} R_n=\frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right ) \lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx P=\{0,\frac{1}{n},\frac{2}{n},...,1\} \inf_{x\in \left [ 0,\frac{1}{n} \right ]}f(x)\leq f\left ( \frac{1}{n} \right )\leq \sup_{x\in \left [ 0,\frac{1}{n} \right ]}f(x) \inf_{x\in \left [ \frac{1}{n},\frac{2}{n} \right ]}f(x)\leq f\left ( \frac{2}{n} \right )\leq \sup_{x\in \left [ \frac{1}{n},\frac{2}{n} \right ]}f(x) \cdot \cdot \cdot  \inf_{x\in \left [ \frac{n-1}{n},1 \right ]}f(x)\leq f\left ( \frac{n}{n} \right )=f(1)\leq \sup_{x\in \left [ \frac{n-1}{n},1 \right ]}f(x) \sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x) \Rightarrow \frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)\; \; \; \cdot \cdot \cdot \bigstar  \Rightarrow \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)\leq \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right )\leq \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x) \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x)=L(f)\; \; \; \text{and} \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{k=1}^{n}\sup_{x\in \left [ t_{k-1},t_{k} \right ]}f(x)=U(f) \lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx \frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x) \bigstar  \forall \varepsilon > 0,\; \exists  P U(f,P)-L(f,P)<\varepsilon  \frac{1}{n}\sum_{k=1}^{n}\inf_{x\in \left [ t_{k-1},t_{k} \right ]} f(x) \frac{1}{n}\sum_{k=1}^{n}\left (\sup_{x \in [t_{k-1},t_k]}f(x)-\inf_{x \in [t_{k-1},t_k]}f(x)  \right )=U(f,P)-L(f,P) \left | \frac{1}{n}\sum_{k=1}^{n}f\left ( \frac{k}{n} \right ) \right |\leq U(f,P)-L(f,P)
 N \forall \varepsilon >0\: \: \exists N \: \: s.t\: \:  |U(f,P)-L(f,P)|<\varepsilon \; \; \forall n>N P \lim [U(f,P)-L(f,P)]=0 \lim L(f,P) = L(f) = U(f) = \lim U(f,P) \lim_{n\rightarrow \infty }R_n=\int_{0}^{1}f(x)dx","['real-analysis', 'integration', 'limits']"
85,composition of homemomerphism and continuous function and limit at $\infty$,composition of homemomerphism and continuous function and limit at,\infty,"Let $h\colon \Bbb R\to \Bbb R$ be a continuous function and $\lim_{x\rightarrow\infty}h(x)=0$ and $\lim_{x\rightarrow -\infty}h(x)=0.$ Let $f\colon (a,b)\to\Bbb R$ be a homeomoerphism. I do not see why $$\lim_{x\rightarrow a^{+}}(h\circ f)(x)=0 \ \  \text{and} \ \  \lim_{x\rightarrow b^{-}}(h\circ f)(x)=0.$$ Does that correct? What does this tell me in terms of cluster point of $h\circ g$ , that is, it says $0$ is cluster point of $h\circ f$ at $a$ and $b$ ? Is that right? Any help will be appreciated greatly.","Let be a continuous function and and Let be a homeomoerphism. I do not see why Does that correct? What does this tell me in terms of cluster point of , that is, it says is cluster point of at and ? Is that right? Any help will be appreciated greatly.","h\colon \Bbb R\to \Bbb R \lim_{x\rightarrow\infty}h(x)=0 \lim_{x\rightarrow -\infty}h(x)=0. f\colon (a,b)\to\Bbb R \lim_{x\rightarrow a^{+}}(h\circ f)(x)=0 \ \  \text{and} \ \  \lim_{x\rightarrow b^{-}}(h\circ f)(x)=0. h\circ g 0 h\circ f a b","['real-analysis', 'calculus', 'limits', 'limsup-and-liminf']"
86,Continuity of a piecewise integral function,Continuity of a piecewise integral function,,"I have a problem in evaluating the limit as $x$ goes to $0$ of: $$F(x)=\int_1^x f(t)\, dt\,\,\,\ \text{with }f(t)\begin{cases}t^3\ln{t}\, \text{ for }t>0\\ \arctan{t} \, \text{ for } t\leq 0\end{cases}$$ Now I have remarked that since $\lim_{t\to 0^+}f(t)=0=f(0)$ then the function $f$ can be continously extended in $0$ and so the integral function will be continous and defined in $\mathbb{R}$ . If it is continous in $0$ this means that $F(x)\to F(0)=\int_1^0t^3\ln{t}\,dx=-\int_0^1t^3\ln{t}\,dx=\frac{1}{16}$ . $\textbf{Problem}$ $$\lim_{x\to 0^-}\int_{1}^x f(t)=-\lim_{x\to 0^-}\int_{x}^1 f(t)=???$$ In fact $x\to 0^-$ so I have that the extreme $x$ tends to $0^-<0$ but the second is $1$ ...how can I write this limit and check that it is equal to $\frac{1}{16}$ ?",I have a problem in evaluating the limit as goes to of: Now I have remarked that since then the function can be continously extended in and so the integral function will be continous and defined in . If it is continous in this means that . In fact so I have that the extreme tends to but the second is ...how can I write this limit and check that it is equal to ?,"x 0 F(x)=\int_1^x f(t)\, dt\,\,\,\ \text{with }f(t)\begin{cases}t^3\ln{t}\, \text{ for }t>0\\ \arctan{t} \, \text{ for } t\leq 0\end{cases} \lim_{t\to 0^+}f(t)=0=f(0) f 0 \mathbb{R} 0 F(x)\to F(0)=\int_1^0t^3\ln{t}\,dx=-\int_0^1t^3\ln{t}\,dx=\frac{1}{16} \textbf{Problem} \lim_{x\to 0^-}\int_{1}^x f(t)=-\lim_{x\to 0^-}\int_{x}^1 f(t)=??? x\to 0^- x 0^-<0 1 \frac{1}{16}","['real-analysis', 'integration', 'limits', 'improper-integrals', 'graphing-functions']"
87,Limits of indicators of sets.,Limits of indicators of sets.,,"Suppose $A_n$ is a sequence of sets such that $\lim_{n\to\infty}A_n = A$ where we make no assumption on the $A_n$ being increasing or decreasing. Is $\lim_{n\to \infty}\chi_{A_n} = \chi_{A}$ ? For example consider a sequence  in the reals $x_n$ that has limit $x$ then define $A_n = [0,x_n]$ . Is it true that $\lim_{n\to\infty} \chi_{[0,x_n]} = \chi_{[0,x]}$ ? If the example given does not hold what are some conditions such that it does? Thanks :) When I say $\chi_S$ I mean the indicator of set $S$ , maybe you have seen it as $\mathbb{1}_S$ :)","Suppose is a sequence of sets such that where we make no assumption on the being increasing or decreasing. Is ? For example consider a sequence  in the reals that has limit then define . Is it true that ? If the example given does not hold what are some conditions such that it does? Thanks :) When I say I mean the indicator of set , maybe you have seen it as :)","A_n \lim_{n\to\infty}A_n = A A_n \lim_{n\to \infty}\chi_{A_n} = \chi_{A} x_n x A_n = [0,x_n] \lim_{n\to\infty} \chi_{[0,x_n]} = \chi_{[0,x]} \chi_S S \mathbb{1}_S","['real-analysis', 'limits', 'measure-theory']"
88,Finding the sum of the series with $a_n = \frac{4n}{6n+7}$,Finding the sum of the series with,a_n = \frac{4n}{6n+7},"In my calc 2 course, one of the homework problems is to let $$a_n = \frac{4n}{6n+7}$$ and then find the limit of its sequence and the sum of its series. I started with the limit: $$\lim_{n\to \infty} \frac{4n}{6n+7}$$ Using L'Hopital, I ended up with $\dfrac{2}{3}$ as the answer. The sum of the series is where I seem to be having trouble. So with $\sum_{n=1}^\infty \frac{4n}{6n+7}$ , I generated the first 3 terms of the series: $$\frac{4}{13} + \frac{8}{19} + \frac{12}{25} + \cdots $$ To me this series does not appear to be geometric, so I tried to utilize a definition I was provided with in class: A series converges iff the sequence, $S_n$ converges. If $S_n$ converges, we say that $\lim_{n\to \infty} S_n = S = \sum_{n=1}^\infty \frac{4n}{6n+7}$ , where $S$ is some real finite number. Based on this, the sum of the series would have to also be $\frac{2}{3}$ but when I entered this as my answer, it was marked as incorrect. So at this point, I'm thinking I might have gotten this definition incorrect or there is some way to algebraically manipulate $a_n$ to get a geometric ""form"" that I'm not seeing.","In my calc 2 course, one of the homework problems is to let and then find the limit of its sequence and the sum of its series. I started with the limit: Using L'Hopital, I ended up with as the answer. The sum of the series is where I seem to be having trouble. So with , I generated the first 3 terms of the series: To me this series does not appear to be geometric, so I tried to utilize a definition I was provided with in class: A series converges iff the sequence, converges. If converges, we say that , where is some real finite number. Based on this, the sum of the series would have to also be but when I entered this as my answer, it was marked as incorrect. So at this point, I'm thinking I might have gotten this definition incorrect or there is some way to algebraically manipulate to get a geometric ""form"" that I'm not seeing.",a_n = \frac{4n}{6n+7} \lim_{n\to \infty} \frac{4n}{6n+7} \dfrac{2}{3} \sum_{n=1}^\infty \frac{4n}{6n+7} \frac{4}{13} + \frac{8}{19} + \frac{12}{25} + \cdots  S_n S_n \lim_{n\to \infty} S_n = S = \sum_{n=1}^\infty \frac{4n}{6n+7} S \frac{2}{3} a_n,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
89,Sequential characterization of integrability,Sequential characterization of integrability,,"I'm having some difficulties trying to understand a step from the following proof, which is about the sequential characterization of integrability. Here's the theorem: And here's the part of the proof I cannot totally understand: I understand why squeeze theorem can be applied, and I agree with the fact that it implies: $$ \lim_{n\rightarrow \infty}{[U(f;P_n)-U(f)]} = 0 $$ Yet still I don't see why the above limit can be 'split' into two different limits, since, in general, the fact that: $$ \lim_{n\rightarrow \infty}{(a_n-b_n)} = 0 $$ does not imply the existence of the following limits: $$ \lim_{n\rightarrow \infty}{a_n}, \,\,\,\,\,\,\,\, \lim_{n\rightarrow \infty}{b_n}.  $$ Thanks beforehand!!","I'm having some difficulties trying to understand a step from the following proof, which is about the sequential characterization of integrability. Here's the theorem: And here's the part of the proof I cannot totally understand: I understand why squeeze theorem can be applied, and I agree with the fact that it implies: Yet still I don't see why the above limit can be 'split' into two different limits, since, in general, the fact that: does not imply the existence of the following limits: Thanks beforehand!!","
\lim_{n\rightarrow \infty}{[U(f;P_n)-U(f)]} = 0
 
\lim_{n\rightarrow \infty}{(a_n-b_n)} = 0
 
\lim_{n\rightarrow \infty}{a_n}, \,\,\,\,\,\,\,\,
\lim_{n\rightarrow \infty}{b_n}. 
","['real-analysis', 'limits', 'proof-explanation', 'riemann-integration']"
90,I am having trouble finding $f'(x)$ given $f(x) = \frac{4}{x}$?,I am having trouble finding  given ?,f'(x) f(x) = \frac{4}{x},I am having trouble finding $f'(x)$ given $f(x) = \frac{4}{x}$ . I used the definition of a derivative but I got stuck. I think the right answer is $\frac{-4}{x^2}$ but I am stuck at $\frac{-4h}{x(x+h)}$ . My work is posted down below. Thank you.,I am having trouble finding given . I used the definition of a derivative but I got stuck. I think the right answer is but I am stuck at . My work is posted down below. Thank you.,f'(x) f(x) = \frac{4}{x} \frac{-4}{x^2} \frac{-4h}{x(x+h)},"['limits', 'derivatives']"
91,Different answers after different methods in solving a limit,Different answers after different methods in solving a limit,,Evaluate $$L=\lim_{x \to 0} \frac{e^{\sin(x)}-(1+\sin(x))}{(\arctan(\sin(x)))^2}$$ Method $1$ : $$\frac{h^2\left(\frac{e^{h}-1}{h^2}-\frac1{h}\right)}{(\arctan(h))^2}=1^2\left(\frac{1*1}{h}-\frac1{h}\right)=\frac1{h}-\frac1{h}=0$$ Therefore $L=0$ . The identities I have used here to simplify the expression are $$\lim_{x \to 0} \frac{\arctan\left(x\right)}{x}=1$$ $$\lim_{x \to 0} \frac{a^{x}-1}{a}=\ln\left(a\right) \implies \lim_{x \to 0} \frac{e^{x}-1}{x}=\ln\left(e\right)=1$$ Method $2$ : $$L=\frac{e^{h}-\left(1+h\right)}{\left(\arctan\left(h\right)\right)^{2}}=\frac{\left(1+h+\frac{h^{2}}{2!}\right)-\left(1+h\right)}{\left(\arctan\left(h\right)\right)^{2}}=\frac{\left(\frac{h^{2}}{2!}\right)}{\left(\arctan\left(h\right)\right)^{2}}=\frac{1}{2}$$ Therefore $L=\frac12$ . I am not at all familiar with $O(n)$ notation BTW.,Evaluate Method : Therefore . The identities I have used here to simplify the expression are Method : Therefore . I am not at all familiar with notation BTW.,L=\lim_{x \to 0} \frac{e^{\sin(x)}-(1+\sin(x))}{(\arctan(\sin(x)))^2} 1 \frac{h^2\left(\frac{e^{h}-1}{h^2}-\frac1{h}\right)}{(\arctan(h))^2}=1^2\left(\frac{1*1}{h}-\frac1{h}\right)=\frac1{h}-\frac1{h}=0 L=0 \lim_{x \to 0} \frac{\arctan\left(x\right)}{x}=1 \lim_{x \to 0} \frac{a^{x}-1}{a}=\ln\left(a\right) \implies \lim_{x \to 0} \frac{e^{x}-1}{x}=\ln\left(e\right)=1 2 L=\frac{e^{h}-\left(1+h\right)}{\left(\arctan\left(h\right)\right)^{2}}=\frac{\left(1+h+\frac{h^{2}}{2!}\right)-\left(1+h\right)}{\left(\arctan\left(h\right)\right)^{2}}=\frac{\left(\frac{h^{2}}{2!}\right)}{\left(\arctan\left(h\right)\right)^{2}}=\frac{1}{2} L=\frac12 O(n),['limits']
92,Calculate the limit $\lim\limits_{n \to \infty}\left(\frac{1}{\sqrt[3]{(8n^{3}+2)}^{2}}+\cdots+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}}\right)$,Calculate the limit,\lim\limits_{n \to \infty}\left(\frac{1}{\sqrt[3]{(8n^{3}+2)}^{2}}+\cdots+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}}\right),I have to calculate the limit $$\lim_{n \to \infty}\left(\frac{1}{\sqrt[3]{(8n^{3}+2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+4)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+6)}^{2}}+\cdots+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2}-2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}}\right)$$ I tried to use Sandwich Theorem like this $$\frac{3n^{2}}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}} \leq \Bigg(\frac{1}{\sqrt[3]{(8n^{3}+2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+4)}^{2}}+\cdots+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2}-2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}}\Bigg) \leq \frac{3n^{2}}{\sqrt[3]{(8n^{3}+2)}^{2}}$$ And for result I got that limit is $\frac{3}{4}$ Is this correct?,I have to calculate the limit I tried to use Sandwich Theorem like this And for result I got that limit is Is this correct?,\lim_{n \to \infty}\left(\frac{1}{\sqrt[3]{(8n^{3}+2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+4)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+6)}^{2}}+\cdots+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2}-2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}}\right) \frac{3n^{2}}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}} \leq \Bigg(\frac{1}{\sqrt[3]{(8n^{3}+2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+4)}^{2}}+\cdots+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2}-2)}^{2}}+\frac{1}{\sqrt[3]{(8n^{3}+6n^{2})}^{2}}\Bigg) \leq \frac{3n^{2}}{\sqrt[3]{(8n^{3}+2)}^{2}} \frac{3}{4},['limits']
93,Limits using Cramer's Rule as determinant approaches 0,Limits using Cramer's Rule as determinant approaches 0,,"I'm in Linear Algebra 1, and having just covered Cramer's Rule, the prof showed this interesting case that I have a further question about the significance of. Say we have a matrix containing a constant that can be adjusted, for instance, the system of equations 2cx+3y=6 4x+(c-1)y=4 giving $\begin{pmatrix}2c&3\\4&c-1\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}6\\4 \end{pmatrix}$ Since Cramer's rule only holds in cases where the determinant is nonzero, a typical question would be to find the values of c for which that is true. In this case, det(A)=0 when c=-2 or c=3. At c=-2 there are no solutions to the system, and at c=3 there are infinitely many solutions. In the case of c=3, we cannot simply apply Cramer's rule, because the denominator of x= $\frac{detA(1)}{detA}$ and y= $\frac{detA(2)}{detA}$ are both detA=0. However, what we can do is go back to the original system, leaving the variable c in the matrix, and calculate the values of detA, detA(1) and detA(2) in relation to c. If I do that, and completely factor, I get: detA=2(c+2)(c-3) detA(1)=6(c-3) detA(2)=8(c-3) Now I can use limits to get an answer from the formulation of Cramer's rule in the case of c=3. x= $\lim\limits_{c \to 3}\frac{6(c-3)}{2(c+2)(c-3)}$ y= $\lim\limits_{c \to 3}\frac{8(c-3)}{2(c+2)(c-3)}$ From which we can easily get the values of x= $\frac{3}{5}$ and y= $\frac{4}{5}$ , which is a valid solution. So Cramer's rule, despite its initial misgivings, has provided a solution to a system with a determinant of 0. My question (which my prof couldn't answer on the spot, which is why I'm bringing it here) is, which solution? What is special about these numbers, that they're the ones that happen to be spat out using this method? My first thought was that perhaps it's the particular solution, but it's not: the solution in parameterized form of the matrix when c=3 is $\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}1\\0 \end{pmatrix}+s\begin{pmatrix}-1/2\\1 \end{pmatrix}$ So I am at a loss as to what these numbers ""are,"" if they ""are"" anything in particular. Surely they're not just random?","I'm in Linear Algebra 1, and having just covered Cramer's Rule, the prof showed this interesting case that I have a further question about the significance of. Say we have a matrix containing a constant that can be adjusted, for instance, the system of equations 2cx+3y=6 4x+(c-1)y=4 giving Since Cramer's rule only holds in cases where the determinant is nonzero, a typical question would be to find the values of c for which that is true. In this case, det(A)=0 when c=-2 or c=3. At c=-2 there are no solutions to the system, and at c=3 there are infinitely many solutions. In the case of c=3, we cannot simply apply Cramer's rule, because the denominator of x= and y= are both detA=0. However, what we can do is go back to the original system, leaving the variable c in the matrix, and calculate the values of detA, detA(1) and detA(2) in relation to c. If I do that, and completely factor, I get: detA=2(c+2)(c-3) detA(1)=6(c-3) detA(2)=8(c-3) Now I can use limits to get an answer from the formulation of Cramer's rule in the case of c=3. x= y= From which we can easily get the values of x= and y= , which is a valid solution. So Cramer's rule, despite its initial misgivings, has provided a solution to a system with a determinant of 0. My question (which my prof couldn't answer on the spot, which is why I'm bringing it here) is, which solution? What is special about these numbers, that they're the ones that happen to be spat out using this method? My first thought was that perhaps it's the particular solution, but it's not: the solution in parameterized form of the matrix when c=3 is So I am at a loss as to what these numbers ""are,"" if they ""are"" anything in particular. Surely they're not just random?",\begin{pmatrix}2c&3\\4&c-1\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}6\\4 \end{pmatrix} \frac{detA(1)}{detA} \frac{detA(2)}{detA} \lim\limits_{c \to 3}\frac{6(c-3)}{2(c+2)(c-3)} \lim\limits_{c \to 3}\frac{8(c-3)}{2(c+2)(c-3)} \frac{3}{5} \frac{4}{5} \begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}1\\0 \end{pmatrix}+s\begin{pmatrix}-1/2\\1 \end{pmatrix},"['linear-algebra', 'limits']"
94,What to think about this limit trick $\lim_{x\to 1}f(x)=\lim_{x\to 1}f(\frac 1x)$?,What to think about this limit trick ?,\lim_{x\to 1}f(x)=\lim_{x\to 1}f(\frac 1x),"While searching for duplicates to a question asked recently I stumbled onto this specific answer: Show that $\lim\limits_{x\to 1}\left(\frac p{1-x^p}-\frac q{1-x^q}\right)=\frac {p-q}2$ I reproduce the sketch here: $$L=\lim\limits_{x\to 1}\left(\frac p{1-x^p}-\frac q{1-x^q}\right)=\lim\limits_{x\to 1}\left(\frac p{1-(\frac 1x)^p}-\frac q{1-(\frac 1x)^q}\right)=\lim\limits_{x\to 1}\left(\frac {-px^p}{1-x^p}-\frac {-qx^q}{1-x^q}\right)$$ So by adding first and third term we get $2L=p-q$ . What annoys me is that it looks like an answer we could put in the ""bad math that gets away with it"" thread... It is true that $\lim\limits_{x\to 1}f(x)=\lim\limits_{x\to 1}f(\frac 1x)$ when the limit exists, however here, I think it is part of the problem to show that this limit actually exists. Looking at it more closely we have that: $f_p(x)=\dfrac{p}{1-x^p}=\frac 1{1-x}+\frac{n-1}2+O(1-x)$ $f_p(\frac 1x)=\dfrac{p}{1-(\frac 1x)^p}=-\frac 1{1-x}+\frac{n+1}2+O(1-x)$ So if we were to apply the same process to $\ell_p=\lim\limits_{x\to 1}\dfrac{p}{1-x^p}$ we would get $2\ell_p=p\iff \ell_p=\frac p2$ which is wrong since $1$ is a pole here (limit does not exist). But the solution proposed in the cited answer let us think that we got the result from $\ell_p-\ell _q=\frac p2-\frac q2=\frac{p-q}2$ While in reality we get it from cancellation of the divergent parts in: $\require{cancel}f_p(x)-f_q(x)=\left(\cancel{\frac 1{1-x}}+\frac 12(p-1)+O(1-x)\right)-\left(\cancel{\frac 1{1-x}}+\frac 12(q-1)+O(1-x)\right)=\frac{(p-\cancel{1})-(q-\cancel{1})}2+O(1-x)\to \frac{p-q}2$ So what is your opinion on this answer, do you agree with my point of view ? How would you convince a student that does this, that his method is flawed, or at least that he should prove first that the limit exists before carrying the $\lim$ operator from equality to equality like this (which BTW seems to be a very common habit among posters...).","While searching for duplicates to a question asked recently I stumbled onto this specific answer: Show that I reproduce the sketch here: So by adding first and third term we get . What annoys me is that it looks like an answer we could put in the ""bad math that gets away with it"" thread... It is true that when the limit exists, however here, I think it is part of the problem to show that this limit actually exists. Looking at it more closely we have that: So if we were to apply the same process to we would get which is wrong since is a pole here (limit does not exist). But the solution proposed in the cited answer let us think that we got the result from While in reality we get it from cancellation of the divergent parts in: So what is your opinion on this answer, do you agree with my point of view ? How would you convince a student that does this, that his method is flawed, or at least that he should prove first that the limit exists before carrying the operator from equality to equality like this (which BTW seems to be a very common habit among posters...).",\lim\limits_{x\to 1}\left(\frac p{1-x^p}-\frac q{1-x^q}\right)=\frac {p-q}2 L=\lim\limits_{x\to 1}\left(\frac p{1-x^p}-\frac q{1-x^q}\right)=\lim\limits_{x\to 1}\left(\frac p{1-(\frac 1x)^p}-\frac q{1-(\frac 1x)^q}\right)=\lim\limits_{x\to 1}\left(\frac {-px^p}{1-x^p}-\frac {-qx^q}{1-x^q}\right) 2L=p-q \lim\limits_{x\to 1}f(x)=\lim\limits_{x\to 1}f(\frac 1x) f_p(x)=\dfrac{p}{1-x^p}=\frac 1{1-x}+\frac{n-1}2+O(1-x) f_p(\frac 1x)=\dfrac{p}{1-(\frac 1x)^p}=-\frac 1{1-x}+\frac{n+1}2+O(1-x) \ell_p=\lim\limits_{x\to 1}\dfrac{p}{1-x^p} 2\ell_p=p\iff \ell_p=\frac p2 1 \ell_p-\ell _q=\frac p2-\frac q2=\frac{p-q}2 \require{cancel}f_p(x)-f_q(x)=\left(\cancel{\frac 1{1-x}}+\frac 12(p-1)+O(1-x)\right)-\left(\cancel{\frac 1{1-x}}+\frac 12(q-1)+O(1-x)\right)=\frac{(p-\cancel{1})-(q-\cancel{1})}2+O(1-x)\to \frac{p-q}2 \lim,"['limits', 'proof-writing', 'soft-question']"
95,Formal proof for the limit of $\frac{\tanh(x)-1}{e^{-2x}}$ as $x \rightarrow \infty$,Formal proof for the limit of  as,\frac{\tanh(x)-1}{e^{-2x}} x \rightarrow \infty,"Formal proof for the limit of $\frac{\tanh(x)-1}{e^{-2x}}$ as $x \rightarrow \infty$ . So far Keep in mind I have to use the definition for a limit. I.e for this would be a proof for the limit at $\infty$ with a finite value (I have checked this to be $-2$ ). I would have to use the definition that states $\lim_{x \ \to \  \infty} f(x) = L \Leftrightarrow    \forall \ \epsilon>0\; (\exists \ \delta : (\;x>\delta\implies |f(x) - L|\leq\epsilon)). $ i.e I start off by by finding that $$\left | \frac{\tanh{x}-1}{e^{-2x}}-(-2) \right | \iff \left | \frac{2}{1+e^{2x}} \right |$$ Or just $\frac{2}{1+e^{2x}}$ since we will assume $\epsilon >0$ . Now I would have to to start my proof and this is where I struggle. I know that ""for alle $\epsilon>0$ let $\delta=\frac{2}{1+e^{2\epsilon}}>0$ and $x>\delta>0$ ....."" *Remark: I know we use $N,M,K$ and not $\delta$ normally.","Formal proof for the limit of as . So far Keep in mind I have to use the definition for a limit. I.e for this would be a proof for the limit at with a finite value (I have checked this to be ). I would have to use the definition that states i.e I start off by by finding that Or just since we will assume . Now I would have to to start my proof and this is where I struggle. I know that ""for alle let and ....."" *Remark: I know we use and not normally.","\frac{\tanh(x)-1}{e^{-2x}} x \rightarrow \infty \infty -2 \lim_{x \ \to \  \infty} f(x) = L \Leftrightarrow 
  \forall \ \epsilon>0\; (\exists \ \delta : (\;x>\delta\implies |f(x) - L|\leq\epsilon)).
 \left | \frac{\tanh{x}-1}{e^{-2x}}-(-2) \right | \iff \left | \frac{2}{1+e^{2x}} \right | \frac{2}{1+e^{2x}} \epsilon >0 \epsilon>0 \delta=\frac{2}{1+e^{2\epsilon}}>0 x>\delta>0 N,M,K \delta","['real-analysis', 'limits']"
96,$\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7}$,,\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7},"How do I solve this example? I tried to point out the fastest growing term $2 ^ n$ and $3 ^ n$ , but that doesn't seem to lead to the result. I know the limit is $0$ that's obvious, but I don't know how to work on it. $$\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7}$$ Or can I make a power estimate of a theorem on a tightened sequence, for example, that $2 ^ n$ / $13 ^ n$ (smaller) goes to zero and the other side $2 ^ n$ / $3 ^ n$ (bigger) also goes to zero? $\lim_{n\to\infty} 2 ^ n$ / $13 ^ n$ < $\lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7}$ < $\lim_{n\to\infty} 2 ^ n$ / $3 ^ n$","How do I solve this example? I tried to point out the fastest growing term and , but that doesn't seem to lead to the result. I know the limit is that's obvious, but I don't know how to work on it. Or can I make a power estimate of a theorem on a tightened sequence, for example, that / (smaller) goes to zero and the other side / (bigger) also goes to zero? / < < /",2 ^ n 3 ^ n 0 \lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7} 2 ^ n 13 ^ n 2 ^ n 3 ^ n \lim_{n\to\infty} 2 ^ n 13 ^ n \lim_{n\to\infty} \frac{2^n(n^4-1)}{4\cdot 3^n + n^7} \lim_{n\to\infty} 2 ^ n 3 ^ n,['calculus']
97,Limit of $(x^3 - 3x^2 + 2x)$ as $x \to 1$ using epsilon delta definition,Limit of  as  using epsilon delta definition,(x^3 - 3x^2 + 2x) x \to 1,"I need to prove the following limit using epsilon delta method. I have come up this on my own just to practice skills. $$\lim \limits_{x \to 1} \,(x^3 - 3x^2 + 2x) = 0 $$ So, I need to come up with some $\delta >0$ such that given $0 < \lvert x - 1 \rvert <\delta$ , I need to prove that $$ |(x^3 - 3x^2 + 2x) - 0| < \varepsilon \tag{1}$$ I can simplify eq $1$ as follows. $$ |x| |x-1||x-2| < \varepsilon $$ When we have a non linear term, then we restrict $x$ to some distance from $1$ . So, suppose we have $ 0 < |x-1| < 10$ . This leads to $-10 < x-1 < 10$ . Doing some algebra, we can get inequalities $$ 0 \leqslant |x| < 11 \tag{2}$$ $$ 0 \leqslant |x-2| < 11 \tag{3}$$ From eq 2 and 3, it follows that $$ 0 \leqslant |x||x-2| < 121 $$ And since $0 < |x-1| $ , it follows that $$ |x||x-1||x-2| < 121 |x-1| \tag{4}$$ To show that $|x||x-1||x-2| < \varepsilon$ , its sufficient to show that $ 121 |x-1| < \varepsilon$ . And this will happen if $ |x-1| < \frac{\varepsilon}{121} $ . So, this is another restriction on $|x-1|$ . So, we can let $$\delta = \text{min}\left(10, \frac{\varepsilon}{121}\right) $$ So, we can let this be our choice of $\delta$ . Its clear that $\delta > 0$ . So, now the official proof will follow. Let $\varepsilon > 0$ be some arbitrary real. Suppose $0 < |x-1| < \delta$ . With the choice of $\delta$ we have done, this means that $ 0 < |x-1| < 10$ . As demonstrated above, equation 4 follows from this inequality $$ |x||x-1||x-2| < 121 |x-1| $$ But now, we also have $ |x-1| < \frac{\varepsilon}{121} $ . It means that $ 121 |x-1| < \varepsilon $ . Using this, we get that $$ |x||x-1||x-2| < \varepsilon  $$ which can be rewritten as $$ | (x^3 - 3x^2 + 2x) - 0 | < \varepsilon  $$ Since, $\varepsilon > 0$ was arbitrary to begin with, its proven that $$\lim \limits_{x \to 1} \,(x^3 - 3x^2 + 2x) = 0 $$ Is the proof correct ?","I need to prove the following limit using epsilon delta method. I have come up this on my own just to practice skills. So, I need to come up with some such that given , I need to prove that I can simplify eq as follows. When we have a non linear term, then we restrict to some distance from . So, suppose we have . This leads to . Doing some algebra, we can get inequalities From eq 2 and 3, it follows that And since , it follows that To show that , its sufficient to show that . And this will happen if . So, this is another restriction on . So, we can let So, we can let this be our choice of . Its clear that . So, now the official proof will follow. Let be some arbitrary real. Suppose . With the choice of we have done, this means that . As demonstrated above, equation 4 follows from this inequality But now, we also have . It means that . Using this, we get that which can be rewritten as Since, was arbitrary to begin with, its proven that Is the proof correct ?","\lim \limits_{x \to 1} \,(x^3 - 3x^2 + 2x) = 0  \delta >0 0 < \lvert x - 1 \rvert <\delta  |(x^3 - 3x^2 + 2x) - 0| < \varepsilon \tag{1} 1  |x| |x-1||x-2| < \varepsilon  x 1  0 < |x-1| < 10 -10 < x-1 < 10  0 \leqslant |x| < 11 \tag{2}  0 \leqslant |x-2| < 11 \tag{3}  0 \leqslant |x||x-2| < 121  0 < |x-1|   |x||x-1||x-2| < 121 |x-1| \tag{4} |x||x-1||x-2| < \varepsilon  121 |x-1| < \varepsilon  |x-1| < \frac{\varepsilon}{121}  |x-1| \delta = \text{min}\left(10, \frac{\varepsilon}{121}\right)  \delta \delta > 0 \varepsilon > 0 0 < |x-1| < \delta \delta  0 < |x-1| < 10  |x||x-1||x-2| < 121 |x-1|   |x-1| < \frac{\varepsilon}{121}   121 |x-1| < \varepsilon   |x||x-1||x-2| < \varepsilon    | (x^3 - 3x^2 + 2x) - 0 | < \varepsilon   \varepsilon > 0 \lim \limits_{x \to 1} \,(x^3 - 3x^2 + 2x) = 0 ","['real-analysis', 'calculus', 'limits', 'epsilon-delta']"
98,How to find derivative of $\int_0^{\sin x} (1+t)^{\frac{1}{t}} dt$,How to find derivative of,\int_0^{\sin x} (1+t)^{\frac{1}{t}} dt,"I am asked to solve this question: ' Which one is the higher order infinitesimals when $\mathbf{x \rightarrow 0 \, \int_0^{\sin x} (1+t)^{\frac{1}{t}} dt}$ ' or $\mathbf{x^2}$ . I know that:I have to solve $\lim_{x \rightarrow 0}\frac{\int_0^{\sin x} (1+t)^{\frac{1}{t}} dt}{x}$ by using L 'Hospital's rule and compare the result with $x^2$ But I cant find the derivative. I have recited that the derivative of $\int_{φ(x)}^{ψ(x)}f(t)dt$ (if x is not in f(t)) is $f[φ(x)]φ'(x)-f[(ψ)]ψ'(x)$ , without actually knowing how it comes. But for now, this equation seems impossible since I cannot get $(1+0)^{\frac{1}{0}}$ , I don't think $\lim (1+0)^{\frac{1}{0}}$ should be right, since the hint said the $\int_0^{\sin x} (1+t)^{\frac{1}{t}} dt$ ~ $ex$ So what should I do? Is the equation I have recited succeed all the time?","I am asked to solve this question: ' Which one is the higher order infinitesimals when ' or . I know that:I have to solve by using L 'Hospital's rule and compare the result with But I cant find the derivative. I have recited that the derivative of (if x is not in f(t)) is , without actually knowing how it comes. But for now, this equation seems impossible since I cannot get , I don't think should be right, since the hint said the ~ So what should I do? Is the equation I have recited succeed all the time?","\mathbf{x \rightarrow 0 \, \int_0^{\sin x} (1+t)^{\frac{1}{t}} dt} \mathbf{x^2} \lim_{x \rightarrow 0}\frac{\int_0^{\sin x} (1+t)^{\frac{1}{t}} dt}{x} x^2 \int_{φ(x)}^{ψ(x)}f(t)dt f[φ(x)]φ'(x)-f[(ψ)]ψ'(x) (1+0)^{\frac{1}{0}} \lim (1+0)^{\frac{1}{0}} \int_0^{\sin x} (1+t)^{\frac{1}{t}} dt ex","['limits', 'derivatives']"
99,Solving this problem without Taylor series ( I made an extra assumption),Solving this problem without Taylor series ( I made an extra assumption),,"I want to show that the derivative of a (one-time differentiable) function $f(x)$ is given by $$f'(a)= \lim \limits_{h \to 0} \dfrac {1}{h} \displaystyle \int_{-1}^{1} \dfrac{3u}{2} f(a+uh) \ du$$ I used the Taylor's expansion about the point $x=a$ and the conventional $h$ as $uh$ [i.e. $f(x+h)= f(x) + h f'(x) + \cdots$ ] and got the answer. However, I made an additional assumption that the function $f(x)$ possesses a convergent series about $uh$ and this includes the assumption that the function is differentiable more than once. Tried applying the Leibniz rule, but could not get to the RHS. How can I prove the result without using the Taylor series?","I want to show that the derivative of a (one-time differentiable) function is given by I used the Taylor's expansion about the point and the conventional as [i.e. ] and got the answer. However, I made an additional assumption that the function possesses a convergent series about and this includes the assumption that the function is differentiable more than once. Tried applying the Leibniz rule, but could not get to the RHS. How can I prove the result without using the Taylor series?",f(x) f'(a)= \lim \limits_{h \to 0} \dfrac {1}{h} \displaystyle \int_{-1}^{1} \dfrac{3u}{2} f(a+uh) \ du x=a h uh f(x+h)= f(x) + h f'(x) + \cdots f(x) uh,"['integration', 'limits', 'derivatives']"
