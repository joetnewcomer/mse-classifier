,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Help figuring out what this textbook did.,Help figuring out what this textbook did.,,"I was reading my classical mechanics textbook and this appeared in the chapter for oscillations. $$\dfrac{\mathrm d^2x}{\mathrm dt^2}+\omega_0^2x=0\tag{3.31}$$ We can obtain the equation for the phase path, however, by a simpler procedure, because Equation $3.31$ can be replaced by the pair of equations $$\dfrac{\mathrm dx}{\mathrm dt}=\dot x,\quad\dfrac{\mathrm d\dot x}{\mathrm dt}=-\omega_0^2x\tag{3.32}$$ How did the author do that replace the first equation with the second one? (The book is Classical Dynamics by Marion)","I was reading my classical mechanics textbook and this appeared in the chapter for oscillations. $$\dfrac{\mathrm d^2x}{\mathrm dt^2}+\omega_0^2x=0\tag{3.31}$$ We can obtain the equation for the phase path, however, by a simpler procedure, because Equation $3.31$ can be replaced by the pair of equations $$\dfrac{\mathrm dx}{\mathrm dt}=\dot x,\quad\dfrac{\mathrm d\dot x}{\mathrm dt}=-\omega_0^2x\tag{3.32}$$ How did the author do that replace the first equation with the second one? (The book is Classical Dynamics by Marion)",,['ordinary-differential-equations']
1,"If $f(x) \cdot x < 0$ for all $x \in \partial B_R(0)$, then the IVP $x' = f(x)$, $x(0) = x_0$ has a global solution.","If  for all , then the IVP ,  has a global solution.",f(x) \cdot x < 0 x \in \partial B_R(0) x' = f(x) x(0) = x_0,"I have a homework problem that asks If $f : \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable and satisfies   $$ f(x) \cdot x < 0 \quad \quad \text{for all } x \in \partial B_R(0) $$   for some $R > 0$ (where $B_R(0)$ is the open ball centered at the origin of radius $R$), then the initial value problem   $$ x' = f(x), \quad x(0) = x_0 $$   has a global solution for every $x_0 \in B_R(0)$. I know that $f$ is locally Lipschitz because $f$ is continuously differentiable and $B_R(0)$ is open and connected, so Picard's existence theorem says there is in fact a solution.  Say the maximal interval of existence is $I = (\alpha, \beta)$, and suppose for the sake of contradiction $\beta \neq \infty$.  I have a theorem at hand that says: For any compact subset $K \subseteq B_R(0)$, there exists a $\beta_K < \beta$ such that $x(t) \in B_R(0) \setminus K$ for all $t \in (\beta_K, \beta)$. Now I think I should consider the sequence of compact sets $\left\{ \overline{B_{R - \frac{1}{n}}(0)} \right\}_{n=1}$.  For each one there is some $\beta_n < \beta$ such that $R - \frac{1}{n} < \| x(t) \| < R$ for every $t \in (\beta_n, \beta)$... This is where I am stuck.  The assumption about the dot product is almost surely going to come into play soon, but I don't see how to use it to show some contradiction.  I feel like I am very close.","I have a homework problem that asks If $f : \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable and satisfies   $$ f(x) \cdot x < 0 \quad \quad \text{for all } x \in \partial B_R(0) $$   for some $R > 0$ (where $B_R(0)$ is the open ball centered at the origin of radius $R$), then the initial value problem   $$ x' = f(x), \quad x(0) = x_0 $$   has a global solution for every $x_0 \in B_R(0)$. I know that $f$ is locally Lipschitz because $f$ is continuously differentiable and $B_R(0)$ is open and connected, so Picard's existence theorem says there is in fact a solution.  Say the maximal interval of existence is $I = (\alpha, \beta)$, and suppose for the sake of contradiction $\beta \neq \infty$.  I have a theorem at hand that says: For any compact subset $K \subseteq B_R(0)$, there exists a $\beta_K < \beta$ such that $x(t) \in B_R(0) \setminus K$ for all $t \in (\beta_K, \beta)$. Now I think I should consider the sequence of compact sets $\left\{ \overline{B_{R - \frac{1}{n}}(0)} \right\}_{n=1}$.  For each one there is some $\beta_n < \beta$ such that $R - \frac{1}{n} < \| x(t) \| < R$ for every $t \in (\beta_n, \beta)$... This is where I am stuck.  The assumption about the dot product is almost surely going to come into play soon, but I don't see how to use it to show some contradiction.  I feel like I am very close.",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
2,Solution of differential equation with some conditions!,Solution of differential equation with some conditions!,,"Find twice differentiable function $f:\Bbb{R}\to \Bbb{R}$  such that  $f''(x)=(x^2-1)f(x)$ with $f(0)=1$ and $f'(0)=0$ I can see that $f(x)$=$e^{-x^2/2}$ satisfies the required conditions but I don't have any proper way to finding this function.I have found this by ""hit and trial"" method.","Find twice differentiable function $f:\Bbb{R}\to \Bbb{R}$  such that  $f''(x)=(x^2-1)f(x)$ with $f(0)=1$ and $f'(0)=0$ I can see that $f(x)$=$e^{-x^2/2}$ satisfies the required conditions but I don't have any proper way to finding this function.I have found this by ""hit and trial"" method.",,['ordinary-differential-equations']
3,Please explain the Quotient Rule,Please explain the Quotient Rule,,I am currently working on an equation but I'm having a hard time understanding how to get the answer. the answer is ${(x^2-4)(x^2+4)(2x+8)-(x^2+8x-4)(4x^3)\over (x^2-4)^2(x^2+4)^2}$ The equation is $f(x)= {x\over x^2-4}-{x-1\over x^2+4}$ When I apply the quotient rule i get $f'(x)= {(1)(x^2-4)-(2x)(1)\over (x^2-4)^2}-{(1)(x^2+4)-(2x)(1)\over (x^2+4)^2}$ but it cancels each other out.  I can't figure out how they had gotten the answer.,I am currently working on an equation but I'm having a hard time understanding how to get the answer. the answer is ${(x^2-4)(x^2+4)(2x+8)-(x^2+8x-4)(4x^3)\over (x^2-4)^2(x^2+4)^2}$ The equation is $f(x)= {x\over x^2-4}-{x-1\over x^2+4}$ When I apply the quotient rule i get $f'(x)= {(1)(x^2-4)-(2x)(1)\over (x^2-4)^2}-{(1)(x^2+4)-(2x)(1)\over (x^2+4)^2}$ but it cancels each other out.  I can't figure out how they had gotten the answer.,,"['calculus', 'ordinary-differential-equations', 'quotient-spaces']"
4,Second order homogenous non-linear DE: $3(x')^2 - 2x''x=0$,Second order homogenous non-linear DE:,3(x')^2 - 2x''x=0,"How do I solve this for $x$? $$3\dot{x}^2-2\ddot{x}x=0$$ $$\Leftrightarrow$$ $$3(x')^2 - 2x''x=0 $$ Note: This comes from my working here (on stack exchange meta sandbox[newest activity]) List of methods is acceptable as an answer. I can do the research with helpful direction. There is likely an approach to this that I don't know, otherwise I may have improperly obtained this DE. I will overlook my working again soon, and see if this possibility is (non) negligible.","How do I solve this for $x$? $$3\dot{x}^2-2\ddot{x}x=0$$ $$\Leftrightarrow$$ $$3(x')^2 - 2x''x=0 $$ Note: This comes from my working here (on stack exchange meta sandbox[newest activity]) List of methods is acceptable as an answer. I can do the research with helpful direction. There is likely an approach to this that I don't know, otherwise I may have improperly obtained this DE. I will overlook my working again soon, and see if this possibility is (non) negligible.",,['calculus']
5,Why do we seek real-valued solutions to second-order linear homogeneous ODEs when the characteristic equation has complex roots?,Why do we seek real-valued solutions to second-order linear homogeneous ODEs when the characteristic equation has complex roots?,,"This is a random question that has been bugging me. In first-year calculus we learned that the second-order linear homogeneous ODE with complex roots $a\pm ib$ to its characteristic equation , has a real-valued general solution of the form: $y(x)=e^{ax}\left(c_1cos(bx)+c_2sin(bx)\right)$. To get to this real-valued general solution, some intermediate steps were performed on the original complex-valued general solution, $y(x)=Pe^{(a+ib)x}+Qe^{(a-ib)x}$. I'm interested in learning why the real-valued case is of particular interest only. Is there a particular reason for doing so?","This is a random question that has been bugging me. In first-year calculus we learned that the second-order linear homogeneous ODE with complex roots $a\pm ib$ to its characteristic equation , has a real-valued general solution of the form: $y(x)=e^{ax}\left(c_1cos(bx)+c_2sin(bx)\right)$. To get to this real-valued general solution, some intermediate steps were performed on the original complex-valued general solution, $y(x)=Pe^{(a+ib)x}+Qe^{(a-ib)x}$. I'm interested in learning why the real-valued case is of particular interest only. Is there a particular reason for doing so?",,['ordinary-differential-equations']
6,"Runge-Kutta 8(5,3)","Runge-Kutta 8(5,3)",,"This is actually three small very related questions about Runge-Kutta methods. I have programmed a RK 7(8) method also RK 4(5). At the beginning I was assuming that the RK 7(8) uses two approximations of different order, one of order 7 an another of order 8. The difference between the two approximations is used to estimate the error of integration, and the algorithm returns the approximation of order 8. But by using an system of ODE's for which I know the exact answer (as a test method), I have seen that the approximation of order 7 gives a smaller error. As when we write RK 7(8) we write first the 7, is it supposed that the method is of order 7 or 8? When we say order $k$ , do we mean that the approximation is up to order $k$ or that the error is of order $k$ ? Python programming language provides a routine called dop853 that performs a Runge Kutta 8(5,3). What does it mean exactly when the method is specified by three numbers. Thank you very much.","This is actually three small very related questions about Runge-Kutta methods. I have programmed a RK 7(8) method also RK 4(5). At the beginning I was assuming that the RK 7(8) uses two approximations of different order, one of order 7 an another of order 8. The difference between the two approximations is used to estimate the error of integration, and the algorithm returns the approximation of order 8. But by using an system of ODE's for which I know the exact answer (as a test method), I have seen that the approximation of order 7 gives a smaller error. As when we write RK 7(8) we write first the 7, is it supposed that the method is of order 7 or 8? When we say order , do we mean that the approximation is up to order or that the error is of order ? Python programming language provides a routine called dop853 that performs a Runge Kutta 8(5,3). What does it mean exactly when the method is specified by three numbers. Thank you very much.",k k k,"['ordinary-differential-equations', 'numerical-methods', 'soft-question', 'math-software', 'runge-kutta-methods']"
7,Is there a closed form solution for this differential equation?,Is there a closed form solution for this differential equation?,,"I was trying to solve the following ODE, but I cannot find an easy way anywhere. I also tried using Mathematica, but it also does not provide me with a solution. $\frac{dx}{dt}=-k_1 x+(1-x)k_2 e^{-k_3 t}$, $x(0)=0$ However on simulating I find a smooth curve increasing till a threshold time and later on, decreasing. Can someone help me solve this ODE?","I was trying to solve the following ODE, but I cannot find an easy way anywhere. I also tried using Mathematica, but it also does not provide me with a solution. $\frac{dx}{dt}=-k_1 x+(1-x)k_2 e^{-k_3 t}$, $x(0)=0$ However on simulating I find a smooth curve increasing till a threshold time and later on, decreasing. Can someone help me solve this ODE?",,"['ordinary-differential-equations', 'dynamical-systems']"
8,Problem on Linear Differential Equations,Problem on Linear Differential Equations,,"If $y_1,y_2$ be two solutions of $y''+p(x)y'+q(x)y=0$, show that the Wronskian can be expressed as $\displaystyle W(y_1,y_2)=ce^{-\int pdx}$ My thoughts: Wronskian, $\displaystyle W= \begin{vmatrix} y_1 &y_2 \\  y_1' &y_2'  \end{vmatrix}$ Now, If p, q are given specifically, I can try to find the complementary function (C.F.) and move on from there. But here, p and q are general functions. How do I proceed to solve this problem ? Any hints ?","If $y_1,y_2$ be two solutions of $y''+p(x)y'+q(x)y=0$, show that the Wronskian can be expressed as $\displaystyle W(y_1,y_2)=ce^{-\int pdx}$ My thoughts: Wronskian, $\displaystyle W= \begin{vmatrix} y_1 &y_2 \\  y_1' &y_2'  \end{vmatrix}$ Now, If p, q are given specifically, I can try to find the complementary function (C.F.) and move on from there. But here, p and q are general functions. How do I proceed to solve this problem ? Any hints ?",,['ordinary-differential-equations']
9,Uniformly bounded solution to a dynamical system.,Uniformly bounded solution to a dynamical system.,,"Prove that all the solution of the problem \begin{equation} \left\{ \begin{array}{ll}       \dot{x}=15x+x^2+y-3x^3\\       \dot{y}=4y^2-x-5y^3, \\ \end{array}\right. \end{equation} are uniformly bounded for $t\geq 0$. I don't even know where to start: I tried bounding $\|(x,y)\|_2^2$ or its derivative with respect to $t$, but my attempts were unsuccessfull.","Prove that all the solution of the problem \begin{equation} \left\{ \begin{array}{ll}       \dot{x}=15x+x^2+y-3x^3\\       \dot{y}=4y^2-x-5y^3, \\ \end{array}\right. \end{equation} are uniformly bounded for $t\geq 0$. I don't even know where to start: I tried bounding $\|(x,y)\|_2^2$ or its derivative with respect to $t$, but my attempts were unsuccessfull.",,['ordinary-differential-equations']
10,"About a Property of maximal solutions of separable ODE's $y'=g(x)h(y)$ for locally Lipschitz $h : U\to\mathbb R$, $U$ open","About a Property of maximal solutions of separable ODE's  for locally Lipschitz ,  open",y'=g(x)h(y) h : U\to\mathbb R U,"Theorem: Let $\varphi : (a,b) \to \mathbb R$ be a maximal solution of the IVP $$  y'(x) = g(x) \cdot h(y(x)), \quad y(x_0) = y_0 \quad (1) $$ with continuous functions $g : I \to \mathbb R$ and $h : U \to \mathbb R$ with open intervals $I, U$ and $(x_0, y_0) \in I \times U$. Let $h : U \to \mathbb R$ be locally Lipschitz-continuous. If $b$ is not the right endpoint of the interval $I$, then there exists for every $\beta < b$ and every compact set $K \subseteq U$ a $\xi \in (\beta; b)$ with $\varphi(\xi) \notin K$. An analogous result holds for the left endpoint $a$ of $I$. Said differently: If $(a,b) \ne I$, then $\varphi$ leaves every compactum, or if all values of $\varphi$ are contained in a compact set, then $\varphi$ is defined on the whole interval $I$. I know that the IVP (1) has, in a sufficiently small open interval around $x_0$, a solution (existence result), and further if $h : U \to \mathbb R$ locally Lipschitz, then on every interval around $x_0$ the IVP (1) has at most one solution (uniqueness result). Also a solution $\varphi : (a,b) \to \mathbb R$ of the IVP (1) is called maximal , if for every other solution $\psi : (\alpha, \beta) \to \mathbb R$ we have: i) $(\alpha,\beta) \subseteq (a,b)$ and ii) $\psi(x) = \varphi(x)$ for $x \in (\alpha,\beta)$. For $h : U \to \mathbb R$ locally Lipschitz, then it has at most one maximal solution. In my textbook the following picture is given: And below it is written that all those solution curves go from ""boundary to boundary"", meaning the property mentioned in the above Theorem. But as I understand it, the lowermost curve does not fulfills the property, because it's ""range"" is contained in a compact set, or have I misunderstood something? I am still a little bit intimidated by this Theorem; for what is it used, what is its essential meaning? Has anybody examples or something for illustration? In my textbook after this Theorem the chapter closes and nothing more is mentioned... Thanks!","Theorem: Let $\varphi : (a,b) \to \mathbb R$ be a maximal solution of the IVP $$  y'(x) = g(x) \cdot h(y(x)), \quad y(x_0) = y_0 \quad (1) $$ with continuous functions $g : I \to \mathbb R$ and $h : U \to \mathbb R$ with open intervals $I, U$ and $(x_0, y_0) \in I \times U$. Let $h : U \to \mathbb R$ be locally Lipschitz-continuous. If $b$ is not the right endpoint of the interval $I$, then there exists for every $\beta < b$ and every compact set $K \subseteq U$ a $\xi \in (\beta; b)$ with $\varphi(\xi) \notin K$. An analogous result holds for the left endpoint $a$ of $I$. Said differently: If $(a,b) \ne I$, then $\varphi$ leaves every compactum, or if all values of $\varphi$ are contained in a compact set, then $\varphi$ is defined on the whole interval $I$. I know that the IVP (1) has, in a sufficiently small open interval around $x_0$, a solution (existence result), and further if $h : U \to \mathbb R$ locally Lipschitz, then on every interval around $x_0$ the IVP (1) has at most one solution (uniqueness result). Also a solution $\varphi : (a,b) \to \mathbb R$ of the IVP (1) is called maximal , if for every other solution $\psi : (\alpha, \beta) \to \mathbb R$ we have: i) $(\alpha,\beta) \subseteq (a,b)$ and ii) $\psi(x) = \varphi(x)$ for $x \in (\alpha,\beta)$. For $h : U \to \mathbb R$ locally Lipschitz, then it has at most one maximal solution. In my textbook the following picture is given: And below it is written that all those solution curves go from ""boundary to boundary"", meaning the property mentioned in the above Theorem. But as I understand it, the lowermost curve does not fulfills the property, because it's ""range"" is contained in a compact set, or have I misunderstood something? I am still a little bit intimidated by this Theorem; for what is it used, what is its essential meaning? Has anybody examples or something for illustration? In my textbook after this Theorem the chapter closes and nothing more is mentioned... Thanks!",,"['analysis', 'functional-analysis', 'ordinary-differential-equations', 'intuition', 'dynamical-systems']"
11,Solve $(x^2y^3+y)dx+(x^2y^3-x)dy=0$,Solve,(x^2y^3+y)dx+(x^2y^3-x)dy=0,"$$(x^2y^3+y)dx+(x^2y^3-x)dy=0$$ So: $$\frac{\partial P}{\partial y}=3x^2y^2+1$$ $$\frac{\partial Q}{\partial x}=2xy^3-1$$ The question is how do I find the integrating factor when neither $$\frac{1}{Q}(\frac{\partial P}{\partial y}-\frac{\partial Q}{\partial x})$$ is not a function of $x$ alone, nor $$\frac{1}{P}(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y})$$ is not a function of $y$ alone. I guess I have to take a more generic approach then, but maybe there's an easier way that I didn't notice.","$$(x^2y^3+y)dx+(x^2y^3-x)dy=0$$ So: $$\frac{\partial P}{\partial y}=3x^2y^2+1$$ $$\frac{\partial Q}{\partial x}=2xy^3-1$$ The question is how do I find the integrating factor when neither $$\frac{1}{Q}(\frac{\partial P}{\partial y}-\frac{\partial Q}{\partial x})$$ is not a function of $x$ alone, nor $$\frac{1}{P}(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y})$$ is not a function of $y$ alone. I guess I have to take a more generic approach then, but maybe there's an easier way that I didn't notice.",,['ordinary-differential-equations']
12,ODE $d^2y/dx^2 + y/a^2 = u(x)$,ODE,d^2y/dx^2 + y/a^2 = u(x),Does the following ODE: $$d^2y/dx^2 + y/a^2 = u(x)$$ have a solution? where $u(x)$ is the step function and a is constant.,Does the following ODE: $$d^2y/dx^2 + y/a^2 = u(x)$$ have a solution? where $u(x)$ is the step function and a is constant.,,"['integration', 'analysis', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
13,How to determine new dimensionless variables when non-dimensionsionalizing a system of ODEs?,How to determine new dimensionless variables when non-dimensionsionalizing a system of ODEs?,,"I am following this a research paper called ""Evolution of Within-Host Antibiotic Resistance in Gonorrhea"", which can be found here . I have one question regarding the method of non-dimensionalization. How do they come up with the new dimensionless state and time variables? Here's what the parameters and variables mean: $S$, $R_l$, and $R_h$ are the bacteria strains. $b_s$, $b_l$, and $b_h$ are birth rates of the bacteria strains. $\mu$ is the natural death rate. $i$ is the immune system clearing rate. $T$ is the treatment clearing rate. $m_1$, $m_2$, $p$ are mutation rates. $\alpha$ is the level of drug resistance. The system is as follows, \begin{align} \frac{dS}{dt} &= S(b_s - \mu - i - T - m_1) - pS(R_l + R_h) \\ \frac{dR_l}{dt} &= R_l(b_l - \mu - i - \alpha T - m_2) + m_1 S + pR_l(S - R_h) \\  \frac{dR_h}{dt} &= R_h(b_h - \mu - i - \alpha T) + m_2R_l + pR_h(S + R_l)  \end{align} Their new dimensionless state and time variables are: \begin{array}{cc}  s = S\frac{p}{m_1} & l = R_l\frac{p}{m_1} & h = R_h\frac{p}{m_1} & \tau = tm_1 \\  \end{array} With these new variables, the new system is  \begin{align} \frac{ds}{d\tau} &= sA - s(l+h) \\ \frac{dl}{d\tau} &= l(B-M) + s + l(s-h)\\ \frac{dh}{d\tau} &= hC + lM + h(s+l) \end{align} where  \begin{array}{cc} A = \frac{a}{m_1} - 1 & B = \frac{b}{m_1} & C = \frac{c}{m_1} & M = \frac{m_2}{m_1} \\ \end{array} \begin{array}{cc} a = b_s - \mu - i - T & b = b_l - \mu - u -\alpha T & c = b_h - \mu - i - \alpha T \end{array} I know how to end up with the new model by using the fact that $$\frac{dS}{dt} = \frac{dS}{d\tau} \frac{d\tau}{dt}$$ How do they know to make $s = S\frac{p}{m_1}$ and so on?","I am following this a research paper called ""Evolution of Within-Host Antibiotic Resistance in Gonorrhea"", which can be found here . I have one question regarding the method of non-dimensionalization. How do they come up with the new dimensionless state and time variables? Here's what the parameters and variables mean: $S$, $R_l$, and $R_h$ are the bacteria strains. $b_s$, $b_l$, and $b_h$ are birth rates of the bacteria strains. $\mu$ is the natural death rate. $i$ is the immune system clearing rate. $T$ is the treatment clearing rate. $m_1$, $m_2$, $p$ are mutation rates. $\alpha$ is the level of drug resistance. The system is as follows, \begin{align} \frac{dS}{dt} &= S(b_s - \mu - i - T - m_1) - pS(R_l + R_h) \\ \frac{dR_l}{dt} &= R_l(b_l - \mu - i - \alpha T - m_2) + m_1 S + pR_l(S - R_h) \\  \frac{dR_h}{dt} &= R_h(b_h - \mu - i - \alpha T) + m_2R_l + pR_h(S + R_l)  \end{align} Their new dimensionless state and time variables are: \begin{array}{cc}  s = S\frac{p}{m_1} & l = R_l\frac{p}{m_1} & h = R_h\frac{p}{m_1} & \tau = tm_1 \\  \end{array} With these new variables, the new system is  \begin{align} \frac{ds}{d\tau} &= sA - s(l+h) \\ \frac{dl}{d\tau} &= l(B-M) + s + l(s-h)\\ \frac{dh}{d\tau} &= hC + lM + h(s+l) \end{align} where  \begin{array}{cc} A = \frac{a}{m_1} - 1 & B = \frac{b}{m_1} & C = \frac{c}{m_1} & M = \frac{m_2}{m_1} \\ \end{array} \begin{array}{cc} a = b_s - \mu - i - T & b = b_l - \mu - u -\alpha T & c = b_h - \mu - i - \alpha T \end{array} I know how to end up with the new model by using the fact that $$\frac{dS}{dt} = \frac{dS}{d\tau} \frac{d\tau}{dt}$$ How do they know to make $s = S\frac{p}{m_1}$ and so on?",,"['ordinary-differential-equations', 'mathematical-modeling']"
14,"laplace transform, initial value problem","laplace transform, initial value problem",,"I have tried this problem multiple times, I have the solution but not the steps. I keep getting the wrong answer. I believe it may be in the algebra after I have taken the Laplace on both sides. $y''+y = \sin(t) ;\:\: y(0) = 1, \:\: y'(0) = -1 $","I have tried this problem multiple times, I have the solution but not the steps. I keep getting the wrong answer. I believe it may be in the algebra after I have taken the Laplace on both sides. $y''+y = \sin(t) ;\:\: y(0) = 1, \:\: y'(0) = -1 $",,['ordinary-differential-equations']
15,Intuition behind convolution identity for Laplace transforms,Intuition behind convolution identity for Laplace transforms,,"Convolutions, relatively speaking, are fairly straightforward for simple systems (from an applied perspective), but I cannot, at all, find the intuition behind the Laplace identity for convolutions. That is: $$ \mathcal{L}\{f\star g\}=\mathcal{L}\{f\}\cdot \mathcal{L}\{g\} $$ How is it possible to make sense of this? Perhaps there is some probability theory example that sheds light on it? Thanks in advance.","Convolutions, relatively speaking, are fairly straightforward for simple systems (from an applied perspective), but I cannot, at all, find the intuition behind the Laplace identity for convolutions. That is: $$ \mathcal{L}\{f\star g\}=\mathcal{L}\{f\}\cdot \mathcal{L}\{g\} $$ How is it possible to make sense of this? Perhaps there is some probability theory example that sheds light on it? Thanks in advance.",,"['functional-analysis', 'ordinary-differential-equations', 'probability-theory', 'laplace-transform', 'convolution']"
16,Finding Fixed Points for Coupled ODE,Finding Fixed Points for Coupled ODE,,"I have two coupled equations $$\frac{dx}{dt}=\gamma x\left(1 - \frac{\alpha x+\beta y}{N}\right)$$ $$\frac{dy}{dt}=\theta y\left(1 - \frac{\alpha x+\beta y}{N}\right)$$ where $\gamma , \alpha , \beta , \theta \space and \space N$ are constants Next, after I nondimensionalise them by setting $$t=\frac{\tau}{\gamma}, \quad \alpha x = X, \space \beta y = Y$$ I get $$\frac{dX}{d\tau}=X\left(1 - \frac{X+Y}{N}\right)$$ $$\frac{dY}{d\tau}=\kappa Y\left(1 - \frac{X+Y}{N}\right)$$ Now I attempt to find the fixed point by setting both ODEs = $0$ I get $X = 0 \text{ or } X = N - Y $ from the first ODE. And when I attempt to substitute $ X  = N - Y $ into the second ODE so that I solve them simuteneously, I get $\kappa Y(1-1) = 0 $. So I have difficulty finding the fixed point at $ X  = N - Y $ Is there another method someone could recommend? Or is there a problem in my nondimensionalization methods?","I have two coupled equations $$\frac{dx}{dt}=\gamma x\left(1 - \frac{\alpha x+\beta y}{N}\right)$$ $$\frac{dy}{dt}=\theta y\left(1 - \frac{\alpha x+\beta y}{N}\right)$$ where $\gamma , \alpha , \beta , \theta \space and \space N$ are constants Next, after I nondimensionalise them by setting $$t=\frac{\tau}{\gamma}, \quad \alpha x = X, \space \beta y = Y$$ I get $$\frac{dX}{d\tau}=X\left(1 - \frac{X+Y}{N}\right)$$ $$\frac{dY}{d\tau}=\kappa Y\left(1 - \frac{X+Y}{N}\right)$$ Now I attempt to find the fixed point by setting both ODEs = $0$ I get $X = 0 \text{ or } X = N - Y $ from the first ODE. And when I attempt to substitute $ X  = N - Y $ into the second ODE so that I solve them simuteneously, I get $\kappa Y(1-1) = 0 $. So I have difficulty finding the fixed point at $ X  = N - Y $ Is there another method someone could recommend? Or is there a problem in my nondimensionalization methods?",,['ordinary-differential-equations']
17,deriving second order transfer function from spring mass damper system..,deriving second order transfer function from spring mass damper system..,,"I am having a hard time understanding how a differential equation based on a spring mass damper system $$ m\ddot{x} + b\dot{x} + kx = 0$$ can be described as an second order transfer function for an inpulse response, which looks something like this  $$\frac{(\omega_n)^2}{s^2+2\zeta\omega_n + (\omega_n)^2}$$","I am having a hard time understanding how a differential equation based on a spring mass damper system $$ m\ddot{x} + b\dot{x} + kx = 0$$ can be described as an second order transfer function for an inpulse response, which looks something like this  $$\frac{(\omega_n)^2}{s^2+2\zeta\omega_n + (\omega_n)^2}$$",,"['ordinary-differential-equations', 'derivatives', 'laplace-transform', 'control-theory']"
18,solve $y''-2y'-y=\sin{3x}$,solve,y''-2y'-y=\sin{3x},"$$y''-2y'-y=\sin{3x}, \quad y = y(x).$$ We find $y_h = Ae^{(1-\sqrt{2})x}+Be^{(1+\sqrt{2})x}$ However I have trouble with finding $y_p$. Using the helping function: $$ u''-2u'-u = e^{\mathrm{i} \, 3x}, \quad \mathrm{i}^2 = -1,$$ and then $$u = e^{\mathrm{i} \, 3x}z$$ I get $z''-2z'+6 \mathrm{i} \, z'-z-6 \mathrm{i} \, z-8=0$,  which I believe is clearly wrong and too complex. The solution should be $$y = Ae^{(1-\sqrt{2})x}+Be^{(1+\sqrt{2})x} + \frac{1}{68}\left(3\cos{3x} - 5 \sin{3x} \right)$$","$$y''-2y'-y=\sin{3x}, \quad y = y(x).$$ We find $y_h = Ae^{(1-\sqrt{2})x}+Be^{(1+\sqrt{2})x}$ However I have trouble with finding $y_p$. Using the helping function: $$ u''-2u'-u = e^{\mathrm{i} \, 3x}, \quad \mathrm{i}^2 = -1,$$ and then $$u = e^{\mathrm{i} \, 3x}z$$ I get $z''-2z'+6 \mathrm{i} \, z'-z-6 \mathrm{i} \, z-8=0$,  which I believe is clearly wrong and too complex. The solution should be $$y = Ae^{(1-\sqrt{2})x}+Be^{(1+\sqrt{2})x} + \frac{1}{68}\left(3\cos{3x} - 5 \sin{3x} \right)$$",,['ordinary-differential-equations']
19,Analytical solution to a nonlinear ODE,Analytical solution to a nonlinear ODE,,How might I analytically solve the following differential equation? $$yy'' = y' + y^3$$ I've tried certain substitutions ($y = ux$ etc.) but none of them work.,How might I analytically solve the following differential equation? $$yy'' = y' + y^3$$ I've tried certain substitutions ($y = ux$ etc.) but none of them work.,,"['ordinary-differential-equations', 'contest-math']"
20,Optimal String Shape Problem,Optimal String Shape Problem,,"So here is the problem I am working on, Given a curve of length L connecting the points (0,1) and (1,0) find an expression for the equation of the curve that minimizes the area underneath it. In other words, given that: $$ y(0) = 1, y(1) = 0$$ minimize $$ \int_0^1{y(x)} dx $$ subject to: $$ \int_0^1{\sqrt{1 + (y')^2} } dx = l $$ So my initial strategy was to do this problem using functional lagrange multipliers (I don't know if thats a real thing but my intuition is that it should work) whereas I make an optimization function $$ \omega = \int_0^1{y(x)} dx + \lambda(\int_0^1{\sqrt{1 + (y')^2} } dx - l) $$ Which can be rewritten as: $$ \omega = \int_0^1{[y(x) + \lambda \sqrt{1 + (y')^2} - \lambda l]} dx  $$ We now compute the functional gradient of it as $$ \nabla \omega  = 0 $$ ---> $$ \frac{\delta \omega}{\delta y} = 0 $$ $$  \frac{\delta \omega}{\delta \lambda} = 0 $$ Which results in $$ 1  - \lambda \frac{y''}{(1 +  (y')^2)^{\frac{3}{2}}} = 0 $$ $$ \int_0^1{\sqrt{1 + (y')^2} } dx = l $$ Now i'm not sure what to do at this stage... Help would be appreciated!! (My problem is I was trying to eliminate $\lambda$ and solve for y as a single differential equation which didn't end up happening As per Alex's Answer The first equation yields that $$ 1  - \lambda \frac{y''}{(1 +  (y')^2)^{\frac{3}{2}}} = 0 $$ We can set $y' = u$ $$ 1  - \lambda \frac{u'}{(1 +  (u)^2)^{\frac{3}{2}}} = 0 $$ Which yields $$ \frac{1}{\lambda} = \frac{1}{(1 +  (u)^2)^{\frac{3}{2}}} u'$$ We can hit this with the Leibniz Chain Rule. To find $$ \int \frac{1}{\lambda} dx = \frac{u}{(1+u^2)^{\frac{1}{2}}} $$ Furthermore that yields $$ \left(\int \frac{1}{\lambda} dx \right)^2 (1 + u^2) = u^2 $$ Which allows us to solve for $u$ as $$ \frac{\int \frac{1}{\lambda} dx}{\left( 1 - \left(\int \frac{1}{\lambda} dx \right)^2 \right)^{\frac{1}{2}} }  = u$$ Then it follows that $$ y = \int  \frac{\int \frac{1}{\lambda} dx}{\left( 1 - \left(\int \frac{1}{\lambda} dx \right)^2 \right)^{\frac{1}{2}} } dx $$ If we add in constants $$ y = \int  \frac{\int \frac{1}{\lambda} dx+C_1}{\left( 1 - \left(\int \frac{1}{\lambda} dx + C_1 \right)^2 \right)^{\frac{1}{2}} } dx + C_2 $$ Now we have the constraints that $$ y(0) = 1, y(1) = 0, \int_0^1{\sqrt{1 + (y')^2} } dx = l $$ Given that $\lambda$ is a function i'm not sure how to proceed.","So here is the problem I am working on, Given a curve of length L connecting the points (0,1) and (1,0) find an expression for the equation of the curve that minimizes the area underneath it. In other words, given that: $$ y(0) = 1, y(1) = 0$$ minimize $$ \int_0^1{y(x)} dx $$ subject to: $$ \int_0^1{\sqrt{1 + (y')^2} } dx = l $$ So my initial strategy was to do this problem using functional lagrange multipliers (I don't know if thats a real thing but my intuition is that it should work) whereas I make an optimization function $$ \omega = \int_0^1{y(x)} dx + \lambda(\int_0^1{\sqrt{1 + (y')^2} } dx - l) $$ Which can be rewritten as: $$ \omega = \int_0^1{[y(x) + \lambda \sqrt{1 + (y')^2} - \lambda l]} dx  $$ We now compute the functional gradient of it as $$ \nabla \omega  = 0 $$ ---> $$ \frac{\delta \omega}{\delta y} = 0 $$ $$  \frac{\delta \omega}{\delta \lambda} = 0 $$ Which results in $$ 1  - \lambda \frac{y''}{(1 +  (y')^2)^{\frac{3}{2}}} = 0 $$ $$ \int_0^1{\sqrt{1 + (y')^2} } dx = l $$ Now i'm not sure what to do at this stage... Help would be appreciated!! (My problem is I was trying to eliminate $\lambda$ and solve for y as a single differential equation which didn't end up happening As per Alex's Answer The first equation yields that $$ 1  - \lambda \frac{y''}{(1 +  (y')^2)^{\frac{3}{2}}} = 0 $$ We can set $y' = u$ $$ 1  - \lambda \frac{u'}{(1 +  (u)^2)^{\frac{3}{2}}} = 0 $$ Which yields $$ \frac{1}{\lambda} = \frac{1}{(1 +  (u)^2)^{\frac{3}{2}}} u'$$ We can hit this with the Leibniz Chain Rule. To find $$ \int \frac{1}{\lambda} dx = \frac{u}{(1+u^2)^{\frac{1}{2}}} $$ Furthermore that yields $$ \left(\int \frac{1}{\lambda} dx \right)^2 (1 + u^2) = u^2 $$ Which allows us to solve for $u$ as $$ \frac{\int \frac{1}{\lambda} dx}{\left( 1 - \left(\int \frac{1}{\lambda} dx \right)^2 \right)^{\frac{1}{2}} }  = u$$ Then it follows that $$ y = \int  \frac{\int \frac{1}{\lambda} dx}{\left( 1 - \left(\int \frac{1}{\lambda} dx \right)^2 \right)^{\frac{1}{2}} } dx $$ If we add in constants $$ y = \int  \frac{\int \frac{1}{\lambda} dx+C_1}{\left( 1 - \left(\int \frac{1}{\lambda} dx + C_1 \right)^2 \right)^{\frac{1}{2}} } dx + C_2 $$ Now we have the constraints that $$ y(0) = 1, y(1) = 0, \int_0^1{\sqrt{1 + (y')^2} } dx = l $$ Given that $\lambda$ is a function i'm not sure how to proceed.",,"['functional-analysis', 'ordinary-differential-equations', 'calculus-of-variations', 'nonlinear-optimization', 'lagrange-multiplier']"
21,Stability of equilibrium in diff EQ symbiotic growth model,Stability of equilibrium in diff EQ symbiotic growth model,,"Consider the following system, which is designed to model a symbiotic   relationship between two species: $$\dot{x}(t) = x(\epsilon_1  -\alpha_1 x + \beta_1 y)\\  \dot{y}(t) = y (\epsilon_2 + \beta_2x - \alpha_2y)$$ Here $\alpha_i,\, \beta_i, \, \epsilon_i\,$ are positive constants. Under what circumstances is there an equilibrium point with both    species present? What can you say about the stability of the equilibrium? For an equilibrium point with both species present, we need $x(\epsilon_1 -\alpha_1 x + \beta_1 y)=y (\epsilon_2 + \beta_2x - \alpha_2y)=0$ and $x,y>0$. This gives us a linear system $$ \left[   \begin{matrix} \alpha_1 & -\beta_1\\ -\beta_2 & \alpha_2 \end{matrix} \right] \left[ \begin{matrix} x\\y \end{matrix} \right] = \left[ \begin{matrix} \epsilon_1\\\epsilon_2 \end{matrix} \right]  $$ I claim that if $\alpha_1\alpha_2 - \beta_1\beta_2 = 0$, there are no solutions with $x,y>0$.  If $\alpha_1\alpha_2 - \beta_1\beta_2 \neq 0$, this has a unique solution $$ x_{\text{eq}} = \frac{1}{\alpha_1\alpha_2 - \beta_1\beta_2}\left( \alpha_2 \epsilon_1 + \beta_1 \epsilon_2 \right)\\ y_{\text{eq}}= \frac{1}{\alpha_1\alpha_2 - \beta_1\beta_2}\left( \beta_2\epsilon_1 + \alpha_1 \epsilon_2 \right), $$ and if we want $x,y>0$, we will need $\alpha_1\alpha_2 - \beta_1\beta_2>0$. So an equilibrium point with both species present exists iff $\alpha_1\alpha_2 - \beta_1\beta_2>0$. Now, the stability of this point is the part I have a question about. I conjecture that any ball around the equilibrium point which stays in the first quadrant will be positive invariant, but I don't really know. I had no luck looking for a Lyapunov function. Any ideas?","Consider the following system, which is designed to model a symbiotic   relationship between two species: $$\dot{x}(t) = x(\epsilon_1  -\alpha_1 x + \beta_1 y)\\  \dot{y}(t) = y (\epsilon_2 + \beta_2x - \alpha_2y)$$ Here $\alpha_i,\, \beta_i, \, \epsilon_i\,$ are positive constants. Under what circumstances is there an equilibrium point with both    species present? What can you say about the stability of the equilibrium? For an equilibrium point with both species present, we need $x(\epsilon_1 -\alpha_1 x + \beta_1 y)=y (\epsilon_2 + \beta_2x - \alpha_2y)=0$ and $x,y>0$. This gives us a linear system $$ \left[   \begin{matrix} \alpha_1 & -\beta_1\\ -\beta_2 & \alpha_2 \end{matrix} \right] \left[ \begin{matrix} x\\y \end{matrix} \right] = \left[ \begin{matrix} \epsilon_1\\\epsilon_2 \end{matrix} \right]  $$ I claim that if $\alpha_1\alpha_2 - \beta_1\beta_2 = 0$, there are no solutions with $x,y>0$.  If $\alpha_1\alpha_2 - \beta_1\beta_2 \neq 0$, this has a unique solution $$ x_{\text{eq}} = \frac{1}{\alpha_1\alpha_2 - \beta_1\beta_2}\left( \alpha_2 \epsilon_1 + \beta_1 \epsilon_2 \right)\\ y_{\text{eq}}= \frac{1}{\alpha_1\alpha_2 - \beta_1\beta_2}\left( \beta_2\epsilon_1 + \alpha_1 \epsilon_2 \right), $$ and if we want $x,y>0$, we will need $\alpha_1\alpha_2 - \beta_1\beta_2>0$. So an equilibrium point with both species present exists iff $\alpha_1\alpha_2 - \beta_1\beta_2>0$. Now, the stability of this point is the part I have a question about. I conjecture that any ball around the equilibrium point which stays in the first quadrant will be positive invariant, but I don't really know. I had no luck looking for a Lyapunov function. Any ideas?",,['ordinary-differential-equations']
22,Uniqueness of the ODE solutions,Uniqueness of the ODE solutions,,"Say we have a continuous function (perhaps not everywhere differentiable) that satisfies an ODE $y^\prime(x)=h(y(x),x)$ for almost all $x$ in $[0,1]$. Are the any references for that deal with basic ODE questions (existence, uniqueness) for these class of solutions? If so which ones you would recommend?","Say we have a continuous function (perhaps not everywhere differentiable) that satisfies an ODE $y^\prime(x)=h(y(x),x)$ for almost all $x$ in $[0,1]$. Are the any references for that deal with basic ODE questions (existence, uniqueness) for these class of solutions? If so which ones you would recommend?",,"['analysis', 'ordinary-differential-equations', 'reference-request']"
23,Laplace Transform Piecewise Function,Laplace Transform Piecewise Function,,"I've never seen these types of bounds on a piecewise function of a Laplace transform before, can someone help explain how to solve this problem, particularly the Laplace transform of g(t)? Thanks in advance.","I've never seen these types of bounds on a piecewise function of a Laplace transform before, can someone help explain how to solve this problem, particularly the Laplace transform of g(t)? Thanks in advance.",,"['ordinary-differential-equations', 'laplace-transform']"
24,"Solving IVP $y'=t|y|^\alpha, \ y(0)=1$",Solving IVP,"y'=t|y|^\alpha, \ y(0)=1","Intro : This is a follow up to my post Application of Picard-Lindelöf to determine uniqueness of a solution to an IVP , where I am trying to verify that the below IVP has a unique solution in some interval that includes $0$. This is a nightmare and doesn't work out for me. So I try to go the opposite direction and see whether or not I can come up with something. I hope it's okay to make this kind of a follow up post, if not please feel free to close it. I also hope someone finds the time to quickly look through this post and tell me whether or not my reasoning is correct. Problem : Solve the following IVP $$ \begin{cases} y' &= t|y|^\alpha \\ y(0)&=1 \end{cases} $$ where $ \alpha \neq 1$ My approach : This is the first time I am dealing with an absolute value function, so I guess it is reasonable to look into the two cases where $y$ is positive and $y$ is (strictly) negative. I first want to show that I can divide through the expression $y^\alpha$. Let $y=0$ then $y'=0 \implies y=c$ but $y(0)=1$ and therefore $c=1 \implies y=1$ which is a contradiction, thus $y \neq 0$. Then I start solving the differential equation. $$ \frac{dy}{dt}=ty^\alpha \iff \frac{dy}{y^\alpha}=tdt \implies \frac{y^{1- \alpha}}{1-\alpha}=\frac{t^2}{2}+C \\ \implies y = \left( \frac{(1-\alpha)t^2}{2}+K \right)^\frac{1}{1-\alpha} $$ Applying initial conditions would lead to $K=1$. Is that correct? Now the same for $y$ negative of course which I will save for now, just substituting $|y|=-y$ into the original IVP. How could these results resemble uniqueness? Update : A sudden idea striking me, it seems very tedious but do I need to do a check analysis for $\alpha$? Meaning do the above calculations for $\alpha < 0$, likewise $\alpha > 1$ and $\alpha \in ]0,1[$ ?","Intro : This is a follow up to my post Application of Picard-Lindelöf to determine uniqueness of a solution to an IVP , where I am trying to verify that the below IVP has a unique solution in some interval that includes $0$. This is a nightmare and doesn't work out for me. So I try to go the opposite direction and see whether or not I can come up with something. I hope it's okay to make this kind of a follow up post, if not please feel free to close it. I also hope someone finds the time to quickly look through this post and tell me whether or not my reasoning is correct. Problem : Solve the following IVP $$ \begin{cases} y' &= t|y|^\alpha \\ y(0)&=1 \end{cases} $$ where $ \alpha \neq 1$ My approach : This is the first time I am dealing with an absolute value function, so I guess it is reasonable to look into the two cases where $y$ is positive and $y$ is (strictly) negative. I first want to show that I can divide through the expression $y^\alpha$. Let $y=0$ then $y'=0 \implies y=c$ but $y(0)=1$ and therefore $c=1 \implies y=1$ which is a contradiction, thus $y \neq 0$. Then I start solving the differential equation. $$ \frac{dy}{dt}=ty^\alpha \iff \frac{dy}{y^\alpha}=tdt \implies \frac{y^{1- \alpha}}{1-\alpha}=\frac{t^2}{2}+C \\ \implies y = \left( \frac{(1-\alpha)t^2}{2}+K \right)^\frac{1}{1-\alpha} $$ Applying initial conditions would lead to $K=1$. Is that correct? Now the same for $y$ negative of course which I will save for now, just substituting $|y|=-y$ into the original IVP. How could these results resemble uniqueness? Update : A sudden idea striking me, it seems very tedious but do I need to do a check analysis for $\alpha$? Meaning do the above calculations for $\alpha < 0$, likewise $\alpha > 1$ and $\alpha \in ]0,1[$ ?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
25,Question about particular solutions of $y''+4xy'+Q(x)y=0$.,Question about particular solutions of .,y''+4xy'+Q(x)y=0,"If we know that $$y''+4xy'+Q(x)y=0$$  have two solutions of the form: $y_1=u(x)$ and $y_2=xu(x)$, where $u(0)=1$. How can we determine $u(x)$ and $Q(x)$ explicitly? (This problem is extracted from: Tom Apostol Calculus II.) If $x$ does not appear in the above equation, (i.e. just y'', y and constants) i can solve similar problems. Because in that case, i can say that if the ODE have a solution of the form $y=e^{kx}$ then the general solution is of the form $y=Ae^{k_1x}+Be^{k_2x}$, etc. Can i do something similar for the above equation? Any help or suggestion is welcome. Thanks.","If we know that $$y''+4xy'+Q(x)y=0$$  have two solutions of the form: $y_1=u(x)$ and $y_2=xu(x)$, where $u(0)=1$. How can we determine $u(x)$ and $Q(x)$ explicitly? (This problem is extracted from: Tom Apostol Calculus II.) If $x$ does not appear in the above equation, (i.e. just y'', y and constants) i can solve similar problems. Because in that case, i can say that if the ODE have a solution of the form $y=e^{kx}$ then the general solution is of the form $y=Ae^{k_1x}+Be^{k_2x}$, etc. Can i do something similar for the above equation? Any help or suggestion is welcome. Thanks.",,['ordinary-differential-equations']
26,ODE using Weierstrass's P function,ODE using Weierstrass's P function,,"I need a hint for the following problem. ""Solve $(x')^2=x^3 − 3x^2 − 4x + 12$ with the initial with initial condition $x(0)=3$"". I know I should somehow use Weierstrass's $P$ function because it satisfies the equation $(P')^2=4P^3+aP+b$. I tried first to obtain the lattice corresponding to the elliptic curve above but without and success.","I need a hint for the following problem. ""Solve $(x')^2=x^3 − 3x^2 − 4x + 12$ with the initial with initial condition $x(0)=3$"". I know I should somehow use Weierstrass's $P$ function because it satisfies the equation $(P')^2=4P^3+aP+b$. I tried first to obtain the lattice corresponding to the elliptic curve above but without and success.",,"['ordinary-differential-equations', 'elliptic-curves']"
27,Question about separation of variables,Question about separation of variables,,"This is for the heat equation, where $$\frac{\partial U}{\partial t}-k \frac{\partial^2 U}{\partial x^2}=1$$ with the conditions $$U(0,t)=0, \; U(x,0)=0 \text{ and } \frac{\partial U}{\partial t} (3,t)=0.$$ I am trying to solve for $U(x,t)$ but am currently stuck with factoring dealing with the ""$+1$"" in the separation of variables. I started with $U(x,t)=F(x) G(t)$ then put it into the heat equation and set it equal to a constant -$\lambda^2$.  To deal with the $+1$, I moved it to the other side with the lambda but now I am can't seem to get the sine or exponential expression I need.","This is for the heat equation, where $$\frac{\partial U}{\partial t}-k \frac{\partial^2 U}{\partial x^2}=1$$ with the conditions $$U(0,t)=0, \; U(x,0)=0 \text{ and } \frac{\partial U}{\partial t} (3,t)=0.$$ I am trying to solve for $U(x,t)$ but am currently stuck with factoring dealing with the ""$+1$"" in the separation of variables. I started with $U(x,t)=F(x) G(t)$ then put it into the heat equation and set it equal to a constant -$\lambda^2$.  To deal with the $+1$, I moved it to the other side with the lambda but now I am can't seem to get the sine or exponential expression I need.",,"['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
28,Question about Poisson formula,Question about Poisson formula,,"We have the Laplace equation in polar coordinates: $$u_{rr}+\frac{1}{r}u_r+\frac{1}{r^2}u_{\theta \theta}=0, 0 \leq r <a, 0 \leq \theta \leq 2 \pi$$ With the separation of variables, the solution is in the form $u(r \theta)=R(r) \Theta(\theta)$ Then after calculations, we get: $$u(r, \theta)=\sum_{n=0}^{\infty}{[A_n \cos{(n \theta)}+B_n \sin{(n \theta)}]r^n}, 0 \leq \theta \leq 2 \pi, 0 \leq r <a$$ The boundary condition is $h(\theta)=u(r=a, \theta)$ $h(\theta)$ is a periodic function with period $ 2 \pi$, so we can write it as a Fourier series. After calculations we get the following formula (Poisson formula): $$u(r, \theta)=\frac{a^2-r^2}{2 \pi} \int_0^{2 \pi}{ \frac{h( \phi) d \phi}{a^2+r^2-2 a r \cos{(\theta-\phi)}}}$$ $$$$ $|r'|=a, |\overrightarrow{r}-\overrightarrow{r'}|^2=|\overrightarrow{r}|^2+|\overrightarrow{r'}|^2-2|\overrightarrow{r}||\overrightarrow{r'}| \cos{(\theta-\phi)}=r^2+a^2-2ar \cos{(\theta - \phi)}$ So we can write the equation also: $$u(r, \theta)=\frac{a^2-r^2}{2 \pi a} \int_{|\overrightarrow{r'}|=a}{\frac{u(\overrightarrow{r'})ds}{|\overrightarrow{r}-\overrightarrow{r'}|^2}}(*)$$ Could you tell me how we get to the relation $(*)$?? $$$$ $\frac{ds}{a}= d \phi$ For $r=0$ $$u(0)=\frac{1}{2 \pi a} \int_{| \overrightarrow{r'}|=a}{u(\overrightarrow{r'})}ds$$ This is the mean value of the field at the circumference of the circle. Could you explain me the sentence above?","We have the Laplace equation in polar coordinates: $$u_{rr}+\frac{1}{r}u_r+\frac{1}{r^2}u_{\theta \theta}=0, 0 \leq r <a, 0 \leq \theta \leq 2 \pi$$ With the separation of variables, the solution is in the form $u(r \theta)=R(r) \Theta(\theta)$ Then after calculations, we get: $$u(r, \theta)=\sum_{n=0}^{\infty}{[A_n \cos{(n \theta)}+B_n \sin{(n \theta)}]r^n}, 0 \leq \theta \leq 2 \pi, 0 \leq r <a$$ The boundary condition is $h(\theta)=u(r=a, \theta)$ $h(\theta)$ is a periodic function with period $ 2 \pi$, so we can write it as a Fourier series. After calculations we get the following formula (Poisson formula): $$u(r, \theta)=\frac{a^2-r^2}{2 \pi} \int_0^{2 \pi}{ \frac{h( \phi) d \phi}{a^2+r^2-2 a r \cos{(\theta-\phi)}}}$$ $$$$ $|r'|=a, |\overrightarrow{r}-\overrightarrow{r'}|^2=|\overrightarrow{r}|^2+|\overrightarrow{r'}|^2-2|\overrightarrow{r}||\overrightarrow{r'}| \cos{(\theta-\phi)}=r^2+a^2-2ar \cos{(\theta - \phi)}$ So we can write the equation also: $$u(r, \theta)=\frac{a^2-r^2}{2 \pi a} \int_{|\overrightarrow{r'}|=a}{\frac{u(\overrightarrow{r'})ds}{|\overrightarrow{r}-\overrightarrow{r'}|^2}}(*)$$ Could you tell me how we get to the relation $(*)$?? $$$$ $\frac{ds}{a}= d \phi$ For $r=0$ $$u(0)=\frac{1}{2 \pi a} \int_{| \overrightarrow{r'}|=a}{u(\overrightarrow{r'})}ds$$ This is the mean value of the field at the circumference of the circle. Could you explain me the sentence above?",,['ordinary-differential-equations']
29,How to find I(t)?,How to find I(t)?,,"I'm working with a SIS model for diseases. Where S stands for susceptibles, and I stands for infected. I have a situation that is modeled by the system: $$S'(t)=\frac{dS}{dt}=-\beta SI-\lambda S$$ $$I'(t)=\frac{dI}{dt}=\beta SI-\alpha I$$ Show that both S(t) and I(t) approach zero as $t \rightarrow \infty$ K is the unit that population size is measured in so we can say that $S=K-I$ and that $I=K-S$. These equations look like separable equations to me. Once I find S(t) and I(t) I can easily find the limit.","I'm working with a SIS model for diseases. Where S stands for susceptibles, and I stands for infected. I have a situation that is modeled by the system: $$S'(t)=\frac{dS}{dt}=-\beta SI-\lambda S$$ $$I'(t)=\frac{dI}{dt}=\beta SI-\alpha I$$ Show that both S(t) and I(t) approach zero as $t \rightarrow \infty$ K is the unit that population size is measured in so we can say that $S=K-I$ and that $I=K-S$. These equations look like separable equations to me. Once I find S(t) and I(t) I can easily find the limit.",,['ordinary-differential-equations']
30,Meaning of $ dx \times dy = k $,Meaning of, dx \times dy = k ,Does $ dx \times dy = k $ have a mathematical meaning? What about when considering $y = y(x)$?,Does $ dx \times dy = k $ have a mathematical meaning? What about when considering $y = y(x)$?,,"['calculus', 'ordinary-differential-equations']"
31,Good Source of Differential Equations Problems with Worked Solutions?,Good Source of Differential Equations Problems with Worked Solutions?,,"I am looking for a good source of problems for differential equations (first order, second order, laplace, convolution, systems). I find it helpful if the question has a worked solution or at the very least a solution so that I can check my work and find my mistake, or try to get the solution given. Thus far I haven't been able to find a great source, many websites have one or two questions with solutions but I need more so I can get a good amount of pratice. Textbooks/Websites etc are all welcome. Thanks","I am looking for a good source of problems for differential equations (first order, second order, laplace, convolution, systems). I find it helpful if the question has a worked solution or at the very least a solution so that I can check my work and find my mistake, or try to get the solution given. Thus far I haven't been able to find a great source, many websites have one or two questions with solutions but I need more so I can get a good amount of pratice. Textbooks/Websites etc are all welcome. Thanks",,['ordinary-differential-equations']
32,Integrate $\int^{\ln(2)}_0 (3e^u - e^{2u} - 2)\sin(nu)du$,Integrate,\int^{\ln(2)}_0 (3e^u - e^{2u} - 2)\sin(nu)du,I'm having trouble integrating this function $$\begin{equation} \begin{split} f(x) & = \int^1_0x(1-x)\sqrt{1+x}\sqrt{1+x}\sin(n \ln(1+x))/[(1+x)^2] = \\  & = \int^1_0x(1-x)(1+x)\sin(n\ln(1+x))/[(1+x)^2] = \\  & = \int^1_0x(1-x)\sin(n\ln(1+x))/[1+x] = \\  & = \int^{ln(2)}_0(e^u - 1)(1 - (e^u - 1))\sin(nu)du = \\  & = \int^{ln(2)}_0(e^u - 1)(2 - e^u)\sin(nu)du = \\  & = \int^{ln(2)}_0(2e^u -e^{2u} - 2 + e^u)\sin(nu)du = \\  & = \int^{ln(2)}_0 (3e^u - e^{2u} - 2)\sin(nu)du \\ \end{split} \end{equation}$$ Where $$u = \ln(1+x)\quad du = \frac{1}{(1+x)}\quad e^u = 1 + x\quad x = e^u - 1$$ So this last part is where I having a lot of trouble in.,I'm having trouble integrating this function $$\begin{equation} \begin{split} f(x) & = \int^1_0x(1-x)\sqrt{1+x}\sqrt{1+x}\sin(n \ln(1+x))/[(1+x)^2] = \\  & = \int^1_0x(1-x)(1+x)\sin(n\ln(1+x))/[(1+x)^2] = \\  & = \int^1_0x(1-x)\sin(n\ln(1+x))/[1+x] = \\  & = \int^{ln(2)}_0(e^u - 1)(1 - (e^u - 1))\sin(nu)du = \\  & = \int^{ln(2)}_0(e^u - 1)(2 - e^u)\sin(nu)du = \\  & = \int^{ln(2)}_0(2e^u -e^{2u} - 2 + e^u)\sin(nu)du = \\  & = \int^{ln(2)}_0 (3e^u - e^{2u} - 2)\sin(nu)du \\ \end{split} \end{equation}$$ Where $$u = \ln(1+x)\quad du = \frac{1}{(1+x)}\quad e^u = 1 + x\quad x = e^u - 1$$ So this last part is where I having a lot of trouble in.,,"['calculus', 'integration', 'ordinary-differential-equations']"
33,How can i convert nonhomogeneous ode to homogeneous ?,How can i convert nonhomogeneous ode to homogeneous ?,,"I have an equation system $$y'(t) = M(t)y(t)+h(t)$$ where $[M(t)]_{2\times2}$ square matrix  and  $[h(t)]_{2 \times1}$ is the nonhomogeneous part of the system. I can solve numerically homogeneous systems as $y'(t)=M(t)y(t)$ with my algorithm which is in my topic( Is it true for solving differential equations by getting constant coefficient matrix with magnus expansion ) but for nonhomogeneous one I am not sure how can i do it. In this paper ( http://personales.upv.es/serblaza/2012APNUMdoi.pdf ) equation(17-18), I found  some useful informations about numerical solutions of nonhomogeneous systems but, i'm still suspicious solving nonhomogeneous systems with my algorithm. Is there any way converting nonhomogeneous systems to homogeneous systems for solving numerically as above type equations ?","I have an equation system $$y'(t) = M(t)y(t)+h(t)$$ where $[M(t)]_{2\times2}$ square matrix  and  $[h(t)]_{2 \times1}$ is the nonhomogeneous part of the system. I can solve numerically homogeneous systems as $y'(t)=M(t)y(t)$ with my algorithm which is in my topic( Is it true for solving differential equations by getting constant coefficient matrix with magnus expansion ) but for nonhomogeneous one I am not sure how can i do it. In this paper ( http://personales.upv.es/serblaza/2012APNUMdoi.pdf ) equation(17-18), I found  some useful informations about numerical solutions of nonhomogeneous systems but, i'm still suspicious solving nonhomogeneous systems with my algorithm. Is there any way converting nonhomogeneous systems to homogeneous systems for solving numerically as above type equations ?",,['ordinary-differential-equations']
34,First order differential equation confusion,First order differential equation confusion,,I have a differential equation $$y' + e^{y'}-x=0$$ that I have simplified like so $$e^{y'}=x-y'$$ $$\ln e^{y'}=\ln (x-y')$$ $$y'= \ln (x-y')$$ but I do not know how to solve this further to obtain the general solution. I have done first order linear differential equation strategies so far. How should I get about doing this question with the strategies I have?,I have a differential equation $$y' + e^{y'}-x=0$$ that I have simplified like so $$e^{y'}=x-y'$$ $$\ln e^{y'}=\ln (x-y')$$ $$y'= \ln (x-y')$$ but I do not know how to solve this further to obtain the general solution. I have done first order linear differential equation strategies so far. How should I get about doing this question with the strategies I have?,,['ordinary-differential-equations']
35,I have a differential equation which solution is periodic. What can I tell about right-hand side of such equation?,I have a differential equation which solution is periodic. What can I tell about right-hand side of such equation?,,"I have equation of form $$ \frac{dx}{dt} = f(x), $$ and know and for some initial value $x_0$ its solution is periodic with unknown period. What can I tell about $f(x)$ apart from non-linearity (or $f(x) \equiv 0$)? Also, may be I'm wrong but I think that period is somehow related to ""non-linearity"" of $f(x)$, being longer for ""more non-linear"" functions.  Is there something behind this intuition?","I have equation of form $$ \frac{dx}{dt} = f(x), $$ and know and for some initial value $x_0$ its solution is periodic with unknown period. What can I tell about $f(x)$ apart from non-linearity (or $f(x) \equiv 0$)? Also, may be I'm wrong but I think that period is somehow related to ""non-linearity"" of $f(x)$, being longer for ""more non-linear"" functions.  Is there something behind this intuition?",,"['analysis', 'ordinary-differential-equations', 'periodic-functions', 'nonlinear-system']"
36,Help with transforming a second order ode into a system of first order ode's,Help with transforming a second order ode into a system of first order ode's,,"I have the following equation: \begin{align*} \frac{d^2\theta}{dt}=\alpha(\theta-1)+\beta(\theta-1)^3-\gamma\frac{d\theta}{dt} \tag{1} \end{align*} Where $\alpha, \beta, \gamma \in \mathbb{R}$. This is my solution in attempting to convert (1) into a system of first order ODE's: \begin{align*} \frac{d\theta}{dt}&=\phi \tag{2} \\ \\ \frac{d\phi}{dt} &= \alpha(\theta-1)+\beta(\theta-1)^3-\gamma\phi \tag{3} \end{align*} Is the above system correct? Also in equation (3) is it okay to have the $\theta$ term?","I have the following equation: \begin{align*} \frac{d^2\theta}{dt}=\alpha(\theta-1)+\beta(\theta-1)^3-\gamma\frac{d\theta}{dt} \tag{1} \end{align*} Where $\alpha, \beta, \gamma \in \mathbb{R}$. This is my solution in attempting to convert (1) into a system of first order ODE's: \begin{align*} \frac{d\theta}{dt}&=\phi \tag{2} \\ \\ \frac{d\phi}{dt} &= \alpha(\theta-1)+\beta(\theta-1)^3-\gamma\phi \tag{3} \end{align*} Is the above system correct? Also in equation (3) is it okay to have the $\theta$ term?",,"['ordinary-differential-equations', 'systems-of-equations']"
37,How to solve this second order DE?,How to solve this second order DE?,,"The equation is: $$y'' -2y' -3y = -3t*e^{-t}$$ So I understand that the complementary solution is: $$y_c = C_1e^{-t}+ C_2e^{3t}$$ And I'm pretty sure the general form of the particular solution is: $$y_p= (A t + B)e^{-t}$$ It's either that or: $$y_p = (A  t^2+B  t)  e^{-t}$$ I tried finding A and B with both options for $y_p$. With the second, I couldn't find a solution at all. With the first however, I did get $A = (3/2)  t$, which I found is wrong, according to the back of the book. Can anyone help me find out what I've done wrong? I spent over an hour on this and I just can't find anything else to try. Thanks.","The equation is: $$y'' -2y' -3y = -3t*e^{-t}$$ So I understand that the complementary solution is: $$y_c = C_1e^{-t}+ C_2e^{3t}$$ And I'm pretty sure the general form of the particular solution is: $$y_p= (A t + B)e^{-t}$$ It's either that or: $$y_p = (A  t^2+B  t)  e^{-t}$$ I tried finding A and B with both options for $y_p$. With the second, I couldn't find a solution at all. With the first however, I did get $A = (3/2)  t$, which I found is wrong, according to the back of the book. Can anyone help me find out what I've done wrong? I spent over an hour on this and I just can't find anything else to try. Thanks.",,['ordinary-differential-equations']
38,Second Order Derivative of a function $f:R^2\to R^2$,Second Order Derivative of a function,f:R^2\to R^2,"The Exercise: My Work: Part 1: $$ Df=\left( \begin{array}{ccc} D_1f_1 & D_2f_1\\ D_1f_2 & D_2f_2 \\\end{array} \right) $$ $$f_1(x,y)=\sin x+\sin y$$ $$f_2(x,y)=\cos x+\cos y$$ $$ Df(x,y)=\left( \begin{array}{ccc} \cos x & \cos y\\ -\sin x & -\sin y \\\end{array} \right) $$ Part 2: I don't know how to do part 2. How do I find the partial derivatives of the components of $Df(x,y)$ ? Edit: (Attempted solution, per user127645's answer) $$D^{k+1}f = \begin{pmatrix} D_1(D^kf) & \bigg| & D_2(D^kf)\end{pmatrix}$$ $$D_1(D^1f)(x,y)=D_1\left( \begin{array}{ccc} \cos x & \cos y\\ -\sin x & -\sin y \\\end{array} \right) = \left( \begin{array}{ccc} -\sin x & 0\\ -\cos x & 0 \\\end{array} \right) $$ $$D_2(D^1f)(x,y)=D_2\left( \begin{array}{ccc} \cos x & \cos y\\ -\sin x & -\sin y \\\end{array} \right) = \left( \begin{array}{ccc} 0 & -\sin y\\ 0 & -\cos y \\\end{array} \right) $$ $$D^2f(x,y)= \left( \begin{array}{ccc} -\sin x & 0 & 0 & -\sin y\\ -\cos x & 0 & 0 & -\cos y \\\end{array} \right)$$","The Exercise: My Work: Part 1: Part 2: I don't know how to do part 2. How do I find the partial derivatives of the components of ? Edit: (Attempted solution, per user127645's answer)","
Df=\left( \begin{array}{ccc}
D_1f_1 & D_2f_1\\
D_1f_2 & D_2f_2 \\\end{array} \right)
 f_1(x,y)=\sin x+\sin y f_2(x,y)=\cos x+\cos y 
Df(x,y)=\left( \begin{array}{ccc}
\cos x & \cos y\\
-\sin x & -\sin y \\\end{array} \right)
 Df(x,y) D^{k+1}f = \begin{pmatrix} D_1(D^kf) & \bigg| & D_2(D^kf)\end{pmatrix} D_1(D^1f)(x,y)=D_1\left( \begin{array}{ccc}
\cos x & \cos y\\
-\sin x & -\sin y \\\end{array} \right) = \left( \begin{array}{ccc}
-\sin x & 0\\
-\cos x & 0 \\\end{array} \right)  D_2(D^1f)(x,y)=D_2\left( \begin{array}{ccc}
\cos x & \cos y\\
-\sin x & -\sin y \\\end{array} \right) = \left( \begin{array}{ccc}
0 & -\sin y\\
0 & -\cos y \\\end{array} \right)  D^2f(x,y)= \left( \begin{array}{ccc}
-\sin x & 0 & 0 & -\sin y\\
-\cos x & 0 & 0 & -\cos y \\\end{array} \right)","['matrices', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-derivative']"
39,Converting 2nd order to 1st order,Converting 2nd order to 1st order,,So I have 2 second order odes that I need to convert into 4 first order odes. The odes are: $$m_1x_1^{\prime\prime} = -k_1(x_1-l_1)+k_3(x_2-x_1-l_3)$$ $$m_2x_2^{\prime\prime} = -k_2(x_2-l_2)-k_3(x_2-x_1-l_3)$$ So I saw these posts so I understand I have to use a change a variable (I think that's what they are called) but I just don't understand the step-by-step portion. I kind of need to be walked through the process to really understand WHAT I'm doing.,So I have 2 second order odes that I need to convert into 4 first order odes. The odes are: $$m_1x_1^{\prime\prime} = -k_1(x_1-l_1)+k_3(x_2-x_1-l_3)$$ $$m_2x_2^{\prime\prime} = -k_2(x_2-l_2)-k_3(x_2-x_1-l_3)$$ So I saw these posts so I understand I have to use a change a variable (I think that's what they are called) but I just don't understand the step-by-step portion. I kind of need to be walked through the process to really understand WHAT I'm doing.,,"['ordinary-differential-equations', 'differential']"
40,Period of linear vs nonlinear pendulum,Period of linear vs nonlinear pendulum,,"I think I have an idea of how to do the problem but am not 100% sure. The question is: (a) From the equation: $$T(E)=2\sqrt\frac{L}{g} \int_0^1 \frac{du}{u^\frac{1}{2}(1-u)^{\frac{1}{2}}[1-\frac{E(1-u)}{2g}]^\frac{1}{2}}$$ where $$u=1-\frac{g}{E}(1-cos \theta )$$ show that a nonlinear pendulum has a longer period than a linearized pendulum. (b) Show that $\frac{dT}{dE} > 0 $. Briefly describe a physical interpretation of this result. -------------------------------------------------------------------------------------------------------------- So I'm thinking I should derive the E equation for both a linear pendulum and non-linear pendulume (i.e E=$\frac{1}{2}mv^2+\frac{1}{2}kx_o^2$. Then plug them in and solve and see which one gives the bigger $T(E)$ for part (a) . Don't know if that is the correct way to do it. Then for part (b) , I just take the derivative of $T(E)$ and show that it's $>0$. But I don't know the physical interpretation of this result.","I think I have an idea of how to do the problem but am not 100% sure. The question is: (a) From the equation: $$T(E)=2\sqrt\frac{L}{g} \int_0^1 \frac{du}{u^\frac{1}{2}(1-u)^{\frac{1}{2}}[1-\frac{E(1-u)}{2g}]^\frac{1}{2}}$$ where $$u=1-\frac{g}{E}(1-cos \theta )$$ show that a nonlinear pendulum has a longer period than a linearized pendulum. (b) Show that $\frac{dT}{dE} > 0 $. Briefly describe a physical interpretation of this result. -------------------------------------------------------------------------------------------------------------- So I'm thinking I should derive the E equation for both a linear pendulum and non-linear pendulume (i.e E=$\frac{1}{2}mv^2+\frac{1}{2}kx_o^2$. Then plug them in and solve and see which one gives the bigger $T(E)$ for part (a) . Don't know if that is the correct way to do it. Then for part (b) , I just take the derivative of $T(E)$ and show that it's $>0$. But I don't know the physical interpretation of this result.",,['ordinary-differential-equations']
41,Different aspects of differential equations,Different aspects of differential equations,,"I sometimes encountered the keywords such as 'geometric', 'analytical' aspects of differential equations, especially in general introduction. However I do not really understand what they are referring to. Could anyone clarify on those terminologies? update: For example, as mentioned in Tao's comment , what does exactly the geometric information of NS equation refer to?","I sometimes encountered the keywords such as 'geometric', 'analytical' aspects of differential equations, especially in general introduction. However I do not really understand what they are referring to. Could anyone clarify on those terminologies? update: For example, as mentioned in Tao's comment , what does exactly the geometric information of NS equation refer to?",,"['ordinary-differential-equations', 'soft-question']"
42,A doubt regarding Picard's theorem.,A doubt regarding Picard's theorem.,,"Picard's theorem states: If $f(x,y)$ and $\frac{\partial f}{\partial x}$ are continuous functions on a closed rectangle $R$, then through each point $(x_0,y_0)\in R$ there passes a unique integral curve of the equation $\frac{dy}{dx}=f(x,y)$. Here, should we assume that $x$ and $y$ are independent? If they were independent, $\frac{dy}{dx}$ would simply equal $0$, which would imply $f(x,y)=0$ passes through every point inside $R$. Why could the theorem not have simply stated $f(x,y)=0$ passes through every point in $R$, instead of saying $\frac{dy}{dx}=f(x,y)$ passes through every such point? Is there some subtle difference between these two statements that I'm missing? Thanks in advance!","Picard's theorem states: If $f(x,y)$ and $\frac{\partial f}{\partial x}$ are continuous functions on a closed rectangle $R$, then through each point $(x_0,y_0)\in R$ there passes a unique integral curve of the equation $\frac{dy}{dx}=f(x,y)$. Here, should we assume that $x$ and $y$ are independent? If they were independent, $\frac{dy}{dx}$ would simply equal $0$, which would imply $f(x,y)=0$ passes through every point inside $R$. Why could the theorem not have simply stated $f(x,y)=0$ passes through every point in $R$, instead of saying $\frac{dy}{dx}=f(x,y)$ passes through every such point? Is there some subtle difference between these two statements that I'm missing? Thanks in advance!",,[]
43,Differential Equations: Separable Equations,Differential Equations: Separable Equations,,"How would you solve $y' = {(3x^2-e^x)}/{(2y-7)}$ with a initial condition of $y(0)=2$? For my attempt, I computed the following with some skipped steps due to being unfamiliar with Latex format. $$y^2 - 7y = x^3 - e^x + c$$ $$c = -11$$ $$(y-7/2)^2  = x^3 - e^x +5/4$$ $$y=7/2 + sqrt(x^3 - e^x +5/4)$$ But my solution is incorrect. Can someone point out where I am making my mistake at? Also how can you determine the interval in which the solution is defined?","How would you solve $y' = {(3x^2-e^x)}/{(2y-7)}$ with a initial condition of $y(0)=2$? For my attempt, I computed the following with some skipped steps due to being unfamiliar with Latex format. $$y^2 - 7y = x^3 - e^x + c$$ $$c = -11$$ $$(y-7/2)^2  = x^3 - e^x +5/4$$ $$y=7/2 + sqrt(x^3 - e^x +5/4)$$ But my solution is incorrect. Can someone point out where I am making my mistake at? Also how can you determine the interval in which the solution is defined?",,"['calculus', 'ordinary-differential-equations']"
44,Particle Motion,Particle Motion,,"So this is a simple problem but I'm just getting stumped. The question is: A particle not connected to a spring, moving in a straight line, is subject to a retardation force of magnitude $\beta(\frac{dx}{dt})^n$, with $\beta > 0$. a) Show that if 0 < n < 1, the particle will come to rest in a finite time. How far will the particle travel, and when will it stop? So I think this would be the starting equation: $m\frac{d^2x}{dt^2}+\beta (\frac{dx}{dt})^n=0$ and the particle will stop when $\frac{dx}{dt}=0$ but that's all I got..I don't really know what to do from here. The question asks for n in a range so that's kind of throwing me off. Any ideas? Thanks","So this is a simple problem but I'm just getting stumped. The question is: A particle not connected to a spring, moving in a straight line, is subject to a retardation force of magnitude $\beta(\frac{dx}{dt})^n$, with $\beta > 0$. a) Show that if 0 < n < 1, the particle will come to rest in a finite time. How far will the particle travel, and when will it stop? So I think this would be the starting equation: $m\frac{d^2x}{dt^2}+\beta (\frac{dx}{dt})^n=0$ and the particle will stop when $\frac{dx}{dt}=0$ but that's all I got..I don't really know what to do from here. The question asks for n in a range so that's kind of throwing me off. Any ideas? Thanks",,"['ordinary-differential-equations', 'physics', 'mathematical-physics']"
45,Eigenvectors Trajectories,Eigenvectors Trajectories,,"I got stuck with a problem while studying for a control systems exam. It goes as following: ""Look at the picture of trajectories of a linear, time-invariant system with the form: $\frac{d\mathbf{x}}{dt}=\mathbf{A}x$ . The Eigenvalues of the matrix $A$ are $s_1=-1$ and $s_2=-2$ . Find the Eigenvectors $p_1$ and $p_2$ considering the given Eigenvalues."" How can I calculate the Eigenvectors with just knowing the Eigenvalues and the trajectories? Thanks in advance","I got stuck with a problem while studying for a control systems exam. It goes as following: ""Look at the picture of trajectories of a linear, time-invariant system with the form: . The Eigenvalues of the matrix are and . Find the Eigenvectors and considering the given Eigenvalues."" How can I calculate the Eigenvectors with just knowing the Eigenvalues and the trajectories? Thanks in advance",\frac{d\mathbf{x}}{dt}=\mathbf{A}x A s_1=-1 s_2=-2 p_1 p_2,"['linear-algebra', 'ordinary-differential-equations', 'systems-of-equations', 'control-theory']"
46,Rewrite this differential equation?,Rewrite this differential equation?,,"I have the following differential equation $$y'(t)=\alpha y(t)-\beta y^2(t), y(0)=N_0$$ and need to use $z(t)=\frac1{y(t)}$ to rewrite it as  $$z'(t)=-\alpha z(t)+\beta$$ I've literally tried every standard algebra trick to work this and can't get it. Can anyone see a trick or technique that I might be missing that could help?","I have the following differential equation $$y'(t)=\alpha y(t)-\beta y^2(t), y(0)=N_0$$ and need to use $z(t)=\frac1{y(t)}$ to rewrite it as  $$z'(t)=-\alpha z(t)+\beta$$ I've literally tried every standard algebra trick to work this and can't get it. Can anyone see a trick or technique that I might be missing that could help?",,['ordinary-differential-equations']
47,private solution after solving nonhomogenous euler equation,private solution after solving nonhomogenous euler equation,,"Solve the equalation: $$x^2y''-3xy'+3y=\ln x$$ First I solved the homogeneous part and got $$y_h=c_1x+c_2x^3.$$ Then i wanted using variation of parameters writing that $$c_1' x'+c_2' (x^3)^\prime.$$ I solved the equations for $c_1,c_2$ and got a function which when i substitute in the equation, I get $x^2\ln x$ (not $\ln x$ ). I verified my answers with MuPaD and seems like the integration was correct. Am I using wrong method?","Solve the equalation: First I solved the homogeneous part and got Then i wanted using variation of parameters writing that I solved the equations for and got a function which when i substitute in the equation, I get (not ). I verified my answers with MuPaD and seems like the integration was correct. Am I using wrong method?","x^2y''-3xy'+3y=\ln x y_h=c_1x+c_2x^3. c_1' x'+c_2' (x^3)^\prime. c_1,c_2 x^2\ln x \ln x",[]
48,Convert the power series solution of $(1+x^2)y''+4xy'+2y=0$ into simple closed-form expression,Convert the power series solution of  into simple closed-form expression,(1+x^2)y''+4xy'+2y=0,"$(a)$Use two power series in $x$ to find the general solution of $$(1+x^2)y''+4xy'+2y=0$$ and state the set of $x$-values on which each series solution is valid. $(b)$ Convert the power series solutions in $(a)$ into simple closed-form expressions. $(c)$ Use $(b)$ to find the general solution of the equation above on the whole real line. For $(a)$, I used  $y(x)=\sum\limits_{n=0}^\infty a_nx^n$,and solved the recurrence relation to be $a_{2k} = (-1)^ka_0$ and $a_{2k+1} = (-1)^ka_1$, where $a_0$ and $a_1$ are arbitrary. And for the solution I got $y(x)=a_0\sum\limits_{k=1}^\infty (-1)^kx^{2k} +a_1\sum\limits_{k=1}^\infty (-1)^kx^{2k+1}$. How do I convert the solution into simple closed-from expressions, and how should I solve $(c)$? Thanks for any help.","$(a)$Use two power series in $x$ to find the general solution of $$(1+x^2)y''+4xy'+2y=0$$ and state the set of $x$-values on which each series solution is valid. $(b)$ Convert the power series solutions in $(a)$ into simple closed-form expressions. $(c)$ Use $(b)$ to find the general solution of the equation above on the whole real line. For $(a)$, I used  $y(x)=\sum\limits_{n=0}^\infty a_nx^n$,and solved the recurrence relation to be $a_{2k} = (-1)^ka_0$ and $a_{2k+1} = (-1)^ka_1$, where $a_0$ and $a_1$ are arbitrary. And for the solution I got $y(x)=a_0\sum\limits_{k=1}^\infty (-1)^kx^{2k} +a_1\sum\limits_{k=1}^\infty (-1)^kx^{2k+1}$. How do I convert the solution into simple closed-from expressions, and how should I solve $(c)$? Thanks for any help.",,['ordinary-differential-equations']
49,Liapunov and asymptotically stable,Liapunov and asymptotically stable,,"I have three maps $F:\mathbb R\rightarrow\mathbb R$ $F(x)=-x, F(x)=x+x^2, F(x)=x-x^3$ I want to check whether the non-hyperbolic fixed point at the origin is Liapunov stable, asymptotically stable or not. Well in all three cases the non-hyperbolic fixed point is $0$, because we know that for being non-hyperbolic the eigenvalues of the linearized system need to have a non-zero real part. The definition for Liapunov stable I use is: $x$ is Liapunov stable iff for all $\epsilon>0$ there exists $\delta >0$ s.t if $|x-y|<\delta$ then $|\phi(x,t)-\phi(y,t)|<\epsilon$ Quasi-asyptotically stable iff there $\exists \delta>0$ such that if $|x-y|<\delta$ then $|\phi(x,t)-\phi(y,t)|\rightarrow 0$ as $t\rightarrow \infty$ Asymptotically stable if both defintions above hold. Question: How can find out now explicitly in my three cases whether there is some kind of stability? Simply by solving the diff equations?","I have three maps $F:\mathbb R\rightarrow\mathbb R$ $F(x)=-x, F(x)=x+x^2, F(x)=x-x^3$ I want to check whether the non-hyperbolic fixed point at the origin is Liapunov stable, asymptotically stable or not. Well in all three cases the non-hyperbolic fixed point is $0$, because we know that for being non-hyperbolic the eigenvalues of the linearized system need to have a non-zero real part. The definition for Liapunov stable I use is: $x$ is Liapunov stable iff for all $\epsilon>0$ there exists $\delta >0$ s.t if $|x-y|<\delta$ then $|\phi(x,t)-\phi(y,t)|<\epsilon$ Quasi-asyptotically stable iff there $\exists \delta>0$ such that if $|x-y|<\delta$ then $|\phi(x,t)-\phi(y,t)|\rightarrow 0$ as $t\rightarrow \infty$ Asymptotically stable if both defintions above hold. Question: How can find out now explicitly in my three cases whether there is some kind of stability? Simply by solving the diff equations?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
50,Can anyone solve this ODE?,Can anyone solve this ODE?,,"Is the following equation solvable analytically: $$uu''+au^3=b$$ Where $a,b$ are positive real numbers? As you can see, I used a u-sub to get to this equation, but I can't see any tricks. Also, DSolve in Mathematica said the problem was equivalent to the following: $$\int_1^{u}\frac{dK_1}{\sqrt{C_1+2\left(4b\ln K_1-a\frac{K_1^3}{3}\right)}}$$ Which it couldn't solve.","Is the following equation solvable analytically: $$uu''+au^3=b$$ Where $a,b$ are positive real numbers? As you can see, I used a u-sub to get to this equation, but I can't see any tricks. Also, DSolve in Mathematica said the problem was equivalent to the following: $$\int_1^{u}\frac{dK_1}{\sqrt{C_1+2\left(4b\ln K_1-a\frac{K_1^3}{3}\right)}}$$ Which it couldn't solve.",,['ordinary-differential-equations']
51,"Ansatz of particular solution, 2nd order ODE","Ansatz of particular solution, 2nd order ODE",,"Find the particular solution of $y'' -4y' +4y = e^{x}$ Helping a student with single variable calculus but perhaps I need some brushing up myself. I suggested y should have the form $Ce^{x}$. This produced the correct answer, but the solution sheet said the correct ansatz would be $z(x)e^x$. I don't understand the point of the $z$ here when $e^x$ isn't accompanied by a polynomial or whatever. Am I missing something?","Find the particular solution of $y'' -4y' +4y = e^{x}$ Helping a student with single variable calculus but perhaps I need some brushing up myself. I suggested y should have the form $Ce^{x}$. This produced the correct answer, but the solution sheet said the correct ansatz would be $z(x)e^x$. I don't understand the point of the $z$ here when $e^x$ isn't accompanied by a polynomial or whatever. Am I missing something?",,['ordinary-differential-equations']
52,Trying to understand a change of function in a ODE,Trying to understand a change of function in a ODE,,"I'm trying to understand the following change. Given this equation: $2 z'' (1 + z^2) + (z')^3 = 0$ and writing $w = z'$, $z'' = w \dfrac{dw}{dz}$ reduces to: $$-\dfrac{dw}{w^2} = \dfrac{dz}{2 (1 +z^2)}$$ I don't understand that change of function. Is it $w(t)$ or $w(z)$? If it is $w(t)$ why the second ODE relates $w$ to $z$ instead of $w$ to $t$? If it is $w(z)$ how can it be $w(z) = z'(t)?$ Could you explain the function change in detail? This is from http://www.liv.ac.uk/~pjgiblin/papers/giblin-warder.pdf page 8 (close to the end)","I'm trying to understand the following change. Given this equation: $2 z'' (1 + z^2) + (z')^3 = 0$ and writing $w = z'$, $z'' = w \dfrac{dw}{dz}$ reduces to: $$-\dfrac{dw}{w^2} = \dfrac{dz}{2 (1 +z^2)}$$ I don't understand that change of function. Is it $w(t)$ or $w(z)$? If it is $w(t)$ why the second ODE relates $w$ to $z$ instead of $w$ to $t$? If it is $w(z)$ how can it be $w(z) = z'(t)?$ Could you explain the function change in detail? This is from http://www.liv.ac.uk/~pjgiblin/papers/giblin-warder.pdf page 8 (close to the end)",,"['ordinary-differential-equations', 'functions']"
53,Eigen values and vectors,Eigen values and vectors,,"$$ X'(t)= \begin{bmatrix}x'(t)\\ y'(t) \end{bmatrix} = \begin{bmatrix}-5 & -2\\-1 & -4\end{bmatrix}\begin{bmatrix}x(t)\\ y(t) \end{bmatrix}$$ Sketch the directions field for the system, plot the straight line solutions for each Eigenvalue, specify the corresponding straight line solution and its x(t) and y(t). I found the Eigenvalues which are $-6, -3$ and the vectors $v_1 = (2,1) , v_2 = (-1,1)$ I need help to plot the lines","$$ X'(t)= \begin{bmatrix}x'(t)\\ y'(t) \end{bmatrix} = \begin{bmatrix}-5 & -2\\-1 & -4\end{bmatrix}\begin{bmatrix}x(t)\\ y(t) \end{bmatrix}$$ Sketch the directions field for the system, plot the straight line solutions for each Eigenvalue, specify the corresponding straight line solution and its x(t) and y(t). I found the Eigenvalues which are $-6, -3$ and the vectors $v_1 = (2,1) , v_2 = (-1,1)$ I need help to plot the lines",,"['ordinary-differential-equations', 'dynamical-systems']"
54,Compute Laplace Transform,Compute Laplace Transform,,$$f(t) = 2t\ e^t\sin(2t)$$ Individually I found the Laplace of $2t\ e^t$ to be $\dfrac{2}{s^2}$ and the Laplace of $\sin(2t)$ to be $\dfrac{1}{1-s} + \dfrac{2}{s^2+4}$ From here I don't know where to go.,$$f(t) = 2t\ e^t\sin(2t)$$ Individually I found the Laplace of $2t\ e^t$ to be $\dfrac{2}{s^2}$ and the Laplace of $\sin(2t)$ to be $\dfrac{1}{1-s} + \dfrac{2}{s^2+4}$ From here I don't know where to go.,,"['ordinary-differential-equations', 'laplace-transform']"
55,How to solve $(2x^2+y)\partial x+(xy^2-x)\partial y=0$,How to solve,(2x^2+y)\partial x+(xy^2-x)\partial y=0,"How can we solve this kinda eq.? $$(2x^2+y)\partial x+(xy^2-x)\partial y=0$$ first I check  if it is entire. (which is not because $M(x,y)=2x^2+y\quad and \quad N(x,y)=xy^2-x\quad thus \quad M_y=1\neq y^2-1=N_x$) I tried to find integral factor.( which I couldn't because $\frac{M_y-N_x}{N}$ doesnt respect to x and $\frac{M_y-N_x}{M}$ doesn't respect to y) how do we approach? (I may wrote  math. expressions wrong)","How can we solve this kinda eq.? $$(2x^2+y)\partial x+(xy^2-x)\partial y=0$$ first I check  if it is entire. (which is not because $M(x,y)=2x^2+y\quad and \quad N(x,y)=xy^2-x\quad thus \quad M_y=1\neq y^2-1=N_x$) I tried to find integral factor.( which I couldn't because $\frac{M_y-N_x}{N}$ doesnt respect to x and $\frac{M_y-N_x}{M}$ doesn't respect to y) how do we approach? (I may wrote  math. expressions wrong)",,['ordinary-differential-equations']
56,Green's Function ODE,Green's Function ODE,,"Solve the following equation using Green's functions: $u''-k^2u=f$  with boundary conditions $u(0)-u'(0)=a$ and $u(1)=b$ For this problem, I was going to find the green's function with homogeneous BC's (set both BC's equal to zero), and then I was going to add the solution to the homogeneous equation Lu = 0 (with the BC's given above) to the green's function solution.  However, when working out the green's function, I end up with constant that can't be solved.  Any help?","Solve the following equation using Green's functions: $u''-k^2u=f$  with boundary conditions $u(0)-u'(0)=a$ and $u(1)=b$ For this problem, I was going to find the green's function with homogeneous BC's (set both BC's equal to zero), and then I was going to add the solution to the homogeneous equation Lu = 0 (with the BC's given above) to the green's function solution.  However, when working out the green's function, I end up with constant that can't be solved.  Any help?",,['ordinary-differential-equations']
57,problem with recurrence relation for series solution for ODE,problem with recurrence relation for series solution for ODE,,"I have $$y''-xy'-y=0$$ and I'm trying to find the series solution around the ordinary point $x_0=1$.  My last post I muscled through to the solution when the ordinary point was $x_0=0$, but this is proving to be tougher.  Now I have obtained through power series analysis $$y''-xy'-y=0=\sum_{k=0}^{\infty}[(k+2)(k+1)a_{k+2}-(k+1)a_{k+1}-(k+1)a_k](x-1)^k$$ which yields the recurrence relation $$a_{k+2}=\frac{a_{k+1}+a_{k}}{k+2}$$ WHen I start plugging in sequential ""k"" values I'm not finding a very good pattern emerging. $$a_2=\frac{a_0}{2!}+\frac{a_1}{2!}$$ $$a_3=\frac{a_0}{3!}+\frac{3a_1}{3!}$$ $$a_4=\frac{4a_0}{4!}+\frac{6a_1}{4!}$$ $$a_5=\frac{8a_0}{5!}+\frac{18a_1}{5!}$$ $$a_6=\frac{28a_0}{6!}+\frac{48a_1}{6!}$$ $$a_7=\frac{76a_0}{7!}+\frac{156a_1}{7!}$$ Outside just writing out term by term, substituting in the appropriate $a_k$, does this have a nice closed form?  Or is is just the case that I have the answer with the $a_k$'s that I have","I have $$y''-xy'-y=0$$ and I'm trying to find the series solution around the ordinary point $x_0=1$.  My last post I muscled through to the solution when the ordinary point was $x_0=0$, but this is proving to be tougher.  Now I have obtained through power series analysis $$y''-xy'-y=0=\sum_{k=0}^{\infty}[(k+2)(k+1)a_{k+2}-(k+1)a_{k+1}-(k+1)a_k](x-1)^k$$ which yields the recurrence relation $$a_{k+2}=\frac{a_{k+1}+a_{k}}{k+2}$$ WHen I start plugging in sequential ""k"" values I'm not finding a very good pattern emerging. $$a_2=\frac{a_0}{2!}+\frac{a_1}{2!}$$ $$a_3=\frac{a_0}{3!}+\frac{3a_1}{3!}$$ $$a_4=\frac{4a_0}{4!}+\frac{6a_1}{4!}$$ $$a_5=\frac{8a_0}{5!}+\frac{18a_1}{5!}$$ $$a_6=\frac{28a_0}{6!}+\frac{48a_1}{6!}$$ $$a_7=\frac{76a_0}{7!}+\frac{156a_1}{7!}$$ Outside just writing out term by term, substituting in the appropriate $a_k$, does this have a nice closed form?  Or is is just the case that I have the answer with the $a_k$'s that I have",,"['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
58,Drawing a bufircation diagram,Drawing a bufircation diagram,,"$\dot x=x(\mu+x-2)(\mu+2x-x^2)$ The first thing I did was to check the fixed points in the $(\mu,x)$-plane: $x=0$ $x=2-\mu$ (saddle node at 0 when $\mu$) $x_{1,2}=1+-\sqrt{\mu+1}$ (no fixed point for $\mu<1$) Did I specified the type for the bufircation points correctly? How does the bifurcation diagram now looks like? I do not know how to draw them.","$\dot x=x(\mu+x-2)(\mu+2x-x^2)$ The first thing I did was to check the fixed points in the $(\mu,x)$-plane: $x=0$ $x=2-\mu$ (saddle node at 0 when $\mu$) $x_{1,2}=1+-\sqrt{\mu+1}$ (no fixed point for $\mu<1$) Did I specified the type for the bufircation points correctly? How does the bifurcation diagram now looks like? I do not know how to draw them.",,"['ordinary-differential-equations', 'dynamical-systems']"
59,Differential operators confussion,Differential operators confussion,,"I want to solve this PDE: $$u_t-6uu_x+u_{xxx} = 0\,(1)$$  with the Inverse Scattering Method. This method is based on showing that the above equation can be expressed as $$L_t=LB-BL,\,(2)$$ where $L$ and $B$ are the differential operators: $$L=-\frac{\partial^2}{\partial x^2}+u(x)$$ $$B=-4\frac{\partial^3}{\partial x^3}+6u\frac{\partial}{\partial x}+3\frac{\partial u}{\partial x}.$$ I have tried to prove that (1) and (2) are equal composing the differential operators $L$ and $B$. However I don't have experience doing this and I got different results. The way I applied the operators is as if they were just derivatives and by using the chain rule.","I want to solve this PDE: $$u_t-6uu_x+u_{xxx} = 0\,(1)$$  with the Inverse Scattering Method. This method is based on showing that the above equation can be expressed as $$L_t=LB-BL,\,(2)$$ where $L$ and $B$ are the differential operators: $$L=-\frac{\partial^2}{\partial x^2}+u(x)$$ $$B=-4\frac{\partial^3}{\partial x^3}+6u\frac{\partial}{\partial x}+3\frac{\partial u}{\partial x}.$$ I have tried to prove that (1) and (2) are equal composing the differential operators $L$ and $B$. However I don't have experience doing this and I got different results. The way I applied the operators is as if they were just derivatives and by using the chain rule.",,"['ordinary-differential-equations', 'partial-differential-equations']"
60,How to determine solutions to this differential equation in terms of this function?,How to determine solutions to this differential equation in terms of this function?,,"Let $s(x) := (\sin x) / x $ if $x \neq 0$, and let $s(0) = 1$; define $T(x) \colon = \int_0^x s(t) \ dt$. Then the function $f(x) \colon = x T(x) $ satisfies the differential equation $xy^\prime - y = x \sin x $ on the interval $(-\infty, +\infty)$. How to determine (in terms of $T(x)$) all solutions to this differential equation?","Let $s(x) := (\sin x) / x $ if $x \neq 0$, and let $s(0) = 1$; define $T(x) \colon = \int_0^x s(t) \ dt$. Then the function $f(x) \colon = x T(x) $ satisfies the differential equation $xy^\prime - y = x \sin x $ on the interval $(-\infty, +\infty)$. How to determine (in terms of $T(x)$) all solutions to this differential equation?",,"['calculus', 'ordinary-differential-equations']"
61,For what values of $r$ does $y=e^{rx}$ satisfy $y'' + 5y' - 6y = 0$?,For what values of  does  satisfy ?,r y=e^{rx} y'' + 5y' - 6y = 0,For what values of $r$ does $y=e^{rx}$ satisfy $y'' + 5y' - 6y = 0$ ? Attempt: $y' = [e^{rx}] (r)$ $y''= r^2e^{rx}$,For what values of does satisfy ? Attempt:,r y=e^{rx} y'' + 5y' - 6y = 0 y' = [e^{rx}] (r) y''= r^2e^{rx},['ordinary-differential-equations']
62,Prove the difference between two second order non homogeneous EDO solutions goes to zero when the independent variable goes to infinity.,Prove the difference between two second order non homogeneous EDO solutions goes to zero when the independent variable goes to infinity.,,"Let the ODE $ay''+by'+cy=g(t)$ where  $a,b,c \in \mathbb{R}$ and $a,b,c>0$. Given that $Y_1(t),Y_2(t)$ are solutions to the ODE, prove $Y_1(t)-Y_2(t) \rightarrow 0$ when $t \rightarrow \infty$.","Let the ODE $ay''+by'+cy=g(t)$ where  $a,b,c \in \mathbb{R}$ and $a,b,c>0$. Given that $Y_1(t),Y_2(t)$ are solutions to the ODE, prove $Y_1(t)-Y_2(t) \rightarrow 0$ when $t \rightarrow \infty$.",,['ordinary-differential-equations']
63,Analytical solution of nonlinear ordinary differential equation,Analytical solution of nonlinear ordinary differential equation,,"I have following first order nonlinear ordinary differential and i was wondering if you can suggest some method by which either i can get an exact solution or approaximate and converging perturbative solution. $$ \frac{dx}{dt} = 2Wx + 2xy - 4x^{3} $$ $$ \frac{dy}{dt} = \gamma (x^{2} - y) $$ Kindly help me with any methods you that might work and it will be great if you can provide few references where i can read about those methods. Also If somebody can help me about how I can use fixed point analytic method to solve this differential equations and some references on it, will be very useful too. Any help will be highly helpful. Thanks a lot in advance. PS. I tried homotopy perturbation analysis and simple iteration procedure to try to solve it and it diverges after some time(good only for early short times).","I have following first order nonlinear ordinary differential and i was wondering if you can suggest some method by which either i can get an exact solution or approaximate and converging perturbative solution. $$ \frac{dx}{dt} = 2Wx + 2xy - 4x^{3} $$ $$ \frac{dy}{dt} = \gamma (x^{2} - y) $$ Kindly help me with any methods you that might work and it will be great if you can provide few references where i can read about those methods. Also If somebody can help me about how I can use fixed point analytic method to solve this differential equations and some references on it, will be very useful too. Any help will be highly helpful. Thanks a lot in advance. PS. I tried homotopy perturbation analysis and simple iteration procedure to try to solve it and it diverges after some time(good only for early short times).",,"['ordinary-differential-equations', 'perturbation-theory']"
64,solving differential equation $(x+y)=\frac{dy}{dx}(4x+y)$,solving differential equation,(x+y)=\frac{dy}{dx}(4x+y),"we've got this differential equation  $$(x+y)=\frac{dy}{dx}(4x+y)$$ now what should be substituted for $x,y$ i can always do $$x+y=k$$ and then $$1+\frac{dy}{dx}=\frac{dk}{dx}$$ but this does't suite here in this case  $x,y$ in previous problems it was very easy to substitue because cofficients of $x,y$ were same and $x+y=k$ was then substituted now here cofficients are not same so how should i do this??","we've got this differential equation  $$(x+y)=\frac{dy}{dx}(4x+y)$$ now what should be substituted for $x,y$ i can always do $$x+y=k$$ and then $$1+\frac{dy}{dx}=\frac{dk}{dx}$$ but this does't suite here in this case  $x,y$ in previous problems it was very easy to substitue because cofficients of $x,y$ were same and $x+y=k$ was then substituted now here cofficients are not same so how should i do this??",,"['calculus', 'ordinary-differential-equations']"
65,How to write Pfaffian differential equation,How to write Pfaffian differential equation,,I am studying the example. And I dont understand how to write the pfaffian diff equation in the first line. What is its formula?,I am studying the example. And I dont understand how to write the pfaffian diff equation in the first line. What is its formula?,,"['ordinary-differential-equations', 'partial-differential-equations', 'self-learning']"
66,Finding the weak derivatives,Finding the weak derivatives,,"Finding the weak derivatives: a/ $f(x)=\left|x_1 \right|$, for all $x=(x_1,\ldots, x_n)$; b/ $f(x)=\operatorname{sign} (x_1)$, where $\Omega=\{\left|x \right|<1\}$. ========================================== We know that $f$ has weak derivative $\partial^\alpha f=g$ if $$\int_{\Omega}g \varphi \ \mathrm{d}x=(-1)^\left|\alpha \right|\int_{\Omega}f\cdot D^\alpha \varphi \ \mathrm{d}x, \ \forall \varphi \in C_{0}^{\infty}(\Omega)$$ ========================================== For Question a/ We assume that $g$ is the weak devirative of $f$. We have $I:=\int_{-1}^{1}g\varphi \ \mathrm{d}x=-\int_{-1}^{1}|x_1|D\varphi \ \mathrm{d}x_1, \ \forall \varphi \in C_{0}^{\infty}(\Omega) \tag 1$ Whence $-I=\int_{-1}^{0}-x_1 D\varphi \ \mathrm{d}x_1+\int_{0}^{1}x_1 D\varphi \ \mathrm{d}x_1=-x_1\varphi \mid_{-1}^{0}+\int_{-1}^{0}\varphi \ \mathrm{d}x_1+x_1 \varphi \mid_{0}^{1}-\int_{0}^{1}\varphi \ \mathrm{d}x_1$ Hence, $$\begin{align*} -I&=\varphi(1)-\varphi(-1)-\left(\int_{0}^{1}\varphi \ \mathrm{d}x_1-\int_{-1}^{0}\varphi \ \mathrm{d}x_1 \right)\\ &=\varphi(1)-\varphi(-1)-\int_{-1}^{1}\operatorname{sign} (x_1) \varphi \ \mathrm{d}x_1 \\ &=-\int_{-1}^{1}\operatorname{sign} (x_1) \varphi \ \mathrm{d}x_1 \tag 2 \end{align*} $$ Since (1) and (2), we have $g(x)=\operatorname{sign} (x_1) \blacksquare$ ======================================== Now, For Question b/ I have stuck when I trying to show that $\not \exists$$g$ is the weak devirative of $f$. Can anyone solve it? Thanks!","Finding the weak derivatives: a/ $f(x)=\left|x_1 \right|$, for all $x=(x_1,\ldots, x_n)$; b/ $f(x)=\operatorname{sign} (x_1)$, where $\Omega=\{\left|x \right|<1\}$. ========================================== We know that $f$ has weak derivative $\partial^\alpha f=g$ if $$\int_{\Omega}g \varphi \ \mathrm{d}x=(-1)^\left|\alpha \right|\int_{\Omega}f\cdot D^\alpha \varphi \ \mathrm{d}x, \ \forall \varphi \in C_{0}^{\infty}(\Omega)$$ ========================================== For Question a/ We assume that $g$ is the weak devirative of $f$. We have $I:=\int_{-1}^{1}g\varphi \ \mathrm{d}x=-\int_{-1}^{1}|x_1|D\varphi \ \mathrm{d}x_1, \ \forall \varphi \in C_{0}^{\infty}(\Omega) \tag 1$ Whence $-I=\int_{-1}^{0}-x_1 D\varphi \ \mathrm{d}x_1+\int_{0}^{1}x_1 D\varphi \ \mathrm{d}x_1=-x_1\varphi \mid_{-1}^{0}+\int_{-1}^{0}\varphi \ \mathrm{d}x_1+x_1 \varphi \mid_{0}^{1}-\int_{0}^{1}\varphi \ \mathrm{d}x_1$ Hence, $$\begin{align*} -I&=\varphi(1)-\varphi(-1)-\left(\int_{0}^{1}\varphi \ \mathrm{d}x_1-\int_{-1}^{0}\varphi \ \mathrm{d}x_1 \right)\\ &=\varphi(1)-\varphi(-1)-\int_{-1}^{1}\operatorname{sign} (x_1) \varphi \ \mathrm{d}x_1 \\ &=-\int_{-1}^{1}\operatorname{sign} (x_1) \varphi \ \mathrm{d}x_1 \tag 2 \end{align*} $$ Since (1) and (2), we have $g(x)=\operatorname{sign} (x_1) \blacksquare$ ======================================== Now, For Question b/ I have stuck when I trying to show that $\not \exists$$g$ is the weak devirative of $f$. Can anyone solve it? Thanks!",,"['ordinary-differential-equations', 'weak-derivatives']"
67,Average number of predators and prey in Lotka–Volterra model?,Average number of predators and prey in Lotka–Volterra model?,,"Once again I wouldn't be surprised if this can be found maybe even on Wikipedia but I'm not a native English speaker and unfortunately couldn't find this myself. So assuming standard Lotka–Volterra equations , exactly as written in Wikipedia, representing number of prey $x(t)$ and number of predators $y(t)$: $$ \frac{dx}{dt} = x(\alpha - \beta y); \\ \frac{dy}{dt} = - y(\gamma - \delta  x); $$ I see that the dynamics are very peculiar at least when initial conditions are sufficiently close to non-trivial equilibrium point i.e. we observe a cycle (closed curve on a plane with one axis representing the number of prey and the other axis representing the number of predators). My question is how to calculate the average number of predators and prey? Where should I apply integration? Maybe even analytical solutions exist in a general case?","Once again I wouldn't be surprised if this can be found maybe even on Wikipedia but I'm not a native English speaker and unfortunately couldn't find this myself. So assuming standard Lotka–Volterra equations , exactly as written in Wikipedia, representing number of prey $x(t)$ and number of predators $y(t)$: $$ \frac{dx}{dt} = x(\alpha - \beta y); \\ \frac{dy}{dt} = - y(\gamma - \delta  x); $$ I see that the dynamics are very peculiar at least when initial conditions are sufficiently close to non-trivial equilibrium point i.e. we observe a cycle (closed curve on a plane with one axis representing the number of prey and the other axis representing the number of predators). My question is how to calculate the average number of predators and prey? Where should I apply integration? Maybe even analytical solutions exist in a general case?",,"['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
68,A particular version of Gronwall's inequality,A particular version of Gronwall's inequality,,"We have this theorem (Gronwall's inequality): Let $f$, $g$ and $h$ be continuous nonnegative functions defined for $t\ge t_0$. If$$f(t)\le h(t)+\int_{t_0}^{t}g(s)f(s)\,ds\>,$$then$$f(t)\le h(t)+\int_{t_0}^{t}g(s)h(s)e^{\int_s^tg(u)\,du}\,ds$$ How do I prove another version of the theorem, letting $h(t)=k$ , where $k$ is any nonnegative constant, i.e If$$f(t)\le k+\int_{t_0}^{t}g(s)f(s)\,ds\>,$$then$$f(t)\le ke^{\int_{t_0}^tg(s)\,ds}\,ds$$ I worked out the following \begin{align} f(t)&\le k+\int_{t_0}^{t}g(s)ke^{\int_s^tg(u)\,du}\,ds\\\\&=k\left(1+\int_{t_0}^{t}g(s)e^{\int_s^tg(u)\,du}\,ds\right) \end{align} How should I proceed?","We have this theorem (Gronwall's inequality): Let $f$, $g$ and $h$ be continuous nonnegative functions defined for $t\ge t_0$. If$$f(t)\le h(t)+\int_{t_0}^{t}g(s)f(s)\,ds\>,$$then$$f(t)\le h(t)+\int_{t_0}^{t}g(s)h(s)e^{\int_s^tg(u)\,du}\,ds$$ How do I prove another version of the theorem, letting $h(t)=k$ , where $k$ is any nonnegative constant, i.e If$$f(t)\le k+\int_{t_0}^{t}g(s)f(s)\,ds\>,$$then$$f(t)\le ke^{\int_{t_0}^tg(s)\,ds}\,ds$$ I worked out the following \begin{align} f(t)&\le k+\int_{t_0}^{t}g(s)ke^{\int_s^tg(u)\,du}\,ds\\\\&=k\left(1+\int_{t_0}^{t}g(s)e^{\int_s^tg(u)\,du}\,ds\right) \end{align} How should I proceed?",,['ordinary-differential-equations']
69,Non Uniqueness - ODE,Non Uniqueness - ODE,,"Consider the ODE: $x'[t]=\left|x\right|^{p/q}$, $p,q > 0$, and share no common factors. Can someone prove that for the initial condition $x[0]=0$ and $p < q$, there are an infinite number of solutions. And if $p > q$, there is a unique solution. Cheers","Consider the ODE: $x'[t]=\left|x\right|^{p/q}$, $p,q > 0$, and share no common factors. Can someone prove that for the initial condition $x[0]=0$ and $p < q$, there are an infinite number of solutions. And if $p > q$, there is a unique solution. Cheers",,['ordinary-differential-equations']
70,Fourier Series and Solving Differential Equations,Fourier Series and Solving Differential Equations,,"I am getting stuck on how to use Fourier Series to solve ODE's. Take the problem where \begin{equation} E(t)=200t(\pi^2-t^2),  \end{equation} for $t$ between $-\pi$ and $\pi$ (period of $2\pi$), $R=100$, $L=10$, $C=10^{-2}$, and \begin{equation} LI'' +RI'+(1/c)I=E(t). \end{equation} I know the first step would be to express $E(t)$ as a Fourier Series, then replace $E(t)$ in the equation with that representation, and solve the homogeneous and non-homogeneous solutions. This is where I am confused - what should I be looking to equate? Any help would be really appreciated! Thank you","I am getting stuck on how to use Fourier Series to solve ODE's. Take the problem where \begin{equation} E(t)=200t(\pi^2-t^2),  \end{equation} for $t$ between $-\pi$ and $\pi$ (period of $2\pi$), $R=100$, $L=10$, $C=10^{-2}$, and \begin{equation} LI'' +RI'+(1/c)I=E(t). \end{equation} I know the first step would be to express $E(t)$ as a Fourier Series, then replace $E(t)$ in the equation with that representation, and solve the homogeneous and non-homogeneous solutions. This is where I am confused - what should I be looking to equate? Any help would be really appreciated! Thank you",,"['ordinary-differential-equations', 'fourier-analysis', 'fourier-series']"
71,"Integrate $\int\frac{Cx}{(\sin x^2)^2}\,\mathrm dx.$",Integrate,"\int\frac{Cx}{(\sin x^2)^2}\,\mathrm dx.","Have been a doing a reduction of order ODE problem and this integral comes up at the last step. Not sure how to go about integrating it. The answers give $\cos x^2$ as the answer. Here's the original question: Verify that $u_1=\sin x^2$ is a solution to the equation $$xu''-u'+4x^3u=0$$ and use reduction of order to find a second, linearly independent solution. I've called the second solution $v$ and as far as I can tell, everything is good with my previous working. The only remaining bit is to integrate $$v'=\frac{Cx}{(\sin x^2)^2}\Leftrightarrow v=\int\frac{Cx}{(\sin x^2)^2}\,\mathrm dx.$$ Integration by parts didn't really help. I think there might be a substitution that I'm missing/forgetting. Thanks.","Have been a doing a reduction of order ODE problem and this integral comes up at the last step. Not sure how to go about integrating it. The answers give $\cos x^2$ as the answer. Here's the original question: Verify that $u_1=\sin x^2$ is a solution to the equation $$xu''-u'+4x^3u=0$$ and use reduction of order to find a second, linearly independent solution. I've called the second solution $v$ and as far as I can tell, everything is good with my previous working. The only remaining bit is to integrate $$v'=\frac{Cx}{(\sin x^2)^2}\Leftrightarrow v=\int\frac{Cx}{(\sin x^2)^2}\,\mathrm dx.$$ Integration by parts didn't really help. I think there might be a substitution that I'm missing/forgetting. Thanks.",,"['ordinary-differential-equations', 'integration', 'trigonometry']"
72,How to sketch the phase portrait near the critical point at the origin.,How to sketch the phase portrait near the critical point at the origin.,,A linear system and its general solution. $dx/dt$ = $6x - 2y$ $dy/dt$ = $4x + 2y$ It has a general solution of this: $$\begin{bmatrix} x(t) \\ y(t) \end{bmatrix} = A\begin{bmatrix} cos(2t) \\ cos(2t)+sin(2t) \end{bmatrix} e^{4t} + B\begin{bmatrix} sin(2t) \\ sin(2t) - cos(2t) \end{bmatrix}e^{4t}$$ Sketch the phase portrait near the critical point at the origin. Discuss the type and stability of the critical point. I don't know how to approach this since I'm used to drawing in the 2 eigenvectors and figuring out if they face in or out against the origin. Then I would draw the orbits. How do I do this one?,A linear system and its general solution. $dx/dt$ = $6x - 2y$ $dy/dt$ = $4x + 2y$ It has a general solution of this: $$\begin{bmatrix} x(t) \\ y(t) \end{bmatrix} = A\begin{bmatrix} cos(2t) \\ cos(2t)+sin(2t) \end{bmatrix} e^{4t} + B\begin{bmatrix} sin(2t) \\ sin(2t) - cos(2t) \end{bmatrix}e^{4t}$$ Sketch the phase portrait near the critical point at the origin. Discuss the type and stability of the critical point. I don't know how to approach this since I'm used to drawing in the 2 eigenvectors and figuring out if they face in or out against the origin. Then I would draw the orbits. How do I do this one?,,"['ordinary-differential-equations', 'graphing-functions', 'eigenvalues-eigenvectors']"
73,How to differentiate an integration?,How to differentiate an integration?,,$$\int_{q_1}^{q_2}f_T(t)dt=0.6826\ldots(1)$$ How differentiating equation $(1)$ with respect to $q_1$ yields  $$f_T(q_2)\frac{dq_2}{dq_1}-f_T(q_1)=0$$,$$\int_{q_1}^{q_2}f_T(t)dt=0.6826\ldots(1)$$ How differentiating equation $(1)$ with respect to $q_1$ yields  $$f_T(q_2)\frac{dq_2}{dq_1}-f_T(q_1)=0$$,,"['ordinary-differential-equations', 'derivatives', 'definite-integrals']"
74,First order Differential Equation. How to bring it to standard form?,First order Differential Equation. How to bring it to standard form?,,I came across this eq. in Agarwal: Ordinary and Partial diff. equations book. $$(y^2-1) + 2(x-y(1+y)^2)y'=0 $$ Is there a clever substitution to bring this to standard form?,I came across this eq. in Agarwal: Ordinary and Partial diff. equations book. $$(y^2-1) + 2(x-y(1+y)^2)y'=0 $$ Is there a clever substitution to bring this to standard form?,,['ordinary-differential-equations']
75,Evaluating integration with Laplace transform,Evaluating integration with Laplace transform,,I am taking a differential equation class and for Laplace transformations and I have to find $$\displaystyle \int_0^\infty \dfrac{\sin t}{t}dt.$$ How can I do that?,I am taking a differential equation class and for Laplace transformations and I have to find $$\displaystyle \int_0^\infty \dfrac{\sin t}{t}dt.$$ How can I do that?,,"['ordinary-differential-equations', 'laplace-transform']"
76,Rate of cooling with unknown starting temp,Rate of cooling with unknown starting temp,,An object with $T_0 = x$ is in a room with fixed temp 30 celcius. After ten minutes the objects temp is negative $10$ celsius and after 10 more minutes it is negative $5$ celsius What was the $T_0$? This is a Newton's law of cooling problem. I guess since there is no k given that the cooling rate is constant and it warms five degrees per 10 minutes to $T_0 = -15$,An object with $T_0 = x$ is in a room with fixed temp 30 celcius. After ten minutes the objects temp is negative $10$ celsius and after 10 more minutes it is negative $5$ celsius What was the $T_0$? This is a Newton's law of cooling problem. I guess since there is no k given that the cooling rate is constant and it warms five degrees per 10 minutes to $T_0 = -15$,,"['calculus', 'ordinary-differential-equations']"
77,How to obtain the arbitrary-constant-free solution of this differential equation?,How to obtain the arbitrary-constant-free solution of this differential equation?,,"I know that the substitution $$ u := y^3 $$ changes the differential equation $$ (x + y^3) + 6xyy^{\prime} = 0 $$ into the linear equation $$ u^{\prime} + \frac{1}{2x} u = - \frac{1}{2}, $$ and the latter has a solution $$ u = c x^{-1/2} - \frac{1}{3}x,$$ where $c$ is an arbitrary constant, for $x > 0$. However, the above equation also has the solution $$ u = -\frac{1}{3}x $$ for all $x$. How to obtain this solution? What is the systematic procedure?","I know that the substitution $$ u := y^3 $$ changes the differential equation $$ (x + y^3) + 6xyy^{\prime} = 0 $$ into the linear equation $$ u^{\prime} + \frac{1}{2x} u = - \frac{1}{2}, $$ and the latter has a solution $$ u = c x^{-1/2} - \frac{1}{3}x,$$ where $c$ is an arbitrary constant, for $x > 0$. However, the above equation also has the solution $$ u = -\frac{1}{3}x $$ for all $x$. How to obtain this solution? What is the systematic procedure?",,['ordinary-differential-equations']
78,Problems with 2 homogeneous differential equations,Problems with 2 homogeneous differential equations,,Hello friends. I do not see at the moment how to solve it. Thank you,Hello friends. I do not see at the moment how to solve it. Thank you,,"['ordinary-differential-equations', 'homogeneous-equation']"
79,Solution of ODE using piecewise functions,Solution of ODE using piecewise functions,,"For a fixed $i$, let $\psi_i$ is piecewise exponential function for the mesh point $x_i$ such that for all $j$, $0\leq j\leq N$, $\psi_i$ is the solution of $$\epsilon\psi_i^{''}+(\bar a(x)\psi_i)^{'}=0, \;\;\; \psi_i(x_j)=\delta_{ij}.$$ The variable $\bar a(x)$ is approximated by a constant $$\bar a(x)=\bar a_i, \forall x\in (x_{i-1},x_i].$$ Letting $\rho_j=\bar a_jh/\epsilon$, where $h=x_i-x_{i-1}$ and assuming a uniform mesh. I want to show that the solution of this ODE is given by $$\psi_j(x)= \left\{      \begin{array}{ll}        \dfrac{1-e^{-\rho_j(x-x_{j-1})/h}}{1-e^{-\rho_j}}, & x_{j-1}\leq x\leq x_j\\        1- \dfrac{1-e^{-\rho_{j+1}(x-x_{j})/h}}{1-e^{-\rho_{j+1}}},& x_{j}\leq x\leq x_{j+1}\\ 0 & \mbox{elsewhere.}      \end{array}    \right. $$ I have been able to do: \begin{align} \psi_j(x)&=A_1+A_2e^{-\bar a(x)x/\epsilon}\\ &=A_1+A_2e^{-\bar a_jhx/(h\epsilon)}, \;\;x_{i-1}\leq x\leq x_i\\ &=A_1+A_2e^{-\rho_{j}x/h} \end{align} I am stuck with the constants now. I am trying to follow this book http://www.amazon.com/Numerical-Methods-Singular-Perturbation-Problems/dp/9814390739 page 27.","For a fixed $i$, let $\psi_i$ is piecewise exponential function for the mesh point $x_i$ such that for all $j$, $0\leq j\leq N$, $\psi_i$ is the solution of $$\epsilon\psi_i^{''}+(\bar a(x)\psi_i)^{'}=0, \;\;\; \psi_i(x_j)=\delta_{ij}.$$ The variable $\bar a(x)$ is approximated by a constant $$\bar a(x)=\bar a_i, \forall x\in (x_{i-1},x_i].$$ Letting $\rho_j=\bar a_jh/\epsilon$, where $h=x_i-x_{i-1}$ and assuming a uniform mesh. I want to show that the solution of this ODE is given by $$\psi_j(x)= \left\{      \begin{array}{ll}        \dfrac{1-e^{-\rho_j(x-x_{j-1})/h}}{1-e^{-\rho_j}}, & x_{j-1}\leq x\leq x_j\\        1- \dfrac{1-e^{-\rho_{j+1}(x-x_{j})/h}}{1-e^{-\rho_{j+1}}},& x_{j}\leq x\leq x_{j+1}\\ 0 & \mbox{elsewhere.}      \end{array}    \right. $$ I have been able to do: \begin{align} \psi_j(x)&=A_1+A_2e^{-\bar a(x)x/\epsilon}\\ &=A_1+A_2e^{-\bar a_jhx/(h\epsilon)}, \;\;x_{i-1}\leq x\leq x_i\\ &=A_1+A_2e^{-\rho_{j}x/h} \end{align} I am stuck with the constants now. I am trying to follow this book http://www.amazon.com/Numerical-Methods-Singular-Perturbation-Problems/dp/9814390739 page 27.",,"['ordinary-differential-equations', 'numerical-methods']"
80,Understanding differentials,Understanding differentials,,"What is a good reference to learn about differentials and related topics.  Some of my questions are: Why is it possible to split $dy/dx$ into individual terms $dx$ and $dy$? In a separated differential equation such as $F(x)dx + G(y)dy = 0$, what is the physical intuition behind ""$F(x)dx$""? When integrating the latter equation, what variable is integrated over?  Usually, if $f(x) = 0$, then we can integrate over $x$, like $\int f(x)\,dx=c$. But how de we arrive at $\int F(x)dx + \int G(y)dy = c$? A detailed, but introductory-level reference is sought.  As a bonus, feel free to shed light on any of the above questions.","What is a good reference to learn about differentials and related topics.  Some of my questions are: Why is it possible to split $dy/dx$ into individual terms $dx$ and $dy$? In a separated differential equation such as $F(x)dx + G(y)dy = 0$, what is the physical intuition behind ""$F(x)dx$""? When integrating the latter equation, what variable is integrated over?  Usually, if $f(x) = 0$, then we can integrate over $x$, like $\int f(x)\,dx=c$. But how de we arrive at $\int F(x)dx + \int G(y)dy = c$? A detailed, but introductory-level reference is sought.  As a bonus, feel free to shed light on any of the above questions.",,"['calculus', 'ordinary-differential-equations', 'reference-request', 'derivatives']"
81,How to solve the given initial-value problem?,How to solve the given initial-value problem?,,"Solve the given problem which the input function $g(x)$ is discontinuous? $y''+4y = g(x)$, $y(0) = 1$, $y'(0) = 2$, where $$g(x) = \begin{cases} \sin x, & 0\leq x\leq\frac{\pi}{2}\\ 0,& x>\frac{\pi}{2} \end{cases}$$ And the given answer is, $$y = \begin{cases} \cos 2x+\frac56\sin2x+\frac13\sin x, & 0\leq x\leq\frac{\pi}{2}\\ \frac23\cos 2x+\frac56\sin2x,& x>\frac{\pi}{2} \end{cases}$$","Solve the given problem which the input function $g(x)$ is discontinuous? $y''+4y = g(x)$, $y(0) = 1$, $y'(0) = 2$, where $$g(x) = \begin{cases} \sin x, & 0\leq x\leq\frac{\pi}{2}\\ 0,& x>\frac{\pi}{2} \end{cases}$$ And the given answer is, $$y = \begin{cases} \cos 2x+\frac56\sin2x+\frac13\sin x, & 0\leq x\leq\frac{\pi}{2}\\ \frac23\cos 2x+\frac56\sin2x,& x>\frac{\pi}{2} \end{cases}$$",,['ordinary-differential-equations']
82,Check solutions of vector Differential Equations,Check solutions of vector Differential Equations,,I have solved the vector ODE: $x\prime = \begin{pmatrix}1& 1 \\ -1 &1 \end{pmatrix}x$ I found an eigenvalue $\lambda=1+i$ and deduced the corresponding eigenvector: \begin{align} (A-\lambda I)x =& 0 \\ \begin{pmatrix}1-1-i & 1 \\-1& 1-1-i \end{pmatrix}x =& 0 \\ \begin{pmatrix} -i&1\\-1&-i\end{pmatrix}x =&0 \end{align} Which is similar to: $\begin{pmatrix}i&-1\\0&0 \end{pmatrix}x = 0$ By Row Reduction. Take $x_2=1$ as $x_2$ is free. We then have the following equation: \begin{align} &ix_1 - x_2 = 0 \\ \iff& ix_1 = 1 \\ \iff& x_1 = \frac{1}{i} \end{align} Thus the corresponding eigenvector for $\lambda=1+i$ is: $\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix}$. My solution should then be:  \begin{align} x(t) =& e^{(1+i)t}\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix} \\      =& e^t e^{it}\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix} \\ =& e^t\left(\cos(t) + i\sin(t)\right)\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix} \\ \end{align} By taking only the real parts we have the general solution: $\left(c_1e^t\cos(t) + c_2e^t\sin(t)\right)\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix}$ How can I quickly check this is correct? Idealy I would like to use Sage to verify. I think this would be faster than differentiating my solution and checking whether I get the original equation.,I have solved the vector ODE: $x\prime = \begin{pmatrix}1& 1 \\ -1 &1 \end{pmatrix}x$ I found an eigenvalue $\lambda=1+i$ and deduced the corresponding eigenvector: \begin{align} (A-\lambda I)x =& 0 \\ \begin{pmatrix}1-1-i & 1 \\-1& 1-1-i \end{pmatrix}x =& 0 \\ \begin{pmatrix} -i&1\\-1&-i\end{pmatrix}x =&0 \end{align} Which is similar to: $\begin{pmatrix}i&-1\\0&0 \end{pmatrix}x = 0$ By Row Reduction. Take $x_2=1$ as $x_2$ is free. We then have the following equation: \begin{align} &ix_1 - x_2 = 0 \\ \iff& ix_1 = 1 \\ \iff& x_1 = \frac{1}{i} \end{align} Thus the corresponding eigenvector for $\lambda=1+i$ is: $\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix}$. My solution should then be:  \begin{align} x(t) =& e^{(1+i)t}\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix} \\      =& e^t e^{it}\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix} \\ =& e^t\left(\cos(t) + i\sin(t)\right)\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix} \\ \end{align} By taking only the real parts we have the general solution: $\left(c_1e^t\cos(t) + c_2e^t\sin(t)\right)\begin{pmatrix} \dfrac{1}{i} \\ 1\end{pmatrix}$ How can I quickly check this is correct? Idealy I would like to use Sage to verify. I think this would be faster than differentiating my solution and checking whether I get the original equation.,,"['ordinary-differential-equations', 'solution-verification', 'sagemath']"
83,How to solve the differential equation for the motion equation of a body in a gravitational field from one fixed source,How to solve the differential equation for the motion equation of a body in a gravitational field from one fixed source,,"I want to develop the motion equation of a body in a classic gravitational field ($F=\frac{Gm_1m_2}{r^2}$). Starting by creating the lagrangian of a body under gravitational force, in polar coordinates. The speed in direction $\hat{r}$ is $\dot{r}$ and the speed in direction $\hat{\theta}$ is $r\dot{\theta}$. So the kinetic energy of the body is $K=\frac{m}{2}\left(\dot{r}^2+r^2\dot{\theta}^2\right)$ and the potential energy is $U=-\frac{GMm}{r}$. $M$ is the mass of the source generating the gravitational field (a star), and $m$ is the mass of the body (a planet). Creating the lagrangian we get: $$\mathcal{L}=K-U=\frac{m}{2}\left(\dot{r}^2+r^2\dot{\theta}^2\right)+\frac{GMm}{r}$$ Writing down the Euler–Lagrange equation ($\frac{{\partial}\mathcal{L}}{{\partial}q}=\frac{d}{dt}\left(\frac{{\partial}\mathcal{L}}{{\partial}\dot{q}}\right)$)we get $$mr^2\ddot{\theta}=0$$ $$\dot{\theta}=\frac{p_{\theta}}{mr^2}$$ $p_\theta$ is the angular momentum which is conserved $$\theta=\frac{p_{\theta}}{mr^2}t+\theta_0$$ $$m\ddot{r}=mr\dot{\theta}^2-\frac{GMm}{r^2}$$ $$\ddot{r}=\frac{p_{\theta}^2}{m^2}r^{-3}-GMr^{-2}$$ How do I solve the differential equation?","I want to develop the motion equation of a body in a classic gravitational field ($F=\frac{Gm_1m_2}{r^2}$). Starting by creating the lagrangian of a body under gravitational force, in polar coordinates. The speed in direction $\hat{r}$ is $\dot{r}$ and the speed in direction $\hat{\theta}$ is $r\dot{\theta}$. So the kinetic energy of the body is $K=\frac{m}{2}\left(\dot{r}^2+r^2\dot{\theta}^2\right)$ and the potential energy is $U=-\frac{GMm}{r}$. $M$ is the mass of the source generating the gravitational field (a star), and $m$ is the mass of the body (a planet). Creating the lagrangian we get: $$\mathcal{L}=K-U=\frac{m}{2}\left(\dot{r}^2+r^2\dot{\theta}^2\right)+\frac{GMm}{r}$$ Writing down the Euler–Lagrange equation ($\frac{{\partial}\mathcal{L}}{{\partial}q}=\frac{d}{dt}\left(\frac{{\partial}\mathcal{L}}{{\partial}\dot{q}}\right)$)we get $$mr^2\ddot{\theta}=0$$ $$\dot{\theta}=\frac{p_{\theta}}{mr^2}$$ $p_\theta$ is the angular momentum which is conserved $$\theta=\frac{p_{\theta}}{mr^2}t+\theta_0$$ $$m\ddot{r}=mr\dot{\theta}^2-\frac{GMm}{r^2}$$ $$\ddot{r}=\frac{p_{\theta}^2}{m^2}r^{-3}-GMr^{-2}$$ How do I solve the differential equation?",,"['ordinary-differential-equations', 'physics']"
84,How to get the linear equation system for finite element method from the variational formulation,How to get the linear equation system for finite element method from the variational formulation,,"Let the problem be $$-u'' + a(x) u = f , \;x \in \Omega = ]0,1[ , \;u(0) = \alpha ; \;u(1) = \beta,$$ where $f \in L^2(\Omega) , a(x) \geq a_0 > 0 , a(x) \in L^{\infty}(\Omega).$ This problem admits a unique solution in $V = H^1(\Omega).$ My question is: How we can use the finite elements $\mathbb{P}_1$ to prove that this problem, in the finite element space, is equivalent to a linear system $A U = b$ . BTW: for this, we can use the relation for linear function $\psi$:  $$\dfrac{1}{h} \int_{x_i}^{x_{i+1}} \psi(x) dx = \psi\left(\dfrac{x_i + x_{i+1}}{2}\right).$$","Let the problem be $$-u'' + a(x) u = f , \;x \in \Omega = ]0,1[ , \;u(0) = \alpha ; \;u(1) = \beta,$$ where $f \in L^2(\Omega) , a(x) \geq a_0 > 0 , a(x) \in L^{\infty}(\Omega).$ This problem admits a unique solution in $V = H^1(\Omega).$ My question is: How we can use the finite elements $\mathbb{P}_1$ to prove that this problem, in the finite element space, is equivalent to a linear system $A U = b$ . BTW: for this, we can use the relation for linear function $\psi$:  $$\dfrac{1}{h} \int_{x_i}^{x_{i+1}} \psi(x) dx = \psi\left(\dfrac{x_i + x_{i+1}}{2}\right).$$",,"['ordinary-differential-equations', 'numerical-methods']"
85,"Relationship between Turing bifurcation, saddle-node bifurcation, and Hopf bifurcation?","Relationship between Turing bifurcation, saddle-node bifurcation, and Hopf bifurcation?",,"Quoting from http://jxshix.people.wm.edu/2009-harbin-course/mississippi-bifurcation-2.pdf a Turing bifurcation occurs when for an ODE and related PDE $u' = f(u,v), v' = g(u,v)$ $u_t = d_1 \nabla u + f(u,v), v_t = d_2 \nabla v + g(u,v)$ a constant solution $u(x,t) = u_0, v(x,t) = v_0$ remains a stable steady state for the ODE and becomes an unstable steady state for the PDE. Stable steady states are generally destabilized either by one real eigenvalue (of the associated Jacobian matrix evaluated in the steady state) crossing the imaginary axis or by a pair of two complex eigenvalues crossing the imaginary axis (i.e. the real part changes from negative to positive values). Generally, one real eigenvalue crossing the imaginary axis is called saddle-node bifurcation. A pair of complex eigenvalues crossing the imaginary axis is referred to as Hopf bifuraction. Are these two types of bifurcations exactly the ones that occur for the PDE (while the steady state remains stable for the ODE) in what is generally referred to as Turing bifurcation / Turing instability? If so, what are the qualitative differences, if any, between the patterns formed past the Turing bifurcation?","Quoting from http://jxshix.people.wm.edu/2009-harbin-course/mississippi-bifurcation-2.pdf a Turing bifurcation occurs when for an ODE and related PDE $u' = f(u,v), v' = g(u,v)$ $u_t = d_1 \nabla u + f(u,v), v_t = d_2 \nabla v + g(u,v)$ a constant solution $u(x,t) = u_0, v(x,t) = v_0$ remains a stable steady state for the ODE and becomes an unstable steady state for the PDE. Stable steady states are generally destabilized either by one real eigenvalue (of the associated Jacobian matrix evaluated in the steady state) crossing the imaginary axis or by a pair of two complex eigenvalues crossing the imaginary axis (i.e. the real part changes from negative to positive values). Generally, one real eigenvalue crossing the imaginary axis is called saddle-node bifurcation. A pair of complex eigenvalues crossing the imaginary axis is referred to as Hopf bifuraction. Are these two types of bifurcations exactly the ones that occur for the PDE (while the steady state remains stable for the ODE) in what is generally referred to as Turing bifurcation / Turing instability? If so, what are the qualitative differences, if any, between the patterns formed past the Turing bifurcation?",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-modeling', 'bifurcation']"
86,"Integrating $ a\,f(x) +b\,y(x)=\frac{dy}{dx}$",Integrating," a\,f(x) +b\,y(x)=\frac{dy}{dx}","Can somebody put me on the right track for integrating the following equation? How do I separate the variables? $$ a\,f(x) +b\,y(x)=\frac{dy}{dx}$$","Can somebody put me on the right track for integrating the following equation? How do I separate the variables? $$ a\,f(x) +b\,y(x)=\frac{dy}{dx}$$",,['ordinary-differential-equations']
87,Second-order ODE with substitution,Second-order ODE with substitution,,"I’m struggling with this question: Use the substitution $y(t) = z(t)\,e^{-t}$ to transform the ordinary differential equation   $$\frac{d^2 y}{dt^2} + 2\,\frac{dy}{dt} + y = t^2 e^{-t}$$   into an ordinary differential equation for $z(t)$. Hence obtain the general solution for $y(t)$. What is the specific solution which satisfies the initial conditions, $y(0) = 1$ and $y'(0) = 0$? I have an exam in two hours, and I can’t get around this question. So far I managed to get the first order and second order differentials for $z(t)$, and found that $z''(t) = t^2$, but I don’t know what to do next. How should I proceed?","I’m struggling with this question: Use the substitution $y(t) = z(t)\,e^{-t}$ to transform the ordinary differential equation   $$\frac{d^2 y}{dt^2} + 2\,\frac{dy}{dt} + y = t^2 e^{-t}$$   into an ordinary differential equation for $z(t)$. Hence obtain the general solution for $y(t)$. What is the specific solution which satisfies the initial conditions, $y(0) = 1$ and $y'(0) = 0$? I have an exam in two hours, and I can’t get around this question. So far I managed to get the first order and second order differentials for $z(t)$, and found that $z''(t) = t^2$, but I don’t know what to do next. How should I proceed?",,['ordinary-differential-equations']
88,How can I prove that the solutions of this differential equation is monotone?,How can I prove that the solutions of this differential equation is monotone?,,"I'm trying to proof if $x:I\to \mathbb R$ a maximal regular solution of $x'=f(x)$, such that the image $x(I)\subset \mathbb R$ is bounded and $f:\mathbb R\to \mathbb R$ is $C^1$, then $x$ is strictly monotone. How can I prove that $f$ doesn't change sign? I need help Thanks a lot","I'm trying to proof if $x:I\to \mathbb R$ a maximal regular solution of $x'=f(x)$, such that the image $x(I)\subset \mathbb R$ is bounded and $f:\mathbb R\to \mathbb R$ is $C^1$, then $x$ is strictly monotone. How can I prove that $f$ doesn't change sign? I need help Thanks a lot",,['ordinary-differential-equations']
89,Hermite's equation of order $\alpha$,Hermite's equation of order,\alpha,Show that the general solution of Hermite's equation of order $\alpha$: $${y}''-2x{y}'+2\alpha y=0$$ $$is$$ $$y(x)=c_{0}y_{1}(x)+c_{1}y_{2}(x)$$ where $y_{1}(x)$ and $y_{2}(x)$ are power series solutions centered at the ordinary point 0. Find the two power series. I am stuck at this question. I don't really know what to do.,Show that the general solution of Hermite's equation of order $\alpha$: $${y}''-2x{y}'+2\alpha y=0$$ $$is$$ $$y(x)=c_{0}y_{1}(x)+c_{1}y_{2}(x)$$ where $y_{1}(x)$ and $y_{2}(x)$ are power series solutions centered at the ordinary point 0. Find the two power series. I am stuck at this question. I don't really know what to do.,,"['sequences-and-series', 'ordinary-differential-equations', 'special-functions', 'power-series', 'summation']"
90,To solve a differential equation,To solve a differential equation,,How to solve the differential equation $3 \Big(\dfrac {dy}{dx}\Big)\Big(\dfrac{d^2y}{dx^2}\Big)^2 + \Bigg(\Big(\dfrac{dy}{dx}\Big)^2 -1\Bigg) \Bigg(\Big(\dfrac {dy}{dx}\Big)^3 -\dfrac{dy}{dx}  -\dfrac{d^3y}{dx^3}\Bigg) =0$   ?,How to solve the differential equation $3 \Big(\dfrac {dy}{dx}\Big)\Big(\dfrac{d^2y}{dx^2}\Big)^2 + \Bigg(\Big(\dfrac{dy}{dx}\Big)^2 -1\Bigg) \Bigg(\Big(\dfrac {dy}{dx}\Big)^3 -\dfrac{dy}{dx}  -\dfrac{d^3y}{dx^3}\Bigg) =0$   ?,,['ordinary-differential-equations']
91,differential equation12,differential equation12,,"After 30 days of radioactive decay,100 mg of a radioactive substance was observed to remain. After 120 days, only 30 mg of this substance was left.  A. How much substance was originally present? B. What is the half-life of this radioactive substance? C. How long will it take before only 1% of the original amount remains? I have no clue what so ever","After 30 days of radioactive decay,100 mg of a radioactive substance was observed to remain. After 120 days, only 30 mg of this substance was left.  A. How much substance was originally present? B. What is the half-life of this radioactive substance? C. How long will it take before only 1% of the original amount remains? I have no clue what so ever",,['ordinary-differential-equations']
92,Is this definite integral impossible?,Is this definite integral impossible?,,"From my understanding when you integrate $f(x)$ you get $F(x)+C$, and when finding a definite integral the $C's$ cancels out due to subtraction. However, I came across an example where the $C$ doesn't cancel out: so I started with the following differential equation: $$(1+x^2) \frac{dy}{dx}=2xy$$ and suppose I wanted to find the area under $dy/dx$ between $a$ and $b$. All you simply have to do is find $y$, evaluate at $b$ and $a$, and subtract. The solution to this equation is $y=(x^2+1)e^C$. Now if you evaluate and subtract, you get $(b^2+1)e^C-(a^2+1)e^C$. Is this integral impossible unless I have more information which allows me to determine $C$? Thanks!","From my understanding when you integrate $f(x)$ you get $F(x)+C$, and when finding a definite integral the $C's$ cancels out due to subtraction. However, I came across an example where the $C$ doesn't cancel out: so I started with the following differential equation: $$(1+x^2) \frac{dy}{dx}=2xy$$ and suppose I wanted to find the area under $dy/dx$ between $a$ and $b$. All you simply have to do is find $y$, evaluate at $b$ and $a$, and subtract. The solution to this equation is $y=(x^2+1)e^C$. Now if you evaluate and subtract, you get $(b^2+1)e^C-(a^2+1)e^C$. Is this integral impossible unless I have more information which allows me to determine $C$? Thanks!",,"['calculus', 'integration', 'ordinary-differential-equations', 'definite-integrals']"
93,Resonance Frequencies of Oscillator,Resonance Frequencies of Oscillator,,"I understand that resonance is when the force term increases the natural oscillation of the system. In the next equation the oscillator has a natural frequency $\omega_0=\sqrt{\frac{k}{m}}$. But I don't know how to find which frequencies $\omega$ create resonance. $$y''\left(t\right)+2\zeta y'\left(t\right)+y\left(t\right)=\sin{(\omega t+\phi)}$$ where $\zeta=\frac{c}{2\sqrt{km}}$, $c$ is the friction coefficient, $k$ the elasticity constant and $m$ the mass. this equation has been rescaled using $y(x)=x(\omega_0t)$.","I understand that resonance is when the force term increases the natural oscillation of the system. In the next equation the oscillator has a natural frequency $\omega_0=\sqrt{\frac{k}{m}}$. But I don't know how to find which frequencies $\omega$ create resonance. $$y''\left(t\right)+2\zeta y'\left(t\right)+y\left(t\right)=\sin{(\omega t+\phi)}$$ where $\zeta=\frac{c}{2\sqrt{km}}$, $c$ is the friction coefficient, $k$ the elasticity constant and $m$ the mass. this equation has been rescaled using $y(x)=x(\omega_0t)$.",,"['ordinary-differential-equations', 'physics']"
94,Proving a Probability Generating Function satisfies a partial differential Equation,Proving a Probability Generating Function satisfies a partial differential Equation,,"We have N animals grazing in a field. The animals graze independently, and periods of grazing and resting alternate for the animals. If an animal is resting at time t, the probability it begins grazing in the interval between $t$ and $t+h$ is $\lambda h+o(h)$, and if it is grazing at time t, the probability it will begin resting between $t$ and $t+h$ is $\mu h+o(h)$. Let $X(t)$ denote the number of animals grazing at time $t\ge 0$ We need to show that the pgf $P\left(t,s\right)=E\left(s^\left(Xt\right)\right)$ satisfies: $${\partial P\over \partial t}= \left(1-s\right) \left(\mu+\lambda s\right) {\partial P\over \partial s}+N\lambda \left(s-1\right)P$$ The problem I have is in finding the forward Kolmogorov equations, since we need to adjust the birth and death coefficients, and I don't really know how to alter the baseline case. Any hint is greatly appreciated!","We have N animals grazing in a field. The animals graze independently, and periods of grazing and resting alternate for the animals. If an animal is resting at time t, the probability it begins grazing in the interval between $t$ and $t+h$ is $\lambda h+o(h)$, and if it is grazing at time t, the probability it will begin resting between $t$ and $t+h$ is $\mu h+o(h)$. Let $X(t)$ denote the number of animals grazing at time $t\ge 0$ We need to show that the pgf $P\left(t,s\right)=E\left(s^\left(Xt\right)\right)$ satisfies: $${\partial P\over \partial t}= \left(1-s\right) \left(\mu+\lambda s\right) {\partial P\over \partial s}+N\lambda \left(s-1\right)P$$ The problem I have is in finding the forward Kolmogorov equations, since we need to adjust the birth and death coefficients, and I don't really know how to alter the baseline case. Any hint is greatly appreciated!",,"['probability', 'ordinary-differential-equations', 'stochastic-processes', 'generating-functions']"
95,A question on differential equation,A question on differential equation,,"Consider the following second order differential equation: $$ \frac{d^2y}{dt^2} + \frac{2v +1}{t} \frac{dy}{dt} + y = 0, v \in \mathbb{R} $$ If this has a solution $y(t)$ that is smooth on an interval about $0$. Show that $y(-t)$ is also a solution. My obstacle is I do not know how to use smoothness of $y$ to get started. Any hint is appreciated.","Consider the following second order differential equation: $$ \frac{d^2y}{dt^2} + \frac{2v +1}{t} \frac{dy}{dt} + y = 0, v \in \mathbb{R} $$ If this has a solution $y(t)$ that is smooth on an interval about $0$. Show that $y(-t)$ is also a solution. My obstacle is I do not know how to use smoothness of $y$ to get started. Any hint is appreciated.",,['ordinary-differential-equations']
96,Linear ODE question,Linear ODE question,,"We have an first order ODE : Equation1 :   $y' + y = x$  ?  We can view the left-hand side as an operator acting on $y$. In that case $L=(d/dx + 1)$ $L(y_1) = x$ $L(y_2)=x$ $L(y_1+y_2)=x$ So, clearly $L(y_1+y_2) = x \neq L(y_1)+L(y_2) = 2x$ So why is $y'+y=x$ is a linear ODE ?","We have an first order ODE : Equation1 :   $y' + y = x$  ?  We can view the left-hand side as an operator acting on $y$. In that case $L=(d/dx + 1)$ $L(y_1) = x$ $L(y_2)=x$ $L(y_1+y_2)=x$ So, clearly $L(y_1+y_2) = x \neq L(y_1)+L(y_2) = 2x$ So why is $y'+y=x$ is a linear ODE ?",,['ordinary-differential-equations']
97,Spectrum of eigenvalues and eigenfunctions,Spectrum of eigenvalues and eigenfunctions,,"Our O.D.Es professor had the ""amazing"" idea of heavily introducing advanced linear algebra material (which is not an official prerequisite for the course) along with boundary value problems. Not being trained in these types of exercises I am facing quite a few difficulties. If anybody would be willing to help, it would be most appreciated. For the following sets of boundary conditions, consider the equation: $$u''(x) + \lambda u(x) = 0, \ \ \ 0 < x < 1$$ and find the spectrum of eigenvalues, in essence the set of values $\lambda$ in the complex plane for which a nontrivial solution exists, and give the eigenfunction (or eigenfunctions) for each such eigenvalue. 1) $u(0) + u(1) = 0$ and $u'(0) + u'(1) = 0$  2) $u(0) + u(1) = 0$ and $u'(0) - u'(1) = 0$","Our O.D.Es professor had the ""amazing"" idea of heavily introducing advanced linear algebra material (which is not an official prerequisite for the course) along with boundary value problems. Not being trained in these types of exercises I am facing quite a few difficulties. If anybody would be willing to help, it would be most appreciated. For the following sets of boundary conditions, consider the equation: $$u''(x) + \lambda u(x) = 0, \ \ \ 0 < x < 1$$ and find the spectrum of eigenvalues, in essence the set of values $\lambda$ in the complex plane for which a nontrivial solution exists, and give the eigenfunction (or eigenfunctions) for each such eigenvalue. 1) $u(0) + u(1) = 0$ and $u'(0) + u'(1) = 0$  2) $u(0) + u(1) = 0$ and $u'(0) - u'(1) = 0$",,"['real-analysis', 'ordinary-differential-equations']"
98,Poincare-Bendixson in a $2\times 2$ system,Poincare-Bendixson in a  system,2\times 2,"Show that the system $x^{'} = x-y-x^{3}$ and $y^{'}=x+y-y^{3}$ Has a periodic solution using Poincare-Bendixson I really have no idea how to apply this theorem i know that i need find an orbit and that is basically the hard part clearly 0,0 is a fixed point i want to remove it and then find a upper bound which should be all that difficult once x and y exceed 1 everything should get dragged inside of it. can i do this without converting to polar?","Show that the system $x^{'} = x-y-x^{3}$ and $y^{'}=x+y-y^{3}$ Has a periodic solution using Poincare-Bendixson I really have no idea how to apply this theorem i know that i need find an orbit and that is basically the hard part clearly 0,0 is a fixed point i want to remove it and then find a upper bound which should be all that difficult once x and y exceed 1 everything should get dragged inside of it. can i do this without converting to polar?",,"['ordinary-differential-equations', 'dynamical-systems']"
99,Need to use another method to solve a 1st order linear differential equation,Need to use another method to solve a 1st order linear differential equation,,"$$y' - \frac{x}{(x^2+1)}y = 2x(x^2+1)$$ I need to solve this differential equation using the normal integrating factor method for 1st order linear DEs, and a second method chosen from: separable equations, homogenous equations, Bernoulli equations and exact equations. I had no problem solving it using the normal method for linear equations, but I don't see how any of these cases apply to the DE above. The only method that seems plausible is exact equations. I tried to use an integrating factor to turn it into an exact equation, but it did not work out. This is what I've tried in terms of exact equations: $$y' - \frac{x}{(x^2+1)}y = 2x(x^2+1)$$ $$\frac{dy}{dx} = 2x(x^2 + 1) + \frac{x}{x^2+1}y$$ $$\frac{dy}{dx} = x \left[2x^2 + 2 + \frac{1}{x^2+1}y\right]$$ $$[\frac{1}{x}] \, dy = \left[2x^2 + 2 + \frac{1}{x^2+1}y\right] \, dx$$ $$ \left[2x^2 + 2 + \frac{1}{x^2+1}y\right] \, dx + \left[\frac{-1}{x}\right] \, dy = 0$$ So that is the exact equation I got. Then: $$\frac{\partial M}{\partial y} = \frac{1}{x^2+1} \text{ and } \frac{\partial N}{\partial x} = \frac{1}{x^2}$$ And now I'm stuck. I tried to get an integrating factor here to make $\frac{\partial M}{\partial y} = \frac{\partial N}{\partial x}$ , but it gets extremely complicated and it can't be what the professor intended for us to do. I think I just screwed up somewhere because $\frac{\partial M}{\partial y}$ and $\frac{\partial N}{\partial x}$ are so similar that it seems like it is the correct method to use. I've been working on this for a long time. Can anyone here please give me a hint or tell me if I'm overlooking something glaringly obvious?","I need to solve this differential equation using the normal integrating factor method for 1st order linear DEs, and a second method chosen from: separable equations, homogenous equations, Bernoulli equations and exact equations. I had no problem solving it using the normal method for linear equations, but I don't see how any of these cases apply to the DE above. The only method that seems plausible is exact equations. I tried to use an integrating factor to turn it into an exact equation, but it did not work out. This is what I've tried in terms of exact equations: So that is the exact equation I got. Then: And now I'm stuck. I tried to get an integrating factor here to make , but it gets extremely complicated and it can't be what the professor intended for us to do. I think I just screwed up somewhere because and are so similar that it seems like it is the correct method to use. I've been working on this for a long time. Can anyone here please give me a hint or tell me if I'm overlooking something glaringly obvious?","y' - \frac{x}{(x^2+1)}y = 2x(x^2+1) y' - \frac{x}{(x^2+1)}y = 2x(x^2+1) \frac{dy}{dx} = 2x(x^2 + 1) + \frac{x}{x^2+1}y \frac{dy}{dx} = x \left[2x^2 + 2 + \frac{1}{x^2+1}y\right] [\frac{1}{x}] \, dy = \left[2x^2 + 2 + \frac{1}{x^2+1}y\right] \, dx  \left[2x^2 + 2 + \frac{1}{x^2+1}y\right] \, dx + \left[\frac{-1}{x}\right] \, dy = 0 \frac{\partial M}{\partial y} = \frac{1}{x^2+1} \text{ and } \frac{\partial N}{\partial x} = \frac{1}{x^2} \frac{\partial M}{\partial y} = \frac{\partial N}{\partial x} \frac{\partial M}{\partial y} \frac{\partial N}{\partial x}",['ordinary-differential-equations']
