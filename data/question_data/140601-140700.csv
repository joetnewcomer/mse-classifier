,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find $ \lim\limits_{{n \to \infty}} \frac1{2^n} \sum\limits_{k=1}^n \frac1{\sqrt{k}} \binom nk$,Find, \lim\limits_{{n \to \infty}} \frac1{2^n} \sum\limits_{k=1}^n \frac1{\sqrt{k}} \binom nk,"Find $$\lim_{n \to \infty} \frac {1}{2^n} \sum_{k=1}^n \frac{1}{\sqrt{k}} \binom{n}{k}.$$ First time I thought about Stirling's approximation but didn't get anything by applying it. I would also think about a Riemann Sum, but no idea how to rewrite... The answer is $0$.","Find $$\lim_{n \to \infty} \frac {1}{2^n} \sum_{k=1}^n \frac{1}{\sqrt{k}} \binom{n}{k}.$$ First time I thought about Stirling's approximation but didn't get anything by applying it. I would also think about a Riemann Sum, but no idea how to rewrite... The answer is $0$.",,['limits']
1,How to prove that $\lim\limits_{n \to \infty} \frac{k^n}{n!} = 0$ [duplicate],How to prove that  [duplicate],\lim\limits_{n \to \infty} \frac{k^n}{n!} = 0,"This question already has answers here : Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. (15 answers) Closed 8 years ago . It recently came to my mind, how to prove that the factorial grows faster than the exponential, or that the linear grows faster than the logarithmic, etc... I thought about writing: $$ a(n) = \frac{k^n}{n!} = \frac{ k \times k \times \dots \times k}{1\times 2\times\dots\times n} = \frac k1 \times \frac k2 \times \dots \times \frac kn = \frac k1 \times \frac k2 \times \dots \times \frac kk \times \frac k{k+1} \times \dots \times \frac kn $$ It's obvious that after k/k, every factor is smaller than 1, and by increasing n, k/n gets closer to 0, like if we had $\lim_{n \to \infty} (k/n) = 0$, for any constant $k$. But, I think this is not a clear proof... so any hint is accepted. Thank you for consideration.","This question already has answers here : Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$. (15 answers) Closed 8 years ago . It recently came to my mind, how to prove that the factorial grows faster than the exponential, or that the linear grows faster than the logarithmic, etc... I thought about writing: $$ a(n) = \frac{k^n}{n!} = \frac{ k \times k \times \dots \times k}{1\times 2\times\dots\times n} = \frac k1 \times \frac k2 \times \dots \times \frac kn = \frac k1 \times \frac k2 \times \dots \times \frac kk \times \frac k{k+1} \times \dots \times \frac kn $$ It's obvious that after k/k, every factor is smaller than 1, and by increasing n, k/n gets closer to 0, like if we had $\lim_{n \to \infty} (k/n) = 0$, for any constant $k$. But, I think this is not a clear proof... so any hint is accepted. Thank you for consideration.",,"['limits', 'factorial']"
2,What could be this number?,What could be this number?,,"In a much larger problem, I need to solve for $x_n$ the equation $$\sum_{i=0}^n (n+i)^{x_n}=(2n+1)^{x_n}$$ where $n$ can be large. From a numerical point of view, it does not make any practical difficulty solving instead $$\log\left(\sum_{i=0}^n (n+i)^{x_n} \right)={x_n}\log(2n+1)$$ which is ""almost"" finding the intersection of two straight lines. My first surprise, working with small values of $n$, was to notice that the solution is almost linearly dependent on $n$. Working with large numbers,  what I noticed is the following $$\left( \begin{array}{cc}  n & \frac {x_n} n \\ 10^1 & 1.351611135 \\ 10^2 & 1.382828492 \\ 10^3 & 1.385947786 \\ 10^4 & 1.386259704 \end{array} \right)$$ and the ratio ""seems"" to converge to something . So, my questions : Could we prove the linear dependency of $x_n$ to $n$ ? Could we prove that $\frac {x_n} n$ tends to a limit ? If there is a limit, what could be this number ? In other words, would it be possible to establish the asymptotics of $x_n$ ? Edit after answers After the beautiful answers I received from Professor Vector and Raymond Manzoni (who made the problem more general), I solved, for $n=10^4$, the equation$$\sum_{i=0}^n (n+i)^{x_n}=(2n+a)^{x_n}$$ In the table below are reproduced the results for the computed ratio $\frac {x_n} n$ and the value computed for $y_a=-2\log(b)$ where $b$ is the positive solution of $1-b=b^a$. $$\left( \begin{array}{cc}  a & \frac {x_n} n & y_a\\  1 & 1.38626 &1.38629 \\  2 & 0.96240 &0.96242 \\  3 & 0.76448 &0.76449 \\  4 & 0.64457 &0.64457 \\  5 & 0.56240 &0.56240 \\  6 & 0.50184 &0.50183 \\  7 & 0.45496 &0.45495 \\  8 & 0.41739 &0.41737 \\  9 & 0.38646 &0.38644 \end{array} \right)$$ Concerning the case where $a=1$, it is interesting to compare the results I gave with the asymptotics  Raymond Manzoni gave in a comment. $$\frac {x_n} n=2\log(2)-\frac{\log(2)}{2n}+O\left(\frac 1 {n^2} \right)$$ $$\left( \begin{array}{cc}  n & \frac {x_n} n &2\log(2)-\frac{\log(2)}{2n}\\ 10^1 & 1.351611135 &1.351637002\\ 10^2 & 1.382828492 &1.382828625\\ 10^3 & 1.385947786 &1.385947788\\ 10^4 & 1.386259704 &1.386259704 \end{array} \right)$$ I do not think that I could add any comment !","In a much larger problem, I need to solve for $x_n$ the equation $$\sum_{i=0}^n (n+i)^{x_n}=(2n+1)^{x_n}$$ where $n$ can be large. From a numerical point of view, it does not make any practical difficulty solving instead $$\log\left(\sum_{i=0}^n (n+i)^{x_n} \right)={x_n}\log(2n+1)$$ which is ""almost"" finding the intersection of two straight lines. My first surprise, working with small values of $n$, was to notice that the solution is almost linearly dependent on $n$. Working with large numbers,  what I noticed is the following $$\left( \begin{array}{cc}  n & \frac {x_n} n \\ 10^1 & 1.351611135 \\ 10^2 & 1.382828492 \\ 10^3 & 1.385947786 \\ 10^4 & 1.386259704 \end{array} \right)$$ and the ratio ""seems"" to converge to something . So, my questions : Could we prove the linear dependency of $x_n$ to $n$ ? Could we prove that $\frac {x_n} n$ tends to a limit ? If there is a limit, what could be this number ? In other words, would it be possible to establish the asymptotics of $x_n$ ? Edit after answers After the beautiful answers I received from Professor Vector and Raymond Manzoni (who made the problem more general), I solved, for $n=10^4$, the equation$$\sum_{i=0}^n (n+i)^{x_n}=(2n+a)^{x_n}$$ In the table below are reproduced the results for the computed ratio $\frac {x_n} n$ and the value computed for $y_a=-2\log(b)$ where $b$ is the positive solution of $1-b=b^a$. $$\left( \begin{array}{cc}  a & \frac {x_n} n & y_a\\  1 & 1.38626 &1.38629 \\  2 & 0.96240 &0.96242 \\  3 & 0.76448 &0.76449 \\  4 & 0.64457 &0.64457 \\  5 & 0.56240 &0.56240 \\  6 & 0.50184 &0.50183 \\  7 & 0.45496 &0.45495 \\  8 & 0.41739 &0.41737 \\  9 & 0.38646 &0.38644 \end{array} \right)$$ Concerning the case where $a=1$, it is interesting to compare the results I gave with the asymptotics  Raymond Manzoni gave in a comment. $$\frac {x_n} n=2\log(2)-\frac{\log(2)}{2n}+O\left(\frac 1 {n^2} \right)$$ $$\left( \begin{array}{cc}  n & \frac {x_n} n &2\log(2)-\frac{\log(2)}{2n}\\ 10^1 & 1.351611135 &1.351637002\\ 10^2 & 1.382828492 &1.382828625\\ 10^3 & 1.385947786 &1.385947788\\ 10^4 & 1.386259704 &1.386259704 \end{array} \right)$$ I do not think that I could add any comment !",,"['limits', 'numerical-methods', 'asymptotics']"
3,Limit of $x^x$ as $x$ tends to $0$,Limit of  as  tends to,x^x x 0,I am trying to solve the following limit: $$\lim \limits_{x\to0} x^x$$ The only thing that comes to mind is to write $x^x$ as $e^{x\ln{x}}$ and getting the right sided limit would be easy but I don't see how I could get the left sided one seeing that the $\ln$ is not defined for negative numbers. Is there something I am missing or is there another way to go about it? P.S.:I don't know anything about derivatives so please keep it to the limits.,I am trying to solve the following limit: The only thing that comes to mind is to write as and getting the right sided limit would be easy but I don't see how I could get the left sided one seeing that the is not defined for negative numbers. Is there something I am missing or is there another way to go about it? P.S.:I don't know anything about derivatives so please keep it to the limits.,\lim \limits_{x\to0} x^x x^x e^{x\ln{x}} \ln,"['limits', 'logarithms', 'exponentiation']"
4,Show $\lim\limits_{n\to\infty} \sqrt[n]{n^e+e^n}=e$,Show,\lim\limits_{n\to\infty} \sqrt[n]{n^e+e^n}=e,Why is $\lim\limits_{n\to\infty} \sqrt[n]{n^e+e^n}$ = $e$? I couldn't get this result.,Why is $\lim\limits_{n\to\infty} \sqrt[n]{n^e+e^n}$ = $e$? I couldn't get this result.,,['limits']
5,"Prove that the derivative of an even differentiable function is odd, and the derivative of an odd is even.","Prove that the derivative of an even differentiable function is odd, and the derivative of an odd is even.",,"Prove that the derivative of an even differentiable function is odd, and the derivative of an odd differentiable function is even. Here are my workings so far. Lets prove the derivative of an odd differentiable function is even first. Let the odd function be $f(x)$ . We have $f(-x)=-f(x)$ and $\lim_{x\to a^-} f(x)=\lim_{x\to a^+}f(x)=\lim_{x\to a}f(x)$ $$f'(-x)= \lim_{h\to 0} \frac {f(-x+h)-f(-x)}{h}= \lim_{h\to 0} \frac {-f(x-h)+f(x)}{h}$$ And so, I am stuck. Thanks in advance. Hints are appreciated. Solutions are even more welcome!","Prove that the derivative of an even differentiable function is odd, and the derivative of an odd differentiable function is even. Here are my workings so far. Lets prove the derivative of an odd differentiable function is even first. Let the odd function be . We have and And so, I am stuck. Thanks in advance. Hints are appreciated. Solutions are even more welcome!",f(x) f(-x)=-f(x) \lim_{x\to a^-} f(x)=\lim_{x\to a^+}f(x)=\lim_{x\to a}f(x) f'(-x)= \lim_{h\to 0} \frac {f(-x+h)-f(-x)}{h}= \lim_{h\to 0} \frac {-f(x-h)+f(x)}{h},"['limits', 'derivatives']"
6,Prove $\lim\limits_{n\toâˆž}{\sum\limits_{x=0}^n\binom nx(1+{\rm e}^{-(x+1)})^{n+1}\over\sum\limits_{x=0}^n\binom nx(1+{\rm e}^{-x})^{n + 1}}=\frac 13$,Prove,\lim\limits_{n\toâˆž}{\sum\limits_{x=0}^n\binom nx(1+{\rm e}^{-(x+1)})^{n+1}\over\sum\limits_{x=0}^n\binom nx(1+{\rm e}^{-x})^{n + 1}}=\frac 13,"I am trying to find limit of the following function:  $$ \lim_{n\rightarrow \infty}\frac{\sum\limits_{x = 0}^{n}\binom{n}{x}\left[1 + \mathrm{e}^{-(x+1)}\right]^{n + 1}}{\sum\limits_{x=0}^{n} \binom{n}{x}\left[1 + \mathrm{e}^{-x}\right]^{n + 1}}. $$ When I wrote a Python code for this, I saw that it converges to $1/3$. I am not sure how to approach this. Could somebody give a pointer about how to go about it? EDIT: I want just some positive lower bound on this. So even if limit cannot be evaluated exactly, it is okay if I get some lower bound on this.","I am trying to find limit of the following function:  $$ \lim_{n\rightarrow \infty}\frac{\sum\limits_{x = 0}^{n}\binom{n}{x}\left[1 + \mathrm{e}^{-(x+1)}\right]^{n + 1}}{\sum\limits_{x=0}^{n} \binom{n}{x}\left[1 + \mathrm{e}^{-x}\right]^{n + 1}}. $$ When I wrote a Python code for this, I saw that it converges to $1/3$. I am not sure how to approach this. Could somebody give a pointer about how to go about it? EDIT: I want just some positive lower bound on this. So even if limit cannot be evaluated exactly, it is okay if I get some lower bound on this.",,"['limits', 'exponential-function', 'binomial-coefficients']"
7,Prove that $\sum_{n=1}^\infty \left(\phi-\frac{F_{n+1}}{F_{n}}\right)=\frac{1}{\pi}$,Prove that,\sum_{n=1}^\infty \left(\phi-\frac{F_{n+1}}{F_{n}}\right)=\frac{1}{\pi},"So, I know that $$\lim_{n\to\infty}\frac{F_{n+1}}{F_n}=\phi$$ where $F_n$ stands for the n'th Fibonacci number I was interested in measuring the error of the convergence of the above limit and was drawn to the conjecture that: $$\sum_{n=1}^\infty \left(\phi-\frac{F_{n+1}}{F_{n}}\right)=\frac{1}{\pi}$$ How might we go about proving this result? Edit: The solution is actually not $\frac{1}{\pi}$. I had thought that it was due to how close the sum and $1/\pi$ actually are. I am curious (if one exists) if there is a closed form for the sum.","So, I know that $$\lim_{n\to\infty}\frac{F_{n+1}}{F_n}=\phi$$ where $F_n$ stands for the n'th Fibonacci number I was interested in measuring the error of the convergence of the above limit and was drawn to the conjecture that: $$\sum_{n=1}^\infty \left(\phi-\frac{F_{n+1}}{F_{n}}\right)=\frac{1}{\pi}$$ How might we go about proving this result? Edit: The solution is actually not $\frac{1}{\pi}$. I had thought that it was due to how close the sum and $1/\pi$ actually are. I am curious (if one exists) if there is a closed form for the sum.",,['limits']
8,"Prove: $\lim_{n\to\infty}{\sum_{m=0}^{n}{\sum_{k=0}^{n-m}{\frac{2^{n-m-k}}{n-m+1}\,\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{{{2n}\choose{n}}}}}}=\pi$",Prove:,"\lim_{n\to\infty}{\sum_{m=0}^{n}{\sum_{k=0}^{n-m}{\frac{2^{n-m-k}}{n-m+1}\,\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{{{2n}\choose{n}}}}}}=\pi","Consider the following limit: $$\lim_{n\to\infty}{\sum_{m=0}^{n}{\sum_{k=0}^{n-m}{\left[\frac{2^{n-m-k}}{n-m+1}\,\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{{{2n}\choose{n}}}\right]}}}=\pi$$ I cooked this up while playing around with power series (details below). Is there a more direct way to prove this limit? Consider the functions $f(x)=\frac{\tan^{-1}(\sqrt{1-x})}{\sqrt{1-x}}$ , $g(x)=\frac{\pi/2-\tan^{-1}(\sqrt{1-x})}{\sqrt{1-x}}$ . We write the power series: $$f(x)=\sum_{n=0}^{\infty}{(s_n\pi-r_n)x^n},\qquad{g}(x)=\sum_{n=0}^{\infty}{(s_n\pi+r_n)x^n}$$ where computing the first few terms suggests that $r_n$ , $s_n$ are rational. Indeed, we have $\frac{\pi/4-\tan^{-1}(\sqrt{1-x})}{\sqrt{1-x}}=\frac{g(x)-f(x)}{2}=\sum{r_nx^n}$ , and $\frac{1/4}{\sqrt{1-x}}=\frac{g(x)+f(x)}{2\pi}=\sum{s_nx^n}$ . Using $\frac{d}{dx}[\tan^{-1}(\sqrt{1-x})]=-\frac{1}{2(2-x)\sqrt{1-x}}$ , we can use the power series for $\frac{1}{2-x}$ and $\frac{1}{\sqrt{1-x}}$ to calculate the power series for $(\pi/4-\tan^{-1}(\sqrt{1-x}))$ , which consists of only rational coefficients. Combining this with the power series for $\frac{1}{\sqrt{1-x}}$ gives: $$r_n=\frac{1}{2}\sum_{m=0}^{n-1}{\sum_{k=0}^{n-m-1}{\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{(n-m)\cdot2^{k+m+n}}}}$$ We easily get $s_n=\frac{1}{2^{2n+2}}{2n\choose{n}}$ . Now, because no branch of $f(z)$ has singularities anywhere (the apparent singularity at $z=1$ is removable), the coefficients of the power series of $f$ must tend to zero. Hence $\lim_{n\to\infty}{\frac{r_{n+1}}{s_{n+1}}}=\pi$ , and the desired limit follows after simplifying. Notes: 1) It is easily shown that: $$s_n\pi-r_n=\int_{0}^{\pi/4}{\sin^{2n}{\theta}\,d\theta},\qquad{s}_n\pi+r_n=\int_{\pi/4}^{\pi/2}{\sin^{2n}{\theta}\,d\theta}$$ 2) The above proof actually shows: $$\lim_{n\to\infty}{\left(1+\frac{1}{2n+1}\right)\sum_{m=0}^{n}{\sum_{k=0}^{n-m}{\left[\frac{2^{n-m-k}}{n-m+1}\,\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{{{2n}\choose{n}}}\right]}}}=\pi$$ which converges much faster than the given limit.","Consider the following limit: I cooked this up while playing around with power series (details below). Is there a more direct way to prove this limit? Consider the functions , . We write the power series: where computing the first few terms suggests that , are rational. Indeed, we have , and . Using , we can use the power series for and to calculate the power series for , which consists of only rational coefficients. Combining this with the power series for gives: We easily get . Now, because no branch of has singularities anywhere (the apparent singularity at is removable), the coefficients of the power series of must tend to zero. Hence , and the desired limit follows after simplifying. Notes: 1) It is easily shown that: 2) The above proof actually shows: which converges much faster than the given limit.","\lim_{n\to\infty}{\sum_{m=0}^{n}{\sum_{k=0}^{n-m}{\left[\frac{2^{n-m-k}}{n-m+1}\,\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{{{2n}\choose{n}}}\right]}}}=\pi f(x)=\frac{\tan^{-1}(\sqrt{1-x})}{\sqrt{1-x}} g(x)=\frac{\pi/2-\tan^{-1}(\sqrt{1-x})}{\sqrt{1-x}} f(x)=\sum_{n=0}^{\infty}{(s_n\pi-r_n)x^n},\qquad{g}(x)=\sum_{n=0}^{\infty}{(s_n\pi+r_n)x^n} r_n s_n \frac{\pi/4-\tan^{-1}(\sqrt{1-x})}{\sqrt{1-x}}=\frac{g(x)-f(x)}{2}=\sum{r_nx^n} \frac{1/4}{\sqrt{1-x}}=\frac{g(x)+f(x)}{2\pi}=\sum{s_nx^n} \frac{d}{dx}[\tan^{-1}(\sqrt{1-x})]=-\frac{1}{2(2-x)\sqrt{1-x}} \frac{1}{2-x} \frac{1}{\sqrt{1-x}} (\pi/4-\tan^{-1}(\sqrt{1-x})) \frac{1}{\sqrt{1-x}} r_n=\frac{1}{2}\sum_{m=0}^{n-1}{\sum_{k=0}^{n-m-1}{\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{(n-m)\cdot2^{k+m+n}}}} s_n=\frac{1}{2^{2n+2}}{2n\choose{n}} f(z) z=1 f \lim_{n\to\infty}{\frac{r_{n+1}}{s_{n+1}}}=\pi s_n\pi-r_n=\int_{0}^{\pi/4}{\sin^{2n}{\theta}\,d\theta},\qquad{s}_n\pi+r_n=\int_{\pi/4}^{\pi/2}{\sin^{2n}{\theta}\,d\theta} \lim_{n\to\infty}{\left(1+\frac{1}{2n+1}\right)\sum_{m=0}^{n}{\sum_{k=0}^{n-m}{\left[\frac{2^{n-m-k}}{n-m+1}\,\frac{{{2k}\choose{k}}{{2m}\choose{m}}}{{{2n}\choose{n}}}\right]}}}=\pi","['limits', 'proof-verification', 'summation', 'binomial-coefficients', 'alternative-proof']"
9,Evalute $ \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} $,Evalute, \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} ,"Evaluate $\displaystyle \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)}$. $\bf{My\; Try::}$ Although we can solve it by converting into definite Integration. But I want to solve it without Using Integration. So $\displaystyle \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} = \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{(n-1)(n-2).......(n-k+1)}{k!\cdot n^{k}\cdot (k+3)}$ Now How can i solve after that, Help required, Thanks","Evaluate $\displaystyle \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)}$. $\bf{My\; Try::}$ Although we can solve it by converting into definite Integration. But I want to solve it without Using Integration. So $\displaystyle \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} = \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{(n-1)(n-2).......(n-k+1)}{k!\cdot n^{k}\cdot (k+3)}$ Now How can i solve after that, Help required, Thanks",,['limits']
10,Summation of an infinite Exponential series,Summation of an infinite Exponential series,,"Q. Find the value of - $$ \lim_{n\to\infty} \sum_{r=0}^{n} \frac{2^r}{5^{2^r} +1} $$ My attempt - I seem to be clueless to this problem. Though I think that later terms would be much small and negligible( Imagine how large would be $ 5^{2^r} $ after 3-4 terms), so I calculated the sum of first 3-4 terms and my answer was just around the actual answer    ( just a difference of $0.002$ ). But I wonder if there is an actual method to solve this problem? If it is, would you please share it to me? Any help would be appreciated","Q. Find the value of - $$ \lim_{n\to\infty} \sum_{r=0}^{n} \frac{2^r}{5^{2^r} +1} $$ My attempt - I seem to be clueless to this problem. Though I think that later terms would be much small and negligible( Imagine how large would be $ 5^{2^r} $ after 3-4 terms), so I calculated the sum of first 3-4 terms and my answer was just around the actual answer    ( just a difference of $0.002$ ). But I wonder if there is an actual method to solve this problem? If it is, would you please share it to me? Any help would be appreciated",,"['limits', 'summation', 'exponential-function']"
11,How find this limit $\lim\limits_{x\to 0^{+}}\frac{\sin{(\tan{x})}-\tan{(\sin{x})}}{x^7}$,How find this limit,\lim\limits_{x\to 0^{+}}\frac{\sin{(\tan{x})}-\tan{(\sin{x})}}{x^7},"Find the limit $$\lim_{x\to 0^{+}}\dfrac{\sin{(\tan{x})}-\tan{(\sin{x})}}{x^7}$$ My attempt: Since $$\sin{x}=x-\dfrac{1}{3!}x^3+\dfrac{1}{5!}x^5-\dfrac{1}{7!}x^7+o(x^7)$$ $$\tan{x}=x+\dfrac{1}{3}x^3+\dfrac{2}{15}x^5+\dfrac{1}{63}x^7+o(x^3)$$ So $$\sin{(\tan{x})}=\tan{x}-\dfrac{1}{3!}(\tan{x})^3+\dfrac{1}{5!}(\tan{x})^5-\dfrac{1}{7!}(\tan{x})^7+o(x^7)$$ Though this method might solve, I think this problem has nicer methods. Thanks.","Find the limit My attempt: Since So Though this method might solve, I think this problem has nicer methods. Thanks.",\lim_{x\to 0^{+}}\dfrac{\sin{(\tan{x})}-\tan{(\sin{x})}}{x^7} \sin{x}=x-\dfrac{1}{3!}x^3+\dfrac{1}{5!}x^5-\dfrac{1}{7!}x^7+o(x^7) \tan{x}=x+\dfrac{1}{3}x^3+\dfrac{2}{15}x^5+\dfrac{1}{63}x^7+o(x^3) \sin{(\tan{x})}=\tan{x}-\dfrac{1}{3!}(\tan{x})^3+\dfrac{1}{5!}(\tan{x})^5-\dfrac{1}{7!}(\tan{x})^7+o(x^7),['limits']
12,"Letting $S(m)$ be the digit sum of $m$, then $\lim_{n\to\infty}S(3^n)=\infty$?","Letting  be the digit sum of , then ?",S(m) m \lim_{n\to\infty}S(3^n)=\infty,"For any $m\in\mathbb N$, let $S(m)$ be the digit sum of $m$ in the decimal system. For example, $S(1234)=1+2+3+4=10, S(2^5)=S(32)=5$. Question 1 :Is the following true? $$\lim_{n\to\infty}S(3^n)=\infty.$$ Question 2 :How about $S(m^n)$ for $m\ge 4$ except some trivial cases? Motivation : I've got the following :  $$\lim_{n\to\infty}S(2^n)=\infty.$$ Proof : The point of this proof is that there exists a non-zero number between the ${m+1}^{th}$ digit and ${4m}^{th}$ digit. If  $$2^n=A\cdot{10}^{4m}+B, B\lt {10}^m, 0\lt A,$$ then $2^n\ge {10}^{4m}\gt 2^{4m}$ leads $n\gt 4m$. Hence, the left side can be divided by $2^{4m}$. Also, $B$ must be divided by $2^{4m}$ because ${10}^{4m}=2^{4m}\cdot 5^{4m}$. However, since $$B\lt {10}^m\lt {16}^m=2^{4m},$$ $B$ can not be divided by $2^{4m}$ if $B\not=0$. If $B=0$, then the right side can be divided by $5$ but the left side cannot be divided by $5$. Hence, we now know that there is a non-zero number between the ${m+1}^{th}$ digit and ${4m}^{th}$ digit. Since $2^n$ cannot be divided by $5$, the first digit is not $0$. There exists non-zero number between the second digit and the fourth digit. Again, there exists non-zero number between $5^{th}$ digit and ${16}^{th}$ digit. By the same argument as above, if $2^n$ has more than $4^k$ digits, then $S(n)\ge {k+1}$. Hence, $$n\log {2}\ge 4^k-1\ \ \Rightarrow \ \ S(n)\ge k+1.$$ Now we know that  $$\lim_{n\to\infty}S(2^n)=\infty$$ as desired. Now the proof is completed. However, I've been facing difficulty for the $m=3$ case. I've got $\lim\sup S(3^n)=\infty$. Proof : Suppose that $3^n$ has $m$ digits. Letting $l=\varphi({10}^m)+n$, then  $$3^l-3^n=3^n(3^{\varphi({10}^m)}-1).$$ Since this can be divided by ${10}^m$, we know that the last $m$ digits of $3^l$ are equal to those of $3^n$. Hence, we get $\lim\sup S(3^n)=\infty$. However, I can't get $\lim\inf S(3^n)$. Can anyone help? Update : I crossposted to MO .","For any $m\in\mathbb N$, let $S(m)$ be the digit sum of $m$ in the decimal system. For example, $S(1234)=1+2+3+4=10, S(2^5)=S(32)=5$. Question 1 :Is the following true? $$\lim_{n\to\infty}S(3^n)=\infty.$$ Question 2 :How about $S(m^n)$ for $m\ge 4$ except some trivial cases? Motivation : I've got the following :  $$\lim_{n\to\infty}S(2^n)=\infty.$$ Proof : The point of this proof is that there exists a non-zero number between the ${m+1}^{th}$ digit and ${4m}^{th}$ digit. If  $$2^n=A\cdot{10}^{4m}+B, B\lt {10}^m, 0\lt A,$$ then $2^n\ge {10}^{4m}\gt 2^{4m}$ leads $n\gt 4m$. Hence, the left side can be divided by $2^{4m}$. Also, $B$ must be divided by $2^{4m}$ because ${10}^{4m}=2^{4m}\cdot 5^{4m}$. However, since $$B\lt {10}^m\lt {16}^m=2^{4m},$$ $B$ can not be divided by $2^{4m}$ if $B\not=0$. If $B=0$, then the right side can be divided by $5$ but the left side cannot be divided by $5$. Hence, we now know that there is a non-zero number between the ${m+1}^{th}$ digit and ${4m}^{th}$ digit. Since $2^n$ cannot be divided by $5$, the first digit is not $0$. There exists non-zero number between the second digit and the fourth digit. Again, there exists non-zero number between $5^{th}$ digit and ${16}^{th}$ digit. By the same argument as above, if $2^n$ has more than $4^k$ digits, then $S(n)\ge {k+1}$. Hence, $$n\log {2}\ge 4^k-1\ \ \Rightarrow \ \ S(n)\ge k+1.$$ Now we know that  $$\lim_{n\to\infty}S(2^n)=\infty$$ as desired. Now the proof is completed. However, I've been facing difficulty for the $m=3$ case. I've got $\lim\sup S(3^n)=\infty$. Proof : Suppose that $3^n$ has $m$ digits. Letting $l=\varphi({10}^m)+n$, then  $$3^l-3^n=3^n(3^{\varphi({10}^m)}-1).$$ Since this can be divided by ${10}^m$, we know that the last $m$ digits of $3^l$ are equal to those of $3^n$. Hence, we get $\lim\sup S(3^n)=\infty$. However, I can't get $\lim\inf S(3^n)$. Can anyone help? Update : I crossposted to MO .",,"['number-theory', 'limits']"
13,Closed form for $\lim\limits_{n\to\infty}\prod\limits_{k=1}^n{\left(2-\frac{2n^2-\pi^2+8}{n^2}\cos{\frac{(2k-1)\pi}{n}}\right)}$?,Closed form for ?,\lim\limits_{n\to\infty}\prod\limits_{k=1}^n{\left(2-\frac{2n^2-\pi^2+8}{n^2}\cos{\frac{(2k-1)\pi}{n}}\right)},"I am looking for a closed form for: $$\lim_{n\to\infty}\prod_{k=1}^n{\left(2-\frac{2n^2-\pi^2+8}{n^2}\cos{\frac{(2k-1)\pi}{n}}\right)}$$ (Wolfram suggests that it's approximately 6.17966.) Context : I was thinking about the curve $y=2^{n-1}(x-\cos{\frac{0\pi}{n}})(x-\cos{\frac{1\pi}{n}})(x-\cos{\frac{2\pi}{n}})...(x-\cos{\frac{n\pi}{n}})$ . Here is the graph when $n=6$ , for example: It is an $n+1$ degree curve tangent to the unit circle at $n$ points, and the total area of the $n$ regions enclosed by the curve and the x -axis is always $1$ (for any $n$ ). The product , let's call it $P$ , of the areas of the regions enclosed by the curve and the x -axis, approaches $0$ as $n\to\infty$ . But then I discovered that if we magnify the curve so that the average area of the regions is always $2$ (for any $n$ ), then $P$ approaches 6.17966... as $n\to\infty$ . The limit in the question is this number. (To derive the limit expression, I used an equivalent expression for the curve: $y=-\sqrt{1-x^2}\sin{(n\arccos{x})}$ , and stretched it vertically and horizontally by scale factor $\sqrt{2n}$ so that the average area of the regions is $2$ .) This idea of the product of areas of regions in a circle approaching some positive number is inspired by this question . My attempt : I tried relating the limit to a similar limit, $\lim_{n\to\infty}\prod_{k=1}^n{\left(2-2\cos{\frac{(2k-1)\pi}{n}}\right)}=4$ , as well as taking the log of the product, but I didn't make any progress. I am not very familiar with evaluating infinite products and would like to learn more.","I am looking for a closed form for: (Wolfram suggests that it's approximately 6.17966.) Context : I was thinking about the curve . Here is the graph when , for example: It is an degree curve tangent to the unit circle at points, and the total area of the regions enclosed by the curve and the x -axis is always (for any ). The product , let's call it , of the areas of the regions enclosed by the curve and the x -axis, approaches as . But then I discovered that if we magnify the curve so that the average area of the regions is always (for any ), then approaches 6.17966... as . The limit in the question is this number. (To derive the limit expression, I used an equivalent expression for the curve: , and stretched it vertically and horizontally by scale factor so that the average area of the regions is .) This idea of the product of areas of regions in a circle approaching some positive number is inspired by this question . My attempt : I tried relating the limit to a similar limit, , as well as taking the log of the product, but I didn't make any progress. I am not very familiar with evaluating infinite products and would like to learn more.",\lim_{n\to\infty}\prod_{k=1}^n{\left(2-\frac{2n^2-\pi^2+8}{n^2}\cos{\frac{(2k-1)\pi}{n}}\right)} y=2^{n-1}(x-\cos{\frac{0\pi}{n}})(x-\cos{\frac{1\pi}{n}})(x-\cos{\frac{2\pi}{n}})...(x-\cos{\frac{n\pi}{n}}) n=6 n+1 n n 1 n P 0 n\to\infty 2 n P n\to\infty y=-\sqrt{1-x^2}\sin{(n\arccos{x})} \sqrt{2n} 2 \lim_{n\to\infty}\prod_{k=1}^n{\left(2-2\cos{\frac{(2k-1)\pi}{n}}\right)}=4,"['limits', 'polynomials', 'circles', 'closed-form', 'infinite-product']"
14,Calculate limit with summation index in formula [duplicate],Calculate limit with summation index in formula [duplicate],,This question already has answers here : Closed 11 years ago . Possible Duplicate: Compute the limit: $\lim_{n\rightarrow\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ I want to calculate the following: $$ \lim_{n \rightarrow \infty} \left( e^{-n} \sum_{i = 0}^{n} \frac{n^i}{i!} \right) $$ Numerical calculations show it has a value close to 0.5. But I am not able to derive this analytically. My problem is that I am lacking a methodology of handling the $n$ both as a summation limit and a variable in the equation.,This question already has answers here : Closed 11 years ago . Possible Duplicate: Compute the limit: $\lim_{n\rightarrow\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ I want to calculate the following: $$ \lim_{n \rightarrow \infty} \left( e^{-n} \sum_{i = 0}^{n} \frac{n^i}{i!} \right) $$ Numerical calculations show it has a value close to 0.5. But I am not able to derive this analytically. My problem is that I am lacking a methodology of handling the $n$ both as a summation limit and a variable in the equation.,,['limits']
15,Is there an algorithm for deciding big/little-O queries?,Is there an algorithm for deciding big/little-O queries?,,"Consider the following question: Is $2^{o(\log n)} = o(n^c)$ for every constant $0 < c < 1$? I can try to answer this question using basic limits to get that the answer is ""yes."" I can also put a query of the following form into Wolfram alpha: What is $\lim_{n \to \infty} \frac{2^{\log^{0.999}n}}{n^{0.0001}}$? And it will spit out zero, while it fails to answer the first question because it apparently does not contain information about little-o notation or I don't know how to specify that this is what I want. My question is: given an input query of the above form, that is, two expressions composed as an elementary combination of elementary functions and Big/Little-O (or $\omega, \Omega, \Theta$), is the problem of answering such queries decidable? Is it in P? If so, could you provide a reference to the algorithm? Are there any existing mathematical software packages that can answer queries of this form?","Consider the following question: Is $2^{o(\log n)} = o(n^c)$ for every constant $0 < c < 1$? I can try to answer this question using basic limits to get that the answer is ""yes."" I can also put a query of the following form into Wolfram alpha: What is $\lim_{n \to \infty} \frac{2^{\log^{0.999}n}}{n^{0.0001}}$? And it will spit out zero, while it fails to answer the first question because it apparently does not contain information about little-o notation or I don't know how to specify that this is what I want. My question is: given an input query of the above form, that is, two expressions composed as an elementary combination of elementary functions and Big/Little-O (or $\omega, \Omega, \Theta$), is the problem of answering such queries decidable? Is it in P? If so, could you provide a reference to the algorithm? Are there any existing mathematical software packages that can answer queries of this form?",,"['limits', 'algorithms', 'asymptotics', 'computer-science', 'math-software']"
16,Iterated Limits [duplicate],Iterated Limits [duplicate],,"This question already has answers here : Limits of 2 variable functions (2 answers) Closed 8 years ago . If $$\lim_{(x,y) \to (a,b)} f(x,y) = L$$ and if the one dimensional limits -  $$ \lim_{x \to a} f(x,y)$$ and $$ \lim_{y \to b} f(x,y)$$                                                                both exist, then prove that - $$ \lim_{x \to a}\left[\lim_{y \to b} f(x,y)\right] = \lim_{y \to b}\left[\lim_{x \to a} f(x,y)\right] = L$$","This question already has answers here : Limits of 2 variable functions (2 answers) Closed 8 years ago . If $$\lim_{(x,y) \to (a,b)} f(x,y) = L$$ and if the one dimensional limits -  $$ \lim_{x \to a} f(x,y)$$ and $$ \lim_{y \to b} f(x,y)$$                                                                both exist, then prove that - $$ \lim_{x \to a}\left[\lim_{y \to b} f(x,y)\right] = \lim_{y \to b}\left[\lim_{x \to a} f(x,y)\right] = L$$",,"['limits', 'multivariable-calculus']"
17,Prove that the limit of $\sqrt{n+1}-\sqrt{n}$ is zero,Prove that the limit of  is zero,\sqrt{n+1}-\sqrt{n},"How would I go about proving that $\lim_{n\to\infty}\sqrt{n+1}-\sqrt{n}=0$? I have tried to use Squeeze theorem but have not been able to come up with bounds that converge to zero. Additionally, I don't think that converting to polar is possible here.","How would I go about proving that $\lim_{n\to\infty}\sqrt{n+1}-\sqrt{n}=0$? I have tried to use Squeeze theorem but have not been able to come up with bounds that converge to zero. Additionally, I don't think that converting to polar is possible here.",,"['limits', 'radicals']"
18,Why does $\sin(0)$ exist?,Why does  exist?,\sin(0),"I can't understand why should $\sin(0)$ exist, because if an angle is $0^{\circ}$, then the triangle doesn't exist i.e. there is no perpendicular or hypotenuse. However, if we take $\lim_{x \to 0} \sin(x)$, then I can understand $$\lim_{x \to 0} \sin(x) = 0$$ since perpendicular $\approx$ 0. So although $\lim_{x \to 0} \sin(x)$ is $0$, I can't understand how $\sin(0)=0$, and if $\sin(0)$ is not defined, then why is graph of $\sin(x)$ continuous ?","I can't understand why should $\sin(0)$ exist, because if an angle is $0^{\circ}$, then the triangle doesn't exist i.e. there is no perpendicular or hypotenuse. However, if we take $\lim_{x \to 0} \sin(x)$, then I can understand $$\lim_{x \to 0} \sin(x) = 0$$ since perpendicular $\approx$ 0. So although $\lim_{x \to 0} \sin(x)$ is $0$, I can't understand how $\sin(0)=0$, and if $\sin(0)$ is not defined, then why is graph of $\sin(x)$ continuous ?",,"['limits', 'trigonometry']"
19,"Why does this process, when iterated, tend towards a certain number? (the golden ratio?)","Why does this process, when iterated, tend towards a certain number? (the golden ratio?)",,"Take any number $x$ (edit: x should be positive, heh) Add 1 to it $x+1$ Find its reciprocal $1/(x+1)$ Repeat from 2 So, taking $x = 1$ to start: 1 2 (the + 1) 0.5 (the reciprocal) 1.5 (the + 1) 0.666... (the reciprocal) 1.666... (the + 1) 0.6 (the reciprocal) 1.6 (the + 1) 0.625 1.625 0.61584... 1.61584... 0.619047... 1.619047... 0.617647058823.. etc. If we look at just the ""step 3""'s (the reciprocals), we get: 1 0.5 0.666... 0.6 0.625 0.61584... 0.619047... 0.617647058823.. This appears to always converge to 0.61803399... no matter where you start from.  I looked up this number and it is often called ""The golden ratio"" - 1, or $\frac{1+\sqrt{5}}{2}-1$. Is there any ""mathematical"" way to represent the above procedure (or the terms of the second series, of ""only reciprocals"") as a limit or series? Why does this converge to what it does for every starting point $x$? edit: darn, I just realized that the golden ratio is actually 1.618... and not 0.618...; I edited my answer to change what the result is apparently (golden ratio - 1). However, I think I could easily make it the golden ratio by taking the +1 ""steps"" of the original series, instead of the reciprocation steps of the original series: 2 1.5 1.666... 1.6 1.625 1.61584... 1.619047... 1.617647058823.. which does converge to $\frac{1+\sqrt{5}}{2}-1$ Explaining either of these series is adequate as I believe that explaining one also explains the other.","Take any number $x$ (edit: x should be positive, heh) Add 1 to it $x+1$ Find its reciprocal $1/(x+1)$ Repeat from 2 So, taking $x = 1$ to start: 1 2 (the + 1) 0.5 (the reciprocal) 1.5 (the + 1) 0.666... (the reciprocal) 1.666... (the + 1) 0.6 (the reciprocal) 1.6 (the + 1) 0.625 1.625 0.61584... 1.61584... 0.619047... 1.619047... 0.617647058823.. etc. If we look at just the ""step 3""'s (the reciprocals), we get: 1 0.5 0.666... 0.6 0.625 0.61584... 0.619047... 0.617647058823.. This appears to always converge to 0.61803399... no matter where you start from.  I looked up this number and it is often called ""The golden ratio"" - 1, or $\frac{1+\sqrt{5}}{2}-1$. Is there any ""mathematical"" way to represent the above procedure (or the terms of the second series, of ""only reciprocals"") as a limit or series? Why does this converge to what it does for every starting point $x$? edit: darn, I just realized that the golden ratio is actually 1.618... and not 0.618...; I edited my answer to change what the result is apparently (golden ratio - 1). However, I think I could easily make it the golden ratio by taking the +1 ""steps"" of the original series, instead of the reciprocation steps of the original series: 2 1.5 1.666... 1.6 1.625 1.61584... 1.619047... 1.617647058823.. which does converge to $\frac{1+\sqrt{5}}{2}-1$ Explaining either of these series is adequate as I believe that explaining one also explains the other.",,"['limits', 'convergence-divergence']"
20,Is a continuous function plus a discontinuous function discontinuous?,Is a continuous function plus a discontinuous function discontinuous?,,"More specifically, I am wondering if $\sin(1/x) - x$ is discontinuous at $0$. I know that $f(x)= \sin(1/x)$ is discontinuous at $0$, but $f(x) = -x$ is continuous at all points. However if I add these two functions together, does that make sum of the function discontinuous?","More specifically, I am wondering if $\sin(1/x) - x$ is discontinuous at $0$. I know that $f(x)= \sin(1/x)$ is discontinuous at $0$, but $f(x) = -x$ is continuous at all points. However if I add these two functions together, does that make sum of the function discontinuous?",,"['limits', 'continuity']"
21,Evaluation of $\lim_{x\rightarrow \infty}\left\{\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\right\}$,Evaluation of,\lim_{x\rightarrow \infty}\left\{\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\right\},"Evaluation of $\displaystyle \lim_{x\rightarrow \infty}\left\{\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\right\}$ $\bf{My\; Try::}$ Here $(x+1)\;,(x+2)\;,(x+3)\;,(x+4)\;,(x+5)>0\;,$ when $x\rightarrow \infty$ So Using $\bf{A.M\geq G.M}\;,$ We get $$\frac{x+1+x+2+x+3+x+4+x+5}{5}\geq \left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}$$ So $$x+3\geq \left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}$$ So $$\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\leq 3$$ and equality hold when $x+1=x+2=x+3=x+4=x+5\;,$ Where $x\rightarrow \infty$ So $$\lim_{x\rightarrow 0}\left[\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\right]=3$$ Can we solve the above limit in that way, If not then how can we calculate it and also plz explain me where i have done wrong in above method Thanks","Evaluation of $\displaystyle \lim_{x\rightarrow \infty}\left\{\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\right\}$ $\bf{My\; Try::}$ Here $(x+1)\;,(x+2)\;,(x+3)\;,(x+4)\;,(x+5)>0\;,$ when $x\rightarrow \infty$ So Using $\bf{A.M\geq G.M}\;,$ We get $$\frac{x+1+x+2+x+3+x+4+x+5}{5}\geq \left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}$$ So $$x+3\geq \left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}$$ So $$\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\leq 3$$ and equality hold when $x+1=x+2=x+3=x+4=x+5\;,$ Where $x\rightarrow \infty$ So $$\lim_{x\rightarrow 0}\left[\left[(x+1)(x+2)(x+3)(x+4)(x+5)\right]^{\frac{1}{5}}-x\right]=3$$ Can we solve the above limit in that way, If not then how can we calculate it and also plz explain me where i have done wrong in above method Thanks",,"['limits', 'radicals']"
22,Limit involving power tower: $\lim\limits_{n\to\infty} \frac{n+1}n^{\frac n{n-1}^\cdots}$,Limit involving power tower:,\lim\limits_{n\to\infty} \frac{n+1}n^{\frac n{n-1}^\cdots},What is the value of the following limit? $$\large \lim_{n \to \infty} \left(\frac{n+1}{n}\right)^{\frac{n}{n-1}^{\frac{n-1}{n-2}^{...}}}$$ In general what do limits of infinite decreasing numbers strung together in familiar ways approach?,What is the value of the following limit? $$\large \lim_{n \to \infty} \left(\frac{n+1}{n}\right)^{\frac{n}{n-1}^{\frac{n-1}{n-2}^{...}}}$$ In general what do limits of infinite decreasing numbers strung together in familiar ways approach?,,"['limits', 'power-towers']"
23,Can $\sin n$ get arbitrarily close to $1$ for $n\in\mathbb{N}?$,Can  get arbitrarily close to  for,\sin n 1 n\in\mathbb{N}?,"Or put differently, does $$\lim_{n \to \infty}\big(\max \{\sin 1, \sin 2, \ldots ,\sin n\}\big) = 1?$$ My intuition says yes, but how can one prove this?","Or put differently, does $$\lim_{n \to \infty}\big(\max \{\sin 1, \sin 2, \ldots ,\sin n\}\big) = 1?$$ My intuition says yes, but how can one prove this?",,"['limits', 'trigonometry']"
24,Limit approaching infinity of sine function,Limit approaching infinity of sine function,,"I'd like to ask a question which I have been reflecting on for some time now. What is the limit of: $f(x) = \sin(x)$ as $x$ tends to infinity? As we know, the function has a definite value for each multiple of a value included between $0$ and $2\pi$, but, how can we know which value it will have at infinity?","I'd like to ask a question which I have been reflecting on for some time now. What is the limit of: $f(x) = \sin(x)$ as $x$ tends to infinity? As we know, the function has a definite value for each multiple of a value included between $0$ and $2\pi$, but, how can we know which value it will have at infinity?",,"['limits', 'trigonometry', 'infinity']"
25,Conjectured connection between $e$ and $\pi$ in a semidisk.,Conjectured connection between  and  in a semidisk.,e \pi,"A semidisk with diameter $\dfrac{e}{\pi}n$ is divided into $n$ regions of equal area by line segments from a diameter endpoint. Here is an example with $n=6$ . Consider the $n$ arcs between neighboring line segment endpoints. Let $P(n)=\text{product of arc lengths}$ . Is the following conjecture true: Conjecture: $\lim\limits_{n\to\infty}P(n)=\dfrac{\pi}{2}$ In other words, if the average arc length is $\color{red}{\dfrac{e}{2}}$ then the product of arc lengths approaches $\color{red}{\dfrac{\pi}{2}}$ . Evidence for my conjecture I got the following approximations. $P(1)\approx 1.359$ $P(2)\approx 1.438$ $P(3)\approx 1.469$ $P(6)\approx 1.507$ $P(12)\approx 1.528$ $P(24)\approx 1.543$ $P(48)\approx 1.552$ $P(96)\approx 1.558$ It seems that if the average arc length is less than $\dfrac{e}{2}$ then the product approaches $0$ , and if the average arc length is greater than $\dfrac{e}{2}$ then the product approaches $\infty$ . What makes this difficult What makes my conjecture difficult for me to prove or disprove, is that I cannot find exact expressions for the arc lengths. For example, in the example shown above with $n=6$ , the length of the longest arc is $\dfrac{3ex}{\pi}$ where $x-\sin x=\frac{\pi}{6}$ . I am aware of Kepler's equation , but that doesn't seem to help. Related questions This question is essentially the converse of my question , ""Product of areas in a disk"". I asked about the product of another kind of arc length related to a circle, in my question , ""Another interesting property of $y=2^{n-1}\prod_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right)$ : product of arc lengths converges, but to what?"". I recently asked a related question , ""Product of lengths in a disk of area $\pi/e$ "".","A semidisk with diameter is divided into regions of equal area by line segments from a diameter endpoint. Here is an example with . Consider the arcs between neighboring line segment endpoints. Let . Is the following conjecture true: Conjecture: In other words, if the average arc length is then the product of arc lengths approaches . Evidence for my conjecture I got the following approximations. It seems that if the average arc length is less than then the product approaches , and if the average arc length is greater than then the product approaches . What makes this difficult What makes my conjecture difficult for me to prove or disprove, is that I cannot find exact expressions for the arc lengths. For example, in the example shown above with , the length of the longest arc is where . I am aware of Kepler's equation , but that doesn't seem to help. Related questions This question is essentially the converse of my question , ""Product of areas in a disk"". I asked about the product of another kind of arc length related to a circle, in my question , ""Another interesting property of : product of arc lengths converges, but to what?"". I recently asked a related question , ""Product of lengths in a disk of area "".",\dfrac{e}{\pi}n n n=6 n P(n)=\text{product of arc lengths} \lim\limits_{n\to\infty}P(n)=\dfrac{\pi}{2} \color{red}{\dfrac{e}{2}} \color{red}{\dfrac{\pi}{2}} P(1)\approx 1.359 P(2)\approx 1.438 P(3)\approx 1.469 P(6)\approx 1.507 P(12)\approx 1.528 P(24)\approx 1.543 P(48)\approx 1.552 P(96)\approx 1.558 \dfrac{e}{2} 0 \dfrac{e}{2} \infty n=6 \dfrac{3ex}{\pi} x-\sin x=\frac{\pi}{6} y=2^{n-1}\prod_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right) \pi/e,"['limits', 'circles', 'infinite-product', 'conjectures', 'arc-length']"
26,Dirac delta function as a limit of sinc function,Dirac delta function as a limit of sinc function,,"I'm looking for a rigorous proof of the statement: $\delta(x) = \lim_{\epsilon->0} \frac{\sin(x/\epsilon)}{\pi x}$ (see (37) ). For any non-zero value of x, LHS of the above is by definition zero. But, for any non-zero value of x, the limit in RHS simply does not exist. So, how is this statement proved?","I'm looking for a rigorous proof of the statement: $\delta(x) = \lim_{\epsilon->0} \frac{\sin(x/\epsilon)}{\pi x}$ (see (37) ). For any non-zero value of x, LHS of the above is by definition zero. But, for any non-zero value of x, the limit in RHS simply does not exist. So, how is this statement proved?",,"['limits', 'distribution-theory', 'dirac-delta']"
27,Estimate of $n$th prime,Estimate of th prime,n,"There is a result that if $p_n$ is the $n$th prime, then $p_n\sim n\log n$ as $n\rightarrow\infty$. I wonder: Is it a direct consequence of the prime number theorem $\pi(x)\sim x/\log x$? The theorem says that there are approximately $n/\log n$ primes less than or equal to $n$. So there are approximately $n$ primes less than or equal to $n\log n$. So $p_n$ is approximately $n\log n$. But I'm having trouble turning this into a formal argument. We have $\lim_{n\rightarrow\infty}\dfrac{\pi(n)\log n}{n}=1$. How would it show that $\lim_{n\rightarrow\infty}\dfrac{p_n}{n\log n}=1$?","There is a result that if $p_n$ is the $n$th prime, then $p_n\sim n\log n$ as $n\rightarrow\infty$. I wonder: Is it a direct consequence of the prime number theorem $\pi(x)\sim x/\log x$? The theorem says that there are approximately $n/\log n$ primes less than or equal to $n$. So there are approximately $n$ primes less than or equal to $n\log n$. So $p_n$ is approximately $n\log n$. But I'm having trouble turning this into a formal argument. We have $\lim_{n\rightarrow\infty}\dfrac{\pi(n)\log n}{n}=1$. How would it show that $\lim_{n\rightarrow\infty}\dfrac{p_n}{n\log n}=1$?",,"['limits', 'prime-numbers']"
28,When can L'Hospital rule be used on Multivariable limits.,When can L'Hospital rule be used on Multivariable limits.,,"I am wondering about a multivariable limit, and in particular, is it ever valid to use L'hospital rule. For example, I am working on $$ \lim_{(x,y) \to (1,1)} \frac{x^3-y}{x-y}$$ This is what I have done, let $$f(x,y)=\frac{x^3-y}{x-y}$$ $f(x,0) \rightarrow 1$ as $(x,y) \rightarrow (1,1)$ and similiary $f(0,y) \rightarrow 1$ as $(x,y) \rightarrow (1,1)$ Okay now here is where I have a few questions ( I haven't looked at the answer or used wolfram or anything because I want to make sure I understand it first), should I continue to try out different parts, or should I try to see if I can prove the limit is 1. in trying different paths, say $$f(x,x^2)=\frac{x^2(1-x^3)}{(1-x)}$$ would it now be valid to use L'hospital? because the y is gone and we would have 0/0 as x $\rightarrow 1$? or is it never valid to use this rule for multi valued? Is this the right approach I should be taking or is there something else I should be thinking of? Thank you","I am wondering about a multivariable limit, and in particular, is it ever valid to use L'hospital rule. For example, I am working on $$ \lim_{(x,y) \to (1,1)} \frac{x^3-y}{x-y}$$ This is what I have done, let $$f(x,y)=\frac{x^3-y}{x-y}$$ $f(x,0) \rightarrow 1$ as $(x,y) \rightarrow (1,1)$ and similiary $f(0,y) \rightarrow 1$ as $(x,y) \rightarrow (1,1)$ Okay now here is where I have a few questions ( I haven't looked at the answer or used wolfram or anything because I want to make sure I understand it first), should I continue to try out different parts, or should I try to see if I can prove the limit is 1. in trying different paths, say $$f(x,x^2)=\frac{x^2(1-x^3)}{(1-x)}$$ would it now be valid to use L'hospital? because the y is gone and we would have 0/0 as x $\rightarrow 1$? or is it never valid to use this rule for multi valued? Is this the right approach I should be taking or is there something else I should be thinking of? Thank you",,"['limits', 'multivariable-calculus']"
29,Proving Thomae's function is nowhere differentiable.,Proving Thomae's function is nowhere differentiable.,,"I am given the function $$f(x)=\begin{cases} 0 \text{ ; when } x \text{ is irrational} \\\frac 1 q \text{ ; for } x=\frac p q \text{ irreducible fraction}\end{cases}$$ Spivak proved that for $a\in (0,1)$, we have $$\lim_{x\to a} f(x)=0$$ That is, this function is only continuous at the irrationals. I assume we consider $f$ for $x\geq 0$, since for negative $x$ it wouldn't be defined: If $x=-\dfrac p q=\dfrac {-p }q=\dfrac {p }{-q}$ should we take $f(x)=-\dfrac 1 q $ or $f(x)=\dfrac 1q$? Also, this function is periodic with period $1$, so we can just prove this on $(0,1)$ (as in the first case when proving continuity). Now, Spivak wants me to prove this function is not differentiable at $a$, for any $a$. It is clear that, since it isn't continuous at the rationals, it is not differentiable there. Thus we need to prove the claim for $a$ irrational. He gives the following hint. Suppose $a=n,a_1a_2a_3\dots$. Consider the expression $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h}$$ for $h$ rational, and also for $h=-0,0\dots0a_{n+1}a_{n+2}\dots$ I have been trying to work this out for a while, but I can't. I am not sure, also, if he meant to have the $n$ in the subindices match the $n$ of $a$, or if it is only a typo. The book has $$\frac{{f\left( {a + h} \right) - f\left( h \right)}}{h}$$ instead of $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h}$$ which is (I guess) a typo, too. For $h$ rational, one gets $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h} = 0$$ and for $h=-0,0\dots0a_{n+1}a_{n+2}\dots$, you get,assuming $a+h=m/u$ $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h} = \frac{{f\left( {a + h} \right)}}{h} = f\left( {\frac{m}{u}} \right)\frac{1}{h}$$ but I really don't know what to do with this. I know I have to show the limit $$\mathop {\lim }\limits_{h \to 0} \frac{{f\left( {a + h} \right) - f\left( a \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {a + h} \right)}}{h}$$ doesn't exist for any $a$, but I can't see how. I'm looking for good hints rather than full solutions.","I am given the function $$f(x)=\begin{cases} 0 \text{ ; when } x \text{ is irrational} \\\frac 1 q \text{ ; for } x=\frac p q \text{ irreducible fraction}\end{cases}$$ Spivak proved that for $a\in (0,1)$, we have $$\lim_{x\to a} f(x)=0$$ That is, this function is only continuous at the irrationals. I assume we consider $f$ for $x\geq 0$, since for negative $x$ it wouldn't be defined: If $x=-\dfrac p q=\dfrac {-p }q=\dfrac {p }{-q}$ should we take $f(x)=-\dfrac 1 q $ or $f(x)=\dfrac 1q$? Also, this function is periodic with period $1$, so we can just prove this on $(0,1)$ (as in the first case when proving continuity). Now, Spivak wants me to prove this function is not differentiable at $a$, for any $a$. It is clear that, since it isn't continuous at the rationals, it is not differentiable there. Thus we need to prove the claim for $a$ irrational. He gives the following hint. Suppose $a=n,a_1a_2a_3\dots$. Consider the expression $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h}$$ for $h$ rational, and also for $h=-0,0\dots0a_{n+1}a_{n+2}\dots$ I have been trying to work this out for a while, but I can't. I am not sure, also, if he meant to have the $n$ in the subindices match the $n$ of $a$, or if it is only a typo. The book has $$\frac{{f\left( {a + h} \right) - f\left( h \right)}}{h}$$ instead of $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h}$$ which is (I guess) a typo, too. For $h$ rational, one gets $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h} = 0$$ and for $h=-0,0\dots0a_{n+1}a_{n+2}\dots$, you get,assuming $a+h=m/u$ $$\frac{{f\left( {a + h} \right) - f\left( a \right)}}{h} = \frac{{f\left( {a + h} \right)}}{h} = f\left( {\frac{m}{u}} \right)\frac{1}{h}$$ but I really don't know what to do with this. I know I have to show the limit $$\mathop {\lim }\limits_{h \to 0} \frac{{f\left( {a + h} \right) - f\left( a \right)}}{h} = \mathop {\lim }\limits_{h \to 0} \frac{{f\left( {a + h} \right)}}{h}$$ doesn't exist for any $a$, but I can't see how. I'm looking for good hints rather than full solutions.",,"['limits', 'derivatives']"
30,Divide a ball of volume $\frac{e^2}{6}n$ into $n$ slices of equal height. What is the product of the volumes of the slices as $n\rightarrow\infty$?,Divide a ball of volume  into  slices of equal height. What is the product of the volumes of the slices as ?,\frac{e^2}{6}n n n\rightarrow\infty,"Divide a ball of volume $\frac{e^2}{6}n$ into $n$ slices of equal height, as shown below with example $n=8$ . What is the limit of the product of the volumes of the slices as $n\rightarrow\infty$ ? (If the image doesn't load for you, just imagine $n+1$ equally-spaced horizontal planes, and a ball that is tangent to the top and bottom planes. The planes, between the top and bottom planes, are where you cut the ball.) I used volume of revolution, and after simplifying I got: $$\lim_{n\to\infty}\exp{\left(2n-2n\ln{n+\sum_{k=1}^{n}}\ln{\left(k(n+1-k)-\frac{n}{2}-\frac{1}{3}\right)}\right)}$$ Wolfram does not evaluate this limit, but desmos tells me that when $n=10, 100, 1000, 10^6$ , the product is approximately $1.847, 1.977, 1.997, 1.99999513$ , respectively. So apparently the limit converges and equals $2$ , but I do not know how to prove this. (In case you're wondering how I got the number $\frac{e^2}{6}$ : I used trial and error on desmos to hunt for the number that makes the limit converge, assuming such a number exists. I obtained a number like 1.231509. I entered this number into Wolfram and it suggested $\frac{e^2}{6}$ .)","Divide a ball of volume into slices of equal height, as shown below with example . What is the limit of the product of the volumes of the slices as ? (If the image doesn't load for you, just imagine equally-spaced horizontal planes, and a ball that is tangent to the top and bottom planes. The planes, between the top and bottom planes, are where you cut the ball.) I used volume of revolution, and after simplifying I got: Wolfram does not evaluate this limit, but desmos tells me that when , the product is approximately , respectively. So apparently the limit converges and equals , but I do not know how to prove this. (In case you're wondering how I got the number : I used trial and error on desmos to hunt for the number that makes the limit converge, assuming such a number exists. I obtained a number like 1.231509. I entered this number into Wolfram and it suggested .)","\frac{e^2}{6}n n n=8 n\rightarrow\infty n+1 \lim_{n\to\infty}\exp{\left(2n-2n\ln{n+\sum_{k=1}^{n}}\ln{\left(k(n+1-k)-\frac{n}{2}-\frac{1}{3}\right)}\right)} n=10, 100, 1000, 10^6 1.847, 1.977, 1.997, 1.99999513 2 \frac{e^2}{6} \frac{e^2}{6}","['limits', 'volume', 'infinite-product']"
31,Does $ \left\lceil \frac{1}{n}\right\rceil\to 0$ or $1$ as $n\to\infty ?$,Does  or  as, \left\lceil \frac{1}{n}\right\rceil\to 0 1 n\to\infty ?,"I think the limit is $1$, because $\left\lceil \frac{1}{n}\right\rceil=1$ for all $n$, but it seems counter intuitive for some reason. If we crudely ""replace $n$ with $\infty$"" we get $\lceil 0\rceil=0$ (also, if it was $0$ I can't see a way of proving it). Is this due to the discontinuous character of $\lceil x\rceil?$","I think the limit is $1$, because $\left\lceil \frac{1}{n}\right\rceil=1$ for all $n$, but it seems counter intuitive for some reason. If we crudely ""replace $n$ with $\infty$"" we get $\lceil 0\rceil=0$ (also, if it was $0$ I can't see a way of proving it). Is this due to the discontinuous character of $\lceil x\rceil?$",,['limits']
32,Show that $\prod_{k=1}^\infty \frac{2k+1}{2\pi}\sin{\left(\frac{2\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{k+2}\right)}=\frac{\pi}{2}$,Show that,\prod_{k=1}^\infty \frac{2k+1}{2\pi}\sin{\left(\frac{2\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{k+2}\right)}=\frac{\pi}{2},"Show that $$\prod\limits_{k=1}^\infty \frac{2k+1}{2\pi}\sin{\left(\frac{2\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{k+2}\right)}=\frac{\pi}{2}$$ Context: Inspired by this question , I considered the product of the areas of every regular odd-gon inscribed in a circle of area $1$ . This is $\prod\limits_{k=1}^\infty \frac{2k+1}{2\pi} \sin{\frac{2\pi}{2k+1}}=0.18055...$ which I guess does not have a closed form. On a whim, I divided this number by the Kepler-Bouwkamp constant , $\prod\limits_{k=1}^\infty \cos{\frac{\pi}{k+2}}=0.11494...$ , and numerical calculation suggests that the result is $\pi/2$ . My attempt: I used the double angle formula for sine to get $\prod\limits_{k=1}^\infty \frac{2k+1}{\pi}\sin{\left(\frac{\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{2k+2}\right)}$ but this seems just as intractable as the original form of the product.","Show that Context: Inspired by this question , I considered the product of the areas of every regular odd-gon inscribed in a circle of area . This is which I guess does not have a closed form. On a whim, I divided this number by the Kepler-Bouwkamp constant , , and numerical calculation suggests that the result is . My attempt: I used the double angle formula for sine to get but this seems just as intractable as the original form of the product.",\prod\limits_{k=1}^\infty \frac{2k+1}{2\pi}\sin{\left(\frac{2\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{k+2}\right)}=\frac{\pi}{2} 1 \prod\limits_{k=1}^\infty \frac{2k+1}{2\pi} \sin{\frac{2\pi}{2k+1}}=0.18055... \prod\limits_{k=1}^\infty \cos{\frac{\pi}{k+2}}=0.11494... \pi/2 \prod\limits_{k=1}^\infty \frac{2k+1}{\pi}\sin{\left(\frac{\pi}{2k+1}\right)}\sec{\left(\frac{\pi}{2k+2}\right)},"['limits', 'trigonometry', 'circles', 'infinite-product', 'polygons']"
33,On the Diophantine equation $a^2+b^2 = c^2+k$,On the Diophantine equation,a^2+b^2 = c^2+k,"Given the Diophantine equation, $$a^2+b^2=c^2+k$$ where k is a constant integer.  Let $0 < a \le b$, and $\Delta_k(N)$ be the number of primitive solutions with $0 < c < N$ for some bound N . For example, for k = 0 and $N = 10^5 $, there are exactly 15919 primitive Pythagorean triples, hence $\Delta_0(10^5) = 15919$. ""Conjecture : $$\lim_{N\to\infty}\frac{\Delta_k(N)}{N} = \text{constant}$$ where the constant, for negative k, involves $\frac{1}{\sqrt{-k}}$."" In the table below, the first column gives selected k , subsequent columns give (rounded to 5 decimal places) the ratio $\frac{\Delta_k(N)}{N}$ up to $N = 10^7$, its conjectured limit L as $N \to \infty$, and error difference of L and ratio at $N = 10^7$. $$\begin{array}{ccccc} k&10^5&10^6&10^7&\infty&\text{Error diff}\\ 0&0.15919&0.15914&0.15916& \to 0.15915 \;=\; \frac{1}{2\pi}&0.00001\\  -1&0.12517&0.12497&0.12499& \to 0.12500 \;=\; \frac{1}{8}&0.00001\\ -2&0.17697&0.17679&0.17680& \to 0.17677 = \frac{1}{4\sqrt{2}}&0.00003\\ -3&0.28871&0.28868&0.28864& \to 0.28867 = \frac{1}{2\sqrt{3}}&0.00003\\ -5&0.22354&0.22367&0.22357& \to 0.22360 = \frac{1}{2\sqrt{5}}&0.00003\\ -7&0.37772&0.37780&0.37799& \to 0.37796 = \frac{1}{\sqrt{7}}&0.00003\\  \end{array}$$ Lehmer (1900) proved the case k = 0. Question: Can you prove the conjectured limits are valid/invalid? Similar simple limits can be found for other negative k , but not for positive k . Code by Daniel Lichtblau of Wolfram Research to find the number of solutions $\le N$ can be found here , though $N = 10^7$ already takes more than an hour, and beyond that takes MUCH more. Do you know of a faster code?","Given the Diophantine equation, $$a^2+b^2=c^2+k$$ where k is a constant integer.  Let $0 < a \le b$, and $\Delta_k(N)$ be the number of primitive solutions with $0 < c < N$ for some bound N . For example, for k = 0 and $N = 10^5 $, there are exactly 15919 primitive Pythagorean triples, hence $\Delta_0(10^5) = 15919$. ""Conjecture : $$\lim_{N\to\infty}\frac{\Delta_k(N)}{N} = \text{constant}$$ where the constant, for negative k, involves $\frac{1}{\sqrt{-k}}$."" In the table below, the first column gives selected k , subsequent columns give (rounded to 5 decimal places) the ratio $\frac{\Delta_k(N)}{N}$ up to $N = 10^7$, its conjectured limit L as $N \to \infty$, and error difference of L and ratio at $N = 10^7$. $$\begin{array}{ccccc} k&10^5&10^6&10^7&\infty&\text{Error diff}\\ 0&0.15919&0.15914&0.15916& \to 0.15915 \;=\; \frac{1}{2\pi}&0.00001\\  -1&0.12517&0.12497&0.12499& \to 0.12500 \;=\; \frac{1}{8}&0.00001\\ -2&0.17697&0.17679&0.17680& \to 0.17677 = \frac{1}{4\sqrt{2}}&0.00003\\ -3&0.28871&0.28868&0.28864& \to 0.28867 = \frac{1}{2\sqrt{3}}&0.00003\\ -5&0.22354&0.22367&0.22357& \to 0.22360 = \frac{1}{2\sqrt{5}}&0.00003\\ -7&0.37772&0.37780&0.37799& \to 0.37796 = \frac{1}{\sqrt{7}}&0.00003\\  \end{array}$$ Lehmer (1900) proved the case k = 0. Question: Can you prove the conjectured limits are valid/invalid? Similar simple limits can be found for other negative k , but not for positive k . Code by Daniel Lichtblau of Wolfram Research to find the number of solutions $\le N$ can be found here , though $N = 10^7$ already takes more than an hour, and beyond that takes MUCH more. Do you know of a faster code?",,"['number-theory', 'limits', 'diophantine-equations']"
34,Limit of $n^2$ and a recurrence relation with ceiling function,Limit of  and a recurrence relation with ceiling function,n^2,"For all positive integer $n$ we define a finite sequence in the following way: $n_0 = n$ , then $n_1\geq n_0$ and has the property that $n_1$ is a multiple of $n_0-1$ such that the difference $n_1 - n_0$ is minimal among all multiple of $n_0 -1 $ that are bigger than $n_0$ . More generally $n_k \geq n_{k-1}$ and has the property that is a multiple of $n_0-k$ such that the difference $n_k - n_{k-1}$ is minimal among all multiple of $n_0 -k $ that are bigger than $n_{k-1}$ . We stop the procedure when $k=n-1$ so that we define $f$ to be $f(n_0) = n_{n-1} $ . Question : What is the value of $\lim_{n \to \infty} \frac{n^2}{f(n)} $ ? I'm able to prove that $$ \frac{8}{3} \leq \lim_{n \to \infty} \frac{n^2}{f(n)} \leq 4 $$ but I can't do better. Someone has an idea? I think that the limit is $ \pi $ but don't know how to prove that. My idea is to prove that $$ f(n) = \frac{n^2}{\pi} + O(n) $$ the general term for $n_k$ is given by $$ n_k = (n-k) \left \lceil \frac{n_{k-1}}{n-k} \right \rceil $$ So that $$ n_k = (n-k) \left \lceil \frac{n-(k-1)}{n-k} \left \lceil \frac{n-(k-2)}{n-(k-1)}   \left \lceil \ldots \left \lceil \frac{n-1}{n-2} \left \lceil \frac{n}{n-1} \right \rceil \right \rceil \right \rceil \right \rceil \right \rceil $$","For all positive integer we define a finite sequence in the following way: , then and has the property that is a multiple of such that the difference is minimal among all multiple of that are bigger than . More generally and has the property that is a multiple of such that the difference is minimal among all multiple of that are bigger than . We stop the procedure when so that we define to be . Question : What is the value of ? I'm able to prove that but I can't do better. Someone has an idea? I think that the limit is but don't know how to prove that. My idea is to prove that the general term for is given by So that",n n_0 = n n_1\geq n_0 n_1 n_0-1 n_1 - n_0 n_0 -1  n_0 n_k \geq n_{k-1} n_0-k n_k - n_{k-1} n_0 -k  n_{k-1} k=n-1 f f(n_0) = n_{n-1}  \lim_{n \to \infty} \frac{n^2}{f(n)}   \frac{8}{3} \leq \lim_{n \to \infty} \frac{n^2}{f(n)} \leq 4   \pi   f(n) = \frac{n^2}{\pi} + O(n)  n_k  n_k = (n-k) \left \lceil \frac{n_{k-1}}{n-k} \right \rceil   n_k = (n-k) \left \lceil \frac{n-(k-1)}{n-k} \left \lceil \frac{n-(k-2)}{n-(k-1)}   \left \lceil \ldots \left \lceil \frac{n-1}{n-2} \left \lceil \frac{n}{n-1} \right \rceil \right \rceil \right \rceil \right \rceil \right \rceil ,"['limits', 'ceiling-and-floor-functions', 'pi']"
35,Constant sequence?,Constant sequence?,,Consider a sequence with strictly positive terms $(a_n)_{n\geq1}$ with the property: $$\lim_{n\rightarrow \infty} \left(\frac{a_1}{a_2}+\frac{a_2}{a_3} + \cdots + \frac{a_{n-1}}{a_n} + \frac{a_n}{a_1}-n\right)=0$$ Prove that this sequence is constant. It's a problem that I have no idea. Maybe someone has an idea? Thank you!,Consider a sequence with strictly positive terms $(a_n)_{n\geq1}$ with the property: $$\lim_{n\rightarrow \infty} \left(\frac{a_1}{a_2}+\frac{a_2}{a_3} + \cdots + \frac{a_{n-1}}{a_n} + \frac{a_n}{a_1}-n\right)=0$$ Prove that this sequence is constant. It's a problem that I have no idea. Maybe someone has an idea? Thank you!,,['limits']
36,Help with proving a statement based on Riemann sums?,Help with proving a statement based on Riemann sums?,,"Suppose we have the original Riemann sum with no removed partitions, where $f(x)$ is continuous and reimmen integratable on the closed interval $[a,b]$. $$\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)$$ If we remove $s$ partitions for every $d$ partitions in the interval $[a,b]$ and add the remaining partitions as $n\to\infty$ the resulting sum is $$\lim_{n\to\infty}\sum_{i=1}^{{\left(d-s\right)}\lfloor\frac{n}{d}\rfloor+\left(n\text{mod}{d}\right)}f\left(a+\left(\frac{b-a}{n}\right)s(i-g_1)+g_2\right)\left(\frac{b-a}{n}\right)$$ Where $S(i)$ is a piece-wise linear vector that skips $s$ for every $d$ partitions. For example if we skip one partition out of every four partitions ,instead of the vector $i$ whose outputs are ($1$,$2$,$3$,$4$,$5$...), we have $s(1)=1$, $s(2)=3$, $s(3)=4$, $s(4)=5$, $s(5)=7$,$s(6)=8$...). So for in my theorem I'm trying to show that $$\lim_{n\to\infty}\sum_{i=1}^{{\left(d-s\right)}\lfloor\frac{n}{d}\rfloor+\left(n\text{mod}{d}\right)}f\left(a+\left(\frac{b-a}{n}\right)s(i-g_1)+g_2\right)\left(\frac{b-a}{n}\right)=$$ $$\frac{d-s}{d}\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)=\frac{d-s}{d}\int_{a}^{b}f(x)$$ I know as all the partitions of the orginal sum ($\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)$) come closer to being equal, the sum of the fraction of remaining partitions will be the same as that fraction of the orginal reimmen sum. To prove the partitions of original reimmen sum comes closer to being equal I found the following. $$\lim_{n\to\infty}f\left(a+\left(\frac{b-a}{n}\right)\right)\left(\frac{b-a}{n}\right)<\frac{\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)}{n}<\lim_{n\to\infty}f(b)\left(\frac{b-a}{n}\right)$$ And $$\lim_{n\to\infty}f(b)\left(\frac{b-a}{n}\right)-\lim_{n\to\infty}f\left(a+\left(\frac{b-a}{n}\right)\right)\left(\frac{b-a}{n}\right)=0$$ Am I on the right direction with proving this? If not can you give expand on a better way of proving this? EDIT: I did posted my incomplete answer but its cluttered. Is there a simpler (and more rigorous proof) that can be done? SECOND EDIT: The person who answered my question deleted his post for unknown reasons. He has sent no reply as to why he did so. I posted my version of his answer down below my incomplete answer. I am waiting for another answer that expands or gives a better proof. Third edit: I deleted my original proof. Christian Blatters answer remains and there is a new answer from another user but Im not sure if its correct.","Suppose we have the original Riemann sum with no removed partitions, where $f(x)$ is continuous and reimmen integratable on the closed interval $[a,b]$. $$\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)$$ If we remove $s$ partitions for every $d$ partitions in the interval $[a,b]$ and add the remaining partitions as $n\to\infty$ the resulting sum is $$\lim_{n\to\infty}\sum_{i=1}^{{\left(d-s\right)}\lfloor\frac{n}{d}\rfloor+\left(n\text{mod}{d}\right)}f\left(a+\left(\frac{b-a}{n}\right)s(i-g_1)+g_2\right)\left(\frac{b-a}{n}\right)$$ Where $S(i)$ is a piece-wise linear vector that skips $s$ for every $d$ partitions. For example if we skip one partition out of every four partitions ,instead of the vector $i$ whose outputs are ($1$,$2$,$3$,$4$,$5$...), we have $s(1)=1$, $s(2)=3$, $s(3)=4$, $s(4)=5$, $s(5)=7$,$s(6)=8$...). So for in my theorem I'm trying to show that $$\lim_{n\to\infty}\sum_{i=1}^{{\left(d-s\right)}\lfloor\frac{n}{d}\rfloor+\left(n\text{mod}{d}\right)}f\left(a+\left(\frac{b-a}{n}\right)s(i-g_1)+g_2\right)\left(\frac{b-a}{n}\right)=$$ $$\frac{d-s}{d}\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)=\frac{d-s}{d}\int_{a}^{b}f(x)$$ I know as all the partitions of the orginal sum ($\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)$) come closer to being equal, the sum of the fraction of remaining partitions will be the same as that fraction of the orginal reimmen sum. To prove the partitions of original reimmen sum comes closer to being equal I found the following. $$\lim_{n\to\infty}f\left(a+\left(\frac{b-a}{n}\right)\right)\left(\frac{b-a}{n}\right)<\frac{\lim_{n\to\infty}\sum_{i=1}^{n}f\left(a+\left(\frac{b-a}{n}\right)i\right)\left(\frac{b-a}{n}\right)}{n}<\lim_{n\to\infty}f(b)\left(\frac{b-a}{n}\right)$$ And $$\lim_{n\to\infty}f(b)\left(\frac{b-a}{n}\right)-\lim_{n\to\infty}f\left(a+\left(\frac{b-a}{n}\right)\right)\left(\frac{b-a}{n}\right)=0$$ Am I on the right direction with proving this? If not can you give expand on a better way of proving this? EDIT: I did posted my incomplete answer but its cluttered. Is there a simpler (and more rigorous proof) that can be done? SECOND EDIT: The person who answered my question deleted his post for unknown reasons. He has sent no reply as to why he did so. I posted my version of his answer down below my incomplete answer. I am waiting for another answer that expands or gives a better proof. Third edit: I deleted my original proof. Christian Blatters answer remains and there is a new answer from another user but Im not sure if its correct.",,"['limits', 'summation', 'riemann-sum']"
37,Evaluate $\lim\limits_{x\to 0 } \frac{ e^{\frac{\ln(1+ax)}{x}} - e^{\frac{a\ln(1+x)}{x}}}{x}$.,Evaluate .,\lim\limits_{x\to 0 } \frac{ e^{\frac{\ln(1+ax)}{x}} - e^{\frac{a\ln(1+x)}{x}}}{x},"Problem Evaluate $$\lim_{x\to 0 } \frac{ e^{\frac{\ln(1+ax)}{x}} - e^{\frac{a\ln(1+x)}{x}}}{x}.$$ Solution For  convenience, denote $u(x)=\dfrac{\ln(1+ax)}{x}$ and $v(x)=\dfrac{a\ln(1+x)}{x}.$ Notice that $u(x),v(x) \to a$ as $x \to 0.$Hence, \begin{align*} \lim_{x\to 0 } \frac{ e^{\frac{\ln(1+ax)}{x}} - e^{\frac{a\ln(1+x)}{x}}}{x}&=\lim_{x\to 0 } \frac{e^{v(x)}(e^{u(x)-v(x)}-1)}{x}\\&=\lim_{x \to 0}\left(e^{v(x)}\cdot\frac{e^{u(x)-v(x)}-1}{u(x)-v(x)}\cdot \frac{u(x)-v(x)}{x}\right)\\&=\lim_{x \to 0}e^{v(x)}\cdot\lim_{x \to 0}\frac{e^{u(x)-v(x)}-1}{u(x)-v(x)}\cdot \lim_{x \to 0}\frac{u(x)-v(x)}{x}\\&=e^a \cdot 1 \cdot\lim_{x \to 0}\frac{\ln(1+ax)-a\ln(1+x)}{x^2}\\&=e^a \cdot\lim_{x \to 0}\dfrac{ax-\dfrac{1}{2}a^2x^2+\mathcal{O}(x^2)-a\left(x-\dfrac{1}{2}x^2+\mathcal{O}(x^2)\right)}{x^2}\\&=e^a \cdot\lim_{x \to 0}\dfrac{-\dfrac{1}{2}a^2x^2+\dfrac{1}{2}ax^2+\mathcal{O}(x^2)}{x^2}\\&=\frac{1}{2}e^a(a-a^2). \end{align*} Please correct me if I'm wrong! Hope to see other solutions.","Problem Evaluate $$\lim_{x\to 0 } \frac{ e^{\frac{\ln(1+ax)}{x}} - e^{\frac{a\ln(1+x)}{x}}}{x}.$$ Solution For  convenience, denote $u(x)=\dfrac{\ln(1+ax)}{x}$ and $v(x)=\dfrac{a\ln(1+x)}{x}.$ Notice that $u(x),v(x) \to a$ as $x \to 0.$Hence, \begin{align*} \lim_{x\to 0 } \frac{ e^{\frac{\ln(1+ax)}{x}} - e^{\frac{a\ln(1+x)}{x}}}{x}&=\lim_{x\to 0 } \frac{e^{v(x)}(e^{u(x)-v(x)}-1)}{x}\\&=\lim_{x \to 0}\left(e^{v(x)}\cdot\frac{e^{u(x)-v(x)}-1}{u(x)-v(x)}\cdot \frac{u(x)-v(x)}{x}\right)\\&=\lim_{x \to 0}e^{v(x)}\cdot\lim_{x \to 0}\frac{e^{u(x)-v(x)}-1}{u(x)-v(x)}\cdot \lim_{x \to 0}\frac{u(x)-v(x)}{x}\\&=e^a \cdot 1 \cdot\lim_{x \to 0}\frac{\ln(1+ax)-a\ln(1+x)}{x^2}\\&=e^a \cdot\lim_{x \to 0}\dfrac{ax-\dfrac{1}{2}a^2x^2+\mathcal{O}(x^2)-a\left(x-\dfrac{1}{2}x^2+\mathcal{O}(x^2)\right)}{x^2}\\&=e^a \cdot\lim_{x \to 0}\dfrac{-\dfrac{1}{2}a^2x^2+\dfrac{1}{2}ax^2+\mathcal{O}(x^2)}{x^2}\\&=\frac{1}{2}e^a(a-a^2). \end{align*} Please correct me if I'm wrong! Hope to see other solutions.",,"['limits', 'solution-verification']"
38,Question about the derivative definition,Question about the derivative definition,,"The derivative at a point $x$ is defined as: $\lim\limits_{h\to0} \frac{f(x+h) - f(x)}h$ But if $h\to0$, wouldn't that mean: $\frac{f(x+0) - f(x)}0 = \frac0{0}$ which is undefined?","The derivative at a point $x$ is defined as: $\lim\limits_{h\to0} \frac{f(x+h) - f(x)}h$ But if $h\to0$, wouldn't that mean: $\frac{f(x+0) - f(x)}0 = \frac0{0}$ which is undefined?",,"['limits', 'derivatives', 'definition', 'indeterminate-forms']"
39,Limits to infinity of a factorial function: $\lim_{n\to\infty}\frac{n!}{n^{n/2}}$,Limits to infinity of a factorial function:,\lim_{n\to\infty}\frac{n!}{n^{n/2}},How can this limit to infinity be solved? I've tried with d'Alembert but it just keeps coming up with the wrong answer. $$\lim\limits_{n\to\infty}\frac{n!}{n^{n/2}}$$ I might have a problem in simplifying factorial numbers.  Thank you in advance.,How can this limit to infinity be solved? I've tried with d'Alembert but it just keeps coming up with the wrong answer. $$\lim\limits_{n\to\infty}\frac{n!}{n^{n/2}}$$ I might have a problem in simplifying factorial numbers.  Thank you in advance.,,"['limits', 'exponentiation', 'factorial', 'radicals', 'infinity']"
40,Proof that $e=\sum\limits_{k=0}^{+\infty}\frac{1}{k!}$,Proof that,e=\sum\limits_{k=0}^{+\infty}\frac{1}{k!},How can it be proved that the Euler constant equals the limit of the sum of all $\frac{1}{k!}$ when $k$ goes from $0$ to $+\infty$ ?,How can it be proved that the Euler constant equals the limit of the sum of all $\frac{1}{k!}$ when $k$ goes from $0$ to $+\infty$ ?,,"['limits', 'exponential-function']"
41,Wrong intuitive understanding of a limit?,Wrong intuitive understanding of a limit?,,"In my textbook, before introducing the epsilon delta definition, they gave a working definition of what a limit is. The definition sounded something like this ""$\lim \limits_{x \to a}f(x) = L$, if when $x$ gets closer to $a$,  $f(x)$ gets closer to $L$"" But is that always the case with limits? What if $f(x) = 4,$ then we have $\lim \limits_{x \to 2}f(x) = 4$, but it is never true that when x gets closer to 2, f(x) gets closer to 4. Maybe instead we should say: ""$\lim \limits_{x \to a}f(x) = L$, if when $x$ gets closer to $a$, $ f(x)$ gets closer to or equals $L$"". Please correct me if I'm wrong. I'm pretty new to this stuff. Btw, i understand that the epsilon delta definition has the constant function limit case covered, but I'm more interested in the working definition.","In my textbook, before introducing the epsilon delta definition, they gave a working definition of what a limit is. The definition sounded something like this ""$\lim \limits_{x \to a}f(x) = L$, if when $x$ gets closer to $a$,  $f(x)$ gets closer to $L$"" But is that always the case with limits? What if $f(x) = 4,$ then we have $\lim \limits_{x \to 2}f(x) = 4$, but it is never true that when x gets closer to 2, f(x) gets closer to 4. Maybe instead we should say: ""$\lim \limits_{x \to a}f(x) = L$, if when $x$ gets closer to $a$, $ f(x)$ gets closer to or equals $L$"". Please correct me if I'm wrong. I'm pretty new to this stuff. Btw, i understand that the epsilon delta definition has the constant function limit case covered, but I'm more interested in the working definition.",,['limits']
42,prove that $\lim_{x\to\infty} \pi(x)/x=0$,prove that,\lim_{x\to\infty} \pi(x)/x=0,"I think I might have asked this question before, but I can't find it on the site, so I sincerely apologize if I am making a duplicate. But anyway, I have been working on this proof for several weeks and am stumped. If $\pi(x)$ is the number of primes less than or equal to $x$, prove that   $$\lim_{x\to\infty}\frac{\pi(x)}{x} = 0.$$ I have this: So far I know that prime numbers can only be (if greater than $k$ for $p \pmod{k}$: $1 \pmod{2}$. $1,2 \pmod{3} \Rightarrow$ upper bound of $\frac{\pi(x)}{x}$ is $\frac{2}{3}$. $1,3 \pmod {4}$. $1,2,3,4 \pmod{5}$. $1,5 \pmod{6}\Rightarrow$ upper bound of $\pi(x)/x$ is $\frac{1}{3}$. $1,2,3,4,5,6 \pmod{7}$. $1,3,5,7\pmod{8}$. $1,2,4,5,7,8\pmod{9}$. Any number prime $p \pmod{k}$ can only have a remainders that are relatively prime to $k$, as a number would not be prime if it could be expressed as a composite plus a factor of that composite. And I know that these possible remainders demonstrate a fraction of the possible numbers that can be prime, given that in any range of numbers there must be at least one that satisfies each possible remainder $\pmod{k}$. ... But I'm not sure what I can conclude from this. I think that I need to find a way to express a number $N$ with respect to a prime $p$ such that $p \pmod N$ has a constant number of possible values, $K$. Then as $N$ increases, $K/N \to 0$. But otherwise I'm really stumped where to go. I have considered the following: multiplying all prime numbers less than an arbitrary value and modding by that, so there are no relative primes less than a certain value except 1. But the problem with this is once you reach a certain value there can be a multiple of this as $2p_1p_2\cdots p_n$. So I don't think that works. Any help would be much appreciated! Also, this is a first-semester number theory class, so I don't have much math knowledge to work with. I've done calc A,B,C, linear algebra A, and this number theory class.","I think I might have asked this question before, but I can't find it on the site, so I sincerely apologize if I am making a duplicate. But anyway, I have been working on this proof for several weeks and am stumped. If $\pi(x)$ is the number of primes less than or equal to $x$, prove that   $$\lim_{x\to\infty}\frac{\pi(x)}{x} = 0.$$ I have this: So far I know that prime numbers can only be (if greater than $k$ for $p \pmod{k}$: $1 \pmod{2}$. $1,2 \pmod{3} \Rightarrow$ upper bound of $\frac{\pi(x)}{x}$ is $\frac{2}{3}$. $1,3 \pmod {4}$. $1,2,3,4 \pmod{5}$. $1,5 \pmod{6}\Rightarrow$ upper bound of $\pi(x)/x$ is $\frac{1}{3}$. $1,2,3,4,5,6 \pmod{7}$. $1,3,5,7\pmod{8}$. $1,2,4,5,7,8\pmod{9}$. Any number prime $p \pmod{k}$ can only have a remainders that are relatively prime to $k$, as a number would not be prime if it could be expressed as a composite plus a factor of that composite. And I know that these possible remainders demonstrate a fraction of the possible numbers that can be prime, given that in any range of numbers there must be at least one that satisfies each possible remainder $\pmod{k}$. ... But I'm not sure what I can conclude from this. I think that I need to find a way to express a number $N$ with respect to a prime $p$ such that $p \pmod N$ has a constant number of possible values, $K$. Then as $N$ increases, $K/N \to 0$. But otherwise I'm really stumped where to go. I have considered the following: multiplying all prime numbers less than an arbitrary value and modding by that, so there are no relative primes less than a certain value except 1. But the problem with this is once you reach a certain value there can be a multiple of this as $2p_1p_2\cdots p_n$. So I don't think that works. Any help would be much appreciated! Also, this is a first-semester number theory class, so I don't have much math knowledge to work with. I've done calc A,B,C, linear algebra A, and this number theory class.",,['number-theory']
43,Show $\lim\left ( 1+ \frac{1}{n} \right )^n = e$ if $e$ is defined by $\int_1^e \frac{1}{x} dx = 1$,Show  if  is defined by,\lim\left ( 1+ \frac{1}{n} \right )^n = e e \int_1^e \frac{1}{x} dx = 1,"I have managed to construct the following bound for $e$, which is defined as the unique positive number such that $\int_1^e \frac{dx}x = 1$. $$\left ( 1+\frac{1}{n} \right )^n \leq e \leq \left (\frac{n}{n-1} \right )^n$$ From here, there must surely be a way to deduce the well-known equality $$\lim_{n \rightarrow \infty} \left ( 1+ \frac{1}{n} \right )^n = e$$ I have come up with the following, but I am not absolutely certain if this is correct or not. PROPOSED SOLUTION: The lower bound is fine as it is, so we shall leave it alone. Note that $$\begin{align*} \left ( \frac{n}{n-1} \right )^n &= \left ( 1+\frac{1}{n-1} \right )^{n} \\ &=  \left ( 1+\frac{1}{n-1} \right )^{n-1} \left ( 1+\frac{1}{n-1} \right ) \end{align*}$$ So using the fact that the limit distributes over multiplication, we have $$\lim_{n \rightarrow \infty} \left ( \frac{n}{n-1} \right )^n = \lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right )^{n-1} \lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right ) $$ Since $$\lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right ) = 1 $$ and $$\lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right )^{n-1} = \lim_{m \rightarrow \infty} \left ( 1+\frac{1}{m} \right )^m = e $$ We then have the required result $$\lim_{n \rightarrow \infty} \left ( 1+ \frac{1}{n} \right )^n = e$$","I have managed to construct the following bound for $e$, which is defined as the unique positive number such that $\int_1^e \frac{dx}x = 1$. $$\left ( 1+\frac{1}{n} \right )^n \leq e \leq \left (\frac{n}{n-1} \right )^n$$ From here, there must surely be a way to deduce the well-known equality $$\lim_{n \rightarrow \infty} \left ( 1+ \frac{1}{n} \right )^n = e$$ I have come up with the following, but I am not absolutely certain if this is correct or not. PROPOSED SOLUTION: The lower bound is fine as it is, so we shall leave it alone. Note that $$\begin{align*} \left ( \frac{n}{n-1} \right )^n &= \left ( 1+\frac{1}{n-1} \right )^{n} \\ &=  \left ( 1+\frac{1}{n-1} \right )^{n-1} \left ( 1+\frac{1}{n-1} \right ) \end{align*}$$ So using the fact that the limit distributes over multiplication, we have $$\lim_{n \rightarrow \infty} \left ( \frac{n}{n-1} \right )^n = \lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right )^{n-1} \lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right ) $$ Since $$\lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right ) = 1 $$ and $$\lim_{n \rightarrow \infty} \left ( 1+\frac{1}{n-1} \right )^{n-1} = \lim_{m \rightarrow \infty} \left ( 1+\frac{1}{m} \right )^m = e $$ We then have the required result $$\lim_{n \rightarrow \infty} \left ( 1+ \frac{1}{n} \right )^n = e$$",,"['limits', 'proof-verification', 'exponential-function']"
44,The limit of $\ln(1+\ln(2+\ln(3+...+\ln(n)))...)$,The limit of,\ln(1+\ln(2+\ln(3+...+\ln(n)))...),"Does this limit:  $$\lim_{n\to\infty}\ln(1+\ln(2+\ln(3+...+\ln(n)))...)$$ exist ? And if yes, which value does it have ?","Does this limit:  $$\lim_{n\to\infty}\ln(1+\ln(2+\ln(3+...+\ln(n)))...)$$ exist ? And if yes, which value does it have ?",,"['limits', 'logarithms']"
45,Prove that $\lim_{n\rightarrow \infty} \frac{\log_{10}\lfloor\text{Denominator of } H_{10^n}\rfloor+1 }{10^n}=\log_{10} e$,Prove that,\lim_{n\rightarrow \infty} \frac{\log_{10}\lfloor\text{Denominator of } H_{10^n}\rfloor+1 }{10^n}=\log_{10} e,"In short, my question is asking to prove that the $$\lim_{n\to\infty}\frac{\text{number of digits in the denominator of} \sum_{k=1}^{10^n} \frac 1k}{10^n}=\log_{10} e$$ I know that the number of digits in a number is $\lfloor \log_{10} n\rfloor +1$ and that the Harmonic numbers are given by $\gamma+\psi_0 (n+1)$, where $\psi_0 (x)=\frac{\Gamma'(x)}{\Gamma(x)}$ but I don't see how to find the number of digits in the denominator of $\psi_0(n+1)+\gamma$. Context:From Wolfram MathWorld, ""The numbers of digits in the denominator of $H_{10^n}$ for $n=0, 1, \ldots$  is given by $1, 4, 40, 433, 4345, 43450, 434110, 4342302, 43428678, \ldots$ (OEIS A114468). These digits converge to what appears to be the decimal digits of $\log_{10}e=0.43429448\ldots$ (OEIS A002285).""","In short, my question is asking to prove that the $$\lim_{n\to\infty}\frac{\text{number of digits in the denominator of} \sum_{k=1}^{10^n} \frac 1k}{10^n}=\log_{10} e$$ I know that the number of digits in a number is $\lfloor \log_{10} n\rfloor +1$ and that the Harmonic numbers are given by $\gamma+\psi_0 (n+1)$, where $\psi_0 (x)=\frac{\Gamma'(x)}{\Gamma(x)}$ but I don't see how to find the number of digits in the denominator of $\psi_0(n+1)+\gamma$. Context:From Wolfram MathWorld, ""The numbers of digits in the denominator of $H_{10^n}$ for $n=0, 1, \ldots$  is given by $1, 4, 40, 433, 4345, 43450, 434110, 4342302, 43428678, \ldots$ (OEIS A114468). These digits converge to what appears to be the decimal digits of $\log_{10}e=0.43429448\ldots$ (OEIS A002285).""",,"['limits', 'conjectures', 'open-problem', 'harmonic-numbers']"
46,A function takes every function value twice - proof it is not continuous,A function takes every function value twice - proof it is not continuous,,"I want to prove the following nice statement I've found: A function $f: [0,1] \rightarrow \mathbb{R}$ takes every function value twice - proof it is not continuous I've already found an answer to my question but one thing is still not clear to my mind. The answer I found was here on math.StackExchange posted by Arthur : Whole post: ""Assume that you have a function $f$ that does take every real value exactly twice. Let $f(a) = f(b) = 0$ with $a < b$. Let $c \in (a,b)$ be given, and assume without loss of generality $f(c) > 0$. Then for every $0 < \epsilon < f(c)$ there exist $d \in (a,c)$ and $e \in (c,b)$ such that $f(d) = f(e) = \epsilon$. So on the intervals $[0,a)$ and $(b,1]$ our function cannot go any higher than zero. That means that $f$ should take every positive real value on the interval $[a,b]$. However the restriction of $f$ to $[a,b]$ is continuous from a closed interval to $\mathbb{R}$ and thus bounded. Contradiction."" Now everything is clear here except for: ""However the restriction of $f$ to $[a,b]$ is continuous from a closed interval to $\mathbb{R}$ and thus bounded. Contradiction."" Well I think know what Arthur is telling there: Because of the boundary there is always at least one function value that can't be taken twice by the function, e.g. just like the peak of $-x^2$. But that is exactly where I was stuck before I started researching that issue. Is it really possible to just say it the way Arthur did? I thought that I'd have to further somehow prove the statement that boundary leads to that contradiction. I hope that it is clear what I was trying to express here... Anyways as always - thank you for your help! FunkyPeanut","I want to prove the following nice statement I've found: A function $f: [0,1] \rightarrow \mathbb{R}$ takes every function value twice - proof it is not continuous I've already found an answer to my question but one thing is still not clear to my mind. The answer I found was here on math.StackExchange posted by Arthur : Whole post: ""Assume that you have a function $f$ that does take every real value exactly twice. Let $f(a) = f(b) = 0$ with $a < b$. Let $c \in (a,b)$ be given, and assume without loss of generality $f(c) > 0$. Then for every $0 < \epsilon < f(c)$ there exist $d \in (a,c)$ and $e \in (c,b)$ such that $f(d) = f(e) = \epsilon$. So on the intervals $[0,a)$ and $(b,1]$ our function cannot go any higher than zero. That means that $f$ should take every positive real value on the interval $[a,b]$. However the restriction of $f$ to $[a,b]$ is continuous from a closed interval to $\mathbb{R}$ and thus bounded. Contradiction."" Now everything is clear here except for: ""However the restriction of $f$ to $[a,b]$ is continuous from a closed interval to $\mathbb{R}$ and thus bounded. Contradiction."" Well I think know what Arthur is telling there: Because of the boundary there is always at least one function value that can't be taken twice by the function, e.g. just like the peak of $-x^2$. But that is exactly where I was stuck before I started researching that issue. Is it really possible to just say it the way Arthur did? I thought that I'd have to further somehow prove the statement that boundary leads to that contradiction. I hope that it is clear what I was trying to express here... Anyways as always - thank you for your help! FunkyPeanut",,"['limits', 'soft-question', 'continuity']"
47,Binomial limit $\left(\binom{3n}{n}\binom{2n}{n}^{-1}\right)^{1/n}$ as $n\to \infty$,Binomial limit  as,\left(\binom{3n}{n}\binom{2n}{n}^{-1}\right)^{1/n} n\to \infty,"The limit:$$\lim_{n\to \infty}\left(\dfrac{\binom{3n}{n}}{\binom{2n}{n}}\right)^\frac{1}{n}$$ What I did was put limit = $L$. Then, $$\begin{align}\log(L)&={\lim_{n \to \infty}}\dfrac{1}{n}\cdot\sum_{r=0}^{{n-1}} \log\left(\dfrac{3-\frac{r}{n}}{2-\frac{r}{n}}\right)\\ &=\int_0^1 \log\left(\dfrac{3-x}{2-x}\right)dx\\ &=\log\left(\dfrac{27}{16}\right) \end{align}$$ Is this aproach correct? Is there other method. Edit: I have corrected the expression for the limit.","The limit:$$\lim_{n\to \infty}\left(\dfrac{\binom{3n}{n}}{\binom{2n}{n}}\right)^\frac{1}{n}$$ What I did was put limit = $L$. Then, $$\begin{align}\log(L)&={\lim_{n \to \infty}}\dfrac{1}{n}\cdot\sum_{r=0}^{{n-1}} \log\left(\dfrac{3-\frac{r}{n}}{2-\frac{r}{n}}\right)\\ &=\int_0^1 \log\left(\dfrac{3-x}{2-x}\right)dx\\ &=\log\left(\dfrac{27}{16}\right) \end{align}$$ Is this aproach correct? Is there other method. Edit: I have corrected the expression for the limit.",,"['limits', 'binomial-coefficients']"
48,Can we prove $\lim _{x\to0}\frac{\sin x}x=1$ with a functional definition for $\sin(x)$?,Can we prove  with a functional definition for ?,\lim _{x\to0}\frac{\sin x}x=1 \sin(x),"We all know the geometric proofs for $$\lim _{x\to0}\frac{\sin(x)}x=1$$ can we find one based on purely functional definition? For example, let's take Apostol's definition: $\sin (x)$ and $\cos (x)$ are defined for all $x \in \mathbb R$ . $\cos (0) = \sin (\frac{\pi}2) = 1$ and $\cos (\pi) = -1$ . $\cos (y-x) = \cos y \cos x + \sin x \sin y$ for all $(x,y) \in \mathbb R ^2$ . and there is a fourth one used to get the limit: $0 < \cos (x) < \frac {\sin (x)}x <1$ for $0 < x< \frac{\pi}2$ . Do we always need this fourth one? Is that limit really unobtainable without geometry? It seems to me we can't reach all values of sine and cosine with just the three properties, as we can only define $\aleph _0$ values for $\cos (x)$ , but could we change the fourth one for "" $\cos(x)$ is continuous"" or something similar and derive the fundamental limit? Notice that in the use of the Taylor expansion we're already assuming the limit to be one as we are using the derivative of $\sin(x)$ .","We all know the geometric proofs for can we find one based on purely functional definition? For example, let's take Apostol's definition: and are defined for all . and . for all . and there is a fourth one used to get the limit: for . Do we always need this fourth one? Is that limit really unobtainable without geometry? It seems to me we can't reach all values of sine and cosine with just the three properties, as we can only define values for , but could we change the fourth one for "" is continuous"" or something similar and derive the fundamental limit? Notice that in the use of the Taylor expansion we're already assuming the limit to be one as we are using the derivative of .","\lim _{x\to0}\frac{\sin(x)}x=1 \sin (x) \cos (x) x \in \mathbb R \cos (0) = \sin (\frac{\pi}2) = 1 \cos (\pi) = -1 \cos (y-x) = \cos y \cos x + \sin x \sin y (x,y) \in \mathbb R ^2 0 < \cos (x) < \frac {\sin (x)}x <1 0 < x< \frac{\pi}2 \aleph _0 \cos (x) \cos(x) \sin(x)","['limits', 'functions', 'trigonometry', 'continuity']"
49,Divergent function of ratio must be logarithm,Divergent function of ratio must be logarithm,,"Given. Consider two functions $F(t)$ and $r(t,x)$ such that $\lim_{t\to\infty} F(t) = \infty$ and $\lim_{t\to\infty} r(t,x)$ is finite for any $x$.  ($x$ and $t$ are always positive in what follows.)  Suppose they sum to a function only of the ratio $x/t$: \begin{align} F(t) + r(t,x) = f (x/t) \qquad (1) \end{align} Claim. $f(x/t)$ must diverge logarithmically at large $t$.  That is, \begin{align} \lim_{t\to\infty} \frac{f(x/t)}{\ln t} = const\ (\text{independent of } x) \qquad (2) \end{align} Flawed proof. The paper cited below does the following.  Define $r(x)= \lim_{t\to\infty} r(t,x)$.  Then we can write: \begin{align} f(x/t) = F(t) + r(x) + \phi(t,x) \qquad (3) \end{align} where $\phi(t,x) = r(t,x) - r(x)$ goes to zero as $t\to\infty$. Now it seems reasonable ( this is the incorrect step! ) that $F(t)+r(x)$ and $\phi(t,x)$ should separately be functions only of the ratio $x/t$, since they have different behavior at large $t$.  In particular, \begin{align} F(t) + r(x) = W(x/t) \qquad (4) \end{align} It is then straightforward to show that $W$ is a logarithm, which completes the proof.  We differentiate both sides of $(4)$ with $x$: \begin{align} r'(x) = \frac{1}{t}W'(x/t)    \qquad (5) \end{align} Then set $x=1, t=\frac{1}{y}$: \begin{align} r'(1) = y\ W'(y)    \qquad (6) \end{align} so $W(y) = r'(1) \ln y + $ const. Counterexample to (4). Consider $F(t) = \ln t + \frac{1}{t}, r(t,x) = -\ln x - \frac{1}{t}.$  Then $r(x) = -\ln x$ and $F(t) + r(x) = \ln \frac{t}{x} + \frac{1}{t}$, so $(4)$ fails. Can this proof be saved?  If not, how else can it be done? Reference. ""A hint of renormalization"" (American Journal of Physics, hep-th/0212049).  See equation (30) and Appendix C.  My $t$ is his $\Lambda$.","Given. Consider two functions $F(t)$ and $r(t,x)$ such that $\lim_{t\to\infty} F(t) = \infty$ and $\lim_{t\to\infty} r(t,x)$ is finite for any $x$.  ($x$ and $t$ are always positive in what follows.)  Suppose they sum to a function only of the ratio $x/t$: \begin{align} F(t) + r(t,x) = f (x/t) \qquad (1) \end{align} Claim. $f(x/t)$ must diverge logarithmically at large $t$.  That is, \begin{align} \lim_{t\to\infty} \frac{f(x/t)}{\ln t} = const\ (\text{independent of } x) \qquad (2) \end{align} Flawed proof. The paper cited below does the following.  Define $r(x)= \lim_{t\to\infty} r(t,x)$.  Then we can write: \begin{align} f(x/t) = F(t) + r(x) + \phi(t,x) \qquad (3) \end{align} where $\phi(t,x) = r(t,x) - r(x)$ goes to zero as $t\to\infty$. Now it seems reasonable ( this is the incorrect step! ) that $F(t)+r(x)$ and $\phi(t,x)$ should separately be functions only of the ratio $x/t$, since they have different behavior at large $t$.  In particular, \begin{align} F(t) + r(x) = W(x/t) \qquad (4) \end{align} It is then straightforward to show that $W$ is a logarithm, which completes the proof.  We differentiate both sides of $(4)$ with $x$: \begin{align} r'(x) = \frac{1}{t}W'(x/t)    \qquad (5) \end{align} Then set $x=1, t=\frac{1}{y}$: \begin{align} r'(1) = y\ W'(y)    \qquad (6) \end{align} so $W(y) = r'(1) \ln y + $ const. Counterexample to (4). Consider $F(t) = \ln t + \frac{1}{t}, r(t,x) = -\ln x - \frac{1}{t}.$  Then $r(x) = -\ln x$ and $F(t) + r(x) = \ln \frac{t}{x} + \frac{1}{t}$, so $(4)$ fails. Can this proof be saved?  If not, how else can it be done? Reference. ""A hint of renormalization"" (American Journal of Physics, hep-th/0212049).  See equation (30) and Appendix C.  My $t$ is his $\Lambda$.",,"['limits', 'logarithms', 'asymptotics']"
50,"Can this be considered a ""rigorous"" definition of a limit?","Can this be considered a ""rigorous"" definition of a limit?",,"I would like to show my definition of a limit, along with an example: Let $f:D\to\mathbb{R}$, where $D\subset\mathbb{R}$. Let   $a\in\mathbb{R}$. If some $l\in\mathbb{R}$ exists, such that for all    $x,y\in D$, $|a-x|<|a-y|\iff |l-f(x)|<|l-f(y)|$, then $l$ is called   the limit of the function $f$, and subsequently: $$\lim_{x\to  a}f(x)=l$$ Using this to prove a standard limit(example is taken from this page): We wish to prove the following limit: $$\lim_{x\to5}(3x-3)=12$$ According to my definition, here $a=5$, and $l=12$. Now, let $f(x)=3x-3$, and $domain(f)=D$. Let $x,y\in D$ such that the following inequality holds:$$|5-x|<|5-y|$$ Now, following steps are followed: $$ |5-x|<|5-y| $$ $$\implies 3|5-x|<3|5-y| $$ $$\implies |3(5-x)|<|3(5-y)|$$ $$\implies |15-3x|<|15-3y| $$ $$\implies |15-3x|<|15-3y| $$ $$\implies |12-f(x)|<|12-f(y)|$$ and thus, we proved the statement. Is this definition correct?","I would like to show my definition of a limit, along with an example: Let $f:D\to\mathbb{R}$, where $D\subset\mathbb{R}$. Let   $a\in\mathbb{R}$. If some $l\in\mathbb{R}$ exists, such that for all    $x,y\in D$, $|a-x|<|a-y|\iff |l-f(x)|<|l-f(y)|$, then $l$ is called   the limit of the function $f$, and subsequently: $$\lim_{x\to  a}f(x)=l$$ Using this to prove a standard limit(example is taken from this page): We wish to prove the following limit: $$\lim_{x\to5}(3x-3)=12$$ According to my definition, here $a=5$, and $l=12$. Now, let $f(x)=3x-3$, and $domain(f)=D$. Let $x,y\in D$ such that the following inequality holds:$$|5-x|<|5-y|$$ Now, following steps are followed: $$ |5-x|<|5-y| $$ $$\implies 3|5-x|<3|5-y| $$ $$\implies |3(5-x)|<|3(5-y)|$$ $$\implies |15-3x|<|15-3y| $$ $$\implies |15-3x|<|15-3y| $$ $$\implies |12-f(x)|<|12-f(y)|$$ and thus, we proved the statement. Is this definition correct?",,"['limits', 'epsilon-delta']"
51,Negative 1 to the power of Infinity,Negative 1 to the power of Infinity,,Can anyone explain me what the result of $$\lim_{n\rightarrow\infty} (-1)^n$$ is and the reason?,Can anyone explain me what the result of $$\lim_{n\rightarrow\infty} (-1)^n$$ is and the reason?,,"['limits', 'infinity', 'exponentiation']"
52,Is the infinite root of any number equal to $1$?,Is the infinite root of any number equal to ?,1,"I was messing around in IRB and I decided to make a $n^{th}$ root function and noticed that for very large roots of numbers, the answer always converges to $1$. It has been a while since I have done any work with infinite series but could someone explain why this is, or offer a proof. (The most similar thing I could think of is the proof of $.9$ repeating equaling $1$)","I was messing around in IRB and I decided to make a $n^{th}$ root function and noticed that for very large roots of numbers, the answer always converges to $1$. It has been a while since I have done any work with infinite series but could someone explain why this is, or offer a proof. (The most similar thing I could think of is the proof of $.9$ repeating equaling $1$)",,"['limits', 'arithmetic', 'infinity']"
53,Limit Notation: $ \lim_{x \to \infty} f(x) =\infty $ or $ \lim_{x \to \infty} f(x) \rightarrow \infty$?,Limit Notation:  or ?, \lim_{x \to \infty} f(x) =\infty   \lim_{x \to \infty} f(x) \rightarrow \infty,"If $f(x) \to \infty $ as $x \to \infty $, Which of the following statements would be more correct: $I.$ $$ \lim_{x \to \infty} f(x) =\infty $$ Or $II. $ $$ \lim_{x \to \infty} f(x) \rightarrow  \infty $$ I thought $II$ would be more appropriate?","If $f(x) \to \infty $ as $x \to \infty $, Which of the following statements would be more correct: $I.$ $$ \lim_{x \to \infty} f(x) =\infty $$ Or $II. $ $$ \lim_{x \to \infty} f(x) \rightarrow  \infty $$ I thought $II$ would be more appropriate?",,"['limits', 'notation']"
54,Finding the limit of a difficult exponential function,Finding the limit of a difficult exponential function,,"I am stumped on this question, any help or tips would be appreciated! Find the limit, if it exists: $$\lim_{x\to\infty } \left(\sqrt{e^{2x}+e^{x}}-\sqrt{e^{2x}-e^{x}}\right)$$","I am stumped on this question, any help or tips would be appreciated! Find the limit, if it exists: $$\lim_{x\to\infty } \left(\sqrt{e^{2x}+e^{x}}-\sqrt{e^{2x}-e^{x}}\right)$$",,['limits']
55,Computing $\lim\limits_{n\to\infty}(1+1/n^2)^n$,Computing,\lim\limits_{n\to\infty}(1+1/n^2)^n,Why is $\lim\limits_{n\to\infty}(1+\frac{1}{n^2})^n = 1$? Could someone elaborate on this? I know that $\lim\limits_{n\to\infty}(1+\frac{1}{n})^n = e$.,Why is $\lim\limits_{n\to\infty}(1+\frac{1}{n^2})^n = 1$? Could someone elaborate on this? I know that $\lim\limits_{n\to\infty}(1+\frac{1}{n})^n = e$.,,['limits']
56,How to compute $\lim_{n\rightarrow\infty}\frac1n\left\{(2n+1)(2n+2)\cdots(2n+n)\right\}^{1/n}$,How to compute,\lim_{n\rightarrow\infty}\frac1n\left\{(2n+1)(2n+2)\cdots(2n+n)\right\}^{1/n},"If $\displaystyle f(n)=\frac1n\Big\{(2n+1)(2n+2)\cdots(2n+n)\Big\}^{1/n}$, then $\lim\limits_{n\to\infty}f(n)$ equals: $$ \begin{array}{} (\mathrm{A})\ \frac4e\qquad&(\mathrm{B})\ \frac{27}{4e}\qquad&(\mathrm{C})\ \frac{27e}{4}\qquad&(\mathrm{D})\ 4e \end{array} $$ I couldn't get the right way to start off with this problem. But, as the options include the constant $e$ I think I will have to work out with logarithms. So, this is what I did.  $$ \lim_{n \to \infty} f(n)\\ =\mathrm{exp}\left(\ln\left(\lim_{n \to \infty} f(n)\right)\right)\\ =\mathrm{exp}\left(\lim_{n \to \infty}\left[\ln\left(f(b)\right)\right]\right)\\ =\mathrm{exp}\left(\lim_{n \to \infty}\left[\frac{1}{n^2}\left(\ln(2n+1)+\ln(2n+2)+\ldots+\ln(2n+n)\right)\right]\right) $$ I'm not able to proceed further. In case my method is correct please give me hints on proceeding further and in case it is wrong give me the same on another method.","If $\displaystyle f(n)=\frac1n\Big\{(2n+1)(2n+2)\cdots(2n+n)\Big\}^{1/n}$, then $\lim\limits_{n\to\infty}f(n)$ equals: $$ \begin{array}{} (\mathrm{A})\ \frac4e\qquad&(\mathrm{B})\ \frac{27}{4e}\qquad&(\mathrm{C})\ \frac{27e}{4}\qquad&(\mathrm{D})\ 4e \end{array} $$ I couldn't get the right way to start off with this problem. But, as the options include the constant $e$ I think I will have to work out with logarithms. So, this is what I did.  $$ \lim_{n \to \infty} f(n)\\ =\mathrm{exp}\left(\ln\left(\lim_{n \to \infty} f(n)\right)\right)\\ =\mathrm{exp}\left(\lim_{n \to \infty}\left[\ln\left(f(b)\right)\right]\right)\\ =\mathrm{exp}\left(\lim_{n \to \infty}\left[\frac{1}{n^2}\left(\ln(2n+1)+\ln(2n+2)+\ldots+\ln(2n+n)\right)\right]\right) $$ I'm not able to proceed further. In case my method is correct please give me hints on proceeding further and in case it is wrong give me the same on another method.",,"['limits', 'logarithms', 'radicals']"
57,An almost impossible limit [duplicate],An almost impossible limit [duplicate],,"This question already has answers here : How find this limit $\lim\limits_{x\to 0^{+}}\frac{\sin{(\tan{x})}-\tan{(\sin{x})}}{x^7}$ (7 answers) Closed 10 years ago . The following limit appeared in a qualification exam: Find the limit of $$\lim_{x \to 0} \left( \frac{\tan (\sin (x))-\sin (\tan (x))}{x^7} \right).$$ I ended up doing it in Mathematica, is there any other way? Thanks in advance!","This question already has answers here : How find this limit $\lim\limits_{x\to 0^{+}}\frac{\sin{(\tan{x})}-\tan{(\sin{x})}}{x^7}$ (7 answers) Closed 10 years ago . The following limit appeared in a qualification exam: Find the limit of $$\lim_{x \to 0} \left( \frac{\tan (\sin (x))-\sin (\tan (x))}{x^7} \right).$$ I ended up doing it in Mathematica, is there any other way? Thanks in advance!",,"['limits', 'trigonometry']"
58,Does $ \lim_{n \to \infty}\sum_{k = 1}^n \zeta\Big(k - \frac{1}{n}\Big)$ equal the Euler-Mascheroni constant?,Does  equal the Euler-Mascheroni constant?, \lim_{n \to \infty}\sum_{k = 1}^n \zeta\Big(k - \frac{1}{n}\Big),"Let $\zeta(s)$ be the Riemann zeta function and $\gamma$ be the Euler-Mascheroni constant. I observed the following result empirically. Looking for a proof or disproof. $$ \lim_{n \to \infty}\sum_{k = 1}^n \zeta\Big(k - \frac{1}{n}\Big) = \gamma $$ Also, I searched for different summation formulas for the Euler-Mascheroni constant using the Riemann zeta function but could not find it anywhere. Is there any reference to this sum in literature? Update: Applying the method of @Simply Beautiful Art, we can show that $$ \sum_{k = 1}^n \zeta\Big(k + \frac{1}{m}\Big)  = \gamma + n + m + \mathcal O(n^{-1} + m^{-1}) $$","Let be the Riemann zeta function and be the Euler-Mascheroni constant. I observed the following result empirically. Looking for a proof or disproof. Also, I searched for different summation formulas for the Euler-Mascheroni constant using the Riemann zeta function but could not find it anywhere. Is there any reference to this sum in literature? Update: Applying the method of @Simply Beautiful Art, we can show that","\zeta(s) \gamma 
\lim_{n \to \infty}\sum_{k = 1}^n \zeta\Big(k - \frac{1}{n}\Big) = \gamma
 
\sum_{k = 1}^n \zeta\Big(k + \frac{1}{m}\Big) 
= \gamma + n + m + \mathcal O(n^{-1} + m^{-1})
","['number-theory', 'limits', 'summation', 'analytic-number-theory', 'riemann-zeta']"
59,Proving the limit,Proving the limit,,"I have to prove that:$$\lim_\limits{x\to\infty}\bigg(\frac{n}{\frac{1}{x+a_1}+\frac{1}{x+a_2}+\cdots+\frac{1}{x+a_n}}-x\bigg)=\frac{a_1+a_2+\cdots+a_n}{n}$$ The way I started doing this is: $$=\lim_\limits{x\to\infty}\left(\frac{n}{\frac{(x+a_1)(x+a_2)\cdots(x+a_n)\sum_{i=1}^{n}\big(\frac{1}{x+a_i}\big)}{(x+a_1)(x+a_2)\cdots(x+a_n)}}-x\right)$$ Then I combine $x$ with the rest, but that leads me nowhere. Any tips on how to do this? Taylor expansion cannot be used.","I have to prove that:$$\lim_\limits{x\to\infty}\bigg(\frac{n}{\frac{1}{x+a_1}+\frac{1}{x+a_2}+\cdots+\frac{1}{x+a_n}}-x\bigg)=\frac{a_1+a_2+\cdots+a_n}{n}$$ The way I started doing this is: $$=\lim_\limits{x\to\infty}\left(\frac{n}{\frac{(x+a_1)(x+a_2)\cdots(x+a_n)\sum_{i=1}^{n}\big(\frac{1}{x+a_i}\big)}{(x+a_1)(x+a_2)\cdots(x+a_n)}}-x\right)$$ Then I combine $x$ with the rest, but that leads me nowhere. Any tips on how to do this? Taylor expansion cannot be used.",,['limits']
60,Prove that $\lim _{x\to \infty \:}(1+\frac{x^x}{x!})^{\frac{1}{x}} = e$ [closed],Prove that  [closed],\lim _{x\to \infty \:}(1+\frac{x^x}{x!})^{\frac{1}{x}} = e,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Using a graphing calculator, it seems that $\lim _{x\to \infty \:}(1+\frac{x^x}{x!})^{\frac{1}{x}} = e$. How can this be proven?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Using a graphing calculator, it seems that $\lim _{x\to \infty \:}(1+\frac{x^x}{x!})^{\frac{1}{x}} = e$. How can this be proven?",,"['limits', 'factorial', 'infinity']"
61,Finding out the limit $\lim_{a \to \infty} \frac{f(a)\ln a}{a}$,Finding out the limit,\lim_{a \to \infty} \frac{f(a)\ln a}{a},For any real number $a \geq 1$ let $f(a)$ denote the real solution of the equation $x(1+\ln x)=a$ then the    question is to find out $$ \lim_{a \to \infty} \frac{f(a)\ln a}{a}$$. It is clear that if we denote $h(a)$ by $h(a)=a(1+\ln a)$ then $f(a)$ is the inverse function of $h(a)$. Also $f(a)$ is increasing function in its domain. Also the limit persuades using lhospital's but I cannot see how to apply it here. Thanks.,For any real number $a \geq 1$ let $f(a)$ denote the real solution of the equation $x(1+\ln x)=a$ then the    question is to find out $$ \lim_{a \to \infty} \frac{f(a)\ln a}{a}$$. It is clear that if we denote $h(a)$ by $h(a)=a(1+\ln a)$ then $f(a)$ is the inverse function of $h(a)$. Also $f(a)$ is increasing function in its domain. Also the limit persuades using lhospital's but I cannot see how to apply it here. Thanks.,,['limits']
62,Derivative of $x^x$ using first principle,Derivative of  using first principle,x^x,Find $f'(x)$ with $f(x)=x^x$ using first principle. i.e. evaluate the limit $$\lim_{h\to0}\frac{{(x+h)}^{x+h}-x^x}{h}$$ EDIT: $x^x=e^{x\ln x}$ so we need to evaluate $$\lim_{h\to0}\frac{e^{{(x+h)}\ln{(x+h)}}-e^{x\ln x}}{h}$$ I know the answer is $x^x(\ln x+1)$ but how can one prove it using first principle?,Find $f'(x)$ with $f(x)=x^x$ using first principle. i.e. evaluate the limit $$\lim_{h\to0}\frac{{(x+h)}^{x+h}-x^x}{h}$$ EDIT: $x^x=e^{x\ln x}$ so we need to evaluate $$\lim_{h\to0}\frac{e^{{(x+h)}\ln{(x+h)}}-e^{x\ln x}}{h}$$ I know the answer is $x^x(\ln x+1)$ but how can one prove it using first principle?,,"['limits', 'derivatives']"
63,$\lim_{x\to\infty}\frac{1-\cos x}{1-\sin x}$,,\lim_{x\to\infty}\frac{1-\cos x}{1-\sin x},Evaluate $$S=\lim_{x\to\infty}\frac{1-\cos x}{1-\sin x}$$ Its quite simple to conclude that this limit does not exist. Now here comes the interesting part. Consider the limit $$L=\lim_{x\to\infty}\frac{x-\sin x}{x+\cos x}=\lim_{x\to\infty}\frac{1-\frac{\sin x}x}{1+\frac{\cos x}x}=1$$ But this limit is of the form $\frac \infty\infty$ . So from L'Hopital's $$L=\lim_{x\to\infty}\frac{x-\sin x}{x+\cos x}=\lim_{x\to\infty}\frac{(x-\sin x)'}{(x+\cos x)'}=S$$ $$\implies S=1$$ Is this reverse reasoning correct? Is there any faulty assumption that is made here?,Evaluate Its quite simple to conclude that this limit does not exist. Now here comes the interesting part. Consider the limit But this limit is of the form . So from L'Hopital's Is this reverse reasoning correct? Is there any faulty assumption that is made here?,S=\lim_{x\to\infty}\frac{1-\cos x}{1-\sin x} L=\lim_{x\to\infty}\frac{x-\sin x}{x+\cos x}=\lim_{x\to\infty}\frac{1-\frac{\sin x}x}{1+\frac{\cos x}x}=1 \frac \infty\infty L=\lim_{x\to\infty}\frac{x-\sin x}{x+\cos x}=\lim_{x\to\infty}\frac{(x-\sin x)'}{(x+\cos x)'}=S \implies S=1,"['limits', 'trigonometry', 'solution-verification']"
64,$\sin (n^2)$ diverges,diverges,\sin (n^2),"One can prove that $\sin n$ diverges, using the fact that the natural numbers modulo $2\pi$ is dense. However, the case for $\sin (n^2)$ looks much more delicate since this is a subsequence of the former one. I strongly believe that this sequence is divergent, but cannot prove it. In general, can one prove that $\sin (n^a)$ diverges for $a>0$?","One can prove that $\sin n$ diverges, using the fact that the natural numbers modulo $2\pi$ is dense. However, the case for $\sin (n^2)$ looks much more delicate since this is a subsequence of the former one. I strongly believe that this sequence is divergent, but cannot prove it. In general, can one prove that $\sin (n^a)$ diverges for $a>0$?",,"['limits', 'trigonometry', 'convergence-divergence', 'equidistribution']"
65,How find $\lim\limits_{n \to \infty } \sqrt{n} \cdot \sqrt[n]{\left( \lim\limits_{n \to \infty } a_n\right)-a_n}$?,How find ?,\lim\limits_{n \to \infty } \sqrt{n} \cdot \sqrt[n]{\left( \lim\limits_{n \to \infty } a_n\right)-a_n},Let $a_n= \sqrt{1+ \sqrt{2+\cdots+ \sqrt{n} } }$ How find $\lim\limits_{n \to \infty }  \sqrt{n} \cdot \sqrt[n]{\left( \lim\limits_{n \to \infty } a_n\right)-a_n}$ ?,Let $a_n= \sqrt{1+ \sqrt{2+\cdots+ \sqrt{n} } }$ How find $\lim\limits_{n \to \infty }  \sqrt{n} \cdot \sqrt[n]{\left( \lim\limits_{n \to \infty } a_n\right)-a_n}$ ?,,['limits']
66,Prove that Wallis' product and Euler's formula directly imply that $(-1/2)!=\sqrt{\pi}$,Prove that Wallis' product and Euler's formula directly imply that,(-1/2)!=\sqrt{\pi},"(This occured to me recently, and I was pretty sure that it was true, so I was pleased that it really was. This has almost certainly been published many times before, but I didn't see it in either of the Wikipedia articles on the Wallis product and Euler's limit formula for factorial so I thought that I would propose it here.) Euler's formula for general factorial is $$z! =\lim_{n \to \infty} \frac{n!n^z}{\displaystyle\prod_{k=1}^n (z+k)} . $$ Wallis' product is $$\frac{\pi}{2} =\prod_{k=1}^{\infty} \frac{4k^2}{4k^2-1} $$ Prove that Wallis' product and Euler's formula directly imply that $(-1/2)! =\sqrt{\pi} $ . I'll post my answer in a few days if no one else does.","(This occured to me recently, and I was pretty sure that it was true, so I was pleased that it really was. This has almost certainly been published many times before, but I didn't see it in either of the Wikipedia articles on the Wallis product and Euler's limit formula for factorial so I thought that I would propose it here.) Euler's formula for general factorial is Wallis' product is Prove that Wallis' product and Euler's formula directly imply that . I'll post my answer in a few days if no one else does.","z!
=\lim_{n \to \infty} \frac{n!n^z}{\displaystyle\prod_{k=1}^n (z+k)}
.  \frac{\pi}{2}
=\prod_{k=1}^{\infty} \frac{4k^2}{4k^2-1}
 (-1/2)!
=\sqrt{\pi}
","['limits', 'factorial', 'pi', 'infinite-product']"
67,Find $\lim_{n \to \infty} \left[\frac{(n+1)^{n + 1}}{n^n} - \frac{n^{n}}{(n-1)^{n-1}} \right]$ (a question asked at trivia),Find  (a question asked at trivia),\lim_{n \to \infty} \left[\frac{(n+1)^{n + 1}}{n^n} - \frac{n^{n}}{(n-1)^{n-1}} \right],"My friend's trivia league had this math question: $$\lim_{n \to \infty} \left[\frac{(n+1)^{n + 1}}{n^n} - \frac{n^{n}}{(n-1)^{n-1}} \right]$$ After computing a few values, one could guess the answer is $e$ = 2.718...But how can we prove that is the limit? Someone offered up a hand-wavy proof like this: \begin{align} \lim_{n \to \infty} \left[\frac{(n+1)^{n + 1}}{n^n} - \frac{n^{n}}{(n-1)^{n-1}} \right] & = \lim_{n \to \infty} \left[\frac{(n+1)(n+1)^{n}}{n^n} - \frac{n \cdot n^{n-1}}{(n-1)^{n-1}} \right] \\ &= \lim_{n \to \infty} \left[(n+1)\frac{(n+1)^{n}}{n^n} - n\frac{n^{n-1}}{(n-1)^{n-1}} \right] \\ &= \lim_{n \to \infty} \left[(n+1)\left(1 + \frac{1}{n} \right)^n - n\left(\frac{n - 1 + 1}{n-1} \right)^{{n-1}} \right] \\ &= \lim_{n \to \infty} \left[(n+1)\left(1 + \frac{1}{n} \right)^n - n\left(1 + \frac{1}{n-1} \right)^{n-1} \right] \\ &= \lim_{n \to \infty} \left[(n+1)e - n \cdot e \right] \\ &= \lim_{n \to \infty} e \\ &= e \end{align} The part about substituting $e$ is hand-wavy since technically this is an indeterminate form of $\infty - \infty$. And using $e$ as as upper bound did not lead me to an easy proof either. Is there a way to rigorously prove the limit? I tried a few approaches: (a) sandwiching the limit--I could prove $e$ was a lower bound, but I could not find a suitable upper bound converging to $e$, (b) using L'Hopital's rule with no luck, (c) using the mean value theorem--but that also got complicated. So this is a pretty tough problem to ask at trivia! Is there a way to prove this limit formally? Sources Trivia question: http://learnedleague.com/question.php?72&16&4 Thread on trivia: http://learnedleague.com/viewtopic.php?f=10&t=7961&hilit=euler Hand-wavy proof: https://i.sstatic.net/dMZzV.jpg Idea for mean value theorem: http://www.pharout.com/trickylimitproblem.pdf","My friend's trivia league had this math question: $$\lim_{n \to \infty} \left[\frac{(n+1)^{n + 1}}{n^n} - \frac{n^{n}}{(n-1)^{n-1}} \right]$$ After computing a few values, one could guess the answer is $e$ = 2.718...But how can we prove that is the limit? Someone offered up a hand-wavy proof like this: \begin{align} \lim_{n \to \infty} \left[\frac{(n+1)^{n + 1}}{n^n} - \frac{n^{n}}{(n-1)^{n-1}} \right] & = \lim_{n \to \infty} \left[\frac{(n+1)(n+1)^{n}}{n^n} - \frac{n \cdot n^{n-1}}{(n-1)^{n-1}} \right] \\ &= \lim_{n \to \infty} \left[(n+1)\frac{(n+1)^{n}}{n^n} - n\frac{n^{n-1}}{(n-1)^{n-1}} \right] \\ &= \lim_{n \to \infty} \left[(n+1)\left(1 + \frac{1}{n} \right)^n - n\left(\frac{n - 1 + 1}{n-1} \right)^{{n-1}} \right] \\ &= \lim_{n \to \infty} \left[(n+1)\left(1 + \frac{1}{n} \right)^n - n\left(1 + \frac{1}{n-1} \right)^{n-1} \right] \\ &= \lim_{n \to \infty} \left[(n+1)e - n \cdot e \right] \\ &= \lim_{n \to \infty} e \\ &= e \end{align} The part about substituting $e$ is hand-wavy since technically this is an indeterminate form of $\infty - \infty$. And using $e$ as as upper bound did not lead me to an easy proof either. Is there a way to rigorously prove the limit? I tried a few approaches: (a) sandwiching the limit--I could prove $e$ was a lower bound, but I could not find a suitable upper bound converging to $e$, (b) using L'Hopital's rule with no luck, (c) using the mean value theorem--but that also got complicated. So this is a pretty tough problem to ask at trivia! Is there a way to prove this limit formally? Sources Trivia question: http://learnedleague.com/question.php?72&16&4 Thread on trivia: http://learnedleague.com/viewtopic.php?f=10&t=7961&hilit=euler Hand-wavy proof: https://i.sstatic.net/dMZzV.jpg Idea for mean value theorem: http://www.pharout.com/trickylimitproblem.pdf",,"['limits', 'indeterminate-forms']"
68,Limits of integrals,Limits of integrals,,"How would you show that if $f : [0,1] \rightarrow \Bbb R$ is continuous, then $$\lim_{n\rightarrow \infty}\int_0^1\int_0^1 \cdots \int_0^1 f\left( \frac{x_1+x_2+\cdots+x_n}{n} \right)~dx_1~dx_2\cdots dx_n = f\left( \frac{1}{2} \right) $$ and $$\lim_{n\rightarrow \infty}\int_0^1\int_0^1 \cdots \int_0^1 f((x_1 x_2 \cdots x_n)^{\frac{1}{n}})~dx_1~dx_2\cdots dx_n = f\left(\frac{1}{e}\right) $$","How would you show that if $f : [0,1] \rightarrow \Bbb R$ is continuous, then $$\lim_{n\rightarrow \infty}\int_0^1\int_0^1 \cdots \int_0^1 f\left( \frac{x_1+x_2+\cdots+x_n}{n} \right)~dx_1~dx_2\cdots dx_n = f\left( \frac{1}{2} \right) $$ and $$\lim_{n\rightarrow \infty}\int_0^1\int_0^1 \cdots \int_0^1 f((x_1 x_2 \cdots x_n)^{\frac{1}{n}})~dx_1~dx_2\cdots dx_n = f\left(\frac{1}{e}\right) $$",,['limits']
69,Proving limit with $\log(n!)$,Proving limit with,\log(n!),"I am trying to calculate the following limits, but I don't know how: $$\lim_{n\to\infty}\frac{3\cdot \sqrt{n}}{\log(n!)}$$ And the second one is $$\lim_{n\to\infty}\frac{\log(n!)}{\log(n)^{\log(n)}}$$ I don't need to show a formal proof, and any tool can be used. Thanks!","I am trying to calculate the following limits, but I don't know how: $$\lim_{n\to\infty}\frac{3\cdot \sqrt{n}}{\log(n!)}$$ And the second one is $$\lim_{n\to\infty}\frac{\log(n!)}{\log(n)^{\log(n)}}$$ I don't need to show a formal proof, and any tool can be used. Thanks!",,"['limits', 'logarithms']"
70,"$e$ is hidden in Pascal's (binomial) triangle. What is hidden in the trinomial triangle, in the same way?","is hidden in Pascal's (binomial) triangle. What is hidden in the trinomial triangle, in the same way?",e,"In Pascal's triangle, denote $S_n=\prod\limits_{k=0}^n\binom{n}{k}$ . It can be shown that $$\lim_{n\to\infty}\frac{S_{n-1}S_{n+1}}{{S_n}^2}=e$$ What is the analogous result for the trinomial triangle ? That is, denote $T_n=\prod\limits_{k=-n}^n\binom{n}{k}'$ where $\binom{n}{k}'$ are the numbers in the $n$ th row of the trinomial triangle*. $$\lim_{n\to\infty}\frac{T_{n-1}T_{n+1}}{{T_n}^2}=\space ?$$ Numerical investigation using A027907 gives: $\dfrac{T_{0}T_{2}}{{T_{\color{red}{1}}}^2}=12$ $\dfrac{T_{1}T_{3}}{{T_{\color{red}{2}}}^2}=15.75$ $\dfrac{T_{2}T_{4}}{{T_{\color{red}{3}}}^2}=18.1555$ $\dfrac{T_{9}T_{11}}{{T_{\color{red}{10}}}^2}=22.4713$ $\dfrac{T_{49}T_{51}}{{T_{\color{red}{50}}}^2}=24.2722$ $\dfrac{T_{73}T_{75}}{{T_{\color{red}{74}}}^2}=24.4284$ $\dfrac{T_{97}T_{99}}{{T_{\color{red}{98}}}^2}=24.5087$ suggesting a limit of approximately $24.7$ . I found that the trinomial coefficients have a closed form expression involving Gegenbauer polynomials , but I am unable to work out the limit. *The article uses the notation $\binom{n}{k}_2$ for trinomial coefficients, but I thought that may be confusing ( $2$ for trinomial?), so I decided to use $\binom{n}{k}'$ .","In Pascal's triangle, denote . It can be shown that What is the analogous result for the trinomial triangle ? That is, denote where are the numbers in the th row of the trinomial triangle*. Numerical investigation using A027907 gives: suggesting a limit of approximately . I found that the trinomial coefficients have a closed form expression involving Gegenbauer polynomials , but I am unable to work out the limit. *The article uses the notation for trinomial coefficients, but I thought that may be confusing ( for trinomial?), so I decided to use .",S_n=\prod\limits_{k=0}^n\binom{n}{k} \lim_{n\to\infty}\frac{S_{n-1}S_{n+1}}{{S_n}^2}=e T_n=\prod\limits_{k=-n}^n\binom{n}{k}' \binom{n}{k}' n \lim_{n\to\infty}\frac{T_{n-1}T_{n+1}}{{T_n}^2}=\space ? \dfrac{T_{0}T_{2}}{{T_{\color{red}{1}}}^2}=12 \dfrac{T_{1}T_{3}}{{T_{\color{red}{2}}}^2}=15.75 \dfrac{T_{2}T_{4}}{{T_{\color{red}{3}}}^2}=18.1555 \dfrac{T_{9}T_{11}}{{T_{\color{red}{10}}}^2}=22.4713 \dfrac{T_{49}T_{51}}{{T_{\color{red}{50}}}^2}=24.2722 \dfrac{T_{73}T_{75}}{{T_{\color{red}{74}}}^2}=24.4284 \dfrac{T_{97}T_{99}}{{T_{\color{red}{98}}}^2}=24.5087 24.7 \binom{n}{k}_2 2 \binom{n}{k}',"['limits', 'binomial-coefficients', 'closed-form', 'products', 'eulers-number-e']"
71,computing the limit $\lim_{\theta \to \frac{\pi}{2}} (\sec \theta - \tan \theta)$,computing the limit,\lim_{\theta \to \frac{\pi}{2}} (\sec \theta - \tan \theta),I'm trying to compute the following limit and would greatly appreciate your heartening feedback on my solution. The limit: $\lim_{\theta \to \frac{\pi}{2}} (\sec \theta - \tan \theta)$ My steps in deriving the solution: Preliminary identities: $\sec \theta = \frac{1}{\cos \theta}$ $\tan \theta = \frac{\sin \theta}{\cos \theta}$ $\frac{1}{\cos \theta}-\frac{\sin\theta}{\cos\theta} = \frac{1-\sin\theta}{\cos\theta} = \frac{1-\sin^2\theta}{(1+\sin\theta)\cos\theta} = \frac{\cos^2\theta}{\cos\theta}\cdot \frac{1}{1+\sin\theta} = \frac{\cos\theta}{1+\sin\theta} = \frac{0}{1+1}$ When $\theta \to \frac{\pi}{2}$ then $\cos(\frac{\pi}{2}) = 0$ and $\sin(\frac{\pi}{2}) = 1$ The answer being: $\lim_{\theta \to \frac{\pi}{2}} (\sec \theta - \tan \theta) = 0$,I'm trying to compute the following limit and would greatly appreciate your heartening feedback on my solution. The limit: My steps in deriving the solution: Preliminary identities: When then and The answer being:,\lim_{\theta \to \frac{\pi}{2}} (\sec \theta - \tan \theta) \sec \theta = \frac{1}{\cos \theta} \tan \theta = \frac{\sin \theta}{\cos \theta} \frac{1}{\cos \theta}-\frac{\sin\theta}{\cos\theta} = \frac{1-\sin\theta}{\cos\theta} = \frac{1-\sin^2\theta}{(1+\sin\theta)\cos\theta} = \frac{\cos^2\theta}{\cos\theta}\cdot \frac{1}{1+\sin\theta} = \frac{\cos\theta}{1+\sin\theta} = \frac{0}{1+1} \theta \to \frac{\pi}{2} \cos(\frac{\pi}{2}) = 0 \sin(\frac{\pi}{2}) = 1 \lim_{\theta \to \frac{\pi}{2}} (\sec \theta - \tan \theta) = 0,"['limits', 'limits-without-lhopital']"
72,Problem of limit with binomial coefficients,Problem of limit with binomial coefficients,,"I thought that the following would made a nice exercise, but I am not sure how to evaluate its difficulty since I often miss elementary solutions. How about you try answering it? It would be great to have several proofs. Let $m$ be a positive integer and $p \in (0,1)$. Prove that $\displaystyle \lim_{n\to\infty}\underset{m\mid k}{\sum_{0 \leq k \leq n}} \binom{n}{k}p^{k}(1-p)^{n-k} = \frac{1}{m}$. I give two proofs of this limit below, but I am still interested in elementary proofs. Edit 1. As noted by @rlgordonma, the sum is $\lim_{n \rightarrow \infty} \sum_{\ell = 0}^{\lfloor \frac{n}{m} \rfloor} \binom{n}{\ell m} p^{\ell m} (1-p)^{n - \ell m} = \frac{1}{m}$. Maybe it makes things clearer like that. Edit2. Thank you for your participation. I will now give two proofs of this limit (one of them being a reformulation of vonbrand's solution). In fact, I will prove a little more: $$\forall r \in \{0,\dots,m-1\},\qquad\lim_{n\to\infty} \sum_{\substack{0 \leq k \leq n\\ k \equiv r\; [m]}} \binom{n}{k}p^k(1-p)^k = \frac{1}{m}$$ Step 1 (probabilistic interpretation). This step is not necessary, but I think it brings a better insight. Let $X_1,X_2,\dots$ be a sequence of independent Bernoulli random variables with parameter $0 < p <1$, that is $P(X_i=1)=1-P(X_i=0)=p$. Then $S_n = X_1 + \dots+ X_n$ is a Binomial random variable with parameters $(n,p)$, and the sum we are interested in is just: $$\sum_{\substack{0 \leq k \leq n\\ k \equiv r\; [m]}} \binom{n}{k}p^k(1-p)^k = P(S_n \equiv r\;[m]).$$ The sequence $(S_n)$ is a random walk over $\mathbb{Z}$ but since we are only interested in its residue modulo $m$ it will be better to see it as a random walk over $\mathbb{Z}/m\mathbb{Z}$ (with an abuse of notation, we will still write $S_n$ for this residue). Step 2 (limit behaviour of the random walk). Quick version, with Markov Chains The random walk $(S_n)$ is an irreducible (and aperiodic) Markov Chain over the finite state space $\mathbb{Z}/m\mathbb{Z}$ with transition matrix given by $Q(i,i)=1-p$ and $Q(i,i+1)=p$ for $i \in \mathbb{Z}/m\mathbb{Z}$. It admits the uniform distribution as a stationnary distribution, hence it converges to this distribution. QED Step 2 (bis). Without Markov Chains The following result (inversion formula for the discrete Fourier transform) is a generalization of @vonbrand's trick: Note $S = \{0,\dots,m-1\}$ and $\omega=\exp\left(i2\pi/m\right)$. For every function $f \colon S\to \mathbb{C}$ and every $x \in S$,   $$ f(x) = \sum_{k = 0}^{m-1} \widehat{f}(k)\,\omega^{kx},\qquad\text{where}\quad \widehat{f}(k) = \frac{1}{m}\sum_{x=0}^{m-1}f(x)\omega^{-kx}. $$ In particular, we have $$ E(f(S_n)) = \sum_{k=0}^{m-1}\widehat{f}(k)\,E\left[\omega^{k S_n}\right]  $$ The sequence $X_1,X_2,\dots$ being i.i.d., $$ E\left[\omega^{k S_n}\right] = \left(E\left[\omega^{k X_1}\right] \right)^n = ((1-p)+p\omega^k)^n \xrightarrow[n\to\infty]{}\begin{cases}1 & \text{if } k=0\\0 & \text{else}\end{cases} $$ Hence, $\lim_{n\to\infty} E[f(S_n)] = \widehat{f}(0)$. Taking $f(x) = 1_{\{x = r\}}$ yields the result.","I thought that the following would made a nice exercise, but I am not sure how to evaluate its difficulty since I often miss elementary solutions. How about you try answering it? It would be great to have several proofs. Let $m$ be a positive integer and $p \in (0,1)$. Prove that $\displaystyle \lim_{n\to\infty}\underset{m\mid k}{\sum_{0 \leq k \leq n}} \binom{n}{k}p^{k}(1-p)^{n-k} = \frac{1}{m}$. I give two proofs of this limit below, but I am still interested in elementary proofs. Edit 1. As noted by @rlgordonma, the sum is $\lim_{n \rightarrow \infty} \sum_{\ell = 0}^{\lfloor \frac{n}{m} \rfloor} \binom{n}{\ell m} p^{\ell m} (1-p)^{n - \ell m} = \frac{1}{m}$. Maybe it makes things clearer like that. Edit2. Thank you for your participation. I will now give two proofs of this limit (one of them being a reformulation of vonbrand's solution). In fact, I will prove a little more: $$\forall r \in \{0,\dots,m-1\},\qquad\lim_{n\to\infty} \sum_{\substack{0 \leq k \leq n\\ k \equiv r\; [m]}} \binom{n}{k}p^k(1-p)^k = \frac{1}{m}$$ Step 1 (probabilistic interpretation). This step is not necessary, but I think it brings a better insight. Let $X_1,X_2,\dots$ be a sequence of independent Bernoulli random variables with parameter $0 < p <1$, that is $P(X_i=1)=1-P(X_i=0)=p$. Then $S_n = X_1 + \dots+ X_n$ is a Binomial random variable with parameters $(n,p)$, and the sum we are interested in is just: $$\sum_{\substack{0 \leq k \leq n\\ k \equiv r\; [m]}} \binom{n}{k}p^k(1-p)^k = P(S_n \equiv r\;[m]).$$ The sequence $(S_n)$ is a random walk over $\mathbb{Z}$ but since we are only interested in its residue modulo $m$ it will be better to see it as a random walk over $\mathbb{Z}/m\mathbb{Z}$ (with an abuse of notation, we will still write $S_n$ for this residue). Step 2 (limit behaviour of the random walk). Quick version, with Markov Chains The random walk $(S_n)$ is an irreducible (and aperiodic) Markov Chain over the finite state space $\mathbb{Z}/m\mathbb{Z}$ with transition matrix given by $Q(i,i)=1-p$ and $Q(i,i+1)=p$ for $i \in \mathbb{Z}/m\mathbb{Z}$. It admits the uniform distribution as a stationnary distribution, hence it converges to this distribution. QED Step 2 (bis). Without Markov Chains The following result (inversion formula for the discrete Fourier transform) is a generalization of @vonbrand's trick: Note $S = \{0,\dots,m-1\}$ and $\omega=\exp\left(i2\pi/m\right)$. For every function $f \colon S\to \mathbb{C}$ and every $x \in S$,   $$ f(x) = \sum_{k = 0}^{m-1} \widehat{f}(k)\,\omega^{kx},\qquad\text{where}\quad \widehat{f}(k) = \frac{1}{m}\sum_{x=0}^{m-1}f(x)\omega^{-kx}. $$ In particular, we have $$ E(f(S_n)) = \sum_{k=0}^{m-1}\widehat{f}(k)\,E\left[\omega^{k S_n}\right]  $$ The sequence $X_1,X_2,\dots$ being i.i.d., $$ E\left[\omega^{k S_n}\right] = \left(E\left[\omega^{k X_1}\right] \right)^n = ((1-p)+p\omega^k)^n \xrightarrow[n\to\infty]{}\begin{cases}1 & \text{if } k=0\\0 & \text{else}\end{cases} $$ Hence, $\lim_{n\to\infty} E[f(S_n)] = \widehat{f}(0)$. Taking $f(x) = 1_{\{x = r\}}$ yields the result.",,"['limits', 'binomial-coefficients', 'random-walk']"
73,Does the dominant prime factor contribute about $62\%$ of the value of the logarithm of numbers?,Does the dominant prime factor contribute about  of the value of the logarithm of numbers?,62\%,"Let $n = p_1^{a_1}p_2^{a_2}\cdots p_k^{a_k}$ be the prime factorization of $n$ for some primes such that $p_i < p_{i+1}$ . We define the minor prime factor and the dominant prime factor of $n$ as the primes which has the smallest the largest contributions respectively to the value of $n$ . Clearly, the dominant and the minor prime factors of $n$ is not necessarily its largest and smallest prime factors. E.g.  if $n = 2^8 3^6 5^2 7^2$ then the largest and the smallest prime factors of $n$ are $7$ and $2$ respectively where as its dominant and the minor prime factors are $3$ and $5$ respectively since $3^6 = 729$ is greater than $2^8 = 256, 5^2=25$ and $7^2=49$ . Definition 1 : The minor prime factor of $n$ is defined as the prime factor $p_m$ such that $p_m^{a_m} \le p_i^{a_i}$ . Definition 2 : If a number as two or more distinct prime factors, then the dominant prime factor of $n$ is defined as the prime factor $p_d$ such that $p_d^{a_d} \ge p_i^{a_i}$ for any $i \ne d$ . Note : If a number has only one distinct prime factor then its minor and its dominant prime factors are the same i.e. $p_m = p_d$ . Since $\frac{a_1 \ln p_1}{\ln n} + \frac{a_2 \ln p_2}{\ln n} + \cdots + \frac{a_k \ln p_k}{\ln n} = 1$ we can say that each term in this sum is the contribution of the respective prime to the value of $\ln n$ and we can ask on and average what is the contribution of the minor/dominant prime to logarithm of a number. Let $p_d$ be the dominant prime factor of $n$ . Experimental data for all $n \le 4 \times 10^{8}$ and the average over several smaller intervals upto $10^{100}$ ; suggests that such that $$ \lim_{x \to \infty}\frac{1}{x}\sum_{n = 1}^x \frac{a_d\ln p_d}{\ln n} \sim 0.62 $$ Conjecture : The dominant prime factor contributes approximately $62\%$ of the value of the logarithm of natural numbers. Can this be proved or disproved? Update 24 Feb 2023 : Digging into literature, I read about the Golomb-Dickman Constant whose value $0.6243299$ is suspiciously close to observed mean value of the dominant prime factor. This constant is related to the largest prime factor (not the dominant prime factor) through the Dickman Function . SageMath Code: n      = 2 step   = target = 10^6 r_max1 = r_max2 = 0  while True:     x = prime_factors(n)     max1 = 1     max2 = 1          for factor in x:         p = 1         check = 1         while True:             check = check*factor             if n%check == 0:                 p = p + 1             else:                 p = p - 1                 check = check/factor                 if check > max1:                     max1 = check                     max2 = check                 break          if len(x) == 1:         max2 = 1      # print(n,max)     l     = ln(n).n()     lmax1  = ln(max1).n()     lmax2  = ln(max2).n()     r_max1 = r_max1 + lmax1/l     r_max2 = r_max2 + lmax2/l      if n == target:         print(n, 'lower_bound:', r_max2/n, 'upper_bound:', r_max1/n, 'mean:', (r_max2/n + r_max1/n)/2)         target = target + step     n = n + 1","Let be the prime factorization of for some primes such that . We define the minor prime factor and the dominant prime factor of as the primes which has the smallest the largest contributions respectively to the value of . Clearly, the dominant and the minor prime factors of is not necessarily its largest and smallest prime factors. E.g.  if then the largest and the smallest prime factors of are and respectively where as its dominant and the minor prime factors are and respectively since is greater than and . Definition 1 : The minor prime factor of is defined as the prime factor such that . Definition 2 : If a number as two or more distinct prime factors, then the dominant prime factor of is defined as the prime factor such that for any . Note : If a number has only one distinct prime factor then its minor and its dominant prime factors are the same i.e. . Since we can say that each term in this sum is the contribution of the respective prime to the value of and we can ask on and average what is the contribution of the minor/dominant prime to logarithm of a number. Let be the dominant prime factor of . Experimental data for all and the average over several smaller intervals upto ; suggests that such that Conjecture : The dominant prime factor contributes approximately of the value of the logarithm of natural numbers. Can this be proved or disproved? Update 24 Feb 2023 : Digging into literature, I read about the Golomb-Dickman Constant whose value is suspiciously close to observed mean value of the dominant prime factor. This constant is related to the largest prime factor (not the dominant prime factor) through the Dickman Function . SageMath Code: n      = 2 step   = target = 10^6 r_max1 = r_max2 = 0  while True:     x = prime_factors(n)     max1 = 1     max2 = 1          for factor in x:         p = 1         check = 1         while True:             check = check*factor             if n%check == 0:                 p = p + 1             else:                 p = p - 1                 check = check/factor                 if check > max1:                     max1 = check                     max2 = check                 break          if len(x) == 1:         max2 = 1      # print(n,max)     l     = ln(n).n()     lmax1  = ln(max1).n()     lmax2  = ln(max2).n()     r_max1 = r_max1 + lmax1/l     r_max2 = r_max2 + lmax2/l      if n == target:         print(n, 'lower_bound:', r_max2/n, 'upper_bound:', r_max1/n, 'mean:', (r_max2/n + r_max1/n)/2)         target = target + step     n = n + 1","n = p_1^{a_1}p_2^{a_2}\cdots p_k^{a_k} n p_i < p_{i+1} n n n n = 2^8 3^6 5^2 7^2 n 7 2 3 5 3^6 = 729 2^8 = 256, 5^2=25 7^2=49 n p_m p_m^{a_m} \le p_i^{a_i} n p_d p_d^{a_d} \ge p_i^{a_i} i \ne d p_m = p_d \frac{a_1 \ln p_1}{\ln n} + \frac{a_2 \ln p_2}{\ln n} + \cdots + \frac{a_k \ln p_k}{\ln n} = 1 \ln n p_d n n \le 4 \times 10^{8} 10^{100} 
\lim_{x \to \infty}\frac{1}{x}\sum_{n = 1}^x \frac{a_d\ln p_d}{\ln n} \sim 0.62
 62\% 0.6243299","['limits', 'number-theory', 'elementary-number-theory', 'prime-numbers', 'divisibility']"
74,A pair of sequences defined by mutual addition/multiplication,A pair of sequences defined by mutual addition/multiplication,,"Define sequences $\{a_n\},\,\{b_n\}$ by mutual recurrence relations: $$a_0=b_0=1,\quad a_{n+1}=a_n+b_n,\quad b_{n+1}=a_n\cdot b_n.\tag1$$ The sequence $\{a_n\}$ begins: $$1,\,2,\,3,\,5,\,11,\,41,\,371,\,13901,\,5033531,\,69782910161,\,...\tag2$$ It appears in OEIS as $A003686$ under the name ""Number of genealogical $1$ - $2$ rooted trees of height $n$ ."" (note that indexing starts with $1$ rather than $0$ in OEIS). It seems that for $n\to\infty$ , $$\ln\ln a_n=n\cdot\ln\phi+c+o(1),\tag3$$ where $\phi=\frac{1+\sqrt5}2$ is the golden ratio, the last term $o(1)$ exponentially decays in absolute value (and seems to have alternating signs for $n\ge8$ ), and the constant $$c\approx-1.11328370529375397170010672464407271138948509227239...\tag4$$ (see more digits here ) The equivalent asymptotics is given in OEIS without proof. How can we prove $(3)$ ? Is there a closed form or some alternative representation (integral, series, product, continued fraction, etc) for the constant $c$ ? Is it irrational? Is there an efficient way to numerically calculate $c$ to a higher precision?","Define sequences by mutual recurrence relations: The sequence begins: It appears in OEIS as under the name ""Number of genealogical - rooted trees of height ."" (note that indexing starts with rather than in OEIS). It seems that for , where is the golden ratio, the last term exponentially decays in absolute value (and seems to have alternating signs for ), and the constant (see more digits here ) The equivalent asymptotics is given in OEIS without proof. How can we prove ? Is there a closed form or some alternative representation (integral, series, product, continued fraction, etc) for the constant ? Is it irrational? Is there an efficient way to numerically calculate to a higher precision?","\{a_n\},\,\{b_n\} a_0=b_0=1,\quad a_{n+1}=a_n+b_n,\quad b_{n+1}=a_n\cdot b_n.\tag1 \{a_n\} 1,\,2,\,3,\,5,\,11,\,41,\,371,\,13901,\,5033531,\,69782910161,\,...\tag2 A003686 1 2 n 1 0 n\to\infty \ln\ln a_n=n\cdot\ln\phi+c+o(1),\tag3 \phi=\frac{1+\sqrt5}2 o(1) n\ge8 c\approx-1.11328370529375397170010672464407271138948509227239...\tag4 (3) c c","['number-theory', 'limits', 'recurrence-relations', 'asymptotics', 'golden-ratio']"
75,How many methods to this limit $\lim_{n\to\infty}\left(\frac{1}{2}+\frac{3}{2^2}+\cdots+\frac{2n-1}{2^n}\right)$,How many methods to this limit,\lim_{n\to\infty}\left(\frac{1}{2}+\frac{3}{2^2}+\cdots+\frac{2n-1}{2^n}\right),"find the limit $$\lim_{n\to\infty}\left(\dfrac{1}{2}+\dfrac{3}{2^2}+\cdots+\dfrac{2n-1}{2^n}\right)$$ My try: let $$S=\dfrac{1}{2}+\dfrac{3}{2^2}+\cdots+\dfrac{2n-1}{2^n}$$   $$\dfrac{1}{2}S=0+\dfrac{1}{2^2}+\cdots+\dfrac{2n-3}{2^n}+\dfrac{2n-1}{2^{n+1}}$$   so   $$\dfrac{1}{2}S=\dfrac{1}{2}+\dfrac{2}{2^2}+\dfrac{2}{2^3}+\cdots+\dfrac{2}{2^n}-\dfrac{2n-1}{2^{n+1}}$$   so   $$S=3-\dfrac{4}{2^n}-2\cdot\dfrac{n}{2^n}+\dfrac{1}{2^n}\longrightarrow 3,n\longrightarrow 3$$ solution 2: let $$S=\sum_{n=1}^{\infty}nx^n,|x|<1|$$   $$S=x\sum_{n=1}^{\infty}nx^{n-1}=x\left(\sum_{n=1}^{\infty}x^n\right)'=x\left(\dfrac{x}{1-x}\right)'$$ Have other nice methods?Thank you","find the limit $$\lim_{n\to\infty}\left(\dfrac{1}{2}+\dfrac{3}{2^2}+\cdots+\dfrac{2n-1}{2^n}\right)$$ My try: let $$S=\dfrac{1}{2}+\dfrac{3}{2^2}+\cdots+\dfrac{2n-1}{2^n}$$   $$\dfrac{1}{2}S=0+\dfrac{1}{2^2}+\cdots+\dfrac{2n-3}{2^n}+\dfrac{2n-1}{2^{n+1}}$$   so   $$\dfrac{1}{2}S=\dfrac{1}{2}+\dfrac{2}{2^2}+\dfrac{2}{2^3}+\cdots+\dfrac{2}{2^n}-\dfrac{2n-1}{2^{n+1}}$$   so   $$S=3-\dfrac{4}{2^n}-2\cdot\dfrac{n}{2^n}+\dfrac{1}{2^n}\longrightarrow 3,n\longrightarrow 3$$ solution 2: let $$S=\sum_{n=1}^{\infty}nx^n,|x|<1|$$   $$S=x\sum_{n=1}^{\infty}nx^{n-1}=x\left(\sum_{n=1}^{\infty}x^n\right)'=x\left(\dfrac{x}{1-x}\right)'$$ Have other nice methods?Thank you",,"['limits', 'summation']"
76,Closed form for $\sqrt{1+\sqrt[2!]{2^2+\sqrt[3!]{3^3+...}}}$,Closed form for,\sqrt{1+\sqrt[2!]{2^2+\sqrt[3!]{3^3+...}}},Is there a close form for the following nested radical? $$\sqrt{1+\sqrt[2!]{2^2+\sqrt[3!]{3^3+...}}}$$ It converges and $$\quad \quad \lim_{n \to\infty}  \sqrt{1+\sqrt[2!]{2^2+\sqrt[3!]{3^3+...+\sqrt[n!]{n^n}}}}=1.8430759846682...$$ Is this number algebraic or transcendental?,Is there a close form for the following nested radical? $$\sqrt{1+\sqrt[2!]{2^2+\sqrt[3!]{3^3+...}}}$$ It converges and $$\quad \quad \lim_{n \to\infty}  \sqrt{1+\sqrt[2!]{2^2+\sqrt[3!]{3^3+...+\sqrt[n!]{n^n}}}}=1.8430759846682...$$ Is this number algebraic or transcendental?,,"['limits', 'nested-radicals']"
77,$\lim_{n\to\infty}|\sin n|^{\frac{1}{n}}$,,\lim_{n\to\infty}|\sin n|^{\frac{1}{n}},"One of my teachers have given a limit to compute: $$\displaystyle\lim_{n\to\infty}|\sin n|^{\frac{1}{n}}$$ I have proved that if the limit exits it has to be $1$. (By using the fact that $\{n\pi\}$ is dense in $[0,1]$) But I seem to have no idea how to approach the problem from here. Any help would be appreciated.","One of my teachers have given a limit to compute: $$\displaystyle\lim_{n\to\infty}|\sin n|^{\frac{1}{n}}$$ I have proved that if the limit exits it has to be $1$. (By using the fact that $\{n\pi\}$ is dense in $[0,1]$) But I seem to have no idea how to approach the problem from here. Any help would be appreciated.",,['limits']
78,Factorial canceling on expansion of binomial coefficients on Concrete Mathematics,Factorial canceling on expansion of binomial coefficients on Concrete Mathematics,,"On Concrete Mathematics section 5.5, which is teaching the hypergeometric functions, generalized factorials is defined as: \[ \frac 1 {z!} = \lim_{n \to \infty} \binom{n+z}{n}n^{-z} \] where \[ \binom r k = \left\{ \begin{array}{ll} r^{\underline k} / k! = r(r-1) \cdots (r-k+1) / k! & k > 0 \\ 1 & k = 0 \\ 0 & k < 0 \end{array} \right. \] follows the ordinary definition. So $z! = z(z-1)!$ for all complex $z$ (except negative integers), then we can check $0! = 1$ and $n! = n(n-1) \cdots 1$ for $n > 0$. Then, a binomial coefficient can be written \[ \binom z w = \lim_{\zeta \to z} \lim_{\omega \to w} \frac{\zeta!}{\omega!(\zeta-\omega)!} \] Let $t_k = \dbinom r k \dbinom s {n-k}$. However, the succeeding paragraph says that \[ t_k = \frac{r!}{(r-k)!k!} \frac{s!}{(s-n+k)!(n-k)!}\]   and we are no longer too shy to use generalized factorials in these expressions. without limits (it is said that we must use appropriate limiting values when these formulas give $\infty / \infty$) and considers the ratio $t_{k+1} / t_k$ for all $t_k \neq 0$ and cancels some factorials using the property $z! = z(z-1)!$ I'm ""too shy"" and my question remains: why can we do such canceling? To observe closely, we take a variety of an example from section 5.7: Considering indefinite summation  \[  \sum \binom n {-k} \delta k, \qquad n < 0  \] Let $t(k) = \dbinom n {-k} = \dfrac{n!}{(-k)!(n+k)!}$, we have  \[  \frac{t(k+1)}{t(k)} = \frac{n!}{(-k-1)!(n+k+1)!} \frac{(-k)!(n+k)!}{n!} = -\frac{k}{n+k+1}  \] Let $n = -1$, we have $t(k+1) / t(k) = -1$ for $t(k) \neq 0$. But it's wrong for $k = 0$, where $t(1) = 0$ and $t(0) = 1$. To see how the error happens, we resume the $\lim$ notation: \begin{align*}  t(k+1)   &= \binom n {-k-1} \\   &= \lim_{z_2 \to 0} \lim_{z_1 \to 0} \frac{(n+z_2)!}{(-k-1+z_1)!(n+k+1-z_1+z_2)!} \\   &= \lim_{z_2 \to 0} \lim_{z_1 \to 0} \frac{-k+z_1}{n+k+1-z_1+z_2} \frac{(n+z_2)!}{(-k+z_1)!(n+k-z_1+z_2)!} \\   &= \binom n {-k} \lim_{z_2 \to 0} \lim_{z_1 \to 0} \frac{-k+z_1}{n+k+1-z_1+z_2}   \end{align*}  So when $n = -1$ and $k = 0$, we have $\lim_{z_2 \to 0} \lim_{z_1 \to 0} (-k+z_1)/(n+k+1-z_1+z_2) = 0$ not $-k/(n+k+1)$. Another example (also from section 5.5) is: \[   \lim_{x \to -1} \frac{x!}{(x-1)!} = \lim_{x \to -1} x = -1  \]  but  \[   \lim_{x \to -1} \frac{x!}{(2x)!} = -2  \]  because of $(-z)! \Gamma(z) = \pi / \sin(z\pi)$, so expression $(-2)! / (-1)!$ is illegal. My question is: in the frame of Concrete Mathematics , how to prevent such errors? Thanks a lot.","On Concrete Mathematics section 5.5, which is teaching the hypergeometric functions, generalized factorials is defined as: \[ \frac 1 {z!} = \lim_{n \to \infty} \binom{n+z}{n}n^{-z} \] where \[ \binom r k = \left\{ \begin{array}{ll} r^{\underline k} / k! = r(r-1) \cdots (r-k+1) / k! & k > 0 \\ 1 & k = 0 \\ 0 & k < 0 \end{array} \right. \] follows the ordinary definition. So $z! = z(z-1)!$ for all complex $z$ (except negative integers), then we can check $0! = 1$ and $n! = n(n-1) \cdots 1$ for $n > 0$. Then, a binomial coefficient can be written \[ \binom z w = \lim_{\zeta \to z} \lim_{\omega \to w} \frac{\zeta!}{\omega!(\zeta-\omega)!} \] Let $t_k = \dbinom r k \dbinom s {n-k}$. However, the succeeding paragraph says that \[ t_k = \frac{r!}{(r-k)!k!} \frac{s!}{(s-n+k)!(n-k)!}\]   and we are no longer too shy to use generalized factorials in these expressions. without limits (it is said that we must use appropriate limiting values when these formulas give $\infty / \infty$) and considers the ratio $t_{k+1} / t_k$ for all $t_k \neq 0$ and cancels some factorials using the property $z! = z(z-1)!$ I'm ""too shy"" and my question remains: why can we do such canceling? To observe closely, we take a variety of an example from section 5.7: Considering indefinite summation  \[  \sum \binom n {-k} \delta k, \qquad n < 0  \] Let $t(k) = \dbinom n {-k} = \dfrac{n!}{(-k)!(n+k)!}$, we have  \[  \frac{t(k+1)}{t(k)} = \frac{n!}{(-k-1)!(n+k+1)!} \frac{(-k)!(n+k)!}{n!} = -\frac{k}{n+k+1}  \] Let $n = -1$, we have $t(k+1) / t(k) = -1$ for $t(k) \neq 0$. But it's wrong for $k = 0$, where $t(1) = 0$ and $t(0) = 1$. To see how the error happens, we resume the $\lim$ notation: \begin{align*}  t(k+1)   &= \binom n {-k-1} \\   &= \lim_{z_2 \to 0} \lim_{z_1 \to 0} \frac{(n+z_2)!}{(-k-1+z_1)!(n+k+1-z_1+z_2)!} \\   &= \lim_{z_2 \to 0} \lim_{z_1 \to 0} \frac{-k+z_1}{n+k+1-z_1+z_2} \frac{(n+z_2)!}{(-k+z_1)!(n+k-z_1+z_2)!} \\   &= \binom n {-k} \lim_{z_2 \to 0} \lim_{z_1 \to 0} \frac{-k+z_1}{n+k+1-z_1+z_2}   \end{align*}  So when $n = -1$ and $k = 0$, we have $\lim_{z_2 \to 0} \lim_{z_1 \to 0} (-k+z_1)/(n+k+1-z_1+z_2) = 0$ not $-k/(n+k+1)$. Another example (also from section 5.5) is: \[   \lim_{x \to -1} \frac{x!}{(x-1)!} = \lim_{x \to -1} x = -1  \]  but  \[   \lim_{x \to -1} \frac{x!}{(2x)!} = -2  \]  because of $(-z)! \Gamma(z) = \pi / \sin(z\pi)$, so expression $(-2)! / (-1)!$ is illegal. My question is: in the frame of Concrete Mathematics , how to prevent such errors? Thanks a lot.",,"['limits', 'discrete-mathematics', 'binomial-coefficients', 'factorial']"
79,Why is it legit to evaluate $\lim_{x\rightarrow 1} \frac{(x-1)(x+1)}{x-1}$ by cancelling common factors?,Why is it legit to evaluate  by cancelling common factors?,\lim_{x\rightarrow 1} \frac{(x-1)(x+1)}{x-1},"I haven't ever taken an analysis course, so maybe that's where I would really learn this, but I've always wondered why it's okay to do this when evaluating a limit. I guess it's the case that there is a theorem which says that the limit of a rational function as $x\rightarrow a$ is equal to the limit of that function in lowest terms as $x\rightarrow a$, regardless of division by zero. Is there a name for that theorem? Or does it follow from some other property of limits or rational functions?","I haven't ever taken an analysis course, so maybe that's where I would really learn this, but I've always wondered why it's okay to do this when evaluating a limit. I guess it's the case that there is a theorem which says that the limit of a rational function as $x\rightarrow a$ is equal to the limit of that function in lowest terms as $x\rightarrow a$, regardless of division by zero. Is there a name for that theorem? Or does it follow from some other property of limits or rational functions?",,['limits']
80,Find the limit of the following expression:,Find the limit of the following expression:,,"$$\lim_{n\to\infty}\frac {1-\frac {1}{2} + \frac {1}{3} -\frac {1}{4}+ ... + \frac {1}{2n-1}-\frac{1}{2n}}{\frac {1}{n+1} + \frac {1}{n+2} + \frac {1}{n+3} + ... + \frac {1}{2n}}$$ I can express the value of the geometric sum of ${\frac {1}{2} + \frac {1}{4}+...+\frac {1}{2n}}$ but the others are ahead of me. Putting both fraction parts under a common denominator makes that part tidy but the numerator seems to get way too complicated, which makes me think there is some simple way to do this.","$$\lim_{n\to\infty}\frac {1-\frac {1}{2} + \frac {1}{3} -\frac {1}{4}+ ... + \frac {1}{2n-1}-\frac{1}{2n}}{\frac {1}{n+1} + \frac {1}{n+2} + \frac {1}{n+3} + ... + \frac {1}{2n}}$$ I can express the value of the geometric sum of ${\frac {1}{2} + \frac {1}{4}+...+\frac {1}{2n}}$ but the others are ahead of me. Putting both fraction parts under a common denominator makes that part tidy but the numerator seems to get way too complicated, which makes me think there is some simple way to do this.",,"['limits', 'harmonic-numbers']"
81,Showing that $\displaystyle\lim_{s \to{1+}}{(s-1)\zeta(s)}=1$,Showing that,\displaystyle\lim_{s \to{1+}}{(s-1)\zeta(s)}=1,"I need prove the following: ($\zeta(s)$ is the Riemann zeta function) $\displaystyle\lim_{s \to{1+}}{(s-1)\zeta(s)}=1$ I really don't know, I have tried hard, but have obtained nothing for now.","I need prove the following: ($\zeta(s)$ is the Riemann zeta function) $\displaystyle\lim_{s \to{1+}}{(s-1)\zeta(s)}=1$ I really don't know, I have tried hard, but have obtained nothing for now.",,"['number-theory', 'limits', 'riemann-zeta']"
82,Why can we resolve indeterminate forms?,Why can we resolve indeterminate forms?,,"I'm  questioning myselfas to why indeterminate forms arise, and why limits that apparently give us indeterminate forms can be resolved with some arithmetic tricks. Why $$\begin{equation*} \lim_{x \rightarrow +\infty} \frac{x+1}{x-1}=\frac{+\infty}{+\infty} \end{equation*} $$ and if I do a simple operation, $$\begin{equation*} \lim_{x \rightarrow +\infty} \frac{x(1+\frac{1}{x})}{x(1-\frac{1}{x})}=\lim_{x \rightarrow +\infty}\frac{(1+\frac{1}{x})}{(1-\frac{1}{x})}=1 \end{equation*} $$ I understand the logic of the process, but I can't understand why we get different results by ""not"" changing anything.","I'm  questioning myselfas to why indeterminate forms arise, and why limits that apparently give us indeterminate forms can be resolved with some arithmetic tricks. Why $$\begin{equation*} \lim_{x \rightarrow +\infty} \frac{x+1}{x-1}=\frac{+\infty}{+\infty} \end{equation*} $$ and if I do a simple operation, $$\begin{equation*} \lim_{x \rightarrow +\infty} \frac{x(1+\frac{1}{x})}{x(1-\frac{1}{x})}=\lim_{x \rightarrow +\infty}\frac{(1+\frac{1}{x})}{(1-\frac{1}{x})}=1 \end{equation*} $$ I understand the logic of the process, but I can't understand why we get different results by ""not"" changing anything.",,"['limits', 'indeterminate-forms']"
83,Reversing the digits of an infinite decimal,Reversing the digits of an infinite decimal,,"Let $x$ be a real number in $[0,1)$, with decimal expansion $$ x = 0.d_1 d_2 d_3 \cdots d_i \cdots \;. $$ If the decimal expansion is finite, ending at $d_i$, then extend with zeros: $d_k = 0$ for all $k > i$. Define a sequence $x_k^R$ by digit reversals, as follows: \begin{eqnarray} x_1^R & = & 0.d_1 \\ x_2^R & = & 0.d_2 d_1 \\ x_3^R & = & 0.d_3 d_2 d_1 \\ x_4^R & = & 0.d_4 d_3 d_2 d_1 \\ & \cdots &\\ x_k^R & = & 0.d_k d_{k-1} \cdots d_3 d_2 d_1\\ & \cdots & \end{eqnarray} Finally, define $x^R = \lim_{k\to\infty} x_k^R$, when that limit exists. Q. For which $x$ does the limit exist?   In particular, must $x$ be rational for the limit $x^R$ to exist?   If not, what are some irrationals with limits? If the decimal expansion of $x$ is finite, then the extension by zeros leads to $\lim_{k\to\infty} x_k^R = 0$.","Let $x$ be a real number in $[0,1)$, with decimal expansion $$ x = 0.d_1 d_2 d_3 \cdots d_i \cdots \;. $$ If the decimal expansion is finite, ending at $d_i$, then extend with zeros: $d_k = 0$ for all $k > i$. Define a sequence $x_k^R$ by digit reversals, as follows: \begin{eqnarray} x_1^R & = & 0.d_1 \\ x_2^R & = & 0.d_2 d_1 \\ x_3^R & = & 0.d_3 d_2 d_1 \\ x_4^R & = & 0.d_4 d_3 d_2 d_1 \\ & \cdots &\\ x_k^R & = & 0.d_k d_{k-1} \cdots d_3 d_2 d_1\\ & \cdots & \end{eqnarray} Finally, define $x^R = \lim_{k\to\infty} x_k^R$, when that limit exists. Q. For which $x$ does the limit exist?   In particular, must $x$ be rational for the limit $x^R$ to exist?   If not, what are some irrationals with limits? If the decimal expansion of $x$ is finite, then the extension by zeros leads to $\lim_{k\to\infty} x_k^R = 0$.",,"['limits', 'real-numbers', 'irrational-numbers', 'rational-numbers']"
84,Why do reciprocal functions work for undefined values?,Why do reciprocal functions work for undefined values?,,"I have a certain problem: When you have a function $f(x)$ and it can have some undefined values, the reciprocal function does some things I don't understand. For example: i) $$f(x)=\tan(x) $$ or another one: ii) $$ f(x)=\frac {5}{x-3}  $$ Clearly you can see that for ii) $x=3$ will be undefined and for i) $\pi/2, 3\pi/2,\ldots$ When I now work with the reciprocal functions my problem appears : i) $$(f(x))^{-1}=\frac{1}{\tan x}=\cot x$$ ii) $$(f(x))^{-1}=\frac{1}{\frac {5}{x-3}}$$ Coming up to my questions: i) The function of $\cot x$ is $0$ at those $x$-values where $\tan x$ is undefined. Why is $1/\text{undefined}=0$ and not undefined too? ii) The same here; $f(x)$ is undefined at $x=3$, but depending on which graphic calculator you use it defines  $$(f(3))^{-1}=0 $$ or as a discontinuity. *What happens at those ""magical"" points when $f(x)=\text{undefined}$ and $(f(x))^{-1}= 1/\text{undefined?}$ What I got so far: ""If y = f (x) = 0 for some value of x, then 1/f (x) is undefined.   There is a jump or discontinuity in its graph for this value of x.   This means that, as f (x) gets close to 0, 1/f (x) will become very   large in value. Equally, if there is a jump or discontinuity in the   graph of y = f (x) for some value of x, then y = 1/f (x) = 0 for that   value of x.""    That's a definition I have from ""Jenny Olive: Math a    student's survival guide"" and of course : Why is cot(x)=0 instead of undefined and my own thesis: Since you can convert $$ y= \frac {1}{\frac {5}{x-3}}=1\cdot\frac{x-3}{5}$$ the value for x=3 is defined Same for $\cot(x) = \cos(x) / \sin(x)$ , where $x= \pi/2$ is defined. Is my thesis working or am I hurting mathematics at this point?","I have a certain problem: When you have a function $f(x)$ and it can have some undefined values, the reciprocal function does some things I don't understand. For example: i) $$f(x)=\tan(x) $$ or another one: ii) $$ f(x)=\frac {5}{x-3}  $$ Clearly you can see that for ii) $x=3$ will be undefined and for i) $\pi/2, 3\pi/2,\ldots$ When I now work with the reciprocal functions my problem appears : i) $$(f(x))^{-1}=\frac{1}{\tan x}=\cot x$$ ii) $$(f(x))^{-1}=\frac{1}{\frac {5}{x-3}}$$ Coming up to my questions: i) The function of $\cot x$ is $0$ at those $x$-values where $\tan x$ is undefined. Why is $1/\text{undefined}=0$ and not undefined too? ii) The same here; $f(x)$ is undefined at $x=3$, but depending on which graphic calculator you use it defines  $$(f(3))^{-1}=0 $$ or as a discontinuity. *What happens at those ""magical"" points when $f(x)=\text{undefined}$ and $(f(x))^{-1}= 1/\text{undefined?}$ What I got so far: ""If y = f (x) = 0 for some value of x, then 1/f (x) is undefined.   There is a jump or discontinuity in its graph for this value of x.   This means that, as f (x) gets close to 0, 1/f (x) will become very   large in value. Equally, if there is a jump or discontinuity in the   graph of y = f (x) for some value of x, then y = 1/f (x) = 0 for that   value of x.""    That's a definition I have from ""Jenny Olive: Math a    student's survival guide"" and of course : Why is cot(x)=0 instead of undefined and my own thesis: Since you can convert $$ y= \frac {1}{\frac {5}{x-3}}=1\cdot\frac{x-3}{5}$$ the value for x=3 is defined Same for $\cot(x) = \cos(x) / \sin(x)$ , where $x= \pi/2$ is defined. Is my thesis working or am I hurting mathematics at this point?",,"['limits', 'functions', 'trigonometry']"
85,Avoiding the limit notation during long algebraic manipulations,Avoiding the limit notation during long algebraic manipulations,,"For example, look at this: $$ \begin{align} \lim_{x \rightarrow \infty}\frac{x}{x+1} &= \lim_{x \rightarrow \infty}{\frac{\frac{x}{x}}{\frac{x}{x}+\frac{1}{x}}}\\ &= \lim_{x \rightarrow \infty}{\frac{1}{1+0}} \\ &= \lim_{x \rightarrow \infty}{\frac{1}{1}} \\ &= \lim_{x \rightarrow \infty}{1} \\ &= 1 \\ \end{align} $$ Maybe this isn't the best example as most of those steps are unnecessary, but sometimes you end up with something like that, having to write $\lim_{x \rightarrow \infty}$ over and over again. Is there any notation or method one can use to avoid having to do this?","For example, look at this: $$ \begin{align} \lim_{x \rightarrow \infty}\frac{x}{x+1} &= \lim_{x \rightarrow \infty}{\frac{\frac{x}{x}}{\frac{x}{x}+\frac{1}{x}}}\\ &= \lim_{x \rightarrow \infty}{\frac{1}{1+0}} \\ &= \lim_{x \rightarrow \infty}{\frac{1}{1}} \\ &= \lim_{x \rightarrow \infty}{1} \\ &= 1 \\ \end{align} $$ Maybe this isn't the best example as most of those steps are unnecessary, but sometimes you end up with something like that, having to write $\lim_{x \rightarrow \infty}$ over and over again. Is there any notation or method one can use to avoid having to do this?",,"['limits', 'notation']"
86,The limit $\lim_{r\to0}\frac1r\left(1-\binom{n}{r}^{-1}\right)$,The limit,\lim_{r\to0}\frac1r\left(1-\binom{n}{r}^{-1}\right),I have deduced numerically (See also Wolfram ) that we have $$\lim_{r\to0}\frac1r\left(1-\frac1{\binom{n}{r}}\right)=H_n$$ where $H_n$ denotes the $n$ th Harmonic number and we define the binomial coefficient by $$\binom{n}{r}=\frac{\Gamma(n+1)}{\Gamma(r+1)\cdot\Gamma(n-r+1)}$$ Has anyone seen this result or similar elsewhere in mathematical literature? Is it possible to analytically prove this result? Would this result be any useful for calculating $H_n$ explicitly (especially for complex arguments)?,I have deduced numerically (See also Wolfram ) that we have where denotes the th Harmonic number and we define the binomial coefficient by Has anyone seen this result or similar elsewhere in mathematical literature? Is it possible to analytically prove this result? Would this result be any useful for calculating explicitly (especially for complex arguments)?,\lim_{r\to0}\frac1r\left(1-\frac1{\binom{n}{r}}\right)=H_n H_n n \binom{n}{r}=\frac{\Gamma(n+1)}{\Gamma(r+1)\cdot\Gamma(n-r+1)} H_n,"['limits', 'binomial-coefficients', 'gamma-function', 'harmonic-numbers']"
87,Limit involving binomial coefficients: $\lim_{n\to\infty}\left(\binom{n}{0}\binom{n}{1}\dots\binom{n}{n}\right)^{\frac{1}{n(n+1)}}$,Limit involving binomial coefficients:,\lim_{n\to\infty}\left(\binom{n}{0}\binom{n}{1}\dots\binom{n}{n}\right)^{\frac{1}{n(n+1)}},I am facing difficulty with the following limit. $$ \lim_{n\to\infty}\left(\binom{n}{0}\binom{n}{1}\dots\binom{n}{n}\right)^{\frac{1}{n(n+1)}} $$ I tried to take log both sides but I could not simplify the resulting expression. Please help in this regard. Thanks.,I am facing difficulty with the following limit. I tried to take log both sides but I could not simplify the resulting expression. Please help in this regard. Thanks., \lim_{n\to\infty}\left(\binom{n}{0}\binom{n}{1}\dots\binom{n}{n}\right)^{\frac{1}{n(n+1)}} ,['limits']
88,Without Taylor series $\lim\limits_{n\to\infty}\frac{n}{\ln \ln n}\cdot \left(\sqrt[n]{1+\frac{1}{2}+\cdots+\frac{1}{n}}-1\right) $,Without Taylor series,\lim\limits_{n\to\infty}\frac{n}{\ln \ln n}\cdot \left(\sqrt[n]{1+\frac{1}{2}+\cdots+\frac{1}{n}}-1\right) ,Is it possible to compute it without Taylor series? $$\lim_{n\to\infty}\frac{n}{\ln \ln n}\cdot \left(\sqrt[n]{1+\frac{1}{2}+\cdots+\frac{1}{n}}-1\right)  $$ Maybe you try your luck without computational software.,Is it possible to compute it without Taylor series? $$\lim_{n\to\infty}\frac{n}{\ln \ln n}\cdot \left(\sqrt[n]{1+\frac{1}{2}+\cdots+\frac{1}{n}}-1\right)  $$ Maybe you try your luck without computational software.,,['limits']
89,Proof of Monotone convergence theorem.,Proof of Monotone convergence theorem.,,"In this proof of MCT, I don't understand the the use of the number $\alpha$ . When i ignore it and read the proof I don't see where it fails. Thanks for help. Proof. Let $(X,\mathbb{X},\mu)$ be a measure space. Then since $f_{n} \leq f$ for all $n$ it follows that $\lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu \leq$ $\int f \mathrm{d} \mu .$ To show the inequality in the other direction we fix a simple function $\phi \leq f, 0<\alpha<1$ and let $$ A_{n}=\left\{x: \alpha \phi(x) \leq f_{n}(x)\right\} $$ Note that for each $n$ , $A_{n} \in \mathbb{X}$ and $A_{n} \leq A_{n+1} .$ We also have that $X=$ $\cup_{n=1}^{\infty} A_{n} .$ By Lemma 4.5 we can define a measure $\nu$ on $(X, \mathbb{X})$ by $$ \nu(A)=\int_{A} \phi \mathrm{d} \mu(x) $$ for all $A \in \mathbb{X} .$ It follows from Lemma 3.4 that $$ \int \phi \mathrm{d} \mu=\nu(X)=\nu\left(\cup_{n=1}^{\infty} A_{n}\right)=\lim _{n \rightarrow \infty} \nu\left(A_{n}\right) $$ However for all $n \in \mathbb{N}$ $$ \int_{X} f_{n} \mathrm{d} \mu \geq \int_{A_n} f_{n} \mathrm{d} \mu \geq \alpha \int_{A_{n}} \phi \mathrm{d} \mu=\alpha \nu\left(A_{n}\right) $$ and thus $\alpha \int \phi \mathrm{d} \mu \leq \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu .$ Since this holds for all $0<\alpha<1$ and simple functions $\phi \leq f$ it follows that $$ \int f \mathrm{d} \mu \leq \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu .$$","In this proof of MCT, I don't understand the the use of the number . When i ignore it and read the proof I don't see where it fails. Thanks for help. Proof. Let be a measure space. Then since for all it follows that To show the inequality in the other direction we fix a simple function and let Note that for each , and We also have that By Lemma 4.5 we can define a measure on by for all It follows from Lemma 3.4 that However for all and thus Since this holds for all and simple functions it follows that","\alpha (X,\mathbb{X},\mu) f_{n} \leq f n \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu \leq \int f \mathrm{d} \mu . \phi \leq f, 0<\alpha<1 
A_{n}=\left\{x: \alpha \phi(x) \leq f_{n}(x)\right\}
 n A_{n} \in \mathbb{X} A_{n} \leq A_{n+1} . X= \cup_{n=1}^{\infty} A_{n} . \nu (X, \mathbb{X}) 
\nu(A)=\int_{A} \phi \mathrm{d} \mu(x)
 A \in \mathbb{X} . 
\int \phi \mathrm{d} \mu=\nu(X)=\nu\left(\cup_{n=1}^{\infty} A_{n}\right)=\lim _{n \rightarrow \infty} \nu\left(A_{n}\right)
 n \in \mathbb{N} 
\int_{X} f_{n} \mathrm{d} \mu \geq \int_{A_n} f_{n} \mathrm{d} \mu \geq \alpha \int_{A_{n}} \phi \mathrm{d} \mu=\alpha \nu\left(A_{n}\right)
 \alpha \int \phi \mathrm{d} \mu \leq \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu . 0<\alpha<1 \phi \leq f 
\int f \mathrm{d} \mu \leq \lim _{n \rightarrow \infty} \int f_{n} \mathrm{d} \mu
.","['limits', 'measure-theory', 'proof-explanation', 'lebesgue-integral', 'lebesgue-measure']"
90,Is $\lim_{x\rightarrow\infty}\frac{x}{\pi(x)}-\ln(x) =-1$?,Is ?,\lim_{x\rightarrow\infty}\frac{x}{\pi(x)}-\ln(x) =-1,"$\pi(x)$ is the number of primes not exceeding $x$. The prime number theorem states that $\lim_{x\rightarrow \infty} \frac{\pi(x)}{x/\ln(x)} = 1.$ So I, naÃ¯vely, inferred that $\lim_{x\rightarrow\infty}\frac{x}{\pi(x)}-\ln(x) =0.$ But I did some tests and it would very much seem that the true value of the limit is minus one. But just because $\lim_{x\rightarrow \infty} \frac{\pi(x)}{x/\ln(x)} = 1,$ this doesn't tell anything about the absolute error.... So I ask if $\lim_{x\rightarrow\infty}\frac{x}{\pi(x)}-\ln(x) =-1$?","$\pi(x)$ is the number of primes not exceeding $x$. The prime number theorem states that $\lim_{x\rightarrow \infty} \frac{\pi(x)}{x/\ln(x)} = 1.$ So I, naÃ¯vely, inferred that $\lim_{x\rightarrow\infty}\frac{x}{\pi(x)}-\ln(x) =0.$ But I did some tests and it would very much seem that the true value of the limit is minus one. But just because $\lim_{x\rightarrow \infty} \frac{\pi(x)}{x/\ln(x)} = 1,$ this doesn't tell anything about the absolute error.... So I ask if $\lim_{x\rightarrow\infty}\frac{x}{\pi(x)}-\ln(x) =-1$?",,"['elementary-number-theory', 'limits', 'prime-numbers', 'logarithms']"
91,"On Ramanujan's approximation, $n!\sim \sqrt{\pi}\big(\frac ne\big)^n\sqrt [6]{(2n)^3+(2n)^2+n+\frac 1{30}}$","On Ramanujan's approximation,",n!\sim \sqrt{\pi}\big(\frac ne\big)^n\sqrt [6]{(2n)^3+(2n)^2+n+\frac 1{30}},"Over here I discovered that Ramanujan gave the following factorial approximation, better than Stirling's formula: $$n!\sim \sqrt{\pi}\left(\frac ne\right)^n\sqrt [6]{(2n)^3+(2n)^2+n+\frac 1{30}}$$ such that the error term decreases rapidly as $n\to \infty$ . In other words, $$\lim_{n\to\infty}\cfrac{n!}{\sqrt{\pi}\left(\frac ne\right)^n\sqrt [6]{8n^3+4n^2+n+\frac 1{30}}}=1$$ Just to add, Stirling's formula is: $$n!\sim \sqrt{2\pi n}\left(\frac ne\right)^n$$ so somehow Ramanujan was able to turn $2n$ into $\sqrt [3]{8n^3+4n^2+n+\frac 1{30}}$ . Notice that $2n=\sqrt [3]{8n^3}$ so the important expression is $4n^2+n+\frac 1{30}$ . Does anybody know how he got this result? Or is this another one of his mysterious results...","Over here I discovered that Ramanujan gave the following factorial approximation, better than Stirling's formula: such that the error term decreases rapidly as . In other words, Just to add, Stirling's formula is: so somehow Ramanujan was able to turn into . Notice that so the important expression is . Does anybody know how he got this result? Or is this another one of his mysterious results...",n!\sim \sqrt{\pi}\left(\frac ne\right)^n\sqrt [6]{(2n)^3+(2n)^2+n+\frac 1{30}} n\to \infty \lim_{n\to\infty}\cfrac{n!}{\sqrt{\pi}\left(\frac ne\right)^n\sqrt [6]{8n^3+4n^2+n+\frac 1{30}}}=1 n!\sim \sqrt{2\pi n}\left(\frac ne\right)^n 2n \sqrt [3]{8n^3+4n^2+n+\frac 1{30}} 2n=\sqrt [3]{8n^3} 4n^2+n+\frac 1{30},"['limits', 'proof-writing', 'asymptotics', 'approximation', 'radicals']"
92,Finding $\lim\limits_{x \to 0}\ \frac{\sin(\cos(x))}{\sec(x)}$,Finding,\lim\limits_{x \to 0}\ \frac{\sin(\cos(x))}{\sec(x)},"The problem is to find: $\lim\limits_{x \to 0}\ \dfrac{\sin(\cos(x))}{\sec(x)}$ I rewrite the equation as follows: $\lim\limits_{x \to 0}\ \dfrac{\sin(\cos(x))}{\dfrac{1}{\cos(x)}}$ And multiply by $\dfrac{\cos(x)}{\cos(x)}$, producing: $\lim\limits_{x \to 0}\ \dfrac{\cos(x)*\sin(\cos(x))}{\dfrac{\cos(x)}{\cos(x)}}$ And rewrite as: $\lim\limits_{x \to 0}\ \cos^2(x)\ \dfrac{\sin(\cos(x))}{\cos(x)}$ Which then becomes: $\lim\limits_{x \to 0}\ \cos^2(x) * 1$ Which becomes 1. However, the answer is apparently $\sin(1)$. What am I doing wrong? Edit: I found a different way to solve this, but I'm still not sure what I did wrong originally.","The problem is to find: $\lim\limits_{x \to 0}\ \dfrac{\sin(\cos(x))}{\sec(x)}$ I rewrite the equation as follows: $\lim\limits_{x \to 0}\ \dfrac{\sin(\cos(x))}{\dfrac{1}{\cos(x)}}$ And multiply by $\dfrac{\cos(x)}{\cos(x)}$, producing: $\lim\limits_{x \to 0}\ \dfrac{\cos(x)*\sin(\cos(x))}{\dfrac{\cos(x)}{\cos(x)}}$ And rewrite as: $\lim\limits_{x \to 0}\ \cos^2(x)\ \dfrac{\sin(\cos(x))}{\cos(x)}$ Which then becomes: $\lim\limits_{x \to 0}\ \cos^2(x) * 1$ Which becomes 1. However, the answer is apparently $\sin(1)$. What am I doing wrong? Edit: I found a different way to solve this, but I'm still not sure what I did wrong originally.",,['limits']
93,A limit on binomial coefficients,A limit on binomial coefficients,,Let $$x_n=\frac{1}{n^2}\sum_{k=0}^n \ln\left(n\atop k\right).$$ Find the limit of $x_n$. What I can do is just use Stolz formula. But I could not proceed.,Let $$x_n=\frac{1}{n^2}\sum_{k=0}^n \ln\left(n\atop k\right).$$ Find the limit of $x_n$. What I can do is just use Stolz formula. But I could not proceed.,,"['limits', 'binomial-coefficients', 'summation']"
94,A limit with an intuitive and wrong answer,A limit with an intuitive and wrong answer,,"In my last question I asked about a limit used in my exploration of tangent circles and whatnot. I decided to come up with a more direct approach to my problem, and now I only have to evaluate the limit  $$ \lim_{d\to x} \frac{\dfrac{f(x)-f(d)}{x-d}-f'(d)}{x-d}$$ Intuition would yield the answer is the second derivative of $f$ at $x$. However, by expanding and whatnot and then using l'HÃ´pital, as well as plugging in some sample $f$'s, I arrive at half of the second derivative. Why is my intuition wrong?","In my last question I asked about a limit used in my exploration of tangent circles and whatnot. I decided to come up with a more direct approach to my problem, and now I only have to evaluate the limit  $$ \lim_{d\to x} \frac{\dfrac{f(x)-f(d)}{x-d}-f'(d)}{x-d}$$ Intuition would yield the answer is the second derivative of $f$ at $x$. However, by expanding and whatnot and then using l'HÃ´pital, as well as plugging in some sample $f$'s, I arrive at half of the second derivative. Why is my intuition wrong?",,"['limits', 'derivatives']"
95,How to prove asymptotic limit of an incomplete Gamma function,How to prove asymptotic limit of an incomplete Gamma function,,"How can I prove: $$\lim_{z\to\infty}{\Gamma(z, x)\over\Gamma(z)} = 1$$ ? Here $\Gamma(z, x)$ is the upper incomplete gamma function and $\Gamma(z)$ is the gamma function. This must be something trivial, but I can't figure it out. Any ideas?","How can I prove: $$\lim_{z\to\infty}{\Gamma(z, x)\over\Gamma(z)} = 1$$ ? Here $\Gamma(z, x)$ is the upper incomplete gamma function and $\Gamma(z)$ is the gamma function. This must be something trivial, but I can't figure it out. Any ideas?",,"['limits', 'special-functions', 'gamma-function']"
96,A limit involving powers and sums of primes,A limit involving powers and sums of primes,,"This limit is one of three from a question that was closed for lacking focus and context. I can't speak to the source of the problem but I can include my own attempts and thoughts in an effort to resuscitate it. Let $p_n$ denote the $n^{th}$ prime number. Claim: $$  \lim_{n\to\infty}\frac{\sqrt[np_n]{p_n^{p_1+\cdots+p_n}}}{\sqrt[np_n]{p_1^{p_1}\cdots p_n^{p_n}}}=e^{1/4} $$ Taking the logarithm and simplifying, it amounts to showing $$ \lim_{n\to\infty} \frac{1}{n p_n} \sum_{k=1}^{n} p_k \log\left(\frac{p_n}{p_k}\right)=\frac{1}{4} $$ Using the very crass approximation $p_n\approx n$ and asymptotics of the hyperfactorial function $H(n) = \prod_{k=1}^n k^k$ (here $A$ is the Glashier constant, but it doesn't matter), it works out: $$ \lim_{n\to\infty} \frac{1}{n^2}\left(\log(n)\cdot \frac{n^2+n}{2}-\log(H(n))\right) $$ $$ =\lim_{n\to\infty} \frac{1}{2}\log(n)  + \frac{\log(n)}{2n}- \frac{1}{n^2}\log\left(e^{-n^2/4} n^{(6n^2+6n+1)/12}(A+O(n^{-2}))\right) $$ $$ =\frac{1}{4}+\lim_{n\to\infty} \frac{1}{2}\log(n)  + \frac{\log(n)}{2n}- \frac{6n^2+6n+1}{12n^2}\log(n)+\frac{o(n)}{n^2}= \frac{1}{4} $$ Of course, this is quite crass and it's not even clear to me that replacing $p_n$ with $n$ provides a lower bound. By the Prime Number Theorem, the first asymptotic we should be using is $p_n\approx n \log(n)$ , but the calculation becomes formidable at this point and I'm not sure how many terms to include in the asymptotic. Still, I believe the claim is true and would like to see a proof.","This limit is one of three from a question that was closed for lacking focus and context. I can't speak to the source of the problem but I can include my own attempts and thoughts in an effort to resuscitate it. Let denote the prime number. Claim: Taking the logarithm and simplifying, it amounts to showing Using the very crass approximation and asymptotics of the hyperfactorial function (here is the Glashier constant, but it doesn't matter), it works out: Of course, this is quite crass and it's not even clear to me that replacing with provides a lower bound. By the Prime Number Theorem, the first asymptotic we should be using is , but the calculation becomes formidable at this point and I'm not sure how many terms to include in the asymptotic. Still, I believe the claim is true and would like to see a proof.","p_n n^{th} 
 \lim_{n\to\infty}\frac{\sqrt[np_n]{p_n^{p_1+\cdots+p_n}}}{\sqrt[np_n]{p_1^{p_1}\cdots p_n^{p_n}}}=e^{1/4}
 
\lim_{n\to\infty} \frac{1}{n p_n} \sum_{k=1}^{n} p_k \log\left(\frac{p_n}{p_k}\right)=\frac{1}{4}
 p_n\approx n H(n) = \prod_{k=1}^n k^k A 
\lim_{n\to\infty} \frac{1}{n^2}\left(\log(n)\cdot \frac{n^2+n}{2}-\log(H(n))\right)
 
=\lim_{n\to\infty} \frac{1}{2}\log(n)  + \frac{\log(n)}{2n}- \frac{1}{n^2}\log\left(e^{-n^2/4} n^{(6n^2+6n+1)/12}(A+O(n^{-2}))\right)
 
=\frac{1}{4}+\lim_{n\to\infty} \frac{1}{2}\log(n)  + \frac{\log(n)}{2n}- \frac{6n^2+6n+1}{12n^2}\log(n)+\frac{o(n)}{n^2}= \frac{1}{4}
 p_n n p_n\approx n \log(n)","['limits', 'elementary-number-theory', 'prime-numbers', 'asymptotics']"
97,Limit approximation for $\pi$ in the four fours puzzle?,Limit approximation for  in the four fours puzzle?,\pi,"The four fours puzzle is a recreational math puzzle whose aim is to express whole numbers using four occurrences of the digit 4 and a specified set of operators. A common variety permits the following: Constants : decimals (e.g. 44, which uses two 4s), leading decimal point (e.g. $.4 = \frac{2}{5}$) and recurring decimals (e.g. $.\overline{4} = \frac{4}{9}$). Unary operators : negation $-x$, square root $\sqrt{x}$ and integer factorial $x!$. Binary operators : +, -, Ã—, Ã·, exponentiation $x^y$ and arbitrary roots $\sqrt[y]{x}$. If we extend the puzzle to arbitrary reals then it is clear by countability that some numbers can't be expressed precisely (though by Gelfondâ€“Schneider some transcendental numbers can). However, for some potentially unexpressable numbers it is possible to express arbitrarily close estimates. For example  $$e_n = \sqrt{\left(\frac{4!^{(n)} + \sqrt{4}}{4!^{(n)}} \right) ^{4!^{(n)}}}$$ where $$4!^{(n)} = 4\overbrace{!...!}^{n\textrm{ times}}$$  is a sequence of four fours expressions whose limit is $e$. Is there a similar expressible limit expression for $\pi$? There are a number of relevant limits at Pi: Limit representations but I can't think of how to express any of them using just four 4s. Note that proving the hypothesis in Repeated Factorials and Repeated Square Rooting would show that arbitrarily close estimates are possible for every number using just one 4, but without necessarily providing them explicitly. Note also that if we permit usage of the $\ln$ operator, then we can approximate any number using just three 4s. For example, the following is a sequence of four fours expressions whose limit is $a \geq 1$: $$a_n = \overbrace{\sqrt{\sqrt{\cdots \sqrt{- \frac{\ln ( \ln \overbrace{\sqrt{\sqrt{\cdots \sqrt{4}}}}^{\lfloor a^{2^n}\rfloor \textrm{ times}} / \ln 4 )}{\ln \sqrt{4}}}}}}^{n \textrm{ times}}$$","The four fours puzzle is a recreational math puzzle whose aim is to express whole numbers using four occurrences of the digit 4 and a specified set of operators. A common variety permits the following: Constants : decimals (e.g. 44, which uses two 4s), leading decimal point (e.g. $.4 = \frac{2}{5}$) and recurring decimals (e.g. $.\overline{4} = \frac{4}{9}$). Unary operators : negation $-x$, square root $\sqrt{x}$ and integer factorial $x!$. Binary operators : +, -, Ã—, Ã·, exponentiation $x^y$ and arbitrary roots $\sqrt[y]{x}$. If we extend the puzzle to arbitrary reals then it is clear by countability that some numbers can't be expressed precisely (though by Gelfondâ€“Schneider some transcendental numbers can). However, for some potentially unexpressable numbers it is possible to express arbitrarily close estimates. For example  $$e_n = \sqrt{\left(\frac{4!^{(n)} + \sqrt{4}}{4!^{(n)}} \right) ^{4!^{(n)}}}$$ where $$4!^{(n)} = 4\overbrace{!...!}^{n\textrm{ times}}$$  is a sequence of four fours expressions whose limit is $e$. Is there a similar expressible limit expression for $\pi$? There are a number of relevant limits at Pi: Limit representations but I can't think of how to express any of them using just four 4s. Note that proving the hypothesis in Repeated Factorials and Repeated Square Rooting would show that arbitrarily close estimates are possible for every number using just one 4, but without necessarily providing them explicitly. Note also that if we permit usage of the $\ln$ operator, then we can approximate any number using just three 4s. For example, the following is a sequence of four fours expressions whose limit is $a \geq 1$: $$a_n = \overbrace{\sqrt{\sqrt{\cdots \sqrt{- \frac{\ln ( \ln \overbrace{\sqrt{\sqrt{\cdots \sqrt{4}}}}^{\lfloor a^{2^n}\rfloor \textrm{ times}} / \ln 4 )}{\ln \sqrt{4}}}}}}^{n \textrm{ times}}$$",,"['limits', 'recreational-mathematics', 'puzzle', 'pi']"
98,Does $\displaystyle \lim_{x \rightarrow 0} \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)}$ exist?,Does  exist?,\displaystyle \lim_{x \rightarrow 0} \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)},"I was playing around with the function $f : \mathbb R \rightarrow \mathbb R$ , defined as follows. $$f(x) = \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)}$$ This function is undefined at $x = \frac{1}{n\pi}$ for all $n \in \mathbb N$ . Namely, this suggests $$\forall \delta > 0 : \exists x : 0 < |x| < \delta \wedge f(x) \text{ is undefined.}$$ I'm curious about $\lim_{x \rightarrow 0} f(x)$ . If this value exists, say set $\lim_{x \rightarrow 0} f(x) = L$ , we naturally must have that $$\forall \varepsilon > 0 : \exists \delta > 0 : 0 < |x| < \delta \Rightarrow |f(x) - L| < \varepsilon.$$ But, since $f$ is not defined for all $x$ in the set $0 < |x| < \delta$ , the conclusion $|f(x) - L| < \varepsilon$ cannot always be guaranteed to hold. So, I'm inclined to say that the limit $L$ does not exist. That being said, the graph looks like this and based on the graph, $f$ appears to satisfy this inequality: $$\forall x : \frac{\sin x}{x} \leq f(x) < 1$$ Since $\displaystyle \lim_{x\rightarrow 0} \frac{\sin x}{x} = 1$ , this suggests in a squeeze-theorem-esque way that $L$ ""wants"" to take the value $1$ , but of course we cannot actually apply the squeeze theorem here for the same reason that we couldn't apply the $\varepsilon$ - $\delta$ definition directly. So, my questions are: Is my analysis correct that $\displaystyle \lim_{x \rightarrow 0} \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)}$ does not exist? Is there any looser definition of a limit in common use (say, ${\lim}^\star$ ) that would set $\displaystyle {\lim_{x \rightarrow 0}}^\star \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)} = 1$ in a way that captures the spirit of what I described above?","I was playing around with the function , defined as follows. This function is undefined at for all . Namely, this suggests I'm curious about . If this value exists, say set , we naturally must have that But, since is not defined for all in the set , the conclusion cannot always be guaranteed to hold. So, I'm inclined to say that the limit does not exist. That being said, the graph looks like this and based on the graph, appears to satisfy this inequality: Since , this suggests in a squeeze-theorem-esque way that ""wants"" to take the value , but of course we cannot actually apply the squeeze theorem here for the same reason that we couldn't apply the - definition directly. So, my questions are: Is my analysis correct that does not exist? Is there any looser definition of a limit in common use (say, ) that would set in a way that captures the spirit of what I described above?",f : \mathbb R \rightarrow \mathbb R f(x) = \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)} x = \frac{1}{n\pi} n \in \mathbb N \forall \delta > 0 : \exists x : 0 < |x| < \delta \wedge f(x) \text{ is undefined.} \lim_{x \rightarrow 0} f(x) \lim_{x \rightarrow 0} f(x) = L \forall \varepsilon > 0 : \exists \delta > 0 : 0 < |x| < \delta \Rightarrow |f(x) - L| < \varepsilon. f x 0 < |x| < \delta |f(x) - L| < \varepsilon L f \forall x : \frac{\sin x}{x} \leq f(x) < 1 \displaystyle \lim_{x\rightarrow 0} \frac{\sin x}{x} = 1 L 1 \varepsilon \delta \displaystyle \lim_{x \rightarrow 0} \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)} {\lim}^\star \displaystyle {\lim_{x \rightarrow 0}}^\star \frac{\sin\left(x \sin \left( \frac 1x \right) \right)}{x \sin \left( \frac 1x \right)} = 1,"['limits', 'continuity', 'epsilon-delta']"
99,"""Continuous composition"" of Lie Bracket","""Continuous composition"" of Lie Bracket",,"Let $A,X\in M_n(\mathbb{R})$ . We denote $[A,X]=AX-XA$ the commutator. It is indeed a Lie Bracket for the matrix Lie algebra. Taking $A$ constant, I'm looking for ""the flow"" of the commutator. That is to say, a ""natural"" function $\Phi([A,\cdot{}],t)$ with $t\in\mathbb{R}$ such that if $t=n\in\mathbb{N}$ : $$\Phi([A,\cdot{}],n)(X) = [A,\;\dots\;[A, [A,X]]\dots\;]=[(A)^n,X]$$ If such a function exists, is there a ""good"" way to ""extend it"" for a matrix $A$ changing continuously as the composition goes on ? That is, for $A:t\mapsto A(t)\in \mathcal{C}([t_0,t_f],M_n(\mathbb{R}))$ being a continuous function. To sum up my question: For $A$ constant: $$\Phi([A,\cdot{}],t)(X) = [(A)^t,X]=\;?$$ And for a continuous matrix $A$ : $$\left(\mathop{\bigcirc}\limits_{t_0}^{t}\Phi([A(s),\cdot{}],ds)\right)(X)=\left(\mathop{\bigcirc}\limits_{t_0}^{t}[(A(s))^{ds},\cdot]\right)(X)=\;?$$ Thank you in advance for your kind help. EDIT: Here is my main idea for the constant case so far: First let's distinguish the exponential of a function and the exponential of a matrix: $$\begin{aligned}\exp(f)&=Id+f+\frac{1}{2!}f\circ f+\frac{1}{3!}f\circ f\circ f+...\\e^{M}&=I_n+M+\frac{1}{2!}M^2+\frac{1}{3!}M^3+...\end{aligned}$$ It is known that $\exp([A,\cdot{}])=e^{A}\times\cdot{}\times e^{-A}$ , by which I mean that for all $X\in M_n(\mathbb{R})$ , we have $\exp([A,\cdot{}])(X)=e^{A}Xe^{-A}$ . Hence: $$\Phi(\exp([A,\cdot{}]),t)=\Phi(e^{A}\times\cdot{}\times e^{-A},t)=e^{tA}\times\cdot{}\times e^{-tA}$$ Now, one can imagine that: $$\exp(\Phi([A,\cdot{}],t))=\Phi(\exp([A,\cdot{}]),t)$$ If one manages to show the above property and to extend the $\log$ operation to functions (when does it converge ?), we should obtain at the end: $$\begin{aligned}\Phi([A,\cdot{}],t)&= \log(\exp(\Phi([A,\cdot{}],t))) \\ &=\log(\Phi(\exp([A,\cdot{}]),t))\\ &=\log(\Phi(e^{A}\times \cdot{}\times e^{-A},t))\\ &=\log (e^{tA}\times \cdot{} \times e^{-tA}) \\ &=\;? \end{aligned}$$ BUT , this approach might be very wrong, since we have: $$\Phi(\exp([A,\cdot{}]),t)=e^{tA}\times\cdot{}\times e^{-tA}=\exp([tA,\cdot{}])$$ and unfortunately I think we are faced with: $$\exp([tA,\cdot{}]) \neq\exp(\Phi([A,\cdot{}],t))$$ I would also like to extend the previous result to a more general setting where $f$ denotes a function taken without a lot of hypothesis: $$\Phi(\exp{f},t)=\exp(t\times f)$$ Any reference or homemade proof on this matter would be very much appreciated: I will be happy to grant the bounty just for this result. I believe it can be useful for the non-constant case, if we get there at some point...","Let . We denote the commutator. It is indeed a Lie Bracket for the matrix Lie algebra. Taking constant, I'm looking for ""the flow"" of the commutator. That is to say, a ""natural"" function with such that if : If such a function exists, is there a ""good"" way to ""extend it"" for a matrix changing continuously as the composition goes on ? That is, for being a continuous function. To sum up my question: For constant: And for a continuous matrix : Thank you in advance for your kind help. EDIT: Here is my main idea for the constant case so far: First let's distinguish the exponential of a function and the exponential of a matrix: It is known that , by which I mean that for all , we have . Hence: Now, one can imagine that: If one manages to show the above property and to extend the operation to functions (when does it converge ?), we should obtain at the end: BUT , this approach might be very wrong, since we have: and unfortunately I think we are faced with: I would also like to extend the previous result to a more general setting where denotes a function taken without a lot of hypothesis: Any reference or homemade proof on this matter would be very much appreciated: I will be happy to grant the bounty just for this result. I believe it can be useful for the non-constant case, if we get there at some point...","A,X\in M_n(\mathbb{R}) [A,X]=AX-XA A \Phi([A,\cdot{}],t) t\in\mathbb{R} t=n\in\mathbb{N} \Phi([A,\cdot{}],n)(X) = [A,\;\dots\;[A, [A,X]]\dots\;]=[(A)^n,X] A A:t\mapsto A(t)\in \mathcal{C}([t_0,t_f],M_n(\mathbb{R})) A \Phi([A,\cdot{}],t)(X) = [(A)^t,X]=\;? A \left(\mathop{\bigcirc}\limits_{t_0}^{t}\Phi([A(s),\cdot{}],ds)\right)(X)=\left(\mathop{\bigcirc}\limits_{t_0}^{t}[(A(s))^{ds},\cdot]\right)(X)=\;? \begin{aligned}\exp(f)&=Id+f+\frac{1}{2!}f\circ f+\frac{1}{3!}f\circ f\circ f+...\\e^{M}&=I_n+M+\frac{1}{2!}M^2+\frac{1}{3!}M^3+...\end{aligned} \exp([A,\cdot{}])=e^{A}\times\cdot{}\times e^{-A} X\in M_n(\mathbb{R}) \exp([A,\cdot{}])(X)=e^{A}Xe^{-A} \Phi(\exp([A,\cdot{}]),t)=\Phi(e^{A}\times\cdot{}\times e^{-A},t)=e^{tA}\times\cdot{}\times e^{-tA} \exp(\Phi([A,\cdot{}],t))=\Phi(\exp([A,\cdot{}]),t) \log \begin{aligned}\Phi([A,\cdot{}],t)&= \log(\exp(\Phi([A,\cdot{}],t))) \\
&=\log(\Phi(\exp([A,\cdot{}]),t))\\
&=\log(\Phi(e^{A}\times \cdot{}\times e^{-A},t))\\
&=\log (e^{tA}\times \cdot{} \times e^{-tA}) \\
&=\;?
\end{aligned} \Phi(\exp([A,\cdot{}]),t)=e^{tA}\times\cdot{}\times e^{-tA}=\exp([tA,\cdot{}]) \exp([tA,\cdot{}]) \neq\exp(\Phi([A,\cdot{}],t)) f \Phi(\exp{f},t)=\exp(t\times f)","['limits', 'lie-algebras', 'matrix-calculus', 'function-and-relation-composition']"
