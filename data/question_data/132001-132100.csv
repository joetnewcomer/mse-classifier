,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Are Exponential and Trigonometric Functions the Only Non-Trivial Solutions to $F'(x)=F(x+a)$?,Are Exponential and Trigonometric Functions the Only Non-Trivial Solutions to ?,F'(x)=F(x+a),"Are exponential & trigonometric functions the only non-trivial solutions to $F'(x)=F(x+a)$? $F(x)=0$ would be the trivial solution. Then, for $a=0$ (or  $a=2\pi i$), we have $F(x)=e^x$, and for $a=\dfrac\pi2$ there are $F(x)=\sin x$ and $F(x)=\cos x$. But the three are connected by Euler's formula $e^{ix}=\cos x$ $+i\sin x$. Indeed, on a more general note, letting $F(x)=e^{\lambda x}$, we have $\lambda=\dfrac{W(-a)}{-a}$ where W is the Lambert W function . My question would be if these are the only ones, due to the special properties of the number e and the exponential function , or if there aren't by any chance more , which do not belong in the same family or category as these, i.e., which are not exponential or trigonometric in nature ? Thank you.","Are exponential & trigonometric functions the only non-trivial solutions to $F'(x)=F(x+a)$? $F(x)=0$ would be the trivial solution. Then, for $a=0$ (or  $a=2\pi i$), we have $F(x)=e^x$, and for $a=\dfrac\pi2$ there are $F(x)=\sin x$ and $F(x)=\cos x$. But the three are connected by Euler's formula $e^{ix}=\cos x$ $+i\sin x$. Indeed, on a more general note, letting $F(x)=e^{\lambda x}$, we have $\lambda=\dfrac{W(-a)}{-a}$ where W is the Lambert W function . My question would be if these are the only ones, due to the special properties of the number e and the exponential function , or if there aren't by any chance more , which do not belong in the same family or category as these, i.e., which are not exponential or trigonometric in nature ? Thank you.",,"['analysis', 'ordinary-differential-equations', 'functional-equations']"
1,4th order differential equations with Bessel Function solutions,4th order differential equations with Bessel Function solutions,,"I am working on a a 4th order linear PDE coming from the modified wave equation of a stiff material. I have radial symmetry which has lead me to a 4th order ODE in $r$: $r^3 R''''(r) + 2r^2 R'''(r) - rR''(r)+R'(r) = m^4 r^3 R$ where $m^4$ is a constant. This ODE is subject to boundary conditions: $|R(0)|<\infty$, $|R'(0)|<\infty$, $R''(R)=0$, $R'''(R)=0$, where $R$ is the radius of my circle. I know that the solution should be in terms of Bessel functions as is typical of problems of this type. Mathematica gives solutions in terms of Bessel functions and Meijer G functions. Looking at the basic formulation (from wikipedia) of the Meijer G functions I do not see how this is obviously a solution (not implying it is obvious, I am implying that I do not see how it fits). My first question is if someone can help me get to the Meijer G formulation or present to me useful information on how this DE relates to Meijer G functions. Secondly, is there anyway to rewrite this to get it in terms of Bessel functions (I do not think this is the case because they fulfill second order ODEs). Is the analytic solution of this easy (easy in terms of doable at a beginning graduate level) or is it very convoluted? EDIT: I went back to the original formulation of the problem and found: $\left( \frac{1}{r}\frac{d}{dr} \left\{ r \frac{d}{dr} \right\} \right) R(r) = m^4 R(r)$. Now, instead of expanding, I found that I can consider this as two PDEs: $\left( \frac{1}{r}\frac{d}{dr} \left\{ r \frac{d}{dr} \right\} \right) R(r) = k^2 R(r)$ and $\left( \frac{1}{r}\frac{d}{dr} \left\{ r \frac{d}{dr} \right\} \right) R(r) = -k^2 R(r)$. From this standpoint the solution becomes much easier to understand in terms of Bessel Functions. I will be posting a solution below.","I am working on a a 4th order linear PDE coming from the modified wave equation of a stiff material. I have radial symmetry which has lead me to a 4th order ODE in $r$: $r^3 R''''(r) + 2r^2 R'''(r) - rR''(r)+R'(r) = m^4 r^3 R$ where $m^4$ is a constant. This ODE is subject to boundary conditions: $|R(0)|<\infty$, $|R'(0)|<\infty$, $R''(R)=0$, $R'''(R)=0$, where $R$ is the radius of my circle. I know that the solution should be in terms of Bessel functions as is typical of problems of this type. Mathematica gives solutions in terms of Bessel functions and Meijer G functions. Looking at the basic formulation (from wikipedia) of the Meijer G functions I do not see how this is obviously a solution (not implying it is obvious, I am implying that I do not see how it fits). My first question is if someone can help me get to the Meijer G formulation or present to me useful information on how this DE relates to Meijer G functions. Secondly, is there anyway to rewrite this to get it in terms of Bessel functions (I do not think this is the case because they fulfill second order ODEs). Is the analytic solution of this easy (easy in terms of doable at a beginning graduate level) or is it very convoluted? EDIT: I went back to the original formulation of the problem and found: $\left( \frac{1}{r}\frac{d}{dr} \left\{ r \frac{d}{dr} \right\} \right) R(r) = m^4 R(r)$. Now, instead of expanding, I found that I can consider this as two PDEs: $\left( \frac{1}{r}\frac{d}{dr} \left\{ r \frac{d}{dr} \right\} \right) R(r) = k^2 R(r)$ and $\left( \frac{1}{r}\frac{d}{dr} \left\{ r \frac{d}{dr} \right\} \right) R(r) = -k^2 R(r)$. From this standpoint the solution becomes much easier to understand in terms of Bessel Functions. I will be posting a solution below.",,"['ordinary-differential-equations', 'special-functions']"
2,Uniqueness of the Laplace Transformation,Uniqueness of the Laplace Transformation,,"Today; when I was doing some Inverse Laplace transformation in the class, I encountered the following problem cited in Zill's book: The inverse Laplace transformation may be not unique . In Problems 29 and 30 evaluate $\mathscr{L}\{f(t)\}$. 29. $f(t) = \left\{         \begin{array}{ll}             1, & \quad t\ge0,~ t\ne1, t\ne2 \\             3, & \quad t=1 \\             4, & \quad t=2 \\         \end{array}     \right.$ 30. $f(t) = \left\{         \begin{array}{ll}             \text{e}^{3t}, & \quad t\ge0,~ t\ne 5, \\             1, & \quad t=5         \end{array}     \right.$ I did them, but I was wondered how to explain the students if they ask me about the criteria for the uniqueness. I think it is rooted in the functional analysis, however, I am weak in this area. Is there any easier way to explain the uniqueness for second year graduate students? Thanks for your time.","Today; when I was doing some Inverse Laplace transformation in the class, I encountered the following problem cited in Zill's book: The inverse Laplace transformation may be not unique . In Problems 29 and 30 evaluate $\mathscr{L}\{f(t)\}$. 29. $f(t) = \left\{         \begin{array}{ll}             1, & \quad t\ge0,~ t\ne1, t\ne2 \\             3, & \quad t=1 \\             4, & \quad t=2 \\         \end{array}     \right.$ 30. $f(t) = \left\{         \begin{array}{ll}             \text{e}^{3t}, & \quad t\ge0,~ t\ne 5, \\             1, & \quad t=5         \end{array}     \right.$ I did them, but I was wondered how to explain the students if they ask me about the criteria for the uniqueness. I think it is rooted in the functional analysis, however, I am weak in this area. Is there any easier way to explain the uniqueness for second year graduate students? Thanks for your time.",,"['ordinary-differential-equations', 'laplace-transform']"
3,Finding a second linearly independent solution to a differential equation,Finding a second linearly independent solution to a differential equation,,"Consider the differential equation $x''-\frac{2t}{1-t^2}x'+\frac{2}{1-t^2}x=0$ for $t$ in $(-1,1)$ . Show that $x(t)=t$ is a solution and find a second linearly independent solution $y(t)$ . So I get how to solve the equation and that $t$ is a solution, but how do I find another linearly independant solution? Any help or suggestions much appreciated. I'm new here so sorry if I have done anything wrong!","Consider the differential equation for in . Show that is a solution and find a second linearly independent solution . So I get how to solve the equation and that is a solution, but how do I find another linearly independant solution? Any help or suggestions much appreciated. I'm new here so sorry if I have done anything wrong!","x''-\frac{2t}{1-t^2}x'+\frac{2}{1-t^2}x=0 t (-1,1) x(t)=t y(t) t",['ordinary-differential-equations']
4,Unique solution of system of differential equation,Unique solution of system of differential equation,,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function and $g:\mathbb{R}\rightarrow\mathbb{R}$ be a Lipschitz function. Would you help me to prove that the system of differential equation $$ x'=g(x)$$ $$y'=f(x)y $$ with initial value $x(t_0)=x_0$ and $y(t_0)=y_0$ has a unique solution. Could I prove the uniqueness solution of $x'=g(x)$, $x(t_0)=x_0$ by Gronwall Inequality first then use the result to prove the second?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be a continuous function and $g:\mathbb{R}\rightarrow\mathbb{R}$ be a Lipschitz function. Would you help me to prove that the system of differential equation $$ x'=g(x)$$ $$y'=f(x)y $$ with initial value $x(t_0)=x_0$ and $y(t_0)=y_0$ has a unique solution. Could I prove the uniqueness solution of $x'=g(x)$, $x(t_0)=x_0$ by Gronwall Inequality first then use the result to prove the second?",,"['ordinary-differential-equations', 'dynamical-systems']"
5,On the extension of solutions to ODEs,On the extension of solutions to ODEs,,"I've been trying to grasp some theorems on the extension of solutions and I still have some questions. The theorems say thing about the solution escaping compact sets, being unbounded, etc. but I'm having trouble applying them to concrete equations. Is there any way to estimate the size of the interval of a solution? E.g., given the equation: $x' = 1 + x^2$ how can we know, without explicitly or numerically solving it, that the interval where its solutions are defined has finite longitude? This is rather easy to see if you solve the equation as you get $x(t) = tan(c + t)$ which is only defined in intervals of finite longitude. But how would one reach the same conclusion just studying $f(x) = 1 + x^2$ ? Is this even possible? On a related note, how does one go about proving that a solution can be extended to infinity? Thank you in advance.","I've been trying to grasp some theorems on the extension of solutions and I still have some questions. The theorems say thing about the solution escaping compact sets, being unbounded, etc. but I'm having trouble applying them to concrete equations. Is there any way to estimate the size of the interval of a solution? E.g., given the equation: $x' = 1 + x^2$ how can we know, without explicitly or numerically solving it, that the interval where its solutions are defined has finite longitude? This is rather easy to see if you solve the equation as you get $x(t) = tan(c + t)$ which is only defined in intervals of finite longitude. But how would one reach the same conclusion just studying $f(x) = 1 + x^2$ ? Is this even possible? On a related note, how does one go about proving that a solution can be extended to infinity? Thank you in advance.",,['ordinary-differential-equations']
6,Can an ODE system never converge to its stable equilibrium in the long run?,Can an ODE system never converge to its stable equilibrium in the long run?,,"I have the following coupled non-linear ODE system, which describes a biological system: $$ \begin{cases} \dfrac{dp}{dt} = -\gamma p f,\\ \\ \dfrac{df}{dt} = \gamma p f,\\ \\ \dfrac{dT}{dt} = \left( 1 - \dfrac{k}{T} \right) \left\{ b (1 - T) \dfrac{r^n}{f^n + r^n} - m \dfrac{f^n}{f^n + r^n} \right\}, \end{cases} $$ where the parameters are: $\gamma = 0.5432$ , $k = 0.0026$ , $b = 0.0885$ , $n = 14.9832$ , $r = 0.8265$ , and $m = 0.9780$ . I want to investigate the stability of the system. To this end, first, one observes that the first two equations imply that $p + f$ must be a constant, let's call it $w$ , and in our case, it must be non-negative (for the chosen parameters' values of the system in the above, it's $0.801$ ). Now, by excluding $p$ from the above system, we can write: $$ \begin{cases} \dfrac{df}{dt} = \gamma (w - f) f,\\ \\ \dfrac{dT}{dt} = \left( 1 - \dfrac{k}{T} \right) \left\{ b (1 - T) \dfrac{r^n}{f^n + r^n} - m \dfrac{f^n}{f^n + r^n} \right\}. \end{cases} $$ The second system has four equilibria, $(f, T)$ : $$(0, k), (0, 1), (w, k), \left( w, 1- \frac{m}{b} (w / r)^n \right).$$ Now, by calculating the eigenvalues of the Jacobian, it turns out that the first two equilibria are unstable, and the last two are stable equilibria. My question is as follows: If we simulate our original, i.e., the first system, using Mathematica for some initial conditions: s = NDSolve[{p'[t] == -gamma p[t] f[t], f'[t] == gamma p[t] f[t],  T'[t] == b (1 - T[t]) (r^n/(f[t]^n + r^n)) (1 - 0.0026/T[t]) - m (f[t]^n/(f[t]^n + r^n)) (1 - 0.0026/T[t]),  p[0] == p0, f[0] == 0.001, T[0] == T0}/.{gamma -> 0.5432, b -> 0.0885, m -> 0.9780, r -> 0.8265, n -> 14.9832, p0 -> 0.8, T0 -> 0.8}, {p, f, T}, {t, 0, 720}];  Plot[Evaluate[{f[t], T[t]}/.s], {t, 0, 720}, PlotStyle -> {Blue, Red}, AxesOrigin -> {-1, 0}, PlotRange -> All] we obtain: From the above plot it's clear that solutions are converging to $(w, k)$ in the long run, which here $w$ is around $0.8$ . However, it seems that the system never converges to $\left( w, 1- \frac{m}{b} (w / r)^n \right)$ , for any initial conditions. What is the reason behind that? Can a system never converge to one of its stable equilibria? EDIT The reason for transforming the original system: If one wants to do the stability analysis for the original system, one runs into a problem. It has two equilibria: $(f \to 0, T \to k)$ and $(f \to 0, T \to 1)$ , regardless of the value of $p$ . Since $p$ can be anything, it isn't clear how one can calculate the Jacobian. However, by transforming the system, we can eliminate this issue.","I have the following coupled non-linear ODE system, which describes a biological system: where the parameters are: , , , , , and . I want to investigate the stability of the system. To this end, first, one observes that the first two equations imply that must be a constant, let's call it , and in our case, it must be non-negative (for the chosen parameters' values of the system in the above, it's ). Now, by excluding from the above system, we can write: The second system has four equilibria, : Now, by calculating the eigenvalues of the Jacobian, it turns out that the first two equilibria are unstable, and the last two are stable equilibria. My question is as follows: If we simulate our original, i.e., the first system, using Mathematica for some initial conditions: s = NDSolve[{p'[t] == -gamma p[t] f[t], f'[t] == gamma p[t] f[t],  T'[t] == b (1 - T[t]) (r^n/(f[t]^n + r^n)) (1 - 0.0026/T[t]) - m (f[t]^n/(f[t]^n + r^n)) (1 - 0.0026/T[t]),  p[0] == p0, f[0] == 0.001, T[0] == T0}/.{gamma -> 0.5432, b -> 0.0885, m -> 0.9780, r -> 0.8265, n -> 14.9832, p0 -> 0.8, T0 -> 0.8}, {p, f, T}, {t, 0, 720}];  Plot[Evaluate[{f[t], T[t]}/.s], {t, 0, 720}, PlotStyle -> {Blue, Red}, AxesOrigin -> {-1, 0}, PlotRange -> All] we obtain: From the above plot it's clear that solutions are converging to in the long run, which here is around . However, it seems that the system never converges to , for any initial conditions. What is the reason behind that? Can a system never converge to one of its stable equilibria? EDIT The reason for transforming the original system: If one wants to do the stability analysis for the original system, one runs into a problem. It has two equilibria: and , regardless of the value of . Since can be anything, it isn't clear how one can calculate the Jacobian. However, by transforming the system, we can eliminate this issue.","
\begin{cases}
\dfrac{dp}{dt} = -\gamma p f,\\
\\
\dfrac{df}{dt} = \gamma p f,\\
\\
\dfrac{dT}{dt} = \left( 1 - \dfrac{k}{T} \right) \left\{ b (1 - T) \dfrac{r^n}{f^n + r^n} - m \dfrac{f^n}{f^n + r^n} \right\},
\end{cases}
 \gamma = 0.5432 k = 0.0026 b = 0.0885 n = 14.9832 r = 0.8265 m = 0.9780 p + f w 0.801 p 
\begin{cases}
\dfrac{df}{dt} = \gamma (w - f) f,\\
\\
\dfrac{dT}{dt} = \left( 1 - \dfrac{k}{T} \right) \left\{ b (1 - T) \dfrac{r^n}{f^n + r^n} - m \dfrac{f^n}{f^n + r^n} \right\}.
\end{cases}
 (f, T) (0, k), (0, 1), (w, k), \left( w, 1- \frac{m}{b} (w / r)^n \right). (w, k) w 0.8 \left( w, 1- \frac{m}{b} (w / r)^n \right) (f \to 0, T \to k) (f \to 0, T \to 1) p p",['ordinary-differential-equations']
7,What is the order of the following system $\frac{dx}{dt} = \sqrt{1-x^2}$,What is the order of the following system,\frac{dx}{dt} = \sqrt{1-x^2},"$\frac{dx}{dt} = \sqrt{1-x^2}$ I’m referring “Non linear dynamics” by Steven Strogatz. Does the above mentioned system belongs to first order? One particular solution of the system is $x(t) = \sin(t)$ . Then there are no fixed point in the system, rather the system exhibits limit cycle. Still I’m able to represent the system in first order format. As per Strogatz, oscillations are prohibited in first order system. What am I missing here?","I’m referring “Non linear dynamics” by Steven Strogatz. Does the above mentioned system belongs to first order? One particular solution of the system is . Then there are no fixed point in the system, rather the system exhibits limit cycle. Still I’m able to represent the system in first order format. As per Strogatz, oscillations are prohibited in first order system. What am I missing here?",\frac{dx}{dt} = \sqrt{1-x^2} x(t) = \sin(t),"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
8,Find a closed form of the generating function,Find a closed form of the generating function,,"I need to find the closed form of the generating function $a(z)$ given $(n+1)a_{n+1} = 3a_n + 1$ and $a_0 = 0$ . My attempt: Using $a_0 = 0$ the first few terms can be found. $a_1 = 1$ , $a_2 = 2$ , $a_3 = \frac{7}{3}$ , $a_4 = 2$ . \begin{align*} (n+1)a_{n+1} &= 3a_n + 1\\ \sum_{n\geq0}(n+1)a_{n+1}z^n &= \sum_{n\geq0}3a_nz^n + \sum_{n\geq0}z^n\\ D(a(z)) &= a(z) + \frac{1}{1-z}\\ \end{align*} Here is a differential equation. I used an integrating factor of $e^{-3z}$ which gives a solution of $a(z) = e^{3z}\int e^{-3z}\frac{1}{1-z}dz$ . I'm not sure how to integrate this. It looks to me that this is too complicated as a solution. I'm not sure if i am doing this correctly. Any help will be appreciated. Thanks.","I need to find the closed form of the generating function given and . My attempt: Using the first few terms can be found. , , , . Here is a differential equation. I used an integrating factor of which gives a solution of . I'm not sure how to integrate this. It looks to me that this is too complicated as a solution. I'm not sure if i am doing this correctly. Any help will be appreciated. Thanks.","a(z) (n+1)a_{n+1} = 3a_n + 1 a_0 = 0 a_0 = 0 a_1 = 1 a_2 = 2 a_3 = \frac{7}{3} a_4 = 2 \begin{align*}
(n+1)a_{n+1} &= 3a_n + 1\\
\sum_{n\geq0}(n+1)a_{n+1}z^n &= \sum_{n\geq0}3a_nz^n + \sum_{n\geq0}z^n\\
D(a(z)) &= a(z) + \frac{1}{1-z}\\
\end{align*} e^{-3z} a(z) = e^{3z}\int e^{-3z}\frac{1}{1-z}dz","['ordinary-differential-equations', 'generating-functions']"
9,The Method Of Frobenius,The Method Of Frobenius,,"The ODE $xy'' + y = 0$ has a real degeneracy.  Use The Method Of Frobenius to find a fundamental set of solutions. Here is the procedure, as I understand it: 1)  Plug the guess $y = x^s \sum_{n = 0}^\infty a_n x^n$ into the ODE and do the algebra/calculus to separate out the indicial equation and the recurrence relation. $xy'' + y = 0$ $x(x^s \sum_{n = 0}^\infty a_n x^n)'' + (x^s \sum_{n = 0}^\infty a_n x^n) = 0$ $\sum_{n = 0}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 0}^\infty a_n x^{n + s} = 0$ $\sum_{n = 0}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 1}^\infty a_{n - 1} x^{n + s - 1} = 0$ $s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 1}^\infty a_{n - 1} x^{n + s - 1} = 0$ $s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + a_{n - 1} x^{n + s - 1} = 0$ $s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty [(n + s)(n + s - 1) a_n + a_{n - 1}] x^{n + s - 1} = 0$ Indicial Equation: $s(s - 1) = 0$ Recurrence Relation: $(n + s)(n + s - 1) a_n + a_{n - 1} = 0$ , where $n \geq 1$ 2)  Solve the recurrence relation, treating $s$ as a constant, but without plugging in any indicial equation solutions. $(n + s)(n + s - 1) a_n + a_{n - 1} = 0$ $a_n = -\frac{1}{(n + s)(n + s - 1)}a_{n - 1}$ $a_n = a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}$ Here I would simplify using a formula I was given, $\Pi_{j = c}^d (j + k) = \frac{(d + k)!}{(c + k - 1)!}$ . $a_n = a_0 [\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}] [\frac{1}{\Pi_{m = 1}^n (m + s - 1)}]$ $a_n = a_0 [(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}]$ $a_n = \frac{(-1)^n s! (s - 1)!}{(n + s)! (n + s - 1)!} a_0$ 3)  To find one fundamental solution, plug the largest indicial equation solution $s_1$ and its associated recurrence relation solution into the guess, and make the particular choice $a_0 = 1$ . $s(s - 1) = 0$ $s = 0, 1$ $s_1 = 1$ $y = x^1 \sum_{n = 0}^\infty \frac{(-1)^n 1! (1 - 1)!}{(n + 1)! (n + 1 - 1)!} 1 x^n$ $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1}}{n! (n + 1)!}\leftarrow$ FIRST FUNDAMENTAL SOLUTION 4)  To find another fundamental solution, plug the other indicial equation solution $s_2$ and its associated recurrence relation solution into $y = [$ FIRST FUNDAMENTAL SOLUTION $]ln|x| +\ x^{s_2} \sum_{n = 0}^\infty [\frac{\partial}{\partial s} ((s - s_2)a_n)]_{s = s_2} x^n$ , and make the particular choice $a_0 = 1$ . $s_2 = 0$ Here I use the earlier, less simplified expression for $a_n$ which is in $\Pi$ product notation, in anticipation of taking the logarithmic partial derivative. $y = (\sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1}}{n! (n + 1)!})ln|x| + x^0 \sum_{n = 0}^\infty [\frac{\partial}{\partial s} ((s - 0) a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})]_{s = 0} x^n$ $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{\partial}{\partial s} (s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})]_{s = 0} x^n$ I make the substitution $b_n = s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}$ , appearing in the solution as $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!}$ $+ \sum_{n = 0}^\infty [\frac{\partial}{\partial s} b_n]_{s = 0} x^n$ . $b_n = s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}$ $ln|b_n| = ln|s| + ln|a_0| + \sum_{m = 1}^n ln|\frac{1}{m + s}| + ln|\frac{1}{1 - m - s}|$ $\frac{\partial}{\partial s} ln|b_n| = \frac{\partial}{\partial s} ln|s| + \frac{\partial}{\partial s} ln|a_0| + \sum_{m = 1}^n \frac{\partial}{\partial s} ln|\frac{1}{m + s}| + \frac{\partial}{\partial s} ln|\frac{1}{1 - m - s}|$ I set $\frac{\partial}{\partial s} ln|a_0|$ to $0$ because the chapter stipulates that this procedure only seeks solutions where $a_0$ is nonzero and does not depend on $s$ . $\frac{\frac{\partial b_n}{\partial s}}{b_n} = \frac{1}{s} + 0 + \sum_{m = 1}^n -\frac{1}{m + s} + \frac{1}{1 - m - s}$ $\frac{\partial b_n}{\partial s} = \frac{b_n}{s} + b_n \sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)}$ $\frac{\partial b_n}{\partial s} = \frac{s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}}{s} + s a_0 (\Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)})$ Now I can simplify the $\Pi$ products using the above formula.  I also make the choice for $a_0$ here. $\frac{\partial b_n}{\partial s} = \frac{s [\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}] [\frac{1}{\Pi_{m = 1}^n (m + s - 1)}]}{s} + s ([\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}][\frac{1}{\Pi_{m = 1}^n (m + s - 1)}])(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)})$ $\frac{\partial b_n}{\partial s} = \frac{s [(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}]}{s} + s ([(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}])(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)})$ $\frac{\partial b_n}{\partial s} = \frac{(-1)^n s! (s - 1)! + (-1)^{n + 1} s!^2 \sum_{m = 1}^n \frac{2m + 2s - 1}{(m + s)(m + s - 1)}}{(n + s)! (n + s - 1)!}$ Unwinding the substitution, I plug the expression for $\frac{\partial b_n}{\partial s}$ back into the fundamental solution. $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{(-1)^n s! (s - 1)! + (-1)^{n + 1} s!^2 \sum_{m = 1}^n \frac{2m + 2s - 1}{(m + s)(m + s - 1)}}{(n + s)! (n + s - 1)!}]_{s = 0} x^n$ $y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty \frac{(-1)^n (-1)! x^n + (-1)^{n + 1} x^n \sum_{m = 1}^n \frac{2m - 1}{m^2 - m}}{n! (n - 1)!}$ And now we see that something is broken. $(-1)!$ is undefined, and can't even be rescued by being generalized to the Gamma or Pi function.  What am I doing wrong, and how can I use this method to solve this ODE?","The ODE has a real degeneracy.  Use The Method Of Frobenius to find a fundamental set of solutions. Here is the procedure, as I understand it: 1)  Plug the guess into the ODE and do the algebra/calculus to separate out the indicial equation and the recurrence relation. Indicial Equation: Recurrence Relation: , where 2)  Solve the recurrence relation, treating as a constant, but without plugging in any indicial equation solutions. Here I would simplify using a formula I was given, . 3)  To find one fundamental solution, plug the largest indicial equation solution and its associated recurrence relation solution into the guess, and make the particular choice . FIRST FUNDAMENTAL SOLUTION 4)  To find another fundamental solution, plug the other indicial equation solution and its associated recurrence relation solution into FIRST FUNDAMENTAL SOLUTION , and make the particular choice . Here I use the earlier, less simplified expression for which is in product notation, in anticipation of taking the logarithmic partial derivative. I make the substitution , appearing in the solution as . I set to because the chapter stipulates that this procedure only seeks solutions where is nonzero and does not depend on . Now I can simplify the products using the above formula.  I also make the choice for here. Unwinding the substitution, I plug the expression for back into the fundamental solution. And now we see that something is broken. is undefined, and can't even be rescued by being generalized to the Gamma or Pi function.  What am I doing wrong, and how can I use this method to solve this ODE?","xy'' + y = 0 y = x^s \sum_{n = 0}^\infty a_n x^n xy'' + y = 0 x(x^s \sum_{n = 0}^\infty a_n x^n)'' + (x^s \sum_{n = 0}^\infty a_n x^n) = 0 \sum_{n = 0}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 0}^\infty a_n x^{n + s} = 0 \sum_{n = 0}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 1}^\infty a_{n - 1} x^{n + s - 1} = 0 s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + \sum_{n = 1}^\infty a_{n - 1} x^{n + s - 1} = 0 s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty (n + s)(n + s - 1) a_n x^{n + s - 1} + a_{n - 1} x^{n + s - 1} = 0 s(s - 1) a_0 x^{s - 1} + \sum_{n = 1}^\infty [(n + s)(n + s - 1) a_n + a_{n - 1}] x^{n + s - 1} = 0 s(s - 1) = 0 (n + s)(n + s - 1) a_n + a_{n - 1} = 0 n \geq 1 s (n + s)(n + s - 1) a_n + a_{n - 1} = 0 a_n = -\frac{1}{(n + s)(n + s - 1)}a_{n - 1} a_n = a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)} \Pi_{j = c}^d (j + k) = \frac{(d + k)!}{(c + k - 1)!} a_n = a_0 [\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}] [\frac{1}{\Pi_{m = 1}^n (m + s - 1)}] a_n = a_0 [(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}] a_n = \frac{(-1)^n s! (s - 1)!}{(n + s)! (n + s - 1)!} a_0 s_1 a_0 = 1 s(s - 1) = 0 s = 0, 1 s_1 = 1 y = x^1 \sum_{n = 0}^\infty \frac{(-1)^n 1! (1 - 1)!}{(n + 1)! (n + 1 - 1)!} 1 x^n y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1}}{n! (n + 1)!}\leftarrow s_2 y = [ ]ln|x| +\ x^{s_2} \sum_{n = 0}^\infty [\frac{\partial}{\partial s} ((s - s_2)a_n)]_{s = s_2} x^n a_0 = 1 s_2 = 0 a_n \Pi y = (\sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1}}{n! (n + 1)!})ln|x| + x^0 \sum_{n = 0}^\infty [\frac{\partial}{\partial s} ((s - 0) a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})]_{s = 0} x^n y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{\partial}{\partial s} (s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})]_{s = 0} x^n b_n = s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)} y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{\partial}{\partial s} b_n]_{s = 0} x^n b_n = s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)} ln|b_n| = ln|s| + ln|a_0| + \sum_{m = 1}^n ln|\frac{1}{m + s}| + ln|\frac{1}{1 - m - s}| \frac{\partial}{\partial s} ln|b_n| = \frac{\partial}{\partial s} ln|s| + \frac{\partial}{\partial s} ln|a_0| + \sum_{m = 1}^n \frac{\partial}{\partial s} ln|\frac{1}{m + s}| + \frac{\partial}{\partial s} ln|\frac{1}{1 - m - s}| \frac{\partial}{\partial s} ln|a_0| 0 a_0 s \frac{\frac{\partial b_n}{\partial s}}{b_n} = \frac{1}{s} + 0 + \sum_{m = 1}^n -\frac{1}{m + s} + \frac{1}{1 - m - s} \frac{\partial b_n}{\partial s} = \frac{b_n}{s} + b_n \sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)} \frac{\partial b_n}{\partial s} = \frac{s a_0 \Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)}}{s} + s a_0 (\Pi_{m = 1}^n -\frac{1}{(m + s)(m + s - 1)})(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)}) \Pi a_0 \frac{\partial b_n}{\partial s} = \frac{s [\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}] [\frac{1}{\Pi_{m = 1}^n (m + s - 1)}]}{s} + s ([\Pi_{m = 1}^n -1] [\frac{1}{\Pi_{m = 1}^n (m + s)}][\frac{1}{\Pi_{m = 1}^n (m + s - 1)}])(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)}) \frac{\partial b_n}{\partial s} = \frac{s [(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}]}{s} + s ([(-1)^n] [\frac{1}{\frac{(n + s)!}{s!}}] [\frac{1}{\frac{(n + s - 1)!}{(s - 1)!}}])(\sum_{m = 1}^n -\frac{2m + 2s - 1}{(m + s)(m + s - 1)}) \frac{\partial b_n}{\partial s} = \frac{(-1)^n s! (s - 1)! + (-1)^{n + 1} s!^2 \sum_{m = 1}^n \frac{2m + 2s - 1}{(m + s)(m + s - 1)}}{(n + s)! (n + s - 1)!} \frac{\partial b_n}{\partial s} y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty [\frac{(-1)^n s! (s - 1)! + (-1)^{n + 1} s!^2 \sum_{m = 1}^n \frac{2m + 2s - 1}{(m + s)(m + s - 1)}}{(n + s)! (n + s - 1)!}]_{s = 0} x^n y = \sum_{n = 0}^\infty \frac{(-1)^n x^{n + 1} ln|x|}{n! (n + 1)!} + \sum_{n = 0}^\infty \frac{(-1)^n (-1)! x^n + (-1)^{n + 1} x^n \sum_{m = 1}^n \frac{2m - 1}{m^2 - m}}{n! (n - 1)!} (-1)!","['ordinary-differential-equations', 'solution-verification', 'power-series', 'frobenius-method', 'fundamental-solution']"
10,Problem in the theory of finding the unique solution of $-u'' = \delta_0$ and $-u'' + cu= \delta_0$,Problem in the theory of finding the unique solution of  and,-u'' = \delta_0 -u'' + cu= \delta_0,"I have theese two problems: (I) Find the unique distribution $u \in H^1(-1,1)$ such that $-u'' = \delta_0$ and $u(-1) = u(1) = 0$ . (II) Let $c > 0$ , find the unique distribution $u \in H^1(\mathbb{R})$ such that $-u'' + cu= \delta_0$ Because we want to use Lax-Milgram's theorem, solving theese problem we began like this: $(I)$ $u \in H^1(-1,1)$ satisfies $-u'' = \delta_0$ in the distributional way if and only if for all $\varphi \in C_c^\infty(-1,1)$ $$\begin{cases} u \in H^1_0(-1,1)\\[8pt] \int_{-1}^1 u' \varphi' \ dx = \varphi(0) \end{cases}$$ $(II)$ $u \in H^1(\mathbb{R})$ satisfies $-u'' +cu= \delta_0$ in the distributional way if and only if for all $\varphi \in H^1(\mathbb{R})$ $$ \begin{cases} u \in H^1(\mathbb{R}) \\[8pt] \int_{-1}^1 u'\varphi' +cu\varphi \ dx = \varphi(0) \end{cases}$$ My questions are: Why in $(I)$ we took $\color{red} {u \in H^1_0(-1,1)}$ and not $u \in H^1(-1,1)$ ? Why in $(II)$ we took $\color{red}{\varphi \in H^1(\mathbb{R})}$ and not $\varphi \in C_c^\infty(\mathbb{R})$ ? What is the general approach to be sure to choose ""the right space for the right function"" in this kind of problems? I'm studying a course of a master's degree (Italy) in mathematics and as you may observe in my last questions on this site, this course was really linked to the solutions of the Dirichlet problem. Introducing the ""weak derivateive"" of a function we mentioned that distributions live in spaces where there is not a normal topology to define continuity (taken for example from a norm) but we did not go deeper. We defined the Sobolev space $W^{k,p}(\Omega)$ as follows: $$W^{k,p}(\Omega) := \{u\in L^p(\Omega) \ : \ D^\alpha u \in L^p(\Omega) \ \text{for all}\  \alpha : |\alpha| \le k\}$$ where $D^\alpha u$ is used as a notion of the distributional derivative of $u$ , that in our definition is the distribution that acts on every $\varphi \in C^\infty_c(\Omega)$ as $$\langle D^\alpha u , \varphi \rangle := (-1)^{|\alpha|}\langle u , D^\alpha \varphi \rangle$$ We define $H^k(\Omega) := W^{k,2}(\Omega)$ and $W^{k,p}_0(\Omega)$ is the closure of $C^\infty_c(\Omega)$ inside $W^{k,p}(\Omega)$ Here I state Lax-Milgram theorem: Let $H$ be an Hilbert real space and let $a:H\times H \to \mathbb{R}$ be a bilinear, continous and coerced form. Then for all $F\in H^*$ there exists one and one only $\bar{u} \in H$ such that $$a(\bar{u},v) = F(v) \qquad \forall v \in H$$ If $a$ is also symmetric then $\bar{u}$ satisfies $$\frac{1}{2}a(\bar{u},\bar{u}) - F(\bar{u}) = \min_{v\in H} \Big\{\frac{1}{2}a(v,v) + F(v)\Big\}$$","I have theese two problems: (I) Find the unique distribution such that and . (II) Let , find the unique distribution such that Because we want to use Lax-Milgram's theorem, solving theese problem we began like this: satisfies in the distributional way if and only if for all satisfies in the distributional way if and only if for all My questions are: Why in we took and not ? Why in we took and not ? What is the general approach to be sure to choose ""the right space for the right function"" in this kind of problems? I'm studying a course of a master's degree (Italy) in mathematics and as you may observe in my last questions on this site, this course was really linked to the solutions of the Dirichlet problem. Introducing the ""weak derivateive"" of a function we mentioned that distributions live in spaces where there is not a normal topology to define continuity (taken for example from a norm) but we did not go deeper. We defined the Sobolev space as follows: where is used as a notion of the distributional derivative of , that in our definition is the distribution that acts on every as We define and is the closure of inside Here I state Lax-Milgram theorem: Let be an Hilbert real space and let be a bilinear, continous and coerced form. Then for all there exists one and one only such that If is also symmetric then satisfies","u \in H^1(-1,1) -u'' = \delta_0 u(-1) = u(1) = 0 c > 0 u \in H^1(\mathbb{R}) -u'' + cu= \delta_0 (I) u \in H^1(-1,1) -u'' = \delta_0 \varphi \in C_c^\infty(-1,1) \begin{cases}
u \in H^1_0(-1,1)\\[8pt]
\int_{-1}^1 u' \varphi' \ dx = \varphi(0)
\end{cases} (II) u \in H^1(\mathbb{R}) -u'' +cu= \delta_0 \varphi \in H^1(\mathbb{R}) 
\begin{cases}
u \in H^1(\mathbb{R}) \\[8pt]
\int_{-1}^1 u'\varphi' +cu\varphi \ dx = \varphi(0)
\end{cases} (I) \color{red} {u \in H^1_0(-1,1)} u \in H^1(-1,1) (II) \color{red}{\varphi \in H^1(\mathbb{R})} \varphi \in C_c^\infty(\mathbb{R}) W^{k,p}(\Omega) W^{k,p}(\Omega) := \{u\in L^p(\Omega) \ : \ D^\alpha u \in L^p(\Omega) \ \text{for all}\  \alpha : |\alpha| \le k\} D^\alpha u u \varphi \in C^\infty_c(\Omega) \langle D^\alpha u , \varphi \rangle := (-1)^{|\alpha|}\langle u , D^\alpha \varphi \rangle H^k(\Omega) := W^{k,2}(\Omega) W^{k,p}_0(\Omega) C^\infty_c(\Omega) W^{k,p}(\Omega) H a:H\times H \to \mathbb{R} F\in H^* \bar{u} \in H a(\bar{u},v) = F(v) \qquad \forall v \in H a \bar{u} \frac{1}{2}a(\bar{u},\bar{u}) - F(\bar{u}) = \min_{v\in H} \Big\{\frac{1}{2}a(v,v) + F(v)\Big\}","['ordinary-differential-equations', 'sobolev-spaces', 'distribution-theory']"
11,Is Theory of ODEs by Coddington and Levinson still a good source for learning ODEs?,Is Theory of ODEs by Coddington and Levinson still a good source for learning ODEs?,,The book seems to cover interesting topics and I read an old review which said the book would be helpful in showing students the concrete side of analysis before delving into the more theoretical side of things. Is this book still relevant today?,The book seems to cover interesting topics and I read an old review which said the book would be helpful in showing students the concrete side of analysis before delving into the more theoretical side of things. Is this book still relevant today?,,"['ordinary-differential-equations', 'reference-request']"
12,Does this differential equation have a solution that has certain properties?,Does this differential equation have a solution that has certain properties?,,"Conjecture: Suppose we have a function $f(x)$ and some real number $\alpha$ such that $f(\alpha)=\alpha$ and $f'(\alpha)<1$ . Then there exists a solution $y(t)$ to the differential equation $f(y)-y=y'$ such that $\lim_{t\to\infty}y(t)=\alpha$ . I've tested some functions and the conjecture always seems to hold: For $f(x)=x^2, \alpha=0$ and the corresponding solution is $\dfrac{1}{ce^t+1}$ , which goes to $\alpha=0$ . If $f(x)=\sin(x)+x, \alpha = (2k+1)\pi$ , then $y(t)=2\cot^{-1}(ce^{-t})+2k\pi\to(2k+1)\pi$ and so on... I would appreciate any kind of help, discussion or source for further reading. Thanks!","Conjecture: Suppose we have a function and some real number such that and . Then there exists a solution to the differential equation such that . I've tested some functions and the conjecture always seems to hold: For and the corresponding solution is , which goes to . If , then and so on... I would appreciate any kind of help, discussion or source for further reading. Thanks!","f(x) \alpha f(\alpha)=\alpha f'(\alpha)<1 y(t) f(y)-y=y' \lim_{t\to\infty}y(t)=\alpha f(x)=x^2, \alpha=0 \dfrac{1}{ce^t+1} \alpha=0 f(x)=\sin(x)+x, \alpha = (2k+1)\pi y(t)=2\cot^{-1}(ce^{-t})+2k\pi\to(2k+1)\pi","['ordinary-differential-equations', 'limits', 'fixed-point-theorems']"
13,Harmonic oscillator with time dependent friction term,Harmonic oscillator with time dependent friction term,,"Suppose I have a harmonic oscillator of the following form: $\ddot x(t)=-F(t)\dot x(t)-x(t), F(t)>0$ for all $t$ . From the physical perspective, the term proportional to $\dot x(t)$ represents a friction term. Hence, if $F(t)>0$ for all $t$ , I would expect that $\lim_{t\to\infty} x(t)=0$ . But I am not sure how to proof that, and perhaps it is not even true, and my thinking is too simplistic. If someone had some thoughts, I would appreciate! : ) EDIT: I can probably also assume $F(t)$ to be $C^\infty$ .","Suppose I have a harmonic oscillator of the following form: for all . From the physical perspective, the term proportional to represents a friction term. Hence, if for all , I would expect that . But I am not sure how to proof that, and perhaps it is not even true, and my thinking is too simplistic. If someone had some thoughts, I would appreciate! : ) EDIT: I can probably also assume to be .","\ddot x(t)=-F(t)\dot x(t)-x(t), F(t)>0 t \dot x(t) F(t)>0 t \lim_{t\to\infty} x(t)=0 F(t) C^\infty","['ordinary-differential-equations', 'asymptotics']"
14,Precise conditions on Sturm-Liouville Theorems,Precise conditions on Sturm-Liouville Theorems,,"In Sturm-Liouville (SL) theory ( https://en.wikipedia.org/wiki/Sturm-Liouville_theory ), there are three fundamental theorems concerning the solutions of the SL differential equation, $ \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x)y(x)=\lambda y(x)$. They are (SL Theorems): The set of eigenvalues $\lambda=\{\lambda_1, \lambda_2,\ldots\}$ are all real, countable and distinct. The set of eigenfunctions $y(x)=\{y_1(x),y_2(x),\ldots\}$, forms a orthogonal system in some interval $(a,b)$ of the real line, so that they satisfy $\int_a^b y_m(x)y_n(x)\mathrm{d}x=K_n\delta_{m,n}$, where $K_n$ are non-null constants. This set of eigenfunctions forms a basis for the vector space of square integrable functions. Now, it is very known that a second-order differential equation of the form $f(x)y''(x)+g(x)y'(x)+h(x)y(x)=\lambda y(x)$ can be put into the SL form after we multiply it by some integrating function $w(x)$ that satisfies the first-order differential equation $\frac{\mathrm{d}}{\mathrm{d}x}\left[w(x)f(x)\right] = w(x)g(x)$. In this case, the solutions of this differential equation will obey the SL theorems, if we agree that the solutions are orthogonal with respect to the weight function $w(x)$, i.e., they satisfy $\int_a^b y_m(x)y_n(x)w(x)\mathrm{d}x=K_n\delta_{m,n}$. My question is: what are the precise conditions (on the functions $p,q,f,g,h,w$, on the interval of orthogonality etc.) for the SL Theorems above to hold? I ask this motivated by the following related problem: it is very known that some second-order differential equations admit an infinite sequence of orthogonal polynomials on the real line -- these are called the Classical Orthogonal Polynomials (COP) and comprehends the Jacobi, Hermite and Laguerre polynomials). There is a theorem due to Bochner (see for instance the book of T. Chihara, ""An Introduction to Orthogonal Polynomials"", p. 150) that these three sequences of COP are the only ones satisfying a second-order differential equation that are orthogonal on the real line (up to linear transformations). However, in Bochner proof appears another infinite sequence of polynomials -- called nowadays as Bessel polynomials -- that although are not orthogonal on the real line, they are orthogonal on the complex unit circle. These Bessel polynomials satisfy the differential equation (see Krall, H.L., and Frink, O. ""A new class of orthogonal polynomials: The Bessel polynomials."" Transactions of the American Mathematical Society 65.1 (1949): 100-115.): $x^2y''(x) + 2(x+1)y'(x)=n(n+1)y(x)$. It can be verified that $w(x)=\exp(-2/x)$ is the integrating function for this differential equation (hence the weight function), so it can be put into the SL form. However, I can't see why the SL Theorems do not hold for it (they seem to not hold, since, for instance, the Bessel polynomial of degree 2, $B_2(x)=3x^2+3x+1$, has imaginary roots, while the roots of any orthogonal polynomial sequence on the real line are all real). I will appreciate any comment about these questions.","In Sturm-Liouville (SL) theory ( https://en.wikipedia.org/wiki/Sturm-Liouville_theory ), there are three fundamental theorems concerning the solutions of the SL differential equation, $ \frac{\mathrm{d}}{\mathrm{d}x}\left[p(x)\frac{\mathrm{d}y(x)}{\mathrm{d}x}\right]+q(x)y(x)=\lambda y(x)$. They are (SL Theorems): The set of eigenvalues $\lambda=\{\lambda_1, \lambda_2,\ldots\}$ are all real, countable and distinct. The set of eigenfunctions $y(x)=\{y_1(x),y_2(x),\ldots\}$, forms a orthogonal system in some interval $(a,b)$ of the real line, so that they satisfy $\int_a^b y_m(x)y_n(x)\mathrm{d}x=K_n\delta_{m,n}$, where $K_n$ are non-null constants. This set of eigenfunctions forms a basis for the vector space of square integrable functions. Now, it is very known that a second-order differential equation of the form $f(x)y''(x)+g(x)y'(x)+h(x)y(x)=\lambda y(x)$ can be put into the SL form after we multiply it by some integrating function $w(x)$ that satisfies the first-order differential equation $\frac{\mathrm{d}}{\mathrm{d}x}\left[w(x)f(x)\right] = w(x)g(x)$. In this case, the solutions of this differential equation will obey the SL theorems, if we agree that the solutions are orthogonal with respect to the weight function $w(x)$, i.e., they satisfy $\int_a^b y_m(x)y_n(x)w(x)\mathrm{d}x=K_n\delta_{m,n}$. My question is: what are the precise conditions (on the functions $p,q,f,g,h,w$, on the interval of orthogonality etc.) for the SL Theorems above to hold? I ask this motivated by the following related problem: it is very known that some second-order differential equations admit an infinite sequence of orthogonal polynomials on the real line -- these are called the Classical Orthogonal Polynomials (COP) and comprehends the Jacobi, Hermite and Laguerre polynomials). There is a theorem due to Bochner (see for instance the book of T. Chihara, ""An Introduction to Orthogonal Polynomials"", p. 150) that these three sequences of COP are the only ones satisfying a second-order differential equation that are orthogonal on the real line (up to linear transformations). However, in Bochner proof appears another infinite sequence of polynomials -- called nowadays as Bessel polynomials -- that although are not orthogonal on the real line, they are orthogonal on the complex unit circle. These Bessel polynomials satisfy the differential equation (see Krall, H.L., and Frink, O. ""A new class of orthogonal polynomials: The Bessel polynomials."" Transactions of the American Mathematical Society 65.1 (1949): 100-115.): $x^2y''(x) + 2(x+1)y'(x)=n(n+1)y(x)$. It can be verified that $w(x)=\exp(-2/x)$ is the integrating function for this differential equation (hence the weight function), so it can be put into the SL form. However, I can't see why the SL Theorems do not hold for it (they seem to not hold, since, for instance, the Bessel polynomial of degree 2, $B_2(x)=3x^2+3x+1$, has imaginary roots, while the roots of any orthogonal polynomial sequence on the real line are all real). I will appreciate any comment about these questions.",,"['ordinary-differential-equations', 'bessel-functions', 'orthogonal-polynomials', 'sturm-liouville']"
15,How to find a Lyapunov function?,How to find a Lyapunov function?,,"$$\dot{x} = x - x^2 - xy$$ $$\dot{y} = y - y^2 - xy$$ Is there a general method to find a Lyapunov function? Why do I feel like finding a Lyapunov function is like shooting in the dark and luck-dependent? I tried to guess some function with $x$ and $y$, but it doesn't work. I also tried Mathematica, and apparently, this is too complicated for Mathematica to solve. A side-note question: if the general method to find a Lyapunov function is currently not available, is that because no one is smart enough to come up with the general method? Or is it due to its nature that it's ""impossible"" to have the general method? And has anyone proved its impossibility? Note: it's not a duplicate question because only my side-note question is a duplicate of the other question. My first question is tailored to this specific system, which is unique.","$$\dot{x} = x - x^2 - xy$$ $$\dot{y} = y - y^2 - xy$$ Is there a general method to find a Lyapunov function? Why do I feel like finding a Lyapunov function is like shooting in the dark and luck-dependent? I tried to guess some function with $x$ and $y$, but it doesn't work. I also tried Mathematica, and apparently, this is too complicated for Mathematica to solve. A side-note question: if the general method to find a Lyapunov function is currently not available, is that because no one is smart enough to come up with the general method? Or is it due to its nature that it's ""impossible"" to have the general method? And has anyone proved its impossibility? Note: it's not a duplicate question because only my side-note question is a duplicate of the other question. My first question is tailored to this specific system, which is unique.",,"['ordinary-differential-equations', 'stability-theory', 'lyapunov-functions']"
16,Limit cycle for a system of ODEs,Limit cycle for a system of ODEs,,"Consider the system   $$\frac{dx}{dt}=x (\lambda-x^2 + (1+\epsilon^2)y^2))+\omega y,~\frac{dy}{dt}=-\omega x + y (\lambda-x^2 + (1+\epsilon^2)y^2)).$$   Show that the system has a stable limit cycle for $\lambda, \epsilon >0.$ My approach: Setting the RHS of each equal to zero and adding the two terms yields: $$(x^2+y^2)\lambda - x^2 (x^2+y^2)-(1+\epsilon^2)(x^2+y^2)y^2=0.$$ Using polar coordinates $x=r \cos \theta,~y=r \sin \theta,$ we get $$r^2 \cdot (\lambda - r^2 \cos^2 \theta - (1+\epsilon^2) r^2 \sin^2 \theta)=0,$$ which gives  $$r=0,~r=\pm \frac{ \sqrt{\lambda}}{\sqrt{(1+\epsilon^2 \sin^2 \theta)}}.$$ ............................................................................................ As one of the comments suggested, I tried setting the right hand sides of each equal to zero. $$x (\lambda-x^2 + (1+\epsilon^2)y^2))+\omega y=0,~-\omega x + x (\lambda-x^2 + (1+\epsilon^2)y^2))=0.$$ I tried method of elimination to solve for $x$ and $y,$ but wasn't successful.  I still don't know how can this be any simpler than what I did before.  I'm still stuck in this problem. Can someone please explain me from this how can I proved the desired conclusion. Thank you for your time.","Consider the system   $$\frac{dx}{dt}=x (\lambda-x^2 + (1+\epsilon^2)y^2))+\omega y,~\frac{dy}{dt}=-\omega x + y (\lambda-x^2 + (1+\epsilon^2)y^2)).$$   Show that the system has a stable limit cycle for $\lambda, \epsilon >0.$ My approach: Setting the RHS of each equal to zero and adding the two terms yields: $$(x^2+y^2)\lambda - x^2 (x^2+y^2)-(1+\epsilon^2)(x^2+y^2)y^2=0.$$ Using polar coordinates $x=r \cos \theta,~y=r \sin \theta,$ we get $$r^2 \cdot (\lambda - r^2 \cos^2 \theta - (1+\epsilon^2) r^2 \sin^2 \theta)=0,$$ which gives  $$r=0,~r=\pm \frac{ \sqrt{\lambda}}{\sqrt{(1+\epsilon^2 \sin^2 \theta)}}.$$ ............................................................................................ As one of the comments suggested, I tried setting the right hand sides of each equal to zero. $$x (\lambda-x^2 + (1+\epsilon^2)y^2))+\omega y=0,~-\omega x + x (\lambda-x^2 + (1+\epsilon^2)y^2))=0.$$ I tried method of elimination to solve for $x$ and $y,$ but wasn't successful.  I still don't know how can this be any simpler than what I did before.  I'm still stuck in this problem. Can someone please explain me from this how can I proved the desired conclusion. Thank you for your time.",,['ordinary-differential-equations']
17,How to derive the power series solution for Associated Legendre Differential Equation,How to derive the power series solution for Associated Legendre Differential Equation,,"This question (similar other questions) have troubled me for last 5 years. Two months ago I thought I would simply solve the Hydrogen Atom problem and see these Associated Legendre Polynomials come to life by themselves. I haven't solved it yet. I began by writing the Schrodinger equation for Hydrogen atom. $$ -\frac{\hbar^2}{2m} \nabla^2 \psi + V \psi = E\psi $$ I followed a well know method of separation of variables. $\psi(r,\theta,\phi) = R(r)P(\theta)Q(\phi)$. I plugged it in the above Schrodinger equation. The result was, $$ \underset{\text{function of r}}{\underbrace{\frac{1}{R}\frac{d}{dr}\left(r^2\frac{dR}{dr}\right) + \frac{2mr^2}{\hbar^2}(E-V)}}  + \underset{\text{function of x}}{\underbrace{\frac{1}{P}\frac{d}{dx}\left((1-x^2)\frac{dP}{dx}\right) + \frac{1}{(1-x^2)}\underset{\text{function of }\phi}{\underbrace{\frac{1}{Q}\frac{d^2Q}{d\phi^2}}}}} = 0 $$ Here $x = \cos\theta$. Now we can proceed step by step to solve it. Step 1. $\frac{1}{Q}\frac{d^2Q}{d\phi^2} = -m^2$ where m is an integer (due to the periodic nature of the $Q(\phi)$). Step 2. This gives us the two following equations, $$ \frac{1}{R}\frac{d}{dr}\left(r^2\frac{dR}{dr}\right) + \frac{2mr^2}{\hbar^2}(E-V) = A $$ $$ \frac{1}{P}\frac{d}{dx}\left((1-x^2)\frac{dP}{dx}\right) - \frac{m^2}{(1-x^2)} = -A $$ where A is some constant. We have not got a clue about this constant at all this point. I simply am interested in associated Legendre so I went for the later equation. Step 3. $$ (1-x^2)^2P'' -2x(1-x^2)P' +\left(A(1-x^2) - m^2\right)P = 0 $$ Now this equation can be solved by assuming power series solution of the form, $P = \sum_{j=0}^{\infty} a_j x^j$. Step 4. When this expansion is plugged into the the Associated Legendre DE I got, $(j+2)(j+1)a_{j+2}+[A-m^2-2j^2]a_j+[(j-2)(j-3)-A]a_{j-2} = 0$ Remembering that $a_j = 0\ \forall\ j<0$ I found few terms, $$a_2 = -\frac{(A-m^2)}{2\cdot1}a_0$$ $$a_3 = -\frac{(A-m^2-2)}{3\cdot2}a_1$$ $$a_4 = \left[A + \frac{(A-m^2)(A-m^2-8)}{2\cdot1}\right]\frac{a_0}{4\cdot3}$$ $$a_5 = \left[A + \frac{(A-m^2-2)(A-m^2-18)}{3\cdot2}\right]\frac{a_1}{5\cdot4}$$ Now I don't see any pattern emerging through which I could place condition on the constant A. Kindly someone help me this question is driving me crazy.","This question (similar other questions) have troubled me for last 5 years. Two months ago I thought I would simply solve the Hydrogen Atom problem and see these Associated Legendre Polynomials come to life by themselves. I haven't solved it yet. I began by writing the Schrodinger equation for Hydrogen atom. $$ -\frac{\hbar^2}{2m} \nabla^2 \psi + V \psi = E\psi $$ I followed a well know method of separation of variables. $\psi(r,\theta,\phi) = R(r)P(\theta)Q(\phi)$. I plugged it in the above Schrodinger equation. The result was, $$ \underset{\text{function of r}}{\underbrace{\frac{1}{R}\frac{d}{dr}\left(r^2\frac{dR}{dr}\right) + \frac{2mr^2}{\hbar^2}(E-V)}}  + \underset{\text{function of x}}{\underbrace{\frac{1}{P}\frac{d}{dx}\left((1-x^2)\frac{dP}{dx}\right) + \frac{1}{(1-x^2)}\underset{\text{function of }\phi}{\underbrace{\frac{1}{Q}\frac{d^2Q}{d\phi^2}}}}} = 0 $$ Here $x = \cos\theta$. Now we can proceed step by step to solve it. Step 1. $\frac{1}{Q}\frac{d^2Q}{d\phi^2} = -m^2$ where m is an integer (due to the periodic nature of the $Q(\phi)$). Step 2. This gives us the two following equations, $$ \frac{1}{R}\frac{d}{dr}\left(r^2\frac{dR}{dr}\right) + \frac{2mr^2}{\hbar^2}(E-V) = A $$ $$ \frac{1}{P}\frac{d}{dx}\left((1-x^2)\frac{dP}{dx}\right) - \frac{m^2}{(1-x^2)} = -A $$ where A is some constant. We have not got a clue about this constant at all this point. I simply am interested in associated Legendre so I went for the later equation. Step 3. $$ (1-x^2)^2P'' -2x(1-x^2)P' +\left(A(1-x^2) - m^2\right)P = 0 $$ Now this equation can be solved by assuming power series solution of the form, $P = \sum_{j=0}^{\infty} a_j x^j$. Step 4. When this expansion is plugged into the the Associated Legendre DE I got, $(j+2)(j+1)a_{j+2}+[A-m^2-2j^2]a_j+[(j-2)(j-3)-A]a_{j-2} = 0$ Remembering that $a_j = 0\ \forall\ j<0$ I found few terms, $$a_2 = -\frac{(A-m^2)}{2\cdot1}a_0$$ $$a_3 = -\frac{(A-m^2-2)}{3\cdot2}a_1$$ $$a_4 = \left[A + \frac{(A-m^2)(A-m^2-8)}{2\cdot1}\right]\frac{a_0}{4\cdot3}$$ $$a_5 = \left[A + \frac{(A-m^2-2)(A-m^2-18)}{3\cdot2}\right]\frac{a_1}{5\cdot4}$$ Now I don't see any pattern emerging through which I could place condition on the constant A. Kindly someone help me this question is driving me crazy.",,"['ordinary-differential-equations', 'special-functions', 'legendre-polynomials']"
18,What is the necessary condition for ODE to have unique solution?,What is the necessary condition for ODE to have unique solution?,,"For the ODE: \begin{align} \dot{x}(t)&=f(x,t) \\ x(t_{0})&=x_{0} \end{align} If $\;\;f:\mathbb{R}^{n}\rightarrow{}\mathbb{R}^{n}$ is Lipschitz continuous on $\mathbb{R}^{n}$, then there exists a unique solution for the ODE. The reverse is not always true, right? and what is the necessary condition for the uniqueness for the ODE? I have read a book about numerical computing for ODE. The author only assumed there exists a unique solution for the ODE. In proving some properties in the numerical scheme, the author used the Lipschitz continuous property of the ODE. I wonder it is not a correct proof.","For the ODE: \begin{align} \dot{x}(t)&=f(x,t) \\ x(t_{0})&=x_{0} \end{align} If $\;\;f:\mathbb{R}^{n}\rightarrow{}\mathbb{R}^{n}$ is Lipschitz continuous on $\mathbb{R}^{n}$, then there exists a unique solution for the ODE. The reverse is not always true, right? and what is the necessary condition for the uniqueness for the ODE? I have read a book about numerical computing for ODE. The author only assumed there exists a unique solution for the ODE. In proving some properties in the numerical scheme, the author used the Lipschitz continuous property of the ODE. I wonder it is not a correct proof.",,"['ordinary-differential-equations', 'numerical-methods']"
19,What is the difference between an integral curve and the solution of a differential equation?,What is the difference between an integral curve and the solution of a differential equation?,,"Can you please explain what the difference between an integral curve and the solution of a differential equation is? My book gives an example that $$\frac {dy}{dx}=\frac {y}{x}$$ defines a direction field everywhere except at the origin. the function $y=kx$ is a solution of this equation, and the integral curve is given by $ax+by=0$, where $a,b$ are arbitrary constants. Then the book concludes that y axis is the integral curve of the differential equation, but not the graph of the solution. I know the definition of the integral curve and the solution of an equation. A detailed help would be much appreciated! Thanks in advance!","Can you please explain what the difference between an integral curve and the solution of a differential equation is? My book gives an example that $$\frac {dy}{dx}=\frac {y}{x}$$ defines a direction field everywhere except at the origin. the function $y=kx$ is a solution of this equation, and the integral curve is given by $ax+by=0$, where $a,b$ are arbitrary constants. Then the book concludes that y axis is the integral curve of the differential equation, but not the graph of the solution. I know the definition of the integral curve and the solution of an equation. A detailed help would be much appreciated! Thanks in advance!",,['ordinary-differential-equations']
20,Differential equation on a manifold,Differential equation on a manifold,,"I want to solve this problem : M is a manifold.  Let $t\mapsto \gamma(t)$ be an integral curve of a   vector field X on M. Suppose there exists $t_0$ such that   $\gamma'(t_0)=0$. Prove that $\gamma(t)=\gamma(t_0)$ for all t. I know that we have $X(\gamma(t))=\gamma'(t)$ and that we have to use uniqueness of a solution but i have some difficulties writing a ""technical"" solution. Thank you for any help.","I want to solve this problem : M is a manifold.  Let $t\mapsto \gamma(t)$ be an integral curve of a   vector field X on M. Suppose there exists $t_0$ such that   $\gamma'(t_0)=0$. Prove that $\gamma(t)=\gamma(t_0)$ for all t. I know that we have $X(\gamma(t))=\gamma'(t)$ and that we have to use uniqueness of a solution but i have some difficulties writing a ""technical"" solution. Thank you for any help.",,"['ordinary-differential-equations', 'differential-geometry', 'manifolds']"
21,Eigenvalues of Differential Equation with Boundary Condition,Eigenvalues of Differential Equation with Boundary Condition,,"Here is a problem from my homework assignment that I am struggling with: Consider the differential equation $\frac{d^2\phi}{dx^2}+\lambda\phi=0 $. Determine the eigenvalues $\lambda$ if $\phi$ satisfies the following boundary conditions: $\phi(a)=0$ $\phi(b)=0$ I have been able to successfully complete this problem with 6 other sets of boundary conditions, but this one is giving me trouble.  The question also states we only need to consider $\lambda>1$.  Here is what I have done so far: Because $\lambda>1$, we get our general solution of: $\phi = C_1cos(\sqrt\lambda x)+C_2sin(\sqrt\lambda x)$. We can plug in our first boundary condition and get: $\phi(a) = C_1cos(\sqrt\lambda a)+C_2sin(\sqrt\lambda a)=0$ Usually I would solve for $C_1$ or $C_2$ to plug into the second equation, so here I solve for $C_1$: $C_1=\frac{-C_2 sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}$ Now I substitute $C_1$ in and plug in the $b$ value to get: $\phi(a) = \frac{-C_2 sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}cos(\sqrt\lambda b)+C_2sin(\sqrt\lambda b)=0$ Now I will factor our $C_2$ to get: $\phi(a) = C_2[\frac{sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}cos(\sqrt\lambda b)+sin(\sqrt\lambda b)]=0$ Here we assume that $C_2$ cannot equal zero, or we would also get $C_1$ equal to zero, which would give us the trivial solution.  Thus we conclude that: $\frac{sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}cos(\sqrt\lambda b)+sin(\sqrt\lambda b)=0$ And that is as far as I can get.... The back of the book says the answer is: $\lambda = (\frac{n \pi}{b-a})^2$ Any help would be greatly appreciated! Thanks.","Here is a problem from my homework assignment that I am struggling with: Consider the differential equation $\frac{d^2\phi}{dx^2}+\lambda\phi=0 $. Determine the eigenvalues $\lambda$ if $\phi$ satisfies the following boundary conditions: $\phi(a)=0$ $\phi(b)=0$ I have been able to successfully complete this problem with 6 other sets of boundary conditions, but this one is giving me trouble.  The question also states we only need to consider $\lambda>1$.  Here is what I have done so far: Because $\lambda>1$, we get our general solution of: $\phi = C_1cos(\sqrt\lambda x)+C_2sin(\sqrt\lambda x)$. We can plug in our first boundary condition and get: $\phi(a) = C_1cos(\sqrt\lambda a)+C_2sin(\sqrt\lambda a)=0$ Usually I would solve for $C_1$ or $C_2$ to plug into the second equation, so here I solve for $C_1$: $C_1=\frac{-C_2 sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}$ Now I substitute $C_1$ in and plug in the $b$ value to get: $\phi(a) = \frac{-C_2 sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}cos(\sqrt\lambda b)+C_2sin(\sqrt\lambda b)=0$ Now I will factor our $C_2$ to get: $\phi(a) = C_2[\frac{sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}cos(\sqrt\lambda b)+sin(\sqrt\lambda b)]=0$ Here we assume that $C_2$ cannot equal zero, or we would also get $C_1$ equal to zero, which would give us the trivial solution.  Thus we conclude that: $\frac{sin(\sqrt\lambda a)}{cos\sqrt\lambda a)}cos(\sqrt\lambda b)+sin(\sqrt\lambda b)=0$ And that is as far as I can get.... The back of the book says the answer is: $\lambda = (\frac{n \pi}{b-a})^2$ Any help would be greatly appreciated! Thanks.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'boundary-value-problem']"
22,Solving the functional equation $f(x)=f\left(\frac{x}{2}\right)+\frac{x}{2}\cdot f'(x)$,Solving the functional equation,f(x)=f\left(\frac{x}{2}\right)+\frac{x}{2}\cdot f'(x),"find all functiions $f:\mathbb{R}\to\mathbb{R}$ such that $f'$ exists and $$f(x)=f\left(\frac{x}{2}\right)+\frac{x}{2}\cdot f'(x),\forall x\in\mathbb{R}$$",find all functiions such that exists and,"f:\mathbb{R}\to\mathbb{R} f' f(x)=f\left(\frac{x}{2}\right)+\frac{x}{2}\cdot f'(x),\forall x\in\mathbb{R}","['ordinary-differential-equations', 'functional-equations']"
23,Software for numerical solution of a non-linear ODE system?,Software for numerical solution of a non-linear ODE system?,,"I have been given a nonlinear system of ODEs which has arisen out of a colleague's engineering research: $$\begin{array}{rcl} \dot{x}_0&=&x_1\\ \dot{x}_1&=&-\frac{\lambda}{(x_2)^n-k^2\lambda}x_0\\ \dot{x}_2&=&\frac{2nx_0x_1\lambda(x_2)^n}{2n(x_2)^{2n-1}-2n\lambda k^2(x_2)^{n-1}-n(n-1)(x_0)^2\lambda(x_2)^{n-1}} \end{array}$$ (I have values for $\lambda$, $k$ and $n$, and also some initial conditions). Basically all I want is an (open source) computer system to fling them at, and see what happens. I have so far tried Octave and its ""lsode"" command (no good; gave errors), Python with ""odeint"" from sympy.integrate (gave a solution); I need to test out a few others. I am far from being an expert (or even vaguely competent) at non-linear ODE systems, and I'm hoping for advice as to which system I can use with confidence to generate trustworthy numerical solutions.","I have been given a nonlinear system of ODEs which has arisen out of a colleague's engineering research: $$\begin{array}{rcl} \dot{x}_0&=&x_1\\ \dot{x}_1&=&-\frac{\lambda}{(x_2)^n-k^2\lambda}x_0\\ \dot{x}_2&=&\frac{2nx_0x_1\lambda(x_2)^n}{2n(x_2)^{2n-1}-2n\lambda k^2(x_2)^{n-1}-n(n-1)(x_0)^2\lambda(x_2)^{n-1}} \end{array}$$ (I have values for $\lambda$, $k$ and $n$, and also some initial conditions). Basically all I want is an (open source) computer system to fling them at, and see what happens. I have so far tried Octave and its ""lsode"" command (no good; gave errors), Python with ""odeint"" from sympy.integrate (gave a solution); I need to test out a few others. I am far from being an expert (or even vaguely competent) at non-linear ODE systems, and I'm hoping for advice as to which system I can use with confidence to generate trustworthy numerical solutions.",,"['ordinary-differential-equations', 'numerical-methods', 'computational-mathematics', 'nonlinear-system']"
24,Transformation nonlinear PDE to system of ODE,Transformation nonlinear PDE to system of ODE,,"How I can transform this PDE to system of ODE  $$a^2 u_{tt}-u_{xx}+ u \cdot u_x = 0$$ I trying to use the following transformation $p=u_{t}$ and $q=u_{x}$ then the equation be  $p_{t}- (1/ a^2)*q_{x} + (1/ a^2)*u*q = 0$ my question now how I can change the $u$ in last term, and what is the ordinary system equation for the original problem ?","How I can transform this PDE to system of ODE  $$a^2 u_{tt}-u_{xx}+ u \cdot u_x = 0$$ I trying to use the following transformation $p=u_{t}$ and $q=u_{x}$ then the equation be  $p_{t}- (1/ a^2)*q_{x} + (1/ a^2)*u*q = 0$ my question now how I can change the $u$ in last term, and what is the ordinary system equation for the original problem ?",,"['ordinary-differential-equations', 'partial-differential-equations']"
25,Textbook Recommendation: Topological Dynamics,Textbook Recommendation: Topological Dynamics,,"I need to take credits satisfying a topology requirement, and can structure it myself. My field of study is dynamical systems, can someone recommend a textbook that handles differential equations/dynamical systems from a topological point of view? Or is there another recommended field of study that would fill this?","I need to take credits satisfying a topology requirement, and can structure it myself. My field of study is dynamical systems, can someone recommend a textbook that handles differential equations/dynamical systems from a topological point of view? Or is there another recommended field of study that would fill this?",,"['ordinary-differential-equations', 'reference-request', 'soft-question', 'dynamical-systems', 'book-recommendation']"
26,Transforming Differential Equation to a Kummer's Equation,Transforming Differential Equation to a Kummer's Equation,,"I'm trying to transform an equation of the form $$ yw^{\prime\prime}(y) - [b - ay] w^\prime(y) - [d + ey]w(y) = 0 $$ into the form of a Kummer's or confluent hypergeometric differential equation: $$ y w^{\prime\prime}(y) + [f - y] w^\prime(y) + g w(y)  = 0 $$ I know it may have something to do with merging two of the singularities of the original equation, and maybe doing something with $y$ , making it $\frac{y}{b}$ and taking b to infinity, but I don't know and can't find the details for this process, and for my equation specifically. There's not too much difference between the two but just enough so that I can't get it. Thanks in advance for any help!","I'm trying to transform an equation of the form into the form of a Kummer's or confluent hypergeometric differential equation: I know it may have something to do with merging two of the singularities of the original equation, and maybe doing something with , making it and taking b to infinity, but I don't know and can't find the details for this process, and for my equation specifically. There's not too much difference between the two but just enough so that I can't get it. Thanks in advance for any help!", yw^{\prime\prime}(y) - [b - ay] w^\prime(y) - [d + ey]w(y) = 0   y w^{\prime\prime}(y) + [f - y] w^\prime(y) + g w(y)  = 0  y \frac{y}{b},"['ordinary-differential-equations', 'transformation', 'hypergeometric-function']"
27,Second order non-linear differential equation,Second order non-linear differential equation,,$ y_n'' -nx\frac{1}{\sqrt {y_n}}=0$ Is there any known method to solve such second order non-linear differential equation? What I tried to solve: $ 2y_n'y_n''= 2nx\frac{y_n'}{\sqrt {y_n}}$ $ y_n'^2= 4nx\sqrt {y_n}-4n\int\sqrt {y_n} dx$ After that I could not see any way how to proceed. Please advice what to do to solve the differential equation. Many thanks,$ y_n'' -nx\frac{1}{\sqrt {y_n}}=0$ Is there any known method to solve such second order non-linear differential equation? What I tried to solve: $ 2y_n'y_n''= 2nx\frac{y_n'}{\sqrt {y_n}}$ $ y_n'^2= 4nx\sqrt {y_n}-4n\int\sqrt {y_n} dx$ After that I could not see any way how to proceed. Please advice what to do to solve the differential equation. Many thanks,,['ordinary-differential-equations']
28,How to show that is unique asymptotic stable,How to show that is unique asymptotic stable,,"Based on this question: Poincaré-Bendixon show periodic solutions. Show that the system $x^{'}=x-y-x^{3}$ , $y^{'}=x+y-y^{3}$ has a unique periodic orbit on annulus $A:=\{(x,y): 1\le x^2+y^2\le 2\}$ and this periodic solution is asymptotic stable. I consider function $V(x,y)=(x^2+y^2)/2$ and show that $\dot V(x,y)>0$ on the circle $x^2+y^2=1$ and $\dot V\le 0$ on $x^2+y^2=2$ . So $A$ is positively invariant. By Poincare-Bendixson theorem, there is at least one periodic orbit in $A$ . Question: But how to apply one theorem to show that is unique asymptotic stable? I have the following theorem: Let $p(t)$ be a $T$ -periodic orbit of our system. If $\int_0^T div(f(p(t))dt<0$ , then $p(t)$ is orbitally asymptotic stable. I am confused how to find our $p(t)$ ? Use the polar coordinate transform $$ % \begin{align} %  x &= r \cos \theta \\ %  y &= r \sin \theta \\ % \end{align} % $$ which implies $$   r^{2} = x^{2} + y^{2} \tag{2} $$ Compute the derivative with respect to time for $(2)$ and use the definitions in $(1)$ . This leads to $$  \dot{r} = r - r^{3} \left( \cos^{4} \theta + \sin^{4} \theta \right)  = r  \left(   1 - \frac{1}{4} \left( 3 + \cos 4 \theta \right) r^{2}  \right) \tag{3} $$","Based on this question: Poincaré-Bendixon show periodic solutions. Show that the system , has a unique periodic orbit on annulus and this periodic solution is asymptotic stable. I consider function and show that on the circle and on . So is positively invariant. By Poincare-Bendixson theorem, there is at least one periodic orbit in . Question: But how to apply one theorem to show that is unique asymptotic stable? I have the following theorem: Let be a -periodic orbit of our system. If , then is orbitally asymptotic stable. I am confused how to find our ? Use the polar coordinate transform which implies Compute the derivative with respect to time for and use the definitions in . This leads to","x^{'}=x-y-x^{3} y^{'}=x+y-y^{3} A:=\{(x,y): 1\le x^2+y^2\le 2\} V(x,y)=(x^2+y^2)/2 \dot V(x,y)>0 x^2+y^2=1 \dot V\le 0 x^2+y^2=2 A A p(t) T \int_0^T div(f(p(t))dt<0 p(t) p(t) 
%
\begin{align}
%
 x &= r \cos \theta \\
%
 y &= r \sin \theta \\
%
\end{align}
%
 
  r^{2} = x^{2} + y^{2}
\tag{2}
 (2) (1) 
 \dot{r} = r - r^{3} \left( \cos^{4} \theta + \sin^{4} \theta \right) 
= r 
\left( 
 1 - \frac{1}{4} \left( 3 + \cos 4 \theta \right) r^{2} 
\right)
\tag{3}
","['ordinary-differential-equations', 'dynamical-systems']"
29,Has anyone looked at the ODE $x_{ssss} - x_{ss} x = c$ before?,Has anyone looked at the ODE  before?,x_{ssss} - x_{ss} x = c,"In my research, I've come across the following inhomogenous nonlinear ODE ( $c \geq 0$ is an undetermined constant): $$x_{ssss} - x_{ss} x = c$$ It has boundary conditions $$x_s(0) = x_{sss}(0) = x(1) = x_{ss}(1) = x_{sss}(1) = 0$$ I'm trying to understand if anyone has studied this equation, or if there are any analytical techniques I can use to better understand it. Thank you so much!","In my research, I've come across the following inhomogenous nonlinear ODE ( is an undetermined constant): It has boundary conditions I'm trying to understand if anyone has studied this equation, or if there are any analytical techniques I can use to better understand it. Thank you so much!",c \geq 0 x_{ssss} - x_{ss} x = c x_s(0) = x_{sss}(0) = x(1) = x_{ss}(1) = x_{sss}(1) = 0,"['ordinary-differential-equations', 'reference-request', 'nonlinear-analysis']"
30,Proving that simple harmonic motion and pendulum motion are periodic,Proving that simple harmonic motion and pendulum motion are periodic,,"Consider the equation of motion for simple harmonic motion, \begin{align} f'' + \omega^2 f = 0, \end{align} where $f: \mathbf{R} \rightarrow \mathbf{R}$ and $\omega \in \mathbf{R}$ . Suppose we have some initial conditions $f(0) = x_0$ , $f'(0) = v_0$ . Question : Without explicitly solving this equation , is it possible to prove that any solution is periodic? That is, can we show that if $f$ is a solution, then \begin{align} f(t + T) = f(t) \end{align} for some $T$ ? I also have the same question for the equation of motion of a simple pendulum, \begin{align} f'' + \omega^2 \sin f = 0. \end{align} Feel free to make any assumptions you want about the niceness of the solutions.","Consider the equation of motion for simple harmonic motion, where and . Suppose we have some initial conditions , . Question : Without explicitly solving this equation , is it possible to prove that any solution is periodic? That is, can we show that if is a solution, then for some ? I also have the same question for the equation of motion of a simple pendulum, Feel free to make any assumptions you want about the niceness of the solutions.","\begin{align}
f'' + \omega^2 f = 0,
\end{align} f: \mathbf{R} \rightarrow \mathbf{R} \omega \in \mathbf{R} f(0) = x_0 f'(0) = v_0 f \begin{align}
f(t + T) = f(t)
\end{align} T \begin{align}
f'' + \omega^2 \sin f = 0.
\end{align}","['ordinary-differential-equations', 'classical-mechanics', 'periodic-functions']"
31,Comparison of Fourier series and Laplace transform solutions to mass-spring system with Dirac-comb excitation,Comparison of Fourier series and Laplace transform solutions to mass-spring system with Dirac-comb excitation,,"I'm trying to solve the differential equation $$y''+y=\sum_{-\infty}^{\infty}\delta(x-n\pi)\qquad n \in \mathbb{Z}\tag{1}$$ with initial conditions $$y(0)=y'(0)=0\tag{2}$$ using both Laplace transform and Fourier series. I would like to see that the two methods give the same steady state time domain response. I know that the solution to this is the half-wave rectified sine, and I can get that using the Laplace transform. But I'm stuck when solving it using Fourier-series. As far as I know, the Fourier-series of the Dirac-comb with period $\pi$ is $$\sum_{-\infty}^{\infty}\delta(x-n\pi)=\dfrac{1}{\pi}+\dfrac{2}{\pi}\sum_{n=1}^\infty\, \cos(2nx).\tag{3}$$ I solved the ODE by separating this sum into $f_0(x)=\dfrac{1}{\pi}$ and then $f_n(x)=\dfrac{2}{\pi} \cos(2nx)$ , found the sum of the particular integral and the homogeneous solution for those, put them into a sum and got $$y(x)=\dfrac{1}{\pi}+\dfrac{2}{\pi}\sum_{n=1}^\infty \dfrac{1}{1-4n^2} \cos(2nx).\tag{4}$$ But the Fourier-series of the half wave rectified sine should be $$y(x)=\dfrac{1}{\pi}+\mathbf{\dfrac{1}{2}}\mathbf{\sin(x)}+\dfrac{2}{\pi}\sum_{n=1}^\infty \dfrac{1}{1-4n^2} \cos(2nx).\tag{5}$$ so I'm missing the sine term from my answer and I just can't see where it comes into the game when I solve the ODE the way I did. Any help with this would be much appreciated as I've been unable to resolve this despite many attempts.","I'm trying to solve the differential equation with initial conditions using both Laplace transform and Fourier series. I would like to see that the two methods give the same steady state time domain response. I know that the solution to this is the half-wave rectified sine, and I can get that using the Laplace transform. But I'm stuck when solving it using Fourier-series. As far as I know, the Fourier-series of the Dirac-comb with period is I solved the ODE by separating this sum into and then , found the sum of the particular integral and the homogeneous solution for those, put them into a sum and got But the Fourier-series of the half wave rectified sine should be so I'm missing the sine term from my answer and I just can't see where it comes into the game when I solve the ODE the way I did. Any help with this would be much appreciated as I've been unable to resolve this despite many attempts.","y''+y=\sum_{-\infty}^{\infty}\delta(x-n\pi)\qquad n \in \mathbb{Z}\tag{1} y(0)=y'(0)=0\tag{2} \pi \sum_{-\infty}^{\infty}\delta(x-n\pi)=\dfrac{1}{\pi}+\dfrac{2}{\pi}\sum_{n=1}^\infty\, \cos(2nx).\tag{3} f_0(x)=\dfrac{1}{\pi} f_n(x)=\dfrac{2}{\pi} \cos(2nx) y(x)=\dfrac{1}{\pi}+\dfrac{2}{\pi}\sum_{n=1}^\infty \dfrac{1}{1-4n^2} \cos(2nx).\tag{4} y(x)=\dfrac{1}{\pi}+\mathbf{\dfrac{1}{2}}\mathbf{\sin(x)}+\dfrac{2}{\pi}\sum_{n=1}^\infty \dfrac{1}{1-4n^2} \cos(2nx).\tag{5}","['ordinary-differential-equations', 'fourier-series', 'laplace-transform', 'dirac-delta']"
32,"Is this inequality valid, or did the authors make a mistake?","Is this inequality valid, or did the authors make a mistake?",,"My question relates to Boucheron et al. (1999) . I paraphrase from their proof of Theorem 6 (p.15-16). Let $\phi(u) = e^u - u - 1$ , then for all $\lambda\in \mathbb{R}$ the inequality, \begin{align} (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) \leq v \phi(-\lambda), \qquad (1)  \end{align} holds true. Now, considering equality in $(1)$ , we obtain a ordinary differential equation, \begin{align} (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) = v \phi(-\lambda), \qquad (2) \end{align} which has as a solution $\Phi_0 = v\phi(\lambda)$ . We want to show that $\Psi \leq \Psi_0$ . In fact if $\Psi_1 = \Psi - \Psi_0$ , inequality (1) can be written as, \begin{align} (1 - e^{-\lambda})\Psi_{1}^{\prime} - \Psi_1(\lambda) \leq 0 \qquad(3) \end{align} That $\Psi_0(\lambda) = v\phi(\lambda)$ is a solution to the posed ODE, is, I believe, clear. By noting that, \begin{align} \Psi_1(\lambda) &= \Psi(\lambda) - v(e^{\lambda} - \lambda - 1), \\ \Psi_{1}^{\prime}(\lambda) &= \Psi^{\prime}(\lambda) - v(e^{\lambda} - 1), \end{align} inequality $(3)$ follows from $(1)$ , since \begin{align} (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) - v (e^{-\lambda} + \lambda - 1) &= (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) + v(e^{\lambda} - \lambda - 1) - v(e^{-\lambda} + e^{\lambda} - 2),  \\[1em] &= (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - (\Psi(\lambda) - v\phi(\lambda)) - v(e^{\lambda} - 1)(1 - e^{-\lambda}),\\[1em] &=  (1 - e^{-\lambda})(\Psi^{\prime}(\lambda) - v(e^{\lambda}-1) - (\Psi(\lambda) - \Psi_{0}(\lambda)), \\[1em] &= (1 - e^{-\lambda})\Psi_{1}^{\prime}(\lambda)- \Psi_1(\lambda). \end{align} Continuing with the paper, Hence defining $f(\lambda) = \ln(e^{\lambda} - 1)$ and $g(\lambda) = e^{-f(\lambda)}\Psi_1(\lambda)$ we have, \begin{align} (1 - e^{-\lambda})[f^{\prime}(\lambda)g(\lambda) + g^{\prime}(\lambda)] - g(\lambda) \leq 0, \qquad (4) \end{align} which yields since $f^{\prime}(\lambda)(1 - e^{-\lambda}) = 1$ \begin{align} (1 - e^{-\lambda})g^{\prime}(\lambda) \leq 0. \qquad (5) \end{align} Now if I work this out for myself, i.e., substitute the functions $f(\lambda)$ and $g(\lambda)$ , I obtain, \begin{align} \Psi_1(\lambda) &= e^{f(\lambda)}g(\lambda), \\ \Psi_{1}^{\prime}(\lambda) &= e^{f(\lambda)}f^{\prime}(\lambda)g(\lambda) + g^{\prime}(\lambda)e^{f(\lambda)}. \end{align} Plugging this into $(3)$ , I obtain, \begin{align} (1-e^{-\lambda})e^{f(\lambda)}\Big(f^{\prime}g(\lambda) + g^{\prime}(\lambda)\Big) &- e^{f(\lambda)}g(\lambda) \leq 0 \\[1em] (1 -e^{-\lambda})(e^{\lambda} - 1)g^{\prime}(\lambda) &\leq 0. \qquad (6) \end{align} Now this extra factor is important, since the authors go on and argue that $g(\lambda)$ is non decreasing on $(-\infty,0)$ and non increasing on $(0,\infty)$ . This claim makes sense for the inequality $(5)$ , but not so much for $(6)$ . What am I missing? Is inequality $(5)$ valid? Or did the authors make a mistake? Edit Not surprisingly, there are multiple versions of this paper. In Boucheron et al. (1999) - v2 the authors caught their error. The corresponding proof is still a bit 'hand wavy', but I can follow it just fine. Addition is just for full disclosure.","My question relates to Boucheron et al. (1999) . I paraphrase from their proof of Theorem 6 (p.15-16). Let , then for all the inequality, holds true. Now, considering equality in , we obtain a ordinary differential equation, which has as a solution . We want to show that . In fact if , inequality (1) can be written as, That is a solution to the posed ODE, is, I believe, clear. By noting that, inequality follows from , since Continuing with the paper, Hence defining and we have, which yields since Now if I work this out for myself, i.e., substitute the functions and , I obtain, Plugging this into , I obtain, Now this extra factor is important, since the authors go on and argue that is non decreasing on and non increasing on . This claim makes sense for the inequality , but not so much for . What am I missing? Is inequality valid? Or did the authors make a mistake? Edit Not surprisingly, there are multiple versions of this paper. In Boucheron et al. (1999) - v2 the authors caught their error. The corresponding proof is still a bit 'hand wavy', but I can follow it just fine. Addition is just for full disclosure.","\phi(u) = e^u - u - 1 \lambda\in \mathbb{R} \begin{align}
(1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) \leq v \phi(-\lambda), \qquad (1) 
\end{align} (1) \begin{align}
(1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) = v \phi(-\lambda), \qquad (2)
\end{align} \Phi_0 = v\phi(\lambda) \Psi \leq \Psi_0 \Psi_1 = \Psi - \Psi_0 \begin{align}
(1 - e^{-\lambda})\Psi_{1}^{\prime} - \Psi_1(\lambda) \leq 0 \qquad(3)
\end{align} \Psi_0(\lambda) = v\phi(\lambda) \begin{align}
\Psi_1(\lambda) &= \Psi(\lambda) - v(e^{\lambda} - \lambda - 1), \\
\Psi_{1}^{\prime}(\lambda) &= \Psi^{\prime}(\lambda) - v(e^{\lambda} - 1),
\end{align} (3) (1) \begin{align}
(1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) - v (e^{-\lambda} + \lambda - 1) &= (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - \Psi(\lambda) + v(e^{\lambda} - \lambda - 1) - v(e^{-\lambda} + e^{\lambda} - 2),  \\[1em]
&= (1 - e^{-\lambda})\Psi^{\prime}(\lambda) - (\Psi(\lambda) - v\phi(\lambda)) - v(e^{\lambda} - 1)(1 - e^{-\lambda}),\\[1em]
&=  (1 - e^{-\lambda})(\Psi^{\prime}(\lambda) - v(e^{\lambda}-1) - (\Psi(\lambda) - \Psi_{0}(\lambda)), \\[1em]
&= (1 - e^{-\lambda})\Psi_{1}^{\prime}(\lambda)- \Psi_1(\lambda).
\end{align} f(\lambda) = \ln(e^{\lambda} - 1) g(\lambda) = e^{-f(\lambda)}\Psi_1(\lambda) \begin{align}
(1 - e^{-\lambda})[f^{\prime}(\lambda)g(\lambda) + g^{\prime}(\lambda)] - g(\lambda) \leq 0, \qquad (4)
\end{align} f^{\prime}(\lambda)(1 - e^{-\lambda}) = 1 \begin{align}
(1 - e^{-\lambda})g^{\prime}(\lambda) \leq 0. \qquad (5)
\end{align} f(\lambda) g(\lambda) \begin{align}
\Psi_1(\lambda) &= e^{f(\lambda)}g(\lambda), \\
\Psi_{1}^{\prime}(\lambda) &= e^{f(\lambda)}f^{\prime}(\lambda)g(\lambda) + g^{\prime}(\lambda)e^{f(\lambda)}.
\end{align} (3) \begin{align}
(1-e^{-\lambda})e^{f(\lambda)}\Big(f^{\prime}g(\lambda) + g^{\prime}(\lambda)\Big) &- e^{f(\lambda)}g(\lambda) \leq 0 \\[1em]
(1 -e^{-\lambda})(e^{\lambda} - 1)g^{\prime}(\lambda) &\leq 0. \qquad (6)
\end{align} g(\lambda) (-\infty,0) (0,\infty) (5) (6) (5)","['ordinary-differential-equations', 'inequality', 'solution-verification']"
33,Clairaut differential equations and elliptic discriminants,Clairaut differential equations and elliptic discriminants,,"I was solving this math.SE question , which was asking to solve the Clairaut differential equation $y= xy' - (y')^3$ . Just to have nicer signs, I then looked at the equivalent equation $$ y= xy' + (y')^3 .$$ The main trajectories of this differential equation are: the algebraic curve given by $27y^2 + 4x^3 = 0$ ; all the lines tangent to this curve. Many of us have already seen this curve somewhere: in fact, the discriminant of the elliptic curve (in Weierstrass form) $$ y^2 = x^3 +ax+b $$ is $$ \Delta = 4a^3+27b^2.$$ My question is: What is the relation between this Clairaut's differential equation and the discriminant of elliptic curves in Weierstrass form? In other words, I ask if this is just a coincidence, or  there is indeed some natural construction that relates the two. On one hand, it is easy to verify that the Clairaut equation is satisfied by the discriminant. It should be easy to verify this without using the discriminant formula, in this fashion: ""Suppose that an elliptic Weierstrass equation with parameters $(a,b)$ is singular (this is a reformulation of $\Delta =0$ ); then if we move the parameters in a curvy special direction, dictated by the Clairaut differential equation, then the cubic Weierstrass curve remains singular. "" This approach is a bit sketchy, because actually also the tangent lines to the zero-discriminant locus are solutions. Then, I guess that one should study what happens to the Weierstrass curve when the parameters move along these lines, and identify some ""quasi-invariant"" or ""property"" along these lines. This property should be, in some sense, some kind of generalization of the property ""the weierstrass curve is singular"". Then the Clairaut differential equation should be a differential equation valid ""along these quasi-invariants"". For completeness, here is the general equation of the tangent lines to the curve $4x^3+27y^2=0$ : $$y = mx+m^3,$$ where $m$ is some parameter. As a follow-up, it would be great if there were some relationship between Clairaut differential equations (or some other class of differential equations) and discriminants of families of algebraic cuves (or better, these ""generalized discriminants"" that include somehow the lines tangents to the ""zero-discriminant locus""). But this would be perhaps a separate question.","I was solving this math.SE question , which was asking to solve the Clairaut differential equation . Just to have nicer signs, I then looked at the equivalent equation The main trajectories of this differential equation are: the algebraic curve given by ; all the lines tangent to this curve. Many of us have already seen this curve somewhere: in fact, the discriminant of the elliptic curve (in Weierstrass form) is My question is: What is the relation between this Clairaut's differential equation and the discriminant of elliptic curves in Weierstrass form? In other words, I ask if this is just a coincidence, or  there is indeed some natural construction that relates the two. On one hand, it is easy to verify that the Clairaut equation is satisfied by the discriminant. It should be easy to verify this without using the discriminant formula, in this fashion: ""Suppose that an elliptic Weierstrass equation with parameters is singular (this is a reformulation of ); then if we move the parameters in a curvy special direction, dictated by the Clairaut differential equation, then the cubic Weierstrass curve remains singular. "" This approach is a bit sketchy, because actually also the tangent lines to the zero-discriminant locus are solutions. Then, I guess that one should study what happens to the Weierstrass curve when the parameters move along these lines, and identify some ""quasi-invariant"" or ""property"" along these lines. This property should be, in some sense, some kind of generalization of the property ""the weierstrass curve is singular"". Then the Clairaut differential equation should be a differential equation valid ""along these quasi-invariants"". For completeness, here is the general equation of the tangent lines to the curve : where is some parameter. As a follow-up, it would be great if there were some relationship between Clairaut differential equations (or some other class of differential equations) and discriminants of families of algebraic cuves (or better, these ""generalized discriminants"" that include somehow the lines tangents to the ""zero-discriminant locus""). But this would be perhaps a separate question.","y= xy' - (y')^3  y= xy' + (y')^3 . 27y^2 + 4x^3 = 0  y^2 = x^3 +ax+b   \Delta = 4a^3+27b^2. (a,b) \Delta =0 4x^3+27y^2=0 y = mx+m^3, m","['ordinary-differential-equations', 'algebraic-geometry', 'elliptic-curves', 'discriminant', 'geometric-invariant']"
34,ODE $y'+ x\sin( 2y) = x e^{-x^2} \cos^2 (y)$,ODE,y'+ x\sin( 2y) = x e^{-x^2} \cos^2 (y),I have the following ODE: $$y'+ x\sin (2y) = x e^{-x^2} \cos^2 (y)$$ I'm stuck trying to get it into a linear form. I've tried $\sin (2y) = \sin y \cos y $ and then dividing the ODE by $( \cos  y ) ^{-1} $ . This got me to nothing so I tried dividing by $( \sin y )^{-2} $ instead. I got stuck as well. I think I need to make a substitution but I don't know which one. Thanks.,I have the following ODE: I'm stuck trying to get it into a linear form. I've tried and then dividing the ODE by . This got me to nothing so I tried dividing by instead. I got stuck as well. I think I need to make a substitution but I don't know which one. Thanks.,y'+ x\sin (2y) = x e^{-x^2} \cos^2 (y) \sin (2y) = \sin y \cos y  ( \cos  y ) ^{-1}  ( \sin y )^{-2} ,['ordinary-differential-equations']
35,How to calculate the envelope of the trajectory of a double pendulum?,How to calculate the envelope of the trajectory of a double pendulum?,,"Consider a double pendulum : Background For the angles $\varphi_i$ and the momenta $p_i$ we have (with equal lengths $l=1$ , masses $m=1$ and gravitational constant $g=1$ ): $\dot{\varphi_1} = 6\frac{2p_1 - 3p_2\cos(\varphi_1 - \varphi_2)}{16 - 9\cos^2(\varphi_1 - \varphi_2)}$ $\dot{\varphi_2} = 6\frac{8p_2 - 3p_1\cos(\varphi_1 - \varphi_2)}{16 - 9\cos^2(\varphi_1 - \varphi_2)}$ $\dot{p_1} = -\frac{1}{2}\big( \dot{\varphi_1}\dot{\varphi_2} \sin(\varphi_1 - \varphi_2) +3\sin(\varphi_1)  \big)$ $\dot{p_2} = -\frac{1}{2}\big( -\dot{\varphi_1}\dot{\varphi_2} \sin(\varphi_1 - \varphi_2) +\sin(\varphi_1)  \big)$ To see the relations more clearly: $\dot{\varphi_1} = B(2p_1 + Ap_2)$ $\dot{\varphi_2} = B(8p_2 + Ap_1)$ $\dot{p_1} = -C + 3D$ $\dot{p_2} = +C + D $ with $A = -3\cos(\varphi_1 - \varphi_2)$ $B = 6/(16 - A^2)$ $C = \dot{\varphi_1}\dot{\varphi_2}\sin(\varphi_1 - \varphi_2)/2$ $D = -\sin(\varphi_1)/2$ Observations With initial angles $\varphi_1^0 = \varphi_2^0 = 0$ and different combinations of small values for $p_1^0$ , $p_2^0$ a number of intriguiung patterns can be observed when plotting the tractory of the tip of the pendulum: $p_1^0 = 1$ , $p_2^0 = 1$ $p_1^0 = 1, p_2^0 = -1$ $p_1^0 = 0$ , $p_2^0 = 1$ $p_1^0 = 0$ , $p_2^0 = 2$ $p_1^0 = 0$ , $p_2^0 = 3$ $p_1^0 = 0$ , $p_2^0 = 3.7$ $p_1^0 = 0$ , $p_2^0 = 4$ What these patterns have in common: The tip of the pendulum draws a curve which more or less slowly ""fills"" an area enclosed by a specific envelope , intermediately exhibiting seemingly regular patterns which inevitably eventually vanish. Questions Can the envelope be given in closed form, depending only on the two parameters $p_1^0, p_2^0$ ? Can the positions of the two inner cusps which can be seen clearly for $p_1=0$ , $p_2=2,3$ be given in closed form, depending only on the two parameters $p_1^0, p_2^0$ ? [The envelope for $p_1^0=0, p_2^0 = 1,2,\dots$ looks like a canoe whose bow and stern bend to each other, eventually amalgamating. Can anyone guess what's the explicit formula for this shape?]","Consider a double pendulum : Background For the angles and the momenta we have (with equal lengths , masses and gravitational constant ): To see the relations more clearly: with Observations With initial angles and different combinations of small values for , a number of intriguiung patterns can be observed when plotting the tractory of the tip of the pendulum: , , , , , , What these patterns have in common: The tip of the pendulum draws a curve which more or less slowly ""fills"" an area enclosed by a specific envelope , intermediately exhibiting seemingly regular patterns which inevitably eventually vanish. Questions Can the envelope be given in closed form, depending only on the two parameters ? Can the positions of the two inner cusps which can be seen clearly for , be given in closed form, depending only on the two parameters ? [The envelope for looks like a canoe whose bow and stern bend to each other, eventually amalgamating. Can anyone guess what's the explicit formula for this shape?]","\varphi_i p_i l=1 m=1 g=1 \dot{\varphi_1} = 6\frac{2p_1 - 3p_2\cos(\varphi_1 - \varphi_2)}{16 - 9\cos^2(\varphi_1 - \varphi_2)} \dot{\varphi_2} = 6\frac{8p_2 - 3p_1\cos(\varphi_1 - \varphi_2)}{16 - 9\cos^2(\varphi_1 - \varphi_2)} \dot{p_1} = -\frac{1}{2}\big( \dot{\varphi_1}\dot{\varphi_2} \sin(\varphi_1 - \varphi_2) +3\sin(\varphi_1)  \big) \dot{p_2} = -\frac{1}{2}\big( -\dot{\varphi_1}\dot{\varphi_2} \sin(\varphi_1 - \varphi_2) +\sin(\varphi_1)  \big) \dot{\varphi_1} = B(2p_1 + Ap_2) \dot{\varphi_2} = B(8p_2 + Ap_1) \dot{p_1} = -C + 3D \dot{p_2} = +C + D  A = -3\cos(\varphi_1 - \varphi_2) B = 6/(16 - A^2) C = \dot{\varphi_1}\dot{\varphi_2}\sin(\varphi_1 - \varphi_2)/2 D = -\sin(\varphi_1)/2 \varphi_1^0 = \varphi_2^0 = 0 p_1^0 p_2^0 p_1^0 = 1 p_2^0 = 1 p_1^0 = 1, p_2^0 = -1 p_1^0 = 0 p_2^0 = 1 p_1^0 = 0 p_2^0 = 2 p_1^0 = 0 p_2^0 = 3 p_1^0 = 0 p_2^0 = 3.7 p_1^0 = 0 p_2^0 = 4 p_1^0, p_2^0 p_1=0 p_2=2,3 p_1^0, p_2^0 p_1^0=0, p_2^0 = 1,2,\dots","['ordinary-differential-equations', 'mathematical-physics', 'classical-mechanics', 'ergodic-theory', 'chaos-theory']"
36,Teaching a differential equations course to computer science majors,Teaching a differential equations course to computer science majors,,"I am currently teaching an undergraduate course on elementary differential equations in a mixed class of natural science (physics, chemistry, biology, etc) and computer science majors. Since none of the students are mathematics majors, I wanted to tailorfit the course by providing a good balance between theory and applications. In the case of physics, chemistry and biology, the applications are readily available in most differential equations texts. Unfortunately, computer science majors seem to be left out, and when I was asked by students on the use of the course in their field, I was unable to provide a satisfactory answer. I did some search on Google, with a hunch on the existence of applications in the study of algorithms and machine learning. I quickly realized that I am entering unfamiliar territory since I have very little background on computer science myself. I found several research papers but no satisfactory text that uses the undergraduate level concepts directly, compared to, say, applications in physics such as spring vibration and escape velocity that directly use and illuminate the concepts. I think my question is already obvious without stating, but are there any applications of elementary differential equations to computer science that I can use in an undergraduate class? The applications I'm looking for should satisfy the following: Use linear, preferably first or second order ODEs. Do not require a lot of background to explain. Appropriate for inclusion in a written exam.","I am currently teaching an undergraduate course on elementary differential equations in a mixed class of natural science (physics, chemistry, biology, etc) and computer science majors. Since none of the students are mathematics majors, I wanted to tailorfit the course by providing a good balance between theory and applications. In the case of physics, chemistry and biology, the applications are readily available in most differential equations texts. Unfortunately, computer science majors seem to be left out, and when I was asked by students on the use of the course in their field, I was unable to provide a satisfactory answer. I did some search on Google, with a hunch on the existence of applications in the study of algorithms and machine learning. I quickly realized that I am entering unfamiliar territory since I have very little background on computer science myself. I found several research papers but no satisfactory text that uses the undergraduate level concepts directly, compared to, say, applications in physics such as spring vibration and escape velocity that directly use and illuminate the concepts. I think my question is already obvious without stating, but are there any applications of elementary differential equations to computer science that I can use in an undergraduate class? The applications I'm looking for should satisfy the following: Use linear, preferably first or second order ODEs. Do not require a lot of background to explain. Appropriate for inclusion in a written exam.",,"['ordinary-differential-equations', 'education']"
37,How to solve the ODE using Newton's method?,How to solve the ODE using Newton's method?,,"Let consider on $[0,20]$, the ODE : $y'(t)=y^2(t)-y^3(t)$ and $y(0)=1$. I start to use a backward Euler method using : $y_{n+1}=y_n+h(y_{n+1}^2-y_{n+1}^3)$ with $h>0$ given. I build $F:y_{n+1}\mapsto y_{n+1}-y_n-h(y_{n+1}^2-y_{n+1}^3)$ such that $F(y_{n+1})=0$. If $F'(y_{n+1})\neq 0$, I can use the Newton's method to determine $y_{n+1}$. Do I have to consider $y_{n+1}-\frac{F(y_{n+1})}{F'(y_{n+1})}=y_{n+1}-\frac{y_{n+1}-y_n-h(y_{n+1}^2-y_{n+1}^3)}{1-2hy_{n+1}+3hy_{n+1}^2}=\frac{y_n-hy_{n+1}^2+2hy_{n+1}^3}{1-2hy_{n+1}+3hy_{n+1}^2}$ ? Then how to continue ? Thanks in advance !","Let consider on $[0,20]$, the ODE : $y'(t)=y^2(t)-y^3(t)$ and $y(0)=1$. I start to use a backward Euler method using : $y_{n+1}=y_n+h(y_{n+1}^2-y_{n+1}^3)$ with $h>0$ given. I build $F:y_{n+1}\mapsto y_{n+1}-y_n-h(y_{n+1}^2-y_{n+1}^3)$ such that $F(y_{n+1})=0$. If $F'(y_{n+1})\neq 0$, I can use the Newton's method to determine $y_{n+1}$. Do I have to consider $y_{n+1}-\frac{F(y_{n+1})}{F'(y_{n+1})}=y_{n+1}-\frac{y_{n+1}-y_n-h(y_{n+1}^2-y_{n+1}^3)}{1-2hy_{n+1}+3hy_{n+1}^2}=\frac{y_n-hy_{n+1}^2+2hy_{n+1}^3}{1-2hy_{n+1}+3hy_{n+1}^2}$ ? Then how to continue ? Thanks in advance !",,"['ordinary-differential-equations', 'numerical-methods', 'newton-raphson']"
38,Continuous dependence of solutions to ODEs on parameters,Continuous dependence of solutions to ODEs on parameters,,"Let $f:V\rightarrow \mathbb{R}^n$ be locally Lipschitz ($V$ is a subset of $\mathbb{R}\times\mathbb{R}^m\times \mathbb{R}^n$). Suppose we have a function $x:[t_0,\beta[\times W\rightarrow \mathbb{R}^n$ differentiable in the first argument ($W$ is an open subset of $\mathbb{R}^m$, $\beta$ is finite) such that for every $(t,\overrightarrow{\alpha})\in [t_0,\beta[\times W$ we have: $$(t,\overrightarrow{\alpha},x(t,\overrightarrow{\alpha}))\in V$$ $$x_1(t,\overrightarrow{\alpha})=f(t,\overrightarrow{\alpha},x(t,\overrightarrow{\alpha}))$$ Here $x_1(t,\overrightarrow{\alpha})$ means partial derivative with respect to first argument. It is also given that the function $g:W\rightarrow \mathbb{R}^n$  given by $g(\overrightarrow{\alpha})=x(t_0,\overrightarrow{\alpha})$ is locally Lipschitz. Question: Does it follow that the function $x:[t_0,\beta[\times W\rightarrow\mathbb{R}^n$ is continuous ? I can only prove the conclusion if the hypotheses are strengthened to $f,g$ Lipschitz instead of just merely locally Lipschitz.I would still like to know the answer in the locally Lipschitz case. Thank you a lot.","Let $f:V\rightarrow \mathbb{R}^n$ be locally Lipschitz ($V$ is a subset of $\mathbb{R}\times\mathbb{R}^m\times \mathbb{R}^n$). Suppose we have a function $x:[t_0,\beta[\times W\rightarrow \mathbb{R}^n$ differentiable in the first argument ($W$ is an open subset of $\mathbb{R}^m$, $\beta$ is finite) such that for every $(t,\overrightarrow{\alpha})\in [t_0,\beta[\times W$ we have: $$(t,\overrightarrow{\alpha},x(t,\overrightarrow{\alpha}))\in V$$ $$x_1(t,\overrightarrow{\alpha})=f(t,\overrightarrow{\alpha},x(t,\overrightarrow{\alpha}))$$ Here $x_1(t,\overrightarrow{\alpha})$ means partial derivative with respect to first argument. It is also given that the function $g:W\rightarrow \mathbb{R}^n$  given by $g(\overrightarrow{\alpha})=x(t_0,\overrightarrow{\alpha})$ is locally Lipschitz. Question: Does it follow that the function $x:[t_0,\beta[\times W\rightarrow\mathbb{R}^n$ is continuous ? I can only prove the conclusion if the hypotheses are strengthened to $f,g$ Lipschitz instead of just merely locally Lipschitz.I would still like to know the answer in the locally Lipschitz case. Thank you a lot.",,"['ordinary-differential-equations', 'examples-counterexamples']"
39,Must the singular solution of ODE be the envelope of the family of general solutions?,Must the singular solution of ODE be the envelope of the family of general solutions?,,"Must the singular solution (if it exists) of ODE be the envelope of the family of general solutions? If a singular solution exists, is it sure that the C-discriminant method and p-discriminant method will not miss it? If not, how can we find a singular solution in general? I have read several books and webpages on finding the singular solutions of an ODE. Most of them say that if a singular solution exists, then it is the envelope of the family of general solutions, and therefore can be found using the C-discriminant, p-discriminant, or the simultaneous C-p method. However, as I work on some ODE problems, I find many examples where the singular solution is not an envelope of the family of general solutions. For example, $$x dy+2y dx=0$$ The general solution is $y=Cx^{-2}$. However, $x=0$ is also a solution to this ODE. Is it called a singular solution or particular solution (since it is not tangent to any integral curve)? In either case, I think I cannot obtain this solution using the C-discriminant method or p-discriminant method. Moreover, to my understanding, $x=0$ is not the envelope of the family of general solutions. In short, I'd like to know how I can obtain the singular solutions (and particular solutions which are not contained in the general solutions) in general. Thank you very much for answering.","Must the singular solution (if it exists) of ODE be the envelope of the family of general solutions? If a singular solution exists, is it sure that the C-discriminant method and p-discriminant method will not miss it? If not, how can we find a singular solution in general? I have read several books and webpages on finding the singular solutions of an ODE. Most of them say that if a singular solution exists, then it is the envelope of the family of general solutions, and therefore can be found using the C-discriminant, p-discriminant, or the simultaneous C-p method. However, as I work on some ODE problems, I find many examples where the singular solution is not an envelope of the family of general solutions. For example, $$x dy+2y dx=0$$ The general solution is $y=Cx^{-2}$. However, $x=0$ is also a solution to this ODE. Is it called a singular solution or particular solution (since it is not tangent to any integral curve)? In either case, I think I cannot obtain this solution using the C-discriminant method or p-discriminant method. Moreover, to my understanding, $x=0$ is not the envelope of the family of general solutions. In short, I'd like to know how I can obtain the singular solutions (and particular solutions which are not contained in the general solutions) in general. Thank you very much for answering.",,"['ordinary-differential-equations', 'singularity', 'singular-solution']"
40,What exactly is the geodesic flow?,What exactly is the geodesic flow?,,"I understand what a geodesic is, but I'm struggling to understand the meaning of the geodesic flow (as defined e.g. by Do Carmo, Riemannian Geometry , page 63). I can state my confusion in two different ways: 1) Do Carmo writes: Why does a geodesic $\gamma$ uniquely define a vector field on an open subset ? In other words, why are the values of the vector fields uniquely defined on those points that are not on the geodesic $\gamma$? 2) In local coordinates, the geodesic flow is defined as the solution to the ordinary differential equation $$ \tag{1}\frac{d^2 x_k}{dt^2}+\sum_{i,j}\Gamma^k_{ij}\frac{dx_i}{dt}\frac{dx_j}{dt}=0 $$ For the solution to be unique on $TM$ (or on an open subset), we need some boundary condition. The only boundary condition I can see is a given geodesic $\gamma(t)$. What are the boundary conditions for this ODE?","I understand what a geodesic is, but I'm struggling to understand the meaning of the geodesic flow (as defined e.g. by Do Carmo, Riemannian Geometry , page 63). I can state my confusion in two different ways: 1) Do Carmo writes: Why does a geodesic $\gamma$ uniquely define a vector field on an open subset ? In other words, why are the values of the vector fields uniquely defined on those points that are not on the geodesic $\gamma$? 2) In local coordinates, the geodesic flow is defined as the solution to the ordinary differential equation $$ \tag{1}\frac{d^2 x_k}{dt^2}+\sum_{i,j}\Gamma^k_{ij}\frac{dx_i}{dt}\frac{dx_j}{dt}=0 $$ For the solution to be unique on $TM$ (or on an open subset), we need some boundary condition. The only boundary condition I can see is a given geodesic $\gamma(t)$. What are the boundary conditions for this ODE?",,"['ordinary-differential-equations', 'geodesic']"
41,Double Exponential Function and Growth of the Human Population,Double Exponential Function and Growth of the Human Population,,"I read a paper: The Hyperexponential Growth of the Human Population on a Macrohistorical Scale ,Varfolomeyev SD, Gurevich KG, J Theor Biol. 2001 Oct 7;212(3):367-72 doi:10.1006/jtbi.2001.238 In the first part they show that the growth of the human population follows the differential equation: $$\frac{\mathrm dN}{\mathrm dt}=kN^2$$ The solution by simply integrating is $$N(t)=\frac{N_0}{1-kN_0t}$$ which is obviously divergent at $t = 1/kN_0$. Because they are not happy with a divergent function, they look for a better solution and introduce an accelerating function $J(t)$: \begin{equation}\frac{\mathrm dN}{\mathrm dt}=kJ(t)N(t)\tag 1\end{equation} Now they say that if we assume that $J(t)=J_0 e^{k_0t}$, the solution to 1 is $$N(t)=N_0 \exp\left(\frac{k_\textrm{app}}{k_0}\exp(k_0t)\right)\tag 2$$ where $k_0$ is a kinetic parameter of the accelerating function and $k_\textrm{app}$ is a complex parameter including $J_0$. Now they say that by Taylor approximation of $\exp(k_0t)$ to first order, they would get back the divergent function $$N(T) = \frac{N_0}{1-(k_\textrm{app}/k_0)-k_\textrm{app}t}\tag 3$$ My questions: 1) I can solve the differential equation as well, but I do not get $k_\textrm{app}$ to be complex. 2) I do not understand how they get from $(2)$ to $(3)$. How do they get to $(3)$ by approximating $\exp(k_0t) \approx 1+ k_0t$","I read a paper: The Hyperexponential Growth of the Human Population on a Macrohistorical Scale ,Varfolomeyev SD, Gurevich KG, J Theor Biol. 2001 Oct 7;212(3):367-72 doi:10.1006/jtbi.2001.238 In the first part they show that the growth of the human population follows the differential equation: $$\frac{\mathrm dN}{\mathrm dt}=kN^2$$ The solution by simply integrating is $$N(t)=\frac{N_0}{1-kN_0t}$$ which is obviously divergent at $t = 1/kN_0$. Because they are not happy with a divergent function, they look for a better solution and introduce an accelerating function $J(t)$: \begin{equation}\frac{\mathrm dN}{\mathrm dt}=kJ(t)N(t)\tag 1\end{equation} Now they say that if we assume that $J(t)=J_0 e^{k_0t}$, the solution to 1 is $$N(t)=N_0 \exp\left(\frac{k_\textrm{app}}{k_0}\exp(k_0t)\right)\tag 2$$ where $k_0$ is a kinetic parameter of the accelerating function and $k_\textrm{app}$ is a complex parameter including $J_0$. Now they say that by Taylor approximation of $\exp(k_0t)$ to first order, they would get back the divergent function $$N(T) = \frac{N_0}{1-(k_\textrm{app}/k_0)-k_\textrm{app}t}\tag 3$$ My questions: 1) I can solve the differential equation as well, but I do not get $k_\textrm{app}$ to be complex. 2) I do not understand how they get from $(2)$ to $(3)$. How do they get to $(3)$ by approximating $\exp(k_0t) \approx 1+ k_0t$",,['ordinary-differential-equations']
42,How do I solve this ODE: $\frac{d^N y}{dx^N}=x^L y^M$,How do I solve this ODE:,\frac{d^N y}{dx^N}=x^L y^M,"I have found a way to solve some unspecified equations of this type using Lie Theory: $$\frac{d^N y}{dx^N}=x^L y^M$$ The answer is $y=\bigg[\beta (\beta -1)(\beta-2)...(\beta-N+1)\bigg]^{\frac{1}{M-1}}x^{\beta}$ with $\beta = \frac{L+N}{1-M}$ I'll post the step-by-step solution in a few days, but I was wondering: aside from group theory, does anybody know another way to solve this problem? I promised I would post the solution, so here it is. In 1886 Sophus Lie stated that if a differential equation is invariant under the transformation of an infinite continuous group (Lie group), it can be rewritten as a function of the stabilizers of that group.  Lie also discovered that the relationships between the stabilizers could be found by taking their derivatives, and that these relationships, expressed in canonical form, could be used to solve the DEQ.  Dresner, Olver and others have noted that when the general solution cannot be derived, a special solution may often be found algebraically.  It was this that prompted me to propose and solve the above equation, and having solved it I began to wonder whether or not anybody else had heard of the technique. (I've used it before, but never on anything so generalized as this DEQ.) Allow primed values to be the transformed variables, such that $x'=\lambda x$ and $y'=\lambda^\beta y$.  The unit conversion for this group occurs when $\lambda =1$. (Values of x' form a subgroup of y', and proving that all values of y' satisfy group properties is a trivial exercise.) Then $dx'=\lambda dx$, $dy'=\lambda^\beta dy$.  The transformed DEQ is  $$\frac{d^N (\lambda^\beta y)}{(\lambda dx)^N}=(\lambda x)^L (\lambda^\beta y)^M$$ For invariance, $\lambda^{\beta -N}=\lambda^{L+\beta M}$ or $\beta = \frac{L+N}{1-M}$.  Once $\beta$ is found, invariance is assured. To find the stabilizers, first find the coefficients of the infinitesimal transformations and then use the method of characteristics.  For convenience, use Newton's notation such that $\dot{y}=\frac{dy}{dx}$, $\ddot{y}=\frac{d^2 y}{dx^2}$, etc.  Then $\dot{y}'=\lambda^{\beta -1}\dot{y}$, $\ddot{y}'=\lambda^{\beta -2} \ddot{y}$, $\dddot{y}'=\lambda^{\beta -3}\dddot{y}$ .... $\frac{\partial x'}{\partial \lambda}\big|_{\lambda_o =1}=x$, $\frac{\partial y'}{\partial \lambda}\big|_{\lambda_o =1}=\beta y$, $\frac{\partial \dot{y}'}{\partial \lambda}\big|_{\lambda_o =1}=(\beta-1) \dot{y}$, $\frac{\partial \ddot{y}'}{\partial \lambda}\big|_{\lambda_o =1}=(\beta-2) \ddot{y}$, $\frac{\partial \dddot{y}'}{\partial \lambda}\big|_{\lambda_o =1}=(\beta-3) \dddot{y}$, .....  $$d\lambda=\frac{dx}{x}=\frac{dy}{\beta y}=\frac{d\dot{y}}{(\beta -1) \dot{y}}=\frac{d\ddot{y}}{(\beta -2) \ddot{y}}=\frac{d\dddot{y}}{(\beta -3) \dddot{y}}=....$$ Integrating $\frac{dx}{x}=\frac{dy}{\beta y}$ produces $S_o =\frac{y}{x^\beta}$ where $S_o$ is a constant of integration, and constants are stabilizers for the group of polynomials . This is easily shown to be true by applying the group transformation to the stabilizer: it does not change. $$S_o'=\frac{y'}{(x')^\beta}=\frac{\lambda^\beta y}{\lambda^\beta x^\beta}=S_o$$ Likewise, $\frac{dx}{x}=\frac{d\dot{y}}{(\beta -1) \dot{y}} \rightarrow S_1 =\frac{\dot{y}}{x^{\beta -1}}$, another stabilizer.  The next one is $\frac{dx}{x}=\frac{d\ddot{y}}{(\beta -2) \ddot{y}} \rightarrow S_2 =\frac{\ddot{y}}{x^{\beta -2}}$.  According to Lie there are an infinite number of stabilizers following the same pattern, so we can assume that $$S_{N-1}=\frac{\overset{(N-1)\bullet}{y}}{x^{\beta -N+1}}$$ $$S_N=\frac{\overset{N \bullet}{y}}{x^{\beta -N}}$$ Unless you are in error, at this point the DEQ may be rewritten in stabilizer form.  This is accomplished by dividing the left side by $x^{\beta -N}$ and dividing the right side by $x^{L+\beta M}$ (remembering that $\beta -N = L+ \beta M)$.  $$\frac{\overset{N \bullet}{y}}{x^{\beta -N}}=\frac{x^L y^M}{x^L y^{\beta M}}=\big(\frac{y}{x^\beta}\big)^M$$ $$S_N=S_o^M$$ Lie asserts that by taking the derivatives of these stabilizers we can find the canonical relationships between them.  With a little thought it is easy to see that $$x\frac{dS_o}{dx}=S_1-\beta S_o$$ $$x\frac{dS_1}{dx}=S_2 -(\beta -1)S_1$$ $$....$$  $$x\frac{dS_{N-1}}{dx}=S_N - (\beta -N+1)S_{N-1}$$ Within the direction field of the DEQ, singularities, saddle points and the separatrices that connect them are found in places where the above derivatives are all set equal to zero.  It is here that the special solutions are found.  Thus, $$S_1=\beta S_o$$ $$S_2=(\beta -1)S_1=\beta (\beta -1)S_o$$ $$....$$ $$S_N=\beta (\beta -1)(\beta -2)...(\beta -N+1)S_o$$ Substitute this last equation into the transformed DEQ and with a little algebraic effort you end up at the given solution. I really enjoy using this method because it exploits a group property of polynomials themselves, of which DEQ's are a subgroup.  It also turns the traditional numerical ""shooting method"" into a back-of-the-envelope calculation. Sophus Lie was a genius.  I highly recommend his book ""Differential Invariants.""  Lie's style is a bit arcane and some modern rigor and nomenclature is missing, but the struggle is well worth it.  Many of his ideas have not been touched since his death.","I have found a way to solve some unspecified equations of this type using Lie Theory: $$\frac{d^N y}{dx^N}=x^L y^M$$ The answer is $y=\bigg[\beta (\beta -1)(\beta-2)...(\beta-N+1)\bigg]^{\frac{1}{M-1}}x^{\beta}$ with $\beta = \frac{L+N}{1-M}$ I'll post the step-by-step solution in a few days, but I was wondering: aside from group theory, does anybody know another way to solve this problem? I promised I would post the solution, so here it is. In 1886 Sophus Lie stated that if a differential equation is invariant under the transformation of an infinite continuous group (Lie group), it can be rewritten as a function of the stabilizers of that group.  Lie also discovered that the relationships between the stabilizers could be found by taking their derivatives, and that these relationships, expressed in canonical form, could be used to solve the DEQ.  Dresner, Olver and others have noted that when the general solution cannot be derived, a special solution may often be found algebraically.  It was this that prompted me to propose and solve the above equation, and having solved it I began to wonder whether or not anybody else had heard of the technique. (I've used it before, but never on anything so generalized as this DEQ.) Allow primed values to be the transformed variables, such that $x'=\lambda x$ and $y'=\lambda^\beta y$.  The unit conversion for this group occurs when $\lambda =1$. (Values of x' form a subgroup of y', and proving that all values of y' satisfy group properties is a trivial exercise.) Then $dx'=\lambda dx$, $dy'=\lambda^\beta dy$.  The transformed DEQ is  $$\frac{d^N (\lambda^\beta y)}{(\lambda dx)^N}=(\lambda x)^L (\lambda^\beta y)^M$$ For invariance, $\lambda^{\beta -N}=\lambda^{L+\beta M}$ or $\beta = \frac{L+N}{1-M}$.  Once $\beta$ is found, invariance is assured. To find the stabilizers, first find the coefficients of the infinitesimal transformations and then use the method of characteristics.  For convenience, use Newton's notation such that $\dot{y}=\frac{dy}{dx}$, $\ddot{y}=\frac{d^2 y}{dx^2}$, etc.  Then $\dot{y}'=\lambda^{\beta -1}\dot{y}$, $\ddot{y}'=\lambda^{\beta -2} \ddot{y}$, $\dddot{y}'=\lambda^{\beta -3}\dddot{y}$ .... $\frac{\partial x'}{\partial \lambda}\big|_{\lambda_o =1}=x$, $\frac{\partial y'}{\partial \lambda}\big|_{\lambda_o =1}=\beta y$, $\frac{\partial \dot{y}'}{\partial \lambda}\big|_{\lambda_o =1}=(\beta-1) \dot{y}$, $\frac{\partial \ddot{y}'}{\partial \lambda}\big|_{\lambda_o =1}=(\beta-2) \ddot{y}$, $\frac{\partial \dddot{y}'}{\partial \lambda}\big|_{\lambda_o =1}=(\beta-3) \dddot{y}$, .....  $$d\lambda=\frac{dx}{x}=\frac{dy}{\beta y}=\frac{d\dot{y}}{(\beta -1) \dot{y}}=\frac{d\ddot{y}}{(\beta -2) \ddot{y}}=\frac{d\dddot{y}}{(\beta -3) \dddot{y}}=....$$ Integrating $\frac{dx}{x}=\frac{dy}{\beta y}$ produces $S_o =\frac{y}{x^\beta}$ where $S_o$ is a constant of integration, and constants are stabilizers for the group of polynomials . This is easily shown to be true by applying the group transformation to the stabilizer: it does not change. $$S_o'=\frac{y'}{(x')^\beta}=\frac{\lambda^\beta y}{\lambda^\beta x^\beta}=S_o$$ Likewise, $\frac{dx}{x}=\frac{d\dot{y}}{(\beta -1) \dot{y}} \rightarrow S_1 =\frac{\dot{y}}{x^{\beta -1}}$, another stabilizer.  The next one is $\frac{dx}{x}=\frac{d\ddot{y}}{(\beta -2) \ddot{y}} \rightarrow S_2 =\frac{\ddot{y}}{x^{\beta -2}}$.  According to Lie there are an infinite number of stabilizers following the same pattern, so we can assume that $$S_{N-1}=\frac{\overset{(N-1)\bullet}{y}}{x^{\beta -N+1}}$$ $$S_N=\frac{\overset{N \bullet}{y}}{x^{\beta -N}}$$ Unless you are in error, at this point the DEQ may be rewritten in stabilizer form.  This is accomplished by dividing the left side by $x^{\beta -N}$ and dividing the right side by $x^{L+\beta M}$ (remembering that $\beta -N = L+ \beta M)$.  $$\frac{\overset{N \bullet}{y}}{x^{\beta -N}}=\frac{x^L y^M}{x^L y^{\beta M}}=\big(\frac{y}{x^\beta}\big)^M$$ $$S_N=S_o^M$$ Lie asserts that by taking the derivatives of these stabilizers we can find the canonical relationships between them.  With a little thought it is easy to see that $$x\frac{dS_o}{dx}=S_1-\beta S_o$$ $$x\frac{dS_1}{dx}=S_2 -(\beta -1)S_1$$ $$....$$  $$x\frac{dS_{N-1}}{dx}=S_N - (\beta -N+1)S_{N-1}$$ Within the direction field of the DEQ, singularities, saddle points and the separatrices that connect them are found in places where the above derivatives are all set equal to zero.  It is here that the special solutions are found.  Thus, $$S_1=\beta S_o$$ $$S_2=(\beta -1)S_1=\beta (\beta -1)S_o$$ $$....$$ $$S_N=\beta (\beta -1)(\beta -2)...(\beta -N+1)S_o$$ Substitute this last equation into the transformed DEQ and with a little algebraic effort you end up at the given solution. I really enjoy using this method because it exploits a group property of polynomials themselves, of which DEQ's are a subgroup.  It also turns the traditional numerical ""shooting method"" into a back-of-the-envelope calculation. Sophus Lie was a genius.  I highly recommend his book ""Differential Invariants.""  Lie's style is a bit arcane and some modern rigor and nomenclature is missing, but the struggle is well worth it.  Many of his ideas have not been touched since his death.",,"['ordinary-differential-equations', 'lie-algebras']"
43,How to this solve this differential equation?,How to this solve this differential equation?,,"$y'=\dfrac{1}{x^2+y^2}$ where $y=f(x)$ and $x$ lies in $[1,\infty)$ and $f(1)=1$ and it is is differentiable in that interval I don't know how to even proceed in this problem. Even the range of $y$ is sufficient","$y'=\dfrac{1}{x^2+y^2}$ where $y=f(x)$ and $x$ lies in $[1,\infty)$ and $f(1)=1$ and it is is differentiable in that interval I don't know how to even proceed in this problem. Even the range of $y$ is sufficient",,['ordinary-differential-equations']
44,Weird differential equation - Jacobi?,Weird differential equation - Jacobi?,,"In class we had differential equations of the type $$y'=\frac{\left(Ax+By \right)y+ \alpha x + \beta y}{\left(Ax+By \right) x+ax+by},$$ where $A,\alpha,a,B,\beta,b$ are constants. The names of the constants were chosen in a way, so that the 3 different constants for each variable look a bit an a and b. For example $$y'=\frac{(x-y)y-x-y}{(x-y)x+x+y}$$ I did not fully understand the method of solution. In class the professor called them ""Jacobi Differtial calculus"" (translated). I did not find anything suitable on the web, except Wolfram Mathworld http://mathworld.wolfram.com/JacobiDifferentialEquation.html There is at least a differential equation named Jacobi. But this does not seem to be the same as in my question. Any help? My question : Q1 What are DE's like those called? Q2 How to solve them?","In class we had differential equations of the type $$y'=\frac{\left(Ax+By \right)y+ \alpha x + \beta y}{\left(Ax+By \right) x+ax+by},$$ where $A,\alpha,a,B,\beta,b$ are constants. The names of the constants were chosen in a way, so that the 3 different constants for each variable look a bit an a and b. For example $$y'=\frac{(x-y)y-x-y}{(x-y)x+x+y}$$ I did not fully understand the method of solution. In class the professor called them ""Jacobi Differtial calculus"" (translated). I did not find anything suitable on the web, except Wolfram Mathworld http://mathworld.wolfram.com/JacobiDifferentialEquation.html There is at least a differential equation named Jacobi. But this does not seem to be the same as in my question. Any help? My question : Q1 What are DE's like those called? Q2 How to solve them?",,['ordinary-differential-equations']
45,"Numerical methods (for ODE/PDE) that could take approximate solutions/good initial guesses, and further refine it to an certain accuracy","Numerical methods (for ODE/PDE) that could take approximate solutions/good initial guesses, and further refine it to an certain accuracy",,"I am currently playing with an old analog computer, which could solve time-dependent ODE/PDEs pretty fast, without time-stepping; thus there is no convergence issues caused by time-stepping because of its computing nature. But the problem with analog computer's solutions is that they are not accurate due to physical limitations. I am very curious that: is there any numerical methods/solvers which can take analog computer's approximate solution (over the time domain) to further process it, and generate a more accurate solution?? Let me give an example of solving second order ODE describing the motion a mass-spring damper. The equation is the following: $$ x'' = -0.2\cdot x' - 0.4\cdot x;\quad x(0)=1, x'(0) =0;\quad t_{stop} = 60s. $$ To solve the above equation on an analog computer, we need to map the above equation to an electrical system. Usually an analog computer could perform several basic arithmetic operation in the continuous-time domain, e.g. addition, subtraction, multiplication, integration etc. The output of an integrator represent an state-variable of the ODE; the input of that integrator represent the corresponding first-order time derivative. By configuring the basic computing blocks in feedback loops, we could map the equation as the following: (I use Simulink) After you load the initial conditions onto the integrators, you can let the analog computer run and solve. If you measure the electrical signal at the output of integrator1, you will get the solution of $x(t)$ over the time domain: But, due to the physical limitations (e.g. electrical noise, offsets), the solution of $x(t)$ is not accurate. What I am looking for is a numerical method that can take the above solution of $x(t)$ by analog computer, e.g. the solutions $x(t=1s), x(t=2s), x(t=3s), x(t=4s)... x(t=60s)$, start from these approximate solution points and further refine these solution $x(t=1s), ... x(t=60s)$ to a much higher accuracy. (This second order ODE is just a simple case for illustration purpose; it happens to have analytic expression of solutions. The more general case would be nonlinear ODEs with no analytic solution.) Thanks in advance!! Any thoughts and suggestions are greatly welcome and appreciated!!","I am currently playing with an old analog computer, which could solve time-dependent ODE/PDEs pretty fast, without time-stepping; thus there is no convergence issues caused by time-stepping because of its computing nature. But the problem with analog computer's solutions is that they are not accurate due to physical limitations. I am very curious that: is there any numerical methods/solvers which can take analog computer's approximate solution (over the time domain) to further process it, and generate a more accurate solution?? Let me give an example of solving second order ODE describing the motion a mass-spring damper. The equation is the following: $$ x'' = -0.2\cdot x' - 0.4\cdot x;\quad x(0)=1, x'(0) =0;\quad t_{stop} = 60s. $$ To solve the above equation on an analog computer, we need to map the above equation to an electrical system. Usually an analog computer could perform several basic arithmetic operation in the continuous-time domain, e.g. addition, subtraction, multiplication, integration etc. The output of an integrator represent an state-variable of the ODE; the input of that integrator represent the corresponding first-order time derivative. By configuring the basic computing blocks in feedback loops, we could map the equation as the following: (I use Simulink) After you load the initial conditions onto the integrators, you can let the analog computer run and solve. If you measure the electrical signal at the output of integrator1, you will get the solution of $x(t)$ over the time domain: But, due to the physical limitations (e.g. electrical noise, offsets), the solution of $x(t)$ is not accurate. What I am looking for is a numerical method that can take the above solution of $x(t)$ by analog computer, e.g. the solutions $x(t=1s), x(t=2s), x(t=3s), x(t=4s)... x(t=60s)$, start from these approximate solution points and further refine these solution $x(t=1s), ... x(t=60s)$ to a much higher accuracy. (This second order ODE is just a simple case for illustration purpose; it happens to have analytic expression of solutions. The more general case would be nonlinear ODEs with no analytic solution.) Thanks in advance!! Any thoughts and suggestions are greatly welcome and appreciated!!",,"['ordinary-differential-equations', 'partial-differential-equations', 'convergence-divergence', 'numerical-methods', 'approximation']"
46,Equation for dye in pool,Equation for dye in pool,,"I recently began my first course in intro do ordinary differential equations. The textbook recommended for the class is ""Elementary Differential Equations and Boundary Value problems, 10th edition"" by Boyce and DiPrima. I also have the solutions manual, but not all questions are worked in it. There is one question near the start that I decided the work, and it wasn't in the back so I thought people here could share their input. I will write the problem, then explain what I have done so far. I am not sure if it is correct however, or if I am making some mistakes. Your pool containing $60\,000$ gallons of water is contaminated by $5\,\rm kg$ of a non-toxic dye. The filtering system can take water from the pool, remove the dye and return the clean water at a rate of $200\,\rm gal/min$. It then asks a variety of questions, such as, write an initial value problem, is the system capable of removing enough dye such that the concentration is less than $0.02\,\rm g/gal$ within $4$ hours, find the time $T$ at which is first is $0.02\,\rm g/gal$, and lastly the flow sufficient to achieve the concentration of $0.02\,\rm g/gal$ in $4$ hours. What I did so far; Let $q(t)$ be the amount of dye present at time $t$. The original concentration of the dye is $$q(0)=\frac{5\,\rm kg}{60\,000\,\rm gal}\quad\text{or}\quad q(0)=8.33\cdot 10^{-2}\,\rm g/gal$$ The filter is capable of removing the dye at $t=0$ of a rate calculated by the product of this initial concentration by $200\,\rm gal/min$, that is, $$16\frac{\rm g}{\rm min}$$ But after this I know I need to calculate it as $$\frac{dq}{dt}$$ I am not sure the best way, I'm thinking along the lines of, the volume of the pool is not changing so the flow of chemical out at time $t$ would be equal to $200\,\rm gal/min$ $(q(t)/60\,000\,\rm gal)$ I was thinking also that since no new dye is being added, it may just be something like $$8.33\cdot 10^{-2}-\left[\left(200\frac{\rm gal}{\rm min}\right)\left(\frac{q(t)}{60\,000\,\rm gal}\right)\right]$$ But I am having a bit of trouble formulating this. Is it on the right track at least? Thank you for reading and taking any time to help or respond.","I recently began my first course in intro do ordinary differential equations. The textbook recommended for the class is ""Elementary Differential Equations and Boundary Value problems, 10th edition"" by Boyce and DiPrima. I also have the solutions manual, but not all questions are worked in it. There is one question near the start that I decided the work, and it wasn't in the back so I thought people here could share their input. I will write the problem, then explain what I have done so far. I am not sure if it is correct however, or if I am making some mistakes. Your pool containing $60\,000$ gallons of water is contaminated by $5\,\rm kg$ of a non-toxic dye. The filtering system can take water from the pool, remove the dye and return the clean water at a rate of $200\,\rm gal/min$. It then asks a variety of questions, such as, write an initial value problem, is the system capable of removing enough dye such that the concentration is less than $0.02\,\rm g/gal$ within $4$ hours, find the time $T$ at which is first is $0.02\,\rm g/gal$, and lastly the flow sufficient to achieve the concentration of $0.02\,\rm g/gal$ in $4$ hours. What I did so far; Let $q(t)$ be the amount of dye present at time $t$. The original concentration of the dye is $$q(0)=\frac{5\,\rm kg}{60\,000\,\rm gal}\quad\text{or}\quad q(0)=8.33\cdot 10^{-2}\,\rm g/gal$$ The filter is capable of removing the dye at $t=0$ of a rate calculated by the product of this initial concentration by $200\,\rm gal/min$, that is, $$16\frac{\rm g}{\rm min}$$ But after this I know I need to calculate it as $$\frac{dq}{dt}$$ I am not sure the best way, I'm thinking along the lines of, the volume of the pool is not changing so the flow of chemical out at time $t$ would be equal to $200\,\rm gal/min$ $(q(t)/60\,000\,\rm gal)$ I was thinking also that since no new dye is being added, it may just be something like $$8.33\cdot 10^{-2}-\left[\left(200\frac{\rm gal}{\rm min}\right)\left(\frac{q(t)}{60\,000\,\rm gal}\right)\right]$$ But I am having a bit of trouble formulating this. Is it on the right track at least? Thank you for reading and taking any time to help or respond.",,['ordinary-differential-equations']
47,Why do we take the odd extension?,Why do we take the odd extension?,,"When we have the initial and boundary value problem $$u_{tt}(x,t)-c^2u_{xx}(x,t)=0, x>0, t>0 \\ u(0,t)=0 \\ u(x,0)=f(x), x \geq 0 \\ u_t(x,0)=g(x), x \geq 0$$ can we apply Green's theorem or does it have to stand that $x \in \mathbb{R}$ to use it?? Because in my notes they take the odd extension and I don't know why... Could you explain it to me?? $$$$ EDIT : In my notes they do it as followed: $$w_{tt}-c^2w_{xx}=0, x \in \mathbb{R}, t>0 \\ w(x,0)=f_{\text{odd}}(x), x \in \mathbb{R} \\ w_t(x,0)=g_{\text{odd}} (x), x \in \mathbb{R}$$ $$w(x,t)=\frac{1}{2}(f_{\text{odd}}(x-ct)+f_{\text{odd}}(x+ct))+\frac{1}{2c}\int_{x-ct}^{x+ct}g_{\text{odd}}(s)ds \\ w(0,t)=\frac{1}{2}(f_{\text{odd}}(-ct)+f_{\text{odd}}(ct))+\frac{1}{2c}\int_{-ct}^{ct}g_{\text{odd}}(s)ds=0$$ So, for $x>0, t>0$ $$u(x,t)=w(x,t)=\frac{1}{2}(f_{\text{odd}}(x-ct)+f_{\text{odd}}(x+ct))+\frac{1}{2c}\int_{x-ct}^{x+ct}g_{\text{odd}}(s)ds$$ $$u(x,t)=\left\{\begin{matrix} \frac{1}{2}{(f(x-ct)+f(x+ct))+\frac{1}{2c}{\int_{x-ct}^{x+ct}g(s)ds, \ \ x-ct \geq 0}}\\ \\ \frac{1}{2}(-f(ct-x)+f(x+ct))+\frac{1}{2c}\int_{ct-x}^{x+ct}g(s)ds \ \ \ \ \ \ \ \ \ \ \ \ \  \end{matrix}\right.$$ Why have we taken the odd extension although we solve $u$ and not $w$ ??","When we have the initial and boundary value problem $$u_{tt}(x,t)-c^2u_{xx}(x,t)=0, x>0, t>0 \\ u(0,t)=0 \\ u(x,0)=f(x), x \geq 0 \\ u_t(x,0)=g(x), x \geq 0$$ can we apply Green's theorem or does it have to stand that $x \in \mathbb{R}$ to use it?? Because in my notes they take the odd extension and I don't know why... Could you explain it to me?? $$$$ EDIT : In my notes they do it as followed: $$w_{tt}-c^2w_{xx}=0, x \in \mathbb{R}, t>0 \\ w(x,0)=f_{\text{odd}}(x), x \in \mathbb{R} \\ w_t(x,0)=g_{\text{odd}} (x), x \in \mathbb{R}$$ $$w(x,t)=\frac{1}{2}(f_{\text{odd}}(x-ct)+f_{\text{odd}}(x+ct))+\frac{1}{2c}\int_{x-ct}^{x+ct}g_{\text{odd}}(s)ds \\ w(0,t)=\frac{1}{2}(f_{\text{odd}}(-ct)+f_{\text{odd}}(ct))+\frac{1}{2c}\int_{-ct}^{ct}g_{\text{odd}}(s)ds=0$$ So, for $x>0, t>0$ $$u(x,t)=w(x,t)=\frac{1}{2}(f_{\text{odd}}(x-ct)+f_{\text{odd}}(x+ct))+\frac{1}{2c}\int_{x-ct}^{x+ct}g_{\text{odd}}(s)ds$$ $$u(x,t)=\left\{\begin{matrix} \frac{1}{2}{(f(x-ct)+f(x+ct))+\frac{1}{2c}{\int_{x-ct}^{x+ct}g(s)ds, \ \ x-ct \geq 0}}\\ \\ \frac{1}{2}(-f(ct-x)+f(x+ct))+\frac{1}{2c}\int_{ct-x}^{x+ct}g(s)ds \ \ \ \ \ \ \ \ \ \ \ \ \  \end{matrix}\right.$$ Why have we taken the odd extension although we solve $u$ and not $w$ ??",,"['ordinary-differential-equations', 'partial-differential-equations']"
48,Transforming a PDE given basis vectors,Transforming a PDE given basis vectors,,"I have a non-orthogonal coordinate system defined by $\mathbf x=\mathbf x(r,\beta,z)$, and so I can find the basis vectors as $$ \mathbf g_r=\frac{\partial \mathbf x}{\partial r},\quad\mathbf g_\beta=\frac{\partial \mathbf x}{\partial \beta},\quad \mathbf g_z=\frac{\partial \mathbf x}{\partial z},$$ where $\mathbf g_r$, $\mathbf g_\beta$, and $\mathbf g_z$ are functions of $r$ and $\beta$. In this coordinate system I have written a system of PDEs, for example $$\frac{\partial v}{\partial r}+\rho(r)\frac{\partial v}{\partial z}=0 $$ for some arbitrary known function $\rho(r)$. Now I want to replace the $\mathbf g_z$ coordinate direction with a new one, defined as $\mathbf g_n=\mathbf g_r\times\mathbf g_\beta$. I can write $\mathbf g_n=f(r)\mathbf g_r+g(r)\mathbf g_\beta+h(r)\mathbf g_z$, or $\mathbf g_z=\tilde f(r)\mathbf g_r+\tilde g(r)\mathbf g_\beta+\tilde h(r)\mathbf g_n$. This is where I'm unsure what to do next. Considering the equation for $\mathbf g_z$ in terms of the other directions, I've considered doing the following: $$\begin{align} \frac{\partial}{\partial z}&=\frac{\partial}{\partial r}\frac{\partial r}{\partial z}+\frac{\partial}{\partial \beta}\frac{\partial\beta}{\partial z}+\frac{\partial}{\partial n}\frac{\partial n}{\partial z} \\ &=\tilde f(r)\frac{\partial}{\partial r}+\tilde g(r)\frac{\partial}{\partial \beta}+\tilde h(r)\frac{\partial}{\partial n} \end{align}$$ however this feels wrong. I think I need to use equations for $r$, $\beta$, and $n$ in terms of $r$, $\beta$, and $z$ then do the chain rule for the derivatives. Given coordinate directions, $\mathbf g_r$, $\mathbf g_\beta$, and $\mathbf g_n$, how can I find these equations for the transformations of the variables in the PDE?","I have a non-orthogonal coordinate system defined by $\mathbf x=\mathbf x(r,\beta,z)$, and so I can find the basis vectors as $$ \mathbf g_r=\frac{\partial \mathbf x}{\partial r},\quad\mathbf g_\beta=\frac{\partial \mathbf x}{\partial \beta},\quad \mathbf g_z=\frac{\partial \mathbf x}{\partial z},$$ where $\mathbf g_r$, $\mathbf g_\beta$, and $\mathbf g_z$ are functions of $r$ and $\beta$. In this coordinate system I have written a system of PDEs, for example $$\frac{\partial v}{\partial r}+\rho(r)\frac{\partial v}{\partial z}=0 $$ for some arbitrary known function $\rho(r)$. Now I want to replace the $\mathbf g_z$ coordinate direction with a new one, defined as $\mathbf g_n=\mathbf g_r\times\mathbf g_\beta$. I can write $\mathbf g_n=f(r)\mathbf g_r+g(r)\mathbf g_\beta+h(r)\mathbf g_z$, or $\mathbf g_z=\tilde f(r)\mathbf g_r+\tilde g(r)\mathbf g_\beta+\tilde h(r)\mathbf g_n$. This is where I'm unsure what to do next. Considering the equation for $\mathbf g_z$ in terms of the other directions, I've considered doing the following: $$\begin{align} \frac{\partial}{\partial z}&=\frac{\partial}{\partial r}\frac{\partial r}{\partial z}+\frac{\partial}{\partial \beta}\frac{\partial\beta}{\partial z}+\frac{\partial}{\partial n}\frac{\partial n}{\partial z} \\ &=\tilde f(r)\frac{\partial}{\partial r}+\tilde g(r)\frac{\partial}{\partial \beta}+\tilde h(r)\frac{\partial}{\partial n} \end{align}$$ however this feels wrong. I think I need to use equations for $r$, $\beta$, and $n$ in terms of $r$, $\beta$, and $z$ then do the chain rule for the derivatives. Given coordinate directions, $\mathbf g_r$, $\mathbf g_\beta$, and $\mathbf g_n$, how can I find these equations for the transformations of the variables in the PDE?",,"['ordinary-differential-equations', 'partial-differential-equations', 'coordinate-systems']"
49,How to know if two power series solutions are linearly independent?,How to know if two power series solutions are linearly independent?,,"I'm currently studying power series to solve differential equations. I would like to know if there's a way to tell whether two solutions are linearly independent or not. I think evaluating the Wronskian would be a tough task because the series may not be expressed in $\sum$ notation. For instance if I'm working on a power series centered at a regular singular point, Frobenius theorem provides me a solution $y_1(x)$. If I were to find another one in the form $$y_2(x)=y_1(x)\int\frac{e^{-\int P(x)\,dx}}{y_1^2(x)}\,dx$$ I know that this solution is independent because I created it to be independent. However if I solve the system and find ""two"" solutions, how do I know if these solutions are  linearly independent?  I appreciate your thoughts.","I'm currently studying power series to solve differential equations. I would like to know if there's a way to tell whether two solutions are linearly independent or not. I think evaluating the Wronskian would be a tough task because the series may not be expressed in $\sum$ notation. For instance if I'm working on a power series centered at a regular singular point, Frobenius theorem provides me a solution $y_1(x)$. If I were to find another one in the form $$y_2(x)=y_1(x)\int\frac{e^{-\int P(x)\,dx}}{y_1^2(x)}\,dx$$ I know that this solution is independent because I created it to be independent. However if I solve the system and find ""two"" solutions, how do I know if these solutions are  linearly independent?  I appreciate your thoughts.",,"['ordinary-differential-equations', 'power-series']"
50,Finding out properties of this ODE system knowing only partial informations about it.,Finding out properties of this ODE system knowing only partial informations about it.,,"I was doing some preparation for my exam, and I found this interesting exercise (text is in italian). Let's consider the planar system \begin{cases} \dot{x} &= P(x,y) \\ \dot{y} &=Q(x,y) \end{cases} where $P,Q \in C^1(\mathbb{R}^2,\mathbb{R})$. Assume that every circle centered at the origin with integer radius is invariant . Let's denote with $\Lambda^+(x,y)$ the limit set of the point $(x,y)$. I have to answer the following questions: 1) Prove that $\Lambda^+(x,y) \neq \emptyset$ for every $(x,y) \in \mathbb{R}^2$. it's trivially true. If the point $(x,y)$ lies in one of the circle, it's entire orbit it's contained in it, and by compactness every sequences ($t_n \to \infty$) of points of the orbit admits a convergent subsequence. A similar argument works for the annuli between the circles. I'm confident with this answer 2) Determine for which points the set $\Lambda^+(x,y)$ can be computed explicitly. If the point is the origin, the circumferencewith radius $0$ is invariant, so the point is fixed, so $\Lambda^+(0,0)=(0,0)$. I can't say anything for the other points, because the other cycles can be a connected set composed of a finite number of fixed points together with homoclinic and heteroclinic orbits connecting the points. According to me, I can't say anything else 3) Give sufficient conditions s.t. the system doesn't have limit cycles. If the the system is Hamiltonian, then it can't have any limit cycles. If the system is a gradient system, I can't have any periodic orbit,in particular any limit cycle. if the system satisfy the hypothesis of the Bendixson-Dulac theorem then it can't have any periodic solution. I don't know any other condition and then I added (as a challenge:) ) this question 4) Study the properties of the orbits in the annuli. It's permitted to ad some hypothesis on the system. If the only critical points lies in the cycles, and the system is Hamiltonian, then using the Poincaré-Bendixson thm I can conclude that every point belongs to a periodic solution. I don't know how to deal with the case that fixed point could exists in the annuli So my questions are: Are my thoughts right? Is there a way to deal with fixed point inside the annuli? (I doubt that there can be fixed points, because of the Continue dependence from initial value. (I can't formalize very well this fact) A few words about this last question: if we consider the system (non hamiltonian) in polar coordinates \begin{cases} \dot{r} &= r \sin(r) \\ \dot{\theta} &= -\cos( r)  \end{cases} it has a phase portrait of this kind (modulo a reparametrization to have integer radii) In which the orbits don't have any cycles inside the annuli. So if I relax the hypothesis on being Hamiltonian, it is not true that I have cycles inside the annuli. Anyway I can't find an example with fixed point inside. NB Highlighted texts are my answers at the question.","I was doing some preparation for my exam, and I found this interesting exercise (text is in italian). Let's consider the planar system \begin{cases} \dot{x} &= P(x,y) \\ \dot{y} &=Q(x,y) \end{cases} where $P,Q \in C^1(\mathbb{R}^2,\mathbb{R})$. Assume that every circle centered at the origin with integer radius is invariant . Let's denote with $\Lambda^+(x,y)$ the limit set of the point $(x,y)$. I have to answer the following questions: 1) Prove that $\Lambda^+(x,y) \neq \emptyset$ for every $(x,y) \in \mathbb{R}^2$. it's trivially true. If the point $(x,y)$ lies in one of the circle, it's entire orbit it's contained in it, and by compactness every sequences ($t_n \to \infty$) of points of the orbit admits a convergent subsequence. A similar argument works for the annuli between the circles. I'm confident with this answer 2) Determine for which points the set $\Lambda^+(x,y)$ can be computed explicitly. If the point is the origin, the circumferencewith radius $0$ is invariant, so the point is fixed, so $\Lambda^+(0,0)=(0,0)$. I can't say anything for the other points, because the other cycles can be a connected set composed of a finite number of fixed points together with homoclinic and heteroclinic orbits connecting the points. According to me, I can't say anything else 3) Give sufficient conditions s.t. the system doesn't have limit cycles. If the the system is Hamiltonian, then it can't have any limit cycles. If the system is a gradient system, I can't have any periodic orbit,in particular any limit cycle. if the system satisfy the hypothesis of the Bendixson-Dulac theorem then it can't have any periodic solution. I don't know any other condition and then I added (as a challenge:) ) this question 4) Study the properties of the orbits in the annuli. It's permitted to ad some hypothesis on the system. If the only critical points lies in the cycles, and the system is Hamiltonian, then using the Poincaré-Bendixson thm I can conclude that every point belongs to a periodic solution. I don't know how to deal with the case that fixed point could exists in the annuli So my questions are: Are my thoughts right? Is there a way to deal with fixed point inside the annuli? (I doubt that there can be fixed points, because of the Continue dependence from initial value. (I can't formalize very well this fact) A few words about this last question: if we consider the system (non hamiltonian) in polar coordinates \begin{cases} \dot{r} &= r \sin(r) \\ \dot{\theta} &= -\cos( r)  \end{cases} it has a phase portrait of this kind (modulo a reparametrization to have integer radii) In which the orbits don't have any cycles inside the annuli. So if I relax the hypothesis on being Hamiltonian, it is not true that I have cycles inside the annuli. Anyway I can't find an example with fixed point inside. NB Highlighted texts are my answers at the question.",,['ordinary-differential-equations']
51,Prove that $x \equiv 0$ of $\dot{x}(t)=a(t)x$ is Uniformly Asymptotically Stable,Prove that  of  is Uniformly Asymptotically Stable,x \equiv 0 \dot{x}(t)=a(t)x,"I have a problem: Consider the scalar equation: $$\dot{x}(t)=a(t)x \tag{I}$$ where $a(t) \in C(\mathbb{R}^+)$. Prove that $x \equiv 0$ of $(I)$  is Uniformly Asymptotically   Stable iff $$\forall M>0, \exists T>0, \forall t_0 \ge 0:  \int_{t_0}^{t}a(s)\mathrm{d}s<-M, \forall t \ge t_0+T$$ Here's my sketch: The first, since the equation $\dot{x}(t)=a(t)x $, we have $$x(t)=x_0 \cdot \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right)$$ The second, $\forall M>0, \exists T>0, \forall t_0 \ge 0: \int_{t_0}^{t}a(s)\mathrm{d}s<-M \iff \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right) < e^{-M} $ Thus, $$\left|x(t) \right|=\left|x_0  \cdot \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right) \right| \le \left|x_0 \right|  \cdot \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right) \le  \left|x_0 \right| e^{-M}$$ Now, I have stuck when I'm trying to show that  $x \equiv 0$ of $(I)$ is Uniformly Stable and Uniformly convergent , we know $1/$ Uniformly convergent : $\forall \epsilon >0,\exists \delta_1>0, \exists T=T(\epsilon)>0, \text{s.t}: \|x(t_0)\|<\delta_1 \implies \|x(t)\|< \epsilon, \forall t \ge t_0 +T$. $2/$ Uniformly Stable : $\forall \epsilon >0,\exists \delta = \delta (\epsilon)>0, \text{s.t}: \|x(t_0)\|<\delta \implies \|x(t)\|< \epsilon, \forall t \ge t_0 \ge 0$. ====================================== I choose $\epsilon = \left|x_0 \right| e^{-M}>0$. But How do we find $T=T(\epsilon)=???$ Whence, I still have no solution. Any help will be appreciated! Thanks!","I have a problem: Consider the scalar equation: $$\dot{x}(t)=a(t)x \tag{I}$$ where $a(t) \in C(\mathbb{R}^+)$. Prove that $x \equiv 0$ of $(I)$  is Uniformly Asymptotically   Stable iff $$\forall M>0, \exists T>0, \forall t_0 \ge 0:  \int_{t_0}^{t}a(s)\mathrm{d}s<-M, \forall t \ge t_0+T$$ Here's my sketch: The first, since the equation $\dot{x}(t)=a(t)x $, we have $$x(t)=x_0 \cdot \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right)$$ The second, $\forall M>0, \exists T>0, \forall t_0 \ge 0: \int_{t_0}^{t}a(s)\mathrm{d}s<-M \iff \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right) < e^{-M} $ Thus, $$\left|x(t) \right|=\left|x_0  \cdot \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right) \right| \le \left|x_0 \right|  \cdot \exp  \left (\int_{t_0}^{t}a(s)\mathrm{d}s \right) \le  \left|x_0 \right| e^{-M}$$ Now, I have stuck when I'm trying to show that  $x \equiv 0$ of $(I)$ is Uniformly Stable and Uniformly convergent , we know $1/$ Uniformly convergent : $\forall \epsilon >0,\exists \delta_1>0, \exists T=T(\epsilon)>0, \text{s.t}: \|x(t_0)\|<\delta_1 \implies \|x(t)\|< \epsilon, \forall t \ge t_0 +T$. $2/$ Uniformly Stable : $\forall \epsilon >0,\exists \delta = \delta (\epsilon)>0, \text{s.t}: \|x(t_0)\|<\delta \implies \|x(t)\|< \epsilon, \forall t \ge t_0 \ge 0$. ====================================== I choose $\epsilon = \left|x_0 \right| e^{-M}>0$. But How do we find $T=T(\epsilon)=???$ Whence, I still have no solution. Any help will be appreciated! Thanks!",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
52,"Non-linear second order DE, with no x term in it","Non-linear second order DE, with no x term in it",,"Okay, I have a second order non linear de, which has no term containing the variable x. assuming $$ y = f(x) $$ , the equation is $$ y'' - Ay' = \cos{y} - B\sin{y} $$ I tried solving it by substituting $$ v = y' $$ and after that using the chain rule, to express it as a first order equation, but I don't seem to be getting anywhere.... Any help would be appreciated.","Okay, I have a second order non linear de, which has no term containing the variable x. assuming $$ y = f(x) $$ , the equation is $$ y'' - Ay' = \cos{y} - B\sin{y} $$ I tried solving it by substituting $$ v = y' $$ and after that using the chain rule, to express it as a first order equation, but I don't seem to be getting anywhere.... Any help would be appreciated.",,['ordinary-differential-equations']
53,Linearization of $ m \dfrac{dy^2}{dt^2} = u(t) - C_d \left( \dfrac{dy}{dt} \right)^2-mg $,Linearization of, m \dfrac{dy^2}{dt^2} = u(t) - C_d \left( \dfrac{dy}{dt} \right)^2-mg ,$$ m \frac{dy^2}{dt^2} = u(t) - C_d \left( \frac{dy}{dt} \right)^2-mg $$ where  $$\begin{align*} y(t)&=\text{missile altitude}\\ u(t)&= \text{force}\\ m&= \text{mass}\\ C_d&= \text{aerodynamic drag coefficient} \end{align*}$$ How do I linearize this beast? I want to obtain a transfer function so that I can create a PID controller for it.. I'm really stumped and could use some help.,$$ m \frac{dy^2}{dt^2} = u(t) - C_d \left( \frac{dy}{dt} \right)^2-mg $$ where  $$\begin{align*} y(t)&=\text{missile altitude}\\ u(t)&= \text{force}\\ m&= \text{mass}\\ C_d&= \text{aerodynamic drag coefficient} \end{align*}$$ How do I linearize this beast? I want to obtain a transfer function so that I can create a PID controller for it.. I'm really stumped and could use some help.,,"['ordinary-differential-equations', 'control-theory']"
54,Differential Equation $y'=x e^y + \cos x$,Differential Equation,y'=x e^y + \cos x,"I am new to differential equations. I tried to find a series solution for this equation, but I don't know how to solve it. \begin{equation} y'=x e^y + \cos x \\y(0)=1 \end{equation} Actually, the problem needs the coefficient of $x^3$ in Maclaurin series solution.","I am new to differential equations. I tried to find a series solution for this equation, but I don't know how to solve it. \begin{equation} y'=x e^y + \cos x \\y(0)=1 \end{equation} Actually, the problem needs the coefficient of $x^3$ in Maclaurin series solution.",,['ordinary-differential-equations']
55,Is there a (introductory) differential equations books from the infinitesimal perspective?,Is there a (introductory) differential equations books from the infinitesimal perspective?,,I am aware that there are books treating calculus solely from the infinitesimal perpective.  Are there books doing the same for ordinary/partial differential equations?  I would be interested in introductory to advanced books - if such books exist.,I am aware that there are books treating calculus solely from the infinitesimal perpective.  Are there books doing the same for ordinary/partial differential equations?  I would be interested in introductory to advanced books - if such books exist.,,"['ordinary-differential-equations', 'nonstandard-analysis']"
56,Does $f'(x) = f(\ln(x))$ have an easily expressed solution?,Does  have an easily expressed solution?,f'(x) = f(\ln(x)),"This is motivated by this question , where we can consider a bound of the form $f((n+1)!) - f(n!) \le 2f(n)$. To find a function with a similar growth rate, I wondered if there's a technique for solving differential equations of the form $$f'(x) = f(\ln(x))$$ or equivalently $$f'(e^y) = f(y).$$","This is motivated by this question , where we can consider a bound of the form $f((n+1)!) - f(n!) \le 2f(n)$. To find a function with a similar growth rate, I wondered if there's a technique for solving differential equations of the form $$f'(x) = f(\ln(x))$$ or equivalently $$f'(e^y) = f(y).$$",,['ordinary-differential-equations']
57,Solving $x f'(x) - f'(x)f(x) + \alpha x^{\alpha - 1} = 0$.,Solving .,x f'(x) - f'(x)f(x) + \alpha x^{\alpha - 1} = 0,"I have been trying to solve the differential equation $$x f'(x) - f'(x)f(x) + \alpha x^{\alpha - 1} = 0 $$ where $\alpha > 0$ is a fixed parameter, $f(0) = 0$ is the boundary condition, and the domain is $\mathbb{R}^+$ . In case of $\alpha = 1$ , Wolfram Alpha suggests that $$ f(x) = W(-e^{-x- 1}) + x+ 1$$ where $W$ is the product log function. However, Wolfram Alpha won't give me a solution for the general case. Is it possible to get a 'nice' expression for $f$ here? If not, can one still gain some qualitative insight into its properties?","I have been trying to solve the differential equation where is a fixed parameter, is the boundary condition, and the domain is . In case of , Wolfram Alpha suggests that where is the product log function. However, Wolfram Alpha won't give me a solution for the general case. Is it possible to get a 'nice' expression for here? If not, can one still gain some qualitative insight into its properties?",x f'(x) - f'(x)f(x) + \alpha x^{\alpha - 1} = 0  \alpha > 0 f(0) = 0 \mathbb{R}^+ \alpha = 1  f(x) = W(-e^{-x- 1}) + x+ 1 W f,['ordinary-differential-equations']
58,How to solve $Cx^2 y'' + xy'- y = 0$?,How to solve ?,Cx^2 y'' + xy'- y = 0,"How to solve the differential equation $Cx^2 y'' + xy'- y = 0$ , if $C$ is positive? Attempt: I use power series and let $$y = \sum_{n=0}^{\infty} a_n x^{n+c}$$ be the solution. Getting the first and second derivatives and substituting these into the given DE, I obtained $$\sum_{n=0}^{\infty} [C(n+c)(n+c-1)] a_n x^{n+c} + \sum_{n=0}^{\infty} (n+c) a_n x^{n+c} - \sum_{n=0}^{\infty} a_n x^{n+c} =0$$ What should we continue here? If this is not the optimal solution or approach, what would you recommend?","How to solve the differential equation , if is positive? Attempt: I use power series and let be the solution. Getting the first and second derivatives and substituting these into the given DE, I obtained What should we continue here? If this is not the optimal solution or approach, what would you recommend?",Cx^2 y'' + xy'- y = 0 C y = \sum_{n=0}^{\infty} a_n x^{n+c} \sum_{n=0}^{\infty} [C(n+c)(n+c-1)] a_n x^{n+c} + \sum_{n=0}^{\infty} (n+c) a_n x^{n+c} - \sum_{n=0}^{\infty} a_n x^{n+c} =0,['ordinary-differential-equations']
59,Optimal speed for approaching red light to maximize velocity with non-uniform probability,Optimal speed for approaching red light to maximize velocity with non-uniform probability,,"Problem statement When I cross red lights, my goal is to being going as fast as possible when the light turns green. I am at distance $D$ from a traffic light when it turns red. Let the time length of the red light be $t_{red}$ with probability density $ p$ on $[0,T_{max}]$ . My velocity $v(t)$ must be such that $\int_{0}^{T_{max}} v(t)dt \leq D$ . ( I can't cross the light while it's red.) Given any $C$ , I would like to find $v$ to maximize $E(v(t_{red}))$ given $Var(v(t_{red})) < C$ . My best solution so far What follows is my best family of solutions so far. I can show that it is optimal for $C=0$ and $C=\infty$ I would appreciate your help confirming or denying it is optimal for other values of $C$ . Let $F(a) = \int_0^{T_{max}}p^a(t)dt$ Note that $F(0)=T_{max}$ and $F(1)=1$ Let $v_a(t) = \frac{D}{F(a)}{p^a(t)}$ . We have $\int_0^{T_{max}}v_a(t)dt=D$ $E(v_a(t_{red}))=\frac{D}{F(a)}\int_0^{T_{max}}p^{a}pdt=D\frac{F(a+1)}{F(a)}$ $E(v_a(t_{red})^2) =\frac{D^2}{F(a)^2}\int_0^{T_{max}}(p^a)^2pdt= D^2\frac{F(2a+1)}{F(a)^2} $ $Var(v_a(t_{red}))= D^2\frac{F(2a+1)-F(a+1)^2}{F(a)^2}$ To get a feel for the expected value and variance of this solution, I plotted it out for a simple example: I can show you that this is optimal for at least two values of $a$ . $a=0$ if $a=0$ then $E(v_0(t_{red})) = D\frac{F(1)}{F(0)}=\frac{D}{T_{max}}$ . Also $Var(v_0(t_{red})) = D\frac{F(1)-F(1)}{F(0)}=0$ . This is optimal for $C=0$ since if $C=0$ then $v$ is constant, and must satisfy $\int_{0}^{T_{max}} v(t)dt \leq D$ while trying to maximize $E(v)$ . Side note: This is also the solution of the problem if you assumed uniform probability distribution from the get-go (which for practical purposes is a fair assumption). $a=\infty$ I can also show that this is optimal for $a=\infty$ in this case $C=\infty$ and this case this is simply the extremal Holder's equality Holder's inequality says: $D||p||_{\infty}=sup(\int_0^{T_{max}} vpdt: ||v||_1=D)$ and since $v_{\infty}$ essentially approaches a delta function around the supremum with weight $D$ we hit this upper bound at $v_{\infty}$ Side note: this solution is hilariously nonphysical. The driver waits for the moment where the light is most likely to turn and accelerates to an astronomic velocity towards the light for a split second and the halts to a dead stop. Thats what happens when we allow infinite variance. Question Of course I have just proved the two easiest cases. I would love to show that for any given $C$ the $v_a: Var(v_a) = C$ will maximize $E(v)$ for all $Var(v)\leq C$ What I've tried One method I have been able to think of for proving optimality is to show if $g: E(g(t))=0, \int_0^{T_{max}}g = 0$ then $E(v_a^2) \leq E((v_a+g)^2) \implies Var(v_a)\leq Var(v_a+g)$ . not much came out of this approach for me. One promising approach considering instead something like $E(v_ag) \geq E(v_a)$ and $ \int_0^{T_{max}}v_ag = \int_0^{T_{max}}v_a$ the problem turns into the following: if $  \int_0^{T_{max}}p^ag = \int_0^{T_{max}}p^a$ and $  \int_0^{T_{max}}p^{a+1}g \geq \int_0^{T_{max}}p^{a+1}$ then does $  \int_0^{T_{max}}p^{2a+1}g^2 \geq \int_0^{T_{max}}p^{2a+1}$ ? this is nice as: $p^{a}gp^{a+1}g= p^{2a+1}g^2$ and $p^{a}p^{a+2}= p^{2a+1}$ . So we almost want a rule like $\int a \geq \int b, \int c = \int d \implies \int ac \geq \int cd$ under our assumptions.","Problem statement When I cross red lights, my goal is to being going as fast as possible when the light turns green. I am at distance from a traffic light when it turns red. Let the time length of the red light be with probability density on . My velocity must be such that . ( I can't cross the light while it's red.) Given any , I would like to find to maximize given . My best solution so far What follows is my best family of solutions so far. I can show that it is optimal for and I would appreciate your help confirming or denying it is optimal for other values of . Let Note that and Let . We have To get a feel for the expected value and variance of this solution, I plotted it out for a simple example: I can show you that this is optimal for at least two values of . if then . Also . This is optimal for since if then is constant, and must satisfy while trying to maximize . Side note: This is also the solution of the problem if you assumed uniform probability distribution from the get-go (which for practical purposes is a fair assumption). I can also show that this is optimal for in this case and this case this is simply the extremal Holder's equality Holder's inequality says: and since essentially approaches a delta function around the supremum with weight we hit this upper bound at Side note: this solution is hilariously nonphysical. The driver waits for the moment where the light is most likely to turn and accelerates to an astronomic velocity towards the light for a split second and the halts to a dead stop. Thats what happens when we allow infinite variance. Question Of course I have just proved the two easiest cases. I would love to show that for any given the will maximize for all What I've tried One method I have been able to think of for proving optimality is to show if then . not much came out of this approach for me. One promising approach considering instead something like and the problem turns into the following: if and then does ? this is nice as: and . So we almost want a rule like under our assumptions.","D t_{red}  p [0,T_{max}] v(t) \int_{0}^{T_{max}} v(t)dt \leq D C v E(v(t_{red})) Var(v(t_{red})) < C C=0 C=\infty C F(a) = \int_0^{T_{max}}p^a(t)dt F(0)=T_{max} F(1)=1 v_a(t) = \frac{D}{F(a)}{p^a(t)} \int_0^{T_{max}}v_a(t)dt=D E(v_a(t_{red}))=\frac{D}{F(a)}\int_0^{T_{max}}p^{a}pdt=D\frac{F(a+1)}{F(a)} E(v_a(t_{red})^2) =\frac{D^2}{F(a)^2}\int_0^{T_{max}}(p^a)^2pdt= D^2\frac{F(2a+1)}{F(a)^2}  Var(v_a(t_{red}))= D^2\frac{F(2a+1)-F(a+1)^2}{F(a)^2} a a=0 a=0 E(v_0(t_{red})) = D\frac{F(1)}{F(0)}=\frac{D}{T_{max}} Var(v_0(t_{red})) = D\frac{F(1)-F(1)}{F(0)}=0 C=0 C=0 v \int_{0}^{T_{max}} v(t)dt \leq D E(v) a=\infty a=\infty C=\infty D||p||_{\infty}=sup(\int_0^{T_{max}} vpdt: ||v||_1=D) v_{\infty} D v_{\infty} C v_a: Var(v_a) = C E(v) Var(v)\leq C g: E(g(t))=0, \int_0^{T_{max}}g = 0 E(v_a^2) \leq E((v_a+g)^2) \implies Var(v_a)\leq Var(v_a+g) E(v_ag) \geq E(v_a)  \int_0^{T_{max}}v_ag = \int_0^{T_{max}}v_a   \int_0^{T_{max}}p^ag = \int_0^{T_{max}}p^a   \int_0^{T_{max}}p^{a+1}g \geq \int_0^{T_{max}}p^{a+1}   \int_0^{T_{max}}p^{2a+1}g^2 \geq \int_0^{T_{max}}p^{2a+1} p^{a}gp^{a+1}g= p^{2a+1}g^2 p^{a}p^{a+2}= p^{2a+1} \int a \geq \int b, \int c = \int d \implies \int ac \geq \int cd","['ordinary-differential-equations', 'statistics', 'optimization', 'optimal-control', 'holder-inequality']"
60,A curious identity involving the Appell hypergeometric series.,A curious identity involving the Appell hypergeometric series.,,"By playing around with exact solutions  to ordinary differential equations from 1 (here I mean with entry 1.4.5.31 in page 180 to be exact) we have discovered a following curious identity involving the Appell hypergeometric function $F_1() $ . Let $\xi_1,\xi_2,\xi_3,\xi_4,\xi_5 $ and $t$ be parameters. Then we define another parameters $\chi_1,\chi_2,\chi_3$ as follows: \begin{array}{lll} \chi_1 &=& -\frac{2 \left(\xi_1^2-\xi_4 \xi_1-\xi_5 \xi_1+\xi_4 \xi_5\right)} {(\xi_1-\xi_2) (\xi_1-\xi_3)}\\ \chi_2 &=& \frac{2 \left(\xi_2^2-\xi_4 \xi_2-\xi_5 \xi_2+\xi_4 \xi_5\right)}{(\xi_1-\xi_2)    (\xi_2-\xi_3)}\\ \chi_3 &=& \frac{2 \left(\xi_3^2-\xi_4 \xi_3-\xi_5 \xi_3+\xi_4 \xi_5\right)} {(\xi_1-\xi_3) (\xi_3-\xi_2)} \end{array} Then we have checked numerically that the following identity holds true: \begin{eqnarray} &&\frac{1}{t-\xi_2} \cdot F_1\left( 1, \chi_1+1,\chi_3+2,2, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) + \\ && \frac{(1+\chi_1)(\xi_1-\xi_2)}{2(t-\xi_2)^2} \cdot F_1\left( 2, \chi_1+2,\chi_3+2,3, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) + \\ && \frac{(2+\chi_3)(\xi_3-\xi_2)}{2(t-\xi_2)^2} \cdot F_1\left( 2, \chi_1+1,\chi_3+3,3, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) = \\ % &&\left(t-\xi_1\right)^{-\chi_1-1} \left(t-\xi_2\right)^{-\chi_2} \left(t-\xi_3\right)^{-\chi_3-2} \end{eqnarray} A snippet of Mathematica code that verifies that identity numerically is included below: t =.; tt =.; {xi[1], xi[2], xi[3], xi[4], xi[5]} =    RandomReal[{-1, 0}, 5, WorkingPrecision -> 50]; {chi[1], chi[2],     chi[3]} = {-((     2 (xi[1]^2 - xi[1] xi[4] - xi[1] xi[5] + xi[4] xi[5]))/((xi[1] -         xi[2]) (xi[1] - xi[3]))), (    2 (xi[2]^2 - xi[2] xi[4] - xi[2] xi[5] + xi[4] xi[5]))/((xi[1] -        xi[2]) (xi[2] - xi[3])), (    2 (xi[3]^2 - xi[3] xi[4] - xi[3] xi[5] + xi[4] xi[5]))/((xi[1] -        xi[3]) (-xi[2] + xi[3]))};  remm1 := 2 a ((1/(t - xi[2])          AppellF1[1, chi[1] + 1, chi[3] + 2,           2, (xi[1] - xi[2])/(t - xi[2]), (xi[3] - xi[2])/(t -             xi[2])] + ( (1 + chi[1]) (xi[1] - xi[2]))/(2 (t - xi[2])^2)          AppellF1[2, 2 + chi[1], 2 + chi[3], 3, (xi[1] - xi[2])/(          t - xi[2]), (-xi[2] + xi[3])/(          t - xi[2])] + ((2 + chi[3]) (-xi[2] + xi[3]))/(         2 (t - xi[2])^2)          AppellF1[2, 1 + chi[1], 3 + chi[3], 3, (xi[1] - xi[2])/(          t - xi[2]), (-xi[2] + xi[3])/(t - xi[2])] ) - (t -          xi[1])^(-chi[1] -         1) (t - xi[2])^-chi[2] (t - xi[3])^(-chi[3] - 2));  t = RandomReal[{0, 1}, WorkingPrecision -> 50]; {remm1} 1 Polyanin, A. D.; Zaitsev, Valentin F. , Handbook of exact solutions for ordinary differential equations., Boca Raton, FL: CRC Press. xxvi, 787 p. (2003). ZBL1015.34001 . The question is now can we prove that identity? I tried to use the recurrence relations from the Wikipedia website Appell hypergeometric function but they do not involve a source term that we have in the right hand side above. How do we go about proving that relation?","By playing around with exact solutions  to ordinary differential equations from 1 (here I mean with entry 1.4.5.31 in page 180 to be exact) we have discovered a following curious identity involving the Appell hypergeometric function . Let and be parameters. Then we define another parameters as follows: Then we have checked numerically that the following identity holds true: A snippet of Mathematica code that verifies that identity numerically is included below: t =.; tt =.; {xi[1], xi[2], xi[3], xi[4], xi[5]} =    RandomReal[{-1, 0}, 5, WorkingPrecision -> 50]; {chi[1], chi[2],     chi[3]} = {-((     2 (xi[1]^2 - xi[1] xi[4] - xi[1] xi[5] + xi[4] xi[5]))/((xi[1] -         xi[2]) (xi[1] - xi[3]))), (    2 (xi[2]^2 - xi[2] xi[4] - xi[2] xi[5] + xi[4] xi[5]))/((xi[1] -        xi[2]) (xi[2] - xi[3])), (    2 (xi[3]^2 - xi[3] xi[4] - xi[3] xi[5] + xi[4] xi[5]))/((xi[1] -        xi[3]) (-xi[2] + xi[3]))};  remm1 := 2 a ((1/(t - xi[2])          AppellF1[1, chi[1] + 1, chi[3] + 2,           2, (xi[1] - xi[2])/(t - xi[2]), (xi[3] - xi[2])/(t -             xi[2])] + ( (1 + chi[1]) (xi[1] - xi[2]))/(2 (t - xi[2])^2)          AppellF1[2, 2 + chi[1], 2 + chi[3], 3, (xi[1] - xi[2])/(          t - xi[2]), (-xi[2] + xi[3])/(          t - xi[2])] + ((2 + chi[3]) (-xi[2] + xi[3]))/(         2 (t - xi[2])^2)          AppellF1[2, 1 + chi[1], 3 + chi[3], 3, (xi[1] - xi[2])/(          t - xi[2]), (-xi[2] + xi[3])/(t - xi[2])] ) - (t -          xi[1])^(-chi[1] -         1) (t - xi[2])^-chi[2] (t - xi[3])^(-chi[3] - 2));  t = RandomReal[{0, 1}, WorkingPrecision -> 50]; {remm1} 1 Polyanin, A. D.; Zaitsev, Valentin F. , Handbook of exact solutions for ordinary differential equations., Boca Raton, FL: CRC Press. xxvi, 787 p. (2003). ZBL1015.34001 . The question is now can we prove that identity? I tried to use the recurrence relations from the Wikipedia website Appell hypergeometric function but they do not involve a source term that we have in the right hand side above. How do we go about proving that relation?","F_1()  \xi_1,\xi_2,\xi_3,\xi_4,\xi_5  t \chi_1,\chi_2,\chi_3 \begin{array}{lll}
\chi_1 &=& -\frac{2 \left(\xi_1^2-\xi_4 \xi_1-\xi_5 \xi_1+\xi_4 \xi_5\right)}
{(\xi_1-\xi_2) (\xi_1-\xi_3)}\\
\chi_2 &=& \frac{2 \left(\xi_2^2-\xi_4 \xi_2-\xi_5 \xi_2+\xi_4 \xi_5\right)}{(\xi_1-\xi_2)
   (\xi_2-\xi_3)}\\
\chi_3 &=& \frac{2 \left(\xi_3^2-\xi_4 \xi_3-\xi_5 \xi_3+\xi_4 \xi_5\right)}
{(\xi_1-\xi_3) (\xi_3-\xi_2)}
\end{array} \begin{eqnarray}
&&\frac{1}{t-\xi_2} \cdot F_1\left( 1, \chi_1+1,\chi_3+2,2, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) + \\
&& \frac{(1+\chi_1)(\xi_1-\xi_2)}{2(t-\xi_2)^2} \cdot F_1\left( 2, \chi_1+2,\chi_3+2,3, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) + \\
&& \frac{(2+\chi_3)(\xi_3-\xi_2)}{2(t-\xi_2)^2} \cdot F_1\left( 2, \chi_1+1,\chi_3+3,3, \frac{\xi_1-\xi_2}{t-\xi_2}, \frac{\xi_3-\xi_2}{t-\xi_2} \right) = \\
%
&&\left(t-\xi_1\right)^{-\chi_1-1}
\left(t-\xi_2\right)^{-\chi_2}
\left(t-\xi_3\right)^{-\chi_3-2}
\end{eqnarray}","['ordinary-differential-equations', 'hypergeometric-function']"
61,Asymptotic solution to $y'=\sin(xy)$,Asymptotic solution to,y'=\sin(xy),"I need to prove that the following equation $y'=\sin (xy)\tag{1}$ Has a solution $y\not\equiv 0$ such that $\lim\limits_{x\to+\infty}y=0$ . I was able to conclude that any solution of this equation (except for $y\equiv 0$ ) cannot cross the line $y=0$ , because by the theorem of existence and uniqueness for any $x_0\in\mathbb R$ there may only exist one solution satisfying starting condition $y(x_0)=0$ , which is $y\equiv 0$ . This means that, for example, if a solution has a point with a value greater than zero, then the entire solution is greater than zero and it is bounded from below. However, I am not sure where to go next. Even if I could prove that a solution is monotonically decreasing starting at some point, being bounded by zero doesn't guarantee that the limit equals zero. Any help would be appreciated","I need to prove that the following equation Has a solution such that . I was able to conclude that any solution of this equation (except for ) cannot cross the line , because by the theorem of existence and uniqueness for any there may only exist one solution satisfying starting condition , which is . This means that, for example, if a solution has a point with a value greater than zero, then the entire solution is greater than zero and it is bounded from below. However, I am not sure where to go next. Even if I could prove that a solution is monotonically decreasing starting at some point, being bounded by zero doesn't guarantee that the limit equals zero. Any help would be appreciated",y'=\sin (xy)\tag{1} y\not\equiv 0 \lim\limits_{x\to+\infty}y=0 y\equiv 0 y=0 x_0\in\mathbb R y(x_0)=0 y\equiv 0,"['ordinary-differential-equations', 'asymptotics']"
62,"Considering the surface $f(x,y)=x^2y$. We know that a parametrization it can be $X(u,v)=(u,v,u^2v)$.",Considering the surface . We know that a parametrization it can be .,"f(x,y)=x^2y X(u,v)=(u,v,u^2v)","QUESTION: Considering the surface $f(x,y)=x^2y$ . We know that a parametrization it can be $X(u,v)=(u,v,u^2v)$ . Find the asymptotic lines in $S$ . MY ATTEMPT: Let $\alpha:I\subset \mathbb{R}\rightarrow S$ be a curve in this surface, such that $\alpha(t)=X(u(t), v(t))$ and $\alpha '(t)=u'X_u+v'X_v$ . Thus, $\alpha $ is asymptotic if, and only if, $$e(u')^2+2fu'v'+ g(v')^2=0. \qquad (*)$$ Where $e=\frac{2v}{\sqrt{1+u^4+4u^2v 2}}$ , $f=\frac{2u}{\sqrt{1+u^4+ 4u^2v^2}}$ and $g=0$ . Replacing this in $(*)$ we can find that $e(u')^2+2fu'v'+ g(v')^2=0\iff u'=0 \; \text{or} \; u'v+2uv'=0$ . In the first case $u=\text{constant}$ . However I'm struggling to resolve the ODE $u'v+2uv'=0$ . Would you help me with this?","QUESTION: Considering the surface . We know that a parametrization it can be . Find the asymptotic lines in . MY ATTEMPT: Let be a curve in this surface, such that and . Thus, is asymptotic if, and only if, Where , and . Replacing this in we can find that . In the first case . However I'm struggling to resolve the ODE . Would you help me with this?","f(x,y)=x^2y X(u,v)=(u,v,u^2v) S \alpha:I\subset \mathbb{R}\rightarrow S \alpha(t)=X(u(t), v(t)) \alpha '(t)=u'X_u+v'X_v \alpha  e(u')^2+2fu'v'+ g(v')^2=0. \qquad (*) e=\frac{2v}{\sqrt{1+u^4+4u^2v 2}} f=\frac{2u}{\sqrt{1+u^4+ 4u^2v^2}} g=0 (*) e(u')^2+2fu'v'+ g(v')^2=0\iff u'=0 \; \text{or} \; u'v+2uv'=0 u=\text{constant} u'v+2uv'=0","['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'asymptotics']"
63,Can we combine this two Lyapunov functions (which imply local stability by separate) to conclude global stability?,Can we combine this two Lyapunov functions (which imply local stability by separate) to conclude global stability?,,"Let $x(t)\in\mathbb{R}^n$ constrained to a dynamical system $$ \dot{x}(t) = f(x(t)) $$ for some vector field $f:\mathbb{R}^n\to\mathbb{R}^n$ . Moreover, the dynamical system has a unique equilibrium point at the origin. My goal is to conclude that for this system (with a particular $f$ I have) the origin is globally asymptotically stable. So far, I have found two Lyapunov function candidates $V_1(x),V_2(x)$ which both are positive definite and radially unbounded. Moreover, I managed to show that they comply $\dot{V}_1(x)<0$ $\ \forall x : \|x\|<r_1$ $\dot{V}_2(x)<-c$ $\ \forall x : \|x\|>r_2$ and some constant $c>0$ $0<r_1<r_2$ If it was the case that $r_1>r_2>0$ instead of item 3), we would be done: global stability follows since item 2) ensures that $x$ reaches $\|x\|\leq r_2$ in finite time from any initial condition, and then one can use 1) to conclude $x$ reaches the origin asymptotically from here. (as pointed out in the comments, this argument requires nuance, nevertheless, the important part is the following) . However, from item 3) this is not the case. Thus, I have this disk $r_1<\|x\|<r_2$ in which my Lyapunov stability test is inconclusive. My hope is to show that trajectories $x(t)$ cannot stay in the disk $r_1<\|x\|<r_2$ forever but ultimately reach $\|x\|<r_1$ . My question is: from the information given here, do you think is possible to conclude global asymptotic stability either by constructing a new Lyapunov function from $V_1,V_2$ or by checking some condition on $f$ to conclude trajectories cannot stay in $r_1<\|x\|<r_2$ ? Indeed, there are many counterexamples in which this is not possible. But my question here is if there is a way to check if it is possible: what properties do you suggest me to check for $f$ . I'm looking for suggestions, ideas or relevant bibliography that you think can help me. EDIT: In case this problem is easier, or adds useful information, I can also show the following instead of the original items: $\dot{V}_1(x)<0$ $\ \forall x \in M_1 = \{x : V_1(x)<r_1'\}$ $\dot{V}_2(x)<-c$ $\ \forall x \in M_2=\{ V_2(x)>r_2'\}$ and some constant $c>0$ $D:=\mathbb{R}^n\setminus(M_1\cup M_2)\neq \emptyset$ We are left to show that there are no invariants in $D$ . EDIT 2: After some discussion in the comments, I think the main essence of my question, is more on what happens when the Lyapunov arguments are inconclusive on this non empty set $D$ (bounded, but not containing the origin). A concrete diagram for the state space of what $r_1,r_2,M_1,M_2,D$ looks in the case I am interested is the following:","Let constrained to a dynamical system for some vector field . Moreover, the dynamical system has a unique equilibrium point at the origin. My goal is to conclude that for this system (with a particular I have) the origin is globally asymptotically stable. So far, I have found two Lyapunov function candidates which both are positive definite and radially unbounded. Moreover, I managed to show that they comply and some constant If it was the case that instead of item 3), we would be done: global stability follows since item 2) ensures that reaches in finite time from any initial condition, and then one can use 1) to conclude reaches the origin asymptotically from here. (as pointed out in the comments, this argument requires nuance, nevertheless, the important part is the following) . However, from item 3) this is not the case. Thus, I have this disk in which my Lyapunov stability test is inconclusive. My hope is to show that trajectories cannot stay in the disk forever but ultimately reach . My question is: from the information given here, do you think is possible to conclude global asymptotic stability either by constructing a new Lyapunov function from or by checking some condition on to conclude trajectories cannot stay in ? Indeed, there are many counterexamples in which this is not possible. But my question here is if there is a way to check if it is possible: what properties do you suggest me to check for . I'm looking for suggestions, ideas or relevant bibliography that you think can help me. EDIT: In case this problem is easier, or adds useful information, I can also show the following instead of the original items: and some constant We are left to show that there are no invariants in . EDIT 2: After some discussion in the comments, I think the main essence of my question, is more on what happens when the Lyapunov arguments are inconclusive on this non empty set (bounded, but not containing the origin). A concrete diagram for the state space of what looks in the case I am interested is the following:","x(t)\in\mathbb{R}^n 
\dot{x}(t) = f(x(t))
 f:\mathbb{R}^n\to\mathbb{R}^n f V_1(x),V_2(x) \dot{V}_1(x)<0 \ \forall x : \|x\|<r_1 \dot{V}_2(x)<-c \ \forall x : \|x\|>r_2 c>0 0<r_1<r_2 r_1>r_2>0 x \|x\|\leq r_2 x r_1<\|x\|<r_2 x(t) r_1<\|x\|<r_2 \|x\|<r_1 V_1,V_2 f r_1<\|x\|<r_2 f \dot{V}_1(x)<0 \ \forall x \in M_1 = \{x : V_1(x)<r_1'\} \dot{V}_2(x)<-c \ \forall x \in M_2=\{ V_2(x)>r_2'\} c>0 D:=\mathbb{R}^n\setminus(M_1\cup M_2)\neq \emptyset D D r_1,r_2,M_1,M_2,D","['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'stability-theory', 'lyapunov-functions']"
64,Laplace Transform for Linear ODEs with Variable Coefficients,Laplace Transform for Linear ODEs with Variable Coefficients,,"If you take the Laplace Transform of the time-domain ODE $x^2 y'' + xy' - 9y = 0$ and do some algebra, you get the new frequency-domain ODE $s^2 Y'' + 3sY' - 8Y = 0$ .  If you then apply the same technique to this ODE, apparently dragging it one step further away from the time-domain, you end up with $r^2 𝖄'' + r𝖄' - 9𝖄 = 0$ . Surely it cannot be a coincidence that this is the original ODE up to new choices of variable names, right?  Why does this cyclic pattern occur, despite apparently moving further away from the time-domain into the ""hyperfrequency-domain""?  What is the right way to think about the domain movement? Also, how should we conceive of the impact this cyclic process would have on a non-homogeneous term, if the original ODE had one?  I can't really find such a term that is amenable to taking the Laplace Transform twice.","If you take the Laplace Transform of the time-domain ODE and do some algebra, you get the new frequency-domain ODE .  If you then apply the same technique to this ODE, apparently dragging it one step further away from the time-domain, you end up with . Surely it cannot be a coincidence that this is the original ODE up to new choices of variable names, right?  Why does this cyclic pattern occur, despite apparently moving further away from the time-domain into the ""hyperfrequency-domain""?  What is the right way to think about the domain movement? Also, how should we conceive of the impact this cyclic process would have on a non-homogeneous term, if the original ODE had one?  I can't really find such a term that is amenable to taking the Laplace Transform twice.",x^2 y'' + xy' - 9y = 0 s^2 Y'' + 3sY' - 8Y = 0 r^2 𝖄'' + r𝖄' - 9𝖄 = 0,"['ordinary-differential-equations', 'laplace-transform', 'domain-theory']"
65,I'm looking for references for generalized confluent hypergeometric differential equation,I'm looking for references for generalized confluent hypergeometric differential equation,,"According to wolfram , A generalization of the confluent hypergeometric differential equation is given by; $$y''+\left(\frac{2R}{x}+2F'+p\frac{H'}{H}-H'-\frac{H''}{H'}\right) y'+\left[\left(p\frac{H'}{H}-H'-\frac{H''}{H'}\right)\left(\frac{R}{x}+F'\right)+\frac{R(R-1)}{x^2}+\frac{2R}{x}F'+F''+(F')^2-\frac{q}{H}(H')^2 \right]y=0$$ Which has the solutions $y_1=x^{-R} e^{-F} M(q,p,H)$ and $y_2=x^{-R} e^{-F} O(q,p,H)$ , where $M(q,p,H)$ is the confluent hypergeometric function of the first kind and $O(q,p,H)$ is the confluent hypergeometric function of the second kind. Meanwhile, $R,F$ and $H$ are fucntions of $x$ . I tried to look on google for more details about this equation but i didn't find anything, can anyone here please give me more references about this particular equation? Like how it was deriven, the relation between the parameters $p$ and $q$ ..etc.","According to wolfram , A generalization of the confluent hypergeometric differential equation is given by; Which has the solutions and , where is the confluent hypergeometric function of the first kind and is the confluent hypergeometric function of the second kind. Meanwhile, and are fucntions of . I tried to look on google for more details about this equation but i didn't find anything, can anyone here please give me more references about this particular equation? Like how it was deriven, the relation between the parameters and ..etc.","y''+\left(\frac{2R}{x}+2F'+p\frac{H'}{H}-H'-\frac{H''}{H'}\right) y'+\left[\left(p\frac{H'}{H}-H'-\frac{H''}{H'}\right)\left(\frac{R}{x}+F'\right)+\frac{R(R-1)}{x^2}+\frac{2R}{x}F'+F''+(F')^2-\frac{q}{H}(H')^2 \right]y=0 y_1=x^{-R} e^{-F} M(q,p,H) y_2=x^{-R} e^{-F} O(q,p,H) M(q,p,H) O(q,p,H) R,F H x p q","['ordinary-differential-equations', 'analysis', 'reference-request', 'hypergeometric-function']"
66,How to obtain a solution for the following IBVP,How to obtain a solution for the following IBVP,,"I am trying to solve the following advection-diffusion equation for transient flow conditions for radial flow. The governing equation is as follows. $$\frac{\partial T}{\partial t} = \frac{\partial^2 T}{\partial x^2} + \frac{1-2v(t)}{x} \frac{\partial T}{\partial x}$$ $$\frac{\partial T}{\partial t} = \frac{\partial^2 T}{\partial x^2} + \frac{f(t)}{x} \frac{\partial T}{\partial x}$$ where $$f(t)=1 -2 v(t)$$ Initial condition $$T(x,t=0)=0$$ BCs. $$T(x=0,t)=1$$ $$\lim_{x \to \infty} T(x,t)=0$$ I have tried to solve the problem using the following solution procedure. Assume the solution takes the following form. $$T(x,t)=\left ( e^{-\frac{x^2}{4t}}\right) F(t)$$ The similarity variable $-\frac{x^2}{4t}$ is appropriate selection for solving diffusion equation for radial flow. The partial derivatives of $T(x,t)$ are as follows. $$\frac{\partial T}{\partial x} =-\frac{x}{2t}\left ( e^{-\frac{x^2}{4t}}\right) F(t)$$ $$\frac{\partial^2 T}{\partial x^2} =F(t)\left( -\frac{1}{2t}\left ( e^{-\frac{x^2}{4t}}\right) + \left( \frac{x}{2t} \right)^2        \left ( e^{-\frac{x^2}{4t}}\right) \right)  $$ $$\frac{\partial T}{\partial t} = \left (\left( \frac{x}{2t} \right)^2 e^{-\frac{x^2}{4t}}\right)F(t) +  \left ( e^{-\frac{x^2}{4t}}\right)\frac{\partial F(t)}{\partial t} $$ By substituting into the governing equation, the following ODE in $F(t)$ is obtained. $$\frac{dF(t)}{dt}=-\left ( \frac{1 + f(t)}{2t}\right)F(t)$$ The solution of the ODE is as follow. $$F(t) = \exp\left ( -\int_{0} ^{t} \left ( \frac{1 + f(u)}{2u}\right) \, du\right)$$ Finally, the solution of the governing is as follow. $$T(x,t) =\left ( e^{-\frac{x^2}{4t}}\right) \exp\left ( -\int_{0} ^{t} \left ( \frac{1 + f(u)}{2u}\right) \, du\right)$$ This solution is the same as that given in Handbook of Linear Partial Differential Equations for Engineers and Scientists - Page 367 (when $a = 1$ )( https://www.taylorfrancis.com/books/9780429166464 ). Unfortunately, this solution satisfies the initial condition as well as the outer BC, however it doesn't satisfy the inner BC. When $x$ is put equals to zero, the resulting solution will be as follows. $$IBC \rightarrow T(x = 0,t) = \exp\left ( -\int_{0} ^{t} \left ( \frac{1 + f(u)}{2u}\right) \, du\right) ≠ 1$$ I was thinking of how to use the given solution to obtain a solution that satisfies the governing equations, initial conditions, and all boundary conditions of my problem. The resulting solution seems to be a solution of the same problem, however with time dependent inner BC. Duhamel's integral can be used to obtain a solution for time-dependent BC problem given the corresponding solution for constant BC problem, however the problem here seems to the opposite. Can anyone give me a hint of how to proceed to obtain the solution that satisfies the inner BC?","I am trying to solve the following advection-diffusion equation for transient flow conditions for radial flow. The governing equation is as follows. where Initial condition BCs. I have tried to solve the problem using the following solution procedure. Assume the solution takes the following form. The similarity variable is appropriate selection for solving diffusion equation for radial flow. The partial derivatives of are as follows. By substituting into the governing equation, the following ODE in is obtained. The solution of the ODE is as follow. Finally, the solution of the governing is as follow. This solution is the same as that given in Handbook of Linear Partial Differential Equations for Engineers and Scientists - Page 367 (when )( https://www.taylorfrancis.com/books/9780429166464 ). Unfortunately, this solution satisfies the initial condition as well as the outer BC, however it doesn't satisfy the inner BC. When is put equals to zero, the resulting solution will be as follows. I was thinking of how to use the given solution to obtain a solution that satisfies the governing equations, initial conditions, and all boundary conditions of my problem. The resulting solution seems to be a solution of the same problem, however with time dependent inner BC. Duhamel's integral can be used to obtain a solution for time-dependent BC problem given the corresponding solution for constant BC problem, however the problem here seems to the opposite. Can anyone give me a hint of how to proceed to obtain the solution that satisfies the inner BC?","\frac{\partial T}{\partial t} = \frac{\partial^2 T}{\partial x^2} + \frac{1-2v(t)}{x} \frac{\partial T}{\partial x} \frac{\partial T}{\partial t} = \frac{\partial^2 T}{\partial x^2} + \frac{f(t)}{x} \frac{\partial T}{\partial x} f(t)=1 -2 v(t) T(x,t=0)=0 T(x=0,t)=1 \lim_{x \to \infty} T(x,t)=0 T(x,t)=\left ( e^{-\frac{x^2}{4t}}\right) F(t) -\frac{x^2}{4t} T(x,t) \frac{\partial T}{\partial x} =-\frac{x}{2t}\left ( e^{-\frac{x^2}{4t}}\right) F(t) \frac{\partial^2 T}{\partial x^2} =F(t)\left( -\frac{1}{2t}\left ( e^{-\frac{x^2}{4t}}\right) + \left( \frac{x}{2t} \right)^2        \left ( e^{-\frac{x^2}{4t}}\right) \right)   \frac{\partial T}{\partial t} = \left (\left( \frac{x}{2t} \right)^2 e^{-\frac{x^2}{4t}}\right)F(t) +  \left ( e^{-\frac{x^2}{4t}}\right)\frac{\partial F(t)}{\partial t}  F(t) \frac{dF(t)}{dt}=-\left ( \frac{1 + f(t)}{2t}\right)F(t) F(t) = \exp\left ( -\int_{0} ^{t} \left ( \frac{1 + f(u)}{2u}\right) \, du\right) T(x,t) =\left ( e^{-\frac{x^2}{4t}}\right) \exp\left ( -\int_{0} ^{t} \left ( \frac{1 + f(u)}{2u}\right) \, du\right) a = 1 x IBC \rightarrow T(x = 0,t) = \exp\left ( -\int_{0} ^{t} \left ( \frac{1 + f(u)}{2u}\right) \, du\right) ≠ 1","['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem', 'heat-equation', 'deconvolution']"
67,"Prove $\partial_t \int f(t,s) ds = \int \partial_t f(t,s)ds$ if $t \mapsto \partial_t f(t,s)$ exists almost everywhere for each $s$",Prove  if  exists almost everywhere for each,"\partial_t \int f(t,s) ds = \int \partial_t f(t,s)ds t \mapsto \partial_t f(t,s) s","Let $f(t,s)$ be a (jointly) continuous bounded function on $(a,b) \times (a,b) \subseteq \mathbb{R}^2$ . Suppose further than for each fixed $s \in (a,b)$ , the continuous function $t \mapsto f(t,s)$ is differentiable (Lebesgue) almost everywhere on $(a,b)$ and that $\partial_t f(t,s) \in L^1(a,b)_t$ . I would like to know whether the continuous function $t \mapsto \int_a^b f(t,s) ds$ is differentiable for almost all $t \in (a,b)$ with derivative equal to $\int_a^b \partial_t f(t, s) ds$ (note there is some subtlety here about showing $s \mapsto \partial_t f(t,s)$ is measurable for almost all $t$ ). The issue is that when looking at the quantity $$ \frac{1}{h} \int_a^b f(t+h,s) - f(t,s) ds,$$ we want to use a statement like ""for almost all $s$ , $\frac{f(t+h,s) - f(t,s)}{h}$ converges to $\partial_t f(t,s)$ as $h \to 0$ ,"" but the only thing we can assume is sort of the opposite (for each fixed $s$ , $\frac{f(t+h,s) - f(t,s)}{h} \to \partial_tf(t,s)$ for almost all $t$ ). Note that if each $t \mapsto f(t,s)$ is differentiable on all of $(a,b)$ , then by dominated convergence, there is no issue (apart from showing $s \mapsto \partial_t f(t,s)$ is measurable for each $t$ ). The reason I care about proving this proposition is because I am trying to establish the variation of parameters formula for Sturm Liouville problems having coefficients that are $L^1$ functions. So, in this situation, we can only expect our solutions to be differentiable almost everywhere. The function $f(t,s)$ is really the the primary fundamental matrix in disguise (i.e., for each $s$ , $t \mapsto f(t,s)$ is the solution of the ODE satisfying the initial condition $f(s,s)= 1$ ). Edit: It seems that the variation of parameters formula I was looking at (Theorem 1.3.1 in Zettl's book Sturm-Liouville Theory ) is not completely accurate. If you look at Theorem 3.1 of Coddington-Levinson, there is a simpler version of the formula which still solves the ODE (and therefore must be the unique solution) and in which the $t$ -dependence appears outside the integral only. So then there is no issue. It would still be interesting to know whether the above assertion is true, but I no longer need it for the application I had in mind.","Let be a (jointly) continuous bounded function on . Suppose further than for each fixed , the continuous function is differentiable (Lebesgue) almost everywhere on and that . I would like to know whether the continuous function is differentiable for almost all with derivative equal to (note there is some subtlety here about showing is measurable for almost all ). The issue is that when looking at the quantity we want to use a statement like ""for almost all , converges to as ,"" but the only thing we can assume is sort of the opposite (for each fixed , for almost all ). Note that if each is differentiable on all of , then by dominated convergence, there is no issue (apart from showing is measurable for each ). The reason I care about proving this proposition is because I am trying to establish the variation of parameters formula for Sturm Liouville problems having coefficients that are functions. So, in this situation, we can only expect our solutions to be differentiable almost everywhere. The function is really the the primary fundamental matrix in disguise (i.e., for each , is the solution of the ODE satisfying the initial condition ). Edit: It seems that the variation of parameters formula I was looking at (Theorem 1.3.1 in Zettl's book Sturm-Liouville Theory ) is not completely accurate. If you look at Theorem 3.1 of Coddington-Levinson, there is a simpler version of the formula which still solves the ODE (and therefore must be the unique solution) and in which the -dependence appears outside the integral only. So then there is no issue. It would still be interesting to know whether the above assertion is true, but I no longer need it for the application I had in mind.","f(t,s) (a,b) \times (a,b) \subseteq \mathbb{R}^2 s \in (a,b) t \mapsto f(t,s) (a,b) \partial_t f(t,s) \in L^1(a,b)_t t \mapsto \int_a^b f(t,s) ds t \in (a,b) \int_a^b \partial_t f(t, s) ds s \mapsto \partial_t f(t,s) t  \frac{1}{h} \int_a^b f(t+h,s) - f(t,s) ds, s \frac{f(t+h,s) - f(t,s)}{h} \partial_t f(t,s) h \to 0 s \frac{f(t+h,s) - f(t,s)}{h} \to \partial_tf(t,s) t t \mapsto f(t,s) (a,b) s \mapsto \partial_t f(t,s) t L^1 f(t,s) s t \mapsto f(t,s) f(s,s)= 1 t","['ordinary-differential-equations', 'measure-theory', 'lebesgue-integral']"
68,Solving non-linear ODEs,Solving non-linear ODEs,,"I am trying to solve a differential equation of the form: \begin{equation}x^2y''+2xy'+x^2e^{ay}=0\end{equation} This arises from calculating the electric potential of ions following the Boltzmann distribution attracted to a spherically symmetric electrode for those who are curious. Anyway, seeing as this is very similar to #33 from here , I made the substitutions $z=x^2e^{ay}$ and $w=xy'$. I then found \begin{equation}dz=2xe^{ay}dx+ax^2e^{ay}y'dx=(2+w)z\frac{dx}{x}\end{equation} \begin{equation}dw=y'dx+xy''dx\end{equation} I therefore got \begin{equation}\frac{dw}{dz}=w'=\frac{x^2y''+xy'}{(2+aw)z}\end{equation} Substituting this and the definition of $z$ into the original ODE gives \begin{equation}z(2+aw)w'+w+z=0\end{equation} I thought this would be easier to solve, as it is a 1st order ODE. However, it is non-linear and not separable so far as I can see. In the original from the link there is no 2 on the $y'$ term, which makes the transformed ODE separable. Can anyone confirm that what I did with the substitution was correct? Also does anyone have any other suggestions for solving either the original or transformed ODE analytically?","I am trying to solve a differential equation of the form: \begin{equation}x^2y''+2xy'+x^2e^{ay}=0\end{equation} This arises from calculating the electric potential of ions following the Boltzmann distribution attracted to a spherically symmetric electrode for those who are curious. Anyway, seeing as this is very similar to #33 from here , I made the substitutions $z=x^2e^{ay}$ and $w=xy'$. I then found \begin{equation}dz=2xe^{ay}dx+ax^2e^{ay}y'dx=(2+w)z\frac{dx}{x}\end{equation} \begin{equation}dw=y'dx+xy''dx\end{equation} I therefore got \begin{equation}\frac{dw}{dz}=w'=\frac{x^2y''+xy'}{(2+aw)z}\end{equation} Substituting this and the definition of $z$ into the original ODE gives \begin{equation}z(2+aw)w'+w+z=0\end{equation} I thought this would be easier to solve, as it is a 1st order ODE. However, it is non-linear and not separable so far as I can see. In the original from the link there is no 2 on the $y'$ term, which makes the transformed ODE separable. Can anyone confirm that what I did with the substitution was correct? Also does anyone have any other suggestions for solving either the original or transformed ODE analytically?",,"['ordinary-differential-equations', 'statistical-mechanics']"
69,Solve the differential equation $x^5\frac{d^2y}{dx^2}+3x^3\frac{dy}{dx}+(3-6x)x^2y=x^4+2x-5$,Solve the differential equation,x^5\frac{d^2y}{dx^2}+3x^3\frac{dy}{dx}+(3-6x)x^2y=x^4+2x-5,"Solve the differential equation $$x^5\frac{d^2y}{dx^2}+3x^3\frac{dy}{dx}+(3-6x)x^2y=x^4+2x-5$$ In the solution of the above the question the author is trying to convert it into an exact differential equation by multiplying $x^m$ and comparing the given equation to $$P_{0}y''+P_{1}y'+P_{2}y=\phi(x)$$ To make it exact it tried to $P_{2}-P_{1}'+P_{0}''=0$ how is he getting to this equation ? Also further on using this equation a value of $m$ was derived and substituted , then a first integral was made. My question is what exactly is first integral and how is it relevant in solving ordinary differential equation . If someone can guide me to a good source or link I would be thankful Also the method shown above is it a standard method of solving ODE ? if yes could you also please let me know the name / link of a source from where I can read this method.","Solve the differential equation $$x^5\frac{d^2y}{dx^2}+3x^3\frac{dy}{dx}+(3-6x)x^2y=x^4+2x-5$$ In the solution of the above the question the author is trying to convert it into an exact differential equation by multiplying $x^m$ and comparing the given equation to $$P_{0}y''+P_{1}y'+P_{2}y=\phi(x)$$ To make it exact it tried to $P_{2}-P_{1}'+P_{0}''=0$ how is he getting to this equation ? Also further on using this equation a value of $m$ was derived and substituted , then a first integral was made. My question is what exactly is first integral and how is it relevant in solving ordinary differential equation . If someone can guide me to a good source or link I would be thankful Also the method shown above is it a standard method of solving ODE ? if yes could you also please let me know the name / link of a source from where I can read this method.",,['ordinary-differential-equations']
70,Understanding solutions to the Mathieu Equation in the context of Classical Mechanics,Understanding solutions to the Mathieu Equation in the context of Classical Mechanics,,"I have have two questions concerning the following system in Classical Mechanics . I will list them here and then give some context in which I will note where the questions occur. The Questions (1) Is it the case that the Mathieu Characteristic Functions have a ""frequency"" half that of the parametric driving term? If so, are there conditions that must be met or is it a general fact? (2) Is there any way to facially distinguish between periodic and divergent solutions to this system? Why do small changes in the parameters of the Mathieu equation sometimes result in huge transitions in the solution space? The Problem Consider a simple pendulum whose hinge is attached to the end of a Hooke's Law spring hanging from the ceiling. Assume that the spring and the hinge of the pendulum are both massless and that the the hinge (and spring) is constrained to vertical motion only. The pendulum is free to swing in the plane. The acceleration due to gravity is $g$, the length of the pendulum is $l$, the mass of the pendulum bob is $m$, and the spring constant is $k$. Taking $\theta$ to be the angle with which the pendulum swings from the vertical and $D$ to be the displacement of the spring from its equilibrium length, this system has the following Lagrangian : $$ \mathscr L=\frac{1}{2}m\dot D^2+\frac{1}{2}ml^2\dot\theta^2+ml\sin(\theta)\dot D \dot\theta - \frac{1}{2}kD^2-mgD+mlg\cos(\theta) $$ Applying the Euler-Lagrange Equation for each dynamical variable yields the following equations of motion: $$ \ddot\theta=-\frac{\sin(\theta)}{l}\ddot D-\frac{\sin(\theta)}{l}g\\ \ddot D+l\cos(\theta)\dot\theta^2+l\sin(\theta)\ddot\theta=-\frac{k}{m}D-g $$ Substituting the first equation into the second to eliminate $\ddot\theta$ and applying the small angle approximation (ignoring all $\theta$ terms of quadratic order or higher) yields $$ \ddot D+l\dot\theta^2=-\frac{k}{m}D-g $$ We can eliminate the non-linear term by requiring $\dot\theta\ll\sqrt{\frac{g}{l}}$. The remaining equation is the simple harmonic motion equation with a constant driving force. Solving this equation with appropriate initial conditions yields $$ D(t)=\frac{mg}{k}\cos(\sqrt{\frac{k}{m}}t)-\frac{mg}{k} $$ Taking two derivatives and substituting this solution into the first differential equation above (still under the small angle approximation, of course) yields $$ \ddot\theta+(\frac{g}{l}-\frac{g}{l}\cos(\sqrt{\frac{k}{m}}t))\theta=0 $$ This is a slightly modified Mathieu Equation which produces the following even solution given appropriate initial conditions (i.e. $\theta(0)=\theta_0$) $$ \theta (t)=\theta_0 C(\frac{4mg}{kl},\frac{2mg}{kl},\sqrt{\frac{k}{4m}}t) $$ I have been told that the pendulum ought to naturally oscillate with a frequency half that of the spring's. The reasoning being that the pendulum passes through its equilibrium position (where the tension in the rod is greatest) twice in every period. This position should correspond to the maximum downward displacement of the spring since the hinge is massless. Assuming that what is meant by ""frequency"" is related to the pendulum's perceived, visual swing rather than a true mathematical repetition, then a little testing produces examples where this appears to be at least approximately true ( example 1 , example 2 ). However, I have heard that this halving of the frequency is closely related to Floquet's Theorem , but that doesn't seem right since it is easy to find examples that definitely don't have half the ""frequency"" if one does not restrict one's solutions too much ( example 3 ). Moreover, it is simple to find solutions which quickly diverge, and surprisingly, these solutions are not very different from bounded solutions ( example 4 , example 5 ). The Questions Re-stated Is it possible to identify bounded or divergent solutions simply by looking at their parameters? Is there a hard-and-fast relationship between the frequency of the spring and the ""frequency"" of the pendulum? How does this relationship relate to Floquet's Theorem?","I have have two questions concerning the following system in Classical Mechanics . I will list them here and then give some context in which I will note where the questions occur. The Questions (1) Is it the case that the Mathieu Characteristic Functions have a ""frequency"" half that of the parametric driving term? If so, are there conditions that must be met or is it a general fact? (2) Is there any way to facially distinguish between periodic and divergent solutions to this system? Why do small changes in the parameters of the Mathieu equation sometimes result in huge transitions in the solution space? The Problem Consider a simple pendulum whose hinge is attached to the end of a Hooke's Law spring hanging from the ceiling. Assume that the spring and the hinge of the pendulum are both massless and that the the hinge (and spring) is constrained to vertical motion only. The pendulum is free to swing in the plane. The acceleration due to gravity is $g$, the length of the pendulum is $l$, the mass of the pendulum bob is $m$, and the spring constant is $k$. Taking $\theta$ to be the angle with which the pendulum swings from the vertical and $D$ to be the displacement of the spring from its equilibrium length, this system has the following Lagrangian : $$ \mathscr L=\frac{1}{2}m\dot D^2+\frac{1}{2}ml^2\dot\theta^2+ml\sin(\theta)\dot D \dot\theta - \frac{1}{2}kD^2-mgD+mlg\cos(\theta) $$ Applying the Euler-Lagrange Equation for each dynamical variable yields the following equations of motion: $$ \ddot\theta=-\frac{\sin(\theta)}{l}\ddot D-\frac{\sin(\theta)}{l}g\\ \ddot D+l\cos(\theta)\dot\theta^2+l\sin(\theta)\ddot\theta=-\frac{k}{m}D-g $$ Substituting the first equation into the second to eliminate $\ddot\theta$ and applying the small angle approximation (ignoring all $\theta$ terms of quadratic order or higher) yields $$ \ddot D+l\dot\theta^2=-\frac{k}{m}D-g $$ We can eliminate the non-linear term by requiring $\dot\theta\ll\sqrt{\frac{g}{l}}$. The remaining equation is the simple harmonic motion equation with a constant driving force. Solving this equation with appropriate initial conditions yields $$ D(t)=\frac{mg}{k}\cos(\sqrt{\frac{k}{m}}t)-\frac{mg}{k} $$ Taking two derivatives and substituting this solution into the first differential equation above (still under the small angle approximation, of course) yields $$ \ddot\theta+(\frac{g}{l}-\frac{g}{l}\cos(\sqrt{\frac{k}{m}}t))\theta=0 $$ This is a slightly modified Mathieu Equation which produces the following even solution given appropriate initial conditions (i.e. $\theta(0)=\theta_0$) $$ \theta (t)=\theta_0 C(\frac{4mg}{kl},\frac{2mg}{kl},\sqrt{\frac{k}{4m}}t) $$ I have been told that the pendulum ought to naturally oscillate with a frequency half that of the spring's. The reasoning being that the pendulum passes through its equilibrium position (where the tension in the rod is greatest) twice in every period. This position should correspond to the maximum downward displacement of the spring since the hinge is massless. Assuming that what is meant by ""frequency"" is related to the pendulum's perceived, visual swing rather than a true mathematical repetition, then a little testing produces examples where this appears to be at least approximately true ( example 1 , example 2 ). However, I have heard that this halving of the frequency is closely related to Floquet's Theorem , but that doesn't seem right since it is easy to find examples that definitely don't have half the ""frequency"" if one does not restrict one's solutions too much ( example 3 ). Moreover, it is simple to find solutions which quickly diverge, and surprisingly, these solutions are not very different from bounded solutions ( example 4 , example 5 ). The Questions Re-stated Is it possible to identify bounded or divergent solutions simply by looking at their parameters? Is there a hard-and-fast relationship between the frequency of the spring and the ""frequency"" of the pendulum? How does this relationship relate to Floquet's Theorem?",,"['ordinary-differential-equations', 'special-functions', 'classical-mechanics', 'periodic-functions', 'euler-lagrange-equation']"
71,"Does there exist continuous functions $P$ and $Q$ on [0,1]","Does there exist continuous functions  and  on [0,1]",P Q,"Does there exist continuous functions  $P$ and $Q$ on $[0,1]$ such that $y(t)=sin(t^2)$  is a solution to $y''+Py'+Qy=0$ on $[\frac1n,1]$ for all $n\geq1$? I find $y'$ and $y''$ and compare with the given differential equation and found that $P(t)= -1/t$ and which is not continuous on [0,1]. So such function $P$ can not exist. Is my concept is correct . Please help me to solve this also if any other method . Thanks in advance.","Does there exist continuous functions  $P$ and $Q$ on $[0,1]$ such that $y(t)=sin(t^2)$  is a solution to $y''+Py'+Qy=0$ on $[\frac1n,1]$ for all $n\geq1$? I find $y'$ and $y''$ and compare with the given differential equation and found that $P(t)= -1/t$ and which is not continuous on [0,1]. So such function $P$ can not exist. Is my concept is correct . Please help me to solve this also if any other method . Thanks in advance.",,"['ordinary-differential-equations', 'derivatives']"
72,Are all solutions to an ordinary differential equation continuous solutions to the corresponding implied differential equation and vice versa? [duplicate],Are all solutions to an ordinary differential equation continuous solutions to the corresponding implied differential equation and vice versa? [duplicate],,"This question already exists : Correlation between the weak solutions of a differential equation and implied differential equations Closed 6 years ago . Regarding the duplicate. Yes, I know the other one has a lot of shared text, but those were just definitions/setup and I was being lazy. The core questions are still different unless you believe derivatives are weak derivatives in which case you might need to read up on them. I don't know the exact definition, but I do know they aren't the same... Now I have to heavily emphasize the fact that I have never studied differential algebra or the concept of other types of differentiation (which is what I believe is the concept behind a differential algebra). So, if I am abusing the terminology a little bit, please forgive me. Let us define a differential algebra known as implied differentiation. It actually does not have a unique value. Let us denote the implied derivative operator as $I(f)$, where $f$ is any function being implied differentiated. This is of course, nonstandard terminology. We define the operator I to be $I(f)(x)(g)$ to be: $$I(f)(x)(g) := g(x) \left(\lim_{h\to 0^+} \frac{f(x+h)-f(x)}{h} \right) + (1-g(x)) \left(\lim_{h\to 0^-} \frac{f(x+h)-f(x)}{h} \right)$$ Where $g(x)$ is an arbitrary characteristic/indicator function. By this I mean that the evaluation of $I(f)$ at $x$ is either one of the one-sided limits and the choice of which one to pick comes from your particular choice of $g$. The general crux of this question is that I wish to determine whether or not the following statement is true. I'm pretty sure it is, though that's just intuition. Is a function a solution to an ordinary differential equation if and only if it is a continuous solution to the corresponding implied differential equation? By corresponding equations, I just mean that they are corresponding if they are the same except with all of the derivative operators replaced with the implied derivative operator. So, the equation $D(y) = e^x$ has a corresponding equation of $I(y) = e^x$. I do not know for sure whether or not anyone will actually be able to prove this statement. I think it is a bit tricky, but even just some advice on how to approach this or how to begin would be greatly appreciated. It isn't for homework or anything like that. It's just a statement and concept I've developed in my head over the years and I want to determine its truthfulness. Note: if something equivalent to this or very similar that just makes this a special case has been proven in the past feel free to use that as an answer. I'm going under the (possibly mistaken) impression that this hasn't actually been proven before. UPDATE: I believe that a possible route to proving this statement might come by proving the following propositions. The solution sets of sub-differential equations are a super set of the solution sets of the corresponding differential equations The solution sets of sub-differential equations are a super set of the solution sets of the corresponding implied differential equations If a solution to a sub-differential equation is continuous then it is a solution to the corresponding differential equation If a solution to a sub-differential equation is continuous then it is a solution to the corresponding implied differential equation I believe that the the first two propositions might follow trivially from the definition of the sub-derivative. The third one might have been proven in the past. I am unsure. The fourth one would then be the true meat and bones of the proof. The subderivative is defined here: https://en.wikipedia.org/wiki/Subderivative Reasons for wanting this statement proved: The implied differential operator is defined via the limits; however, the purpose of that is to emulate a differential algebra wherein all step functions have a derivative of 0 and all the other normal rules are preserved. If it wasn't apparent, because of this any implied differential equation involving step functions is trivial to solve (at least in terms of the step functions themselves providing difficulty). If the statement were true, it would provide a new avenue by which to attack differential equations, some of which might be made trivial to solve via this method.","This question already exists : Correlation between the weak solutions of a differential equation and implied differential equations Closed 6 years ago . Regarding the duplicate. Yes, I know the other one has a lot of shared text, but those were just definitions/setup and I was being lazy. The core questions are still different unless you believe derivatives are weak derivatives in which case you might need to read up on them. I don't know the exact definition, but I do know they aren't the same... Now I have to heavily emphasize the fact that I have never studied differential algebra or the concept of other types of differentiation (which is what I believe is the concept behind a differential algebra). So, if I am abusing the terminology a little bit, please forgive me. Let us define a differential algebra known as implied differentiation. It actually does not have a unique value. Let us denote the implied derivative operator as $I(f)$, where $f$ is any function being implied differentiated. This is of course, nonstandard terminology. We define the operator I to be $I(f)(x)(g)$ to be: $$I(f)(x)(g) := g(x) \left(\lim_{h\to 0^+} \frac{f(x+h)-f(x)}{h} \right) + (1-g(x)) \left(\lim_{h\to 0^-} \frac{f(x+h)-f(x)}{h} \right)$$ Where $g(x)$ is an arbitrary characteristic/indicator function. By this I mean that the evaluation of $I(f)$ at $x$ is either one of the one-sided limits and the choice of which one to pick comes from your particular choice of $g$. The general crux of this question is that I wish to determine whether or not the following statement is true. I'm pretty sure it is, though that's just intuition. Is a function a solution to an ordinary differential equation if and only if it is a continuous solution to the corresponding implied differential equation? By corresponding equations, I just mean that they are corresponding if they are the same except with all of the derivative operators replaced with the implied derivative operator. So, the equation $D(y) = e^x$ has a corresponding equation of $I(y) = e^x$. I do not know for sure whether or not anyone will actually be able to prove this statement. I think it is a bit tricky, but even just some advice on how to approach this or how to begin would be greatly appreciated. It isn't for homework or anything like that. It's just a statement and concept I've developed in my head over the years and I want to determine its truthfulness. Note: if something equivalent to this or very similar that just makes this a special case has been proven in the past feel free to use that as an answer. I'm going under the (possibly mistaken) impression that this hasn't actually been proven before. UPDATE: I believe that a possible route to proving this statement might come by proving the following propositions. The solution sets of sub-differential equations are a super set of the solution sets of the corresponding differential equations The solution sets of sub-differential equations are a super set of the solution sets of the corresponding implied differential equations If a solution to a sub-differential equation is continuous then it is a solution to the corresponding differential equation If a solution to a sub-differential equation is continuous then it is a solution to the corresponding implied differential equation I believe that the the first two propositions might follow trivially from the definition of the sub-derivative. The third one might have been proven in the past. I am unsure. The fourth one would then be the true meat and bones of the proof. The subderivative is defined here: https://en.wikipedia.org/wiki/Subderivative Reasons for wanting this statement proved: The implied differential operator is defined via the limits; however, the purpose of that is to emulate a differential algebra wherein all step functions have a derivative of 0 and all the other normal rules are preserved. If it wasn't apparent, because of this any implied differential equation involving step functions is trivial to solve (at least in terms of the step functions themselves providing difficulty). If the statement were true, it would provide a new avenue by which to attack differential equations, some of which might be made trivial to solve via this method.",,"['ordinary-differential-equations', 'limits', 'derivatives', 'differential-algebra']"
73,Solve $y ^2-x(\frac{dy}{dx})^2 = 1$ using proposed change of variables,Solve  using proposed change of variables,y ^2-x(\frac{dy}{dx})^2 = 1,"I am kind of stuck with this non linear differential. I am preparing for my finals and I cannot get this one. Full question goes like this: Find all the solutions to the equation $y ^2-x(\frac{dy}{dx})^2 - 1= 0$ stating in each case the maximal solution interval. Hint: Use $u=y'\sqrt{-x},\,x<0$ and $u=y'\sqrt{x},\,x>0$ Also the final solutions are also given: $y=1$ and $y=-1 \quad\forall x $ $y(x)=cosh(2\sqrt{x}+K),\quad x>0$ $y=cos(2\sqrt{-x}+K),\quad x<0$ What I have done so far. Let $u=y'\sqrt{x}\Rightarrow u'=y''\sqrt{x}+y'\frac{1}{2\sqrt{x}}$ and I plug it into the original equation and I differentiate w.r.t x: $y^2-u^2=1\Rightarrow 2yy'-2u(y''\sqrt{x}+\frac{y'}{2\sqrt{x}})=0$. Switching back the change $u=y'\sqrt{x},\,x>0$ we get: $2yy'-2y'\sqrt{x}(y''\sqrt{x}+\frac{y'}{\sqrt{x}})=yy'-y'y''x-y'^2 =y'(y-y''x-y')=0$. So we get $y'=0 \Rightarrow y=C$, which is not one of the stated solutions or $(y-y''x-y')=0$ which does not make a lot of sense to me as we ended up with a second order equation, which needs two arbitrary constants, when we actually started with a first order equation. I actually got the first three solutions using a different approach, which is not the one hinted, but I posted nonetheless in case it might help someone in order to help me :) $y'^2=\frac{y^2-1}{x}\Rightarrow\frac{1}{\sqrt{y^2-1}}dy=\pm\frac{1}{\sqrt{x}}dx$. Solutions $y=\pm 1$ appear at this step. Using the substitution $y=cosht$ we get $t=\pm2\sqrt{x}+C$ which gives the third one: $y=cosh(2\sqrt{x}+C)$. However I just cannot get the proposed substitution to work and I cannot find the last solution when x is negative. Any help is really appreciated!!!! Thanks!!!","I am kind of stuck with this non linear differential. I am preparing for my finals and I cannot get this one. Full question goes like this: Find all the solutions to the equation $y ^2-x(\frac{dy}{dx})^2 - 1= 0$ stating in each case the maximal solution interval. Hint: Use $u=y'\sqrt{-x},\,x<0$ and $u=y'\sqrt{x},\,x>0$ Also the final solutions are also given: $y=1$ and $y=-1 \quad\forall x $ $y(x)=cosh(2\sqrt{x}+K),\quad x>0$ $y=cos(2\sqrt{-x}+K),\quad x<0$ What I have done so far. Let $u=y'\sqrt{x}\Rightarrow u'=y''\sqrt{x}+y'\frac{1}{2\sqrt{x}}$ and I plug it into the original equation and I differentiate w.r.t x: $y^2-u^2=1\Rightarrow 2yy'-2u(y''\sqrt{x}+\frac{y'}{2\sqrt{x}})=0$. Switching back the change $u=y'\sqrt{x},\,x>0$ we get: $2yy'-2y'\sqrt{x}(y''\sqrt{x}+\frac{y'}{\sqrt{x}})=yy'-y'y''x-y'^2 =y'(y-y''x-y')=0$. So we get $y'=0 \Rightarrow y=C$, which is not one of the stated solutions or $(y-y''x-y')=0$ which does not make a lot of sense to me as we ended up with a second order equation, which needs two arbitrary constants, when we actually started with a first order equation. I actually got the first three solutions using a different approach, which is not the one hinted, but I posted nonetheless in case it might help someone in order to help me :) $y'^2=\frac{y^2-1}{x}\Rightarrow\frac{1}{\sqrt{y^2-1}}dy=\pm\frac{1}{\sqrt{x}}dx$. Solutions $y=\pm 1$ appear at this step. Using the substitution $y=cosht$ we get $t=\pm2\sqrt{x}+C$ which gives the third one: $y=cosh(2\sqrt{x}+C)$. However I just cannot get the proposed substitution to work and I cannot find the last solution when x is negative. Any help is really appreciated!!!! Thanks!!!",,['ordinary-differential-equations']
74,Why is there no general solution for the general 2nd order linear ODE,Why is there no general solution for the general 2nd order linear ODE,,"We can always solve a general first order linear ODE: $$y'(x)+a(x)y(x)=b(x).$$ I am looking for some intuition why the general 2nd order linear ODE $$y''(x)+a(x)y'(x)+b(x)y(x)=c(x) $$ does not have a gerneral formula. Is it mathematically impossible, or is there a chance, that someone will find a general solution? If it is mathematically impossible, is there any intuitive explanation to this phenomenon?","We can always solve a general first order linear ODE: $$y'(x)+a(x)y(x)=b(x).$$ I am looking for some intuition why the general 2nd order linear ODE $$y''(x)+a(x)y'(x)+b(x)y(x)=c(x) $$ does not have a gerneral formula. Is it mathematically impossible, or is there a chance, that someone will find a general solution? If it is mathematically impossible, is there any intuitive explanation to this phenomenon?",,['ordinary-differential-equations']
75,Bounding a solution of an ODE with a small source,Bounding a solution of an ODE with a small source,,"I have an ODE of the form $$ f''(x) + f(x) = \epsilon g(x)$$ with initial conditions $$ f(0) = f'(0) = 0 $$ $g(x)$ is $O(1)$ as $\epsilon \to 0$, and $g(x)$ is as smooth as necessary.  Is there a way I can bound $f(x)$?  In particular, I would like to be able to claim that $f(x)$ must be $O(\epsilon)$ as $\epsilon \to 0$ for all $x>0$, but I'm not sure if this is true or how to approach this.  Any hints/advice would be greatly appreciated. EDIT: I also know that $g(0) = g'(0) = 0$, if that helps with anything. EDIT2: Upon seeing the answers, I think we can see that the values of $g$ and $g'$ at $0$ are irrelevant as long as $g(x) = O(1)$.","I have an ODE of the form $$ f''(x) + f(x) = \epsilon g(x)$$ with initial conditions $$ f(0) = f'(0) = 0 $$ $g(x)$ is $O(1)$ as $\epsilon \to 0$, and $g(x)$ is as smooth as necessary.  Is there a way I can bound $f(x)$?  In particular, I would like to be able to claim that $f(x)$ must be $O(\epsilon)$ as $\epsilon \to 0$ for all $x>0$, but I'm not sure if this is true or how to approach this.  Any hints/advice would be greatly appreciated. EDIT: I also know that $g(0) = g'(0) = 0$, if that helps with anything. EDIT2: Upon seeing the answers, I think we can see that the values of $g$ and $g'$ at $0$ are irrelevant as long as $g(x) = O(1)$.",,"['ordinary-differential-equations', 'asymptotics']"
76,Closed form solutions to Abel equation,Closed form solutions to Abel equation,,"Consider a (somewhat simplified) Abel equation of the first kind for $\alpha$: $\left[\alpha(x)\right]^2 \left[1-f(x)\alpha(x)\right] + \alpha'(x) = 0$, for some smooth function $f$. Is it known what conditions on $f$ are necessary (and sufficient) to ensure a closed form solution? One particular case I am interested in is $f(x) = \lambda x^3$ for some $\lambda \in \mathbb{R}$; is there any hope of getting a closed form in this case? What other cases have been studied?","Consider a (somewhat simplified) Abel equation of the first kind for $\alpha$: $\left[\alpha(x)\right]^2 \left[1-f(x)\alpha(x)\right] + \alpha'(x) = 0$, for some smooth function $f$. Is it known what conditions on $f$ are necessary (and sufficient) to ensure a closed form solution? One particular case I am interested in is $f(x) = \lambda x^3$ for some $\lambda \in \mathbb{R}$; is there any hope of getting a closed form in this case? What other cases have been studied?",,['ordinary-differential-equations']
77,Differential equation related to Golomb's sequence,Differential equation related to Golomb's sequence,,"While studying Golomb's sequence , the following differential equation arouse: $$ f(f(x))=\frac{1}{f'(x)} $$ I don't know much about differential equations so I am a bit clueless. Is there a way to solve it nicely? Edit: With the link provided by Michael Galuza, I could find a function which satisfies the given equation, namely: $$ f(x)=\varphi^{2-\varphi}x^{\varphi-1} $$ Where $\varphi=\frac{1+\sqrt5}{2}$ is the golden ratio. However, I did not succeed in proving that this is the only one. Is this feasible?","While studying Golomb's sequence , the following differential equation arouse: $$ f(f(x))=\frac{1}{f'(x)} $$ I don't know much about differential equations so I am a bit clueless. Is there a way to solve it nicely? Edit: With the link provided by Michael Galuza, I could find a function which satisfies the given equation, namely: $$ f(x)=\varphi^{2-\varphi}x^{\varphi-1} $$ Where $\varphi=\frac{1+\sqrt5}{2}$ is the golden ratio. However, I did not succeed in proving that this is the only one. Is this feasible?",,['ordinary-differential-equations']
78,Are there closed curves for which acceleration is orthogonal to position?,Are there closed curves for which acceleration is orthogonal to position?,,Can we find $\vec{f} : \mathbb{R}\rightarrow \mathbb{R}^3 $ such that $\vec{f}(t) \cdot \frac{d^2 \vec{f}(t)}{dt^2} =0$ and $\vec{f}(0) = \vec{f}(T)$ for some $T >0$ ? Exclude the trivial cases. I want $\frac{d \vec{f}}{dt} \neq 0$ for some part of the trajectory.,Can we find $\vec{f} : \mathbb{R}\rightarrow \mathbb{R}^3 $ such that $\vec{f}(t) \cdot \frac{d^2 \vec{f}(t)}{dt^2} =0$ and $\vec{f}(0) = \vec{f}(T)$ for some $T >0$ ? Exclude the trivial cases. I want $\frac{d \vec{f}}{dt} \neq 0$ for some part of the trajectory.,,"['ordinary-differential-equations', 'vectors']"
79,"Legendre Differential Equation, $y_1,y_2$ linearly independent solutions","Legendre Differential Equation,  linearly independent solutions","y_1,y_2","$$(1-x^2)y''-2xy'+p(p+1)y=0, p \in \mathbb{R} \text{ constant } \\ -1 < x<1$$ At the interval $(-1,1)$ the above differential equation can be written equivalently $$y''+p(x)y'+q(x)y=0, -1<x<1 \text{ where } \\p(x)=\frac{-2x}{1-x^2} \\ q(x)= \frac{p(p+1)}{1-x^2}$$ $p,q$ can be written as power series $\sum_{n=0}^{\infty} p_n x^n, \sum_{n=0}^{\infty} q_n x^n$ respectively with centre $0$ and $\sum_{n=0}^{\infty} p_n x^n=p(x)$ and $\sum_{n=0}^{\infty} q_nx^n=q(x), \ \forall -1<x<1$ $$p(x)= \sum_{n=0}^{\infty} (-2) x^{2n+1}, -1<x<1$$ $$q(x)= \sum_{n=0}^{\infty} p(p+1) x^{2n}, \forall -1<x<1$$ Since $p,q$ can be written as power series with centre $0$ and radius of convergence $1$, it's logical to look for  a solution of the differential equation of the form $$y(x)=\sum_{n=0}^{\infty} a_n x^n \text{ with radius of convergence } R>0$$ $$-2xy'(x)= \sum_{n=1}^{\infty} -2n a_n x^n$$ $$y''(x)= \sum_{n=0}^{\infty} (n+2)(n+1) a_{n+2} x^n \\ -x^2y''(x)=\sum_{n=2}^{\infty} -n(n-1)a_nx^n$$ We have: $$\sum_{n=0}^{\infty} \left[ (n+2)(n+1) a_{n+2}-n(n-1)a_n-2na_n+p(p+1)a_n\right]x^n=0, \forall x \in (-R,R)$$ It has to hold: $(n+2)(n+1)a_{n+2}-n(n-1)a_n-2na_n+p(p+1)a_n=0, \forall n=0,1,2, \dots$ Thus: $$a_{n+2}=-\frac{(p-n)(p+n+1)}{(n+1)(n+2)}a_n, \forall n=0,1,2, \dots$$ So the solution is written as follows: $$y(x)=a_0 \left[ 1- \frac{p(p+1)}{2!}x^2+ \frac{p(p-2)(p+1)(p+3)}{4!}x^4- \frac{p(p-2)(p-4)(p+1)(p+3)(p+5)}{6!}x^6+ \dots  \right] +a_1 \left[ x- \frac{(p-1)(p+2)}{3!}x^3+ \frac{(p-1)(p-3)(p+2)(p+4)}{5!}x^5-\frac{(p-1)(p-3)(p-5)(p+2)(p+4)(p+6)}{7!}x^7+ \dots\right]$$ We will show that if $p \in \mathbb{R} \setminus{\mathbb{Z}}$ then the power series at the right of $a_0,a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$, that are infinitely many times differentiable at $(-1,1)$. We have for $-1<x<1, x \neq 0$: $$\left | \frac{a_{2(n+1)} x^{2(n+1)}}{a_{2n} x^{2n}}\right |= \left |-  \frac{(p-2n)(p+2n+1)}{(2n+1)(2n+2)}\right| |x|^2 \to |x|^2<1$$ So the series $\sum_{n=0}^{\infty} \overline{a_{2n}} x^{2n}$ converges for $-1<x<1$ In the same way, we show that the series $\sum_{n=0}^{\infty} \overline{a_{2n+1}}x^{2n+1}$ converges for $-1<x<1$. According to the above, if $p \in \mathbb{R} \setminus{\mathbb{Z}}$ the power series at the right of $a_0, a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$ that are infinitely many times differentiable at $(-1,1)$. Then we show that $y_1(x)$ converges and in the same way we could show that $y_2(x)$ converges. But have we shown like that that the radius of convergence is $1$? And how do we deduce that the functions are infinitely many times differentiable? Also what is meant with $\overline{a_{2n}}$? Furthermore, what happens if $p \in \mathbb{Z}$? EDIT : Couldn't we also write the solution $y$ in the following form? $$y(x)= \sum_{k=0}^{\infty} a_{2k} x^{2k}+ \sum_{k=0}^{\infty} a_{2k+1} x^{2k+1}$$ where : $$a_{2k}= \frac{\prod_{j=0}^{2k-1} (j+(-1)^{j+1} p)}{(2k)!}a_0$$ and $$a_{2k+1}= \frac{\prod_{j=1}^{2k} (j+(-1)^j p)}{(2k+1)!}a_1$$ Or am I wrong? If it is like that, applying the ratio test we would get: For $n=2k$: $$\left| \frac{\frac{\prod_{j=0}^{2k+1} (j+(-1)^{j+1} p) a_0 x^{2k+2}}{(2k+2)!}}{\frac{\prod_{j=0}^{2k-1} (j+(-1)^{j+1} p) a_0 x^{2k}}{(2k)!}}\right| = \left| \frac{(2k-p)(2k+1+p) x^2}{(2k+1)(2k+2)} \right| \to |x^2|<1$$ So the series $\sum_{k=0}^{\infty} a_{2k} x^{2k}$ converges for all $x$ such that $-1<x<1$. Is it right so far? If so,how do we deduce that if $p \in \mathbb{R} \setminus{\mathbb{Z}}$ then the power series at the right of $a_0,a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$, that are infinitely many times differentiable at $(-1,1)$ ? EDIT 2 : If $p$ is a positive integer, then one of the series terminates and becomes polynomial. For example if $p=7$, then $a_7, a_9, a_{11}, \dots, a_{2n+1}=0$ and so $\sum_{k=0}^{\infty} a_{2k+1} x^{2k+1}= a_1 x+ a_3 x^3+ a_5 x^5$, right? So does this mean that in such a case it doesn't hold that  the power series at the right of $a_0,a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$, that are infinitely many times differentiable at $(-1,1)$?","$$(1-x^2)y''-2xy'+p(p+1)y=0, p \in \mathbb{R} \text{ constant } \\ -1 < x<1$$ At the interval $(-1,1)$ the above differential equation can be written equivalently $$y''+p(x)y'+q(x)y=0, -1<x<1 \text{ where } \\p(x)=\frac{-2x}{1-x^2} \\ q(x)= \frac{p(p+1)}{1-x^2}$$ $p,q$ can be written as power series $\sum_{n=0}^{\infty} p_n x^n, \sum_{n=0}^{\infty} q_n x^n$ respectively with centre $0$ and $\sum_{n=0}^{\infty} p_n x^n=p(x)$ and $\sum_{n=0}^{\infty} q_nx^n=q(x), \ \forall -1<x<1$ $$p(x)= \sum_{n=0}^{\infty} (-2) x^{2n+1}, -1<x<1$$ $$q(x)= \sum_{n=0}^{\infty} p(p+1) x^{2n}, \forall -1<x<1$$ Since $p,q$ can be written as power series with centre $0$ and radius of convergence $1$, it's logical to look for  a solution of the differential equation of the form $$y(x)=\sum_{n=0}^{\infty} a_n x^n \text{ with radius of convergence } R>0$$ $$-2xy'(x)= \sum_{n=1}^{\infty} -2n a_n x^n$$ $$y''(x)= \sum_{n=0}^{\infty} (n+2)(n+1) a_{n+2} x^n \\ -x^2y''(x)=\sum_{n=2}^{\infty} -n(n-1)a_nx^n$$ We have: $$\sum_{n=0}^{\infty} \left[ (n+2)(n+1) a_{n+2}-n(n-1)a_n-2na_n+p(p+1)a_n\right]x^n=0, \forall x \in (-R,R)$$ It has to hold: $(n+2)(n+1)a_{n+2}-n(n-1)a_n-2na_n+p(p+1)a_n=0, \forall n=0,1,2, \dots$ Thus: $$a_{n+2}=-\frac{(p-n)(p+n+1)}{(n+1)(n+2)}a_n, \forall n=0,1,2, \dots$$ So the solution is written as follows: $$y(x)=a_0 \left[ 1- \frac{p(p+1)}{2!}x^2+ \frac{p(p-2)(p+1)(p+3)}{4!}x^4- \frac{p(p-2)(p-4)(p+1)(p+3)(p+5)}{6!}x^6+ \dots  \right] +a_1 \left[ x- \frac{(p-1)(p+2)}{3!}x^3+ \frac{(p-1)(p-3)(p+2)(p+4)}{5!}x^5-\frac{(p-1)(p-3)(p-5)(p+2)(p+4)(p+6)}{7!}x^7+ \dots\right]$$ We will show that if $p \in \mathbb{R} \setminus{\mathbb{Z}}$ then the power series at the right of $a_0,a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$, that are infinitely many times differentiable at $(-1,1)$. We have for $-1<x<1, x \neq 0$: $$\left | \frac{a_{2(n+1)} x^{2(n+1)}}{a_{2n} x^{2n}}\right |= \left |-  \frac{(p-2n)(p+2n+1)}{(2n+1)(2n+2)}\right| |x|^2 \to |x|^2<1$$ So the series $\sum_{n=0}^{\infty} \overline{a_{2n}} x^{2n}$ converges for $-1<x<1$ In the same way, we show that the series $\sum_{n=0}^{\infty} \overline{a_{2n+1}}x^{2n+1}$ converges for $-1<x<1$. According to the above, if $p \in \mathbb{R} \setminus{\mathbb{Z}}$ the power series at the right of $a_0, a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$ that are infinitely many times differentiable at $(-1,1)$. Then we show that $y_1(x)$ converges and in the same way we could show that $y_2(x)$ converges. But have we shown like that that the radius of convergence is $1$? And how do we deduce that the functions are infinitely many times differentiable? Also what is meant with $\overline{a_{2n}}$? Furthermore, what happens if $p \in \mathbb{Z}$? EDIT : Couldn't we also write the solution $y$ in the following form? $$y(x)= \sum_{k=0}^{\infty} a_{2k} x^{2k}+ \sum_{k=0}^{\infty} a_{2k+1} x^{2k+1}$$ where : $$a_{2k}= \frac{\prod_{j=0}^{2k-1} (j+(-1)^{j+1} p)}{(2k)!}a_0$$ and $$a_{2k+1}= \frac{\prod_{j=1}^{2k} (j+(-1)^j p)}{(2k+1)!}a_1$$ Or am I wrong? If it is like that, applying the ratio test we would get: For $n=2k$: $$\left| \frac{\frac{\prod_{j=0}^{2k+1} (j+(-1)^{j+1} p) a_0 x^{2k+2}}{(2k+2)!}}{\frac{\prod_{j=0}^{2k-1} (j+(-1)^{j+1} p) a_0 x^{2k}}{(2k)!}}\right| = \left| \frac{(2k-p)(2k+1+p) x^2}{(2k+1)(2k+2)} \right| \to |x^2|<1$$ So the series $\sum_{k=0}^{\infty} a_{2k} x^{2k}$ converges for all $x$ such that $-1<x<1$. Is it right so far? If so,how do we deduce that if $p \in \mathbb{R} \setminus{\mathbb{Z}}$ then the power series at the right of $a_0,a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$, that are infinitely many times differentiable at $(-1,1)$ ? EDIT 2 : If $p$ is a positive integer, then one of the series terminates and becomes polynomial. For example if $p=7$, then $a_7, a_9, a_{11}, \dots, a_{2n+1}=0$ and so $\sum_{k=0}^{\infty} a_{2k+1} x^{2k+1}= a_1 x+ a_3 x^3+ a_5 x^5$, right? So does this mean that in such a case it doesn't hold that  the power series at the right of $a_0,a_1$ have radius of convergence $1$ and so they define functions $y_1(x), y_2(x)$, that are infinitely many times differentiable at $(-1,1)$?",,"['ordinary-differential-equations', 'convergence-divergence']"
80,"Completeness implies geodesic completeness, a more conceptual way?","Completeness implies geodesic completeness, a more conceptual way?",,"We know from Riemannian geometry that for Riemannian manifolds, completeness and geodesic completeness are equivalent, which is usually a consequence of Hopf-Rinow theorem. However, I'm considering a more conceptual reformalization of this fact. Let's consider the simpler direction for this. Suppose $M$ is a Riemannian manifold and $UM$ is its unit sphere bundle. Levi-Civita connection gives an horizontal vector field $W$ on $UM$, which determines the local geodesic flow. If $M$ is complete, we need to show that the geodesic flow on $UM$ is complete, i.e. integral curves are indefinitely extendable. I guess that it will follow from a more general result on fiber bundles and vector fields on it. For example, we know that the fibers of $UM\to M$ are compact, and $W$ is horizontal. It's just like the theorem on completeness of flows of vector fields on compact manifolds. It seems a more natural way to formalize the statement. Any help? Thanks!","We know from Riemannian geometry that for Riemannian manifolds, completeness and geodesic completeness are equivalent, which is usually a consequence of Hopf-Rinow theorem. However, I'm considering a more conceptual reformalization of this fact. Let's consider the simpler direction for this. Suppose $M$ is a Riemannian manifold and $UM$ is its unit sphere bundle. Levi-Civita connection gives an horizontal vector field $W$ on $UM$, which determines the local geodesic flow. If $M$ is complete, we need to show that the geodesic flow on $UM$ is complete, i.e. integral curves are indefinitely extendable. I guess that it will follow from a more general result on fiber bundles and vector fields on it. For example, we know that the fibers of $UM\to M$ are compact, and $W$ is horizontal. It's just like the theorem on completeness of flows of vector fields on compact manifolds. It seems a more natural way to formalize the statement. Any help? Thanks!",,"['ordinary-differential-equations', 'differential-geometry', 'differential-topology', 'riemannian-geometry']"
81,Solve the given initial value problem.I need your help.,Solve the given initial value problem.I need your help.,,"Solve the $$x'=tx^2+x-t^3\,,\quad x\left(\, 2\,\right)=1$$ I need its exact solution not a numerical solution.In fact I have to compare the exact solution with the numerical solution.I tried it but I am unable to solve it for the previous one day.I need your kind help. Thanks.","Solve the $$x'=tx^2+x-t^3\,,\quad x\left(\, 2\,\right)=1$$ I need its exact solution not a numerical solution.In fact I have to compare the exact solution with the numerical solution.I tried it but I am unable to solve it for the previous one day.I need your kind help. Thanks.",,['ordinary-differential-equations']
82,How to solve system of stochastic differential equations?,How to solve system of stochastic differential equations?,,"I have the following two SDEs $$dN_1=(2a-1)pN_1dt+\alpha_1 N_1dW_1$$ $$dN_2=(2pN_1-\mu N_2)dt+\alpha_2 N_2dW_2$$ where $W$ is the standard Brownian motion/Wiener process. This isn't homework, I'm just curious. I can solve the first one but the second one is in terms of $N_1$ and $N_2$ so I don't know how to go about it. I'm new to SDEs so any help is appreciated! $$N_1(t)=N_1(0)exp\left\{((2a-1)p-\frac{1}{2}\alpha_1^2)t+\alpha_1 W_1\right\}$$","I have the following two SDEs $$dN_1=(2a-1)pN_1dt+\alpha_1 N_1dW_1$$ $$dN_2=(2pN_1-\mu N_2)dt+\alpha_2 N_2dW_2$$ where $W$ is the standard Brownian motion/Wiener process. This isn't homework, I'm just curious. I can solve the first one but the second one is in terms of $N_1$ and $N_2$ so I don't know how to go about it. I'm new to SDEs so any help is appreciated! $$N_1(t)=N_1(0)exp\left\{((2a-1)p-\frac{1}{2}\alpha_1^2)t+\alpha_1 W_1\right\}$$",,"['ordinary-differential-equations', 'stochastic-processes', 'stochastic-differential-equations']"
83,Find a solution to the following ordinary differential equation : $\frac{dy}{dx}=e^{x−y}(e^x−e^y).$,Find a solution to the following ordinary differential equation :,\frac{dy}{dx}=e^{x−y}(e^x−e^y).,Find a solution to the following ordinary differential equation - $$\frac{dy}{dx}=e^{x−y}(e^x−e^y).$$ A change of variables so that it becomes variable separable may be required.,Find a solution to the following ordinary differential equation - $$\frac{dy}{dx}=e^{x−y}(e^x−e^y).$$ A change of variables so that it becomes variable separable may be required.,,['ordinary-differential-equations']
84,Tricky Integral equation - where to start?,Tricky Integral equation - where to start?,,"How would you go about solving this? $$p(x,t)=C\exp\left[-x+\int_0^t\int_0^\infty y\,p(y,\tau)\,\mathrm{d}y\,\mathrm{d}\tau\right]$$ Here $p(x,t)$ is the time-dependent probability distribution of a variable $x$, so it should be normalized to 1, and positive everywhere. Also, we have the initial condition: $$p(x,0) = \exp\left[-x\right]$$ I hit across this equation in some work I'm doing, and I'm stuck. Don't know enough about integral equations to know where to begin. Is there any hope at all to solve this? Where would you start?","How would you go about solving this? $$p(x,t)=C\exp\left[-x+\int_0^t\int_0^\infty y\,p(y,\tau)\,\mathrm{d}y\,\mathrm{d}\tau\right]$$ Here $p(x,t)$ is the time-dependent probability distribution of a variable $x$, so it should be normalized to 1, and positive everywhere. Also, we have the initial condition: $$p(x,0) = \exp\left[-x\right]$$ I hit across this equation in some work I'm doing, and I'm stuck. Don't know enough about integral equations to know where to begin. Is there any hope at all to solve this? Where would you start?",,"['ordinary-differential-equations', 'improper-integrals', 'problem-solving', 'integral-equations']"
85,"Solution of the IVP $\dot{y}=x^2+y^2,y(0)=0$",Solution of the IVP,"\dot{y}=x^2+y^2,y(0)=0","The solution of the IVP $$\dot{y}=x^2+y^2,y(0)=0$$ I know this IVP has a unique solution but I can't find the interval in which it has a unique solution can someone help me in finding the interval.","The solution of the IVP $$\dot{y}=x^2+y^2,y(0)=0$$ I know this IVP has a unique solution but I can't find the interval in which it has a unique solution can someone help me in finding the interval.",,['ordinary-differential-equations']
86,"How to interpret the meaning of ""$y$ solves the DE"" to have nice properties.","How to interpret the meaning of "" solves the DE"" to have nice properties.",y,"Assume that $I$ is an open interval $0 \in I$ $x$ varies in $I$ $y$ is a differentiable function of $x$. Now in the context of these assumptions, consider the following problem. $$x\dfrac{dy}{dx}=y$$ To solve, probably the easiest way is via a separation of variables. Thus, the first step would be to rewrite the equation in the form of: $$\dfrac{1}{y}\dfrac{dy}{dx}=\dfrac{1}{x}$$ Thus, it would be useful if the general solution to this new problem were to coincide precisely with the general solution to the old problem. However, according to the definitions that I've always been taught, the general solution to the new problem is the empty set. That is, it has no solutions. That's because we require that our solutions $y : I \rightarrow \mathbb{R}$ be total functions that satisfy the DE for all $x \in I$. But since $0 \in I$, and since the new problem involves raising $x$ to a negative power, no total function $y : I \rightarrow \mathbb{R}$ solves it. Thus, according to the definitions I've been taught, the general solution to the new problem fails to coincide with the general solution to the old problem. Question . What are the major approaches to generalizing the notion of a ""solution"" to a DE such that both equations described above have precisely the same general solution? And, where can I learn more about these approaches?","Assume that $I$ is an open interval $0 \in I$ $x$ varies in $I$ $y$ is a differentiable function of $x$. Now in the context of these assumptions, consider the following problem. $$x\dfrac{dy}{dx}=y$$ To solve, probably the easiest way is via a separation of variables. Thus, the first step would be to rewrite the equation in the form of: $$\dfrac{1}{y}\dfrac{dy}{dx}=\dfrac{1}{x}$$ Thus, it would be useful if the general solution to this new problem were to coincide precisely with the general solution to the old problem. However, according to the definitions that I've always been taught, the general solution to the new problem is the empty set. That is, it has no solutions. That's because we require that our solutions $y : I \rightarrow \mathbb{R}$ be total functions that satisfy the DE for all $x \in I$. But since $0 \in I$, and since the new problem involves raising $x$ to a negative power, no total function $y : I \rightarrow \mathbb{R}$ solves it. Thus, according to the definitions I've been taught, the general solution to the new problem fails to coincide with the general solution to the old problem. Question . What are the major approaches to generalizing the notion of a ""solution"" to a DE such that both equations described above have precisely the same general solution? And, where can I learn more about these approaches?",,"['ordinary-differential-equations', 'reference-request']"
87,Conditions for Unique Solution for this PDE,Conditions for Unique Solution for this PDE,,"$$ U_{xy}+\frac{2}{x+y}\left(U_{x}-U_{y}\right)=0 $$ with the boundary conditions $$ U(x_{0},y)=k(x_{0}-y)^{3}\\ U(x,y_{0})=k(x-y_{0})^{3} $$ where $k$ is a constant given by $k=U_{0}(x_{0}-y_{0})^{3}$. $x_{0}$, $y_{0}$ and $U(x_{0},y_{0})=U_{0}$ are known. The solution for the PDE is given by $$ U(x,y)=(x-y)^{5}\frac{\partial ^{4}}{\partial x^{2}\partial y^{2}}\left(\frac{f(x)-g(y)}{x-y}\right) $$ After some simplifications I get $$ U(x,y)=2\left(f''(x)-g''(y)\right)(x-y)^{2}-12\left(f'(x)+g'(y)\right)(x-y)+24\left(f(x)-g(y)\right) $$ where $f(x)$ and $g(y)$ are to be determined. I am looking for conditions that ensure uniqueness for the solution of this PDE. Any help will be appreciated. Thanks, Abiyo p.s I tried the following approach but it didn't work. $$ U(x_{0},y_{0})=2\left(f''(x_{0})-g''(y_{0})\right)(x_{0}-y_{0})^{2}-12\left(f'(x_{0})+g'(y_{0})\right)(x_{0}-y_{0})+24\left(f(x_{0})-g(y_{0})\right) $$ There are six unknowns $f(x_{0}),f'(x_{0}),f''(x_{0}),g(x_{0}),g'(x_{0})$ and $g''(y_{0})$. Assume $5$ values and the sixth one is determined. From there I proceed to find two ODEs and can find a solution to the PDE. The solution depends on my choice of these constants and hence I am looking for a constraint on this constants.","$$ U_{xy}+\frac{2}{x+y}\left(U_{x}-U_{y}\right)=0 $$ with the boundary conditions $$ U(x_{0},y)=k(x_{0}-y)^{3}\\ U(x,y_{0})=k(x-y_{0})^{3} $$ where $k$ is a constant given by $k=U_{0}(x_{0}-y_{0})^{3}$. $x_{0}$, $y_{0}$ and $U(x_{0},y_{0})=U_{0}$ are known. The solution for the PDE is given by $$ U(x,y)=(x-y)^{5}\frac{\partial ^{4}}{\partial x^{2}\partial y^{2}}\left(\frac{f(x)-g(y)}{x-y}\right) $$ After some simplifications I get $$ U(x,y)=2\left(f''(x)-g''(y)\right)(x-y)^{2}-12\left(f'(x)+g'(y)\right)(x-y)+24\left(f(x)-g(y)\right) $$ where $f(x)$ and $g(y)$ are to be determined. I am looking for conditions that ensure uniqueness for the solution of this PDE. Any help will be appreciated. Thanks, Abiyo p.s I tried the following approach but it didn't work. $$ U(x_{0},y_{0})=2\left(f''(x_{0})-g''(y_{0})\right)(x_{0}-y_{0})^{2}-12\left(f'(x_{0})+g'(y_{0})\right)(x_{0}-y_{0})+24\left(f(x_{0})-g(y_{0})\right) $$ There are six unknowns $f(x_{0}),f'(x_{0}),f''(x_{0}),g(x_{0}),g'(x_{0})$ and $g''(y_{0})$. Assume $5$ values and the sixth one is determined. From there I proceed to find two ODEs and can find a solution to the PDE. The solution depends on my choice of these constants and hence I am looking for a constraint on this constants.",,"['ordinary-differential-equations', 'partial-differential-equations']"
88,Delay-differential equation,Delay-differential equation,,"Consider the equation $$ f'(t)=\frac{f(t-b)}{t-b}$$ $f'(t)=\frac{df(t)}{dt}$ and $b$ is a constant. Does anyone know if this equation has a name, an analytic solution and how to find the solution? This is not a question about how to solve the equation numerically.","Consider the equation $$ f'(t)=\frac{f(t-b)}{t-b}$$ $f'(t)=\frac{df(t)}{dt}$ and $b$ is a constant. Does anyone know if this equation has a name, an analytic solution and how to find the solution? This is not a question about how to solve the equation numerically.",,"['analysis', 'ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'delay-differential-equations']"
89,Is Euler's lemma of fluid mechanics a nonlinear version of Liouville's theorem of ODEs?,Is Euler's lemma of fluid mechanics a nonlinear version of Liouville's theorem of ODEs?,,"Liouville's Theorem Consider the following linear system of ordinary differential equations:  $$\tag{1}  \dot{\mathbf{x}}=A(t)\mathbf{x}(t).$$ Let $\mathbf{x}_1, \mathbf{x}_2,  \ldots, \mathbf{x}_n$ be solutions of (1). Define the Wronskian   determinant to be  $$W(t)=\det \begin{bmatrix} \mathbf{x}_1 &  \mathbf{x}_2 & \ldots & \mathbf{x}_n\end{bmatrix}.$$ Then we have the   following differential relation for $W(t)$: $$\tag{L}  \dot{W}(t)=\verb+trace+\, A(t)\,\cdot\,W(t).$$ Compare with a lemma due to Euler which I encountered in a course of fluid mechanics I am attending. Here $\mathbf{x}$ refers to Eulerian coordinates and $\mathbf{y}$ refers to Lagrangian coordinates. Euler's Lemma Let $\mathbf{u}(\mathbf{x}, t)$ be the velocity field of a fluid flow $\mathbf{\Phi}(\mathbf{y}, t)$, meaning that:   $$\begin{array}{cc} \displaystyle \begin{cases} \dot{\mathbf{x}}(t)=\mathbf{u}(\mathbf{x}(t), t) \\ \mathbf{x}(0)=\mathbf{y}\end{cases}, & \mathbf{x}(t)=\mathbf{\Phi}(\mathbf{y}, t)\end{array}.$$   Denote with $J$ the Jacobian of the deformation gradient, that is    $$J(\mathbf{y}, t)=\det D_{\mathbf{y}}\mathbf{\Phi}(\mathbf{y}, t).$$   Then we have the following differential relation for $J$:   $$\tag{E}\frac{dJ}{dt}(t)=(\verb+div+\,\mathbf{u})\,J.$$ Even if it is formulated with the language of fluid mechanics, this lemma is essentially a result in ordinary differential equations, just like Liouville's theorem. My question is if Euler's lemma can be viewed as a nonlinear version of Liouville's theorem and if either one of the two results can be derived from the other. Thank you for reading.","Liouville's Theorem Consider the following linear system of ordinary differential equations:  $$\tag{1}  \dot{\mathbf{x}}=A(t)\mathbf{x}(t).$$ Let $\mathbf{x}_1, \mathbf{x}_2,  \ldots, \mathbf{x}_n$ be solutions of (1). Define the Wronskian   determinant to be  $$W(t)=\det \begin{bmatrix} \mathbf{x}_1 &  \mathbf{x}_2 & \ldots & \mathbf{x}_n\end{bmatrix}.$$ Then we have the   following differential relation for $W(t)$: $$\tag{L}  \dot{W}(t)=\verb+trace+\, A(t)\,\cdot\,W(t).$$ Compare with a lemma due to Euler which I encountered in a course of fluid mechanics I am attending. Here $\mathbf{x}$ refers to Eulerian coordinates and $\mathbf{y}$ refers to Lagrangian coordinates. Euler's Lemma Let $\mathbf{u}(\mathbf{x}, t)$ be the velocity field of a fluid flow $\mathbf{\Phi}(\mathbf{y}, t)$, meaning that:   $$\begin{array}{cc} \displaystyle \begin{cases} \dot{\mathbf{x}}(t)=\mathbf{u}(\mathbf{x}(t), t) \\ \mathbf{x}(0)=\mathbf{y}\end{cases}, & \mathbf{x}(t)=\mathbf{\Phi}(\mathbf{y}, t)\end{array}.$$   Denote with $J$ the Jacobian of the deformation gradient, that is    $$J(\mathbf{y}, t)=\det D_{\mathbf{y}}\mathbf{\Phi}(\mathbf{y}, t).$$   Then we have the following differential relation for $J$:   $$\tag{E}\frac{dJ}{dt}(t)=(\verb+div+\,\mathbf{u})\,J.$$ Even if it is formulated with the language of fluid mechanics, this lemma is essentially a result in ordinary differential equations, just like Liouville's theorem. My question is if Euler's lemma can be viewed as a nonlinear version of Liouville's theorem and if either one of the two results can be derived from the other. Thank you for reading.",,"['ordinary-differential-equations', 'fluid-dynamics']"
90,Existence of an extremum for the solution of the ODE $\ddot{x}+\frac32x^2=0$,Existence of an extremum for the solution of the ODE,\ddot{x}+\frac32x^2=0,"Consider the 2nd order ODE $$ \ddot{x}+\frac32x^2=0. $$ Denote by $u$ the maximal solution of the associated Cauchy problem with initial condition $(x(0),\dot{x}(0))=(0,1)$. The problem is to prove the existence of some $T>0$ such that $\dot{u}(T)=0$. Any help will be appreciated. Remark: So far I know that if $I$ denotes the domain of existence of $u$, then $$ \dot{u}^2(t)+u^3(t)=1\quad \forall t \in I. $$ In particular, if such a $T$ exists then $u(T)=1$.","Consider the 2nd order ODE $$ \ddot{x}+\frac32x^2=0. $$ Denote by $u$ the maximal solution of the associated Cauchy problem with initial condition $(x(0),\dot{x}(0))=(0,1)$. The problem is to prove the existence of some $T>0$ such that $\dot{u}(T)=0$. Any help will be appreciated. Remark: So far I know that if $I$ denotes the domain of existence of $u$, then $$ \dot{u}^2(t)+u^3(t)=1\quad \forall t \in I. $$ In particular, if such a $T$ exists then $u(T)=1$.",,['ordinary-differential-equations']
91,Center manifold of sets of equilibria,Center manifold of sets of equilibria,,"My question is regarding Center Manifolds containing a continuous set of equilibrium points. The theory I have studied talks about the existence of a center manifold for equilibrium points, but what happens if we do not have an isolated equilibrium point but a continuous set of equilibrium points? Let me give a toy example: $\dot x=-y^2\\ \dot y=-y^2x\\ \dot z=-z+y^2$ The $x$-axis is a continuous set of equilibrium points, the linear part is (all along this line) $ A=\begin{bmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & -1    \end{bmatrix} $ So my interpretation is that there exists a 2-dimensional center manifold which contains the $x-$axis. Is this correct? Do you know a source for a proof? Next if I would like to compute this center manifold? Does power series still apply? This is, can I propose an expression of the form: $ z=\displaystyle\sum_{i,j}a_{ij}x^iy^j  $? I guess it can´t be that ""easy"" since we would like to capture that the center manifold, at least in this case, is tangent to the 2-dimensional center space all along the $x-$axis. Anyway I am stuck here... Thanks for any help in understanding this. Literature for reference is also appreciated.","My question is regarding Center Manifolds containing a continuous set of equilibrium points. The theory I have studied talks about the existence of a center manifold for equilibrium points, but what happens if we do not have an isolated equilibrium point but a continuous set of equilibrium points? Let me give a toy example: $\dot x=-y^2\\ \dot y=-y^2x\\ \dot z=-z+y^2$ The $x$-axis is a continuous set of equilibrium points, the linear part is (all along this line) $ A=\begin{bmatrix} 0 & 0 & 0\\ 0 & 0 & 0\\ 0 & 0 & -1    \end{bmatrix} $ So my interpretation is that there exists a 2-dimensional center manifold which contains the $x-$axis. Is this correct? Do you know a source for a proof? Next if I would like to compute this center manifold? Does power series still apply? This is, can I propose an expression of the form: $ z=\displaystyle\sum_{i,j}a_{ij}x^iy^j  $? I guess it can´t be that ""easy"" since we would like to capture that the center manifold, at least in this case, is tangent to the 2-dimensional center space all along the $x-$axis. Anyway I am stuck here... Thanks for any help in understanding this. Literature for reference is also appreciated.",,"['ordinary-differential-equations', 'dynamical-systems']"
92,How to solve this differential equation for $y$ in terms of $x$ and $k$,How to solve this differential equation for  in terms of  and,y x k,$$yy'+\frac yx+k=0$$ How to solve this differential equation for $y$ in terms of $x$ and $k$ where $k$ is a parameter of $x$? $y(x)=y$ is a function and $x(k)=x$ is a gamma function,$$yy'+\frac yx+k=0$$ How to solve this differential equation for $y$ in terms of $x$ and $k$ where $k$ is a parameter of $x$? $y(x)=y$ is a function and $x(k)=x$ is a gamma function,,"['ordinary-differential-equations', 'multivariable-calculus']"
93,Eigenvalues for Sturm Liouville problems and more general ODE/PDE Problems,Eigenvalues for Sturm Liouville problems and more general ODE/PDE Problems,,"I'm struggling to find a geometric, or at least some intuitive understanding of eigenvalues and eigenfunctions in Sturm-Liouville problems (which I've been looking at in a PDE course). For instance, for the Sturm-Liouville problem: $$(p(x)\phi')'+q(x)\phi+\lambda\sigma(x)\phi=0$$ with boundary conditions of course, I struggle to see why this rather arbitrarily placed $\lambda$ deserves the high honour of being called an eigenvalue. And then we solve for $\phi$ and call it an eigenfunction. In linear algebra, eigenvalues and eigenvectors of a transformation have a number of nice geometric interpretations, and frankly I feel quite comfortable in seeing their importance in that setting, but I'm unsure why we can prescribe this terminology here. In the course I'm taking we're making a great deal of fuss about non-negativity of eigenvalues, the orthogonality of eigenfunctions, etc., etc., but while I can follow the various proofs algebraically I must admit I feel quite lost without this basic understanding. Thanks","I'm struggling to find a geometric, or at least some intuitive understanding of eigenvalues and eigenfunctions in Sturm-Liouville problems (which I've been looking at in a PDE course). For instance, for the Sturm-Liouville problem: $$(p(x)\phi')'+q(x)\phi+\lambda\sigma(x)\phi=0$$ with boundary conditions of course, I struggle to see why this rather arbitrarily placed $\lambda$ deserves the high honour of being called an eigenvalue. And then we solve for $\phi$ and call it an eigenfunction. In linear algebra, eigenvalues and eigenvectors of a transformation have a number of nice geometric interpretations, and frankly I feel quite comfortable in seeing their importance in that setting, but I'm unsure why we can prescribe this terminology here. In the course I'm taking we're making a great deal of fuss about non-negativity of eigenvalues, the orthogonality of eigenfunctions, etc., etc., but while I can follow the various proofs algebraically I must admit I feel quite lost without this basic understanding. Thanks",,"['ordinary-differential-equations', 'partial-differential-equations', 'terminology']"
94,Solving differential equation $y^{(5)} + 2y^{(3)} + y' = 2x + \sin(x) + \cos(x)$,Solving differential equation,y^{(5)} + 2y^{(3)} + y' = 2x + \sin(x) + \cos(x),"I'm trying to solve the differential equation $$L[y] = y^{(5)} + 2y^{(3)} + y' = 2x + \sin(x) + \cos(x)$$ using the method of undetermined coefficients. I'm having a problem in that my solution differs from that given, and I am trying to find out why. First we note that the characteristic equation is $$r^5 + 2r^3 + 1 = r(r^2 + 1)^2$$ whose roots are $0$ and $\pm i$ twice. So the complementary function to the homogeneous case $L[y]=0$ is  $$C_0 + C_1 e^{ix} + C_2 e^{-ix} + C_3 x e^{ix} + C_4 x e^{-ix}.$$ Now to tackle the particular solution. So I thought if I solve the differential equation for each of the terms of the right hand side, namely Solve $L[y]=2x$, $L[y] = \sin x$ and $L[y] = \cos x$ then add them up, I will get a particular solution. The first involving a linear function is easy, $y_p$ for that just works out as $x^2$. However for the other two involving trigonometric functions, I try and solve $L[y] = e^{ix}$ and take the imaginary and real parts respectively for $L[y]=\sin x$ and $L[y]=\cos x$. Now as my R.H.S. is $e^{ix}$ which is already contained in the complementary function, I make the ansatz that my $y_p$ should be of the form $Bx^2 e^{ix}$. So calculating substituting this ansatz for $y_p$ into the differential equation $L[y] = e^{ix}$ and comparing coefficients, I get that $B = \frac{i}{8}$, so that $$y_p = \frac{ie^{ix}}{8}$$ and $$\text{Re}(y_p) = \frac{- \sin x}{8},\qquad \text{Im}(y_p) = \frac{\cos x}{8}.$$ So the full solution to my differential equation $L[y] = 2x + \sin x + \cos x$ is $$y_c + y_p = C_0 + C_1 e^{ix} + C_2 e^{-ix} + C_3 x e^{ix} + C_4 x e^{-ix} + x^2 + \frac{- \sin x}{8} + \frac{\cos x}{8}.$$ But when I check the solution given it says that my cosine and sine terms have to have a $x^2$ term in the front of them. What's wrong, is my initial guess not right? Ben","I'm trying to solve the differential equation $$L[y] = y^{(5)} + 2y^{(3)} + y' = 2x + \sin(x) + \cos(x)$$ using the method of undetermined coefficients. I'm having a problem in that my solution differs from that given, and I am trying to find out why. First we note that the characteristic equation is $$r^5 + 2r^3 + 1 = r(r^2 + 1)^2$$ whose roots are $0$ and $\pm i$ twice. So the complementary function to the homogeneous case $L[y]=0$ is  $$C_0 + C_1 e^{ix} + C_2 e^{-ix} + C_3 x e^{ix} + C_4 x e^{-ix}.$$ Now to tackle the particular solution. So I thought if I solve the differential equation for each of the terms of the right hand side, namely Solve $L[y]=2x$, $L[y] = \sin x$ and $L[y] = \cos x$ then add them up, I will get a particular solution. The first involving a linear function is easy, $y_p$ for that just works out as $x^2$. However for the other two involving trigonometric functions, I try and solve $L[y] = e^{ix}$ and take the imaginary and real parts respectively for $L[y]=\sin x$ and $L[y]=\cos x$. Now as my R.H.S. is $e^{ix}$ which is already contained in the complementary function, I make the ansatz that my $y_p$ should be of the form $Bx^2 e^{ix}$. So calculating substituting this ansatz for $y_p$ into the differential equation $L[y] = e^{ix}$ and comparing coefficients, I get that $B = \frac{i}{8}$, so that $$y_p = \frac{ie^{ix}}{8}$$ and $$\text{Re}(y_p) = \frac{- \sin x}{8},\qquad \text{Im}(y_p) = \frac{\cos x}{8}.$$ So the full solution to my differential equation $L[y] = 2x + \sin x + \cos x$ is $$y_c + y_p = C_0 + C_1 e^{ix} + C_2 e^{-ix} + C_3 x e^{ix} + C_4 x e^{-ix} + x^2 + \frac{- \sin x}{8} + \frac{\cos x}{8}.$$ But when I check the solution given it says that my cosine and sine terms have to have a $x^2$ term in the front of them. What's wrong, is my initial guess not right? Ben",,[]
95,Proof of global Peano existence theorem in ZF without mathematical logic,Proof of global Peano existence theorem in ZF without mathematical logic,,"There is a proof of Peano existence theorem in ZF. Peano existence theorem: For any open $D \subseteq \mathbb{R}^2$ , continuous $f:D \to \mathbb{R}$ and initial condition $\langle t_0,x_0\rangle \in D$ , there is an open interval $I \subseteq \mathbb{R}$ with $t_0 \in I$ and a differentiable function $X: I \to \mathbb{R}$ such that $X(t_0)=x_0$ and $X'(t) = f(X(t),t)$ for all $t \in I$ , and no strictly larger $I_\ast \supset I$ has such a function extending $f$ . reference: https://mathoverflow.net/a/455875 The proof in the link uses a theorem in mathematical logic called Shoenfield absoluteness to automatically translate ZFC proofs of low quantifier complexity statements about countably coded objects to ZF proofs. I am not familiar with mathematical logic and advanced set theory, nor do I understand Shoenfield absoluteness. Even if I understand it, I expect the proof generated by automatic translation to be not very human readable. Is there a proof of this theorem in ZF that can be understood by an undergraduate student who is not a set theory major?","There is a proof of Peano existence theorem in ZF. Peano existence theorem: For any open , continuous and initial condition , there is an open interval with and a differentiable function such that and for all , and no strictly larger has such a function extending . reference: https://mathoverflow.net/a/455875 The proof in the link uses a theorem in mathematical logic called Shoenfield absoluteness to automatically translate ZFC proofs of low quantifier complexity statements about countably coded objects to ZF proofs. I am not familiar with mathematical logic and advanced set theory, nor do I understand Shoenfield absoluteness. Even if I understand it, I expect the proof generated by automatic translation to be not very human readable. Is there a proof of this theorem in ZF that can be understood by an undergraduate student who is not a set theory major?","D \subseteq \mathbb{R}^2 f:D \to \mathbb{R} \langle t_0,x_0\rangle \in D I \subseteq \mathbb{R} t_0 \in I X: I \to \mathbb{R} X(t_0)=x_0 X'(t) = f(X(t),t) t \in I I_\ast \supset I f","['ordinary-differential-equations', 'logic', 'reference-request', 'set-theory', 'axiom-of-choice']"
96,Proper linearization of ODEs of the form $\dot{x}(t) + f(x(t)) + \sigma(t) = 0$?,Proper linearization of ODEs of the form ?,\dot{x}(t) + f(x(t)) + \sigma(t) = 0,"For a scalar ODE of the form $$\dot{x}(t) + f\left(x(t)\right) = 0 \label{1}\tag{1}$$ where $f \colon \mathbb R \to \mathbb R$ is some smooth function admitting a unique root $x^*$ such that $f(x^*) = 0$ . The linearization of \eqref{1} is straightforward as we can insert $$ x(t) = x^* + \varepsilon g(t) \label{2}\tag{2}$$ into \eqref{1}, where $0 < |\varepsilon| \ll 1$ , to see that the equation for $g$ (neglecting $\mathcal{O}(\varepsilon^2)$ terms) will be $$ \dot{g}(t) + f'(x^*) g(t) = 0 \label{3} \tag{3}.$$ However, suppose that we want to perturb the ODE \eqref{1} by some external signal modelled by another time-dependent function $\sigma(t)$ (whose precise expression is not available) such that $\sigma(t) \to 0$ as $ t \to \infty$ (we can also impose the condition that $|\sigma(t)|$ is bounded by some exponentially decaying function $\mathrm{e}^{-\lambda t}$ ), i.e., we consider the perturbed ODE $$\dot{x}(t) + f\left(x(t)\right) + \sigma(t) = 0 \label{4}\tag{4}$$ such that $x^*$ remains to be a long-time equilibrium state. May I know how can we can ""linearize"" the equation \eqref{4}? Apparently, employing the ansatz \eqref{2} will not give us a equation for $g$ at the order of $\varepsilon$ ...","For a scalar ODE of the form where is some smooth function admitting a unique root such that . The linearization of \eqref{1} is straightforward as we can insert into \eqref{1}, where , to see that the equation for (neglecting terms) will be However, suppose that we want to perturb the ODE \eqref{1} by some external signal modelled by another time-dependent function (whose precise expression is not available) such that as (we can also impose the condition that is bounded by some exponentially decaying function ), i.e., we consider the perturbed ODE such that remains to be a long-time equilibrium state. May I know how can we can ""linearize"" the equation \eqref{4}? Apparently, employing the ansatz \eqref{2} will not give us a equation for at the order of ...",\dot{x}(t) + f\left(x(t)\right) = 0 \label{1}\tag{1} f \colon \mathbb R \to \mathbb R x^* f(x^*) = 0  x(t) = x^* + \varepsilon g(t) \label{2}\tag{2} 0 < |\varepsilon| \ll 1 g \mathcal{O}(\varepsilon^2)  \dot{g}(t) + f'(x^*) g(t) = 0 \label{3} \tag{3}. \sigma(t) \sigma(t) \to 0  t \to \infty |\sigma(t)| \mathrm{e}^{-\lambda t} \dot{x}(t) + f\left(x(t)\right) + \sigma(t) = 0 \label{4}\tag{4} x^* g \varepsilon,"['ordinary-differential-equations', 'nonlinear-system', 'stability-in-odes', 'linearization']"
97,How does this expression follow algebraically from the last one?,How does this expression follow algebraically from the last one?,,"I was reading this paper: Global stability for an HIV/AIDS epidemic model with different latent stages and treatment Everything is understood apart from on page 7 of the pdf (page 1486 in the document). How does the author algebraically go from the second line to the last line for the equation of $\dot V$ ? I don't understand how he ""generates"" more terms in the last line compared to the one preceding it. EDIT: For those who cannot access the paper The system: \begin{align*} \dot S &= \Lambda - (\beta_1 S I_2 +\beta_2 S J )-\mu S \\ \dot I_1 &= p\beta_1 S I_2  +q\beta_2 S J  +\xi_1 J -b_1 I_1\\ \dot I_2 &= (1-P)\beta_1 S I_2  +(1-q)\beta_2 S J +\epsilon I_1 +\xi_2 J -b_2 I_2\\ \dot J &= p_1 I_2 -b_3 J\\ \dot A &= p_2 J - b_4 A \end{align*} Equilibrium point: \begin{align*} S^* &= \frac{\Lambda}{\mu \mathcal{R}_0}\\ I_1^* &= \frac{1}{b_1}\left[ p \beta_1 \frac{\Lambda b_3 }{\left(\beta_1 b_3 +\beta_2 p_1 \right)J^* +\mu p_1}\right. +\left. q \beta_2 \frac{\Lambda  p_1}{\left(\beta_1 b_3 +\beta_2 p_1 \right)J^* +\mu p_1}+\xi_1\right]J^*\\ I_2^* &= \frac{b_3}{p_1}J^*\\ J^*&= \frac{\mu  p_1 }{\beta_1 b_3  +\beta_2 p_1}\left(\mathcal{R}_0-1\right)\\ A^*&=\frac{p_2}{b_4}J^*  \end{align*} Theorem: If $p=q$ and $\mathcal{R}_0 >1$ then the above equilibrium point is globally stable. Proof: Define Lyapunov function: $$V = S-S^* \ln S+ B( I_1-{I_1}^* \ln I_1) +C(I_2-{I_2}^* \ln I_2) + D( J -J^* \ln J)$$ Derivative is given by: $$\dot V =\left(1-\frac{S^*}{S}\right) \dot S + B\left(1-\frac{{I_1}^*}{I_1} \right)\dot I_1  + C\left(1-\frac{{I_2}^*}{I_2}\right)\dot I_2+D\left(1-\frac{J^*}{J} \right)\dot J$$ The author then does substitutions i.e replaces $\Lambda$ , $b_1$ , $b_2$ , $b_3$ by making the original system equal to $0$ . He finds the constants $B$ , $C$ and $D$ by killing the variable co-efficients, giving: \begin{align*} B &= \frac{\epsilon}{\epsilon p +b_1(1-p)}\\ C&= \frac{b_1}{\epsilon p +b_1(1-p)}\\ D &= \frac{b1 b_2}{p_1[\epsilon p +b_1(1-p)]} -\frac{\beta_1 S^*}{p_1} \end{align*} Next he does another substitution $ x=\dfrac{S}{S^*}$ , $y=\dfrac{I_1}{I_1^*}$ , $z=\dfrac{I_2}{I_2^*}$ and $u=\dfrac{J}{J^*}$ to which he arrives at: \begin{align*} \dot V &= -\mu S^* \frac{\left(1-x\right)^2}{x}+ \left[ \beta_1 S^* {I_2}^*+\beta_2 S^* J^*+B p\beta_1 S^* {I_2}^* + B q \beta_2 S^* J^*\right.\\ &+ B \xi_1 J^*+ C(1-p)\beta_1 S^* {I_2}^*+C(1-q)\beta_2 S^* J^*\\ &+\left. C \epsilon {I_1}^*+C \xi_2 J^*+D p_1 {I_2}^*\right]-x\left[ C(1-p)\beta_1 S^* {I_2}^* \right] -\frac{xz}{y}B p\beta_1 S^* {I_2}^*\\ & -\frac{xu}{y}B q \beta_2 S^* J^* -\frac{u}{y}B \xi_1 J^* - \frac{xu}{z}C(1-q)\beta_2 S^* J^*\\ &- \frac{y}{z}C \epsilon {I_1}^* - \frac{u}{z}C \xi_2 J^*-\frac{z}{u}D p_1  {I_2}^*\\ &- \frac{1}{x}\left[\beta_1 S^* {I_2}^*+ \beta_2 S^* J^*\right]\\ &= -\mu S^* \frac{\left(1-x\right)^2}{x} + \frac{b_1}{\epsilon p +b_1(1-p)}(1-p)\beta_1 S^* I_2^* \left(2-x-\frac{1}{x}\right)\\ &+\frac{b_1}{\epsilon p +b_1(1-p)}\xi_2 J^*\left(2-\frac{u}{z}-\frac{z}{u}\right)\\ &+\frac{b_1}{\epsilon p +b_1(1-p)}(1-q)\beta_2 S^* J^* \left(3-\frac{1}{x}-\frac{xu}{z}-\frac{z}{u}\right)\\ &+\frac{\epsilon}{\epsilon p +b_1(1-p)}p \beta_1 S^* I_2^*\left(3-\frac{1}{x}-\frac{xz}{y}-\frac{y}{z}\right)\\ &+\frac{\epsilon}{\epsilon p +b_1(1-p)} q\beta_2 S^* J^*\left(4-\frac{1}{x}-\frac{y}{z}-\frac{z}{u}-\frac{xu}{y} \right)\\ &+ \frac{\epsilon}{\epsilon p +b_1(1-p)}\xi_1 J^*\left(3-\frac{y}{z}-\frac{z}{u}-\frac{u}{y} \right) \end{align*} I don't understand how he ""generates"" more terms in the last line compared to the one preceding it.","I was reading this paper: Global stability for an HIV/AIDS epidemic model with different latent stages and treatment Everything is understood apart from on page 7 of the pdf (page 1486 in the document). How does the author algebraically go from the second line to the last line for the equation of ? I don't understand how he ""generates"" more terms in the last line compared to the one preceding it. EDIT: For those who cannot access the paper The system: Equilibrium point: Theorem: If and then the above equilibrium point is globally stable. Proof: Define Lyapunov function: Derivative is given by: The author then does substitutions i.e replaces , , , by making the original system equal to . He finds the constants , and by killing the variable co-efficients, giving: Next he does another substitution , , and to which he arrives at: I don't understand how he ""generates"" more terms in the last line compared to the one preceding it.","\dot V \begin{align*}
\dot S &= \Lambda - (\beta_1 S I_2 +\beta_2 S J )-\mu S \\
\dot I_1 &= p\beta_1 S I_2  +q\beta_2 S J  +\xi_1 J -b_1 I_1\\
\dot I_2 &= (1-P)\beta_1 S I_2  +(1-q)\beta_2 S J +\epsilon I_1 +\xi_2 J -b_2 I_2\\
\dot J &= p_1 I_2 -b_3 J\\
\dot A &= p_2 J - b_4 A
\end{align*} \begin{align*}
S^* &= \frac{\Lambda}{\mu \mathcal{R}_0}\\
I_1^* &= \frac{1}{b_1}\left[ p \beta_1 \frac{\Lambda b_3 }{\left(\beta_1 b_3 +\beta_2 p_1 \right)J^* +\mu p_1}\right.
+\left. q \beta_2 \frac{\Lambda  p_1}{\left(\beta_1 b_3 +\beta_2 p_1 \right)J^* +\mu p_1}+\xi_1\right]J^*\\
I_2^* &= \frac{b_3}{p_1}J^*\\
J^*&= \frac{\mu  p_1 }{\beta_1 b_3  +\beta_2 p_1}\left(\mathcal{R}_0-1\right)\\
A^*&=\frac{p_2}{b_4}J^* 
\end{align*} p=q \mathcal{R}_0 >1 V = S-S^* \ln S+ B( I_1-{I_1}^* \ln I_1) +C(I_2-{I_2}^* \ln I_2) + D( J -J^* \ln J) \dot V =\left(1-\frac{S^*}{S}\right) \dot S + B\left(1-\frac{{I_1}^*}{I_1} \right)\dot I_1  + C\left(1-\frac{{I_2}^*}{I_2}\right)\dot I_2+D\left(1-\frac{J^*}{J} \right)\dot J \Lambda b_1 b_2 b_3 0 B C D \begin{align*}
B &= \frac{\epsilon}{\epsilon p +b_1(1-p)}\\
C&= \frac{b_1}{\epsilon p +b_1(1-p)}\\
D &= \frac{b1 b_2}{p_1[\epsilon p +b_1(1-p)]} -\frac{\beta_1 S^*}{p_1}
\end{align*}  x=\dfrac{S}{S^*} y=\dfrac{I_1}{I_1^*} z=\dfrac{I_2}{I_2^*} u=\dfrac{J}{J^*} \begin{align*}
\dot V &= -\mu S^* \frac{\left(1-x\right)^2}{x}+ \left[ \beta_1 S^* {I_2}^*+\beta_2 S^* J^*+B p\beta_1 S^* {I_2}^* + B q \beta_2 S^* J^*\right.\\
&+ B \xi_1 J^*+ C(1-p)\beta_1 S^* {I_2}^*+C(1-q)\beta_2 S^* J^*\\
&+\left. C \epsilon {I_1}^*+C \xi_2 J^*+D p_1 {I_2}^*\right]-x\left[ C(1-p)\beta_1 S^* {I_2}^* \right] -\frac{xz}{y}B p\beta_1 S^* {I_2}^*\\
& -\frac{xu}{y}B q \beta_2 S^* J^* -\frac{u}{y}B \xi_1 J^* - \frac{xu}{z}C(1-q)\beta_2 S^* J^*\\
&- \frac{y}{z}C \epsilon {I_1}^* - \frac{u}{z}C \xi_2 J^*-\frac{z}{u}D p_1  {I_2}^*\\
&- \frac{1}{x}\left[\beta_1 S^* {I_2}^*+ \beta_2 S^* J^*\right]\\
&= -\mu S^* \frac{\left(1-x\right)^2}{x} + \frac{b_1}{\epsilon p +b_1(1-p)}(1-p)\beta_1 S^* I_2^* \left(2-x-\frac{1}{x}\right)\\
&+\frac{b_1}{\epsilon p +b_1(1-p)}\xi_2 J^*\left(2-\frac{u}{z}-\frac{z}{u}\right)\\
&+\frac{b_1}{\epsilon p +b_1(1-p)}(1-q)\beta_2 S^* J^* \left(3-\frac{1}{x}-\frac{xu}{z}-\frac{z}{u}\right)\\
&+\frac{\epsilon}{\epsilon p +b_1(1-p)}p \beta_1 S^* I_2^*\left(3-\frac{1}{x}-\frac{xz}{y}-\frac{y}{z}\right)\\
&+\frac{\epsilon}{\epsilon p +b_1(1-p)} q\beta_2 S^* J^*\left(4-\frac{1}{x}-\frac{y}{z}-\frac{z}{u}-\frac{xu}{y} \right)\\
&+ \frac{\epsilon}{\epsilon p +b_1(1-p)}\xi_1 J^*\left(3-\frac{y}{z}-\frac{z}{u}-\frac{u}{y} \right)
\end{align*}",['ordinary-differential-equations']
98,Constructing a Lyapunov function for an ODE system that describes epidemic spreading on scale-free networks,Constructing a Lyapunov function for an ODE system that describes epidemic spreading on scale-free networks,,"I was recently studying an epidemic spreading model, where two competing viruses spread over a scale-free network. $$ \begin{aligned} \frac{dI_{1,k}(t)}{dt} = - I_{1,k}(t) + \psi_1 k (1-I_{1,k} - I_{2,k}) \Theta_1(t)\\ \frac{dI_{2,k}(t)}{dt} = - I_{2,k}(t) + \psi_2 k (1-I_{1,k} - I_{2,k}) \Theta_2(t), \end{aligned} $$ where $\Theta_1(t) =  \frac{\sum_{k'}k'P(k')I_{1,k'}}{\langle k \rangle}$ and $\Theta_2(t) =  \frac{\sum_{k'}k'P(k')I_{2,k'}}{\langle k \rangle}$ . Here is the interpretation: $k$ represents the degree of a vertex. There are only finite degrees. $P(k)$ is the portion of the vertices that has degree $k$ , hence $\sum_{k'}P(k') =1$ . $\langle k \rangle \triangleq \sum_{k'} k'P(k')$ is the average degree. $I_{1,k}$ represents the portion of nodes that are infected by virus $1$ among the nodes with degree $k$ . $0 \leq I_{1,k},I_{2,k}$ and $I_{1,k}+I_{2,k}\leq 1$ . The term $-I_{1,k}(t)$ in the ODEs gives the recovery speed. The term $\psi_1 k (1-I_{1,k} - I_{2,k}) \Theta_1(t)$ is the transmission speed. Checking the steady-state by setting $$ \begin{aligned}  - I_{1,k}(t) + \psi_1 k (1-I_{1,k} - I_{2,k}) \Theta_1(t) = 0\\  - I_{2,k}(t) + \psi_2 k (1-I_{1,k} - I_{2,k}) \Theta_2(t) = 0, \end{aligned} $$ gives the equilibrium $(I_{1,k}^*,I_{2,k}^*)= (I_{1,k}^*,0)$ when $\psi_1 > \psi_2$ , where $I_{i,k}^*$ satisfies the relation $I_{i,k}^* = \frac{\psi_1 k \Theta_1^*}{1 + \psi_1 k \Theta_1^*}$ for all $k$ . The question is how to show this equilibrium is actually globally stable whenever $0< I_{1,k}(0)$ . Similation shows global stability. But I have not found a Lypunov function to show theoretical guarantees. I have tried several forms of Lyapunov candidates $$ \begin{aligned} V(t)= \sum_{k} \left\{ b_1(k) (I_{1,k} - I_{1,k}^*)^2 + b_2(k) (I_{2,k}-0)^2 \right\} + \Theta_1 - \Theta_1^* - \ln \frac{\Theta_1}{\Theta_1^*} + \Theta_2 \\ V(t)= \sum_{k} \left\{ b_1(k) (I_{1,k}- I_{1,k}^* - \ln \frac{I_{1,k}}{I_{1,k}^*}) + b_2(k) I_{2,k} \right\} + \Theta_1 - \Theta_1^* - \ln \frac{\Theta_1}{\Theta_1^*} + \Theta_2 \end{aligned}. $$ I was trying to find $b_1(k),b_2(k)$ which are constant functions of $k$ . I tried $b_1(k) = \frac{kP(k)\psi_1}{\langle k \rangle}$ , $b_1(k) = \frac{kP(k)\psi_1}{\langle k \rangle}$ . Just couldn't find my way to show that $\dot{V} \leq 0$ . Do anybody have any idea about how to find a Lyapunov function for this kind of ODE systems? Or any form of Lyapunov candidates should I look after?","I was recently studying an epidemic spreading model, where two competing viruses spread over a scale-free network. where and . Here is the interpretation: represents the degree of a vertex. There are only finite degrees. is the portion of the vertices that has degree , hence . is the average degree. represents the portion of nodes that are infected by virus among the nodes with degree . and . The term in the ODEs gives the recovery speed. The term is the transmission speed. Checking the steady-state by setting gives the equilibrium when , where satisfies the relation for all . The question is how to show this equilibrium is actually globally stable whenever . Similation shows global stability. But I have not found a Lypunov function to show theoretical guarantees. I have tried several forms of Lyapunov candidates I was trying to find which are constant functions of . I tried , . Just couldn't find my way to show that . Do anybody have any idea about how to find a Lyapunov function for this kind of ODE systems? Or any form of Lyapunov candidates should I look after?","
\begin{aligned}
\frac{dI_{1,k}(t)}{dt} = - I_{1,k}(t) + \psi_1 k (1-I_{1,k} - I_{2,k}) \Theta_1(t)\\
\frac{dI_{2,k}(t)}{dt} = - I_{2,k}(t) + \psi_2 k (1-I_{1,k} - I_{2,k}) \Theta_2(t),
\end{aligned}
 \Theta_1(t) =  \frac{\sum_{k'}k'P(k')I_{1,k'}}{\langle k \rangle} \Theta_2(t) =  \frac{\sum_{k'}k'P(k')I_{2,k'}}{\langle k \rangle} k P(k) k \sum_{k'}P(k') =1 \langle k \rangle \triangleq \sum_{k'} k'P(k') I_{1,k} 1 k 0 \leq I_{1,k},I_{2,k} I_{1,k}+I_{2,k}\leq 1 -I_{1,k}(t) \psi_1 k (1-I_{1,k} - I_{2,k}) \Theta_1(t) 
\begin{aligned}
 - I_{1,k}(t) + \psi_1 k (1-I_{1,k} - I_{2,k}) \Theta_1(t) = 0\\
 - I_{2,k}(t) + \psi_2 k (1-I_{1,k} - I_{2,k}) \Theta_2(t) = 0,
\end{aligned}
 (I_{1,k}^*,I_{2,k}^*)= (I_{1,k}^*,0) \psi_1 > \psi_2 I_{i,k}^* I_{i,k}^* = \frac{\psi_1 k \Theta_1^*}{1 + \psi_1 k \Theta_1^*} k 0< I_{1,k}(0) 
\begin{aligned}
V(t)= \sum_{k} \left\{ b_1(k) (I_{1,k} - I_{1,k}^*)^2 + b_2(k) (I_{2,k}-0)^2 \right\} + \Theta_1 - \Theta_1^* - \ln \frac{\Theta_1}{\Theta_1^*} + \Theta_2 \\
V(t)= \sum_{k} \left\{ b_1(k) (I_{1,k}- I_{1,k}^* - \ln \frac{I_{1,k}}{I_{1,k}^*}) + b_2(k) I_{2,k} \right\} + \Theta_1 - \Theta_1^* - \ln \frac{\Theta_1}{\Theta_1^*} + \Theta_2
\end{aligned}.
 b_1(k),b_2(k) k b_1(k) = \frac{kP(k)\psi_1}{\langle k \rangle} b_1(k) = \frac{kP(k)\psi_1}{\langle k \rangle} \dot{V} \leq 0","['ordinary-differential-equations', 'nonlinear-system', 'lyapunov-functions']"
99,Second order ODE of unknown format problem,Second order ODE of unknown format problem,,"I'm looking for a Real function $R(r)$ that satisfies: $$r^2R''+R'+m^2 rR=0$$ where $m\in\Bbb R$ . It looks a bit like an Euler DE but it isn't, and a bit like a Bessel DE but isn't either. Wolfram alpha (link to the ODE) doesn't recognise it and provides no solution unfortunately. I think I need a substitution like $r=f(u)$ to get going. Any help is appreciated. A little background - The equation is the radial equation of two ODEs obtained after separation of variables, of a heat conduction problem (a very thin disk of radius $R_1$ ). Boundary conditions are: $$R(R_0)=0$$ $$R'(R_1)=0$$ I upvoted the first answer because it looked like a good idea but it turned out to be incorrect, as I showed in my comment. In response to commenter 'tomasliam', the Sturm Liouville form of the DE is: $$\frac{\text{d}}{\text{d}r}\left[e^{-1/r}R'(r)\right]+\frac{e^{-1/r}m^2R(r)}{r}=0$$ On request of @themaker: A very thin disc of radius $R_1$ is at temperature $T_i$ . It is insulated on both sides, as well as the outer edge. At $t=0$ the area $[0,R_0]$ is suddenly heated to $T_0$ . What is the temperature evolution $T(t,r)$ of the disc (on $[R_0,R_1]$ )? Fourier heat equation for the disc, taking symmetry into account: $$T_t=\frac{\alpha}r\frac{\partial}{\partial r}\Big(r\frac{\partial T}{\partial r}\Big)$$ $$\frac{T_t}{\alpha}=\frac1r(T_{r}+rT_{rr})$$ For homogeneity, we make a substitution: $$u(t,r)=T(t,r)-T_0$$ $$\frac{u_t}{\alpha}=\frac1r(u_{r}+ru_{rr})$$ Initial: $$u(0,r)=T_i-T_0$$ Boundaries: $$u(t,R_0)=0$$ $$u_r(t,R_1)=0$$ Ansatz: $$u(t,r)=\Theta (t)R(r)$$ Substitute, then divide by $u$ : $$\frac{\Theta'}{\alpha \Theta}=\frac{1}{r}\frac{R'}R+\frac{R''}R=-m^2$$ $$\frac{1}{r}\frac{R'}R+\frac{R''}R=-m^2$$ $$rR''+R'+m^2 rR=0$$ So it looks an error was made in setting up the original ODE! Mea culpa. The solution of the last equation is: $$R(r)=c_1J_0(mr)+c_2Y_0(mr)$$","I'm looking for a Real function that satisfies: where . It looks a bit like an Euler DE but it isn't, and a bit like a Bessel DE but isn't either. Wolfram alpha (link to the ODE) doesn't recognise it and provides no solution unfortunately. I think I need a substitution like to get going. Any help is appreciated. A little background - The equation is the radial equation of two ODEs obtained after separation of variables, of a heat conduction problem (a very thin disk of radius ). Boundary conditions are: I upvoted the first answer because it looked like a good idea but it turned out to be incorrect, as I showed in my comment. In response to commenter 'tomasliam', the Sturm Liouville form of the DE is: On request of @themaker: A very thin disc of radius is at temperature . It is insulated on both sides, as well as the outer edge. At the area is suddenly heated to . What is the temperature evolution of the disc (on )? Fourier heat equation for the disc, taking symmetry into account: For homogeneity, we make a substitution: Initial: Boundaries: Ansatz: Substitute, then divide by : So it looks an error was made in setting up the original ODE! Mea culpa. The solution of the last equation is:","R(r) r^2R''+R'+m^2 rR=0 m\in\Bbb R r=f(u) R_1 R(R_0)=0 R'(R_1)=0 \frac{\text{d}}{\text{d}r}\left[e^{-1/r}R'(r)\right]+\frac{e^{-1/r}m^2R(r)}{r}=0 R_1 T_i t=0 [0,R_0] T_0 T(t,r) [R_0,R_1] T_t=\frac{\alpha}r\frac{\partial}{\partial r}\Big(r\frac{\partial T}{\partial r}\Big) \frac{T_t}{\alpha}=\frac1r(T_{r}+rT_{rr}) u(t,r)=T(t,r)-T_0 \frac{u_t}{\alpha}=\frac1r(u_{r}+ru_{rr}) u(0,r)=T_i-T_0 u(t,R_0)=0 u_r(t,R_1)=0 u(t,r)=\Theta (t)R(r) u \frac{\Theta'}{\alpha \Theta}=\frac{1}{r}\frac{R'}R+\frac{R''}R=-m^2 \frac{1}{r}\frac{R'}R+\frac{R''}R=-m^2 rR''+R'+m^2 rR=0 R(r)=c_1J_0(mr)+c_2Y_0(mr)",['ordinary-differential-equations']
