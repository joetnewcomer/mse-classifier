,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"$A \in M_{n}(\mathbb{R})$ satisfies $A+A^{t}=I$, then does it imply $\text{det}(A)>0$","satisfies , then does it imply",A \in M_{n}(\mathbb{R}) A+A^{t}=I \text{det}(A)>0,"I am stuck on this simple question for a long time. If $A \in M_{n}(\mathbb{R})$ satisfies $A+A^{t}=I$, then does it imply $\text{det}(A)>0$? I tried finding a counter-example as well as tried proving it. But couldn't succeed. One question, which I would like to ask the experts is: How does one Judge the "" Truth/False ness "" of the question by seeing it. Is it something which comes by experience.","I am stuck on this simple question for a long time. If $A \in M_{n}(\mathbb{R})$ satisfies $A+A^{t}=I$, then does it imply $\text{det}(A)>0$? I tried finding a counter-example as well as tried proving it. But couldn't succeed. One question, which I would like to ask the experts is: How does one Judge the "" Truth/False ness "" of the question by seeing it. Is it something which comes by experience.",,['linear-algebra']
1,There is a $3\times 3 $ orthogonal matrix with all non zero entries.?,There is a  orthogonal matrix with all non zero entries.?,3\times 3 ,"Is the statement true? There is a $3\times 3 $ real orthogonal matrix with all non zero entries. for orthogonality, $AA^T=A^TA=I_3$, please give me hint","Is the statement true? There is a $3\times 3 $ real orthogonal matrix with all non zero entries. for orthogonality, $AA^T=A^TA=I_3$, please give me hint",,"['linear-algebra', 'matrices']"
2,"Determining four rational numbers, given their pairwise sums","Determining four rational numbers, given their pairwise sums",,"The pairwise sums of four weights (rational numbers) are: 6, 8, 10, 12, 15, 16. What are the four weights? I tried intuitively solving as well as making equations to solve it. If we assume weights in ascending order as: A, B, C, D, then the smallest two weights should sum to 6 (A + B = 6), and the largest two should sum to 16 (C + D = 16). We can't comment on the order of the rest of the pairs. I tried hit and trial approach with some pairs and working on 4 equations at a time, but I haven't been able to get values which fit all 6 equations. What would be be a good way to solve this?","The pairwise sums of four weights (rational numbers) are: 6, 8, 10, 12, 15, 16. What are the four weights? I tried intuitively solving as well as making equations to solve it. If we assume weights in ascending order as: A, B, C, D, then the smallest two weights should sum to 6 (A + B = 6), and the largest two should sum to 16 (C + D = 16). We can't comment on the order of the rest of the pairs. I tried hit and trial approach with some pairs and working on 4 equations at a time, but I haven't been able to get values which fit all 6 equations. What would be be a good way to solve this?",,"['linear-algebra', 'discrete-mathematics', 'contest-math', 'recreational-mathematics', 'puzzle']"
3,Can two 'different' vector spaces have the same vector?,Can two 'different' vector spaces have the same vector?,,"Consider $ v_1 $ and $ v_2 $: $ \{v_1 \in \mathbb{R}^m\mid v_1 = (x_1,x_2,...,x_{m})\}\\ \{v_2 \in \mathbb{R}^{m+1}\mid v_2 = (x_1,x_2,...,x_m,0)\} $ Is $v_1 = v_2$ even they're belonging to different spaces or in this case, $ \mathbb{R}^m = \mathbb{R}^{m+1}$ when the last coordinate is 0?","Consider $ v_1 $ and $ v_2 $: $ \{v_1 \in \mathbb{R}^m\mid v_1 = (x_1,x_2,...,x_{m})\}\\ \{v_2 \in \mathbb{R}^{m+1}\mid v_2 = (x_1,x_2,...,x_m,0)\} $ Is $v_1 = v_2$ even they're belonging to different spaces or in this case, $ \mathbb{R}^m = \mathbb{R}^{m+1}$ when the last coordinate is 0?",,['linear-algebra']
4,"For all square matrices $A$ and $B$ of the same size, it is true that $(A+B)^2 = A^2 + 2AB + B^2$?","For all square matrices  and  of the same size, it is true that ?",A B (A+B)^2 = A^2 + 2AB + B^2,"The below statement is a true/false exercise. Statement: For all square matrices A and B of the same size, it is true that $(A + B)2 = A^2 + 2AB + B^2$. My thought process: Since it is not a proof, I figure I can show by example and come to a valid conclusion based on such example. My work: Come up with a square matrix A and B let both be a 2 by 2 matrix(rows and cols must be same). Matrix $A$: $A = \begin{array}{ccc} 3 & 5 \\ 4 & 6 \\ \end{array} $ Matrix $B$: $B =  \begin{array}{ccc} 5 & 8 \\ 9 & 4 \\ \end{array} $ $A + B =  \begin{array}{ccc} 8 & 13 \\ 13 & 10 \\ \end{array}$ $(A + B)^2 =  \begin{array}{ccc} 233 & 234 \\ 234 & 264 \\ \end{array}$ $A^2 =  \begin{array}{ccc} 29 & 45 \\ 36 & 56 \\ \end{array}$ $(AB) =  \begin{array}{ccc} 60 & 44 \\ 74 & 56 \\ \end{array}$ $2(AB) =  \begin{array}{ccc} 120 & 88 \\ 234 & 112 \\ \end{array}$ $B^2 =  \begin{array}{ccc} 97 & 72 \\ 81 & 88 \\ \end{array}$ $A^2 + 2AB + B^2 =  \begin{array}{ccc} 246 & 205 \\ 265 & 256 \\ \end{array}$ Based my above work, the answer is false. Is there another way to approach the problem? It seems like a lot of work needed to be done for a true/false question which raised my suspicion about whether there is a better way to look at the problem.","The below statement is a true/false exercise. Statement: For all square matrices A and B of the same size, it is true that $(A + B)2 = A^2 + 2AB + B^2$. My thought process: Since it is not a proof, I figure I can show by example and come to a valid conclusion based on such example. My work: Come up with a square matrix A and B let both be a 2 by 2 matrix(rows and cols must be same). Matrix $A$: $A = \begin{array}{ccc} 3 & 5 \\ 4 & 6 \\ \end{array} $ Matrix $B$: $B =  \begin{array}{ccc} 5 & 8 \\ 9 & 4 \\ \end{array} $ $A + B =  \begin{array}{ccc} 8 & 13 \\ 13 & 10 \\ \end{array}$ $(A + B)^2 =  \begin{array}{ccc} 233 & 234 \\ 234 & 264 \\ \end{array}$ $A^2 =  \begin{array}{ccc} 29 & 45 \\ 36 & 56 \\ \end{array}$ $(AB) =  \begin{array}{ccc} 60 & 44 \\ 74 & 56 \\ \end{array}$ $2(AB) =  \begin{array}{ccc} 120 & 88 \\ 234 & 112 \\ \end{array}$ $B^2 =  \begin{array}{ccc} 97 & 72 \\ 81 & 88 \\ \end{array}$ $A^2 + 2AB + B^2 =  \begin{array}{ccc} 246 & 205 \\ 265 & 256 \\ \end{array}$ Based my above work, the answer is false. Is there another way to approach the problem? It seems like a lot of work needed to be done for a true/false question which raised my suspicion about whether there is a better way to look at the problem.",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
5,"Center of Heisenberg group- Dummit and Foote, pg 54, 2.2","Center of Heisenberg group- Dummit and Foote, pg 54, 2.2",,"Let $H(F)$ be the Heisenberg group over the field $F$ introduced in Exercise 11 of Section 1.4. Determine which matrices lie in the center of $H(F)$ and prove that $Z(H(F))$ is isomorphic to the additive group $F$ . Section 1.4 defines the Heisenberg group $H(F)$ over the field $F$ to be the group of all upper-unitriangular $3\times 3$ -matrices $\begin{pmatrix} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{pmatrix}$ with entries in $F$ . When I worked out the elements in the center, I got its elements such that the diagonal was multiplied by a scalar and the element in the first row,third column possibly non-zero. So, you see that my problem was compounded as I just did not see how the center was isomorphic to only F. Thanks for any help that I might receive.","Let be the Heisenberg group over the field introduced in Exercise 11 of Section 1.4. Determine which matrices lie in the center of and prove that is isomorphic to the additive group . Section 1.4 defines the Heisenberg group over the field to be the group of all upper-unitriangular -matrices with entries in . When I worked out the elements in the center, I got its elements such that the diagonal was multiplied by a scalar and the element in the first row,third column possibly non-zero. So, you see that my problem was compounded as I just did not see how the center was isomorphic to only F. Thanks for any help that I might receive.",H(F) F H(F) Z(H(F)) F H(F) F 3\times 3 \begin{pmatrix} 1 & a & b \\ 0 & 1 & c \\ 0 & 0 & 1 \end{pmatrix} F,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory']"
6,Prove properties of $A^2 = -I$,Prove properties of,A^2 = -I,"Given an $n\times n$ matrix A with real entries such that $A^2=-I$, prove (a) that $n$ is even and (b) that $A$ has no real eigenvalues. How do you do this? I have no idea where to start.","Given an $n\times n$ matrix A with real entries such that $A^2=-I$, prove (a) that $n$ is even and (b) that $A$ has no real eigenvalues. How do you do this? I have no idea where to start.",,['linear-algebra']
7,"Prove that if $\sum_{i=1}^n x_i=0$ and $\sum_{i=1}^n x_i^2=1$ there exist $i,j$ such that $x_ix_j\leq -{1\over n}$",Prove that if  and  there exist  such that,"\sum_{i=1}^n x_i=0 \sum_{i=1}^n x_i^2=1 i,j x_ix_j\leq -{1\over n}","Problem Let $x_1, x_2, \ldots, x_n$ be reals such that \begin{align} \sum_{i=1}^n x_i=0 \qquad \text{ and } \qquad \sum_{i=1}^n x_i^2=1 . \end{align} Prove that there exist $i,j$ such that $x_ix_j\leq -{1\over n}$ . my attempt I found that if there exist $x_i,x_j$ such that $$x_i\geq {1\over\sqrt{n}}, x_j\leq -{1\over \sqrt{n}} $$ then the problem solved, but I can't prove this.","Problem Let be reals such that Prove that there exist such that . my attempt I found that if there exist such that then the problem solved, but I can't prove this.","x_1, x_2, \ldots, x_n \begin{align}
\sum_{i=1}^n x_i=0
\qquad \text{ and } \qquad
\sum_{i=1}^n x_i^2=1 .
\end{align} i,j x_ix_j\leq -{1\over n} x_i,x_j x_i\geq {1\over\sqrt{n}}, x_j\leq -{1\over \sqrt{n}} ","['linear-algebra', 'inequality', 'contest-math', 'quadratic-forms']"
8,What is the intuition behind $x^T A x$?,What is the intuition behind ?,x^T A x,"Consider an $N$-dimensional vector $x$, and a $N \times N$ matrix $A$. Throughout linear algebra, I find the expression $x^T A x$ to be extremely common. Is there some fundamental intuition or geometric meaning behind it?","Consider an $N$-dimensional vector $x$, and a $N \times N$ matrix $A$. Throughout linear algebra, I find the expression $x^T A x$ to be extremely common. Is there some fundamental intuition or geometric meaning behind it?",,"['linear-algebra', 'matrices']"
9,"Is the intersection of two orthogonal planes a line, or the zero vector?","Is the intersection of two orthogonal planes a line, or the zero vector?",,I am struggling to grasp a relatively simple concept in linear algebra: I know that the intersection between two orthogonal subspaces is the zero vector. But I also know that the intersection between two orthogonal planes is a line. A plane is a subspace. But....a line is not the zero vector. Where did I go wrong in my logic?,I am struggling to grasp a relatively simple concept in linear algebra: I know that the intersection between two orthogonal subspaces is the zero vector. But I also know that the intersection between two orthogonal planes is a line. A plane is a subspace. But....a line is not the zero vector. Where did I go wrong in my logic?,,"['linear-algebra', 'vector-spaces']"
10,"Why is the power function considered an algebraic function, but the exponential function is NOT algebraic?","Why is the power function considered an algebraic function, but the exponential function is NOT algebraic?",,"I am curious to know why if a mathematical expression contains an exponential function that expression may NOT be considered an algebraic expression, but if it contains a power function (if the variable is the base of a power expression) then that expression as a whole can be considered an algebraic expression.","I am curious to know why if a mathematical expression contains an exponential function that expression may NOT be considered an algebraic expression, but if it contains a power function (if the variable is the base of a power expression) then that expression as a whole can be considered an algebraic expression.",,"['linear-algebra', 'exponential-function', 'exponentiation']"
11,How to differentiate product of vectors (that gives scalar) by vector?,How to differentiate product of vectors (that gives scalar) by vector?,,"I'm trying to understand derivation of the least squares method in matrices terms: $$S(\beta) = y^Ty - 2 \beta X^Ty + \beta ^ T X^TX \beta$$ Where $\beta$ is $m \times 1$ vertical vector, $X$ is $n \times m$ matrix and $y$ is $n \times 1$ vector. The question is: why $$\frac{d(2\beta X^Ty)}{d \beta} = 2X^Ty$$ I tried to derive it directly via definition of derivative:  $$\frac{d(2\beta X^Ty)}{d \beta} = \lim_{\Delta \beta \to 0} \frac{2\Delta\beta X^T y}{\Delta \beta} = \lim_{\Delta \beta \to 0}   2\Delta\beta X^T y \cdot \Delta \beta^{-1}$$ May be the last equality must be as in the next line, but anyway I don't understand why $$2\Delta\beta \Delta \beta^{-1} X^T y  $$And, what is $\Delta \beta^{-1}$? Vectors don't have the inverse form. The same questions I have to this quasion: $$(\beta ^ T X^TX \beta)' =2 X^T X \beta$$","I'm trying to understand derivation of the least squares method in matrices terms: $$S(\beta) = y^Ty - 2 \beta X^Ty + \beta ^ T X^TX \beta$$ Where $\beta$ is $m \times 1$ vertical vector, $X$ is $n \times m$ matrix and $y$ is $n \times 1$ vector. The question is: why $$\frac{d(2\beta X^Ty)}{d \beta} = 2X^Ty$$ I tried to derive it directly via definition of derivative:  $$\frac{d(2\beta X^Ty)}{d \beta} = \lim_{\Delta \beta \to 0} \frac{2\Delta\beta X^T y}{\Delta \beta} = \lim_{\Delta \beta \to 0}   2\Delta\beta X^T y \cdot \Delta \beta^{-1}$$ May be the last equality must be as in the next line, but anyway I don't understand why $$2\Delta\beta \Delta \beta^{-1} X^T y  $$And, what is $\Delta \beta^{-1}$? Vectors don't have the inverse form. The same questions I have to this quasion: $$(\beta ^ T X^TX \beta)' =2 X^T X \beta$$",,"['linear-algebra', 'derivatives', 'scalar-fields']"
12,"Express the polynomial $x^3-4x-4$ as a linear combination of $x-2$, $(x-2)^2$ and $(x-2)^3$","Express the polynomial  as a linear combination of ,  and",x^3-4x-4 x-2 (x-2)^2 (x-2)^3,"Express the polynomial $x^3-4x-4$ as a linear combination of $x-2$, $(x-2)^2$ and $(x-2)^3$ I've been looking everywhere but I still don't quite understand the question. I know that a linear combination is like a matrix consisting of a specific combination of vectors multiplied by a coefficient. In the form... $$a_1v_1+a_2v_2 +a_3v_3 ,\text{for }a_1 \,to \, a_n \, real \, numbers$$ So to express it as the question asks i think i have to find the coefficients off...  $$x^3-4x-4 = a(x-2)+b(x-2)^2 +c(x-2)^3$$ But i'm not too sure about it or where to go. I thought I'd had to involve vectors and matrices somehow. Please help, I really want to understand this content well, I've been having trouble picking up content from this new class. We've also been talking about basis and span. I think a vector forms a basis for a system. If for a $R^n$ system if for 3 row (have 3 pivots showing 1 = 0, after row reducing) then the vectors/columns corresponding to those rows make a basis for the system and the system hence contains/span all of ""R^3"". EDIT: You guys are right the question given to me was inconsistent. I asked the teacher who then admitted there was a typo and re-wrote the question. I think I've got plenty to work on anyway with this already. He actually meant to ask Express the polynomial $x^3-2x-4$ as a linear combination of $x-2$, $(x-2)^2$ and $(x-2)^3$ To wich i got C=1, B=6, a=10","Express the polynomial $x^3-4x-4$ as a linear combination of $x-2$, $(x-2)^2$ and $(x-2)^3$ I've been looking everywhere but I still don't quite understand the question. I know that a linear combination is like a matrix consisting of a specific combination of vectors multiplied by a coefficient. In the form... $$a_1v_1+a_2v_2 +a_3v_3 ,\text{for }a_1 \,to \, a_n \, real \, numbers$$ So to express it as the question asks i think i have to find the coefficients off...  $$x^3-4x-4 = a(x-2)+b(x-2)^2 +c(x-2)^3$$ But i'm not too sure about it or where to go. I thought I'd had to involve vectors and matrices somehow. Please help, I really want to understand this content well, I've been having trouble picking up content from this new class. We've also been talking about basis and span. I think a vector forms a basis for a system. If for a $R^n$ system if for 3 row (have 3 pivots showing 1 = 0, after row reducing) then the vectors/columns corresponding to those rows make a basis for the system and the system hence contains/span all of ""R^3"". EDIT: You guys are right the question given to me was inconsistent. I asked the teacher who then admitted there was a typo and re-wrote the question. I think I've got plenty to work on anyway with this already. He actually meant to ask Express the polynomial $x^3-2x-4$ as a linear combination of $x-2$, $(x-2)^2$ and $(x-2)^3$ To wich i got C=1, B=6, a=10",,['linear-algebra']
13,Proving Distributivity of Matrix Multiplication,Proving Distributivity of Matrix Multiplication,,"If $A,B,C$ are matrices I am thinking how to show that $$ A(B + C) = AB + AC$$ Is possible to show without sums like $\sum_i a_i, ..., \sum_j b_j$? It seems if I do the proof with many indexes then is tedious and I don't learn much from it.","If $A,B,C$ are matrices I am thinking how to show that $$ A(B + C) = AB + AC$$ Is possible to show without sums like $\sum_i a_i, ..., \sum_j b_j$? It seems if I do the proof with many indexes then is tedious and I don't learn much from it.",,['linear-algebra']
14,Having difficulty understanding what happens when reversing order of quaternion multiplication.,Having difficulty understanding what happens when reversing order of quaternion multiplication.,,The textbook I am reading claims that quaternion multiplication works like so: $ q_1q_2 = V_1 \times V_2 + s_1V_2+s_2V_1+s_1s_2-V_1 \cdot V_2 $ Which is a simplified view of $ q_1q_2 = (x_1w_2+y_1z_2-z_1y_2+w_1x_2)i\\ +(y_1w_2+z_1x_2+w_1y_2-x_1z_2)j\\ +(z_1w_2+w_1z_2+x_1y_2-y_1x_2)k\\ +(w_1w_2-x_1x_2-y_1y_2-z_1z_2) $ where $q$ is defined as: $q=xi+yj+zk+w$ Which makes sense to me if you take into consideration the multiplication rule for quaternions: $i^2=j^2=k^2=ijk=-1$ The thing that I don't understand is how reversing the order of quaternion multiplication works. The textbook defines it as: $q_2q_1=q_1q_2-2(V_1 \times V_2)$ However I have no idea how to go about obtaining this result.,The textbook I am reading claims that quaternion multiplication works like so: Which is a simplified view of where is defined as: Which makes sense to me if you take into consideration the multiplication rule for quaternions: The thing that I don't understand is how reversing the order of quaternion multiplication works. The textbook defines it as: However I have no idea how to go about obtaining this result.,"
q_1q_2 = V_1 \times V_2 + s_1V_2+s_2V_1+s_1s_2-V_1 \cdot V_2
 
q_1q_2 = (x_1w_2+y_1z_2-z_1y_2+w_1x_2)i\\
+(y_1w_2+z_1x_2+w_1y_2-x_1z_2)j\\
+(z_1w_2+w_1z_2+x_1y_2-y_1x_2)k\\
+(w_1w_2-x_1x_2-y_1y_2-z_1z_2)
 q q=xi+yj+zk+w i^2=j^2=k^2=ijk=-1 q_2q_1=q_1q_2-2(V_1 \times V_2)","['linear-algebra', 'quaternions']"
15,What is the benefit of defining a positive norm for vectors?,What is the benefit of defining a positive norm for vectors?,,"I read that the reason we have the property $\langle A|B\rangle=\langle B|A\rangle^*$ is to make define a positive norm with the formula $\langle A|A\rangle$ . But I do not understand how having this norm benefits us. I guess we're doing this to make an analogy with arrows, which also have a positive norm. But this can't be the only reason. After all, a lot of things which are true for arrows are not true for general vectors. For instance, angle values of $-2\pi$ to $2\pi$ are not carried over from arrows to general vectors. The formula for the angle between general vectors, $\cos \theta=\frac{\langle A|B\rangle}{|A||B|}$ , can result in complex values of $\theta$ . The commutativity of the inner product isn't carried over from arrows to general vectors either (though this is the very reason a positive norm gets carried over). Keeping a positive norm for general vectors must be allowing us to carry over some nice properties from the world of arrows to general vectors. What are those nice things? Like, even if we drop this property, we'd still be able to prove the existence of an orthonormal basis, as Gram Schmidt does not require $\langle A|B\rangle=\langle B|A\rangle ^*$ . So at least that stuff still works out. EDIT- I just realised that, while Gram Schmidt may not require $\langle V|V\rangle$ to be strictly positive, it does require $\langle V|V\rangle$ not to be 0 for non-zero vectors $|V\rangle$ , because only then can we rescale the basis vectors by their norm to get a unit vector. EDIT- I also realised that the Cauchy Schwarz and Triangle Inequalities would no longer make sense without this norm. Maybe these are useful results too.","I read that the reason we have the property is to make define a positive norm with the formula . But I do not understand how having this norm benefits us. I guess we're doing this to make an analogy with arrows, which also have a positive norm. But this can't be the only reason. After all, a lot of things which are true for arrows are not true for general vectors. For instance, angle values of to are not carried over from arrows to general vectors. The formula for the angle between general vectors, , can result in complex values of . The commutativity of the inner product isn't carried over from arrows to general vectors either (though this is the very reason a positive norm gets carried over). Keeping a positive norm for general vectors must be allowing us to carry over some nice properties from the world of arrows to general vectors. What are those nice things? Like, even if we drop this property, we'd still be able to prove the existence of an orthonormal basis, as Gram Schmidt does not require . So at least that stuff still works out. EDIT- I just realised that, while Gram Schmidt may not require to be strictly positive, it does require not to be 0 for non-zero vectors , because only then can we rescale the basis vectors by their norm to get a unit vector. EDIT- I also realised that the Cauchy Schwarz and Triangle Inequalities would no longer make sense without this norm. Maybe these are useful results too.",\langle A|B\rangle=\langle B|A\rangle^* \langle A|A\rangle -2\pi 2\pi \cos \theta=\frac{\langle A|B\rangle}{|A||B|} \theta \langle A|B\rangle=\langle B|A\rangle ^* \langle V|V\rangle \langle V|V\rangle |V\rangle,"['linear-algebra', 'normed-spaces', 'inner-products']"
16,Does there exist a nonzero ring homomorphism from the ring of square rational matrices to the ring of rational numbers?,Does there exist a nonzero ring homomorphism from the ring of square rational matrices to the ring of rational numbers?,,"I am wondering if it is possible to construct a nonzero ring homomorphism from $M_n(\mathbb{Q})$ to $\mathbb{Q}$ . So far, I've been unsuccessful in constructing such a nonzero ring homomorphism. Is there a possible construction? If not, how can we prove this? Thanks!","I am wondering if it is possible to construct a nonzero ring homomorphism from to . So far, I've been unsuccessful in constructing such a nonzero ring homomorphism. Is there a possible construction? If not, how can we prove this? Thanks!",M_n(\mathbb{Q}) \mathbb{Q},"['linear-algebra', 'abstract-algebra', 'rational-numbers', 'ring-homomorphism']"
17,Spectral radius is not matrix norm.,Spectral radius is not matrix norm.,,I have seen an example of matrix $$A = \begin{bmatrix} 			0 & 1  \\ 			0 & 0 		\end{bmatrix}$$ whose spectral radius is zero therefore the spectral radius is not matrix norm. Why the spectral radius is not matrix norm in this case Is it possible that $\|A\|=\epsilon$?,I have seen an example of matrix $$A = \begin{bmatrix} 			0 & 1  \\ 			0 & 0 		\end{bmatrix}$$ whose spectral radius is zero therefore the spectral radius is not matrix norm. Why the spectral radius is not matrix norm in this case Is it possible that $\|A\|=\epsilon$?,,"['linear-algebra', 'matrices', 'normed-spaces', 'spectral-radius']"
18,"If $A$ is an $n \times n$ matrix and $ A^2 = 0$, then $\text{rank}(A)\le n/2$.","If  is an  matrix and , then .",A n \times n  A^2 = 0 \text{rank}(A)\le n/2,"Was revising for a Linear Algebra when I came across this question. ""Given a Matrix $A$ over $\mathbb{R}^{n \times n}$, and $A^2 = 0$,   show that rank($A$) $\leq n/2$"" My attempt: I was thinking of the theorem from Cayley-Hamilton but the matrix is a $n \times n$ matrix hence that's probably invalid. Is there perhaps a significance of $A^2 = 0$? apart from a special case of the nilpotent matrix?","Was revising for a Linear Algebra when I came across this question. ""Given a Matrix $A$ over $\mathbb{R}^{n \times n}$, and $A^2 = 0$,   show that rank($A$) $\leq n/2$"" My attempt: I was thinking of the theorem from Cayley-Hamilton but the matrix is a $n \times n$ matrix hence that's probably invalid. Is there perhaps a significance of $A^2 = 0$? apart from a special case of the nilpotent matrix?",,['linear-algebra']
19,"How to minimize a determinant, without a computer?","How to minimize a determinant, without a computer?",,"I was asked to find the minimum value of a determinant of order $3\times 3$ having elements $-1$ and $1$ , and after trial and  error, was able to come up with this $$ \begin{vmatrix} -1 & 1 & -1 \\  1& 1 & 1 \\ -1 & - 1& 1 \\ \end{vmatrix} = -4$$ However, this seems like a brutalist approach, and was looking for simpler methods. I have come across answers like this on maximizing and minimising a 3 by 3 matrix , however, as this is for school, I can't use computers for more complicated problems","I was asked to find the minimum value of a determinant of order having elements and , and after trial and  error, was able to come up with this However, this seems like a brutalist approach, and was looking for simpler methods. I have come across answers like this on maximizing and minimising a 3 by 3 matrix , however, as this is for school, I can't use computers for more complicated problems","3\times 3 -1 1 
\begin{vmatrix}
-1 & 1 & -1 \\
 1& 1 & 1 \\
-1 & - 1& 1 \\
\end{vmatrix}
= -4","['linear-algebra', 'matrices', 'determinant', 'maxima-minima']"
20,Determinant of a matrix and linear independence (explanation needed),Determinant of a matrix and linear independence (explanation needed),,"It is written on Wikipedia that: $n$ vectors in $\mathbb R^n$ are linearly independent if and only if the determinant of the matrix formed by taking the vectors as its columns is non-zero Can someone explain this to me? You do not have to give a complete proof, just in simple terms explain what the determinant of that matrix has to do with linear independence? And why it has to be non-zero? And are vectors allowed to be rows instead of columns in that matrix?","It is written on Wikipedia that: $n$ vectors in $\mathbb R^n$ are linearly independent if and only if the determinant of the matrix formed by taking the vectors as its columns is non-zero Can someone explain this to me? You do not have to give a complete proof, just in simple terms explain what the determinant of that matrix has to do with linear independence? And why it has to be non-zero? And are vectors allowed to be rows instead of columns in that matrix?",,"['linear-algebra', 'matrices', 'determinant']"
21,Determinant of Matrix with uncomputable values.,Determinant of Matrix with uncomputable values.,,"Calculate the determinant of the matrix   $$ \begin{pmatrix}   10^{10} & 10^{10^{10}} & 11^{11^{11}} & 1 & 0 \\   2^{2^2} & 3^{3^3} & 7^{7^7} & 0 & 1 \\   11 & 17 & 12 & 2 & 7 \\   2 & 3 & 5 & 1 & 1 \\   9 & 14 & 7 & 1 & 6 \\ \end{pmatrix} $$ My wonderful Russian Professor put this one up on the board. Obviously it's not something one can put in Wolfram Alpha. I can't see any obvious linear dependencies between rows or columns, I've tried assigning variables to the big values (just for ease of notation) and doing row operations, transposing to get it into upper triangular it all still ends in a mess. Now, I know our Prof HATES computation and is terrible at it, so he wouldn't write this up unless there is some structural simplicity I can't see...?","Calculate the determinant of the matrix   $$ \begin{pmatrix}   10^{10} & 10^{10^{10}} & 11^{11^{11}} & 1 & 0 \\   2^{2^2} & 3^{3^3} & 7^{7^7} & 0 & 1 \\   11 & 17 & 12 & 2 & 7 \\   2 & 3 & 5 & 1 & 1 \\   9 & 14 & 7 & 1 & 6 \\ \end{pmatrix} $$ My wonderful Russian Professor put this one up on the board. Obviously it's not something one can put in Wolfram Alpha. I can't see any obvious linear dependencies between rows or columns, I've tried assigning variables to the big values (just for ease of notation) and doing row operations, transposing to get it into upper triangular it all still ends in a mess. Now, I know our Prof HATES computation and is terrible at it, so he wouldn't write this up unless there is some structural simplicity I can't see...?",,"['linear-algebra', 'determinant']"
22,"If $\,A^3-A+I=0,\,$ then $A$ is invertible",If  then  is invertible,"\,A^3-A+I=0,\, A","Prove or disprove. If $A$ is a square matrix and $A^3-A+I=0,$ then $A$ is invertible. Is it possible to say the characteristic polynomial of $A$ is $\,p(t)=t^3-t+1$, and $A$ is invertible since $0$ is not an eigenvalue of the characteristic polynomial?","Prove or disprove. If $A$ is a square matrix and $A^3-A+I=0,$ then $A$ is invertible. Is it possible to say the characteristic polynomial of $A$ is $\,p(t)=t^3-t+1$, and $A$ is invertible since $0$ is not an eigenvalue of the characteristic polynomial?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'matrix-calculus']"
23,How should I prove $(a+b)^3= a^3+3ab(a+b)+b^3$ --- Model or figure?,How should I prove  --- Model or figure?,(a+b)^3= a^3+3ab(a+b)+b^3,"In what way can I prove/verify $(a+b)^3= a^3+3ab(a+b)+b^3$ ? Should I make a 3D model, or create 2D figure? In the case of 3D model, I have made $a^3$ and $b^3$; i.e cube'a' and cube'b'. I don't know what to do with $3a^2b+ 3b^2a$, in case I'm making a model. Previously, I made a figure by cutting paper pieces to verify $(a+b+c)^2$. I want to know whether model can be made or not. If not, then how must I prove $(a+b)^3=a^3+3ab(a+b)+b^3$ ?","In what way can I prove/verify $(a+b)^3= a^3+3ab(a+b)+b^3$ ? Should I make a 3D model, or create 2D figure? In the case of 3D model, I have made $a^3$ and $b^3$; i.e cube'a' and cube'b'. I don't know what to do with $3a^2b+ 3b^2a$, in case I'm making a model. Previously, I made a figure by cutting paper pieces to verify $(a+b+c)^2$. I want to know whether model can be made or not. If not, then how must I prove $(a+b)^3=a^3+3ab(a+b)+b^3$ ?",,"['linear-algebra', 'algebra-precalculus', 'geometry']"
24,Union of two subspaces versus intersection of two subspaces,Union of two subspaces versus intersection of two subspaces,,"According to the definition, the union of two subspaces is not a subspace. That is easily proved to be true. For instance, Let $U$ contain the general vector $(x,0)$, and $W$ contain the general vector $(0,y).$ Clearly, the union of these two subspaces would not be in either of the subspaces as it will violate closure axioms. As for the intersection of the two subspaces, I believe I understand the concept. However, I want to be sure of that, and I believe it comes down to the difference between union and intersection as applied to vector/subspaces. Basically, union - in this context - is being used to indicate that vectors can be taken from both subspaces, but when operated upon they have to be in one or the other subspace. Intersection, on the other hand, also means that vectors from both subspaces can be taken. But, a new subspace is formed by combining both subspaces into one. To explain, I'll use the same subspaces as above. Let $U$ contain the general vector $(x,0)$, and $W$ contain the general vector $(0,y).$ So, the intersection of $U$ and $V$ would contain the general vector $(x,y)$ (I should say based on what I said above). Therefore, the closure axioms are fulfilled. Am I correct in my reasoning? Any feedback is appreciated.","According to the definition, the union of two subspaces is not a subspace. That is easily proved to be true. For instance, Let $U$ contain the general vector $(x,0)$, and $W$ contain the general vector $(0,y).$ Clearly, the union of these two subspaces would not be in either of the subspaces as it will violate closure axioms. As for the intersection of the two subspaces, I believe I understand the concept. However, I want to be sure of that, and I believe it comes down to the difference between union and intersection as applied to vector/subspaces. Basically, union - in this context - is being used to indicate that vectors can be taken from both subspaces, but when operated upon they have to be in one or the other subspace. Intersection, on the other hand, also means that vectors from both subspaces can be taken. But, a new subspace is formed by combining both subspaces into one. To explain, I'll use the same subspaces as above. Let $U$ contain the general vector $(x,0)$, and $W$ contain the general vector $(0,y).$ So, the intersection of $U$ and $V$ would contain the general vector $(x,y)$ (I should say based on what I said above). Therefore, the closure axioms are fulfilled. Am I correct in my reasoning? Any feedback is appreciated.",,"['linear-algebra', 'vector-spaces']"
25,Sufficient condition for a matrix to be diagonalizable and similar matrices,Sufficient condition for a matrix to be diagonalizable and similar matrices,,"my question is about diagonalizable matrices and similar matrices. I have a trouble proving a matrix is diagonalizable. I know some options to do that: Matrix $A$ $(n \times n)$, is diagonalizable if: Number of eigenvectors equals to number of eigenvalues. There exists an invertible matrix $B$ and a diagonal matrix $D$ such that: $D=B^{-1}AB$. But i have a trouble to determine it according the second option, Do i really need to search if there exists an invertible matrix $B$ and a diagonal matrix $D$ such that: $D=B^{-1}AB?$ I really sorry to ask an additional question here: If a matrix has a row of $0$'s (one of its eigenvalues is $0$), That matrix is diagonalizable? in general, given a matrix, how do i know if is a diagonalizable matrix? Are there some additional formulas to do that? Thanks for help!!","my question is about diagonalizable matrices and similar matrices. I have a trouble proving a matrix is diagonalizable. I know some options to do that: Matrix $A$ $(n \times n)$, is diagonalizable if: Number of eigenvectors equals to number of eigenvalues. There exists an invertible matrix $B$ and a diagonal matrix $D$ such that: $D=B^{-1}AB$. But i have a trouble to determine it according the second option, Do i really need to search if there exists an invertible matrix $B$ and a diagonal matrix $D$ such that: $D=B^{-1}AB?$ I really sorry to ask an additional question here: If a matrix has a row of $0$'s (one of its eigenvalues is $0$), That matrix is diagonalizable? in general, given a matrix, how do i know if is a diagonalizable matrix? Are there some additional formulas to do that? Thanks for help!!",,"['linear-algebra', 'matrices', 'diagonalization', 'similar-matrices']"
26,Are there ways to solve equations with multiple variables?,Are there ways to solve equations with multiple variables?,,"I am not at a high level in math, so I have a simple question a simple Google search cannot answer, and the other Stack Exchange questions does not either. I thought about this question after reading a creative math book. Here is the question I was doing, which I used the solutions manual in shame(Not exact wording, but same idea): The question in the blockquotes below is not the question I am asking for answers to. Some misunderstood what I am asking. What I am asking is in the last single-sentence paragraph. Suppose $a_2,a_3,a_4,a_5,a_6,a_7$ are integers, where $0\le a_i< i$ . $\frac 57 = \frac {a_2}{2!}+\frac {a_3}{3!}+\frac {a_4}{4!}+\frac {a_5}{5!}+\frac {a_6}{6!}+\frac {a_7}{7!}$ Find $a_2+a_3+a_4+a_5+a_6+a_7$ . The solution to this particular question requires that $a_7$ and the other variables in later steps of the algebra process to be remainders when both sides are to be divided by an integer. I am now wondering what if an equation ever comes up where the method to solve for the question above cannot work due to the variables not being able to return remainders. Thus, my question is whether it is possible to solve algebraic equations with more than two variables and most variables having constant coefficients, is not a system of equations, and the variables are assumed to be integers, and the solution is unique. Does such a way to solve such equations in general exist? If so, please explain. What is this part of math called? Thank you.","I am not at a high level in math, so I have a simple question a simple Google search cannot answer, and the other Stack Exchange questions does not either. I thought about this question after reading a creative math book. Here is the question I was doing, which I used the solutions manual in shame(Not exact wording, but same idea): The question in the blockquotes below is not the question I am asking for answers to. Some misunderstood what I am asking. What I am asking is in the last single-sentence paragraph. Suppose are integers, where . Find . The solution to this particular question requires that and the other variables in later steps of the algebra process to be remainders when both sides are to be divided by an integer. I am now wondering what if an equation ever comes up where the method to solve for the question above cannot work due to the variables not being able to return remainders. Thus, my question is whether it is possible to solve algebraic equations with more than two variables and most variables having constant coefficients, is not a system of equations, and the variables are assumed to be integers, and the solution is unique. Does such a way to solve such equations in general exist? If so, please explain. What is this part of math called? Thank you.","a_2,a_3,a_4,a_5,a_6,a_7 0\le a_i< i \frac 57 = \frac {a_2}{2!}+\frac {a_3}{3!}+\frac {a_4}{4!}+\frac {a_5}{5!}+\frac {a_6}{6!}+\frac {a_7}{7!} a_2+a_3+a_4+a_5+a_6+a_7 a_7","['linear-algebra', 'diophantine-equations', 'integer-programming']"
27,Translating an Italian exercise precisely,Translating an Italian exercise precisely,,"I am trying to help a friend with his algebra course.  However, his exercises are in Italian, and unfortunately he translates them poorly for me since he does not know the mathematical terms in English. I have tried to translate it myself, but without luck. I still do not know what exactly to do. More precisely, it is the description in the exercise and exercise (a) that I do not understand completely; the rest I understand. E2) Sia $f\colon\mathbb{C}^4\to\mathbb{C}^4$ una trasformazione lineare e si supponga che la matrice associata a $f$ rispetto alla base $\mathcal{B} = \{\mathbf{e}_2; \mathbf{e}_1; \mathbf{e}_3+\mathbf{e}_4; \mathbf{e}_3+\mathbf{e}_2\}$ su dominio e codominio ( $\mathbf{e}_i$ sono i vettori della base canonica di $\mathbb{C}^4$ ) sia $$\mathbf{A} = \begin{bmatrix}3&3&2&2\\ 3&3&2&2\\ 0&0&1&1\\ 0&0&1&1\end{bmatrix}$$ (a) Si determini la matrice $\mathbf{B}$ associata a $f$ rispetto alle basi canoniche. (b) Si calcoli la dimensione dell'immagine di $f$ . (c) Si dica se la matrice $\mathbf{B}$ è diagonalizzabile. (d) Si calcoli una base dello spazio nullo dell'applicazione lineare $f$ .","I am trying to help a friend with his algebra course.  However, his exercises are in Italian, and unfortunately he translates them poorly for me since he does not know the mathematical terms in English. I have tried to translate it myself, but without luck. I still do not know what exactly to do. More precisely, it is the description in the exercise and exercise (a) that I do not understand completely; the rest I understand. E2) Sia una trasformazione lineare e si supponga che la matrice associata a rispetto alla base su dominio e codominio ( sono i vettori della base canonica di ) sia (a) Si determini la matrice associata a rispetto alle basi canoniche. (b) Si calcoli la dimensione dell'immagine di . (c) Si dica se la matrice è diagonalizzabile. (d) Si calcoli una base dello spazio nullo dell'applicazione lineare .",f\colon\mathbb{C}^4\to\mathbb{C}^4 f \mathcal{B} = \{\mathbf{e}_2; \mathbf{e}_1; \mathbf{e}_3+\mathbf{e}_4; \mathbf{e}_3+\mathbf{e}_2\} \mathbf{e}_i \mathbb{C}^4 \mathbf{A} = \begin{bmatrix}3&3&2&2\\ 3&3&2&2\\ 0&0&1&1\\ 0&0&1&1\end{bmatrix} \mathbf{B} f f \mathbf{B} f,"['linear-algebra', 'matrices', 'linear-transformations', 'translation-request']"
28,Proving or disproving matrix $A+B$ is invertible,Proving or disproving matrix  is invertible,A+B,"Given $A, B \in M_n (\Bbb F)$ , where $A$ is $k$ -nilpotent and $B$ is invertible, is $A+B$ also invertible? I was having trouble on how to prove this, and then I thought maybe this statement is incorrect, but couldn't find a counter example. Perhaps someone can assist?","Given , where is -nilpotent and is invertible, is also invertible? I was having trouble on how to prove this, and then I thought maybe this statement is incorrect, but couldn't find a counter example. Perhaps someone can assist?","A, B \in M_n (\Bbb F) A k B A+B","['linear-algebra', 'matrices', 'examples-counterexamples']"
29,Inverse of constant matrix plus diagonal matrix,Inverse of constant matrix plus diagonal matrix,,"Is there an efficient way to calculate the inverse of an $N \times N$ diagonal matrix plus a constant matrix? I am looking at $N$ of around $40,000$. $$\left[\begin{array}{cccc} a & b & \cdots & b\\ b & a &  & \vdots\\ \vdots &  & \ddots & b\\ b & \cdots & b & a \end{array}\right]^{-1} = \,\,?$$ Putting this in to mathematica, for $N \in \{2, 3, 4\}$, the result is: $$\left[ \begin{array}{cc}  a & b \\  b & a \\ \end{array} \right]^{-1} = \left[ \begin{array}{cc}  \frac{a}{a^2-b^2} & -\frac{b}{a^2-b^2} \\  -\frac{b}{a^2-b^2} & \frac{a}{a^2-b^2} \\ \end{array} \right]$$ $$\left[ \begin{array}{ccc}  a & b & b \\  b & a & b \\  b & b & a \\ \end{array} \right]^{-1} = \left[ \begin{array}{ccc}  \frac{a^2-b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} \\  \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{a^2-b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} \\  \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{a^2-b^2}{a^3-3 a b^2+2 b^3} \\ \end{array} \right]$$ $$\left[ \begin{array}{cccc}  a & b & b & b \\  b & a & b & b \\  b & b & a & b \\  b & b & b & a \\ \end{array} \right]^{-1} = \left[ \begin{array}{cccc}  \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\  \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\  \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\  \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\ \end{array} \right]$$ It appears that there should be a formula but I am not sure how to derive it. In the end, I am looking for a numerical result.","Is there an efficient way to calculate the inverse of an $N \times N$ diagonal matrix plus a constant matrix? I am looking at $N$ of around $40,000$. $$\left[\begin{array}{cccc} a & b & \cdots & b\\ b & a &  & \vdots\\ \vdots &  & \ddots & b\\ b & \cdots & b & a \end{array}\right]^{-1} = \,\,?$$ Putting this in to mathematica, for $N \in \{2, 3, 4\}$, the result is: $$\left[ \begin{array}{cc}  a & b \\  b & a \\ \end{array} \right]^{-1} = \left[ \begin{array}{cc}  \frac{a}{a^2-b^2} & -\frac{b}{a^2-b^2} \\  -\frac{b}{a^2-b^2} & \frac{a}{a^2-b^2} \\ \end{array} \right]$$ $$\left[ \begin{array}{ccc}  a & b & b \\  b & a & b \\  b & b & a \\ \end{array} \right]^{-1} = \left[ \begin{array}{ccc}  \frac{a^2-b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} \\  \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{a^2-b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} \\  \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{-a b+b^2}{a^3-3 a b^2+2 b^3} & \frac{a^2-b^2}{a^3-3 a b^2+2 b^3} \\ \end{array} \right]$$ $$\left[ \begin{array}{cccc}  a & b & b & b \\  b & a & b & b \\  b & b & a & b \\  b & b & b & a \\ \end{array} \right]^{-1} = \left[ \begin{array}{cccc}  \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\  \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\  \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\  \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{-a^2 b+2 a b^2-b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} & \frac{a^3-3 a b^2+2 b^3}{a^4-6 a^2 b^2+8 a b^3-3 b^4} \\ \end{array} \right]$$ It appears that there should be a formula but I am not sure how to derive it. In the end, I am looking for a numerical result.",,"['linear-algebra', 'matrices', 'inverse', 'numerical-linear-algebra']"
30,Suppose A is an n-by-n matrix with its diagonal entries are n and other entries are one. Find determinant of A.,Suppose A is an n-by-n matrix with its diagonal entries are n and other entries are one. Find determinant of A.,,"For $n \geq 2$, find the determinant of $A_{n}=\begin{bmatrix} n &  1  & 1 &\ldots &1 \\ 1 & n  & 1 &\ldots &1 \\ 1 & 1  & n &\ldots &1 \\ \vdots & \vdots &\vdots & \ddots & \vdots\\ 1 & 1 & 1 &\ldots  & n \end{bmatrix}_{n \times n} $ One can deduce the determinant of $A_{n}=(n-1)^{n-1}(2n-1)$ by taking $n=2,3,4...$. I solve it as follows but I found it is not elegant. Any better approach ? Here is my attempt: Let's find the eigenvalues of $A_{n}$ by solving $det(\lambda I-A_{n})=0$ because $det(A_{n})=\lambda_{1}\lambda_{2}...\lambda_{n}$ where $\lambda_{i}$ are eigenvalues of $A_{n}$. Express $A_{n}=(n-1)I+J$ where $I$ is the n-by-n identity matrix while $J$ is an n-by-n matrix with all entries equal to 1. Denote $B=(n-1)I$ so that $$\lambda I-A_{n}=\lambda I-(B+J)=(\lambda I - B)(I-(\lambda I - B)^{-1}J)$$ Since $det(XY)=det(X)det(Y)$ for square matrices $X$ and $Y$ $$det(\lambda I-A_{n})=det(\lambda I - B)det(I-(\lambda I - B)^{-1}J)$$ Since $det(kX)=k^{n}det(X)$ for a scalar k and $det(I)=1$ $$det(\lambda I-B)=\det(\lambda I - (n-1)I)=(\lambda-(n-1))^{n}det(I)=(\lambda-(n-1))^{n}$$ If $u$ be a column vector of one's in $\mathbb{R}^{n}$, then $uu^{T}=J$ so that: $$det(I-(\lambda I - B)^{-1}J)=det(I-(\lambda I - B)^{-1}uu^{T}) $$ By Sylvester's Determinant Theorem:  \begin{equation*} \begin{split} det(I-(\lambda I - B)^{-1}J)&=&det(I-(\lambda I - B)^{-1}uu^{T})=det(I-u^{T}(\lambda I - B)^{-1}u)\\ &=&1-u^{T}(\lambda I - B)^{-1}u=1-u^{T}(\lambda-(n-1))^{-1}Iu\\ &=&1-(\lambda-(n-1))^{-1}u^{T}Iu\\ &=&1-\frac{u^{T}u}{\lambda-(n-1)}=1-\frac{n}{\lambda-(n-1)}\\ &=&\frac{\lambda-(2n-1)}{\lambda-(n-1)} \end{split} \end{equation*} This yields $$det(\lambda I-A_{n})=(\lambda-(n-1))^{n}\frac{\lambda-(2n-1)}{\lambda-(n-1)}=(\lambda-(n-1))^{n-1}(\lambda -(2n-1))=0$$ The eigenvalues of $A_{n}$ are $n-1$ (with algebraic multiplicity of $n-1$) and $2n-1$  (with algebraic multiplicity of 1). Thus, $det(A_{n}=(n-1)^{n-1}(2n-1)$ I wonder if it is possible to obtain the determinant without solving for the eigenvalues. Or at least without using the Sylvester's determinant theorem....","For $n \geq 2$, find the determinant of $A_{n}=\begin{bmatrix} n &  1  & 1 &\ldots &1 \\ 1 & n  & 1 &\ldots &1 \\ 1 & 1  & n &\ldots &1 \\ \vdots & \vdots &\vdots & \ddots & \vdots\\ 1 & 1 & 1 &\ldots  & n \end{bmatrix}_{n \times n} $ One can deduce the determinant of $A_{n}=(n-1)^{n-1}(2n-1)$ by taking $n=2,3,4...$. I solve it as follows but I found it is not elegant. Any better approach ? Here is my attempt: Let's find the eigenvalues of $A_{n}$ by solving $det(\lambda I-A_{n})=0$ because $det(A_{n})=\lambda_{1}\lambda_{2}...\lambda_{n}$ where $\lambda_{i}$ are eigenvalues of $A_{n}$. Express $A_{n}=(n-1)I+J$ where $I$ is the n-by-n identity matrix while $J$ is an n-by-n matrix with all entries equal to 1. Denote $B=(n-1)I$ so that $$\lambda I-A_{n}=\lambda I-(B+J)=(\lambda I - B)(I-(\lambda I - B)^{-1}J)$$ Since $det(XY)=det(X)det(Y)$ for square matrices $X$ and $Y$ $$det(\lambda I-A_{n})=det(\lambda I - B)det(I-(\lambda I - B)^{-1}J)$$ Since $det(kX)=k^{n}det(X)$ for a scalar k and $det(I)=1$ $$det(\lambda I-B)=\det(\lambda I - (n-1)I)=(\lambda-(n-1))^{n}det(I)=(\lambda-(n-1))^{n}$$ If $u$ be a column vector of one's in $\mathbb{R}^{n}$, then $uu^{T}=J$ so that: $$det(I-(\lambda I - B)^{-1}J)=det(I-(\lambda I - B)^{-1}uu^{T}) $$ By Sylvester's Determinant Theorem:  \begin{equation*} \begin{split} det(I-(\lambda I - B)^{-1}J)&=&det(I-(\lambda I - B)^{-1}uu^{T})=det(I-u^{T}(\lambda I - B)^{-1}u)\\ &=&1-u^{T}(\lambda I - B)^{-1}u=1-u^{T}(\lambda-(n-1))^{-1}Iu\\ &=&1-(\lambda-(n-1))^{-1}u^{T}Iu\\ &=&1-\frac{u^{T}u}{\lambda-(n-1)}=1-\frac{n}{\lambda-(n-1)}\\ &=&\frac{\lambda-(2n-1)}{\lambda-(n-1)} \end{split} \end{equation*} This yields $$det(\lambda I-A_{n})=(\lambda-(n-1))^{n}\frac{\lambda-(2n-1)}{\lambda-(n-1)}=(\lambda-(n-1))^{n-1}(\lambda -(2n-1))=0$$ The eigenvalues of $A_{n}$ are $n-1$ (with algebraic multiplicity of $n-1$) and $2n-1$  (with algebraic multiplicity of 1). Thus, $det(A_{n}=(n-1)^{n-1}(2n-1)$ I wonder if it is possible to obtain the determinant without solving for the eigenvalues. Or at least without using the Sylvester's determinant theorem....",,"['linear-algebra', 'matrices', 'determinant']"
31,Is the Scalar Product Definition in my book wrong?,Is the Scalar Product Definition in my book wrong?,,"In Rainer Kress'es book ""linear integral equations"" (2nd edition) on page 9 it says Definition 1.19 Let X be a complex (or real) linear space. Then a function $(\cdot , \cdot): \rightarrow X \times X \rightarrow \mathbb{C}\; (or \mathbb{R})$ with the properties. (H1) $(\varphi, \varphi) \geq 0 $ (positivity) [...]. for all $\varphi \in X$ is called a Scalar product. Now, if the mapping goes from $X\times X$ to $\mathbb{C}$, we might have to compare imaginary numbers with the > relation, which is not possible to my knowledge. Is this a mistake in the book, or did I miss something? Edit: pardon my formatting, I'm typing this on my phone Clarification Why do we map $X \times X \rightarrow \mathbb{C}$ in the first place if we implicitly assume it is real anyway. I find this confusing. Result My confusion came from, that I thought of a mapping to be a  Scalar product which turned out it was not.","In Rainer Kress'es book ""linear integral equations"" (2nd edition) on page 9 it says Definition 1.19 Let X be a complex (or real) linear space. Then a function $(\cdot , \cdot): \rightarrow X \times X \rightarrow \mathbb{C}\; (or \mathbb{R})$ with the properties. (H1) $(\varphi, \varphi) \geq 0 $ (positivity) [...]. for all $\varphi \in X$ is called a Scalar product. Now, if the mapping goes from $X\times X$ to $\mathbb{C}$, we might have to compare imaginary numbers with the > relation, which is not possible to my knowledge. Is this a mistake in the book, or did I miss something? Edit: pardon my formatting, I'm typing this on my phone Clarification Why do we map $X \times X \rightarrow \mathbb{C}$ in the first place if we implicitly assume it is real anyway. I find this confusing. Result My confusion came from, that I thought of a mapping to be a  Scalar product which turned out it was not.",,"['linear-algebra', 'inner-products']"
32,Is the self-adjoint condition required in the definition of a positive operator?,Is the self-adjoint condition required in the definition of a positive operator?,,"I'm reading Linear Algebra Done Right and it defines a positive operator $T$ as one which is self adjoint and has the property $$\langle Tv,v \rangle \geq 0$$ for all $v\in V$. I am confused as to why the self adjoint condition must be included. Here is what I came up with: Suppose $T$ is an operator such that $\langle Tv, v\rangle \geq 0$ for all $v$. This implies that $\langle Tv, v\rangle$ is a real number, since the greater than sign doesn't make sense for complex numbers. Then, using the definition of adjoint, $$\langle Tv, v\rangle = \langle v, T^*v\rangle = \overline{\langle T^*v,v\rangle} = \langle T^*v, v\rangle$$ for all $v\in V$. Therefore, $Tv=T^*v$ for all $v$ and $T$ is self adjoint. Where did I go wrong?","I'm reading Linear Algebra Done Right and it defines a positive operator $T$ as one which is self adjoint and has the property $$\langle Tv,v \rangle \geq 0$$ for all $v\in V$. I am confused as to why the self adjoint condition must be included. Here is what I came up with: Suppose $T$ is an operator such that $\langle Tv, v\rangle \geq 0$ for all $v$. This implies that $\langle Tv, v\rangle$ is a real number, since the greater than sign doesn't make sense for complex numbers. Then, using the definition of adjoint, $$\langle Tv, v\rangle = \langle v, T^*v\rangle = \overline{\langle T^*v,v\rangle} = \langle T^*v, v\rangle$$ for all $v\in V$. Therefore, $Tv=T^*v$ for all $v$ and $T$ is self adjoint. Where did I go wrong?",,"['linear-algebra', 'positive-semidefinite', 'self-adjoint-operators']"
33,Can set of integers form a vector space over field of rationals? [duplicate],Can set of integers form a vector space over field of rationals? [duplicate],,"This question already has an answer here : Prove $\mathbb{Z}$ is not a vector space over a field (1 answer) Closed 5 years ago . As field of reals $\mathbb{R}$ can be made  a vector space over field of complex numbers $\mathbb{C}$ but not in the usual way. In the same way can we make the ring of integers $\mathbb{Z}$ as a vector space the field of rationals $\mathbb{Q}$? It is clear if it forms a vector space, then $\dim_{\mathbb{Q}}\mathbb{Z}$ will be finite. Now i am stuck. Please help me. Thanks in advance.","This question already has an answer here : Prove $\mathbb{Z}$ is not a vector space over a field (1 answer) Closed 5 years ago . As field of reals $\mathbb{R}$ can be made  a vector space over field of complex numbers $\mathbb{C}$ but not in the usual way. In the same way can we make the ring of integers $\mathbb{Z}$ as a vector space the field of rationals $\mathbb{Q}$? It is clear if it forms a vector space, then $\dim_{\mathbb{Q}}\mathbb{Z}$ will be finite. Now i am stuck. Please help me. Thanks in advance.",,"['linear-algebra', 'vector-spaces', 'rational-numbers']"
34,What is the importance of the spectral theorem?,What is the importance of the spectral theorem?,,"I know that the spectral theorem tells us that in the case of a real inner product space, an operator is self adjoint if and only if there is an orthonormal basis with only eigenvectors of that operator and that in the case of a complex inner product space, an operator is normal if and only if there is an orthonormal basis with only eigenvectors. However, I am unable to see what is so important about this. I know that this means I can diagonalize the matrix but I don't know what is so good about diagonalizing a matrix. Also, why do we have to have an orthonormal basis? Why can it not be a standard basis? Thank you!","I know that the spectral theorem tells us that in the case of a real inner product space, an operator is self adjoint if and only if there is an orthonormal basis with only eigenvectors of that operator and that in the case of a complex inner product space, an operator is normal if and only if there is an orthonormal basis with only eigenvectors. However, I am unable to see what is so important about this. I know that this means I can diagonalize the matrix but I don't know what is so good about diagonalizing a matrix. Also, why do we have to have an orthonormal basis? Why can it not be a standard basis? Thank you!",,"['linear-algebra', 'motivation', 'big-picture']"
35,How to prove $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$?,How to prove ?,\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert,I'm trying to show that $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$. A hint would be nice.,I'm trying to show that $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$. A hint would be nice.,,"['linear-algebra', 'inequality', 'vector-spaces', 'normed-spaces']"
36,Is $\;\det(A^n) =\left(\det (A)\right)^n\;$? [duplicate],Is ? [duplicate],\;\det(A^n) =\left(\det (A)\right)^n\;,This question already has answers here : How to show that $\det(AB) =\det(A) \det(B)$? (12 answers) Closed 2 years ago . How can the value of $\;\det\left(A^{11}\right)\;$ be calculated from $\;\det(A)$? Generally how can $\;\det\left(A^n\right)\;$ be obtained from $\;\det(A)$?,This question already has answers here : How to show that $\det(AB) =\det(A) \det(B)$? (12 answers) Closed 2 years ago . How can the value of $\;\det\left(A^{11}\right)\;$ be calculated from $\;\det(A)$? Generally how can $\;\det\left(A^n\right)\;$ be obtained from $\;\det(A)$?,,"['linear-algebra', 'matrices', 'determinant']"
37,What is the importance of determinants in linear algebra?,What is the importance of determinants in linear algebra?,,"In some literature on linear algebra determinants play a critical role and are emphasized in the earlier chapters (see books by Anton & Rorres, and Lay). However in other literature it is totally ignored until the latter chapters (see Gilbert Strang). How much importance should we give the topic of determinants? I tend to use it to find linear independence of vectors and might extend this to finding the inverse but I think Gauss Jordan and LU might be easier for inverse. Does it have any other uses in Linear Algebra. Are there areas where determinants are used and have a real impact? Are there any real life applications of determinants? Is there a really good motivating example or explanation which will hook students into this topic?  In linear algebra, where should determinants be placed? Like I said in my comment - in some literature it is at the beginning whilst in others it is bolted on at the end. I like the idea of checking if vectors are independent by using determinants so think they should be placed before independence of vectors. What do you think? If you teach a linear algebra course where do you place this topic.","In some literature on linear algebra determinants play a critical role and are emphasized in the earlier chapters (see books by Anton & Rorres, and Lay). However in other literature it is totally ignored until the latter chapters (see Gilbert Strang). How much importance should we give the topic of determinants? I tend to use it to find linear independence of vectors and might extend this to finding the inverse but I think Gauss Jordan and LU might be easier for inverse. Does it have any other uses in Linear Algebra. Are there areas where determinants are used and have a real impact? Are there any real life applications of determinants? Is there a really good motivating example or explanation which will hook students into this topic?  In linear algebra, where should determinants be placed? Like I said in my comment - in some literature it is at the beginning whilst in others it is bolted on at the end. I like the idea of checking if vectors are independent by using determinants so think they should be placed before independence of vectors. What do you think? If you teach a linear algebra course where do you place this topic.",,"['linear-algebra', 'matrices', 'education']"
38,Probability that a vector in $\mathbb{Z}^n$ is primitive,Probability that a vector in  is primitive,\mathbb{Z}^n,"A vector $v \in \mathbb{Z}^n$ is primitive if there does not exist some vector $v' \in \mathbb{Z}^n$ and some $k \in \mathbb{Z}$ such that $v = k v'$ and $k \geq 2$. For a paper I'm writing right now, I'd like to know that a ""random"" vector in $\mathbb{Z}^n$ is primitive.  Let me make this precise. Let $\|\cdot\|_{1}$ be the $L^{1}$ norm on $\mathbb{Z}^n$, so $\|v\|_1 = \sum_{i=1}^n |v_i|$, where the $v_i$ are the components of $v$.  Define $\mathcal{V}_k$ to be the number of vectors $v$ in $\mathbb{Z}^n$ such that $\|v\|_1 \leq k$.  Define $\mathcal{P}_k$ to be the number of primitive vectors $v$ in $\mathbb{Z}^n$ such that $\|v\|_1 \leq k$. I then want $$\lim_{k \rightarrow \infty} \frac{\mathcal{P}_k}{\mathcal{V}_k} = 1.$$ Assuming this is true, is there any nice estimate as to how fast it approaches $1$?","A vector $v \in \mathbb{Z}^n$ is primitive if there does not exist some vector $v' \in \mathbb{Z}^n$ and some $k \in \mathbb{Z}$ such that $v = k v'$ and $k \geq 2$. For a paper I'm writing right now, I'd like to know that a ""random"" vector in $\mathbb{Z}^n$ is primitive.  Let me make this precise. Let $\|\cdot\|_{1}$ be the $L^{1}$ norm on $\mathbb{Z}^n$, so $\|v\|_1 = \sum_{i=1}^n |v_i|$, where the $v_i$ are the components of $v$.  Define $\mathcal{V}_k$ to be the number of vectors $v$ in $\mathbb{Z}^n$ such that $\|v\|_1 \leq k$.  Define $\mathcal{P}_k$ to be the number of primitive vectors $v$ in $\mathbb{Z}^n$ such that $\|v\|_1 \leq k$. I then want $$\lim_{k \rightarrow \infty} \frac{\mathcal{P}_k}{\mathcal{V}_k} = 1.$$ Assuming this is true, is there any nice estimate as to how fast it approaches $1$?",,"['linear-algebra', 'probability']"
39,Finding the rank of the matrix directly from eigenvalues,Finding the rank of the matrix directly from eigenvalues,,Let $e$ denote eigenvalues and $$ e_1=0\\ e_2=2 \\ e_3=2 \\ $$ Let $B$ be a $3\times3$ matrix. This information is certainly enough to find the rank of the matrix B(according to Gilbert Strang) And the rank would be $r=2$. But how come? I know that the determinant is $0$ so rank can't be 3. But how come it's 2 but not 1? Can't it be 1? My approach of thinking is below . Is it because these three eigenvalues would correspond the two different eigenvectors. And since two different eigenvectors are in the $C(B)$ this would make B $r=2$. But how do we know that eigenvectors are linearly independent? are they always linearly independent? And do distinct eigenvalues always form the same eigenvector?,Let $e$ denote eigenvalues and $$ e_1=0\\ e_2=2 \\ e_3=2 \\ $$ Let $B$ be a $3\times3$ matrix. This information is certainly enough to find the rank of the matrix B(according to Gilbert Strang) And the rank would be $r=2$. But how come? I know that the determinant is $0$ so rank can't be 3. But how come it's 2 but not 1? Can't it be 1? My approach of thinking is below . Is it because these three eigenvalues would correspond the two different eigenvectors. And since two different eigenvectors are in the $C(B)$ this would make B $r=2$. But how do we know that eigenvectors are linearly independent? are they always linearly independent? And do distinct eigenvalues always form the same eigenvector?,,"['linear-algebra', 'matrices']"
40,When is $\mathbf{X}^{T}\mathbf{X}+\lambda\mathbf{I}$ invertible?,When is  invertible?,\mathbf{X}^{T}\mathbf{X}+\lambda\mathbf{I},"The question is quite simple: for a $N \times p$ matrix $\mathbf{X}$ with real entries, when is $\mathbf{X}^{T}\mathbf{X}+\lambda\mathbf{I}$ invertible (where $\mathbf{I}$ is the $p \times p$ identity matrix and $\lambda > 0$)? This comes up in ridge regression. In Elements of Statistical Learning (Hastie et al.), [The equation] adds a positive constant to the diagonal of $\mathbf{X}^{T}\mathbf{X}$ before inversion. This makes the problem nonsingular, even if $\mathbf{X}^{T}\mathbf{X}$ is not of full rank. I know that $\mathbf{X}^{T}\mathbf{X}$ is invertible if and only if it is of full rank if and only if $\mathbf{X}$ is of full column rank. The explanation is quite intuitive, but how do I prove it?","The question is quite simple: for a $N \times p$ matrix $\mathbf{X}$ with real entries, when is $\mathbf{X}^{T}\mathbf{X}+\lambda\mathbf{I}$ invertible (where $\mathbf{I}$ is the $p \times p$ identity matrix and $\lambda > 0$)? This comes up in ridge regression. In Elements of Statistical Learning (Hastie et al.), [The equation] adds a positive constant to the diagonal of $\mathbf{X}^{T}\mathbf{X}$ before inversion. This makes the problem nonsingular, even if $\mathbf{X}^{T}\mathbf{X}$ is not of full rank. I know that $\mathbf{X}^{T}\mathbf{X}$ is invertible if and only if it is of full rank if and only if $\mathbf{X}$ is of full column rank. The explanation is quite intuitive, but how do I prove it?",,"['linear-algebra', 'matrices', 'statistics', 'regression']"
41,Find all matrices satisfying $X^3=I-X$,Find all matrices satisfying,X^3=I-X,We are studying for a qualifying exam and have come across the following problem in a previous exam. Determine the solutions (if any) of the matrix equation $X^3=I-X$ in the $2 \times 2$-matrices over $\mathbb{R}$. Any hints in the right direction would be much appreciate for this.,We are studying for a qualifying exam and have come across the following problem in a previous exam. Determine the solutions (if any) of the matrix equation $X^3=I-X$ in the $2 \times 2$-matrices over $\mathbb{R}$. Any hints in the right direction would be much appreciate for this.,,['linear-algebra']
42,"Prove that $v_1, \dots v_n$ is a basis of V.",Prove that  is a basis of V.,"v_1, \dots v_n","Prove that if $e_1, \dots e_n$ is an orthonormal basis of $ V$ and $v_1, \dots , v_n$ are vectors in $ V$ such that $$\|e_j - v_j\| < \frac{1}{\sqrt{n}}$$ for each j, then $v_1, \dots v_n$ is a basis of $V$. I have fiddled around with this for a while now unfortunately nothing came close, so I'll just say I have no idea how to do this. Help :(?","Prove that if $e_1, \dots e_n$ is an orthonormal basis of $ V$ and $v_1, \dots , v_n$ are vectors in $ V$ such that $$\|e_j - v_j\| < \frac{1}{\sqrt{n}}$$ for each j, then $v_1, \dots v_n$ is a basis of $V$. I have fiddled around with this for a while now unfortunately nothing came close, so I'll just say I have no idea how to do this. Help :(?",,['linear-algebra']
43,When does a system of equations have no solution?,When does a system of equations have no solution?,,"I have performed Gaussian elimination on this matrix to reduce it to $$ \left[ \begin{array}{@{}ccc|c@{}} -3&-1&2 & 1 \\ 0& \frac{-5}{3}& \frac{10}{3} & \frac{8}{3} \\ 0&0& a+2 & b + \frac{6}{5} \\ \end{array} \right] $$ I thought that setting $a$ equal to $-2$ and having $b$ not equal to $-\frac{6}{5}$ would be the answer to this problem, but it apparently isn't. Could someone please explain why?","I have performed Gaussian elimination on this matrix to reduce it to $$ \left[ \begin{array}{@{}ccc|c@{}} -3&-1&2 & 1 \\ 0& \frac{-5}{3}& \frac{10}{3} & \frac{8}{3} \\ 0&0& a+2 & b + \frac{6}{5} \\ \end{array} \right] $$ I thought that setting $a$ equal to $-2$ and having $b$ not equal to $-\frac{6}{5}$ would be the answer to this problem, but it apparently isn't. Could someone please explain why?",,"['linear-algebra', 'matrices']"
44,Commutation when minimal and characteristic polynomial agree,Commutation when minimal and characteristic polynomial agree,,"Hello I am studying for the qualifying exam in algebra and I am having trouble solving this seemingly easy problem. If $A$ is a matrix whose minimal polynomial and characteristic polynomial agree, and $B$ commutes with $A$ then $B$ is a polynomial in $A$ . I have shown that the dimension of the subspace of polynomials in $A$ must be equal to the dimension of the underlying vector space. Clearly this subspace is contained in the subspace of matrices that commute with $A$ . So if I can show the latter must have dimension less than or equal to the dimension of $V$ , I'll be done. But I don't see how to show that. Or is there an easier way?","Hello I am studying for the qualifying exam in algebra and I am having trouble solving this seemingly easy problem. If is a matrix whose minimal polynomial and characteristic polynomial agree, and commutes with then is a polynomial in . I have shown that the dimension of the subspace of polynomials in must be equal to the dimension of the underlying vector space. Clearly this subspace is contained in the subspace of matrices that commute with . So if I can show the latter must have dimension less than or equal to the dimension of , I'll be done. But I don't see how to show that. Or is there an easier way?",A B A B A A A V,['linear-algebra']
45,Proof that a strictly diagonally dominant matrix is invertible [duplicate],Proof that a strictly diagonally dominant matrix is invertible [duplicate],,"This question already has answers here : Strictly diagonally dominant matrices are non singular (4 answers) Closed 4 years ago . Let $A$ be a strictly diagonally dominant matrix of dimensions $n \times n$ . (""Strictly diagonally dominant"" means that $\left|a_{i,i}\right| > \sum\limits_{j \neq i} \left|a_{i,j}\right|$ for all $i \in \left\{1,2,\ldots,n\right\}$ , where $a_{u,v}$ denotes the $\left(u,v\right)$ -th entry of $A$ .) Prove that $A$ is invertible. My attempt builds on the proof of Gershgorin's circle theorem, given in the Wikipedia article https://en.wikipedia.org/wiki/Gershgorin_circle_theorem Let $\lambda$ be an eigenvalue of $A$ , and scale its corresponding eigenvector $x$ so that $x_{i} = 1$ and $|x_{j}| \leq 1$ for $j \neq i$ Then $Ax = \lambda x$ , and in particular $\sum_{i\neq j}a_{i,j}x_{j} = \lambda - a_{ii}$ (1) Now because $A$ is strictly diagonally dominant it holds for every $i$ that, $\sum_{j \neq i}|a_{i,j}| < |a_{i,i}|$ and since $|x_{i}| \leq 1$ the following should hold: $\sum_{i\neq j}|a_{i,j}x_{j}| \leq \sum_{j \neq i}|a_{i,j}| < |a_{i,i}|$ But here I get stuck, feel like I want to use (1) in some way to complete the proof and put $\lambda = 0$ to get a contradiction, thus proving that if $A$ is strictly diagonally dominant, it has non-zero eigenvalues which should imply invertibility.. Am I in the right direction?","This question already has answers here : Strictly diagonally dominant matrices are non singular (4 answers) Closed 4 years ago . Let be a strictly diagonally dominant matrix of dimensions . (""Strictly diagonally dominant"" means that for all , where denotes the -th entry of .) Prove that is invertible. My attempt builds on the proof of Gershgorin's circle theorem, given in the Wikipedia article https://en.wikipedia.org/wiki/Gershgorin_circle_theorem Let be an eigenvalue of , and scale its corresponding eigenvector so that and for Then , and in particular (1) Now because is strictly diagonally dominant it holds for every that, and since the following should hold: But here I get stuck, feel like I want to use (1) in some way to complete the proof and put to get a contradiction, thus proving that if is strictly diagonally dominant, it has non-zero eigenvalues which should imply invertibility.. Am I in the right direction?","A n \times n \left|a_{i,i}\right| > \sum\limits_{j \neq i} \left|a_{i,j}\right| i \in \left\{1,2,\ldots,n\right\} a_{u,v} \left(u,v\right) A A \lambda A x x_{i} = 1 |x_{j}| \leq 1 j \neq i Ax = \lambda x \sum_{i\neq j}a_{i,j}x_{j} = \lambda - a_{ii} A i \sum_{j \neq i}|a_{i,j}| < |a_{i,i}| |x_{i}| \leq 1 \sum_{i\neq j}|a_{i,j}x_{j}| \leq \sum_{j \neq i}|a_{i,j}| < |a_{i,i}| \lambda = 0 A","['linear-algebra', 'matrices', 'proof-verification', 'inequality']"
46,Can any 3d rotation be done with only two angles?,Can any 3d rotation be done with only two angles?,,"Up until now I didn't really have to deal with rotation matrices, but now, a question has come up: Can I rotate a 3d vector in any way I'd like in 3d by only specifying two angles of rotation? My intuition tells me it is possible: Any 3d vector can be defined using two angles and the vector's norm. Using two rotations about an axis, we can rotate the vector relative to the XY plane and then relative to the XZ/YZ plane to get another vector in any direction we want. I found a few other intuitive explanations, but these aren't proofs. I have been told I'm wrong by a few people. Almost anywhere I look I see 3D rotations expressed in terms of 3 angles, but if I'm missing something, I can't for the life of me figure out what it is.","Up until now I didn't really have to deal with rotation matrices, but now, a question has come up: Can I rotate a 3d vector in any way I'd like in 3d by only specifying two angles of rotation? My intuition tells me it is possible: Any 3d vector can be defined using two angles and the vector's norm. Using two rotations about an axis, we can rotate the vector relative to the XY plane and then relative to the XZ/YZ plane to get another vector in any direction we want. I found a few other intuitive explanations, but these aren't proofs. I have been told I'm wrong by a few people. Almost anywhere I look I see 3D rotations expressed in terms of 3 angles, but if I'm missing something, I can't for the life of me figure out what it is.",,"['linear-algebra', 'rotations']"
47,Difference between orthogonal projection and least squares solution,Difference between orthogonal projection and least squares solution,,"When you find the least squares solution you solve $$A^TA = A\vec b$$ but to find the orthogonal projection into the ""subspace"" A, you multiply this result (the least squares solution) with the original matrix. Why is this? If you use the analogy with the light shining orthogonally on to the subspace and the orthogonal projection is the shadow in the subspace, isn't this shadow also the least squares solution?","When you find the least squares solution you solve $$A^TA = A\vec b$$ but to find the orthogonal projection into the ""subspace"" A, you multiply this result (the least squares solution) with the original matrix. Why is this? If you use the analogy with the light shining orthogonally on to the subspace and the orthogonal projection is the shadow in the subspace, isn't this shadow also the least squares solution?",,"['linear-algebra', 'vector-spaces', 'vectors', 'least-squares']"
48,Does an overdetermined system always have no solutions? [closed],Does an overdetermined system always have no solutions? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question What is the problem with over-determined systems in linear algebra? Do they always have no solution? Is there a proof of that?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question What is the problem with over-determined systems in linear algebra? Do they always have no solution? Is there a proof of that?",,"['linear-algebra', 'matrices', 'systems-of-equations']"
49,A $2 \times 2$ matrix $A$ such that $A^n$ is the identity matrix,A  matrix  such that  is the identity matrix,2 \times 2 A A^n,"So basically determine a $2 \times 2$ matrix $A$ such that $A^n$ is an identity matrix, but none of $A^1, A^2,..., A^{n-1}$ are the identity matrix. (Hint: Think geometric mappings) I don't understand this question at all, can someone help please?","So basically determine a $2 \times 2$ matrix $A$ such that $A^n$ is an identity matrix, but none of $A^1, A^2,..., A^{n-1}$ are the identity matrix. (Hint: Think geometric mappings) I don't understand this question at all, can someone help please?",,['linear-algebra']
50,"If $AA^T$ is the zero matrix, then $A$ is the zero matrix","If  is the zero matrix, then  is the zero matrix",AA^T A,"Let $A$ be a $4 \times 4$ matrix. Show that if $A^TA$ or $AA^T$ is the zero matrix, then $A$ is the zero matrix. I feel very close to solving the problem so far. I have said that $$[0]_{ij}=\sum_{k=1}^4 [A]_{ik}[A^T]_{kj} =\sum_{k=1}^4 [A]_{ik}[A]_{jk} \qquad \text{for all }i,j \in \{1,2,3,4\} $$ and proven for the case if $i=j$. However, I cannot seem to find a good way to prove it to be true if $i\neq j$.","Let $A$ be a $4 \times 4$ matrix. Show that if $A^TA$ or $AA^T$ is the zero matrix, then $A$ is the zero matrix. I feel very close to solving the problem so far. I have said that $$[0]_{ij}=\sum_{k=1}^4 [A]_{ik}[A^T]_{kj} =\sum_{k=1}^4 [A]_{ik}[A]_{jk} \qquad \text{for all }i,j \in \{1,2,3,4\} $$ and proven for the case if $i=j$. However, I cannot seem to find a good way to prove it to be true if $i\neq j$.",,"['linear-algebra', 'matrices']"
51,Number of bases of an n-dimensional vector space over q-element field.,Number of bases of an n-dimensional vector space over q-element field.,,"If I have an n-dimensional vector space over a field with q elements, how can I find the number of bases of this vector space?","If I have an n-dimensional vector space over a field with q elements, how can I find the number of bases of this vector space?",,['linear-algebra']
52,Dot product in coordinates,Dot product in coordinates,,"Dot product of two vectors on plane could be defined as product of lengths of those vectors and cosine of angle between them. In cartesian coordinates dot product of vectors with coordinates $(x_1, y_1)$ and $(x_2, y_2)$ is equal to $x_1x_2 + y_1y_2$. How to prove it?","Dot product of two vectors on plane could be defined as product of lengths of those vectors and cosine of angle between them. In cartesian coordinates dot product of vectors with coordinates $(x_1, y_1)$ and $(x_2, y_2)$ is equal to $x_1x_2 + y_1y_2$. How to prove it?",,"['geometry', 'linear-algebra']"
53,Computing the product $(\frac{d}{dx}+x)^n(-\frac{d}{dx}+x)^n$,Computing the product,(\frac{d}{dx}+x)^n(-\frac{d}{dx}+x)^n,"I want to compute the product $$ (\frac{d}{dx}+x)^n(-\frac{d}{dx}+x)^n, $$ for a natural number $n$ . For $n$ equal to 0 or 1, the computation is very simple but for such a low number as 2 the brute force calculation begins to be rather cumbersome and I cannot see any pattern emerging. I tried to find some connection with the Rodrigues' formula for the Hermite polynomials but I could not. These operators come up in the algebraic approach to the quantum harmonic oscillator . Explicit Example To avoid any misunderstanding, I am going to show explicitly the computation for the case $n=1$ : $$ (\frac{d}{dx}+x)(-\frac{d}{dx}+x)=-\frac{d^2}{dx^2}+1+x\frac{d}{dx}-x\frac{d}{dx}+x^2=-\frac{d^2}{dx^2}+x^2+1. $$ One can think of a function $f$ the operators are acting on. For example, $$ (\frac{d}{dx}\circ x) f= (\frac{d}{dx}x)f+x\frac{d}{dx}f=(1+\frac{d}{dx})f,   $$ then $$ \frac{d}{dx}\circ x=1+x\frac{d}{dx} $$","I want to compute the product for a natural number . For equal to 0 or 1, the computation is very simple but for such a low number as 2 the brute force calculation begins to be rather cumbersome and I cannot see any pattern emerging. I tried to find some connection with the Rodrigues' formula for the Hermite polynomials but I could not. These operators come up in the algebraic approach to the quantum harmonic oscillator . Explicit Example To avoid any misunderstanding, I am going to show explicitly the computation for the case : One can think of a function the operators are acting on. For example, then","
(\frac{d}{dx}+x)^n(-\frac{d}{dx}+x)^n,
 n n n=1 
(\frac{d}{dx}+x)(-\frac{d}{dx}+x)=-\frac{d^2}{dx^2}+1+x\frac{d}{dx}-x\frac{d}{dx}+x^2=-\frac{d^2}{dx^2}+x^2+1.
 f 
(\frac{d}{dx}\circ x) f= (\frac{d}{dx}x)f+x\frac{d}{dx}f=(1+\frac{d}{dx})f,  
 
\frac{d}{dx}\circ x=1+x\frac{d}{dx}
","['linear-algebra', 'derivatives', 'operator-theory', 'quantum-mechanics']"
54,Understanding the distance between a line and a point in 3D space,Understanding the distance between a line and a point in 3D space,,"I know that there are quicker ways to do what I am about to present. But I want to understand why my approach does not work. Let the point $P = (-6, 3, 3)$ and the line $L=(-2t,-6t,t)$ . I am trying to find the shortest distance between the point and the line. From my observation, I believe the line passes through the origin because it can be written as $$L=\begin{bmatrix}0 \\ 0 \\ 0 \end{bmatrix} +t \begin{bmatrix}-2 \\ -6 \\ 1 \end{bmatrix} $$ . Let $Q$ denoted $(a, b, c)$ be a point on $L$ such that $\vec{QP}$ is the shortest distance between $L$ and $P$ . Note that $\vec{QP}$ is normal to $L$ . Therefore, I need to find $\vec{QP}$ which is $\vec{P}-\vec{Q}$ . $\vec{QP} = (-6 - a, 3 - b, 3 -c)$ We know that $\vec{QP}$ and $L$ are perpendicular so the dot product is 0. $$-2(-6 - a) - 6(3 - b) + (3 - c) = 0$$ Simplifying gives us $$2a + 6b - c -3 = 0$$ let $a=0$ , $b=1$ , then by solving we know that $c=3$ . From my understanding, we should have found $Q$ which intersects $L$ and $\vec{QP}$ . Unfortunately, it seems $||\vec{QP}||$ is not the correct answer. I think that the way I managed to pull out the $a$ , $b$ and $c$ is the culprit, however I just don't understand what I did wrong.","I know that there are quicker ways to do what I am about to present. But I want to understand why my approach does not work. Let the point and the line . I am trying to find the shortest distance between the point and the line. From my observation, I believe the line passes through the origin because it can be written as . Let denoted be a point on such that is the shortest distance between and . Note that is normal to . Therefore, I need to find which is . We know that and are perpendicular so the dot product is 0. Simplifying gives us let , , then by solving we know that . From my understanding, we should have found which intersects and . Unfortunately, it seems is not the correct answer. I think that the way I managed to pull out the , and is the culprit, however I just don't understand what I did wrong.","P = (-6, 3, 3) L=(-2t,-6t,t) L=\begin{bmatrix}0 \\ 0 \\ 0 \end{bmatrix} +t \begin{bmatrix}-2 \\ -6 \\ 1 \end{bmatrix}  Q (a, b, c) L \vec{QP} L P \vec{QP} L \vec{QP} \vec{P}-\vec{Q} \vec{QP} = (-6 - a, 3 - b, 3 -c) \vec{QP} L -2(-6 - a) - 6(3 - b) + (3 - c) = 0 2a + 6b - c -3 = 0 a=0 b=1 c=3 Q L \vec{QP} ||\vec{QP}|| a b c",['linear-algebra']
55,What does calculating the inverse of a matrix mean?,What does calculating the inverse of a matrix mean?,,"Assume I have 3 equations, $x+2y+z=2, 3x+8y+z=12, 4y+z=2$ which could be represented in matrix form ( $Ax = b$ ) like this: $\begin{pmatrix} 1 & 2 & 1\\  3 & 8 & 1\\  0 & 4 & 1 \end{pmatrix}\bigl .\begin{pmatrix} x\\  y\\  z \end{pmatrix} = \begin{pmatrix} 2\\  12\\  2 \end{pmatrix}$ Then, the inverse of $A$ , $A^{-1}$ , would be: $\begin{pmatrix} 2/5 & 1/5 & -3/5\\  -3/10 & 1/10 & 1/5\\  6/5 & -2/5 & 1/5 \end{pmatrix} $ . So, my question is, what does this even mean? We know that $A$ is a coefficients matrix that represents the 3 equations above, so what does $A^{-1}$ mean with respect to these 3 equations? What have I done to the 3 equations is exactly my question. Please note that I understand very well how to find the inverse of a matrix, I just don't understand the intuition of what's happening and sort of the meaning of the manipulations I am applying to the equations when they are in matrix form.","Assume I have 3 equations, which could be represented in matrix form ( ) like this: Then, the inverse of , , would be: . So, my question is, what does this even mean? We know that is a coefficients matrix that represents the 3 equations above, so what does mean with respect to these 3 equations? What have I done to the 3 equations is exactly my question. Please note that I understand very well how to find the inverse of a matrix, I just don't understand the intuition of what's happening and sort of the meaning of the manipulations I am applying to the equations when they are in matrix form.","x+2y+z=2, 3x+8y+z=12, 4y+z=2 Ax = b \begin{pmatrix}
1 & 2 & 1\\ 
3 & 8 & 1\\ 
0 & 4 & 1
\end{pmatrix}\bigl .\begin{pmatrix}
x\\ 
y\\ 
z
\end{pmatrix} = \begin{pmatrix}
2\\ 
12\\ 
2
\end{pmatrix} A A^{-1} \begin{pmatrix}
2/5 & 1/5 & -3/5\\ 
-3/10 & 1/10 & 1/5\\ 
6/5 & -2/5 & 1/5
\end{pmatrix}  A A^{-1}",['linear-algebra']
56,True or false: Every real homogeneous linear system of equation which has more than one solution has infinite solutions,True or false: Every real homogeneous linear system of equation which has more than one solution has infinite solutions,,"This is a task from a test exam you can find here (in German): http://docdro.id/QRtdXkM Is the following statement true or false? Every real homogeneous linear system of equation that has more than   one solution, has infinite solutions. I think the statement is true because a linear system of equations can only have either one solution, no solution or infinite solutions. This statement clearly says ""more than one solution $\rightarrow$ infinite solutions"" which is true. Is it really correct like that or there is some special case which can make this statement false?","This is a task from a test exam you can find here (in German): http://docdro.id/QRtdXkM Is the following statement true or false? Every real homogeneous linear system of equation that has more than   one solution, has infinite solutions. I think the statement is true because a linear system of equations can only have either one solution, no solution or infinite solutions. This statement clearly says ""more than one solution $\rightarrow$ infinite solutions"" which is true. Is it really correct like that or there is some special case which can make this statement false?",,"['linear-algebra', 'matrices']"
57,The matrix of a projection can never be invertible,The matrix of a projection can never be invertible,,"I am currently studying linear transformations in order to refresh my knowledge of linear algebra. One statement in my textbook (by David Poole) is: When considering linear transformations from $\mathbb{R}^2$ to $\mathbb{R}^2$, the matrix of a projection can never be invertible. I know that a projection matrix satisfies the equation $P^2 = P$. Taking determinant of both sides gives $$\text{det}(P)^2 = \text{det}(P)$$ which is always true when $P$ is singular. However take $\color{blue} {P = I_2}$, then the equality is true and the projection matrix is invertible. What mistake do I make in my reasoning?","I am currently studying linear transformations in order to refresh my knowledge of linear algebra. One statement in my textbook (by David Poole) is: When considering linear transformations from $\mathbb{R}^2$ to $\mathbb{R}^2$, the matrix of a projection can never be invertible. I know that a projection matrix satisfies the equation $P^2 = P$. Taking determinant of both sides gives $$\text{det}(P)^2 = \text{det}(P)$$ which is always true when $P$ is singular. However take $\color{blue} {P = I_2}$, then the equality is true and the projection matrix is invertible. What mistake do I make in my reasoning?",,"['linear-algebra', 'matrices']"
58,Additive rotation matrices,Additive rotation matrices,,"Let's assume that we want to find a rotation matrix which added to a given rotation matrix  gives also a rotation matrix. I would name such matrix a rotation additive matrix for a given rotation matrix. First consider a 2D case  for identity matrix.  It is relatively easy to find such matrix. $          R= \begin{bmatrix}           -\dfrac{1}{2} & -\dfrac {\sqrt{3}}{2}   \\             \dfrac{\sqrt{3}}{2}  & -\dfrac{1}{2}  \\         \end{bmatrix} $ Really we have $            \begin{bmatrix}           -\dfrac{1}{2} & -\dfrac { \sqrt{3}}{2}   \\             \dfrac{ \sqrt{3}}{2}  & -\dfrac{1}{2}  \\         \end{bmatrix} + \begin{bmatrix}            1 & 0    \\             0  & 1  \\         \end{bmatrix} =  \begin{bmatrix}            \dfrac{1}{2} & -\dfrac { \sqrt{3}}{2}   \\              \dfrac{\sqrt{3}}{2}  &  \dfrac{1}{2}  \\         \end{bmatrix}   $ Also symmetrical matrix to R is additive for identity matrix, so we have at least 2 such matrices.  If it exists  for identity matrix should, I believe, exist for other 2D rotation matrices. I was searching also  for a such matrices in 3D . However without positive effects. Question Do  such matrices exist in 3D ? If so how to find them. If not how to prove it.","Let's assume that we want to find a rotation matrix which added to a given rotation matrix  gives also a rotation matrix. I would name such matrix a rotation additive matrix for a given rotation matrix. First consider a 2D case  for identity matrix.  It is relatively easy to find such matrix. $          R= \begin{bmatrix}           -\dfrac{1}{2} & -\dfrac {\sqrt{3}}{2}   \\             \dfrac{\sqrt{3}}{2}  & -\dfrac{1}{2}  \\         \end{bmatrix} $ Really we have $            \begin{bmatrix}           -\dfrac{1}{2} & -\dfrac { \sqrt{3}}{2}   \\             \dfrac{ \sqrt{3}}{2}  & -\dfrac{1}{2}  \\         \end{bmatrix} + \begin{bmatrix}            1 & 0    \\             0  & 1  \\         \end{bmatrix} =  \begin{bmatrix}            \dfrac{1}{2} & -\dfrac { \sqrt{3}}{2}   \\              \dfrac{\sqrt{3}}{2}  &  \dfrac{1}{2}  \\         \end{bmatrix}   $ Also symmetrical matrix to R is additive for identity matrix, so we have at least 2 such matrices.  If it exists  for identity matrix should, I believe, exist for other 2D rotation matrices. I was searching also  for a such matrices in 3D . However without positive effects. Question Do  such matrices exist in 3D ? If so how to find them. If not how to prove it.",,"['linear-algebra', 'matrices', 'rotations']"
59,If $A^2=A$ then prove that $\textrm{tr}(A)=\textrm{rank}(A)$.,If  then prove that .,A^2=A \textrm{tr}(A)=\textrm{rank}(A),"Let $A\not=I_n$ be an $n\times n$ matrix such that $A^2=A$ , where $I_n$ is the identity matrix of order $n$.  Then prove that , (A) $\textrm{tr}(A)=\textrm{rank}(A)$. (B) $\textrm{rank}(A)+\textrm{rank}(I_n-A)=n$ I found by example that these hold, but I am unable to prove them.","Let $A\not=I_n$ be an $n\times n$ matrix such that $A^2=A$ , where $I_n$ is the identity matrix of order $n$.  Then prove that , (A) $\textrm{tr}(A)=\textrm{rank}(A)$. (B) $\textrm{rank}(A)+\textrm{rank}(I_n-A)=n$ I found by example that these hold, but I am unable to prove them.",,"['linear-algebra', 'matrices']"
60,What is the rule for using $| \cdot |$ and $\| \cdot \|$ in Cauchy-Schwarz inequality,What is the rule for using  and  in Cauchy-Schwarz inequality,| \cdot | \| \cdot \|,"In this widely cited and wildly popular proof of the Cauchy-Schwarz inequality, the authors write ( http://www.math.lsa.umich.edu/~speyer/417/CauchySchwartz.pdf ) Let $u$ and $v$ be two vectors in $R^n$. The Cauchy-Schwartz inequality   states that $$|u \cdot v| ≤ |u||v|$$ I go to another source ( Proof that the Euclidean norm is indeed a norm ), a comment states states: Cauchy-Schwarz: $u \cdot v \le \lVert u\rVert \lVert v\rVert$ Yet another source ( http://rgmia.org/papers/v12e/Cauchy-Schwarzinequality.pdf ): $$\alpha \cdot \beta = |\alpha| |\beta| \cos (\alpha, \beta)$$ we deduce that $$\alpha \cdot \beta \le |\alpha| |\beta|$$ Is Cauchy Schwarz inequality Lastly, On Wikipedia: $$|\langle x,y\rangle| \leq \|x\| \cdot \|y\|$$ I must say I am slightly disappointed in the notational inconsistency in the literature. What is the rule/logic for applying the absolute value sign and the norm when it comes to the Cauchy-Schwarz inequality? What is the most correct way to express the CS inequality?","In this widely cited and wildly popular proof of the Cauchy-Schwarz inequality, the authors write ( http://www.math.lsa.umich.edu/~speyer/417/CauchySchwartz.pdf ) Let $u$ and $v$ be two vectors in $R^n$. The Cauchy-Schwartz inequality   states that $$|u \cdot v| ≤ |u||v|$$ I go to another source ( Proof that the Euclidean norm is indeed a norm ), a comment states states: Cauchy-Schwarz: $u \cdot v \le \lVert u\rVert \lVert v\rVert$ Yet another source ( http://rgmia.org/papers/v12e/Cauchy-Schwarzinequality.pdf ): $$\alpha \cdot \beta = |\alpha| |\beta| \cos (\alpha, \beta)$$ we deduce that $$\alpha \cdot \beta \le |\alpha| |\beta|$$ Is Cauchy Schwarz inequality Lastly, On Wikipedia: $$|\langle x,y\rangle| \leq \|x\| \cdot \|y\|$$ I must say I am slightly disappointed in the notational inconsistency in the literature. What is the rule/logic for applying the absolute value sign and the norm when it comes to the Cauchy-Schwarz inequality? What is the most correct way to express the CS inequality?",,"['linear-algebra', 'inequality', 'vector-spaces', 'notation', 'inner-products']"
61,How can Hamilton's quaternion equation be true?,How can Hamilton's quaternion equation be true?,,"I'm reading Ken Shoemake's explanation of quaternions in David Eberly's book Game Physics .  In it, he describes the $\mathbf{i}, \mathbf{j},  \mathbf{k}$ components of quaternions to all equal $\sqrt{-1}$.  Then it states Hamilton's quaternion equation: $\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{ijk} = \mathbf{-1}$ If $\mathbf{i} = \mathbf{j} = \mathbf{k} = \sqrt{-1}$, then it makes sense how $\mathbf{i}^2 = \mathbf{-1}$.  But $\mathbf{ijk}$ should equal $\mathbf{i}^3$, not $\mathbf{i}^2$.  How does $\mathbf{ijk} = \mathbf{-1}$? The book's notation says that lowercase bold letters denote a vector, so I'm thinking of $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ as the basis of the quaternion, similar to the basis of a vector, and can be written $(\sqrt{-1}, \sqrt{-1}, \sqrt{-1})$.  Having the result of $\mathbf{ijk}$ as a bold $\mathbf{-1}$ to me implies that it is the vector $(-1, -1, -1)$.  Is this understanding correct?  In this context, what does it mean to square vector $\mathbf{i}$?  If it equals another vector, then the only operation that makes sense is the cross product, but the cross product of a vector and itself is the zero vector.","I'm reading Ken Shoemake's explanation of quaternions in David Eberly's book Game Physics .  In it, he describes the $\mathbf{i}, \mathbf{j},  \mathbf{k}$ components of quaternions to all equal $\sqrt{-1}$.  Then it states Hamilton's quaternion equation: $\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{ijk} = \mathbf{-1}$ If $\mathbf{i} = \mathbf{j} = \mathbf{k} = \sqrt{-1}$, then it makes sense how $\mathbf{i}^2 = \mathbf{-1}$.  But $\mathbf{ijk}$ should equal $\mathbf{i}^3$, not $\mathbf{i}^2$.  How does $\mathbf{ijk} = \mathbf{-1}$? The book's notation says that lowercase bold letters denote a vector, so I'm thinking of $\mathbf{i}$, $\mathbf{j}$, and $\mathbf{k}$ as the basis of the quaternion, similar to the basis of a vector, and can be written $(\sqrt{-1}, \sqrt{-1}, \sqrt{-1})$.  Having the result of $\mathbf{ijk}$ as a bold $\mathbf{-1}$ to me implies that it is the vector $(-1, -1, -1)$.  Is this understanding correct?  In this context, what does it mean to square vector $\mathbf{i}$?  If it equals another vector, then the only operation that makes sense is the cross product, but the cross product of a vector and itself is the zero vector.",,"['linear-algebra', 'quaternions']"
62,How to express the Frobenius norm of a matrix as the squared norm of its singular values?,How to express the Frobenius norm of a matrix as the squared norm of its singular values?,,"Let the Frobenius norm of an $m \times n$ matrix $M$ be defined as follows $$ \| M \|_{F} := \sqrt{\sum_{i,j} M^2_{i,j}}$$ I was told that it can be proved that, if $M$ can be expressed as follows (which we can because of SVD) $$ M = \sum_{i=1}^r \sigma_i u_i v^T_i $$ Then one can show that the Frobenius norm equivalently be expressed as follows? $$ \| M \|_{F} = \sqrt{\sum_{i} \sigma_i^2} $$ I was a little stuck on how to do such a proof. This is what I had so far: I was thinking that maybe since the second expression is a linear combination of outer produced scaled by $\sigma_i$ , then one could express each entry of M as follow: $M_{i,j} = \sum^{r}_{i=1} \sigma_i (u_i v^T_i)_{i,j}$ . Thus we can substitute: $$\| M \|^2_{F} = \sum_{i,j} M^2_{i,j} = \sum^n_{j=1} \sum^m_{i=1} \left(\sum^{r}_{i=1} \sigma_i \left(u_i v^T_i \right)_{i,j} \right)^2 = \sum^n_{j=1} \sum^m_{i=1} \left(\sum^{r}_{i=1} \sigma_i \left(u_i v^T_i \right)_{i,j} \right) \left(\sum^{r}_{i=1} \sigma_i \left(u_i v^T_i \right)_{i,j} \right) $$ After that line I got kind of stuck. Though my intuition tells me that if I expand what I have somehow, something magical is going to happens with the combination of outer products of orthonormal vectors and get a bunch of zeros! Probably by re-arranging and forming inner products that evaluate to zero (due to orthogonality) ... Though, not sure how to expand that nasty little guy. Anyway has any suggestion on how to move on or if maybe there is a better approach?","Let the Frobenius norm of an matrix be defined as follows I was told that it can be proved that, if can be expressed as follows (which we can because of SVD) Then one can show that the Frobenius norm equivalently be expressed as follows? I was a little stuck on how to do such a proof. This is what I had so far: I was thinking that maybe since the second expression is a linear combination of outer produced scaled by , then one could express each entry of M as follow: . Thus we can substitute: After that line I got kind of stuck. Though my intuition tells me that if I expand what I have somehow, something magical is going to happens with the combination of outer products of orthonormal vectors and get a bunch of zeros! Probably by re-arranging and forming inner products that evaluate to zero (due to orthogonality) ... Though, not sure how to expand that nasty little guy. Anyway has any suggestion on how to move on or if maybe there is a better approach?","m \times n M  \| M \|_{F} := \sqrt{\sum_{i,j} M^2_{i,j}} M  M = \sum_{i=1}^r \sigma_i u_i v^T_i   \| M \|_{F} = \sqrt{\sum_{i} \sigma_i^2}  \sigma_i M_{i,j} = \sum^{r}_{i=1} \sigma_i (u_i v^T_i)_{i,j} \| M \|^2_{F} = \sum_{i,j} M^2_{i,j} = \sum^n_{j=1} \sum^m_{i=1} \left(\sum^{r}_{i=1} \sigma_i \left(u_i v^T_i \right)_{i,j} \right)^2 = \sum^n_{j=1} \sum^m_{i=1} \left(\sum^{r}_{i=1} \sigma_i \left(u_i v^T_i \right)_{i,j} \right) \left(\sum^{r}_{i=1} \sigma_i \left(u_i v^T_i \right)_{i,j} \right) ","['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms', 'singular-values']"
63,"Power of a matrix, given its jordan form","Power of a matrix, given its jordan form",,"Can someone please explain how to find the power of a matrix $A$, given $A=MJM^{-1}$ where the matrix $J$ is in the Jordan canonical form? Or else please explain how to find the powers of a  matrix $J$ that is in the Jordan canonical form.","Can someone please explain how to find the power of a matrix $A$, given $A=MJM^{-1}$ where the matrix $J$ is in the Jordan canonical form? Or else please explain how to find the powers of a  matrix $J$ that is in the Jordan canonical form.",,['linear-algebra']
64,$AB-BA=A$ implies $A$ is singular and $A$ is nilpotent. [duplicate],implies  is singular and  is nilpotent. [duplicate],AB-BA=A A A,"This question already has answers here : If $A=AB-BA$, is $A$ nilpotent? [duplicate] (5 answers) Closed 10 years ago . Let $A$ and $B$ be two real $n\times n$ matrices such that $AB-BA=A$ Prove that $A$ is not invertible and that $A$ is nilpotent. My attempt is the following. It holds that $AB=(B+I)A$ If $A$ were invertible, $B=A^{-1}(B+I)A$ . Taking trace on both sides yields $tr(B)=tr(B)+n$ , hence $n=0$ , which is a contradiction. I can't prove that $A$ is nilpotent though.","This question already has answers here : If $A=AB-BA$, is $A$ nilpotent? [duplicate] (5 answers) Closed 10 years ago . Let and be two real matrices such that Prove that is not invertible and that is nilpotent. My attempt is the following. It holds that If were invertible, . Taking trace on both sides yields , hence , which is a contradiction. I can't prove that is nilpotent though.",A B n\times n AB-BA=A A A AB=(B+I)A A B=A^{-1}(B+I)A tr(B)=tr(B)+n n=0 A,"['linear-algebra', 'matrices']"
65,"Dimension of Annihilator: $\text{dim} \, U^0 + \text{dim} \, U = \text{dim} \, V$",Dimension of Annihilator:,"\text{dim} \, U^0 + \text{dim} \, U = \text{dim} \, V","First there is a vector space $V$ and $U$ is vector subspace of $V$. Furthermore, $U^{0}=\{\varphi \in V^{*} |\space\forall u \in U: \varphi(u) = 0\}$ is the annihilator of $U$. I need to show that: $$\text{dim} \, U^0 + \text{dim} \, U = \text{dim} \, V$$ Do I need the rank and nullity theorem?","First there is a vector space $V$ and $U$ is vector subspace of $V$. Furthermore, $U^{0}=\{\varphi \in V^{*} |\space\forall u \in U: \varphi(u) = 0\}$ is the annihilator of $U$. I need to show that: $$\text{dim} \, U^0 + \text{dim} \, U = \text{dim} \, V$$ Do I need the rank and nullity theorem?",,"['linear-algebra', 'vector-spaces']"
66,Endomorphisms of a finite dimensional vector space,Endomorphisms of a finite dimensional vector space,,"From Humphreys' Introduction to Lie Algebras and Representation Theory: If $V$ is a finite dimensional vector space over $F$, denote by $\text{End }V$ the set of linear transformations $V\rightarrow V$. As a vector space over $F$, $\text{End }V$ has dimension $n^2$ ($n=\text{dim }V$), and $\text{End }V$ is a ring relative to the usual product operation. 1) Why does $\text{End }V$ have dimension $n^2$? I think of an example like $V=\mathbb{R}^n$, with the basis $e_1=(1,0,\ldots,0),e_2=(0,1,\ldots,0),\ldots,e_n=(0,0,\ldots,1)$. Then a linear transformation $V\rightarrow V$ can take $e_1$ to $e_j$ for any $j=1,2,\ldots,n$, take $e_2$ to $e_j$ for any $j=1,2,\ldots,n$, and so on. This should give rise to $n^n$ independent linear transformations. 2) ""$\text{End }V$ is a ring relative to the usual product operation."" What is the product operation referred to here?","From Humphreys' Introduction to Lie Algebras and Representation Theory: If $V$ is a finite dimensional vector space over $F$, denote by $\text{End }V$ the set of linear transformations $V\rightarrow V$. As a vector space over $F$, $\text{End }V$ has dimension $n^2$ ($n=\text{dim }V$), and $\text{End }V$ is a ring relative to the usual product operation. 1) Why does $\text{End }V$ have dimension $n^2$? I think of an example like $V=\mathbb{R}^n$, with the basis $e_1=(1,0,\ldots,0),e_2=(0,1,\ldots,0),\ldots,e_n=(0,0,\ldots,1)$. Then a linear transformation $V\rightarrow V$ can take $e_1$ to $e_j$ for any $j=1,2,\ldots,n$, take $e_2$ to $e_j$ for any $j=1,2,\ldots,n$, and so on. This should give rise to $n^n$ independent linear transformations. 2) ""$\text{End }V$ is a ring relative to the usual product operation."" What is the product operation referred to here?",,"['linear-algebra', 'abstract-algebra']"
67,When do two matrices have the same column space?,When do two matrices have the same column space?,,"Recently I started learning about matrices and know for example that the pivot columns of a matrix form a basis for the column space of this matrix. I just can't seem to find out when two matrices have the same column space. Wouldn't it be enough to show that both the matrices have the same reduced echolon form (like with the null space) and therefore the same basis for the column space and thus the same column space? Can someone help me on this? Edit: Maybe another question: how can you tell that two matrices don't have the same column space? Can I for example use the fact that if two matrices don't have the same dimension for the column space, that the column spaces cannot be equal?","Recently I started learning about matrices and know for example that the pivot columns of a matrix form a basis for the column space of this matrix. I just can't seem to find out when two matrices have the same column space. Wouldn't it be enough to show that both the matrices have the same reduced echolon form (like with the null space) and therefore the same basis for the column space and thus the same column space? Can someone help me on this? Edit: Maybe another question: how can you tell that two matrices don't have the same column space? Can I for example use the fact that if two matrices don't have the same dimension for the column space, that the column spaces cannot be equal?",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
68,How to define sparseness of a vector?,How to define sparseness of a vector?,,"I would like to construct a measure to calculate the sparseness of a vector of length $k$. Let $X = [x_i]$ be a vector of length $k$ such that there exist an $x_i \neq 0$ . Assume $x_i \geq 0$ for all $i$. One such measure I came across is defined as $$\frac{\sqrt{k} - \frac{\|X\|_1}{{\|X\|_2}}} {\sqrt{k} -1}\;,$$ where $\|X\|_1$ is $L_1$ norm and $\|X\|_2$ is $L_2$ norm. Here, $\operatorname{Sparseness}(X) = 0$ whenever the vector is dense (all components are equal and non-zero) and $\operatorname{Sparseness}(X) = 1$ whenever the vector is sparse (only one component is non zero). This post only explains the when $0$ and $1$ achieved by the above mentioned measure. Is there any other function defining the sparseness of the vector.","I would like to construct a measure to calculate the sparseness of a vector of length $k$. Let $X = [x_i]$ be a vector of length $k$ such that there exist an $x_i \neq 0$ . Assume $x_i \geq 0$ for all $i$. One such measure I came across is defined as $$\frac{\sqrt{k} - \frac{\|X\|_1}{{\|X\|_2}}} {\sqrt{k} -1}\;,$$ where $\|X\|_1$ is $L_1$ norm and $\|X\|_2$ is $L_2$ norm. Here, $\operatorname{Sparseness}(X) = 0$ whenever the vector is dense (all components are equal and non-zero) and $\operatorname{Sparseness}(X) = 1$ whenever the vector is sparse (only one component is non zero). This post only explains the when $0$ and $1$ achieved by the above mentioned measure. Is there any other function defining the sparseness of the vector.",,['linear-algebra']
69,Sum of Rows of Vandermonde Matrix,Sum of Rows of Vandermonde Matrix,,"Question Consider some Vandermonde matrix defined by $$ V = \begin{bmatrix} 1 & x_1 &x_1^2& ... & x_1^{M-1}\\ 1 & x_2 &x_2^2& ... & x_2^{M-1}\\ \vdots & \vdots & \vdots && \vdots\\ 1 & x_M &x_M^2& ... & x_M^{M-1}\\ \end{bmatrix} $$ for some $\{x_i :i = 1, 2, ..., M \land x_i \in \mathbb{R}\}$ . Numerically, I have observed that the sum of the rows of the inverse may have an interesting property: $$ \sum_k (V^{-1})_{i,k} = \delta_{i,0}. $$ That is, this sum is equal to one for the first row of $V^{-1}$ and zero for all others. Is there any way for me to demonstrate this property? Attempts I began with elementwise analytical expressions for the inverse of the Vandermonde matrix but was unable to simplify my expressions. Inspired by this question , I used expressions for the sums of a truncated geometric series and obtained the expression: $$ \sum_k V^{-1}_{i,k}\left(\frac{1 - x_k^M}{1-x_k} \right) = 1 $$ But am unable to see a way forward. Finally, I have found one proof which is necessary for $\sum_k (V^{-1})_{i,k} = \delta_{i,0}$ but certainly not sufficient.","Question Consider some Vandermonde matrix defined by for some . Numerically, I have observed that the sum of the rows of the inverse may have an interesting property: That is, this sum is equal to one for the first row of and zero for all others. Is there any way for me to demonstrate this property? Attempts I began with elementwise analytical expressions for the inverse of the Vandermonde matrix but was unable to simplify my expressions. Inspired by this question , I used expressions for the sums of a truncated geometric series and obtained the expression: But am unable to see a way forward. Finally, I have found one proof which is necessary for but certainly not sufficient.","
V = \begin{bmatrix}
1 & x_1 &x_1^2& ... & x_1^{M-1}\\
1 & x_2 &x_2^2& ... & x_2^{M-1}\\
\vdots & \vdots & \vdots && \vdots\\
1 & x_M &x_M^2& ... & x_M^{M-1}\\
\end{bmatrix}
 \{x_i :i = 1, 2, ..., M \land x_i \in \mathbb{R}\} 
\sum_k (V^{-1})_{i,k} = \delta_{i,0}.
 V^{-1} 
\sum_k V^{-1}_{i,k}\left(\frac{1 - x_k^M}{1-x_k} \right) = 1
 \sum_k (V^{-1})_{i,k} = \delta_{i,0}","['linear-algebra', 'matrices', 'summation', 'proof-writing']"
70,Why are Tensors (Vectors of the form a⊗b...⊗z) multilinear maps?,Why are Tensors (Vectors of the form a⊗b...⊗z) multilinear maps?,,"In our linear algebra course we defined the Tensor Product between two vector spaces as an Operation so that this Diagram commutes: Here $\varphi$ and $\iota$ are multilinear maps and $\psi$ is a linear map, that is uniquely determined by $\varphi$ . $\iota$ is uniquely 1 determined by $U$ and $V$ . We constructed the tensor product between two vector spaces $U$ and $V$ as the quotient of the free space on $U \times W$ and a certain subspace $R$ . However as far as I know this construction is not really that important, important is that it obeys the above diagram, so an alternative construction would be equally suitable. Now I have seen multiple posts on the math and physics stack-exchange that seem to define Tensors as multilinear maps. Of course I am very aware that multilinear (or in this case bilinear) maps form a vector space and I suppose that this vector space could somehow be isomorphic to $U \otimes V$ . And I see that the concept of a tensor is very closely linked to multilinear maps (especially the multilinear map $\iota$ ). But I don't see a way to say that tensors are multilinear maps, so I am very confused. Is the statement true, that a tensor is a multilinear map? 1 up to a unique isomorphism","In our linear algebra course we defined the Tensor Product between two vector spaces as an Operation so that this Diagram commutes: Here and are multilinear maps and is a linear map, that is uniquely determined by . is uniquely 1 determined by and . We constructed the tensor product between two vector spaces and as the quotient of the free space on and a certain subspace . However as far as I know this construction is not really that important, important is that it obeys the above diagram, so an alternative construction would be equally suitable. Now I have seen multiple posts on the math and physics stack-exchange that seem to define Tensors as multilinear maps. Of course I am very aware that multilinear (or in this case bilinear) maps form a vector space and I suppose that this vector space could somehow be isomorphic to . And I see that the concept of a tensor is very closely linked to multilinear maps (especially the multilinear map ). But I don't see a way to say that tensors are multilinear maps, so I am very confused. Is the statement true, that a tensor is a multilinear map? 1 up to a unique isomorphism",\varphi \iota \psi \varphi \iota U V U V U \times W R U \otimes V \iota,"['linear-algebra', 'tensor-products', 'tensors', 'multilinear-algebra']"
71,"If $AB=BA$, prove that $ A=\begin{bmatrix} a & 0 \\ 0 & a \end{bmatrix} $","If , prove that",AB=BA  A=\begin{bmatrix} a & 0 \\ 0 & a \end{bmatrix} ,"Let $A$ a $2\times2$ matrix, if $AB=BA$ for every $B$ of the size $2\times2$ , Prove that: $$ A=\begin{bmatrix} a & 0 \\ 0 & a \end{bmatrix} $$ $a \in \mathbb{R}$ My attempt: Let $$ A=\begin{bmatrix} a_1 & b_1 \\ c_1 & d_1 \end{bmatrix} $$ $$B=\begin{bmatrix} a_2 & b_2 \\ c_2 & d_2 \end{bmatrix}$$ And since $AB=BA$ , then $a_1 a_2 + b_1 c_2 = a_1a_2+b_2 c_1$ So $b_2 c_1=b_1 c_2$ And $a_1 b_2+b_1 d_2=a_2 b_1+b_2 d_1$ $c_1 a_2+d_1 c_2=c_2 a_1+d_2 c_1$ But what can I do now ? Thanks :)","Let a matrix, if for every of the size , Prove that: My attempt: Let And since , then So And But what can I do now ? Thanks :)","A 2\times2 AB=BA B 2\times2 
A=\begin{bmatrix} a & 0 \\ 0 & a \end{bmatrix}
 a \in \mathbb{R} 
A=\begin{bmatrix} a_1 & b_1 \\ c_1 & d_1 \end{bmatrix}
 B=\begin{bmatrix} a_2 & b_2 \\ c_2 & d_2 \end{bmatrix} AB=BA a_1 a_2 + b_1 c_2 = a_1a_2+b_2 c_1 b_2 c_1=b_1 c_2 a_1 b_2+b_1 d_2=a_2 b_1+b_2 d_1 c_1 a_2+d_1 c_2=c_2 a_1+d_2 c_1","['linear-algebra', 'matrices']"
72,Why is $A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$ not diagonalizable,Why is  not diagonalizable,A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix},"I have a number of sufficient conditions as to when a matrix $A$ is diagonalizable, namely: When $A$ is symmetric When $A$ has distinct eigenvalues Given $A = \begin{bmatrix} 0 & 1 \\ 0  & 0 \end{bmatrix}$ $A$ has nondistinct eigenvalues $\lambda = 0$ with algebraic multiplicity $2$, is there some conditions that says this is when the matrix fails to be diagonalizable?","I have a number of sufficient conditions as to when a matrix $A$ is diagonalizable, namely: When $A$ is symmetric When $A$ has distinct eigenvalues Given $A = \begin{bmatrix} 0 & 1 \\ 0  & 0 \end{bmatrix}$ $A$ has nondistinct eigenvalues $\lambda = 0$ with algebraic multiplicity $2$, is there some conditions that says this is when the matrix fails to be diagonalizable?",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-equations', 'diagonalization']"
73,How to show that $A^2=AB+BA$ implies $\det(AB-BA)=0$ for $3\times3$ matrices?,How to show that  implies  for  matrices?,A^2=AB+BA \det(AB-BA)=0 3\times3,Let $A$ and $B$ be two $3\times 3$ matrices with complex entries such that $A^2=AB+BA$ . Prove that $\det(AB-BA)=0$ . (Is the above result true for matrices with real entries?),Let and be two matrices with complex entries such that . Prove that . (Is the above result true for matrices with real entries?),A B 3\times 3 A^2=AB+BA \det(AB-BA)=0,"['linear-algebra', 'matrices']"
74,Find the determinant of the following;,Find the determinant of the following;,,"Find the determinant of the following matrix, and for which value of $x$ is it invertible;   $$\begin{pmatrix}   x & 1 & 0 & 0 & 0 & \ldots & 0 & 0 \\   0 & x & 1 & 0 & 0 & \ldots & 0 & 0 \\   0 & 0 & x & 1 & 0 & \ldots & 0 & 0 \\   \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\   \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\   \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\   0 & 0 & 0 & 0 & 0 & \ldots & x & 1 \\   1 & 0 & 0 & 0 & 0 & \ldots & 0 & x  \end{pmatrix}$$ Now I don't really know how to procees as I get find a suitable row operations that will simplify the process so I thought I would look at cases, maybe see a pattern. $\mathbf{2 \times 2}$ $\begin{bmatrix}x & 1\\1 & x\end{bmatrix}$ This has determinant; $x^2-1$ $\mathbf{3 \times 3}$ $\begin{bmatrix}x & 1 & 0\\0 & x & 1\\1 & 0 & x\end{bmatrix}$ This has determinant $x^3+1$ So is that the pattern?  determinant is $x^n-1$ if $n$ is even, determinant is $x^n+1$ if $n$ is odd??","Find the determinant of the following matrix, and for which value of $x$ is it invertible;   $$\begin{pmatrix}   x & 1 & 0 & 0 & 0 & \ldots & 0 & 0 \\   0 & x & 1 & 0 & 0 & \ldots & 0 & 0 \\   0 & 0 & x & 1 & 0 & \ldots & 0 & 0 \\   \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\   \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\   \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\   0 & 0 & 0 & 0 & 0 & \ldots & x & 1 \\   1 & 0 & 0 & 0 & 0 & \ldots & 0 & x  \end{pmatrix}$$ Now I don't really know how to procees as I get find a suitable row operations that will simplify the process so I thought I would look at cases, maybe see a pattern. $\mathbf{2 \times 2}$ $\begin{bmatrix}x & 1\\1 & x\end{bmatrix}$ This has determinant; $x^2-1$ $\mathbf{3 \times 3}$ $\begin{bmatrix}x & 1 & 0\\0 & x & 1\\1 & 0 & x\end{bmatrix}$ This has determinant $x^3+1$ So is that the pattern?  determinant is $x^n-1$ if $n$ is even, determinant is $x^n+1$ if $n$ is odd??",,"['linear-algebra', 'matrices', 'determinant']"
75,Prove that the eigenvalues of skew-Hermitian matrices are purely imaginary,Prove that the eigenvalues of skew-Hermitian matrices are purely imaginary,,"I would like some help on proving that the eigenvalues of skew-Hermitian matrices are all pure imaginary. I have gotten started on it, but am getting stuck. Attempt at proof: $Av=\lambda v \implies A \bar{v}=\bar{\lambda}\bar{v}.$ Also, $v^TA^T=\lambda v^T \implies v^TA^T\bar{v}=\lambda v^T \bar{v} \implies ?$ Should I conjugate both sides next?","I would like some help on proving that the eigenvalues of skew-Hermitian matrices are all pure imaginary. I have gotten started on it, but am getting stuck. Attempt at proof: $Av=\lambda v \implies A \bar{v}=\bar{\lambda}\bar{v}.$ Also, $v^TA^T=\lambda v^T \implies v^TA^T\bar{v}=\lambda v^T \bar{v} \implies ?$ Should I conjugate both sides next?",,['linear-algebra']
76,Intuition behind definition of transpose map,Intuition behind definition of transpose map,,"The linear map $T^t:V' \to V'$ (where $V'$ is the set of all linear maps from $V$ to its scalar field $\Bbb F$) is defined by : $$T^t(f)(v)=f(T(v))$$ This looks like some ""commutative"" definition which helps in many cases (for example, to prove the existence of a basis for which triangular matrix exists). Can someone elaborate on the motivation of this definition.","The linear map $T^t:V' \to V'$ (where $V'$ is the set of all linear maps from $V$ to its scalar field $\Bbb F$) is defined by : $$T^t(f)(v)=f(T(v))$$ This looks like some ""commutative"" definition which helps in many cases (for example, to prove the existence of a basis for which triangular matrix exists). Can someone elaborate on the motivation of this definition.",,"['linear-algebra', 'vector-spaces']"
77,difference between normed linear space and inner product space,difference between normed linear space and inner product space,,I've seen that the definitions of normed linear space and inner product space for a complex vector space $V$ are very close to each other except for the fact that one is defined on $V$ and the other on $V\times V.$ The properties are almost the same. I don't understand what are the differences except for the fact mentioned above. Do both of them have their own importance in the context that both of them provide a properties that are not obtainable by studying the other?,I've seen that the definitions of normed linear space and inner product space for a complex vector space $V$ are very close to each other except for the fact that one is defined on $V$ and the other on $V\times V.$ The properties are almost the same. I don't understand what are the differences except for the fact mentioned above. Do both of them have their own importance in the context that both of them provide a properties that are not obtainable by studying the other?,,"['linear-algebra', 'functional-analysis', 'normed-spaces', 'inner-products']"
78,"How to interpret ""rank"" of a matrix intuitively?","How to interpret ""rank"" of a matrix intuitively?",,"What is the physical interpretation of ""rank"" of a matrix ? Why is it called ""rank"" ?","What is the physical interpretation of ""rank"" of a matrix ? Why is it called ""rank"" ?",,['linear-algebra']
79,"Prove that if matrix $A$ is nilpotent, then $I+A$ is invertible. [duplicate]","Prove that if matrix  is nilpotent, then  is invertible. [duplicate]",A I+A,"This question already has answers here : Units and Nilpotents (9 answers) Closed 11 years ago . So my friend and I are working on this and here is what we have so far. We want to show that $\exists \, B$ s.t. $(I+A)B = I$. We considered the fact that $I - A^k = I$ for some positive $k$. Now, if $B = (I-A+A^2-A^3+ \cdots -A^{k-1})$, then $(I+A)B = I-A^k = I$. My question is: in matrix $B$, why is the sign for $A^{k-1}$ negative? Couldn't it be positive, in which case we'd get $(I+A)B = I + A^k$? Thank you.","This question already has answers here : Units and Nilpotents (9 answers) Closed 11 years ago . So my friend and I are working on this and here is what we have so far. We want to show that $\exists \, B$ s.t. $(I+A)B = I$. We considered the fact that $I - A^k = I$ for some positive $k$. Now, if $B = (I-A+A^2-A^3+ \cdots -A^{k-1})$, then $(I+A)B = I-A^k = I$. My question is: in matrix $B$, why is the sign for $A^{k-1}$ negative? Couldn't it be positive, in which case we'd get $(I+A)B = I + A^k$? Thank you.",,['linear-algebra']
80,Eigenvalues of a rectangular matrix,Eigenvalues of a rectangular matrix,,I've read that the singular values of a matrix are equal to the $$\sigma=\sqrt{\lambda_{K}}$$ where $\lambda$ are the eigenvalue but I'm assuming this only applies to square matrices. How could I determine the eigenvalues of a non-square matrix. Pardon my ignorance.,I've read that the singular values of a matrix are equal to the $$\sigma=\sqrt{\lambda_{K}}$$ where $\lambda$ are the eigenvalue but I'm assuming this only applies to square matrices. How could I determine the eigenvalues of a non-square matrix. Pardon my ignorance.,,['linear-algebra']
81,Complex vector spaces,Complex vector spaces,,"I was wondering if someone could please shed some light on how the following two vector spaces are different. The examples are from Paul Halmos's ""Finite Dimensional vector spaces"" book and the author asks the reader to verify that they are different, but i am not so sure if i understand the two examples well enough to spot all the differences. (1) Let $\mathbb C^1$ be the set of all complex numbers; if we interpret ""$x + y$"" and ""$ax$"" as ordinary complex numerical addition and multiplication, $\mathbb C^1$ becomes a complex vector space. (2) If, in the set $\mathbb C$ of all complex numbers, addition is defined as usual and multiplication of a complex number by a real number is defined as usual, then $\mathbb C$ becomes a real vector space. Is the difference between these two vector spaces examples only that in the first the scalar field is also the set $\mathbb C$ and in the second one the scalar field is $\mathbb R$ and hence only the multiplication and distributive properties would behave differently? What about dimensions of the two examples? Any Help would be highly appreciated. Cheers Hardy","I was wondering if someone could please shed some light on how the following two vector spaces are different. The examples are from Paul Halmos's ""Finite Dimensional vector spaces"" book and the author asks the reader to verify that they are different, but i am not so sure if i understand the two examples well enough to spot all the differences. (1) Let $\mathbb C^1$ be the set of all complex numbers; if we interpret ""$x + y$"" and ""$ax$"" as ordinary complex numerical addition and multiplication, $\mathbb C^1$ becomes a complex vector space. (2) If, in the set $\mathbb C$ of all complex numbers, addition is defined as usual and multiplication of a complex number by a real number is defined as usual, then $\mathbb C$ becomes a real vector space. Is the difference between these two vector spaces examples only that in the first the scalar field is also the set $\mathbb C$ and in the second one the scalar field is $\mathbb R$ and hence only the multiplication and distributive properties would behave differently? What about dimensions of the two examples? Any Help would be highly appreciated. Cheers Hardy",,"['linear-algebra', 'vector-spaces']"
82,$A\in M_n(\mathbb C)$ invertible and non-diagonalizable matrix. Prove $A^{2005}$ is not diagonalizable,invertible and non-diagonalizable matrix. Prove  is not diagonalizable,A\in M_n(\mathbb C) A^{2005},"$A\in M_n(\mathbb C)$ invertible and non-diagonalizable matrix. I need to prove that $A^{2005}$ is not diagonalizable as well. I am asked as well if Is it true also for  $A\in M_n(\mathbb R)$. (clearly a question from 2005). This is what I did: If $A\in M_n(\mathbb C)$ is invertible so $0$ is not an eigenvalue, We can look on its Jordan form, Since we under $\mathbb C$, and it is nilpotent for sure since $0$ is not an eigenvalue, and it has at least one 1 in it's semi-diagonal. Let $P$ be the matrix with Jordan base, so $P^{-1}AP=J$ and $P^{-1}A^{2005}P$ but it leads me nowhere. I tried to suppose that $A^{2005}$ is diagonalizable and than we have this $P^{-1}A^{2005}P=D$ When D is diagonal and we can take 2005th root out of each eigenvalue, but how can I show that this is what A after being diagonalizable suppose to look like, for as contradiction? Thanks","$A\in M_n(\mathbb C)$ invertible and non-diagonalizable matrix. I need to prove that $A^{2005}$ is not diagonalizable as well. I am asked as well if Is it true also for  $A\in M_n(\mathbb R)$. (clearly a question from 2005). This is what I did: If $A\in M_n(\mathbb C)$ is invertible so $0$ is not an eigenvalue, We can look on its Jordan form, Since we under $\mathbb C$, and it is nilpotent for sure since $0$ is not an eigenvalue, and it has at least one 1 in it's semi-diagonal. Let $P$ be the matrix with Jordan base, so $P^{-1}AP=J$ and $P^{-1}A^{2005}P$ but it leads me nowhere. I tried to suppose that $A^{2005}$ is diagonalizable and than we have this $P^{-1}A^{2005}P=D$ When D is diagonal and we can take 2005th root out of each eigenvalue, but how can I show that this is what A after being diagonalizable suppose to look like, for as contradiction? Thanks",,['linear-algebra']
83,Minimal polynomial and diagonalizable matrix,Minimal polynomial and diagonalizable matrix,,"Let $B,C$ square matrices above a field,and $D$ Rectangle matrix in the correct size above the same field. $A=\begin{pmatrix} B &D \\   0& C \end{pmatrix}$ I need to prove that if $A$ is Diagonalizable so Do $C$ and $B$, And if $B$ and $c$ are Diagonalizable matrices with no common eigenvalue, so $A$ is Diagonalizable. So I want to use the claim that says that if a matrix is Diagonalizable so it's Minimal polynomial splits to different factors with degree 1. The second claim is easy because We know that $M_{A}(x)=M_{B}(x)M_{C}(x)$ and it is easily can be shown, but is this claim works the other way? Can I use it also the other way? And if not, what should I do to prove it? (Please write simple and clear as you can cause I lose it with explanations of Linear algebra in English, differently from calculus, that for some reason it's more easy and fluent to me) Thank you very much!","Let $B,C$ square matrices above a field,and $D$ Rectangle matrix in the correct size above the same field. $A=\begin{pmatrix} B &D \\   0& C \end{pmatrix}$ I need to prove that if $A$ is Diagonalizable so Do $C$ and $B$, And if $B$ and $c$ are Diagonalizable matrices with no common eigenvalue, so $A$ is Diagonalizable. So I want to use the claim that says that if a matrix is Diagonalizable so it's Minimal polynomial splits to different factors with degree 1. The second claim is easy because We know that $M_{A}(x)=M_{B}(x)M_{C}(x)$ and it is easily can be shown, but is this claim works the other way? Can I use it also the other way? And if not, what should I do to prove it? (Please write simple and clear as you can cause I lose it with explanations of Linear algebra in English, differently from calculus, that for some reason it's more easy and fluent to me) Thank you very much!",,[]
84,Natural isomorphism in linear algebra: is the naturalness asymmetric?,Natural isomorphism in linear algebra: is the naturalness asymmetric?,,"Let $V$ be a finite-dimensional vector space. It is well known that there is a natural isomorphism between $V$ and its double dual $V^{\ast\ast}$ defined by $T(x)(f)=f(x)$ for every $x\in V$ and $f\in V^\ast$ . However, I am unable to write down a definition of $T^{-1}$ directly without any reference to $T$ and without picking any basis. Thus it seems to me that the isomorphism between the two vector spaces is not really so natural if we look at the direction from $V^{\ast\ast}$ to $V$ . Is it possible to define $T^{-1}$ in a direct and natural way? If not, does this asymmetry in the easiness of definition have any significance in linear algebra or other branches of mathematics?","Let be a finite-dimensional vector space. It is well known that there is a natural isomorphism between and its double dual defined by for every and . However, I am unable to write down a definition of directly without any reference to and without picking any basis. Thus it seems to me that the isomorphism between the two vector spaces is not really so natural if we look at the direction from to . Is it possible to define in a direct and natural way? If not, does this asymmetry in the easiness of definition have any significance in linear algebra or other branches of mathematics?",V V V^{\ast\ast} T(x)(f)=f(x) x\in V f\in V^\ast T^{-1} T V^{\ast\ast} V T^{-1},"['linear-algebra', 'soft-question', 'vector-space-isomorphism']"
85,Exponential of Pauli Matrices,Exponential of Pauli Matrices,,"Let $\vec{v}$ be any real three-dimensional unit vector and $\theta$ a real number. Prove that $\exp(i\theta \vec{v}\cdot\vec{\sigma}) = \cos(\theta)I + i\sin(\theta)\vec{v}\cdot\vec{\sigma}$ , where $\vec{v}\cdot\vec{\sigma} \equiv \sum_{k=1}^{3}v_{k}\sigma_{k}$ . $\vec{v}\cdot\vec{\sigma}$ is a scalar so let this scalar be a non-zero integer $n$ . Effectively, this gives $\exp(i\theta n)$ . $\exp(i\theta n) \Rightarrow (\cos(\theta) + i \sin(\theta))^{n}$ I would appreciates if anyone could provide to me a hint to this. The computation becomes messy very quickly. Thanks in advance.","Let be any real three-dimensional unit vector and a real number. Prove that , where . is a scalar so let this scalar be a non-zero integer . Effectively, this gives . I would appreciates if anyone could provide to me a hint to this. The computation becomes messy very quickly. Thanks in advance.",\vec{v} \theta \exp(i\theta \vec{v}\cdot\vec{\sigma}) = \cos(\theta)I + i\sin(\theta)\vec{v}\cdot\vec{\sigma} \vec{v}\cdot\vec{\sigma} \equiv \sum_{k=1}^{3}v_{k}\sigma_{k} \vec{v}\cdot\vec{\sigma} n \exp(i\theta n) \exp(i\theta n) \Rightarrow (\cos(\theta) + i \sin(\theta))^{n},"['linear-algebra', 'trigonometry', 'quantum-computation']"
86,Is a linearly independent set whose span is dense a Schauder basis?,Is a linearly independent set whose span is dense a Schauder basis?,,"If $X$ is a Banach space, then a Schauder basis of $X$ is a subset $B$ of $X$ such that every element of $X$ can be written uniquely as an infinite linear combination of elements of $B$ .  My question is, if $A$ is a linearly independent subset of $X$ such that the closure of the span of $A$ equals $X$ , then is $A$ necessarily a Schauder basis of $X$ ? If not, does anyone know of any counterexamples?","If is a Banach space, then a Schauder basis of is a subset of such that every element of can be written uniquely as an infinite linear combination of elements of .  My question is, if is a linearly independent subset of such that the closure of the span of equals , then is necessarily a Schauder basis of ? If not, does anyone know of any counterexamples?",X X B X X B A X A X A X,"['linear-algebra', 'functional-analysis', 'banach-spaces', 'normed-spaces', 'schauder-basis']"
87,How do you differentiate a matrix equation with respect to a vector?,How do you differentiate a matrix equation with respect to a vector?,,"I'm having lots of trouble piecing this together (from Elements of Statistical Learning by Hastie/Friedman): I don't understand the step: ""[d]ifferentiating w.r.t  $B$"", specifically how to calculate the derivative of an equation involving matrix products and transposes with respect to a vector.  Is this standard matrix calculus?","I'm having lots of trouble piecing this together (from Elements of Statistical Learning by Hastie/Friedman): I don't understand the step: ""[d]ifferentiating w.r.t  $B$"", specifically how to calculate the derivative of an equation involving matrix products and transposes with respect to a vector.  Is this standard matrix calculus?",,"['linear-algebra', 'derivatives']"
88,"Is there a clean way to write down this ""product"" of a vector and a matrix? Does it have any interpretation?","Is there a clean way to write down this ""product"" of a vector and a matrix? Does it have any interpretation?",,"Let $$v = \begin{pmatrix}   v_1 \\   \vdots \\   v_m  \end{pmatrix}$$ and $$A =   \begin{pmatrix}   a_{11} & \cdots & a_{1n} \\   \vdots & \ddots &  \vdots\\   a_{m1} & \cdots & a_{mn}  \end{pmatrix}$$ If we define the operation $\Box$ as follows $$v \Box A = \begin{pmatrix}   v_1 a_{11} & \cdots & v_1 a_{1n} \\   \vdots & \ddots &  \vdots\\   v_m a_{m1} & \cdots & v_m a_{mn}  \end{pmatrix},$$ is there a clean way to write it down? Does the operation have any interpretation? I was hoping it has something to do with tensor products but, after looking up about tensors, it doesn't look like it.","Let $$v = \begin{pmatrix}   v_1 \\   \vdots \\   v_m  \end{pmatrix}$$ and $$A =   \begin{pmatrix}   a_{11} & \cdots & a_{1n} \\   \vdots & \ddots &  \vdots\\   a_{m1} & \cdots & a_{mn}  \end{pmatrix}$$ If we define the operation $\Box$ as follows $$v \Box A = \begin{pmatrix}   v_1 a_{11} & \cdots & v_1 a_{1n} \\   \vdots & \ddots &  \vdots\\   v_m a_{m1} & \cdots & v_m a_{mn}  \end{pmatrix},$$ is there a clean way to write it down? Does the operation have any interpretation? I was hoping it has something to do with tensor products but, after looking up about tensors, it doesn't look like it.",,"['linear-algebra', 'matrices', 'notation', 'vectors', 'tensor-products']"
89,Jordan form of a power of Jordan block?,Jordan form of a power of Jordan block?,,"How, in general, does one find the Jordan form of a power of a Jordan block? Because of the comments on this question I think there is a simple answer.","How, in general, does one find the Jordan form of a power of a Jordan block? Because of the comments on this question I think there is a simple answer.",,"['linear-algebra', 'matrices', 'exponentiation', 'jordan-normal-form']"
90,"Matrices that are not diagonal or triangular, whose eigenvalues are the diagonal elements","Matrices that are not diagonal or triangular, whose eigenvalues are the diagonal elements",,"I want to learn about matrices whose diagonal elements are the eigenvalues... but the matrix is neither diagonal nor triangular. Is there a term for such matrices, and have they been researched?","I want to learn about matrices whose diagonal elements are the eigenvalues... but the matrix is neither diagonal nor triangular. Is there a term for such matrices, and have they been researched?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
91,Linear Algebra - eigenvalues,Linear Algebra - eigenvalues,,"I have this problem: Let $A$ be any $n \times n$ matrix, defined over the real numbers, such that $A-A^2=I$. Then prove that $A$ does not have any real eigenvalues. What I did: $$A-A^2=I$$ $$A-A^2-I=0$$ $$A(A-I)-I=0$$ Now I need to show that $A(A-I) \neq I$. Assume that $A=2I$. (If $A=I$ then $0 \neq I$.) Then $$A(A-I)=I$$ $$2I(2I-I)=I$$ $$2I(I)=I$$ $$2I \neq I$$ This equation doesn't have a solution, but I don't think I proved correctly. Any help will be appreciated, thanks.","I have this problem: Let $A$ be any $n \times n$ matrix, defined over the real numbers, such that $A-A^2=I$. Then prove that $A$ does not have any real eigenvalues. What I did: $$A-A^2=I$$ $$A-A^2-I=0$$ $$A(A-I)-I=0$$ Now I need to show that $A(A-I) \neq I$. Assume that $A=2I$. (If $A=I$ then $0 \neq I$.) Then $$A(A-I)=I$$ $$2I(2I-I)=I$$ $$2I(I)=I$$ $$2I \neq I$$ This equation doesn't have a solution, but I don't think I proved correctly. Any help will be appreciated, thanks.",,['linear-algebra']
92,$A$ is invertible matrix iff $Ax=0$ has the trivial solution only.,is invertible matrix iff  has the trivial solution only.,A Ax=0,"Why does the following statemnet true? $A$ is invertible matrix iff $Ax=0$ has only the trivial solution. My try : Let $x$ a solution of $Ax = 0$. Then, because $A$ is invertible there is $A^{-1}$. Hence, $$AA^{-1}x = 0$$ $$I_nx = 0$$ $$x=0$$ I used the associative property of matrix multiplication.","Why does the following statemnet true? $A$ is invertible matrix iff $Ax=0$ has only the trivial solution. My try : Let $x$ a solution of $Ax = 0$. Then, because $A$ is invertible there is $A^{-1}$. Hence, $$AA^{-1}x = 0$$ $$I_nx = 0$$ $$x=0$$ I used the associative property of matrix multiplication.",,"['linear-algebra', 'matrices']"
93,Puzzle in Percentages,Puzzle in Percentages,,"Okay, this is a real-time problem. The following is a picture of Customer satisfaction rating, which was displayed next to an item in an online shopping website. Satisfied customers click the vote-up button, and unsatisfied customers click the vote-down button. Every time a button is clicked, the rating changes accordingly. From the above image, we can infer that 94% of the people are satisfied with the product and the rest 6% aren't satisfied. Now, is there any way to find out the number of people who participated in voting? Edit: It's clear that there could be infinite solutions evident from Lord Soth's answer. So, I'll change the question a bit. What could be the minimum no of people who participated in voting??","Okay, this is a real-time problem. The following is a picture of Customer satisfaction rating, which was displayed next to an item in an online shopping website. Satisfied customers click the vote-up button, and unsatisfied customers click the vote-down button. Every time a button is clicked, the rating changes accordingly. From the above image, we can infer that 94% of the people are satisfied with the product and the rest 6% aren't satisfied. Now, is there any way to find out the number of people who participated in voting? Edit: It's clear that there could be infinite solutions evident from Lord Soth's answer. So, I'll change the question a bit. What could be the minimum no of people who participated in voting??",,"['linear-algebra', 'probability', 'puzzle', 'average']"
94,What are the relations between eigenvectors of $A$ and its adjoint $A^*$?,What are the relations between eigenvectors of  and its adjoint ?,A A^*,Everywhere I can read that a matrix and its adjoint have pretty much the same eigenvalues ( only complex conjugation is the difference between them). Now I was wondering whether such a relation also exists between the eigenvectors of both matrices. Do they have something in common?,Everywhere I can read that a matrix and its adjoint have pretty much the same eigenvalues ( only complex conjugation is the difference between them). Now I was wondering whether such a relation also exists between the eigenvectors of both matrices. Do they have something in common?,,[]
95,Number of matrices whose square is identity,Number of matrices whose square is identity,,"How many matrices are such that $A^2 =I$, where $A$ is a $2\times2$ matrix and $I$ is a $2\times2$ identity matrix? I can only think of the identity and it negative are they more? Is it an application of Cayley-Hamilton theorem. I have seen a similarly post by I cannot follow it. Could someone answer in simple and understandable terms.","How many matrices are such that $A^2 =I$, where $A$ is a $2\times2$ matrix and $I$ is a $2\times2$ identity matrix? I can only think of the identity and it negative are they more? Is it an application of Cayley-Hamilton theorem. I have seen a similarly post by I cannot follow it. Could someone answer in simple and understandable terms.",,['linear-algebra']
96,"Meaning of, and how to verify, a vector space *over* $\mathbb{R}$","Meaning of, and how to verify, a vector space *over*",\mathbb{R},"In Axler's book on Linear Algebra he writes ($\mathbb{F}$ is here either $\mathbb{R}$ or $\mathbb{C}$): The scalar multiplication in a vector space depends upon $\mathbb{F}$. Thus when we need to be precise, we will say that $V$ is a vector space over $\mathbb{F}$ instead of saying simply that $V$ is a vector space. For example, $\mathbb{R}^n$ is a vector space over $\mathbb{R}$, and $\mathbb{C}^n$ is a vector space over $\mathbb{C}$. What is actually meant by $\mathbb{R}^n$ being a vector space over $\mathbb{R}$ and how can one verify that it is? What are all the implications of this? Is it just that any scalar multiplication in $\mathbb{R}^n$ depends upon numbers in $\mathbb{R}$? To me, $\mathbb{R}^n$ feels larger than $\mathbb{R}$, so I find the wording, that one is over the other, hard to digest...","In Axler's book on Linear Algebra he writes ($\mathbb{F}$ is here either $\mathbb{R}$ or $\mathbb{C}$): The scalar multiplication in a vector space depends upon $\mathbb{F}$. Thus when we need to be precise, we will say that $V$ is a vector space over $\mathbb{F}$ instead of saying simply that $V$ is a vector space. For example, $\mathbb{R}^n$ is a vector space over $\mathbb{R}$, and $\mathbb{C}^n$ is a vector space over $\mathbb{C}$. What is actually meant by $\mathbb{R}^n$ being a vector space over $\mathbb{R}$ and how can one verify that it is? What are all the implications of this? Is it just that any scalar multiplication in $\mathbb{R}^n$ depends upon numbers in $\mathbb{R}$? To me, $\mathbb{R}^n$ feels larger than $\mathbb{R}$, so I find the wording, that one is over the other, hard to digest...",,"['linear-algebra', 'vector-spaces']"
97,Involuted vs Idempotent,Involuted vs Idempotent,,"What is the difference between an ""involuted"" and an ""idempotent"" matrix? I believe that they both have to do with inverse, perhaps ""self inverse"" matrices. Or do they happen to refer to the same thing?","What is the difference between an ""involuted"" and an ""idempotent"" matrix? I believe that they both have to do with inverse, perhaps ""self inverse"" matrices. Or do they happen to refer to the same thing?",,"['linear-algebra', 'terminology']"
98,Fourier basis functions,Fourier basis functions,,What are Fourier basis functions? And how do I prove that Fourier basis functions are orthonormal?,What are Fourier basis functions? And how do I prove that Fourier basis functions are orthonormal?,,"['linear-algebra', 'fourier-analysis', 'orthonormal']"
99,On the commutativity of matrices and their exponentials,On the commutativity of matrices and their exponentials,,"It is fairly easy to see that if $A$ and $B $ , real square matrices, commute, then $A $ and $e^B $ commute. In fact, $$Ae^B = \sum_{n=0}^\infty A\frac{B^n}{n!} = \sum_{n=0}^\infty \frac{B^n}{n!}A = e^BA $$ But is the reverse true as well? If $A $ and $e^B $ commute, do $A $ and $B $ commute as well? If not, can you help me finding a counter-example? What if both $A $ and $B $ are not constant matrices but matricial functions of $t $ ?","It is fairly easy to see that if and , real square matrices, commute, then and commute. In fact, But is the reverse true as well? If and commute, do and commute as well? If not, can you help me finding a counter-example? What if both and are not constant matrices but matricial functions of ?",A B  A  e^B  Ae^B = \sum_{n=0}^\infty A\frac{B^n}{n!} = \sum_{n=0}^\infty \frac{B^n}{n!}A = e^BA  A  e^B  A  B  A  B  t ,"['linear-algebra', 'matrices', 'matrix-exponential']"
