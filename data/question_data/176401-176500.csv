,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lower bound of a function,Lower bound of a function,,"Given $x \geq y > 0$ and an integer $n$. We want to minimize the following term $\sum_{i=1}^n (x_i^2 - x_iy_i)$ over $(x_1,\ldots,x_n)$, $(y_1,\ldots,y_n)$ non-negative such that $\sum_{i=1}^n x_i = x$ and $\sum_{i=1}^n y_i = y$. Could we express the lower bound as a function of $x$ and $y$ (and probably $n$)? For example, $\sum_{i=1}^n (x_i^2 - x_iy_i) \geq -y^2/4$. However, is there a better bound? What happen if we modify the simplex as the following: $\sum_i x_i \leq x$, $\sum_i y_i \leq y$ and $\sum_i y_i \leq \sum_i x_i$. (All $x$ and $y$ are non-negative).","Given $x \geq y > 0$ and an integer $n$. We want to minimize the following term $\sum_{i=1}^n (x_i^2 - x_iy_i)$ over $(x_1,\ldots,x_n)$, $(y_1,\ldots,y_n)$ non-negative such that $\sum_{i=1}^n x_i = x$ and $\sum_{i=1}^n y_i = y$. Could we express the lower bound as a function of $x$ and $y$ (and probably $n$)? For example, $\sum_{i=1}^n (x_i^2 - x_iy_i) \geq -y^2/4$. However, is there a better bound? What happen if we modify the simplex as the following: $\sum_i x_i \leq x$, $\sum_i y_i \leq y$ and $\sum_i y_i \leq \sum_i x_i$. (All $x$ and $y$ are non-negative).",,"['real-analysis', 'multivariable-calculus', 'convex-optimization']"
1,What strategy do you use when solving vector equations involving $\nabla$?,What strategy do you use when solving vector equations involving ?,\nabla,"$\Phi, \Lambda$ are both scalars dependent upon, and $\mathbf u$ is a vector independent of coordinates. I'm trying to express $\Lambda$ in terms from $\mathbf U \cdot \nabla\Lambda = \Phi$ and to start with, since I'm just familiar with the basics, it looks pretty hopless. So: Get help from people more expert in this area Trawl through the list of standard vector identitites involving $\nabla$ Search for an online mathematics program to solve it. Make it simpler to start with by solving in one dimension, and progress from there what strategy would/did you use in tackling this problem? Solving it would be a bonus ;) Edit: I've added the context of the problem since some people think this will help: I'm trying to work out the conserved canonical momentum for a static electric field, using Noether's theorem on the relativistic electromagnetic Lagrangian $$L = \frac{m_0c^2}{\gamma} + \frac{e}{c}\mathbf{u \cdot A} - e\Phi(x,y,z)$$ $L$ needs to be independent of coordinates which can be done by transforming $\mathbf A\rightarrow \mathbf A'= \mathbf A + \nabla\Lambda$ The conserved canonical momentum $$ P = \frac\partial{\partial \mathbf u} ( \frac{-m_0c^2}{\gamma} + \frac{e}{c}\mathbf{u \cdot (A+\nabla\Lambda}) - e\Phi)$$ With no magnetic field $A=0$ $$P = -m_0\mathbf u\gamma +\frac e c \frac\partial {\partial\mathbf u}\mathbf u  \cdot\nabla\Lambda$$ becomes $$P = -m_0\mathbf u\gamma +\frac e c (\nabla\Lambda + \mathbf u  \cdot\frac\partial {\partial\mathbf u}\mathbf\nabla\Lambda)$$ To get any further, I need to know the form $\Lambda$ must take, which comes from making $L$ independent of the coordinates before, and so $$\nabla( \frac{e}{c}\mathbf u \nabla\Lambda - e\Phi) = 0$$","$\Phi, \Lambda$ are both scalars dependent upon, and $\mathbf u$ is a vector independent of coordinates. I'm trying to express $\Lambda$ in terms from $\mathbf U \cdot \nabla\Lambda = \Phi$ and to start with, since I'm just familiar with the basics, it looks pretty hopless. So: Get help from people more expert in this area Trawl through the list of standard vector identitites involving $\nabla$ Search for an online mathematics program to solve it. Make it simpler to start with by solving in one dimension, and progress from there what strategy would/did you use in tackling this problem? Solving it would be a bonus ;) Edit: I've added the context of the problem since some people think this will help: I'm trying to work out the conserved canonical momentum for a static electric field, using Noether's theorem on the relativistic electromagnetic Lagrangian $$L = \frac{m_0c^2}{\gamma} + \frac{e}{c}\mathbf{u \cdot A} - e\Phi(x,y,z)$$ $L$ needs to be independent of coordinates which can be done by transforming $\mathbf A\rightarrow \mathbf A'= \mathbf A + \nabla\Lambda$ The conserved canonical momentum $$ P = \frac\partial{\partial \mathbf u} ( \frac{-m_0c^2}{\gamma} + \frac{e}{c}\mathbf{u \cdot (A+\nabla\Lambda}) - e\Phi)$$ With no magnetic field $A=0$ $$P = -m_0\mathbf u\gamma +\frac e c \frac\partial {\partial\mathbf u}\mathbf u  \cdot\nabla\Lambda$$ becomes $$P = -m_0\mathbf u\gamma +\frac e c (\nabla\Lambda + \mathbf u  \cdot\frac\partial {\partial\mathbf u}\mathbf\nabla\Lambda)$$ To get any further, I need to know the form $\Lambda$ must take, which comes from making $L$ independent of the coordinates before, and so $$\nabla( \frac{e}{c}\mathbf u \nabla\Lambda - e\Phi) = 0$$",,"['multivariable-calculus', 'physics']"
2,Brackets around gradients,Brackets around gradients,,"I am told that when transforming coordinates $\nabla (f(Ax)) = A^T(\nabla f)(Ax)$, however I read both of these as ""the gradient of f, where f is a function of Ax, where A is a matrix and x is a vector"", with the second multiplied with $A^T$. This is obviously incorrect, so how am I misunderstanding the notation? Thanks, Ash","I am told that when transforming coordinates $\nabla (f(Ax)) = A^T(\nabla f)(Ax)$, however I read both of these as ""the gradient of f, where f is a function of Ax, where A is a matrix and x is a vector"", with the second multiplied with $A^T$. This is obviously incorrect, so how am I misunderstanding the notation? Thanks, Ash",,['multivariable-calculus']
3,Multiple integral Issue,Multiple integral Issue,,"I'm given the following exercise: $\iint\limits_D \exp(x^{2}+y^{2})dA$ And I dont even know where to start, any chance someone could give me a hint? D is a half circle, given by: $9\le x^{2}+y^{2}\le 16, y\ge 0$ Wheres A, I don't know what it is, its not given in the exercise. - Thats partly whats bugging me.","I'm given the following exercise: $\iint\limits_D \exp(x^{2}+y^{2})dA$ And I dont even know where to start, any chance someone could give me a hint? D is a half circle, given by: $9\le x^{2}+y^{2}\le 16, y\ge 0$ Wheres A, I don't know what it is, its not given in the exercise. - Thats partly whats bugging me.",,['calculus']
4,Area of a Quater-Circle with hyperbolic elements,Area of a Quater-Circle with hyperbolic elements,,"The actual question states the following; ""Find the mass of a Quater-Disc (in terms of R), in the first quadrant, of radius 'R' if density varies as D = xy"" My first thought was somehow turning this problem into another one in hyperbolic coordinates and integrating for the transformation of a circle there with density varying radially, but then I realised I can't do that because I don't know how to. I also thought about taking hyperbolic elements and the density function as xy=k where k goes from 0 to R/2 but I just couldn't figure out how to take elements that don't have an anchor for me to keep constant. I can visually grasp the problem statement and its intention but I fail to materialize it into equations I can evaluate.. Any help is appreciated.","The actual question states the following; ""Find the mass of a Quater-Disc (in terms of R), in the first quadrant, of radius 'R' if density varies as D = xy"" My first thought was somehow turning this problem into another one in hyperbolic coordinates and integrating for the transformation of a circle there with density varying radially, but then I realised I can't do that because I don't know how to. I also thought about taking hyperbolic elements and the density function as xy=k where k goes from 0 to R/2 but I just couldn't figure out how to take elements that don't have an anchor for me to keep constant. I can visually grasp the problem statement and its intention but I fail to materialize it into equations I can evaluate.. Any help is appreciated.",,"['calculus', 'integration', 'multivariable-calculus']"
5,A simple question about the Hodge star,A simple question about the Hodge star,,"The usual definition of the Hodge star says that it maps $\Lambda^k(V)$ to $\Lambda^{n-k}(V)$ in such a way that for each pair $\omega, \eta \in \Lambda^k(V)$ holds $\omega \wedge *\eta = \langle \omega, \eta \rangle \operatorname{vol}$ . I was curious whether this definition is equivalent to saying that $\omega \wedge *\omega = \operatorname{vol}$ for each $\omega$ of unit norm. Initially I thought that it should follow immediately from the polarization identity as follows: $$ \begin{aligned} 2 \, \langle \omega, \eta \rangle \operatorname{vol} & = \langle \omega, \omega \rangle \operatorname{vol} + \langle \eta, \eta \rangle \operatorname{vol} -\langle \omega-\eta, \omega-\eta\rangle \operatorname{vol} \\[3pt] & = \omega \wedge *\omega + \eta \wedge *\eta - (\omega-\eta) \wedge *(\omega-\eta) \\[3pt] & = \omega \wedge *\eta + \eta \wedge *\omega. \end{aligned} $$ This would give a proof if $\omega \wedge * \eta$ were equal to $\eta \wedge *\omega$ , but this doesn't seem to follow from the definition of $*\omega$ by $\omega \wedge *\omega = \operatorname{vol}$ . So my question is what am I missing? Or does the definition with one $\omega$ instead of two $\omega, \eta$ allow for a different choice of $*\omega$ ?","The usual definition of the Hodge star says that it maps to in such a way that for each pair holds . I was curious whether this definition is equivalent to saying that for each of unit norm. Initially I thought that it should follow immediately from the polarization identity as follows: This would give a proof if were equal to , but this doesn't seem to follow from the definition of by . So my question is what am I missing? Or does the definition with one instead of two allow for a different choice of ?","\Lambda^k(V) \Lambda^{n-k}(V) \omega, \eta \in \Lambda^k(V) \omega \wedge *\eta = \langle \omega, \eta \rangle \operatorname{vol} \omega \wedge *\omega = \operatorname{vol} \omega 
\begin{aligned}
2 \, \langle \omega, \eta \rangle \operatorname{vol}
& =
\langle \omega, \omega \rangle \operatorname{vol}
+ \langle \eta, \eta \rangle \operatorname{vol}
-\langle \omega-\eta, \omega-\eta\rangle \operatorname{vol}
\\[3pt]
& = \omega \wedge *\omega + \eta \wedge *\eta - (\omega-\eta) \wedge *(\omega-\eta)
\\[3pt]
& = \omega \wedge *\eta + \eta \wedge *\omega.
\end{aligned}
 \omega \wedge * \eta \eta \wedge *\omega *\omega \omega \wedge *\omega = \operatorname{vol} \omega \omega, \eta *\omega","['multivariable-calculus', 'differential-geometry', 'exterior-algebra']"
6,Showing that $ f(x) = 0 $ if the triple integral $\frac{f(x)}{(1+x^2+y^2+z^2)} $ converges,Showing that  if the triple integral  converges, f(x) = 0  \frac{f(x)}{(1+x^2+y^2+z^2)} ,"I'm told the following integral converges, and I'm asked to prove that $ f = 0 $ , when it's a continuous function $ f: \mathbb{R} \to \mathbb{R}$ . $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{f(x)}{(1+x^2+y^2+z^2)}\,dx\,dy\,dz.$ My attempt: Suppose that $ f(x_0) = M > 0$ . Then there's a small enough interval such that $ f(x) > M/2, \forall x \in(x_0 - \delta, x_0 + \delta).$ Then at that interval, $\int_{x_0 - \delta}^{x_0 + \delta}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{f(x)}{(1+x^2+y^2+z^2)}\, dx\,dy\,dz > \int_{x_0 - \delta}^{x_0 +\delta}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{M/2}{(1+x^2+y^2+z^2)}\,dx\,dy\,dz$ . If I can show that the double integral that corresponds to $y, z$ diverges, so does our original integral, and that is a contradiction - so $f = 0$ for all $x$ . I want to change to spherical coordinates, $x = r \cos a \cos b, y = r \cos a \cos b$ , $z = r \sin a$ . Then we have a triple integral, and one of those integrals is of the form $\int_{0}^{\infty}\frac{r^2}{1+ r^2} dr $ , and since its limit at $\infty$ is 1, our triple integral in this interval diverges - but we know that it must converge, so we reach a contradiction. Is this attempt okay? What is not correct and needs refinement?","I'm told the following integral converges, and I'm asked to prove that , when it's a continuous function . My attempt: Suppose that . Then there's a small enough interval such that Then at that interval, . If I can show that the double integral that corresponds to diverges, so does our original integral, and that is a contradiction - so for all . I want to change to spherical coordinates, , . Then we have a triple integral, and one of those integrals is of the form , and since its limit at is 1, our triple integral in this interval diverges - but we know that it must converge, so we reach a contradiction. Is this attempt okay? What is not correct and needs refinement?"," f = 0   f: \mathbb{R} \to \mathbb{R} \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{f(x)}{(1+x^2+y^2+z^2)}\,dx\,dy\,dz.  f(x_0) = M > 0  f(x) > M/2, \forall x \in(x_0 - \delta, x_0 + \delta). \int_{x_0 - \delta}^{x_0 + \delta}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{f(x)}{(1+x^2+y^2+z^2)}\, dx\,dy\,dz > \int_{x_0 - \delta}^{x_0 +\delta}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} \frac{M/2}{(1+x^2+y^2+z^2)}\,dx\,dy\,dz y, z f = 0 x x = r \cos a \cos b, y = r \cos a \cos b z = r \sin a \int_{0}^{\infty}\frac{r^2}{1+ r^2} dr  \infty","['calculus', 'integration', 'multivariable-calculus']"
7,Area Vector of a flat region,Area Vector of a flat region,,"I don't understand the answers provided in the following problem (by the professor) Let $\vec{v}=\vec{i}+8\vec{j}-7\vec{k}$ and S be the Rectangular region with the orientation shown below. a) Find a normal vector to the plane in the Upward Direction. In order to do this, I first found from the given points the equation of the plane which was $2y+z-4=0$ . Then using the formula $\vec{n} = \pm (A,B,C)/ \sqrt {A^2+B^2+C^2}$ I found the possible unit normal vectors to be $\vec{n}=\pm (0,\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}})$ . However, according to the given answers, the correct answer is considered to be $\vec{n}= (0,\frac{-2}{\sqrt{5}},\frac{-1}{\sqrt{5}})$ . Why is this the case? Why is $\vec{n}=(0,\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}})$ incorrect? How can the selection of the positive or negative normal vector be explained by the upward direction asked in this case? b) Find the area vector. According to the theory, the area vector is $\vec{A}=\vec{n}A$ where $A$ is the area ofthe region. In this case, we have a rectangle whose width is $2$ and length is $2\sqrt{5}$ , as seen in the figure, which means its Area is $4\sqrt{5}$ . So, considering the answer provided in the previous question, the area vector would be $\vec{A}= (0,-8,-4)$ . However, the accepted answer is given to be $\vec{A}=(0,8,4)$ . What am I missing?","I don't understand the answers provided in the following problem (by the professor) Let and S be the Rectangular region with the orientation shown below. a) Find a normal vector to the plane in the Upward Direction. In order to do this, I first found from the given points the equation of the plane which was . Then using the formula I found the possible unit normal vectors to be . However, according to the given answers, the correct answer is considered to be . Why is this the case? Why is incorrect? How can the selection of the positive or negative normal vector be explained by the upward direction asked in this case? b) Find the area vector. According to the theory, the area vector is where is the area ofthe region. In this case, we have a rectangle whose width is and length is , as seen in the figure, which means its Area is . So, considering the answer provided in the previous question, the area vector would be . However, the accepted answer is given to be . What am I missing?","\vec{v}=\vec{i}+8\vec{j}-7\vec{k} 2y+z-4=0 \vec{n} = \pm (A,B,C)/ \sqrt {A^2+B^2+C^2} \vec{n}=\pm (0,\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}}) \vec{n}= (0,\frac{-2}{\sqrt{5}},\frac{-1}{\sqrt{5}}) \vec{n}=(0,\frac{2}{\sqrt{5}},\frac{1}{\sqrt{5}}) \vec{A}=\vec{n}A A 2 2\sqrt{5} 4\sqrt{5} \vec{A}= (0,-8,-4) \vec{A}=(0,8,4)","['multivariable-calculus', 'vector-analysis']"
8,"In geometric algebra, what is the dot product of a vector and a scalar? what is the wedge product of a vector and a scalar?","In geometric algebra, what is the dot product of a vector and a scalar? what is the wedge product of a vector and a scalar?",,"I am watching a series on geometric calculus by Alan Mcdonald and in the first episodes he states that for any vector u and multivector M: $uM = u \cdot M + u \land M$ This doesn't really take into account what happens with the 0-grade i.e scalar part of M, let's call it s. For the identity to be true either $u \cdot s$ or $u \wedge s$ has to be zero, and from what i read previously in some instances $u \land s = us$ , which implies $u \cdot s = 0$ , which kind of makes intuitive sense i guess?","I am watching a series on geometric calculus by Alan Mcdonald and in the first episodes he states that for any vector u and multivector M: This doesn't really take into account what happens with the 0-grade i.e scalar part of M, let's call it s. For the identity to be true either or has to be zero, and from what i read previously in some instances , which implies , which kind of makes intuitive sense i guess?",uM = u \cdot M + u \land M u \cdot s u \wedge s u \land s = us u \cdot s = 0,"['multivariable-calculus', 'differential-geometry', 'bilinear-form', 'clifford-algebras', 'geometric-algebras']"
9,positively homogeneous function that differentiable at $0^k$,positively homogeneous function that differentiable at,0^k,"Let $f:\Bbb R^k \to \Bbb R$ be continious function and positively homogeneous function with degree $\alpha$ in $\Bbb R^k \setminus {0^k} $ (the definition of  positively homogeneous function in our course is $f(\lambda\cdot x)=\lambda^\alpha \cdot f(x)$ for $\lambda > 0$ and x $\neq 0^k$ ) if $\alpha > 1$ and $f(0^k)=0$ prove that f is differentiable at $0^k$ note : the definition of differentiability at $0^k$ in our course is $lim_{h\to 0^k}$ $f(0^k+h)-f(0^k)-\nabla f(0^k)\cdot h \over |h|$ = $0$ I succeeded in proving that $∇f(0^k) = 0^k$ , which means that with the data $f(0^k) = 0$ What is left to prove is $lim_{h\to 0^k}$ $f(0^k+h) \over |h|$ = $0$ The question is taken from an old exam in Calculus 3","Let be continious function and positively homogeneous function with degree in (the definition of  positively homogeneous function in our course is for and x ) if and prove that f is differentiable at note : the definition of differentiability at in our course is = I succeeded in proving that , which means that with the data What is left to prove is = The question is taken from an old exam in Calculus 3",f:\Bbb R^k \to \Bbb R \alpha \Bbb R^k \setminus {0^k}  f(\lambda\cdot x)=\lambda^\alpha \cdot f(x) \lambda > 0 \neq 0^k \alpha > 1 f(0^k)=0 0^k 0^k lim_{h\to 0^k} f(0^k+h)-f(0^k)-\nabla f(0^k)\cdot h \over |h| 0 ∇f(0^k) = 0^k f(0^k) = 0 lim_{h\to 0^k} f(0^k+h) \over |h| 0,"['real-analysis', 'multivariable-calculus', 'special-functions']"
10,Understanding $\frac{\text{d}u}{\text{d}t}$ vs. $\frac{\text{D}u}{\text{D}t}$,Understanding  vs.,\frac{\text{d}u}{\text{d}t} \frac{\text{D}u}{\text{D}t},"I'm working on an unassessed course problem. The setup might be unnecessary for my question, but I give it in case it helps. An incompressible fluid of constant density $\rho$ has velocity $\boldsymbol{u}(x,t)$ . For fluid occupying a closed volume $V$ bounded by a surface $S$ , with outward normal unit vector $\boldsymbol{n}$ , deduce that $$\frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_S\sigma_{ij}n_j\text{ d}S+\int_V\rho F_i\text{ d}V,$$ where $\sigma_{ij}$ is the stress tensor, and $\boldsymbol{F}$ is the body force per unit mass. Use Gauss' divergence theorem with $f_j=\sigma_{ij}a_i$ , where $\boldsymbol{a}$ is an arbitrary constant vector, to deduce that $$\rho\frac{\text{D}u_i}{\text{D}t}=\frac{\partial\sigma_{ij}}{\partial x_j}+\rho F_i,$$ throughout the fluid. The solution booklet contains the line, $\rho\text{ d}V$ is the mass of a fluid element, and does not change as the element moves around, so $$\frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_V\left(\frac{\text{D}u_i}{\text{D}t}\right)\rho\text{ d}V.$$ I don't follow. I would interpret the verbal statement to mean $$\frac{\text{D}}{\text{D}t}(\rho\text{ d}V)=0,$$ and I could follow the symbolic step $$\frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_V\left(\frac{\text{d}u_i}{\text{d}t}\right)\rho\text{ d}V$$ but I don't get how the verbal statement implies the symbolic step given. Doesn't this require $$(\boldsymbol{u}\cdot\nabla)\boldsymbol{u}=\boldsymbol{0}?$$ and how should we know that's true? A related question I asked a few months ago yielded a link to a more substantial answer than I can take in without giving significant time to background which is outside the scope of my module.","I'm working on an unassessed course problem. The setup might be unnecessary for my question, but I give it in case it helps. An incompressible fluid of constant density has velocity . For fluid occupying a closed volume bounded by a surface , with outward normal unit vector , deduce that where is the stress tensor, and is the body force per unit mass. Use Gauss' divergence theorem with , where is an arbitrary constant vector, to deduce that throughout the fluid. The solution booklet contains the line, is the mass of a fluid element, and does not change as the element moves around, so I don't follow. I would interpret the verbal statement to mean and I could follow the symbolic step but I don't get how the verbal statement implies the symbolic step given. Doesn't this require and how should we know that's true? A related question I asked a few months ago yielded a link to a more substantial answer than I can take in without giving significant time to background which is outside the scope of my module.","\rho \boldsymbol{u}(x,t) V S \boldsymbol{n} \frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_S\sigma_{ij}n_j\text{ d}S+\int_V\rho F_i\text{ d}V, \sigma_{ij} \boldsymbol{F} f_j=\sigma_{ij}a_i \boldsymbol{a} \rho\frac{\text{D}u_i}{\text{D}t}=\frac{\partial\sigma_{ij}}{\partial x_j}+\rho F_i, \rho\text{ d}V \frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_V\left(\frac{\text{D}u_i}{\text{D}t}\right)\rho\text{ d}V. \frac{\text{D}}{\text{D}t}(\rho\text{ d}V)=0, \frac{\text{d}}{\text{d}t}\int_V\rho u_i\text{ d}V=\int_V\left(\frac{\text{d}u_i}{\text{d}t}\right)\rho\text{ d}V (\boldsymbol{u}\cdot\nabla)\boldsymbol{u}=\boldsymbol{0}?","['multivariable-calculus', 'physics', 'fluid-dynamics']"
11,A simple contrained optimization problem,A simple contrained optimization problem,,"Suppose we have to find out the solution of the following optimization problem $$ {\min \atop x\,y =1}{x^2+y^2 \atop } $$ Clearly, in this case, we can apply the classical graphical method of finding where the level curves of the function $f(x,y)=x^2+y^2$ are tangent to the constraint $x\,y=1$ . On the other hand if we want to solve $$ {\max \atop x\,y =1}{x^2+y^2 \atop } $$ it looks like the solutions are $x=+\infty$ and $y=0$ or $x=0$ and $y=+\infty$ , in the sense that moving toward any of these two directions will provide a better solution than that given by the classical graphical method (i.e. $(x,y)=(1,1)$ ). So when the graphical method is not applicable to this kind of problems? In standard textbooks look like it is given for granted (i.e. it is typically said that the curve of the constraint must be tangent and not cross the level curve), so probably I am missing something or my reasoning is flawed and I do not see it.","Suppose we have to find out the solution of the following optimization problem Clearly, in this case, we can apply the classical graphical method of finding where the level curves of the function are tangent to the constraint . On the other hand if we want to solve it looks like the solutions are and or and , in the sense that moving toward any of these two directions will provide a better solution than that given by the classical graphical method (i.e. ). So when the graphical method is not applicable to this kind of problems? In standard textbooks look like it is given for granted (i.e. it is typically said that the curve of the constraint must be tangent and not cross the level curve), so probably I am missing something or my reasoning is flawed and I do not see it.","
{\min \atop x\,y =1}{x^2+y^2 \atop }
 f(x,y)=x^2+y^2 x\,y=1 
{\max \atop x\,y =1}{x^2+y^2 \atop }
 x=+\infty y=0 x=0 y=+\infty (x,y)=(1,1)","['multivariable-calculus', 'optimization', 'constraints']"
12,The definition of the derivative of $f$. Matrix vs. Linear Mapping.,The definition of the derivative of . Matrix vs. Linear Mapping.,f,"The following definitions are two definitions of the derivative of $f$ . Definition. (Definition 1) Let $A\subset\mathbb{R}^m$ , let $f:A\to\mathbb{R}^n$ . Suppose $A$ contains a neighborhood of $a$ . We say that $f$ is differentiable at $a$ if there is an $n$ by $m$ matrix $B$ such that $$\frac{f(a+h)-f(a)-B\cdot h}{|h|}\to 0\,\,\,\,\,\,\text{as}\,\,\,\,\,\,h\to0.$$ The matrix $B$ , which is unique, is called the derivative of $f$ at $a$ ; it is denoted $Df(a)$ . Another Definition. (Definition 2) Let $A\subset\mathbb{R}^m$ , let $f:A\to\mathbb{R}^n$ . Suppose $A$ contains a neighborhood of $a$ . We say that $f$ is differentiable at $a$ if there is a linear mapping $B$ such that $$\frac{f(a+h)-f(a)-B(h)}{|h|}\to 0\,\,\,\,\,\,\text{as}\,\,\,\,\,\,h\to0.$$ The linear mapping $B$ , which is unique, is called the derivative of $f$ at $a$ ; it is denoted $Df(a)$ . Suppose we must solve the following problem. Let $f:\mathbb{R}^2\to\mathbb{R}^2$ be a function such that $f(\begin{pmatrix}x\\y\end{pmatrix})=\begin{pmatrix}e^x\sin y\\x^2 e^y\end{pmatrix}$ . Let $c:=\begin{pmatrix}a\\b\end{pmatrix}$ . Find $Df(c)$ . If we adopt the Definition 1, our answer is like the following: $Df(c)=\begin{pmatrix}e^a\sin b&e^a \cos b\\2a e^b&a^2 e^b\end{pmatrix}.$ If we adopt the Definition 2, our answer is like the following: $Df(c)$ is the lienar mapping such that $\mathbb{R}^2\ni \begin{pmatrix}x\\y\end{pmatrix}\to\begin{pmatrix}e^a\sin b&e^a \cos b\\2a e^b&a^2 e^b\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}\in\mathbb{R}^2.$ I think Definition 1 is better than Definition 2. But some authors adopt Definition 2. I want to know an advantage of Definition 2. peek-a-boo, Thank you very much for your kind answer. Let $f:GL(2,\mathbb{R})\ni A\to A^{-1}\in GL(2,\mathbb{R})$ . I checked $Df_A(\xi)=-A^{-1}\xi A^{-1}$ holds when $n=2$ by Wolfram Engine. (Please see the answer by peek-a-boo.)","The following definitions are two definitions of the derivative of . Definition. (Definition 1) Let , let . Suppose contains a neighborhood of . We say that is differentiable at if there is an by matrix such that The matrix , which is unique, is called the derivative of at ; it is denoted . Another Definition. (Definition 2) Let , let . Suppose contains a neighborhood of . We say that is differentiable at if there is a linear mapping such that The linear mapping , which is unique, is called the derivative of at ; it is denoted . Suppose we must solve the following problem. Let be a function such that . Let . Find . If we adopt the Definition 1, our answer is like the following: If we adopt the Definition 2, our answer is like the following: is the lienar mapping such that I think Definition 1 is better than Definition 2. But some authors adopt Definition 2. I want to know an advantage of Definition 2. peek-a-boo, Thank you very much for your kind answer. Let . I checked holds when by Wolfram Engine. (Please see the answer by peek-a-boo.)","f A\subset\mathbb{R}^m f:A\to\mathbb{R}^n A a f a n m B \frac{f(a+h)-f(a)-B\cdot h}{|h|}\to 0\,\,\,\,\,\,\text{as}\,\,\,\,\,\,h\to0. B f a Df(a) A\subset\mathbb{R}^m f:A\to\mathbb{R}^n A a f a B \frac{f(a+h)-f(a)-B(h)}{|h|}\to 0\,\,\,\,\,\,\text{as}\,\,\,\,\,\,h\to0. B f a Df(a) f:\mathbb{R}^2\to\mathbb{R}^2 f(\begin{pmatrix}x\\y\end{pmatrix})=\begin{pmatrix}e^x\sin y\\x^2 e^y\end{pmatrix} c:=\begin{pmatrix}a\\b\end{pmatrix} Df(c) Df(c)=\begin{pmatrix}e^a\sin b&e^a \cos b\\2a e^b&a^2 e^b\end{pmatrix}. Df(c) \mathbb{R}^2\ni \begin{pmatrix}x\\y\end{pmatrix}\to\begin{pmatrix}e^a\sin b&e^a \cos b\\2a e^b&a^2 e^b\end{pmatrix}\begin{pmatrix}x\\y\end{pmatrix}\in\mathbb{R}^2. f:GL(2,\mathbb{R})\ni A\to A^{-1}\in GL(2,\mathbb{R}) Df_A(\xi)=-A^{-1}\xi A^{-1} n=2","['multivariable-calculus', 'derivatives', 'soft-question']"
13,Proving that set is a hyperspace by definition,Proving that set is a hyperspace by definition,,"While reading a necessary condition for an extremum with constraint from Zorich's book (volume I, page 528) I asked myself the following question: Question. Let $D$ be an open subset in $\mathbb{R}^n$ and $f:D\to \mathbb{R}$ such that $f\in C^1(D;\mathbb{R})$ . Assume that $x_0\in D$ and $\nabla f(x_0)\neq\vec{0}$ . Consider $N:=\{x\in D: f(x)=f(x_0)\}$ . How to prove that $N$ is an $(n-1)$ -dimensional submanifold of $\mathbb{R}^n$ ? Zorich gives the following definition of a $k$ -dimensional submanifold of $\mathbb{R}^n$ . Definition 1. We shall call a set $S\subset \mathbb{R}^n$ a $k$ -dimensional smooth surface in $\mathbb{R}^n$ (or a $k$ -dimensional submanifold of $\mathbb{R}^n$ ) if for every point $x_0\in S$ there exist a neighborhood $U(x_0)$ in $\mathbb{R}^n$ and a diffeomorphism $\varphi:U(x_0)\to I^n$ of this neighborhood onto the standard $n$ -dimensional cube $I^n=\{t\in \mathbb{R}^n: |t^i|<1,\  i=1,\dots,n\}$ of the space $\mathbb{R}^n$ under which the image of the set $S\cap  U(x_0)$ is the portion of the $k$ -dimensional plane in $\mathbb{R}^n$ defined by the relations $t^{k+1}=0,\dots, t^n=0$ lying inside $I^n$ . EDIT 1: I can answer my question if we assume $\nabla f(x)\neq \vec{0}$ for all $x\in D$ . I will show that $N$ is a $(n-1)$ -dimensional submanifold of $\mathbb{R}^n$ . Indeed, let $a=(a_1,\dots,a_n)\in N$ then we know that $\nabla f(a)\neq \vec{0}$ . WLOG, assume that $\frac{\partial f}{\partial x^1}(a)\neq 0$ . Consider the mapping $\Phi:D\to \mathbb{R}^n$ defined as $\vec{x}\mapsto(f(x)-f(x_0),x^2,\dots,x^n)$ , where $x=(x^1,\dots,x^n)$ . It is not difficult to see that $\Phi\in C^1(D;\mathbb{R}^n)$ and $\det J_{\Phi}(a)\neq 0$ , where $J_{\Phi}(a)$ is a Jacobian of our mapping at $a$ . By inverse function theorem there exist $O_1=O_1(a)$ neighborhood of $a$ and $O_2=O_2(\Phi(a))$ neighborhood of $\Phi(a)$ such that $\Phi:O_1\to O_2$ is a diffeomorphism. We notice that $\Phi(a)=(0,a_2,\dots,a_n)$ and we can find a cube around $\Phi(a)$ which is contained in $O_2$ and assume that $$Q=(-r,r)\times \prod_{j=2}^n (a_j-r,a_j+r).$$ Let $P:=\Phi^{-1}(Q)$ , then $P\subset O_1$ and $P$ is a neighborhood of $a$ (because $\Phi$ is a diffeomorphism). Therefore, for $a\in N$ we were able to find its neghborhood $P=P(a)$ and diffeomorphism $\Phi$ such that $\Phi:P(a)\to Q$ . Moreover, it is not difficult to check that $$\Phi(P(a)\cap N)=\{0\}\times \prod_{j=2}^n (a_j-r,a_j+r).$$ By definition 1, it follows that $N$ is a $(n-1)$ -dimensional submanifold. Remark. But in the original question we assumed that $\nabla f(x_0)\neq 0$ . WLOG suppose that $\partial_1 f(x_0)\neq 0$ Since $f\in C^1(D;\mathbb{R})$ , then there exists $O(x_0)\subset D$ , where $O(x_0)$ is a neighborhood of $x_0$ such that $\partial_1f(x)\neq 0$ for all $x\in O(x_0)$ . It implies that $\nabla f(x)\neq \vec{0}$ for all $x\in O(a)$ . Using previous reasoning it implies that $\tilde{N}:=\{x\in O(x_0): f(x)=f(x_0)\}$ is a $(n-1)$ -dimensional submanifold. But it does not imply that $N$ is a $(n-1)$ -dimensional submanifold. I hope now my question is posed correctly and it maked sense what am I asking. Thank you! EDIT 2: I created this post because Zorich proves the following result in his book: Theorem 1. Let $f:D\to \mathbb{R}$ be a function defined on an open set $D\subset \mathbb{R}^n$ and belonging to $C^1(D;\mathbb{R})$ . Let $S$ be a smooth surface in $D$ .  A necessary condition for a point $x_0\in S$ that is noncritical for $f$ to be a  local extremum of $\left.f\right|_S$ is that $$TS_{x_0}\subset TN_{x_0},$$ where $TS_{x_0}$ is the tangent space to the surface $S$ at $x_0$ and $TN_{x_0}$ is the  tangent space to the level surface $N = \{x \in D: f(x) = f(x_0)\}$ of $f$ to which $x_0$ belongs. So my concern was that one cannot define surface $N$ in the way he did. I mean $N$ should be defined as $\{x\in O(x_0): f(x)=f(x_0)\}$ , where $O(x_0)\subset D$ is a neighborhood of $x_0$ , where $\nabla f(x)\neq 0$ . Is that correct?","While reading a necessary condition for an extremum with constraint from Zorich's book (volume I, page 528) I asked myself the following question: Question. Let be an open subset in and such that . Assume that and . Consider . How to prove that is an -dimensional submanifold of ? Zorich gives the following definition of a -dimensional submanifold of . Definition 1. We shall call a set a -dimensional smooth surface in (or a -dimensional submanifold of ) if for every point there exist a neighborhood in and a diffeomorphism of this neighborhood onto the standard -dimensional cube of the space under which the image of the set is the portion of the -dimensional plane in defined by the relations lying inside . EDIT 1: I can answer my question if we assume for all . I will show that is a -dimensional submanifold of . Indeed, let then we know that . WLOG, assume that . Consider the mapping defined as , where . It is not difficult to see that and , where is a Jacobian of our mapping at . By inverse function theorem there exist neighborhood of and neighborhood of such that is a diffeomorphism. We notice that and we can find a cube around which is contained in and assume that Let , then and is a neighborhood of (because is a diffeomorphism). Therefore, for we were able to find its neghborhood and diffeomorphism such that . Moreover, it is not difficult to check that By definition 1, it follows that is a -dimensional submanifold. Remark. But in the original question we assumed that . WLOG suppose that Since , then there exists , where is a neighborhood of such that for all . It implies that for all . Using previous reasoning it implies that is a -dimensional submanifold. But it does not imply that is a -dimensional submanifold. I hope now my question is posed correctly and it maked sense what am I asking. Thank you! EDIT 2: I created this post because Zorich proves the following result in his book: Theorem 1. Let be a function defined on an open set and belonging to . Let be a smooth surface in .  A necessary condition for a point that is noncritical for to be a  local extremum of is that where is the tangent space to the surface at and is the  tangent space to the level surface of to which belongs. So my concern was that one cannot define surface in the way he did. I mean should be defined as , where is a neighborhood of , where . Is that correct?","D \mathbb{R}^n f:D\to \mathbb{R} f\in C^1(D;\mathbb{R}) x_0\in D \nabla f(x_0)\neq\vec{0} N:=\{x\in D: f(x)=f(x_0)\} N (n-1) \mathbb{R}^n k \mathbb{R}^n S\subset \mathbb{R}^n k \mathbb{R}^n k \mathbb{R}^n x_0\in S U(x_0) \mathbb{R}^n \varphi:U(x_0)\to I^n n I^n=\{t\in \mathbb{R}^n: |t^i|<1,\  i=1,\dots,n\} \mathbb{R}^n S\cap
 U(x_0) k \mathbb{R}^n t^{k+1}=0,\dots, t^n=0 I^n \nabla f(x)\neq \vec{0} x\in D N (n-1) \mathbb{R}^n a=(a_1,\dots,a_n)\in N \nabla f(a)\neq \vec{0} \frac{\partial f}{\partial x^1}(a)\neq 0 \Phi:D\to \mathbb{R}^n \vec{x}\mapsto(f(x)-f(x_0),x^2,\dots,x^n) x=(x^1,\dots,x^n) \Phi\in C^1(D;\mathbb{R}^n) \det J_{\Phi}(a)\neq 0 J_{\Phi}(a) a O_1=O_1(a) a O_2=O_2(\Phi(a)) \Phi(a) \Phi:O_1\to O_2 \Phi(a)=(0,a_2,\dots,a_n) \Phi(a) O_2 Q=(-r,r)\times \prod_{j=2}^n (a_j-r,a_j+r). P:=\Phi^{-1}(Q) P\subset O_1 P a \Phi a\in N P=P(a) \Phi \Phi:P(a)\to Q \Phi(P(a)\cap N)=\{0\}\times \prod_{j=2}^n (a_j-r,a_j+r). N (n-1) \nabla f(x_0)\neq 0 \partial_1 f(x_0)\neq 0 f\in C^1(D;\mathbb{R}) O(x_0)\subset D O(x_0) x_0 \partial_1f(x)\neq 0 x\in O(x_0) \nabla f(x)\neq \vec{0} x\in O(a) \tilde{N}:=\{x\in O(x_0): f(x)=f(x_0)\} (n-1) N (n-1) f:D\to \mathbb{R} D\subset \mathbb{R}^n C^1(D;\mathbb{R}) S D x_0\in S f \left.f\right|_S TS_{x_0}\subset TN_{x_0}, TS_{x_0} S x_0 TN_{x_0} N = \{x \in D: f(x) = f(x_0)\} f x_0 N N \{x\in O(x_0): f(x)=f(x_0)\} O(x_0)\subset D x_0 \nabla f(x)\neq 0","['real-analysis', 'multivariable-calculus', 'derivatives', 'differential-geometry']"
14,Convergence/Divergence of a double integral,Convergence/Divergence of a double integral,,"Problem: Decide if the integral $$\iint_D \frac{x-y}{(x+y)^3}dxdy$$ where $D = \{(x,y) \in \mathbb{R^2}: x > 0, y > 0\}$ is convergent This is an old exam question where calculators are not allowed. My attempt was to use standard polar coordinates substitution $x = r\cos\theta, y = r\sin\theta$ and integrate $r$ from $0$ to $\infty$ and $\theta$ from $0$ to $\frac{\pi}{2}$ . The integral I got after the substitution is as follows. $$\int_{0}^{\infty} \frac{1}{r}dr\int_{0}^{\frac{\pi}{2}}\frac{\cos\theta - sin\theta}{(\cos\theta + \sin\theta)^3}d\theta$$ I then thought that the integral over $\theta$ converges to a constant C and the integral for the radius diverges, and therefore the total integral diverges. Out of curiosity i computed the $\theta$ integral and it turns out the value is $0$ . Now I am very unsure of my answer because my old integral in the $xy$ world turns out to be the following in the $r,\theta$ world. $$0 \cdot \int_{0}^{\infty} \frac{1}{r}dr$$ Now to my question. What is the value of this expression? Can I say that  the $r$ integral is divergent and therefore the expression is divergent, or is the expression above 0 and the  integral in the problem convergent?. The solution to the problem divided the first quadrant into two parts with the $y=x$ line, and studied one of these parts with polar coordinates and came to the conclusion that the integral diverges in one of these parts which leads to the main integral diverging. In their case, they didn't get 0 from their $\theta$ integral because they took $\theta$ from $0$ to $\frac{\pi}{4}$ . In my case, I got that specific integral to $0$ . Is my solution fine, and what should the answer look like with my method, or in general?","Problem: Decide if the integral where is convergent This is an old exam question where calculators are not allowed. My attempt was to use standard polar coordinates substitution and integrate from to and from to . The integral I got after the substitution is as follows. I then thought that the integral over converges to a constant C and the integral for the radius diverges, and therefore the total integral diverges. Out of curiosity i computed the integral and it turns out the value is . Now I am very unsure of my answer because my old integral in the world turns out to be the following in the world. Now to my question. What is the value of this expression? Can I say that  the integral is divergent and therefore the expression is divergent, or is the expression above 0 and the  integral in the problem convergent?. The solution to the problem divided the first quadrant into two parts with the line, and studied one of these parts with polar coordinates and came to the conclusion that the integral diverges in one of these parts which leads to the main integral diverging. In their case, they didn't get 0 from their integral because they took from to . In my case, I got that specific integral to . Is my solution fine, and what should the answer look like with my method, or in general?","\iint_D \frac{x-y}{(x+y)^3}dxdy D = \{(x,y) \in \mathbb{R^2}: x > 0, y > 0\} x = r\cos\theta, y = r\sin\theta r 0 \infty \theta 0 \frac{\pi}{2} \int_{0}^{\infty} \frac{1}{r}dr\int_{0}^{\frac{\pi}{2}}\frac{\cos\theta - sin\theta}{(\cos\theta + \sin\theta)^3}d\theta \theta \theta 0 xy r,\theta 0 \cdot \int_{0}^{\infty} \frac{1}{r}dr r y=x \theta \theta 0 \frac{\pi}{4} 0","['integration', 'multivariable-calculus']"
15,Confusion with the mean value theorem for integration of vector-valued functions,Confusion with the mean value theorem for integration of vector-valued functions,,"Let $f: \mathbb{R}^N \to \mathbb{R}^N$ be a smooth function. Then, I wonder if the following limit exists as $n \to \infty$ : \begin{equation} \frac{n^2}{Vol[B(y,1/n)]}\int_{B(y,1/n)}[f(x) \cdot f(x)-f(y)\cdot f(y)]d^Nx \end{equation} Here, $B(y,1/n)$ is the closed ball centere at some fixed $y \in \mathbb{R}^N$ with radius $1/n$ and $Vol[B(y,1/n)$ is its $N-$ dimensional volume. Also, $f(x) \cdot f(x)$ is the Euclidean dot product. What comes to mind is something like second order derivative of $f$ at $y$ , but it seems so complicated...I am aware that the L'Hospital rule does not apply for higher dimensions.. More concretely, what about we take $f(x)=x$ ? Could anyone please help me?","Let be a smooth function. Then, I wonder if the following limit exists as : Here, is the closed ball centere at some fixed with radius and is its dimensional volume. Also, is the Euclidean dot product. What comes to mind is something like second order derivative of at , but it seems so complicated...I am aware that the L'Hospital rule does not apply for higher dimensions.. More concretely, what about we take ? Could anyone please help me?","f: \mathbb{R}^N \to \mathbb{R}^N n \to \infty \begin{equation}
\frac{n^2}{Vol[B(y,1/n)]}\int_{B(y,1/n)}[f(x) \cdot f(x)-f(y)\cdot f(y)]d^Nx
\end{equation} B(y,1/n) y \in \mathbb{R}^N 1/n Vol[B(y,1/n) N- f(x) \cdot f(x) f y f(x)=x","['real-analysis', 'calculus', 'multivariable-calculus']"
16,gradient of functions on a unit ball,gradient of functions on a unit ball,,"Assume $\Omega$ is a unit open ball, and $f \in C^1(\Omega) \cap C(\overline{\Omega})$ , satisfies $|f(x)|\leq 1$ for all $x$ . How to prove there exists $x_0$ s.t. $|\nabla f(x_0)|\leq 1$ ? My idea is to assume all the gradient have norm larger than $1$ . Then we create a curve started at origin, and its tangent has direction the same (or opposite direction) as the gradient. Then since the length of the curve must larger than $1$ , then we could find a dot $x_1$ s.t. $|f(x_1)|>1$ , a contradiction. However it's hard to describe the curve, and I don't know if it's right.","Assume is a unit open ball, and , satisfies for all . How to prove there exists s.t. ? My idea is to assume all the gradient have norm larger than . Then we create a curve started at origin, and its tangent has direction the same (or opposite direction) as the gradient. Then since the length of the curve must larger than , then we could find a dot s.t. , a contradiction. However it's hard to describe the curve, and I don't know if it's right.",\Omega f \in C^1(\Omega) \cap C(\overline{\Omega}) |f(x)|\leq 1 x x_0 |\nabla f(x_0)|\leq 1 1 1 x_1 |f(x_1)|>1,"['multivariable-calculus', 'vector-analysis']"
17,Differentials as linear maps on $\Bbb R^n$,Differentials as linear maps on,\Bbb R^n,"Currently going through some multivariable analysis and I'm trying to understand why the differentials $\left(dx_{1}\right)_{p},\left(dx_{2}\right)_{p},\ldots ,\left(dx_{n}\right)_{p}$ at a point $p$ form a basis for the vector space of linear maps from $\mathbb {R}^{n}$ to $\mathbb{R}$ and I'm reading through the this article on Wikipedia. Is there some simple example which I could use to understand this better? They refer to this trick on defining the coordinates $x_1, \dots, x_n$ in $\Bbb R^n$ such that for $p = (p_1, \dots, p_n)$ one has $x_j : \Bbb R^n \to \Bbb R$ for which $p \mapsto p_j$ . Is there any visual way to understand why the differentials $\left(dx_{1}\right)_{p},\left(dx_{2}\right)_{p},\ldots ,\left(dx_{n}\right)_{p}$ at a point $p$ form a basis for the dual space $(\Bbb R^n)^*$ ? I would be even interested in the cases when $n$ is small say $2$ or $3$ . It would be perhaps easier for me to first understand why the partial derivatives form a basis for the tangent space on $\Bbb R^n$ since we can use those to define the dual?",Currently going through some multivariable analysis and I'm trying to understand why the differentials at a point form a basis for the vector space of linear maps from to and I'm reading through the this article on Wikipedia. Is there some simple example which I could use to understand this better? They refer to this trick on defining the coordinates in such that for one has for which . Is there any visual way to understand why the differentials at a point form a basis for the dual space ? I would be even interested in the cases when is small say or . It would be perhaps easier for me to first understand why the partial derivatives form a basis for the tangent space on since we can use those to define the dual?,"\left(dx_{1}\right)_{p},\left(dx_{2}\right)_{p},\ldots ,\left(dx_{n}\right)_{p} p \mathbb {R}^{n} \mathbb{R} x_1, \dots, x_n \Bbb R^n p = (p_1, \dots, p_n) x_j : \Bbb R^n \to \Bbb R p \mapsto p_j \left(dx_{1}\right)_{p},\left(dx_{2}\right)_{p},\ldots ,\left(dx_{n}\right)_{p} p (\Bbb R^n)^* n 2 3 \Bbb R^n","['real-analysis', 'calculus', 'multivariable-calculus']"
18,Fubini's Theorem solving an integral,Fubini's Theorem solving an integral,,"I've been given the following integral $\displaystyle \underset{0}{\overset{1}{\int}}\underset{0}{\overset{1}{\int}} \frac{y}{\left(1+x^{2}+y^{2}\right)^{\frac{3}{2}}}dxdy$ And the topic of the exercise is using Fubini's theorem to solve it, I was able to reach a strange integral after swapping the integration order but I have a feeling I'm missing something. Does anyone have an approach I should use instead of brute-forcing it? This is the outcome of the swap : $ \displaystyle \underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{1+x^{2}}}-\frac{1}{\sqrt{2+x^{2}}}dx$ EDIT: So after all this help, I was able to finish the integral properly, my main take on this is to remember the quick substitution which really come in handy in cases like this. Im adding my final answer if someone ever was wondering : $\displaystyle \underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{1+x^{2}}}dx-\underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{2+x^{2}}}dx=\ln\left(\sqrt{2}+1\right)-\ln\left(\frac{\sqrt{3}+1}{\sqrt{2}}\right)=\ln\left(\frac{\sqrt{2}+1}{\frac{\sqrt{3}+1}{\sqrt{2}}}\right)=\ln\left(\frac{\sqrt{2}+2}{\sqrt{3}+1}\right)$","I've been given the following integral And the topic of the exercise is using Fubini's theorem to solve it, I was able to reach a strange integral after swapping the integration order but I have a feeling I'm missing something. Does anyone have an approach I should use instead of brute-forcing it? This is the outcome of the swap : EDIT: So after all this help, I was able to finish the integral properly, my main take on this is to remember the quick substitution which really come in handy in cases like this. Im adding my final answer if someone ever was wondering :",\displaystyle \underset{0}{\overset{1}{\int}}\underset{0}{\overset{1}{\int}} \frac{y}{\left(1+x^{2}+y^{2}\right)^{\frac{3}{2}}}dxdy  \displaystyle \underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{1+x^{2}}}-\frac{1}{\sqrt{2+x^{2}}}dx \displaystyle \underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{1+x^{2}}}dx-\underset{0}{\overset{1}{\int}}\frac{1}{\sqrt{2+x^{2}}}dx=\ln\left(\sqrt{2}+1\right)-\ln\left(\frac{\sqrt{3}+1}{\sqrt{2}}\right)=\ln\left(\frac{\sqrt{2}+1}{\frac{\sqrt{3}+1}{\sqrt{2}}}\right)=\ln\left(\frac{\sqrt{2}+2}{\sqrt{3}+1}\right),"['integration', 'multivariable-calculus', 'definite-integrals', 'fubini-tonelli-theorems']"
19,A detail about multivariable calculus concerning $\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta$ whether equal to $0$,A detail about multivariable calculus concerning  whether equal to,\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta 0,"In the book of ""Classical Fourier Analysis"" by Loukas Grafakos, I am confused by the following detail, the author says that $$\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta$$ is $0$ when at least one $\alpha_{j}$ is odd, why? My attemption: suppose that $\alpha_{1}$ is odd then we can split the integral $\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta$ into two parts according the sign of $\theta_{1}$ , we denote $\mathbb{S}^{n-1}_{+}$ the positive $\theta_{1}$ part and $\mathbb{S}^{n-1}_{-}$ the negtive $\theta_{1}$ part, and so we write $$\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta=\int_{\mathbb{S}_{+}^{n-1}}\theta^{\alpha}d\theta+\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta$$ We make the change of variable $\theta_{1}$ to $-\theta_{1}$ in the integral $$\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta$$ can we get $$\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta=-\int_{\mathbb{S}_{+}^{n-1}}\theta^{\alpha}d\theta$$ ? Is sphere measure $d\theta$ invariant under the reflection $\theta_{1}\to-\theta_{1}$ transformation? Notation: $\mathbb{S}^{n-1}$ is the sphere in $n$ dimension Euclidean space and $d\theta$ is surface measure on sphere, $\alpha=(\alpha_{1},\dots,\alpha_{n})\in\mathbb{Z}^{n}_{+}$ , $\theta=(\theta_{1},\dots,\theta_{n})\in\mathbb{S}^{n-1}$ , $\theta^{\alpha}=\theta_{1}^{\alpha_{1}}\theta_{2}^{\alpha_{2}}\cdots\theta_{n}^{\alpha_{n}}$","In the book of ""Classical Fourier Analysis"" by Loukas Grafakos, I am confused by the following detail, the author says that is when at least one is odd, why? My attemption: suppose that is odd then we can split the integral into two parts according the sign of , we denote the positive part and the negtive part, and so we write We make the change of variable to in the integral can we get ? Is sphere measure invariant under the reflection transformation? Notation: is the sphere in dimension Euclidean space and is surface measure on sphere, , ,","\int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta 0 \alpha_{j} \alpha_{1} \int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta \theta_{1} \mathbb{S}^{n-1}_{+} \theta_{1} \mathbb{S}^{n-1}_{-} \theta_{1} \int_{\mathbb{S}^{n-1}}\theta^{\alpha}d\theta=\int_{\mathbb{S}_{+}^{n-1}}\theta^{\alpha}d\theta+\int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta \theta_{1} -\theta_{1} \int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta \int_{\mathbb{S}_{-}^{n-1}}\theta^{\alpha}d\theta=-\int_{\mathbb{S}_{+}^{n-1}}\theta^{\alpha}d\theta d\theta \theta_{1}\to-\theta_{1} \mathbb{S}^{n-1} n d\theta \alpha=(\alpha_{1},\dots,\alpha_{n})\in\mathbb{Z}^{n}_{+} \theta=(\theta_{1},\dots,\theta_{n})\in\mathbb{S}^{n-1} \theta^{\alpha}=\theta_{1}^{\alpha_{1}}\theta_{2}^{\alpha_{2}}\cdots\theta_{n}^{\alpha_{n}}","['calculus', 'multivariable-calculus', 'vector-analysis', 'harmonic-analysis']"
20,Calculating a line integral using Green's theorem in a region with a singularity,Calculating a line integral using Green's theorem in a region with a singularity,,"Problem : Calculate the line integral $$ \int_{A}\frac{y\,dx-(x+1)\,dy}{x^2+y^2+2x+1} $$ where $A$ is the line $|x|+|y|=4$ , travelling clockwise and making one rotation. Answer : $-2\pi$ Solution : The denominator $x^2+y^2+2x+1$ can be rewritten as $(x+1)^2+y^2$ , which shows that there is a singularity at $(-1, 0)$ . Now, if $$ \left\{\begin{aligned} M &= \frac{y}{(x+1)^2+y^2} \\[2pt]  N &= \frac{-(x+1)}{(x+1)^2+y^2}  \end{aligned}\right. $$ then $$ \frac{\partial N}{\partial x} = \frac{\partial M}{\partial y} $$ Thus, Green's theorem yields: $$ \tag{0}\int_{A-B} M\,dx + N\,dy  = \iint_{D} \frac{\partial N}{\partial x}  - \frac{\partial M}{\partial y}\,dx\,dy = 0 $$ $B$ can be parameterized with $$ \left\{\begin{aligned} x &= -1 + \cos t \\ y &= \sin t, \end{aligned}\right.  \qquad 0 \leq t \leq 2\pi $$ which finally yields \begin{align} \int_{A} M\,dx + N\,dy  &= \int_{B} M\,dx + N\,dy \\ &= \int_{0}^{2\pi} -\frac{\sin^2 t}{1} - \frac{\cos^2 t}{1}\,dt \\ &= \int_{0}^{2\pi} - 1 \,dt \\[2pt]  &= -2\pi   \end{align} I don't understand how to interpret the "" $A - B$ "" on the left-hand side of equation $(0)$ . It seems to me that it does not represent a simple closed curve, as shown in the image above. How can Green's theorem then still be used? Also, why is $B$ a unit circle around the singularity? Why not a circle of radius $2$ , or a square? Why is $B$ necessary at all? I can't figure out why $B$ appears as part of the solution.","Problem : Calculate the line integral where is the line , travelling clockwise and making one rotation. Answer : Solution : The denominator can be rewritten as , which shows that there is a singularity at . Now, if then Thus, Green's theorem yields: can be parameterized with which finally yields I don't understand how to interpret the "" "" on the left-hand side of equation . It seems to me that it does not represent a simple closed curve, as shown in the image above. How can Green's theorem then still be used? Also, why is a unit circle around the singularity? Why not a circle of radius , or a square? Why is necessary at all? I can't figure out why appears as part of the solution.","
\int_{A}\frac{y\,dx-(x+1)\,dy}{x^2+y^2+2x+1}
 A |x|+|y|=4 -2\pi x^2+y^2+2x+1 (x+1)^2+y^2 (-1, 0) 
\left\{\begin{aligned}
M &= \frac{y}{(x+1)^2+y^2} \\[2pt] 
N &= \frac{-(x+1)}{(x+1)^2+y^2} 
\end{aligned}\right.
 
\frac{\partial N}{\partial x} = \frac{\partial M}{\partial y}
 
\tag{0}\int_{A-B} M\,dx + N\,dy 
= \iint_{D} \frac{\partial N}{\partial x} 
- \frac{\partial M}{\partial y}\,dx\,dy = 0
 B 
\left\{\begin{aligned}
x &= -1 + \cos t \\
y &= \sin t,
\end{aligned}\right. 
\qquad 0 \leq t \leq 2\pi
 \begin{align}
\int_{A} M\,dx + N\,dy 
&= \int_{B} M\,dx + N\,dy \\
&= \int_{0}^{2\pi} -\frac{\sin^2 t}{1} - \frac{\cos^2 t}{1}\,dt \\
&= \int_{0}^{2\pi} - 1 \,dt \\[2pt] 
&= -2\pi  
\end{align} A - B (0) B 2 B B","['integration', 'multivariable-calculus', 'differential-geometry', 'vector-analysis', 'greens-theorem']"
21,Derivative of a matrix inverse with respect to a parameter,Derivative of a matrix inverse with respect to a parameter,,"Given some real symmetric positive semidefinite matrix $X\in\mathbb{R}^{n\times n}$ and a parameter $\lambda \in\mathbb{R}_+$ , I want to differentiate: $$f(\lambda) = (\lambda \mathrm{Id} + X)^{-1}$$ but I don't know where to start. Clearly $f:\mathbb{R}_+\to \mathbb{R}^{n\times n}$ but not sure how to handle the inverse.","Given some real symmetric positive semidefinite matrix and a parameter , I want to differentiate: but I don't know where to start. Clearly but not sure how to handle the inverse.",X\in\mathbb{R}^{n\times n} \lambda \in\mathbb{R}_+ f(\lambda) = (\lambda \mathrm{Id} + X)^{-1} f:\mathbb{R}_+\to \mathbb{R}^{n\times n},"['linear-algebra', 'multivariable-calculus', 'derivatives']"
22,How are surface integrals converted from dS to dA?,How are surface integrals converted from dS to dA?,,"In class, we learned that: $$ \iint \vec{F} \cdot d \vec{S} = \iint \vec{F} \cdot \hat{n} \|r_{u} \times r_{v}\| dA = \iint \vec{F} \cdot \hat{n} \|r_{u} \times r_{v}\| dudv, $$ where the normal unit vector is equal to $\frac{r_{u} \times r_{v}}{\|r_{u} \times r_{v}\|}$ . Moreover in addition to this, the bounds of the double integral depend on the parametrization $r_{u}$ and $r_{v}$ , or any other vector that is normal to the surface we are integrating over (for example, the three standard unit vectors, or a gradient vector). However, in this problem: I'm really confused by the second part, where they had to take the surface integral of the bottom disk of the top hemisphere. It seems like they bypassed the entire parametrization $dS = \|r_{u} \times r_{v}\|dA$ . How do I make sense of this? Did something happen that was not shown?","In class, we learned that: where the normal unit vector is equal to . Moreover in addition to this, the bounds of the double integral depend on the parametrization and , or any other vector that is normal to the surface we are integrating over (for example, the three standard unit vectors, or a gradient vector). However, in this problem: I'm really confused by the second part, where they had to take the surface integral of the bottom disk of the top hemisphere. It seems like they bypassed the entire parametrization . How do I make sense of this? Did something happen that was not shown?","
\iint \vec{F} \cdot d \vec{S} = \iint \vec{F} \cdot \hat{n} \|r_{u} \times r_{v}\| dA = \iint \vec{F} \cdot \hat{n} \|r_{u} \times r_{v}\| dudv,
 \frac{r_{u} \times r_{v}}{\|r_{u} \times r_{v}\|} r_{u} r_{v} dS = \|r_{u} \times r_{v}\|dA","['multivariable-calculus', 'vector-analysis', 'surface-integrals']"
23,Intuition behind derivative as the best linear approximation,Intuition behind derivative as the best linear approximation,,"I'm trying to understand how derivative for functions $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ works, which is defined as ""the best linear approximation"". Intuitively, given $T,M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ and $a\in\mathbb{R}^n$ , the affine approximation at $a$ , $\ell(x) = f(a) + T(x-a)$ is better than the approximation $F(x) = f(a) + M(x-a)$ if $||f(x) - \ell(x)|| \rightarrow 0$ faster than $||f(x) - F(x)||$ does as $x\rightarrow a$ , i.e, $$\lim_{x\to a} \frac{||f(x) - (f(a) + T(x-a))||}{||f(x) - (f(a) + M(x-a))||} = 0$$ The intuition says that if $T\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ is such that for all $M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ with $M\not=T$ the previous limit is zero (this means that $\ell$ is a better affine approximation than any other affine approximation), then $T$ should be the derivative of $f$ at $a$ , for functions of one variable this is true, i.e. $\textbf{Proposition.}$ Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a function, $a\in\mathbb{R}$ and $L\in\mathcal L(\mathbb{R})$ , the following statements are equivalent: (a) $f$ is differentiable at $a$ with derivative L: $$\lim_{x\to a} \frac{|f(x) - (f(a) + L(x-a))|}{|x-a|} = 0$$ (b) For all $M\in\mathcal L(\mathbb{R})$ with $M\not= L$ , we get: $$\lim_{x\to a} \frac{|f(x) - (f(a) + L(x-a))|}{|f(x) - (f(a) + M(x-a))|} = 0$$ My question is: Is this true for functions in higher dimensions? i.e. $\textbf{Proposition.}$ Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$ be a function, $\hat{a}\in\mathbb{R}^n$ and $L\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ , the following statements are equivalent: (a) $f$ is differentiable at $\hat{a}$ with derivative L: $$\lim_{x\to \hat{a}} \frac{||f(x) - (f(\hat{a}) + L(x-\hat{a}))||}{||x-\hat{a}||} = 0$$ (b) For all $M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m)$ with $M\not= L$ , we get: $$\lim_{x\to \hat{a}} \frac{||f(x) - (f(\hat{a}) + L(x-\hat{a}))||}{||f(x) - (f(\hat{a}) + M(x-\hat{a}))||} = 0$$ Is the above statement true?","I'm trying to understand how derivative for functions works, which is defined as ""the best linear approximation"". Intuitively, given and , the affine approximation at , is better than the approximation if faster than does as , i.e, The intuition says that if is such that for all with the previous limit is zero (this means that is a better affine approximation than any other affine approximation), then should be the derivative of at , for functions of one variable this is true, i.e. Let be a function, and , the following statements are equivalent: (a) is differentiable at with derivative L: (b) For all with , we get: My question is: Is this true for functions in higher dimensions? i.e. Let be a function, and , the following statements are equivalent: (a) is differentiable at with derivative L: (b) For all with , we get: Is the above statement true?","f:\mathbb{R}^n \rightarrow \mathbb{R}^m T,M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m) a\in\mathbb{R}^n a \ell(x) = f(a) + T(x-a) F(x) = f(a) + M(x-a) ||f(x) - \ell(x)|| \rightarrow 0 ||f(x) - F(x)|| x\rightarrow a \lim_{x\to a} \frac{||f(x) - (f(a) + T(x-a))||}{||f(x) - (f(a) + M(x-a))||} = 0 T\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m) M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m) M\not=T \ell T f a \textbf{Proposition.} f:\mathbb{R} \rightarrow \mathbb{R} a\in\mathbb{R} L\in\mathcal L(\mathbb{R}) f a \lim_{x\to a} \frac{|f(x) - (f(a) + L(x-a))|}{|x-a|} = 0 M\in\mathcal L(\mathbb{R}) M\not= L \lim_{x\to a} \frac{|f(x) - (f(a) + L(x-a))|}{|f(x) - (f(a) + M(x-a))|} = 0 \textbf{Proposition.} f:\mathbb{R}^n \rightarrow \mathbb{R}^m \hat{a}\in\mathbb{R}^n L\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m) f \hat{a} \lim_{x\to \hat{a}} \frac{||f(x) - (f(\hat{a}) + L(x-\hat{a}))||}{||x-\hat{a}||} = 0 M\in\mathcal L(\mathbb{R}^n, \mathbb{R}^m) M\not= L \lim_{x\to \hat{a}} \frac{||f(x) - (f(\hat{a}) + L(x-\hat{a}))||}{||f(x) - (f(\hat{a}) + M(x-\hat{a}))||} = 0","['real-analysis', 'multivariable-calculus']"
24,Rate of change of a function,Rate of change of a function,,"Consider the surface $$ (S): z=f(x, y)=\dfrac{1}{x^2+y^2} $$ and the point $P_0\left(1, 1, \dfrac12 \right)$ . Find the rate of change of $f$ at the point $P_0$ in the direction of the vector $\vec{u}=\hat{\imath}+\hat{\jmath}$ . I start solving in a straightforward proof, first we have $$ \|\vec{u}\|=\sqrt{1+1}=\sqrt{2} \implies \vec{v}=\dfrac{1}{\sqrt{2}}\hat{\imath}+\dfrac{1}{\sqrt{2}}\hat{\jmath} $$ and $$ \vec{\nabla f}=\left(\dfrac{-2x}{(x^2+y^2)^2}, \dfrac{-2y}{(x^2+y^2)^2}\right) \implies  \vec{\nabla f}(P_0)=-\dfrac{1}{2}\hat{\imath}-\dfrac{1}{2}\hat{\jmath}. $$ Consequently, $$ D_{\vec{v}}f(P_0)=\vec{\nabla f}(P_0) \cdot \vec{v}=-\dfrac{\sqrt{2}}{2} $$ But my question is, if this answer is correct, why on introduce a point of 3 dimensions? when I find the directional derivative, do I need to introduce a new function $F(x,y,z)=f(x,y)-z$ ? Is my answer correct? Next, how one can write the equation of the plane parallel to (S) at $P_0$ ?","Consider the surface and the point . Find the rate of change of at the point in the direction of the vector . I start solving in a straightforward proof, first we have and Consequently, But my question is, if this answer is correct, why on introduce a point of 3 dimensions? when I find the directional derivative, do I need to introduce a new function ? Is my answer correct? Next, how one can write the equation of the plane parallel to (S) at ?","
(S): z=f(x, y)=\dfrac{1}{x^2+y^2}
 P_0\left(1, 1, \dfrac12 \right) f P_0 \vec{u}=\hat{\imath}+\hat{\jmath} 
\|\vec{u}\|=\sqrt{1+1}=\sqrt{2} \implies \vec{v}=\dfrac{1}{\sqrt{2}}\hat{\imath}+\dfrac{1}{\sqrt{2}}\hat{\jmath}
 
\vec{\nabla f}=\left(\dfrac{-2x}{(x^2+y^2)^2}, \dfrac{-2y}{(x^2+y^2)^2}\right) \implies  \vec{\nabla f}(P_0)=-\dfrac{1}{2}\hat{\imath}-\dfrac{1}{2}\hat{\jmath}.
 
D_{\vec{v}}f(P_0)=\vec{\nabla f}(P_0) \cdot \vec{v}=-\dfrac{\sqrt{2}}{2}
 F(x,y,z)=f(x,y)-z P_0","['multivariable-calculus', 'vector-analysis', 'surfaces']"
25,Length of boundary of level set,Length of boundary of level set,,"Say we have $\phi\in C^1(\overline{\Omega})$ of a bounded domain $\Omega$ in $\mathbb{R}^2$ and $D=\{x:\phi(x)>0\}$ and $\nabla \phi(x) \neq 0$ for $x$ on the curve satisfying $\phi(x)=0$ . Question: What can we say about an upper bound of $L(\partial D)$ ? Do we have: $$\|\phi\|_{C^1}\leq M_1 \quad \Rightarrow \quad L(\partial D) \leq M_2,$$ where $M_1$ is some constant and $M_2$ is only dependent on $M_1$ ? Progress: My intuition (so far) is that $L(\partial D)$ is finite and we need a lower bound $|\nabla \phi|>c$ on $\phi^{-1}(0)$ and then $M_2=M_2(c)$ . Indeed for each $x\in \phi^{-1}(0)$ we can find a neighborhood of $x$ and a local $C^1$ parametrization $\psi$ (inverse function theorem) of that local piece of the surface $\phi^{-1}(0)$ . The length of this piece is finite and relates to $|\psi'|$ which somehow relates to $(\nabla \phi)^{-1}$ . Then by compactness we add finitely many pieces to get the total surface area. How to more precisely relate $|\psi'|$ to $|\nabla\phi|$ ?",Say we have of a bounded domain in and and for on the curve satisfying . Question: What can we say about an upper bound of ? Do we have: where is some constant and is only dependent on ? Progress: My intuition (so far) is that is finite and we need a lower bound on and then . Indeed for each we can find a neighborhood of and a local parametrization (inverse function theorem) of that local piece of the surface . The length of this piece is finite and relates to which somehow relates to . Then by compactness we add finitely many pieces to get the total surface area. How to more precisely relate to ?,"\phi\in C^1(\overline{\Omega}) \Omega \mathbb{R}^2 D=\{x:\phi(x)>0\} \nabla \phi(x) \neq 0 x \phi(x)=0 L(\partial D) \|\phi\|_{C^1}\leq M_1 \quad \Rightarrow \quad L(\partial D) \leq M_2, M_1 M_2 M_1 L(\partial D) |\nabla \phi|>c \phi^{-1}(0) M_2=M_2(c) x\in \phi^{-1}(0) x C^1 \psi \phi^{-1}(0) |\psi'| (\nabla \phi)^{-1} |\psi'| |\nabla\phi|","['multivariable-calculus', 'differential-geometry', 'curves', 'surfaces']"
26,"In multivariable calculus, why do we normalize $\frac{\partial}{\partial \theta}$ in polar coordinates?","In multivariable calculus, why do we normalize  in polar coordinates?",\frac{\partial}{\partial \theta},"I'm TA'ing multivariable this semester, and I just noticed that we always tend to normalize all our basis vectors when using polar coordinates. This is in stark contrast what I'm used to in differential geometry, as we'd prefer that our coordinate basis to transform by the law \begin{align*}  \frac{\partial}{\partial x}&=\frac{\partial r}{\partial x}\frac{\partial}{\partial r}+\frac{\partial \theta}{\partial x}\frac{\partial}{\partial \theta}\\ &=\cos\theta\frac{\partial}{\partial r}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta}, \end{align*} and similarly with $\frac{\partial}{\partial y}$ . This $\frac{1}{r}$ factor makes up for the fact that if we travel in the angular direction, we cover more ground the further we are from the origin. So for example, the gradient in these ""geometric"" polar coordinates would take on the form $$\nabla f |_{(r,\theta)}=(\frac{\partial}{\partial r},\frac{1}{r^2}\frac{\partial}{\partial \theta})$$ which agrees with the usual way of defining gradients locally by $\nabla f=g^{ij}(\partial_if)\partial_j$ . This in opposition to the more common $\frac{1}{r}$ factor which comes using the normalized polar coordinate system. So why are we normalizing these coordinates? If you insist on working in an orthonormal frame, why not call it a polar frame instead of polar coordinates to avoid bad practices in the future? Edit: Let me put in an explicit computation in with the ""geometric"" (which I learned is called holonomic) basis. Consider $$f(x,y)=\frac{x}{x^2+y^2},$$ so that in polar coordinates, $$f(r,\theta)=\frac{\cos\theta}{r}.$$ One sees: \begin{align*}     \nabla f&=\frac{\partial f}{\partial x}\bigg\vert_{(r,\theta)}\frac{\partial}{\partial x}+\frac{\partial f}{\partial y}\bigg\vert_{(r,\theta)}\frac{\partial}{\partial y}\\     &=\frac{\sin^2\theta-\cos^2\theta}{r^2}\left(\cos\theta\frac{\partial}{\partial r}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta}\right)-\frac{2\cos\theta\sin\theta}{r^2}\left(\sin\theta\frac{\partial}{\partial r}+\frac{\cos\theta}{r}\frac{\partial}{\partial \theta}\right)\\     &=\frac{\sin^2\theta\cos\theta-\cos^3\theta-2\cos\theta\sin^2\theta}{r^2}\frac{\partial}{\partial r}+\frac{-\sin^3\theta+\cos^2\theta\sin\theta-2\cos^2\theta\sin\theta}{r^3}\frac{\partial}{\partial \theta}\\     &=-\frac{\cos\theta}{r^2}\frac{\partial}{\partial r}-\frac{\sin\theta}{r^3}\frac{\partial}{\partial \theta}\\     &=\frac{\partial f}{\partial r}\frac{\partial }{\partial r}+\frac{1}{r^2}\frac{\partial f}{\partial \theta}\frac{\partial}{\partial \theta}. \end{align*}","I'm TA'ing multivariable this semester, and I just noticed that we always tend to normalize all our basis vectors when using polar coordinates. This is in stark contrast what I'm used to in differential geometry, as we'd prefer that our coordinate basis to transform by the law and similarly with . This factor makes up for the fact that if we travel in the angular direction, we cover more ground the further we are from the origin. So for example, the gradient in these ""geometric"" polar coordinates would take on the form which agrees with the usual way of defining gradients locally by . This in opposition to the more common factor which comes using the normalized polar coordinate system. So why are we normalizing these coordinates? If you insist on working in an orthonormal frame, why not call it a polar frame instead of polar coordinates to avoid bad practices in the future? Edit: Let me put in an explicit computation in with the ""geometric"" (which I learned is called holonomic) basis. Consider so that in polar coordinates, One sees:","\begin{align*} 
\frac{\partial}{\partial x}&=\frac{\partial r}{\partial x}\frac{\partial}{\partial r}+\frac{\partial \theta}{\partial x}\frac{\partial}{\partial \theta}\\
&=\cos\theta\frac{\partial}{\partial r}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta},
\end{align*} \frac{\partial}{\partial y} \frac{1}{r} \nabla f |_{(r,\theta)}=(\frac{\partial}{\partial r},\frac{1}{r^2}\frac{\partial}{\partial \theta}) \nabla f=g^{ij}(\partial_if)\partial_j \frac{1}{r} f(x,y)=\frac{x}{x^2+y^2}, f(r,\theta)=\frac{\cos\theta}{r}. \begin{align*}
    \nabla f&=\frac{\partial f}{\partial x}\bigg\vert_{(r,\theta)}\frac{\partial}{\partial x}+\frac{\partial f}{\partial y}\bigg\vert_{(r,\theta)}\frac{\partial}{\partial y}\\
    &=\frac{\sin^2\theta-\cos^2\theta}{r^2}\left(\cos\theta\frac{\partial}{\partial r}-\frac{\sin\theta}{r}\frac{\partial}{\partial \theta}\right)-\frac{2\cos\theta\sin\theta}{r^2}\left(\sin\theta\frac{\partial}{\partial r}+\frac{\cos\theta}{r}\frac{\partial}{\partial \theta}\right)\\
    &=\frac{\sin^2\theta\cos\theta-\cos^3\theta-2\cos\theta\sin^2\theta}{r^2}\frac{\partial}{\partial r}+\frac{-\sin^3\theta+\cos^2\theta\sin\theta-2\cos^2\theta\sin\theta}{r^3}\frac{\partial}{\partial \theta}\\
    &=-\frac{\cos\theta}{r^2}\frac{\partial}{\partial r}-\frac{\sin\theta}{r^3}\frac{\partial}{\partial \theta}\\
    &=\frac{\partial f}{\partial r}\frac{\partial }{\partial r}+\frac{1}{r^2}\frac{\partial f}{\partial \theta}\frac{\partial}{\partial \theta}.
\end{align*}","['multivariable-calculus', 'soft-question', 'coordinate-systems', 'education', 'polar-coordinates']"
27,Is curl of a particle's velocity zero?,Is curl of a particle's velocity zero?,,"The question Consider the motion of a particle specified by $\mathbf{x} (t): \mathbb{R} \mapsto \mathbb{R}^3$ , where $\mathbf{x} = (x_1,x_2,x_3)$ in cartesian coordinates. The curl of its velocity $\mathbf{v} = (v_1, v_2, v_3)$ can be calculated as $$ \nabla \times \mathbf{v} = (\frac{\partial v_3}{\partial x_2} -\frac{\partial v_2}{\partial x_3},\frac{\partial v_1}{\partial x_3} -\frac{\partial v_3}{\partial x_1}, \frac{\partial v_2}{\partial x_1} -\frac{\partial v_1}{\partial x_2} ). $$ From the interchangeability of ordinary and partial derivatives , $$ \frac{\partial v_i}{\partial x_j}  = \frac{\partial}{\partial x_j} \frac{\mathrm{d}x_i}{\mathrm{d}t}  = \frac{\mathrm{d}}{\mathrm{d}t} \frac{\partial x_i}{\partial x_j}  = \frac{\mathrm{d}}{\mathrm{d}t} \delta_{ij} =0, $$ which makes every component of the curl zero for any $\mathbf{x}(t)$ . However, this does not make much sense to me, since I can easily imagine a rotating velocity field. Also, such fields seem to be quite common in rotational dynamics and fluid dynamics, such as in this post . Some contexts I am trying to derive the equation of motion for a charged particle with rest charge $m$ and charge $q$ in a given electric potential $\phi(t,\mathbf{x})$ and magnetic vector potential $\mathbf{A}(t,\mathbf{x})$ , whose Lagrangian is given by $$ \mathcal{L} = -mc^2 \sqrt{1-|\mathbf{\dot{x}}|^2/c^2} - q \phi + q \mathbf{\dot{x}} \cdot \mathbf{A}. $$ The above question arises when evaluating the term $\nabla (\mathbf{\dot{x}} \cdot \mathbf{A})$ (the spatial gradient), which appears in ${\partial \mathcal{L}}/{\partial x_i}$ . Using the vector calculus identities, we can evaluate the spatial gradient as $$ \nabla(\mathbf{A} \cdot \mathbf{\mathbf{\dot{x}}})  =\  (\mathbf{A} \cdot \nabla)\mathbf{\mathbf{\dot{x}}} \,+\,  (\mathbf{\mathbf{\dot{x}}} \cdot \nabla)\mathbf{A} \,+\,  \mathbf{A} {\times} (\nabla {\times} \mathbf{\mathbf{\dot{x}}}) \,+\,  \mathbf{\mathbf{\dot{x}}} {\times} (\nabla {\times} \mathbf{A}) \\ = \,  (\mathbf{\mathbf{\dot{x}}} \cdot \nabla)\mathbf{A} +  \mathbf{\mathbf{\dot{x}}} {\times} (\nabla {\times} \mathbf{A}), $$ where the second step uses the identity in the question. The answer to a similar question by Shuhao Cao (non-relativistic, but the terms of interest are identical) explains the second step as ""the usual postulate that $\mathbf{v}$ is not explicitly a function of $\mathbf{x}$ .""","The question Consider the motion of a particle specified by , where in cartesian coordinates. The curl of its velocity can be calculated as From the interchangeability of ordinary and partial derivatives , which makes every component of the curl zero for any . However, this does not make much sense to me, since I can easily imagine a rotating velocity field. Also, such fields seem to be quite common in rotational dynamics and fluid dynamics, such as in this post . Some contexts I am trying to derive the equation of motion for a charged particle with rest charge and charge in a given electric potential and magnetic vector potential , whose Lagrangian is given by The above question arises when evaluating the term (the spatial gradient), which appears in . Using the vector calculus identities, we can evaluate the spatial gradient as where the second step uses the identity in the question. The answer to a similar question by Shuhao Cao (non-relativistic, but the terms of interest are identical) explains the second step as ""the usual postulate that is not explicitly a function of .""","\mathbf{x} (t): \mathbb{R} \mapsto \mathbb{R}^3 \mathbf{x} = (x_1,x_2,x_3) \mathbf{v} = (v_1, v_2, v_3) 
\nabla \times \mathbf{v} = (\frac{\partial v_3}{\partial x_2} -\frac{\partial v_2}{\partial x_3},\frac{\partial v_1}{\partial x_3} -\frac{\partial v_3}{\partial x_1}, \frac{\partial v_2}{\partial x_1} -\frac{\partial v_1}{\partial x_2} ).
 
\frac{\partial v_i}{\partial x_j} 
= \frac{\partial}{\partial x_j} \frac{\mathrm{d}x_i}{\mathrm{d}t} 
= \frac{\mathrm{d}}{\mathrm{d}t} \frac{\partial x_i}{\partial x_j} 
= \frac{\mathrm{d}}{\mathrm{d}t} \delta_{ij}
=0,
 \mathbf{x}(t) m q \phi(t,\mathbf{x}) \mathbf{A}(t,\mathbf{x}) 
\mathcal{L} = -mc^2 \sqrt{1-|\mathbf{\dot{x}}|^2/c^2} - q \phi + q \mathbf{\dot{x}} \cdot \mathbf{A}.
 \nabla (\mathbf{\dot{x}} \cdot \mathbf{A}) {\partial \mathcal{L}}/{\partial x_i} 
\nabla(\mathbf{A} \cdot \mathbf{\mathbf{\dot{x}}})  =\  (\mathbf{A} \cdot \nabla)\mathbf{\mathbf{\dot{x}}} \,+\,  (\mathbf{\mathbf{\dot{x}}} \cdot \nabla)\mathbf{A} \,+\,  \mathbf{A} {\times} (\nabla {\times} \mathbf{\mathbf{\dot{x}}}) \,+\,  \mathbf{\mathbf{\dot{x}}} {\times} (\nabla {\times} \mathbf{A}) \\
= \,  (\mathbf{\mathbf{\dot{x}}} \cdot \nabla)\mathbf{A} +  \mathbf{\mathbf{\dot{x}}} {\times} (\nabla {\times} \mathbf{A}),
 \mathbf{v} \mathbf{x}","['multivariable-calculus', 'vector-analysis', 'physics', 'euler-lagrange-equation', 'electromagnetism']"
28,What is the geometrical difference between $\frac{dr}{dt}$ and $dr$?,What is the geometrical difference between  and ?,\frac{dr}{dt} dr,"I am currently studying multivariate calculus and came across a section in my book which perplexes me. It states that a field $F(x, y) = (P(x, y), Q(x, y))$ can have the parameter $r(t) = (x(t), y(t))$ . I follow thus far. The book then continues with explaining that the formal calculation with differentials is $$ \frac{dr}{dt}=(\frac{dx}{dt},\frac{dy}{dt}) \Rightarrow dr = (dx, dy) $$ The book mentioned it simplified by multiplying with $dt$ on each side. I follow the math but struggle with understanding what this is actually saying geometrically. what does $dr$ and $\frac{dr}{dt}$ mean geometrically? I have never fully grasped the implication of multiplying and moving around the ""delta"" of equations and when it is nonsensical. My guess would be that $dr$ is a vector and $\frac{dr}{dt}$ is the rate of change the curve has at a given point. However, the book does not mention these things so I cannot verify this. For context: The parametrisation above, in the book, is used one section down to explain the formula for curve integrals.","I am currently studying multivariate calculus and came across a section in my book which perplexes me. It states that a field can have the parameter . I follow thus far. The book then continues with explaining that the formal calculation with differentials is The book mentioned it simplified by multiplying with on each side. I follow the math but struggle with understanding what this is actually saying geometrically. what does and mean geometrically? I have never fully grasped the implication of multiplying and moving around the ""delta"" of equations and when it is nonsensical. My guess would be that is a vector and is the rate of change the curve has at a given point. However, the book does not mention these things so I cannot verify this. For context: The parametrisation above, in the book, is used one section down to explain the formula for curve integrals.","F(x, y) = (P(x, y), Q(x, y)) r(t) = (x(t), y(t)) 
\frac{dr}{dt}=(\frac{dx}{dt},\frac{dy}{dt}) \Rightarrow dr = (dx, dy)
 dt dr \frac{dr}{dt} dr \frac{dr}{dt}",['multivariable-calculus']
29,"Find the minimum value of u, where $xyz = k^3$ and $u = (x + a)(y + b)(z + c)$","Find the minimum value of u, where  and",xyz = k^3 u = (x + a)(y + b)(z + c),"Find the minimum value of u, where a) $$x^2 + y^2 = 1$$ and $$u = \frac{(ax^2 + by^2)}{\sqrt{(a^2x^2 + b^2y^2)}}$$ b) $$xyz = k^3$$ and $$u = (x + a)(y + b)(z + c)$$ and $$a > 0; b > 0; c > 0$$ I could do the first one with Lagrange  multiplier but it's really tedious. Can anyone give me any better way to solve the second one?? I don't want to use another tedious calculation with Lagrange multipliers. I've asked this same question few months ago but I couldn't work with the Holder inequality hint and also I failed to solve the equations arising using Lagrange multipliers.","Find the minimum value of u, where a) and b) and and I could do the first one with Lagrange  multiplier but it's really tedious. Can anyone give me any better way to solve the second one?? I don't want to use another tedious calculation with Lagrange multipliers. I've asked this same question few months ago but I couldn't work with the Holder inequality hint and also I failed to solve the equations arising using Lagrange multipliers.",x^2 + y^2 = 1 u = \frac{(ax^2 + by^2)}{\sqrt{(a^2x^2 + b^2y^2)}} xyz = k^3 u = (x + a)(y + b)(z + c) a > 0; b > 0; c > 0,"['real-analysis', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
30,Rotation invariant 1-forms on spheres,Rotation invariant 1-forms on spheres,,"I'm currently learning calculus on manifolds, and I find myself feeling rather uncomfortable with basic calculations to do with forms. I am therefore seeking a second opinion on my solution to the following problem: [5.9.1, Berger and Gostiaux] Let $\xi$ be a one-form on $\mathbb{S}^2$ . Assume that $\xi$ is invariant under rotation, that is, $s^{\star} \xi = \xi$ for any $s \in \text{SO}(3)$ . Prove that $\xi = 0$ . Solution : Let $U = \mathbb{S}^2 - \{N\}$ , where $N = (0,0,1)$ is the north pole, and let $f: U \to \mathbb{R}^2$ be the standard stereographic projection. Then let $\hat{\xi} := (f^{-1})^{\star} \xi$ be the coordinate representation of $\xi$ in $U$ . Next, let $s \in \text{SO}(3)$ be a rotation that fixes the poles, so that it restricts to a map of $U$ into itself. Then $\hat{\xi}$ must remain invariant under the coordinate representation of $s$ , since (restricting everything in the sphere to $U$ ) $$(f \circ s \circ f^{-1})^{\star} \hat{\xi} = (s \circ f^{-1})^{\star} \xi = (f^{-1})^{\star} s^{\star} \xi = \hat{\xi}.$$ Now the coordinate representation of $s$ is simply the top-left $2 \times 2$ block in $s$ , i.e. this is a rotation $r \in \text{SO}(2)$ . Conversely, any such rotation is the coordinate representation of a rotation in $\text{SO}(3)$ (just add an extra column and row). So let $r$ be a half-turn of the plane, and consider the effect of $(r^{\star}\hat{\xi})(0)$ on $v \in T_0 \mathbb{R}^2$ . We have $$(r^{\star}\hat{\xi})(0)(v) = \hat{\xi}(r(0))((T_0 r) v) = \hat{\xi}(0)((T_0 r)v)$$ Now since $r$ is a linear map, $T_0 r = r$ , so this reduces to $-\hat{\xi}(0)(v)$ . But by the $r$ -invariance of $\hat{\xi}$ , this is also equal to $\hat{\xi}(0)(v)$ . This is true for every $v$ , so $\hat{\xi}(0) = 0$ . It follows (since $f$ is a diffeomorphism) that $\xi(S) = 0$ . Finally, let $p \in \mathbb{S}^2$ be arbitrary. Since $\text{SO}(3)$ acts transitively, we can find an $s$ that sends $p \leadsto S$ . Then we have that $\xi(p) = (s^{\star} \xi)(p)$ , and this is zero: $$(s^{\star}\xi)(p) = (T_p s)^{\star} (\xi(s(p)) = 0,$$ and we are done. I am moderately confident that this procedure works, but it feels like there should be cleaner solutions. Could one approach this by looking at $S^{2}$ as an embedded submanifold of $R^2$ ? Or is passing to charts often necessary? Thanks in advance.","I'm currently learning calculus on manifolds, and I find myself feeling rather uncomfortable with basic calculations to do with forms. I am therefore seeking a second opinion on my solution to the following problem: [5.9.1, Berger and Gostiaux] Let be a one-form on . Assume that is invariant under rotation, that is, for any . Prove that . Solution : Let , where is the north pole, and let be the standard stereographic projection. Then let be the coordinate representation of in . Next, let be a rotation that fixes the poles, so that it restricts to a map of into itself. Then must remain invariant under the coordinate representation of , since (restricting everything in the sphere to ) Now the coordinate representation of is simply the top-left block in , i.e. this is a rotation . Conversely, any such rotation is the coordinate representation of a rotation in (just add an extra column and row). So let be a half-turn of the plane, and consider the effect of on . We have Now since is a linear map, , so this reduces to . But by the -invariance of , this is also equal to . This is true for every , so . It follows (since is a diffeomorphism) that . Finally, let be arbitrary. Since acts transitively, we can find an that sends . Then we have that , and this is zero: and we are done. I am moderately confident that this procedure works, but it feels like there should be cleaner solutions. Could one approach this by looking at as an embedded submanifold of ? Or is passing to charts often necessary? Thanks in advance.","\xi \mathbb{S}^2 \xi s^{\star} \xi = \xi s \in \text{SO}(3) \xi = 0 U = \mathbb{S}^2 - \{N\} N = (0,0,1) f: U \to \mathbb{R}^2 \hat{\xi} := (f^{-1})^{\star} \xi \xi U s \in \text{SO}(3) U \hat{\xi} s U (f \circ s \circ f^{-1})^{\star} \hat{\xi} = (s \circ f^{-1})^{\star} \xi = (f^{-1})^{\star} s^{\star} \xi = \hat{\xi}. s 2 \times 2 s r \in \text{SO}(2) \text{SO}(3) r (r^{\star}\hat{\xi})(0) v \in T_0 \mathbb{R}^2 (r^{\star}\hat{\xi})(0)(v) = \hat{\xi}(r(0))((T_0 r) v) = \hat{\xi}(0)((T_0 r)v) r T_0 r = r -\hat{\xi}(0)(v) r \hat{\xi} \hat{\xi}(0)(v) v \hat{\xi}(0) = 0 f \xi(S) = 0 p \in \mathbb{S}^2 \text{SO}(3) s p \leadsto S \xi(p) = (s^{\star} \xi)(p) (s^{\star}\xi)(p) = (T_p s)^{\star} (\xi(s(p)) = 0, S^{2} R^2","['multivariable-calculus', 'differential-geometry', 'solution-verification', 'smooth-manifolds', 'spheres']"
31,"Meaning of the derivative $\frac{d f(x,y)}{dx}$",Meaning of the derivative,"\frac{d f(x,y)}{dx}","Suppose we are given a function $f(x,y) = x^2 + y$ defined over $\mathbb{R}^2$ . Then $$\frac{df}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \cdot \frac{dy}{dx} = 2x + \frac{dy}{dx}$$ and $$\frac{\partial f}{\partial x} = 2x$$ How do I interpret $\frac{df}{dx}$ , both mathematically and intuitively? We know derivatives refer to tangents and in the case of $\frac{\partial f}{\partial x}$ , we fix the $y$ and look at the tangent of the now-single-variable function $g(x) = f(x,y_0)$ . I don't know what it means in the case of $\frac{df}{dx}$ .","Suppose we are given a function defined over . Then and How do I interpret , both mathematically and intuitively? We know derivatives refer to tangents and in the case of , we fix the and look at the tangent of the now-single-variable function . I don't know what it means in the case of .","f(x,y) = x^2 + y \mathbb{R}^2 \frac{df}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \cdot \frac{dy}{dx} = 2x + \frac{dy}{dx} \frac{\partial f}{\partial x} = 2x \frac{df}{dx} \frac{\partial f}{\partial x} y g(x) = f(x,y_0) \frac{df}{dx}","['calculus', 'multivariable-calculus']"
32,Calculate double integral $\iint_D\frac{\sqrt{x^2+y^2 - a^2}x}{(x^2+y^2)^2}dxdy$ over unbounded region $D$,Calculate double integral  over unbounded region,\iint_D\frac{\sqrt{x^2+y^2 - a^2}x}{(x^2+y^2)^2}dxdy D,"Calculate $$I = \iint\limits_D\frac{\sqrt{x^2+y^2 - a^2}x}{(x^2+y^2)^2}\,dx\,dy$$ where the region of integration is: $$D = \{(x,y) \in \mathbb{R}^2 \mid x+y - a\sqrt{2} \geq 0, -x+y +a\sqrt{2} \geq 0, a > 0\}$$ I tried using polar coordinates, but then, I don't know how to proceed because of the region $D$ and of the integrand: $$I = \iint\limits_D\frac{\sqrt{r^2-a^2}\cos\theta}{r^2}\,dr\,d\theta$$","Calculate where the region of integration is: I tried using polar coordinates, but then, I don't know how to proceed because of the region and of the integrand:","I = \iint\limits_D\frac{\sqrt{x^2+y^2 - a^2}x}{(x^2+y^2)^2}\,dx\,dy D = \{(x,y) \in \mathbb{R}^2 \mid x+y - a\sqrt{2} \geq 0, -x+y +a\sqrt{2} \geq 0, a > 0\} D I = \iint\limits_D\frac{\sqrt{r^2-a^2}\cos\theta}{r^2}\,dr\,d\theta","['integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
33,Directional derivative dotted with function,Directional derivative dotted with function,,"I appreciate your help. In this problem, a function is dotted with its own derivative. I'm not sure if this is a case of directional derivative or just scaling. The problem is as follows: Let $\vec{x}: \mathbb{R} \to \mathbb{R}^3$ be a differentiable function and $r: \mathbb{R} \to \mathbb{R}$ be the function $r(t) = \lVert \vec{x}(t) \rVert$ denote the $l_2$ length. Let $t_0$ be a real number and $r(t_0) \neq 0$ , then $r$ is differentiable at $t_0$ and $$ r'(t_0) = \frac{\vec{x}'(t_0) \cdot \vec{x}(t_0)}{r(t_0)} $$ I appreciate your help.","I appreciate your help. In this problem, a function is dotted with its own derivative. I'm not sure if this is a case of directional derivative or just scaling. The problem is as follows: Let be a differentiable function and be the function denote the length. Let be a real number and , then is differentiable at and I appreciate your help.","\vec{x}: \mathbb{R} \to \mathbb{R}^3 r: \mathbb{R} \to \mathbb{R} r(t) = \lVert \vec{x}(t) \rVert l_2 t_0 r(t_0) \neq 0 r t_0 
r'(t_0) = \frac{\vec{x}'(t_0) \cdot \vec{x}(t_0)}{r(t_0)}
","['calculus', 'multivariable-calculus', 'derivatives', 'vectors']"
34,Double integral and change of variable [closed],Double integral and change of variable [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question $$ \iint_D \left(x^2-y^2\right)\ dxdy $$ over $D$ which is bounded by region enclosed by the four curves $y = x, y = x + 1, xy = 1$ and $xy = 2$ in the first quadrant. What will be a suitable change of variable for this question?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question over which is bounded by region enclosed by the four curves and in the first quadrant. What will be a suitable change of variable for this question?","
\iint_D \left(x^2-y^2\right)\ dxdy
 D y = x, y = x + 1, xy = 1 xy = 2","['integration', 'multivariable-calculus', 'multiple-integral', 'change-of-variable']"
35,Volume computed by a double integral,Volume computed by a double integral,,"We are asked to find the volume of the following solid: $$x=0, y=0, z=0, x+y+z=2, y^2=1-z, y>0$$ We should compute $\displaystyle \iint_{T}(x+y-y^2-1)dxdy$ , but I cannot figure out which is the orthogonal $T$ , in which we can integrate. I have tried to transform the above expressions into polar coordinated, but this just made things worse, so I was trying to consider the zyx coordinates system instead. I know that the result is $\frac{49}{60}$ . Any advice on this problem would be appreciated.","We are asked to find the volume of the following solid: We should compute , but I cannot figure out which is the orthogonal , in which we can integrate. I have tried to transform the above expressions into polar coordinated, but this just made things worse, so I was trying to consider the zyx coordinates system instead. I know that the result is . Any advice on this problem would be appreciated.","x=0, y=0, z=0, x+y+z=2, y^2=1-z, y>0 \displaystyle \iint_{T}(x+y-y^2-1)dxdy T \frac{49}{60}","['multivariable-calculus', 'volume']"
36,"For the function $f(x,y)=|xy|^{p}$ , find the value of $p$ for which $f$ is differentiable at $(0,0)$.","For the function  , find the value of  for which  is differentiable at .","f(x,y)=|xy|^{p} p f (0,0)","MY ATTEMPT: \begin{equation} fx(0,0)= \lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h}=0 \end{equation} \begin{equation} fy(0,0)= \lim_{k\to 0}\frac{f(0,0+k)-f(0,0)}{k}=0 \end{equation} By Using the definition of differentiability for the functions of two variables: \begin{equation}  \lim_{(h,k)\to (0,0)}\frac{f(a+h,b+k)-f(a,b)-hf_x(a,b)-kf_y(a,b))}{{\sqrt{h^2+k^2}}} \end{equation} \begin{equation} \lim_{(h,k)\to (0,0)}\frac{|hk|^p}{\sqrt{h^2+k^2}}=\text{0 if and only if p=2n where n=1,2,3..}  \end{equation} so, $p=2n (n=1,2,3\ldots)$ PLEASE CHECK whether I attempted this question correctly or not and if there is any error please give suggestions to resolve it. Thank you.","MY ATTEMPT: By Using the definition of differentiability for the functions of two variables: so, PLEASE CHECK whether I attempted this question correctly or not and if there is any error please give suggestions to resolve it. Thank you.","\begin{equation}
fx(0,0)= \lim_{h\to 0}\frac{f(0+h,0)-f(0,0)}{h}=0
\end{equation} \begin{equation}
fy(0,0)= \lim_{k\to 0}\frac{f(0,0+k)-f(0,0)}{k}=0
\end{equation} \begin{equation}
 \lim_{(h,k)\to (0,0)}\frac{f(a+h,b+k)-f(a,b)-hf_x(a,b)-kf_y(a,b))}{{\sqrt{h^2+k^2}}}
\end{equation} \begin{equation}
\lim_{(h,k)\to (0,0)}\frac{|hk|^p}{\sqrt{h^2+k^2}}=\text{0 if and only if p=2n where n=1,2,3..} 
\end{equation} p=2n (n=1,2,3\ldots)","['real-analysis', 'calculus', 'multivariable-calculus']"
37,Can all covector field be written as a product of a function and a differential of another function?,Can all covector field be written as a product of a function and a differential of another function?,,"Is it available to write every covector field $$ \alpha = \sum_{i=1}^{n}\alpha^i\mathrm dx_i $$ on a manifold into the form $$ \alpha = f\mathrm{d}g, $$ where $f$ and $g$ are smooth functions? Or, can we do this at least locally?","Is it available to write every covector field on a manifold into the form where and are smooth functions? Or, can we do this at least locally?","
\alpha = \sum_{i=1}^{n}\alpha^i\mathrm dx_i
 
\alpha = f\mathrm{d}g,
 f g","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'differential-forms']"
38,Why is $D(f\circ g)=Df\circ Dg$,Why is,D(f\circ g)=Df\circ Dg,"I was reading on Wikipedia about total derivatives of functions and they stated the following about the chain rule for total derivatives: Let $f:\mathbb R^m\to \mathbb R^k$ and $g:\mathbb R^n \to \mathbb R^m$ be two differentiable functions and let $a \in \mathbb R^n$ . Let $D_{g(a)}f$ denote the total derivative of $f$ at $g(a)$ and $D_a g$ denote the total derivative of $g$ at a. Then: $$D_a(f\circ g)=D_{g(a)}f\circ D_a g$$ or, for short: $$D(f\circ g)=Df\circ Dg$$ The thing I'm not understanding is the following: What does $Df\circ Dg$ mean? Those two total derivatives are defined as functions: $Df: \mathbb R^m\to \cal L(\mathbb R^m,\mathbb R^k)$ , and $Dg: \mathbb R^n\to \cal L(\mathbb R^n,\mathbb R^m)$ So how is the composition $D(f\circ g)=Df\circ Dg$ defined? Am I missing something or is this a typo?","I was reading on Wikipedia about total derivatives of functions and they stated the following about the chain rule for total derivatives: Let and be two differentiable functions and let . Let denote the total derivative of at and denote the total derivative of at a. Then: or, for short: The thing I'm not understanding is the following: What does mean? Those two total derivatives are defined as functions: , and So how is the composition defined? Am I missing something or is this a typo?","f:\mathbb R^m\to \mathbb R^k g:\mathbb R^n \to \mathbb R^m a \in \mathbb R^n D_{g(a)}f f g(a) D_a g g D_a(f\circ g)=D_{g(a)}f\circ D_a g D(f\circ g)=Df\circ Dg Df\circ Dg Df: \mathbb R^m\to \cal L(\mathbb R^m,\mathbb R^k) Dg: \mathbb R^n\to \cal L(\mathbb R^n,\mathbb R^m) D(f\circ g)=Df\circ Dg",['multivariable-calculus']
39,What does the notation for a line integral of a vector field actually mean?,What does the notation for a line integral of a vector field actually mean?,,"I have been told that the line integral of a vector field, F ( r ) along a curve $C$ is: $$I =\int_C\textbf{F}\cdot \text{d}\textbf{r}=\int_C(F_x,F_y)\cdot (\text{d}x,\text{d}y),$$ where $\text{d}\textbf{r}=(\text{d}x,\text{d}y)$ is the line element on $C$ . What does the bit at the end of the integral mean? I was always told that it was there to show the variable of integration but clearly it means more than that as here the dot product is being taken between it and a vector. Also, by extension, what is $(\text{d}x,\text{d}y)$ ?","I have been told that the line integral of a vector field, F ( r ) along a curve is: where is the line element on . What does the bit at the end of the integral mean? I was always told that it was there to show the variable of integration but clearly it means more than that as here the dot product is being taken between it and a vector. Also, by extension, what is ?","C I =\int_C\textbf{F}\cdot \text{d}\textbf{r}=\int_C(F_x,F_y)\cdot (\text{d}x,\text{d}y), \text{d}\textbf{r}=(\text{d}x,\text{d}y) C (\text{d}x,\text{d}y)","['calculus', 'integration', 'multivariable-calculus', 'vector-fields', 'line-integrals']"
40,"Is curvature independent of ""domain scaling""?","Is curvature independent of ""domain scaling""?",,"$\def\vv#1{\mathbf{\vec{#1}}} \def\unitvv#1{\mathbf{\hat{#1}}} \def\derivative#1#2#3{\frac{d^{#3}#1}{{d#2^{#3}}}}$ I'm not sure if ""domain scaling"" is the correct word, but I had a problem: Find the curvature of the curve $\vv r(t)= (9 + \cos 6t - \sin 6t)\unitvv i + (9 + \sin 6t + \cos 6t)\unitvv j+ 4 \unitvv k$ To do this, I used the formula that I had already derived for the curvature: $\kappa := \frac{d\vv T}{ds} = \frac{\|{\vv{r'}\times\vv{r''}}\|}{{{\|\vv{r'}}\|}^3}$ It was an absolute mess with all sorts of factors of $6$ flying around the page, and I kept losing track of my work, so I decided to try and rescale by using $ u := \frac{t}{6}$ into: $\vv r(u) = (9+\cos u - \sin u)\unitvv i + (9 + \sin u + \cos u) \unitvv j + 4 \unitvv k$ . I justified that the curvature would be the same by lemma: Lemma 1: $ u(t) = \alpha t ; \alpha \in \mathbb{R \implies}\kappa (\vv r(t)) = \kappa(\vv r(u))$ Proof: $\kappa (\vv r(u)) = \frac{\|{{\derivative{\vv r}{u}{}}\times{\derivative{\vv r}{u}{2}}}\|}{{{\|\derivative{\vv r}{u}{}}\|}^3} =  \frac{\|({\derivative{t}{u}{}\derivative{\vv r}{t}{} )\times((\derivative{t}{u}{})^2(\derivative{\vv r}{t}{2}) + (\derivative{t}{u}{2})(\derivative{\vv r}{t}{}))}\|}{{\|\derivative{t}{u}{}\derivative{\vv r}{t}{}\|}^3}  = \frac{\|({\alpha^{-1}\derivative{\vv r}{t}{} )\times(\alpha^{-2}(\derivative{\vv r}{t}{2}) + 0(\derivative{\vv r}{t}{}))}\|}{{\|\alpha^{-1}\derivative{\vv r}{t}{}\|}^3}  = \frac{\|\alpha^{-3}\|\|{{\derivative{\vv r}{t}{}}\times{\derivative{\vv r}{t}{2}}}\|}{{{\|\alpha^{-3}\|\|\derivative{\vv r}{t}{}}\|}^3}  = \frac{\|{{\derivative{\vv r}{t}{}}\times{\derivative{\vv r}{t}{2}}}\|}{{{\|\derivative{\vv r}{t}{}}\|}^3} = \kappa(\vv r(t))$ It got me the right answer of $\frac{\sqrt2}{2}$ in my example problem, but I can't seem to wrap my head around it conceptually from what my idea of $\kappa$ represents. Perhaps I made an error in my lemma that made this only work because the curvature was a constant? If not, maybe my idea of $\kappa$ is wrong and someone can give be a better intuition of it.","I'm not sure if ""domain scaling"" is the correct word, but I had a problem: Find the curvature of the curve To do this, I used the formula that I had already derived for the curvature: It was an absolute mess with all sorts of factors of flying around the page, and I kept losing track of my work, so I decided to try and rescale by using into: . I justified that the curvature would be the same by lemma: Lemma 1: Proof: It got me the right answer of in my example problem, but I can't seem to wrap my head around it conceptually from what my idea of represents. Perhaps I made an error in my lemma that made this only work because the curvature was a constant? If not, maybe my idea of is wrong and someone can give be a better intuition of it.","\def\vv#1{\mathbf{\vec{#1}}}
\def\unitvv#1{\mathbf{\hat{#1}}}
\def\derivative#1#2#3{\frac{d^{#3}#1}{{d#2^{#3}}}} \vv r(t)= (9 + \cos 6t - \sin 6t)\unitvv i + (9 + \sin 6t + \cos 6t)\unitvv j+ 4 \unitvv k \kappa := \frac{d\vv T}{ds} = \frac{\|{\vv{r'}\times\vv{r''}}\|}{{{\|\vv{r'}}\|}^3} 6  u := \frac{t}{6} \vv r(u) = (9+\cos u - \sin u)\unitvv i + (9 + \sin u + \cos u) \unitvv j + 4 \unitvv k  u(t) = \alpha t ; \alpha \in \mathbb{R \implies}\kappa (\vv r(t)) = \kappa(\vv r(u)) \kappa (\vv r(u)) = \frac{\|{{\derivative{\vv r}{u}{}}\times{\derivative{\vv r}{u}{2}}}\|}{{{\|\derivative{\vv r}{u}{}}\|}^3} = 
\frac{\|({\derivative{t}{u}{}\derivative{\vv r}{t}{} )\times((\derivative{t}{u}{})^2(\derivative{\vv r}{t}{2}) + (\derivative{t}{u}{2})(\derivative{\vv r}{t}{}))}\|}{{\|\derivative{t}{u}{}\derivative{\vv r}{t}{}\|}^3} 
= \frac{\|({\alpha^{-1}\derivative{\vv r}{t}{} )\times(\alpha^{-2}(\derivative{\vv r}{t}{2}) + 0(\derivative{\vv r}{t}{}))}\|}{{\|\alpha^{-1}\derivative{\vv r}{t}{}\|}^3} 
= \frac{\|\alpha^{-3}\|\|{{\derivative{\vv r}{t}{}}\times{\derivative{\vv r}{t}{2}}}\|}{{{\|\alpha^{-3}\|\|\derivative{\vv r}{t}{}}\|}^3} 
= \frac{\|{{\derivative{\vv r}{t}{}}\times{\derivative{\vv r}{t}{2}}}\|}{{{\|\derivative{\vv r}{t}{}}\|}^3} = \kappa(\vv r(t)) \frac{\sqrt2}{2} \kappa \kappa","['multivariable-calculus', 'vector-analysis', 'curves']"
41,Converse of path test for multivariable limits. [duplicate],Converse of path test for multivariable limits. [duplicate],,"This question already has answers here : If the limit along all continuous paths is $0$ for $f(x,y)$, must the limit actually be $0$? (2 answers) Closed 2 years ago . I'm familiar with the fact that, if $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ exists and is equal to some scalar $L$ and $r:\mathbb{R}\to \mathbb{R}^2$ is a function such that $\lim_{t\to 1} r(t) = (x_0,y_0)$ , then the limit $\lim_{t\to 1} f(r(t))$ also exists and is equal to $L$ . It can be easily proven with the $\varepsilon -\delta$ definition of limits. This allows to show the non-existence of $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ by showing the limit respect some path does not exist, or by showing that two paths lead to different limits. My question is: is the converse true? If $\lim_{t\to 1} f(r(t))=L$ for all paths $r:\mathbb{R}\to \mathbb{R}^2$ such that $\lim_{t\to 1} r(t) = (x_0,y_0)$ , can I conclude the existence of the multivariable limit $\lim_{(x,y)\to (x_0,y_0)} f(x,y)$ ? If so, how could I prove it?","This question already has answers here : If the limit along all continuous paths is $0$ for $f(x,y)$, must the limit actually be $0$? (2 answers) Closed 2 years ago . I'm familiar with the fact that, if exists and is equal to some scalar and is a function such that , then the limit also exists and is equal to . It can be easily proven with the definition of limits. This allows to show the non-existence of by showing the limit respect some path does not exist, or by showing that two paths lead to different limits. My question is: is the converse true? If for all paths such that , can I conclude the existence of the multivariable limit ? If so, how could I prove it?","\lim_{(x,y)\to (x_0,y_0)} f(x,y) L r:\mathbb{R}\to \mathbb{R}^2 \lim_{t\to 1} r(t) = (x_0,y_0) \lim_{t\to 1} f(r(t)) L \varepsilon -\delta \lim_{(x,y)\to (x_0,y_0)} f(x,y) \lim_{t\to 1} f(r(t))=L r:\mathbb{R}\to \mathbb{R}^2 \lim_{t\to 1} r(t) = (x_0,y_0) \lim_{(x,y)\to (x_0,y_0)} f(x,y)","['real-analysis', 'calculus', 'multivariable-calculus']"
42,"How can I solve this integral $\int_{-1}^1 \int_0^{π}\sin(x)e^{\cos y} \,dy\,dx$",How can I solve this integral,"\int_{-1}^1 \int_0^{π}\sin(x)e^{\cos y} \,dy\,dx","I have trouble solving this integral $$\int_{-1}^1 \int_0^{π} \sin(x)e^{\cos y} \,dy\,dx$$ Any advice on this would be really helpful.",I have trouble solving this integral Any advice on this would be really helpful.,"\int_{-1}^1 \int_0^{π} \sin(x)e^{\cos y} \,dy\,dx","['integration', 'multivariable-calculus', 'definite-integrals']"
43,"""Find the volume of the ellipsoid $x^2+\frac{y^2}{100}+\frac{z^2}{4} = 1$"" was the question and here's my attempt.","""Find the volume of the ellipsoid "" was the question and here's my attempt.",x^2+\frac{y^2}{100}+\frac{z^2}{4} = 1,"So, I asked this question and got downvotes and someone saying I should present my attempt. Here lies my thought process: I know the volume is $\iiint dxdydz$ on any W $\subset R^3$ and I ""just"" had to find the proper values for the W, namely x, y, z, their range. I said $-1≤x≤1$ , $-10≤y≤10$ and $-2≤z≤2$ . Expressing them on the integral, for every variable we set the next one to 0 and the previous as constant so it's: $-1≤x≤1$ $-10 \sqrt{1-x^2} ≤y≤10 \sqrt{1-x^2}$ $-2 \sqrt{1-x^2-\frac{y^2}{100}} ≤ z ≤ 2 \sqrt{1-x^2-\frac{y^2}{100}}$ and it actually went from $dxdydz$ to $dzdydz$ Any thoughts? Thanks in advance.","So, I asked this question and got downvotes and someone saying I should present my attempt. Here lies my thought process: I know the volume is on any W and I ""just"" had to find the proper values for the W, namely x, y, z, their range. I said , and . Expressing them on the integral, for every variable we set the next one to 0 and the previous as constant so it's: and it actually went from to Any thoughts? Thanks in advance.",\iiint dxdydz \subset R^3 -1≤x≤1 -10≤y≤10 -2≤z≤2 -1≤x≤1 -10 \sqrt{1-x^2} ≤y≤10 \sqrt{1-x^2} -2 \sqrt{1-x^2-\frac{y^2}{100}} ≤ z ≤ 2 \sqrt{1-x^2-\frac{y^2}{100}} dxdydz dzdydz,"['integration', 'multivariable-calculus']"
44,Proving the non-existence of a sequence satisfying a set of inequalities,Proving the non-existence of a sequence satisfying a set of inequalities,,"There exist some type of conditions called sequential optimality conditions that usually require some inequalities to be ensured along with it,  see short introdution . Based on a conjecture on this type of sequences, It would be enlightening to this conjecture to find an analytic function $\boldsymbol{c} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{p}$ and sequences $\{\boldsymbol{\mu}^{k}\} \subset \mathbb{R}^p_{+}$ , which means $\mu^k_i \geq 0$ for all $i\leq p$ and $k\in\mathbb{N}$ , and a convergent sequence $\{\boldsymbol{x}^{k}\} \subset \mathbb{R}^n$ such that, for all $k\in\mathbb{N}$ , it's true that $\boldsymbol{c}(\boldsymbol{x}^{k}) \geq \boldsymbol{0}$ , \begin{gather*} \left( \sum_{i=1}^{p} c_{i}(\boldsymbol{x}^{k}) \mu_i^{k}\right)^{2\theta} \left( \sum_{i=1}^{p}\mu_i^{k}\right)^{2(1-\theta)} \leq M + \left( \sum_{i=1}^{p} (c_{i}(\boldsymbol{x}^{k}))^2 \right) \left( \sum_{i=1}^{p}(\mu_i^{k})^2\right),\\ \left( \sum_{i=1}^{p} c_{i}(\boldsymbol{x}^{k}) \mu_i^{k}\right) \geq \delta \end{gather*} and $$ \lim_{k\rightarrow \infty} \sum_{i=1}^{p} c_{i}(\boldsymbol{x}^{k}) = 0 $$ for some $\delta>0$ and $M>0$ and all $0<\theta<1$ . Is it possible?","There exist some type of conditions called sequential optimality conditions that usually require some inequalities to be ensured along with it,  see short introdution . Based on a conjecture on this type of sequences, It would be enlightening to this conjecture to find an analytic function and sequences , which means for all and , and a convergent sequence such that, for all , it's true that , and for some and and all . Is it possible?","\boldsymbol{c} : \mathbb{R}^{n} \rightarrow \mathbb{R}^{p} \{\boldsymbol{\mu}^{k}\} \subset \mathbb{R}^p_{+} \mu^k_i \geq 0 i\leq p k\in\mathbb{N} \{\boldsymbol{x}^{k}\} \subset \mathbb{R}^n k\in\mathbb{N} \boldsymbol{c}(\boldsymbol{x}^{k}) \geq \boldsymbol{0} \begin{gather*}
\left( \sum_{i=1}^{p} c_{i}(\boldsymbol{x}^{k}) \mu_i^{k}\right)^{2\theta} \left( \sum_{i=1}^{p}\mu_i^{k}\right)^{2(1-\theta)} \leq M + \left( \sum_{i=1}^{p} (c_{i}(\boldsymbol{x}^{k}))^2 \right) \left( \sum_{i=1}^{p}(\mu_i^{k})^2\right),\\
\left( \sum_{i=1}^{p} c_{i}(\boldsymbol{x}^{k}) \mu_i^{k}\right) \geq \delta
\end{gather*} 
\lim_{k\rightarrow \infty} \sum_{i=1}^{p} c_{i}(\boldsymbol{x}^{k}) = 0
 \delta>0 M>0 0<\theta<1","['multivariable-calculus', 'inequality', 'examples-counterexamples', 'nonlinear-optimization']"
45,Local extrema of the implicit function defined by $5x^2+5y^2+5z^2-2xy-2xz-2yz-72=0$,Local extrema of the implicit function defined by,5x^2+5y^2+5z^2-2xy-2xz-2yz-72=0,"A problem asked us to determine the local extrema of an implicit function $z=z(x,y)$ defined by the equation $5x^2+5y^2+5z^2-2xy-2xz-2yz-72=0$ . My instructor went about this as follows: Let $F:\mathbb{R}^3 \to \mathbb{R}, F(x, y, z)=5x^2+5y^2+5z^2-2xy-2xz-2yz-72$ . Then, $F$ defines explicitely the function $z=z(x,y)$ in the neighborhood of a point $(x_0, y_0, z_0) \in \mathbb{R}^3$ if $F(x_0, y_0, z_0)=0$ and $\frac{\partial F}{\partial z}(x_0, y_0, z_0) \ne 0$ (we wish to use the implicit function theorem, I forgot to mention that). After writing the conclusion of the implicit function theorem, he drew the conclusion that we need to solve the system $\begin{cases} F(x,y,z)=0 \\ \frac{\partial F}{\partial z}(x, y, z)\ne 0 \\  \frac{\partial F}{\partial x}(x, y, z)=0 \\ \frac{\partial F}{\partial y}(x, y, z)=0  \end{cases} $ in order to find the critical points of our implicit function. I don't really understand how we get this system. I understand that the first two equations come from the fact that we are searching for those points $(x_0, y_0, z_0)$ where we can apply the implicit function theorem. My instructor said that the last two equations are there  because we want to have $\frac{\partial z}{\partial x}(x, y)=0$ and $\frac{\partial z}{\partial y}(x, y)=0$ , but I don't understand why this is the case (I know the formula for implicit differentiation, but I still don't get it). Furthermore, why should all these $4$ equations hold simultaneously? I don't understand why the point for which we apply the implicit function theorem should necessarily also be a critical point for our function $z(x, y)$ .","A problem asked us to determine the local extrema of an implicit function defined by the equation . My instructor went about this as follows: Let . Then, defines explicitely the function in the neighborhood of a point if and (we wish to use the implicit function theorem, I forgot to mention that). After writing the conclusion of the implicit function theorem, he drew the conclusion that we need to solve the system in order to find the critical points of our implicit function. I don't really understand how we get this system. I understand that the first two equations come from the fact that we are searching for those points where we can apply the implicit function theorem. My instructor said that the last two equations are there  because we want to have and , but I don't understand why this is the case (I know the formula for implicit differentiation, but I still don't get it). Furthermore, why should all these equations hold simultaneously? I don't understand why the point for which we apply the implicit function theorem should necessarily also be a critical point for our function .","z=z(x,y) 5x^2+5y^2+5z^2-2xy-2xz-2yz-72=0 F:\mathbb{R}^3 \to \mathbb{R}, F(x, y, z)=5x^2+5y^2+5z^2-2xy-2xz-2yz-72 F z=z(x,y) (x_0, y_0, z_0) \in \mathbb{R}^3 F(x_0, y_0, z_0)=0 \frac{\partial F}{\partial z}(x_0, y_0, z_0) \ne 0 \begin{cases}
F(x,y,z)=0 \\
\frac{\partial F}{\partial z}(x, y, z)\ne 0 \\ 
\frac{\partial F}{\partial x}(x, y, z)=0 \\
\frac{\partial F}{\partial y}(x, y, z)=0 
\end{cases}
 (x_0, y_0, z_0) \frac{\partial z}{\partial x}(x, y)=0 \frac{\partial z}{\partial y}(x, y)=0 4 z(x, y)","['real-analysis', 'calculus', 'multivariable-calculus', 'proof-explanation', 'maxima-minima']"
46,"Show there exists differentiable $g : (0, \infty) \to \mathbb{R}$ s.t $f(\vec{x}) = g(||\vec{x}||)$ for $f : \mathbb{R}^3 \to \mathbb{R}$",Show there exists differentiable  s.t  for,"g : (0, \infty) \to \mathbb{R} f(\vec{x}) = g(||\vec{x}||) f : \mathbb{R}^3 \to \mathbb{R}","Let $f : \mathbb{R}^3 \setminus \left \{0 \right \} \to \mathbb{R}$ be a differentiable function s.t $\nabla f \neq 0$ and: $y \frac{\partial f}{\partial x} -  x \frac{\partial f}{\partial y} =0 \\ z \frac{\partial f}{\partial y} -  y \frac{\partial f}{\partial z} =0$ Show there is a differentiable $g : (0, \infty) \to \mathbb{R}$ s.t $f(x,y,z) = g(||(x,y,z)||)$ I tried using spherical coordinates to show that the gradient is only determined by $r$ which I think should help, but I had some trouble with the algebra, help appreciated.","Let be a differentiable function s.t and: Show there is a differentiable s.t I tried using spherical coordinates to show that the gradient is only determined by which I think should help, but I had some trouble with the algebra, help appreciated.","f : \mathbb{R}^3 \setminus \left \{0 \right \} \to \mathbb{R} \nabla f \neq 0 y \frac{\partial f}{\partial x} -  x \frac{\partial f}{\partial y} =0 \\
z \frac{\partial f}{\partial y} -  y \frac{\partial f}{\partial z} =0 g : (0, \infty) \to \mathbb{R} f(x,y,z) = g(||(x,y,z)||) r",['multivariable-calculus']
47,Square of the Error Function,Square of the Error Function,,The author defines the probability integral as follows $$\Phi(z)=\frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2}\mathrm{d}t$$ . (S)he instructs the reader to derive the following integral representation of the square of the probability integral by transforming it to polar coordinates $$\Phi(z)^2=1-\frac{4}{\pi}\int_0^1\frac{\exp(-z^2(1+t^2))}{1+t^2}\mathrm{d}t$$ . I am uncertain how transforming the integral from cartesian coordinates to polar coordinates would yield the integral representation. Can someone explain this to me? $$I^2=\frac{4}{\pi}\int_{0}^{z}\int_{0}^{z}e^{-(x^2+y^2)}dydx=-\frac{2}{\pi}\int_{0}^\frac{\pi}{2}\int_{0}^{z}-2re^{-r^2}drd\theta$$ ?,The author defines the probability integral as follows . (S)he instructs the reader to derive the following integral representation of the square of the probability integral by transforming it to polar coordinates . I am uncertain how transforming the integral from cartesian coordinates to polar coordinates would yield the integral representation. Can someone explain this to me? ?,\Phi(z)=\frac{2}{\sqrt{\pi}}\int_0^z e^{-t^2}\mathrm{d}t \Phi(z)^2=1-\frac{4}{\pi}\int_0^1\frac{\exp(-z^2(1+t^2))}{1+t^2}\mathrm{d}t I^2=\frac{4}{\pi}\int_{0}^{z}\int_{0}^{z}e^{-(x^2+y^2)}dydx=-\frac{2}{\pi}\int_{0}^\frac{\pi}{2}\int_{0}^{z}-2re^{-r^2}drd\theta,"['multivariable-calculus', 'special-functions']"
48,"Calculate $I=\int_0^{2\pi}dx \int_0^{\pi} e^{\sin y(\cos x-\sin x)}\sin y\,dy$",Calculate,"I=\int_0^{2\pi}dx \int_0^{\pi} e^{\sin y(\cos x-\sin x)}\sin y\,dy","I'm not quite sure how to calculate this integral: $$I=\int_0^{2\pi}dx \int_0^{\pi}  e^{\sin y(\cos x-\sin x)}\sin y\,dy$$ With Mathematica I see the result is $2\pi \sqrt 2 \sinh \sqrt 2$ . Is there any special technique for calculating this integral? Any help will be appreciated. Thank you for these brilliant solutions using Bessel function or complex analysis. After looking into this integral for a long time I found another ""primary"" solution: Rewrite it as $$I=\int_0^{2\pi}d\theta \int_0^{\pi}  e^{\sin \varphi(\cos \theta-\sin \theta)}\sin \varphi\,d\varphi$$ By polar coordinates it becomes $$I=\int_{u^2+v^2+w^2=1} e^{u-v}\,dS$$ since $dS=\sin\varphi d\theta d\varphi$ . Then choose an orthogonal transform such that $z=\frac{u-v}{\sqrt 2}$ , we get $$I=\int_{x^2+y^2+z^2=1} e^{\sqrt 2 z}\,dS$$ Going back to polar coordinates it becomes $$I=\int_0^{2\pi}d\theta \int_0^{\pi}  e^{\sqrt 2 \cos\varphi}\sin \varphi\,d\varphi$$ And finally we get $$I=2\pi \int_{-1}^1 e^{\sqrt 2 t}\,dt$$","I'm not quite sure how to calculate this integral: With Mathematica I see the result is . Is there any special technique for calculating this integral? Any help will be appreciated. Thank you for these brilliant solutions using Bessel function or complex analysis. After looking into this integral for a long time I found another ""primary"" solution: Rewrite it as By polar coordinates it becomes since . Then choose an orthogonal transform such that , we get Going back to polar coordinates it becomes And finally we get","I=\int_0^{2\pi}dx \int_0^{\pi}  e^{\sin y(\cos x-\sin x)}\sin y\,dy 2\pi \sqrt 2 \sinh \sqrt 2 I=\int_0^{2\pi}d\theta \int_0^{\pi}  e^{\sin \varphi(\cos \theta-\sin \theta)}\sin \varphi\,d\varphi I=\int_{u^2+v^2+w^2=1} e^{u-v}\,dS dS=\sin\varphi d\theta d\varphi z=\frac{u-v}{\sqrt 2} I=\int_{x^2+y^2+z^2=1} e^{\sqrt 2 z}\,dS I=\int_0^{2\pi}d\theta \int_0^{\pi}  e^{\sqrt 2 \cos\varphi}\sin \varphi\,d\varphi I=2\pi \int_{-1}^1 e^{\sqrt 2 t}\,dt","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
49,Why do each of these cylindrical triple integrals evaluate differently?,Why do each of these cylindrical triple integrals evaluate differently?,,"The problem in question is thus: Find the volume cut out of the sphere of a radius $a$ centered at the origin by the polar curve $r = a\cos\theta$ . I attempted to solve the problem using this cylindrical triple integral, taking advantage of $x$ -axis symmetry. $$ 2\int_0^{\frac{\pi}{2}}\int_0^{a\cos\theta}\int_{-\sqrt{a^2 - r^2}}^{\sqrt{a^2 - r^2}}r\,dz\,dr\,d\theta = \frac{2\pi a^3}{3} - \frac{8a^3}{9} $$ However, when this, similar integral is evaluated (which in my mind should yield the same results as the first one), the answer is different as the last term cancels out. $$ \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \int_0^{a\cos\theta}\int_{-\sqrt{a^2 - r^2}}^{\sqrt{a^2 - r^2}} r \, dz\,dr\,d\theta = \frac{2\pi a^3}{3} $$ Which one of these answers is correct and why is the other one wrong? I understand that it has something to do with the theta integral, but I have no idea where my error is in assuming that these two triple integrals should produce identical results. Thanks! This is my first time on the Math Stack Exchange so let me know if there is anything else I can do to improve the nature of my question. THANK YOU ALL!!! Many wonderful explanations below, have a great day everyone!","The problem in question is thus: Find the volume cut out of the sphere of a radius centered at the origin by the polar curve . I attempted to solve the problem using this cylindrical triple integral, taking advantage of -axis symmetry. However, when this, similar integral is evaluated (which in my mind should yield the same results as the first one), the answer is different as the last term cancels out. Which one of these answers is correct and why is the other one wrong? I understand that it has something to do with the theta integral, but I have no idea where my error is in assuming that these two triple integrals should produce identical results. Thanks! This is my first time on the Math Stack Exchange so let me know if there is anything else I can do to improve the nature of my question. THANK YOU ALL!!! Many wonderful explanations below, have a great day everyone!","a r = a\cos\theta x 
2\int_0^{\frac{\pi}{2}}\int_0^{a\cos\theta}\int_{-\sqrt{a^2 - r^2}}^{\sqrt{a^2 - r^2}}r\,dz\,dr\,d\theta = \frac{2\pi a^3}{3} - \frac{8a^3}{9}
 
\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \int_0^{a\cos\theta}\int_{-\sqrt{a^2 - r^2}}^{\sqrt{a^2 - r^2}} r \, dz\,dr\,d\theta = \frac{2\pi a^3}{3}
","['integration', 'multivariable-calculus', 'polar-coordinates', 'spherical-coordinates', 'cylindrical-coordinates']"
50,Geometrical difference between exact and inexact differentials,Geometrical difference between exact and inexact differentials,,"Suppose we have a surface which has an explicit function $ z(x,y)$ then we can write the equation of surface around some point $ (x_o,y_o)$ as: $$ \Delta z = \frac{\partial z}{\partial x}_{y} \Delta x + \frac{ \partial z}{\partial y}_{x} \Delta y$$ The geometrical picture of this is as follows: Consider the $ z-x$ plane , in it we have a cross-section of the surface for a fixed $y$ value of $y_o$ , for this curve we can write the change in height as we move $\Delta x$ as $ (\frac{ \partial z}{\partial x})_y \Delta x$ and similarly we can argue for the idea behind addition second term in the sum by considering the $ z-y$ plane. Now, from my understanding if we have an inexact differential then it is an expression of form: $$ f(x,y) =  A dx + B dy$$ Then this can't really be considered as a differential because we can't find a surface given by an explicit function $z$ for which : $$ (\frac{\partial z}{\partial x})_y = A$$ and, $$ (\frac{ \partial z}{\partial y})_x = B$$ Now, we can figure out if a differential is exact or inexact by considering the mixed partial derivatives: $$ \frac{ \partial^2 z}{ \partial y \partial x} =  \frac{ \partial^2 z}{ \partial x \partial y}$$ If the above equality holds then it is exact and otherwise it isn't. An easy way that I got to think of this is by thinking of the differentials as the one forms of a vector field. Now, the vector field would only have a potential function if the differential is exact and this condition is equivalent to the vector field having zero curl. Now, what I don't understand is how the above idea above idea of curl, vector fields etc relate to the original idea of approximating the surface? What exactly is the nature of a surface given by an inexact differential, I mean I know a surface corresponding to it doesn't exist but what if we just 'welded' together all the approximation planes at different points $x$ and $y$ someway?","Suppose we have a surface which has an explicit function then we can write the equation of surface around some point as: The geometrical picture of this is as follows: Consider the plane , in it we have a cross-section of the surface for a fixed value of , for this curve we can write the change in height as we move as and similarly we can argue for the idea behind addition second term in the sum by considering the plane. Now, from my understanding if we have an inexact differential then it is an expression of form: Then this can't really be considered as a differential because we can't find a surface given by an explicit function for which : and, Now, we can figure out if a differential is exact or inexact by considering the mixed partial derivatives: If the above equality holds then it is exact and otherwise it isn't. An easy way that I got to think of this is by thinking of the differentials as the one forms of a vector field. Now, the vector field would only have a potential function if the differential is exact and this condition is equivalent to the vector field having zero curl. Now, what I don't understand is how the above idea above idea of curl, vector fields etc relate to the original idea of approximating the surface? What exactly is the nature of a surface given by an inexact differential, I mean I know a surface corresponding to it doesn't exist but what if we just 'welded' together all the approximation planes at different points and someway?"," z(x,y)  (x_o,y_o)  \Delta z = \frac{\partial z}{\partial x}_{y} \Delta x + \frac{ \partial z}{\partial y}_{x} \Delta y  z-x y y_o \Delta x  (\frac{ \partial z}{\partial x})_y \Delta x  z-y  f(x,y) =  A dx + B dy z  (\frac{\partial z}{\partial x})_y = A  (\frac{ \partial z}{\partial y})_x = B  \frac{ \partial^2 z}{ \partial y \partial x} =  \frac{ \partial^2 z}{ \partial x \partial y} x y","['multivariable-calculus', 'differential-forms', 'surfaces']"
51,Showing that $\nabla(\frac{1}{f})=-\frac{\nabla f}{f^2}$,Showing that,\nabla(\frac{1}{f})=-\frac{\nabla f}{f^2},"If $f$ and $g$ are real-valued differentiable functions in an open set $E \subset \mathbb{R}^n$ , show that $$\nabla\left(\frac{1}{f}\right)=-\frac{\nabla f}{f^2}$$ So if I have $$ \nabla(\frac1f)  = \left( \frac{\partial}{̛\partial x}\frac{1}{f},           \frac{\partial}{̛\partial y}\frac{1}{f},           \frac{\partial}{̛\partial z}\frac{1}{f} \right) $$ wouldn't this equal $$ \left(-\frac{\partial}{̛\partial x}\frac{1}{f^2},       -\frac{\partial}{̛\partial y}\frac{1}{f^2},       -\frac{\partial}{̛\partial z}\frac{1}{f^2}\right)  = -\frac{\nabla f}{f^2}? $$ I'm taking the factor $-\frac{1}{f^2}$ just out which leaves me with $\nabla f.$ I'm a bit confused about the term $\left(-\frac{\partial}{̛\partial x}\frac{1}{f^2}, -\frac{\partial}{̛\partial y}\frac{1}{f^2}, -\frac{\partial}{̛\partial z}\frac{1}{f^2}\right)$ I'm thinking this the way that after differentiating with respect to $x$ I get $-\frac{\partial}{̛\partial x}\frac{1}{f^2}$ from $\frac{\partial}{̛\partial x}\frac{1}{f}$ and $f^2$ would still contain the other variables in this case up to $z$ , but it could be probably any number one desires.","If and are real-valued differentiable functions in an open set , show that So if I have wouldn't this equal I'm taking the factor just out which leaves me with I'm a bit confused about the term I'm thinking this the way that after differentiating with respect to I get from and would still contain the other variables in this case up to , but it could be probably any number one desires.","f g E \subset \mathbb{R}^n \nabla\left(\frac{1}{f}\right)=-\frac{\nabla f}{f^2} 
\nabla(\frac1f)
 = \left( \frac{\partial}{̛\partial x}\frac{1}{f},
          \frac{\partial}{̛\partial y}\frac{1}{f},
          \frac{\partial}{̛\partial z}\frac{1}{f} \right)
 
\left(-\frac{\partial}{̛\partial x}\frac{1}{f^2},
      -\frac{\partial}{̛\partial y}\frac{1}{f^2},
      -\frac{\partial}{̛\partial z}\frac{1}{f^2}\right)
 = -\frac{\nabla f}{f^2}?
 -\frac{1}{f^2} \nabla f. \left(-\frac{\partial}{̛\partial x}\frac{1}{f^2}, -\frac{\partial}{̛\partial y}\frac{1}{f^2}, -\frac{\partial}{̛\partial z}\frac{1}{f^2}\right) x -\frac{\partial}{̛\partial x}\frac{1}{f^2} \frac{\partial}{̛\partial x}\frac{1}{f} f^2 z",['multivariable-calculus']
52,"$\mid Df(u)\cdot w\mid \leq L \mid w\,\mid$",,"\mid Df(u)\cdot w\mid \leq L \mid w\,\mid","Suppose $f:C\to \mathbb{R}^n$ is a $C^1$ -function, where $C$ is a compact subset of $\mathbb{R}^m$ . I want to show that there exists an $L\in\mathbb{R}_{>0}$ such that $$\mid Df(u)\cdot w\mid\leq L\mid w\,\mid$$ for all $u\in C$ and $w\in \mathbb{R}^m$ , where $D$ denotes the Jacobian of $f$ at $u$ . I have already shown this using the extreme value theorem and the Heine-Borel-theorem, but my proof is too long for my purposes. I am looking for a proof, which is as short as possible. Could that be a well known fact?","Suppose is a -function, where is a compact subset of . I want to show that there exists an such that for all and , where denotes the Jacobian of at . I have already shown this using the extreme value theorem and the Heine-Borel-theorem, but my proof is too long for my purposes. I am looking for a proof, which is as short as possible. Could that be a well known fact?","f:C\to \mathbb{R}^n C^1 C \mathbb{R}^m L\in\mathbb{R}_{>0} \mid Df(u)\cdot w\mid\leq L\mid w\,\mid u\in C w\in \mathbb{R}^m D f u",['multivariable-calculus']
53,"Calculate $\iiiint_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}\,dx\,dy\,du\,dv$",Calculate,"\iiiint_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}\,dx\,dy\,du\,dv","$$\iiiint_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}\,dx\,dy\,du\,dv$$ So we just learned substitution and i thought maybe for this integral doing 2 polar subs for x,y and for u,v but i'm not sure this is the right way for this. Any hints will be welcome","So we just learned substitution and i thought maybe for this integral doing 2 polar subs for x,y and for u,v but i'm not sure this is the right way for this. Any hints will be welcome","\iiiint_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}\,dx\,dy\,du\,dv","['integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
54,Deriving the Jacobian and Hessian of the nonlinear least-squares function,Deriving the Jacobian and Hessian of the nonlinear least-squares function,,"I'm working on a problem that involves deriving the Jacobian and Hessian of the following nonlinear least squares function. I've been thinking of expanding the function with Taylor Series expansion then try to match and find the Jacobian and Hessian, but I'm stuck right now. $f(x) = \sum_1^m [y_i - (a_i^Tx)^2]^2$ Where $x \in \Bbb R^{n}$ , $a_i^T$ is the row vector in $x \in \Bbb R^{n}$ . Multiple sources have the derivations in a general form which doesn't allow me to find the gradients in a nice vector-matrix form. Any help/hint would be incredibly helpful.","I'm working on a problem that involves deriving the Jacobian and Hessian of the following nonlinear least squares function. I've been thinking of expanding the function with Taylor Series expansion then try to match and find the Jacobian and Hessian, but I'm stuck right now. Where , is the row vector in . Multiple sources have the derivations in a general form which doesn't allow me to find the gradients in a nice vector-matrix form. Any help/hint would be incredibly helpful.",f(x) = \sum_1^m [y_i - (a_i^Tx)^2]^2 x \in \Bbb R^{n} a_i^T x \in \Bbb R^{n},"['linear-algebra', 'multivariable-calculus', 'partial-derivative', 'matrix-calculus', 'least-squares']"
55,Minimum value when $abc+ab+4bc+9ca=144$,Minimum value when,abc+ab+4bc+9ca=144,"If $a,b,c$ are non-negative real numbers such that $abc+ab+4bc+9ca=144$ , find the minimum value of $a+b+c$ . I tried with Lagrange multipliers. I got the system: $bc+b+9c=ca+a+4c=ab+4b+9a$ Replacing in the condition, I found four solutions, but only one $(4,0,4)$ is non-negative. So the minimum value is $8$ . My question is, can this be done without Lagrange Multipliers?","If are non-negative real numbers such that , find the minimum value of . I tried with Lagrange multipliers. I got the system: Replacing in the condition, I found four solutions, but only one is non-negative. So the minimum value is . My question is, can this be done without Lagrange Multipliers?","a,b,c abc+ab+4bc+9ca=144 a+b+c bc+b+9c=ca+a+4c=ab+4b+9a (4,0,4) 8","['multivariable-calculus', 'inequality']"
56,What kind of surface is this? Is there a way to plot this?,What kind of surface is this? Is there a way to plot this?,,"I am given the surface: $$S=\{ \vec{x} \in \mathbb R^3: {\|\vec{x} \|}_2^2=4, x^2+y^2 \le 1, z >0 \}$$ and I want to calculate the Mass of $S$ given a density $\rho$ . It sort of looks like the upper half of a sphere. The problem I have is that the first equation ${\| \vec{x} \|}_2^2=4$ means that the radius of this sphere is $R=2$ . However, the condition $x^2+y^2 \le1$ would mean that it is some kind of half sphere with a smaller ""base"". I tried to plot this in Wolfram Alpha but I couldn't get it to work. Is there any way I can parameterize/transform this surface in spherical coordinates?","I am given the surface: and I want to calculate the Mass of given a density . It sort of looks like the upper half of a sphere. The problem I have is that the first equation means that the radius of this sphere is . However, the condition would mean that it is some kind of half sphere with a smaller ""base"". I tried to plot this in Wolfram Alpha but I couldn't get it to work. Is there any way I can parameterize/transform this surface in spherical coordinates?","S=\{ \vec{x} \in \mathbb R^3: {\|\vec{x} \|}_2^2=4, x^2+y^2 \le 1, z >0 \} S \rho {\| \vec{x} \|}_2^2=4 R=2 x^2+y^2 \le1",['multivariable-calculus']
57,Lighter Version of Mean-Value Theorem Involving Directional Derivatives,Lighter Version of Mean-Value Theorem Involving Directional Derivatives,,"If $f$ is real-valued and $f'(\mathbf{c}+t\mathbf{u};\mathbf{u})$ exists for $t\in [0,1]$ , show there exists a $\theta\in (0,1)$ such that $f(\mathbf{c}+\mathbf{u})-f(\mathbf{c})=f'(\mathbf{c}+\theta \mathbf{u};\mathbf{u})$ . At first I thought maybe I should define an auxliary function with respect to $t$ ? I was thinking $g(t)=f\circ \gamma$ , where $\gamma(t)=(1-t)\mathbf{c}+t(\mathbf{c}+\mathbf{u})$ . Then $g(1)-g(0)$ gets me the left result. However, I am not sure this is what is needed. If the directional derivative exists, does this mean $f'(\mathbf{c}+t\mathbf{u};\mathbf{u})=\sum_{k=1}^{n}D_{k}f(\mathbf{c}+t\mathbf{u})u_{k}$ ? I don't think I can assume this because $f$ is not assumed differentiable. A lot of problems are due to the fact that $f$ is not assumed differentiable. How do I apply the Mean-Value Theorem? Any tips are appeciated on what I should begin doing!","If is real-valued and exists for , show there exists a such that . At first I thought maybe I should define an auxliary function with respect to ? I was thinking , where . Then gets me the left result. However, I am not sure this is what is needed. If the directional derivative exists, does this mean ? I don't think I can assume this because is not assumed differentiable. A lot of problems are due to the fact that is not assumed differentiable. How do I apply the Mean-Value Theorem? Any tips are appeciated on what I should begin doing!","f f'(\mathbf{c}+t\mathbf{u};\mathbf{u}) t\in [0,1] \theta\in (0,1) f(\mathbf{c}+\mathbf{u})-f(\mathbf{c})=f'(\mathbf{c}+\theta \mathbf{u};\mathbf{u}) t g(t)=f\circ \gamma \gamma(t)=(1-t)\mathbf{c}+t(\mathbf{c}+\mathbf{u}) g(1)-g(0) f'(\mathbf{c}+t\mathbf{u};\mathbf{u})=\sum_{k=1}^{n}D_{k}f(\mathbf{c}+t\mathbf{u})u_{k} f f",['real-analysis']
58,Calculating the Jacobian of a transformation,Calculating the Jacobian of a transformation,,"I have the functions $y_i = \frac{S_i}{S_n}, \ i=1,...,n-1$ and $y_n=S_n$ , where $S_m = \sum_{k=1}^m x_k$ . Now I found the inverse functions to be \begin{align}  x_1&=y_1y_n, \\  x_2&=y_2y_n-y_1y_n, \\  &\ \, \vdots  \\ x_{n-1}&=y_{n-1}y_n-y_{n-2}y_n, \\ x_n&=y_n(1-y_{n-1}).\end{align} I need to get the Jacobian of these inverse functions but I cannot get the right answer, which my textbook says is supposed to be $y_n^{n-1}$ ?","I have the functions and , where . Now I found the inverse functions to be I need to get the Jacobian of these inverse functions but I cannot get the right answer, which my textbook says is supposed to be ?","y_i = \frac{S_i}{S_n}, \ i=1,...,n-1 y_n=S_n S_m = \sum_{k=1}^m x_k \begin{align} 
x_1&=y_1y_n, \\
 x_2&=y_2y_n-y_1y_n, \\
 &\ \, \vdots 
\\ x_{n-1}&=y_{n-1}y_n-y_{n-2}y_n, \\ x_n&=y_n(1-y_{n-1}).\end{align} y_n^{n-1}",['multivariable-calculus']
59,"Nature of the critical point $(0,0)$ of the function $f(x,y)=x^6-2x^2y-x^4y+2y^2$",Nature of the critical point  of the function,"(0,0) f(x,y)=x^6-2x^2y-x^4y+2y^2","Consider the function $$f(x,y)=x^6-2x^2y-x^4y+2y^2.$$ The point $(0,0)$ is a critical point. Observe, \begin{align*} f_x & = 6x^5-4xy-4x^3y, f_x(0,0)=0\\  f_y & = 2x^2-x^4+4y. f_y(0,0)=0\\ f_{xx} & = 30x^4-4y-12x^2y, f_{xx}(0,0)=0\\ f_{xy} & = 4x-4x^3, f_{xy}(0,0)=0\\ f_{yy} & = 4, f_{yy}=4 \end{align*} So, in order to determine the nature of the above critical point, we need to check the Hessian at $(0,0)$ which is $0$ and hence the test is inconclusive. $$ H(x,y)= \det \begin{pmatrix} f_{xx} & f_{xy}\\ f_{yx} & f_{yy} \end{pmatrix}=\det \begin{pmatrix} 0 & 0 \\ 0 & 4 \end{pmatrix}=0$$ So, I tried to see the function on slices like $y=0$ and $y=x$ but nothing worked. So please suggest me how do I find the nature of the critical point in this case?","Consider the function The point is a critical point. Observe, So, in order to determine the nature of the above critical point, we need to check the Hessian at which is and hence the test is inconclusive. So, I tried to see the function on slices like and but nothing worked. So please suggest me how do I find the nature of the critical point in this case?","f(x,y)=x^6-2x^2y-x^4y+2y^2. (0,0) \begin{align*}
f_x & = 6x^5-4xy-4x^3y, f_x(0,0)=0\\ 
f_y & = 2x^2-x^4+4y. f_y(0,0)=0\\
f_{xx} & = 30x^4-4y-12x^2y, f_{xx}(0,0)=0\\
f_{xy} & = 4x-4x^3, f_{xy}(0,0)=0\\
f_{yy} & = 4, f_{yy}=4
\end{align*} (0,0) 0  H(x,y)= \det \begin{pmatrix} f_{xx} & f_{xy}\\ f_{yx} & f_{yy} \end{pmatrix}=\det \begin{pmatrix} 0 & 0 \\ 0 & 4 \end{pmatrix}=0 y=0 y=x","['calculus', 'multivariable-calculus']"
60,Gradient of $F(\mathbf{x}) = \|\mathbf{Ax}-\mathbf{b}\|_{2}^2$,Gradient of,F(\mathbf{x}) = \|\mathbf{Ax}-\mathbf{b}\|_{2}^2,"So, I have a matrix $A\in\mathbb{R}^{M\times N}$ , $M\geq N$ with rank $N$ and $\mathbf{b}\in\mathbb{R}^M$ . Unfortunately I'm a bit rusty on how to do multivariable calculus, so I would like to know how to calculate $\nabla F$ of $$F(\mathbf{x})=\|\mathbf{Ax}-\mathbf{b}\|_{2}^2$$ I would furthermore also appreciate literature suggestions regarding sources where multivariable calculus is well explained, especially differentiation.","So, I have a matrix , with rank and . Unfortunately I'm a bit rusty on how to do multivariable calculus, so I would like to know how to calculate of I would furthermore also appreciate literature suggestions regarding sources where multivariable calculus is well explained, especially differentiation.",A\in\mathbb{R}^{M\times N} M\geq N N \mathbf{b}\in\mathbb{R}^M \nabla F F(\mathbf{x})=\|\mathbf{Ax}-\mathbf{b}\|_{2}^2,"['real-analysis', 'linear-algebra', 'multivariable-calculus', 'derivatives']"
61,Name of partial derivatives where the order of differentiation can be reversed.,Name of partial derivatives where the order of differentiation can be reversed.,,"Is there a name given to partial differential equations of the form: $$\frac{\partial{F}}{\partial{x}\partial{y}}=\frac{\partial{F}}{\partial{y}\partial{x}}$$ Not asking for any kind of proof, just specifically wondering if there is a name given to PDE's that satisfy this condition.","Is there a name given to partial differential equations of the form: Not asking for any kind of proof, just specifically wondering if there is a name given to PDE's that satisfy this condition.",\frac{\partial{F}}{\partial{x}\partial{y}}=\frac{\partial{F}}{\partial{y}\partial{x}},"['multivariable-calculus', 'partial-differential-equations', 'terminology']"
62,What is the actual meaning of $\frac{\partial}{\partial{x}}$,What is the actual meaning of,\frac{\partial}{\partial{x}},"When I searched total derivative on wikipedia it says: For $L=L(t,x_1(t),x_2(t),x_3(t)...x_n(t))$ the total derivative is given by: $$\dfrac{\rm{d}L}{\rm{d}t}=\dfrac{\partial L}{\partial t}+\sum_i^n\dfrac{\partial L}{\partial x_i}\dfrac{\partial x_i}{\partial t}~.$$ So here $\dfrac{\partial L}{\partial t}$ is derivative of $L$ w.r.t an explicit independent variable $t$ . However, when I look for generalized chain rule , wikipedia says: For $$y=y\left(u_1(x_1,x_2,...,x_i),u_2(x_1,x_2,...,x_i),...,u_m(x_1,x_2,...,x_i)\right)$$ the chain rule is given by: $$\frac{\partial y}{\partial x_i}=\sum_{l=1}^m\dfrac{\partial y}{\partial u_l}\dfrac{\partial u_l}{\partial  x_i}$$ Now the partial derivative operator $\dfrac{\partial}{\partial{x_i}}$ looks more like a total derivative, rather than a derivative w.r.t an explicit independent variable. Could anybody tell me what on earth is $\dfrac{\partial}{\partial{x_i}}$ ? Thanks!","When I searched total derivative on wikipedia it says: For the total derivative is given by: So here is derivative of w.r.t an explicit independent variable . However, when I look for generalized chain rule , wikipedia says: For the chain rule is given by: Now the partial derivative operator looks more like a total derivative, rather than a derivative w.r.t an explicit independent variable. Could anybody tell me what on earth is ? Thanks!","L=L(t,x_1(t),x_2(t),x_3(t)...x_n(t)) \dfrac{\rm{d}L}{\rm{d}t}=\dfrac{\partial L}{\partial t}+\sum_i^n\dfrac{\partial L}{\partial x_i}\dfrac{\partial x_i}{\partial t}~. \dfrac{\partial L}{\partial t} L t y=y\left(u_1(x_1,x_2,...,x_i),u_2(x_1,x_2,...,x_i),...,u_m(x_1,x_2,...,x_i)\right) \frac{\partial y}{\partial x_i}=\sum_{l=1}^m\dfrac{\partial y}{\partial u_l}\dfrac{\partial u_l}{\partial 
x_i} \dfrac{\partial}{\partial{x_i}} \dfrac{\partial}{\partial{x_i}}","['calculus', 'multivariable-calculus']"
63,Problems with solving a 2 variable limit,Problems with solving a 2 variable limit,,"There is this exercise I'm trying to solve but can't seem to get it, it states the following: Study the continuity in the origin (depending of $\alpha \in \mathbb{R}$ ) of $$f_\alpha(x,y) = \frac{x^6y^3}{(x^8+y^6)^\alpha}$$ when $(x,y) \neq (0,0)$ and $$f_\alpha(x,y) = 0$$ when $(x,y) = (0,0)$ I have tried various methods, passing to polar coordinates and so on but can't really seem to get anywhere, I know the only possible candidate for the limit is $0$ and thats the result I got several times, so I'm guessing it is continous at $0$ (maybe I'm wrong) and I would have to prove this using the definition but don't know how to do it. Any hints are appreciated and sorry for my broken English.","There is this exercise I'm trying to solve but can't seem to get it, it states the following: Study the continuity in the origin (depending of ) of when and when I have tried various methods, passing to polar coordinates and so on but can't really seem to get anywhere, I know the only possible candidate for the limit is and thats the result I got several times, so I'm guessing it is continous at (maybe I'm wrong) and I would have to prove this using the definition but don't know how to do it. Any hints are appreciated and sorry for my broken English.","\alpha \in \mathbb{R} f_\alpha(x,y) = \frac{x^6y^3}{(x^8+y^6)^\alpha} (x,y) \neq (0,0) f_\alpha(x,y) = 0 (x,y) = (0,0) 0 0","['real-analysis', 'calculus', 'multivariable-calculus']"
64,Using symmetry to evaluate integral,Using symmetry to evaluate integral,,I have this integral: $$\int\int_D(x^3y^2+\ln(x^2+x+1)\sin(y^3))dA$$ Where $D: x^2+y^2 \le 1$ I was wondering what the best way to solve this using symmetry rather than using calculations? Any hints or tips would be appreciated!,I have this integral: Where I was wondering what the best way to solve this using symmetry rather than using calculations? Any hints or tips would be appreciated!,\int\int_D(x^3y^2+\ln(x^2+x+1)\sin(y^3))dA D: x^2+y^2 \le 1,['calculus']
65,Computer the flux of $\nabla \ln \sqrt{x^2 + y^2 + z^2}$ across an icosahedron centered at the origin,Computer the flux of  across an icosahedron centered at the origin,\nabla \ln \sqrt{x^2 + y^2 + z^2},"Let $S$ be the surface of an icosahedron centered at origin) and let $$f(x,y,z)=\ln \sqrt{x^2+y^2+z^2} .$$ Calculate the flux $$\iint_S (\nabla f \cdot n) d\sigma,$$ where $n$ is the outward unit normal vector on $S$ . [My attempt] I tried to use divergence theorem. $$\iint (\nabla f \cdot n) d\sigma = \iiint (\nabla \cdot \nabla f ) dV ,$$ where $(\nabla \cdot \nabla f ) = \frac{1}{x^2 + y^2+z^2}$ . However, I can't find a parametrization of the icosahedron to find the triple integral. How to find this value?","Let be the surface of an icosahedron centered at origin) and let Calculate the flux where is the outward unit normal vector on . [My attempt] I tried to use divergence theorem. where . However, I can't find a parametrization of the icosahedron to find the triple integral. How to find this value?","S f(x,y,z)=\ln \sqrt{x^2+y^2+z^2} . \iint_S (\nabla f \cdot n) d\sigma, n S \iint (\nabla f \cdot n) d\sigma = \iiint (\nabla \cdot \nabla f ) dV , (\nabla \cdot \nabla f ) = \frac{1}{x^2 + y^2+z^2}","['calculus', 'multivariable-calculus', 'surface-integrals']"
66,Evaluating an integral with divergence theorem,Evaluating an integral with divergence theorem,,Evaluate the integral $\int_S(x^2+y^2)dS$ where S is the unit sphere in $\mathbb{R}^3$ I'm being tripped up when the question asks me to evaluate this integral with the divergence theorem because I keep getting $2\pi/3$ but I should get $8\pi/3$ since I got that for the integral.,Evaluate the integral where S is the unit sphere in I'm being tripped up when the question asks me to evaluate this integral with the divergence theorem because I keep getting but I should get since I got that for the integral.,\int_S(x^2+y^2)dS \mathbb{R}^3 2\pi/3 8\pi/3,"['calculus', 'multivariable-calculus']"
67,Is is incorrect to integrate multiple surfaces when using Stokes Theorem,Is is incorrect to integrate multiple surfaces when using Stokes Theorem,,"So far, I have understood that when using Stokes Theorem to find the flux of the curl on a solid that has a boundary, one can use any of the surfaces of that solid. For example in my book for this problem (Transcendental Functions, Smith and Minton 14.8 #13) $C$ is the boundary of the portion of the paraboloid $y = 4 − x^2 − z^2$ with $y > 0$ , $\mathbf n$ to the right, $F = \langle x^2z, 3 \cos y, 4z^3 \rangle$ . the solution is to just integrate over the disc $x^2+z^2 \le 4$ . However, elsewhere it says to integrate multiple surfaces! For example, here (Transcendental Functions, Smith and Minton 14.8 #23) $ S$ is the boundary of the solid bounded by the hyperboloid $x^2 + y^2 − z^2 = 4$ , $z = 0$ and $z = 2$ with $z < 2$ , $\mathbf n$ downward   at bottom, $F = \langle 2^y − x \cos x, y^2 + 1, e^{−z^2} \rangle$ it says to integrate two surfaces, the portion of the hyperboloid and the circle with radius 2. Is there something different about the second problem causing them to integrate over two surfaces, or is there a mistake?","So far, I have understood that when using Stokes Theorem to find the flux of the curl on a solid that has a boundary, one can use any of the surfaces of that solid. For example in my book for this problem (Transcendental Functions, Smith and Minton 14.8 #13) is the boundary of the portion of the paraboloid with , to the right, . the solution is to just integrate over the disc . However, elsewhere it says to integrate multiple surfaces! For example, here (Transcendental Functions, Smith and Minton 14.8 #23) is the boundary of the solid bounded by the hyperboloid , and with , downward   at bottom, it says to integrate two surfaces, the portion of the hyperboloid and the circle with radius 2. Is there something different about the second problem causing them to integrate over two surfaces, or is there a mistake?","C y = 4 − x^2 − z^2 y > 0 \mathbf n F = \langle x^2z, 3 \cos y, 4z^3 \rangle x^2+z^2 \le 4  S x^2 + y^2 − z^2 = 4 z = 0 z = 2 z < 2 \mathbf n F = \langle 2^y − x \cos x, y^2 + 1, e^{−z^2} \rangle","['multivariable-calculus', 'vector-fields', 'surface-integrals', 'stokes-theorem']"
68,Assumptions for a variation of Clairaut's theorem,Assumptions for a variation of Clairaut's theorem,,"This question is similar to this one . I am unsure about the assumptions made in Theorem 6.20 of Apostol's Mathematical Analysis , first edition: Let $S \subseteq \mathbb{R}^2$ be open and $f\colon S \to \mathbb{R}$ . If $D_1 f$ , $D_2 f$ and $D_{2,1} f$ are continuous in a neighbourhood of the point $(x_0, y_0) \in S$ , then $D_{1,2} f(x_0, y_0)$ exists and is equal to $D_{2,1} f(x_0, y_0)$ . The theorem and proof is basically the same as the one featured in this blog post , also mentioned in the above question. By definition of partial derivatives, we need to prove that the limit $$ \lim_{h \to 0} \frac{ D_2 f(x_0 + h, y_0) - D_2 f(x_0, y_0) }{h}$$ exists and is equal to $D_{2,1} f(x_0, y_0)$ . For fixed $k$ , introducing the function $$ g_k(t) = f(x_0 + t, y_0 + k) - f(x_0 + t, y_0) $$ lets us write the numerator above as $$ D_2 f(x_0 + h, y_0) - D_2 f(x_0, y_0) = \lim_{k \to 0} \frac{g_k(h) - g_k(0)}{k}.$$ Since $D_1 f$ exists (we do not require continuity of $D_1 f$ ) in a neighbourhood around $x_0, y_0$ , $g_k$ is differentiable (with the obvious derivative) in a suitably small open interval around $0$ . If we require $h$ to be inside this interval, the mean value theorem yields a $\overline h$ between $0$ and $h$ such that $$ \frac{g_k(h) - g_k(0)}{k} = h \frac{g_k'(\overline h)}{k} = h \frac{D_1 f(x_0 + \overline h, y_0 + k) - D_1 f(x_0 + \overline h, y_0)}{k}.$$ For convenience, define the function (Apostol skips the details of the following steps) $$ \phi(s) = D_1 f(x_0 + \overline h, y_0 + s) - D_1 f(x_0 + \overline h, y_0),$$ analogous to the above function $g_k$ . By the existence (again, not continuity) of $D_{2,1}$ in a neighbourhood around $(x_0, y_0)$ , $\phi$ is differentiable in an open interval around $y_0$ . For fixed $k$ , the mean value theorem then gives us a $\overline y$ between $y_0$ and $y_0 + k$ such that $$ \frac{D_1 f(x_0 + \overline h, y_0 + k) - D_1 f(x_0 + \overline h, y_0)}{k} = D_{2,1} f(x_0 + \overline h, \overline y). $$ In total we get $$ \frac{ D_2 f(x_0 + h, y_0) - D_2 f(x_0, y_0) }{h} = \lim_{k \to 0} D_{2,1}f(x_0 + \overline h, \overline y),$$ so the original limit is equal to $$\lim_{h \to 0} \lim_{k \to 0} D_{2,1}f(x_0 + \overline h, \overline y).$$ So far we have not, as far as I can see, used continuity at all. We cannot evaluate the limit directly, so introduce the function $$ F(h) = \lim_{k \to 0} D_{2,1}f(x_0 + \overline h, \overline y), $$ i.e. just the inner limit as a function of $h$ . Now by continuity of $D_{2,1}f$ at $(x_0, y_0)$ (that is, not in a neighbourhood around the point, only at the point itself), given $\epsilon > 0$ there is some $\delta > 0$ such that for $(x,y) \in N((x_0, y_0); \delta)$ we have $$ \lVert D_{2,1} f(x,y) - D_{2,1}f(x_0, y_0) \rVert < \frac{\epsilon}{2}.$$ Choosing $h$ and $k$ such that $\lvert h \rvert, \lvert k \rvert < \delta/2$ , the point $(x_0 + \overline h, \overline y)$ lies in this $\delta$ -neighbourhood, so that $$ 0 \leq \lVert D_{2,1} f(x_0 + \overline h, \overline y) - D_{2,1}f(x_0, y_0) \rVert < \frac{\epsilon}{2}.$$ Now taking the inner limit $k \to 0$ we get, by continuity of the norm, $$ 0 \leq \lVert F(h) - D_{2,1}f(x_0, y_0) \rVert \leq \frac{\epsilon}{2} < \epsilon.$$ As far as I can tell, this does not require continuity of any of the derivatives, but only relies on the definition of the function $F$ . This proves the theorem. Only I cannot see how we use any type of continuity of $D_1 f$ and $D_2 f$ , and it seems like only continuity of $D_{2,1} f$ at the point itself is needed. Apostol does not include a proof of this theorem in the second edition of Mathematical Analysis , but he does mention it (with the same assumptions) on page 360.","This question is similar to this one . I am unsure about the assumptions made in Theorem 6.20 of Apostol's Mathematical Analysis , first edition: Let be open and . If , and are continuous in a neighbourhood of the point , then exists and is equal to . The theorem and proof is basically the same as the one featured in this blog post , also mentioned in the above question. By definition of partial derivatives, we need to prove that the limit exists and is equal to . For fixed , introducing the function lets us write the numerator above as Since exists (we do not require continuity of ) in a neighbourhood around , is differentiable (with the obvious derivative) in a suitably small open interval around . If we require to be inside this interval, the mean value theorem yields a between and such that For convenience, define the function (Apostol skips the details of the following steps) analogous to the above function . By the existence (again, not continuity) of in a neighbourhood around , is differentiable in an open interval around . For fixed , the mean value theorem then gives us a between and such that In total we get so the original limit is equal to So far we have not, as far as I can see, used continuity at all. We cannot evaluate the limit directly, so introduce the function i.e. just the inner limit as a function of . Now by continuity of at (that is, not in a neighbourhood around the point, only at the point itself), given there is some such that for we have Choosing and such that , the point lies in this -neighbourhood, so that Now taking the inner limit we get, by continuity of the norm, As far as I can tell, this does not require continuity of any of the derivatives, but only relies on the definition of the function . This proves the theorem. Only I cannot see how we use any type of continuity of and , and it seems like only continuity of at the point itself is needed. Apostol does not include a proof of this theorem in the second edition of Mathematical Analysis , but he does mention it (with the same assumptions) on page 360.","S \subseteq \mathbb{R}^2 f\colon S \to \mathbb{R} D_1 f D_2 f D_{2,1} f (x_0, y_0) \in S D_{1,2} f(x_0, y_0) D_{2,1} f(x_0, y_0)  \lim_{h \to 0} \frac{ D_2 f(x_0 + h, y_0) - D_2 f(x_0, y_0) }{h} D_{2,1} f(x_0, y_0) k  g_k(t) = f(x_0 + t, y_0 + k) - f(x_0 + t, y_0)   D_2 f(x_0 + h, y_0) - D_2 f(x_0, y_0) = \lim_{k \to 0} \frac{g_k(h) - g_k(0)}{k}. D_1 f D_1 f x_0, y_0 g_k 0 h \overline h 0 h  \frac{g_k(h) - g_k(0)}{k} = h \frac{g_k'(\overline h)}{k} = h \frac{D_1 f(x_0 + \overline h, y_0 + k) - D_1 f(x_0 + \overline h, y_0)}{k}.  \phi(s) = D_1 f(x_0 + \overline h, y_0 + s) - D_1 f(x_0 + \overline h, y_0), g_k D_{2,1} (x_0, y_0) \phi y_0 k \overline y y_0 y_0 + k  \frac{D_1 f(x_0 + \overline h, y_0 + k) - D_1 f(x_0 + \overline h, y_0)}{k} = D_{2,1} f(x_0 + \overline h, \overline y).   \frac{ D_2 f(x_0 + h, y_0) - D_2 f(x_0, y_0) }{h} = \lim_{k \to 0} D_{2,1}f(x_0 + \overline h, \overline y), \lim_{h \to 0} \lim_{k \to 0} D_{2,1}f(x_0 + \overline h, \overline y).  F(h) = \lim_{k \to 0} D_{2,1}f(x_0 + \overline h, \overline y),  h D_{2,1}f (x_0, y_0) \epsilon > 0 \delta > 0 (x,y) \in N((x_0, y_0); \delta)  \lVert D_{2,1} f(x,y) - D_{2,1}f(x_0, y_0) \rVert < \frac{\epsilon}{2}. h k \lvert h \rvert, \lvert k \rvert < \delta/2 (x_0 + \overline h, \overline y) \delta  0 \leq \lVert D_{2,1} f(x_0 + \overline h, \overline y) - D_{2,1}f(x_0, y_0) \rVert < \frac{\epsilon}{2}. k \to 0  0 \leq \lVert F(h) - D_{2,1}f(x_0, y_0) \rVert \leq \frac{\epsilon}{2} < \epsilon. F D_1 f D_2 f D_{2,1} f","['real-analysis', 'calculus', 'multivariable-calculus']"
69,"Use the directional derivative to estimate the change in $f(x,y)=\cos\pi xy+xy^2$ as point $(-1,-1)$ moves distance of $0.1$ along vector $i+j$",Use the directional derivative to estimate the change in  as point  moves distance of  along vector,"f(x,y)=\cos\pi xy+xy^2 (-1,-1) 0.1 i+j","Use the directional derivative to estimate how much $f(x,y)=\cos\pi xy+xy^2$ will change if the point $P$ is moved from $(-1,-1)$ a distance of $0.1$ along the vector $i+j$ I don't know how to solve this problem can someone please give me some hints about what the problem wants? the answer in my textbook is $0.15\sqrt2$",Use the directional derivative to estimate how much will change if the point is moved from a distance of along the vector I don't know how to solve this problem can someone please give me some hints about what the problem wants? the answer in my textbook is,"f(x,y)=\cos\pi xy+xy^2 P (-1,-1) 0.1 i+j 0.15\sqrt2","['multivariable-calculus', 'derivatives', 'partial-derivative']"
70,"Solving the$\underset{[0,{\pi}]\times[0,{\pi}]}{\iint}{\cos}{(x+y)}dxdy$ with change of variables",Solving the with change of variables,"\underset{[0,{\pi}]\times[0,{\pi}]}{\iint}{\cos}{(x+y)}dxdy","so I need to solve the following integral using some change of variables: $$\underset{[0,{\pi}]\times[0,{\pi}]}{\iint}{\cos}{(x+y)}dxdy$$ It's easy without a change of variables, of course, but any suggestion as to how to do it with a change of variables? Thanks","so I need to solve the following integral using some change of variables: It's easy without a change of variables, of course, but any suggestion as to how to do it with a change of variables? Thanks","\underset{[0,{\pi}]\times[0,{\pi}]}{\iint}{\cos}{(x+y)}dxdy","['integration', 'multivariable-calculus', 'change-of-variable']"
71,Why is it possible to calculate multivariable limits using polar coordinates?,Why is it possible to calculate multivariable limits using polar coordinates?,,"Why is it possible to calculate multivariable limits using polar coordinates? Let's say I'm looking for some $\lim_{(x,y) \to (0,0)}$ and I'm substituting $x = r cos\theta$ and $y = rsin\theta$ so that I can look at $\lim_{r \to 0}$ . Why can I do this? Am I not just looking at ""straight lines"" going to $(0,0)$ now? What about all the other possible sequences that converging in straight lines to (0,0)?","Why is it possible to calculate multivariable limits using polar coordinates? Let's say I'm looking for some and I'm substituting and so that I can look at . Why can I do this? Am I not just looking at ""straight lines"" going to now? What about all the other possible sequences that converging in straight lines to (0,0)?","\lim_{(x,y) \to (0,0)} x = r cos\theta y = rsin\theta \lim_{r \to 0} (0,0)","['real-analysis', 'calculus', 'multivariable-calculus']"
72,"If $\nabla f(x,y) \cdot (x,y) = f(x,y)$, then $f$ is first degree homogeneous","If , then  is first degree homogeneous","\nabla f(x,y) \cdot (x,y) = f(x,y) f","Given $f$ differentiable in $\mathbb{R}^2 \setminus \{(0,0)\}$ . We're asked to prove that if $\nabla f(x,y)  \cdot (x,y) = f(x,y)$ for all $\mathbb{R}^2$ , then $f$ is an homogeneous function, i.e. $f(t(x,y))=tf(x,y)$ for $t>0$ and $(x,y) \neq (0,0)$ . My prof. explained that proving the consequent amounts to proving that $$g(t) = \frac{f(tx_0, ty_0)}{t}, t>0,$$ is a constant function, which in turn amounts to showing that its derivative is 0 everywhere. But how do we justify that $g(t) = f(x_0,y_0)$ ?","Given differentiable in . We're asked to prove that if for all , then is an homogeneous function, i.e. for and . My prof. explained that proving the consequent amounts to proving that is a constant function, which in turn amounts to showing that its derivative is 0 everywhere. But how do we justify that ?","f \mathbb{R}^2 \setminus \{(0,0)\} \nabla f(x,y)  \cdot (x,y) = f(x,y) \mathbb{R}^2 f f(t(x,y))=tf(x,y) t>0 (x,y) \neq (0,0) g(t) = \frac{f(tx_0, ty_0)}{t}, t>0, g(t) = f(x_0,y_0)","['multivariable-calculus', 'derivatives', 'homogeneous-equation']"
73,"Let $A= \{(0,y,z):z^2 + (y-2)^2=1 \}$ and let $B$ be the set obtained by rotation of $A$ around the $z$-axis. Determine a parametrization for $B$.",Let  and let  be the set obtained by rotation of  around the -axis. Determine a parametrization for .,"A= \{(0,y,z):z^2 + (y-2)^2=1 \} B A z B","I am stuck on the following problem: Let $A= \{(0,y,z):z^2 + (y-2)^2=1 \}$ and let $B$ be the set obtained by rotation of $A$ around the $z$ -axis. Determine a parametrization for $B$ . I have a faint idea that the parametrization should be along the lines of: \begin{align}x&= 2+\cos u \\y&=2 + \cos v \\ z&=\sin v \end{align} In which $y,z$ are the polar coordinates for the circle given in $A$ in the $y,z-$ plane. As $B$ is the rotation along the $z$ -axis, I guessed $x$ should be something like $x=2 + \cos \psi$ akin to $y$ , but the answer in the book is: \begin{align}x&=(2+ \cos v) \cos u \\ y&=(2 + \cos v) \sin u \\ z&=\sin v\end{align} I checked on Wolfram Alpha and my parametrization describes a tube while the solution (correctly) describes a torus, but I don't know how to arrive at it. Is there some general procedure? The book I am reading basically said: ""Look at these (simple) examples. Now handle this"". I think that taking the vector $(2+\cos v,2+ \cos v )$ we'd need to ""regulate"" it's length by multiplying each coordinate by $\cos u, \sin u$ but I can't put it into words in a way that makes sense. Perhaps thinking about polar coordinates with $r=2+\cos v$ ?","I am stuck on the following problem: Let and let be the set obtained by rotation of around the -axis. Determine a parametrization for . I have a faint idea that the parametrization should be along the lines of: In which are the polar coordinates for the circle given in in the plane. As is the rotation along the -axis, I guessed should be something like akin to , but the answer in the book is: I checked on Wolfram Alpha and my parametrization describes a tube while the solution (correctly) describes a torus, but I don't know how to arrive at it. Is there some general procedure? The book I am reading basically said: ""Look at these (simple) examples. Now handle this"". I think that taking the vector we'd need to ""regulate"" it's length by multiplying each coordinate by but I can't put it into words in a way that makes sense. Perhaps thinking about polar coordinates with ?","A= \{(0,y,z):z^2 + (y-2)^2=1 \} B A z B \begin{align}x&= 2+\cos u \\y&=2 + \cos v \\ z&=\sin v \end{align} y,z A y,z- B z x x=2 + \cos \psi y \begin{align}x&=(2+ \cos v) \cos u \\ y&=(2 + \cos v) \sin u \\ z&=\sin v\end{align} (2+\cos v,2+ \cos v ) \cos u, \sin u r=2+\cos v",['multivariable-calculus']
74,Volume of a truncated paraboloid,Volume of a truncated paraboloid,,"A body is surrounded by its lateral faces: $$z(x,y) = h \left(1 - \left(\frac{x}{a}\right)^2 - \left(\frac{y}{b}\right)^2 \right)$$ and $$z(x,y)=0$$ It should be a paraboloid, right? How can I calculate its volume via integration over $x$ and $y$ in Cartesian coordinates? Thanks a lot in advance!","A body is surrounded by its lateral faces: and It should be a paraboloid, right? How can I calculate its volume via integration over and in Cartesian coordinates? Thanks a lot in advance!","z(x,y) = h \left(1 - \left(\frac{x}{a}\right)^2 - \left(\frac{y}{b}\right)^2 \right) z(x,y)=0 x y","['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
75,Gradient and Hessian of $g(x) = f(Ax + b)$,Gradient and Hessian of,g(x) = f(Ax + b),"Given scalar field $f : \Bbb R^m \to \Bbb R$ , matrix $A \in \Bbb R^{m \times n}$ and vector $b \in \Bbb R^m$ , find the gradient and the Hessian of the scalar field $g : \Bbb R^n \to \Bbb R$ defined by $g(x) := f(Ax + b)$ . I cannot find the expression for the derivative: $g'(x) = f'(Ax + b)*(Ax + b)'$ I believe the derivative $f'(Ax + b)$ is simply A*partial derivatives. But I do dot know how to proceed with the other terms. I know the expressions for gradient and Hessian, but I did never see it in matrix form.","Given scalar field , matrix and vector , find the gradient and the Hessian of the scalar field defined by . I cannot find the expression for the derivative: I believe the derivative is simply A*partial derivatives. But I do dot know how to proceed with the other terms. I know the expressions for gradient and Hessian, but I did never see it in matrix form.",f : \Bbb R^m \to \Bbb R A \in \Bbb R^{m \times n} b \in \Bbb R^m g : \Bbb R^n \to \Bbb R g(x) := f(Ax + b) g'(x) = f'(Ax + b)*(Ax + b)' f'(Ax + b),"['multivariable-calculus', 'derivatives', 'hessian-matrix', 'scalar-fields']"
76,If $ Df(x) = 0$ then is $M$ necessarily not a manifold?,If  then is  necessarily not a manifold?, Df(x) = 0 M,"This is from a course which closely follows Munkres' ""Analysis on Manifolds"". We are asked to prove / disprove whether $M = \{ (x,y) \in \mathbb{R}^2: y^2 = x^3 \} $ is a differentiable manifold, which it isn't. Theorem 24.4 from Munkres allows one to prove that for a given function $ f \colon \mathbb{R}^n \to \mathbb{R}$ of class $C^r$ , $ \{ \textbf{x} \in \mathbb{R}^n: f( \textbf{x} ) = 0 \} $ is a manifold provided that $Df( \textbf{x} ) \neq \textbf{0}$ ( and the set is nonempty). Is the converse true, that if $Df( \textbf{x} ) = \textbf{0}$ somewhere on $M$ that it fails to be a manifold? Setting $g(x,y) = x^3 - y^2 $ , since $Dg(0,0) = \textbf{0}$ this would provide a quick proof that $M$ is not a manifold. I wouldn't have suspected it would be true but a solution to an older question seems to use this method. I have not found anything in Munkres about it. I have a different solution, my question is just whether the method described above is valid.","This is from a course which closely follows Munkres' ""Analysis on Manifolds"". We are asked to prove / disprove whether is a differentiable manifold, which it isn't. Theorem 24.4 from Munkres allows one to prove that for a given function of class , is a manifold provided that ( and the set is nonempty). Is the converse true, that if somewhere on that it fails to be a manifold? Setting , since this would provide a quick proof that is not a manifold. I wouldn't have suspected it would be true but a solution to an older question seems to use this method. I have not found anything in Munkres about it. I have a different solution, my question is just whether the method described above is valid.","M = \{ (x,y) \in \mathbb{R}^2: y^2 = x^3 \}   f \colon \mathbb{R}^n \to \mathbb{R} C^r  \{ \textbf{x} \in \mathbb{R}^n: f( \textbf{x} ) = 0 \}  Df( \textbf{x} ) \neq \textbf{0} Df( \textbf{x} ) = \textbf{0} M g(x,y) = x^3 - y^2  Dg(0,0) = \textbf{0} M","['real-analysis', 'multivariable-calculus', 'manifolds']"
77,How to prove this subsets of $\mathbb{R}^n$ are homeomorphic to $\mathbb{R}^n$?,How to prove this subsets of  are homeomorphic to ?,\mathbb{R}^n \mathbb{R}^n,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be a nonzero linear map. Let $\alpha \in \mathbb{R}$ e define $A=\{x\in \mathbb{R}^n:f(x)<\alpha\}$ . By continuity of $f$ , $A$ is open in $\mathbb{R}^n$ . How can I prove (rigorously) that $A$ and $\mathbb{R}^n$ are homeomorphic ? I have some thoughts but I'm not able to make them rigorous. Intuitively I see that $\{x\in \mathbb{R}^n:f(x)=\alpha\}$ is an hyperplane in $\mathbb{R}^n$ which divides $\mathbb{R}^n$ into to ""open rectangles"" homeomorphics between them, and one of this ""rectangles"" is $A$ . For example, if $n=2$ , I think I can show that $A$ is homeomorphic with $\mathbb{R}\times(-\infty,\alpha)$ maybe using some rotation, and then I easily can show that $\mathbb{R}\times(-\infty,\alpha)$ is homeomrphic to $\mathbb{R}^2$ since I know that $(-\infty,\alpha)$ is homeomrphic to $\mathbb{R}$ . For example if $(a,b)\in \mathbb{R}^2$ is the vector such that $f(x,y)=ax+by$ , such homeomorphism could be the linear map $\mathbb{R}\times(-\infty,\alpha)\to A$ represented by the matrix \begin{pmatrix} \cos(b/a)& -\sin(b/a) \\\sin(p/a) & \cos(b/a) \end{pmatrix} What is a rigorous (and possibly elegant) way to prove that $A$ and $\mathbb{R}^n$ are homeomorphic for general $n$ ? Suppose also that $\beta\in \mathbb{R}$ and $\beta<\alpha$ . Let $B=\{x\in \mathbb{R}^n:f(x)>\beta\}$ . How can I prove that $A \cap B$ is non empty and homoemorphic to $\mathbb{R}^n$ ?","Let be a nonzero linear map. Let e define . By continuity of , is open in . How can I prove (rigorously) that and are homeomorphic ? I have some thoughts but I'm not able to make them rigorous. Intuitively I see that is an hyperplane in which divides into to ""open rectangles"" homeomorphics between them, and one of this ""rectangles"" is . For example, if , I think I can show that is homeomorphic with maybe using some rotation, and then I easily can show that is homeomrphic to since I know that is homeomrphic to . For example if is the vector such that , such homeomorphism could be the linear map represented by the matrix What is a rigorous (and possibly elegant) way to prove that and are homeomorphic for general ? Suppose also that and . Let . How can I prove that is non empty and homoemorphic to ?","f:\mathbb{R}^n\to\mathbb{R} \alpha \in \mathbb{R} A=\{x\in \mathbb{R}^n:f(x)<\alpha\} f A \mathbb{R}^n A \mathbb{R}^n \{x\in \mathbb{R}^n:f(x)=\alpha\} \mathbb{R}^n \mathbb{R}^n A n=2 A \mathbb{R}\times(-\infty,\alpha) \mathbb{R}\times(-\infty,\alpha) \mathbb{R}^2 (-\infty,\alpha) \mathbb{R} (a,b)\in \mathbb{R}^2 f(x,y)=ax+by \mathbb{R}\times(-\infty,\alpha)\to A \begin{pmatrix}
\cos(b/a)& -\sin(b/a)
\\\sin(p/a) & \cos(b/a)
\end{pmatrix} A \mathbb{R}^n n \beta\in \mathbb{R} \beta<\alpha B=\{x\in \mathbb{R}^n:f(x)>\beta\} A \cap B \mathbb{R}^n","['calculus', 'general-topology', 'multivariable-calculus']"
78,Smooth curve segments and smooth charts,Smooth curve segments and smooth charts,,"Let $M$ be a smooth manifold with or without boundary, and let $\gamma:[a,b]\to M$ be a smooth curve. I want to show that there exist a finite partition $a=a_0<a_1<\dots<a_k=b$ such that $\gamma([a_{i-1},a_i])$ is contained in the domain of a single smooth chart for each $i=1,\dots,k$ . Here is my argument but I'm not able to conclude it By compactness of $\gamma([a,b])$ there exists a finite number of smooth charts $(U_i,\varphi_i)_{i=1}^k$ such that $$\gamma([a,b])\subseteq U_1\cup\dots\cup U_k.$$ Suppose $\gamma(a)\in U_1$ , so we have $a\in \gamma^{-1}(U_1)$ wich is open in $[a,b]$ . So there is an interval of the form $[a,\varepsilon)$ contained in $\gamma^{-1}(U_1)$ with $\varepsilon>a$ . Let $\delta:=$ sup $\{\varepsilon>a:[a,\varepsilon)\subseteq \gamma^{-1}(U_1)\}$ . Then I can show that $[a,\delta) \subseteq \gamma^{-1}(U_1)$ . Thus we have $\gamma([a,\delta))\subseteq U_1$ and $\delta>a$ . Now suppose that $\gamma(\delta)\in U_2$ , so $\delta\in \gamma^{-1}(U_2)$ which is open in $[a,b]$ . Thus there is $\varepsilon>0$ such that $(\delta-\varepsilon,\delta+\varepsilon)\subseteq \gamma^{-1}(U_2)$ and $\delta-\varepsilon>a$ . Let $a_1=\delta-\varepsilon/2$ . Thus we have $\gamma([a,a_1])\subseteq U_1$ and $\gamma(a_1)\in U_2$ . Repeating the argument with $a_1$ in place of $a$ , then i can find $a_2$ such that $\gamma([a_1,a_2])\subseteq U_2$ and $\gamma(a_2)\in U_3$ . Contiuing this way I find points $a=a_0<a_1<\dots<a_k$ such that $\gamma([a_{i-1},a_i])$ is contained in $U_i$ for each $i=1,\dots,k$ . I'm not able to show that $a_k=b$ . Edit As noted in the comment, this argument is wrong. So how can I prove this statement?","Let be a smooth manifold with or without boundary, and let be a smooth curve. I want to show that there exist a finite partition such that is contained in the domain of a single smooth chart for each . Here is my argument but I'm not able to conclude it By compactness of there exists a finite number of smooth charts such that Suppose , so we have wich is open in . So there is an interval of the form contained in with . Let sup . Then I can show that . Thus we have and . Now suppose that , so which is open in . Thus there is such that and . Let . Thus we have and . Repeating the argument with in place of , then i can find such that and . Contiuing this way I find points such that is contained in for each . I'm not able to show that . Edit As noted in the comment, this argument is wrong. So how can I prove this statement?","M \gamma:[a,b]\to M a=a_0<a_1<\dots<a_k=b \gamma([a_{i-1},a_i]) i=1,\dots,k \gamma([a,b]) (U_i,\varphi_i)_{i=1}^k \gamma([a,b])\subseteq U_1\cup\dots\cup U_k. \gamma(a)\in U_1 a\in \gamma^{-1}(U_1) [a,b] [a,\varepsilon) \gamma^{-1}(U_1) \varepsilon>a \delta:= \{\varepsilon>a:[a,\varepsilon)\subseteq \gamma^{-1}(U_1)\} [a,\delta) \subseteq \gamma^{-1}(U_1) \gamma([a,\delta))\subseteq U_1 \delta>a \gamma(\delta)\in U_2 \delta\in \gamma^{-1}(U_2) [a,b] \varepsilon>0 (\delta-\varepsilon,\delta+\varepsilon)\subseteq \gamma^{-1}(U_2) \delta-\varepsilon>a a_1=\delta-\varepsilon/2 \gamma([a,a_1])\subseteq U_1 \gamma(a_1)\in U_2 a_1 a a_2 \gamma([a_1,a_2])\subseteq U_2 \gamma(a_2)\in U_3 a=a_0<a_1<\dots<a_k \gamma([a_{i-1},a_i]) U_i i=1,\dots,k a_k=b","['general-topology', 'multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
79,"$[a,b]$ as a smooth manifold with boundary has global coordinates?",as a smooth manifold with boundary has global coordinates?,"[a,b]","Consider a compact interval $[a,b]$ . If $[a,b]$ had global coordinates, then there would be an homeomorphism $f:[a,b]\to U$ where $U$ is an open subset of $\mathbb{R}$ or an open subset of $[0,\infty)$ . But $U$ cannot be open in $\mathbb{R}$ because $U$ is compact, so $U$ must be open in $[0,\infty)$ . But also in this case I think U must be a closed interval, precisely $U=$ [min $f$ ,max $f]$ . But this set cannot be open in $[0,\infty)$ . So i deduce that $[a,b]$ does not have global coordinates. But then in John Lee's book Introduction to smooth manifolds i read a statement of the form ""Let $t$ denote the stadard coordinate on $[a,b]$ "". So how should I interpret the statement above?","Consider a compact interval . If had global coordinates, then there would be an homeomorphism where is an open subset of or an open subset of . But cannot be open in because is compact, so must be open in . But also in this case I think U must be a closed interval, precisely [min ,max . But this set cannot be open in . So i deduce that does not have global coordinates. But then in John Lee's book Introduction to smooth manifolds i read a statement of the form ""Let denote the stadard coordinate on "". So how should I interpret the statement above?","[a,b] [a,b] f:[a,b]\to U U \mathbb{R} [0,\infty) U \mathbb{R} U U [0,\infty) U= f f] [0,\infty) [a,b] t [a,b]","['general-topology', 'multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
80,Derive the volume of a torus using the divergence theorem and Fubini's theorem.,Derive the volume of a torus using the divergence theorem and Fubini's theorem.,,"Given is the vector field $f=(x,y,0)$ and the set $\Omega :=((\sqrt{x^2+y^2}-2)^2+z^2 <1)$ (which is obviously a torus) I'm asked to : a) Calculate the outward unit normal field $v: \delta \Omega \to \mathbb{R}^3$ b) Calculate $\int_{\delta\Omega}\langle f,v\rangle dS$ using the Divergence Theorem. Hint : It is expected that you again derive the volume of a torus. For that, calculate first the area enclosed of the set $A_z:=((\sqrt{x^2+y^2}-2)^2\leq1-z^2) $ for $ -1\leq z \leq 1$ and then use Fubini's theorem. a) So here I think that the outward unit field vector is just the gradient of $f$ normalized. So the gradient is $(1,1,0)$ and when normalized, it is $(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0)$ . b) Now, here I'm not really sure. For the area enclosed, I'm obviously supposed to use polar coordinates. So $(r-2)^2\leq1-z^2$ . So $\pm(r-2)=\pm\sqrt{(1-z^2)}$ .  So $r=2+\sqrt{1-z^2}$ or $r=2-\sqrt{1-z^2}$ .  Now, the area is $\int_{2-\sqrt{1-z^2}}^{2+\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta=\int_{-\sqrt{1-z^2}}^{\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta$ .  And so the volume will be $\int_{-1}^{1}\int_{-\sqrt{1-z^2}}^{\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta$ And our divergence is $2$ , so we will just multiply the result by $2$ . Now, my questions are: Is my procedure for a) correct ? Is my procedure for b) correct and if yes, am I now just supposed to evaluate that integral ? Thanks for you help !","Given is the vector field and the set (which is obviously a torus) I'm asked to : a) Calculate the outward unit normal field b) Calculate using the Divergence Theorem. Hint : It is expected that you again derive the volume of a torus. For that, calculate first the area enclosed of the set for and then use Fubini's theorem. a) So here I think that the outward unit field vector is just the gradient of normalized. So the gradient is and when normalized, it is . b) Now, here I'm not really sure. For the area enclosed, I'm obviously supposed to use polar coordinates. So . So .  So or .  Now, the area is .  And so the volume will be And our divergence is , so we will just multiply the result by . Now, my questions are: Is my procedure for a) correct ? Is my procedure for b) correct and if yes, am I now just supposed to evaluate that integral ? Thanks for you help !","f=(x,y,0) \Omega :=((\sqrt{x^2+y^2}-2)^2+z^2 <1) v: \delta \Omega \to \mathbb{R}^3 \int_{\delta\Omega}\langle f,v\rangle dS A_z:=((\sqrt{x^2+y^2}-2)^2\leq1-z^2)   -1\leq z \leq 1 f (1,1,0) (\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0) (r-2)^2\leq1-z^2 \pm(r-2)=\pm\sqrt{(1-z^2)} r=2+\sqrt{1-z^2} r=2-\sqrt{1-z^2} \int_{2-\sqrt{1-z^2}}^{2+\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta=\int_{-\sqrt{1-z^2}}^{\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta \int_{-1}^{1}\int_{-\sqrt{1-z^2}}^{\sqrt{1-z^2}}\int_{0}^{2\pi}1rdrd\theta 2 2","['real-analysis', 'calculus']"
81,Finding a vector field such that its curl equals a given vector field,Finding a vector field such that its curl equals a given vector field,,"Let $$g(x,y,z) = \frac{(x,y,z+1)}{(x^2+y^2+(z+1)^2)^{3/2}}$$ where $z \gt 0$ . Find a vector field $f$ , such that $$\nabla \times f = g$$ My attempt. I know that $$\operatorname{curl}\vec f = \vec\nabla\times\vec f=\left(\frac{\partial f_z}{\partial y} - \frac{\partial f_y}{\partial z}\right)\hat x+\left(\frac{\partial f_x}{\partial z} - \frac{\partial f_z}{\partial x}\right)\hat y+\left(\frac{\partial f_y}{\partial x} - \frac{\partial f_x}{\partial y}\right)\hat z$$ for vector fields over $\mathbb{R}^3$ . I'm trying to find $f=(f_x,f_y,f_z)$ which satisfies the following equations: $$\frac{\partial f_z}{\partial y} - \frac{\partial f_y}{\partial z} = \frac{1}{(x^2+y^2+(z+1)^2)^{3/2}}$$ $$\frac{\partial f_x}{\partial z} - \frac{\partial f_z}{\partial x} = \frac{1}{(x^2+y^2+(z+1)^2)^{3/2}}$$ $$\frac{\partial f_y}{\partial x} - \frac{\partial f_x}{\partial y} = \frac{1+{1/z}}{(x^2+y^2+(z+1)^2)^{3/2}}$$ How can I work it out from here? Is there any usual way to do this? Or do I need to go on each $f_i$ and try finding how to make it satisfy the equations?","Let where . Find a vector field , such that My attempt. I know that for vector fields over . I'm trying to find which satisfies the following equations: How can I work it out from here? Is there any usual way to do this? Or do I need to go on each and try finding how to make it satisfy the equations?","g(x,y,z) = \frac{(x,y,z+1)}{(x^2+y^2+(z+1)^2)^{3/2}} z \gt 0 f \nabla \times f = g \operatorname{curl}\vec f = \vec\nabla\times\vec f=\left(\frac{\partial f_z}{\partial y} - \frac{\partial f_y}{\partial z}\right)\hat x+\left(\frac{\partial f_x}{\partial z} - \frac{\partial f_z}{\partial x}\right)\hat y+\left(\frac{\partial f_y}{\partial x} - \frac{\partial f_x}{\partial y}\right)\hat z \mathbb{R}^3 f=(f_x,f_y,f_z) \frac{\partial f_z}{\partial y} - \frac{\partial f_y}{\partial z} = \frac{1}{(x^2+y^2+(z+1)^2)^{3/2}} \frac{\partial f_x}{\partial z} - \frac{\partial f_z}{\partial x} = \frac{1}{(x^2+y^2+(z+1)^2)^{3/2}} \frac{\partial f_y}{\partial x} - \frac{\partial f_x}{\partial y} = \frac{1+{1/z}}{(x^2+y^2+(z+1)^2)^{3/2}} f_i","['calculus', 'multivariable-calculus', 'vector-analysis', 'vector-fields', 'curl']"
82,"Finding $\iiint x^2\,{\rm d}x{\rm d}y{\rm d}z$ over the volume bounded by $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2 }= 1$",Finding  over the volume bounded by,"\iiint x^2\,{\rm d}x{\rm d}y{\rm d}z \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2 }= 1","Finding the value of $$\iiint x^2\,\mathrm dx\mathrm dy\mathrm dz$$ over the volume bounded by the ellipsoid $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1.$$ I am taking limits as follow: $$-a \leq x \leq a$$ $$-\frac{b}{a} (a^2-x^2)^{1/2} \leq y \leq \frac{b}{a} (a^2-x^2)^{1/2}$$ $$-\frac{c}{ab} ((ab)^2 - (bx)^2 - (ay)^2)^{1/2} \leq z \leq \frac{c}{ab} ((ab)^2 - (bx)^2 - (ay)^2)^{1/2}$$ And solving the integral as $\mathrm dz$ first then $\mathrm dy$ and then $\mathrm dx$ . But it is just becoming harder and harder with each step. I do not have a tutor. So, I highly need help of you masters. Please help me.","Finding the value of over the volume bounded by the ellipsoid I am taking limits as follow: And solving the integral as first then and then . But it is just becoming harder and harder with each step. I do not have a tutor. So, I highly need help of you masters. Please help me.","\iiint x^2\,\mathrm dx\mathrm dy\mathrm dz \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1. -a \leq x \leq a -\frac{b}{a} (a^2-x^2)^{1/2} \leq y \leq \frac{b}{a} (a^2-x^2)^{1/2} -\frac{c}{ab} ((ab)^2 - (bx)^2 - (ay)^2)^{1/2} \leq z \leq \frac{c}{ab} ((ab)^2 - (bx)^2 - (ay)^2)^{1/2} \mathrm dz \mathrm dy \mathrm dx","['multivariable-calculus', 'definite-integrals']"
83,Two Variable Function with Specific Properties (Challenging),Two Variable Function with Specific Properties (Challenging),,"I am looking for a two variable function (a surface), $g(x,b)$ , with the following properties: $$g:\ [0,\pi] \times (0,\pi) \to[0,\pi] $$ $$g(0,b)=0$$ $$g(\pi,b)=\pi$$ $$g(b,b)=\frac{\pi}{2}$$ $$g\left( x,\frac{\pi}{2}\right)=x$$ Where $x\in[0,\pi]$ and $b\in(0,\pi)$ . Can you define such a $g$ ?  I have been looking for a while and am unable to find one.  If you cannot define one, does such a $g$ even exist?","I am looking for a two variable function (a surface), , with the following properties: Where and . Can you define such a ?  I have been looking for a while and am unable to find one.  If you cannot define one, does such a even exist?","g(x,b) g:\ [0,\pi] \times (0,\pi) \to[0,\pi]  g(0,b)=0 g(\pi,b)=\pi g(b,b)=\frac{\pi}{2} g\left( x,\frac{\pi}{2}\right)=x x\in[0,\pi] b\in(0,\pi) g g","['calculus', 'multivariable-calculus', 'functions', 'surfaces']"
84,Problem understanding a solenoidal vector field that is not a curl.,Problem understanding a solenoidal vector field that is not a curl.,,"Problem In Apostal's calculus volume 2 , there is an example which shows that a solenoidal vector field that is not a curl. Example states that proof is difficult at this stage . Can anyone please me some understanding why this can happen. That is on what kind of open sets a solenoidal vector field is always a curl of some other vector field in that set? NB-Currently a sophomore .","Problem In Apostal's calculus volume 2 , there is an example which shows that a solenoidal vector field that is not a curl. Example states that proof is difficult at this stage . Can anyone please me some understanding why this can happen. That is on what kind of open sets a solenoidal vector field is always a curl of some other vector field in that set? NB-Currently a sophomore .",,"['calculus', 'multivariable-calculus']"
85,Find the equation in spherical coordinates of $x^2 + y^2 – z^2 = 4$.,Find the equation in spherical coordinates of .,x^2 + y^2 – z^2 = 4,"Find the equation in spherical coordinates of $x^2 + y^2 – z^2 = 4$ . $$\begin{align} x^2 + y^2 &= r^2\sin^2(\theta)\\ z^2 &= r^2 \cos(\theta) \\ x^2 + y^2 - z^2&=r^2(\sin^2(\theta) - \cos^2(\theta)) = 4 \end{align}$$ Thus, $$-r^2(\cos(2\theta)) = 4$$ Is this right?","Find the equation in spherical coordinates of . Thus, Is this right?","x^2 + y^2 – z^2 = 4 \begin{align}
x^2 + y^2 &= r^2\sin^2(\theta)\\
z^2 &= r^2 \cos(\theta) \\
x^2 + y^2 - z^2&=r^2(\sin^2(\theta) - \cos^2(\theta)) = 4
\end{align} -r^2(\cos(2\theta)) = 4","['multivariable-calculus', 'spherical-coordinates']"
86,"Calculate $\int_{S_A} x^T B x \, dx$ where $S_A$ is an ellipsoid",Calculate  where  is an ellipsoid,"\int_{S_A} x^T B x \, dx S_A","Suppose that $S_A \subset \mathbb{R}^n$ denotes an ellipsoid centered at the origin, \begin{align} S_A= \{ x \in \mathbb{R}^n :  x^T A x \le 1    \} , \end{align} where matrix $A$ is symmetric and positive definite. Let matrix $B$ be positive definite.  Can the following integral be calculated in closed form? \begin{align} \int_{S_A}   x^T B x  \, dx \end{align} I understand we have to switch to spherical coordinates here, but there a few details that are not clear to me.  For example, how to change the coordinates in this case.","Suppose that denotes an ellipsoid centered at the origin, where matrix is symmetric and positive definite. Let matrix be positive definite.  Can the following integral be calculated in closed form? I understand we have to switch to spherical coordinates here, but there a few details that are not clear to me.  For example, how to change the coordinates in this case.","S_A \subset \mathbb{R}^n \begin{align}
S_A= \{ x \in \mathbb{R}^n :  x^T A x \le 1    \} ,
\end{align} A B \begin{align}
\int_{S_A}   x^T B x  \, dx
\end{align}","['integration', 'multivariable-calculus', 'definite-integrals', 'quadratic-forms', 'ellipsoids']"
87,How to calculate the following $ \frac{\partial}{\partial x} \log (\det X(x))$?,How to calculate the following ?, \frac{\partial}{\partial x} \log (\det X(x)),How to calculate the following $$ \frac{\partial}{\partial x} \log (\det X(x))$$ where $X$ is a matrix in $\mathbb{R}^{n\times n}$ which is a function of $x\in \mathbb{R}^d$?,How to calculate the following $$ \frac{\partial}{\partial x} \log (\det X(x))$$ where $X$ is a matrix in $\mathbb{R}^{n\times n}$ which is a function of $x\in \mathbb{R}^d$?,,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
88,Integrating Heaviside Step Function of two Variables,Integrating Heaviside Step Function of two Variables,,"Suppose we have a definite integral like $$ I=\int_0^\infty dx \int_0^\infty dy \,  Θ(α-x-y) $$ where $a \in R_+^*$ and $Θ$ is the Heaviside step function . Of course this is easy in that we can find the answer without working with integrals,  since it's simply the area of a triangle with vertices $O(0,0)$ , $A(a,0)$, $B(0,a)$, which is $\frac{a^2}{2}$. However trying to work the integral out doesn't seem so trivial (to me at least). One could, naively, do for example $I=\int_0^\infty dx \int_0^{a-x} dy = -\infty$ (since $Θ=0$ for $y \ge a-x$) which is obviously wrong- somehow $Θ$ should affect both integral limits, but I can't see how this can happen. Probably I'm misunderstanding the definition of $Θ(α-x-y)$. EDIT : As @John Polcari pointed out, in the above exaple, we should limit $x$ such that $a-x \ge 0$, so we should have $I= \int_0^\infty dx \int_0^{a-x}Θ(a-x) \, dy $ which indeed gives the right result. However this seems to me like adding the $Θ(α-x)$ by hand, which is no different from the first argument with the triangle's area. Is there a prettier-more strict- way of doing it?","Suppose we have a definite integral like $$ I=\int_0^\infty dx \int_0^\infty dy \,  Θ(α-x-y) $$ where $a \in R_+^*$ and $Θ$ is the Heaviside step function . Of course this is easy in that we can find the answer without working with integrals,  since it's simply the area of a triangle with vertices $O(0,0)$ , $A(a,0)$, $B(0,a)$, which is $\frac{a^2}{2}$. However trying to work the integral out doesn't seem so trivial (to me at least). One could, naively, do for example $I=\int_0^\infty dx \int_0^{a-x} dy = -\infty$ (since $Θ=0$ for $y \ge a-x$) which is obviously wrong- somehow $Θ$ should affect both integral limits, but I can't see how this can happen. Probably I'm misunderstanding the definition of $Θ(α-x-y)$. EDIT : As @John Polcari pointed out, in the above exaple, we should limit $x$ such that $a-x \ge 0$, so we should have $I= \int_0^\infty dx \int_0^{a-x}Θ(a-x) \, dy $ which indeed gives the right result. However this seems to me like adding the $Θ(α-x)$ by hand, which is no different from the first argument with the triangle's area. Is there a prettier-more strict- way of doing it?",,"['integration', 'multivariable-calculus', 'step-function']"
89,How do you change the order of integration without sketching?,How do you change the order of integration without sketching?,,"Specifically, for a double integral $$\int_a^b \int_{g_1(x)}^{g_2(x)} f(x,y) \, dy \, dx$$ how would you change the order of integration without having to sketch it out? I came across this while researching which talks about the use of the Heaviside function, however I am unsure how to apply this process to all double integrals. Thanks!","Specifically, for a double integral $$\int_a^b \int_{g_1(x)}^{g_2(x)} f(x,y) \, dy \, dx$$ how would you change the order of integration without having to sketch it out? I came across this while researching which talks about the use of the Heaviside function, however I am unsure how to apply this process to all double integrals. Thanks!",,"['calculus', 'integration', 'multivariable-calculus']"
90,Lagrange multiplier when decisions variables are not in the same set,Lagrange multiplier when decisions variables are not in the same set,,"Find the maximum of $2x+y$ over the constraint set $$S = \left\{ (x,y) \in \mathbb R^2 : 2x^2 + y^2 \leq 1, \; x \leq 0 \right\}$$ I want to use Lagrange multipliers to find the optimal solution. However, Lagrange requires $\vec x \in A$. In our case $R$, however $x$ can only be negative or zero. How can I get rid of this constraint? My idea is to do $x=w-z, w-z \le 0, w,z \in R$, but I am not sure if this is the right way to do it.","Find the maximum of $2x+y$ over the constraint set $$S = \left\{ (x,y) \in \mathbb R^2 : 2x^2 + y^2 \leq 1, \; x \leq 0 \right\}$$ I want to use Lagrange multipliers to find the optimal solution. However, Lagrange requires $\vec x \in A$. In our case $R$, however $x$ can only be negative or zero. How can I get rid of this constraint? My idea is to do $x=w-z, w-z \le 0, w,z \in R$, but I am not sure if this is the right way to do it.",,"['multivariable-calculus', 'optimization', 'convex-optimization', 'lagrange-multiplier', 'qclp']"
91,"Minimise the functional $\int_D \| \operatorname{grad}u(M)\|^2\,\mathrm dM$",Minimise the functional,"\int_D \| \operatorname{grad}u(M)\|^2\,\mathrm dM","I have the following problem: Let $k \in \mathbb{N}^*$ and $D$ be the closed disk of radius $1$ in $\mathbb{C}$ and $g\colon \mathbb{U} \to \mathbb{R}:z \mapsto \sin(k\mathrm{Arg}(z))$. Now let $E$ be the set of functions from $D$ to $\mathbb{R}$ which are continuous on $D$, $C^1$ on $\overset{\circ}{D}$ and whose partial derivatives can be continuously extend to $D$ and whose restrictions at $\mathbb{U}$ is $g$. Find the minimum of the functional:    $$u \in E \mapsto\int_D \| \operatorname{grad}u(M)\|^2 \,\mathrm{d}M.$$ I would like to have some thoughts on it, the problem is that I do not know at all how to abord it and where to look at. I know that most of the time in order to find the maximum of a function we need to find the critical point of this function in order to see if these points are relatives minimum and then ajust. But here it is not clear to me how I should find the critical points or if I should apply an other technique. Thank you.","I have the following problem: Let $k \in \mathbb{N}^*$ and $D$ be the closed disk of radius $1$ in $\mathbb{C}$ and $g\colon \mathbb{U} \to \mathbb{R}:z \mapsto \sin(k\mathrm{Arg}(z))$. Now let $E$ be the set of functions from $D$ to $\mathbb{R}$ which are continuous on $D$, $C^1$ on $\overset{\circ}{D}$ and whose partial derivatives can be continuously extend to $D$ and whose restrictions at $\mathbb{U}$ is $g$. Find the minimum of the functional:    $$u \in E \mapsto\int_D \| \operatorname{grad}u(M)\|^2 \,\mathrm{d}M.$$ I would like to have some thoughts on it, the problem is that I do not know at all how to abord it and where to look at. I know that most of the time in order to find the maximum of a function we need to find the critical point of this function in order to see if these points are relatives minimum and then ajust. But here it is not clear to me how I should find the critical points or if I should apply an other technique. Thank you.",,"['real-analysis', 'multivariable-calculus', 'calculus-of-variations', 'maxima-minima']"
92,Multivariate Mean Value Theorem Reference,Multivariate Mean Value Theorem Reference,,"There is this result that I'm almost sure I've proven it a few times, but every once in a while I forget wheter it is true or false. Also, everytime I forget it, I try to look it up and never find a reference of it again (wasn't in one of the analysis books on my hard drive?). The result is: Let $f\colon U \subseteq \mathbb{R}^n \to \mathbb{R}^m$ a differentiable function on a connected subset of $\mathbb{R}^n$. Then, for any $x,y \in U$ there is a $z \in U$ satisfying:   $$|f(x)-f(y)| \leq |Df(z)(x-y)|$$ I find it on wikipedia for the case $m = 1$ or $n = 1$, but I can't seem to find the general proof anywhere. Am I mad? Could someone please point me to a reference of this theorem?","There is this result that I'm almost sure I've proven it a few times, but every once in a while I forget wheter it is true or false. Also, everytime I forget it, I try to look it up and never find a reference of it again (wasn't in one of the analysis books on my hard drive?). The result is: Let $f\colon U \subseteq \mathbb{R}^n \to \mathbb{R}^m$ a differentiable function on a connected subset of $\mathbb{R}^n$. Then, for any $x,y \in U$ there is a $z \in U$ satisfying:   $$|f(x)-f(y)| \leq |Df(z)(x-y)|$$ I find it on wikipedia for the case $m = 1$ or $n = 1$, but I can't seem to find the general proof anywhere. Am I mad? Could someone please point me to a reference of this theorem?",,"['calculus', 'multivariable-calculus', 'derivatives', 'reference-request']"
93,Spivak: Calculus on Manifolds Norms (Problem 1.1),Spivak: Calculus on Manifolds Norms (Problem 1.1),,"I found it difficult to prove this problem due to the following fact. I want to derive it myself but am having trouble laying out the proof. First, I want to show the following holds: Show that $$(\sum_{i=1}^n |x_i|)^2= \sum_{i=1}^n x_{i}^2 + 2\sum_{i\neq j}|x_i||x_j|$$ I would also like a little more emphasis on the notation being used for the second sum. Thanks!","I found it difficult to prove this problem due to the following fact. I want to derive it myself but am having trouble laying out the proof. First, I want to show the following holds: Show that $$(\sum_{i=1}^n |x_i|)^2= \sum_{i=1}^n x_{i}^2 + 2\sum_{i\neq j}|x_i||x_j|$$ I would also like a little more emphasis on the notation being used for the second sum. Thanks!",,"['real-analysis', 'multivariable-calculus']"
94,is the Laplacian just the gradient dotted with itself?,is the Laplacian just the gradient dotted with itself?,,"I guess no but in the solution of this problem (see attached picture) at some point, it is stated that $\Delta v = D_xv \cdot D_xv $ but $D_xv = [v_{x_1} \cdots v_{x_n}]^T$, right ? so shouldn't $D_xv \cdot D_xv=v^2_{x_1}+ \cdots + v^2_{x_n}$ ? also I did notice that sometimes they write $D_xv$ and sometimes $D_x \cdot v $. I suspect that I am missing something here...","I guess no but in the solution of this problem (see attached picture) at some point, it is stated that $\Delta v = D_xv \cdot D_xv $ but $D_xv = [v_{x_1} \cdots v_{x_n}]^T$, right ? so shouldn't $D_xv \cdot D_xv=v^2_{x_1}+ \cdots + v^2_{x_n}$ ? also I did notice that sometimes they write $D_xv$ and sometimes $D_x \cdot v $. I suspect that I am missing something here...",,"['multivariable-calculus', 'partial-differential-equations', 'laplacian']"
95,"Find the equation of tangent line to the intersection of the two surfaces in $P(4,-2,20)$ $z=x^2+y^2 \,\,,\,z=2x+4y+20$",Find the equation of tangent line to the intersection of the two surfaces in,"P(4,-2,20) z=x^2+y^2 \,\,,\,z=2x+4y+20","Find the equation of tangent line to the intersection of the two surfaces in $P(4,-2,20)$ $$z=x^2+y^2 \,\,,\,z=2x+4y+20$$ Let $f,g:\mathbb{R}^3\rightarrow\mathbb{R}$ such that $f(x,y,z)=x^2+y^2-z\,\,\,$ and $g(x,y,z)=2x+4y-z+20$. Then, $$\nabla f(x,y,z)=(2x,2x,-1)\implies\nabla f(4,-2,20)=(8,-4,-1)$$ $$\nabla g(x,y,z)=(2,4,-1)\implies\nabla g(4,-2,20)=(2,4,-1)$$ The equation of tangent plane for $f$ is: $$8x-4y-z-20=0$$ The equation of tangent plane for $g$ is: $$2x+4y-z+20=0$$ In this step, i'm a little stuck, can someone help me?","Find the equation of tangent line to the intersection of the two surfaces in $P(4,-2,20)$ $$z=x^2+y^2 \,\,,\,z=2x+4y+20$$ Let $f,g:\mathbb{R}^3\rightarrow\mathbb{R}$ such that $f(x,y,z)=x^2+y^2-z\,\,\,$ and $g(x,y,z)=2x+4y-z+20$. Then, $$\nabla f(x,y,z)=(2x,2x,-1)\implies\nabla f(4,-2,20)=(8,-4,-1)$$ $$\nabla g(x,y,z)=(2,4,-1)\implies\nabla g(4,-2,20)=(2,4,-1)$$ The equation of tangent plane for $f$ is: $$8x-4y-z-20=0$$ The equation of tangent plane for $g$ is: $$2x+4y-z+20=0$$ In this step, i'm a little stuck, can someone help me?",,"['real-analysis', 'multivariable-calculus']"
96,"Evaluate the line integral $\int_C \mathbf F \cdot \, \mathrm d \mathbf r$",Evaluate the line integral,"\int_C \mathbf F \cdot \, \mathrm d \mathbf r","Evaluate the line integral $\int_C \mathbf F \cdot \, \mathrm d \mathbf r$ where, $\mathbf F = x \mathbf i+y \mathbf j + xy \mathbf k$ and C is parameterised by $\mathbf r(t)= \cos t \mathbf i+\sin t \mathbf j + t \mathbf k,t\in[0,\pi]$ Using $$\int_C \mathbf F \cdot \, \mathrm d \mathbf r = \int^b_a \mathbf F (\mathbf r(t))\mathbf r'(t) dt $$ I have begun to answer the question, this is my current unfinished solution: $$=\int^\pi_0 (\cos t\mathbf i + \sin t\mathbf j +\cos t \sin t\mathbf k)(-\sin t\mathbf i + \cos t\mathbf j + \mathbf k) dt$$ $$=\int^\pi_0(-\sin t \cos t+ \sin t \cos t+\sin t \cos t) dt$$ $$=\int^\pi_0(\sin t \cos t) dt$$ Not sure if this is correct up until this point.","Evaluate the line integral $\int_C \mathbf F \cdot \, \mathrm d \mathbf r$ where, $\mathbf F = x \mathbf i+y \mathbf j + xy \mathbf k$ and C is parameterised by $\mathbf r(t)= \cos t \mathbf i+\sin t \mathbf j + t \mathbf k,t\in[0,\pi]$ Using $$\int_C \mathbf F \cdot \, \mathrm d \mathbf r = \int^b_a \mathbf F (\mathbf r(t))\mathbf r'(t) dt $$ I have begun to answer the question, this is my current unfinished solution: $$=\int^\pi_0 (\cos t\mathbf i + \sin t\mathbf j +\cos t \sin t\mathbf k)(-\sin t\mathbf i + \cos t\mathbf j + \mathbf k) dt$$ $$=\int^\pi_0(-\sin t \cos t+ \sin t \cos t+\sin t \cos t) dt$$ $$=\int^\pi_0(\sin t \cos t) dt$$ Not sure if this is correct up until this point.",,"['calculus', 'multivariable-calculus', 'line-integrals']"
97,Example of multivalued function (includes pairwise comparisons) that attains maximum when values form evenly spaced vector,Example of multivalued function (includes pairwise comparisons) that attains maximum when values form evenly spaced vector,,"Suppose, that I have 4 points $x,y,z,w$ (positions on the x-axis), such that $x\le y\le z\le w$. I find pairwise distances between them: $y-x,z-x,w-x,z-y,w-y,w-z$. Distances should be less or equal to some constant $k$. Is there a function that contains all pairwise distances and attains the only maximum when $x,y,z,w$ are evenly spaced on the interval $[1, k+1]$ and the only minima when all variables are equal? The function shouldn't depend on $k$.","Suppose, that I have 4 points $x,y,z,w$ (positions on the x-axis), such that $x\le y\le z\le w$. I find pairwise distances between them: $y-x,z-x,w-x,z-y,w-y,w-z$. Distances should be less or equal to some constant $k$. Is there a function that contains all pairwise distances and attains the only maximum when $x,y,z,w$ are evenly spaced on the interval $[1, k+1]$ and the only minima when all variables are equal? The function shouldn't depend on $k$.",,"['calculus', 'multivariable-calculus', 'optimization', 'partial-derivative', 'nonlinear-optimization']"
98,"Show that if $c$ is a point of $\Bbb{R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution.","Show that if  is a point of  sufficiently close to , then the equation  has a solution.",c \Bbb{R}^n 0 f(x)=c,"Let $f:\Bbb{R}^{k+n}\xrightarrow{}\Bbb{R}^n$ be of Class $C^1$; suppose that $f(a)=0$ and that $Df(a)$ has rank $n$. Show that if $c$ is a point of $\Bbb{R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution. My attempt: Since, $Df(a)$ has full rank we know that the $\det Df(a) \ne 0$, as we all $f\in C^1$ and $f(a)=0$. So we can apply the implicit function theorem giving us an open neighbourhood $B$ of $a$. Such that $c\in B(0,\epsilon)$ then $f(x)=c$ must have a solution. How can I improve my answer?","Let $f:\Bbb{R}^{k+n}\xrightarrow{}\Bbb{R}^n$ be of Class $C^1$; suppose that $f(a)=0$ and that $Df(a)$ has rank $n$. Show that if $c$ is a point of $\Bbb{R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution. My attempt: Since, $Df(a)$ has full rank we know that the $\det Df(a) \ne 0$, as we all $f\in C^1$ and $f(a)=0$. So we can apply the implicit function theorem giving us an open neighbourhood $B$ of $a$. Such that $c\in B(0,\epsilon)$ then $f(x)=c$ must have a solution. How can I improve my answer?",,"['real-analysis', 'linear-algebra']"
99,"Visual resources like ""better explained"", but for higher math?","Visual resources like ""better explained"", but for higher math?",,"I've always been a very, very visual thinker (to the point that I suspect I have a mild autism) and basically only visual explanations of math make any sense. Of course, it is not taught that way in school! I stumbled across this site https://betterexplained.com/ and it basically changed my life. All the math I've ever truly understood was stored in my brain like one of Kalid Azad's articles.  I'm now furiously gobbling up anything like it. If anybody else shares this thinking style, what other resources helped you? I know you always have to sit and ""play with"" problems yourself, but I'm looking for books, videos, etc to assist this. Also to clarify, by ""higher math"" I mean basically anything you'd see in a typical mechanical engineering undergrad series (calc 3, differential equations, linear algebra etc). Geometry is fun, but it's also self explanatory for me. I'm looking for visual explanations of not explicitly visual math. The ""art of problem solving"" by Richard Rusczyk and the rest of that textbook series phrase things in a way that helps me make my own visuals/analogies even though there arent many diagrams. Also ""proofs without words"" by Roger Nelsen was helpful.","I've always been a very, very visual thinker (to the point that I suspect I have a mild autism) and basically only visual explanations of math make any sense. Of course, it is not taught that way in school! I stumbled across this site https://betterexplained.com/ and it basically changed my life. All the math I've ever truly understood was stored in my brain like one of Kalid Azad's articles.  I'm now furiously gobbling up anything like it. If anybody else shares this thinking style, what other resources helped you? I know you always have to sit and ""play with"" problems yourself, but I'm looking for books, videos, etc to assist this. Also to clarify, by ""higher math"" I mean basically anything you'd see in a typical mechanical engineering undergrad series (calc 3, differential equations, linear algebra etc). Geometry is fun, but it's also self explanatory for me. I'm looking for visual explanations of not explicitly visual math. The ""art of problem solving"" by Richard Rusczyk and the rest of that textbook series phrase things in a way that helps me make my own visuals/analogies even though there arent many diagrams. Also ""proofs without words"" by Roger Nelsen was helpful.",,"['linear-algebra', 'multivariable-calculus', 'soft-question']"
