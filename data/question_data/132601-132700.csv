,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to solve a 2D ODE system of the form $\frac{\text{d}\vec{x}}{\text{d}t}=(M-\Delta e^{-\lambda t})\vec{x}+\vec{x}_0$,How to solve a 2D ODE system of the form,\frac{\text{d}\vec{x}}{\text{d}t}=(M-\Delta e^{-\lambda t})\vec{x}+\vec{x}_0,"Let $x=x(t)$ and $y=y(t)$ for $t>0$ , with $x(0)=y(0)=1$ . \begin{align} \frac{\text{d}x}{\text{d}t} %%% &= %%% -\left( A + \alpha e^{-\lambda t}\right)x + By + x_0\\ %%% \frac{\text{d}y}{\text{d}t} %%% &= %%% Cx -\left( D + \beta e^{-\lambda t}\right)y + y_0 \end{align} where $A,B,C,D,\alpha,\beta,x_0,y_0\in \mathbb{R}_{>0}$ . We can rewrite this in the following vector format \begin{align} \frac{\text{d}\vec{x}}{\text{d}t} %%% &= %%% \left( M - \Delta e^{-\lambda t} \right) \vec{x} + \vec{x}_0 \end{align} where \begin{align} %%% \vec{x} =  \left(\begin{array}{c} x \\ y \end{array}\right),\qquad %%% M =  \left(\begin{array}{cc} -A  &  B \\  C  & -D \end{array}\right),\qquad %%% \Delta =  \left(\begin{array}{cc} \alpha & 0 \\ 0      & \beta \end{array}\right),\qquad %%% \vec{x}_0 =  \left(\begin{array}{c} x_0 \\ y_0 \end{array}\right) %%% \end{align} where $\vec{x}(0)=(1,1)^T$ . The only issue here, that separates it from a standard non-homogeneous linear system of ODEs ( see here ), is the presence of the exponential vector term. This 2D system is a reduction of a larger ODE system and this is as simple as I can get it. There does exist a small parameter ( $\alpha\simeq\beta\sim O(1/\varepsilon)$ , where $0<\varepsilon\ll1$ and all other terms are $O(1)$ ), however I would rather consider an exact solution as asymptotic analysis on similar systems as this has proved not useful past small/intermediate time in the parameter spaces of interest. If I make any progress on this I will add it below, if this system has been solved before I would really appreciate a reference, or a reference to a more general solution. EDIT 1 Ok, so thank you all for your help. I have consulted with my colleagues, discussed their ideas and your suggestions, we believe we have proven that you can't find a solution in terms of elementary functions, even in a reduced case (which is what I need). With the aid of some biological arguments I can restrict my parameter regimes of interest to ones that allow the following expression to be surprisingly accurate: $$y = \frac{1}{1+\gamma e^{-\lambda t}}$$ where $\gamma>0$ and can be expressed in terms of the other coefficients in this model. This reduces the system to a single ODE, as follows: \begin{align} \frac{\text{d}x}{\text{d}t} %%% &= %%% -\left( A + \alpha e^{-\lambda t}\right)x + \frac{B}{1+\gamma e^{-\lambda t}} + x_0 \end{align} with $x(0)=1$ . NOTE 1: If we define $\bar{M}$ as follows \begin{align} \bar{M} = \left(\begin{array}{cc} -(A-1)  &  B/\delta \\ \delta C  & -(D-1) \end{array}\right) \end{align} where $\delta\in\mathbb{R}_{>0}$ , then the eigenvalues of $\bar{M}$ are $-|\lambda|$ and $-|\mu|$ , where $0<\lambda\ll\mu$ . If we consider the similar system: \begin{align} \frac{\text{d}\vec{z}}{\text{d}t} %%% &= %%% \bar{M} \vec{z} \end{align} where $\vec{z}=(z_1(t),z_2(t))^T$ and $\vec{z}(0)=(0,1)^T$ . Then this system has approximate solution \begin{align} \vec{z} &\simeq \left(\begin{array}{c} \alpha \\ \beta \end{array}\right) e^{-\lambda t} \end{align} This solution does not hold for $t\ll1$ , as can be seen from the initial conditions, however for all intents and purposes it can be treated as exact. NOTE 2: Here is a plot using suitable parameter values:","Let and for , with . where . We can rewrite this in the following vector format where where . The only issue here, that separates it from a standard non-homogeneous linear system of ODEs ( see here ), is the presence of the exponential vector term. This 2D system is a reduction of a larger ODE system and this is as simple as I can get it. There does exist a small parameter ( , where and all other terms are ), however I would rather consider an exact solution as asymptotic analysis on similar systems as this has proved not useful past small/intermediate time in the parameter spaces of interest. If I make any progress on this I will add it below, if this system has been solved before I would really appreciate a reference, or a reference to a more general solution. EDIT 1 Ok, so thank you all for your help. I have consulted with my colleagues, discussed their ideas and your suggestions, we believe we have proven that you can't find a solution in terms of elementary functions, even in a reduced case (which is what I need). With the aid of some biological arguments I can restrict my parameter regimes of interest to ones that allow the following expression to be surprisingly accurate: where and can be expressed in terms of the other coefficients in this model. This reduces the system to a single ODE, as follows: with . NOTE 1: If we define as follows where , then the eigenvalues of are and , where . If we consider the similar system: where and . Then this system has approximate solution This solution does not hold for , as can be seen from the initial conditions, however for all intents and purposes it can be treated as exact. NOTE 2: Here is a plot using suitable parameter values:","x=x(t) y=y(t) t>0 x(0)=y(0)=1 \begin{align}
\frac{\text{d}x}{\text{d}t}
%%%
&=
%%%
-\left( A + \alpha e^{-\lambda t}\right)x + By + x_0\\
%%%
\frac{\text{d}y}{\text{d}t}
%%%
&=
%%%
Cx -\left( D + \beta e^{-\lambda t}\right)y + y_0
\end{align} A,B,C,D,\alpha,\beta,x_0,y_0\in \mathbb{R}_{>0} \begin{align}
\frac{\text{d}\vec{x}}{\text{d}t}
%%%
&=
%%%
\left(
M
-
\Delta e^{-\lambda t}
\right)
\vec{x}
+
\vec{x}_0
\end{align} \begin{align}
%%%
\vec{x} = 
\left(\begin{array}{c}
x \\
y
\end{array}\right),\qquad
%%%
M = 
\left(\begin{array}{cc}
-A  &  B \\
 C  & -D
\end{array}\right),\qquad
%%%
\Delta = 
\left(\begin{array}{cc}
\alpha & 0 \\
0      & \beta
\end{array}\right),\qquad
%%%
\vec{x}_0 = 
\left(\begin{array}{c}
x_0 \\
y_0
\end{array}\right)
%%%
\end{align} \vec{x}(0)=(1,1)^T \alpha\simeq\beta\sim O(1/\varepsilon) 0<\varepsilon\ll1 O(1) y = \frac{1}{1+\gamma e^{-\lambda t}} \gamma>0 \begin{align}
\frac{\text{d}x}{\text{d}t}
%%%
&=
%%%
-\left( A + \alpha e^{-\lambda t}\right)x + \frac{B}{1+\gamma e^{-\lambda t}} + x_0
\end{align} x(0)=1 \bar{M} \begin{align}
\bar{M} = \left(\begin{array}{cc}
-(A-1)  &  B/\delta \\
\delta C  & -(D-1)
\end{array}\right)
\end{align} \delta\in\mathbb{R}_{>0} \bar{M} -|\lambda| -|\mu| 0<\lambda\ll\mu \begin{align}
\frac{\text{d}\vec{z}}{\text{d}t}
%%%
&=
%%%
\bar{M}
\vec{z}
\end{align} \vec{z}=(z_1(t),z_2(t))^T \vec{z}(0)=(0,1)^T \begin{align}
\vec{z} &\simeq
\left(\begin{array}{c}
\alpha \\
\beta
\end{array}\right)
e^{-\lambda t}
\end{align} t\ll1","['ordinary-differential-equations', 'mathematical-modeling', 'nonlinear-system']"
1,"Relationship between $t$ and $x$ in $x'=f(t,x)$?",Relationship between  and  in ?,"t x x'=f(t,x)","I'm a couple weeks into a course on differential equations. I've been told that it's more accurate to think of a differential equation $x'=f(t,x)$ as being determined by two separate variables, $t$ and $x$. So, my understanding is that while it is true that for a particular solution of the DE, say $x(t)$, the dependence of $x$ on $t$ is evident, it is more accurate to view $x'=f(t,x)$ as a statement about two separate variables. Okay! So I'm holding $t$ and $x$ as distinct in my mind, and I go to solve a basic differential equation: $$x'=-2tx^2$$ $$-\frac{x'}{x^2}=2t$$ And it's at this point that I suddenly remember a fortuitous fact: There is an implicit dependence of $x$ on $t$, after all! This allows me to integrate with respect to $t$, appealing to the chain rule and the FTC I: If $F(x)$ is an antiderivative of $-\frac{1}{x^2},$ then $\frac{d}{dt}(F(x(t)))=-\frac{x'}{x^2}$ by the chain rule, and $\int{-\frac{x'}{x^2}dt}=\int{\frac{d}{dt}F(x(t))}=F(x(t))$ by FTC I (since $\frac{1}{x^2}$ is continuous everywhere it is defined). So, $$\int{-\frac{x'}{x^2}dt}=\int{2t dt}$$ $$\frac{1}{x}=t^2+C$$ $$x(t)=\frac{1}{t^2+C}.$$ It's only the very real dependence of $x$ on $t$ that allows me to use the chain rule and the FTC I to justify all that integrating. So, is the $x$ in $x'=f(t,x)$ truly a second variable? Another question is, Does it even matter? Like anyone, I can sometimes make mountains of molehills when I'm learning something new, and create difficulties for myself where none exists (as if things weren't difficult enough!). Is that what I'm doing here? Does it truly matter how I think of the relationship between $t$ and $x$ in the statement $x'=f(t,x)$?","I'm a couple weeks into a course on differential equations. I've been told that it's more accurate to think of a differential equation $x'=f(t,x)$ as being determined by two separate variables, $t$ and $x$. So, my understanding is that while it is true that for a particular solution of the DE, say $x(t)$, the dependence of $x$ on $t$ is evident, it is more accurate to view $x'=f(t,x)$ as a statement about two separate variables. Okay! So I'm holding $t$ and $x$ as distinct in my mind, and I go to solve a basic differential equation: $$x'=-2tx^2$$ $$-\frac{x'}{x^2}=2t$$ And it's at this point that I suddenly remember a fortuitous fact: There is an implicit dependence of $x$ on $t$, after all! This allows me to integrate with respect to $t$, appealing to the chain rule and the FTC I: If $F(x)$ is an antiderivative of $-\frac{1}{x^2},$ then $\frac{d}{dt}(F(x(t)))=-\frac{x'}{x^2}$ by the chain rule, and $\int{-\frac{x'}{x^2}dt}=\int{\frac{d}{dt}F(x(t))}=F(x(t))$ by FTC I (since $\frac{1}{x^2}$ is continuous everywhere it is defined). So, $$\int{-\frac{x'}{x^2}dt}=\int{2t dt}$$ $$\frac{1}{x}=t^2+C$$ $$x(t)=\frac{1}{t^2+C}.$$ It's only the very real dependence of $x$ on $t$ that allows me to use the chain rule and the FTC I to justify all that integrating. So, is the $x$ in $x'=f(t,x)$ truly a second variable? Another question is, Does it even matter? Like anyone, I can sometimes make mountains of molehills when I'm learning something new, and create difficulties for myself where none exists (as if things weren't difficult enough!). Is that what I'm doing here? Does it truly matter how I think of the relationship between $t$ and $x$ in the statement $x'=f(t,x)$?",,['ordinary-differential-equations']
2,Proving that the function $f(x) = \int_0^\infty \cos (w^3/3 - x w ) d w$ satisfies the equation $f'' + x f = 0$,Proving that the function  satisfies the equation,f(x) = \int_0^\infty \cos (w^3/3 - x w ) d w f'' + x f = 0,"It is assumed that $x$ is real. Formally, we have $$ f'' = \int_0^\infty  -\cos (w^3/3 - x w ) w^2  d w , $$ and hence $$f'' + x f  = \int_0^\infty  \cos (w^3/3 - x w ) (-w^2 + x )  d w \\ = -\int_0^\infty  \cos (w^3/3 - x w ) d(w^3/3 - x w ) \\ = - \sin(w^3/3- x w ) |_0^\infty .  $$ The problem is that $\sin(w^3/3- x w ) $ does not converges as $w\rightarrow \infty$. Apparently, the problem is rooted in the fact that the expression for $f''$ is not well defined---it does not converge. So, could anyone give a simple, elementary proof?","It is assumed that $x$ is real. Formally, we have $$ f'' = \int_0^\infty  -\cos (w^3/3 - x w ) w^2  d w , $$ and hence $$f'' + x f  = \int_0^\infty  \cos (w^3/3 - x w ) (-w^2 + x )  d w \\ = -\int_0^\infty  \cos (w^3/3 - x w ) d(w^3/3 - x w ) \\ = - \sin(w^3/3- x w ) |_0^\infty .  $$ The problem is that $\sin(w^3/3- x w ) $ does not converges as $w\rightarrow \infty$. Apparently, the problem is rooted in the fact that the expression for $f''$ is not well defined---it does not converge. So, could anyone give a simple, elementary proof?",,"['calculus', 'ordinary-differential-equations', 'special-functions']"
3,Studying ODEs... is it worth it?,Studying ODEs... is it worth it?,,"I am a first year Master's student in applied mathematics. I'm currently taking a Mathematical Physics class and we have been studying techniques for solving ODEs. There is some overlap with undergrad ODEs. Solving these problems (ex: $x^2y''+xy'-16y=8x^4$) often takes lots of time and lots of algebraic manipulation, all to find a solution that I won't use. I understand that it is important to become familiar with these equations and have an idea about how they are solved. But I am failing to see the big picture. How will knowing myriad solution techniques for these ODEs help my future research in applied mathematics? Have you found this knowledge useful in your research?","I am a first year Master's student in applied mathematics. I'm currently taking a Mathematical Physics class and we have been studying techniques for solving ODEs. There is some overlap with undergrad ODEs. Solving these problems (ex: $x^2y''+xy'-16y=8x^4$) often takes lots of time and lots of algebraic manipulation, all to find a solution that I won't use. I understand that it is important to become familiar with these equations and have an idea about how they are solved. But I am failing to see the big picture. How will knowing myriad solution techniques for these ODEs help my future research in applied mathematics? Have you found this knowledge useful in your research?",,"['ordinary-differential-equations', 'learning', 'research']"
4,"How to determine X,Y position from point P based on time, velocity and rate of turn?","How to determine X,Y position from point P based on time, velocity and rate of turn?",,"We need to figure out an estimated position along an arc using a set of known variables. The starting position on the arc would be called point P, here are the known/measured variables: P = initial position of point P x = x coordinate of P (longitude) y = y coordinate of P (latitude) V = velocity of point P (nautical miles / min, or nautical miles / hour) H = heading / directional angle of point P in degrees (000 = North, 180 = South) R = rate of turn of P in degrees/minute. All of those variables are known. We actually have a device that is able to calculate point P's rate of turn. See this link here for the device . We need to calculate the future X,Y positions of point P along the arc based on the variables above with respect to time. The dynamically changing variable to plug into the formula would be Time (in minutes or hours). For example: at future time T, what would be the position of P (or Px1, Py1) along the arc. My thinking is that it would require some kind of calculus based derivative calculation. Thanks.","We need to figure out an estimated position along an arc using a set of known variables. The starting position on the arc would be called point P, here are the known/measured variables: P = initial position of point P x = x coordinate of P (longitude) y = y coordinate of P (latitude) V = velocity of point P (nautical miles / min, or nautical miles / hour) H = heading / directional angle of point P in degrees (000 = North, 180 = South) R = rate of turn of P in degrees/minute. All of those variables are known. We actually have a device that is able to calculate point P's rate of turn. See this link here for the device . We need to calculate the future X,Y positions of point P along the arc based on the variables above with respect to time. The dynamically changing variable to plug into the formula would be Time (in minutes or hours). For example: at future time T, what would be the position of P (or Px1, Py1) along the arc. My thinking is that it would require some kind of calculus based derivative calculation. Thanks.",,"['calculus', 'ordinary-differential-equations', 'trigonometry', 'derivatives', 'arc-length']"
5,Solution to an annoying ODE,Solution to an annoying ODE,,"I'm very sorry if this is a duplicate, but searching for a specific equation is rather difficult. I have encountered the following ODE during my physics research (have a PDE which has a similarity solution), and am wondering whether there's any chance of obtaining an analytic solution to it: \begin{equation} x y' = -2(A(y) y')' ,\end{equation} for $y(x)$ with $y'=\frac{\mathrm{d}y}{\mathrm{d}x}$, just to be clear. $A(y) = 1-\zeta y(4-3y)$, for $\zeta \ge 0$, although I suspect that if we find ourselves invoking that we've probably already failed, so maybe treat $A(y)$ as being generic for now. I've fiddled around with it, but without much success. In terms of boundary conditions, well, they're up for grabs, was hoping the solution would tell me a bit more about them; physically $0 \le y \le 1$, and would probably like to prescribe Dirichlet conditions on either side of a finite domain. If it helps, I got to this equation using this: https://arxiv.org/pdf/0710.4000.pdf , around page 5. Of course, asymptotic solutions as $x\rightarrow 0$ or $x\rightarrow \pm \infty$ would also be very cool, as would advice about how to tackle it numerically if analytic solution attempts prove fruitless.","I'm very sorry if this is a duplicate, but searching for a specific equation is rather difficult. I have encountered the following ODE during my physics research (have a PDE which has a similarity solution), and am wondering whether there's any chance of obtaining an analytic solution to it: \begin{equation} x y' = -2(A(y) y')' ,\end{equation} for $y(x)$ with $y'=\frac{\mathrm{d}y}{\mathrm{d}x}$, just to be clear. $A(y) = 1-\zeta y(4-3y)$, for $\zeta \ge 0$, although I suspect that if we find ourselves invoking that we've probably already failed, so maybe treat $A(y)$ as being generic for now. I've fiddled around with it, but without much success. In terms of boundary conditions, well, they're up for grabs, was hoping the solution would tell me a bit more about them; physically $0 \le y \le 1$, and would probably like to prescribe Dirichlet conditions on either side of a finite domain. If it helps, I got to this equation using this: https://arxiv.org/pdf/0710.4000.pdf , around page 5. Of course, asymptotic solutions as $x\rightarrow 0$ or $x\rightarrow \pm \infty$ would also be very cool, as would advice about how to tackle it numerically if analytic solution attempts prove fruitless.",,"['calculus', 'analysis', 'ordinary-differential-equations']"
6,Find a superior alternative to Backward Euler method,Find a superior alternative to Backward Euler method,,"Currently, we are using the backward Euler (or implicit Euler) method for the solution of stiff ordinary differential equations during scientific computing. Assuming a quite performant computer hardware and an identical step size which is smaller than 100us. Are there other stable integration methods that are able to compute y(n+1) in just one time step (real-time) and have lower truncation errors? What are their pros and cons? I would like to implement the most promising ones and benchmark their results. External references: Numerical Solution of Ordinary Diﬀerential Equations One-Step Methods: Chapter 3.3 John Butcher´s Tutorials","Currently, we are using the backward Euler (or implicit Euler) method for the solution of stiff ordinary differential equations during scientific computing. Assuming a quite performant computer hardware and an identical step size which is smaller than 100us. Are there other stable integration methods that are able to compute y(n+1) in just one time step (real-time) and have lower truncation errors? What are their pros and cons? I would like to implement the most promising ones and benchmark their results. External references: Numerical Solution of Ordinary Diﬀerential Equations One-Step Methods: Chapter 3.3 John Butcher´s Tutorials",,"['ordinary-differential-equations', 'numerical-methods', 'algorithms', 'computational-mathematics', 'runge-kutta-methods']"
7,Using the Hartman-Grobman theorem on a polar system,Using the Hartman-Grobman theorem on a polar system,,"I have a system of differential equations, in polar form. It is quite simple this way, but rather ugly if I transform it into Cartesian coordinates. Is there any way to apply the Hartman-Grobman theorem without having to change coordinates? The equation is $$r' = r^3-r$$ $$\theta' = \sin(\theta)^2-\mu$$ Please DO NOT SOLVE THE PROBLEM FOR ME. I am just giving it so you know what kind of equation I am talking about. The goal is to analyze the five equilibria using Hartman-Grobman without changing coordinates. Please just give me hint(s).","I have a system of differential equations, in polar form. It is quite simple this way, but rather ugly if I transform it into Cartesian coordinates. Is there any way to apply the Hartman-Grobman theorem without having to change coordinates? The equation is $$r' = r^3-r$$ $$\theta' = \sin(\theta)^2-\mu$$ Please DO NOT SOLVE THE PROBLEM FOR ME. I am just giving it so you know what kind of equation I am talking about. The goal is to analyze the five equilibria using Hartman-Grobman without changing coordinates. Please just give me hint(s).",,['ordinary-differential-equations']
8,Find the orthogonal trajectories of the family of curves,Find the orthogonal trajectories of the family of curves,,"Find the orthogonal trajectories of the family of curves $y^5=kx^2$. I start by knowing that I need a differential equation satisfying all members of family, which means $k$ needs to be eliminated. Differentiating both sides with respect to $x$: \begin{align*} y^5 &= kx^2\\ 5y^4 \frac{dy}{dx} &= 2kx\quad\text{Replacing }k\\ 5y^4 \frac{dy}{dx} &= 2 \frac{y^5}{x^2}x\\ \frac{5}{y} \frac{dy}{dx} &= \frac{2}{x}\\ \frac{dy}{dx} &= \frac{2}{5} \frac{y}{x} \end{align*} Now I have a differential equation, and the orthogonal trajectory should be represented by the negative reciprocal. \begin{align*} \frac{dy}{dx}&=-\frac{5}{2}\frac{x}{y}\\ \int ydy&=\int-\frac{5}{2}xdx\\ y&=-\frac{5}{4}x^2+C \end{align*} This equation looks orthogonal to me if I plot it, but all the valid answers are of a higher order (e.g., $y^2+\frac{5}{2}x^2=C$). There must be a gap in my understanding somewhere. https://www.desmos.com/calculator/v6on063jfn","Find the orthogonal trajectories of the family of curves $y^5=kx^2$. I start by knowing that I need a differential equation satisfying all members of family, which means $k$ needs to be eliminated. Differentiating both sides with respect to $x$: \begin{align*} y^5 &= kx^2\\ 5y^4 \frac{dy}{dx} &= 2kx\quad\text{Replacing }k\\ 5y^4 \frac{dy}{dx} &= 2 \frac{y^5}{x^2}x\\ \frac{5}{y} \frac{dy}{dx} &= \frac{2}{x}\\ \frac{dy}{dx} &= \frac{2}{5} \frac{y}{x} \end{align*} Now I have a differential equation, and the orthogonal trajectory should be represented by the negative reciprocal. \begin{align*} \frac{dy}{dx}&=-\frac{5}{2}\frac{x}{y}\\ \int ydy&=\int-\frac{5}{2}xdx\\ y&=-\frac{5}{4}x^2+C \end{align*} This equation looks orthogonal to me if I plot it, but all the valid answers are of a higher order (e.g., $y^2+\frac{5}{2}x^2=C$). There must be a gap in my understanding somewhere. https://www.desmos.com/calculator/v6on063jfn",,"['calculus', 'ordinary-differential-equations']"
9,Two distinct solutions of $x'(t)=f(x(t))$ cannot intersect at any point.,Two distinct solutions of  cannot intersect at any point.,x'(t)=f(x(t)),"Let $f:\mathbb{R}^n \to \mathbb{R}^n $ be a continuously differentiable function. Prove that two distinct solutions of $x'(t)=f(x(t))$ (the ODE is autonomous) cannot intersect at any point, not even at different times. Notes: Clearly there is a relationship between this question and the theorems for the existence and uniqueness of ODEs. Theorem: Let A be an open subset of $\mathbb{R}^n$ and that $f \in C^1(A)$. Then $\forall x_0 \in A \ \exists \ \alpha>0$ such that the IVP $$x'(t)=f(x(t)), \ x(0)=x_0$$ has a unique solution $x(t)=x(t,x_0)$ on the interval $[-\alpha,\alpha]$. As $f \in C^1$ the theorem applies so the solutions to $x'(t)=f(x(t))$ are unique (they will not be in the same position at the same time). However I do not know how to show that the two solutions cannot intersect at later times.","Let $f:\mathbb{R}^n \to \mathbb{R}^n $ be a continuously differentiable function. Prove that two distinct solutions of $x'(t)=f(x(t))$ (the ODE is autonomous) cannot intersect at any point, not even at different times. Notes: Clearly there is a relationship between this question and the theorems for the existence and uniqueness of ODEs. Theorem: Let A be an open subset of $\mathbb{R}^n$ and that $f \in C^1(A)$. Then $\forall x_0 \in A \ \exists \ \alpha>0$ such that the IVP $$x'(t)=f(x(t)), \ x(0)=x_0$$ has a unique solution $x(t)=x(t,x_0)$ on the interval $[-\alpha,\alpha]$. As $f \in C^1$ the theorem applies so the solutions to $x'(t)=f(x(t))$ are unique (they will not be in the same position at the same time). However I do not know how to show that the two solutions cannot intersect at later times.",,"['ordinary-differential-equations', 'dynamical-systems']"
10,Darboux coordinate for contact geometry,Darboux coordinate for contact geometry,,"I'm reading Geiges' notes. ( https://arxiv.org/pdf/math/0307242.pdf ) In the proof of Theorem 2.44 on page 17, the existence of the contact version Darboux coordinate is reduced to solving $H_t$ for each $t$, the PDE near the origin of $\mathbb{R}^{2n+1}$  $$\dot{\alpha}_t (R_{\alpha_t})+dH_t(R_{\alpha_t} )= 0$$ where $\alpha_t$ is a $1$-parameter family of contact forms and $R_{\alpha_t}$ is the corresponding reeb vector field. And he said that this equation always has a solution by integration if the neighborhood is small enough so that $R_{\alpha_t}$ has no closed orbit. My question is why this is obvious? What I know is that this equation is a quasilinear first order PDE and can possibly be solved by the method of characteristics. But I can't find a reference that contains a clear statement when this kind of equation can be solved. Thank you.","I'm reading Geiges' notes. ( https://arxiv.org/pdf/math/0307242.pdf ) In the proof of Theorem 2.44 on page 17, the existence of the contact version Darboux coordinate is reduced to solving $H_t$ for each $t$, the PDE near the origin of $\mathbb{R}^{2n+1}$  $$\dot{\alpha}_t (R_{\alpha_t})+dH_t(R_{\alpha_t} )= 0$$ where $\alpha_t$ is a $1$-parameter family of contact forms and $R_{\alpha_t}$ is the corresponding reeb vector field. And he said that this equation always has a solution by integration if the neighborhood is small enough so that $R_{\alpha_t}$ has no closed orbit. My question is why this is obvious? What I know is that this equation is a quasilinear first order PDE and can possibly be solved by the method of characteristics. But I can't find a reference that contains a clear statement when this kind of equation can be solved. Thank you.",,"['ordinary-differential-equations', 'partial-differential-equations', 'differential-topology', 'symplectic-geometry', 'contact-topology']"
11,Abel differential equation with periodic coefficient,Abel differential equation with periodic coefficient,,"Consider differential equation $$y'=a_3(x)y^3+a_2(x)y^2+a_1(x)y+a_0(x)$$ where $a_i(x)$ is continuous and periodic with period $2\pi$, i=0, 1, 2, 3. Assume that $a_3(x)\ge0$ and $a_3(x)$ is not equal to 0 for all x. Prove that the equation has at most three different periodic solutions with period $2\pi$.","Consider differential equation $$y'=a_3(x)y^3+a_2(x)y^2+a_1(x)y+a_0(x)$$ where $a_i(x)$ is continuous and periodic with period $2\pi$, i=0, 1, 2, 3. Assume that $a_3(x)\ge0$ and $a_3(x)$ is not equal to 0 for all x. Prove that the equation has at most three different periodic solutions with period $2\pi$.",,['ordinary-differential-equations']
12,References for gradient systems,References for gradient systems,,"I am interested in the gradient system $$\dot{x}(t)=-\nabla f(x(t))$$ where $f:\mathbb{R}^n \to \mathbb{R}$ is a $C^{1,1}$ function (that is, a differentiable function whose gradient is Lipschitz continuous). I would be grateful if someone could propose some reference books related to this system.","I am interested in the gradient system $$\dot{x}(t)=-\nabla f(x(t))$$ where $f:\mathbb{R}^n \to \mathbb{R}$ is a $C^{1,1}$ function (that is, a differentiable function whose gradient is Lipschitz continuous). I would be grateful if someone could propose some reference books related to this system.",,"['ordinary-differential-equations', 'dynamical-systems', 'gradient-flows']"
13,Solving Laplace's equation in a sphere with mixed boundary conditions on the surface.,Solving Laplace's equation in a sphere with mixed boundary conditions on the surface.,,"Can anyone help point me to a solution method for this problem? Solve $C(\vec{x})$, where $\vec{x}=(r,\theta,\phi)$ on $\Omega=\{\vec{x}\in\mathbb{R}^3\ |\ r\in[0,R],\ \phi\in[0,2\pi),\ \theta\in[0,\pi)\}$, where $R>0$. We define the boundaries and regions within $\Omega$ as follows: \begin{align} \partial\Omega_1 &= % \{\vec{x}\in\mathbb{R}^3\ |\ r=R,\ \theta\in[0,\theta_1),\ \phi\in[0,2\pi)\}\\ % \partial\Omega_2 &= % \{\vec{x}\in\mathbb{R}^3\ |\ r=R,\ \theta\in[\theta_1,\theta_2),\ \phi\in[0,2\pi)\}\\ % \partial\Omega_3 &= % \{\vec{x}\in\mathbb{R}^3\ |\ r=R,\ \theta\in[\theta_2,\pi),\ \phi\in[0,2\pi)\} \end{align} $C(\vec{x})$ is governed by the diffusion equation within $\Omega$ with boundary conditions given below, \begin{align} % 0 &= \nabla^2 C % \qquad &\text{for}\ \vec{x}\in\Omega \\ % -\vec{n}\cdot\nabla C &= -\mu % \qquad &\text{for}\ \vec{x}\in\partial\Omega_1\\ % -\vec{n}\cdot\nabla C &= \sigma C % \qquad &\text{for}\ \vec{x}\in\partial\Omega_2\\ % -\vec{n}\cdot\nabla C &= 0 % \qquad &\text{for}\ \vec{x}\in\partial\Omega_3 \end{align} where $\mu,\sigma>0$. By symmetry the problem reduces to \begin{align}  0 =& % \frac{\partial }{\partial r}\left( r^2 \frac{\partial C}{\partial r} \right) % + \frac{1}{\sin{\theta}} \frac{\partial}{\partial \theta} \left( \sin{\theta} \frac{\partial C}{\partial \theta} \right) \end{align} With the same BC, however I can't find a solution method that does not cause the problem to become badly posed. EDIT: I have come across this paper by Mottin, I am unsure of its applicability here due to the piecewise definition of our Robin boundary condition. Does this invalidate the result of this paper?","Can anyone help point me to a solution method for this problem? Solve $C(\vec{x})$, where $\vec{x}=(r,\theta,\phi)$ on $\Omega=\{\vec{x}\in\mathbb{R}^3\ |\ r\in[0,R],\ \phi\in[0,2\pi),\ \theta\in[0,\pi)\}$, where $R>0$. We define the boundaries and regions within $\Omega$ as follows: \begin{align} \partial\Omega_1 &= % \{\vec{x}\in\mathbb{R}^3\ |\ r=R,\ \theta\in[0,\theta_1),\ \phi\in[0,2\pi)\}\\ % \partial\Omega_2 &= % \{\vec{x}\in\mathbb{R}^3\ |\ r=R,\ \theta\in[\theta_1,\theta_2),\ \phi\in[0,2\pi)\}\\ % \partial\Omega_3 &= % \{\vec{x}\in\mathbb{R}^3\ |\ r=R,\ \theta\in[\theta_2,\pi),\ \phi\in[0,2\pi)\} \end{align} $C(\vec{x})$ is governed by the diffusion equation within $\Omega$ with boundary conditions given below, \begin{align} % 0 &= \nabla^2 C % \qquad &\text{for}\ \vec{x}\in\Omega \\ % -\vec{n}\cdot\nabla C &= -\mu % \qquad &\text{for}\ \vec{x}\in\partial\Omega_1\\ % -\vec{n}\cdot\nabla C &= \sigma C % \qquad &\text{for}\ \vec{x}\in\partial\Omega_2\\ % -\vec{n}\cdot\nabla C &= 0 % \qquad &\text{for}\ \vec{x}\in\partial\Omega_3 \end{align} where $\mu,\sigma>0$. By symmetry the problem reduces to \begin{align}  0 =& % \frac{\partial }{\partial r}\left( r^2 \frac{\partial C}{\partial r} \right) % + \frac{1}{\sin{\theta}} \frac{\partial}{\partial \theta} \left( \sin{\theta} \frac{\partial C}{\partial \theta} \right) \end{align} With the same BC, however I can't find a solution method that does not cause the problem to become badly posed. EDIT: I have come across this paper by Mottin, I am unsure of its applicability here due to the piecewise definition of our Robin boundary condition. Does this invalidate the result of this paper?",,"['ordinary-differential-equations', 'partial-differential-equations', 'spherical-geometry', 'laplacian']"
14,Stability of limit cycle associated with a homogenous linear quation,Stability of limit cycle associated with a homogenous linear quation,,"Study the stability of the limit cycle r=1 for the system given in polar coordinates by the equations  $\dot{r}=(r^2−1)(2x−1), \dot{\phi}=1$, where $x=r\cos \phi$. I've been trying to solve this problem by estimating the return function, but haven't made any progress. Can anyone give me some hints?","Study the stability of the limit cycle r=1 for the system given in polar coordinates by the equations  $\dot{r}=(r^2−1)(2x−1), \dot{\phi}=1$, where $x=r\cos \phi$. I've been trying to solve this problem by estimating the return function, but haven't made any progress. Can anyone give me some hints?",,"['ordinary-differential-equations', 'homogeneous-equation']"
15,Solutions intervals of a differential equation,Solutions intervals of a differential equation,,"I cannot understand the difference between these two ODEs in terms of the intervals in which solutions are defined $y' y=(x+1)$ $y'=\frac{x+1}{y}$ The equation is actually the same but in the first case for $y=0$ I get $x=-1$ and I don't see what is the problem in that point, while in the second one it is necessary to impose $y\neq 0$ of course. What does that mean? Are the solutions different for the two equations? If yes, in what do they differ? Thanks a lot for your help","I cannot understand the difference between these two ODEs in terms of the intervals in which solutions are defined $y' y=(x+1)$ $y'=\frac{x+1}{y}$ The equation is actually the same but in the first case for $y=0$ I get $x=-1$ and I don't see what is the problem in that point, while in the second one it is necessary to impose $y\neq 0$ of course. What does that mean? Are the solutions different for the two equations? If yes, in what do they differ? Thanks a lot for your help",,"['calculus', 'ordinary-differential-equations']"
16,Qualitative behavior of critical point at the origin,Qualitative behavior of critical point at the origin,,"Determine the qualitative behavior of the critical point at the origin for the following system for all possible values of $a$: $\dot{x} = -y + ax(x^2+y^2)$ $\dot{y} = x + ay(x^2+y^2)$ My question: I attempted to use the Local Center Manifold theorem to show that the center manifold: $x = h(y) = a_0 + a_1y+a_2y^2 +...$ for $a_0, a_1, ...$ are parameters to be determined and $h(0) = h'(0) = 0$, must be $0$. To do this, assume $h(y)\neq 0$ for $y\neq 0$. Now, we replace $x$ by $h(y)$ from the sysem above, and from the identity: $\dot{x} = \dot{y}\  h'(y)$, we get the following equation for all values of $a$: $-y + a(a_0 + a_1y+ a_2y^2 + ...) (a_0^2+a_1^2y^2 + 2a_0a_1y+2a_0a_2y^2 +...+y^2) = (a_1 + 2a_2y + 3a_3y^2 + ...)[a_0 + a_1y+ a_2y^2 + ... + ay(a_0^2 + a_1^2y^2 + 2a_0a_1y + 2a_0a_2y^2 + ... +y^2)]$ Since $h(0) = h'(0) = 0$, we instantly get $a_0 = a_1 = 0$. But then the $-y$ term on the LHS of the equation above is never cancelled with anything, so the equation, after matching terms by terms, cannot be true for every $y\neq 0$. Thus $h(y)$ does not exist in this case. Therefore, $h(y) = 0$ is the only choice, which implies $x = 0$. But if this is the case, then $0 = -y$, so $y = 0$ as well. Thus the critical point is a saddle point? Is this a correct conclusion?","Determine the qualitative behavior of the critical point at the origin for the following system for all possible values of $a$: $\dot{x} = -y + ax(x^2+y^2)$ $\dot{y} = x + ay(x^2+y^2)$ My question: I attempted to use the Local Center Manifold theorem to show that the center manifold: $x = h(y) = a_0 + a_1y+a_2y^2 +...$ for $a_0, a_1, ...$ are parameters to be determined and $h(0) = h'(0) = 0$, must be $0$. To do this, assume $h(y)\neq 0$ for $y\neq 0$. Now, we replace $x$ by $h(y)$ from the sysem above, and from the identity: $\dot{x} = \dot{y}\  h'(y)$, we get the following equation for all values of $a$: $-y + a(a_0 + a_1y+ a_2y^2 + ...) (a_0^2+a_1^2y^2 + 2a_0a_1y+2a_0a_2y^2 +...+y^2) = (a_1 + 2a_2y + 3a_3y^2 + ...)[a_0 + a_1y+ a_2y^2 + ... + ay(a_0^2 + a_1^2y^2 + 2a_0a_1y + 2a_0a_2y^2 + ... +y^2)]$ Since $h(0) = h'(0) = 0$, we instantly get $a_0 = a_1 = 0$. But then the $-y$ term on the LHS of the equation above is never cancelled with anything, so the equation, after matching terms by terms, cannot be true for every $y\neq 0$. Thus $h(y)$ does not exist in this case. Therefore, $h(y) = 0$ is the only choice, which implies $x = 0$. But if this is the case, then $0 = -y$, so $y = 0$ as well. Thus the critical point is a saddle point? Is this a correct conclusion?",,['ordinary-differential-equations']
17,Solve $f ' (x) + f '' (x)/2 = \sqrt f(x)$,Solve,f ' (x) + f '' (x)/2 = \sqrt f(x),How to solve the differential equation $$f ' (x) + f '' (x)/2 = \sqrt {f(x)}$$ Edit My efforts Assume $f(x) = a x^2 + b x + c$. Then we plug this into the differential equation $2 a x + b + a = \sqrt (a x^2 + b x + c)$. I assume that $b = 2 \sqrt a \sqrt c$ So that i can simplify the RHS. Therefore $2 a x + a + 2 \sqrt a \sqrt c = \sqrt a x + \sqrt c$. When i solve this system of equations i get $a= c = 0$ ( trivial solution ) Or $a = 1/4 $ and $c = \infty $ A pretty useless result. ( if $ a = 1/4 $ then $a + \sqrt c = \sqrt c $ hence the weird result ). This attempt was not succesful.,How to solve the differential equation $$f ' (x) + f '' (x)/2 = \sqrt {f(x)}$$ Edit My efforts Assume $f(x) = a x^2 + b x + c$. Then we plug this into the differential equation $2 a x + b + a = \sqrt (a x^2 + b x + c)$. I assume that $b = 2 \sqrt a \sqrt c$ So that i can simplify the RHS. Therefore $2 a x + a + 2 \sqrt a \sqrt c = \sqrt a x + \sqrt c$. When i solve this system of equations i get $a= c = 0$ ( trivial solution ) Or $a = 1/4 $ and $c = \infty $ A pretty useless result. ( if $ a = 1/4 $ then $a + \sqrt c = \sqrt c $ hence the weird result ). This attempt was not succesful.,,['ordinary-differential-equations']
18,Online PDE course,Online PDE course,,"I want to teach myself PDE . I am asking here because I am mainly interested in applications in electromagnetism / optics. I would like to have a nice series of online lectures. I would also appreciate some recommendation on a textbook, for which solution manual can be found somewhere.","I want to teach myself PDE . I am asking here because I am mainly interested in applications in electromagnetism / optics. I would like to have a nice series of online lectures. I would also appreciate some recommendation on a textbook, for which solution manual can be found somewhere.",,['ordinary-differential-equations']
19,Technique for predicting attractor capture in nonlinear differential equations? (quasi-pendulum equation),Technique for predicting attractor capture in nonlinear differential equations? (quasi-pendulum equation),,"I'm working on simulating this equation (application is motor control, not that it matters): $$\frac{d^2\theta}{dt^2}+b\frac{d\theta}{dt}=a \sin (x-\theta)$$ where $x = vt$ for $t > 0$, and I'm finding that for given initial conditions $\frac{d\theta}{dt}|_{t=0}$ and $\theta|_{t=0}$, there seems to be a critical value $v_{crit}$ such that: if $v < v_{crit}$, $x-\theta$ oscillates but settles down to its equilibrium value $\phi = \sin^{-1} \frac{bv}{a}$ (""capture"") if $v > v_{crit}$, $x-\theta$ tends to a linearly increasing difference + a small oscillating term. Predicting the value of $v_{crit}$ is important in my application, and I would like to understand what is going on. My training in nonlinear differential equations is rather limited and rusty, and I never took advanced classes... I think there might be some insight using energy techniques (Hamiltonians? Lyapunov stability ?) since there is similarity to a driven pendulum equation ($\frac{d^2\theta}{dt^2}+\frac{g}{l}\sin \theta = u(t)$), but I can't figure out what. How can I figure out this critical value? Can anyone point me towards some reference material (or even the right terms to look up) so I could learn a technique to solve my problem? It seems like if I can show that $|x-\theta|$ reverses direction before it hits $\pi$, then capture is guaranteed. okay, splitting into 1st-order systems: $\begin{eqnarray} \dot{\omega} &=& -a \sin(\theta - x) - b\omega \cr \dot{\theta} &=& \omega \end{eqnarray}$ Change of variable $u = \theta - x$ so $\dot{u} = \omega - v$ and $\ddot{u} = \ddot{\theta} = \dot{\omega}$: $\begin{eqnarray} \dot{\omega} &=& -a \sin u - b\omega \cr \dot{u} &=& \omega - v \end{eqnarray}$ If I try to write a Lyapunov equation $E = c\omega^2 + d \cos u$ I get $$\begin{eqnarray} \dot{E} &=& 2c\dot{\omega}\omega - d \dot{u} \sin u \cr &=&2c\omega(-a\sin u - b\omega) - d (\omega - v) \sin u \cr &=&\omega \sin u (-2ac - d) -2bc\omega^2 + dv \sin u  \end{eqnarray} $$ I can make the first term go away if I choose d=-2ac; the second term is negative if $c>0$ but I can't get rid of the third term. Attempt #2: $E = (\omega-v)^2 - 2a \cos u$ I get $$\begin{eqnarray} \dot{E} &=& 2\dot{\omega}(\omega-v) +2a \dot{u} \sin u \cr &=&2(\omega-v)(-a\sin u - b\omega) +2a (\omega - v) \sin u \cr &=&(\omega-v) \sin u (-2a + 2a) -2b\omega(\omega-v) \cr &=& -2b\omega(\omega-v) \cr  &=& -2b\left(\left(\omega-\frac{v}{2}\right)^2 - \frac{v^2}{4}\right) \cr  \end{eqnarray} $$ but that's not necessarily negative. Urk. Some numerical sample points: I'm using a = 2.3086177e5, b = 1.78179103 (max v with equilibrium at $a/b \approx 129567$, but $v_{crit}$ tends to be much smaller in practice), and in my simulations I'm seeing: $\omega|_{t=0} = 0, \theta|_{t=0} = 0 : v_{crit} \approx 958.929$ $\omega|_{t=0} = 0, \theta|_{t=0} = 0.5 : v_{crit} \approx 929.408$ $\omega|_{t=0} = 0, \theta|_{t=0} = 1.0 : v_{crit} \approx 842.336$ $\omega|_{t=0} = 0, \theta|_{t=0} = \pi/2 : v_{crit} \approx 680.156$ For the last case, here's a phase plot (it's a very underdamped system so the turns of the plotted curve come very close together; the cusp on the left is where things slow down for a moment): and a timeseries plot:","I'm working on simulating this equation (application is motor control, not that it matters): $$\frac{d^2\theta}{dt^2}+b\frac{d\theta}{dt}=a \sin (x-\theta)$$ where $x = vt$ for $t > 0$, and I'm finding that for given initial conditions $\frac{d\theta}{dt}|_{t=0}$ and $\theta|_{t=0}$, there seems to be a critical value $v_{crit}$ such that: if $v < v_{crit}$, $x-\theta$ oscillates but settles down to its equilibrium value $\phi = \sin^{-1} \frac{bv}{a}$ (""capture"") if $v > v_{crit}$, $x-\theta$ tends to a linearly increasing difference + a small oscillating term. Predicting the value of $v_{crit}$ is important in my application, and I would like to understand what is going on. My training in nonlinear differential equations is rather limited and rusty, and I never took advanced classes... I think there might be some insight using energy techniques (Hamiltonians? Lyapunov stability ?) since there is similarity to a driven pendulum equation ($\frac{d^2\theta}{dt^2}+\frac{g}{l}\sin \theta = u(t)$), but I can't figure out what. How can I figure out this critical value? Can anyone point me towards some reference material (or even the right terms to look up) so I could learn a technique to solve my problem? It seems like if I can show that $|x-\theta|$ reverses direction before it hits $\pi$, then capture is guaranteed. okay, splitting into 1st-order systems: $\begin{eqnarray} \dot{\omega} &=& -a \sin(\theta - x) - b\omega \cr \dot{\theta} &=& \omega \end{eqnarray}$ Change of variable $u = \theta - x$ so $\dot{u} = \omega - v$ and $\ddot{u} = \ddot{\theta} = \dot{\omega}$: $\begin{eqnarray} \dot{\omega} &=& -a \sin u - b\omega \cr \dot{u} &=& \omega - v \end{eqnarray}$ If I try to write a Lyapunov equation $E = c\omega^2 + d \cos u$ I get $$\begin{eqnarray} \dot{E} &=& 2c\dot{\omega}\omega - d \dot{u} \sin u \cr &=&2c\omega(-a\sin u - b\omega) - d (\omega - v) \sin u \cr &=&\omega \sin u (-2ac - d) -2bc\omega^2 + dv \sin u  \end{eqnarray} $$ I can make the first term go away if I choose d=-2ac; the second term is negative if $c>0$ but I can't get rid of the third term. Attempt #2: $E = (\omega-v)^2 - 2a \cos u$ I get $$\begin{eqnarray} \dot{E} &=& 2\dot{\omega}(\omega-v) +2a \dot{u} \sin u \cr &=&2(\omega-v)(-a\sin u - b\omega) +2a (\omega - v) \sin u \cr &=&(\omega-v) \sin u (-2a + 2a) -2b\omega(\omega-v) \cr &=& -2b\omega(\omega-v) \cr  &=& -2b\left(\left(\omega-\frac{v}{2}\right)^2 - \frac{v^2}{4}\right) \cr  \end{eqnarray} $$ but that's not necessarily negative. Urk. Some numerical sample points: I'm using a = 2.3086177e5, b = 1.78179103 (max v with equilibrium at $a/b \approx 129567$, but $v_{crit}$ tends to be much smaller in practice), and in my simulations I'm seeing: $\omega|_{t=0} = 0, \theta|_{t=0} = 0 : v_{crit} \approx 958.929$ $\omega|_{t=0} = 0, \theta|_{t=0} = 0.5 : v_{crit} \approx 929.408$ $\omega|_{t=0} = 0, \theta|_{t=0} = 1.0 : v_{crit} \approx 842.336$ $\omega|_{t=0} = 0, \theta|_{t=0} = \pi/2 : v_{crit} \approx 680.156$ For the last case, here's a phase plot (it's a very underdamped system so the turns of the plotted curve come very close together; the cusp on the left is where things slow down for a moment): and a timeseries plot:",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
20,Solve the PDE: $u_{xx} - 3u_{xt} - 4u_{tt} = 0$ [duplicate],Solve the PDE:  [duplicate],u_{xx} - 3u_{xt} - 4u_{tt} = 0,"This question already has answers here : Solve $u_{xx}-3u_{xt}-4u_{tt}=0$ where $u(x,0)=x^2$ and $u_t(x,0)=e^x$ (2 answers) Closed 6 years ago . It is asked to solve the PDE $$u_{xx} - 3u_{xt} - 4u_{tt} = 0$$ using a factorization, that consists in $$\left( \frac{\partial}{\partial x} - 4 \frac{\partial}{\partial t} \right) \left( \frac{\partial}{\partial x} + \frac{\partial}{\partial t} \right) u = 0$$ My attempt: I called $$\left( \frac{\partial}{\partial x} + \frac{\partial}{\partial t} \right) = v$$  and solved  $$ v_x - 4 v_t = 0$$ making the change of variables: $x' = x-4t, \; t' = -4x - t$.  This change of variables leave us with the following ""ODE"": $$17 u_{x'} = 0 \Rightarrow u(x,t) = f(t') = f(-4x-t) = g(4x+t)$$ where g and f are arbitrary functions of one variable For the second part, we have the PDE: $$u_x+u_t = g(4x+t)$$ I made the following change of variables: $x' = x+t, t' = x-t$, and this leave us with the following ""ODE"": $$2u_{x'} = g(4x+t)$$ The answer is  $$ u(x,t) = c_1(4x+t)+c_2(x-t)$$ I understand that the last ""ODE"" give us the solution $$u(x,t) = \int g(4x+t) dx' + c_2(x-t)$$ but why is the integral also a function of 4x+t? Where I commited a mistake? Thanks in advance! @Edit: Please, don't give me a solution using another technique that is not a factorization. I think my mistake, if there is one, consists on the variables that I am choosing.","This question already has answers here : Solve $u_{xx}-3u_{xt}-4u_{tt}=0$ where $u(x,0)=x^2$ and $u_t(x,0)=e^x$ (2 answers) Closed 6 years ago . It is asked to solve the PDE $$u_{xx} - 3u_{xt} - 4u_{tt} = 0$$ using a factorization, that consists in $$\left( \frac{\partial}{\partial x} - 4 \frac{\partial}{\partial t} \right) \left( \frac{\partial}{\partial x} + \frac{\partial}{\partial t} \right) u = 0$$ My attempt: I called $$\left( \frac{\partial}{\partial x} + \frac{\partial}{\partial t} \right) = v$$  and solved  $$ v_x - 4 v_t = 0$$ making the change of variables: $x' = x-4t, \; t' = -4x - t$.  This change of variables leave us with the following ""ODE"": $$17 u_{x'} = 0 \Rightarrow u(x,t) = f(t') = f(-4x-t) = g(4x+t)$$ where g and f are arbitrary functions of one variable For the second part, we have the PDE: $$u_x+u_t = g(4x+t)$$ I made the following change of variables: $x' = x+t, t' = x-t$, and this leave us with the following ""ODE"": $$2u_{x'} = g(4x+t)$$ The answer is  $$ u(x,t) = c_1(4x+t)+c_2(x-t)$$ I understand that the last ""ODE"" give us the solution $$u(x,t) = \int g(4x+t) dx' + c_2(x-t)$$ but why is the integral also a function of 4x+t? Where I commited a mistake? Thanks in advance! @Edit: Please, don't give me a solution using another technique that is not a factorization. I think my mistake, if there is one, consists on the variables that I am choosing.",,"['ordinary-differential-equations', 'partial-differential-equations']"
21,Differential equation with shifited term,Differential equation with shifited term,,"I have a differential equation (Or integral equation) of the form: $$ f(x) = a e^{-x} + b \int_0^x f(cz+dx) e^{-z} dz$$ $a,b,c,d$ are constants. I am considering whether the above equation has a closed form solution. If not, why it is the case? If so, I think guessing a functional form and using method of undetermined coefficients. But I am not sure how to guess. Thanks so much!","I have a differential equation (Or integral equation) of the form: $$ f(x) = a e^{-x} + b \int_0^x f(cz+dx) e^{-z} dz$$ $a,b,c,d$ are constants. I am considering whether the above equation has a closed form solution. If not, why it is the case? If so, I think guessing a functional form and using method of undetermined coefficients. But I am not sure how to guess. Thanks so much!",,"['calculus', 'ordinary-differential-equations', 'integral-equations', 'delay-differential-equations']"
22,Finding the frequencies of vibration of a drum; PDE,Finding the frequencies of vibration of a drum; PDE,,"I want to find the frequencies of vibration of a circular and square drum.  To do this, I need to solve a 2-dimensional wave equation (PDE) with boundary conditions. Every method that I have researched to solve this uses separation of variables to generate two ODE's, and this is referred to as the ""eigenvalue problem"".  My question is, is there any other way to do this that does not use separation of variables? I want to find approximate eigenvalues using a numerical method (like power method) but not sure if it is possible. Any information would be appreciated","I want to find the frequencies of vibration of a circular and square drum.  To do this, I need to solve a 2-dimensional wave equation (PDE) with boundary conditions. Every method that I have researched to solve this uses separation of variables to generate two ODE's, and this is referred to as the ""eigenvalue problem"".  My question is, is there any other way to do this that does not use separation of variables? I want to find approximate eigenvalues using a numerical method (like power method) but not sure if it is possible. Any information would be appreciated",,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'boundary-value-problem']"
23,System of 3 differential equations,System of 3 differential equations,,I'm trying to solve this system $$ \begin{align} x'&=x-3y+3z\\ y'&=-2x-6y+13z\\ z'&=-x-4y+8z \end{align} $$ must be reduced to a single equation I tried to express the x 3 and substitute in the other two but then I have not reduced y or z I can not understand how to solve it Euler method can not be solved because it is written in the job to reduce to a single equation,I'm trying to solve this system $$ \begin{align} x'&=x-3y+3z\\ y'&=-2x-6y+13z\\ z'&=-x-4y+8z \end{align} $$ must be reduced to a single equation I tried to express the x 3 and substitute in the other two but then I have not reduced y or z I can not understand how to solve it Euler method can not be solved because it is written in the job to reduce to a single equation,,"['ordinary-differential-equations', 'systems-of-equations']"
24,Find the particular solution of $y''+y=\cos(t)\cos(2t)$,Find the particular solution of,y''+y=\cos(t)\cos(2t),"Find the particular solution of $L[y]=y''+y=\cos(t)\cos(2t)$ Here my steps: Homogeneous: $y''+y=0$ Roots $+i$ and $-i$ General solution : $y(t)=c_1\cos(t)+c_2\sin(t)+\psi(t)$ Since we are dealing with cosine then we can rewrite the right hand side as: $y''+y=e^{it}e^{2it}=e^{{it}(1+2)}v$ Now to guess: Let: $\psi=e^{{it}(1+2)}v$ $\psi'=e^{{it}(1+2)}v'+3ie^{{it}(1+2)}v$ $\psi''=e^{{it}(1+2)}v''+3ie^{{it}(1+2)}v'+3ie^{{it}(1+2)}v'-9e^{{it}(1+2)}v$ Plug back into $L[y]$: $L[y]=e^{{it}(1+2)}v''+6ie^{{it}(1+2)}v'-9e^{{it}(1+2)}v+e^{{it}(1+2)}v=e^{{it}(1+2)}v$ Dividing each side by $e^{{it}(1+2)}$ We obtain: $L[y]=v''+6iv'-9v+v=v''+6iv'-8v=1$ Let : $v(t)=a_0+a_1t+a_2t^2$ $v'(t)=a_1+2ta_2$ $v''(t)=2a_2$ Plug back into: $L[y]=v''+6iv'-9v+v=v''+6iv'-8v=1$ $L[y]=2a_2+6ia_1+12ita_2-8a_0-8a_1t-8a_2t^2=1$ Equating coefficients: $$2a_2+6ia_1-8a_0=1$$ $$ 12ia_2-8a_1=0$$ $$-8a_2=0$$ from there I got: $$ a_2=0$$ $$ a_1=0$$ $$a_0=\frac{-1}{8}$$ Plugging back into $v(t)$ $$v(t)=\frac{-1}{8}$$ Plugging back into $\psi=e^{{it}(1+2)}v$ $$\psi=e^{{it}(1+2)}\frac{-1}{8}$$ Replacing the exponential with sine and cosines. The particular solution must be contain real values since the right hand side has cosine. $$\psi=[\cos(t)+i\sin(t)][\cos(2t)+i\sin(2t)]\frac{-1}{8}$$ I only multiplied the real parts: $$\psi=\frac{-1}{8}[\cos(t)\cos(2t)-\sin(t)\sin(2t)]$$ The inside looks like the sum and angle formula so we have: $$\psi=\frac{-1}{8}\cos(2t+t)$$ but sadly, I went wrong since the answer key says: $\psi(t)=-\frac{1}{16}\cos(3t)+\frac{1}{4}t\sin(t)$ Honestly, I'm not sure where I went wrong","Find the particular solution of $L[y]=y''+y=\cos(t)\cos(2t)$ Here my steps: Homogeneous: $y''+y=0$ Roots $+i$ and $-i$ General solution : $y(t)=c_1\cos(t)+c_2\sin(t)+\psi(t)$ Since we are dealing with cosine then we can rewrite the right hand side as: $y''+y=e^{it}e^{2it}=e^{{it}(1+2)}v$ Now to guess: Let: $\psi=e^{{it}(1+2)}v$ $\psi'=e^{{it}(1+2)}v'+3ie^{{it}(1+2)}v$ $\psi''=e^{{it}(1+2)}v''+3ie^{{it}(1+2)}v'+3ie^{{it}(1+2)}v'-9e^{{it}(1+2)}v$ Plug back into $L[y]$: $L[y]=e^{{it}(1+2)}v''+6ie^{{it}(1+2)}v'-9e^{{it}(1+2)}v+e^{{it}(1+2)}v=e^{{it}(1+2)}v$ Dividing each side by $e^{{it}(1+2)}$ We obtain: $L[y]=v''+6iv'-9v+v=v''+6iv'-8v=1$ Let : $v(t)=a_0+a_1t+a_2t^2$ $v'(t)=a_1+2ta_2$ $v''(t)=2a_2$ Plug back into: $L[y]=v''+6iv'-9v+v=v''+6iv'-8v=1$ $L[y]=2a_2+6ia_1+12ita_2-8a_0-8a_1t-8a_2t^2=1$ Equating coefficients: $$2a_2+6ia_1-8a_0=1$$ $$ 12ia_2-8a_1=0$$ $$-8a_2=0$$ from there I got: $$ a_2=0$$ $$ a_1=0$$ $$a_0=\frac{-1}{8}$$ Plugging back into $v(t)$ $$v(t)=\frac{-1}{8}$$ Plugging back into $\psi=e^{{it}(1+2)}v$ $$\psi=e^{{it}(1+2)}\frac{-1}{8}$$ Replacing the exponential with sine and cosines. The particular solution must be contain real values since the right hand side has cosine. $$\psi=[\cos(t)+i\sin(t)][\cos(2t)+i\sin(2t)]\frac{-1}{8}$$ I only multiplied the real parts: $$\psi=\frac{-1}{8}[\cos(t)\cos(2t)-\sin(t)\sin(2t)]$$ The inside looks like the sum and angle formula so we have: $$\psi=\frac{-1}{8}\cos(2t+t)$$ but sadly, I went wrong since the answer key says: $\psi(t)=-\frac{1}{16}\cos(3t)+\frac{1}{4}t\sin(t)$ Honestly, I'm not sure where I went wrong",,['ordinary-differential-equations']
25,$\frac{d^2y}{dt^2}+4y=t\sin(2t)$'s particular solution,'s particular solution,\frac{d^2y}{dt^2}+4y=t\sin(2t),"Finding the particular solution of : $\frac{d^2y}{dt^2}+4y=t\sin(2t)$ Hey everyone! My professor recently went over this problem and I can't seem to find where he derived a particular equation. Hopefully, someone can help me out with that equation. I shall indicate when I explain below: We have: $$\frac{d^2y}{dt^2}+4y=t\sin(2t)$$ which can be rewritten as: $$\frac{d^2y}{dt^2}+4y=te^{2it}$$ Next, $$ a=1, b=0, c=4, \alpha=2i$$ Set $y=e^{2it} v$ Now my professor said this: $$ v''+4iv'=t$$ and I have no idea where he obtained that from. I shall show the rest   of the problem. Guess=polynomial and since there is no v term, the highest power is 2. $$v=a_1t+a_2t^2$$ $$v'=a_1+2a_2t$$ $$v''=2a_2$$ Plugging back into $ v''+4iv'=t$, we have: $$ 2a_2+4ia_1+8ia_2t=t$$ Equating: $$ 8ia_2=1 \rightarrow a_2=\frac{-i}{8}$$ $$ 2_a2+4ia_1=0 \rightarrow a_1=\frac{1}{16}$$ Plugging back into v(t): $$v(t)=\frac{1}{16}t-\frac{i}{8}t^2$$ Putting it all together: $$y=e^{2it}[\frac{1}{16}t-\frac{i}{8}t^2]$$ $$y=[\cos(2t)+i\sin(2t)][\frac{1}{16}t-\frac{i}{8}t^2]$$ Take note how the original problem says $\sin(2t)$, therefore our answer is the imaginary part which means the particular solution is: $$\psi(t)=\frac{t}{16}\sin(2t)-\frac{t^2}{8}\cos(2t)$$ I hope someone can help me find the part above. My professor mention there is two ways of solving this problem and normally I use another way but when I realize my professor's solution is much shorter, I wanted to learn his way but I'm just stuck with the part above.","Finding the particular solution of : $\frac{d^2y}{dt^2}+4y=t\sin(2t)$ Hey everyone! My professor recently went over this problem and I can't seem to find where he derived a particular equation. Hopefully, someone can help me out with that equation. I shall indicate when I explain below: We have: $$\frac{d^2y}{dt^2}+4y=t\sin(2t)$$ which can be rewritten as: $$\frac{d^2y}{dt^2}+4y=te^{2it}$$ Next, $$ a=1, b=0, c=4, \alpha=2i$$ Set $y=e^{2it} v$ Now my professor said this: $$ v''+4iv'=t$$ and I have no idea where he obtained that from. I shall show the rest   of the problem. Guess=polynomial and since there is no v term, the highest power is 2. $$v=a_1t+a_2t^2$$ $$v'=a_1+2a_2t$$ $$v''=2a_2$$ Plugging back into $ v''+4iv'=t$, we have: $$ 2a_2+4ia_1+8ia_2t=t$$ Equating: $$ 8ia_2=1 \rightarrow a_2=\frac{-i}{8}$$ $$ 2_a2+4ia_1=0 \rightarrow a_1=\frac{1}{16}$$ Plugging back into v(t): $$v(t)=\frac{1}{16}t-\frac{i}{8}t^2$$ Putting it all together: $$y=e^{2it}[\frac{1}{16}t-\frac{i}{8}t^2]$$ $$y=[\cos(2t)+i\sin(2t)][\frac{1}{16}t-\frac{i}{8}t^2]$$ Take note how the original problem says $\sin(2t)$, therefore our answer is the imaginary part which means the particular solution is: $$\psi(t)=\frac{t}{16}\sin(2t)-\frac{t^2}{8}\cos(2t)$$ I hope someone can help me find the part above. My professor mention there is two ways of solving this problem and normally I use another way but when I realize my professor's solution is much shorter, I wanted to learn his way but I'm just stuck with the part above.",,['ordinary-differential-equations']
26,Find $f(x) $ given that: $f'(x)=\frac{f(x)-x}{f(x)+x}$ [closed],Find  given that:  [closed],f(x)  f'(x)=\frac{f(x)-x}{f(x)+x},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Find $f(x)$ given that: $f \colon \mathbb{R^+} \rightarrow  \mathbb{R^+}$ , $f$ is differentiable function, and $f'(x)=\frac{f(x)-x}{f(x)+x}$ I tried  but couldn't get it that way.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Find given that: , is differentiable function, and I tried  but couldn't get it that way.",f(x) f \colon \mathbb{R^+} \rightarrow  \mathbb{R^+} f f'(x)=\frac{f(x)-x}{f(x)+x},"['ordinary-differential-equations', 'functional-equations']"
27,Solve a PDE with Feynman-Kac Formula,Solve a PDE with Feynman-Kac Formula,,"So there is the following PDE given: $\frac{\partial}{\partial t}f(t,x) + rx\frac{\partial}{\partial x}f(t,x)+\frac{\sigma^2 x^2}{2}\frac{{\partial}^2}{\partial x^2}f(t,x) = rf(t,x)$ With boundary condition $f(T,x) = x^{\frac{2r}{\sigma^2}}$ Here $r$ and $\sigma$ are positive constants. From what I have learned, I the solution is from the boundary condition $f(t,x) =e^{-r(T-t)}E[x^{\frac{2r}{\sigma^2}}]$ So first I look fro the stochastic representation which I find as: $dX(t) = rdt + \sigma dW(t)$ with X(t) = x The solution is: $X(T) = x + r(T-t) + \sigma(W(T)-W(t))$ This is normally distributed with mean $x + r(T-t)$ and variance $\sigma \sqrt{T-t}$ Now from boundary condition I have $f(t,x) =e^{-r(T-t)}E[(x + r(T-t) + \sigma(W(T)-W(t)))^{\frac{2r}{\sigma^2}}]$ However I don't know if this method is correct. If it is correct, how should I calculate this expectation? By the way, I take this expectation under $Q$ martingale measure. Thanks","So there is the following PDE given: $\frac{\partial}{\partial t}f(t,x) + rx\frac{\partial}{\partial x}f(t,x)+\frac{\sigma^2 x^2}{2}\frac{{\partial}^2}{\partial x^2}f(t,x) = rf(t,x)$ With boundary condition $f(T,x) = x^{\frac{2r}{\sigma^2}}$ Here $r$ and $\sigma$ are positive constants. From what I have learned, I the solution is from the boundary condition $f(t,x) =e^{-r(T-t)}E[x^{\frac{2r}{\sigma^2}}]$ So first I look fro the stochastic representation which I find as: $dX(t) = rdt + \sigma dW(t)$ with X(t) = x The solution is: $X(T) = x + r(T-t) + \sigma(W(T)-W(t))$ This is normally distributed with mean $x + r(T-t)$ and variance $\sigma \sqrt{T-t}$ Now from boundary condition I have $f(t,x) =e^{-r(T-t)}E[(x + r(T-t) + \sigma(W(T)-W(t)))^{\frac{2r}{\sigma^2}}]$ However I don't know if this method is correct. If it is correct, how should I calculate this expectation? By the way, I take this expectation under $Q$ martingale measure. Thanks",,"['ordinary-differential-equations', 'partial-differential-equations', 'stochastic-calculus']"
28,$\cos(x+y)\frac{\partial z}{\partial x}+\sin(x+y)\frac{\partial z}{\partial y}=z+\frac{1}{z}$,,\cos(x+y)\frac{\partial z}{\partial x}+\sin(x+y)\frac{\partial z}{\partial y}=z+\frac{1}{z},Having difficulty in solving the following partial differential equation:  $$\cos(x+y)\frac{\partial z}{\partial x}+\sin(x+y)\frac{\partial z}{\partial y}=z+\frac{1}{z}.$$ Will it be easier if we substitute $x+y$ as $s$ and proceed?,Having difficulty in solving the following partial differential equation:  $$\cos(x+y)\frac{\partial z}{\partial x}+\sin(x+y)\frac{\partial z}{\partial y}=z+\frac{1}{z}.$$ Will it be easier if we substitute $x+y$ as $s$ and proceed?,,"['ordinary-differential-equations', 'partial-differential-equations']"
29,Solving Kepler's second law,Solving Kepler's second law,,"Kepler's second law, about equal areas in equal times, is a differential equation: it gives velocity as a function of location. Where are the best expository accounts of the process of solving this equation, giving position as a function of time?","Kepler's second law, about equal areas in equal times, is a differential equation: it gives velocity as a function of location. Where are the best expository accounts of the process of solving this equation, giving position as a function of time?",,"['ordinary-differential-equations', 'reference-request', 'applications', 'mathematical-astronomy']"
30,Fundamental matrix for a linear ODE system with periodic coefficients,Fundamental matrix for a linear ODE system with periodic coefficients,,"I'm trying to find the fundamental matrix for the following ODE system: $$ y'=\left( \begin{matrix} 3-\sin^2 t&\cos t\\  \sin t+1&\cos t\sin t \end{matrix} \right)y.  $$ If one can come up with one solution, then the Liouville's formula can be used to find another one. But I have no idea how to find one. Is there a general method for solving $x'=A(t)x$ when $A(t)$ is nonconstant periodic? [Added:] Since the coefficient matrix is periodic, one might want to restrict the question a little bit. I'm wondering if one can transform it to the Hill differential equation , which can give a solution.","I'm trying to find the fundamental matrix for the following ODE system: $$ y'=\left( \begin{matrix} 3-\sin^2 t&\cos t\\  \sin t+1&\cos t\sin t \end{matrix} \right)y.  $$ If one can come up with one solution, then the Liouville's formula can be used to find another one. But I have no idea how to find one. Is there a general method for solving $x'=A(t)x$ when $A(t)$ is nonconstant periodic? [Added:] Since the coefficient matrix is periodic, one might want to restrict the question a little bit. I'm wondering if one can transform it to the Hill differential equation , which can give a solution.",,[]
31,Laplace transformation $y''+2y'+2y=3\sin x+\cos x$,Laplace transformation,y''+2y'+2y=3\sin x+\cos x,Given$$y''+2y'+2y=3\sin x+\cos x$$ Transform to image region $$Y(s)(s^2+2s+2)=\frac{3}{s^2+1}+\frac{s}{s^2+1}-s-2$$ $$Y(s)((s^2+2s+1)+1)=\frac{3}{s^2+1}+\frac{s}{s^2+1}-s-2$$ $$Y(s)((s+1)^2+1)=\frac{3}{s^2+1}+\frac{s}{s^2+1}-s-2$$ $$Y(s)=\frac{3}{(s^2+1)((s+1)^2+1)}+\frac{s}{(s^2+1)((s+1)^2+1)}-\frac{s}{((s+1)^2+1)}-\frac{2}{((s+1)^2+1)}$$ Now I need the inverse of $$\frac{1}{((s+1)^2+1)}$$ to continue. Can't find it anywhere,Given$$y''+2y'+2y=3\sin x+\cos x$$ Transform to image region $$Y(s)(s^2+2s+2)=\frac{3}{s^2+1}+\frac{s}{s^2+1}-s-2$$ $$Y(s)((s^2+2s+1)+1)=\frac{3}{s^2+1}+\frac{s}{s^2+1}-s-2$$ $$Y(s)((s+1)^2+1)=\frac{3}{s^2+1}+\frac{s}{s^2+1}-s-2$$ $$Y(s)=\frac{3}{(s^2+1)((s+1)^2+1)}+\frac{s}{(s^2+1)((s+1)^2+1)}-\frac{s}{((s+1)^2+1)}-\frac{2}{((s+1)^2+1)}$$ Now I need the inverse of $$\frac{1}{((s+1)^2+1)}$$ to continue. Can't find it anywhere,,"['ordinary-differential-equations', 'laplace-transform']"
32,Does Hyperbolic + Not Asymptotically Linearly Stable imply Not Asymptotically Stable?,Does Hyperbolic + Not Asymptotically Linearly Stable imply Not Asymptotically Stable?,,"Topic: Stability of Autonomous Non-linear ODEs I'm wondering whether having a hyperbolic critical point that's not asymptotically linearly stable (ALS) in the linearisation of a system implies that the critical point is not asymptotically stable (AS) in the full non-linear system... I know that in general, not ALS doesn't imply not AS, but it seems that the Hartman-Grobman theorem should make it true for hyperbolic critical points. I'll lay out an example, just to make it a bit clearer. Say we have the system: \begin{align} \dot{x} &= -6y + 2xy - 8\\ \dot{y} &= y^2 - x^2 \end{align} With critical points $(-1, -1)$ and $(4, 4)$. The Jacobian is: $$ DF(x, y) = \begin{bmatrix} 	2y & 2x - 6\\ 	-2x & 2y \end{bmatrix} $$ Now, for $(4, 4)$ the linearisation is: $$ DF(4, 4) = \begin{bmatrix} 	8 & 2\\ 	-8 & 8 \end{bmatrix} $$ Which has trace $\tau = 16$ and determinant $\delta = 80$. The eigenvalues are $8 \pm 4i$. Hence, $(4, 4)$ is hyperbolic in the linearisation and describes a spiral source. By the Hartman-Grobman theorem, we can conclude that the critical point also ""looks like"" a spiral source in the full non-linear system. My question is, in general, is it safe to conclude that the critical point is not asymptotically stable, i.e. it is not the case that solutions in some neighbourhood tend to the critical point as time goes to infinity? In this case, plotting the thing with Mathematica reveals that it is properly unstable at $(4, 4)$ (sorry about the lack of axes).","Topic: Stability of Autonomous Non-linear ODEs I'm wondering whether having a hyperbolic critical point that's not asymptotically linearly stable (ALS) in the linearisation of a system implies that the critical point is not asymptotically stable (AS) in the full non-linear system... I know that in general, not ALS doesn't imply not AS, but it seems that the Hartman-Grobman theorem should make it true for hyperbolic critical points. I'll lay out an example, just to make it a bit clearer. Say we have the system: \begin{align} \dot{x} &= -6y + 2xy - 8\\ \dot{y} &= y^2 - x^2 \end{align} With critical points $(-1, -1)$ and $(4, 4)$. The Jacobian is: $$ DF(x, y) = \begin{bmatrix} 	2y & 2x - 6\\ 	-2x & 2y \end{bmatrix} $$ Now, for $(4, 4)$ the linearisation is: $$ DF(4, 4) = \begin{bmatrix} 	8 & 2\\ 	-8 & 8 \end{bmatrix} $$ Which has trace $\tau = 16$ and determinant $\delta = 80$. The eigenvalues are $8 \pm 4i$. Hence, $(4, 4)$ is hyperbolic in the linearisation and describes a spiral source. By the Hartman-Grobman theorem, we can conclude that the critical point also ""looks like"" a spiral source in the full non-linear system. My question is, in general, is it safe to conclude that the critical point is not asymptotically stable, i.e. it is not the case that solutions in some neighbourhood tend to the critical point as time goes to infinity? In this case, plotting the thing with Mathematica reveals that it is properly unstable at $(4, 4)$ (sorry about the lack of axes).",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
33,Analysis of stability of a linearized ODE with a periodic solution,Analysis of stability of a linearized ODE with a periodic solution,,"I am asked to find the stability of the following ODE: \begin{equation*} \dot{y} = y^{2} + 2\cos(t)\sin(t) - \sin^{4}(t) \end{equation*} by linearizing around a particular solution $\eta = \sin^{2}(t)$, and investigating the solution. My understanding is that I should look at the first variational equation \begin{equation*} \dot{y} = A(t)y \end{equation*} where \begin{equation*} A(t) = df_{y}(t,\eta), \quad A(t+T) = A(t), \quad f(t,y) = \dot{y}(t). \end{equation*} Thus, I get \begin{alignat*}{2} f(t,y) &= y^{2} + 2\cos(t)\sin(t) - \sin^{4}(t) &&\Rightarrow \\ f_{y}(t,y) &= 2y &&\Rightarrow \\ A(t) &= 2\sin^{2}(t). \end{alignat*} Now $A(t)$ is periodic with period $T=2\pi$, and my next understanding is that stability of the solution depends on the sign of the eigenvalue for this one-dimensional system which are the Floquet multipliers (or characteristic exponents).  This is given by the formula (using notion from Teschl's ODE text): \begin{alignat*}{2} \bar{a} &= \frac{1}{T}\int_{0}^{T}A(s)ds \\ &= 1 \end{alignat*} which seems to imply that the solution is not stable.  Is this understanding correct?  Either way, I think part of my confusion comes from not understanding the connection between the first variation and the principal matrix solution of the original equation.  Any help towards that would be greatly appreciated.","I am asked to find the stability of the following ODE: \begin{equation*} \dot{y} = y^{2} + 2\cos(t)\sin(t) - \sin^{4}(t) \end{equation*} by linearizing around a particular solution $\eta = \sin^{2}(t)$, and investigating the solution. My understanding is that I should look at the first variational equation \begin{equation*} \dot{y} = A(t)y \end{equation*} where \begin{equation*} A(t) = df_{y}(t,\eta), \quad A(t+T) = A(t), \quad f(t,y) = \dot{y}(t). \end{equation*} Thus, I get \begin{alignat*}{2} f(t,y) &= y^{2} + 2\cos(t)\sin(t) - \sin^{4}(t) &&\Rightarrow \\ f_{y}(t,y) &= 2y &&\Rightarrow \\ A(t) &= 2\sin^{2}(t). \end{alignat*} Now $A(t)$ is periodic with period $T=2\pi$, and my next understanding is that stability of the solution depends on the sign of the eigenvalue for this one-dimensional system which are the Floquet multipliers (or characteristic exponents).  This is given by the formula (using notion from Teschl's ODE text): \begin{alignat*}{2} \bar{a} &= \frac{1}{T}\int_{0}^{T}A(s)ds \\ &= 1 \end{alignat*} which seems to imply that the solution is not stable.  Is this understanding correct?  Either way, I think part of my confusion comes from not understanding the connection between the first variation and the principal matrix solution of the original equation.  Any help towards that would be greatly appreciated.",,['ordinary-differential-equations']
34,How to show no periodic orbits exist,How to show no periodic orbits exist,,"I am trying to show that no periodic orbits exist for the system: $$ x_1'=y+x^2+xy^3$$ $$y'=-2x-y^3$$ I have tried using Dulac's criterion to find a function $g(x,y)$ such that $\Phi(x,y)$ given by : $$\Phi(x,y)=\frac{\partial(gx')}{\partial x}+\frac{\partial(gy')}{\partial y}$$ Is always $>0$ or $<0$ . But I have had no luck guessing such $g(x,y)$ and leaving $g(x,y)$ general results in a complicated first order PDE. Plotting the system on the phase space seems to imply there is no periodic orbits and that the origin is a focus (although a very slowly converging one) . There is one other critical point at $(x,y)=(-2^{1/5} , 2^{2/5})$ . Any ideas how I can show that no periodic orbits exist?","I am trying to show that no periodic orbits exist for the system: $$ x_1'=y+x^2+xy^3$$ $$y'=-2x-y^3$$ I have tried using Dulac's criterion to find a function $g(x,y)$ such that $\Phi(x,y)$ given by : $$\Phi(x,y)=\frac{\partial(gx')}{\partial x}+\frac{\partial(gy')}{\partial y}$$ Is always $>0$ or $<0$ . But I have had no luck guessing such $g(x,y)$ and leaving $g(x,y)$ general results in a complicated first order PDE. Plotting the system on the phase space seems to imply there is no periodic orbits and that the origin is a focus (although a very slowly converging one) . There is one other critical point at $(x,y)=(-2^{1/5} , 2^{2/5})$ . Any ideas how I can show that no periodic orbits exist?",,"['ordinary-differential-equations', 'chaos-theory']"
35,Interesting first order ODEs in applied science,Interesting first order ODEs in applied science,,"I'm teaching first and second order ODEs this term and would like some nice examples that are easy enough to solve. I have plenty of second order ODEs, e.g. simple/damped/driven harmonic motion. As for first order ODEs, I have alreay used the following: $M' = - kM$ (nuclear half-life) $\theta' = -k(\theta-R)$ (Newton's law of cooling in a room of temperature $R$) $P' = kP(T-P)$ (Infection in a population of $T$) Can anyone suggest any other first order ODEs which can be solved and that are used in applied sciences? Any help would be greatly appreciated.","I'm teaching first and second order ODEs this term and would like some nice examples that are easy enough to solve. I have plenty of second order ODEs, e.g. simple/damped/driven harmonic motion. As for first order ODEs, I have alreay used the following: $M' = - kM$ (nuclear half-life) $\theta' = -k(\theta-R)$ (Newton's law of cooling in a room of temperature $R$) $P' = kP(T-P)$ (Infection in a population of $T$) Can anyone suggest any other first order ODEs which can be solved and that are used in applied sciences? Any help would be greatly appreciated.",,"['ordinary-differential-equations', 'soft-question']"
36,Bifurcations for 1-dimensional map,Bifurcations for 1-dimensional map,,"Consinder the 1-dim map $F(x,\mu)=\mu-\frac{1}{4} x^2$ as $\mu$ increases from $-\infty$ to $5$ and analyse the bifurcations. I start the analysis by considering $F^2(x,\mu)=\mu^2-\frac{1}{2}\mu x^2+\frac{1}{16} x^4$ Setting $F^2(x,\mu)=0=1/16(x^2-4\mu)^2$ gives me the following solutions: $+-2\sqrt{\mu}$ They are also fixed points of $F$ We have stability in case of $+2\sqrt{\mu}$ and instability in $-2\sqrt{\mu}$ for $\mu>0$. What else can I say and how can I include the increasing value $\mu$ from $-\infty$ to $5$?","Consinder the 1-dim map $F(x,\mu)=\mu-\frac{1}{4} x^2$ as $\mu$ increases from $-\infty$ to $5$ and analyse the bifurcations. I start the analysis by considering $F^2(x,\mu)=\mu^2-\frac{1}{2}\mu x^2+\frac{1}{16} x^4$ Setting $F^2(x,\mu)=0=1/16(x^2-4\mu)^2$ gives me the following solutions: $+-2\sqrt{\mu}$ They are also fixed points of $F$ We have stability in case of $+2\sqrt{\mu}$ and instability in $-2\sqrt{\mu}$ for $\mu>0$. What else can I say and how can I include the increasing value $\mu$ from $-\infty$ to $5$?",,"['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
37,"dropping a particle into a vector field, part 2","dropping a particle into a vector field, part 2",,"Okay, so earlier I posted this question "" dropping a particle into a vector field "" as sort of a feeler question as i study line integrals in order to go into surface integrals and eventually differential forms and differential geometry and I got to thinking about my question's example and how to solve it.  I've taken an advanced linear algebra course and the required ODE course, but the ODE course never made it to solving systems of differential equations.  So my posited vector field in which I'm dropping a particle into is $$\mathbf{F}(x,y)=y\mathbf{i}-x\mathbf{j}$$ An answer came in the form that in order to solve the question I posited, I needed to find an $\mathbf{r}$ such that $\mathbf{r}'(t)=\mathbf{F}(\mathbf{r}(t))$.  So my arbitrary $\mathbf{r}(t)=(\mathbf{x}(t),\mathbf{y}(t))$, if i take the derivative of this since it's just a vector valued function i get $$\mathbf{r}'(t)=(\mathbf{x}'(t),\mathbf{y}'(t))$$ Now,  $$\mathbf{F}(\mathbf{r}(t))=\mathbf{F}(\mathbf{x}(t),\mathbf{y}(t))=\mathbf{y}(t)\mathbf{i}-\mathbf{x}(t)\mathbf{j}=(\mathbf{y}(t),-\mathbf{x}(t))$$ So my equation has become $$\mathbf{r}'(t)=(\mathbf{x}'(t),\mathbf{y}'(t))=(\mathbf{y}(t),-\mathbf{x}(t))$$ This gives me a system of differential equations to solve: $$\mathbf{x}'(t)=\mathbf{y}(t)$$ $$\mathbf{y}'(t)=-\mathbf{x}(t)$$ And here I'm not sure how to procede, or if this is even the correct intution.  The person that wrote the answer on the last post gave me numeric Euler method as a general case, but this seems pretty solvable and straight forward, but I'm not seeing where to move.","Okay, so earlier I posted this question "" dropping a particle into a vector field "" as sort of a feeler question as i study line integrals in order to go into surface integrals and eventually differential forms and differential geometry and I got to thinking about my question's example and how to solve it.  I've taken an advanced linear algebra course and the required ODE course, but the ODE course never made it to solving systems of differential equations.  So my posited vector field in which I'm dropping a particle into is $$\mathbf{F}(x,y)=y\mathbf{i}-x\mathbf{j}$$ An answer came in the form that in order to solve the question I posited, I needed to find an $\mathbf{r}$ such that $\mathbf{r}'(t)=\mathbf{F}(\mathbf{r}(t))$.  So my arbitrary $\mathbf{r}(t)=(\mathbf{x}(t),\mathbf{y}(t))$, if i take the derivative of this since it's just a vector valued function i get $$\mathbf{r}'(t)=(\mathbf{x}'(t),\mathbf{y}'(t))$$ Now,  $$\mathbf{F}(\mathbf{r}(t))=\mathbf{F}(\mathbf{x}(t),\mathbf{y}(t))=\mathbf{y}(t)\mathbf{i}-\mathbf{x}(t)\mathbf{j}=(\mathbf{y}(t),-\mathbf{x}(t))$$ So my equation has become $$\mathbf{r}'(t)=(\mathbf{x}'(t),\mathbf{y}'(t))=(\mathbf{y}(t),-\mathbf{x}(t))$$ This gives me a system of differential equations to solve: $$\mathbf{x}'(t)=\mathbf{y}(t)$$ $$\mathbf{y}'(t)=-\mathbf{x}(t)$$ And here I'm not sure how to procede, or if this is even the correct intution.  The person that wrote the answer on the last post gave me numeric Euler method as a general case, but this seems pretty solvable and straight forward, but I'm not seeing where to move.",,"['ordinary-differential-equations', 'multivariable-calculus', 'vector-fields']"
38,How to solve piecewise differential equation when which branch is active depends on function value,How to solve piecewise differential equation when which branch is active depends on function value,,"I want to solve a differential equation that is piecewise, but the conditions of the piecewise function depend on the value of the equation, ie, $$f^\prime(x)=\cases{ c_1 & $f(x) < c_5$ \cr \frac{c_2}{c_3 + c_4x} & $f(x) \geq c_5$},$$ where $c_i$ for $i\in \{1,\ldots, 5\}$ are constants. I wouldn't be surprised if this is standard knowledge, but I don't know the proper language to use to describe my problem, and so pointers and what to search for would be appreciated.","I want to solve a differential equation that is piecewise, but the conditions of the piecewise function depend on the value of the equation, ie, $$f^\prime(x)=\cases{ c_1 & $f(x) < c_5$ \cr \frac{c_2}{c_3 + c_4x} & $f(x) \geq c_5$},$$ where $c_i$ for $i\in \{1,\ldots, 5\}$ are constants. I wouldn't be surprised if this is standard knowledge, but I don't know the proper language to use to describe my problem, and so pointers and what to search for would be appreciated.",,['ordinary-differential-equations']
39,Solve $\frac{dy}{dx} = f^{-1}(x)$,Solve,\frac{dy}{dx} = f^{-1}(x),"I am doing a differential equations subject, and have been given this as a challenge question. Solve $\frac{dy}{dx} = f^{-1}(x)$ ($f^{-1}(x)$ is the inverse function of x.) I assume that the answer involves separable differential equations, but I can't quite see how to do it. Is there a substitution that will help solve this? Edit: I've tried the following ideas: Idea 1. $$ \begin{align} y &= \int f^{-1}(x) \, dx\\ \mathrm{let}\,\,u &= f^{-1}(x)\\ x &= f(u)\\ \frac{dx}{du} &= f'(u)\\ dx &= f'(u) \, \, du\\ \mathrm{so} \,\, y &= \int u f'(u)\, du\\ \end{align} $$ Is this right, and if so, would I just integrate by parts?","I am doing a differential equations subject, and have been given this as a challenge question. Solve $\frac{dy}{dx} = f^{-1}(x)$ ($f^{-1}(x)$ is the inverse function of x.) I assume that the answer involves separable differential equations, but I can't quite see how to do it. Is there a substitution that will help solve this? Edit: I've tried the following ideas: Idea 1. $$ \begin{align} y &= \int f^{-1}(x) \, dx\\ \mathrm{let}\,\,u &= f^{-1}(x)\\ x &= f(u)\\ \frac{dx}{du} &= f'(u)\\ dx &= f'(u) \, \, du\\ \mathrm{so} \,\, y &= \int u f'(u)\, du\\ \end{align} $$ Is this right, and if so, would I just integrate by parts?",,['ordinary-differential-equations']
40,"differential equation, bit off more than i could chew","differential equation, bit off more than i could chew",,"I started down the path of a project for school that was inspired by mythbusters.  The oritinal thought was showing why a bullet will effectively cease effectiveness past a given depth.  So I'm rocking $F_d = \frac{1}{2}pv^2AC_d$ which we can effectively compress down to $F = Kv^2$ as the rest for my purposes will be constants.  Applying newton's laws to this we turn it into $y' = a / y'' = a$ making $F=ma = my''$, after a little help from a friend we arrive at $y'' = \frac{F}{m} = \frac{K}{m}(y')^2$. Letting $\alpha=\frac{K}{m}$ thus collapsing the problem finally down to $y'' = \alpha(y')^2$ This is the crux of my problem.  Can anyone explain the approach to solving this?  Seems this is a little beyond the calc II course this was originally for.  But seeing as I'm already neck deep in the problem in other ways, I see no point in giving up on it now.","I started down the path of a project for school that was inspired by mythbusters.  The oritinal thought was showing why a bullet will effectively cease effectiveness past a given depth.  So I'm rocking $F_d = \frac{1}{2}pv^2AC_d$ which we can effectively compress down to $F = Kv^2$ as the rest for my purposes will be constants.  Applying newton's laws to this we turn it into $y' = a / y'' = a$ making $F=ma = my''$, after a little help from a friend we arrive at $y'' = \frac{F}{m} = \frac{K}{m}(y')^2$. Letting $\alpha=\frac{K}{m}$ thus collapsing the problem finally down to $y'' = \alpha(y')^2$ This is the crux of my problem.  Can anyone explain the approach to solving this?  Seems this is a little beyond the calc II course this was originally for.  But seeing as I'm already neck deep in the problem in other ways, I see no point in giving up on it now.",,['ordinary-differential-equations']
41,Particular solutions of ODE,Particular solutions of ODE,,"I'm reading Morris Tenenbaum's book and it says that given a solvable n-th order differential equation, we can have more than one n-parameter family of solutions. By n-parameter family of solution he means what is usually called a ""general solution"" , a possible solution with n coeficients.He calls it n-parameter family of solutions because it is not necessarily a general solution ( it might not contain some possible particular solutions ). He gives many examples : Given the first order ODE $y' = -2y^{3/2}$  while we might obtain the following 1-parameter family of solutions $y = \frac{1}{(x+c)^2}$ , it's not the only possible one. There's also the 1-parameter family of solutions $y = \frac{C^2}{(Cx + 1)^2}$ . While the particular (or singular ) solution $y=0$ for example, can't be obtained by the first 1-parameter family of solutions ( no matter what $C$ we choose ), it can be obtained by the second 1-parameter family of solutions ( by setting $C = 0$ ). The problem i'm having : My doubt is if, even tho a single n-parameter family of solutions might not provide all possible particular solutions, any particular solution can be obtained by any n-parameter family of solutions. For example, given the first order ODE $y = xy' + (y')^2$ We can easily find one possible 1-parameter family of solutions as $y = cx + c^2$. But it also has a singular solution $y = -\frac{x^2}{4}$ that can't be obtained by $y = cx + c^2$ ( no matter what c we choose ). My question is : Could $y = -\frac{x^2}{4}$ be obtained by another 1-parameter family of solutions that would satisfy $y = xy' + (y')^2$ ? Is it a rule that there's always is a n-parameter family of solutions that can provide any particular solution ?","I'm reading Morris Tenenbaum's book and it says that given a solvable n-th order differential equation, we can have more than one n-parameter family of solutions. By n-parameter family of solution he means what is usually called a ""general solution"" , a possible solution with n coeficients.He calls it n-parameter family of solutions because it is not necessarily a general solution ( it might not contain some possible particular solutions ). He gives many examples : Given the first order ODE $y' = -2y^{3/2}$  while we might obtain the following 1-parameter family of solutions $y = \frac{1}{(x+c)^2}$ , it's not the only possible one. There's also the 1-parameter family of solutions $y = \frac{C^2}{(Cx + 1)^2}$ . While the particular (or singular ) solution $y=0$ for example, can't be obtained by the first 1-parameter family of solutions ( no matter what $C$ we choose ), it can be obtained by the second 1-parameter family of solutions ( by setting $C = 0$ ). The problem i'm having : My doubt is if, even tho a single n-parameter family of solutions might not provide all possible particular solutions, any particular solution can be obtained by any n-parameter family of solutions. For example, given the first order ODE $y = xy' + (y')^2$ We can easily find one possible 1-parameter family of solutions as $y = cx + c^2$. But it also has a singular solution $y = -\frac{x^2}{4}$ that can't be obtained by $y = cx + c^2$ ( no matter what c we choose ). My question is : Could $y = -\frac{x^2}{4}$ be obtained by another 1-parameter family of solutions that would satisfy $y = xy' + (y')^2$ ? Is it a rule that there's always is a n-parameter family of solutions that can provide any particular solution ?",,"['ordinary-differential-equations', 'functions']"
42,Semilinear Parabolic PDE,Semilinear Parabolic PDE,,I'm considering the following type of PDE:  $u_{t}=u_{xx}+u_{x}+u_{x}^2+u_{x}^3+\frac{u_{x}}{x(1-x)}+\left(\frac{u_{x}}{x(1-x)}\right)^3$ with periodic boundary conditions $u_{x}(0)=u_{x}(1)=0$. Does anyone know of literature/articles I can look at to possibly show local existence(or blow up?). I've spent quite a bit of time looking through literature in particular Henry's Geometric theory of Semi-Linear Parabolic equations and papers by H.Amman on Quasi linear Parabolic equations.  Also due to the singularity I considered a weighted sobolev space approach but had no success.  I apologize if I'm too vague.,I'm considering the following type of PDE:  $u_{t}=u_{xx}+u_{x}+u_{x}^2+u_{x}^3+\frac{u_{x}}{x(1-x)}+\left(\frac{u_{x}}{x(1-x)}\right)^3$ with periodic boundary conditions $u_{x}(0)=u_{x}(1)=0$. Does anyone know of literature/articles I can look at to possibly show local existence(or blow up?). I've spent quite a bit of time looking through literature in particular Henry's Geometric theory of Semi-Linear Parabolic equations and papers by H.Amman on Quasi linear Parabolic equations.  Also due to the singularity I considered a weighted sobolev space approach but had no success.  I apologize if I'm too vague.,,"['ordinary-differential-equations', 'partial-differential-equations']"
43,Problem books in ODE,Problem books in ODE,,I'm studying Ordinary differential equations right now in the level of Hartman's book . I've never seen problem books in ODE in this level even if you consider it without solutions. I would like to know if one of you know any problem book in ODE in this level? Thanks a lot,I'm studying Ordinary differential equations right now in the level of Hartman's book . I've never seen problem books in ODE in this level even if you consider it without solutions. I would like to know if one of you know any problem book in ODE in this level? Thanks a lot,,"['ordinary-differential-equations', 'reference-request']"
44,Differential Equation$\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{1}{x^{2}+y^{2}}$,Differential Equation,\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{1}{x^{2}+y^{2}},"Please give me a hint to solve for $y$. Given, $$\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{1}{x^{2}+y^{2}}$$ I took $y=x\cdot\tan\theta$ but it was of no use.","Please give me a hint to solve for $y$. Given, $$\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{1}{x^{2}+y^{2}}$$ I took $y=x\cdot\tan\theta$ but it was of no use.",,"['calculus', 'ordinary-differential-equations']"
45,Heat Kernel Asymptotics on Manifold with Boundary,Heat Kernel Asymptotics on Manifold with Boundary,,"On a closed Riemannian manifold $M$, the heat kernel $k_t(x, y)$ of the Laplace-Beltrami operator (or more general of any generalized symmetric Laplace-type operator acting on sections of a vector bundle) admits an asymptotic expansion of the form $$  k_t(x, y) \sim \exp\left( -\frac{1}{4t}d(x, y)^2\right) \sum_{j=0}^\infty t^j \Phi_j(x, y) $$ where $d(x, y)$ denotes the Riemannian distance and $\Phi_j$ are appropriate smooth functions, not depending on $t$. This is meant in the sense that for each $N \in \mathbb{N}$, there exists a constant $C>0$ such that for all $x, y \in M$, we have $$ \left| k_t(x, y) -  \chi(x, y)\exp\left( -\frac{1}{4t}d(x, y)^2\right) \sum_{j=0}^Nt^j \Phi_j(x, y) \right| < C t^{N+1}$$ where $\chi(x, y)$ is an appropriate cutoff function that is $\equiv 1$ near the diagonal. In the case that $M$ is still compact but has a boundary, in many books there can be found an asymptotic expansion of the trace, but I could not find an asymptotic expansion of the kernel itself, uniform on $M \times M$. Is there such an expansion?","On a closed Riemannian manifold $M$, the heat kernel $k_t(x, y)$ of the Laplace-Beltrami operator (or more general of any generalized symmetric Laplace-type operator acting on sections of a vector bundle) admits an asymptotic expansion of the form $$  k_t(x, y) \sim \exp\left( -\frac{1}{4t}d(x, y)^2\right) \sum_{j=0}^\infty t^j \Phi_j(x, y) $$ where $d(x, y)$ denotes the Riemannian distance and $\Phi_j$ are appropriate smooth functions, not depending on $t$. This is meant in the sense that for each $N \in \mathbb{N}$, there exists a constant $C>0$ such that for all $x, y \in M$, we have $$ \left| k_t(x, y) -  \chi(x, y)\exp\left( -\frac{1}{4t}d(x, y)^2\right) \sum_{j=0}^Nt^j \Phi_j(x, y) \right| < C t^{N+1}$$ where $\chi(x, y)$ is an appropriate cutoff function that is $\equiv 1$ near the diagonal. In the case that $M$ is still compact but has a boundary, in many books there can be found an asymptotic expansion of the trace, but I could not find an asymptotic expansion of the kernel itself, uniform on $M \times M$. Is there such an expansion?",,"['ordinary-differential-equations', 'differential-geometry']"
46,Determining whether an IVP has a unique solution if uniqueness theorem doesn't hold,Determining whether an IVP has a unique solution if uniqueness theorem doesn't hold,,"I have the following initial value problems: $$ \begin{cases} y' = y^{\frac{1}{2}}\cos(x) \\[6pt] y(-2)= 1 \end{cases} $$ $$ \begin{cases} y' = y^{\frac{1}{2}}\cos(x) \\[6pt] y(5)=-8 \end{cases} $$ $$ \begin{cases} y' = y^{\frac{1}{2}} \cos(x) \\[6pt] y(0)= 0 \end{cases} $$ I want to determine if they have a unique solution. I'm using the uniqueness theorem that says if $f(x, y)$ and $\partial f/\partial y$ are continuous on  a rectangular region enclosing $(x_0, y_0)$, then there is a unique solution on an interval $I$. $$\frac{\partial f}{\partial y} = \frac{1}{2} \cdot \frac{\cos(x)}{y^{\frac{1}{2}}}$$ So I know that I can only have unique solutions for $y > 0$. For the first IVP, it's easy to find a rectangular region containing $(-2, 1)$, so it must have a unique solution. But the next one lies below the line $y = 0$ and the last one on the line. The textbook says that if the hypothesis of the theorem doesn't hold, then it's inconclusive how many solutions there are. My question is, how can I determine if there is a unique solution if I can't use the theorem? Should I try to solve the IVPs and just see what happens?","I have the following initial value problems: $$ \begin{cases} y' = y^{\frac{1}{2}}\cos(x) \\[6pt] y(-2)= 1 \end{cases} $$ $$ \begin{cases} y' = y^{\frac{1}{2}}\cos(x) \\[6pt] y(5)=-8 \end{cases} $$ $$ \begin{cases} y' = y^{\frac{1}{2}} \cos(x) \\[6pt] y(0)= 0 \end{cases} $$ I want to determine if they have a unique solution. I'm using the uniqueness theorem that says if $f(x, y)$ and $\partial f/\partial y$ are continuous on  a rectangular region enclosing $(x_0, y_0)$, then there is a unique solution on an interval $I$. $$\frac{\partial f}{\partial y} = \frac{1}{2} \cdot \frac{\cos(x)}{y^{\frac{1}{2}}}$$ So I know that I can only have unique solutions for $y > 0$. For the first IVP, it's easy to find a rectangular region containing $(-2, 1)$, so it must have a unique solution. But the next one lies below the line $y = 0$ and the last one on the line. The textbook says that if the hypothesis of the theorem doesn't hold, then it's inconclusive how many solutions there are. My question is, how can I determine if there is a unique solution if I can't use the theorem? Should I try to solve the IVPs and just see what happens?",,['ordinary-differential-equations']
47,How to solve following differential equation?,How to solve following differential equation?,,$$ \int \limits_{0}^{\infty}\sqrt{1 + y'^{2}(x)}dx = 2 \sqrt{x} + y \qquad (.1)  $$ The solution is $$ 3y = x\sqrt{x} - 3\sqrt{x} . $$ I don't know how to solve this type of equations. Also I don't understand how the right-hand side of the equation (.1) may depend on the variable x. Can you help me?,$$ \int \limits_{0}^{\infty}\sqrt{1 + y'^{2}(x)}dx = 2 \sqrt{x} + y \qquad (.1)  $$ The solution is $$ 3y = x\sqrt{x} - 3\sqrt{x} . $$ I don't know how to solve this type of equations. Also I don't understand how the right-hand side of the equation (.1) may depend on the variable x. Can you help me?,,"['ordinary-differential-equations', 'integral-equations']"
48,Which ODE Solution Method to Use?,Which ODE Solution Method to Use?,,"It has been awhile since I've taken a course in differential equations, and I have problem, which requires I solve an ODE (after transforming a PDE) of the following structure: $$ f(x) - (x + c_1)f'(x) = c_2xf''(x) $$ where $c_1$ and $c_2$ are nonzero constants. I'm not looking for the actual solution, I'm looking for the name of the solution method/procedure so that I can look into how to solve these types of ODEs.  Any pointers? Thanks!","It has been awhile since I've taken a course in differential equations, and I have problem, which requires I solve an ODE (after transforming a PDE) of the following structure: $$ f(x) - (x + c_1)f'(x) = c_2xf''(x) $$ where $c_1$ and $c_2$ are nonzero constants. I'm not looking for the actual solution, I'm looking for the name of the solution method/procedure so that I can look into how to solve these types of ODEs.  Any pointers? Thanks!",,['ordinary-differential-equations']
49,Which solution does a particle follow if uniqueness fails in ODE?,Which solution does a particle follow if uniqueness fails in ODE?,,"If there are multiple solutions to an ODE at y(0) = 0 and a particle is ""dropped"" into the flow field where the solutions intersect,which path does the particle take?  In other words, which solution do we follow if we start at (0,0)?","If there are multiple solutions to an ODE at y(0) = 0 and a particle is ""dropped"" into the flow field where the solutions intersect,which path does the particle take?  In other words, which solution do we follow if we start at (0,0)?",,['ordinary-differential-equations']
50,Initial value problem with non-Lipschitz condition but with unique solution,Initial value problem with non-Lipschitz condition but with unique solution,,"Consider the following initial value problem: $$ \begin{cases} y^\prime = f(x,y)\\ y(0)=0 \end{cases} $$ where $f$ is the function  $$ f(x,y) =\begin{cases}          y\sin(1/y) & \text{if}\; y\neq 0\\           0 & \text{if}\; y=0. \end{cases} $$ It is clear that $f$ is continuous, but not Lipschitz in a neighbourhood of $(0,0)$. Nevertheless, the IVP has one (and only one) solution in that neighbourhood. Which is the solution?","Consider the following initial value problem: $$ \begin{cases} y^\prime = f(x,y)\\ y(0)=0 \end{cases} $$ where $f$ is the function  $$ f(x,y) =\begin{cases}          y\sin(1/y) & \text{if}\; y\neq 0\\           0 & \text{if}\; y=0. \end{cases} $$ It is clear that $f$ is continuous, but not Lipschitz in a neighbourhood of $(0,0)$. Nevertheless, the IVP has one (and only one) solution in that neighbourhood. Which is the solution?",,"['real-analysis', 'ordinary-differential-equations']"
51,Second order equations on manifolds,Second order equations on manifolds,,"In my notes, the lecturer considers a smooth vector field $v: TM\to T(TM)$, with $M$ a smooth manifold. Let's write $$v(u,e)=((u,e), (a(u,e),b(u,e)).$$ It is said that $v$ is a second order equation if $T\pi_M\circ v=\text{id}$. It implies that $a(u,e)=e$, i.e. $$v(u,e)=((u,e),(e,b(u,e))).$$ Here, my notes claim that if $c(t)=(u(t),e(t))$ is a curve on $TM$ satisfying the above, then $$\frac{du}{dt}=e(t), \ \frac{de}{dt}=b(t)$$ This is probably very stupid but I can get why...","In my notes, the lecturer considers a smooth vector field $v: TM\to T(TM)$, with $M$ a smooth manifold. Let's write $$v(u,e)=((u,e), (a(u,e),b(u,e)).$$ It is said that $v$ is a second order equation if $T\pi_M\circ v=\text{id}$. It implies that $a(u,e)=e$, i.e. $$v(u,e)=((u,e),(e,b(u,e))).$$ Here, my notes claim that if $c(t)=(u(t),e(t))$ is a curve on $TM$ satisfying the above, then $$\frac{du}{dt}=e(t), \ \frac{de}{dt}=b(t)$$ This is probably very stupid but I can get why...",,"['ordinary-differential-equations', 'manifolds', 'vector-bundles']"
52,Upper bound of nonlinear first-order ordinary differential equation,Upper bound of nonlinear first-order ordinary differential equation,,"Given: $y(x)$ is defined for $x \geq 1$ and satisfies $$y'=\frac{1}{x^2+y^2},  y(1)=1$$ Show that $$ y(x) < \frac{5\pi}{4} $$ for all $x \geq 1$ I don't see an easy way to solve for $y(x)$, and I don't know how to demonstrate that the function is always less than $5\pi/4$ without solving for $y(x)$. Any suggestions on how to approach this?","Given: $y(x)$ is defined for $x \geq 1$ and satisfies $$y'=\frac{1}{x^2+y^2},  y(1)=1$$ Show that $$ y(x) < \frac{5\pi}{4} $$ for all $x \geq 1$ I don't see an easy way to solve for $y(x)$, and I don't know how to demonstrate that the function is always less than $5\pi/4$ without solving for $y(x)$. Any suggestions on how to approach this?",,['ordinary-differential-equations']
53,How to prove $(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})=xE(x)$,How to prove,(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})=xE(x),"$$E(x)=\int_0^{\frac{\pi}{2}} \sqrt{1-x^2 \sin^2 t}\, dt$$ Where $E(x)$ is complete elliptic integral of the second kind. $u=\sin t$ $$E(x)=\int_0^{1} \frac{\sqrt{1-x^2 u^2}}{\sqrt{1-u^2}}\, du$$ $$\frac{dE(x)}{dx}=-x\int_0^{1} \frac{u^2}{\sqrt{1-u^2}\sqrt{1-x^2 u^2}} \, du$$ $$\frac{d}{dx}(x\frac{dE(x)}{dx})=-2x\int_0^{1} \frac{u^2}{\sqrt{1-u^2}\sqrt{1-x^2 u^2}} \, du-x^2\int_0^{1} \frac{xu^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du$$ $$\frac{d}{dx}(x\frac{dE(x)}{dx})=\int_0^{1} \frac{-2xu^2(1-x^2 u^2)-x^3u^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du$$ $$(x^2-1)\frac{d}{dx}(x\frac{dE(x)}{dx})=\int_0^{1} \frac{(-2xu^2+x^3u^4)(x^2-1)}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du \tag1$$ $$xE(x)=\int_0^{1} \frac{x\sqrt{1-x^2 u^2}}{\sqrt{1-u^2}}\, du \tag 2$$ According to Wikipedia, Equation 1 and 2 are equal  but I could not prove it.  Could you please help me to prove that? $$(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})=xE(x)$$ http://en.wikipedia.org/wiki/Elliptic_integral EDIT: If $$(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})=xE(x)$$  is true, then $$(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})-xE(x)=0$$ $$\int_0^{1} \frac{(-2xu^2+x^3u^4)(x^2-1)}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du-\int_0^{1} \frac{x\sqrt{1-x^2 u^2}}{\sqrt{1-u^2}}\, du=0$$  must be. And then $$\int_0^{1} \frac{(-2xu^2+x^3u^4)(x^2-1)-x (1-x^2 u^2)^2}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ $$-x\int_0^{1} \frac{1-2u^2+x^2u^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ If Wikipedia differential equation is true , $$\int_0^{1} \frac{1-2u^2+x^2u^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ $$\int_0^{1} \frac{1-u^2}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du-\int_0^{1} \frac{u^2(1-x^2u^2)}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ $$\int_0^{1} \frac{\sqrt{1-u^2}}{(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=\int_0^{1} \frac{u^2}{\sqrt{1-u^2}\sqrt{1-x^2 u^2}} \, du$$  must be true too. Now I need to prove that last equation. Any idea how to proceed? Thanks a lot for advice.","$$E(x)=\int_0^{\frac{\pi}{2}} \sqrt{1-x^2 \sin^2 t}\, dt$$ Where $E(x)$ is complete elliptic integral of the second kind. $u=\sin t$ $$E(x)=\int_0^{1} \frac{\sqrt{1-x^2 u^2}}{\sqrt{1-u^2}}\, du$$ $$\frac{dE(x)}{dx}=-x\int_0^{1} \frac{u^2}{\sqrt{1-u^2}\sqrt{1-x^2 u^2}} \, du$$ $$\frac{d}{dx}(x\frac{dE(x)}{dx})=-2x\int_0^{1} \frac{u^2}{\sqrt{1-u^2}\sqrt{1-x^2 u^2}} \, du-x^2\int_0^{1} \frac{xu^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du$$ $$\frac{d}{dx}(x\frac{dE(x)}{dx})=\int_0^{1} \frac{-2xu^2(1-x^2 u^2)-x^3u^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du$$ $$(x^2-1)\frac{d}{dx}(x\frac{dE(x)}{dx})=\int_0^{1} \frac{(-2xu^2+x^3u^4)(x^2-1)}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du \tag1$$ $$xE(x)=\int_0^{1} \frac{x\sqrt{1-x^2 u^2}}{\sqrt{1-u^2}}\, du \tag 2$$ According to Wikipedia, Equation 1 and 2 are equal  but I could not prove it.  Could you please help me to prove that? $$(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})=xE(x)$$ http://en.wikipedia.org/wiki/Elliptic_integral EDIT: If $$(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})=xE(x)$$  is true, then $$(x^2-1) \frac{d}{dx}(x \frac{dE(x)}{dx})-xE(x)=0$$ $$\int_0^{1} \frac{(-2xu^2+x^3u^4)(x^2-1)}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du-\int_0^{1} \frac{x\sqrt{1-x^2 u^2}}{\sqrt{1-u^2}}\, du=0$$  must be. And then $$\int_0^{1} \frac{(-2xu^2+x^3u^4)(x^2-1)-x (1-x^2 u^2)^2}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ $$-x\int_0^{1} \frac{1-2u^2+x^2u^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ If Wikipedia differential equation is true , $$\int_0^{1} \frac{1-2u^2+x^2u^4}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ $$\int_0^{1} \frac{1-u^2}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du-\int_0^{1} \frac{u^2(1-x^2u^2)}{\sqrt{1-u^2}(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=0$$ $$\int_0^{1} \frac{\sqrt{1-u^2}}{(1-x^2 u^2)\sqrt{1-x^2 u^2}} \, du=\int_0^{1} \frac{u^2}{\sqrt{1-u^2}\sqrt{1-x^2 u^2}} \, du$$  must be true too. Now I need to prove that last equation. Any idea how to proceed? Thanks a lot for advice.",,"['ordinary-differential-equations', 'elliptic-integrals']"
54,Functions whose $n^{th}$ Derivatives form a cycle,Functions whose  Derivatives form a cycle,n^{th},"The simplest example of this is $e^x$ which we could say has period 1 (it is its own derivative). $e^{-x}$ would have period 2. Using similar constructions, I can get a function that has a derivative of period $n$ by doing $e^{x\cdot (1)^{1/n}}$ Is this the only way to get periodic derivatives? Note: I am treating sin and cos as special cases of this when $n=4$ Is there a proof to this effect?","The simplest example of this is $e^x$ which we could say has period 1 (it is its own derivative). $e^{-x}$ would have period 2. Using similar constructions, I can get a function that has a derivative of period $n$ by doing $e^{x\cdot (1)^{1/n}}$ Is this the only way to get periodic derivatives? Note: I am treating sin and cos as special cases of this when $n=4$ Is there a proof to this effect?",,"['ordinary-differential-equations', 'periodic-functions']"
55,Differential Equations reference for Putnam preparation,Differential Equations reference for Putnam preparation,,"I have two problem collections I am currently working through, the ""Berkeley Problems in Mathematics"" book, and the first of the three volumes of Putnam problems compiled by the MAA. These both contain many problems on basic differential equations. Unfortunately, I never had a course in differential equations. Otherwise, my background is reasonably good, and I have knowledge of real analysis (at the level of baby Rudin), basic abstract algebra, topology, and complex analysis. I feel I could handle a more concise and mathematically mature approach to differential equations than the ""cookbook"" style that is normally given to first and second year students. I was wondering if someone to access to the above books that I am working through could suggest a concise reference that would cover what I need to know to solve the problems in them. In particular, it seems I need to know basic solution methods and basic existence and uniqueness theorem. On the other hand, I have no desire to specialize in differential equations, so a reference work like V.I Arnold's book on ordinary differential equations would not suit my needs, and I certainly don't have any need for knowledge of, say, the Laplace transform or PDEs. To reiterate, I just need a concise, high level overview of the basic (by hand) solution techniques for differential equations, along with some discussion of the basic uniqueness and existence theorems. I realize this is rather vague, but looking through the two problem books I listed above should give a more precise idea of what I mean. Worked examples would be a plus. I am very unfamiliar with the subject matter, so thanks in advance for putting up with my very nebulous request. EDIT: I found Coddington's ""Intoduction to Ordinary Differential Equations"" to be what I needed. Thanks guys.","I have two problem collections I am currently working through, the ""Berkeley Problems in Mathematics"" book, and the first of the three volumes of Putnam problems compiled by the MAA. These both contain many problems on basic differential equations. Unfortunately, I never had a course in differential equations. Otherwise, my background is reasonably good, and I have knowledge of real analysis (at the level of baby Rudin), basic abstract algebra, topology, and complex analysis. I feel I could handle a more concise and mathematically mature approach to differential equations than the ""cookbook"" style that is normally given to first and second year students. I was wondering if someone to access to the above books that I am working through could suggest a concise reference that would cover what I need to know to solve the problems in them. In particular, it seems I need to know basic solution methods and basic existence and uniqueness theorem. On the other hand, I have no desire to specialize in differential equations, so a reference work like V.I Arnold's book on ordinary differential equations would not suit my needs, and I certainly don't have any need for knowledge of, say, the Laplace transform or PDEs. To reiterate, I just need a concise, high level overview of the basic (by hand) solution techniques for differential equations, along with some discussion of the basic uniqueness and existence theorems. I realize this is rather vague, but looking through the two problem books I listed above should give a more precise idea of what I mean. Worked examples would be a plus. I am very unfamiliar with the subject matter, so thanks in advance for putting up with my very nebulous request. EDIT: I found Coddington's ""Intoduction to Ordinary Differential Equations"" to be what I needed. Thanks guys.",,"['reference-request', 'ordinary-differential-equations']"
56,How can I solve this non-linear differential equation?,How can I solve this non-linear differential equation?,,"I'm trying to solve the equation $$y' = 1 - y^2$$ Here is my attempt: $$y' = 1 - y^2$$ Divide by (1-y^2) $$\frac{y'}{1-y^2} = 1$$ Integrate both sides: $$\frac{1}{2}\log|\frac{y+1}{y-1}|=t+c$$ Rearrange $$y = \frac{ke^{2t}+1}{ke^{2t}-1}$$ I'd have thought that solution was right, but we have to figure out a specific solution with y(0) = 0. But this isn't possible with the above equation.","I'm trying to solve the equation $$y' = 1 - y^2$$ Here is my attempt: $$y' = 1 - y^2$$ Divide by (1-y^2) $$\frac{y'}{1-y^2} = 1$$ Integrate both sides: $$\frac{1}{2}\log|\frac{y+1}{y-1}|=t+c$$ Rearrange $$y = \frac{ke^{2t}+1}{ke^{2t}-1}$$ I'd have thought that solution was right, but we have to figure out a specific solution with y(0) = 0. But this isn't possible with the above equation.",,['ordinary-differential-equations']
57,How to solve DE that relate values of derivatives at different points?,How to solve DE that relate values of derivatives at different points?,,I try to solve for the specific function $f(x) = \frac{2-2a}{x-1} \int_0^{x-1} f(y) dy + af(x-1)$ It looks similar to the function used to find the Renyi's parking constant because it came out from a simple generalization of that problem. The skill I have gained in my differential class can't even solve $f(x) = f'(x-1)$ I'm not looking for anyone to solve it. I just want to know the techniques for solving DE where functions and it's derivatives are evaluated at different points.(What's the terminology for this kind of DE?),I try to solve for the specific function $f(x) = \frac{2-2a}{x-1} \int_0^{x-1} f(y) dy + af(x-1)$ It looks similar to the function used to find the Renyi's parking constant because it came out from a simple generalization of that problem. The skill I have gained in my differential class can't even solve $f(x) = f'(x-1)$ I'm not looking for anyone to solve it. I just want to know the techniques for solving DE where functions and it's derivatives are evaluated at different points.(What's the terminology for this kind of DE?),,"['ordinary-differential-equations', 'functional-equations']"
58,Discretization Error of Mirror Descent,Discretization Error of Mirror Descent,,"It is well known that for sufficiently differentiable functions $f$ and small $\eta>0$ the iterate given by gradient descent $$ x_{k+1}=x_k-\eta \nabla f(x_k)$$ is within $\mathcal O(\eta^2)$ of the gradient flow solution $$\dot x(t)=-\nabla f(x(t)), \hspace{1cm}x(0)=x_k$$ which is a consequence of Taylor's Theorem. Now Mirror Descent is a generalization of Gradient Descent, where one considers a strongly convex potential $\psi:\mathbb R^d\rightarrow \mathbb R$ and performs the update $$x_{k+1}=(\nabla\psi)^{-1}\bigg(\nabla\psi(x_k)-\eta \nabla f(x_k)\bigg).\hspace{2cm}(MD)$$ One may similarly derive a continuous-time dynamic given by noting that $$\frac{\nabla \psi(x_{k+1})-\nabla\psi(x_k)}{\eta}=-\nabla f(x_k)$$ and thus the most natural candidate for an ODE describing Mirror descent is given by $$\frac{d}{dt}\nabla\psi(x(t))=-\nabla f(x(t))$$ which in turn yields $$\dot x(t)=-\nabla^2\psi(x(t))^{-1}\cdot \nabla f(x(t)). \hspace{2cm}(CMD)$$ Question: Can we derive a similar error estimate between the Mirror Descent updates (MD) and the continuous-time Mirror Descent (CMD)? (I am particularly interested in the case $\psi(x)=p^{-1}\Vert x\Vert_p^p$ .) ( You may assume $\psi$ to be as smooth as you need it to be ) The naive approach of performing a Taylor Approximation on (CMD) does not seem to yield the desired bound.","It is well known that for sufficiently differentiable functions and small the iterate given by gradient descent is within of the gradient flow solution which is a consequence of Taylor's Theorem. Now Mirror Descent is a generalization of Gradient Descent, where one considers a strongly convex potential and performs the update One may similarly derive a continuous-time dynamic given by noting that and thus the most natural candidate for an ODE describing Mirror descent is given by which in turn yields Question: Can we derive a similar error estimate between the Mirror Descent updates (MD) and the continuous-time Mirror Descent (CMD)? (I am particularly interested in the case .) ( You may assume to be as smooth as you need it to be ) The naive approach of performing a Taylor Approximation on (CMD) does not seem to yield the desired bound.","f \eta>0  x_{k+1}=x_k-\eta \nabla f(x_k) \mathcal O(\eta^2) \dot x(t)=-\nabla f(x(t)), \hspace{1cm}x(0)=x_k \psi:\mathbb R^d\rightarrow \mathbb R x_{k+1}=(\nabla\psi)^{-1}\bigg(\nabla\psi(x_k)-\eta \nabla f(x_k)\bigg).\hspace{2cm}(MD) \frac{\nabla \psi(x_{k+1})-\nabla\psi(x_k)}{\eta}=-\nabla f(x_k) \frac{d}{dt}\nabla\psi(x(t))=-\nabla f(x(t)) \dot x(t)=-\nabla^2\psi(x(t))^{-1}\cdot \nabla f(x(t)). \hspace{2cm}(CMD) \psi(x)=p^{-1}\Vert x\Vert_p^p \psi","['ordinary-differential-equations', 'optimization', 'numerical-methods', 'gradient-descent', 'gradient-flows']"
59,Why is this approximate solution correct?,Why is this approximate solution correct?,,"Consider the following differential equation $$ y''=-y + \alpha y |y|^2, $$ where $y=y(x)$ is complex in general and $\alpha$ is a real constant such that the second term is small compared to $y$ ( $||^2$ is the absolute square). Numerically I find that a good approximation for this is found by taking the solution for $\alpha=0$ , which is $ae^{ix}+be^{-ix}$ (with $a$ and $b$ determined by the initial conditions) and applying a little shift to the exponents: $ae^{i(1+\Delta_a)x}+be^{-i(1+\Delta_b)x}$ , where $\Delta_a=-\alpha(|a|^2+2|b|^2)/2$ and $\Delta_b=-\alpha(2|a|^2+|b|^2)/2$ . I want to find a simple justification for this approximate solution. My attempt so far is this: assuming the solution $ae^{i(1+\Delta_a)x}+be^{-i(1+\Delta_b)x}$ , and inserting it in the differential equation, we find (neglecting the $\Delta^2$ terms, and expanding the absolute square): $$\Delta_a ae^{i(1+\Delta_a)x}+\Delta_b be^{-i(1+\Delta_b)x}=-\alpha/2 \bigg [ (|a|^2+2|b|^2)ae^{i(1+\Delta_a)x} + (2|a|^2+|b|^2)be^{-i(1+\Delta_b)x} +ab^*ae^{i(3+2\Delta_a+\Delta_b)x}+ba^*be^{-i(3+\Delta_a+2\Delta_b)x}  \bigg ]$$ By matching the exponentials of same power, we directly find the expression of the $\Delta$ s. However there are those two (roughly-) third power terms left, that seem to indicate at first that the assumed form is not correct (because these are not small compared to the first two terms). And yet the $\Delta$ s found in this way give a very good approximation. (You can choose initial conditions such that the last two terms are small compared to the first two, but my approximate solution is good even when this is not the case.) How can we somehow disregard the last two terms? Or can we justify the $\Delta$ s in a different way?","Consider the following differential equation where is complex in general and is a real constant such that the second term is small compared to ( is the absolute square). Numerically I find that a good approximation for this is found by taking the solution for , which is (with and determined by the initial conditions) and applying a little shift to the exponents: , where and . I want to find a simple justification for this approximate solution. My attempt so far is this: assuming the solution , and inserting it in the differential equation, we find (neglecting the terms, and expanding the absolute square): By matching the exponentials of same power, we directly find the expression of the s. However there are those two (roughly-) third power terms left, that seem to indicate at first that the assumed form is not correct (because these are not small compared to the first two terms). And yet the s found in this way give a very good approximation. (You can choose initial conditions such that the last two terms are small compared to the first two, but my approximate solution is good even when this is not the case.) How can we somehow disregard the last two terms? Or can we justify the s in a different way?"," y''=-y + \alpha y |y|^2,  y=y(x) \alpha y ||^2 \alpha=0 ae^{ix}+be^{-ix} a b ae^{i(1+\Delta_a)x}+be^{-i(1+\Delta_b)x} \Delta_a=-\alpha(|a|^2+2|b|^2)/2 \Delta_b=-\alpha(2|a|^2+|b|^2)/2 ae^{i(1+\Delta_a)x}+be^{-i(1+\Delta_b)x} \Delta^2 \Delta_a ae^{i(1+\Delta_a)x}+\Delta_b be^{-i(1+\Delta_b)x}=-\alpha/2 \bigg [ (|a|^2+2|b|^2)ae^{i(1+\Delta_a)x} + (2|a|^2+|b|^2)be^{-i(1+\Delta_b)x} +ab^*ae^{i(3+2\Delta_a+\Delta_b)x}+ba^*be^{-i(3+\Delta_a+2\Delta_b)x}  \bigg ] \Delta \Delta \Delta","['ordinary-differential-equations', 'nonlinear-system', 'perturbation-theory', 'nonlinear-dynamics']"
60,Proving my IVP for a Piecewise Decay Function (Diff Eq),Proving my IVP for a Piecewise Decay Function (Diff Eq),,"Setup So... I kinda handled most of my proof but I need help with some of the stuff I just kinda went with until it worked out. The problem relates to medicine and its decay in the body. We are given that the medicine will release over a period of $b$ hours and another dose is given at time $T$ . Known Values Decay Constant is 1. Each dose contains 1 gram of medicine. We know that $b=\frac{5}{4}$ & $T=\frac{5}{2}$ . We are also given that $y(0)=0$ y being the amount of medicine in the body at time $t$ . Finally we're given the simple equation $rate=rate_{in}-rate_{out}$ . Looking at our values we find that $\frac{4}{5}$ grams are released per hour over the course of $\frac{5}{4}$ hours. Using this and our value of T it's possible to make a piecewise function for the release $(rate_{in})$ of the medicine, $g(t)$ . This function is ""on"" over the intervals $0<t<\frac{5}{4}$ & $\frac{5}{2}<t<\frac{15}{4}$ (note that $\frac{15}{4}$ just comes from $\frac{5}{2}+\frac{5}{4}$ ) and is ""off"" (to explain only having decay) between $\frac{5}{4}<t<\frac{5}{2}$ & $\frac{15}{4}<t<5$ (Idk if those should be $\le$ signs or not but that's less important right now). I only know how to turn this into a rectangular window function like: $u(\frac{5}{4})-u(\frac{5}{2})$ and so on and so on but regardless... we can gather that because our $rate_{out}$ is reliant of the amount of grams present, that it's just something times our $y(t)$ function. Okay here's the two parts I need help on (three technically because I'm still not happy with my explanation for why the out function is $y(t)$ : What I Need Help With I set up an initial value problem that looks like this: $y'(t)=\frac{4}{5}g(t)-\frac{4}{5}y(t)$ and I'm pretty confident it's right.... but I don't know how to explain why I multiplied by $\frac{4}{5}$ (I just guessed based off of searches I had been doing but I've lost what I actually searched up) because the only thing I can think of is that because $\frac{4}{5}$ is $\frac{g}{h}$ that means that there's SOME kinda rate of change equal to $\frac{4}{5}$ but surely it can't be $\frac{dy}{dt}$ since then our IVP wouldn't work right? I guess it could also be a way of making our $rate=rate_{in}-rate_{out}$ equation EQUAL $\frac{4}{5}$ or something silly? Idk, this one probably needs more help than the next. Also once I found that the Laplace transform of this solves to: $\frac{\frac{4}{5}G(s)}{s+\frac{4}{5}}$ I then basically tried to get $G(s)$ to equal the laplace of what I thought was the rectangular window function: $g(t)=(u(t)-u(t-\frac{5}{4}))+(u(t-\frac{5}{4})-u(t-\frac{5}{2}))$ but you can already see the problem right there. Stuff would cancel out. I remedied this by realizing that the equation through the next bounds $(\frac{5}{4}<t<\frac{5}{2})$ would have to pass through the last point given by the equation in the previous bounds... or in other words, to find the correct $y(t)$ we'd have to multiply say $y(t-b)\cdot(u(t-b)-u(t-c))$ by the result of $y(b)\cdot(1-u(b-b))$ (1 comes from $u(t)=1$ ) and so on and so forth and so I ended up getting: $y(t)=(1-e^{-\frac{4}{5}t})(u(t)-u(t-\frac{5}{4}))+(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})((u(t-\frac{5}{4})-u(t-\frac{5}{2}))+(e^{-\frac{4}{5}(t-\frac{5}{2})})(1-(1-(e-1)e^{-2}))((u(t-\frac{5}{2})-u(t-\frac{15}{4}))+etc$ but see the problem is that we get that weird $1-(e-1)e^{-2}$ for the 3rd window which I can't come up with a good explanation for other than some weird shenanigans turning making one of our $u$ somehow equal $u(t)$ ? Like best I got is that because technically we're looking at the point $\frac{5}{2}$ for both sides you actually get 1-whatever because the step functions are interacting with each other??? Anyways if someone's willing to help just prove or AT LEAST EXPLAIN WITH WORDS why these things work that'd be much appreciated. Just something so I can justify what I did because while it did in fact work... it's been like a day since I wrote half of this stuff down and for some reason I didn't write why it works so... EDIT: MAJOR BREAKTHROUGH ON THE SECOND HALF (Potentially) (I think I've got the first half but it doesn't hurt to check so I'm leaving it), I still need some help explaining why it works BUT, I've found that for whatever reason, you can find the bit out in front of the second window (the $(1-e^{-1})$ part of $(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})$ just by subtracting $e^{-\frac{4}{5}t}$ from $(e^{-\frac{4}{5}(t-\frac{5}{4})}$ . It's just that the $(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})$ is oversimplified, that entire equation is equal to $-e^{-\frac{4}{5}t}+e^{-\frac{4}{5}(t-\frac{5}{4})}$ The only problem is... where does that negative $e^{-\frac{4}{5}t}$ come from? My current way of thinking about this is that it's almost as if the first bound $u(t), u(t-5/4), etc$ is what's looked at during the inverse Laplace transform? Because all the unit step functions should be (in theory according to my IVP) multiplied by $1-e^{-\frac{4}{5}(t-a)}$ (a is $u(t-a)$ ) once the inverse Laplace transform is all said and done. However, if this were truly the case many things would cancel out before actually separating in order to do this leading me to believe that either the first $u(t-a)$ speaks for the entire rectangular window function OR it's something even deeper in the algebraic solving, something like: $(-e^{-\frac{4}{5}t}+e^{-\frac{4}{5}(t-\frac{5}{4})})+(1-e^{-\frac{4}{5}\left(t-\frac{5}{2}\right)})$ (this one is specifically for the third window function and I found it to be correct). Thing is... WHY? Why does that work? New Update I think I've figured out a way to make this work actually. What if we just set $g(t)=0$ when $\frac{5}{4}<t<\frac{5}{2}$ and when $\frac{15}{4}<t<5$ since we said that $g(t)$ was our rate in right? Well I think we have that by just setting $g(t)=1$ for when the medicine is active we end up with the correct function THAT I'VE KNOWN ALL THIS TIME BTW BUT DOUBTED IT WOULD BE ACCEPTED: $\left(1-e^{-\frac{4}{5}t}\right)u\left(t\right)-\left(1-e^{-\frac{4}{5}\left(t-\frac{5}{4}\right)}\right)u\left(t-\frac{5}{4}\right)+\left(1-e^{-\frac{4}{5}\left(t-\frac{5}{2}\right)}\right)u\left(t-\frac{5}{2}\right)-\left(1-e^{-\frac{4}{5}\left(t-\frac{15}{4}\right)}\right)u\left(t-\frac{15}{4}\right)-\left(1-e^{-\frac{4}{5}\left(t-5\right)}\right)u\left(t-5\right)$ . I think that this just straight up works so... idk I'll leave this up but this is probably our answer.","Setup So... I kinda handled most of my proof but I need help with some of the stuff I just kinda went with until it worked out. The problem relates to medicine and its decay in the body. We are given that the medicine will release over a period of hours and another dose is given at time . Known Values Decay Constant is 1. Each dose contains 1 gram of medicine. We know that & . We are also given that y being the amount of medicine in the body at time . Finally we're given the simple equation . Looking at our values we find that grams are released per hour over the course of hours. Using this and our value of T it's possible to make a piecewise function for the release of the medicine, . This function is ""on"" over the intervals & (note that just comes from ) and is ""off"" (to explain only having decay) between & (Idk if those should be signs or not but that's less important right now). I only know how to turn this into a rectangular window function like: and so on and so on but regardless... we can gather that because our is reliant of the amount of grams present, that it's just something times our function. Okay here's the two parts I need help on (three technically because I'm still not happy with my explanation for why the out function is : What I Need Help With I set up an initial value problem that looks like this: and I'm pretty confident it's right.... but I don't know how to explain why I multiplied by (I just guessed based off of searches I had been doing but I've lost what I actually searched up) because the only thing I can think of is that because is that means that there's SOME kinda rate of change equal to but surely it can't be since then our IVP wouldn't work right? I guess it could also be a way of making our equation EQUAL or something silly? Idk, this one probably needs more help than the next. Also once I found that the Laplace transform of this solves to: I then basically tried to get to equal the laplace of what I thought was the rectangular window function: but you can already see the problem right there. Stuff would cancel out. I remedied this by realizing that the equation through the next bounds would have to pass through the last point given by the equation in the previous bounds... or in other words, to find the correct we'd have to multiply say by the result of (1 comes from ) and so on and so forth and so I ended up getting: but see the problem is that we get that weird for the 3rd window which I can't come up with a good explanation for other than some weird shenanigans turning making one of our somehow equal ? Like best I got is that because technically we're looking at the point for both sides you actually get 1-whatever because the step functions are interacting with each other??? Anyways if someone's willing to help just prove or AT LEAST EXPLAIN WITH WORDS why these things work that'd be much appreciated. Just something so I can justify what I did because while it did in fact work... it's been like a day since I wrote half of this stuff down and for some reason I didn't write why it works so... EDIT: MAJOR BREAKTHROUGH ON THE SECOND HALF (Potentially) (I think I've got the first half but it doesn't hurt to check so I'm leaving it), I still need some help explaining why it works BUT, I've found that for whatever reason, you can find the bit out in front of the second window (the part of just by subtracting from . It's just that the is oversimplified, that entire equation is equal to The only problem is... where does that negative come from? My current way of thinking about this is that it's almost as if the first bound is what's looked at during the inverse Laplace transform? Because all the unit step functions should be (in theory according to my IVP) multiplied by (a is ) once the inverse Laplace transform is all said and done. However, if this were truly the case many things would cancel out before actually separating in order to do this leading me to believe that either the first speaks for the entire rectangular window function OR it's something even deeper in the algebraic solving, something like: (this one is specifically for the third window function and I found it to be correct). Thing is... WHY? Why does that work? New Update I think I've figured out a way to make this work actually. What if we just set when and when since we said that was our rate in right? Well I think we have that by just setting for when the medicine is active we end up with the correct function THAT I'VE KNOWN ALL THIS TIME BTW BUT DOUBTED IT WOULD BE ACCEPTED: . I think that this just straight up works so... idk I'll leave this up but this is probably our answer.","b T b=\frac{5}{4} T=\frac{5}{2} y(0)=0 t rate=rate_{in}-rate_{out} \frac{4}{5} \frac{5}{4} (rate_{in}) g(t) 0<t<\frac{5}{4} \frac{5}{2}<t<\frac{15}{4} \frac{15}{4} \frac{5}{2}+\frac{5}{4} \frac{5}{4}<t<\frac{5}{2} \frac{15}{4}<t<5 \le u(\frac{5}{4})-u(\frac{5}{2}) rate_{out} y(t) y(t) y'(t)=\frac{4}{5}g(t)-\frac{4}{5}y(t) \frac{4}{5} \frac{4}{5} \frac{g}{h} \frac{4}{5} \frac{dy}{dt} rate=rate_{in}-rate_{out} \frac{4}{5} \frac{\frac{4}{5}G(s)}{s+\frac{4}{5}} G(s) g(t)=(u(t)-u(t-\frac{5}{4}))+(u(t-\frac{5}{4})-u(t-\frac{5}{2})) (\frac{5}{4}<t<\frac{5}{2}) y(t) y(t-b)\cdot(u(t-b)-u(t-c)) y(b)\cdot(1-u(b-b)) u(t)=1 y(t)=(1-e^{-\frac{4}{5}t})(u(t)-u(t-\frac{5}{4}))+(1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})})((u(t-\frac{5}{4})-u(t-\frac{5}{2}))+(e^{-\frac{4}{5}(t-\frac{5}{2})})(1-(1-(e-1)e^{-2}))((u(t-\frac{5}{2})-u(t-\frac{15}{4}))+etc 1-(e-1)e^{-2} u u(t) \frac{5}{2} (1-e^{-1}) (1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})}) e^{-\frac{4}{5}t} (e^{-\frac{4}{5}(t-\frac{5}{4})} (1-e^{-1})(e^{-\frac{4}{5}(t-\frac{5}{4})}) -e^{-\frac{4}{5}t}+e^{-\frac{4}{5}(t-\frac{5}{4})} e^{-\frac{4}{5}t} u(t), u(t-5/4), etc 1-e^{-\frac{4}{5}(t-a)} u(t-a) u(t-a) (-e^{-\frac{4}{5}t}+e^{-\frac{4}{5}(t-\frac{5}{4})})+(1-e^{-\frac{4}{5}\left(t-\frac{5}{2}\right)}) g(t)=0 \frac{5}{4}<t<\frac{5}{2} \frac{15}{4}<t<5 g(t) g(t)=1 \left(1-e^{-\frac{4}{5}t}\right)u\left(t\right)-\left(1-e^{-\frac{4}{5}\left(t-\frac{5}{4}\right)}\right)u\left(t-\frac{5}{4}\right)+\left(1-e^{-\frac{4}{5}\left(t-\frac{5}{2}\right)}\right)u\left(t-\frac{5}{2}\right)-\left(1-e^{-\frac{4}{5}\left(t-\frac{15}{4}\right)}\right)u\left(t-\frac{15}{4}\right)-\left(1-e^{-\frac{4}{5}\left(t-5\right)}\right)u\left(t-5\right)","['ordinary-differential-equations', 'solution-verification', 'exponential-function']"
61,Proving $S_1-S_2$ is monotone decreasing,Proving  is monotone decreasing,S_1-S_2,"I have system number (1) as: $S_1’=-bS_1I_1+(1-c)aI_1$ $I_1’=bS_1I_1-aI_1$ And on the other hand I have system number (2) as: $S_2’=-bS_2I_2+aI_2$ $I_2’=bS_2I_2-aI_2$ All parameters $a,b,c>0$ . Both systems have the same initial conditions as in $S_1(0)=S_2(0)=s_0>0$ and $I_1(0)=I_2(0)=i_0>0$ . I want to prove that $S_1-S_2$ is decreasing for $s_0>\frac{a}{b}$ . What I proved so far: I proved $S_1-S_2$ and $I_1-I_2$ are negative for all $t>0$ . Using the initial conditions then I have $S_1=  \frac{(1-c)a}{b}+(s_0- \frac{(1-c)a}{b}$$ )e^{-\int_{0}^{t} bI_1(t) \ dt}$ . Similarly, I obtained $S_2=  \frac{a}{b}+(s_0- \frac{a}{b}$$ )e^{-\int_{0}^{t} bI_2(t) \ dt}$ . From 2 and 3 we conclude that $S_1,S_2$ are monotone decreasing and that for $s_0>\frac{a}{b}$ we have $\frac{-ca}{b}<S_1-S_2<0$ . Initially we have $(S_1-S_2)’<0$ and because $S_1,S_2$ are $C^\infty$ then $(S_1-S_2)$ decreases initially and $$\lim_{t\rightarrow \infty}(S_1-S_2)’(t)=0.$$ I expressed $(S_1-S_2)’=F(t)(S_1-S_2)+G(t)$ where $G(t),F(t)$ are negative ( $F(t)$ starts being negative from a certain point $t_0$ in case that $F(0)>0$ ) and descending toward finite limit. Despite all that I can’t seem to prove that $(S_1-S_2)(t)$ is decreasing for $t>0$ . I checked this via simulations and indeed it is the case but I’m trying by contradiction and can’t find something that can lead to a contradiction in anything. Help is appreciated!","I have system number (1) as: And on the other hand I have system number (2) as: All parameters . Both systems have the same initial conditions as in and . I want to prove that is decreasing for . What I proved so far: I proved and are negative for all . Using the initial conditions then I have . Similarly, I obtained . From 2 and 3 we conclude that are monotone decreasing and that for we have . Initially we have and because are then decreases initially and I expressed where are negative ( starts being negative from a certain point in case that ) and descending toward finite limit. Despite all that I can’t seem to prove that is decreasing for . I checked this via simulations and indeed it is the case but I’m trying by contradiction and can’t find something that can lead to a contradiction in anything. Help is appreciated!","S_1’=-bS_1I_1+(1-c)aI_1 I_1’=bS_1I_1-aI_1 S_2’=-bS_2I_2+aI_2 I_2’=bS_2I_2-aI_2 a,b,c>0 S_1(0)=S_2(0)=s_0>0 I_1(0)=I_2(0)=i_0>0 S_1-S_2 s_0>\frac{a}{b} S_1-S_2 I_1-I_2 t>0 S_1=  \frac{(1-c)a}{b}+(s_0- \frac{(1-c)a}{b} )e^{-\int_{0}^{t} bI_1(t) \ dt} S_2=  \frac{a}{b}+(s_0- \frac{a}{b} )e^{-\int_{0}^{t} bI_2(t) \ dt} S_1,S_2 s_0>\frac{a}{b} \frac{-ca}{b}<S_1-S_2<0 (S_1-S_2)’<0 S_1,S_2 C^\infty (S_1-S_2) \lim_{t\rightarrow \infty}(S_1-S_2)’(t)=0. (S_1-S_2)’=F(t)(S_1-S_2)+G(t) G(t),F(t) F(t) t_0 F(0)>0 (S_1-S_2)(t) t>0","['real-analysis', 'ordinary-differential-equations', 'monotone-functions']"
62,closed-form Newton flow of tanh(ln(1+x^2)),closed-form Newton flow of tanh(ln(1+x^2)),,"The differential equation for the Newton flow $z (t)$ of $f (t)$ is given by \begin{equation}   \dot{z} (t) = - \frac{f (z (t))}{\frac{d}{d t} f (z (t))} = -   \frac{f (z (t))}{\dot{f} (z (t))} \end{equation} If we let \begin{equation}   b (a) = 2 (1 + a^2) + a^4 \end{equation} then define \begin{equation}   g (t, a) = e^t - 1 + \frac{2}{b (a)} \end{equation} and \begin{equation}   h (t, a) = \sqrt{e^{2 t} - \frac{a^4 (2 + a^2)^2}{b (a)^2}} \end{equation} then there are 4 solutions of $z (t)$ for the Newton flow given by \begin{equation}   z (t,a) = \pm \sqrt{\pm \frac{g (t, a) + h (t, a)}{g (t, a)}} \end{equation} where lim_(t→∞)z(t,a)∈{0,±i√2}∀a∈ℂ and S(lim_(t→∞)z(t,a))=0","The differential equation for the Newton flow of is given by If we let then define and then there are 4 solutions of for the Newton flow given by where lim_(t→∞)z(t,a)∈{0,±i√2}∀a∈ℂ and S(lim_(t→∞)z(t,a))=0","z (t) f (t) \begin{equation}
  \dot{z} (t) = - \frac{f (z (t))}{\frac{d}{d t} f (z (t))} = -
  \frac{f (z (t))}{\dot{f} (z (t))}
\end{equation} \begin{equation}
  b (a) = 2 (1 + a^2) + a^4
\end{equation} \begin{equation}
  g (t, a) = e^t - 1 + \frac{2}{b (a)}
\end{equation} \begin{equation}
  h (t, a) = \sqrt{e^{2 t} - \frac{a^4 (2 + a^2)^2}{b (a)^2}}
\end{equation} z (t) \begin{equation}
  z (t,a) = \pm \sqrt{\pm \frac{g (t, a) + h (t, a)}{g (t, a)}}
\end{equation}","['ordinary-differential-equations', 'derivatives']"
63,"$ -\frac{1}{3}(1-x^2)G''=2(1-\frac{1}{3}G')G, x\in(-1,1)$ only admits zero solution.",only admits zero solution.," -\frac{1}{3}(1-x^2)G''=2(1-\frac{1}{3}G')G, x\in(-1,1)","Suppose there is a smooth function $u$ on $[-1,1]$ satisfying $$ -\frac{1}{3}((1-x^2)u')'+1=e^{2u}. $$ Prove that $u\equiv 0$ . If we define $G(x)=(1-x^2)u'$ , simple conputation gives $$ -\frac{1}{3}(1-x^2)G''=2(1-\frac{1}{3}G')G, x\in(-1,1), $$ and resonable to assume $G(1)=G(-1)=0$ . Thus it suffices to prove $G\equiv 0$ . I have tried to give some expressions of $\int_{-1}^1G^2$ , for example, by the ODE with respect to $G$ and the boundary value it is easy to show $$ \int_{-1}^1(1-x^2)(G')^2=5\int_{-1}^1G^2, $$ but I have no idea how to continue. This problem is actually from the post Axially symmetric solution of a PDE on $S^2$ . Appreciate any help!","Suppose there is a smooth function on satisfying Prove that . If we define , simple conputation gives and resonable to assume . Thus it suffices to prove . I have tried to give some expressions of , for example, by the ODE with respect to and the boundary value it is easy to show but I have no idea how to continue. This problem is actually from the post Axially symmetric solution of a PDE on . Appreciate any help!","u [-1,1] 
-\frac{1}{3}((1-x^2)u')'+1=e^{2u}.
 u\equiv 0 G(x)=(1-x^2)u' 
-\frac{1}{3}(1-x^2)G''=2(1-\frac{1}{3}G')G, x\in(-1,1),
 G(1)=G(-1)=0 G\equiv 0 \int_{-1}^1G^2 G 
\int_{-1}^1(1-x^2)(G')^2=5\int_{-1}^1G^2,
 S^2","['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
64,"Does the ""physicist common knowledge"" that ""solenoidal vector fields have closed integral curves"" have any mathematical foundation?","Does the ""physicist common knowledge"" that ""solenoidal vector fields have closed integral curves"" have any mathematical foundation?",,"I remember having heard some physicist claiming that the integral curves of the magnetic field have to be closed, or ""closed at $\infty$ "", due to the fact that the magnetic field is solenoidal, i.e., that its divergence is zero. I'm looking for a mathematical proof of this fact. Here's a (possible) mathematical formulation of the problem. Suppose $F:\mathbb{R}^3 \to \mathbb{R}^3$ is a $C^1$ solenoidal (i.e., such that $\mathrm{div}(F) = 0$ ) vector field. Let $I$ be an open interval and suppose that $\gamma:I\to\mathbb{R}^3$ is an integral curve for $F$ , i.e., $\gamma$ is a $C^1$ curve such that $\forall t\in I, \gamma'(t)=F\big(\gamma(t)\big)$ . Suppose that $(\gamma,I)$ is maximal, in the sense that if $J$ is another open interval such that $I \subset J$ and $\delta: J \to \mathbb{R}^3$ is another $C^1$ curve such that $\forall t\in J, \delta'(t)=F\big(\delta(t)\big)$ and $\delta \mid_I = \gamma$ , then it holds that $J=I$ . Suppose that $I = (a,b)$ , where $-\infty \le a < b \le +\infty$ . Is it true that one of the following must hold: $\lim_{t \to a} \|\gamma(t)\| = \lim_{t \to b} \|\gamma(t)\| = +\infty $ (i.e., the curve is closed at $\infty$ ), $\exists c\in(a,b), \exists d\in(c,b), \gamma(c) = \gamma(d)$ (i.e., the curve is closed)? Any proof (or pointer to a mathematical proof in the literature) or counterexample is very welcome.","I remember having heard some physicist claiming that the integral curves of the magnetic field have to be closed, or ""closed at "", due to the fact that the magnetic field is solenoidal, i.e., that its divergence is zero. I'm looking for a mathematical proof of this fact. Here's a (possible) mathematical formulation of the problem. Suppose is a solenoidal (i.e., such that ) vector field. Let be an open interval and suppose that is an integral curve for , i.e., is a curve such that . Suppose that is maximal, in the sense that if is another open interval such that and is another curve such that and , then it holds that . Suppose that , where . Is it true that one of the following must hold: (i.e., the curve is closed at ), (i.e., the curve is closed)? Any proof (or pointer to a mathematical proof in the literature) or counterexample is very welcome.","\infty F:\mathbb{R}^3 \to \mathbb{R}^3 C^1 \mathrm{div}(F) = 0 I \gamma:I\to\mathbb{R}^3 F \gamma C^1 \forall t\in I, \gamma'(t)=F\big(\gamma(t)\big) (\gamma,I) J I \subset J \delta: J \to \mathbb{R}^3 C^1 \forall t\in J, \delta'(t)=F\big(\delta(t)\big) \delta \mid_I = \gamma J=I I = (a,b) -\infty \le a < b \le +\infty \lim_{t \to a} \|\gamma(t)\| = \lim_{t \to b} \|\gamma(t)\| = +\infty  \infty \exists c\in(a,b), \exists d\in(c,b), \gamma(c) = \gamma(d)","['ordinary-differential-equations', 'dynamical-systems', 'mathematical-physics', 'vector-fields']"
65,Find largest ellipse in basin of attraction,Find largest ellipse in basin of attraction,,"Consider the following system: $$ \begin{align} x' &= -x+y^2 \\ y' &= -2y + 3x^2 \\ \end{align} $$ I am asked to find the largest ellipse of the form: $$ V(x,y) = \tfrac{x^2}{2} + \tfrac{y^2}{4} = r$$ contained in the region of attraction of the system. We can use $V(x,y)$ as a Lyapunov function of the system and we get: $$V'(x,y) = xx' + \tfrac{y}{2}y' = -x^2 +xy^2 -y^2 + \tfrac{3}{2} x^2y$$ I deduced we are looking for $r$ such that: \begin{align} r &= \min_{(x,y) \neq (0,0)} \tfrac{x^2}{2} + \tfrac{y^2}{4}\\ &\text{ s.t } -x^2 +xy^2 -y^2 + \tfrac{3}{2} x^2y=0  \end{align} I have not been able to solve this problem analytically using the Lagrangian (a 3rd-degree polynomial comes up). Wolfram Alpha solution seems to be right ( $r \simeq 0.4782$ ) when plotted though: Is this way of reasoning sound? Is there some simplification that allows us to get an analytical solution? Thanks in advance EDIT: I have found the answer in the book, apparently the correct result is $\frac{1}{9}$ . So there must be an error in my reasoning I cannot find. The book offers no solution, so I still don't know how to correctly solve the exercise.","Consider the following system: I am asked to find the largest ellipse of the form: contained in the region of attraction of the system. We can use as a Lyapunov function of the system and we get: I deduced we are looking for such that: I have not been able to solve this problem analytically using the Lagrangian (a 3rd-degree polynomial comes up). Wolfram Alpha solution seems to be right ( ) when plotted though: Is this way of reasoning sound? Is there some simplification that allows us to get an analytical solution? Thanks in advance EDIT: I have found the answer in the book, apparently the correct result is . So there must be an error in my reasoning I cannot find. The book offers no solution, so I still don't know how to correctly solve the exercise.","
\begin{align}
x' &= -x+y^2 \\
y' &= -2y + 3x^2 \\
\end{align}
  V(x,y) = \tfrac{x^2}{2} + \tfrac{y^2}{4} = r V(x,y) V'(x,y) = xx' + \tfrac{y}{2}y' = -x^2 +xy^2 -y^2 + \tfrac{3}{2} x^2y r \begin{align}
r &= \min_{(x,y) \neq (0,0)} \tfrac{x^2}{2} + \tfrac{y^2}{4}\\
&\text{ s.t } -x^2 +xy^2 -y^2 + \tfrac{3}{2} x^2y=0 
\end{align} r \simeq 0.4782 \frac{1}{9}","['ordinary-differential-equations', 'dynamical-systems']"
66,"Differential equation, tricky","Differential equation, tricky",,"How would I solve the following: $(x^2-y^2)dy+(y^2+x^2y^2)dx=0$ ? Here is what I did: $\frac{dy}{dx}(x^2-y^2)=-(y^2+x^2y^2)$ Dividing through, to leave differential on one side: $\frac{dy}{dx}=\frac{-(y^2+x^2y^2)}{x^2-y^2}$ I then proved this is a homogeneous differential equation. This is how I Proved it: Let $A(x,y)=-y^2-x^2y^2$ . $A(tx,ty)=-t^2y^2-t^2x^2t^2y^2$ When you factorize, you get: $t^2A(x,y)$ Do the same for the denominator, and make the conclusion that these are homogeneous. Is this right? I let $y=vx$ so that $\frac{dy}{dx}=x\frac{dv}{dx}+v$ However I get stuck from here, as the equation seems to not make sense for. Anyone to please guide me","How would I solve the following: ? Here is what I did: Dividing through, to leave differential on one side: I then proved this is a homogeneous differential equation. This is how I Proved it: Let . When you factorize, you get: Do the same for the denominator, and make the conclusion that these are homogeneous. Is this right? I let so that However I get stuck from here, as the equation seems to not make sense for. Anyone to please guide me","(x^2-y^2)dy+(y^2+x^2y^2)dx=0 \frac{dy}{dx}(x^2-y^2)=-(y^2+x^2y^2) \frac{dy}{dx}=\frac{-(y^2+x^2y^2)}{x^2-y^2} A(x,y)=-y^2-x^2y^2 A(tx,ty)=-t^2y^2-t^2x^2t^2y^2 t^2A(x,y) y=vx \frac{dy}{dx}=x\frac{dv}{dx}+v","['calculus', 'ordinary-differential-equations']"
67,If there is a periodic solution then every solution is periodic with the same period.,If there is a periodic solution then every solution is periodic with the same period.,,"Let $A$ be a $2\times 2 $ real matrix. Suppose the ordinary differential equation $$x'=Ax$$ has a periodic nonconstant solution $\varphi(t,x_0)$ such that $\varphi(0,x_0)=x_0$ for some $x_0 \neq (0,0),$ i.e., there exists $\tau >0$ such that $\varphi(t+\tau,x_0)=\varphi(t,x_0)$ for every $t \in \mathbb{R}.$ Prove that for every initial value $x \neq(0,0)$ we have that the solution $\varphi(t,x)$ is periodic with period $\tau$ . I think I can solve it by separating in 3 cases: $A$ is similar to $\begin{bmatrix}\lambda_1 & 0 \\ 0 & \lambda_2\end{bmatrix}$ , where $\lambda_1, \lambda_2 \in \mathbb{R}$ are the eigenvalues of $A$ . In that case, the solution will be constant, or it will go to infinity or 0. That is, it is not periodic. $A$ is similar to $\begin{bmatrix}\lambda & 1 \\ 0 & \lambda\end{bmatrix}$ , where $\lambda \in \mathbb{R}$ is the eigenvalue of $A$ with multiplicity 2. Similarly, the solution will be constant, or it will go to infinity or 0 depending on the sign of $\lambda$ . $A$ is similar to $\begin{bmatrix}a & -b \\ b & a\end{bmatrix}$ , where $a\pm bi$ are the eigenvalues of $A$ . Then we would conclude that the only way to get periodic solutions is with $a=0$ and $b\neq 0$ . And then we see that in this case all the solutions in fact will be periodic with the same period. I have two questions. Is that solution correct? Is there a cleaner or simpler solution?","Let be a real matrix. Suppose the ordinary differential equation has a periodic nonconstant solution such that for some i.e., there exists such that for every Prove that for every initial value we have that the solution is periodic with period . I think I can solve it by separating in 3 cases: is similar to , where are the eigenvalues of . In that case, the solution will be constant, or it will go to infinity or 0. That is, it is not periodic. is similar to , where is the eigenvalue of with multiplicity 2. Similarly, the solution will be constant, or it will go to infinity or 0 depending on the sign of . is similar to , where are the eigenvalues of . Then we would conclude that the only way to get periodic solutions is with and . And then we see that in this case all the solutions in fact will be periodic with the same period. I have two questions. Is that solution correct? Is there a cleaner or simpler solution?","A 2\times 2  x'=Ax \varphi(t,x_0) \varphi(0,x_0)=x_0 x_0 \neq (0,0), \tau >0 \varphi(t+\tau,x_0)=\varphi(t,x_0) t \in \mathbb{R}. x \neq(0,0) \varphi(t,x) \tau A \begin{bmatrix}\lambda_1 & 0 \\ 0 & \lambda_2\end{bmatrix} \lambda_1, \lambda_2 \in \mathbb{R} A A \begin{bmatrix}\lambda & 1 \\ 0 & \lambda\end{bmatrix} \lambda \in \mathbb{R} A \lambda A \begin{bmatrix}a & -b \\ b & a\end{bmatrix} a\pm bi A a=0 b\neq 0","['ordinary-differential-equations', 'solution-verification']"
68,$1$D mass-spring chain leading to a 'discrete ODE' in space,D mass-spring chain leading to a 'discrete ODE' in space,1,"Background (Not strictly necessary, you can jump to 'Question') Problem statement I'm studying a $1$ D system of $N$ masses, of equal mass $m$ , connected between them by springs, of equal stiffness $k$ . Masses are at starting locations $x_n\equiv x(n)=na$ where $n\in[0,..,N]$ and $a\in\mathbb{R}$ , the initial distance between masses, is a known constant parameter. No external forces are applied. System then, is modeled by $N$ constant-coefficient differential equations of the kind $$\qquad\qquad m\,\ddot u(x_n,t)+2k\,u(x_n,t)-k\,[u(x_{n+1},t)+u(x_{n-1},t)]=0\qquad n=1,..,N\qquad (1)$$ where $u(x_n,t)$ is the dispacement of mass $n$ at time $t$ , relative to its initial position $x_n$ . This set of equations account for inertia force of mass $n$ and spring force contributions of left and right-side springs, that connect mass $n$ respectively to masses $n-1$ and $n+1$ . Note : for $a\to0$ system reduces to the so-called wave equation $$u_{tt}-c^{2}\,u_{xx}=0$$ with $c=\sqrt{E/\rho}$ . To do this, just consider $m\equiv \rho Aa$ and $k\equiv EA/a$ , where $\rho$ is the volume density of the continuous rod (whose discretization creates the $1$ D system), $A$ is the cross-sectional area and $E$ is the elasticity. Finally, the fact that $\lim_{a\to 0}\,[u(x+a,t)-2\,u(x,t)+u(x-a,t)]/a^2=u_{xx}$ by definition of $2$ nd derivative. Note : Last 'Note' justify intuetively why following steps are taken, although, i'm posting this question because i want to justify them rigorously, being this, at the end, a purely mathematical problem. Solution - Step I Impose an harmonic solution: $$u(x_n,t)=u_n(t)=u_n(\omega)\,e^{-i\omega t}$$ No further justification given, i explained this to myself in the following way: If we define $u_n(t)\equiv u(x_n,t)$ and develop the system $(1)$ in matrix form we have \begin{align} \underset{M}{\underbrace{\begin{bmatrix} m&0&0&-&0\\ 0&m&0&-&0\\ 0&0&m&-&0\\ |&|&|& &|\\ 0&0&0&-&m \end{bmatrix}}} \underset{\ddot{\mathbf{u}}(t) }{\underbrace{\begin{Bmatrix} \ddot u_1(t)\\ \ddot u_2(t)\\ \ddot u_3(t)\\ |\\ \ddot u_n(t) \end{Bmatrix}}} + \underset{K}{\underbrace{\begin{bmatrix} 2k&-k&0&-&0\\ -k&2k&-k&-&0\\ 0&-k&2k&-&0\\ |&|&|& &|\\ 0&0&0&-&2k \end{bmatrix}}} \underset{\mathbf{u}(t) }{\underbrace{\begin{Bmatrix} u_1(t)\\ u_2(t)\\ u_3(t)\\ |\\ u_n(t) \end{Bmatrix}}} = \mathbf{0} \end{align} Then, being $det(M)\neq 0$ and thus matrix $M$ invertible, we can write \begin{align} \left\{\begin{matrix} \dot{\mathbf{u}}(t)=\dot{\mathbf{u}}(t)\qquad\qquad\\ \ddot{\mathbf{u}}(t)=-M^{-1}K\,\mathbf{u}(t) \end{matrix}\right. \end{align} Next, defining $\mathbf{z}(t)\equiv \begin{Bmatrix}\mathbf{u}(t)\\\dot{\mathbf{u}}(t)\end{Bmatrix}$ , we can compact last system in the form $$\dot{\mathbf{z}}(t)= \underset{A}{\underbrace{\begin{bmatrix} 0&\mathcal{I_{n\times n}}\\ M^{-1}K&0 \end{bmatrix}}}\mathbf{z}(t)$$ where $\mathcal{I}_{n\times n}$ is the Identity matrix of order $n$ Eventually, we have achieved the manageable form of the constant-coefficient linear ODE system $$\dot{\mathbf{z}}(t)=A\,\mathbf{z}(t)$$ which has solution $$\mathbf{z}(t)=e^{A(t-t_0)}\,\mathbf{z}(t_0)$$ This solution, under the condition of $A$ being diagonalizable * (see this stack question about ), can be reduced in the form $$\mathbf{z}(t)=\sum_{i=1}^{N}c_i\,\mathbf{z}_0\,(\omega_i)\,e^{\omega_i t}$$ From this form, finally, for a specific $\omega_i$ , can be extracted the type of harmonic solution used in the quote $$\boxed{u_{n,\,\omega_i}(t)=u_n(\omega_i)\,e^{-i\omega_i t}}$$ * Note : Regarding $A$ being diagonalizable, for this case, i couldn't actually managed to carry out a proof, maybe it will be the object of a specific stack question about it Solution - Step II Impose a wave solution: $$u_n(\omega)=u_0[\kappa(\omega)]\,e^{-i\kappa(\omega) x_n}$$ My explanation: Imposing what obtained for 'Solution - Step I' , system $(1)$ reduces to \begin{align} \{-\omega_i^2\,m\,u_{n}(\omega_i)+2k\,u_{n}(\omega_i)-k\,[u_{n+1}(\omega_i)+u_{n-1}(\omega_i)]\}\,e^{-i\omega_i t}&=0\\ & n=1,..,N,\; i=1,..,N \end{align} Eventually, remembering that $u_n=u(x_n)$ , dropping $i,n$ notation and simplifying $e^{-i\omega_i t}$ as $\neq 0$ , we end up with $$-\omega^2\,m\,u(x,\omega)+2k\,u(x,\omega)-k\,[u(x+a,\omega)+u(x-a,\omega)]=0$$ Eventually, this equation should be solved with the solution provided in the second quote. Now, this is not a differential equation, although its solution is imposed to be so. Neither is an usual form i know how to deal with. From this non-understanding arises my question, which i will formulate better in the next section (there i will omit $\omega$ dependency as $\omega$ is set, i.e. a different equation is meant to exist for each $\omega$ ). Question Consider an equation of the form $$-\omega^2a^2\,u(x)+2\,c^2\,[u(x)-u(x+a)-u(x-a)]=0$$ with $u(x)\in C^2:\mathbb{R}\to\mathbb{R}$ and $a,c,\omega\in\mathbb{R}$ known parameters. So, i ask Which kind of equation is this? Does it fall in some particular category? How to find the complete set of solution for $u(x)$ ? Hint1 : Some solutions are in the form $u(x)=u_0(\kappa)\,e^{\kappa x}$ Hint2 : If we divide by $a^2$ and take the limit for $a\to 0$ , equation transforms in the $2$ nd order constant-coefficient linear ODE $\omega^2u-c^2\,u_{xx}=0$ (but $a$ is actually meant to stay not null) Hint3 : In equation there are many parameters which i just used to stay coherent with theory where equation is from, if these parameters bother you, feel free to rename them or group them My solution attempt (incomplete) Due to the presence of arguments like $x\pm a$ , and the fact that solution can have exponential form, my idea is to apply Fourier Transform at right and left hand-side, so to obtain \begin{align} -\omega^2 a^2\,\hat u(\kappa)+2 c^2\,[\hat u(\kappa)-e^{\kappa a}\,\hat u(\kappa)-e^{-\kappa a}\,\hat u(\kappa)]&=0\\ [-\omega^2 a^2+2 c^2(1-e^{\kappa a}-e^{-\kappa a})]\,\hat u(\kappa)&=0 \end{align} Eventually, looking for non-zero solutions and expanding exponential with Euler formula, we obtain $$-\omega^2 a^2+2 c^2\,[1-cos(\kappa a)]=0$$ Solving for $\kappa$ $$\kappa=\frac{1}{a}\,{cos}^{-1}\bigg(1-\frac{\omega^2 a^2}{2\,c^2}\bigg)$$ Then, $\kappa$ can have either two distinct solutions or one solution, based on parameters value. To find back $u(x)$ , we can compute anti-Fourier transform, so to have \begin{align} u(x)&=\frac 1 {2\pi}\,\int_{-\infty}^{+\infty}\hat u(\kappa)\,e^{i\kappa x}\,d\kappa\\ &=\kappa\in[\kappa_1,\kappa_2] \text{ or } \kappa=\kappa_1\text{ (trouble fomalizing this step in the integration)}\\ &= \left\{\begin{matrix} \frac 1 {2\pi}\,\hat u(\kappa_1)\,e^{i\kappa_1 x}+\frac 1 {2\pi}\,\hat u(\kappa_2)\,e^{i\kappa_2 x}\qquad\qquad\qquad\qquad \\ [\frac 1 {2\pi}\,\hat u(\kappa_1)+\frac 1 {2\pi}\,\hat u(\kappa_1)\,x]\,e^{i\kappa_1 x}\text{(just a sound guess)} \end{matrix}\right.\\ &= \left\{\begin{matrix} u_0(\kappa_1)\,e^{i\kappa_1 x}+u_0(\kappa_2)\,e^{i\kappa_2 x}\qquad\qquad\qquad\qquad \\ [u_0(\kappa_1)+u_0(\kappa_1)\,x]\,e^{i\kappa_1 x}\text{(just a sound guess)} \end{matrix}\right. \end{align} Which, should be the complete set of solution Update : Googling 'Discrete ODE' i just found out that an equation like the one i'm looking solutions for might be called 'difference equation', and so it shall be rewritten like $$-\omega^2 a^2\,u_n+2c^2\,[u_n-u_{n+1}-u_{n-1}]=0$$","Background (Not strictly necessary, you can jump to 'Question') Problem statement I'm studying a D system of masses, of equal mass , connected between them by springs, of equal stiffness . Masses are at starting locations where and , the initial distance between masses, is a known constant parameter. No external forces are applied. System then, is modeled by constant-coefficient differential equations of the kind where is the dispacement of mass at time , relative to its initial position . This set of equations account for inertia force of mass and spring force contributions of left and right-side springs, that connect mass respectively to masses and . Note : for system reduces to the so-called wave equation with . To do this, just consider and , where is the volume density of the continuous rod (whose discretization creates the D system), is the cross-sectional area and is the elasticity. Finally, the fact that by definition of nd derivative. Note : Last 'Note' justify intuetively why following steps are taken, although, i'm posting this question because i want to justify them rigorously, being this, at the end, a purely mathematical problem. Solution - Step I Impose an harmonic solution: No further justification given, i explained this to myself in the following way: If we define and develop the system in matrix form we have Then, being and thus matrix invertible, we can write Next, defining , we can compact last system in the form where is the Identity matrix of order Eventually, we have achieved the manageable form of the constant-coefficient linear ODE system which has solution This solution, under the condition of being diagonalizable * (see this stack question about ), can be reduced in the form From this form, finally, for a specific , can be extracted the type of harmonic solution used in the quote * Note : Regarding being diagonalizable, for this case, i couldn't actually managed to carry out a proof, maybe it will be the object of a specific stack question about it Solution - Step II Impose a wave solution: My explanation: Imposing what obtained for 'Solution - Step I' , system reduces to Eventually, remembering that , dropping notation and simplifying as , we end up with Eventually, this equation should be solved with the solution provided in the second quote. Now, this is not a differential equation, although its solution is imposed to be so. Neither is an usual form i know how to deal with. From this non-understanding arises my question, which i will formulate better in the next section (there i will omit dependency as is set, i.e. a different equation is meant to exist for each ). Question Consider an equation of the form with and known parameters. So, i ask Which kind of equation is this? Does it fall in some particular category? How to find the complete set of solution for ? Hint1 : Some solutions are in the form Hint2 : If we divide by and take the limit for , equation transforms in the nd order constant-coefficient linear ODE (but is actually meant to stay not null) Hint3 : In equation there are many parameters which i just used to stay coherent with theory where equation is from, if these parameters bother you, feel free to rename them or group them My solution attempt (incomplete) Due to the presence of arguments like , and the fact that solution can have exponential form, my idea is to apply Fourier Transform at right and left hand-side, so to obtain Eventually, looking for non-zero solutions and expanding exponential with Euler formula, we obtain Solving for Then, can have either two distinct solutions or one solution, based on parameters value. To find back , we can compute anti-Fourier transform, so to have Which, should be the complete set of solution Update : Googling 'Discrete ODE' i just found out that an equation like the one i'm looking solutions for might be called 'difference equation', and so it shall be rewritten like","1 N m k x_n\equiv x(n)=na n\in[0,..,N] a\in\mathbb{R} N \qquad\qquad m\,\ddot u(x_n,t)+2k\,u(x_n,t)-k\,[u(x_{n+1},t)+u(x_{n-1},t)]=0\qquad n=1,..,N\qquad (1) u(x_n,t) n t x_n n n n-1 n+1 a\to0 u_{tt}-c^{2}\,u_{xx}=0 c=\sqrt{E/\rho} m\equiv \rho Aa k\equiv EA/a \rho 1 A E \lim_{a\to 0}\,[u(x+a,t)-2\,u(x,t)+u(x-a,t)]/a^2=u_{xx} 2 u(x_n,t)=u_n(t)=u_n(\omega)\,e^{-i\omega t} u_n(t)\equiv u(x_n,t) (1) \begin{align}
\underset{M}{\underbrace{\begin{bmatrix}
m&0&0&-&0\\
0&m&0&-&0\\
0&0&m&-&0\\
|&|&|& &|\\
0&0&0&-&m
\end{bmatrix}}}
\underset{\ddot{\mathbf{u}}(t) }{\underbrace{\begin{Bmatrix}
\ddot u_1(t)\\
\ddot u_2(t)\\
\ddot u_3(t)\\
|\\
\ddot u_n(t)
\end{Bmatrix}}}
+
\underset{K}{\underbrace{\begin{bmatrix}
2k&-k&0&-&0\\
-k&2k&-k&-&0\\
0&-k&2k&-&0\\
|&|&|& &|\\
0&0&0&-&2k
\end{bmatrix}}}
\underset{\mathbf{u}(t) }{\underbrace{\begin{Bmatrix}
u_1(t)\\
u_2(t)\\
u_3(t)\\
|\\
u_n(t)
\end{Bmatrix}}}
=
\mathbf{0}
\end{align} det(M)\neq 0 M \begin{align}
\left\{\begin{matrix}
\dot{\mathbf{u}}(t)=\dot{\mathbf{u}}(t)\qquad\qquad\\
\ddot{\mathbf{u}}(t)=-M^{-1}K\,\mathbf{u}(t)
\end{matrix}\right.
\end{align} \mathbf{z}(t)\equiv \begin{Bmatrix}\mathbf{u}(t)\\\dot{\mathbf{u}}(t)\end{Bmatrix} \dot{\mathbf{z}}(t)=
\underset{A}{\underbrace{\begin{bmatrix}
0&\mathcal{I_{n\times n}}\\
M^{-1}K&0
\end{bmatrix}}}\mathbf{z}(t) \mathcal{I}_{n\times n} n \dot{\mathbf{z}}(t)=A\,\mathbf{z}(t) \mathbf{z}(t)=e^{A(t-t_0)}\,\mathbf{z}(t_0) A \mathbf{z}(t)=\sum_{i=1}^{N}c_i\,\mathbf{z}_0\,(\omega_i)\,e^{\omega_i t} \omega_i \boxed{u_{n,\,\omega_i}(t)=u_n(\omega_i)\,e^{-i\omega_i t}} A u_n(\omega)=u_0[\kappa(\omega)]\,e^{-i\kappa(\omega) x_n} (1) \begin{align}
\{-\omega_i^2\,m\,u_{n}(\omega_i)+2k\,u_{n}(\omega_i)-k\,[u_{n+1}(\omega_i)+u_{n-1}(\omega_i)]\}\,e^{-i\omega_i t}&=0\\
& n=1,..,N,\; i=1,..,N
\end{align} u_n=u(x_n) i,n e^{-i\omega_i t} \neq 0 -\omega^2\,m\,u(x,\omega)+2k\,u(x,\omega)-k\,[u(x+a,\omega)+u(x-a,\omega)]=0 \omega \omega \omega -\omega^2a^2\,u(x)+2\,c^2\,[u(x)-u(x+a)-u(x-a)]=0 u(x)\in C^2:\mathbb{R}\to\mathbb{R} a,c,\omega\in\mathbb{R} u(x) u(x)=u_0(\kappa)\,e^{\kappa x} a^2 a\to 0 2 \omega^2u-c^2\,u_{xx}=0 a x\pm a \begin{align}
-\omega^2 a^2\,\hat u(\kappa)+2 c^2\,[\hat u(\kappa)-e^{\kappa a}\,\hat u(\kappa)-e^{-\kappa a}\,\hat u(\kappa)]&=0\\
[-\omega^2 a^2+2 c^2(1-e^{\kappa a}-e^{-\kappa a})]\,\hat u(\kappa)&=0
\end{align} -\omega^2 a^2+2 c^2\,[1-cos(\kappa a)]=0 \kappa \kappa=\frac{1}{a}\,{cos}^{-1}\bigg(1-\frac{\omega^2 a^2}{2\,c^2}\bigg) \kappa u(x) \begin{align}
u(x)&=\frac 1 {2\pi}\,\int_{-\infty}^{+\infty}\hat u(\kappa)\,e^{i\kappa x}\,d\kappa\\
&=\kappa\in[\kappa_1,\kappa_2] \text{ or } \kappa=\kappa_1\text{ (trouble fomalizing this step in the integration)}\\
&=
\left\{\begin{matrix}
\frac 1 {2\pi}\,\hat u(\kappa_1)\,e^{i\kappa_1 x}+\frac 1 {2\pi}\,\hat u(\kappa_2)\,e^{i\kappa_2 x}\qquad\qquad\qquad\qquad
\\
[\frac 1 {2\pi}\,\hat u(\kappa_1)+\frac 1 {2\pi}\,\hat u(\kappa_1)\,x]\,e^{i\kappa_1 x}\text{(just a sound guess)}
\end{matrix}\right.\\
&=
\left\{\begin{matrix}
u_0(\kappa_1)\,e^{i\kappa_1 x}+u_0(\kappa_2)\,e^{i\kappa_2 x}\qquad\qquad\qquad\qquad
\\
[u_0(\kappa_1)+u_0(\kappa_1)\,x]\,e^{i\kappa_1 x}\text{(just a sound guess)}
\end{matrix}\right.
\end{align} -\omega^2 a^2\,u_n+2c^2\,[u_n-u_{n+1}-u_{n-1}]=0","['ordinary-differential-equations', 'discrete-mathematics', 'partial-differential-equations', 'recurrence-relations', 'fourier-transform']"
69,Second order inhomogeneous equation: $y''-2xy'-11y=e^{-ax}$,Second order inhomogeneous equation:,y''-2xy'-11y=e^{-ax},"My question relates to an a second order inhomogeneous equation: $$y''-2xy'-11y=e^{-ax}$$ First I need to investigate the homogeneous equation: $$y''-2xy'-11y=0$$ $$y''-2xy'=11y$$ Forms Hermite's Equation where $\lambda = 11$ So I need a general solution of the homogeneous equation $Ly(x) = 0$ . To do this I need to linearly independent solutions, $y_1(x)$ and $y_2(x)$ say, and then the general solution of $Ly(x) = 0$ . becomes: $$Ay_1(x) + By_2(x)$$ where $A$ and $B$ are arbitrary constants. I am struggling to find two independent solutions of the hermite equation above: My attempt If I take a solution to be if the form $$y=\sum_{n=0}^{\infty}a_{n}x^n$$ Then putting this into the ODE I get the following: $$\sum_{n=2}^{\infty}n(n-1)a_{n}x^{n-2}-2\sum_{n=1}^{\infty}na_{n}x^{n}-11\sum_{n=0}^{\infty}a_{n}x^{n}=0$$ Which I can reduce to: $$\sum_{n=0}^{\infty}[(n+2)(n+1)a_{n+2}-2(n+\frac{11}{2})a_{n}]x^{n}=0$$ And I have found that rearranging gives: $$a_{n+2} = \frac{-2(n+\frac{11}{2})a_{n}}{(n+2)(n+1)}$$ This can be used to develop a recurrence relation... How can I use this to find my two independent solutions $y_1(x)$ and $y_2(x)$ which I need in order to calculate the inhomogeneous solution? Maybe there are more efficient ways of calculating this ODE that I am unaware of. Edit I displayed the equation incorrectly and have since edited it. Should my solution be in the form of an infinite sum rather than a finite polynomial because I believe that the solution could only be constructed as a series solution which terminates if and only if $\lambda = −2n$ where $n \in \mathbb N$ . In my case $\lambda$ isn't of the above form hence an infinte series is required.","My question relates to an a second order inhomogeneous equation: First I need to investigate the homogeneous equation: Forms Hermite's Equation where So I need a general solution of the homogeneous equation . To do this I need to linearly independent solutions, and say, and then the general solution of . becomes: where and are arbitrary constants. I am struggling to find two independent solutions of the hermite equation above: My attempt If I take a solution to be if the form Then putting this into the ODE I get the following: Which I can reduce to: And I have found that rearranging gives: This can be used to develop a recurrence relation... How can I use this to find my two independent solutions and which I need in order to calculate the inhomogeneous solution? Maybe there are more efficient ways of calculating this ODE that I am unaware of. Edit I displayed the equation incorrectly and have since edited it. Should my solution be in the form of an infinite sum rather than a finite polynomial because I believe that the solution could only be constructed as a series solution which terminates if and only if where . In my case isn't of the above form hence an infinte series is required.",y''-2xy'-11y=e^{-ax} y''-2xy'-11y=0 y''-2xy'=11y \lambda = 11 Ly(x) = 0 y_1(x) y_2(x) Ly(x) = 0 Ay_1(x) + By_2(x) A B y=\sum_{n=0}^{\infty}a_{n}x^n \sum_{n=2}^{\infty}n(n-1)a_{n}x^{n-2}-2\sum_{n=1}^{\infty}na_{n}x^{n}-11\sum_{n=0}^{\infty}a_{n}x^{n}=0 \sum_{n=0}^{\infty}[(n+2)(n+1)a_{n+2}-2(n+\frac{11}{2})a_{n}]x^{n}=0 a_{n+2} = \frac{-2(n+\frac{11}{2})a_{n}}{(n+2)(n+1)} y_1(x) y_2(x) \lambda = −2n n \in \mathbb N \lambda,"['ordinary-differential-equations', 'homogeneous-equation', 'hermite-polynomials']"
70,Solving the iterated equation $f^{\circ n}(x)=f(x)^k$,Solving the iterated equation,f^{\circ n}(x)=f(x)^k,"On my spare time, I'm trying to solve equations of the form $$f^{\circ n}(x)=f(x)^k,\quad n,k\in \mathbb{Z}$$ where $f^{\circ n}(x)=f\circ f\circ\dots\circ f$ , $n$ times. I know $f(x)=x^{\sqrt[n-1]{k}}$ is a solution, but I cannot prove if there's a more general solution. How did I get the solution? I assumed that the equation had a solution of the form $x^t$ , then I solved for $t$ in : $x^{tk}=f(x)^k = f^{\circ n}(x) = x^{t^n}$ For $n=2$ and any $k$ , that is $f\circ f(x) = f(x)^k$ , I am able to prove that the only solution is $f(x)=x^k$ , without using my general solution. My problem lies when $n>2$ or $n<0$ (inverse functions). For instance, for $f\circ f\circ f(x)=f(x)$ , I know the solution $f(x)=x^{\pm1}$ works (from my general solution), but I can't prove how to get it without using my general solution, and I can't prove its uniqueness. Same goes for $n=k=-1$ , in other words $f^{-1}(x) = \frac{1}{f(x)}$ . I know $f(x)=x^{\pm i}$ is a solution (from my general solution), I can't get there without my general solution and I can't prove the uniqueness. I tried taking the derivative and solving this way (it worked for $n=2$ ), but got nowhere.","On my spare time, I'm trying to solve equations of the form where , times. I know is a solution, but I cannot prove if there's a more general solution. How did I get the solution? I assumed that the equation had a solution of the form , then I solved for in : For and any , that is , I am able to prove that the only solution is , without using my general solution. My problem lies when or (inverse functions). For instance, for , I know the solution works (from my general solution), but I can't prove how to get it without using my general solution, and I can't prove its uniqueness. Same goes for , in other words . I know is a solution (from my general solution), I can't get there without my general solution and I can't prove the uniqueness. I tried taking the derivative and solving this way (it worked for ), but got nowhere.","f^{\circ n}(x)=f(x)^k,\quad n,k\in \mathbb{Z} f^{\circ n}(x)=f\circ f\circ\dots\circ f n f(x)=x^{\sqrt[n-1]{k}} x^t t x^{tk}=f(x)^k = f^{\circ n}(x) = x^{t^n} n=2 k f\circ f(x) = f(x)^k f(x)=x^k n>2 n<0 f\circ f\circ f(x)=f(x) f(x)=x^{\pm1} n=k=-1 f^{-1}(x) = \frac{1}{f(x)} f(x)=x^{\pm i} n=2","['ordinary-differential-equations', 'function-and-relation-composition', 'iterated-function-system']"
71,Dirichlet to Neumann operator in the unit ball with Fourier Analysis,Dirichlet to Neumann operator in the unit ball with Fourier Analysis,,"I am working on exercise in Fourier analysis, but it really confused me since it involves some differential equation. Define $B_{1}:=\{x\in\mathbb{R}^{2}:x_{1}^{2}+x_{2}^{2}<1\}$ . Note that $\partial B_{1}=\mathbb{S}^{1}$ . Let $f\in C^{\infty}(\mathbb{S}^{1})$ and $u$ be the harmonic extension of $f$ to $B_{1}$ . Let $\nu$ be the unit outer normal direction of $\mathbb{S}^{1}$ , we define the Dirichlet to Neumann operator $\mathcal{A}$ by $\mathcal{A}f:=\dfrac{\partial u}{\partial \nu}.$ (a) Let $\alpha>0$ , $\alpha\notin\mathbb{N}$ . If $f\in C^{\infty}(\mathbb{S}^{1})$ is a solution to $\mathcal{A}f+\alpha=e^{f},$ then show that $f$ must be a constant, i.e. $f=\log\alpha$ . (b) What happens if $\alpha\in \mathbb{N}$ ? For the first one, I tired to use Fourier expansion of $f$ to compute the Fourier coefficients but I failed... For the second one,  what is the difference between $\alpha\in\mathbb{N}$ and $\alpha\notin\mathbb{N}$ ? I am sorry for not giving enough details since I really don't have idea about this exercise.. Thank you! Edit 1: (Partial Solution) Okay, I figured out a proof for $\alpha\notin\mathbb{N}$ .  I also worked out some part of $\alpha\in\mathbb{N}$ but could not finish, so if anyone has a refined proof, please let me know. I believe if $\alpha\notin\mathbb{N}$ does not make the solution different, but the case of $\alpha\in\mathbb{N}$ is much much more complicated, and I could not prove it completely. I have answered my own post.","I am working on exercise in Fourier analysis, but it really confused me since it involves some differential equation. Define . Note that . Let and be the harmonic extension of to . Let be the unit outer normal direction of , we define the Dirichlet to Neumann operator by (a) Let , . If is a solution to then show that must be a constant, i.e. . (b) What happens if ? For the first one, I tired to use Fourier expansion of to compute the Fourier coefficients but I failed... For the second one,  what is the difference between and ? I am sorry for not giving enough details since I really don't have idea about this exercise.. Thank you! Edit 1: (Partial Solution) Okay, I figured out a proof for .  I also worked out some part of but could not finish, so if anyone has a refined proof, please let me know. I believe if does not make the solution different, but the case of is much much more complicated, and I could not prove it completely. I have answered my own post.","B_{1}:=\{x\in\mathbb{R}^{2}:x_{1}^{2}+x_{2}^{2}<1\} \partial B_{1}=\mathbb{S}^{1} f\in C^{\infty}(\mathbb{S}^{1}) u f B_{1} \nu \mathbb{S}^{1} \mathcal{A} \mathcal{A}f:=\dfrac{\partial u}{\partial \nu}. \alpha>0 \alpha\notin\mathbb{N} f\in C^{\infty}(\mathbb{S}^{1}) \mathcal{A}f+\alpha=e^{f}, f f=\log\alpha \alpha\in \mathbb{N} f \alpha\in\mathbb{N} \alpha\notin\mathbb{N} \alpha\notin\mathbb{N} \alpha\in\mathbb{N} \alpha\notin\mathbb{N} \alpha\in\mathbb{N}","['ordinary-differential-equations', 'fourier-analysis', 'fourier-series', 'harmonic-analysis', 'partial-differential-equations']"
72,"spectrum of $-\frac{d^2}{dx^2}$ with respect to $C[0,2\pi]$ with Dirichlet boundary conditions",spectrum of  with respect to  with Dirichlet boundary conditions,"-\frac{d^2}{dx^2} C[0,2\pi]","Let $\mathcal{B}$ denote the Banach space of all continuous functions $f: [0,2\pi] \to \mathbb{C}$ such that $f(0) = f(2\pi) =0$ . Let $A$ denote the operator $$Af = -f'', \qquad \text{Dom}(A) = \{ f \in \mathcal{B} : f \in C^2[0,2\pi] \}.$$ I would like to find the spectrum of $A$ . Note that if $\sqrt{\lambda} \in \left\{\frac{n}{2} | n \in \mathbb{N} \right\}$ , then $$f(x) =  e^{i \sqrt{\lambda}x}- e^{-i \sqrt{\lambda}x} = i2\sin(\sqrt{\lambda}x) \in \text{Dom}(A)$$ is an eigenfunction of $A$ with eigenvalue $\lambda$ . Therefore the spectrum of $A$ contains $\left\{\frac{n^2}{4} | n \in \mathbb{N}\right\}.$ Showing that this is all the spectrum where I am stuck. If $\lambda \notin \left\{\frac{n^2}{4} | n \in \mathbb{N}\right\}$ , I need to conjecture a formula for the resolvent $u =(A - \lambda)^{-1}f$ , $f \in \mathcal{B}$ , and then show it is a bounded map, and that $u$ is smooth and obeys the boundary condition. My best guess is that the resolvent is described using Fourier series $$(A - \lambda)^{-1} f = \sum_{k = -\infty}^\infty \frac{e^{ikx}}{k^2 - \lambda}\hat{f}(k), \qquad \hat{f}(k) = \int^{2\pi}_0 e^{-ikt}f(t) dt,$$ or some variant thereof. Hints or solutions are greatly appreciated!","Let denote the Banach space of all continuous functions such that . Let denote the operator I would like to find the spectrum of . Note that if , then is an eigenfunction of with eigenvalue . Therefore the spectrum of contains Showing that this is all the spectrum where I am stuck. If , I need to conjecture a formula for the resolvent , , and then show it is a bounded map, and that is smooth and obeys the boundary condition. My best guess is that the resolvent is described using Fourier series or some variant thereof. Hints or solutions are greatly appreciated!","\mathcal{B} f: [0,2\pi] \to \mathbb{C} f(0) = f(2\pi) =0 A Af = -f'', \qquad \text{Dom}(A) = \{ f \in \mathcal{B} : f \in C^2[0,2\pi] \}. A \sqrt{\lambda} \in \left\{\frac{n}{2} | n \in \mathbb{N} \right\} f(x) =  e^{i \sqrt{\lambda}x}- e^{-i \sqrt{\lambda}x} = i2\sin(\sqrt{\lambda}x) \in \text{Dom}(A) A \lambda A \left\{\frac{n^2}{4} | n \in \mathbb{N}\right\}. \lambda \notin \left\{\frac{n^2}{4} | n \in \mathbb{N}\right\} u =(A - \lambda)^{-1}f f \in \mathcal{B} u (A - \lambda)^{-1} f = \sum_{k = -\infty}^\infty \frac{e^{ikx}}{k^2 - \lambda}\hat{f}(k), \qquad \hat{f}(k) = \int^{2\pi}_0 e^{-ikt}f(t) dt,","['ordinary-differential-equations', 'fourier-analysis', 'spectral-theory']"
73,Solve for $y'\cos{y}=\sin(x+y)$,Solve for,y'\cos{y}=\sin(x+y),"Solve for $y'\cos{y}=\sin(x+y)$ My attempt $$y'\cos{y}=\sin x \cos y + \cos x \sin y$$ Divide both side by $\cos y$ $$\frac{dy}{dx}=\sin x + \cos x \tan y$$ Integrate both sides and I got $$y = -\cos x + \int \cos x \tan y \,dx$$ As Maximilian Janisch said that $\int\cos(𝑥) \tan(𝑦(𝑥))d𝑥≠\sin(𝑥) \tan(𝑦(𝑥))$ I didn't know what to do next.",Solve for My attempt Divide both side by Integrate both sides and I got As Maximilian Janisch said that I didn't know what to do next.,"y'\cos{y}=\sin(x+y) y'\cos{y}=\sin x \cos y + \cos x \sin y \cos y \frac{dy}{dx}=\sin x + \cos x \tan y y = -\cos x + \int \cos x \tan y \,dx \int\cos(𝑥) \tan(𝑦(𝑥))d𝑥≠\sin(𝑥) \tan(𝑦(𝑥))","['calculus', 'ordinary-differential-equations']"
74,Rate of mixing of two regions containing particles undergoing Brownian motion,Rate of mixing of two regions containing particles undergoing Brownian motion,,"Suppose the plane is densely populated with particles at a density of $\delta_0$ per unit area, except for in a bounded region $\Omega$ , in which there are none. Suppose the particles move according to Brownian motion, and that they may wander into $\Omega$ through a small opening $\ell$ in the boundary of $\Omega$ . See my diagram: How does the density of $\Omega$ change over time? I know that as $t\to\infty$ , the density of $\Omega$ approaches $\delta_0$ . I suspect that the rate of convergence to $\delta_0$ is determined by the length of $\ell$ , and also by the parameters of the random motion. My question is: what is an expression for the density of $\Omega$ at a given time $t\in\Bbb R$ ?","Suppose the plane is densely populated with particles at a density of per unit area, except for in a bounded region , in which there are none. Suppose the particles move according to Brownian motion, and that they may wander into through a small opening in the boundary of . See my diagram: How does the density of change over time? I know that as , the density of approaches . I suspect that the rate of convergence to is determined by the length of , and also by the parameters of the random motion. My question is: what is an expression for the density of at a given time ?",\delta_0 \Omega \Omega \ell \Omega \Omega t\to\infty \Omega \delta_0 \delta_0 \ell \Omega t\in\Bbb R,['ordinary-differential-equations']
75,Solve $ y=({\rm d}y/{\rm d}x)^2 $,Solve, y=({\rm d}y/{\rm d}x)^2 ,"It is obvious that we can differentiate both sides of the equation with respect to $x$ and then discuss the result. But can we just make a square root of both sides and integrate? Sorry the original version is so naive. The following is more detailed. If we take square root $$\pm \sqrt y=\frac{{\rm d}y}{{\rm d}x}$$ then $$\frac{{\rm d}y}{\sqrt y}=\pm {\rm d}x$$ $$2\sqrt y=\pm x+c$$ The solutions are $$y=(\frac{\pm x+c}{2})^2$$ or $y=0$ If we differentiate both sides with respect to $x$ $$y'=2y'y''$$ $$y'(2y''-1)=0$$ If we choose $y'=0$ , then $y=c$ . And plug it in to the original equation, we get $c=0$ . If we choose $y''=\frac12$ , then $y=\frac14x^2+dx+f$ . Plugging in, we get $y=\frac14x^2+dx+d^2$ , which is enough. (How could the non-trivial solutions differ???-this is a solved question now.) Now, everything is great. From this example, I know that there are something a little bit different with the liner differential equation in a nonlinear one. Sometimes, one needs to plug in the solution with constants under some circumstances to determine some constant. Besides, the form of the solution of a nonlinear differential equation could be more complicated-it could be piecewise-combined. One could choose a set of solution satisfying the equation and match them at every boundary of each segment correspondingly. Thank @Allawonder and @Gae. S. for pointing them out.","It is obvious that we can differentiate both sides of the equation with respect to and then discuss the result. But can we just make a square root of both sides and integrate? Sorry the original version is so naive. The following is more detailed. If we take square root then The solutions are or If we differentiate both sides with respect to If we choose , then . And plug it in to the original equation, we get . If we choose , then . Plugging in, we get , which is enough. (How could the non-trivial solutions differ???-this is a solved question now.) Now, everything is great. From this example, I know that there are something a little bit different with the liner differential equation in a nonlinear one. Sometimes, one needs to plug in the solution with constants under some circumstances to determine some constant. Besides, the form of the solution of a nonlinear differential equation could be more complicated-it could be piecewise-combined. One could choose a set of solution satisfying the equation and match them at every boundary of each segment correspondingly. Thank @Allawonder and @Gae. S. for pointing them out.",x \pm \sqrt y=\frac{{\rm d}y}{{\rm d}x} \frac{{\rm d}y}{\sqrt y}=\pm {\rm d}x 2\sqrt y=\pm x+c y=(\frac{\pm x+c}{2})^2 y=0 x y'=2y'y'' y'(2y''-1)=0 y'=0 y=c c=0 y''=\frac12 y=\frac14x^2+dx+f y=\frac14x^2+dx+d^2,['ordinary-differential-equations']
76,"If $X,Y$ are two equivalent vector fields in two open sets $A$ and $B$, such that $A\cup B = M$. Are $X$ and $Y$ equivalent?","If  are two equivalent vector fields in two open sets  and , such that . Are  and  equivalent?","X,Y A B A\cup B = M X Y","Let $X$ and $Y$ be smooth vector fields on $\mathbb{T}^2$ . Definition 1: Let $A$ be an open subset of $\mathbb{T}^2$ , we say that $X$ and $Y$ are equivalent in $A$ , if there exists a homeomorphism $h: A \to A$ such that maps orbits of $\left.X\right|_{A}$ in orbits of $\left.Y\right|_A$ , preserving the orientation of the orbits. Definition 2: We say that $X$ and $Y$ are equivalent if $X$ and $Y$ are equivalent in $\mathbb{T}^2$ . Then my question arises My Question: If there exists $A,B \subset \mathbb{T}^2$ open subsets of $\mathbb{T}^2$ such that: $X$ and $Y$ are equivalent in $A$ and $B$ . $A \cup B = \mathbb{T}^2$ . Is it true that $X$ and $Y$ are equivalent? This seems true but I wasn't able to find a way to prove this proposition. Can anyone help me?","Let and be smooth vector fields on . Definition 1: Let be an open subset of , we say that and are equivalent in , if there exists a homeomorphism such that maps orbits of in orbits of , preserving the orientation of the orbits. Definition 2: We say that and are equivalent if and are equivalent in . Then my question arises My Question: If there exists open subsets of such that: and are equivalent in and . . Is it true that and are equivalent? This seems true but I wasn't able to find a way to prove this proposition. Can anyone help me?","X Y \mathbb{T}^2 A \mathbb{T}^2 X Y A h: A \to A \left.X\right|_{A} \left.Y\right|_A X Y X Y \mathbb{T}^2 A,B \subset \mathbb{T}^2 \mathbb{T}^2 X Y A B A \cup B = \mathbb{T}^2 X Y","['ordinary-differential-equations', 'dynamical-systems', 'vector-fields']"
77,How to arrive at $(f'(x))^2<2f(x)$?,How to arrive at ?,(f'(x))^2<2f(x),"Let we see a problem ahead of I asking what I want to ask. Define $f:\mathbb{R}\to(0,+\infty)$ differentiable satisfying $|f'(x)-f'(y)|\le|x-y|\,\, \forall x,y\in\mathbb{R}$ . Prove: $(f'(x))^2<2f(x)$ I came up with a proof which is as follows: In order to make the proof more rigorous, we pointed out in advance that $f'$ is (uniformly) continuous. 1. $\,$ If $f'(x)=0$ . Then through $f>0$ we arrive at the conclusion. 2. $\,$ If $f'(x)>0$ . $\,\,\,$ Let $x_0=x-f'(x)$ . so $f(x)=\int_{x_0}^{x}f'(t)\,dt+f(x_0)>\int_{x_0}^{x}f'(t)\,dt\ge\int_{x_0}^{x}(f'(x)+t-x)\,dt=\frac{1}{2}(f'(x))^2$ 3. $\,$ If $f'(x)<0$ . $\,\,\,$ Let $x_0=x-f'(x)$ . so $f(x)=f(x_0)-\int_{x}^{x_0}f'(t)\,dt>\int_{x}^{x_0}-f'(t)\,dt\ge\int_{x}^{x_0}(-f'(x)+x-t)\,dt=\frac{1}{2}(f'(x))^2$ Actually I got stuck on how to arrive at $(f'(x))^2<2f(x)$ . I thought $f(x)$ is influenced by $f'(t)$ where $t$ is around $x$ , so I came up with the proof as mentioned above. But at the same time I thought that for the similar questions such as $f(x)>f'(x)$ we can construct the auxiliary function $g(x)=\frac{f(x)}{e^x}$ . So what I really want to ask is how to arrive at $(f'(x))^2<2f(x)$ ? I thought this maybe relates to the solution of $(f'(x))^2=2f(x)$ . What's more, any new ideas for the above problem are welcomed. Thank you in advance!","Let we see a problem ahead of I asking what I want to ask. Define differentiable satisfying . Prove: I came up with a proof which is as follows: In order to make the proof more rigorous, we pointed out in advance that is (uniformly) continuous. 1. If . Then through we arrive at the conclusion. 2. If . Let . so 3. If . Let . so Actually I got stuck on how to arrive at . I thought is influenced by where is around , so I came up with the proof as mentioned above. But at the same time I thought that for the similar questions such as we can construct the auxiliary function . So what I really want to ask is how to arrive at ? I thought this maybe relates to the solution of . What's more, any new ideas for the above problem are welcomed. Thank you in advance!","f:\mathbb{R}\to(0,+\infty) |f'(x)-f'(y)|\le|x-y|\,\, \forall x,y\in\mathbb{R} (f'(x))^2<2f(x) f' \, f'(x)=0 f>0 \, f'(x)>0 \,\,\, x_0=x-f'(x) f(x)=\int_{x_0}^{x}f'(t)\,dt+f(x_0)>\int_{x_0}^{x}f'(t)\,dt\ge\int_{x_0}^{x}(f'(x)+t-x)\,dt=\frac{1}{2}(f'(x))^2 \, f'(x)<0 \,\,\, x_0=x-f'(x) f(x)=f(x_0)-\int_{x}^{x_0}f'(t)\,dt>\int_{x}^{x_0}-f'(t)\,dt\ge\int_{x}^{x_0}(-f'(x)+x-t)\,dt=\frac{1}{2}(f'(x))^2 (f'(x))^2<2f(x) f(x) f'(t) t x f(x)>f'(x) g(x)=\frac{f(x)}{e^x} (f'(x))^2<2f(x) (f'(x))^2=2f(x)","['real-analysis', 'ordinary-differential-equations']"
78,Hunt for exact solutions of second order ordinary differential equations with varying coefficients.,Hunt for exact solutions of second order ordinary differential equations with varying coefficients.,,"Let $a,a_1,a_2,b \in {\mathbb R}$ . Being inspired by the answer to Solve $y''(x)=[a(x^2-1)^2+b]y(x)$ we found solutions of the following second order ODE : \begin{equation} \frac{d^2 y(x)}{d x^2} + \left(  a x^4 + a_1 x^2 + a_2 x + b\right) y(x)=0 \end{equation} Indeed if we write: \begin{equation} y(x) = \exp\left( -\imath \frac{\sqrt{a}}{3} x^3 - \imath \frac{a_1}{2 \sqrt{a}} x\right) \cdot v(x) \end{equation} the function $v(x)$ satisfies the triconfluent Heun equation https://dlmf.nist.gov/31.12 . We have: \begin{equation} \frac{d^2 v(u)}{d u^2} + u(u+\gamma) \frac{d v(u)}{d u} + (\alpha u - q) v(u)=0 \end{equation} where \begin{eqnarray} \gamma &=&  \sqrt[3]{-1} 2^{5/6} \sqrt[6]{a} \sqrt{\frac{a_1}{a}}\\ \alpha &=& 1+\frac{\imath a_2}{2\sqrt{a}} \\ q &=& -\left( \frac{\sqrt[3]{-1} \left(4 \sqrt{2} a^{3/2} \sqrt{\frac{a_1}{a}}+2 i \sqrt{2} a a_2 \sqrt{\frac{a_1}{a}}+4 a b-a_1^2\right)}{4\ 2^{2/3} a^{4/3}}\right) \end{eqnarray} and \begin{equation} u:=\frac{(-1)^{1/6}}{2^{1/3} a^{1/6}}\left(x - \imath \sqrt{\frac{a_1}{(2 a)})}\right) \end{equation} Here is a code snippet that verifies our claim: a =.; a0 =.; a1 =.; a2 =.; b =.; m =.; n = -I Sqrt[a]/    3; Clear[y]; Clear[u]; Clear[v]; y[x_] = Exp[n x^3] u[x]; myeqn = Collect[(D[        y[x], {x, 2}] + (a x^4 + a1 x^2 + a2 x + b) y[         x]) Exp[-n x^3], {u[x], u'[x], u''[x]}, Simplify]; u[x_] = Exp[m x] v[x]; m = -I a1/(2 Sqrt[a]); myeqn1 = Collect[Simplify[myeqn Exp[-m x]], {v[x], v'[x], v''[x]},     Simplify]; myeqn2 = Collect[    myeqn1 /. x :> u + I Sqrt[a1/(2 a)] /. v[u + A_] :> v[u] /.       Derivative[1][v][u + A_] :> Derivative[1][v][u] /.      Derivative[2][v][u + A_] :> Derivative[2][v][u], {u[x], u'[x],      u''[x]}, Simplify]; Ab = (-1)^(1/6)/(2^(1/3) a^(1/6)); subst = {u :> Ab u, Derivative[1][v][u] :> 1/Ab Derivative[1][v][u],     Derivative[2][v][u] :> 1/(Ab)^2 Derivative[2][v][u]}; Collect[Expand[(Ab^2 myeqn2)] /. subst /. v[Ab u] :> v[u], {v[u],    v'[u], v''[u], u^_}, Simplify] Update: Now let $a$ , $a_0$ , $a_1$ , $a_2$ and $b$ be real numbers. Likewise consider another second order ODE. We have: \begin{equation} \frac{d^2 y(x)}{d x^2} + \left( \frac{a}{x^4} + \frac{a_0}{x^3} + \frac{a_1}{x^2} + \frac{a_2}{x} +b\right) y(x)=0 \end{equation} Then by writing : \begin{equation} y(x)= x^{1+\frac{a_0}{2 \imath \sqrt{a}}} \exp\left[\imath \left(\frac{\sqrt{a}}{x} + \sqrt{b} x  \right)\right] \cdot v(x) \end{equation} The function $v$ satisfies the doubly-confluent Heun equation. We have: \begin{equation} \frac{d^2 v(u)}{d u^2} + \left( \frac{\delta}{u^2} + \frac{\gamma}{u} + 1\right) \frac{d v(u)}{d u} + \frac{\alpha u-q}{u^2} v(u) = 0 \end{equation} where: \begin{eqnarray} \delta &=& 4 \sqrt{a b}\\ \gamma &=&2 - \frac{\imath a_0}{\sqrt{a}}\\ \alpha &=& 1-\frac{\imath a_0}{2 \sqrt{a}} - \frac{\imath a_2}{2 \sqrt{b}}\\ q &=& \frac{\imath a_0}{2 \sqrt{a}} + \frac{a_0^2}{4 a}-a_1-2 \sqrt{a b} \end{eqnarray} and $u:=x/(2 \imath \sqrt{b})$ . The following Mathematica code snippet provides the ""proof"". We have: a =.; a1 =.; a2 =.; b =.; a0 =.; m =.; n =.; p =.; Clear[y]; \ Clear[v]; Clear[m]; x =.; m[x_] = x^(1 + a0/(2 I Sqrt[a])) Exp[I (Sqrt[a]/x + Sqrt[b] x)] ; y[x_] = m[x] v[x]; myeqn = Collect[    Simplify[(D[         y[x], {x, 2}] + (a /x^4 + a0 /x^3 + a1 /x^2 + a2 /x + b) y[          x])/m[x]], {v[x], v'[x], v''[x]}, Simplify]; myeqn = Collect[Simplify[myeqn ], {v[x], v'[x], v''[x], x^_},     Simplify]; Ab = 1/(2 I Sqrt[b]); subst = {x :> Ab x, Derivative[1][v][x] :> 1/Ab Derivative[1][v][x],     Derivative[2][v][x] :> 1/(Ab)^2 Derivative[2][v][x]}; Collect[Expand[(Ab^2 myeqn)] /. subst /. v[Ab x] :> v[x], {v[x],    v'[x], v''[x], x^_}, Simplify] Finally let $a$ , $a_0$ , $a_1$ , $a_2$ and $b$ be real numbers. Consider the following ODE. We have: \begin{equation} \frac{d^2 y(x)}{d x^2} + \left( a x^2 + a_0 x + a_1 + \frac{a_2}{x} +\frac{b}{x^2}\right) y(x)=0 \end{equation} Then by writing: \begin{equation} y(x)=\exp\left( -\frac{\imath}{2\sqrt{a}} x(a_0+a x)\right) \cdot x^{\frac{1}{2}(1+\sqrt{1-4 b})} \cdot v(x) \end{equation} the function $v$ satisfies the biconfluent Heun equation. We have: \begin{equation} \frac{d^2 v(u)}{d u^2} -\left( \frac{\gamma}{u} + \delta + u\right)\frac{d v(u)}{d u} + \frac{\alpha u - q}{u} v(u) = 0 \end{equation} where \begin{eqnarray} \delta &=& -\frac{1}{2}\left( 1-\imath \right) \frac{a_0}{a^{3/4}}\\ \gamma &=& - 1-\sqrt{1-4 b}\\ \alpha &=& \frac{4 a^{3/2} \left(\sqrt{1-4 b}+2\right)+4 \imath a a_1-\imath a_0^2}{8 a^{3/2}}\\ q &=& -\frac{(2+2 \imath) \sqrt{a} a_2+(1-i) a_0 \left(\sqrt{1-4 b}+1\right)}{4 a^{3/4}} \end{eqnarray} and $u:=(-1)^{1/4} x/(\sqrt{2} a^{1/4})$ . Again we used Mathematica to verify the result: Clear[v]; Clear[y]; a =.; a0 =.; a1 =.; a2 =.; b =.; A =.; d =.; \ Clear[m]; Clear[y]; Clear[v];  m[x_] = E^(-((I x (a0 + a x))/(2 Sqrt[a]))) x^(    1/2 (1 + Sqrt[1 - 4 b])); y[x_] = m[x] v[x]; ll = Collect[    Simplify[(D[         y[x], {x, 2}] + (a x^2 + a0 x + a1 + a2/x + b/x^2) y[x])/      m[x]], {v[x], v'[x], v''[x]}, Simplify]; ll = Collect[    Simplify[ll/Coefficient[ll, v''[x]]], {v[x], v'[x], v''[x], x^_},     Simplify]; Ab = (-1)^(1/4)/(Sqrt[2] a^(1/4)); subst = {x :> Ab x, Derivative[1][v][x] :> 1/Ab Derivative[1][v][x],     Derivative[2][v][x] :> 1/(Ab)^2 Derivative[2][v][x]}; ll1 = Collect[   Ab^2 (ll /. subst /. v[Ab x] :> v[x]), {v[x], v'[x], v''[x], x^_},    Simplify] Now my question would be twofold. Firstly, is there any mathematical software which can handle confluent Heun functions (just as Mathematica handles hypergeometric functions for example). Secondly, can we actually find similar solutions (i.e. map our ODE onto hte Heun equation) in the case when the coefficient at the function $y(x)$ in the ODE is an arbitrary polynomial of order strictly bigger than two ?","Let . Being inspired by the answer to Solve $y''(x)=[a(x^2-1)^2+b]y(x)$ we found solutions of the following second order ODE : Indeed if we write: the function satisfies the triconfluent Heun equation https://dlmf.nist.gov/31.12 . We have: where and Here is a code snippet that verifies our claim: a =.; a0 =.; a1 =.; a2 =.; b =.; m =.; n = -I Sqrt[a]/    3; Clear[y]; Clear[u]; Clear[v]; y[x_] = Exp[n x^3] u[x]; myeqn = Collect[(D[        y[x], {x, 2}] + (a x^4 + a1 x^2 + a2 x + b) y[         x]) Exp[-n x^3], {u[x], u'[x], u''[x]}, Simplify]; u[x_] = Exp[m x] v[x]; m = -I a1/(2 Sqrt[a]); myeqn1 = Collect[Simplify[myeqn Exp[-m x]], {v[x], v'[x], v''[x]},     Simplify]; myeqn2 = Collect[    myeqn1 /. x :> u + I Sqrt[a1/(2 a)] /. v[u + A_] :> v[u] /.       Derivative[1][v][u + A_] :> Derivative[1][v][u] /.      Derivative[2][v][u + A_] :> Derivative[2][v][u], {u[x], u'[x],      u''[x]}, Simplify]; Ab = (-1)^(1/6)/(2^(1/3) a^(1/6)); subst = {u :> Ab u, Derivative[1][v][u] :> 1/Ab Derivative[1][v][u],     Derivative[2][v][u] :> 1/(Ab)^2 Derivative[2][v][u]}; Collect[Expand[(Ab^2 myeqn2)] /. subst /. v[Ab u] :> v[u], {v[u],    v'[u], v''[u], u^_}, Simplify] Update: Now let , , , and be real numbers. Likewise consider another second order ODE. We have: Then by writing : The function satisfies the doubly-confluent Heun equation. We have: where: and . The following Mathematica code snippet provides the ""proof"". We have: a =.; a1 =.; a2 =.; b =.; a0 =.; m =.; n =.; p =.; Clear[y]; \ Clear[v]; Clear[m]; x =.; m[x_] = x^(1 + a0/(2 I Sqrt[a])) Exp[I (Sqrt[a]/x + Sqrt[b] x)] ; y[x_] = m[x] v[x]; myeqn = Collect[    Simplify[(D[         y[x], {x, 2}] + (a /x^4 + a0 /x^3 + a1 /x^2 + a2 /x + b) y[          x])/m[x]], {v[x], v'[x], v''[x]}, Simplify]; myeqn = Collect[Simplify[myeqn ], {v[x], v'[x], v''[x], x^_},     Simplify]; Ab = 1/(2 I Sqrt[b]); subst = {x :> Ab x, Derivative[1][v][x] :> 1/Ab Derivative[1][v][x],     Derivative[2][v][x] :> 1/(Ab)^2 Derivative[2][v][x]}; Collect[Expand[(Ab^2 myeqn)] /. subst /. v[Ab x] :> v[x], {v[x],    v'[x], v''[x], x^_}, Simplify] Finally let , , , and be real numbers. Consider the following ODE. We have: Then by writing: the function satisfies the biconfluent Heun equation. We have: where and . Again we used Mathematica to verify the result: Clear[v]; Clear[y]; a =.; a0 =.; a1 =.; a2 =.; b =.; A =.; d =.; \ Clear[m]; Clear[y]; Clear[v];  m[x_] = E^(-((I x (a0 + a x))/(2 Sqrt[a]))) x^(    1/2 (1 + Sqrt[1 - 4 b])); y[x_] = m[x] v[x]; ll = Collect[    Simplify[(D[         y[x], {x, 2}] + (a x^2 + a0 x + a1 + a2/x + b/x^2) y[x])/      m[x]], {v[x], v'[x], v''[x]}, Simplify]; ll = Collect[    Simplify[ll/Coefficient[ll, v''[x]]], {v[x], v'[x], v''[x], x^_},     Simplify]; Ab = (-1)^(1/4)/(Sqrt[2] a^(1/4)); subst = {x :> Ab x, Derivative[1][v][x] :> 1/Ab Derivative[1][v][x],     Derivative[2][v][x] :> 1/(Ab)^2 Derivative[2][v][x]}; ll1 = Collect[   Ab^2 (ll /. subst /. v[Ab x] :> v[x]), {v[x], v'[x], v''[x], x^_},    Simplify] Now my question would be twofold. Firstly, is there any mathematical software which can handle confluent Heun functions (just as Mathematica handles hypergeometric functions for example). Secondly, can we actually find similar solutions (i.e. map our ODE onto hte Heun equation) in the case when the coefficient at the function in the ODE is an arbitrary polynomial of order strictly bigger than two ?","a,a_1,a_2,b \in {\mathbb R} \begin{equation}
\frac{d^2 y(x)}{d x^2} + \left(  a x^4 + a_1 x^2 + a_2 x + b\right) y(x)=0
\end{equation} \begin{equation}
y(x) = \exp\left( -\imath \frac{\sqrt{a}}{3} x^3 - \imath \frac{a_1}{2 \sqrt{a}} x\right) \cdot v(x)
\end{equation} v(x) \begin{equation}
\frac{d^2 v(u)}{d u^2} + u(u+\gamma) \frac{d v(u)}{d u} + (\alpha u - q) v(u)=0
\end{equation} \begin{eqnarray}
\gamma &=&  \sqrt[3]{-1} 2^{5/6} \sqrt[6]{a} \sqrt{\frac{a_1}{a}}\\
\alpha &=& 1+\frac{\imath a_2}{2\sqrt{a}} \\
q &=& -\left( \frac{\sqrt[3]{-1} \left(4 \sqrt{2} a^{3/2} \sqrt{\frac{a_1}{a}}+2 i \sqrt{2} a a_2 \sqrt{\frac{a_1}{a}}+4 a b-a_1^2\right)}{4\ 2^{2/3} a^{4/3}}\right)
\end{eqnarray} \begin{equation}
u:=\frac{(-1)^{1/6}}{2^{1/3} a^{1/6}}\left(x - \imath \sqrt{\frac{a_1}{(2 a)})}\right)
\end{equation} a a_0 a_1 a_2 b \begin{equation}
\frac{d^2 y(x)}{d x^2} + \left( \frac{a}{x^4} + \frac{a_0}{x^3} + \frac{a_1}{x^2} + \frac{a_2}{x} +b\right) y(x)=0
\end{equation} \begin{equation}
y(x)= x^{1+\frac{a_0}{2 \imath \sqrt{a}}} \exp\left[\imath \left(\frac{\sqrt{a}}{x} + \sqrt{b} x  \right)\right] \cdot v(x)
\end{equation} v \begin{equation}
\frac{d^2 v(u)}{d u^2} + \left( \frac{\delta}{u^2} + \frac{\gamma}{u} + 1\right) \frac{d v(u)}{d u} + \frac{\alpha u-q}{u^2} v(u) = 0
\end{equation} \begin{eqnarray}
\delta &=& 4 \sqrt{a b}\\
\gamma &=&2 - \frac{\imath a_0}{\sqrt{a}}\\
\alpha &=& 1-\frac{\imath a_0}{2 \sqrt{a}} - \frac{\imath a_2}{2 \sqrt{b}}\\
q &=& \frac{\imath a_0}{2 \sqrt{a}} + \frac{a_0^2}{4 a}-a_1-2 \sqrt{a b}
\end{eqnarray} u:=x/(2 \imath \sqrt{b}) a a_0 a_1 a_2 b \begin{equation}
\frac{d^2 y(x)}{d x^2} + \left( a x^2 + a_0 x + a_1 + \frac{a_2}{x} +\frac{b}{x^2}\right) y(x)=0
\end{equation} \begin{equation}
y(x)=\exp\left( -\frac{\imath}{2\sqrt{a}} x(a_0+a x)\right) \cdot x^{\frac{1}{2}(1+\sqrt{1-4 b})} \cdot v(x)
\end{equation} v \begin{equation}
\frac{d^2 v(u)}{d u^2} -\left( \frac{\gamma}{u} + \delta + u\right)\frac{d v(u)}{d u} + \frac{\alpha u - q}{u} v(u) = 0
\end{equation} \begin{eqnarray}
\delta &=& -\frac{1}{2}\left( 1-\imath \right) \frac{a_0}{a^{3/4}}\\
\gamma &=& - 1-\sqrt{1-4 b}\\
\alpha &=& \frac{4 a^{3/2} \left(\sqrt{1-4 b}+2\right)+4 \imath a a_1-\imath a_0^2}{8 a^{3/2}}\\
q &=& -\frac{(2+2 \imath) \sqrt{a} a_2+(1-i) a_0 \left(\sqrt{1-4 b}+1\right)}{4 a^{3/4}}
\end{eqnarray} u:=(-1)^{1/4} x/(\sqrt{2} a^{1/4}) y(x)","['ordinary-differential-equations', 'special-functions']"
79,What can be done about this far-out differential equation?,What can be done about this far-out differential equation?,,"I am searching for a function $u: \mathbb{R} \to \mathbb{R}$ satisfying the differential equation: $$0=g(u') u' + g'(u') u'' u + c_1 u g(u') + c_2 u $$ Edit: we also need a starting condition: $ u(0) = c > 0 $ Where $g:\mathbb{R} \to \mathbb{R}$ is smooth and strictly monotone and $c_1,c_2 \in \mathbb{R}$ are some constants. I have a hunch that this might be a really tough question, even assuming $g$ to be linear (read: you're welcome to assume $g$ linear). Any solution/approximation technique would surprise me very much in a positive way :) As to how this equation popped up: There was a conversation about drinking coffee continuously, but at a speed changing with the temperature of the coffee. $u$ is the volume of coffee and $g$ is the inverse of a function describing the speed at which you a drink coffee at a certain temperature ($u' = g^{-1}(T)$). Using some physics: $E = c_3 u T$, $E' = c_4 A (T - T_s)$, where $A = 2 \pi r^2 + r^{-1} u$ is area of the surface of the coffee (a cylinder with height proportional to $u$) and $E$ is the thermic energy of remaining coffee, the formula above was obtained. To further detail:  The height of the cylinder is proportional to the volume as $u = h \pi r^2 \iff h = u/(\pi r^2)$ so $ A = 2 \pi r^2 + h 2 \pi r = 2 \pi r^2 + r^{-1} u$ Since $E = c_3 u T$ $$ E' = c_3 (u' T + u T') = c_4 (c_5 + c_6 u) $$ Choosing the constants so that it fits. Combining with $T = g(u') \implies T' = g'(u') u''$ we can put it all together to get the formula in question.","I am searching for a function $u: \mathbb{R} \to \mathbb{R}$ satisfying the differential equation: $$0=g(u') u' + g'(u') u'' u + c_1 u g(u') + c_2 u $$ Edit: we also need a starting condition: $ u(0) = c > 0 $ Where $g:\mathbb{R} \to \mathbb{R}$ is smooth and strictly monotone and $c_1,c_2 \in \mathbb{R}$ are some constants. I have a hunch that this might be a really tough question, even assuming $g$ to be linear (read: you're welcome to assume $g$ linear). Any solution/approximation technique would surprise me very much in a positive way :) As to how this equation popped up: There was a conversation about drinking coffee continuously, but at a speed changing with the temperature of the coffee. $u$ is the volume of coffee and $g$ is the inverse of a function describing the speed at which you a drink coffee at a certain temperature ($u' = g^{-1}(T)$). Using some physics: $E = c_3 u T$, $E' = c_4 A (T - T_s)$, where $A = 2 \pi r^2 + r^{-1} u$ is area of the surface of the coffee (a cylinder with height proportional to $u$) and $E$ is the thermic energy of remaining coffee, the formula above was obtained. To further detail:  The height of the cylinder is proportional to the volume as $u = h \pi r^2 \iff h = u/(\pi r^2)$ so $ A = 2 \pi r^2 + h 2 \pi r = 2 \pi r^2 + r^{-1} u$ Since $E = c_3 u T$ $$ E' = c_3 (u' T + u T') = c_4 (c_5 + c_6 u) $$ Choosing the constants so that it fits. Combining with $T = g(u') \implies T' = g'(u') u''$ we can put it all together to get the formula in question.",,"['ordinary-differential-equations', 'mathematical-physics']"
80,Let $X$ be a left-invariant vector field. If $\exp(X)\in Z(G)$ then $X$ is right-invariant.,Let  be a left-invariant vector field. If  then  is right-invariant.,X \exp(X)\in Z(G) X,"Let $G$ be a Lie Group and $X$ a left-invariant vector field. Show that if $\exp(X)$ is in the center of $G$, i.e $\exp(X)\in Z(G)$, then $X$ is right-invariant. I'm trying to use the fact that $X$ is a left-invariant and right-invariant vector field iff $\mathrm{Ad}(g)(X)=X$ for all $g\in G$. Using this, and the relation $C_g(\exp(X))=\exp(\mathrm{Ad}(g)(X))$, where $C_g(h)=ghg^{-1}$, we have: $$ \exp(X)=\exp(\mathrm{Ad}(g)(X)), \quad \forall g \in G. $$ But I don't know how to conclude that $\mathrm{Ad}(g)(X)=X$ for all $g\in G$.","Let $G$ be a Lie Group and $X$ a left-invariant vector field. Show that if $\exp(X)$ is in the center of $G$, i.e $\exp(X)\in Z(G)$, then $X$ is right-invariant. I'm trying to use the fact that $X$ is a left-invariant and right-invariant vector field iff $\mathrm{Ad}(g)(X)=X$ for all $g\in G$. Using this, and the relation $C_g(\exp(X))=\exp(\mathrm{Ad}(g)(X))$, where $C_g(h)=ghg^{-1}$, we have: $$ \exp(X)=\exp(\mathrm{Ad}(g)(X)), \quad \forall g \in G. $$ But I don't know how to conclude that $\mathrm{Ad}(g)(X)=X$ for all $g\in G$.",,"['ordinary-differential-equations', 'exponential-function', 'lie-groups', 'smooth-manifolds', 'vector-fields']"
81,Find the general solution in implicit form $dy/dx =xy/(x-y)$,Find the general solution in implicit form,dy/dx =xy/(x-y),"I am trying to find the solution to the following differential equation in implicit form, and I seem not to be getting anywhere: $$\frac{dy}{dx} = \frac{xy}{x-y}$$ This is not separable, but I tried separating them anyway, such that on the $dx$ side there were only $x$ terms, and then I figured the $x$ terms of the $dy$ side I could hold constant since $x$ is not a function of $y$, but I realized I don't actually know what $y$ is so I can't say that for sure. Any ideas? EDIT: I've tried it on Wolfram and it won't/can't do it. EDIT: The actual problem I had to solve was: $$\frac{dy}{dx} = \frac{xy}{x^2-2y^2}$$ I thought this was of the same form as the equation I wrote above, and that If I knew how to solve that, I could solve this. I see however that they are actually quite different. Thank you for the responses.","I am trying to find the solution to the following differential equation in implicit form, and I seem not to be getting anywhere: $$\frac{dy}{dx} = \frac{xy}{x-y}$$ This is not separable, but I tried separating them anyway, such that on the $dx$ side there were only $x$ terms, and then I figured the $x$ terms of the $dy$ side I could hold constant since $x$ is not a function of $y$, but I realized I don't actually know what $y$ is so I can't say that for sure. Any ideas? EDIT: I've tried it on Wolfram and it won't/can't do it. EDIT: The actual problem I had to solve was: $$\frac{dy}{dx} = \frac{xy}{x^2-2y^2}$$ I thought this was of the same form as the equation I wrote above, and that If I knew how to solve that, I could solve this. I see however that they are actually quite different. Thank you for the responses.",,['ordinary-differential-equations']
82,differential equation contains definite integral,differential equation contains definite integral,,"I am stuck on solving the following differential equation which contains a definite integral that I don't know how to deal with: $$ f^{\prime\prime} + a^2 f - b\int_0^L f(t) \, dt = c$$ The boundary condition is $f(0)=0$ and $f(L)=R$. Anyone help me out of here? Thank you in advance.","I am stuck on solving the following differential equation which contains a definite integral that I don't know how to deal with: $$ f^{\prime\prime} + a^2 f - b\int_0^L f(t) \, dt = c$$ The boundary condition is $f(0)=0$ and $f(L)=R$. Anyone help me out of here? Thank you in advance.",,"['ordinary-differential-equations', 'definite-integrals', 'integro-differential-equations']"
83,Solution to second order non constant differential equation without first derivative,Solution to second order non constant differential equation without first derivative,,"Cheers! So I am studying the book ‘Introduction to quantum mechanics’ by David J. Griffiths for my introductory course of quantum mechanics. On page 51 at the bottom it introduces the differential expression: \begin{equation} \Phi’’ = \xi^2 \Phi \end{equation} Without saying much further, it presents the general solution as: \begin{equation} \Phi = A e^{-\xi^2/2} + B e^{\xi^2/2} \end{equation} Indeed, when I check this solution with the differential equation, I can see that it can match it. However I am still completely missing on how one would arrive from the differential equation to the general solution. I have already been trying several different methods, all unsuccessful, so I would indeed appreciate any help I could get.","Cheers! So I am studying the book ‘Introduction to quantum mechanics’ by David J. Griffiths for my introductory course of quantum mechanics. On page 51 at the bottom it introduces the differential expression: \begin{equation} \Phi’’ = \xi^2 \Phi \end{equation} Without saying much further, it presents the general solution as: \begin{equation} \Phi = A e^{-\xi^2/2} + B e^{\xi^2/2} \end{equation} Indeed, when I check this solution with the differential equation, I can see that it can match it. However I am still completely missing on how one would arrive from the differential equation to the general solution. I have already been trying several different methods, all unsuccessful, so I would indeed appreciate any help I could get.",,['ordinary-differential-equations']
84,Laplace Transform is difficult. What to do?,Laplace Transform is difficult. What to do?,,"I am trying to solve a differential equation that has cumulative normal distributions on the non-homogenous part: $$ay'' + by' + cy = \gamma e^{mx} \operatorname{erf}(\alpha x + \beta)$$ There are many more terms on the RHS, all of which are functions of erf. The Laplace Transform is easy, but the inverse is not. Matlab is not giving me an answer. Is there something I am missing to study? What can help me deal with these type of ""hard"" laplace/inverse laplace problems?","I am trying to solve a differential equation that has cumulative normal distributions on the non-homogenous part: $$ay'' + by' + cy = \gamma e^{mx} \operatorname{erf}(\alpha x + \beta)$$ There are many more terms on the RHS, all of which are functions of erf. The Laplace Transform is easy, but the inverse is not. Matlab is not giving me an answer. Is there something I am missing to study? What can help me deal with these type of ""hard"" laplace/inverse laplace problems?",,"['ordinary-differential-equations', 'laplace-transform']"
85,Laplace Transform - Why it works,Laplace Transform - Why it works,,"The definition of the Laplace Transform is $$F(s) = \int^{\infty}_0 f(t)e^{-st}  dt$$ It is very useful in terms of solving linear, constant coefficient ordinary differential equations, but why exactly does it work? Why does taking the Laplace transform of each term in in the differential equation, using the Laplace Transform's linearity to get each individual term in the differential equation in its own Laplace Transform work in solving differential equations? Why does it work? I am not so sure why I am hesitant to accept this. Having seen generating functions of recursive sequences as a way to describe the sequence of numbers  and aids in obtaining a (closed) formula for the recursive sequence - this is a similar analogy, we are essentially ""transforming"" sequences to a power series, but I am more accepting of this idea than the Laplace transform.","The definition of the Laplace Transform is $$F(s) = \int^{\infty}_0 f(t)e^{-st}  dt$$ It is very useful in terms of solving linear, constant coefficient ordinary differential equations, but why exactly does it work? Why does taking the Laplace transform of each term in in the differential equation, using the Laplace Transform's linearity to get each individual term in the differential equation in its own Laplace Transform work in solving differential equations? Why does it work? I am not so sure why I am hesitant to accept this. Having seen generating functions of recursive sequences as a way to describe the sequence of numbers  and aids in obtaining a (closed) formula for the recursive sequence - this is a similar analogy, we are essentially ""transforming"" sequences to a power series, but I am more accepting of this idea than the Laplace transform.",,"['ordinary-differential-equations', 'laplace-transform']"
86,"$e^{tA}e^{tB}=e^{t(A+B)}e^{\frac{t^2}{2}([A,B])}$",,"e^{tA}e^{tB}=e^{t(A+B)}e^{\frac{t^2}{2}([A,B])}","Proof that: $$e^{tB}e^{tA}=e^{t(A+B)}e^{\frac{t^2}{2}([A,B])}$$ for all $t\in \mathbb{R}$ if [A,[A,B]]=[B,[A,B]]=0 Hint: $[A,B]=BA-AB$ and for hypothesis $A^2B+BA^2=2ABA$ and $AB^2+B^2A=2BAB$ $\phi(t)=e^{-t(A+B)}e^{tB}e^{tA}$ is solution of $X'=t[A,B]X$ My progress:  $\phi'(t)=-(A+B)e^{-t(A+B)}e^{tB}e^{tA}+e^{-t(A+B)}Be^{tB}e^{tA}+e^{-t(A+B)}e^{tB}Ae^{tA}$ Then: $$\phi'(t)=[-(A+B) + e^{-t(A+B)}Be^{t(A+B)} + e^{-t(A+B)}e^{tB}Ae^{-tB}e^{-t(A+B)}]e^{-t(A+B)}e^{tB}e^{tA}$$ $$\phi'(t)=[-(A+B) + e^{-t(A+B)}Be^{t(A+B)} + e^{-t(A+B)}e^{tB}Ae^{-tB}e^{-t(A+B)}]\phi(t)$$ I should have $t(BA-AB)=[-(A+B) + e^{-t(A+B)}Be^{t(A+B)} + e^{-t(A+B)}e^{tB}Ae^{-tB}e^{-t(A+B)}]$ Thanks a lot.","Proof that: $$e^{tB}e^{tA}=e^{t(A+B)}e^{\frac{t^2}{2}([A,B])}$$ for all $t\in \mathbb{R}$ if [A,[A,B]]=[B,[A,B]]=0 Hint: $[A,B]=BA-AB$ and for hypothesis $A^2B+BA^2=2ABA$ and $AB^2+B^2A=2BAB$ $\phi(t)=e^{-t(A+B)}e^{tB}e^{tA}$ is solution of $X'=t[A,B]X$ My progress:  $\phi'(t)=-(A+B)e^{-t(A+B)}e^{tB}e^{tA}+e^{-t(A+B)}Be^{tB}e^{tA}+e^{-t(A+B)}e^{tB}Ae^{tA}$ Then: $$\phi'(t)=[-(A+B) + e^{-t(A+B)}Be^{t(A+B)} + e^{-t(A+B)}e^{tB}Ae^{-tB}e^{-t(A+B)}]e^{-t(A+B)}e^{tB}e^{tA}$$ $$\phi'(t)=[-(A+B) + e^{-t(A+B)}Be^{t(A+B)} + e^{-t(A+B)}e^{tB}Ae^{-tB}e^{-t(A+B)}]\phi(t)$$ I should have $t(BA-AB)=[-(A+B) + e^{-t(A+B)}Be^{t(A+B)} + e^{-t(A+B)}e^{tB}Ae^{-tB}e^{-t(A+B)}]$ Thanks a lot.",,['ordinary-differential-equations']
87,Why do these limit cycles appear?,Why do these limit cycles appear?,,"Consider the system of ODEs \begin{align*}x'&=y+x(\varepsilon+\ell_1(x^2+y^2)+\ell_2(x^4+y^4)) \\y'&=-x+y(\varepsilon+\ell_1(x^2+y^2)+\ell_2(x^4+y^4)).\end{align*} Going through the linearization process at the equilibrium point $(0,0)$, we find the eigenvalues $\lambda_{1,2}(\varepsilon)=\varepsilon\pm i$. Using the Hopf Bifurcation theorem (p. 5-6), it is easy enough to show that a Andronov-Hopf Bifurcation appears when $\varepsilon=0$. The issue is, if $\ell_1=0$, we fail to satisfy the conditions of the Hopf Bifurcation theorem, but a bifurcation still appears. For example, suppose $\ell_1=0$ and $\ell_2=-1$. The following two images show our system for $\varepsilon=-0.1$ and $\varepsilon=0.1$, respectively. So my first question is: How would we show, analytically, that the system \begin{align*}x'&=y+x(\varepsilon+\ell_2(x^4+y^4)) \\y'&=-x+y(\varepsilon+\ell_2(x^4+y^4))\end{align*} has a Hopf Bifurcation at $\varepsilon=0$? Secondly, if $\ell_1$ and $\ell_2$ have different signs, then we can have two limit cycles, depending on $\varepsilon$. For example, suppose $\ell_1=1$ and $\ell_2=-1$. The next two images show when $\varepsilon=0.1$ and $\varepsilon=-0.1$, respectively. Now, these two limit cycles will eventually collapse on each other as $\varepsilon$ decreases, but otherwise, there is always at least one limit cycle. So my second question is: How would we show, analytically, that the system \begin{align*}x'&=y+x(\varepsilon+(x^2+y^2)-(x^4+y^4)) \\y'&=-x+y(\varepsilon+(x^2+y^2)-(x^4+y^4)) \end{align*} has a single limit cycle for all $\varepsilon>0$?","Consider the system of ODEs \begin{align*}x'&=y+x(\varepsilon+\ell_1(x^2+y^2)+\ell_2(x^4+y^4)) \\y'&=-x+y(\varepsilon+\ell_1(x^2+y^2)+\ell_2(x^4+y^4)).\end{align*} Going through the linearization process at the equilibrium point $(0,0)$, we find the eigenvalues $\lambda_{1,2}(\varepsilon)=\varepsilon\pm i$. Using the Hopf Bifurcation theorem (p. 5-6), it is easy enough to show that a Andronov-Hopf Bifurcation appears when $\varepsilon=0$. The issue is, if $\ell_1=0$, we fail to satisfy the conditions of the Hopf Bifurcation theorem, but a bifurcation still appears. For example, suppose $\ell_1=0$ and $\ell_2=-1$. The following two images show our system for $\varepsilon=-0.1$ and $\varepsilon=0.1$, respectively. So my first question is: How would we show, analytically, that the system \begin{align*}x'&=y+x(\varepsilon+\ell_2(x^4+y^4)) \\y'&=-x+y(\varepsilon+\ell_2(x^4+y^4))\end{align*} has a Hopf Bifurcation at $\varepsilon=0$? Secondly, if $\ell_1$ and $\ell_2$ have different signs, then we can have two limit cycles, depending on $\varepsilon$. For example, suppose $\ell_1=1$ and $\ell_2=-1$. The next two images show when $\varepsilon=0.1$ and $\varepsilon=-0.1$, respectively. Now, these two limit cycles will eventually collapse on each other as $\varepsilon$ decreases, but otherwise, there is always at least one limit cycle. So my second question is: How would we show, analytically, that the system \begin{align*}x'&=y+x(\varepsilon+(x^2+y^2)-(x^4+y^4)) \\y'&=-x+y(\varepsilon+(x^2+y^2)-(x^4+y^4)) \end{align*} has a single limit cycle for all $\varepsilon>0$?",,"['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
88,Solving fully nonlinear 1st order PDE,Solving fully nonlinear 1st order PDE,,"I am new to PDE and I am solving the following: $$u_x^2+u_y^2=u^2$$ with the curve $$\Gamma:\cos(s),\sin(s),1.$$ This is what I got so far. First, I set my PDE into the general first-order PDE: $$F(x,y,z,p,q)=p^2+q^2-z^2=0.$$ Then I obtained the following: $$\dfrac{dx}{dt}=F_p=2p$$ $$\dfrac{dy}{dt}=F_q=2q$$ $$\dfrac{dz}{dt}=pF_p+qF_q=2z^2$$ $$\dfrac{dp}{dt}=-F_x-pF_z=2pz$$ $$\dfrac{dq}{dt}=-F_y-qF_q=2qz.$$ By my initial curve $\Gamma,$ I need to find functions $$p=\phi(s)\text{ and }q=\psi(s)$$ such that $$F(f(s),g(s),h(s),\phi(s),\psi(s))=0$$ $$h'(s)=f'(s)\phi(s)+g'(s)\psi(s).$$ Using this relationship, I obtained the following functions: $$\phi(s)=\pm\cos(s)$$ $$\psi(s)=\pm\sin(s)$$ where the signs for my functions have to be the same. First, I am checking when both $\phi(s)$ and $\psi(s)$ have positive sign. I obtained the following solutions for my functions when I solved by system of ODEs above: $$x(s,t)=(2te^{\frac{2t}{1-2t}}+1)\cos(s)$$ $$y(s,t)=(2te^{\frac{2t}{1-2t}}+1)\sin(s)$$ $$z(s,t)=\dfrac{1}{1-2t}$$ $$p(s,t)=\cos(s)e^{\frac{2t}{1-2t}}$$ $$q(s,t)=\sin(s)e^{\frac{2t}{1-2t}}$$ I know that the next step is to solve for $s$ and $t$. Before I do this step, am I on the right track? Did I make any errors, and if so how can I make sure I avoid them when I solve first-order fully nonlinear PDEs in the future?","I am new to PDE and I am solving the following: $$u_x^2+u_y^2=u^2$$ with the curve $$\Gamma:\cos(s),\sin(s),1.$$ This is what I got so far. First, I set my PDE into the general first-order PDE: $$F(x,y,z,p,q)=p^2+q^2-z^2=0.$$ Then I obtained the following: $$\dfrac{dx}{dt}=F_p=2p$$ $$\dfrac{dy}{dt}=F_q=2q$$ $$\dfrac{dz}{dt}=pF_p+qF_q=2z^2$$ $$\dfrac{dp}{dt}=-F_x-pF_z=2pz$$ $$\dfrac{dq}{dt}=-F_y-qF_q=2qz.$$ By my initial curve $\Gamma,$ I need to find functions $$p=\phi(s)\text{ and }q=\psi(s)$$ such that $$F(f(s),g(s),h(s),\phi(s),\psi(s))=0$$ $$h'(s)=f'(s)\phi(s)+g'(s)\psi(s).$$ Using this relationship, I obtained the following functions: $$\phi(s)=\pm\cos(s)$$ $$\psi(s)=\pm\sin(s)$$ where the signs for my functions have to be the same. First, I am checking when both $\phi(s)$ and $\psi(s)$ have positive sign. I obtained the following solutions for my functions when I solved by system of ODEs above: $$x(s,t)=(2te^{\frac{2t}{1-2t}}+1)\cos(s)$$ $$y(s,t)=(2te^{\frac{2t}{1-2t}}+1)\sin(s)$$ $$z(s,t)=\dfrac{1}{1-2t}$$ $$p(s,t)=\cos(s)e^{\frac{2t}{1-2t}}$$ $$q(s,t)=\sin(s)e^{\frac{2t}{1-2t}}$$ I know that the next step is to solve for $s$ and $t$. Before I do this step, am I on the right track? Did I make any errors, and if so how can I make sure I avoid them when I solve first-order fully nonlinear PDEs in the future?",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
89,Adjoint differential equations,Adjoint differential equations,,"Consider the vector differential equations \begin{equation} \mathbf{x}^{\prime}=\mathbf{A}(t)\cdot\mathbf{x}\tag{1} \end{equation} and \begin{equation} \mathbf{y}^{\prime}=-\mathbf{A}^{\ast}(t)\cdot\mathbf{y},\tag{2} \end{equation} where $\mathbf{A}^{\ast}$ is the complex conjugate transpose of $\mathbf{A}$ and $\mathbf{x},\mathbf{y}$ are column vectors. It is well-known that (1) and (2) are said to be adjoint to one another. Further, we know that if $\mathbf{x}$ and $\mathbf{y}$ are solutions of (1) and (2), respectively, then \begin{equation} \mathbf{y}^{\ast}\cdot\mathbf{x}=\text{constant}.\notag \end{equation} Now, consider the higher-order (scalar) differential equations \begin{equation} \sum_{i=1}^{n}p_{i}(t)x^{(i)}(t)=0,\tag{3} \end{equation} where $p_{n}(t)\neq0$, and \begin{equation} \sum_{i=1}^{n}(-1)^{(i)}[p_{i}y]^{(i)}(t)=0.\tag{4} \end{equation} Also, (3) and (4) are said to be adjoint to one another. Further, if $x$ and $y$ are solutions of (3) and (4), respectively, then (see [1, (8.17) on pp. 67]) \begin{equation} \sum_{i=0}^{n}\sum_{j=0}^{i-1}(-1)^{j}x^{(i-j-1)}(t)[p_{i}z]^{(j)}(t)=\text{constant}.\label{hmfeq}\tag{*} \end{equation} The inner sum in \eqref{hmfeq} resembles the matrix multiplication formula. So, recognizing the similarities between systems and scalar equations, is it possible to obtain the result for higher-order equations by transforming them into vector equations? I could not establish any bridge here. I am experiencing problems in transforming (4) into a useful matrix representation. References [1]. P. Hartman, Ordinary Differential Equations , SIAM, 2002.","Consider the vector differential equations \begin{equation} \mathbf{x}^{\prime}=\mathbf{A}(t)\cdot\mathbf{x}\tag{1} \end{equation} and \begin{equation} \mathbf{y}^{\prime}=-\mathbf{A}^{\ast}(t)\cdot\mathbf{y},\tag{2} \end{equation} where $\mathbf{A}^{\ast}$ is the complex conjugate transpose of $\mathbf{A}$ and $\mathbf{x},\mathbf{y}$ are column vectors. It is well-known that (1) and (2) are said to be adjoint to one another. Further, we know that if $\mathbf{x}$ and $\mathbf{y}$ are solutions of (1) and (2), respectively, then \begin{equation} \mathbf{y}^{\ast}\cdot\mathbf{x}=\text{constant}.\notag \end{equation} Now, consider the higher-order (scalar) differential equations \begin{equation} \sum_{i=1}^{n}p_{i}(t)x^{(i)}(t)=0,\tag{3} \end{equation} where $p_{n}(t)\neq0$, and \begin{equation} \sum_{i=1}^{n}(-1)^{(i)}[p_{i}y]^{(i)}(t)=0.\tag{4} \end{equation} Also, (3) and (4) are said to be adjoint to one another. Further, if $x$ and $y$ are solutions of (3) and (4), respectively, then (see [1, (8.17) on pp. 67]) \begin{equation} \sum_{i=0}^{n}\sum_{j=0}^{i-1}(-1)^{j}x^{(i-j-1)}(t)[p_{i}z]^{(j)}(t)=\text{constant}.\label{hmfeq}\tag{*} \end{equation} The inner sum in \eqref{hmfeq} resembles the matrix multiplication formula. So, recognizing the similarities between systems and scalar equations, is it possible to obtain the result for higher-order equations by transforming them into vector equations? I could not establish any bridge here. I am experiencing problems in transforming (4) into a useful matrix representation. References [1]. P. Hartman, Ordinary Differential Equations , SIAM, 2002.",,"['ordinary-differential-equations', 'adjoint-operators']"
90,Solve $(y'')^2 + 2y(y') + y^2 = 0$,Solve,(y'')^2 + 2y(y') + y^2 = 0,How do I solve this differential equation? $$(y'')^2 + 2y(y') + y^2 = 0$$ I am assuming this is easy to solve. However I haven't worked on differential equations for quite some time and I'm forgetting most of it. Any hint/reference would be appreciated.,How do I solve this differential equation? $$(y'')^2 + 2y(y') + y^2 = 0$$ I am assuming this is easy to solve. However I haven't worked on differential equations for quite some time and I'm forgetting most of it. Any hint/reference would be appreciated.,,['ordinary-differential-equations']
91,The solutions of $y^{\prime \prime}+y=g$ are bounded,The solutions of  are bounded,y^{\prime \prime}+y=g,"Suppose that $g$ is a continuous differentiable, increasing and bounded real function. How can one prove that the solutions of the differential equation $(E)$ $$y^{\prime \prime}+y=g$$ are bounded? And that $(E)$ has a unique solution having a finite limit at $\infty $?","Suppose that $g$ is a continuous differentiable, increasing and bounded real function. How can one prove that the solutions of the differential equation $(E)$ $$y^{\prime \prime}+y=g$$ are bounded? And that $(E)$ has a unique solution having a finite limit at $\infty $?",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
92,Positively invariant neightbourhood using Lyapunov function,Positively invariant neightbourhood using Lyapunov function,,"Given the following system of nonlinear ODEs, $$x_1'=-x_1-x_2$$ $$x_2'=2x_1-x_2^3$$ I need to use the quadratic Lyapunov function $$V(x) = x^TQx$$ where $Q$ is a positive definite matrix such that $$A^TQ+QA=-I$$ and where $A=Df(0,0)$ , to find a neighbourhood $U$ around the origin, as large as possible, such that $U$ is positively invariant and all solutions starting in $U$ tend to the origin as $t\to\infty$ . I've carefully examined the course notes and the book by Perko ( Dynamical Systems and Differential Equations ), but couldn't find a clue how to do this problem. I'd appreciate it very much if someone could please explain: How is this neighbourhood found? How is $V$ related to all this?","Given the following system of nonlinear ODEs, I need to use the quadratic Lyapunov function where is a positive definite matrix such that and where , to find a neighbourhood around the origin, as large as possible, such that is positively invariant and all solutions starting in tend to the origin as . I've carefully examined the course notes and the book by Perko ( Dynamical Systems and Differential Equations ), but couldn't find a clue how to do this problem. I'd appreciate it very much if someone could please explain: How is this neighbourhood found? How is related to all this?","x_1'=-x_1-x_2 x_2'=2x_1-x_2^3 V(x) = x^TQx Q A^TQ+QA=-I A=Df(0,0) U U U t\to\infty V","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions', 'set-invariance']"
93,A nice way to do Euler's method on a calculator?,A nice way to do Euler's method on a calculator?,,"As part of the calculator paper for IB (International Baccalaureate), we may be asked to do Euler's method, for say, 10 iterations. While it is feasible to do with a calculator (slightly easier if you have 2 calculators), it is quite a nuisance, and it is quite easy to make a mistake when I have to retype the numbers back into the calculator. The problem here is that calculators can only store 1 answer ( Ans ) at a time, and that any attempt to store your answer into a memory slot would delete your calculation line which you would have to type in again. On the other hand, you can easily use the calculator to execute recursive functions which only rely on 1 variable. For example, I can calculate the $n$th term of the following recursive sequence by repeatedly pressing the = button: $$T_{n+1}=T_n^3+\sqrt{T_n}-\frac{1}{T_n},\, T_0=5$$ In this example, I would first press 5 = and then type in ${Ans}^3+\sqrt{Ans}-\frac{1}{Ans}$ and press = $n$ times to get $T_n$. However, this is not possible with Euler's method which has recursive formulas: $$x_{n+1}=x_n+h$$ $$y_{n+1}=y_n+h\frac{dy_n}{dx_n}$$ where $h$ is the step (which can be pre-stored in a memory slot) and $\frac{dy_n}{dx_n}=f(x_n, y_n)$. We are allowed to use a graphical calculator (GDC) which has several functions. I have found a way to do this using the spreadsheet function which is quite fiddly, but at least avoids copying mistakes when retyping numbers into my calculator. I was wondering if there were yet easier methods to do this, so I shall ask for the following methods, if possible: Method A I would prefer it if a method with the following restrictions could be used to execute Euler's method: You have a calculator which is an ordinary scientific calculator which has the ability to store the previous answer (Ans). You have the ability to type in a whole function in terms of (Ans) as shown in the first example. Is it possible to type in a one-liner which would do Euler's method by repeatedly pressing = ? *Note: I am aware that this method is probably possible but may be quite complicated to type in/learn. Therefore I would prefer Method B if it is simpler, as I would be under the time pressure of an exam. Nevertheless, I would still be interested in a method A which can do Euler's method for me as I personally prefer my ordinary scientific calculator to my GDC. Method B In addition to the Ans tool given in Method A, you also have the ability to type in $m\times n$ matrices in your calculator. The calculator can take in matrices in its input, and can output matrices which will be stored to Ans. Matrix multiplication (and raising to a power) can be carried out along with transpose, determinant and inverse functions (where possible) on matrices. Note that I cannot ""get"" cell $(i,j)$ of a matrix - I have to use the matrix as a whole. However, I can still do something like the following: $$\pi\begin{bmatrix}\sqrt{\det\left({Ans}^{T} \times \begin{bmatrix}3 & 4\\12 & \det(Ans)\end{bmatrix}^{-1}\right)} & 4\\\det(Ans)^2 & -\det({Ans}^2)\end{bmatrix}^3$$ Similarly, is there a one-liner which would be able to do Euler's method? EDIT: Method C Unfortunately, it turns out that my calculator cannot accept matrices within matrices which is a real shame - my mistake that I thought it could. It can only manipulate matrix answers, and I can only type in constant matrices. Nevertheless, I'm sure there is a way to circumvent this problem with some matrix multiplication. So ny calculator can only do something like: $$\begin{bmatrix}1 & 2 \\ 8 & 2\end{bmatrix}^{-2}\times {Ans}^T \times {(Mat\, A)}^{-1}$$ (Where Mat A is previously stored in the memory. Note that Ans can still be a matrix) Note that my calculators can store values (and matrices in the case of method B) into their memory, but that would only be initial values for variables (like $h$, $x_0$ and $y_0$), as the one-liner must be deleted before I can type in the line which can store memory. In addition to these methods, if there is some function/tool I am completely missing on these calculators which I could use to simplify my task, I would be glad to hear it.","As part of the calculator paper for IB (International Baccalaureate), we may be asked to do Euler's method, for say, 10 iterations. While it is feasible to do with a calculator (slightly easier if you have 2 calculators), it is quite a nuisance, and it is quite easy to make a mistake when I have to retype the numbers back into the calculator. The problem here is that calculators can only store 1 answer ( Ans ) at a time, and that any attempt to store your answer into a memory slot would delete your calculation line which you would have to type in again. On the other hand, you can easily use the calculator to execute recursive functions which only rely on 1 variable. For example, I can calculate the $n$th term of the following recursive sequence by repeatedly pressing the = button: $$T_{n+1}=T_n^3+\sqrt{T_n}-\frac{1}{T_n},\, T_0=5$$ In this example, I would first press 5 = and then type in ${Ans}^3+\sqrt{Ans}-\frac{1}{Ans}$ and press = $n$ times to get $T_n$. However, this is not possible with Euler's method which has recursive formulas: $$x_{n+1}=x_n+h$$ $$y_{n+1}=y_n+h\frac{dy_n}{dx_n}$$ where $h$ is the step (which can be pre-stored in a memory slot) and $\frac{dy_n}{dx_n}=f(x_n, y_n)$. We are allowed to use a graphical calculator (GDC) which has several functions. I have found a way to do this using the spreadsheet function which is quite fiddly, but at least avoids copying mistakes when retyping numbers into my calculator. I was wondering if there were yet easier methods to do this, so I shall ask for the following methods, if possible: Method A I would prefer it if a method with the following restrictions could be used to execute Euler's method: You have a calculator which is an ordinary scientific calculator which has the ability to store the previous answer (Ans). You have the ability to type in a whole function in terms of (Ans) as shown in the first example. Is it possible to type in a one-liner which would do Euler's method by repeatedly pressing = ? *Note: I am aware that this method is probably possible but may be quite complicated to type in/learn. Therefore I would prefer Method B if it is simpler, as I would be under the time pressure of an exam. Nevertheless, I would still be interested in a method A which can do Euler's method for me as I personally prefer my ordinary scientific calculator to my GDC. Method B In addition to the Ans tool given in Method A, you also have the ability to type in $m\times n$ matrices in your calculator. The calculator can take in matrices in its input, and can output matrices which will be stored to Ans. Matrix multiplication (and raising to a power) can be carried out along with transpose, determinant and inverse functions (where possible) on matrices. Note that I cannot ""get"" cell $(i,j)$ of a matrix - I have to use the matrix as a whole. However, I can still do something like the following: $$\pi\begin{bmatrix}\sqrt{\det\left({Ans}^{T} \times \begin{bmatrix}3 & 4\\12 & \det(Ans)\end{bmatrix}^{-1}\right)} & 4\\\det(Ans)^2 & -\det({Ans}^2)\end{bmatrix}^3$$ Similarly, is there a one-liner which would be able to do Euler's method? EDIT: Method C Unfortunately, it turns out that my calculator cannot accept matrices within matrices which is a real shame - my mistake that I thought it could. It can only manipulate matrix answers, and I can only type in constant matrices. Nevertheless, I'm sure there is a way to circumvent this problem with some matrix multiplication. So ny calculator can only do something like: $$\begin{bmatrix}1 & 2 \\ 8 & 2\end{bmatrix}^{-2}\times {Ans}^T \times {(Mat\, A)}^{-1}$$ (Where Mat A is previously stored in the memory. Note that Ans can still be a matrix) Note that my calculators can store values (and matrices in the case of method B) into their memory, but that would only be initial values for variables (like $h$, $x_0$ and $y_0$), as the one-liner must be deleted before I can type in the line which can store memory. In addition to these methods, if there is some function/tool I am completely missing on these calculators which I could use to simplify my task, I would be glad to hear it.",,"['ordinary-differential-equations', 'calculator', 'eulers-method']"
94,How to solve $(a^2 - x^2) \frac {\mathrm d y} {\mathrm d x} + 2xy + (a^2 - y^2) \frac {\mathrm d x} {\mathrm d y}=0$,How to solve,(a^2 - x^2) \frac {\mathrm d y} {\mathrm d x} + 2xy + (a^2 - y^2) \frac {\mathrm d x} {\mathrm d y}=0,"The differential equation I am trying to solve is: $\displaystyle (a^2 - x^2) \frac {\mathrm d y} {\mathrm d x} + 2xy + (a^2 - y^2) \frac {\mathrm d x} {\mathrm d y}=0$ How do I go about this? I have tried integrating it but I'm not sure how to manage the first term, do I just integrate it twice with respect to y?","The differential equation I am trying to solve is: $\displaystyle (a^2 - x^2) \frac {\mathrm d y} {\mathrm d x} + 2xy + (a^2 - y^2) \frac {\mathrm d x} {\mathrm d y}=0$ How do I go about this? I have tried integrating it but I'm not sure how to manage the first term, do I just integrate it twice with respect to y?",,['ordinary-differential-equations']
95,Why is Schouten-Nijnhuis bracket trivial on Poisson cohomology?,Why is Schouten-Nijnhuis bracket trivial on Poisson cohomology?,,"For a commutative algebra $A$, let a biderivation $P$ be called a Poisson structure if $[[P,P]]=0$ (the bracket is Schouten-Nijenhuis). Then one obtains a complex of multiderivations with $[[P,{}]]$ being the differential, and so one gets Poisson cohomology. Due to graded Jacobi identity for Schouten-Nijenhuis bracket, Poisson cohomology inherits this bracket. Let $P$ be called nondegenerate if $da \mapsto P(a,{})$ defines an isomorphism between 1-forms and 1-derivations. (Here one also needs to assume that the module of 1-forms is projective and finitely generated.) How can one see that for a nondegenerate Poisson structure Schouten-Nijenhuis bracket on cohomology is trivial? I highly suspect that for X and Y from cohomology we have $[[X,Y]]=[[P,i_\Omega(X\wedge Y)]]$ where $\Omega$ is the symplectic 2-form corresponding to $P$ and $i$ is insertion, but I only managed to show that for $|X|=1$ and $|Y|=0$ which is a little bit not enough.","For a commutative algebra $A$, let a biderivation $P$ be called a Poisson structure if $[[P,P]]=0$ (the bracket is Schouten-Nijenhuis). Then one obtains a complex of multiderivations with $[[P,{}]]$ being the differential, and so one gets Poisson cohomology. Due to graded Jacobi identity for Schouten-Nijenhuis bracket, Poisson cohomology inherits this bracket. Let $P$ be called nondegenerate if $da \mapsto P(a,{})$ defines an isomorphism between 1-forms and 1-derivations. (Here one also needs to assume that the module of 1-forms is projective and finitely generated.) How can one see that for a nondegenerate Poisson structure Schouten-Nijenhuis bracket on cohomology is trivial? I highly suspect that for X and Y from cohomology we have $[[X,Y]]=[[P,i_\Omega(X\wedge Y)]]$ where $\Omega$ is the symplectic 2-form corresponding to $P$ and $i$ is insertion, but I only managed to show that for $|X|=1$ and $|Y|=0$ which is a little bit not enough.",,"['abstract-algebra', 'ordinary-differential-equations', 'differential-geometry', 'homology-cohomology', 'differential-forms']"
96,Solving a system of differential equations of order 3,Solving a system of differential equations of order 3,,"We have the set of differential equations $$w_{1}+\frac{d^{2}}{dt^{2}}w_{1}-3w_{2}-\frac{d}{dt}w_{2}+\frac{d^{2}}{dt^{2}}w_{2}+\frac{d^{3}}{dt^{3}}w_{2}=0$$ $$w_{1}-\frac{d}{dt}w_{1}-w_{2}+\frac{d}{dt}w_{2}=0$$ The question is to show that every strong solution of the above can be written as $$w(t)=\begin{bmatrix}\alpha_{1}-3\alpha_{2}\\\alpha_{1}\end{bmatrix}e^{t}+\begin{bmatrix}\alpha_{2}\\\alpha_{2}\end{bmatrix}te^{t}+\begin{bmatrix}\beta\\\beta\end{bmatrix}e^{-2t}+\begin{bmatrix}\gamma\\\gamma\end{bmatrix}e^{-t}$$ In other words, I want to find a solution to $(P(\frac{d}{dt})w)(t)=0$ for all $t\in\mathbb{R}$. That is $$P(\frac{d}{dt})w=\begin{bmatrix}1+\frac{d^{2}}{dt^{2}} & -3-\frac{d}{dt}+\frac{d^{2}}{dt^{2}}+\frac{d^{3}}{dt^{3}}\\1-\frac{d}{dt} & -1+\frac{d}{dt}\end{bmatrix}\begin{bmatrix}w_{1}\\w_{2}\end{bmatrix}(t)=0$$ I calculated $\det P(\xi)=(\xi-1)^{2}(\xi+1)(\xi+2)$, the roots of which are $\lambda_{1,2}=\pm 1$ and $\lambda_{3}=-2$, with $\lambda_{1}=1$ having multiplicity 2. I don't know where to go from here though. I think I have to calculate the eigenvectors, but given the nature of my matrix (which is composed of $\frac{d}{dt}$ rather than constant values) it seems a bit tricky. Edit: I tried taking the Fourier transform $$\begin{cases}F(w_{1}(t))+F(\frac{d^{2}}{dt^{2}}w_{1}(t))-F(\frac{d}{dt}w_{2}(t))+F(\frac{d^{2}}{dt^{2}}w_{2}(t))+F(\frac{d^{3}}{dt^{3}}w_{2}(t))=0\\F(w_{1}(t))-F(\frac{d}{dt}w_{1}(t))-F(w_{2}(t))+F(\frac{d}{dt}w_{2}(t))=0\end{cases}$$ To get $$\begin{cases}\hat{w}_{1}(f)+(2\pi if)^{2}\hat{w}_{1}(f)-2\pi if\hat{w}_{2}(f)+(2\pi if)^{2}\hat{w}_{2}(f)+(2\pi if)^{3}\hat{w}_{2}(f)=0 \\ \hat{w}_{1}(f)-2\pi if\hat{w}_{1}(f)-\hat{w}_{2}(f)+2\pi if\hat{w}_{2}(f)=0\end{cases}$$ But when I simplify, I get $$\begin{cases}(1-4\pi^{2}f^{2})\hat{w}_{1}(f)=(2\pi if+4\pi^{2}f^{2}+8\pi^{3}if^{3})\hat{w}_{2}(f) \\ (1-2\pi if)\hat{w}_{1}(f)=(1-2\pi if)\hat{w}_{2}(f)\end{cases}$$ Which cannot be right because then we would have that $\hat{w}_{1}(f)=\hat{w}_{2}(f)$. Edit 2: (Correction of Edit 2) Edit 3: For the second equation, we have $$\frac{d}{dt}(e^{-t}(w_{1}(t)-w_{2}(t)))=0$$ Now, plugging in $w_{1}(t)=w_{2}(t)+ce^{t}$ into the first equation yields $$w_{2}(t)+ce^{t}+\frac{d^{2}}{dt^{2}}(w_{2}(t)+ce^{t})-3w_{2}-\frac{d}{dt}w_{2}+\frac{d^{2}}{dt^{2}}w_{2}+\frac{d^{3}}{dt^{3}}w_{2}=0$$ i.e. $$\frac{d^{3}}{dt^{3}}w_{2}(t)+2\frac{d^{2}}{dt^{2}}w_{2}(t)-\frac{d}{dt}w_{2}(t)-2w_{2}(t)=-2ce^{t}$$ First we want to calculate the homogeneous part of the equation. That is $$\frac{d^{3}}{dt^{3}}w_{2}(t)+2\frac{d^{2}}{dt^{2}}w_{2}(t)-\frac{d}{dt}w_{2}(t)-2w_{2}(t)=0$$ The characteristic equation is $$r^{3}+2r^{2}-r-2=0$$ i.e. $$(r-1)(r+2)(r+1)=0$$ Hence we get the general solution: $$w_{2}(t)_{h}=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}$$ Now I want to find the particular solution, $w_{2}(t)_{p}$. The inhomogeneous part is $f(t)=-2ce^{t}$ Take $w_{2}(t)_{p}=-kte^{t}$. Then $\frac{d}{dt}w_{2}(t)_{p}=-kte^{t}-ke^{t}$; $\frac{d^{2}}{dt^{2}}w_{2}(t)_{p}=-kte^{t}-2ke^{t}$; $\frac{d^{3}}{dt^{3}}w_{2}(t)_{p}=-kte^{t}-3ke^{t}$. $$-kte^{t}-3ke^{t}-2kte^{t}-4ke^{t}+kte^{t}+ke^{t}+2kte^{t}=-2ce^{t}$$ i.e. $$-6ke^{t}=2ce^{t}$$ So $k=\frac{c}{3}$. That gives us the general solution $w_{2}(t)_{h}+w_{2}(t)_{p}$: $$w_{2}(t)=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}-\frac{c}{3}te^{t}$$ Now we plug $w_{2}(t)=w_{1}(t)-ce^{t}$ into the first equation to get $$w_{1}(t)+\frac{d^{2}}{dt^{2}}w_{1}(t)-3w_{1}(t)+3ce^{t}-\frac{d}{dt}w_{1}(t)-ce^{t}+\frac{d^{2}}{dt^{2}}w_{1}(t)-ce^{t}+\frac{d^{3}}{dt^{3}}w_{1}(t)-ce^{t}=0$$ i.e. $$\frac{d^{3}}{dt^{3}}w_{1}(t)+2\frac{d^{2}}{dt^{2}}w_{1}(t)-\frac{d}{dt}w_{1}(t)-2w_{1}(t)=-ce^{t}$$ The characteristic equation of the homogeneous part of the equation is $$r^{3}+2r^{2}-r-2=0$$ Again, we get $$w_{1}(t)_{h}=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}$$ Take $w_{1}(t)_{p}=-kte^{t}$. Then substituting gives $-6ke^{t}=-ce^{t}$, thus $k=\frac{c}{6}$. So $w_{1}(t)_{p}=-\frac{c}{6}te^{t}$. Hence $$w_{1}(t)=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}-\frac{c}{6}te^{t}$$ So we can write $$w(t)=\begin{bmatrix} w_{1}(t) \\ w_{2}(t) \end{bmatrix}=\begin{bmatrix} C_{3} \\ C_{3} \end{bmatrix}e^{t}+\begin{bmatrix} -\frac{c}{6} \\ -\frac{c}{3} \end{bmatrix}te^{t}+\begin{bmatrix} C_{1} \\ C_{1} \end{bmatrix}e^{-2t}+\begin{bmatrix} C_{2} \\ C_{2} \end{bmatrix}e^{-t}$$ However, this is not the solution I was after.","We have the set of differential equations $$w_{1}+\frac{d^{2}}{dt^{2}}w_{1}-3w_{2}-\frac{d}{dt}w_{2}+\frac{d^{2}}{dt^{2}}w_{2}+\frac{d^{3}}{dt^{3}}w_{2}=0$$ $$w_{1}-\frac{d}{dt}w_{1}-w_{2}+\frac{d}{dt}w_{2}=0$$ The question is to show that every strong solution of the above can be written as $$w(t)=\begin{bmatrix}\alpha_{1}-3\alpha_{2}\\\alpha_{1}\end{bmatrix}e^{t}+\begin{bmatrix}\alpha_{2}\\\alpha_{2}\end{bmatrix}te^{t}+\begin{bmatrix}\beta\\\beta\end{bmatrix}e^{-2t}+\begin{bmatrix}\gamma\\\gamma\end{bmatrix}e^{-t}$$ In other words, I want to find a solution to $(P(\frac{d}{dt})w)(t)=0$ for all $t\in\mathbb{R}$. That is $$P(\frac{d}{dt})w=\begin{bmatrix}1+\frac{d^{2}}{dt^{2}} & -3-\frac{d}{dt}+\frac{d^{2}}{dt^{2}}+\frac{d^{3}}{dt^{3}}\\1-\frac{d}{dt} & -1+\frac{d}{dt}\end{bmatrix}\begin{bmatrix}w_{1}\\w_{2}\end{bmatrix}(t)=0$$ I calculated $\det P(\xi)=(\xi-1)^{2}(\xi+1)(\xi+2)$, the roots of which are $\lambda_{1,2}=\pm 1$ and $\lambda_{3}=-2$, with $\lambda_{1}=1$ having multiplicity 2. I don't know where to go from here though. I think I have to calculate the eigenvectors, but given the nature of my matrix (which is composed of $\frac{d}{dt}$ rather than constant values) it seems a bit tricky. Edit: I tried taking the Fourier transform $$\begin{cases}F(w_{1}(t))+F(\frac{d^{2}}{dt^{2}}w_{1}(t))-F(\frac{d}{dt}w_{2}(t))+F(\frac{d^{2}}{dt^{2}}w_{2}(t))+F(\frac{d^{3}}{dt^{3}}w_{2}(t))=0\\F(w_{1}(t))-F(\frac{d}{dt}w_{1}(t))-F(w_{2}(t))+F(\frac{d}{dt}w_{2}(t))=0\end{cases}$$ To get $$\begin{cases}\hat{w}_{1}(f)+(2\pi if)^{2}\hat{w}_{1}(f)-2\pi if\hat{w}_{2}(f)+(2\pi if)^{2}\hat{w}_{2}(f)+(2\pi if)^{3}\hat{w}_{2}(f)=0 \\ \hat{w}_{1}(f)-2\pi if\hat{w}_{1}(f)-\hat{w}_{2}(f)+2\pi if\hat{w}_{2}(f)=0\end{cases}$$ But when I simplify, I get $$\begin{cases}(1-4\pi^{2}f^{2})\hat{w}_{1}(f)=(2\pi if+4\pi^{2}f^{2}+8\pi^{3}if^{3})\hat{w}_{2}(f) \\ (1-2\pi if)\hat{w}_{1}(f)=(1-2\pi if)\hat{w}_{2}(f)\end{cases}$$ Which cannot be right because then we would have that $\hat{w}_{1}(f)=\hat{w}_{2}(f)$. Edit 2: (Correction of Edit 2) Edit 3: For the second equation, we have $$\frac{d}{dt}(e^{-t}(w_{1}(t)-w_{2}(t)))=0$$ Now, plugging in $w_{1}(t)=w_{2}(t)+ce^{t}$ into the first equation yields $$w_{2}(t)+ce^{t}+\frac{d^{2}}{dt^{2}}(w_{2}(t)+ce^{t})-3w_{2}-\frac{d}{dt}w_{2}+\frac{d^{2}}{dt^{2}}w_{2}+\frac{d^{3}}{dt^{3}}w_{2}=0$$ i.e. $$\frac{d^{3}}{dt^{3}}w_{2}(t)+2\frac{d^{2}}{dt^{2}}w_{2}(t)-\frac{d}{dt}w_{2}(t)-2w_{2}(t)=-2ce^{t}$$ First we want to calculate the homogeneous part of the equation. That is $$\frac{d^{3}}{dt^{3}}w_{2}(t)+2\frac{d^{2}}{dt^{2}}w_{2}(t)-\frac{d}{dt}w_{2}(t)-2w_{2}(t)=0$$ The characteristic equation is $$r^{3}+2r^{2}-r-2=0$$ i.e. $$(r-1)(r+2)(r+1)=0$$ Hence we get the general solution: $$w_{2}(t)_{h}=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}$$ Now I want to find the particular solution, $w_{2}(t)_{p}$. The inhomogeneous part is $f(t)=-2ce^{t}$ Take $w_{2}(t)_{p}=-kte^{t}$. Then $\frac{d}{dt}w_{2}(t)_{p}=-kte^{t}-ke^{t}$; $\frac{d^{2}}{dt^{2}}w_{2}(t)_{p}=-kte^{t}-2ke^{t}$; $\frac{d^{3}}{dt^{3}}w_{2}(t)_{p}=-kte^{t}-3ke^{t}$. $$-kte^{t}-3ke^{t}-2kte^{t}-4ke^{t}+kte^{t}+ke^{t}+2kte^{t}=-2ce^{t}$$ i.e. $$-6ke^{t}=2ce^{t}$$ So $k=\frac{c}{3}$. That gives us the general solution $w_{2}(t)_{h}+w_{2}(t)_{p}$: $$w_{2}(t)=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}-\frac{c}{3}te^{t}$$ Now we plug $w_{2}(t)=w_{1}(t)-ce^{t}$ into the first equation to get $$w_{1}(t)+\frac{d^{2}}{dt^{2}}w_{1}(t)-3w_{1}(t)+3ce^{t}-\frac{d}{dt}w_{1}(t)-ce^{t}+\frac{d^{2}}{dt^{2}}w_{1}(t)-ce^{t}+\frac{d^{3}}{dt^{3}}w_{1}(t)-ce^{t}=0$$ i.e. $$\frac{d^{3}}{dt^{3}}w_{1}(t)+2\frac{d^{2}}{dt^{2}}w_{1}(t)-\frac{d}{dt}w_{1}(t)-2w_{1}(t)=-ce^{t}$$ The characteristic equation of the homogeneous part of the equation is $$r^{3}+2r^{2}-r-2=0$$ Again, we get $$w_{1}(t)_{h}=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}$$ Take $w_{1}(t)_{p}=-kte^{t}$. Then substituting gives $-6ke^{t}=-ce^{t}$, thus $k=\frac{c}{6}$. So $w_{1}(t)_{p}=-\frac{c}{6}te^{t}$. Hence $$w_{1}(t)=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}-\frac{c}{6}te^{t}$$ So we can write $$w(t)=\begin{bmatrix} w_{1}(t) \\ w_{2}(t) \end{bmatrix}=\begin{bmatrix} C_{3} \\ C_{3} \end{bmatrix}e^{t}+\begin{bmatrix} -\frac{c}{6} \\ -\frac{c}{3} \end{bmatrix}te^{t}+\begin{bmatrix} C_{1} \\ C_{1} \end{bmatrix}e^{-2t}+\begin{bmatrix} C_{2} \\ C_{2} \end{bmatrix}e^{-t}$$ However, this is not the solution I was after.",,"['analysis', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
97,Autonomous exponentially stable steady-state and small non-vanishing perturbations,Autonomous exponentially stable steady-state and small non-vanishing perturbations,,"My question considers if an autonomous system having a exponentially stable steady-state will continue to do so for non-vanishing small perturbations. Consider the system $\frac{d}{dt}x=f(x)+\epsilon g(x)$, with $x\in\mathbb{R^n}$, and $f,g$ continuously differentiable vector fields. Assume that the system for $\epsilon=0$ has an (not necessarily globally) exponentially stable steady-state at $x_0^{SS}$. Intuitively, I would assume that for $|\epsilon|\ll 1$, the system should also have an exponentially stable steady-state $x_\epsilon^{SS}$. My questions: (i) Does this hold, (ii) if it holds, what is the name of the theorem saying so, or where do I find it? Note: I am talking about non-vanishing perturbations, i.e. $g(x_0^{SS})\neq 0$, implying in general that $x_\epsilon^{SS}\neq x_0^{SS}$, if $x_\epsilon^{SS}$ exists (what I don't know). What I tried: (1) I read the corresponding chapters in Khalil's ""Nonlinear systems"". There, it seems to be always assumed that $g$ depends on the time $t$. Consequently, I only found theorems saying that one can bound the difference: $\|x_\epsilon(t)-x_0(t)\|<b$ if $|\epsilon|\ll 1$, but not that the system has still a steady state (it could e.g. oscillate). My question seems to be easier, maybe too easy. (2) I found the concept of structural stability, and I understand it such that the system is structurally stable if $\epsilon=0$ is not a bifurcation point. Structural stability seems to imply that the system with $|\epsilon|\ll 1$ will still have a stable SS, which is what I want. But I couldn't connect exponential stability with structural stability. Again, everything I found did only concentrate on (for me) too general cases.","My question considers if an autonomous system having a exponentially stable steady-state will continue to do so for non-vanishing small perturbations. Consider the system $\frac{d}{dt}x=f(x)+\epsilon g(x)$, with $x\in\mathbb{R^n}$, and $f,g$ continuously differentiable vector fields. Assume that the system for $\epsilon=0$ has an (not necessarily globally) exponentially stable steady-state at $x_0^{SS}$. Intuitively, I would assume that for $|\epsilon|\ll 1$, the system should also have an exponentially stable steady-state $x_\epsilon^{SS}$. My questions: (i) Does this hold, (ii) if it holds, what is the name of the theorem saying so, or where do I find it? Note: I am talking about non-vanishing perturbations, i.e. $g(x_0^{SS})\neq 0$, implying in general that $x_\epsilon^{SS}\neq x_0^{SS}$, if $x_\epsilon^{SS}$ exists (what I don't know). What I tried: (1) I read the corresponding chapters in Khalil's ""Nonlinear systems"". There, it seems to be always assumed that $g$ depends on the time $t$. Consequently, I only found theorems saying that one can bound the difference: $\|x_\epsilon(t)-x_0(t)\|<b$ if $|\epsilon|\ll 1$, but not that the system has still a steady state (it could e.g. oscillate). My question seems to be easier, maybe too easy. (2) I found the concept of structural stability, and I understand it such that the system is structurally stable if $\epsilon=0$ is not a bifurcation point. Structural stability seems to imply that the system with $|\epsilon|\ll 1$ will still have a stable SS, which is what I want. But I couldn't connect exponential stability with structural stability. Again, everything I found did only concentrate on (for me) too general cases.",,"['ordinary-differential-equations', 'perturbation-theory', 'stability-theory']"
98,A question on ordinary differential inequality,A question on ordinary differential inequality,,"Could we find a solution $f=f(x)$ to the following initial problem for the OD inequality? $$3xf'+f-\sqrt{6f}\leq 0,\quad f(0)=0,\quad f(8/3)=6.$$ . Added: The above question is in fact a special case of the following question: Could we find a solution $f=f(x)$ to the following initial problem for the OD inequality? $$xf'+(u+1/6)f-2\sqrt{uf}\leq 0,\quad f(0)=0,\quad f(8/3)=6,$$ where $u$ is a to-be-determined function.","Could we find a solution $f=f(x)$ to the following initial problem for the OD inequality? $$3xf'+f-\sqrt{6f}\leq 0,\quad f(0)=0,\quad f(8/3)=6.$$ . Added: The above question is in fact a special case of the following question: Could we find a solution $f=f(x)$ to the following initial problem for the OD inequality? $$xf'+(u+1/6)f-2\sqrt{uf}\leq 0,\quad f(0)=0,\quad f(8/3)=6,$$ where $u$ is a to-be-determined function.",,"['analysis', 'ordinary-differential-equations']"
99,"Poincaré-Bendixson theorem, periodic solutions/periodic orbits","Poincaré-Bendixson theorem, periodic solutions/periodic orbits",,"According to my book (Hsu: ODE), a solution $\phi(t)$ to the system $x' = f(x)$ that is bounded for all $t \geq 0$ satisfies one of: 1) $\omega(\phi)$ contains an equilibrium, or 2) either $\phi(t)$ is periodic or $\omega(\phi)$ is a periodic orbit. My question is: what is the difference between the solution being periodic and the limit set $\omega(\phi)$ being a periodic orbit?","According to my book (Hsu: ODE), a solution $\phi(t)$ to the system $x' = f(x)$ that is bounded for all $t \geq 0$ satisfies one of: 1) $\omega(\phi)$ contains an equilibrium, or 2) either $\phi(t)$ is periodic or $\omega(\phi)$ is a periodic orbit. My question is: what is the difference between the solution being periodic and the limit set $\omega(\phi)$ being a periodic orbit?",,"['ordinary-differential-equations', 'dynamical-systems']"
