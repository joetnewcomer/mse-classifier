,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivative inside a trace,Derivative inside a trace,,"Let $B : \mathbb R^{m \times n} \to \mathbb R^{p \times p}$ . Let $k_{ij}$ denote the $(i,j)$ -th entry of matrix $K$ . Is the following equation correct? $$\text{Tr} \left(\frac{\partial B(K)}{\partial k_{ij}}\right)=\left(\frac{\partial \text{Tr}(B(K))}{\partial K}\right)_{ij}$$ Thanks for any ideas.",Let . Let denote the -th entry of matrix . Is the following equation correct? Thanks for any ideas.,"B : \mathbb R^{m \times n} \to \mathbb R^{p \times p} k_{ij} (i,j) K \text{Tr} \left(\frac{\partial B(K)}{\partial k_{ij}}\right)=\left(\frac{\partial \text{Tr}(B(K))}{\partial K}\right)_{ij}","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'trace']"
1,How to solve a simple non-linear optimisation problem in order to find the minimum point in a hyperbola?,How to solve a simple non-linear optimisation problem in order to find the minimum point in a hyperbola?,,"Consider the equation $$0.26639x-0.043941y+(5.9313\times10^{-5})xy-(3.9303\times{10^{-6}}) y^2-7242.0404=0$$ with $x,y>0$ . If you plot it, it'll look like below: Now, I want to find a minimum point on this hyperbola, such that $x+y$ is a minimum.  In other words: $$\min(x+y)$$ $$Constraints: $$ $$0.26639x-0.043941y+(5.9313\times10^{-5})xy-(3.9303\times{10^{-6}}) y^2-7242.0404=0$$ Any help on how to mathematically find this point would be really helpful. I've asked a similar question here , but in this one, I wanted to find a corner point such that the hyperbola has the maximum curvature. But this is not the case in this question.","Consider the equation with . If you plot it, it'll look like below: Now, I want to find a minimum point on this hyperbola, such that is a minimum.  In other words: Any help on how to mathematically find this point would be really helpful. I've asked a similar question here , but in this one, I wanted to find a corner point such that the hyperbola has the maximum curvature. But this is not the case in this question.","0.26639x-0.043941y+(5.9313\times10^{-5})xy-(3.9303\times{10^{-6}}) y^2-7242.0404=0 x,y>0 x+y \min(x+y) Constraints:  0.26639x-0.043941y+(5.9313\times10^{-5})xy-(3.9303\times{10^{-6}}) y^2-7242.0404=0","['geometry', 'analysis', 'derivatives', 'optimization', 'nonlinear-optimization']"
2,"How to determine whether $\Bbb P ( B_{t_1} \in [x - c , x + c ], \ldots , B_{t_n} \in [x -c , x + c ])$ is decreasing in $x$?",How to determine whether  is decreasing in ?,"\Bbb P ( B_{t_1} \in [x - c , x + c ], \ldots , B_{t_n} \in [x -c , x + c ]) x","Let $\phi_t (z) := \frac{1}{\sqrt{2\pi t}} e^{-\frac {z^2} {2t}}$ . By intuition $\omega : [0,\infty ) \to [0,1]$ \begin{align} \omega (x) &:= \Bbb P ( B_{t_1} \in [x - c , x + c ], \ldots , B_{t_n} \in [x -c , x + c ])\\ &=\int_{x-c}^{x+c} \phi_{t_1} (y_1) \int_{x-c}^{x+c} \phi_{t_2-t_1} (y_2 - y_1) \ldots \int_{x-c}^{x+c} \phi_{t_n -t_{n-1}} (y_n - y_{n-1}) \text d y_n \ldots\text d y_1 \end{align} should be decreasing in $x$ . For $n=1$ this is easily done by derivating in $x$ . Does anyone see a better way for this case here?",Let . By intuition should be decreasing in . For this is easily done by derivating in . Does anyone see a better way for this case here?,"\phi_t (z) := \frac{1}{\sqrt{2\pi t}} e^{-\frac {z^2} {2t}} \omega : [0,\infty ) \to [0,1] \begin{align}
\omega (x) &:= \Bbb P ( B_{t_1} \in [x - c , x + c ], \ldots , B_{t_n} \in [x -c , x + c ])\\ &=\int_{x-c}^{x+c} \phi_{t_1} (y_1) \int_{x-c}^{x+c} \phi_{t_2-t_1} (y_2 - y_1) \ldots \int_{x-c}^{x+c} \phi_{t_n -t_{n-1}} (y_n - y_{n-1}) \text d y_n \ldots\text d y_1
\end{align} x n=1 x","['probability', 'integration', 'derivatives', 'normal-distribution']"
3,Mathematics behind Neural Networks,Mathematics behind Neural Networks,,"I wasn't sure to ask this here, but based on the related questions, I think this is appropriate. For starters, I'm trying to build a Neural Network using the website below as a reference. It seems like it's just trying to implement Perceptron with a single hidden layer. https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/ Following the guide, it makes sense, but I'm having trouble understanding the mathematics behind the back propagation part. I understand that you have to use the error function to derive the rate of change for the biases and weights, but I'm confused as to how the derivatives (w.r.t. the parameters) ends up being a 'scalar' multiplication. The derivatives have to be the same size as the parameters, but how do you get to that point (using ${\partial J\over\partial B_2}$ as the example here)? Also, when deriving ${\partial J\over\partial W_2}$ , using the chain rule, how does the derivative of ${\partial Y\over\partial W_2}$ result in the transpose of $H,$ and why does it end up being a dot product with the derivative of the error function (w.r.t the result)? Sorry, my background in Matrix Algebra is not incredibly strong, so having the mathematics explained to me would really help a lot.","I wasn't sure to ask this here, but based on the related questions, I think this is appropriate. For starters, I'm trying to build a Neural Network using the website below as a reference. It seems like it's just trying to implement Perceptron with a single hidden layer. https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/ Following the guide, it makes sense, but I'm having trouble understanding the mathematics behind the back propagation part. I understand that you have to use the error function to derive the rate of change for the biases and weights, but I'm confused as to how the derivatives (w.r.t. the parameters) ends up being a 'scalar' multiplication. The derivatives have to be the same size as the parameters, but how do you get to that point (using as the example here)? Also, when deriving , using the chain rule, how does the derivative of result in the transpose of and why does it end up being a dot product with the derivative of the error function (w.r.t the result)? Sorry, my background in Matrix Algebra is not incredibly strong, so having the mathematics explained to me would really help a lot.","{\partial J\over\partial B_2} {\partial J\over\partial W_2} {\partial Y\over\partial W_2} H,","['derivatives', 'matrix-calculus', 'neural-networks', 'transpose']"
4,Gradient of scalar field $a^T X^{-1} b$,Gradient of scalar field,a^T X^{-1} b,"During the derivation of GDA as generative algorithm, I am stuck at how to take the gradient $$\nabla_X \left( a^TX^{-1}b \right)$$ where $a, b$ are column vectors independent of $X$ . I have tried using trace operator and chain rule, but could not crack it. How should this derivative be approached? The answer is $$-X^{-T}ab^TX^{-T}$$","During the derivation of GDA as generative algorithm, I am stuck at how to take the gradient where are column vectors independent of . I have tried using trace operator and chain rule, but could not crack it. How should this derivative be approached? The answer is","\nabla_X \left( a^TX^{-1}b \right) a, b X -X^{-T}ab^TX^{-T}","['matrices', 'derivatives', 'inverse', 'matrix-calculus', 'scalar-fields']"
5,Continuity and differentiability for $g(x)$,Continuity and differentiability for,g(x),"Below is my working: CONTINUITY AT $x=a$ * $\lim_{x\to a^-}g(x)=\lim_{h\to0}g(a-h)=0$ * $\lim_{x\to a^+}g(x)=\lim_{h\to0}g(a+h)=\lim_{h\to0}\int_{a}^{a+h}f(t)dt=\int_{a}^{a}f(t)dt=0$ * $g(a)=\int_{a}^{a}f(t)dt=0$ Therefore $g(x)$ is continuous at $x=a$ CONTINUITY AT $x=b$ * $\lim_{x\to b^-}g(x)=\lim_{h\to0}g(b-h)=\lim_{h\to0}\int_{a}^{b-h}f(t)dt=\int_{a}^{b}f(t)dt$ * $\lim_{x\to b^+}g(x)=\lim_{h\to0}g(b+h)=\lim_{h\to0}\int_{a}^{b}f(t)dt=\int_{a}^{b}f(t)dt$ * $g(b)=\int_{a}^{b}f(t)dt$ Therefore $g(x)$ is continuous at $x=b$ CONTINUITY AT $x=k\in (a,b)$ * $\lim_{x\to k^-}g(x)=\lim_{h\to0}g(k-h)=\lim_{h\to0}\int_{a}^{k-h}f(t)dt=\int_{a}^{k}f(t)dt$ * $\lim_{x\to k^+}g(x)=\lim_{h\to0}g(k+h)=\lim_{h\to0}\int_{a}^{k+h}f(t)dt=\int_{a}^{k}f(t)dt$ * $g(k)=\int_{a}^{k}f(t)dt$ CONTINUITY AT $x=k<a$ and $x=k>b$ It can be shown in a similar way that $g(x)$ is continuous at $x=k<a$ and $x=k>b$ DIFFERENTIABILITY AT $x=a$ * $\lim_{x\to a^-}\frac{g(x)-g(a)}{x-a}=\lim_{h\to 0}\frac{g(a-h)-g(a)}{-h}=\lim_{h\to 0}\frac{0}{h}=0$ * $\lim_{x\to a^+}\frac{g(x)-g(a)}{x-a}=\lim_{h\to 0}\frac{g(a+h)-g(a)}{h}=\lim_{h\to 0}\frac{\int_{a}^{a+h}f(t)dt-0}{h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{a+h}f(t)dt}{1}=\lim_{h\to 0}f(a+h)=f(a)=0$ Therefore $g(x)$ is differentiable at $x=a$ DIFFERENTIABILITY AT $x=b$ * $\lim_{x\to b^-}\frac{g(x)-g(b)}{x-b}=\lim_{h\to 0}\frac{g(b-h)-g(b)}{-h}=\lim_{h\to 0}\frac{\int_{a}^{b-h}f(t)dt-\int_{a}^{b}f(t)dt}{-h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{b-h}f(t)dt-0}{-1}=\lim_{h\to 0}f(b-h)=f(b)=\int_{a}^{b}f(t)dt$ * $\lim_{x\to b^+}\frac{g(x)-g(b)}{x-b}=\lim_{h\to 0}\frac{g(b+h)-g(b)}{h}=\lim_{h\to 0}\frac{\int_{a}^{b}f(t)dt-\int_{a}^{b}f(t)dt}{h}=\lim_{h\to 0}\frac{0}{h}=\lim_{h\to 0}0=0$ Therefore $g(x)$ is not differentiable at $x=b$ DIFFERENTIABILITY AT $x=k\in(a,b)$ * $\lim_{x\to k^-}\frac{g(x)-g(k)}{x-k}=\lim_{h\to 0}\frac{g(k-h)-g(k)}{-h}=\lim_{h\to 0}\frac{\int_{a}^{k-h}f(t)dt-\int_{a}^{k}f(t)dt}{-h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{k-h}f(t)dt-0}{-1}=\lim_{h\to 0}f(k-h)=f(k)=\int_{a}^{k}f(t)dt$ * $\lim_{x\to k^+}\frac{g(x)-g(k)}{x-k}=\lim_{h\to 0}\frac{g(k+h)-g(k)}{h}=\lim_{h\to 0}\frac{\int_{a}^{k+h}f(t)dt-\int_{a}^{k}f(t)dt}{h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{k+h}f(t)dt-0}{1}=\lim_{h\to 0}f(k+h)=f(k)=\int_{a}^{k}f(t)dt$ Therefore $g(x)$ is differentiable in $(a,b)$ DIFFERENTIABILITY AT $x=k<a$ and $x=k>b$ Similarly it can be shown that $g(x)$ is differentiable at $x=k<a$ and $x=k>b$ Am I correct? Is there any short way to solve this?",Below is my working: CONTINUITY AT * * * Therefore is continuous at CONTINUITY AT * * * Therefore is continuous at CONTINUITY AT * * * CONTINUITY AT and It can be shown in a similar way that is continuous at and DIFFERENTIABILITY AT * * Therefore is differentiable at DIFFERENTIABILITY AT * * Therefore is not differentiable at DIFFERENTIABILITY AT * * Therefore is differentiable in DIFFERENTIABILITY AT and Similarly it can be shown that is differentiable at and Am I correct? Is there any short way to solve this?,"x=a \lim_{x\to a^-}g(x)=\lim_{h\to0}g(a-h)=0 \lim_{x\to a^+}g(x)=\lim_{h\to0}g(a+h)=\lim_{h\to0}\int_{a}^{a+h}f(t)dt=\int_{a}^{a}f(t)dt=0 g(a)=\int_{a}^{a}f(t)dt=0 g(x) x=a x=b \lim_{x\to b^-}g(x)=\lim_{h\to0}g(b-h)=\lim_{h\to0}\int_{a}^{b-h}f(t)dt=\int_{a}^{b}f(t)dt \lim_{x\to b^+}g(x)=\lim_{h\to0}g(b+h)=\lim_{h\to0}\int_{a}^{b}f(t)dt=\int_{a}^{b}f(t)dt g(b)=\int_{a}^{b}f(t)dt g(x) x=b x=k\in (a,b) \lim_{x\to k^-}g(x)=\lim_{h\to0}g(k-h)=\lim_{h\to0}\int_{a}^{k-h}f(t)dt=\int_{a}^{k}f(t)dt \lim_{x\to k^+}g(x)=\lim_{h\to0}g(k+h)=\lim_{h\to0}\int_{a}^{k+h}f(t)dt=\int_{a}^{k}f(t)dt g(k)=\int_{a}^{k}f(t)dt x=k<a x=k>b g(x) x=k<a x=k>b x=a \lim_{x\to a^-}\frac{g(x)-g(a)}{x-a}=\lim_{h\to 0}\frac{g(a-h)-g(a)}{-h}=\lim_{h\to 0}\frac{0}{h}=0 \lim_{x\to a^+}\frac{g(x)-g(a)}{x-a}=\lim_{h\to 0}\frac{g(a+h)-g(a)}{h}=\lim_{h\to 0}\frac{\int_{a}^{a+h}f(t)dt-0}{h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{a+h}f(t)dt}{1}=\lim_{h\to 0}f(a+h)=f(a)=0 g(x) x=a x=b \lim_{x\to b^-}\frac{g(x)-g(b)}{x-b}=\lim_{h\to 0}\frac{g(b-h)-g(b)}{-h}=\lim_{h\to 0}\frac{\int_{a}^{b-h}f(t)dt-\int_{a}^{b}f(t)dt}{-h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{b-h}f(t)dt-0}{-1}=\lim_{h\to 0}f(b-h)=f(b)=\int_{a}^{b}f(t)dt \lim_{x\to b^+}\frac{g(x)-g(b)}{x-b}=\lim_{h\to 0}\frac{g(b+h)-g(b)}{h}=\lim_{h\to 0}\frac{\int_{a}^{b}f(t)dt-\int_{a}^{b}f(t)dt}{h}=\lim_{h\to 0}\frac{0}{h}=\lim_{h\to 0}0=0 g(x) x=b x=k\in(a,b) \lim_{x\to k^-}\frac{g(x)-g(k)}{x-k}=\lim_{h\to 0}\frac{g(k-h)-g(k)}{-h}=\lim_{h\to 0}\frac{\int_{a}^{k-h}f(t)dt-\int_{a}^{k}f(t)dt}{-h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{k-h}f(t)dt-0}{-1}=\lim_{h\to 0}f(k-h)=f(k)=\int_{a}^{k}f(t)dt \lim_{x\to k^+}\frac{g(x)-g(k)}{x-k}=\lim_{h\to 0}\frac{g(k+h)-g(k)}{h}=\lim_{h\to 0}\frac{\int_{a}^{k+h}f(t)dt-\int_{a}^{k}f(t)dt}{h}=\lim_{h\to 0}\frac{\frac{d}{dh}\int_{a}^{k+h}f(t)dt-0}{1}=\lim_{h\to 0}f(k+h)=f(k)=\int_{a}^{k}f(t)dt g(x) (a,b) x=k<a x=k>b g(x) x=k<a x=k>b","['calculus', 'integration', 'limits', 'derivatives', 'definite-integrals']"
6,$n$-derivative of $m$-power of function,-derivative of -power of function,n m,"There is well-known Leibniz rule generalization  for the $n$ -th derivative of product of $m$ functions $f_1, f_2, \ldots, f_m$ , namely: $$ D^n(f_1 f_2 \cdots f_m)=\sum_{k_1+k_2+\cdots+k_m=n} \binom{n}{k_1 \,k_2 \, \cdots k_m} D^{k_1}(f_1)D^{k_2}(f_2)\cdots D^{k_m}(f_m). $$ Is there any simplification of the formula for the case $f_1=f_2=\cdots=f_m=f$ ? I hope there is a formula without the multinomial coefficients.","There is well-known Leibniz rule generalization  for the -th derivative of product of functions , namely: Is there any simplification of the formula for the case ? I hope there is a formula without the multinomial coefficients.","n m f_1, f_2, \ldots, f_m 
D^n(f_1 f_2 \cdots f_m)=\sum_{k_1+k_2+\cdots+k_m=n} \binom{n}{k_1 \,k_2 \, \cdots k_m} D^{k_1}(f_1)D^{k_2}(f_2)\cdots D^{k_m}(f_m).
 f_1=f_2=\cdots=f_m=f","['combinatorics', 'derivatives', 'algebraic-combinatorics']"
7,Matrix derivative and product rule,Matrix derivative and product rule,,"Let $A$ be a square matrix with non-negative elements. Let $n$ be a positive integer. How to evaluate the following for all possible $n$ ? $$f_{n}(A, i, j) = \frac{\partial }{\partial A_{ij}}\left(\vec{1}A^{n}\vec{1}^{\intercal}\right)$$ where $\vec{1}$ is a row vector and $\vec{1}^{\intercal}$ is transpose of $\vec{1}$ . Attempt to solve I thought $$f_{3}(A, i, j) =\vec{1}(BAA + ABA +AAB)\vec{1}^{\intercal}$$ where $$B_{i'j'} = \begin{cases} 1, & i = i', j=j' \\ 0, &\text{otherwise}\end{cases}$$ But the problem is that the expression is always non-negative. Unless the function is monotonic, I would not expect the first derivative to always have the same sign. What is the correct answer?","Let be a square matrix with non-negative elements. Let be a positive integer. How to evaluate the following for all possible ? where is a row vector and is transpose of . Attempt to solve I thought where But the problem is that the expression is always non-negative. Unless the function is monotonic, I would not expect the first derivative to always have the same sign. What is the correct answer?","A n n f_{n}(A, i, j) = \frac{\partial }{\partial A_{ij}}\left(\vec{1}A^{n}\vec{1}^{\intercal}\right) \vec{1} \vec{1}^{\intercal} \vec{1} f_{3}(A, i, j) =\vec{1}(BAA + ABA +AAB)\vec{1}^{\intercal} B_{i'j'} = \begin{cases} 1, & i = i', j=j' \\ 0, &\text{otherwise}\end{cases}","['matrices', 'derivatives', 'matrix-calculus']"
8,How does this Lie derivative axiom follow from my definition of the Lie derivative?,How does this Lie derivative axiom follow from my definition of the Lie derivative?,,"$\newcommand{\L}{\mathcal{L}}$ $\newcommand{\der}[2][]{\frac{d#1}{d#2}}$ $\newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}$ The definition of the Lie derivative which I'm starting with is $$ \L_v(\alpha) := \der{t} (\phi_t^*(\alpha))|_{t=0}$$ where $X$ is a manifold, $\alpha$ is some tensor, $v \in \Gamma(TX)$ is a vector field and $\phi : \mathbb{R} \times X \rightarrow X$ (writing $\phi_t(x) = \phi(t, x)$ ) is the 1-parameter group of diffeomorphisms on $X$ . Wikipedia says that this definition follows from four axioms, the first one being $$ \L_v(f) = v(f)$$ for $f$ a smooth function and the second one being $$ \L_v(S \otimes T) = \L_v(S) \otimes T + S \otimes \L_v(T) $$ (plus two more). I'd like to know how to show the first one I've written. Referring back to the definition I started with, I know that if $\alpha = f$ a smooth function then $\phi_t^*(f) = f \circ \phi_t$ . So using the chain rule $$ \begin{align} \L_v(f) &= \der{t}(f \circ \phi_t)|_{t=0} \\  &=  \sum_{k=1}^n\pder[f]{x_k} \der[x_k]{t}|_{t=0} \\ &= \sum_{k=1}^n\pder[f]{x_k} v^k \end{align} $$ Here $(x_1, \dots, x_n)$ are local coordinates on $X$ and I've written $v^k = \der[x_k]{t}|_{t=0}$ since these are real numbers (coefficients). But then I think that $\sum_{k=1}^n\pder[f]{x_k} v^k$ is what we get if we ""apply $v$ the vector field to $f$ "", i.e. $$ \L_v(f) = \sum_{k=1}^n\pder[f]{x_k} v^k = v(f) $$ Is this the correct way to do it? Thank you.","The definition of the Lie derivative which I'm starting with is where is a manifold, is some tensor, is a vector field and (writing ) is the 1-parameter group of diffeomorphisms on . Wikipedia says that this definition follows from four axioms, the first one being for a smooth function and the second one being (plus two more). I'd like to know how to show the first one I've written. Referring back to the definition I started with, I know that if a smooth function then . So using the chain rule Here are local coordinates on and I've written since these are real numbers (coefficients). But then I think that is what we get if we ""apply the vector field to "", i.e. Is this the correct way to do it? Thank you.","\newcommand{\L}{\mathcal{L}} \newcommand{\der}[2][]{\frac{d#1}{d#2}} \newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}  \L_v(\alpha) := \der{t} (\phi_t^*(\alpha))|_{t=0} X \alpha v \in \Gamma(TX) \phi : \mathbb{R} \times X \rightarrow X \phi_t(x) = \phi(t, x) X  \L_v(f) = v(f) f  \L_v(S \otimes T) = \L_v(S) \otimes T + S \otimes \L_v(T)  \alpha = f \phi_t^*(f) = f \circ \phi_t  \begin{align} \L_v(f) &= \der{t}(f \circ \phi_t)|_{t=0} \\ 
&=  \sum_{k=1}^n\pder[f]{x_k} \der[x_k]{t}|_{t=0} \\
&= \sum_{k=1}^n\pder[f]{x_k} v^k
\end{align}  (x_1, \dots, x_n) X v^k = \der[x_k]{t}|_{t=0} \sum_{k=1}^n\pder[f]{x_k} v^k v f  \L_v(f) = \sum_{k=1}^n\pder[f]{x_k} v^k = v(f) ","['derivatives', 'differential-geometry', 'smooth-manifolds', 'vector-fields', 'lie-derivative']"
9,How many dimensions will a derivative of a 3-D tensor by a 4-D tensor have? [closed],How many dimensions will a derivative of a 3-D tensor by a 4-D tensor have? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question As the title above, I find it hard to imagine or illustrate. It is a question from Coursera.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question As the title above, I find it hard to imagine or illustrate. It is a question from Coursera.",,"['matrices', 'derivatives', 'tensors', 'tensor-rank']"
10,How to compute the derivative of $\frac{\partial\mathbf{a}_k }{\partial{X}_{ij}}$ when $X = A^TB$,How to compute the derivative of  when,\frac{\partial\mathbf{a}_k }{\partial{X}_{ij}} X = A^TB,"Assume I have $$ X = A^TB $$ where $X \in R^{m \times n}, A \in R^{r \times m}, A \in R^{r \times n}$ . Let's define $A = [\mathbf{a}_1, ..., \mathbf{a}_m]$ where $\mathbf{a}_k $ is the $k$ -th column of matrix $A$ . How to compute the following derivative? $$\frac{\partial\mathbf{a}_k }{\partial{X}_{ij}}$$",Assume I have where . Let's define where is the -th column of matrix . How to compute the following derivative?," X = A^TB  X \in R^{m \times n}, A \in R^{r \times m}, A \in R^{r \times n} A = [\mathbf{a}_1, ..., \mathbf{a}_m] \mathbf{a}_k  k A \frac{\partial\mathbf{a}_k }{\partial{X}_{ij}}","['linear-algebra', 'derivatives', 'partial-derivative', 'matrix-calculus']"
11,"Differentiability, linear operators","Differentiability, linear operators",,"Let $Y$ be a complete normed linear space, and let $M$ denote the space of bounded linear operators from $Y$ to itself. Let $L : M → M$ be the map defined by $L(A) := A^2$ . I am supposed to show that L is differentiable at each $A\in M$ , and then find the derivatives. Tried to find the derivatives like this: $L'(A) = \lim_{h \to 0} \frac{L(A+h)-L(A)}{h}= \lim_{h \to 0} \frac{(A+h)^2-A^2}{h}= \lim_{h \to 0} \frac{A^2+2Ah-h^2-A^2}{h} = \lim_{h \to 0} 2A+h = \underline{2A}$ If this is correct, all I have to do now is to show that $L$ is differentiable at all $A\in M$ , and I'm wondering if I can do that by saying that $L$ is a composition of $A$ and $A$ since $A$ is a linear operator, and all linear operators are differentiable?","Let be a complete normed linear space, and let denote the space of bounded linear operators from to itself. Let be the map defined by . I am supposed to show that L is differentiable at each , and then find the derivatives. Tried to find the derivatives like this: If this is correct, all I have to do now is to show that is differentiable at all , and I'm wondering if I can do that by saying that is a composition of and since is a linear operator, and all linear operators are differentiable?",Y M Y L : M → M L(A) := A^2 A\in M L'(A) = \lim_{h \to 0} \frac{L(A+h)-L(A)}{h}= \lim_{h \to 0} \frac{(A+h)^2-A^2}{h}= \lim_{h \to 0} \frac{A^2+2Ah-h^2-A^2}{h} = \lim_{h \to 0} 2A+h = \underline{2A} L A\in M L A A A,"['real-analysis', 'derivatives', 'normed-spaces', 'frechet-derivative']"
12,Baby Rudin 5.2: Continuity Required to prove Differentiability?,Baby Rudin 5.2: Continuity Required to prove Differentiability?,,"(From Rudin Principles of Mathematical Analysis , 5.2) Suppose $f'(x) > 0$ in ( $a, b$ ). Prove that $f$ is strictly increasing in ( $a, b$ ), and let $g$ be its inverse function. Prove that $g$ is differentiable, and that $g'(f(x)) = \frac{1}{f′(x)}   \quad (a < x < b)$ Here's an answer I found online: Let $g : f(a, b) → (a, b) $ be the inverse function of $f$ , i.e., $ g(f(x)) = x $ for all $x ∈ (a, b)$ . We now show that $g′(y) = \lim\limits_{z→y} \frac {g(z) − g(y)}{z − y}$ exists for all $y ∈ f(a, b)$ . Put $y = f(x)$ and $z = f(t)$ , where $x, t ∈ (a, b)$ , then since $f$ is continuous (by Theorem 5.2), so is $g$ (by Theorem 4.17), and $z → y$ implies $t → x$ . It follows that $$\begin{align*} \lim_{z→y} \frac{g(z) − g(y)}{z − y} &= \lim_{t→x}\frac{g(f(t)) − g(f(x))}{f(t) − f(x)} \\ &= \lim_{t→x}\frac{t − x}{f(t) − f(x)} \\ &= \lim_{t→x}\frac{1}{\frac{f(t) − f(x)}{t − x}} \\ &= \frac{1}{f′(x)} \end{align*}$$ Question: Why it is necessary to for g to be continuous? The only step that uses continuity is the changing of the limit values ( $z → y$ implies $t → x$ ), but that comes from $f$ I think?","(From Rudin Principles of Mathematical Analysis , 5.2) Suppose in ( ). Prove that is strictly increasing in ( ), and let be its inverse function. Prove that is differentiable, and that Here's an answer I found online: Let be the inverse function of , i.e., for all . We now show that exists for all . Put and , where , then since is continuous (by Theorem 5.2), so is (by Theorem 4.17), and implies . It follows that Question: Why it is necessary to for g to be continuous? The only step that uses continuity is the changing of the limit values ( implies ), but that comes from I think?","f'(x) > 0 a, b f a, b g g g'(f(x)) = \frac{1}{f′(x)}   \quad
(a < x < b) g : f(a, b) → (a, b)  f  g(f(x)) = x  x ∈ (a, b) g′(y) = \lim\limits_{z→y} \frac {g(z) − g(y)}{z − y} y ∈ f(a, b) y = f(x) z = f(t) x, t ∈ (a, b) f g z → y t → x \begin{align*}
\lim_{z→y} \frac{g(z) − g(y)}{z − y} &= \lim_{t→x}\frac{g(f(t)) − g(f(x))}{f(t) − f(x)} \\
&= \lim_{t→x}\frac{t − x}{f(t) − f(x)} \\
&= \lim_{t→x}\frac{1}{\frac{f(t) − f(x)}{t − x}} \\
&= \frac{1}{f′(x)}
\end{align*} z → y t → x f","['real-analysis', 'derivatives', 'continuity', 'proof-explanation']"
13,How to compute the gradient $\nabla_W \left( x^TW^{-T}W^{-1}x \right)$?,How to compute the gradient ?,\nabla_W \left( x^TW^{-T}W^{-1}x \right),Calculate the following gradient $$\nabla_W \left( x^TW^{-T}W^{-1}x \right)$$ where $W$ is a $\mathbb{R}^{d×d}$ matrix and $x$ is a $\mathbb{R}^d$ vector. The result should be a $\mathbb{R}^{d×d}$ matrix. I wonder whether there is a clean and compact form of the result. I first tried to write this as $$2(W^{-1}x)\frac{d(W^{-1}x)}{dW}$$ but the latter one is a tensor (actually it should be $2 \sum_{i=1}^d(W^{-1}x)_i\frac{d(W^{-1}x)_i}{dW}$ but the next step is messy.,Calculate the following gradient where is a matrix and is a vector. The result should be a matrix. I wonder whether there is a clean and compact form of the result. I first tried to write this as but the latter one is a tensor (actually it should be but the next step is messy.,\nabla_W \left( x^TW^{-T}W^{-1}x \right) W \mathbb{R}^{d×d} x \mathbb{R}^d \mathbb{R}^{d×d} 2(W^{-1}x)\frac{d(W^{-1}x)}{dW} 2 \sum_{i=1}^d(W^{-1}x)_i\frac{d(W^{-1}x)_i}{dW},"['matrices', 'derivatives', 'matrix-calculus']"
14,Going from the differential to the derivative (Frechet and matrix calculus),Going from the differential to the derivative (Frechet and matrix calculus),,"For a function $f: A \rightarrow B$ , Frechet differentiability tells us that we want to find a linear operator that satisfies $$\lim_{H\rightarrow 0} \frac{||f[X+H] - f[X] - G[H]||}{||H||} = 0$$ This would mean that $G$ is a good approximation of the change in $f$ at $X$ for some small $H\in A$ . That is for the operator $df: A \rightarrow B$ $$df(X)[H] = G[H]$$ Wikipedia says that $G$ is defined as the Frechet derivative of $f$ at $X$ . But I have a litte trouble connecting this to the traditional notion of the derivative, where we have a fraction i.e. something like $\frac{dy}{dx}$ . For example, consider a standard formula from the matrix cookbook $$\frac{dTr(XA)}{dX} = A^T$$ The Frechet differentiability definition gets me up to $$dTr(XA)[H] = G[H] = Tr(HA).$$ What is then done is \begin{align} dTr(XA)[H] &= Tr(HA) \\ &= A^T :H, \end{align} where we just use the notation $A:B = Tr(A^TB)$ . What is the correct way to go from here to the conclusion that $$\frac{dTr(XA)}{dX} = A^T$$ and what does the LHS even represent exactly since it's not a fraction in the traditional sense? More generally, what is involved in going from the differential form i.e. $df(X)[H] = G[H]$ to the derivative form $G = \frac{df(X)}{dX}$ ?","For a function , Frechet differentiability tells us that we want to find a linear operator that satisfies This would mean that is a good approximation of the change in at for some small . That is for the operator Wikipedia says that is defined as the Frechet derivative of at . But I have a litte trouble connecting this to the traditional notion of the derivative, where we have a fraction i.e. something like . For example, consider a standard formula from the matrix cookbook The Frechet differentiability definition gets me up to What is then done is where we just use the notation . What is the correct way to go from here to the conclusion that and what does the LHS even represent exactly since it's not a fraction in the traditional sense? More generally, what is involved in going from the differential form i.e. to the derivative form ?","f: A \rightarrow B \lim_{H\rightarrow 0} \frac{||f[X+H] - f[X] - G[H]||}{||H||} = 0 G f X H\in A df: A \rightarrow B df(X)[H] = G[H] G f X \frac{dy}{dx} \frac{dTr(XA)}{dX} = A^T dTr(XA)[H] = G[H] = Tr(HA). \begin{align}
dTr(XA)[H] &= Tr(HA) \\
&= A^T :H,
\end{align} A:B = Tr(A^TB) \frac{dTr(XA)}{dX} = A^T df(X)[H] = G[H] G = \frac{df(X)}{dX}","['derivatives', 'matrix-calculus', 'frechet-derivative']"
15,Galilean transformation and differentiation,Galilean transformation and differentiation,,"Given $x=x’-vt$ and $t=t’$ , why is $\frac{\partial t}{\partial x’}=0$ instead of $1/v$ ? Maybe the answer has something to do with the fact that $dx’=dx$ in this Galilean transformation. Is $dx’=dx$ always the case for Galilean transformations?","Given and , why is instead of ? Maybe the answer has something to do with the fact that in this Galilean transformation. Is always the case for Galilean transformations?",x=x’-vt t=t’ \frac{\partial t}{\partial x’}=0 1/v dx’=dx dx’=dx,"['calculus', 'derivatives', 'physics', 'transformation']"
16,Proof that $\lim\limits_{x\to \infty} (1+\frac{1}{\ln x})^x = \infty$,Proof that,\lim\limits_{x\to \infty} (1+\frac{1}{\ln x})^x = \infty,"I have been trying to prove that $$ \lim\limits_{x\to \infty} (1+\frac{1}{\ln x})^x = \infty $$ and this is what I got: $$ \lim\limits_{x\to \infty} (1+\frac{1}{\ln x})^x = \lim\limits_{x\to \infty} e^{\ln (1+\frac{1}{\ln x})^x} = \lim\limits_{x\to \infty} e^{x * \ln (1+\frac{1}{\ln x})}   $$ Then due to the fact that the e function is continuous and that $a*b=\frac{a}{\frac1b}$ $$ =  e^{ \lim\limits_{x\to \infty} \frac{\ln (1+\frac{1}{\ln x})}{\frac1x} }   $$ Since both the top and the bottom go to 0 as ${x\to \infty}$ we can apply L'Hospital and after deriving both we get $$ e^{ \lim\limits_{x\to \infty} \frac{-\frac{1}{x*\ln x+x*\ln^2 x}}{-\frac{1}{x^2}}} =  e^{ \lim\limits_{x\to \infty} \frac{x^2}{x*(\ln x+\ln^2 x)}} = e^{ \lim\limits_{x\to \infty} \frac{x}{\ln x+\ln^2 x}} $$ and then since the x function grows much more rapidly than the logarithmic functions at any power, we get $$ e^{ \lim\limits_{x\to \infty} \frac{x}{\ln x+\ln^2 x}} = e^{\infty} = \infty  $$ Since I am quite new to calculus I don't feel sure at all about what I just did so it would be great if I could get some feedback from experienced people. Also it's my first post here, I hope I didn't break any rule. In the meantime, I wish everyone a nice day.","I have been trying to prove that and this is what I got: Then due to the fact that the e function is continuous and that Since both the top and the bottom go to 0 as we can apply L'Hospital and after deriving both we get and then since the x function grows much more rapidly than the logarithmic functions at any power, we get Since I am quite new to calculus I don't feel sure at all about what I just did so it would be great if I could get some feedback from experienced people. Also it's my first post here, I hope I didn't break any rule. In the meantime, I wish everyone a nice day."," \lim\limits_{x\to \infty} (1+\frac{1}{\ln x})^x = \infty   \lim\limits_{x\to \infty} (1+\frac{1}{\ln x})^x =
\lim\limits_{x\to \infty} e^{\ln (1+\frac{1}{\ln x})^x} = \lim\limits_{x\to \infty} e^{x * \ln (1+\frac{1}{\ln x})}  
 a*b=\frac{a}{\frac1b} 
=  e^{ \lim\limits_{x\to \infty} \frac{\ln (1+\frac{1}{\ln x})}{\frac1x} }  
 {x\to \infty} 
e^{ \lim\limits_{x\to \infty} \frac{-\frac{1}{x*\ln x+x*\ln^2 x}}{-\frac{1}{x^2}}} = 
e^{ \lim\limits_{x\to \infty} \frac{x^2}{x*(\ln x+\ln^2 x)}} =
e^{ \lim\limits_{x\to \infty} \frac{x}{\ln x+\ln^2 x}}
 
e^{ \lim\limits_{x\to \infty} \frac{x}{\ln x+\ln^2 x}} = e^{\infty} = \infty 
","['calculus', 'limits', 'derivatives', 'logarithms', 'exponential-function']"
17,Derivative of the definite integral,Derivative of the definite integral,,"I have to find the derivative for $$\int_{-1}^x \frac{t^2}{t^2+4}dt-\int_{3}^x \frac{t^2}{t^2+4}dt$$ When I calculate their derivative separately $$\frac {d}{dt} \left(\int_{-1}^x \frac{t^2}{t^2+4}dt\right)-\frac {d}{dt}\left(\int_{3}^x \frac{t^2}{t^2+4}dt\right)$$ it gives me the right result, that is $0$ . But according to the law of additive \begin{aligned} &\int_{-1}^x \frac{t^2}{t^2+4}dt-\int_{3}^x \frac{t^2}{t^2+4}dt \\ = & \int_{-1}^x \frac{t^2}{t^2+4}dt+\int_{x}^3 \frac{t^2}{t^2+4}dt \\= &\int_{-1}^3 \frac{t^2}{t^2+4}dt \\[2em] \end{aligned} $$d/dt\int_{-1}^3 \frac{t^2}{t^2+4}dt = \frac{3^2}{3^2+4} = \frac 9{13}$$ Thanks.","I have to find the derivative for When I calculate their derivative separately it gives me the right result, that is . But according to the law of additive Thanks.","\int_{-1}^x \frac{t^2}{t^2+4}dt-\int_{3}^x \frac{t^2}{t^2+4}dt \frac {d}{dt} \left(\int_{-1}^x \frac{t^2}{t^2+4}dt\right)-\frac {d}{dt}\left(\int_{3}^x \frac{t^2}{t^2+4}dt\right) 0 \begin{aligned}
&\int_{-1}^x \frac{t^2}{t^2+4}dt-\int_{3}^x \frac{t^2}{t^2+4}dt \\
= & \int_{-1}^x \frac{t^2}{t^2+4}dt+\int_{x}^3 \frac{t^2}{t^2+4}dt \\= &\int_{-1}^3 \frac{t^2}{t^2+4}dt \\[2em]
\end{aligned} d/dt\int_{-1}^3 \frac{t^2}{t^2+4}dt = \frac{3^2}{3^2+4} = \frac 9{13}","['derivatives', 'definite-integrals']"
18,Calculate the derivative $f(x)=\lfloor x\rfloor(\sin(\pi x))^{2}$,Calculate the derivative,f(x)=\lfloor x\rfloor(\sin(\pi x))^{2},"I have a problem with this task because answer which I have does not match the right answer and I don't know where is a mistake. My try: For $x\in \mathbb Z$ $f'_{+}(x)=f'_{-}(x)=0$ so $f'(x)$ exist For $x \in (n,n+1), n \in \mathbb Z, f(x)=n(\sin(\pi x))^{2}$ So \begin{align}f'(x)&=1\cdot n^{0}(\sin(\pi x))^{2}+n((\sin(\pi x))^{2})'\\&=(\sin(\pi x))^{2}+n2\sin(\pi x)\cos(\pi x)\\&=(\sin(\pi x))^{2}+\sin(2\pi x)n\end{align} In the answer is: $f'(x)=n \pi\sin(2\pi x)$ and then $f'(x)=\lfloor x\rfloor\pi\sin(2\pi x)$ because for $x \in \mathbb Z$ $f'(x)$ also exist. Why?",I have a problem with this task because answer which I have does not match the right answer and I don't know where is a mistake. My try: For so exist For So In the answer is: and then because for also exist. Why?,"x\in \mathbb Z f'_{+}(x)=f'_{-}(x)=0 f'(x) x \in (n,n+1), n \in \mathbb Z, f(x)=n(\sin(\pi x))^{2} \begin{align}f'(x)&=1\cdot n^{0}(\sin(\pi x))^{2}+n((\sin(\pi x))^{2})'\\&=(\sin(\pi x))^{2}+n2\sin(\pi x)\cos(\pi x)\\&=(\sin(\pi x))^{2}+\sin(2\pi x)n\end{align} f'(x)=n \pi\sin(2\pi x) f'(x)=\lfloor x\rfloor\pi\sin(2\pi x) x \in \mathbb Z f'(x)","['real-analysis', 'derivatives', 'ceiling-and-floor-functions']"
19,Questions on the proof of $f*g\in C^\infty(\mathbb R)$ when $f\in L^2(\mathbb R)$ and $g\in C_c^\infty(\mathbb R)$,Questions on the proof of  when  and,f*g\in C^\infty(\mathbb R) f\in L^2(\mathbb R) g\in C_c^\infty(\mathbb R),"I am working through the proof that the convolution of a square integrable function with a compactly supported continuously differentiable function is itself continuously differentiable: ""Let $f\in L^2(\mathbb R)$ and $g\in C_c^\infty(\mathbb R)$ . Show that $f*g\in C^\infty(\mathbb R)$ and that $(f*g)^{(k)}=f*g^{(k)}$ for $k\in\mathbb N$ ."" To this end I have been making use of the following questions and the particularly linked answers: Convolution of locally integrable and compactly supported infinitely differentiable function Derivative of convolution Differentiating under integral for convolution I am confident that I understand the idea of the proof, however, there are some points which are common throughout each of the attempts which I am not entirely certain on and would appreciate to have better explained. How, exactly, is the Lebesgue Dominated Convergence Theorem being applied? In taking the difference quotient (in working with the derivative) we obtain something like, $$\lim_{h\to0}\int_\mathbb{R}f(z)\frac{g(x+h-z)-g(x-z)}h\text{d}z,$$ for which we want to find some integrable function $\psi$ so that for all $z\in\mathbb R$ , $$\left|\frac{g(x+h-z)-g(x-z)}h\right|<\psi(z).$$ That is to say, we want to find a function, $\psi$ , which dominates the above difference quotient. I see that that the hypotheses of the LDCT are satisfied (since $g\in C_c^\infty$ it is Borel measurable), but how do we rectify the fact that our sequence (the difference quotient) isn't indexed by the natural numbers? How do we apply LDCT when we have $0<h<1$ , which is uncountable. In order to find the dominating $\psi$ , as mentioned above, we make use of the Mean Value Theorem (Rather than just assume that such a dominating function exists, we should move to draw out the specific existence of such a function). In the first of the linked questions, this is done as follows, \begin{eqnarray*} 	\left|g(x + h - z) - g(x - z)\right| 	& = &  	\left| 	\int_0^1\frac{\rm d}{{\rm d}s} g(x - z + sh)\; {\rm d} s 	\right| 	\\ 	&\leq &  	|h|\max_{x\in \mathbb R} |g'(x)|.  \end{eqnarray*} How is it that $g'(x-z+sh)$ for $s\in(0,1)$ is bounded by $g'(x)$ as a function of $x$ alone? I am thinking that one defines $g'_s(x):=g'(x - z + sh)$ for $s\in(0,1)$ and then argues that for all $s\in(0,1)$ , $|g'_s(x)|<|g'(x)|$ for all $x\in\mathbb R$ so that the sequence of functions $(g_s)_{s\in(0,1)}$ is uniformly bounded. But how does one transition from dealing with $g'(x-z+sh)$ to $g'(x)$ ? And how does this affect our considerations of the support we are on?","I am working through the proof that the convolution of a square integrable function with a compactly supported continuously differentiable function is itself continuously differentiable: ""Let and . Show that and that for ."" To this end I have been making use of the following questions and the particularly linked answers: Convolution of locally integrable and compactly supported infinitely differentiable function Derivative of convolution Differentiating under integral for convolution I am confident that I understand the idea of the proof, however, there are some points which are common throughout each of the attempts which I am not entirely certain on and would appreciate to have better explained. How, exactly, is the Lebesgue Dominated Convergence Theorem being applied? In taking the difference quotient (in working with the derivative) we obtain something like, for which we want to find some integrable function so that for all , That is to say, we want to find a function, , which dominates the above difference quotient. I see that that the hypotheses of the LDCT are satisfied (since it is Borel measurable), but how do we rectify the fact that our sequence (the difference quotient) isn't indexed by the natural numbers? How do we apply LDCT when we have , which is uncountable. In order to find the dominating , as mentioned above, we make use of the Mean Value Theorem (Rather than just assume that such a dominating function exists, we should move to draw out the specific existence of such a function). In the first of the linked questions, this is done as follows, How is it that for is bounded by as a function of alone? I am thinking that one defines for and then argues that for all , for all so that the sequence of functions is uniformly bounded. But how does one transition from dealing with to ? And how does this affect our considerations of the support we are on?","f\in L^2(\mathbb R) g\in C_c^\infty(\mathbb R) f*g\in C^\infty(\mathbb R) (f*g)^{(k)}=f*g^{(k)} k\in\mathbb N \lim_{h\to0}\int_\mathbb{R}f(z)\frac{g(x+h-z)-g(x-z)}h\text{d}z, \psi z\in\mathbb R \left|\frac{g(x+h-z)-g(x-z)}h\right|<\psi(z). \psi g\in C_c^\infty 0<h<1 \psi \begin{eqnarray*}
	\left|g(x + h - z) - g(x - z)\right|
	& = & 
	\left|
	\int_0^1\frac{\rm d}{{\rm d}s} g(x - z + sh)\; {\rm d} s
	\right|
	\\
	&\leq & 
	|h|\max_{x\in \mathbb R} |g'(x)|. 
\end{eqnarray*} g'(x-z+sh) s\in(0,1) g'(x) x g'_s(x):=g'(x - z + sh) s\in(0,1) s\in(0,1) |g'_s(x)|<|g'(x)| x\in\mathbb R (g_s)_{s\in(0,1)} g'(x-z+sh) g'(x)","['calculus', 'measure-theory', 'derivatives', 'lebesgue-integral', 'convolution']"
20,Derivation of $\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$ [duplicate],Derivation of  [duplicate],\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T,"This question already has answers here : Gradient of $a^T X b$ with respect to $X$ (3 answers) Closed 3 years ago . I would like to see a detailed, step-by-step derivation of the following identity $$\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T$$ where $x, y \in \mathbb R^n$ and $A \in \mathbb R^{n \times n}$ . I thought it would be easy to do using Einstein notation, but I am messing up with the reciprocal basis.","This question already has answers here : Gradient of $a^T X b$ with respect to $X$ (3 answers) Closed 3 years ago . I would like to see a detailed, step-by-step derivation of the following identity where and . I thought it would be easy to do using Einstein notation, but I am messing up with the reciprocal basis.","\frac{\partial}{\partial A} \left( y^T A x \right) = y x^T x, y \in \mathbb R^n A \in \mathbb R^{n \times n}","['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields']"
21,Derivative of power series :,Derivative of power series :,,"If $ m  ∈ ]0,1[ $ and $ f(x)=\sum_{k>=1}\frac{m^kx^k}{k}$ , $-1/m < x < 1/m,$ then $ f'(1)$ equals to: a) $m$ b) $0$ c) $m/(1+m)$ d) $1/(1-m)$ e) $m/(1-m)$ I tried solving it and got $$ f(x)=\sum_{k\geq 1}\frac{m^kx^k}{k},$$ $$ f'(x)=\sum_{k\geq 1}\frac{m^kkx^{k-1}}{k},$$ $$ f'(1)=\sum_{k\geq 1}{m^k1^{k-1}}$$ Now, what should I do? Can someone help me, please?","If and , then equals to: a) b) c) d) e) I tried solving it and got Now, what should I do? Can someone help me, please?"," m  ∈ ]0,1[   f(x)=\sum_{k>=1}\frac{m^kx^k}{k} -1/m < x < 1/m,  f'(1) m 0 m/(1+m) 1/(1-m) m/(1-m)  f(x)=\sum_{k\geq 1}\frac{m^kx^k}{k},  f'(x)=\sum_{k\geq 1}\frac{m^kkx^{k-1}}{k},  f'(1)=\sum_{k\geq 1}{m^k1^{k-1}}","['calculus', 'sequences-and-series', 'derivatives']"
22,Calc2 Finding $f'(2)$ from tangent line,Calc2 Finding  from tangent line,f'(2),"So, I have a Calc 2 problem I am stuck on. The tangent line to $h(x)$ at $x = 2$ is $3x - 2$ . It says to find $f'(2)$ given $f(x) = -3[h(x)]^2 + 2x + 2$ Any ideas on how to go about this? Thank you!","So, I have a Calc 2 problem I am stuck on. The tangent line to at is . It says to find given Any ideas on how to go about this? Thank you!",h(x) x = 2 3x - 2 f'(2) f(x) = -3[h(x)]^2 + 2x + 2,"['derivatives', 'tangent-line']"
23,"In the derivative of the product of two functions , why (dx)² is ignored?","In the derivative of the product of two functions , why (dx)² is ignored?",,"I was digging deeply in the fundamentals of calculus, I found that in the famous '3Blue1Brown' channel, when demonstrating the process of finding the derivative of the product of two functions , (dx) ² is ignored because it becomes so tiny when dx is going closer and closer to zero. I think that our calculus can be more precise if we don't ignore it, actually I think we shouldn't ignore it as this is math and not physics or such, the question is how our calculus is still valid and reliable if we give up even a tiny bit of precision. The following image shows the details of the demonstration:","I was digging deeply in the fundamentals of calculus, I found that in the famous '3Blue1Brown' channel, when demonstrating the process of finding the derivative of the product of two functions , (dx) ² is ignored because it becomes so tiny when dx is going closer and closer to zero. I think that our calculus can be more precise if we don't ignore it, actually I think we shouldn't ignore it as this is math and not physics or such, the question is how our calculus is still valid and reliable if we give up even a tiny bit of precision. The following image shows the details of the demonstration:",,"['calculus', 'derivatives', 'products']"
24,"Is there a function $f \in C[0,1]$ such that $f(0)=f(1)=0$, $f'(0)=0$, $f'(1)=1$ and $\|f\|_{C[0,1]} < \epsilon$, $\|f'\|_{C[0,1]} < \epsilon$","Is there a function  such that , ,  and ,","f \in C[0,1] f(0)=f(1)=0 f'(0)=0 f'(1)=1 \|f\|_{C[0,1]} < \epsilon \|f'\|_{C[0,1]} < \epsilon","Given $\epsilon \in (0,1)$ I am looking for a twice continuously differentiable function $f\colon [0,1] \to \mathbb{R}$ satisfying the following conditions: $$ f(0)=f(1) =f'(1)=0, \quad f'(0)=1, $$ $$ \sup_{x \in [0,1]} |f(x)| \leq \epsilon, \qquad \int_0^1 |f'(x)| dx \leq \epsilon. $$ It is quite obvious that such function exists, since we can set $f(x) = 0$ for $x \in [\epsilon,1]$ , and on the interval $[0,\epsilon]$ , $f$ can resemble $$1_{[0,\epsilon]}(x)x + 1_{[\epsilon,2\epsilon]}(x)(2\epsilon - x).$$ Of course the above function is not (even once) differentiable, so I would like a smooth version of it. Is there some simple formula for such function?","Given I am looking for a twice continuously differentiable function satisfying the following conditions: It is quite obvious that such function exists, since we can set for , and on the interval , can resemble Of course the above function is not (even once) differentiable, so I would like a smooth version of it. Is there some simple formula for such function?","\epsilon \in (0,1) f\colon [0,1] \to \mathbb{R} 
f(0)=f(1) =f'(1)=0, \quad f'(0)=1,
 
\sup_{x \in [0,1]} |f(x)| \leq \epsilon, \qquad \int_0^1 |f'(x)| dx \leq \epsilon.
 f(x) = 0 x \in [\epsilon,1] [0,\epsilon] f 1_{[0,\epsilon]}(x)x + 1_{[\epsilon,2\epsilon]}(x)(2\epsilon - x).","['real-analysis', 'derivatives', 'continuity']"
25,Take the derivative of a product of sequence,Take the derivative of a product of sequence,,"How to take the derivative of $\prod_{i=1}^{n}(1-e^{-\lambda _{i}\cdot x})I(x>0)$ with regard to x? Here $F(X)=\prod_{i=1}^{n}(1-e^{-\lambda _{i}\cdot x})I(x>0)$ is a CDF, and I want to take the derivative of it and get the pdf of X. I try to take the log of F(X), so $\frac{d}{d x}logF(x)=\frac{d logF(x)}{d F(x)}\cdot \frac{d F(x)}{d x}$ , @Ankit Kumar help me with: $$\frac{d(logF(x))}{dx}=\sum_{i=1}^n\frac{d(log(1-e^{-\lambda_ix}))}{dx}$$ , then $$\frac{1}{F(x)}\frac{d(F(x))}{dx}=\sum_{i=1}^n\frac{1}{\log(1-e^{-\lambda_ix})}{\lambda_ie^{-\lambda_ix}}$$ , but I don't know how to move on with $\frac{d(F(x))}{dx}=\sum_{i=1}^n\frac{1}{\log(1-e^{-\lambda_ix})}{\lambda_ie^{-\lambda_ix}}\cdot \prod_{i=1}^{n}(1-e^{-\lambda _{i}\cdot x})$ , how to simplify it? since it is a multiplication of a sum of sequence and a product of sequence.","How to take the derivative of with regard to x? Here is a CDF, and I want to take the derivative of it and get the pdf of X. I try to take the log of F(X), so , @Ankit Kumar help me with: , then , but I don't know how to move on with , how to simplify it? since it is a multiplication of a sum of sequence and a product of sequence.",\prod_{i=1}^{n}(1-e^{-\lambda _{i}\cdot x})I(x>0) F(X)=\prod_{i=1}^{n}(1-e^{-\lambda _{i}\cdot x})I(x>0) \frac{d}{d x}logF(x)=\frac{d logF(x)}{d F(x)}\cdot \frac{d F(x)}{d x} \frac{d(logF(x))}{dx}=\sum_{i=1}^n\frac{d(log(1-e^{-\lambda_ix}))}{dx} \frac{1}{F(x)}\frac{d(F(x))}{dx}=\sum_{i=1}^n\frac{1}{\log(1-e^{-\lambda_ix})}{\lambda_ie^{-\lambda_ix}} \frac{d(F(x))}{dx}=\sum_{i=1}^n\frac{1}{\log(1-e^{-\lambda_ix})}{\lambda_ie^{-\lambda_ix}}\cdot \prod_{i=1}^{n}(1-e^{-\lambda _{i}\cdot x}),"['calculus', 'probability', 'derivatives']"
26,global max and min as limit to +-infinity equals 0,global max and min as limit to +-infinity equals 0,,"In an exercise I have to make I am asked to show that a function $f$ has a global maximum and minimum, given that: $f:\mathbb{R}\to\mathbb{R}$ is continuous and $$\lim_{x\to\infty}f(x)=0.$$ However, in my mind, the function $f(x)=e^{-x^2}$ satisfies those conditions and has an global maximum, but no minimum at all. Am I misinterpreting the given information?","In an exercise I have to make I am asked to show that a function has a global maximum and minimum, given that: is continuous and However, in my mind, the function satisfies those conditions and has an global maximum, but no minimum at all. Am I misinterpreting the given information?",f f:\mathbb{R}\to\mathbb{R} \lim_{x\to\infty}f(x)=0. f(x)=e^{-x^2},"['real-analysis', 'limits', 'derivatives', 'continuity']"
27,Proof with theorems like mean value theorem,Proof with theorems like mean value theorem,,"This must be a very elementary problem, someone may asked before, I am not much exposed to math, I don't know what keyword to search it. I'm learning from Spivak's calculus, in Chapter 11, question 28, he asked to prove: if $f'(x)\le M$ , for all $x$ in $[a,b]$ , then $f(b)-f(a)\le M(b-a)$ . The answer given in the answer book is: We have $$\frac{f(b)-f(a)}{b-a}=f'(x) \quad for\,some\,x\,in\,(a,b) $$ $$\le M,\quad\quad\quad\quad\,\,\,$$ so $f(b)-f(a)\le M(b-a)$ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Since the mean value theorem states only ""for some"", then for some function I can choose $\frac{f(b)-f(a)}{b-a}>f'(x)$ , even though, I can't prove such case must exist, but would the possibility that such case exist weaker the proof? And more I want to know is how to make the most use of theorems contain ""for some $x$ ""? What kind of stuff should I read as a math beginner?","This must be a very elementary problem, someone may asked before, I am not much exposed to math, I don't know what keyword to search it. I'm learning from Spivak's calculus, in Chapter 11, question 28, he asked to prove: if , for all in , then . The answer given in the answer book is: We have so ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Since the mean value theorem states only ""for some"", then for some function I can choose , even though, I can't prove such case must exist, but would the possibility that such case exist weaker the proof? And more I want to know is how to make the most use of theorems contain ""for some ""? What kind of stuff should I read as a math beginner?","f'(x)\le M x [a,b] f(b)-f(a)\le M(b-a) \frac{f(b)-f(a)}{b-a}=f'(x) \quad for\,some\,x\,in\,(a,b)  \le M,\quad\quad\quad\quad\,\,\, f(b)-f(a)\le M(b-a) \frac{f(b)-f(a)}{b-a}>f'(x) x","['real-analysis', 'derivatives', 'logic']"
28,Calculate the following limit without L'Hopital,Calculate the following limit without L'Hopital,,"$\lim\limits_{x\to 0}\frac{e^x-\sqrt{1+2x+2x^2}}{x+\tan (x)-\sin (2x)}$ I know how to count this limit with the help of l'Hopital rule. But it is very awful, because I need 3 times derivate it. So, there is very difficult calculations. I have the answer $\frac{2}{5}$ . I want to know if there is other ways to calculate it, without 3 times using l'Hopital rule? (I could write my steps, but they are very big. I just took third derivative of numerator and denominator)","I know how to count this limit with the help of l'Hopital rule. But it is very awful, because I need 3 times derivate it. So, there is very difficult calculations. I have the answer . I want to know if there is other ways to calculate it, without 3 times using l'Hopital rule? (I could write my steps, but they are very big. I just took third derivative of numerator and denominator)",\lim\limits_{x\to 0}\frac{e^x-\sqrt{1+2x+2x^2}}{x+\tan (x)-\sin (2x)} \frac{2}{5},['limits']
29,Do not understand L'Hopital Rule,Do not understand L'Hopital Rule,,"I have just started learning L'Hopital rule, and so far I thought I understood everything until I stumbled upon this question $$\lim_{x\to 0}  \frac{\ln(\cos(ax))}{\ln(\cos(bx))}.$$ To this, eventually got $$\lim_{x\to 0}  \frac{a \sin(ax) \cos(bx)}{b \sin(bx)\cos(ax)}$$ From my knowledge, $\sin(0)$ is 0!! and the whole thing will be '' $\frac{0}{0}$ '', however the answer key I was given does not continue the implementation of the L'Hopital rule, but instead obtains the answer $\frac{a^2}{b^2}$ . Is there some important concept I'm missing out? Or is the differentiation supposed to continue and the answer key just skipped the steps?","I have just started learning L'Hopital rule, and so far I thought I understood everything until I stumbled upon this question To this, eventually got From my knowledge, is 0!! and the whole thing will be '' '', however the answer key I was given does not continue the implementation of the L'Hopital rule, but instead obtains the answer . Is there some important concept I'm missing out? Or is the differentiation supposed to continue and the answer key just skipped the steps?",\lim_{x\to 0}  \frac{\ln(\cos(ax))}{\ln(\cos(bx))}. \lim_{x\to 0}  \frac{a \sin(ax) \cos(bx)}{b \sin(bx)\cos(ax)} \sin(0) \frac{0}{0} \frac{a^2}{b^2},"['calculus', 'real-analysis', 'limits', 'derivatives']"
30,Construction of function with function going to 0 but derivative not as X tends to infinity,Construction of function with function going to 0 but derivative not as X tends to infinity,,"I am interested in finding Example of twice differentiable function on $(0,\infty)$ $f(x)\to 0 , x\to \infty$ but $f'(x)$ not tends to 0 as $x$ goes to infinity. I had already proved that in case of $f''(x)$ is bounded then we can show that above is not true. I tried to get some examples, but did not succeed. Any help will be appreciated!","I am interested in finding Example of twice differentiable function on but not tends to 0 as goes to infinity. I had already proved that in case of is bounded then we can show that above is not true. I tried to get some examples, but did not succeed. Any help will be appreciated!","(0,\infty) f(x)\to 0 , x\to \infty f'(x) x f''(x)","['real-analysis', 'derivatives', 'examples-counterexamples']"
31,"Clarification of Notion of a ""Good Approximation""","Clarification of Notion of a ""Good Approximation""",,"My textbook says the following: $$\lim_{x \to x_0} \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} = 0$$ Thus, the tangent line $l$ through $(x_0, f(x_0))$ with slope $f'(x_0)$ is close to $f$ in the sense that the difference between $f(x)$ and $l(x) = f(x_0) + f'(x_0)(x - x_0)$ , the equation of the tangent line, goes to zero even when divided by $x - x_0$ as $x$ goes to $x_0$ . This is the notion of a ""good approximation that we will adapt to functions of several variables, with the tangent line replaced by the tangent plane. I'm finding it difficult to understand how this notion of ""good approximation"" makes logical sense, given logical reasoning and all of the other mathematics I've learned. For $\dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0}$ , as $x \to x_0$ , we have that the numerator and denominator are approaching $0$ at the same rate -- after all, there are no exponents to indicate that one is approaching $0$ quicker than the other. Given this, we would usually say that $\lim_{x \to x_0} \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} = \dfrac{0}{0}$ , which is an indeterminate form. In order to have that $\lim_{x \to x_0} \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} = 0$ , we would require that the numerator approach $0$ quicker than the denominator, which, as I said, there is no indication of. So can someone please clarify this notion of ""good approximation"" and explain why it makes mathematical sense? I'm also wondering if this is just a crude/""hand-wavey"" way of explaining the notion of ""good approximation"", since, as I said, it doesn't seem very sensible, and a more sensible and rigorous way would use epsilon-delta notions? If anyone has a better mathematical explanation of the notion of ""good approximation"" feel free to share. I would greatly appreciate it if people could please take the time to clarify this.","My textbook says the following: Thus, the tangent line through with slope is close to in the sense that the difference between and , the equation of the tangent line, goes to zero even when divided by as goes to . This is the notion of a ""good approximation that we will adapt to functions of several variables, with the tangent line replaced by the tangent plane. I'm finding it difficult to understand how this notion of ""good approximation"" makes logical sense, given logical reasoning and all of the other mathematics I've learned. For , as , we have that the numerator and denominator are approaching at the same rate -- after all, there are no exponents to indicate that one is approaching quicker than the other. Given this, we would usually say that , which is an indeterminate form. In order to have that , we would require that the numerator approach quicker than the denominator, which, as I said, there is no indication of. So can someone please clarify this notion of ""good approximation"" and explain why it makes mathematical sense? I'm also wondering if this is just a crude/""hand-wavey"" way of explaining the notion of ""good approximation"", since, as I said, it doesn't seem very sensible, and a more sensible and rigorous way would use epsilon-delta notions? If anyone has a better mathematical explanation of the notion of ""good approximation"" feel free to share. I would greatly appreciate it if people could please take the time to clarify this.","\lim_{x \to x_0} \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} = 0 l (x_0, f(x_0)) f'(x_0) f f(x) l(x) = f(x_0) + f'(x_0)(x - x_0) x - x_0 x x_0 \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} x \to x_0 0 0 \lim_{x \to x_0} \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} = \dfrac{0}{0} \lim_{x \to x_0} \dfrac{f(x) - f(x_0) - f'(x_0)(x - x_0)}{x - x_0} = 0 0","['calculus', 'real-analysis', 'limits', 'derivatives', 'linear-approximation']"
32,Does existence of limit $x\to 0$ of derivative imply function is differentiable at 0?,Does existence of limit  of derivative imply function is differentiable at 0?,x\to 0,"Suppose $f\in C(\mathbb R)\cap C^1(\mathbb R\setminus \{ 0\})$ and $\lim_{x\to 0}f'(x) $ exists and equals $L\in\mathbb R$ . Does this imply that $f\in C^1(\mathbb R)$ ? Derivatives cannot have jump singularities, since they satisfy the intermediate value property. Therefore, if $f'(0)$ exists, its value must be $L$ , and $f$ is then $C^1$ . But this doesn't prove the result. I also can't imagine a counterexample... Remarks: This seems simple enough that it should have been asked before, but I can't find a duplicate. Moreover, (see rem. 3) its not so unnatural to assume its true. This is kind of close. Most questions with similar names seem to be about multivariate calculus . Note the standard example of a function whose derivative is not continuous at 0 does not help ( $f(x) = x\sin(1/x)$ ) If true, this result would validate attempts to prove the differentiability of e.g. $e^{-1/|x|}$ by establishing the existence of the above limit instead.","Suppose and exists and equals . Does this imply that ? Derivatives cannot have jump singularities, since they satisfy the intermediate value property. Therefore, if exists, its value must be , and is then . But this doesn't prove the result. I also can't imagine a counterexample... Remarks: This seems simple enough that it should have been asked before, but I can't find a duplicate. Moreover, (see rem. 3) its not so unnatural to assume its true. This is kind of close. Most questions with similar names seem to be about multivariate calculus . Note the standard example of a function whose derivative is not continuous at 0 does not help ( ) If true, this result would validate attempts to prove the differentiability of e.g. by establishing the existence of the above limit instead.",f\in C(\mathbb R)\cap C^1(\mathbb R\setminus \{ 0\}) \lim_{x\to 0}f'(x)  L\in\mathbb R f\in C^1(\mathbb R) f'(0) L f C^1 f(x) = x\sin(1/x) e^{-1/|x|},"['real-analysis', 'limits', 'derivatives', 'continuity']"
33,Why does this method for differentiating work? [duplicate],Why does this method for differentiating work? [duplicate],,"This question already has answers here : Proof of this fairly obscure differentiation trick? (2 answers) Why does this differentation output correct result? [duplicate] (1 answer) Closed 5 years ago . Consider the function $$f(x)=x^x.$$ If I differentiate with respect to $x$ treating the exponent as a constant and then sum the derivative treating the base as a constant, I get \begin{align} f'(x)&= xx^{x-1}+x^x\ln x\\ f'(x)&= x^x(1+\ln x). \end{align}","This question already has answers here : Proof of this fairly obscure differentiation trick? (2 answers) Why does this differentation output correct result? [duplicate] (1 answer) Closed 5 years ago . Consider the function If I differentiate with respect to treating the exponent as a constant and then sum the derivative treating the base as a constant, I get","f(x)=x^x. x \begin{align}
f'(x)&= xx^{x-1}+x^x\ln x\\
f'(x)&= x^x(1+\ln x).
\end{align}","['calculus', 'derivatives', 'recreational-mathematics']"
34,Is it legal to multiply left side of equation by $\mathrm{d}y$ and right side by $\mathrm{d}x$?,Is it legal to multiply left side of equation by  and right side by ?,\mathrm{d}y \mathrm{d}x,"I have a question about calculus rules which I find difficult to find an answer to. Say I have an equation of $x$ and $y$ , e.g.: $y=x^2$ . Am I then allowed to multiply the left side by a small change in y and the right side by a small change in $x$ : $y\mathrm{d}y=x^2\mathrm{d}x$ ? If the equation is true, then it should be true for an infinitesimal change in $x$ and $y$ , but I am not sure about the Leibnitz notation and how we are allowed to use it in equations.","I have a question about calculus rules which I find difficult to find an answer to. Say I have an equation of and , e.g.: . Am I then allowed to multiply the left side by a small change in y and the right side by a small change in : ? If the equation is true, then it should be true for an infinitesimal change in and , but I am not sure about the Leibnitz notation and how we are allowed to use it in equations.",x y y=x^2 x y\mathrm{d}y=x^2\mathrm{d}x x y,['calculus']
35,Second derivative test using $f_{yy}$ instead of $f_{xx}$?,Second derivative test using  instead of ?,f_{yy} f_{xx},"The second derivative test for functions of two variables says to first find critical points. For each critical point one finds $$ D = f_{xx}f_{yy} - f_{xy}^2 $$ If $D>0$ , the sign of $f_{xx}$ says something about whether the point is a local maximum or local minimum. My question is: Why do we use $f_{xx}$ ? Could we use $f_{yy}$ instead?","The second derivative test for functions of two variables says to first find critical points. For each critical point one finds If , the sign of says something about whether the point is a local maximum or local minimum. My question is: Why do we use ? Could we use instead?","
D = f_{xx}f_{yy} - f_{xy}^2
 D>0 f_{xx} f_{xx} f_{yy}","['calculus', 'derivatives', 'maxima-minima']"
36,Find the derivative of an inverse function,Find the derivative of an inverse function,,"Let $f:\mathbb{R} \to \mathbb{R}$ be a function, $f(x)=\frac{x^3}{3}+\frac{x^2}{2}-6x+4$ . Let $I$ be the longest closed interval such that $0 \in I$ and $f$ is invertible. And let $g$ be the inverse function of $f$ in $I$ . Find $I$ and $g'(4)$ . What I've been doing: I found $f'(x)=x^2+x-6$ and I found the roots which are $-3$ and $2$ , and then I looked where the function decreases and increases. So $f$ is strictly decreasing in $I=[-3, 2]$ so $f$ must be bijective, and then invertible (right?), also $0 \in I$ . Now I have to find $g'(4)$ : I have that $g(x)=f^{-1}(x)$ so $(f^{-1})'(x)=\frac{1}{f'(f^{-1}(x))}$ and now my problem is: How do I find $f^{-1}(x)$ ?","Let be a function, . Let be the longest closed interval such that and is invertible. And let be the inverse function of in . Find and . What I've been doing: I found and I found the roots which are and , and then I looked where the function decreases and increases. So is strictly decreasing in so must be bijective, and then invertible (right?), also . Now I have to find : I have that so and now my problem is: How do I find ?","f:\mathbb{R} \to \mathbb{R} f(x)=\frac{x^3}{3}+\frac{x^2}{2}-6x+4 I 0 \in I f g f I I g'(4) f'(x)=x^2+x-6 -3 2 f I=[-3, 2] f 0 \in I g'(4) g(x)=f^{-1}(x) (f^{-1})'(x)=\frac{1}{f'(f^{-1}(x))} f^{-1}(x)","['calculus', 'real-analysis', 'derivatives', 'inverse-function']"
37,Find derivative of gas pressure formula,Find derivative of gas pressure formula,,"I did really well with derivatives until I hit this question. I'm not sure how to treat the different variables which are considered constants. I keep getting zero. Can anyone give me a pointer or just start it for me so I can see how to begin and take it from there? If gas in a cylinder is maintained at a constant temperature​ T, the pressure P is related to the volume V by a formula of the form $$P=\frac{nRT}{V-nb}-\frac{an^2}{V^2}$$ ,  in which​ a, b,​ n, and R are constants. Find $\frac{\partial P}{\partial V}$ .","I did really well with derivatives until I hit this question. I'm not sure how to treat the different variables which are considered constants. I keep getting zero. Can anyone give me a pointer or just start it for me so I can see how to begin and take it from there? If gas in a cylinder is maintained at a constant temperature​ T, the pressure P is related to the volume V by a formula of the form ,  in which​ a, b,​ n, and R are constants. Find .",P=\frac{nRT}{V-nb}-\frac{an^2}{V^2} \frac{\partial P}{\partial V},"['calculus', 'derivatives', 'physics']"
38,Why should one use the quotient rule instead of the power rule to differentiate a quotient?,Why should one use the quotient rule instead of the power rule to differentiate a quotient?,,"There's a lot of emphasis on the difference quotient, as it's on the AP test and all that, but honestly using the power rule plus the product rule is soo much easier and gives you the same answer and slope. What am I missing here? For example, if you wanted to differentiate  $$f(x)=\frac{x^2+4x-2}{x-1},$$ you could either do  $$f'(x)=\frac{d}{dx}[(x^2+4x-2)(x-1)^{-1}]=(2x+4)(x-1)^{-1}-(x^2+4x-2)(x-1)^{-2}, $$ or you could do $$ f'(x)=\frac{(x-1)(2x+4)-(x^2+4x-2)(1)}{(x-1)^2}. $$","There's a lot of emphasis on the difference quotient, as it's on the AP test and all that, but honestly using the power rule plus the product rule is soo much easier and gives you the same answer and slope. What am I missing here? For example, if you wanted to differentiate  $$f(x)=\frac{x^2+4x-2}{x-1},$$ you could either do  $$f'(x)=\frac{d}{dx}[(x^2+4x-2)(x-1)^{-1}]=(2x+4)(x-1)^{-1}-(x^2+4x-2)(x-1)^{-2}, $$ or you could do $$ f'(x)=\frac{(x-1)(2x+4)-(x^2+4x-2)(1)}{(x-1)^2}. $$",,"['calculus', 'derivatives']"
39,Obtaining equation of Tangent(s) to A Curve at origin by equating the lowest degree terms to zero. [closed],Obtaining equation of Tangent(s) to A Curve at origin by equating the lowest degree terms to zero. [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question In a rational algebraic expression in 2 variable having no constant term, equation of the tangent at origin can be formed by equating the lowest degree term to be zero. Ex. For $ x^4+y^4+2xy^2-2y=0 $ the equation of tangent at origin is $ 2y=0 $ or $ y=0 $ Is there an Analytical Method to prove this or is this pure observation?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question In a rational algebraic expression in 2 variable having no constant term, equation of the tangent at origin can be formed by equating the lowest degree term to be zero. Ex. For the equation of tangent at origin is or Is there an Analytical Method to prove this or is this pure observation?", x^4+y^4+2xy^2-2y=0   2y=0   y=0 ,"['calculus', 'derivatives', 'implicit-differentiation', 'tangent-line']"
40,On Differentiable Functions (Khan Academy),On Differentiable Functions (Khan Academy),,"I am learning Khan Academy Calculus, specifically how to tell when the graph of a function is differentiable or not. Khan Academy tells me that a point on the graph of is not differentiable when: But I don't get why 2. and 3. are true.  For 2, say we have a vertical tangent line for function 1/x^2 Would it not be continuous here? And therefore, wouldn't there be at least the chance that it's differentiable? Can someone please explain to me why a function is never differentiable at the vertical asymptote? For 3, why would a function not be differentiable when it has a sharp turn? I don't get this either. Can someone please explain?","I am learning Khan Academy Calculus, specifically how to tell when the graph of a function is differentiable or not. Khan Academy tells me that a point on the graph of is not differentiable when: But I don't get why 2. and 3. are true.  For 2, say we have a vertical tangent line for function 1/x^2 Would it not be continuous here? And therefore, wouldn't there be at least the chance that it's differentiable? Can someone please explain to me why a function is never differentiable at the vertical asymptote? For 3, why would a function not be differentiable when it has a sharp turn? I don't get this either. Can someone please explain?",,"['calculus', 'derivatives', 'continuity']"
41,Prove that a function is not differentiable with $\epsilon - \delta$,Prove that a function is not differentiable with,\epsilon - \delta,"We have this function $$f(x) = \begin{cases} x^2,  & \text{if $x$ $\in \mathbb{Q}$} \\[2ex] 0, & \text{if $x$ $\in\mathbb{I}$} \end{cases}$$ We have to determine where the function is differentiable and calculate the derivative. Then we need to determine where is not differentiable and prove this with $\epsilon - \delta$ Attempt: We define: $$\text{$h(x)=0$ and $g(x)=x²$, then $h(0)=0=g(0)=f(0)$}$$ Now we have: $$\text{$h(x)$ ≤ $f(x)$ ≤ $g(x)$, for all x$\in\mathbb{R}$}$$ $$\text{that is equal to: $h(x)-h(0)$ ≤$ f(x) - f(0) $≤$ g(x) - g(0)$}$$  $$\text{And without losing generality assuming that $x>0$:}$$ $$\text{$\frac{h(x)-h(0)}{x}$ ≤ $\frac{f(x)-f(0)}{x}$ ≤ $\frac{g(x)-g(0)}{x}$}$$ $$\text{Finally: $\lim_{x\to 0} \frac{h(x)-h(0)}{x}$ ≤ $\lim_{x\to 0} \frac{f(x)-f(0)}{x}$ ≤ $\lim_{x\to 0} \frac{g(x)-g(0)}{x}$}$$ $$\text{By the squeeze theorem we have that:}$$ $$\lim_{x\to 0} \frac{f(x)-f(0)}{x}=0$$ $$\text{So $f$ is differentiable at $c=0$ and $f'(0)=0$}$$ The problem comes when we have to show that is not differentiable at any other point. I managed to do it with limits but I don't know how to put in into $\epsilon - \delta$ $$\text{Let $x\in\mathbb{Q}$ and $x≠0$. Then $f(x)=x²$. Now we know that exist a sequence $(y_n)_{n\in\mathbb{N}}$ of irrational numbers such that:}$$ $$\lim_{n\to \infty}y_n = x$$ $$\text{Also we have that $f(y_n)=0$ for all $n$, because $y_n$ is irrational, but:}$$ $$\lim_{n\to \infty}f(y_n) = f(x) = x²$$ $$\text{So we can see that:}$$ $$\lim_{n\to \infty}f(y_n) ≠ f(y_n)$$ $$\text{This implies that $f$ is not continuous at $\mathbb{I}$, therefore $f$ is not differentiable at $\mathbb{I}$}$$ $$\text{The same thing works to prove that $f$ is not continuous at $\mathbb{Q}$ so $f$ is not differentiable at any point, except $x=0$.}$$ As I said the problem comes when I have to write that second thing with $\text{$\epsilon - \delta$ }$, cause I don't really know how to start. Someone has any ideas? Thanks to everyone.","We have this function $$f(x) = \begin{cases} x^2,  & \text{if $x$ $\in \mathbb{Q}$} \\[2ex] 0, & \text{if $x$ $\in\mathbb{I}$} \end{cases}$$ We have to determine where the function is differentiable and calculate the derivative. Then we need to determine where is not differentiable and prove this with $\epsilon - \delta$ Attempt: We define: $$\text{$h(x)=0$ and $g(x)=x²$, then $h(0)=0=g(0)=f(0)$}$$ Now we have: $$\text{$h(x)$ ≤ $f(x)$ ≤ $g(x)$, for all x$\in\mathbb{R}$}$$ $$\text{that is equal to: $h(x)-h(0)$ ≤$ f(x) - f(0) $≤$ g(x) - g(0)$}$$  $$\text{And without losing generality assuming that $x>0$:}$$ $$\text{$\frac{h(x)-h(0)}{x}$ ≤ $\frac{f(x)-f(0)}{x}$ ≤ $\frac{g(x)-g(0)}{x}$}$$ $$\text{Finally: $\lim_{x\to 0} \frac{h(x)-h(0)}{x}$ ≤ $\lim_{x\to 0} \frac{f(x)-f(0)}{x}$ ≤ $\lim_{x\to 0} \frac{g(x)-g(0)}{x}$}$$ $$\text{By the squeeze theorem we have that:}$$ $$\lim_{x\to 0} \frac{f(x)-f(0)}{x}=0$$ $$\text{So $f$ is differentiable at $c=0$ and $f'(0)=0$}$$ The problem comes when we have to show that is not differentiable at any other point. I managed to do it with limits but I don't know how to put in into $\epsilon - \delta$ $$\text{Let $x\in\mathbb{Q}$ and $x≠0$. Then $f(x)=x²$. Now we know that exist a sequence $(y_n)_{n\in\mathbb{N}}$ of irrational numbers such that:}$$ $$\lim_{n\to \infty}y_n = x$$ $$\text{Also we have that $f(y_n)=0$ for all $n$, because $y_n$ is irrational, but:}$$ $$\lim_{n\to \infty}f(y_n) = f(x) = x²$$ $$\text{So we can see that:}$$ $$\lim_{n\to \infty}f(y_n) ≠ f(y_n)$$ $$\text{This implies that $f$ is not continuous at $\mathbb{I}$, therefore $f$ is not differentiable at $\mathbb{I}$}$$ $$\text{The same thing works to prove that $f$ is not continuous at $\mathbb{Q}$ so $f$ is not differentiable at any point, except $x=0$.}$$ As I said the problem comes when I have to write that second thing with $\text{$\epsilon - \delta$ }$, cause I don't really know how to start. Someone has any ideas? Thanks to everyone.",,"['calculus', 'limits', 'derivatives', 'continuity', 'epsilon-delta']"
42,How can one define ordinary derivative of a vector field along a curve?,How can one define ordinary derivative of a vector field along a curve?,,"Definition 1: A curve $C$ on a manifold $M$ is a smooth function $C:(a,b) \rightarrow M$. Definition 2: A vector field $v^a=v^a(t)$ along a curve $C$ is an assignment of vectors on the tangent space at each point $C(t)$ on the curve. Definition 3: Let $v^b$ be a vector field on $M$. The derivative operator $\partial_a v^b$ is defined by taking partial derivative at each component of $v^b$, given that a fixed coordinate system is chosen. Definition 4: $ v^a$ is said to be parallelly transported along the curve $C$ if $ t^a \nabla_a v^b=0$. In Wald's General Relativity, the derivative operator $\nabla_a$ is defined on vector field on the manifold, but still the above is valid by defining similar thing for vector fields along a curve. Theorem: Suppose a coordinate system is fixed. The following are equivalent. (1) A vector $ v^a$ is parallelly transported along a curve $C$. (2) $t^a\partial_a v_b+t^a {\Gamma^c}_{ab} v_c=0$ [Tensor form] (3) $\displaystyle\frac{dv^\nu}{dt}+\sum_{\mu,\lambda=1}^n t^\mu {\Gamma^\lambda}_{\mu\nu}v_\lambda=0$ [Component form] This confuses me a lot. In particular, we are unable to define the derivative operator on vector field along a curve in the manner in definition 3. To make sense of taking partial derivatives at each component, formally speaking, it is defined as partial derivatives of the composition of the components and the parametrization of the manifold (which makes sense because the components themselves are formally functions from $M$ to $\mathbb{R}$). However, the curve may have self-intersection and two different tangent vectors at the same point might be defined for a vector field along a curve, which means the components now cannot be a function from $M$ to $\mathbb{R}$. So we cannot make composition with parametrizations now in order to allow us to define partial derivatives. Therefore, I am unable to understand the meaning of statement (2) in the above theorem. Can anyone give me some clue?","Definition 1: A curve $C$ on a manifold $M$ is a smooth function $C:(a,b) \rightarrow M$. Definition 2: A vector field $v^a=v^a(t)$ along a curve $C$ is an assignment of vectors on the tangent space at each point $C(t)$ on the curve. Definition 3: Let $v^b$ be a vector field on $M$. The derivative operator $\partial_a v^b$ is defined by taking partial derivative at each component of $v^b$, given that a fixed coordinate system is chosen. Definition 4: $ v^a$ is said to be parallelly transported along the curve $C$ if $ t^a \nabla_a v^b=0$. In Wald's General Relativity, the derivative operator $\nabla_a$ is defined on vector field on the manifold, but still the above is valid by defining similar thing for vector fields along a curve. Theorem: Suppose a coordinate system is fixed. The following are equivalent. (1) A vector $ v^a$ is parallelly transported along a curve $C$. (2) $t^a\partial_a v_b+t^a {\Gamma^c}_{ab} v_c=0$ [Tensor form] (3) $\displaystyle\frac{dv^\nu}{dt}+\sum_{\mu,\lambda=1}^n t^\mu {\Gamma^\lambda}_{\mu\nu}v_\lambda=0$ [Component form] This confuses me a lot. In particular, we are unable to define the derivative operator on vector field along a curve in the manner in definition 3. To make sense of taking partial derivatives at each component, formally speaking, it is defined as partial derivatives of the composition of the components and the parametrization of the manifold (which makes sense because the components themselves are formally functions from $M$ to $\mathbb{R}$). However, the curve may have self-intersection and two different tangent vectors at the same point might be defined for a vector field along a curve, which means the components now cannot be a function from $M$ to $\mathbb{R}$. So we cannot make composition with parametrizations now in order to allow us to define partial derivatives. Therefore, I am unable to understand the meaning of statement (2) in the above theorem. Can anyone give me some clue?",,"['general-relativity', 'differential-geometry', 'derivatives', 'vector-fields']"
43,Derivative of $d^2x/dy^2$,Derivative of,d^2x/dy^2,Using the quotient rule and the chain rule you get $$ \frac{d}{dy}\left(\frac{dx}{dy}\right)=\frac{d}{dy}\left(\frac{1}{\frac{dy}{dx}}\right) =\frac{d}{dx}\left(\frac{1}{\frac{dy}{dx}}\right)\times \frac{dx}{dy}\\$$ I don't understand how this came. First is dy/dx always gonna be equal to dx/dy?  Wouldn't the function have to be inverse to each other. And how to do chain rule in cases like this... I didn't get how the differentiation is done here. Can you explain with a simple example how chain differentiation like these are done. I am not too good at differentiation,Using the quotient rule and the chain rule you get $$ \frac{d}{dy}\left(\frac{dx}{dy}\right)=\frac{d}{dy}\left(\frac{1}{\frac{dy}{dx}}\right) =\frac{d}{dx}\left(\frac{1}{\frac{dy}{dx}}\right)\times \frac{dx}{dy}\\$$ I don't understand how this came. First is dy/dx always gonna be equal to dx/dy?  Wouldn't the function have to be inverse to each other. And how to do chain rule in cases like this... I didn't get how the differentiation is done here. Can you explain with a simple example how chain differentiation like these are done. I am not too good at differentiation,,['derivatives']
44,"$|f'(x)|\le k|f(x)|$ for some $k$ and $f(a) = 0$ imply that $f(x)$ is zero on $[a,b]$.",for some  and  imply that  is zero on .,"|f'(x)|\le k|f(x)| k f(a) = 0 f(x) [a,b]","Let $f: [a,b] \to \mathbb R$ be continuous on $[a,b]$ and differentiable on $(a,b)$. If $f(a) = 0$ and $|f'(x)|\le k|f(x)|$ for some $k$, then $f(x)$ is zero on $[a,b]$. I tried proving it using Legrange's Mean Value Theorem but couldn't get it. $f(x)$ is differentiable on $(a,b)$. $f(x)$ is continuous on $[a,b]$. Let $x$ belong to $[a,b]$ s.t $a < x$. Consider the interval $[a,x]$. By Legrange's Mean Value Theorem, $$\frac{f(x)-f(a)}{x-a} = f'(t) ; \ \ a \le t \le x.$$ Since $f(a)=0$, $$f(x)=(x-a)f'(t)$$ $$|f(x)| \le (x-a)k|f(t)|.$$ After this, I was thinking of proceeding with the inequality by putting $f(a)$ in the place of $f(t)$.","Let $f: [a,b] \to \mathbb R$ be continuous on $[a,b]$ and differentiable on $(a,b)$. If $f(a) = 0$ and $|f'(x)|\le k|f(x)|$ for some $k$, then $f(x)$ is zero on $[a,b]$. I tried proving it using Legrange's Mean Value Theorem but couldn't get it. $f(x)$ is differentiable on $(a,b)$. $f(x)$ is continuous on $[a,b]$. Let $x$ belong to $[a,b]$ s.t $a < x$. Consider the interval $[a,x]$. By Legrange's Mean Value Theorem, $$\frac{f(x)-f(a)}{x-a} = f'(t) ; \ \ a \le t \le x.$$ Since $f(a)=0$, $$f(x)=(x-a)f'(t)$$ $$|f(x)| \le (x-a)k|f(t)|.$$ After this, I was thinking of proceeding with the inequality by putting $f(a)$ in the place of $f(t)$.",,"['calculus', 'derivatives']"
45,Reversing sign of third derivative,Reversing sign of third derivative,,I don't understand some part of the solution given to this question: I can understand how the sign of $f''(x)$ can be reversed (i.e. flipping over the x-axis) but I don't get why changing $x$ to $-x$ would change the sign of $f'''(x)$?,I don't understand some part of the solution given to this question: I can understand how the sign of $f''(x)$ can be reversed (i.e. flipping over the x-axis) but I don't get why changing $x$ to $-x$ would change the sign of $f'''(x)$?,,"['calculus', 'derivatives']"
46,Why is the derivative of these functions a secant line?,Why is the derivative of these functions a secant line?,,"I have been trying to understand the relationship between derivatives of functions, their tangents, and secant lines. I came across this thread , but it has not really helped me understand why certain functions, when derived, give a secant line. For example, if I have a function $$f(x) = x^3 + 2x^2 + 3$$  and its derivative, $$f'(x) = 3x^2 + 4x$$ when I graph them, the derivative is a second-degree function, so it will be a parabola and intersect the original function twice. From my understanding, this is a secant line, since it intersects the function two times, whereas a tangent only intersects once at a certain point. Since this is the derivative, how does it give me a secant line? It may be false that the tangent line can only intersect at one point, but that still does not make sense to me. If that is true, how would you know the difference between a tangent line and the secant line? In my case here, how would I find the tangent line? it seems to me that functions with a degree greater than two will have this complication as well. $$f(x) = x^n, n > 2$$ So, if someone would help me understand that relationship between these three concepts and why the tangent line may, in most cases, intersect the function more than once it would help me very much. Thank you.","I have been trying to understand the relationship between derivatives of functions, their tangents, and secant lines. I came across this thread , but it has not really helped me understand why certain functions, when derived, give a secant line. For example, if I have a function $$f(x) = x^3 + 2x^2 + 3$$  and its derivative, $$f'(x) = 3x^2 + 4x$$ when I graph them, the derivative is a second-degree function, so it will be a parabola and intersect the original function twice. From my understanding, this is a secant line, since it intersects the function two times, whereas a tangent only intersects once at a certain point. Since this is the derivative, how does it give me a secant line? It may be false that the tangent line can only intersect at one point, but that still does not make sense to me. If that is true, how would you know the difference between a tangent line and the secant line? In my case here, how would I find the tangent line? it seems to me that functions with a degree greater than two will have this complication as well. $$f(x) = x^n, n > 2$$ So, if someone would help me understand that relationship between these three concepts and why the tangent line may, in most cases, intersect the function more than once it would help me very much. Thank you.",,"['calculus', 'derivatives']"
47,Parametrically Defined Curves: $f'$ and $g'$ Are Not Simultaneously Zero,Parametrically Defined Curves:  and  Are Not Simultaneously Zero,f' g',"I can't find a clear, comprehensive explanation, on this site or elsewhere, for why parametrically defined curves frequently have the condition that the the derivatives of their points $x = f(t)$ and $y = g(t)$ cannot simultaneously be zero on the interval $[a, b]$. Most of the explanations use language that assumes that the reader already understands the concept they're explaining, or the explanations make the meaningless claim that the curve must be ""nice"". I would appreciate it if people could please take the time to explain, comprehensively (not rigorously), what is meant by this condition. If you're going to use words that are likely to be unfamiliar to someone who doesn't understand this concept, like ""regular"", then please take the time to define what it means.","I can't find a clear, comprehensive explanation, on this site or elsewhere, for why parametrically defined curves frequently have the condition that the the derivatives of their points $x = f(t)$ and $y = g(t)$ cannot simultaneously be zero on the interval $[a, b]$. Most of the explanations use language that assumes that the reader already understands the concept they're explaining, or the explanations make the meaningless claim that the curve must be ""nice"". I would appreciate it if people could please take the time to explain, comprehensively (not rigorously), what is meant by this condition. If you're going to use words that are likely to be unfamiliar to someone who doesn't understand this concept, like ""regular"", then please take the time to define what it means.",,"['real-analysis', 'geometry', 'derivatives', 'curves', 'parametrization']"
48,Find the derivative of a function that contains a sum,Find the derivative of a function that contains a sum,,"If $$f(x)=\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{n\cdot3^n}{(x-3)}^n}$$ find $f''(-2)$. I know that, by ratio test, the previous sum converges iff $x\in(0,6)$. I did: $$\begin{matrix} f'(x)&=&\left(\displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{n\cdot3^n}{(x-3)}^n}\right)'&=& \displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{n\cdot3^n}n{(x-3)}^{n-1}}&=& \displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{3^n}{(x-3)}^{n-1}} \\ f''(x)&=&\left(\displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{3^n}{(x-3)}^{n-1}}\right)'&=& \displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{3^n}(n-1){(x-3)}^{n-2}}, \end{matrix}$$ so I did the radio test on the last sum: $$\begin{matrix} &&\displaystyle\lim_{n\to\infty}{\left|\dfrac{a_{n+1}}{a_n}\right|} \\ &=&\displaystyle\lim_{n\to\infty}{\left|\dfrac{{(-1)}^{n+2}n{(x-3)}^{n-1}}{3^{n+1}}\dfrac{3^n}{{(-1)}^{n+1}(n-1){(x-3)}^{n-2}}\right|}\\ &=&\dfrac{|x-3|}{3}\underbrace{\displaystyle\lim_{n\to\infty}{\left|\dfrac{n}{n-1}\right|}}_{=\:1}\\ &\Rightarrow&|x-3|<3\\ &\Rightarrow&x\in(0,6),& \end{matrix}$$ and since $-2\not\in(0,6)$ we cannot find $f''(-2)$. First question: is my reasoning right? If yes, to my amazement the radius of convergence of both functions are the same. Out of curiosity, why is this happening? The convergence can be generalize to $n$-th derivative? Thank you!","If $$f(x)=\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{n\cdot3^n}{(x-3)}^n}$$ find $f''(-2)$. I know that, by ratio test, the previous sum converges iff $x\in(0,6)$. I did: $$\begin{matrix} f'(x)&=&\left(\displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{n\cdot3^n}{(x-3)}^n}\right)'&=& \displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{n\cdot3^n}n{(x-3)}^{n-1}}&=& \displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{3^n}{(x-3)}^{n-1}} \\ f''(x)&=&\left(\displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{3^n}{(x-3)}^{n-1}}\right)'&=& \displaystyle\sum_{n=1}^\infty{\frac{{(-1)}^{n+1}}{3^n}(n-1){(x-3)}^{n-2}}, \end{matrix}$$ so I did the radio test on the last sum: $$\begin{matrix} &&\displaystyle\lim_{n\to\infty}{\left|\dfrac{a_{n+1}}{a_n}\right|} \\ &=&\displaystyle\lim_{n\to\infty}{\left|\dfrac{{(-1)}^{n+2}n{(x-3)}^{n-1}}{3^{n+1}}\dfrac{3^n}{{(-1)}^{n+1}(n-1){(x-3)}^{n-2}}\right|}\\ &=&\dfrac{|x-3|}{3}\underbrace{\displaystyle\lim_{n\to\infty}{\left|\dfrac{n}{n-1}\right|}}_{=\:1}\\ &\Rightarrow&|x-3|<3\\ &\Rightarrow&x\in(0,6),& \end{matrix}$$ and since $-2\not\in(0,6)$ we cannot find $f''(-2)$. First question: is my reasoning right? If yes, to my amazement the radius of convergence of both functions are the same. Out of curiosity, why is this happening? The convergence can be generalize to $n$-th derivative? Thank you!",,"['derivatives', 'convergence-divergence', 'summation', 'power-series']"
49,How do I express the second derivative of this function,How do I express the second derivative of this function,,We have  $$F(X)=f(x)\psi(x)$$ $$X=\chi(x)$$ I can show that  $$\frac{dF}{dX}=\frac{1}{\chi^\prime}(\psi f^\prime+f \psi^\prime)$$ But don't know how to express $$\frac{d^2F}{dX^2}=\text{?}$$,We have  $$F(X)=f(x)\psi(x)$$ $$X=\chi(x)$$ I can show that  $$\frac{dF}{dX}=\frac{1}{\chi^\prime}(\psi f^\prime+f \psi^\prime)$$ But don't know how to express $$\frac{d^2F}{dX^2}=\text{?}$$,,"['calculus', 'derivatives', 'chain-rule']"
50,Problem defining a function via Step function and Dirac's Delta,Problem defining a function via Step function and Dirac's Delta,,"First: I know Dirac's Delta isn't a function and hence shouldn't be treated like one. But this arose in a physics textbook so I'm looking for an answer that oversees that. Consider the following function: $$f(x) = \begin{cases} \cos x , \quad \textrm{if} \quad  -\pi/2 < x < \pi/2 \\ 0, \quad \textrm{otherwise} \end{cases}$$ And I want $f''(x)$. So I can just differentiate two times directly, to obtain: $$f(x) = \begin{cases} -\cos x , \quad \textrm{if} \quad  -\pi/2 < x < \pi/2 \\ 0, \quad \textrm{otherwise} \end{cases}$$ However, if I decide to write: $$f(x) = H(-\pi/2)\cos x - H(\pi/2)\cos x$$ ($H(x)$ stands for Heaviside's step function).  I can't differentiate twice. Moreover, if I decide to write: $$f'(x) = - H(-\pi/2)\sin x + H(\pi/2)\sin x$$ I'll obtain: $$f''(x) = -\delta(-\pi/2)\cos x + \delta(\pi/2)\cos x$$  However $$\int f''(x) dx = - \cos(-\pi/2) + \cos (\pi/2) = 0$$ Which also doesn't make sense. Now, for another problem (where the discotinuity at the first derivative occured in a point where $f(x) \neq 0$) the author used Dirac's Delta and Heaviside to obtain the derivative. But here he just claims that since $f(x) = f''(x)  = 0$ at the end points, we shouldn't worry about the deltas and just differentiate directly (as I did) to obtain the correct expression. My question comes from the fact that I can seemingly define the function and it's first derivative in the three ways I did, and the three ways give me different expressions for the second derivative. Why?","First: I know Dirac's Delta isn't a function and hence shouldn't be treated like one. But this arose in a physics textbook so I'm looking for an answer that oversees that. Consider the following function: $$f(x) = \begin{cases} \cos x , \quad \textrm{if} \quad  -\pi/2 < x < \pi/2 \\ 0, \quad \textrm{otherwise} \end{cases}$$ And I want $f''(x)$. So I can just differentiate two times directly, to obtain: $$f(x) = \begin{cases} -\cos x , \quad \textrm{if} \quad  -\pi/2 < x < \pi/2 \\ 0, \quad \textrm{otherwise} \end{cases}$$ However, if I decide to write: $$f(x) = H(-\pi/2)\cos x - H(\pi/2)\cos x$$ ($H(x)$ stands for Heaviside's step function).  I can't differentiate twice. Moreover, if I decide to write: $$f'(x) = - H(-\pi/2)\sin x + H(\pi/2)\sin x$$ I'll obtain: $$f''(x) = -\delta(-\pi/2)\cos x + \delta(\pi/2)\cos x$$  However $$\int f''(x) dx = - \cos(-\pi/2) + \cos (\pi/2) = 0$$ Which also doesn't make sense. Now, for another problem (where the discotinuity at the first derivative occured in a point where $f(x) \neq 0$) the author used Dirac's Delta and Heaviside to obtain the derivative. But here he just claims that since $f(x) = f''(x)  = 0$ at the end points, we shouldn't worry about the deltas and just differentiate directly (as I did) to obtain the correct expression. My question comes from the fact that I can seemingly define the function and it's first derivative in the three ways I did, and the three ways give me different expressions for the second derivative. Why?",,"['calculus', 'derivatives', 'dirac-delta']"
51,"Can I prove that $f(x+1) - f(x)$ is a monotonic increasing function, given $f'(x) > x$ for every $x>0$?","Can I prove that  is a monotonic increasing function, given  for every ?",f(x+1) - f(x) f'(x) > x x>0,"Given: $f$ has a derivative for every $x \in (0,\infty)$ $f'(x) > x$ for every $x>0$ Can I prove that $f(x+1) - f(x)$ is a monotonic increasing function? From Lagrange I know that in every $I = [x,x+1]$ where $x>0$, $f(x+1) - f(x) > x$. $f$ has derivative so $f'(x+1) - f'(x)$ is defined for $x>0$. But, How can I show that $f'(x+1) - f'(x) > 0$? Thanks! Edit: Thank you al for your answers! If I want to prove a weaker statement, that there exists some $M \in \mathbb R$ such that $f'(x+1) - f'(x)$ is increasing in $(M,\infty)$. Is it, then, can be proved? Thanks again!","Given: $f$ has a derivative for every $x \in (0,\infty)$ $f'(x) > x$ for every $x>0$ Can I prove that $f(x+1) - f(x)$ is a monotonic increasing function? From Lagrange I know that in every $I = [x,x+1]$ where $x>0$, $f(x+1) - f(x) > x$. $f$ has derivative so $f'(x+1) - f'(x)$ is defined for $x>0$. But, How can I show that $f'(x+1) - f'(x) > 0$? Thanks! Edit: Thank you al for your answers! If I want to prove a weaker statement, that there exists some $M \in \mathbb R$ such that $f'(x+1) - f'(x)$ is increasing in $(M,\infty)$. Is it, then, can be proved? Thanks again!",,"['calculus', 'derivatives']"
52,What is the value of $\frac{d\alpha}{dt} $?,What is the value of ?,\frac{d\alpha}{dt} ,"The following question was asked in an exam:  Given $\alpha $=$\int_{0}^{\infty}{\frac {dt}{1+t^2}} $. Then is $\frac {d\alpha}{dt} $=$\frac{1}{1+t^2}$? I know that the value $\alpha$=$\frac {\pi}{2} $. So as $\alpha $ is a constant, it's derivative is 0. Am I correct?","The following question was asked in an exam:  Given $\alpha $=$\int_{0}^{\infty}{\frac {dt}{1+t^2}} $. Then is $\frac {d\alpha}{dt} $=$\frac{1}{1+t^2}$? I know that the value $\alpha$=$\frac {\pi}{2} $. So as $\alpha $ is a constant, it's derivative is 0. Am I correct?",,"['real-analysis', 'derivatives', 'improper-integrals']"
53,What does it mean to differentiate entropy and information contents?,What does it mean to differentiate entropy and information contents?,,"I'm taking an information theory class and encountered some concepts regarding differentiation that I wasn't sure how to understand. In class, my instructor gave many examples and solutions done by ""finding the derivative"" for certain probability distribution functions, or finding the derivatives for other concepts like information entropy or mutual information. But in general, what does it exactly mean to differentiate a probability, and what is the motivation for doing so? $\space$ Edit Here's a specific example to what I'm referring to. It's an exercise problem from the textbook Information Theory, Inference, and Learning Algorithms - David Mackay . $\space$ Exercise Problem Consider the $Z$ channel with $f = 0.15$ . Identifying the optimal input distribution is not so straightforward. We evaluate $I(X;Y)$ explicitly for $P_X = \{p_0, p_1\}$ . First, we need to compute $P(y)$ . The probability of $y = 1$ is easiest to write down: $$P(y = 1) = p_1(1 - f)$$ Then the mutual information is: $$I(X; Y) = H(Y) - H(Y|X)$$ $$ = H_2(p_1(1 - f)) - (p_0H_2(0) + p_1H_2(f))$$ $$ = H_2(p_1(1 - f)) - p_1H_2(f))$$ This is a non-trivial function of $p_1$ . It is maximized for $f = 0.15$ by $p_1^* = 0.445$ . We find capacity $C(Q_Z) = 0.685$ . Now, in the case of general $f$ , show that the optimal input distribution is $$p_1^* = \frac{1/(1-f)}{1 + 2^{(H_2(f)/(1 - f))}}$$ $\space$ Solution In the exercise, we showed that the mutual information between input and output of the $Z$ channel is $$I(X;Y) = H(Y) - H(Y|X)$$ $$ = H_2(p_1(1 - f)) - p_1H_2(f)$$ We differentiate this expression with respect to $p_1$ , taking care not to confuse $\log_2$ with $\log_e$ : $$\frac{d}{dp_1}I(X;Y) = (1 - f)\log_2\frac{1 - p_1(1 - f)}{p_1(1 - f)} - H_2(f)$$ Setting this derivative to zero and rearranging it, we obtain: $$p_1^*(1 - f) = \frac{1}{1 + 2^{H_2(f)/(1 - f)}}$$ so the optimal input distribution is $$p_1^* = \frac{1/(1-f)}{1 + 2^{(H_2(f)/(1 - f))}}$$ $\space$ So that's the particular question and solution that motivated me to ask this question. I just wasn't sure how the solution came to the idea of differentiating the mutual information, and if there was a more ""general"" motivation for differentiating probability functions what they might be. Thank you.","I'm taking an information theory class and encountered some concepts regarding differentiation that I wasn't sure how to understand. In class, my instructor gave many examples and solutions done by ""finding the derivative"" for certain probability distribution functions, or finding the derivatives for other concepts like information entropy or mutual information. But in general, what does it exactly mean to differentiate a probability, and what is the motivation for doing so? Edit Here's a specific example to what I'm referring to. It's an exercise problem from the textbook Information Theory, Inference, and Learning Algorithms - David Mackay . Exercise Problem Consider the channel with . Identifying the optimal input distribution is not so straightforward. We evaluate explicitly for . First, we need to compute . The probability of is easiest to write down: Then the mutual information is: This is a non-trivial function of . It is maximized for by . We find capacity . Now, in the case of general , show that the optimal input distribution is Solution In the exercise, we showed that the mutual information between input and output of the channel is We differentiate this expression with respect to , taking care not to confuse with : Setting this derivative to zero and rearranging it, we obtain: so the optimal input distribution is So that's the particular question and solution that motivated me to ask this question. I just wasn't sure how the solution came to the idea of differentiating the mutual information, and if there was a more ""general"" motivation for differentiating probability functions what they might be. Thank you.","\space \space Z f = 0.15 I(X;Y) P_X = \{p_0, p_1\} P(y) y = 1 P(y = 1) = p_1(1 - f) I(X; Y) = H(Y) - H(Y|X)  = H_2(p_1(1 - f)) - (p_0H_2(0) + p_1H_2(f))  = H_2(p_1(1 - f)) - p_1H_2(f)) p_1 f = 0.15 p_1^* = 0.445 C(Q_Z) = 0.685 f p_1^* = \frac{1/(1-f)}{1 + 2^{(H_2(f)/(1 - f))}} \space Z I(X;Y) = H(Y) - H(Y|X)  = H_2(p_1(1 - f)) - p_1H_2(f) p_1 \log_2 \log_e \frac{d}{dp_1}I(X;Y) = (1 - f)\log_2\frac{1 - p_1(1 - f)}{p_1(1 - f)} - H_2(f) p_1^*(1 - f) = \frac{1}{1 + 2^{H_2(f)/(1 - f)}} p_1^* = \frac{1/(1-f)}{1 + 2^{(H_2(f)/(1 - f))}} \space","['probability', 'derivatives', 'information-theory']"
54,"For $0 < x < \pi/2 $, prove that $x + \frac{x^{3}}{3} <\tan x $","For , prove that",0 < x < \pi/2  x + \frac{x^{3}}{3} <\tan x ,"I proved as follows: Let $f(x) = \tan x -  x - \frac{x^{3}}{3}.$ Then $f '(x) = \sec^{2}x - 1 - x^{2}$ and $f''(x) = 2\sec^{2}x\tan x-x > \tan x - x,$ since $\\tan x > x,$ $f''(x) > 0$ so $f'(x)$ is increasing. Since $f '(0) = 0,$ $f '(x) > 0$ for all $x \in (0, \frac{\pi}{2}).$ So $f(x)$ is increasing. since $f(0) = 0,$ $f(x) > 0$ for all $x \in (0, \frac{\pi}{2}).$ But my proof seems complex. Is there any simpler way to prove this?",I proved as follows: Let Then and since so is increasing. Since for all So is increasing. since for all But my proof seems complex. Is there any simpler way to prove this?,"f(x) = \tan x -  x - \frac{x^{3}}{3}. f '(x) = \sec^{2}x - 1 - x^{2} f''(x) = 2\sec^{2}x\tan x-x > \tan x - x, \\tan x > x, f''(x) > 0 f'(x) f '(0) = 0, f '(x) > 0 x \in (0, \frac{\pi}{2}). f(x) f(0) = 0, f(x) > 0 x \in (0, \frac{\pi}{2}).","['derivatives', 'inequality']"
55,Rademacher theorem for 2nd order derivative,Rademacher theorem for 2nd order derivative,,"The (simplest form of the) Rademacher theorem reads as follows: Any Lipschitz continuous function $f: \mathbb{R} \to \mathbb{R}$ is Lebesgue-almost everywhere differentiable. In other words: If the finite difference $\Delta_h^1[f](x) := f(x+h)-f(x)$ satisfies $$|\Delta_h^1[f](x)| \leq C |h|, \qquad x,h \in \mathbb{R} \tag{1}$$ for some absolute constant $C>0$, then $f$ is almost everywhere differentiable. Question: Is there a generalization for derivatives of second order? More precisely, if we replace the finite difference $\Delta_h^1$ by a second-order difference, for instance $$\Delta_h^2[f](x) := f(x+h)-2f(x)+f(x-h), \tag{2}$$ then does $$|\Delta_h^1[f](x)| \leq C_1|h| \qquad \quad |\Delta_h^2[f](x)| \leq C_2 |h|^2, \qquad x,h \in \mathbb{R} \tag{3}$$ imply that $f$ is almost everywhere twice differentiable? If not, then what additional information on the regularity gives the estimate $(3)$ compared to the almost everyhwere differentiability which follows from the weaker assumption $(1)$? The obvious idea would be to try to apply the Rademacher theorem twice (first to $f$ and then to its derivative $f'$), but unfortunately the estimate $$|\Delta_h^1[f'](x)| \leq C |h|$$ will, in general, only hold for Lebesgue almost every $x,h$, and therefore it is not possible to apply the Rademacher theorem directly to $f'$.","The (simplest form of the) Rademacher theorem reads as follows: Any Lipschitz continuous function $f: \mathbb{R} \to \mathbb{R}$ is Lebesgue-almost everywhere differentiable. In other words: If the finite difference $\Delta_h^1[f](x) := f(x+h)-f(x)$ satisfies $$|\Delta_h^1[f](x)| \leq C |h|, \qquad x,h \in \mathbb{R} \tag{1}$$ for some absolute constant $C>0$, then $f$ is almost everywhere differentiable. Question: Is there a generalization for derivatives of second order? More precisely, if we replace the finite difference $\Delta_h^1$ by a second-order difference, for instance $$\Delta_h^2[f](x) := f(x+h)-2f(x)+f(x-h), \tag{2}$$ then does $$|\Delta_h^1[f](x)| \leq C_1|h| \qquad \quad |\Delta_h^2[f](x)| \leq C_2 |h|^2, \qquad x,h \in \mathbb{R} \tag{3}$$ imply that $f$ is almost everywhere twice differentiable? If not, then what additional information on the regularity gives the estimate $(3)$ compared to the almost everyhwere differentiability which follows from the weaker assumption $(1)$? The obvious idea would be to try to apply the Rademacher theorem twice (first to $f$ and then to its derivative $f'$), but unfortunately the estimate $$|\Delta_h^1[f'](x)| \leq C |h|$$ will, in general, only hold for Lebesgue almost every $x,h$, and therefore it is not possible to apply the Rademacher theorem directly to $f'$.",,"['real-analysis', 'derivatives', 'lipschitz-functions']"
56,Find point in which 2 functions are tangent,Find point in which 2 functions are tangent,,Let $f(x)=x^6$ and $g(x)=2x^5-2x-1$. Find x such that these to functions are tangent. My attempt: $f(x)=g(x)\implies x^6-2x^5+2x+1=0$ and I know it's a polynomial and I could approximate it's roots but I have to find the exact $x$ such that they are tangent. Or I thought their tangents to the graphic in that point must be equal so $$f'(x)=g'(x)\implies 3x^5-5x^4+1=0.$$ and yet again another polynomial which does not have any nice roots. Are there any other ways or am I just supposed to guess the solutions? Or approximate the solutions and the answers?,Let $f(x)=x^6$ and $g(x)=2x^5-2x-1$. Find x such that these to functions are tangent. My attempt: $f(x)=g(x)\implies x^6-2x^5+2x+1=0$ and I know it's a polynomial and I could approximate it's roots but I have to find the exact $x$ such that they are tangent. Or I thought their tangents to the graphic in that point must be equal so $$f'(x)=g'(x)\implies 3x^5-5x^4+1=0.$$ and yet again another polynomial which does not have any nice roots. Are there any other ways or am I just supposed to guess the solutions? Or approximate the solutions and the answers?,,"['calculus', 'real-analysis', 'derivatives', 'tangent-line']"
57,Reciprocal of a convex decreasing function:,Reciprocal of a convex decreasing function:,,"I have the following math problem that I haven't been able to solve. I would be very grateful of any idea or solution you can give me. Thanks! Let $f(x): [0,\infty)\rightarrow (0,\infty)$ such that $f'(x)<0$ and  $f''(x)>0$. What are the necessary and sufficient conditions for $1/f(x)$ to be concave. I approach to this problem by using derivatives and got to the equation: $$2(f'(x))^2 -f''(x)f(x)\leq0$$ Yet I feel something smarter can be done that gives a more precise characterization. Thanks!","I have the following math problem that I haven't been able to solve. I would be very grateful of any idea or solution you can give me. Thanks! Let $f(x): [0,\infty)\rightarrow (0,\infty)$ such that $f'(x)<0$ and  $f''(x)>0$. What are the necessary and sufficient conditions for $1/f(x)$ to be concave. I approach to this problem by using derivatives and got to the equation: $$2(f'(x))^2 -f''(x)f(x)\leq0$$ Yet I feel something smarter can be done that gives a more precise characterization. Thanks!",,"['derivatives', 'convex-analysis']"
58,Derivatives for non Real/Complex functions,Derivatives for non Real/Complex functions,,"I recently started on my journey into Abstract Algebra and am interested in learning about derivatives in a generalised sense. By that I mean all of my current understanding (or the overwhelming majority) about derivatives is in terms of Real and Complex Valued functions. I'm curious about what restrictions must exists so that we can define a functional limits on a given ordered Field $F$. It would seem that it must be ordered and complete, but how does continuity/limits come into play for functions of the following form $f:F \longrightarrow F$ I was wondering if anyone knew of any good papers/textbooks/etc that speak to this. Thanks in Advance, David","I recently started on my journey into Abstract Algebra and am interested in learning about derivatives in a generalised sense. By that I mean all of my current understanding (or the overwhelming majority) about derivatives is in terms of Real and Complex Valued functions. I'm curious about what restrictions must exists so that we can define a functional limits on a given ordered Field $F$. It would seem that it must be ordered and complete, but how does continuity/limits come into play for functions of the following form $f:F \longrightarrow F$ I was wondering if anyone knew of any good papers/textbooks/etc that speak to this. Thanks in Advance, David",,['abstract-algebra']
59,Derivative transpose (follow up),Derivative transpose (follow up),,"I'm working through the Elements of Statistical learning, and I have a quick followup to the below question: derivative transpose In the answer accepted, it states the below:  $$ \frac{\partial}{\partial\beta}(\beta^T X^TX\beta) = 2X^TX\beta $$ For context, $X$ is a $n\times p$ matrix, and $\beta$ is a $p \times 1$ matrix. I'm a bit confused about that. When I do the product rule, I see the below: $$ \frac{\partial}{\partial\beta}(\beta^T X^TX\beta) = (\beta^TX^T)'(X\beta)+(\beta^TX^T)(X\beta)' = X^TX\beta+ \beta^TX^TX $$ My issue now is that $X^TX\beta$ is a $p\times 1$ matrix, and $\beta^TX^TX$ is a $1\times p$ matrix. I mean, I guess it doesn't matter since they're both vectors and each other's transposes, but this seems shaky from a dimensionality perspective. Am I missing something? Thanks!","I'm working through the Elements of Statistical learning, and I have a quick followup to the below question: derivative transpose In the answer accepted, it states the below:  $$ \frac{\partial}{\partial\beta}(\beta^T X^TX\beta) = 2X^TX\beta $$ For context, $X$ is a $n\times p$ matrix, and $\beta$ is a $p \times 1$ matrix. I'm a bit confused about that. When I do the product rule, I see the below: $$ \frac{\partial}{\partial\beta}(\beta^T X^TX\beta) = (\beta^TX^T)'(X\beta)+(\beta^TX^T)(X\beta)' = X^TX\beta+ \beta^TX^TX $$ My issue now is that $X^TX\beta$ is a $p\times 1$ matrix, and $\beta^TX^TX$ is a $1\times p$ matrix. I mean, I guess it doesn't matter since they're both vectors and each other's transposes, but this seems shaky from a dimensionality perspective. Am I missing something? Thanks!",,"['matrices', 'derivatives', 'linear-regression', 'transpose']"
60,How to prove identity $\nabla f(x) = \nabla F(x)*F(x)$ for 2-norm squared when chain rule says otherwise?,How to prove identity  for 2-norm squared when chain rule says otherwise?,\nabla f(x) = \nabla F(x)*F(x),"I know that the chain rule says that $f(x)=h(g(x))$ and if we want the gradient then $\nabla f = \nabla h*g'(x)$. I am trying to prove the identity: $\nabla f(x) = \nabla F(x)*F(x)$  when $f(x)=\frac{1}{2}||F(x)||^2$ and $F:\mathbb{E_1}\rightarrow\mathbb{E_2}$ where $\mathbb{E}$ is a Euclidean space and $F$ is a $C^1$ smooth mapping. I am struggling with this because it seems to me like based on the definition for chain rule that I said above then wecan say $f(x)=h(g(x))$, and $h(y)=1/2||y||^2$ and $g(x)=F(x)$.  Then obviously $\nabla h=F(x)$, but then how do I get $\nabla F$ for the next part? question: Unless we are able to assume that $\nabla F=g'(x)$, and hence also assume that $g'(x)=F'(x)$ then how can I get this identity to work out? and then in general when can I assume that the derivative equals the gradient?  I was told to not assume that in general.  thanks.","I know that the chain rule says that $f(x)=h(g(x))$ and if we want the gradient then $\nabla f = \nabla h*g'(x)$. I am trying to prove the identity: $\nabla f(x) = \nabla F(x)*F(x)$  when $f(x)=\frac{1}{2}||F(x)||^2$ and $F:\mathbb{E_1}\rightarrow\mathbb{E_2}$ where $\mathbb{E}$ is a Euclidean space and $F$ is a $C^1$ smooth mapping. I am struggling with this because it seems to me like based on the definition for chain rule that I said above then wecan say $f(x)=h(g(x))$, and $h(y)=1/2||y||^2$ and $g(x)=F(x)$.  Then obviously $\nabla h=F(x)$, but then how do I get $\nabla F$ for the next part? question: Unless we are able to assume that $\nabla F=g'(x)$, and hence also assume that $g'(x)=F'(x)$ then how can I get this identity to work out? and then in general when can I assume that the derivative equals the gradient?  I was told to not assume that in general.  thanks.",,"['calculus', 'derivatives', 'vector-spaces', 'vector-analysis', 'gradient-descent']"
61,finding a function satisfiying a certain equation,finding a function satisfiying a certain equation,,"I want to know that if there exists any function $f\in L^2[0,1]$ satisfying $$f(x) = \int^x_0 f(y)\,dy$$ I do not know the value of $f(0)$ and if $f$ is differentiable.","I want to know that if there exists any function $f\in L^2[0,1]$ satisfying $$f(x) = \int^x_0 f(y)\,dy$$ I do not know the value of $f(0)$ and if $f$ is differentiable.",,"['real-analysis', 'derivatives', 'lebesgue-integral']"
62,proof of Taylor's theorem,proof of Taylor's theorem,,"I am struggling to understand this proof. At the near last part, I don't understand how the author derive this equation $g^{(n+1)}(s)=f^{(n+1)}(s)-(n+1)!M_{x,x_0}$. I think it should be $g^{(n+1)}(s)=f^{(n+1)}(s)-(P^{x_0}_n)^{(n+1)}(s)-M_{x,x_0}(s-x_0)^{(n+1)}$. Could you help me understand this part? Thank you in advance!","I am struggling to understand this proof. At the near last part, I don't understand how the author derive this equation $g^{(n+1)}(s)=f^{(n+1)}(s)-(n+1)!M_{x,x_0}$. I think it should be $g^{(n+1)}(s)=f^{(n+1)}(s)-(P^{x_0}_n)^{(n+1)}(s)-M_{x,x_0}(s-x_0)^{(n+1)}$. Could you help me understand this part? Thank you in advance!",,"['real-analysis', 'derivatives']"
63,"Extrema of $(\tan{x})^{\sin{2x}}$ on $(0,\frac{\pi}{2})$ interval",Extrema of  on  interval,"(\tan{x})^{\sin{2x}} (0,\frac{\pi}{2})","Function $(\tan{x})^{\sin{2x}}$ has its minimun for $x=u\in(0,\frac{\pi}{2})$ and maximum for $x=v\in(0,\frac{\pi}{2})$. Find $u+v$ Having calculated the derivative: $$\frac{d}{dx}(\tan{x})^{\sin{2x}} = (\tan{x})^{\sin{2x}}\cdot(2\cos{2x\cdot\ln{\tan{x}}+2})$$  I find it quite difficult to find roots of it (as if there was some other special way/technique to approach this problem) Can You give me any hint? I will appreciate everything.","Function $(\tan{x})^{\sin{2x}}$ has its minimun for $x=u\in(0,\frac{\pi}{2})$ and maximum for $x=v\in(0,\frac{\pi}{2})$. Find $u+v$ Having calculated the derivative: $$\frac{d}{dx}(\tan{x})^{\sin{2x}} = (\tan{x})^{\sin{2x}}\cdot(2\cos{2x\cdot\ln{\tan{x}}+2})$$  I find it quite difficult to find roots of it (as if there was some other special way/technique to approach this problem) Can You give me any hint? I will appreciate everything.",,"['calculus', 'real-analysis', 'trigonometry', 'derivatives']"
64,Derivation of a vector-valued function,Derivation of a vector-valued function,,"i have trouble to calculate the derivation of the the norm. I have given a curve $\gamma :[0,L] \rightarrow \mathbb{R^3}$ with $\gamma(0)=\gamma(L)$, $\Vert \gamma^´(s) \Vert=1$ and a second function $\beta(s)=\Vert \gamma(s)-\gamma(0)\Vert$. I´d like to calculate $\beta'(s)$ My approach: I set $f(\cdot):= \Vert \cdot\Vert$ and $g(s):=\gamma(s)-\gamma(0)$, then i get by chain rule $\frac{d}{ds}f(g(s)) =f'(g(s))g'(s)=\frac{\gamma(s)-\gamma(0)}{\Vert \gamma(s)-\gamma(0)\Vert}\gamma'(s) $ where i use that $f'(\vec{v})=\frac{\vec{v}}{\Vert \vec{v} \Vert}$ Is it correct or did i miss something ?","i have trouble to calculate the derivation of the the norm. I have given a curve $\gamma :[0,L] \rightarrow \mathbb{R^3}$ with $\gamma(0)=\gamma(L)$, $\Vert \gamma^´(s) \Vert=1$ and a second function $\beta(s)=\Vert \gamma(s)-\gamma(0)\Vert$. I´d like to calculate $\beta'(s)$ My approach: I set $f(\cdot):= \Vert \cdot\Vert$ and $g(s):=\gamma(s)-\gamma(0)$, then i get by chain rule $\frac{d}{ds}f(g(s)) =f'(g(s))g'(s)=\frac{\gamma(s)-\gamma(0)}{\Vert \gamma(s)-\gamma(0)\Vert}\gamma'(s) $ where i use that $f'(\vec{v})=\frac{\vec{v}}{\Vert \vec{v} \Vert}$ Is it correct or did i miss something ?",,"['derivatives', 'proof-verification']"
65,Function value change with constrained arguments,Function value change with constrained arguments,,"Suppose I have a function: $$f(a,b)=h(a)+g(b)$$ where $a+b=1$. I'm interested in the values of $a$ and $b$ that maximize $f(a,b)$.  Without substitution (e.g. by using the fact that $b=1-a$), how could I find out whether $f(a,b)$ is increasing in $a$? My approach is as follows: since an increase in $a$ leads to a corresponding decrease in $b$, there are two effects of increasing $a$: $$\frac{dh(a)}{da};-\frac{dg(b)}{db}$$ Such that the net effect is positive iff $$\frac{dh(a)}{da}-\frac{dg(b)}{db}>0$$ Am I correct?","Suppose I have a function: $$f(a,b)=h(a)+g(b)$$ where $a+b=1$. I'm interested in the values of $a$ and $b$ that maximize $f(a,b)$.  Without substitution (e.g. by using the fact that $b=1-a$), how could I find out whether $f(a,b)$ is increasing in $a$? My approach is as follows: since an increase in $a$ leads to a corresponding decrease in $b$, there are two effects of increasing $a$: $$\frac{dh(a)}{da};-\frac{dg(b)}{db}$$ Such that the net effect is positive iff $$\frac{dh(a)}{da}-\frac{dg(b)}{db}>0$$ Am I correct?",,"['calculus', 'real-analysis', 'derivatives']"
66,Derivative of $BAA^Tx$ with respect to $A$,Derivative of  with respect to,BAA^Tx A,"Find the derivative of $B A A^T x$ with respect to $A$, where $A, B$ are $n \times n$ matrices and $x$ is a vector. Clarification: When I say derivative I want to find the derivative of each element of the expression $BAA^Tx$ with respect to each element in $A$. In other words we are trying to find the derivative of a vector with respect to a matrix. Edit: This question and answer here is relevant. What I know/tried: (some wishful thinking) Well I know that I can write $y:=BAA^Tx$ in vectorized form as $y = (x^T \otimes B) \mathrm{vec}(AA^T)$. Then by ""chain rule"" we should get $$ \frac{\partial y}{\partial A} = (x^T \otimes B)\, \circ\,\frac{\partial({\mathrm{vec}(AA^T)})}{\partial A} $$ Thus, the problem actually boils down to finding the derivative of $AA^T$ with respect to $A$. How does one go about finding that last derivative?. Am I even on the right track?. Thanks.","Find the derivative of $B A A^T x$ with respect to $A$, where $A, B$ are $n \times n$ matrices and $x$ is a vector. Clarification: When I say derivative I want to find the derivative of each element of the expression $BAA^Tx$ with respect to each element in $A$. In other words we are trying to find the derivative of a vector with respect to a matrix. Edit: This question and answer here is relevant. What I know/tried: (some wishful thinking) Well I know that I can write $y:=BAA^Tx$ in vectorized form as $y = (x^T \otimes B) \mathrm{vec}(AA^T)$. Then by ""chain rule"" we should get $$ \frac{\partial y}{\partial A} = (x^T \otimes B)\, \circ\,\frac{\partial({\mathrm{vec}(AA^T)})}{\partial A} $$ Thus, the problem actually boils down to finding the derivative of $AA^T$ with respect to $A$. How does one go about finding that last derivative?. Am I even on the right track?. Thanks.",,"['matrices', 'derivatives', 'matrix-calculus']"
67,Mean Value Theorem (another),Mean Value Theorem (another),,"Does the mean value theorem also apply to lateral derivatives? Let $f: [a,b] \longrightarrow \mathbb{R}$ continuous and $f$ right differentiable in $(a,b)$, then, theres exists $c \in (a,b)$ such that    $$f'_{+}(c) = \frac{f(b)-f(a)}{b-a}$$ In the proof of the Means Value Theorem, based on Rolle's Theorem, it wasn't clear to me what would happen if we changed this hypothesis","Does the mean value theorem also apply to lateral derivatives? Let $f: [a,b] \longrightarrow \mathbb{R}$ continuous and $f$ right differentiable in $(a,b)$, then, theres exists $c \in (a,b)$ such that    $$f'_{+}(c) = \frac{f(b)-f(a)}{b-a}$$ In the proof of the Means Value Theorem, based on Rolle's Theorem, it wasn't clear to me what would happen if we changed this hypothesis",,"['real-analysis', 'derivatives']"
68,Intuitive explanation of a derivative with an example,Intuitive explanation of a derivative with an example,,"I am having a hard time grasping a concept of derivative intuitively, perhaps due to a lack of a good example of how it can be used in practice. I am looking for an explanation in laymen terms with a practical example that can be deconstructed and would give an idea of how a derivative can be used in practice. I am not looking for mathematical proof or strict mathematical definition. Here is my current understanding, please point out where it is correct or incorrect intuitively: Let's say that we have $y$ (dependent variable or output) and $x$ (independent variable or input). If we have a function of   $y=x^{3}$. Does derivative tell us by how much the output of a function (dependent variable $y$) when we change input (independent variable $x$) by a certain amount ($dx$)? In other words, derivative tells us how sensitive the function is to the changes in its input. P.S. I could not find a satisfactory explanation of this question anywhere on stack exchange.","I am having a hard time grasping a concept of derivative intuitively, perhaps due to a lack of a good example of how it can be used in practice. I am looking for an explanation in laymen terms with a practical example that can be deconstructed and would give an idea of how a derivative can be used in practice. I am not looking for mathematical proof or strict mathematical definition. Here is my current understanding, please point out where it is correct or incorrect intuitively: Let's say that we have $y$ (dependent variable or output) and $x$ (independent variable or input). If we have a function of   $y=x^{3}$. Does derivative tell us by how much the output of a function (dependent variable $y$) when we change input (independent variable $x$) by a certain amount ($dx$)? In other words, derivative tells us how sensitive the function is to the changes in its input. P.S. I could not find a satisfactory explanation of this question anywhere on stack exchange.",,"['calculus', 'derivatives']"
69,How to interpret probability density function of transformed variable?,How to interpret probability density function of transformed variable?,,"I am currently reading digital image processing by Rafael c. Gonzalez (pdf link page 92 in page and 103 in pdf, equation 3.3-3).  Basically what it says is if: r : denotes the intensities of an image and has range  [ $0$ - ($L-1$)]. s = $T(r)$ Let $p_s$(r) and $p_s$(s)  denote the PDFs of r and s. A fundamental result from basic probability theory is that if $p_s$(r) and $p_s$(s) are known and $T(r)$ is continuous and differentiable over the range of values of interest then the PDF of the transformed (mapped) variable $s$ can be obtained using simple formula. $p_s$(s) = $p_s$(r)$|$$\frac{dr}{ds}$$|$ Where did this simple formula came from? I am trying to wrap my head around where did $|$$\frac{dr}{ds}$$|$  came from?","I am currently reading digital image processing by Rafael c. Gonzalez (pdf link page 92 in page and 103 in pdf, equation 3.3-3).  Basically what it says is if: r : denotes the intensities of an image and has range  [ $0$ - ($L-1$)]. s = $T(r)$ Let $p_s$(r) and $p_s$(s)  denote the PDFs of r and s. A fundamental result from basic probability theory is that if $p_s$(r) and $p_s$(s) are known and $T(r)$ is continuous and differentiable over the range of values of interest then the PDF of the transformed (mapped) variable $s$ can be obtained using simple formula. $p_s$(s) = $p_s$(r)$|$$\frac{dr}{ds}$$|$ Where did this simple formula came from? I am trying to wrap my head around where did $|$$\frac{dr}{ds}$$|$  came from?",,"['probability', 'derivatives', 'image-processing']"
70,What does the second derivative of a quadratic function actually mean?,What does the second derivative of a quadratic function actually mean?,,"The second derivative of $y=ax^2 +bx +x$ is $\frac{d^2y}{dx^2}=2a$ But what does $2a$ mean in terms of the graph of this function? Take the function $f(x)=\frac{x^2}{2}$ It has a $2a$ value of $1$. I understand that the second derivative expresses the concavity of a graph, but I can't see how a concavity of $1$ makes sense for this graph. There have been posts similar to this topic but I have not seen a satisfactory answer. Can anyone explain what it means to say that $f(x)=\frac{x^2}{2}$ has a concavity of $1$ everywhere along the graph?","The second derivative of $y=ax^2 +bx +x$ is $\frac{d^2y}{dx^2}=2a$ But what does $2a$ mean in terms of the graph of this function? Take the function $f(x)=\frac{x^2}{2}$ It has a $2a$ value of $1$. I understand that the second derivative expresses the concavity of a graph, but I can't see how a concavity of $1$ makes sense for this graph. There have been posts similar to this topic but I have not seen a satisfactory answer. Can anyone explain what it means to say that $f(x)=\frac{x^2}{2}$ has a concavity of $1$ everywhere along the graph?",,"['derivatives', 'curvature']"
71,Studying the differentiability of $f(P) = P(0)P'(1)$,Studying the differentiability of,f(P) = P(0)P'(1),"Let $E= \mathbb{R}_n[X]$ and $f: E \rightarrow \mathbb{R}$ such that $f(P) = P(0)P'(1)$. Show that it is differentiable at every $P \in E$ My teacher proposed a solution without giving full justification, which doesn't allow me to fully understand why it works. And also, I am unsure which norm to use. He states that if we study for $P, H \in E$, we have: $$f(P+H) = (P+H)(0)(P+H)'(1) = P(0)P'(1) + P(0)H'(1) + H(0)P'(1) + H(0)H'(1) $$ Thus we can put $df(P)(H) = P(0)H'(1) + H(0)P'(1)$ (which is linear) and as $H(0)H'(1)$, we get the differentiability. Yet, if I study (with the condition that $H \not = 0$), we have: $$\frac{||f(P+H)-f(P) - df(P)(H)||}{||H||} = \frac{||H(0)H'(1)||}{||H||} $$ it doesn't seem that this would have a limit equal to $0$ when $||H|| \rightarrow 0$","Let $E= \mathbb{R}_n[X]$ and $f: E \rightarrow \mathbb{R}$ such that $f(P) = P(0)P'(1)$. Show that it is differentiable at every $P \in E$ My teacher proposed a solution without giving full justification, which doesn't allow me to fully understand why it works. And also, I am unsure which norm to use. He states that if we study for $P, H \in E$, we have: $$f(P+H) = (P+H)(0)(P+H)'(1) = P(0)P'(1) + P(0)H'(1) + H(0)P'(1) + H(0)H'(1) $$ Thus we can put $df(P)(H) = P(0)H'(1) + H(0)P'(1)$ (which is linear) and as $H(0)H'(1)$, we get the differentiability. Yet, if I study (with the condition that $H \not = 0$), we have: $$\frac{||f(P+H)-f(P) - df(P)(H)||}{||H||} = \frac{||H(0)H'(1)||}{||H||} $$ it doesn't seem that this would have a limit equal to $0$ when $||H|| \rightarrow 0$",,"['derivatives', 'differential-topology']"
72,Are Stock Market graphs differentiable?,Are Stock Market graphs differentiable?,,"I read investing in Stock Market was about predicting the future of a graph. Suppose we know the value of the graph at all points before $a$ and also at $a$. Then one could use a very small number $h$ to calculate the approximate left hand derivative at $a$ by using the formula: $$f'(a)=\frac{f(a-h)-f(a)}{-h}$$ Similarly, the successive left hand derivatives could be calculated by the formulas: $$f''(a)=\frac{f(a-2h)-2f(a-h)+f(a)}{h^2}$$ I believe the general formula is: $$f^n(a)=\frac{\sum_{r=0}^n (-1)^r\binom{n}{r}f(a-(n-r)h)}{(-h)^n}$$ Then one assumes all Left Hand Derivatives=Right Hand Derivatives to plot the approximate future of the graph by using the formula: $$f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\frac{f'''(a)}{3!}(x-a)^3+.....$$","I read investing in Stock Market was about predicting the future of a graph. Suppose we know the value of the graph at all points before $a$ and also at $a$. Then one could use a very small number $h$ to calculate the approximate left hand derivative at $a$ by using the formula: $$f'(a)=\frac{f(a-h)-f(a)}{-h}$$ Similarly, the successive left hand derivatives could be calculated by the formulas: $$f''(a)=\frac{f(a-2h)-2f(a-h)+f(a)}{h^2}$$ I believe the general formula is: $$f^n(a)=\frac{\sum_{r=0}^n (-1)^r\binom{n}{r}f(a-(n-r)h)}{(-h)^n}$$ Then one assumes all Left Hand Derivatives=Right Hand Derivatives to plot the approximate future of the graph by using the formula: $$f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\frac{f'''(a)}{3!}(x-a)^3+.....$$",,"['calculus', 'derivatives', 'taylor-expansion', 'graphing-functions']"
73,What is the second derivative of $f^{-1}(g(x))$?,What is the second derivative of ?,f^{-1}(g(x)),"What is the second derivative of $f^{-1}(g(x))$? $f^{-1}$ is the inverse of $f$: The first derivative of $f^{-1}(x)$ is given by $\frac{1}{f'(f^{-1}(x))}$ The second derivative of $f^{-1}(x)$ is given by $-\frac{f''(f^{-1}(x))}{[f'(f^{-1}(x))]^3}$ The second derivative of $f(g(x))$ (by the chain rule) is given by $g'(x)^2\cdot f''(g(x))+g''(x) \cdot f'(g(x))$. Substituting the derivatives into this equation, we have that the second derivative of $f^{-1}(g(x))$ is $$-\frac{g'(x)^2\cdot f''(f^{-1}(g(x)))}{[f'(f^{-1}(g(x)))]^3}+\frac{g''(x)}{f'(f^{-1}(g(x)))}$$ Which is the same as $$\frac{g''(x)*[f'(f^{-1}(g(x))]^2-g'(x)^2*f''(f^{-1}(g(x)))}{[f'(f^{-1}(g(x)))]^3}$$ Is this correct?","What is the second derivative of $f^{-1}(g(x))$? $f^{-1}$ is the inverse of $f$: The first derivative of $f^{-1}(x)$ is given by $\frac{1}{f'(f^{-1}(x))}$ The second derivative of $f^{-1}(x)$ is given by $-\frac{f''(f^{-1}(x))}{[f'(f^{-1}(x))]^3}$ The second derivative of $f(g(x))$ (by the chain rule) is given by $g'(x)^2\cdot f''(g(x))+g''(x) \cdot f'(g(x))$. Substituting the derivatives into this equation, we have that the second derivative of $f^{-1}(g(x))$ is $$-\frac{g'(x)^2\cdot f''(f^{-1}(g(x)))}{[f'(f^{-1}(g(x)))]^3}+\frac{g''(x)}{f'(f^{-1}(g(x)))}$$ Which is the same as $$\frac{g''(x)*[f'(f^{-1}(g(x))]^2-g'(x)^2*f''(f^{-1}(g(x)))}{[f'(f^{-1}(g(x)))]^3}$$ Is this correct?",,"['calculus', 'real-analysis', 'functional-analysis', 'derivatives']"
74,Why don't negative root answer count when using derivatives?,Why don't negative root answer count when using derivatives?,,"I was completing the question, Find $f'(2)$ if $f(x)= \sqrt{3x-2}$. When I was calculating it as $1.5/\sqrt4$, I got the answers, $3/4$ and $-3/4$ but $-3/4$ wasn't counted in the answers. Why is that?","I was completing the question, Find $f'(2)$ if $f(x)= \sqrt{3x-2}$. When I was calculating it as $1.5/\sqrt4$, I got the answers, $3/4$ and $-3/4$ but $-3/4$ wasn't counted in the answers. Why is that?",,"['calculus', 'derivatives']"
75,Example for a continuous function that has directional derivative at every point but not differentiable at the origin,Example for a continuous function that has directional derivative at every point but not differentiable at the origin,,"Can we find a function $f:\mathbb R^n\to\mathbb R$ that such that $f$ is continuous and $\partial_v f(p)$ exists for all $p\in\mathbb R^n$ and $v\in\mathbb R^n$. But $f$ is not differentiable at $0$? Is such function $f$ exists? Here give a example that has directional derivative everywhere, but it's not continuous at the origin.","Can we find a function $f:\mathbb R^n\to\mathbb R$ that such that $f$ is continuous and $\partial_v f(p)$ exists for all $p\in\mathbb R^n$ and $v\in\mathbb R^n$. But $f$ is not differentiable at $0$? Is such function $f$ exists? Here give a example that has directional derivative everywhere, but it's not continuous at the origin.",,"['real-analysis', 'analysis', 'derivatives']"
76,How many dimensions will a derivative of a 1-D vector by a 2-D matrix have?,How many dimensions will a derivative of a 1-D vector by a 2-D matrix have?,,"As the title above, I find it hard to imagine or illustrate. It is a question from Coursera and the answer is 3. I do not get it why the answer is 3?","As the title above, I find it hard to imagine or illustrate. It is a question from Coursera and the answer is 3. I do not get it why the answer is 3?",,"['matrices', 'derivatives', 'vectors']"
77,Determine if $x=0$ is a point of relative extremum for $f(x)= \sin(x) + \frac{x^3}{6}$,Determine if  is a point of relative extremum for,x=0 f(x)= \sin(x) + \frac{x^3}{6},"Determine if $x=0$ is a point of relative extremum for $f(x)= \sin(x) + \frac{x^3}{6}$ I am trying to use this test Here, $f(x)= \sin(x) + \frac{x^3}{6}$ $f'(x)=\cos(x) + \frac{x^2}{2} \Rightarrow f'(0)=1 \neq 0$ So I am unable to proceed further.","Determine if $x=0$ is a point of relative extremum for $f(x)= \sin(x) + \frac{x^3}{6}$ I am trying to use this test Here, $f(x)= \sin(x) + \frac{x^3}{6}$ $f'(x)=\cos(x) + \frac{x^2}{2} \Rightarrow f'(0)=1 \neq 0$ So I am unable to proceed further.",,"['real-analysis', 'derivatives']"
78,Relationship between the roots of derivatives and their functions,Relationship between the roots of derivatives and their functions,,"Given that $f$ is differentiable on $\mathbb{R}$, I know that ""between the zeroes of $f$, there is a zero of $f'$. But given that $f'$ has $k$ roots, then is it true that $f$ has at most $k+1$ roots.","Given that $f$ is differentiable on $\mathbb{R}$, I know that ""between the zeroes of $f$, there is a zero of $f'$. But given that $f'$ has $k$ roots, then is it true that $f$ has at most $k+1$ roots.",,"['calculus', 'derivatives', 'roots']"
79,"Given $f(x)=\frac{\sin \pi x}{\pi\sin x}$ and $f'(x_0)=0$,find value of $(f(x_0))^2(1+(\pi^2-1)\sin^2x_0)$","Given  and ,find value of",f(x)=\frac{\sin \pi x}{\pi\sin x} f'(x_0)=0 (f(x_0))^2(1+(\pi^2-1)\sin^2x_0),"Let $$f(x)=\frac{\sin (\pi x)}{\pi\sin x}$$ and let $x_0\in(0,\pi)$ such that $f'(x_0)=0$   Then find the value of   $$(f(x_0))^2(1+(\pi^2-1)\sin^2x_0)$$ My attempt: $$f'(x_0)=\lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}$$ $$=\lim_{x\to x_0}\frac{\frac{\sin (\pi x)}{\pi \sin x}-\frac{\sin (\pi x_0)}{\pi \sin x_0}}{x-x_0}$$ $$\lim_{x \to x_0}\frac{\sin(\pi x)\sin x_0-\sin (\pi x_0)\sin x}{\pi \sin x\sin x_0(x-x_0)}$$ Applying L'Hopital's rule and then subsituting the value of limit,I got $$\frac{\pi\cos (\pi x_0)\sin x_0-\cos x_0\sin(\pi x_0)}{\pi\sin^2 x_0}=0$$ How will I make into the form asked in the question?","Let $$f(x)=\frac{\sin (\pi x)}{\pi\sin x}$$ and let $x_0\in(0,\pi)$ such that $f'(x_0)=0$   Then find the value of   $$(f(x_0))^2(1+(\pi^2-1)\sin^2x_0)$$ My attempt: $$f'(x_0)=\lim_{x \to x_0}\frac{f(x)-f(x_0)}{x-x_0}$$ $$=\lim_{x\to x_0}\frac{\frac{\sin (\pi x)}{\pi \sin x}-\frac{\sin (\pi x_0)}{\pi \sin x_0}}{x-x_0}$$ $$\lim_{x \to x_0}\frac{\sin(\pi x)\sin x_0-\sin (\pi x_0)\sin x}{\pi \sin x\sin x_0(x-x_0)}$$ Applying L'Hopital's rule and then subsituting the value of limit,I got $$\frac{\pi\cos (\pi x_0)\sin x_0-\cos x_0\sin(\pi x_0)}{\pi\sin^2 x_0}=0$$ How will I make into the form asked in the question?",,"['calculus', 'derivatives']"
80,Derivative of the product of matrices,Derivative of the product of matrices,,"please give me a hint for the following exercise: It is the third part of this exercise Complementar Lie subalgebra of $\mathfrak{o}(2n)$ Let $O(n)$ be the real orthogonal group and define the map $$F:O(n)\times GL(n;\mathbb{R})\to GL(n;\mathbb{R})$$ given by $(M,N)\longmapsto MN^{-1}$. Find $D_{(I,I)}F$. I see that $$F(M,N)=m(M,i(N)),$$ where $m$ is the multiplication of matrices and $i$ the invertion. Also I know that $D_{(I,I)}m(A,B)=A+B$ and $D_Ii=-I_d$ How can I continue? Thank you","please give me a hint for the following exercise: It is the third part of this exercise Complementar Lie subalgebra of $\mathfrak{o}(2n)$ Let $O(n)$ be the real orthogonal group and define the map $$F:O(n)\times GL(n;\mathbb{R})\to GL(n;\mathbb{R})$$ given by $(M,N)\longmapsto MN^{-1}$. Find $D_{(I,I)}F$. I see that $$F(M,N)=m(M,i(N)),$$ where $m$ is the multiplication of matrices and $i$ the invertion. Also I know that $D_{(I,I)}m(A,B)=A+B$ and $D_Ii=-I_d$ How can I continue? Thank you",,"['linear-algebra', 'derivatives', 'differential-geometry', 'lie-groups', 'matrix-calculus']"
81,Transpose formula to find a value,Transpose formula to find a value,,"Can someone help with this please? Ive differentiated a formula to get a value, now I need to find the positive value for t for when $\frac{dR}{dt} = 0$ So: $0 = (27t^{0.5}    e^{-3t}) + (-54t^{1.5} e^{-3t})$ How would go about finding t here?","Can someone help with this please? Ive differentiated a formula to get a value, now I need to find the positive value for t for when $\frac{dR}{dt} = 0$ So: $0 = (27t^{0.5}    e^{-3t}) + (-54t^{1.5} e^{-3t})$ How would go about finding t here?",,['derivatives']
82,Construct a function exactly belongs to $H^1(\mathbb{R})$,Construct a function exactly belongs to,H^1(\mathbb{R}),"Question: How to construct a function exactly belongs to $H^1(\mathbb{R})$ but does not belong to $H^{1+s}(\mathbb{R})$ for any $s>0$? My try: An obviously try is $u(x)=|x|$. However, I find that $u(x)\in H^{1+1/2-\epsilon}(\mathbb{R})$ actually. Is there any way to find out this function? Thanks in advance.","Question: How to construct a function exactly belongs to $H^1(\mathbb{R})$ but does not belong to $H^{1+s}(\mathbb{R})$ for any $s>0$? My try: An obviously try is $u(x)=|x|$. However, I find that $u(x)\in H^{1+1/2-\epsilon}(\mathbb{R})$ actually. Is there any way to find out this function? Thanks in advance.",,"['derivatives', 'sobolev-spaces']"
83,Apply Leibniz integral rule to derive the formula for power $P=\mathbf{F}\cdot\mathbf{v}$,Apply Leibniz integral rule to derive the formula for power,P=\mathbf{F}\cdot\mathbf{v},"I have posed this problem for myself. I am asking it here because I need to know about the calculus specifically, not the physics. Suppose that $\mathbf{r}$ varies with $t$ and that $\mathbf{F}$ varies with $\mathbf{r}$ and $t$. Using the definitions of power and work, derive $P=\mathbf{F}\cdot\mathbf{v}$. When integrating across constant limits $a$ and $b$, Leibniz integral rule reduces to $$\frac{d}{dx}\int_a^b f(x,t)\,dt=\int_a^b\frac{\partial}{\partial x}f(x,t)\,dt$$ Using this rule and the definitions of power and work— $$\begin{align} P &= \frac{dW}{dt} \\[2ex] W &= \int\mathbf{F}\cdot d\mathbf{r} \\ \end{align}$$ —I am trying to proceed with the derivation thusly: $$\begin{align} P &= \frac{dW}{dt} \tag1\\[2ex] &= \frac{d}{dt}\int\mathbf{F}\cdot d\mathbf{r} \tag2\\ \end{align}$$ Here is where I get stuck: is the next step $$P=\int\frac{\partial\mathbf{F}}{\partial t}\cdot d\mathbf{r} \tag{3a}$$ or is it $$P=\int\frac{\partial}{\partial t}\left(\mathbf{F}\cdot d\mathbf{r}\right) \tag{3b}$$ These are the vibes I am getting for how to execute the derivation: $\partial/\partial t$ and $d\mathbf{r}$ somehow come together to make $\mathbf{v}$ $\partial/\partial t$ needs to end up with $\mathbf{F}$ as well somehow the integral will somehow “undo” the time-derivative of $\mathbf{F}$ I am also not sure what to do with the extra differential of $\mathbf{r}$: will it become $d(\partial\mathbf{r})$ or $\partial(d\mathbf{r})$, and how does this interact with the integral?","I have posed this problem for myself. I am asking it here because I need to know about the calculus specifically, not the physics. Suppose that $\mathbf{r}$ varies with $t$ and that $\mathbf{F}$ varies with $\mathbf{r}$ and $t$. Using the definitions of power and work, derive $P=\mathbf{F}\cdot\mathbf{v}$. When integrating across constant limits $a$ and $b$, Leibniz integral rule reduces to $$\frac{d}{dx}\int_a^b f(x,t)\,dt=\int_a^b\frac{\partial}{\partial x}f(x,t)\,dt$$ Using this rule and the definitions of power and work— $$\begin{align} P &= \frac{dW}{dt} \\[2ex] W &= \int\mathbf{F}\cdot d\mathbf{r} \\ \end{align}$$ —I am trying to proceed with the derivation thusly: $$\begin{align} P &= \frac{dW}{dt} \tag1\\[2ex] &= \frac{d}{dt}\int\mathbf{F}\cdot d\mathbf{r} \tag2\\ \end{align}$$ Here is where I get stuck: is the next step $$P=\int\frac{\partial\mathbf{F}}{\partial t}\cdot d\mathbf{r} \tag{3a}$$ or is it $$P=\int\frac{\partial}{\partial t}\left(\mathbf{F}\cdot d\mathbf{r}\right) \tag{3b}$$ These are the vibes I am getting for how to execute the derivation: $\partial/\partial t$ and $d\mathbf{r}$ somehow come together to make $\mathbf{v}$ $\partial/\partial t$ needs to end up with $\mathbf{F}$ as well somehow the integral will somehow “undo” the time-derivative of $\mathbf{F}$ I am also not sure what to do with the extra differential of $\mathbf{r}$: will it become $d(\partial\mathbf{r})$ or $\partial(d\mathbf{r})$, and how does this interact with the integral?",,"['integration', 'derivatives', 'partial-derivative', 'physics']"
84,Rigorous proof needed for: If $\vec{r}(t)\times d{\vec{r}(t)}=0$ then $\vec{r}(t)$ is a constant vector.,Rigorous proof needed for: If  then  is a constant vector.,\vec{r}(t)\times d{\vec{r}(t)}=0 \vec{r}(t),"If $\vec{r}(t)\times d{\vec{r}(t)}=0$, prove that $\hat{r}$   is a constant vector. My Attempt: Let $\vec{r}(t)=x \hat{i} + y \hat{j} +z \hat{k}$. Thus, $d\vec{r}(t)=dx \hat{i} + dy \hat{j} +dz \hat{k}$ $\therefore \vec{r}(t)\times d\vec{r}(t) = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ x & y & z \\ dx & dy & dz \end{vmatrix}=0 $ From here, we get the following: $ydz-zdy=0$ $zdy-xdz=0$ $xdy-ydx=0$ After this, we can write $\frac{dx}{x}=\frac{dy}{y}=\frac{dz}{z}$. From there we can conclude $x=Ay$ and $z=By$, where $A$ and $B$ are constants. So, $\hat{r}=\frac{\vec{r}}{|\vec{r}|}=\frac{Ay\hat{i}+y\hat{j}+By\hat{k}}{\sqrt{A^2y^2+y^2+B^2y^2}}$ I feel my solution is not rigorous enough. I think it is not always correct that $\frac{dx}{x}=\frac{dy}{y}=\frac{dz}{z}$ If $x$ or $y$ or $z$ were $0$ for some $t$, then that would not hold true. Is there any more rigorous proof for the quoted claim?","If $\vec{r}(t)\times d{\vec{r}(t)}=0$, prove that $\hat{r}$   is a constant vector. My Attempt: Let $\vec{r}(t)=x \hat{i} + y \hat{j} +z \hat{k}$. Thus, $d\vec{r}(t)=dx \hat{i} + dy \hat{j} +dz \hat{k}$ $\therefore \vec{r}(t)\times d\vec{r}(t) = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ x & y & z \\ dx & dy & dz \end{vmatrix}=0 $ From here, we get the following: $ydz-zdy=0$ $zdy-xdz=0$ $xdy-ydx=0$ After this, we can write $\frac{dx}{x}=\frac{dy}{y}=\frac{dz}{z}$. From there we can conclude $x=Ay$ and $z=By$, where $A$ and $B$ are constants. So, $\hat{r}=\frac{\vec{r}}{|\vec{r}|}=\frac{Ay\hat{i}+y\hat{j}+By\hat{k}}{\sqrt{A^2y^2+y^2+B^2y^2}}$ I feel my solution is not rigorous enough. I think it is not always correct that $\frac{dx}{x}=\frac{dy}{y}=\frac{dz}{z}$ If $x$ or $y$ or $z$ were $0$ for some $t$, then that would not hold true. Is there any more rigorous proof for the quoted claim?",,"['calculus', 'derivatives']"
85,Proof of $\frac{\mathrm{d}}{\mathrm{d}x}a^x$ and $\lim\frac{a^h-1}{h}$,Proof of  and,\frac{\mathrm{d}}{\mathrm{d}x}a^x \lim\frac{a^h-1}{h},"I was looking for the proof of $\frac{\mathrm{d}}{\mathrm{d}x}a^x$ as the $\lim_{h\to0}\frac{a^{x+h}-a^{x}}{h}$ which is $a^x\lim_{h\to0}\frac{a^{h}-1}{h}$. To solve that, I need to know the $\lim_{h\to0}\frac{a^{h}-1}{h}$. In terms of $t$ I defined $\frac{1}{t}=a^h-1=e^{h\ln(a)}-1$, therefore $e^{h\ln(a)} = 1+\frac{1}{t}$ and $h=\frac{\ln\left (1+\frac{1}{t}  \right )}{\ln(a)}$. When $h\to0$, $t\to\infty$. $$\begin{aligned}\lim_{h\to0}\frac{a^{h}-1}{h}&=\lim_{t\to\infty}\frac{\frac{1}{t}}{\frac{\ln\left (1+\frac{1}{t}  \right )}{\ln(a)}}\\ &=\lim_{t\to\infty}\frac{\ln(a)}{t\cdot \ln\left (1+\frac{1}{t}  \right )}\\ &=\lim_{t\to\infty}\frac{\ln(a)}{\ln\left (\left (1+\frac{1}{t}  \right )^t  \right )}\\ &=\frac{\ln(a)}{\ln\left ( \lim_{t\to\infty}\left (1+\frac{1}{t}  \right )^t \right )}\end{aligned}$$ In terms of $n$ Now, I need to solve the limit $\lim_{t\to\infty}\left (1+\frac{1}{t}  \right )^t$. I defined $t=\frac{1}{n}$. When $t\to\infty$, $n\to0$. $$\begin{aligned}\lim_{t\to\infty}\left (1+\frac{x}{t}  \right )^t&=\lim_{n\to0}\left (1+\frac{n}{\frac{1}{x}}  \right )^\frac{1}{n}\\ &=\lim_{n\to0}\left (\frac{\frac{1}{x}+n}{\frac{1}{x}}  \right )^\frac{1}{n}\\ &=e^{\lim_{n\to0}\ln\left (\frac{\frac{1}{x}+n}{\frac{1}{x}}  \right )^\frac{1}{n}}\\ &=e^{\lim_{n\to0}\frac{\ln\left (\frac{\frac{1}{x}+n}{\frac{1}{x}}  \right )}{n}}\\ &=e^{\lim_{n\to0}\frac{\ln\left ( \frac{1}{x}+n \right )-\ln\left ( \frac{1}{x} \right )}{n}}\\ &=e^{\frac{\mathrm{d}}{\mathrm{d}\frac{1}{x}}\ln\left ( \frac{1}{x} \right )}\end{aligned}$$ Now I tried to solve $\frac{\mathrm{d}}{\mathrm{d}x}\ln(x)=\frac{1}{\frac{\mathrm{d}}{\mathrm{d}\ln(x)}x}=\frac{1}{\frac{\mathrm{d}}{\mathrm{d}\ln(x)}e^{\ln(x)}}$. And I get back to $\frac{\mathrm{d}}{\mathrm{d}x}a^x$, when $a = e$. Is there a way to solve one of the limits or the derivatives in another way that does not create a loop? Note that I can't use L'Hopital rule, since I did now proof the derivatives. Thanks.","I was looking for the proof of $\frac{\mathrm{d}}{\mathrm{d}x}a^x$ as the $\lim_{h\to0}\frac{a^{x+h}-a^{x}}{h}$ which is $a^x\lim_{h\to0}\frac{a^{h}-1}{h}$. To solve that, I need to know the $\lim_{h\to0}\frac{a^{h}-1}{h}$. In terms of $t$ I defined $\frac{1}{t}=a^h-1=e^{h\ln(a)}-1$, therefore $e^{h\ln(a)} = 1+\frac{1}{t}$ and $h=\frac{\ln\left (1+\frac{1}{t}  \right )}{\ln(a)}$. When $h\to0$, $t\to\infty$. $$\begin{aligned}\lim_{h\to0}\frac{a^{h}-1}{h}&=\lim_{t\to\infty}\frac{\frac{1}{t}}{\frac{\ln\left (1+\frac{1}{t}  \right )}{\ln(a)}}\\ &=\lim_{t\to\infty}\frac{\ln(a)}{t\cdot \ln\left (1+\frac{1}{t}  \right )}\\ &=\lim_{t\to\infty}\frac{\ln(a)}{\ln\left (\left (1+\frac{1}{t}  \right )^t  \right )}\\ &=\frac{\ln(a)}{\ln\left ( \lim_{t\to\infty}\left (1+\frac{1}{t}  \right )^t \right )}\end{aligned}$$ In terms of $n$ Now, I need to solve the limit $\lim_{t\to\infty}\left (1+\frac{1}{t}  \right )^t$. I defined $t=\frac{1}{n}$. When $t\to\infty$, $n\to0$. $$\begin{aligned}\lim_{t\to\infty}\left (1+\frac{x}{t}  \right )^t&=\lim_{n\to0}\left (1+\frac{n}{\frac{1}{x}}  \right )^\frac{1}{n}\\ &=\lim_{n\to0}\left (\frac{\frac{1}{x}+n}{\frac{1}{x}}  \right )^\frac{1}{n}\\ &=e^{\lim_{n\to0}\ln\left (\frac{\frac{1}{x}+n}{\frac{1}{x}}  \right )^\frac{1}{n}}\\ &=e^{\lim_{n\to0}\frac{\ln\left (\frac{\frac{1}{x}+n}{\frac{1}{x}}  \right )}{n}}\\ &=e^{\lim_{n\to0}\frac{\ln\left ( \frac{1}{x}+n \right )-\ln\left ( \frac{1}{x} \right )}{n}}\\ &=e^{\frac{\mathrm{d}}{\mathrm{d}\frac{1}{x}}\ln\left ( \frac{1}{x} \right )}\end{aligned}$$ Now I tried to solve $\frac{\mathrm{d}}{\mathrm{d}x}\ln(x)=\frac{1}{\frac{\mathrm{d}}{\mathrm{d}\ln(x)}x}=\frac{1}{\frac{\mathrm{d}}{\mathrm{d}\ln(x)}e^{\ln(x)}}$. And I get back to $\frac{\mathrm{d}}{\mathrm{d}x}a^x$, when $a = e$. Is there a way to solve one of the limits or the derivatives in another way that does not create a loop? Note that I can't use L'Hopital rule, since I did now proof the derivatives. Thanks.",,"['limits', 'derivatives']"
86,How to find values such that the curves $y=\frac{a}{x-1}$ and $y=x^2-2x+1$ intersect at right angles?,How to find values such that the curves  and  intersect at right angles?,y=\frac{a}{x-1} y=x^2-2x+1,"Problem: Find all values of $a$ such that the curves $y = \frac{a}{x-1}$ and $y = x^2-2x+1$ intersect at right angles. My attempt: First, I set the two curves equal to each other: $ \frac{a}{x-1} = x^2 - 2x + 1 $ $ \frac{a}{x-1} = (x-1)^2 $ $ \frac{a}{x-1}(x-1) = (x-1)^2(x-1) $ $ a = (x-1)^3 $ $ \sqrt[3] a = x-1 $ $ \sqrt[3] a + 1 = x $ ​ I found the derivative of the first curve: $ y = \frac{a}{x-1} $ $ y = a(x-1)^{-1} $ $ y' = -a(x-1)^{-2} $ $ y' = \frac{-a}{(x-1)^{2}} $ Then I found the derivative of the second curve: $ y = x^2 - 2x + 1 $ $ y' = 2x - 2 $ Next, I multiplied them together and set them equal to -1: $ \frac{-a}{(x-1)^{2}} ⋅ 2x - 2 = -1 $ $ \frac{-2ax + 2a}{(x-1)^2} = -1 $ $ \frac{-2a(x-1)}{(x-1)^2} = -1 $ $ \frac{-2a}{x-1} = -1 $ $ \frac{-2a}{x-1} ⋅ (x-1) = -1(x-1) $ $ -2a = -x+1 $ $ a = \frac{-x+1}{-2} $ Now I am unsure how to finish the problem. Do I substitute what I found for $x$ into $ a = \frac{-x+1}{-2} $? But my problem shows that there are two possible answers for $a$ so I am confused. Any help would be appreciated! Thank you in advance!","Problem: Find all values of $a$ such that the curves $y = \frac{a}{x-1}$ and $y = x^2-2x+1$ intersect at right angles. My attempt: First, I set the two curves equal to each other: $ \frac{a}{x-1} = x^2 - 2x + 1 $ $ \frac{a}{x-1} = (x-1)^2 $ $ \frac{a}{x-1}(x-1) = (x-1)^2(x-1) $ $ a = (x-1)^3 $ $ \sqrt[3] a = x-1 $ $ \sqrt[3] a + 1 = x $ ​ I found the derivative of the first curve: $ y = \frac{a}{x-1} $ $ y = a(x-1)^{-1} $ $ y' = -a(x-1)^{-2} $ $ y' = \frac{-a}{(x-1)^{2}} $ Then I found the derivative of the second curve: $ y = x^2 - 2x + 1 $ $ y' = 2x - 2 $ Next, I multiplied them together and set them equal to -1: $ \frac{-a}{(x-1)^{2}} ⋅ 2x - 2 = -1 $ $ \frac{-2ax + 2a}{(x-1)^2} = -1 $ $ \frac{-2a(x-1)}{(x-1)^2} = -1 $ $ \frac{-2a}{x-1} = -1 $ $ \frac{-2a}{x-1} ⋅ (x-1) = -1(x-1) $ $ -2a = -x+1 $ $ a = \frac{-x+1}{-2} $ Now I am unsure how to finish the problem. Do I substitute what I found for $x$ into $ a = \frac{-x+1}{-2} $? But my problem shows that there are two possible answers for $a$ so I am confused. Any help would be appreciated! Thank you in advance!",,"['calculus', 'derivatives']"
87,Find the equation of a parabola and a point on the parabola given a two tangent lines.,Find the equation of a parabola and a point on the parabola given a two tangent lines.,,"I've been trying to find a solution to this problem but I'm not too sure how to go about solving it. I need to find the unknown values of $A, B$ and $C$ in the parabola   equation $y(x) = Ax^2 + Bx + C$ given that the parabola passes through   point (0,0) and is tangent to the line $y1(x) = 0.1x$ which also   passes through the point (0,0). I also need to find an unknown point (x-coordinate, y-coordinate) on   the parabola which is tangent to $y2(x) = -0.08x + 10$ given that   $y2(x) = -0.08x + 10$ passes through (200,-6). I would greatly appreciate any help with solving this problem. So far I've tried to use the $y1(x)$ line to determine the vale of $B$ and $C$ in the parabola: As the parabola passes through (0,0) I tried, $$y(0) = 0$$ so  $$A(0)^2 + B(0) + C = 0\\ C = 0$$ As the line $y1(x) = 0.1x$ is tangent to the parabola at (0,0) I tried, $$ y'(0) = 0.1$$ so $$ 2A(0) + B = 0.1\\                B = 0.1$$ However when trying to find a point on the parabola where the parabola is tangent to $y2(x)$ it seems to me that the value for B would be different, so this has me really confused on how to solve this problem. Once again, any help with this would be very very appreciated :)","I've been trying to find a solution to this problem but I'm not too sure how to go about solving it. I need to find the unknown values of $A, B$ and $C$ in the parabola   equation $y(x) = Ax^2 + Bx + C$ given that the parabola passes through   point (0,0) and is tangent to the line $y1(x) = 0.1x$ which also   passes through the point (0,0). I also need to find an unknown point (x-coordinate, y-coordinate) on   the parabola which is tangent to $y2(x) = -0.08x + 10$ given that   $y2(x) = -0.08x + 10$ passes through (200,-6). I would greatly appreciate any help with solving this problem. So far I've tried to use the $y1(x)$ line to determine the vale of $B$ and $C$ in the parabola: As the parabola passes through (0,0) I tried, $$y(0) = 0$$ so  $$A(0)^2 + B(0) + C = 0\\ C = 0$$ As the line $y1(x) = 0.1x$ is tangent to the parabola at (0,0) I tried, $$ y'(0) = 0.1$$ so $$ 2A(0) + B = 0.1\\                B = 0.1$$ However when trying to find a point on the parabola where the parabola is tangent to $y2(x)$ it seems to me that the value for B would be different, so this has me really confused on how to solve this problem. Once again, any help with this would be very very appreciated :)",,"['calculus', 'derivatives', 'learning']"
88,What does this notation mean? (See the picture.),What does this notation mean? (See the picture.),,It's a problem from a past-paper. So what is this notation tell me to do? How should I start solving this? Please tell me where should I start? I'm confused with the notation that $J$ points,It's a problem from a past-paper. So what is this notation tell me to do? How should I start solving this? Please tell me where should I start? I'm confused with the notation that $J$ points,,"['calculus', 'derivatives', 'notation', 'partial-derivative']"
89,Finite difference second derivative: Error analysis,Finite difference second derivative: Error analysis,,"I want to approximate $f''(x)$ using finite differences. From  \begin{align*} f(x+h)=f(x)+f'(x)h+\frac{1}{2}f''(x)h^2+\frac{1}{6}f'''(x)h^3+O(h^4),\\ f(x-h)=f(x)-f'(x)h+\frac{1}{2}f''(x)h^2-\frac{1}{6}f'''(x)h^3+O(h^4), \end{align*} we easily obtain \begin{align} f''(x)=\frac{f(x+h)-2f(x)+f(x-h)}{h^2}+O(h^2). \end{align} Alternatively, we can apply centered finite differences twice with step size $h/2$ and error $O(h^2)$ to write:  \begin{align} f''(x) &= \frac{f'(x+h/2)-f'(x-h/2)}{h}+O(h^2)\\ &=\frac{\frac{f(x+h)-f(x)}{h}-\frac{f(x)-f(x-h)}{h}+O(h^2)}{h}+O(h^2)\\ &=\frac{f(x+h)-2f(x)+f(x-h)}{h^2}+O(h)+O(h^2). \end{align} I do not understand the inconsistency between the two approaches. Should the $O(h)$ error term above vanish?","I want to approximate $f''(x)$ using finite differences. From  \begin{align*} f(x+h)=f(x)+f'(x)h+\frac{1}{2}f''(x)h^2+\frac{1}{6}f'''(x)h^3+O(h^4),\\ f(x-h)=f(x)-f'(x)h+\frac{1}{2}f''(x)h^2-\frac{1}{6}f'''(x)h^3+O(h^4), \end{align*} we easily obtain \begin{align} f''(x)=\frac{f(x+h)-2f(x)+f(x-h)}{h^2}+O(h^2). \end{align} Alternatively, we can apply centered finite differences twice with step size $h/2$ and error $O(h^2)$ to write:  \begin{align} f''(x) &= \frac{f'(x+h/2)-f'(x-h/2)}{h}+O(h^2)\\ &=\frac{\frac{f(x+h)-f(x)}{h}-\frac{f(x)-f(x-h)}{h}+O(h^2)}{h}+O(h^2)\\ &=\frac{f(x+h)-2f(x)+f(x-h)}{h^2}+O(h)+O(h^2). \end{align} I do not understand the inconsistency between the two approaches. Should the $O(h)$ error term above vanish?",,"['derivatives', 'numerical-methods', 'taylor-expansion', 'finite-differences']"
90,"If a function is constant on some domain and differentiable, is it constant on a larger domain?","If a function is constant on some domain and differentiable, is it constant on a larger domain?",,"If $f(x) = C$ on $(a,b)$ and differentiable does this imply that $f(x) = C$ on the whole domain ($\mathbb{R}$)?","If $f(x) = C$ on $(a,b)$ and differentiable does this imply that $f(x) = C$ on the whole domain ($\mathbb{R}$)?",,"['calculus', 'derivatives']"
91,First and second derivative or Moore's Law,First and second derivative or Moore's Law,,"I am tasked with taking the first and second derivative of a polynomial function. I chose Moore's law which states that every two years, the amount of transistors double. The equation for this law is: $$ P_n = P_0\times2^{1/2n} $$ $P_n$ is the computer processing power n years after the current year. $P_0$ is the computer processing power in the current year. $n$ is the number of years after the current year. My question is to take the derivative of this do I drop the $P_n$ because it is a constant? Or is this a matter of using Leibniz's notation vs Lagrange's notation? Should I be using implicit differentiation because I have more than one variable? I know the rules of differentiation I am just not sure of the format because I am used to seeing equations like $f(x)= ...$ Thank you for your help.","I am tasked with taking the first and second derivative of a polynomial function. I chose Moore's law which states that every two years, the amount of transistors double. The equation for this law is: $$ P_n = P_0\times2^{1/2n} $$ $P_n$ is the computer processing power n years after the current year. $P_0$ is the computer processing power in the current year. $n$ is the number of years after the current year. My question is to take the derivative of this do I drop the $P_n$ because it is a constant? Or is this a matter of using Leibniz's notation vs Lagrange's notation? Should I be using implicit differentiation because I have more than one variable? I know the rules of differentiation I am just not sure of the format because I am used to seeing equations like $f(x)= ...$ Thank you for your help.",,"['calculus', 'derivatives']"
92,Differentiability of a supremum of a family of functions with respect to a parameter,Differentiability of a supremum of a family of functions with respect to a parameter,,"Let $f : \Bbb R \times M \to [0, \infty)$ be smooth, with $M$ a compact smooth manifold. Let $g : \Bbb R \to [0, \infty)$ be given by $g(t) = \sup _{x \in M} f(t,x)$. Is it true that $g$ is smooth? The issue here is the permutation of $\lim$ and $\sup$ - what techniques should I apply? Uniform convergence is out of the question. If instead of $\sup$ I had had an integral, I would have tried to use the dominated convergence theorem, or any other result from this family of theorems, but what to try here? Since $\sup$ is not additive, I cannot view it as a positive linear functional, therefore I cannot use the Riesz-Markov representation theorem on it (which would have given me a positive measure to which the dominated convergence theorem would have applied).","Let $f : \Bbb R \times M \to [0, \infty)$ be smooth, with $M$ a compact smooth manifold. Let $g : \Bbb R \to [0, \infty)$ be given by $g(t) = \sup _{x \in M} f(t,x)$. Is it true that $g$ is smooth? The issue here is the permutation of $\lim$ and $\sup$ - what techniques should I apply? Uniform convergence is out of the question. If instead of $\sup$ I had had an integral, I would have tried to use the dominated convergence theorem, or any other result from this family of theorems, but what to try here? Since $\sup$ is not additive, I cannot view it as a positive linear functional, therefore I cannot use the Riesz-Markov representation theorem on it (which would have given me a positive measure to which the dominated convergence theorem would have applied).",,"['functional-analysis', 'limits', 'derivatives', 'continuity']"
93,Time Derivative Under Integral Sign,Time Derivative Under Integral Sign,,"I've been banging my head recently with a doubt concerning the application of the Leibniz Integral Rule. Suppose you have a function of $x$ and $t$, $\phi (x,t)$, and that you have a certain quantity $L$, which is given by $$ L = \int_{0}^{2\pi}\phi (x,t)dx $$ According to the Leibniz Integral Rule, if I wanted to compute the time derivative of $L$ at some $t$, then $$ \frac{dL}{dt} = \int_{0}^{2\pi}\frac{\partial \phi}{\partial t} dx $$ Thats all nice and good. But now suppose further that $L$ satisfies $$ \frac{dL}{dt} = 0 $$ for all $t$. Is it valid then to say that the integrand must satisfy $$ \frac{\partial \phi}{\partial t} = 0 $$ for all $t$? I'll appreciate any help; I honestly feel like I'm drowning in a glass of water.","I've been banging my head recently with a doubt concerning the application of the Leibniz Integral Rule. Suppose you have a function of $x$ and $t$, $\phi (x,t)$, and that you have a certain quantity $L$, which is given by $$ L = \int_{0}^{2\pi}\phi (x,t)dx $$ According to the Leibniz Integral Rule, if I wanted to compute the time derivative of $L$ at some $t$, then $$ \frac{dL}{dt} = \int_{0}^{2\pi}\frac{\partial \phi}{\partial t} dx $$ Thats all nice and good. But now suppose further that $L$ satisfies $$ \frac{dL}{dt} = 0 $$ for all $t$. Is it valid then to say that the integrand must satisfy $$ \frac{\partial \phi}{\partial t} = 0 $$ for all $t$? I'll appreciate any help; I honestly feel like I'm drowning in a glass of water.",,"['integration', 'derivatives', 'definite-integrals', 'partial-derivative']"
94,Number of tangent lines through a point,Number of tangent lines through a point,,"The problem asks to find find equations for the two lines through the point $(3, 13)$ that are tangent to the parabola $y=6x-x^2$. I'm trying to play with finding slopes and points of tangency but then I asked myself if there are only two tangents through that point or if there are infinitely many and the problem asks to find two of them. I do understand that the in the problem description suggests there are only two but I don't know how to prove it mathematically (if that's true).","The problem asks to find find equations for the two lines through the point $(3, 13)$ that are tangent to the parabola $y=6x-x^2$. I'm trying to play with finding slopes and points of tangency but then I asked myself if there are only two tangents through that point or if there are infinitely many and the problem asks to find two of them. I do understand that the in the problem description suggests there are only two but I don't know how to prove it mathematically (if that's true).",,"['calculus', 'derivatives', 'tangent-line']"
95,Functional Derivative of Fourier Transform,Functional Derivative of Fourier Transform,,"Say I have a functional $F$ which depends on the Fourier transform of $g^{2}(x_{1},x_{2},x_{3})$: $$F[g] = \int \mathcal{F}(g^{2}) d^{3}x$$ The Fourier transform is given by: $$\mathcal{F}(g^{2})=\int g^{2}e^{-i\vec{k}\vec{r}} d^{3}k$$ I was wondering if the following logic was sound and mathematically legal: $$\frac{\delta F}{\delta g} = \frac{\partial \mathcal{F}(g^{2})}{\partial g}$$ by Euler Lagrange Equation $$\frac{\partial \mathcal{F}(g^{2})}{\partial g}=\mathcal{F}(2g)$$ $$\frac{\delta F[g]}{\delta g} = \mathcal{F}(2g)$$","Say I have a functional $F$ which depends on the Fourier transform of $g^{2}(x_{1},x_{2},x_{3})$: $$F[g] = \int \mathcal{F}(g^{2}) d^{3}x$$ The Fourier transform is given by: $$\mathcal{F}(g^{2})=\int g^{2}e^{-i\vec{k}\vec{r}} d^{3}k$$ I was wondering if the following logic was sound and mathematically legal: $$\frac{\delta F}{\delta g} = \frac{\partial \mathcal{F}(g^{2})}{\partial g}$$ by Euler Lagrange Equation $$\frac{\partial \mathcal{F}(g^{2})}{\partial g}=\mathcal{F}(2g)$$ $$\frac{\delta F[g]}{\delta g} = \mathcal{F}(2g)$$",,"['calculus', 'derivatives', 'fourier-analysis', 'calculus-of-variations']"
96,Find the upper and the lower bounds for a definite integral,Find the upper and the lower bounds for a definite integral,,"I have a question like Find a lower bound and an upper bound for the area under the curve by   finding the minimum and maximum values of the integrand on the given   integral: $$ \int_1^6t^2-6t+11 \ dt $$ It asks for two answers; a minimum area and a maximum   area. So, I integrate this; $$ \left(\frac{t^3}{3}-3t^2+11t\right)\Bigg|_1^6 $$ I know I have a minimum at $x = 3$ because; $$ f(t) = t^2-6t+11 \\ f'(t) = 2t-6 = 0 \\ 2(t-3) = 0 \\ t = 3 \\ f(5) = 4 \\ f(1) = -4 \\ $$ Very confused by what is going on when it asks for a maximum area and a minimum area.","I have a question like Find a lower bound and an upper bound for the area under the curve by   finding the minimum and maximum values of the integrand on the given   integral: $$ \int_1^6t^2-6t+11 \ dt $$ It asks for two answers; a minimum area and a maximum   area. So, I integrate this; $$ \left(\frac{t^3}{3}-3t^2+11t\right)\Bigg|_1^6 $$ I know I have a minimum at $x = 3$ because; $$ f(t) = t^2-6t+11 \\ f'(t) = 2t-6 = 0 \\ 2(t-3) = 0 \\ t = 3 \\ f(5) = 4 \\ f(1) = -4 \\ $$ Very confused by what is going on when it asks for a maximum area and a minimum area.",,"['calculus', 'derivatives', 'definite-integrals', 'maxima-minima']"
97,Differentiable inverse of a vector valued function,Differentiable inverse of a vector valued function,,"Let $f:\mathbb R^2\to \mathbb R^2$ be a continuously differentiable function and suppose that there is some $(x_0,y_0)\in \mathbb R^2$ such that $\det[f^{\prime}(x_0,y_0)]=0$ . I have to prove that $f$ can not have a differentiable inverse $f^{-1}:\mathbb R^2\to \mathbb R^2$ . The main issue is that I do not know anything regarding global inverse. I know only that $f$ may be an invertible function even though there exists $(x_0,y_0)\in \mathbb R^2$ such that $\det[f^{\prime}(x_0,y_0)]=0$ . But how to comment on the differentiability of $f^{-1}$ ? Any help is appreciated.",Let be a continuously differentiable function and suppose that there is some such that . I have to prove that can not have a differentiable inverse . The main issue is that I do not know anything regarding global inverse. I know only that may be an invertible function even though there exists such that . But how to comment on the differentiability of ? Any help is appreciated.,"f:\mathbb R^2\to \mathbb R^2 (x_0,y_0)\in \mathbb R^2 \det[f^{\prime}(x_0,y_0)]=0 f f^{-1}:\mathbb R^2\to \mathbb R^2 f (x_0,y_0)\in \mathbb R^2 \det[f^{\prime}(x_0,y_0)]=0 f^{-1}","['real-analysis', 'derivatives', 'inverse-function']"
98,Derivative of $\|X-\alpha Y\|_2$ with respect to $\alpha$.,Derivative of  with respect to .,\|X-\alpha Y\|_2 \alpha,"Let $X$ and $Y$ be operators on a real or complex Hilbert space $\mathcal{H}$ and $f(\alpha) = \|X - \alpha Y\|_2$ where $\alpha$ is real and $\|A\|_2 = \sigma_{\mathsf{max}}(A)$ is the $\ell^2$-induced operator norm. What is $\frac{df}{d\alpha}$? Even if the function is not differentiable everywhere, $f$ is convex in which case a sub-gradient will suffice. Also, if it helps we can assume $X=I$ and $Y$ is positive definite but I'd rather see a more general result. Also considering $f^2$ instead of $f$ is also fine if that helps. Plots of $f$ : I ran two simple numerical examples which might be enlightening. In the following plot 1 , $X = I\in M_{50}(\mathbb{R})$ and $Y = Z^\mathsf{T}Z + I$ where $Z_{ij}\sim\mathcal{N}(0,1)$ is normally distributed. As we can see, the plot seems piecewise linear. In the next plot 2 , we take $X_{ij}\sim\mathcal{N}(0,1) - I \in M_{50}(\mathbb{R})$ and $Y_{ij}\sim\mathcal{N}(0,1)$. Note that neither $X$ nor $Y$ are symmetric. This example looks differentiable and practically quadratic.","Let $X$ and $Y$ be operators on a real or complex Hilbert space $\mathcal{H}$ and $f(\alpha) = \|X - \alpha Y\|_2$ where $\alpha$ is real and $\|A\|_2 = \sigma_{\mathsf{max}}(A)$ is the $\ell^2$-induced operator norm. What is $\frac{df}{d\alpha}$? Even if the function is not differentiable everywhere, $f$ is convex in which case a sub-gradient will suffice. Also, if it helps we can assume $X=I$ and $Y$ is positive definite but I'd rather see a more general result. Also considering $f^2$ instead of $f$ is also fine if that helps. Plots of $f$ : I ran two simple numerical examples which might be enlightening. In the following plot 1 , $X = I\in M_{50}(\mathbb{R})$ and $Y = Z^\mathsf{T}Z + I$ where $Z_{ij}\sim\mathcal{N}(0,1)$ is normally distributed. As we can see, the plot seems piecewise linear. In the next plot 2 , we take $X_{ij}\sim\mathcal{N}(0,1) - I \in M_{50}(\mathbb{R})$ and $Y_{ij}\sim\mathcal{N}(0,1)$. Note that neither $X$ nor $Y$ are symmetric. This example looks differentiable and practically quadratic.",,"['functional-analysis', 'derivatives', 'operator-theory', 'convex-analysis', 'spectral-norm']"
99,Derivative of a function defined by integral,Derivative of a function defined by integral,,This question popped up somewhere on the internet and I thought it was interesting. I attempted to solve it but I don't know if it is correct. Find the derivative of $$F(x)=\int_{\cos{x^3}}^{\int_{1}^{x} {1/(1+t^2)dt}} {\sin{w} dw}$$ $$\begin{align} \implies F(x) & =-\cos{w}]_{\cos{x^3}}^{\arctan{x}-\frac{\pi}{4}}\\  & = -\cos{(\arctan{x}-\frac{\pi}{4})}+\cos{(\cos{x^3})}\\ \implies F'(x) & = \frac{1}{1+x^2} \sin{(\arctan{x}-\frac{\pi}{4}})+3x^2\sin{(x^3)} \sin{(\cos{(x^3)})}\\ \end{align}$$ Is it this simple? Or is there something I should know before solving this that changes the normal differentiation and integration techniques?,This question popped up somewhere on the internet and I thought it was interesting. I attempted to solve it but I don't know if it is correct. Find the derivative of $$F(x)=\int_{\cos{x^3}}^{\int_{1}^{x} {1/(1+t^2)dt}} {\sin{w} dw}$$ $$\begin{align} \implies F(x) & =-\cos{w}]_{\cos{x^3}}^{\arctan{x}-\frac{\pi}{4}}\\  & = -\cos{(\arctan{x}-\frac{\pi}{4})}+\cos{(\cos{x^3})}\\ \implies F'(x) & = \frac{1}{1+x^2} \sin{(\arctan{x}-\frac{\pi}{4}})+3x^2\sin{(x^3)} \sin{(\cos{(x^3)})}\\ \end{align}$$ Is it this simple? Or is there something I should know before solving this that changes the normal differentiation and integration techniques?,,"['calculus', 'integration', 'derivatives']"
