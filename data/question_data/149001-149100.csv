,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Using DeMoivre's Theorem to prove some identities regarding trigonometric functions,Using DeMoivre's Theorem to prove some identities regarding trigonometric functions,,"The following question is from a previous post. The reason that I am posting this again is because it had two questions which were not really related and hence, one of the questions was not answered. The question is from a book, ""Mathematical Analysis - 2nd Edition"" by Apostol. The question has three parts and is as follows: By equating imaginary parts in DeMoivre's formula, prove that $$\sin{\left(n\theta\right)} = \sin^n\theta \left\lbrace \binom{n}{1}\cot^{n - 1}\theta - \binom{n}{3} \cot^{n - 3}\theta + \binom{n}{5} \cot^{n - 5}\theta - + \cdots \right\rbrace$$ If $0 < \theta < \dfrac{\pi}{2}$ , prove that $$\sin{\left(\left( 2m + 1 \right)\theta\right)} = \sin^{2m + 1}\theta P_m(\cot^2\theta)$$ where $P_m$ is the polynomial of degree $m$ given by $$P_m\left( x \right) = \binom{2m + 1}{1}x^m - \binom{2m + 1}{3}x^{m-1} + \binom{2m + 1}{5}x^{m-2} - + \cdots$$ Use this to show that $P_m$ has zeros at $m$ distinct points $x_k = \cot^2\left( \dfrac{\pi k}{2m + 1} \right)$ for $k = 1, 2, \dots, m$ . Show that the sum of zeros of $P_m$ is given by $$\sum\limits_{k=1}^{m} \cot^2\dfrac{\pi k}{2m + 1} = \dfrac{m \left( 2m - 1 \right)}{3}$$ and that the sum of theie squares is given by $$\sum\limits_{k=1}^{m} \cot^4\dfrac{\pi k}{2m + 1} = \dfrac{m \left( 2m - 1 \right) \left( 4m^2 + 10m - 9 \right)}{45}$$ As far as the solution is concerned, I have been able to prove the first part and upto proving the existence of the polynomial in the second part. For later parts, I do not have any insights in proceeding towards the solution. Help will be appreciated!","The following question is from a previous post. The reason that I am posting this again is because it had two questions which were not really related and hence, one of the questions was not answered. The question is from a book, ""Mathematical Analysis - 2nd Edition"" by Apostol. The question has three parts and is as follows: By equating imaginary parts in DeMoivre's formula, prove that If , prove that where is the polynomial of degree given by Use this to show that has zeros at distinct points for . Show that the sum of zeros of is given by and that the sum of theie squares is given by As far as the solution is concerned, I have been able to prove the first part and upto proving the existence of the polynomial in the second part. For later parts, I do not have any insights in proceeding towards the solution. Help will be appreciated!","\sin{\left(n\theta\right)} = \sin^n\theta \left\lbrace \binom{n}{1}\cot^{n - 1}\theta - \binom{n}{3} \cot^{n - 3}\theta + \binom{n}{5} \cot^{n - 5}\theta - + \cdots \right\rbrace 0 < \theta < \dfrac{\pi}{2} \sin{\left(\left( 2m + 1 \right)\theta\right)} = \sin^{2m + 1}\theta P_m(\cot^2\theta) P_m m P_m\left( x \right) = \binom{2m + 1}{1}x^m - \binom{2m + 1}{3}x^{m-1} + \binom{2m + 1}{5}x^{m-2} - + \cdots P_m m x_k = \cot^2\left( \dfrac{\pi k}{2m + 1} \right) k = 1, 2, \dots, m P_m \sum\limits_{k=1}^{m} \cot^2\dfrac{\pi k}{2m + 1} = \dfrac{m \left( 2m - 1 \right)}{3} \sum\limits_{k=1}^{m} \cot^4\dfrac{\pi k}{2m + 1} = \dfrac{m \left( 2m - 1 \right) \left( 4m^2 + 10m - 9 \right)}{45}","['analysis', 'complex-numbers']"
1,"Given $f: \mathbb{R}^5\to \mathbb{R}^2$, of class $C^1$. Let $a= (1,2,-1,3,0)$; suppose that $f(a) = 0$ and $Df(a)=$","Given , of class . Let ; suppose that  and","f: \mathbb{R}^5\to \mathbb{R}^2 C^1 a= (1,2,-1,3,0) f(a) = 0 Df(a)=","Given $f: \mathbb{R}^5\to \mathbb{R}^2$, of class $C^1$. Let $a= (1,2,-1,3,0)$; suppose that $f(a) = 0$ and $Df(a)=\begin{bmatrix}1 & 3 & 1 & -1 & 2\\ 0 & 0 & 1 & 2 & -4\end{bmatrix}$ (a) Show there is a function $g : B\to \mathbb{R}^2$ of class $C^1$ defined on an open set $B$ of $\mathbb{R}^3$ such that $f(x_1, g_1(x), g_2(x), x_2, x_3)=0$ for $x=(x_1, x_2, x_3)\in B$, and $g(1, 3, 0)=(2, -1)$. (b) Find $Dg(1, 3, 0)$ (c) Discuss the problem of solving the equation $f(x) = 0$ for an arbitrary pair of the unknowns in terms of the others, near the point $a$. For (a), I know that $\frac{\partial f}{\partial x}(a)=\begin{bmatrix}1 & -1 & 2\\ 0 & 2 & -4\end{bmatrix}$ and $\frac{\partial f}{\partial y}(a)=\begin{bmatrix}3 & 1\\ 0 & 1\end{bmatrix}$ and as $\det \frac{\partial f}{\partial y}(a)=3\neq 0$, then I can apply the implicit function theorem to find an open $B\subset \mathbb{R}^3$ and a continuous function $g: B\to \mathbb{R}^2$ such that $f(x_1, g_1(x), g_2(x), x_2, x_3)=0$ for $x=(x_1, x_2, x_3)\in B$, and $g(1, 3, 0)=(2, -1)$. (b) I know that $Dg(x)=-[\frac{\partial f}{\partial y}(x, g(x))]^{-1}\frac{\partial f}{\partial x}(x, g(x))$, with which $Dg(1,3,0)=-[\frac{\partial f}{\partial y}(1,2,-1,3,0)]^{-1}\frac{\partial f}{\partial x}(1,2,-1,3,0)=-\begin{bmatrix}3 & 1\\ 0 & 1\end{bmatrix}^{-1}\begin{bmatrix}1 & -1 & 2\\ 0 & 2 & -4\end{bmatrix}=\begin{bmatrix}-1/3 & 1 & 2\\ 0 & -2 & 4\end{bmatrix}$ (c) What this says is that I can clear $g_1(x)$ and $g_2(x)$ in terms of $x_1, x_1, x_2$ and $x_3$? Are you all the right arguments? Thank you very much.","Given $f: \mathbb{R}^5\to \mathbb{R}^2$, of class $C^1$. Let $a= (1,2,-1,3,0)$; suppose that $f(a) = 0$ and $Df(a)=\begin{bmatrix}1 & 3 & 1 & -1 & 2\\ 0 & 0 & 1 & 2 & -4\end{bmatrix}$ (a) Show there is a function $g : B\to \mathbb{R}^2$ of class $C^1$ defined on an open set $B$ of $\mathbb{R}^3$ such that $f(x_1, g_1(x), g_2(x), x_2, x_3)=0$ for $x=(x_1, x_2, x_3)\in B$, and $g(1, 3, 0)=(2, -1)$. (b) Find $Dg(1, 3, 0)$ (c) Discuss the problem of solving the equation $f(x) = 0$ for an arbitrary pair of the unknowns in terms of the others, near the point $a$. For (a), I know that $\frac{\partial f}{\partial x}(a)=\begin{bmatrix}1 & -1 & 2\\ 0 & 2 & -4\end{bmatrix}$ and $\frac{\partial f}{\partial y}(a)=\begin{bmatrix}3 & 1\\ 0 & 1\end{bmatrix}$ and as $\det \frac{\partial f}{\partial y}(a)=3\neq 0$, then I can apply the implicit function theorem to find an open $B\subset \mathbb{R}^3$ and a continuous function $g: B\to \mathbb{R}^2$ such that $f(x_1, g_1(x), g_2(x), x_2, x_3)=0$ for $x=(x_1, x_2, x_3)\in B$, and $g(1, 3, 0)=(2, -1)$. (b) I know that $Dg(x)=-[\frac{\partial f}{\partial y}(x, g(x))]^{-1}\frac{\partial f}{\partial x}(x, g(x))$, with which $Dg(1,3,0)=-[\frac{\partial f}{\partial y}(1,2,-1,3,0)]^{-1}\frac{\partial f}{\partial x}(1,2,-1,3,0)=-\begin{bmatrix}3 & 1\\ 0 & 1\end{bmatrix}^{-1}\begin{bmatrix}1 & -1 & 2\\ 0 & 2 & -4\end{bmatrix}=\begin{bmatrix}-1/3 & 1 & 2\\ 0 & -2 & 4\end{bmatrix}$ (c) What this says is that I can clear $g_1(x)$ and $g_2(x)$ in terms of $x_1, x_1, x_2$ and $x_3$? Are you all the right arguments? Thank you very much.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
2,"Pointwise and uniform convergence of $\sum_{n=0}^\infty \frac{x^n}{e^{nx}}, n \in \mathbb{N}$",Pointwise and uniform convergence of,"\sum_{n=0}^\infty \frac{x^n}{e^{nx}}, n \in \mathbb{N}","$$\sum_{n=0}^\infty \frac{x^n}{e^{nx}}, n \in \mathbb{N}, D=[0, +\infty) \rightarrow \mathbb{R}$$ I tried using Weierstrass M-test $\frac{x^n}{e^{nx}} =({\frac{x}{e^{x}}})^n \le (\frac{1}{e})^n$ and $|\frac{1}{e}|\lt1$, so I thought because $(\frac{1}{e})^n$ converges (geometric series), that $\sum_{n=0}^\infty \frac{x^n}{e^{nx}}$ converges absolutely and uniformly and therefore pointwise. Is that correct and if not, where are my mystakes?","$$\sum_{n=0}^\infty \frac{x^n}{e^{nx}}, n \in \mathbb{N}, D=[0, +\infty) \rightarrow \mathbb{R}$$ I tried using Weierstrass M-test $\frac{x^n}{e^{nx}} =({\frac{x}{e^{x}}})^n \le (\frac{1}{e})^n$ and $|\frac{1}{e}|\lt1$, so I thought because $(\frac{1}{e})^n$ converges (geometric series), that $\sum_{n=0}^\infty \frac{x^n}{e^{nx}}$ converges absolutely and uniformly and therefore pointwise. Is that correct and if not, where are my mystakes?",,"['calculus', 'sequences-and-series', 'analysis']"
3,"Proof of theorem 7.7.1. in Lars Hormander's ""The Analysis of Linear Partial Differential Operators I""","Proof of theorem 7.7.1. in Lars Hormander's ""The Analysis of Linear Partial Differential Operators I""",,"I have been trying to understand this proof for a while but I just can't follow what the author writes. If anyone can either explain Hormander's proof or has an alternate proof I would highly appreciate it. THEOREM 7.7.1. Let $K\subset\mathbb{R}^n$ be a compact set, $X$ an open neighborhood containing $K$ and $j, k$ non-negative integers. If $u\in C^{k}_c(K), f\in C^{k+1}(X)$ and $\Im[f]\geq 0$ in $X$ then for all $w > 0$,   $$ w^{j+k}\left\lvert \int u(x)\left(\Im[f(x)]\right)^j e^{iwi(x)} dx \right\rvert \leq C \sum_{\lvert\alpha\rvert \leq k}\sup\lvert D^\alpha u\rvert \left(\lvert f^\prime\rvert^2 + \Im[f]\right)^{\lvert{\alpha}/2-k} $$ Proof available here In the statement of the theorem the author also mentions that ""$C$ is bounded when $f$ stays in a bounded set in $C^{k+1}(X)$"". I am not sure what he means by that. What exactly does the constant $C$ depend on? The proof makes sense to me until equation (7.7.3). Note that the supremum is outside of the sum, so how did the author use the induction hypothesis? I also don't see how equation (7.7.4) follows from the lemma. On the top of page 218, how is the inequality $$ N\lvert{u_\nu}\rvert_\mu \leq C\left(\lvert{N}_1\lvert{u_\nu}\rvert_{\mu-1}\dots \lvert{u_\nu}\rvert_0\right) $$ obtained? Finally, how does (7.7.6) follow from (7.7.5)?","I have been trying to understand this proof for a while but I just can't follow what the author writes. If anyone can either explain Hormander's proof or has an alternate proof I would highly appreciate it. THEOREM 7.7.1. Let $K\subset\mathbb{R}^n$ be a compact set, $X$ an open neighborhood containing $K$ and $j, k$ non-negative integers. If $u\in C^{k}_c(K), f\in C^{k+1}(X)$ and $\Im[f]\geq 0$ in $X$ then for all $w > 0$,   $$ w^{j+k}\left\lvert \int u(x)\left(\Im[f(x)]\right)^j e^{iwi(x)} dx \right\rvert \leq C \sum_{\lvert\alpha\rvert \leq k}\sup\lvert D^\alpha u\rvert \left(\lvert f^\prime\rvert^2 + \Im[f]\right)^{\lvert{\alpha}/2-k} $$ Proof available here In the statement of the theorem the author also mentions that ""$C$ is bounded when $f$ stays in a bounded set in $C^{k+1}(X)$"". I am not sure what he means by that. What exactly does the constant $C$ depend on? The proof makes sense to me until equation (7.7.3). Note that the supremum is outside of the sum, so how did the author use the induction hypothesis? I also don't see how equation (7.7.4) follows from the lemma. On the top of page 218, how is the inequality $$ N\lvert{u_\nu}\rvert_\mu \leq C\left(\lvert{N}_1\lvert{u_\nu}\rvert_{\mu-1}\dots \lvert{u_\nu}\rvert_0\right) $$ obtained? Finally, how does (7.7.6) follow from (7.7.5)?",,"['real-analysis', 'analysis', 'partial-differential-equations', 'asymptotics', 'proof-explanation']"
4,Show convergence/divergence of series,Show convergence/divergence of series,,"We have that $(a_k)$ and $(b_k)$ are two sequences of positive numbers. I want to show the following: If $\lim_{k\rightarrow\infty}\frac{a_k}{ b_k}= c > 0$, then both $\sum_{k=1}^{\infty}a_k$ and $\sum_{k=1}^{\infty}b_k$ converge  or both diverge. If $\frac{a_k}{ b_k}\geq \frac{a_{k+1}}{ b_{k+1}}$ for almost each $k$, then from the convergence of $\sum_{k=1}^{\infty}b_k$ we get the convergence of $\sum_{k=1}^{\infty}a_k$ and from the divergence of $\sum_{k=1}^{\infty}a_k$ we get the divergence of $\sum_{k=1}^{\infty}b_k$. $$$$ Could you give me a hint how we could show that? Do we have to apply a convergence test? We have that $$\frac{a_k}{ b_k}\geq \frac{a_{k+1}}{ b_{k+1}}\Rightarrow \frac{b_{k+1}}{ b_k}\geq \frac{a_{k+1}}{ a_k}$$ Do we apply here the ration and the direct comparison test?","We have that $(a_k)$ and $(b_k)$ are two sequences of positive numbers. I want to show the following: If $\lim_{k\rightarrow\infty}\frac{a_k}{ b_k}= c > 0$, then both $\sum_{k=1}^{\infty}a_k$ and $\sum_{k=1}^{\infty}b_k$ converge  or both diverge. If $\frac{a_k}{ b_k}\geq \frac{a_{k+1}}{ b_{k+1}}$ for almost each $k$, then from the convergence of $\sum_{k=1}^{\infty}b_k$ we get the convergence of $\sum_{k=1}^{\infty}a_k$ and from the divergence of $\sum_{k=1}^{\infty}a_k$ we get the divergence of $\sum_{k=1}^{\infty}b_k$. $$$$ Could you give me a hint how we could show that? Do we have to apply a convergence test? We have that $$\frac{a_k}{ b_k}\geq \frac{a_{k+1}}{ b_{k+1}}\Rightarrow \frac{b_{k+1}}{ b_k}\geq \frac{a_{k+1}}{ a_k}$$ Do we apply here the ration and the direct comparison test?",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'divergent-series']"
5,Bounded variation of $\frac1f$ when $\inf(|f|)>0$ & $f$ bounded variation,Bounded variation of  when  &  bounded variation,\frac1f \inf(|f|)>0 f,"I want to show if  $\frac{1}{f}\in BV[a,b]$ when $\inf(|f|)>0 \land  f\in BV[a,b]$. I tried to find a partition that $V(\frac{1}{f},P)$ is upper-bounded using the partition that makes $V(f,P)$ upper-bounded in which I failed. (in case $f$ is not continuous which is countable..) Showing by subtraction of two increasing functions also seems hard. Hope if someone can help me or give me a hint. Thank you.","I want to show if  $\frac{1}{f}\in BV[a,b]$ when $\inf(|f|)>0 \land  f\in BV[a,b]$. I tried to find a partition that $V(\frac{1}{f},P)$ is upper-bounded using the partition that makes $V(f,P)$ upper-bounded in which I failed. (in case $f$ is not continuous which is countable..) Showing by subtraction of two increasing functions also seems hard. Hope if someone can help me or give me a hint. Thank you.",,"['real-analysis', 'analysis', 'bounded-variation', 'partitions-for-integration']"
6,Interchange of integral and derivative,Interchange of integral and derivative,,"I want to show for all $t\in \mathbb R$:  $$\frac{d}{dt}\int_{\mathbb R} e^{-x^4 + tx^2}dx = \int_{\mathbb R}x^2e^{-x^4+tx^2}dx$$ Now once we justified the interchange of the differential operator and the integral it's easy to see. Obviously $x \mapsto e^{-x^4 + tx^2} $ is differentiable for all $t\in \mathbb R$. But I can't seem to find a suitable dominating integrable function $f(x)$ satisfying $$\forall t\in \mathbb R: \left \lvert \frac{\partial}{\partial t} e^{-x^4 + tx^2}\right \rvert = \left \lvert x^2 e^{-x^4 + tx^2}\right \rvert \leq f(x)$$ I assume we can split this into compact intervals and get an estimate for each fixed $t_0 \in \mathbb R$, but then our $f(x)$ depends on $t$. Any hints?","I want to show for all $t\in \mathbb R$:  $$\frac{d}{dt}\int_{\mathbb R} e^{-x^4 + tx^2}dx = \int_{\mathbb R}x^2e^{-x^4+tx^2}dx$$ Now once we justified the interchange of the differential operator and the integral it's easy to see. Obviously $x \mapsto e^{-x^4 + tx^2} $ is differentiable for all $t\in \mathbb R$. But I can't seem to find a suitable dominating integrable function $f(x)$ satisfying $$\forall t\in \mathbb R: \left \lvert \frac{\partial}{\partial t} e^{-x^4 + tx^2}\right \rvert = \left \lvert x^2 e^{-x^4 + tx^2}\right \rvert \leq f(x)$$ I assume we can split this into compact intervals and get an estimate for each fixed $t_0 \in \mathbb R$, but then our $f(x)$ depends on $t$. Any hints?",,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
7,"How to prove continuity of a concave, non-decreasing $f : [0, 1] \to [0, 1]$.","How to prove continuity of a concave, non-decreasing .","f : [0, 1] \to [0, 1]","Let $f : [0, 1] \to [0, 1]$ be non-decreasing, $f(0) = 0$ and $f(1) = 1$. In a paper which I have read, concavity for this $f$ is defined as follows: $f$ is called concave, if for each $q \in (0,1]$, there are reals $a_q, b_q$ and a line $l_q(x) = a_q x + b_q$ such that $l_q(q) = f(q)$ and $l_q(p) \ge f(p)$ for all $p \in (0, 1]$. Such an $f$ is called convex, if $-f$ is concave. Now there are made three statements, and I do not quite see how to prove them with the given definition. Let $f : [0, 1] \to [0, 1]$ be non-decreasing. If $f$ is concave, then $f$ is continuous on $(0, 1]$ and a jump of f can only occur at $0$. If $f$ is convex, then $f$ is continuous on $[0, 1)$ and a jump of $f$ can only occur at $1$. If $f$ is concave or convex and does not have a jump, then $f$ is absolutely continuous. I have managed to prove in (1) that $f$ is right-continuous, but I still need to prove the left-continuity and struggle with that. Does (2) follow directly from statement (1) as $-f$ is concave? About (3), I could already prove for the case that $f$ is concave, $f$ is absolutely continuous on $(0, 1]$ but I do not know how to show that this is the case for the entire interval $[0, 1]$. I suppose that in the scenario that $f$ is convex one uses the statement for $-f$. Any help is appreciated. Thanks!","Let $f : [0, 1] \to [0, 1]$ be non-decreasing, $f(0) = 0$ and $f(1) = 1$. In a paper which I have read, concavity for this $f$ is defined as follows: $f$ is called concave, if for each $q \in (0,1]$, there are reals $a_q, b_q$ and a line $l_q(x) = a_q x + b_q$ such that $l_q(q) = f(q)$ and $l_q(p) \ge f(p)$ for all $p \in (0, 1]$. Such an $f$ is called convex, if $-f$ is concave. Now there are made three statements, and I do not quite see how to prove them with the given definition. Let $f : [0, 1] \to [0, 1]$ be non-decreasing. If $f$ is concave, then $f$ is continuous on $(0, 1]$ and a jump of f can only occur at $0$. If $f$ is convex, then $f$ is continuous on $[0, 1)$ and a jump of $f$ can only occur at $1$. If $f$ is concave or convex and does not have a jump, then $f$ is absolutely continuous. I have managed to prove in (1) that $f$ is right-continuous, but I still need to prove the left-continuity and struggle with that. Does (2) follow directly from statement (1) as $-f$ is concave? About (3), I could already prove for the case that $f$ is concave, $f$ is absolutely continuous on $(0, 1]$ but I do not know how to show that this is the case for the entire interval $[0, 1]$. I suppose that in the scenario that $f$ is convex one uses the statement for $-f$. Any help is appreciated. Thanks!",,"['real-analysis', 'analysis', 'continuity', 'convex-analysis']"
8,"Why is $\lim_{n\to\infty} 2^n \Psi\left(\frac{r}{2^n}\right)=0$, for some specific function $\Psi$ defined in the question?","Why is , for some specific function  defined in the question?",\lim_{n\to\infty} 2^n \Psi\left(\frac{r}{2^n}\right)=0 \Psi,"This comes from the proof of the first theorem in this blog article. The paths $x:[0,T]\to\mathbb{R}^d$ and $y:[0,T]\to\mathbb{R}^{e\times d}$ are of bounded total variation. The real numbers $p,q>1$ are such that $\frac{1}{p}+\frac{1}{q}>1$. Finally, $0\leq s\le t\leq T$. We have the following definitions: $$\theta := \frac{1}{p}+\frac{1}{q}.$$ $$\omega(s,t):=\|x\|_{p\text{-var}[s,t]}^{1/\theta}\|y\|_{q\text{-var}[s,t]}^{1/\theta}.$$ $$\omega_{\epsilon}(s,t) := \omega(s,t)+\epsilon\left(\|x\|_{1\text{-var}[s,t]}+\|y\|_{1\text{-var}[s,t]}\right).$$ $$\Gamma_{s,t}=\int_s^t(y(u)-y(s))\mathrm{d}x(u).$$ $$\Psi(r) := \sup_{s,u,\omega_{\epsilon}(s,u)\leq r}\|\Gamma_{s,u}\|.$$ It is then claimed that $$2^n \Psi\left(\frac{r}{2^n}\right)\xrightarrow{n\to\infty}0.$$ I can see that for any fixed $r$, $\Psi\left(\frac{r}{2^n}\right)$ goes to $0$ as $n$ goes to infinity, as $\omega$ is continuous and $\omega(s,s)=0$. But $2^n$ goes to infinity, so how do we know that $\Psi\left(\frac{r}{2^n}\right)$ goes to zero faster than $2^n$ goes to infinity?","This comes from the proof of the first theorem in this blog article. The paths $x:[0,T]\to\mathbb{R}^d$ and $y:[0,T]\to\mathbb{R}^{e\times d}$ are of bounded total variation. The real numbers $p,q>1$ are such that $\frac{1}{p}+\frac{1}{q}>1$. Finally, $0\leq s\le t\leq T$. We have the following definitions: $$\theta := \frac{1}{p}+\frac{1}{q}.$$ $$\omega(s,t):=\|x\|_{p\text{-var}[s,t]}^{1/\theta}\|y\|_{q\text{-var}[s,t]}^{1/\theta}.$$ $$\omega_{\epsilon}(s,t) := \omega(s,t)+\epsilon\left(\|x\|_{1\text{-var}[s,t]}+\|y\|_{1\text{-var}[s,t]}\right).$$ $$\Gamma_{s,t}=\int_s^t(y(u)-y(s))\mathrm{d}x(u).$$ $$\Psi(r) := \sup_{s,u,\omega_{\epsilon}(s,u)\leq r}\|\Gamma_{s,u}\|.$$ It is then claimed that $$2^n \Psi\left(\frac{r}{2^n}\right)\xrightarrow{n\to\infty}0.$$ I can see that for any fixed $r$, $\Psi\left(\frac{r}{2^n}\right)$ goes to $0$ as $n$ goes to infinity, as $\omega$ is continuous and $\omega(s,s)=0$. But $2^n$ goes to infinity, so how do we know that $\Psi\left(\frac{r}{2^n}\right)$ goes to zero faster than $2^n$ goes to infinity?",,"['real-analysis', 'analysis', 'bounded-variation', 'p-variation']"
9,A smooth path passing through infinitely many points of a given sequence,A smooth path passing through infinitely many points of a given sequence,,"While doing my research I came across an interesting question.  Let $\{\bf{x}_n \}\subset R^m$ be any sequence of points with $\bf{x}_n\rightarrow \bf{0}$, as $n \rightarrow \infty$. Is there a smooth path $c: [0,a) \rightarrow \mathbf{R}^m$, $a>0$, $c(0)=\bf{0}$, a (decreasing) sequence $\{t_k\}\subset (0,a)$, an (infinite) subsequence $\{\bf{x}_{n_k} \}\subset \{\bf{x}_n \}$ so that $c(t_k)=\bf{x}_{n_k}$? Can we make the path analytic at zero? Any help, or suggestion would be appreciated.","While doing my research I came across an interesting question.  Let $\{\bf{x}_n \}\subset R^m$ be any sequence of points with $\bf{x}_n\rightarrow \bf{0}$, as $n \rightarrow \infty$. Is there a smooth path $c: [0,a) \rightarrow \mathbf{R}^m$, $a>0$, $c(0)=\bf{0}$, a (decreasing) sequence $\{t_k\}\subset (0,a)$, an (infinite) subsequence $\{\bf{x}_{n_k} \}\subset \{\bf{x}_n \}$ so that $c(t_k)=\bf{x}_{n_k}$? Can we make the path analytic at zero? Any help, or suggestion would be appreciated.",,['analysis']
10,evaluate limit of a sequence,evaluate limit of a sequence,,"The problem is: Prove the convergence of the sequence $\sqrt7,\; \sqrt{7-\sqrt7}, \; \sqrt{7-\sqrt{7+\sqrt7}},\; \sqrt{7-\sqrt{7+\sqrt{7-\sqrt7}}}$, .... AND evaluate its limit. If the convergen is proved, I can evaluate the limit by the recurrence relation $a_{n+2} = \sqrt{7-\sqrt{7+a_n}}$. A quickly find solution to this quartic equation is 2; and other roots (if I find them all) can be disposed (since they are either too large or negative). But this method presupposes that I can find all roots of a quartic equation. Can I have other method that bypasses this? For example can I find another recurrence relation such that I dont have to solve a quartic (or cubic) equation? or at least a quintic equation that involvs only quadratic terms (thus can be reduced to quadratic equation)? If these attempts are futile, I shall happily take my above mathod as an answer.","The problem is: Prove the convergence of the sequence $\sqrt7,\; \sqrt{7-\sqrt7}, \; \sqrt{7-\sqrt{7+\sqrt7}},\; \sqrt{7-\sqrt{7+\sqrt{7-\sqrt7}}}$, .... AND evaluate its limit. If the convergen is proved, I can evaluate the limit by the recurrence relation $a_{n+2} = \sqrt{7-\sqrt{7+a_n}}$. A quickly find solution to this quartic equation is 2; and other roots (if I find them all) can be disposed (since they are either too large or negative). But this method presupposes that I can find all roots of a quartic equation. Can I have other method that bypasses this? For example can I find another recurrence relation such that I dont have to solve a quartic (or cubic) equation? or at least a quintic equation that involvs only quadratic terms (thus can be reduced to quadratic equation)? If these attempts are futile, I shall happily take my above mathod as an answer.",,"['sequences-and-series', 'analysis']"
11,Dominated convergence theorem for absolutely continuous function,Dominated convergence theorem for absolutely continuous function,,"Let $f$ be an absolutely continuous function with $f' \in L^1$. Is it then true that $\lim_{h \downarrow 0}\int_0^t \frac{f(x+h)-f(x)}{h}dx= \int_0^t f'(s) ds$? Here $t>0$ is finite. What is problematic for me is to find an upper bound for the integrand. Obviously the right side exists, but is there a way to justify this?","Let $f$ be an absolutely continuous function with $f' \in L^1$. Is it then true that $\lim_{h \downarrow 0}\int_0^t \frac{f(x+h)-f(x)}{h}dx= \int_0^t f'(s) ds$? Here $t>0$ is finite. What is problematic for me is to find an upper bound for the integrand. Obviously the right side exists, but is there a way to justify this?",,"['real-analysis', 'analysis']"
12,a little problem in the proof of Peter-Weyl theorem,a little problem in the proof of Peter-Weyl theorem,,"I am trying to understand the proof of Peter-Weyl theorem which has the following form. $Theorem.$ If $G$ is a compact group, then the linear span of all matrix coefficients for all finite-dimensional irreducible unitary representations of $G$ is dense in $L^2(G)$. Let $U$ be the closure in $L^2(G)$ of the linear span of all matrix coefficients of all finite-dimensional irreducible unitary representations of $G$.  Arguing by contradiction, suppose $U\neq L^2(G)$. Then $U^\perp\neq 0$. Note that if $h(x)=\langle R(x)u,v\rangle$ is a matrix coefficient in $U$, then the following functions of $x$ are also matrix coefficients for the same representation $R$: \begin{align*}         \overline{h(x^{-1})}&=\overline{\langle R(x^{-1})u,v\rangle}=\langle v,R(x^{-1})u\rangle=\langle R(x)v,u\rangle \\         h(gx)&=\langle R(gx)u,v\rangle=\langle R(x)u,R(g^{-1})v\rangle \\         h(xg)&=\langle R(xg)u,v\rangle=\langle R(x)R(g)u,v\rangle  \end{align*}  How can we conclude that for any $h\in U^\perp$, the function $h'$ defined by $h'(x):=h(y^{-1}x)$ for some $y\in G$ is in $U^\perp$ ? Thanks. $My \ attempt.$ If $h=0$ then there is nothing to do. So let $h\in U^\perp \setminus \{0\}.$ Suppose $h'\notin U^\perp$. Let $\mathcal{M}$ be a basis  for $U$ consisting of some matrix coefficients in $U$. Then we may write $$h'=g+\sum_{i=1}^n a_if_i$$ for some $f_1,...,f_n\in \mathcal{M}$ and $g\in U^\perp$ and for some scalars $a_1,...,a_n$. Note that at least one of these scalars is nonzero. Now we  have for all $x\in G$ \begin{equation} h'(x)=h(y^{-1}x)=g(x)+\sum_{i=1}^n a_i f_i(x)\qquad (*) \end{equation} For each $i$, put $f_i(x)=\langle R_i(x)u_i,v_i\rangle$  where $R_i$ is the corresponding representation of $G$ for $f_i$ and also $u_i$ and $v_i$ are corresponding vectors for $f_i$. By the unitarity, we have  $$f_i(x)=\langle R_i(y^{-1}x)u_i,R_i(y^{-1})v_i\rangle$$  So for any $t\in G$, the equation ($*$) becomes  $$h'(yt)=h(t)=g_1(t)+\sum_{i=1}^n a_if'_i(t)$$ where  $g_1(t):=g(yt)$ and $f'_i(t):=\langle R_i(t)u_i,R_i(y^{-1})v_i\rangle$. Notice that the set $\{f'_i:i=1,...n\}$ is also linearly independent. For simplicity, we write  $$h=g_1+\sum_{i=1}^n a_if'_i$$ where $g_1=r+s$ for some $r\in U\setminus\{0\},s\in U^\perp\setminus\{0\}$. Then we get  $$h-s=r+\sum_{i=1}^na_if'_i\in U\cap U^\perp =\{0\}$$ I couldn't get any contradiction from here. Could anyone help me? Thanks.","I am trying to understand the proof of Peter-Weyl theorem which has the following form. $Theorem.$ If $G$ is a compact group, then the linear span of all matrix coefficients for all finite-dimensional irreducible unitary representations of $G$ is dense in $L^2(G)$. Let $U$ be the closure in $L^2(G)$ of the linear span of all matrix coefficients of all finite-dimensional irreducible unitary representations of $G$.  Arguing by contradiction, suppose $U\neq L^2(G)$. Then $U^\perp\neq 0$. Note that if $h(x)=\langle R(x)u,v\rangle$ is a matrix coefficient in $U$, then the following functions of $x$ are also matrix coefficients for the same representation $R$: \begin{align*}         \overline{h(x^{-1})}&=\overline{\langle R(x^{-1})u,v\rangle}=\langle v,R(x^{-1})u\rangle=\langle R(x)v,u\rangle \\         h(gx)&=\langle R(gx)u,v\rangle=\langle R(x)u,R(g^{-1})v\rangle \\         h(xg)&=\langle R(xg)u,v\rangle=\langle R(x)R(g)u,v\rangle  \end{align*}  How can we conclude that for any $h\in U^\perp$, the function $h'$ defined by $h'(x):=h(y^{-1}x)$ for some $y\in G$ is in $U^\perp$ ? Thanks. $My \ attempt.$ If $h=0$ then there is nothing to do. So let $h\in U^\perp \setminus \{0\}.$ Suppose $h'\notin U^\perp$. Let $\mathcal{M}$ be a basis  for $U$ consisting of some matrix coefficients in $U$. Then we may write $$h'=g+\sum_{i=1}^n a_if_i$$ for some $f_1,...,f_n\in \mathcal{M}$ and $g\in U^\perp$ and for some scalars $a_1,...,a_n$. Note that at least one of these scalars is nonzero. Now we  have for all $x\in G$ \begin{equation} h'(x)=h(y^{-1}x)=g(x)+\sum_{i=1}^n a_i f_i(x)\qquad (*) \end{equation} For each $i$, put $f_i(x)=\langle R_i(x)u_i,v_i\rangle$  where $R_i$ is the corresponding representation of $G$ for $f_i$ and also $u_i$ and $v_i$ are corresponding vectors for $f_i$. By the unitarity, we have  $$f_i(x)=\langle R_i(y^{-1}x)u_i,R_i(y^{-1})v_i\rangle$$  So for any $t\in G$, the equation ($*$) becomes  $$h'(yt)=h(t)=g_1(t)+\sum_{i=1}^n a_if'_i(t)$$ where  $g_1(t):=g(yt)$ and $f'_i(t):=\langle R_i(t)u_i,R_i(y^{-1})v_i\rangle$. Notice that the set $\{f'_i:i=1,...n\}$ is also linearly independent. For simplicity, we write  $$h=g_1+\sum_{i=1}^n a_if'_i$$ where $g_1=r+s$ for some $r\in U\setminus\{0\},s\in U^\perp\setminus\{0\}$. Then we get  $$h-s=r+\sum_{i=1}^na_if'_i\in U\cap U^\perp =\{0\}$$ I couldn't get any contradiction from here. Could anyone help me? Thanks.",,"['analysis', 'representation-theory']"
13,How to show Rademacher functions are independent.,How to show Rademacher functions are independent.,,"Rademacher functions are defined on [0,1] in the following way. Let $x= 0.d_1 d_2 d_3...$ , the binary expansion of x in [0,1], where the expansion is non terminating (so 1= 0.111111111.... and not 1). Then the kth radamacher functions is defined as $R_k(x)= -1$ if $d_k=0$ and $R_k(x)=1$ if $d_k=1$ . The measure used is borel measure on [0,1]. We say $f$ and $g$ are independent if $m(f^{-1}(A) \cap g^{-1}(B))= m(f^{-1}(A)) m(g^{-1}(B))$ for all borel sets A,B. I have tried to show any two Rademacher functions are independent but have not succeeded. I thought of splitting the cases where each function equals 1 and -1 and so on but it gets more and more complicated. Can anyone help me out? Thanks.","Rademacher functions are defined on [0,1] in the following way. Let , the binary expansion of x in [0,1], where the expansion is non terminating (so 1= 0.111111111.... and not 1). Then the kth radamacher functions is defined as if and if . The measure used is borel measure on [0,1]. We say and are independent if for all borel sets A,B. I have tried to show any two Rademacher functions are independent but have not succeeded. I thought of splitting the cases where each function equals 1 and -1 and so on but it gets more and more complicated. Can anyone help me out? Thanks.",x= 0.d_1 d_2 d_3... R_k(x)= -1 d_k=0 R_k(x)=1 d_k=1 f g m(f^{-1}(A) \cap g^{-1}(B))= m(f^{-1}(A)) m(g^{-1}(B)),"['analysis', 'measure-theory']"
14,When are the sup and euclidean metric interchangeable in analysis on $\mathbb{R}^{n}$,When are the sup and euclidean metric interchangeable in analysis on,\mathbb{R}^{n},"In Analysis on Manifolds, Munkres makes the statement that for most purposes, the sup metric, which is $$max\{|x_{i} - y_{i}| \space i \in \{1 \dots n\}\}$$ and the euclidean metric are ""equivalent."" In other words, they can be interchanged in theorems like this: I cannot find an instance when interchanging the metrics in a theorem is unacceptable -- I suspect that this has to do with the fact that one can place, within any neighborhood of one metric in $\mathbb{R^{n}}$, another neighborhood of the second metric. Is my hunch correct -- that since neighborhoods formed using either metric can be placed within each other, all theorems involving a metric can be formulated using either metric? At least with respect to analysis on $\mathbb{R^{n}}$?","In Analysis on Manifolds, Munkres makes the statement that for most purposes, the sup metric, which is $$max\{|x_{i} - y_{i}| \space i \in \{1 \dots n\}\}$$ and the euclidean metric are ""equivalent."" In other words, they can be interchanged in theorems like this: I cannot find an instance when interchanging the metrics in a theorem is unacceptable -- I suspect that this has to do with the fact that one can place, within any neighborhood of one metric in $\mathbb{R^{n}}$, another neighborhood of the second metric. Is my hunch correct -- that since neighborhoods formed using either metric can be placed within each other, all theorems involving a metric can be formulated using either metric? At least with respect to analysis on $\mathbb{R^{n}}$?",,"['real-analysis', 'analysis', 'metric-spaces', 'supremum-and-infimum', 'equivalent-metrics']"
15,Solve $(y* \sin)(t)=t^2$,Solve,(y* \sin)(t)=t^2,Solve $(y* \sin)(t)=t^2$ What I did: We know that $$\mathscr{L} \{ y(t) \}=Y(s)\quad\mathscr{L} \{ \sin(t) \}=\frac{1}{s^2+1} \quad\mathscr{L} \{ t^2 \}=\frac{1}{s^3} \quad\mathscr{L} \{ f(t)*g(t) \}=F(s) \ G(s)$$ hence $$Y(s)=\frac{s^2+1}{s^3}=\frac{A}{s^3}+\frac{B}{s^2}+\frac{C}{s}$$ with $$A=\lim_{s\rightarrow 0}\ s^2+1=1$$ $$B=\lim_{s\rightarrow 0} \ D(s^2+1)=\lim_{s\rightarrow 0} \ 2s=0$$ $$C=\frac{1}{2} \lim_{s\rightarrow 0} \ D^2(s^2+1)=\frac{1}{2} \lim_{s\rightarrow 0} \ 2=1$$ hence $$Y(s)=\frac{1}{s^3}+\frac{1}{s}$$ and $$y(t)=t^2+1$$ Is this correct?,Solve $(y* \sin)(t)=t^2$ What I did: We know that $$\mathscr{L} \{ y(t) \}=Y(s)\quad\mathscr{L} \{ \sin(t) \}=\frac{1}{s^2+1} \quad\mathscr{L} \{ t^2 \}=\frac{1}{s^3} \quad\mathscr{L} \{ f(t)*g(t) \}=F(s) \ G(s)$$ hence $$Y(s)=\frac{s^2+1}{s^3}=\frac{A}{s^3}+\frac{B}{s^2}+\frac{C}{s}$$ with $$A=\lim_{s\rightarrow 0}\ s^2+1=1$$ $$B=\lim_{s\rightarrow 0} \ D(s^2+1)=\lim_{s\rightarrow 0} \ 2s=0$$ $$C=\frac{1}{2} \lim_{s\rightarrow 0} \ D^2(s^2+1)=\frac{1}{2} \lim_{s\rightarrow 0} \ 2=1$$ hence $$Y(s)=\frac{1}{s^3}+\frac{1}{s}$$ and $$y(t)=t^2+1$$ Is this correct?,,"['analysis', 'laplace-transform', 'convolution']"
16,Breaking Banach's Fixed Point Theorem,Breaking Banach's Fixed Point Theorem,,"In trying to see how Banach's fixed point theorem would break down in an incomplete space, I tried to come up with an example of a function: $f: \mathbb{Q} \longrightarrow  \mathbb{Q} \ \ $such that $ \forall x,y \in \mathbb{Q}$ our function is contracted with factor $\frac12$ and has no fixed points. i.e. $$ \left | {f(x)-f(y)} \right |\leq \frac12 \left | x-y \right | ,\\ \nexists \ \  x^* \in \mathbb{Q}  \ \ s.t. \  f(x^*)=x^* \ $$  Starting with $f(x)=x^2-2$ and using Newton-Raphson I was able to construct the following function which works $$ f(x) := \left\{\begin{array}{lr}         \frac12 (x-1) + \frac32, & \text{for }  x < 1\\         \frac12- x^{-2}, & \text{for } 1\leq x\leq 2\\         \frac12 (x-2) + \frac32, & \text{for } x>2         \end{array}\right\}$$ That led me to thinking if there was a bijection which satisfied the same criteria. I think I can see why one could exist but am not sure how to prove that one necessarily exists. I am also keen on an explicit example of a bijection that works.","In trying to see how Banach's fixed point theorem would break down in an incomplete space, I tried to come up with an example of a function: $f: \mathbb{Q} \longrightarrow  \mathbb{Q} \ \ $such that $ \forall x,y \in \mathbb{Q}$ our function is contracted with factor $\frac12$ and has no fixed points. i.e. $$ \left | {f(x)-f(y)} \right |\leq \frac12 \left | x-y \right | ,\\ \nexists \ \  x^* \in \mathbb{Q}  \ \ s.t. \  f(x^*)=x^* \ $$  Starting with $f(x)=x^2-2$ and using Newton-Raphson I was able to construct the following function which works $$ f(x) := \left\{\begin{array}{lr}         \frac12 (x-1) + \frac32, & \text{for }  x < 1\\         \frac12- x^{-2}, & \text{for } 1\leq x\leq 2\\         \frac12 (x-2) + \frac32, & \text{for } x>2         \end{array}\right\}$$ That led me to thinking if there was a bijection which satisfied the same criteria. I think I can see why one could exist but am not sure how to prove that one necessarily exists. I am also keen on an explicit example of a bijection that works.",,"['analysis', 'examples-counterexamples', 'banach-fixed-point']"
17,What's a closed form for $\sum_{k=0}^n\frac{1}{k+1}\sum_{r=0\\r~is~odd}^k(-1)^r{k\choose r}r^n$,What's a closed form for,\sum_{k=0}^n\frac{1}{k+1}\sum_{r=0\\r~is~odd}^k(-1)^r{k\choose r}r^n,I want to use a closed form of $$\sum_{k=0}^n\frac{1}{k+1}\sum_{\ \ \ r=0\\r\text{ is odd}}^k(-1)^r{k\choose r}r^n$$ and $$\sum_{k=0}^n\frac{1}{k+1}\sum_{\ \ \ r=0\\r\text{ is even}}^k(-1)^r{k\choose r}r^n$$ Thanks.,I want to use a closed form of $$\sum_{k=0}^n\frac{1}{k+1}\sum_{\ \ \ r=0\\r\text{ is odd}}^k(-1)^r{k\choose r}r^n$$ and $$\sum_{k=0}^n\frac{1}{k+1}\sum_{\ \ \ r=0\\r\text{ is even}}^k(-1)^r{k\choose r}r^n$$ Thanks.,,"['calculus', 'analysis', 'summation', 'binomial-coefficients', 'combinations']"
18,Some basic properties of Riemann integrable functions,Some basic properties of Riemann integrable functions,,"Let $I=[a,b]\subset\mathbb{R}$ be a closed and bounded intervall and let $f:I\to\mathbb{R}$ be a bounded function. For a subdivision $\sigma$ of $I$ (i.e. a finite strictly increasing sequence $(x_0,...,x_N)$ with $x_0=a$ and $x_N=b$) we define $$ \overline{S}_{\sigma}(f)=\sum_{i=1}^{N}(x_i-x_{i-1})\sup_{x\in[x_{i-1},x_i]}f(x)\\ \underline{S}_{\sigma}(f)=\sum_{i=1}^{N}(x_i-x_{i-1})\inf_{x\in[x_{i-1},x_i]}f(x) $$ and thereby $$ \overline{S}(f)=\inf\{\overline{S}_\sigma(f)\ :\ \sigma\text{ subdivision of } I\}\\ \underline{S}(f)=\sup\{\underline{S}_\sigma(f)\ :\ \sigma\text{ subdivision of } I\} $$ Using this, we define the space of Riemann integrable functions over $I$, denoted by $\mathcal{R}(I)$, as $$ \mathcal{R}(I)=\{f:I\to\mathbb{R}\ :\ f \text{ bounded},\ \overline{S}(f)=\underline{S}(f)\} $$ and for $f\in\mathcal{R}(I)$ $$ \int_I f=\overline{S}(f). $$ I have three questions: (1) If $\|\cdot\|$ denotes the uniform function norm over $I$ (i.e. $\|f\|=\sup_{x\in I} f(x)$), then is it true that $(\mathcal{R}(I),\|\cdot\|)$ is complete? (2) Is it equivalent to the above definition when we consider only the uniform subdivisions $\tau_{N}=(x_0,...,x_N)$ with $x_i=a+i\frac{b-a}{N}$? More precisely, is it always true that $$ \overline{S}(f)=\inf\{\overline{S}_{\tau_N}(f)\ :\ N\in\mathbb{N}\}\\ \underline{S}(f)=\sup\{\underline{S}_{\tau_N}(f)\ :\ N\in\mathbb{N}\}? $$ (3) For a subdivision $\sigma=(x_0,...,x_N)$ of $I$ we define $\Delta(\sigma)=\max_{i=1}^{N}x_i-x_{i-1}$. Is it true that for a Riemann integrable function $f$ and a sequence of subdivisions $\{\sigma_n\}_{n\in\mathbb{N}}$ with $\lim_{n\to\infty}\Delta(\sigma_n)=0$ we have $$ \int_I f=\lim_{n\to\infty}\overline{S}_{\sigma_n}(f)=\lim_{n\to\infty}\underline{S}_{\sigma_n}(f)? $$ This is true if $f$ is continuous (note that $C^0(I)\subset\mathcal{R}(I)$), however I fail to see if it is true for non-continuous functions. How to prove the answers to the three questions? Questions (2) and (3) are somewhat related, although not completely equivalent. If you think it would be better to create one post for one question each, then leave a comment and I will change it.","Let $I=[a,b]\subset\mathbb{R}$ be a closed and bounded intervall and let $f:I\to\mathbb{R}$ be a bounded function. For a subdivision $\sigma$ of $I$ (i.e. a finite strictly increasing sequence $(x_0,...,x_N)$ with $x_0=a$ and $x_N=b$) we define $$ \overline{S}_{\sigma}(f)=\sum_{i=1}^{N}(x_i-x_{i-1})\sup_{x\in[x_{i-1},x_i]}f(x)\\ \underline{S}_{\sigma}(f)=\sum_{i=1}^{N}(x_i-x_{i-1})\inf_{x\in[x_{i-1},x_i]}f(x) $$ and thereby $$ \overline{S}(f)=\inf\{\overline{S}_\sigma(f)\ :\ \sigma\text{ subdivision of } I\}\\ \underline{S}(f)=\sup\{\underline{S}_\sigma(f)\ :\ \sigma\text{ subdivision of } I\} $$ Using this, we define the space of Riemann integrable functions over $I$, denoted by $\mathcal{R}(I)$, as $$ \mathcal{R}(I)=\{f:I\to\mathbb{R}\ :\ f \text{ bounded},\ \overline{S}(f)=\underline{S}(f)\} $$ and for $f\in\mathcal{R}(I)$ $$ \int_I f=\overline{S}(f). $$ I have three questions: (1) If $\|\cdot\|$ denotes the uniform function norm over $I$ (i.e. $\|f\|=\sup_{x\in I} f(x)$), then is it true that $(\mathcal{R}(I),\|\cdot\|)$ is complete? (2) Is it equivalent to the above definition when we consider only the uniform subdivisions $\tau_{N}=(x_0,...,x_N)$ with $x_i=a+i\frac{b-a}{N}$? More precisely, is it always true that $$ \overline{S}(f)=\inf\{\overline{S}_{\tau_N}(f)\ :\ N\in\mathbb{N}\}\\ \underline{S}(f)=\sup\{\underline{S}_{\tau_N}(f)\ :\ N\in\mathbb{N}\}? $$ (3) For a subdivision $\sigma=(x_0,...,x_N)$ of $I$ we define $\Delta(\sigma)=\max_{i=1}^{N}x_i-x_{i-1}$. Is it true that for a Riemann integrable function $f$ and a sequence of subdivisions $\{\sigma_n\}_{n\in\mathbb{N}}$ with $\lim_{n\to\infty}\Delta(\sigma_n)=0$ we have $$ \int_I f=\lim_{n\to\infty}\overline{S}_{\sigma_n}(f)=\lim_{n\to\infty}\underline{S}_{\sigma_n}(f)? $$ This is true if $f$ is continuous (note that $C^0(I)\subset\mathcal{R}(I)$), however I fail to see if it is true for non-continuous functions. How to prove the answers to the three questions? Questions (2) and (3) are somewhat related, although not completely equivalent. If you think it would be better to create one post for one question each, then leave a comment and I will change it.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'riemann-integration']"
19,"Why Tao's Analysis introduce a direct sum $(f\oplus g)(x)=(f(x),g(x))$ to prove something about continuity?",Why Tao's Analysis introduce a direct sum  to prove something about continuity?,"(f\oplus g)(x)=(f(x),g(x))","In Tao's Analysis II , the ""Continuity and Product Space"" chapter, he introduced an operation, which is uncommon in other books, called a direct sum , as in the figure. And then, he proved that the addition function ""plus""($+$): $(x,y)\mapsto x+y$ the subtraction function ""minus""($-$): $(x,y)\mapsto x-y$ the multiplication function ""times"": $(x,y)\mapsto x\cdot y$ the maximum function: $(x,y)\mapsto \max(x,y)$ the minimum function $(x,y)\mapsto \min(x,y)$ are all continuous function from $\Bbb R^2$ to $\Bbb R$. Then he stated a theorem: Let $(X,d)$ be a metric space, and let $f,g:X\to\Bbb R$. Then if $f$ and $g$ are continuous at $c\in X$, then $f+g,f-g,f\cdot g,\max(f,g),\min(f,g):X\to\Bbb R$ are all continuous at $c$. His proof, take $f+g$ case for example, went by: since $f\oplus g:X\to\Bbb R^2$ is continuous at $c$, and $(x,y)\mapsto x+y$ is continuous, so the composite function of $(f\oplus g)$ and $+$, $$\begin{alignat*}{2} &+\circ (f\oplus g)\\ =&x\mapsto +((f\oplus g)(x))\\ =&x\mapsto +((f(x),g(x)))\\ =&x\mapsto f(x)+g(x)\\ =&f+g\end{alignat*}$$ the last is what we want, is continuous at $c$. My question: It seems it can be more direct and natural to prove this theorem by a similar theorem in one-variable anaylsis, that is $\lim_{x\to c}(f+g)(x)=\lim_{x\to c}f(x)+\lim_{x\to c}g(x)$, with a little modify to the metric space cases. And then it can quite easily be made to derive that the continuity  case also hold. So why Tao didn't follow this way and he chose to define a rarely seen terminlogy direct sum (in analysis)? Is Tao's way more general or the theorem more strong to use in particular circumstances? Or are these often used in the more advanced courses?","In Tao's Analysis II , the ""Continuity and Product Space"" chapter, he introduced an operation, which is uncommon in other books, called a direct sum , as in the figure. And then, he proved that the addition function ""plus""($+$): $(x,y)\mapsto x+y$ the subtraction function ""minus""($-$): $(x,y)\mapsto x-y$ the multiplication function ""times"": $(x,y)\mapsto x\cdot y$ the maximum function: $(x,y)\mapsto \max(x,y)$ the minimum function $(x,y)\mapsto \min(x,y)$ are all continuous function from $\Bbb R^2$ to $\Bbb R$. Then he stated a theorem: Let $(X,d)$ be a metric space, and let $f,g:X\to\Bbb R$. Then if $f$ and $g$ are continuous at $c\in X$, then $f+g,f-g,f\cdot g,\max(f,g),\min(f,g):X\to\Bbb R$ are all continuous at $c$. His proof, take $f+g$ case for example, went by: since $f\oplus g:X\to\Bbb R^2$ is continuous at $c$, and $(x,y)\mapsto x+y$ is continuous, so the composite function of $(f\oplus g)$ and $+$, $$\begin{alignat*}{2} &+\circ (f\oplus g)\\ =&x\mapsto +((f\oplus g)(x))\\ =&x\mapsto +((f(x),g(x)))\\ =&x\mapsto f(x)+g(x)\\ =&f+g\end{alignat*}$$ the last is what we want, is continuous at $c$. My question: It seems it can be more direct and natural to prove this theorem by a similar theorem in one-variable anaylsis, that is $\lim_{x\to c}(f+g)(x)=\lim_{x\to c}f(x)+\lim_{x\to c}g(x)$, with a little modify to the metric space cases. And then it can quite easily be made to derive that the continuity  case also hold. So why Tao didn't follow this way and he chose to define a rarely seen terminlogy direct sum (in analysis)? Is Tao's way more general or the theorem more strong to use in particular circumstances? Or are these often used in the more advanced courses?",,"['analysis', 'metric-spaces', 'continuity']"
20,$SL_2(\mathbb{R})$ is a 3-dimensional differentiable manifold,is a 3-dimensional differentiable manifold,SL_2(\mathbb{R}),"I've seen this result posed in several places as an exercise, and I've also seen the proof for $SL_n(\mathbb{R})$, for example in this answer . But I'm not quite able to follow the details. Can someone work out the details of the $n=2$ case in the most elementary way possible? Thanks in advance for any help.","I've seen this result posed in several places as an exercise, and I've also seen the proof for $SL_n(\mathbb{R})$, for example in this answer . But I'm not quite able to follow the details. Can someone work out the details of the $n=2$ case in the most elementary way possible? Thanks in advance for any help.",,"['analysis', 'differential-geometry', 'manifolds']"
21,Exercise 2.1.1 from Einsiedler and Ward,Exercise 2.1.1 from Einsiedler and Ward,,"I am studying Ergodic Theory for the first time, and am using the book ""Ergodic Theory with a view towards Number Theory"" by Einsiedler and Ward. I got stuck at the very first exercise problem, numbered 2.1.1. which says: Show that the spaces $(\mathbb T,B_{\mathbb T},m_{\mathbb T})$ and $(\mathbb T^2, B_{\mathbb T^2},m_{\mathbb T^2})$ are isomorphic as measure spaces, where $\mathbb T=\mathbb R/\mathbb Z$ and $m_{\mathbb T}$ is the Lebesgue measure on $\mathbb T$. The book does not define isomorphic measure spaces, but I think it means I hae to give a bijective map $f:(\mathbb T^2, B_{\mathbb T^2},m_{\mathbb T^2})\to (\mathbb T,B_{\mathbb T},m_{\mathbb T})$ so that $f$ and $f^{-1}$ are measurable. Notice, $B_{\mathbb T}$ is the Borel $\sigma-$algebra on $\mathbb T$. I also am not sure I understand what $\mathbb T^2$ means. Does it mean $\mathbb T\times \mathbb T$? That is $\{(x+\mathbb Z,y+\mathbb Z):x,y\in[0,1)\}$? So I have two real numbers, $x$ and $y$, and I want to map them bijectively to a new number in $[0,1)$. Can I do that? Should this at all be my approach? Update I read from Bogachev that two measure spaces $(X,A,\mu)$ and $Y,B,\nu)$ are isomorphic iff there exists a bijective map $\phi:X\to Y$ so that $\phi$ and $\phi^{-1}$ are measurable and there exist $X'\subset X$, $Y'\subset Y$ and $\nu=\mu\circ \phi^{-1}$. Here I take $X=[0,1), Y=[0,1)^2$ and $X'=[0,1)\cap \mathbb Q^c$, $Y'=([0,1)\cap\mathbb Q^c)^2$ then each has Lebesgue measure $1$. I chose irrationals because they have unique representation. I define $\phi:X'\to Y'$ as $\phi(0.a_1a_2a_3a_4...)=(0.a_1a_3a_5...,0.a_2a_4a_6...)$ basically taking the odd and even parts in the decimal expansion. Then I saw that $\phi$ is bijective (injectivity follows due to unique representation in base 10, for irrationals). I am not sure but I think this map is measurable and its inverse also is measurable. But I don't think the last property is true i.e. the value of the measures being same are not true in general. Can you please help me to find a map, then, probably using mine or otherwise?","I am studying Ergodic Theory for the first time, and am using the book ""Ergodic Theory with a view towards Number Theory"" by Einsiedler and Ward. I got stuck at the very first exercise problem, numbered 2.1.1. which says: Show that the spaces $(\mathbb T,B_{\mathbb T},m_{\mathbb T})$ and $(\mathbb T^2, B_{\mathbb T^2},m_{\mathbb T^2})$ are isomorphic as measure spaces, where $\mathbb T=\mathbb R/\mathbb Z$ and $m_{\mathbb T}$ is the Lebesgue measure on $\mathbb T$. The book does not define isomorphic measure spaces, but I think it means I hae to give a bijective map $f:(\mathbb T^2, B_{\mathbb T^2},m_{\mathbb T^2})\to (\mathbb T,B_{\mathbb T},m_{\mathbb T})$ so that $f$ and $f^{-1}$ are measurable. Notice, $B_{\mathbb T}$ is the Borel $\sigma-$algebra on $\mathbb T$. I also am not sure I understand what $\mathbb T^2$ means. Does it mean $\mathbb T\times \mathbb T$? That is $\{(x+\mathbb Z,y+\mathbb Z):x,y\in[0,1)\}$? So I have two real numbers, $x$ and $y$, and I want to map them bijectively to a new number in $[0,1)$. Can I do that? Should this at all be my approach? Update I read from Bogachev that two measure spaces $(X,A,\mu)$ and $Y,B,\nu)$ are isomorphic iff there exists a bijective map $\phi:X\to Y$ so that $\phi$ and $\phi^{-1}$ are measurable and there exist $X'\subset X$, $Y'\subset Y$ and $\nu=\mu\circ \phi^{-1}$. Here I take $X=[0,1), Y=[0,1)^2$ and $X'=[0,1)\cap \mathbb Q^c$, $Y'=([0,1)\cap\mathbb Q^c)^2$ then each has Lebesgue measure $1$. I chose irrationals because they have unique representation. I define $\phi:X'\to Y'$ as $\phi(0.a_1a_2a_3a_4...)=(0.a_1a_3a_5...,0.a_2a_4a_6...)$ basically taking the odd and even parts in the decimal expansion. Then I saw that $\phi$ is bijective (injectivity follows due to unique representation in base 10, for irrationals). I am not sure but I think this map is measurable and its inverse also is measurable. But I don't think the last property is true i.e. the value of the measures being same are not true in general. Can you please help me to find a map, then, probably using mine or otherwise?",,"['analysis', 'measure-theory', 'ergodic-theory']"
22,Error in my reasoning to prove a bounded convergence theorem for the Riemann integral,Error in my reasoning to prove a bounded convergence theorem for the Riemann integral,,"I was trying to prove this fact: If $f_n$ is a sequence of continuous functions from $[0;1]\to[0;1]$ such that $f_n(x)\to0\ \forall x\in[0;1]$ then $\lim_{n\to\infty}\int_0^1 f_n(x)dx=0$. While trying to prove this, I reached the result that convergence is uniform, which sounds absurd. I'd like to see the error in my reasoning and see a proof of this fact (that doesn't depend on measure theory): Fix $\varepsilon$ Let $E_n=\{x\in[0;1] / f_n(x)>\varepsilon\}$ Let $A_k=\cup_k^\infty E_n$. Finally, let $A=\cap_1^\infty \overline{A_k}$. There are two cases: 1) $A=\emptyset$. Then, since each $\overline{A_k}$ is compact and $\overline{A_{k+1}}\subset\overline{A_k}$ then $A_K$ is empty for some $A_K$. Thus, $E_n=\emptyset\ \forall n>K$. Thus, convergence is uniform. 2) $A\neq\emptyset$. Then, some $x$ belongs to $\overline{A_k}\forall k$. Then, $f_n(x)\geq\varepsilon\ \forall n_i$ and $n_i$ an infinite subsequence of the integers which means that $f_n(x)$ doesn't converge to $0$. I suspect that my error is in the last statement but I am not sure why and I have struggled to find a proof of the fact but I didn't reach anywhere. Thanks in advance.","I was trying to prove this fact: If $f_n$ is a sequence of continuous functions from $[0;1]\to[0;1]$ such that $f_n(x)\to0\ \forall x\in[0;1]$ then $\lim_{n\to\infty}\int_0^1 f_n(x)dx=0$. While trying to prove this, I reached the result that convergence is uniform, which sounds absurd. I'd like to see the error in my reasoning and see a proof of this fact (that doesn't depend on measure theory): Fix $\varepsilon$ Let $E_n=\{x\in[0;1] / f_n(x)>\varepsilon\}$ Let $A_k=\cup_k^\infty E_n$. Finally, let $A=\cap_1^\infty \overline{A_k}$. There are two cases: 1) $A=\emptyset$. Then, since each $\overline{A_k}$ is compact and $\overline{A_{k+1}}\subset\overline{A_k}$ then $A_K$ is empty for some $A_K$. Thus, $E_n=\emptyset\ \forall n>K$. Thus, convergence is uniform. 2) $A\neq\emptyset$. Then, some $x$ belongs to $\overline{A_k}\forall k$. Then, $f_n(x)\geq\varepsilon\ \forall n_i$ and $n_i$ an infinite subsequence of the integers which means that $f_n(x)$ doesn't converge to $0$. I suspect that my error is in the last statement but I am not sure why and I have struggled to find a proof of the fact but I didn't reach anywhere. Thanks in advance.",,"['real-analysis', 'analysis']"
23,Riemann Stieltjes integral and uniformly convergence.,Riemann Stieltjes integral and uniformly convergence.,,"I am trying to prove that If $g_n$ is a sequence of increasing functions on $[a,b]$ which converges uniformly to $g$, and if an increasing function $f$ is integrable w.r.t $g_n$ for all $n$, then f is integrable with respect to $g$ and $$\int_a^b f dg=\lim \int_a^b fdg_n$$ Can anyone give me a hint?","I am trying to prove that If $g_n$ is a sequence of increasing functions on $[a,b]$ which converges uniformly to $g$, and if an increasing function $f$ is integrable w.r.t $g_n$ for all $n$, then f is integrable with respect to $g$ and $$\int_a^b f dg=\lim \int_a^b fdg_n$$ Can anyone give me a hint?",,"['integration', 'analysis']"
24,"Prove or Disprove there is a sequence $f_n$ of continuous function on [0,1] such that for each x $ \in [0,1] $, $f_n(x)$ converges to $f(x)$","Prove or Disprove there is a sequence  of continuous function on [0,1] such that for each x ,  converges to","f_n  \in [0,1]  f_n(x) f(x)","Prove or disprove: If $f$ is non-decreasing real valued function on $[0,1]$ then there is a sequence  $f_n$ of continuous function on $[0,1]$ such that for each x $ \in [0,1] $,  we have  $f_n(x)$ converges to  $f(x)$ I am thinking of If assume f is continuous then we can find sequence of polynomial $p_n(x)$ which converges to $f$ even uniformly. but f is not given to be continuous then how do we do? any suggestions and hints are welcomed.","Prove or disprove: If $f$ is non-decreasing real valued function on $[0,1]$ then there is a sequence  $f_n$ of continuous function on $[0,1]$ such that for each x $ \in [0,1] $,  we have  $f_n(x)$ converges to  $f(x)$ I am thinking of If assume f is continuous then we can find sequence of polynomial $p_n(x)$ which converges to $f$ even uniformly. but f is not given to be continuous then how do we do? any suggestions and hints are welcomed.",,"['real-analysis', 'analysis', 'convergence-divergence', 'examples-counterexamples', 'uniform-convergence']"
25,Lebesgue measure on a continuous function,Lebesgue measure on a continuous function,,"Let $f(x)$ be a continuous function on [1, 1]. Show that there exists a constant $c$ such that the Lebesgue measures $\mu (\{x  [1, 1] : f(x)  c\})  1$,$\quad$ $\mu (\{x  [1, 1] : f(x)  c\})  1$. I can do the trivial case choosing $c$ to be the supremum and infimum of $f$ over the interval. But I think this is not a kosher proof.","Let $f(x)$ be a continuous function on [1, 1]. Show that there exists a constant $c$ such that the Lebesgue measures $\mu (\{x  [1, 1] : f(x)  c\})  1$,$\quad$ $\mu (\{x  [1, 1] : f(x)  c\})  1$. I can do the trivial case choosing $c$ to be the supremum and infimum of $f$ over the interval. But I think this is not a kosher proof.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
26,The concept of negative numbers in the $4^{th}$ field axiom,The concept of negative numbers in the  field axiom,4^{th},"I just started working my way trough ""Mathematical Analysis"", $2^{nd}$ Edition by Apostol. I am reading every detail very carefully to try to get a rigorous understanding. The $4^{th}$ axiom in that book (one of the field axioms states): Given any 2 real numbers $x$ and $y$, there exists a real number $z$ such that $x + z = y$. This $z$ is denoted by $y - x$; the number $x    - x$ is denoted by $0$. (It can be proved that $0$ is independent of $x$.)  We write $-x$ for $0 - x$ and call $-x$ the negative of $x$. My question:  Is $y-x$ already assumed to be $y+(-x)$, or should I take $y-x$ as just a whole symbol for now, only representing the number $z$? This is a little confusing because we have only assumed the existence of the addition and multiplication operator.  (Please try to refrain from using too much set notation in the answers, as I'm not familiar with much of it.) I think if I can interpret $y-x$ as $y + (-x)$, (I think?) we can prove that the negative of $x$ is one possible number which you could add to $x$ in order to get $0$: By axiom 4, there is always a $z$ such that $x + z = y$ That $z$ is denoted as $y + (-x)$ Let $y = 0$ $z = 0 - x$ $z = -x$","I just started working my way trough ""Mathematical Analysis"", $2^{nd}$ Edition by Apostol. I am reading every detail very carefully to try to get a rigorous understanding. The $4^{th}$ axiom in that book (one of the field axioms states): Given any 2 real numbers $x$ and $y$, there exists a real number $z$ such that $x + z = y$. This $z$ is denoted by $y - x$; the number $x    - x$ is denoted by $0$. (It can be proved that $0$ is independent of $x$.)  We write $-x$ for $0 - x$ and call $-x$ the negative of $x$. My question:  Is $y-x$ already assumed to be $y+(-x)$, or should I take $y-x$ as just a whole symbol for now, only representing the number $z$? This is a little confusing because we have only assumed the existence of the addition and multiplication operator.  (Please try to refrain from using too much set notation in the answers, as I'm not familiar with much of it.) I think if I can interpret $y-x$ as $y + (-x)$, (I think?) we can prove that the negative of $x$ is one possible number which you could add to $x$ in order to get $0$: By axiom 4, there is always a $z$ such that $x + z = y$ That $z$ is denoted as $y + (-x)$ Let $y = 0$ $z = 0 - x$ $z = -x$",,"['real-analysis', 'analysis', 'axioms']"
27,Proving that there's no translation invariant measure on the power set of $\mathbb{R}$,Proving that there's no translation invariant measure on the power set of,\mathbb{R},"The goal of this task given to me is to show that there is no (non-trivial) translation invariant measure on $P(\mathbb{R})$, the power set of $\mathbb{R}$, and I think I almost completed it, but I just can't find a way to prove the very last bit that's missing. But let's start at the beginning. Let $\mu: P(\mathbb{R}) \to [0, ]$ be a measure that satisfies the following conditions: $(*) \mu([0, 1]) = 1$ and $\mu(x + A) = \mu(A)$ for all $x \in \mathbb{R}, A \subseteq \mathbb{R}$. The assignment asks me to consider the equivalence relation $x \sim y :<=> x - y \in \mathbb{Q}$ Out of every equivalence class, we choose a representative $x \in [0, 1]$. Let $X \subseteq [0, 1]$ be the set of representatives that we got that way. I am now to consider the sets $\mathbb{R} = \cup_{x \in X} (x + \mathbb{Q})$ and $\mathbb{R} = \cup_{q \in \mathbb{Q}} (X + q)$, and find a contradiction by showing that $\mu$ would need to satisfy both $\mu(X) = 0$ aswell as $\mu(X) > 0$. What I've shown so far: I showed that $\mu(\mathbb{R}) = $ (using the fact that $[0, 1]$ is sent to $1$), aswell as that $\mu(\{a\}) = 0$ for all $a \in \mathbb{R}$. From this, it also follows that all countable subsets of $\mathbb{R}$ are sent to $0$ by $\mu$ (so especially $\mu(\mathbb{Q}) = 0$). I've also shown that, since $\mu(\cup_{q \in \mathbb{Q}} (X + q)) = $ and since $\cup_{q \in \mathbb{Q}} (X + q)$ is a countable, disjoint union, we have that $\mu(X) > 0$. The only part I'm still missing is to show that $\mu(X) = 0$ via the fact that $\mathbb{R} = \cup_{x \in X} (x + \mathbb{Q})$, as written above. I just can't get my head around how I could show this. $\cup_{x \in X} (x + \mathbb{Q})$ is a disjoint, but uncountable union because $X$ contains uncountably many elements; therefore, we can't use the $\sigma$-additivity of a measure. I see that $\mu(x + \mathbb{Q}) = 0$ for each $x \in X$, because $\mathbb{Q})$ is countable, but I don't know how that helps me. I've also thought about decomposing $[0,1]$ into a disjoint union of sets, but that didn't lead anywhere so far. If we ""remove"" countably many points of $[0,1]$, the remaining set would still be sent to $1$ by $\mu$, for the reasons given above. So how could I conclude that $X$ must be sent to $0$? I'm out of ideas.","The goal of this task given to me is to show that there is no (non-trivial) translation invariant measure on $P(\mathbb{R})$, the power set of $\mathbb{R}$, and I think I almost completed it, but I just can't find a way to prove the very last bit that's missing. But let's start at the beginning. Let $\mu: P(\mathbb{R}) \to [0, ]$ be a measure that satisfies the following conditions: $(*) \mu([0, 1]) = 1$ and $\mu(x + A) = \mu(A)$ for all $x \in \mathbb{R}, A \subseteq \mathbb{R}$. The assignment asks me to consider the equivalence relation $x \sim y :<=> x - y \in \mathbb{Q}$ Out of every equivalence class, we choose a representative $x \in [0, 1]$. Let $X \subseteq [0, 1]$ be the set of representatives that we got that way. I am now to consider the sets $\mathbb{R} = \cup_{x \in X} (x + \mathbb{Q})$ and $\mathbb{R} = \cup_{q \in \mathbb{Q}} (X + q)$, and find a contradiction by showing that $\mu$ would need to satisfy both $\mu(X) = 0$ aswell as $\mu(X) > 0$. What I've shown so far: I showed that $\mu(\mathbb{R}) = $ (using the fact that $[0, 1]$ is sent to $1$), aswell as that $\mu(\{a\}) = 0$ for all $a \in \mathbb{R}$. From this, it also follows that all countable subsets of $\mathbb{R}$ are sent to $0$ by $\mu$ (so especially $\mu(\mathbb{Q}) = 0$). I've also shown that, since $\mu(\cup_{q \in \mathbb{Q}} (X + q)) = $ and since $\cup_{q \in \mathbb{Q}} (X + q)$ is a countable, disjoint union, we have that $\mu(X) > 0$. The only part I'm still missing is to show that $\mu(X) = 0$ via the fact that $\mathbb{R} = \cup_{x \in X} (x + \mathbb{Q})$, as written above. I just can't get my head around how I could show this. $\cup_{x \in X} (x + \mathbb{Q})$ is a disjoint, but uncountable union because $X$ contains uncountably many elements; therefore, we can't use the $\sigma$-additivity of a measure. I see that $\mu(x + \mathbb{Q}) = 0$ for each $x \in X$, because $\mathbb{Q})$ is countable, but I don't know how that helps me. I've also thought about decomposing $[0,1]$ into a disjoint union of sets, but that didn't lead anywhere so far. If we ""remove"" countably many points of $[0,1]$, the remaining set would still be sent to $1$ by $\mu$, for the reasons given above. So how could I conclude that $X$ must be sent to $0$? I'm out of ideas.",,"['real-analysis', 'analysis', 'measure-theory']"
28,Finding $\max$ and min of function,Finding  and min of function,\max,"I have a problem with following assignment. I need to find critical points of function $$f (x,y)=x^{3}y-3x^{2}y+y^2$$ and determine whether function has min/max or not. $$Df=[3x^{2}y-6xy,x^3-3x^2+2y]$$ Critical points are:$(0,0)(3,0)(2,2)$ and  $$D^2f=\begin{pmatrix}6xy-6y &3x^2-6x\\3x^2-6x&2\end{pmatrix}$$ For $(2,2)$ we get positive definite matrix so it has $\min$ in $(2,2)$. For the other two I have no idea how to examine them. Because matrix is:  $$\begin{vmatrix}0 & 0 \\ 0 & 2\end{vmatrix}\quad \text{ and } \quad \begin{vmatrix}0 & 0 \\ 0 & 9\end{vmatrix}$$ I will be very glad for help.","I have a problem with following assignment. I need to find critical points of function $$f (x,y)=x^{3}y-3x^{2}y+y^2$$ and determine whether function has min/max or not. $$Df=[3x^{2}y-6xy,x^3-3x^2+2y]$$ Critical points are:$(0,0)(3,0)(2,2)$ and  $$D^2f=\begin{pmatrix}6xy-6y &3x^2-6x\\3x^2-6x&2\end{pmatrix}$$ For $(2,2)$ we get positive definite matrix so it has $\min$ in $(2,2)$. For the other two I have no idea how to examine them. Because matrix is:  $$\begin{vmatrix}0 & 0 \\ 0 & 2\end{vmatrix}\quad \text{ and } \quad \begin{vmatrix}0 & 0 \\ 0 & 9\end{vmatrix}$$ I will be very glad for help.",,"['real-analysis', 'analysis']"
29,Which of the following sets are uncountable?,Which of the following sets are uncountable?,,"Which of the following sets are uncountable? $A= \{ f:\mathbb{N} \to \{1,2\} \}$ $B= \{ f: \{1,2\}\to \mathbb{N} \}$ $C= \{ f:\mathbb{N} \to \{1,2\} :f(1) \leq f(2) \}$ $D= \{ f:\{1,2\}\to \mathbb{N} :f(1) \leq f(2) \}$ Since $A$ has the cardinality $2^{\aleph_0},\quad A$ is. Since finite product of countable set is countable and subset of a countable set is countable, $B\  \text{and}\ D$  are countable. Finally,   $C$ can be interpreted as  $C= \{ f:\mathbb{N} \to \{1,2\} :f(2) \neq 1 \}.$ I think $C$ too uncountable but getting stuck on proving this. Hints will be appreciated.","Which of the following sets are uncountable? $A= \{ f:\mathbb{N} \to \{1,2\} \}$ $B= \{ f: \{1,2\}\to \mathbb{N} \}$ $C= \{ f:\mathbb{N} \to \{1,2\} :f(1) \leq f(2) \}$ $D= \{ f:\{1,2\}\to \mathbb{N} :f(1) \leq f(2) \}$ Since $A$ has the cardinality $2^{\aleph_0},\quad A$ is. Since finite product of countable set is countable and subset of a countable set is countable, $B\  \text{and}\ D$  are countable. Finally,   $C$ can be interpreted as  $C= \{ f:\mathbb{N} \to \{1,2\} :f(2) \neq 1 \}.$ I think $C$ too uncountable but getting stuck on proving this. Hints will be appreciated.",,"['real-analysis', 'analysis']"
30,Monotone Convergence for Decreasing Functions [duplicate],Monotone Convergence for Decreasing Functions [duplicate],,"This question already has an answer here : Monotone Convergence Theorem for non-negative decreasing sequence of measurable functions (1 answer) Closed 1 year ago . I'm trying to prove the Monotone Convergence Theorem for decreasing sequences, namely if Let $(X,\mathcal{M},\mu)$ be a measure space and suppose $\{f_n\}$ are non-negative measurable functions decreasing pointwise to $f$. Suppose also that $f_1 \in \mathscr{L}(\mu)$. Then $$\int_X f~d\mu = \lim_{n\to\infty}\int_X f_n~d\mu.$$ Why does this statement not follow from LDCT with $f_n$ being dominated by $f_1$? I'm also aware of the solutions with $g_n=f_1-f_n$, but the question asks to prove it using Fatou's lemma","This question already has an answer here : Monotone Convergence Theorem for non-negative decreasing sequence of measurable functions (1 answer) Closed 1 year ago . I'm trying to prove the Monotone Convergence Theorem for decreasing sequences, namely if Let $(X,\mathcal{M},\mu)$ be a measure space and suppose $\{f_n\}$ are non-negative measurable functions decreasing pointwise to $f$. Suppose also that $f_1 \in \mathscr{L}(\mu)$. Then $$\int_X f~d\mu = \lim_{n\to\infty}\int_X f_n~d\mu.$$ Why does this statement not follow from LDCT with $f_n$ being dominated by $f_1$? I'm also aware of the solutions with $g_n=f_1-f_n$, but the question asks to prove it using Fatou's lemma",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
31,How to state that a sequence is Cauchy in terms of $\limsup$ and $\liminf$?,How to state that a sequence is Cauchy in terms of  and ?,\limsup \liminf,"How to state that a sequence is Cauchy in terms of $\limsup$ and $\liminf$? For example, is it true that a sequence $(a_n)_{n=1}^{\infty}$ is Cauchy iff $\displaystyle\limsup_{n\to\infty}|a_{n+k}-a_n|=0$ for all $k\in\mathbb{N}$?","How to state that a sequence is Cauchy in terms of $\limsup$ and $\liminf$? For example, is it true that a sequence $(a_n)_{n=1}^{\infty}$ is Cauchy iff $\displaystyle\limsup_{n\to\infty}|a_{n+k}-a_n|=0$ for all $k\in\mathbb{N}$?",,"['real-analysis', 'analysis']"
32,Residue theorem for infinitely many singularities,Residue theorem for infinitely many singularities,,"The residue theorem is a standard result in complex analysis, I state it below so we are on the same page: note that $\overline{\mathbb{C}}$ is the extended complex plane (ie. $\simeq$ Riemann sphere) Let $f : \overline{\mathbb{C}} \rightarrow \overline{\mathbb{C}}$ be a function which is analytic inside a positively-oriented closed contour $C \subset \overline{\mathbb{C}}$ apart from at a finite number of singular points $a_{1}, \ldots a_{n}$ which are contained inside $C$. Then we have the following result known as the residue theorem, \begin{eqnarray} \int_{C} f(z) \ dz = 2 \pi i \sum_{\nu = 1}^{n} \text{Res}_{z=a_{\nu}}f(z). \quad \quad \quad \quad \quad (1)\end{eqnarray} My question is: what if $n = \infty$? There must be extra conditions in this case because, while the left hand side of (1) may converge, the right hand side of (1) becomes a series and may not converge. A thought: if $\sum_{\nu=1}^{\infty} \text{Res}_{z=a_{\nu}}f(z)$ is only a formal series, then perhaps it represents $\frac{1}{2\pi i}\int_{C}f(z) \ dz$ asymptotically. ie. $\frac{1}{2\pi i}\int_{C}f(z) \ dz \sim \sum_{\nu=1}^{n} \text{Res}_{z=a_{\nu}}f(z)$ as $n \rightarrow \infty$, is this the answer?","The residue theorem is a standard result in complex analysis, I state it below so we are on the same page: note that $\overline{\mathbb{C}}$ is the extended complex plane (ie. $\simeq$ Riemann sphere) Let $f : \overline{\mathbb{C}} \rightarrow \overline{\mathbb{C}}$ be a function which is analytic inside a positively-oriented closed contour $C \subset \overline{\mathbb{C}}$ apart from at a finite number of singular points $a_{1}, \ldots a_{n}$ which are contained inside $C$. Then we have the following result known as the residue theorem, \begin{eqnarray} \int_{C} f(z) \ dz = 2 \pi i \sum_{\nu = 1}^{n} \text{Res}_{z=a_{\nu}}f(z). \quad \quad \quad \quad \quad (1)\end{eqnarray} My question is: what if $n = \infty$? There must be extra conditions in this case because, while the left hand side of (1) may converge, the right hand side of (1) becomes a series and may not converge. A thought: if $\sum_{\nu=1}^{\infty} \text{Res}_{z=a_{\nu}}f(z)$ is only a formal series, then perhaps it represents $\frac{1}{2\pi i}\int_{C}f(z) \ dz$ asymptotically. ie. $\frac{1}{2\pi i}\int_{C}f(z) \ dz \sim \sum_{\nu=1}^{n} \text{Res}_{z=a_{\nu}}f(z)$ as $n \rightarrow \infty$, is this the answer?",,"['analysis', 'residue-calculus', 'complex-integration']"
33,Every ordered field that has the least upper bound property is isomorphic to the real number system.,Every ordered field that has the least upper bound property is isomorphic to the real number system.,,"Okay, so here's a theorem from Rudin: ""Every ordered field that has the least upper bound property is isomorphic to the real number system."" Here's a definition: ""Ordered fields are isomorphic if there is a bijection between the underlying sets that preserves the field operations and the orders."" i.e. Ordered fields (F, +, , ) and (K, , , ) are isomorphic if there exists a bijection h : F  K such that  (a) for all x,y  F, h(x+y)=h(x)h(y),   (b) for all x,y  F, h(xy)=h(x)h(y), and   (c) for all x,y  F, if xy,then h(x)h(y). Here are some things that I think are true: 1.) The field T with two elements {0, 1} is an ordered field that has the least upper bound property. Because T is finite, every nonempty subset of T has a maximum. Max(T) = LUB(T) for all subsets of T that have a maximum element. Therefore all subsets of T have a least upper bound in T. Thus T has the least upper bound property. 2.) Since T has the LUB property, it is isomorphic to the real # system and therefore there exists a bijection between the real numbers and {0, 1}. 3.) ...but since {0, 1} is finite and the real numbers are infinite, there can be no surjection from {0,1} -> real numbers. Therefore there's no bijection between the reals and {0, 1}. So... Which thing that I think is true is not actually true? Thanks!","Okay, so here's a theorem from Rudin: ""Every ordered field that has the least upper bound property is isomorphic to the real number system."" Here's a definition: ""Ordered fields are isomorphic if there is a bijection between the underlying sets that preserves the field operations and the orders."" i.e. Ordered fields (F, +, , ) and (K, , , ) are isomorphic if there exists a bijection h : F  K such that  (a) for all x,y  F, h(x+y)=h(x)h(y),   (b) for all x,y  F, h(xy)=h(x)h(y), and   (c) for all x,y  F, if xy,then h(x)h(y). Here are some things that I think are true: 1.) The field T with two elements {0, 1} is an ordered field that has the least upper bound property. Because T is finite, every nonempty subset of T has a maximum. Max(T) = LUB(T) for all subsets of T that have a maximum element. Therefore all subsets of T have a least upper bound in T. Thus T has the least upper bound property. 2.) Since T has the LUB property, it is isomorphic to the real # system and therefore there exists a bijection between the real numbers and {0, 1}. 3.) ...but since {0, 1} is finite and the real numbers are infinite, there can be no surjection from {0,1} -> real numbers. Therefore there's no bijection between the reals and {0, 1}. So... Which thing that I think is true is not actually true? Thanks!",,"['real-analysis', 'analysis', 'ordered-fields']"
34,Proving cauchy sequence is convergent sequence.,Proving cauchy sequence is convergent sequence.,,"I know it's very petty question in real-analysis but I want to make the proof finely logical. There is 3 steps of the proof, we all know well, that cauchy sequence is convergent sequence. The first is proving cauchy sequence is bounded. The second is that by Bolzano-Weierstrass theorem, there exists a cluster point. The third is showing the original cauchy sequence is convergent to the cluster point. I think there is a logical gap in my text book, but i cannot make it fine. Let $L$ be the cluster point of $\{a_n\}$ which is cauchy sequence. Then there exists a subsequence $\{a_{n_k}\}$ convergent to $L$. Given  $\epsilon>0$, $\exists N_1 \in \mathbb{N}, \forall k : [k>N_1 \rightarrow |a_{n_k} - L| < \epsilon/2$. By cauchy condition, $\exists N_2 \in \mathbb{N}, \forall n,m : [n,m > N_2 \rightarrow |a_m - a_n| < \epsilon/2$. $\textbf{N := max\{N_1, N_2\}}$, then for all $n>N$     $$|a_n-L| = |a_n - a_{n_{N+1}} + a_{n_{N+1}} -L| \leq |a_n - a_{n_{N+1}}| + |a_{n_{N+1}} -L| < \epsilon.$$ I think the bold text is the logical gap in the proof. If we set $N$ like the above proof, there is no verification that $n_{N+1} > N_2$. So, if we make this proof well, additory condition is required such as $\textbf{N :=max\{N_1, N_2\}}$, then for all $n>N$ $\textbf{and $n_k>N$} $. However the definition of convergence has only $n>N$ assumption to make it sense the inequality that $|a_n -L|<\epsilon$. How do we set the index numbering to make the proof have no logical gap?","I know it's very petty question in real-analysis but I want to make the proof finely logical. There is 3 steps of the proof, we all know well, that cauchy sequence is convergent sequence. The first is proving cauchy sequence is bounded. The second is that by Bolzano-Weierstrass theorem, there exists a cluster point. The third is showing the original cauchy sequence is convergent to the cluster point. I think there is a logical gap in my text book, but i cannot make it fine. Let $L$ be the cluster point of $\{a_n\}$ which is cauchy sequence. Then there exists a subsequence $\{a_{n_k}\}$ convergent to $L$. Given  $\epsilon>0$, $\exists N_1 \in \mathbb{N}, \forall k : [k>N_1 \rightarrow |a_{n_k} - L| < \epsilon/2$. By cauchy condition, $\exists N_2 \in \mathbb{N}, \forall n,m : [n,m > N_2 \rightarrow |a_m - a_n| < \epsilon/2$. $\textbf{N := max\{N_1, N_2\}}$, then for all $n>N$     $$|a_n-L| = |a_n - a_{n_{N+1}} + a_{n_{N+1}} -L| \leq |a_n - a_{n_{N+1}}| + |a_{n_{N+1}} -L| < \epsilon.$$ I think the bold text is the logical gap in the proof. If we set $N$ like the above proof, there is no verification that $n_{N+1} > N_2$. So, if we make this proof well, additory condition is required such as $\textbf{N :=max\{N_1, N_2\}}$, then for all $n>N$ $\textbf{and $n_k>N$} $. However the definition of convergence has only $n>N$ assumption to make it sense the inequality that $|a_n -L|<\epsilon$. How do we set the index numbering to make the proof have no logical gap?",,"['real-analysis', 'analysis']"
35,Why can the elements of $L^\infty$ be approximated in $L^p$ by $C^1$-functions?,Why can the elements of  be approximated in  by -functions?,L^\infty L^p C^1,Let $\Omega\subseteq\mathbb R^n$ be a bounded domain and $f\in L^\infty(\Omega)$. From which theorem does the existence of $(f_k)_{k\in\mathbb N}\subseteq C^1(\overline\Omega)$ with $$f_k\stackrel{L^p(\Omega)}{\to}f$$ for some $p>n$ folow?,Let $\Omega\subseteq\mathbb R^n$ be a bounded domain and $f\in L^\infty(\Omega)$. From which theorem does the existence of $(f_k)_{k\in\mathbb N}\subseteq C^1(\overline\Omega)$ with $$f_k\stackrel{L^p(\Omega)}{\to}f$$ for some $p>n$ folow?,,"['analysis', 'convergence-divergence', 'approximation', 'lp-spaces']"
36,Integrability of $(x+y) ^{-3}$.,Integrability of .,(x+y) ^{-3},"I'm asked to determine for what positive values of $\alpha$ is $(x+y)^{-3}$ integrable in the region where $0<x<1$ and $0<y<x^\alpha$. I've found that the function is integrable when $\alpha \geq 3$, but I'm not sure about the case when $0<\alpha<3.$","I'm asked to determine for what positive values of $\alpha$ is $(x+y)^{-3}$ integrable in the region where $0<x<1$ and $0<y<x^\alpha$. I've found that the function is integrable when $\alpha \geq 3$, but I'm not sure about the case when $0<\alpha<3.$",,['analysis']
37,Compute the derivatives of $\frac{d^{2\ell}}{dx^{2\ell}}\tanh(x)^{2k}$ in $x=0$,Compute the derivatives of  in,\frac{d^{2\ell}}{dx^{2\ell}}\tanh(x)^{2k} x=0,"I would like to compute the derivatives $\frac{d^{2\ell}}{dx^{2\ell}}_{\vert x=0}\tanh(x)^{2k}$ at $x=0$ where $k,\ell\in \mathbb{N}$ positive integers with $\ell\geq k$. I am not sure how to attack this problem seriously. Can you give me an instruction what to do in this situation? Best wishes Edit: In case of $k>\ell$ we have $\frac{d^{2\ell}}{dx^{2\ell}}_{\vert x=0}\tanh(x)^{2k}=0$. (see the commentary and answer below).","I would like to compute the derivatives $\frac{d^{2\ell}}{dx^{2\ell}}_{\vert x=0}\tanh(x)^{2k}$ at $x=0$ where $k,\ell\in \mathbb{N}$ positive integers with $\ell\geq k$. I am not sure how to attack this problem seriously. Can you give me an instruction what to do in this situation? Best wishes Edit: In case of $k>\ell$ we have $\frac{d^{2\ell}}{dx^{2\ell}}_{\vert x=0}\tanh(x)^{2k}=0$. (see the commentary and answer below).",,"['real-analysis', 'analysis', 'derivatives']"
38,Proof Verification: Converse of Intermediate Value Theorem,Proof Verification: Converse of Intermediate Value Theorem,,"A function $f$ is increasing on $A$ if $f(x)\leq f(y)$ for all $x<y$ in $A$. Show that the Intermediate Value Theorem does have a converse if we assume $f$ is increasing on $[a,b]$. The converse of IVT along with the additional hypothesis would look like this: Lef $f:[a,b] \rightarrow \mathbb{R}$ be an increasing function on $[a,b]$ which satisfies: if $L$ is a real number such that $f(a)<L<f(b)$ (or $f(a)>L>f(b)$), then there exists a point $c\in (a,b)$ where $f(c)=L.$ If the preceding conditions are met, then $f$ is continuous on $[a,b]$. The proof (if it's correct) I'm going to show is only for the $c \in (a,b)$, but I think for end points ($a$ or $b$), the method should be similar. We want to show that given $c \in (a,b)$, for all $\epsilon>0$, there exists a $\delta>0$ such that $|x-c|<\delta \implies |f(x)-f(c)|<\epsilon$. Since $f$ is increasing, we know that $f(a)\leq f(c)$. If $f(c)-\epsilon<f(a)$, then set $x_1=a$, if $f(c)-\epsilon\geq f(a)$, then we know by hypothesis that a $x_1<c$ exists such that $f(x_1)=f(c)-\epsilon.$ In either case, we have for $x \in (x_1,c]$ $$ f(c)-\epsilon \leq f(x_1) \leq f(x) \leq f(c) .$$ In a similar way, since we know that $f(c)\leq f(b)$, we can deduce that a $x_2>c$ exists such that, for $x \in [c,x_2)$ $$f(c)\leq f(x)\leq f(x_2)\leq f(c)+\epsilon .$$ Pick $\delta = \min \{c-x_1,x_2-c \}$, we then have $$|x-c|<\delta \implies |f(x)-f(c)|<\epsilon .$$ Is this correct? In the proof that I saw, they originally used $\frac{\epsilon}{2}$, instead of just $\epsilon$, but I think this one works just as well, what do you guys think?","A function $f$ is increasing on $A$ if $f(x)\leq f(y)$ for all $x<y$ in $A$. Show that the Intermediate Value Theorem does have a converse if we assume $f$ is increasing on $[a,b]$. The converse of IVT along with the additional hypothesis would look like this: Lef $f:[a,b] \rightarrow \mathbb{R}$ be an increasing function on $[a,b]$ which satisfies: if $L$ is a real number such that $f(a)<L<f(b)$ (or $f(a)>L>f(b)$), then there exists a point $c\in (a,b)$ where $f(c)=L.$ If the preceding conditions are met, then $f$ is continuous on $[a,b]$. The proof (if it's correct) I'm going to show is only for the $c \in (a,b)$, but I think for end points ($a$ or $b$), the method should be similar. We want to show that given $c \in (a,b)$, for all $\epsilon>0$, there exists a $\delta>0$ such that $|x-c|<\delta \implies |f(x)-f(c)|<\epsilon$. Since $f$ is increasing, we know that $f(a)\leq f(c)$. If $f(c)-\epsilon<f(a)$, then set $x_1=a$, if $f(c)-\epsilon\geq f(a)$, then we know by hypothesis that a $x_1<c$ exists such that $f(x_1)=f(c)-\epsilon.$ In either case, we have for $x \in (x_1,c]$ $$ f(c)-\epsilon \leq f(x_1) \leq f(x) \leq f(c) .$$ In a similar way, since we know that $f(c)\leq f(b)$, we can deduce that a $x_2>c$ exists such that, for $x \in [c,x_2)$ $$f(c)\leq f(x)\leq f(x_2)\leq f(c)+\epsilon .$$ Pick $\delta = \min \{c-x_1,x_2-c \}$, we then have $$|x-c|<\delta \implies |f(x)-f(c)|<\epsilon .$$ Is this correct? In the proof that I saw, they originally used $\frac{\epsilon}{2}$, instead of just $\epsilon$, but I think this one works just as well, what do you guys think?",,"['analysis', 'proof-verification']"
39,Revolving a $k$-manifold around an axis gives a $(k+1)$-manifold,Revolving a -manifold around an axis gives a -manifold,k (k+1),"I want to solve the following problem from M. Spivak's Calculus on Manifolds: Let $\mathbb{K}^n=\{x \in \mathbb{R}^n:x^1=0 \text{ and }x^2>0,\dots,x^{n-1}>0\}$ . If $M \subseteq \mathbb{K}^n$ is a $k$ -dimensional manifold, and $N$ is obtained by revolving $M$ around the axis $x^1=\cdots=x^{n-1}=0$ , show that $N$ is a $(k+1)$ dimensional manifold. Example: the Torus (Figure 5-4). My attempt: At first I considered the case $n=3$ , where $\mathbb{K}^3=\{(x,y,z) \in \mathbb{R}^3:x=0,y>0\}$ . for a point $\mathbf{x}$ not on the $z$ axis consider the angle $\theta(\mathbf{x})$ which is the one between the vector projection of $\mathbf{x}$ to the $[xy]$ plane and the positive $x$ -axis (this is the angle $\theta$ from polar coordinates). Now, since $M$ is $1$ -dimensional manifold, for each $p \in M$ there exists open sets $U_p \ni p,V_p \subseteq \mathbb{R}^3$ and a diffeomorphism $h_p:U_p \to V_p$ such that $h_p(U_p \cap M)=V_p \cap (\mathbb{R}^1 \times \{0\}^2 )$ . Let $q \in N$ be some point. Define $$k_q(\mathbf{x}):=[R_z(\theta(\mathbf{x})) \circ h \circ R_z(-\theta(\mathbf{x}))](\mathbf{x})$$ where $R_z$ is the rotation matrix around the $z$ -axis, and a branch of $\theta$ is chosen so that it is smooth around $q$ . Say $p$ is the (unique) point in $\mathbb{K}^3$ such that $q$ is the result of some rotation of it around the $z$ -axis. If $U_p$ is taken to be a sufficiently small ball around $p$ (so that it doesn't intersect the $z$ -axis), I claim that $k_q$ is a diffeomorphism with domain $\overline{U}_q:=R_z(\theta(q))[U_p]$ and codomain $\overline{V}_q:=k_q(\overline{U}_q)$ . I also claim that $$k_q(\overline{U}_q \cap N)=\overline{V}_q \cap (\mathbb{R}^2 \times \{0\}) .$$ Now, trying to generalize this to arbitrary $n$ seems difficult to me, as I have no idea how rotations in $\geq 4$ dimensions work. My questions are: Is the proof for $n=3$ valid? If not,  please help me correct it. How can one prove the general case? Thank you!","I want to solve the following problem from M. Spivak's Calculus on Manifolds: Let . If is a -dimensional manifold, and is obtained by revolving around the axis , show that is a dimensional manifold. Example: the Torus (Figure 5-4). My attempt: At first I considered the case , where . for a point not on the axis consider the angle which is the one between the vector projection of to the plane and the positive -axis (this is the angle from polar coordinates). Now, since is -dimensional manifold, for each there exists open sets and a diffeomorphism such that . Let be some point. Define where is the rotation matrix around the -axis, and a branch of is chosen so that it is smooth around . Say is the (unique) point in such that is the result of some rotation of it around the -axis. If is taken to be a sufficiently small ball around (so that it doesn't intersect the -axis), I claim that is a diffeomorphism with domain and codomain . I also claim that Now, trying to generalize this to arbitrary seems difficult to me, as I have no idea how rotations in dimensions work. My questions are: Is the proof for valid? If not,  please help me correct it. How can one prove the general case? Thank you!","\mathbb{K}^n=\{x \in \mathbb{R}^n:x^1=0 \text{ and }x^2>0,\dots,x^{n-1}>0\} M \subseteq \mathbb{K}^n k N M x^1=\cdots=x^{n-1}=0 N (k+1) n=3 \mathbb{K}^3=\{(x,y,z) \in \mathbb{R}^3:x=0,y>0\} \mathbf{x} z \theta(\mathbf{x}) \mathbf{x} [xy] x \theta M 1 p \in M U_p \ni p,V_p \subseteq \mathbb{R}^3 h_p:U_p \to V_p h_p(U_p \cap M)=V_p \cap (\mathbb{R}^1 \times \{0\}^2 ) q \in N k_q(\mathbf{x}):=[R_z(\theta(\mathbf{x})) \circ h \circ R_z(-\theta(\mathbf{x}))](\mathbf{x}) R_z z \theta q p \mathbb{K}^3 q z U_p p z k_q \overline{U}_q:=R_z(\theta(q))[U_p] \overline{V}_q:=k_q(\overline{U}_q) k_q(\overline{U}_q \cap N)=\overline{V}_q \cap (\mathbb{R}^2 \times \{0\}) . n \geq 4 n=3","['analysis', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
40,Cauchy Criteria for Series,Cauchy Criteria for Series,,"We know that the Cauchy Criterion of a series is as follow: Theorem : A series $\sum\limits_{i=1}^{\infty}x_i$ converges iff for all $\epsilon>0$ there is an $N\in \mathbb{N}$ so that for all $n> m > N$ we have $|\sum\limits_{i=m}^{n} x_i|< \epsilon$. The proof follows from the fact that ""A sequence is convergent if and only if it is Cauchy"". So $(s_n)$ is a Cauchy sequence, where $s_n=\sum\limits_{i=1}^n x_i$. From the definition of Cauchy sequence, given $\epsilon>0$, there exists $N \in \mathbb{N}$ such that for all $n > m > N$, $|s_n - s_m|< \epsilon$. I think it should be  $|s_n- s_m|=|\sum\limits_{i=m+1}^{n} x_i|< \epsilon$ how could we write $|s_n- s_m|=|\sum\limits_{i=m}^{n} x_i|< \epsilon$ ? Are these same things ? What is the point that I missed?","We know that the Cauchy Criterion of a series is as follow: Theorem : A series $\sum\limits_{i=1}^{\infty}x_i$ converges iff for all $\epsilon>0$ there is an $N\in \mathbb{N}$ so that for all $n> m > N$ we have $|\sum\limits_{i=m}^{n} x_i|< \epsilon$. The proof follows from the fact that ""A sequence is convergent if and only if it is Cauchy"". So $(s_n)$ is a Cauchy sequence, where $s_n=\sum\limits_{i=1}^n x_i$. From the definition of Cauchy sequence, given $\epsilon>0$, there exists $N \in \mathbb{N}$ such that for all $n > m > N$, $|s_n - s_m|< \epsilon$. I think it should be  $|s_n- s_m|=|\sum\limits_{i=m+1}^{n} x_i|< \epsilon$ how could we write $|s_n- s_m|=|\sum\limits_{i=m}^{n} x_i|< \epsilon$ ? Are these same things ? What is the point that I missed?",,"['sequences-and-series', 'analysis']"
41,"Poincar Lemma, differential forms and I do have troubles","Poincar Lemma, differential forms and I do have troubles",,"I think I need some hints about a proof I am currently reading in order to understand it. This question is similar to the construction used in Lemma 17.9 in the book ""Introduction to smooth manifolds"" by John Lee. Unfortunately, I am currently not at the level to understand his notation, although it would probably answer my question. Let $\omega: M \times [0,1]$ be a differential 2-form ( $M$ is a smooth manifold), then I want to transform it into a  1-form on $M$ by a linear map $I : \Omega^2(M \times [0,1]) \rightarrow \Omega^1(M).$ So the idea is somehow to integrate with an operator $I$ over the $[0,1]$ part in order to get the right differential form. First, I notice that each $2-$ form on $M \times[0,1]$ is necessarily of the form  $\omega = \omega_1 \wedge dt + \omega_2,$ where $\omega_1$ is a 1-form depending only on coordinates of the manifold, $t$ is the coordinate of the interval here and $\omega_2$ is a 2-form depending only on coordinates of $M$. Clearly, by skew symmetry of the wedge product, there cannot be a 2-form with $dt \wedge dt.$ Now, the big question is: How do I define $I(\omega)$ to get the following results; cause I am currently reading some lecture notes (not online unfortunately) where exactly such a construction is used, and it is said that its integration with respect to the $[0,1]$ part, but it is not explicitely mentioned how it is defined, so what this operator explicitly does to a form on $M \times [0,1]$. Altogether, I am looking for an explanation of what this operator $I$ exactly does. Let me provide you with the calculation given in the notes: So $\omega_1 = \sum_{i=1}^{n} a_i(x,t) dx_i,$ where $x_1,...,x_n$ are coordinates of the manifold $M$ and $\omega_2 = \sum_{i<j} b_{i,j}(x,t)dx_i \wedge dx_j$ Now, it is said that $I(\omega ) = \int_0^1 \omega_{p-1} dt.$ So somehow this operator does not affect $\omega_p$ if I get this correctly(maybe because there is no $dt$). Furthermore, for $d \omega = d \omega_{p-1} \wedge dt + d \omega_{p}$ which can be written in coordinates as $$ d\omega_{p-1} \wedge dt= \sum_{i=1}^{n} \sum_{j=1}^{n} \partial_{x_j} a_i(x,t) dx_j\wedge dx_i \wedge dt$$ and  $$d \omega_p = \sum_{i<j} \left(\sum_{k=1}^{n} \partial_{x_k}b_{i,j}(x,t) dx_k \wedge dx_i \wedge dx_j +  \partial_t b_{i,j}(x,t)dt\wedge dx_i \wedge dx_j \right),$$  we get by linearity $I(d\omega) = I( d \omega_p) + I (d\omega_{p-1}\wedge dt)$ which shall altogether give us: $d(I(\omega))-I(d\omega) = (-1) (\sum_{i<j} b_{i,j}(x,1)-b_{i,j}(x,0)dx_i\wedge dx_j).$ This is what the lecture notes contain, can we tell from this how $I$ exactly acts on forms on $M \times[0,1]$? Especially, why this operator apparently gives zero, if the form does not depend on $t$ via $dt$, although $a,b$ depend on $t$ in general? If anything is unclear, please let me know.","I think I need some hints about a proof I am currently reading in order to understand it. This question is similar to the construction used in Lemma 17.9 in the book ""Introduction to smooth manifolds"" by John Lee. Unfortunately, I am currently not at the level to understand his notation, although it would probably answer my question. Let $\omega: M \times [0,1]$ be a differential 2-form ( $M$ is a smooth manifold), then I want to transform it into a  1-form on $M$ by a linear map $I : \Omega^2(M \times [0,1]) \rightarrow \Omega^1(M).$ So the idea is somehow to integrate with an operator $I$ over the $[0,1]$ part in order to get the right differential form. First, I notice that each $2-$ form on $M \times[0,1]$ is necessarily of the form  $\omega = \omega_1 \wedge dt + \omega_2,$ where $\omega_1$ is a 1-form depending only on coordinates of the manifold, $t$ is the coordinate of the interval here and $\omega_2$ is a 2-form depending only on coordinates of $M$. Clearly, by skew symmetry of the wedge product, there cannot be a 2-form with $dt \wedge dt.$ Now, the big question is: How do I define $I(\omega)$ to get the following results; cause I am currently reading some lecture notes (not online unfortunately) where exactly such a construction is used, and it is said that its integration with respect to the $[0,1]$ part, but it is not explicitely mentioned how it is defined, so what this operator explicitly does to a form on $M \times [0,1]$. Altogether, I am looking for an explanation of what this operator $I$ exactly does. Let me provide you with the calculation given in the notes: So $\omega_1 = \sum_{i=1}^{n} a_i(x,t) dx_i,$ where $x_1,...,x_n$ are coordinates of the manifold $M$ and $\omega_2 = \sum_{i<j} b_{i,j}(x,t)dx_i \wedge dx_j$ Now, it is said that $I(\omega ) = \int_0^1 \omega_{p-1} dt.$ So somehow this operator does not affect $\omega_p$ if I get this correctly(maybe because there is no $dt$). Furthermore, for $d \omega = d \omega_{p-1} \wedge dt + d \omega_{p}$ which can be written in coordinates as $$ d\omega_{p-1} \wedge dt= \sum_{i=1}^{n} \sum_{j=1}^{n} \partial_{x_j} a_i(x,t) dx_j\wedge dx_i \wedge dt$$ and  $$d \omega_p = \sum_{i<j} \left(\sum_{k=1}^{n} \partial_{x_k}b_{i,j}(x,t) dx_k \wedge dx_i \wedge dx_j +  \partial_t b_{i,j}(x,t)dt\wedge dx_i \wedge dx_j \right),$$  we get by linearity $I(d\omega) = I( d \omega_p) + I (d\omega_{p-1}\wedge dt)$ which shall altogether give us: $d(I(\omega))-I(d\omega) = (-1) (\sum_{i<j} b_{i,j}(x,1)-b_{i,j}(x,0)dx_i\wedge dx_j).$ This is what the lecture notes contain, can we tell from this how $I$ exactly acts on forms on $M \times[0,1]$? Especially, why this operator apparently gives zero, if the form does not depend on $t$ via $dt$, although $a,b$ depend on $t$ in general? If anything is unclear, please let me know.",,"['real-analysis', 'analysis', 'differential-geometry', 'differential-topology', 'differential-forms']"
42,"A possible inequality related to binomial theorem (or, convex/concave functions)","A possible inequality related to binomial theorem (or, convex/concave functions)",,"Let $x, \ y, \ p$ be any real numbers with $x>0$, $y>0$, and $p>1$. The question is about (most probably) an elementary inequality: Is it always true that  $x^p+y^p\leq (x+y)^p$ ? Note that if $p$ is any positive integer, then the above inequality is obviously correct. What about if the number $p \ (\text{with} \ p>1)$ is any non-integer real number? I guess that (by my intuition) the answer should be positive. But how can we proceed to prove this inequality?","Let $x, \ y, \ p$ be any real numbers with $x>0$, $y>0$, and $p>1$. The question is about (most probably) an elementary inequality: Is it always true that  $x^p+y^p\leq (x+y)^p$ ? Note that if $p$ is any positive integer, then the above inequality is obviously correct. What about if the number $p \ (\text{with} \ p>1)$ is any non-integer real number? I guess that (by my intuition) the answer should be positive. But how can we proceed to prove this inequality?",,"['real-analysis', 'analysis', 'inequality']"
43,proving recursively defined sequence by induction,proving recursively defined sequence by induction,,"I would like to prove the following recursively defined sequence from $n-1$ to $n$ by induction. Im not realy sure about it. Any help or alternative ways to understand and prove it are highly appreciated : $0,1,4,12,35,98$ $a_0=0$, $a_1=1$, $a_n=a_{n-1}+5a_{n-2}+3$ for  $n\geq2$ To prove $a_n\leq 3^n$ I thougt of it as: $a_{n-1}\leq 3^{n-1}$, $a_{n-2}\leq 3^{n-2}$ and thus: $a_n\leq 3^{n-1} + 5\cdot 3^{n-2}+3$ $=3^{n-2} \cdot(3+5)+3$ $=3^{n-2} \cdot(8)+3$ $=3^{n-2} \cdot(9)+3$ $=3^{n-2} \cdot(3 \cdot 3)+3$ $=3^{n}+3$ $\leq 3^{n}$","I would like to prove the following recursively defined sequence from $n-1$ to $n$ by induction. Im not realy sure about it. Any help or alternative ways to understand and prove it are highly appreciated : $0,1,4,12,35,98$ $a_0=0$, $a_1=1$, $a_n=a_{n-1}+5a_{n-2}+3$ for  $n\geq2$ To prove $a_n\leq 3^n$ I thougt of it as: $a_{n-1}\leq 3^{n-1}$, $a_{n-2}\leq 3^{n-2}$ and thus: $a_n\leq 3^{n-1} + 5\cdot 3^{n-2}+3$ $=3^{n-2} \cdot(3+5)+3$ $=3^{n-2} \cdot(8)+3$ $=3^{n-2} \cdot(9)+3$ $=3^{n-2} \cdot(3 \cdot 3)+3$ $=3^{n}+3$ $\leq 3^{n}$",,['analysis']
44,Convergence of monotone decreasing series $\sum_{n=1}^\infty a_n < \infty \iff \sum_{n=1}^\infty 2^na_{2^n} < \infty$,Convergence of monotone decreasing series,\sum_{n=1}^\infty a_n < \infty \iff \sum_{n=1}^\infty 2^na_{2^n} < \infty,"Suppose $\{a_n\}$ is a monotone decreasing sequence of positive terms.  Prove that $$\sum_{n=1}^\infty a_n \text{ converges } \iff \sum_{n=1}^\infty 2^na_{2^n} \text{ converges}$$ thought about the integral test but there's no function to integrate, can't assume $a_n\to 0$ implies convergence, and having a tough time using comparison.  Would ratio test do it?","Suppose $\{a_n\}$ is a monotone decreasing sequence of positive terms.  Prove that $$\sum_{n=1}^\infty a_n \text{ converges } \iff \sum_{n=1}^\infty 2^na_{2^n} \text{ converges}$$ thought about the integral test but there's no function to integrate, can't assume $a_n\to 0$ implies convergence, and having a tough time using comparison.  Would ratio test do it?",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
45,Boundary on a manifold,Boundary on a manifold,,"I was wondering how I can see if a manifold has a boundary just by looking at the surface? The thing is that I want to understand how to apply the Gau Bonnet theorem to surfaces and there I need to integrate over the boundary, too. If you are just dealing with sets in $\mathbb{R}^n$ it is pretty simple to find out if a point is on the boundary, as soon as you have a visual picture of the set, cause you can just ask yourself if this point would be in the closure of the complement or if there is an open set containig this point which is still part of the set. This seems to be no longer possible for manifolds. Despite, it seems to be still possible to say that no point on the sphere for example is not in the interior which is different from the triangle in the plane, where the edges are visually not part of the interior. So somehow this boils down to: Think you are standing on a $n-$ dimensional manifold and you are standing at a point $p$. If you can walk in any of the $n-$ directions, this point is not part of the boundary, otherwise it is. Still, I don't like this characterization, because a cube for example does not have a boundary, although standing on any of the edges might give you the idea, that this should be part of a boundary. So my question is: Does anybody have a good way to find out if a point belongs to the boundary of a surface or not?","I was wondering how I can see if a manifold has a boundary just by looking at the surface? The thing is that I want to understand how to apply the Gau Bonnet theorem to surfaces and there I need to integrate over the boundary, too. If you are just dealing with sets in $\mathbb{R}^n$ it is pretty simple to find out if a point is on the boundary, as soon as you have a visual picture of the set, cause you can just ask yourself if this point would be in the closure of the complement or if there is an open set containig this point which is still part of the set. This seems to be no longer possible for manifolds. Despite, it seems to be still possible to say that no point on the sphere for example is not in the interior which is different from the triangle in the plane, where the edges are visually not part of the interior. So somehow this boils down to: Think you are standing on a $n-$ dimensional manifold and you are standing at a point $p$. If you can walk in any of the $n-$ directions, this point is not part of the boundary, otherwise it is. Still, I don't like this characterization, because a cube for example does not have a boundary, although standing on any of the edges might give you the idea, that this should be part of a boundary. So my question is: Does anybody have a good way to find out if a point belongs to the boundary of a surface or not?",,"['real-analysis', 'analysis']"
46,"Identity involving ""the distance to the nearest integer"" function","Identity involving ""the distance to the nearest integer"" function",,"It's a problem of an exercise list: I want to prove that, if $||x||$ = the distance to the nearest integer to $x$, then: $$\sum_{n=1}^{\infty}4^{-n}||2^nx||=||x||(1-2||x||)$$ is true for every $x \in \mathbb{R}/\mathbb{Z}$. What I've tried so far: We know that the set $D=\{\sum_{k=1}^T\frac{a_k}{2^k}$, where $a_i = 0, 1 \forall i \in \{1,2,\ldots,T\}; T \in \mathbb{N}\}$ is dense in $(0,1)$. So, it's enought to prove the identity for these numbers. Because then, we can use the Weierstrass M Test to guarantee that the identity is valid to any other number in $(0,1)$. Why these specific numbers? Because then, the infinity sum $\sum_{n=1}^{\infty}4^{-n}||2^nx||$ will become finity and it will be easier to work with. So, considering numbers of the form above, we obtain that $$\sum_{n=1}^{\infty}4^{-n}||2^nx||=\sum_{n=1}^{\infty}4^{-n}\left|\left|2^n\sum_{k=1}^T\frac{a_k}{2^k}\right|\right| = \sum_{k=1}^{T-1}4^{-n}\left|\left|2^n\sum_{k=1}^{T}\frac{a_k}{2^k}\right|\right|. $$ Firt of all, the way I constructed that specific numbers, we can't obtain 1, but that's OK, because in $\mathbb{R}/\mathbb{Z}$ is 0. Now, note that: $||x||=x$, if $0 \leq x \leq 1/2$ and $||x|| = 1- x$, if $1/2 < x <1$ (Remember we are working in $\mathbb{R}/\mathbb{Z}$). So, if we define $S_n=2^n\sum_{k=1}^{T-1}\frac{a_k}{2^k}$, we get that $||S_n||=||2^n\sum_{k=1}^{T-1}\frac{a_k}{2^k}||=a_{n+1}-2a_{n+1}S_n+S_n$, because if $a_{n+1}=0$, $S_n \leq 1/2$ and if $a_{n+1}=1$, $S_n > 1/2$. So, $$\sum_{n=1}^{\infty}4^{-n}\left|\left|2^n\sum_{k=1}^T\frac{a_k}{2^k}\right|\right|=\sum_{n=1}^{T-1}4^{-n}(a_{n+1}-2a_{n+1}S_n + S_n)$$ $$= \sum_{n=1}^{T-1}\frac{a_{n+1}}{4^n}-\sum_{n=1}^{T-1}\frac{1}{4^n}(2a_{n+1}S_n - S_n)$$ $$=\sum_{n=1}^{T-1}\frac{a_{n+1}}{4^n} + \sum_{n=1}^{T-1}\frac{S_n}{4^n}(1-2a_{n+1})$$ $$=\sum_{n=1}^{T-1}\frac{a_{n+1}}{4^n}+\sum_{n=1}^{T-1}\sum_{i=1}^{T}\frac{a_i}{2^i}\left(\frac{1-2a_{n+1}}{2^n}\right)$$. On the other hand, we have that $$||x||(1-2||x||)=\sum_{k=2}^{T}\frac{a_k}{2^k}-2\left(\sum_{k=2}^{T}\frac{a_k}{2^k}\right)^2$$. I'm having trouble finding a way to prove that the two expressions are, indeed, equal. I tried to write the last expression as an open product of two series, but it didn't work... Does someone have a hint or a solution? Thanks!","It's a problem of an exercise list: I want to prove that, if $||x||$ = the distance to the nearest integer to $x$, then: $$\sum_{n=1}^{\infty}4^{-n}||2^nx||=||x||(1-2||x||)$$ is true for every $x \in \mathbb{R}/\mathbb{Z}$. What I've tried so far: We know that the set $D=\{\sum_{k=1}^T\frac{a_k}{2^k}$, where $a_i = 0, 1 \forall i \in \{1,2,\ldots,T\}; T \in \mathbb{N}\}$ is dense in $(0,1)$. So, it's enought to prove the identity for these numbers. Because then, we can use the Weierstrass M Test to guarantee that the identity is valid to any other number in $(0,1)$. Why these specific numbers? Because then, the infinity sum $\sum_{n=1}^{\infty}4^{-n}||2^nx||$ will become finity and it will be easier to work with. So, considering numbers of the form above, we obtain that $$\sum_{n=1}^{\infty}4^{-n}||2^nx||=\sum_{n=1}^{\infty}4^{-n}\left|\left|2^n\sum_{k=1}^T\frac{a_k}{2^k}\right|\right| = \sum_{k=1}^{T-1}4^{-n}\left|\left|2^n\sum_{k=1}^{T}\frac{a_k}{2^k}\right|\right|. $$ Firt of all, the way I constructed that specific numbers, we can't obtain 1, but that's OK, because in $\mathbb{R}/\mathbb{Z}$ is 0. Now, note that: $||x||=x$, if $0 \leq x \leq 1/2$ and $||x|| = 1- x$, if $1/2 < x <1$ (Remember we are working in $\mathbb{R}/\mathbb{Z}$). So, if we define $S_n=2^n\sum_{k=1}^{T-1}\frac{a_k}{2^k}$, we get that $||S_n||=||2^n\sum_{k=1}^{T-1}\frac{a_k}{2^k}||=a_{n+1}-2a_{n+1}S_n+S_n$, because if $a_{n+1}=0$, $S_n \leq 1/2$ and if $a_{n+1}=1$, $S_n > 1/2$. So, $$\sum_{n=1}^{\infty}4^{-n}\left|\left|2^n\sum_{k=1}^T\frac{a_k}{2^k}\right|\right|=\sum_{n=1}^{T-1}4^{-n}(a_{n+1}-2a_{n+1}S_n + S_n)$$ $$= \sum_{n=1}^{T-1}\frac{a_{n+1}}{4^n}-\sum_{n=1}^{T-1}\frac{1}{4^n}(2a_{n+1}S_n - S_n)$$ $$=\sum_{n=1}^{T-1}\frac{a_{n+1}}{4^n} + \sum_{n=1}^{T-1}\frac{S_n}{4^n}(1-2a_{n+1})$$ $$=\sum_{n=1}^{T-1}\frac{a_{n+1}}{4^n}+\sum_{n=1}^{T-1}\sum_{i=1}^{T}\frac{a_i}{2^i}\left(\frac{1-2a_{n+1}}{2^n}\right)$$. On the other hand, we have that $$||x||(1-2||x||)=\sum_{k=2}^{T}\frac{a_k}{2^k}-2\left(\sum_{k=2}^{T}\frac{a_k}{2^k}\right)^2$$. I'm having trouble finding a way to prove that the two expressions are, indeed, equal. I tried to write the last expression as an open product of two series, but it didn't work... Does someone have a hint or a solution? Thanks!",,"['real-analysis', 'sequences-and-series', 'analysis']"
47,subsequence converges to L implies L is a limit point of sequence,subsequence converges to L implies L is a limit point of sequence,,"Proposition: Let $(a_n)^\infty_0$ be a sequence of real numbers, and let $L$ be a real number. Then the following two statements are logically equivalent: (a) $L$ is a limit point of $(a_n)^\infty_0$ (b) There exists a subsequence of $(a_n)^\infty_0$ which converges to $L$. I am trying to show that (b) implies (a). I am confused how to relate the different epsilons and Ns together to serve the proof: Proof : Let $(b_n)^\infty_0$ be a subsequence that converges to $L$: $\forall \epsilon> 0$ there exists an $N$ such that $$|b_n  L|\leq\epsilon,\forall n\geq N$$ For $L$ to be a limit point of the sequence $(a_n)^\infty_0$, we need: $$\forall \epsilon >0, \forall N\geq0, \exists n\geq N,\text{ such that }|a_n  L| \epsilon$$ I am confused how to continue. Should I fix an epsilon and relate the Ns?","Proposition: Let $(a_n)^\infty_0$ be a sequence of real numbers, and let $L$ be a real number. Then the following two statements are logically equivalent: (a) $L$ is a limit point of $(a_n)^\infty_0$ (b) There exists a subsequence of $(a_n)^\infty_0$ which converges to $L$. I am trying to show that (b) implies (a). I am confused how to relate the different epsilons and Ns together to serve the proof: Proof : Let $(b_n)^\infty_0$ be a subsequence that converges to $L$: $\forall \epsilon> 0$ there exists an $N$ such that $$|b_n  L|\leq\epsilon,\forall n\geq N$$ For $L$ to be a limit point of the sequence $(a_n)^\infty_0$, we need: $$\forall \epsilon >0, \forall N\geq0, \exists n\geq N,\text{ such that }|a_n  L| \epsilon$$ I am confused how to continue. Should I fix an epsilon and relate the Ns?",,"['real-analysis', 'analysis', 'proof-writing']"
48,Mean value theorem for line integral,Mean value theorem for line integral,,"I am wondering if there is a mean value theorem for line integral. For example, let $f(x):\mathbb{R}^n\rightarrow \mathbb{R}$ be a continuous (not necessarily monotonic) function defined on smooth curve $C$, do we have the following theorem: $F(x)=\int_Cf(x)dx=f(x_0)\cdot L_C$ where $x_0$ is some point on the curve $C$ (i.e., $x_0\in C$ ) and $L_C$ is the length of the curve $C$? In addition, if $f(x)=\nabla F(x)$, do we have $F(a)-F(b)=||f(x_0)||\cdot L_C$, where $||\cdot||$ is some norm (e.g., $\ell_2$ norm)? Thanks.","I am wondering if there is a mean value theorem for line integral. For example, let $f(x):\mathbb{R}^n\rightarrow \mathbb{R}$ be a continuous (not necessarily monotonic) function defined on smooth curve $C$, do we have the following theorem: $F(x)=\int_Cf(x)dx=f(x_0)\cdot L_C$ where $x_0$ is some point on the curve $C$ (i.e., $x_0\in C$ ) and $L_C$ is the length of the curve $C$? In addition, if $f(x)=\nabla F(x)$, do we have $F(a)-F(b)=||f(x_0)||\cdot L_C$, where $||\cdot||$ is some norm (e.g., $\ell_2$ norm)? Thanks.",,"['analysis', 'multivariable-calculus']"
49,When is an oscillating integral small?,When is an oscillating integral small?,,"I hope, the title is not too confusing. My question is the following: We all know the Riemann-Lebesgue-Lemma stating that for $f\in L^1(\mathbb R)$, one has $$ \lim_{k\to\infty} \int f(x)\,e^{ikx}\,dx=0. $$ Intuitively this means that the fast oscillation of $e^{ikx}$ makes the integral small. Question: Can one show something like $$\int f(x)\,e^{i\phi(x)}\,dx \leq \int f(x)\,e^{i\psi(x)}\,dx, $$ if $\phi>\psi$? (or maybe, if $\phi-\psi\geq\text{(some constant)}$, or so) Thanks, Frank","I hope, the title is not too confusing. My question is the following: We all know the Riemann-Lebesgue-Lemma stating that for $f\in L^1(\mathbb R)$, one has $$ \lim_{k\to\infty} \int f(x)\,e^{ikx}\,dx=0. $$ Intuitively this means that the fast oscillation of $e^{ikx}$ makes the integral small. Question: Can one show something like $$\int f(x)\,e^{i\phi(x)}\,dx \leq \int f(x)\,e^{i\psi(x)}\,dx, $$ if $\phi>\psi$? (or maybe, if $\phi-\psi\geq\text{(some constant)}$, or so) Thanks, Frank",,"['integration', 'analysis', 'fourier-analysis']"
50,"Determine whether $\sum\limits_{n=1}^\infty \frac{1}{n^x}$ converges uniformly on $(1,\infty)$",Determine whether  converges uniformly on,"\sum\limits_{n=1}^\infty \frac{1}{n^x} (1,\infty)","Detemine whether $\sum\limits_{n=1}^\infty \frac{1}{n^x}$ converges   uniformly on $(1,\infty)$. My attempt: Upon attempting to use the Weierstrauss M-test I get $$0\leqslant\|f_n(x)\|_\infty=\sup_{x\in (1,\infty)}|\frac{1}{n^x}|\leqslant\frac{1}{n}=M_n$$ But by definition, $\sum\limits_{n=1}^\infty M_n=\sum\limits_{n=1}^\infty \frac{1}{n}$ diverges. So the Weierstrauss M-test is not useful here. Is there some way I could possibly use the uniform Cauchy principle? Thanks for the help.","Detemine whether $\sum\limits_{n=1}^\infty \frac{1}{n^x}$ converges   uniformly on $(1,\infty)$. My attempt: Upon attempting to use the Weierstrauss M-test I get $$0\leqslant\|f_n(x)\|_\infty=\sup_{x\in (1,\infty)}|\frac{1}{n^x}|\leqslant\frac{1}{n}=M_n$$ But by definition, $\sum\limits_{n=1}^\infty M_n=\sum\limits_{n=1}^\infty \frac{1}{n}$ diverges. So the Weierstrauss M-test is not useful here. Is there some way I could possibly use the uniform Cauchy principle? Thanks for the help.",,"['real-analysis', 'analysis', 'uniform-convergence', 'cauchy-sequences']"
51,question about property of $L^p$ Lipschitz space,question about property of  Lipschitz space,L^p,"$f\in L^p$ is said to satisfy $L^p$ Lipschitz condition of order $\alpha$ if there exists $C>0$ such that $\displaystyle|h|^{-\alpha}\Big(\int_{\mathbb{R}^d}|f(x-h)-f(x)|^p \,dx\Big)^\frac{1}{p}\leq C$ for every $h\neq0$ In this case, we write $f\in\Lambda_\alpha^p$ and define $\lVert f\rVert_{\Lambda_\alpha^p}:=\lVert f\rVert_{L^p}+\sup\limits_{h\neq 0} \displaystyle|h|^{-\alpha}\Big(\int_{\mathbb{R}^d}|f(x-h)-f(x)|^p \,dx\Big)^\frac{1}{p}$ This is the definition of $L^p$ Lipschitz function that I first saw in some lecture note. But when I tried to search information about this space, I could not find any reference or wiki document. I would like to know if there is a density argument relating schwartz function and $L^p$ Lipschitz space(with respect to $\Lambda_\alpha^p$ norm). Does anybody know good reference about this? My question arose when studying the following theorem. Let $f\in \Lambda_\alpha^1$ and $\displaystyle p<1+\frac{\alpha}{d}$. Then, $f\in L^p$ and $\lVert f \rVert_{L^p}\leq B\lVert f\rVert_{\Lambda_\alpha^1}$ In the lecture note it is proved that this is true if $f$ is in addition schwartz function. But it does not say more and ends proof, so I was wondering if any density argument was used here. (The argument in the lecture note proceeded as follows. $S$ and $S'$ will denote the space of schwartz function and the space of tempered distribution respectively. It defines particular $\psi_j\in S'(j\in\mathbb{N})$ such that  $\displaystyle\sum_{j=1}^\infty\psi_j=\delta_0$ where the limit is in $S'$sense. Then $f=f*\delta_0=f*\sum_{j=1}^\infty\psi_j$ pointwisely. (Note that this makes sense only when $f\in S$). using particular $\psi_j$, author shows that the last term converges to some function(say $g$) in $L^p$ sense, and this function $g$ should be $f$. The whole argument is contained in Lemma 4.22 of http://www.mat.unimi.it/users/peloso/Matematica/ha-aa1011.pdf )","$f\in L^p$ is said to satisfy $L^p$ Lipschitz condition of order $\alpha$ if there exists $C>0$ such that $\displaystyle|h|^{-\alpha}\Big(\int_{\mathbb{R}^d}|f(x-h)-f(x)|^p \,dx\Big)^\frac{1}{p}\leq C$ for every $h\neq0$ In this case, we write $f\in\Lambda_\alpha^p$ and define $\lVert f\rVert_{\Lambda_\alpha^p}:=\lVert f\rVert_{L^p}+\sup\limits_{h\neq 0} \displaystyle|h|^{-\alpha}\Big(\int_{\mathbb{R}^d}|f(x-h)-f(x)|^p \,dx\Big)^\frac{1}{p}$ This is the definition of $L^p$ Lipschitz function that I first saw in some lecture note. But when I tried to search information about this space, I could not find any reference or wiki document. I would like to know if there is a density argument relating schwartz function and $L^p$ Lipschitz space(with respect to $\Lambda_\alpha^p$ norm). Does anybody know good reference about this? My question arose when studying the following theorem. Let $f\in \Lambda_\alpha^1$ and $\displaystyle p<1+\frac{\alpha}{d}$. Then, $f\in L^p$ and $\lVert f \rVert_{L^p}\leq B\lVert f\rVert_{\Lambda_\alpha^1}$ In the lecture note it is proved that this is true if $f$ is in addition schwartz function. But it does not say more and ends proof, so I was wondering if any density argument was used here. (The argument in the lecture note proceeded as follows. $S$ and $S'$ will denote the space of schwartz function and the space of tempered distribution respectively. It defines particular $\psi_j\in S'(j\in\mathbb{N})$ such that  $\displaystyle\sum_{j=1}^\infty\psi_j=\delta_0$ where the limit is in $S'$sense. Then $f=f*\delta_0=f*\sum_{j=1}^\infty\psi_j$ pointwisely. (Note that this makes sense only when $f\in S$). using particular $\psi_j$, author shows that the last term converges to some function(say $g$) in $L^p$ sense, and this function $g$ should be $f$. The whole argument is contained in Lemma 4.22 of http://www.mat.unimi.it/users/peloso/Matematica/ha-aa1011.pdf )",,"['analysis', 'harmonic-analysis', 'besov-space']"
52,Rearrangement of absolutely convergent series,Rearrangement of absolutely convergent series,,"I would be very grateful if someone would verify whether my proof below is correct. Many thanks. Theorem. $\,$ Let $(b_k)$ be a rearrangement of the complex sequence $(a_k)$. If $\sum_{k\geq 0}a_k = s$ and is absolutely convergent, then $\sum_{k\geq 0}b_k = s$. Proof. $\,$ Let $\varepsilon>0$ be given. Choose $n\geq 0$ such that $$\sum_{k=n+1}^\infty|a_k|<\varepsilon.$$ Choose $N\geq 0$ such that $$\{a_1,\ldots,a_n\}\subseteq\{b_1,\ldots,b_N\},$$ Then, for any $m\geq N$, we have that $$\sum_{k=0}^{\infty}a_k-\sum_{k=0}^mb_k=\sum_{k\in A_m}a_k.$$ where $A_m=\{n+1,n+2,\ldots\}\setminus\{\text{finitely many points}\}$. Hence, for any $m\geq N$, it follows that $$\left|\sum_{k=0}^{\infty}a_k-\sum_{k=0}^mb_k\right|\leq\sum_{k=n+1}^\infty|a_k|<\varepsilon.$$ This is what we were required to prove.","I would be very grateful if someone would verify whether my proof below is correct. Many thanks. Theorem. $\,$ Let $(b_k)$ be a rearrangement of the complex sequence $(a_k)$. If $\sum_{k\geq 0}a_k = s$ and is absolutely convergent, then $\sum_{k\geq 0}b_k = s$. Proof. $\,$ Let $\varepsilon>0$ be given. Choose $n\geq 0$ such that $$\sum_{k=n+1}^\infty|a_k|<\varepsilon.$$ Choose $N\geq 0$ such that $$\{a_1,\ldots,a_n\}\subseteq\{b_1,\ldots,b_N\},$$ Then, for any $m\geq N$, we have that $$\sum_{k=0}^{\infty}a_k-\sum_{k=0}^mb_k=\sum_{k\in A_m}a_k.$$ where $A_m=\{n+1,n+2,\ldots\}\setminus\{\text{finitely many points}\}$. Hence, for any $m\geq N$, it follows that $$\left|\sum_{k=0}^{\infty}a_k-\sum_{k=0}^mb_k\right|\leq\sum_{k=n+1}^\infty|a_k|<\varepsilon.$$ This is what we were required to prove.",,"['sequences-and-series', 'analysis', 'proof-verification', 'solution-verification']"
53,How is the interchange of the limit and the maximum valid at this point in Erwin Kreyszig?,How is the interchange of the limit and the maximum valid at this point in Erwin Kreyszig?,,"In 1.5-5 in Erwin Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS, the author shows completeness of the space $C[a,b]$ of all (real- or complex-valued) functions defined and continuous on the closed interval $[a,b]$ on the real line with the metric $$d(x,y) \colon= \max_{t\in[a,b]} |x(t) - y(t)|.$$ Now during the course of the proof, Kryszeg uses the following result:  $$ \lim_{n\to\infty} \max_{t\in[a,b]} | x_m(t) - x_n(t)| = \max_{t\in[a,b]} | x_m(t) - \lim_{n\to\infty} x_n(t)| .$$ How is this sort of an interchange of the limit and the maximum valid? How to rigorously prove this? To sum up, how to rigorously prove that  $$\lim_{n\to\infty} \max_{t\in[a,b]} |x_n(t)| = \max_{t\in[a,b]} |\lim_{n\to\infty} x_n(t)|, $$ where $x_n \in C[a,b]$ for $n=1,2,3,\ldots$.","In 1.5-5 in Erwin Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS, the author shows completeness of the space $C[a,b]$ of all (real- or complex-valued) functions defined and continuous on the closed interval $[a,b]$ on the real line with the metric $$d(x,y) \colon= \max_{t\in[a,b]} |x(t) - y(t)|.$$ Now during the course of the proof, Kryszeg uses the following result:  $$ \lim_{n\to\infty} \max_{t\in[a,b]} | x_m(t) - x_n(t)| = \max_{t\in[a,b]} | x_m(t) - \lim_{n\to\infty} x_n(t)| .$$ How is this sort of an interchange of the limit and the maximum valid? How to rigorously prove this? To sum up, how to rigorously prove that  $$\lim_{n\to\infty} \max_{t\in[a,b]} |x_n(t)| = \max_{t\in[a,b]} |\lim_{n\to\infty} x_n(t)|, $$ where $x_n \in C[a,b]$ for $n=1,2,3,\ldots$.",,"['calculus', 'real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence']"
54,"checking slope = $0$ at a point for a function using $\epsilon $, $\delta $ definition","checking slope =  at a point for a function using ,  definition",0 \epsilon  \delta ,"From the continuity definition, a function is continuous at a point $a$ if  : $$\forall \epsilon \gt 0 \exists \delta \gt 0  : |x-a| \lt \delta \implies |f(x)-f(a)|\lt \epsilon$$ If I change the order of quantifiers like below do I get a definition for checking slope = $0$ at $x = a$ ?  $$\exists \delta \gt 0   \forall \epsilon \gt 0 : |x-a| \lt \delta \implies |f(x)-f(a)|\lt \epsilon$$ If my interpretation of second quantifier order is flawed, could you please tell me how to interpret it correctly in this context ? I realized that order matters by going through other mse posts already.","From the continuity definition, a function is continuous at a point $a$ if  : $$\forall \epsilon \gt 0 \exists \delta \gt 0  : |x-a| \lt \delta \implies |f(x)-f(a)|\lt \epsilon$$ If I change the order of quantifiers like below do I get a definition for checking slope = $0$ at $x = a$ ?  $$\exists \delta \gt 0   \forall \epsilon \gt 0 : |x-a| \lt \delta \implies |f(x)-f(a)|\lt \epsilon$$ If my interpretation of second quantifier order is flawed, could you please tell me how to interpret it correctly in this context ? I realized that order matters by going through other mse posts already.",,"['calculus', 'real-analysis', 'analysis']"
55,"What is meant by ""on se ramne par rgularisation""?","What is meant by ""on se ramne par rgularisation""?",,"I am currently attempting to translate the paper 'Sur l'quation de convolution $\mu = \mu \ast \sigma$ ' by Choquet and Deny.  In the paper, a locally compact abelian group $G$ and a positive measure $\sigma$ on $G$ are given and the measures $\mu$ which satisfy the convolution equation above are found. I've mostly been able to translate the paper, but there are a few phrases which I think are important which I don't understand. The first phrase is Inversement pour montrer que toute solution $\mu$ borne est priodique on se ramne par rgularisation au cas o $\mu$ est une fonction $f$ borne uniformment continue sur $G$ , et l'on peut supposer G dnombrable  l'infini. In particular, the following part $\mu$ borne est priodique on se ramne par rgularisation au cas o $\mu$ est une fonction $f$ What is meant there? The second similar phrase I'm wondering about is ..., i.e. telles que les rgularises $\mu \ast \varphi$ soient des fonctions. Finally, I'm also wondering about donc telle que $g_0(x) = 2 \alpha$ en tout point $x$ du symtrique de $S_\sigma$ [support of $\sigma$ ] par rapport  l'origine O, done du semi-groupe engendr. I'd greatly appreciate any help.","I am currently attempting to translate the paper 'Sur l'quation de convolution ' by Choquet and Deny.  In the paper, a locally compact abelian group and a positive measure on are given and the measures which satisfy the convolution equation above are found. I've mostly been able to translate the paper, but there are a few phrases which I think are important which I don't understand. The first phrase is Inversement pour montrer que toute solution borne est priodique on se ramne par rgularisation au cas o est une fonction borne uniformment continue sur , et l'on peut supposer G dnombrable  l'infini. In particular, the following part borne est priodique on se ramne par rgularisation au cas o est une fonction What is meant there? The second similar phrase I'm wondering about is ..., i.e. telles que les rgularises soient des fonctions. Finally, I'm also wondering about donc telle que en tout point du symtrique de [support of ] par rapport  l'origine O, done du semi-groupe engendr. I'd greatly appreciate any help.",\mu = \mu \ast \sigma G \sigma G \mu \mu \mu f G \mu \mu f \mu \ast \varphi g_0(x) = 2 \alpha x S_\sigma \sigma,"['analysis', 'measure-theory', 'probability-distributions', 'translation-request', 'mathematical-french']"
56,Proof that the set of irrational numbers is dense in the reals,Proof that the set of irrational numbers is dense in the reals,,"The hint I was given was to simply prove that $y=xz$ is irrational given that $x$ is nonzero, $x$ is rational and $z$ is irrational.  Here's how I did it: Claim: $y=xz$ is irrational. Proof: Assume $x\neq0$, $x$ is rational and $z$ is irrational. By contradiction  assume that $y=xz$ is rational.  This means $y$ can be expressed as $m/n$, $m$ and $n$ being integers; $y$ can be expressed similarly as $p/q$, $p$ and $q$ being integers.  By substitution, we have that $$ p/q=mz/n$$ and $$z=pn/qm, qm \neq 0.$$ Since $pn$ and $qm$ are integers $z$ has to be rational. In addition to this it seems like there's a part 2 as follows: Proof: Given an interval $(x,y)$ we will choose a positive irrational number, $z$, say.  By density of the rationals there is a rational $p$ in the interval $(x/z, y/z)$ s.t. $$ x/z <p< y/z.$$  From this we see that $pz$ is irrational since it is the product of a rational and irrational number. Is the $pz$ the $xz$ that we proved is irrational in the first proof?  So ideally when presenting a full proof like this, should we do part 2 then part 1?","The hint I was given was to simply prove that $y=xz$ is irrational given that $x$ is nonzero, $x$ is rational and $z$ is irrational.  Here's how I did it: Claim: $y=xz$ is irrational. Proof: Assume $x\neq0$, $x$ is rational and $z$ is irrational. By contradiction  assume that $y=xz$ is rational.  This means $y$ can be expressed as $m/n$, $m$ and $n$ being integers; $y$ can be expressed similarly as $p/q$, $p$ and $q$ being integers.  By substitution, we have that $$ p/q=mz/n$$ and $$z=pn/qm, qm \neq 0.$$ Since $pn$ and $qm$ are integers $z$ has to be rational. In addition to this it seems like there's a part 2 as follows: Proof: Given an interval $(x,y)$ we will choose a positive irrational number, $z$, say.  By density of the rationals there is a rational $p$ in the interval $(x/z, y/z)$ s.t. $$ x/z <p< y/z.$$  From this we see that $pz$ is irrational since it is the product of a rational and irrational number. Is the $pz$ the $xz$ that we proved is irrational in the first proof?  So ideally when presenting a full proof like this, should we do part 2 then part 1?",,"['real-analysis', 'analysis', 'proof-verification']"
57,Generator of complex-valued functions vanishing at infinity,Generator of complex-valued functions vanishing at infinity,,"Let $C_0(\mathbb{R})$ be the $C^{\ast}$-algebra of continuous complex-valued functions vanishing at infinity, with involution given by $f^{\ast}(x) = \overline{f(x)}$. How can I prove that this commutative $C^{\ast}$-algebra is generated by the resolvent functions $f_{\pm}(x) = (i \pm x)^{-1}$?","Let $C_0(\mathbb{R})$ be the $C^{\ast}$-algebra of continuous complex-valued functions vanishing at infinity, with involution given by $f^{\ast}(x) = \overline{f(x)}$. How can I prove that this commutative $C^{\ast}$-algebra is generated by the resolvent functions $f_{\pm}(x) = (i \pm x)^{-1}$?",,"['abstract-algebra', 'analysis', 'c-star-algebras']"
58,Closed form of the inverse of a function,Closed form of the inverse of a function,,Does anyone know what the analytic form of the inverse of $f(x)=e^x+x$? Thanks in advance,Does anyone know what the analytic form of the inverse of $f(x)=e^x+x$? Thanks in advance,,"['calculus', 'analysis', 'inverse']"
59,Mixed partial derivatives are different,Mixed partial derivatives are different,,"Let $f: \Bbb R^2 \to \Bbb R$ be defined as $$f(x) = \left\{ \begin{matrix} x_1^2 \operatorname{arctan} \left( \frac{x_2}{x_1} \right) - x_2^2 \operatorname{arctan} \left( \frac{x_1}{x_2} \right), & x_1 x_2 \neq 0, \\ 0, & x_1 x_2 = 0. \end{matrix} \right.$$ Notation: $D_j f$ means the partial derivative with respect to the $j$-th coordinate. I have shown that $D_2 D_1 f(0) \neq D_1 D_2 f(0)$. My question : we know $f(x) = 0$ whenever $x_1 = 0$ or $x_2 = 0$. This implies that $D_1 f(0) = D_2 f(0) = 0$ applying the definition. However, would this mean that $D_1 f(x)$ or $D_2 f(x)$ equals zero whenever $x_1 = 0$ or $x_2 = 0$, in the inclusive sense of ""or""? I have used that $D_i f$ is not necessarily zero unless at the origin to show that the mixed partials are different. The expressions for them are $$\begin{align} D_1 f(x) & = 2x_1 \operatorname{arctan} \left( \frac{x_2}{x_1} \right) - x_2, \\ D_2 f(x) & = x_1 - 2x_2 \operatorname{arctan} \left( \frac{x_1}{x_2} \right). \end{align}$$ Applying the definition again to this I found $D_2 D_1 f(0) = -1$ and $D_1 D_2 f(0) = 1$.","Let $f: \Bbb R^2 \to \Bbb R$ be defined as $$f(x) = \left\{ \begin{matrix} x_1^2 \operatorname{arctan} \left( \frac{x_2}{x_1} \right) - x_2^2 \operatorname{arctan} \left( \frac{x_1}{x_2} \right), & x_1 x_2 \neq 0, \\ 0, & x_1 x_2 = 0. \end{matrix} \right.$$ Notation: $D_j f$ means the partial derivative with respect to the $j$-th coordinate. I have shown that $D_2 D_1 f(0) \neq D_1 D_2 f(0)$. My question : we know $f(x) = 0$ whenever $x_1 = 0$ or $x_2 = 0$. This implies that $D_1 f(0) = D_2 f(0) = 0$ applying the definition. However, would this mean that $D_1 f(x)$ or $D_2 f(x)$ equals zero whenever $x_1 = 0$ or $x_2 = 0$, in the inclusive sense of ""or""? I have used that $D_i f$ is not necessarily zero unless at the origin to show that the mixed partials are different. The expressions for them are $$\begin{align} D_1 f(x) & = 2x_1 \operatorname{arctan} \left( \frac{x_2}{x_1} \right) - x_2, \\ D_2 f(x) & = x_1 - 2x_2 \operatorname{arctan} \left( \frac{x_1}{x_2} \right). \end{align}$$ Applying the definition again to this I found $D_2 D_1 f(0) = -1$ and $D_1 D_2 f(0) = 1$.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
60,"Show that the set $\left\{\sin\frac{1}{2}x,\sin\frac{3}{2}x,\sin\frac{5}{2}x,\ldots\right\}$ is complete on $[0,\pi]$",Show that the set  is complete on,"\left\{\sin\frac{1}{2}x,\sin\frac{3}{2}x,\sin\frac{5}{2}x,\ldots\right\} [0,\pi]","Show that the set $$\left\{\sin\frac{1}{2}x,\sin\frac{3}{2}x,\sin\frac{5}{2}x,\ldots\right\}$$ is complete on $[0,\pi]$ I think I can change it to $\left\{\sin\left(\frac{2n-1}{2}x\right)\right\}_{n=1}^\infty$ Now I think I'm supposed to show that Parseval's equality holds for every $f \in R[0,\pi]$ which is $$\frac{2}{\pi}\int_0^\pi f^2(x)\,dx = \sum_{n=1}^\infty b_n^2$$ $$b_n = \frac{2}{\pi}\int_0^\pi f(x)\sin(nx)\,dx$$ Now I'm not sure what to do. Edited. So let $f_0$ denote the odd extensions of $f$ to $[-\pi,\pi]$. We know that $\left\{1, \cos nx, \sin nx\right\}_{n=1}^\infty$ is complete on $[-\pi,\pi]$. So now we have $$\sum_{n=1}^\infty b_n^2 = \frac{1}{\pi}\int_{-\pi}^\pi f_0^2(x)dx = \frac{2}{\pi}\int_0^\pi f^2(x)dx$$ $$\frac{1}{\pi}\int_{-\pi}^\pi f_0^2(x)dx = \frac{1}{\pi}\int_{-\pi}^\pi \sin\left(\frac{2n-1}{2}x\right) dx = \frac{1}{\pi} \left[\frac{-2\cos(nx - \frac{x}{2})}{2n-1}\right]_{-\pi}^\pi$$ $$= \frac{1}{\pi}\left[\frac{-2\sin(n\pi)}{2n-1} - \frac{-2\sin(n\pi)}{2n-1}\right] = 0$$ Would this be right?","Show that the set $$\left\{\sin\frac{1}{2}x,\sin\frac{3}{2}x,\sin\frac{5}{2}x,\ldots\right\}$$ is complete on $[0,\pi]$ I think I can change it to $\left\{\sin\left(\frac{2n-1}{2}x\right)\right\}_{n=1}^\infty$ Now I think I'm supposed to show that Parseval's equality holds for every $f \in R[0,\pi]$ which is $$\frac{2}{\pi}\int_0^\pi f^2(x)\,dx = \sum_{n=1}^\infty b_n^2$$ $$b_n = \frac{2}{\pi}\int_0^\pi f(x)\sin(nx)\,dx$$ Now I'm not sure what to do. Edited. So let $f_0$ denote the odd extensions of $f$ to $[-\pi,\pi]$. We know that $\left\{1, \cos nx, \sin nx\right\}_{n=1}^\infty$ is complete on $[-\pi,\pi]$. So now we have $$\sum_{n=1}^\infty b_n^2 = \frac{1}{\pi}\int_{-\pi}^\pi f_0^2(x)dx = \frac{2}{\pi}\int_0^\pi f^2(x)dx$$ $$\frac{1}{\pi}\int_{-\pi}^\pi f_0^2(x)dx = \frac{1}{\pi}\int_{-\pi}^\pi \sin\left(\frac{2n-1}{2}x\right) dx = \frac{1}{\pi} \left[\frac{-2\cos(nx - \frac{x}{2})}{2n-1}\right]_{-\pi}^\pi$$ $$= \frac{1}{\pi}\left[\frac{-2\sin(n\pi)}{2n-1} - \frac{-2\sin(n\pi)}{2n-1}\right] = 0$$ Would this be right?",,"['real-analysis', 'analysis']"
61,Help with Rudin rank theorem proof!,Help with Rudin rank theorem proof!,,"I am struggling through Rudin's proof of the rank theorem (9.32) in the baby Rudin book. There is a part in the proof where he claims that for a finite-dimensional linear operator A, if the set V is open, then A(V) is an open subset of the range of A. I have seem things about the open mapping theorem involving Banach spaces, but I am not on that level yet and I don't see why the justification of this statement could possibly involve Banach spaces, considering this book does not talk about those. How does Rudin justify this statement, at the level of this book? Thanks!","I am struggling through Rudin's proof of the rank theorem (9.32) in the baby Rudin book. There is a part in the proof where he claims that for a finite-dimensional linear operator A, if the set V is open, then A(V) is an open subset of the range of A. I have seem things about the open mapping theorem involving Banach spaces, but I am not on that level yet and I don't see why the justification of this statement could possibly involve Banach spaces, considering this book does not talk about those. How does Rudin justify this statement, at the level of this book? Thanks!",,['analysis']
62,Generalizing the Monotone Subsequence theorem,Generalizing the Monotone Subsequence theorem,,"In proving the Bolzaono-Weierstrass theorem, one proves the lemma that every infinite real sequence has a(n infinite) monotone subsequence. In all of the proofs I've seen so far, this is done by constructing the monotone sequence incrementally, either by taking the maximum of a the sequence starting from some point, or (if the maximum does not exist) by taking increasingly large values. This made me curious about a more genral hypothesis, that cannot be proved using the same method: Hypothesis : Given linearily ordered sets, $I$ and $A$, such that $|I|\ge\aleph_0$, and a family $\{a_\alpha\}_{\alpha\in I}\in A^I$, then there exists a subset $J\subseteq I$, $|J|=|I|$, such that $\{a_\alpha\}_{\alpha\in J}$ is monotone, i.e. for all $\beta,\gamma\in J$, either $a_\beta \le a_\gamma$ whenever $\beta\lt\gamma$, or $a_\beta \ge a_\gamma$ whenever $\beta\lt\gamma$. Any ideas on how can I go about proving, or disproving, this hypothesis?","In proving the Bolzaono-Weierstrass theorem, one proves the lemma that every infinite real sequence has a(n infinite) monotone subsequence. In all of the proofs I've seen so far, this is done by constructing the monotone sequence incrementally, either by taking the maximum of a the sequence starting from some point, or (if the maximum does not exist) by taking increasingly large values. This made me curious about a more genral hypothesis, that cannot be proved using the same method: Hypothesis : Given linearily ordered sets, $I$ and $A$, such that $|I|\ge\aleph_0$, and a family $\{a_\alpha\}_{\alpha\in I}\in A^I$, then there exists a subset $J\subseteq I$, $|J|=|I|$, such that $\{a_\alpha\}_{\alpha\in J}$ is monotone, i.e. for all $\beta,\gamma\in J$, either $a_\beta \le a_\gamma$ whenever $\beta\lt\gamma$, or $a_\beta \ge a_\gamma$ whenever $\beta\lt\gamma$. Any ideas on how can I go about proving, or disproving, this hypothesis?",,"['sequences-and-series', 'analysis', 'elementary-set-theory']"
63,a complicated question about double improper integral,a complicated question about double improper integral,,"how to evaluate $$\iint_{y\ge x^2+1}{dx\,dy\over{x^4+y^2}}$$ My solution: the initial intergral $$ =2\int_0^\infty \left(\int_{x^2+1}^\infty {dy\over {x^4+y^2}}\right)\,dx = \int_0^\infty  \int_{x^2+1}^\infty {{1\over{x^2} }d({y\over {x^2}})\over{1+({y\over x^2}})^2} $$ $$ = \int_0^\infty {1\over x^2} \left({\pi\over 2} - \arctan\left(1+{1\over x^2}\right) \right)dx, $$ I feel confused here. Can somebody give me other methods to solve the question?","how to evaluate $$\iint_{y\ge x^2+1}{dx\,dy\over{x^4+y^2}}$$ My solution: the initial intergral $$ =2\int_0^\infty \left(\int_{x^2+1}^\infty {dy\over {x^4+y^2}}\right)\,dx = \int_0^\infty  \int_{x^2+1}^\infty {{1\over{x^2} }d({y\over {x^2}})\over{1+({y\over x^2}})^2} $$ $$ = \int_0^\infty {1\over x^2} \left({\pi\over 2} - \arctan\left(1+{1\over x^2}\right) \right)dx, $$ I feel confused here. Can somebody give me other methods to solve the question?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'improper-integrals']"
64,"What $\alpha$ such that if $xy=\alpha$, then $e^{-x}+e^{-y}\geq 2e^{-\sqrt \alpha} $?","What  such that if , then ?",\alpha xy=\alpha e^{-x}+e^{-y}\geq 2e^{-\sqrt \alpha} ,"For every $ x,y \gt 0$, if $ xy=\alpha$, then we have $$e^{-x}+e^{-y}\geq 2e^{-\sqrt \alpha} $$ What are the possible values of   $\alpha$? $2 < e^{1/(n+1)} + e^{-1/n}$ led to  this problem. so, $\alpha=1$  is allowed. The problem is difficult, if not very:  $e^{-x}+e^{-\frac1x}\geq 2e^{-1}$ is not easy to prove. P.S.: 1)  $\quad e^{-\sqrt \alpha}\geq e^{\dfrac{-x-y}2}$ 2) $\quad e^{-x}+e^{-y}\geq 2e^{-\sqrt {xy}} $ does not hold for all $x,y\gt0$! Just think $x=1,y\to0$ Any help will be appreciated!","For every $ x,y \gt 0$, if $ xy=\alpha$, then we have $$e^{-x}+e^{-y}\geq 2e^{-\sqrt \alpha} $$ What are the possible values of   $\alpha$? $2 < e^{1/(n+1)} + e^{-1/n}$ led to  this problem. so, $\alpha=1$  is allowed. The problem is difficult, if not very:  $e^{-x}+e^{-\frac1x}\geq 2e^{-1}$ is not easy to prove. P.S.: 1)  $\quad e^{-\sqrt \alpha}\geq e^{\dfrac{-x-y}2}$ 2) $\quad e^{-x}+e^{-y}\geq 2e^{-\sqrt {xy}} $ does not hold for all $x,y\gt0$! Just think $x=1,y\to0$ Any help will be appreciated!",,"['real-analysis', 'analysis', 'inequality', 'exponential-function']"
65,Showing $f^{(n-1)}(\xi) = 0$ for some $\xi$,Showing  for some,f^{(n-1)}(\xi) = 0 \xi,"Let $f$ be an $n$ times differentiable function on the interval $A$. If $x_1 < x_2 < \cdots < x_p$ are points on $A$ and $n_i, 1 \leq i \leq p,$ are natural numbers such that $n_1 + n_2 + \cdots + n_p = n$ and $f^{(k)}(x_i)=0$ for $0 \leq k \leq n_i -1,$ then there exists a point $\xi$ in the closed interval $[x_1,x_p]$ at which $f^{(n-1)}(\xi) = 0$. I have no real idea how to approach this question and would like some advice. I've tested the theorem on some polynomials and it has worked. For example, the polynomial $(x-3)^2(x-6)^3$ obviously fulfils the requirements with $n_i=2,3$ for $3,6$, respectively. The fourth derivative has a root at $4.8$. These are quite obvious examples of course. Edit: I should stress that this cannot seemingly be done using Rolle's theorem as it only implies that $f^{(p-1)}(\xi) = 0$ for some $\xi$, and not for $n-1$.","Let $f$ be an $n$ times differentiable function on the interval $A$. If $x_1 < x_2 < \cdots < x_p$ are points on $A$ and $n_i, 1 \leq i \leq p,$ are natural numbers such that $n_1 + n_2 + \cdots + n_p = n$ and $f^{(k)}(x_i)=0$ for $0 \leq k \leq n_i -1,$ then there exists a point $\xi$ in the closed interval $[x_1,x_p]$ at which $f^{(n-1)}(\xi) = 0$. I have no real idea how to approach this question and would like some advice. I've tested the theorem on some polynomials and it has worked. For example, the polynomial $(x-3)^2(x-6)^3$ obviously fulfils the requirements with $n_i=2,3$ for $3,6$, respectively. The fourth derivative has a root at $4.8$. These are quite obvious examples of course. Edit: I should stress that this cannot seemingly be done using Rolle's theorem as it only implies that $f^{(p-1)}(\xi) = 0$ for some $\xi$, and not for $n-1$.",,"['analysis', 'derivatives']"
66,Algebra of pseudo-differential operators,Algebra of pseudo-differential operators,,"The class of pseudod-ifferential operators form an associative algebra of Fourier integral operators. Moreover, given symbols $a,b,c\in C^\infty$ (each associated to some pseudo differential operator), for the composition of symbols (#) there holds: $$\text{op}(a)\circ \text{op}(b) = \text{op}(a\# b),$$ so, obviously, $\#$ should be an associative operation as well. The latter is equivalent to prove for $$(a \# b)(x,y)=\sum_{|\alpha|\geq 0} \frac 1 {\alpha!} \partial_y^\alpha a(x,y) D_x^\alpha b(x,y)\quad (1)$$ there holds $(a\#b)\# c = a\#(b\# c)$, where $D=-i\partial$. To check this, I first defined $\circ_N$ which considers in $(1)$ only the multi-indexes up to length $N$, i.e. $$(a \circ_N b)(x,y)=\sum_{|\alpha|\leq N} \frac 1 {\alpha!} \partial_y^\alpha a(x,y) D_x^\alpha b(x,y).$$ Now, using the general Leibniz product rule I get $$LS := (a\circ_N b) \circ_N c = \sum_{|\alpha|\leq N} \frac 1 {\alpha!} \partial_\xi^\alpha (\sum_{|\beta|\leq N}) \partial_\xi^\beta a D_x^\beta b) D_x^\alpha c = \sum_{|\alpha|\leq N,\ |\beta|\leq N,\ \alpha_1+\alpha_2=\alpha} \frac{1} {\beta!\alpha_1!\alpha_2!} (\partial_\xi^{\alpha_1+\beta} a) (\partial_\xi^{\alpha_2}D_x^\beta b) (D_x^\alpha c).$$ Similarly, we see $$RS := a \circ_N (b\circ_N c) = \sum_{|\alpha|\leq N,\ |\beta|\leq N,\ \alpha_1+\alpha_2=\alpha} \frac{1} {\beta!\alpha_1!\alpha_2!} (\partial_\xi^\alpha a) (\partial_\xi^{\beta}D_x^{\alpha_2} b) (D_x^{\alpha_1+\beta} c).$$ But unfortunately it is $LS\neq RS$; the larger $N$, the more different terms arise on each side and they will never cancel. What's my mistake?","The class of pseudod-ifferential operators form an associative algebra of Fourier integral operators. Moreover, given symbols $a,b,c\in C^\infty$ (each associated to some pseudo differential operator), for the composition of symbols (#) there holds: $$\text{op}(a)\circ \text{op}(b) = \text{op}(a\# b),$$ so, obviously, $\#$ should be an associative operation as well. The latter is equivalent to prove for $$(a \# b)(x,y)=\sum_{|\alpha|\geq 0} \frac 1 {\alpha!} \partial_y^\alpha a(x,y) D_x^\alpha b(x,y)\quad (1)$$ there holds $(a\#b)\# c = a\#(b\# c)$, where $D=-i\partial$. To check this, I first defined $\circ_N$ which considers in $(1)$ only the multi-indexes up to length $N$, i.e. $$(a \circ_N b)(x,y)=\sum_{|\alpha|\leq N} \frac 1 {\alpha!} \partial_y^\alpha a(x,y) D_x^\alpha b(x,y).$$ Now, using the general Leibniz product rule I get $$LS := (a\circ_N b) \circ_N c = \sum_{|\alpha|\leq N} \frac 1 {\alpha!} \partial_\xi^\alpha (\sum_{|\beta|\leq N}) \partial_\xi^\beta a D_x^\beta b) D_x^\alpha c = \sum_{|\alpha|\leq N,\ |\beta|\leq N,\ \alpha_1+\alpha_2=\alpha} \frac{1} {\beta!\alpha_1!\alpha_2!} (\partial_\xi^{\alpha_1+\beta} a) (\partial_\xi^{\alpha_2}D_x^\beta b) (D_x^\alpha c).$$ Similarly, we see $$RS := a \circ_N (b\circ_N c) = \sum_{|\alpha|\leq N,\ |\beta|\leq N,\ \alpha_1+\alpha_2=\alpha} \frac{1} {\beta!\alpha_1!\alpha_2!} (\partial_\xi^\alpha a) (\partial_\xi^{\beta}D_x^{\alpha_2} b) (D_x^{\alpha_1+\beta} c).$$ But unfortunately it is $LS\neq RS$; the larger $N$, the more different terms arise on each side and they will never cancel. What's my mistake?",,"['analysis', 'associativity', 'pseudo-differential-operators']"
67,Integral inequality $\int_0^{+\infty}|\frac{\sin x}x|^p dx\leq\frac\pi{\sqrt{2p}}$,Integral inequality,\int_0^{+\infty}|\frac{\sin x}x|^p dx\leq\frac\pi{\sqrt{2p}},"$p\geq2$ , then we have $$\int_0^{+\infty}\Bigg|\frac{\sin x}x\Bigg|^p\,\mathrm dx\leq\frac\pi{\sqrt{2p}}$$ I  try to use $\Bigg|\frac{\sin x}x\Bigg|\leq1$ , and $\frac{\sin x}x\geq\frac2\pi(x\in(0,\frac\pi2])$ , but without any progress. Thank you very much for your help",", then we have I  try to use , and , but without any progress. Thank you very much for your help","p\geq2 \int_0^{+\infty}\Bigg|\frac{\sin x}x\Bigg|^p\,\mathrm dx\leq\frac\pi{\sqrt{2p}} \Bigg|\frac{\sin x}x\Bigg|\leq1 \frac{\sin x}x\geq\frac2\pi(x\in(0,\frac\pi2])","['calculus', 'real-analysis', 'integration', 'analysis', 'integral-inequality']"
68,Determine whether following series converges:,Determine whether following series converges:,,"$$ \sum_{n=1}^\infty \frac{\sqrt{n}}{\sqrt{n^3}-i} $$ I determined that series diverges, because it's less than $\frac{1}{n}$ (I assumed that $i$ has no influence here) and $\frac{1}{n}$ diverges, so, by Comparison test, given series diverges. But I am not sure about this. Am I correctly assuming that imaginary number has no influence in this case?","$$ \sum_{n=1}^\infty \frac{\sqrt{n}}{\sqrt{n^3}-i} $$ I determined that series diverges, because it's less than $\frac{1}{n}$ (I assumed that $i$ has no influence here) and $\frac{1}{n}$ diverges, so, by Comparison test, given series diverges. But I am not sure about this. Am I correctly assuming that imaginary number has no influence in this case?",,"['sequences-and-series', 'analysis']"
69,"The pointwise limit of increasing functions on $[0,1]$ is increasing.",The pointwise limit of increasing functions on  is increasing.,"[0,1]","For each $n \in \mathbb{N}$, let $f_{n}: [0,1] \rightarrow \mathbb{R}$ be an increasing function on $[0,1]$.  Suppose that $\{f_{n}\}$ converges pointwise to a continuous function $f$ on $[0,1]$. Prove first that $f$ is an increasing function on $[0,1]$, and then show that $f_{n}$ converges uniformly to $f$.","For each $n \in \mathbb{N}$, let $f_{n}: [0,1] \rightarrow \mathbb{R}$ be an increasing function on $[0,1]$.  Suppose that $\{f_{n}\}$ converges pointwise to a continuous function $f$ on $[0,1]$. Prove first that $f$ is an increasing function on $[0,1]$, and then show that $f_{n}$ converges uniformly to $f$.",,"['real-analysis', 'analysis']"
70,Is it true that there is no algorithm to approximate the least upper bound?,Is it true that there is no algorithm to approximate the least upper bound?,,"I just read the following text below from Bishop's Foundations of Contructive Analysis , is it true that there is no such algorithm? The book is from 1967 - I don't know if someone managed to invent such algorithm until the present date.","I just read the following text below from Bishop's Foundations of Contructive Analysis , is it true that there is no such algorithm? The book is from 1967 - I don't know if someone managed to invent such algorithm until the present date.",,"['real-analysis', 'analysis', 'algorithms', 'constructive-mathematics']"
71,"Find sup, inf, min, max of the set B","Find sup, inf, min, max of the set B",,"Given the set $$B=\left\{\frac{1}{n}+(-1)^n, n \in \mathbb N\right\}$$ I have to find $\sup B$ , $\inf B$ , $\max B$ , $\min B$ . $$$$ For $n=even:$ $$B_{even}=\left\{\frac{1}{2k}+1, k=1,2,...\right\}$$ For $n=odd:$ $$B_{odd}=\left\{\frac{1}{2k+1}-1, k=0,1,2,...\right\}$$ So, $\max B= 1+ \frac{1}{2}=\frac{3}{2}$ , $\sup B =\frac{3}{2}$ , $\min B=-1$ , $\nexists \inf B$ . Is this correct?","Given the set I have to find , , , . For For So, , , , . Is this correct?","B=\left\{\frac{1}{n}+(-1)^n, n \in \mathbb N\right\} \sup B \inf B \max B \min B  n=even: B_{even}=\left\{\frac{1}{2k}+1, k=1,2,...\right\} n=odd: B_{odd}=\left\{\frac{1}{2k+1}-1, k=0,1,2,...\right\} \max B= 1+ \frac{1}{2}=\frac{3}{2} \sup B =\frac{3}{2} \min B=-1 \nexists \inf B",['analysis']
72,$L^{p}$ inequality with a lower bound on measure,inequality with a lower bound on measure,L^{p},"I am working on the following problem: Suppose $f \in L^{p}(X)$ for some $0 < p < \infty$ and the space $X$ is such that each set of positive measure has measure $\geq m$ for some $m > 0$. Show that $f \in L^{q}(X)$ for $p < q \leq \infty$ with $\|f\|_{L^{q}} \leq m^{1/q - 1/p}\|f\|_{L^{p}}$. I think one could first do it for the characteristic functions, then simple functions, and then use the Dominated/Monotone Convergence Theorem, but is there a way to do it with Holder's Inequality (or other inequalities)? Using simple functions doesn't seem to illuminate when the case of equality holds.","I am working on the following problem: Suppose $f \in L^{p}(X)$ for some $0 < p < \infty$ and the space $X$ is such that each set of positive measure has measure $\geq m$ for some $m > 0$. Show that $f \in L^{q}(X)$ for $p < q \leq \infty$ with $\|f\|_{L^{q}} \leq m^{1/q - 1/p}\|f\|_{L^{p}}$. I think one could first do it for the characteristic functions, then simple functions, and then use the Dominated/Monotone Convergence Theorem, but is there a way to do it with Holder's Inequality (or other inequalities)? Using simple functions doesn't seem to illuminate when the case of equality holds.",,"['real-analysis', 'analysis', 'lp-spaces']"
73,Proof for convert $\left\{\int_a^tf(x)dx\right\}^{n}$ to multiplication of integrals,Proof for convert  to multiplication of integrals,\left\{\int_a^tf(x)dx\right\}^{n},"In a book, author use a relation like this: $$\left\{\int_a^tf(x)dx\right\}^{n} = n!\int_a^tf(x_1)dx_1\int_a^{x_1}f(x_2)dx_2\dots\int_a^{x_{n-1}}f(x)dx_{n}.$$ How I can make a proof for this? Which condition on $f$ is necessary for that equation became true? Thanks, Meysam.","In a book, author use a relation like this: $$\left\{\int_a^tf(x)dx\right\}^{n} = n!\int_a^tf(x_1)dx_1\int_a^{x_1}f(x_2)dx_2\dots\int_a^{x_{n-1}}f(x)dx_{n}.$$ How I can make a proof for this? Which condition on $f$ is necessary for that equation became true? Thanks, Meysam.",,"['calculus', 'analysis', 'multivariable-calculus']"
74,"Show that $f, f^{-1}$ are continuous",Show that  are continuous,"f, f^{-1}","Let $A,B \subset \mathbb{R}$ be open, and $f:A\rightarrow B$ be surjective and strictly monotonic increasing. Show that $f,f^{-1}$ are continuous. Proof: I first show $f$ is injective. Let $x,y \in A, \mbox{and } x\neq y.$ This means either $x<y \mbox{  or } y<x.$ As $f$ is monotonic increasing, $f(x)<f(y) \mbox{ or }f(y)<f(x).\Rightarrow f(x)\neq f(y)\Rightarrow f$ is injective. This shows $f$ is bijective. To show $f$ is continuous, let $D \subset B$ be open. I need to show $f^{-1}(D)$ is open in $A$. Suppose not, i.e., $(f^{-1}(D))^{c}$ is not closed. $\quad\Rightarrow \exists$ a sequence $(x_n)_{n\in\mathbb{N}}$ in  $(f^{-1}(D))^{c}$ that converges to $x$ which is in $f^{-1}(D)$. As $f$ is bijective, $\exists$ a unique $y_n,y$ for each $x_n$ such that $f(x_n)=y_n,f(x)=y,\forall n\in\mathbb{N}$, where $(y_n)_{n\in\mathbb{N}}$ is in $D^{c}, y\in D$. Here, $y_n\rightarrow y$. If it does not, this means $f^{-1}(y_n)=x_n$ does not converge to $f^{-1}(y)=x$, contradiction.  $\quad \Rightarrow D^c$ is not closed. $\Rightarrow D$ is not open. Contradiction. $f^{-1}$ can also be proved to be continuous in the same way as above. I somehow get a feeling that I am not allowed to argue $y_n \rightarrow y$ because a priori, my argument, which I think, assumes $f$ is continuous, which I have not proven yet! Is my proof correct or my doubt? Please help me get out of this situation!","Let $A,B \subset \mathbb{R}$ be open, and $f:A\rightarrow B$ be surjective and strictly monotonic increasing. Show that $f,f^{-1}$ are continuous. Proof: I first show $f$ is injective. Let $x,y \in A, \mbox{and } x\neq y.$ This means either $x<y \mbox{  or } y<x.$ As $f$ is monotonic increasing, $f(x)<f(y) \mbox{ or }f(y)<f(x).\Rightarrow f(x)\neq f(y)\Rightarrow f$ is injective. This shows $f$ is bijective. To show $f$ is continuous, let $D \subset B$ be open. I need to show $f^{-1}(D)$ is open in $A$. Suppose not, i.e., $(f^{-1}(D))^{c}$ is not closed. $\quad\Rightarrow \exists$ a sequence $(x_n)_{n\in\mathbb{N}}$ in  $(f^{-1}(D))^{c}$ that converges to $x$ which is in $f^{-1}(D)$. As $f$ is bijective, $\exists$ a unique $y_n,y$ for each $x_n$ such that $f(x_n)=y_n,f(x)=y,\forall n\in\mathbb{N}$, where $(y_n)_{n\in\mathbb{N}}$ is in $D^{c}, y\in D$. Here, $y_n\rightarrow y$. If it does not, this means $f^{-1}(y_n)=x_n$ does not converge to $f^{-1}(y)=x$, contradiction.  $\quad \Rightarrow D^c$ is not closed. $\Rightarrow D$ is not open. Contradiction. $f^{-1}$ can also be proved to be continuous in the same way as above. I somehow get a feeling that I am not allowed to argue $y_n \rightarrow y$ because a priori, my argument, which I think, assumes $f$ is continuous, which I have not proven yet! Is my proof correct or my doubt? Please help me get out of this situation!",,"['real-analysis', 'analysis', 'continuity']"
75,"How prove this integral limit is exsit $\lim_{\varepsilon\to 0^{+} }f(x,y)dxdy$",How prove this integral limit is exsit,"\lim_{\varepsilon\to 0^{+} }f(x,y)dxdy","Question: This problem  is the 2013 Beijing university mathematics examination the last question,and I consider sometimes,and I can't, let  $D$ is with smooth boundary bounded region in plane,and the function  $f(x,y)$  is Continuous differentiable on $\overline{D}$,and $\forall P_{0}=(x_{0},y_{0})\in D$, show that this limit $$A=\lim_{\varepsilon\to 0^{+}}\int\int_{D/B_{\varepsilon}(P_{0})}\dfrac{\dfrac{\partial f}{\partial y}(x,y)(y-y_{0})+\dfrac{\partial f}{\partial x}(x,y)(x-x_{0})}{(x-x_{0})^2+(y-y_{0})^2}dxdy$$   is exsit,where $B_{\varepsilon}(P_{0})=\{(x,y)|(x-x_{0})^2+(y-y_{0})^2\le\varepsilon^2,,x_{0},y_{0}\in D\}$ (2):show that   $$f(x_{0},y_{0})=\dfrac{1}{2\pi}\left(\int_{\partial D}\dfrac{f(x,y)}{(x-x_{0})^2+(y-y_{0})^2}((x-x_{0})dy-(y-y_{0})dx)-A\right)$$ First,I want use this Taylor lemma: $$f(x,y)=f(x_{0},y_{0})+f'_{x}(x_{0},y_{0})(x-x_{0})+f'_{y}(x_{0},y_{0})(y-y_{0})+\cdots $$ But I can't any work .Thank you for you help!","Question: This problem  is the 2013 Beijing university mathematics examination the last question,and I consider sometimes,and I can't, let  $D$ is with smooth boundary bounded region in plane,and the function  $f(x,y)$  is Continuous differentiable on $\overline{D}$,and $\forall P_{0}=(x_{0},y_{0})\in D$, show that this limit $$A=\lim_{\varepsilon\to 0^{+}}\int\int_{D/B_{\varepsilon}(P_{0})}\dfrac{\dfrac{\partial f}{\partial y}(x,y)(y-y_{0})+\dfrac{\partial f}{\partial x}(x,y)(x-x_{0})}{(x-x_{0})^2+(y-y_{0})^2}dxdy$$   is exsit,where $B_{\varepsilon}(P_{0})=\{(x,y)|(x-x_{0})^2+(y-y_{0})^2\le\varepsilon^2,,x_{0},y_{0}\in D\}$ (2):show that   $$f(x_{0},y_{0})=\dfrac{1}{2\pi}\left(\int_{\partial D}\dfrac{f(x,y)}{(x-x_{0})^2+(y-y_{0})^2}((x-x_{0})dy-(y-y_{0})dx)-A\right)$$ First,I want use this Taylor lemma: $$f(x,y)=f(x_{0},y_{0})+f'_{x}(x_{0},y_{0})(x-x_{0})+f'_{y}(x_{0},y_{0})(y-y_{0})+\cdots $$ But I can't any work .Thank you for you help!",,"['calculus', 'analysis']"
76,basic exercise about Schwartz spaces,basic exercise about Schwartz spaces,,"Let $f \in S(R)$ (the Schwartz space of rapidly decaying functions) such that $f(0)=0.$ Show that exists $g \in S(R) $such  that $f(x) = xg(x)$. My try : By the calculus fundamental theorem $$ f(x) = \displaystyle\int_{0}^{1} \displaystyle\frac{d}{dt} f(tx) \ dt = \displaystyle\int_{0}^{1}  f^{'}(tx) x \ dt = x \displaystyle\int_{0}^{1}  f^{'}(tx)  \ dt.$$ Define $g(x) = \displaystyle\int_{0}^{1}  f^{'}(tx)  \ dt$. I know how prove that $g \in C^{\infty}(R).$ But to conclude that $g \in S(R)$ i need to show that $\displaystyle\sup_{x \in R} |x^{\alpha} g^{(\beta)}(x)| < \infty$ for all $(\alpha,\beta)\in N \times N.$ I dont know how to do that . For example for the situation when $\alpha$ is arbitrary and $\beta = 0$ my best is this : $$|x^{\alpha} g(x)| = |\displaystyle\int_{0}^{1}  f^{'}(tx)  \ dt|\leq \displaystyle\int_{0}^{1}|x^{\alpha } f^{'}(tx)| \ dt \leq |x|^{\alpha} \displaystyle\sup_{t \in [0,1]} |f^{'}(tx)|$$ someone can give me a hint ? Thanks in advance.","Let $f \in S(R)$ (the Schwartz space of rapidly decaying functions) such that $f(0)=0.$ Show that exists $g \in S(R) $such  that $f(x) = xg(x)$. My try : By the calculus fundamental theorem $$ f(x) = \displaystyle\int_{0}^{1} \displaystyle\frac{d}{dt} f(tx) \ dt = \displaystyle\int_{0}^{1}  f^{'}(tx) x \ dt = x \displaystyle\int_{0}^{1}  f^{'}(tx)  \ dt.$$ Define $g(x) = \displaystyle\int_{0}^{1}  f^{'}(tx)  \ dt$. I know how prove that $g \in C^{\infty}(R).$ But to conclude that $g \in S(R)$ i need to show that $\displaystyle\sup_{x \in R} |x^{\alpha} g^{(\beta)}(x)| < \infty$ for all $(\alpha,\beta)\in N \times N.$ I dont know how to do that . For example for the situation when $\alpha$ is arbitrary and $\beta = 0$ my best is this : $$|x^{\alpha} g(x)| = |\displaystyle\int_{0}^{1}  f^{'}(tx)  \ dt|\leq \displaystyle\int_{0}^{1}|x^{\alpha } f^{'}(tx)| \ dt \leq |x|^{\alpha} \displaystyle\sup_{t \in [0,1]} |f^{'}(tx)|$$ someone can give me a hint ? Thanks in advance.",,"['real-analysis', 'analysis', 'partial-differential-equations', 'schwartz-space']"
77,Show that $\operatorname{div} X = - \delta X^\flat$,Show that,\operatorname{div} X = - \delta X^\flat,"I want to show the equality $\operatorname{div} X = -\delta X^\flat$, where $X \in \Gamma(TM)$ and $M$ is some Riemannian manifold with metric tensor $g_{ij}$. If I'm not mistaken it holds for the Riemannian connection. Okay, let $X =X^k e_k$, then $$    \nabla_i (X^k e_k) = \frac{\partial X^k}{\partial x^i}e_k+X^j \Gamma^k_{ij} e_k,  \\     \operatorname{div} X= \frac{\partial X^i}{\partial x^i}+X^j \Gamma^i_{ij}, \\     \Gamma^i_{ij} = \frac 1 2 g^{ik}(\partial_i g_{kj} + \partial_j g_{ki} - \partial_k g_{ij}). $$ On the other hand  $$     X^\flat = g_{ij}X^j \theta^i, \\     \nabla_k X^\flat = g_{ij} \nabla_k (X^j \theta^i) = g_{ij} \frac{\partial X^j}{\partial x^k} \theta^i - g_{sj}X^j \Gamma^s_{ki} \theta^i, \\     \delta X^\flat=-g^{ki}\left( g_{ij} \frac{\partial X^j}{\partial x^k} - g_{sj}X^j \Gamma^s_{ki} \right) = -\frac{\partial X^i}{\partial x^i} + g^{ki}g_{sj} X^j \Gamma^s_{ki}. $$ We can see that $\operatorname{div} X = -\delta X^\flat$ holds iff  $$    X^j \Gamma^i_{ij} = -g^{ki}g_{sj}X^j \Gamma^s_{ki}. $$ Now recall that $$    \Gamma^s_{ki} = \frac 1 2 g^{ls}( \partial_{k} g_{li}+\partial_i g_{lk} - \partial_l g_{ki}), \\     g_{sj}\Gamma^s_{ki}=\frac 1 2 (\partial_k g_{ji}+\partial_i g_{jk}-\partial_j g_{ki}), \\    g^{ki}g_{sj}\Gamma^s_{ki} = \frac 1 2 g^{ki}(\partial_k g_{ji}+\partial_i g_{jk}-\partial_j g_{ki}). $$ Hence the equality $\operatorname{div} X = -\delta X^\flat$ holds if $$ g^{ik}(\partial_i g_{kj} + \partial_j g_{ki} - \partial_k g_{ij}) = g^{ik}(-\partial_k g_{ji}-\partial_i g_{jk}+\partial_j g_{ki}), \\    g^{ik} \partial_i g_{kj} = -g^{ik} \partial_i g_{jk}. $$ But the equality $g^{ik} \partial_i g_{kj} = -g^{ik} \partial_i g_{jk}$ is very unreal. Please, tell me, where is the problem?","I want to show the equality $\operatorname{div} X = -\delta X^\flat$, where $X \in \Gamma(TM)$ and $M$ is some Riemannian manifold with metric tensor $g_{ij}$. If I'm not mistaken it holds for the Riemannian connection. Okay, let $X =X^k e_k$, then $$    \nabla_i (X^k e_k) = \frac{\partial X^k}{\partial x^i}e_k+X^j \Gamma^k_{ij} e_k,  \\     \operatorname{div} X= \frac{\partial X^i}{\partial x^i}+X^j \Gamma^i_{ij}, \\     \Gamma^i_{ij} = \frac 1 2 g^{ik}(\partial_i g_{kj} + \partial_j g_{ki} - \partial_k g_{ij}). $$ On the other hand  $$     X^\flat = g_{ij}X^j \theta^i, \\     \nabla_k X^\flat = g_{ij} \nabla_k (X^j \theta^i) = g_{ij} \frac{\partial X^j}{\partial x^k} \theta^i - g_{sj}X^j \Gamma^s_{ki} \theta^i, \\     \delta X^\flat=-g^{ki}\left( g_{ij} \frac{\partial X^j}{\partial x^k} - g_{sj}X^j \Gamma^s_{ki} \right) = -\frac{\partial X^i}{\partial x^i} + g^{ki}g_{sj} X^j \Gamma^s_{ki}. $$ We can see that $\operatorname{div} X = -\delta X^\flat$ holds iff  $$    X^j \Gamma^i_{ij} = -g^{ki}g_{sj}X^j \Gamma^s_{ki}. $$ Now recall that $$    \Gamma^s_{ki} = \frac 1 2 g^{ls}( \partial_{k} g_{li}+\partial_i g_{lk} - \partial_l g_{ki}), \\     g_{sj}\Gamma^s_{ki}=\frac 1 2 (\partial_k g_{ji}+\partial_i g_{jk}-\partial_j g_{ki}), \\    g^{ki}g_{sj}\Gamma^s_{ki} = \frac 1 2 g^{ki}(\partial_k g_{ji}+\partial_i g_{jk}-\partial_j g_{ki}). $$ Hence the equality $\operatorname{div} X = -\delta X^\flat$ holds if $$ g^{ik}(\partial_i g_{kj} + \partial_j g_{ki} - \partial_k g_{ij}) = g^{ik}(-\partial_k g_{ji}-\partial_i g_{jk}+\partial_j g_{ki}), \\    g^{ik} \partial_i g_{kj} = -g^{ik} \partial_i g_{jk}. $$ But the equality $g^{ik} \partial_i g_{kj} = -g^{ik} \partial_i g_{jk}$ is very unreal. Please, tell me, where is the problem?",,"['analysis', 'differential-geometry', 'riemannian-geometry']"
78,Non isolated minimum,Non isolated minimum,,"Consider the $C^2$ function $F:\mathbb{R}^k \rightarrow \mathbb{R}^k$ is it possible for it to be such that $x_n$ is a strict local minimizer for all $n$ and $x_n \rightarrow x$ where $x$ is a strict local minimizer itself. I know that $\sin(\frac{1}{x})$ has a somewhat similar property, but it is not continuous at zero. Thanks!","Consider the $C^2$ function $F:\mathbb{R}^k \rightarrow \mathbb{R}^k$ is it possible for it to be such that $x_n$ is a strict local minimizer for all $n$ and $x_n \rightarrow x$ where $x$ is a strict local minimizer itself. I know that $\sin(\frac{1}{x})$ has a somewhat similar property, but it is not continuous at zero. Thanks!",,"['real-analysis', 'analysis']"
79,"Prove that, if $g(x)$ is concave, for $S = {x : g(x) > 0}$, $f(x) = 1/g(x)$ is convex over $S$.","Prove that, if  is concave, for ,  is convex over .",g(x) S = {x : g(x) > 0} f(x) = 1/g(x) S,"Using the definitions of convexity and concavity, I need to show the following: $$g(ax + (1-a)y)\geq ag(x) + (1-a)g(y),\ \ a \in (0, 1)$$ implies that  $$f(ax + (1-a)y) \leq af(x) + (1-a)f(y)\ , a \in (0, 1)\ .$$ I have tried every algebraic manipulation I can come up with to no avail. If I were to assume $f$ and $g$ are both twice differentiable, it is easy to show the second partial derivative of $f(x)$ w.r.t. $x$ is non negative, and hence, $f(x)$ is convex. However, I would like to prove it using the formal definition of convexity.","Using the definitions of convexity and concavity, I need to show the following: $$g(ax + (1-a)y)\geq ag(x) + (1-a)g(y),\ \ a \in (0, 1)$$ implies that  $$f(ax + (1-a)y) \leq af(x) + (1-a)f(y)\ , a \in (0, 1)\ .$$ I have tried every algebraic manipulation I can come up with to no avail. If I were to assume $f$ and $g$ are both twice differentiable, it is easy to show the second partial derivative of $f(x)$ w.r.t. $x$ is non negative, and hence, $f(x)$ is convex. However, I would like to prove it using the formal definition of convexity.",,"['real-analysis', 'analysis']"
80,Roots of $x^x-\tan (x)$,Roots of,x^x-\tan (x),"I conjecture, that the function $f(x)=x^x-\tan x$ has exactly one root in any of the intervals $\left[\dfrac{2n+1}{2}\pi,\dfrac{2n+3}{2}\pi\right]$ , where $n$ is a nonnegative integer.  Does anyone know a proof? I tried the trick using the function $g(x)=\log\left(\dfrac{x^x}{\tan x}\right)$ , which has the same roots, but it did not help either.","I conjecture, that the function $f(x)=x^x-\tan x$ has exactly one root in any of the intervals $\left[\dfrac{2n+1}{2}\pi,\dfrac{2n+3}{2}\pi\right]$ , where $n$ is a nonnegative integer.  Does anyone know a proof? I tried the trick using the function $g(x)=\log\left(\dfrac{x^x}{\tan x}\right)$ , which has the same roots, but it did not help either.",,['analysis']
81,Minimum difference of roots of a polynomial and its derivative,Minimum difference of roots of a polynomial and its derivative,,"Let $P(x) = (x-x_1)(x-x_2)...(x-x_n)$ where all the n roots are real and distinct. Let $y_1,y_2,...,y_{n-1}$ be the roots of $P'$. Show that $\min_{i\neq j}|x_i-x_j|<\min_{i\neq j}|y_i-y_j|$. My thoughts: We have $P'(x) = P(x)(\frac1{x-x_1}+...+\frac1{x-x_n})$. So we may consider $P'(x)/P(x)$, which has poles at the roots of $P$.","Let $P(x) = (x-x_1)(x-x_2)...(x-x_n)$ where all the n roots are real and distinct. Let $y_1,y_2,...,y_{n-1}$ be the roots of $P'$. Show that $\min_{i\neq j}|x_i-x_j|<\min_{i\neq j}|y_i-y_j|$. My thoughts: We have $P'(x) = P(x)(\frac1{x-x_1}+...+\frac1{x-x_n})$. So we may consider $P'(x)/P(x)$, which has poles at the roots of $P$.",,"['real-analysis', 'analysis', 'polynomials', 'contest-math', 'roots']"
82,A question on Vitali convergence theorem,A question on Vitali convergence theorem,,"Let $(X,\mu)$ be a measure space. Vitali convergence theorem says that if (a) $\mu(X)\lt \infty$ (b) $\{ f_n \}$ is uniformly integrable (c) $f_n \to f$ a.e. (d) $|f(x)| \lt \infty $ a.e. then $f\in L^1$ and $f_n \to f$ in $L^1.$ It is not difficult to prove this, but what if the condition (d) is omitted? I tried to construct a counterexample, but I couldn't. Would you please give me one?","Let $(X,\mu)$ be a measure space. Vitali convergence theorem says that if (a) $\mu(X)\lt \infty$ (b) $\{ f_n \}$ is uniformly integrable (c) $f_n \to f$ a.e. (d) $|f(x)| \lt \infty $ a.e. then $f\in L^1$ and $f_n \to f$ in $L^1.$ It is not difficult to prove this, but what if the condition (d) is omitted? I tried to construct a counterexample, but I couldn't. Would you please give me one?",,"['real-analysis', 'analysis']"
83,"$f_1, f_2 : \mathbb{R} \rightarrow \mathbb{R}$ nonconstant, continuous, with period $1, \sqrt{2}$, respectively, then $f_1 + f_2$ is not periodic","nonconstant, continuous, with period , respectively, then  is not periodic","f_1, f_2 : \mathbb{R} \rightarrow \mathbb{R} 1, \sqrt{2} f_1 + f_2","I've been working on this problem for several hours, but I keep getting stuck. Suppose $f_1, f_2 : \mathbb{R} \rightarrow \mathbb{R}$ periodic with period $1, \sqrt{2}$, respectively, and that each of $f_1, f_2$ is nonconstant, continuous. Then $f_1 + f_2$ is not periodic. My thoughts so far: Suppose $p$ is the period of $f_1 + f_2$. I want to use the fact that $\{n\sqrt{2}\}$ is dense in $\mathbb{R}/(x \sim x+p)$ to come up with a contradiction. I've tried numerous paths, which would take forever to write up. Now I'm just hoping to find the solution. I can provide more of my attempt if necessary.","I've been working on this problem for several hours, but I keep getting stuck. Suppose $f_1, f_2 : \mathbb{R} \rightarrow \mathbb{R}$ periodic with period $1, \sqrt{2}$, respectively, and that each of $f_1, f_2$ is nonconstant, continuous. Then $f_1 + f_2$ is not periodic. My thoughts so far: Suppose $p$ is the period of $f_1 + f_2$. I want to use the fact that $\{n\sqrt{2}\}$ is dense in $\mathbb{R}/(x \sim x+p)$ to come up with a contradiction. I've tried numerous paths, which would take forever to write up. Now I'm just hoping to find the solution. I can provide more of my attempt if necessary.",,"['real-analysis', 'analysis', 'irrational-numbers', 'periodic-functions']"
84,How to prove and what are the necessary hypothesis to prove that $\frac{f(x+te_i)-f(x)}{t}\to\frac{\partial f}{\partial x_i}(x)$ uniformly?,How to prove and what are the necessary hypothesis to prove that  uniformly?,\frac{f(x+te_i)-f(x)}{t}\to\frac{\partial f}{\partial x_i}(x),"Let $U\subset\mathbb{R}^n$ be a open set and $f:U\to\mathbb{R}$ a function in $C^\infty_c(U)$. Evans PDE book uses the following result $$\frac{f(x+te_i)-f(x)}{t}\to\frac{\partial f}{\partial x_i}(x)\text{ uniformly as }h\to 0\;\;\;[\#]$$ The definition of uniformly convergence that I know uses sequences, but in this case no sequence is mentioned (and the book doesn't presents a definition). I think it's means that for each $\varepsilon>0$, there exists $\delta>0$ (which depends on $\varepsilon$ and doesn't depend on $x$) such that $$x,x+te_i\in U, \; 0<|t|<\delta \Rightarrow \left|\frac{f(x+te_i)-f(x)}{t}-\frac{\partial f}{\partial x_i}(x)\right|<\varepsilon$$ My first question is: is the above definition correct? My second question is: is it enough $f\in C^1_c(U)$ to conclude $[\#]$? I think so, but some posts suggests that $f\in C^2_c(U)$ it's necessary (see here , here and here ). Then: My third question is: is the proof below correct? Proof of $[\#]$ (based on this post ): Suppose $f\in C^1_c(U)$. Let $g:\mathbb{R}^n\to\mathbb{R}$ be an extension of $f$ defined by $$g(x)=\left\{\begin{matrix} f(x), &\text{if } x\in U\\  0, & \text{if }x\in\mathbb{R}^n\backslash U \end{matrix}\right.\;\;\;(1)$$ So, $g\in C^1_c(\mathbb{R}^n)$. Consequently $\frac{\partial g}{\partial x_i}\in C^1_c(\mathbb{R}^n)$. Therefore $\frac{\partial g}{\partial x_i}$ is uniformly continuous. Follows that given $\varepsilon>0$, there exists $\delta>0$ (which depends on $\varepsilon$ and doesn't depend on $x$) such that $$y,x\in\mathbb{R}^n,\;\|y-x\|<\delta\Rightarrow \left|\frac{\partial g}{\partial x_i}(y)-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon\;\;\;(2)$$ By Mean Value Theorem, given $x\in\mathbb{R}^n$ and $t\in\mathbb{R}$, there exists $\theta_{x,t}\in (0,1)$ such that $$\frac{\partial g}{\partial x_i}(x+\theta_{x,t}te_i)=\frac{g(x+te_i)-g(x)}{t}\;\;\;(3)$$ Thus we can conclude that $$\begin{align*}   x\in\mathbb{R}^n,\;0<|t|<\delta &\Rightarrow\|(x+\theta_{x,t}te_i)-x\|<|t|<\delta\\            &\overset{(2)}\Rightarrow \left|\frac{\partial g}{\partial x_i}(x+\theta_{x,t}te_i)-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon\\            &\overset{(3)}\Rightarrow \left|\frac{g(x+te_i)-g(x)}{t}-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon \end{align*}$$ Particularly, $$\begin{align*} x,x+te_i\in U,\;0<|t|<\delta &\Rightarrow  \left|\frac{g(x+te_i)-g(x)}{t}-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon \\ &\overset{(1)}\Rightarrow \left|\frac{f(x+te_i)-f(x)}{t}-\frac{\partial f}{\partial x_i}(x)\right|<\varepsilon \end{align*}$$ Thanks.","Let $U\subset\mathbb{R}^n$ be a open set and $f:U\to\mathbb{R}$ a function in $C^\infty_c(U)$. Evans PDE book uses the following result $$\frac{f(x+te_i)-f(x)}{t}\to\frac{\partial f}{\partial x_i}(x)\text{ uniformly as }h\to 0\;\;\;[\#]$$ The definition of uniformly convergence that I know uses sequences, but in this case no sequence is mentioned (and the book doesn't presents a definition). I think it's means that for each $\varepsilon>0$, there exists $\delta>0$ (which depends on $\varepsilon$ and doesn't depend on $x$) such that $$x,x+te_i\in U, \; 0<|t|<\delta \Rightarrow \left|\frac{f(x+te_i)-f(x)}{t}-\frac{\partial f}{\partial x_i}(x)\right|<\varepsilon$$ My first question is: is the above definition correct? My second question is: is it enough $f\in C^1_c(U)$ to conclude $[\#]$? I think so, but some posts suggests that $f\in C^2_c(U)$ it's necessary (see here , here and here ). Then: My third question is: is the proof below correct? Proof of $[\#]$ (based on this post ): Suppose $f\in C^1_c(U)$. Let $g:\mathbb{R}^n\to\mathbb{R}$ be an extension of $f$ defined by $$g(x)=\left\{\begin{matrix} f(x), &\text{if } x\in U\\  0, & \text{if }x\in\mathbb{R}^n\backslash U \end{matrix}\right.\;\;\;(1)$$ So, $g\in C^1_c(\mathbb{R}^n)$. Consequently $\frac{\partial g}{\partial x_i}\in C^1_c(\mathbb{R}^n)$. Therefore $\frac{\partial g}{\partial x_i}$ is uniformly continuous. Follows that given $\varepsilon>0$, there exists $\delta>0$ (which depends on $\varepsilon$ and doesn't depend on $x$) such that $$y,x\in\mathbb{R}^n,\;\|y-x\|<\delta\Rightarrow \left|\frac{\partial g}{\partial x_i}(y)-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon\;\;\;(2)$$ By Mean Value Theorem, given $x\in\mathbb{R}^n$ and $t\in\mathbb{R}$, there exists $\theta_{x,t}\in (0,1)$ such that $$\frac{\partial g}{\partial x_i}(x+\theta_{x,t}te_i)=\frac{g(x+te_i)-g(x)}{t}\;\;\;(3)$$ Thus we can conclude that $$\begin{align*}   x\in\mathbb{R}^n,\;0<|t|<\delta &\Rightarrow\|(x+\theta_{x,t}te_i)-x\|<|t|<\delta\\            &\overset{(2)}\Rightarrow \left|\frac{\partial g}{\partial x_i}(x+\theta_{x,t}te_i)-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon\\            &\overset{(3)}\Rightarrow \left|\frac{g(x+te_i)-g(x)}{t}-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon \end{align*}$$ Particularly, $$\begin{align*} x,x+te_i\in U,\;0<|t|<\delta &\Rightarrow  \left|\frac{g(x+te_i)-g(x)}{t}-\frac{\partial g}{\partial x_i}(x)\right|<\varepsilon \\ &\overset{(1)}\Rightarrow \left|\frac{f(x+te_i)-f(x)}{t}-\frac{\partial f}{\partial x_i}(x)\right|<\varepsilon \end{align*}$$ Thanks.",,"['analysis', 'multivariable-calculus', 'partial-differential-equations', 'proof-verification']"
85,How to decide convergence or otherwise of these series?,How to decide convergence or otherwise of these series?,,"How to decide whether the following three series converge or diverge? $$\sum_{n=2}^{\infty} \frac{(-1)^n}{\sqrt{n} + (-1)^n} $$ By the limit comparison with the divergent series $\sum_{n=1}^{\infty} \frac{1}{\sqrt{n}}$ we can say that this series is not absolutely convergent. However since $$ \frac{1}{\sqrt{n}+(-1)^n} - \frac{1}{\sqrt{n+1}+(-1)^{n+1}} = \frac{ \sqrt{n+1} - \sqrt{n} + 2 (-1)^{n+1}}{(\sqrt{n}+(-1)^n)(\sqrt{n+1}+(-1)^{n+1})},$$ which is positive if $n$ is odd but negative if $n$ is even. So we can't apply the Leibnitz's rule. $$ \sum_{n=1}^{\infty} \frac{1}{\log ( e^n + e^{-n} ) } $$  By the Leibnitz's rule we can conclude that the corresponding alternating series converges (conditionally). $$ \sum_{n=1}^{\infty} (-1)^n \int_{n}^{n+1} \frac{e^{-x}}{x} \ dx $$ The integral here satisfies the inequalities $$ \frac{e^{-n}-e^{-n-1}}{n+1} \leq \int_{n}^{n+1} \frac{e^{-x}}{x} \ dx  \leq \frac{e^{-n}-e^{-n-1}}{n}, $$ and both the upper and lower bounds approach $0$ as $n$ increases without bound.  So we can't apply the $n$-th term test for divergence.","How to decide whether the following three series converge or diverge? $$\sum_{n=2}^{\infty} \frac{(-1)^n}{\sqrt{n} + (-1)^n} $$ By the limit comparison with the divergent series $\sum_{n=1}^{\infty} \frac{1}{\sqrt{n}}$ we can say that this series is not absolutely convergent. However since $$ \frac{1}{\sqrt{n}+(-1)^n} - \frac{1}{\sqrt{n+1}+(-1)^{n+1}} = \frac{ \sqrt{n+1} - \sqrt{n} + 2 (-1)^{n+1}}{(\sqrt{n}+(-1)^n)(\sqrt{n+1}+(-1)^{n+1})},$$ which is positive if $n$ is odd but negative if $n$ is even. So we can't apply the Leibnitz's rule. $$ \sum_{n=1}^{\infty} \frac{1}{\log ( e^n + e^{-n} ) } $$  By the Leibnitz's rule we can conclude that the corresponding alternating series converges (conditionally). $$ \sum_{n=1}^{\infty} (-1)^n \int_{n}^{n+1} \frac{e^{-x}}{x} \ dx $$ The integral here satisfies the inequalities $$ \frac{e^{-n}-e^{-n-1}}{n+1} \leq \int_{n}^{n+1} \frac{e^{-x}}{x} \ dx  \leq \frac{e^{-n}-e^{-n-1}}{n}, $$ and both the upper and lower bounds approach $0$ as $n$ increases without bound.  So we can't apply the $n$-th term test for divergence.",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
86,Problem with integral,Problem with integral,,Is there a number $1<y<\infty$ such that $$ \int_{1}^\infty \frac{1}{t} \frac{\sin t}{t} dt =\int_y ^\infty \frac{\sin t}{t} dt  \textrm{ ? } $$,Is there a number $1<y<\infty$ such that $$ \int_{1}^\infty \frac{1}{t} \frac{\sin t}{t} dt =\int_y ^\infty \frac{\sin t}{t} dt  \textrm{ ? } $$,,['analysis']
87,If $Df(c)=0$ $\forall c\in V$ then $f$ is constant on $V$ [duplicate],If   then  is constant on  [duplicate],Df(c)=0 \forall c\in V f V,This question already has answers here : Why does zero derivative imply a function is locally constant? (2 answers) Closed 10 years ago . Theorem: Suppose that $V$ is open and connected in $\Bbb R^n$ $f:V\to \Bbb R^m$ is differentiable on $V$ If $Df(c)=0$ $\forall c\in V$ then $f$ is constant on $V$ I want to prove this theorem with mean value theorem for real valued functions. How to prove this?,This question already has answers here : Why does zero derivative imply a function is locally constant? (2 answers) Closed 10 years ago . Theorem: Suppose that $V$ is open and connected in $\Bbb R^n$ $f:V\to \Bbb R^m$ is differentiable on $V$ If $Df(c)=0$ $\forall c\in V$ then $f$ is constant on $V$ I want to prove this theorem with mean value theorem for real valued functions. How to prove this?,,"['calculus', 'real-analysis', 'analysis']"
88,Proof that a function with continuous partial derivatives has directional derivatives in all directions,Proof that a function with continuous partial derivatives has directional derivatives in all directions,,"I tried to prove it, but I would appreciate if someone could check my answer. I am just starting to learn real analysis on my own Thank you for helping. :) Theorem Let $f\colon \Bbb R^2 \to \Bbb R$ have continuous first partial derivatives at each point. Then $f$ has directional derivatives in all directions at each point. Proof Let $\vec p$ be any non-zero vector in $\Bbb R^2$. Choose $r>0$. By the mean value proposition, we see that if $t$ is any number such that $|t|\lVert \vec p \rVert < r$, then there are $n$ points $z_1,\ldots,z_n\in\Bbb R^2$ such that $$f(x+t\vec p)-f(x)=\sum_{i=1}^n t p_i \frac{\partial f}{\partial x_i}(z_i)$$ and $\lVert z_i - x \rVert < |t|\lVert p \rVert$ for each $i\in\{1,\ldots,n\}$. Then we can write $$\frac{f(x+t\vec p)-f(x)} t =\sum_{i=1}^n p_i \frac{\partial f}{\partial x_i}(z_i)$$ Since $\frac{\partial f_i}{\partial x_i}$ is continuous for each $i$, $$\lim_{t\to 0} \frac{f(x+t\vec p)-f(x)} t = \lim_{t\to 0}\sum_{i=1}^n p_i \frac{\partial f}{\partial x_i}(z_i)$$ exists, so $f$ has a derivative in the $\vec p$ direction at $x$.","I tried to prove it, but I would appreciate if someone could check my answer. I am just starting to learn real analysis on my own Thank you for helping. :) Theorem Let $f\colon \Bbb R^2 \to \Bbb R$ have continuous first partial derivatives at each point. Then $f$ has directional derivatives in all directions at each point. Proof Let $\vec p$ be any non-zero vector in $\Bbb R^2$. Choose $r>0$. By the mean value proposition, we see that if $t$ is any number such that $|t|\lVert \vec p \rVert < r$, then there are $n$ points $z_1,\ldots,z_n\in\Bbb R^2$ such that $$f(x+t\vec p)-f(x)=\sum_{i=1}^n t p_i \frac{\partial f}{\partial x_i}(z_i)$$ and $\lVert z_i - x \rVert < |t|\lVert p \rVert$ for each $i\in\{1,\ldots,n\}$. Then we can write $$\frac{f(x+t\vec p)-f(x)} t =\sum_{i=1}^n p_i \frac{\partial f}{\partial x_i}(z_i)$$ Since $\frac{\partial f_i}{\partial x_i}$ is continuous for each $i$, $$\lim_{t\to 0} \frac{f(x+t\vec p)-f(x)} t = \lim_{t\to 0}\sum_{i=1}^n p_i \frac{\partial f}{\partial x_i}(z_i)$$ exists, so $f$ has a derivative in the $\vec p$ direction at $x$.",,"['calculus', 'real-analysis', 'analysis', 'solution-verification']"
89,Using geometric arguments to solve an analysis problem,Using geometric arguments to solve an analysis problem,,"Im not good in geometric interpretations... any help is very welcome. Consider the unitary disc $$D=\{(x,y,0)\in\mathbb{R}^3, x^2+y^2\leq1\},$$ parameterized by  $$\varphi(r,\theta)=(r\cos\theta,r\sin\theta), (r,\theta)\in[0,1]\times[0,2\pi].$$ Let $\Omega(x,y,z)$ be the solid angle of $\varphi$, viewed from $(x,y,z)$. Consider a closed curve $\gamma:[a,b]\rightarrow\mathbb{R}^3\backslash S$ of class $C^1$, with $$S=\{(x,y,0)\in\mathbb{R}^3, x^2+y^2=1\}.$$ Let $p$ be the number of times that $\gamma$ cuts $D$, coming from $z>0$ to $z<0$, and $q$ the number of times that $\gamma$ cuts $D$, coming from $z<0$ to $z>0$. Use geometric arguments to conclude that $$\int_\gamma d\Omega=4\pi(p-q).$$ PS: if someone wants to know about Solid Angle, take a look at http://en.wikipedia.org/wiki/Solid_angle or http://mathworld.wolfram.com/SolidAngle.html","Im not good in geometric interpretations... any help is very welcome. Consider the unitary disc $$D=\{(x,y,0)\in\mathbb{R}^3, x^2+y^2\leq1\},$$ parameterized by  $$\varphi(r,\theta)=(r\cos\theta,r\sin\theta), (r,\theta)\in[0,1]\times[0,2\pi].$$ Let $\Omega(x,y,z)$ be the solid angle of $\varphi$, viewed from $(x,y,z)$. Consider a closed curve $\gamma:[a,b]\rightarrow\mathbb{R}^3\backslash S$ of class $C^1$, with $$S=\{(x,y,0)\in\mathbb{R}^3, x^2+y^2=1\}.$$ Let $p$ be the number of times that $\gamma$ cuts $D$, coming from $z>0$ to $z<0$, and $q$ the number of times that $\gamma$ cuts $D$, coming from $z<0$ to $z>0$. Use geometric arguments to conclude that $$\int_\gamma d\Omega=4\pi(p-q).$$ PS: if someone wants to know about Solid Angle, take a look at http://en.wikipedia.org/wiki/Solid_angle or http://mathworld.wolfram.com/SolidAngle.html",,"['analysis', 'integration', 'parametric']"
90,(1)Questions about differentiable functions,(1)Questions about differentiable functions,,"1)The functions $f$ and $g$: $\mathbb{R} \rightarrow \mathbb{R} $ shall be 3-times differentiable. Calculate $(f \cdot g)^{(3)}$. 1) $(f \cdot g)'=(f'g+fg')$ $(f'g+fg')'= (f''g+f'g')+(f'g'+fg'')= f''g+2f'g'+fg''$ $(f''g+2f'g'+fg'')'=(f'''g+f''g')+2(f''g'+f'g'')+(f'g''+fg''')$ $=f'''g+3(f''g'+f'g'')+fg'''=(f \cdot g)^{(3)}$ 2)Find a function f:$\mathbb{R} \rightarrow \mathbb{R} $, which is 2-times differentiable on $\mathbb{R}$ 2)$f(x)=x^2$ $f'(x)=2x$ and $f''(x)=2$ Are my solutions correct or did I sth. wrong?","1)The functions $f$ and $g$: $\mathbb{R} \rightarrow \mathbb{R} $ shall be 3-times differentiable. Calculate $(f \cdot g)^{(3)}$. 1) $(f \cdot g)'=(f'g+fg')$ $(f'g+fg')'= (f''g+f'g')+(f'g'+fg'')= f''g+2f'g'+fg''$ $(f''g+2f'g'+fg'')'=(f'''g+f''g')+2(f''g'+f'g'')+(f'g''+fg''')$ $=f'''g+3(f''g'+f'g'')+fg'''=(f \cdot g)^{(3)}$ 2)Find a function f:$\mathbb{R} \rightarrow \mathbb{R} $, which is 2-times differentiable on $\mathbb{R}$ 2)$f(x)=x^2$ $f'(x)=2x$ and $f''(x)=2$ Are my solutions correct or did I sth. wrong?",,['analysis']
91,derivative of exponential of matrix trace,derivative of exponential of matrix trace,,"What is the derivative of $\sum_{ij}e^{-d_{ij}^2(X)}=\sum_{ij}e^{-\operatorname{tr}(X^TC_{ij}X)}$, w.r.t $X$ where $C_{ij}$ is a constant matrix and $d_{ij}^2(X)$ denotes the squared Euclidean distance between the rows $i,j$ of $X$. All the entries here are real","What is the derivative of $\sum_{ij}e^{-d_{ij}^2(X)}=\sum_{ij}e^{-\operatorname{tr}(X^TC_{ij}X)}$, w.r.t $X$ where $C_{ij}$ is a constant matrix and $d_{ij}^2(X)$ denotes the squared Euclidean distance between the rows $i,j$ of $X$. All the entries here are real",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
92,"Prove that if $s_n  b$ for all but finitely many $n$, then $\lim s_n  b$.","Prove that if  for all but finitely many , then .",s_n  b n \lim s_n  b,"The question asks me to prove  that if $s_n  b$ for all but finitely many $n$, then $\lim s_n  b$ where $(s_n)$ be a sequence that converges. .  Here is how I did it but im not sure if its entirely correct. I used proof by contradiction. Suppose $\lim s_n>b$ and $s_n \leq b$ for all but finitely many $n$. Let $S=\lim s_n$. By definition we have for every $n>N$ which implies $|s_n-s|<\epsilon$. Let $\epsilon= b-2s+s_n$. Now we get $s_n-s<\epsilon=b-2s+s_n$ which simplifies to $s<b=\lim s_n<b$ which is a contradiction.","The question asks me to prove  that if $s_n  b$ for all but finitely many $n$, then $\lim s_n  b$ where $(s_n)$ be a sequence that converges. .  Here is how I did it but im not sure if its entirely correct. I used proof by contradiction. Suppose $\lim s_n>b$ and $s_n \leq b$ for all but finitely many $n$. Let $S=\lim s_n$. By definition we have for every $n>N$ which implies $|s_n-s|<\epsilon$. Let $\epsilon= b-2s+s_n$. Now we get $s_n-s<\epsilon=b-2s+s_n$ which simplifies to $s<b=\lim s_n<b$ which is a contradiction.",,['analysis']
93,Mean value theorem for essentially bounded functions,Mean value theorem for essentially bounded functions,,"I have the problem with the following: Let $f \in L^{\infty}(\mathbb{R}_+)$ is the mean value theorem in the following form: Let $0 \leq a < b < \infty$ , then $\int_{a}^{b} |f| \ d\mu = m(b-a)$ , for some $m \leq \mathrm{ess} \sup |f|$ . valid for $f$ ? We can see that $$\int_{a}^b |f| \ d\mu \leq \mathrm{ess} \sup |f|(b-a)$$ thus $f \in L^1([a,b])$ . Then $\mathrm{ess} \inf |f| = - \mathrm{ess} \sup |f|$ , so $$\mathrm{ess} \inf |f|(b-a) \leq \int_{a}^b |f| \ d\mu.$$ Thus the intermidiate value theorem applied to $|f|$ shows that there exists $x \in [a,b]$ such that $|f(x)| \in [\mathrm{ess} \inf |f|,\mathrm{ess} \sup|f| ]$ and $$ \int_{a}^b |f| \ d\mu = |f(x)| (b-a) .$$ Is it correct? How about the following: Let $0 \leq a < b < \infty$ ,  does it exist a complex number $m$ such that $\int_{a}^{b} f \ d\mu = m(b-a)$ , for some $|m| \leq \mathrm{ess} \sup |f|$ . I think for that we could use that $f= \mathrm{Re}f + i \mathrm{Im} f$ and then apply thesis from the first question (if it is true). Thank you for any help.","I have the problem with the following: Let is the mean value theorem in the following form: Let , then , for some . valid for ? We can see that thus . Then , so Thus the intermidiate value theorem applied to shows that there exists such that and Is it correct? How about the following: Let ,  does it exist a complex number such that , for some . I think for that we could use that and then apply thesis from the first question (if it is true). Thank you for any help.","f \in L^{\infty}(\mathbb{R}_+) 0 \leq a < b < \infty \int_{a}^{b} |f| \ d\mu = m(b-a) m \leq \mathrm{ess} \sup |f| f \int_{a}^b |f| \ d\mu \leq \mathrm{ess} \sup |f|(b-a) f \in L^1([a,b]) \mathrm{ess} \inf |f| = - \mathrm{ess} \sup |f| \mathrm{ess} \inf |f|(b-a) \leq \int_{a}^b |f| \ d\mu. |f| x \in [a,b] |f(x)| \in [\mathrm{ess} \inf |f|,\mathrm{ess} \sup|f| ]  \int_{a}^b |f| \ d\mu = |f(x)| (b-a) . 0 \leq a < b < \infty m \int_{a}^{b} f \ d\mu = m(b-a) |m| \leq \mathrm{ess} \sup |f| f= \mathrm{Re}f + i \mathrm{Im} f","['real-analysis', 'analysis']"
94,Is integration continuity-preserving?,Is integration continuity-preserving?,,"I am curious about the following question: Suppose that $f(x;\theta)$ is a bounded function of $x$, where the domain of $x$ is $[a,b]\subseteq \mathbb{R}$, and $\theta$ is viewed as a parameter. If $f(x;\theta)$ is continuous in $\theta$, then under what condition(s) can we guarantee that the function $$F(\theta)=\int_a^b f(x;\theta)\,\mathrm{d}x$$ is continuous in $\theta$ ? Can anyone give me some help? Thanks so much!","I am curious about the following question: Suppose that $f(x;\theta)$ is a bounded function of $x$, where the domain of $x$ is $[a,b]\subseteq \mathbb{R}$, and $\theta$ is viewed as a parameter. If $f(x;\theta)$ is continuous in $\theta$, then under what condition(s) can we guarantee that the function $$F(\theta)=\int_a^b f(x;\theta)\,\mathrm{d}x$$ is continuous in $\theta$ ? Can anyone give me some help? Thanks so much!",,['analysis']
95,Counterexamples in Double Integral,Counterexamples in Double Integral,,"I need to: $a.$ Give an example of function $f:\mathbb{R\times R}$ $\to$  $\mathbb{R}$ with domain in $[0,1]^2$ so that double integral exists but the function is not Riemann integrable. $b.$ Give an example (if any) for a non-integrable function $f:\mathbb{R\times R}$ $\to$  $\mathbb{R}$ with domain in $[0,1]^2$ such that both iterated integrals exists(i.e. in both order of integration.) Here is what I think about $(a)$: $$ f(x,y) = \begin{cases} 1, & \text{if }x=1/3, \text {and } y\in \mathbb{Q} \\ 0, & \text{otherwise } \end{cases} $$ According to me, this function should work. As irrationals are dense in $[0,1]$, points of discontinuities are not finite, thus Jordan measure is not zero. This implies function is not Riemann integrable.  As the infimums are zero for all partions of the $[0,1]^2$, lower riemann integral is zero. But refinement of partitions can also make the volume of the each little ""cube"" approach zero. Hence double integral exists. Please correct this if need be. About $(b)$, intuitively, I don't think such function exists. But I cannot give a rigorous proof for this. Again, I look forward for your help. Thanks.","I need to: $a.$ Give an example of function $f:\mathbb{R\times R}$ $\to$  $\mathbb{R}$ with domain in $[0,1]^2$ so that double integral exists but the function is not Riemann integrable. $b.$ Give an example (if any) for a non-integrable function $f:\mathbb{R\times R}$ $\to$  $\mathbb{R}$ with domain in $[0,1]^2$ such that both iterated integrals exists(i.e. in both order of integration.) Here is what I think about $(a)$: $$ f(x,y) = \begin{cases} 1, & \text{if }x=1/3, \text {and } y\in \mathbb{Q} \\ 0, & \text{otherwise } \end{cases} $$ According to me, this function should work. As irrationals are dense in $[0,1]$, points of discontinuities are not finite, thus Jordan measure is not zero. This implies function is not Riemann integrable.  As the infimums are zero for all partions of the $[0,1]^2$, lower riemann integral is zero. But refinement of partitions can also make the volume of the each little ""cube"" approach zero. Hence double integral exists. Please correct this if need be. About $(b)$, intuitively, I don't think such function exists. But I cannot give a rigorous proof for this. Again, I look forward for your help. Thanks.",,['analysis']
96,Equivalence of seminorms of Schwartz space,Equivalence of seminorms of Schwartz space,,"I have seen the definitions of the family of seminorms defined on Schwartz spaces vary slightly in literature. For example the following $$\rho_{a,b}(f) = \sup_{x\in \mathbb{R}^n}|x^aD^bf(x)|,$$ $$\hat{\rho_{a,b}}(f) = \sup_{x\in \mathbb{R}^n}|x|^{|a|}|D^bf(x)|$$ and $$\tilde{\rho_{a,b}}(f) = \sup_{x\in \mathbb{R}^n}(1+|x|)^{|a|}|D^bf(x)|,$$ where $x\in\mathbb{R}^n$, and $a$ is multiindex. How can I show the equivalence of these definitions? And another question would be whether we have $$\sup_{a,b\in\mathbb{N}}\rho_{a,b}(f)=M<\infty$$ for $f\in\mathcal{S}(\mathbb{R}^n)$. Thank you.","I have seen the definitions of the family of seminorms defined on Schwartz spaces vary slightly in literature. For example the following $$\rho_{a,b}(f) = \sup_{x\in \mathbb{R}^n}|x^aD^bf(x)|,$$ $$\hat{\rho_{a,b}}(f) = \sup_{x\in \mathbb{R}^n}|x|^{|a|}|D^bf(x)|$$ and $$\tilde{\rho_{a,b}}(f) = \sup_{x\in \mathbb{R}^n}(1+|x|)^{|a|}|D^bf(x)|,$$ where $x\in\mathbb{R}^n$, and $a$ is multiindex. How can I show the equivalence of these definitions? And another question would be whether we have $$\sup_{a,b\in\mathbb{N}}\rho_{a,b}(f)=M<\infty$$ for $f\in\mathcal{S}(\mathbb{R}^n)$. Thank you.",,"['analysis', 'fourier-analysis']"
97,Is the product of a measurable and bounded function with a Lebesgue integrable function Lebesgue integrable?,Is the product of a measurable and bounded function with a Lebesgue integrable function Lebesgue integrable?,,"If we have one function $f$ which is Lebesgue integrable and one function $g$ which is both measurable and bounded, is the product of $f \cdot g$ Lebesgue integrable or not? Thx in advance","If we have one function $f$ which is Lebesgue integrable and one function $g$ which is both measurable and bounded, is the product of $f \cdot g$ Lebesgue integrable or not? Thx in advance",,"['real-analysis', 'analysis', 'measure-theory']"
98,Generalization of Bernoulli's Inequality.,Generalization of Bernoulli's Inequality.,,"When you're first taking an Introduction to Proofs class, you eventually learn about induction. One of the first induction proofs I did was to prove for $n \geq 1$ that $(1+x)^n \geq 1+nx$. You first prove the base case, $n=1$, which yields $(1+x)^1 \geq 1+1x$, which is true. Then you assume that the inequality holds for for the arbitrary $n$ case. Then you try to prove that the inequality holds for $n+1$, i.e. $(1+x)^(n+1) \geq 1+nx$ as well. Using our assumption that $(1+x)^n \geq 1+nx$, we can multiply both sides by $1+x$, which yields $(1+x)*(1+x)^n \geq (1+nx)*(1+x).$ Since $1+nx^2+nx +x \geq 1+(n+1)x$, the inequality is proven. Now what Im trying to do is prove a generalization of Bernoullis inequality, i.e. the following. Find the least possible $\alpha < 0$ so that $\forall x \geq \alpha$ and $n \in \mathbb{N}$, that $(1+x)^n \geq 1+nx$. Prove the inequality as well as prove that your $\alpha$ is best possible. What Im trying to figure out is what is more special about this question rather than the typical inductive Bernoullis inequality proof? What is the purpose of introducing this alpha restricted to less than zero? There must be a reason for it. I read a few different textbooks and they all say that it (it referring to Bernoulli's inequality) works for $x > -1$, so Im assuming that the alpha would also be -1 because then for any $x \in \mathbb{R}$ such that $x \geq \alpha$, the normal Bernoullis inequality holds. Am I completely going off the deep end? Any input would be very much appreciated.","When you're first taking an Introduction to Proofs class, you eventually learn about induction. One of the first induction proofs I did was to prove for $n \geq 1$ that $(1+x)^n \geq 1+nx$. You first prove the base case, $n=1$, which yields $(1+x)^1 \geq 1+1x$, which is true. Then you assume that the inequality holds for for the arbitrary $n$ case. Then you try to prove that the inequality holds for $n+1$, i.e. $(1+x)^(n+1) \geq 1+nx$ as well. Using our assumption that $(1+x)^n \geq 1+nx$, we can multiply both sides by $1+x$, which yields $(1+x)*(1+x)^n \geq (1+nx)*(1+x).$ Since $1+nx^2+nx +x \geq 1+(n+1)x$, the inequality is proven. Now what Im trying to do is prove a generalization of Bernoullis inequality, i.e. the following. Find the least possible $\alpha < 0$ so that $\forall x \geq \alpha$ and $n \in \mathbb{N}$, that $(1+x)^n \geq 1+nx$. Prove the inequality as well as prove that your $\alpha$ is best possible. What Im trying to figure out is what is more special about this question rather than the typical inductive Bernoullis inequality proof? What is the purpose of introducing this alpha restricted to less than zero? There must be a reason for it. I read a few different textbooks and they all say that it (it referring to Bernoulli's inequality) works for $x > -1$, so Im assuming that the alpha would also be -1 because then for any $x \in \mathbb{R}$ such that $x \geq \alpha$, the normal Bernoullis inequality holds. Am I completely going off the deep end? Any input would be very much appreciated.",,"['real-analysis', 'analysis', 'inequality']"
99,Proof that $f'(x) = g'(x)$ if and only if $f = g + c$,Proof that  if and only if,f'(x) = g'(x) f = g + c,"Good evening, I need to prove that $f'(x) = g'(x)$ for all $x \in (a,b)$ if and only if there exist a $c \in \mathbb{R}$ with $f = g+c$. We know that $f:[a,b] \rightarrow \mathbb{R}$ and $g:[a,b] \rightarrow \mathbb{R}$ are continuous and differentiable on $(a,b)$. My attempt: Define $t(x):=c$ Let $t:[a,b] \rightarrow \mathbb{R}$ be continuous and differentiable on $(a,b)$. $t'(x)=0$ for all $x \in (a,b)$ $\Rightarrow t(x)=c$ , $c \in \mathbb{R}$ for all $x \in [a,b]$. Proof (with contraposition): assume that $t(x)$ isn't $const$ on $[a,b]$ $\Rightarrow \exists x_0 \in [a,b]$ that $t'(x_0) \neq 0$ Under the assumption there exists a $x,y \in [a,b] : f(x) \neq f(y)$ $w.l.o.g.$ assume that $x<y \Rightarrow (x,y) \leq (a,b)$ $\Rightarrow t(x)$ is differentiable on $(x,y)$ and continuous on $(x,y) \leq (a,b)$ With the mean value theorem $\Rightarrow \exists  \eta_0 \in (x,y): t(y)-t(x) = t'(\eta_0) \cdot (y-x)$ Under the assumption we know that $(y-x) > 0$ and $t(y)-t(x) \neq 0$. $\Leftrightarrow \frac {t(y)-t(x)}{y-x} = t'(\eta_0) \Rightarrow f'(\eta_0) \neq 0 \Rightarrow Assumption.$ Now since we know that the differentiation of a constant function is zero we can define our $c:= f-g \Rightarrow f'(x)-g'(x)= 0 \Rightarrow f'(x) = g'(x)$. Am I on the right track?","Good evening, I need to prove that $f'(x) = g'(x)$ for all $x \in (a,b)$ if and only if there exist a $c \in \mathbb{R}$ with $f = g+c$. We know that $f:[a,b] \rightarrow \mathbb{R}$ and $g:[a,b] \rightarrow \mathbb{R}$ are continuous and differentiable on $(a,b)$. My attempt: Define $t(x):=c$ Let $t:[a,b] \rightarrow \mathbb{R}$ be continuous and differentiable on $(a,b)$. $t'(x)=0$ for all $x \in (a,b)$ $\Rightarrow t(x)=c$ , $c \in \mathbb{R}$ for all $x \in [a,b]$. Proof (with contraposition): assume that $t(x)$ isn't $const$ on $[a,b]$ $\Rightarrow \exists x_0 \in [a,b]$ that $t'(x_0) \neq 0$ Under the assumption there exists a $x,y \in [a,b] : f(x) \neq f(y)$ $w.l.o.g.$ assume that $x<y \Rightarrow (x,y) \leq (a,b)$ $\Rightarrow t(x)$ is differentiable on $(x,y)$ and continuous on $(x,y) \leq (a,b)$ With the mean value theorem $\Rightarrow \exists  \eta_0 \in (x,y): t(y)-t(x) = t'(\eta_0) \cdot (y-x)$ Under the assumption we know that $(y-x) > 0$ and $t(y)-t(x) \neq 0$. $\Leftrightarrow \frac {t(y)-t(x)}{y-x} = t'(\eta_0) \Rightarrow f'(\eta_0) \neq 0 \Rightarrow Assumption.$ Now since we know that the differentiation of a constant function is zero we can define our $c:= f-g \Rightarrow f'(x)-g'(x)= 0 \Rightarrow f'(x) = g'(x)$. Am I on the right track?",,['analysis']
