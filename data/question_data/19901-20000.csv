,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$\left( \begin{array}{cc} 1 & 1 \\ 0 & 1 \\ \end{array} \right)$ not diagonalizable,not diagonalizable,\left( \begin{array}{cc} 1 & 1 \\ 0 & 1 \\ \end{array} \right),"I would like to ask you about this problem, that I encountered: Show that there exists no matrix T such that $$T^{-1}\cdot   \left( \begin{array}{cc} 1 & 1  \\ 0 & 1 \\  \end{array} \right)\cdot T $$ is diagonal. In other words our matrix let's call it A cannot be diagonalizable. (A being the matrix ""in between the T's""). I saw the following: $$\left( \begin{array}{cc} 1 & 1  \\ 0 & 1 \\  \end{array} \right)=\left( \begin{array}{cc} 1 & 0  \\ 0 & 1 \\  \end{array} \right)+\left( \begin{array}{cc} 0 & 1  \\ 0 & 0 \\  \end{array} \right)$$ Let's denote them: $$A=D+N$$ Also easy to see is that $DN =ND$ and $N^{2}=0$. It follows that $(D+N)^{t}=D^{t}+tN = \text{Identity}^{t}+t\left( \begin{array}{cc} 0 & 1  \\ 0 & 0 \\  \end{array} \right)=\left( \begin{array}{cc} 1 & t  \\ 0 & 1 \\  \end{array} \right)$ Note: the algebraic expression was reduced to this, since all terms $N^2$ and higher are $0$, also $D=\text{Identity}$. But I somehow fail to see why from here one can deduce (or not) that $A$ is not diagonalizable. Any hint or help greatly appreciated! Thanks","I would like to ask you about this problem, that I encountered: Show that there exists no matrix T such that $$T^{-1}\cdot   \left( \begin{array}{cc} 1 & 1  \\ 0 & 1 \\  \end{array} \right)\cdot T $$ is diagonal. In other words our matrix let's call it A cannot be diagonalizable. (A being the matrix ""in between the T's""). I saw the following: $$\left( \begin{array}{cc} 1 & 1  \\ 0 & 1 \\  \end{array} \right)=\left( \begin{array}{cc} 1 & 0  \\ 0 & 1 \\  \end{array} \right)+\left( \begin{array}{cc} 0 & 1  \\ 0 & 0 \\  \end{array} \right)$$ Let's denote them: $$A=D+N$$ Also easy to see is that $DN =ND$ and $N^{2}=0$. It follows that $(D+N)^{t}=D^{t}+tN = \text{Identity}^{t}+t\left( \begin{array}{cc} 0 & 1  \\ 0 & 0 \\  \end{array} \right)=\left( \begin{array}{cc} 1 & t  \\ 0 & 1 \\  \end{array} \right)$ Note: the algebraic expression was reduced to this, since all terms $N^2$ and higher are $0$, also $D=\text{Identity}$. But I somehow fail to see why from here one can deduce (or not) that $A$ is not diagonalizable. Any hint or help greatly appreciated! Thanks",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
1,Show that there isn't $A \in M_2(\mathbb R)$ such that $A^{2004}=\begin{pmatrix} -1 &0 \\ 0&-2 \end{pmatrix}$?,Show that there isn't  such that ?,A \in M_2(\mathbb R) A^{2004}=\begin{pmatrix} -1 &0 \\ 0&-2 \end{pmatrix},What's the easiest and simplest way to show that there isn't $A \in M_2(\mathbb R)$ such that $A^{2004}=\begin{pmatrix} -1 &0 \\   0&-2  \end{pmatrix}$ ? Thank you.,What's the easiest and simplest way to show that there isn't such that ? Thank you.,"A \in M_2(\mathbb R) A^{2004}=\begin{pmatrix}
-1 &0 \\ 
 0&-2 
\end{pmatrix}",['linear-algebra']
2,"Why do we need ""span"" in linear algebra?","Why do we need ""span"" in linear algebra?",,"In my linear algebra course in university we started learning about span and I was curious what is it good for? and if someone know, how does it relate to 3D graphics? Thank you.","In my linear algebra course in university we started learning about span and I was curious what is it good for? and if someone know, how does it relate to 3D graphics? Thank you.",,"['linear-algebra', 'span']"
3,"If $A=AB-BA$, is $A$ nilpotent? [duplicate]","If , is  nilpotent? [duplicate]",A=AB-BA A,"This question already has answers here : If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent [duplicate] (2 answers) Closed 3 years ago . Let matrix $A_{n\times n}$, be such that there exists a matrix $B$ for which $$AB-BA=A$$ Prove or disprove that there exists $m\in \mathbb N^{+}$such  $$A^m=0,$$ I know $$tr(A)=tr(AB)-tr(BA)=0$$ then I can't.Thank you","This question already has answers here : If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent [duplicate] (2 answers) Closed 3 years ago . Let matrix $A_{n\times n}$, be such that there exists a matrix $B$ for which $$AB-BA=A$$ Prove or disprove that there exists $m\in \mathbb N^{+}$such  $$A^m=0,$$ I know $$tr(A)=tr(AB)-tr(BA)=0$$ then I can't.Thank you",,"['linear-algebra', 'matrices']"
4,Does there exist a basis for the set of $2\times 2$ matrices such that all basis elements are invertible?,Does there exist a basis for the set of  matrices such that all basis elements are invertible?,2\times 2,"As the title says, I'm wondering whether there exists a basis for the set of $2\times 2$ matrices (with entries from the real numbers) such that all basis elements are invertible. I have a gut feeling that it is false, but don't know how to prove it. I know that for a matrix to be invertible, it must be row equivalent to the identity matrix and I think I may be able to use this in the proof, but I don't know how. Thanks in advance for any help, Jack","As the title says, I'm wondering whether there exists a basis for the set of $2\times 2$ matrices (with entries from the real numbers) such that all basis elements are invertible. I have a gut feeling that it is false, but don't know how to prove it. I know that for a matrix to be invertible, it must be row equivalent to the identity matrix and I think I may be able to use this in the proof, but I don't know how. Thanks in advance for any help, Jack",,"['linear-algebra', 'matrices']"
5,Action of a matrix on the exterior algebra,Action of a matrix on the exterior algebra,,"I read in a paper that if $M$ is a real square matrix of size $n$, then we can consider the action of $M$ in the third exterior algebra $\Lambda^3 \mathbb{R}^n$, and the matrix of this action are the $3\times 3$ minors of $M$. Here I am not clear what the action mentioned above is, and thus I do not understand the latter statement. Can someone help me? Thanks a lot!","I read in a paper that if $M$ is a real square matrix of size $n$, then we can consider the action of $M$ in the third exterior algebra $\Lambda^3 \mathbb{R}^n$, and the matrix of this action are the $3\times 3$ minors of $M$. Here I am not clear what the action mentioned above is, and thus I do not understand the latter statement. Can someone help me? Thanks a lot!",,['linear-algebra']
6,Singular value decomposition of product of matrices,Singular value decomposition of product of matrices,,"Given SVD(A) and SVD(B) and B is a diagonal matrix, is there a way or method to construct SVD(AB) ?","Given SVD(A) and SVD(B) and B is a diagonal matrix, is there a way or method to construct SVD(AB) ?",,['linear-algebra']
7,Distinction between vectors and points,Distinction between vectors and points,,"I've been wondering for some time now about the difference between a point and a vector. In high school, it was very important to distinguish them from each other, and we used the notation $(x,y,z)$ for points and $[x,y,z]$ for vectors. We always had to translate the point $P=(a,b,c)$ to the vector $\overrightarrow{OP} =[a,b,c]$ before we started calculating with them. Now, after I started at the university, people don't seem to care anymore. My professors either say that they're the same, or that they're almost the same, and the books I have seem to share that view. The book I use for my calculus course (Colley's Vector Calculus) says, among other things, the following: [...] we adopt the point of view that a vector field assigns to each point $\textbf{x}$ in X a vector $\textbf{F}(\textbf{x})$ in $\mathbb{R}^n$, represented by an arrow whose tail is at the point $\textbf{x}$. So it seems like a point is also a vector. My question is this: Do mathematicians distinguish between points and vectors, and if they do, in what circumstances?","I've been wondering for some time now about the difference between a point and a vector. In high school, it was very important to distinguish them from each other, and we used the notation $(x,y,z)$ for points and $[x,y,z]$ for vectors. We always had to translate the point $P=(a,b,c)$ to the vector $\overrightarrow{OP} =[a,b,c]$ before we started calculating with them. Now, after I started at the university, people don't seem to care anymore. My professors either say that they're the same, or that they're almost the same, and the books I have seem to share that view. The book I use for my calculus course (Colley's Vector Calculus) says, among other things, the following: [...] we adopt the point of view that a vector field assigns to each point $\textbf{x}$ in X a vector $\textbf{F}(\textbf{x})$ in $\mathbb{R}^n$, represented by an arrow whose tail is at the point $\textbf{x}$. So it seems like a point is also a vector. My question is this: Do mathematicians distinguish between points and vectors, and if they do, in what circumstances?",,"['linear-algebra', 'multivariable-calculus']"
8,Examples for proof of geometric vs. algebraic multiplicity,Examples for proof of geometric vs. algebraic multiplicity,,"Here you see a supposedly easy proof of a well-known theorem in linear algebra: Although I know I should understand this, I don't :-( Obviously there are too many indices and stuff, so I don't see the forest for the trees. Please could anyone give me some examples which show what is going on in this proof? I really need a step-by-step tour which connects this proof with some examples, I guess... (You could also give references which provide this kind of step-by-step approach, unfortunately most references I found were even more abstract...) Thank you in advance!","Here you see a supposedly easy proof of a well-known theorem in linear algebra: Although I know I should understand this, I don't :-( Obviously there are too many indices and stuff, so I don't see the forest for the trees. Please could anyone give me some examples which show what is going on in this proof? I really need a step-by-step tour which connects this proof with some examples, I guess... (You could also give references which provide this kind of step-by-step approach, unfortunately most references I found were even more abstract...) Thank you in advance!",,"['linear-algebra', 'intuition', 'eigenvalues-eigenvectors', 'examples-counterexamples']"
9,Prove vector space is the direct sum of subspace and its orthogonal complement,Prove vector space is the direct sum of subspace and its orthogonal complement,,"$V$ is finite-dimensional over $\Bbb{C}$ and the form $\langle \cdot , \cdot \rangle$ is Hermitian. $U$ is a subspace of $V$ . Show that $V = U \oplus U^\perp$ I've been able to show that $U \cap U^\perp = \{0\}$ . I don't know how to approach the problem showing that every vector $v\in V$ can be written as $v = u + u'$ , where $u, u'$ are in $U$ and $U^\perp$ respectively.","is finite-dimensional over and the form is Hermitian. is a subspace of . Show that I've been able to show that . I don't know how to approach the problem showing that every vector can be written as , where are in and respectively.","V \Bbb{C} \langle \cdot , \cdot \rangle U V V = U \oplus U^\perp U \cap U^\perp = \{0\} v\in V v = u + u' u, u' U U^\perp","['linear-algebra', 'vector-spaces', 'inner-products', 'bilinear-form', 'direct-sum']"
10,What is step by step logic of pinv (pseudoinverse)?,What is step by step logic of pinv (pseudoinverse)?,,"So we have a matrix $A$ size of $M \times N$ with elements $a_{i,j}$ . What is a step by step algorithm that returns the Moore-Penrose inverse $A^+$ for a given $A$ (on level of manipulations/operations with $a_{i,j}$ elements, not vectors)?","So we have a matrix size of with elements . What is a step by step algorithm that returns the Moore-Penrose inverse for a given (on level of manipulations/operations with elements, not vectors)?","A M \times N a_{i,j} A^+ A a_{i,j}","['linear-algebra', 'matrices', 'algorithms', 'numerical-linear-algebra', 'pseudoinverse']"
11,Why is the condition enough for a matrix to be diagonalizable?,Why is the condition enough for a matrix to be diagonalizable?,,"I've heard that for a matrix $A\in M_n(\mathbb{C})$, if $A^3=A$, then $A$ is diagonalizable. Does there happen to be a proof or reference as to why this is true? Out of curiosity, is it necessary that the entries be from $\mathbb{C}$? Would any field $F$ work just as well, or is this possibly a special fact related to the properties of $\mathbb{C}$?","I've heard that for a matrix $A\in M_n(\mathbb{C})$, if $A^3=A$, then $A$ is diagonalizable. Does there happen to be a proof or reference as to why this is true? Out of curiosity, is it necessary that the entries be from $\mathbb{C}$? Would any field $F$ work just as well, or is this possibly a special fact related to the properties of $\mathbb{C}$?",,['linear-algebra']
12,Reduced row echelon form and linear independence,Reduced row echelon form and linear independence,,"Let's say I have the set of vectors $S = \{v_1, v_2, ..., v_n\}$ where $v_j \in R^m$, $v_j = (a_{1j}, a_{2j}, ..., a_{mj})$. If the matrix formed by each of the vectors $A=[v_1, v_2, ..., v_n]$ looks like this (I believe), which is not a square matrix : $$A = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix}$$ Then does A's reduced row echelon form help me determine whether the vectors of $S$ are linear dependent or independent? If so, how? I hope I got all the indices, notation and terminology right, since I am a beginner in linear algebra, and English is not my native language.","Let's say I have the set of vectors $S = \{v_1, v_2, ..., v_n\}$ where $v_j \in R^m$, $v_j = (a_{1j}, a_{2j}, ..., a_{mj})$. If the matrix formed by each of the vectors $A=[v_1, v_2, ..., v_n]$ looks like this (I believe), which is not a square matrix : $$A = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix}$$ Then does A's reduced row echelon form help me determine whether the vectors of $S$ are linear dependent or independent? If so, how? I hope I got all the indices, notation and terminology right, since I am a beginner in linear algebra, and English is not my native language.",,"['linear-algebra', 'independence']"
13,"If $\lambda$ is an eigenvalue of $A^2$, then either $\sqrt{\lambda}$ or $-\sqrt{\lambda}$ is an eigenvalue of $A$","If  is an eigenvalue of , then either  or  is an eigenvalue of",\lambda A^2 \sqrt{\lambda} -\sqrt{\lambda} A,"$A$ is an $n\times n$ matrix of complex numbers. Prove that if $\lambda$ is an eigenvalue of $A^2,$ then $\sqrt{\lambda}$ or $-\sqrt{\lambda}$ is an eigenvalue of $A.$ If $\lambda$ is an eigenvalue of $A^2,$ we have $\lambda\alpha=A^2\alpha$ for some $\alpha.$ Then how can we find a $\beta$ s.t. $\sqrt{\lambda}\beta=A\beta?$","$A$ is an $n\times n$ matrix of complex numbers. Prove that if $\lambda$ is an eigenvalue of $A^2,$ then $\sqrt{\lambda}$ or $-\sqrt{\lambda}$ is an eigenvalue of $A.$ If $\lambda$ is an eigenvalue of $A^2,$ we have $\lambda\alpha=A^2\alpha$ for some $\alpha.$ Then how can we find a $\beta$ s.t. $\sqrt{\lambda}\beta=A\beta?$",,"['linear-algebra', 'eigenvalues-eigenvectors']"
14,Why do positive definite symmetric matrices have the same singular values as eigenvalues?,Why do positive definite symmetric matrices have the same singular values as eigenvalues?,,I realize that this is because when the eigenvalues are either 0 or 1 they will have the same square root. But why does this happen?,I realize that this is because when the eigenvalues are either 0 or 1 they will have the same square root. But why does this happen?,,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd', 'positive-definite', 'symmetric-matrices']"
15,"Example of a subset of $\mathbb{R}^2$ that is closed under vector addition, but not closed under scalar multiplication?","Example of a subset of  that is closed under vector addition, but not closed under scalar multiplication?",\mathbb{R}^2,"I've found several examples which are closed under scalar multiplication, but not vector addition, but I can't come up with one that is closed under vector addition, but not scalar multiplication.","I've found several examples which are closed under scalar multiplication, but not vector addition, but I can't come up with one that is closed under vector addition, but not scalar multiplication.",,['linear-algebra']
16,Basic Linear Algebra Proof - Orthogonal Vectors,Basic Linear Algebra Proof - Orthogonal Vectors,,"Prove that if $\mathbf{u}$ and $\mathbf{v}$ are nonzero orthogonal vectors in $\Bbb R^n$ they are linearly Independent. I've struggled with this a bit, here is what I know so far: Suppose $\mathbf{u}$ and $\mathbf{v}$ are orthogonal.  Then $\mathbf{u\cdot v}=0$ and $c_1\mathbf{u}+c_2\mathbf{v}=0$ is linearly Independent iff $c_1=c_2=0$ I know I need to end with $c_1=c_2=0$ but I can't find a path that reaches this conclusion.  I feel like I need to use properties of the dot product to connect my first assumption to my second assumption but I'm lost on the way.","Prove that if $\mathbf{u}$ and $\mathbf{v}$ are nonzero orthogonal vectors in $\Bbb R^n$ they are linearly Independent. I've struggled with this a bit, here is what I know so far: Suppose $\mathbf{u}$ and $\mathbf{v}$ are orthogonal.  Then $\mathbf{u\cdot v}=0$ and $c_1\mathbf{u}+c_2\mathbf{v}=0$ is linearly Independent iff $c_1=c_2=0$ I know I need to end with $c_1=c_2=0$ but I can't find a path that reaches this conclusion.  I feel like I need to use properties of the dot product to connect my first assumption to my second assumption but I'm lost on the way.",,['linear-algebra']
17,"If $A,B$ symmetric positive semidefinite, show tr$(AB) \geq 0$","If  symmetric positive semidefinite, show tr","A,B (AB) \geq 0","Supposing $V$ is a finite dimensional vector space (over $\mathbb{R}$) of dimension $n$, and $A,B$ are symmetric positive definite linear mappings from $V$ to $V$, how can I show that in any orthonormal basis $\mathrm{tr}(AB) \geq 0$? I noticed that since they are symmetric we have that  $$\mathrm{tr}(AB) = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ji} = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ij}$$ which is the sum of the elements of the element-wise product of $A,B$. I don't know if this is helpful.","Supposing $V$ is a finite dimensional vector space (over $\mathbb{R}$) of dimension $n$, and $A,B$ are symmetric positive definite linear mappings from $V$ to $V$, how can I show that in any orthonormal basis $\mathrm{tr}(AB) \geq 0$? I noticed that since they are symmetric we have that  $$\mathrm{tr}(AB) = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ji} = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ij}$$ which is the sum of the elements of the element-wise product of $A,B$. I don't know if this is helpful.",,['linear-algebra']
18,Why is this true for matrices? Linearly dependent columns $\implies$ not invertible,Why is this true for matrices? Linearly dependent columns  not invertible,\implies,"If $A$ is a square matrix with linearly dependent columns, then $A$ is not invertible. Why is this true for matrices?","If $A$ is a square matrix with linearly dependent columns, then $A$ is not invertible. Why is this true for matrices?",,"['linear-algebra', 'matrices']"
19,"With regards to vector spaces, what does it mean to be 'closed under addition?'","With regards to vector spaces, what does it mean to be 'closed under addition?'",,My linear algebra book uses this term in the definition of a vector space with no prior explanation whatsoever.  It proceeds to then use this term to explain the proofs. Is there something painfully obvious I'm missing about this terminology and is this something I should already be familiar with? The proof uses $u + v$ is in $V$,My linear algebra book uses this term in the definition of a vector space with no prior explanation whatsoever.  It proceeds to then use this term to explain the proofs. Is there something painfully obvious I'm missing about this terminology and is this something I should already be familiar with? The proof uses $u + v$ is in $V$,,"['linear-algebra', 'vector-spaces']"
20,What exactly does linear dependence and linear independence imply?,What exactly does linear dependence and linear independence imply?,,"I have a very hard time remembering which is which between linear independence and linear dependence... that is, if I am asked to specify whether a set of vectors are linearly dependent or independent, I'd be able to find out if $\vec{x}=\vec{0}$ is the only solution to $A\vec{x}=\vec{0}$, but I would be stuck guessing whether that means that the vectors are linearly dependent or independent. Is there a way that I can understand what the consequence of this trait is so that I can confidently answer such a question on a test?","I have a very hard time remembering which is which between linear independence and linear dependence... that is, if I am asked to specify whether a set of vectors are linearly dependent or independent, I'd be able to find out if $\vec{x}=\vec{0}$ is the only solution to $A\vec{x}=\vec{0}$, but I would be stuck guessing whether that means that the vectors are linearly dependent or independent. Is there a way that I can understand what the consequence of this trait is so that I can confidently answer such a question on a test?",,['linear-algebra']
21,How do I calculate the $p$-norm of a matrix?,How do I calculate the -norm of a matrix?,p,"I know that the $p$-norm for a matrix is: $$\|A\| = \max_{x\neq 0} \frac{\|Ax\|_p}{\|x\|_p}$$ but I don't know what this really means. So how would I compute the $2$-norm, $3$-norm, etc for the matrix. $$A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$$ UPDATE Apparently, the above matrix is too easy :) Let's try something harder. $$A = \begin{bmatrix} 2 & 1 & 4 \\ 3 & 0 & -1 \\ 1 & 1 & 2 \end{bmatrix}$$ Thanks,","I know that the $p$-norm for a matrix is: $$\|A\| = \max_{x\neq 0} \frac{\|Ax\|_p}{\|x\|_p}$$ but I don't know what this really means. So how would I compute the $2$-norm, $3$-norm, etc for the matrix. $$A = \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$$ UPDATE Apparently, the above matrix is too easy :) Let's try something harder. $$A = \begin{bmatrix} 2 & 1 & 4 \\ 3 & 0 & -1 \\ 1 & 1 & 2 \end{bmatrix}$$ Thanks,",,"['linear-algebra', 'matrices', 'normed-spaces']"
22,What is the relation between the high school definition of a vector and the abstract algebra definition?,What is the relation between the high school definition of a vector and the abstract algebra definition?,,"I'm studying linear algebra, and I've been getting to grips with the idea of groups and fields and vector spaces. From what I understand, to be a vector is just to be an element of a vector space, and all sorts of unusual (to the beginner) things turn out to be vector spaces. For example, the set of all real-valued functions is a vector space over $\mathbb{R}$ . I take it this means that all real-valued functions (e.g. $\sin x$ ) are vectors over $\mathbb{R}$ . But how does this abstract definition fit with the earlier account we are given of vectors as quantities with magnitude and direction or directed line segments?","I'm studying linear algebra, and I've been getting to grips with the idea of groups and fields and vector spaces. From what I understand, to be a vector is just to be an element of a vector space, and all sorts of unusual (to the beginner) things turn out to be vector spaces. For example, the set of all real-valued functions is a vector space over . I take it this means that all real-valued functions (e.g. ) are vectors over . But how does this abstract definition fit with the earlier account we are given of vectors as quantities with magnitude and direction or directed line segments?",\mathbb{R} \sin x \mathbb{R},"['linear-algebra', 'vector-spaces', 'vectors']"
23,"Showing $A-I$ is invertible, when $A$ is a skew-symmetric matrix","Showing  is invertible, when  is a skew-symmetric matrix",A-I A,"Let $A\in M_{n\times n}(\mathbb{R})$ be a skew-symmetric  matrix. show that $A-I$ is invertible and $(A-I)^{-1}(A+I)$ is an orthogonal matrix. $|A-I|=|(A-I)^T|=|-(A+I)|=(-1)^n|A+I|$ I have no clue to solve this problem, I appreciate any help.","Let $A\in M_{n\times n}(\mathbb{R})$ be a skew-symmetric  matrix. show that $A-I$ is invertible and $(A-I)^{-1}(A+I)$ is an orthogonal matrix. $|A-I|=|(A-I)^T|=|-(A+I)|=(-1)^n|A+I|$ I have no clue to solve this problem, I appreciate any help.",,"['linear-algebra', 'matrices', 'orthogonality']"
24,Is the $L$ in $LU$ factorization unique?,Is the  in  factorization unique?,L LU,"I was doing an $LU$ factorization problem \begin{bmatrix}     2       & 3 & 2  \\     4       & 13 & 9  \\ -6 & 5 &4 \end{bmatrix} and I was going to multiply the second row by .$5$ and subtract the result from row $1$, then do something similar to row reduce row $3$. My book tells me that you first row-reduce to get $U$, then you extract certain columns from various steps in the reduction process to get $L$. That's when I realized that if I instead multiplied row $1$ by $2$ subtracted from one times row $2$, this would be equivalent to getting $U$, but it would change the resultant $L$ matrix! What is wrong with this approach?","I was doing an $LU$ factorization problem \begin{bmatrix}     2       & 3 & 2  \\     4       & 13 & 9  \\ -6 & 5 &4 \end{bmatrix} and I was going to multiply the second row by .$5$ and subtract the result from row $1$, then do something similar to row reduce row $3$. My book tells me that you first row-reduce to get $U$, then you extract certain columns from various steps in the reduction process to get $L$. That's when I realized that if I instead multiplied row $1$ by $2$ subtracted from one times row $2$, this would be equivalent to getting $U$, but it would change the resultant $L$ matrix! What is wrong with this approach?",,"['linear-algebra', 'matrices', 'numerical-methods', 'matrix-decomposition', 'lu-decomposition']"
25,"Why the spectral theorem is named ""spectral theorem""?","Why the spectral theorem is named ""spectral theorem""?",,"""If $V$ is a complex inner product space and $T\in \mathcal{L}(V)$. Then $V$ has an orthonormal basis Consisting of eigenvectors of T if and only if $T$ is normal"". I know that the set of orthonormal vectors is called the ""spectrum"" and I guess that's where the name of the theorem. But what is the reason for naming it?","""If $V$ is a complex inner product space and $T\in \mathcal{L}(V)$. Then $V$ has an orthonormal basis Consisting of eigenvectors of T if and only if $T$ is normal"". I know that the set of orthonormal vectors is called the ""spectrum"" and I guess that's where the name of the theorem. But what is the reason for naming it?",,['linear-algebra']
26,Rank of a matrix,Rank of a matrix,,"If a $3 \times 3$ matrix has determinant zero, then is it possible that its rank could be $3$? I think it only could be $2$ or less. I am right or wrong? Please explain.","If a $3 \times 3$ matrix has determinant zero, then is it possible that its rank could be $3$? I think it only could be $2$ or less. I am right or wrong? Please explain.",,['linear-algebra']
27,Spectrum vs eigenvalues,Spectrum vs eigenvalues,,Easy question about linear operators - in physics (often) the terms spectrum and (set of) eigenvalues of an operator are used interchangeably. I'd like a simple compare and contrast to know the difference according to mathematicians. Many thanks!,Easy question about linear operators - in physics (often) the terms spectrum and (set of) eigenvalues of an operator are used interchangeably. I'd like a simple compare and contrast to know the difference according to mathematicians. Many thanks!,,"['linear-algebra', 'functional-analysis', 'spectral-theory']"
28,"If $\ker f\subset \ker g$ where $f,g $ are non-zero linear functionals then show that $f=cg$ for some $c\in F$.",If  where  are non-zero linear functionals then show that  for some .,"\ker f\subset \ker g f,g  f=cg c\in F","Let $V$ be a vector space with $\dim V=n$ . If $\ker f\subset \ker g$ where $f,g $ are non-zero linear functionals then show that $f=cg$ for some $c\in F$ . Now let $\mathcal B=\{v_1,v_2,\ldots ,v_n\}$ be a basis of $V$ , since $f,g$ are non-zero linear functionals then $\exists v_i\in \mathcal B $ such that $g(v_i)\neq 0\implies f(v_i)\neq 0$ Take $i=1$ without any loss of generality so take $g(v_1)\neq 0,f(v_1)\neq 0$ . Now take $c=\dfrac{f(v_1)}{g(v_1)}$ Then we need to show that $(f-cg)(v_i)=0\forall i$ Now $(f-cg)(v_1)=0$ How to show that $(f-cg)(v_i)=0\forall i\ge 2$ Can someone please help? Note::Another Question Why do we need the dimension of the vector space to be finite?","Let be a vector space with . If where are non-zero linear functionals then show that for some . Now let be a basis of , since are non-zero linear functionals then such that Take without any loss of generality so take . Now take Then we need to show that Now How to show that Can someone please help? Note::Another Question Why do we need the dimension of the vector space to be finite?","V \dim V=n \ker f\subset \ker g f,g  f=cg c\in F \mathcal B=\{v_1,v_2,\ldots ,v_n\} V f,g \exists v_i\in \mathcal B  g(v_i)\neq 0\implies f(v_i)\neq 0 i=1 g(v_1)\neq 0,f(v_1)\neq 0 c=\dfrac{f(v_1)}{g(v_1)} (f-cg)(v_i)=0\forall i (f-cg)(v_1)=0 (f-cg)(v_i)=0\forall i\ge 2",['linear-algebra']
29,Mistake in proof about vector spaces,Mistake in proof about vector spaces,,"Hello this is my first post on here and I have a question about an error in a proof. My Linear Algebra professor was covering Linear Independence, Basis, and Dimension for Vector Spaces. We got to a proof from the textbook and he said the proof is wrong. He said to try and figure out what is wrong with it. I have been trying to figure out what is wrong and after thinking about it, talking with peers, and with my professor I have narrowed it down to a single sentence within the proof. I think that it is a wrong word choice. He said it is very subtle. I was hoping some of you can provide some insight. THM 6.11: Let $W$ be a subspace of a finite-dimensional vector space $V$. Then: (a) $W$ is finite-dimensional and $\dim W \le \dim V$. (b) $\dim W = \dim V$ if and only if $W = V$. He has a problem with the proof of part (a) which is as follows. Proof (a) Let $\dim V = n$. If $W = \{\mathbf{0}\}$, then $\dim(W) = 0 \le n = \dim V$. If $W$ is nonzero, then any basis $\mathcal{B}$ for $V$ (containing $n$ vectors) certainly spans $W$, since $W$ is contained in $V$. But $\mathcal{B}$  can be reduced to a basis $\mathcal{B'}$ for $W$ (containing at most $n$ vectors), by Theorem 6.10(f). Hence, $W$ is finite-dimensional and $\dim W \le n = \dim V$. Theorem 6.10(f) states: Let $V$ be a vector space with $\dim V = n$. Then: (f) Any spanning set for $V$ can be reduced to a basis for $V$. My professor said that me and my peers were right about the error being in the sentence, ""But $\mathcal{B}$  can be reduced to a basis $\mathcal{B'}$ for $W$ (containing at most $n$ vectors), by Theorem 6.10(f)."" I believe that the error has something to do with with the word reduced . I asked him and he said that I was on the right track but he didn't give me a clear answer. What is wrong with this proof? More specifically the sentence above? Any help would be much appreciated. (Textbook: Linear Algebra: A Modern Introduction by David Poole 4th edition. The proof is found in section 6.2: page 456)","Hello this is my first post on here and I have a question about an error in a proof. My Linear Algebra professor was covering Linear Independence, Basis, and Dimension for Vector Spaces. We got to a proof from the textbook and he said the proof is wrong. He said to try and figure out what is wrong with it. I have been trying to figure out what is wrong and after thinking about it, talking with peers, and with my professor I have narrowed it down to a single sentence within the proof. I think that it is a wrong word choice. He said it is very subtle. I was hoping some of you can provide some insight. THM 6.11: Let $W$ be a subspace of a finite-dimensional vector space $V$. Then: (a) $W$ is finite-dimensional and $\dim W \le \dim V$. (b) $\dim W = \dim V$ if and only if $W = V$. He has a problem with the proof of part (a) which is as follows. Proof (a) Let $\dim V = n$. If $W = \{\mathbf{0}\}$, then $\dim(W) = 0 \le n = \dim V$. If $W$ is nonzero, then any basis $\mathcal{B}$ for $V$ (containing $n$ vectors) certainly spans $W$, since $W$ is contained in $V$. But $\mathcal{B}$  can be reduced to a basis $\mathcal{B'}$ for $W$ (containing at most $n$ vectors), by Theorem 6.10(f). Hence, $W$ is finite-dimensional and $\dim W \le n = \dim V$. Theorem 6.10(f) states: Let $V$ be a vector space with $\dim V = n$. Then: (f) Any spanning set for $V$ can be reduced to a basis for $V$. My professor said that me and my peers were right about the error being in the sentence, ""But $\mathcal{B}$  can be reduced to a basis $\mathcal{B'}$ for $W$ (containing at most $n$ vectors), by Theorem 6.10(f)."" I believe that the error has something to do with with the word reduced . I asked him and he said that I was on the right track but he didn't give me a clear answer. What is wrong with this proof? More specifically the sentence above? Any help would be much appreciated. (Textbook: Linear Algebra: A Modern Introduction by David Poole 4th edition. The proof is found in section 6.2: page 456)",,"['linear-algebra', 'vector-spaces', 'proof-explanation']"
30,Skew symmetric matrix of vector,Skew symmetric matrix of vector,,"During my course in linear algebra, the instructor stated that A cross B is the same as the ""skew symmetric matrix"" of A times B. So, first of all, can someone clarify or provide sources about skew symmetric matrices? Secondly, I can't really comprehend the idea of how a single column vector crossed with another could be represented by a matrix. Anyhow, thanks in advance!","During my course in linear algebra, the instructor stated that A cross B is the same as the ""skew symmetric matrix"" of A times B. So, first of all, can someone clarify or provide sources about skew symmetric matrices? Secondly, I can't really comprehend the idea of how a single column vector crossed with another could be represented by a matrix. Anyhow, thanks in advance!",,"['linear-algebra', 'matrices', 'cross-product']"
31,Help with proving that the transpose of the product of any number of matrices is equal to the product of their transposes in reverse,Help with proving that the transpose of the product of any number of matrices is equal to the product of their transposes in reverse,,"Specifically I am trying to show that (A n ) T = (A T ) n where A is an mxm square matrix and n is a positive integer. This is where I'm stuck: To prove the theorem I would like to show that ((A n ) T ) ij = ((A T ) n ) ij for all ij . All I can think of is expanding the definition of matrix multiplication. Left side of equation: ((A n ) T ) ij = (A n ) ji = (a n-1 ) 1i a j1 + (a n-1 ) 2i a j2 +...+ (a n-1 ) mi a jm Right side of the equation: Let a' denote the ij th entry of A T ((A T ) n ) ij = (a' n-1 ) i1 a' 1j + (a' n-1 ) i2 a' 2j +...+ (a' n-1 ) im a' mj = (a n-1 ) 1i a j1 + (a n-1 ) 2i a j2 +...+ (a n-1 ) mi a jm Now the left and the right side of the equation are shown to be equal. In this proof, I mean for A n to represent the product AAA... up to n. So if n= 3, this would represent the matrix resulting from the product of (AAA). The problem I have with this is that with my proof,  determining the value in a specific position, say (AAA) ij , you must first determine the values of AA, and so on depending on the value of n. It just seems like there must be a better way of doing this proof. Can anyone help me out or show me why what I am doing is correct, or more likely, incorrect? I am teaching myself linear algebra from Howard Anton's Elementary Linear Algebra text and unfortunately there seems to be a lot of assumed prior knowledge. I could really use detailed to the point of redundant explanations at this point in my learning! Also, since I have no one to interact with in constructing my proofs, I fear that I am missing some common practices. So feel free to be very critical of my format, so that I can get on track.","Specifically I am trying to show that (A n ) T = (A T ) n where A is an mxm square matrix and n is a positive integer. This is where I'm stuck: To prove the theorem I would like to show that ((A n ) T ) ij = ((A T ) n ) ij for all ij . All I can think of is expanding the definition of matrix multiplication. Left side of equation: ((A n ) T ) ij = (A n ) ji = (a n-1 ) 1i a j1 + (a n-1 ) 2i a j2 +...+ (a n-1 ) mi a jm Right side of the equation: Let a' denote the ij th entry of A T ((A T ) n ) ij = (a' n-1 ) i1 a' 1j + (a' n-1 ) i2 a' 2j +...+ (a' n-1 ) im a' mj = (a n-1 ) 1i a j1 + (a n-1 ) 2i a j2 +...+ (a n-1 ) mi a jm Now the left and the right side of the equation are shown to be equal. In this proof, I mean for A n to represent the product AAA... up to n. So if n= 3, this would represent the matrix resulting from the product of (AAA). The problem I have with this is that with my proof,  determining the value in a specific position, say (AAA) ij , you must first determine the values of AA, and so on depending on the value of n. It just seems like there must be a better way of doing this proof. Can anyone help me out or show me why what I am doing is correct, or more likely, incorrect? I am teaching myself linear algebra from Howard Anton's Elementary Linear Algebra text and unfortunately there seems to be a lot of assumed prior knowledge. I could really use detailed to the point of redundant explanations at this point in my learning! Also, since I have no one to interact with in constructing my proofs, I fear that I am missing some common practices. So feel free to be very critical of my format, so that I can get on track.",,"['linear-algebra', 'matrices', 'transpose']"
32,"Example that the Jordan canonical form is not ""robust.""","Example that the Jordan canonical form is not ""robust.""",,"I'm working on this problem that asks to show that the Jordan canonical form is not robust in the sense that small changes in the entries of a matrix $A$ can cause large changes in the entries of its Jordan form $J$. The suggestion is the consider the matrix $$ A_\epsilon=\begin{bmatrix} \epsilon & 0 \\ 1 & 0\end{bmatrix} $$ and to see what happens to the Jordan form of $A_\epsilon$ as $\epsilon\to 0$. To me, the minimal polynomial of $A_\epsilon$ is then $x^2-\epsilon x$, so its eigenvalues are $0$ and $\epsilon$, and the Jordan canonical form is  $$\begin{bmatrix} \epsilon & 0 \\ 0 & 0\end{bmatrix}. $$ But then it seems small changes in the entries of $A$ correspond to equally small changes in the Jordan form of $A$. Did I do something wrong? The problem is 13 of Chapter 8 in Steven Roman's Advanced Linear Algebra.","I'm working on this problem that asks to show that the Jordan canonical form is not robust in the sense that small changes in the entries of a matrix $A$ can cause large changes in the entries of its Jordan form $J$. The suggestion is the consider the matrix $$ A_\epsilon=\begin{bmatrix} \epsilon & 0 \\ 1 & 0\end{bmatrix} $$ and to see what happens to the Jordan form of $A_\epsilon$ as $\epsilon\to 0$. To me, the minimal polynomial of $A_\epsilon$ is then $x^2-\epsilon x$, so its eigenvalues are $0$ and $\epsilon$, and the Jordan canonical form is  $$\begin{bmatrix} \epsilon & 0 \\ 0 & 0\end{bmatrix}. $$ But then it seems small changes in the entries of $A$ correspond to equally small changes in the Jordan form of $A$. Did I do something wrong? The problem is 13 of Chapter 8 in Steven Roman's Advanced Linear Algebra.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
33,why scalar projection does not yield coordinates?,why scalar projection does not yield coordinates?,,"Suppose we have an ordered basis $\{v_1,\dots,v_n\}$ in some inner product space. Let us project a vector $v$ on each $v_i$ by multiplying $v_i$ by the ""scalar projection"" $(v,v_i)/\|v_i\|$. Intuitively, it seems that each scalar projection $(v,v_i)/\|v_i\|$ indicates the amount of $v$ that goes in $v_i$ and therefore the $i^{th}$ coordinate of $v$ should be $(v,v_i)/\|v_i\|$. But that does not happen unless the basis is orthogonal. Mathematically I can justify this but can someone give an intuitive reason as to what goes wrong. For example with $B=\{(1,0),(1,1)\}$ in the Euclidean space $\mathbb R^2$ where $v=(0,1)$?","Suppose we have an ordered basis $\{v_1,\dots,v_n\}$ in some inner product space. Let us project a vector $v$ on each $v_i$ by multiplying $v_i$ by the ""scalar projection"" $(v,v_i)/\|v_i\|$. Intuitively, it seems that each scalar projection $(v,v_i)/\|v_i\|$ indicates the amount of $v$ that goes in $v_i$ and therefore the $i^{th}$ coordinate of $v$ should be $(v,v_i)/\|v_i\|$. But that does not happen unless the basis is orthogonal. Mathematically I can justify this but can someone give an intuitive reason as to what goes wrong. For example with $B=\{(1,0),(1,1)\}$ in the Euclidean space $\mathbb R^2$ where $v=(0,1)$?",,['linear-algebra']
34,How can I find the dimension of the eigenspace?,How can I find the dimension of the eigenspace?,,The matrix $A = \begin{bmatrix}9&-1\\1&7\end{bmatrix}$ has one eigenvalue of multiplicity 2. Find this eigenvalue and the dimension of the eigenspace. So I found the eigenvalue by doing $A - \lambda I$ to get: $\lambda = 8$ But how exactly do I find the dimension of the eigenspace?,The matrix has one eigenvalue of multiplicity 2. Find this eigenvalue and the dimension of the eigenspace. So I found the eigenvalue by doing to get: But how exactly do I find the dimension of the eigenspace?,A = \begin{bmatrix}9&-1\\1&7\end{bmatrix} A - \lambda I \lambda = 8,"['linear-algebra', 'eigenvalues-eigenvectors']"
35,"What forms does the Moore-Penrose inverse take under systems with full rank, full column rank, and full row rank?","What forms does the Moore-Penrose inverse take under systems with full rank, full column rank, and full row rank?",,The normal form $ (A'A)x = A'b$ gives a solution to the least square problem. When $A$ has full rank $x = (A'A)^{-1}A'b$ is the least square solution. How can we show that the moore-penrose solves the least square problem and hence is equal to $(A'A)^{-1}A'$. Also what happens in a rank deficient matrix ? $(A'A)^{-1}$ would not exist so is the moore-penrose inverse still equal to $(A'A)^{-1}A'$ ? Thanks,The normal form $ (A'A)x = A'b$ gives a solution to the least square problem. When $A$ has full rank $x = (A'A)^{-1}A'b$ is the least square solution. How can we show that the moore-penrose solves the least square problem and hence is equal to $(A'A)^{-1}A'$. Also what happens in a rank deficient matrix ? $(A'A)^{-1}$ would not exist so is the moore-penrose inverse still equal to $(A'A)^{-1}A'$ ? Thanks,,"['linear-algebra', 'least-squares', 'svd', 'pseudoinverse']"
36,why does the reduced row echelon form have the same null space as the original matrix?,why does the reduced row echelon form have the same null space as the original matrix?,,What is the proof for this and the intuitive explanation for why the reduced row echelon form have the same null space as the original matrix?,What is the proof for this and the intuitive explanation for why the reduced row echelon form have the same null space as the original matrix?,,"['linear-algebra', 'matrices']"
37,Inverse of orthogonal matrix is orthogonal matrix?,Inverse of orthogonal matrix is orthogonal matrix?,,"Is inverse of an orthogonal matrix an orthogonal matrix? I know its inverse is equal to its transpose, but I don't see where the orthogonality would come from.","Is inverse of an orthogonal matrix an orthogonal matrix? I know its inverse is equal to its transpose, but I don't see where the orthogonality would come from.",,"['linear-algebra', 'matrices', 'orthonormal']"
38,Does certain matrix commutes with square root of another one?,Does certain matrix commutes with square root of another one?,,"I can't figure out if this is true: Suppose $A^T=-A$ and that the symmetric matrix $AA^T$ is a positive definite (so diagonalizable, lets say $AA^T=O\Lambda O^T$, with all eigenvalues positive). Therefore, we can define a square root by $$\sqrt{AA^T}=O\sqrt{\Lambda}O^T,$$ where $\sqrt{\Lambda}=\textrm{diag}(\sqrt{\lambda_1},\ldots\sqrt{\lambda_n})$, $\lambda_i$ eigenvalues of $AA^T$. One can see that $A$ commutes with $AA^T$, once $A^T=-A$. But I can't see if it is true that: $$A\sqrt{AA^T}=\sqrt{AA^T}A$$ In other words, does $A$ commute with $AA^T$ implies $A$ commutes with $\sqrt{AA^T}$? Any help will be appreciated.","I can't figure out if this is true: Suppose $A^T=-A$ and that the symmetric matrix $AA^T$ is a positive definite (so diagonalizable, lets say $AA^T=O\Lambda O^T$, with all eigenvalues positive). Therefore, we can define a square root by $$\sqrt{AA^T}=O\sqrt{\Lambda}O^T,$$ where $\sqrt{\Lambda}=\textrm{diag}(\sqrt{\lambda_1},\ldots\sqrt{\lambda_n})$, $\lambda_i$ eigenvalues of $AA^T$. One can see that $A$ commutes with $AA^T$, once $A^T=-A$. But I can't see if it is true that: $$A\sqrt{AA^T}=\sqrt{AA^T}A$$ In other words, does $A$ commute with $AA^T$ implies $A$ commutes with $\sqrt{AA^T}$? Any help will be appreciated.",,"['linear-algebra', 'matrices']"
39,How to show a matrix is full rank?,How to show a matrix is full rank?,,"I have some discussion with my friend about matrix rank. But we find that even we know how to compute rank, we don't know how to show the matrix is full rank. How to show this?","I have some discussion with my friend about matrix rank. But we find that even we know how to compute rank, we don't know how to show the matrix is full rank. How to show this?",,"['linear-algebra', 'matrices']"
40,Froebenius norm is unitarily invariant.,Froebenius norm is unitarily invariant.,,"I'm considering the norm defined on matrices by $$\|A\|_F = \sqrt{\sum_{i,j}|a_{ij}|^2}$$ I want to show that it is unitarily invariant, so that for unitary $U$ we have that $$\|UA\|_F = \|A\|_F = \|AU\|_F$$ however I have trouble doing it directly. Writing $\|UA\|_F$ directly I find by Cauchy-Schwarz that $$\|UA\|_F = \sqrt{\sum_{i,j}\left|\sum_{k=1}^{n}u_{ik}a_{kj}\right|^2}= \sqrt{\sum_{i,j}|\langle U_i,\overline{A_j}\rangle|^2}\leq \sqrt{\sum_{i,j}\|A_j\|^2}$$ where $U_i$ denotes the $i$ th row of $U$ and $A_j$ the $j$ th column of $A$ . However this estimate is to crude and will not equal $\|A\|_F$ . I would like to prove this without refering to trace or singular values and would appreciate a hint, rather than a full solution, on how to tackle this problem. EDIT: Completion of the proof based on the answer from $A.\Gamma$ : Since the rows of $U$ constitute an orthonormal basis for $\mathbb{C}^n$ we find by Parsevals theorem that $$\|UA_j\|_2^2 = \sum_{i=1}^{n}\left|\sum_{k=1}^{n}u_{ik}a_{kj}\right|^2 = \sum_{i=1}^{n}|\langle U_i,\overline{A_j}\rangle|^2 = \|\overline{A_j}\|_2^2 = \|A_j\|_2^2$$","I'm considering the norm defined on matrices by I want to show that it is unitarily invariant, so that for unitary we have that however I have trouble doing it directly. Writing directly I find by Cauchy-Schwarz that where denotes the th row of and the th column of . However this estimate is to crude and will not equal . I would like to prove this without refering to trace or singular values and would appreciate a hint, rather than a full solution, on how to tackle this problem. EDIT: Completion of the proof based on the answer from : Since the rows of constitute an orthonormal basis for we find by Parsevals theorem that","\|A\|_F = \sqrt{\sum_{i,j}|a_{ij}|^2} U \|UA\|_F = \|A\|_F = \|AU\|_F \|UA\|_F \|UA\|_F = \sqrt{\sum_{i,j}\left|\sum_{k=1}^{n}u_{ik}a_{kj}\right|^2}= \sqrt{\sum_{i,j}|\langle U_i,\overline{A_j}\rangle|^2}\leq \sqrt{\sum_{i,j}\|A_j\|^2} U_i i U A_j j A \|A\|_F A.\Gamma U \mathbb{C}^n \|UA_j\|_2^2 = \sum_{i=1}^{n}\left|\sum_{k=1}^{n}u_{ik}a_{kj}\right|^2 = \sum_{i=1}^{n}|\langle U_i,\overline{A_j}\rangle|^2 = \|\overline{A_j}\|_2^2 = \|A_j\|_2^2","['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
41,Transpose matrix and inner product [duplicate],Transpose matrix and inner product [duplicate],,"This question already has an answer here : Property of the conjugate transpose matrix with inner product (1 answer) Closed 5 years ago . In one of the proofs in class there was given the equality for the dot product: $$\langle Ax, Ax\rangle = \langle x, A^tAx\rangle$$ I don't understand why this is correct. Is there a way to show this without explicitly looking at the multiplications and sums? thanks.","This question already has an answer here : Property of the conjugate transpose matrix with inner product (1 answer) Closed 5 years ago . In one of the proofs in class there was given the equality for the dot product: $$\langle Ax, Ax\rangle = \langle x, A^tAx\rangle$$ I don't understand why this is correct. Is there a way to show this without explicitly looking at the multiplications and sums? thanks.",,"['linear-algebra', 'inner-products']"
42,Why is a positive definite matrix needed in the ellipsoid matrix representation?,Why is a positive definite matrix needed in the ellipsoid matrix representation?,,"An ellipsoid centered at the origin is defined by the solutions $\mathbf{x}$ to the equation $\mathbf{x}^TM\mathbf{x} = 1$, where M is a positive definite matrix. How can I see why M needs to be positive definite, based on the equation of an ellipse $Ax^2 + Bxy + Cy^2 = 1$ where $B-4AC < 0$? It looks like the idea is to make $B-4AC < 0$ equate to the requirement that all eigenvalues of $M$ are positive for a 2x2 matrix, but I can't seem to make it work. Also, what other shapes can we represent with $\mathbf{x}^TM\mathbf{x} = 1$ when $M$ is not positive definite?","An ellipsoid centered at the origin is defined by the solutions $\mathbf{x}$ to the equation $\mathbf{x}^TM\mathbf{x} = 1$, where M is a positive definite matrix. How can I see why M needs to be positive definite, based on the equation of an ellipse $Ax^2 + Bxy + Cy^2 = 1$ where $B-4AC < 0$? It looks like the idea is to make $B-4AC < 0$ equate to the requirement that all eigenvalues of $M$ are positive for a 2x2 matrix, but I can't seem to make it work. Also, what other shapes can we represent with $\mathbf{x}^TM\mathbf{x} = 1$ when $M$ is not positive definite?",,"['linear-algebra', 'matrices', 'analytic-geometry']"
43,Can a basis for a vector space be made up of matrices instead of vectors?,Can a basis for a vector space be made up of matrices instead of vectors?,,"I'm sorry if this is a silly question. I'm new to the notion of bases and all the examples I've dealt with before have involved sets of vectors containing real numbers. This has led me to assume that bases, by definition, are made up of a number of $n$-tuples. However, now I've been thinking about a basis for all $n\times n$ matrices and I keep coming back to the idea that the simplest basis would be $n^2$ matrices, each with a single $1$ in a unique position. Is this a valid basis? Or should I be trying to get column vectors on their own somehow?","I'm sorry if this is a silly question. I'm new to the notion of bases and all the examples I've dealt with before have involved sets of vectors containing real numbers. This has led me to assume that bases, by definition, are made up of a number of $n$-tuples. However, now I've been thinking about a basis for all $n\times n$ matrices and I keep coming back to the idea that the simplest basis would be $n^2$ matrices, each with a single $1$ in a unique position. Is this a valid basis? Or should I be trying to get column vectors on their own somehow?",,"['linear-algebra', 'matrices', 'vector-spaces']"
44,Determinant inequality $ \det(A^2+B^2+(A-B)^2)\ge 3\det(AB-BA) $,Determinant inequality, \det(A^2+B^2+(A-B)^2)\ge 3\det(AB-BA) ,"$A$ and $B$ are two $2\times2$ reals matrices. then $$ \det \Big(A^2+B^2+(A-B)^2\Big)\ge 3\det\left(AB-BA\right).$$ Well, it is seems interesting, but it is really hard to get started. Thank you very much!","and are two reals matrices. then Well, it is seems interesting, but it is really hard to get started. Thank you very much!",A B 2\times2  \det \Big(A^2+B^2+(A-B)^2\Big)\ge 3\det\left(AB-BA\right).,"['linear-algebra', 'inequality', 'determinant']"
45,Order of general linear group of $2 \times 2$ matrices over $\mathbb{Z}_3$,Order of general linear group of  matrices over,2 \times 2 \mathbb{Z}_3,"From problem 2.3.25 in Topics in Algebra , 2$\varepsilon$ by I. N. Herstein: Let $G$ be the group of all $2 \times 2$ matrices $\left(\begin{array}{c c}a & b \\ c & d\end{array}\right)$ where $ad-bc \ne 0$ and $a,b,c,d$ are integers modulo 3, relative to matrix multiplication. Show that $o(G) = 48$. I know that $o(G) \le 3^4 = 81$, since $a,b,c,d$ can each take one of 3 values (mod 3). I attempted to tighten this bound by finding the number of matrices such that $ad=bc$ (mod 3): Suppose $ad=bc=0$ (mod 3). Then ($a = 0$ or $d = 0$) and ($b = 0$ or $c = 0$), leading to 36 possible values for $(a,b,c,d)$. Suppose $ad=bc=1$ (mod 3). Then ($a=d=1$ or $a=d=2$) and ($b=c=1$ or $b=c=2$), leading to 4 possible values for $(a,b,c,d)$. Suppose $ad=bc=2$ (mod 3). Then ($(a,d)=(1,2)$ or $(a,d)=(2,1)$) and ($(b,c)=(1,2)$ or $(b,c)=(2,1)$), leading to 4 possible values for $(a,b,c,d)$. So, there are in total $36+4+4 = 44$ such $\left(\begin{array}{c c}a & b \\ c & d\end{array}\right)$ where $ad-bc=0$ (mod 3). That means there are at most $81-44 = 37$ such $\left(\begin{array}{c c}a & b \\ c & d\end{array}\right)$ where $ad-bc\ne 0$, i.e., $o(G) \le 37$. However, this contradicts the problem. Where did I go wrong? Can someone set me on the right path?","From problem 2.3.25 in Topics in Algebra , 2$\varepsilon$ by I. N. Herstein: Let $G$ be the group of all $2 \times 2$ matrices $\left(\begin{array}{c c}a & b \\ c & d\end{array}\right)$ where $ad-bc \ne 0$ and $a,b,c,d$ are integers modulo 3, relative to matrix multiplication. Show that $o(G) = 48$. I know that $o(G) \le 3^4 = 81$, since $a,b,c,d$ can each take one of 3 values (mod 3). I attempted to tighten this bound by finding the number of matrices such that $ad=bc$ (mod 3): Suppose $ad=bc=0$ (mod 3). Then ($a = 0$ or $d = 0$) and ($b = 0$ or $c = 0$), leading to 36 possible values for $(a,b,c,d)$. Suppose $ad=bc=1$ (mod 3). Then ($a=d=1$ or $a=d=2$) and ($b=c=1$ or $b=c=2$), leading to 4 possible values for $(a,b,c,d)$. Suppose $ad=bc=2$ (mod 3). Then ($(a,d)=(1,2)$ or $(a,d)=(2,1)$) and ($(b,c)=(1,2)$ or $(b,c)=(2,1)$), leading to 4 possible values for $(a,b,c,d)$. So, there are in total $36+4+4 = 44$ such $\left(\begin{array}{c c}a & b \\ c & d\end{array}\right)$ where $ad-bc=0$ (mod 3). That means there are at most $81-44 = 37$ such $\left(\begin{array}{c c}a & b \\ c & d\end{array}\right)$ where $ad-bc\ne 0$, i.e., $o(G) \le 37$. However, this contradicts the problem. Where did I go wrong? Can someone set me on the right path?",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
46,Every vector space has a basis proof using Zorn's lemma in linear algebra,Every vector space has a basis proof using Zorn's lemma in linear algebra,,Every vector space has a basis proof  by using Zorn's lemma. How will I prove this statement by using Zorn's lemma ?,Every vector space has a basis proof  by using Zorn's lemma. How will I prove this statement by using Zorn's lemma ?,,"['linear-algebra', 'vector-spaces', 'hamel-basis']"
47,"Dual spaces isomorphic, implies vector spaces itself are isomorphic?","Dual spaces isomorphic, implies vector spaces itself are isomorphic?",,"When I have two vector spaces $W, V$ over $k$ a field. And I know that the algebraic dual spaces of $V$ and $W$ are isomorphic. Can I conclude, (in the infinite dimensional case) that $V$ and $W$ are isomorphic? I am saying algebraic dual, because i don't want to be confused with the dual of continuous linear functionals. But I am just using the definition of dual space, that everyone uses in linear algebra. Say I know, that I have a linear map $\varphi: V \to W$ such that its dual map is an isomorphism, can I then conclude it was an isomorphism all along? I know this is true if $\varphi$ is assumed to be injective, because then the injectivity of the dual map, implies surjectivity of the original map. Can I say something, if my map $\varphi$ was assumed to be surjective and the dual map is an isomorphism?","When I have two vector spaces over a field. And I know that the algebraic dual spaces of and are isomorphic. Can I conclude, (in the infinite dimensional case) that and are isomorphic? I am saying algebraic dual, because i don't want to be confused with the dual of continuous linear functionals. But I am just using the definition of dual space, that everyone uses in linear algebra. Say I know, that I have a linear map such that its dual map is an isomorphism, can I then conclude it was an isomorphism all along? I know this is true if is assumed to be injective, because then the injectivity of the dual map, implies surjectivity of the original map. Can I say something, if my map was assumed to be surjective and the dual map is an isomorphism?","W, V k V W V W \varphi: V \to W \varphi \varphi",['linear-algebra']
48,Why Do We Only Take Norms Over Real/Complex Numbers?,Why Do We Only Take Norms Over Real/Complex Numbers?,,"By definition, norms are defined over some $\mathbb{R}$ or $\mathbb{C}$ vector space. Why do we only restrict ourselves to these fields when other fields give rise to interesting objects as well? (e.g. p-adic evaluation) Is it a historic reason or because other fields would give properties so different that we‘d rather not also associate the term “norm“ with it? If so, then I assume that $\mathbb{R}$ and $\mathbb{C}$ are similar enough to make those the two fields that give rise to norms?","By definition, norms are defined over some or vector space. Why do we only restrict ourselves to these fields when other fields give rise to interesting objects as well? (e.g. p-adic evaluation) Is it a historic reason or because other fields would give properties so different that we‘d rather not also associate the term “norm“ with it? If so, then I assume that and are similar enough to make those the two fields that give rise to norms?",\mathbb{R} \mathbb{C} \mathbb{R} \mathbb{C},"['linear-algebra', 'normed-spaces']"
49,Proving that the nullspace$(A) = $nullspace$(A^TA)$ where $A$ is a real m x n matrix.,Proving that the nullspacenullspace where  is a real m x n matrix.,(A) =  (A^TA) A,Note: This is a homework problem. I've been working on this one for hours and can't come up with anything. A hint was given to pay attention to the fact that $(AX)^TAX = 0$. Can anybody help me out? Thank you for your time and I'm sorry I don't have more work done as nothing I've tried has lead anywhere.,Note: This is a homework problem. I've been working on this one for hours and can't come up with anything. A hint was given to pay attention to the fact that $(AX)^TAX = 0$. Can anybody help me out? Thank you for your time and I'm sorry I don't have more work done as nothing I've tried has lead anywhere.,,"['linear-algebra', 'matrices']"
50,Showing that minimal polynomial has the same irreducible factors as characteristic polynomial [duplicate],Showing that minimal polynomial has the same irreducible factors as characteristic polynomial [duplicate],,"This question already has answers here : Minimal and characteristic polynomial have same set of irreducible factors (2 answers) Closed 4 years ago . I'm trying to show that the minimal polynomial of a linear transformation $T:V \to V$ over some field $k$ has the same irreducible factors as the characteristic polynomial of $T$. So if $m = {f_1}^{m_1} ... {f_n}^{m_n}$ then $\chi = {f_1}^{d_1} ... {f_n}^{d_n}$ with $f_i$ irreducible and $m_i \le d_i$. Now I've managed to prove this using the primary decomposition theorem and then restricting $T$ to $ker({f_i}^{m_i})$ and then using the fact that the minimal polynomial must divide the characteristic polynomial (Cayley-Hamilton) and then the irreducibility of $f_i$ gives us the result. However I would like to be able to prove this directly using facts about polynomials/fields without relying on the primary decomposition theorem for vector spaces. Is this fact about polynomials true in general? We know that $m$ divides $\chi$  and so certainly $\chi = {f_1}^{m_1} ... {f_n}^{m_n} \times g$ but then how do we show that $g$ must have only $f_i$ as it's factors? I'm guessing I need to use the fact that they share the same roots. And I'm also guessing that it depends on $k$, i.e. if $k$ is algebraically closed then it is easy because the polynomials split completely into linear factors. Help is much appreciated, Thanks","This question already has answers here : Minimal and characteristic polynomial have same set of irreducible factors (2 answers) Closed 4 years ago . I'm trying to show that the minimal polynomial of a linear transformation $T:V \to V$ over some field $k$ has the same irreducible factors as the characteristic polynomial of $T$. So if $m = {f_1}^{m_1} ... {f_n}^{m_n}$ then $\chi = {f_1}^{d_1} ... {f_n}^{d_n}$ with $f_i$ irreducible and $m_i \le d_i$. Now I've managed to prove this using the primary decomposition theorem and then restricting $T$ to $ker({f_i}^{m_i})$ and then using the fact that the minimal polynomial must divide the characteristic polynomial (Cayley-Hamilton) and then the irreducibility of $f_i$ gives us the result. However I would like to be able to prove this directly using facts about polynomials/fields without relying on the primary decomposition theorem for vector spaces. Is this fact about polynomials true in general? We know that $m$ divides $\chi$  and so certainly $\chi = {f_1}^{m_1} ... {f_n}^{m_n} \times g$ but then how do we show that $g$ must have only $f_i$ as it's factors? I'm guessing I need to use the fact that they share the same roots. And I'm also guessing that it depends on $k$, i.e. if $k$ is algebraically closed then it is easy because the polynomials split completely into linear factors. Help is much appreciated, Thanks",,"['linear-algebra', 'polynomials', 'ring-theory', 'field-theory', 'irreducible-polynomials']"
51,Simple examples of $3 \times 3$ rotation matrices,Simple examples of  rotation matrices,3 \times 3,"I'd like to have some numerically simple examples of $3 \times 3$ rotation matrices that are easy to handle in hand calculations (using only your brain and a pencil). Matrices that contain too many zeros and ones are boring, and ones with square roots are undesirable. A good example is something like $$ M = \frac19 \begin{bmatrix} 1 & -4 & 8 \\ 8 &  4 & 1  \\ -4 & 7 & 4 \end{bmatrix} $$ Does anyone have any other examples, or a process for generating them? One general formula for a rotation matrix is given here . So one possible approach would be to choose $u_x$, $u_y$, $u_z$ and $\theta$ so that you get something simple. Simple enough for hand calculations, but not trivial. Like the example given above.","I'd like to have some numerically simple examples of $3 \times 3$ rotation matrices that are easy to handle in hand calculations (using only your brain and a pencil). Matrices that contain too many zeros and ones are boring, and ones with square roots are undesirable. A good example is something like $$ M = \frac19 \begin{bmatrix} 1 & -4 & 8 \\ 8 &  4 & 1  \\ -4 & 7 & 4 \end{bmatrix} $$ Does anyone have any other examples, or a process for generating them? One general formula for a rotation matrix is given here . So one possible approach would be to choose $u_x$, $u_y$, $u_z$ and $\theta$ so that you get something simple. Simple enough for hand calculations, but not trivial. Like the example given above.",,"['linear-algebra', 'matrices', 'rotations']"
52,Eigenvalues and Eigenvectors of $2 \times 2$ Matrix,Eigenvalues and Eigenvectors of  Matrix,2 \times 2,"Let's say I have a $2 \times 2$ matrix (actually the structure tensor of a discrete image - I): $$ \begin{bmatrix}  \frac{\partial I}{\partial x}\frac{\partial I}{\partial x} & \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} \\\      \frac{\partial I}{\partial y}\frac{\partial I}{\partial x} & \frac{\partial I}{\partial y}\frac{\partial I}{\partial y} \end{bmatrix}$$ It has 2 properties: Symmetric. Positive Semidefinite. Given those properties, what would be the easiest method to numerically compute its eigenvectors (orthogonal) and eigenvalues?","Let's say I have a $2 \times 2$ matrix (actually the structure tensor of a discrete image - I): $$ \begin{bmatrix}  \frac{\partial I}{\partial x}\frac{\partial I}{\partial x} & \frac{\partial I}{\partial x}\frac{\partial I}{\partial y} \\\      \frac{\partial I}{\partial y}\frac{\partial I}{\partial x} & \frac{\partial I}{\partial y}\frac{\partial I}{\partial y} \end{bmatrix}$$ It has 2 properties: Symmetric. Positive Semidefinite. Given those properties, what would be the easiest method to numerically compute its eigenvectors (orthogonal) and eigenvalues?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
53,"When we raise a matrix, whose rows or columns sum to 1, to the power of 2, 3, ..., n, why does the resultant matrix tend to stabilize after some n?","When we raise a matrix, whose rows or columns sum to 1, to the power of 2, 3, ..., n, why does the resultant matrix tend to stabilize after some n?",,"I noticed this interesting pattern while doing a matrix multiplication homework given in school. Also, if the rows sum to 1, after raising it to a sufficiently large n, both its rows will be identical. Same with the columns if the column values sum to 1 in the matrix. Why does this happen? For example, if $B=\begin{pmatrix}0.6&  0.4\\ 0.15&  0.85\end{pmatrix}$ is our matrix, then: $$B=\begin{pmatrix}0.6000&  0.4000\\ 0.1500&  0.8500\end{pmatrix}$$ $$B^2=\begin{pmatrix}0.4200&  0.5800\\ 0.2175&  0.7825\end{pmatrix}$$ $$B^3=\begin{pmatrix}0.3390&  0.6610\\ 0.2479&  0.7521\end{pmatrix}$$ $$B^{10}=\begin{pmatrix}0.2730&  0.7270\\ 0.2726&  0.7274\end{pmatrix}$$ $$B^{20}=\begin{pmatrix}0.2727&  0.7273\\ 0.2727&  0.7273\end{pmatrix}$$ $$B^{100}=\begin{pmatrix}0.2727&  0.7273\\ 0.2727&  0.7273\end{pmatrix}$$ $$B^{1000}=\begin{pmatrix}0.2727&  0.7273\\ 0.2727&  0.7273\end{pmatrix}$$","I noticed this interesting pattern while doing a matrix multiplication homework given in school. Also, if the rows sum to 1, after raising it to a sufficiently large n, both its rows will be identical. Same with the columns if the column values sum to 1 in the matrix. Why does this happen? For example, if $B=\begin{pmatrix}0.6&  0.4\\ 0.15&  0.85\end{pmatrix}$ is our matrix, then: $$B=\begin{pmatrix}0.6000&  0.4000\\ 0.1500&  0.8500\end{pmatrix}$$ $$B^2=\begin{pmatrix}0.4200&  0.5800\\ 0.2175&  0.7825\end{pmatrix}$$ $$B^3=\begin{pmatrix}0.3390&  0.6610\\ 0.2479&  0.7521\end{pmatrix}$$ $$B^{10}=\begin{pmatrix}0.2730&  0.7270\\ 0.2726&  0.7274\end{pmatrix}$$ $$B^{20}=\begin{pmatrix}0.2727&  0.7273\\ 0.2727&  0.7273\end{pmatrix}$$ $$B^{100}=\begin{pmatrix}0.2727&  0.7273\\ 0.2727&  0.7273\end{pmatrix}$$ $$B^{1000}=\begin{pmatrix}0.2727&  0.7273\\ 0.2727&  0.7273\end{pmatrix}$$",,"['linear-algebra', 'matrices']"
54,How to know if vector is in column space of a matrix?,How to know if vector is in column space of a matrix?,,"I have an exercise where I have to say if a vector $\vec{u} = \left(\begin{matrix}1 \\ 2\end{matrix}\right)$ is or not in a column space of a matrix $A = \left(\begin{matrix}1 & -2\\ -2 & 4\end{matrix}\right)$. I am quite newbie, and I am still not so comfortable with these concepts. What I did was to create an augment matrix: $$\left(\begin{array}{cc|c}1 & -2 & 1 \\ -2 & 4 & 2\end{array}\right)$$ If I try to reduce this I get the second row like $\left(\begin{array}{cc|c}0 & 0 & 4 \end{array}\right)$. Which I think means that the $\vec{v}$ is not in $A$, because $0 +/- 0 \neq 4$. Could you provide a more technical explanation of why? I know this might seem to easy, but just to understand better...","I have an exercise where I have to say if a vector $\vec{u} = \left(\begin{matrix}1 \\ 2\end{matrix}\right)$ is or not in a column space of a matrix $A = \left(\begin{matrix}1 & -2\\ -2 & 4\end{matrix}\right)$. I am quite newbie, and I am still not so comfortable with these concepts. What I did was to create an augment matrix: $$\left(\begin{array}{cc|c}1 & -2 & 1 \\ -2 & 4 & 2\end{array}\right)$$ If I try to reduce this I get the second row like $\left(\begin{array}{cc|c}0 & 0 & 4 \end{array}\right)$. Which I think means that the $\vec{v}$ is not in $A$, because $0 +/- 0 \neq 4$. Could you provide a more technical explanation of why? I know this might seem to easy, but just to understand better...",,['linear-algebra']
55,How to expand equation inside the L2-norm?,How to expand equation inside the L2-norm?,,"I want expand an L2-norm with some matrix operation inside. Assume I have a regression $Y=X\beta+\epsilon$. I want to solve (meaning expand), $$\displaystyle\|Y-X\beta \|_{2}^2$$ Should I do: 1) $$\displaystyle\|Y\|_{2}^2+2\beta^TX^TX\beta+\|X\|_{2}^2$$ or 2) $$\sum_{i=1}^{k}(y_i-x_i\beta_i)^2$$ $$\sum_{i=1}^{k}(y_i^2-2x_i\beta_i+x_i^2)$$ X: n by k matrix $\beta$: k by 1 vector $Y$: n by 1 vector I think I have seen both: keeping the norm notation as I did in 1), and expand in summation for as I did in 2). Could someone show me how to do it?","I want expand an L2-norm with some matrix operation inside. Assume I have a regression $Y=X\beta+\epsilon$. I want to solve (meaning expand), $$\displaystyle\|Y-X\beta \|_{2}^2$$ Should I do: 1) $$\displaystyle\|Y\|_{2}^2+2\beta^TX^TX\beta+\|X\|_{2}^2$$ or 2) $$\sum_{i=1}^{k}(y_i-x_i\beta_i)^2$$ $$\sum_{i=1}^{k}(y_i^2-2x_i\beta_i+x_i^2)$$ X: n by k matrix $\beta$: k by 1 vector $Y$: n by 1 vector I think I have seen both: keeping the norm notation as I did in 1), and expand in summation for as I did in 2). Could someone show me how to do it?",,"['linear-algebra', 'normed-spaces']"
56,"Finding the order of an element in $GL(2,\mathbb{R})$",Finding the order of an element in,"GL(2,\mathbb{R})","I am working on a problem involving basic abstract algebra/group theory and am getting confused. I am following an online course by Dr. Bob found here, and am currently on assignment two . My difficulty lies with problem 1b where I am given a matrix $A=$ $\left( \begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ \end{array} \right)$ and asked to find its order. Now I am fairly sure that matrix multiplication is not commutative so this makes me suspect that there are either multiple answers or a convention we must adopt (which I dont think he mentioned). If I multiply on the right I get $A\cdot A = -I, A^3 = A\cdot A^2 = \left( \begin{array}{ccc} 0 & 1 \\ -1 & 0 \\ \end{array} \right)$, and $A^4 = A\cdot A^3 = I$ so $|A| =4$. Now when I do this on the by multiplying on the left by $A$ I get the same answer, but my intuition says this is a coincidence because of the trivial chosen matrix. Is it true in general that the order of elements in $GL(2,\mathbb{R})$ is the same regardless of which side you multiply on, or are there criterion when this property holds? Finally, since I'm guessing that this is just a special case situation, which side do I multiply on when asked to find the order of an element? Thanks for the help!","I am working on a problem involving basic abstract algebra/group theory and am getting confused. I am following an online course by Dr. Bob found here, and am currently on assignment two . My difficulty lies with problem 1b where I am given a matrix $A=$ $\left( \begin{array}{ccc} 0 & -1 \\ 1 & 0 \\ \end{array} \right)$ and asked to find its order. Now I am fairly sure that matrix multiplication is not commutative so this makes me suspect that there are either multiple answers or a convention we must adopt (which I dont think he mentioned). If I multiply on the right I get $A\cdot A = -I, A^3 = A\cdot A^2 = \left( \begin{array}{ccc} 0 & 1 \\ -1 & 0 \\ \end{array} \right)$, and $A^4 = A\cdot A^3 = I$ so $|A| =4$. Now when I do this on the by multiplying on the left by $A$ I get the same answer, but my intuition says this is a coincidence because of the trivial chosen matrix. Is it true in general that the order of elements in $GL(2,\mathbb{R})$ is the same regardless of which side you multiply on, or are there criterion when this property holds? Finally, since I'm guessing that this is just a special case situation, which side do I multiply on when asked to find the order of an element? Thanks for the help!",,"['linear-algebra', 'abstract-algebra', 'group-theory']"
57,"If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent [duplicate]","If  and  commute, show that  is nilpotent [duplicate]",A AB-BA AB-BA,"This question already has an answer here : Closed 11 years ago . Possible Duplicate: Linear Algebra,regarding commutator Let $A$ and $B$ be two $n\times n$-matrices over some field of characteristic $0$. If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent. My brother asked me for help on this but I quite embarrassingly can't figure it out. I vaguely recall solving a similar problem in school, but with the added condition that $B$ also commutes with $AB-BA$. Any help would be appreciated.","This question already has an answer here : Closed 11 years ago . Possible Duplicate: Linear Algebra,regarding commutator Let $A$ and $B$ be two $n\times n$-matrices over some field of characteristic $0$. If $A$ and $AB-BA$ commute, show that $AB-BA$ is nilpotent. My brother asked me for help on this but I quite embarrassingly can't figure it out. I vaguely recall solving a similar problem in school, but with the added condition that $B$ also commutes with $AB-BA$. Any help would be appreciated.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
58,Cross product in complex vector spaces,Cross product in complex vector spaces,,"When inner product is defined in complex vector space, conjugation is performed on one of the vectors. What about is the cross product of two complex 3D vectors? I suppose that one possible generalization is  $A\otimes B  \rightarrow  \left ( A\times B \right )^*$  where $\times$ denotes the normal cross product. The conjugation here is to ensure that the result of the cross product is orthogonal to both vectors $A$ and $B$. Is that correct ?","When inner product is defined in complex vector space, conjugation is performed on one of the vectors. What about is the cross product of two complex 3D vectors? I suppose that one possible generalization is  $A\otimes B  \rightarrow  \left ( A\times B \right )^*$  where $\times$ denotes the normal cross product. The conjugation here is to ensure that the result of the cross product is orthogonal to both vectors $A$ and $B$. Is that correct ?",,"['linear-algebra', 'complex-numbers', 'complex-geometry', 'cross-product']"
59,Finding number of matrices whose square is the identity matrix,Finding number of matrices whose square is the identity matrix,,"how can we find the number of matrices with real entries of size $9 \times 9$ (up to similarity) such that $A^{2}=I$? I first thought about the following: Notice $A$ satisfies the polynomial $f(t)=t^{2}-1$ hence its minimal polynomial divides $(t-1)(t+1)$. So its characteristic polynomial is of the form $p(t)=(t-1)^r(t+1)^j$ where $r+j = 9$, right? Then I'm not sure what to do, I tried to consider the rational canonical form but in order to do this we need to know the minimal polynomial right? because in the rational canonical form the last term in the array is exactly the minimal polynomial, how to find it? Can you please help?","how can we find the number of matrices with real entries of size $9 \times 9$ (up to similarity) such that $A^{2}=I$? I first thought about the following: Notice $A$ satisfies the polynomial $f(t)=t^{2}-1$ hence its minimal polynomial divides $(t-1)(t+1)$. So its characteristic polynomial is of the form $p(t)=(t-1)^r(t+1)^j$ where $r+j = 9$, right? Then I'm not sure what to do, I tried to consider the rational canonical form but in order to do this we need to know the minimal polynomial right? because in the rational canonical form the last term in the array is exactly the minimal polynomial, how to find it? Can you please help?",,"['linear-algebra', 'matrices', 'examples-counterexamples']"
60,Characteristic polynomial of the matrix with zeros on the diagonal and ones elsewhere,Characteristic polynomial of the matrix with zeros on the diagonal and ones elsewhere,,"Find the characteristic polynomial of the matrix with zeros on the diagonal and ones elsewhere. I've been able (I believe) to guess how it looks like (by considering matrices of small orders): $(x-n+1)(x-1)^{n-1}$. I suppose I should prove it by induction. But I don't know how to obtain the characteristic polynomial of a matrix of order $n+1$ from that of a matrix of order $n$ (i.e., how to make the inductive step). Other methods of solution are also welcome. (Is it possible to use row reduction?)","Find the characteristic polynomial of the matrix with zeros on the diagonal and ones elsewhere. I've been able (I believe) to guess how it looks like (by considering matrices of small orders): $(x-n+1)(x-1)^{n-1}$. I suppose I should prove it by induction. But I don't know how to obtain the characteristic polynomial of a matrix of order $n+1$ from that of a matrix of order $n$ (i.e., how to make the inductive step). Other methods of solution are also welcome. (Is it possible to use row reduction?)",,"['linear-algebra', 'matrices']"
61,To prove: $n!=\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n$ [duplicate],To prove:  [duplicate],n!=\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n,"This question already has answers here : Proof of the summation $n!=\sum_{k=0}^n \binom{n}{k}(n-k+1)^n(-1)^k$? (4 answers) Closed 5 years ago . How to algebraically prove that $$n!=\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n$$ I was trying to find number of onto functions from $A$ to $A$ containing $n$ elements. Using the inclusion-exclusion principle I am getting  $$\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n.$$ We can also do it by simple combinatorics, as every element has to have a pre-image and number of elements in the domain equal to the number of elements in the codomain, the number of functions is $n!.$ Is there an algebraic way to prove these two are equal?","This question already has answers here : Proof of the summation $n!=\sum_{k=0}^n \binom{n}{k}(n-k+1)^n(-1)^k$? (4 answers) Closed 5 years ago . How to algebraically prove that $$n!=\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n$$ I was trying to find number of onto functions from $A$ to $A$ containing $n$ elements. Using the inclusion-exclusion principle I am getting  $$\sum\limits_{r=0}^n (-1)^r \binom{n}{r} (n-r)^n.$$ We can also do it by simple combinatorics, as every element has to have a pre-image and number of elements in the domain equal to the number of elements in the codomain, the number of functions is $n!.$ Is there an algebraic way to prove these two are equal?",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'permutations']"
62,For which sets of parameters will this system of equations have no solution?,For which sets of parameters will this system of equations have no solution?,,"If $S$ is the set of distinct values of '$b$' for which the following system of linear equations \begin{align}   x + y + z = 1\\  x + ay + z = 1\\   ax + by + z = 0  \end{align}   has no solution, then $S$ is: a singleton an empty set an infinite set a finite set containing two or more elements There's no information given about value of $a$. You will be getting two cases: For $a=1, b$ has one value $b=1$ For $a$ not equal to $1 , b$ has no value.","If $S$ is the set of distinct values of '$b$' for which the following system of linear equations \begin{align}   x + y + z = 1\\  x + ay + z = 1\\   ax + by + z = 0  \end{align}   has no solution, then $S$ is: a singleton an empty set an infinite set a finite set containing two or more elements There's no information given about value of $a$. You will be getting two cases: For $a=1, b$ has one value $b=1$ For $a$ not equal to $1 , b$ has no value.",,"['linear-algebra', 'systems-of-equations']"
63,Diagonalization of restrictions of a diagonalizable linear operator,Diagonalization of restrictions of a diagonalizable linear operator,,"I realized that I have some difficulties for prove this exercise. Let $T : V \rightarrow V$ be a linear operator on a finite dimensional vector space $V$ over a field $F$, and invariant subspaces $U,W \subset V$  such that $V = U \oplus W$. Show that if $T$ is diagonalizable then $T_{|U}, T_{|W}$ are  diagonalizable. Any help would be greatly appreciated. Thanks!","I realized that I have some difficulties for prove this exercise. Let $T : V \rightarrow V$ be a linear operator on a finite dimensional vector space $V$ over a field $F$, and invariant subspaces $U,W \subset V$  such that $V = U \oplus W$. Show that if $T$ is diagonalizable then $T_{|U}, T_{|W}$ are  diagonalizable. Any help would be greatly appreciated. Thanks!",,['linear-algebra']
64,Where can I calculate the exponential of a matrix online?,Where can I calculate the exponential of a matrix online?,,Where can I exponentiate a $3\times 3$ matrix like $\left[\begin{array}{ccc} 0 & 1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 0 \end{array}\right]$   online? Is there some website where this is possible? Thank you very much.,Where can I exponentiate a $3\times 3$ matrix like $\left[\begin{array}{ccc} 0 & 1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 0 \end{array}\right]$   online? Is there some website where this is possible? Thank you very much.,,"['linear-algebra', 'matrices', 'online-resources']"
65,Are all multiplicative functions additive?,Are all multiplicative functions additive?,,"Suppose $cf(x)=f(cx)$ and $f:\mathbb{R}\to\mathbb{R}$. I believe it follows that $f(x+y)=f(x)+f(y)$. Proof: There is some $c$ such that $y=cx$. Then  $$f(x+y)=f\left((1+c)x\right)=(1+c)f(x)=f(x)+cf(x)=f(x)+f(cx)=f(x)+f(y)$$ QED. I wonder if the same thing holds for when $f:\mathbb{R}^n\to\mathbb{R}^n$? I can't use the same trick, because all vectors are not scalar multiples. I tried thinking of it in terms of basis units, but didn't get anywhere.","Suppose $cf(x)=f(cx)$ and $f:\mathbb{R}\to\mathbb{R}$. I believe it follows that $f(x+y)=f(x)+f(y)$. Proof: There is some $c$ such that $y=cx$. Then  $$f(x+y)=f\left((1+c)x\right)=(1+c)f(x)=f(x)+cf(x)=f(x)+f(cx)=f(x)+f(y)$$ QED. I wonder if the same thing holds for when $f:\mathbb{R}^n\to\mathbb{R}^n$? I can't use the same trick, because all vectors are not scalar multiples. I tried thinking of it in terms of basis units, but didn't get anywhere.",,"['linear-algebra', 'functions', 'functional-equations']"
66,Maximizing the sum $\sum\limits_{i=1}^nx_ix_{i+1}$ subject to $\sum\limits_{i=1}^nx_i=0$ and $\sum\limits_{i=1}^nx_i^2=1$,Maximizing the sum  subject to  and,\sum\limits_{i=1}^nx_ix_{i+1} \sum\limits_{i=1}^nx_i=0 \sum\limits_{i=1}^nx_i^2=1,"Is there an efficient way of solving the following problem? Given $x_i\in \mathbb R$, and that $\sum\limits_{i=1}^nx_i=0$ and $\sum\limits_{i=1}^nx_i^2=1$. I want to maximize $\sum\limits_{i=1}^nx_ix_{i+1}$ where we take $x_{n+1}=x_1$. I don't know if this is relevant/useful at all but maybe representing the systems as $\vec{x}^TI\,\,\vec{x}$ and $\vec{x}^T\{\delta_{i , \,\,i+1}\}\,\,\vec{x}$ might help? Thanks.","Is there an efficient way of solving the following problem? Given $x_i\in \mathbb R$, and that $\sum\limits_{i=1}^nx_i=0$ and $\sum\limits_{i=1}^nx_i^2=1$. I want to maximize $\sum\limits_{i=1}^nx_ix_{i+1}$ where we take $x_{n+1}=x_1$. I don't know if this is relevant/useful at all but maybe representing the systems as $\vec{x}^TI\,\,\vec{x}$ and $\vec{x}^T\{\delta_{i , \,\,i+1}\}\,\,\vec{x}$ might help? Thanks.",,"['linear-algebra', 'matrices', 'optimization']"
67,Null space for $AA^{T}$ is the same as Null space for $A^{T}$,Null space for  is the same as Null space for,AA^{T} A^{T},"$A$ is an $n\times m$ matrix and $AA^{T}$ is a symmetric real matrix.  Also, we have: $\operatorname{rank}(AA^{T})=r\stackrel{?}{=}\operatorname{rank}(A)$.  Let $Q= \begin{Bmatrix} q_1,...,q_{n-r} \end{Bmatrix}$ be a basis for the Null space of $AA^{T}$. i.e. $AA^{T}q_i=0$, show that $A^{T}q_i=0$. I guess one proof can be that the Null space for $A^{T}$ is a subspace for Null space for $AA^{T}$, then the question would be why $\operatorname{rank}(A)=\operatorname{rank}(AA^{T})$?","$A$ is an $n\times m$ matrix and $AA^{T}$ is a symmetric real matrix.  Also, we have: $\operatorname{rank}(AA^{T})=r\stackrel{?}{=}\operatorname{rank}(A)$.  Let $Q= \begin{Bmatrix} q_1,...,q_{n-r} \end{Bmatrix}$ be a basis for the Null space of $AA^{T}$. i.e. $AA^{T}q_i=0$, show that $A^{T}q_i=0$. I guess one proof can be that the Null space for $A^{T}$ is a subspace for Null space for $AA^{T}$, then the question would be why $\operatorname{rank}(A)=\operatorname{rank}(AA^{T})$?",,['linear-algebra']
68,Determinant of Abstract Matrix,Determinant of Abstract Matrix,,"Given an $n \times n$ matrix $A$, where $x$ is any real number: $A = \left[   \begin{array}{ c c c c c c c c }     1 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 \\      1 & x & x & x & x & x & \cdots & x \\      1 & x & 2x & 2x & 2x & 2x & \cdots & 2x \\      1 & x & 2x & 3x & 3x & 3x & \cdots & 3x \\      1 & x & 2x & 3x & 4x & 4x & \cdots & 4x \\     1 & x & 2x & 3x & 4x & 5x & \cdots & 5x \\     \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\     1 & x & 2x & 3x & 4x & 5x & \cdots & (n-1)x    \end{array} \right]$ Find the determinant. By using $n=2,3,4,5,...$ and random $x=1,2,3,4...$ I have found that $det(A) = (x-1)(x)^{n-2}$ through observing a pattern. However, I would like to be able to prove this through a proof, yet I have no idea where to start. When I try to solve for the determinant using the abstract matrix A and using the property that the determinant of a square matrix is $(-1)^r * (\text{products of pivots})$, where r is the number of row interchanges, my answer is of the form $(x-1)(x)(x)(x)(x)...(n-?)x$ where ""?"" depends on how many rows I include in the abstract form of A. How do I show that $(x)(x)(x)...(n-?)x$ equals $(x)^{n-2}$? Here is my work: https://i.sstatic.net/2DQFJ.jpg Any hints? Thanks for the help!","Given an $n \times n$ matrix $A$, where $x$ is any real number: $A = \left[   \begin{array}{ c c c c c c c c }     1 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 \\      1 & x & x & x & x & x & \cdots & x \\      1 & x & 2x & 2x & 2x & 2x & \cdots & 2x \\      1 & x & 2x & 3x & 3x & 3x & \cdots & 3x \\      1 & x & 2x & 3x & 4x & 4x & \cdots & 4x \\     1 & x & 2x & 3x & 4x & 5x & \cdots & 5x \\     \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\     1 & x & 2x & 3x & 4x & 5x & \cdots & (n-1)x    \end{array} \right]$ Find the determinant. By using $n=2,3,4,5,...$ and random $x=1,2,3,4...$ I have found that $det(A) = (x-1)(x)^{n-2}$ through observing a pattern. However, I would like to be able to prove this through a proof, yet I have no idea where to start. When I try to solve for the determinant using the abstract matrix A and using the property that the determinant of a square matrix is $(-1)^r * (\text{products of pivots})$, where r is the number of row interchanges, my answer is of the form $(x-1)(x)(x)(x)(x)...(n-?)x$ where ""?"" depends on how many rows I include in the abstract form of A. How do I show that $(x)(x)(x)...(n-?)x$ equals $(x)^{n-2}$? Here is my work: https://i.sstatic.net/2DQFJ.jpg Any hints? Thanks for the help!",,"['linear-algebra', 'polynomials', 'determinant']"
69,Pseudoinverse matrix and SVD,Pseudoinverse matrix and SVD,,"I'm trying to solve an homework question but I got stuck. Let A be a m x n matrix with the SVD $A = U \Sigma V^*$ and $A^+ = (A^* A)^{-1} A^*$ its pseudoinverse. I'm trying to get $A^+ = V \Sigma^{-1} U^*$, but I'm missing something. Can anyone help me with this please? Thanks!","I'm trying to solve an homework question but I got stuck. Let A be a m x n matrix with the SVD $A = U \Sigma V^*$ and $A^+ = (A^* A)^{-1} A^*$ its pseudoinverse. I'm trying to get $A^+ = V \Sigma^{-1} U^*$, but I'm missing something. Can anyone help me with this please? Thanks!",,"['linear-algebra', 'matrices', 'pseudoinverse']"
70,Prove $\rm AB = BA = 0$ if the set of nonzero eigenvalues of $\rm A + B$ is union of set of nonzero eigenvalues of $\rm A$ and $\rm B$.,Prove  if the set of nonzero eigenvalues of  is union of set of nonzero eigenvalues of  and .,\rm AB = BA = 0 \rm A + B \rm A \rm B,"$A$ and $B$ are $n×n$ symmetric matrices, their non-zero eigenvalues are $(\lambda_{1},\ldots,\lambda_{r} )$，$( \mu_{1},\ldots,\mu_{s} )$.  If the nonzero eigenvalues of $A + B$ are  $(\lambda_{1},\ldots,\lambda_{r}，\mu_{1},\ldots,\mu_{s} )$,  show that $AB = BA = 0$ $\bf{Note}$ that the (non-zero) eigenvalues of a matrix form a multiset, so the above is a union of multisets.","$A$ and $B$ are $n×n$ symmetric matrices, their non-zero eigenvalues are $(\lambda_{1},\ldots,\lambda_{r} )$，$( \mu_{1},\ldots,\mu_{s} )$.  If the nonzero eigenvalues of $A + B$ are  $(\lambda_{1},\ldots,\lambda_{r}，\mu_{1},\ldots,\mu_{s} )$,  show that $AB = BA = 0$ $\bf{Note}$ that the (non-zero) eigenvalues of a matrix form a multiset, so the above is a union of multisets.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
71,Evaluate the determinant of a Hessenberg matrix,Evaluate the determinant of a Hessenberg matrix,,"The following question is taken from here exercise $1$: Question Evaluate the determinant:    \begin{vmatrix} a_0 & a_1 & a_2 & \dots & a_n \\ -y_1 & x_1 & 0 & \dots & 0 \\ 0 & -y_2 & x_2 & \dots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & x_n \end{vmatrix} Since this is a competition question, I suppose the normal determinant formula will not work. Indeed, tedious and messy calculations are inevitable if one just expand the determinant. I observe that the matrix has $-y_i$ at super diagonal for all $1\leq i \leq n.$ However, I do not think this helps. Any hint would be appreciated.","The following question is taken from here exercise $1$: Question Evaluate the determinant:    \begin{vmatrix} a_0 & a_1 & a_2 & \dots & a_n \\ -y_1 & x_1 & 0 & \dots & 0 \\ 0 & -y_2 & x_2 & \dots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & x_n \end{vmatrix} Since this is a competition question, I suppose the normal determinant formula will not work. Indeed, tedious and messy calculations are inevitable if one just expand the determinant. I observe that the matrix has $-y_i$ at super diagonal for all $1\leq i \leq n.$ However, I do not think this helps. Any hint would be appreciated.",,"['linear-algebra', 'matrices', 'contest-math', 'determinant']"
72,Why is this determinant positive?,Why is this determinant positive?,,"I have seen that the $k$-dimensional volume of an parallelepiped in $\mathbb{R}^n$, i.e., $$P(v_1, \ldots, v_k) = \{t_1v_1 + \dotsb + t_kv_k : 0 \le t_i \le 1 \}$$ is $\sqrt{\det(T^{\top}T)}$, where $T$ is the $n\times k$ matrix with columns $v_1, \ldots, v_k$. How do we know that $\det(T^{\top}T)$ is non-negative?","I have seen that the $k$-dimensional volume of an parallelepiped in $\mathbb{R}^n$, i.e., $$P(v_1, \ldots, v_k) = \{t_1v_1 + \dotsb + t_kv_k : 0 \le t_i \le 1 \}$$ is $\sqrt{\det(T^{\top}T)}$, where $T$ is the $n\times k$ matrix with columns $v_1, \ldots, v_k$. How do we know that $\det(T^{\top}T)$ is non-negative?",,"['linear-algebra', 'matrices', 'determinant', 'positive-semidefinite']"
73,Kernels and reduced row echelon form - explanation,Kernels and reduced row echelon form - explanation,,"The following text is written in my textbook and I don't really understand it: If $A = (a_{ij}) \in$ Mat $(m x N, F)$ is a matrix in reduced row echelon form with $r$ nonzero rows and pivots in the columns numbered $j_1 < ... < j_r$ , then the kernel ker $(A)$ is generated by the $n-r$ elements $ w_k = e_k - \sum\limits_{1 \le i \le r, j_i \le k} a_{i_k}e_{j_i}$ for $k \in \{ 1, \cdots , n\} \setminus \{j_1, \cdots, j_r\}$ , where $e_1, \cdots, e_n$ are the standard generators of $F^n$ . There are no computational examples and the notation is a bit overwhelming so I don't know how I can use it. Can somebody give an example of this in practice?","The following text is written in my textbook and I don't really understand it: If Mat is a matrix in reduced row echelon form with nonzero rows and pivots in the columns numbered , then the kernel ker is generated by the elements for , where are the standard generators of . There are no computational examples and the notation is a bit overwhelming so I don't know how I can use it. Can somebody give an example of this in practice?","A = (a_{ij}) \in (m x N, F) r j_1 < ... < j_r (A) n-r  w_k = e_k - \sum\limits_{1 \le i \le r, j_i \le k} a_{i_k}e_{j_i} k \in \{ 1, \cdots , n\} \setminus \{j_1, \cdots, j_r\} e_1, \cdots, e_n F^n","['linear-algebra', 'matrices']"
74,tangent space of manifold and Kernel,tangent space of manifold and Kernel,,"I want to understand the proof of this Theorem : Theorem :  Let $ S $ be a submanifold of $ E $ and let $ a \in S $. Assume that there exists a submersion $f : E \supset U  \rightarrow F$  of class $ \textit{C}\ ^{1} $ and $ b\in f(U) $ such that $ S=f^{-1}(b) $ then $ T_{a}S=Ker f'(a) $. Proof :  Since $ f(S)\subset \{b\}$, $f'(a).T_{a}S\subset \{0\}$. Hence $T_{a}S\subset Ker f'(a)$. Since both spaces have the same dimension, the result follows. My problem is that I am not able to understand all the steps in the proof. Can some one help me?","I want to understand the proof of this Theorem : Theorem :  Let $ S $ be a submanifold of $ E $ and let $ a \in S $. Assume that there exists a submersion $f : E \supset U  \rightarrow F$  of class $ \textit{C}\ ^{1} $ and $ b\in f(U) $ such that $ S=f^{-1}(b) $ then $ T_{a}S=Ker f'(a) $. Proof :  Since $ f(S)\subset \{b\}$, $f'(a).T_{a}S\subset \{0\}$. Hence $T_{a}S\subset Ker f'(a)$. Since both spaces have the same dimension, the result follows. My problem is that I am not able to understand all the steps in the proof. Can some one help me?",,"['linear-algebra', 'analysis', 'differential-geometry']"
75,Relationship between Nilpotent Matrix and Matrix with all zero diagonal factors.,Relationship between Nilpotent Matrix and Matrix with all zero diagonal factors.,,"solving Linear Algebra HW, I suddenly became curious about the relationship between Nilpotent Matrix and matrix with all zero diagonal factors such that $A_{11} = A_{22} = \cdots = A_{nn} = 0$ Does Nilpotent Matrix implies the matrix with all zero diagonal factors? What about converse? I guess it might be iff relationship. Could you develop more? Thank you very much.","solving Linear Algebra HW, I suddenly became curious about the relationship between Nilpotent Matrix and matrix with all zero diagonal factors such that $A_{11} = A_{22} = \cdots = A_{nn} = 0$ Does Nilpotent Matrix implies the matrix with all zero diagonal factors? What about converse? I guess it might be iff relationship. Could you develop more? Thank you very much.",,"['linear-algebra', 'matrices']"
76,Invertibility of a Kronecker Product,Invertibility of a Kronecker Product,,Prove that $A\otimes B$ is invertible if and only if $B\otimes A$ is invertible. I don't have a clue where to start to be honest. I am not very familiar yet to the Kronecker Product so could you please help me with providing an easily understandable proof. Thanks in advance.,Prove that $A\otimes B$ is invertible if and only if $B\otimes A$ is invertible. I don't have a clue where to start to be honest. I am not very familiar yet to the Kronecker Product so could you please help me with providing an easily understandable proof. Thanks in advance.,,"['linear-algebra', 'matrices', 'kronecker-product']"
77,Reason for reversing the order when transpose and inverse of a group of matrices,Reason for reversing the order when transpose and inverse of a group of matrices,,"Whenever there is a transpose or inverse of a  group of matrices, I just reverse their order. For eg: $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ and $(ABC)^{T} = C^{T}B^{T}A^{T}$ But usually, I am taking this reverse ""rule"" for granted without really knowing why I have to reverse their order whenever there is an inverse or transpose. What is the reason for reversing their order?","Whenever there is a transpose or inverse of a  group of matrices, I just reverse their order. For eg: $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ and $(ABC)^{T} = C^{T}B^{T}A^{T}$ But usually, I am taking this reverse ""rule"" for granted without really knowing why I have to reverse their order whenever there is an inverse or transpose. What is the reason for reversing their order?",,"['linear-algebra', 'matrices']"
78,What is the least value of $k$ for which $B^k = I$?,What is the least value of  for which ?,k B^k = I,"I found the following question in a textbook, and I wasn't able to solve it. Suppose $A$ and $B$ are two non-singular matrices such that $B \neq I$, $A^6 = I$, and $AB^2 = BA$, then what is the least value of $k$ for which $B^k = I$? My attempt We can see that $A^6 = I$, so, I tried to manipulate the second equation to involve $A^6$, so that we can get the identity matrix. But this didn't work, and I ended up getting something of the form, $B^{-1}AB^2 = A$. I wasn't able to reduce it further. Answer The answer is $k = 127$.","I found the following question in a textbook, and I wasn't able to solve it. Suppose $A$ and $B$ are two non-singular matrices such that $B \neq I$, $A^6 = I$, and $AB^2 = BA$, then what is the least value of $k$ for which $B^k = I$? My attempt We can see that $A^6 = I$, so, I tried to manipulate the second equation to involve $A^6$, so that we can get the identity matrix. But this didn't work, and I ended up getting something of the form, $B^{-1}AB^2 = A$. I wasn't able to reduce it further. Answer The answer is $k = 127$.",,"['linear-algebra', 'matrices']"
79,Prove that $\det(M-I)=0$ if $\det(M)=1$ and $MM^T=I$,Prove that  if  and,\det(M-I)=0 \det(M)=1 MM^T=I,"$M$ is a $3 \times 3$ matrix such that $\det(M)=1$ and $MM^T=I$, where $I$ is the identity matrix. Prove that $\det(M-I)=0$ I tried to take $M$ $=$ $$ \begin{pmatrix}         a  &b & c \\         d & e & f \\         g & h & i         \end{pmatrix} $$ but its expansion is too long.","$M$ is a $3 \times 3$ matrix such that $\det(M)=1$ and $MM^T=I$, where $I$ is the identity matrix. Prove that $\det(M-I)=0$ I tried to take $M$ $=$ $$ \begin{pmatrix}         a  &b & c \\         d & e & f \\         g & h & i         \end{pmatrix} $$ but its expansion is too long.",,"['linear-algebra', 'matrices', 'determinant']"
80,Thinking of mathematics in terms of analogs,Thinking of mathematics in terms of analogs,,"I think the way that I've come to think about mathematics is becoming problematic and I'm wondering if I should abandon it. When I study mathematics, I find myself trying to compare the mathematical constructs, operations, entities, and even the basic terminology (which I have come to understand is incredibly elegant, precise, and deliberate) to real world, physical, even visible phenomena. I think under the pretense that the things I do in the mathematical world represent real, fundamental structures in this Universe. For example, the fact that terms can 'cancel' out in an equation has profound implications on the workings of the Universe and should be heeded and studied as such. In other words, I try to make sense of the things I learn in math classes by finding their analogs in the real word, because I assume they must have at least one. Thinking with this frame of mind has led me to appreciate mathematics in a deeply profound and beautiful way, and it's the mindset that I try to share with other people when explaining why mathematics should be studied and why people describe it as beautiful. When I learn something new in a math class, I try to understand and remember that these are not simply tedious equations and formulas that mean nothing and come from nowhere, but that they have real physical and, mostly, intuitive meaning. All that being said, I'm taking my first liner algebra course this term, and it's becoming harder to utilize this mentality, not simply because linear algebra deals with such things as infinite dimensionality which we obviously have no intuitive way of grasping or visualizing, but really just because the class seems more about computation and calculation than concept and philosophy. I worry that my thinking has led me astray, primarily because it becomes hard to focus on just doing sheer, brute force calculation without wondering and worrying about what these constructs really mean. This leads me to fall behind in lecture, take hours longer than is probably necessary on the homework, and add to an overall level of frustration that has been building for some time now because of it, which only clouds my understanding even more. My question is really more of a plea for advice. Should I abandon my way of thinking about mathematics as though it will become increasingly unhelpful in future courses and topics, or is linear algebra truly more about numerical gymnastics than tangible interpretation? Should I focus, currently, on simply learning the algorithms for computation now assuming that the philosophical groundwork will be exposed later on, after which the conceptual work that I'm looking for will yield itself? I'd really appreciate responses from the people that frequent this site. I've been nothing but overwhelmed at the level of quality, thought, and sincerity in the answers I've read here and throughout the conversations I've eavesdropped so far. Also, please direct me to similar questions if you know of any, and help me with the tagging of this question, as it is the first one I've ever asked on this site.","I think the way that I've come to think about mathematics is becoming problematic and I'm wondering if I should abandon it. When I study mathematics, I find myself trying to compare the mathematical constructs, operations, entities, and even the basic terminology (which I have come to understand is incredibly elegant, precise, and deliberate) to real world, physical, even visible phenomena. I think under the pretense that the things I do in the mathematical world represent real, fundamental structures in this Universe. For example, the fact that terms can 'cancel' out in an equation has profound implications on the workings of the Universe and should be heeded and studied as such. In other words, I try to make sense of the things I learn in math classes by finding their analogs in the real word, because I assume they must have at least one. Thinking with this frame of mind has led me to appreciate mathematics in a deeply profound and beautiful way, and it's the mindset that I try to share with other people when explaining why mathematics should be studied and why people describe it as beautiful. When I learn something new in a math class, I try to understand and remember that these are not simply tedious equations and formulas that mean nothing and come from nowhere, but that they have real physical and, mostly, intuitive meaning. All that being said, I'm taking my first liner algebra course this term, and it's becoming harder to utilize this mentality, not simply because linear algebra deals with such things as infinite dimensionality which we obviously have no intuitive way of grasping or visualizing, but really just because the class seems more about computation and calculation than concept and philosophy. I worry that my thinking has led me astray, primarily because it becomes hard to focus on just doing sheer, brute force calculation without wondering and worrying about what these constructs really mean. This leads me to fall behind in lecture, take hours longer than is probably necessary on the homework, and add to an overall level of frustration that has been building for some time now because of it, which only clouds my understanding even more. My question is really more of a plea for advice. Should I abandon my way of thinking about mathematics as though it will become increasingly unhelpful in future courses and topics, or is linear algebra truly more about numerical gymnastics than tangible interpretation? Should I focus, currently, on simply learning the algorithms for computation now assuming that the philosophical groundwork will be exposed later on, after which the conceptual work that I'm looking for will yield itself? I'd really appreciate responses from the people that frequent this site. I've been nothing but overwhelmed at the level of quality, thought, and sincerity in the answers I've read here and throughout the conversations I've eavesdropped so far. Also, please direct me to similar questions if you know of any, and help me with the tagging of this question, as it is the first one I've ever asked on this site.",,"['linear-algebra', 'soft-question', 'education', 'advice']"
81,How find this matrix $A=(\sqrt{i^2+j^2})$ eigenvalue,How find this matrix  eigenvalue,A=(\sqrt{i^2+j^2}),"let the matrix  $$A=(a_{ij})_{n\times n}$$ where $$a_{ij}=\sqrt{i^2+j^2}$$ Question: Find the difference $sign{(A)}$ can see this define: http://en.wikipedia.org/wiki/Sylvester's_law_of_inertia My try: consider the  $$|\lambda I-A|=\begin{vmatrix} \lambda-\sqrt{2}&-\sqrt{3}&\cdots&-\sqrt{1^2+n^2}\\ -\sqrt{3}&\lambda-\sqrt{8}&\cdots&-\sqrt{n^2+2^2}\\ \cdots&\cdots&\cdots\cdots\\ -\sqrt{n^2+1}&-\sqrt{n^2+2}&\cdots&\lambda-\sqrt{n^2+n^2} \end{vmatrix}$$ and I found this determinant is not easy, maybe we can consider this characteristic polynomial.and this problem is from china hard linear algebra book problem  Thank you","let the matrix  $$A=(a_{ij})_{n\times n}$$ where $$a_{ij}=\sqrt{i^2+j^2}$$ Question: Find the difference $sign{(A)}$ can see this define: http://en.wikipedia.org/wiki/Sylvester's_law_of_inertia My try: consider the  $$|\lambda I-A|=\begin{vmatrix} \lambda-\sqrt{2}&-\sqrt{3}&\cdots&-\sqrt{1^2+n^2}\\ -\sqrt{3}&\lambda-\sqrt{8}&\cdots&-\sqrt{n^2+2^2}\\ \cdots&\cdots&\cdots\cdots\\ -\sqrt{n^2+1}&-\sqrt{n^2+2}&\cdots&\lambda-\sqrt{n^2+n^2} \end{vmatrix}$$ and I found this determinant is not easy, maybe we can consider this characteristic polynomial.and this problem is from china hard linear algebra book problem  Thank you",,"['linear-algebra', 'matrices', 'quadratic-forms']"
82,Probability that $n$ vectors drawn randomly from $\mathbb{R}^n$ are linearly independent,Probability that  vectors drawn randomly from  are linearly independent,n \mathbb{R}^n,"Let's take $n$ vectors in $\mathbb{R}^n$ at random. What is the probability that these vectors are linearly independent? (i.e. they form a basis of $\mathbb{R}^n$) (of course the problem is equivalent of ""taken a matrix at random from $M_{\mathbb{R}}(n,n)$, what is the probability that its determinant $\neq 0$) Don't know if this question is difficult to answer or not. Please share any information about it! :-) (the $n$ vectors are meant with real values, I'm interested in solutions in $\mathbb{N}$ or $\mathbb{Q}$ or whatever fields you like)","Let's take $n$ vectors in $\mathbb{R}^n$ at random. What is the probability that these vectors are linearly independent? (i.e. they form a basis of $\mathbb{R}^n$) (of course the problem is equivalent of ""taken a matrix at random from $M_{\mathbb{R}}(n,n)$, what is the probability that its determinant $\neq 0$) Don't know if this question is difficult to answer or not. Please share any information about it! :-) (the $n$ vectors are meant with real values, I'm interested in solutions in $\mathbb{N}$ or $\mathbb{Q}$ or whatever fields you like)",,"['linear-algebra', 'vector-spaces', 'random-matrices']"
83,Can a non-zero vector have zero image under every linear functional?,Can a non-zero vector have zero image under every linear functional?,,"Let $X$ be an infinite-dimensional vector space, and let $x_0$ be an element of $X$ such that $f(x_0)=0$ for every linear functional $f$ defined on $X$. Then can we prove that $x_0$ is the zero vector in $X$? Or, is it possible to find a non-zero $x_0$ whose image under every linear functional is zero? In the case of $X$ being finite-dimensional, we can prove that if $f(x_0)=0$ for every linear functional $f$ on $X$, then $x_0$ must be the zero vector. What is the situation in the infinite-dimensional case?","Let $X$ be an infinite-dimensional vector space, and let $x_0$ be an element of $X$ such that $f(x_0)=0$ for every linear functional $f$ defined on $X$. Then can we prove that $x_0$ is the zero vector in $X$? Or, is it possible to find a non-zero $x_0$ whose image under every linear functional is zero? In the case of $X$ being finite-dimensional, we can prove that if $f(x_0)=0$ for every linear functional $f$ on $X$, then $x_0$ must be the zero vector. What is the situation in the infinite-dimensional case?",,"['linear-algebra', 'analysis', 'functional-analysis', 'vector-spaces']"
84,"Non surjectivity of the exponential map to GL(2,R)","Non surjectivity of the exponential map to GL(2,R)",,"I was asked to show that the exponential map $\exp: \mathfrak{g} \mapsto G$ is not surjective by proving that the matrix $\left(\matrix{-1 & 0 \\ 0 & -2}\right)\in \text{GL}(2,\mathbb{R})$ can't be the exponential of any matrix $A \in \mathfrak{gl}(2,\mathbb{R})$. My proof (edited) Lemma : A diagonal matrix $M \in \rm{GL}(2,\mathbb{R})$ is the exponential of a matrix $A\in\mathfrak{gl}(2,\mathbb{R})$ if it has positive eigenvalues or a unique (double) negative eigenvalue. For any diagonal matrix $A = \left(\matrix{a & 0 \\ 0 & d}\right)$, we have  $\displaystyle\text{e}^A=\sum_{k=0}^\infty\dfrac{A^k}{k!} = \left(\matrix{\text{e}^a & 0 \\ 0 & \text{e}^d}\right)$. Then,      $$\forall\; \lambda,\,\mu > 0, \qquad\left(\matrix{\lambda & 0 \\ 0 & \mu}\right) = \exp \left(\matrix{\ln\lambda & 0 \\ 0 & \ln\mu}\right)$$ By the Cayley-Hamilton theorem, every square matrix is a root of its characteristic polynomial. Thus I can express any power of $A$ as a linear combination of $A$ itself and the identity matrix. For instance, $A^2= tA-d\mathbb{I}\;$ and $A^3=(t^2-d)A-td\mathbb{I}\;$, where $t=\text{tr} A, \; d=\det A$. It follows that $M=\text{e}^A=\alpha A+\beta\mathbb{I}\;$ (even if I don't know how to calculate those coefficients). If $M$ has distinct negative eigenvalues, $\alpha \neq 0$: otherwise we'd have $M = \beta \mathbb{I}$, a contradiction. This implies that  $A$ must be diagonal too, but then it can't exponentiate to something with negative eigenvalues (see below the conclusion of my exercise). If $M$ has a double negative eigenvalue, it is a negative multiple of $\mathbb{I}$ and $\alpha$ must be $0$. Observing that $\exp\left(\matrix{0 & \pi \\ -\pi & 0}\right)=\left(\matrix{-1 & 0 \\ 0 & -1}\right)=-\mathbb{I}$ and experimenting a bit, it is straightforward to prove that     $$\forall\; \lambda>0, \qquad -\lambda\mathbb{I}=\left(\matrix{-\lambda & 0 \\ 0 & -\lambda}\right)=\exp\left(\matrix{\ln\lambda & \pm\, m\pi \\ \mp\, m\pi & \ln\lambda}\right), \qquad \forall m \; \text{odd integer}$$ The conclusion is immediate: the diagonal matrix I'm given must be the exponential of a diagonal matrix (if it exists) $A = \left(\matrix{a & 0 \\ 0 & d}\right)$ such that $\text{e}^a=-1\;\text{and}\;\text{e}^d=-2$, which is impossible. Alternative approach, problems and questions My efforts were aimed at using that $\exp(PXP^{-1})=P\exp XP^{-1},\; \forall X\in \mathfrak{gl}(n,\mathbb{R}),\forall P\in\text{GL}(n,\mathbb{R})$. The problem is that if $\exp X$ is a diagonal non-scalar matrix, then it doesn't lie in the center of the algebra and thus doesn't commute with $P$. That's a pity, because I can (almost) always diagonalize $A=\left(\matrix{a & b \\ c & d}\right)$, writing $A=PXP^{-1}$, where $X$ is diagonal: If $\det A = 0$, since $\text{tr}A \neq 0$, $A$ has two distinct eigenvalues ($0$ and $\text{tr}A$) and is diagonalizable If $\det A \neq 0$, the characteristic equation is $\lambda^2-\text{tr}A\lambda +\det A = 0$ and if its discriminant is $\Delta \neq 0$, then again I get two (possibly complex) distinct eigenvalues and $A$ is diagonalizable. if $\Delta = 0$, the only eigenvalue is $\lambda=\dfrac12 \text{tr}A$, whose geometric multiplicity is only $1$, making $A$ not diagonalizable. This line of thought seems quite promising to me. I was wondering if it exists some way to conclude the proof of the ""diagonalizable $A$"" case from $P\left(\matrix{\text{e}^{\lambda_1} & 0 \\ 0 & \text{e}^{\lambda_2}}\right)P^{-1} = \left(\matrix{-1 & 0 \\ 0 & -2}\right)$ and then to handle the ""non-diagonalizable $A$"" case. Bonus question I was also trying to work with the other property $\exp(X+Y)=\exp X \exp Y \quad \text{iff}\quad [X,Y]=0$, exploiting some decomposition of $A$, such as $$A = \left(\matrix{a & b \\ c & d}\right) = A_{\text{sym}}+A_{\text{antisym}}\;, \quad\text{where}\quad A_{\text{sym}}=\frac12(A+A^T) \;,\;A_{\text{antisym}}=\frac12(A-A^T)$$  $$\text{or} \quad A = \left(\matrix{a & b \\ c & d}\right) = A_{\text{diag}}+A_{\text{antidiag}}\;, \quad\text{where}\quad A_{\text{diag}}=\left(\matrix{a & 0 \\ 0 & d}\right) \;,\;A_{\text{antisym}}=\left(\matrix{0 & b \\ c & 0}\right)\;. $$ Unfortunately, both decomposition don't help, because $[A_{\text{sym}},A_{\text{antisym}}] \neq 0$, $[A_{\text{diag}},A_{\text{antidiag}}] \neq 0$. Is there a chance that something like this could lead to a solution? Update: Thanks to Pink Elephants, who pointed out a flaw in my proof, I modified the statement of the lemma and added some detail. I think now it works well for my exercise.","I was asked to show that the exponential map $\exp: \mathfrak{g} \mapsto G$ is not surjective by proving that the matrix $\left(\matrix{-1 & 0 \\ 0 & -2}\right)\in \text{GL}(2,\mathbb{R})$ can't be the exponential of any matrix $A \in \mathfrak{gl}(2,\mathbb{R})$. My proof (edited) Lemma : A diagonal matrix $M \in \rm{GL}(2,\mathbb{R})$ is the exponential of a matrix $A\in\mathfrak{gl}(2,\mathbb{R})$ if it has positive eigenvalues or a unique (double) negative eigenvalue. For any diagonal matrix $A = \left(\matrix{a & 0 \\ 0 & d}\right)$, we have  $\displaystyle\text{e}^A=\sum_{k=0}^\infty\dfrac{A^k}{k!} = \left(\matrix{\text{e}^a & 0 \\ 0 & \text{e}^d}\right)$. Then,      $$\forall\; \lambda,\,\mu > 0, \qquad\left(\matrix{\lambda & 0 \\ 0 & \mu}\right) = \exp \left(\matrix{\ln\lambda & 0 \\ 0 & \ln\mu}\right)$$ By the Cayley-Hamilton theorem, every square matrix is a root of its characteristic polynomial. Thus I can express any power of $A$ as a linear combination of $A$ itself and the identity matrix. For instance, $A^2= tA-d\mathbb{I}\;$ and $A^3=(t^2-d)A-td\mathbb{I}\;$, where $t=\text{tr} A, \; d=\det A$. It follows that $M=\text{e}^A=\alpha A+\beta\mathbb{I}\;$ (even if I don't know how to calculate those coefficients). If $M$ has distinct negative eigenvalues, $\alpha \neq 0$: otherwise we'd have $M = \beta \mathbb{I}$, a contradiction. This implies that  $A$ must be diagonal too, but then it can't exponentiate to something with negative eigenvalues (see below the conclusion of my exercise). If $M$ has a double negative eigenvalue, it is a negative multiple of $\mathbb{I}$ and $\alpha$ must be $0$. Observing that $\exp\left(\matrix{0 & \pi \\ -\pi & 0}\right)=\left(\matrix{-1 & 0 \\ 0 & -1}\right)=-\mathbb{I}$ and experimenting a bit, it is straightforward to prove that     $$\forall\; \lambda>0, \qquad -\lambda\mathbb{I}=\left(\matrix{-\lambda & 0 \\ 0 & -\lambda}\right)=\exp\left(\matrix{\ln\lambda & \pm\, m\pi \\ \mp\, m\pi & \ln\lambda}\right), \qquad \forall m \; \text{odd integer}$$ The conclusion is immediate: the diagonal matrix I'm given must be the exponential of a diagonal matrix (if it exists) $A = \left(\matrix{a & 0 \\ 0 & d}\right)$ such that $\text{e}^a=-1\;\text{and}\;\text{e}^d=-2$, which is impossible. Alternative approach, problems and questions My efforts were aimed at using that $\exp(PXP^{-1})=P\exp XP^{-1},\; \forall X\in \mathfrak{gl}(n,\mathbb{R}),\forall P\in\text{GL}(n,\mathbb{R})$. The problem is that if $\exp X$ is a diagonal non-scalar matrix, then it doesn't lie in the center of the algebra and thus doesn't commute with $P$. That's a pity, because I can (almost) always diagonalize $A=\left(\matrix{a & b \\ c & d}\right)$, writing $A=PXP^{-1}$, where $X$ is diagonal: If $\det A = 0$, since $\text{tr}A \neq 0$, $A$ has two distinct eigenvalues ($0$ and $\text{tr}A$) and is diagonalizable If $\det A \neq 0$, the characteristic equation is $\lambda^2-\text{tr}A\lambda +\det A = 0$ and if its discriminant is $\Delta \neq 0$, then again I get two (possibly complex) distinct eigenvalues and $A$ is diagonalizable. if $\Delta = 0$, the only eigenvalue is $\lambda=\dfrac12 \text{tr}A$, whose geometric multiplicity is only $1$, making $A$ not diagonalizable. This line of thought seems quite promising to me. I was wondering if it exists some way to conclude the proof of the ""diagonalizable $A$"" case from $P\left(\matrix{\text{e}^{\lambda_1} & 0 \\ 0 & \text{e}^{\lambda_2}}\right)P^{-1} = \left(\matrix{-1 & 0 \\ 0 & -2}\right)$ and then to handle the ""non-diagonalizable $A$"" case. Bonus question I was also trying to work with the other property $\exp(X+Y)=\exp X \exp Y \quad \text{iff}\quad [X,Y]=0$, exploiting some decomposition of $A$, such as $$A = \left(\matrix{a & b \\ c & d}\right) = A_{\text{sym}}+A_{\text{antisym}}\;, \quad\text{where}\quad A_{\text{sym}}=\frac12(A+A^T) \;,\;A_{\text{antisym}}=\frac12(A-A^T)$$  $$\text{or} \quad A = \left(\matrix{a & b \\ c & d}\right) = A_{\text{diag}}+A_{\text{antidiag}}\;, \quad\text{where}\quad A_{\text{diag}}=\left(\matrix{a & 0 \\ 0 & d}\right) \;,\;A_{\text{antisym}}=\left(\matrix{0 & b \\ c & 0}\right)\;. $$ Unfortunately, both decomposition don't help, because $[A_{\text{sym}},A_{\text{antisym}}] \neq 0$, $[A_{\text{diag}},A_{\text{antidiag}}] \neq 0$. Is there a chance that something like this could lead to a solution? Update: Thanks to Pink Elephants, who pointed out a flaw in my proof, I modified the statement of the lemma and added some detail. I think now it works well for my exercise.",,"['linear-algebra', 'differential-geometry', 'lie-groups']"
85,"Same eigenvalues, different eigenvectors","Same eigenvalues, different eigenvectors",,"I'm interested in the case of a specific matrix having different eigenvectors corresponding to two identical eigenvalues. The method I use for spectral decomposition returns different eigenvectors, even though the eigenvalue is the same. Is this possible, and if so, what this tells about the matrix?","I'm interested in the case of a specific matrix having different eigenvectors corresponding to two identical eigenvalues. The method I use for spectral decomposition returns different eigenvectors, even though the eigenvalue is the same. Is this possible, and if so, what this tells about the matrix?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
86,Eigenvalues of a matrix with repeating pattern of entries,Eigenvalues of a matrix with repeating pattern of entries,,"I observed that if $$A = \begin{bmatrix} a & b  \\ c & d \end{bmatrix}$$ with non-zero eigenvalues $\alpha$ and $\beta$, then $$\begin{bmatrix} A & A\\ A & A \end{bmatrix}$$ has eigenvalues $2 \alpha$, $2 \beta$, and $0$. Also, $$\begin{bmatrix} A & A & A \\ A & A & A \\ A & A & A \end{bmatrix}$$ has eigenvalues $3 \alpha$, $3 \beta$, $0$. Therefore, my conjecture is that for some $r$, $A^{[r]}$ has eigenvalues $(r+1) \alpha$, $(r+1) \beta$, $0$. Is it correct? Is there some theorems related to this? How about their eigenvectors? Can you please send me links that can help me with this kind of problem? PS. This is my first time asking here. I am an undergrad math student. Please help me.  Thank u so much.","I observed that if $$A = \begin{bmatrix} a & b  \\ c & d \end{bmatrix}$$ with non-zero eigenvalues $\alpha$ and $\beta$, then $$\begin{bmatrix} A & A\\ A & A \end{bmatrix}$$ has eigenvalues $2 \alpha$, $2 \beta$, and $0$. Also, $$\begin{bmatrix} A & A & A \\ A & A & A \\ A & A & A \end{bmatrix}$$ has eigenvalues $3 \alpha$, $3 \beta$, $0$. Therefore, my conjecture is that for some $r$, $A^{[r]}$ has eigenvalues $(r+1) \alpha$, $(r+1) \beta$, $0$. Is it correct? Is there some theorems related to this? How about their eigenvectors? Can you please send me links that can help me with this kind of problem? PS. This is my first time asking here. I am an undergrad math student. Please help me.  Thank u so much.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
87,Prove the positive definiteness of Hilbert matrix,Prove the positive definiteness of Hilbert matrix,,"This is so called Hilbert matrix which is known as a poorly conditioned matrix.  $$ A =        \left(\begin{matrix}         1 & \frac{1}{2} & \frac{1}{3} & ... & \frac{1}{n} \\         \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & ... & \frac{1}{n + 1} \\         \vdots & \vdots & \vdots & \ddots & \vdots \\         \frac{1}{n} & \frac{1}{n + 1} & \frac{1}{n + 2} & ... & \frac{1}{2n - 1} \\         \end{matrix}\right) $$ The task is to prove that matrix A is positively definite. A possible way to go is to look at the scalar product: $$(f, g) = \int_0^1f(x)g(x)dx$$ within the space of polynomials of degree not higher than $(n - 1)$. There our form $(\cdot, \cdot)$ is bilinear, symmetric and positively defined thus our polynomials' space with that form is, in fact, an euclidean space of degree $n$. Now let's take the following basis: ${1, x, x^2, ..., x^\left(n - 1\right)}$. It's easy to see that matrix A is a Gram matrix for this basis. But is there another approach?","This is so called Hilbert matrix which is known as a poorly conditioned matrix.  $$ A =        \left(\begin{matrix}         1 & \frac{1}{2} & \frac{1}{3} & ... & \frac{1}{n} \\         \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & ... & \frac{1}{n + 1} \\         \vdots & \vdots & \vdots & \ddots & \vdots \\         \frac{1}{n} & \frac{1}{n + 1} & \frac{1}{n + 2} & ... & \frac{1}{2n - 1} \\         \end{matrix}\right) $$ The task is to prove that matrix A is positively definite. A possible way to go is to look at the scalar product: $$(f, g) = \int_0^1f(x)g(x)dx$$ within the space of polynomials of degree not higher than $(n - 1)$. There our form $(\cdot, \cdot)$ is bilinear, symmetric and positively defined thus our polynomials' space with that form is, in fact, an euclidean space of degree $n$. Now let's take the following basis: ${1, x, x^2, ..., x^\left(n - 1\right)}$. It's easy to see that matrix A is a Gram matrix for this basis. But is there another approach?",,"['linear-algebra', 'matrices', 'bilinear-form', 'positive-definite']"
88,What is the meaning of the eigenvalues of the matrix representation of a bilinear form?,What is the meaning of the eigenvalues of the matrix representation of a bilinear form?,,"Given a bilinear form $B$ on some finite-dimensional vector space $V$, we can always represent $B$ by some matrix $A$ such that $B(v,w) = [v]^TA[w]$.  Thus we could associate the eigenvalues of $A$ with $B$.  But does that have any meaning? What, geometrically or algebraically, do the eigenvalues of the matrix representation of a bilinear form represent?","Given a bilinear form $B$ on some finite-dimensional vector space $V$, we can always represent $B$ by some matrix $A$ such that $B(v,w) = [v]^TA[w]$.  Thus we could associate the eigenvalues of $A$ with $B$.  But does that have any meaning? What, geometrically or algebraically, do the eigenvalues of the matrix representation of a bilinear form represent?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'bilinear-form']"
89,Determinant of a $2\times 2$ block matrix,Determinant of a  block matrix,2\times 2,"I would like to know the proof for: The determinant of the block matrix $$\begin{pmatrix} A & B\\ C& D\end{pmatrix}$$ equals $$(D-1) \det(A) + \det(A-BC) = (D+1) \det(A) - \det(A+BC)$$ when $A$ is a square matrix, $D$ is a scalar, $C$ is a row vector and $B$ is a column vector. I appreciate your help.","I would like to know the proof for: The determinant of the block matrix equals when is a square matrix, is a scalar, is a row vector and is a column vector. I appreciate your help.",\begin{pmatrix} A & B\\ C& D\end{pmatrix} (D-1) \det(A) + \det(A-BC) = (D+1) \det(A) - \det(A+BC) A D C B,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
90,Finding nonnegative solutions to an underdetermined linear system,Finding nonnegative solutions to an underdetermined linear system,,"Here's the environment of my problem: I have a linear system of 4 equations in 8 unknowns (i.e. $Ax = b$, where $A$ is $4 \times 8$, $x$ is $8 \times 1$, and $b$ is $4 \times 1$, with $A$ given and $b$ varying with some exogenous parameters, i.e. essentially given). So, after reducing the system, the solution space can be described by a linear function $f \, : \, \mathbb{R}^4 \rightarrow \mathbb{R}^4$, which I can find explicitly. Because of the nature of the problem, I am interested only in nonnegative values for each of the 8 unknowns. So, I can restrict the function $f$ to the domain $\mathbb{R}^4_{\geq 0}$ (i.e. examine only nonnegative values for each of the 4 free variables). What I'm interested in knowing (and classifying for different parameter values, which will change $b$) is whether solutions exist for which all 8 unknowns are nonnegative--or equivalently, whether the function $f$ takes nonnegative values in the nonnegative domain $\mathbb{R}^4_{\geq 0}$. If there's a way to figure this out analytically, that would be great; alternatively, if there's a good way to do this numerically (I use MATLAB), that would be only slightly less great. Thanks in advance! Thank you all so much for your answers--this has been a huge help. Working out the solution by hand using the LL Dines paper may prove to be too cumbersome, as it requires knowing the sign of the coefficients, and we examine thousands of different sets of coefficients (as we vary several parameters); however, if nothing else, it's really nice to know that there is an algorithm for (analytically) determining the existence of nonnegative solutions. Moreover, all of the software advice given has been extremely helpful. I've looked into all of your suggestions, and for now I think we'll try using lsqnonneg. Thanks again for your help!","Here's the environment of my problem: I have a linear system of 4 equations in 8 unknowns (i.e. $Ax = b$, where $A$ is $4 \times 8$, $x$ is $8 \times 1$, and $b$ is $4 \times 1$, with $A$ given and $b$ varying with some exogenous parameters, i.e. essentially given). So, after reducing the system, the solution space can be described by a linear function $f \, : \, \mathbb{R}^4 \rightarrow \mathbb{R}^4$, which I can find explicitly. Because of the nature of the problem, I am interested only in nonnegative values for each of the 8 unknowns. So, I can restrict the function $f$ to the domain $\mathbb{R}^4_{\geq 0}$ (i.e. examine only nonnegative values for each of the 4 free variables). What I'm interested in knowing (and classifying for different parameter values, which will change $b$) is whether solutions exist for which all 8 unknowns are nonnegative--or equivalently, whether the function $f$ takes nonnegative values in the nonnegative domain $\mathbb{R}^4_{\geq 0}$. If there's a way to figure this out analytically, that would be great; alternatively, if there's a good way to do this numerically (I use MATLAB), that would be only slightly less great. Thanks in advance! Thank you all so much for your answers--this has been a huge help. Working out the solution by hand using the LL Dines paper may prove to be too cumbersome, as it requires knowing the sign of the coefficients, and we examine thousands of different sets of coefficients (as we vary several parameters); however, if nothing else, it's really nice to know that there is an algorithm for (analytically) determining the existence of nonnegative solutions. Moreover, all of the software advice given has been extremely helpful. I've looked into all of your suggestions, and for now I think we'll try using lsqnonneg. Thanks again for your help!",,"['linear-algebra', 'numerical-methods', 'linear-programming', 'numerical-linear-algebra']"
91,Finding a invariant subspaces for a specific matrix,Finding a invariant subspaces for a specific matrix,,"Good morning, How does one find the subspaces that are invariant under $A$ for $$A = \begin{pmatrix} 1 & 0 & 0 & 0\\  0 &2  & 0 & 0\\  0 & 0 & 2 & 0\\  0 & 0 & 0 & 3 \end{pmatrix}\  \in M_{4} (\mathbb{R}).$$ Thank you.","Good morning, How does one find the subspaces that are invariant under $A$ for $$A = \begin{pmatrix} 1 & 0 & 0 & 0\\  0 &2  & 0 & 0\\  0 & 0 & 2 & 0\\  0 & 0 & 0 & 3 \end{pmatrix}\  \in M_{4} (\mathbb{R}).$$ Thank you.",,[]
92,"For $A \in \mathcal{M}_3(\mathbb{C})$, does $\mathrm{tr}(A^2) = \mathrm{tr}(A^3) \in \mathbb{Q}$ imply $\mathrm{tr}(A^4) \in \mathbb{Q}$?","For , does  imply ?",A \in \mathcal{M}_3(\mathbb{C}) \mathrm{tr}(A^2) = \mathrm{tr}(A^3) \in \mathbb{Q} \mathrm{tr}(A^4) \in \mathbb{Q},"Let $A \in \mathcal{M}_3(\mathbb{C})$ such that $\mathrm{tr}(A^2) = \mathrm{tr}(A^3) \in \mathbb{Q},$ where $\mathrm{tr}(A)$ is the trace of $A.$ It is possible to prove that $\mathrm{tr}(A^4) \in \mathbb{Q}?$",Let such that where is the trace of It is possible to prove that,"A \in \mathcal{M}_3(\mathbb{C}) \mathrm{tr}(A^2) = \mathrm{tr}(A^3) \in \mathbb{Q}, \mathrm{tr}(A) A. \mathrm{tr}(A^4) \in \mathbb{Q}?","['linear-algebra', 'matrices', 'rational-numbers', 'trace']"
93,Proving the Lie-Product formula,Proving the Lie-Product formula,,"Let $A,B\in gl(n)$. Then prove $$e^{A+B} = \lim_{k\to\infty} \left (e^{\frac{A}{k}}e^{\frac{B}{k}}\right )^k$$ I found this theorem in this notes http://www4.ncsu.edu/~aalexan3/articles/liegroups.pdf but there is no proof. I am new to exponential of matrices and can't prove this by my own.","Let $A,B\in gl(n)$. Then prove $$e^{A+B} = \lim_{k\to\infty} \left (e^{\frac{A}{k}}e^{\frac{B}{k}}\right )^k$$ I found this theorem in this notes http://www4.ncsu.edu/~aalexan3/articles/liegroups.pdf but there is no proof. I am new to exponential of matrices and can't prove this by my own.",,"['linear-algebra', 'matrices', 'functional-analysis', 'matrix-exponential']"
94,Can all linear transformations be calculated with a matrix?,Can all linear transformations be calculated with a matrix?,,"For all finite vector spaces and all linear transformations to/from those spaces, how can you prove/show from the definition of a linear transformation that all linear transformations can be calculated using a matrix.","For all finite vector spaces and all linear transformations to/from those spaces, how can you prove/show from the definition of a linear transformation that all linear transformations can be calculated using a matrix.",,"['linear-algebra', 'matrices', 'linear-transformations']"
95,Prove that a subspace of dimension $n$ of a vector space of dimension $n$ is the whole space.,Prove that a subspace of dimension  of a vector space of dimension  is the whole space.,n n,"Maybe this is a stupid question. I was brought to this from the observation that an infinite dimensional vector space can have proper subspace that have the same dimension of the whole space. But, for a finite dimensional vector space, as $\mathbb{R}^n$ it seems obvious that, if $S$ is a subspace of dimension $n$,  than  $S=\mathbb{R}^n$. I've tempted to give a proof this statement, thinking that was a simple exercise, but I am no able to find  a good way. I suppose that I'm lost in a teacup so I need an help. Added after the first answers. The answer of @Mark Joshi was my first idea, so I llustrate better where is my trouble. Suppose $S \in \mathbb{R}^n$ be a subspace of dimension $n$. This means, by definition, that: 1) there exists a set of linearly independent vectors $\{v_1, v_2, \cdots, v_n\}$ with $v_i \in S$ and 2) $\forall v \in S$ we have $v \ne v_i \; \forall i \Rightarrow \{v, v_1,\cdots,v_n\} $ are linearly dependent. Note that 2) is valid for $v\in S$ and all $v_i \in S$. Since $\mathbb{R}^n$ is a vector space, we know that there exists a set $\{u_1,u_2,\cdots,u_n\}$ of vectors $u_i \in \mathbb{R}^n $ such that: 3) $\{u_1,u_2,\cdots,u_n\}$ are linearly independent and 4) $\forall u \in \mathbb{R}^n$ we have $u \ne u_i \; \forall i \Rightarrow \{u, u_1,\cdots,u_n\} $ are linearly dependent. Now, if $u, u_i \in \mathbb{R}^n/S$ I can not conclude from 1) and 2) that $u$ is a linear combination of $\{v_i\}$ and this means that I cannot decide if it is in the span of $ \{v, v_1,\cdots,v_n\} $. Surely it is a linear combination of $\{u_1,u_2,\cdots,u_n\}$ , but some (or all) of these $u_i$ can well not be elements of $S$. So it seams that we can have a vector $u$ that is a linear combination of $n$ linearly independent $\{u_i\}$ but we connot proof that it is also a linear combination of $\{v_i\}$. And this is my trouble. It seems to me that the way suggested by @Awllower has the some problem when we search to show that $f :S \rightarrow \mathbb{R}^n$ defined by the correspondence $v_i \rightarrow u_i$ is bijective, because we have to proof that $\forall u \in \mathbb{R}^n$ there exists $v \in S$ such that $v=\sum_i a_i v_i \Rightarrow u=\sum_i a_i u_i$ and this come to the same trouble. At last, I am sure that there is something of logically wrong in my reasoning, but it is so subtle or so stupid that i cannot see it.","Maybe this is a stupid question. I was brought to this from the observation that an infinite dimensional vector space can have proper subspace that have the same dimension of the whole space. But, for a finite dimensional vector space, as $\mathbb{R}^n$ it seems obvious that, if $S$ is a subspace of dimension $n$,  than  $S=\mathbb{R}^n$. I've tempted to give a proof this statement, thinking that was a simple exercise, but I am no able to find  a good way. I suppose that I'm lost in a teacup so I need an help. Added after the first answers. The answer of @Mark Joshi was my first idea, so I llustrate better where is my trouble. Suppose $S \in \mathbb{R}^n$ be a subspace of dimension $n$. This means, by definition, that: 1) there exists a set of linearly independent vectors $\{v_1, v_2, \cdots, v_n\}$ with $v_i \in S$ and 2) $\forall v \in S$ we have $v \ne v_i \; \forall i \Rightarrow \{v, v_1,\cdots,v_n\} $ are linearly dependent. Note that 2) is valid for $v\in S$ and all $v_i \in S$. Since $\mathbb{R}^n$ is a vector space, we know that there exists a set $\{u_1,u_2,\cdots,u_n\}$ of vectors $u_i \in \mathbb{R}^n $ such that: 3) $\{u_1,u_2,\cdots,u_n\}$ are linearly independent and 4) $\forall u \in \mathbb{R}^n$ we have $u \ne u_i \; \forall i \Rightarrow \{u, u_1,\cdots,u_n\} $ are linearly dependent. Now, if $u, u_i \in \mathbb{R}^n/S$ I can not conclude from 1) and 2) that $u$ is a linear combination of $\{v_i\}$ and this means that I cannot decide if it is in the span of $ \{v, v_1,\cdots,v_n\} $. Surely it is a linear combination of $\{u_1,u_2,\cdots,u_n\}$ , but some (or all) of these $u_i$ can well not be elements of $S$. So it seams that we can have a vector $u$ that is a linear combination of $n$ linearly independent $\{u_i\}$ but we connot proof that it is also a linear combination of $\{v_i\}$. And this is my trouble. It seems to me that the way suggested by @Awllower has the some problem when we search to show that $f :S \rightarrow \mathbb{R}^n$ defined by the correspondence $v_i \rightarrow u_i$ is bijective, because we have to proof that $\forall u \in \mathbb{R}^n$ there exists $v \in S$ such that $v=\sum_i a_i v_i \Rightarrow u=\sum_i a_i u_i$ and this come to the same trouble. At last, I am sure that there is something of logically wrong in my reasoning, but it is so subtle or so stupid that i cannot see it.",,"['linear-algebra', 'vector-spaces']"
96,Sphere equation given 4 points,Sphere equation given 4 points,,"Find the equation of the Sphere give the 4 points (3,2,1), (1,-2,-3), (2,1,3) and (-1,1,2). The *failed* solution I tried is kinda straigh forward: We need to find the center of the sphere. Having the points: $$ p_{1}(3,2,1),\, p_{2}(1,-2,-3),\, p_{3}(2,1,3),\, p_{4}(-1,1,2) $$ 2 Triangles can be created using these points, let's call $A$ our triangle formed by the points $p_{1},\,p_{2}\, and\,\, p_{3}$; And $B$ our triangle formed by the points $p_{1},\, p_{3}\, and \,\,p_{4}$. Calculate the centroids of each triangle: $$ CA = (2,1/3,1/3)\\ CB = (4/3,4/3,2) $$ And also, a normal vector for each triangle: $$ \overrightarrow{NA} = \overrightarrow{p_{1}p_{2}} \times \overrightarrow{p_{1}p_{3}}\\ \overrightarrow{NB} = \overrightarrow{p_{1}p_{3}} \times \overrightarrow{p_{1}p_{4}} $$ $$ \overrightarrow{p_{1}p_{2}} = <-2,-4,-4>\\ \overrightarrow{p_{1}p_{3}} = <-1,-1,2>\\ \overrightarrow{p_{1}p_{4}} = <-4,-1,1>\\ \:\\ \overrightarrow{NA} = <-12, 8, -2>\\ \overrightarrow{NB} = <1, -7, -3>\\ $$ With the centroids and normals of triangles $A$ and $B$, we can build two parametric equations for a line, the first one intersects the centroid of $A$ and the other one the centroid $B$. $$ Line \enspace A\\ x = 2-12t \quad y = 1/3+8t \quad z = 1/3-2t\\ \:\\ Line \enspace B\\ x = 4/3 + s \quad y = 4/3 - 7s \quad z = 2 - 3s $$ The point where this lines intersect should be the center of the sphere, unfortunately this system of equations is not linearly dependent, that means that they do not intersect each other. What could be the problem here?","Find the equation of the Sphere give the 4 points (3,2,1), (1,-2,-3), (2,1,3) and (-1,1,2). The *failed* solution I tried is kinda straigh forward: We need to find the center of the sphere. Having the points: $$ p_{1}(3,2,1),\, p_{2}(1,-2,-3),\, p_{3}(2,1,3),\, p_{4}(-1,1,2) $$ 2 Triangles can be created using these points, let's call $A$ our triangle formed by the points $p_{1},\,p_{2}\, and\,\, p_{3}$; And $B$ our triangle formed by the points $p_{1},\, p_{3}\, and \,\,p_{4}$. Calculate the centroids of each triangle: $$ CA = (2,1/3,1/3)\\ CB = (4/3,4/3,2) $$ And also, a normal vector for each triangle: $$ \overrightarrow{NA} = \overrightarrow{p_{1}p_{2}} \times \overrightarrow{p_{1}p_{3}}\\ \overrightarrow{NB} = \overrightarrow{p_{1}p_{3}} \times \overrightarrow{p_{1}p_{4}} $$ $$ \overrightarrow{p_{1}p_{2}} = <-2,-4,-4>\\ \overrightarrow{p_{1}p_{3}} = <-1,-1,2>\\ \overrightarrow{p_{1}p_{4}} = <-4,-1,1>\\ \:\\ \overrightarrow{NA} = <-12, 8, -2>\\ \overrightarrow{NB} = <1, -7, -3>\\ $$ With the centroids and normals of triangles $A$ and $B$, we can build two parametric equations for a line, the first one intersects the centroid of $A$ and the other one the centroid $B$. $$ Line \enspace A\\ x = 2-12t \quad y = 1/3+8t \quad z = 1/3-2t\\ \:\\ Line \enspace B\\ x = 4/3 + s \quad y = 4/3 - 7s \quad z = 2 - 3s $$ The point where this lines intersect should be the center of the sphere, unfortunately this system of equations is not linearly dependent, that means that they do not intersect each other. What could be the problem here?",,"['linear-algebra', 'cross-product', 'centroid']"
97,Symmetry of Grassmanians,Symmetry of Grassmanians,,"I thought this might be simple (now I'm not sure) but can't solve it: why is it true that for $X,Y$ two linear $n$-subspaces of $\mathbb{R}^{n+k}$ there exists an orthogonal transformation of $\mathbb{R}^{n+k}$ that takes $X$ to $Y$ and $Y$ to $X$? (This is excercise 5-D in Milnor's Characteristic classes , so the statement should hold.)","I thought this might be simple (now I'm not sure) but can't solve it: why is it true that for $X,Y$ two linear $n$-subspaces of $\mathbb{R}^{n+k}$ there exists an orthogonal transformation of $\mathbb{R}^{n+k}$ that takes $X$ to $Y$ and $Y$ to $X$? (This is excercise 5-D in Milnor's Characteristic classes , so the statement should hold.)",,"['linear-algebra', 'algebraic-topology']"
98,The difference between norm and modulus,The difference between norm and modulus,,"I'd like to know the difference between norm of a vector, ||v|| and the modulus of a vector, |v|","I'd like to know the difference between norm of a vector, ||v|| and the modulus of a vector, |v|",,"['linear-algebra', 'vector-spaces']"
99,Can vector spaces over different fields be isomorphic?,Can vector spaces over different fields be isomorphic?,,"Two vector spaces are said to be isomorphic iff there's an invertible linear map between them. It can be shown that isomorphic vectors spaces would have to have the same finite dimension or both be infinite dimensional. But what if they are over different fields? For example, would the trivial vector space over $\mathbb C$ be considered isomorphic to the trivial vector space over $\mathbb R$? Or would it not, since, if we let $T$ be the only possible linear map between them, $T(i0)\neq iT(0)=i0$, since $i0$ is not defined in the codomain (since $i$ is not a scalar in $\mathbb R$)? Also, are there any other examples of vector spaces over different field that would be ""isomorphic"" like this?","Two vector spaces are said to be isomorphic iff there's an invertible linear map between them. It can be shown that isomorphic vectors spaces would have to have the same finite dimension or both be infinite dimensional. But what if they are over different fields? For example, would the trivial vector space over $\mathbb C$ be considered isomorphic to the trivial vector space over $\mathbb R$? Or would it not, since, if we let $T$ be the only possible linear map between them, $T(i0)\neq iT(0)=i0$, since $i0$ is not defined in the codomain (since $i$ is not a scalar in $\mathbb R$)? Also, are there any other examples of vector spaces over different field that would be ""isomorphic"" like this?",,['linear-algebra']
