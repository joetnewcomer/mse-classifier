,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Rigorously evaluating $\lim\limits_{t\to 1^-}\int_{\sin^{-1}(t/2)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$.,Rigorously evaluating .,\lim\limits_{t\to 1^-}\int_{\sin^{-1}(t/2)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx,"I'd like to find a way to rigorously evaluate the limit $$\lim\limits_{t\to 1^-}\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$$ This limit arises when one attempts to solve the Basel problem by evaluating the integral $$I(t)=\int_0^t\int_0^t\frac{1}{1-xy}dydx$$ using two methods: (1) expanding the integrand as a geometric series, and (2) using the change of variables $x=(u-v)/\sqrt2$ , $y=(u+v)/\sqrt2$ . Doing so, we find that for every $t\in(-1,1)$ , $$I(t)=\sum_{n=1}^{\infty}\frac{t^{2n}}{n^2}$$ $$\text{ and }$$ $$I(t)=2\left(\sin^{-1}\frac{t}{2}\right)^2+4\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$$ Now, the series $\sum t^{2n}/n^2$ evaluated at $t=1$ is $\sum 1/n^2$ . This series is convergent, so Abel's theorem implies that $I$ is continuous from the left at $1$ , and that $I(1)$ is the sum of the series $\sum 1/n^2$ (a similar conclusion can be reached for the value $-1$ ). Since $2\left(\sin^{-1} t/2\right)^2$ is continuous on $[-1,1]$ , it follows that the integral $$\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx = \frac{1}{4}\left(I(t)-2\left(\sin^{-1}\frac{t}{2}\right)^2\right)$$ is continuous on $[-1,1]$ , and that we should be able to compute $$\lim\limits_{t\to 1^-}\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx$$ by directly substituting in the value $t=1$ . I'm a bit skeptical about the validity of this reasoning, though. What makes me uncomfortable is the fact that the integrand of the resulting integral $$\int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\sin x}{\cos x}\right)dx$$ has a singularity at $\pi/2$ , an element of the domain of integration, so the integrand has suddenly gone from being free of singularities to having one. Granted, the singularity in question is removable, but the fact that singularities aren't present for $t\in(-1,1)$ and that $I$ is continuous at $1$ really makes me wonder ""Why does the integrand suddenly accrue a singularity at $t=1$ ?"". Can someone reassure me that this procedure is valid, and, if you can, explain what's going on for $t=1$ ? If it's not valid, how can I rigorously evaluate the limit in question? Edit I should've mentioned this in the original post. Heuristically, I know that the limit is exactly $\pi^2/36$ . This follows from ""evaluating"" the limiting integral as follows: \begin{align*} \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\sin x}{\cos x}\right)dx &= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\cos\left(\frac{\pi}{2}-x\right)}{\sin\left(\frac{\pi}{2}-x\right)}\right)dx\\ &= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\tan \frac{\frac{\pi}{2}-x}{2}\right)dx\\ &= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\left(\frac{\pi}{4}-\frac{x}{2}\right)dx\\ &= \frac{\pi^2}{36} \end{align*} What I'm more interested in is learning a way to establish this limit rigorously, provided that this integral method does not suffice. I apologize for any confusion.","I'd like to find a way to rigorously evaluate the limit This limit arises when one attempts to solve the Basel problem by evaluating the integral using two methods: (1) expanding the integrand as a geometric series, and (2) using the change of variables , . Doing so, we find that for every , Now, the series evaluated at is . This series is convergent, so Abel's theorem implies that is continuous from the left at , and that is the sum of the series (a similar conclusion can be reached for the value ). Since is continuous on , it follows that the integral is continuous on , and that we should be able to compute by directly substituting in the value . I'm a bit skeptical about the validity of this reasoning, though. What makes me uncomfortable is the fact that the integrand of the resulting integral has a singularity at , an element of the domain of integration, so the integrand has suddenly gone from being free of singularities to having one. Granted, the singularity in question is removable, but the fact that singularities aren't present for and that is continuous at really makes me wonder ""Why does the integrand suddenly accrue a singularity at ?"". Can someone reassure me that this procedure is valid, and, if you can, explain what's going on for ? If it's not valid, how can I rigorously evaluate the limit in question? Edit I should've mentioned this in the original post. Heuristically, I know that the limit is exactly . This follows from ""evaluating"" the limiting integral as follows: What I'm more interested in is learning a way to establish this limit rigorously, provided that this integral method does not suffice. I apologize for any confusion.","\lim\limits_{t\to 1^-}\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx I(t)=\int_0^t\int_0^t\frac{1}{1-xy}dydx x=(u-v)/\sqrt2 y=(u+v)/\sqrt2 t\in(-1,1) I(t)=\sum_{n=1}^{\infty}\frac{t^{2n}}{n^2} \text{ and } I(t)=2\left(\sin^{-1}\frac{t}{2}\right)^2+4\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx \sum t^{2n}/n^2 t=1 \sum 1/n^2 I 1 I(1) \sum 1/n^2 -1 2\left(\sin^{-1} t/2\right)^2 [-1,1] \int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx = \frac{1}{4}\left(I(t)-2\left(\sin^{-1}\frac{t}{2}\right)^2\right) [-1,1] \lim\limits_{t\to 1^-}\int_{\sin^{-1}\left(\frac{t}{2}\right)}^{\sin^{-1}(t)}\tan^{-1}\left(\frac{t - \sin x}{\cos x}\right)dx t=1 \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\sin x}{\cos x}\right)dx \pi/2 t\in(-1,1) I 1 t=1 t=1 \pi^2/36 \begin{align*}
\int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\sin x}{\cos x}\right)dx &= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\frac{1-\cos\left(\frac{\pi}{2}-x\right)}{\sin\left(\frac{\pi}{2}-x\right)}\right)dx\\
&= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\tan^{-1}\left(\tan \frac{\frac{\pi}{2}-x}{2}\right)dx\\
&= \int_{\frac{\pi}{6}}^{\frac{\pi}{2}}\left(\frac{\pi}{4}-\frac{x}{2}\right)dx\\
&= \frac{\pi^2}{36}
\end{align*}","['real-analysis', 'calculus', 'integration']"
1,"Does the image of a moving square under the map $(r,\theta) \mapsto (r,\theta +\log r)$ converges?",Does the image of a moving square under the map  converges?,"(r,\theta) \mapsto (r,\theta +\log r)","Let $\phi:\mathbb{R}^2 \setminus\{0\} \to \mathbb{R}^2 \setminus\{0\}$ be defined in polar coordinates by $$ \phi:(r,\theta) \mapsto (r,\theta +\log r).$$ Let $S_n=[n-1,n+1]^2$ be a square of edge length $2$ centered around $(n,n)$ . Does $\phi(S_n)$ converges to a parallelogram when $n \to \infty$ w.r.t the Hausdorff distance (up to a translation and rotation which depend on $n$ )? If so, what are the edge-lengths/angles of that parallelogram? If not, does it converge to something else? Can we characterize the limit somehow? Note that $\phi$ is area-preserving. It is the one-time flow of the divergence-free vector field $\log(r)\, \partial_{\theta}$ . Heuristically, when $n$ is very large, $r$ and $\theta$ are approximately constant along the square $S_n$ . I am not sure how to use this formally though. Note that the corner $C=(x_n,y_n)=(n+1,n+1)""=""(r_n,\theta_n)=(\sqrt 2(n+1),\pi/4)$ of $S_n$ is mapped into $$ \tilde C=(\tilde r_n,\tilde  \theta_n)=\big(\sqrt 2(n+1),\pi/4+\log (\sqrt 2(n+1))\big). $$ and the corner $B=(x_n,y_n)=(n+1,n-1)""=""(r_n,\theta_n)=(\sqrt 2\sqrt{n^2+1},\arctan{\frac{n-1}{n+1}})$ is mapped into $$ \tilde B=(\tilde r_n,\tilde  \theta_n)=\big(\sqrt 2\sqrt{n^2+1},\arctan{\frac{n-1}{n+1}}+\log (\sqrt 2\sqrt{n^2+1})\big). $$ Thus, I guess we first need to decide whether the images of the four corners converge (up to translations) to the vertices of a parallelogram. The  difference of angles of $\tilde B,\tilde C$ diverges, since $$ \log\big( \frac{n+1}{\sqrt{n^2+1}}\big)\to -\infty $$ when $n \to \infty$ . I am not sure  if this has any relevance though. Below are pictures of $\operatorname{Image}(\phi)=\phi(S_n)$ for different values of $n$ : $n=2$ : $n=80$ : $n=800$ :","Let be defined in polar coordinates by Let be a square of edge length centered around . Does converges to a parallelogram when w.r.t the Hausdorff distance (up to a translation and rotation which depend on )? If so, what are the edge-lengths/angles of that parallelogram? If not, does it converge to something else? Can we characterize the limit somehow? Note that is area-preserving. It is the one-time flow of the divergence-free vector field . Heuristically, when is very large, and are approximately constant along the square . I am not sure how to use this formally though. Note that the corner of is mapped into and the corner is mapped into Thus, I guess we first need to decide whether the images of the four corners converge (up to translations) to the vertices of a parallelogram. The  difference of angles of diverges, since when . I am not sure  if this has any relevance though. Below are pictures of for different values of : : : :","\phi:\mathbb{R}^2 \setminus\{0\} \to \mathbb{R}^2 \setminus\{0\}  \phi:(r,\theta) \mapsto (r,\theta +\log r). S_n=[n-1,n+1]^2 2 (n,n) \phi(S_n) n \to \infty n \phi \log(r)\, \partial_{\theta} n r \theta S_n C=(x_n,y_n)=(n+1,n+1)""=""(r_n,\theta_n)=(\sqrt 2(n+1),\pi/4) S_n 
\tilde C=(\tilde r_n,\tilde  \theta_n)=\big(\sqrt 2(n+1),\pi/4+\log (\sqrt 2(n+1))\big).
 B=(x_n,y_n)=(n+1,n-1)""=""(r_n,\theta_n)=(\sqrt 2\sqrt{n^2+1},\arctan{\frac{n-1}{n+1}}) 
\tilde B=(\tilde r_n,\tilde  \theta_n)=\big(\sqrt 2\sqrt{n^2+1},\arctan{\frac{n-1}{n+1}}+\log (\sqrt 2\sqrt{n^2+1})\big).
 \tilde B,\tilde C 
\log\big( \frac{n+1}{\sqrt{n^2+1}}\big)\to -\infty
 n \to \infty \operatorname{Image}(\phi)=\phi(S_n) n n=2 n=80 n=800","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'symmetry', 'hausdorff-distance']"
2,"Comparison of the Bounded Convergence Theorem (BCT), Monotone Convergence Theorem (MCT), and Dominated Convergence Theorem (DCT)","Comparison of the Bounded Convergence Theorem (BCT), Monotone Convergence Theorem (MCT), and Dominated Convergence Theorem (DCT)",,"I'm trying to understand the relationship between the following theorems: Bounded Convergence Theorem (BCT) : For a uniformly bounded sequence $f_n \to f$ a.e. on a set $E$ of finite measure, we have $$ \lim_{n \to \infty} \int_E f_n = \int_E \lim_{n \to \infty} f_n $$ Monotone Convergence Theorem (MCT) : If $f_n \ge 0$ and $f_n \uparrow f$ a.e., then $$ \int f_n \to \int f $$ Dominated Convergence Theorem (DCT) : If $f_n \to f$ a.e., $|f_n| \le g$ , $\int g < \infty$ , then $$ \int \lim f_n = \lim \int f_n $$ I'm trying to understand how these convergence theorems are induced from uniform convergence, Egorov's theorem, and Fatou's lemma. Another topic I want to explore is if these convergence theorems have analogous relationships within sequence continuities (without the integrals), as well as if these convergence theorems involving integrals are subsets/supersets of each other. I would like to find out which cases are more general, and what types of specific cases make one convergence theorem equivalent to another.","I'm trying to understand the relationship between the following theorems: Bounded Convergence Theorem (BCT) : For a uniformly bounded sequence a.e. on a set of finite measure, we have Monotone Convergence Theorem (MCT) : If and a.e., then Dominated Convergence Theorem (DCT) : If a.e., , , then I'm trying to understand how these convergence theorems are induced from uniform convergence, Egorov's theorem, and Fatou's lemma. Another topic I want to explore is if these convergence theorems have analogous relationships within sequence continuities (without the integrals), as well as if these convergence theorems involving integrals are subsets/supersets of each other. I would like to find out which cases are more general, and what types of specific cases make one convergence theorem equivalent to another.",f_n \to f E  \lim_{n \to \infty} \int_E f_n = \int_E \lim_{n \to \infty} f_n  f_n \ge 0 f_n \uparrow f  \int f_n \to \int f  f_n \to f |f_n| \le g \int g < \infty  \int \lim f_n = \lim \int f_n ,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-integral', 'lebesgue-measure']"
3,"Is there a differentiable real function so that no tangent line of $f$ is ""undisturbed"" by other points of $f$ (in some neighbourhood).","Is there a differentiable real function so that no tangent line of  is ""undisturbed"" by other points of  (in some neighbourhood).",f f,"Is there a non-linear differentiable real function $f:\mathbb{R}\to\mathbb{R}$ so that every $x \in \mathbb{R}$ has the following property: in every neighbourhood of $x$ , the tangent line to $f$ at $x$ intersects at least one other (and therefore infinitely many) points of the graph of $f$ ? Or in other words, there is no tangent line of $f$ that is ""undisturbed"" by other points of $f$ in a neighbourhood centred at where the tangent line meets the curve. I was thinking about starting by drawing a sine curve and then drawing a courser sine curve that wraps around the previous sine curve and repeating ad infinitum, but I'm not sure this would work, or if the function would remain everywhere differentiable. Maybe some function to do with the Pompeiu derivative can satisfy the requirements? Edit: Also vaguely relevant: Differentiable function for which the tangent at each point has infinitely many common points with the graph","Is there a non-linear differentiable real function so that every has the following property: in every neighbourhood of , the tangent line to at intersects at least one other (and therefore infinitely many) points of the graph of ? Or in other words, there is no tangent line of that is ""undisturbed"" by other points of in a neighbourhood centred at where the tangent line meets the curve. I was thinking about starting by drawing a sine curve and then drawing a courser sine curve that wraps around the previous sine curve and repeating ad infinitum, but I'm not sure this would work, or if the function would remain everywhere differentiable. Maybe some function to do with the Pompeiu derivative can satisfy the requirements? Edit: Also vaguely relevant: Differentiable function for which the tangent at each point has infinitely many common points with the graph",f:\mathbb{R}\to\mathbb{R} x \in \mathbb{R} x f x f f f,"['real-analysis', 'derivatives', 'graphing-functions']"
4,"Continuity with respect to Hausdorff metric, also with intersection?","Continuity with respect to Hausdorff metric, also with intersection?",,"Let $H$ be a real Hilbert space and let $v\in H$ , $\|v\|=1$ . Now, consider the affine subspaces $A(t) := tv + v^\perp$ of codimension $1$ , $t\in\mathbb R$ . Obviously $$ d_H(A(t),A(s)) = |t-s| $$ for all $t,s\in\mathbb R$ . Hence, $t\mapsto A(t)$ is Lipschitz-continuous with respect to the Hausdorff metric. That was quite easy. However, what I really would like to show is that $f : t\mapsto A(t)\cap B$ is continuous, where $B$ is a bounded, closed, and convex subset of $H$ and $f$ is defined on $K := \{t\in\mathbb R : A(t)\cap B\neq\emptyset\}$ , which is a closed set. I've tried a lot, but I can't seem to prove this. So, I'd be happy if anyone had an idea. For clarification: the Hausdorff distance between two sets $A,B\subset H$ is defined by $$ d_H(A,B) := \max\left\{\sup_{a\in A}\operatorname{dist}(a,B),\,\sup_{b\in B}\operatorname{dist}(b,A)\right\}. $$","Let be a real Hilbert space and let , . Now, consider the affine subspaces of codimension , . Obviously for all . Hence, is Lipschitz-continuous with respect to the Hausdorff metric. That was quite easy. However, what I really would like to show is that is continuous, where is a bounded, closed, and convex subset of and is defined on , which is a closed set. I've tried a lot, but I can't seem to prove this. So, I'd be happy if anyone had an idea. For clarification: the Hausdorff distance between two sets is defined by","H v\in H \|v\|=1 A(t) := tv + v^\perp 1 t\in\mathbb R 
d_H(A(t),A(s)) = |t-s|
 t,s\in\mathbb R t\mapsto A(t) f : t\mapsto A(t)\cap B B H f K := \{t\in\mathbb R : A(t)\cap B\neq\emptyset\} A,B\subset H 
d_H(A,B) := \max\left\{\sup_{a\in A}\operatorname{dist}(a,B),\,\sup_{b\in B}\operatorname{dist}(b,A)\right\}.
","['real-analysis', 'general-topology', 'functional-analysis', 'convex-analysis', 'hausdorff-distance']"
5,$ \sum_{n=1}^{\infty} \frac{f(n)}{n^2} < +\infty$ [duplicate],[duplicate], \sum_{n=1}^{\infty} \frac{f(n)}{n^2} < +\infty,"This question already has answers here : Infinite series, injective function and rearrangement inequality (3 answers) Closed 3 years ago . Let $ f: \mathbb N  \to \mathbb N$ be a bijective function such that: $$ \sum_{n=1}^{\infty} \frac{f(n)}{n^2} < +\infty$$ Now my question is does any such $f$ exists?","This question already has answers here : Infinite series, injective function and rearrangement inequality (3 answers) Closed 3 years ago . Let be a bijective function such that: Now my question is does any such exists?", f: \mathbb N  \to \mathbb N  \sum_{n=1}^{\infty} \frac{f(n)}{n^2} < +\infty f,"['real-analysis', 'sequences-and-series']"
6,Integrability of Fourier Transform of Lipshitz functions,Integrability of Fourier Transform of Lipshitz functions,,"If a function $f$ is Lebesgue integrable and satisfies Lipschitz condition, must its Fourier transform $\hat{f} $ be Lebesgue integrable or not? There's a theorem about Fourier series, saying the Fourier series of periodic Lipschitz function must be absolutely convergent, and I'm just thinking about if it has a similar analog in Fourier transform. I have proven that the improper integral $$\displaystyle{\int _{- \infty }^{\infty }\hat {f} (\xi )e^{2 \pi i \xi x} d \xi }$$ is uniformly convergent for $x\in \mathbb{R}$ . My thought is to find a sequence of Lipschitz integrable functions, with $L^1$ -norm and Lipschitz norm both $\le 1$ , whose Fourier transform have an increasing $L^1$ -norm, then by the closed graph theorem, there must exist an integrable Lipschitz function whose Fourier transform is not integrable. But I find that kind of construction extremely difficult to be accomplished. Anyone has ideas?","If a function is Lebesgue integrable and satisfies Lipschitz condition, must its Fourier transform be Lebesgue integrable or not? There's a theorem about Fourier series, saying the Fourier series of periodic Lipschitz function must be absolutely convergent, and I'm just thinking about if it has a similar analog in Fourier transform. I have proven that the improper integral is uniformly convergent for . My thought is to find a sequence of Lipschitz integrable functions, with -norm and Lipschitz norm both , whose Fourier transform have an increasing -norm, then by the closed graph theorem, there must exist an integrable Lipschitz function whose Fourier transform is not integrable. But I find that kind of construction extremely difficult to be accomplished. Anyone has ideas?",f \hat{f}  \displaystyle{\int _{- \infty }^{\infty }\hat {f} (\xi )e^{2 \pi i \xi x} d \xi } x\in \mathbb{R} L^1 \le 1 L^1,"['real-analysis', 'complex-analysis', 'functional-analysis', 'fourier-analysis', 'fourier-transform']"
7,"Smoothness of quotient of Holder continuous functions, provided the decay","Smoothness of quotient of Holder continuous functions, provided the decay",,"Let $I=(-1,1)$ and $u \in \text{Lips}(I)$ , the space of Lipschitz continuous functions in I. Suppose that $$ |u(x)|  \le C |x|^\alpha $$ for $x \in I$ , for some $\alpha \in (1,2)$ . I would like to estimate the Holder continuity of $$ v(x)= \begin{cases} |x|^{-\beta}\cdot u(x) &\text{ for } x \neq 0 \\ 0& \text{ for } x =0 \end{cases} $$ for some $1<\beta<\alpha$ (so that $|x|^{\beta}$ is also in $\text{Lips}(I)$ ) in the whole interval $I$ . It is clear that $v$ is in $\text{Lips}(I \setminus (-\varepsilon,\varepsilon))$ (and therefore $C^{\alpha-\beta}(I\setminus (-\varepsilon,\varepsilon))$ ) for any $\varepsilon \in (0,1)$ . and We also know that $v$ is ""Holder continuous at $0$ "", that is, $|v(x)-v(0)| \le C |x|^{\alpha-\beta}$ . Is it true that $v$ is Holder of order $\alpha - \beta$ ? The problem, of course, is to check if the Holder condition holds over sequences $\{x_n\}_n$ and $\{y_n\}_n$ such that $|x_n-y_n|\ll |y_n| \longrightarrow 0$ . I managed to prove that $v$ is in $C^{1-\beta/\alpha}(I)$ . Without further hypothesis, is this the best Holder smoothness $v$ can have in $I$ ? The choices of $\alpha$ and $\beta$ seemed to be a convenient regime to start, but the question could be done in a more general setting. EDIT Let me put my proof of the Holder regularity $1- \beta/\alpha$ . Assum w.l.o.g that $|x|<|y|$ . We have from hypothesis that $$ |u(x)-u(y)| \le L |x-y| $$ and for some constant $L>0$ and that $$ |u(x)-u(y)| \le C (|x|^\alpha+|y|^\alpha) \le 2C |y|^\alpha.  $$ thefore, writing $|u(x)-u(y)|=|u(x)-u(y)|^{\delta}|u(x)-u(y)|^{1-\delta}$ , we have $$ \tag{1} |u(x)-u(y)| \le C^\prime |x-y|^{(1-\delta)}|y|^{\delta\alpha}. $$ Now, we write $$ \left | \frac{u(x)}{|x|^\beta}-\frac{u(y)}{|y|^\beta} \right| = \left | \frac{u(x)}{|x|^\beta} \frac{u(x)-u(y)}{|y|^\beta}-\frac{u(x)-u(y)}{|y|^\beta} \right|. $$ Using the triangular inequality and $(1)$ and the fact that $|x|^\alpha$ is Lipchitz, we have \begin{align*} \left | \frac{u(x)}{|x|^\beta}-\frac{u(y)}{|y|^\beta} \right| & \le C^{\prime \prime} \left [ |x|^{\alpha-\beta} |x-y| +|y|^{\delta\alpha-\beta}|x-y|^{1-\delta} \right] \end{align*} We then choose $\delta = \beta/\alpha$ , and we get that the RHS is bounded by $C^{\prime \prime}|x-y|^{1-\beta/\alpha}$ .","Let and , the space of Lipschitz continuous functions in I. Suppose that for , for some . I would like to estimate the Holder continuity of for some (so that is also in ) in the whole interval . It is clear that is in (and therefore ) for any . and We also know that is ""Holder continuous at "", that is, . Is it true that is Holder of order ? The problem, of course, is to check if the Holder condition holds over sequences and such that . I managed to prove that is in . Without further hypothesis, is this the best Holder smoothness can have in ? The choices of and seemed to be a convenient regime to start, but the question could be done in a more general setting. EDIT Let me put my proof of the Holder regularity . Assum w.l.o.g that . We have from hypothesis that and for some constant and that thefore, writing , we have Now, we write Using the triangular inequality and and the fact that is Lipchitz, we have We then choose , and we get that the RHS is bounded by .","I=(-1,1) u \in \text{Lips}(I) 
|u(x)|  \le C |x|^\alpha
 x \in I \alpha \in (1,2) 
v(x)=
\begin{cases}
|x|^{-\beta}\cdot u(x) &\text{ for } x \neq 0
\\
0& \text{ for } x =0
\end{cases}
 1<\beta<\alpha |x|^{\beta} \text{Lips}(I) I v \text{Lips}(I \setminus (-\varepsilon,\varepsilon)) C^{\alpha-\beta}(I\setminus (-\varepsilon,\varepsilon)) \varepsilon \in (0,1) v 0 |v(x)-v(0)| \le C |x|^{\alpha-\beta} v \alpha - \beta \{x_n\}_n \{y_n\}_n |x_n-y_n|\ll |y_n| \longrightarrow 0 v C^{1-\beta/\alpha}(I) v I \alpha \beta 1- \beta/\alpha |x|<|y| 
|u(x)-u(y)| \le L |x-y|
 L>0 
|u(x)-u(y)| \le C (|x|^\alpha+|y|^\alpha) \le 2C |y|^\alpha. 
 |u(x)-u(y)|=|u(x)-u(y)|^{\delta}|u(x)-u(y)|^{1-\delta} 
\tag{1}
|u(x)-u(y)| \le C^\prime |x-y|^{(1-\delta)}|y|^{\delta\alpha}.
 
\left | \frac{u(x)}{|x|^\beta}-\frac{u(y)}{|y|^\beta} \right|
=
\left | \frac{u(x)}{|x|^\beta} \frac{u(x)-u(y)}{|y|^\beta}-\frac{u(x)-u(y)}{|y|^\beta} \right|.
 (1) |x|^\alpha \begin{align*}
\left | \frac{u(x)}{|x|^\beta}-\frac{u(y)}{|y|^\beta} \right|
&
\le C^{\prime \prime} \left [
|x|^{\alpha-\beta} |x-y|
+|y|^{\delta\alpha-\beta}|x-y|^{1-\delta}
\right]
\end{align*} \delta = \beta/\alpha C^{\prime \prime}|x-y|^{1-\beta/\alpha}","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'holder-spaces']"
8,The interior of $\{f \leq 0\}$ is $\{ f < 0\}$,The interior of  is,\{f \leq 0\} \{ f < 0\},"Let $X\subseteq \mathbb{R}^n$ , $f: X \rightarrow \mathbb{R}$ : what further conditions on $X$ and $f$ do we need to ensure that $\text{Int}\{x|f(x)\leq 0\}=\{x|f(x)<0\}$ ? Context : I've encountered this kind of question when dealing with open bounded $X$ with $C^1$ boundary, in partial differential equations, but I'm sure that one could benefit from such a fact in optimization, for instance. Clearly, continuity is not enough, since $f:[0,1]\rightarrow \mathbb{R} , f(x)=0$ is such that $[0,1]=\{f\leq 0\}$ , but the interior of this set is not the empty set.","Let , : what further conditions on and do we need to ensure that ? Context : I've encountered this kind of question when dealing with open bounded with boundary, in partial differential equations, but I'm sure that one could benefit from such a fact in optimization, for instance. Clearly, continuity is not enough, since is such that , but the interior of this set is not the empty set.","X\subseteq \mathbb{R}^n f: X \rightarrow \mathbb{R} X f \text{Int}\{x|f(x)\leq 0\}=\{x|f(x)<0\} X C^1 f:[0,1]\rightarrow \mathbb{R} , f(x)=0 [0,1]=\{f\leq 0\}","['real-analysis', 'general-topology']"
9,Finding solution of the PDE : An Initial value problem,Finding solution of the PDE : An Initial value problem,,"I've been given a PDE of the form $$xu_x+(x^2+y)u_y=1-\left(\frac{y}{x}-x\right)u~~; ~~u(1,y)=0$$ Attempt : Firstly it's a first order linear PDE which has the general form $$a(x,y)u_x+b(x,y)u_y=c(x,y)u+d(x,y)$$ Where $a,b,c,d \in \mathcal{C}^1(\Omega)$ , where $\Omega \subseteq \mathbb{R}^2$ (open and connected) and we also have $a^2+b^2 \neq 0$ on $\Omega_2$ . Then first of all the largest $\Omega_2$ on which $F:=a^2+b^2=x^2+(x^2+y)\neq 0$ is : $(x,y) \in \Omega_2\setminus \{(0,0)\}$ right? We have $s \mapsto \Gamma_s:=(f(s_0):=1,g(s_0):=s,h(s_0):=0))$ . Now, from transversality conditions we get : $$J:=\mathrm{det}\, \begin{pmatrix}a&f'\\b&g'\end{pmatrix}\Bigg{|}_{f(s_0),g(s_0),h(s_0)}=\mathrm{det}\begin{pmatrix}1&0\\1+s&1\end{pmatrix}=1$$ So we atleast expect solutions to exist $\textit{locally}$ for all $s \in (-\delta,\delta),\delta>0$ . Now to find the solutions explicitly i'll employ the method of characteristics: \begin{align}\frac{\mathrm{d}x}{\mathrm{d}t}&=x~~;~x(0,s):=1 \implies x(t,s):=e^t\\\frac{\mathrm{d}y}{\mathrm{d}t}&=x^2+y~~;~ y(0,s):=s \\&=y+e^{2t} \end{align} Solving this we get \begin{align}   y(t,s)&=e^{2t}+c_2e^{t}~~;~y(0,s):=s \implies  y(t,s)=e^{t}\left(e^t+s-1\right)\end{align} Lastly \begin{align}\frac{\mathrm{d}z}{\mathrm{d}t}&=1-\left(\frac{y}{x}-x\right)z~~;~z(0,s):=0 \\&=1-\left(\frac{e^t(e^t+s-1)}{e^t}-e^t\right)z=1+(1-s)z\end{align} Which gives me $$z(t,s)=\frac{1}{s-1}\left(1-e^{t(1-s)}\right)$$ As the Jacobian was non-zero by inverse function theorem i can expect that i can invert the map $$(t,s)\mapsto \left(x(t,s),y(t,s)\right)$$ and i can solve $x=x(t,s),y=y(t,s)$ uniquely for $t$ and $s$ and the map $$(x,y)\mapsto \left(T(x,y);S(x,y)\right)$$ should be of class $\mathcal{C}^1$ . So i have $x(t,s)=e^t \implies t=\ln\,x=:T(x,y)$ and $y(t,s)=e^{\ln\,x}\left(e^{\ln \,x}+s-1\right)=x(x+s-1)$ i.e $s=(y/x)-x+1=:S(x,y)$ . From here i can write $$u=z(T(x,y),S(x,y))=\frac{x}{y-x^2}\left(1-x^{\frac{x^2-y}{x}}\right)$$ Which is valid when $y>x^2$ . I'm not sure about my workings here, appreciate any hints and help. Thanks.","I've been given a PDE of the form Attempt : Firstly it's a first order linear PDE which has the general form Where , where (open and connected) and we also have on . Then first of all the largest on which is : right? We have . Now, from transversality conditions we get : So we atleast expect solutions to exist for all . Now to find the solutions explicitly i'll employ the method of characteristics: Solving this we get Lastly Which gives me As the Jacobian was non-zero by inverse function theorem i can expect that i can invert the map and i can solve uniquely for and and the map should be of class . So i have and i.e . From here i can write Which is valid when . I'm not sure about my workings here, appreciate any hints and help. Thanks.","xu_x+(x^2+y)u_y=1-\left(\frac{y}{x}-x\right)u~~; ~~u(1,y)=0 a(x,y)u_x+b(x,y)u_y=c(x,y)u+d(x,y) a,b,c,d \in \mathcal{C}^1(\Omega) \Omega \subseteq \mathbb{R}^2 a^2+b^2 \neq 0 \Omega_2 \Omega_2 F:=a^2+b^2=x^2+(x^2+y)\neq 0 (x,y) \in \Omega_2\setminus \{(0,0)\} s \mapsto \Gamma_s:=(f(s_0):=1,g(s_0):=s,h(s_0):=0)) J:=\mathrm{det}\, \begin{pmatrix}a&f'\\b&g'\end{pmatrix}\Bigg{|}_{f(s_0),g(s_0),h(s_0)}=\mathrm{det}\begin{pmatrix}1&0\\1+s&1\end{pmatrix}=1 \textit{locally} s \in (-\delta,\delta),\delta>0 \begin{align}\frac{\mathrm{d}x}{\mathrm{d}t}&=x~~;~x(0,s):=1 \implies x(t,s):=e^t\\\frac{\mathrm{d}y}{\mathrm{d}t}&=x^2+y~~;~ y(0,s):=s \\&=y+e^{2t} \end{align} \begin{align}   y(t,s)&=e^{2t}+c_2e^{t}~~;~y(0,s):=s \implies 
y(t,s)=e^{t}\left(e^t+s-1\right)\end{align} \begin{align}\frac{\mathrm{d}z}{\mathrm{d}t}&=1-\left(\frac{y}{x}-x\right)z~~;~z(0,s):=0 \\&=1-\left(\frac{e^t(e^t+s-1)}{e^t}-e^t\right)z=1+(1-s)z\end{align} z(t,s)=\frac{1}{s-1}\left(1-e^{t(1-s)}\right) (t,s)\mapsto \left(x(t,s),y(t,s)\right) x=x(t,s),y=y(t,s) t s (x,y)\mapsto \left(T(x,y);S(x,y)\right) \mathcal{C}^1 x(t,s)=e^t \implies t=\ln\,x=:T(x,y) y(t,s)=e^{\ln\,x}\left(e^{\ln \,x}+s-1\right)=x(x+s-1) s=(y/x)-x+1=:S(x,y) u=z(T(x,y),S(x,y))=\frac{x}{y-x^2}\left(1-x^{\frac{x^2-y}{x}}\right) y>x^2","['real-analysis', 'partial-differential-equations', 'solution-verification']"
10,"If the difference quotient $\frac{f(y)-f(x)}{y-x}$ has a limit along a line $(x,y)\to(c,c)$, does the ordinary derivative $f'(c)$ exist?","If the difference quotient  has a limit along a line , does the ordinary derivative  exist?","\frac{f(y)-f(x)}{y-x} (x,y)\to(c,c) f'(c)","Given a function $f:\mathbb R\to\mathbb R$ , we define the difference quotient function $$q(x,y)=\frac{f(y)-f(x)}{y-x}$$ for all $(x,y)\in\mathbb R^2$ not on the diagonal line $x=y$ . The ordinary derivative $f'(c)$ is defined as a limit of $q$ along a horizontal ( $y=c$ ) or vertical ( $x=c$ ) line through $(c,c)$ . The symmetric derivative is a limit along a diagonal line $y-c=c-x$ . The left derivative is a limit along a horizontal ray $y=c,\,x<c$ . The right derivative is a limit along a vertical ray $x=c,\,y>c$ . The strong derivative is the limit of $q$ at $(c,c)$ , not along any particular path. If the ordinary derivative exists, then $q(x,y)\to f'(c)$ along any line through $(c,c)$ , or in any region (a ""cone"") separated from the diagonal by lines through $(c,c)$ : $$q(x,y)=\frac{y-c}{y-x}\cdot\frac{f(y)-f(c)}{y-c}+\frac{c-x}{y-x}\cdot\frac{f(c)-f(x)}{c-x}$$ $$=\frac{y-c}{y-x}\cdot q(c,y)+\frac{c-x}{y-x}\cdot q(x,c).$$ Note that the coefficients sum to $1$ , and we're given $q(c,x)-f'(c)\to0$ as $x\to c$ , so $$q(x,y)-f'(c)=\frac{y-c}{y-x}\big(q(c,y)-f'(c)\big)+\frac{c-x}{y-x}\big(q(x,c)-f'(c)\big)$$ $$\to0,$$ provided that the coefficients $\frac{y-c}{y-x}$ and $\frac{c-x}{y-x}$ are bounded. If $q$ has a limit $f^*(c)$ along a line, say $y-c=k(x-c)$ with $0\neq|k|\neq1$ , does the derivative exist? To simplify the notation, let's assume $c=f(c)=f^*(c)=0$ . We're given, as $x\to0$ , $$\frac{f(kx)-f(x)}{kx-x}=\frac{1}{k-1}\cdot\frac{f(kx)-f(x)}{x}\to0,$$ and we want to know whether $\frac{f(x)}{x}\to0$ . There are discontinuous counter-examples: Let $f(k^n)=1$ for $n\in\mathbb Z$ , and otherwise $f(x)=0$ ; then $q(x,kx)=0\to0$ , but $q(0,x)\not\to0$ . So let's assume that $f$ is continuous at $c$ , and maybe in a neighbourhood of $c$ . Since $q(x,y)=q(y,x)$ is symmetric, the limit along a line with slope $k$ is the same as with slope $1/k$ . So, without loss of generality, $0<|k|<1$ . If the limit is $0$ for two lines with slopes $k$ and $l$ , then it's also $0$ for a line with slope $k\cdot l$ : $$\lim_{x\to0}\frac{f(klx)-f(x)}{x}=\lim_{x\to0}\frac{f(klx)-f(lx)+f(lx)-f(x)}{x}$$ $$=l\cdot\lim_{lx\to0}\frac{f(klx)-f(lx)}{lx}+\lim_{x\to0}\frac{f(lx)-f(x)}{x}=0.$$ Thus, the limit is $0$ for any line with slope $k^n$ where $n\in\mathbb N$ . Now the derivative is $$f'(0)=\lim_{x\to0}\frac{f(x)-f(0)}{x}$$ $$=\lim_{x\to0}\frac{f(x)-f(\lim_{n\to\infty} k^nx)}{x}$$ and we assumed that $f$ is continuous at $0$ : $$=\lim_{x\to0}\frac{f(x)-\lim_{n\to\infty}f(k^nx)}{x}$$ $$=\lim_{x\to0}\lim_{n\to\infty}\frac{f(x)-f(k^nx)}{x}$$ $$\overset?=\lim_{n\to\infty}\lim_{x\to0}\frac{f(x)-f(k^nx)}{x}$$ $$=\lim_{n\to\infty}(0)=0.$$ Is swapping the limits valid here?","Given a function , we define the difference quotient function for all not on the diagonal line . The ordinary derivative is defined as a limit of along a horizontal ( ) or vertical ( ) line through . The symmetric derivative is a limit along a diagonal line . The left derivative is a limit along a horizontal ray . The right derivative is a limit along a vertical ray . The strong derivative is the limit of at , not along any particular path. If the ordinary derivative exists, then along any line through , or in any region (a ""cone"") separated from the diagonal by lines through : Note that the coefficients sum to , and we're given as , so provided that the coefficients and are bounded. If has a limit along a line, say with , does the derivative exist? To simplify the notation, let's assume . We're given, as , and we want to know whether . There are discontinuous counter-examples: Let for , and otherwise ; then , but . So let's assume that is continuous at , and maybe in a neighbourhood of . Since is symmetric, the limit along a line with slope is the same as with slope . So, without loss of generality, . If the limit is for two lines with slopes and , then it's also for a line with slope : Thus, the limit is for any line with slope where . Now the derivative is and we assumed that is continuous at : Is swapping the limits valid here?","f:\mathbb R\to\mathbb R q(x,y)=\frac{f(y)-f(x)}{y-x} (x,y)\in\mathbb R^2 x=y f'(c) q y=c x=c (c,c) y-c=c-x y=c,\,x<c x=c,\,y>c q (c,c) q(x,y)\to f'(c) (c,c) (c,c) q(x,y)=\frac{y-c}{y-x}\cdot\frac{f(y)-f(c)}{y-c}+\frac{c-x}{y-x}\cdot\frac{f(c)-f(x)}{c-x} =\frac{y-c}{y-x}\cdot q(c,y)+\frac{c-x}{y-x}\cdot q(x,c). 1 q(c,x)-f'(c)\to0 x\to c q(x,y)-f'(c)=\frac{y-c}{y-x}\big(q(c,y)-f'(c)\big)+\frac{c-x}{y-x}\big(q(x,c)-f'(c)\big) \to0, \frac{y-c}{y-x} \frac{c-x}{y-x} q f^*(c) y-c=k(x-c) 0\neq|k|\neq1 c=f(c)=f^*(c)=0 x\to0 \frac{f(kx)-f(x)}{kx-x}=\frac{1}{k-1}\cdot\frac{f(kx)-f(x)}{x}\to0, \frac{f(x)}{x}\to0 f(k^n)=1 n\in\mathbb Z f(x)=0 q(x,kx)=0\to0 q(0,x)\not\to0 f c c q(x,y)=q(y,x) k 1/k 0<|k|<1 0 k l 0 k\cdot l \lim_{x\to0}\frac{f(klx)-f(x)}{x}=\lim_{x\to0}\frac{f(klx)-f(lx)+f(lx)-f(x)}{x} =l\cdot\lim_{lx\to0}\frac{f(klx)-f(lx)}{lx}+\lim_{x\to0}\frac{f(lx)-f(x)}{x}=0. 0 k^n n\in\mathbb N f'(0)=\lim_{x\to0}\frac{f(x)-f(0)}{x} =\lim_{x\to0}\frac{f(x)-f(\lim_{n\to\infty} k^nx)}{x} f 0 =\lim_{x\to0}\frac{f(x)-\lim_{n\to\infty}f(k^nx)}{x} =\lim_{x\to0}\lim_{n\to\infty}\frac{f(x)-f(k^nx)}{x} \overset?=\lim_{n\to\infty}\lim_{x\to0}\frac{f(x)-f(k^nx)}{x} =\lim_{n\to\infty}(0)=0.","['real-analysis', 'calculus', 'limits', 'derivatives']"
11,Does $L^p(\Bbb R^n)$ have Schauder basis?,Does  have Schauder basis?,L^p(\Bbb R^n),"I can only find this result for compact subsets for some reason, but it should be true. Does $L^p(\Bbb R^n)$ have Schauder basis?","I can only find this result for compact subsets for some reason, but it should be true. Does have Schauder basis?",L^p(\Bbb R^n),['real-analysis']
12,"Find all strictly monotone $f:(0,+\infty) \to (0, +\infty)$ such that $f(\frac{x^2}{f(x)})=x.$",Find all strictly monotone  such that,"f:(0,+\infty) \to (0, +\infty) f(\frac{x^2}{f(x)})=x.","Find all strictly monotone functions $f:(0,+\infty) \to (0,+\infty)$ such that $$f\left(\frac{x^2}{f(x)}\right)=x.$$ My try: it is clear that $f$ is surjective. And because it is monotone it must also be injective. Therefore we can take $f^{-1}$ from both sides: $x^2 = f(x) \cdot f^{-1}(x)$ . We can take $x = f(y)$ (because of surjectivity) and get that: $\frac{f(y)}{y} = \frac{f(f(y))}{f(y)}$ . So, if we define $g(x) = \frac{f(x)}{x}$ we have that $g(y) = g\big(f(y)\big)$ and I was hoping to prove that $g$ is injective so we would have $f(x) = x$ only. But I couldn't figure that last step. There may be a better way to deal with this problem. EDIT: There is another solution on AOPS, problem 312 .","Find all strictly monotone functions such that My try: it is clear that is surjective. And because it is monotone it must also be injective. Therefore we can take from both sides: . We can take (because of surjectivity) and get that: . So, if we define we have that and I was hoping to prove that is injective so we would have only. But I couldn't figure that last step. There may be a better way to deal with this problem. EDIT: There is another solution on AOPS, problem 312 .","f:(0,+\infty) \to (0,+\infty) f\left(\frac{x^2}{f(x)}\right)=x. f f^{-1} x^2 = f(x) \cdot f^{-1}(x) x = f(y) \frac{f(y)}{y} = \frac{f(f(y))}{f(y)} g(x) = \frac{f(x)}{x} g(y) = g\big(f(y)\big) g f(x) = x","['real-analysis', 'functions', 'functional-equations']"
13,"If $f : \mathbb R \to [-2 , 2]$ with $(f(0))^2 + (f'(0))^2 =85$, then there exists $x \in (-4 , 4)$ such that $f(x) +f''(x) = 0$ and $f'(x) \neq 0$.","If  with , then there exists  such that  and .","f : \mathbb R \to [-2 , 2] (f(0))^2 + (f'(0))^2 =85 x \in (-4 , 4) f(x) +f''(x) = 0 f'(x) \neq 0","For every twice differentiable function $f : \mathbb R \to [-2 , 2]$ with $(f(0))^2 + (f'(0))^2 =85$ there exists $x \in (-4 , 4)$ such that $f(x) +f''(x) = 0$ and $f'(x) \neq 0$ . I was trying to get the answer by constructing a function $g(x)  = f(x) ^2  + f'(x) ^2$ . But I can not proceed much. Can anyone help me?",For every twice differentiable function with there exists such that and . I was trying to get the answer by constructing a function . But I can not proceed much. Can anyone help me?,"f : \mathbb R \to [-2 , 2] (f(0))^2 + (f'(0))^2 =85 x \in (-4 , 4) f(x) +f''(x) = 0 f'(x) \neq 0 g(x)  = f(x) ^2  + f'(x) ^2","['real-analysis', 'calculus', 'derivatives']"
14,Bound on remainder for vector valued Taylor series,Bound on remainder for vector valued Taylor series,,"Consider an $n$ -times differentiable map $f$ between finite dimensional real vector spaces. Specifically, let $f$ be defined on an open subset $U$ . Suppose the closed interval $[x,x+h]\subset U$ is entirely contained in $U$ and that the $(n+1)^\text{th}$ derivative exists on the interior $(x,x+h)$ of the interval. Write $M=\sup_{(x,x+h)}\|f^{(n+1)}(c)\|$ . I would like to prove the remainder of order $n$ satisfies the inequality $$\|R_nf(x)\|\leq\frac{M}{(n+1)!}\|h\|^{n+1}.$$ The only thing I can think of is precomposing with the straight path from $x$ to $x+h$ and then choosing coordinates on the target space to apply the mean value theorem in each coordinate. However, this gives an estimate in terms of the operator norms of the coordinates of the $(n+1)^\text{th}$ derivative which is looser than the ""total"" operator norm. I probed a bit online and found the following ""guide"". Consider the comopsite $f(x+th)$ with $t\in [0,1]$ . Write $g_n(t)$ for the $n^\text{th}$ order remainder for $f(x+th)$ . Prove the estimate for every $t\in (0,1)$ $$\|g_n^\prime(t)\|\leq \frac{1}{n!}M\|th\|^n\|h\|.$$ Use it to deduce $\|g_n(1)-g_n(0)\|\leq \frac{1}{(n+1)!}M\|h\|^{n+1}$ as desired. Given the estimate on $g^\prime _n$ I know how to get the second inequality. However, I don't see how to prove this first estimate. Induction does not seem to help.","Consider an -times differentiable map between finite dimensional real vector spaces. Specifically, let be defined on an open subset . Suppose the closed interval is entirely contained in and that the derivative exists on the interior of the interval. Write . I would like to prove the remainder of order satisfies the inequality The only thing I can think of is precomposing with the straight path from to and then choosing coordinates on the target space to apply the mean value theorem in each coordinate. However, this gives an estimate in terms of the operator norms of the coordinates of the derivative which is looser than the ""total"" operator norm. I probed a bit online and found the following ""guide"". Consider the comopsite with . Write for the order remainder for . Prove the estimate for every Use it to deduce as desired. Given the estimate on I know how to get the second inequality. However, I don't see how to prove this first estimate. Induction does not seem to help.","n f f U [x,x+h]\subset U U (n+1)^\text{th} (x,x+h) M=\sup_{(x,x+h)}\|f^{(n+1)}(c)\| n \|R_nf(x)\|\leq\frac{M}{(n+1)!}\|h\|^{n+1}. x x+h (n+1)^\text{th} f(x+th) t\in [0,1] g_n(t) n^\text{th} f(x+th) t\in (0,1) \|g_n^\prime(t)\|\leq \frac{1}{n!}M\|th\|^n\|h\|. \|g_n(1)-g_n(0)\|\leq \frac{1}{(n+1)!}M\|h\|^{n+1} g^\prime _n","['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
15,$\frac{a_{max}}{\sum a_i} \to 0$: Proof or Counterexample,: Proof or Counterexample,\frac{a_{max}}{\sum a_i} \to 0,"Suppose I have a sequence of positive integers $\{a_n\}$ . Let us denote $b_n=\max_{1\le i\le n} a_i$ . Suppose $$\frac{b_n}{\sum\limits_{i=1}^n a_i} \to 0$$ then show that $$\frac{b_n^2}{\sum\limits_{i=1}^n a_i^2} \to 0$$ I am not sure if it is true. But I didn't find any Counterexample. I was trying to get a reasonable lower bound for the denominator. I could not find any. Bounds like $$\sum_{i=1}^n a_i^2 \ge \sum_{i=1}^n a_i$$ won't help though. Note that the converse is true. As you can easily get an upper bound using: $$\sum_{i=1} a_i^2 \le b_n\sum_{i=1} a_i$$ Any help/suggestions? Edit: Note that $a_n$ 's are positive integers, that's why $\sum a_i^2 \ge \sum a_i$ is true.","Suppose I have a sequence of positive integers . Let us denote . Suppose then show that I am not sure if it is true. But I didn't find any Counterexample. I was trying to get a reasonable lower bound for the denominator. I could not find any. Bounds like won't help though. Note that the converse is true. As you can easily get an upper bound using: Any help/suggestions? Edit: Note that 's are positive integers, that's why is true.",\{a_n\} b_n=\max_{1\le i\le n} a_i \frac{b_n}{\sum\limits_{i=1}^n a_i} \to 0 \frac{b_n^2}{\sum\limits_{i=1}^n a_i^2} \to 0 \sum_{i=1}^n a_i^2 \ge \sum_{i=1}^n a_i \sum_{i=1} a_i^2 \le b_n\sum_{i=1} a_i a_n \sum a_i^2 \ge \sum a_i,"['real-analysis', 'sequences-and-series', 'number-theory', 'limits', 'analysis']"
16,How to develop $\frac{1}{1-x}$ into $1+\frac{x}{1+x}+\frac{1\cdot2x^2}{(1+x)(1+2x)}+\dots$,How to develop  into,\frac{1}{1-x} 1+\frac{x}{1+x}+\frac{1\cdot2x^2}{(1+x)(1+2x)}+\dots,"I know that, by long division, or binomial formula or Taylor formula that this function can be developed into the geometric series: $1/(1-x)=1+x+x^2+x^3+x^4+\ldots $ and I thought that this is the only series that represent the function mentioned above. However, recently, I read ""Traite Elementaire des Series"" by Eugene Catalan and he said, on page 60, that ""The same function can admit multiple expansions"". He gives the series below: $$\frac{1}{1-x}=1+\frac{x}{1+x}+\frac{1\cdot2x^2}{(1+x)(1+2x)}+\frac{1\cdot2\cdot3x^3}{(1+x)(1+2x)(1+3x)}+\dots$$ I am just curious how can we derive this second series. So a function may admit multiple series representation? This is something new that I don't know. Maybe a better way to think about this is that two or multiple series can converge to the same function, which is the same as multiple numerical series may converge to the same sum.","I know that, by long division, or binomial formula or Taylor formula that this function can be developed into the geometric series: and I thought that this is the only series that represent the function mentioned above. However, recently, I read ""Traite Elementaire des Series"" by Eugene Catalan and he said, on page 60, that ""The same function can admit multiple expansions"". He gives the series below: I am just curious how can we derive this second series. So a function may admit multiple series representation? This is something new that I don't know. Maybe a better way to think about this is that two or multiple series can converge to the same function, which is the same as multiple numerical series may converge to the same sum.",1/(1-x)=1+x+x^2+x^3+x^4+\ldots  \frac{1}{1-x}=1+\frac{x}{1+x}+\frac{1\cdot2x^2}{(1+x)(1+2x)}+\frac{1\cdot2\cdot3x^3}{(1+x)(1+2x)(1+3x)}+\dots,"['real-analysis', 'calculus', 'power-series', 'taylor-expansion']"
17,Proving following holds for almost everywhere,Proving following holds for almost everywhere,,"I am studying real analysis and encountered this problem. Prove that for almost everywhere $x\in\mathbb{R}$ , $\lim_{n\rightarrow\infty}|\cos{nx}|^{\frac{1}{n}}=1$ . What theorem can I use to solve this problem? I don't know how to start. Thanks.","I am studying real analysis and encountered this problem. Prove that for almost everywhere , . What theorem can I use to solve this problem? I don't know how to start. Thanks.",x\in\mathbb{R} \lim_{n\rightarrow\infty}|\cos{nx}|^{\frac{1}{n}}=1,[]
18,How to think intuitively about compact injections?,How to think intuitively about compact injections?,,"Motivation is the Sobolev inequalities, but there are more basic examples. For example, consider the inclusion $$i: C^1([0,1]) \rightarrow C([0,1])$$ This is compact since if $\{f_n\} \subset B_1(0,1)$ , then for all $x, y \in [0,1]$ we have $$|f_n(x) - f_n(y)| \leq |f_n'|_{\infty}|x - y| \leq |x - y|$$ by the mean value theorem. So $\{i(f_n)\}$ is uniformly bounded and equicontinous, hence has a convergent subsequence. How should one think of a compact injection intuitively or informally?","Motivation is the Sobolev inequalities, but there are more basic examples. For example, consider the inclusion This is compact since if , then for all we have by the mean value theorem. So is uniformly bounded and equicontinous, hence has a convergent subsequence. How should one think of a compact injection intuitively or informally?","i: C^1([0,1]) \rightarrow C([0,1]) \{f_n\} \subset B_1(0,1) x, y \in [0,1] |f_n(x) - f_n(y)| \leq |f_n'|_{\infty}|x - y| \leq |x - y| \{i(f_n)\}","['real-analysis', 'soft-question', 'banach-spaces', 'sobolev-spaces']"
19,"""Baby"" Rudin theorem 8.5: is this proof correct?","""Baby"" Rudin theorem 8.5: is this proof correct?",,"Theorem 8.5 of ""Baby"" Rudin is the following: Suppose the series $\sum a_{n} x^{n} $ and $ \sum b_{n} x^{n} $ converge in the segment $S=(-R, R)$ . Let $E$ be the set of all $x \in S$ at which $\sum_{n=0}^{\infty} a_n x^n = \sum_{n=0}^{\infty} b_n x^n $ If $E$ has a limit point in $S$ , then $a_n = b_n $ for $n=0,1,2,\cdots$ . And here is my proof, which I want to get verified. Put $c_n = a_n - b_n $ , then $f(x) = \sum c_n x^n =0$ for all $x \in E $ Let $t$ be a limit point of $E$ . Then there is a sequence in $E$ which converges to $t$ , and there exist a subsequence of it which consists of numbers bigger than $t$ or smaller than $t$ . wlog assume there is a increasing sequence $ \{ t_n \} $ which converges to $t$ and $t_n < t$ for all $n$ Then we have $f(t_n )=0$ for all $n$ . Since $f$ is continuous on $(-R, R)$ , we have $f(t)=0$ . (Note that $t \in S$ , so $f(t)$ can be defined.) Also, since $f$ is differentiable on $(-R, R)$ , by mean value theorem, there exists a sequence $s_n $ such that $t_{n} < s_n < t_{n+1} $ and $f'(s_n )=0$ for all $n$ . It is obvious that $s_n$ converges to $t$ , and since $f'$ is continuous on $(-R, R)$ too, so we have $f'(t)=0$ . We can repeat this, (rigorous proof can done by induction) to get $f^{(n)}(t)=0$ for all $n$ . Thus by Theorem 8.4 (which I state at the end of this question), $f(x)=0$ for $x \in ( t-\epsilon , t+\epsilon ) \subset (-R, R) $ , where $\epsilon >0$ is arbitrary. Now we can repeat this stuff to get $f(x) =0$ for all $x \in (-R,R)$ . So $f^{(n)}(0)=0$ for all $n$ , which gives $c_n =0$ for all $n$ . Here is Theorem 8.4, which I used above. Suppose $f(x) = \sum_{n=0}^{\infty} c_{n} x^{n} $ , the series converging in $|x|<R$ . If $-R < a < R$ , then $f$ can be expanded in a power series about the point $x=a$ which converges in $|x-a| < R-|a|$ , and $f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^{n} $ Thank you for reading my question. Any commnets will help me a lot.","Theorem 8.5 of ""Baby"" Rudin is the following: Suppose the series and converge in the segment . Let be the set of all at which If has a limit point in , then for . And here is my proof, which I want to get verified. Put , then for all Let be a limit point of . Then there is a sequence in which converges to , and there exist a subsequence of it which consists of numbers bigger than or smaller than . wlog assume there is a increasing sequence which converges to and for all Then we have for all . Since is continuous on , we have . (Note that , so can be defined.) Also, since is differentiable on , by mean value theorem, there exists a sequence such that and for all . It is obvious that converges to , and since is continuous on too, so we have . We can repeat this, (rigorous proof can done by induction) to get for all . Thus by Theorem 8.4 (which I state at the end of this question), for , where is arbitrary. Now we can repeat this stuff to get for all . So for all , which gives for all . Here is Theorem 8.4, which I used above. Suppose , the series converging in . If , then can be expanded in a power series about the point which converges in , and Thank you for reading my question. Any commnets will help me a lot.","\sum a_{n} x^{n}   \sum b_{n} x^{n}  S=(-R, R) E x \in S \sum_{n=0}^{\infty} a_n x^n = \sum_{n=0}^{\infty} b_n x^n  E S a_n = b_n  n=0,1,2,\cdots c_n = a_n - b_n  f(x) = \sum c_n x^n =0 x \in E  t E E t t t  \{ t_n \}  t t_n < t n f(t_n )=0 n f (-R, R) f(t)=0 t \in S f(t) f (-R, R) s_n  t_{n} < s_n < t_{n+1}  f'(s_n )=0 n s_n t f' (-R, R) f'(t)=0 f^{(n)}(t)=0 n f(x)=0 x \in ( t-\epsilon , t+\epsilon ) \subset (-R, R)  \epsilon >0 f(x) =0 x \in (-R,R) f^{(n)}(0)=0 n c_n =0 n f(x) = \sum_{n=0}^{\infty} c_{n} x^{n}  |x|<R -R < a < R f x=a |x-a| < R-|a| f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^{n} ","['real-analysis', 'proof-verification']"
20,Example of a continuous function with discontinuous quadratic variation,Example of a continuous function with discontinuous quadratic variation,,"Let $f: [0,\infty)\to \mathbb{R}$ . The quadratic variation of $f$ , if it exists, is defined as the function $\langle f\rangle: [0,\infty) \to \mathbb{R}$ with $$  \langle f\rangle_t := \lim_{n\to \infty} \sum_{t_i \in \pi_n(t)} \left( f(t_{i+1}) - f(t_i) \right)^2  $$ for $t \in [0,\infty)$ where $\{\pi_n(t): n\in \mathbb{N}\}$ is a sequence of refining partitions of $[0,t]$ . I am looking for an example of a function $f$ such that $f$ is continuous and its quadratic variation $\langle f\rangle$ exists, but $\langle f \rangle$ is not continuous. Motivation: In probability lecture notes, one sometimes reads (e.g. in the context of the pathwise Ito formula) that a path $X(\omega)$ is assumed to be continuous with continuous quadratic variation. Therefore, I would like to understand why it is necessary to explicitly demand the quadratic variation to be continuous if this is desired.","Let . The quadratic variation of , if it exists, is defined as the function with for where is a sequence of refining partitions of . I am looking for an example of a function such that is continuous and its quadratic variation exists, but is not continuous. Motivation: In probability lecture notes, one sometimes reads (e.g. in the context of the pathwise Ito formula) that a path is assumed to be continuous with continuous quadratic variation. Therefore, I would like to understand why it is necessary to explicitly demand the quadratic variation to be continuous if this is desired.","f: [0,\infty)\to \mathbb{R} f \langle f\rangle: [0,\infty) \to \mathbb{R}   \langle f\rangle_t := \lim_{n\to \infty} \sum_{t_i \in \pi_n(t)} \left( f(t_{i+1}) - f(t_i) \right)^2   t \in [0,\infty) \{\pi_n(t): n\in \mathbb{N}\} [0,t] f f \langle f\rangle \langle f \rangle X(\omega)","['real-analysis', 'stochastic-calculus', 'quadratic-variation']"
21,Understanding Optimal Transport in One Dimension.,Understanding Optimal Transport in One Dimension.,,"I'm trying to understand these lecture notes. https://sites.ualberta.ca/~mathirl/IUSEP/IUSEP_2018/lecture_notes/Pass1.pdf I understand the formulation of the Monge Problem. However, I'm having trouble understanding one-dimensional optimal transport with cost function being $c(x-y)^2$ .In particular I do not really understand this lecture slide. In particular, I do not understand why the solution must satisfy $c(x_o,T(x_0))+c(x_1,T(x_1)) \leq c(x_0,T(x_1))+c(x_1,T(x_0))$ . Also, I do not fully understand why this implies that if $x_1>x_0$ then $T(x_1) \geq T(x_0)$ . (I understand this by doing a few concrete examples, but I do not understand how the condition $c(x_o,T(x_0))+c(x_1,T(x_1)) \leq c(x_0,T(x_1))+c(x_1,T(x_0))$ means that $T$ is monotone increasing.) Finally, I do not understand why we must choose T(x) so that $\int_{-\infty}^{x} f(t) dt= \int_{-\infty}^{T(x)} g(s) ds$ . Thank you very much and sorry for the basic questions.","I'm trying to understand these lecture notes. https://sites.ualberta.ca/~mathirl/IUSEP/IUSEP_2018/lecture_notes/Pass1.pdf I understand the formulation of the Monge Problem. However, I'm having trouble understanding one-dimensional optimal transport with cost function being .In particular I do not really understand this lecture slide. In particular, I do not understand why the solution must satisfy . Also, I do not fully understand why this implies that if then . (I understand this by doing a few concrete examples, but I do not understand how the condition means that is monotone increasing.) Finally, I do not understand why we must choose T(x) so that . Thank you very much and sorry for the basic questions.","c(x-y)^2 c(x_o,T(x_0))+c(x_1,T(x_1)) \leq c(x_0,T(x_1))+c(x_1,T(x_0)) x_1>x_0 T(x_1) \geq T(x_0) c(x_o,T(x_0))+c(x_1,T(x_1)) \leq c(x_0,T(x_1))+c(x_1,T(x_0)) T \int_{-\infty}^{x} f(t) dt= \int_{-\infty}^{T(x)} g(s) ds","['real-analysis', 'probability-theory', 'measure-theory', 'partial-differential-equations', 'optimization']"
22,Sobolev embedding for the $L^q$ norm.,Sobolev embedding for the  norm.,L^q,"Suppose $f \in H^1(\mathbb R^2)$ , where $H^1$ is the Sobolev space, then how to use this information to bound $\Vert f \Vert_{L^q}$ , where $q>2$ ? It seems like Sobolev embedding, but it's not.","Suppose , where is the Sobolev space, then how to use this information to bound , where ? It seems like Sobolev embedding, but it's not.",f \in H^1(\mathbb R^2) H^1 \Vert f \Vert_{L^q} q>2,"['real-analysis', 'partial-differential-equations', 'sobolev-spaces', 'regularity-theory-of-pdes', 'fractional-sobolev-spaces']"
23,Solving a functional equation arising from a probability problem: $g(kx)^2=g(x)$,Solving a functional equation arising from a probability problem:,g(kx)^2=g(x),"I am trying to find solutions to the following functional equation: $$g(kx)^2=g(x)\text.$$ Here, $x$ is in $\mathbb{R}$ and $k$ is a constant. In particular, I'm looking for solutions for $k=2^{-1/4}$ . Furthermore, I need that the Fourier inverse of $g$ is a density (i.e., nonnegative and integral over $\mathbb{R}$ is 1). The function $g(x)=e^{-x^4}$ satisfies the equation, but does not have a nonnegative Fourier inverse. I have deduced that $g(0)$ must be one, but have little experience with functional equations and am at a loss at how to proceed. (It may very well be the case that there are no other solutions.) The context is the following. I am tasked with finding (or showing that there exist none) i.i.d. random variables $X$ and $Y$ such that $\frac{X+Y}{2^{1/4}}\sim X$ . If you assume that $X$ and $Y$ have density $f$ , then standard Fourier arguments show that $\hat{f}=g$ must satisfy the functional equation above. The requirement that the Fourier inverse of $g$ be nonnegative comes from the fact that $f$ is a density. Any hint, either with the functional equation or the original problem, would be greatly appreciated. In particular, with regards to the original problem, I have been able to deduce that $E[X]$ is either 0 or infinite, and in either case, $E[X^2]$ is infinite, but I am not sure how to proceed to show either existence or non-existence.","I am trying to find solutions to the following functional equation: Here, is in and is a constant. In particular, I'm looking for solutions for . Furthermore, I need that the Fourier inverse of is a density (i.e., nonnegative and integral over is 1). The function satisfies the equation, but does not have a nonnegative Fourier inverse. I have deduced that must be one, but have little experience with functional equations and am at a loss at how to proceed. (It may very well be the case that there are no other solutions.) The context is the following. I am tasked with finding (or showing that there exist none) i.i.d. random variables and such that . If you assume that and have density , then standard Fourier arguments show that must satisfy the functional equation above. The requirement that the Fourier inverse of be nonnegative comes from the fact that is a density. Any hint, either with the functional equation or the original problem, would be greatly appreciated. In particular, with regards to the original problem, I have been able to deduce that is either 0 or infinite, and in either case, is infinite, but I am not sure how to proceed to show either existence or non-existence.",g(kx)^2=g(x)\text. x \mathbb{R} k k=2^{-1/4} g \mathbb{R} g(x)=e^{-x^4} g(0) X Y \frac{X+Y}{2^{1/4}}\sim X X Y f \hat{f}=g g f E[X] E[X^2],"['real-analysis', 'probability', 'analysis', 'fourier-analysis', 'functional-equations']"
24,Switching improper integrals without Fubini,Switching improper integrals without Fubini,,"I'm trying to understand general conditions that permit switching integrals as in $$\int_a^\infty \int_a^\infty f(x,y) dx dy = \int_a^\infty \int_a^\infty f(x,y) dy dx $$ if $f$ is not nonnegative or nonpositive and the integrals $\int_a^\infty f(x,y)dx, \int_a^\infty f(x,y)dy$ are improper Riemann integrals that are not absolutely convergent . So Fubini-Tonelli theorem does not apply here. Is it sufficient that $\int_a^\infty f(x,y)dx, \int_a^\infty f(x,y)dy$ are uniformly convergent for $x,y \in [0,\infty)$ ? How is it proved if true?",I'm trying to understand general conditions that permit switching integrals as in if is not nonnegative or nonpositive and the integrals are improper Riemann integrals that are not absolutely convergent . So Fubini-Tonelli theorem does not apply here. Is it sufficient that are uniformly convergent for ? How is it proved if true?,"\int_a^\infty \int_a^\infty f(x,y) dx dy = \int_a^\infty \int_a^\infty f(x,y) dy dx  f \int_a^\infty f(x,y)dx, \int_a^\infty f(x,y)dy \int_a^\infty f(x,y)dx, \int_a^\infty f(x,y)dy x,y \in [0,\infty)","['real-analysis', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
25,Does Radon-Nikodym imply Riesz Representation Theorem?,Does Radon-Nikodym imply Riesz Representation Theorem?,,"In Axler's Linear Algebra Done Right we have the theorem 6.42:  (Riesz Representation Theorem) Suppose $V$ is a finite dimensional inner product space and $\phi$ is a linear functional on $V$ . Then there is a unique vector $u \in V$ such that $$\phi(v) = \langle v, u\rangle$$ for every $v \in V$ I'm currently learning measure theory and have came across Radon-Nikodym (Radon-Nikodym) Consider a measurable space $(X,\mathcal{M})$ on which two $\sigma$ -finite signed measures $\mu,\nu$ are defined such that $\nu << \mu$ ( $\nu$ is absolutely continuous with respect to $\mu$ ) then there is a $\mu$ -integrable function $f: X \to  \mathbb{R}$ such that $$\nu(E) = \int_E f d\mu$$ for every $E \in \mathcal{M}$ and any other function $g$ satisfying this is equal to $f$ almost everywhere with respect to $\mu$ . These two theorems seems very similar. Is it possible to go from Radon-Nikodym and get Riesz Representation? The integral in Radon-Nikodym ""acts"" like the inner product in Riesz Representation, the function $f$ ""acts"" like the vector $u$ in Riesz, and the signed measure $\nu$ acts like the linear functional $\phi$ . I am inclined to think that somehow we can recover Riesz from Radon-Nikodym. To start, we would need to somehow get a $\sigma$ -algebra, $\mathcal{M}$ on $V$ so that $(V, \mathcal{M})$ is a measurable space. This has to be a very particular $\sigma$ -algebra so that somehow the integral can be reduced to the inner product on $V$ . We would also need to show that the linear functional is absolutely continuous with respect to the inner product. So is it possible to recover Riesz from Radon-Nikodym? If so, how? If not, what's the issue?","In Axler's Linear Algebra Done Right we have the theorem 6.42:  (Riesz Representation Theorem) Suppose is a finite dimensional inner product space and is a linear functional on . Then there is a unique vector such that for every I'm currently learning measure theory and have came across Radon-Nikodym (Radon-Nikodym) Consider a measurable space on which two -finite signed measures are defined such that ( is absolutely continuous with respect to ) then there is a -integrable function such that for every and any other function satisfying this is equal to almost everywhere with respect to . These two theorems seems very similar. Is it possible to go from Radon-Nikodym and get Riesz Representation? The integral in Radon-Nikodym ""acts"" like the inner product in Riesz Representation, the function ""acts"" like the vector in Riesz, and the signed measure acts like the linear functional . I am inclined to think that somehow we can recover Riesz from Radon-Nikodym. To start, we would need to somehow get a -algebra, on so that is a measurable space. This has to be a very particular -algebra so that somehow the integral can be reduced to the inner product on . We would also need to show that the linear functional is absolutely continuous with respect to the inner product. So is it possible to recover Riesz from Radon-Nikodym? If so, how? If not, what's the issue?","V \phi V u \in V \phi(v) = \langle v, u\rangle v \in V (X,\mathcal{M}) \sigma \mu,\nu \nu << \mu \nu \mu \mu f: X \to  \mathbb{R} \nu(E) = \int_E f d\mu E \in \mathcal{M} g f \mu f u \nu \phi \sigma \mathcal{M} V (V, \mathcal{M}) \sigma V","['real-analysis', 'linear-algebra', 'analysis', 'measure-theory', 'riesz-representation-theorem']"
26,"Show that $f_n \to f$ over $\| \cdot \|_\infty$ implies $f_n \to f$ over $\| \cdot \|_2$ and is $(C[a,b], \|\cdot\|_2)$ Banach?",Show that  over  implies  over  and is  Banach?,"f_n \to f \| \cdot \|_\infty f_n \to f \| \cdot \|_2 (C[a,b], \|\cdot\|_2)","Exercise : Over the space $C[a,b]$ we define the norm $$\|f\|_2 = \sqrt{\int_a^bf(x)^2\mathrm{d}x}$$ (i) Show that if the sequence $(f_n)$ converges to $f$ with respect to $\| \cdot \|_\infty$ , then it also converges to $f$ with respect to $\| \cdot \|_2$ . (ii) Is $(C[a,b], \| \cdot \|_2)$ a Banach space ? Attempt : (i) Let $(f_n)$ be a sequence defined over $C[a,b]$ that converges to $f$ with respect to the norm $\| \cdot \|_\infty$ . Then, this means that $\exists n_0 \in \mathbb N$ : $$\|f_n-f\|_\infty< e \Leftrightarrow \max_{x \in [a,b]}|f_n(x) - f(x)| < \epsilon \; \forall n \geq n_0$$ Now, let's consider the quantity $\| f_n - f \|_2$ . This is defined as : $$\|f_n - f\|_2 = \sqrt{\int_a^b (f_n-f)^2(x)\mathrm{d}x}=\sqrt{\int_a^b[f_n(x)-f(x)]^2\mathrm{d}x}$$ Now, the last expression can be rewritten as : $$\sqrt{\int_a^b[f_n(x)-f(x)]^2\mathrm{d}x}=\sqrt{\int_a^b |f_n(x) - f(x)|^2\mathrm{d}x}$$ But, it is : $$|f_n(x)-f(x)|<\max_{x \in [a,b]}|f_n(x)-f(x)| < \epsilon \; \forall n \geq n_0$$ Now, since $|f_n(x)-f(x)| \geq 0$ and $\epsilon >0$ , we yield : $$|f_n(x)-f(x)|<\epsilon \Rightarrow |f_n(x)-f(x)|^2 < \epsilon^2 \equiv \epsilon' \; \forall n\geq n_0$$ And by integrating from $a$ to $b$ since $f_n, f$ are defined over $C[a,b]$ : $$\int_a^b|f_n(x)-f(x)|^2\mathrm{d}x < \int_a^b\epsilon'\mathrm{d}x=\epsilon'(b-a) \equiv \epsilon'' \forall n\geq n_0$$ Thus, we have $\|f_n-f\|_2 < \epsilon'' \; \forall n\geq n_0$ , which means that $(f_n)$ converges to $f$ with respect to the $\|\cdot\|_2$ norm. Question : Is my approach to (i) correct ? How would one approach (ii) though ?","Exercise : Over the space we define the norm (i) Show that if the sequence converges to with respect to , then it also converges to with respect to . (ii) Is a Banach space ? Attempt : (i) Let be a sequence defined over that converges to with respect to the norm . Then, this means that : Now, let's consider the quantity . This is defined as : Now, the last expression can be rewritten as : But, it is : Now, since and , we yield : And by integrating from to since are defined over : Thus, we have , which means that converges to with respect to the norm. Question : Is my approach to (i) correct ? How would one approach (ii) though ?","C[a,b] \|f\|_2 = \sqrt{\int_a^bf(x)^2\mathrm{d}x} (f_n) f \| \cdot \|_\infty f \| \cdot \|_2 (C[a,b], \| \cdot \|_2) (f_n) C[a,b] f \| \cdot \|_\infty \exists n_0 \in \mathbb N \|f_n-f\|_\infty< e \Leftrightarrow \max_{x \in [a,b]}|f_n(x) - f(x)| < \epsilon \; \forall n \geq n_0 \| f_n - f \|_2 \|f_n - f\|_2 = \sqrt{\int_a^b (f_n-f)^2(x)\mathrm{d}x}=\sqrt{\int_a^b[f_n(x)-f(x)]^2\mathrm{d}x} \sqrt{\int_a^b[f_n(x)-f(x)]^2\mathrm{d}x}=\sqrt{\int_a^b |f_n(x) - f(x)|^2\mathrm{d}x} |f_n(x)-f(x)|<\max_{x \in [a,b]}|f_n(x)-f(x)| < \epsilon \; \forall n \geq n_0 |f_n(x)-f(x)| \geq 0 \epsilon >0 |f_n(x)-f(x)|<\epsilon \Rightarrow |f_n(x)-f(x)|^2 < \epsilon^2 \equiv \epsilon' \; \forall n\geq n_0 a b f_n, f C[a,b] \int_a^b|f_n(x)-f(x)|^2\mathrm{d}x < \int_a^b\epsilon'\mathrm{d}x=\epsilon'(b-a) \equiv \epsilon'' \forall n\geq n_0 \|f_n-f\|_2 < \epsilon'' \; \forall n\geq n_0 (f_n) f \|\cdot\|_2","['real-analysis', 'functional-analysis', 'banach-spaces', 'normed-spaces']"
27,Identifiability of Normal From Conditional Probability,Identifiability of Normal From Conditional Probability,,"Let $Z_x \sim \mathcal{N}(x,1)$ , $D_1 = [0,c]$ , and $D=[-c,c]$ . Can we determine $x$ from $$f(x) = \mathbb{P}(Z_x\in D_1 | Z_x\in D) = \frac{\Phi(c - x) - \Phi(-x)}{\Phi(c - x) - \Phi(-c-x)}?$$ In particular, can we validate the (numerically obvious) claim that $f$ is monotone, ranging from $0$ to $1$ ? Even $\lim_{x\to\infty}f(x) = 1$ doesn't seem obvious to me; L'Hospital's rule isn't illuminating there. A clear approach to this is to consider the derivative $$ \begin{align*} f'(x) &= \frac{f(x)\bigl(\phi(c-x)-\phi(-c-x)\bigr) - \bigl(\phi(c-x) - \phi(-x)\bigr)}{\mathbb{P}(Z_x\in D)}\\ &\propto f(x)\bigl(\phi(c-x)-\phi(-c-x)\bigr) - \bigl(\phi(c-x) - \phi(-x)\bigr), \end{align*} $$ and show that $f'>0$ uniformly, but I can't seem to bound this either. Answers to either would be extremely helpful, but injectivity of $f$ is more important for my application. If you could come up with a version of this that works for higher dimensional Gaussians ( $D_i$ are orthants/quadrants of spheres then) that would be perfect.","Let , , and . Can we determine from In particular, can we validate the (numerically obvious) claim that is monotone, ranging from to ? Even doesn't seem obvious to me; L'Hospital's rule isn't illuminating there. A clear approach to this is to consider the derivative and show that uniformly, but I can't seem to bound this either. Answers to either would be extremely helpful, but injectivity of is more important for my application. If you could come up with a version of this that works for higher dimensional Gaussians ( are orthants/quadrants of spheres then) that would be perfect.","Z_x \sim \mathcal{N}(x,1) D_1 = [0,c] D=[-c,c] x f(x) = \mathbb{P}(Z_x\in D_1 | Z_x\in D) = \frac{\Phi(c - x) - \Phi(-x)}{\Phi(c - x) - \Phi(-c-x)}? f 0 1 \lim_{x\to\infty}f(x) = 1 
\begin{align*}
f'(x) &= \frac{f(x)\bigl(\phi(c-x)-\phi(-c-x)\bigr) - \bigl(\phi(c-x) - \phi(-x)\bigr)}{\mathbb{P}(Z_x\in D)}\\
&\propto f(x)\bigl(\phi(c-x)-\phi(-c-x)\bigr) - \bigl(\phi(c-x) - \phi(-x)\bigr),
\end{align*}
 f'>0 f D_i","['real-analysis', 'probability', 'monotone-functions', 'upper-lower-bounds']"
28,"If $a_n\sim b_n$ decrease to $0$ and $a_{n+1}b_n-a_nb_{n+1}$ changes sign i.o. , does it imply the same for $(1-a_{n+1})(1-b_n)-(1-a_n)(1-b_{n+1})$?","If  decrease to  and  changes sign i.o. , does it imply the same for ?",a_n\sim b_n 0 a_{n+1}b_n-a_nb_{n+1} (1-a_{n+1})(1-b_n)-(1-a_n)(1-b_{n+1}),"Suppose $a_n<b_n$ for all $n$, both are strictly decreasing to $0$ and $a_n\sim b_n$. If $A_n=a_{n+1}b_n-a_nb_{n+1}$ changes sign infinitely often, does it follow that $B_n=(1-a_{n+1})(1-b_n)-(1-a_n)(1-b_{n+1})$ also does? In some sense, the implication should not be there, because while $A_n<0$ does not seem to be crucial for $B_n$, I have found that whenever $A_n>0$ we have $B_n<0$: the former is equivalent to $$a_{n+1}>a_n\frac{b_{n+1}}{b_n}$$ and since for large enough $n$, $0<a_n<1, a_n<b_n<1, 0<b_{n+1}<b_n$ this is stronger than $$a_{n+1}>1-\frac{1-a_n}{1-b_n}(1-b_{n+1}),$$ equivalent to $B_n<0$. $A_n<0$ does not seem to be crucial because it should be possible to have this happen without making $a_{n+1}$ so small that $B_n$ becomes positive. On the other hand, maybe the fact that $A_n$ changes sign infinitely often gives more weight to the instances of $A_n<0$. Not for too long, but I have tried to construct some simple $A_n,B_n$ contradicting the claim and I have not found them yet. Are there any, with $B_n<0$? Besides, how does the situation change if $a_n-b_n$ is also assumed to change sign infinitely often?","Suppose $a_n<b_n$ for all $n$, both are strictly decreasing to $0$ and $a_n\sim b_n$. If $A_n=a_{n+1}b_n-a_nb_{n+1}$ changes sign infinitely often, does it follow that $B_n=(1-a_{n+1})(1-b_n)-(1-a_n)(1-b_{n+1})$ also does? In some sense, the implication should not be there, because while $A_n<0$ does not seem to be crucial for $B_n$, I have found that whenever $A_n>0$ we have $B_n<0$: the former is equivalent to $$a_{n+1}>a_n\frac{b_{n+1}}{b_n}$$ and since for large enough $n$, $0<a_n<1, a_n<b_n<1, 0<b_{n+1}<b_n$ this is stronger than $$a_{n+1}>1-\frac{1-a_n}{1-b_n}(1-b_{n+1}),$$ equivalent to $B_n<0$. $A_n<0$ does not seem to be crucial because it should be possible to have this happen without making $a_{n+1}$ so small that $B_n$ becomes positive. On the other hand, maybe the fact that $A_n$ changes sign infinitely often gives more weight to the instances of $A_n<0$. Not for too long, but I have tried to construct some simple $A_n,B_n$ contradicting the claim and I have not found them yet. Are there any, with $B_n<0$? Besides, how does the situation change if $a_n-b_n$ is also assumed to change sign infinitely often?",,"['real-analysis', 'sequences-and-series', 'examples-counterexamples']"
29,Proving Holder inequality using Lagrange multipliers,Proving Holder inequality using Lagrange multipliers,,"Let $x \geq 0, y \geq 0$ and $p > 0, q > 0$ with $\dfrac{1}{p} + \dfrac{1}{q} = 1$. Show that $$xy \leq \frac{1}{p}x^p + \frac{1}{q}y^q.$$ Given $v = (v_1,\cdots,v_m) \in \mathbb{R}^{m}$, define $\displaystyle |v|_{p} = \left(\sum_{i}|v_{i}|^p \right)^{\frac{1}{p}}$. Use the previous inequality for show that, if $\displaystyle \frac{1}{p} + \frac{1}{q} = 1$ so, given $u,v \in \mathbb{R}^{m}$ the Holder inequality $|\langle u,v \rangle| \leq |u|_{p}|v|_{q}$ is true. My idea is: Show that $S = \lbrace (x,y)\in \mathbb{R}^{2}\mid x \geq 0, y \geq 0, xy=1 \rbrace$ is a surface of $\mathbb{R}^{3}$ and defining the function $\varphi: \mathbb{R}^{2}_{+} \to \mathbb{R}$ by $\varphi(x,y) = xy$, $S = \varphi^{-1}(1)$. Then I want to apply Lagrange Multipliers to determine the minimum of $f|_M$ where\begin{align*} f: &&\mathbb{R}^{2} &\longrightarrow \mathbb{R},\\ &&(x,y) &\longmapsto  \frac{1}{p}x^p + \frac{1}{q}y^q \end{align*} with $p,q > 0$ and $\displaystyle \frac{1}{p} + \frac{1}{q} = 1$. I know that probably is the way, or at least a part of it. But I cannot determine the critical points. Can someone help me?","Let $x \geq 0, y \geq 0$ and $p > 0, q > 0$ with $\dfrac{1}{p} + \dfrac{1}{q} = 1$. Show that $$xy \leq \frac{1}{p}x^p + \frac{1}{q}y^q.$$ Given $v = (v_1,\cdots,v_m) \in \mathbb{R}^{m}$, define $\displaystyle |v|_{p} = \left(\sum_{i}|v_{i}|^p \right)^{\frac{1}{p}}$. Use the previous inequality for show that, if $\displaystyle \frac{1}{p} + \frac{1}{q} = 1$ so, given $u,v \in \mathbb{R}^{m}$ the Holder inequality $|\langle u,v \rangle| \leq |u|_{p}|v|_{q}$ is true. My idea is: Show that $S = \lbrace (x,y)\in \mathbb{R}^{2}\mid x \geq 0, y \geq 0, xy=1 \rbrace$ is a surface of $\mathbb{R}^{3}$ and defining the function $\varphi: \mathbb{R}^{2}_{+} \to \mathbb{R}$ by $\varphi(x,y) = xy$, $S = \varphi^{-1}(1)$. Then I want to apply Lagrange Multipliers to determine the minimum of $f|_M$ where\begin{align*} f: &&\mathbb{R}^{2} &\longrightarrow \mathbb{R},\\ &&(x,y) &\longmapsto  \frac{1}{p}x^p + \frac{1}{q}y^q \end{align*} with $p,q > 0$ and $\displaystyle \frac{1}{p} + \frac{1}{q} = 1$. I know that probably is the way, or at least a part of it. But I cannot determine the critical points. Can someone help me?",,"['real-analysis', 'derivatives', 'lagrange-multiplier']"
30,Infinitely many $n$ such that $2^n$ in base $10$ starts with $7777777$,Infinitely many  such that  in base  starts with,n 2^n 10 7777777,"I'm attempting to prove that there are infinitely many $n$ such that the first $7$ digits in the base $10$ expressions of $2^{n}$ are $7777777$. However, I don't even know where to start. Apparently I'm supposed to use the fact that the set $$\{x_{n} = n\alpha-\lfloor n\alpha \rfloor \mid n \in \mathbb{N}\}$$ is dense in $[0,1]$, for a fixed irrational $\alpha \in \mathbb{R}$ (I've already proven this).  Any help would be appreciated! (I'm more looking for a hint not a solution)","I'm attempting to prove that there are infinitely many $n$ such that the first $7$ digits in the base $10$ expressions of $2^{n}$ are $7777777$. However, I don't even know where to start. Apparently I'm supposed to use the fact that the set $$\{x_{n} = n\alpha-\lfloor n\alpha \rfloor \mid n \in \mathbb{N}\}$$ is dense in $[0,1]$, for a fixed irrational $\alpha \in \mathbb{R}$ (I've already proven this).  Any help would be appreciated! (I'm more looking for a hint not a solution)",,['real-analysis']
31,"Finding $\lim\limits_{x+}\int_{-}^{+}|f(t-x)-f(t)|\,\mathrm dt$",Finding,"\lim\limits_{x+}\int_{-}^{+}|f(t-x)-f(t)|\,\mathrm dt","Let $f$ be integrable and continuous function on $\mathbb{R}$. Then I would like to find the value of: $$\lim_{x \to +\infty} \int_{-\infty}^{+\infty} | f(t-x)-f(t) | \,\mathrm{d}t.$$ I have a lot of problem with these kind of ""theorical"" integration problems. Moreover, it is hard for me to even guess what the result is, maybe it is $0$, but I am not sure. So my little try, is the following: $$\int_{-\infty}^{+\infty} | f(t-x)-f(t) | \,\mathrm{d}t \leq \int_{-\infty}^{+\infty}\sup_{\mathbb{R}} | f(t-x)-f(t) | \,\mathrm{d}t.$$ Here my goal is to prove that: $$\sup_{\mathbb{R}} | f(t-x)-f(t) | \rightarrow 0,$$ yet the problem is that I do not know how to proceed, and it is possible that $0$ is not the right answer.","Let $f$ be integrable and continuous function on $\mathbb{R}$. Then I would like to find the value of: $$\lim_{x \to +\infty} \int_{-\infty}^{+\infty} | f(t-x)-f(t) | \,\mathrm{d}t.$$ I have a lot of problem with these kind of ""theorical"" integration problems. Moreover, it is hard for me to even guess what the result is, maybe it is $0$, but I am not sure. So my little try, is the following: $$\int_{-\infty}^{+\infty} | f(t-x)-f(t) | \,\mathrm{d}t \leq \int_{-\infty}^{+\infty}\sup_{\mathbb{R}} | f(t-x)-f(t) | \,\mathrm{d}t.$$ Here my goal is to prove that: $$\sup_{\mathbb{R}} | f(t-x)-f(t) | \rightarrow 0,$$ yet the problem is that I do not know how to proceed, and it is possible that $0$ is not the right answer.",,"['real-analysis', 'limits']"
32,The Sobolev space with respect to different norms,The Sobolev space with respect to different norms,,"Consider two different norms on the Sobolev space $W^{1,2}([0,1])$: the $L_2$ norm $$ \|f\|_{L_2}^2:=\int_{[0,1]} f^2(x)\, dx $$ and the usual standard norm: $$ \|f\|_{H^1}^2:=\|f\|_{L_2}^2+\|f'\|_{L_2}^2. $$ It is obvious that this two norms are not equivalent and generate different topologies. Is it true that these two norms, nevertheless, generate the same Borel $\sigma$--algebra?","Consider two different norms on the Sobolev space $W^{1,2}([0,1])$: the $L_2$ norm $$ \|f\|_{L_2}^2:=\int_{[0,1]} f^2(x)\, dx $$ and the usual standard norm: $$ \|f\|_{H^1}^2:=\|f\|_{L_2}^2+\|f'\|_{L_2}^2. $$ It is obvious that this two norms are not equivalent and generate different topologies. Is it true that these two norms, nevertheless, generate the same Borel $\sigma$--algebra?",,"['real-analysis', 'general-topology', 'functional-analysis', 'measure-theory', 'sobolev-spaces']"
33,CheckMyProof: Proofs involving Big-O Notation,CheckMyProof: Proofs involving Big-O Notation,,"This is a homework exercise for a few weeks ago and I wanted your feedback on my improved proofs. For $g: \mathbb{N} \to \mathbb{R}$ let   $$o(g):= \{f:\mathbb{N} \to \mathbb{R} |\forall \alpha > 0 : \exists n_0 \in\mathbb{N} : \forall n \geq n_0 : 0 \leq f(n) \leq \alpha g(n)\}$$   Let $g: \mathbb{N} \to \mathbb{R}$ so that $g(n)\not= 0$ for infinitely many $n \in \mathbb{N}$. Prove or disprove $\mathcal{O}(g) \setminus \Theta(g) \subseteq o(g)$ $o(g) \subseteq \mathcal{O}(g) \setminus \Theta(g)$ $f \in o(g) \implies g \notin o(f)$ Our definitions of $\mathcal{O}$ and $\Omega$ and $\Theta$ are as follows $$ f \in \mathcal{O}(g) \iff \exists n_0 \in \mathbb{N} ~\exists \alpha >0 : 0 \leq f(n) \leq \alpha g(n) ~~\forall n \geq n_0 ~~~\textrm{and} $$ $$ f \in \Omega(g) \iff \exists n_0 \in \mathbb{N} ~\exists \beta >0 : 0 \leq \beta g(n) \leq f(n) ~~\forall n \geq n_0 $$ $$ f \in \Theta(g) \iff f \in \Omega(g) \land f \in \mathcal{O}(g) $$ The incorrectness of the statement is proven by counterexample. Let $f(n) := \begin{cases} 1 & n ~\textrm{odd} \\ 0 & n ~\textrm{even} \end{cases}~~~$ and $g(n) = 1$, then  $f \in \mathcal{O}(g) \setminus \Theta(g)$ but $f \notin o(g)$. Proof: From the definition we know, that $$ f \in \mathcal{O}(g) \iff \exists \hat{n_0} \in \mathbb{N} ~\exists \alpha >0 : 0 \leq f(n) \leq \alpha g(n) ~~\forall n \geq \hat{n_0} ~~~\textrm{and} $$ $$ f \notin \Theta(g) \iff \exists \tilde{n_0} \in \mathbb{N} ~\nexists \beta >0 : 0 \leq \beta g(n) \leq f(n) ~~\forall n \geq \tilde{n_0} $$ Combining those statements we obtain for all  $ n \geq \tilde{n_0}, \hat{n_0} \in \mathbb{N}$ $$ 0 \leq \beta g(n) \leq f(n) \leq \alpha g(n) \iff 0 < \beta \leq f(n) \leq \alpha $$ Since $~f(n) \in \{0,1\}$, there exists no $\beta >0$ to make $0 < \beta  \leq f(n)$ true, because for every even $n$ we obtain $0 = f(n) < \beta  ~\forall \beta > 0$. $~~~~\square$ (Question specifically here: $\beta > 0 $, but the condition is $\beta g(n) = \beta \geq 0$. In the last paragraph, do I use $\geq$ or $>$ ?) The statement is correct. Let $f \in o(g)$, then clearly $f \in \mathcal{O}(g)$. Now we have to show that $f \notin \Theta (g)$. Let's assume $f \in \Theta (g)$, while $f \in o(g)$, then follows that $$ f(n) \leq \alpha g(n) ~\forall \alpha > 0 ~\textrm{and}~ n \geq n_0 \in \mathbb{N} \land \beta g(n) \leq f(n) ~\textrm{for one}~ \beta > 0 ~\forall n \geq \tilde{n_0} \in \mathbb{N} $$ Which is a contradiction, because the first condition is only true if $f(n) = \beta g(n)$. Then $\beta g(n) > \alpha \beta g(n)$ for a $\alpha <1$, so the second condition can't hold. So $f \notin \Theta (g)$ and therefore the statement is true. The statement is correct. Let $f \in o(g)$ and $g \in o(f)$. Then $$ \exists n_0 \in \mathbb{N} ~\forall \alpha > 0: 0 \leq f(n) \leq \alpha g(n) ~~\forall n \geq n_0 ~~~\textrm{and} $$ $$ \exists \tilde{n_0} \in \mathbb{N} ~\forall \tilde{\alpha} > 0: 0 \leq g(n) \leq \tilde{\alpha} f(n) ~~\forall n \geq \tilde{n_0} $$ Combing both conditions we obtain $$ \exists \hat{n_0} := \max\{n_0, \tilde{n_0}\} ~\forall \alpha, \tilde{\alpha} >0 : 0 \leq f(n) \leq \alpha g(n) \leq \alpha \tilde{\alpha} f(n)~~\forall \hat{n_0} \geq n \in \mathbb{N} $$ Which is only possible if $f = g = 0$, but $g \not= 0$ for infinitely many $n \in \mathbb{N}$, producing a contradiction, so we know that $g \notin o(g)$, proving the statement correct. Alternatively choose $\alpha = 1, \tilde{\alpha} = 0.5$ to obtain $$ 0 \leq f(n) \leq g(n) \leq 0.5 f(n) \implies 2f(n) \leq f(n) \implies 2 \leq 1 $$ arriving at a contradiction. (Here: searching for a better justification of the contradiction)","This is a homework exercise for a few weeks ago and I wanted your feedback on my improved proofs. For $g: \mathbb{N} \to \mathbb{R}$ let   $$o(g):= \{f:\mathbb{N} \to \mathbb{R} |\forall \alpha > 0 : \exists n_0 \in\mathbb{N} : \forall n \geq n_0 : 0 \leq f(n) \leq \alpha g(n)\}$$   Let $g: \mathbb{N} \to \mathbb{R}$ so that $g(n)\not= 0$ for infinitely many $n \in \mathbb{N}$. Prove or disprove $\mathcal{O}(g) \setminus \Theta(g) \subseteq o(g)$ $o(g) \subseteq \mathcal{O}(g) \setminus \Theta(g)$ $f \in o(g) \implies g \notin o(f)$ Our definitions of $\mathcal{O}$ and $\Omega$ and $\Theta$ are as follows $$ f \in \mathcal{O}(g) \iff \exists n_0 \in \mathbb{N} ~\exists \alpha >0 : 0 \leq f(n) \leq \alpha g(n) ~~\forall n \geq n_0 ~~~\textrm{and} $$ $$ f \in \Omega(g) \iff \exists n_0 \in \mathbb{N} ~\exists \beta >0 : 0 \leq \beta g(n) \leq f(n) ~~\forall n \geq n_0 $$ $$ f \in \Theta(g) \iff f \in \Omega(g) \land f \in \mathcal{O}(g) $$ The incorrectness of the statement is proven by counterexample. Let $f(n) := \begin{cases} 1 & n ~\textrm{odd} \\ 0 & n ~\textrm{even} \end{cases}~~~$ and $g(n) = 1$, then  $f \in \mathcal{O}(g) \setminus \Theta(g)$ but $f \notin o(g)$. Proof: From the definition we know, that $$ f \in \mathcal{O}(g) \iff \exists \hat{n_0} \in \mathbb{N} ~\exists \alpha >0 : 0 \leq f(n) \leq \alpha g(n) ~~\forall n \geq \hat{n_0} ~~~\textrm{and} $$ $$ f \notin \Theta(g) \iff \exists \tilde{n_0} \in \mathbb{N} ~\nexists \beta >0 : 0 \leq \beta g(n) \leq f(n) ~~\forall n \geq \tilde{n_0} $$ Combining those statements we obtain for all  $ n \geq \tilde{n_0}, \hat{n_0} \in \mathbb{N}$ $$ 0 \leq \beta g(n) \leq f(n) \leq \alpha g(n) \iff 0 < \beta \leq f(n) \leq \alpha $$ Since $~f(n) \in \{0,1\}$, there exists no $\beta >0$ to make $0 < \beta  \leq f(n)$ true, because for every even $n$ we obtain $0 = f(n) < \beta  ~\forall \beta > 0$. $~~~~\square$ (Question specifically here: $\beta > 0 $, but the condition is $\beta g(n) = \beta \geq 0$. In the last paragraph, do I use $\geq$ or $>$ ?) The statement is correct. Let $f \in o(g)$, then clearly $f \in \mathcal{O}(g)$. Now we have to show that $f \notin \Theta (g)$. Let's assume $f \in \Theta (g)$, while $f \in o(g)$, then follows that $$ f(n) \leq \alpha g(n) ~\forall \alpha > 0 ~\textrm{and}~ n \geq n_0 \in \mathbb{N} \land \beta g(n) \leq f(n) ~\textrm{for one}~ \beta > 0 ~\forall n \geq \tilde{n_0} \in \mathbb{N} $$ Which is a contradiction, because the first condition is only true if $f(n) = \beta g(n)$. Then $\beta g(n) > \alpha \beta g(n)$ for a $\alpha <1$, so the second condition can't hold. So $f \notin \Theta (g)$ and therefore the statement is true. The statement is correct. Let $f \in o(g)$ and $g \in o(f)$. Then $$ \exists n_0 \in \mathbb{N} ~\forall \alpha > 0: 0 \leq f(n) \leq \alpha g(n) ~~\forall n \geq n_0 ~~~\textrm{and} $$ $$ \exists \tilde{n_0} \in \mathbb{N} ~\forall \tilde{\alpha} > 0: 0 \leq g(n) \leq \tilde{\alpha} f(n) ~~\forall n \geq \tilde{n_0} $$ Combing both conditions we obtain $$ \exists \hat{n_0} := \max\{n_0, \tilde{n_0}\} ~\forall \alpha, \tilde{\alpha} >0 : 0 \leq f(n) \leq \alpha g(n) \leq \alpha \tilde{\alpha} f(n)~~\forall \hat{n_0} \geq n \in \mathbb{N} $$ Which is only possible if $f = g = 0$, but $g \not= 0$ for infinitely many $n \in \mathbb{N}$, producing a contradiction, so we know that $g \notin o(g)$, proving the statement correct. Alternatively choose $\alpha = 1, \tilde{\alpha} = 0.5$ to obtain $$ 0 \leq f(n) \leq g(n) \leq 0.5 f(n) \implies 2f(n) \leq f(n) \implies 2 \leq 1 $$ arriving at a contradiction. (Here: searching for a better justification of the contradiction)",,"['real-analysis', 'discrete-mathematics', 'proof-verification', 'asymptotics', 'computer-science']"
34,Which values of Lambert W function are rational?,Which values of Lambert W function are rational?,,"I mean the branch of Lambert-W function when it is single-valued (The $W_0$ function):  Defined on $x\geq \dfrac{1}{e}$ and by the relation $w = W(x) \iff x = we^{w}$. I know that $0$ is one of the rational values, since $W(0)=0$, because $0e^{0} = 0$. I wonder what are the others? I see here ( https://cs.uwaterloo.ca/research/tr/1993/03/W.pdf ) that I can write $W_0(x) = \displaystyle\sum_{n=1}^{\infty} \frac{(-n)^{n-1}}{n!} x^n$, then it is analytic around $0$, and the range is $[-1,\infty)$, so by the Intermediate Value Theorem there must be infinite values where $W(x)$ is rational, but I have no idea how to analyse that. Any help would be appreciated.","I mean the branch of Lambert-W function when it is single-valued (The $W_0$ function):  Defined on $x\geq \dfrac{1}{e}$ and by the relation $w = W(x) \iff x = we^{w}$. I know that $0$ is one of the rational values, since $W(0)=0$, because $0e^{0} = 0$. I wonder what are the others? I see here ( https://cs.uwaterloo.ca/research/tr/1993/03/W.pdf ) that I can write $W_0(x) = \displaystyle\sum_{n=1}^{\infty} \frac{(-n)^{n-1}}{n!} x^n$, then it is analytic around $0$, and the range is $[-1,\infty)$, so by the Intermediate Value Theorem there must be infinite values where $W(x)$ is rational, but I have no idea how to analyse that. Any help would be appreciated.",,['real-analysis']
35,Intuition behind the irrationality measure,Intuition behind the irrationality measure,,"The irrationality measure $\mu(x)$ of a real number $x$ is defined to be the supremum of the set of real numbers $\mu$ such that the inequalities $$0 < \left| x - \frac{p}{q} \right| < \frac{1}{q^\mu} \qquad (1)$$ hold for an infinite number of integer pairs $(p, q)$ with $q > 0$. Wikipedia says that $\mu(x)$ measures ""how 'closely' $x$ can be approximated by rationals,"" but I'm very unclear about exactly how it does it, because the ""approximability"" of a real number seems to depend non-monotonically on $\mu$, with real numbers with low and high values of $\mu(x)$ easily approximable by rationals, and real numbers with intermediate values of $\mu(x)$ difficult to approximate by rationals. Specifically, we have $\mu(x) \geq 1$, with the preimage of $\mu(x) = 1$ is exactly the rationals $\mathbb{Q}$. the preimage of $\mu(x) = 2$ contains all of the irrational algebraic numbers $\bar{Q} \setminus Q$ (by Roth's theorem), as well as almost all of the transcendental numbers (in the Lebesgue-measure sense), including $e$ and $\varphi$. the preimage of $\mu(x) \in (2, \infty)$ is a measure-zero subset of the transcendental numbers the preimage of $\mu(x) = \infty$ is the set of Liouville numbers (this set is ""large"" in the sense of having the cardinality of the continuum and being dense in the reals, but ""small"" in the sense of having Lebesgue measure zero). The name ""irrationality measure"" seems to imply that if $\mu(x) > \mu(y)$, then $x$ is ""more irrational"" than $y$, i.e. is harder to approximate by a sequence of rational numbers. But in fact the opposite is true; the Louiville numbers, which have $\mu(x) = \infty$, are unusually easy to approximate by a sequence of rationals, although of course not as easy as the rationals themselves, which have $\mu(x) = 1$. How do I understand this strange non-monotonicity? As I understand it, the problem stems entirely from the first inequality in (1), which seems extremely arbitrary and conceptually unnatural. If we remove that inequality, then the second inequality has a very nice interpretation: the error in the Diophantine approximation sequence decreases with $q$ as a power law with exponent $\mu$, and higher $\mu$ means that the error decays faster. So under this proposed modification, $\mu$ would be interpreted as a rationality measure: almost all irrational numbers would have the minimal value $\mu(x) = 2$, but a few numbers would be unusally easy to approximate and have $\mu(x) > 2$. For a rational number we would trivially have $\mu = \infty$ (under this modified definition), because the errors would vanish identically after some finite $q$. Liouville numbers would be unusual in that their Diophantine approximations would vanish with $q$ faster than any power law, although never hitting zero, so they would also have $\mu(x) = \infty$ just like the rationals. Is there some motivation for the first inequality that I'm missing? It seems to enormously decrease the conceptual clarity of $\mu$ by making it a non-monotonic measure of Diophantine approximability.","The irrationality measure $\mu(x)$ of a real number $x$ is defined to be the supremum of the set of real numbers $\mu$ such that the inequalities $$0 < \left| x - \frac{p}{q} \right| < \frac{1}{q^\mu} \qquad (1)$$ hold for an infinite number of integer pairs $(p, q)$ with $q > 0$. Wikipedia says that $\mu(x)$ measures ""how 'closely' $x$ can be approximated by rationals,"" but I'm very unclear about exactly how it does it, because the ""approximability"" of a real number seems to depend non-monotonically on $\mu$, with real numbers with low and high values of $\mu(x)$ easily approximable by rationals, and real numbers with intermediate values of $\mu(x)$ difficult to approximate by rationals. Specifically, we have $\mu(x) \geq 1$, with the preimage of $\mu(x) = 1$ is exactly the rationals $\mathbb{Q}$. the preimage of $\mu(x) = 2$ contains all of the irrational algebraic numbers $\bar{Q} \setminus Q$ (by Roth's theorem), as well as almost all of the transcendental numbers (in the Lebesgue-measure sense), including $e$ and $\varphi$. the preimage of $\mu(x) \in (2, \infty)$ is a measure-zero subset of the transcendental numbers the preimage of $\mu(x) = \infty$ is the set of Liouville numbers (this set is ""large"" in the sense of having the cardinality of the continuum and being dense in the reals, but ""small"" in the sense of having Lebesgue measure zero). The name ""irrationality measure"" seems to imply that if $\mu(x) > \mu(y)$, then $x$ is ""more irrational"" than $y$, i.e. is harder to approximate by a sequence of rational numbers. But in fact the opposite is true; the Louiville numbers, which have $\mu(x) = \infty$, are unusually easy to approximate by a sequence of rationals, although of course not as easy as the rationals themselves, which have $\mu(x) = 1$. How do I understand this strange non-monotonicity? As I understand it, the problem stems entirely from the first inequality in (1), which seems extremely arbitrary and conceptually unnatural. If we remove that inequality, then the second inequality has a very nice interpretation: the error in the Diophantine approximation sequence decreases with $q$ as a power law with exponent $\mu$, and higher $\mu$ means that the error decays faster. So under this proposed modification, $\mu$ would be interpreted as a rationality measure: almost all irrational numbers would have the minimal value $\mu(x) = 2$, but a few numbers would be unusally easy to approximate and have $\mu(x) > 2$. For a rational number we would trivially have $\mu = \infty$ (under this modified definition), because the errors would vanish identically after some finite $q$. Liouville numbers would be unusual in that their Diophantine approximations would vanish with $q$ faster than any power law, although never hitting zero, so they would also have $\mu(x) = \infty$ just like the rationals. Is there some motivation for the first inequality that I'm missing? It seems to enormously decrease the conceptual clarity of $\mu$ by making it a non-monotonic measure of Diophantine approximability.",,"['real-analysis', 'real-numbers', 'rational-numbers', 'diophantine-approximation', 'liouville-numbers']"
36,Sufficient condition under which a pointwise convergent becomes uniform convergence,Sufficient condition under which a pointwise convergent becomes uniform convergence,,Let $K$ be any compact subset of $\mathbb{R}$. Then what are the   sufficient conditions so that any pointwise convergent sequence of functions on $K$ converges uniformly. The conditions can be given as in Dini's Theorem. Can we have other conditions (Weaker) apart from Dini's theorem so that this becomes true?,Let $K$ be any compact subset of $\mathbb{R}$. Then what are the   sufficient conditions so that any pointwise convergent sequence of functions on $K$ converges uniformly. The conditions can be given as in Dini's Theorem. Can we have other conditions (Weaker) apart from Dini's theorem so that this becomes true?,,"['real-analysis', 'convergence-divergence']"
37,Dense subspace of $L^p$,Dense subspace of,L^p,"Working on a larger problem relating to harmonic analysis I have come upon this measure theory issue. Suppose $\varphi$ is a measurable function on $U$ and $f \in L^p(U)$. $\varphi$ is not necessarily bounded, but consider the set $$\{ f \in L^p(U) \ : \ \varphi f \in L^p(U) \}.$$ How do I show this space is dense in $L^p$? If we consider that $f \cdot \chi_{\{ \left| \varphi \right| \leq N\} } \to f$ in $L^p$ can this help us?","Working on a larger problem relating to harmonic analysis I have come upon this measure theory issue. Suppose $\varphi$ is a measurable function on $U$ and $f \in L^p(U)$. $\varphi$ is not necessarily bounded, but consider the set $$\{ f \in L^p(U) \ : \ \varphi f \in L^p(U) \}.$$ How do I show this space is dense in $L^p$? If we consider that $f \cdot \chi_{\{ \left| \varphi \right| \leq N\} } \to f$ in $L^p$ can this help us?",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
38,Equivalence of Cauchy integral with Riemann integral,Equivalence of Cauchy integral with Riemann integral,,"There has already been some discussion on this topic . However my question is about a specific solution to this problem and for the benefit of readers I think it is better to add some context (even though it means repetition of some stuff mentioned in the linked question). In what follows $f$ is a function of type $f:[a, b]\to\mathbb{R}$ and $f$ is bounded. A partition $P$ of $[a, b]$ is a set of type $$P = \{x_{0}, x_{1}, x_{2}, \dots, x_{n}\}$$ where $$a = x_{0} < x_{1} < x_{2} < \dots < x_{n} = b$$ The norm $||P||$ of partition $P$ is defined by $||P|| = \max_{i = 1}^{n}(x_{i} - x_{i - 1})$. We define the following sums for $f$ over $P$ \begin{align} C(f, P) &= \sum_{i = 1}^{n}f(x_{i - 1})(x_{i} - x_{i - 1})\notag\\ S(f, P) &= \sum_{i = 1}^{n}f(t_{i})(x_{i} - x_{i - 1})\notag\\ U(f, P) &= \sum_{i = 1}^{n}M_{i}(x_{i} - x_{i - 1})\notag\\ L(f, P) &= \sum_{i = 1}^{n}m_{i}(x_{i} - x_{i - 1})\notag \end{align} where $t_{i}$ are arbitrary points in $[x_{i - 1}, x_{i}]$ and $$M_{i} = \sup\,\{f(x)\mid x\in [x_{i - 1}, x_{i}]\},\,m_{i} = \inf\,\{f(x)\mid x\in [x_{i - 1}, x_{i}]\}$$ The sum $C(f, P)$ is called (left) Cauchy sum for $f$ over $P$. The Riemann sum $S(f, P)$ depends on choice of tags $t_{i}$ but this dependence in not shown in the notation and should be evident from the context. And finally $U(f, P), L(f, P)$ are upper and lower Darboux sums for $f$ over $P$. Cauchy Integral : The function $f$ is said be said to be Cauchy integrable over $[a, b] $ with Cauchy integral $I$ if for every $\epsilon >0$ there is a number $\delta > 0$ such that $|C(f, P) - I| < \epsilon$ whenever $P$ is a partition of $[a, b]$ with $||P|| < \delta$. A similar definition is available for Riemann integral if $C(f, P)$ is replaced by $S(f, P)$. Both these notions are equivalent and since every Cauchy sum is also a Riemann sum, the inference from Riemann to Cauchy is trivial. The converse appears to be hard and perhaps not popular enough to be seen in textbooks. User Tony Piccolo in his answer gives three references for the proof that Cauchy integrability implies Riemann integrability. It is the second proof from that answer which I want to discuss here (as other two proofs use somewhat complicated ideas and some very non-obvious tricks). This is provided as a hint that Given any partition $P$ of $[a, b]$ and a number $\epsilon > 0$ there is a partition $Q\supseteq P$ of $[a, b]$ such that $C(f, Q) > U(f, P) - \epsilon$. Using the counterpart equation $C(f, P) < L(f, P) + \epsilon$ we can easily show that difference $U(f, P) - L(f, P)$ can be made small if sums $C(f, P)$ tend to a finite limit and thus we get Riemann integrability (via Darboux integrability, also this link between Darboux and Riemann integral is popular and available in good textbooks). Here are my questions: It is easy to prove that we can choose tags $t_{i}$ such that $S(f, P) > U(f, P) - \epsilon$. We just have to choose tags so that $f(t_{i})$ is sufficiently near $M_{i}$. My hunch is that if we add the tags $t_{i}$ to $P$ we get a partition $Q\supseteq P$ and that is the needed partition which ensures $C(f, Q) > U(f, P) - k\epsilon$ where $k$ is some fixed positive constant. Is this correct? And if so how do we go about proving this? Another doubt is whether the relation between $C(f, P)$ and $U(f, P)$ is valid in general? Or does it hold only for Cauchy integrable functions? My guess is that it holds only for Cauchy integrable functions. Is this correct?","There has already been some discussion on this topic . However my question is about a specific solution to this problem and for the benefit of readers I think it is better to add some context (even though it means repetition of some stuff mentioned in the linked question). In what follows $f$ is a function of type $f:[a, b]\to\mathbb{R}$ and $f$ is bounded. A partition $P$ of $[a, b]$ is a set of type $$P = \{x_{0}, x_{1}, x_{2}, \dots, x_{n}\}$$ where $$a = x_{0} < x_{1} < x_{2} < \dots < x_{n} = b$$ The norm $||P||$ of partition $P$ is defined by $||P|| = \max_{i = 1}^{n}(x_{i} - x_{i - 1})$. We define the following sums for $f$ over $P$ \begin{align} C(f, P) &= \sum_{i = 1}^{n}f(x_{i - 1})(x_{i} - x_{i - 1})\notag\\ S(f, P) &= \sum_{i = 1}^{n}f(t_{i})(x_{i} - x_{i - 1})\notag\\ U(f, P) &= \sum_{i = 1}^{n}M_{i}(x_{i} - x_{i - 1})\notag\\ L(f, P) &= \sum_{i = 1}^{n}m_{i}(x_{i} - x_{i - 1})\notag \end{align} where $t_{i}$ are arbitrary points in $[x_{i - 1}, x_{i}]$ and $$M_{i} = \sup\,\{f(x)\mid x\in [x_{i - 1}, x_{i}]\},\,m_{i} = \inf\,\{f(x)\mid x\in [x_{i - 1}, x_{i}]\}$$ The sum $C(f, P)$ is called (left) Cauchy sum for $f$ over $P$. The Riemann sum $S(f, P)$ depends on choice of tags $t_{i}$ but this dependence in not shown in the notation and should be evident from the context. And finally $U(f, P), L(f, P)$ are upper and lower Darboux sums for $f$ over $P$. Cauchy Integral : The function $f$ is said be said to be Cauchy integrable over $[a, b] $ with Cauchy integral $I$ if for every $\epsilon >0$ there is a number $\delta > 0$ such that $|C(f, P) - I| < \epsilon$ whenever $P$ is a partition of $[a, b]$ with $||P|| < \delta$. A similar definition is available for Riemann integral if $C(f, P)$ is replaced by $S(f, P)$. Both these notions are equivalent and since every Cauchy sum is also a Riemann sum, the inference from Riemann to Cauchy is trivial. The converse appears to be hard and perhaps not popular enough to be seen in textbooks. User Tony Piccolo in his answer gives three references for the proof that Cauchy integrability implies Riemann integrability. It is the second proof from that answer which I want to discuss here (as other two proofs use somewhat complicated ideas and some very non-obvious tricks). This is provided as a hint that Given any partition $P$ of $[a, b]$ and a number $\epsilon > 0$ there is a partition $Q\supseteq P$ of $[a, b]$ such that $C(f, Q) > U(f, P) - \epsilon$. Using the counterpart equation $C(f, P) < L(f, P) + \epsilon$ we can easily show that difference $U(f, P) - L(f, P)$ can be made small if sums $C(f, P)$ tend to a finite limit and thus we get Riemann integrability (via Darboux integrability, also this link between Darboux and Riemann integral is popular and available in good textbooks). Here are my questions: It is easy to prove that we can choose tags $t_{i}$ such that $S(f, P) > U(f, P) - \epsilon$. We just have to choose tags so that $f(t_{i})$ is sufficiently near $M_{i}$. My hunch is that if we add the tags $t_{i}$ to $P$ we get a partition $Q\supseteq P$ and that is the needed partition which ensures $C(f, Q) > U(f, P) - k\epsilon$ where $k$ is some fixed positive constant. Is this correct? And if so how do we go about proving this? Another doubt is whether the relation between $C(f, P)$ and $U(f, P)$ is valid in general? Or does it hold only for Cauchy integrable functions? My guess is that it holds only for Cauchy integrable functions. Is this correct?",,"['real-analysis', 'riemann-integration']"
39,Is closeness in total variation preserved in conditioning?,Is closeness in total variation preserved in conditioning?,,"If we have two joint distributions on $(X,Y)$, $P_{X,Y}$ and $Q_{X,Y}$ that are close in $L^1$ or ""total variation"" with $\|P_{X,Y}-Q_{X,Y}\|_1<\varepsilon$ then: Are the distributions on the conditional expectations $P_{E[X|Y]}, Q_{E[X|Y]}$ also close in $L^1$, e.g. $\|P_{E[X|Y]}-Q_{E[X|Y]}\|_1<N\varepsilon$? In particular, what about the case where $Y=X+Z$ with $X\perp Z$? If $X,Y$ are finite mean and variance, then are the means and variances of $E[X|Y]$ under the two distributions close? Assuming they are all continuous RVs, the difference is: \begin{align} \left|P_{E[X|Y]}(w)-Q_{E[X|Y]}(w)\right| &= \left|\int_{y\in A_P} P_Y(y) dy -\int_{y\in A_Q } Q_Y(y) dy\right|  \end{align} with $A_P=\left\{y:\int x P(X=x|Y=y) dx = w\right\},\ A_Q=\left\{y:\int x Q(X=x|Y=y) dx = w\right\}.$ I can't figure out how to make the difference between the two sets small, which makes me suspicious that conditional expectation is unfortunately not continuous in this sense.","If we have two joint distributions on $(X,Y)$, $P_{X,Y}$ and $Q_{X,Y}$ that are close in $L^1$ or ""total variation"" with $\|P_{X,Y}-Q_{X,Y}\|_1<\varepsilon$ then: Are the distributions on the conditional expectations $P_{E[X|Y]}, Q_{E[X|Y]}$ also close in $L^1$, e.g. $\|P_{E[X|Y]}-Q_{E[X|Y]}\|_1<N\varepsilon$? In particular, what about the case where $Y=X+Z$ with $X\perp Z$? If $X,Y$ are finite mean and variance, then are the means and variances of $E[X|Y]$ under the two distributions close? Assuming they are all continuous RVs, the difference is: \begin{align} \left|P_{E[X|Y]}(w)-Q_{E[X|Y]}(w)\right| &= \left|\int_{y\in A_P} P_Y(y) dy -\int_{y\in A_Q } Q_Y(y) dy\right|  \end{align} with $A_P=\left\{y:\int x P(X=x|Y=y) dx = w\right\},\ A_Q=\left\{y:\int x Q(X=x|Y=y) dx = w\right\}.$ I can't figure out how to make the difference between the two sets small, which makes me suspicious that conditional expectation is unfortunately not continuous in this sense.",,"['real-analysis', 'functional-analysis', 'probability-theory', 'conditional-expectation']"
40,"Help solving the differential equation $y''+y'+y=0$ with initial conditions $y(0)=4,y'(0)=-3$ using Laplace transform.",Help solving the differential equation  with initial conditions  using Laplace transform.,"y''+y'+y=0 y(0)=4,y'(0)=-3","UPDATED WTH ANSWER: Please help me solve the following differential equation using Laplace transform: $$y''+y'+y=0;y(0)=4,y'(0)=-3$$ My answer so far: $$\mathcal{L}\{y''\} + \mathcal{L}\{y'\} + \mathcal{L}\{y\}=0$$ We know that $\mathcal{L}\{y'\} = s\mathcal{L}\{y\}-y(o)$ and $\mathcal{L}\{y''\}=s\mathcal{L}\{y'\}-y'(0)$. Upon substituting this to the equation,  it becomes $$s\mathcal{L}\{y'\}-y'(0)+s\mathcal{L}\{y\}-y(0)+\mathcal{L}\{y\}=0$$ $$s(s\mathcal{L}\{y\}-y(0))-y'(0)+s\mathcal{L}\{y\}-y(0)+\mathcal{L}\{y\}=0$$ $$s^2\mathcal{L}\{y\}-sy(0)-y'(0)+s\mathcal{L}\{y\}-y(0)+\mathcal{L}\{y\}=0$$ Upon substituting the values of initial conditions the equation becomes $$s^2\mathcal{L}\{y\}-4s+3+s\mathcal{L}\{y\}-4+\mathcal{L}\{y\}=0$$ Solving for $\mathcal{L}\{y\}$: $$\mathcal{L}\{y\}(s^2+s+1)-4s+3-4=0$$ $$\mathcal{L}\{y\}(s^2+s+1)=4s+1$$ $$\mathcal{L}\{y\}=\frac{4s+1}{s^2+s+1}$$ Modifying the numerator and denominator to obtain the form similar to one of the them in the Laplace transform table .  First applying the method of completing the square to the denominator: $$\frac{4s+1}{s^2+s+1} = \frac{4s+1}{(s^2+s+\frac{1}{4})+(1-\frac{1}{4})}=\frac{4s+1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}$$ Then modifying the numerator: $$\frac{4s+1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}=\frac{4s+2-1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}=\frac{4(s+\frac{1}{2})-1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}$$ Divide the fractions: $$\mathcal{L}\{y\}=\frac{4(s+\frac{1}{2})}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}-\frac{1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}$$ Taking the inverse of Laplace transform of $\mathcal{L}\{y\}$ and let it be $y$. Then the equation becomes $$y=4\mathcal{L}^{-1}[\frac{(s+\frac{1}{2})}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2)}] - \mathcal{L}^{-1}[\frac{1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}]$$ The inverse of the first fraction is: $$4e^{\frac{-1x}{2}}\cos(\frac{\sqrt3x}{2})$$ To match the second fraction to one of the forms from the Laplace Transform table, we need to multiply the numerator and denominator by $\frac{\sqrt3}{2}$. Then it becomes:  $$\frac{2}{\sqrt3}\mathcal{L}^{-1}[\frac{\frac{\sqrt3}{2}}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}]$$ Therefore the inverse of the second fraction is: $$\frac{2}{\sqrt3}e^{\frac{-1x}{2}}\sin(\frac{\sqrt3x}{2})$$ FINAL ANSWER: $$y=4e^{\frac{-1x}{2}}\cos(\frac{\sqrt3x}{2})+\frac{2}{\sqrt3}e^{\frac{-1x}{2}}\sin(\frac{\sqrt3x}{2})$$                       //","UPDATED WTH ANSWER: Please help me solve the following differential equation using Laplace transform: $$y''+y'+y=0;y(0)=4,y'(0)=-3$$ My answer so far: $$\mathcal{L}\{y''\} + \mathcal{L}\{y'\} + \mathcal{L}\{y\}=0$$ We know that $\mathcal{L}\{y'\} = s\mathcal{L}\{y\}-y(o)$ and $\mathcal{L}\{y''\}=s\mathcal{L}\{y'\}-y'(0)$. Upon substituting this to the equation,  it becomes $$s\mathcal{L}\{y'\}-y'(0)+s\mathcal{L}\{y\}-y(0)+\mathcal{L}\{y\}=0$$ $$s(s\mathcal{L}\{y\}-y(0))-y'(0)+s\mathcal{L}\{y\}-y(0)+\mathcal{L}\{y\}=0$$ $$s^2\mathcal{L}\{y\}-sy(0)-y'(0)+s\mathcal{L}\{y\}-y(0)+\mathcal{L}\{y\}=0$$ Upon substituting the values of initial conditions the equation becomes $$s^2\mathcal{L}\{y\}-4s+3+s\mathcal{L}\{y\}-4+\mathcal{L}\{y\}=0$$ Solving for $\mathcal{L}\{y\}$: $$\mathcal{L}\{y\}(s^2+s+1)-4s+3-4=0$$ $$\mathcal{L}\{y\}(s^2+s+1)=4s+1$$ $$\mathcal{L}\{y\}=\frac{4s+1}{s^2+s+1}$$ Modifying the numerator and denominator to obtain the form similar to one of the them in the Laplace transform table .  First applying the method of completing the square to the denominator: $$\frac{4s+1}{s^2+s+1} = \frac{4s+1}{(s^2+s+\frac{1}{4})+(1-\frac{1}{4})}=\frac{4s+1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}$$ Then modifying the numerator: $$\frac{4s+1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}=\frac{4s+2-1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}=\frac{4(s+\frac{1}{2})-1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}$$ Divide the fractions: $$\mathcal{L}\{y\}=\frac{4(s+\frac{1}{2})}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}-\frac{1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}$$ Taking the inverse of Laplace transform of $\mathcal{L}\{y\}$ and let it be $y$. Then the equation becomes $$y=4\mathcal{L}^{-1}[\frac{(s+\frac{1}{2})}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2)}] - \mathcal{L}^{-1}[\frac{1}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}]$$ The inverse of the first fraction is: $$4e^{\frac{-1x}{2}}\cos(\frac{\sqrt3x}{2})$$ To match the second fraction to one of the forms from the Laplace Transform table, we need to multiply the numerator and denominator by $\frac{\sqrt3}{2}$. Then it becomes:  $$\frac{2}{\sqrt3}\mathcal{L}^{-1}[\frac{\frac{\sqrt3}{2}}{(s+\frac{1}{2})^2+(\frac{\sqrt{3}}{2})^2}]$$ Therefore the inverse of the second fraction is: $$\frac{2}{\sqrt3}e^{\frac{-1x}{2}}\sin(\frac{\sqrt3x}{2})$$ FINAL ANSWER: $$y=4e^{\frac{-1x}{2}}\cos(\frac{\sqrt3x}{2})+\frac{2}{\sqrt3}e^{\frac{-1x}{2}}\sin(\frac{\sqrt3x}{2})$$                       //",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'proof-verification', 'laplace-transform']"
41,Uniform Lipschitz constants of family of bilipschitz homeomorphisms,Uniform Lipschitz constants of family of bilipschitz homeomorphisms,,"Let's asume that we are given a family of linear maps $\lbrace{A_i,B_i \rbrace}_{i \in I, A_i,B_i \in Gl_n(\mathbb{Z})}$ and a bilipschitz homeomorphism $F: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$such that for each $i$ there exists a positive constant $C_i$ and a $(K,\lambda)$ quasi isometry ($K,\lambda$ are fixed) $G_i$ such that $sup_{x \in \mathbb{R}^{n}}( ||A_i \circ F \circ B_i(x) - G_i(x)|| < C_i$ (which means that for all $(x,y) \in \mathbb{R}^2 $ we have that $||A_i \circ F \circ B_i(x) - A_i \circ F \circ B_i(y)|| \le C_i +K ||x-y|| $). Does it mean that family of bilipschitz homeomorphisms $\lbrace{ A_i \circ F \circ B_i \rbrace}_{i \in I}$ has uniform lipschitz constant - lispchitz constants in the sense of real analysis ? It's certainly true for $F$ affine or if either $\lbrace{A_i \rbrace}_{i \in I}$ or $\lbrace{B_i \rbrace}_{i \in I}$ have uniform quasi-isometry constants.  The weaker question is - can we choose two constants $(K',C')$ in such a way that all functions $\lbrace{ A_i \circ F \circ B_i \rbrace}_{i \in I}$ are $(K',C')$ coarse (large scale) Lipschitz?","Let's asume that we are given a family of linear maps $\lbrace{A_i,B_i \rbrace}_{i \in I, A_i,B_i \in Gl_n(\mathbb{Z})}$ and a bilipschitz homeomorphism $F: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$such that for each $i$ there exists a positive constant $C_i$ and a $(K,\lambda)$ quasi isometry ($K,\lambda$ are fixed) $G_i$ such that $sup_{x \in \mathbb{R}^{n}}( ||A_i \circ F \circ B_i(x) - G_i(x)|| < C_i$ (which means that for all $(x,y) \in \mathbb{R}^2 $ we have that $||A_i \circ F \circ B_i(x) - A_i \circ F \circ B_i(y)|| \le C_i +K ||x-y|| $). Does it mean that family of bilipschitz homeomorphisms $\lbrace{ A_i \circ F \circ B_i \rbrace}_{i \in I}$ has uniform lipschitz constant - lispchitz constants in the sense of real analysis ? It's certainly true for $F$ affine or if either $\lbrace{A_i \rbrace}_{i \in I}$ or $\lbrace{B_i \rbrace}_{i \in I}$ have uniform quasi-isometry constants.  The weaker question is - can we choose two constants $(K',C')$ in such a way that all functions $\lbrace{ A_i \circ F \circ B_i \rbrace}_{i \in I}$ are $(K',C')$ coarse (large scale) Lipschitz?",,"['real-analysis', 'functional-analysis', 'metric-spaces', 'lipschitz-functions', 'geometric-group-theory']"
42,An almost classic inequality,An almost classic inequality,,"It is a classical exercise to prove that $e^\pi>\pi^e$. But... is there a way to prove $\sin(e^\pi)<\sin(\pi^e)$ without calculator? I was trying to prove that $13\pi/2<\pi^e$ and $e^\pi<15\pi/2$ and use monotone decreasing property of $\sin$ in $[13\pi/2,15\pi/3]$, but i couldn't prove the last inequalities.","It is a classical exercise to prove that $e^\pi>\pi^e$. But... is there a way to prove $\sin(e^\pi)<\sin(\pi^e)$ without calculator? I was trying to prove that $13\pi/2<\pi^e$ and $e^\pi<15\pi/2$ and use monotone decreasing property of $\sin$ in $[13\pi/2,15\pi/3]$, but i couldn't prove the last inequalities.",,"['real-analysis', 'inequality', 'exponential-function', 'pi']"
43,Corollary to Theorem 5.12 in Rudin's PMA: The derivative of a function differentiable on an interval cannot have any simple discontinuities [duplicate],Corollary to Theorem 5.12 in Rudin's PMA: The derivative of a function differentiable on an interval cannot have any simple discontinuities [duplicate],,"This question already has answers here : Discontinuities of the derivative of a differentiable function on closed interval (2 answers) Closed 1 year ago . Here is Theorem 5.12 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real differentiable function on $[a, b]$ and suppose $f^\prime(a) < \lambda < f^\prime(b)$. Then there is a point $x \in (a, b)$ such that $f^\prime(x) = \lambda$. A similar result holds of course if $f^\prime(a) > \lambda > f^\prime(b)$. And, here is the Corollary by Rudin to the above theorem. If $f$ is differentiable on $[a, b]$, then $f^\prime$ cannot have any simple discontinuities on $[a, b]$. Finally, here is Definition 4.26 in Baby Rudin, 3rd edition: Let $f$ be defined on $(a, b)$. If $f$ is discontinuous at a point $x$, and if $f(x+)$ and $f(x-)$ exist, then $f$ is said to have a discontinuity of the first kind , or a simple discontinuity , at $x$. Otherwise the discontinuity is said to be of the second kind . There are two ways in which a function can have a simple discontinuity: either $f(x+) \neq f(x-)$ [in which case the value $f(x)$ is immaterial], or $f(x+) = f(x-) \neq f(x)$. I've just found out that this very question has an answer at the following link. Discontinuities of the derivative of a differentiable function on closed interval However, I'm stuck on the following part of the answer. For the the second kind, say $f'(x-)<f'(x+)$ and pick $\lambda\in(f'(x-),f'(x+))$, and let $u<x<v$ with $u$, $v$ sufficiently close to $x$ so that $f'(z)<\lambda$ for $z\in[u,x)$, and $f'(z)>\lambda$ for $z\in(x,v]$. Why can we not have $f^\prime(x) = \lambda$ in this case (and thus fail to get our desired contradiction)? In order to demonstrate my understanding of the answer, I'll rephrase it, or rather expand upon it. Suppose $f$ is a real function which is differentiable on a closed interval $[a, b]$, and suppose $f^\prime$ has a simple discontinuity at a point $p \in [a, b]$. Then as both $f^\prime(p+)$ and $f^\prime(p-)$ exist, so we must have $p \in (a, b)$. Am I right? Now there are two possible cases, according to whether (i) $f^\prime(p-) = f^\prime(p+) \neq f^\prime(p)$ or (ii) $f^\prime(p-) \neq f^\prime(p+)$. Case (i): We can assume without loss of generality that $f^\prime(p-) = f^\prime(p+) < f^\prime(p)$. Let $\lambda$ be a real number such that    $$\lambda \in \left( \ \lim_{x \to p} f^\prime(x), \ f^\prime(p) \ \right);$$   that is,    $$ \lim_{x \to p} f^\prime(x) < \lambda < f^\prime(p).$$   Now let $\varepsilon$ be any real number such that    $$0 < \varepsilon <  \lambda - \lim_{x \to p} f^\prime(x).$$   Then we can find a real number $\delta > 0$ such that    $$ (p-\delta, p+\delta) \subset (a, b), $$   and    $$ \left\vert f^\prime(x) - \lim_{x \to p} f^\prime(x) \right\vert < \varepsilon$$   for all $x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}$;    that is,    $$ \lim_{x \to p} f^\prime(x)  - \varepsilon < f^\prime(x) < \lim_{x \to p} f^\prime(x) + \varepsilon $$   for all $x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}$. But    $$ \lim_{x \to p} f^\prime(x) + \varepsilon < \lambda.$$   So we can conclude that    $$ f^\prime(x) < \lambda \ \mbox{ for all } \ x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}.$$   Let us put    $$y \colon= p - \frac{\delta}{2}, \ \mbox{ and } \ z \colon= p + \frac{\delta}{2}.$$   Then we can conclude that    $$ f^\prime(x) <  \lambda \ \mbox{ for all } \ x \in [y, p). \tag{1} $$   Thus in particular,    $$f^\prime(y) < \lambda.$$ But we have assumed that    $$\lambda < f^\prime(p). $$    So we must have a point $x \in (y, p)$ for which $f^\prime(x) = \lambda$, which contradicts (1) above. [We can of course take the argument forward with $z$ instead of $y$.] Am I right? Case (ii): We can assume without loss of generality that $f^\prime(p-) < f^\prime(p+)$. Let $\lambda$ be a real number such that    $$ f^\prime(p-) < \lambda < f^\prime(p+).$$   Let's choose a real number $\varepsilon$ such that    $$0 < \varepsilon < \min \left\{ \ f^\prime(p+) - \lambda, \ \lambda - f^\prime(p-) \ \right\}. \tag{2} $$    For this $\varepsilon$, we can find positive real numbers $\delta_1$ and $\delta_2$ such that    $$(p, p+\delta_1) \subset (a, b), \ \mbox{ and }  \ (p-\delta_2, p) \subset (a, b),$$   and also    $$ \left\vert f^\prime(x) - f^\prime(p+) \right\vert < \varepsilon \ \mbox{ whenever } \ x \in (p, p+\delta_1),$$   and    $$ \left\vert f^\prime(x) - f^\prime(p-) \right\vert < \varepsilon \ \mbox{ whenever } \ x \in (p-\delta_2, p).$$   That is,    $$ f^\prime(p+) - \varepsilon < f^\prime(x) < f^\prime(p+) + \varepsilon \ \mbox{ whenever } \ x \in (p, p+\delta_1), \tag{3} $$   and    $$ f^\prime(p-) - \varepsilon < f^\prime(x) < f^\prime(p-) + \varepsilon \ \mbox{ whenever } \ x \in (p-\delta_2, p). \tag{4}$$   But from (2), we note that    $$f^\prime(p-) + \varepsilon < \lambda < f^\prime(p+) - \varepsilon. \tag{5} $$    From (3) and (5) we conclude that    $$ \lambda < f^\prime(x) \ \mbox{ whenever } \ x \in (p, p+\delta_1).$$   And, from (4) and (5) we conclude that    $$f^\prime(x) < \lambda \ \mbox{ whenever} \ x \in (p - \delta_2, p). $$    Then if we put    $$y \colon= p-\frac{\delta_2}{2} \ \mbox{ and } \ z \colon= p + \frac{\delta_1}{2},$$   then we conclude that    $$ \lambda < f^\prime(x) \ \mbox{ whenever } \ x \in (p, z ], \tag{6} $$   and    $$f^\prime(x) < \lambda \ \mbox{ whenever} \ x \in [y, p). \tag{7}$$    In particular,    $$f^\prime(y) < \lambda < f^\prime(z).$$   So there is some point $x \in (y, z)$ such that $$f^\prime(x) = \lambda, $$    which in the light of (6) and (7) implies that $$f^\prime(p) = \lambda.$$ Is my argument up to this point correct? If so, then where do we get our desired contradiction?","This question already has answers here : Discontinuities of the derivative of a differentiable function on closed interval (2 answers) Closed 1 year ago . Here is Theorem 5.12 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real differentiable function on $[a, b]$ and suppose $f^\prime(a) < \lambda < f^\prime(b)$. Then there is a point $x \in (a, b)$ such that $f^\prime(x) = \lambda$. A similar result holds of course if $f^\prime(a) > \lambda > f^\prime(b)$. And, here is the Corollary by Rudin to the above theorem. If $f$ is differentiable on $[a, b]$, then $f^\prime$ cannot have any simple discontinuities on $[a, b]$. Finally, here is Definition 4.26 in Baby Rudin, 3rd edition: Let $f$ be defined on $(a, b)$. If $f$ is discontinuous at a point $x$, and if $f(x+)$ and $f(x-)$ exist, then $f$ is said to have a discontinuity of the first kind , or a simple discontinuity , at $x$. Otherwise the discontinuity is said to be of the second kind . There are two ways in which a function can have a simple discontinuity: either $f(x+) \neq f(x-)$ [in which case the value $f(x)$ is immaterial], or $f(x+) = f(x-) \neq f(x)$. I've just found out that this very question has an answer at the following link. Discontinuities of the derivative of a differentiable function on closed interval However, I'm stuck on the following part of the answer. For the the second kind, say $f'(x-)<f'(x+)$ and pick $\lambda\in(f'(x-),f'(x+))$, and let $u<x<v$ with $u$, $v$ sufficiently close to $x$ so that $f'(z)<\lambda$ for $z\in[u,x)$, and $f'(z)>\lambda$ for $z\in(x,v]$. Why can we not have $f^\prime(x) = \lambda$ in this case (and thus fail to get our desired contradiction)? In order to demonstrate my understanding of the answer, I'll rephrase it, or rather expand upon it. Suppose $f$ is a real function which is differentiable on a closed interval $[a, b]$, and suppose $f^\prime$ has a simple discontinuity at a point $p \in [a, b]$. Then as both $f^\prime(p+)$ and $f^\prime(p-)$ exist, so we must have $p \in (a, b)$. Am I right? Now there are two possible cases, according to whether (i) $f^\prime(p-) = f^\prime(p+) \neq f^\prime(p)$ or (ii) $f^\prime(p-) \neq f^\prime(p+)$. Case (i): We can assume without loss of generality that $f^\prime(p-) = f^\prime(p+) < f^\prime(p)$. Let $\lambda$ be a real number such that    $$\lambda \in \left( \ \lim_{x \to p} f^\prime(x), \ f^\prime(p) \ \right);$$   that is,    $$ \lim_{x \to p} f^\prime(x) < \lambda < f^\prime(p).$$   Now let $\varepsilon$ be any real number such that    $$0 < \varepsilon <  \lambda - \lim_{x \to p} f^\prime(x).$$   Then we can find a real number $\delta > 0$ such that    $$ (p-\delta, p+\delta) \subset (a, b), $$   and    $$ \left\vert f^\prime(x) - \lim_{x \to p} f^\prime(x) \right\vert < \varepsilon$$   for all $x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}$;    that is,    $$ \lim_{x \to p} f^\prime(x)  - \varepsilon < f^\prime(x) < \lim_{x \to p} f^\prime(x) + \varepsilon $$   for all $x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}$. But    $$ \lim_{x \to p} f^\prime(x) + \varepsilon < \lambda.$$   So we can conclude that    $$ f^\prime(x) < \lambda \ \mbox{ for all } \ x \in (p-\delta, p+\delta) \setminus \left\{ p \right\}.$$   Let us put    $$y \colon= p - \frac{\delta}{2}, \ \mbox{ and } \ z \colon= p + \frac{\delta}{2}.$$   Then we can conclude that    $$ f^\prime(x) <  \lambda \ \mbox{ for all } \ x \in [y, p). \tag{1} $$   Thus in particular,    $$f^\prime(y) < \lambda.$$ But we have assumed that    $$\lambda < f^\prime(p). $$    So we must have a point $x \in (y, p)$ for which $f^\prime(x) = \lambda$, which contradicts (1) above. [We can of course take the argument forward with $z$ instead of $y$.] Am I right? Case (ii): We can assume without loss of generality that $f^\prime(p-) < f^\prime(p+)$. Let $\lambda$ be a real number such that    $$ f^\prime(p-) < \lambda < f^\prime(p+).$$   Let's choose a real number $\varepsilon$ such that    $$0 < \varepsilon < \min \left\{ \ f^\prime(p+) - \lambda, \ \lambda - f^\prime(p-) \ \right\}. \tag{2} $$    For this $\varepsilon$, we can find positive real numbers $\delta_1$ and $\delta_2$ such that    $$(p, p+\delta_1) \subset (a, b), \ \mbox{ and }  \ (p-\delta_2, p) \subset (a, b),$$   and also    $$ \left\vert f^\prime(x) - f^\prime(p+) \right\vert < \varepsilon \ \mbox{ whenever } \ x \in (p, p+\delta_1),$$   and    $$ \left\vert f^\prime(x) - f^\prime(p-) \right\vert < \varepsilon \ \mbox{ whenever } \ x \in (p-\delta_2, p).$$   That is,    $$ f^\prime(p+) - \varepsilon < f^\prime(x) < f^\prime(p+) + \varepsilon \ \mbox{ whenever } \ x \in (p, p+\delta_1), \tag{3} $$   and    $$ f^\prime(p-) - \varepsilon < f^\prime(x) < f^\prime(p-) + \varepsilon \ \mbox{ whenever } \ x \in (p-\delta_2, p). \tag{4}$$   But from (2), we note that    $$f^\prime(p-) + \varepsilon < \lambda < f^\prime(p+) - \varepsilon. \tag{5} $$    From (3) and (5) we conclude that    $$ \lambda < f^\prime(x) \ \mbox{ whenever } \ x \in (p, p+\delta_1).$$   And, from (4) and (5) we conclude that    $$f^\prime(x) < \lambda \ \mbox{ whenever} \ x \in (p - \delta_2, p). $$    Then if we put    $$y \colon= p-\frac{\delta_2}{2} \ \mbox{ and } \ z \colon= p + \frac{\delta_1}{2},$$   then we conclude that    $$ \lambda < f^\prime(x) \ \mbox{ whenever } \ x \in (p, z ], \tag{6} $$   and    $$f^\prime(x) < \lambda \ \mbox{ whenever} \ x \in [y, p). \tag{7}$$    In particular,    $$f^\prime(y) < \lambda < f^\prime(z).$$   So there is some point $x \in (y, z)$ such that $$f^\prime(x) = \lambda, $$    which in the light of (6) and (7) implies that $$f^\prime(p) = \lambda.$$ Is my argument up to this point correct? If so, then where do we get our desired contradiction?",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
44,Convergence of $\sum_k \epsilon_k a_k$ implies $(\epsilon_1 + \cdots +\epsilon_k)a_k \rightarrow 0$,Convergence of  implies,\sum_k \epsilon_k a_k (\epsilon_1 + \cdots +\epsilon_k)a_k \rightarrow 0,"Let $\epsilon_k \in \{\pm 1\}$, and suppose $\sum_k \epsilon_k a_k$ is convergent, where $a_k \geq a_{k+1} \geq 0$. Prove that $(\epsilon_1 + \cdots + \epsilon_k)a_k \rightarrow 0$ as $k \rightarrow \infty$.","Let $\epsilon_k \in \{\pm 1\}$, and suppose $\sum_k \epsilon_k a_k$ is convergent, where $a_k \geq a_{k+1} \geq 0$. Prove that $(\epsilon_1 + \cdots + \epsilon_k)a_k \rightarrow 0$ as $k \rightarrow \infty$.",,"['calculus', 'real-analysis', 'sequences-and-series']"
45,Show that if $f(x+y)=g(x)+g(y) $ a.e. then functions are linear,Show that if  a.e. then functions are linear,f(x+y)=g(x)+g(y) ,"Let  $f(x), g(x), h(x)$ be measureable functions such that \begin{align} f(x+y)=g(x)+h(y), \end{align} almost everywhere $(x,y) \in \mathbb{R}^2$. Can we show that  \begin{align} g(x)&=ax+b_1,\\ h(x)&=ax+b_2, \end{align} almost everywhere $(x,y) \in \mathbb{R}^2$ for some $a,b_1,b_2$. My attempt: Which I think is problematic. Since, $f(x+y)=g(x)+h(y)$, we have that  \begin{align} f(x+0)=g(x)+h(0)\\ f(0+y)=g(0)+h(y) \end{align} Now adding the two equations and use our identity we get \begin{align} f(x+0)+f(y+0)=g(x)+h(y)+h(0)+g(0)= f(x+y)+f(0). \end{align} So, we have that \begin{align} f(x)+f(y)=f(x+y)+f(0) \end{align} Next, difine a function $\phi(x)=f(x)-f(0)$ and we get  \begin{align} \phi(x)+\phi(y)=\phi(x+y). \end{align} The above is know as Cauchy's functional equation and if $\phi(x)$ is measureable (which it is since $f$ is measurable) then it must be linear (i.e., $\phi(x)=ax$). Going backward this shows that $g(x)$ and $h(x)$ are given by \begin{align} g(x)&=ax+b_1,\\ h(x)&=ax+b_2, \end{align} for some $a,b_1,b_2$. Questions: 1) Is this proof correct? 2) Since, we are assuming that equation $f(x+y)=g(x)+h(y)$ holds almost everywhere and not everywhere. Is this problem?   For example, in the very first step of the proof \begin{align} f(x+0)=g(x)+h(0)\\ f(0+y)=g(0)+h(y) \end{align} can we do this? Is there problem of choosing $(x,0)$ and $(0,y)$? I think there is a possibility that $(x,0)$ and $(0,y)$ might belong to a set of measure zero on which the equations don't hold. 3) Another, question very similar to 2). Since,  $\phi(x)+\phi(y)=\phi(x+y)$ holds a.e. can we apply Cauchy equation?","Let  $f(x), g(x), h(x)$ be measureable functions such that \begin{align} f(x+y)=g(x)+h(y), \end{align} almost everywhere $(x,y) \in \mathbb{R}^2$. Can we show that  \begin{align} g(x)&=ax+b_1,\\ h(x)&=ax+b_2, \end{align} almost everywhere $(x,y) \in \mathbb{R}^2$ for some $a,b_1,b_2$. My attempt: Which I think is problematic. Since, $f(x+y)=g(x)+h(y)$, we have that  \begin{align} f(x+0)=g(x)+h(0)\\ f(0+y)=g(0)+h(y) \end{align} Now adding the two equations and use our identity we get \begin{align} f(x+0)+f(y+0)=g(x)+h(y)+h(0)+g(0)= f(x+y)+f(0). \end{align} So, we have that \begin{align} f(x)+f(y)=f(x+y)+f(0) \end{align} Next, difine a function $\phi(x)=f(x)-f(0)$ and we get  \begin{align} \phi(x)+\phi(y)=\phi(x+y). \end{align} The above is know as Cauchy's functional equation and if $\phi(x)$ is measureable (which it is since $f$ is measurable) then it must be linear (i.e., $\phi(x)=ax$). Going backward this shows that $g(x)$ and $h(x)$ are given by \begin{align} g(x)&=ax+b_1,\\ h(x)&=ax+b_2, \end{align} for some $a,b_1,b_2$. Questions: 1) Is this proof correct? 2) Since, we are assuming that equation $f(x+y)=g(x)+h(y)$ holds almost everywhere and not everywhere. Is this problem?   For example, in the very first step of the proof \begin{align} f(x+0)=g(x)+h(0)\\ f(0+y)=g(0)+h(y) \end{align} can we do this? Is there problem of choosing $(x,0)$ and $(0,y)$? I think there is a possibility that $(x,0)$ and $(0,y)$ might belong to a set of measure zero on which the equations don't hold. 3) Another, question very similar to 2). Since,  $\phi(x)+\phi(y)=\phi(x+y)$ holds a.e. can we apply Cauchy equation?",,"['real-analysis', 'functional-analysis', 'functional-equations']"
46,Nonlinear Grnwall inequality,Nonlinear Grnwall inequality,,"Let $T>0$, $\alpha,\beta>0$ and consider a non-negative continuous function $x$ on $[0,T]$ such that for all $t \in [0,T]$ one has $$x(t) \leq \alpha+\beta\left(\int_0^t x(s)\,\mathrm ds \right)^{1/2}.$$ Does anyone knows what kind of Grnwall inequality I can get from this ? It would be fantastic if I can get something like $x(t) \leq Ct$.","Let $T>0$, $\alpha,\beta>0$ and consider a non-negative continuous function $x$ on $[0,T]$ such that for all $t \in [0,T]$ one has $$x(t) \leq \alpha+\beta\left(\int_0^t x(s)\,\mathrm ds \right)^{1/2}.$$ Does anyone knows what kind of Grnwall inequality I can get from this ? It would be fantastic if I can get something like $x(t) \leq Ct$.",,"['real-analysis', 'integral-inequality', 'functional-inequalities']"
47,Is there a function whose all limits at rational points approach infinity? [closed],Is there a function whose all limits at rational points approach infinity? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Is there a function $f\colon\mathbb R \to \mathbb R$, such that its limits at rational points approach infinity?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Is there a function $f\colon\mathbb R \to \mathbb R$, such that its limits at rational points approach infinity?",,"['calculus', 'real-analysis']"
48,Approximating a Lebesgue measurable set by a finite union of intervals,Approximating a Lebesgue measurable set by a finite union of intervals,,"I am reading Real Analysis by Yeh and have a question about the following result (Thm 3.25 in the book) Theorem If $E \in \mathfrak{M}_L$ and $\mu_L(E)< \infty$, then $\forall \epsilon>0$ $\exists$ finitely many open intervals $I_1, \ldots, I_N$ s.t. $\mu_L(E \triangle \cup_{n=1}^NI_n)< \epsilon$. Notations: $\mathfrak{M}_L$ is the $\sigma$-algebra of Lebesgue measurable subsets of $\mathbb{R}$, $\mu_L$ denotes Lebesgue measure.  If we're not sure $E \in \mathfrak{M}_L$, then $\mu_L^*(E)$ denotes the Lebesgue outer measure of E.  For intervals $I$, Yeh sometimes uses $\ell(I)$ to denote their length.  The symmetric difference of 2 sets is $A \triangle B:=(A \setminus B) \cup (B \setminus A)$. I didn't know how to prove this, so I read Yeh's proof.  After a while I tried to prove it again but came up with something simpler that seems to work, so wanted to ask if I might have missed something.  My attempt is below, followed by Yeh's proof (it's long, so I of course want something simpler in my notes if possible).  Thanks in advance for any help. My attempt: Fix $\epsilon>0$.  Use the definition of outer measure as an infimum to pick a sequence $(I_n)$ of open intervals s.t.  $E \subseteq \cup_{n=1}^ \infty I_n$ and $\sum_{n=1}^\infty \mu_L(I_n) \leq \mu_L(E) + \epsilon$. Since $\mu_L(E)< \infty$, $\sum_{n=1}^\infty \mu_L(I_n)$ converges, so pick $N \in \mathbb{N}$ s.t. $\sum_{n=N+1}^\infty \mu_L(I_n)< \epsilon$. Then \begin{split} \mu_L(E \triangle \cup_{n=1}^NI_n) &= \mu_L \big( (E \setminus \cup_{n=1}^N I_n) \cup ( \cup_{n=1}^N I_n \setminus E) \big) \\ & \leq \mu_L(E \setminus \cup_{n=1}^N I_n)+\mu_L( \cup_{n=1}^N I_n \setminus E) \\ & \leq \mu_L(\cup_{n=1}^\infty I_n \setminus \cup_{n=1}^N I_n)+\mu_L( \cup_{n=1}^\infty I_n \setminus E) \\ & \leq \mu_L(\cup_{n=N+1}^\infty I_n)+\mu_L( \cup_{n=1}^\infty I_n)- \mu_L(E) \\ & \leq \sum_{n=N+1}^\infty \mu_L(I_n)+ \sum_{n=1}^\infty \mu_L(I_n)- \mu_L(E) <2 \epsilon  \end{split} where we have repeatedly used that a measure is monotone and subadditive.   In the 4th line, we have also used an earlier fact that $A,B \in \mathfrak{M}_L$, $A \subseteq B$, $\mu_L(A)< \infty$ implies $\mu_L(B \setminus A)= \mu_L(B)- \mu_L(A)$.  Since $\epsilon$ is arbitrary, this is the desired result. (I'm cutting the proof here, but Yeh pretty much bounds each of the 3 pieces on the RHS separately after this).","I am reading Real Analysis by Yeh and have a question about the following result (Thm 3.25 in the book) Theorem If $E \in \mathfrak{M}_L$ and $\mu_L(E)< \infty$, then $\forall \epsilon>0$ $\exists$ finitely many open intervals $I_1, \ldots, I_N$ s.t. $\mu_L(E \triangle \cup_{n=1}^NI_n)< \epsilon$. Notations: $\mathfrak{M}_L$ is the $\sigma$-algebra of Lebesgue measurable subsets of $\mathbb{R}$, $\mu_L$ denotes Lebesgue measure.  If we're not sure $E \in \mathfrak{M}_L$, then $\mu_L^*(E)$ denotes the Lebesgue outer measure of E.  For intervals $I$, Yeh sometimes uses $\ell(I)$ to denote their length.  The symmetric difference of 2 sets is $A \triangle B:=(A \setminus B) \cup (B \setminus A)$. I didn't know how to prove this, so I read Yeh's proof.  After a while I tried to prove it again but came up with something simpler that seems to work, so wanted to ask if I might have missed something.  My attempt is below, followed by Yeh's proof (it's long, so I of course want something simpler in my notes if possible).  Thanks in advance for any help. My attempt: Fix $\epsilon>0$.  Use the definition of outer measure as an infimum to pick a sequence $(I_n)$ of open intervals s.t.  $E \subseteq \cup_{n=1}^ \infty I_n$ and $\sum_{n=1}^\infty \mu_L(I_n) \leq \mu_L(E) + \epsilon$. Since $\mu_L(E)< \infty$, $\sum_{n=1}^\infty \mu_L(I_n)$ converges, so pick $N \in \mathbb{N}$ s.t. $\sum_{n=N+1}^\infty \mu_L(I_n)< \epsilon$. Then \begin{split} \mu_L(E \triangle \cup_{n=1}^NI_n) &= \mu_L \big( (E \setminus \cup_{n=1}^N I_n) \cup ( \cup_{n=1}^N I_n \setminus E) \big) \\ & \leq \mu_L(E \setminus \cup_{n=1}^N I_n)+\mu_L( \cup_{n=1}^N I_n \setminus E) \\ & \leq \mu_L(\cup_{n=1}^\infty I_n \setminus \cup_{n=1}^N I_n)+\mu_L( \cup_{n=1}^\infty I_n \setminus E) \\ & \leq \mu_L(\cup_{n=N+1}^\infty I_n)+\mu_L( \cup_{n=1}^\infty I_n)- \mu_L(E) \\ & \leq \sum_{n=N+1}^\infty \mu_L(I_n)+ \sum_{n=1}^\infty \mu_L(I_n)- \mu_L(E) <2 \epsilon  \end{split} where we have repeatedly used that a measure is monotone and subadditive.   In the 4th line, we have also used an earlier fact that $A,B \in \mathfrak{M}_L$, $A \subseteq B$, $\mu_L(A)< \infty$ implies $\mu_L(B \setminus A)= \mu_L(B)- \mu_L(A)$.  Since $\epsilon$ is arbitrary, this is the desired result. (I'm cutting the proof here, but Yeh pretty much bounds each of the 3 pieces on the RHS separately after this).",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
49,Is every $F_{\sigma\delta}$-set a set of points of convergence of a sequence of continuous functions?,Is every -set a set of points of convergence of a sequence of continuous functions?,F_{\sigma\delta},"It is well known that if $\langle f_n:n\in\mathbb{N}\rangle$ is a sequence of continuous functions, $f_n\colon\mathbb{R}\to\mathbb{R}$, then $\big\{x\in\mathbb{R}:\lim_{n\to\infty}f_n(x)\text{ exists}\big\}$ is an $F_{\sigma\delta}$-set ( see this post ). I am asking if the converse is true, i.e., whether for every $F_{\sigma\delta}$-set $E\subseteq\mathbb{R}$ there exists a sequence $\langle f_n:n\in\mathbb{N}\rangle$ of continuous functions, $f_n\colon\mathbb{R}\to\mathbb{R}$, such that $\big\{x\in\mathbb{R}:\lim_{n\to\infty}f_n(x)\text{ exists}\big\}=E$. My attempt : I would try to prove it in two steps. (1) Given an $F_{\sigma\delta}$-set $E$, find closed sets $E^k_n$, $n,k\in\mathbb{N}$, such that $E^k_n\supseteq E^l_n$ and $E^k_n\subseteq E^k_m$ for $k\le l$ and $n\le m$, and $E=\bigcap_k\bigcup_n E^k_n$. (2) Given $E^k_n$ as above, find continuous functions $f_n\colon\mathbb{R}\to\mathbb{R}$ such that for every $x$, $x\in E^k_N$ iff $\left|f_n(x)-f_m(x)\right|\le 2^{-k}$ for all $m\ge n\ge N$. (1) would be accomplished as follows. Let $E=\bigcap_k\bigcup_n F^k_n$, $F^k_n$ closed. Let $\langle G^0_n:n\in\mathbb{N}\rangle$ consists of all elements of $\langle F^0_n:n\in\mathbb{N}\rangle$, each repeating infinitely many times. Let $\langle G^1_n:n\in\mathbb{N}\rangle$ consists of all possible intersections $F^0_i\cap F^1_j$, $i,j\in\mathbb{N}$, each repeating infinitely many times and ordered so that $G^1_n\subseteq G^0_n$ for every $n$. Similarly, let $\langle G^k_n:n\in\mathbb{N}\rangle$ consists of all possible intersections $G^0_{i_0}\cap\cdots\cap G^k_{i_k}$, $i_0\dots,i_k\in\mathbb{N}$, each repeating infinitely many times and ordered so that $G^k_n\subseteq G^l_n$ for every $n$, whenever $l<k$. This way we obtain $\bigcup_n G^k_n=\bigcap_{l\le k}\bigcup_n F^l_n$. Finally, put $E^k_n=\bigcup_{m\le n}G^k_m$. Then $\bigcup_n E^k_n=\bigcup_n G^k_n$ and hence $\bigcap_k\bigcup_n E^k_n=\bigcap_k\bigcup_n G^k_n=\bigcap_k\bigcup_n F^k_n=E$. Is that correct? And can (2) be accomplished, too?","It is well known that if $\langle f_n:n\in\mathbb{N}\rangle$ is a sequence of continuous functions, $f_n\colon\mathbb{R}\to\mathbb{R}$, then $\big\{x\in\mathbb{R}:\lim_{n\to\infty}f_n(x)\text{ exists}\big\}$ is an $F_{\sigma\delta}$-set ( see this post ). I am asking if the converse is true, i.e., whether for every $F_{\sigma\delta}$-set $E\subseteq\mathbb{R}$ there exists a sequence $\langle f_n:n\in\mathbb{N}\rangle$ of continuous functions, $f_n\colon\mathbb{R}\to\mathbb{R}$, such that $\big\{x\in\mathbb{R}:\lim_{n\to\infty}f_n(x)\text{ exists}\big\}=E$. My attempt : I would try to prove it in two steps. (1) Given an $F_{\sigma\delta}$-set $E$, find closed sets $E^k_n$, $n,k\in\mathbb{N}$, such that $E^k_n\supseteq E^l_n$ and $E^k_n\subseteq E^k_m$ for $k\le l$ and $n\le m$, and $E=\bigcap_k\bigcup_n E^k_n$. (2) Given $E^k_n$ as above, find continuous functions $f_n\colon\mathbb{R}\to\mathbb{R}$ such that for every $x$, $x\in E^k_N$ iff $\left|f_n(x)-f_m(x)\right|\le 2^{-k}$ for all $m\ge n\ge N$. (1) would be accomplished as follows. Let $E=\bigcap_k\bigcup_n F^k_n$, $F^k_n$ closed. Let $\langle G^0_n:n\in\mathbb{N}\rangle$ consists of all elements of $\langle F^0_n:n\in\mathbb{N}\rangle$, each repeating infinitely many times. Let $\langle G^1_n:n\in\mathbb{N}\rangle$ consists of all possible intersections $F^0_i\cap F^1_j$, $i,j\in\mathbb{N}$, each repeating infinitely many times and ordered so that $G^1_n\subseteq G^0_n$ for every $n$. Similarly, let $\langle G^k_n:n\in\mathbb{N}\rangle$ consists of all possible intersections $G^0_{i_0}\cap\cdots\cap G^k_{i_k}$, $i_0\dots,i_k\in\mathbb{N}$, each repeating infinitely many times and ordered so that $G^k_n\subseteq G^l_n$ for every $n$, whenever $l<k$. This way we obtain $\bigcup_n G^k_n=\bigcap_{l\le k}\bigcup_n F^l_n$. Finally, put $E^k_n=\bigcup_{m\le n}G^k_m$. Then $\bigcup_n E^k_n=\bigcup_n G^k_n$ and hence $\bigcap_k\bigcup_n E^k_n=\bigcap_k\bigcup_n G^k_n=\bigcap_k\bigcup_n F^k_n=E$. Is that correct? And can (2) be accomplished, too?",,"['real-analysis', 'general-topology', 'convergence-divergence', 'descriptive-set-theory']"
50,"Prob. 5, Chap. 4 in Baby Rudin: Continuous extension of a function defined on a closed set","Prob. 5, Chap. 4 in Baby Rudin: Continuous extension of a function defined on a closed set",,"Here is Prob. 5, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f$ is a real continuous function defined on a closed set $E \subset \mathbb{R}^1$ , prove that there exist real continuous functions $g$ on $\mathbb{R}^1$ such that $g(x) = f(x)$ for all $x \in E$ . (Such functions $g$ are called continuous extensions of $f$ from $E$ to $\mathbb{R}^1$ .) Show that the result becomes false if the word ""closed"" is omitted. Extend the result to vector-valued functions. Hint: Let the graph of $g$ be a straight line on each of the segments which constitute the complement of $E$ (compare Exercise 29, Chap. 2). The result remains true if $\mathbb{R}^1$ is replaced by any metric space, but the proof is not so simple. First of all, we show that the word ""closed"" is essential. Let $E = (-\infty, 0) \cup (0, +\infty)$ , and let $f \colon  E \to \mathbb{R}^1$ be defined as $$ f(x) = \frac{1}{x} \ \mbox{ for all } x \in E.$$ Then the set $E$ is not closed in $\mathbb{R}^1$ , but there is no continuous function $g \colon \mathbb{R}^1 \to \mathbb{R}^1$ such that $g(x) = f(x)$ for all $x \in E$ , although $f$ is certainly continuous. Am I right? Now we show the main result: First of all, here is Exercise 29, Chap. 2 in Baby Rudin, 3rd edition: Prove that every open set in $\mathbb{R}^1$ is the union of an at most countable collection of disjoint segments. ... Since $\mathbb{R}^1 - E$ is open in $\mathbb{R}^1$ , it is the union of an at most countable collection of disjoint segments $\left\{ \left(a_n, b_n \right) \right\}_{n \in K}$ , where $K$ is either the set $J = \left\{ 1, 2, 3, \ldots \right\}$ or $K$ is some $J_N = \left\{ 1, \ldots, N \right\}$ for some $N \in \mathbb{N}$ ; moreover, $\left( a_m, b_m \right) \cap \left( a_n, b_n \right) = \emptyset$ for any two distinct $m, n \in K$ ; finally some $\left( a_n, b_n \right)$ can possibly be infinite. Now since $$\mathbb{R}^1 - E = \cup_{n \in K} \left( a_n, b_n \right),$$ each of the (finite) endpoints of these segments is an element of set $E$ . Now if some segment $\left( a_n, b_n \right)$ has $-\infty$ as its left endpoint, then we put $g(x) = f \left( b_n \right)$ for all $x \in \left( a_n, b_n \right)$ . If some segment $\left(a_m, b_m \right)$ has $+\infty$ as its right endpoint, then we put $g(x) = f\left( a_m \right)$ for all $x \in \left( a_m, b_m \right)$ . And, for any segment $\left( a_n, b_n \right)$ , where $-\infty < a_n < b_n < +\infty$ , we put $$ g(x) = f\left( a_n \right) + \left[ \frac{ f\left( b_n \right) - f\left( a_n \right)}{b_n - a_n} \right] \left( x - a_n \right) $$ for all $x \in \left( a_n, b_n \right)$ . I hope I've used the hint given by Rudin correctly. If so, then how to rigorously show that the resulting real function $g$ defined on $\mathbb{R}^1$ is continuous on all of $\mathbb{R}^1$ ? This is intuitively clear though, but how does the countability of these segments become significant? Up to this point, Rudin hasn't stated any result about the continuity of a function defined using several continuous functions as pieces, like our function $g$ is defined here. Moreover, are there any other possibilities for $g$ besides the one we have defined above? Now we state the generalization of the above result for vector-valued functions. Let $E$ be a closed set in $\mathbb{R}^1$ , and let $\mathbf{f}$ be a continuous function defined on $E$ with values in some $\mathbb{R}^k$ . Then there are continuous functions $\mathbf{g}$ defined on $\mathbb{R}^1$ with values in the same $\mathbb{R}^k$ such that $\mathbf{g}(x) = \mathbb{f}(x)$ for all $x \in E$ . In order to prove this generalized result, let's put $\mathbb{f}(x) = \left( f_1(x), \ldots, f_k(x) \right)$ for all $x \in E$ , where $f_1, \ldots, f_k$ are real continuous functions defined on $E$ , and each of these functions we can continuously extend to all of $\mathbb{R}^1$ , thereby getting a continuous extension $\mathbf{g}$ of $\mathbf{f}$ from $E$ to all of $\mathbb{R}^1$ . Is this reasoning correct? Last but not the least, I have the following query. Let $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$ be metric spaces, let $E \subset X$ such that $E$ is closed in $\left( X, d_X \right)$ , and let $f$ be a continuous mapping of the induced metric space $E$ into the metric space $Y$ . Then can we find---or prove the existence of---a continuous mapping $g$ of $X$ into $Y$ such that $g(x) = f(x)$ for all $x \in E$ ? If so, then how to find such a map or prove it exists? If not, then what is the general result that Rudin is referring to and how to come up with the not-so-simple proof that he has alluded to?","Here is Prob. 5, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If is a real continuous function defined on a closed set , prove that there exist real continuous functions on such that for all . (Such functions are called continuous extensions of from to .) Show that the result becomes false if the word ""closed"" is omitted. Extend the result to vector-valued functions. Hint: Let the graph of be a straight line on each of the segments which constitute the complement of (compare Exercise 29, Chap. 2). The result remains true if is replaced by any metric space, but the proof is not so simple. First of all, we show that the word ""closed"" is essential. Let , and let be defined as Then the set is not closed in , but there is no continuous function such that for all , although is certainly continuous. Am I right? Now we show the main result: First of all, here is Exercise 29, Chap. 2 in Baby Rudin, 3rd edition: Prove that every open set in is the union of an at most countable collection of disjoint segments. ... Since is open in , it is the union of an at most countable collection of disjoint segments , where is either the set or is some for some ; moreover, for any two distinct ; finally some can possibly be infinite. Now since each of the (finite) endpoints of these segments is an element of set . Now if some segment has as its left endpoint, then we put for all . If some segment has as its right endpoint, then we put for all . And, for any segment , where , we put for all . I hope I've used the hint given by Rudin correctly. If so, then how to rigorously show that the resulting real function defined on is continuous on all of ? This is intuitively clear though, but how does the countability of these segments become significant? Up to this point, Rudin hasn't stated any result about the continuity of a function defined using several continuous functions as pieces, like our function is defined here. Moreover, are there any other possibilities for besides the one we have defined above? Now we state the generalization of the above result for vector-valued functions. Let be a closed set in , and let be a continuous function defined on with values in some . Then there are continuous functions defined on with values in the same such that for all . In order to prove this generalized result, let's put for all , where are real continuous functions defined on , and each of these functions we can continuously extend to all of , thereby getting a continuous extension of from to all of . Is this reasoning correct? Last but not the least, I have the following query. Let and be metric spaces, let such that is closed in , and let be a continuous mapping of the induced metric space into the metric space . Then can we find---or prove the existence of---a continuous mapping of into such that for all ? If so, then how to find such a map or prove it exists? If not, then what is the general result that Rudin is referring to and how to come up with the not-so-simple proof that he has alluded to?","f E \subset \mathbb{R}^1 g \mathbb{R}^1 g(x) = f(x) x \in E g f E \mathbb{R}^1 g E \mathbb{R}^1 E = (-\infty, 0) \cup (0, +\infty) f \colon  E \to \mathbb{R}^1  f(x) = \frac{1}{x} \ \mbox{ for all } x \in E. E \mathbb{R}^1 g \colon \mathbb{R}^1 \to \mathbb{R}^1 g(x) = f(x) x \in E f \mathbb{R}^1 \mathbb{R}^1 - E \mathbb{R}^1 \left\{ \left(a_n, b_n \right) \right\}_{n \in K} K J = \left\{ 1, 2, 3, \ldots \right\} K J_N = \left\{ 1, \ldots, N \right\} N \in \mathbb{N} \left( a_m, b_m \right) \cap \left( a_n, b_n \right) = \emptyset m, n \in K \left( a_n, b_n \right) \mathbb{R}^1 - E = \cup_{n \in K} \left( a_n, b_n \right), E \left( a_n, b_n \right) -\infty g(x) = f \left( b_n \right) x \in \left( a_n, b_n \right) \left(a_m, b_m \right) +\infty g(x) = f\left( a_m \right) x \in \left( a_m, b_m \right) \left( a_n, b_n \right) -\infty < a_n < b_n < +\infty  g(x) = f\left( a_n \right) + \left[ \frac{ f\left( b_n \right) - f\left( a_n \right)}{b_n - a_n} \right] \left( x - a_n \right)  x \in \left( a_n, b_n \right) g \mathbb{R}^1 \mathbb{R}^1 g g E \mathbb{R}^1 \mathbf{f} E \mathbb{R}^k \mathbf{g} \mathbb{R}^1 \mathbb{R}^k \mathbf{g}(x) = \mathbb{f}(x) x \in E \mathbb{f}(x) = \left( f_1(x), \ldots, f_k(x) \right) x \in E f_1, \ldots, f_k E \mathbb{R}^1 \mathbf{g} \mathbf{f} E \mathbb{R}^1 \left( X, d_X \right) \left( Y, d_Y \right) E \subset X E \left( X, d_X \right) f E Y g X Y g(x) = f(x) x \in E","['calculus', 'real-analysis', 'analysis', 'metric-spaces', 'continuity']"
51,Continuity and Right Derivative,Continuity and Right Derivative,,"$f(x)$ is continuous on $\mathbb{R}$, $f_+'(x)$ exists and is continuous on $\mathbb{R}$. Prove that $f'(x) $ exists on $\mathbb{R}$. It's OK that $|f_+'(x)-f_+'(y)|<\epsilon$ if x and y are enclosed in a small interval. For a fixed x, we have $|\frac{f(y)-f(x)}{y-x}-f_+'(x)|<\epsilon$ if $y\in (x,x+\delta)$ but $\delta$ depends greatly on $x$. I wonder if there are better methods.","$f(x)$ is continuous on $\mathbb{R}$, $f_+'(x)$ exists and is continuous on $\mathbb{R}$. Prove that $f'(x) $ exists on $\mathbb{R}$. It's OK that $|f_+'(x)-f_+'(y)|<\epsilon$ if x and y are enclosed in a small interval. For a fixed x, we have $|\frac{f(y)-f(x)}{y-x}-f_+'(x)|<\epsilon$ if $y\in (x,x+\delta)$ but $\delta$ depends greatly on $x$. I wonder if there are better methods.",,"['real-analysis', 'continuity']"
52,Why are the Bernoulli numbers of even index nonzero?,Why are the Bernoulli numbers of even index nonzero?,,The Bernoulli numbers are defined through: $\frac{z}{e^z-1}=\sum_{i=0}^{\infty}B_i \frac{z^i}{i!} $. One can show easily that $B_i=0$ for all $i\geq3$ odd. Is there a similar way to show that $B_i\neq 0$ for all even $i$? Best regards,The Bernoulli numbers are defined through: $\frac{z}{e^z-1}=\sum_{i=0}^{\infty}B_i \frac{z^i}{i!} $. One can show easily that $B_i=0$ for all $i\geq3$ odd. Is there a similar way to show that $B_i\neq 0$ for all even $i$? Best regards,,"['real-analysis', 'number-theory', 'elementary-number-theory', 'special-functions', 'bernoulli-numbers']"
53,Behavior of a logarithmic derivative,Behavior of a logarithmic derivative,,"Suppose the function $f:(0,1] \to \mathbb{R}$ is differentiable and satisfies $f(x) \geqslant 0$ and $\lim_{x \to 0+}f(x) = 0$. One way demonstrate that the derivative of a function tends to behave worse than the function is as follows. Show that the logarithmic derivative $f'/f$ diverges to $\pm \infty$ as $ x \to 0+$ or is, at least, unbounded in a neighborhood of $x =0$. This is quite simple to show under more restrictive conditions. If we assume that that $f'/f$ is positive, bounded and integrable on $[\delta,1]$ for all $\delta > 0$, then, with a change of variable $y = f(x),$ we have $$\lim_{ \delta \to 0+}\int_{\delta}^{1}\frac{f'(x)}{f(x)}\,dx = \lim_{ \delta \to 0+}\int_{f(\delta)}^{f(1)} \frac{dy}{y} = \lim_{ \delta \to 0+}[\log f(1) - \log f(\delta)] = + \infty,$$ and this implies that $f'(x)/f(x) \to +\infty$ as $x \to 0+.$ The problem then is to show that, under the weaker conditions where $f'/f$ need not be integrable, etc., we have either divergence or unboundedness of $f'/f$ near $x =0$. I suspect this is true, but I am not sure that a divergent limit is always necessary.","Suppose the function $f:(0,1] \to \mathbb{R}$ is differentiable and satisfies $f(x) \geqslant 0$ and $\lim_{x \to 0+}f(x) = 0$. One way demonstrate that the derivative of a function tends to behave worse than the function is as follows. Show that the logarithmic derivative $f'/f$ diverges to $\pm \infty$ as $ x \to 0+$ or is, at least, unbounded in a neighborhood of $x =0$. This is quite simple to show under more restrictive conditions. If we assume that that $f'/f$ is positive, bounded and integrable on $[\delta,1]$ for all $\delta > 0$, then, with a change of variable $y = f(x),$ we have $$\lim_{ \delta \to 0+}\int_{\delta}^{1}\frac{f'(x)}{f(x)}\,dx = \lim_{ \delta \to 0+}\int_{f(\delta)}^{f(1)} \frac{dy}{y} = \lim_{ \delta \to 0+}[\log f(1) - \log f(\delta)] = + \infty,$$ and this implies that $f'(x)/f(x) \to +\infty$ as $x \to 0+.$ The problem then is to show that, under the weaker conditions where $f'/f$ need not be integrable, etc., we have either divergence or unboundedness of $f'/f$ near $x =0$. I suspect this is true, but I am not sure that a divergent limit is always necessary.",,"['real-analysis', 'asymptotics']"
54,Is a function that integrates to zero against all polynomials constant?,Is a function that integrates to zero against all polynomials constant?,,"Suppose $f := [0,\infty) \rightarrow \mathbb{R}$ satisfies $|f(x)| \leq e^{-x}$ for all $x \in (0,\infty)$, and also has the property that $$ \int_0^{\infty} f(x) x^n dx = 0 \qquad \forall n \in \{0,1,2,3,...\}. $$ Does it follow that $f$ is constant? If so, is this a standard theorem? Many thanks for your help.","Suppose $f := [0,\infty) \rightarrow \mathbb{R}$ satisfies $|f(x)| \leq e^{-x}$ for all $x \in (0,\infty)$, and also has the property that $$ \int_0^{\infty} f(x) x^n dx = 0 \qquad \forall n \in \{0,1,2,3,...\}. $$ Does it follow that $f$ is constant? If so, is this a standard theorem? Many thanks for your help.",,"['real-analysis', 'integration', 'functional-analysis', 'power-series', 'harmonic-analysis']"
55,Examples of functions that do not belong to any Baire class,Examples of functions that do not belong to any Baire class,,"Recently I read this post , which gives examples of Baire class 2 functions.  I have also been reading the Wikipedia article on Baire functions .  The article claims that ""Henri Lebesgue proved that ... there exist functions that are not in any Baire class.""  I would like to see an example of such a function. I am having a hard time finding anything that directly addresses my specific question.  What I know so far is the definition: such a function is not the pointwise limit of any sequence of Baire class $\alpha$ functions, for any countable ordinal number $\alpha$.  I have also found this article, which shows that any such function is not Lebesgue measurable. I would also like to see some details of Lebesgue's proof that such functions exist.  To be more specific, I would like to know whether it is constructive or is only an existence result.","Recently I read this post , which gives examples of Baire class 2 functions.  I have also been reading the Wikipedia article on Baire functions .  The article claims that ""Henri Lebesgue proved that ... there exist functions that are not in any Baire class.""  I would like to see an example of such a function. I am having a hard time finding anything that directly addresses my specific question.  What I know so far is the definition: such a function is not the pointwise limit of any sequence of Baire class $\alpha$ functions, for any countable ordinal number $\alpha$.  I have also found this article, which shows that any such function is not Lebesgue measurable. I would also like to see some details of Lebesgue's proof that such functions exist.  To be more specific, I would like to know whether it is constructive or is only an existence result.",,"['real-analysis', 'analysis', 'continuity', 'examples-counterexamples']"
56,"What exactly is an ""analytic function""?","What exactly is an ""analytic function""?",,"This is for real analysis so I'm not worried about complex analytic functions. The definition in my book just says: ""A function f(x) which is represented by a power series with a positive radius of convergence is said to be 'real analytic at the origin', or simply 'analytic'."" To me it seems like this definition ends prematurely. Like when I was reading it I expected there to be an ""if"" and then a list of qualifications. I guess I just don't really understand the rationale behind creating this definition in the first place. What would even be the point of representing a function by a power series that didn't have a positive radius of convergence? If the power series doesn't converge for any values of x, it's not really representative of anything , right? I feel like this is actually obvious and I'm just overthinking it, but is this definition literally just giving a name to functions that can be represented as power series that actually converge for x values in some radius? There was an exercise earlier in the chapter before analytic functions were defined that asked us to find an infinite power series which represented a function; was that function analytic as well? Thanks in advance and sorry if this is a silly question.","This is for real analysis so I'm not worried about complex analytic functions. The definition in my book just says: ""A function f(x) which is represented by a power series with a positive radius of convergence is said to be 'real analytic at the origin', or simply 'analytic'."" To me it seems like this definition ends prematurely. Like when I was reading it I expected there to be an ""if"" and then a list of qualifications. I guess I just don't really understand the rationale behind creating this definition in the first place. What would even be the point of representing a function by a power series that didn't have a positive radius of convergence? If the power series doesn't converge for any values of x, it's not really representative of anything , right? I feel like this is actually obvious and I'm just overthinking it, but is this definition literally just giving a name to functions that can be represented as power series that actually converge for x values in some radius? There was an exercise earlier in the chapter before analytic functions were defined that asked us to find an infinite power series which represented a function; was that function analytic as well? Thanks in advance and sorry if this is a silly question.",,['real-analysis']
57,epsilon-N proof of divergent sequence with another divergence assumed,epsilon-N proof of divergent sequence with another divergence assumed,,"I'd like to prove the following proposition: $$ \lim_{n\rightarrow\infty} a_n = \infty \Rightarrow \lim_{n\rightarrow\infty} b_n = \infty \\ \text{where}\;b_n = \dfrac{1}{n}\sum_{k=1}^{n}a_k, $$ but my proof was stuck. Would you tell me how should I finish the proof? Suppose $\lim_{n\rightarrow\infty} a_n = \infty$ ($\forall\epsilon\in\mathbb{R},\,\exists N\in\mathbb{N},\,\forall n\in \mathbb{N},\,n\geq{}N\Rightarrow a_n > \epsilon$), then I'll prove $\lim_{n\rightarrow\infty} b_n = \infty$. Let $\epsilon\in \mathbb{R}$. For $n\geq N$, $b_n=\dfrac{1}{n}\sum_{k=1}^{N}a_k+\dfrac{1}{n}\sum_{k=N+1}^{n}a_k>\dfrac{1}{n}\sum_{k=1}^{N}a_k+\dfrac{n-N}{n}\epsilon=\dfrac{1}{n}\sum_{k=1}^{N}(a_k-\epsilon) + \epsilon,$ from the supposition. In order to prove $b_n>\epsilon$, I only prove $\dfrac{1}{n}\sum_{k=1}^{N}(a_k-\epsilon)\geq0$ ($N_0$ exists and for all $n\geq N_0$), but $a_n-\epsilon$ for $n < N$ can't be proved. What's wrong? And how do I let $N_0$?","I'd like to prove the following proposition: $$ \lim_{n\rightarrow\infty} a_n = \infty \Rightarrow \lim_{n\rightarrow\infty} b_n = \infty \\ \text{where}\;b_n = \dfrac{1}{n}\sum_{k=1}^{n}a_k, $$ but my proof was stuck. Would you tell me how should I finish the proof? Suppose $\lim_{n\rightarrow\infty} a_n = \infty$ ($\forall\epsilon\in\mathbb{R},\,\exists N\in\mathbb{N},\,\forall n\in \mathbb{N},\,n\geq{}N\Rightarrow a_n > \epsilon$), then I'll prove $\lim_{n\rightarrow\infty} b_n = \infty$. Let $\epsilon\in \mathbb{R}$. For $n\geq N$, $b_n=\dfrac{1}{n}\sum_{k=1}^{N}a_k+\dfrac{1}{n}\sum_{k=N+1}^{n}a_k>\dfrac{1}{n}\sum_{k=1}^{N}a_k+\dfrac{n-N}{n}\epsilon=\dfrac{1}{n}\sum_{k=1}^{N}(a_k-\epsilon) + \epsilon,$ from the supposition. In order to prove $b_n>\epsilon$, I only prove $\dfrac{1}{n}\sum_{k=1}^{N}(a_k-\epsilon)\geq0$ ($N_0$ exists and for all $n\geq N_0$), but $a_n-\epsilon$ for $n < N$ can't be proved. What's wrong? And how do I let $N_0$?",,"['real-analysis', 'sequences-and-series', 'epsilon-delta']"
58,Showing that $xy \leq \frac{x^p}{p} + \frac{y^q}{q}$,Showing that,xy \leq \frac{x^p}{p} + \frac{y^q}{q},"Question: Let $x \geq 0$ , $y \geq 0$ and $p > 0$, $q>0$ with $\frac{1}{p} + \frac{1}{q} = 1$. Show that  $$xy \leq \frac{x^p}{p} + \frac{y^q}{q} $$    [Suggestion: Without loss of generality suppose $xy = 1$]. Attempt: Let $f, \varphi : U \to \mathbb R$, $U = \{(x,y) \in \mathbb R^2; x > 0 , y > 0 \}$ given by $f(x,y) = \frac{x^p}{p} + \frac{y^q}{q}$ and $\varphi (x,y) = xy$. Then we have $$\mathrm {grad}\, f(x,y) = (x^{p-1}, y^{q-1}) \,\,\,\text{and}\,\,\, \mathrm {grad} \,\varphi (x,y) = (y,x)$$ Then $1$ is a regular value of $\varphi$. Consider $M = \varphi^{-1} (1)$, the hyperbola $xy =1$. Now $(x,y) \in M$ is a critical point of $f|_M$ iff $$\mathrm {grad}\, f(x,y) = \lambda\, \mathrm{grad} \, \varphi (x,y) \,\,\,\text{and}\,\,\, \varphi (x,y) = 1$$ As $x> 0 $ and $y>0$ we have $$x^{p-1} = \lambda y \,\,\, , y ^{q-1} = \lambda x \,\,\,\text{and} \,\,\,xy = 1$$ Then $$\frac{x}{y} = \frac{y ^{q-1}}{x ^{p-1}} \implies x^p = y^q$$ This gives us $$\begin{align}\frac{x^p}{p} + \frac{y^q}{q} &= \frac{qy^q + py^q}{pq } = y^q \frac{p + q}{pq}\\&= y^q = y ^{1 + \frac{q}{p}}\\&=y^{\frac{q}{p}}\cdot y = x \cdot y\end{align}$$ Now $f$ is of class $C^{\infty}$ and its Hessian is given by $$Hf(x,y) = \begin{pmatrix} (p-1)x^{p-2} & 0 \\ 0 & (q-1)y^{q-2}\end{pmatrix}  $$ and it is positive, therefore $xy$ is a local minimum. It follows then $$xy \leq \frac{x^p}{p} + \frac{y^q}{q}$$ as we wanted. The cases $x = y = 0$, $x = 0 $ and $y> 0$ were considered trivially true. Note: This inequality is used to prove Hlder's Inequality .","Question: Let $x \geq 0$ , $y \geq 0$ and $p > 0$, $q>0$ with $\frac{1}{p} + \frac{1}{q} = 1$. Show that  $$xy \leq \frac{x^p}{p} + \frac{y^q}{q} $$    [Suggestion: Without loss of generality suppose $xy = 1$]. Attempt: Let $f, \varphi : U \to \mathbb R$, $U = \{(x,y) \in \mathbb R^2; x > 0 , y > 0 \}$ given by $f(x,y) = \frac{x^p}{p} + \frac{y^q}{q}$ and $\varphi (x,y) = xy$. Then we have $$\mathrm {grad}\, f(x,y) = (x^{p-1}, y^{q-1}) \,\,\,\text{and}\,\,\, \mathrm {grad} \,\varphi (x,y) = (y,x)$$ Then $1$ is a regular value of $\varphi$. Consider $M = \varphi^{-1} (1)$, the hyperbola $xy =1$. Now $(x,y) \in M$ is a critical point of $f|_M$ iff $$\mathrm {grad}\, f(x,y) = \lambda\, \mathrm{grad} \, \varphi (x,y) \,\,\,\text{and}\,\,\, \varphi (x,y) = 1$$ As $x> 0 $ and $y>0$ we have $$x^{p-1} = \lambda y \,\,\, , y ^{q-1} = \lambda x \,\,\,\text{and} \,\,\,xy = 1$$ Then $$\frac{x}{y} = \frac{y ^{q-1}}{x ^{p-1}} \implies x^p = y^q$$ This gives us $$\begin{align}\frac{x^p}{p} + \frac{y^q}{q} &= \frac{qy^q + py^q}{pq } = y^q \frac{p + q}{pq}\\&= y^q = y ^{1 + \frac{q}{p}}\\&=y^{\frac{q}{p}}\cdot y = x \cdot y\end{align}$$ Now $f$ is of class $C^{\infty}$ and its Hessian is given by $$Hf(x,y) = \begin{pmatrix} (p-1)x^{p-2} & 0 \\ 0 & (q-1)y^{q-2}\end{pmatrix}  $$ and it is positive, therefore $xy$ is a local minimum. It follows then $$xy \leq \frac{x^p}{p} + \frac{y^q}{q}$$ as we wanted. The cases $x = y = 0$, $x = 0 $ and $y> 0$ were considered trivially true. Note: This inequality is used to prove Hlder's Inequality .",,"['real-analysis', 'proof-verification', 'lagrange-multiplier', 'young-inequality']"
59,Derivative at Endpoint,Derivative at Endpoint,,"In Rudin's ""Principles of Mathematical Analysis"" he defines the limit of a function as follows. Let $X$ and $Y$ be metric spaces; suppose $E \subset X$, $f$ maps $E$ into $Y$, and $p$ is a limit point of $E$.  Then   $$ \lim_{x \to p} f(x) = q $$   if there is a point $q \in Y$ with the following property: For every $\epsilon > 0$ there exists a $\delta > 0$ such that   $$ d_Y(f(x),q) < \epsilon $$   for all points $x \in E$ for which   $$ 0 < d_X(x,p) < \delta. $$ The functions $d_X$ and $d_Y$ are the metrics on $X$ and $Y$, respectively.  He then defines the derivative of a real function as follows. Let $f$ be defined on $[a,b]$.  For any $x \in [a,b]$ form the quotient   $$ \phi(t) = \frac{f(t) - f(x)}{t - x} \qquad a < t < b,\, t \neq x, $$   and define   $$ f^\prime(x) = \lim_{t \to x}\, \phi(t), \qquad (1) $$   provided this limit exists in accordance with [the above definition].  We thus associate with the function $f$ a function $f^\prime$ whose domain is the set of points $x$ at which the limit (1) exists. I can't reconcile what's wrong with derivatives at endpoints .  As a concrete example, let $f: [0,1] \to \mathbb{R}$ be defined by $f(x) = x^2$.  I claim that $f^\prime(0) = 0$.  Indeed, the difference quotient is $$ \phi(t) = \frac{t^2 - 0^2}{t - 0} = t $$ and so we need to show $$ \lim_{t \to 0}\, t = 0. $$ Let $\epsilon > 0$ and choose $\delta = \epsilon$.  Then for all $t \in [0,1]$ such that $0 < |t| < \delta$ we have $|t| < \epsilon$.  Hence $f^\prime(0) = 0$, even though it is at an endpoint of the domain of $f$. In particular, there's no use to introduce one-sided derivatives.  Am I missing something?","In Rudin's ""Principles of Mathematical Analysis"" he defines the limit of a function as follows. Let $X$ and $Y$ be metric spaces; suppose $E \subset X$, $f$ maps $E$ into $Y$, and $p$ is a limit point of $E$.  Then   $$ \lim_{x \to p} f(x) = q $$   if there is a point $q \in Y$ with the following property: For every $\epsilon > 0$ there exists a $\delta > 0$ such that   $$ d_Y(f(x),q) < \epsilon $$   for all points $x \in E$ for which   $$ 0 < d_X(x,p) < \delta. $$ The functions $d_X$ and $d_Y$ are the metrics on $X$ and $Y$, respectively.  He then defines the derivative of a real function as follows. Let $f$ be defined on $[a,b]$.  For any $x \in [a,b]$ form the quotient   $$ \phi(t) = \frac{f(t) - f(x)}{t - x} \qquad a < t < b,\, t \neq x, $$   and define   $$ f^\prime(x) = \lim_{t \to x}\, \phi(t), \qquad (1) $$   provided this limit exists in accordance with [the above definition].  We thus associate with the function $f$ a function $f^\prime$ whose domain is the set of points $x$ at which the limit (1) exists. I can't reconcile what's wrong with derivatives at endpoints .  As a concrete example, let $f: [0,1] \to \mathbb{R}$ be defined by $f(x) = x^2$.  I claim that $f^\prime(0) = 0$.  Indeed, the difference quotient is $$ \phi(t) = \frac{t^2 - 0^2}{t - 0} = t $$ and so we need to show $$ \lim_{t \to 0}\, t = 0. $$ Let $\epsilon > 0$ and choose $\delta = \epsilon$.  Then for all $t \in [0,1]$ such that $0 < |t| < \delta$ we have $|t| < \epsilon$.  Hence $f^\prime(0) = 0$, even though it is at an endpoint of the domain of $f$. In particular, there's no use to introduce one-sided derivatives.  Am I missing something?",,"['calculus', 'real-analysis', 'derivatives']"
60,Constructing a Convergent/Divergent Series from a Positive Sequence,Constructing a Convergent/Divergent Series from a Positive Sequence,,"Suppose that $\{x_n\}$ is a positive sequence such that $x_n \to 0$. Construct a positive sequence $s_n$ such that $\displaystyle \sum_{n=1}^\infty s_n$ diverges while $\displaystyle \sum_{n=1}^\infty x_ns_n$ converges. It is clear that if $\sum x_n$ converges, then taking $s_n=1$ for all $n$ will work. We are then reduced to the case where $\sum x_n$ diverges. My only thought is that as $\sum x_n$ does not converge, its tail does not form a Cauchy sequence, hence for some $\epsilon>0$, we have $$ \sum_{n=m}^\infty x_n >\epsilon $$ Any hint as to how I should proceed?","Suppose that $\{x_n\}$ is a positive sequence such that $x_n \to 0$. Construct a positive sequence $s_n$ such that $\displaystyle \sum_{n=1}^\infty s_n$ diverges while $\displaystyle \sum_{n=1}^\infty x_ns_n$ converges. It is clear that if $\sum x_n$ converges, then taking $s_n=1$ for all $n$ will work. We are then reduced to the case where $\sum x_n$ diverges. My only thought is that as $\sum x_n$ does not converge, its tail does not form a Cauchy sequence, hence for some $\epsilon>0$, we have $$ \sum_{n=m}^\infty x_n >\epsilon $$ Any hint as to how I should proceed?",,"['real-analysis', 'sequences-and-series']"
61,"Showing this function on the Cantor set is onto [0,1]","Showing this function on the Cantor set is onto [0,1]",,"The excerpt below is taken from Rosenthal's A First Look at Rigorous Probability . $K$ refers to the cantor set. My question refers to the statement ""It is easily checked that $f(K) =[0,1]$. I am thinking that this can by proved by taking any number in $[0,1]$, writing the binary expansion for it (that is, write it of the form $\sum_{n=1}^\infty b_n\cdot2^{-n},\ b_n\in \{0,1\}$ and then show that there is a point in the cantor set that will give $d_n = b_n \forall n$. How would I do this last step? That is, how would I show that such a point exists in the cantor set, $K$? To state the question again: how I would show that there is a $y\in K$ which corresponds to some point in $[0,1]$? A secondary question is: Can we show that $f(K) = [0,1]$, where $f$ is as in the attached image, without explicitly using binary/ternary expansions, and preferably also not using compactness? It is not important that this secondary question is answered: if the first question is answered and this one is not, I will select an answer for the question and then probably just create a separate question eventually for this and add a bounty if necessary. Thank you. Note: This question is technically addressed here , but the answers seems to say to me ""here is a function, it is a surjection, without explaining why it is a surjection.","The excerpt below is taken from Rosenthal's A First Look at Rigorous Probability . $K$ refers to the cantor set. My question refers to the statement ""It is easily checked that $f(K) =[0,1]$. I am thinking that this can by proved by taking any number in $[0,1]$, writing the binary expansion for it (that is, write it of the form $\sum_{n=1}^\infty b_n\cdot2^{-n},\ b_n\in \{0,1\}$ and then show that there is a point in the cantor set that will give $d_n = b_n \forall n$. How would I do this last step? That is, how would I show that such a point exists in the cantor set, $K$? To state the question again: how I would show that there is a $y\in K$ which corresponds to some point in $[0,1]$? A secondary question is: Can we show that $f(K) = [0,1]$, where $f$ is as in the attached image, without explicitly using binary/ternary expansions, and preferably also not using compactness? It is not important that this secondary question is answered: if the first question is answered and this one is not, I will select an answer for the question and then probably just create a separate question eventually for this and add a bounty if necessary. Thank you. Note: This question is technically addressed here , but the answers seems to say to me ""here is a function, it is a surjection, without explaining why it is a surjection.",,"['real-analysis', 'probability', 'measure-theory', 'decimal-expansion', 'cantor-set']"
62,Set of critical values of one-dimensional continuously differentiable function has measure zero.,Set of critical values of one-dimensional continuously differentiable function has measure zero.,,"I am fighting with the following exercise: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuously differentiable function, $A:=\{x \in \mathbb{R} \ \vert \ f^{\prime}(x)=0 \}$ and $\lambda$ the Lebesgue-measure on the real line. Show that $\lambda(f(A))=0$.   Hint: The mean-value theorem may turn out to be useful. What can we say about the measurability of $f(A)$? Its 2 a.m. now and I've tried for the last few hours to find a way to prove the statement, but nothing has worked so far. I don't think that $f(A)$ is always countable and I'm sure that $A$ can sometimes be uncountable. Thanks for any advice!","I am fighting with the following exercise: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be a continuously differentiable function, $A:=\{x \in \mathbb{R} \ \vert \ f^{\prime}(x)=0 \}$ and $\lambda$ the Lebesgue-measure on the real line. Show that $\lambda(f(A))=0$.   Hint: The mean-value theorem may turn out to be useful. What can we say about the measurability of $f(A)$? Its 2 a.m. now and I've tried for the last few hours to find a way to prove the statement, but nothing has worked so far. I don't think that $f(A)$ is always countable and I'm sure that $A$ can sometimes be uncountable. Thanks for any advice!",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
63,Problem in understanding the proof of Lebesgue-Radon-Nikodym Theorem,Problem in understanding the proof of Lebesgue-Radon-Nikodym Theorem,,"On Folland's Real Analysis book page $90$, the Lebesgue-Radon-Nikodym Theorem is given as Let $\nu$ be a $\sigma$-finite signed measure and $\mu$ a $\sigma$-finite positive measure on $(X,\mathcal{M})$. There exists unique $\sigma$-finite signed measure $\lambda,\rho$ on $(X,\mathcal{M})$ such that $\lambda\perp \mu$, $\rho\ll\mu$, and $\nu=\lambda+\rho$. Moreover, there is an extended $\mu$-integrable function $f: X\to\mathbb{R}$ such that $d\rho=fd\mu$, and any two functions are equal $\mu$-a.e. To prove this theorem, we first can consider the case that $\nu$ and $\mu$ are ""finite"" and ""positive"". Then, we can extend that to the case where $\nu$ and $\mu$ are ""$\sigma-$finite"" and ""positive"". Finally, since $\nu = \nu^+ - \nu^-$, we can conclude that for signed measure $\nu$. But I have problem in understanding the second step. In this step, we can write $X = \cup_j A_j$ where $A_j$'s are disjoint and $\nu(A_j)< \infty$ and $\mu(A_j) < \infty$. Then, we can define, $\nu_j(E) = \nu(E \cap A_j)$ and $\mu_j(E) = \mu(E \cap A_j)$ where $\nu_j$ and $\mu_j$ are finite. So, from the results of the first step, we know that $\lambda_j\perp \mu_j$, $\rho_j\ll\mu_j$, and $\nu_j=\lambda_j+\rho_j$, $d\rho_j=f_jd\mu_j$. But then, it says that if we define $\lambda = \sum_j \lambda_j$ and $f = \sum_j f_j$, we have $\nu=\lambda+\rho$ where $d\rho = fd\mu$. We know that $\rho_j\ll\mu_j$ and $\lambda_j\perp \mu_j$. To show that $\rho\ll\mu$ and $\lambda \perp \mu$, is it true to say that since for every $j$, $\rho_j\ll\mu_j$ and $\lambda_j\perp \mu_j$, then  we can conclude that $\sum_j\rho_j\ll \sum_j\mu_j$ and $\sum_j\lambda_j\perp \sum_j\mu_j$?","On Folland's Real Analysis book page $90$, the Lebesgue-Radon-Nikodym Theorem is given as Let $\nu$ be a $\sigma$-finite signed measure and $\mu$ a $\sigma$-finite positive measure on $(X,\mathcal{M})$. There exists unique $\sigma$-finite signed measure $\lambda,\rho$ on $(X,\mathcal{M})$ such that $\lambda\perp \mu$, $\rho\ll\mu$, and $\nu=\lambda+\rho$. Moreover, there is an extended $\mu$-integrable function $f: X\to\mathbb{R}$ such that $d\rho=fd\mu$, and any two functions are equal $\mu$-a.e. To prove this theorem, we first can consider the case that $\nu$ and $\mu$ are ""finite"" and ""positive"". Then, we can extend that to the case where $\nu$ and $\mu$ are ""$\sigma-$finite"" and ""positive"". Finally, since $\nu = \nu^+ - \nu^-$, we can conclude that for signed measure $\nu$. But I have problem in understanding the second step. In this step, we can write $X = \cup_j A_j$ where $A_j$'s are disjoint and $\nu(A_j)< \infty$ and $\mu(A_j) < \infty$. Then, we can define, $\nu_j(E) = \nu(E \cap A_j)$ and $\mu_j(E) = \mu(E \cap A_j)$ where $\nu_j$ and $\mu_j$ are finite. So, from the results of the first step, we know that $\lambda_j\perp \mu_j$, $\rho_j\ll\mu_j$, and $\nu_j=\lambda_j+\rho_j$, $d\rho_j=f_jd\mu_j$. But then, it says that if we define $\lambda = \sum_j \lambda_j$ and $f = \sum_j f_j$, we have $\nu=\lambda+\rho$ where $d\rho = fd\mu$. We know that $\rho_j\ll\mu_j$ and $\lambda_j\perp \mu_j$. To show that $\rho\ll\mu$ and $\lambda \perp \mu$, is it true to say that since for every $j$, $\rho_j\ll\mu_j$ and $\lambda_j\perp \mu_j$, then  we can conclude that $\sum_j\rho_j\ll \sum_j\mu_j$ and $\sum_j\lambda_j\perp \sum_j\mu_j$?",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'radon-nikodym']"
64,About CauchySchwarz inequality,About CauchySchwarz inequality,,"For the vectors $x$ and $y$, the CauchySchwarz inequality reads $$ |x\cdot y|\leq||x||\cdot||y|| $$ Does this inequality only hold for 2-norm? Or for any norms? Thanks in advance.","For the vectors $x$ and $y$, the CauchySchwarz inequality reads $$ |x\cdot y|\leq||x||\cdot||y|| $$ Does this inequality only hold for 2-norm? Or for any norms? Thanks in advance.",,"['real-analysis', 'linear-algebra', 'vectors', 'normed-spaces', 'cauchy-schwarz-inequality']"
65,Every dominated sequence of measurable functions is uniformly integrable?,Every dominated sequence of measurable functions is uniformly integrable?,,"This is an exercise from tao's blog . A sequence $f_n:X\to\mathbf{C}$ of absolutely integrable functions is said to be uniformly integrate if the following three statements hold: (Uniform bound on $L^1$ norm) One has $\sup_n\|f_n\|_{L^1(\mu)}=\sup_n\int_X |f_n|\,d\mu<\infty$. (No escape to vertical infinity) One has $\sup_n\int_{|f_n|\geq M}|f_n|\,d\mu\to 0$ as $M\to \infty$. (No escape with width infinity) One has $\sup_n\int_{|f_n|\leq\delta}|f_n|\,d\mu\to 0$ as $\delta\to 0$. Show that every dominated sequence of measurable functions is uniformly integrable. Let $(f_n)_{n=1}^\infty$ is a sequence of measurable function dominated by some absolutely integrable function $g$, that is $|f_n|\leq g$ for all $n=1,2,\dots$. The first statement is trivial. Note that $$\int_{|f_n|\geq M}|f_n|\,d\mu\leq \int_{|f_n|\geq M}g\,d\mu\leq\int_{|g|\geq M}g\,d\mu,$$  then using dominated convergence theorem, the second statement follows. Now  I have trouble in verifying the third statement, since $g\leq\delta$ is contained in $|f_n|\leq\delta$, the above argument doesn't work.","This is an exercise from tao's blog . A sequence $f_n:X\to\mathbf{C}$ of absolutely integrable functions is said to be uniformly integrate if the following three statements hold: (Uniform bound on $L^1$ norm) One has $\sup_n\|f_n\|_{L^1(\mu)}=\sup_n\int_X |f_n|\,d\mu<\infty$. (No escape to vertical infinity) One has $\sup_n\int_{|f_n|\geq M}|f_n|\,d\mu\to 0$ as $M\to \infty$. (No escape with width infinity) One has $\sup_n\int_{|f_n|\leq\delta}|f_n|\,d\mu\to 0$ as $\delta\to 0$. Show that every dominated sequence of measurable functions is uniformly integrable. Let $(f_n)_{n=1}^\infty$ is a sequence of measurable function dominated by some absolutely integrable function $g$, that is $|f_n|\leq g$ for all $n=1,2,\dots$. The first statement is trivial. Note that $$\int_{|f_n|\geq M}|f_n|\,d\mu\leq \int_{|f_n|\geq M}g\,d\mu\leq\int_{|g|\geq M}g\,d\mu,$$  then using dominated convergence theorem, the second statement follows. Now  I have trouble in verifying the third statement, since $g\leq\delta$ is contained in $|f_n|\leq\delta$, the above argument doesn't work.",,"['real-analysis', 'measure-theory']"
66,Does continuous injective functions preserve disconnectedness?,Does continuous injective functions preserve disconnectedness?,,"It is well known result that continuous functions send connected spaces to connected spaces. It is also known that if $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is continuous and injective, then if $A\subset\mathbb{R}^n$ is open, so it is $f(A)$. And it is also known that a continuous bijective function between topological spaces doesn't necessarily have to be an open function. So, what if we give a little twist to all these results? Let $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a continuous injective function and let $A\subset\mathbb{R}^n$ be an open and disconnected set. Does $f(A)$ need to be open or disconnected? I already know that $f(A)$ isn't necessarily open, since you can consider inclusion $i: \mathbb{R} \rightarrow\mathbb{R}^2$ with $i(x) = (x,0)$. Clearly $i$ is continuous and injective and any open set in $\mathbb{R}$ will be sent to a line segment in $\mathbb{R}^2$ which isn't an open set under its standard metric topology. But now I'm having troubles with either proving or finding a counterexample for disconnectedness. If there is a counterexample out there, then due to the Invariance of Domain Theorem, $m\neq n$ necessarily. But I couldn't think of any. Then I've also tried to give a proof, but I got stuck. Here's what I've tried: Take $f$ and $A$ as stated. Since $A$ is disconnected, there are two open sets $B,C$ such that $A=B\cup C$ and $B\cap C=\emptyset$, hence $\overline{B}\cap C=B\cap\overline{C}=\emptyset$ (where $\overline B$ means the closure of $B$). This means that $f(A)=f(B)\cup f(C)$ and due to injectivity we can easily check that $f(B)\cap f(C)=\emptyset$. But $f(B),f(C)$ aren't necessarily open in $\mathbb{R}^m$ as shown by the previous example, so we can't conclude that $f(A)$ is disconnected. I would need to show that $\overline{f(B)}\cap f(C)=f(B)\cap \overline{f(C)}=\emptyset$ Let $z\in\overline{f(B)}\cap f(C)$. Then there is $c\in C$ such that $f(c)=z$. Due to continuity, we also know that $f(\overline B)\subset \overline{f(B)}$. If $z\in f(\overline B)$ there is $b\in\overline B$ such that $f(b)=z$. Due to injectivity, $b=c$, hence $\overline{f(B)}\cap C \neq \emptyset$ which is a contradiction. Therefore, $z\in\overline{f(B)}\setminus f(\overline B)$... and here's where I'm stuck. Any help will be dearly appreciated.","It is well known result that continuous functions send connected spaces to connected spaces. It is also known that if $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is continuous and injective, then if $A\subset\mathbb{R}^n$ is open, so it is $f(A)$. And it is also known that a continuous bijective function between topological spaces doesn't necessarily have to be an open function. So, what if we give a little twist to all these results? Let $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$ be a continuous injective function and let $A\subset\mathbb{R}^n$ be an open and disconnected set. Does $f(A)$ need to be open or disconnected? I already know that $f(A)$ isn't necessarily open, since you can consider inclusion $i: \mathbb{R} \rightarrow\mathbb{R}^2$ with $i(x) = (x,0)$. Clearly $i$ is continuous and injective and any open set in $\mathbb{R}$ will be sent to a line segment in $\mathbb{R}^2$ which isn't an open set under its standard metric topology. But now I'm having troubles with either proving or finding a counterexample for disconnectedness. If there is a counterexample out there, then due to the Invariance of Domain Theorem, $m\neq n$ necessarily. But I couldn't think of any. Then I've also tried to give a proof, but I got stuck. Here's what I've tried: Take $f$ and $A$ as stated. Since $A$ is disconnected, there are two open sets $B,C$ such that $A=B\cup C$ and $B\cap C=\emptyset$, hence $\overline{B}\cap C=B\cap\overline{C}=\emptyset$ (where $\overline B$ means the closure of $B$). This means that $f(A)=f(B)\cup f(C)$ and due to injectivity we can easily check that $f(B)\cap f(C)=\emptyset$. But $f(B),f(C)$ aren't necessarily open in $\mathbb{R}^m$ as shown by the previous example, so we can't conclude that $f(A)$ is disconnected. I would need to show that $\overline{f(B)}\cap f(C)=f(B)\cap \overline{f(C)}=\emptyset$ Let $z\in\overline{f(B)}\cap f(C)$. Then there is $c\in C$ such that $f(c)=z$. Due to continuity, we also know that $f(\overline B)\subset \overline{f(B)}$. If $z\in f(\overline B)$ there is $b\in\overline B$ such that $f(b)=z$. Due to injectivity, $b=c$, hence $\overline{f(B)}\cap C \neq \emptyset$ which is a contradiction. Therefore, $z\in\overline{f(B)}\setminus f(\overline B)$... and here's where I'm stuck. Any help will be dearly appreciated.",,"['real-analysis', 'general-topology']"
67,$\lim_{t \to + \infty}(y'(t)+\alpha y(t))=0 \implies \lim_{t \to +\infty}y(t)=0$,,\lim_{t \to + \infty}(y'(t)+\alpha y(t))=0 \implies \lim_{t \to +\infty}y(t)=0,"Problem: Let $y \in C^1([0,+\infty), \mathbb{R})$ and $\alpha >0$. Prove that $$ \lim_{t \to + \infty}(y'(t)+\alpha y(t))=0 \implies \lim_{t \to +\infty}y(t)=0$$ I will show two attempts I have made, both of them didn't work out for me 1st Attempt : Let $\alpha > 0$ such that $$\lim_{t \to + \infty} (y'(t) + \alpha y(t))=0 \\ \implies \forall \epsilon >0, \exists S \in \mathbb{R}: \forall t \in [0, \infty) \text{ with }t\geq S \implies y'(t)+\alpha y(t)  \leq \epsilon  $$ But I can easily work with $y'(t)+ \alpha y(t) \leq \epsilon$ by multiplying it with $e^{\alpha t}$ one easily finds that  $$ \left(y(t)e^{\alpha t}\right)' \leq \epsilon e^{\alpha t} $$ Integration of both sides and using the fact that integration preserves the inequality I obtain that $$y(t) \leq \frac{\epsilon}{\alpha}+ C \exp(-\alpha t), \\text{ for } C \in \mathbb{R} \\ \implies |y(t)| \leq \frac{\epsilon}{\alpha}+|C| \exp(-\alpha t)  $$ Which looks promising at first, because as $t \to + \infty$ the most right term vanishes, however the fraction $\epsilon / \alpha$ doesn't provide any useful information, because possibly $\alpha, \epsilon$ are arbitrarily, so I am not sure if it is a rigorous statement to just say ""$\epsilon$ can always be smaller than $\alpha$"" 2nd Attempt : I was hoping that with the help of Gronwall's inequality I could get rid of the missing rigor of the above attempt Gronwall Inequality : Assume that $y'(t) \leq f(t) + g(t)y(t)$ then we have $$ y(t) \leq y(a) \exp \left( \int_a^t g(s)ds\right) + \int_a^t f(s) \exp \left( \int_s^t g(r)dr \right) ds $$ So I did use again the only thing that I know which is that $y'(t) + \alpha y(t) \leq \epsilon $ and to apply Gronwalls Lemma I did set $f(t)= \epsilon$ and $g(t)=- \alpha$ for all $t \in [0, \infty)$ Doing the necessary integration for Gronwalls Inequality I obtain that $$ y(t) \leq y(a) \exp(-\alpha (t-a)) + \frac{\epsilon}{\alpha} \left(1-\exp(-\alpha(t-a)  \right)$$ Which somehow just shows that my 1st attempt was equally good/bad as my 2nd attempt. Any hints? Corrections?","Problem: Let $y \in C^1([0,+\infty), \mathbb{R})$ and $\alpha >0$. Prove that $$ \lim_{t \to + \infty}(y'(t)+\alpha y(t))=0 \implies \lim_{t \to +\infty}y(t)=0$$ I will show two attempts I have made, both of them didn't work out for me 1st Attempt : Let $\alpha > 0$ such that $$\lim_{t \to + \infty} (y'(t) + \alpha y(t))=0 \\ \implies \forall \epsilon >0, \exists S \in \mathbb{R}: \forall t \in [0, \infty) \text{ with }t\geq S \implies y'(t)+\alpha y(t)  \leq \epsilon  $$ But I can easily work with $y'(t)+ \alpha y(t) \leq \epsilon$ by multiplying it with $e^{\alpha t}$ one easily finds that  $$ \left(y(t)e^{\alpha t}\right)' \leq \epsilon e^{\alpha t} $$ Integration of both sides and using the fact that integration preserves the inequality I obtain that $$y(t) \leq \frac{\epsilon}{\alpha}+ C \exp(-\alpha t), \\text{ for } C \in \mathbb{R} \\ \implies |y(t)| \leq \frac{\epsilon}{\alpha}+|C| \exp(-\alpha t)  $$ Which looks promising at first, because as $t \to + \infty$ the most right term vanishes, however the fraction $\epsilon / \alpha$ doesn't provide any useful information, because possibly $\alpha, \epsilon$ are arbitrarily, so I am not sure if it is a rigorous statement to just say ""$\epsilon$ can always be smaller than $\alpha$"" 2nd Attempt : I was hoping that with the help of Gronwall's inequality I could get rid of the missing rigor of the above attempt Gronwall Inequality : Assume that $y'(t) \leq f(t) + g(t)y(t)$ then we have $$ y(t) \leq y(a) \exp \left( \int_a^t g(s)ds\right) + \int_a^t f(s) \exp \left( \int_s^t g(r)dr \right) ds $$ So I did use again the only thing that I know which is that $y'(t) + \alpha y(t) \leq \epsilon $ and to apply Gronwalls Lemma I did set $f(t)= \epsilon$ and $g(t)=- \alpha$ for all $t \in [0, \infty)$ Doing the necessary integration for Gronwalls Inequality I obtain that $$ y(t) \leq y(a) \exp(-\alpha (t-a)) + \frac{\epsilon}{\alpha} \left(1-\exp(-\alpha(t-a)  \right)$$ Which somehow just shows that my 1st attempt was equally good/bad as my 2nd attempt. Any hints? Corrections?",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
68,Egorov's theorem application,Egorov's theorem application,,"Problem Let $\{f_k\}_{k \in \mathbb N}$ be a sequence of measurable functions defined on $E \subset \mathbb R^n$ with $E$ measurable and $|E|<\infty$, such that $f_k \to 0$ a.e.. Show that there exists a subsequence $\{f_{k_j}\}_{j \in \mathbb N}$ such that $\sum_{j \in \mathbb N} |f_{k_j}|<\infty$ a.e. I thought of using Egorov's theorem, so for each $\epsilon_n=\dfrac{1}{n}$, there exists a closed subset $F_n \subset E$ with $|E \setminus F_n|<\epsilon_n$ and such that $f_n \rightrightarrows 0$ on $F_n$. For each $j$, I can pick $n_j$ with $|f_{n_j}|<\dfrac{1}{2^j}$ in $F_j$ and I can also pick $n_1<n_2<...<n_j<...$ It is easy to see that complement of the set $F=\bigcup_{j \in \mathbb N} F_j$ has measure zero. The problem is that I cannot affirm $|f_{n_j}|<\dfrac{1}{2^j}$ in all $F$ but just in $F_{n_j}$. I can assure this on the intersection $\bigcap_{j \in \mathbb N} F_j$ but the complement of this set is not of measure zero, so the series is not convergent almost everywhere. Any hints to solve this problem would be greatly appreciated.","Problem Let $\{f_k\}_{k \in \mathbb N}$ be a sequence of measurable functions defined on $E \subset \mathbb R^n$ with $E$ measurable and $|E|<\infty$, such that $f_k \to 0$ a.e.. Show that there exists a subsequence $\{f_{k_j}\}_{j \in \mathbb N}$ such that $\sum_{j \in \mathbb N} |f_{k_j}|<\infty$ a.e. I thought of using Egorov's theorem, so for each $\epsilon_n=\dfrac{1}{n}$, there exists a closed subset $F_n \subset E$ with $|E \setminus F_n|<\epsilon_n$ and such that $f_n \rightrightarrows 0$ on $F_n$. For each $j$, I can pick $n_j$ with $|f_{n_j}|<\dfrac{1}{2^j}$ in $F_j$ and I can also pick $n_1<n_2<...<n_j<...$ It is easy to see that complement of the set $F=\bigcup_{j \in \mathbb N} F_j$ has measure zero. The problem is that I cannot affirm $|f_{n_j}|<\dfrac{1}{2^j}$ in all $F$ but just in $F_{n_j}$. I can assure this on the intersection $\bigcap_{j \in \mathbb N} F_j$ but the complement of this set is not of measure zero, so the series is not convergent almost everywhere. Any hints to solve this problem would be greatly appreciated.",,"['real-analysis', 'sequences-and-series', 'measure-theory']"
69,"Show that $\lim_{t\to \infty}u(x,t)=\frac{A+B}{2}$, for each $x\in\Bbb R$.","Show that , for each .","\lim_{t\to \infty}u(x,t)=\frac{A+B}{2} x\in\Bbb R","Let $u(x,t)$ be a $C^2$ bounded solution of $$u_t(x,t)-u_{xx}(x,t)=0,x\in \Bbb R, u(x,0)=f(x)$$ where $f\in C(\Bbb R)$ satisfies: $\lim_{x\to+\infty}f(x)=A,\lim_{x\to-\infty}f(x)=B$. Show that $\lim_{t\to \infty}u(x,t)=\frac{A+B}{2}$, for each $x\in\Bbb R$. My attempt: I try to use the integral representation for the solution of heat equation (here is one dimensional $n=1$) $$u(x,t)=\int_{\Bbb R}\frac{1}{\sqrt {4\pi t}}e^{-\frac{|x-y|^2}{4t}}f(y)dy$$ Then $\lim_{t\to\infty}u(x,t)=\lim_{t\to\infty}\int_{\Bbb R}\frac{1}{\sqrt {4\pi t}}e^{-\frac{|x-y|^2}{4t}}f(y)dy$. Then I got stuck. Could anyone kindly help? Thanks!","Let $u(x,t)$ be a $C^2$ bounded solution of $$u_t(x,t)-u_{xx}(x,t)=0,x\in \Bbb R, u(x,0)=f(x)$$ where $f\in C(\Bbb R)$ satisfies: $\lim_{x\to+\infty}f(x)=A,\lim_{x\to-\infty}f(x)=B$. Show that $\lim_{t\to \infty}u(x,t)=\frac{A+B}{2}$, for each $x\in\Bbb R$. My attempt: I try to use the integral representation for the solution of heat equation (here is one dimensional $n=1$) $$u(x,t)=\int_{\Bbb R}\frac{1}{\sqrt {4\pi t}}e^{-\frac{|x-y|^2}{4t}}f(y)dy$$ Then $\lim_{t\to\infty}u(x,t)=\lim_{t\to\infty}\int_{\Bbb R}\frac{1}{\sqrt {4\pi t}}e^{-\frac{|x-y|^2}{4t}}f(y)dy$. Then I got stuck. Could anyone kindly help? Thanks!",,"['real-analysis', 'partial-differential-equations', 'heat-equation']"
70,The quadratic and cubic versions of a tough intregral,The quadratic and cubic versions of a tough intregral,,"In this post, Proving that $\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$ , it's proved that $$I_1=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$$ but then some natural questions arise. Might we possibly hope to find nice closed forms for the  following integrals too? $$I_2=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^2 (t+2)}{t+1} \, dt$$ $$I_3=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^3 (t+2)}{t+1} \, dt$$ What tools, strategies would you like to propose? UPDATE: According to David H's comment we have that $$I_2=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^2 (t+2)}{t+1} \, dt$$ $$=\operatorname{Li}_{4}{\left(\frac13\right)}-\frac12\,\operatorname{Li}_{4}{\left(-\frac13\right)}+\frac{13}{12}\ln{(3)}\zeta{(3)}-\frac14\ln^{2}{(3)}\zeta{(2)}+\frac{1}{48}\ln^{4}{(3)}-\frac78\,\zeta{(4)}$$ that is also confirmed numerically by Mathematica.","In this post, Proving that $\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$ , it's proved that $$I_1=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log (t+2)}{t+1} \, dt=\frac{13}{24} \zeta (3)$$ but then some natural questions arise. Might we possibly hope to find nice closed forms for the  following integrals too? $$I_2=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^2 (t+2)}{t+1} \, dt$$ $$I_3=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^3 (t+2)}{t+1} \, dt$$ What tools, strategies would you like to propose? UPDATE: According to David H's comment we have that $$I_2=\int_0^1 \frac{\log \left(\frac{1}{t}\right) \log^2 (t+2)}{t+1} \, dt$$ $$=\operatorname{Li}_{4}{\left(\frac13\right)}-\frac12\,\operatorname{Li}_{4}{\left(-\frac13\right)}+\frac{13}{12}\ln{(3)}\zeta{(3)}-\frac14\ln^{2}{(3)}\zeta{(2)}+\frac{1}{48}\ln^{4}{(3)}-\frac78\,\zeta{(4)}$$ that is also confirmed numerically by Mathematica.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
71,Making a set into a manifold,Making a set into a manifold,,"Let $n \in \mathbb{N}$, $M$ be a set and let $\mathcal{A} = \{(\varphi_a, U_a)\}_{a \in \mathcal{A}}$ be a system of tuples so that: $U_{a} \subseteq \mathbb{R}^n$ is open for all $a$; $\varphi_a: U_a \to M$ is injective for all $a$, with image denoted $V_a$; $\varphi_a^{-1} \circ \varphi_b: \varphi_b^{-1}(V_a \cap V_b) \to \varphi_a^{-1}(V_a \cap V_b)$ is a homeomorphism for all $(a, b)$. Question : Can we in general conclude that there is a topology on $M$ so that $V_a$ is open in $M$ and $\varphi_a: U_a \to V_a$ is a homeomorphism for all $a$? My attempts: I wanted to define $U$ to be open in $M$ if and only if $\varphi_a^{-1}(U \cap V_a)$ is open in $U_a$ for all $a$. Clearly this is a topology and also every $\varphi_a$ is continuous (regardless if we see it as a map to $V_a$ or M). But I have problems with the rest. For example: Why is $V_a$ open in $M$? Is there anything which tells us that $\varphi_b^{-1}(V_a \cap V_b)$ is open in $U_b$ for all $b$? If the statement is not true, then you could help me finding conditions that make it true. The question arose while trying to solve an exercise which - roughly speaking - says that the tangent bundle $TM$ is a smooth manifold when $M$ is one. Thanks for any help.","Let $n \in \mathbb{N}$, $M$ be a set and let $\mathcal{A} = \{(\varphi_a, U_a)\}_{a \in \mathcal{A}}$ be a system of tuples so that: $U_{a} \subseteq \mathbb{R}^n$ is open for all $a$; $\varphi_a: U_a \to M$ is injective for all $a$, with image denoted $V_a$; $\varphi_a^{-1} \circ \varphi_b: \varphi_b^{-1}(V_a \cap V_b) \to \varphi_a^{-1}(V_a \cap V_b)$ is a homeomorphism for all $(a, b)$. Question : Can we in general conclude that there is a topology on $M$ so that $V_a$ is open in $M$ and $\varphi_a: U_a \to V_a$ is a homeomorphism for all $a$? My attempts: I wanted to define $U$ to be open in $M$ if and only if $\varphi_a^{-1}(U \cap V_a)$ is open in $U_a$ for all $a$. Clearly this is a topology and also every $\varphi_a$ is continuous (regardless if we see it as a map to $V_a$ or M). But I have problems with the rest. For example: Why is $V_a$ open in $M$? Is there anything which tells us that $\varphi_b^{-1}(V_a \cap V_b)$ is open in $U_b$ for all $b$? If the statement is not true, then you could help me finding conditions that make it true. The question arose while trying to solve an exercise which - roughly speaking - says that the tangent bundle $TM$ is a smooth manifold when $M$ is one. Thanks for any help.",,"['real-analysis', 'analysis', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
72,Prove that $|\sin^{1}(a)\sin^{1}(b)||ab|$,Prove that,|\sin^{1}(a)\sin^{1}(b)||ab|,"Question: Using the Mean Value Theorem, prove that $$|\sin^{1}(a)\sin^{1}(b)||ab|$$ for all $a,b(1/2,1)$. Here, $\sin^{1}$ denotes the inverse of the sine function. Attempt: I think I know how to do this but I want to make sure that I am as detailed as possible so I get all the marks. Here is my attempt: Define $f:[-1,1] \rightarrow [-\pi/2,\pi/2]$ by $f(x)=\sin^{-1}(x)$. This is a differentiable function on $(-1,1)$ and continuous on $[-1,1]$. 'Without loss of generality' assume $a<b$. Our $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ since $[a,b] \subset [-1,1]$. By MVT, there exists $c \in (-1,1)$ such that $$\frac{f(b)-f(a)}{b-a}=\frac{\sin^{1}(a)\sin^{1}(b)}{b-a}=f'(c)=\frac1{\sqrt{1-c^2}}\geq 1$$ which gives us: $\sin^{1}(a)\sin^{1}(b) \geq b-a$ and then giving the desired result by putting modulus on both sides. My concern is that i said $a<b$. Am i allowed to do that? And more importantly I let $c \in(-1,1)$ and not $(a,b)$. Is that wrong? If I did let it in $(a,b)$ then its impossible to say that it is $\geq 1$...","Question: Using the Mean Value Theorem, prove that $$|\sin^{1}(a)\sin^{1}(b)||ab|$$ for all $a,b(1/2,1)$. Here, $\sin^{1}$ denotes the inverse of the sine function. Attempt: I think I know how to do this but I want to make sure that I am as detailed as possible so I get all the marks. Here is my attempt: Define $f:[-1,1] \rightarrow [-\pi/2,\pi/2]$ by $f(x)=\sin^{-1}(x)$. This is a differentiable function on $(-1,1)$ and continuous on $[-1,1]$. 'Without loss of generality' assume $a<b$. Our $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ since $[a,b] \subset [-1,1]$. By MVT, there exists $c \in (-1,1)$ such that $$\frac{f(b)-f(a)}{b-a}=\frac{\sin^{1}(a)\sin^{1}(b)}{b-a}=f'(c)=\frac1{\sqrt{1-c^2}}\geq 1$$ which gives us: $\sin^{1}(a)\sin^{1}(b) \geq b-a$ and then giving the desired result by putting modulus on both sides. My concern is that i said $a<b$. Am i allowed to do that? And more importantly I let $c \in(-1,1)$ and not $(a,b)$. Is that wrong? If I did let it in $(a,b)$ then its impossible to say that it is $\geq 1$...",,['real-analysis']
73,Convergence to the Dirac Delta Function,Convergence to the Dirac Delta Function,,"Let $h\colon[0,1]\to \mathbb{R}^+$ be any bounded measurable non-negative function with a unique maximum at $a$ and $h$ is continuous at $a$. For $\lambda>0$ define $h_\lambda(x)=C_\lambda h(x)^\lambda$ where $C_\lambda$ normalizes such that $\displaystyle\int_0^1 h_\lambda(x)\,dx=1$. $f$ is any continuous function on $[0,1]$ and $\epsilon>0$. Are the following assertions true? 1)$\displaystyle\lim\limits_{\lambda\to\infty}\int_{h(x)\le h(a)\epsilon}h_(x)f(x)\,dx=0$ and $\displaystyle\lim\limits_{\lambda\to\infty}\int_0^1 h_(x)f(x)\,dx=f(a)$. In words, the limit of $h_\lambda$ is the Dirac delta function $\delta(\cdot-a)$. 2) If $h$ is continuous, $h_\lambda(x)$ converges uniformly to $0$ in $\{x: h(x) \le h(a)-\epsilon\}$. It seems correct since the exponentiation suppresses the part somewhere below $h(a)$ while heightens the part of $h$ near $a$. The difficulty seems to be giving a partitions of $[0,1]$ where I can utilize the exponentiation to either suppress or heighten $h$ by the exponentiation of $\lambda$ after $h$ is normalized by $C_\lambda$. This question is a generalization of my more specific case , and was suggested by one commentator whuber. However, he did not supply a proof and I am stumped by the difficulty described the last paragraph.","Let $h\colon[0,1]\to \mathbb{R}^+$ be any bounded measurable non-negative function with a unique maximum at $a$ and $h$ is continuous at $a$. For $\lambda>0$ define $h_\lambda(x)=C_\lambda h(x)^\lambda$ where $C_\lambda$ normalizes such that $\displaystyle\int_0^1 h_\lambda(x)\,dx=1$. $f$ is any continuous function on $[0,1]$ and $\epsilon>0$. Are the following assertions true? 1)$\displaystyle\lim\limits_{\lambda\to\infty}\int_{h(x)\le h(a)\epsilon}h_(x)f(x)\,dx=0$ and $\displaystyle\lim\limits_{\lambda\to\infty}\int_0^1 h_(x)f(x)\,dx=f(a)$. In words, the limit of $h_\lambda$ is the Dirac delta function $\delta(\cdot-a)$. 2) If $h$ is continuous, $h_\lambda(x)$ converges uniformly to $0$ in $\{x: h(x) \le h(a)-\epsilon\}$. It seems correct since the exponentiation suppresses the part somewhere below $h(a)$ while heightens the part of $h$ near $a$. The difficulty seems to be giving a partitions of $[0,1]$ where I can utilize the exponentiation to either suppress or heighten $h$ by the exponentiation of $\lambda$ after $h$ is normalized by $C_\lambda$. This question is a generalization of my more specific case , and was suggested by one commentator whuber. However, he did not supply a proof and I am stumped by the difficulty described the last paragraph.",,"['real-analysis', 'probability-theory', 'distribution-theory', 'dirac-delta']"
74,Prove that $\sum_{n=1}^{\infty}{f\left({1\over n}\right)}$ converges absolutely.,Prove that  converges absolutely.,\sum_{n=1}^{\infty}{f\left({1\over n}\right)},"Let $f:\Bbb{R}\to \Bbb{R}$ be continuously twice differentiable around $0$ such that $f(0)=f'(0)=0$. Prove that $\sum_{n=1}^{\infty}{f\left({1\over n}\right)}$ converges absolutely. What I did to far is :  $f(x)=o+o+R_n{(x,0)}={x^2 \over 2}f''(c)$. Therefore ${f\left({1\over n}\right)}={1 \over 2n^2}f''(c)$ where $0<c<{1\over n}$. By Heine definition of limits, $\lim_{n\to \infty}{f\left({1\over n}\right)}=f(0)=0$. I don't know how to show that either $f''(0)$ is bounded or uninfluential. I would really appreciate your help.","Let $f:\Bbb{R}\to \Bbb{R}$ be continuously twice differentiable around $0$ such that $f(0)=f'(0)=0$. Prove that $\sum_{n=1}^{\infty}{f\left({1\over n}\right)}$ converges absolutely. What I did to far is :  $f(x)=o+o+R_n{(x,0)}={x^2 \over 2}f''(c)$. Therefore ${f\left({1\over n}\right)}={1 \over 2n^2}f''(c)$ where $0<c<{1\over n}$. By Heine definition of limits, $\lim_{n\to \infty}{f\left({1\over n}\right)}=f(0)=0$. I don't know how to show that either $f''(0)$ is bounded or uninfluential. I would really appreciate your help.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'taylor-expansion']"
75,Proving the inequality $ \left|\prod_{i=0}^n \left(x - \frac{i}{n}\right)\right| \le \frac{n!}{4n^{n+1}}$,Proving the inequality, \left|\prod_{i=0}^n \left(x - \frac{i}{n}\right)\right| \le \frac{n!}{4n^{n+1}},"Let $n \in \Bbb{N}$ and $x \in [0,1]$ prove  $$ \left|\prod_{i=0}^n \left(x - \frac{i}{n}\right)\right| \le \frac{n!}{4n^{n+1}}$$ I manage to show that $\left| (x-\frac{n-1}{n})(x-\frac{n}{n})\right| \le \frac{1}{4n^2}$ by taking derivative and finding the maximum, but I didn't manage to show that the rest  $\le \frac{n!}{n^{n-1}}$","Let $n \in \Bbb{N}$ and $x \in [0,1]$ prove  $$ \left|\prod_{i=0}^n \left(x - \frac{i}{n}\right)\right| \le \frac{n!}{4n^{n+1}}$$ I manage to show that $\left| (x-\frac{n-1}{n})(x-\frac{n}{n})\right| \le \frac{1}{4n^2}$ by taking derivative and finding the maximum, but I didn't manage to show that the rest  $\le \frac{n!}{n^{n-1}}$",,"['calculus', 'real-analysis', 'inequality', 'products', 'infinite-product']"
76,length of an interval as a limit,length of an interval as a limit,,"Let $I$ be an interval and $A_{n}$ be the set of $k/n$ where $k$ is an integer. Prove that the length of $I$ (the positive difference between the end points) is the limit as $n$ tends to infinity of $\frac{1}{n}|(I \cap A_{n})|$ . My plan was to split it up into cases for the different type of intervals and come up with formulas for $|(I \cap A_{n})|$ , but I'm finding that very tricky. Thanks","Let be an interval and be the set of where is an integer. Prove that the length of (the positive difference between the end points) is the limit as tends to infinity of . My plan was to split it up into cases for the different type of intervals and come up with formulas for , but I'm finding that very tricky. Thanks",I A_{n} k/n k I n \frac{1}{n}|(I \cap A_{n})| |(I \cap A_{n})|,"['real-analysis', 'measure-theory']"
77,Separability of a set with norm $\thickapprox$ $L^1$ +$L^{\infty}$,Separability of a set with norm   +,\thickapprox L^1 L^{\infty},"Let $(M, \mathcal{A}, \mu)$ a complete separable probability space. Recall that complete means that any subset of a measurable set with zero measure is measure (and has zero measure) and separable means that there exists a countable family $E\subset \mathcal{A}$ such that for any $\varepsilon >0$ and any $B \in \mathcal{A}$ the exists $A_k \in E$ such that $\mu(B\triangle A_k)<\varepsilon$. Let $Y$ compact metric space. We define the family $\mathcal{F}$ so that $\phi \in \mathcal{F}$ sss $\phi$ is measurable ($Y$ with borel sigma algebra) for all $x\in M$, $\ $  $\phi(x,.): Y \rightarrow \mathbb{R}$ is continuous i.e. $\phi(x,.)\in (C^0(Y),\Vert . \Vert_0  )$ (supremum norm) $x \mapsto \Vert \phi(x,.) \Vert_0 \in L^1(\mu)$ equip $\mathcal{F}$ of a norm:   $\Vert \phi \Vert_1=\int \Vert \phi(x,.) \Vert_0 d\mu$ Then $(\mathcal{F}, \Vert . \Vert_1 )$ is a separable Banach space. I tried that $(\mathcal{F}, \Vert . \Vert_1 )$ is a Banach space I have difficulty with the separability: I show my progress using the fact that $C^0(Y)$ is separable, and the assumption that the probability space is separable we obtain: $\lbrace f_i\rbrace $ countable and dense in $C^0(Y)$ and $\Gamma= \lbrace \sum _{k=1}^{r}  c_k\chi_{A_k} : A_k \in E, c_k\in \mathbb{Q} \rbrace $ countable and dense in $L^1(\mu)$. Now note that for $\phi \in \mathcal{F}$, $\phi(x,.)$ can be approximated by $\lbrace f_i\rbrace $ and  $\phi(.,y) \in L^1(\mu)$ since for fixed $y$ $$\int \vert\phi(x,y)\vert d\mu(x)\leq \Vert \phi\Vert_1$$ then $\phi(.,y)$ can be approximated by $\Gamma$ but how to unite these facts... I appreciate the patience to read my query and hope you can help","Let $(M, \mathcal{A}, \mu)$ a complete separable probability space. Recall that complete means that any subset of a measurable set with zero measure is measure (and has zero measure) and separable means that there exists a countable family $E\subset \mathcal{A}$ such that for any $\varepsilon >0$ and any $B \in \mathcal{A}$ the exists $A_k \in E$ such that $\mu(B\triangle A_k)<\varepsilon$. Let $Y$ compact metric space. We define the family $\mathcal{F}$ so that $\phi \in \mathcal{F}$ sss $\phi$ is measurable ($Y$ with borel sigma algebra) for all $x\in M$, $\ $  $\phi(x,.): Y \rightarrow \mathbb{R}$ is continuous i.e. $\phi(x,.)\in (C^0(Y),\Vert . \Vert_0  )$ (supremum norm) $x \mapsto \Vert \phi(x,.) \Vert_0 \in L^1(\mu)$ equip $\mathcal{F}$ of a norm:   $\Vert \phi \Vert_1=\int \Vert \phi(x,.) \Vert_0 d\mu$ Then $(\mathcal{F}, \Vert . \Vert_1 )$ is a separable Banach space. I tried that $(\mathcal{F}, \Vert . \Vert_1 )$ is a Banach space I have difficulty with the separability: I show my progress using the fact that $C^0(Y)$ is separable, and the assumption that the probability space is separable we obtain: $\lbrace f_i\rbrace $ countable and dense in $C^0(Y)$ and $\Gamma= \lbrace \sum _{k=1}^{r}  c_k\chi_{A_k} : A_k \in E, c_k\in \mathbb{Q} \rbrace $ countable and dense in $L^1(\mu)$. Now note that for $\phi \in \mathcal{F}$, $\phi(x,.)$ can be approximated by $\lbrace f_i\rbrace $ and  $\phi(.,y) \in L^1(\mu)$ since for fixed $y$ $$\int \vert\phi(x,y)\vert d\mu(x)\leq \Vert \phi\Vert_1$$ then $\phi(.,y)$ can be approximated by $\Gamma$ but how to unite these facts... I appreciate the patience to read my query and hope you can help",,"['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'lebesgue-measure']"
78,"Smooth Manifold, covered by 2 Charts is orientable if the Intersection is Connected","Smooth Manifold, covered by 2 Charts is orientable if the Intersection is Connected",,"I came across this Question: Atlas on a smooth manifold that contains 2 charts in which Professor Lee commented that this Proposition is true only if the Intersection of the two Maps is connected, so I've been trying to prove the following: Let $M$ be a smooth Manifold, covered by an Atlas $\mathcal{A}$ containing two charts $\phi,\psi$ and $M= U_\phi \cup V_\psi$. Show that $M$ is orientable if $U_\phi \cap V_\psi$ is connected. So in other words I will have to prove that,for $\tau = \phi \circ \psi^{-1}$, the following holds: $det(Jac(\tau))>0$. First of all I tried showing that $det(Jac(\tau)) > 0 \vee det(Jac(\tau)) < 0$ holds. I reasoned : Since $M$ is smooth it follows that $\tau$ is a Diffeomorphism, which implies $\tau$ is continuous. Suppose $\tau$  would take values both bigger and smaller than $0$, this would imply that there is a point at which $\tau$ becomes $0$ and therefore would no longer be a Diffeomorphism. The next step would be to show that $det(Jac(\tau))> 0$ however I do not see why this is indeed the case. Is my reasoning so far correct and if so how do I continue?","I came across this Question: Atlas on a smooth manifold that contains 2 charts in which Professor Lee commented that this Proposition is true only if the Intersection of the two Maps is connected, so I've been trying to prove the following: Let $M$ be a smooth Manifold, covered by an Atlas $\mathcal{A}$ containing two charts $\phi,\psi$ and $M= U_\phi \cup V_\psi$. Show that $M$ is orientable if $U_\phi \cap V_\psi$ is connected. So in other words I will have to prove that,for $\tau = \phi \circ \psi^{-1}$, the following holds: $det(Jac(\tau))>0$. First of all I tried showing that $det(Jac(\tau)) > 0 \vee det(Jac(\tau)) < 0$ holds. I reasoned : Since $M$ is smooth it follows that $\tau$ is a Diffeomorphism, which implies $\tau$ is continuous. Suppose $\tau$  would take values both bigger and smaller than $0$, this would imply that there is a point at which $\tau$ becomes $0$ and therefore would no longer be a Diffeomorphism. The next step would be to show that $det(Jac(\tau))> 0$ however I do not see why this is indeed the case. Is my reasoning so far correct and if so how do I continue?",,"['real-analysis', 'differential-geometry', 'smooth-manifolds']"
79,Compute $\sum_{n=1}^{+\infty}\frac{\mathrm{e}^{-\sqrt{n}}}{\sqrt{n}}$,Compute,\sum_{n=1}^{+\infty}\frac{\mathrm{e}^{-\sqrt{n}}}{\sqrt{n}},Does the following series have a closed form ? $$\sum_{n=1}^{+\infty}\frac{\mathrm{e}^{-\sqrt{n}}}{\sqrt{n}}$$ Motivation : The original exercise is Compute $\int_{1}^{+\infty} \sum_{n=1}^{+\infty} \exp(-x\cdot \sqrt{n})dx$ I switched sums and integral using my course. But I am stuck to compute the series.,Does the following series have a closed form ? $$\sum_{n=1}^{+\infty}\frac{\mathrm{e}^{-\sqrt{n}}}{\sqrt{n}}$$ Motivation : The original exercise is Compute $\int_{1}^{+\infty} \sum_{n=1}^{+\infty} \exp(-x\cdot \sqrt{n})dx$ I switched sums and integral using my course. But I am stuck to compute the series.,,['real-analysis']
80,Generating function of the squared Riemann zeta function,Generating function of the squared Riemann zeta function,,"It's a well known fact that $$\sum_{k=2}^{\infty} \zeta(k) x^k=-x \psi(1-x)-x\gamma \space (|x|<1) $$  but I didn't meet yet a version for squared Riemann zeta function  $$\sum_{k=2}^{\infty}\zeta(k)^2 x^k$$ Do you know such a generating function? If yes, what is this one and how to derive it?","It's a well known fact that $$\sum_{k=2}^{\infty} \zeta(k) x^k=-x \psi(1-x)-x\gamma \space (|x|<1) $$  but I didn't meet yet a version for squared Riemann zeta function  $$\sum_{k=2}^{\infty}\zeta(k)^2 x^k$$ Do you know such a generating function? If yes, what is this one and how to derive it?",,"['calculus', 'real-analysis', 'complex-analysis', 'riemann-zeta']"
81,In search for a topology,In search for a topology,,"I'm looking for a way to convergence on subspaces. If $G_k(\mathbb{R}^m)=\{ W: W$ is subspace of $\mathbb{R}^m, \dim W=k \}$ and consider in $G_k(\mathbb{R}^m)$ one topology $\tau$. I would like to find a non-trivial topology for the following statement is true: $\lbrace S_k\rbrace \subset G_n(\mathbb{R}^m)$ converges to $S\in G_n(\mathbb{R}^m)$ sss for every $k\in \mathbb{N}$ there is a basis $\lbrace u^k_1,\ldots,u^k_n\rbrace$ of $S_k$ such that $\lbrace \displaystyle{\lim_{ k \rightarrow +\infty}}u^k_1,\ldots,\displaystyle{\lim_{ k \rightarrow +\infty}}u^k_n\rbrace$ is a basis of $S$. All suggestions are welcome! Note: I thought that the appropriate topology was $\tau$ where $U\in \tau$ is open iff  the set $\widehat{U}=\lbrace v: v\in W\backslash \lbrace 0\rbrace, \mbox{for some}  \ W\in U \rbrace$ is open in $\mathbb{R}^m$ but as shown in the following post is not true.","I'm looking for a way to convergence on subspaces. If $G_k(\mathbb{R}^m)=\{ W: W$ is subspace of $\mathbb{R}^m, \dim W=k \}$ and consider in $G_k(\mathbb{R}^m)$ one topology $\tau$. I would like to find a non-trivial topology for the following statement is true: $\lbrace S_k\rbrace \subset G_n(\mathbb{R}^m)$ converges to $S\in G_n(\mathbb{R}^m)$ sss for every $k\in \mathbb{N}$ there is a basis $\lbrace u^k_1,\ldots,u^k_n\rbrace$ of $S_k$ such that $\lbrace \displaystyle{\lim_{ k \rightarrow +\infty}}u^k_1,\ldots,\displaystyle{\lim_{ k \rightarrow +\infty}}u^k_n\rbrace$ is a basis of $S$. All suggestions are welcome! Note: I thought that the appropriate topology was $\tau$ where $U\in \tau$ is open iff  the set $\widehat{U}=\lbrace v: v\in W\backslash \lbrace 0\rbrace, \mbox{for some}  \ W\in U \rbrace$ is open in $\mathbb{R}^m$ but as shown in the following post is not true.",,"['real-analysis', 'general-topology', 'convergence-divergence', 'grassmannian']"
82,A subsequence of a convergent sequence converges to the same limit. Questions on proof. (Abbott p 57 2.5.1),A subsequence of a convergent sequence converges to the same limit. Questions on proof. (Abbott p 57 2.5.1),,"Solutions to Homework 3 doesn`t duplicate . We have to prove that if $(a_{n})$ is a sequence in $\mathbb{R}$ with $\displaystyle \lim_{n\rightarrow\infty} a_n =a$, and if $(a_{n_{k}})_{k\in \mathbb{N}+}$ is a subsequence of $(a_{n})$ , then $\displaystyle \lim_{k\rightarrow\infty}a_{n_{k}}=a$. We first need to know that $n_{k}\geq k$ for all $k\in Z_{>0}$. This is proved by induction on $k$. I omit this. Let $e >0$. $\displaystyle \lim_{n\rightarrow\infty}a_{n}=a$ is posited, so there's $\color{violet}{N}\in \mathbb{N}$ such that for all $n\in\mathbb{N}$, $n \ge N \implies |a_n-a| < e$. Let $k\in \mathbb{N}$ with $k\geq N.$ Then ${n_k}\geq k\geq N$. Therefore $|a_{n_{k}}-a|<e$. I know we must find $ N\in\mathbb{N}$ such that  $\color{red}{n_k} \ge N\implies |a_\color{red}{n_k}-a| < e \quad ()$. As the proof overhead shows, this $N$ is the same as the posited $\color{violet}{N}$. But what engenders $()$ ? Is proof saying $n \ge N \implies |a_n-a| < e$ implies $\color{red}{k} \ge N \implies |a_\color{red}{k}-a| < e$ implies $()$, because $n_k \ge k \ge N$? If yes, then I don't understand how $k$ can be replaced by $n_k$?","Solutions to Homework 3 doesn`t duplicate . We have to prove that if $(a_{n})$ is a sequence in $\mathbb{R}$ with $\displaystyle \lim_{n\rightarrow\infty} a_n =a$, and if $(a_{n_{k}})_{k\in \mathbb{N}+}$ is a subsequence of $(a_{n})$ , then $\displaystyle \lim_{k\rightarrow\infty}a_{n_{k}}=a$. We first need to know that $n_{k}\geq k$ for all $k\in Z_{>0}$. This is proved by induction on $k$. I omit this. Let $e >0$. $\displaystyle \lim_{n\rightarrow\infty}a_{n}=a$ is posited, so there's $\color{violet}{N}\in \mathbb{N}$ such that for all $n\in\mathbb{N}$, $n \ge N \implies |a_n-a| < e$. Let $k\in \mathbb{N}$ with $k\geq N.$ Then ${n_k}\geq k\geq N$. Therefore $|a_{n_{k}}-a|<e$. I know we must find $ N\in\mathbb{N}$ such that  $\color{red}{n_k} \ge N\implies |a_\color{red}{n_k}-a| < e \quad ()$. As the proof overhead shows, this $N$ is the same as the posited $\color{violet}{N}$. But what engenders $()$ ? Is proof saying $n \ge N \implies |a_n-a| < e$ implies $\color{red}{k} \ge N \implies |a_\color{red}{k}-a| < e$ implies $()$, because $n_k \ge k \ge N$? If yes, then I don't understand how $k$ can be replaced by $n_k$?",,['real-analysis']
83,Which properties do still hold for the limit of a sequence of functions?,Which properties do still hold for the limit of a sequence of functions?,,"I think it would be very useful to have a list of properties that are preserved for the limit of a sequence of functions, and was wondering if you could help me making the list more complete. Let $f_n$ be a sequence of functions on $\mathbb{R}$ which converges (uniformly or only pointwise) to a limiting function $f$. Which properties of $f_n$ still hold for $f$? The easy bits are probably: If convergence is uniform, then -continuity -boundedness -Riemann integrability -analyticity are preserved as $n\to\infty$ (see, for example, https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/ch9.pdf and of course http://en.wikipedia.org/wiki/Uniform_convergence ). Do you know others? Are there any properties that are preserved under pointwise convergence? References or (sketches of) proofs would be great, too, if you know any. Thanks for helping with this list! Edit: Jochen pointed out in his answer below that in general differentiability does not need to be preserved when taking the limit. However, if $f_n\to f$ pointwise, and $f_n'\to g$ uniformly, then $f'=g$ (see https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/ch9.pdf ). Does this also hold for left- or right-differentiability? If $f_n$ is not differentiable but right-derivatives exist and tend to $g$, is that then the right-derivative of $f$?","I think it would be very useful to have a list of properties that are preserved for the limit of a sequence of functions, and was wondering if you could help me making the list more complete. Let $f_n$ be a sequence of functions on $\mathbb{R}$ which converges (uniformly or only pointwise) to a limiting function $f$. Which properties of $f_n$ still hold for $f$? The easy bits are probably: If convergence is uniform, then -continuity -boundedness -Riemann integrability -analyticity are preserved as $n\to\infty$ (see, for example, https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/ch9.pdf and of course http://en.wikipedia.org/wiki/Uniform_convergence ). Do you know others? Are there any properties that are preserved under pointwise convergence? References or (sketches of) proofs would be great, too, if you know any. Thanks for helping with this list! Edit: Jochen pointed out in his answer below that in general differentiability does not need to be preserved when taking the limit. However, if $f_n\to f$ pointwise, and $f_n'\to g$ uniformly, then $f'=g$ (see https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/ch9.pdf ). Does this also hold for left- or right-differentiability? If $f_n$ is not differentiable but right-derivatives exist and tend to $g$, is that then the right-derivative of $f$?",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'convergence-divergence', 'uniform-convergence']"
84,"Continuous curve, traps itself outside the unit circle.","Continuous curve, traps itself outside the unit circle.",,"Lets say i have an injective continuous curve $\sigma$ in $\mathbb{C}$, indexed on $[0,\infty)$ and converging to $\infty$. If $\vert \sigma(0)\vert>0$ , is it possible that it can trap itself outside the unit circle? By that i mean, that there doesn't exist an extension of the curve, so that the beginning point is on the unit circle? My intuitive guess if of course no, but i wonder if there is a simple proof that doesn't require more than the first course in topology . I would also appreciate if someone could confirm that my guess is correct. thanks for reading.","Lets say i have an injective continuous curve $\sigma$ in $\mathbb{C}$, indexed on $[0,\infty)$ and converging to $\infty$. If $\vert \sigma(0)\vert>0$ , is it possible that it can trap itself outside the unit circle? By that i mean, that there doesn't exist an extension of the curve, so that the beginning point is on the unit circle? My intuitive guess if of course no, but i wonder if there is a simple proof that doesn't require more than the first course in topology . I would also appreciate if someone could confirm that my guess is correct. thanks for reading.",,"['real-analysis', 'general-topology', 'geometry']"
85,"If $f_j \rightharpoonup f$ weakly in $W^{1,p}$ then $f_j \to f$ strongly in $L^p$?",If  weakly in  then  strongly in ?,"f_j \rightharpoonup f W^{1,p} f_j \to f L^p","Suppose $1<p<\infty$ and $\Omega$ is an open bounded set in $\mathbb R^n$ with nice boundary (say Lipschitz or even better). Let $(f_j)_j \subset W^{1,p}(\Omega)$ s.t. $f_j \rightharpoonup f$ weakly in $W^{1,p}(\Omega)$. Is it true that $f_j \to f$ strongly in $L^p(\Omega)$? For sure it is true that $f_j \rightharpoonup f$ and $\nabla f_j \rightharpoonup\nabla f$.  Moreover, we should have the strong convergence of a subsequence thanks to reflexivity: $(f_j)_j$ is bounded hence is has a strong convergent subsequence in $L^p(\Omega)$ because the embedding $W^{1,p} \to L^p$ is (always) compact. Thanks.","Suppose $1<p<\infty$ and $\Omega$ is an open bounded set in $\mathbb R^n$ with nice boundary (say Lipschitz or even better). Let $(f_j)_j \subset W^{1,p}(\Omega)$ s.t. $f_j \rightharpoonup f$ weakly in $W^{1,p}(\Omega)$. Is it true that $f_j \to f$ strongly in $L^p(\Omega)$? For sure it is true that $f_j \rightharpoonup f$ and $\nabla f_j \rightharpoonup\nabla f$.  Moreover, we should have the strong convergence of a subsequence thanks to reflexivity: $(f_j)_j$ is bounded hence is has a strong convergent subsequence in $L^p(\Omega)$ because the embedding $W^{1,p} \to L^p$ is (always) compact. Thanks.",,"['real-analysis', 'convergence-divergence', 'sobolev-spaces', 'weak-convergence', 'weak-derivatives']"
86,Termwise differentiation for absolutely convergent series,Termwise differentiation for absolutely convergent series,,"Suppose $f(x,y)=\sum_{n=1}^\infty f_n(x,y)$ converges absolutely. Is it true that $$\dfrac{\partial}{\partial x}f(x,y)=\sum_{n=1}^\infty\dfrac{\partial}{\partial x}f_n(x,y)?$$ If not, what extra condition do we need (e.g. uniform convergence)?","Suppose $f(x,y)=\sum_{n=1}^\infty f_n(x,y)$ converges absolutely. Is it true that $$\dfrac{\partial}{\partial x}f(x,y)=\sum_{n=1}^\infty\dfrac{\partial}{\partial x}f_n(x,y)?$$ If not, what extra condition do we need (e.g. uniform convergence)?",,"['real-analysis', 'derivatives', 'convergence-divergence']"
87,Embedding of continuous functions into differentiable functions,Embedding of continuous functions into differentiable functions,,"This question refers to a solution printed in the current (December 2013, 120 (10)) issue of The American Mathematical Monthly , p. 944.  There, the authors intend to show that any ring homomorphism from the set $C$ of continuous functions R $\to$ R to the set of differentiable functions R $\to$ R cannot be an injection.  (Note we are talking of only an algebrac homomorphism.  That is, we have no topology on the function spaces.) I follow the solution through ""the image of [such a homomorphism] $\phi$ consists only of constant functions.""  What I don't get is the following (and terminal) sentence, ""In particular, $\phi$ is not injective.""  This does not follow from cardinality considerations, as $C$ has the same cardinality as R .  Could someone please elucidate?","This question refers to a solution printed in the current (December 2013, 120 (10)) issue of The American Mathematical Monthly , p. 944.  There, the authors intend to show that any ring homomorphism from the set $C$ of continuous functions R $\to$ R to the set of differentiable functions R $\to$ R cannot be an injection.  (Note we are talking of only an algebrac homomorphism.  That is, we have no topology on the function spaces.) I follow the solution through ""the image of [such a homomorphism] $\phi$ consists only of constant functions.""  What I don't get is the following (and terminal) sentence, ""In particular, $\phi$ is not injective.""  This does not follow from cardinality considerations, as $C$ has the same cardinality as R .  Could someone please elucidate?",,"['real-analysis', 'ring-theory']"
88,Darboux's theorem of several variables,Darboux's theorem of several variables,,"Let $f:U\longrightarrow \mathbb{R}$ a differentiable function where $U\subset\mathbb{R}^n$ open connected. What can we say about the image of the derivative $f'(U)\subset \mathbb{R}^n$ ? $f'(U)$ is connected? If $n=1$ , $\;f'(U)$ is an interval by Darboux's Theorem , some reference? Any hints would be appreciated.","Let a differentiable function where open connected. What can we say about the image of the derivative ? is connected? If , is an interval by Darboux's Theorem , some reference? Any hints would be appreciated.",f:U\longrightarrow \mathbb{R} U\subset\mathbb{R}^n f'(U)\subset \mathbb{R}^n f'(U) n=1 \;f'(U),['real-analysis']
89,Bounded sequence with $a_n=|a_{n+1}-a_{n+2}|$,Bounded sequence with,a_n=|a_{n+1}-a_{n+2}|,"Suppose $a_0,a_1>0$ are distinct, and $a_n=|a_{n+1}-a_{n+2}|$ for all $n\geq 0$. Is it possible that the sequence is bounded? From my experiment, the sequence seem to always be unbounded. We have $\pm a_n=a_{n+1}-a_{n+2}$, so $a_{n+2}=a_{n+1}\pm a_n$. For each $n$ we can choose plus or minus. What can we do to show that the magnitude of the sequence must grow?","Suppose $a_0,a_1>0$ are distinct, and $a_n=|a_{n+1}-a_{n+2}|$ for all $n\geq 0$. Is it possible that the sequence is bounded? From my experiment, the sequence seem to always be unbounded. We have $\pm a_n=a_{n+1}-a_{n+2}$, so $a_{n+2}=a_{n+1}\pm a_n$. For each $n$ we can choose plus or minus. What can we do to show that the magnitude of the sequence must grow?",,"['real-analysis', 'sequences-and-series']"
90,Show that every monotonic increasing and bounded sequence is Cauchy.,Show that every monotonic increasing and bounded sequence is Cauchy.,,"The title is kind of misleading because the task actually to show Every monotonic increasing and bounded sequence $(x_n)_{n\in\mathbb{N}}$ is Cauchy without knowing that: Every bounded non-empty set of real numbers has a least upper bound. (Supremum/Completeness Axiom) A sequence converges if and only if it is Cauchy. (Cauchy Criterion) Every monotonic increasing/decreasing, bounded and real sequence converges to the supremum/infimum of the codomain (not sure  if this is the right word). However, what is allowed to use listed as well: A sequence is called covergent, if for $\forall\varepsilon>0\,\,\exists N\in\mathbb{N}$ so that $|\,a_n - a\,| < \varepsilon$ for $\forall n>N$. (Definition of Convergence) A sequence $(a'_k)_{k1}$ is called a subsequence of a sequence $(a_n)_{n1}$, if there is a monotonic increasing sequence $(n_k)_{k1}\in\mathbb{N}$ so that $a'_{k} = a_{n_{k}}$ for $\forall k1$. (Definition of a Subsequence) A sequence $(a_n)_{n1}$ is Cauchy, if for $\forall\varepsilon>0\,\,\exists N=N(\varepsilon)\in\mathbb{N}$ so that $|\,a_m - a_n\,| < \varepsilon$ for $\forall m,n>N$. (Definition of a Cauchy Sequence) (Hint) The sequence $(\varepsilon\cdot\ell)_{\ell\in\mathbb{N}}$ is unbounded for $\varepsilon>0$. (Archimedes Principle) Would appreciate any help.","The title is kind of misleading because the task actually to show Every monotonic increasing and bounded sequence $(x_n)_{n\in\mathbb{N}}$ is Cauchy without knowing that: Every bounded non-empty set of real numbers has a least upper bound. (Supremum/Completeness Axiom) A sequence converges if and only if it is Cauchy. (Cauchy Criterion) Every monotonic increasing/decreasing, bounded and real sequence converges to the supremum/infimum of the codomain (not sure  if this is the right word). However, what is allowed to use listed as well: A sequence is called covergent, if for $\forall\varepsilon>0\,\,\exists N\in\mathbb{N}$ so that $|\,a_n - a\,| < \varepsilon$ for $\forall n>N$. (Definition of Convergence) A sequence $(a'_k)_{k1}$ is called a subsequence of a sequence $(a_n)_{n1}$, if there is a monotonic increasing sequence $(n_k)_{k1}\in\mathbb{N}$ so that $a'_{k} = a_{n_{k}}$ for $\forall k1$. (Definition of a Subsequence) A sequence $(a_n)_{n1}$ is Cauchy, if for $\forall\varepsilon>0\,\,\exists N=N(\varepsilon)\in\mathbb{N}$ so that $|\,a_m - a_n\,| < \varepsilon$ for $\forall m,n>N$. (Definition of a Cauchy Sequence) (Hint) The sequence $(\varepsilon\cdot\ell)_{\ell\in\mathbb{N}}$ is unbounded for $\varepsilon>0$. (Archimedes Principle) Would appreciate any help.",,"['calculus', 'real-analysis', 'sequences-and-series', 'cauchy-sequences']"
91,A pathological example of a differentiable function whose derivative is not integrable,A pathological example of a differentiable function whose derivative is not integrable,,"First I'll make a definition:  $$\operatorname{Loc-int}(g):=\left\lbrace x\in[0,1] : \exists \epsilon>0\text{ s.t. }\int_{(x-\epsilon,x+\epsilon)\cap[0,1]}|g|dm<\infty\right\rbrace,$$ where $m$ is the Lebesgue measure. I've been searching for weeks now for an example that suits the next terms: $$\forall \lambda \in(0,1) \exists f:[0,1] \to \mathbb{R}\text{ measurable, differentiable s.t. } m\left([0,1]/\operatorname{Loc-int}(f')\right)>\lambda.$$ Until now I've only managed to find an example of differentiable function that is not locally integrable  at $x=0$.  Any help would be appreciated!","First I'll make a definition:  $$\operatorname{Loc-int}(g):=\left\lbrace x\in[0,1] : \exists \epsilon>0\text{ s.t. }\int_{(x-\epsilon,x+\epsilon)\cap[0,1]}|g|dm<\infty\right\rbrace,$$ where $m$ is the Lebesgue measure. I've been searching for weeks now for an example that suits the next terms: $$\forall \lambda \in(0,1) \exists f:[0,1] \to \mathbb{R}\text{ measurable, differentiable s.t. } m\left([0,1]/\operatorname{Loc-int}(f')\right)>\lambda.$$ Until now I've only managed to find an example of differentiable function that is not locally integrable  at $x=0$.  Any help would be appreciated!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
92,Interpolation using trigonometric polynomials of bounded modulus,Interpolation using trigonometric polynomials of bounded modulus,,"Consider a grid of points $T=\{t_1,\ldots,t_m\}$ with $0\le t_i\le 1$. I would like to derive conditions on $t_1,\ldots,t_m$ (interpolation points) under which for any sequence of complex numbers $c_1,c_2,\ldots,c_m\in\mathbb{C}$ with $|c_k|=1$, there exists a function $f(t):[0,1]\rightarrow \mathbb{C}$ of the form \begin{equation*} f(t)=\sum_{k=-n}^n a_k e^{2\pi i k t} \end{equation*} (with n as small as possible) such that 1) $f(t)$ interpolates $c_1,c_2,\ldots,c_m\in\mathbb{C}$ on $T$. That is \begin{equation*} f(t_k)=c_k \end{equation*} 2) $|f(t)|< 1$ for $t\notin T$. I know that the condition on the interpolation nodes $t_1,\ldots,t_m$ should look something like the ones appearing below 1) $min_{k,\ell}|t_k-t_\ell|\ge 1/n$ (with the distance meant to be circular that is |0.9-0.1|=0.2). or 2) more sophisticated conditions like: $D_{m}(t_1,\ldots,t_m)$ needs to be small. The discrepancy of a a finite sequence of real numbers $x_1,x_2,\ldots,x_N\in[0,1]$ is defined as  \begin{equation*} D_N(x_1,x_2,\ldots,x_N)=\underset{0\le\alpha<\beta\le 1}{sup}\bigg|\frac{A([\alpha,\beta);N)}{N}-(\beta-\alpha)\bigg|, \end{equation*} with $A([\alpha,\beta);N)$ denoting the number of $x_i$ such that $x_i\in[\alpha,\beta)$ (Based on section 2 of Uniform Distribution of Sequences by Kuipers and Niederreiter).","Consider a grid of points $T=\{t_1,\ldots,t_m\}$ with $0\le t_i\le 1$. I would like to derive conditions on $t_1,\ldots,t_m$ (interpolation points) under which for any sequence of complex numbers $c_1,c_2,\ldots,c_m\in\mathbb{C}$ with $|c_k|=1$, there exists a function $f(t):[0,1]\rightarrow \mathbb{C}$ of the form \begin{equation*} f(t)=\sum_{k=-n}^n a_k e^{2\pi i k t} \end{equation*} (with n as small as possible) such that 1) $f(t)$ interpolates $c_1,c_2,\ldots,c_m\in\mathbb{C}$ on $T$. That is \begin{equation*} f(t_k)=c_k \end{equation*} 2) $|f(t)|< 1$ for $t\notin T$. I know that the condition on the interpolation nodes $t_1,\ldots,t_m$ should look something like the ones appearing below 1) $min_{k,\ell}|t_k-t_\ell|\ge 1/n$ (with the distance meant to be circular that is |0.9-0.1|=0.2). or 2) more sophisticated conditions like: $D_{m}(t_1,\ldots,t_m)$ needs to be small. The discrepancy of a a finite sequence of real numbers $x_1,x_2,\ldots,x_N\in[0,1]$ is defined as  \begin{equation*} D_N(x_1,x_2,\ldots,x_N)=\underset{0\le\alpha<\beta\le 1}{sup}\bigg|\frac{A([\alpha,\beta);N)}{N}-(\beta-\alpha)\bigg|, \end{equation*} with $A([\alpha,\beta);N)$ denoting the number of $x_i$ such that $x_i\in[\alpha,\beta)$ (Based on section 2 of Uniform Distribution of Sequences by Kuipers and Niederreiter).",,"['real-analysis', 'complex-analysis', 'trigonometry', 'fourier-analysis', 'harmonic-analysis']"
93,convergence of series with $k!$,convergence of series with,k!,"check if the following series converges: $\sum\limits_{k=1}^{\infty} (-1)^k \dfrac{(k-1)!!}{k!!}$ where $k!!=k(k-2)(k-4)(k-6)...$ I came across this exercise while going trough some old exams. I'm pretty sure we have to bound the sequence and apply Leibniz-criteria but after a while i gave up. If you have a little Hint for me just to get me strted , that would be great.","check if the following series converges: $\sum\limits_{k=1}^{\infty} (-1)^k \dfrac{(k-1)!!}{k!!}$ where $k!!=k(k-2)(k-4)(k-6)...$ I came across this exercise while going trough some old exams. I'm pretty sure we have to bound the sequence and apply Leibniz-criteria but after a while i gave up. If you have a little Hint for me just to get me strted , that would be great.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
94,Continuous function that is only differentiable on irrationals,Continuous function that is only differentiable on irrationals,,Can you help me finding a function $f : \mathbb{R} \rightarrow \mathbb{R}$ that is continuous in $\mathbb{R}$ and differentiable at $x$ iff $x \notin \mathbb{Q}$  ? Thank you very much !,Can you help me finding a function $f : \mathbb{R} \rightarrow \mathbb{R}$ that is continuous in $\mathbb{R}$ and differentiable at $x$ iff $x \notin \mathbb{Q}$  ? Thank you very much !,,"['real-analysis', 'functions']"
95,The open Mbius Band is not orientable,The open Mbius Band is not orientable,,"Can you explain my green underlying please.I have confused and  dont understand why by the continuity of the orientation at the points  $(0,0)$ and $(1,0)$ are also $e_{1},e_{2}$","Can you explain my green underlying please.I have confused and  dont understand why by the continuity of the orientation at the points  $(0,0)$ and $(1,0)$ are also $e_{1},e_{2}$",,"['real-analysis', 'linear-algebra', 'general-topology', 'differential-geometry', 'manifolds']"
96,"$M$ is compact, non-empty, perfect, and $M \cong M \times M$. Must $M$ be homeomorphic to the Cantor set, the Hilbert cube, or some combination?","is compact, non-empty, perfect, and . Must  be homeomorphic to the Cantor set, the Hilbert cube, or some combination?",M M \cong M \times M M,"Assume that $M$ is compact, non-empty, perfect, and homeomorphic to its Cartesian square, $M \cong M \times M$.  Must $M$ be homeomorphic to the Cantor set, the Hilbert cube, or some combination of them? An interesting triple-starred problem from Pugh's ""Real Mathematical Analysis"".  This is not from an assignment or anything graded, I'm just curious as to what the right answer is and the route that one may take to get there.","Assume that $M$ is compact, non-empty, perfect, and homeomorphic to its Cartesian square, $M \cong M \times M$.  Must $M$ be homeomorphic to the Cantor set, the Hilbert cube, or some combination of them? An interesting triple-starred problem from Pugh's ""Real Mathematical Analysis"".  This is not from an assignment or anything graded, I'm just curious as to what the right answer is and the route that one may take to get there.",,"['real-analysis', 'general-topology', 'analysis']"
97,One epsilon-delta statement implies the other. Darboux Integrability.,One epsilon-delta statement implies the other. Darboux Integrability.,,"I'm trying to show that one definition implies the other for Darboux Integrability. But, I don't really how to proceed. Here's the first statement. For all $\epsilon > 0$, there exists a partition P of $[a,b]$ such that $\vert U_p (f) - L_p (f) \vert < \epsilon$. Here's the second statement. For all $\epsilon > 0$, there exists a $\delta > 0$ such that $mesh (P) < \delta$ implies $\vert U_p (f) - L_p (f) \vert < \epsilon $. How do I prove something like this?  Any help? What's the strategy?","I'm trying to show that one definition implies the other for Darboux Integrability. But, I don't really how to proceed. Here's the first statement. For all $\epsilon > 0$, there exists a partition P of $[a,b]$ such that $\vert U_p (f) - L_p (f) \vert < \epsilon$. Here's the second statement. For all $\epsilon > 0$, there exists a $\delta > 0$ such that $mesh (P) < \delta$ implies $\vert U_p (f) - L_p (f) \vert < \epsilon $. How do I prove something like this?  Any help? What's the strategy?",,"['real-analysis', 'integration']"
98,Inverse Fourier transform and decay at infinity,Inverse Fourier transform and decay at infinity,,"Suppose $1<p<\infty$, $g:\mathbb{R} \to \mathbb{R}$. How can I see $\mathcal{F}^{-1}g \in L^p(\mathbb{R})$ as a condition on the decay of $g$ at infinity?","Suppose $1<p<\infty$, $g:\mathbb{R} \to \mathbb{R}$. How can I see $\mathcal{F}^{-1}g \in L^p(\mathbb{R})$ as a condition on the decay of $g$ at infinity?",,"['real-analysis', 'functional-analysis', 'fourier-analysis']"
99,"If $\lim_{x \to +\infty} f'(x) = L$, then $\lim_{x \to \infty} \frac {f(x)}{x} = L$","If , then",\lim_{x \to +\infty} f'(x) = L \lim_{x \to \infty} \frac {f(x)}{x} = L,"I'm trying to solve this question: Let $f:[0,+\infty) \to \mathbb{R}$ be derivable and $\lim_{x \to +\infty} f'(x) = L$, then $\lim_{x\to \infty}\frac {f(x)}{x}=L$. I'm trying to solve this question using l'Hpital rule, but I couldn't use it, because I don't know if $\lim_{x\to +\infty} f(x)=+\infty$. I need help. Thanks a lot.","I'm trying to solve this question: Let $f:[0,+\infty) \to \mathbb{R}$ be derivable and $\lim_{x \to +\infty} f'(x) = L$, then $\lim_{x\to \infty}\frac {f(x)}{x}=L$. I'm trying to solve this question using l'Hpital rule, but I couldn't use it, because I don't know if $\lim_{x\to +\infty} f(x)=+\infty$. I need help. Thanks a lot.",,['real-analysis']
