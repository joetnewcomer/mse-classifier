,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Decay of Fourier Transform of a Schwartz Function,Decay of Fourier Transform of a Schwartz Function,,"Suppose we have a function $f(x)\in \mathcal{S}(\mathbb R)$ ; that is, it is a function in Schwartz space . Further, suppose we know that $$|f(x)|\leq Ce^{-|x|}.$$ If it is helpful, we can actually replace the exponent on $|x|$ by any $1<c<2$ (in other words, it doesn't seem to be ""too far"" from a Gaussian). With this information, is there anything we can say about the decay of the Fourier transform of $f(x)$ beyond the fact that it is in the Schwartz class? In particular, does it necessarily decay like $f(x)$ , or could it decay much slower, say like $\exp(-(1+x^2)^c)$ for some arbitrarily small $c>0$ ? I've tried looking online and haven't found much. What I have found is: The Fourier transform is a linear isomorphism of the Schwartz space; in particular, we know that the Fourier transform is also in the Schwartz space The Gaussian, $g(x)=e^{-x^2}$ , is essentially a fixed point of this isomorphism (we introduce some constants, but the decay of the function and the decay of the transform is identical - since I'm only worried about the decay, I'm using the term ""fixed point"" a bit loosely). Some more information that might be helpful, though I couldn't find any way to use it specifically: $f(x)$ is essentially the characteristic function of a given random variable, which means that the Fourier transform is the corresponding density function. Specifically, this means the Fourier transform takes a maximum value at $0$ (which is equal to $1$ ) and decreases to $0$ as $|x|\to\infty$ . Even without anything specific, references would be appreciated. I've tried looking in Folland's book as well as Stein/Shakarchi's books, but these have not offered any insight for this problem.","Suppose we have a function ; that is, it is a function in Schwartz space . Further, suppose we know that If it is helpful, we can actually replace the exponent on by any (in other words, it doesn't seem to be ""too far"" from a Gaussian). With this information, is there anything we can say about the decay of the Fourier transform of beyond the fact that it is in the Schwartz class? In particular, does it necessarily decay like , or could it decay much slower, say like for some arbitrarily small ? I've tried looking online and haven't found much. What I have found is: The Fourier transform is a linear isomorphism of the Schwartz space; in particular, we know that the Fourier transform is also in the Schwartz space The Gaussian, , is essentially a fixed point of this isomorphism (we introduce some constants, but the decay of the function and the decay of the transform is identical - since I'm only worried about the decay, I'm using the term ""fixed point"" a bit loosely). Some more information that might be helpful, though I couldn't find any way to use it specifically: is essentially the characteristic function of a given random variable, which means that the Fourier transform is the corresponding density function. Specifically, this means the Fourier transform takes a maximum value at (which is equal to ) and decreases to as . Even without anything specific, references would be appreciated. I've tried looking in Folland's book as well as Stein/Shakarchi's books, but these have not offered any insight for this problem.",f(x)\in \mathcal{S}(\mathbb R) |f(x)|\leq Ce^{-|x|}. |x| 1<c<2 f(x) f(x) \exp(-(1+x^2)^c) c>0 g(x)=e^{-x^2} f(x) 0 1 0 |x|\to\infty,"['real-analysis', 'reference-request', 'fourier-analysis', 'fourier-transform']"
1,Surjectivity of real continuous expansive-type functions,Surjectivity of real continuous expansive-type functions,,"Let $f:\mathbb{R}^n \to \mathbb{R}^n$ be continuous and let there exist $\alpha > 0$ such that $||f(\mathbf{x}) - f(\mathbf{y})|| \geq \alpha || \mathbf{x} - \mathbf{y}||$ for all $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$ . Prove that $f$ is one-one, onto and that $f^{-1}$ is continuous. One-one is trivial. It is onto-ness that I can't show. Write $S = f(\mathbb{R}^n)$ . Using sequential continuity, it is possible to show that $S$ is closed. If I could show $S$ is open, I would be done, but I can't. Also, writing $g(\mathbf{x}) := \dfrac{f(\mathbf{x})}{\alpha}$ , the condition can be converted to that of proper expansive map, $||g(\mathbf{x}) - g(\mathbf{y})|| \geq || \mathbf{x} - \mathbf{y}||$ . But since $\mathbb{R}^n$ is not compact, I cannot use the result here . Any help is appreciated! EDIT: As commented below, the Invariance of Domain theorem seems to work in this case, but that result does not use the expansive-type condition provided here (except for showing the injectivity), and so it appears that an easier proof would be possible.","Let be continuous and let there exist such that for all . Prove that is one-one, onto and that is continuous. One-one is trivial. It is onto-ness that I can't show. Write . Using sequential continuity, it is possible to show that is closed. If I could show is open, I would be done, but I can't. Also, writing , the condition can be converted to that of proper expansive map, . But since is not compact, I cannot use the result here . Any help is appreciated! EDIT: As commented below, the Invariance of Domain theorem seems to work in this case, but that result does not use the expansive-type condition provided here (except for showing the injectivity), and so it appears that an easier proof would be possible.","f:\mathbb{R}^n \to \mathbb{R}^n \alpha > 0 ||f(\mathbf{x}) - f(\mathbf{y})|| \geq \alpha || \mathbf{x} - \mathbf{y}|| \mathbf{x}, \mathbf{y} \in \mathbb{R}^n f f^{-1} S = f(\mathbb{R}^n) S S g(\mathbf{x}) := \dfrac{f(\mathbf{x})}{\alpha} ||g(\mathbf{x}) - g(\mathbf{y})|| \geq || \mathbf{x} - \mathbf{y}|| \mathbb{R}^n","['real-analysis', 'general-topology', 'continuity']"
2,"For what metric spaces $(X, d) \ \exists s > 0$ s.t for all $\epsilon < s$ and all $x \in X$, we have $\text{diam}(B_d(x, \epsilon)) = 2\epsilon$?","For what metric spaces  s.t for all  and all , we have ?","(X, d) \ \exists s > 0 \epsilon < s x \in X \text{diam}(B_d(x, \epsilon)) = 2\epsilon","I was trying to prove that, in general, the diameter of an open ball $B_d(x, \epsilon)$ in a metric space $(X, d)$ is equal to $2\epsilon$ . It then occurred to me that this is not the case if the metric induces the discrete topology, so one necessary (but probably not sufficient) condition is that $d$ doesn't induce the discrete topology. A trivial example of where this is true is $\mathbb{R}^n$ with the euclidean metric. So, what can we impose to make sure that the diameters are exactly twice the radius? Observation: if $A\subset X$ is bounded, the diameter of $A$ is $:= \displaystyle{\sup_{a_1, a_2 \in A} d(a_1, a_2)}$ EDIT: to the people voting to close the question - could you at least mention your reasons in a comment? It's hard to know just what I'm doing wrong otherwise... EDIT 2: It's been brought to my attention that maybe the initial question isn't that interesting so I've edited the title. Still, if we could find a necessary and sufficient condition, that would be super nice!","I was trying to prove that, in general, the diameter of an open ball in a metric space is equal to . It then occurred to me that this is not the case if the metric induces the discrete topology, so one necessary (but probably not sufficient) condition is that doesn't induce the discrete topology. A trivial example of where this is true is with the euclidean metric. So, what can we impose to make sure that the diameters are exactly twice the radius? Observation: if is bounded, the diameter of is EDIT: to the people voting to close the question - could you at least mention your reasons in a comment? It's hard to know just what I'm doing wrong otherwise... EDIT 2: It's been brought to my attention that maybe the initial question isn't that interesting so I've edited the title. Still, if we could find a necessary and sufficient condition, that would be super nice!","B_d(x, \epsilon) (X, d) 2\epsilon d \mathbb{R}^n A\subset X A := \displaystyle{\sup_{a_1, a_2 \in A} d(a_1, a_2)}","['real-analysis', 'general-topology', 'metric-spaces']"
3,Integration with respect to the Lebesgue-Stieltjes measure associated with floor function,Integration with respect to the Lebesgue-Stieltjes measure associated with floor function,,"For two different measures, I am trying to identify $L^1(\mu)$ (the set of integrable functions, i.e. $\{f:\int \lvert f\rvert du<\infty\}$ , and trying to compute $\int fd\mu$ for $f\in L^1(\mu)$ . on $\mathbb R, \mu=\mu_F$ , the Lebesgue-Stieltjes measure with $F(x)=\lfloor x\rfloor$ . Attempts I know that the unique measure derived from the floor function would measure the number of integers between the end points, or in the set. But I'm not sure how to build on that to use to calculate the integral for a general $f$ , or what would make a function integrable. Update : $$\int 1_{E}(x)d\mu(x)=\mu(E)=\sum_{n\in\mathbb Z}1_E(n)$$ should be true for any characteristic function. And for this integral to be finite, in any Borel set $E$ , there should only be finite number of integers in the set. But I'm not sure how to bootsrap up from this point to simple functions, and then functions in general. Any help would be greatly appreciated!","For two different measures, I am trying to identify (the set of integrable functions, i.e. , and trying to compute for . on , the Lebesgue-Stieltjes measure with . Attempts I know that the unique measure derived from the floor function would measure the number of integers between the end points, or in the set. But I'm not sure how to build on that to use to calculate the integral for a general , or what would make a function integrable. Update : should be true for any characteristic function. And for this integral to be finite, in any Borel set , there should only be finite number of integers in the set. But I'm not sure how to bootsrap up from this point to simple functions, and then functions in general. Any help would be greatly appreciated!","L^1(\mu) \{f:\int \lvert f\rvert du<\infty\} \int fd\mu f\in L^1(\mu) \mathbb R, \mu=\mu_F F(x)=\lfloor x\rfloor f \int 1_{E}(x)d\mu(x)=\mu(E)=\sum_{n\in\mathbb Z}1_E(n) E","['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral']"
4,Convergence of an integral involving the radical of an integer versus the convergence of $\int_2^\infty\frac{dx}{x(\log x)^2}$,Convergence of an integral involving the radical of an integer versus the convergence of,\int_2^\infty\frac{dx}{x(\log x)^2},"For positive integers $n\geq 2$, let $\operatorname{rad}(n)$ the radical of the integer $n$. For example $\operatorname{rad}(24)=6$. See this Wikipedia to know this definition. Question. I would like to know if it is possible to discuss the convergence of this integral $$\int_2^\infty\frac{dx}{\left(\operatorname{rad}\left(\lfloor x\rfloor\right)\right)^{\alpha}\log ^2(x)},\tag{1}$$ where $\lfloor x\rfloor$ is the floor function and $\alpha\geq 1$ a fixed real number. Thanks in advance.","For positive integers $n\geq 2$, let $\operatorname{rad}(n)$ the radical of the integer $n$. For example $\operatorname{rad}(24)=6$. See this Wikipedia to know this definition. Question. I would like to know if it is possible to discuss the convergence of this integral $$\int_2^\infty\frac{dx}{\left(\operatorname{rad}\left(\lfloor x\rfloor\right)\right)^{\alpha}\log ^2(x)},\tag{1}$$ where $\lfloor x\rfloor$ is the floor function and $\alpha\geq 1$ a fixed real number. Thanks in advance.",,"['real-analysis', 'convergence-divergence']"
5,Improved Bernoulli's Inequality,Improved Bernoulli's Inequality,,"Applying the first theorem from this paper directly to the function $(1+x)^r$, with $0<r<1$ and $r$ rational (strictly speaking), I was able to show this refinement of Bernoulli's Inequality $$(1+x)^r\leq 1+rx-r(1-r)(\sqrt{1+x}-1)^2$$ when $r$ lies between $0$ and $1$ (we use continuity to extend this result to irrationals) and for $x\geq -1$. Besides being quite sharp, this inequality also wraps up the case of equality in the more canonical Bernoulli's Inequality. Naturally, this result begs to have a dual proved. That is, can we   find a function $f$ and a constant $C_r$ such that $$(1+x)^r\geq 1+rx+C_r(f(x))^2$$ where $r>1$, $x\geq -1$, $C_r>0$ is allowed to   depend on $r$, and $f$ has a unique root at $0$? The thought that occurred to me was to choose the positive integer $n$ such that $n-1\leq r<n$ and then apply the previous result to $$\left(1+\frac{r}{n}x\right)^{n/r}=\left(1+\frac{r}{n}x\right)^{(n-1)/r}\left(1+\frac{r}{n}x\right)^{1/r}$$ and hopefully take powers and apply the integer form of Bernoulli to the left-hand side. The algebra have proved tedious and subtle however. I've thought about trudging through the Taylor Formula for $(1+x)^r$ (Newton's binomial expansion), but the case analysis on $r$ and $x$ doesn't seem fruitful or pretty.","Applying the first theorem from this paper directly to the function $(1+x)^r$, with $0<r<1$ and $r$ rational (strictly speaking), I was able to show this refinement of Bernoulli's Inequality $$(1+x)^r\leq 1+rx-r(1-r)(\sqrt{1+x}-1)^2$$ when $r$ lies between $0$ and $1$ (we use continuity to extend this result to irrationals) and for $x\geq -1$. Besides being quite sharp, this inequality also wraps up the case of equality in the more canonical Bernoulli's Inequality. Naturally, this result begs to have a dual proved. That is, can we   find a function $f$ and a constant $C_r$ such that $$(1+x)^r\geq 1+rx+C_r(f(x))^2$$ where $r>1$, $x\geq -1$, $C_r>0$ is allowed to   depend on $r$, and $f$ has a unique root at $0$? The thought that occurred to me was to choose the positive integer $n$ such that $n-1\leq r<n$ and then apply the previous result to $$\left(1+\frac{r}{n}x\right)^{n/r}=\left(1+\frac{r}{n}x\right)^{(n-1)/r}\left(1+\frac{r}{n}x\right)^{1/r}$$ and hopefully take powers and apply the integer form of Bernoulli to the left-hand side. The algebra have proved tedious and subtle however. I've thought about trudging through the Taylor Formula for $(1+x)^r$ (Newton's binomial expansion), but the case analysis on $r$ and $x$ doesn't seem fruitful or pretty.",,['real-analysis']
6,"Prob. 8, Chap. 5, in Rudin's PMS, 3rd ed: If $f^\prime$ is continuous on $[a, b]$, then $f$ is uniformly differentiable on $[a,b]$","Prob. 8, Chap. 5, in Rudin's PMS, 3rd ed: If  is continuous on , then  is uniformly differentiable on","f^\prime [a, b] f [a,b]","Here is Prob. 8, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f^\prime$ is continuous on $[a, b]$ and $\varepsilon > 0$ . Prove that there exists $\delta > 0$ such that $$ \left\lvert \frac{f(t)-f(x)}{t-x} - f^\prime(x) \right\rvert < \varepsilon  $$ whenever $0 < |t-x| < \delta$ , $a \leq x \leq b$ , $a \leq t \leq b$ . (This could be expressed by saying that $f$ is uniformly differentiable on $[a, b]$ if $f^\prime$ is continuous on $[a, b]$ .) Does this hold for vector-valued functions too? My Attempt: As $f^\prime$ is continuous on $[a, b]$ and as $[a, b]$ is compact, so $f^\prime$ is uniformly continuous on $[a, b]$ . So, for any real number $\varepsilon > 0$ , we can find a real number $\delta > 0$ such that $$  \left\lvert f^\prime(x) - f^\prime(y) \right\rvert < \varepsilon \tag{1} $$ for all $x, y \in [a, b]$ for which $\lvert x-y \rvert < \delta$ . Now suppose $a \leq t \leq b$ , $a \leq x \leq b$ , and $0 < \lvert t-x \rvert < \delta$ . Then by the Mean Value Theorem there is some point $y$ between $t$ and $x$ such that $$ f(t) - f(x) = (t-x) f^\prime(y), \tag{2} $$ and also $$ \lvert y-x \rvert < \lvert t-x \rvert  < \delta; \tag{3} $$ moreover, as $\lvert t-x\rvert > 0$ , so $t \neq x$ , and from (2) we can write $$ \frac{ f(t) - f(x)}{t-x} = f^\prime(y),$$ which together with (3) and (1) yields \begin{align}   \left\lvert \frac{f(t)-f(x)}{t-x} - f^\prime(x) \right\rvert &= \left\lvert f^\prime(y) - f^\prime(x) \right\rvert \\  &< \varepsilon. \end{align} Am I right? Now for vector-valued functions. Suppose $$\mathbf{f} = \left( f_1, \ldots, f_k \right) $$ be a mapping of $[a, b]$ into some $\mathbb{R}^k$ and suppose that $$  \mathbf{f}^\prime = \left( f_1^\prime, \ldots, f_k^\prime \right)  $$ is continuous on $[a, b]$ . Then each of the component functions $f_1^\prime, \ldots, f_k^\prime$ is also continuous on $[a, b]$ . So, given any real number $\varepsilon > 0$ , we can find real numbers $\delta_i$ , for $i = 1, \ldots, k$ , such that $$ \left\lvert  \frac{ f_i(t) - f_i(x) }{ t-x } - f_i^\prime(x) \right\rvert  < \frac{ \varepsilon }{\sqrt{k}} $$ whenever $0 < \lvert t-x \rvert < \delta_i$ , $a \leq t \leq b$ , and $a \leq x \leq b$ . Now let $$\delta := \min \left\{ \delta_1, \ldots, \delta_k \right\}.$$ Therefore, If $a \leq t \leq b$ , $a \leq x \leq b$ , and $0 < \lvert t-x \rvert  < \delta$ , then for each $i = 1, \ldots, k$ , we obtain $0 < \lvert t-x \rvert < \delta_i$ and so $$ \left\lvert  \frac{ f_i(t) - f_i(x) }{ t-x } - f_i^\prime(x) \right\rvert < \frac{ \varepsilon }{\sqrt{k}}, $$ which then implies that \begin{align} & \ \ \ \left\lvert  \frac{ \mathbf{f}(t) - \mathbf{f}(x) }{ t-x } - \mathbf{f}^\prime(x) \right\rvert \\  &= \left\lvert   \left( \frac{f_1(t) - f_1(x)}{t-x}, \ldots, \frac{f_k(t) - f_k(x) }{t-x} \right) - \left( f_1^\prime(x), \ldots, f_k^\prime(x) \right) \right\rvert  \\  &= \left\lvert \left( \frac{ f_1(t) - f_1(x)}{t-x} - f_1^\prime(x), \ldots, \frac{ f_k(t) - f_k(x)}{t-x} - f_k^\prime(x) \right) \right\rvert  \\ &= \sqrt{ \sum_{i=1}^k \left\lvert \frac{ f_i(t) - f_i(x)}{t-x} - f_i^\prime(x) \right\rvert^2 } \\ &< \sqrt{ \sum_{i=1}^k \frac{\varepsilon^2}{k} } \\ &= \varepsilon. \end{align} Thus the above result holds for vector-valued functions as well. Am I right? Is my reasoning correct in each of the above two cases? If not, then where have I erred?","Here is Prob. 8, Chap. 5, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose is continuous on and . Prove that there exists such that whenever , , . (This could be expressed by saying that is uniformly differentiable on if is continuous on .) Does this hold for vector-valued functions too? My Attempt: As is continuous on and as is compact, so is uniformly continuous on . So, for any real number , we can find a real number such that for all for which . Now suppose , , and . Then by the Mean Value Theorem there is some point between and such that and also moreover, as , so , and from (2) we can write which together with (3) and (1) yields Am I right? Now for vector-valued functions. Suppose be a mapping of into some and suppose that is continuous on . Then each of the component functions is also continuous on . So, given any real number , we can find real numbers , for , such that whenever , , and . Now let Therefore, If , , and , then for each , we obtain and so which then implies that Thus the above result holds for vector-valued functions as well. Am I right? Is my reasoning correct in each of the above two cases? If not, then where have I erred?","f^\prime [a, b] \varepsilon > 0 \delta > 0  \left\lvert \frac{f(t)-f(x)}{t-x} - f^\prime(x) \right\rvert < \varepsilon 
 0 < |t-x| < \delta a \leq x \leq b a \leq t \leq b f [a, b] f^\prime [a, b] f^\prime [a, b] [a, b] f^\prime [a, b] \varepsilon > 0 \delta > 0  
\left\lvert f^\prime(x) - f^\prime(y) \right\rvert < \varepsilon \tag{1}  x, y \in [a, b] \lvert x-y \rvert < \delta a \leq t \leq b a \leq x \leq b 0 < \lvert t-x \rvert < \delta y t x  f(t) - f(x) = (t-x) f^\prime(y), \tag{2}   \lvert y-x \rvert < \lvert t-x \rvert  < \delta; \tag{3}  \lvert t-x\rvert > 0 t \neq x 
\frac{ f(t) - f(x)}{t-x} = f^\prime(y), \begin{align}  
\left\lvert \frac{f(t)-f(x)}{t-x} - f^\prime(x) \right\rvert &= \left\lvert f^\prime(y) - f^\prime(x) \right\rvert \\ 
&< \varepsilon.
\end{align} \mathbf{f} = \left( f_1, \ldots, f_k \right)  [a, b] \mathbb{R}^k  
\mathbf{f}^\prime = \left( f_1^\prime, \ldots, f_k^\prime \right) 
 [a, b] f_1^\prime, \ldots, f_k^\prime [a, b] \varepsilon > 0 \delta_i i = 1, \ldots, k 
\left\lvert  \frac{ f_i(t) - f_i(x) }{ t-x } - f_i^\prime(x) \right\rvert  < \frac{ \varepsilon }{\sqrt{k}}  0 < \lvert t-x \rvert < \delta_i a \leq t \leq b a \leq x \leq b \delta := \min \left\{ \delta_1, \ldots, \delta_k \right\}. a \leq t \leq b a \leq x \leq b 0 < \lvert t-x \rvert  < \delta i = 1, \ldots, k 0 < \lvert t-x \rvert < \delta_i 
\left\lvert  \frac{ f_i(t) - f_i(x) }{ t-x } - f_i^\prime(x) \right\rvert < \frac{ \varepsilon }{\sqrt{k}},  \begin{align}
& \ \ \ \left\lvert  \frac{ \mathbf{f}(t) - \mathbf{f}(x) }{ t-x } - \mathbf{f}^\prime(x) \right\rvert \\ 
&= \left\lvert   \left( \frac{f_1(t) - f_1(x)}{t-x}, \ldots, \frac{f_k(t) - f_k(x) }{t-x} \right) - \left( f_1^\prime(x), \ldots, f_k^\prime(x) \right) \right\rvert  \\ 
&= \left\lvert \left( \frac{ f_1(t) - f_1(x)}{t-x} - f_1^\prime(x), \ldots, \frac{ f_k(t) - f_k(x)}{t-x} - f_k^\prime(x) \right) \right\rvert  \\
&= \sqrt{ \sum_{i=1}^k \left\lvert \frac{ f_i(t) - f_i(x)}{t-x} - f_i^\prime(x) \right\rvert^2 } \\
&< \sqrt{ \sum_{i=1}^k \frac{\varepsilon^2}{k} } \\
&= \varepsilon.
\end{align}","['calculus', 'real-analysis', 'analysis', 'derivatives']"
7,"On the monotony of $-\int_0^1\frac{e^y}{y}(\operatorname{Li}_x(1-y)-\zeta(x)) \,\mathrm dy$",On the monotony of,"-\int_0^1\frac{e^y}{y}(\operatorname{Li}_x(1-y)-\zeta(x)) \,\mathrm dy","$\def\d{\mathrm{d}}$ After I was studying variations of the integral representation for $\zeta(3)$ due to Beukers, see the section More complicated formulas from this Wikipedia , I've thought an exercise. The motivation is this antiderivative $$-\int_0^1e^y\frac{\log(xy)}{1-xy}\,\d x=-\frac{e^y}{y}\operatorname{Li}_2(1-xy)+\mathrm{constant}.$$ For real numbers $x\geq 2$ , $$f(x):=-\int_0^1\frac{e^y}{y}(\operatorname{Li}_x(1-y)-\zeta(x)) \,\d y,$$ I've calculated with Wolfram Alpha online calculator some particular values at integer values $x=n\geq 2$ . Question. A) Is it possible to prove that $f(x)$ is decreasing for $x\geq 2$ ? B) Is it possible to calculate $$\lim_{x\to\infty}f(x)?$$ Thanks in advance. Also I know Euler's Zeta function $\zeta(x)$ tends to $1$ as $x\to\infty$ . With respect the definite integral I believe that is (impossible) difficult to get a closed-form for such values.","After I was studying variations of the integral representation for due to Beukers, see the section More complicated formulas from this Wikipedia , I've thought an exercise. The motivation is this antiderivative For real numbers , I've calculated with Wolfram Alpha online calculator some particular values at integer values . Question. A) Is it possible to prove that is decreasing for ? B) Is it possible to calculate Thanks in advance. Also I know Euler's Zeta function tends to as . With respect the definite integral I believe that is (impossible) difficult to get a closed-form for such values.","\def\d{\mathrm{d}} \zeta(3) -\int_0^1e^y\frac{\log(xy)}{1-xy}\,\d x=-\frac{e^y}{y}\operatorname{Li}_2(1-xy)+\mathrm{constant}. x\geq 2 f(x):=-\int_0^1\frac{e^y}{y}(\operatorname{Li}_x(1-y)-\zeta(x)) \,\d y, x=n\geq 2 f(x) x\geq 2 \lim_{x\to\infty}f(x)? \zeta(x) 1 x\to\infty","['calculus', 'real-analysis']"
8,Finding limit of sequence: $\lim _{n \to \infty} {\frac{n!}{n^k(n-k)!}}=1$,Finding limit of sequence:,\lim _{n \to \infty} {\frac{n!}{n^k(n-k)!}}=1,"$k$ is nonnegative integer. I want to show that$$ \lim _{n \to \infty} {\frac{n!}{n^k(n-k)!}}=1$$ My try : $$ \frac{n!}{n^k(n-k)!} = \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n}$$ I wanted use multiplicative rule If $a_n \to a$ and $b_n \to b$ , then $a_n b_n \to ab$. But It is impossible because  $$ \frac{n!}{n^k(n-k)!} = \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n}$$ It is infinite product. I want you to help me.","$k$ is nonnegative integer. I want to show that$$ \lim _{n \to \infty} {\frac{n!}{n^k(n-k)!}}=1$$ My try : $$ \frac{n!}{n^k(n-k)!} = \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n}$$ I wanted use multiplicative rule If $a_n \to a$ and $b_n \to b$ , then $a_n b_n \to ab$. But It is impossible because  $$ \frac{n!}{n^k(n-k)!} = \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n}$$ It is infinite product. I want you to help me.",,"['real-analysis', 'limits', 'factorial']"
9,Showing $\sum_{n = 0}^\infty \int f^n$ converges,Showing  converges,\sum_{n = 0}^\infty \int f^n,"I am having trouble solving a real analysis qualifying exam problem. The question assumes $\mu(X) < \infty$ and $\left| f \right| < 1$ (EDIT: Assume $f$ is real-valued).  We are to show that $$ \lim_{n \to \infty} \int_X 1 + f + \dots + f^n d\mu$$ exists, possibly equal to $\infty$. My work so far. Each integral in the sequence makes sense since $\int 1 + \left| f \right| + \dots + \left| f \right|^n < (n+1) \mu(X) < \infty$.  Rephrasing the problem, we want to show $\sum_{n = 0}^\infty \int f^n$ converges.  It is immediate by the Monotone Convergence Theorem that the result is true for nonnegative functions $f$.  Considering absolute convergence, we have $$\sum \left| \int f^n \right| \leq \sum \int \left| f \right|^n$$ where the series on the right converges by what we just said.  If said series is finite, then $\sum \int f^n$ converges absolutely, hence converges. Question. I am stuck on the case that $$ \sum \int \left| f \right|^n = \infty. \;\;\;\;\;\;\;\;\; (*)$$ I know from the statement of the problem that we are allowing for $\sum \int f^n = \infty$, but it is not clear to me whether this should follow from $(*)$.  We do know $$\sum \int \left| f \right|^n = \int \sum \left| f \right|^n = \frac{1}{1 - \left| f \right|}.$$  So if this equals $\infty$, then $\mu \left\{ x \colon \left| f(x) \right| > 1 - \frac{1}{n} \right\} > 0$ for all $n$.  And of course $\sum f^n = \frac{1}{1 - f}$ as well.  But I can't see how to put this all together. Any help would be much appreciated.  Thanks.","I am having trouble solving a real analysis qualifying exam problem. The question assumes $\mu(X) < \infty$ and $\left| f \right| < 1$ (EDIT: Assume $f$ is real-valued).  We are to show that $$ \lim_{n \to \infty} \int_X 1 + f + \dots + f^n d\mu$$ exists, possibly equal to $\infty$. My work so far. Each integral in the sequence makes sense since $\int 1 + \left| f \right| + \dots + \left| f \right|^n < (n+1) \mu(X) < \infty$.  Rephrasing the problem, we want to show $\sum_{n = 0}^\infty \int f^n$ converges.  It is immediate by the Monotone Convergence Theorem that the result is true for nonnegative functions $f$.  Considering absolute convergence, we have $$\sum \left| \int f^n \right| \leq \sum \int \left| f \right|^n$$ where the series on the right converges by what we just said.  If said series is finite, then $\sum \int f^n$ converges absolutely, hence converges. Question. I am stuck on the case that $$ \sum \int \left| f \right|^n = \infty. \;\;\;\;\;\;\;\;\; (*)$$ I know from the statement of the problem that we are allowing for $\sum \int f^n = \infty$, but it is not clear to me whether this should follow from $(*)$.  We do know $$\sum \int \left| f \right|^n = \int \sum \left| f \right|^n = \frac{1}{1 - \left| f \right|}.$$  So if this equals $\infty$, then $\mu \left\{ x \colon \left| f(x) \right| > 1 - \frac{1}{n} \right\} > 0$ for all $n$.  And of course $\sum f^n = \frac{1}{1 - f}$ as well.  But I can't see how to put this all together. Any help would be much appreciated.  Thanks.",,"['real-analysis', 'measure-theory']"
10,Fractional order Riemann Stieltjes integral,Fractional order Riemann Stieltjes integral,,The definition of fractional order integral is well-known. Is there any definition for fractional order Riemann Stieltjes integral?,The definition of fractional order integral is well-known. Is there any definition for fractional order Riemann Stieltjes integral?,,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'fractional-calculus']"
11,What are the solutions for $a(n)$ and $b(n)$ when $a(n+1)=a(n)b(n)$ and $b(n+1)=a(n)+b(n)$?,What are the solutions for  and  when  and ?,a(n) b(n) a(n+1)=a(n)b(n) b(n+1)=a(n)+b(n),"If you have the following recurrence relations : $a(n+1)= a(n) b(n)$ $b(n+1)= a(n) + b(n) $ How do you find the form of $a(n)$ and $b(n)$ ? I suspect there isn't a closed form , but a infinit sum is good enough . Also what should the values $a(0)$ and $b(0)$, be so that the limit of $b(n)$ as n goes to infinity is 1 ?","If you have the following recurrence relations : $a(n+1)= a(n) b(n)$ $b(n+1)= a(n) + b(n) $ How do you find the form of $a(n)$ and $b(n)$ ? I suspect there isn't a closed form , but a infinit sum is good enough . Also what should the values $a(0)$ and $b(0)$, be so that the limit of $b(n)$ as n goes to infinity is 1 ?",,"['real-analysis', 'recurrence-relations', 'recursive-algorithms']"
12,"If $f$ maps sets of measure zero to sets of measure zero, then so does $g(x)=x+f(x)$.","If  maps sets of measure zero to sets of measure zero, then so does .",f g(x)=x+f(x),"I want to prove the following. Let $f:[a,b]\to\mathbb{R}$ be continuous and non-decreasing, and suppose that $f$ maps sets of (Lebesgue) measure zero to sets of measure zero. Then, so does   $$g(x)=x+f(x).$$ This is used in a proof in Rudin Real and Complex Analysis, but I can't understand the argument. He simply say that this ""follows easily"" from the fact that: ""If the $f$-image of some segment of length $\eta$ has length $\eta'$, then the $g$-image of that same segment has length $\eta+\eta'$."" I am able to prove that this statement is true, but how does that imply that $g$ maps null sets to null sets?","I want to prove the following. Let $f:[a,b]\to\mathbb{R}$ be continuous and non-decreasing, and suppose that $f$ maps sets of (Lebesgue) measure zero to sets of measure zero. Then, so does   $$g(x)=x+f(x).$$ This is used in a proof in Rudin Real and Complex Analysis, but I can't understand the argument. He simply say that this ""follows easily"" from the fact that: ""If the $f$-image of some segment of length $\eta$ has length $\eta'$, then the $g$-image of that same segment has length $\eta+\eta'$."" I am able to prove that this statement is true, but how does that imply that $g$ maps null sets to null sets?",,"['real-analysis', 'measure-theory']"
13,Understanding Lipschitz domain,Understanding Lipschitz domain,,"Here is the definition of Lipschitz domain given by Wikipedia. Let n ∈ N, and let Ω be an open subset of Rn. Let ∂Ω denote the boundary of Ω. Then Ω is said to have Lipschitz boundary, and is called a Lipschitz domain, if, for every point p ∈ ∂Ω, there exists a radius r > 0 and a map $h_p$ : $B_r(p)$ → Q such that (i) $h_p$ is a bijection; (ii) $h_p$ and $h^{-1}_p $ are both Lipschitz continuous functions; (iii)$h_p$(∂Ω ∩ Br(p)) = $Q_0$ (iv) $h_p$(Ω ∩ Br(p)) = $Q_+$; where $B_{r} (p) := \{ x \in \mathbb{R}^{n} | \| x - p \| < r \}$ denotes the n-dimensional open ball of radius r about p, Q denotes the unit ball B1(0), and $Q_{0} := \{ (x_{1}, \dots, x_{n}) \in Q | x_{n} = 0 \}$; $Q_{+} := \{ (x_{1}, \dots, x_{n}) \in Q | x_{n} > 0 \}$. Then, it says that  ""a Lipschitz domain (or domain with Lipschitz boundary) is a domain in Euclidean space whose boundary is ""sufficiently regular"" in the sense that it can be thought of as locally being the graph of a Lipschitz continuous function."" What I do not understand is the part that says that the boundary of Lipschitz can be thought of as the graph of a Lipschitz continuous function. What does it mean by the graph of a Lipschitz function? Which Lipschitz function does it talk about?  Does it refer to the function $h_p$ as given above? Please help me understand this!!","Here is the definition of Lipschitz domain given by Wikipedia. Let n ∈ N, and let Ω be an open subset of Rn. Let ∂Ω denote the boundary of Ω. Then Ω is said to have Lipschitz boundary, and is called a Lipschitz domain, if, for every point p ∈ ∂Ω, there exists a radius r > 0 and a map $h_p$ : $B_r(p)$ → Q such that (i) $h_p$ is a bijection; (ii) $h_p$ and $h^{-1}_p $ are both Lipschitz continuous functions; (iii)$h_p$(∂Ω ∩ Br(p)) = $Q_0$ (iv) $h_p$(Ω ∩ Br(p)) = $Q_+$; where $B_{r} (p) := \{ x \in \mathbb{R}^{n} | \| x - p \| < r \}$ denotes the n-dimensional open ball of radius r about p, Q denotes the unit ball B1(0), and $Q_{0} := \{ (x_{1}, \dots, x_{n}) \in Q | x_{n} = 0 \}$; $Q_{+} := \{ (x_{1}, \dots, x_{n}) \in Q | x_{n} > 0 \}$. Then, it says that  ""a Lipschitz domain (or domain with Lipschitz boundary) is a domain in Euclidean space whose boundary is ""sufficiently regular"" in the sense that it can be thought of as locally being the graph of a Lipschitz continuous function."" What I do not understand is the part that says that the boundary of Lipschitz can be thought of as the graph of a Lipschitz continuous function. What does it mean by the graph of a Lipschitz function? Which Lipschitz function does it talk about?  Does it refer to the function $h_p$ as given above? Please help me understand this!!",,"['real-analysis', 'differential-geometry', 'partial-differential-equations']"
14,"Use $C^\infty$ function to approximate $W^{1,\infty}$ function in finite domain",Use  function to approximate  function in finite domain,"C^\infty W^{1,\infty}","This is exercise 10.21 from Leoni's book. The exercise asks me to prove that for any $u\in W^{1,\infty}(\Omega)$ where $\Omega$ is open FINITE, there exists a sequence $(u_n)\subset C^\infty(\Omega)$ such that  $$\|u_n-u\|_{L^\infty(\Omega)}\to 0 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ and $$\|\nabla u_n\|_{L^\infty(\Omega)}\to \|\nabla u\|_{L^\infty(\Omega)}\,\,\text{with }\,\,\nabla u_n\to\nabla u \,\,\text{a.e.} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ The exercise gives a hint that I should try to modify the prove of Meyers-Serrin theorem that shows every $W^{1,p}$ function can be approximated by $C^\infty$ function in $W^{1,p}$ norm. This is what I did so far: I first make a claim. Assume $U\subset R^n$ is open and $U'\subset\subset U$. Hence, for $\epsilon>0$ small enough we can define $u_\epsilon:=\eta_\epsilon \ast u$ inside $U'$ where $\eta_\epsilon$ is the standard mollifier. Then I claim that there is a subsequence of $u_\epsilon$, still denote as $u_\epsilon$, such that $u_\epsilon$ satisfies $(1)$ and $(2)$ in $U'$ (with $\Omega$ replaced by $U'$). To show $u_\epsilon\to u$ uniformly in $U'$, I use ascoli-arzela theorem. Clearly, from the definition of mollification, we have $$\sup_{\epsilon>0}\|u_\epsilon\|_{L^{\infty}(U')}\leq \|u\|_{L^{\infty}(U)},\,\sup_{\epsilon>0}\|\nabla u_\epsilon\|_{L^{\infty}(U')}\leq \|\nabla u\|_{L^{\infty}(U)} \,\,\,\,\,\,\,\,\,\,(3)$$ Hence, ascoli-arzela theorem states that, up to a subsequence, that $u_\epsilon\to u$ uniformly, i.e., in $L^\infty(U')$ because $U'$ is compact. This gives $(1)$ in $U'$. To show $(2)$, we deduce from $(3)$ that, up to a subsequence,$\nabla u_\epsilon\to \nabla u$ in weak star sense, and hence we have $\liminf \|\nabla u_\epsilon\|\geq \|\nabla u\|$, together with $(3)$ again, we have $\lim \|\nabla u_\epsilon\|=\|\nabla u\|$. In the end, we also have $\nabla u_\epsilon\to \nabla u$ a.e., because $u_\epsilon\to u$ in $W^{1,p}$ on any compact domain. Now go back to our original question. We will use partition of unity as it be used in the prove of Meyers-Serrin theorem. The prove is on page 285. It is too long I can not type everything here. We use the same partition of unity $\phi_i$ as it used in the book. Briefly, spt$\phi_i\subset\subset \Omega_{i+1}\setminus\Omega_{i-1}$ and $\Omega=\cup \Omega_i$ for $\Omega_i\subset\subset \Omega_{i+1}$. Then by my claim I could choose $\epsilon_i$ for each $i$ such that, for arbitrary $\eta>0$, $$\|(\phi_iu)_{\epsilon_i}-\phi_iu\|_{L^\infty(\Omega)}< \frac{\eta}{2^i} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(4)$$ and $$\left|\|\nabla (\phi_iu)_{\epsilon_i}\|_{L^\infty(\Omega)}- \|\nabla (\phi_iu)\|_{L^\infty(\Omega)}\right|<\frac{\eta}{2^i} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(5)$$ So far I am comfortable and confident with my proof. But the rest I am NOT sure at all. Now we define $v:=\sum_{i=1}^\infty (\phi_iu)_{\epsilon_i}$ and by $u=\sum_{i=1}^\infty (\phi_iu)$ I want to conclude  $$\|v-u\|_{L^\infty(\Omega)}\leq \eta \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(6)$$ and $$\left|\|\nabla v\|_{L^\infty(\Omega)}- \|\nabla u\|_{L^\infty(\Omega)}\right|<\eta\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(7)$$ And here is my CONCERN. 1: I am NOT sure at all about my conclusion of $(6)$ and $(7)$. It should go like this way but I can not prove it formally. After all, $L^\infty$ norm is so different then $L^p$ norm. I never have monotone convergence thm in $L^\infty$ nor LDCT. Thx for @sanjab's answer, but my concern is still remain. More specific, we never have $$\lim_{n\to \infty}\|\chi_{\Omega_n}u\|_{L^\infty(\Omega)} = \|u\|_{L^\infty(\Omega)}$$ That is, I accept @sanjab's answer that $$\|v_l-u_l\|_{L^\infty(\Omega_l)} \leq \sum_{i=1}^l \|(\phi_iu)_{\epsilon_i}-\phi_iu\|_{L^\infty(\Omega)}<  \sum_{i=1}^l \frac{\eta}{2^i} \leq \eta$$ and hence  $$\limsup_{l\to\infty}\|v_l-u_l\|_{L^\infty(\Omega_l)} \leq \eta$$ However, it may happen that  $$\limsup_{l\to\infty}\|v_l-u_l\|_{L^\infty(\Omega_l)}<\|v-u\|_{L^\infty(\Omega)}$$ but not $$\limsup_{l\to\infty}\|v_l-u_l\|_{L^\infty(\Omega_l)}=\|v-u\|_{L^\infty(\Omega)}$$ so we lose the upper bound. I understand this case won't happen if $u$ is continuous. But as we have no control of domain, we can not use embedding theorem to conclude that $u$ is actually Lipschitz. 2: So far I NEVER use the fact the $\Omega$ is finite. So either I prove a stronger version then this exercise, or I make some mistake in my prove. (I think the second assumption will hold :)) Please help! Thx!","This is exercise 10.21 from Leoni's book. The exercise asks me to prove that for any $u\in W^{1,\infty}(\Omega)$ where $\Omega$ is open FINITE, there exists a sequence $(u_n)\subset C^\infty(\Omega)$ such that  $$\|u_n-u\|_{L^\infty(\Omega)}\to 0 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(1)$$ and $$\|\nabla u_n\|_{L^\infty(\Omega)}\to \|\nabla u\|_{L^\infty(\Omega)}\,\,\text{with }\,\,\nabla u_n\to\nabla u \,\,\text{a.e.} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(2)$$ The exercise gives a hint that I should try to modify the prove of Meyers-Serrin theorem that shows every $W^{1,p}$ function can be approximated by $C^\infty$ function in $W^{1,p}$ norm. This is what I did so far: I first make a claim. Assume $U\subset R^n$ is open and $U'\subset\subset U$. Hence, for $\epsilon>0$ small enough we can define $u_\epsilon:=\eta_\epsilon \ast u$ inside $U'$ where $\eta_\epsilon$ is the standard mollifier. Then I claim that there is a subsequence of $u_\epsilon$, still denote as $u_\epsilon$, such that $u_\epsilon$ satisfies $(1)$ and $(2)$ in $U'$ (with $\Omega$ replaced by $U'$). To show $u_\epsilon\to u$ uniformly in $U'$, I use ascoli-arzela theorem. Clearly, from the definition of mollification, we have $$\sup_{\epsilon>0}\|u_\epsilon\|_{L^{\infty}(U')}\leq \|u\|_{L^{\infty}(U)},\,\sup_{\epsilon>0}\|\nabla u_\epsilon\|_{L^{\infty}(U')}\leq \|\nabla u\|_{L^{\infty}(U)} \,\,\,\,\,\,\,\,\,\,(3)$$ Hence, ascoli-arzela theorem states that, up to a subsequence, that $u_\epsilon\to u$ uniformly, i.e., in $L^\infty(U')$ because $U'$ is compact. This gives $(1)$ in $U'$. To show $(2)$, we deduce from $(3)$ that, up to a subsequence,$\nabla u_\epsilon\to \nabla u$ in weak star sense, and hence we have $\liminf \|\nabla u_\epsilon\|\geq \|\nabla u\|$, together with $(3)$ again, we have $\lim \|\nabla u_\epsilon\|=\|\nabla u\|$. In the end, we also have $\nabla u_\epsilon\to \nabla u$ a.e., because $u_\epsilon\to u$ in $W^{1,p}$ on any compact domain. Now go back to our original question. We will use partition of unity as it be used in the prove of Meyers-Serrin theorem. The prove is on page 285. It is too long I can not type everything here. We use the same partition of unity $\phi_i$ as it used in the book. Briefly, spt$\phi_i\subset\subset \Omega_{i+1}\setminus\Omega_{i-1}$ and $\Omega=\cup \Omega_i$ for $\Omega_i\subset\subset \Omega_{i+1}$. Then by my claim I could choose $\epsilon_i$ for each $i$ such that, for arbitrary $\eta>0$, $$\|(\phi_iu)_{\epsilon_i}-\phi_iu\|_{L^\infty(\Omega)}< \frac{\eta}{2^i} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(4)$$ and $$\left|\|\nabla (\phi_iu)_{\epsilon_i}\|_{L^\infty(\Omega)}- \|\nabla (\phi_iu)\|_{L^\infty(\Omega)}\right|<\frac{\eta}{2^i} \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(5)$$ So far I am comfortable and confident with my proof. But the rest I am NOT sure at all. Now we define $v:=\sum_{i=1}^\infty (\phi_iu)_{\epsilon_i}$ and by $u=\sum_{i=1}^\infty (\phi_iu)$ I want to conclude  $$\|v-u\|_{L^\infty(\Omega)}\leq \eta \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(6)$$ and $$\left|\|\nabla v\|_{L^\infty(\Omega)}- \|\nabla u\|_{L^\infty(\Omega)}\right|<\eta\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(7)$$ And here is my CONCERN. 1: I am NOT sure at all about my conclusion of $(6)$ and $(7)$. It should go like this way but I can not prove it formally. After all, $L^\infty$ norm is so different then $L^p$ norm. I never have monotone convergence thm in $L^\infty$ nor LDCT. Thx for @sanjab's answer, but my concern is still remain. More specific, we never have $$\lim_{n\to \infty}\|\chi_{\Omega_n}u\|_{L^\infty(\Omega)} = \|u\|_{L^\infty(\Omega)}$$ That is, I accept @sanjab's answer that $$\|v_l-u_l\|_{L^\infty(\Omega_l)} \leq \sum_{i=1}^l \|(\phi_iu)_{\epsilon_i}-\phi_iu\|_{L^\infty(\Omega)}<  \sum_{i=1}^l \frac{\eta}{2^i} \leq \eta$$ and hence  $$\limsup_{l\to\infty}\|v_l-u_l\|_{L^\infty(\Omega_l)} \leq \eta$$ However, it may happen that  $$\limsup_{l\to\infty}\|v_l-u_l\|_{L^\infty(\Omega_l)}<\|v-u\|_{L^\infty(\Omega)}$$ but not $$\limsup_{l\to\infty}\|v_l-u_l\|_{L^\infty(\Omega_l)}=\|v-u\|_{L^\infty(\Omega)}$$ so we lose the upper bound. I understand this case won't happen if $u$ is continuous. But as we have no control of domain, we can not use embedding theorem to conclude that $u$ is actually Lipschitz. 2: So far I NEVER use the fact the $\Omega$ is finite. So either I prove a stronger version then this exercise, or I make some mistake in my prove. (I think the second assumption will hold :)) Please help! Thx!",,"['real-analysis', 'functional-analysis', 'proof-verification', 'sobolev-spaces']"
15,How to evaluate the integral $e^{-(c\ln(\frac{1}{x}))^s} dx$?,How to evaluate the integral ?,e^{-(c\ln(\frac{1}{x}))^s} dx,"Can anyone help me evaluate $$\int_{\alpha}^1  \exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}} dx$$, Where $0 \leq \alpha \leq 1$ and $s \in \mathbb{R}$. I tried changing variables, integration by parts etc. and got nowhere. Any clues on how to handle this would be appreciated. I tried integration by parts with $dv=1$ and $u = \exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}}$. This gives us: $$x \exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}} {\Huge|}_{\alpha}^1 - s c\int_{\alpha}^1   \left(c\ln\left(\frac{1}{x}\right)\right)^{s-1}\exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}} dx .$$ Note that the second term reminds the incomplete Gamma function (with $ln(1/x)$ instead of $t$), however, i couldn't reach further than this.","Can anyone help me evaluate $$\int_{\alpha}^1  \exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}} dx$$, Where $0 \leq \alpha \leq 1$ and $s \in \mathbb{R}$. I tried changing variables, integration by parts etc. and got nowhere. Any clues on how to handle this would be appreciated. I tried integration by parts with $dv=1$ and $u = \exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}}$. This gives us: $$x \exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}} {\Huge|}_{\alpha}^1 - s c\int_{\alpha}^1   \left(c\ln\left(\frac{1}{x}\right)\right)^{s-1}\exp{\left\{-\left(c\ln\left(\frac{1}{x}\right)\right)^s\right\}} dx .$$ Note that the second term reminds the incomplete Gamma function (with $ln(1/x)$ instead of $t$), however, i couldn't reach further than this.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'approximation']"
16,Monotone Convergence Theorem for Riemann Integrable functions,Monotone Convergence Theorem for Riemann Integrable functions,,"I'm having a really hard time proving this statement (this is not homework): If $f_{n} : [0,1] \rightarrow \mathbb{R}$ is a Riemann integrable function for all $n \in \mathbb{N}$, and $0 \leq f_{n + 1} \leq f_{n}$, and $\lim \limits_{n \rightarrow \infty} f_{n} = 0$, I need to prove that $\lim \limits_{n \rightarrow \infty} \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x = 0$. I'm not allowed to use the Monotone Convergence Theorem for Riemann integrable functions (proving this is actually the first step in proving the MCT). Now, I know $\lim \limits_{n \rightarrow \infty} \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x$ exists because the sequence $\left \{ \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x \right \}_{n =1}^{\infty}$ is a monotonically decreasing sequence that is bounded from below by $0$.  However, I have no idea how to prove the limit is $0$. Also, there is a hint to the problem.  Assume $\lim \limits_{n \rightarrow \infty} \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x = \epsilon > 0$.  I must choose a partition $P_{n}$ for $f_{n}$ such that $P_{n + 1}$ is a refinement partition of $P_{n}$ and show that there exists an element in $[0,1]$ such that $f_{n}$ converges to some strictly positive value on that element, which would lead to a contradiction of the hypothesis of pointwise convergence to $0$. I thought of using a sequence of closed intervals that are nested, because their intersection would be nonempty (since closed subsets of compact spaces are compact), but I can't construct the sequence.  If the hint makes the problem harder, is there an easier way to prove this statement?  Any help would be greatly appreciated.","I'm having a really hard time proving this statement (this is not homework): If $f_{n} : [0,1] \rightarrow \mathbb{R}$ is a Riemann integrable function for all $n \in \mathbb{N}$, and $0 \leq f_{n + 1} \leq f_{n}$, and $\lim \limits_{n \rightarrow \infty} f_{n} = 0$, I need to prove that $\lim \limits_{n \rightarrow \infty} \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x = 0$. I'm not allowed to use the Monotone Convergence Theorem for Riemann integrable functions (proving this is actually the first step in proving the MCT). Now, I know $\lim \limits_{n \rightarrow \infty} \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x$ exists because the sequence $\left \{ \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x \right \}_{n =1}^{\infty}$ is a monotonically decreasing sequence that is bounded from below by $0$.  However, I have no idea how to prove the limit is $0$. Also, there is a hint to the problem.  Assume $\lim \limits_{n \rightarrow \infty} \int \limits_{0}^{1} f_{n}(x) \text{ } \mathrm{d}x = \epsilon > 0$.  I must choose a partition $P_{n}$ for $f_{n}$ such that $P_{n + 1}$ is a refinement partition of $P_{n}$ and show that there exists an element in $[0,1]$ such that $f_{n}$ converges to some strictly positive value on that element, which would lead to a contradiction of the hypothesis of pointwise convergence to $0$. I thought of using a sequence of closed intervals that are nested, because their intersection would be nonempty (since closed subsets of compact spaces are compact), but I can't construct the sequence.  If the hint makes the problem harder, is there an easier way to prove this statement?  Any help would be greatly appreciated.",,"['real-analysis', 'analysis', 'functional-analysis']"
17,Uniform convergence of $\sum\limits_{n=1}^\infty \sin \left(\frac{x}{n^2}\right)$,Uniform convergence of,\sum\limits_{n=1}^\infty \sin \left(\frac{x}{n^2}\right),"Can someone please verify my answers? Consider the series $$\sum\limits_{n=1}^\infty \sin \left(\frac{x}{n^2}\right)$$ Prove that the series converges uniformly on the bounded interval $[-M, M]$ , for any $M > 0$ . Note that for all real numbers $y$ , $$|\sin y| \leq |y|$$ Therefore, for all $M > 0$ , we have $$\left|\sin \left( \frac{x}{n^2}\right)\right| \leq \left|\frac{x}{n^2} \right| \leq \frac{M}{n^2}$$ By the Weierstrass M-test, the series converges uniformly on $[-M, M]$ . To what function does this series converge pointwise? I'm stuck over here.","Can someone please verify my answers? Consider the series Prove that the series converges uniformly on the bounded interval , for any . Note that for all real numbers , Therefore, for all , we have By the Weierstrass M-test, the series converges uniformly on . To what function does this series converge pointwise? I'm stuck over here.","\sum\limits_{n=1}^\infty \sin \left(\frac{x}{n^2}\right) [-M, M] M > 0 y |\sin y| \leq |y| M > 0 \left|\sin \left( \frac{x}{n^2}\right)\right| \leq \left|\frac{x}{n^2} \right| \leq \frac{M}{n^2} [-M, M]","['real-analysis', 'proof-verification', 'uniform-convergence']"
18,Existence of $2^{1/2}$,Existence of,2^{1/2},"I'm trying to prove, that the square root of 2 exists in $\mathbb{R}$. I'm looking at the set $A:=\{y\in\mathbb{R}: y\geq 0, y^2\geq 2\}$. I have already proven that the greatest lower bound $b$ of $A$ exists. However, now I don't know how to proceed. I know that I should consider the cases $b^2 > 2$ and $b^2 < 2$, and bring them to a contradiction, so only the possibility $b^2=2$ is left. I have been given the tip to use the quantities $a:=\frac{1}{2}(1 - \frac{2}{b^2}),\; a^\prime := \frac{1}{2}(1-\frac{b^2}{2}),\; b(1-a),\; \frac{b}{1-a^\prime}$. I believe I should get to the contradiction that $b$ can't be the greatest lower bound in both cases, but I don't see how to get there. Any ideas?","I'm trying to prove, that the square root of 2 exists in $\mathbb{R}$. I'm looking at the set $A:=\{y\in\mathbb{R}: y\geq 0, y^2\geq 2\}$. I have already proven that the greatest lower bound $b$ of $A$ exists. However, now I don't know how to proceed. I know that I should consider the cases $b^2 > 2$ and $b^2 < 2$, and bring them to a contradiction, so only the possibility $b^2=2$ is left. I have been given the tip to use the quantities $a:=\frac{1}{2}(1 - \frac{2}{b^2}),\; a^\prime := \frac{1}{2}(1-\frac{b^2}{2}),\; b(1-a),\; \frac{b}{1-a^\prime}$. I believe I should get to the contradiction that $b$ can't be the greatest lower bound in both cases, but I don't see how to get there. Any ideas?",,['real-analysis']
19,Read Binary and Write Ternary,Read Binary and Write Ternary,,"Working with cantor set I came up with the function $f : [0,1] \longrightarrow \Bbb{R}$ defined as follows: Let $0.a_1a_2a_3\cdots$ be the binary expansion of $x \in [0,1]$ any define $f(x) := 0.a_1a_2a_3\cdots$ (considered in base 3) i.e., $f(x) = \sum_{n=1}^\infty \frac{a_n}{3^n}$. And note that for $f$ to be well-defined we never consider expansions  ending with infinitely many $1$'s. Some interesting questions about $f$ would be as follows: Determination of the set of points which $f$ is differentiable at them. Evaluation of $\int_0^1 f(x)dx$. (Since $f$ is monotone, it is integrable)","Working with cantor set I came up with the function $f : [0,1] \longrightarrow \Bbb{R}$ defined as follows: Let $0.a_1a_2a_3\cdots$ be the binary expansion of $x \in [0,1]$ any define $f(x) := 0.a_1a_2a_3\cdots$ (considered in base 3) i.e., $f(x) = \sum_{n=1}^\infty \frac{a_n}{3^n}$. And note that for $f$ to be well-defined we never consider expansions  ending with infinitely many $1$'s. Some interesting questions about $f$ would be as follows: Determination of the set of points which $f$ is differentiable at them. Evaluation of $\int_0^1 f(x)dx$. (Since $f$ is monotone, it is integrable)",,"['calculus', 'real-analysis']"
20,Euler's proof for the infinitude of the primes,Euler's proof for the infinitude of the primes,,"I am trying to recast the proof of Euler for the infinitude of the primes in modern mathematical language, but am not sure how it is to be done. The statement is that: $$\prod_{p\in P} \frac{1}{1-1/p}=\prod_{p\in P} \sum_{k\geq 0} \frac{1}{p^k}=\sum_n\frac{1}{n}$$ Here $P$ is the set of primes. What bothers me is the second equality above which is obtained by the distributive law, applied not neccessarily finitely many times. Is that justified?","I am trying to recast the proof of Euler for the infinitude of the primes in modern mathematical language, but am not sure how it is to be done. The statement is that: $$\prod_{p\in P} \frac{1}{1-1/p}=\prod_{p\in P} \sum_{k\geq 0} \frac{1}{p^k}=\sum_n\frac{1}{n}$$ Here $P$ is the set of primes. What bothers me is the second equality above which is obtained by the distributive law, applied not neccessarily finitely many times. Is that justified?",,['real-analysis']
21,How I can find this limit?,How I can find this limit?,,"If $a_n=(1+\frac{2}{n})^n$ , then find $$\lim_{n \to \infty}(1-\frac{a_n}{n})^n$$. Trial: Can I use $$\lim_{n \to \infty}a_n=e^2$$ Again $$\lim_{n \to \infty}(1-\frac{a_n}{n})^n=\exp(-e^2)$$ Please help.","If $a_n=(1+\frac{2}{n})^n$ , then find $$\lim_{n \to \infty}(1-\frac{a_n}{n})^n$$. Trial: Can I use $$\lim_{n \to \infty}a_n=e^2$$ Again $$\lim_{n \to \infty}(1-\frac{a_n}{n})^n=\exp(-e^2)$$ Please help.",,"['real-analysis', 'sequences-and-series', 'limits']"
22,Laplace-Beltrami Operator for Euclidean Space,Laplace-Beltrami Operator for Euclidean Space,,"Consider the space $\mathbb{R}^n$ and let $x_1,\ldots, x_n$ be the coordinates. Fix the orientation $dx_1\wedge dx_2\ldots\wedge dx_n$. Let $E^p$ denote the space of smooth $p$ forms and let $d$ denote the exterior derivation map from $E^p$ to $E^{p+1}$. Using the orientation, we get the Hodge star operator $$*:E^p\to E^{n-p}$$ The Laplace-Beltrami operator is defined as $$\Delta:=d\delta+\delta d=(-1)^{n(p+1)+1}d*d* + (-1)^{np+1}*d*d$$ It can be easily checked that for a 0 form (smooth function) $f$, we have  $$\Delta(f)=-\sum_{i=1}^n\frac{\partial^2 f}{\partial x_i^2}$$ It is an exercise in Warner's book (Foundations of Differentiable Manifolds and Lie Groups), last Chapter, exercise 6 to show that  $$\Delta(fdx_I)=\Delta(f)dx_I$$ Does someone know a clean/neat way to do this problem. Thanks in advance.","Consider the space $\mathbb{R}^n$ and let $x_1,\ldots, x_n$ be the coordinates. Fix the orientation $dx_1\wedge dx_2\ldots\wedge dx_n$. Let $E^p$ denote the space of smooth $p$ forms and let $d$ denote the exterior derivation map from $E^p$ to $E^{p+1}$. Using the orientation, we get the Hodge star operator $$*:E^p\to E^{n-p}$$ The Laplace-Beltrami operator is defined as $$\Delta:=d\delta+\delta d=(-1)^{n(p+1)+1}d*d* + (-1)^{np+1}*d*d$$ It can be easily checked that for a 0 form (smooth function) $f$, we have  $$\Delta(f)=-\sum_{i=1}^n\frac{\partial^2 f}{\partial x_i^2}$$ It is an exercise in Warner's book (Foundations of Differentiable Manifolds and Lie Groups), last Chapter, exercise 6 to show that  $$\Delta(fdx_I)=\Delta(f)dx_I$$ Does someone know a clean/neat way to do this problem. Thanks in advance.",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'hodge-theory']"
23,What is the Lebesgue mean of the fat Cantor set?,What is the Lebesgue mean of the fat Cantor set?,,"Everything that follows takes place in the Borel $\sigma$-algebra with Lebesgue measure. The Lebesgue mean of $f$ at $x$ is defined as $\displaystyle \lim_{\epsilon\to 0} \int_{x-\epsilon}^{x+\epsilon} \frac{f}{2\epsilon}$. The function defined by $[f](x) =$ ""Lebesgue mean of $f$ at $x$"" is equal to $f$ almost everywhere so integrals of it will be equal to integrals of $f$, call this property $P$. Starting with the interval $[0,1]$ and removing the middle $1/4$ then the two middle $1/8$-ths then the four middle $1/16$-ths and so on produces the fat Cantor set which has measure $1/2$ but does not contain any interval (Since measures are countably additive this informs us that the set contains uncountably many points). Let $I$ be the indicator function for the fat Cantor set - it is equal to $1$ when applied on a point of the set and $0$ otherwise. Intuitively I would have thought that the Lebesgue mean $[I]$ of the indicator for the fat Cantor set would be the zero function. Since that contradicts $P$, it seems more plausible that $[I] = I$ but I cannot prove this. How can we construct the function $[I]$?","Everything that follows takes place in the Borel $\sigma$-algebra with Lebesgue measure. The Lebesgue mean of $f$ at $x$ is defined as $\displaystyle \lim_{\epsilon\to 0} \int_{x-\epsilon}^{x+\epsilon} \frac{f}{2\epsilon}$. The function defined by $[f](x) =$ ""Lebesgue mean of $f$ at $x$"" is equal to $f$ almost everywhere so integrals of it will be equal to integrals of $f$, call this property $P$. Starting with the interval $[0,1]$ and removing the middle $1/4$ then the two middle $1/8$-ths then the four middle $1/16$-ths and so on produces the fat Cantor set which has measure $1/2$ but does not contain any interval (Since measures are countably additive this informs us that the set contains uncountably many points). Let $I$ be the indicator function for the fat Cantor set - it is equal to $1$ when applied on a point of the set and $0$ otherwise. Intuitively I would have thought that the Lebesgue mean $[I]$ of the indicator for the fat Cantor set would be the zero function. Since that contradicts $P$, it seems more plausible that $[I] = I$ but I cannot prove this. How can we construct the function $[I]$?",,['analysis']
24,A very complete proof on the separability of $L^p$.,A very complete proof on the separability of .,L^p,"I have to prove the following important result. Theorem . Let $(X,\mathcal{A},\mu)$ be a measure space such that: $(1)\;$ the measurable space $(X,\mathcal{A})$ is separable. $(2)\;$ $\mu$ is sigma finite. Then $L^p(X,\mathcal{A},\mu)$ is separable for $p\in [1,\infty)$ . We remember that a measurable space $(X,\mathcal{A})$ is said separable if exists a countable family $\mathcal{C}\subseteq \mathcal{P}(X)$ sich that $\mathcal{\sigma}_0(\mathcal{C})=\mathcal{A}$ , where $\sigma_0$ denotes the generated sigma algebra. We denote with $S(X,\mathcal{A},\mu)$ the set of all simple measurable function on $X$ a complex values. We state two results that we will use. Theorem 1. Let $p\in [1,\infty)$ , then le simple function $S(X,\mathcal{A}, \mu)\cap L^p(X,\mathcal{A},\mu)$ are dense in $L^p$ . Lemma 2. Let $(X,\mathcal{A},\mu)$ be a finite measure space. Let $\mathcal{C}\subseteq\mathcal{P}(X)$ a family such that $\sigma_0(\mathcal{C})=\mathcal{A}$ ; let $\mathcal{A}_0:=\mathcal{A}_0(\mathcal{C})$ the generated algebra of $\mathcal{C}$ . Then for all $F\in \mathcal{A}$ e for all $\varepsilon >0$ exists $G\in\mathcal{A}_0$ such that $$\mu(F\setminus G)+\mu(G\setminus F)<\varepsilon.$$ First case $\Large \mu(X)<\infty$ We introduce the collection $$S_{\mathbb{Q}}(X,\mathcal{A},\mu)=\{s\in S(X,\mathcal{A},\mu)\cap L^p\;:\; s(X)\subseteq\mathbb{Q}+i\mathbb{Q}\}$$ First step $S_{\mathbb{Q}}(X)$ is dense in $S(X)\cap L^p$ Let $\varepsilon > 0 $ and $s\in S\cap L^p$ fixed. Let $$s=\sum_{k=1}^n c_k \chi_{E_k}$$ the standard representation of $s$ . For all $k=1,\dots, n$ let $q_k$ be the complex number such that the real and imaginary parts are such that $$\max_{k=1,\dots, n}{|c_k-q_k|}<\frac{\varepsilon}{[n\mu(X)]^{\frac{1}{p}}}.$$ Then the simple function $$s_{\mathbb{Q}}=\sum_{k=1}^nq_k\chi_{E_k}\in S_{\mathbb{Q}}(X)$$ and results that \begin{eqnarray*} \lVert s-s_{\mathbb{Q}} \rVert_p^p &=& \int_X \left | \sum_{k=1}^n(c_k-q_k)\chi_{E_k}\right |^p\; d\mu \\ &\color{red}{=}& \int_X\sum_{k=1}^n\lvert c_k-q_k \rvert^p\chi_{E_k}\;d\mu \\ &=&\sum_{k=1}^n\lvert c_k-q_k\rvert^p\mu(E_k) \\ &\le& \sum_{k=1}^n\frac{\varepsilon^p}{n}\frac{\mu(E_k)}{\mu(X)}\\ &\le& \sum_{k=1}^n\frac{\varepsilon^p}{n}=\varepsilon^p \end{eqnarray*} The red inequality arises from the fact that the $E_k$ are disjoint and the last inequality follows from the fact that $\frac{\mu(E_k)}{\mu(X)}\le 1$ Now, since the space $(X,\mathcal{A})$ is separable, exists a countable family $\mathcal{C}\subseteq \mathcal{P}(X)$ such that $\sigma_0(\mathcal{C})=\mathcal{A}$ . Evidently the generate algebra $\mathcal{A}_0:=\mathcal{A}_0(\mathcal{C})$ is countable. Now, we introduce the collection $$S_{\mathbb{Q},\mathcal{A}_0}(X,\mathcal{A},\mu)=\left\{s=\sum_{k=1}^n d_k\chi_{G_k}\in S_{\mathbb{Q}}\;|\; G_k\in\mathcal{A}_0, n\in\mathbb{N}\right\}.$$ We observe that this collection is also countable. Second step $S_{\mathbb{Q}, \mathcal{A}_0}(X)$ is dense in $S_{\mathbb{Q}}(X)$ Let $t\in S_{\mathbb{Q}}(X)$ , then $$t=\sum_{k=1}^nc_k\chi_{F_k},$$ where $F_k\in\mathcal{A}$ and $c_k\in \mathbb{Q}+i\mathbb{Q}$ ( $k=1,\dots, n$ ). From the above lemma 2 for all $k=1,\dots, n$ exists $G_k\in\mathcal{A}_0$ such that $$\mu(F_k\setminus G_k)+\mu(G_k\setminus F_k)<\left(\frac{\varepsilon}{nM} \right)^p,$$ where $M:=\max _{k=1,\dots, n}|c_k|$ . Define $$s=\sum_{k=1}^nc_k\chi_{G_k},$$ then $s\in S_{\mathbb{Q},\mathcal{A}_0}$ and results that \begin{eqnarray*} \lVert t-s \rVert_p &=& \left\lVert \sum_{k=1}^n c_k(\chi_{F_k}-\chi_{G_k})\right\rVert_p \\ & \stackrel{Minkowski}{\leq}& \sum_{k=1}^{n} \left\lVert c_k (\chi_{F_k}-\chi_{G_k})\right \rVert_p \\ &=& \sum_{k=1}^{n} \lvert c_k \rvert  \left\lVert \chi_{F_k}-\chi_{G_k}\right\rVert_p \\ &=& \sum_{k=1}^{n} \lvert c_k \rvert \{\mu(F_k\setminus G_k)+\mu(G_k\setminus F_k)\}^{1/p}< nM\left(\frac{\varepsilon}{nM} \right)=\varepsilon. \end{eqnarray*} Now, let $f\in L^p$ , then for Theorem 1. exists $s\in S(X)\cap L^p(X)$ such that $\lVert f-s \rVert_p<\varepsilon$ . For the frist step exists $s_{\mathbb{Q}}\in S_{\mathbb{Q}}(X)$ such that $\lVert s-s_{\mathbb{Q}}\rVert_p<\varepsilon$ , for the step 2 exists $t_{\mathbb{Q},\mathcal{A}_0}\in S_{\mathbb{Q},\mathcal{A}_0}$ such that $$\lVert s_{\mathbb{Q}}-t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<\varepsilon$$ , then $$\lVert f-t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<3\varepsilon$$ This completely proves the theorem in the finite case. Second case $\mu(X)=\infty$ Since $\mu$ is sigma finite exists an increasing sequence $\{E_n\}\subseteq\mathcal{A}$ such that $$X=\bigcup_{n=1}^\infty E_n\quad\text{and}\quad \mu(E_n)<\infty\;\forall n\in\mathbb{N}.$$ Let $s\in S(X)\cap L^p$ , then $$\infty > \int_X \lvert s\rvert^p\;d\mu=\lim_{n\to\infty}\int_{E_n}\lvert s \rvert^p\; d\mu,$$ then $$(\forall \varepsilon>0)\quad (\exists n_0\in\mathbb{N})\quad(\forall n>n_0)\quad \int_{X\setminus E_n} \lvert s \rvert^p\;d\mu < \varepsilon$$ We define $t_n:=s\chi_{E_n}$ , then $\{t_n\}\subseteq S(X)\cap L^p(X)$ . $$\lVert t_n-s \rVert_p=\int_{X\setminus E_n} \lvert s \rvert\; d\mu<\varepsilon$$ for all $n>n_0$ . We choose $$t:=t_{n_0+1},$$ then $$\lVert t-s\rVert_p<\varepsilon.$$ The simple function $t$ is zero outside $Y:=E_{n_0+1}$ , then can be seen defined only on $Y$ , that is $t\in S(Y)\cap L^p$ , observe that $\mu(Y)<\infty$ . Thus we have that: Let $f\in L^p$ be a function, then fro theorem 1 exists $s\in S(X)\cap L^p$ such that $\lVert f - s \rVert_p<\varepsilon$ . For above passage, exists $t\in S(Y)\cap L^p$ where $Y\in\mathcal{A}$ and $\mu(Y)<\infty$ such that $\lvert t - s\rVert_p<\varepsilon$ . For the first step applied to $Y$ exists $s_{\mathbb{Q}}\in S_{\mathbb{Q}}(Y)$ such that $\lVert t -s_{\mathbb{Q}}\rVert_{L^p(Y)}<\varepsilon$ and for the second step exists $t_{\mathbb{Q},\mathcal{A}_0}\in S_{\mathbb{Q},\mathcal{A}_0}(Y)$ such that $\lVert s_{\mathbb{Q}}-t_{\mathbb{Q},\mathcal{A}_0}\rVert_{L^p(Y)}<\varepsilon$ . Defining $s_{\mathbb{Q}}$ and $t_{\mathbb{Q},\mathcal{A}_0}$ ugual to zero in $X\setminus Y$ we have that $$\lVert f - t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<4\varepsilon$$ Question Is this a correct proof?","I have to prove the following important result. Theorem . Let be a measure space such that: the measurable space is separable. is sigma finite. Then is separable for . We remember that a measurable space is said separable if exists a countable family sich that , where denotes the generated sigma algebra. We denote with the set of all simple measurable function on a complex values. We state two results that we will use. Theorem 1. Let , then le simple function are dense in . Lemma 2. Let be a finite measure space. Let a family such that ; let the generated algebra of . Then for all e for all exists such that First case We introduce the collection First step is dense in Let and fixed. Let the standard representation of . For all let be the complex number such that the real and imaginary parts are such that Then the simple function and results that The red inequality arises from the fact that the are disjoint and the last inequality follows from the fact that Now, since the space is separable, exists a countable family such that . Evidently the generate algebra is countable. Now, we introduce the collection We observe that this collection is also countable. Second step is dense in Let , then where and ( ). From the above lemma 2 for all exists such that where . Define then and results that Now, let , then for Theorem 1. exists such that . For the frist step exists such that , for the step 2 exists such that , then This completely proves the theorem in the finite case. Second case Since is sigma finite exists an increasing sequence such that Let , then then We define , then . for all . We choose then The simple function is zero outside , then can be seen defined only on , that is , observe that . Thus we have that: Let be a function, then fro theorem 1 exists such that . For above passage, exists where and such that . For the first step applied to exists such that and for the second step exists such that . Defining and ugual to zero in we have that Question Is this a correct proof?","(X,\mathcal{A},\mu) (1)\; (X,\mathcal{A}) (2)\; \mu L^p(X,\mathcal{A},\mu) p\in [1,\infty) (X,\mathcal{A}) \mathcal{C}\subseteq \mathcal{P}(X) \mathcal{\sigma}_0(\mathcal{C})=\mathcal{A} \sigma_0 S(X,\mathcal{A},\mu) X p\in [1,\infty) S(X,\mathcal{A}, \mu)\cap L^p(X,\mathcal{A},\mu) L^p (X,\mathcal{A},\mu) \mathcal{C}\subseteq\mathcal{P}(X) \sigma_0(\mathcal{C})=\mathcal{A} \mathcal{A}_0:=\mathcal{A}_0(\mathcal{C}) \mathcal{C} F\in \mathcal{A} \varepsilon >0 G\in\mathcal{A}_0 \mu(F\setminus G)+\mu(G\setminus F)<\varepsilon. \Large \mu(X)<\infty S_{\mathbb{Q}}(X,\mathcal{A},\mu)=\{s\in S(X,\mathcal{A},\mu)\cap L^p\;:\; s(X)\subseteq\mathbb{Q}+i\mathbb{Q}\} S_{\mathbb{Q}}(X) S(X)\cap L^p \varepsilon > 0  s\in S\cap L^p s=\sum_{k=1}^n c_k \chi_{E_k} s k=1,\dots, n q_k \max_{k=1,\dots, n}{|c_k-q_k|}<\frac{\varepsilon}{[n\mu(X)]^{\frac{1}{p}}}. s_{\mathbb{Q}}=\sum_{k=1}^nq_k\chi_{E_k}\in S_{\mathbb{Q}}(X) \begin{eqnarray*}
\lVert s-s_{\mathbb{Q}} \rVert_p^p &=& \int_X \left | \sum_{k=1}^n(c_k-q_k)\chi_{E_k}\right |^p\; d\mu \\
&\color{red}{=}& \int_X\sum_{k=1}^n\lvert c_k-q_k \rvert^p\chi_{E_k}\;d\mu \\
&=&\sum_{k=1}^n\lvert c_k-q_k\rvert^p\mu(E_k) \\
&\le& \sum_{k=1}^n\frac{\varepsilon^p}{n}\frac{\mu(E_k)}{\mu(X)}\\
&\le& \sum_{k=1}^n\frac{\varepsilon^p}{n}=\varepsilon^p
\end{eqnarray*} E_k \frac{\mu(E_k)}{\mu(X)}\le 1 (X,\mathcal{A}) \mathcal{C}\subseteq \mathcal{P}(X) \sigma_0(\mathcal{C})=\mathcal{A} \mathcal{A}_0:=\mathcal{A}_0(\mathcal{C}) S_{\mathbb{Q},\mathcal{A}_0}(X,\mathcal{A},\mu)=\left\{s=\sum_{k=1}^n d_k\chi_{G_k}\in S_{\mathbb{Q}}\;|\; G_k\in\mathcal{A}_0, n\in\mathbb{N}\right\}. S_{\mathbb{Q}, \mathcal{A}_0}(X) S_{\mathbb{Q}}(X) t\in S_{\mathbb{Q}}(X) t=\sum_{k=1}^nc_k\chi_{F_k}, F_k\in\mathcal{A} c_k\in \mathbb{Q}+i\mathbb{Q} k=1,\dots, n k=1,\dots, n G_k\in\mathcal{A}_0 \mu(F_k\setminus G_k)+\mu(G_k\setminus F_k)<\left(\frac{\varepsilon}{nM} \right)^p, M:=\max _{k=1,\dots, n}|c_k| s=\sum_{k=1}^nc_k\chi_{G_k}, s\in S_{\mathbb{Q},\mathcal{A}_0} \begin{eqnarray*}
\lVert t-s \rVert_p &=& \left\lVert \sum_{k=1}^n c_k(\chi_{F_k}-\chi_{G_k})\right\rVert_p \\
& \stackrel{Minkowski}{\leq}& \sum_{k=1}^{n} \left\lVert c_k (\chi_{F_k}-\chi_{G_k})\right \rVert_p \\
&=& \sum_{k=1}^{n} \lvert c_k \rvert  \left\lVert \chi_{F_k}-\chi_{G_k}\right\rVert_p \\
&=& \sum_{k=1}^{n} \lvert c_k \rvert \{\mu(F_k\setminus G_k)+\mu(G_k\setminus F_k)\}^{1/p}< nM\left(\frac{\varepsilon}{nM} \right)=\varepsilon.
\end{eqnarray*} f\in L^p s\in S(X)\cap L^p(X) \lVert f-s \rVert_p<\varepsilon s_{\mathbb{Q}}\in S_{\mathbb{Q}}(X) \lVert s-s_{\mathbb{Q}}\rVert_p<\varepsilon t_{\mathbb{Q},\mathcal{A}_0}\in S_{\mathbb{Q},\mathcal{A}_0} \lVert s_{\mathbb{Q}}-t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<\varepsilon \lVert f-t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<3\varepsilon \mu(X)=\infty \mu \{E_n\}\subseteq\mathcal{A} X=\bigcup_{n=1}^\infty E_n\quad\text{and}\quad \mu(E_n)<\infty\;\forall n\in\mathbb{N}. s\in S(X)\cap L^p \infty > \int_X \lvert s\rvert^p\;d\mu=\lim_{n\to\infty}\int_{E_n}\lvert s \rvert^p\; d\mu, (\forall \varepsilon>0)\quad (\exists n_0\in\mathbb{N})\quad(\forall n>n_0)\quad \int_{X\setminus E_n} \lvert s \rvert^p\;d\mu < \varepsilon t_n:=s\chi_{E_n} \{t_n\}\subseteq S(X)\cap L^p(X) \lVert t_n-s \rVert_p=\int_{X\setminus E_n} \lvert s \rvert\; d\mu<\varepsilon n>n_0 t:=t_{n_0+1}, \lVert t-s\rVert_p<\varepsilon. t Y:=E_{n_0+1} Y t\in S(Y)\cap L^p \mu(Y)<\infty f\in L^p s\in S(X)\cap L^p \lVert f - s \rVert_p<\varepsilon t\in S(Y)\cap L^p Y\in\mathcal{A} \mu(Y)<\infty \lvert t - s\rVert_p<\varepsilon Y s_{\mathbb{Q}}\in S_{\mathbb{Q}}(Y) \lVert t -s_{\mathbb{Q}}\rVert_{L^p(Y)}<\varepsilon t_{\mathbb{Q},\mathcal{A}_0}\in S_{\mathbb{Q},\mathcal{A}_0}(Y) \lVert s_{\mathbb{Q}}-t_{\mathbb{Q},\mathcal{A}_0}\rVert_{L^p(Y)}<\varepsilon s_{\mathbb{Q}} t_{\mathbb{Q},\mathcal{A}_0} X\setminus Y \lVert f - t_{\mathbb{Q},\mathcal{A}_0} \rVert_p<4\varepsilon","['real-analysis', 'solution-verification', 'proof-writing', 'proof-explanation']"
25,"Prove that $\int_1^2 g(x^3 - 3x)\,\mathrm dx=\int_0^1 g(x^3 - 3x)\,\mathrm dx$",Prove that,"\int_1^2 g(x^3 - 3x)\,\mathrm dx=\int_0^1 g(x^3 - 3x)\,\mathrm dx","Let $g:[-2, 2]\to\mathbb{R}$ be an even continuous function. Prove that $$\int_1^2g(x^3-3x)\,\mathrm dx=\int_0^1g(x^3-3x)\,\mathrm dx.$$ I found the following solution online, but I'm not sure why $$\int_0^1f(t)\,\mathrm d[x(t)]=\int_0^1 g(x^3-3x)\,\mathrm dx.$$ Also, why does $$\int_0^1g(t)\,\mathrm d[x(t)]=\int_{t=-1}^0 g(t)\,\mathrm d[y(t)]+\int_{t=-1}^0 g(t)\,\mathrm d[z(t)]\,?$$ Is it because $x(t) = -z(t)-y(t)$ and does $$\int_a^b\,\mathrm d(x(t))=-\int_a^b\,\mathrm d[y(t)]-\int_a^b\,\mathrm d[z(t)]\,?$$ For each $t\in [-1,0]$ , the equation $x^3-3x=t$ has three roots $x(t)\in[0,1]$ , $y(t)\in[1,\sqrt3]$ , $z(t)\in [-2,-\sqrt3]$ . By Vieta's Theorem, $x(t)+y(t)+z(t)=0$ $\forall$ $t$ . Also, $x^3-3x$ is differentiable on $\mathbb{R},$ increasing in $(-2,-\sqrt{3}), $ decreasing in $[0,1],$ and increasing in $[1,\sqrt{3}],$ so by the inverse function theorem, $x,y,z$ are all differentiable for $t\in (-1,0).$ We then have \begin{align*} \int_0^1 g(x^3-3x)\,\mathrm dx&=\int_0^{-1}g(t)\,\mathrm d[x(t)]\\ &= \int_{-1}^0 g(t)\,\mathrm d[y(t)]+\int_{-1}^0g(t)\,\mathrm d[z(t)]\\ &=\int_1^{\sqrt3} g(y^3 -3y)\,\mathrm dy+\int_{-2}^{-\sqrt3}g(z^3- 3z)\,\mathrm dz\\ &=\int_1^{\sqrt3} g(y^3-3y)\,\mathrm dy + \int_{\sqrt3}^2g(z^3 - 3z)\,\mathrm dz&\text{(as g is even)}\\ &=\int_1^2 g(x^3-3x)\,\mathrm dx \end{align*}","Let be an even continuous function. Prove that I found the following solution online, but I'm not sure why Also, why does Is it because and does For each , the equation has three roots , , . By Vieta's Theorem, . Also, is differentiable on increasing in decreasing in and increasing in so by the inverse function theorem, are all differentiable for We then have","g:[-2, 2]\to\mathbb{R} \int_1^2g(x^3-3x)\,\mathrm dx=\int_0^1g(x^3-3x)\,\mathrm dx. \int_0^1f(t)\,\mathrm d[x(t)]=\int_0^1 g(x^3-3x)\,\mathrm dx. \int_0^1g(t)\,\mathrm d[x(t)]=\int_{t=-1}^0 g(t)\,\mathrm d[y(t)]+\int_{t=-1}^0 g(t)\,\mathrm d[z(t)]\,? x(t) = -z(t)-y(t) \int_a^b\,\mathrm d(x(t))=-\int_a^b\,\mathrm d[y(t)]-\int_a^b\,\mathrm d[z(t)]\,? t\in [-1,0] x^3-3x=t x(t)\in[0,1] y(t)\in[1,\sqrt3] z(t)\in [-2,-\sqrt3] x(t)+y(t)+z(t)=0 \forall t x^3-3x \mathbb{R}, (-2,-\sqrt{3}),  [0,1], [1,\sqrt{3}], x,y,z t\in (-1,0). \begin{align*}
\int_0^1 g(x^3-3x)\,\mathrm dx&=\int_0^{-1}g(t)\,\mathrm d[x(t)]\\
&= \int_{-1}^0 g(t)\,\mathrm d[y(t)]+\int_{-1}^0g(t)\,\mathrm d[z(t)]\\
&=\int_1^{\sqrt3} g(y^3 -3y)\,\mathrm dy+\int_{-2}^{-\sqrt3}g(z^3- 3z)\,\mathrm dz\\
&=\int_1^{\sqrt3} g(y^3-3y)\,\mathrm dy + \int_{\sqrt3}^2g(z^3 - 3z)\,\mathrm dz&\text{(as g is even)}\\
&=\int_1^2 g(x^3-3x)\,\mathrm dx
\end{align*}","['real-analysis', 'calculus', 'integration', 'derivatives', 'continuity']"
26,Limits of negative-power tower,Limits of negative-power tower,,"Consider the following: $$n \uparrow -\Bigl((n+1) \uparrow -\bigl((n+2) \uparrow \cdots \uparrow -m \bigr)\Bigr)= n^{{{-(n+1)}^{-(n+2)}}^{\cdots^{-m}}}$$ It doesn't converge for $m \to \infty$ , but eventually alternates between two values where the larger one occurs at even numbers of exponents because for $x \to \infty : n^{-x^{-x}} \approx n^0 > n^{-(n+1)^{-x^{-x}}} \approx n^{-1}$ . $\qquad$ For $2^{-3^{-4^{-5^\cdots}}}$ the limits are for example about $0.6903471$ and $0.6583656$ . Is there a closed form for the two limits of the power tower? Are the limits always irrational? Thanks in advance.","Consider the following: It doesn't converge for , but eventually alternates between two values where the larger one occurs at even numbers of exponents because for . For the limits are for example about and . Is there a closed form for the two limits of the power tower? Are the limits always irrational? Thanks in advance.",n \uparrow -\Bigl((n+1) \uparrow -\bigl((n+2) \uparrow \cdots \uparrow -m \bigr)\Bigr)= n^{{{-(n+1)}^{-(n+2)}}^{\cdots^{-m}}} m \to \infty x \to \infty : n^{-x^{-x}} \approx n^0 > n^{-(n+1)^{-x^{-x}}} \approx n^{-1} \qquad 2^{-3^{-4^{-5^\cdots}}} 0.6903471 0.6583656,"['real-analysis', 'algebra-precalculus', 'limits', 'elementary-number-theory', 'power-towers']"
27,Find $ \lim_{r \to 1^{-}} \int_{-\pi}^{\pi} (\frac{1+2r^2}{1-r^2\cos2\theta})^{1/3}d\theta$,Find, \lim_{r \to 1^{-}} \int_{-\pi}^{\pi} (\frac{1+2r^2}{1-r^2\cos2\theta})^{1/3}d\theta,"Evaluate $$ \lim_{r \to 1^{-}} \int_{-\pi}^{\pi} \left[\frac{1+2r^2}{1-r^2\cos\left(2\theta\right)}\right]^{1/3}{\rm d}\theta  $$ Question - Can I take the limit inside the integral? My try- $$I= \lim_{r \to 1^{-}} \int_{-\pi}^{\pi} \left(\frac{1+2r^2}{1-r^2\cos2\theta} \right)^{1/3}\, d\theta  $$ $$ I= \int_{-\pi}^{\pi}   \lim_{r \to 1^{-}}  (\frac{1+2r^2}{1-r^2\cos2\theta})^{1/3}d\theta  $$ $$ I=3^{1/3} \int_{-\pi}^{\pi}  \frac{1}{(1-\cos2\theta)^{1/3}}d\theta  $$ $$ I= 3^{1/3}2\int_{0}^{\pi}    \frac{1}{(1-\cos2\theta)^{1/3}}   d\theta       $$ $$ I= 3^{1/3}4\int_{0}^{\pi/2}    \frac{1}{(1-\cos2\theta)^{1/3}}   d\theta       $$ $$ I= (3/2)^{1/3}4\int_0^{\pi/2}  \sin^{-2/3}\theta  \  d\theta       $$ $$I= 4(3/2)^{1/3} \frac{\Gamma(1/6)\Gamma(1/2) }{\Gamma(2/3)}. $$",Evaluate Question - Can I take the limit inside the integral? My try-," \lim_{r \to 1^{-}} \int_{-\pi}^{\pi} \left[\frac{1+2r^2}{1-r^2\cos\left(2\theta\right)}\right]^{1/3}{\rm d}\theta   I= \lim_{r \to 1^{-}} \int_{-\pi}^{\pi} \left(\frac{1+2r^2}{1-r^2\cos2\theta} \right)^{1/3}\, d\theta    I= \int_{-\pi}^{\pi}   \lim_{r \to 1^{-}}  (\frac{1+2r^2}{1-r^2\cos2\theta})^{1/3}d\theta    I=3^{1/3} \int_{-\pi}^{\pi}  \frac{1}{(1-\cos2\theta)^{1/3}}d\theta    I= 3^{1/3}2\int_{0}^{\pi}    \frac{1}{(1-\cos2\theta)^{1/3}}   d\theta         I= 3^{1/3}4\int_{0}^{\pi/2}    \frac{1}{(1-\cos2\theta)^{1/3}}   d\theta         I= (3/2)^{1/3}4\int_0^{\pi/2} 
\sin^{-2/3}\theta  \  d\theta        I= 4(3/2)^{1/3} \frac{\Gamma(1/6)\Gamma(1/2) }{\Gamma(2/3)}. ","['real-analysis', 'calculus']"
28,JEE Main 2019:finding $\lim_{x \to \infty} \frac{f(x)}{x}$,JEE Main 2019:finding,\lim_{x \to \infty} \frac{f(x)}{x},"let $f$ be a differentiable function such that $$f'(x)=7-\frac{3}{4} \frac{f(x)}{x}\tag 1$$ then $\lim_{x \to \infty} \frac{f(x)}{x}=?$ This came in JEE Main 2019.Although I got the answer $4$ which matches with the answer key,I( think my solution is not complete. Approach :Assuming the limit exists we have by L-Hospitals rule $$\lim_{x \to \infty} \frac{f(x)}{x}=\lim_{x\to \infty}f'(x)=L \tag{say}$$ Then By taking $\lim_{x\to \infty}$ on both sides of equation $(1)$ we have $$L=7-\frac{3}{4}L \implies L=4$$ Question :Although this approach has quickly given the answer the problem is that I have assumed that the Limit exists which need not be the case.I am aware of the method of actually solving the differential equation and then calculating the limit as done here but I was wondering if my method could be actually tweaked  to show that the limit exists or any other method without solving the differential equation.","let be a differentiable function such that then This came in JEE Main 2019.Although I got the answer which matches with the answer key,I( think my solution is not complete. Approach :Assuming the limit exists we have by L-Hospitals rule Then By taking on both sides of equation we have Question :Although this approach has quickly given the answer the problem is that I have assumed that the Limit exists which need not be the case.I am aware of the method of actually solving the differential equation and then calculating the limit as done here but I was wondering if my method could be actually tweaked  to show that the limit exists or any other method without solving the differential equation.",f f'(x)=7-\frac{3}{4} \frac{f(x)}{x}\tag 1 \lim_{x \to \infty} \frac{f(x)}{x}=? 4 \lim_{x \to \infty} \frac{f(x)}{x}=\lim_{x\to \infty}f'(x)=L \tag{say} \lim_{x\to \infty} (1) L=7-\frac{3}{4}L \implies L=4,"['real-analysis', 'calculus', 'limits']"
29,Asymptotic behaviour of integral. How should I proceed?,Asymptotic behaviour of integral. How should I proceed?,,"Let us consider the following SDE: $$dY_t=b(Y_t)dt+\sigma(Y_t)dW_t\tag{1}$$ with $b, \sigma: (l, r)\to\mathbb{R}$ , $−\infty \leq l < r \leq \infty$ bounded functions on compact intervals of $(l, r)$ . In particular, $$b(Y_t)=(u-(u+i)Y_t)$$ $$\sigma(Y_t)=o\sqrt{(Y_t)(1-Y_t)}$$ with $u$ , $i$ and $o$ arbitrary parameters. Hence, focus will be on the following SDE: $$dY_t=(u-(u+i)Y_t)dt+o\sqrt{(Y_t)(1-Y_t)}dW_t\tag{2}$$ I must check whether the process $\{X_t\}$ remains within the interval $(l,r)$ or not for each $0\leq t\leq T$ . To this, I use the Feller test for explosions . Such a test requires that the following two integrals must be defined and computed: $$p(x)=\int_c^x \exp\bigg\{-2\int_c^{\xi}\frac{b(\zeta)}{\sigma^2(\zeta)}d\zeta\bigg\}d\xi\tag{3}$$ $$v(x)=\int_c^x\frac{2(p(x)-p(y))}{p\hspace{0.1cm}'(y)\sigma^2(y)}dy\tag{4}$$ with $c\in(l,r)$ . According to Feller test, probability that the process at least touches the bounds of interval $I$ equals $1$ or is less than $1$ according to whether $v(l+)=v(r-)=\infty$ or not. Let us fix $(l,r)=(0,1)$ and $c=\frac{1}{2}$ . I would like to study the asymptotic behaviour of the integral $(4)$ with $c=\frac{1}{2}$ at bounds $l=0$ and $r=1$ , but I have not any experience with analyses like that. Is there a good standard method or is it just a matter of manipulation? Could you please help me understand how could I study asymptotic behaviour of $(4)$ ?","Let us consider the following SDE: with , bounded functions on compact intervals of . In particular, with , and arbitrary parameters. Hence, focus will be on the following SDE: I must check whether the process remains within the interval or not for each . To this, I use the Feller test for explosions . Such a test requires that the following two integrals must be defined and computed: with . According to Feller test, probability that the process at least touches the bounds of interval equals or is less than according to whether or not. Let us fix and . I would like to study the asymptotic behaviour of the integral with at bounds and , but I have not any experience with analyses like that. Is there a good standard method or is it just a matter of manipulation? Could you please help me understand how could I study asymptotic behaviour of ?","dY_t=b(Y_t)dt+\sigma(Y_t)dW_t\tag{1} b, \sigma: (l, r)\to\mathbb{R} −\infty \leq l < r \leq \infty (l, r) b(Y_t)=(u-(u+i)Y_t) \sigma(Y_t)=o\sqrt{(Y_t)(1-Y_t)} u i o dY_t=(u-(u+i)Y_t)dt+o\sqrt{(Y_t)(1-Y_t)}dW_t\tag{2} \{X_t\} (l,r) 0\leq t\leq T p(x)=\int_c^x \exp\bigg\{-2\int_c^{\xi}\frac{b(\zeta)}{\sigma^2(\zeta)}d\zeta\bigg\}d\xi\tag{3} v(x)=\int_c^x\frac{2(p(x)-p(y))}{p\hspace{0.1cm}'(y)\sigma^2(y)}dy\tag{4} c\in(l,r) I 1 1 v(l+)=v(r-)=\infty (l,r)=(0,1) c=\frac{1}{2} (4) c=\frac{1}{2} l=0 r=1 (4)","['real-analysis', 'integration', 'asymptotics', 'stochastic-calculus', 'stochastic-differential-equations']"
30,Is the multiplication in the ring of functions which are flat at the origin a surjective map?,Is the multiplication in the ring of functions which are flat at the origin a surjective map?,,"Denote by $C^{\infty}_0(\mathbb{R}^n)$ the ring of all smooth functions which are flat at the origin, i.e \begin{align}C^{\infty}_0(\mathbb{R}^n):=\{f\in C^{\infty}(\mathbb{R}^n)|\forall i_1, \dots ,i_n \in \mathbb{N}_0 : \partial_1^{i_1}\dots \partial_n^{i_n}f(0)=0\}  \end{align} In particular we have \begin{align}f\in C^{\infty}_0(\mathbb{R}^n) \ \Rightarrow f(0)=0 \end{align} Let $C^{\infty}_0(\mathbb{R}^n)$ be equipped with the standard ring structure induced from the ring of functions. Is the multiplication \begin{align}C^{\infty}_0(\mathbb{R}^n)\times C^{\infty}_0(\mathbb{R}^n)&\to C^{\infty}_0(\mathbb{R}^n)\\(f,g)\ \ \ \ \ \ \ \ \ \ \ \ &\mapsto \ \ \ f\cdot g \end{align} a surjective map? What are (standard) references in the literature, where this type on non-unital rings are discussed?","Denote by the ring of all smooth functions which are flat at the origin, i.e In particular we have Let be equipped with the standard ring structure induced from the ring of functions. Is the multiplication a surjective map? What are (standard) references in the literature, where this type on non-unital rings are discussed?","C^{\infty}_0(\mathbb{R}^n) \begin{align}C^{\infty}_0(\mathbb{R}^n):=\{f\in C^{\infty}(\mathbb{R}^n)|\forall i_1, \dots ,i_n \in \mathbb{N}_0 : \partial_1^{i_1}\dots \partial_n^{i_n}f(0)=0\} 
\end{align} \begin{align}f\in C^{\infty}_0(\mathbb{R}^n) \ \Rightarrow f(0)=0
\end{align} C^{\infty}_0(\mathbb{R}^n) \begin{align}C^{\infty}_0(\mathbb{R}^n)\times C^{\infty}_0(\mathbb{R}^n)&\to C^{\infty}_0(\mathbb{R}^n)\\(f,g)\ \ \ \ \ \ \ \ \ \ \ \ &\mapsto \ \ \ f\cdot g
\end{align}","['real-analysis', 'abstract-algebra', 'reference-request']"
31,Limit of alternating sum of fractional parts,Limit of alternating sum of fractional parts,,"Find $\newcommand{\pars}[1]{\left\{ \frac{n}{#1} \right\}}$ $$\lim_{n\to\infty}\dfrac{1}{n} \left( \pars{1} - \pars{2} + ... + (-1)^{n+1} \pars{n} \right),$$ where $\left\{ x \right\} $ denotes the fractional part of $x$ . My guess is that the limit is equal to 0; I tried finding some asymptotics for the fractional part sum, by looking for example at ways to bound $\pars{k} - \pars{k+1}$ ; My intuition is that this difference is rather small(perhaps less than $\frac{n}{k(k+1)}$ ) and that it is big enough to be relevant only when one of them is zero, meaning that $k$ or $k+1$ divides $n$ . This would lead me to conjecture that it grows at most as $O(\sqrt{n})$ , which would make the limit zero, but I have not been able to make this rigorous. Another idea I had would be to look at the sum with odd denominator and the sum with even denominators and show that they must be ""rather"" close; this seems pretty intuitive but the fractional part is very chaotic and I have not been able to get any bounds. Any ideas/tips would be appreciated!","Find where denotes the fractional part of . My guess is that the limit is equal to 0; I tried finding some asymptotics for the fractional part sum, by looking for example at ways to bound ; My intuition is that this difference is rather small(perhaps less than ) and that it is big enough to be relevant only when one of them is zero, meaning that or divides . This would lead me to conjecture that it grows at most as , which would make the limit zero, but I have not been able to make this rigorous. Another idea I had would be to look at the sum with odd denominator and the sum with even denominators and show that they must be ""rather"" close; this seems pretty intuitive but the fractional part is very chaotic and I have not been able to get any bounds. Any ideas/tips would be appreciated!","\newcommand{\pars}[1]{\left\{ \frac{n}{#1} \right\}} \lim_{n\to\infty}\dfrac{1}{n} \left( \pars{1} - \pars{2} + ... + (-1)^{n+1} \pars{n} \right), \left\{ x \right\}  x \pars{k} - \pars{k+1} \frac{n}{k(k+1)} k k+1 n O(\sqrt{n})","['real-analysis', 'sequences-and-series', 'limits', 'fractional-part']"
32,How can I prove that $\ln(e^x)=x$ using the Taylor series of $e^x$ and $\ln(1+x)$?,How can I prove that  using the Taylor series of  and ?,\ln(e^x)=x e^x \ln(1+x),I'm stuck to prove that $\ln(e^x)=x$ using the facts that \begin{align} e^x &= \sum_{k=0}^\infty \frac{x^k}{k!} \quad\text{and}  \\ \ln(1+x) &= \sum_{k=1}^\infty \frac{(-1)^{k+1}}{k}x^k. \end{align} I tried as follows: $$\ln(e^x)=\ln\left(\sum_{k=0}^\infty \frac{x^k}{k!}\right)=\ln\left(1+\sum_{k=1}^\infty \frac{x^k}{k!}\right)=\sum_{k=1}^\infty \frac{(-1)^{k+1}}{k}\left(\sum_{n=1}^\infty \frac{x^n}{n!}\right)^k$$ but the last sum looks very complicate to simplify. Any ideas?,I'm stuck to prove that using the facts that I tried as follows: but the last sum looks very complicate to simplify. Any ideas?,"\ln(e^x)=x \begin{align}
e^x &= \sum_{k=0}^\infty \frac{x^k}{k!} \quad\text{and}  \\
\ln(1+x) &= \sum_{k=1}^\infty \frac{(-1)^{k+1}}{k}x^k.
\end{align} \ln(e^x)=\ln\left(\sum_{k=0}^\infty \frac{x^k}{k!}\right)=\ln\left(1+\sum_{k=1}^\infty \frac{x^k}{k!}\right)=\sum_{k=1}^\infty \frac{(-1)^{k+1}}{k}\left(\sum_{n=1}^\infty \frac{x^n}{n!}\right)^k","['real-analysis', 'sequences-and-series']"
33,$2$-dimensional intermediate value theorem,-dimensional intermediate value theorem,2,"23/06/2018 I'm looking for a proof of the following result. I don't know if it is true or not, but it seems to be true. I have tried it from different points of view, but I can not formalize them correctly. My ideas were to apply the intermediate value theorem for each radius or maybe in a topological manner considering the closed sets $Z_2 := \{z\in\mathbb{C}\;|\;g_2(z) = 0\}$ and $Z := \{z\in\mathbb{C}\;|\;g(z) = 0\}$. Let $g_1,g_2:\overline{\mathbb{D}}\to [0,\infty)$ be continuous   functions, where $\mathbb{D}:=\{z\in\mathbb{C}\;|\;|z| < 1\}$, and   define $g:= g_1 - g_2$. Suppose that $g_2(0) = 0$; $g_2(z) > 0$ for all $z\in\mathbb{S}^1$; $g_1(z) > 0$ for all $z\in \overline{\mathbb{D}}$. Then, there exists some simple and closed curve (homeomorphic to   $\mathbb{S}^1$) $\gamma\subset \overline{\mathbb{D}}$ such that $0\in \text{int }\gamma$; $g_2(z) > 0$ for all $z\in \gamma$; $g(z) > 0$ for all $z\in \gamma\cup\text{int }\gamma$. Solved. It is false and a counterexample given by a collegue at the University of Barcelona (Jordi) is the following: $g_1(z) = 1$ for all $z\in \overline{\mathbb{D}}$ and $g_2 := g_2' + g_2''$, where \begin{equation}   g_2'(z) = \begin{cases} 0, & \text{if }|z| \leq b\\ |z| - b,& \text{if }|z| > b \end{cases} \end{equation} and \begin{equation}   g_2''(z) = \begin{cases} 1 - \frac{2}{a}|z - a|, & \text{if }|z - a| \leq \frac{a}{2}\\ 0,& \text{if }|z - a| > \frac{a}{2} \end{cases} \end{equation} for some $0<b<1$ and $a < b/2$. 25/06/2018 I forgot an extra hypothesis, which is that the connected components of $Z_2$ are simply connected, so that this is the new statement: Let $g_1,g_2:\overline{\mathbb{D}}\to [0,\infty)$ be continuous   functions, where $\mathbb{D}:=\{z\in\mathbb{C}\;|\;|z| < 1\}$, and   define $g:= g_1 - g_2$. Assume that the connected components of $Z_2:= \{z\in\mathbb{C}\;|\;g_2(z) = 0\}$ are simply connected and that $g_2(0) = 0$; $g_2(z) > 0$ for all $z\in\mathbb{S}^1$; $g_1(z) > 0$ for all $z\in \overline{\mathbb{D}}$. Then, there exists some simple and closed curve (homeomorphic to   $\mathbb{S}^1$) $\gamma\subset \overline{\mathbb{D}}$ such that $0\in \text{int }\gamma$; $g_2(z) > 0$ for all $z\in \gamma$; $g(z) > 0$ for all $z\in \gamma\cup\text{int }\gamma$.","23/06/2018 I'm looking for a proof of the following result. I don't know if it is true or not, but it seems to be true. I have tried it from different points of view, but I can not formalize them correctly. My ideas were to apply the intermediate value theorem for each radius or maybe in a topological manner considering the closed sets $Z_2 := \{z\in\mathbb{C}\;|\;g_2(z) = 0\}$ and $Z := \{z\in\mathbb{C}\;|\;g(z) = 0\}$. Let $g_1,g_2:\overline{\mathbb{D}}\to [0,\infty)$ be continuous   functions, where $\mathbb{D}:=\{z\in\mathbb{C}\;|\;|z| < 1\}$, and   define $g:= g_1 - g_2$. Suppose that $g_2(0) = 0$; $g_2(z) > 0$ for all $z\in\mathbb{S}^1$; $g_1(z) > 0$ for all $z\in \overline{\mathbb{D}}$. Then, there exists some simple and closed curve (homeomorphic to   $\mathbb{S}^1$) $\gamma\subset \overline{\mathbb{D}}$ such that $0\in \text{int }\gamma$; $g_2(z) > 0$ for all $z\in \gamma$; $g(z) > 0$ for all $z\in \gamma\cup\text{int }\gamma$. Solved. It is false and a counterexample given by a collegue at the University of Barcelona (Jordi) is the following: $g_1(z) = 1$ for all $z\in \overline{\mathbb{D}}$ and $g_2 := g_2' + g_2''$, where \begin{equation}   g_2'(z) = \begin{cases} 0, & \text{if }|z| \leq b\\ |z| - b,& \text{if }|z| > b \end{cases} \end{equation} and \begin{equation}   g_2''(z) = \begin{cases} 1 - \frac{2}{a}|z - a|, & \text{if }|z - a| \leq \frac{a}{2}\\ 0,& \text{if }|z - a| > \frac{a}{2} \end{cases} \end{equation} for some $0<b<1$ and $a < b/2$. 25/06/2018 I forgot an extra hypothesis, which is that the connected components of $Z_2$ are simply connected, so that this is the new statement: Let $g_1,g_2:\overline{\mathbb{D}}\to [0,\infty)$ be continuous   functions, where $\mathbb{D}:=\{z\in\mathbb{C}\;|\;|z| < 1\}$, and   define $g:= g_1 - g_2$. Assume that the connected components of $Z_2:= \{z\in\mathbb{C}\;|\;g_2(z) = 0\}$ are simply connected and that $g_2(0) = 0$; $g_2(z) > 0$ for all $z\in\mathbb{S}^1$; $g_1(z) > 0$ for all $z\in \overline{\mathbb{D}}$. Then, there exists some simple and closed curve (homeomorphic to   $\mathbb{S}^1$) $\gamma\subset \overline{\mathbb{D}}$ such that $0\in \text{int }\gamma$; $g_2(z) > 0$ for all $z\in \gamma$; $g(z) > 0$ for all $z\in \gamma\cup\text{int }\gamma$.",,"['real-analysis', 'general-topology', 'complex-analysis', 'algebraic-topology']"
34,"If $x_n \to 0$ and $\sigma : \mathbb{N} \to \mathbb{N}$ is a bijection, then show that $x_{\sigma (n)} \to 0$","If  and  is a bijection, then show that",x_n \to 0 \sigma : \mathbb{N} \to \mathbb{N} x_{\sigma (n)} \to 0,"Here's the question: Let $(x_n)$ be a sequence. Assume that $x_n \to 0$. Let $\sigma :  \mathbb{N} \to \mathbb{N} $ be a bijection. Define a new sequence $y_n:= x_{\sigma(n)}$. Show that $y_n \to 0$. Here's my attempted proof: Let $\epsilon >0$ be arbitrary. Then there exists $N\in\mathbb{N}$ such that for $n\ge N$, we have $\left| x_n \right|<\epsilon$. Now, define the set $A:=\{n\in \mathbb{N}  : \sigma(n) < N \}$. Clearly, $A$ is finite set (since $\sigma$ is a bijection and there are precisely $N-1$ elements in the set). Let $M=\max A +1$. It is evident by the definition of $(y_n)$ that for $n\ge \sigma(M)$, we have $|y_n|< \epsilon$. Is this proof correct? What are some alternative proofs?","Here's the question: Let $(x_n)$ be a sequence. Assume that $x_n \to 0$. Let $\sigma :  \mathbb{N} \to \mathbb{N} $ be a bijection. Define a new sequence $y_n:= x_{\sigma(n)}$. Show that $y_n \to 0$. Here's my attempted proof: Let $\epsilon >0$ be arbitrary. Then there exists $N\in\mathbb{N}$ such that for $n\ge N$, we have $\left| x_n \right|<\epsilon$. Now, define the set $A:=\{n\in \mathbb{N}  : \sigma(n) < N \}$. Clearly, $A$ is finite set (since $\sigma$ is a bijection and there are precisely $N-1$ elements in the set). Let $M=\max A +1$. It is evident by the definition of $(y_n)$ that for $n\ge \sigma(M)$, we have $|y_n|< \epsilon$. Is this proof correct? What are some alternative proofs?",,"['real-analysis', 'sequences-and-series', 'proof-verification', 'alternative-proof']"
35,$f(n)/g(n) \rightarrow 1$ implies $h(f(n))/h(g(n)) \rightarrow 1$?,implies ?,f(n)/g(n) \rightarrow 1 h(f(n))/h(g(n)) \rightarrow 1,Let $f: \mathbb{N} \rightarrow \mathbb{R}_{>0}$ and $g: \mathbb{N} \rightarrow \mathbb{R}_{>0}$ where $\lim_{n \rightarrow \infty} f(n) = \infty$ and $\lim_{n \rightarrow \infty} g(n) \rightarrow \infty$ and $$\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} = 1.$$  Further let $h: \mathbb{R} \rightarrow \mathbb{R}$ be an increasing function. What further conditions do we need on $h$ in order to conclude $$ \lim_{n \rightarrow \infty}\frac{h(f(n))}{h(g(n))} = 1 \text{ ?} $$,Let $f: \mathbb{N} \rightarrow \mathbb{R}_{>0}$ and $g: \mathbb{N} \rightarrow \mathbb{R}_{>0}$ where $\lim_{n \rightarrow \infty} f(n) = \infty$ and $\lim_{n \rightarrow \infty} g(n) \rightarrow \infty$ and $$\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)} = 1.$$  Further let $h: \mathbb{R} \rightarrow \mathbb{R}$ be an increasing function. What further conditions do we need on $h$ in order to conclude $$ \lim_{n \rightarrow \infty}\frac{h(f(n))}{h(g(n))} = 1 \text{ ?} $$,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'asymptotics']"
36,"About the (non-trivial, this time) zeroes of an almost-periodic function","About the (non-trivial, this time) zeroes of an almost-periodic function",,"This is a follow-up on my previous question that turned out to be almost-trivial. Let $\varphi(t)=\sin(t)+\sin(t\sqrt{2})+\sin(t\sqrt{3})$. Such function is not periodic, but it is bounded, Lipshitz-continuous and with mean zero, i.e. $\lim_{b\to +\infty}\frac{1}{b-a}\int_{a}^{b}\varphi(t)\,dt = 0$. Its real zeroes are simple, hence by denoting as $\zeta_0<\zeta_1<\zeta_2<\zeta_3<\ldots$ the real positive zeroes we have that $$ E = \sup_{n\in\mathbb{N}}\left(\zeta_{n+1}-\zeta_n\right) < +\infty. $$ Now my actual question : Q: How can we improve the previous inequality and show, for instance, that $E\leq 2\pi$? My thoughts: If one is able to produce accurate bounds for $\frac{1}{2\pi i}\oint_{\gamma}\frac{\varphi'(t)}{\varphi(t)}\,dt$, with $\gamma$ being the boundary of a thin rectangle in the complex plane enclosing the real interval $[a,b]$, is also able to estimate the density of real zeroes; If for some non-negative function $\psi(t)$ over the interval $[a,b]$ the integrals $\int_{a}^{\frac{a+b}{2}}\varphi(t)\psi(t)\,dt $ and $\int_{\frac{a+b}{2}}^{b}\varphi(t)\psi(t)\,dt $ have opposite signs, $\varphi(t)$ has a zero in $[a,b]$. But what is an efficient way for constructing such weigth functions $\psi$? Can we exploit the convergents of the continued fractions of $\sqrt{2}$ and/or $\sqrt{3}$? It might by practical to consider the winding number of the curve $\gamma:[0,T]\to \mathbb{C}$ given by $\gamma(t) = e^{it}+e^{it\sqrt{2}}+e^{it\sqrt{3}}$. Addendum: an explicit proof of $E<+\infty$ through Diophantine Approximation. Let $R\subset\mathbb{R}^+$ the set of real numbers such that $r,r\sqrt{2},r\sqrt{3}$ are simultaneously close to integer multiples of $\pi$. For any $r\in R$ we have that $\varphi(r)$ is close to zero: since $\varphi$ is bounded and Lipschitz-continuous, it is enough to show that $R$ is syndetic to have that $E$ is finite. If we consider a cube with side length $\varepsilon>0$ centered at $\frac{m}{\pi}\left(1,\sqrt{2},\sqrt{3}\right)\pmod{1}$ we easily get than for some integer $m\leq\frac{1}{\varepsilon^3}$ the numbers $m,m\sqrt{2},m\sqrt{3}$ are simultaneously at most $\pi\varepsilon$-apart from an integer multiple of $\pi$. By choosing $\varepsilon=\frac{1}{3\pi}$ the ridiculous bound $E\leq 6+27\pi^3$ can be easily derived. The inequality $E\leq 12$ can be deduced from my approach below, however the optimal bound for $E$ seems to be around $4.5$, so there still is some work to be done.","This is a follow-up on my previous question that turned out to be almost-trivial. Let $\varphi(t)=\sin(t)+\sin(t\sqrt{2})+\sin(t\sqrt{3})$. Such function is not periodic, but it is bounded, Lipshitz-continuous and with mean zero, i.e. $\lim_{b\to +\infty}\frac{1}{b-a}\int_{a}^{b}\varphi(t)\,dt = 0$. Its real zeroes are simple, hence by denoting as $\zeta_0<\zeta_1<\zeta_2<\zeta_3<\ldots$ the real positive zeroes we have that $$ E = \sup_{n\in\mathbb{N}}\left(\zeta_{n+1}-\zeta_n\right) < +\infty. $$ Now my actual question : Q: How can we improve the previous inequality and show, for instance, that $E\leq 2\pi$? My thoughts: If one is able to produce accurate bounds for $\frac{1}{2\pi i}\oint_{\gamma}\frac{\varphi'(t)}{\varphi(t)}\,dt$, with $\gamma$ being the boundary of a thin rectangle in the complex plane enclosing the real interval $[a,b]$, is also able to estimate the density of real zeroes; If for some non-negative function $\psi(t)$ over the interval $[a,b]$ the integrals $\int_{a}^{\frac{a+b}{2}}\varphi(t)\psi(t)\,dt $ and $\int_{\frac{a+b}{2}}^{b}\varphi(t)\psi(t)\,dt $ have opposite signs, $\varphi(t)$ has a zero in $[a,b]$. But what is an efficient way for constructing such weigth functions $\psi$? Can we exploit the convergents of the continued fractions of $\sqrt{2}$ and/or $\sqrt{3}$? It might by practical to consider the winding number of the curve $\gamma:[0,T]\to \mathbb{C}$ given by $\gamma(t) = e^{it}+e^{it\sqrt{2}}+e^{it\sqrt{3}}$. Addendum: an explicit proof of $E<+\infty$ through Diophantine Approximation. Let $R\subset\mathbb{R}^+$ the set of real numbers such that $r,r\sqrt{2},r\sqrt{3}$ are simultaneously close to integer multiples of $\pi$. For any $r\in R$ we have that $\varphi(r)$ is close to zero: since $\varphi$ is bounded and Lipschitz-continuous, it is enough to show that $R$ is syndetic to have that $E$ is finite. If we consider a cube with side length $\varepsilon>0$ centered at $\frac{m}{\pi}\left(1,\sqrt{2},\sqrt{3}\right)\pmod{1}$ we easily get than for some integer $m\leq\frac{1}{\varepsilon^3}$ the numbers $m,m\sqrt{2},m\sqrt{3}$ are simultaneously at most $\pi\varepsilon$-apart from an integer multiple of $\pi$. By choosing $\varepsilon=\frac{1}{3\pi}$ the ridiculous bound $E\leq 6+27\pi^3$ can be easily derived. The inequality $E\leq 12$ can be deduced from my approach below, however the optimal bound for $E$ seems to be around $4.5$, so there still is some work to be done.",,"['real-analysis', 'trigonometry', 'inequality', 'integral-inequality']"
37,Setwise convergence with martingales,Setwise convergence with martingales,,"Can someone please tell me whether the following reasoning is correct and/or help me conclude? Let $(\Omega, \mathcal{F}, P)$ be a probability space equipped with a filtration $\{\mathcal{F}_n \}_n \uparrow \mathcal{F}$, and let $\{P_n\}_n$ be a sequence of probability measures that is uniformly absolutely continuous with respect to $P$. Suppose the following condition holds: for all $n$ and $F \in \mathcal{F}_n$, \begin{equation}\tag{1} P_n(F) = P_{n+1}(F). \end{equation} I want to show that $\{P_n\}_n$ converges setwise to a probability measure that is absolutely continuous with respect to $P$. Let $X_n = \frac{dP_n|_{\mathcal{F}_n}}{dP|_{\mathcal{F}_n}}$ be the Radon-Nikodym derivative of $P_n$ with respect to $P$ restricted to $(\Omega, \mathcal{F}_n)$. Then $\{X_n \}_n$ is a nonnegative martingale because each $X_n$ is integrable, $\mathcal{F}_n$-measurable and if $F \in \mathcal{F}_n \subset \mathcal{F}_{n+1}$, then (1) implies $$\int_F X_n dP = P_n(F) = P_{n+1}(F) = \int_F X_{n+1}dP.$$ Therefore, $\{X_n\}_n$ converges almost surely to a nonnegative integrable limit $X_\infty$. So $P_\infty$ defined by $P_\infty(A)=\int_A X_\infty dP$ is a measure. Now let $F \in \cup_n \mathcal{F}_n$. For all sufficiently large $n$, $$P_n(F) = \int_F X_n dP,$$ so $$\lim_{n \to \infty} P_n(F) = \lim_{n \to \infty} \int_F X_n dP = \int_F X_\infty dP = P_\infty(F),$$ where the interchange of limit and integral is justified by the uniform integrability of $\{X_n \}_n$, which is equivalent to the uniform absolute continuity of $\{P_n \}_n$. It's clear from the above that $P_\infty(\Omega)=1$. We have shown that $\{ P_n\}_n$ converges setwise to $P_\infty$ on $\cup_n \mathcal{F}_n$, and I'd like to conclude by asserting that since $\cup_n \mathcal{F}_n$ is a pi-system that generates $\mathcal{F}$, $\{P_n \}_n$ converges setwise to $P_\infty$ on all of $\mathcal{F}$. I think that this follows from the pi-lambda theorem, but it would be great if someone could confirm that.","Can someone please tell me whether the following reasoning is correct and/or help me conclude? Let $(\Omega, \mathcal{F}, P)$ be a probability space equipped with a filtration $\{\mathcal{F}_n \}_n \uparrow \mathcal{F}$, and let $\{P_n\}_n$ be a sequence of probability measures that is uniformly absolutely continuous with respect to $P$. Suppose the following condition holds: for all $n$ and $F \in \mathcal{F}_n$, \begin{equation}\tag{1} P_n(F) = P_{n+1}(F). \end{equation} I want to show that $\{P_n\}_n$ converges setwise to a probability measure that is absolutely continuous with respect to $P$. Let $X_n = \frac{dP_n|_{\mathcal{F}_n}}{dP|_{\mathcal{F}_n}}$ be the Radon-Nikodym derivative of $P_n$ with respect to $P$ restricted to $(\Omega, \mathcal{F}_n)$. Then $\{X_n \}_n$ is a nonnegative martingale because each $X_n$ is integrable, $\mathcal{F}_n$-measurable and if $F \in \mathcal{F}_n \subset \mathcal{F}_{n+1}$, then (1) implies $$\int_F X_n dP = P_n(F) = P_{n+1}(F) = \int_F X_{n+1}dP.$$ Therefore, $\{X_n\}_n$ converges almost surely to a nonnegative integrable limit $X_\infty$. So $P_\infty$ defined by $P_\infty(A)=\int_A X_\infty dP$ is a measure. Now let $F \in \cup_n \mathcal{F}_n$. For all sufficiently large $n$, $$P_n(F) = \int_F X_n dP,$$ so $$\lim_{n \to \infty} P_n(F) = \lim_{n \to \infty} \int_F X_n dP = \int_F X_\infty dP = P_\infty(F),$$ where the interchange of limit and integral is justified by the uniform integrability of $\{X_n \}_n$, which is equivalent to the uniform absolute continuity of $\{P_n \}_n$. It's clear from the above that $P_\infty(\Omega)=1$. We have shown that $\{ P_n\}_n$ converges setwise to $P_\infty$ on $\cup_n \mathcal{F}_n$, and I'd like to conclude by asserting that since $\cup_n \mathcal{F}_n$ is a pi-system that generates $\mathcal{F}$, $\{P_n \}_n$ converges setwise to $P_\infty$ on all of $\mathcal{F}$. I think that this follows from the pi-lambda theorem, but it would be great if someone could confirm that.",,"['real-analysis', 'probability-theory', 'measure-theory', 'martingales']"
38,"Show that $\int_a^b |f(x)|^2 \,\mathrm dx \le \frac{(b-a)^2}{\pi^2}\int_a^b |f'(x)|^2 \,\mathrm dx$",Show that,"\int_a^b |f(x)|^2 \,\mathrm dx \le \frac{(b-a)^2}{\pi^2}\int_a^b |f'(x)|^2 \,\mathrm dx","$\def\d{\mathrm{d}}$Show that if $f \in C^1[a,b]$ and $f(a)=f(b)=0$, then $$\int_a^b |f(x)|^2 \,\d x \le \frac{(b-a)^2}{\pi^2}\int_a^b |f'(x)|^2 \,\d x.$$ By a change of variable, it suffices to assume that $a=0$ and $b=\dfrac{1}{2}$. Extend $f$ to $\left[-\dfrac{1}{2},\dfrac{1}{2}\right]$ by setting $f(-x)=-f(x)$, then extend $f$ to be periodic on $\mathbb{R}$, $f$ thus extended is in $C^1(\mathbb{T})$. $$\hat{f}(k)=\int_{-\frac{1}{2}}^{\frac{1}{2}}f(x)e^{-2ik\pi x} \,\d x=\frac{1}{ik\pi}\int_0^{\frac{1}{2}}f'(x)e^{-2i\pi kx} \,\d x=\frac{1}{2i\pi k}\hat{f'}(k).$$ Now $$\sum_{k\in \mathbb{Z}}|\hat{f}(k)|^2=\sum_{k\in \mathbb{Z}}\frac{1}{4\pi^2k^2}|\hat{f'}(k)|^2\le \frac{1}{4\pi^2}\sum_{k \in \mathbb{Z}}|\hat{f'}(k)|^2.$$ This gives us that $$\int_{0}^{\frac{1}{2}}|f(x)|^2 \,\d x \le \frac{1}{4\pi^2}\int_0^{\frac{1}{2}}|f'(x)|^2 \,\d x.$$ For the general case define $g:\left[0,\dfrac{1}{2}\right] \to \mathbb{R}$ by $g(x)=f(2(b-a)x+a)$. Then by the above we have $$\int_0^{\frac{1}{2}}|g(x)|^2\,\d x \le \frac{1}{4\pi^2}\int_0^{\frac{1}{2}}|g'(x)|^2\,\d x,$$ which gives that $$\int_{0}^{\frac{1}{2}}|f(2(b-a)x+a)|^2\,\d x \le \frac{1}{4\pi^2}\int_0^{\frac{1}{2}}|f'(2(b-a)x+a)|^24(b-a)^2\,\d x.$$ Let $u=2(b-a)x+a$. Then $\d u=2(b-a)\,\d x$ and the integral changes to $$\int_{a}^b|f(u)|^2\,\d u \le \frac{(b-a)^2}{\pi^2}\int_a^b|f'(u)|^2\,\d u.$$","$\def\d{\mathrm{d}}$Show that if $f \in C^1[a,b]$ and $f(a)=f(b)=0$, then $$\int_a^b |f(x)|^2 \,\d x \le \frac{(b-a)^2}{\pi^2}\int_a^b |f'(x)|^2 \,\d x.$$ By a change of variable, it suffices to assume that $a=0$ and $b=\dfrac{1}{2}$. Extend $f$ to $\left[-\dfrac{1}{2},\dfrac{1}{2}\right]$ by setting $f(-x)=-f(x)$, then extend $f$ to be periodic on $\mathbb{R}$, $f$ thus extended is in $C^1(\mathbb{T})$. $$\hat{f}(k)=\int_{-\frac{1}{2}}^{\frac{1}{2}}f(x)e^{-2ik\pi x} \,\d x=\frac{1}{ik\pi}\int_0^{\frac{1}{2}}f'(x)e^{-2i\pi kx} \,\d x=\frac{1}{2i\pi k}\hat{f'}(k).$$ Now $$\sum_{k\in \mathbb{Z}}|\hat{f}(k)|^2=\sum_{k\in \mathbb{Z}}\frac{1}{4\pi^2k^2}|\hat{f'}(k)|^2\le \frac{1}{4\pi^2}\sum_{k \in \mathbb{Z}}|\hat{f'}(k)|^2.$$ This gives us that $$\int_{0}^{\frac{1}{2}}|f(x)|^2 \,\d x \le \frac{1}{4\pi^2}\int_0^{\frac{1}{2}}|f'(x)|^2 \,\d x.$$ For the general case define $g:\left[0,\dfrac{1}{2}\right] \to \mathbb{R}$ by $g(x)=f(2(b-a)x+a)$. Then by the above we have $$\int_0^{\frac{1}{2}}|g(x)|^2\,\d x \le \frac{1}{4\pi^2}\int_0^{\frac{1}{2}}|g'(x)|^2\,\d x,$$ which gives that $$\int_{0}^{\frac{1}{2}}|f(2(b-a)x+a)|^2\,\d x \le \frac{1}{4\pi^2}\int_0^{\frac{1}{2}}|f'(2(b-a)x+a)|^24(b-a)^2\,\d x.$$ Let $u=2(b-a)x+a$. Then $\d u=2(b-a)\,\d x$ and the integral changes to $$\int_{a}^b|f(u)|^2\,\d u \le \frac{(b-a)^2}{\pi^2}\int_a^b|f'(u)|^2\,\d u.$$",,"['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
39,Does the matrix exponential have this convexity property?,Does the matrix exponential have this convexity property?,,"Let $A$ be a real matrix (not necessarily symmetric) whose off-diagonal elements are all non-negative, so that the elements of the matrix exponential $\exp(A)$ are all non-negative. Let $\mathbf x$ be a vector whose elements are all non-negative. Let $\sum(\cdot)$ denote the sum of all the terms in a vector. I hypothesise that the function $f(t) = \log \,\sum\big(\exp(At)\, \mathbf x\big)$ is a convex function of $t$. Note that '$\log$' in this formula is an ordinary logarithm while '$\exp$' is a matrix exponential. From playing around with this I haven't found an easy way to show it, but I haven't found an obvious counterexample either. So my question is, is $f(t) = \log\, \sum\big(\exp(At)\, \mathbf x\big)$ a convex function of $t$, if the entries of $A$ and $\mathbf x$ are non-negative? For the avoidance of doubt, and for ease of searching for counterexamples, here is some Python code that calculates $f(t)$. ( A and x should be numpy arrays of the appropriate dimensions.) import numpy as np import scipy.linalg as spl  def f(t, A, x):     return np.log(np.sum(spl.expm(A*t).dot(x))) Notes Originally I was using the convention that ""convex"" means a downward curve, i.e. the second derivative is never positive. However, Jonas Meyer points out in a comment that there are examples where the second derivative is consistently positive. So now I guess my question is, can the second derivative of $f(t)$ change sign as a function of $t$? I'm mostly only interested in part of the function where $t>0$, though in examples it seems to work for $t<0$ as well, until the sum of the elements becomes negative. It might be necessary to make additional assumptions about $A$ and $\mathbf x$, e.g. that $\exp(A)$ is irreducible, or that $\mathbf x$ has all positive elements. If that's the case I'd like to know. The motivation has to do with growing populations in biology. It's equivalent to asking whether the per capita rate of population growth is always non-increasing in a simple model of a growing (or shrinking) population. This question is obviously closely related but not the same, and my hypothesis doesn't seem to immediately follow from that result. The result will be true if the elements of $\exp(At)$ are log-convex, which would be a generally useful result to know about if it's true. It turns out not to be the case that all the elements of $\exp(At)$ are either consistently all convex upward or convex downward; they can change from one to the other as a function of $t$. See my related MathOverflow question . This makes me skeptical that the hypothesis in this (Math.SE) question is true. Here is an example of what it tends to look like. For this example, $$ A = \begin{pmatrix}1,1\\4,1\end{pmatrix}, \qquad \mathbf{x} = \begin{pmatrix}1\\0\end{pmatrix}. $$","Let $A$ be a real matrix (not necessarily symmetric) whose off-diagonal elements are all non-negative, so that the elements of the matrix exponential $\exp(A)$ are all non-negative. Let $\mathbf x$ be a vector whose elements are all non-negative. Let $\sum(\cdot)$ denote the sum of all the terms in a vector. I hypothesise that the function $f(t) = \log \,\sum\big(\exp(At)\, \mathbf x\big)$ is a convex function of $t$. Note that '$\log$' in this formula is an ordinary logarithm while '$\exp$' is a matrix exponential. From playing around with this I haven't found an easy way to show it, but I haven't found an obvious counterexample either. So my question is, is $f(t) = \log\, \sum\big(\exp(At)\, \mathbf x\big)$ a convex function of $t$, if the entries of $A$ and $\mathbf x$ are non-negative? For the avoidance of doubt, and for ease of searching for counterexamples, here is some Python code that calculates $f(t)$. ( A and x should be numpy arrays of the appropriate dimensions.) import numpy as np import scipy.linalg as spl  def f(t, A, x):     return np.log(np.sum(spl.expm(A*t).dot(x))) Notes Originally I was using the convention that ""convex"" means a downward curve, i.e. the second derivative is never positive. However, Jonas Meyer points out in a comment that there are examples where the second derivative is consistently positive. So now I guess my question is, can the second derivative of $f(t)$ change sign as a function of $t$? I'm mostly only interested in part of the function where $t>0$, though in examples it seems to work for $t<0$ as well, until the sum of the elements becomes negative. It might be necessary to make additional assumptions about $A$ and $\mathbf x$, e.g. that $\exp(A)$ is irreducible, or that $\mathbf x$ has all positive elements. If that's the case I'd like to know. The motivation has to do with growing populations in biology. It's equivalent to asking whether the per capita rate of population growth is always non-increasing in a simple model of a growing (or shrinking) population. This question is obviously closely related but not the same, and my hypothesis doesn't seem to immediately follow from that result. The result will be true if the elements of $\exp(At)$ are log-convex, which would be a generally useful result to know about if it's true. It turns out not to be the case that all the elements of $\exp(At)$ are either consistently all convex upward or convex downward; they can change from one to the other as a function of $t$. See my related MathOverflow question . This makes me skeptical that the hypothesis in this (Math.SE) question is true. Here is an example of what it tends to look like. For this example, $$ A = \begin{pmatrix}1,1\\4,1\end{pmatrix}, \qquad \mathbf{x} = \begin{pmatrix}1\\0\end{pmatrix}. $$",,"['real-analysis', 'linear-algebra', 'biology']"
40,This function must be open if these points are isolated,This function must be open if these points are isolated,,"Let $U\subset \mathbb R^m$ be an open and $f:U\to \mathbb R^m$ be a function of class $C^1$. If the points where the determinant of the Jacobian Matrix of $f$ is zero are isolated ones and $m>1$, how do I prove this function must be open? I don't know even how to begin, I need some hints.","Let $U\subset \mathbb R^m$ be an open and $f:U\to \mathbb R^m$ be a function of class $C^1$. If the points where the determinant of the Jacobian Matrix of $f$ is zero are isolated ones and $m>1$, how do I prove this function must be open? I don't know even how to begin, I need some hints.",,"['real-analysis', 'general-topology', 'differential-topology']"
41,Why is $a^{x+y}=a^xa^y$,Why is,a^{x+y}=a^xa^y,"After reading chapter 1 of Rudin's Principles of mathematical analysis, I started working on the exercises. Exercise 6 was a big surprise since exercises 1-5 had all been very simple, whereas 6 had me stumped for a ver long time until I read solutions online. The problem is: For integers $m,n,p,q$ let $r=m/n=p/q$, prove that for positive $b$ $$(b^m)^{1/n}=(b^p)^{1/q}$$ Then show that for rationals $r,s$ $$b^{r+s}=b^rb^s$$ If $$b^x=\sup \{ b^t : t\le x,t\in \mathbb{Q}\}$$ Show that for reals $x,y$ $$b^{x+y}=b^xb^y$$ After reading the solutions I think I could have solved the first and second parts of the question, but for the last part, the solution made use of several propositions, corollaries and had to prove a lemma before starting the demonstration. This made me a little suspicious, since exercise 7 asked for the proof of several of these facts. So I was wondering if there are alternate proofs, and if there are, what is the simplest you know? This is the link to the solutions I am talking about","After reading chapter 1 of Rudin's Principles of mathematical analysis, I started working on the exercises. Exercise 6 was a big surprise since exercises 1-5 had all been very simple, whereas 6 had me stumped for a ver long time until I read solutions online. The problem is: For integers $m,n,p,q$ let $r=m/n=p/q$, prove that for positive $b$ $$(b^m)^{1/n}=(b^p)^{1/q}$$ Then show that for rationals $r,s$ $$b^{r+s}=b^rb^s$$ If $$b^x=\sup \{ b^t : t\le x,t\in \mathbb{Q}\}$$ Show that for reals $x,y$ $$b^{x+y}=b^xb^y$$ After reading the solutions I think I could have solved the first and second parts of the question, but for the last part, the solution made use of several propositions, corollaries and had to prove a lemma before starting the demonstration. This made me a little suspicious, since exercise 7 asked for the proof of several of these facts. So I was wondering if there are alternate proofs, and if there are, what is the simplest you know? This is the link to the solutions I am talking about",,"['real-analysis', 'proof-writing']"
42,"If $\sum_{n=1}^\infty\vert a_n\sin(nx)\vert$ converges, then $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$.","If  converges, then .",\sum_{n=1}^\infty\vert a_n\sin(nx)\vert \sum_{n=1}^{\infty}\vert a_n\vert<\infty,"Suppose $\sum_{n=1}^\infty\vert a_n\sin(nx)\vert$ converges for all $x$ in a set of positive measure $A$. I'm trying to prove $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$. The only useful result I can recall of periodic functions is the that if $f\in L^1(\mathbb{R})$ and $g$ is continuous on $\mathbb{R}$ with period $T$ then  $\lim_{n\to\infty}\int_{\mathbb{R}}f(x)g(nx)dx=\bigg(\int_{\mathbb{R}}f(x)dx \bigg)\bigg(\frac{1}{T} \int_0^Tg(x)dx\bigg)$. I don't claim this is the best way to go. But if we let $g(x)=\sin x$, then $T=2\pi$. I am not sure what I should define $f$ as since it is required to be in $L^1(\mathbb{R})$, but if I had a decent $f$ my idea is to write $A\subset\bigcup_{k=N}^M[k,k+1]$ then show $\int_{l}^{l+1}\sum\vert a_n \vert<\infty$ for any $l\in [N,M-1]$. Another potential problem is that with my choice of $g$, $1/T\int_0^Tg=1/2\pi\int_0^{2\pi}\sin=0$. Am I anywhere near showing $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$?","Suppose $\sum_{n=1}^\infty\vert a_n\sin(nx)\vert$ converges for all $x$ in a set of positive measure $A$. I'm trying to prove $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$. The only useful result I can recall of periodic functions is the that if $f\in L^1(\mathbb{R})$ and $g$ is continuous on $\mathbb{R}$ with period $T$ then  $\lim_{n\to\infty}\int_{\mathbb{R}}f(x)g(nx)dx=\bigg(\int_{\mathbb{R}}f(x)dx \bigg)\bigg(\frac{1}{T} \int_0^Tg(x)dx\bigg)$. I don't claim this is the best way to go. But if we let $g(x)=\sin x$, then $T=2\pi$. I am not sure what I should define $f$ as since it is required to be in $L^1(\mathbb{R})$, but if I had a decent $f$ my idea is to write $A\subset\bigcup_{k=N}^M[k,k+1]$ then show $\int_{l}^{l+1}\sum\vert a_n \vert<\infty$ for any $l\in [N,M-1]$. Another potential problem is that with my choice of $g$, $1/T\int_0^Tg=1/2\pi\int_0^{2\pi}\sin=0$. Am I anywhere near showing $\sum_{n=1}^{\infty}\vert a_n\vert<\infty$?",,"['real-analysis', 'analysis']"
43,Theorems or results one can use to prove properties of integrands,Theorems or results one can use to prove properties of integrands,,"Consider a function $f:[a,b]\times \mathbb R \rightarrow \mathbb R$, $(t,x)\mapsto f(t,x)$ for which we do not know much about its regularity in $x$. Define now $$(t,x) \mapsto g(t,x):= \int_a^t f(s,x)ds, \quad t\in [a,b], \quad x\in \mathbb{R}.$$ Imagine we now know much more about $g$, for instance, $g(t,\cdot)$ is $C^1$ for all $t\in [a,b]$ or whatever property we know about $g$ on $x\in \mathbb R$. My question is: what results or theorems transfer such knowledge on $g$ to $f$? That is, under what conditions can $f$ inherit the same properties of $g$? Thank you very much!","Consider a function $f:[a,b]\times \mathbb R \rightarrow \mathbb R$, $(t,x)\mapsto f(t,x)$ for which we do not know much about its regularity in $x$. Define now $$(t,x) \mapsto g(t,x):= \int_a^t f(s,x)ds, \quad t\in [a,b], \quad x\in \mathbb{R}.$$ Imagine we now know much more about $g$, for instance, $g(t,\cdot)$ is $C^1$ for all $t\in [a,b]$ or whatever property we know about $g$ on $x\in \mathbb R$. My question is: what results or theorems transfer such knowledge on $g$ to $f$? That is, under what conditions can $f$ inherit the same properties of $g$? Thank you very much!",,"['calculus', 'real-analysis', 'integration', 'measure-theory']"
44,How to prove that superadditive function has this property? [closed],How to prove that superadditive function has this property? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $f: [0, \infty) \to \mathbb R$ be a function satisfying the following conditions: For any $x,y \ge 0$ , $f(x+y) \ge f(x) + f(y)$ . For any $x \in [0,2]$ , $f(x) \ge x^2 - x$ . Prove that, for any positive integer $M$ and positive reals $n_1,\dots,n_M$ with $n_1+\dots+n_M = M$ , we have $$f(n_1)+\dots+f(n_M) \ge 0$$ How can I prove this statement?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let be a function satisfying the following conditions: For any , . For any , . Prove that, for any positive integer and positive reals with , we have How can I prove this statement?","f: [0, \infty) \to \mathbb R x,y \ge 0 f(x+y) \ge f(x) + f(y) x \in [0,2] f(x) \ge x^2 - x M n_1,\dots,n_M n_1+\dots+n_M = M f(n_1)+\dots+f(n_M) \ge 0","['real-analysis', 'functional-equations']"
45,Significance and applications of the Riesz Representation Theorem in locally compact Hausdorff spaces,Significance and applications of the Riesz Representation Theorem in locally compact Hausdorff spaces,,"Can anyone tell me the signification of Theorem $2.14$ (The Riesz Representation Theorem in locally compact Hausdorff spaces), page $40, 41$ in Rudin - Real and Complex Analysis? And some applications of that theorem? Thanks in advance.","Can anyone tell me the signification of Theorem $2.14$ (The Riesz Representation Theorem in locally compact Hausdorff spaces), page $40, 41$ in Rudin - Real and Complex Analysis? And some applications of that theorem? Thanks in advance.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure', 'riesz-representation-theorem']"
46,Uniqueness solutions of $dx/dt = f^2(x) + e^{-t}$.,Uniqueness solutions of .,dx/dt = f^2(x) + e^{-t},"Someone can help me in the following problem? Is a question of Zhang . Let $f(x)$ be continuous for $x \in \mathbb{R}$, show that $dx/dt = f^2(x) + e^{-t}$ has the property of uniqueness of solution. First, I'm not sure that $f^2(x)$ referred composition or power. I tried both ways and I failed. My idea was repeated the proof of Cauchy-Picard Theorem showing that $$ F(x(t)) = x(0) + \int_{0}^{t}\left[ f^2(x(t)) - e^{-t} \right] dt$$ is a contraction. But I end up getting to the point where it seems I have to use that $f^2(x)$ satisfy a Lipschitz condition. But that does not seem true both in the case of the case of a composition as in case of the square power. For example, consider $f(x) = \sqrt{|x|}$, we have $f^2(x)$, seen as composition is $\sqrt[4]{|x|}$, which don't satisfy a Lipschitz condition. And consider $f(x) = \sqrt[4]{|x|}$, we have $f^2(x)$, seen as square power, is $\sqrt{|x|}$, which don't satisfy a Lipschitz condition. Some progress: We have  $$x'(t) = f^2(x) + e^{-t} \Leftrightarrow x'(t) - e^{-t} = f^2(x)  \Leftrightarrow  (x(t) + e^{-t})'= f^2(x) $$ By the variable change $y(t) = x(t) + e^{-t}$, we have $$y'(t) = f^2(y(t) - e^{-t}) = g^2(y(t))$$ See that the last step is valid both to composition case as in the power case. So, we have reduced the problem for the uniqueness of the problem $$ y' = g^2(y), $$ where $g$ is merely a continuous function. I do not know about you, but I think that this is not true. I think that $g(y) = \sqrt[4]{y}$, for $y\geq 0$ e $g(y)$, for $y<0$, is a counterexample in the power case.","Someone can help me in the following problem? Is a question of Zhang . Let $f(x)$ be continuous for $x \in \mathbb{R}$, show that $dx/dt = f^2(x) + e^{-t}$ has the property of uniqueness of solution. First, I'm not sure that $f^2(x)$ referred composition or power. I tried both ways and I failed. My idea was repeated the proof of Cauchy-Picard Theorem showing that $$ F(x(t)) = x(0) + \int_{0}^{t}\left[ f^2(x(t)) - e^{-t} \right] dt$$ is a contraction. But I end up getting to the point where it seems I have to use that $f^2(x)$ satisfy a Lipschitz condition. But that does not seem true both in the case of the case of a composition as in case of the square power. For example, consider $f(x) = \sqrt{|x|}$, we have $f^2(x)$, seen as composition is $\sqrt[4]{|x|}$, which don't satisfy a Lipschitz condition. And consider $f(x) = \sqrt[4]{|x|}$, we have $f^2(x)$, seen as square power, is $\sqrt{|x|}$, which don't satisfy a Lipschitz condition. Some progress: We have  $$x'(t) = f^2(x) + e^{-t} \Leftrightarrow x'(t) - e^{-t} = f^2(x)  \Leftrightarrow  (x(t) + e^{-t})'= f^2(x) $$ By the variable change $y(t) = x(t) + e^{-t}$, we have $$y'(t) = f^2(y(t) - e^{-t}) = g^2(y(t))$$ See that the last step is valid both to composition case as in the power case. So, we have reduced the problem for the uniqueness of the problem $$ y' = g^2(y), $$ where $g$ is merely a continuous function. I do not know about you, but I think that this is not true. I think that $g(y) = \sqrt[4]{y}$, for $y\geq 0$ e $g(y)$, for $y<0$, is a counterexample in the power case.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
47,Solving Riemann-Stieltjes integral:$\int_{- \pi/4}^{\pi/4} f(x)dg(x)$,Solving Riemann-Stieltjes integral:,\int_{- \pi/4}^{\pi/4} f(x)dg(x),"I'm having trouble solving this Riemann-Stieltjes integral: $$\int_{- \pi/4}^{\pi/4} f(x)dg(x),$$ where $$f(x):= \begin{cases} \frac{\sin^4x}{\cos^2x}{} &\text{if }x\ge0,  \\{}\\ \frac1{\cos^3x} &\text{if }x<0,\end{cases}$$ and $$g(x)=\begin{cases} \phantom{-} 1+\sin(x) &\text{if }-\pi/4 <x<\pi/4,  \\ -1 &\text{otherwise}.\end{cases}$$ I believe the only jump discontinuities are at $-\pi/4$ and $\pi/4$ . Which $g=-1$ at both of those points. I'm struggling with the rest. What formula should I be using to compute the integral and what should my answer look like? Thanks for any help!",I'm having trouble solving this Riemann-Stieltjes integral: where and I believe the only jump discontinuities are at and . Which at both of those points. I'm struggling with the rest. What formula should I be using to compute the integral and what should my answer look like? Thanks for any help!,"\int_{- \pi/4}^{\pi/4} f(x)dg(x), f(x):= \begin{cases} \frac{\sin^4x}{\cos^2x}{} &\text{if }x\ge0,  \\{}\\ \frac1{\cos^3x} &\text{if }x<0,\end{cases} g(x)=\begin{cases} \phantom{-} 1+\sin(x) &\text{if }-\pi/4 <x<\pi/4,  \\ -1 &\text{otherwise}.\end{cases} -\pi/4 \pi/4 g=-1","['real-analysis', 'calculus', 'integration', 'trigonometry', 'stieltjes-integral']"
48,Riemann-Stieltjes integrability criterion,Riemann-Stieltjes integrability criterion,,"I am currently reading through chapter 11 of Rudin's Principles of Mathematical Analysis, and I'm trying to solve problem 7: Find a necessary and sufficient condition that $f \in \mathfrak R(\alpha)$ on $[a,b]$. [the class of Rienmann-Stieltjes integrable functions, with integrator $\alpha$] Hint: Consider Example 11.6(b) and Theorem 11.33 . In example 11.6(b) Rudin defines an additive set function on intervals according to a monotonically increasing function $\alpha$: $$\mu((a,b))=\alpha(b^- )-\alpha(a^+)\\    \mu((a,b])=\alpha(b^+)-\alpha(a^+)\\    \mu([a,b))=\alpha(b^-)-\alpha(a^-) \\    \mu([a,b])=\alpha(b^+)-\alpha(a^-) $$ and states that it can be extended to a countably-additive set function over a richer sigma-algebra. In Theorem 11.33 he proves that any Rieamann integrable function is Lebesgue integrable (dm) and that their values coincide. He also proves the criterion for Riemann integrability - continuity almost everywhere (dm). The method of the proof relies on Darboux sums. I'd greatly appreciate any help. EDIT: This is my attempt at the solution. Please tell me what do you think about it. First of all we extend $\alpha:[a,b] \to \mathbb R$ to a monotonically increasing function over $\mathbb R$, as follows: for $x<a$, $\alpha(x):=\alpha(a)$ and for $x>b$, $\alpha(x):=\alpha(b)$. We now can use example 11.6(b) to get a measure $\mu_\alpha$ over a sigma-algebra of subsets of $\mathbb R$, and we will immediately restrict it back to the interval $[a,b]$. I will try to prove that $f \in \mathfrak R(\alpha) \Leftrightarrow$ $f$ is continuous a.e. ($d \mu_\alpha$) Suppose $f$ is bounded. By definition 6.1 and theorem 6.4 there is a sequence $\{P_k \}$, of partitions of $[a,b]$, such that $P_{k+1}$ is a refinement of $P_k$, such that the distance between adjacent points of $P_k$ is less than $\frac{1}{k}$, and such that: $$\lim_{k \to \infty} L(P_k,f,\alpha)=\underline{\int}_a^b f d\alpha, \, \lim_{k \to \infty} U(P_k,f,\alpha)= \overline{\int}_a^b f d\alpha $$ Furthermore, since $\alpha$ is monotonic, it has only countably many points of discontinuity, so we can take the points of the partitions to be points where $\alpha$ is continuous. If $P_k=\{x_0,x_1,\ldots,x_n\}$ with $x_0=a,x_n=b$, define the functions $L_k,U_k:[a,b] \to \mathbb R$ according to: $$L_k(x)=m_1,U_k(x)=M_1$$ for $a \leq x \leq x_1$ and $$L_k(x)=m_i,U_k(x)=M_i $$ for $x_{i-1} < x \leq x_i,2 \leq i \leq n$. Notice that: $$\int_{[a,b]} L_k d\mu_\alpha=L(P_k,f,\alpha), \, \int_{[a,b]} U_k d\mu_\alpha=U(P_k,f,\alpha)$$ due to the continuity of $\alpha$ at the points of the partitions $\{ P_k \}$. We also have $$L_1(x) \leq L_2(x) \leq \ldots \leq f(x) \leq \ldots \leq U_2(x) \leq U_1(x) $$ for all $x \in [a,b]$, since $P_{k+1}$ refines $P_k$. We therefore have the existence of $$L(x):=\lim_{k \to \infty} L_k(x),U(x):= \lim_{k \to \infty} U_k(x) $$ and $L,U$ are bounded and measurable on $[a,b]$, $L(x) \leq f(x) \leq U(x)$ and $$\int_{[a,b]} L d\mu_\alpha=\underline{\int}_a^b f d\alpha, \, \int_{[a,b]} U d\mu_\alpha=\overline{\int}_a^b f d\alpha $$ thanks to Lebesgue's monotone convergence theorem. So far, nothing has been assumed on $f$, except that $f$ is a bounded real function on $[a,b]$. Note that $f \in \mathfrak R(\alpha)$ if and only if its lower and upper integrals coincide, hence if and only if $$\int_{[a,b]} L d\mu_\alpha=\int_{[a,b]} U d\mu_\alpha $$. Since $L \leq U$, this happens if and only if $L(x)=U(x)$ a.e. ($d\mu_\alpha$). In that case $f$ equals to a measurable function a.e. and therefore is measurable itself [because $\mu_\alpha$ is complete?]. Furthermore, if $x$ belongs to no $P_k$, it is quite easy to see that $L(x)=U(x)$ if and only $f$ is continuous at $x$. Since $\cup_k P_k$ is a countable set of continuity points of $\alpha$ it has $\mu_\alpha$-measure zero. So that $L=U$ a.e. if and only if $f$ is continuous a.e. QED","I am currently reading through chapter 11 of Rudin's Principles of Mathematical Analysis, and I'm trying to solve problem 7: Find a necessary and sufficient condition that $f \in \mathfrak R(\alpha)$ on $[a,b]$. [the class of Rienmann-Stieltjes integrable functions, with integrator $\alpha$] Hint: Consider Example 11.6(b) and Theorem 11.33 . In example 11.6(b) Rudin defines an additive set function on intervals according to a monotonically increasing function $\alpha$: $$\mu((a,b))=\alpha(b^- )-\alpha(a^+)\\    \mu((a,b])=\alpha(b^+)-\alpha(a^+)\\    \mu([a,b))=\alpha(b^-)-\alpha(a^-) \\    \mu([a,b])=\alpha(b^+)-\alpha(a^-) $$ and states that it can be extended to a countably-additive set function over a richer sigma-algebra. In Theorem 11.33 he proves that any Rieamann integrable function is Lebesgue integrable (dm) and that their values coincide. He also proves the criterion for Riemann integrability - continuity almost everywhere (dm). The method of the proof relies on Darboux sums. I'd greatly appreciate any help. EDIT: This is my attempt at the solution. Please tell me what do you think about it. First of all we extend $\alpha:[a,b] \to \mathbb R$ to a monotonically increasing function over $\mathbb R$, as follows: for $x<a$, $\alpha(x):=\alpha(a)$ and for $x>b$, $\alpha(x):=\alpha(b)$. We now can use example 11.6(b) to get a measure $\mu_\alpha$ over a sigma-algebra of subsets of $\mathbb R$, and we will immediately restrict it back to the interval $[a,b]$. I will try to prove that $f \in \mathfrak R(\alpha) \Leftrightarrow$ $f$ is continuous a.e. ($d \mu_\alpha$) Suppose $f$ is bounded. By definition 6.1 and theorem 6.4 there is a sequence $\{P_k \}$, of partitions of $[a,b]$, such that $P_{k+1}$ is a refinement of $P_k$, such that the distance between adjacent points of $P_k$ is less than $\frac{1}{k}$, and such that: $$\lim_{k \to \infty} L(P_k,f,\alpha)=\underline{\int}_a^b f d\alpha, \, \lim_{k \to \infty} U(P_k,f,\alpha)= \overline{\int}_a^b f d\alpha $$ Furthermore, since $\alpha$ is monotonic, it has only countably many points of discontinuity, so we can take the points of the partitions to be points where $\alpha$ is continuous. If $P_k=\{x_0,x_1,\ldots,x_n\}$ with $x_0=a,x_n=b$, define the functions $L_k,U_k:[a,b] \to \mathbb R$ according to: $$L_k(x)=m_1,U_k(x)=M_1$$ for $a \leq x \leq x_1$ and $$L_k(x)=m_i,U_k(x)=M_i $$ for $x_{i-1} < x \leq x_i,2 \leq i \leq n$. Notice that: $$\int_{[a,b]} L_k d\mu_\alpha=L(P_k,f,\alpha), \, \int_{[a,b]} U_k d\mu_\alpha=U(P_k,f,\alpha)$$ due to the continuity of $\alpha$ at the points of the partitions $\{ P_k \}$. We also have $$L_1(x) \leq L_2(x) \leq \ldots \leq f(x) \leq \ldots \leq U_2(x) \leq U_1(x) $$ for all $x \in [a,b]$, since $P_{k+1}$ refines $P_k$. We therefore have the existence of $$L(x):=\lim_{k \to \infty} L_k(x),U(x):= \lim_{k \to \infty} U_k(x) $$ and $L,U$ are bounded and measurable on $[a,b]$, $L(x) \leq f(x) \leq U(x)$ and $$\int_{[a,b]} L d\mu_\alpha=\underline{\int}_a^b f d\alpha, \, \int_{[a,b]} U d\mu_\alpha=\overline{\int}_a^b f d\alpha $$ thanks to Lebesgue's monotone convergence theorem. So far, nothing has been assumed on $f$, except that $f$ is a bounded real function on $[a,b]$. Note that $f \in \mathfrak R(\alpha)$ if and only if its lower and upper integrals coincide, hence if and only if $$\int_{[a,b]} L d\mu_\alpha=\int_{[a,b]} U d\mu_\alpha $$. Since $L \leq U$, this happens if and only if $L(x)=U(x)$ a.e. ($d\mu_\alpha$). In that case $f$ equals to a measurable function a.e. and therefore is measurable itself [because $\mu_\alpha$ is complete?]. Furthermore, if $x$ belongs to no $P_k$, it is quite easy to see that $L(x)=U(x)$ if and only $f$ is continuous at $x$. Since $\cup_k P_k$ is a countable set of continuity points of $\alpha$ it has $\mu_\alpha$-measure zero. So that $L=U$ a.e. if and only if $f$ is continuous a.e. QED",,"['real-analysis', 'analysis', 'measure-theory', 'integration', 'lebesgue-integral']"
49,An idea for this difficult integral,An idea for this difficult integral,,"I am being stuck in caculating this integral: $$J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}dx$$ I tried to change to another variable: $x = - t$ then $dx = - dt$ , also I got: $$J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos(-t)}{\sqrt{1-t^2}(1+e^t)}dt=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}dx.$$ Therefore $$2J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\left[\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}+\dfrac{\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}\right]dx=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{e^x\arccos x+\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}dx$$ The numerator looks very complex and I really do not know how to do next. Can you guys give me some ideas ?","I am being stuck in caculating this integral: I tried to change to another variable: then , also I got: Therefore The numerator looks very complex and I really do not know how to do next. Can you guys give me some ideas ?",J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}dx x = - t dx = - dt J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos(-t)}{\sqrt{1-t^2}(1+e^t)}dt=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}dx. 2J=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\left[\dfrac{\arccos x}{\sqrt{1-x^2}(1+e^{-x})}+\dfrac{\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}\right]dx=\int_{-\tfrac{1}{2}}^{\tfrac{1}{2}}\dfrac{e^x\arccos x+\arccos(-x)}{\sqrt{1-x^2}(1+e^x)}dx,"['real-analysis', 'calculus', 'integration', 'inverse-function', 'trigonometric-integrals']"
50,Bound a function with parameter involving logarithm,Bound a function with parameter involving logarithm,,"In my master project I encounter the following function $$f_\varepsilon(x) = \ln\left(\frac{x^{1 + a} + \varepsilon^\beta}{\lambda(x)^2 + \varepsilon^2}\right)$$ for $a$ close to zero, $\beta \in (1, 2)$ and $$\lambda(x) = \frac{x}{|\ln(x)|^2}.$$ I aim to bound the following quantity $$g_\varepsilon(x) = \bigg|\frac{1}{f_{\varepsilon}(x)} - \frac{1}{f_{0}(x)}\bigg|$$ on a certain interval $[0, x_0]$ , $x_0 \ll 1$ , where $f_0$ is just $f_\varepsilon$ with the parameter $\epsilon = 0$ . Clearly, for $x \to 0$ , we get $$g_\varepsilon (0) = \frac{1}{(2 - \beta)|\ln \varepsilon|}$$ so I feel that we should get something like $$g_\varepsilon(x) \le \frac{C}{(2 - \beta)|\ln \varepsilon|}$$ for $C > 0$ independent of $\varepsilon$ . I tried to bound $|\ln(\varepsilon)|g_\varepsilon(x)$ by such a constant but I couldn't prove rigourously that it didn't depend on $\varepsilon$ . Any help would be greatly appreciated.","In my master project I encounter the following function for close to zero, and I aim to bound the following quantity on a certain interval , , where is just with the parameter . Clearly, for , we get so I feel that we should get something like for independent of . I tried to bound by such a constant but I couldn't prove rigourously that it didn't depend on . Any help would be greatly appreciated.","f_\varepsilon(x) = \ln\left(\frac{x^{1 + a} + \varepsilon^\beta}{\lambda(x)^2 + \varepsilon^2}\right) a \beta \in (1, 2) \lambda(x) = \frac{x}{|\ln(x)|^2}. g_\varepsilon(x) = \bigg|\frac{1}{f_{\varepsilon}(x)} - \frac{1}{f_{0}(x)}\bigg| [0, x_0] x_0 \ll 1 f_0 f_\varepsilon \epsilon = 0 x \to 0 g_\varepsilon (0) = \frac{1}{(2 - \beta)|\ln \varepsilon|} g_\varepsilon(x) \le \frac{C}{(2 - \beta)|\ln \varepsilon|} C > 0 \varepsilon |\ln(\varepsilon)|g_\varepsilon(x) \varepsilon","['real-analysis', 'calculus', 'inequality', 'logarithms', 'upper-lower-bounds']"
51,Covering number/Metric Entropy of the unit ball with respect to Mahalanobis distance,Covering number/Metric Entropy of the unit ball with respect to Mahalanobis distance,,"Let $B$ denote the unit ball on $\mathbb{R}^d$ and $N(\epsilon, B, d)$ be the cardinality of the smallest $\epsilon$ -cover of $B$ . An epsilon cover is a set $T \subset B$ such that for any $x \in B$ , there is a $t \in T$ with $d(t,x) \le \epsilon$ . See for example here . $N$ is referred to as the covering number, and $\log N$ is the Metric entropy. Consider the following result: let $\|\cdot\|$ be a norm on $\mathbb{R}^d$ then $$ \frac{1}{\epsilon^d} \le N(\epsilon, B, \|\cdot\|) \le \left (1+\frac{2}{\epsilon} \right)^d. $$ I would like to know if there are bounds on the covering numbers that are dimension free when we choose the metric to be the Mahalanobis distance $d_S(x,y) = \|S^{-1/2}(x-y)\|_2$ for some positive definite covariance matrix $S$ . Are there results along the lines of: $$ \frac{1}{\epsilon^{f(S)}} \le N(\epsilon, B, d_S) \le \left (1+\frac{2}{\epsilon} \right)^{f(S)}. $$ where $f(S)$ is some quantity depending on $S$ ? An example I have in mind is when $S$ is diagonal with quickly decaying diagonal elements, e.g. $S_{ii} = i^{-2}$ .","Let denote the unit ball on and be the cardinality of the smallest -cover of . An epsilon cover is a set such that for any , there is a with . See for example here . is referred to as the covering number, and is the Metric entropy. Consider the following result: let be a norm on then I would like to know if there are bounds on the covering numbers that are dimension free when we choose the metric to be the Mahalanobis distance for some positive definite covariance matrix . Are there results along the lines of: where is some quantity depending on ? An example I have in mind is when is diagonal with quickly decaying diagonal elements, e.g. .","B \mathbb{R}^d N(\epsilon, B, d) \epsilon B T \subset B x \in B t \in T d(t,x) \le \epsilon N \log N \|\cdot\| \mathbb{R}^d 
\frac{1}{\epsilon^d} \le N(\epsilon, B, \|\cdot\|) \le \left (1+\frac{2}{\epsilon} \right)^d.
 d_S(x,y) = \|S^{-1/2}(x-y)\|_2 S 
\frac{1}{\epsilon^{f(S)}} \le N(\epsilon, B, d_S) \le \left (1+\frac{2}{\epsilon} \right)^{f(S)}.
 f(S) S S S_{ii} = i^{-2}","['real-analysis', 'combinatorics', 'geometry', 'analysis', 'statistics']"
52,Tight bounds for $L_1$ norm of Hermite polynomial: $\int_{-\infty}^\infty |\operatorname{He}_n(x)| \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2} ) dx$,Tight bounds for  norm of Hermite polynomial:,L_1 \int_{-\infty}^\infty |\operatorname{He}_n(x)| \frac{1}{\sqrt{2 \pi}} \exp(-\frac{x^2}{2} ) dx,"Is there a closed-form expression (or tight upper bound) for the integral $$C^{(1)}_n =\int_{-\infty}^\infty |\operatorname{He}_n(x)|  \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{x^2}{2} \right) \mathrm dx$$ Here $\operatorname{He}_n$ is a Hermite polynomial (probabilists' version). Motivation: we know that \begin{align} C_n^{(2)}=\sqrt{\int_{-\infty}^\infty |\operatorname{He}_n(  x)|^2 \frac{1}{\sqrt{2 \pi}}\exp\left(-\frac{x^2}{2} \right) \mathrm dx }=  \sqrt{ n!} \end{align} I am curious in how different is $C^{(1)}_n$ from $C^{(2)}_n$ . Using Jensen's inequality, it is not difficult to show that \begin{align} \frac{C_n^{(1)}}{C_n^{(2)}} \le 1 \end{align} I am curious if there is a more refined bounds, for example, the one that would exactly characterize what \begin{align} \lim_{n \to \infty} \frac{C_n^{(1)}}{C_n^{(2)}} =??  \end{align}","Is there a closed-form expression (or tight upper bound) for the integral Here is a Hermite polynomial (probabilists' version). Motivation: we know that I am curious in how different is from . Using Jensen's inequality, it is not difficult to show that I am curious if there is a more refined bounds, for example, the one that would exactly characterize what","C^{(1)}_n =\int_{-\infty}^\infty |\operatorname{He}_n(x)|  \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{x^2}{2} \right) \mathrm dx \operatorname{He}_n \begin{align}
C_n^{(2)}=\sqrt{\int_{-\infty}^\infty |\operatorname{He}_n(  x)|^2 \frac{1}{\sqrt{2 \pi}}\exp\left(-\frac{x^2}{2} \right) \mathrm dx }=  \sqrt{ n!}
\end{align} C^{(1)}_n C^{(2)}_n \begin{align}
\frac{C_n^{(1)}}{C_n^{(2)}} \le 1
\end{align} \begin{align}
\lim_{n \to \infty} \frac{C_n^{(1)}}{C_n^{(2)}} =?? 
\end{align}","['real-analysis', 'normal-distribution', 'hermite-polynomials']"
53,When does coordinate-wise differentiability imply $\ell^{p}$ differentiability?,When does coordinate-wise differentiability imply  differentiability?,\ell^{p},"For $p \in [1,\infty]$ , consider the sequence space $\ell^{p}(\mathbb{N})$ consisting of real sequences $x = (x_{i})_{i =1}^{\infty}$ with norm $\lVert x \rVert_{p} = \left(\sum_{i=1}^{\infty}|x_{i}|^{p} \right)^{1/p}$ for $p < \infty$ and $\lVert x \rVert_{\infty} = \sup_{i}|x_{i}|$ . Let $A$ be a bounded linear operator on $\ell^{p}(\mathbb{N})$ for some $p$ . Let $u: [0,\infty) \to \ell^{p}(\mathbb{N})$ , and write $u(t) = (u_{1}(t), u_{2}(t),\dots)$ . Suppose that each coordinate function $u_{i}: [0,\infty) \to \mathbb{R}$ is differentiable on $(0, \infty)$ , right-differentiable at $0$ , and satisfies $$u_{i}'(t) = (Au(t))_{i}$$ for all $t \geq 0$ . My question: is it the case that $$\lim_{h \to 0}\left \lVert \frac{u(t+h) - u(t)}{h} - Au(t) \right \rVert_{p} = 0$$ for all $t$ ? If not, is there a reasonable counterexample?","For , consider the sequence space consisting of real sequences with norm for and . Let be a bounded linear operator on for some . Let , and write . Suppose that each coordinate function is differentiable on , right-differentiable at , and satisfies for all . My question: is it the case that for all ? If not, is there a reasonable counterexample?","p \in [1,\infty] \ell^{p}(\mathbb{N}) x = (x_{i})_{i =1}^{\infty} \lVert x \rVert_{p} = \left(\sum_{i=1}^{\infty}|x_{i}|^{p} \right)^{1/p} p < \infty \lVert x \rVert_{\infty} = \sup_{i}|x_{i}| A \ell^{p}(\mathbb{N}) p u: [0,\infty) \to \ell^{p}(\mathbb{N}) u(t) = (u_{1}(t), u_{2}(t),\dots) u_{i}: [0,\infty) \to \mathbb{R} (0, \infty) 0 u_{i}'(t) = (Au(t))_{i} t \geq 0 \lim_{h \to 0}\left \lVert \frac{u(t+h) - u(t)}{h} - Au(t) \right \rVert_{p} = 0 t","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'lp-spaces']"
54,"What is known about the 'Double log Eulers constant', $\lim_{n \to \infty}{\sum_{k=2}^n\frac{1}{k\ln{k}}-\ln\ln{n}}$?","What is known about the 'Double log Eulers constant', ?",\lim_{n \to \infty}{\sum_{k=2}^n\frac{1}{k\ln{k}}-\ln\ln{n}},"The Euler constant is defined as $$\gamma = \lim_{n \to \infty}{\sum_{k=1}^n\frac{1}{k}-\ln{n}}$$ Let $$q = \lim_{n \to \infty}{\sum_{k=2}^n\frac{1}{k\ln{k}}-\ln\ln{n}}$$ I managed to prove that $$\frac{1}{3\ln{3}}+\frac{1}{2\ln{2}}-\ln\ln{3} \geq q \geq \frac{1}{2\ln{2}}-\ln\ln{3}$$ Is there something known about the constant $q$? For instance, is $q$ expressible in terms of $\gamma$?","The Euler constant is defined as $$\gamma = \lim_{n \to \infty}{\sum_{k=1}^n\frac{1}{k}-\ln{n}}$$ Let $$q = \lim_{n \to \infty}{\sum_{k=2}^n\frac{1}{k\ln{k}}-\ln\ln{n}}$$ I managed to prove that $$\frac{1}{3\ln{3}}+\frac{1}{2\ln{2}}-\ln\ln{3} \geq q \geq \frac{1}{2\ln{2}}-\ln\ln{3}$$ Is there something known about the constant $q$? For instance, is $q$ expressible in terms of $\gamma$?",,"['limits', 'asymptotics', 'analytic-number-theory', 'constants', 'euler-mascheroni-constant']"
55,Prove that $f(x) = x^2 + x$ is uniformly continuous.,Prove that  is uniformly continuous.,f(x) = x^2 + x,"Using the definition of uniform continuity, prove that $f(x) = x^2 + x$ is uniformly continuous on $(0,1)$ . Proof: We have a function $f:(0,1)\to\mathbb R$ . Let $\epsilon > 0$ . Note $\forall x,y\in\mathbb (0,1)$ that $|x+y| \le |x| + |y| < 2$ . Set $\delta = \frac \epsilon 3$ . Then for $x,y\in(0,1)$ , if $|x-y|<\delta$ , implies \begin{split} |x^2+x-(y^2+y)|&\le|x^2-y^2|+|x-y| \\  &= |x-y||x+y| + |x-y|\\ &=|x-y|(|x+y|+1)\\ &< |x-y|(2+1)\\ &= 3|x-y| \\ &<\epsilon. \end{split} Is this an ok proof?","Using the definition of uniform continuity, prove that is uniformly continuous on . Proof: We have a function . Let . Note that . Set . Then for , if , implies Is this an ok proof?","f(x) = x^2 + x (0,1) f:(0,1)\to\mathbb R \epsilon > 0 \forall x,y\in\mathbb (0,1) |x+y| \le |x| + |y| < 2 \delta = \frac \epsilon 3 x,y\in(0,1) |x-y|<\delta \begin{split}
|x^2+x-(y^2+y)|&\le|x^2-y^2|+|x-y| \\ 
&= |x-y||x+y| + |x-y|\\
&=|x-y|(|x+y|+1)\\
&< |x-y|(2+1)\\
&= 3|x-y| \\
&<\epsilon.
\end{split}","['real-analysis', 'solution-verification']"
56,Estimate product of Hölder functions,Estimate product of Hölder functions,,"Let $U \subseteq \mathbb{R}^n$ be the unit ball, let $k \in \mathbb{Z}_{\ge 0}$ , and let $0 < \alpha < 1$ . Let $f, g \in C^{k, \alpha}(U)$ be Hölder functions, i.e. $f$ and $g$ are of class $C^k$ , $$ |f|_{k, \alpha} = \sum_{|I| \le k} \sup_{x \in U} |D^If(x)| + \sum_{|I| = k} \sup_{x \neq y \in U}\frac{|D^If(x) - D^If(y)|}{|x - y|^\alpha} < \infty $$ and $|g|_{k, \alpha} < \infty$ . Question. Is there a way to estimate $|fg|_{k,\alpha}$ in terms of some Hölder norms of $f$ and $g$ ? For instance, is it true that there exists $C > 0$ such that $$ |fg|_{k, \alpha} \le C(|f|_{k, \alpha} |g|_k + |f|_k |g|_{k, \alpha}) $$ or something of that sort? I read in a book that the above inequality is true when $k = 0$ and I'm wondering what is the right generalization.","Let be the unit ball, let , and let . Let be Hölder functions, i.e. and are of class , and . Question. Is there a way to estimate in terms of some Hölder norms of and ? For instance, is it true that there exists such that or something of that sort? I read in a book that the above inequality is true when and I'm wondering what is the right generalization.","U \subseteq \mathbb{R}^n k \in \mathbb{Z}_{\ge 0} 0 < \alpha < 1 f, g \in C^{k, \alpha}(U) f g C^k 
|f|_{k, \alpha} = \sum_{|I| \le k} \sup_{x \in U} |D^If(x)| + \sum_{|I| = k} \sup_{x \neq y \in U}\frac{|D^If(x) - D^If(y)|}{|x - y|^\alpha} < \infty
 |g|_{k, \alpha} < \infty |fg|_{k,\alpha} f g C > 0 
|fg|_{k, \alpha} \le C(|f|_{k, \alpha} |g|_k + |f|_k |g|_{k, \alpha})
 k = 0","['real-analysis', 'functional-analysis', 'holder-spaces']"
57,Is a function of several variables convex near a local minimum when the derivatives are non-degenerate?,Is a function of several variables convex near a local minimum when the derivatives are non-degenerate?,,"Let $U \subseteq \mathbb R^n$ be an open subset, and let $f:U \to \mathbb R$ be smooth. Suppose that $x \in U$ is a strict local minimum point of $f$ . Let $df^k(x):(\mathbb R^n)^k \to \mathbb R$ be its $k$ ""derivative"", i.e. the symmetric multilinear map defined by setting $df^k(x)(e_{i_1},\dots,e_{i_k})=\partial_{i_1} \dots \partial_{i_k}f(x)$ . Assume that $df^j(x) \neq 0$ for some natural $j$ . Let $k$ be the minimal such that $df^k(x) \neq 0$ . Since $x$ is a local minimum, $k$ must be even. Suppose now that $df^k(x)$ is non-degenerate , i.e. $df^k(x)(h,\dots,h) \neq 0$ for any non-zero $h \in \mathbb R^n$ . (Since $x$ is a minimum, I think this is equivalent to $df^k(x)$ being positive-definite, i.e. $df^k(x)(h,\dots,h) > 0$ for any non-zero $h \in \mathbb R^n$ ). Question: Is $f$ is strictly convex in some neighbourhood of $x$ ? In the one-dimensional case, when $f$ is a map $\mathbb R \to \mathbb R$ , the answer is positive: We have $f^k(x)>0$ , and the Taylor expansion of $f''$ near $x$ is $$ f''(y) = {1 \over (k-2)!} f^{(k)}(x)(y - x)^{k-2} + O((y - x)^{k-1}). $$ Thus, $f''(y)>0$ for $y \ne x$ sufficiently close to $x$ , so $f$ is strictly convex around $x$ . Returning back to the high-dimensional case, if $k>2$ , we have $\text{Hess}f(x)=df^2(x)=0$ , and I guess that we should somehow prove that $\text{Hess}f(y)$ becomes positive-definite for $y$ sufficiently close to $x$ . Perhaps we need to understand the Taylor's expansion of $\text{Hess}f$ around $x$ , similarly to the one-dimensional case, but I am not sure how to do that. Is there a nice way? Edit: It is certainly not enough to assume that $df^k(x)$ is non-zero. Indeed, consider $ f(x,y) = x^2 y^2 + x^8 + y^8$ . (I thank Robert Israel for this nice example). $f$ has a strict global minimum at $(0,0)$ . $$\det(\text{Hess}f(x,y))=3136 x^6 y^6 + 112 x^8 + 112 y^8 - 12 x^2 y^2,$$ which is negative when $x=y$ is small and nonzero. Thus, $f$ is not convex at a neighbourhood of zero. Note that $\text{Hess}f(0,0)=0$ ; The first non-zero derivative at $(0,0)$ is the fourth-order derivative $df^4(0)$ . It is not non-degenerate, however, since if $h=h^1e_1+h^2e_2$ then $df^4(0)(h,h,h,h)=4(h^1)^2(h^2)^2$ vanishes when either $h_i$ is zero. So, non-vanishing of some derivatives does not ensure convexity.","Let be an open subset, and let be smooth. Suppose that is a strict local minimum point of . Let be its ""derivative"", i.e. the symmetric multilinear map defined by setting . Assume that for some natural . Let be the minimal such that . Since is a local minimum, must be even. Suppose now that is non-degenerate , i.e. for any non-zero . (Since is a minimum, I think this is equivalent to being positive-definite, i.e. for any non-zero ). Question: Is is strictly convex in some neighbourhood of ? In the one-dimensional case, when is a map , the answer is positive: We have , and the Taylor expansion of near is Thus, for sufficiently close to , so is strictly convex around . Returning back to the high-dimensional case, if , we have , and I guess that we should somehow prove that becomes positive-definite for sufficiently close to . Perhaps we need to understand the Taylor's expansion of around , similarly to the one-dimensional case, but I am not sure how to do that. Is there a nice way? Edit: It is certainly not enough to assume that is non-zero. Indeed, consider . (I thank Robert Israel for this nice example). has a strict global minimum at . which is negative when is small and nonzero. Thus, is not convex at a neighbourhood of zero. Note that ; The first non-zero derivative at is the fourth-order derivative . It is not non-degenerate, however, since if then vanishes when either is zero. So, non-vanishing of some derivatives does not ensure convexity.","U \subseteq \mathbb R^n f:U \to \mathbb R x \in U f df^k(x):(\mathbb R^n)^k \to \mathbb R k df^k(x)(e_{i_1},\dots,e_{i_k})=\partial_{i_1} \dots \partial_{i_k}f(x) df^j(x) \neq 0 j k df^k(x) \neq 0 x k df^k(x) df^k(x)(h,\dots,h) \neq 0 h \in \mathbb R^n x df^k(x) df^k(x)(h,\dots,h) > 0 h \in \mathbb R^n f x f \mathbb R \to \mathbb R f^k(x)>0 f'' x 
f''(y) = {1 \over (k-2)!} f^{(k)}(x)(y - x)^{k-2} + O((y - x)^{k-1}).
 f''(y)>0 y \ne x x f x k>2 \text{Hess}f(x)=df^2(x)=0 \text{Hess}f(y) y x \text{Hess}f x df^k(x)  f(x,y) = x^2 y^2 + x^8 + y^8 f (0,0) \det(\text{Hess}f(x,y))=3136 x^6 y^6 + 112 x^8 + 112 y^8 - 12 x^2 y^2, x=y f \text{Hess}f(0,0)=0 (0,0) df^4(0) h=h^1e_1+h^2e_2 df^4(0)(h,h,h,h)=4(h^1)^2(h^2)^2 h_i","['real-analysis', 'multivariable-calculus', 'optimization', 'convex-analysis', 'maxima-minima']"
58,If continuity condition is necessary for Miklós Schweitzer 2015 Problem 8,If continuity condition is necessary for Miklós Schweitzer 2015 Problem 8,,"Update : The continuity condition is necessary, according to Ian Morris's comment in https://mathoverflow.net/questions/269064/for-a-continuous-function-f-mathbbr-to-mathbbr-does-fx-fy . Miklós Schweitzer Competition 2015 Problem 8: Problem 1 : Prove that all continuous solutions of the functional equation $$[f(x) - f(y)]\left(f(\tfrac{x+y}{2}) - f(\sqrt{xy})\right) = 0, \forall x, y \in (0, +\infty)$$ are the constant functions. http://www.math.u-szeged.hu/~mmaroti/schweitzer/schweitzer-2015-eng.pdf https://artofproblemsolving.com/community/c6h1224690p6149915 https://mathoverflow.net/questions/269064/for-a-continuous-function-f-mathbbr-to-mathbbr-does-fx-fy To make the domain and range of $f$ clear ( Thank @Calvin Lin for his valuable comments ), I rephrased the problem above as follows. Problem 1 (rephrased) : Let $f : \ (0, \infty) \to \mathbb{R}$ be a continuous function satisfying $$[f(x) - f(y)]\left(f(\tfrac{x+y}{2}) - f(\sqrt{xy})\right) = 0, \forall x, y \in (0, +\infty).$$ Prove that $f$ is the constant function. My question : If the continuity condition on $(0, \infty)$ is necessary? Any comments are welcome and appreciated. The solutions in the 2nd link above use the continuity condition. Terry Tao's proof in the 3rd link above also use the continuity condition. My solution (at the end) requires the continuity condition as well. On the other hand, the continuity condition is not necessary for the following problem (from a math exam): Problem 2 : Let $f : (0, \infty) \to (0, \infty)$ be a continuous function satisfying $f(\tfrac{x+y}{2}) - f(\sqrt{xy}) = 0,\ \forall x, y > 0$ . Find all $f$ . Solution for Problem 2 : $f$ is the constant function. For any $0 < b < a$ , let $x = a + \sqrt{a^2 - b^2}$ and $y = a - \sqrt{a^2 - b^2}$ . We have $f(a)=f(b)$ . [Remark: We do not use the continuity condition in the proof.] My solution for Problem 1 : Assume, for the sake of contradiction, that $f$ is not the constant function. WLOG, assume there exist two real numbers $0 < A < C$ with $f(A) < f(C)$ . Since $f$ is continuous, there exists $A < B \le C$ such that $f(x) < f(B)$ for all $x$ in $[A, B)$ . Consider the sequence $$x_1 = \sqrt{AB}; \ x_{k+1} = \frac{x_k^2 + B^2}{2B}, \ k\ge 1.$$ By using Mathematical Induction, it is easy to prove that $x_k < x_{k+1} < B$ for all $k \ge 1$ . Thus, $\lim_{k\to \infty} x_k$ exists. Let $L = \lim_{k\to \infty} x_k$ . Then, $L = \frac{L^2 + B^2}{2B}$ and $L = B$ . Thus, $\lim_{k\to \infty} x_k = B$ . Denote $a = x_{k+1}, b = x_k$ ( $k\ge 1$ ). Let $X = a - \sqrt{a^2 - b^2}$ and $Y = a + \sqrt{a^2 - b^2}$ . It is easy to verify that $X < Y$ , $Y = B$ , $\frac{X+Y}{2} = a$ , and $\sqrt{XY} = b$ . Thus, $X = \frac{b^2}{Y} \ge \frac{x_1^2}{B} = A$ . Thus, $f(X) - f(Y) \ne 0$ . From $[f(X) - f(Y)] [f(\frac{X+Y}{2}) - f(\sqrt{XY})] = 0$ , we have $f(a) = f(b)$ , i.e., $f(x_k) = f(x_{k+1})$ for all $k \ge 1$ . Thus, $f(x_k) = f(x_1) = f(\sqrt{AB})$ for all $k \ge 1$ . Since $f$ is continuous, we have $f(B) = f(\lim_{k\to \infty} x_k) = \lim_{k\to \infty} f(x_k)  = f(\sqrt{AB})$ . Contradiction.","Update : The continuity condition is necessary, according to Ian Morris's comment in https://mathoverflow.net/questions/269064/for-a-continuous-function-f-mathbbr-to-mathbbr-does-fx-fy . Miklós Schweitzer Competition 2015 Problem 8: Problem 1 : Prove that all continuous solutions of the functional equation are the constant functions. http://www.math.u-szeged.hu/~mmaroti/schweitzer/schweitzer-2015-eng.pdf https://artofproblemsolving.com/community/c6h1224690p6149915 https://mathoverflow.net/questions/269064/for-a-continuous-function-f-mathbbr-to-mathbbr-does-fx-fy To make the domain and range of clear ( Thank @Calvin Lin for his valuable comments ), I rephrased the problem above as follows. Problem 1 (rephrased) : Let be a continuous function satisfying Prove that is the constant function. My question : If the continuity condition on is necessary? Any comments are welcome and appreciated. The solutions in the 2nd link above use the continuity condition. Terry Tao's proof in the 3rd link above also use the continuity condition. My solution (at the end) requires the continuity condition as well. On the other hand, the continuity condition is not necessary for the following problem (from a math exam): Problem 2 : Let be a continuous function satisfying . Find all . Solution for Problem 2 : is the constant function. For any , let and . We have . [Remark: We do not use the continuity condition in the proof.] My solution for Problem 1 : Assume, for the sake of contradiction, that is not the constant function. WLOG, assume there exist two real numbers with . Since is continuous, there exists such that for all in . Consider the sequence By using Mathematical Induction, it is easy to prove that for all . Thus, exists. Let . Then, and . Thus, . Denote ( ). Let and . It is easy to verify that , , , and . Thus, . Thus, . From , we have , i.e., for all . Thus, for all . Since is continuous, we have . Contradiction.","[f(x) - f(y)]\left(f(\tfrac{x+y}{2}) - f(\sqrt{xy})\right) = 0, \forall x, y \in (0, +\infty) f f : \ (0, \infty) \to \mathbb{R} [f(x) - f(y)]\left(f(\tfrac{x+y}{2}) - f(\sqrt{xy})\right) = 0, \forall x, y \in (0, +\infty). f (0, \infty) f : (0, \infty) \to (0, \infty) f(\tfrac{x+y}{2}) - f(\sqrt{xy}) = 0,\ \forall x, y > 0 f f 0 < b < a x = a + \sqrt{a^2 - b^2} y = a - \sqrt{a^2 - b^2} f(a)=f(b) f 0 < A < C f(A) < f(C) f A < B \le C f(x) < f(B) x [A, B) x_1 = \sqrt{AB}; \ x_{k+1} = \frac{x_k^2 + B^2}{2B}, \ k\ge 1. x_k < x_{k+1} < B k \ge 1 \lim_{k\to \infty} x_k L = \lim_{k\to \infty} x_k L = \frac{L^2 + B^2}{2B} L = B \lim_{k\to \infty} x_k = B a = x_{k+1}, b = x_k k\ge 1 X = a - \sqrt{a^2 - b^2} Y = a + \sqrt{a^2 - b^2} X < Y Y = B \frac{X+Y}{2} = a \sqrt{XY} = b X = \frac{b^2}{Y} \ge \frac{x_1^2}{B} = A f(X) - f(Y) \ne 0 [f(X) - f(Y)] [f(\frac{X+Y}{2}) - f(\sqrt{XY})] = 0 f(a) = f(b) f(x_k) = f(x_{k+1}) k \ge 1 f(x_k) = f(x_1) = f(\sqrt{AB}) k \ge 1 f f(B) = f(\lim_{k\to \infty} x_k) = \lim_{k\to \infty} f(x_k) 
= f(\sqrt{AB})","['real-analysis', 'contest-math', 'functional-equations']"
59,What exactly does curl measure? What is rotating about what?,What exactly does curl measure? What is rotating about what?,,"This question is a bit long, the next two paragraphs give some context, but you can skip it. Thank you. I have seen many different explanations for the meaning of curl, or what exactly does it measures, but so far none of those gives a truly unambiguous, rigorous, definitive and clear explanation that clear all my doubts. Often it is mentioned that curl measures ""the tendency of the vector field to rotate"" or ""curl measures local rotation"", without mentioning what exactly does ""local"" mean, or what is ""tendency"". There is no mention of what exactly is rotating about what. And what exactly is the measurement of the rotation, is it ""angular speed""? In which case, is it measured in radians/unit time? But this seems very arbitrary, and does not feel right, as mathematical concept should not rely on a physical concept. So, I found this to give the most sense. But I still have lots of doubts regarding the explanation. Here is what I understood from reading it; along with some hypothesis I made (which may be wrong), which I believe is necessary to make the entire reasoning logically consistent. Please verify if my understanding is correct, help to correct any mistake in the assumptions/hypothesis I made. And most importantly, the following is written in natural language, which is not precise, but I lack all of the analysis machinery to spell all things out in precise mathematical terms. Please help to rephrase the story using fully rigorous analysis. Suppose we are dealing with 3 dimensional space. Given a vector field F, curl F is also a vector field, such that at each point (x, y, z), curl F measures this: draw a sphere centered at (x, y, z) with infinitely small radius, the sphere is infinitely small but not a singularity, so there are other points in the interior other than (x, y, z). F will give the direction and magnitude of the vector at each of the interior points. Then curl F gives 2x the total average instantaneous ""angular velocity"" of all the interior points about (x, y, z). It's not a real angular velocity as it does not carry the same physical units or meaning. It would be more precise to say it's 2x the total average instantaneous "" ratio between the length of tangential component of vector to the radius "" of all the interior points about (x, y, z) The rotation of the points as defined in 1 can be broken down into 3 orthogonal components, for example in 3-D euclidean coordinate system the x, y and z components. Hence it is valid to compute individual components of curl F separately in each of the three orthogonal planes. When computing one of the components (say z component in the x-y plane). Because we are only concerned with a sphere with infinitely small radius, we can regard the sphere as just a plane disc. As each of the level plane of the small sphere will have the same average ""angular velocity"". Similarly, when computing the average rotation of the disc, because we are letting the disc's radius tending to zero, so every ring on that disc will again have the same average ""angular velocity"". Thus, it is valid to just consider a single ring around the point (x, y, z). Hence the computation simply involves calculating the circulation of F around (x, y, z) with infinitely small radius. Hence the analogy of a small ""paddle wheel"" in the linked notes. But there is a problem here, if we apply the same argument in 3 and 4, we can say that the ring will shrink to a point, so that the value of the vector field F is the same at all points. But then this means that there will be no rotation, and the curl of any field is the zero vector. This is obviously wrong. But why is it valid to apply 3 and 4, but not 5? I need an analytic proof. Also, why should we be concerned with a small sphere, but not a small enclosed shape of any sort? A cube, a tetrahedron or some irregular shape? My hypothesis is that because for every closed shape one can come up with, there will be two spheres that bound the shape from inside and outside. If we can show that as the radii tend to zero the average ""angular velocity"" of all interior points in the two spheres are the same, then whatever shape one comes up with must also have the same average ""angular velocity"". with regard to 3-6, there seems to be a sense of when boundary shapes are ""topologically"" equivalent when considering limits. When or when not can someone ""simplify"" the shape to another form. Is there a formal branch of mathematics that deal with this, and gives the precise analysis tool to reason about this? the term ""instantaneous"" is important in ""total average instantaneous angular velocity of all the interior points about (x, y, z)"". Because, this way we will not be concerning ourselves with physics. Since, the outcome of filling the small sphere with a elastics/inelastic rigid-body/fluid will be different. If it was a rigid body, with perfect in-elasticity, then in general the object will disintegrate as F does not guarantee that all points will always have the same relative distance and orientation after some movement specified by the vector field. If it was a elastic rigid body, then some parts of the object could be compressed, and some parts will be stretched out. So, we are only concerned (on a more abstract level) about all the points enclosed by the sphere we draw, nothing to do with physics.","This question is a bit long, the next two paragraphs give some context, but you can skip it. Thank you. I have seen many different explanations for the meaning of curl, or what exactly does it measures, but so far none of those gives a truly unambiguous, rigorous, definitive and clear explanation that clear all my doubts. Often it is mentioned that curl measures ""the tendency of the vector field to rotate"" or ""curl measures local rotation"", without mentioning what exactly does ""local"" mean, or what is ""tendency"". There is no mention of what exactly is rotating about what. And what exactly is the measurement of the rotation, is it ""angular speed""? In which case, is it measured in radians/unit time? But this seems very arbitrary, and does not feel right, as mathematical concept should not rely on a physical concept. So, I found this to give the most sense. But I still have lots of doubts regarding the explanation. Here is what I understood from reading it; along with some hypothesis I made (which may be wrong), which I believe is necessary to make the entire reasoning logically consistent. Please verify if my understanding is correct, help to correct any mistake in the assumptions/hypothesis I made. And most importantly, the following is written in natural language, which is not precise, but I lack all of the analysis machinery to spell all things out in precise mathematical terms. Please help to rephrase the story using fully rigorous analysis. Suppose we are dealing with 3 dimensional space. Given a vector field F, curl F is also a vector field, such that at each point (x, y, z), curl F measures this: draw a sphere centered at (x, y, z) with infinitely small radius, the sphere is infinitely small but not a singularity, so there are other points in the interior other than (x, y, z). F will give the direction and magnitude of the vector at each of the interior points. Then curl F gives 2x the total average instantaneous ""angular velocity"" of all the interior points about (x, y, z). It's not a real angular velocity as it does not carry the same physical units or meaning. It would be more precise to say it's 2x the total average instantaneous "" ratio between the length of tangential component of vector to the radius "" of all the interior points about (x, y, z) The rotation of the points as defined in 1 can be broken down into 3 orthogonal components, for example in 3-D euclidean coordinate system the x, y and z components. Hence it is valid to compute individual components of curl F separately in each of the three orthogonal planes. When computing one of the components (say z component in the x-y plane). Because we are only concerned with a sphere with infinitely small radius, we can regard the sphere as just a plane disc. As each of the level plane of the small sphere will have the same average ""angular velocity"". Similarly, when computing the average rotation of the disc, because we are letting the disc's radius tending to zero, so every ring on that disc will again have the same average ""angular velocity"". Thus, it is valid to just consider a single ring around the point (x, y, z). Hence the computation simply involves calculating the circulation of F around (x, y, z) with infinitely small radius. Hence the analogy of a small ""paddle wheel"" in the linked notes. But there is a problem here, if we apply the same argument in 3 and 4, we can say that the ring will shrink to a point, so that the value of the vector field F is the same at all points. But then this means that there will be no rotation, and the curl of any field is the zero vector. This is obviously wrong. But why is it valid to apply 3 and 4, but not 5? I need an analytic proof. Also, why should we be concerned with a small sphere, but not a small enclosed shape of any sort? A cube, a tetrahedron or some irregular shape? My hypothesis is that because for every closed shape one can come up with, there will be two spheres that bound the shape from inside and outside. If we can show that as the radii tend to zero the average ""angular velocity"" of all interior points in the two spheres are the same, then whatever shape one comes up with must also have the same average ""angular velocity"". with regard to 3-6, there seems to be a sense of when boundary shapes are ""topologically"" equivalent when considering limits. When or when not can someone ""simplify"" the shape to another form. Is there a formal branch of mathematics that deal with this, and gives the precise analysis tool to reason about this? the term ""instantaneous"" is important in ""total average instantaneous angular velocity of all the interior points about (x, y, z)"". Because, this way we will not be concerning ourselves with physics. Since, the outcome of filling the small sphere with a elastics/inelastic rigid-body/fluid will be different. If it was a rigid body, with perfect in-elasticity, then in general the object will disintegrate as F does not guarantee that all points will always have the same relative distance and orientation after some movement specified by the vector field. If it was a elastic rigid body, then some parts of the object could be compressed, and some parts will be stretched out. So, we are only concerned (on a more abstract level) about all the points enclosed by the sphere we draw, nothing to do with physics.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
60,"If $f$ is continuous and $f'(x)\ge 0$, outside of a countable set, then $f$ is increasing","If  is continuous and , outside of a countable set, then  is increasing",f f'(x)\ge 0 f,"PROBLEM. Let $f:[a,b]\to\mathbb R$ be a continuous function, such that $f'(x)\ge 0$ , for all $x\in [a,b]\setminus A$ , where $A\subset [a,b]$ is a countable set. Show that $f$ is increasing. Attention. In this problem, we DO NOT assume that $f$ is differentiable in the whole $[a,b]$ . Notes. (1) If we assume that $f$ is differentiable in the whole interval, then we can easily show that $f'(x)\ge 0$ , everywhere. For otherwise, if $f'(x_0)=c<0$ , for some $x_0\in [a,b]$ , then by virtue of Darboux's Theorem , $(c,0)\subset f'([a,b])$ , and hence, $f'(x)<0$ , for uncountably many $x$ 's. (2) The conclusion of the problem does not hold if we replace the assumption $A$ is countable with $A$ is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis $f'(x)\ge 0$ , is replaced by $f'(x)=0$ , then the conclusion becomes f is constant .","PROBLEM. Let be a continuous function, such that , for all , where is a countable set. Show that is increasing. Attention. In this problem, we DO NOT assume that is differentiable in the whole . Notes. (1) If we assume that is differentiable in the whole interval, then we can easily show that , everywhere. For otherwise, if , for some , then by virtue of Darboux's Theorem , , and hence, , for uncountably many 's. (2) The conclusion of the problem does not hold if we replace the assumption is countable with is a set of measure zero . Take for example the Devil's staircase , with a negative sign in front. (3) If the hypothesis , is replaced by , then the conclusion becomes f is constant .","f:[a,b]\to\mathbb R f'(x)\ge 0 x\in [a,b]\setminus A A\subset [a,b] f f [a,b] f f'(x)\ge 0 f'(x_0)=c<0 x_0\in [a,b] (c,0)\subset f'([a,b]) f'(x)<0 x A A f'(x)\ge 0 f'(x)=0","['real-analysis', 'calculus', 'derivatives', 'continuity', 'monotone-functions']"
61,Existence and Uniqueness of Poisson Equation with Robin Boundary Condition using First Variation Methods,Existence and Uniqueness of Poisson Equation with Robin Boundary Condition using First Variation Methods,,"I'm currently stuck on the following exercise from Evans PDE Chapter 8 Exercise 11. Let $\beta: \mathbb{R} \rightarrow \mathbb{R}$ be smooth with \begin{equation} 0 < a \leq \beta'(z) \leq b, \text{ } z \in \mathbb{R} \end{equation} for constants $a,b$ . Let $f \in L^2(U)$ where $U$ is a bounded subset of $\mathbb{R}^n$ with smooth boundary. Formulate what it means for $u \in H^1(U)$ to be a weak solution of the non-linear boundary value problem \begin{equation*} \begin{cases} -\Delta u = f \text{ in } U\\ \frac{\partial u}{\partial \nu} + \beta(u) = 0 \text{ on } \partial U \end{cases} \end{equation*} Prove there exists a unique solution.( $\nu$ is the outward normal vector) Let $\mathrm{Tr}$ be the trace operator, then I was able to formulate what a weak solution meant e.g. for any $v \in H^1(U)$ \begin{equation*} \int_{\partial U} \beta\big(\mathrm{Tr}(u)\big) \mathrm{Tr}(v) + \int_{\Omega} Du \cdot Dv - fv = 0 \end{equation*} However, I have problems finding a corresponding energy for this PDE. From the condition that $\beta'(z)$ is strictly positive and that we want a unique solution, I deduced that our energy probably has an expression for the anti-derivative of $\beta$ to make the energy strictly convex. I believe the energy is \begin{equation*} E(u) := \int_{U} \frac{1}{2} |Du|^2 - fu \text{ } dx + \int_{\partial U}\int_{0}^{\mathrm{Tr}(u)} \beta'(t) \text{ } dt dx \end{equation*} and our admissible set $\mathcal{A} = H^1(U)$ . Indeed, the Euler Lagrange Equation matches the weak formulation. And we know from joint convexity of the Lagrangian associated with the energy that any solution of the Euler-Lagrange is a minimizer, so there is at most one solution by Strict Convexity. However, I cannot prove there exists a solution e.g. I can't prove the minimizing sequence is bounded. Any hints or help would be appreciated.","I'm currently stuck on the following exercise from Evans PDE Chapter 8 Exercise 11. Let be smooth with for constants . Let where is a bounded subset of with smooth boundary. Formulate what it means for to be a weak solution of the non-linear boundary value problem Prove there exists a unique solution.( is the outward normal vector) Let be the trace operator, then I was able to formulate what a weak solution meant e.g. for any However, I have problems finding a corresponding energy for this PDE. From the condition that is strictly positive and that we want a unique solution, I deduced that our energy probably has an expression for the anti-derivative of to make the energy strictly convex. I believe the energy is and our admissible set . Indeed, the Euler Lagrange Equation matches the weak formulation. And we know from joint convexity of the Lagrangian associated with the energy that any solution of the Euler-Lagrange is a minimizer, so there is at most one solution by Strict Convexity. However, I cannot prove there exists a solution e.g. I can't prove the minimizing sequence is bounded. Any hints or help would be appreciated.","\beta: \mathbb{R} \rightarrow \mathbb{R} \begin{equation} 0 < a \leq \beta'(z) \leq b, \text{ } z \in \mathbb{R} \end{equation} a,b f \in L^2(U) U \mathbb{R}^n u \in H^1(U) \begin{equation*} \begin{cases} -\Delta u = f \text{ in } U\\ \frac{\partial u}{\partial \nu} + \beta(u) = 0 \text{ on } \partial U \end{cases} \end{equation*} \nu \mathrm{Tr} v \in H^1(U) \begin{equation*} \int_{\partial U} \beta\big(\mathrm{Tr}(u)\big) \mathrm{Tr}(v) + \int_{\Omega} Du \cdot Dv - fv = 0 \end{equation*} \beta'(z) \beta \begin{equation*} E(u) := \int_{U} \frac{1}{2} |Du|^2 - fu \text{ } dx + \int_{\partial U}\int_{0}^{\mathrm{Tr}(u)} \beta'(t) \text{ } dt dx \end{equation*} \mathcal{A} = H^1(U)","['real-analysis', 'partial-differential-equations', 'calculus-of-variations', 'poissons-equation']"
62,How to prove using elementary methods that this function is everywhere continuous but nowhere differentiable?,How to prove using elementary methods that this function is everywhere continuous but nowhere differentiable?,,"Let $f$ be the function defined on all of $\mathbb{R}$ by the formula $$ f(x) \colon= \sum_{n=0}^\infty \frac{1}{2^n} \cos \left( 3^n x \right). $$ How to show (rigorously but through elementary logic) that the function $f$ is (1) continuous everywhere? (2) differentiable nowhere? This example has been given in Sec. 6.1 in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition. So ideally I would like to have an argument based purely on the machinary developed in the book upto this point. However, a proof using the relevant results in the subsequent chapters and sections of the book would also be fine, provided that due references are given of all the facts used. I do know that the infinite series in question does converge (in fact it converges absolutely). So the function is defined everywhere on the real line.","Let be the function defined on all of by the formula How to show (rigorously but through elementary logic) that the function is (1) continuous everywhere? (2) differentiable nowhere? This example has been given in Sec. 6.1 in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition. So ideally I would like to have an argument based purely on the machinary developed in the book upto this point. However, a proof using the relevant results in the subsequent chapters and sections of the book would also be fine, provided that due references are given of all the facts used. I do know that the infinite series in question does converge (in fact it converges absolutely). So the function is defined everywhere on the real line.",f \mathbb{R}  f(x) \colon= \sum_{n=0}^\infty \frac{1}{2^n} \cos \left( 3^n x \right).  f,"['real-analysis', 'sequences-and-series', 'analysis', 'derivatives', 'continuity']"
63,Is there any two variable function which has no representation of the form $\sum\limits_{n=1}^{\infty} f_n(x)g_n(y)$?,Is there any two variable function which has no representation of the form ?,\sum\limits_{n=1}^{\infty} f_n(x)g_n(y),"Is there any function $f:\Bbb R^2\to\Bbb R$ which has no representation of the form below? $$f(x,y)=\sum_{n=1}^{\infty} g_n(x)h_n(y)\quad(x,y\in \Bbb R).$$ Editor's note: A possible source of motivation for this question lies in a trick used to find solutions to linear partial diferential equations, in particular Laplace's equation in two dimensions. As a first step, a solution of the form $f(x,y)=g(x)h(y)$ is sought; and then more-general solutions of the form displayed above can be generated. While it is far from obvious that all solutions to the 2-D Laplace equation must be of this form, it is also hard to find a counterexample, namely a solution that cannot be thus structured. (Indeed, is such a solution known at all?) If we relax the conditions so as not to require even continuity of $f$ (let alone its being a solution of Laplace's equation), then there is in principle more room to find a counterexample. Thus it seems that the question posed above may be answerable. In fact, while this editor is unable to come up with a counterexample, his (admittedly unreliable) intuition says that there probably is one and that the answer is yes.","Is there any function which has no representation of the form below? Editor's note: A possible source of motivation for this question lies in a trick used to find solutions to linear partial diferential equations, in particular Laplace's equation in two dimensions. As a first step, a solution of the form is sought; and then more-general solutions of the form displayed above can be generated. While it is far from obvious that all solutions to the 2-D Laplace equation must be of this form, it is also hard to find a counterexample, namely a solution that cannot be thus structured. (Indeed, is such a solution known at all?) If we relax the conditions so as not to require even continuity of (let alone its being a solution of Laplace's equation), then there is in principle more room to find a counterexample. Thus it seems that the question posed above may be answerable. In fact, while this editor is unable to come up with a counterexample, his (admittedly unreliable) intuition says that there probably is one and that the answer is yes.","f:\Bbb R^2\to\Bbb R f(x,y)=\sum_{n=1}^{\infty} g_n(x)h_n(y)\quad(x,y\in \Bbb R). f(x,y)=g(x)h(y) f",['real-analysis']
64,A Lipschitz Implicit Function Theorem.,A Lipschitz Implicit Function Theorem.,,"I look for a reference (book or article) that contains the statement of a version of the implicit function theorem as stated below. This statement I found in notes (with due proof) on the implicit function written by KC Border . In these notes the author states that the proof is based on the proof of the implicit function theorem for two-variable functions given by Apostol in his calculus book. I looked at the proof given by Apostol in his calculus book (Calculus vol.2) and in fact the proof method of the above theorem is, with due adaptations, the same. I am writing notes on the implicit function theorem. However, I did not want to have to repeat the proof, but to indicate to the reader a formal reference containing the proof. Update 1 13 November, 2018 I have already searched in the following textbooks Implicit Functions and Solution Mappings. A View from Variational Analysis The Implicit Function Theorem: History, Theory, and Applications Nonlinear Functional Analysis But I did not succeed. Update 2 January 27, 2021 I originally found this theorem in the Book Aplicações da Topologia à Análise (Applications Of Topology To Analysis) in Portuguese. But I'd like it to be a reference in English language. I have already searched in Treatise on Analysis. Foundations of modern analysis. vol. I by J. Dieudonne but again I didn't find anything like that.","I look for a reference (book or article) that contains the statement of a version of the implicit function theorem as stated below. This statement I found in notes (with due proof) on the implicit function written by KC Border . In these notes the author states that the proof is based on the proof of the implicit function theorem for two-variable functions given by Apostol in his calculus book. I looked at the proof given by Apostol in his calculus book (Calculus vol.2) and in fact the proof method of the above theorem is, with due adaptations, the same. I am writing notes on the implicit function theorem. However, I did not want to have to repeat the proof, but to indicate to the reader a formal reference containing the proof. Update 1 13 November, 2018 I have already searched in the following textbooks Implicit Functions and Solution Mappings. A View from Variational Analysis The Implicit Function Theorem: History, Theory, and Applications Nonlinear Functional Analysis But I did not succeed. Update 2 January 27, 2021 I originally found this theorem in the Book Aplicações da Topologia à Análise (Applications Of Topology To Analysis) in Portuguese. But I'd like it to be a reference in English language. I have already searched in Treatise on Analysis. Foundations of modern analysis. vol. I by J. Dieudonne but again I didn't find anything like that.",,"['real-analysis', 'general-topology', 'analysis', 'reference-request', 'implicit-function-theorem']"
65,"What kind of ""geometric"" regularity $f'^2$ gives on $f$","What kind of ""geometric"" regularity  gives on",f'^2 f,"When solving real-analysis' problems I like to represent the functions involved and think geometrically what is going on. Today I got the following exercise : Let $f \in \mathcal{C}^1(\mathbb{R},\mathbb{R})$ , such that : $$\int_\mathbb{R} \mid f \mid \in \mathbb{R}$$ $$\int_\mathbb{R}  f'^2  \in \mathbb{R}$$ Proove that $f$ is Hölder continuous. When I see the assumption involved it's not hard to see that there must be some Cauchy-Schwartz inequality somewhere (because the assumption of absolutely integrable is on the square of the derivative and not only on the derivative itself). So, with this in mind we easily get the following proof : We have (using CS) : $$ \mid f(x) -f(y) \mid \leq \int_x^y 1 \times f' \leq \sqrt{\int_x^y f'^2}\sqrt{y-x} $$ Hence it follows that $f$ is $\frac{1}{2}-$ Hölder continous since that $\sqrt{\int_x^y f'^2}$ is bounded. As you noticed it's not hard to come up with the proof since the assumption of the problem on the absolute integrability of the square of the derivative of $f$ directly leads to thinking we must use the Cauchy-Schwartz inequality. The problem is that I don't like this way of thinking. That's why I am seeking a geometric intuition of the problem. For example, if we only have the assumption of absolute integrability on $f'$ instead of $f'^2$ does the result still holds? Why the fact that: $\int_\mathbb{R}  f'^2  \in \mathbb{R}$ imply so much regularity for $f$ ? $\ldots$","When solving real-analysis' problems I like to represent the functions involved and think geometrically what is going on. Today I got the following exercise : Let , such that : Proove that is Hölder continuous. When I see the assumption involved it's not hard to see that there must be some Cauchy-Schwartz inequality somewhere (because the assumption of absolutely integrable is on the square of the derivative and not only on the derivative itself). So, with this in mind we easily get the following proof : We have (using CS) : Hence it follows that is Hölder continous since that is bounded. As you noticed it's not hard to come up with the proof since the assumption of the problem on the absolute integrability of the square of the derivative of directly leads to thinking we must use the Cauchy-Schwartz inequality. The problem is that I don't like this way of thinking. That's why I am seeking a geometric intuition of the problem. For example, if we only have the assumption of absolute integrability on instead of does the result still holds? Why the fact that: imply so much regularity for ?","f \in \mathcal{C}^1(\mathbb{R},\mathbb{R}) \int_\mathbb{R} \mid f \mid \in \mathbb{R} \int_\mathbb{R}  f'^2  \in \mathbb{R} f  \mid f(x) -f(y) \mid \leq \int_x^y 1 \times f' \leq \sqrt{\int_x^y f'^2}\sqrt{y-x}  f \frac{1}{2}- \sqrt{\int_x^y f'^2} f f' f'^2 \int_\mathbb{R}  f'^2  \in \mathbb{R} f \ldots","['calculus', 'real-analysis', 'integration', 'uniform-continuity', 'cauchy-schwarz-inequality']"
66,Prove $\sum\frac{\sin{(n)}\tan{(n)}}{n^3}$ diverges/converges,Prove  diverges/converges,\sum\frac{\sin{(n)}\tan{(n)}}{n^3},"The original problem is $$\sum \frac{\sin(n)\tan(n)\ln(e-\frac{1}{n})}{n^3}$$ I know that $\sum\frac{1}{n^3}$ converges absolutely, also $\sin(n)\leq 1$ and $\ln(e-1)<\ln(e-1/n)<\ln(e)=1$ Now how to determine whether $$\sum \frac{\tan(n)}{n^3}$$ converges? (If this converged, it would assure that the original converges, because it is an upper bound and by comparison test, we are done). Otherwise, if $\sum \tan(n)/n^3$ diverged, we wish to show about the series $$\sum\frac{\sin(n)\tan(n)}{n^3}$$ whether it converges/diverges. It seems obvious that $\tan(n)$ has no limit (also, how to formally prove this?), but what about the $n^3$ doesn't it actually send it to $0$ ? I would have an argument which I am quite unsure of,... $\tan(n)$ is only "" $\pm\infty$ "" in each $(k+\frac{1}{2})\pi$ , but we can't ever reach those points, because $n$ is from $\mathbb{N}$ ?","The original problem is I know that converges absolutely, also and Now how to determine whether converges? (If this converged, it would assure that the original converges, because it is an upper bound and by comparison test, we are done). Otherwise, if diverged, we wish to show about the series whether it converges/diverges. It seems obvious that has no limit (also, how to formally prove this?), but what about the doesn't it actually send it to ? I would have an argument which I am quite unsure of,... is only "" "" in each , but we can't ever reach those points, because is from ?",\sum \frac{\sin(n)\tan(n)\ln(e-\frac{1}{n})}{n^3} \sum\frac{1}{n^3} \sin(n)\leq 1 \ln(e-1)<\ln(e-1/n)<\ln(e)=1 \sum \frac{\tan(n)}{n^3} \sum \tan(n)/n^3 \sum\frac{\sin(n)\tan(n)}{n^3} \tan(n) n^3 0 \tan(n) \pm\infty (k+\frac{1}{2})\pi n \mathbb{N},"['real-analysis', 'sequences-and-series']"
67,Proving ${\frac{n+2}{2n+3}} $ converges to $\frac{1}{2}$,Proving  converges to,{\frac{n+2}{2n+3}}  \frac{1}{2},"I'd just like to verify whether I've done it correctly. Proof strategy. First we determine what we need to set $N$ equal to; $$\left|\frac{n+2}{2n+3}-\frac12\right|=\left|\frac{2n+4-(2n+3)}{4n+6}\right|= \frac1{4n+6}<\epsilon$$ Some rewriting get us, $$n>\frac{1}{4\epsilon}-\frac{6}4{}$$ When $\epsilon$ is $\frac{1}{6}$ $N$ will be 0 but this is not allowed since N should be a positive integer. We notice: $$n>\frac1{4\epsilon}>\frac1{4\epsilon} -\frac64$$ Hence choose $N=\lceil\frac1{4\epsilon}\rceil$ Proof: Let $\epsilon>0$ and choose $N=\lceil1/4\epsilon\rceil$ . Let $n>N$ where $n \in \mathbb{Z}$ . Then $n>\frac1{4\epsilon}>\frac1{4\epsilon}-\frac64$ . Thus, rewrite to get, $ \frac{1}{4n+6} < \epsilon$ Therefore, $$\left|\frac{n+2}{2n+3}-\frac12\right|=\left|\frac{2n+4-(2n+3)}{4n+6}\right|=\frac1{4n+6}<\epsilon$$ Hence the sequence converges to $\frac{1}{2}$ . $$\space \blacksquare$$","I'd just like to verify whether I've done it correctly. Proof strategy. First we determine what we need to set equal to; Some rewriting get us, When is will be 0 but this is not allowed since N should be a positive integer. We notice: Hence choose Proof: Let and choose . Let where . Then . Thus, rewrite to get, Therefore, Hence the sequence converges to .",N \left|\frac{n+2}{2n+3}-\frac12\right|=\left|\frac{2n+4-(2n+3)}{4n+6}\right|= \frac1{4n+6}<\epsilon n>\frac{1}{4\epsilon}-\frac{6}4{} \epsilon \frac{1}{6} N n>\frac1{4\epsilon}>\frac1{4\epsilon} -\frac64 N=\lceil\frac1{4\epsilon}\rceil \epsilon>0 N=\lceil1/4\epsilon\rceil n>N n \in \mathbb{Z} n>\frac1{4\epsilon}>\frac1{4\epsilon}-\frac64  \frac{1}{4n+6} < \epsilon \left|\frac{n+2}{2n+3}-\frac12\right|=\left|\frac{2n+4-(2n+3)}{4n+6}\right|=\frac1{4n+6}<\epsilon \frac{1}{2} \space \blacksquare,"['real-analysis', 'proof-verification']"
68,Is Hilbert's space-filling curve measure preserving?,Is Hilbert's space-filling curve measure preserving?,,"Say $f_n:[0,1]\to [0,1]^d$ is the $n$-th iteration of a $d$-dimensional Hilbert curve touring its range. Is it true that for any open $S\subset [0,1]^d$, then amount of time $f_n$ spends in $S$ is approximately  $\mathrm{vol}(S)$? That is, for $S\subset [0,1]^d$ open, taking $f^{-1}_n(S)=\{t:f_n(t)\in S\}\subset [0,1],$ is it true that: \begin{equation}     \mu(f^{-1}_n(S))\to_n \mu(S)? \end{equation} I think so. Each $f_n$ maps to the boundary of some lattice of cuboids, $(c^{(n)}_\lambda)_{\lambda \in \Lambda^{(n)}}$, where each $c^{(n)}_\lambda$ corresponds to a single cuboidal cell in $[0,1]^d$ with side length $\ell^{(n)}$.   Take $\mathcal{S}^{(n)}$ to be the collection of cuboids $c^{(n)}_\lambda$ which come within $\ell^{(n)}$ of $S$ and compute:   \begin{align} \mu(S) &\leq \mu(\cup_{\mathcal{S}^{(n)}})\\ &=\left|\mathcal{S}^{(n)}\right|\cdot(\ell^{(n)})^d \\ &\leq \mu(f_n^{-1}(\cup_{\mathcal{S}^{(n)}}))/(\ell^{(n)})^d\cdot (\ell^{(n)})^d \\ &\leq \mu(f^{-1}_n(\cup_{\mathcal{S}^{(n)}}\backslash S))+\mu(f^{-1}_n(S)) \\  &\leq O(1/n)+\mu(f_n^{-1}(S)). \end{align}    Get a similar upper bound on $ \mu(f_n^{-1}(S))$ by identically inspecting the collection of cuboids contained in $S$ and at least $\ell^{(n)}$ away from the boundary of $S$. Is this correct? Is there a better way?","Say $f_n:[0,1]\to [0,1]^d$ is the $n$-th iteration of a $d$-dimensional Hilbert curve touring its range. Is it true that for any open $S\subset [0,1]^d$, then amount of time $f_n$ spends in $S$ is approximately  $\mathrm{vol}(S)$? That is, for $S\subset [0,1]^d$ open, taking $f^{-1}_n(S)=\{t:f_n(t)\in S\}\subset [0,1],$ is it true that: \begin{equation}     \mu(f^{-1}_n(S))\to_n \mu(S)? \end{equation} I think so. Each $f_n$ maps to the boundary of some lattice of cuboids, $(c^{(n)}_\lambda)_{\lambda \in \Lambda^{(n)}}$, where each $c^{(n)}_\lambda$ corresponds to a single cuboidal cell in $[0,1]^d$ with side length $\ell^{(n)}$.   Take $\mathcal{S}^{(n)}$ to be the collection of cuboids $c^{(n)}_\lambda$ which come within $\ell^{(n)}$ of $S$ and compute:   \begin{align} \mu(S) &\leq \mu(\cup_{\mathcal{S}^{(n)}})\\ &=\left|\mathcal{S}^{(n)}\right|\cdot(\ell^{(n)})^d \\ &\leq \mu(f_n^{-1}(\cup_{\mathcal{S}^{(n)}}))/(\ell^{(n)})^d\cdot (\ell^{(n)})^d \\ &\leq \mu(f^{-1}_n(\cup_{\mathcal{S}^{(n)}}\backslash S))+\mu(f^{-1}_n(S)) \\  &\leq O(1/n)+\mu(f_n^{-1}(S)). \end{align}    Get a similar upper bound on $ \mu(f_n^{-1}(S))$ by identically inspecting the collection of cuboids contained in $S$ and at least $\ell^{(n)}$ away from the boundary of $S$. Is this correct? Is there a better way?",,"['real-analysis', 'general-topology', 'limits', 'measure-theory', 'proof-verification']"
69,Show that Minkowski functional is a sublinear functional,Show that Minkowski functional is a sublinear functional,,"A set $C\subseteq X$ is convex if for any $x,y\in C$ and any $0\leq t \leq 1,$ we have $tx+(1-t)y\in C.$ A set $C\subseteq X$ is absorbing if for any $x\in X,$ there exists $t>0$ such that $tx\in C.$ Let $X$ be a vector space over $\mathbb{R}$ and let $C$ be a convex and absorbing subset of $X.$ Define $\rho:X\to\mathbb{R}$ by $$\rho(x) = \inf\{t>0: \frac{x}{t}\in C \}.$$ Show that (a) $\rho$ is well-defined and that $\rho(x)\geq 0$ for all $x\in X.$ (b) $\rho(ax) = a\rho(x)$ for any $x\in X$ and any $a\geq 0.$ (c) $\rho(x+y) \leq \rho(x)+\rho(y)$ for any $x,y\in X.$ Moreover, $$\{x\in X:\rho(x)<1 \} \subseteq C \subseteq \{ x\in X:\rho(x)\leq 1\}.$$ My attempt: part (a): Fix $x\in X.$ Clearly $\{t>0:\frac{x}{t}\in C\}$ is bounded below by $0.$ Since $C$ is absorbing, there exists $t>0$ such that $$tx = \frac{x}{\frac{1}{t}} \in C.$$ Therefore, the set $\{t>0:\frac{x}{t}\in C \}\neq \emptyset$ and hence $\rho$ is well-defined. Fix $\varepsilon>0.$ Then there exists $t>0$ such that $\frac{x}{t}\in C$ and $$\rho(x) +\varepsilon>t>0.$$ Since $\varepsilon >0$ is arbitrary, therefore $\rho(x)\geq 0.$ part (b): Fix $x\in X$ Clearly $\rho(ax)=a\rho(x)$ holds if $a=0$ as $\rho(0)= 0.$ Assume that $\alpha>0.$ Then $$\rho(ax) = \inf\{a\frac{t}{a} : \frac{ax}{t} = \frac{x}{\frac{t}{a}} \in C\} = a\inf\{\frac{t}{a}: \frac{x}{\frac{t}{a}}\in C \} = a\rho(x).$$ part (c): Fix $\varepsilon>0.$ Then there exist $t_x$ and $t_y$ such that $\frac{x}{t_x} \in C, \frac{y}{t_y}\in C,$ $$t_x<\rho(x)+\frac{\varepsilon}{2} \text{ and } t_y < \rho(y) + \frac{\varepsilon}{2}.$$ It follows that $$t_x+t_y < \rho(x) + \rho(y) + \varepsilon.$$ By convexity of $C,$ $$\frac{x+y}{t_x+t_y} = (\frac{t_x}{t_x+t_y})\frac{x}{t_x} + (\frac{t_y}{t_x+t_y})\frac{y}{t_y} \in C.$$ Therefore, $$\rho(x+y) \leq t_x+t_y < \rho(x)+\rho(y)+\varepsilon.$$ Since $\varepsilon>0$ is arbitrary, hence $$\rho(x+y)\leq \rho(x)+\rho(y).$$ 'Moreover' part: Let $x\in \{x\in X:\rho(x)<1\}.$ Let $\varepsilon = 1 - \rho(x)>0.$ Then there exists $t>0$ such that $\frac{x}{t}\in C$ and $$1 = \rho(x)+\varepsilon > t.$$ Note that If $C$ is absorbing, then $0\in C.$ By convexity of $C,$ $$x = t(\frac{x}{t}) + (1-t)0\in C.$$ Finally, let $x\in C.$ Then $\frac{x}{1} = x \in C.$ Therefore, $\rho(x)\leq 1.$ Are my proofs correct?","A set is convex if for any and any we have A set is absorbing if for any there exists such that Let be a vector space over and let be a convex and absorbing subset of Define by Show that (a) is well-defined and that for all (b) for any and any (c) for any Moreover, My attempt: part (a): Fix Clearly is bounded below by Since is absorbing, there exists such that Therefore, the set and hence is well-defined. Fix Then there exists such that and Since is arbitrary, therefore part (b): Fix Clearly holds if as Assume that Then part (c): Fix Then there exist and such that It follows that By convexity of Therefore, Since is arbitrary, hence 'Moreover' part: Let Let Then there exists such that and Note that If is absorbing, then By convexity of Finally, let Then Therefore, Are my proofs correct?","C\subseteq X x,y\in C 0\leq t \leq 1, tx+(1-t)y\in C. C\subseteq X x\in X, t>0 tx\in C. X \mathbb{R} C X. \rho:X\to\mathbb{R} \rho(x) = \inf\{t>0: \frac{x}{t}\in C \}. \rho \rho(x)\geq 0 x\in X. \rho(ax) = a\rho(x) x\in X a\geq 0. \rho(x+y) \leq \rho(x)+\rho(y) x,y\in X. \{x\in X:\rho(x)<1 \} \subseteq C \subseteq \{ x\in X:\rho(x)\leq 1\}. x\in X. \{t>0:\frac{x}{t}\in C\} 0. C t>0 tx = \frac{x}{\frac{1}{t}} \in C. \{t>0:\frac{x}{t}\in C \}\neq \emptyset \rho \varepsilon>0. t>0 \frac{x}{t}\in C \rho(x) +\varepsilon>t>0. \varepsilon >0 \rho(x)\geq 0. x\in X \rho(ax)=a\rho(x) a=0 \rho(0)= 0. \alpha>0. \rho(ax) = \inf\{a\frac{t}{a} : \frac{ax}{t} = \frac{x}{\frac{t}{a}} \in C\} = a\inf\{\frac{t}{a}: \frac{x}{\frac{t}{a}}\in C \} = a\rho(x). \varepsilon>0. t_x t_y \frac{x}{t_x} \in C, \frac{y}{t_y}\in C, t_x<\rho(x)+\frac{\varepsilon}{2} \text{ and } t_y < \rho(y) + \frac{\varepsilon}{2}. t_x+t_y < \rho(x) + \rho(y) + \varepsilon. C, \frac{x+y}{t_x+t_y} = (\frac{t_x}{t_x+t_y})\frac{x}{t_x} + (\frac{t_y}{t_x+t_y})\frac{y}{t_y} \in C. \rho(x+y) \leq t_x+t_y < \rho(x)+\rho(y)+\varepsilon. \varepsilon>0 \rho(x+y)\leq \rho(x)+\rho(y). x\in \{x\in X:\rho(x)<1\}. \varepsilon = 1 - \rho(x)>0. t>0 \frac{x}{t}\in C 1 = \rho(x)+\varepsilon > t. C 0\in C. C, x = t(\frac{x}{t}) + (1-t)0\in C. x\in C. \frac{x}{1} = x \in C. \rho(x)\leq 1.","['real-analysis', 'functional-analysis', 'proof-verification', 'supremum-and-infimum']"
70,"If trigonometric series is $0$ everywhere, then its coefficients are $0$","If trigonometric series is  everywhere, then its coefficients are",0 0,"How to prove that if $\forall x \in \mathbb R, \sum_{n=-\infty}^\infty c_n e^{inx} = 0$, then $c_n=0$ for all $n$ ? I feel something could be done by integrating the series, but how to switch the sum and the integral then ? Nothing is assumed on the type of convergence or the regularity of $c_n$ (is it in $\ell^2$).","How to prove that if $\forall x \in \mathbb R, \sum_{n=-\infty}^\infty c_n e^{inx} = 0$, then $c_n=0$ for all $n$ ? I feel something could be done by integrating the series, but how to switch the sum and the integral then ? Nothing is assumed on the type of convergence or the regularity of $c_n$ (is it in $\ell^2$).",,"['real-analysis', 'fourier-series']"
71,$f(g(x))=g(f(x))$ implies $f(c)=g(c)$ for some $c$,implies  for some,f(g(x))=g(f(x)) f(c)=g(c) c,"Let $f$ and $g$ be continuous functions and map from $[0,1]$ to $[0,1]$. Also let $f(g(x)) = g(f(x))$ . Prove that there exists $c$ from $[0,1]$ such that $f(c)=g(c)$. I will try with contradiction. Let $h(x) = f(x) - g(x) > 0$ for all $x$ from $[0,1]$. Since $f(x)$ maps from $[0,1]$ to $[0,1]$ and is greater then $g$ for all $x$ from $[0,1]$ then this implies that $g(x)$ is not an element of $[0,1]$ for all $x$ from $[0,1]$. This is a contradiction, so there exists $c$ from $[0,1]$ such that $f(c)=g(c)$. Problem here is that I never used fact that  $f(g(x)) = g(f(x))$ which bothers me. Is proof correct or there is hole somewhere?","Let $f$ and $g$ be continuous functions and map from $[0,1]$ to $[0,1]$. Also let $f(g(x)) = g(f(x))$ . Prove that there exists $c$ from $[0,1]$ such that $f(c)=g(c)$. I will try with contradiction. Let $h(x) = f(x) - g(x) > 0$ for all $x$ from $[0,1]$. Since $f(x)$ maps from $[0,1]$ to $[0,1]$ and is greater then $g$ for all $x$ from $[0,1]$ then this implies that $g(x)$ is not an element of $[0,1]$ for all $x$ from $[0,1]$. This is a contradiction, so there exists $c$ from $[0,1]$ such that $f(c)=g(c)$. Problem here is that I never used fact that  $f(g(x)) = g(f(x))$ which bothers me. Is proof correct or there is hole somewhere?",,"['real-analysis', 'proof-verification']"
72,Proving $\lim\limits_{n\to \infty}\int\limits_{0}^{1} f_n(x)dx=0$,Proving,\lim\limits_{n\to \infty}\int\limits_{0}^{1} f_n(x)dx=0,"If $\{f_n\}$ is a sequence of continuous functions on $[0,1]$ such that $0\leq f_n\leq 1$ and such that $f_n(x)\to 0$ as $n\to \infty$ , for every $x\in [0,1]$ , then $$\lim\limits_{n\to \infty}\int\limits_{0}^{1} f_n(x)dx=0$$ Try to prove this without using any measure theory or any theorems about Lebesgue integration. I am quite stuck as to what method I should use to go about solving this proof. I am thinking of showing that the set $A_n=\{x\in[0,1]: f_n(x)\geq \epsilon/2\}$ has a measure smaller than $\epsilon/2$ for large n. But proving that especially without measure theory is pretty hard. Anyone have any hints? (I would like to thank everyone that offered help. You were all fantastic.)","If is a sequence of continuous functions on such that and such that as , for every , then Try to prove this without using any measure theory or any theorems about Lebesgue integration. I am quite stuck as to what method I should use to go about solving this proof. I am thinking of showing that the set has a measure smaller than for large n. But proving that especially without measure theory is pretty hard. Anyone have any hints? (I would like to thank everyone that offered help. You were all fantastic.)","\{f_n\} [0,1] 0\leq f_n\leq 1 f_n(x)\to 0 n\to \infty x\in [0,1] \lim\limits_{n\to \infty}\int\limits_{0}^{1} f_n(x)dx=0 A_n=\{x\in[0,1]: f_n(x)\geq \epsilon/2\} \epsilon/2","['real-analysis', 'measure-theory']"
73,Closed form of $\sum _{n=0}^{\infty} \frac{\left(-\pi ^2\right)^n \cos \left(2^nb\right)}{(2 n)!}$,Closed form of,\sum _{n=0}^{\infty} \frac{\left(-\pi ^2\right)^n \cos \left(2^nb\right)}{(2 n)!},"Is it possible to calculate the sum   $$ \sum _{n=0}^{\infty} \frac{\left(-\pi ^2\right)^n \cos \left(2^nb\right)}{(2 n)!} $$   in closed form? Formal naive argument gives  $$ \sum _{n=0}^{\infty} \frac{\left(-\pi ^2\right)^n}{(2 n)!}\sum _{m=0}^{\infty} \frac{\left(-b ^2\right)^m}{(2 m)!}2^{2mn}=\sum _{m=0}^{\infty} \frac{\left(-b ^2\right)^m\cos\left(2^m\pi\right)}{(2 m)!}=\cos b-2. $$ However, numerical calculation shows that this is wrong. For example, for $b=1/3$ http://www.wolframalpha.com/input/?i=N%5B1%2F(Cos%5B1%2F++++3%5D+-2)Sum%5B(-%5C%5BPi%5D%5E2)%5En%2FFactorial%5B2+n%5D+Cos%5B1%2F3+2%5En%5D,+%7Bn,+0,+150%7D%5D,+35%5D","Is it possible to calculate the sum   $$ \sum _{n=0}^{\infty} \frac{\left(-\pi ^2\right)^n \cos \left(2^nb\right)}{(2 n)!} $$   in closed form? Formal naive argument gives  $$ \sum _{n=0}^{\infty} \frac{\left(-\pi ^2\right)^n}{(2 n)!}\sum _{m=0}^{\infty} \frac{\left(-b ^2\right)^m}{(2 m)!}2^{2mn}=\sum _{m=0}^{\infty} \frac{\left(-b ^2\right)^m\cos\left(2^m\pi\right)}{(2 m)!}=\cos b-2. $$ However, numerical calculation shows that this is wrong. For example, for $b=1/3$ http://www.wolframalpha.com/input/?i=N%5B1%2F(Cos%5B1%2F++++3%5D+-2)Sum%5B(-%5C%5BPi%5D%5E2)%5En%2FFactorial%5B2+n%5D+Cos%5B1%2F3+2%5En%5D,+%7Bn,+0,+150%7D%5D,+35%5D",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'closed-form']"
74,Generalizing an inequality involving a convex function,Generalizing an inequality involving a convex function,,"If $V:[0,1] \rightarrow \mathbb{R}$ is a convex and nondecreasing function, then for all $(p,q,r,t,\lambda,\mu) \in [0,1]^{6}$ such that $p \geq q \geq r \geq t$ $\lambda p + (1-\lambda) t=\mu q + (1-\mu) r$ we have \begin{equation*} \lambda V(p) + (1-\lambda) V(t) \geq \mu V(q) + (1-\mu) V(r) \end{equation*} I am trying to generalize this observation to the multidimensional case. Consider $n \geq 2$ and the simplex  \begin{equation} \Delta=\{(p_1,\cdots,p_n) \in \mathbb{R}^{n} \mid \forall i, p_i > 0 \text{ and } \sum_{i=1}^{n}{p_i}=1\} \end{equation} Suppose that $\Delta$ is endowed with a (partial) order $\succeq$ such that \begin{equation*} p \succeq q \Rightarrow p \succeq \alpha p + (1-\alpha) q \succeq q \end{equation*} for all $\alpha \in [0,1]$. Consider a continuous function $V:\Delta \rightarrow \mathbb{R}$ that is convex and nondecreasing with respect to $\succeq$, i.e. $V(p) \geq V(q)$ whenever $p \succeq q$. Finally, consider four vectors $p,q,r,t$ and two scalars $\lambda \in [0,1], \mu \in [0,1]$ such that: $p \succeq q \succeq r \succeq t$ $\lambda p + (1-\lambda) t = \mu q + (1-\mu)r$ I am trying to prove that these conditions are sufficient to guarantee that \begin{equation*} \lambda V(p) + (1-\lambda) V(t) \geq \mu V(q) + (1-\mu) V(r) \end{equation*} It is easy when $q=\alpha p + (1-\alpha) t$ and $r=\beta p + (1-\beta) t$ for some $(\alpha,\beta) \in [0,1]^2$ since this case essentially boils down to the unidimensional problem. But I haven't made any progress otherwise. Any hint, help or reference would be greatly appreciated. Thank you!","If $V:[0,1] \rightarrow \mathbb{R}$ is a convex and nondecreasing function, then for all $(p,q,r,t,\lambda,\mu) \in [0,1]^{6}$ such that $p \geq q \geq r \geq t$ $\lambda p + (1-\lambda) t=\mu q + (1-\mu) r$ we have \begin{equation*} \lambda V(p) + (1-\lambda) V(t) \geq \mu V(q) + (1-\mu) V(r) \end{equation*} I am trying to generalize this observation to the multidimensional case. Consider $n \geq 2$ and the simplex  \begin{equation} \Delta=\{(p_1,\cdots,p_n) \in \mathbb{R}^{n} \mid \forall i, p_i > 0 \text{ and } \sum_{i=1}^{n}{p_i}=1\} \end{equation} Suppose that $\Delta$ is endowed with a (partial) order $\succeq$ such that \begin{equation*} p \succeq q \Rightarrow p \succeq \alpha p + (1-\alpha) q \succeq q \end{equation*} for all $\alpha \in [0,1]$. Consider a continuous function $V:\Delta \rightarrow \mathbb{R}$ that is convex and nondecreasing with respect to $\succeq$, i.e. $V(p) \geq V(q)$ whenever $p \succeq q$. Finally, consider four vectors $p,q,r,t$ and two scalars $\lambda \in [0,1], \mu \in [0,1]$ such that: $p \succeq q \succeq r \succeq t$ $\lambda p + (1-\lambda) t = \mu q + (1-\mu)r$ I am trying to prove that these conditions are sufficient to guarantee that \begin{equation*} \lambda V(p) + (1-\lambda) V(t) \geq \mu V(q) + (1-\mu) V(r) \end{equation*} It is easy when $q=\alpha p + (1-\alpha) t$ and $r=\beta p + (1-\beta) t$ for some $(\alpha,\beta) \in [0,1]^2$ since this case essentially boils down to the unidimensional problem. But I haven't made any progress otherwise. Any hint, help or reference would be greatly appreciated. Thank you!",,"['real-analysis', 'convex-analysis']"
75,Strong Counterexample to MVT on Q,Strong Counterexample to MVT on Q,,"A well-known application of the MVT is to prove that any $f: \mathbb{R} \to \mathbb{R}$ with $f'= 0$ is constant. But of course, the MVT relies fundamentally on the properties of $\mathbb{R}$, and in particular does not hold in $\mathbb{Q}$. A standard counter-example is the function $f :\mathbb{Q} \to \mathbb{Q}$ defined by $ f(x)=0 $ if $ x <\sqrt{2}$, and $f(x)=1$ if $ x > \sqrt{2}$, for this function is locally constant. There are several ways of thinking about the difference between $\mathbb{R}$ and $\mathbb{Q}$ : $\mathbb{R}$ is connected while $\mathbb{Q}$ is totally disconnected, closed bounded intervals are compact in $\mathbb{R}$, but not in $\mathbb{Q}$, $\mathbb{R}$ satisfies the least upper bound axiom, $\mathbb{Q}$ does not. Now, I wanted to see how far one could push this counterexample - is there a function $f :\mathbb{Q} \to \mathbb{Q}$ such that $f'=0$ and $f$ is strictly increasing? Clearly, any slight modification of the usual counterexample where $f$ is locally constant will not do. Also, one can show that if $f$ is uniformly differentiable with derivative zero, it must be constant, even in $\mathbb{Q}$, so I knew I was looking for non-uniformly differentiable but still differentiable $f$. Eventually, I came up with a construction, albeit not a very explicit one: the idea is that, since $\mathbb{Q}$ is countable, one can enumerate it $q_{1}, q_{2}, q_{3}, \cdots$, and then define $f: \mathbb{Q} \to \mathbb{Q}$ inductively and recursively. If one could define $f$ in such a way that $ \forall \ q_{n} \in \mathbb{Q} \ \exists \ c_{n} \in \mathbb{Q} : \forall \ x \in \mathbb{Q} \ |f(x)-f(q_{n})| \leq c_{n}|x-q_{n}|^2$, and $f$ is strictly increasing, we would be done. The recursive definition essentially defines $f(q_{n+1})$, after having defined $f(q_{1}),f(q_{2}),f(q_{3}), \cdots , f(q_{n})$, in such a way that these inequalities hold and $f$ is strictly increasing, and at each step the new $c_{n}$ is chosen large enough so that these inequalities do not become incompatible at subsequent steps ( I am willing to provide more details of this construction if asked). Note that, as $n \to \infty$, we must have $c_{n} \to \infty$, so that $f$ is not uniformly differentiable. Interestingly, the only key fact about $\mathbb{Q}$ used in the construction is that it is countable, so in a sense the construction provides a proof that $\mathbb{R}$ is uncountable, given the MVT. While the construction works, it is not very simple nor explicit, which is perhaps to be expected, and the function must be evaluated point by point in the order given by the enumeration of $\mathbb{Q}$. So I was wondering if anyone else knows or can come up with a simpler construction of a function $ f : \mathbb{Q} \to \mathbb{Q}$ such that $f'=0$ and $f$ is strictly increasing. Any thoughts and ideas are welcome. Edit: I've asked the same question on MO: https://mathoverflow.net/questions/248913/strong-counterexample-to-mvt-on-q","A well-known application of the MVT is to prove that any $f: \mathbb{R} \to \mathbb{R}$ with $f'= 0$ is constant. But of course, the MVT relies fundamentally on the properties of $\mathbb{R}$, and in particular does not hold in $\mathbb{Q}$. A standard counter-example is the function $f :\mathbb{Q} \to \mathbb{Q}$ defined by $ f(x)=0 $ if $ x <\sqrt{2}$, and $f(x)=1$ if $ x > \sqrt{2}$, for this function is locally constant. There are several ways of thinking about the difference between $\mathbb{R}$ and $\mathbb{Q}$ : $\mathbb{R}$ is connected while $\mathbb{Q}$ is totally disconnected, closed bounded intervals are compact in $\mathbb{R}$, but not in $\mathbb{Q}$, $\mathbb{R}$ satisfies the least upper bound axiom, $\mathbb{Q}$ does not. Now, I wanted to see how far one could push this counterexample - is there a function $f :\mathbb{Q} \to \mathbb{Q}$ such that $f'=0$ and $f$ is strictly increasing? Clearly, any slight modification of the usual counterexample where $f$ is locally constant will not do. Also, one can show that if $f$ is uniformly differentiable with derivative zero, it must be constant, even in $\mathbb{Q}$, so I knew I was looking for non-uniformly differentiable but still differentiable $f$. Eventually, I came up with a construction, albeit not a very explicit one: the idea is that, since $\mathbb{Q}$ is countable, one can enumerate it $q_{1}, q_{2}, q_{3}, \cdots$, and then define $f: \mathbb{Q} \to \mathbb{Q}$ inductively and recursively. If one could define $f$ in such a way that $ \forall \ q_{n} \in \mathbb{Q} \ \exists \ c_{n} \in \mathbb{Q} : \forall \ x \in \mathbb{Q} \ |f(x)-f(q_{n})| \leq c_{n}|x-q_{n}|^2$, and $f$ is strictly increasing, we would be done. The recursive definition essentially defines $f(q_{n+1})$, after having defined $f(q_{1}),f(q_{2}),f(q_{3}), \cdots , f(q_{n})$, in such a way that these inequalities hold and $f$ is strictly increasing, and at each step the new $c_{n}$ is chosen large enough so that these inequalities do not become incompatible at subsequent steps ( I am willing to provide more details of this construction if asked). Note that, as $n \to \infty$, we must have $c_{n} \to \infty$, so that $f$ is not uniformly differentiable. Interestingly, the only key fact about $\mathbb{Q}$ used in the construction is that it is countable, so in a sense the construction provides a proof that $\mathbb{R}$ is uncountable, given the MVT. While the construction works, it is not very simple nor explicit, which is perhaps to be expected, and the function must be evaluated point by point in the order given by the enumeration of $\mathbb{Q}$. So I was wondering if anyone else knows or can come up with a simpler construction of a function $ f : \mathbb{Q} \to \mathbb{Q}$ such that $f'=0$ and $f$ is strictly increasing. Any thoughts and ideas are welcome. Edit: I've asked the same question on MO: https://mathoverflow.net/questions/248913/strong-counterexample-to-mvt-on-q",,"['calculus', 'real-analysis']"
76,Sard's Theorem Using Integration?,Sard's Theorem Using Integration?,,"How exactly do we clean up this heuristic proof of Sard's theorem, from Schwarz' `Differential Topology for Physicists' using the Jacobian $J(f)(x)$: ""A heuristic justification for Sard's theorem is the following: if $f$ is a smooth one-to-one map on $S \subset E$, the volume of $S' = f(S)$ equals $\int_S |J(f)(x)|dx$. If $f$ is smooth, but not necessarily one-to-one, the volume of $S'$ is at most this integral. Applying this to the case where $S$ is the set of singular points of $f$, we conclude that the volume of $S'$ cannot be greater than zero, since $J(f)(x) = 0$ for all $x \in S$. This is not a complete proof because, strictly speaking, the formula $\mathrm{vol}S' = \int_S |J(f)(x)| dx$ only applies if $S$ satisfies certain conditions (for example, if $S$ is open). But it is not hard to make the proof watertight."" ? Does it link up with, or even maybe motivate, the proof from Milnor of Sard, If $F:M^m \to N^n$ is a smooth map between smooth manifolds of dimension $m$ and $n$ respectively, then $F(C)$ has measure zero in $N$ (where $C = \{x \in M : {rank}\; dF_x < n\}$) motivated here & given here , which amounts to writing $C = (C\smallsetminus C_1)\cup (C_1 \smallsetminus C_2) \cup \cdots \cup (C_{k-1} \smallsetminus C_k) \cup C_k$ (where $C_i$ denotes the set of points in $M$ for which all partial derivatives of order $\le i$ vanish at $x$) and then show that the image under $F$ of each of the sets in the union on the r.h.s. has measure zero in $N$. and invoking the constant rank theorem, Fubini's theorem & Taylor's theorem at various steps?","How exactly do we clean up this heuristic proof of Sard's theorem, from Schwarz' `Differential Topology for Physicists' using the Jacobian $J(f)(x)$: ""A heuristic justification for Sard's theorem is the following: if $f$ is a smooth one-to-one map on $S \subset E$, the volume of $S' = f(S)$ equals $\int_S |J(f)(x)|dx$. If $f$ is smooth, but not necessarily one-to-one, the volume of $S'$ is at most this integral. Applying this to the case where $S$ is the set of singular points of $f$, we conclude that the volume of $S'$ cannot be greater than zero, since $J(f)(x) = 0$ for all $x \in S$. This is not a complete proof because, strictly speaking, the formula $\mathrm{vol}S' = \int_S |J(f)(x)| dx$ only applies if $S$ satisfies certain conditions (for example, if $S$ is open). But it is not hard to make the proof watertight."" ? Does it link up with, or even maybe motivate, the proof from Milnor of Sard, If $F:M^m \to N^n$ is a smooth map between smooth manifolds of dimension $m$ and $n$ respectively, then $F(C)$ has measure zero in $N$ (where $C = \{x \in M : {rank}\; dF_x < n\}$) motivated here & given here , which amounts to writing $C = (C\smallsetminus C_1)\cup (C_1 \smallsetminus C_2) \cup \cdots \cup (C_{k-1} \smallsetminus C_k) \cup C_k$ (where $C_i$ denotes the set of points in $M$ for which all partial derivatives of order $\le i$ vanish at $x$) and then show that the image under $F$ of each of the sets in the union on the r.h.s. has measure zero in $N$. and invoking the constant rank theorem, Fubini's theorem & Taylor's theorem at various steps?",,"['real-analysis', 'differential-geometry', 'manifolds', 'differential-topology']"
77,On the convergence of a more complex iterated radical,On the convergence of a more complex iterated radical,,"After exploring Ramanujan's famed $$3=\sqrt{1+2\sqrt{1+3\sqrt{1+\cdots}}} $$ and $$4=\sqrt{6+2\sqrt{7+3\sqrt{8+\cdots}}},$$ both of which can be expressed more generally by $$x+n+a=\sqrt{ax+(n+a)^2+x\sqrt{a(x+n)+(n+a)^2+(x+n)\sqrt{a(x+2n)+\cdots}}},$$ I came across a nested radical of the form \begin{equation}P=\sqrt{a+\sqrt{ab+\sqrt{ab^2+\sqrt{ab^3+\cdots}}}} \end{equation} and am struggling to derive a formula for determining to what value $P$ converges. I know from a simple reformulation of $$\sqrt{a+b\sqrt{a+b\sqrt{a+\cdots}}}$$ that $$\sqrt{a+\sqrt{ab^2+\sqrt{ab^6+\sqrt{ab^{14}+\cdots}}}} $$ converges to $$\frac{b+\sqrt{b^2+4a}}{2}. $$ This result, however, fails to be helpful. Any advice on determining the convergence of $P$ would be appreciated. UPDATE: Let \begin{equation} P=\sqrt{a+\sqrt{ab+\sqrt{ab^2+\sqrt{ab^3+\cdots}}}}.\end{equation} Notice $$P(x)=\sqrt{ab^x+P(x+1)} $$ denotes the recurrence relation for $P$. My new attempt was to consider Ramanujan's recurrence relation: $$\psi(x)=\alpha+\beta+x=\sqrt{\alpha x +(\alpha+\beta)^2+x\psi(x+1)}, $$ let $\beta=1$, then set \begin{equation}ab^x=\alpha x +(\alpha+1)^2.\end{equation} Because $\alpha\geq0$, the above produces $$\alpha=\frac{-(x+2)+\sqrt{(x+2)^2-4(1-ab^x)}}{2}. $$ Through Ramanujan's formula, setting $x=1$ then implies that $$\psi(1)=\frac{1+\sqrt{5+4ab}}{2}=\sqrt{ab+\psi(2)}.$$ I realize, as suggest in Infinitely nested radical problem? , which is nearly identical to my new approach, that $P$ and $\psi$ are not in complete equitable forms. That post also suggests that this may not be the correct approach to solving this problem. Could someone in different words than the cited post please explain why this method is not sufficient and perhaps propose a new method or resource for computing radicals of this form?","After exploring Ramanujan's famed $$3=\sqrt{1+2\sqrt{1+3\sqrt{1+\cdots}}} $$ and $$4=\sqrt{6+2\sqrt{7+3\sqrt{8+\cdots}}},$$ both of which can be expressed more generally by $$x+n+a=\sqrt{ax+(n+a)^2+x\sqrt{a(x+n)+(n+a)^2+(x+n)\sqrt{a(x+2n)+\cdots}}},$$ I came across a nested radical of the form \begin{equation}P=\sqrt{a+\sqrt{ab+\sqrt{ab^2+\sqrt{ab^3+\cdots}}}} \end{equation} and am struggling to derive a formula for determining to what value $P$ converges. I know from a simple reformulation of $$\sqrt{a+b\sqrt{a+b\sqrt{a+\cdots}}}$$ that $$\sqrt{a+\sqrt{ab^2+\sqrt{ab^6+\sqrt{ab^{14}+\cdots}}}} $$ converges to $$\frac{b+\sqrt{b^2+4a}}{2}. $$ This result, however, fails to be helpful. Any advice on determining the convergence of $P$ would be appreciated. UPDATE: Let \begin{equation} P=\sqrt{a+\sqrt{ab+\sqrt{ab^2+\sqrt{ab^3+\cdots}}}}.\end{equation} Notice $$P(x)=\sqrt{ab^x+P(x+1)} $$ denotes the recurrence relation for $P$. My new attempt was to consider Ramanujan's recurrence relation: $$\psi(x)=\alpha+\beta+x=\sqrt{\alpha x +(\alpha+\beta)^2+x\psi(x+1)}, $$ let $\beta=1$, then set \begin{equation}ab^x=\alpha x +(\alpha+1)^2.\end{equation} Because $\alpha\geq0$, the above produces $$\alpha=\frac{-(x+2)+\sqrt{(x+2)^2-4(1-ab^x)}}{2}. $$ Through Ramanujan's formula, setting $x=1$ then implies that $$\psi(1)=\frac{1+\sqrt{5+4ab}}{2}=\sqrt{ab+\psi(2)}.$$ I realize, as suggest in Infinitely nested radical problem? , which is nearly identical to my new approach, that $P$ and $\psi$ are not in complete equitable forms. That post also suggests that this may not be the correct approach to solving this problem. Could someone in different words than the cited post please explain why this method is not sufficient and perhaps propose a new method or resource for computing radicals of this form?",,['real-analysis']
78,a continuous path between two sobolev functions without increasing energy,a continuous path between two sobolev functions without increasing energy,,"This question has been post on MO a week ago. I move it here to get more luck. Let $\Omega\subset \mathbb R^N$ be open bounded, smooth boundary. Let $u_1$, $u_2\in H^{1}(\Omega)$ such that $T[u_1]=T[u_2]=T[\omega]$ where $T$ stands for the trace operator and $\omega\in H^1(\Omega)$ is a fixed function. Define $$ F(u):=\inf_{v\in\mathcal V}\left\{\int_\Omega |\nabla u|^2v^2dx + \int_\Omega |\nabla v|^2+(1-v)^2dx \right\}, $$ where $\mathcal V:=\{v\in H^1(\Omega),\,0\leq v\leq 1\}$. Question: does there exist a path $a(t): [0,1]\to H^1(\Omega)$ between $u_1$ and $u_2$ satisfies the following conditions? $a(0)=u_1$, $a(1)=u_2$ $T[a(t)]=T[\omega]$ for all $t\in (0,1)$ $a(t)$ is continuous in $L^2$ sense, i.e., if $t\to t_0$, then $a(t)\to a(t_0)$ in $L^2$. $F(a(t))\leq \max\{F(u_1),F(u_2)\}$, for all $t\in (0,1)$. Any help, hint, or reference would be really welcome! Update: based on @Jason's answer, I wrote, for arbitrary $v\in\mathcal V$, $$ F(a(t))\leq G(a(t),v)\leq tG(u_1,v)+(1-t)G(u_2,v). $$ Let's denote by $v_1$ and $v_2$ that $F(u_1)=G(u_1,v_1)$ and $F(u_2)=G(u_2,v_2)$, such $v_1$ and $v_2$ exists and unique by the properties of $H^1$ function. I understand that $v$ on the right hand side is arbitrary so we may replace it with either $v_1$ or $v_2$ and we have $$ F(a(t))\leq tG(u_1,v_1)+(1-t)G(u_2,v_1)=t F(u_1)+(1-t)G(u_2,v_1)\tag 1 $$ or $$ F(a(t))\leq tG(u_1,v_2)+(1-t)F(u_2） $$ But we may can not go further from here. Take, from example, $(1)$. We can not switch $v_1$ by $v_2$ here since if we do, we will change $F(u_1)$ as well.","This question has been post on MO a week ago. I move it here to get more luck. Let $\Omega\subset \mathbb R^N$ be open bounded, smooth boundary. Let $u_1$, $u_2\in H^{1}(\Omega)$ such that $T[u_1]=T[u_2]=T[\omega]$ where $T$ stands for the trace operator and $\omega\in H^1(\Omega)$ is a fixed function. Define $$ F(u):=\inf_{v\in\mathcal V}\left\{\int_\Omega |\nabla u|^2v^2dx + \int_\Omega |\nabla v|^2+(1-v)^2dx \right\}, $$ where $\mathcal V:=\{v\in H^1(\Omega),\,0\leq v\leq 1\}$. Question: does there exist a path $a(t): [0,1]\to H^1(\Omega)$ between $u_1$ and $u_2$ satisfies the following conditions? $a(0)=u_1$, $a(1)=u_2$ $T[a(t)]=T[\omega]$ for all $t\in (0,1)$ $a(t)$ is continuous in $L^2$ sense, i.e., if $t\to t_0$, then $a(t)\to a(t_0)$ in $L^2$. $F(a(t))\leq \max\{F(u_1),F(u_2)\}$, for all $t\in (0,1)$. Any help, hint, or reference would be really welcome! Update: based on @Jason's answer, I wrote, for arbitrary $v\in\mathcal V$, $$ F(a(t))\leq G(a(t),v)\leq tG(u_1,v)+(1-t)G(u_2,v). $$ Let's denote by $v_1$ and $v_2$ that $F(u_1)=G(u_1,v_1)$ and $F(u_2)=G(u_2,v_2)$, such $v_1$ and $v_2$ exists and unique by the properties of $H^1$ function. I understand that $v$ on the right hand side is arbitrary so we may replace it with either $v_1$ or $v_2$ and we have $$ F(a(t))\leq tG(u_1,v_1)+(1-t)G(u_2,v_1)=t F(u_1)+(1-t)G(u_2,v_1)\tag 1 $$ or $$ F(a(t))\leq tG(u_1,v_2)+(1-t)F(u_2） $$ But we may can not go further from here. Take, from example, $(1)$. We can not switch $v_1$ by $v_2$ here since if we do, we will change $F(u_1)$ as well.",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'calculus-of-variations']"
79,Differentiation under the integral sign when derivative exists only almost everywhere,Differentiation under the integral sign when derivative exists only almost everywhere,,"Regarding the Theorem 3 from here (or pdf ver. ). Let $X$ be an open subset of $\mathbb{R}$ , and $\Omega$ be a measure space. Suppose that a function $f\colon X\times\Omega\to \mathbb{R}$ satisfies the following conditions: $f(x,\omega)$ is a measurable function of $x$ and $\omega$ jointly, and is integrable over $\omega$ ,  for almost all $x\in X$ held fixed. For almost all $\omega\in\Omega$ , $f(x,\omega)$ is an absolutely continuous function of $x$ . (This guarantees that $\frac{\partial f}{\partial x}(x,\omega)$ exists almost everywhere.) $\frac{\partial f}{\partial x}(x,\omega)$ is ""locally integrable"" --- that is, for all compact intervals $[a,b]\subset X$ : $$ \int_a^b\!\!\int_{\Omega}\left| \frac{\partial f}{\partial x}(x,\omega) \right| d\omega dx<\infty. $$ Then, $\int_\Omega f(x,\omega)d\omega$ is an absolutely continuous function of $x$ , and for almost every $x\in X$ , its derivatives exists and is given by $$ \frac{d}{dx}\int_{\Omega}f(x,\omega)d\omega = \int_{\Omega}\frac{d}{dx}f(x,\omega)d\omega. $$ Question 1: I do not see how to show $\int_\Omega f(x,\omega)d\omega$ is absolutely continuous in $x$ . Question 2: I also wonder where I can find this result preferably in a book or a paper etc. ==================================================== Here is my thought: Notations basically follow the plametmath note by Steven Cheng (the link above). Let $(\Omega,\mu)$ be the measure space and $(a,b):=X\subset\mathbb{R}$ . For $\mu$ -almost all $\omega\in \Omega$ , we assumed that $f(\cdot,\omega)$ is an absolutely continuous function in the first variable. Hence for a.e.- $x$ we have \begin{align} f(x,\omega)&=f(a,\omega)+\int_{a}^x\frac{\partial}{\partial y}f(y,\omega)\,dy \end{align} Letting $G(x):=\int_{a}^x\int_{\Omega}\frac{\partial}{\partial y}f(y,\omega)\, d\mu(\omega ){d}y$ and $g(x):=\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)\, d\mu(\omega )$ by virtue of Lebesgue differentiation theorem for the indefinite integral, for a.e. $x$ we have \begin{align} \int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)d\mu(\omega ) &=g(x)= \frac{\partial}{\partial x}G(x) = \frac{\partial}{\partial x}\int_{\Omega}\int_{a}^x \frac{\partial}{\partial y}f(y,\omega)\, dyd\mu(\omega ). \end{align} where the last equality is ok from the assumption $ \int_{\Omega}\int_{a}^x\left|\frac{\partial}{\partial y} 	f(y,\omega)\right| \,dyd\mu(\omega ) 	<\infty $ . Adding $0=\frac{d}{d  t}\int_{\Omega}f(a,\omega)d\mu(\omega )$ to the above yields \begin{align} \int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)d\mu(\omega ) &=\frac{\partial}{\partial x}\left(\int_{\Omega}\left\{ \int_{a}^x \frac{\partial}{\partial y}f(y,\omega)\, dy+f(a,\omega)\right\}d\mu(\omega ) \right)\\ &=\frac{\partial}{\partial x}\int_{\Omega} f(x,\omega)d\mu(\omega ). \end{align} I think the RHS exists for almost every $x$ because LHS does, which is the second claim of Theorem 3. I am not sure if I can immediately say $\int_\Omega f(x,\omega)d\omega$ is AC in $x$ , which is the first claim of 3., Theorem 3.","Regarding the Theorem 3 from here (or pdf ver. ). Let be an open subset of , and be a measure space. Suppose that a function satisfies the following conditions: is a measurable function of and jointly, and is integrable over ,  for almost all held fixed. For almost all , is an absolutely continuous function of . (This guarantees that exists almost everywhere.) is ""locally integrable"" --- that is, for all compact intervals : Then, is an absolutely continuous function of , and for almost every , its derivatives exists and is given by Question 1: I do not see how to show is absolutely continuous in . Question 2: I also wonder where I can find this result preferably in a book or a paper etc. ==================================================== Here is my thought: Notations basically follow the plametmath note by Steven Cheng (the link above). Let be the measure space and . For -almost all , we assumed that is an absolutely continuous function in the first variable. Hence for a.e.- we have Letting and by virtue of Lebesgue differentiation theorem for the indefinite integral, for a.e. we have where the last equality is ok from the assumption . Adding to the above yields I think the RHS exists for almost every because LHS does, which is the second claim of Theorem 3. I am not sure if I can immediately say is AC in , which is the first claim of 3., Theorem 3.","X \mathbb{R} \Omega f\colon X\times\Omega\to \mathbb{R} f(x,\omega) x \omega \omega x\in X \omega\in\Omega f(x,\omega) x \frac{\partial f}{\partial x}(x,\omega) \frac{\partial f}{\partial x}(x,\omega) [a,b]\subset X 
\int_a^b\!\!\int_{\Omega}\left| \frac{\partial f}{\partial x}(x,\omega) \right| d\omega dx<\infty.
 \int_\Omega f(x,\omega)d\omega x x\in X 
\frac{d}{dx}\int_{\Omega}f(x,\omega)d\omega
=
\int_{\Omega}\frac{d}{dx}f(x,\omega)d\omega.
 \int_\Omega f(x,\omega)d\omega x (\Omega,\mu) (a,b):=X\subset\mathbb{R} \mu \omega\in \Omega f(\cdot,\omega) x \begin{align}
f(x,\omega)&=f(a,\omega)+\int_{a}^x\frac{\partial}{\partial y}f(y,\omega)\,dy
\end{align} G(x):=\int_{a}^x\int_{\Omega}\frac{\partial}{\partial y}f(y,\omega)\,
d\mu(\omega ){d}y g(x):=\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)\,
d\mu(\omega ) x \begin{align}
\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)d\mu(\omega )
&=g(x)=
\frac{\partial}{\partial x}G(x)
=
\frac{\partial}{\partial x}\int_{\Omega}\int_{a}^x
\frac{\partial}{\partial y}f(y,\omega)\,
dyd\mu(\omega ).
\end{align} 
\int_{\Omega}\int_{a}^x\left|\frac{\partial}{\partial y}
	f(y,\omega)\right|
\,dyd\mu(\omega )
	<\infty
 0=\frac{d}{d  t}\int_{\Omega}f(a,\omega)d\mu(\omega ) \begin{align}
\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)d\mu(\omega )
&=\frac{\partial}{\partial x}\left(\int_{\Omega}\left\{
\int_{a}^x
\frac{\partial}{\partial y}f(y,\omega)\,
dy+f(a,\omega)\right\}d\mu(\omega )
\right)\\
&=\frac{\partial}{\partial x}\int_{\Omega}
f(x,\omega)d\mu(\omega ).
\end{align} x \int_\Omega f(x,\omega)d\omega x","['real-analysis', 'measure-theory', 'reference-request', 'solution-verification', 'lebesgue-measure']"
80,Rudin's Rank theorem,Rudin's Rank theorem,,"Rudin states the following: 9.32 Theorem: Suppose $m,n,$ are nonnegative integers, $m\geq r,n\geq r$, $F$ is a $C^1$ mapping of an open set $E\subset R^n$ into $R^m$, and $F'(x)$ has rank $r$ for every $x\in E$. Fix $a\in E$, put $A=F'(a)$, Let $Y_1$ be the range of A, and let $P$ be the projection in $R^m$ whose range if $Y_1$. Let $Y_2$ be the null space of $P$. Then there are open sets $U$ and $V$ in $R^n$, with $a\in U,U\subset E$ and there is a 1-1 mapping $H$ of $V$ onto $U$(whose inverse is also of class $C^1$) such that (66) $F(H(x))=Ax+\phi(Ax)$ $(x\in V)$ where $\phi$ is a $C^1$ mapping of open set $A(V)\subset Y_1$ into $Y_2$. So basically, I got the intuition (from his remarks after the proof) that $F$ can basically be projected in a one-to-one manner in some neighborhood of $a$ and that this projection determines $F$. However, I don't think my intuition fully got it. The problem however is the following, why use $H$? What's the point of introducing something this absurd? Can anyone please explain? Thanks. Edit: I just thought that maybe this is because $P$ is arbitrary and we need to make a certain change of coordinates to accomodate for that, but I don't see why.","Rudin states the following: 9.32 Theorem: Suppose $m,n,$ are nonnegative integers, $m\geq r,n\geq r$, $F$ is a $C^1$ mapping of an open set $E\subset R^n$ into $R^m$, and $F'(x)$ has rank $r$ for every $x\in E$. Fix $a\in E$, put $A=F'(a)$, Let $Y_1$ be the range of A, and let $P$ be the projection in $R^m$ whose range if $Y_1$. Let $Y_2$ be the null space of $P$. Then there are open sets $U$ and $V$ in $R^n$, with $a\in U,U\subset E$ and there is a 1-1 mapping $H$ of $V$ onto $U$(whose inverse is also of class $C^1$) such that (66) $F(H(x))=Ax+\phi(Ax)$ $(x\in V)$ where $\phi$ is a $C^1$ mapping of open set $A(V)\subset Y_1$ into $Y_2$. So basically, I got the intuition (from his remarks after the proof) that $F$ can basically be projected in a one-to-one manner in some neighborhood of $a$ and that this projection determines $F$. However, I don't think my intuition fully got it. The problem however is the following, why use $H$? What's the point of introducing something this absurd? Can anyone please explain? Thanks. Edit: I just thought that maybe this is because $P$ is arbitrary and we need to make a certain change of coordinates to accomodate for that, but I don't see why.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
81,Definition of the Limit of a Function for the Extended Reals,Definition of the Limit of a Function for the Extended Reals,,"Definition 4.33 of Rudin's Principles of Real Analysis : Let $f$ be a real function defined on $E \subset R$. We say that $f(t) \rightarrow A$ as $t \rightarrow x$ where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$, $t \neq x$. Rudin then goes on to say this definition coincides with the epsilon-delta definition for when $A$ and $x$ are confined to $\mathbb{R}$. However for the epsilon-delta definition $x$ must be a limit point of $E$, whereas for Definition 4.33 this need not be the case. For example: Let $E \equiv \{1\}$ and $f(1) \equiv 5$. Then $f(t) \rightarrow 5$ as $t \rightarrow 2$ since, for any neighborhood $U$ of $5$, $V \equiv (0, 4)$ is a neighborhood of $2$ for which $V \cap E = \{1\} \neq \emptyset$ and $f(1) = 5 \in U$. Clearly $x = 2$ is not a limit point of $E = \{1\}$. It seems like this confusion could be avoided if, like in the epsilon-delta definition, Definition 4.33 required $x$ to be a limit point of $E$. My question is: Is there a reason $x$ is not required to be a limit point in definition 4.33? Edit: My feeling is that the definition is only useful if $x$ is a limit point of $E$ and so there is no point in stipulating it. Therefore when Rudin states the definitions coincide (for real values) he implicitly considers $x$ a limit point of $E$.","Definition 4.33 of Rudin's Principles of Real Analysis : Let $f$ be a real function defined on $E \subset R$. We say that $f(t) \rightarrow A$ as $t \rightarrow x$ where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t) \in U$ for all $t \in V \cap E$, $t \neq x$. Rudin then goes on to say this definition coincides with the epsilon-delta definition for when $A$ and $x$ are confined to $\mathbb{R}$. However for the epsilon-delta definition $x$ must be a limit point of $E$, whereas for Definition 4.33 this need not be the case. For example: Let $E \equiv \{1\}$ and $f(1) \equiv 5$. Then $f(t) \rightarrow 5$ as $t \rightarrow 2$ since, for any neighborhood $U$ of $5$, $V \equiv (0, 4)$ is a neighborhood of $2$ for which $V \cap E = \{1\} \neq \emptyset$ and $f(1) = 5 \in U$. Clearly $x = 2$ is not a limit point of $E = \{1\}$. It seems like this confusion could be avoided if, like in the epsilon-delta definition, Definition 4.33 required $x$ to be a limit point of $E$. My question is: Is there a reason $x$ is not required to be a limit point in definition 4.33? Edit: My feeling is that the definition is only useful if $x$ is a limit point of $E$ and so there is no point in stipulating it. Therefore when Rudin states the definitions coincide (for real values) he implicitly considers $x$ a limit point of $E$.",,"['real-analysis', 'limits']"
82,"If any integer to the power of $x$ is integer, must $x$ be integer? [duplicate]","If any integer to the power of  is integer, must  be integer? [duplicate]",x x,"This question already has an answer here : If $n^c\in\mathbb N$ for every $n\in\mathbb N$, then $c$ is a non-negative integer? (1 answer) Closed 4 years ago . My apologies if this has been asked already, I've searched but couldn't find it... Let $x$ such that for every $y \in N$, $y^x$ is an integer. Does that necessarily mean that $x$ is an integer?","This question already has an answer here : If $n^c\in\mathbb N$ for every $n\in\mathbb N$, then $c$ is a non-negative integer? (1 answer) Closed 4 years ago . My apologies if this has been asked already, I've searched but couldn't find it... Let $x$ such that for every $y \in N$, $y^x$ is an integer. Does that necessarily mean that $x$ is an integer?",,['exponentiation']
83,Difficult Fourier transform exercise,Difficult Fourier transform exercise,,"I am currently dealing with a problem in functional analysis where I want to show the following. Let $\phi_{k+1}(x):=\sqrt{2} \sum_{n \in \mathbb{Z}} h(n) \phi_k(2x-n)$ and $\phi_0= \chi_{[0,1]},$ if we define $a_k(n):=\langle \phi_k(.),\phi_k(.-n) \rangle$ and take the operator $$P:L^2(\mathbb{R}) \rightarrow L^2(\mathbb{R})$$ such that $$P\hat{f}(\xi) = \frac{1}{2} \left( |\hat{h}(\frac{\xi}{2})|^2 \hat{f}(\frac{\xi}{2})+|\hat{h}(\frac{\xi}{2}+\pi)|^2 \hat{f}(\frac{\xi}{2}+\pi) \right) $$ then this one also satisfies $\hat{a}_{k+1}= P \hat{a}_k$. Here I give a few thoughts about this, but maybe they are just misleading and the exercise is much simpler: Now I tried to show this, but somehow I could not get anywhere. $h$ is taken in such a way that $\hat{h} \in L^{\infty}$ where $\hat{h}(\xi):= \sum_{n \in \mathbb{Z}} h(n)e^{-in \xi}$ and $\phi_{k+1},a_{k+1} \in L^2$ again. Otherwise, $h$ is arbitrary. Furthermore $\hat{a_{k}}:=\sum_{n \in \mathbb{Z}} \langle \phi(.),\phi(.-n)\rangle e^{-in \xi}.$ It is useful to see ( I guess) that $$\hat{\phi}_{k+1}(\xi) = \frac{1}{\sqrt{2}}\hat{h}(\frac{\xi}{2}) \phi_{k}(\frac{\xi}{2}) = \Pi_{i=1}^{k+1} \left( \frac{\hat{h}(2^{-p}\xi)}{\sqrt{2}} \right) \cdot \hat{\phi_0}(\frac{\xi}{2}).$$ Furthermore by Plancherel and the definition of the Fourier transform we get $\langle \phi(.), \phi(.-n) \rangle=  \langle \hat{\phi} , e^{-in(.)} \hat{\phi} \rangle  =\mathcal{F}(|\hat{\phi}|^2)(n).$ $\hat{a_{k+1}}(\xi) = \sum_{n \in \mathbb{Z}} \langle \phi_{k+1}(.), \phi_{k+1}(.-n) \rangle e^{-i n \xi}$ which can be rewritten using Plancherel in the inner-product as $$\hat{a_{k+1}}(\xi) = \frac{1}{2} \sum_{n \in \mathbb{Z}} \langle |\hat{h}(\frac{.}{2})|^2 |\phi_{k}(\frac{.}{2})|^2 ,e^{-in(.)} \rangle e^{-i n \xi}$$ I guess these are all the simple calculations one would do, but somehow this got me nowhere. Probably you need to use that $\hat{\phi}_0(x) = \frac{i(1-e^{ix})}{x}.$ If anything is unclear, please let me know.","I am currently dealing with a problem in functional analysis where I want to show the following. Let $\phi_{k+1}(x):=\sqrt{2} \sum_{n \in \mathbb{Z}} h(n) \phi_k(2x-n)$ and $\phi_0= \chi_{[0,1]},$ if we define $a_k(n):=\langle \phi_k(.),\phi_k(.-n) \rangle$ and take the operator $$P:L^2(\mathbb{R}) \rightarrow L^2(\mathbb{R})$$ such that $$P\hat{f}(\xi) = \frac{1}{2} \left( |\hat{h}(\frac{\xi}{2})|^2 \hat{f}(\frac{\xi}{2})+|\hat{h}(\frac{\xi}{2}+\pi)|^2 \hat{f}(\frac{\xi}{2}+\pi) \right) $$ then this one also satisfies $\hat{a}_{k+1}= P \hat{a}_k$. Here I give a few thoughts about this, but maybe they are just misleading and the exercise is much simpler: Now I tried to show this, but somehow I could not get anywhere. $h$ is taken in such a way that $\hat{h} \in L^{\infty}$ where $\hat{h}(\xi):= \sum_{n \in \mathbb{Z}} h(n)e^{-in \xi}$ and $\phi_{k+1},a_{k+1} \in L^2$ again. Otherwise, $h$ is arbitrary. Furthermore $\hat{a_{k}}:=\sum_{n \in \mathbb{Z}} \langle \phi(.),\phi(.-n)\rangle e^{-in \xi}.$ It is useful to see ( I guess) that $$\hat{\phi}_{k+1}(\xi) = \frac{1}{\sqrt{2}}\hat{h}(\frac{\xi}{2}) \phi_{k}(\frac{\xi}{2}) = \Pi_{i=1}^{k+1} \left( \frac{\hat{h}(2^{-p}\xi)}{\sqrt{2}} \right) \cdot \hat{\phi_0}(\frac{\xi}{2}).$$ Furthermore by Plancherel and the definition of the Fourier transform we get $\langle \phi(.), \phi(.-n) \rangle=  \langle \hat{\phi} , e^{-in(.)} \hat{\phi} \rangle  =\mathcal{F}(|\hat{\phi}|^2)(n).$ $\hat{a_{k+1}}(\xi) = \sum_{n \in \mathbb{Z}} \langle \phi_{k+1}(.), \phi_{k+1}(.-n) \rangle e^{-i n \xi}$ which can be rewritten using Plancherel in the inner-product as $$\hat{a_{k+1}}(\xi) = \frac{1}{2} \sum_{n \in \mathbb{Z}} \langle |\hat{h}(\frac{.}{2})|^2 |\phi_{k}(\frac{.}{2})|^2 ,e^{-in(.)} \rangle e^{-i n \xi}$$ I guess these are all the simple calculations one would do, but somehow this got me nowhere. Probably you need to use that $\hat{\phi}_0(x) = \frac{i(1-e^{ix})}{x}.$ If anything is unclear, please let me know.",,"['real-analysis', 'analysis', 'functional-analysis', 'fourier-analysis', 'fourier-series']"
84,"A sequence of functions that is uniformly continuous, pointwise equicontinuous, but not uniformly equicontinuous when their domain is noncompact","A sequence of functions that is uniformly continuous, pointwise equicontinuous, but not uniformly equicontinuous when their domain is noncompact",,"I'm trying to prove my sequence of functions $(f_n) = \frac{n}{n+1}\cos(x^2)$ on (0,1) is pointwise equicontinuous, uniformly continuous, but not uniformly equicontinuous.  But, I'm having a lot of trouble with showing it's not uniformly equicontinuous. I've tried numericals, but I can't seem to find and $\epsilon$>0 s.t. $\forall \delta >0$, $|x-y|<\delta$ and $|f_n(x) - f_m(y)|< \epsilon$.  And I'm not sure how to start this proof. But I know that $\frac{n}{n+1} \cos(x^2)$ is not uniformly continuous because its derivative is unbounded. Any help would be greatly appreciated.","I'm trying to prove my sequence of functions $(f_n) = \frac{n}{n+1}\cos(x^2)$ on (0,1) is pointwise equicontinuous, uniformly continuous, but not uniformly equicontinuous.  But, I'm having a lot of trouble with showing it's not uniformly equicontinuous. I've tried numericals, but I can't seem to find and $\epsilon$>0 s.t. $\forall \delta >0$, $|x-y|<\delta$ and $|f_n(x) - f_m(y)|< \epsilon$.  And I'm not sure how to start this proof. But I know that $\frac{n}{n+1} \cos(x^2)$ is not uniformly continuous because its derivative is unbounded. Any help would be greatly appreciated.",,"['real-analysis', 'functional-analysis', 'continuity', 'uniform-continuity']"
85,Taylor Formula: Lagrange's remainder vs Cauchy's remainder (and other less known forms),Taylor Formula: Lagrange's remainder vs Cauchy's remainder (and other less known forms),,"While solving problems and exercises, so far I've only used Lagrange's form of the remainder. Indeed, it must be said that many textbooks don't even mention other forms of the remainder for Taylor's formula. So my question is: Could you show some examples of exercises where Cauchy's (or other)   forms of the remainder come in handy for some reason? Could   Lagrange's form be nevertheless applied in such cases to solve the problems? Is there a   rule of thumb to decide which form is better to use in a given   occasion?","While solving problems and exercises, so far I've only used Lagrange's form of the remainder. Indeed, it must be said that many textbooks don't even mention other forms of the remainder for Taylor's formula. So my question is: Could you show some examples of exercises where Cauchy's (or other)   forms of the remainder come in handy for some reason? Could   Lagrange's form be nevertheless applied in such cases to solve the problems? Is there a   rule of thumb to decide which form is better to use in a given   occasion?",,"['calculus', 'real-analysis', 'analysis', 'taylor-expansion', 'approximation']"
86,Fréchet differentiability from Gâteaux differentiability,Fréchet differentiability from Gâteaux differentiability,,"Let $X$ be a Banach space and $\Omega \subset X$ be open. The functional $f$ has a Gâteaux derivative $g \in X'$ at $u \in \Omega$ if, $\forall h\in X,$ $$\lim_{t \rightarrow 0}[f(u+th)-f(u)- \langle g,th \rangle]=0$$ How can I prove the following: If $f$ has a continuous Gâteaux derivative on $\Omega$, then $f \in C^1(\Omega,\mathbb R)$.","Let $X$ be a Banach space and $\Omega \subset X$ be open. The functional $f$ has a Gâteaux derivative $g \in X'$ at $u \in \Omega$ if, $\forall h\in X,$ $$\lim_{t \rightarrow 0}[f(u+th)-f(u)- \langle g,th \rangle]=0$$ How can I prove the following: If $f$ has a continuous Gâteaux derivative on $\Omega$, then $f \in C^1(\Omega,\mathbb R)$.",,"['real-analysis', 'banach-spaces']"
87,Understanding the roots of homomorphism and homeomorphism,Understanding the roots of homomorphism and homeomorphism,,"I understand the formal (i.e. mathematical) definitions of the terms ""homomorphism"" and ""homeomorphism"" as they relate to functions , but I am curious as to the origin of these terms. I don't know Greek (it's all Greek to me), but ""morph"", I believe means ""form"", while both and ""homo"" and ""homeo"" mean two objects are the ""same"" (in some sense). So understanding the distinction between these terms by solely examining the words themselves is somewhat baffling to me. If we talk about a function mapping some set $A$ (endowed with some binary operation ""+"") to some set $B$ (with a binary operation ""*""), then I interpret a ""homomorphism"" to be a function which preserves the ""form"" in the sense that it preserves the structure of the operations, i.e. for all $x,y$ in $A$, $f(x+y) = f(x)*f(y)$. I think of a function which is a homeomorphism as a function which preserves ""form"" in the sense that both $f$ and its inverse preserves the basic fundamental topological properties of sets, i.e. both $f$ and its inverse map open sets to open sets. I am just curious if someone with a better understanding of these terms can shed some light on why I should think of ""homomorphism"" and ""homeomorphism"" as being different in some sense (other than falling back on the fact that they are just definitions) and if so, how one can see this distinction from the words themselves. Thanks, Jack","I understand the formal (i.e. mathematical) definitions of the terms ""homomorphism"" and ""homeomorphism"" as they relate to functions , but I am curious as to the origin of these terms. I don't know Greek (it's all Greek to me), but ""morph"", I believe means ""form"", while both and ""homo"" and ""homeo"" mean two objects are the ""same"" (in some sense). So understanding the distinction between these terms by solely examining the words themselves is somewhat baffling to me. If we talk about a function mapping some set $A$ (endowed with some binary operation ""+"") to some set $B$ (with a binary operation ""*""), then I interpret a ""homomorphism"" to be a function which preserves the ""form"" in the sense that it preserves the structure of the operations, i.e. for all $x,y$ in $A$, $f(x+y) = f(x)*f(y)$. I think of a function which is a homeomorphism as a function which preserves ""form"" in the sense that both $f$ and its inverse preserves the basic fundamental topological properties of sets, i.e. both $f$ and its inverse map open sets to open sets. I am just curious if someone with a better understanding of these terms can shed some light on why I should think of ""homomorphism"" and ""homeomorphism"" as being different in some sense (other than falling back on the fact that they are just definitions) and if so, how one can see this distinction from the words themselves. Thanks, Jack",,"['real-analysis', 'general-topology', 'definition']"
88,Showing range is countable,Showing range is countable,,"Let $f: \mathbb{R} \to \mathbb{R}$.  For every $x \in \mathbb{R}$, there exists $\delta$, for every $y \in N(x, \delta)$ ($N$ stands for neighborhood) $f(y) \geq f(x)$. Show that the range of $f$ is countable.","Let $f: \mathbb{R} \to \mathbb{R}$.  For every $x \in \mathbb{R}$, there exists $\delta$, for every $y \in N(x, \delta)$ ($N$ stands for neighborhood) $f(y) \geq f(x)$. Show that the range of $f$ is countable.",,['real-analysis']
89,Invariant functions on the space of finite sequences of reals,Invariant functions on the space of finite sequences of reals,,"Let $S$ be a space of all finite sequences of real numbers (we don't endow it with metric or topology in general). Before asking the main question, some notation. 1. For each $\mathbf s\in S$ we define $|\mathbf s|$ to be the length of $\mathbf s$. 2. Let $\mathbf s',\mathbf s''\in S$ be equivalent ($\mathbf s'\sim \mathbf s''$) if $|\mathbf s'| = |\mathbf s''|$ and $\mathbf s''$ can be obtained by $\mathbf s'$ by only permuting elements of $\mathbf s'$. Say, $\{1,2,3\}\sim\{2,3,1\}$. 3. Define an addition of sequences in the following way: $$ \mathbf s = \mathbf s'+\mathbf s'' = \{s'_1,...,s'_{k},s''_1,...,s''_{l}\}. $$ where $k=|\mathbf s'|$ and $l = |\mathbf s''|$ and multiplication by scalar: $$ \alpha\mathbf s = \{\alpha s_1,...,\alpha s_{|s|}\}. $$ 4. For $n\in\mathbb N$ put $\mathbf 1_n = \{1,1,...,1\}$ such that $|\mathbf 1_n|=n$. In probability theory often the following function $L\to \mathbb R$ are used:  $$ \min \mathbf s = \min_{1\leq i\leq |s|}s_i, $$ $$ \max \mathbf s = \max_{1\leq i\leq |s|}s_i, $$ $$ \overline{\mathbf s} = \frac{1}{|\mathbf s|}\sum s_i. $$ All these functions have a nice invariance property: $f(\mathbf s) = f\left(\mathbf s+f(\mathbf s)\mathbf 1_n\right)$ for all $n\in\mathbb N$. I am interested if this class of functions was already discussed in details? Some thoughts: a) if we put $S$ to be a class of real-valued continuous maps with compact domains from $\mathbb R$ then we can extend these three functions to admit the same property. Moreover, the outcome for each of these functions will be in an image of $\mathbf s$. b) all these three functions are constant on class of equivalence $[\mathbf s]$ for any $\mathbf s\in S$. c) For sure, $\overline{\mathbf s}$ is an expectation with respect to class of uniform probability measures each of them defined over a finite set. So, we can extend it to a wider class of measures. On the other hand, then we will lose the property b) . d) Constant function of course also admit these property, however is not of too much interest.","Let $S$ be a space of all finite sequences of real numbers (we don't endow it with metric or topology in general). Before asking the main question, some notation. 1. For each $\mathbf s\in S$ we define $|\mathbf s|$ to be the length of $\mathbf s$. 2. Let $\mathbf s',\mathbf s''\in S$ be equivalent ($\mathbf s'\sim \mathbf s''$) if $|\mathbf s'| = |\mathbf s''|$ and $\mathbf s''$ can be obtained by $\mathbf s'$ by only permuting elements of $\mathbf s'$. Say, $\{1,2,3\}\sim\{2,3,1\}$. 3. Define an addition of sequences in the following way: $$ \mathbf s = \mathbf s'+\mathbf s'' = \{s'_1,...,s'_{k},s''_1,...,s''_{l}\}. $$ where $k=|\mathbf s'|$ and $l = |\mathbf s''|$ and multiplication by scalar: $$ \alpha\mathbf s = \{\alpha s_1,...,\alpha s_{|s|}\}. $$ 4. For $n\in\mathbb N$ put $\mathbf 1_n = \{1,1,...,1\}$ such that $|\mathbf 1_n|=n$. In probability theory often the following function $L\to \mathbb R$ are used:  $$ \min \mathbf s = \min_{1\leq i\leq |s|}s_i, $$ $$ \max \mathbf s = \max_{1\leq i\leq |s|}s_i, $$ $$ \overline{\mathbf s} = \frac{1}{|\mathbf s|}\sum s_i. $$ All these functions have a nice invariance property: $f(\mathbf s) = f\left(\mathbf s+f(\mathbf s)\mathbf 1_n\right)$ for all $n\in\mathbb N$. I am interested if this class of functions was already discussed in details? Some thoughts: a) if we put $S$ to be a class of real-valued continuous maps with compact domains from $\mathbb R$ then we can extend these three functions to admit the same property. Moreover, the outcome for each of these functions will be in an image of $\mathbf s$. b) all these three functions are constant on class of equivalence $[\mathbf s]$ for any $\mathbf s\in S$. c) For sure, $\overline{\mathbf s}$ is an expectation with respect to class of uniform probability measures each of them defined over a finite set. So, we can extend it to a wider class of measures. On the other hand, then we will lose the property b) . d) Constant function of course also admit these property, however is not of too much interest.",,"['real-analysis', 'sequences-and-series']"
90,Using Jensen's inequality to prove another inequality?,Using Jensen's inequality to prove another inequality?,,"Suppose $u(\cdot)$ and $v(\cdot)$ are two differentiable, strictly increasing, and strictly concave real functions. Specifically, $v(\cdot)$ is ""more concave"" than $u(\cdot)$ in the sense that there exists an increasing and strictly concave function $\phi(\cdot)$ such that $v(x)=\phi(u(x))$ at all $x$. It is also equivalent to \begin{equation} \frac{v''(x)}{v'(x)}<\frac{u''(x)}{u'(x)} \textrm{ for any }x\,. \end{equation} Let $p_i\in(0,1), \sum_{i\in I}p_i=1$ be probabilities and $|I|>2$. Let $x_i$ and $y_i$ be strictly positive for all $i\in I$. Assume \begin{equation} \sum_{i\in I}p_ix_i<\sum_{i\in I}p_iy_i, \end{equation} and \begin{equation} \sum_{i\in I}p_iu(x_i)=\sum_{i\in I}p_iu(y_i). \end{equation} Conjecture: \begin{equation} \sum_{i\in I}p_iv(x_i)>\sum_{i\in I}p_iv(y_i). \end{equation} I believe this is right (after trying many numerical examples) and I think a clever use of Jensen's inequality (or its variants) will do this. But I'm stuck on doing it formally. Any hints/thoughts on providing a formal proof? Remark: this is related to my other post: Proving an inequality of the expectation of concave functions? Update: after some more attempts, I believe some techniques in convex analysis would be helpful. Geometrically, the middle equation represents a hyperplane in the $R^{|I|}$ space, and the desired result (very roughly) says that a concave transformation of that hyperplane should be separated from a convex transformation of it. To be clear, I wasn't saying the conjecture should be generally true. Any thoughts on finding any sufficient conditions to make it work would be very helpful.","Suppose $u(\cdot)$ and $v(\cdot)$ are two differentiable, strictly increasing, and strictly concave real functions. Specifically, $v(\cdot)$ is ""more concave"" than $u(\cdot)$ in the sense that there exists an increasing and strictly concave function $\phi(\cdot)$ such that $v(x)=\phi(u(x))$ at all $x$. It is also equivalent to \begin{equation} \frac{v''(x)}{v'(x)}<\frac{u''(x)}{u'(x)} \textrm{ for any }x\,. \end{equation} Let $p_i\in(0,1), \sum_{i\in I}p_i=1$ be probabilities and $|I|>2$. Let $x_i$ and $y_i$ be strictly positive for all $i\in I$. Assume \begin{equation} \sum_{i\in I}p_ix_i<\sum_{i\in I}p_iy_i, \end{equation} and \begin{equation} \sum_{i\in I}p_iu(x_i)=\sum_{i\in I}p_iu(y_i). \end{equation} Conjecture: \begin{equation} \sum_{i\in I}p_iv(x_i)>\sum_{i\in I}p_iv(y_i). \end{equation} I believe this is right (after trying many numerical examples) and I think a clever use of Jensen's inequality (or its variants) will do this. But I'm stuck on doing it formally. Any hints/thoughts on providing a formal proof? Remark: this is related to my other post: Proving an inequality of the expectation of concave functions? Update: after some more attempts, I believe some techniques in convex analysis would be helpful. Geometrically, the middle equation represents a hyperplane in the $R^{|I|}$ space, and the desired result (very roughly) says that a concave transformation of that hyperplane should be separated from a convex transformation of it. To be clear, I wasn't saying the conjecture should be generally true. Any thoughts on finding any sufficient conditions to make it work would be very helpful.",,"['real-analysis', 'probability', 'inequality', 'convex-analysis', 'expectation']"
91,Directional derivative of the determinant,Directional derivative of the determinant,,"Please help me find the mistake in my derivation: Let $f:M_{n,n}(\mathbb{R}) \to \mathbb{R}$ be the determinant function, $f(A)=det(A)$. Let $p_A(x)$ denote the charecteristic polynomial of $A$. Computing the directional derivative we get: $$\nabla_If(A)=lim_{x\to0}\frac{det(A+Ix)-det(A)}{x}=lim_{x\to0}\frac{-p_A(-x)-(-p_A(0))}{x}=p_A'(0)$$ According to notes of a course i'm taking: $\nabla_If(A)=tr(A)$.  Where is my mistake? EDIT: I just looked again and i got it backwards! It says that the derivative of the determinant at $I$ along any matrix is the trace of the matrix. $$Df_I(A)=lim_{x\to0}\frac{det(I+Ax)-1}{x}=det(A)lim_{x\to0}\frac{det(A^{-1}+xI)-det(A^{-1})}{x}=det(A)p'_{A^{-1}}(0)$$ The last expression is the trace since $p_{A^{-1}}(x)=\frac{(-x)^n}{det(A)}p_A(\frac{1}{x})$ I would delete if I could...","Please help me find the mistake in my derivation: Let $f:M_{n,n}(\mathbb{R}) \to \mathbb{R}$ be the determinant function, $f(A)=det(A)$. Let $p_A(x)$ denote the charecteristic polynomial of $A$. Computing the directional derivative we get: $$\nabla_If(A)=lim_{x\to0}\frac{det(A+Ix)-det(A)}{x}=lim_{x\to0}\frac{-p_A(-x)-(-p_A(0))}{x}=p_A'(0)$$ According to notes of a course i'm taking: $\nabla_If(A)=tr(A)$.  Where is my mistake? EDIT: I just looked again and i got it backwards! It says that the derivative of the determinant at $I$ along any matrix is the trace of the matrix. $$Df_I(A)=lim_{x\to0}\frac{det(I+Ax)-1}{x}=det(A)lim_{x\to0}\frac{det(A^{-1}+xI)-det(A^{-1})}{x}=det(A)p'_{A^{-1}}(0)$$ The last expression is the trace since $p_{A^{-1}}(x)=\frac{(-x)^n}{det(A)}p_A(\frac{1}{x})$ I would delete if I could...",,"['real-analysis', 'matrices', 'lie-groups', 'determinant']"
92,Why is $\sqrt{2\sqrt{2\sqrt{2\cdots}}} = 2$? [duplicate],Why is ? [duplicate],\sqrt{2\sqrt{2\sqrt{2\cdots}}} = 2,"This question already has answers here : How I can prove that the sequence $\sqrt{2} , \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}$ converges to 2? (8 answers) Closed 7 years ago . Why is $\sqrt{2\sqrt{2\sqrt{2\sqrt{2\sqrt{2\sqrt{2\cdots}}}}}}$ equal to 2? Does this work for other numbers?","This question already has answers here : How I can prove that the sequence $\sqrt{2} , \sqrt{2\sqrt{2}}, \sqrt{2\sqrt{2\sqrt{2}}}$ converges to 2? (8 answers) Closed 7 years ago . Why is $\sqrt{2\sqrt{2\sqrt{2\sqrt{2\sqrt{2\sqrt{2\cdots}}}}}}$ equal to 2? Does this work for other numbers?",,"['real-analysis', 'limits']"
93,Shouldn't the harmonic series converge?,Shouldn't the harmonic series converge?,,"If a sequence converges in a metric space, it is Cauchy, and in $\mathbb{R}^k$ every Cauchy sequence converges. Therefore, in $\mathbb{R}^k$ a sequence converges iff it is Cauchy. Let $\{s_n\}$ be a sequence in $\mathbb{R}$ where each $s_n=\sum_{k=1}^na_k$. Therefore, by the above, every series converges iff  $$\left | \sum_{k=m}^n a_k\right| <\epsilon$$ For a given $\epsilon >0$ and an integer $N$ such that $N\le m\le n$. If $n=m$ then the statement reduces to: A series converges if and only if $$|a_n| < \epsilon $$ For a given $\epsilon >0$ and an integer $N$ such that $N\le n$. This clearly cannot be (e.g Harmonic series). When does the equivalence become an implication.","If a sequence converges in a metric space, it is Cauchy, and in $\mathbb{R}^k$ every Cauchy sequence converges. Therefore, in $\mathbb{R}^k$ a sequence converges iff it is Cauchy. Let $\{s_n\}$ be a sequence in $\mathbb{R}$ where each $s_n=\sum_{k=1}^na_k$. Therefore, by the above, every series converges iff  $$\left | \sum_{k=m}^n a_k\right| <\epsilon$$ For a given $\epsilon >0$ and an integer $N$ such that $N\le m\le n$. If $n=m$ then the statement reduces to: A series converges if and only if $$|a_n| < \epsilon $$ For a given $\epsilon >0$ and an integer $N$ such that $N\le n$. This clearly cannot be (e.g Harmonic series). When does the equivalence become an implication.",,"['real-analysis', 'metric-spaces', 'proof-writing']"
94,Construct a continuous real valued function which takes zero on integers and such that image of function is not closed.,Construct a continuous real valued function which takes zero on integers and such that image of function is not closed.,,I am trying to construct a continuous real valued function $f:\mathbb{R}\to \mathbb{R}$ which takes zero on all integer points(that is $f(k)=0$ for all $k\in \mathbb{Z}$ ) and Image(f) is not closed in $\mathbb{R}$ I had $f(x)=\sin(\pi x) $ in mind. But image of $f(x)$ is closed. I have a feeling that we can use some clever idea to modify this function such that it satisfy our given condition.,I am trying to construct a continuous real valued function which takes zero on all integer points(that is for all ) and Image(f) is not closed in I had in mind. But image of is closed. I have a feeling that we can use some clever idea to modify this function such that it satisfy our given condition.,f:\mathbb{R}\to \mathbb{R} f(k)=0 k\in \mathbb{Z} \mathbb{R} f(x)=\sin(\pi x)  f(x),"['real-analysis', 'general-topology', 'functions', 'continuity']"
95,Evaluating: $\lim_{n\to\infty} \int_{0}^{\pi} e^x\cos(nx)\space dx$,Evaluating:,\lim_{n\to\infty} \int_{0}^{\pi} e^x\cos(nx)\space dx,"Evaluate the limit: $$\lim_{n\to\infty} \int_{0}^{\pi} e^x\cos(nx)\space dx$$ W|A tells that the limit is $0$, but i'm not sure why is that result or if this is the correct result.","Evaluate the limit: $$\lim_{n\to\infty} \int_{0}^{\pi} e^x\cos(nx)\space dx$$ W|A tells that the limit is $0$, but i'm not sure why is that result or if this is the correct result.",,"['real-analysis', 'integration', 'limits', 'definite-integrals']"
96,Limit at infinity of cubic roots and square roots without using conjugate $\lim_{x \to \infty} \frac{\sqrt[3]{x+2}}{\sqrt{x+3}}$,Limit at infinity of cubic roots and square roots without using conjugate,\lim_{x \to \infty} \frac{\sqrt[3]{x+2}}{\sqrt{x+3}},"$$\lim_{x \to \infty} \frac{\sqrt[3]{x+2}}{\sqrt{x+3}}   $$ How would you proceed to find this limit, by eyeballing I would guess it foes to zero since the numerator has a smaller power than the denominator, normaly I would use the binomial theorem if I had something like $$\lim_{x \to \infty} \frac{\sqrt[3]{x+2}-1}{\sqrt{x+3}-1}   $$ But here I don't know how to find the limit since I can't really use the binomial theorem.","$$\lim_{x \to \infty} \frac{\sqrt[3]{x+2}}{\sqrt{x+3}}   $$ How would you proceed to find this limit, by eyeballing I would guess it foes to zero since the numerator has a smaller power than the denominator, normaly I would use the binomial theorem if I had something like $$\lim_{x \to \infty} \frac{\sqrt[3]{x+2}-1}{\sqrt{x+3}-1}   $$ But here I don't know how to find the limit since I can't really use the binomial theorem.",,"['real-analysis', 'limits', 'radicals', 'limits-without-lhopital']"
97,Is The Union of Intervals an Interval or not?,Is The Union of Intervals an Interval or not?,,"I have a question on union of intervals. My teacher says the union of intervals is not an interval. Is this always true? I mean $[0,2] \cup [4,5]$ is not an interval, because $[0,2] \cap [4,5]= \varnothing ,$ but what about $ (0,8) \cup (7,9).$ I think this is an interval because $(0,8) \cup (7,9)=(0,9)$.","I have a question on union of intervals. My teacher says the union of intervals is not an interval. Is this always true? I mean $[0,2] \cup [4,5]$ is not an interval, because $[0,2] \cap [4,5]= \varnothing ,$ but what about $ (0,8) \cup (7,9).$ I think this is an interval because $(0,8) \cup (7,9)=(0,9)$.",,['real-analysis']
98,Example of two functions that are equal almost everywhere?,Example of two functions that are equal almost everywhere?,,"We shall say that two functions $f$ and $g$ defined on a set $E$ are equal almost everywhere, and write $f(x)=g(x)$ a.e $x\in E$, if the set $\{x\in E: f(x)\neq g(x)\}$ has measure zero. I just can't wrap my brain around the fact that such functions exist! Certainly, we can take the cantor set which has measure zero but how to pick $f$ and $g$? Is there an example that I am not aware of ?","We shall say that two functions $f$ and $g$ defined on a set $E$ are equal almost everywhere, and write $f(x)=g(x)$ a.e $x\in E$, if the set $\{x\in E: f(x)\neq g(x)\}$ has measure zero. I just can't wrap my brain around the fact that such functions exist! Certainly, we can take the cantor set which has measure zero but how to pick $f$ and $g$? Is there an example that I am not aware of ?",,"['real-analysis', 'analysis', 'measure-theory']"
99,"Which field property enables us to multiply on both sides by the same value, while preserving equality? [duplicate]","Which field property enables us to multiply on both sides by the same value, while preserving equality? [duplicate]",,"This question already has answers here : Is there a law that you can add or multiply to both sides of an equation? [duplicate] (9 answers) Closed 4 years ago . I am currently reading through Rudin's Principles of Mathematical Analysis and I am learning about fields and their properties. Note that this is the initial chapter - I am just starting off. I was wondering which field property enables us to multiply on both sides of an equation and still preserve equality. There is a very clear proposition stated in the book that gives me this for inequalities: $$ \text{If} \ \ x > 0 \ \ \text{and} \ \ y < z \ \ \text{then} \ \ xy<xz. $$ However, the only proposition that seems useful for this in the case of equalities, is stated as an implication and not an equivalence: $$ \text{If} \ x\not= 0 \ \ \text{and} \ \ xy=xz \ \ \text{then} \ \ y=z. $$ Any help would be much appreciated.","This question already has answers here : Is there a law that you can add or multiply to both sides of an equation? [duplicate] (9 answers) Closed 4 years ago . I am currently reading through Rudin's Principles of Mathematical Analysis and I am learning about fields and their properties. Note that this is the initial chapter - I am just starting off. I was wondering which field property enables us to multiply on both sides of an equation and still preserve equality. There is a very clear proposition stated in the book that gives me this for inequalities: However, the only proposition that seems useful for this in the case of equalities, is stated as an implication and not an equivalence: Any help would be much appreciated.","
\text{If} \ \ x > 0 \ \ \text{and} \ \ y < z \ \ \text{then} \ \ xy<xz.
 
\text{If} \ x\not= 0 \ \ \text{and} \ \ xy=xz \ \ \text{then} \ \ y=z.
","['real-analysis', 'field-theory']"
