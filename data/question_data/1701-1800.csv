,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Limit of $(x_n)$ with $0<x_1<1$ and $x_{n + 1} = x_n - x_n^{n + 1}$,Limit of  with  and,(x_n) 0<x_1<1 x_{n + 1} = x_n - x_n^{n + 1},Let $0 < x_1 < 1$ and $x_{n + 1} = x_n - x_n^{n + 1}$ for $n \geqslant 1$. Prove that the limit exists and find the limit in terms of $x_1$. I have proved the existence but cannot manage the other part. Thanks for any help.,Let $0 < x_1 < 1$ and $x_{n + 1} = x_n - x_n^{n + 1}$ for $n \geqslant 1$. Prove that the limit exists and find the limit in terms of $x_1$. I have proved the existence but cannot manage the other part. Thanks for any help.,,['real-analysis']
1,Sum inequality: $\sum_{k=1}^n \frac{\sin k}{k} \le \pi-1$ [duplicate],Sum inequality:  [duplicate],\sum_{k=1}^n \frac{\sin k}{k} \le \pi-1,"This question already has answers here : Proving that the sequence $F_{n}(x)=\sum\limits_{k=1}^{n} \frac{\sin{kx}}{k}$ is boundedly convergent on $\mathbb{R}$ (2 answers) Closed 6 years ago . I'm interested in finding an elementary proof for the following sum inequality: $$\sum_{k=1}^n \frac{\sin k}{k} \le \pi-1$$ If this inequality is easy to prove, then one may easily prove that the sum is bounded.","This question already has answers here : Proving that the sequence $F_{n}(x)=\sum\limits_{k=1}^{n} \frac{\sin{kx}}{k}$ is boundedly convergent on $\mathbb{R}$ (2 answers) Closed 6 years ago . I'm interested in finding an elementary proof for the following sum inequality: $$\sum_{k=1}^n \frac{\sin k}{k} \le \pi-1$$ If this inequality is easy to prove, then one may easily prove that the sum is bounded.",,"['real-analysis', 'sequences-and-series', 'inequality']"
2,Theorem of Steinhaus,Theorem of Steinhaus,,"The Steinhaus theorem says that if a set $A \subset \mathbb R^n$ is of positive inner Lebesgue measure then $\operatorname{int}{(A+A)} \neq \emptyset$. Is it true that also $\operatorname{int}{(tA+(1-t)A)} \neq \emptyset$ for $t \in(0,1)$? It is clear for $t=\frac{1}{2}$? But in general? Thanks.","The Steinhaus theorem says that if a set $A \subset \mathbb R^n$ is of positive inner Lebesgue measure then $\operatorname{int}{(A+A)} \neq \emptyset$. Is it true that also $\operatorname{int}{(tA+(1-t)A)} \neq \emptyset$ for $t \in(0,1)$? It is clear for $t=\frac{1}{2}$? But in general? Thanks.",,"['real-analysis', 'measure-theory', 'harmonic-analysis', 'locally-compact-groups']"
3,is the inverse of a absolutely continuous function with almost everywhere positive derivation absolutely continuous?,is the inverse of a absolutely continuous function with almost everywhere positive derivation absolutely continuous?,,"suppose $f$ is an absolutely continuous on $[0,1]$,that almost everywhere $f'>0$. is the inverse of $f$ necessarily absolutely continuous on $[f(0),f(1)]$? thank you very much!","suppose $f$ is an absolutely continuous on $[0,1]$,that almost everywhere $f'>0$. is the inverse of $f$ necessarily absolutely continuous on $[f(0),f(1)]$? thank you very much!",,"['real-analysis', 'functional-analysis', 'continuity']"
4,Continuous surjections onto $\mathbb{R}$,Continuous surjections onto,\mathbb{R},"I have two questions about continuous functions: Suppose $X \subseteq \mathbb{R}$ and $X$ has same cardinality as $\mathbb{R}$. Can we find a continuous function from $X$ onto $\mathbb{R}$? Suppose $X \cup Y = \mathbb{R}$. Can we find a continuous function from one of $X, Y$ onto $\mathbb{R}$?","I have two questions about continuous functions: Suppose $X \subseteq \mathbb{R}$ and $X$ has same cardinality as $\mathbb{R}$. Can we find a continuous function from $X$ onto $\mathbb{R}$? Suppose $X \cup Y = \mathbb{R}$. Can we find a continuous function from one of $X, Y$ onto $\mathbb{R}$?",,"['real-analysis', 'general-topology', 'descriptive-set-theory']"
5,user friendly proof of fundamental theorem of calculus,user friendly proof of fundamental theorem of calculus,,"Silly question. Can someone show me a nice easy to follow proof on the fundamental theorem of calculus. More specifically, $\displaystyle\int_{a}^{b}f(x)dx = F(b) - F(a)$ I know that by just googling fundamental theorem of calculus, one can get all sorts of answers, but for some odd reason I have a hard time following the arguments.","Silly question. Can someone show me a nice easy to follow proof on the fundamental theorem of calculus. More specifically, $\displaystyle\int_{a}^{b}f(x)dx = F(b) - F(a)$ I know that by just googling fundamental theorem of calculus, one can get all sorts of answers, but for some odd reason I have a hard time following the arguments.",,"['calculus', 'real-analysis']"
6,Real polynomial in two variables,Real polynomial in two variables,,"I have problems proving the following result: Each $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ such that $\forall a,b \in \mathbb{R} \ : \ f_a(y) := f(a,y), \ f_b(x) := f(x,b) $ are polynomials is a polynomial with two variables. If I consider $f$ as a function of $x$, then its derivative  $f'(x) = \frac{\partial f}{\partial b}(x)$. Similarly if we treat $f$ as a function of $y$. I assume that $f_a(y) = \frac{\partial f}{\partial a} (y) $ and $f_b(x)=\frac{\partial f}{\partial b}(x)$. But I am not sure if we can assume that, because  the degree of the derivative should be smaller than the degree of the original function (and it isn't). Actually, I'm not even sure if what I'm trying to prove is true, because in the original formulation of the problems there is written $f_a(y) := (a,y), \ f_b(x):=(x,b)$. But that didn't make sense. Could you help me here? Thank you.","I have problems proving the following result: Each $f: \mathbb{R}^2 \rightarrow \mathbb{R}$ such that $\forall a,b \in \mathbb{R} \ : \ f_a(y) := f(a,y), \ f_b(x) := f(x,b) $ are polynomials is a polynomial with two variables. If I consider $f$ as a function of $x$, then its derivative  $f'(x) = \frac{\partial f}{\partial b}(x)$. Similarly if we treat $f$ as a function of $y$. I assume that $f_a(y) = \frac{\partial f}{\partial a} (y) $ and $f_b(x)=\frac{\partial f}{\partial b}(x)$. But I am not sure if we can assume that, because  the degree of the derivative should be smaller than the degree of the original function (and it isn't). Actually, I'm not even sure if what I'm trying to prove is true, because in the original formulation of the problems there is written $f_a(y) := (a,y), \ f_b(x):=(x,b)$. But that didn't make sense. Could you help me here? Thank you.",,"['real-analysis', 'polynomials', 'partial-derivative']"
7,Boundedness and total boundedness,Boundedness and total boundedness,,"We say that a metric space $M$ is totally bounded if for every $\epsilon>0$ , there exist $x_1,\ldots,x_n\in M$ such that $M=B_\epsilon(x_1)\cup\ldots\cup B_\epsilon(x_n)$ . Prove that if $M$ is a totally bounded metric space, then $M$ is bounded. Given an example to show that the converse is false. The ""prove"" part is routine. Given points $a,b$ , suppose $a$ is in the ball of $x_i$ and $y$ is in the ball of $x_j$ . Then $d(a,b)\le d(a,x_i)+d(x_i,x_j)+d(x_j,b)<\epsilon+d(x_i,x_j)+\epsilon$ . Since there are only finitely many values of $d(x_i,x_j)$ , we are done. For the ""example"" part, the space must be bounded, but somehow there exists $\epsilon$ such that the space cannot be covered with finitely many balls of radius $\epsilon$ . I can't think of what that space should look like... things like $[a,b]$ , open ball, etc. are totally bounded.","We say that a metric space is totally bounded if for every , there exist such that . Prove that if is a totally bounded metric space, then is bounded. Given an example to show that the converse is false. The ""prove"" part is routine. Given points , suppose is in the ball of and is in the ball of . Then . Since there are only finitely many values of , we are done. For the ""example"" part, the space must be bounded, but somehow there exists such that the space cannot be covered with finitely many balls of radius . I can't think of what that space should look like... things like , open ball, etc. are totally bounded.","M \epsilon>0 x_1,\ldots,x_n\in M M=B_\epsilon(x_1)\cup\ldots\cup B_\epsilon(x_n) M M a,b a x_i y x_j d(a,b)\le d(a,x_i)+d(x_i,x_j)+d(x_j,b)<\epsilon+d(x_i,x_j)+\epsilon d(x_i,x_j) \epsilon \epsilon [a,b]","['real-analysis', 'metric-spaces']"
8,Every multiplicative linear functional on $\ell^{\infty}$ is the limit along an ultrafilter.,Every multiplicative linear functional on  is the limit along an ultrafilter.,\ell^{\infty},"It is well-known that for any ultrafilter $\mathscr{u}$ in $\mathbb{N}$, the map\begin{equation}a\mapsto \lim_{\mathscr{u}}a\end{equation} is a multiplicative linear functional, where $\lim_{\mathscr{u}}a$ is the limit of the sequence $a$ along $\mathscr{u}$. I vaguely remember someone once told me that every multiplicative linear functional on $\ell^{\infty}$ is of this form. That is, given a multiplicative linear functional $h$ on $\ell^{\infty}$, there is an ultrafilter $\mathscr{u}$ such that \begin{equation} h(a)=\lim_{\mathscr{u}}a \end{equation} for all $a\in\ell^{\infty}$. However, I cannot find a proof to this. I can show that if $h$ is the evaluation at $n$, then $h$ corresponds to the principal ultrafilter centered at $n$, but there are other kinds of multiplicative functionals (all these must vanish on any linear combinations of point masses though). Can somebody give a hint on how to do this latter case? Thanks!","It is well-known that for any ultrafilter $\mathscr{u}$ in $\mathbb{N}$, the map\begin{equation}a\mapsto \lim_{\mathscr{u}}a\end{equation} is a multiplicative linear functional, where $\lim_{\mathscr{u}}a$ is the limit of the sequence $a$ along $\mathscr{u}$. I vaguely remember someone once told me that every multiplicative linear functional on $\ell^{\infty}$ is of this form. That is, given a multiplicative linear functional $h$ on $\ell^{\infty}$, there is an ultrafilter $\mathscr{u}$ such that \begin{equation} h(a)=\lim_{\mathscr{u}}a \end{equation} for all $a\in\ell^{\infty}$. However, I cannot find a proof to this. I can show that if $h$ is the evaluation at $n$, then $h$ corresponds to the principal ultrafilter centered at $n$, but there are other kinds of multiplicative functionals (all these must vanish on any linear combinations of point masses though). Can somebody give a hint on how to do this latter case? Thanks!",,"['real-analysis', 'analysis', 'functional-analysis', 'banach-spaces', 'filters']"
9,"Enumeration of rationals from Stein-Shakarchi's Real Analysis (Chapter 1, Exercise 24)","Enumeration of rationals from Stein-Shakarchi's Real Analysis (Chapter 1, Exercise 24)",,"The exercise is from Stein-Shakarchi's Real Analysis (Chapter 1, ex. 24). Does there exist an enumeration $\{r_{n}\}_{n=1}^\infty$ of the rationals such that the complement of $\bigcup_{n=1}^{\infty}{\left(r_{n}-\frac{1}{n},r_{n}+\frac{1}{n}\right)}$ in $\mathbb{R}$ is non-empty. [Hint: Find an enumeration where the only rationals outside of a fixed bounded interval take the form $r_n$ , with $n=m^2$ for some integer $m$ .] While, I understand that we probably need some enumeration of the rationals such that the only rationals outside a fixed bounded interval are of the form $r_{m^{2}}$ for some $m$ , I'm having trouble seeing how to get such an enumeration. As always, help is very appreciated :)","The exercise is from Stein-Shakarchi's Real Analysis (Chapter 1, ex. 24). Does there exist an enumeration of the rationals such that the complement of in is non-empty. [Hint: Find an enumeration where the only rationals outside of a fixed bounded interval take the form , with for some integer .] While, I understand that we probably need some enumeration of the rationals such that the only rationals outside a fixed bounded interval are of the form for some , I'm having trouble seeing how to get such an enumeration. As always, help is very appreciated :)","\{r_{n}\}_{n=1}^\infty \bigcup_{n=1}^{\infty}{\left(r_{n}-\frac{1}{n},r_{n}+\frac{1}{n}\right)} \mathbb{R} r_n n=m^2 m r_{m^{2}} m","['real-analysis', 'real-numbers', 'rational-numbers']"
10,"When studying a book (like Rudin ) where the problems are not intended to be fully solvable by a student, what criteria show you're ready to advance?","When studying a book (like Rudin ) where the problems are not intended to be fully solvable by a student, what criteria show you're ready to advance?",,"When self studying a text where it is not expected to be able to solve all (or most) of the problems, what are the appropriate criteria to use for advancement? A word about the problems. There are a great number of them. It would be an extraordinary student indeed who could solve them all.... Many are introduced not so much to be solved as to be tackled. The value of a problem is not so much in coming up with the answer as in the ideas and attempted ideas it forces on the would-be solver. --Herstein Advanced books, like Rudin and Herstein, are intentionally written with problems ""not so much to be solved as to be tackled.""  The value in this is self-evident.  But it raises a question: How does a self-learner know when they should continue tackling more problems in a section (or book), and when they should say ""well, I can't solve every problem here, and I haven't even attempted many of them; but I've learned quite a bit, and my time is now best spent learning the next thing."" The inherent challenge here is that learning math is not linear .  Often the only true way to master Section N is to roughly learn Section N+1, and the only way to master Topic A is to roughly learn Topic B. A full solution to every problem in Rudin would probably take years, but, more importantly, can't really be done without learning more advanced topics.  The insight gained from those gives clarity and depth and tools to solve Rudin's problems.  Yet jumping ahead prematurely is a road to nowhere. In a course, this is not an issue: You do the problem sets, take the test, and if you pass, that indicates sufficient mastery to move on.  But a self-learner doesn't have this external cadence.  What criteria, then, should they use? I emphasize: The question is not ""What criteria suggest to go on when stuck on a specific problem ?"".  Rather, it is: When self-studying a book in which you're not expected to be able to solve all the problems of a section, due to their difficulty, what criteria indicate that someone can or should nonetheless advance to the next section?. For an elementary book, the answer is clear ""When you can do the vast majority of problems at the end of a section without error.""  But for an advanced, proof based book, you may never be able to solve all the proofs for a given section; even an honest attempt to do so could take years.  So there must be other criteria a self-learner can use to advance.  What are they? Of course, there is no rigorous objective test for this.  This question is looking for general patterns and soft criteria of the form ""Stay in a section until... but once... it's generally good to move further."" This question is not about a course, academic program, or career path , but about self-study , which is explicitly on topic at Math.SE. Neither is this question about anyone's ""specific circumstances.""  The question is applicable to anyone engaged in advanced self-study: When self studying a text where it is not expected to be able to solve all (or most) of the problems, what are the appropriate criteria to use for advancement? Thus, the question meets Math.SE criteria for self-learning and soft-question , both of which are explicitly on topic of Math.SE: The process of studying mathematics without formal instruction . Don't use this tag just because you were self-studying when you came across the mathematical question you're asking; it is only for when the fact that you're self-studying is what your question is about .","When self studying a text where it is not expected to be able to solve all (or most) of the problems, what are the appropriate criteria to use for advancement? A word about the problems. There are a great number of them. It would be an extraordinary student indeed who could solve them all.... Many are introduced not so much to be solved as to be tackled. The value of a problem is not so much in coming up with the answer as in the ideas and attempted ideas it forces on the would-be solver. --Herstein Advanced books, like Rudin and Herstein, are intentionally written with problems ""not so much to be solved as to be tackled.""  The value in this is self-evident.  But it raises a question: How does a self-learner know when they should continue tackling more problems in a section (or book), and when they should say ""well, I can't solve every problem here, and I haven't even attempted many of them; but I've learned quite a bit, and my time is now best spent learning the next thing."" The inherent challenge here is that learning math is not linear .  Often the only true way to master Section N is to roughly learn Section N+1, and the only way to master Topic A is to roughly learn Topic B. A full solution to every problem in Rudin would probably take years, but, more importantly, can't really be done without learning more advanced topics.  The insight gained from those gives clarity and depth and tools to solve Rudin's problems.  Yet jumping ahead prematurely is a road to nowhere. In a course, this is not an issue: You do the problem sets, take the test, and if you pass, that indicates sufficient mastery to move on.  But a self-learner doesn't have this external cadence.  What criteria, then, should they use? I emphasize: The question is not ""What criteria suggest to go on when stuck on a specific problem ?"".  Rather, it is: When self-studying a book in which you're not expected to be able to solve all the problems of a section, due to their difficulty, what criteria indicate that someone can or should nonetheless advance to the next section?. For an elementary book, the answer is clear ""When you can do the vast majority of problems at the end of a section without error.""  But for an advanced, proof based book, you may never be able to solve all the proofs for a given section; even an honest attempt to do so could take years.  So there must be other criteria a self-learner can use to advance.  What are they? Of course, there is no rigorous objective test for this.  This question is looking for general patterns and soft criteria of the form ""Stay in a section until... but once... it's generally good to move further."" This question is not about a course, academic program, or career path , but about self-study , which is explicitly on topic at Math.SE. Neither is this question about anyone's ""specific circumstances.""  The question is applicable to anyone engaged in advanced self-study: When self studying a text where it is not expected to be able to solve all (or most) of the problems, what are the appropriate criteria to use for advancement? Thus, the question meets Math.SE criteria for self-learning and soft-question , both of which are explicitly on topic of Math.SE: The process of studying mathematics without formal instruction . Don't use this tag just because you were self-studying when you came across the mathematical question you're asking; it is only for when the fact that you're self-studying is what your question is about .",,"['real-analysis', 'abstract-algebra', 'soft-question', 'self-learning', 'education']"
11,A characterization of Weak Convergence in $L^p$ spaces,A characterization of Weak Convergence in  spaces,L^p,"I'm working on the following problem, I'm having trouble with the reverse direction. My question is bolded below. Also could someone check my forward direction?: Let $(X, \mathcal{M}, \mu)$ be a $\sigma$ finite measure space and $\{f_n\},f \in L^P(X)$ . Prove that $f_n \rightharpoonup f$ in $L^p(X)$ iff $\|f_n\|_p \leq c$ for all $n$ and $\int_A f_n\, d\mu \rightarrow \int_A f \, d\mu$ for all $A$ with $\mu(A) < \infty$ . For the reverse direction, we can use the characteristic functions in $L^q$ to build arbitrary functions in $L^q$ and use Monotone Convergence on $A$ equals a ball. Then increase the radius of the ball at each step making error $\epsilon/2^n$ . However, I'm having trouble seeing how I use the boundedness of the sequence $f_n$ ) (For the forward direction, choosing $\chi_{A}\in L^q(X)$ will get the integral condition and the $\|f_n\|_p$ were bounded because the sequence originally lived in $L^p(X)$ .)","I'm working on the following problem, I'm having trouble with the reverse direction. My question is bolded below. Also could someone check my forward direction?: Let be a finite measure space and . Prove that in iff for all and for all with . For the reverse direction, we can use the characteristic functions in to build arbitrary functions in and use Monotone Convergence on equals a ball. Then increase the radius of the ball at each step making error . However, I'm having trouble seeing how I use the boundedness of the sequence ) (For the forward direction, choosing will get the integral condition and the were bounded because the sequence originally lived in .)","(X, \mathcal{M}, \mu) \sigma \{f_n\},f \in L^P(X) f_n \rightharpoonup f L^p(X) \|f_n\|_p \leq c n \int_A f_n\, d\mu \rightarrow \int_A f \, d\mu A \mu(A) < \infty L^q L^q A \epsilon/2^n f_n \chi_{A}\in L^q(X) \|f_n\|_p L^p(X)","['real-analysis', 'functional-analysis', 'proof-verification', 'lp-spaces', 'weak-convergence']"
12,Computing a tricky limit $\lim_{n\to\infty} \sqrt{n}\int_0^{\infty} \cos^{2n-1}(x) e^{- \pi x} \ dx$,Computing a tricky limit,\lim_{n\to\infty} \sqrt{n}\int_0^{\infty} \cos^{2n-1}(x) e^{- \pi x} \ dx,"I'm interested in some neat approaches for $$\lim_{n\to\infty} \sqrt{n}\int_0^{\infty} \cos^{2n-1}(x) e^{- \pi x} \ dx$$ Since I suspect my approach is wrong, and I don't wanna influence you in any way, I'll add it in a comment  after the correct approach is posted. I also plan to ofer 100 bounty for the nicer,   simpler  approach.","I'm interested in some neat approaches for $$\lim_{n\to\infty} \sqrt{n}\int_0^{\infty} \cos^{2n-1}(x) e^{- \pi x} \ dx$$ Since I suspect my approach is wrong, and I don't wanna influence you in any way, I'll add it in a comment  after the correct approach is posted. I also plan to ofer 100 bounty for the nicer,   simpler  approach.",,"['calculus', 'real-analysis', 'integration', 'limits']"
13,Fourier Transform of $\frac{1}{(1+x^2)^2}$,Fourier Transform of,\frac{1}{(1+x^2)^2},"I need to find the Fourier Transform of $f(x) = \frac{1}{(1+x^2)^2}$ Where the Fourier Tranform is of $f$ is denoted as $\hat{f}$, where $\hat{f}$ is defined as $$\hat{f}(y)=\int_\mathbb{R}f(x)e^{-ixy}dx$$ I think I need to use the Fourier Inversion Theorem. From this theorem, I think we know that $\widehat{\widehat{\frac{1}{(1+x^2)^2}}} = (2\pi)\frac{1}{(1+x^2)^2}$ Then from the Fourier Inversion Theorem, I got that $\widehat{\frac{1}{(1+x^2)^2}} = \int_\mathbb{R}\frac{1}{(1+x^2)^2}e^{ixy}dx = \int_\mathbb{R}\frac{cos(xy)}{(1+x^2)^2}dx$ From here, I am unsure about how to calculate the integral, assuming I did everything right so far. Thanks for your help!","I need to find the Fourier Transform of $f(x) = \frac{1}{(1+x^2)^2}$ Where the Fourier Tranform is of $f$ is denoted as $\hat{f}$, where $\hat{f}$ is defined as $$\hat{f}(y)=\int_\mathbb{R}f(x)e^{-ixy}dx$$ I think I need to use the Fourier Inversion Theorem. From this theorem, I think we know that $\widehat{\widehat{\frac{1}{(1+x^2)^2}}} = (2\pi)\frac{1}{(1+x^2)^2}$ Then from the Fourier Inversion Theorem, I got that $\widehat{\frac{1}{(1+x^2)^2}} = \int_\mathbb{R}\frac{1}{(1+x^2)^2}e^{ixy}dx = \int_\mathbb{R}\frac{cos(xy)}{(1+x^2)^2}dx$ From here, I am unsure about how to calculate the integral, assuming I did everything right so far. Thanks for your help!",,"['real-analysis', 'integration', 'fourier-analysis', 'fourier-transform']"
14,Continuous and additive implies linear,Continuous and additive implies linear,,"The following problem is from Golan's linear algebra book. I have posted a solution in the comments. Problem: Let $f(x):\mathbb{R}\rightarrow \mathbb{R}$ be a continuous function satisfying $f(x+y)=f(x)+f(y)$ for all $x,y\in \mathbb{R}$. Show $f$ is a linear transformation.","The following problem is from Golan's linear algebra book. I have posted a solution in the comments. Problem: Let $f(x):\mathbb{R}\rightarrow \mathbb{R}$ be a continuous function satisfying $f(x+y)=f(x)+f(y)$ for all $x,y\in \mathbb{R}$. Show $f$ is a linear transformation.",,"['real-analysis', 'linear-algebra', 'analysis', 'functional-equations']"
15,$\lim\limits_{n\to\infty}\big(x_n-\ln(n+1)\big)$ for $x_{n+1}=x_n+e^{-x_n}$,for,\lim\limits_{n\to\infty}\big(x_n-\ln(n+1)\big) x_{n+1}=x_n+e^{-x_n},"I was just going through some of the problems from an intro analysis course I took in the fall $2010$ and was hoping people could shed some light on some problems I feel that I never fully solved. Here's a problem in question: Let $x_{0}$ be a real number, and define a sequence $\left(\,{x_{n}}\,\right)_{n\ \geq\ 0}\ $ recursively by setting $$ x_{n + 1} = x_{n} + {\rm e}^{-x_{n}}\,, $$ for all $n \geq 0$ . Evaluate ( with proof ) $$ \lim_{n \to \infty}\,\,\left[x_{n} - \ln\left(\,{n + 1}\,\right)\right]. $$ We solved several problems involving such recursively defined limits, and almost always employed $\mbox{Ces}{\rm \grave{a}}\mbox{ro}$ means, and I always struggled with them. If someone could sketch a proof ( or give steps for me to fill in ) I would be more than grateful. I've googled a little bit looking for similar examples to get more practice, but haven't had much luck so if anyone knows any other similar problems ( this is about the only thing I've found), I'd be happy to hear them as well.","I was just going through some of the problems from an intro analysis course I took in the fall and was hoping people could shed some light on some problems I feel that I never fully solved. Here's a problem in question: Let be a real number, and define a sequence recursively by setting for all . Evaluate ( with proof ) We solved several problems involving such recursively defined limits, and almost always employed means, and I always struggled with them. If someone could sketch a proof ( or give steps for me to fill in ) I would be more than grateful. I've googled a little bit looking for similar examples to get more practice, but haven't had much luck so if anyone knows any other similar problems ( this is about the only thing I've found), I'd be happy to hear them as well.","2010 x_{0} \left(\,{x_{n}}\,\right)_{n\ \geq\ 0}\  
x_{n + 1} = x_{n} + {\rm e}^{-x_{n}}\,,
 n \geq 0 
\lim_{n \to \infty}\,\,\left[x_{n} - \ln\left(\,{n + 1}\,\right)\right].
 \mbox{Ces}{\rm \grave{a}}\mbox{ro}","['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
16,"Folland, Real Analysis problem 1.18","Folland, Real Analysis problem 1.18",,"This comes from an exercise from Real Analysis by Folland. Let $\mathcal{A}\subset P(X)$ be an algebra, $\mathcal{A}_\sigma$ the collection of countable unions of sets in $\mathcal{A}$ , and $\mathcal{A}_{\sigma\delta}$ the collection of countable intersections of sets in $\mathcal{A}_\sigma$ . Let $\mu_{0}$ be a premeasure on $\mathcal{A}$ and $\mu^*$ the induced outer measure. a.) For any $E\subset X$ and $\epsilon > 0$ there exists $A\in \mathcal{A}_\sigma$ with $E\subset A$ and $\mu^*(A) \leq \mu^*(E) + \epsilon$ . b.) If $\mu^{*}(E) < \infty$ , then $E$ is $\mu^{*}$ -measurable if and only if there exists $B\in A_{\sigma\delta}$ with $E\subset B$ and $\mu^{*}(B\setminus E) = 0$ c.) If $\mu_0$ is $\sigma$ -finite, the restriction $\mu^{*}(E) < \infty$ in (b) is superfluous proof a.): By the definition of outermeasure we know that $$\mu^{*}(E) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} ( A_{j} ) : A_{j} \in \mathcal{A}, E \subset \bigcup_{j=1}^{\infty} A_{j} \right \}$$ Let $A = \bigcup_{j=1}^{\infty} A_{j}$ as above. Then $A \in \mathcal{A}_{\sigma}$ and $E \subset A$ . For each $j$ we can construct a sequence $\{ B_{j}^{k} \} _{k=1}^{\infty}\subset \mathcal{A}$ such that $A_j\subset \bigcup_{j,k=1}^{\infty}B_{j}^{k}$ . It follows that since $$\mu^{*}(A_j) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} (B_{j}^{k} ) : B_{j}^{k} \in \mathcal{A}, A_j \subset \bigcup_{j,k=1}^{\infty} B_{j}^{k} \right \}$$ We have that, $$\mu^{*}(A_{j}) \leq \mu_{0}(A_{j}) + \epsilon 2^{-j}, \forall j \ \ \text{and} \ \ \epsilon>0$$ Thus: $$\mu^{*}(A) \leq  \sum_{j=1}^{\infty} \mu^{*} ( A_{j} ) \leq \sum_{j=1}^{\infty} ( \mu_{0}(A_{j}) + \epsilon 2^{-j}) = \mu^{*}(E) + \epsilon$$ Since $\epsilon$ is arbitrary we are done. b.) Suppose $E$ is $\mu^{*}$ -measurable. From part (a), we know that $\forall n\in\mathbb{N}$ there exists $B_n\in A_\sigma$ with $E\subset B_n$ and $\mu^{*}(B_n) - \mu^{*}(E) \leq 1/n$ . Let, $$B = \bigcap_{n = 1}^{\infty}B_n\in A_\sigma$$ since E is $\mu^{*}$ -measurable, we have $\mu^{*}(B_n) = \mu^{*}(B_n\cap E) + \mu^{*}(B_n\cap E^{c})$ hence, $$\mu^{*}(B\cap E^{c})\leq \mu^{*}(B_n\cap E^{c})= \mu^{*}(B_n) - \mu^{*}(E)\leq 1/n$$ for every $n\in\mathbb{N}$ . Hence we have $\mu^{*}(B\setminus E) = 0$ \ To show the converse, let's suppose $B\in A_{\sigma\delta}$ with $E\subset B$ and $\mu^{*}(B\setminus E) = 0$ . From part (a), we know that $\forall n\in\mathbb{N}$ there exists $A_n\in A_\sigma$ with $(B\setminus E)\subset A_n$ and $\mu^{*}(A_n) - \mu^{*}(B\setminus E)\leq 1/n$ . But, since $\mu^{*}(B\setminus E) = 0$ then, $\mu^{*}(A_n)\leq 1/n$ . Let, $$A = \bigcap_{n=1}^{\infty}A_n$$ then A is $\mu^{*}$ -measurable (since $A\in A_{\sigma\delta}$ and the set of all $\mu^{*}$ -measurable sets is a $\sigma$ -algebra) such that $(B\setminus E)\subset A$ and $\mu^{*}(A) = 0$ .\ By Carathedors's theorem we know that the restriction of $\mu^{*}$ to $\mu^{*}$ -measurable sets is a complete measure. From this, we know that $(B\setminus E)$ is $\mu^{*}$ -measurable. Also, since $B\in A_{\sigma\delta}$ then $B$ is also $\mu^{*}$ -measurable and we can express $E$ as $$E = (B^{c}\cup (B\cap E^{c}))^{c}$$ Thus $E$ is $\mu^{*}$ -measurable. c.) Let $\mu_0$ be $\sigma$ -finite, then let $$X = \bigcup_{1}^{\infty}X_i$$ where $X_i\in M$ and $\mu(X_i) < \infty$ Now, suppose $E$ is $\mu^{*}$ -measurable and $\mu^{*}(E) = \infty$ , set $$E_n = (E\cap \bigcup_{1}^{n}X_i)$$ then $\mu^{*}(E_n) < \infty$ and $E = \bigcup_{1}^{\infty}E_n$ . Let $\epsilon > 0$ , from part (a) $\forall n\in\mathbb{N} \exists C_n\in A_\sigma$ such that $E_n\subset C_n$ and $$\mu^{*}(C_n\setminus E_n) = \mu^{*}(C_n) - \mu^{*}(E_n) \leq \epsilon/2^{n}$$ $$\mu^{*}(E_n) = \mu^{*}(E\cap \bigcup_{1}^{n}X_i) = \mu^{*}(E) \cap \mu^{*}(\bigcup_{1}^{n}X_i) = \infty$$ hence $\mu^{*}(C_n\setminus E_n) = 0$ I am not sure where to go from here, any hints or suggestions is greatly appreciated","This comes from an exercise from Real Analysis by Folland. Let be an algebra, the collection of countable unions of sets in , and the collection of countable intersections of sets in . Let be a premeasure on and the induced outer measure. a.) For any and there exists with and . b.) If , then is -measurable if and only if there exists with and c.) If is -finite, the restriction in (b) is superfluous proof a.): By the definition of outermeasure we know that Let as above. Then and . For each we can construct a sequence such that . It follows that since We have that, Thus: Since is arbitrary we are done. b.) Suppose is -measurable. From part (a), we know that there exists with and . Let, since E is -measurable, we have hence, for every . Hence we have \ To show the converse, let's suppose with and . From part (a), we know that there exists with and . But, since then, . Let, then A is -measurable (since and the set of all -measurable sets is a -algebra) such that and .\ By Carathedors's theorem we know that the restriction of to -measurable sets is a complete measure. From this, we know that is -measurable. Also, since then is also -measurable and we can express as Thus is -measurable. c.) Let be -finite, then let where and Now, suppose is -measurable and , set then and . Let , from part (a) such that and hence I am not sure where to go from here, any hints or suggestions is greatly appreciated","\mathcal{A}\subset P(X) \mathcal{A}_\sigma \mathcal{A} \mathcal{A}_{\sigma\delta} \mathcal{A}_\sigma \mu_{0} \mathcal{A} \mu^* E\subset X \epsilon > 0 A\in \mathcal{A}_\sigma E\subset A \mu^*(A) \leq \mu^*(E) + \epsilon \mu^{*}(E) < \infty E \mu^{*} B\in A_{\sigma\delta} E\subset B \mu^{*}(B\setminus E) = 0 \mu_0 \sigma \mu^{*}(E) < \infty \mu^{*}(E) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} ( A_{j} ) : A_{j} \in \mathcal{A}, E \subset \bigcup_{j=1}^{\infty} A_{j} \right \} A = \bigcup_{j=1}^{\infty} A_{j} A \in \mathcal{A}_{\sigma} E \subset A j \{ B_{j}^{k} \} _{k=1}^{\infty}\subset \mathcal{A} A_j\subset \bigcup_{j,k=1}^{\infty}B_{j}^{k} \mu^{*}(A_j) = \inf\left\{ \sum_{j=1}^{\infty} \mu_{0} (B_{j}^{k} ) : B_{j}^{k} \in \mathcal{A}, A_j \subset \bigcup_{j,k=1}^{\infty} B_{j}^{k} \right \} \mu^{*}(A_{j}) \leq \mu_{0}(A_{j}) + \epsilon 2^{-j}, \forall j \ \ \text{and} \ \ \epsilon>0 \mu^{*}(A) \leq  \sum_{j=1}^{\infty} \mu^{*} ( A_{j} ) \leq \sum_{j=1}^{\infty} ( \mu_{0}(A_{j}) + \epsilon 2^{-j}) = \mu^{*}(E) + \epsilon \epsilon E \mu^{*} \forall n\in\mathbb{N} B_n\in A_\sigma E\subset B_n \mu^{*}(B_n) - \mu^{*}(E) \leq 1/n B = \bigcap_{n = 1}^{\infty}B_n\in A_\sigma \mu^{*} \mu^{*}(B_n) = \mu^{*}(B_n\cap E) + \mu^{*}(B_n\cap E^{c}) \mu^{*}(B\cap E^{c})\leq \mu^{*}(B_n\cap E^{c})= \mu^{*}(B_n) - \mu^{*}(E)\leq 1/n n\in\mathbb{N} \mu^{*}(B\setminus E) = 0 B\in A_{\sigma\delta} E\subset B \mu^{*}(B\setminus E) = 0 \forall n\in\mathbb{N} A_n\in A_\sigma (B\setminus E)\subset A_n \mu^{*}(A_n) - \mu^{*}(B\setminus E)\leq 1/n \mu^{*}(B\setminus E) = 0 \mu^{*}(A_n)\leq 1/n A = \bigcap_{n=1}^{\infty}A_n \mu^{*} A\in A_{\sigma\delta} \mu^{*} \sigma (B\setminus E)\subset A \mu^{*}(A) = 0 \mu^{*} \mu^{*} (B\setminus E) \mu^{*} B\in A_{\sigma\delta} B \mu^{*} E E = (B^{c}\cup (B\cap E^{c}))^{c} E \mu^{*} \mu_0 \sigma X = \bigcup_{1}^{\infty}X_i X_i\in M \mu(X_i) < \infty E \mu^{*} \mu^{*}(E) = \infty E_n = (E\cap \bigcup_{1}^{n}X_i) \mu^{*}(E_n) < \infty E = \bigcup_{1}^{\infty}E_n \epsilon > 0 \forall n\in\mathbb{N} \exists C_n\in A_\sigma E_n\subset C_n \mu^{*}(C_n\setminus E_n) = \mu^{*}(C_n) - \mu^{*}(E_n) \leq \epsilon/2^{n} \mu^{*}(E_n) = \mu^{*}(E\cap \bigcup_{1}^{n}X_i) = \mu^{*}(E) \cap \mu^{*}(\bigcup_{1}^{n}X_i) = \infty \mu^{*}(C_n\setminus E_n) = 0",['real-analysis']
17,Difference between a measure and a premeasure,Difference between a measure and a premeasure,,I am new to measure theory and am wondering: Is the only difference between a measure and a premeasure the fact that measures are defined on $\sigma$-algebras and premeasures are defined only on algebras? If there are more fundamental differences than this please elaborate on them.,I am new to measure theory and am wondering: Is the only difference between a measure and a premeasure the fact that measures are defined on $\sigma$-algebras and premeasures are defined only on algebras? If there are more fundamental differences than this please elaborate on them.,,"['real-analysis', 'measure-theory']"
18,Maximum value of $\int_{0}^{1}e^x\log f(x)dx$ when $\int_{0}^{1}f(x)dx=1$,Maximum value of  when,\int_{0}^{1}e^x\log f(x)dx \int_{0}^{1}f(x)dx=1,"Suppose that $f(x)\ (0\le x\le 1)$ is continuous and strictly positive and satisfies $$\int_{0}^{1}f(x)dx=1.$$ Then, can we find the maximum and the minimum value of the following? If yes, then how? $$\int_{0}^{1}e^x\log f(x)dx$$ I even don't know if this is solvable. Is there any theorem?","Suppose that $f(x)\ (0\le x\le 1)$ is continuous and strictly positive and satisfies $$\int_{0}^{1}f(x)dx=1.$$ Then, can we find the maximum and the minimum value of the following? If yes, then how? $$\int_{0}^{1}e^x\log f(x)dx$$ I even don't know if this is solvable. Is there any theorem?",,"['real-analysis', 'integration', 'definite-integrals']"
19,Why do we essentially need complete measure space?,Why do we essentially need complete measure space?,,"While reading the motivation of complete measure space on Wikipedia , what I concluded was,  completeness is not really necessary when we define on one measure space  and it is necessary when we want to measure on product of measure spaces (is it true ?).  If $\lambda$ is measure on $X$ and $Y$ then is it true that $\lambda^2$ is measure of $A$x$B$ and how ?  I am not able to understand that $\lambda^2(A\times B)=\lambda(A)\times\lambda(B)$ ?  Essentially what is the flaw in the measure without being complete ?  Waiting for response. Thanks!","While reading the motivation of complete measure space on Wikipedia , what I concluded was,  completeness is not really necessary when we define on one measure space  and it is necessary when we want to measure on product of measure spaces (is it true ?).  If $\lambda$ is measure on $X$ and $Y$ then is it true that $\lambda^2$ is measure of $A$x$B$ and how ?  I am not able to understand that $\lambda^2(A\times B)=\lambda(A)\times\lambda(B)$ ?  Essentially what is the flaw in the measure without being complete ?  Waiting for response. Thanks!",,"['real-analysis', 'measure-theory']"
20,"Almost finished with integral but hit roadblock $\int_{0}^{\infty}\frac{\arctan^2(x^2)}{x^2}\,dx$",Almost finished with integral but hit roadblock,"\int_{0}^{\infty}\frac{\arctan^2(x^2)}{x^2}\,dx","I was evaluating the following integral $$I:=\int_{0}^{\infty}\frac{\arctan^2(x^2)}{x^2}\,dx$$ Here is my process. I just need some guidance to finish my work. Solution process: Define $$I(t):=\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx$$ $$I’(t)=\int_{0}^{\infty}\frac{2\arctan(tx^2)}{t^2x^4+1}\,dx$$ $$I’(t)=\frac{1}{t}\int_{0}^{\infty}\frac{\arctan(tx^2)}{x}\frac{d}{dx}\arctan(tx^2)\,dx$$ $I.B.P$ $$I’(t)=\frac{1}{2t}\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx$$ $$I’(t)=\frac{1}{2t}I(t)$$ solving the separable ODE yields: $$I(t)=C\sqrt{t}$$ Here is the problem! Recalling the definition of $I(t)$ : $$I(t):=\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx$$ I cannot find any initial conditions that can help me fish out the value of $C$ . Is it possible? How can we do it?",I was evaluating the following integral Here is my process. I just need some guidance to finish my work. Solution process: Define solving the separable ODE yields: Here is the problem! Recalling the definition of : I cannot find any initial conditions that can help me fish out the value of . Is it possible? How can we do it?,"I:=\int_{0}^{\infty}\frac{\arctan^2(x^2)}{x^2}\,dx I(t):=\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx I’(t)=\int_{0}^{\infty}\frac{2\arctan(tx^2)}{t^2x^4+1}\,dx I’(t)=\frac{1}{t}\int_{0}^{\infty}\frac{\arctan(tx^2)}{x}\frac{d}{dx}\arctan(tx^2)\,dx I.B.P I’(t)=\frac{1}{2t}\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx I’(t)=\frac{1}{2t}I(t) I(t)=C\sqrt{t} I(t) I(t):=\int_{0}^{\infty}\frac{\arctan^2(tx^2)}{x^2}\,dx C","['real-analysis', 'calculus', 'integration', 'definite-integrals']"
21,Is there a discontinuous function on the plane having partial derivatives of all orders?,Is there a discontinuous function on the plane having partial derivatives of all orders?,,"If one requires simply the existence of partial derivatives of first order rather than all orders, then a standard example is the function $$ f(x,y) = \left\{\begin{array}{l l} \frac{2xy}{x^2+y^2} & \quad \text{if $(x,y)\neq(0,0),$}\\ 0                   & \quad \text{if $(x,y)=(0,0).$} \end{array} \right.$$ However, this does not constitute an answer to my question since the partial derivative of $\frac{\partial f}{\partial x}$ with respect to $y$ does not exist at the origin. PS: This question rose out of my wonder as to whether, in the definition of a smooth function, continuity of partials is an essential requirement or not.","If one requires simply the existence of partial derivatives of first order rather than all orders, then a standard example is the function $$ f(x,y) = \left\{\begin{array}{l l} \frac{2xy}{x^2+y^2} & \quad \text{if $(x,y)\neq(0,0),$}\\ 0                   & \quad \text{if $(x,y)=(0,0).$} \end{array} \right.$$ However, this does not constitute an answer to my question since the partial derivative of $\frac{\partial f}{\partial x}$ with respect to $y$ does not exist at the origin. PS: This question rose out of my wonder as to whether, in the definition of a smooth function, continuity of partials is an essential requirement or not.",,"['real-analysis', 'multivariable-calculus', 'examples-counterexamples', 'partial-derivative']"
22,Limit point versus limit and implications for convergence,Limit point versus limit and implications for convergence,,"I understand that similar questions to this have been asked here (1) and here (2) but I believe my question is different because with regards to (1) I am not asking about the set $X$ that contains the points of my sequence and with regard to (2) because I have the distinct definitions of limit and limit point in mind. That said: My Question : My textbook, Fundamental Ideas of Analysis by Michael Reed, defines a limit point as: Definition : Let $(a_n)$ be a sequence of real numbers. A number $d$ is called a limit point if, given $\epsilon > 0$ and an integer $N$, there exists $\color{red}{\mathrm{ an }} \ n \ge N$ so that $|a_n - d| \le \epsilon$. The definition of a limit is as follows: Definition : We say that a sequence $(a_n)$ converges to a limit $a$ if, for every $\epsilon \ge 0$, there is an integer $N(\epsilon)$ so that $|a_n - a| \le \epsilon \color{red}{\text{  for all  }} n \ge N$. From these definitions, and given this question: If a sequence converges it has exactly one limit point. Is the converse true? If not, provide a counterexample Can I conclude that the converse is indeed true simply by appealing to the definitions, or do I need a tighter proof. And if I do, I'm a little stuck on how to refine this proof.","I understand that similar questions to this have been asked here (1) and here (2) but I believe my question is different because with regards to (1) I am not asking about the set $X$ that contains the points of my sequence and with regard to (2) because I have the distinct definitions of limit and limit point in mind. That said: My Question : My textbook, Fundamental Ideas of Analysis by Michael Reed, defines a limit point as: Definition : Let $(a_n)$ be a sequence of real numbers. A number $d$ is called a limit point if, given $\epsilon > 0$ and an integer $N$, there exists $\color{red}{\mathrm{ an }} \ n \ge N$ so that $|a_n - d| \le \epsilon$. The definition of a limit is as follows: Definition : We say that a sequence $(a_n)$ converges to a limit $a$ if, for every $\epsilon \ge 0$, there is an integer $N(\epsilon)$ so that $|a_n - a| \le \epsilon \color{red}{\text{  for all  }} n \ge N$. From these definitions, and given this question: If a sequence converges it has exactly one limit point. Is the converse true? If not, provide a counterexample Can I conclude that the converse is indeed true simply by appealing to the definitions, or do I need a tighter proof. And if I do, I'm a little stuck on how to refine this proof.",,"['real-analysis', 'sequences-and-series']"
23,Is the total variation function uniform continuous or continuous?,Is the total variation function uniform continuous or continuous?,,"I have been doing some excercises on total variation when the following questions came up to my mind: (1) Let $f$ be continuous on the interval $[0,1]$ and be of bounded variation. Is it true that its total vatiation function $TV(f_{[0,x]})$ is uniformly continuous? i.e. Is it true that for $\forall\space\epsilon>0$, $\exists\space\delta>0$, such that for arbitrary interval $[a,b]$ with $|b-a|<\delta$, we have $TV(f_{[a,b]})<\epsilon$? (2) Alternatively, if it is not uniformly continuous, can I say that for $\forall\space{}x\in[0,1]\text{ and } \forall\space\epsilon>0$ , $\exists\space\text{ nondegenerate interval }I \text{ such that }x\in{}I\text{ and } TV(f_{I})<\epsilon$? Thank you!","I have been doing some excercises on total variation when the following questions came up to my mind: (1) Let $f$ be continuous on the interval $[0,1]$ and be of bounded variation. Is it true that its total vatiation function $TV(f_{[0,x]})$ is uniformly continuous? i.e. Is it true that for $\forall\space\epsilon>0$, $\exists\space\delta>0$, such that for arbitrary interval $[a,b]$ with $|b-a|<\delta$, we have $TV(f_{[a,b]})<\epsilon$? (2) Alternatively, if it is not uniformly continuous, can I say that for $\forall\space{}x\in[0,1]\text{ and } \forall\space\epsilon>0$ , $\exists\space\text{ nondegenerate interval }I \text{ such that }x\in{}I\text{ and } TV(f_{I})<\epsilon$? Thank you!",,"['real-analysis', 'continuity', 'bounded-variation']"
24,Find all functions f such that $f(f(x))=f(x)+x$,Find all functions f such that,f(f(x))=f(x)+x,"Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that   $f(f(x))=f(x)+x, \forall x\in\mathbb{R}$. Find all such functions $f$. Clearly, $f$ is an ""one-to-one function"". I have tried setting where $x\rightarrow f(x)$ so: $f(f(f(x)))=f(f(x))+f(x)=2f(x)+x$. Also, from the original relation, we get that: $f(f(f(x)))=f(f(x)+x)$. Equating the RHS of the last two relations we get that: $f(f(x)+x)=2f(x)+x$. Probably to exploit the 1-1 we need a relation of the form $f(f(x))=f(\cdots)$. I don't know how to carry this further... Any hint?","Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that   $f(f(x))=f(x)+x, \forall x\in\mathbb{R}$. Find all such functions $f$. Clearly, $f$ is an ""one-to-one function"". I have tried setting where $x\rightarrow f(x)$ so: $f(f(f(x)))=f(f(x))+f(x)=2f(x)+x$. Also, from the original relation, we get that: $f(f(f(x)))=f(f(x)+x)$. Equating the RHS of the last two relations we get that: $f(f(x)+x)=2f(x)+x$. Probably to exploit the 1-1 we need a relation of the form $f(f(x))=f(\cdots)$. I don't know how to carry this further... Any hint?",,['real-analysis']
25,Zeros of analytic function of several real variables,Zeros of analytic function of several real variables,,"Suppose $f$ is a non-constant analytic function on $R^n$, what can we say about its zeros set $Z(f)=\{x\in R^n|f(x)=0\}$? Does $Z(f)$ have measure 0? Is it true that $Z(f)$ is union of manifolds of dimension less than $n$?","Suppose $f$ is a non-constant analytic function on $R^n$, what can we say about its zeros set $Z(f)=\{x\in R^n|f(x)=0\}$? Does $Z(f)$ have measure 0? Is it true that $Z(f)$ is union of manifolds of dimension less than $n$?",,"['real-analysis', 'analysis']"
26,Uniform convergence and weak convergence,Uniform convergence and weak convergence,,"Assume $F_{n},F$ are distribution functions of r.v.$X_{n}$ and $X$, $F_{n}$ weakly converge to $F$. If $F$ is pointwise continuous in the interval $[a,b]\subset\mathbb{R}$, show that $$\sup_{x\in[a,b]}|F_{n}(x)-F(x)|\rightarrow 0,n\rightarrow \infty.$$Thanks.","Assume $F_{n},F$ are distribution functions of r.v.$X_{n}$ and $X$, $F_{n}$ weakly converge to $F$. If $F$ is pointwise continuous in the interval $[a,b]\subset\mathbb{R}$, show that $$\sup_{x\in[a,b]}|F_{n}(x)-F(x)|\rightarrow 0,n\rightarrow \infty.$$Thanks.",,"['real-analysis', 'probability-theory', 'uniform-convergence', 'weak-convergence']"
27,Is a non-repeating and non-terminating decimal always an irrational?,Is a non-repeating and non-terminating decimal always an irrational?,,"We can build $\frac{1}{33}$ like this, $.030303$ $\cdots$ ($03$ repeats). $.0303$ $\cdots$ tends to $\frac{1}{33}$. So,I was wondering this: In the decimal representation, if we start writing the $10$ numerals in such a way that the decimal portion never ends and never repeats; then am I getting closer and closer to some irrational number?","We can build $\frac{1}{33}$ like this, $.030303$ $\cdots$ ($03$ repeats). $.0303$ $\cdots$ tends to $\frac{1}{33}$. So,I was wondering this: In the decimal representation, if we start writing the $10$ numerals in such a way that the decimal portion never ends and never repeats; then am I getting closer and closer to some irrational number?",,"['real-analysis', 'numerical-methods', 'irrational-numbers', 'number-systems', 'rational-numbers']"
28,Positive outer measure set and nonmeasurable subset,Positive outer measure set and nonmeasurable subset,,"I'm attending a course of Measure and Integration and have some homework to do. We don't have a specific book to follow, neither for exercise. I'm asked to proof that every set $A\subseteq\mathbb{R}$ with strictly positive outer measure contains a set nonmeasurable (Lebesgue). I thought about the Cantor set. Thinking that A is homeomorphic to Cantor set and about sharing $[0,1]$ with a numerable family of copies disjointed of A. But honestly I'm not sure how to do exactly, and I would like to get a precise answer 'cause I'll have to explain in a foreigner language so the better my notes are the easier it will be. I checked other questions on this website before asking without finding anything, just some references to this property as known; probably it's my fault, so if somebody knows where to find the answer please let me know. Update: May be is not clear but A is strictly positive in the outer Lebesgue measure and I think that the subset has to be strictly included.","I'm attending a course of Measure and Integration and have some homework to do. We don't have a specific book to follow, neither for exercise. I'm asked to proof that every set with strictly positive outer measure contains a set nonmeasurable (Lebesgue). I thought about the Cantor set. Thinking that A is homeomorphic to Cantor set and about sharing with a numerable family of copies disjointed of A. But honestly I'm not sure how to do exactly, and I would like to get a precise answer 'cause I'll have to explain in a foreigner language so the better my notes are the easier it will be. I checked other questions on this website before asking without finding anything, just some references to this property as known; probably it's my fault, so if somebody knows where to find the answer please let me know. Update: May be is not clear but A is strictly positive in the outer Lebesgue measure and I think that the subset has to be strictly included.","A\subseteq\mathbb{R} [0,1]","['real-analysis', 'measure-theory']"
29,No function that is continuous at all rational points and discontinuous at irrational points. [duplicate],No function that is continuous at all rational points and discontinuous at irrational points. [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Set of continuity points of a real function I think I saw somewhere(but I'm not sure) that there is no function $g$ on $[0,1]$ that is continuous at all rational points and discontinuous at all irrational points. Please, is this true? If yes how can I show it?  Thanks.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Set of continuity points of a real function I think I saw somewhere(but I'm not sure) that there is no function $g$ on $[0,1]$ that is continuous at all rational points and discontinuous at all irrational points. Please, is this true? If yes how can I show it?  Thanks.",,['real-analysis']
30,Proving the measure of an increasing sequence of measurable sets is the limit of the measures,Proving the measure of an increasing sequence of measurable sets is the limit of the measures,,"Show that if $A_1\subseteq A_2\subseteq A_3\subseteq\cdots$ is an increasing sequence of measurable sets (so $A_j\subseteq A_{j+1}$ for every positive integer $j$ ), then we have $$m\left(\bigcup_{j=1}^\infty A_j\right)=\lim_{j\to\infty}m(A_j)$$ Here is my proof: According to the $\sigma$ -algebra property, $\bigcup_{j=1}^{\infty}A_j$ is a measurable set, so it makes sense to talk about $m(\bigcup_{j=1}^{\infty}A_j)$ . Firstly, I prove that $\lim_{j\to\infty}m(A_j)\leq m(\bigcup_{j=1}^{\infty}A_j)$ . This is because for any given positive integer $N$ , $A_N\subseteq \bigcup_{j=1}^{\infty}A_i$ , according to monotonicity, we have $m(A_N)\leq m(\bigcup_{j=1}^{\infty}A_i)$ . Take the limit,we will have $\lim_{j\to\infty}m(A_j)\leq m(\bigcup_{j=1}^{\infty}A_j)$ . Secondly, I prove that $m(\bigcup_{j=1}^{\infty}A_j)\leq \lim_{j\to\infty}m(A_j)$ . For any given positive integer $N$ , $\bigcup_{j=1}^N A_j = A_N$ . According to monotonicity,we have $m\left(\bigcup_{j=1}^N A_j\right)=m(A_N)\leq \lim_{j\to\infty}m(A_j)$ . Take the limit, we will have $m\left(\bigcup_{j=1}^\infty A_j\right) \leq \lim_{j\to\infty} m(A_j)$ . Combine the above two arguments, we will see that $$m\left(\bigcup_{j=1}^\infty A_j\right)=\lim_{j\to\infty} m(A_j)$$ $\Box$ The above is my proof, unlike many books, my proof does not use the property of countable additivity. So I doubt my proof is correct. Who can point out where are my mistakes?","Show that if is an increasing sequence of measurable sets (so for every positive integer ), then we have Here is my proof: According to the -algebra property, is a measurable set, so it makes sense to talk about . Firstly, I prove that . This is because for any given positive integer , , according to monotonicity, we have . Take the limit,we will have . Secondly, I prove that . For any given positive integer , . According to monotonicity,we have . Take the limit, we will have . Combine the above two arguments, we will see that The above is my proof, unlike many books, my proof does not use the property of countable additivity. So I doubt my proof is correct. Who can point out where are my mistakes?",A_1\subseteq A_2\subseteq A_3\subseteq\cdots A_j\subseteq A_{j+1} j m\left(\bigcup_{j=1}^\infty A_j\right)=\lim_{j\to\infty}m(A_j) \sigma \bigcup_{j=1}^{\infty}A_j m(\bigcup_{j=1}^{\infty}A_j) \lim_{j\to\infty}m(A_j)\leq m(\bigcup_{j=1}^{\infty}A_j) N A_N\subseteq \bigcup_{j=1}^{\infty}A_i m(A_N)\leq m(\bigcup_{j=1}^{\infty}A_i) \lim_{j\to\infty}m(A_j)\leq m(\bigcup_{j=1}^{\infty}A_j) m(\bigcup_{j=1}^{\infty}A_j)\leq \lim_{j\to\infty}m(A_j) N \bigcup_{j=1}^N A_j = A_N m\left(\bigcup_{j=1}^N A_j\right)=m(A_N)\leq \lim_{j\to\infty}m(A_j) m\left(\bigcup_{j=1}^\infty A_j\right) \leq \lim_{j\to\infty} m(A_j) m\left(\bigcup_{j=1}^\infty A_j\right)=\lim_{j\to\infty} m(A_j) \Box,"['real-analysis', 'measure-theory']"
31,On the closedness of $L^2$ under convolution,On the closedness of  under convolution,L^2,"It is a direct consequence of Fubini's theorem that if $f,g \in L^1(\mathbb{R})$, then the convolution $f *g$ is well defined almost everywhere and $f*g \in L^1(\mathbb{R})$. Thus, $L^1(\mathbb{R})$ is closed under convolution, and it is a Banach algebra without unit since we have the inequality $$\|f*g\|_{1} \leq \|f\|_1 \|g\|_1 \qquad (f,g \in L^1(\mathbb{R})).$$ Now, it follows from Hölder's inequality that if $f,g \in L^2(\mathbb{R})$, then $f*g$ is bounded. My question is the following : Does $f*g$ necessarily belongs to $L^2(\mathbb{R})$? In other words, is $L^2(\mathbb{R})$ closed under convolution? Since a quick google search seem to result in a negative answer, I also ask the following question : Can you give an explicit example of two functions $f, g \in L^2(\mathbb{R})$ such that $f*g \notin L^2(\mathbb{R})$? Thank you, Malik","It is a direct consequence of Fubini's theorem that if $f,g \in L^1(\mathbb{R})$, then the convolution $f *g$ is well defined almost everywhere and $f*g \in L^1(\mathbb{R})$. Thus, $L^1(\mathbb{R})$ is closed under convolution, and it is a Banach algebra without unit since we have the inequality $$\|f*g\|_{1} \leq \|f\|_1 \|g\|_1 \qquad (f,g \in L^1(\mathbb{R})).$$ Now, it follows from Hölder's inequality that if $f,g \in L^2(\mathbb{R})$, then $f*g$ is bounded. My question is the following : Does $f*g$ necessarily belongs to $L^2(\mathbb{R})$? In other words, is $L^2(\mathbb{R})$ closed under convolution? Since a quick google search seem to result in a negative answer, I also ask the following question : Can you give an explicit example of two functions $f, g \in L^2(\mathbb{R})$ such that $f*g \notin L^2(\mathbb{R})$? Thank you, Malik",,"['real-analysis', 'convolution']"
32,Yet another definition of uniform integrability. Is it equivalent to the classical definition by G.A. Hunt?,Yet another definition of uniform integrability. Is it equivalent to the classical definition by G.A. Hunt?,,"I came across this posting where a definition of uniform integrability (found in Tao, T., Introduction Measure Theory. AMS, GTM vol 126, 2011) is given as follows: Definition T:  Suppose $(X,\mathscr{B},\mu)$ is a measure space (not necessarily finite). A sequence $f_n:X \rightarrow \mathbb{C}$ of absolutely Integrable functions is said to be uniformly Integrable if the following three statements hold (Uniform bounded on $L^1$ ) One has $\sup_n\|f_n\|_{L^1(\mu)}=\sup_n\int_{X}|f_n|d\mu <+\infty$ . (No escape to vertical infinity) One has $\sup_n\int_{|f_n|\ge M}|f_n|d\mu\xrightarrow{M\rightarrow\infty} 0$ . (No escape to width infinity) One has $\sup_n\int_{|f_n|\le\delta}|f_n|d\mu\xrightarrow{\delta\rightarrow0} 0$ . This is in contrast to the common used definition of uniform integrability by Hunt (Hunt, G.A., Martingales et Precessus de Markov , Paris: Dunod, 1966 pp. 254) which I state as follows: Definition H: A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\begin{align} \inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu=0 \tag{1}\label{one} \end{align}$$ Definition H is widely used on Probability theory and in functional analysis, for example, it provides an extension of Pettis-Dunford's theorem for $\sigma$ -finite spaces. There are several known equivalences to Hunt's definition, some of which I will state below. Observation: For a finite measure space, condition (3) Definition T is rather superfluous and, as it can be easily seen, Definitions H and T are equivalent in this setting. Problem(s): (a): Are  Definitions H and T  equivalent for general measure spaces (or at least for infinite $\sigma$ -finite measures, or only for countable families $\mathcal{F}\subset L_1$ )? (b): If not, are there any applications (problems) where Definition H does a job that Definition T can't deliver (and vice versa). I spent some considerable amount trying to work through (a). I was unsuccessful to show either of the following statements: Prop 1: Definition H implies Definition T. Definition H does imply condition (1) and (2) in general measure spaces. Whether (3) also holds, escapes me. Prop 2: Definition T implies Definition H. Did not go far at all. If all this is well known, a reference would be appreciated. Here are some well known equivalencies of Definition H. Notation: Suppose $(X,\mathscr{B},\mu)$ is a measure space. When the the ambient space is clear from the context, we use $L_1$ as a shorthand for $L_1(X,\mathscr{B},\mu)$ , and $\|f\|_1:=\int|f|\,d\mu$ (the $L_1$ -norm). We also use the notation $L^+_1=\{f\in L_1: f\geq0\}$ . Given $g,h\in L_1$ with $g\leq h$ $\mu$ -a.s., we denote $$[g,h]=\{f\in L_1: g\leq f\leq h\}$$ For any $\mathcal{A}\subset L_1$ and $f\in L_1$ , $$d(f,\mathcal{A}):=\inf\{\|f-\phi\|_1: \phi\in\mathcal{A}\}$$ The following is from K. Bichteler, Integration: A functional approach , Birkhäuser Verlag, 1998. p.p. 57. Definition B: A family $\mathcal{F}\subset L_1$ is uniformly integrable if for any $\varepsilon>0$ , there are $g,h\in L_1$ with $g\leq h$ such that \begin{align}\sup_{f\in\mathcal{F}}d(f,[g,h])<\varepsilon\tag{2}\label{2} \end{align} Comments: For any $x\in\mathbb{R}$ and $a<b$ , define $x^b_a:=(a\vee x)\wedge b$ . It is easy to check that if $c\leq a\leq b\leq d$ , then $$|x-x^d_c|\leq |x-x^b_a|$$ Thus, Definition B may be rewritten as: Definition B': $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\inf_{g\in L^+_1}\sup_{f\in \mathcal{F}}d(f,[-g,g])=0$$ Comments: Definition B means that  a family $\mathcal{F}\subset L_1$ is uniformly integrable if their elements are close to being dominated by an integrable function. Now, for $g\geq0$ , $|f-f^g_{-g}|=(|f|-g)_+$ ; hence, as $|x-x^a_{-a}|\leq |x-b|$ for all $|b|\leq a$ ,  Definition B  can also be rewritten as (see, for example (Klenke, A., Probability Theory, Springer 2006. pp. 134) Definition H': A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\begin{align} \inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_X(|f|-g)_+\,d\mu=0 \tag{1'}\label{onep} \end{align}$$ Comments: Observe that if $g\geq0$ , then $$|f|\mathbb{1}_{\{|f|>2g\}}\leq (|f|-g)_+\mathbb{1}_{\{|f|>2g\}}+g\mathbb{1}_{\{|f|>2g\}}\leq 2(|f|-g)_+\mathbb{1}_{|f|>2g\}}$$ Hence, if $\mathcal{F}$ is uniformly integrable in the sense of Definition H' then, $$\begin{align} \inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu=0, \end{align}$$ that is, $\mathcal{F}$ is uniformly integrable in the sense of Definition H.  Conversely, notice that for any $g\geq0$ $$(|f|-g)_+\leq|f|\mathbb{1}_{\{|f|>g\}}.$$ Hence, if $\mathcal{F}\subset L_1$ satisfies \eqref{one} then, it is uniformly integrable in the sense of Definition H'. This establishes the equivalence of Definitions B, H and H'. Notice that for any $A\in\mathscr{B}$ and $g\geq0$ $$|f|\mathbb{1}_A\leq|f|\mathbb{1}_{\{|f|>g\}}+g\mathbb{1}_A$$ This leads to another equivalency Theorem 1: A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $a:=\sup_{f\in\mathcal{F}}\|f\|_1<\infty$ , and for any $\varepsilon>0$ there exists $g_\varepsilon\in L^+_1$ and $\delta>0$ such that for any $A\in\mathscr{B}$ , $$\int_Ag_\varepsilon<\delta\qquad\text{implies}\quad \sup_{f\in\mathcal{F}}\int_A |f|\,d\mu<\varepsilon$$ Proof: Necessity is obvious. For sufficiency, suppose $\mathcal{F}$ has the property described in the Theorem above. For $\varepsilon>0$ choose let $a$ , $g_\varepsilon\in L^+_1$ and $\delta_\varepsilon$ as in the statement of the Theorem. For any $c>0$ $$\int_{\{|f|>cg\}}g\,d\mu \leq\frac{1}{c}\int_{\{|f|>cg_\varepsilon\}}|f|\,d\mu\leq\frac{a}{c}$$ Thus, for $c>\frac{a}{\delta_\varepsilon}$ , we obtain that $\int_{\{|f|>cg_\varepsilon\}}g_\varepsilon\,d\mu<\delta_\varepsilon$ and so, $$\int_{\{|f|>cg_\varepsilon\}}|f|\,d\mu<\varepsilon$$ The uniform integrability follows. The case $\mu(\Omega)<\infty$ is of interest in the Theory of Probability. In this case, the infimum in  Definition H (equivalently in Definition H') can be taken over nonnegative numbers: Theorem 2: Suppose $(X,\mathscr{F},\mu)$ is a finite measure space. A family $\mathcal{F}\subset L_1$ is uniformly integrable iff $$\begin{align} \inf_{a\geq0}\sup_{f\in\mathcal{F}}\int_{\{|f|>a\}}|f|\,d\mu=0\tag{4}\label{four} \end{align}$$ or equivalently $$\begin{align} \inf_{a>0}\sup_{f\in\mathcal{F}}\int_X(|f|-a)_+\,d\mu=0\tag{4'}\label{fourp} \end{align}$$ Proof: Given $\varepsilon>0$ choose $g_\varepsilon\in L^+_1$ and $a_\varepsilon\in(0,\infty)$ such that $$\begin{align} \sup_{f\in \mathcal{F}}\int_{\{|f|>g_\varepsilon\}}|f|\,d\mu<\varepsilon/2, \qquad \int_{\{g_\varepsilon>a_\varepsilon\}}g\,d\mu<\varepsilon/2\end{align}$$ Then, for any $f\in \mathcal{F}$ $$\int_{\{|f|>a_\varepsilon\}}|f|\,d\mu\leq\int_{\{|f|>g_\varepsilon\}}|f|\,d\mu+\int_{\{g>a_\varepsilon\}}g_\varepsilon\,d\mu<\varepsilon$$ Conversely, when $\mu(X)<\infty$ , the family of nonnegative constant functions is contained in $L^+_1$ and so, \eqref{four} implies \eqref{one} Comment: Versions of this theorem are stated as a definition in many probability textbooks. For $\sigma$ -finite measures, here is another equivalency found in the literature. Theorem 3: Suppose $(X,\mathscr{B},\mu)$ is $\sigma$ -finite and let $h\in L^+_1$ with $h>0$ a.s. A family $\mathcal{F}\subset L_1$ is uniformly integrable iff (i) $a:=\sup_{f\in\mathcal{F}}\|f\|_1<\infty$ , and (ii) for any $\varepsilon>0$ there is $\delta>0$ such that, for any $A\in\mathscr{B}$ $$\int_Ah\,d\mu<\delta,\qquad\text{then}\quad \sup_{f\in\mathcal{F}}\int_A|f|\,d\mu<\varepsilon$$ Comment: Notice that if $\mu(X)<\infty$ , one may take $h\equiv1$ to obtain another familiar definition of uniform integrability that appears in many probability textbooks. Proof to Theorem 3: $\sigma$ -finiteness implies the existence of functions $h\in L^+_1$ with $h>0$ a.s. If $\mathcal{F}$ is uniformly integrable then, choose $g\in\mathcal{L}^+_1$ such that $$\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu\leq\varepsilon/3$$ Then, for any $f\in\mathcal{F}$ $$\int_X|f|\,d\mu\leq\int_{\{|f|>g\}}|f|\,d\mu+\int_{\{|f|\leq  g\}}g\,d\mu<\varepsilon/3 +\|g\|_1$$ Since $\phi_n:=\mathbb{1}_{\{g>nh\}}\xrightarrow{n\rightarrow\infty}0$ a.s., dominated convergence implies that $\int\phi_n g\,d\mu\xrightarrow{n\rightarrow\infty}0$ . Choose $N\in\mathbb{N}$ large enough so that $\int \phi_n g\,d\mu<\varepsilon/3$ . For any $A\in\mathscr{B}$ $$ |f|\mathbb{1}_A\leq |f|\mathbb{1}_{\{|f|>g\}} + g\mathbb{1}_{\{g>Nh\}}+ Nh\mathbb{1}_A$$ Thus, for $\delta=\varepsilon/(3N)$ , $\int_Ag\,d\mu<\delta$ implies $\sup_{f\in\mathcal{F}}\int_A|f|\,d\mu<\varepsilon$ . The converse follows as in the proof of Theorem 3.","I came across this posting where a definition of uniform integrability (found in Tao, T., Introduction Measure Theory. AMS, GTM vol 126, 2011) is given as follows: Definition T:  Suppose is a measure space (not necessarily finite). A sequence of absolutely Integrable functions is said to be uniformly Integrable if the following three statements hold (Uniform bounded on ) One has . (No escape to vertical infinity) One has . (No escape to width infinity) One has . This is in contrast to the common used definition of uniform integrability by Hunt (Hunt, G.A., Martingales et Precessus de Markov , Paris: Dunod, 1966 pp. 254) which I state as follows: Definition H: A family is uniformly integrable iff Definition H is widely used on Probability theory and in functional analysis, for example, it provides an extension of Pettis-Dunford's theorem for -finite spaces. There are several known equivalences to Hunt's definition, some of which I will state below. Observation: For a finite measure space, condition (3) Definition T is rather superfluous and, as it can be easily seen, Definitions H and T are equivalent in this setting. Problem(s): (a): Are  Definitions H and T  equivalent for general measure spaces (or at least for infinite -finite measures, or only for countable families )? (b): If not, are there any applications (problems) where Definition H does a job that Definition T can't deliver (and vice versa). I spent some considerable amount trying to work through (a). I was unsuccessful to show either of the following statements: Prop 1: Definition H implies Definition T. Definition H does imply condition (1) and (2) in general measure spaces. Whether (3) also holds, escapes me. Prop 2: Definition T implies Definition H. Did not go far at all. If all this is well known, a reference would be appreciated. Here are some well known equivalencies of Definition H. Notation: Suppose is a measure space. When the the ambient space is clear from the context, we use as a shorthand for , and (the -norm). We also use the notation . Given with -a.s., we denote For any and , The following is from K. Bichteler, Integration: A functional approach , Birkhäuser Verlag, 1998. p.p. 57. Definition B: A family is uniformly integrable if for any , there are with such that Comments: For any and , define . It is easy to check that if , then Thus, Definition B may be rewritten as: Definition B': is uniformly integrable iff Comments: Definition B means that  a family is uniformly integrable if their elements are close to being dominated by an integrable function. Now, for , ; hence, as for all ,  Definition B  can also be rewritten as (see, for example (Klenke, A., Probability Theory, Springer 2006. pp. 134) Definition H': A family is uniformly integrable iff Comments: Observe that if , then Hence, if is uniformly integrable in the sense of Definition H' then, that is, is uniformly integrable in the sense of Definition H.  Conversely, notice that for any Hence, if satisfies \eqref{one} then, it is uniformly integrable in the sense of Definition H'. This establishes the equivalence of Definitions B, H and H'. Notice that for any and This leads to another equivalency Theorem 1: A family is uniformly integrable iff , and for any there exists and such that for any , Proof: Necessity is obvious. For sufficiency, suppose has the property described in the Theorem above. For choose let , and as in the statement of the Theorem. For any Thus, for , we obtain that and so, The uniform integrability follows. The case is of interest in the Theory of Probability. In this case, the infimum in  Definition H (equivalently in Definition H') can be taken over nonnegative numbers: Theorem 2: Suppose is a finite measure space. A family is uniformly integrable iff or equivalently Proof: Given choose and such that Then, for any Conversely, when , the family of nonnegative constant functions is contained in and so, \eqref{four} implies \eqref{one} Comment: Versions of this theorem are stated as a definition in many probability textbooks. For -finite measures, here is another equivalency found in the literature. Theorem 3: Suppose is -finite and let with a.s. A family is uniformly integrable iff (i) , and (ii) for any there is such that, for any Comment: Notice that if , one may take to obtain another familiar definition of uniform integrability that appears in many probability textbooks. Proof to Theorem 3: -finiteness implies the existence of functions with a.s. If is uniformly integrable then, choose such that Then, for any Since a.s., dominated convergence implies that . Choose large enough so that . For any Thus, for , implies . The converse follows as in the proof of Theorem 3.","(X,\mathscr{B},\mu) f_n:X \rightarrow \mathbb{C} L^1 \sup_n\|f_n\|_{L^1(\mu)}=\sup_n\int_{X}|f_n|d\mu <+\infty \sup_n\int_{|f_n|\ge M}|f_n|d\mu\xrightarrow{M\rightarrow\infty} 0 \sup_n\int_{|f_n|\le\delta}|f_n|d\mu\xrightarrow{\delta\rightarrow0} 0 \mathcal{F}\subset L_1 \begin{align}
\inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu=0
\tag{1}\label{one}
\end{align} \sigma \sigma \mathcal{F}\subset L_1 (X,\mathscr{B},\mu) L_1 L_1(X,\mathscr{B},\mu) \|f\|_1:=\int|f|\,d\mu L_1 L^+_1=\{f\in L_1: f\geq0\} g,h\in L_1 g\leq h \mu [g,h]=\{f\in L_1: g\leq f\leq h\} \mathcal{A}\subset L_1 f\in L_1 d(f,\mathcal{A}):=\inf\{\|f-\phi\|_1: \phi\in\mathcal{A}\} \mathcal{F}\subset L_1 \varepsilon>0 g,h\in L_1 g\leq h \begin{align}\sup_{f\in\mathcal{F}}d(f,[g,h])<\varepsilon\tag{2}\label{2}
\end{align} x\in\mathbb{R} a<b x^b_a:=(a\vee x)\wedge b c\leq a\leq b\leq d |x-x^d_c|\leq |x-x^b_a| \mathcal{F}\subset L_1 \inf_{g\in L^+_1}\sup_{f\in \mathcal{F}}d(f,[-g,g])=0 \mathcal{F}\subset L_1 g\geq0 |f-f^g_{-g}|=(|f|-g)_+ |x-x^a_{-a}|\leq |x-b| |b|\leq a \mathcal{F}\subset L_1 \begin{align}
\inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_X(|f|-g)_+\,d\mu=0 \tag{1'}\label{onep}
\end{align} g\geq0 |f|\mathbb{1}_{\{|f|>2g\}}\leq (|f|-g)_+\mathbb{1}_{\{|f|>2g\}}+g\mathbb{1}_{\{|f|>2g\}}\leq 2(|f|-g)_+\mathbb{1}_{|f|>2g\}} \mathcal{F} \begin{align}
\inf_{g\in L^+_1}\sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu=0,
\end{align} \mathcal{F} g\geq0 (|f|-g)_+\leq|f|\mathbb{1}_{\{|f|>g\}}. \mathcal{F}\subset L_1 A\in\mathscr{B} g\geq0 |f|\mathbb{1}_A\leq|f|\mathbb{1}_{\{|f|>g\}}+g\mathbb{1}_A \mathcal{F}\subset L_1 a:=\sup_{f\in\mathcal{F}}\|f\|_1<\infty \varepsilon>0 g_\varepsilon\in L^+_1 \delta>0 A\in\mathscr{B} \int_Ag_\varepsilon<\delta\qquad\text{implies}\quad \sup_{f\in\mathcal{F}}\int_A |f|\,d\mu<\varepsilon \mathcal{F} \varepsilon>0 a g_\varepsilon\in L^+_1 \delta_\varepsilon c>0 \int_{\{|f|>cg\}}g\,d\mu \leq\frac{1}{c}\int_{\{|f|>cg_\varepsilon\}}|f|\,d\mu\leq\frac{a}{c} c>\frac{a}{\delta_\varepsilon} \int_{\{|f|>cg_\varepsilon\}}g_\varepsilon\,d\mu<\delta_\varepsilon \int_{\{|f|>cg_\varepsilon\}}|f|\,d\mu<\varepsilon \mu(\Omega)<\infty (X,\mathscr{F},\mu) \mathcal{F}\subset L_1 \begin{align}
\inf_{a\geq0}\sup_{f\in\mathcal{F}}\int_{\{|f|>a\}}|f|\,d\mu=0\tag{4}\label{four}
\end{align} \begin{align}
\inf_{a>0}\sup_{f\in\mathcal{F}}\int_X(|f|-a)_+\,d\mu=0\tag{4'}\label{fourp}
\end{align} \varepsilon>0 g_\varepsilon\in L^+_1 a_\varepsilon\in(0,\infty) \begin{align}
\sup_{f\in \mathcal{F}}\int_{\{|f|>g_\varepsilon\}}|f|\,d\mu<\varepsilon/2, \qquad
\int_{\{g_\varepsilon>a_\varepsilon\}}g\,d\mu<\varepsilon/2\end{align} f\in \mathcal{F} \int_{\{|f|>a_\varepsilon\}}|f|\,d\mu\leq\int_{\{|f|>g_\varepsilon\}}|f|\,d\mu+\int_{\{g>a_\varepsilon\}}g_\varepsilon\,d\mu<\varepsilon \mu(X)<\infty L^+_1 \sigma (X,\mathscr{B},\mu) \sigma h\in L^+_1 h>0 \mathcal{F}\subset L_1 a:=\sup_{f\in\mathcal{F}}\|f\|_1<\infty \varepsilon>0 \delta>0 A\in\mathscr{B} \int_Ah\,d\mu<\delta,\qquad\text{then}\quad \sup_{f\in\mathcal{F}}\int_A|f|\,d\mu<\varepsilon \mu(X)<\infty h\equiv1 \sigma h\in L^+_1 h>0 \mathcal{F} g\in\mathcal{L}^+_1 \sup_{f\in\mathcal{F}}\int_{\{|f|>g\}}|f|\,d\mu\leq\varepsilon/3 f\in\mathcal{F} \int_X|f|\,d\mu\leq\int_{\{|f|>g\}}|f|\,d\mu+\int_{\{|f|\leq  g\}}g\,d\mu<\varepsilon/3 +\|g\|_1 \phi_n:=\mathbb{1}_{\{g>nh\}}\xrightarrow{n\rightarrow\infty}0 \int\phi_n g\,d\mu\xrightarrow{n\rightarrow\infty}0 N\in\mathbb{N} \int \phi_n g\,d\mu<\varepsilon/3 A\in\mathscr{B} 
|f|\mathbb{1}_A\leq |f|\mathbb{1}_{\{|f|>g\}} + g\mathbb{1}_{\{g>Nh\}}+ Nh\mathbb{1}_A \delta=\varepsilon/(3N) \int_Ag\,d\mu<\delta \sup_{f\in\mathcal{F}}\int_A|f|\,d\mu<\varepsilon","['real-analysis', 'integration', 'measure-theory']"
33,Is this function nowhere differentiable?,Is this function nowhere differentiable?,,"I was looking at the following function, $$\displaystyle f(x) := \sum_{n=0}^\infty {\sin(2^nx) \over 2^n}.$$ It is pretty obvious that $f$ is continous everywhere in $\mathbb{R}$ . But I can't figure out where it is differentiable. Differentiating term by term would lead me to believe it is differentiable nowhere but I'm not sure if I can do that.","I was looking at the following function, It is pretty obvious that is continous everywhere in . But I can't figure out where it is differentiable. Differentiating term by term would lead me to believe it is differentiable nowhere but I'm not sure if I can do that.",\displaystyle f(x) := \sum_{n=0}^\infty {\sin(2^nx) \over 2^n}. f \mathbb{R},"['real-analysis', 'sequences-and-series', 'derivatives']"
34,Interesting inequality $\frac{x^{m+1}+1}{x^m+1} \ge \sqrt[2m+1]{\frac{1+x^{2m+1}}{2}}$,Interesting inequality,\frac{x^{m+1}+1}{x^m+1} \ge \sqrt[2m+1]{\frac{1+x^{2m+1}}{2}},Prove that $$\frac{x^{m+1}+1}{x^m+1} \ge \sqrt[k]{\frac{1+x^k}{2}}$$ for all real $x \ge 1$ and for all positive integers $m$ and $k \le 2m+1$ . My work. If $k \le 2m+1$ then $$\sqrt[k]{\frac{1+x^k}{2}}\le \sqrt[2m+1]{\frac{1+x^{2m+1}}{2}}.$$ I need prove that for all real $x \ge 1$ and for all positive integers $m$ the inequality $$\frac{x^{m+1}+1}{x^m+1} \ge \sqrt[2m+1]{\frac{1+x^{2m+1}}{2}}$$ holds.,Prove that for all real and for all positive integers and . My work. If then I need prove that for all real and for all positive integers the inequality holds.,\frac{x^{m+1}+1}{x^m+1} \ge \sqrt[k]{\frac{1+x^k}{2}} x \ge 1 m k \le 2m+1 k \le 2m+1 \sqrt[k]{\frac{1+x^k}{2}}\le \sqrt[2m+1]{\frac{1+x^{2m+1}}{2}}. x \ge 1 m \frac{x^{m+1}+1}{x^m+1} \ge \sqrt[2m+1]{\frac{1+x^{2m+1}}{2}},"['real-analysis', 'inequality']"
35,"The function $f'+f'''$ has at least $3$ zeros on $[0,2\pi]$.",The function  has at least  zeros on .,"f'+f''' 3 [0,2\pi]","Show that if $f\in \mathcal{C}^3$ and $2\cdot\pi$ periodic then the function $f'+f'''$ has at least $3$ zeros on $[0,2\pi]$. My attempt : f is $2\pi$ periodic and $\mathcal{C}^3$, we have : $$\lim_{h\rightarrow 0, h>0} \frac{f(h)-f(0)}{h}=\lim_{h\rightarrow 0, h>0} \frac{f(2\pi+h)-f(2\pi)}{h}\Rightarrow f'(0)=f'(2\pi)$$ After that I tried to compute differently the limit  $$ \lim_{h\rightarrow 0, h>0}\frac{f(h)-f(0)}{h}=\lim_{h\rightarrow 0, h<0}\frac{f(h)-f(0)}{|h|}=\lim_{h\rightarrow 0, h<0}-\frac{f(2\pi+h)-f(0)}{h}\Rightarrow f'(0)=-f'(2\pi)  $$ wich is clearly false because I get $f'(0)=0$ ( $f(x)=\sin(x)$ is a counterexample). For $2$ zeros is relatively easy but I am stuck for the additional zero. EDIT : I find this exercice (as usual) here : Revue de la filière Mathématique. This was asked during an oral examination of École normale supérieure rue d'Ulm.","Show that if $f\in \mathcal{C}^3$ and $2\cdot\pi$ periodic then the function $f'+f'''$ has at least $3$ zeros on $[0,2\pi]$. My attempt : f is $2\pi$ periodic and $\mathcal{C}^3$, we have : $$\lim_{h\rightarrow 0, h>0} \frac{f(h)-f(0)}{h}=\lim_{h\rightarrow 0, h>0} \frac{f(2\pi+h)-f(2\pi)}{h}\Rightarrow f'(0)=f'(2\pi)$$ After that I tried to compute differently the limit  $$ \lim_{h\rightarrow 0, h>0}\frac{f(h)-f(0)}{h}=\lim_{h\rightarrow 0, h<0}\frac{f(h)-f(0)}{|h|}=\lim_{h\rightarrow 0, h<0}-\frac{f(2\pi+h)-f(0)}{h}\Rightarrow f'(0)=-f'(2\pi)  $$ wich is clearly false because I get $f'(0)=0$ ( $f(x)=\sin(x)$ is a counterexample). For $2$ zeros is relatively easy but I am stuck for the additional zero. EDIT : I find this exercice (as usual) here : Revue de la filière Mathématique. This was asked during an oral examination of École normale supérieure rue d'Ulm.",,['real-analysis']
36,"""Strong"" derivative of a monotone function","""Strong"" derivative of a monotone function",,"It is well known that if a function $f\colon \mathbb{R} \to \mathbb{R}$ is monotone then $f'$ exists almost everywhere. Is it true that if $f$ is monotone then there exists ( edit: I mean exists a.e.) ""strong"" derivative $$ f'_s(x) = \lim_{(y,z)\to (x,x)} \frac{f(y) - f(z)}{y-z}? $$ (If this is not true, what about the case when $y \nearrow x$ and $z \nearrow x$?) In general (if $f$ is not monotone) it is not true of course. For example $x \mapsto x^2 \sin{\frac{1}{x}}$ has derivative at $0$ but does not have ""strong"" derivative at $0$. Moreover if $f'$ is continuous at $x$ then the strong derivative $f'_s(x)$ exists and is of course equal to $f'(x)$. This follows easily from mean value theorem.","It is well known that if a function $f\colon \mathbb{R} \to \mathbb{R}$ is monotone then $f'$ exists almost everywhere. Is it true that if $f$ is monotone then there exists ( edit: I mean exists a.e.) ""strong"" derivative $$ f'_s(x) = \lim_{(y,z)\to (x,x)} \frac{f(y) - f(z)}{y-z}? $$ (If this is not true, what about the case when $y \nearrow x$ and $z \nearrow x$?) In general (if $f$ is not monotone) it is not true of course. For example $x \mapsto x^2 \sin{\frac{1}{x}}$ has derivative at $0$ but does not have ""strong"" derivative at $0$. Moreover if $f'$ is continuous at $x$ then the strong derivative $f'_s(x)$ exists and is of course equal to $f'(x)$. This follows easily from mean value theorem.",,"['real-analysis', 'derivatives']"
37,"Find $f(x,y )$ such that $f_{x},f_{y},f_{yx}$ are continuous,but $f_{xy}$ is not","Find  such that  are continuous,but  is not","f(x,y ) f_{x},f_{y},f_{yx} f_{xy}","Let $f$ be a function  of two variables,let$(a,b)$ be a point and let $D$ be an open disk with center $(a,b)$. Assume that $f$ is $\mathcal C^{1}$ on $D$, and $f_{yx}$ exist on $D$. Further, the  mixed second partial derivative $f_{yx}$ is continuous at $(a,b)$,is the other mixed second partial derivative  $f_{xy}$ continuous at $(a,b)?$ I think there must be some counterexample to deny it, I tried to find them. Do you have some nice counterexamples?","Let $f$ be a function  of two variables,let$(a,b)$ be a point and let $D$ be an open disk with center $(a,b)$. Assume that $f$ is $\mathcal C^{1}$ on $D$, and $f_{yx}$ exist on $D$. Further, the  mixed second partial derivative $f_{yx}$ is continuous at $(a,b)$,is the other mixed second partial derivative  $f_{xy}$ continuous at $(a,b)?$ I think there must be some counterexample to deny it, I tried to find them. Do you have some nice counterexamples?",,"['calculus', 'real-analysis']"
38,On finding polynomials that approximate a function and its derivative (extensions of Stone-Weierstrass?),On finding polynomials that approximate a function and its derivative (extensions of Stone-Weierstrass?),,"The Stone-Weierstrass Theorem tells us that we can approximate any continuous $f:\mathbb{R}^n\to\mathbb{R}$ arbitrary well on a compact subset of $\mathbb{R}^n$ by some polynomial. Suppose that $f$ is continuously differentiable, under what conditions can we guarantee we can find a polynomial $p$ such that $p$ approximates arbitrarily well $f$ and the partial derivatives approximate of $p$ arbitrarily well those of $f$ (on a compact subset of $\mathbb{R}^n$)? References are welcome. See below for my (probably naive) approach to the question. It is straightforward to find conditions if $n=1$: Let $K:=[a,b]\subseteq\mathbb{R}$ and let $K_+$ denote some open cover of $K$. Suppose that $f:K_+\to\mathbb{R}$ is continuously differentiable and let $f'$ denote its derivative. Then, by Weierstrass's approximation Theorem we have that there exists a polynomial $q$ such that $$||f'(x)-q(x)||\leq \varepsilon$$ for all $x\in K$. Choose some $y\in K$. For any $x\in K$ define $$p(x):=f(y)+\int_y^xq(t)dt.\quad\quad(*)$$ By the Fundamental Theorem of calculus we have that $p'=q$. So for any $x\in K$ $$||f(x)-p(x)||=\left|\left|\int_y^x (f'(t)-q(t))dt\right|\right|\leq \int_y^x ||f'(t)-q(t)||dt\leq \varepsilon||x-y||\leq \varepsilon (a-b).$$ However, the above argument doesn't work in higher dimensions because the analogue of $(*)$ does not imply that $\nabla p = q$ (since not every vector of polynomials spans a gradient field). Can anyone suggest a way around this? That is, a way to to show that for continuously differentiable $f:\mathbb{R}^n\to\mathbb{R}$ and for any $\varepsilon>0$, we can always find a polynomial $p$ such that $$||f(x)-p(x)||+||\nabla f(x)-\nabla p(x)||\leq \varepsilon$$ for all $x$ in some compact set $K$? Feel free to add extra conditions on $K$, if it makes life easier.","The Stone-Weierstrass Theorem tells us that we can approximate any continuous $f:\mathbb{R}^n\to\mathbb{R}$ arbitrary well on a compact subset of $\mathbb{R}^n$ by some polynomial. Suppose that $f$ is continuously differentiable, under what conditions can we guarantee we can find a polynomial $p$ such that $p$ approximates arbitrarily well $f$ and the partial derivatives approximate of $p$ arbitrarily well those of $f$ (on a compact subset of $\mathbb{R}^n$)? References are welcome. See below for my (probably naive) approach to the question. It is straightforward to find conditions if $n=1$: Let $K:=[a,b]\subseteq\mathbb{R}$ and let $K_+$ denote some open cover of $K$. Suppose that $f:K_+\to\mathbb{R}$ is continuously differentiable and let $f'$ denote its derivative. Then, by Weierstrass's approximation Theorem we have that there exists a polynomial $q$ such that $$||f'(x)-q(x)||\leq \varepsilon$$ for all $x\in K$. Choose some $y\in K$. For any $x\in K$ define $$p(x):=f(y)+\int_y^xq(t)dt.\quad\quad(*)$$ By the Fundamental Theorem of calculus we have that $p'=q$. So for any $x\in K$ $$||f(x)-p(x)||=\left|\left|\int_y^x (f'(t)-q(t))dt\right|\right|\leq \int_y^x ||f'(t)-q(t)||dt\leq \varepsilon||x-y||\leq \varepsilon (a-b).$$ However, the above argument doesn't work in higher dimensions because the analogue of $(*)$ does not imply that $\nabla p = q$ (since not every vector of polynomials spans a gradient field). Can anyone suggest a way around this? That is, a way to to show that for continuously differentiable $f:\mathbb{R}^n\to\mathbb{R}$ and for any $\varepsilon>0$, we can always find a polynomial $p$ such that $$||f(x)-p(x)||+||\nabla f(x)-\nabla p(x)||\leq \varepsilon$$ for all $x$ in some compact set $K$? Feel free to add extra conditions on $K$, if it makes life easier.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'approximation']"
39,Integrability: Neither improper Riemann nor Lebesgue but Henstock-Kurzweil,Integrability: Neither improper Riemann nor Lebesgue but Henstock-Kurzweil,,"Can you think of a function that is neither improper Riemann nor Lebesgue integrable, but is Henstock-Kurzweil integrable? I'd like to put a bounty on this question, but my reputation is not nearly enough yet. Translated to math, find $f$ such that $$ f \notin \mathscr{L,R^*} $$  but  $$ f\in \mathscr{HK} $$  where $\mathscr{HK}$ denotes the set of Henstock-Kurzweil integrable functions.","Can you think of a function that is neither improper Riemann nor Lebesgue integrable, but is Henstock-Kurzweil integrable? I'd like to put a bounty on this question, but my reputation is not nearly enough yet. Translated to math, find $f$ such that $$ f \notin \mathscr{L,R^*} $$  but  $$ f\in \mathscr{HK} $$  where $\mathscr{HK}$ denotes the set of Henstock-Kurzweil integrable functions.",,"['real-analysis', 'integration', 'examples-counterexamples', 'gauge-integral']"
40,Evaluating the challenging sum $\sum _{k=1}^{\infty }\frac{H_{2k}}{k^3\:4^k}\binom{2k}{k}$.,Evaluating the challenging sum .,\sum _{k=1}^{\infty }\frac{H_{2k}}{k^3\:4^k}\binom{2k}{k},"I managed to evaluate the sum , my approach can be found $\underline{\operatorname{below as an answer}}$ , I'd truly appreciate if any of you could share new methods to evaluate this series, thank you. The following are the short proofs of the other series I encountered during my approach. $$\sum _{k=1}^{\infty }\frac{x^k}{4^k}\binom{2k}{k}=\frac{1}{\sqrt{1-x}}-1\tag{$\ast$}$$ $$\sum _{k=1}^{\infty }\frac{x^k}{k\:4^k}\binom{2k}{k}=\:-2\ln \left(1+\sqrt{1-x}\right)+2\ln \left(2\right)$$ $$\sum _{k=1}^{\infty }\frac{1}{k\:4^k}\binom{2k}{k}=\:2\ln \left(2\right)$$ On $\left(\ast\right)$ perform the following manipulations. $$-\sum _{k=1}^{\infty }\frac{1}{4^k}\binom{2k}{k}\int _0^1x^{k-1}\ln \left(x\right)\:dx=-\int _0^1\frac{\ln \left(x\right)\left(1-\sqrt{1-x}\right)}{x\sqrt{1-x}}\:dx$$ $$\sum _{k=1}^{\infty }\frac{1}{k^2\:4^k}\binom{2k}{k}=-2\int _0^1\frac{\ln \left(1-x^2\right)}{1+x}\:dx$$ $$=-2\int _0^1\frac{\ln \left(1-x\right)}{1+x}\:dx-2\int _0^1\frac{\ln \left(1+x\right)}{1+x}\:dx$$ $$=2\operatorname{Li}_2\left(\frac{1}{2}\right)-\ln ^2\left(2\right)$$ $$\sum _{k=1}^{\infty }\frac{1}{k^2\:4^k}\binom{2k}{k}=\zeta \left(2\right)-2\ln ^2\left(2\right)$$","I managed to evaluate the sum , my approach can be found , I'd truly appreciate if any of you could share new methods to evaluate this series, thank you. The following are the short proofs of the other series I encountered during my approach. On perform the following manipulations.",\underline{\operatorname{below as an answer}} \sum _{k=1}^{\infty }\frac{x^k}{4^k}\binom{2k}{k}=\frac{1}{\sqrt{1-x}}-1\tag{\ast} \sum _{k=1}^{\infty }\frac{x^k}{k\:4^k}\binom{2k}{k}=\:-2\ln \left(1+\sqrt{1-x}\right)+2\ln \left(2\right) \sum _{k=1}^{\infty }\frac{1}{k\:4^k}\binom{2k}{k}=\:2\ln \left(2\right) \left(\ast\right) -\sum _{k=1}^{\infty }\frac{1}{4^k}\binom{2k}{k}\int _0^1x^{k-1}\ln \left(x\right)\:dx=-\int _0^1\frac{\ln \left(x\right)\left(1-\sqrt{1-x}\right)}{x\sqrt{1-x}}\:dx \sum _{k=1}^{\infty }\frac{1}{k^2\:4^k}\binom{2k}{k}=-2\int _0^1\frac{\ln \left(1-x^2\right)}{1+x}\:dx =-2\int _0^1\frac{\ln \left(1-x\right)}{1+x}\:dx-2\int _0^1\frac{\ln \left(1+x\right)}{1+x}\:dx =2\operatorname{Li}_2\left(\frac{1}{2}\right)-\ln ^2\left(2\right) \sum _{k=1}^{\infty }\frac{1}{k^2\:4^k}\binom{2k}{k}=\zeta \left(2\right)-2\ln ^2\left(2\right),"['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals', 'harmonic-numbers']"
41,Every linear operator $T:X \to Y$ on a finite-dimensional normed space is bounded,Every linear operator  on a finite-dimensional normed space is bounded,T:X \to Y,"Exercise : Show that if $X$ is a finite-dimensional normed space and $Y$ is a normed space, then every linear operator $T:X \to Y$ is bounded. Attempt : Since $X$ is finite-dimensional, say $\dim(X)=n$ , there exists a basis $\{e1,e2,...,en\}$ of $X$ such that every element $x\in X$ can be written uniquely in the form: $$\begin{align} \quad x = a_1e_1 + a_2e_2 + ... + a_ne_n \end{align}$$ where $a_1,a_2,\dots,a_n \in \mathbb R$ . Now, $\forall \; x \in X$ , it is : $$\begin{align} \quad \| T(x) \|_Y &= \| T (a_1e_1 + a_2e_2 + ... + a_ne_n) \|_Y \\ &= \| a_1 T(e_1) + a_2 T(e_2) + ... + a_n T(e_n) \|_Y \\ & \leq \sum_{k=1}^{n} |a_k| \| T(e_k) \|_Y \end{align}$$ Using the Cauchy-Schwarz inequality, we yield : $$\begin{align} \quad \| T(x) \| & \leq \left ( \sum_{k=1}^{n} |a_k|^2 \right )^{1/2} \left ( \sum_{k=1}^{n} \| T(e_k) \|_Y^2 \right )^{1/2} \\ & \leq \left ( \sum_{k=1}^{n} |a_k|^2 \right )^{1/2} \cdot M \end{align}$$ But regarding equivalence of norm in correlation to finite-dimensional spaces, we have that : $$\begin{align} \quad \| T(x) \|_Y & \leq M \| x \|_* \end{align}$$ Then, $\exists c_1,c_2 \in \mathbb R^+ : \forall x \in X$ it is : $$\begin{align} \quad c_1 \| x \|_X \leq \| x \|_* \leq c_2 \| x \|_X \end{align}$$ Thus $\forall x \in X$ it is : $$\begin{align} \quad \| T(x) \| & \leq c_1M \| x \|_X \end{align}$$ which tells us that $T$ is bounded. Question : It seemed like a rather hard exercise to me so I am not sure if my proof/approach is definitely correct or rigorous enough. Any insight will be very helpful !","Exercise : Show that if is a finite-dimensional normed space and is a normed space, then every linear operator is bounded. Attempt : Since is finite-dimensional, say , there exists a basis of such that every element can be written uniquely in the form: where . Now, , it is : Using the Cauchy-Schwarz inequality, we yield : But regarding equivalence of norm in correlation to finite-dimensional spaces, we have that : Then, it is : Thus it is : which tells us that is bounded. Question : It seemed like a rather hard exercise to me so I am not sure if my proof/approach is definitely correct or rigorous enough. Any insight will be very helpful !","X Y T:X \to Y X \dim(X)=n \{e1,e2,...,en\} X x\in X \begin{align} \quad x = a_1e_1 + a_2e_2 + ... + a_ne_n \end{align} a_1,a_2,\dots,a_n \in \mathbb R \forall \; x \in X \begin{align} \quad \| T(x) \|_Y &= \| T (a_1e_1 + a_2e_2 + ... + a_ne_n) \|_Y \\ &= \| a_1 T(e_1) + a_2 T(e_2) + ... + a_n T(e_n) \|_Y \\ & \leq \sum_{k=1}^{n} |a_k| \| T(e_k) \|_Y \end{align} \begin{align} \quad \| T(x) \| & \leq \left ( \sum_{k=1}^{n} |a_k|^2 \right )^{1/2} \left ( \sum_{k=1}^{n} \| T(e_k) \|_Y^2 \right )^{1/2} \\ & \leq \left ( \sum_{k=1}^{n} |a_k|^2 \right )^{1/2} \cdot M \end{align} \begin{align} \quad \| T(x) \|_Y & \leq M \| x \|_* \end{align} \exists c_1,c_2 \in \mathbb R^+ : \forall x \in X \begin{align} \quad c_1 \| x \|_X \leq \| x \|_* \leq c_2 \| x \|_X \end{align} \forall x \in X \begin{align} \quad \| T(x) \| & \leq c_1M \| x \|_X \end{align} T","['real-analysis', 'functional-analysis', 'proof-verification', 'operator-theory']"
42,Integrals of the form $\int_0^{+\infty} \sin g(x) \ dx$,Integrals of the form,\int_0^{+\infty} \sin g(x) \ dx,"I'm interested in the convergence of integrals of the form $$\int_0^{+\infty} \sin g(x) \ dx$$ where $g$ is nonnegative, increasing and growing without bound as $x \to +\infty$ (hence $\sin g(x)$ oscillates as $x \to +\infty$). For example, it's known that $$\int_0^{+\infty} \sin x^p \ dx$$ converges for $p>1$. Similarly, $\int_0^{+\infty} \sin(\exp x) \ dx$ converges. Here are three questions (they are related enough that I didn't think it was worth making three separate questions). Consider functions $g:[0, +\infty) \to [0, +\infty)$ which are continuous, strictly increasing and unbounded. $(i)$ Suppose we add the additional hypothesis that $g$ is strictly convex. Does $\int_0^{+\infty} \sin g(x) \ dx$ converge? $(ii)$ Can we characterize those $g$'s for which $\int_0^{+\infty} \sin g(x) \ dx$ converges? $(iii)$ Is there a $g$ such that $\int_0^{+\infty} \sin g(x) \ dx$ converges absolutely?","I'm interested in the convergence of integrals of the form $$\int_0^{+\infty} \sin g(x) \ dx$$ where $g$ is nonnegative, increasing and growing without bound as $x \to +\infty$ (hence $\sin g(x)$ oscillates as $x \to +\infty$). For example, it's known that $$\int_0^{+\infty} \sin x^p \ dx$$ converges for $p>1$. Similarly, $\int_0^{+\infty} \sin(\exp x) \ dx$ converges. Here are three questions (they are related enough that I didn't think it was worth making three separate questions). Consider functions $g:[0, +\infty) \to [0, +\infty)$ which are continuous, strictly increasing and unbounded. $(i)$ Suppose we add the additional hypothesis that $g$ is strictly convex. Does $\int_0^{+\infty} \sin g(x) \ dx$ converge? $(ii)$ Can we characterize those $g$'s for which $\int_0^{+\infty} \sin g(x) \ dx$ converges? $(iii)$ Is there a $g$ such that $\int_0^{+\infty} \sin g(x) \ dx$ converges absolutely?",,"['calculus', 'real-analysis', 'integration']"
43,Cantor set in base 3.,Cantor set in base 3.,,"I'm trying to prove the cantor set $C$ is equivalent to the set of all numbers with ternary expansion of $2$'s and $0$'s. That is: Let $A_0=[0,1]$. $A_n$ is defined to equal $A_{n-1}$ with it's middle third removed. let $C=\bigcap_{n\in\mathbb{N}} A_n$.  Prove $C =\{x=0.a_1a_2a_3...| a_n\in\{0,2\} \text{ for all }n\in\mathbb{N} \} $ where the decimal expansion is in base $3$. Let $x=0.a_1a_2a_3...$ $\space$ be a ternary expansion of $x$ My strategy is this: Prove that $a_n=1 \iff x\notin A_n$ (a) Proceed by induction on $n$.  For $n=1$ we have: $a_1 = 1 \iff \frac{1}{3} \le  x < \frac{2}{3} \iff x \notin A_1$ Assume it's true for $n$ that is:$\space a_n = 1 \iff x \notin A_n$. For $n+1$ we have: $a_{n+1}=1 \iff \text{there's some integer $m$ such that:} \frac{1}{3} \le 3^{n}x - m  < \frac{2}{3} $ Furthermore $m = a_1a_2a_3...a_{n}$ (in base 3 digit expansion). Here i got stuck.","I'm trying to prove the cantor set $C$ is equivalent to the set of all numbers with ternary expansion of $2$'s and $0$'s. That is: Let $A_0=[0,1]$. $A_n$ is defined to equal $A_{n-1}$ with it's middle third removed. let $C=\bigcap_{n\in\mathbb{N}} A_n$.  Prove $C =\{x=0.a_1a_2a_3...| a_n\in\{0,2\} \text{ for all }n\in\mathbb{N} \} $ where the decimal expansion is in base $3$. Let $x=0.a_1a_2a_3...$ $\space$ be a ternary expansion of $x$ My strategy is this: Prove that $a_n=1 \iff x\notin A_n$ (a) Proceed by induction on $n$.  For $n=1$ we have: $a_1 = 1 \iff \frac{1}{3} \le  x < \frac{2}{3} \iff x \notin A_1$ Assume it's true for $n$ that is:$\space a_n = 1 \iff x \notin A_n$. For $n+1$ we have: $a_{n+1}=1 \iff \text{there's some integer $m$ such that:} \frac{1}{3} \le 3^{n}x - m  < \frac{2}{3} $ Furthermore $m = a_1a_2a_3...a_{n}$ (in base 3 digit expansion). Here i got stuck.",,"['real-analysis', 'general-topology']"
44,Prove that $|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2}$,Prove that,|f''(\xi)|\geqslant\frac{4|f(a)-f(b)|}{(b-a)^2},"Let ${\rm f}:\left[a, b\right]\to\mathbb{R}$ be twice differentiable, and suppose $$\lim_{x\to a^{+}} \frac{{\rm f}\left(x\right) - {\rm f}\left(a\right)}{x - a} = \lim_{x\to b^{-}}\frac{{\rm f}\left(x\right) - {\rm f}\left(b\right)}{x - b} =0 $$ Show that there exists $\xi \in \left(a, b\right)$ such that $\displaystyle{% \left\vert\vphantom{\Large A}\,{\rm f}''\left(\xi\right)\right\vert \geq \frac{4\left\vert\vphantom{\Large A}% \,{\rm f}\left(a\right) - {\rm f}\left(b\right)\right\vert} {\left(b - a\right)^{2}}}$. I don't know how to start. Any hints ?.","Let ${\rm f}:\left[a, b\right]\to\mathbb{R}$ be twice differentiable, and suppose $$\lim_{x\to a^{+}} \frac{{\rm f}\left(x\right) - {\rm f}\left(a\right)}{x - a} = \lim_{x\to b^{-}}\frac{{\rm f}\left(x\right) - {\rm f}\left(b\right)}{x - b} =0 $$ Show that there exists $\xi \in \left(a, b\right)$ such that $\displaystyle{% \left\vert\vphantom{\Large A}\,{\rm f}''\left(\xi\right)\right\vert \geq \frac{4\left\vert\vphantom{\Large A}% \,{\rm f}\left(a\right) - {\rm f}\left(b\right)\right\vert} {\left(b - a\right)^{2}}}$. I don't know how to start. Any hints ?.",,"['calculus', 'real-analysis', 'analysis']"
45,"Choice of $\delta$ for ""brute force"" proof of continuity of exponential function $e^x$","Choice of  for ""brute force"" proof of continuity of exponential function",\delta e^x,"I have read several answers ( example 1 , example 2 ) about continuity of $e^x$ , but most rely on Power Series definition of $e^x$ , or sequential definition of a limit, or squeeze theorem. I would like a brute-force proof that meets the following criteria: Does NOT use sequential definition of limit Does NOT use Squeeze Theorem Uses $\epsilon-\delta$ definition of continuity directly Does NOT use perturbations (e.g. $|e^{a + h} - e^a|$ ) Uses definition of limit, starting with a $0 < |x-a| < \delta$ and ending with $|e^x - e^a| < \epsilon$ Is NOT based on power series definition of $e^x$ Is based on elementary limit definition $e^x = \lim_{n \to 0} (1+n)^{\frac{x}{n}}$ I would like to use exponential bounds (which come from Bernoulli's Inequality) like this answer : \begin{align*} y+1 \le \ & \ e^y \le \frac{1}{1-y} \\ \to \quad \quad y \le \ & \ e^y - 1 \ \le \ \frac{y}{1-y} \\ \to \quad x-a \le \ & \ e^{x-a}-1 \ \le \ \frac{x-a}{1-(x-a)} \end{align*} except I am trying to modify that proof so it doesn't depend on Squeeze Theorem. Proof attempt : Let $\epsilon > 0$ and $a > 0$ arbitrary. Choose $\delta = \frac{\epsilon}{e^a}$ . Then \begin{align*} & \quad 0 < |x - a| < \delta \quad \quad \quad \textrm{ (Given)}\\ &\to \quad |e^{x-a}-1| \quad < \delta \quad \quad \textrm{ (Reason unknown?)} \\ &\to \quad |e^{x-a}-1| < \frac{\epsilon}{e^a} \quad \quad \textrm{ (Substitute $\delta=\frac{\epsilon}{e^a}$)} \\ &\to \quad e^a|e^{x-a}-1| < \epsilon \quad \quad \textrm{ (Multiply both sides by $e^a$)} \\ &\to \quad |e^x-e^a| < \epsilon \quad \quad  \quad \textrm{ (Distribute $e^a$ into absolute value)} \\ & \to \quad \lim_{x \to a} e^x = e^a \quad \quad \quad \textrm{ (Definition of limit)} \end{align*} I know my proof is supposed to use the exponential bounds, $$x-a \le e^{x-a}-1 \le \frac{x-a}{1-(x-a)},$$ so I tried using it (probably incorrectly) in step 2. Just because $|x-a| < \delta$ doesn't mean $e^{x-a}-1$ (bigger) is also less than $\delta$ . It may be bigger than $\delta$ . So I am having trouble going from step 1 to step 2. Edit 7/29 ( $2^{nd}$ Proof Attempt) : Some comments are suggesting, based on Chappers' answer here , that I should choose $$\delta=\max\left\{|x-a|, \left|\frac{x-a}{1-(x-a)}\right|\right\}.$$ Making this substitution, our proof becomes \begin{align*} & \quad 0 < |x - a| < \delta \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad\textrm{ (Given)}\\ & \quad 0 < |x - a| < \max\left\{|x-a|, \left|\frac{x-a}{1-(x-a)}\right| \right\} \quad \quad \quad \textrm{ (Substitution of $\delta$)}\\ &\quad \quad \quad \vdots \\ &\quad \quad \quad ? \\ &\quad \quad \quad \vdots \\ &\to \quad e^a|e^{x-a}-1| < \epsilon \quad \quad \textrm{ (Multiply both sides by $e^a$)} \\ &\to \quad |e^x-e^a| < \epsilon \quad \quad  \quad \textrm{ (Distribute $e^a$ into absolute value)} \\ & \to \quad \lim_{x \to a} e^x = e^a \quad \quad \quad \textrm{ (Definition of limit)} \end{align*} I am not sure how to fill in the gaps. The left hand side needs to somehow become $e^a |e^{x-a}-1|$ . The right hand side needs to become $\epsilon$ . But it seems to me that by making this choice, $\delta$ is no longer a function of $\epsilon$ . Edit 8/19 (Final proof): For those it helps, here's my final proof based off Paramanand Singh's answer: Let $\epsilon > 0$ and $a>0$ arbitrary. Choose $\delta= \frac{1}{2}\min\left\{1,\frac{\epsilon}{e^a}\right\}$ . Then \begin{align*} & \quad \left|x - a\right| < \delta \tag{Given} \\ \to& \quad \left|x - a\right| < \frac{1}{2} \min\left\{1, \frac{\epsilon}{e^a}\right\} \tag{$\delta = \frac{1}{2}\min\left\{1,\frac{\epsilon}{e^a}\right\}$} \\ \to& \quad 2\left|x - a\right| < \min\left\{1, \frac{\epsilon}{e^a}\right\} \tag{Multiplication by 2} \\ \to& \quad \left|\frac{x-a}{1-(x-a)}\right| < \min\left\{1,\frac{\epsilon}{e^a}\right\} \tag{$\left|\frac{h}{1-h}\right|<2|h|$ if $|h|<\frac{1}{2}$} \\ \to& \quad \left|\frac{x-a}{1-(x-a)}\right| < \frac{\epsilon}{e^a} \tag{$\min\left\{1,\frac{\epsilon}{e^a}\right\}< \frac{\epsilon}{e^a}$} \\ \to& \quad \left|e^{x-a}-1\right| < \frac{\epsilon}{e^a} \tag{Exponential Bound Lemma} \\ \to& \quad e^a\left|e^{x-a}-1\right| < \epsilon \tag{Multiplication by $e^a$} \\ \to& \quad \left|e^a\cdot e^{x-a}-e^a\right| < \epsilon \tag{Distribution Property} \\ \to& \quad \left|e^x-e^a\right| < \epsilon \tag{$e^s\cdot e^t = e^{s+t}$} \\ \to& \quad \lim_{x \to a} e^x = e^a \tag{Definition of limit} \end{align*} The above proof relies on the facts $e^x\cdot e^y=e^{x+y}$ and also the Exponential Bound Lemma $e^x \ge 1+x$ , which gives \begin{align*} & \quad e^h \ge \left(1+\frac{h}{n}\right)^n \\ \to& \quad e^h \ge 1+h \\ \to& \quad e^{-h} \ge 1-h \\ \to& \quad e^h \le \frac{1}{1-h} \\ \to& \quad e^h-1 \le \frac{h}{1-h}. \end{align*}","I have read several answers ( example 1 , example 2 ) about continuity of , but most rely on Power Series definition of , or sequential definition of a limit, or squeeze theorem. I would like a brute-force proof that meets the following criteria: Does NOT use sequential definition of limit Does NOT use Squeeze Theorem Uses definition of continuity directly Does NOT use perturbations (e.g. ) Uses definition of limit, starting with a and ending with Is NOT based on power series definition of Is based on elementary limit definition I would like to use exponential bounds (which come from Bernoulli's Inequality) like this answer : except I am trying to modify that proof so it doesn't depend on Squeeze Theorem. Proof attempt : Let and arbitrary. Choose . Then I know my proof is supposed to use the exponential bounds, so I tried using it (probably incorrectly) in step 2. Just because doesn't mean (bigger) is also less than . It may be bigger than . So I am having trouble going from step 1 to step 2. Edit 7/29 ( Proof Attempt) : Some comments are suggesting, based on Chappers' answer here , that I should choose Making this substitution, our proof becomes I am not sure how to fill in the gaps. The left hand side needs to somehow become . The right hand side needs to become . But it seems to me that by making this choice, is no longer a function of . Edit 8/19 (Final proof): For those it helps, here's my final proof based off Paramanand Singh's answer: Let and arbitrary. Choose . Then The above proof relies on the facts and also the Exponential Bound Lemma , which gives","e^x e^x \epsilon-\delta |e^{a + h} - e^a| 0 < |x-a| < \delta |e^x - e^a| < \epsilon e^x e^x = \lim_{n \to 0} (1+n)^{\frac{x}{n}} \begin{align*}
y+1 \le \ & \ e^y \le \frac{1}{1-y} \\
\to \quad \quad y \le \ & \ e^y - 1 \ \le \ \frac{y}{1-y} \\
\to \quad x-a \le \ & \ e^{x-a}-1 \ \le \ \frac{x-a}{1-(x-a)}
\end{align*} \epsilon > 0 a > 0 \delta = \frac{\epsilon}{e^a} \begin{align*}
& \quad 0 < |x - a| < \delta \quad \quad \quad \textrm{ (Given)}\\
&\to \quad |e^{x-a}-1| \quad < \delta \quad \quad \textrm{ (Reason unknown?)} \\
&\to \quad |e^{x-a}-1| < \frac{\epsilon}{e^a} \quad \quad \textrm{ (Substitute \delta=\frac{\epsilon}{e^a})} \\
&\to \quad e^a|e^{x-a}-1| < \epsilon \quad \quad \textrm{ (Multiply both sides by e^a)} \\
&\to \quad |e^x-e^a| < \epsilon \quad \quad  \quad \textrm{ (Distribute e^a into absolute value)} \\
& \to \quad \lim_{x \to a} e^x = e^a \quad \quad \quad \textrm{ (Definition of limit)}
\end{align*} x-a \le e^{x-a}-1 \le \frac{x-a}{1-(x-a)}, |x-a| < \delta e^{x-a}-1 \delta \delta 2^{nd} \delta=\max\left\{|x-a|, \left|\frac{x-a}{1-(x-a)}\right|\right\}. \begin{align*}
& \quad 0 < |x - a| < \delta \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad\textrm{ (Given)}\\
& \quad 0 < |x - a| < \max\left\{|x-a|, \left|\frac{x-a}{1-(x-a)}\right| \right\} \quad \quad \quad \textrm{ (Substitution of \delta)}\\
&\quad \quad \quad \vdots \\
&\quad \quad \quad ? \\
&\quad \quad \quad \vdots \\
&\to \quad e^a|e^{x-a}-1| < \epsilon \quad \quad \textrm{ (Multiply both sides by e^a)} \\
&\to \quad |e^x-e^a| < \epsilon \quad \quad  \quad \textrm{ (Distribute e^a into absolute value)} \\
& \to \quad \lim_{x \to a} e^x = e^a \quad \quad \quad \textrm{ (Definition of limit)}
\end{align*} e^a |e^{x-a}-1| \epsilon \delta \epsilon \epsilon > 0 a>0 \delta= \frac{1}{2}\min\left\{1,\frac{\epsilon}{e^a}\right\} \begin{align*}
& \quad \left|x - a\right| < \delta \tag{Given} \\
\to& \quad \left|x - a\right| < \frac{1}{2} \min\left\{1, \frac{\epsilon}{e^a}\right\} \tag{\delta = \frac{1}{2}\min\left\{1,\frac{\epsilon}{e^a}\right\}} \\
\to& \quad 2\left|x - a\right| < \min\left\{1, \frac{\epsilon}{e^a}\right\} \tag{Multiplication by 2} \\
\to& \quad \left|\frac{x-a}{1-(x-a)}\right| < \min\left\{1,\frac{\epsilon}{e^a}\right\} \tag{\left|\frac{h}{1-h}\right|<2|h| if |h|<\frac{1}{2}} \\
\to& \quad \left|\frac{x-a}{1-(x-a)}\right| < \frac{\epsilon}{e^a} \tag{\min\left\{1,\frac{\epsilon}{e^a}\right\}< \frac{\epsilon}{e^a}} \\
\to& \quad \left|e^{x-a}-1\right| < \frac{\epsilon}{e^a} \tag{Exponential Bound Lemma} \\
\to& \quad e^a\left|e^{x-a}-1\right| < \epsilon \tag{Multiplication by e^a} \\
\to& \quad \left|e^a\cdot e^{x-a}-e^a\right| < \epsilon \tag{Distribution Property} \\
\to& \quad \left|e^x-e^a\right| < \epsilon \tag{e^s\cdot e^t = e^{s+t}} \\
\to& \quad \lim_{x \to a} e^x = e^a \tag{Definition of limit}
\end{align*} e^x\cdot e^y=e^{x+y} e^x \ge 1+x \begin{align*}
& \quad e^h \ge \left(1+\frac{h}{n}\right)^n \\
\to& \quad e^h \ge 1+h \\
\to& \quad e^{-h} \ge 1-h \\
\to& \quad e^h \le \frac{1}{1-h} \\
\to& \quad e^h-1 \le \frac{h}{1-h}.
\end{align*}","['real-analysis', 'calculus', 'limits', 'continuity', 'epsilon-delta']"
46,"Egg vs. chicken: trig functions, exponential, real and complex","Egg vs. chicken: trig functions, exponential, real and complex",,"This is something I was shaky about when I took calculus, real analysis, and then complex analysis. Specifically, is the following chain of definitions circular in any way? Define the set $\mathbb{N}$ of natural numbers, define basic arithmetic operations Extend $\mathbb{N}$ to $\mathbb{Z}$, then to $\mathbb{Q}$, and extend the arithmetic operations to these sets as well Use the arithmetic operations to define distance and use that to define convergence, limit, and related concepts Use 3 to extend $\mathbb{Q}$ to $\mathbb{R}$ Define cosine and sine on $\mathbb{R}$ using the series definitions and then define other trig functions using cosine and sine Define $e=\lim(1+1/n)^n$ and define $\pi$ as $\ldots$ Use 5 and 6 to derive the usual properties of trig functions, of $e$, and of $\pi$ Define $e^x$, $x$ real, as $\lim(1+x/n)^n$ Finally, define, for $z=x+iy$, $e^z=e^x(\cos y+i\sin y)$, and derive properties of $e^z$ from 7 and 8 and complex arithmetic operations. I also have 3 auxiliary questions: (i) what is a standard / convenient definition for $\pi$ above? (ii) how does one fork off from 5 and 6 above to get the geometric interpretations of $\pi$ and the trig functions? (iii) is there a book (or a few books) that give students a sort of big picture (""big"" for a guy like me) view above?","This is something I was shaky about when I took calculus, real analysis, and then complex analysis. Specifically, is the following chain of definitions circular in any way? Define the set $\mathbb{N}$ of natural numbers, define basic arithmetic operations Extend $\mathbb{N}$ to $\mathbb{Z}$, then to $\mathbb{Q}$, and extend the arithmetic operations to these sets as well Use the arithmetic operations to define distance and use that to define convergence, limit, and related concepts Use 3 to extend $\mathbb{Q}$ to $\mathbb{R}$ Define cosine and sine on $\mathbb{R}$ using the series definitions and then define other trig functions using cosine and sine Define $e=\lim(1+1/n)^n$ and define $\pi$ as $\ldots$ Use 5 and 6 to derive the usual properties of trig functions, of $e$, and of $\pi$ Define $e^x$, $x$ real, as $\lim(1+x/n)^n$ Finally, define, for $z=x+iy$, $e^z=e^x(\cos y+i\sin y)$, and derive properties of $e^z$ from 7 and 8 and complex arithmetic operations. I also have 3 auxiliary questions: (i) what is a standard / convenient definition for $\pi$ above? (ii) how does one fork off from 5 and 6 above to get the geometric interpretations of $\pi$ and the trig functions? (iii) is there a book (or a few books) that give students a sort of big picture (""big"" for a guy like me) view above?",,"['real-analysis', 'complex-analysis', 'trigonometry', 'reference-request', 'soft-question']"
47,Difference between a limit and accumulation point? [duplicate],Difference between a limit and accumulation point? [duplicate],,"This question already has answers here : Difference Between Limit Point and Accumulation Point? (8 answers) Closed 1 year ago . What is the exact difference between a limit point and an accumulation point? An accumulation point of a set is a point, every neighborhood of which has infinitely many points of the set. Alternatively, it has a sequence of DISTINCT terms converging to it? Whereas a limit point simply has a sequence which converges to it? i.e. something like $(1)^n$ which is a constant sequence. Is this the right idea? As much detail and intuition as possible would be greatly appreciated.","This question already has answers here : Difference Between Limit Point and Accumulation Point? (8 answers) Closed 1 year ago . What is the exact difference between a limit point and an accumulation point? An accumulation point of a set is a point, every neighborhood of which has infinitely many points of the set. Alternatively, it has a sequence of DISTINCT terms converging to it? Whereas a limit point simply has a sequence which converges to it? i.e. something like $(1)^n$ which is a constant sequence. Is this the right idea? As much detail and intuition as possible would be greatly appreciated.",,"['real-analysis', 'soft-question']"
48,"Are the addition and multiplication of real numbers, as we know them, unique?","Are the addition and multiplication of real numbers, as we know them, unique?",,"After recently concluding my Real Analysis course in this semester I got the following question bugging me: Is the canonical operation of addition on real numbers unique? Otherwise: Can we define another operation on Reals in a such way it has the same properties of usual addition and behaves exactly like that? Or even: How I can reliable know if there is no two different ways of summing real numbers? Naturally these dense questions led me to further investigations, like: The following properties are sufficient to fully characterize the canonical addition on Reals? Closure Associativity Commutativity Identity being 0 Unique inverse Multiplication distributes over If so, property 6 raises the question: Is the canonical multiplication on Reals unique? But then, if are them not unique, different additions are differently related with different multiplications? And so on... The motivation comes from the construction of real numbers. From Peano's Axioms and the set-theoretic definition of Natural numbers to Dedekind and Cauchy's construction of Real numbers we haven't talked about uniqueness of operations in classes nor I could find relevant discussion about this topic on the internet and in ubiquitous Real Analysis reference books by authors as: Walter Rudin Robert G. Bartle Stephen Abbott William F. Trench Not talking about the uniqueness of the operations, as we know them, in a first Real Analysis course seems rather common and not elementary matter. Thus, introduced the subject and its context, would someone care to expand it eventually revealing the formal name of this field of study?","After recently concluding my Real Analysis course in this semester I got the following question bugging me: Is the canonical operation of addition on real numbers unique? Otherwise: Can we define another operation on Reals in a such way it has the same properties of usual addition and behaves exactly like that? Or even: How I can reliable know if there is no two different ways of summing real numbers? Naturally these dense questions led me to further investigations, like: The following properties are sufficient to fully characterize the canonical addition on Reals? Closure Associativity Commutativity Identity being 0 Unique inverse Multiplication distributes over If so, property 6 raises the question: Is the canonical multiplication on Reals unique? But then, if are them not unique, different additions are differently related with different multiplications? And so on... The motivation comes from the construction of real numbers. From Peano's Axioms and the set-theoretic definition of Natural numbers to Dedekind and Cauchy's construction of Real numbers we haven't talked about uniqueness of operations in classes nor I could find relevant discussion about this topic on the internet and in ubiquitous Real Analysis reference books by authors as: Walter Rudin Robert G. Bartle Stephen Abbott William F. Trench Not talking about the uniqueness of the operations, as we know them, in a first Real Analysis course seems rather common and not elementary matter. Thus, introduced the subject and its context, would someone care to expand it eventually revealing the formal name of this field of study?",,"['real-analysis', 'foundations', 'binary-operations']"
49,Which functions are derivatives of some other function?,Which functions are derivatives of some other function?,,There is a fundamental formula in integral calculus which states that $\int_a^bF'(x)dx=F(b)-F(a)$. This formula gives a connection between definite and indefinite integrals. There are plenty of functions which are integrable (in the definite sense)-for example each bounded measurable function is ok. However I don't know precisely which functions do have indefinite integral: in other words which functions are derivatives of some other function? For example continuity is enough: on the other hand one cas show that each function being the derivative (although need not to be continuous) has the Darboux property. So my question is Which functions are derivatives of some other function?,There is a fundamental formula in integral calculus which states that $\int_a^bF'(x)dx=F(b)-F(a)$. This formula gives a connection between definite and indefinite integrals. There are plenty of functions which are integrable (in the definite sense)-for example each bounded measurable function is ok. However I don't know precisely which functions do have indefinite integral: in other words which functions are derivatives of some other function? For example continuity is enough: on the other hand one cas show that each function being the derivative (although need not to be continuous) has the Darboux property. So my question is Which functions are derivatives of some other function?,,"['real-analysis', 'integration']"
50,Existence of smooth extension of a function defined on a closed interval,Existence of smooth extension of a function defined on a closed interval,,"Suppose $ f: [0,1] \rightarrow \mathbb{R}$ is a function such that derivatives of all orders exist ( at the end points of the interval the appropriate one-sided derivatives exist) and are continuous $ \forall x \in [0,1] $ . How to prove that $f$ is smooth, in the sense that it admits a $C^\infty$  extension to an open interval containing the interval $[0,1]$ ?","Suppose $ f: [0,1] \rightarrow \mathbb{R}$ is a function such that derivatives of all orders exist ( at the end points of the interval the appropriate one-sided derivatives exist) and are continuous $ \forall x \in [0,1] $ . How to prove that $f$ is smooth, in the sense that it admits a $C^\infty$  extension to an open interval containing the interval $[0,1]$ ?",,"['real-analysis', 'analysis', 'functions']"
51,Clausen and Riemann zeta function,Clausen and Riemann zeta function,,This is an exercise from the American Monthly Problems from last year. I would like prove two formulas: (1) $\int_0^{2\pi}\int_0^{2\pi}\log(3+2\cos(x)+2\cos(y)+2\cos(x-y)) dxdy=8\pi Cl(\frac{\pi}{3})$ (2) $\int_0^{\pi}\int_0^{\pi}\log(3+2\cos(x)+2\cos(y)+2\cos(x-y)) dxdy=\frac{28}{3}\zeta(3)$ $Cl(\phi)=\sum_{n=1}^{\infty}\frac{\sin(n\phi)}{n^2}$ The first I did was to reqwirte $Cl(\phi)$. I took the derivative and received $Cl(\phi)=-\int_0^{\phi}\log(2\sin(\frac{t}{2}))dt$ for $0\le\phi\le\pi$ For (2) I have no idea. It seems to me these integrals deliver a nice approximation mathod for $Cl(\frac{\pi}{3})$ and $\zeta(3)$ but I could not find the formulas somewhere else.,This is an exercise from the American Monthly Problems from last year. I would like prove two formulas: (1) $\int_0^{2\pi}\int_0^{2\pi}\log(3+2\cos(x)+2\cos(y)+2\cos(x-y)) dxdy=8\pi Cl(\frac{\pi}{3})$ (2) $\int_0^{\pi}\int_0^{\pi}\log(3+2\cos(x)+2\cos(y)+2\cos(x-y)) dxdy=\frac{28}{3}\zeta(3)$ $Cl(\phi)=\sum_{n=1}^{\infty}\frac{\sin(n\phi)}{n^2}$ The first I did was to reqwirte $Cl(\phi)$. I took the derivative and received $Cl(\phi)=-\int_0^{\phi}\log(2\sin(\frac{t}{2}))dt$ for $0\le\phi\le\pi$ For (2) I have no idea. It seems to me these integrals deliver a nice approximation mathod for $Cl(\frac{\pi}{3})$ and $\zeta(3)$ but I could not find the formulas somewhere else.,,"['real-analysis', 'special-functions', 'riemann-zeta']"
52,Inequality : $\Big(\frac{x^n+1+(\frac{x+1}{2})^n}{x^{n-1}+1+(\frac{x+1}{2})^{n-1}}\Big)^n+\Big(\frac{x+1}{2}\Big)^n\leq x^n+1$,Inequality :,\Big(\frac{x^n+1+(\frac{x+1}{2})^n}{x^{n-1}+1+(\frac{x+1}{2})^{n-1}}\Big)^n+\Big(\frac{x+1}{2}\Big)^n\leq x^n+1,"I have the following problem to solve : Let $x,y>0$ and $n>1$ a natural number then we have : $$\Big(\frac{x^n+y^n+(\frac{x+y}{2})^n}{x^{n-1}+y^{n-1}+(\frac{x+y}{2})^{n-1}}\Big)^n+\Big(\frac{x+y}{2}\Big)^n\leq x^n+y^n$$ The problem is equivalent to : $$\Big(\frac{x^n+1+(\frac{x+1}{2})^n}{x^{n-1}+1+(\frac{x+1}{2})^{n-1}}\Big)^n+\Big(\frac{x+1}{2}\Big)^n\leq x^n+1$$ Or ( $y^2=x$ ): $$\Big(\frac{y^{2n}+1+(\frac{y^2+1}{2})^n}{y^{2(n-1)}+1+(\frac{y^2+1}{2})^{n-1}}\Big)^n+\Big(\frac{y^2+1}{2}\Big)^n\leq y^{2n}+1$$ I try the following identity : $$ch^2(x)-sh^2(x)=1$$ So we put $y=sh(x)$ we get : $$\Big(\frac{sh^{2n}(x)+1+(\frac{ch^2(x)}{2})^n}{sh^{2(n-1)}(x)+1+(\frac{ch^2(x)}{2})^{n-1}}\Big)^n+\Big(\frac{ch^2(x)}{2}\Big)^n\leq sh^{2n}(x)+1$$ And after I'm stuck... Update case $n=3$ : Due to homogeneity we can assume : $$x^{2}+y^{2}+\Big(\frac{x+y}{2}\Big)^{2}=1$$ Remains to show : $$\Big(x^3+y^3+\Big(\frac{x+y}{2}\Big)^3\Big)^3+\Big(\frac{x+y}{2}\Big)^{3}-x^3-y^3\leq 0$$ Or : $$\frac{1}{512} (x + y) (729 x^8 + 972 x^6 y^2 + 1728 x^5 y^3 + 54 x^4 y^4 + 1728 x^3 y^5 + 972 x^2 y^6 - 448 x^2 + 640 x y + 729 y^8 - 448 y^2)\quad(1)$$ Or : $$\frac{1}{512} (x + y)(27 (x + y)^2 (3 x^2 - 2 x y + 3 y^2)^3-448y^2-448z^2+640xy)$$ Or : $$\frac{1}{512} (x + y)(27 (x + y)^2 (3 x^2 - 2 x y + 3 y^2)^3-64 (7 x^2 - 10 x y + 7 y^2))$$ But with the constraint : $$\frac{5x^2}{4}+\frac{xy}{2}+\frac{5y^2}{4}=1$$ Or : $$x^2+y^2=\Big(1-\frac{xy}{2}\Big)\frac{4}{5}$$ It gives : $$\frac{1}{512} (x + y)\Big(27 (x + y)^2 \Big(\Big(1-\frac{xy}{2}\Big)\frac{12}{5}-2xy\Big)^3-64 \Big(\Big(1-\frac{xy}{2}\Big)\frac{28}{5}-10xy\Big)\Big )$$ Or : $$\frac{1}{512} (x + y)\Big(27 \Big(\Big(1-\frac{xy}{2}\Big)\Big)\frac{4}{5}+2xy) \Big(\Big(1-\frac{xy}{2}\Big)\frac{12}{5}-2xy\Big)^3-64 \Big(\Big(1-\frac{xy}{2}\Big)\frac{28}{5}-10xy\Big)\Big)$$ We put the substitution $a=xy$ .There is a root at $a=\frac{1}{3}$ it gives : $$\frac{1}{512} (x + y)\Big(-\frac{512}{625} (3 a - 1) (576 a^3 - 816 a^2 + 52 a - 73)\Big)$$ Now with the constraint it's not hard to see that $a\leq \frac{1}{3}$ And $$f(a)=(576 a^3 - 816 a^2 + 52 a - 73)\leq 0$$ on $[0,\frac{1}{3}]$ So the quantity $(1)$ is negative . We are done for this case . If you have a hint it would be cool . Thanks a lot !",I have the following problem to solve : Let and a natural number then we have : The problem is equivalent to : Or ( ): I try the following identity : So we put we get : And after I'm stuck... Update case : Due to homogeneity we can assume : Remains to show : Or : Or : Or : But with the constraint : Or : It gives : Or : We put the substitution .There is a root at it gives : Now with the constraint it's not hard to see that And on So the quantity is negative . We are done for this case . If you have a hint it would be cool . Thanks a lot !,"x,y>0 n>1 \Big(\frac{x^n+y^n+(\frac{x+y}{2})^n}{x^{n-1}+y^{n-1}+(\frac{x+y}{2})^{n-1}}\Big)^n+\Big(\frac{x+y}{2}\Big)^n\leq x^n+y^n \Big(\frac{x^n+1+(\frac{x+1}{2})^n}{x^{n-1}+1+(\frac{x+1}{2})^{n-1}}\Big)^n+\Big(\frac{x+1}{2}\Big)^n\leq x^n+1 y^2=x \Big(\frac{y^{2n}+1+(\frac{y^2+1}{2})^n}{y^{2(n-1)}+1+(\frac{y^2+1}{2})^{n-1}}\Big)^n+\Big(\frac{y^2+1}{2}\Big)^n\leq y^{2n}+1 ch^2(x)-sh^2(x)=1 y=sh(x) \Big(\frac{sh^{2n}(x)+1+(\frac{ch^2(x)}{2})^n}{sh^{2(n-1)}(x)+1+(\frac{ch^2(x)}{2})^{n-1}}\Big)^n+\Big(\frac{ch^2(x)}{2}\Big)^n\leq sh^{2n}(x)+1 n=3 x^{2}+y^{2}+\Big(\frac{x+y}{2}\Big)^{2}=1 \Big(x^3+y^3+\Big(\frac{x+y}{2}\Big)^3\Big)^3+\Big(\frac{x+y}{2}\Big)^{3}-x^3-y^3\leq 0 \frac{1}{512} (x + y) (729 x^8 + 972 x^6 y^2 + 1728 x^5 y^3 + 54 x^4 y^4 + 1728 x^3 y^5 + 972 x^2 y^6 - 448 x^2 + 640 x y + 729 y^8 - 448 y^2)\quad(1) \frac{1}{512} (x + y)(27 (x + y)^2 (3 x^2 - 2 x y + 3 y^2)^3-448y^2-448z^2+640xy) \frac{1}{512} (x + y)(27 (x + y)^2 (3 x^2 - 2 x y + 3 y^2)^3-64 (7 x^2 - 10 x y + 7 y^2)) \frac{5x^2}{4}+\frac{xy}{2}+\frac{5y^2}{4}=1 x^2+y^2=\Big(1-\frac{xy}{2}\Big)\frac{4}{5} \frac{1}{512} (x + y)\Big(27 (x + y)^2 \Big(\Big(1-\frac{xy}{2}\Big)\frac{12}{5}-2xy\Big)^3-64 \Big(\Big(1-\frac{xy}{2}\Big)\frac{28}{5}-10xy\Big)\Big ) \frac{1}{512} (x + y)\Big(27 \Big(\Big(1-\frac{xy}{2}\Big)\Big)\frac{4}{5}+2xy) \Big(\Big(1-\frac{xy}{2}\Big)\frac{12}{5}-2xy\Big)^3-64 \Big(\Big(1-\frac{xy}{2}\Big)\frac{28}{5}-10xy\Big)\Big) a=xy a=\frac{1}{3} \frac{1}{512} (x + y)\Big(-\frac{512}{625} (3 a - 1) (576 a^3 - 816 a^2 + 52 a - 73)\Big) a\leq \frac{1}{3} f(a)=(576 a^3 - 816 a^2 + 52 a - 73)\leq 0 [0,\frac{1}{3}] (1)","['real-analysis', 'inequality', 'jensen-inequality']"
53,Integrating on open vs. closed intervals,Integrating on open vs. closed intervals,,"What is one difference in the values of $$\int\limits_{\left[0,1\right]}y\, dx$$$$\int\limits_{\left(0,1\right)}y\, dx$$ and how would you calculate the values? For the sake of simplicity, let $y=x$. Conceptualizing integration as the area bounded by the function, the $x$-axis and the limits of integration, the latter should be smaller.","What is one difference in the values of $$\int\limits_{\left[0,1\right]}y\, dx$$$$\int\limits_{\left(0,1\right)}y\, dx$$ and how would you calculate the values? For the sake of simplicity, let $y=x$. Conceptualizing integration as the area bounded by the function, the $x$-axis and the limits of integration, the latter should be smaller.",,"['calculus', 'real-analysis', 'definite-integrals']"
54,Integral $ \int_0^\infty \frac{\ln(1+\sigma x)\ln(1+\omega x^2)}{x^3}dx$,Integral, \int_0^\infty \frac{\ln(1+\sigma x)\ln(1+\omega x^2)}{x^3}dx,"Hello there I am trying to calculate $$ \int_0^\infty \frac{\ln(1+\sigma x)\ln(1+\omega x^2)}{x^3}dx $$ NOT using mathematica, matlab, etc.  We are given that $\sigma, \omega$ are complex.  Note, the integral should have different values for $|\sigma \omega^{-1/2}| < 1$ and $|\sigma \omega^{-1/2}| > 1.$  I am stuck now and not sure how to approach it. Note this integral is useful since in the limit $\sigma \to \sqrt{\omega}$ and using $Li_2(-1)=-\pi^2/12$ we obtain $$ \int_0^\infty \frac{\ln(1+x)\ln(1+x^2)}{x^3}dx=\frac{\pi}{2}. $$ We also know that $$ \ln(1+x)=-\sum_{n=1}^\infty \frac{(-1)^nx^n}{n}, \ |x|\leq 1. $$ Thanks","Hello there I am trying to calculate $$ \int_0^\infty \frac{\ln(1+\sigma x)\ln(1+\omega x^2)}{x^3}dx $$ NOT using mathematica, matlab, etc.  We are given that $\sigma, \omega$ are complex.  Note, the integral should have different values for $|\sigma \omega^{-1/2}| < 1$ and $|\sigma \omega^{-1/2}| > 1.$  I am stuck now and not sure how to approach it. Note this integral is useful since in the limit $\sigma \to \sqrt{\omega}$ and using $Li_2(-1)=-\pi^2/12$ we obtain $$ \int_0^\infty \frac{\ln(1+x)\ln(1+x^2)}{x^3}dx=\frac{\pi}{2}. $$ We also know that $$ \ln(1+x)=-\sum_{n=1}^\infty \frac{(-1)^nx^n}{n}, \ |x|\leq 1. $$ Thanks",,"['real-analysis', 'integration', 'definite-integrals', 'contest-math', 'contour-integration']"
55,Commutativity of iterated limits,Commutativity of iterated limits,,"The following is a weird result I've obtained with iterated limits. There must be a flaw somewhere in someone's reasoning but I can't discover what it is.  The problem is that, in general, iterated limits are not supposed to be commutative. However, the purported proof below seems to indicate that they always are. Did I make a mistake somewhere? Let $F$ be a real valued function of two real variables defined in some region around $(a,b)$. Then the standard limit [ correct? ] of $F$ as $(x,y)$ approaches $(a,b)$ equals $L\,$ if and only if for every $\epsilon > 0$ there exists $\delta > 0$ such that $F$ satisfies: $$   | F(x,y) - L | < \epsilon $$ whenever the distance between $(x,y)$ and $(a,b)$ satisfies: $$   0 < \sqrt{ (x-a)^2 + (y-b)^2 } < \delta $$ We will use the following notation for such limits of functions of two variables: $$    \lim_{(x,y)\rightarrow (a,b)} F(x,y) = L $$ Note. At this moment, we do not wish to consider limits where (one of) the independent variable(s) approaches infinity. Next we consider the following iterated limit: $$   \lim_{y\rightarrow b} \left[ \lim_{x\rightarrow a} F(x,y) \right] = L $$ Theorem. Commutativity of iterated limits. $$   \lim_{y\rightarrow b} \left[ \lim_{x\rightarrow a} F(x,y) \right] =   \lim_{x\rightarrow a} \left[ \lim_{y\rightarrow b} F(x,y) \right] =   \lim_{(x,y)\rightarrow (a,b)} F(x,y) $$ Proof. We split the first iterated limit in two pieces: $$   \lim_{x\rightarrow a} F(x,y) = F_a(y) $$ And: $$   \lim_{y\rightarrow b} F_a(y) = L $$ Thus it becomes evident that the (first) iterated limit is actually defined as follows. For every $\epsilon_x > 0$ there is some $\delta_x > 0$ such that: $$   | F(x,y) - F_a(y) | < \epsilon_x \quad \mbox{whenever}              \quad 0 < | x - a | < \delta_x $$ For every $\epsilon_y > 0$ there is some $\delta_y > 0$ such that: $$   | F_a(y) - L | < \epsilon_y \quad \mbox{whenever}              \quad 0 < | y - b | < \delta_y $$ Applying the triangle inequality $|a| + |b| \ge |a + b|$ gives: $$   | F(x,y) - F_a(y) | + | F_a(y) - L | \ge | F(x,y) - L | $$ Consequently: $$   | F(x,y) - L | < \epsilon_x + \epsilon_y $$ On the other hand we have: $$    0 < | x - a | < \delta_x \qquad \mbox{and} \qquad 0 < | y - b | < \delta_y $$ Hence: $$     0 < \sqrt{ (x-a)^2 + (y-b)^2 } < \sqrt{ \delta_x^2 + \delta_y^2 }   $$ This is exactly the definition of the above standard limit of a function of two variables if we put: $$ \epsilon = \epsilon_y + \epsilon_y \qquad \mbox{and} \qquad \delta = \sqrt{\delta_x^2 + \delta_y^2} $$ Therefore: $$   \lim_{y\rightarrow b} \left[ \lim_{x\rightarrow a} F(x,y) \right] =   \lim_{(x,y)\rightarrow (a,b)} F(x,y) $$ In very much the same way we can prove that: $$   \lim_{x\rightarrow a} \left[ \lim_{y\rightarrow b} F(x,y) \right] =   \lim_{(x,y)\rightarrow (a,b)} F(x,y) $$ QED","The following is a weird result I've obtained with iterated limits. There must be a flaw somewhere in someone's reasoning but I can't discover what it is.  The problem is that, in general, iterated limits are not supposed to be commutative. However, the purported proof below seems to indicate that they always are. Did I make a mistake somewhere? Let $F$ be a real valued function of two real variables defined in some region around $(a,b)$. Then the standard limit [ correct? ] of $F$ as $(x,y)$ approaches $(a,b)$ equals $L\,$ if and only if for every $\epsilon > 0$ there exists $\delta > 0$ such that $F$ satisfies: $$   | F(x,y) - L | < \epsilon $$ whenever the distance between $(x,y)$ and $(a,b)$ satisfies: $$   0 < \sqrt{ (x-a)^2 + (y-b)^2 } < \delta $$ We will use the following notation for such limits of functions of two variables: $$    \lim_{(x,y)\rightarrow (a,b)} F(x,y) = L $$ Note. At this moment, we do not wish to consider limits where (one of) the independent variable(s) approaches infinity. Next we consider the following iterated limit: $$   \lim_{y\rightarrow b} \left[ \lim_{x\rightarrow a} F(x,y) \right] = L $$ Theorem. Commutativity of iterated limits. $$   \lim_{y\rightarrow b} \left[ \lim_{x\rightarrow a} F(x,y) \right] =   \lim_{x\rightarrow a} \left[ \lim_{y\rightarrow b} F(x,y) \right] =   \lim_{(x,y)\rightarrow (a,b)} F(x,y) $$ Proof. We split the first iterated limit in two pieces: $$   \lim_{x\rightarrow a} F(x,y) = F_a(y) $$ And: $$   \lim_{y\rightarrow b} F_a(y) = L $$ Thus it becomes evident that the (first) iterated limit is actually defined as follows. For every $\epsilon_x > 0$ there is some $\delta_x > 0$ such that: $$   | F(x,y) - F_a(y) | < \epsilon_x \quad \mbox{whenever}              \quad 0 < | x - a | < \delta_x $$ For every $\epsilon_y > 0$ there is some $\delta_y > 0$ such that: $$   | F_a(y) - L | < \epsilon_y \quad \mbox{whenever}              \quad 0 < | y - b | < \delta_y $$ Applying the triangle inequality $|a| + |b| \ge |a + b|$ gives: $$   | F(x,y) - F_a(y) | + | F_a(y) - L | \ge | F(x,y) - L | $$ Consequently: $$   | F(x,y) - L | < \epsilon_x + \epsilon_y $$ On the other hand we have: $$    0 < | x - a | < \delta_x \qquad \mbox{and} \qquad 0 < | y - b | < \delta_y $$ Hence: $$     0 < \sqrt{ (x-a)^2 + (y-b)^2 } < \sqrt{ \delta_x^2 + \delta_y^2 }   $$ This is exactly the definition of the above standard limit of a function of two variables if we put: $$ \epsilon = \epsilon_y + \epsilon_y \qquad \mbox{and} \qquad \delta = \sqrt{\delta_x^2 + \delta_y^2} $$ Therefore: $$   \lim_{y\rightarrow b} \left[ \lim_{x\rightarrow a} F(x,y) \right] =   \lim_{(x,y)\rightarrow (a,b)} F(x,y) $$ In very much the same way we can prove that: $$   \lim_{x\rightarrow a} \left[ \lim_{y\rightarrow b} F(x,y) \right] =   \lim_{(x,y)\rightarrow (a,b)} F(x,y) $$ QED",,"['real-analysis', 'limits', 'functions']"
56,"Every complete, countable metric space has a discrete, dense subset.","Every complete, countable metric space has a discrete, dense subset.",,"Given a complete, countable metric space, say $X$, I'd like to show it has a discrete, dense subset. This seems like an application of the Baire Category Theorem, but that doesn't seem to go anywhere. Any help would be appreciated.","Given a complete, countable metric space, say $X$, I'd like to show it has a discrete, dense subset. This seems like an application of the Baire Category Theorem, but that doesn't seem to go anywhere. Any help would be appreciated.",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
57,A paradox on Hilbert spaces and their duals,A paradox on Hilbert spaces and their duals,,"I am making some elementary mistakes here. Could you please help me point out the problems? Thank you very much! Suppose on some space $H$ we have two inner products, which make $H$ after completion two real Hilbert spaces. Suppose that these two inner products are comparable: $$ (f,f)_1 \le (f,f)_2,\quad \forall f\in H.  $$ Denote these two Hilbert spaces by $H_1$ and $H_2$. It is clear that $$ H_2 \subseteq H_1. $$ Let $H_1'$ and $H_2'$ be the dual spaces of $H_1$ and $H_2$, respectively. They are also Hilbert spaces. Noticing that the fact that $$ ||u||_{H_i'} = \sup_{(x,x)_i \le 1} |u(x)|,\qquad i=1,2, $$ we have that $$ H_1' \subseteq H_2'\:. $$ Being Hilbert spaces, $H_i \cong H_i'$ (i.e., $H_i$ is isomorphic to $H_i'$). Since the Hilbert spaces are real, we can identify $H_i'$ by $H_i$. Then the above two inclusions imply that $$ H_1 = H_2 , $$ which cannot be true in general. What is wrong in my arguments? Thank you very much for your great help! :-) Anand","I am making some elementary mistakes here. Could you please help me point out the problems? Thank you very much! Suppose on some space $H$ we have two inner products, which make $H$ after completion two real Hilbert spaces. Suppose that these two inner products are comparable: $$ (f,f)_1 \le (f,f)_2,\quad \forall f\in H.  $$ Denote these two Hilbert spaces by $H_1$ and $H_2$. It is clear that $$ H_2 \subseteq H_1. $$ Let $H_1'$ and $H_2'$ be the dual spaces of $H_1$ and $H_2$, respectively. They are also Hilbert spaces. Noticing that the fact that $$ ||u||_{H_i'} = \sup_{(x,x)_i \le 1} |u(x)|,\qquad i=1,2, $$ we have that $$ H_1' \subseteq H_2'\:. $$ Being Hilbert spaces, $H_i \cong H_i'$ (i.e., $H_i$ is isomorphic to $H_i'$). Since the Hilbert spaces are real, we can identify $H_i'$ by $H_i$. Then the above two inclusions imply that $$ H_1 = H_2 , $$ which cannot be true in general. What is wrong in my arguments? Thank you very much for your great help! :-) Anand",,"['real-analysis', 'functional-analysis', 'hilbert-spaces']"
58,Is this condition sufficient to ensure monotonicity of a function?,Is this condition sufficient to ensure monotonicity of a function?,,"Suppose $f:[a,b]\to\mathbb{R}$ is continuous and $$\limsup_{h\to0}\frac{f(x+h)-f(x)}{h}\geq0$$ for every $x\in(a,b)$. Does it follow that $f$ increases monotonically on $[a,b]$? It is a problem in the 4th edition of Royden's Real Analysis (Exercise 19, Ch. 6) to prove under these hypotheses that $f$ is in fact nondecreasing. But that problem is listed in the errata for the text, where it says to replace $\limsup$ with $\liminf$ (which obviously makes the problem much easier). However, no counterexample is given in the errata, and I'm not sure whether it's false as stated or just hard to prove (or neither; maybe I'm just not seeing a simple solution!).","Suppose $f:[a,b]\to\mathbb{R}$ is continuous and $$\limsup_{h\to0}\frac{f(x+h)-f(x)}{h}\geq0$$ for every $x\in(a,b)$. Does it follow that $f$ increases monotonically on $[a,b]$? It is a problem in the 4th edition of Royden's Real Analysis (Exercise 19, Ch. 6) to prove under these hypotheses that $f$ is in fact nondecreasing. But that problem is listed in the errata for the text, where it says to replace $\limsup$ with $\liminf$ (which obviously makes the problem much easier). However, no counterexample is given in the errata, and I'm not sure whether it's false as stated or just hard to prove (or neither; maybe I'm just not seeing a simple solution!).",,"['calculus', 'real-analysis', 'analysis']"
59,"Find $X,Y\subseteq \Bbb{R}$ such that there are $f\colon X\to Y$ and $g\colon Y\to X$, both bijective and continuous but $X\not\cong Y$.","Find  such that there are  and , both bijective and continuous but .","X,Y\subseteq \Bbb{R} f\colon X\to Y g\colon Y\to X X\not\cong Y","I came across the following problem: Let $\Bbb{R}$ be the euclidean space. Find $X,Y\subseteq \Bbb{R}$ such that there are maps $f\colon X\to Y$ and $g\colon Y\to X$ , both bijective and continuous but $X\not\cong Y$ ( $X$ not homeomorphic to $Y$ ). The structure for a proof that I'm thinking is fairly obvios: Find the sets, find the maps, find a topological invariant that one holds that the other doesn't, QED. Easier said than done. I tried the sets $\Bbb{R}$ , $[0,1]$ but can't really find a bijective, continuous map (of course, the points $a,b$ are the ones that are causing trouble since I allready know how to do these for the set $(a,b)$ , of course this set doen's work since $\Bbb{R}\cong (a,b)$ ). I'm not being able to make much progress in this one. Any hints on how to find the sets/maps are apreciated.","I came across the following problem: Let be the euclidean space. Find such that there are maps and , both bijective and continuous but ( not homeomorphic to ). The structure for a proof that I'm thinking is fairly obvios: Find the sets, find the maps, find a topological invariant that one holds that the other doesn't, QED. Easier said than done. I tried the sets , but can't really find a bijective, continuous map (of course, the points are the ones that are causing trouble since I allready know how to do these for the set , of course this set doen's work since ). I'm not being able to make much progress in this one. Any hints on how to find the sets/maps are apreciated.","\Bbb{R} X,Y\subseteq \Bbb{R} f\colon X\to Y g\colon Y\to X X\not\cong Y X Y \Bbb{R} [0,1] a,b (a,b) \Bbb{R}\cong (a,b)","['real-analysis', 'general-topology', 'continuity', 'open-map']"
60,"Is there a sequence such that $a_n\to0$, $na_n\to\infty$ and $\left(na_{n+1}-\sum_{k=1}^n a_k\right)$ is convergent?","Is there a sequence such that ,  and  is convergent?",a_n\to0 na_n\to\infty \left(na_{n+1}-\sum_{k=1}^n a_k\right),"Is there a positive decreasing sequence $(a_n)_{n\ge 0}$ such that ${\it i.}$  $\lim\limits_{n\to\infty} a_n=0$. ${\it ii.}$  $\lim\limits_{n\to\infty} na_n=\infty$. ${\it iii.}$  there is a real $\ell$ such that $\lim\limits_{n\to\infty} \left(na_{n+1}-\sum_{k=1}^n a_k\right)=\ell.$ Of course without condition ${\it i.}$ constant sequences are non-increasing examples that satisfy the other two conditions, but the requirement that the sequence must be decreasing to zero makes it hard to find!.","Is there a positive decreasing sequence $(a_n)_{n\ge 0}$ such that ${\it i.}$  $\lim\limits_{n\to\infty} a_n=0$. ${\it ii.}$  $\lim\limits_{n\to\infty} na_n=\infty$. ${\it iii.}$  there is a real $\ell$ such that $\lim\limits_{n\to\infty} \left(na_{n+1}-\sum_{k=1}^n a_k\right)=\ell.$ Of course without condition ${\it i.}$ constant sequences are non-increasing examples that satisfy the other two conditions, but the requirement that the sequence must be decreasing to zero makes it hard to find!.",,"['real-analysis', 'sequences-and-series']"
61,"Is there a countable cover of $\mathbb{R}^2$ by balls $B(x_n, n^{-1/2})$?",Is there a countable cover of  by balls ?,"\mathbb{R}^2 B(x_n, n^{-1/2})","Is it possible to cover all of $\mathbb{R}^2$ using balls $\{ B(x_n,n^{-1/2})\}_{n=1}^\infty$ of decreasing radius $n^{-1/2}$? I know that if we chose e.g. radius $n^{-1}$ it could never work because $\sum \pi (n^{-1})^2 < \infty$. But in this case the balls cover an infinite amount of area, so it seems that it may be possible to construct. Edit: It can definitely be done with balls of radius $n^{-1/2+\epsilon}$ for any $\epsilon > 0$.","Is it possible to cover all of $\mathbb{R}^2$ using balls $\{ B(x_n,n^{-1/2})\}_{n=1}^\infty$ of decreasing radius $n^{-1/2}$? I know that if we chose e.g. radius $n^{-1}$ it could never work because $\sum \pi (n^{-1})^2 < \infty$. But in this case the balls cover an infinite amount of area, so it seems that it may be possible to construct. Edit: It can definitely be done with balls of radius $n^{-1/2+\epsilon}$ for any $\epsilon > 0$.",,"['real-analysis', 'measure-theory', 'real-numbers']"
62,Suppose $1\le p < r < q < \infty$. Prove that $L^p\cap L^q \subset L^r$.,Suppose . Prove that .,1\le p < r < q < \infty L^p\cap L^q \subset L^r,"Suppose $1\le p < r < q < \infty$. Prove that $L^p\cap L^q \subset L^r$. So suppose $f\in L^p\cap L^q$. Then both $\int |f|^p d\mu$ and $\int|f|^q d\mu$ exist. For each $x$ in the domain of $f$, $|f(x)|^r$ is between $|f(x)|^p$ and $|f(x)|^q$. So $|f|^r\le\max(|f|^p,|f|^q)=\frac12({|f|^p+|f|^q+||f|^p-|f|^q|})$. The RHS is integrable, and since $|f|^r$ is bounded above by an integrable function, it is also integrable. So $f\in L^r$. Is my argument correct? Actually is this still true when $q=\infty$? In this case, we have $1\le p < r < \infty$. Is $L^p\cap L^{\infty}$ necessarily a subset of $L^r$? Why? Please show that if $f\in L^p\cap L^{\infty}$, then $||f||_r \le ||f|_p^{p/r} ||f||_{\infty}^{1-p/r}$ EDIT: Thanks for the link to the first part of my question. I'm still quite interested in the case $q = \infty$.","Suppose $1\le p < r < q < \infty$. Prove that $L^p\cap L^q \subset L^r$. So suppose $f\in L^p\cap L^q$. Then both $\int |f|^p d\mu$ and $\int|f|^q d\mu$ exist. For each $x$ in the domain of $f$, $|f(x)|^r$ is between $|f(x)|^p$ and $|f(x)|^q$. So $|f|^r\le\max(|f|^p,|f|^q)=\frac12({|f|^p+|f|^q+||f|^p-|f|^q|})$. The RHS is integrable, and since $|f|^r$ is bounded above by an integrable function, it is also integrable. So $f\in L^r$. Is my argument correct? Actually is this still true when $q=\infty$? In this case, we have $1\le p < r < \infty$. Is $L^p\cap L^{\infty}$ necessarily a subset of $L^r$? Why? Please show that if $f\in L^p\cap L^{\infty}$, then $||f||_r \le ||f|_p^{p/r} ||f||_{\infty}^{1-p/r}$ EDIT: Thanks for the link to the first part of my question. I'm still quite interested in the case $q = \infty$.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
63,Find the terms of the sequence $a_{n+1}=1+n/a_n$ that are natural numbers,Find the terms of the sequence  that are natural numbers,a_{n+1}=1+n/a_n,"Let's consider the sequence $(a_n)_{n\in\mathbb{N}}$, defined by the following recurrence relation:  $$ a_{n+1} = \begin{cases} 1 + \frac{n}{a_{n}}\quad&n\gt0\\ 1&n=0 \end{cases} $$ Find all terms of the sequence that are natural numbers. Two things to mention here: first one is that $a_{n}$ goes to $\infty$ when $n$ goes to $\infty$, and the second point is that the product of the first $k$ terms of the sequence is an integer number. This is all I have so far.","Let's consider the sequence $(a_n)_{n\in\mathbb{N}}$, defined by the following recurrence relation:  $$ a_{n+1} = \begin{cases} 1 + \frac{n}{a_{n}}\quad&n\gt0\\ 1&n=0 \end{cases} $$ Find all terms of the sequence that are natural numbers. Two things to mention here: first one is that $a_{n}$ goes to $\infty$ when $n$ goes to $\infty$, and the second point is that the product of the first $k$ terms of the sequence is an integer number. This is all I have so far.",,"['calculus', 'real-analysis', 'sequences-and-series', 'elementary-number-theory']"
64,When $(-)+M$ sends closed sets to closed sets?,When  sends closed sets to closed sets?,(-)+M,"Let $M$ be a subset of $\mathbb R^n$ . I wonder when $M+S$ remains closed for arbitrary closed set $S\subseteq \mathbb R^n$ ? I thought of this question in the study of topological group; but unfortunately, I still have few clues about the simple case of $\mathbb R^n$ . Here are some of my attemps. So far I have proved that $M$ has the closed-set-preserving property if ( $\Leftarrow$ ) $M$ is finite (this is trivial); $M$ is compact (this is well-known); $M$ is the intersection of finite many sets which are isometrically homeomorphic to $\mathbb R^{n-1}\times \mathbb R_{\geq 0}$ ; $M$ is the finite sum of 1.-3. above; $M$ is the finite union of 1.-4. above. It is also clear that $M$ has the closed-set-preserving property only if ( $\Rightarrow$ ) $M$ is closed, since [ $M$ is closed] $\Leftrightarrow $ [ $M+\{\mathrm{pt}\}$ is closed]. And I have proved that $M$ doesnot have to be convex. For $N=2$ , set $\Gamma:=\{(x,y)\mid xy\geq 1,x,y\geq 0\}$ , and its reflection $\Gamma':=\{(x,-y)\mid (x,y)\in \Gamma\}$ . We see that $\Gamma+\Gamma'=\mathbb R_{>0}\times \mathbb R$ is not closed. One can also generalise it to $\mathbb R^n$ .","Let be a subset of . I wonder when remains closed for arbitrary closed set ? I thought of this question in the study of topological group; but unfortunately, I still have few clues about the simple case of . Here are some of my attemps. So far I have proved that has the closed-set-preserving property if ( ) is finite (this is trivial); is compact (this is well-known); is the intersection of finite many sets which are isometrically homeomorphic to ; is the finite sum of 1.-3. above; is the finite union of 1.-4. above. It is also clear that has the closed-set-preserving property only if ( ) is closed, since [ is closed] [ is closed]. And I have proved that doesnot have to be convex. For , set , and its reflection . We see that is not closed. One can also generalise it to .","M \mathbb R^n M+S S\subseteq \mathbb R^n \mathbb R^n M \Leftarrow M M M \mathbb R^{n-1}\times \mathbb R_{\geq 0} M M M \Rightarrow M M \Leftrightarrow  M+\{\mathrm{pt}\} M N=2 \Gamma:=\{(x,y)\mid xy\geq 1,x,y\geq 0\} \Gamma':=\{(x,-y)\mid (x,y)\in \Gamma\} \Gamma+\Gamma'=\mathbb R_{>0}\times \mathbb R \mathbb R^n","['real-analysis', 'general-topology', 'analysis', 'algebraic-topology']"
65,A triple series evaluating to $\sqrt{3}$,A triple series evaluating to,\sqrt{3},How would you suggest me to approach the following triple series? I miss a starting point. $$\sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} (-1)^{i+j+k}\frac{1}{\displaystyle \sqrt{\left(i-\frac{1}{6}\right)^2+\left(j-\frac{1}{6}\right)^2+\left(k-\frac{1}{6}\right)^2}}=\sqrt{3}$$,How would you suggest me to approach the following triple series? I miss a starting point. $$\sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} \sum_{k=-\infty}^{\infty} (-1)^{i+j+k}\frac{1}{\displaystyle \sqrt{\left(i-\frac{1}{6}\right)^2+\left(j-\frac{1}{6}\right)^2+\left(k-\frac{1}{6}\right)^2}}=\sqrt{3}$$,,"['calculus', 'real-analysis', 'integration', 'sequences-and-series']"
66,Evaluating $\int_0^{\pi/2} \int_0^{\pi/2} \frac{\cos(x)}{ \cos(a \cos(x) \cos(y))} dx dy $,Evaluating,\int_0^{\pi/2} \int_0^{\pi/2} \frac{\cos(x)}{ \cos(a \cos(x) \cos(y))} dx dy ,Can we avoid the use of the geometric interpretation combined with polar coordinates change of variable for proving that $$\int_0^{\pi/2} \int_0^{\pi/2} \frac{\cos(x)}{ \cos(a \cos(x)  \cos(y))} d x dy =\frac{\pi}{ 2a}\log\left( \frac{\displaystyle 1+ \tan \left(\frac{a}{2}\right)}{ \displaystyle 1-\tan\left(\frac{a}{2}\right)}\right)$$  ? EDIT What if we go further and we also consider the case $$\int_0^{\pi/2}\int_0^{\pi/2} \int_0^{\pi/2} \frac{\cos(x)}{ \cos(a \cos(x)  \cos(y) \cos(z))} d x dy \ dz$$ ? What can we say about the closed form of this one?,Can we avoid the use of the geometric interpretation combined with polar coordinates change of variable for proving that $$\int_0^{\pi/2} \int_0^{\pi/2} \frac{\cos(x)}{ \cos(a \cos(x)  \cos(y))} d x dy =\frac{\pi}{ 2a}\log\left( \frac{\displaystyle 1+ \tan \left(\frac{a}{2}\right)}{ \displaystyle 1-\tan\left(\frac{a}{2}\right)}\right)$$  ? EDIT What if we go further and we also consider the case $$\int_0^{\pi/2}\int_0^{\pi/2} \int_0^{\pi/2} \frac{\cos(x)}{ \cos(a \cos(x)  \cos(y) \cos(z))} d x dy \ dz$$ ? What can we say about the closed form of this one?,,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
67,Can a sequence of functions have infinity as limit exactly at rationals?,Can a sequence of functions have infinity as limit exactly at rationals?,,"Someone asked me this question. And he said it's an exercise from Rudin's Real and Complex Analysis . Does there exist a sequence of continuous functions $f_n(x)$, such that $\lim_{n \to \infty} f_n(x)=+\infty$ iff $x \in \mathbb Q$ (or irrationals)? On the one hand, we know that if the limit of this function exists, then we can't have both as Baire's category theorem applies. But maybe it will happen that the suplim of this sequence is infinity at other points(at which the limit doesn't exists). Because lim equals infinity is the same as inflim equals infinity, so if someone can prove that rationals can't be the (countable)intersections of (countable)unions of G delta set then it's done. But on the other hand, I suspect that if we let $f_n(x)=\cos(\pi\cdot n!x)^n \cdot n$ is an example.","Someone asked me this question. And he said it's an exercise from Rudin's Real and Complex Analysis . Does there exist a sequence of continuous functions $f_n(x)$, such that $\lim_{n \to \infty} f_n(x)=+\infty$ iff $x \in \mathbb Q$ (or irrationals)? On the one hand, we know that if the limit of this function exists, then we can't have both as Baire's category theorem applies. But maybe it will happen that the suplim of this sequence is infinity at other points(at which the limit doesn't exists). Because lim equals infinity is the same as inflim equals infinity, so if someone can prove that rationals can't be the (countable)intersections of (countable)unions of G delta set then it's done. But on the other hand, I suspect that if we let $f_n(x)=\cos(\pi\cdot n!x)^n \cdot n$ is an example.",,"['real-analysis', 'sequences-and-series', 'baire-category']"
68,"Closure, Interior, and Boundary of Jordan Measurable Sets.","Closure, Interior, and Boundary of Jordan Measurable Sets.",,"This question has a number of parts.  Let $E\subset\mathbb{R}^{d}$ be a bounded subset. (1) Show that $m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E})$ (closure) (2) Show that $m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})$ (interior) (3) Show that $E$ is Jordan measurable if and only if $m^{\star}(\partial E)=0$. I originally had different proofs for parts (1) and (2), but I revised them because I wanted a more rigorous $\epsilon$-estimate type proof (I'm trying to practice getting better at this).  But, looking at them again, I think there is a flaw.  In particular, where I make the justification that we can ""fatten up"" or ""shrink down"" the covers in each part (see the proofs).  Basically, the reason why I think it is now flawed is because if one replaces $\frac{\epsilon}{N}$ by $\frac{\epsilon}{2^{n}}$, the proof for the outer Jordan measure (part (1)) seems to carry over word-for-word in the case of Lebesgue outer measure, and that's clearly false since $\mu^{\star}([0,1]\cap\mathbb{Q})=0$, yet $\mu^{\star}(\overline{[0,1]\cap\mathbb{Q}})=1$. Anyway, I would appreciate anyone's assistance in helping me correct these proofs.  Also, for part (3), I'm stuck on proving the implication $m^{\star,(J)}(\partial E)=0\rightarrow E\in\mathscr{J}(\mathbb{R}^{d})$. SOLUTION (1) By definition of $m^{\star,(J)}$, there exists a collection of boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} E\subset\bigcup\limits_{j=1}^{N}B_{j} &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\leq m^{\star,(J)}(E)+\epsilon \end{align*} hold for every $\epsilon>0$.  Because $E\Delta\bar{E}\subset E'$ (the set of limit points of $E$), by enlarging each box $B_{j}$ (as necessary), we can find a new collection of boxes $\{B'_{j}\}_{j=1}^{N}$ such that each of the conditions \begin{align*} |B_{j}|\leq|B'_{j}|\leq|B_{j}|+\frac{\epsilon}{N}, &&\bigcup_{j=1}^{N}B'_{j}\supset\bar{E}\supset E, &&\text{and} &&\sum\limits_{j=1}^{N}|B'_{j}|\leq m^{\star,(J)}(\bar{E})+\epsilon \end{align*} also holds for the same $\epsilon$.  Montonicity and the fact that $E\subset\bar{E}$ then gives \begin{align*} m^{\star,(J)}(E) &\leq m^{\star,(J)}(\bar{E})\\ &\leq\sum\limits_{j=1}^{N}|B'_{j}|\\ &\leq\sum\limits_{j=1}^{N}\left(|B_{j}|+\frac{\epsilon}{N}\right)\\ &\leq m^{\star, (J)}(E)+2\epsilon. \end{align*} In particular, we obtain the estimate $$\Bigg|m^{\star,(J)}(E)-m^{\star,(J)}(\bar{E})\Bigg|\leq2\epsilon,$$ and since $\epsilon$ was arbitrary, we conclude $m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E})$ as required. SOLUTION (2) By definition of $m_{\star, (J)}$ there exists boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} \bigcup\limits_{j=1}^{N}B_{j}\subset E &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\geq m_{\star,(J)}(E)-\epsilon \end{align*} hold for every $\epsilon>0$.  Because $E\Delta E^{\circ}\subset\partial E\subset E'\cup\{x\in E:N_{\delta}(x)\cap E=\{x\}\forall\delta>0\}$, and isolated points have inner (and outer) measure $0$, by shrinking each box $B_{j}$ (as necessary), we can find a new collection of boxes $\{B'_{j}\}_{j=1}^{N}$ such each of the conditions \begin{align*} |B_{j}|\geq|B'_{j}|\geq|B_{j}|-\frac{\epsilon}{N}, &&\bigcup\limits_{j=1}^{N}B'_{j}\subset E^{\circ}\subset E &&\text{and} &&\sum\limits_{j=1}^{N}|B'_{j}|\geq m_{\star,(J)}(E^{\circ})-\epsilon \end{align*} also holds for the same $\epsilon$.  Montonicity and the fact that $E^{\circ}\subset E$ then gives \begin{align*} m_{\star,(J)} &\geq m_{\star,(J)}(E^{\circ})\\ &\geq\sum\limits_{j=1}^{N}|B'_{j}|\\ &\geq\sum\limits_{j=1}^{N}\left(|B_{j}|-\frac{\epsilon}{N}\right)\\ &\geq m_{\star,(J)}(E)-2\epsilon. \end{align*} In particular, we obtain the estimate $$\Bigg|m_{\star,(J)}(E^{\circ})-m_{\star,(J)}(E)\Bigg|\leq2\epsilon,$$ and since $\epsilon>0$ was arbitrary, we conclude $m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})$ as required. SOLUTION (3) We have the inequality $$m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})\leq m^{\star,(J)}(\bar{E})=m^{\star,(J)}(E).$$ In the case that $E$ is Jordan measurable, this immediately becomes an equality $$m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})=m(E)=m^{\star,(J)}(\bar{E})=m^{\star,(J)}(E),$$ and monotonicity then implies that $m^{\star,(J)}(E\Delta\bar{E})=m^{\star,(J)}(E\Delta E^{\circ})=0$, which means that both $\bar{E}$ and $E^{\circ}$ are Jordan measurable with measure $m(E)$.  Boolean closure then implies $\partial E$ is Jordan measurable since $\partial E=\bar{E}-E^{\circ}$, and from additivity we have $$m(\partial E)=m(\bar{E})-m(E^{\circ})=0.$$ Now suppose $\partial E=0$. Then sub-additivity of $m^{\star,(J)}$ and $m_{\star,(J)}$ implies $$0=m^{\star,(J)}_{\star,(J)}(\partial E)\geq m^{\star,(J)}_{\star,(J)}(\bar{E})-m^{\star,(J)}_{\star,(J)}(E^{\circ})\geq0,$$ so that the original inequality is extended to $$m_{\star,(J)}(E^{\circ})=m_{\star,(J)}(E)=m_{\star,(J)}(\bar{E})\leq m^{\star,(J)}(E^{\circ})=m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E}).$$ COMMENTS (Update 1) I want to look a little closer at my argument in (1).  I can't see why it's incorrect, but perhaps a modification will make it more rigorous. For convenience, let's used cubes $Q_{j}$ instead of boxes $B_{j}$ (that the former can be used in the definition of Jordan content is an easy consequence of the fact that any box $B_{j}$ can be arbitrarily approximated by a finite number of cubes by an obvious dissection process). Then I have a collection of cubes $\{Q_{j}\}_{j=1}^{N}$ such that $\bigcup_{j=1}^{N} Q_{j}\supset E$ and $\sum_{j=1}^{N}|Q_{j}|\leq m^{\star,(J)}(E)+\epsilon$. Suppose each cube has some index length $\ell_{j}$, and consider the cubes $\{Q_{j}'\}_{j=1}^{N}$ obtained by enlarging each $\ell_{j}\mapsto\ell_{j}+\frac{epsilon}{N}$. Then this collection covers $\bar{E}$ for the following reasons.  If $E$ has any isolated points, then the $Q_{j}$ already covered them, so the $Q'_{j}$ certainly cover them as well.  Therefore, the only other points in $\bar{E}\backslash E$ are limit points of the set $E$.  But limit points are arbitrarily close to $E$, the set covered by the $Q_{j}$ (they have distance $0$ from the cover), and since by the explicit construction $dist(Q_{j},Q'_{j})=\frac{\epsilon}{N}>0$ and $Q'_{j}$ also cover $E$, we conclude the $Q'_{j}$ also cover $\bar{E}$.  (If this is wrong, pleeaaassseee explain to me the logical fault). Then we have \begin{align*} m^{\star,(J)}(\bar{E}) &\leq\sum_{j=1}^{N}|Q'_{j}|\\ &=\sum_{j=1}^{N}\left(\ell_{j}+\frac{\epsilon}{N}\right)^{d}\\ &=\sum_{j=1}^{N}\sum_{k=0}^{d}\binom{d}{k}\left(\ell_{j}^{k}\left(\frac{\epsilon}{N}\right)^{d-k}\right)\\ &=\sum_{j=1}^{N}\left(\ell_{j}^{d}+d\frac{\epsilon}{N}\ell_{j}^{d-1}+\ldots+\frac{\epsilon^{d}}{N^{d}}\right)\\ &=\sum_{j=1}^{N}|Q_{j}|+NO\left(\frac{\epsilon}{N}\right)\\ &\leq m^{\star,(J)}(E)+O(\epsilon) \end{align*} which shows that the outer measure of $\bar{E}$ is less than the outer measure of $E$. I still don't know if this is fully rigorous or not though.  It still has the problem that the argument for outer Lebesgue measure carries over by using $\ell_{j}'=\ell_{j}+\frac{\epsilon}{2^{j}}$ as each of the terms in the binomial expansion now are convergent infinite series and the result is still $O(\epsilon)$.  Moreover, we would still also have $dist(Q_{j},Q'_{j})=\frac{\epsilon}{2^{j}}>0$. So I guess that basically proves there's something wrong with my logic.  I just don't know how to fix it. (Update 2) So I have concluded there is nothing inherently wrong with my argument, just that it is incomplete.  For one thing, you cannot conclude that an ""$\epsilon$-close"" countable covering of a set $E$ can be ""$\epsilon$-fattened"" to produce a cover of $\bar{E}$.  The counter example is $[0,1]\cap\mathbb{Q}$.  The reason for this somewhat peculiar fact is illustrated in this question I asked earlier today ( Paradox as to Measure of Countable Dense Subsets? ).  But to put it summarily, countable coverings allow you to circumvent density arguments, in the sense that you can still cover $E$ while avoiding $\bar{E}$, and in fact do so on a set of positive outer measure; hence once $\epsilon$ is sufficiently small, the fattened cubes will still fail to cover $\bar{E}$.  That this cannot happen in the finite case is made rigorous in the following (completely) redone solutions to (1) and (2).  Actually, fattening of the original cover of $E$ is not even necessary, as it is shown that a finite cover of $E$ is necessarily a finite cover of $\bar{E}$, leading to a new characterization of outer Jordan measure involving closed boxes only.  Moreover, we also obtain another surprising explanation as to why countable coverings can avoid this conclusion, the main idea being that countable unions of closed sets need not be closed.  The same arguments apply in reverse for the case of inner Jordan measure of $E$ and $E^{\circ}$, where it is shown that any containment by $E$ is necessarily a containment by $E^{\circ}$, hence leading to a new definition of inner Jordan measure involving open boxes only.  It is interesting to note that countable containments cannot avoid this conclusion (in contrast to the analogous situation for covers), and this stems from the fact that countable unions of open boxes are open.  Incidentally, we conclude that nothing is gained by considering countable containments, hence why we do not consider a Lebesgue inner measure. I will formulate this all into a complete answer once I prove (3). Let $E\subset\mathbb{R}^{d}$ be a bounded set. (1) By definition of $m^{\star,(J)}$, there exists a collection of boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} E\subset\bigcup\limits_{j=1}^{N}B_{j} &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\leq m^{\star,(J)}(E)+\epsilon \end{align*} hold for every $\epsilon>0$.  Now the definition of elementary measure implies $m(B_{j})=|B_{j}|=\bar{B_{j}}|=m(B_{j})$, and because $\bigcup_{j=1}^{N}\bar{B_{j}}\supset E,$ we may assume each $B_{j}$ is closed.  But this implies that the cover is itself closed since $N$ is finite, e.g. $\bigcup_{j=1}^{N}B_{j}=\overline{\bigcup_{j=1}^{N}B_{j}}.$  Moreover, as $\bar{E}$ is the \emph{smallest} closed set which contains $E$, we have that $\{B_{j}\}_{j=1}^{N}$ is an ``$\epsilon$-close'' cover of $\bar{E}$ as well.  It follows from monotonicity and the previous remarks that $$m^{\star,(J)}(E)\leq m^{\star,(J)}(\bar{E})\leq\sum_{j=1}^{N}|B_{j}|\leq m^{\star,(J)}(E)+\epsilon\leq m^{\star,(J)}(\bar{E})+\epsilon.$$ In particular, we obtain the estimate $$\Bigg|m^{\star,(J)}(E)-m^{\star,(J)}(\bar{E})\Bigg|\leq\epsilon,$$ and since $\epsilon$ was arbitrary, we conclude $m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E})$ as required. (2) By definition of $m_{\star,(J)}$, there exists a collection of boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} E\supset\bigcup\limits_{j=1}^{N}B_{j} &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\geq m_{\star,(J)}(E)-\epsilon \end{align*} hold for every $\epsilon>0$.  Now the definition of elementary measure implies $m(B_{j})=|B_{j}|=|B_{j}^{\circ}|=m(B_{j}^{\circ})$, and because $E\supset\bigcup_{j=1}^{N}B^{\circ}_{j}$, we may assume each $B_{j}$ is open.  But this implies that the contained set is itself open, e.g. $\bigcup_{j=1}^{N}B_{j}=\left(\bigcup_{j=1}^{N}B_{j}\right)^{\circ}.$  Moreover, as $E^{\circ}$ is the \emph{largest} open set which is contained in $E$, we have that $\{B_{j}\}_{j=1}^{N}$ is an ``$\epsilon$-close'' set contained in $E^{\circ}$ as well.  It follows from monotonicity and the previous remarks that $$m_{\star,(J)}(E)\geq m_{\star,(J)}(E^{\circ})\geq\sum_{j=1}^{N}|B_{j}|\geq m_{\star,(J)}(E)-\epsilon\geq m_{\star,(J)}(E^{\circ})-\epsilon.$$ In particular, we obtain the estimate $$\Bigg|m_{\star,(J)}(E)-m_{\star,(J)}(E^{\circ})\Bigg|\leq\epsilon,$$ and since $\epsilon$ was arbitrary, we conclude $m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})$ as required. (Some Remarks) These results allow us to redefine $m^{\star,(J)}$ as the infimal elementary measure of a cover of $E$ by a finite number of closed boxes, and $m_{\star,(J)}$ as the supremal elementary measure of a containment by $E$ consisting of finite numbers of open boxes.  These characterizations are are just as valid for countable coverings and containments.  However, to obtain the conclusion from (1), finiteness of the cover is essential.  The issue is that a countable union of closed boxes need not be closed, and so we can not conclude that the outer measures of $E$ and $\bar{E}$ coincide when countable covers are permitted.  The set $A=[0,1]\cap\mathbb{Q}$ is a typical example of how the conclusion of (1) can fail when countable coverings are allowed.  The Lebesgue outer measure of $A$ is $0$ since it can be covered by a countable union of degenerate boxes, while the Jordan outer measure of $A$ is $1$ since $\bar{A}=[0,1]$.  On the other hand, no problem occurs when finite containments are upgraded to countable containments.  This is because a countable union of open boxes is always open, and so the ""Lebesgue inner measure"" of $E$ must still coincide with the Lebesgue inner measure of $E^{\circ}$, since $E^{\circ}$ is the largest open set contained in $E$.  Moreover, every open set $\mathscr{O}$ is the countable union of pairwise disjoint boxes, and by being a union of such boxes (as opposed to an intersection), we see that every finite subcollection of such boxes is contained in $\mathscr{O}$.  It is easy to see then that the inner Jordan and Lebesgue measures of $\mathscr{O}$ coincide, and the conclusion from (2) implies that they agree for all sets.  Incidentally, this demonstrates our lack of need for an inner Lebesgue measure.  In fact, Littlewood's characterization of Lebesgue measurable sets being ``nearly open'' is basically equivalent to showing that the inner and outer Lebesgue measures are equal; in any case, clearly nothing is gained by upgrading finite containments to countable ones.","This question has a number of parts.  Let $E\subset\mathbb{R}^{d}$ be a bounded subset. (1) Show that $m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E})$ (closure) (2) Show that $m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})$ (interior) (3) Show that $E$ is Jordan measurable if and only if $m^{\star}(\partial E)=0$. I originally had different proofs for parts (1) and (2), but I revised them because I wanted a more rigorous $\epsilon$-estimate type proof (I'm trying to practice getting better at this).  But, looking at them again, I think there is a flaw.  In particular, where I make the justification that we can ""fatten up"" or ""shrink down"" the covers in each part (see the proofs).  Basically, the reason why I think it is now flawed is because if one replaces $\frac{\epsilon}{N}$ by $\frac{\epsilon}{2^{n}}$, the proof for the outer Jordan measure (part (1)) seems to carry over word-for-word in the case of Lebesgue outer measure, and that's clearly false since $\mu^{\star}([0,1]\cap\mathbb{Q})=0$, yet $\mu^{\star}(\overline{[0,1]\cap\mathbb{Q}})=1$. Anyway, I would appreciate anyone's assistance in helping me correct these proofs.  Also, for part (3), I'm stuck on proving the implication $m^{\star,(J)}(\partial E)=0\rightarrow E\in\mathscr{J}(\mathbb{R}^{d})$. SOLUTION (1) By definition of $m^{\star,(J)}$, there exists a collection of boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} E\subset\bigcup\limits_{j=1}^{N}B_{j} &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\leq m^{\star,(J)}(E)+\epsilon \end{align*} hold for every $\epsilon>0$.  Because $E\Delta\bar{E}\subset E'$ (the set of limit points of $E$), by enlarging each box $B_{j}$ (as necessary), we can find a new collection of boxes $\{B'_{j}\}_{j=1}^{N}$ such that each of the conditions \begin{align*} |B_{j}|\leq|B'_{j}|\leq|B_{j}|+\frac{\epsilon}{N}, &&\bigcup_{j=1}^{N}B'_{j}\supset\bar{E}\supset E, &&\text{and} &&\sum\limits_{j=1}^{N}|B'_{j}|\leq m^{\star,(J)}(\bar{E})+\epsilon \end{align*} also holds for the same $\epsilon$.  Montonicity and the fact that $E\subset\bar{E}$ then gives \begin{align*} m^{\star,(J)}(E) &\leq m^{\star,(J)}(\bar{E})\\ &\leq\sum\limits_{j=1}^{N}|B'_{j}|\\ &\leq\sum\limits_{j=1}^{N}\left(|B_{j}|+\frac{\epsilon}{N}\right)\\ &\leq m^{\star, (J)}(E)+2\epsilon. \end{align*} In particular, we obtain the estimate $$\Bigg|m^{\star,(J)}(E)-m^{\star,(J)}(\bar{E})\Bigg|\leq2\epsilon,$$ and since $\epsilon$ was arbitrary, we conclude $m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E})$ as required. SOLUTION (2) By definition of $m_{\star, (J)}$ there exists boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} \bigcup\limits_{j=1}^{N}B_{j}\subset E &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\geq m_{\star,(J)}(E)-\epsilon \end{align*} hold for every $\epsilon>0$.  Because $E\Delta E^{\circ}\subset\partial E\subset E'\cup\{x\in E:N_{\delta}(x)\cap E=\{x\}\forall\delta>0\}$, and isolated points have inner (and outer) measure $0$, by shrinking each box $B_{j}$ (as necessary), we can find a new collection of boxes $\{B'_{j}\}_{j=1}^{N}$ such each of the conditions \begin{align*} |B_{j}|\geq|B'_{j}|\geq|B_{j}|-\frac{\epsilon}{N}, &&\bigcup\limits_{j=1}^{N}B'_{j}\subset E^{\circ}\subset E &&\text{and} &&\sum\limits_{j=1}^{N}|B'_{j}|\geq m_{\star,(J)}(E^{\circ})-\epsilon \end{align*} also holds for the same $\epsilon$.  Montonicity and the fact that $E^{\circ}\subset E$ then gives \begin{align*} m_{\star,(J)} &\geq m_{\star,(J)}(E^{\circ})\\ &\geq\sum\limits_{j=1}^{N}|B'_{j}|\\ &\geq\sum\limits_{j=1}^{N}\left(|B_{j}|-\frac{\epsilon}{N}\right)\\ &\geq m_{\star,(J)}(E)-2\epsilon. \end{align*} In particular, we obtain the estimate $$\Bigg|m_{\star,(J)}(E^{\circ})-m_{\star,(J)}(E)\Bigg|\leq2\epsilon,$$ and since $\epsilon>0$ was arbitrary, we conclude $m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})$ as required. SOLUTION (3) We have the inequality $$m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})\leq m^{\star,(J)}(\bar{E})=m^{\star,(J)}(E).$$ In the case that $E$ is Jordan measurable, this immediately becomes an equality $$m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})=m(E)=m^{\star,(J)}(\bar{E})=m^{\star,(J)}(E),$$ and monotonicity then implies that $m^{\star,(J)}(E\Delta\bar{E})=m^{\star,(J)}(E\Delta E^{\circ})=0$, which means that both $\bar{E}$ and $E^{\circ}$ are Jordan measurable with measure $m(E)$.  Boolean closure then implies $\partial E$ is Jordan measurable since $\partial E=\bar{E}-E^{\circ}$, and from additivity we have $$m(\partial E)=m(\bar{E})-m(E^{\circ})=0.$$ Now suppose $\partial E=0$. Then sub-additivity of $m^{\star,(J)}$ and $m_{\star,(J)}$ implies $$0=m^{\star,(J)}_{\star,(J)}(\partial E)\geq m^{\star,(J)}_{\star,(J)}(\bar{E})-m^{\star,(J)}_{\star,(J)}(E^{\circ})\geq0,$$ so that the original inequality is extended to $$m_{\star,(J)}(E^{\circ})=m_{\star,(J)}(E)=m_{\star,(J)}(\bar{E})\leq m^{\star,(J)}(E^{\circ})=m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E}).$$ COMMENTS (Update 1) I want to look a little closer at my argument in (1).  I can't see why it's incorrect, but perhaps a modification will make it more rigorous. For convenience, let's used cubes $Q_{j}$ instead of boxes $B_{j}$ (that the former can be used in the definition of Jordan content is an easy consequence of the fact that any box $B_{j}$ can be arbitrarily approximated by a finite number of cubes by an obvious dissection process). Then I have a collection of cubes $\{Q_{j}\}_{j=1}^{N}$ such that $\bigcup_{j=1}^{N} Q_{j}\supset E$ and $\sum_{j=1}^{N}|Q_{j}|\leq m^{\star,(J)}(E)+\epsilon$. Suppose each cube has some index length $\ell_{j}$, and consider the cubes $\{Q_{j}'\}_{j=1}^{N}$ obtained by enlarging each $\ell_{j}\mapsto\ell_{j}+\frac{epsilon}{N}$. Then this collection covers $\bar{E}$ for the following reasons.  If $E$ has any isolated points, then the $Q_{j}$ already covered them, so the $Q'_{j}$ certainly cover them as well.  Therefore, the only other points in $\bar{E}\backslash E$ are limit points of the set $E$.  But limit points are arbitrarily close to $E$, the set covered by the $Q_{j}$ (they have distance $0$ from the cover), and since by the explicit construction $dist(Q_{j},Q'_{j})=\frac{\epsilon}{N}>0$ and $Q'_{j}$ also cover $E$, we conclude the $Q'_{j}$ also cover $\bar{E}$.  (If this is wrong, pleeaaassseee explain to me the logical fault). Then we have \begin{align*} m^{\star,(J)}(\bar{E}) &\leq\sum_{j=1}^{N}|Q'_{j}|\\ &=\sum_{j=1}^{N}\left(\ell_{j}+\frac{\epsilon}{N}\right)^{d}\\ &=\sum_{j=1}^{N}\sum_{k=0}^{d}\binom{d}{k}\left(\ell_{j}^{k}\left(\frac{\epsilon}{N}\right)^{d-k}\right)\\ &=\sum_{j=1}^{N}\left(\ell_{j}^{d}+d\frac{\epsilon}{N}\ell_{j}^{d-1}+\ldots+\frac{\epsilon^{d}}{N^{d}}\right)\\ &=\sum_{j=1}^{N}|Q_{j}|+NO\left(\frac{\epsilon}{N}\right)\\ &\leq m^{\star,(J)}(E)+O(\epsilon) \end{align*} which shows that the outer measure of $\bar{E}$ is less than the outer measure of $E$. I still don't know if this is fully rigorous or not though.  It still has the problem that the argument for outer Lebesgue measure carries over by using $\ell_{j}'=\ell_{j}+\frac{\epsilon}{2^{j}}$ as each of the terms in the binomial expansion now are convergent infinite series and the result is still $O(\epsilon)$.  Moreover, we would still also have $dist(Q_{j},Q'_{j})=\frac{\epsilon}{2^{j}}>0$. So I guess that basically proves there's something wrong with my logic.  I just don't know how to fix it. (Update 2) So I have concluded there is nothing inherently wrong with my argument, just that it is incomplete.  For one thing, you cannot conclude that an ""$\epsilon$-close"" countable covering of a set $E$ can be ""$\epsilon$-fattened"" to produce a cover of $\bar{E}$.  The counter example is $[0,1]\cap\mathbb{Q}$.  The reason for this somewhat peculiar fact is illustrated in this question I asked earlier today ( Paradox as to Measure of Countable Dense Subsets? ).  But to put it summarily, countable coverings allow you to circumvent density arguments, in the sense that you can still cover $E$ while avoiding $\bar{E}$, and in fact do so on a set of positive outer measure; hence once $\epsilon$ is sufficiently small, the fattened cubes will still fail to cover $\bar{E}$.  That this cannot happen in the finite case is made rigorous in the following (completely) redone solutions to (1) and (2).  Actually, fattening of the original cover of $E$ is not even necessary, as it is shown that a finite cover of $E$ is necessarily a finite cover of $\bar{E}$, leading to a new characterization of outer Jordan measure involving closed boxes only.  Moreover, we also obtain another surprising explanation as to why countable coverings can avoid this conclusion, the main idea being that countable unions of closed sets need not be closed.  The same arguments apply in reverse for the case of inner Jordan measure of $E$ and $E^{\circ}$, where it is shown that any containment by $E$ is necessarily a containment by $E^{\circ}$, hence leading to a new definition of inner Jordan measure involving open boxes only.  It is interesting to note that countable containments cannot avoid this conclusion (in contrast to the analogous situation for covers), and this stems from the fact that countable unions of open boxes are open.  Incidentally, we conclude that nothing is gained by considering countable containments, hence why we do not consider a Lebesgue inner measure. I will formulate this all into a complete answer once I prove (3). Let $E\subset\mathbb{R}^{d}$ be a bounded set. (1) By definition of $m^{\star,(J)}$, there exists a collection of boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} E\subset\bigcup\limits_{j=1}^{N}B_{j} &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\leq m^{\star,(J)}(E)+\epsilon \end{align*} hold for every $\epsilon>0$.  Now the definition of elementary measure implies $m(B_{j})=|B_{j}|=\bar{B_{j}}|=m(B_{j})$, and because $\bigcup_{j=1}^{N}\bar{B_{j}}\supset E,$ we may assume each $B_{j}$ is closed.  But this implies that the cover is itself closed since $N$ is finite, e.g. $\bigcup_{j=1}^{N}B_{j}=\overline{\bigcup_{j=1}^{N}B_{j}}.$  Moreover, as $\bar{E}$ is the \emph{smallest} closed set which contains $E$, we have that $\{B_{j}\}_{j=1}^{N}$ is an ``$\epsilon$-close'' cover of $\bar{E}$ as well.  It follows from monotonicity and the previous remarks that $$m^{\star,(J)}(E)\leq m^{\star,(J)}(\bar{E})\leq\sum_{j=1}^{N}|B_{j}|\leq m^{\star,(J)}(E)+\epsilon\leq m^{\star,(J)}(\bar{E})+\epsilon.$$ In particular, we obtain the estimate $$\Bigg|m^{\star,(J)}(E)-m^{\star,(J)}(\bar{E})\Bigg|\leq\epsilon,$$ and since $\epsilon$ was arbitrary, we conclude $m^{\star,(J)}(E)=m^{\star,(J)}(\bar{E})$ as required. (2) By definition of $m_{\star,(J)}$, there exists a collection of boxes $\{B_{j}\}_{j=1}^{N}$ such that both \begin{align*} E\supset\bigcup\limits_{j=1}^{N}B_{j} &&\text{and} &&\sum\limits_{j=1}^{N}|B_{j}|\geq m_{\star,(J)}(E)-\epsilon \end{align*} hold for every $\epsilon>0$.  Now the definition of elementary measure implies $m(B_{j})=|B_{j}|=|B_{j}^{\circ}|=m(B_{j}^{\circ})$, and because $E\supset\bigcup_{j=1}^{N}B^{\circ}_{j}$, we may assume each $B_{j}$ is open.  But this implies that the contained set is itself open, e.g. $\bigcup_{j=1}^{N}B_{j}=\left(\bigcup_{j=1}^{N}B_{j}\right)^{\circ}.$  Moreover, as $E^{\circ}$ is the \emph{largest} open set which is contained in $E$, we have that $\{B_{j}\}_{j=1}^{N}$ is an ``$\epsilon$-close'' set contained in $E^{\circ}$ as well.  It follows from monotonicity and the previous remarks that $$m_{\star,(J)}(E)\geq m_{\star,(J)}(E^{\circ})\geq\sum_{j=1}^{N}|B_{j}|\geq m_{\star,(J)}(E)-\epsilon\geq m_{\star,(J)}(E^{\circ})-\epsilon.$$ In particular, we obtain the estimate $$\Bigg|m_{\star,(J)}(E)-m_{\star,(J)}(E^{\circ})\Bigg|\leq\epsilon,$$ and since $\epsilon$ was arbitrary, we conclude $m_{\star,(J)}(E)=m_{\star,(J)}(E^{\circ})$ as required. (Some Remarks) These results allow us to redefine $m^{\star,(J)}$ as the infimal elementary measure of a cover of $E$ by a finite number of closed boxes, and $m_{\star,(J)}$ as the supremal elementary measure of a containment by $E$ consisting of finite numbers of open boxes.  These characterizations are are just as valid for countable coverings and containments.  However, to obtain the conclusion from (1), finiteness of the cover is essential.  The issue is that a countable union of closed boxes need not be closed, and so we can not conclude that the outer measures of $E$ and $\bar{E}$ coincide when countable covers are permitted.  The set $A=[0,1]\cap\mathbb{Q}$ is a typical example of how the conclusion of (1) can fail when countable coverings are allowed.  The Lebesgue outer measure of $A$ is $0$ since it can be covered by a countable union of degenerate boxes, while the Jordan outer measure of $A$ is $1$ since $\bar{A}=[0,1]$.  On the other hand, no problem occurs when finite containments are upgraded to countable containments.  This is because a countable union of open boxes is always open, and so the ""Lebesgue inner measure"" of $E$ must still coincide with the Lebesgue inner measure of $E^{\circ}$, since $E^{\circ}$ is the largest open set contained in $E$.  Moreover, every open set $\mathscr{O}$ is the countable union of pairwise disjoint boxes, and by being a union of such boxes (as opposed to an intersection), we see that every finite subcollection of such boxes is contained in $\mathscr{O}$.  It is easy to see then that the inner Jordan and Lebesgue measures of $\mathscr{O}$ coincide, and the conclusion from (2) implies that they agree for all sets.  Incidentally, this demonstrates our lack of need for an inner Lebesgue measure.  In fact, Littlewood's characterization of Lebesgue measurable sets being ``nearly open'' is basically equivalent to showing that the inner and outer Lebesgue measures are equal; in any case, clearly nothing is gained by upgrading finite containments to countable ones.",,"['real-analysis', 'measure-theory']"
69,"Show $\lim\limits_{n \to \infty } {a_n} = \frac{1}{\pi }{\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)$",Show,"\lim\limits_{n \to \infty } {a_n} = \frac{1}{\pi }{\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)","Let $a_1= 0,a_2=1$ and $a_n = \sqrt {\frac{{{a_{n - 1}} + {a_{n - 2}}}}{2} \cdot {a_{n - 1}}}$ for $n \geqslant 3$ . Is it true that $\lim\limits_{n \to \infty } {a_n} = \frac{1}{\pi}{\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)$ , where $\operatorname{B}(x_1,x_2)$ is the Beta function? Having experimented with Maple and wxMaxima, I think this statement should be provable(?). For example, $a_{20}=.7627597635$ while $\frac{1}{\pi }{\rm B}\left( {\frac{1}{2},\frac{3}{4}} \right)=.7627597633...$ . I was also wondering whether this sequence is convergent, and I think I can show that. In fact, the subsequence $\langle a_{2n+1} \mid n\in\mathbb{N} \rangle$ is increasing while $\langle a_{2n} \mid n\in\mathbb{N} \rangle$ is decreasing and both subsequences are bounded, so they converge. Moreover, their limits are the same, so $\langle a_{n} \mid n\in\mathbb{N} \rangle$ converges, and even rough estimates show that the limit lies in $(0.758,0.766)$ . But to show that it is exactly $\frac{1}{\pi }{\rm B}\left( {\frac{1}{2},\frac{3}{4}} \right)$ is more difficult. Nonetheless, I will write down the proof of convergence. Perhaps, it may hint somehow to ${\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right)$ expressed as an infinite sum or infinite product or whatever. Let's start with the observation that for all $n$ : $0\leq a_{n}\leq1$ . This holds since given two values in the interval $[0,1]$ , their average and product lie again in this interval. So in particular the sequence $\langle a_{n} \mid n\in\mathbb{N} \rangle$ is bounded. What’s more, the subsequence $\langle a_{2n+1} \mid n\in\mathbb{N} \rangle$ is increasing while $\langle a_{2n} \mid n\in\mathbb{N} \rangle$ is decreasing. We shall prove this by induction, making use of the well known inequality $\sqrt{x y}\leq{\frac{x+y}{2}}$ . Hence we get $$\tag{1}a_{n-2}^{\frac{1}{4}}\cdot a_{n-1}^{\frac{3}{4}}\leq a_{n}\leq\frac{a_{n-2}}{4}+\frac{3a_{n-1}}{4}$$ Now we can prove that for all $n$ we have $$\tag{2}a_{1}\lt \cdots\lt a_{2n-1}$$ $$\tag{3}a_{2}\gt \cdot\cdot\cdot\gt a_{2n}$$ $$\tag{4}a_{2n-1}\lt a_{2n}$$ For $n=1$ we see that (2) and (3) are vacuously true and (4) is obvious. And for the induction step we have to show $$\tag{5}a_{2n-1}\lt a_{2n+1}\lt a_{2n+2}\lt a_{2n}$$ We start by observing that $$\tag{6}a_{2n+1}\lt a_{2n}$$ since by definition $\displaystyle a_{2n+1}=\sqrt{\frac{a_{2n-1}+a_{2n}}{2}\cdot a_{2n}}$ and from (4) it follows that $\displaystyle{\frac{a_{2n-1}+a_{2n}}{2}}\,\lt \,a_{2n}$ . Now for the first inequality of (5), note that by the first inequality of (1) we have $a_{2n+1}^{4}\ge a_{2n-1}\cdot a_{2n}^{3}$ , and by (6) this is greater than $a_{2n-1}\cdot a_{2n+1}^{3}$ , so $a_{2n+1}\gt a_{2n-1}$ as required. Similarly, $a_{2n+2}^{4}\geq a_{2n}\cdot a_{2n+1}^{3}\gt a_{2n+1}^{4}$ , which proves the second inequality of (5). For the last inequality of (5) we use the second inequality of (1), so $\displaystyle a_{2n+2}\,\leq\,\frac{a_{2n}+3a_{3n+1}}{4\,a n+1}\,\lt \,a_{2n}$ (by (6)). That completes the induction proof. Now we can easily prove that both subsequences converge to the same limit. This is because by (1) $\displaystyle a_{2n+2}-a_{2n+1}\leq{\frac{a_{2n}-a_{2n+1}}{4}}$ and by (5) this last difference is less than $\displaystyle\frac{a_{2n}-a_{2n-1}}{4}$ . So the distance between $a_{2n+2}$ and $a_{2n+1}$ is less than a quarter of the distance between $a_{2n}$ and $a_{2n-1}$ , and therefore these distances go to zero. That implies both subsequences have the same limit. Regarding the value $l$ of the limit, if - inspired by (1) - we define the sequence $\langle b_{n}|n\in\mathbb{N}\rangle $ by $\displaystyle b_{n}={\frac{b_{n-2}}{4}}+{\frac{3b_{n-1}}{4}}$ and $b_1=0,b_2=1$ , then clearly $a_{n}\leq b_{n}$ . Also, it is not hard to prove that $\displaystyle b_{n}=\frac{4}{5}\left(1-\left(-{\frac{1}{4}}\right)^{n-1}\right)$ , which converges to $0.8$ . In the same way, if we define $c_{n}=c_{n-2}^{\frac{1}{4}}\cdot c_{n-1}^{\frac{3}{4}}$ , then we better ignore the initial value $0$ , so we just put $c_{2}=1,c_{3}=a_{3}={\sqrt{\frac{1}{2}}}$ and when we switch to $d_{n}:=\log_{2}c_{n}$ , then $d_{2}=0,d_{3}=-{\frac{1}{2}}$ and $d_n$ satisfies the same recurrence relation as $b_n$ . Therefore, for $n≥2$ , $\displaystyle d_{n}=\frac{2}{5}\left(-1+\left(-\frac{1}{4}\right)^{n-2}\right)$ , which converges to $-\frac{2}{5}$ , so $c_n$ converges to $2^{-{\frac{2}{5}}}$ , which is approximately $0.758$ . In fact, returning to $b_n$ , but using initial values $b_{2}=1,b_{3}=a_{3}={\sqrt{\frac{1}{2}}}$ , we can show $b_n$ converges to $\displaystyle\frac{1+2{\sqrt{2}}}{5}$ , which is approximately $0.766$ .","Let and for . Is it true that , where is the Beta function? Having experimented with Maple and wxMaxima, I think this statement should be provable(?). For example, while . I was also wondering whether this sequence is convergent, and I think I can show that. In fact, the subsequence is increasing while is decreasing and both subsequences are bounded, so they converge. Moreover, their limits are the same, so converges, and even rough estimates show that the limit lies in . But to show that it is exactly is more difficult. Nonetheless, I will write down the proof of convergence. Perhaps, it may hint somehow to expressed as an infinite sum or infinite product or whatever. Let's start with the observation that for all : . This holds since given two values in the interval , their average and product lie again in this interval. So in particular the sequence is bounded. What’s more, the subsequence is increasing while is decreasing. We shall prove this by induction, making use of the well known inequality . Hence we get Now we can prove that for all we have For we see that (2) and (3) are vacuously true and (4) is obvious. And for the induction step we have to show We start by observing that since by definition and from (4) it follows that . Now for the first inequality of (5), note that by the first inequality of (1) we have , and by (6) this is greater than , so as required. Similarly, , which proves the second inequality of (5). For the last inequality of (5) we use the second inequality of (1), so (by (6)). That completes the induction proof. Now we can easily prove that both subsequences converge to the same limit. This is because by (1) and by (5) this last difference is less than . So the distance between and is less than a quarter of the distance between and , and therefore these distances go to zero. That implies both subsequences have the same limit. Regarding the value of the limit, if - inspired by (1) - we define the sequence by and , then clearly . Also, it is not hard to prove that , which converges to . In the same way, if we define , then we better ignore the initial value , so we just put and when we switch to , then and satisfies the same recurrence relation as . Therefore, for , , which converges to , so converges to , which is approximately . In fact, returning to , but using initial values , we can show converges to , which is approximately .","a_1= 0,a_2=1 a_n = \sqrt {\frac{{{a_{n - 1}} + {a_{n - 2}}}}{2} \cdot {a_{n - 1}}} n \geqslant 3 \lim\limits_{n \to \infty } {a_n} = \frac{1}{\pi}{\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right) \operatorname{B}(x_1,x_2) a_{20}=.7627597635 \frac{1}{\pi }{\rm B}\left( {\frac{1}{2},\frac{3}{4}} \right)=.7627597633... \langle a_{2n+1} \mid n\in\mathbb{N} \rangle \langle a_{2n} \mid n\in\mathbb{N} \rangle \langle a_{n} \mid n\in\mathbb{N} \rangle (0.758,0.766) \frac{1}{\pi }{\rm B}\left( {\frac{1}{2},\frac{3}{4}} \right) {\operatorname{B}}\left( {\frac{1}{2},\frac{3}{4}} \right) n 0\leq a_{n}\leq1 [0,1] \langle a_{n} \mid n\in\mathbb{N} \rangle \langle a_{2n+1} \mid n\in\mathbb{N} \rangle \langle a_{2n} \mid n\in\mathbb{N} \rangle \sqrt{x y}\leq{\frac{x+y}{2}} \tag{1}a_{n-2}^{\frac{1}{4}}\cdot a_{n-1}^{\frac{3}{4}}\leq a_{n}\leq\frac{a_{n-2}}{4}+\frac{3a_{n-1}}{4} n \tag{2}a_{1}\lt \cdots\lt a_{2n-1} \tag{3}a_{2}\gt \cdot\cdot\cdot\gt a_{2n} \tag{4}a_{2n-1}\lt a_{2n} n=1 \tag{5}a_{2n-1}\lt a_{2n+1}\lt a_{2n+2}\lt a_{2n} \tag{6}a_{2n+1}\lt a_{2n} \displaystyle a_{2n+1}=\sqrt{\frac{a_{2n-1}+a_{2n}}{2}\cdot a_{2n}} \displaystyle{\frac{a_{2n-1}+a_{2n}}{2}}\,\lt \,a_{2n} a_{2n+1}^{4}\ge a_{2n-1}\cdot a_{2n}^{3} a_{2n-1}\cdot a_{2n+1}^{3} a_{2n+1}\gt a_{2n-1} a_{2n+2}^{4}\geq a_{2n}\cdot a_{2n+1}^{3}\gt a_{2n+1}^{4} \displaystyle a_{2n+2}\,\leq\,\frac{a_{2n}+3a_{3n+1}}{4\,a n+1}\,\lt \,a_{2n} \displaystyle a_{2n+2}-a_{2n+1}\leq{\frac{a_{2n}-a_{2n+1}}{4}} \displaystyle\frac{a_{2n}-a_{2n-1}}{4} a_{2n+2} a_{2n+1} a_{2n} a_{2n-1} l \langle b_{n}|n\in\mathbb{N}\rangle  \displaystyle b_{n}={\frac{b_{n-2}}{4}}+{\frac{3b_{n-1}}{4}} b_1=0,b_2=1 a_{n}\leq b_{n} \displaystyle b_{n}=\frac{4}{5}\left(1-\left(-{\frac{1}{4}}\right)^{n-1}\right) 0.8 c_{n}=c_{n-2}^{\frac{1}{4}}\cdot c_{n-1}^{\frac{3}{4}} 0 c_{2}=1,c_{3}=a_{3}={\sqrt{\frac{1}{2}}} d_{n}:=\log_{2}c_{n} d_{2}=0,d_{3}=-{\frac{1}{2}} d_n b_n n≥2 \displaystyle d_{n}=\frac{2}{5}\left(-1+\left(-\frac{1}{4}\right)^{n-2}\right) -\frac{2}{5} c_n 2^{-{\frac{2}{5}}} 0.758 b_n b_{2}=1,b_{3}=a_{3}={\sqrt{\frac{1}{2}}} b_n \displaystyle\frac{1+2{\sqrt{2}}}{5} 0.766","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
70,"Show that $|b-a|\geq|\cos a-\cos b|$ for all real numbers $\,a\,$ and $\,b$",Show that  for all real numbers  and,"|b-a|\geq|\cos a-\cos b| \,a\, \,b","$\mathbf{Question:}$ Show that $|b-a|\geq|\cos a-\cos b|$ for all real numbers a and b. $\mathbf{My\ attempt:}$ The Mean Value Theorem states that if $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$ then there exists $c \in (a,b)$ such that $f'(c)=\frac {f(b)-f(a)}{b-a}.$ Using the MVT where $f(c)=\cos c:$ $$ f'(c)=\frac {f(b)-f(a)}{b-a} $$ $$ f'(c)(b-a)=f(b)-f(a) $$ $$ (-\sin c)(b-a)=\cos(b)-\cos(a) $$ $$ (\sin c)(b-a)=\cos(a)-\cos(b) $$ Taking the absolute value of both sides: $$ |\sin c||b-a|= |\cos(a)-\cos(b)| $$ Because $|\sin c |\leq 1,\,$ we can bound $|\sin c|$ by $1$ $$ 1 \cdot|b-a| \geq |\cos(a)-\cos(b)| $$ Thus $|b-a|\geq|\cos a-\cos b|$ for all real numbers $a$ and $b$",Show that for all real numbers a and b. The Mean Value Theorem states that if is continuous on and differentiable on then there exists such that Using the MVT where Taking the absolute value of both sides: Because we can bound by Thus for all real numbers and,"\mathbf{Question:} |b-a|\geq|\cos a-\cos b| \mathbf{My\ attempt:} f [a,b] (a,b) c \in (a,b) f'(c)=\frac {f(b)-f(a)}{b-a}. f(c)=\cos c: 
f'(c)=\frac {f(b)-f(a)}{b-a}
 
f'(c)(b-a)=f(b)-f(a)
 
(-\sin c)(b-a)=\cos(b)-\cos(a)
 
(\sin c)(b-a)=\cos(a)-\cos(b)
 
|\sin c||b-a|= |\cos(a)-\cos(b)|
 |\sin c |\leq 1,\, |\sin c| 1 
1 \cdot|b-a| \geq |\cos(a)-\cos(b)|
 |b-a|\geq|\cos a-\cos b| a b","['real-analysis', 'calculus', 'functions', 'absolute-value']"
71,Sum of the inverse squares of the hypotenuse of Pythagorean triangles,Sum of the inverse squares of the hypotenuse of Pythagorean triangles,,"What is the sum of the series $$ S = \frac{1}{5^2} + \frac{1}{13^2} + \frac{1}{17^2} + \frac{1}{25^2} + \frac{1}{29^2} + \frac{1}{37^2} + \cdots $$ where the sum is taken over all hypotenuse of primitive Pythagorean triangles. By numerical computation, I found the sum to be $0.056840308812554488$ correct to $18$ decimal places. I would like to know if this sum has a closed form. Using the general formula for primitive Pythagorean triangles, $$ S = \sum_{r>s\ge 1, \\ \gcd(r,s)= 1}\frac{1}{(r^2 + s^2)^2} $$ Trivially, for all primitive and non primitive Pythagorean triangles, the sum will be $\zeta(2) = \pi^2/6$ times the corresponding sum for primitive Pythagorean triangles which turn out to be about $0.09349856033594433852$ . Motivation : We have equated the sum of the square of the sides of a right triangle to the square of the hypotenuse, so I was curious to know what the sum of the reciprocal of the square of the hypotenuse would be. Also since $\zeta(2)$ converges, and the density of hypotenuse is smaller than that of natural numbers, this sum must trivially converge. Related question: What is the sum of the reciprocal of the hypotenuse of Pythagorean triangles?","What is the sum of the series where the sum is taken over all hypotenuse of primitive Pythagorean triangles. By numerical computation, I found the sum to be correct to decimal places. I would like to know if this sum has a closed form. Using the general formula for primitive Pythagorean triangles, Trivially, for all primitive and non primitive Pythagorean triangles, the sum will be times the corresponding sum for primitive Pythagorean triangles which turn out to be about . Motivation : We have equated the sum of the square of the sides of a right triangle to the square of the hypotenuse, so I was curious to know what the sum of the reciprocal of the square of the hypotenuse would be. Also since converges, and the density of hypotenuse is smaller than that of natural numbers, this sum must trivially converge. Related question: What is the sum of the reciprocal of the hypotenuse of Pythagorean triangles?"," S = \frac{1}{5^2} + \frac{1}{13^2} + \frac{1}{17^2} +
\frac{1}{25^2} + \frac{1}{29^2} + \frac{1}{37^2} + \cdots  0.056840308812554488 18  S = \sum_{r>s\ge 1, \\
\gcd(r,s)= 1}\frac{1}{(r^2 + s^2)^2}  \zeta(2) = \pi^2/6 0.09349856033594433852 \zeta(2)","['real-analysis', 'sequences-and-series', 'geometry', 'number-theory', 'summation']"
72,"Finding all differentiable $f: [0,+\infty) \rightarrow [0,+\infty)$ such that $f(x) = f'(x^2)$ and $f(0)=0$",Finding all differentiable  such that  and,"f: [0,+\infty) \rightarrow [0,+\infty) f(x) = f'(x^2) f(0)=0","After some investigation it seems fairly obvious to me that the only such function is the zero function, however I haven't been able to prove it. By considering $$\alpha  =\sup\{x\in[0,+\infty) :f(x) = 0\},$$ I was able to show that $\alpha$ can only be $1$ or $0$ but I could not weed out those two possibilities. Any hints/solutions welcome. EDIT 1 Because of the continuity of $f$, we must have $f(\alpha) = 0$. Note that because of the relation given we have $$\int_0^{\sqrt \alpha}2xf'(x^2)\,\mathrm dx = f(\alpha),$$ but because of the relationship given this implies $$\int_0^{\sqrt \alpha}2xf(x)\,\mathrm dx = f(\alpha).$$ If $\alpha$ is strictly between $0$ and $1$, then $\sqrt \alpha > \alpha$, but then splitting the integral we get  $$\int_{\alpha}^{\sqrt \alpha}2xf(x)\,\mathrm dx = f(\alpha) = 0.$$ But by our choice of $α$, this integral should be non-zero since our function is positive. Hence $\alpha$ cannot be between $0$ and $1$. Now suppose it is greater than $1$, then we have $$f(\alpha^2) =\int_0^{\alpha}2xf(x)\,\mathrm dx = 0.$$ Since our function is $0$ on $[0,\alpha]$ (Note that it is increasing), this is again a contradiction because $\alpha^2 > \alpha$. Therefore $\alpha$ is $0$ or $1$. EDIT 2 I forgot to mention the important condition that $f(0)=0$.","After some investigation it seems fairly obvious to me that the only such function is the zero function, however I haven't been able to prove it. By considering $$\alpha  =\sup\{x\in[0,+\infty) :f(x) = 0\},$$ I was able to show that $\alpha$ can only be $1$ or $0$ but I could not weed out those two possibilities. Any hints/solutions welcome. EDIT 1 Because of the continuity of $f$, we must have $f(\alpha) = 0$. Note that because of the relation given we have $$\int_0^{\sqrt \alpha}2xf'(x^2)\,\mathrm dx = f(\alpha),$$ but because of the relationship given this implies $$\int_0^{\sqrt \alpha}2xf(x)\,\mathrm dx = f(\alpha).$$ If $\alpha$ is strictly between $0$ and $1$, then $\sqrt \alpha > \alpha$, but then splitting the integral we get  $$\int_{\alpha}^{\sqrt \alpha}2xf(x)\,\mathrm dx = f(\alpha) = 0.$$ But by our choice of $α$, this integral should be non-zero since our function is positive. Hence $\alpha$ cannot be between $0$ and $1$. Now suppose it is greater than $1$, then we have $$f(\alpha^2) =\int_0^{\alpha}2xf(x)\,\mathrm dx = 0.$$ Since our function is $0$ on $[0,\alpha]$ (Note that it is increasing), this is again a contradiction because $\alpha^2 > \alpha$. Therefore $\alpha$ is $0$ or $1$. EDIT 2 I forgot to mention the important condition that $f(0)=0$.",,"['real-analysis', 'functional-equations']"
73,A function is convex if and only if its gradient is monotone.,A function is convex if and only if its gradient is monotone.,,"Let a convex $ U \subset_{op} \mathbb{R^n} , n \geq 2$, with the usual inner product. A function $F: U \rightarrow \mathbb{R^n} $ is monotone if $ \langle F(x) - F(y), x-y \rangle \geq 0, \forall x,y \in \mathbb{R^n}.$ Let $f:U \rightarrow \mathbb{R}$ differentiable. Show that $f$ is convex $\iff \nabla f:U \rightarrow \mathbb{R^n}$ is monotone. My attempt on the right implication: I already proved that if $f$ is convex and 2-differentiable then $f''(x) \geq 0$. But this exercise only says f is 1-differentiable. Then I tried the following: $f$ is convex $\iff \forall x,y \in U $ the function $\varphi:[0,1] \rightarrow \mathbb{R}$, defined by $ \varphi(t) = f((1-t)x+ty)$ is convex. Then $\varphi'$ is non-decreasing, then $\nabla \varphi(x) \geq 0$... but I'm stucked here. My attempt on the left implication: $ |\nabla \varphi (x) - \nabla \varphi (y)|| x-y| \geq | \langle \nabla \varphi (x) - \nabla \varphi (y), x-y \rangle | \geq 0$ And so $ |\nabla \varphi (x) - \nabla \varphi (y)| \geq 0 $ then $\nabla \varphi $ is non-increasing and then (By an already proved Theorem)  it is convex. Can someone please verify what I did and give me a hint? Thanks.","Let a convex $ U \subset_{op} \mathbb{R^n} , n \geq 2$, with the usual inner product. A function $F: U \rightarrow \mathbb{R^n} $ is monotone if $ \langle F(x) - F(y), x-y \rangle \geq 0, \forall x,y \in \mathbb{R^n}.$ Let $f:U \rightarrow \mathbb{R}$ differentiable. Show that $f$ is convex $\iff \nabla f:U \rightarrow \mathbb{R^n}$ is monotone. My attempt on the right implication: I already proved that if $f$ is convex and 2-differentiable then $f''(x) \geq 0$. But this exercise only says f is 1-differentiable. Then I tried the following: $f$ is convex $\iff \forall x,y \in U $ the function $\varphi:[0,1] \rightarrow \mathbb{R}$, defined by $ \varphi(t) = f((1-t)x+ty)$ is convex. Then $\varphi'$ is non-decreasing, then $\nabla \varphi(x) \geq 0$... but I'm stucked here. My attempt on the left implication: $ |\nabla \varphi (x) - \nabla \varphi (y)|| x-y| \geq | \langle \nabla \varphi (x) - \nabla \varphi (y), x-y \rangle | \geq 0$ And so $ |\nabla \varphi (x) - \nabla \varphi (y)| \geq 0 $ then $\nabla \varphi $ is non-increasing and then (By an already proved Theorem)  it is convex. Can someone please verify what I did and give me a hint? Thanks.",,['real-analysis']
74,are singletons always closed?,are singletons always closed?,,"I am learning about metric spaces and I find it very confusing. Is this a valid proof that a singleton must be closed? If $(X,d)$ is a metric space, to show that $\{a\}$ is closed, let's show that $X \setminus \{a\}$ is open. Choose $y \in X \setminus \{a\}$ and set $\epsilon = d(a,y)$. Then since $a \not \in B(y,\epsilon)$, we have that $B(y,\epsilon) \subset X \setminus \{a\}$ so that $X \setminus \{a\}$ is open.","I am learning about metric spaces and I find it very confusing. Is this a valid proof that a singleton must be closed? If $(X,d)$ is a metric space, to show that $\{a\}$ is closed, let's show that $X \setminus \{a\}$ is open. Choose $y \in X \setminus \{a\}$ and set $\epsilon = d(a,y)$. Then since $a \not \in B(y,\epsilon)$, we have that $B(y,\epsilon) \subset X \setminus \{a\}$ so that $X \setminus \{a\}$ is open.",,"['real-analysis', 'general-topology', 'metric-spaces']"
75,Every function $f: \mathbb{N} \to \mathbb{R}$ is continuous?,Every function  is continuous?,f: \mathbb{N} \to \mathbb{R},"This is a question that came up as a true false question in my textbook, and I was wondering what you thought of my reasoning. I claim that even though a graph of such a function doesn't look continuous, that by the sequential definition of continuity, that the function is continuous. According to the text book, a function $f:D \to R$ is said to be continuous at the point $x_0$ provided that whenever $\{x_n\}$ is a sequence in $D$ that converges to $x_0$, the image sequence $\{f(x_n)\}$ converges to $f(x_0)$. The function is said to be continuous if this holds for all $x \in D$. It would seem that by this definition, a function $f:\mathbb N \to \mathbb R$ would be continuous. Loosely speaking, a sequence of natural numbers has to eventually be constant in order to be convergent (i.e. {1,2,3,4,5,5,5,5,5,...}), since the smallest difference between natural numbers is $1$, and we can pick $0 < \epsilon <1$. Therefore, the image sequence will eventually be constant as well, so no matter what $\epsilon$ we choose, we can always find an index $N$ such that $f(x_n) - f(x_0) = 0 < \epsilon$ for all indices $n \geq N$. Therefore we can always meet the requirement for convergence of $f(x_n) \to f(x_0)$ and thus $f$ is continuous. What do you all think?","This is a question that came up as a true false question in my textbook, and I was wondering what you thought of my reasoning. I claim that even though a graph of such a function doesn't look continuous, that by the sequential definition of continuity, that the function is continuous. According to the text book, a function $f:D \to R$ is said to be continuous at the point $x_0$ provided that whenever $\{x_n\}$ is a sequence in $D$ that converges to $x_0$, the image sequence $\{f(x_n)\}$ converges to $f(x_0)$. The function is said to be continuous if this holds for all $x \in D$. It would seem that by this definition, a function $f:\mathbb N \to \mathbb R$ would be continuous. Loosely speaking, a sequence of natural numbers has to eventually be constant in order to be convergent (i.e. {1,2,3,4,5,5,5,5,5,...}), since the smallest difference between natural numbers is $1$, and we can pick $0 < \epsilon <1$. Therefore, the image sequence will eventually be constant as well, so no matter what $\epsilon$ we choose, we can always find an index $N$ such that $f(x_n) - f(x_0) = 0 < \epsilon$ for all indices $n \geq N$. Therefore we can always meet the requirement for convergence of $f(x_n) \to f(x_0)$ and thus $f$ is continuous. What do you all think?",,"['real-analysis', 'proof-verification']"
76,Help me correct my ideas of continuity,Help me correct my ideas of continuity,,"I've been studying real analysis over the past few months, and I'm having trouble organizing the different notions of continuity and ideas related to continuity in my head geometrically. I will explain my notions in general terms (in other words, without delta-epsilon definitions). Can you tell me how to sharpen my intuitive thinking wherever my ideas are incorrect or too general? Continuous - The preimage of every open set is an open set. Lipshitz Continuous - The absolute values of the slopes of all secant lines for the function are bounded. Uniformly Continuous - The best notion I have is that it's just what a continuous function over a closed and bounded set is. One idea I have is that it means the function is the uniform limit of some series of piecewise linear functions, but does this hold for uniformly continuous functions over domains that are not compact? Absolutely Continuous - This is my shakiest notion geometrically. The delta-epsilon definition gives me a loose notion of being able to break up the condition for uniform continuity over disconnected unions of open intervals. I know absolutely continuous functions have to be of bounded variation, which carries a geometrical notion of a continuous function whose image over any partition of the domain has a finite arc length, but I can't see what makes this notion stronger visually, nor can I grasp how it connects so well to the Fundamental Theorem of Calculus. Another question: how do these different types relate to one another, and what examples can show these relations? I know the Cantor function is a good example of a uniformly continuous function (one of bounded variation) that is not absolutely continuous, but is it Lipschitz? If not, is there a function that is Lipschitz but not absolutely continuous? I appreciate your input, and I apologize if this question is too general and lacking in rigor - I am still learning my way around this site!","I've been studying real analysis over the past few months, and I'm having trouble organizing the different notions of continuity and ideas related to continuity in my head geometrically. I will explain my notions in general terms (in other words, without delta-epsilon definitions). Can you tell me how to sharpen my intuitive thinking wherever my ideas are incorrect or too general? Continuous - The preimage of every open set is an open set. Lipshitz Continuous - The absolute values of the slopes of all secant lines for the function are bounded. Uniformly Continuous - The best notion I have is that it's just what a continuous function over a closed and bounded set is. One idea I have is that it means the function is the uniform limit of some series of piecewise linear functions, but does this hold for uniformly continuous functions over domains that are not compact? Absolutely Continuous - This is my shakiest notion geometrically. The delta-epsilon definition gives me a loose notion of being able to break up the condition for uniform continuity over disconnected unions of open intervals. I know absolutely continuous functions have to be of bounded variation, which carries a geometrical notion of a continuous function whose image over any partition of the domain has a finite arc length, but I can't see what makes this notion stronger visually, nor can I grasp how it connects so well to the Fundamental Theorem of Calculus. Another question: how do these different types relate to one another, and what examples can show these relations? I know the Cantor function is a good example of a uniformly continuous function (one of bounded variation) that is not absolutely continuous, but is it Lipschitz? If not, is there a function that is Lipschitz but not absolutely continuous? I appreciate your input, and I apologize if this question is too general and lacking in rigor - I am still learning my way around this site!",,"['real-analysis', 'continuity']"
77,$L^p$ Dominated Convergence Theorem,Dominated Convergence Theorem,L^p,"I want to prove the $L^p$ Dominated Convergence Theorem which says : Let $\{ f_n \}$ be a sequence of measurable functions that converges pointwise a.e. on E to $f$. For$ 1 \leq p < \infty$, suppose that there is a function $g$ in $L^p(E)$ such that for all $n$, $|f_n|<g$ a.e. on $E$. Prove that $\{ f_n \} \to f$ in $L^p(E)$. This is my attempt : $$ |f_n-f|<|g|+|f| \ \ since \  |f_n - f|\leq |f_n| + |f| \leq |f|+|g| \leq 2 max\{|f|,|g|\} \Rightarrow |f_n - f|^p<2^p(|g|^p + |f|^p) $$ If I show that $2^p(|g|^p + |f|^p)$ is integrable over $E$ then the result follows by Lebesgue's Dominated Convergence theorem. But this is where I am having a question : If $\{ f_n \}$ is a sequence in $L^p(E)$ and  $\{ f_n \} \to f$ pointwise a.e. on $E$, does this necessarily mean that $f \in L^p(E) $ ? If yes,   the result follows after using $2^p(|g|^p + |f|^p)$ as the dominating function in LDC.. ADDED LATER For the bold-faced question, I tried like this $$|f|=|f-f_n+f_n|\leq |f-f_n|+|f_n| \Rightarrow \int_E |f|^p \leq \int_E|f-f_n|^p+\int_E|f_n|^p$$ Is there a  theorem that says if  $\{f_n\}→f$ pointwise a.e. on E, $\{f_n\}→f$ uniformly on $E\setminus E_0$ where $m(E_0)$ ? If thats true,  for any $\epsilon$ and for some $N$ , we can say $|f-f_N|^p < \epsilon^{p}$ and since $f_n \in L^p(E)$, this ensures that the $\int_E |f|^p < \infty$. I'd appreciate if you tell me what is going wrong in my statement. Thank you","I want to prove the $L^p$ Dominated Convergence Theorem which says : Let $\{ f_n \}$ be a sequence of measurable functions that converges pointwise a.e. on E to $f$. For$ 1 \leq p < \infty$, suppose that there is a function $g$ in $L^p(E)$ such that for all $n$, $|f_n|<g$ a.e. on $E$. Prove that $\{ f_n \} \to f$ in $L^p(E)$. This is my attempt : $$ |f_n-f|<|g|+|f| \ \ since \  |f_n - f|\leq |f_n| + |f| \leq |f|+|g| \leq 2 max\{|f|,|g|\} \Rightarrow |f_n - f|^p<2^p(|g|^p + |f|^p) $$ If I show that $2^p(|g|^p + |f|^p)$ is integrable over $E$ then the result follows by Lebesgue's Dominated Convergence theorem. But this is where I am having a question : If $\{ f_n \}$ is a sequence in $L^p(E)$ and  $\{ f_n \} \to f$ pointwise a.e. on $E$, does this necessarily mean that $f \in L^p(E) $ ? If yes,   the result follows after using $2^p(|g|^p + |f|^p)$ as the dominating function in LDC.. ADDED LATER For the bold-faced question, I tried like this $$|f|=|f-f_n+f_n|\leq |f-f_n|+|f_n| \Rightarrow \int_E |f|^p \leq \int_E|f-f_n|^p+\int_E|f_n|^p$$ Is there a  theorem that says if  $\{f_n\}→f$ pointwise a.e. on E, $\{f_n\}→f$ uniformly on $E\setminus E_0$ where $m(E_0)$ ? If thats true,  for any $\epsilon$ and for some $N$ , we can say $|f-f_N|^p < \epsilon^{p}$ and since $f_n \in L^p(E)$, this ensures that the $\int_E |f|^p < \infty$. I'd appreciate if you tell me what is going wrong in my statement. Thank you",,"['real-analysis', 'lp-spaces']"
78,"How do I solve $\min_x \max(c_1^Tx, c_2^Tx, \dots, c_k^Tx)$ for $\lVert x \rVert_2 = 1$.",How do I solve  for .,"\min_x \max(c_1^Tx, c_2^Tx, \dots, c_k^Tx) \lVert x \rVert_2 = 1","Let $f(x) = \max(c_1^Tx, c_2^Tx, \dots, c_k^Tx)$ . where $x, c_1, c_2, \dots, c_k \in \mathbb R^n$ . What fast iterative methods are available for finding the (approximate) min of $f$ with the constraint $\lVert x \rVert_2 = 1$ ? Notes: $f$ is convex and and non negative, that is, $c_i$ positively span $\mathbb R^n$ For my use case $k \gg n$ and (rougly) $3 \le n \le 50$ and $100 \le k \le 10000$ . I tried projected subgradients (projected to the sphere) but it can be slow to converge and improving the initial guess doesn't seem to accelerate the method. From the literature, this seems to be equivalent to Riemannian manifold subgradient methods which use the exponential map. I haven't tried bundle methods yet but I am investigating them now. They seem like they might be too slow. I've also tried approximating $f$ with a smooth maximum LogSumExp and doing gradient descent with projection. I also tried unconstrained gradient descent with a quadratic penalty . The approximation is too inaccurate and the evaluation of exp is too slow for my use case. I'm not interested in SQP because I suspect whatever method used to smooth (e.g. LogSumExp, hyperbolic) will be too costly/inaccurate to evaluate. Edit: I believe this problem is equivalent to a linear programming problem with quadratic equality constraints. I haven't been able to to find much literature on this type of problem. Perhaps I can use a SDP method but I'm not sure.","Let . where . What fast iterative methods are available for finding the (approximate) min of with the constraint ? Notes: is convex and and non negative, that is, positively span For my use case and (rougly) and . I tried projected subgradients (projected to the sphere) but it can be slow to converge and improving the initial guess doesn't seem to accelerate the method. From the literature, this seems to be equivalent to Riemannian manifold subgradient methods which use the exponential map. I haven't tried bundle methods yet but I am investigating them now. They seem like they might be too slow. I've also tried approximating with a smooth maximum LogSumExp and doing gradient descent with projection. I also tried unconstrained gradient descent with a quadratic penalty . The approximation is too inaccurate and the evaluation of exp is too slow for my use case. I'm not interested in SQP because I suspect whatever method used to smooth (e.g. LogSumExp, hyperbolic) will be too costly/inaccurate to evaluate. Edit: I believe this problem is equivalent to a linear programming problem with quadratic equality constraints. I haven't been able to to find much literature on this type of problem. Perhaps I can use a SDP method but I'm not sure.","f(x) = \max(c_1^Tx, c_2^Tx, \dots, c_k^Tx) x, c_1, c_2, \dots, c_k \in \mathbb R^n f \lVert x \rVert_2 = 1 f c_i \mathbb R^n k \gg n 3 \le n \le 50 100 \le k \le 10000 f","['real-analysis', 'optimization', 'convex-optimization', 'nonlinear-optimization', 'non-convex-optimization']"
79,"$f$ be a smooth function on real line , $f(0)=0$ , $f(x)>0, \forall x \ne 0$ and any $f^{(n)}(0)=0$ ; is $\sqrt f$ smooth?","be a smooth function on real line ,  ,  and any  ; is  smooth?","f f(0)=0 f(x)>0, \forall x \ne 0 f^{(n)}(0)=0 \sqrt f","Let $f: \mathbb R \to \mathbb R$ be an infinitely differentiable function such that $f(0)=0$ , $f(x)>0 , \forall x \ne 0$ and $f^{(n)}(0)=0$ ( the $n$-th derivative ) $,  \forall n \in \mathbb N$ , then is the function $g(x):=\sqrt{f(x)}$ infinitely differentiable ? I am having difficulty checking whether any $n$-th derivative exist at $0$ . Please help . Thanks in advance","Let $f: \mathbb R \to \mathbb R$ be an infinitely differentiable function such that $f(0)=0$ , $f(x)>0 , \forall x \ne 0$ and $f^{(n)}(0)=0$ ( the $n$-th derivative ) $,  \forall n \in \mathbb N$ , then is the function $g(x):=\sqrt{f(x)}$ infinitely differentiable ? I am having difficulty checking whether any $n$-th derivative exist at $0$ . Please help . Thanks in advance",,['real-analysis']
80,What's the minimal structure needed to define a notion of derivative?,What's the minimal structure needed to define a notion of derivative?,,"I know that, for example, to define a limit all you need is the notion of ""closeness"" generated by a topology; and to define an integral you need a measure function and a sigma-algebra on which it is defined. This lets you generalize these ideas to spaces well beyond the simple $\mathbb{R}^n$ of early calculus. My question is, what sort of structure do you need to define a derivative? I'm familiar with the idea of a differential ring, but I'm looking for something defined on functions. Specifically, given a base space $X$ and a target space $Y$, I'd like some operator $D$ that takes certain functions $f:X\rightarrow Y$ and yields a derivative function $Df=f':X\to Y$. This should agree with the usual definition of the derivative on Euclidean space, so it should be linear, obey the chain rule, etc. What structure do you need before this $D$ operator can exist? Will we need so much structure that we're forced to make our spaces Euclidean? And if so, could we loosen the requirements for our ""derivative"" to get some more generality? Any help and input would be appreciated!","I know that, for example, to define a limit all you need is the notion of ""closeness"" generated by a topology; and to define an integral you need a measure function and a sigma-algebra on which it is defined. This lets you generalize these ideas to spaces well beyond the simple $\mathbb{R}^n$ of early calculus. My question is, what sort of structure do you need to define a derivative? I'm familiar with the idea of a differential ring, but I'm looking for something defined on functions. Specifically, given a base space $X$ and a target space $Y$, I'd like some operator $D$ that takes certain functions $f:X\rightarrow Y$ and yields a derivative function $Df=f':X\to Y$. This should agree with the usual definition of the derivative on Euclidean space, so it should be linear, obey the chain rule, etc. What structure do you need before this $D$ operator can exist? Will we need so much structure that we're forced to make our spaces Euclidean? And if so, could we loosen the requirements for our ""derivative"" to get some more generality? Any help and input would be appreciated!",,"['real-analysis', 'abstract-algebra', 'general-topology', 'functional-analysis', 'derivatives']"
81,Integral $\int_{0}^{\pi/2} \arctan \left(2\tan^2 x\right) \mathrm{d}x$,Integral,\int_{0}^{\pi/2} \arctan \left(2\tan^2 x\right) \mathrm{d}x,The following integral may seem easy to evaluate ... $$ \int_{0}^{\Large\frac{\pi}{2}} \arctan \left(2 \tan^2 x\right) \mathrm{d}x = \pi \arctan \left( \frac{1}{2} \right). $$ Could you prove it?,The following integral may seem easy to evaluate ... $$ \int_{0}^{\Large\frac{\pi}{2}} \arctan \left(2 \tan^2 x\right) \mathrm{d}x = \pi \arctan \left( \frac{1}{2} \right). $$ Could you prove it?,,"['calculus', 'real-analysis', 'integration', 'trigonometry', 'definite-integrals']"
82,"Integral $\frac{1}{\pi}\int_0^{\pi/3}\log\big( \mu(\theta)+\sqrt{\mu^2(\theta)-1} \big)\ d\theta, \quad \mu(\theta)=\frac{1+2\cos\theta}{2}.$",Integral,"\frac{1}{\pi}\int_0^{\pi/3}\log\big( \mu(\theta)+\sqrt{\mu^2(\theta)-1} \big)\ d\theta, \quad \mu(\theta)=\frac{1+2\cos\theta}{2}.","Hi I am trying to calculate this integral: $$ I=\frac{1}{\pi}\int_0^{\pi/3}\log\left( \frac{1+2\cos\theta}{2}+\sqrt{\bigg( \frac{1+2\cos\theta}{2}   \bigg)^2-1}         \right)\ d\theta. $$ The integral evaluation is related to Mahler measures.  You may also recognize that it is related to the evaluation of log-sine integrals at $\dfrac{\pi}{3}$. This integral $I$ is somewhat related to $$ \int_0^1\log\big|2a+2b\cos (2\pi \theta)\big|\ d\theta=\log \big( |a|+\sqrt{a^2-b^2} \big), $$ for $a,b\in\mathbb{R}$ with $|a|\geq|b|> 0$. This can be seen in Gradstein and Ryzhik's tables of integrals, but I am not sure how to use that to help me to solve $I$. Thanks!","Hi I am trying to calculate this integral: $$ I=\frac{1}{\pi}\int_0^{\pi/3}\log\left( \frac{1+2\cos\theta}{2}+\sqrt{\bigg( \frac{1+2\cos\theta}{2}   \bigg)^2-1}         \right)\ d\theta. $$ The integral evaluation is related to Mahler measures.  You may also recognize that it is related to the evaluation of log-sine integrals at $\dfrac{\pi}{3}$. This integral $I$ is somewhat related to $$ \int_0^1\log\big|2a+2b\cos (2\pi \theta)\big|\ d\theta=\log \big( |a|+\sqrt{a^2-b^2} \big), $$ for $a,b\in\mathbb{R}$ with $|a|\geq|b|> 0$. This can be seen in Gradstein and Ryzhik's tables of integrals, but I am not sure how to use that to help me to solve $I$. Thanks!",,"['real-analysis', 'integration', 'measure-theory', 'definite-integrals', 'contour-integration']"
83,The integral of a characteristic function with respect to a product measure.,The integral of a characteristic function with respect to a product measure.,,"Problem: Let $ (X,\mathcal{A},\mu) $ and $ (Y,\mathcal{B},\nu) $ be measure spaces, where $ X = Y $ is the interval $ [0,1] $, $ \mathcal{A} = \mathcal{B} $ is the collection of Borel subsets of $ [0,1] $, $ \mu $ is the Lebesgue measure and $ \nu $ is the counting measure, both on $ [0,1] $. Show that the diagonal set $ \Delta \stackrel{\text{def}}{=} \{ (x,y) \in X \times Y \mid x = y \} $ is measurable with respect to the product measure $ \mu \otimes \nu $. (In fact, it is an $ F_{\sigma \delta} $-set.) Show also that if $ f $ is the characteristic function of $ \Delta $, then   $$      \int_{X \times Y} f ~ \mathrm{d}{(\mu \otimes \nu)} \neq \int_{X} \left[ \int_{Y} f(x,y) ~ \mathrm{d}{\nu(y)} \right] \mathrm{d}{\mu(x)}. $$ Is this a contradiction of either Fubini’s Theorem or Tonelli’s Theorem? Solution : For each $ n \in \mathbb{N} $ and each $ j \in \{ 1,\ldots,n \} $, let $ I_{j,n} $ denote the interval $ \left[ \dfrac{j - 1}{n},\dfrac{j}{n} \right] $. Also, for each $ n \in \mathbb{N} $, let $ I_{n} $ denote the union $ \displaystyle \bigcup_{j = 1}^{n} I_{j,n} $. Then we have $ \displaystyle \Delta = \bigcap_{n = 1}^{\infty} I_{n} $, so $ \Delta $ is measurable. Next, observe that $$   \int_{X}   \left[ \int_{Y} {\chi_{\Delta}}(x,y) ~ \mathrm{d}{\nu(y)} \right]   \mathrm{d}{\mu(x)} = \int_{X} \nu(\{ y \in Y \mid y = x \}) ~ \mathrm{d}{\mu(x)} = \int_{X} 1 ~ \mathrm{d}{\mu} = 1, $$ whereas $$   \int_{Y}   \left[ \int_{X} {\chi_{\Delta}}(x,y) ~ \mathrm{d}{\mu(x)} \right]   \mathrm{d}{\nu(y)} = \int_{Y} \mu(\{ x \in X \mid x = y \}) ~ \mathrm{d}{\nu(y)} = \int_{Y} 0 ~ \mathrm{d}{\nu} = 0. $$ Now, suppose that $ (A_{n})_{n \in \mathbb{N}} $ and $ (B_{n})_{n \in \mathbb{N}} $ are sequences of Borel subsets of $ [0,1] $ such that $$ \Delta \subseteq \bigcup_{n = 1}^{\infty} (A_{n} \times B_{n}). $$ We can find an $ N \in \mathbb{N} $ such that $ B_{N} $ is infinite, which gives us $ (\mu \otimes \nu)(A_{N} \times B_{N}) = \infty $. Hence, $$ \sum_{n = 1}^{\infty} (\mu \otimes \nu)(A_{n} \times B_{n}) = \infty. $$ The definition of an outer measure therefore yields $$   \int_{X \times Y} \chi_{\Delta} ~ \mathrm{d}{(\mu \otimes \nu)} = (\mu \otimes \nu)(\Delta) = \infty. $$","Problem: Let $ (X,\mathcal{A},\mu) $ and $ (Y,\mathcal{B},\nu) $ be measure spaces, where $ X = Y $ is the interval $ [0,1] $, $ \mathcal{A} = \mathcal{B} $ is the collection of Borel subsets of $ [0,1] $, $ \mu $ is the Lebesgue measure and $ \nu $ is the counting measure, both on $ [0,1] $. Show that the diagonal set $ \Delta \stackrel{\text{def}}{=} \{ (x,y) \in X \times Y \mid x = y \} $ is measurable with respect to the product measure $ \mu \otimes \nu $. (In fact, it is an $ F_{\sigma \delta} $-set.) Show also that if $ f $ is the characteristic function of $ \Delta $, then   $$      \int_{X \times Y} f ~ \mathrm{d}{(\mu \otimes \nu)} \neq \int_{X} \left[ \int_{Y} f(x,y) ~ \mathrm{d}{\nu(y)} \right] \mathrm{d}{\mu(x)}. $$ Is this a contradiction of either Fubini’s Theorem or Tonelli’s Theorem? Solution : For each $ n \in \mathbb{N} $ and each $ j \in \{ 1,\ldots,n \} $, let $ I_{j,n} $ denote the interval $ \left[ \dfrac{j - 1}{n},\dfrac{j}{n} \right] $. Also, for each $ n \in \mathbb{N} $, let $ I_{n} $ denote the union $ \displaystyle \bigcup_{j = 1}^{n} I_{j,n} $. Then we have $ \displaystyle \Delta = \bigcap_{n = 1}^{\infty} I_{n} $, so $ \Delta $ is measurable. Next, observe that $$   \int_{X}   \left[ \int_{Y} {\chi_{\Delta}}(x,y) ~ \mathrm{d}{\nu(y)} \right]   \mathrm{d}{\mu(x)} = \int_{X} \nu(\{ y \in Y \mid y = x \}) ~ \mathrm{d}{\mu(x)} = \int_{X} 1 ~ \mathrm{d}{\mu} = 1, $$ whereas $$   \int_{Y}   \left[ \int_{X} {\chi_{\Delta}}(x,y) ~ \mathrm{d}{\mu(x)} \right]   \mathrm{d}{\nu(y)} = \int_{Y} \mu(\{ x \in X \mid x = y \}) ~ \mathrm{d}{\nu(y)} = \int_{Y} 0 ~ \mathrm{d}{\nu} = 0. $$ Now, suppose that $ (A_{n})_{n \in \mathbb{N}} $ and $ (B_{n})_{n \in \mathbb{N}} $ are sequences of Borel subsets of $ [0,1] $ such that $$ \Delta \subseteq \bigcup_{n = 1}^{\infty} (A_{n} \times B_{n}). $$ We can find an $ N \in \mathbb{N} $ such that $ B_{N} $ is infinite, which gives us $ (\mu \otimes \nu)(A_{N} \times B_{N}) = \infty $. Hence, $$ \sum_{n = 1}^{\infty} (\mu \otimes \nu)(A_{n} \times B_{n}) = \infty. $$ The definition of an outer measure therefore yields $$   \int_{X \times Y} \chi_{\Delta} ~ \mathrm{d}{(\mu \otimes \nu)} = (\mu \otimes \nu)(\Delta) = \infty. $$",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
84,How should I prove the duality?,How should I prove the duality?,,"Rudin asked ( Real Complex Analysis, First edition, Chapter 6, Problem 4 ): Suppose $1\le p\le \infty$, and $q$ is the exponent conjugate to $p$. Suppose $u$ is a $\sigma$-finite measure and $g$ is a measurable function such that $fg\in L^{1}(\mu)$ for every $f\in L^{p}(\mu)$. Prove that then $g\in L^{q}(\mu)$. I am being troubled with the fact that $|g|_{q}$ might be unbounded if we select wierd enough $f$. Holder inequality only gives us $$|fg|_{1}\le |f|_{p}|g|_{q}\leftrightarrow |g|_{q}\ge \frac{|fg|_{1}}{|f|_{p}}$$All  the constructions I know proving $g\in L^{q}$ starts by assuming $f\rightarrow fg$ is a bounded linear operator，so I cannot use circular reasoning at here. It suffice to prove the statement for finite measure spaces and simple functions. So emulating Rudin we can assume $|g|=\alpha g$, where $|\alpha|=1$ and $\alpha$ is measurable. Let $E_{n}=x:|g(x)|\le n$ and let $f=\chi_{E_{n}}\alpha g^{q-1}$. Then we have $$\int_{E_{n}}|g|^{q}d\mu=\int_{X}|fg|d\mu\le K_{n}$$ for some $K_{n}<\infty$. But this constant obviously shift with the $n$ I choose, hence probably does not have a finite upper bound (for example $K_{n}=n$). And I got stuck. In problem 6, Rudin now ask: Suppose $1<p<\infty$, and prove that $L^{q}(\mu)$ is the dual space of $L^{p}(\mu)$ even if $\mu$ is not $\sigma$-finite. I keep thinking about it but do not know what is the best way to prove it.","Rudin asked ( Real Complex Analysis, First edition, Chapter 6, Problem 4 ): Suppose $1\le p\le \infty$, and $q$ is the exponent conjugate to $p$. Suppose $u$ is a $\sigma$-finite measure and $g$ is a measurable function such that $fg\in L^{1}(\mu)$ for every $f\in L^{p}(\mu)$. Prove that then $g\in L^{q}(\mu)$. I am being troubled with the fact that $|g|_{q}$ might be unbounded if we select wierd enough $f$. Holder inequality only gives us $$|fg|_{1}\le |f|_{p}|g|_{q}\leftrightarrow |g|_{q}\ge \frac{|fg|_{1}}{|f|_{p}}$$All  the constructions I know proving $g\in L^{q}$ starts by assuming $f\rightarrow fg$ is a bounded linear operator，so I cannot use circular reasoning at here. It suffice to prove the statement for finite measure spaces and simple functions. So emulating Rudin we can assume $|g|=\alpha g$, where $|\alpha|=1$ and $\alpha$ is measurable. Let $E_{n}=x:|g(x)|\le n$ and let $f=\chi_{E_{n}}\alpha g^{q-1}$. Then we have $$\int_{E_{n}}|g|^{q}d\mu=\int_{X}|fg|d\mu\le K_{n}$$ for some $K_{n}<\infty$. But this constant obviously shift with the $n$ I choose, hence probably does not have a finite upper bound (for example $K_{n}=n$). And I got stuck. In problem 6, Rudin now ask: Suppose $1<p<\infty$, and prove that $L^{q}(\mu)$ is the dual space of $L^{p}(\mu)$ even if $\mu$ is not $\sigma$-finite. I keep thinking about it but do not know what is the best way to prove it.",,"['real-analysis', 'lp-spaces']"
85,A question on convergence of series,A question on convergence of series,,"Suppose $(z_i)$ is a sequence of complex numbers such that $|z_i|\to 0$ strictly decreasing. If $(a_i)$ is a sequence of complex numbers that has the property that for any $n\in\mathbb{N}$ $$  \sum_{i}a_iz_i^{n}=0   $$  does this imply that $a_i=0$ for any $i$? Edit: For the ""finite dimensional"" case, when we have $n$ distinct $(z_i)$, then $(a_i)$ must be $0$. This amounts to solving a homogeneous system of $n$ equations with $n$ unknowns, which only has the trivial solution in the case of distinct $(z_i)$. I am really curious what happens in the infinite dimensional case. My intuition tells me the same must be true, but I don't have a proof for it. Edit 2: Very interesting, looking were this question originated, the fact that all $a_i=0$ when $(a_i)\in l_1$ is ""expected"". I was hoping to get a counterexample otherwise. However, if a non-trivial sequence $a_i$ exists (at least for some sequences $z_i$), I would ""expect"" to be able to choose it in $l_2$. Looking at Davide and Julien answers below, it seems $(a_i)\in l_1$ is an essential assumption in their argument.","Suppose $(z_i)$ is a sequence of complex numbers such that $|z_i|\to 0$ strictly decreasing. If $(a_i)$ is a sequence of complex numbers that has the property that for any $n\in\mathbb{N}$ $$  \sum_{i}a_iz_i^{n}=0   $$  does this imply that $a_i=0$ for any $i$? Edit: For the ""finite dimensional"" case, when we have $n$ distinct $(z_i)$, then $(a_i)$ must be $0$. This amounts to solving a homogeneous system of $n$ equations with $n$ unknowns, which only has the trivial solution in the case of distinct $(z_i)$. I am really curious what happens in the infinite dimensional case. My intuition tells me the same must be true, but I don't have a proof for it. Edit 2: Very interesting, looking were this question originated, the fact that all $a_i=0$ when $(a_i)\in l_1$ is ""expected"". I was hoping to get a counterexample otherwise. However, if a non-trivial sequence $a_i$ exists (at least for some sequences $z_i$), I would ""expect"" to be able to choose it in $l_2$. Looking at Davide and Julien answers below, it seems $(a_i)\in l_1$ is an essential assumption in their argument.",,"['linear-algebra', 'real-analysis', 'complex-analysis', 'analysis', 'convergence-divergence']"
86,A question about converging derivatives,A question about converging derivatives,,"Suppose $f \in C^{\infty}(\mathbb{R})$ and $\forall x \in \mathbb{R} \text{ } \exists \lim_{n \to \infty} f^{(n)}(x) = g(x)$ . Does this mean that $$ \exists a \in \mathbb{R} \forall x \in \mathbb{R} \text{ } g(x) = ae^x? $$ If $f^{(n)}$ converge to $g$ uniformly, then it does, as $$ \forall x_0 \in \mathbb{R} \implies  \begin{split} g’(x_0) &= \lim_{x \to x_0} \frac{g(x) - g(x_0)}{x - x_0} \\ &= \lim_{x \to x_0} \lim_{n \to \infty} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0} \\ &=\lim_{n \to \infty} \lim_{x \to x_0} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0} \\ & = \lim_{n \to \infty} f^{(n + 1)}(x_0) \\ & = \lim_{n \to \infty} f^{(n)}(x_0) \\ & = g(x_0) \end{split} $$ and all solutions of a differential equation $g’(x) = g(x)$ have the form $ae^x$ for some $a \in \mathbb{R}$ . However, this proof does not work in case, when $f^{(n)}$ does not converge uniformly, as in this case $$ \lim_{x \to x_0} \lim_{n \to \infty} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0}\text{ not necessarily equals }\lim_{n \to \infty} \lim_{x \to x_0} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0}.$$ And I do not know how to proceed in this case. Any help will be appreciated.","Suppose and . Does this mean that If converge to uniformly, then it does, as and all solutions of a differential equation have the form for some . However, this proof does not work in case, when does not converge uniformly, as in this case And I do not know how to proceed in this case. Any help will be appreciated.","f \in C^{\infty}(\mathbb{R}) \forall x \in \mathbb{R} \text{ } \exists \lim_{n \to \infty} f^{(n)}(x) = g(x) 
\exists a \in \mathbb{R} \forall x \in \mathbb{R} \text{ } g(x) = ae^x?
 f^{(n)} g 
\forall x_0 \in \mathbb{R} \implies 
\begin{split}
g’(x_0) &= \lim_{x \to x_0} \frac{g(x) - g(x_0)}{x - x_0} \\
&= \lim_{x \to x_0} \lim_{n \to \infty} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0} \\
&=\lim_{n \to \infty} \lim_{x \to x_0} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0} \\
& = \lim_{n \to \infty} f^{(n + 1)}(x_0) \\
& = \lim_{n \to \infty} f^{(n)}(x_0) \\
& = g(x_0)
\end{split}
 g’(x) = g(x) ae^x a \in \mathbb{R} f^{(n)} 
\lim_{x \to x_0} \lim_{n \to \infty} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0}\text{ not necessarily equals }\lim_{n \to \infty} \lim_{x \to x_0} \frac{f^{(n)}(x) - f^{(n)}(x_0)}{x - x_0}.","['real-analysis', 'calculus', 'ordinary-differential-equations', 'limits', 'derivatives']"
87,Why this set is dense in $C_0(\mathbb{R})$?,Why this set is dense in ?,C_0(\mathbb{R}),"Let $C_0=\{f~|~ f:\mathbb{R}\to\mathbb{R}, \textrm{$f$ is continuous},\lim\limits_{\vert x\vert \to\infty}f(x)=0\}$ $A=\{f~|~f(x)=p(x)e^{-x^2}, \textrm{$p(x)$ is polynomials}\}$ Why $A$ is dense in $C_0$? The topology induced by the supremum metric $d(f,g)=\sup\limits_{x\in\mathbb{R}}|f(x)-g(x)|$.","Let $C_0=\{f~|~ f:\mathbb{R}\to\mathbb{R}, \textrm{$f$ is continuous},\lim\limits_{\vert x\vert \to\infty}f(x)=0\}$ $A=\{f~|~f(x)=p(x)e^{-x^2}, \textrm{$p(x)$ is polynomials}\}$ Why $A$ is dense in $C_0$? The topology induced by the supremum metric $d(f,g)=\sup\limits_{x\in\mathbb{R}}|f(x)-g(x)|$.",,"['real-analysis', 'general-topology', 'functional-analysis']"
88,Probabilistic interpretation for representation of unity using the zeta function,Probabilistic interpretation for representation of unity using the zeta function,,"There's a cute identity, I believe due to Borwein, Bradley and Crandall (Section 4): $$1=\sum_{n=2}^\infty (\zeta(n)-1).$$ There are some generalizations in the linked paper as well. Question: Is there an interesting probabilistic interpretation of this that comes up somewhere? In other words, a random variable $X$ such that $P(X=n)=\zeta(n)-1$, for $n\geq 2$. Note that $\zeta(n)>1$ for all $n>1$, and $0<\zeta(n)-1<1$ for all $n\geq 2$.","There's a cute identity, I believe due to Borwein, Bradley and Crandall (Section 4): $$1=\sum_{n=2}^\infty (\zeta(n)-1).$$ There are some generalizations in the linked paper as well. Question: Is there an interesting probabilistic interpretation of this that comes up somewhere? In other words, a random variable $X$ such that $P(X=n)=\zeta(n)-1$, for $n\geq 2$. Note that $\zeta(n)>1$ for all $n>1$, and $0<\zeta(n)-1<1$ for all $n\geq 2$.",,"['real-analysis', 'probability', 'probability-theory', 'riemann-zeta']"
89,Condition for an additive function to be continuous,Condition for an additive function to be continuous,,"The problem below is Problem 7 from this year's Miklos Schweitzer competition (contest ended Nov 4th). Suppose that $f: \Bbb{R} \to \Bbb{R}$ is an additive function (that is $f(x+y) = f(x)+f(y)$ for all $x, y \in \Bbb{R}$) for which $x \mapsto f(x)f(\sqrt{1-x^2})$ is bounded of some nonempty subinterval of $(0,1)$. Prove that $f$ is continuous. I attempted a solution by contradiction, but didn't got very far. I was thinking to exploit the fact that a discontinuous additive function is unbounded on every interval and its graph is dense in $\Bbb{R}^2$.","The problem below is Problem 7 from this year's Miklos Schweitzer competition (contest ended Nov 4th). Suppose that $f: \Bbb{R} \to \Bbb{R}$ is an additive function (that is $f(x+y) = f(x)+f(y)$ for all $x, y \in \Bbb{R}$) for which $x \mapsto f(x)f(\sqrt{1-x^2})$ is bounded of some nonempty subinterval of $(0,1)$. Prove that $f$ is continuous. I attempted a solution by contradiction, but didn't got very far. I was thinking to exploit the fact that a discontinuous additive function is unbounded on every interval and its graph is dense in $\Bbb{R}^2$.",,"['real-analysis', 'continuity', 'functional-equations']"
90,Kinky functions,Kinky functions,,Are there functions whereby the left-handed and right-handed derivatives are always defined but different? What made me think about this is price elasticity of demand which for psychological reasons I think wouldn't be the same from the left and right.,Are there functions whereby the left-handed and right-handed derivatives are always defined but different? What made me think about this is price elasticity of demand which for psychological reasons I think wouldn't be the same from the left and right.,,"['calculus', 'real-analysis']"
91,Fractional Part of $ a^n $,Fractional Part of, a^n ,"Prove that there exists a real number $ a>1 $, such that $ \{a^n\} $ belongs to $[\frac{1}{3},\frac{2}{3}]$ for all positive integers $n$ and $\lfloor a^n\rfloor$ is even iff $n$ is a prime. $a^n=\{a^n\}+\lfloor a^n\rfloor$, where $ \{a^n\} $ is the fractional part of $a^n$ and $\lfloor a^n\rfloor$ is the largest integer not greater than $ a^n $","Prove that there exists a real number $ a>1 $, such that $ \{a^n\} $ belongs to $[\frac{1}{3},\frac{2}{3}]$ for all positive integers $n$ and $\lfloor a^n\rfloor$ is even iff $n$ is a prime. $a^n=\{a^n\}+\lfloor a^n\rfloor$, where $ \{a^n\} $ is the fractional part of $a^n$ and $\lfloor a^n\rfloor$ is the largest integer not greater than $ a^n $",,"['real-analysis', 'contest-math']"
92,Spivak's proof that every polynomial of odd degree has a root,Spivak's proof that every polynomial of odd degree has a root,,"I have the second edition of Spivak. Consider Can someone tell me why he considers $2n|a_{n-1}| \dots$? Later he shows everything is squeezed between -1/2 and 1/2 and he gets the desired result. I am perplexed as to why $2n$? He could have just done it wit $n$ and it would work out. Also he never considered the case for $|x| < 1$, what happens then? EDIT : On another matter, why does he write $|x| > 1, 2n|a_{n-1}| \dots$ instead of just writing $|x| > \max \{1, 2n|a_{n-1}|, \dots \}$? EDIT2 :What is more interesting is that he assumes that $\dfrac{|a_{n-k}|}{|x|} < \dfrac{|a_{n-k}|}{2n|a_{n-k}|}$. Is there a reason why he it will not be just $\dfrac{|a_{n-k}|}{|x|} < |a_{n-k}|$ and why does he even consider $|x| > 1$, why not $|x| > 0$?","I have the second edition of Spivak. Consider Can someone tell me why he considers $2n|a_{n-1}| \dots$? Later he shows everything is squeezed between -1/2 and 1/2 and he gets the desired result. I am perplexed as to why $2n$? He could have just done it wit $n$ and it would work out. Also he never considered the case for $|x| < 1$, what happens then? EDIT : On another matter, why does he write $|x| > 1, 2n|a_{n-1}| \dots$ instead of just writing $|x| > \max \{1, 2n|a_{n-1}|, \dots \}$? EDIT2 :What is more interesting is that he assumes that $\dfrac{|a_{n-k}|}{|x|} < \dfrac{|a_{n-k}|}{2n|a_{n-k}|}$. Is there a reason why he it will not be just $\dfrac{|a_{n-k}|}{|x|} < |a_{n-k}|$ and why does he even consider $|x| > 1$, why not $|x| > 0$?",,"['calculus', 'real-analysis']"
93,"An alternative lower bound for $\prod_{ i,j = 1}^n\frac{1+a_ia_j}{1-a_ia_j} $",An alternative lower bound for,"\prod_{ i,j = 1}^n\frac{1+a_ia_j}{1-a_ia_j} ","In Prove that $\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1$ for $n$ real numbers $a_i\in(-1,1)$ it was shown that for real numbers $a_1, \ldots, a_n \in (-1, 1)$ we always have $$  P = \prod_{ i,j = 1}^n\frac{1+a_ia_j}{1-a_ia_j} \ge 1 \, . $$ The proof (originally from AoPS ) uses the Taylor series of the logarithm to derive an explicit formula for the logarithm of that product as an infinite sum of squares with positive coefficients: $$ \tag{*}  \ln P = 2 \sum_{k=1}^\infty \frac{1}{2k-1} \left( \sum_{i=1}^n a_i^{2k-1}\right)^2 \ge 0 \, . $$ If we omit all terms for $k \ge 2$ on the right-hand side then we get the weaker inequality $$ \tag 1  \ln P \ge 2 ( a_1 + a_2 + \cdots + a_n)^2  $$ or equivalently $$ \tag 2  P \ge e^{2( a_1 + a_2 + \cdots + a_n)^2 } $$ My question: Is there a simpler/more direct way to obtain $(1)$ or $(2)$ without the use of infinite series? We cannot use $$  \ln(1+a_ia_j) - \ln(1-a_ia_j) \ge 2 a_i a_j $$ because that holds only if $a_ia_j \ge 0$ . Another idea is to consider the function $$  f(x) = \prod_{ i,j = 1}^n\frac{1+x^2a_ia_j}{1-x^2a_ia_j} \, . $$ for $0 \le x \le 1$ . From the representation $(*)$ we know that $\ln f(x)$ is increasing in $x$ , but it is not obvious (to me) how to prove that directly, since the terms in $$  \frac{d}{dx} \ln f(x) = \sum_{i, j=1}^n \frac{4a_i a_j x}{1-a_i^2a_j^2 x^4} $$ can be both positive and negative.","In Prove that $\prod_{1\leq i,j\leq n}\frac{1+a_ia_j}{1-a_ia_j}\geq1$ for $n$ real numbers $a_i\in(-1,1)$ it was shown that for real numbers we always have The proof (originally from AoPS ) uses the Taylor series of the logarithm to derive an explicit formula for the logarithm of that product as an infinite sum of squares with positive coefficients: If we omit all terms for on the right-hand side then we get the weaker inequality or equivalently My question: Is there a simpler/more direct way to obtain or without the use of infinite series? We cannot use because that holds only if . Another idea is to consider the function for . From the representation we know that is increasing in , but it is not obvious (to me) how to prove that directly, since the terms in can be both positive and negative.","a_1, \ldots, a_n \in (-1, 1) 
 P = \prod_{ i,j = 1}^n\frac{1+a_ia_j}{1-a_ia_j} \ge 1 \, .
  \tag{*}
 \ln P = 2 \sum_{k=1}^\infty \frac{1}{2k-1} \left( \sum_{i=1}^n a_i^{2k-1}\right)^2 \ge 0 \, .
 k \ge 2  \tag 1
 \ln P \ge 2 ( a_1 + a_2 + \cdots + a_n)^2 
  \tag 2
 P \ge e^{2( a_1 + a_2 + \cdots + a_n)^2 }
 (1) (2) 
 \ln(1+a_ia_j) - \ln(1-a_ia_j) \ge 2 a_i a_j
 a_ia_j \ge 0 
 f(x) = \prod_{ i,j = 1}^n\frac{1+x^2a_ia_j}{1-x^2a_ia_j} \, .
 0 \le x \le 1 (*) \ln f(x) x 
 \frac{d}{dx} \ln f(x) = \sum_{i, j=1}^n \frac{4a_i a_j x}{1-a_i^2a_j^2 x^4}
","['real-analysis', 'inequality', 'alternative-proof']"
94,How did I solve this (triply logarithmic) equation?,How did I solve this (triply logarithmic) equation?,,"In an optimiation problem I came across the following daunting equation: $$ \log \left(\frac{1-t_2}{1-y}\right) \log \left(\frac{(1-x) (t_1-t_2)}{(1-t_1) (x-y)}\right) \log \left(\frac{t_2 x}{t_1 y}\right)\\=\log \left(\frac{t_2}{y}\right) \log \left(\frac{x (t_1-t_2)}{t_1 (x-y)}\right) \log \left(\frac{(1-t_2) (1-x)}{(1-t_1) (1-y)}\right) $$ in which $x,y\in[0,1]$ are given constants and $t_1,t_2\in[0,1]$ are the variables. I ended up plotting it numerically and noting that at the solutions (over $t_1,t_2$ ) the logarithmic factors would be pairwise equal, which in particular implied the hyperbola $$t_1 (1-x) y=t_1 t_2 (y-x)+t_2 x (1-y)$$ along with a few trivial solutions ( $t_2=y$ , $t_1=x$ , $x t_2 = t_1 y$ and $t_2=t_1$ ). Checking that these are indeed solutions is easy, but coming up with them was not. I wonder if there was a more structural approach I could have taken to come up with this result? In fact I have some even more formidable equations which I now hope have simple polynomial solutions, but those I haven't been lucky enough to guess.","In an optimiation problem I came across the following daunting equation: in which are given constants and are the variables. I ended up plotting it numerically and noting that at the solutions (over ) the logarithmic factors would be pairwise equal, which in particular implied the hyperbola along with a few trivial solutions ( , , and ). Checking that these are indeed solutions is easy, but coming up with them was not. I wonder if there was a more structural approach I could have taken to come up with this result? In fact I have some even more formidable equations which I now hope have simple polynomial solutions, but those I haven't been lucky enough to guess.","
\log \left(\frac{1-t_2}{1-y}\right) \log \left(\frac{(1-x) (t_1-t_2)}{(1-t_1) (x-y)}\right) \log \left(\frac{t_2 x}{t_1 y}\right)\\=\log \left(\frac{t_2}{y}\right) \log \left(\frac{x (t_1-t_2)}{t_1 (x-y)}\right) \log \left(\frac{(1-t_2) (1-x)}{(1-t_1) (1-y)}\right)
 x,y\in[0,1] t_1,t_2\in[0,1] t_1,t_2 t_1 (1-x) y=t_1 t_2 (y-x)+t_2 x (1-y) t_2=y t_1=x x t_2 = t_1 y t_2=t_1","['real-analysis', 'calculus', 'polynomials', 'logarithms', 'alternative-proof']"
95,Evaluation of definite Integral,Evaluation of definite Integral,,"Evaluate $$ \int_{\ln(0.5)}^{\ln(2)}\left( 	\frac{\displaystyle\sin x 	\frac{\sqrt{\sin^2(\cos x)+\pi 	e^{(x^4)}}}{1+(xe^{\cos x}\sin x)^2}+ 2\sin(x^2+2)\arctan\left(\frac{x^3}{3}\right) } 	{\displaystyle 1+e^{-\frac{x^2}{2}}+x^7 \sin(-\pi 	x)+\frac{12}{11}|x|^{2\pi+1}} \,d x\right) $$ This is my solution, it's correct? First we observe that $\ln (0.5)=\ln \frac{1}{2}=-\ln 2$ therefore, the integral is, said $ f (x) $ the integrand,  $\int_{-\ln 2}^{\ln 2} f(x)\,\,dx$ that is, an integral over an interval symmetrical about the origin; without taking roads for the search of all the primitives, and by exploiting the symmetry of the interval, we check if the function is odd, in that case one can immediately conclude that the value of 'integral is $ 0 $, then we have: therefore, the integral is, that $ f (x) $ the integrand, \begin{align} f(-x)&= \frac{\displaystyle\sin (-x)\frac{\sqrt{\sin^2(\cos (-x))+\pi e^{((-x)^4)}}}{1+((-x)e^{\cos (-x)}\sin (-x))^2}+2\sin((-x)^2+2)\arctan\left(\frac{(-x)^3}{3}\right)}{\displaystyle 1+e^{-\frac{(-x)^2}{2}}+(-x)^7 \sin(-\pi (-x))+\frac{12}{11}|(-x)|^{2\pi+1}}\\ &= \frac{\displaystyle-\sin x\frac{\sqrt{\sin^2(\cos x)+\pi e^{(x^4)}}}{1+(  x e^{\cos x}\sin x )^2}-2\sin(x^2+2)\arctan\left(\frac{ x ^3}{3}\right)}{\displaystyle 1+e^{-\frac{x^2}{2}}+x^7 \sin(-\pi  x )+\frac{12}{11}|x|^{2\pi+1}}\\ &= -\frac{\displaystyle \sin x\frac{\sqrt{\sin^2(\cos x)+\pi e^{(x^4)}}}{1+(  x e^{\cos x}\sin x )^2}+2\sin(x^2+2)\arctan\left(\frac{ x ^3}{3}\right)}{\displaystyle 1+e^{-\frac{x^2}{2}}+x^7 \sin(-\pi  x )+\frac{12}{11}|x|^{2\pi+1}}\\ &=-f(x) \end{align} the function is therefore odd and therefore the integral is equal to $ 0 $","Evaluate $$ \int_{\ln(0.5)}^{\ln(2)}\left( 	\frac{\displaystyle\sin x 	\frac{\sqrt{\sin^2(\cos x)+\pi 	e^{(x^4)}}}{1+(xe^{\cos x}\sin x)^2}+ 2\sin(x^2+2)\arctan\left(\frac{x^3}{3}\right) } 	{\displaystyle 1+e^{-\frac{x^2}{2}}+x^7 \sin(-\pi 	x)+\frac{12}{11}|x|^{2\pi+1}} \,d x\right) $$ This is my solution, it's correct? First we observe that $\ln (0.5)=\ln \frac{1}{2}=-\ln 2$ therefore, the integral is, said $ f (x) $ the integrand,  $\int_{-\ln 2}^{\ln 2} f(x)\,\,dx$ that is, an integral over an interval symmetrical about the origin; without taking roads for the search of all the primitives, and by exploiting the symmetry of the interval, we check if the function is odd, in that case one can immediately conclude that the value of 'integral is $ 0 $, then we have: therefore, the integral is, that $ f (x) $ the integrand, \begin{align} f(-x)&= \frac{\displaystyle\sin (-x)\frac{\sqrt{\sin^2(\cos (-x))+\pi e^{((-x)^4)}}}{1+((-x)e^{\cos (-x)}\sin (-x))^2}+2\sin((-x)^2+2)\arctan\left(\frac{(-x)^3}{3}\right)}{\displaystyle 1+e^{-\frac{(-x)^2}{2}}+(-x)^7 \sin(-\pi (-x))+\frac{12}{11}|(-x)|^{2\pi+1}}\\ &= \frac{\displaystyle-\sin x\frac{\sqrt{\sin^2(\cos x)+\pi e^{(x^4)}}}{1+(  x e^{\cos x}\sin x )^2}-2\sin(x^2+2)\arctan\left(\frac{ x ^3}{3}\right)}{\displaystyle 1+e^{-\frac{x^2}{2}}+x^7 \sin(-\pi  x )+\frac{12}{11}|x|^{2\pi+1}}\\ &= -\frac{\displaystyle \sin x\frac{\sqrt{\sin^2(\cos x)+\pi e^{(x^4)}}}{1+(  x e^{\cos x}\sin x )^2}+2\sin(x^2+2)\arctan\left(\frac{ x ^3}{3}\right)}{\displaystyle 1+e^{-\frac{x^2}{2}}+x^7 \sin(-\pi  x )+\frac{12}{11}|x|^{2\pi+1}}\\ &=-f(x) \end{align} the function is therefore odd and therefore the integral is equal to $ 0 $",,"['calculus', 'real-analysis', 'integration']"
96,"n'th derivative does not vanish, but $\lim_{n\to \infty} f^{(n)}=0$.","n'th derivative does not vanish, but .",\lim_{n\to \infty} f^{(n)}=0,"Let $f\,$$\in$$\,C^\infty[\mathbb{R},\mathbb{R}]$ .  Apparently the only functions $f$ for which there exists $n\in\mathbb{N}$ such that $f^{(n)}=0$ are polynomials in $\mathbb{R}[x]$. Is it possible to characterize the functions $f\,$$\in$$\,C^\infty[\mathbb{R},\mathbb{R}]$ for which $\lim_{n\to \infty} f^{(n)}=0$, but $f^{(n)}\neq 0$ for all $n\in\mathbb{N}$. For example are they dense in ($C^\infty[\mathbb{R},\mathbb{R}]$,$||.||_{\infty}$)? EDIT: It maybe easier if we resctrict to $C^\infty[(0;1),\mathbb{R}]$. Any discussion is welcome for this one too.","Let $f\,$$\in$$\,C^\infty[\mathbb{R},\mathbb{R}]$ .  Apparently the only functions $f$ for which there exists $n\in\mathbb{N}$ such that $f^{(n)}=0$ are polynomials in $\mathbb{R}[x]$. Is it possible to characterize the functions $f\,$$\in$$\,C^\infty[\mathbb{R},\mathbb{R}]$ for which $\lim_{n\to \infty} f^{(n)}=0$, but $f^{(n)}\neq 0$ for all $n\in\mathbb{N}$. For example are they dense in ($C^\infty[\mathbb{R},\mathbb{R}]$,$||.||_{\infty}$)? EDIT: It maybe easier if we resctrict to $C^\infty[(0;1),\mathbb{R}]$. Any discussion is welcome for this one too.",,['real-analysis']
97,Smoothing a Sobolev function,Smoothing a Sobolev function,,"Let $u \in H^1({\mathbb R}^n)$, $n \geq 2$.   Let $\varphi \in C^\infty_0({\mathbb R}^n)$ with $\varphi \geq 0$.  Let $\eta$ be a smoothing kernel with $\eta \in C^\infty_0({\mathbb R}^n)$, $\eta \geq 0$, $\int \eta \,dx = 1$.  For $t > 0$, define $\eta_t$ by $\eta_t(x)=\frac{1}{t^n}\eta(\frac{x}{t})$.  Define ${\tilde u}$ by  $$ {\tilde u}(x)=    \begin{cases}     u(x); &\text{if } \varphi(x)=0, \\ \\    \int_{{\mathbb R}^n} \eta_{\varphi(x)}(y-x) u(y)\, dy; & \text{if } \varphi(x) > 0.    \end{cases} $$ My question is, is ${\tilde u}$ in $H^1({\mathbb R}^n)$?","Let $u \in H^1({\mathbb R}^n)$, $n \geq 2$.   Let $\varphi \in C^\infty_0({\mathbb R}^n)$ with $\varphi \geq 0$.  Let $\eta$ be a smoothing kernel with $\eta \in C^\infty_0({\mathbb R}^n)$, $\eta \geq 0$, $\int \eta \,dx = 1$.  For $t > 0$, define $\eta_t$ by $\eta_t(x)=\frac{1}{t^n}\eta(\frac{x}{t})$.  Define ${\tilde u}$ by  $$ {\tilde u}(x)=    \begin{cases}     u(x); &\text{if } \varphi(x)=0, \\ \\    \int_{{\mathbb R}^n} \eta_{\varphi(x)}(y-x) u(y)\, dy; & \text{if } \varphi(x) > 0.    \end{cases} $$ My question is, is ${\tilde u}$ in $H^1({\mathbb R}^n)$?",,"['real-analysis', 'sobolev-spaces']"
98,"Is it true that $\,(1+\sin n)^{1/n}\to 1$?",Is it true that ?,"\,(1+\sin n)^{1/n}\to 1","In order to show that $\lim_{n\to\infty}(1+\sin n)^{1/n}=1$, we need to show that $\,1+\sin n\,$ cannot not become too small . It suffices for example to show that $$ 1+\sin n\ge \frac{c}{n^k}, $$ for some $c,k>0$. This could be a consequence of showing that there exist $d,m>0$, such that $$ \Big|\,\pi-\frac{p}{q}\,\Big|\ge \frac{d}{q^{m}} $$ for every rational $p/q$ (cf. irrationality measure ). It would be very interesting if we could produce a more elementary proof.","In order to show that $\lim_{n\to\infty}(1+\sin n)^{1/n}=1$, we need to show that $\,1+\sin n\,$ cannot not become too small . It suffices for example to show that $$ 1+\sin n\ge \frac{c}{n^k}, $$ for some $c,k>0$. This could be a consequence of showing that there exist $d,m>0$, such that $$ \Big|\,\pi-\frac{p}{q}\,\Big|\ge \frac{d}{q^{m}} $$ for every rational $p/q$ (cf. irrationality measure ). It would be very interesting if we could produce a more elementary proof.",,"['calculus', 'real-analysis', 'sequences-and-series']"
99,Integral$\int_1^\infty \log \log \left(x\right)\frac{dx}{1-x+x^2}=\frac{2\pi}{\sqrt 3}\left(\frac{5}{6}\log (2\pi)-\log \Gamma \frac{1}{6}\right)$,Integral,\int_1^\infty \log \log \left(x\right)\frac{dx}{1-x+x^2}=\frac{2\pi}{\sqrt 3}\left(\frac{5}{6}\log (2\pi)-\log \Gamma \frac{1}{6}\right),"UPDATED Hi I am trying to prove the following$$ I:=\int_1^\infty \log \log \left(x\right)\frac{dx}{1-x+x^2}=\frac{2\pi}{\sqrt 3}\left(\frac{5}{6}\log (2\pi)-\log \Gamma \big(\frac{1}{6}\big)\right). $$ I am not sure at all how to get started on this one.  This looks quite intimidating.  Something I realized was  $$ \int_1^\infty \log \log \left(x\right)\frac{dx}{1-x+x^2}=\int_0^1 \log \log \left(\frac{1}{x}\right)\frac{dx}{1-x+x^2}. $$Thanks. Note the Gamma function is given by  $$ \Gamma(n)=(n-1)!,\quad \Gamma(z)=\int_0^\infty t^{z-1}e^{-t}\, dt. $$ EDIT:  THE incorrect integral I first posted was because of a typo.  The result of it is given by (notice the denominator sign mistake I made) $$ I_2:=\int_0^1 \log \log \left(\frac{1}{x}\right)\frac{dx}{1+x+x^2}=\frac{\pi}{\sqrt 3}\log\left(\frac{\sqrt[3]{2\pi}\Gamma(2/3)}{\Gamma(1/3)}\right). $$ as you can see the results are different, enjoy both.  Obviously, I am ONLY interested in solving I thanks.","UPDATED Hi I am trying to prove the following$$ I:=\int_1^\infty \log \log \left(x\right)\frac{dx}{1-x+x^2}=\frac{2\pi}{\sqrt 3}\left(\frac{5}{6}\log (2\pi)-\log \Gamma \big(\frac{1}{6}\big)\right). $$ I am not sure at all how to get started on this one.  This looks quite intimidating.  Something I realized was  $$ \int_1^\infty \log \log \left(x\right)\frac{dx}{1-x+x^2}=\int_0^1 \log \log \left(\frac{1}{x}\right)\frac{dx}{1-x+x^2}. $$Thanks. Note the Gamma function is given by  $$ \Gamma(n)=(n-1)!,\quad \Gamma(z)=\int_0^\infty t^{z-1}e^{-t}\, dt. $$ EDIT:  THE incorrect integral I first posted was because of a typo.  The result of it is given by (notice the denominator sign mistake I made) $$ I_2:=\int_0^1 \log \log \left(\frac{1}{x}\right)\frac{dx}{1+x+x^2}=\frac{\pi}{\sqrt 3}\log\left(\frac{\sqrt[3]{2\pi}\Gamma(2/3)}{\Gamma(1/3)}\right). $$ as you can see the results are different, enjoy both.  Obviously, I am ONLY interested in solving I thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
