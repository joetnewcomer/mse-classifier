,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to change the base cases of a tribonacci sequence solved by matrix exponentation,How to change the base cases of a tribonacci sequence solved by matrix exponentation,,"So the base case of a Fibonacci sequence is the following matrix: \begin{bmatrix}1&1\\1&0\end{bmatrix}  Which I understood as being: \begin{bmatrix}F_2&F_1\\F_1&F_0\end{bmatrix}  And that seems logical. But I don't really understand what each of the numbers in a tribonacci base case mean: \begin{bmatrix}1&1&1\\1&0&0\\0&1&0\end{bmatrix} I don't really understand what each of these numbers represent so if someone could help me, I would appreciate it. Also, I need to compute a tribonacci sequence where the base cases are not t[0]=0, t[1]=0 and t[2]=1 (like in the above matrix), but t[0]=1, t[1]=1 and t[2]=2 So how exactly could I implement that into the matrix that represents the base case? Do I just replace it like the following: \begin{bmatrix}2&2&2\\2&1&1\\1&2&1\end{bmatrix} But what if I would want to replace them with distinct numbers, say for example t[0]=3, t[1]=5 and t[2]=8 ? Where would each of these numbers be placed in the matrix?","So the base case of a Fibonacci sequence is the following matrix: \begin{bmatrix}1&1\\1&0\end{bmatrix}  Which I understood as being: \begin{bmatrix}F_2&F_1\\F_1&F_0\end{bmatrix}  And that seems logical. But I don't really understand what each of the numbers in a tribonacci base case mean: \begin{bmatrix}1&1&1\\1&0&0\\0&1&0\end{bmatrix} I don't really understand what each of these numbers represent so if someone could help me, I would appreciate it. Also, I need to compute a tribonacci sequence where the base cases are not t[0]=0, t[1]=0 and t[2]=1 (like in the above matrix), but t[0]=1, t[1]=1 and t[2]=2 So how exactly could I implement that into the matrix that represents the base case? Do I just replace it like the following: \begin{bmatrix}2&2&2\\2&1&1\\1&2&1\end{bmatrix} But what if I would want to replace them with distinct numbers, say for example t[0]=3, t[1]=5 and t[2]=8 ? Where would each of these numbers be placed in the matrix?",,['matrices']
1,Problem on Matrix Calculus.,Problem on Matrix Calculus.,,"How do I find $$\arg\min_{\alpha \in\mathbb R^n} (K\alpha-y)^T(K\alpha-y)+\lambda \alpha^T K \alpha$$ using Matrix calculus ? Here $K$ is $n\times n$ matrix, $\alpha$ and $y$ are $n\times 1$ vectors, $\lambda$ is positive real number. I am completely new to Matrix Calculus, a solution might help me to relate with the Wikipedia's article on Matrix Calculus.","How do I find $$\arg\min_{\alpha \in\mathbb R^n} (K\alpha-y)^T(K\alpha-y)+\lambda \alpha^T K \alpha$$ using Matrix calculus ? Here $K$ is $n\times n$ matrix, $\alpha$ and $y$ are $n\times 1$ vectors, $\lambda$ is positive real number. I am completely new to Matrix Calculus, a solution might help me to relate with the Wikipedia's article on Matrix Calculus.",,"['calculus', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
2,Show that $f(x)=x^r+r^x$ is increasing,Show that  is increasing,f(x)=x^r+r^x,I know that the function $f(x)=x^r+r^x$ is increasing where $0<r<1$ and $x\geq 2$ by the graph. How to show it? I couldn't prove it .,I know that the function $f(x)=x^r+r^x$ is increasing where $0<r<1$ and $x\geq 2$ by the graph. How to show it? I couldn't prove it .,,"['calculus', 'matrices', 'inequality']"
3,"If $\text{ad}(A)$ is nilpotent and $\text{tr}(A) = 0$, then $A$ is a nilpotent matrix.","If  is nilpotent and , then  is a nilpotent matrix.",\text{ad}(A) \text{tr}(A) = 0 A,"Let $\mathfrak{gl}\left(\mathbb{R}^n\right)$ be the vector space of all linear transformations $T:\mathbb{R}^n \to \mathbb{R}^n$ . If $A$ $\in$ $\mathfrak{gl}\left(\mathbb{R}^n\right)$ , we can define the linear map \begin{align*} \text{ad}(A):\mathfrak{gl}\left(\mathbb{R}^n\right)&\to \mathfrak{gl}\left(\mathbb{R}^n\right)\\ B &\mapsto AB-BA. \\ \end{align*} I would like to know how to prove the following theorem: Theorem : Let $A$ $\in$ $\mathfrak{gl}\left(\mathbb{R}^n\right)$ , such that $\text{ad}(A)$ is nilpotent and $\text{tr}(A) = 0$ , then $A$ is nilpotent. I was trying to prove the theorem above by induction in the dimension of $\mathbb{R}^n$ , but I did not make much progress. Does anyone have a nice idea?","Let be the vector space of all linear transformations . If , we can define the linear map I would like to know how to prove the following theorem: Theorem : Let , such that is nilpotent and , then is nilpotent. I was trying to prove the theorem above by induction in the dimension of , but I did not make much progress. Does anyone have a nice idea?","\mathfrak{gl}\left(\mathbb{R}^n\right) T:\mathbb{R}^n \to \mathbb{R}^n A \in \mathfrak{gl}\left(\mathbb{R}^n\right) \begin{align*}
\text{ad}(A):\mathfrak{gl}\left(\mathbb{R}^n\right)&\to \mathfrak{gl}\left(\mathbb{R}^n\right)\\
B &\mapsto AB-BA. \\
\end{align*} A \in \mathfrak{gl}\left(\mathbb{R}^n\right) \text{ad}(A) \text{tr}(A) = 0 A \mathbb{R}^n","['linear-algebra', 'matrices', 'lie-algebras']"
4,Find the matrix X such that $A\cdot X = X\cdot A^T$,Find the matrix X such that,A\cdot X = X\cdot A^T,"I am working at some linear transformations and I need to know if it is possible given a square real matrix $A$, to find a real square invertible matrix $X$ such that  $$ X^{-1} \cdot A \cdot X = A^T$$ where $A^T$ means the transposed of $A$","I am working at some linear transformations and I need to know if it is possible given a square real matrix $A$, to find a real square invertible matrix $X$ such that  $$ X^{-1} \cdot A \cdot X = A^T$$ where $A^T$ means the transposed of $A$",,"['linear-algebra', 'matrices', 'linear-transformations']"
5,Combining Matrices?,Combining Matrices?,,"Let’s say there are two matrices $A$ and $B$ where $$A=\begin{bmatrix}a&b\\b&a\end{bmatrix}$$ $$B=\begin{bmatrix}c&d\\d&c\end{bmatrix}$$ $A$ and $B$ together make up a third matrix $C$ where $$C=\begin{bmatrix}A&B\\B&A\end{bmatrix}=\begin{bmatrix}\begin{bmatrix}a&b\\b&a\end{bmatrix}&\begin{bmatrix}c&d\\d&c\end{bmatrix}\\\begin{bmatrix}c&d\\d&c\end{bmatrix}&\begin{bmatrix}a&b\\b&a\end{bmatrix}\end{bmatrix}$$ My question is of syntax. Specifically, did I define $C$ using $A$ and $B$ properly? It seems ambiguous to me in the sense of $C$ being a matrix of matrices and not of $A$ and $B$ ‘s elements; when I in fact want $C$ to be a matrix of the elements and not of the matrices. Edit: it looks like I have described a block matrix.","Let’s say there are two matrices $A$ and $B$ where $$A=\begin{bmatrix}a&b\\b&a\end{bmatrix}$$ $$B=\begin{bmatrix}c&d\\d&c\end{bmatrix}$$ $A$ and $B$ together make up a third matrix $C$ where $$C=\begin{bmatrix}A&B\\B&A\end{bmatrix}=\begin{bmatrix}\begin{bmatrix}a&b\\b&a\end{bmatrix}&\begin{bmatrix}c&d\\d&c\end{bmatrix}\\\begin{bmatrix}c&d\\d&c\end{bmatrix}&\begin{bmatrix}a&b\\b&a\end{bmatrix}\end{bmatrix}$$ My question is of syntax. Specifically, did I define $C$ using $A$ and $B$ properly? It seems ambiguous to me in the sense of $C$ being a matrix of matrices and not of $A$ and $B$ ‘s elements; when I in fact want $C$ to be a matrix of the elements and not of the matrices. Edit: it looks like I have described a block matrix.",,['matrices']
6,Diagonal matrices,Diagonal matrices,,Let $D_{n}$ be the set of all n*n complex diagonal matrices. Does there exist a unitary matrix $U$ in $M_{n}(\mathbb{C})$  but not in $D_{n}$ such that $UD_{n}U^{*}= D_{n}$?,Let $D_{n}$ be the set of all n*n complex diagonal matrices. Does there exist a unitary matrix $U$ in $M_{n}(\mathbb{C})$  but not in $D_{n}$ such that $UD_{n}U^{*}= D_{n}$?,,"['linear-algebra', 'matrices', 'linear-transformations']"
7,Find $B$ if $B=A-{{1}\over{2}} A^2+{{1}\over{3}} A^3 -{{1}\over{4}} A^4+...$,Find  if,B B=A-{{1}\over{2}} A^2+{{1}\over{3}} A^3 -{{1}\over{4}} A^4+...,"Let $$ \ A=\begin{bmatrix} 0 & a & a^2 & a^3 \\ 0 & 0 & a & a^2 \\ 0 & 0 & 0 & a \\ 0 & 0 & 0 & 0 \end{bmatrix} $$ and   $B=A-{{1}\over{2}} A^2+{{1}\over{3}} A^3 -{{1}\over{4}} A^4+...$ $i)$ Find the matrix $B$ $ii)$ Prove that $A=B+ {{1}\over{2!}} B^2+ {{1}\over{3!}} B^3+...$ My attempt: $i)$ I calculated $A^2$ by multiplying $A$ by itself, then foundnd $A^3$ by multiplying $A$ by $A^2$, ans so on. Then I noted that $A^n=0$ for $n\geq 4$ $A^2= A.A=\begin{bmatrix} 0 & 0 & a^2 & 2a^3 \\ 0 & 0 & 0 & a^2 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $ $A^3= A^2.A=\begin{bmatrix} 0 & 0 & 0 & a^3 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $ $A^4=A^3.A =\begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $ $A^n=0$ for every $n\geq 4$, so: $B= \begin{bmatrix} 0 & a & {{1}\over{2}} a^2 & {{1}\over {3}}a^3 \\ 0 & 0 & a & {{1}\over{2}} a^2 \\ 0 & 0 & 0 & a \\ 0 & 0 & 0 & 0 \end{bmatrix} $ But is there any way easier than my way ? And what about $(ii)$ ?","Let $$ \ A=\begin{bmatrix} 0 & a & a^2 & a^3 \\ 0 & 0 & a & a^2 \\ 0 & 0 & 0 & a \\ 0 & 0 & 0 & 0 \end{bmatrix} $$ and   $B=A-{{1}\over{2}} A^2+{{1}\over{3}} A^3 -{{1}\over{4}} A^4+...$ $i)$ Find the matrix $B$ $ii)$ Prove that $A=B+ {{1}\over{2!}} B^2+ {{1}\over{3!}} B^3+...$ My attempt: $i)$ I calculated $A^2$ by multiplying $A$ by itself, then foundnd $A^3$ by multiplying $A$ by $A^2$, ans so on. Then I noted that $A^n=0$ for $n\geq 4$ $A^2= A.A=\begin{bmatrix} 0 & 0 & a^2 & 2a^3 \\ 0 & 0 & 0 & a^2 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $ $A^3= A^2.A=\begin{bmatrix} 0 & 0 & 0 & a^3 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $ $A^4=A^3.A =\begin{bmatrix} 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{bmatrix} $ $A^n=0$ for every $n\geq 4$, so: $B= \begin{bmatrix} 0 & a & {{1}\over{2}} a^2 & {{1}\over {3}}a^3 \\ 0 & 0 & a & {{1}\over{2}} a^2 \\ 0 & 0 & 0 & a \\ 0 & 0 & 0 & 0 \end{bmatrix} $ But is there any way easier than my way ? And what about $(ii)$ ?",,"['linear-algebra', 'matrices']"
8,Is there a standard name for doing $B^T A B$ when $B$ is not necessarily square?,Is there a standard name for doing  when  is not necessarily square?,B^T A B B,"Given a (generic, rectangular) matrix $B$ and a (square) matrix $A$, is there a name for doing: $$ B^T A B\ ? $$ My memory wanted to call this ""conjugating $A$ with $B$,"" but  according to mathworld this is used to refer to $$ B^{-1} A B\ . $$ (Sometimes $B^T = B^{-1}$, but obviously not usually in the general case).","Given a (generic, rectangular) matrix $B$ and a (square) matrix $A$, is there a name for doing: $$ B^T A B\ ? $$ My memory wanted to call this ""conjugating $A$ with $B$,"" but  according to mathworld this is used to refer to $$ B^{-1} A B\ . $$ (Sometimes $B^T = B^{-1}$, but obviously not usually in the general case).",,"['linear-algebra', 'matrices', 'terminology', 'definition', 'transpose']"
9,LU decomposition of a diagonally dominant matrix,LU decomposition of a diagonally dominant matrix,,"Let $A \in M_{n\times n}(\mathbb{R})$ a matrix such that $a_{ij}< 0$ for $i \ne j$ and  $A$ is diagonally (row) dominant, that is $a_{ii}>\sum_{j\ne i} |a_{ij}|$. I know that all the leading minors of $A$ are $>0$, and so $A$ has an LU decomposition  $$A= L \cdot D \cdot U$$ where $L$ is lower triangular with $1$ on the diagonal, $D$ is a diagonal matrix with positive diagonal elements, $U$ is upper triangular with $1$ on the diagonal. I would like to get a confirmation whether the off-diagonal elements of $L$ and $U$ are negative.","Let $A \in M_{n\times n}(\mathbb{R})$ a matrix such that $a_{ij}< 0$ for $i \ne j$ and  $A$ is diagonally (row) dominant, that is $a_{ii}>\sum_{j\ne i} |a_{ij}|$. I know that all the leading minors of $A$ are $>0$, and so $A$ has an LU decomposition  $$A= L \cdot D \cdot U$$ where $L$ is lower triangular with $1$ on the diagonal, $D$ is a diagonal matrix with positive diagonal elements, $U$ is upper triangular with $1$ on the diagonal. I would like to get a confirmation whether the off-diagonal elements of $L$ and $U$ are negative.",,"['linear-algebra', 'matrices']"
10,More understanding of Schur's Lemma (representation theory),More understanding of Schur's Lemma (representation theory),,"One of the famous theorem from Schur's Lemma is the following: Let $\vartheta$: $s \rightarrow D(s)$ be an irreducible representation of the finite group $G$ in the representation space $V$, and let $Q$ be a linear operator $V\rightarrow V$ habing the symmetry of $\vartheta$; i.e., $QD(s) = D(s)Q$ for all $s\in G$. Then $Q = \lambda\cdot I$, where $I$ is the identity matrix.  This $\lambda$ is the eigenvalue of $Q$. My question is: Does this theorem say that if any square matrix $Q$ satisfying the above requirements, then this $Q$ can be written as $\lambda\cdot I$, for any eigenvalue $\lambda$? Is $Q = \lambda\cdot I$ based on a special basis? (I think this is like a basis transformation or similar transformation). If it is, how to find such transformation? thanks","One of the famous theorem from Schur's Lemma is the following: Let $\vartheta$: $s \rightarrow D(s)$ be an irreducible representation of the finite group $G$ in the representation space $V$, and let $Q$ be a linear operator $V\rightarrow V$ habing the symmetry of $\vartheta$; i.e., $QD(s) = D(s)Q$ for all $s\in G$. Then $Q = \lambda\cdot I$, where $I$ is the identity matrix.  This $\lambda$ is the eigenvalue of $Q$. My question is: Does this theorem say that if any square matrix $Q$ satisfying the above requirements, then this $Q$ can be written as $\lambda\cdot I$, for any eigenvalue $\lambda$? Is $Q = \lambda\cdot I$ based on a special basis? (I think this is like a basis transformation or similar transformation). If it is, how to find such transformation? thanks",,"['matrices', 'group-theory', 'finite-groups', 'representation-theory']"
11,How is this a linear matrix inequality?,How is this a linear matrix inequality?,,"In example 3.4 of Stephen Boyd & Lieven Vandenberghe's Convex Optimization , it is mentioned that the last condition of $$\text{epi} = \left\{ (x,Y,t) \mid Y \succ 0, x^T Y^{-1} x \leq t \right\}$$ is a linear matrix inequality (LMI) in $(x,Y,t)$ . However the linear matrix inequality is written as (in Eq. 2.11 of same book) $$A(x) = x_1 A_1 + x_2 A_2 + \cdots + x_n A_n \preceq B$$ where $A_i$ and $B$ are symmetric matrices. How to show that $x^TY^{-1}x\leq t$ is a linear inequality in $(x,Y,t)$ ? Any help in this regard will be much appreciated.","In example 3.4 of Stephen Boyd & Lieven Vandenberghe's Convex Optimization , it is mentioned that the last condition of is a linear matrix inequality (LMI) in . However the linear matrix inequality is written as (in Eq. 2.11 of same book) where and are symmetric matrices. How to show that is a linear inequality in ? Any help in this regard will be much appreciated.","\text{epi} = \left\{ (x,Y,t) \mid Y \succ 0, x^T Y^{-1} x \leq t \right\} (x,Y,t) A(x) = x_1 A_1 + x_2 A_2 + \cdots + x_n A_n \preceq B A_i B x^TY^{-1}x\leq t (x,Y,t)","['matrices', 'symmetric-matrices', 'linear-matrix-inequality']"
12,Inverse of Hermitian matrix with all negative eigenvalues?,Inverse of Hermitian matrix with all negative eigenvalues?,,"I am asked to prove that for a Hermitian matrix with all eigenvalues negative, the inverse is given by $$A^{-1} = - \int_0^\infty e^{tA} {\text { d}}t.$$ (I have the corrected the missing minus sign above) I have tried expanding out the integrand in series form, and can show that without taking into account the limits, I have $e^{tA}-I$ as the antiderivative, but I don’t really know how to play around with the limits to get a non-divergent solution. I was thinking of transforming to the diagonal basis, but I can’t seem to move on from there. Is there a trick I’m missing out on? Thanks!","I am asked to prove that for a Hermitian matrix with all eigenvalues negative, the inverse is given by $$A^{-1} = - \int_0^\infty e^{tA} {\text { d}}t.$$ (I have the corrected the missing minus sign above) I have tried expanding out the integrand in series form, and can show that without taking into account the limits, I have $e^{tA}-I$ as the antiderivative, but I don’t really know how to play around with the limits to get a non-divergent solution. I was thinking of transforming to the diagonal basis, but I can’t seem to move on from there. Is there a trick I’m missing out on? Thanks!",,"['linear-algebra', 'sequences-and-series', 'matrices']"
13,"Is $\text{SL}(2,\mathbb{R}) \to \mathbb{R}^2 \setminus \{0\},\; A \mapsto Ae_1$ a closed map?",Is  a closed map?,"\text{SL}(2,\mathbb{R}) \to \mathbb{R}^2 \setminus \{0\},\; A \mapsto Ae_1","Question: Consider the map from the title $$f \colon \text{SL}(2,\mathbb{R}) \to \mathbb{R}^2 \setminus \{0\},\qquad A \mapsto A\begin{pmatrix}1 \\ 0 \end{pmatrix}.$$ Is it a closed map, i.e. does it map closed sets $A \subseteq \text{SL}(2,\mathbb{R})$ to closed sets $f(A) \subseteq \mathbb{R}^2 \setminus \{0\}$? What I've tried : According to Wikipedia , this is equivalent to the condition that for all subsets $A \subseteq \text{SL}(2,\mathbb{R})$, we have $$ \overline{f(A)} \subseteq f(\overline{A}). $$ So, for example, if I take the sequence of matrices $$ a_n =\begin{pmatrix} 2 & n \\ 1/n & 1 \end{pmatrix}, $$ then we have $f(a_n) \to \begin{pmatrix}2 \\ 0 \end{pmatrix}$ but $(a_n)_n$ itself does not converge. So maybe one can construct a set $A$ containing the image of this sequence, but its closure does not contain a matrix with first row $\begin{pmatrix}2 \\ 0 \end{pmatrix}$?","Question: Consider the map from the title $$f \colon \text{SL}(2,\mathbb{R}) \to \mathbb{R}^2 \setminus \{0\},\qquad A \mapsto A\begin{pmatrix}1 \\ 0 \end{pmatrix}.$$ Is it a closed map, i.e. does it map closed sets $A \subseteq \text{SL}(2,\mathbb{R})$ to closed sets $f(A) \subseteq \mathbb{R}^2 \setminus \{0\}$? What I've tried : According to Wikipedia , this is equivalent to the condition that for all subsets $A \subseteq \text{SL}(2,\mathbb{R})$, we have $$ \overline{f(A)} \subseteq f(\overline{A}). $$ So, for example, if I take the sequence of matrices $$ a_n =\begin{pmatrix} 2 & n \\ 1/n & 1 \end{pmatrix}, $$ then we have $f(a_n) \to \begin{pmatrix}2 \\ 0 \end{pmatrix}$ but $(a_n)_n$ itself does not converge. So maybe one can construct a set $A$ containing the image of this sequence, but its closure does not contain a matrix with first row $\begin{pmatrix}2 \\ 0 \end{pmatrix}$?",,"['general-topology', 'matrices', 'closed-map']"
14,Partial derivative of matrix product in neural network,Partial derivative of matrix product in neural network,,"I'm reading a book about neural network. In the section about back-propagation of an Affine-layer of the network, the author provides a formula and omits the details. Say $$\mathbf{X}\cdot\mathbf{W}=\mathbf{Y},$$ which $\mathbf{X}$ is of dimension $(N, 2)$, $\mathbf{W}$ of $(2, 3)$ respectively. The loss function, say $L$, will do some modification to the $\mathbf{Y}$. Then the provided formula is: $$\begin{align}\\ \frac{\partial L}{\partial\mathbf{X}} &= \frac{\partial L}{\partial\mathbf{Y}}\cdot\mathbf{W}^\mathrm{T},\\ \frac{\partial L}{\partial\mathbf{W}} &= \mathbf{X}^\mathrm{T}\cdot\frac{\partial L}{\partial\mathbf{X}}.\\ \end{align}$$ What I really want to know first is that what's the dimension of $\Large\frac{\partial\mathbf{Y}}{\partial\mathbf{X}}$? Since $$\large y_{ij} = \sum^{2}x_{ik}\cdot w_{kj},$$ so for each $y_{ij}$ there are two related $x_{ik}$ to with respect to? I'm confused at this point. What's the definition of the partial derivative of a matrix multiplication/product? And I don't know why there come up with the transposes $\mathbf{W}^\mathrm{T}$ and $\mathbf{X}^\mathrm{T}$? I draw a picture about this process(and I omit the $\mathbf{B}$, which means bias but since it's out of concern here I assume it be zero matrix):","I'm reading a book about neural network. In the section about back-propagation of an Affine-layer of the network, the author provides a formula and omits the details. Say $$\mathbf{X}\cdot\mathbf{W}=\mathbf{Y},$$ which $\mathbf{X}$ is of dimension $(N, 2)$, $\mathbf{W}$ of $(2, 3)$ respectively. The loss function, say $L$, will do some modification to the $\mathbf{Y}$. Then the provided formula is: $$\begin{align}\\ \frac{\partial L}{\partial\mathbf{X}} &= \frac{\partial L}{\partial\mathbf{Y}}\cdot\mathbf{W}^\mathrm{T},\\ \frac{\partial L}{\partial\mathbf{W}} &= \mathbf{X}^\mathrm{T}\cdot\frac{\partial L}{\partial\mathbf{X}}.\\ \end{align}$$ What I really want to know first is that what's the dimension of $\Large\frac{\partial\mathbf{Y}}{\partial\mathbf{X}}$? Since $$\large y_{ij} = \sum^{2}x_{ik}\cdot w_{kj},$$ so for each $y_{ij}$ there are two related $x_{ik}$ to with respect to? I'm confused at this point. What's the definition of the partial derivative of a matrix multiplication/product? And I don't know why there come up with the transposes $\mathbf{W}^\mathrm{T}$ and $\mathbf{X}^\mathrm{T}$? I draw a picture about this process(and I omit the $\mathbf{B}$, which means bias but since it's out of concern here I assume it be zero matrix):",,"['calculus', 'linear-algebra', 'matrices', 'matrix-calculus', 'neural-networks']"
15,Eigenvalues of a symmetric matrix with blocks that are diagonal matrices,Eigenvalues of a symmetric matrix with blocks that are diagonal matrices,,I have the following symmetric matrix of the form (had trouble finding out if there was a name for this kind of special structure): $$A = \begin{bmatrix}    X & Y \\ Y & X \end{bmatrix}$$ where $X$ and $Y$ $\in \mathbb{R}^{n \times n}$ and diagonal. How can I find the eigenvalues?,I have the following symmetric matrix of the form (had trouble finding out if there was a name for this kind of special structure): $$A = \begin{bmatrix}    X & Y \\ Y & X \end{bmatrix}$$ where $X$ and $Y$ $\in \mathbb{R}^{n \times n}$ and diagonal. How can I find the eigenvalues?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
16,If $x^TAx$ = $x^TBx$ for all $x\in \mathbb R^n$. Then what can I say about the matrices? Are they congruent to each other?,If  =  for all . Then what can I say about the matrices? Are they congruent to each other?,x^TAx x^TBx x\in \mathbb R^n,If $x^TAx$ = $x^TBx$ for all $x\in \mathbb R^n$ . Then what can I say about the matrices? Are they congruent to each other? My attempt: $x^TAx$ = $x^TBx$ for all $x\in \mathbb R^n$ . Then I can say $A - B$ is skew-symmetric. I cannot see more than this. Can anyone please help me?,If = for all . Then what can I say about the matrices? Are they congruent to each other? My attempt: = for all . Then I can say is skew-symmetric. I cannot see more than this. Can anyone please help me?,x^TAx x^TBx x\in \mathbb R^n x^TAx x^TBx x\in \mathbb R^n A - B,"['linear-algebra', 'matrices', 'quadratic-forms']"
17,"Determinant of a block matrix, blocks are transformations of the same matrix","Determinant of a block matrix, blocks are transformations of the same matrix",,"Let's say that $\Phi, U_{11},U_{12},U_{21},U_{22}$ are $n\times n$ matrices with coefficients in $\mathbb{C}$. Consider the block matrix: $$ M = \begin{bmatrix}  U_{11}\Phi & U_{12}\Phi \\ U_{21}\Phi & U_{22}\Phi  \end{bmatrix} $$ where $U_{ij}\Phi$ is the usual matrix product. Can anything be deduced about the determinant of this matrix in terms of the determinant of $\Phi$?","Let's say that $\Phi, U_{11},U_{12},U_{21},U_{22}$ are $n\times n$ matrices with coefficients in $\mathbb{C}$. Consider the block matrix: $$ M = \begin{bmatrix}  U_{11}\Phi & U_{12}\Phi \\ U_{21}\Phi & U_{22}\Phi  \end{bmatrix} $$ where $U_{ij}\Phi$ is the usual matrix product. Can anything be deduced about the determinant of this matrix in terms of the determinant of $\Phi$?",,"['linear-algebra', 'matrices']"
18,Find shape function for finite element,Find shape function for finite element,,"Using the unisolvent method to find the shape functions for a regular unit hexagon with vertices: $\{x,y\}=\{(1,0),(\frac{1}{2},\frac{\sqrt{3}}{2})(-\frac{1}{2},\frac{\sqrt{3}}{2}),(-1,0),(-\frac{1}{2},-\frac{\sqrt{3}}{2}),(\frac{1}{2}, -\frac{\sqrt{3}}{2}\}$, using a quadratic polynomial basis for the approximation. Can you give me a hint for this case?  By trying to solve it, the matrix $A$ resultet singular.","Using the unisolvent method to find the shape functions for a regular unit hexagon with vertices: $\{x,y\}=\{(1,0),(\frac{1}{2},\frac{\sqrt{3}}{2})(-\frac{1}{2},\frac{\sqrt{3}}{2}),(-1,0),(-\frac{1}{2},-\frac{\sqrt{3}}{2}),(\frac{1}{2}, -\frac{\sqrt{3}}{2}\}$, using a quadratic polynomial basis for the approximation. Can you give me a hint for this case?  By trying to solve it, the matrix $A$ resultet singular.",,"['linear-algebra', 'matrices', 'functional-analysis', 'finite-element-method']"
19,Gradient of a function with respect to a matrix,Gradient of a function with respect to a matrix,,"How can I compute the gradient of the following function with respect to $X$, $$g(X) = \frac{1}{2}\|y-AX\|^2$$ where $X\in\mathbb{R}^{n\times n}$, $y\in\mathbb{R}^m$, and $A:\mathbb{R}^{n\times n}\to \mathbb{R}^m$ is linear. We can assume that $A$ is of the form, $$A = \begin{pmatrix}\langle X| A_1\rangle\\\vdots\\\langle X|A_m\rangle\end{pmatrix}$$ where $A_1,\ldots,A_m$ are $n\times n$ real matrices and the inner product is the Frobenius inner product. Edit: my attempt at finding the gradient, $$g(X+H) = \frac{1}{2}\langle y-A(X+H), y-A(X+H)\rangle,\\ = \frac{1}{2} \langle y-AX-AH, y-AX-AH\rangle,\\ =\frac{1}{2} \left(\langle y-AX, y-AX\rangle -\langle y-AX,AH\rangle -\langle AH, y-AX\rangle +o(\|H\|)\right),\\ =g(X) - \langle y-AX, AH\rangle,\\ =g(X)-\langle A^*\left(y-AX\right),H\rangle,\\ \implies \nabla g(X) = -A^*\left(y-AX\right)$$ Now I must compute the adjoint operator $A^*$ of $A$. To find $A^*$ we do the following, $$\langle y, AX\rangle = \sum\limits_{i=1}^m y_i\langle X, A_i\rangle=\sum\limits_{i=1}^m \langle X, y_iA_i\rangle = \langle X, \sum\limits_{i=1}^my_iA_i\rangle$$ to see that $A^*y = \sum\limits_{i=1}^m y_iA_i$. Applying this to the expression we found above gives, $$\nabla_Xg(X) = -A^*(y-AX) = -\sum\limits_{i=1}^m\left(y_i-\mbox{tr}(X^TA_i)\right)A_i.$$","How can I compute the gradient of the following function with respect to $X$, $$g(X) = \frac{1}{2}\|y-AX\|^2$$ where $X\in\mathbb{R}^{n\times n}$, $y\in\mathbb{R}^m$, and $A:\mathbb{R}^{n\times n}\to \mathbb{R}^m$ is linear. We can assume that $A$ is of the form, $$A = \begin{pmatrix}\langle X| A_1\rangle\\\vdots\\\langle X|A_m\rangle\end{pmatrix}$$ where $A_1,\ldots,A_m$ are $n\times n$ real matrices and the inner product is the Frobenius inner product. Edit: my attempt at finding the gradient, $$g(X+H) = \frac{1}{2}\langle y-A(X+H), y-A(X+H)\rangle,\\ = \frac{1}{2} \langle y-AX-AH, y-AX-AH\rangle,\\ =\frac{1}{2} \left(\langle y-AX, y-AX\rangle -\langle y-AX,AH\rangle -\langle AH, y-AX\rangle +o(\|H\|)\right),\\ =g(X) - \langle y-AX, AH\rangle,\\ =g(X)-\langle A^*\left(y-AX\right),H\rangle,\\ \implies \nabla g(X) = -A^*\left(y-AX\right)$$ Now I must compute the adjoint operator $A^*$ of $A$. To find $A^*$ we do the following, $$\langle y, AX\rangle = \sum\limits_{i=1}^m y_i\langle X, A_i\rangle=\sum\limits_{i=1}^m \langle X, y_iA_i\rangle = \langle X, \sum\limits_{i=1}^my_iA_i\rangle$$ to see that $A^*y = \sum\limits_{i=1}^m y_iA_i$. Applying this to the expression we found above gives, $$\nabla_Xg(X) = -A^*(y-AX) = -\sum\limits_{i=1}^m\left(y_i-\mbox{tr}(X^TA_i)\right)A_i.$$",,"['matrices', 'normed-spaces', 'matrix-calculus', 'scalar-fields']"
20,"Surjective map from $\mathrm{SL}(2, \mathbb{Z})$ to $\mathrm{SL}(2,\mathbb{Z}/n\mathbb{Z})$",Surjective map from  to,"\mathrm{SL}(2, \mathbb{Z}) \mathrm{SL}(2,\mathbb{Z}/n\mathbb{Z})","Can anyone refer me to a complete proof of that there exist such surjective maps? I looked this up and in these notes the part of showing $b'$ exist is omitted. I still haven't figured out how to show, for example, $(b+x·(a,n),a)=1$ can be solved for $x$ when $(a,b,n)=1$ . Thank you! Edit: I'm not sure how to show that taking all coefficients $\bmod n$ will give you a surjective map. In Conrad's notes from the link above, one step is omitted: given three integers $a,b,n$ such that $\gcd(a,b,n)=1$ , there must exist an integer $b'$ such that $b'\equiv b \bmod n$ and $b'$ is coprime with $a$ . If there is another way of showing that the $\bmod n$ map is surjective, I'd like to hear about that, too.","Can anyone refer me to a complete proof of that there exist such surjective maps? I looked this up and in these notes the part of showing exist is omitted. I still haven't figured out how to show, for example, can be solved for when . Thank you! Edit: I'm not sure how to show that taking all coefficients will give you a surjective map. In Conrad's notes from the link above, one step is omitted: given three integers such that , there must exist an integer such that and is coprime with . If there is another way of showing that the map is surjective, I'd like to hear about that, too.","b' (b+x·(a,n),a)=1 x (a,b,n)=1 \bmod n a,b,n \gcd(a,b,n)=1 b' b'\equiv b \bmod n b' a \bmod n","['matrices', 'elementary-number-theory', 'determinant', 'gcd-and-lcm', 'smith-normal-form']"
21,How to tell whether the ranges of two matrices intersect,How to tell whether the ranges of two matrices intersect,,"Assume that we have two matrices over the complex field i.e. $A\in\mathbb{C}^{m\times n_1}$ and $B\in\mathbb{C}^{m\times n_2}$. Let their range spaces be the sets $$R(A)=\{A\mathbf{x}|\mathbf{x}\in\mathbb{C}^{n_1}\}$$  and  $$R(B)=\{B\mathbf{x}|\mathbf{x}\in\mathbb{C}^{n_2}\}$$ respectively. I want to know if there is a systematic way to tell whether the range spaces have common elements. I am trying to check this in Matlab thus I am trying to find an algorithm. In other words if the columns are s.t. $A=\begin{bmatrix}\mathbf{a}_1&\dots&\mathbf{a}_{n_1}\end{bmatrix}$ and $B=\begin{bmatrix}\mathbf{b}_1&\dots&\mathbf{b}_{n_2}\end{bmatrix}$ how to tell if the following is true $$\sum_{i=1}^{n_1}x_i\mathbf{a}_i=\sum_{i=1}^{n_2}y_i\mathbf{b}_i\Leftrightarrow x_i=y_i=0 \text{ for all }i$$ The matrices are fixed, so I am not looking for a way to construct them.","Assume that we have two matrices over the complex field i.e. $A\in\mathbb{C}^{m\times n_1}$ and $B\in\mathbb{C}^{m\times n_2}$. Let their range spaces be the sets $$R(A)=\{A\mathbf{x}|\mathbf{x}\in\mathbb{C}^{n_1}\}$$  and  $$R(B)=\{B\mathbf{x}|\mathbf{x}\in\mathbb{C}^{n_2}\}$$ respectively. I want to know if there is a systematic way to tell whether the range spaces have common elements. I am trying to check this in Matlab thus I am trying to find an algorithm. In other words if the columns are s.t. $A=\begin{bmatrix}\mathbf{a}_1&\dots&\mathbf{a}_{n_1}\end{bmatrix}$ and $B=\begin{bmatrix}\mathbf{b}_1&\dots&\mathbf{b}_{n_2}\end{bmatrix}$ how to tell if the following is true $$\sum_{i=1}^{n_1}x_i\mathbf{a}_i=\sum_{i=1}^{n_2}y_i\mathbf{b}_i\Leftrightarrow x_i=y_i=0 \text{ for all }i$$ The matrices are fixed, so I am not looking for a way to construct them.",,"['linear-algebra', 'matrices', 'vector-spaces']"
22,On a condition for skew-symmetry,On a condition for skew-symmetry,,Let $A\in\mathbb{R}^{n\times n}$ be a generic lower triangular matrix and let $P\in\mathbb{R}^{n\times n}$ be a symmetric positive definite matrix. True or false. Does $AP + PA^\top=0$ imply $AP=0$?,Let $A\in\mathbb{R}^{n\times n}$ be a generic lower triangular matrix and let $P\in\mathbb{R}^{n\times n}$ be a symmetric positive definite matrix. True or false. Does $AP + PA^\top=0$ imply $AP=0$?,,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-decomposition']"
23,"Find the general formula to compute $\det(A_n)$ and then proof by induction, problem with the proof...","Find the general formula to compute  and then proof by induction, problem with the proof...",\det(A_n),"I have to find the general formula to compute the determinant of a matrix which has all the diagonal elements $0$ and all non-diagonal elements $1$. I have calculated the $\det$ starting from $n=1$, $n=2$, $n=3$, $n=4$ and $n=5$ For $n=1$,  $\det(A) =  0$ For $n=2$,  $\det(A) = -1$ For $n=3$,  $\det(A) =  2$ For $n=4$,  $\det(A) = -3$ For $n=5$,  $\det(A) =  4$ For $n=6$,  $\det(A) = -5$ From this pattern I saw that every time the value is $n-1$ and the sign is alternating, so I created this general formula based on the results I got: $$\det(A) = (-1)^{n+1} \cdot (n-1)$$ Now I need to proof this by induction, when I do the base case $n=0$, I get $0=1$, does that mean that I should only start from $n\ge 1$ since there is no matrix with dimension $0$, or I am doing something wrong which I cannot see?","I have to find the general formula to compute the determinant of a matrix which has all the diagonal elements $0$ and all non-diagonal elements $1$. I have calculated the $\det$ starting from $n=1$, $n=2$, $n=3$, $n=4$ and $n=5$ For $n=1$,  $\det(A) =  0$ For $n=2$,  $\det(A) = -1$ For $n=3$,  $\det(A) =  2$ For $n=4$,  $\det(A) = -3$ For $n=5$,  $\det(A) =  4$ For $n=6$,  $\det(A) = -5$ From this pattern I saw that every time the value is $n-1$ and the sign is alternating, so I created this general formula based on the results I got: $$\det(A) = (-1)^{n+1} \cdot (n-1)$$ Now I need to proof this by induction, when I do the base case $n=0$, I get $0=1$, does that mean that I should only start from $n\ge 1$ since there is no matrix with dimension $0$, or I am doing something wrong which I cannot see?",,"['linear-algebra', 'matrices', 'determinant']"
24,How many degrees of freedom do orthogonal skew-symmetric matrices have?,How many degrees of freedom do orthogonal skew-symmetric matrices have?,,"$n$ by $n$ real orthogonal matrices have $n (n-1)/2$ degrees of freedom . So do the skew-symmetric matrices. But what about matrices that are both skew-symmetric and orthogonal? Is the number of such matrices finite for any given $n$ ? If not, how many degrees of freedom do they have? We know that such matrices exist only if $n$ is even, in which case they are equal to $$\bigoplus_{i=1}^{n/2}\begin{bmatrix} 0 & 1\\ -1 & 0\end{bmatrix}$$ up to an orthogonal change of basis . However, the number of their degrees of freedom is still unclear to me.","by real orthogonal matrices have degrees of freedom . So do the skew-symmetric matrices. But what about matrices that are both skew-symmetric and orthogonal? Is the number of such matrices finite for any given ? If not, how many degrees of freedom do they have? We know that such matrices exist only if is even, in which case they are equal to up to an orthogonal change of basis . However, the number of their degrees of freedom is still unclear to me.","n n n (n-1)/2 n n \bigoplus_{i=1}^{n/2}\begin{bmatrix} 0 & 1\\
-1 & 0\end{bmatrix}","['linear-algebra', 'matrices', 'orthogonal-matrices']"
25,Diagonalization of the Fourier transformation.,Diagonalization of the Fourier transformation.,,"Browsing the following link: https://webusers.imj-prg.fr/~bernard.maurey/agreg/Textes/ag001_a2.pdf , I was able to understand how to diagonalize the Fourier Transform $ \mathcal {F} $, but unfortunately, the article does not tackle the subject of knowing how to find the eigenspaces relative to the eigenvalues ​​of $ \mathcal {F} $. Can you please tell me how to find them ? Thanks in advance for your help.","Browsing the following link: https://webusers.imj-prg.fr/~bernard.maurey/agreg/Textes/ag001_a2.pdf , I was able to understand how to diagonalize the Fourier Transform $ \mathcal {F} $, but unfortunately, the article does not tackle the subject of knowing how to find the eigenspaces relative to the eigenvalues ​​of $ \mathcal {F} $. Can you please tell me how to find them ? Thanks in advance for your help.",,"['linear-algebra', 'matrices', 'fourier-transform', 'diagonalization']"
26,Find $A$ if $B=A-A^T$,Find  if,A B=A-A^T,"Suppose $B=A-A^T$ and I know $B\in\mathbb R^{n\times n}$. What is a simple way to get $A$? And what if I have the constraint that $A_{ij}\ge 0$ $\forall i,j$? Clarifications: $B$ is skew symmetric $a_{ii}$ (the diagonal elements of $A$) are zero","Suppose $B=A-A^T$ and I know $B\in\mathbb R^{n\times n}$. What is a simple way to get $A$? And what if I have the constraint that $A_{ij}\ge 0$ $\forall i,j$? Clarifications: $B$ is skew symmetric $a_{ii}$ (the diagonal elements of $A$) are zero",,"['linear-algebra', 'matrices', 'matrix-equations']"
27,Geometrical Interpretation of Linearly independent vectors,Geometrical Interpretation of Linearly independent vectors,,Suppose we have two linearly independent vectors $X_1$ and $X_2$ as:  $$X_1=(0 \quad 1 \quad 1) \quad and \quad X_2=(1 \quad 1 \quad -1)$$ then how can we interpret it as geometrically or what does it signifies geometrically ? please help....?,Suppose we have two linearly independent vectors $X_1$ and $X_2$ as:  $$X_1=(0 \quad 1 \quad 1) \quad and \quad X_2=(1 \quad 1 \quad -1)$$ then how can we interpret it as geometrically or what does it signifies geometrically ? please help....?,,"['matrices', 'vectors']"
28,Prove there exists $A \in M_n{(\mathbb R)}$ s.t. $A^2 = \begin{pmatrix} M & 0 \\ 0 & M \\ \end{pmatrix}$,Prove there exists  s.t.,A \in M_n{(\mathbb R)} A^2 = \begin{pmatrix} M & 0 \\ 0 & M \\ \end{pmatrix},"Prove that for all $M \in M_n{(\mathbb R)}$, there exists $A \in M_{2n}{(\mathbb R)}$ s.t. $A^2$ = $$\begin{pmatrix}     M & 0 \\     0 & M \\     \end{pmatrix} $$ I have tried by induction on $n$ but I don't see how to pass from $n$ to $n+1$.","Prove that for all $M \in M_n{(\mathbb R)}$, there exists $A \in M_{2n}{(\mathbb R)}$ s.t. $A^2$ = $$\begin{pmatrix}     M & 0 \\     0 & M \\     \end{pmatrix} $$ I have tried by induction on $n$ but I don't see how to pass from $n$ to $n+1$.",,['linear-algebra']
29,Kernel of sum of linear self-adjoint operators,Kernel of sum of linear self-adjoint operators,,"Let $A$, $B$ be linear operators on vector spaces. (We may take $A$, $B$ to be matrices). Let $AB=0$, and $B^*A^*=0$, where $A^*$ denotes the adjoint of $A$ (we may just take it to be conjugate transpose of the matrix.) Then, we can show that $\text{Im}\ BB^*\subseteq\ker A^*A$ and $\text{Im}\ A^*A\subseteq\ker BB^*$. If we define $C=BB^*+A^*A$, can we show that $\ker C=\ker BB^*\cap \ker A^*A$? One direction is clear to me, $\ker BB^*\cap \ker A^*A\subseteq \ker C$, but I am not sure how to show the other direction. Thanks for any help.","Let $A$, $B$ be linear operators on vector spaces. (We may take $A$, $B$ to be matrices). Let $AB=0$, and $B^*A^*=0$, where $A^*$ denotes the adjoint of $A$ (we may just take it to be conjugate transpose of the matrix.) Then, we can show that $\text{Im}\ BB^*\subseteq\ker A^*A$ and $\text{Im}\ A^*A\subseteq\ker BB^*$. If we define $C=BB^*+A^*A$, can we show that $\ker C=\ker BB^*\cap \ker A^*A$? One direction is clear to me, $\ker BB^*\cap \ker A^*A\subseteq \ker C$, but I am not sure how to show the other direction. Thanks for any help.",,"['linear-algebra', 'matrices', 'operator-theory']"
30,Proof of minimum eigenvalue of non-symmetric matrix with real eigenvalues,Proof of minimum eigenvalue of non-symmetric matrix with real eigenvalues,,"I am wondering if this true: $\lambda_{\min}(A) \ge \lambda_{\min}(\frac{A+A^T}{2})$, given that $A$ is non-symmetric but with real eigenvalues. I came across this inequality in one of the math-stackexchange posts but wonder why it is true?  I did MATLAB simulations with for many rand(2,2) matrices and it seems to hold up but is not sufficient to be taken as a fact. Please let me know.","I am wondering if this true: $\lambda_{\min}(A) \ge \lambda_{\min}(\frac{A+A^T}{2})$, given that $A$ is non-symmetric but with real eigenvalues. I came across this inequality in one of the math-stackexchange posts but wonder why it is true?  I did MATLAB simulations with for many rand(2,2) matrices and it seems to hold up but is not sufficient to be taken as a fact. Please let me know.",,"['linear-algebra', 'matrices']"
31,Bound on the norm of a matrix power,Bound on the norm of a matrix power,,"Suppose we have the square matrix $A$ and we know that its spectral radius $\rho(A)$ is less than $1$, therefore matrix $A$ is stable. How can we prove that $\exists \gamma \in(0,1)$ and $\exists M >0$ such that  $$\|A^k\|\leq M\gamma^k, \:\:\:\: \forall k\geq0$$ What I tried so far is $\|A^k\|=\|A\dots A\|\leq\|A\|\dots \|A\| =\|A\|^k$ so taking $\gamma=\|A\|$ I should be close to the above inequality, but I am not sure it is correct.","Suppose we have the square matrix $A$ and we know that its spectral radius $\rho(A)$ is less than $1$, therefore matrix $A$ is stable. How can we prove that $\exists \gamma \in(0,1)$ and $\exists M >0$ such that  $$\|A^k\|\leq M\gamma^k, \:\:\:\: \forall k\geq0$$ What I tried so far is $\|A^k\|=\|A\dots A\|\leq\|A\|\dots \|A\| =\|A\|^k$ so taking $\gamma=\|A\|$ I should be close to the above inequality, but I am not sure it is correct.",,"['matrices', 'normed-spaces', 'stability-theory']"
32,Solve $AX^2 + BX + C = 0$ [duplicate],Solve  [duplicate],AX^2 + BX + C = 0,"This question already has answers here : Is there a unique solution for this quadratic matrix equation? (2 answers) Closed 6 years ago . How does one solve equations of the form $AX^2+BX+C=0$ where $A,B,C$ are square matrices and $X$ is a matrix to be solved for? More generally how does one solve equations of the form $AX^2B+CXD+E=0$? Even more generally how does one solve higher order equations of this form such as $AX^3B+CX^2D+EXF+G=0$ In all cases, I could simply express the entries of the matrix $X$ as variables $x_1,x_2,\dots$ and multiply out the matrices and derive equations corresponding to each entry of the matrices however this would result in (for the first and most simple case at least) a system of 4 quadratic equations in 4 variables which I have no experience with. Is this the best approach or can the fact that we are dealing with matrices help to simplify the problem? Thanks","This question already has answers here : Is there a unique solution for this quadratic matrix equation? (2 answers) Closed 6 years ago . How does one solve equations of the form $AX^2+BX+C=0$ where $A,B,C$ are square matrices and $X$ is a matrix to be solved for? More generally how does one solve equations of the form $AX^2B+CXD+E=0$? Even more generally how does one solve higher order equations of this form such as $AX^3B+CX^2D+EXF+G=0$ In all cases, I could simply express the entries of the matrix $X$ as variables $x_1,x_2,\dots$ and multiply out the matrices and derive equations corresponding to each entry of the matrices however this would result in (for the first and most simple case at least) a system of 4 quadratic equations in 4 variables which I have no experience with. Is this the best approach or can the fact that we are dealing with matrices help to simplify the problem? Thanks",,"['linear-algebra', 'matrices', 'quadratic-forms']"
33,Fundamental Subspaces Theorem's proof,Fundamental Subspaces Theorem's proof,,"According to the theorem of fundamental subspaces if A is $m \times n$ matrix, then $N(A) = R(A^T)^\bot$ and $N(A^T)$ = $R(A)^\bot$. The proof from Linear Algebra with Applications book by Steven J.Leon is: On the other hand, we have already seen that $N(A) \bot R(A)^\bot$, and this implies that $N(A)\subset R(A^T)^\bot$. On the other hand, if $\mathbf{x}$ is any vector in $R(A^T)^\bot$, then $\textbf{x}$ is orthogonal to each of the column vectors of $A^T$ and consequently, $A\textbf{x} = 0$. Thus, x must be an element of $N(A)$ and hence $N(A) = R(A^T)^\bot$. Can someone explain this proof with more details? Especially the part where it says On the other hand, if x is any vector in $R(A^T)^\bot$, then x is orthogonal to each of the column vectors of $A^T$ and consequently, $A\textbf{x} = 0.$","According to the theorem of fundamental subspaces if A is $m \times n$ matrix, then $N(A) = R(A^T)^\bot$ and $N(A^T)$ = $R(A)^\bot$. The proof from Linear Algebra with Applications book by Steven J.Leon is: On the other hand, we have already seen that $N(A) \bot R(A)^\bot$, and this implies that $N(A)\subset R(A^T)^\bot$. On the other hand, if $\mathbf{x}$ is any vector in $R(A^T)^\bot$, then $\textbf{x}$ is orthogonal to each of the column vectors of $A^T$ and consequently, $A\textbf{x} = 0$. Thus, x must be an element of $N(A)$ and hence $N(A) = R(A^T)^\bot$. Can someone explain this proof with more details? Especially the part where it says On the other hand, if x is any vector in $R(A^T)^\bot$, then x is orthogonal to each of the column vectors of $A^T$ and consequently, $A\textbf{x} = 0.$",,"['linear-algebra', 'matrices', 'orthogonality']"
34,A problem about the dimension of the intersection of two subspaces,A problem about the dimension of the intersection of two subspaces,,"I'm trying to solve the problem below. $U$ and $W$ are subspaces of polynomials over $\mathbb{R}$. $U = Span(t^3 + 4t^2 - t + 3, t^3 + 5t^2 + 5, 3t^3 + 10t^2 -5t + 5)$   $W = Span(t^3 + 4t^2 + 6, t^3 + 2t^2 - t + 5, 2t^3 + 2t^2 -3t + 9)$ What is $dim(U \cap W)$? I have solved it using the fact that $dim(U) + dim(W) - dim(U \cap W) = dim(U \cup W)$, but was wondering how to solve it without using this fact. In order to find $dim(U \cap W)$, I first try and find $U \cap W$. Clearly if $v \in U \cap W$, then $$\alpha_1(t^3 + 4t^2 - t + 3) +\alpha_2(t^3 + 5t^2 + 5) +\alpha_3(3t^3 + 10t^2 -5t + 5) = \beta_1(t^3 + 4t^2 + 6) + \beta_2(t^3 + 2t^2 - t + 5) + \beta_3(2t^3 + 2t^2 -3t + 9)$$ for some $\alpha_1, \alpha_2, \alpha_3, \beta_1, \beta_2, \beta_3 \in \mathbb{R}$. Using this fact, you can reduce a system of linear equations to work out that: $\alpha_1 + 5\alpha_3 - \beta_2 - 3\beta_3 = 0$ $\alpha_2 -2 \alpha_3 + 2\beta_2 + 6\beta_3 = 0$ $\beta_1 + 2\beta_2 + 5\beta_3 = 0$ But I don't know where to go from here. Any help would be greatly appreciated.","I'm trying to solve the problem below. $U$ and $W$ are subspaces of polynomials over $\mathbb{R}$. $U = Span(t^3 + 4t^2 - t + 3, t^3 + 5t^2 + 5, 3t^3 + 10t^2 -5t + 5)$   $W = Span(t^3 + 4t^2 + 6, t^3 + 2t^2 - t + 5, 2t^3 + 2t^2 -3t + 9)$ What is $dim(U \cap W)$? I have solved it using the fact that $dim(U) + dim(W) - dim(U \cap W) = dim(U \cup W)$, but was wondering how to solve it without using this fact. In order to find $dim(U \cap W)$, I first try and find $U \cap W$. Clearly if $v \in U \cap W$, then $$\alpha_1(t^3 + 4t^2 - t + 3) +\alpha_2(t^3 + 5t^2 + 5) +\alpha_3(3t^3 + 10t^2 -5t + 5) = \beta_1(t^3 + 4t^2 + 6) + \beta_2(t^3 + 2t^2 - t + 5) + \beta_3(2t^3 + 2t^2 -3t + 9)$$ for some $\alpha_1, \alpha_2, \alpha_3, \beta_1, \beta_2, \beta_3 \in \mathbb{R}$. Using this fact, you can reduce a system of linear equations to work out that: $\alpha_1 + 5\alpha_3 - \beta_2 - 3\beta_3 = 0$ $\alpha_2 -2 \alpha_3 + 2\beta_2 + 6\beta_3 = 0$ $\beta_1 + 2\beta_2 + 5\beta_3 = 0$ But I don't know where to go from here. Any help would be greatly appreciated.",,"['linear-algebra', 'matrices', 'vector-spaces', 'systems-of-equations']"
35,What does the notation $[\boldsymbol{A} | \boldsymbol{B} ]$ mean (line through a matrix)?,What does the notation  mean (line through a matrix)?,[\boldsymbol{A} | \boldsymbol{B} ],"I have just stumbled upon the following lecture slide, in which $G$ refers to a $k \times n$ matrix with linearly independent rows: What does the notation with the line going vertically down the center of the matrix denote?","I have just stumbled upon the following lecture slide, in which $G$ refers to a $k \times n$ matrix with linearly independent rows: What does the notation with the line going vertically down the center of the matrix denote?",,"['linear-algebra', 'matrices', 'notation', 'coding-theory']"
36,Taylor Expansion of Eigenvector Perturbation,Taylor Expansion of Eigenvector Perturbation,,"Consider symmetric matrices $A,B \in \mathbb{R}^{n \times n}$ and let the difference matrix  $D = B - A.$ If $\bf{\hat{a}}_i$ and $\bf{\hat{b}}_i$ are the $i^{\text{th}}$ eigenvectors corresponding to $A$ and $B$ respectively, and $\lambda_i$ is the $i^{\text{th}}$ eigenvalue of $A$, then the first-order Taylor expansion gives the following approximation for the difference between the primary eigenvectors: \begin{align} {\bf{\hat{b}}_1 - \bf{\hat{a}}_1} &= \sum_{i = 2}^n  \frac{{{\bf{\hat{a}}_i}^T} D\; {\bf{\hat{a}}_1}}{\lambda_1 - \lambda_i}{\bf{\hat{a}}_i} + O(D^2)\\ &\approx \sum_{i = 2}^n  \frac{{{\bf{\hat{a}}_i}^T} D\; {\bf{\hat{a}}_1}}{\lambda_1 - \lambda_i}{\bf{\hat{a}}_i} \end{align} My question is, where does this Taylor expansion come from?","Consider symmetric matrices $A,B \in \mathbb{R}^{n \times n}$ and let the difference matrix  $D = B - A.$ If $\bf{\hat{a}}_i$ and $\bf{\hat{b}}_i$ are the $i^{\text{th}}$ eigenvectors corresponding to $A$ and $B$ respectively, and $\lambda_i$ is the $i^{\text{th}}$ eigenvalue of $A$, then the first-order Taylor expansion gives the following approximation for the difference between the primary eigenvectors: \begin{align} {\bf{\hat{b}}_1 - \bf{\hat{a}}_1} &= \sum_{i = 2}^n  \frac{{{\bf{\hat{a}}_i}^T} D\; {\bf{\hat{a}}_1}}{\lambda_1 - \lambda_i}{\bf{\hat{a}}_i} + O(D^2)\\ &\approx \sum_{i = 2}^n  \frac{{{\bf{\hat{a}}_i}^T} D\; {\bf{\hat{a}}_1}}{\lambda_1 - \lambda_i}{\bf{\hat{a}}_i} \end{align} My question is, where does this Taylor expansion come from?",,"['calculus', 'matrices', 'eigenvalues-eigenvectors', 'taylor-expansion', 'matrix-calculus']"
37,"Computing the matrix of the linear map $X\rightsquigarrow AXA^t$, where $A=\left[\begin{smallmatrix}2&1\\0&1\end{smallmatrix}\right]$","Computing the matrix of the linear map , where",X\rightsquigarrow AXA^t A=\left[\begin{smallmatrix}2&1\\0&1\end{smallmatrix}\right],"Let $V$ be the vector space of real $2\times 2$ symmetric matrices   $X=\begin{bmatrix}x&y\\y&z\end{bmatrix}$ and let   $A=\begin{bmatrix}2&1\\0&1\end{bmatrix}$. Determine the matrix of the   linear operator on $V$ defined by $X\rightsquigarrow AXA^t$ with   respect to a suitable basis. So far, I got $AXA^t=\begin{bmatrix}4x+4y+z&2y+z\\2y+z&z\end{bmatrix}$ I need to find a matrix $C$ such that $CX=AXA^t$, but not find $C$. Am I doing anything wrong. Please someone tell me what is wrong here.","Let $V$ be the vector space of real $2\times 2$ symmetric matrices   $X=\begin{bmatrix}x&y\\y&z\end{bmatrix}$ and let   $A=\begin{bmatrix}2&1\\0&1\end{bmatrix}$. Determine the matrix of the   linear operator on $V$ defined by $X\rightsquigarrow AXA^t$ with   respect to a suitable basis. So far, I got $AXA^t=\begin{bmatrix}4x+4y+z&2y+z\\2y+z&z\end{bmatrix}$ I need to find a matrix $C$ such that $CX=AXA^t$, but not find $C$. Am I doing anything wrong. Please someone tell me what is wrong here.",,"['linear-algebra', 'matrices', 'linear-transformations']"
38,Hankel matrix of Catalan numbers,Hankel matrix of Catalan numbers,,"Recall that the $n$-th Catalan number $C_n=\frac{1}{n+1}{2n\choose n}$ counts the number of paths connecting $(0, 0)$ to $(n, n)$ that travel along the grid of integer lattice points of $R^2$ where each path moves up or right in one-unit steps and no path extends above the line $y = x$. In linear algebra, a Hankel matrix of Catalan numbers is defined as following: $$H_n^t=(C_{i+j+t})_{0\leq i,j\leq n-1}= \begin{bmatrix}     c_{t} & c_{t+1} & c_{t+2} & \dots  & c_{t+n-1} \\     c_{t+1} & c_{t+2} & c_{t+3} & \dots  & c_{t+n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     c_{t+n-1} & c_{t+n} & c_{t+n+1} & \dots  & c_{t+2n-2} \end{bmatrix} $$ How can I calculate the Hankel determinant of Catalan numbers for $t=1$? Is it possible obtain the Hankel determinant of Catalan numbers for $t>1$?","Recall that the $n$-th Catalan number $C_n=\frac{1}{n+1}{2n\choose n}$ counts the number of paths connecting $(0, 0)$ to $(n, n)$ that travel along the grid of integer lattice points of $R^2$ where each path moves up or right in one-unit steps and no path extends above the line $y = x$. In linear algebra, a Hankel matrix of Catalan numbers is defined as following: $$H_n^t=(C_{i+j+t})_{0\leq i,j\leq n-1}= \begin{bmatrix}     c_{t} & c_{t+1} & c_{t+2} & \dots  & c_{t+n-1} \\     c_{t+1} & c_{t+2} & c_{t+3} & \dots  & c_{t+n} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     c_{t+n-1} & c_{t+n} & c_{t+n+1} & \dots  & c_{t+2n-2} \end{bmatrix} $$ How can I calculate the Hankel determinant of Catalan numbers for $t=1$? Is it possible obtain the Hankel determinant of Catalan numbers for $t>1$?",,"['combinatorics', 'matrices', 'determinant', 'catalan-numbers', 'hankel-matrices']"
39,Operator norm in terms of matrix norm,Operator norm in terms of matrix norm,,"Given a matrix $A$ yielding a linear operator $L$, how can I express the operator norm in terms of $A$'s norm? As far as I've understood, the operator norm in terms of a matrix is given by $$ \|L\|_{op} = \sup_{x\neq0}\frac{\|Ax\|_{\infty}}{\|x\|_\infty} $$ This should mean that $$ \sup_{x\neq0} \frac{\|Ax\|_{\infty}}{\|x\|_\infty} = \sup_{x\neq0} \left\|\frac{Ax_{\infty}}{\|x\|_\infty}\right\|_\infty = \sup_{x\neq0} \left\|A\left(\frac{x_{\infty}}{\|x\|_\infty}\right)\right\|_\infty $$ However, is there a way to simplify this further, removing any reference to $x$? As I see it, I should be able to reduce $\frac{x_{\infty}}{\|x\|_\infty} \to 1$ somehow. But I don't see quite how?","Given a matrix $A$ yielding a linear operator $L$, how can I express the operator norm in terms of $A$'s norm? As far as I've understood, the operator norm in terms of a matrix is given by $$ \|L\|_{op} = \sup_{x\neq0}\frac{\|Ax\|_{\infty}}{\|x\|_\infty} $$ This should mean that $$ \sup_{x\neq0} \frac{\|Ax\|_{\infty}}{\|x\|_\infty} = \sup_{x\neq0} \left\|\frac{Ax_{\infty}}{\|x\|_\infty}\right\|_\infty = \sup_{x\neq0} \left\|A\left(\frac{x_{\infty}}{\|x\|_\infty}\right)\right\|_\infty $$ However, is there a way to simplify this further, removing any reference to $x$? As I see it, I should be able to reduce $\frac{x_{\infty}}{\|x\|_\infty} \to 1$ somehow. But I don't see quite how?",,"['matrices', 'normed-spaces', 'supremum-and-infimum']"
40,Is there a universal or general “matrix-builder” notation?,Is there a universal or general “matrix-builder” notation?,,"If one wants to compactly construct a series, one writes $$\sum_{i=1}^w a_i=a_1+a_2+a_3+\cdots+a_w$$ If one wants to compactly construct a set, one writes $$\bigcup_{j=1}^x B_j = B_1\cup B_2\cup B_3\cup\cdots\cup B_x$$ A benefit to these notations is that they require no extra explanation except for their arguments, which can actually be defined after the “big symbol” itself. If one wants to compactly construct a matrix , what “big operator” might one use to notate or stand in for $$\boldsymbol{C}(y,z) = \begin{pmatrix} c(1,1) & c(1,2) & c(1,3) & \cdots & c(1,z) \\ c(2,1) & c(2,2) & c(2,3) & \cdots & c(2,z) \\ c(3,1) & c(3,2) & c(3,3) & \cdots & c(3,z) \\  \vdots & \vdots & \vdots & \ddots & \vdots \\ c(y,1) & c(y,2) & c(y,3) & \cdots & c(y,z) \\ \end{pmatrix}$$ I suppose one could do something weird like defining $$\mathop{\LARGE\mathrm{M}}_{(k,\ell)=(1,1)}^{(y,z)}c(k,\ell)$$ but I would like something more universal, perhaps involving $\prod$, $\sum$, and $\boldsymbol{I}_{y\times x}$ (the $y\times z$ identity matrix).","If one wants to compactly construct a series, one writes $$\sum_{i=1}^w a_i=a_1+a_2+a_3+\cdots+a_w$$ If one wants to compactly construct a set, one writes $$\bigcup_{j=1}^x B_j = B_1\cup B_2\cup B_3\cup\cdots\cup B_x$$ A benefit to these notations is that they require no extra explanation except for their arguments, which can actually be defined after the “big symbol” itself. If one wants to compactly construct a matrix , what “big operator” might one use to notate or stand in for $$\boldsymbol{C}(y,z) = \begin{pmatrix} c(1,1) & c(1,2) & c(1,3) & \cdots & c(1,z) \\ c(2,1) & c(2,2) & c(2,3) & \cdots & c(2,z) \\ c(3,1) & c(3,2) & c(3,3) & \cdots & c(3,z) \\  \vdots & \vdots & \vdots & \ddots & \vdots \\ c(y,1) & c(y,2) & c(y,3) & \cdots & c(y,z) \\ \end{pmatrix}$$ I suppose one could do something weird like defining $$\mathop{\LARGE\mathrm{M}}_{(k,\ell)=(1,1)}^{(y,z)}c(k,\ell)$$ but I would like something more universal, perhaps involving $\prod$, $\sum$, and $\boldsymbol{I}_{y\times x}$ (the $y\times z$ identity matrix).",,"['matrices', 'notation']"
41,How does a column of Zeros affect a Matrix?,How does a column of Zeros affect a Matrix?,,"This was a sample test question and was not the answer to the question. I'm just curious about what the column of 0 does. For reference if needed, the question was ""Which of the following is the coefficient matrix for a homogeneous system Ax = 0 with only the trivial solution"" Here's the Matrix: \begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\end{bmatrix} I personally would assume that there would be only 1 solution as despite there being no pivot in the 4th column, the values in the 4th column are all 0. (I'm definitely doing this part wrong) Writing this in parametric vector form, I would get x 1 = 0 x 2 = 0 x 3 = 0 x 4 = free and now I'm kinda lost about what to do.. Taking a guess, would the result be: \begin{bmatrix}0\\0\\0\\1\end{bmatrix} * x 4 ? To be honest, I'm not exactly sure how to examine this matrix.. Simply put, how does the 0 column affect this matrix? Is the initial matrix equivalent to a 3x3 identity matrix? How many solutions are there? Infinitely many or just 1?","This was a sample test question and was not the answer to the question. I'm just curious about what the column of 0 does. For reference if needed, the question was ""Which of the following is the coefficient matrix for a homogeneous system Ax = 0 with only the trivial solution"" Here's the Matrix: \begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\end{bmatrix} I personally would assume that there would be only 1 solution as despite there being no pivot in the 4th column, the values in the 4th column are all 0. (I'm definitely doing this part wrong) Writing this in parametric vector form, I would get x 1 = 0 x 2 = 0 x 3 = 0 x 4 = free and now I'm kinda lost about what to do.. Taking a guess, would the result be: \begin{bmatrix}0\\0\\0\\1\end{bmatrix} * x 4 ? To be honest, I'm not exactly sure how to examine this matrix.. Simply put, how does the 0 column affect this matrix? Is the initial matrix equivalent to a 3x3 identity matrix? How many solutions are there? Infinitely many or just 1?",,"['matrices', 'matrix-calculus']"
42,Two invertible matrices,Two invertible matrices,,"Let $A,B$ be two $n\times n$ invertible matrices with complex entries. Also, let $\alpha, \beta \in \mathbb{C}$ with $|\alpha| \neq |\beta|$ such that $\alpha AB+\beta BA=I_n$. Prove that $\det(AB-BA)=0$. I tried to manipulate the given equation in order two get $(AB-BA)$ as a factor somewhere, but didn't manage to get anything useful. I also thought of using $A^{-1}$ and $B^{-1}$ somewhere, but I only got messier relations.","Let $A,B$ be two $n\times n$ invertible matrices with complex entries. Also, let $\alpha, \beta \in \mathbb{C}$ with $|\alpha| \neq |\beta|$ such that $\alpha AB+\beta BA=I_n$. Prove that $\det(AB-BA)=0$. I tried to manipulate the given equation in order two get $(AB-BA)$ as a factor somewhere, but didn't manage to get anything useful. I also thought of using $A^{-1}$ and $B^{-1}$ somewhere, but I only got messier relations.",,"['linear-algebra', 'matrices', 'determinant', 'matrix-equations']"
43,Variables that will make this matrix positive semi-definite,Variables that will make this matrix positive semi-definite,,"I have a matrix $$M=\begin{bmatrix} 1+t+m &n&t+n&m+c \\ n &1+t-m&m-c & t-n \\ t+n & m-c&1-t-m & -n \\ m+c & t-n & -n & 1-t+m \end{bmatrix}$$ where I know that $0 \leq c \leq 1$ and $ t=a-(m+n)b$ for some fixed $0 \leq a,b\leq 1$. Here $m$ and $n$ are free parameters with $t$ depending on $m,n$. I'm trying to find a pair of real numbers $(m,n)$ which ensure that $M$ is positive semi-definite. For a fixed $a,b,c \in \mathbb R$, what is the best way to determine some $m,n$ which make $M$ positive semi-definite? The eigenvalues of this matrix are $$\lambda=1 + (m+n) \pm \sqrt{c^2+m^2+n^2+2cm-2cn-2mn+2t^2}$$ and $$\lambda=1 - (m+n) \pm \sqrt{c^2+m^2+n^2-2cm+2cn-2mn+2t^2}.$$ If not, are there conditions on $a,b$ so that $m,n$ exist?","I have a matrix $$M=\begin{bmatrix} 1+t+m &n&t+n&m+c \\ n &1+t-m&m-c & t-n \\ t+n & m-c&1-t-m & -n \\ m+c & t-n & -n & 1-t+m \end{bmatrix}$$ where I know that $0 \leq c \leq 1$ and $ t=a-(m+n)b$ for some fixed $0 \leq a,b\leq 1$. Here $m$ and $n$ are free parameters with $t$ depending on $m,n$. I'm trying to find a pair of real numbers $(m,n)$ which ensure that $M$ is positive semi-definite. For a fixed $a,b,c \in \mathbb R$, what is the best way to determine some $m,n$ which make $M$ positive semi-definite? The eigenvalues of this matrix are $$\lambda=1 + (m+n) \pm \sqrt{c^2+m^2+n^2+2cm-2cn-2mn+2t^2}$$ and $$\lambda=1 - (m+n) \pm \sqrt{c^2+m^2+n^2-2cm+2cn-2mn+2t^2}.$$ If not, are there conditions on $a,b$ so that $m,n$ exist?",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
44,Realation between determinant and minimum eigenvalue,Realation between determinant and minimum eigenvalue,,"For all $k\in\mathbb{N}$, let $P(k)\in\mathbb{R}^{n\times n}$ be positive semidefinite. Assume $P(k)$ is bounded. Can we prove or disprove by counterexample that \begin{equation}  \det P(k)\to 0 \text{ as } k\to \infty \iff \lambda_\min\big(P(k)\big)\to 0 \text{ as } k\to \infty. \end{equation} I know that if $P(k)$ was unbounded then something like $P(k)=\left[\begin{array}{cc}1/k&0\\0&k\end{array}\right]$ would be a counterexample. Thanks","For all $k\in\mathbb{N}$, let $P(k)\in\mathbb{R}^{n\times n}$ be positive semidefinite. Assume $P(k)$ is bounded. Can we prove or disprove by counterexample that \begin{equation}  \det P(k)\to 0 \text{ as } k\to \infty \iff \lambda_\min\big(P(k)\big)\to 0 \text{ as } k\to \infty. \end{equation} I know that if $P(k)$ was unbounded then something like $P(k)=\left[\begin{array}{cc}1/k&0\\0&k\end{array}\right]$ would be a counterexample. Thanks",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
45,Understanding SVD Notation,Understanding SVD Notation,,"Given any $m \times n$ matrix $M$, one can write $$ M = U\Sigma V^T $$  is the Singular Value Decomposition, where $U$ and $V$ are orthonormal and $\Sigma$ is a diagonal matrix. Now, the same $M$ can be written as: $$M = \sum_{i=1}^r u_i  c_i  v_i^T\,,$$ where $u_i$ is the $i$th column of $U$, $v_i$ is the $i$th column of $V$ and $c_i$ is the $i$th diagonal entry of $\Sigma$. I don't understand why the second representation is the same as first one? In general, how could matrix multiplication be expressed as product of columns, I have learnt that matrices are multiplied row by column. This is the only way even Profs do, so how can a matrix multiplication be expressed as only involving column vectors? Sorry if the question is too basic, but I am having lot of trouble understanding how people are using column vectors in matrix multiplications.","Given any $m \times n$ matrix $M$, one can write $$ M = U\Sigma V^T $$  is the Singular Value Decomposition, where $U$ and $V$ are orthonormal and $\Sigma$ is a diagonal matrix. Now, the same $M$ can be written as: $$M = \sum_{i=1}^r u_i  c_i  v_i^T\,,$$ where $u_i$ is the $i$th column of $U$, $v_i$ is the $i$th column of $V$ and $c_i$ is the $i$th diagonal entry of $\Sigma$. I don't understand why the second representation is the same as first one? In general, how could matrix multiplication be expressed as product of columns, I have learnt that matrices are multiplied row by column. This is the only way even Profs do, so how can a matrix multiplication be expressed as only involving column vectors? Sorry if the question is too basic, but I am having lot of trouble understanding how people are using column vectors in matrix multiplications.",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-decomposition', 'svd']"
46,A $3\times 3$ matrix to the power of $n$.,A  matrix to the power of .,3\times 3 n,I can't find a formula for : $$         A =\begin{pmatrix}         1 & 1 & 0 \\         0 & 2 & 1 \\         0 & 0 & 3 \\         \end{pmatrix}^n $$ I tried to separate $A = I + J$ with $J$ nilpotent but I didn't success. Can you give me a hint? Thanks.,I can't find a formula for : $$         A =\begin{pmatrix}         1 & 1 & 0 \\         0 & 2 & 1 \\         0 & 0 & 3 \\         \end{pmatrix}^n $$ I tried to separate $A = I + J$ with $J$ nilpotent but I didn't success. Can you give me a hint? Thanks.,,[]
47,Compute the determinant,Compute the determinant,,"The following problem is taken from here exercise $2:$ Question: Evaluate the determinant:    \begin{vmatrix} 0 & x & x & \dots & x \\ y & 0 & x & \dots & x \\ y & y & 0 & \dots & x \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y & y & y & \dots & 0 \end{vmatrix} My attempt: I tried to use first row substract second row to obtain  \begin{pmatrix} y & -x & 0 \dots & 0 \end{pmatrix} and also first row subtracts remaining rows.  However, I have no idea how to proceed.","The following problem is taken from here exercise $2:$ Question: Evaluate the determinant:    \begin{vmatrix} 0 & x & x & \dots & x \\ y & 0 & x & \dots & x \\ y & y & 0 & \dots & x \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ y & y & y & \dots & 0 \end{vmatrix} My attempt: I tried to use first row substract second row to obtain  \begin{pmatrix} y & -x & 0 \dots & 0 \end{pmatrix} and also first row subtracts remaining rows.  However, I have no idea how to proceed.",,"['linear-algebra', 'matrices', 'contest-math', 'determinant']"
48,Statement of the Perron–Frobenius theorem,Statement of the Perron–Frobenius theorem,,"Let $A=(a_{{ij}})$ be an $n\times n$ positive matrix: $ a_{{ij}}>0$ for $1\leq i,j\leq n$. On Wikipedia, the statement of Perron–Frobenius theorem indicates, among others, the following claims: There exists an eigenvector $v = (v_1,\dots,v_n)$ of $A$ with eigenvalue $r$ such that all components of $v$ are positive: $A v = r v$, $v_i > 0$ for $1 \leq i \leq n$. (Respectively, there exists a positive left eigenvector $w : w^T A = r w^T$, $w_i > 0$.) $\lim _{{k\rightarrow \infty }}A^{k}/r^{k}=vw^{T}$, where the left and right eigenvectors for A are normalized so that $w^Tv = 1$. I'm trying to prove the second claim, but so far I have not gone well. Any suggestion, please? Thanks in advance.","Let $A=(a_{{ij}})$ be an $n\times n$ positive matrix: $ a_{{ij}}>0$ for $1\leq i,j\leq n$. On Wikipedia, the statement of Perron–Frobenius theorem indicates, among others, the following claims: There exists an eigenvector $v = (v_1,\dots,v_n)$ of $A$ with eigenvalue $r$ such that all components of $v$ are positive: $A v = r v$, $v_i > 0$ for $1 \leq i \leq n$. (Respectively, there exists a positive left eigenvector $w : w^T A = r w^T$, $w_i > 0$.) $\lim _{{k\rightarrow \infty }}A^{k}/r^{k}=vw^{T}$, where the left and right eigenvectors for A are normalized so that $w^Tv = 1$. I'm trying to prove the second claim, but so far I have not gone well. Any suggestion, please? Thanks in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-matrices']"
49,Determinant of sum of squares of two matrices.,Determinant of sum of squares of two matrices.,,"Problem : $\rm P$ and $\rm Q$ such that $\rm P^3 = Q^3$, $\rm Q^2P = P^2Q$ and $\rm P \ne Q$. Find $\rm \det(P^2 + Q^2)$ if (i) If both matrices are $3\times 3$ (ii) If both matrices are $n\times n$,  for $n \in \Bbb N$ $$\rm (P^2 + Q^2)(P+ Q) = P^3 + P^2Q + Q^2P + Q^3 = 2(P^3 + P^2Q) = 2P^2(P + Q)\tag{1}$$ Taking determinant on both sides, $$\rm \det(P^2 + Q^2)\det(P+ Q) = 2^n\det(P^2)\det(P+Q) \\ \implies \det(P^2 + Q^2) = 2^n\det(P^2) = 2^n (\det(P))^2$$ Also, $$\rm Q^2P = P^2Q \\ \implies  (\det (Q))^2 \det P = (\det P)^2 \det(Q) \\\implies  \det P\det Q (\det Q - \det P) = 0 $$ Therefore $\rm \det P = 0$ or $\rm \det Q = 0$ or $\rm \det P = \det Q$. If $\rm\det P =0$ then I can say $\rm \det(P^2 + Q^2) = 0$ but I am not sure about other case, i.e when $\rm \det Q = 0$ or $\rm \det P = \det Q$. Can I still say $\rm \det(P^2 + Q^2) = 0$ ?","Problem : $\rm P$ and $\rm Q$ such that $\rm P^3 = Q^3$, $\rm Q^2P = P^2Q$ and $\rm P \ne Q$. Find $\rm \det(P^2 + Q^2)$ if (i) If both matrices are $3\times 3$ (ii) If both matrices are $n\times n$,  for $n \in \Bbb N$ $$\rm (P^2 + Q^2)(P+ Q) = P^3 + P^2Q + Q^2P + Q^3 = 2(P^3 + P^2Q) = 2P^2(P + Q)\tag{1}$$ Taking determinant on both sides, $$\rm \det(P^2 + Q^2)\det(P+ Q) = 2^n\det(P^2)\det(P+Q) \\ \implies \det(P^2 + Q^2) = 2^n\det(P^2) = 2^n (\det(P))^2$$ Also, $$\rm Q^2P = P^2Q \\ \implies  (\det (Q))^2 \det P = (\det P)^2 \det(Q) \\\implies  \det P\det Q (\det Q - \det P) = 0 $$ Therefore $\rm \det P = 0$ or $\rm \det Q = 0$ or $\rm \det P = \det Q$. If $\rm\det P =0$ then I can say $\rm \det(P^2 + Q^2) = 0$ but I am not sure about other case, i.e when $\rm \det Q = 0$ or $\rm \det P = \det Q$. Can I still say $\rm \det(P^2 + Q^2) = 0$ ?",,"['linear-algebra', 'matrices', 'contest-math', 'determinant']"
50,Have $A$ and $A^{-1}$ the same set of eigenvectors?,Have  and  the same set of eigenvectors?,A A^{-1},Do $A$ and $A^{-1}$ have the same set of eigenvectors? My try : Yes they have the same set of eigenvectors. $AX = cX$ we can multiply $A^{-1}$ both sides we get $$A^{-1} X = c^{-1} X$$ Am I right? Any help will be highly appreciated.,Do $A$ and $A^{-1}$ have the same set of eigenvectors? My try : Yes they have the same set of eigenvectors. $AX = cX$ we can multiply $A^{-1}$ both sides we get $$A^{-1} X = c^{-1} X$$ Am I right? Any help will be highly appreciated.,,"['linear-algebra', 'matrices']"
51,Can't figure out why this Eigenvector and Engenvalue doesn't work,Can't figure out why this Eigenvector and Engenvalue doesn't work,,"For this matrix $$\begin{bmatrix} 3 &  0 & 0 \\ 1  & 2 &  0 \\   -4  & 5 &  -1 \end{bmatrix}$$ The Eigenvalues = $-1, 2, 3$ The Eigenvectors I got were $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 0 \\ \frac35 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 4 \\ 4 \\ 1 \end{bmatrix}$. However, only the last pair, $3$ and $\begin{bmatrix} 4 \\ 4 \\ 1 \end{bmatrix}$ were correct. I've looked over and over my algebra, but I don't see where I went wrong. EDIT: For anyone in the future, the Eigenvalues/vectors I got were right, but I used the wrong matrix. I should have multiplied the Eigenvector with the matrix $$A=\begin{bmatrix} 3 &  0 & 0 \\ 1  & 2 &  0 \\   -4  & 5 &  -1 \end{bmatrix}$$but instead I used the wrong matrix, (A-lambda*I) and substituted the corresponding Eigenvalue into lambda.","For this matrix $$\begin{bmatrix} 3 &  0 & 0 \\ 1  & 2 &  0 \\   -4  & 5 &  -1 \end{bmatrix}$$ The Eigenvalues = $-1, 2, 3$ The Eigenvectors I got were $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 0 \\ \frac35 \\ 1 \end{bmatrix}$, $\begin{bmatrix} 4 \\ 4 \\ 1 \end{bmatrix}$. However, only the last pair, $3$ and $\begin{bmatrix} 4 \\ 4 \\ 1 \end{bmatrix}$ were correct. I've looked over and over my algebra, but I don't see where I went wrong. EDIT: For anyone in the future, the Eigenvalues/vectors I got were right, but I used the wrong matrix. I should have multiplied the Eigenvector with the matrix $$A=\begin{bmatrix} 3 &  0 & 0 \\ 1  & 2 &  0 \\   -4  & 5 &  -1 \end{bmatrix}$$but instead I used the wrong matrix, (A-lambda*I) and substituted the corresponding Eigenvalue into lambda.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
52,The Product $AB$ can be Written as a Sum of Rank $1$,The Product  can be Written as a Sum of Rank,AB 1,"Let $A$ be an $m \times n$ matrix and $B$ be a $n \times \ell$ matrix. Prove that $AB$  can be written as a sum of $n$ matrices of rank one. I am having trouble seeing how this is true. Here is the best solution I could come up with. Let $A_p \in M_{m \times n}$ have its $p$-th column equal to $A$' $p$-th column, with zeros elsewhere, and let  $B_q$ be matrix whose $j$-th column is $B$'s $j$-th column, with zeros elsewhere. Clearly then $A = \sum A_p$ and $B = \sum B_q$. Then $$AB = \sum_{p,q=1}^n A_p B_q$$ $$= \sum_{p=1}^n A_p B_p + \sum_{p \neq q}^n A_p B_q$$ I claim that $A_p B_q = 0$ if $p \neq q$. To see this, consider the $(i,j)$-th entry of $A_pB_q$: $$(A_pB_q)_{ij} = \sum_{k=1}^n (A_p)_{ik}(B_q)_{kj}$$ $$= (A_p)_{ip}(B_q)_{pj} + (A_p)_{iq}(B_q)_{qj} + \sum_{k \neq p,q}^n (A_p)_{ik}(B_q)_{kj}$$ Now, if neither $k=p$ nor $k=q$, then $(A_p)_{ik}=(B_q)_{kj}=0$, which means the sum on the LHS vanishes. When $k=p$, $(B_q)_{pj}=0$ since $B_q$ has zeros across every row, except possibly the $q$-th row; and a similar conclusion is drawn when $k=q$. Thus $(A_pB_q)_{ij}=0$. As I was typing this up, I made a crucial mistake: I thought that the $p$-th column of  $A_pB_P$ was equal to $AB$ and had zeros elsewhere. I tried this on specific $3 \times 3$ $A$ and $B$ and found that every entry of $A_1B_1$ was nonzero; I did find that the $1$-st and $3$-rd column of my example were multiples of each other. So, it may be possible to fix this proof, but I cannot see it. In any case, these matrices $A_pB_p$ are clearly not necessarily rank $1$ matrices. What I find disconcerting is that both link1 and and link2 give roughly the same answer (cf problem 18 on page 75 and 170, respectively), except they conclude that matrices in the sum are of rank at most $1$, which is the conclusion I am coming to draw. So, is this an error in the book.","Let $A$ be an $m \times n$ matrix and $B$ be a $n \times \ell$ matrix. Prove that $AB$  can be written as a sum of $n$ matrices of rank one. I am having trouble seeing how this is true. Here is the best solution I could come up with. Let $A_p \in M_{m \times n}$ have its $p$-th column equal to $A$' $p$-th column, with zeros elsewhere, and let  $B_q$ be matrix whose $j$-th column is $B$'s $j$-th column, with zeros elsewhere. Clearly then $A = \sum A_p$ and $B = \sum B_q$. Then $$AB = \sum_{p,q=1}^n A_p B_q$$ $$= \sum_{p=1}^n A_p B_p + \sum_{p \neq q}^n A_p B_q$$ I claim that $A_p B_q = 0$ if $p \neq q$. To see this, consider the $(i,j)$-th entry of $A_pB_q$: $$(A_pB_q)_{ij} = \sum_{k=1}^n (A_p)_{ik}(B_q)_{kj}$$ $$= (A_p)_{ip}(B_q)_{pj} + (A_p)_{iq}(B_q)_{qj} + \sum_{k \neq p,q}^n (A_p)_{ik}(B_q)_{kj}$$ Now, if neither $k=p$ nor $k=q$, then $(A_p)_{ik}=(B_q)_{kj}=0$, which means the sum on the LHS vanishes. When $k=p$, $(B_q)_{pj}=0$ since $B_q$ has zeros across every row, except possibly the $q$-th row; and a similar conclusion is drawn when $k=q$. Thus $(A_pB_q)_{ij}=0$. As I was typing this up, I made a crucial mistake: I thought that the $p$-th column of  $A_pB_P$ was equal to $AB$ and had zeros elsewhere. I tried this on specific $3 \times 3$ $A$ and $B$ and found that every entry of $A_1B_1$ was nonzero; I did find that the $1$-st and $3$-rd column of my example were multiples of each other. So, it may be possible to fix this proof, but I cannot see it. In any case, these matrices $A_pB_p$ are clearly not necessarily rank $1$ matrices. What I find disconcerting is that both link1 and and link2 give roughly the same answer (cf problem 18 on page 75 and 170, respectively), except they conclude that matrices in the sum are of rank at most $1$, which is the conclusion I am coming to draw. So, is this an error in the book.",,"['linear-algebra', 'matrices', 'matrix-rank']"
53,Matrix similarity equivalent to same characteristic polynomial and same geometric multiplicity,Matrix similarity equivalent to same characteristic polynomial and same geometric multiplicity,,"I'm wondering whether the similarity of two square matrices is equivalent to them having the same characteristic polynomial and the same geometric multiplicity for each eigenvalue. It's obvious in case they are both diagonalizable but is it true when they are not? I can't seem to figure it out, any help would be nice.","I'm wondering whether the similarity of two square matrices is equivalent to them having the same characteristic polynomial and the same geometric multiplicity for each eigenvalue. It's obvious in case they are both diagonalizable but is it true when they are not? I can't seem to figure it out, any help would be nice.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
54,How to find the quaternion representing the rotation between two 3-D vectors?,How to find the quaternion representing the rotation between two 3-D vectors?,,"I have two 3-D vectors: $$ V_1 =  \left[  \begin{array}{r}  -0.9597 \\ -0.9597 \\ 8.8703 \end{array}  \right] $$ and $$ V_2 =  \left[ \begin{array}{r} -0.9568 \\ -0.9368 \\ 8.8432 \end{array} \right] $$ How would I find the quaternion matrix to represent the rotation between $V_1$ and $V_2$? Specifically, what algorithm would I have to utilize to find it? MatLab code would be of great use! Thanks in advance.","I have two 3-D vectors: $$ V_1 =  \left[  \begin{array}{r}  -0.9597 \\ -0.9597 \\ 8.8703 \end{array}  \right] $$ and $$ V_2 =  \left[ \begin{array}{r} -0.9568 \\ -0.9368 \\ 8.8432 \end{array} \right] $$ How would I find the quaternion matrix to represent the rotation between $V_1$ and $V_2$? Specifically, what algorithm would I have to utilize to find it? MatLab code would be of great use! Thanks in advance.",,"['linear-algebra', 'matrices', 'vectors', 'rotations', 'quaternions']"
55,"the determinant of the $n \times n$ matrix $A = (\alpha_v^u)$, where $\alpha_v^u = 1- \delta_v^u,$ is equal to $(n-1)(-1)^{n-1}$","the determinant of the  matrix , where  is equal to","n \times n A = (\alpha_v^u) \alpha_v^u = 1- \delta_v^u, (n-1)(-1)^{n-1}","In the book of Linear Algebra by Werner Greub, at page $111$, question $2$, Prove that the determinant of the $n \times n$ matrix $A = (\alpha_v^u)$, where $$\alpha_v^u = 1- \delta_v^u,$$ is equal to   $(n-1)(-1)^{n-1}$. If we consider $A$ as the matrix of the map $\phi : E \to E, (dim E = n)$, with respect to the basis $e_v$ , we can say that $$\phi (e_v) = (\sum_u e_u) - e_v$$, so by definition $$\Delta_\phi(e_1,..., e_n)= \Delta(\phi(e_1), ..., \phi (e_n)) = \Delta (\sum_{u \not = 1} e_u, ..., \sum_{u \not = n} e_u) = det \phi$$, where $\Delta$ is a nonzero determinant function and $\Delta (e_1, ..., e_n) = 1$, and from that the the only contribution will come from the derangement of $e_v$s, but the number of derangements is huge compare to the $(n-1)(-1)^{n-1}$, so how can continue from that ? I would appreciate help. Note: In here , there is a answer to my question, and its link is given, but I would specifically like to learn how to continue from the point that I have arrived because, for example, if I tried to solve this very same question after a month, I will again use a method similar to this one. Edit: I'm particularly looking for a proof that continues from where I left. Note 2: After 3 months that I have first faced with this question, I have tried to solve it again, and used the same method as the my first attempt above, and stuck in a similar point in the answer given to this question.","In the book of Linear Algebra by Werner Greub, at page $111$, question $2$, Prove that the determinant of the $n \times n$ matrix $A = (\alpha_v^u)$, where $$\alpha_v^u = 1- \delta_v^u,$$ is equal to   $(n-1)(-1)^{n-1}$. If we consider $A$ as the matrix of the map $\phi : E \to E, (dim E = n)$, with respect to the basis $e_v$ , we can say that $$\phi (e_v) = (\sum_u e_u) - e_v$$, so by definition $$\Delta_\phi(e_1,..., e_n)= \Delta(\phi(e_1), ..., \phi (e_n)) = \Delta (\sum_{u \not = 1} e_u, ..., \sum_{u \not = n} e_u) = det \phi$$, where $\Delta$ is a nonzero determinant function and $\Delta (e_1, ..., e_n) = 1$, and from that the the only contribution will come from the derangement of $e_v$s, but the number of derangements is huge compare to the $(n-1)(-1)^{n-1}$, so how can continue from that ? I would appreciate help. Note: In here , there is a answer to my question, and its link is given, but I would specifically like to learn how to continue from the point that I have arrived because, for example, if I tried to solve this very same question after a month, I will again use a method similar to this one. Edit: I'm particularly looking for a proof that continues from where I left. Note 2: After 3 months that I have first faced with this question, I have tried to solve it again, and used the same method as the my first attempt above, and stuck in a similar point in the answer given to this question.",,"['linear-algebra', 'matrices', 'linear-transformations', 'determinant']"
56,Hadamard product of matrices,Hadamard product of matrices,,"I've encountered a notion of Hadamard product, if $A=[a_{ij}],B=[b_{ij}]$ are $n\times n$ matrices then their Hadamard product is $A\circ B=[a_{ij}\ b_{ij}]$. My question is what does it represent, as standard matrix multiplication represents composition of linear maps? I didn't find too much about it safe for definition and basic properties.","I've encountered a notion of Hadamard product, if $A=[a_{ij}],B=[b_{ij}]$ are $n\times n$ matrices then their Hadamard product is $A\circ B=[a_{ij}\ b_{ij}]$. My question is what does it represent, as standard matrix multiplication represents composition of linear maps? I didn't find too much about it safe for definition and basic properties.",,"['linear-algebra', 'matrices']"
57,How to derive the Rotation Matrix from the Euler Formula,How to derive the Rotation Matrix from the Euler Formula,,"I'm trying to understand how the two dimensional rotation matrix (i.e. $R \in \mathbb{R}^2$) can be derived from the Euler Formula ($e^{i\theta} = \cos \theta + i \sin \theta$). $R$ is given as: $$ R(\theta) = \begin{bmatrix}   \cos\theta & -\sin\theta\\   \sin\theta & \cos\theta  \end{bmatrix} $$ $$ \begin{bmatrix}   x' \\   y'  \end{bmatrix} = \begin{bmatrix}   \cos\theta & -\sin\theta\\   \sin\theta & \cos\theta  \end{bmatrix} \begin{bmatrix}   x \\   y  \end{bmatrix} $$ $$   x' = x \cos \theta - y \sin \theta  $$ $$   y' = x \sin \theta + y \cos \theta $$ My questions are: Why can be $i$ omitted from the rotation matrix? (I tried to look for explanations 1 , 2 but none of these explanations goes beyond that  $i$ is omitted) Why can we derive a rotation matrix for $\mathbb{R}^2$ from a form that is defined in $\mathbb{C}^2$? How comes we don't get complex numbers as a result after some rotations?","I'm trying to understand how the two dimensional rotation matrix (i.e. $R \in \mathbb{R}^2$) can be derived from the Euler Formula ($e^{i\theta} = \cos \theta + i \sin \theta$). $R$ is given as: $$ R(\theta) = \begin{bmatrix}   \cos\theta & -\sin\theta\\   \sin\theta & \cos\theta  \end{bmatrix} $$ $$ \begin{bmatrix}   x' \\   y'  \end{bmatrix} = \begin{bmatrix}   \cos\theta & -\sin\theta\\   \sin\theta & \cos\theta  \end{bmatrix} \begin{bmatrix}   x \\   y  \end{bmatrix} $$ $$   x' = x \cos \theta - y \sin \theta  $$ $$   y' = x \sin \theta + y \cos \theta $$ My questions are: Why can be $i$ omitted from the rotation matrix? (I tried to look for explanations 1 , 2 but none of these explanations goes beyond that  $i$ is omitted) Why can we derive a rotation matrix for $\mathbb{R}^2$ from a form that is defined in $\mathbb{C}^2$? How comes we don't get complex numbers as a result after some rotations?",,"['matrices', 'rotations']"
58,If $AA^T=A^TA=I$ and $\det(A)=1$ then $p_A(1)=0$,If  and  then,AA^T=A^TA=I \det(A)=1 p_A(1)=0,"If $AA^T=A^TA=I$ and $\det(A)=1$ then $p_A(1)=0$. Where $A\in M_3(\mathbb{R})$ My approach: We known from the first equation that $A^T=A^{-1}$ and $\lambda_1\lambda_2\lambda_3=1$. Now,since $A$ and $A^T$ have the same characteristic polynomial, they have the same eigenvalues. We also know that the eigenvalues for $A^{-1}$ are $\frac{1}{\lambda_1},\frac{1}{\lambda_2},\frac{1}{\lambda_3}$ and since $A^T=A^{-1}$ this basically means that $\frac{1}{\lambda_1}=\lambda_1,\frac{1}{\lambda_2}=\lambda_2,\frac{1}{\lambda_3}=\lambda_3$ From here there are 2 possible solutions: $\lambda_1=\lambda_2=-1,\lambda_3=1$ or $\lambda_1=\lambda_2=\lambda_3=1$ Is my approach correct?","If $AA^T=A^TA=I$ and $\det(A)=1$ then $p_A(1)=0$. Where $A\in M_3(\mathbb{R})$ My approach: We known from the first equation that $A^T=A^{-1}$ and $\lambda_1\lambda_2\lambda_3=1$. Now,since $A$ and $A^T$ have the same characteristic polynomial, they have the same eigenvalues. We also know that the eigenvalues for $A^{-1}$ are $\frac{1}{\lambda_1},\frac{1}{\lambda_2},\frac{1}{\lambda_3}$ and since $A^T=A^{-1}$ this basically means that $\frac{1}{\lambda_1}=\lambda_1,\frac{1}{\lambda_2}=\lambda_2,\frac{1}{\lambda_3}=\lambda_3$ From here there are 2 possible solutions: $\lambda_1=\lambda_2=-1,\lambda_3=1$ or $\lambda_1=\lambda_2=\lambda_3=1$ Is my approach correct?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
59,Existence of square root of a matrix,Existence of square root of a matrix,,"Testing a method with the use of C.-H. theorem for finding square roots of real $ 2 \times 2 $ matrices I have noticed that some matrices probably don't have  their square roots with  real and complex entries. An example the matrix $A= \begin{bmatrix}  0 & 1 \\ 0 & 0 \end{bmatrix}$. However is it at all a proof that it is impossible  to extend somehow the field of entries in order to satisfy equation $B^2=A$ similar to the situation when many years ago solution of $a^2=-1$ seemed to be impossible to solve for real numbers hence imaginary numbers $i$ were introduced ? Is it   possible to devise such numbers (...quaternions? octonions ? or others..) that $B^2=A$ would be however satisfied ? Additionally, when we are sure in general case for $n \times n$ matrices that a square root exists  if we are free to vastly extend a field?","Testing a method with the use of C.-H. theorem for finding square roots of real $ 2 \times 2 $ matrices I have noticed that some matrices probably don't have  their square roots with  real and complex entries. An example the matrix $A= \begin{bmatrix}  0 & 1 \\ 0 & 0 \end{bmatrix}$. However is it at all a proof that it is impossible  to extend somehow the field of entries in order to satisfy equation $B^2=A$ similar to the situation when many years ago solution of $a^2=-1$ seemed to be impossible to solve for real numbers hence imaginary numbers $i$ were introduced ? Is it   possible to devise such numbers (...quaternions? octonions ? or others..) that $B^2=A$ would be however satisfied ? Additionally, when we are sure in general case for $n \times n$ matrices that a square root exists  if we are free to vastly extend a field?",,"['linear-algebra', 'matrices']"
60,On eigenvalues of complex-orthogonal matrices,On eigenvalues of complex-orthogonal matrices,,"Suppose that $A$ is a complex matrix satisfying $A^TA = I$ (so $A$ is the entrywise transpose, not the conjugate transpose).  What can be said about the eigenvalues of $A$, if $A$ is ""complex-orthogonal"" in this sense? Of course, for any eigenpair $(\lambda,x)$, we have $$ x^Tx = x^TA^TAx = (Ax)^TAx = \lambda^2 (x^Tx) $$ which allows us to conclude that $\lambda^2 = 1$... so long as $x^Tx \neq 0$.  Can anything else be said?  Does the case in which $A$ has real entries allow us to conclude that $|\lambda| = 1$?","Suppose that $A$ is a complex matrix satisfying $A^TA = I$ (so $A$ is the entrywise transpose, not the conjugate transpose).  What can be said about the eigenvalues of $A$, if $A$ is ""complex-orthogonal"" in this sense? Of course, for any eigenpair $(\lambda,x)$, we have $$ x^Tx = x^TA^TAx = (Ax)^TAx = \lambda^2 (x^Tx) $$ which allows us to conclude that $\lambda^2 = 1$... so long as $x^Tx \neq 0$.  Can anything else be said?  Does the case in which $A$ has real entries allow us to conclude that $|\lambda| = 1$?",,"['linear-algebra', 'matrices']"
61,Difference between complement and orthogonal complement (vector spaces),Difference between complement and orthogonal complement (vector spaces),,"I have been learning some linear algebra and, specifically, solutions to simultaneous equations using matrices and the different cases that arise according to the relative number of unknowns and linearly independent equations that we have. The term 'kernel' came up when this was being taught but wasn't properly explained. On looking into this myself a bit further, I came to the conclusion that the kernel was the complement of the space spanned by the rows of the matrix, in the n dimensional space (there are n unknowns). I tried to look this up to verify if I am using the terms right (I know with complements there are sometimes difficulties with something being 'open' or 'closed'- I think in topological spaces anyway. But I am not too familiar with this and am not sure if it applies to vector spaces anyway. So I just wanted to check if the kernel really could be described as the 'complement' of the subspace spanned by the rows). On looking into this, I keep seeing the terms 'orthogonal complement' pop up, and was wondering if 'orthogonal complement' is strictly the same thing as 'complement'? I get that the vectors in the row space are orthogonal to those in the kernel, but i'm not sure if the terms are the same. Also, I was wondering how one could generate a set of vectors spanning the kernel of some matrix (I am not asking for someone to give this method here- I see it has been asked several times!). In the simple case of 2 linearly independent equations and three unknowns, you can take the cross product of the two linearly independent vectors that are two of your equations. I thought that perhaps this could be extended, and perhaps you could find all the vectors by taking the cross product of all of your linearly independent vectors/equations in different orders, maybe all of the cyclic permutations would do it. For example, say that you have n-k linearly independent equations which give the row vectors $\textbf{r}_1,\textbf{r}_2,...,\textbf{r}_{n-k}$, then you could find a set of vectors spanning the kernel by $(\textbf{r}_{n-k} \times (\textbf{r}_{n-k-1} \times ... (\textbf{r}_2 \times \textbf{r}_1)...))$ And you would cycle these around. However on looking at other posts here I have not seen this being used (instead, curiously, I have seen integrals being used). So I was wondering: would this work to generate a set of vectors spanning the kernel? Why/why not?","I have been learning some linear algebra and, specifically, solutions to simultaneous equations using matrices and the different cases that arise according to the relative number of unknowns and linearly independent equations that we have. The term 'kernel' came up when this was being taught but wasn't properly explained. On looking into this myself a bit further, I came to the conclusion that the kernel was the complement of the space spanned by the rows of the matrix, in the n dimensional space (there are n unknowns). I tried to look this up to verify if I am using the terms right (I know with complements there are sometimes difficulties with something being 'open' or 'closed'- I think in topological spaces anyway. But I am not too familiar with this and am not sure if it applies to vector spaces anyway. So I just wanted to check if the kernel really could be described as the 'complement' of the subspace spanned by the rows). On looking into this, I keep seeing the terms 'orthogonal complement' pop up, and was wondering if 'orthogonal complement' is strictly the same thing as 'complement'? I get that the vectors in the row space are orthogonal to those in the kernel, but i'm not sure if the terms are the same. Also, I was wondering how one could generate a set of vectors spanning the kernel of some matrix (I am not asking for someone to give this method here- I see it has been asked several times!). In the simple case of 2 linearly independent equations and three unknowns, you can take the cross product of the two linearly independent vectors that are two of your equations. I thought that perhaps this could be extended, and perhaps you could find all the vectors by taking the cross product of all of your linearly independent vectors/equations in different orders, maybe all of the cyclic permutations would do it. For example, say that you have n-k linearly independent equations which give the row vectors $\textbf{r}_1,\textbf{r}_2,...,\textbf{r}_{n-k}$, then you could find a set of vectors spanning the kernel by $(\textbf{r}_{n-k} \times (\textbf{r}_{n-k-1} \times ... (\textbf{r}_2 \times \textbf{r}_1)...))$ And you would cycle these around. However on looking at other posts here I have not seen this being used (instead, curiously, I have seen integrals being used). So I was wondering: would this work to generate a set of vectors spanning the kernel? Why/why not?",,"['matrices', 'vector-spaces', 'matrix-equations']"
62,Matrices and Divisibility,Matrices and Divisibility,,"Let $p$ be an odd prime number and $T_p$ be the following set of $2$ x $2$ matrices $$T_p=\{A=\left(\begin{array}{cc} a & b\\ c & a \end{array}\right); a,b,c \in\{0,1,2,3...,p-1\}\}$$ Q.1) The no. of $A$ in $T_p$ such that $det(A)$ is not divisible by p. (A) $2p^2$ (B) $p^3-5p$ (C) $p^3-3p$ (D) $p^3 - p^2$ Q.2) The no. of $A$ in $T_p$ such that the trace of $A$ is not divisible by $p$ but $det(A)$ is divisible by $p$ (A) $(p-1)(p^2-p+1)$ (B) $p^3-(p-1)^2$ (C) $(p-1)^2$ (D) $(p-1)(p^2-2)$ Q.3) The no. of $A$ in $T_p$ such that $A$ is either symmetric or skew-symmetric or both and $det(A)$ is divisible by $p$ (A) $(p-1)^2$ (B) $2(p-1)$ (C) $(p-1)^2 +1$ (D) $2p-1$ I wa able to solve Q.3 only. Approach:- Considering the values of $a,b,c,$ $A$ can never be skew-symmetric. Now $det(A) = a^2-bc$ For symmetric matrix, $b=c$ So, $det(A)=a^2-b^2=(a+b)(a-b)$ Case $I$ : $a=b$ There are $p$ ways of selecting $a$ or $b$ (Select any no. in $\{1,2,3...,p-1\}$ Case $II$ : $a \neq b$ $a+b$ must be a multiple of $p$ since $a-b$ will always given a no. less than $p$ according to the given set of $a,b,c$ and also $p$ is a prime no. So there are $p-1$ ways to select $a$ and $b$. Possible ordered pairs of $(a,b)$ $(1,p-1), (2,p-2),...(p-1,1)$ Total ways: $2p-1$ Need help for Q.1 and Q.2","Let $p$ be an odd prime number and $T_p$ be the following set of $2$ x $2$ matrices $$T_p=\{A=\left(\begin{array}{cc} a & b\\ c & a \end{array}\right); a,b,c \in\{0,1,2,3...,p-1\}\}$$ Q.1) The no. of $A$ in $T_p$ such that $det(A)$ is not divisible by p. (A) $2p^2$ (B) $p^3-5p$ (C) $p^3-3p$ (D) $p^3 - p^2$ Q.2) The no. of $A$ in $T_p$ such that the trace of $A$ is not divisible by $p$ but $det(A)$ is divisible by $p$ (A) $(p-1)(p^2-p+1)$ (B) $p^3-(p-1)^2$ (C) $(p-1)^2$ (D) $(p-1)(p^2-2)$ Q.3) The no. of $A$ in $T_p$ such that $A$ is either symmetric or skew-symmetric or both and $det(A)$ is divisible by $p$ (A) $(p-1)^2$ (B) $2(p-1)$ (C) $(p-1)^2 +1$ (D) $2p-1$ I wa able to solve Q.3 only. Approach:- Considering the values of $a,b,c,$ $A$ can never be skew-symmetric. Now $det(A) = a^2-bc$ For symmetric matrix, $b=c$ So, $det(A)=a^2-b^2=(a+b)(a-b)$ Case $I$ : $a=b$ There are $p$ ways of selecting $a$ or $b$ (Select any no. in $\{1,2,3...,p-1\}$ Case $II$ : $a \neq b$ $a+b$ must be a multiple of $p$ since $a-b$ will always given a no. less than $p$ according to the given set of $a,b,c$ and also $p$ is a prime no. So there are $p-1$ ways to select $a$ and $b$. Possible ordered pairs of $(a,b)$ $(1,p-1), (2,p-2),...(p-1,1)$ Total ways: $2p-1$ Need help for Q.1 and Q.2",,"['matrices', 'determinant']"
63,Does this type of matrix have orthogonal eigenvectors?,Does this type of matrix have orthogonal eigenvectors?,,"In this paper , it is claimed that the following matrix has orthogonal eigenvectors: $$M=\left[\begin{matrix} I & \Delta A \\ -\Delta A^T & I + \Delta^2 A^T A\end{matrix}\right]$$ where $A$ is a real matrix, $I$ is the identity matrix, and $\Delta$ is a real number. $M$ is neither Hermitian nor anti-Hermitian, either of which would imply having orthogonal eigenvectors. Does $M$ have orthogonal eigenvectors? How can this be proven?","In this paper , it is claimed that the following matrix has orthogonal eigenvectors: $$M=\left[\begin{matrix} I & \Delta A \\ -\Delta A^T & I + \Delta^2 A^T A\end{matrix}\right]$$ where $A$ is a real matrix, $I$ is the identity matrix, and $\Delta$ is a real number. $M$ is neither Hermitian nor anti-Hermitian, either of which would imply having orthogonal eigenvectors. Does $M$ have orthogonal eigenvectors? How can this be proven?",,"['matrices', 'eigenvalues-eigenvectors']"
64,Mapping a 3D point inside a hexahedron to a unit cube,Mapping a 3D point inside a hexahedron to a unit cube,,"Let A 4x8 be a hexahedron defined by 8 points in 3D homogeneous coordinates. M 8x8 is a map that transforms the hexahedron to a unit cube, B 4x8 . If A = B . M , then M = B -1 . A How can I map a 3D point, P 4x1 in A to B using M ? Example: A = \begin{bmatrix} -0.909198 & -0.717041 & -0.745330 & -0.938879 & -1.435481 & -1.196454 & -1.231644 & -1.472402 \\ 0.867548 & 0.893142 & 0.633169 & 0.615845 & 1.071125 & 1.102962 & 0.779577 & 0.758028 \\ 2.259478 & 2.391340 & 2.435814 & 2.300889 & 2.942843 & 3.106868 & 3.162191 & 2.994355 \\ 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \end{bmatrix} B = \begin{bmatrix} 0.0 & 1.0 &	1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0\\ 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0\\ 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0\\ 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \end{bmatrix} P = \begin{bmatrix} -1.052645\\ 0.833213\\ 2.661413\\ 1.0 \end{bmatrix}","Let A 4x8 be a hexahedron defined by 8 points in 3D homogeneous coordinates. M 8x8 is a map that transforms the hexahedron to a unit cube, B 4x8 . If A = B . M , then M = B -1 . A How can I map a 3D point, P 4x1 in A to B using M ? Example: A = \begin{bmatrix} -0.909198 & -0.717041 & -0.745330 & -0.938879 & -1.435481 & -1.196454 & -1.231644 & -1.472402 \\ 0.867548 & 0.893142 & 0.633169 & 0.615845 & 1.071125 & 1.102962 & 0.779577 & 0.758028 \\ 2.259478 & 2.391340 & 2.435814 & 2.300889 & 2.942843 & 3.106868 & 3.162191 & 2.994355 \\ 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \end{bmatrix} B = \begin{bmatrix} 0.0 & 1.0 &	1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0\\ 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 0.0 & 0.0\\ 0.0 & 0.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0\\ 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \end{bmatrix} P = \begin{bmatrix} -1.052645\\ 0.833213\\ 2.661413\\ 1.0 \end{bmatrix}",,"['matrices', 'solid-geometry']"
65,Linear Independence of Matrices with Powers,Linear Independence of Matrices with Powers,,"I can't seem to figure this out, could someone explain the properties of linear independence when it comes to a matrix to the power of something. The question I'm having trouble with is: Let $A$ be an $n \times n$ matrix and $x$ is an element of $\mathbb{R}^n$ be a non zero vector. Assume that $x$ is an element of $\operatorname{null}(A^{3})$ but $x$ is not an element of $\operatorname{null}(A^{2})$. Show that the set $\{x,Ax, A^{2}x\}$ is linearly independent. Thanks for any help!","I can't seem to figure this out, could someone explain the properties of linear independence when it comes to a matrix to the power of something. The question I'm having trouble with is: Let $A$ be an $n \times n$ matrix and $x$ is an element of $\mathbb{R}^n$ be a non zero vector. Assume that $x$ is an element of $\operatorname{null}(A^{3})$ but $x$ is not an element of $\operatorname{null}(A^{2})$. Show that the set $\{x,Ax, A^{2}x\}$ is linearly independent. Thanks for any help!",,"['linear-algebra', 'matrices', 'independence']"
66,Diagonalization of an infinite matrix,Diagonalization of an infinite matrix,,Let A be an infinite matrix with all its first column elements equal to 1 and the rest of them equal to 0. A =\begin{pmatrix} 1  & 0 & 0 & 0 & \cdots\\ 1  & 0 & 0 & 0 &\cdots\\ 1  & 0 & 0 & 0 & \cdots\\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix} Can A be diagonalized?,Let A be an infinite matrix with all its first column elements equal to 1 and the rest of them equal to 0. A =\begin{pmatrix} 1  & 0 & 0 & 0 & \cdots\\ 1  & 0 & 0 & 0 &\cdots\\ 1  & 0 & 0 & 0 & \cdots\\ \vdots & \vdots & \vdots & \vdots & \ddots \end{pmatrix} Can A be diagonalized?,,"['matrices', 'operator-theory', 'diagonalization']"
67,Derivative of matrix valued function $f(A)=AA^T$,Derivative of matrix valued function,f(A)=AA^T,This is part of a larger proof on the orthonormal group viewed as a manifold. I have never done anything with matrix calculus and am trying to find $df$ where  $$ f(A)=AA^T $$ where  $$ f:M_{n\times n}\to S(n) $$ wehre $S(n)$ are the symmetric $n\times n$ matrices. So  we should have  $$ df:M_{n\times n}\to T_pS(n) $$ where $p\in S(n)$.,This is part of a larger proof on the orthonormal group viewed as a manifold. I have never done anything with matrix calculus and am trying to find $df$ where  $$ f(A)=AA^T $$ where  $$ f:M_{n\times n}\to S(n) $$ wehre $S(n)$ are the symmetric $n\times n$ matrices. So  we should have  $$ df:M_{n\times n}\to T_pS(n) $$ where $p\in S(n)$.,,"['matrices', 'manifolds', 'differential-topology', 'matrix-calculus']"
68,Proof that the incidence matrix of a laminar family is TU.,Proof that the incidence matrix of a laminar family is TU.,,"A friend and I wrote a proof for this using the consecutive ones property that I haven't seen anywhere, so I thought I would share it here. $\textit{Def:}$ A matrix has the Total-Unimodularity (TU) property if the determinant of all of its square submatrices has determinant 1, -1, or 0. $\textit{Def:}$ A family of subsets $S$, is called laminar if for all $s,t \subset S$, if $s \cap t$ is not empty, then either $s \subset t$, or $s \supset t$. Let $S$ be a laminar family of a nonempty finite set $V$, and let $A_5$ denote its $M \times N$ incidence matrix. Prove that $A_S$ is TU.","A friend and I wrote a proof for this using the consecutive ones property that I haven't seen anywhere, so I thought I would share it here. $\textit{Def:}$ A matrix has the Total-Unimodularity (TU) property if the determinant of all of its square submatrices has determinant 1, -1, or 0. $\textit{Def:}$ A family of subsets $S$, is called laminar if for all $s,t \subset S$, if $s \cap t$ is not empty, then either $s \subset t$, or $s \supset t$. Let $S$ be a laminar family of a nonempty finite set $V$, and let $A_5$ denote its $M \times N$ incidence matrix. Prove that $A_S$ is TU.",,"['linear-algebra', 'matrices', 'total-unimodularity']"
69,Determining a matrix given the characteristic and minimal polynomial,Determining a matrix given the characteristic and minimal polynomial,,"Let $p_a=(x-2)^2(x-7)^4x$ be the characteristic polynomial of the matrix $A$ and $(x-2)^2(x-7)x$ the minimal polynomial. Determine the matrix $A$. My work: I know the matrix has to be $7x7$ and in its diagonal it must have two $2$, four $7$ and one $0$, so: \begin{bmatrix}{}     2&  &  &  & & & \\     &  2&  &  & & &\\     & & 7 &  & & &\\    &  &  & 7 & & &\\     &  &  &  & 7& & \\     &  &  &  & & 7 &\\     &  &  &  &  &  & 0\\  \end{bmatrix} I don't know how to follow, what information gives me the minimal polynomial?","Let $p_a=(x-2)^2(x-7)^4x$ be the characteristic polynomial of the matrix $A$ and $(x-2)^2(x-7)x$ the minimal polynomial. Determine the matrix $A$. My work: I know the matrix has to be $7x7$ and in its diagonal it must have two $2$, four $7$ and one $0$, so: \begin{bmatrix}{}     2&  &  &  & & & \\     &  2&  &  & & &\\     & & 7 &  & & &\\    &  &  & 7 & & &\\     &  &  &  & 7& & \\     &  &  &  & & 7 &\\     &  &  &  &  &  & 0\\  \end{bmatrix} I don't know how to follow, what information gives me the minimal polynomial?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
70,Properties of inverse Matrices,Properties of inverse Matrices,,Is it possible for $A^3$ to be an Identity matrix without A being invertibe? If I do the following would it be correct: $A$.$A^2$= I Therefore A has to be invertible,Is it possible for $A^3$ to be an Identity matrix without A being invertibe? If I do the following would it be correct: $A$.$A^2$= I Therefore A has to be invertible,,"['linear-algebra', 'matrices', 'inverse']"
71,How to find the 'real' jordan canonical form of a matrix,How to find the 'real' jordan canonical form of a matrix,,"Given that the the Jordan normal form of a matrix is, $J=\begin{bmatrix}2&1&0&0\\0&2&0&0\\0&0&1-i&0\\0&0&0&1+i\end{bmatrix}$ How do you find the 'real' canonical form of the matrix?","Given that the the Jordan normal form of a matrix is, $J=\begin{bmatrix}2&1&0&0\\0&2&0&0\\0&0&1-i&0\\0&0&0&1+i\end{bmatrix}$ How do you find the 'real' canonical form of the matrix?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
72,What is wrong with this proof that anti-commutative matrices have product zero?,What is wrong with this proof that anti-commutative matrices have product zero?,,"Suppose $AB-BA=0$, where $A$ and $B$ are $n\times n$ matrices. Then $AB=-BA$. Then \begin{aligned} AB=\frac{1}{2}(AB+AB)=\frac{1}{2}(AB-BA)=\frac{1}{2}(0)=0. \end{aligned} I can't seem to find the error in this argument, although I know it's wrong because I found a counterexample by google search.","Suppose $AB-BA=0$, where $A$ and $B$ are $n\times n$ matrices. Then $AB=-BA$. Then \begin{aligned} AB=\frac{1}{2}(AB+AB)=\frac{1}{2}(AB-BA)=\frac{1}{2}(0)=0. \end{aligned} I can't seem to find the error in this argument, although I know it's wrong because I found a counterexample by google search.",,"['linear-algebra', 'matrices']"
73,Inverse of matrix of ones + nI,Inverse of matrix of ones + nI,,"Having a vector $\mathbf{1} \in \mathbb{R}^{n}$ containing only ones, following equality should be true according to a paper I am currently reading: \begin{equation}   \left( nI+\mathbf{1}\mathbf{1}^T \right)^{-1}= \frac{1}{n}\left( I - \frac{1}{2n} \mathbf{1}\mathbf{1}^T \right) \end{equation} EDIT: what is the general rule for constructing an inverse of a matrix with $n$ on diagonal and $1$ elsewhere and how is this rule derived?","Having a vector $\mathbf{1} \in \mathbb{R}^{n}$ containing only ones, following equality should be true according to a paper I am currently reading: \begin{equation}   \left( nI+\mathbf{1}\mathbf{1}^T \right)^{-1}= \frac{1}{n}\left( I - \frac{1}{2n} \mathbf{1}\mathbf{1}^T \right) \end{equation} EDIT: what is the general rule for constructing an inverse of a matrix with $n$ on diagonal and $1$ elsewhere and how is this rule derived?",,"['matrices', 'inverse']"
74,How to prove my matrix's powers?,How to prove my matrix's powers?,,"Being bored I played around with matrices I stumbled upon this matrix. $$T = \left[\begin{array}{ll|cc} 0&2&0&0\\ \frac{1}{2}&0&0&0\\ \hline 1&0&1&0\\ 1&1&0&1 \end{array}\right]$$ What is $T^{2^k}$ for $\cases{k\in \mathbb N\\k>1}$? I have my suspicions what it should be, but how can we prove it?","Being bored I played around with matrices I stumbled upon this matrix. $$T = \left[\begin{array}{ll|cc} 0&2&0&0\\ \frac{1}{2}&0&0&0\\ \hline 1&0&1&0\\ 1&1&0&1 \end{array}\right]$$ What is $T^{2^k}$ for $\cases{k\in \mathbb N\\k>1}$? I have my suspicions what it should be, but how can we prove it?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'proof-writing', 'representation-theory']"
75,How is this a basis for the vector space of symmetric 2x2 matrices?,How is this a basis for the vector space of symmetric 2x2 matrices?,,"*EDIT, I posted the incorrect set of matrices. Unsure how to indicate this,but I've correct the question now.  \begin{bmatrix}0&1\\1&1\end{bmatrix} \begin{bmatrix}1&1\\1&0\end{bmatrix} \begin{bmatrix}1&2\\2&-3\end{bmatrix} This comes from a MC question that has 6 sets of three 2x2 matrices. 5 are basis for the vector space of symmetric 2x2 matrices and one is not. According to the answer, this one should be a basis. However, I'm having difficulty seeing how. My understanding is that if it's a basis for symmetric 2x2 matrices, then there must exist c1, c2, c3 such that: c1[M1] + c2[M2] +c2[M3]= any symmetric 2x matrix, ie.  \begin{bmatrix}1&0\\0&1\end{bmatrix} But, I can't find any such constants that would make this set of matrices equal to that matrix.","*EDIT, I posted the incorrect set of matrices. Unsure how to indicate this,but I've correct the question now.  \begin{bmatrix}0&1\\1&1\end{bmatrix} \begin{bmatrix}1&1\\1&0\end{bmatrix} \begin{bmatrix}1&2\\2&-3\end{bmatrix} This comes from a MC question that has 6 sets of three 2x2 matrices. 5 are basis for the vector space of symmetric 2x2 matrices and one is not. According to the answer, this one should be a basis. However, I'm having difficulty seeing how. My understanding is that if it's a basis for symmetric 2x2 matrices, then there must exist c1, c2, c3 such that: c1[M1] + c2[M2] +c2[M3]= any symmetric 2x matrix, ie.  \begin{bmatrix}1&0\\0&1\end{bmatrix} But, I can't find any such constants that would make this set of matrices equal to that matrix.",,"['matrices', 'symmetric-matrices']"
76,Derivative of matrix-vector products w.r.t. the matrix,Derivative of matrix-vector products w.r.t. the matrix,,"Given the function $$F(X,Y,Z) = \alpha^TXYZ$$ in which $X, Y, Z $ are matrices of size $n \times n$ and $\alpha$ is a vector of size $n \times 1$, how to compute the derivative of $F$ with respect to $Y$? Actually I found some related questions but did not help. Edit: if the function is of the form: $F(X,Y,Z) = \alpha^TXYZ\beta$, then based on the Matrix Cookbook, derivative is : $f' = (\alpha^T X)^T (Z\beta)^T$, but if there is no $\beta$, then the dimensions do not match. Thank you,","Given the function $$F(X,Y,Z) = \alpha^TXYZ$$ in which $X, Y, Z $ are matrices of size $n \times n$ and $\alpha$ is a vector of size $n \times 1$, how to compute the derivative of $F$ with respect to $Y$? Actually I found some related questions but did not help. Edit: if the function is of the form: $F(X,Y,Z) = \alpha^TXYZ\beta$, then based on the Matrix Cookbook, derivative is : $f' = (\alpha^T X)^T (Z\beta)^T$, but if there is no $\beta$, then the dimensions do not match. Thank you,",,"['matrices', 'derivatives', 'matrix-calculus']"
77,Can this symmetric matrix be an orthogonal matrix?,Can this symmetric matrix be an orthogonal matrix?,,"So the question is to prove whether this matrix M is orthogonal? $$         M = \begin{bmatrix}         a & k & k \\         k & a & k \\         k & k & a \\         \end{bmatrix} $$ My attempt to find the inverse of M : $$det(M)=(a+2k)(a-k)(a-k)$$ The inverse of $M$ is : $$         \begin{matrix}         (a+k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k))\\         (-k)/((a-k)(a+2k)) & (a+k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k))\\         (-k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k)) & (a+k)/((a-k)(a+2k))\\         \end{matrix} $$ $M$ is orthogonal if $M^T = M^{-1}$ So I end up with a system of 2 equations to solve : $a=\dfrac{a+k}{(a-k)(a+2k)}$ and $k=\dfrac{-k}{(a-k)(a+2k)}$ and now I am stucked. I would really appreciate feedbacks about whether my steps are all correct and if yes, what to do next ?","So the question is to prove whether this matrix M is orthogonal? $$         M = \begin{bmatrix}         a & k & k \\         k & a & k \\         k & k & a \\         \end{bmatrix} $$ My attempt to find the inverse of M : $$det(M)=(a+2k)(a-k)(a-k)$$ The inverse of $M$ is : $$         \begin{matrix}         (a+k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k))\\         (-k)/((a-k)(a+2k)) & (a+k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k))\\         (-k)/((a-k)(a+2k)) & (-k)/((a-k)(a+2k)) & (a+k)/((a-k)(a+2k))\\         \end{matrix} $$ $M$ is orthogonal if $M^T = M^{-1}$ So I end up with a system of 2 equations to solve : $a=\dfrac{a+k}{(a-k)(a+2k)}$ and $k=\dfrac{-k}{(a-k)(a+2k)}$ and now I am stucked. I would really appreciate feedbacks about whether my steps are all correct and if yes, what to do next ?",,"['linear-algebra', 'matrices', 'determinant', 'orthogonality']"
78,What should be the value of a for it to be a singular matrix,What should be the value of a for it to be a singular matrix,,"for what value of a, $\begin{bmatrix}2a & -1\\-8 & 3\end{bmatrix}$ is a singular matrix. Can you also explain to me how to prove that a matrix is a singular matrix?","for what value of a, $\begin{bmatrix}2a & -1\\-8 & 3\end{bmatrix}$ is a singular matrix. Can you also explain to me how to prove that a matrix is a singular matrix?",,['matrices']
79,Matrix derivative w.r.t time $\frac{d}{dt}\|A-X(t)\|^2=2\operatorname{tr}\big((X-A)\dot{X}\big)$,Matrix derivative w.r.t time,\frac{d}{dt}\|A-X(t)\|^2=2\operatorname{tr}\big((X-A)\dot{X}\big),"My question is just how to derive this equality? $$\frac{d}{dt}\|A-X(t)\|^2=2\operatorname{tr}\big((X-A)\dot{X}\big)$$ where $A, X\in \mathbb{R}^{n\times n}$ In particular, how to obtain the matrices trace product term? It seems there are three levels chain rule, one for square, one for norm and then $X(t)$ I just know the following:    $$\nabla_x \|Ax-b\|^2=2A^T(Ax-b)$$","My question is just how to derive this equality? $$\frac{d}{dt}\|A-X(t)\|^2=2\operatorname{tr}\big((X-A)\dot{X}\big)$$ where $A, X\in \mathbb{R}^{n\times n}$ In particular, how to obtain the matrices trace product term? It seems there are three levels chain rule, one for square, one for norm and then $X(t)$ I just know the following:    $$\nabla_x \|Ax-b\|^2=2A^T(Ax-b)$$",,"['matrices', 'derivatives', 'chain-rule']"
80,How can I quickly know the rank of this / any other matrix?,How can I quickly know the rank of this / any other matrix?,,"I have looked this up on several sites but they confused me because some of the given information was wrong / unclear / contradicting whatever. I hope you can tell me all / most important ways to calculate the rank of a matrix. As example, I take the matrix $$A = \begin{pmatrix} 1 & 2 & 3\\  0 & 5 & 4\\  0 & 10& 2 \end{pmatrix}$$ Now several sites included that info so it must be true: If, we are looking at this example, there is no line with zeroes only, the rank of this matrix will be $3$. (?) Here is the problem. It will cost time to form this matrix to see if there will be lines with zeroes only. For this I can use Gaussian Elimination . I have tested it with that Gauss and I couldn't get a line with zeroes only, so I conclude that this matrix $rank(A)=3$. This however seems very inefficient way, I hope you can tell me better ways?","I have looked this up on several sites but they confused me because some of the given information was wrong / unclear / contradicting whatever. I hope you can tell me all / most important ways to calculate the rank of a matrix. As example, I take the matrix $$A = \begin{pmatrix} 1 & 2 & 3\\  0 & 5 & 4\\  0 & 10& 2 \end{pmatrix}$$ Now several sites included that info so it must be true: If, we are looking at this example, there is no line with zeroes only, the rank of this matrix will be $3$. (?) Here is the problem. It will cost time to form this matrix to see if there will be lines with zeroes only. For this I can use Gaussian Elimination . I have tested it with that Gauss and I couldn't get a line with zeroes only, so I conclude that this matrix $rank(A)=3$. This however seems very inefficient way, I hope you can tell me better ways?",,"['linear-algebra', 'matrices', 'number-theory', 'elementary-number-theory']"
81,Derivative of trace of log of matrix products w.r.t. a matrix,Derivative of trace of log of matrix products w.r.t. a matrix,,"Is there a way to calculate $$\frac{\partial\; \mbox{tr}\{\log(X^tBX)\}}{\partial X},$$ where $X$ and $B$ are $n\times n$ matrices?","Is there a way to calculate $$\frac{\partial\; \mbox{tr}\{\log(X^tBX)\}}{\partial X},$$ where $X$ and $B$ are $n\times n$ matrices?",,['matrices']
82,Find the determinant of a 5x5 matrix,Find the determinant of a 5x5 matrix,,"Find the determinant of the following matrix:  $$\begin{bmatrix} 1& 1& 1& 1& 1\\ 3 & 3 &3 &3 &2\\ 4& 4& 4& 3& 3\\ 5& 5& 4& 4&  4\\  6& 5& 5& 5 &5\end{bmatrix}$$ Laplace doesn't seem like the best method here, can we somehow turn this into a triangular matrix so that the determinant is the product of the elements on the main diagonal?  I multiplied the first row by $(-3)$ and added it to he second one, then by $(-4)$ and added it to the third one, by $(-5)$ and added it to the fourth one, and by $(-6)$ and added it to last one.  $$\begin{vmatrix} 1& 1& 1& 1& 1\\ 3 & 3 &3 &3 &2\\ 4& 4& 4& 3& 3\\ 5& 5& 4& 4&  4\\  6& 5& 5& 5 &5 \end{vmatrix}=\begin{vmatrix} 1& 1& 1& 1& 1\\ 0& 0 &0 &0 &-1\\ 0& 0& 0& -1& -1\\ 0& 0& -1& -1&  -1\\  0& -1& -1& -1 &-1 \end{vmatrix}$$ What should I do now?","Find the determinant of the following matrix:  $$\begin{bmatrix} 1& 1& 1& 1& 1\\ 3 & 3 &3 &3 &2\\ 4& 4& 4& 3& 3\\ 5& 5& 4& 4&  4\\  6& 5& 5& 5 &5\end{bmatrix}$$ Laplace doesn't seem like the best method here, can we somehow turn this into a triangular matrix so that the determinant is the product of the elements on the main diagonal?  I multiplied the first row by $(-3)$ and added it to he second one, then by $(-4)$ and added it to the third one, by $(-5)$ and added it to the fourth one, and by $(-6)$ and added it to last one.  $$\begin{vmatrix} 1& 1& 1& 1& 1\\ 3 & 3 &3 &3 &2\\ 4& 4& 4& 3& 3\\ 5& 5& 4& 4&  4\\  6& 5& 5& 5 &5 \end{vmatrix}=\begin{vmatrix} 1& 1& 1& 1& 1\\ 0& 0 &0 &0 &-1\\ 0& 0& 0& -1& -1\\ 0& 0& -1& -1&  -1\\  0& -1& -1& -1 &-1 \end{vmatrix}$$ What should I do now?",,"['linear-algebra', 'matrices', 'determinant']"
83,Is it possible for a matrix to have no leading ones?,Is it possible for a matrix to have no leading ones?,,Is it possible for a matrix to have no leading ones? So that its reduced row echelon form has no leading ones. Is this possible? Would the empty set be the only possible solution to this problem?,Is it possible for a matrix to have no leading ones? So that its reduced row echelon form has no leading ones. Is this possible? Would the empty set be the only possible solution to this problem?,,"['matrices', 'matrix-equations', 'matrix-calculus']"
84,Dimensions of image and kernel of a $n \times n$ matrix,Dimensions of image and kernel of a  matrix,n \times n,"Let $K$ be a field, $V$ a vector space over $K$ of a finite dimension $n=dim_K(V)$. Let $f : V \rightarrow V$ be a  $K$-linear map and $\mathfrak{B}$ an ordered basis of $V$ with $$M_{f, \mathfrak{B}, \mathfrak{B}} =   \begin{pmatrix}   0 & 1 & 1 & 1 & \cdots &1 \\   0 & 0 & 1 & 1 &\cdots&1 \\   0 & 0 & 0 & 1 & \cdots &1 \\   \vdots  & \vdots  & \vdots & \ddots & \ddots & \vdots  \\   &&&&&1\\   0 & 0 & 0 & 0 & \cdots & 0   \end{pmatrix}$$ Side questions : Does this matrix have a name? Does the basis with the columns as vectors have a name? I need to calculate the dimensions of the image and the kernel of $f$, I know how to do it with a completely given matrix and basis, but without it I have problems. Any hints welcome.","Let $K$ be a field, $V$ a vector space over $K$ of a finite dimension $n=dim_K(V)$. Let $f : V \rightarrow V$ be a  $K$-linear map and $\mathfrak{B}$ an ordered basis of $V$ with $$M_{f, \mathfrak{B}, \mathfrak{B}} =   \begin{pmatrix}   0 & 1 & 1 & 1 & \cdots &1 \\   0 & 0 & 1 & 1 &\cdots&1 \\   0 & 0 & 0 & 1 & \cdots &1 \\   \vdots  & \vdots  & \vdots & \ddots & \ddots & \vdots  \\   &&&&&1\\   0 & 0 & 0 & 0 & \cdots & 0   \end{pmatrix}$$ Side questions : Does this matrix have a name? Does the basis with the columns as vectors have a name? I need to calculate the dimensions of the image and the kernel of $f$, I know how to do it with a completely given matrix and basis, but without it I have problems. Any hints welcome.",,"['linear-algebra', 'matrices']"
85,Diagonalizable matrix with eigenvalues $\pm 1$,Diagonalizable matrix with eigenvalues,\pm 1,Let $A \in \mathbb{R}^{n \times n}$ be a diagonalizable matrix with $1$ or $-1$ as its only eigenvalues. Prove that $A^{2} = I_{n}$. Could someone help me on this one? I have no idea how to start.,Let $A \in \mathbb{R}^{n \times n}$ be a diagonalizable matrix with $1$ or $-1$ as its only eigenvalues. Prove that $A^{2} = I_{n}$. Could someone help me on this one? I have no idea how to start.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'involutions']"
86,Is there such a thing as an infinity vector?,Is there such a thing as an infinity vector?,,"I've been see the following question on group theory: Let $p$ be a prime, and let $G = SL2(p)$ be the group of $2 \times 2$ matrices of determinant $1$ with entries in the ﬁeld $F(p)$ of integers $\mod p$. (i) Deﬁne the action of $G$ on $X = F(p) \cup \{ \infty \}$ by Mobius transformations. [You need not show that it is a group action.] State the orbit-stabiliser theorem. Determine the orbit of $\infty$ and the stabiliser of $\infty$. Hence compute the order of $SL2(p)$. I know matrices are isomorphic to Mobius maps, but not how the action of a mobius map can be used to define the action of a matrix (I don't really know what this part means to be honest). I tried the next part, but wasnn't sure whether the consider the vector $(\infty,\infty)$, $(\infty,a)$ or $(b,\infty)$ $(a,b \in F(p))$. Any help would be greatly appreciated!! (Sorry the question title isn't very related to the question, I just didn't know what to put specifically!)","I've been see the following question on group theory: Let $p$ be a prime, and let $G = SL2(p)$ be the group of $2 \times 2$ matrices of determinant $1$ with entries in the ﬁeld $F(p)$ of integers $\mod p$. (i) Deﬁne the action of $G$ on $X = F(p) \cup \{ \infty \}$ by Mobius transformations. [You need not show that it is a group action.] State the orbit-stabiliser theorem. Determine the orbit of $\infty$ and the stabiliser of $\infty$. Hence compute the order of $SL2(p)$. I know matrices are isomorphic to Mobius maps, but not how the action of a mobius map can be used to define the action of a matrix (I don't really know what this part means to be honest). I tried the next part, but wasnn't sure whether the consider the vector $(\infty,\infty)$, $(\infty,a)$ or $(b,\infty)$ $(a,b \in F(p))$. Any help would be greatly appreciated!! (Sorry the question title isn't very related to the question, I just didn't know what to put specifically!)",,"['matrices', 'group-theory', 'infinity', 'mobius-transformation']"
87,Eigenstructure of a matrix polynomial,Eigenstructure of a matrix polynomial,,"Given a square matrix A and a polynomial p(x), what is the eigenstructure of p(A)? I can show that if $\{ \lambda_i \}_{i=1,2,...n}$ is the spectrum of $A$, then $\{ p(\lambda_i \}_{i=1,2,...n}$ is the spectrum of $p(A)$. the geometric multiplicity of an eigenvalue $\lambda$ of $A$ is less than or equal to the geometric multiplicity of $p(\lambda)$ of $p(A)$. What I do not know is what happens to the algebraic multiplicites. Any pointers?","Given a square matrix A and a polynomial p(x), what is the eigenstructure of p(A)? I can show that if $\{ \lambda_i \}_{i=1,2,...n}$ is the spectrum of $A$, then $\{ p(\lambda_i \}_{i=1,2,...n}$ is the spectrum of $p(A)$. the geometric multiplicity of an eigenvalue $\lambda$ of $A$ is less than or equal to the geometric multiplicity of $p(\lambda)$ of $p(A)$. What I do not know is what happens to the algebraic multiplicites. Any pointers?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
88,Inverse of $I+BA$ is $(I+BA)^{-1} = I-B(I+AB)^{-1}A$ [duplicate],Inverse of  is  [duplicate],I+BA (I+BA)^{-1} = I-B(I+AB)^{-1}A,"This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . Assume $I+AB$ is invertible, prove then that $I+BA$ is invertible and $(I+BA)^{-1} = I-B(I+AB)^{-1}A$. My work: $(I+AB)(I+AB)^{-1} =  I$ $B(I+AB)(I+AB)^{-1} = B $ $(I+BA)B(I+AB)^{-1} = B$ $(I+BA)B(I+AB)^{-1}B^{-1} = I$ Thus $(I+BA)$ is invertible and $B(I+AB)^{-1}B^{-1}$ is its inverse. But I have no clue how to arrive to the given inverse formula. I feel like I'm missing something. Can anyone help?","This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . Assume $I+AB$ is invertible, prove then that $I+BA$ is invertible and $(I+BA)^{-1} = I-B(I+AB)^{-1}A$. My work: $(I+AB)(I+AB)^{-1} =  I$ $B(I+AB)(I+AB)^{-1} = B $ $(I+BA)B(I+AB)^{-1} = B$ $(I+BA)B(I+AB)^{-1}B^{-1} = I$ Thus $(I+BA)$ is invertible and $B(I+AB)^{-1}B^{-1}$ is its inverse. But I have no clue how to arrive to the given inverse formula. I feel like I'm missing something. Can anyone help?",,"['linear-algebra', 'matrices', 'inverse']"
89,What happens to the eigenvalues of a diagonal matrix $\text{diag}(v)$ after a symmetric rank 1 modification?,What happens to the eigenvalues of a diagonal matrix  after a symmetric rank 1 modification?,\text{diag}(v),"Let $M$ be a symmetric $n \times n$ matrix,  let $v \in \mathbb{R}^n$.   Then $M - vv^T$ is said to be a rank-1 modification of $M$ since   $\text{rank}(vv^T) = 1$ Suppose I have a diagonal matrix:  $$M = \text{diag}(v)$$ $v \in \mathbb{R}^n,  \alpha \geq v_i \geq 0, \alpha \geq 0$ Clearly, $v_i$ are the eigenvalues of $M$. What happens to the eigenvalues after a rank-1 modification ? Is it possible to know the precise values of the eigenvalues in this case? Or is it possible to bound it? $$M - vv^T = \text{diag}(v) - vv^T?$$ Is this a well known result? Any reference would help! For example: Let $z = \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix}$ Then $zz^T = \begin{bmatrix} 0.09 &    0.12 \\  0.12  &  0.16 \end{bmatrix}$ So $\text{diag}(z) - zz^T = \begin{bmatrix} 0.21 &   -0.12 \\  -0.12 &   0.24 \end{bmatrix}$ $\text{eigs}(\text{diag}(z) - zz^T)  = \{ 0.3459,   0.1041\}$ Seems this answer might help? Maybe not Determinant of rank-one perturbations of (invertible) matrices","Let $M$ be a symmetric $n \times n$ matrix,  let $v \in \mathbb{R}^n$.   Then $M - vv^T$ is said to be a rank-1 modification of $M$ since   $\text{rank}(vv^T) = 1$ Suppose I have a diagonal matrix:  $$M = \text{diag}(v)$$ $v \in \mathbb{R}^n,  \alpha \geq v_i \geq 0, \alpha \geq 0$ Clearly, $v_i$ are the eigenvalues of $M$. What happens to the eigenvalues after a rank-1 modification ? Is it possible to know the precise values of the eigenvalues in this case? Or is it possible to bound it? $$M - vv^T = \text{diag}(v) - vv^T?$$ Is this a well known result? Any reference would help! For example: Let $z = \begin{bmatrix} 0.3 \\ 0.4 \end{bmatrix}$ Then $zz^T = \begin{bmatrix} 0.09 &    0.12 \\  0.12  &  0.16 \end{bmatrix}$ So $\text{diag}(z) - zz^T = \begin{bmatrix} 0.21 &   -0.12 \\  -0.12 &   0.24 \end{bmatrix}$ $\text{eigs}(\text{diag}(z) - zz^T)  = \{ 0.3459,   0.1041\}$ Seems this answer might help? Maybe not Determinant of rank-one perturbations of (invertible) matrices",,"['linear-algebra', 'matrices', 'reference-request', 'eigenvalues-eigenvectors', 'spectral-theory']"
90,Adjoint matrix as pseudo-inverse,Adjoint matrix as pseudo-inverse,,"I'm very new to signal processing (seismic 1,2 and 3D-signal) and have read many papers recently. One thing I encounter quite often is the use of adjoint matrix. If $d = Am$ where $d$ is the data, $A$ is an operator that models a physical process and $m$ is the model, many people define $\widetilde m=A^Td$  or $\widetilde m=A^*d$ be the pseudo-inverse for $m$ and sometimes use $\widetilde m$ as a starting model. I know $A^{-1}=A^T$ sometimes but in general it's not the case. So how good is this pseudo-inverse? Can someone give me some more insight or give me some reference on this?","I'm very new to signal processing (seismic 1,2 and 3D-signal) and have read many papers recently. One thing I encounter quite often is the use of adjoint matrix. If $d = Am$ where $d$ is the data, $A$ is an operator that models a physical process and $m$ is the model, many people define $\widetilde m=A^Td$  or $\widetilde m=A^*d$ be the pseudo-inverse for $m$ and sometimes use $\widetilde m$ as a starting model. I know $A^{-1}=A^T$ sometimes but in general it's not the case. So how good is this pseudo-inverse? Can someone give me some more insight or give me some reference on this?",,"['matrices', 'adjoint-operators', 'pseudoinverse', 'transpose']"
91,Prove that the determinant of a matrix is zero,Prove that the determinant of a matrix is zero,,"Hi I need some help with this question: Let $A$ be an $n \times n$ matrix, let $i, j, k$ be pairwise distinct indices, $1 \leq i, j, k \leq n$ , and let $\lambda,\mu \in \mathbb R$ be arbitrary real numbers. Suppose that $a_k$ , the $k-$ th row vector of $A$ , is equal to $\lambda a_i + \mu a_j$ , where $a_i, a_j ∈ \mathbb R^n$ denote the $i-$ th and the $j-$ th row vectors of $A$ respectively. Prove that $\det(A) = 0$ . I think I need to split the matrix up into two separate ones then use the fact that one of these matrices has either a row of zeros or a row is a multiple of another then use $\det(AB)=\det(A)\det(B)$ to show one of these matrices has a determinant of zero so the whole thing has a determinant of zero. So I was wondering is there a way to split these matrices up so it suits my method?","Hi I need some help with this question: Let be an matrix, let be pairwise distinct indices, , and let be arbitrary real numbers. Suppose that , the th row vector of , is equal to , where denote the th and the th row vectors of respectively. Prove that . I think I need to split the matrix up into two separate ones then use the fact that one of these matrices has either a row of zeros or a row is a multiple of another then use to show one of these matrices has a determinant of zero so the whole thing has a determinant of zero. So I was wondering is there a way to split these matrices up so it suits my method?","A n \times n i, j, k 1 \leq i, j, k \leq n \lambda,\mu \in \mathbb R a_k k- A \lambda a_i + \mu a_j a_i, a_j ∈ \mathbb R^n i- j- A \det(A) = 0 \det(AB)=\det(A)\det(B)","['linear-algebra', 'matrices', 'proof-writing', 'determinant']"
92,Do we have $\|A\|_F\leq \|B\|_F$ if $-B\preceq A\preceq B$?,Do we have  if ?,\|A\|_F\leq \|B\|_F -B\preceq A\preceq B,"In the question, $A$ is a symmetric matrix, and $B$ is a positive semi-definite matrix. $A\preceq B$ means that $B-A$ is a positive semi-definite matrix. $\|\|_F$ means the Frobenius norm.","In the question, $A$ is a symmetric matrix, and $B$ is a positive semi-definite matrix. $A\preceq B$ means that $B-A$ is a positive semi-definite matrix. $\|\|_F$ means the Frobenius norm.",,"['calculus', 'linear-algebra', 'matrices']"
93,"Max and min eigenvalues of the ""normalized"" adjacency matrix of a path","Max and min eigenvalues of the ""normalized"" adjacency matrix of a path",,"Setup : Let $A$ be the $n \times n$ adjacency matrix of a path with $n$ nodes, so the $ij^\text{th}$ element of $A$ is $a_{ij} = 1(|i-j|=1)$.  I.e. the elements just off the main diagonal are $1$, and everything else is $0$. Now define $B$ as equal to $A$, but with row sums normalized to equal $1$.  I.e., $b_{ij} = a_{ij} / a_{i+}$, where $a_{i+}$ is the $i^\text{th}$ row sum of $A$.  Here are the matrices: $$ A = \begin{bmatrix}   0&1 \\   1&0&1&& \mathbf 0 \\   &1&0& \ddots \\   &&\ddots&\ddots &1& \\   & \mathbf 0&&1&0&1 \\   &&&&1&0 \end{bmatrix},\,\,\,\, B = \begin{bmatrix}   0&1 \\   1/2&0&1/2&& \mathbf 0 \\   &1/2&0& \ddots \\   &&\ddots&\ddots &1/2& \\   & \mathbf 0&&1/2&0&1/2 \\   &&&&1&0 \end{bmatrix}. $$ Note that the 1st and last rows have 1 non-zero element (since the nodes at the ends of the path have only 1 neighbor), while the middle rows have two non-zero elements (since the middle nodes have a neighbor on each side). My question : Can you prove that the max and min eigenvalues of $B$ are $1$ and $-1$, respectively, for any $n \ge 2$?  I've checked that this is true (with a computer program) for all $n$ from 2 to 500.  So I'm pretty confident that it's true in general, but I'm wondering if there's a proof. Context: The reason I care about this is that $B$ appears in a statistical model I'm using (a CAR spatial model), and the max and min eigenvalues of $B$ determine the parameter space for a parameter in the model. This is clearly closely related to the following asked-and-answered question: Computing eigenvalue of the adjacency matrix of a path . There they wanted all eigenvalues of $A$, whereas I want the max and min eigenvalues of $B$. Also, although this might not be helpful, here's a plot of all $n$ eigenvalues of $B$, for $n = 2,\ldots,16$.","Setup : Let $A$ be the $n \times n$ adjacency matrix of a path with $n$ nodes, so the $ij^\text{th}$ element of $A$ is $a_{ij} = 1(|i-j|=1)$.  I.e. the elements just off the main diagonal are $1$, and everything else is $0$. Now define $B$ as equal to $A$, but with row sums normalized to equal $1$.  I.e., $b_{ij} = a_{ij} / a_{i+}$, where $a_{i+}$ is the $i^\text{th}$ row sum of $A$.  Here are the matrices: $$ A = \begin{bmatrix}   0&1 \\   1&0&1&& \mathbf 0 \\   &1&0& \ddots \\   &&\ddots&\ddots &1& \\   & \mathbf 0&&1&0&1 \\   &&&&1&0 \end{bmatrix},\,\,\,\, B = \begin{bmatrix}   0&1 \\   1/2&0&1/2&& \mathbf 0 \\   &1/2&0& \ddots \\   &&\ddots&\ddots &1/2& \\   & \mathbf 0&&1/2&0&1/2 \\   &&&&1&0 \end{bmatrix}. $$ Note that the 1st and last rows have 1 non-zero element (since the nodes at the ends of the path have only 1 neighbor), while the middle rows have two non-zero elements (since the middle nodes have a neighbor on each side). My question : Can you prove that the max and min eigenvalues of $B$ are $1$ and $-1$, respectively, for any $n \ge 2$?  I've checked that this is true (with a computer program) for all $n$ from 2 to 500.  So I'm pretty confident that it's true in general, but I'm wondering if there's a proof. Context: The reason I care about this is that $B$ appears in a statistical model I'm using (a CAR spatial model), and the max and min eigenvalues of $B$ determine the parameter space for a parameter in the model. This is clearly closely related to the following asked-and-answered question: Computing eigenvalue of the adjacency matrix of a path . There they wanted all eigenvalues of $A$, whereas I want the max and min eigenvalues of $B$. Also, although this might not be helpful, here's a plot of all $n$ eigenvalues of $B$, for $n = 2,\ldots,16$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-graph-theory', 'adjacency-matrix']"
94,Finding perturbed eigenvectors in degenerate first order perturbation theory,Finding perturbed eigenvectors in degenerate first order perturbation theory,,"Consider a symmetric matrix $H(t)$ parametrized smoothly by $t\in\mathbb R$. Suppose there are orthonormal eigenvectors $\phi_i(t)$ with corresponding eigenvalues $\lambda_i(t)$: $$ H(t)\phi_i(t)=\lambda_i(t)\phi_i(t). \tag{1} $$ Given $H(0)$ and $H'(0)$, I would like to find $\phi_i'(0)$. If $H(0)$ has degenerate eigenvalues, I do not see how to find all components of the eigenfunctions corresponding to a degenerate eigenvalue at $t=0$. Let me elaborate. $\newcommand{\ip}[2]{\langle#1,#2\rangle}$ Let me drop the argument $(t)$ whenever $t=0$. First, the eigenvectors are normalized, yielding $$ \ip{\phi_i'}{\phi_i}=0. \tag{2} $$ Differentiating (1) with respect to $t$ at $t=0$ gives $$ H'\phi_i +H\phi_i' = \lambda_i'\phi_i +\lambda_i\phi_i'. \tag{3} $$ Taking inner product with $\phi_i$ and using symmetry and (2) gives $$ \lambda_i'=\ip{\phi_i}{H'\phi_i}. \tag{4} $$ These are the first order perturbations of the eigenvalues. I also want to find the first order perturbation of the eigenvectors, $\phi_i'$. Doing so is equivalent with finding the inner product $\ip{\phi_j}{\phi_i'}$ for all $j$. Taking inner product of (3) with $\phi_j$ for $j\neq i$ gives $$ \ip{\phi_j}{H'\phi_i} = (\lambda_i-\lambda_j)\ip{\phi_j}{\phi_i'}. $$ Thus if $\lambda_j\neq\lambda_i$, we obtain $$ \ip{\phi_j}{\phi_i'} = \frac{\ip{\phi_j}{H'\phi_i}}{\lambda_i-\lambda_j}. \tag{5} $$ On the other hand, if $\lambda_j=\lambda_i$, we obtain $\ip{\phi_j}{H'\phi_i}=0$. That is, the block of $H'$ corresponding to the eigenspace of $H$ of eigenvalue $\lambda_i$ is diagonal. We have freedom in choosing the orthonormal basis when the spectrum degenerates, and it must be chosen in this way to make $\phi_i(t)$ depend continuously on $t$. Therefore the diagonal values in (4) are in fact eigenvalues of this block matrix. For $i=j$ the desired inner product $\ip{\phi_j}{\phi_i'}$ is given by (2). If $i\neq j$ and $\lambda_i\neq\lambda_j$, it is given by (5). But what is the inner product $\ip{\phi_j}{\phi_i'}$ when $i\neq j$ and $\lambda_i=\lambda_j$? It seems to me that the presented calculations give no constraints. I have lost no information by taking the inner product of (3) with all $\phi_j$ and looking at all the inner products instead of the vector equation. I must be missing something. (This puzzled me for quite a while and I found no answer elsewhere, so I decided to share the question and an answer here. Actually, I figured it out while writing this question. Other answers are very much welcome!)","Consider a symmetric matrix $H(t)$ parametrized smoothly by $t\in\mathbb R$. Suppose there are orthonormal eigenvectors $\phi_i(t)$ with corresponding eigenvalues $\lambda_i(t)$: $$ H(t)\phi_i(t)=\lambda_i(t)\phi_i(t). \tag{1} $$ Given $H(0)$ and $H'(0)$, I would like to find $\phi_i'(0)$. If $H(0)$ has degenerate eigenvalues, I do not see how to find all components of the eigenfunctions corresponding to a degenerate eigenvalue at $t=0$. Let me elaborate. $\newcommand{\ip}[2]{\langle#1,#2\rangle}$ Let me drop the argument $(t)$ whenever $t=0$. First, the eigenvectors are normalized, yielding $$ \ip{\phi_i'}{\phi_i}=0. \tag{2} $$ Differentiating (1) with respect to $t$ at $t=0$ gives $$ H'\phi_i +H\phi_i' = \lambda_i'\phi_i +\lambda_i\phi_i'. \tag{3} $$ Taking inner product with $\phi_i$ and using symmetry and (2) gives $$ \lambda_i'=\ip{\phi_i}{H'\phi_i}. \tag{4} $$ These are the first order perturbations of the eigenvalues. I also want to find the first order perturbation of the eigenvectors, $\phi_i'$. Doing so is equivalent with finding the inner product $\ip{\phi_j}{\phi_i'}$ for all $j$. Taking inner product of (3) with $\phi_j$ for $j\neq i$ gives $$ \ip{\phi_j}{H'\phi_i} = (\lambda_i-\lambda_j)\ip{\phi_j}{\phi_i'}. $$ Thus if $\lambda_j\neq\lambda_i$, we obtain $$ \ip{\phi_j}{\phi_i'} = \frac{\ip{\phi_j}{H'\phi_i}}{\lambda_i-\lambda_j}. \tag{5} $$ On the other hand, if $\lambda_j=\lambda_i$, we obtain $\ip{\phi_j}{H'\phi_i}=0$. That is, the block of $H'$ corresponding to the eigenspace of $H$ of eigenvalue $\lambda_i$ is diagonal. We have freedom in choosing the orthonormal basis when the spectrum degenerates, and it must be chosen in this way to make $\phi_i(t)$ depend continuously on $t$. Therefore the diagonal values in (4) are in fact eigenvalues of this block matrix. For $i=j$ the desired inner product $\ip{\phi_j}{\phi_i'}$ is given by (2). If $i\neq j$ and $\lambda_i\neq\lambda_j$, it is given by (5). But what is the inner product $\ip{\phi_j}{\phi_i'}$ when $i\neq j$ and $\lambda_i=\lambda_j$? It seems to me that the presented calculations give no constraints. I have lost no information by taking the inner product of (3) with all $\phi_j$ and looking at all the inner products instead of the vector equation. I must be missing something. (This puzzled me for quite a while and I found no answer elsewhere, so I decided to share the question and an answer here. Actually, I figured it out while writing this question. Other answers are very much welcome!)",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'perturbation-theory']"
95,Why is interchanging rows or columns called an elementary operation on a matrix?,Why is interchanging rows or columns called an elementary operation on a matrix?,,"In my first programming course, I learnt how to swap two variables, suppose denoted by $x$ and $y$, without holding a value in a third variable. Run this code snippet in C. int x=5, y=6; x=x+y; y=x-y; x=x-y; if(!(x-6 || y-5)) printf(""Variables Swapped.""); So, as it shows, interchanging rows and columns can be achieved in exactly the same way, a series of scalar multiplications and addition. So, if the interchanging is essentially a composition of other operations, is it fair to call it an elementary operation?","In my first programming course, I learnt how to swap two variables, suppose denoted by $x$ and $y$, without holding a value in a third variable. Run this code snippet in C. int x=5, y=6; x=x+y; y=x-y; x=x-y; if(!(x-6 || y-5)) printf(""Variables Swapped.""); So, as it shows, interchanging rows and columns can be achieved in exactly the same way, a series of scalar multiplications and addition. So, if the interchanging is essentially a composition of other operations, is it fair to call it an elementary operation?",,"['linear-algebra', 'matrices', 'linear-transformations', 'matrix-equations']"
96,"Show an $n\times n$ matrix $A$ is sim. to the companion matrix for $p_A(t)\iff \exists$ a vector $x$ such that $x,Ax,\ldots,A^{n-1}x$ is a basis",Show an  matrix  is sim. to the companion matrix for  a vector  such that  is a basis,"n\times n A p_A(t)\iff \exists x x,Ax,\ldots,A^{n-1}x","Show that an $n\times n$ matrix $A$ is similar to the companion matrix for $p_A(t)$ if and only if there exists a vector $x$ such that $$x, Ax, \ldots, A^{n-1}x$$ is a basis for $\mathbb C^n$. The companion matrix for $p(t) = t^n + a_{n-1}t^{n-1} + \cdots + a_1t + a_0$ is given by $$A_p = [e_2 \;|\; e_3 \;|\; \cdots \;|\; e_n \;|\; -{\bf a}]\;\;,\;\; { \bf a} = (a_0, a_1, \ldots , a_{n-1})^T$$ My attempt: I tried to prove the backwards direction because I had a little bit of an idea on what to do... Because $x, Ax, \ldots, A^{n-1}x$ is a basis for $\mathbb C^n$, we know that $x, Ax, \ldots, A^{n-1}x$ spans $\mathbb C^n$ and is linearly independent with $n$ vectors. Hence, we may create an $n\times n$ matrix $S$ such that $$S = [x \;|\; Ax \;|\; \cdots \;|\; A^{n-1}x],$$ of which by Equivalent Conditions for a Nonsingular Matrix , we have that $S$ is invertible because its columns are linearly independent. So $S^{-1}$ exists. Questions: How can I show that $A = S^{-1}A_pS$? My method might be a lost cause unfortunately, so I apologize if this question is somewhat dumb. I could probably show it by multiplying out $S^{-1}AS$ and showing it equals $A$, but I'm not entirely sure what $S^{-1}$ looks like in terms of its columns I've defined. Thus, the more important question--What does the inverse of $S$ look like exactly given its columns? Is it even possible to say?","Show that an $n\times n$ matrix $A$ is similar to the companion matrix for $p_A(t)$ if and only if there exists a vector $x$ such that $$x, Ax, \ldots, A^{n-1}x$$ is a basis for $\mathbb C^n$. The companion matrix for $p(t) = t^n + a_{n-1}t^{n-1} + \cdots + a_1t + a_0$ is given by $$A_p = [e_2 \;|\; e_3 \;|\; \cdots \;|\; e_n \;|\; -{\bf a}]\;\;,\;\; { \bf a} = (a_0, a_1, \ldots , a_{n-1})^T$$ My attempt: I tried to prove the backwards direction because I had a little bit of an idea on what to do... Because $x, Ax, \ldots, A^{n-1}x$ is a basis for $\mathbb C^n$, we know that $x, Ax, \ldots, A^{n-1}x$ spans $\mathbb C^n$ and is linearly independent with $n$ vectors. Hence, we may create an $n\times n$ matrix $S$ such that $$S = [x \;|\; Ax \;|\; \cdots \;|\; A^{n-1}x],$$ of which by Equivalent Conditions for a Nonsingular Matrix , we have that $S$ is invertible because its columns are linearly independent. So $S^{-1}$ exists. Questions: How can I show that $A = S^{-1}A_pS$? My method might be a lost cause unfortunately, so I apologize if this question is somewhat dumb. I could probably show it by multiplying out $S^{-1}AS$ and showing it equals $A$, but I'm not entirely sure what $S^{-1}$ looks like in terms of its columns I've defined. Thus, the more important question--What does the inverse of $S$ look like exactly given its columns? Is it even possible to say?",,"['linear-algebra', 'matrices', 'solution-verification', 'companion-matrices', 'similar-matrices']"
97,A transpose B equals B transpose A?,A transpose B equals B transpose A?,,I am trying to learn about decision surfaces in Bayesian decision theory but my linear algebra is a little rusty. There is a derivation for a discriminant function that assumes normal densities for the classes. Each with their own mean and the same covariance (the covariance matrix is symmetric). At one point there is this expression: $g_i(\mathbf{x}) = -\dfrac{1}{2}(\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x} + \mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i - \mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i - \mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{x}) + \ln{P(w_i)}$ The $\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}$ is discarded because it has no impact. Then the expression is simplified as: $g_i(\mathbf{x}) = -\dfrac{1}{2}\mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i + (\mathbf{\Sigma}^{-1}\mathbf{\mu}_i)^T\mathbf{x} + \ln{P(w_i)}$ But how is $\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i = \mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{x}$? If we have $\mathbf{A} = \mathbf{x}$ and $\mathbf{B} = \mathbf{\Sigma}^{-1}\mathbf{\mu}_i$ then isn't this saying that $\mathbf{A}^T\mathbf{B} = \mathbf{B}^T\mathbf{A}$?,I am trying to learn about decision surfaces in Bayesian decision theory but my linear algebra is a little rusty. There is a derivation for a discriminant function that assumes normal densities for the classes. Each with their own mean and the same covariance (the covariance matrix is symmetric). At one point there is this expression: $g_i(\mathbf{x}) = -\dfrac{1}{2}(\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x} + \mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i - \mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i - \mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{x}) + \ln{P(w_i)}$ The $\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{x}$ is discarded because it has no impact. Then the expression is simplified as: $g_i(\mathbf{x}) = -\dfrac{1}{2}\mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i + (\mathbf{\Sigma}^{-1}\mathbf{\mu}_i)^T\mathbf{x} + \ln{P(w_i)}$ But how is $\mathbf{x}^T\mathbf{\Sigma}^{-1}\mathbf{\mu}_i = \mathbf{\mu}_i^T\mathbf{\Sigma}^{-1}\mathbf{x}$? If we have $\mathbf{A} = \mathbf{x}$ and $\mathbf{B} = \mathbf{\Sigma}^{-1}\mathbf{\mu}_i$ then isn't this saying that $\mathbf{A}^T\mathbf{B} = \mathbf{B}^T\mathbf{A}$?,,"['matrices', 'transpose']"
98,Square roots in ring of strictly upper triangular matrices,Square roots in ring of strictly upper triangular matrices,,"Let $k$ be field, $M_n(k)$ the algebra of $n\times n$ matrices over $k$, and let $N\subset M_n(k)$ be the subring of strictly upper triangular matrices in $M_n(k)$.  Note that every element of $N$ is nilpotent. Let $N^2=\{X^2|X\in N\}$, i.e. the set of matrices in $N$ with a square root in $N$.  My question is: can we describe $N^2$? I conjecture that it is $N^2$ is the set of all matrices with zeroes below the second main diagonal, i.e. $$ \begin{pmatrix}0 & 0 & * & \cdots & * \\\vdots & \ddots & \ddots & \ddots & \\ & & &\ddots & * \\ \vdots & & &  \ddots & 0 \\ 0 & \cdots & & \cdots & 0 \end{pmatrix} $$ where $*$ denotes any element of $k$. Thoughts/ideas?  Any help is appreciated!","Let $k$ be field, $M_n(k)$ the algebra of $n\times n$ matrices over $k$, and let $N\subset M_n(k)$ be the subring of strictly upper triangular matrices in $M_n(k)$.  Note that every element of $N$ is nilpotent. Let $N^2=\{X^2|X\in N\}$, i.e. the set of matrices in $N$ with a square root in $N$.  My question is: can we describe $N^2$? I conjecture that it is $N^2$ is the set of all matrices with zeroes below the second main diagonal, i.e. $$ \begin{pmatrix}0 & 0 & * & \cdots & * \\\vdots & \ddots & \ddots & \ddots & \\ & & &\ddots & * \\ \vdots & & &  \ddots & 0 \\ 0 & \cdots & & \cdots & 0 \end{pmatrix} $$ where $*$ denotes any element of $k$. Thoughts/ideas?  Any help is appreciated!",,"['linear-algebra', 'matrices', 'algebraic-geometry']"
99,Linear Algebra involving matrix equations and determinants,Linear Algebra involving matrix equations and determinants,,I'm studying for a linear algebra exam and one of the questions on a practice exam is as follows: For which positive integers n does there exist a matrix $A\in\mathbb{R}^{n\times n}$ such that $A^2+A+I=0$? Note: Here I is the identity matrix. I know how to solve it for $A^2 + I=0$ by rearranging it as $A^2=-I$ then taking the determinant of both sides to get $\det(A)^2=(-1)^n$ and because $\det(A)^2$ must be positive there is no solution for odd values of n . When I apply this technique to the above question I get: $A^2+A=-I\implies A(A+I)=-I\implies \det(A)\det(A+I)=(-1)^n$ but I'm stuck there because if $\det(A)<0$ then $\det(A+I)$ can be positive or negative so I can't use the same argument as above.  I've tried manipulating the equation in other ways and applying the determinant and haven't been able to come up with anything. Any help is appreciated!,I'm studying for a linear algebra exam and one of the questions on a practice exam is as follows: For which positive integers n does there exist a matrix $A\in\mathbb{R}^{n\times n}$ such that $A^2+A+I=0$? Note: Here I is the identity matrix. I know how to solve it for $A^2 + I=0$ by rearranging it as $A^2=-I$ then taking the determinant of both sides to get $\det(A)^2=(-1)^n$ and because $\det(A)^2$ must be positive there is no solution for odd values of n . When I apply this technique to the above question I get: $A^2+A=-I\implies A(A+I)=-I\implies \det(A)\det(A+I)=(-1)^n$ but I'm stuck there because if $\det(A)<0$ then $\det(A+I)$ can be positive or negative so I can't use the same argument as above.  I've tried manipulating the equation in other ways and applying the determinant and haven't been able to come up with anything. Any help is appreciated!,,"['linear-algebra', 'matrices', 'determinant', 'matrix-equations']"
