,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Rudin's RCA; Exercise 3.4 Part (b) Only,Rudin's RCA; Exercise 3.4 Part (b) Only,,"I'm working from Rudin's $Real~And~Complex~Analysis$ (3rd Ed.) on my own -- more specifically on Exercise 4 from Chapter 3 which is partially restated below (slightly modified). I only reference part (b) of the exercise since this is the only part I'm struggling with. For the rest of the problem (namely part (a) and parts (c), (d) & (e) of the exercise), refer to this link pointing to a free download of Rudin's book referenced above (see PG. 71 in his book), or see this post , etc. Please note that I'm working out of this book exclusively, and all concepts, definitions, theorems, etc., therein is what I'm working with. Exercise 3.4 Suppose $(X,\mathfrak{M},\mu)$ is a measure space, where $\mu$ is a positive measure, and suppose $f\!:\!X\!\rightarrow\!\mathbb{C}$ is a measurable function. At any $p_{0}\in(0,+\infty)$ we define $\varphi(p_{0}):=\big(\|f\|_{p_{0}}\big)^{p_{0}}\!\!={\displaystyle{\!\!\int_{X}|f|^{p_{0}}~\!d\mu}}$ , as well as we define the set $E:=\big\{\widetilde{p}\in(0,+\infty):\varphi(\!~{\widetilde{p}}\!~)\!<\!+\infty\big\}$ . We also assume that $\|f\|_{+\infty}\!>0$ . (b) Prove that $\log(\varphi)$ is convex on the interior of $E$ as well as that $\varphi$ is continuous on $E$ . I'm having difficulty showing that $\varphi$ is continuous at all points in $E$ . As far as my dilemma in particular, I graciously need help with an elaboration of the very last, strict, inequality found in this proof for part (b) only. The estimate in that proof gives continuity on all of $E$ (note also that my comment is at the bottom of that webpage also requesting the same elaboration, to which I haven't received a response as of yet, and I figured I'd come here to see what I can find in order to move on in the book overall). In case the link for this proof is broken (or breaks at some point in the future), I will provide the relevant part of the proof posted there below (also slightly modified) in regards to part (b) in particular. //Proof (b): Begin by letting $r,s\in E\subseteq\mathbb{R}^{+}$ where $r<s$ , and then fix $\lambda\in(0,1)$ arbitrarily in order to set $p=p_{\lambda}=(1-\lambda)r+\lambda s\in(r,s)$ . By appealing to H $\ddot{\text{o}}$ lder's inequality, we can now deduce that $\varphi(p)\leq\big(\varphi(r)\big)^{1-\lambda}\big(\varphi(s)\big)^{\lambda}\!<\!+\infty$ . We can use this inequality to show the first part of (b) -- we need $\varphi>0$ on the interior of $E$ , which is indeed the case since $\|f\|_{+\infty}\!>0$ (I showed this with a quick proof by contradiction), and so $\log(\varphi)$ is convex on the interior of $E$ yielding $\varphi$ is convex on the interior of $E$ and we can conclude $\varphi$ is continuous on the interior of $E$ . In order to show that $\varphi$ is continuous at all points in $E$ , we first let $\varepsilon>0$ , and then we can find an $N\in\mathbb{N}$ such that the set $E_{N}:=\big\{\widetilde{x}\in X:|f(\widetilde{x})|>N\big\}\cup\big\{\widetilde{x}\in X:0<|f(\widetilde{x})|<\frac{1}{N}\big\}$ has measure less than $\varepsilon$ -- namely, $0\leq\mu(E_{N})<\varepsilon$ , and this is indeed possible since $\|f\|_{+\infty}\!>0$ . This being said, choose a $\delta>0$ such that $0\leq\text{min}\big\{|1-N^{\delta}|,\big|1-\frac{1}{N^{\delta}}\big|\big\}<\varepsilon$ . Then, for any $x,y\in E$ where $|x-y|<\delta$ we estimate that: ${\displaystyle{|\varphi(x)-\varphi(y)|=\bigg|\int_{X}|f|^{x}-|f|^{y}d\mu\bigg|=\bigg|\int_{E_{N}~\sqcup~X\backslash E_{N}}\!\!\!\!\!\!|f|^{x}-|f|^{y}d\mu\bigg|}}$ ${\displaystyle{~~~~~~~~~~~~~~~~~~~~~\leq\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|+\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|}}$ ${\displaystyle{~~~~~~~~~~~~~~~~~~~~~<2\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ , which shows that $\varphi$ is continuous on $E$ . I can't seem to simplify everything properly in order to show the very last, strict inequality above ${\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|+\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<2\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ holds. I was hoping, not only if this is correct, but to determine how to establish this, since this actually shows $\varphi$ is uniformly continuous on $E$ implying continuity on $E$ (right [?] -- it looks correct as $\varphi(x)+\varphi(y)<+\infty$ whenever $x,y\in E$ ). I figured we need to take the first integral ${\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|}}$ and use our assumption that $\mu(E_{N})<\varepsilon$ in order to get ${\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ ; additionally, I figured our choice of $\delta>0$ will enable us to show ${\displaystyle{\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<\varepsilon\big(\varphi(x)+\varphi(y)\big)}}$ as well -- I think I can do this alone, but, if I'm correct, then my difficult arises with the first integral...I'm stumped. Any help is greatly appreciated!","I'm working from Rudin's (3rd Ed.) on my own -- more specifically on Exercise 4 from Chapter 3 which is partially restated below (slightly modified). I only reference part (b) of the exercise since this is the only part I'm struggling with. For the rest of the problem (namely part (a) and parts (c), (d) & (e) of the exercise), refer to this link pointing to a free download of Rudin's book referenced above (see PG. 71 in his book), or see this post , etc. Please note that I'm working out of this book exclusively, and all concepts, definitions, theorems, etc., therein is what I'm working with. Exercise 3.4 Suppose is a measure space, where is a positive measure, and suppose is a measurable function. At any we define , as well as we define the set . We also assume that . (b) Prove that is convex on the interior of as well as that is continuous on . I'm having difficulty showing that is continuous at all points in . As far as my dilemma in particular, I graciously need help with an elaboration of the very last, strict, inequality found in this proof for part (b) only. The estimate in that proof gives continuity on all of (note also that my comment is at the bottom of that webpage also requesting the same elaboration, to which I haven't received a response as of yet, and I figured I'd come here to see what I can find in order to move on in the book overall). In case the link for this proof is broken (or breaks at some point in the future), I will provide the relevant part of the proof posted there below (also slightly modified) in regards to part (b) in particular. //Proof (b): Begin by letting where , and then fix arbitrarily in order to set . By appealing to H lder's inequality, we can now deduce that . We can use this inequality to show the first part of (b) -- we need on the interior of , which is indeed the case since (I showed this with a quick proof by contradiction), and so is convex on the interior of yielding is convex on the interior of and we can conclude is continuous on the interior of . In order to show that is continuous at all points in , we first let , and then we can find an such that the set has measure less than -- namely, , and this is indeed possible since . This being said, choose a such that . Then, for any where we estimate that: , which shows that is continuous on . I can't seem to simplify everything properly in order to show the very last, strict inequality above holds. I was hoping, not only if this is correct, but to determine how to establish this, since this actually shows is uniformly continuous on implying continuity on (right [?] -- it looks correct as whenever ). I figured we need to take the first integral and use our assumption that in order to get ; additionally, I figured our choice of will enable us to show as well -- I think I can do this alone, but, if I'm correct, then my difficult arises with the first integral...I'm stumped. Any help is greatly appreciated!","Real~And~Complex~Analysis (X,\mathfrak{M},\mu) \mu f\!:\!X\!\rightarrow\!\mathbb{C} p_{0}\in(0,+\infty) \varphi(p_{0}):=\big(\|f\|_{p_{0}}\big)^{p_{0}}\!\!={\displaystyle{\!\!\int_{X}|f|^{p_{0}}~\!d\mu}} E:=\big\{\widetilde{p}\in(0,+\infty):\varphi(\!~{\widetilde{p}}\!~)\!<\!+\infty\big\} \|f\|_{+\infty}\!>0 \log(\varphi) E \varphi E \varphi E E r,s\in E\subseteq\mathbb{R}^{+} r<s \lambda\in(0,1) p=p_{\lambda}=(1-\lambda)r+\lambda s\in(r,s) \ddot{\text{o}} \varphi(p)\leq\big(\varphi(r)\big)^{1-\lambda}\big(\varphi(s)\big)^{\lambda}\!<\!+\infty \varphi>0 E \|f\|_{+\infty}\!>0 \log(\varphi) E \varphi E \varphi E \varphi E \varepsilon>0 N\in\mathbb{N} E_{N}:=\big\{\widetilde{x}\in X:|f(\widetilde{x})|>N\big\}\cup\big\{\widetilde{x}\in X:0<|f(\widetilde{x})|<\frac{1}{N}\big\} \varepsilon 0\leq\mu(E_{N})<\varepsilon \|f\|_{+\infty}\!>0 \delta>0 0\leq\text{min}\big\{|1-N^{\delta}|,\big|1-\frac{1}{N^{\delta}}\big|\big\}<\varepsilon x,y\in E |x-y|<\delta {\displaystyle{|\varphi(x)-\varphi(y)|=\bigg|\int_{X}|f|^{x}-|f|^{y}d\mu\bigg|=\bigg|\int_{E_{N}~\sqcup~X\backslash E_{N}}\!\!\!\!\!\!|f|^{x}-|f|^{y}d\mu\bigg|}} {\displaystyle{~~~~~~~~~~~~~~~~~~~~~\leq\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|+\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|}} {\displaystyle{~~~~~~~~~~~~~~~~~~~~~<2\varepsilon\big(\varphi(x)+\varphi(y)\big)}} \varphi E {\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|+\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<2\varepsilon\big(\varphi(x)+\varphi(y)\big)}} \varphi E E \varphi(x)+\varphi(y)<+\infty x,y\in E {\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|}} \mu(E_{N})<\varepsilon {\displaystyle{\bigg|\int_{E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<\varepsilon\big(\varphi(x)+\varphi(y)\big)}} \delta>0 {\displaystyle{\bigg|\int_{X\backslash E_{N}}|f|^{x}-|f|^{y}d\mu\bigg|<\varepsilon\big(\varphi(x)+\varphi(y)\big)}}","['analysis', 'measure-theory', 'continuity', 'lebesgue-integral', 'integral-inequality']"
1,Generalized version of L’Hospital’s rule?,Generalized version of L’Hospital’s rule?,,"I was wondering if the following inequalities are true:$\liminf_{x \to c} \frac{f'(x)}{g'(x)} \leq \liminf_{x \to c} \frac{f(x)}{g(x)} \leq \limsup_{x \to c} \frac{f(x)}{g(x)} \leq \limsup_{x \to c} \frac{f'(x)}{g'(x)}$ under some conditions on $f$ and $g$. Under assumptions for L'Hospital's rule, if $\lim_{x \to c} \frac{f'(x)}{g'(x)}$ exists, then the above inequalities imply the conclusion of L'Hospital's rule. I think the above inequalities are the generalized version of L'Hospital's rule if they are right. Is it true? Would you give me any comment about it? Thanks in advance.","I was wondering if the following inequalities are true:$\liminf_{x \to c} \frac{f'(x)}{g'(x)} \leq \liminf_{x \to c} \frac{f(x)}{g(x)} \leq \limsup_{x \to c} \frac{f(x)}{g(x)} \leq \limsup_{x \to c} \frac{f'(x)}{g'(x)}$ under some conditions on $f$ and $g$. Under assumptions for L'Hospital's rule, if $\lim_{x \to c} \frac{f'(x)}{g'(x)}$ exists, then the above inequalities imply the conclusion of L'Hospital's rule. I think the above inequalities are the generalized version of L'Hospital's rule if they are right. Is it true? Would you give me any comment about it? Thanks in advance.",,"['calculus', 'real-analysis', 'analysis']"
2,How can we prove that slopes increase in a convex function $f: \mathbb{R} \rightarrow \mathbb{R}$ from the definition?,How can we prove that slopes increase in a convex function  from the definition?,f: \mathbb{R} \rightarrow \mathbb{R},"The definition I have of a convex function $f: \mathbb{R} \rightarrow \mathbb{R}$ is that for every $x, y \in \mathbb{R}$ and every $\lambda \in [0, 1]$,  $$ f(\lambda x + (1-\lambda )y) \leq \lambda f(x) + (1- \lambda )f(y).$$ By proving that slopes increase I mean that for $x \leq y \leq z$, we get $$\frac{f(y) - f(x)}{y-x} \leq  \frac{f(z) - f(x)}{z-x} \leq \frac{f(z) - f(y)}{z-y}. $$ Is there a simple proof of this which doesn't assume that such a convex function has a non-negative second derivative? It's difficult to see how the definition gets us here.","The definition I have of a convex function $f: \mathbb{R} \rightarrow \mathbb{R}$ is that for every $x, y \in \mathbb{R}$ and every $\lambda \in [0, 1]$,  $$ f(\lambda x + (1-\lambda )y) \leq \lambda f(x) + (1- \lambda )f(y).$$ By proving that slopes increase I mean that for $x \leq y \leq z$, we get $$\frac{f(y) - f(x)}{y-x} \leq  \frac{f(z) - f(x)}{z-x} \leq \frac{f(z) - f(y)}{z-y}. $$ Is there a simple proof of this which doesn't assume that such a convex function has a non-negative second derivative? It's difficult to see how the definition gets us here.",,"['real-analysis', 'analysis']"
3,"Proving that if $a,b \in \mathbb{N}^*$ then $\frac{\pi a^n}{n!} \int ^{1}_{0}x^n(1-x)^n \sin (\pi x) dx \in \mathbb{N}$",Proving that if  then,"a,b \in \mathbb{N}^* \frac{\pi a^n}{n!} \int ^{1}_{0}x^n(1-x)^n \sin (\pi x) dx \in \mathbb{N}","I have an exercise, whose aim is to show that $\pi^2$ is irrational by contradiction. We suppose that $\frac{a}{b} = \pi ^2$ with $a,b \in \mathbb{N} ^*$. We put $$N_n = \frac{\pi a^n}{n!} \int ^{1}_{0}x^n(1-x)^n \sin (\pi x) dx$$ I was first asked to show that $N_n > 0 $ and that $\lim_{n\to\infty} N_n = 0$, which I did. Then I am asked to show that $\forall n \in \mathbb{N}, N_n \in \mathbb{N}$ and then conclude. I am asked to use integration by parts. By integrating twice, I find that $N_n$ is also equal to $$ \frac{\pi a^n}{(n-1)!} \int ^{1}_{0}x^n (1-x)^n \sin(\pi x) dx $$ which allows me to conclude that $$\frac{\pi a^n}{n!} = \frac{\pi a^n}{(n-1)!} $$ which is absurd. I guess at this point I managed to prove that $\pi ^2 $ is irrational, but I still want to know how could I proceed to show that $\forall n \mathbb{N}, N_n \in \mathbb{N} $?","I have an exercise, whose aim is to show that $\pi^2$ is irrational by contradiction. We suppose that $\frac{a}{b} = \pi ^2$ with $a,b \in \mathbb{N} ^*$. We put $$N_n = \frac{\pi a^n}{n!} \int ^{1}_{0}x^n(1-x)^n \sin (\pi x) dx$$ I was first asked to show that $N_n > 0 $ and that $\lim_{n\to\infty} N_n = 0$, which I did. Then I am asked to show that $\forall n \in \mathbb{N}, N_n \in \mathbb{N}$ and then conclude. I am asked to use integration by parts. By integrating twice, I find that $N_n$ is also equal to $$ \frac{\pi a^n}{(n-1)!} \int ^{1}_{0}x^n (1-x)^n \sin(\pi x) dx $$ which allows me to conclude that $$\frac{\pi a^n}{n!} = \frac{\pi a^n}{(n-1)!} $$ which is absurd. I guess at this point I managed to prove that $\pi ^2 $ is irrational, but I still want to know how could I proceed to show that $\forall n \mathbb{N}, N_n \in \mathbb{N} $?",,"['analysis', 'pi']"
4,Proving the inquality $\int_{0}^{\infty} f^\lambda(t)d(t^\lambda) \le \left(\int_{0}^{\infty} f(t)dt\right)^\lambda$ for $\lambda\ge 1$,Proving the inquality  for,\int_{0}^{\infty} f^\lambda(t)d(t^\lambda) \le \left(\int_{0}^{\infty} f(t)dt\right)^\lambda \lambda\ge 1,"I am actually reading an article where the authors used the following  whithout mentioning any proof of it $$\int_{0}^{\infty} f^\lambda(t)d(t^\lambda) \le \lambda\int_{0}^{\infty} \left(\int_0^t f(\tau)d\tau\right)^{\lambda -1}f(t) dt =  \left(\int_{0}^{\infty} f(t)dt\right)^\lambda $$ Where $\lambda\ge 1$, the expression $d(t^\lambda)$ stand for $\lambda t^{\lambda-1}dt$  and the function $f$ is a nonnegative nonincreasing and integrable function on $(0,\infty).$ My first Guess is that this inequality must be related to some convex inequalities (which I do not know): Because for $\lambda\ge 1$ the function $x\mapsto x^\lambda$ is convex and this function is very linked to this problem. Now the equality  $$\lambda\int_{0}^{\infty} \left(\int_0^t f(\tau)d\tau\right)^{\lambda -1}f(t) dt =  \left(\int_{0}^{\infty} f(t)dt\right)^\lambda $$ is rather easy to check since by setting  $$F(t) = \int_0^t f(\tau)d\tau$$ we end up with $$\lambda\int_{0}^{\infty} \left(\int_0^t f(\tau)d\tau\right)^{\lambda -1}f(t) dt = \lambda\int_{0}^{\infty} F(t)^{\lambda -1}F'(t) dt \\= F^\lambda(\infty)-F^\lambda(0)=  \left(\int_{0}^{\infty} f(t)dt\right)^\lambda $$ Any reference, idea,  help or proposal is very welcome.","I am actually reading an article where the authors used the following  whithout mentioning any proof of it $$\int_{0}^{\infty} f^\lambda(t)d(t^\lambda) \le \lambda\int_{0}^{\infty} \left(\int_0^t f(\tau)d\tau\right)^{\lambda -1}f(t) dt =  \left(\int_{0}^{\infty} f(t)dt\right)^\lambda $$ Where $\lambda\ge 1$, the expression $d(t^\lambda)$ stand for $\lambda t^{\lambda-1}dt$  and the function $f$ is a nonnegative nonincreasing and integrable function on $(0,\infty).$ My first Guess is that this inequality must be related to some convex inequalities (which I do not know): Because for $\lambda\ge 1$ the function $x\mapsto x^\lambda$ is convex and this function is very linked to this problem. Now the equality  $$\lambda\int_{0}^{\infty} \left(\int_0^t f(\tau)d\tau\right)^{\lambda -1}f(t) dt =  \left(\int_{0}^{\infty} f(t)dt\right)^\lambda $$ is rather easy to check since by setting  $$F(t) = \int_0^t f(\tau)d\tau$$ we end up with $$\lambda\int_{0}^{\infty} \left(\int_0^t f(\tau)d\tau\right)^{\lambda -1}f(t) dt = \lambda\int_{0}^{\infty} F(t)^{\lambda -1}F'(t) dt \\= F^\lambda(\infty)-F^\lambda(0)=  \left(\int_{0}^{\infty} f(t)dt\right)^\lambda $$ Any reference, idea,  help or proposal is very welcome.",,"['calculus', 'real-analysis', 'analysis', 'inequality', 'integral-inequality']"
5,"Is the set $\{2, 3, 4\}$ open in some metric spaces and not open in others?",Is the set  open in some metric spaces and not open in others?,"\{2, 3, 4\}","I just want to check my understanding. This is from Baby Rudin: 2.18 Definition Let $X$ be a metric space. All points and sets mentioned below are understood to be elements and subsets of $X$ . $(a)$ A neighborhood of $p$ is a set $N_r(p)$ consisting of all $q$ such that $d(p, q)<r$ for some $r>0$ . The number $r$ is called the radius of $N_r( p)$ $(e)$ A point $p$ is an interior point of $E$ if there is a neighborhood $N$ of $p$ such that $N \subset E$ $(f)$ $E$ is open if every point of $E$ is an interior point of $E$ . Suppose we have the metric space with set $X=\{1, 2, 3, 4, 5\}$ and distance function $d(x, y)=|x-y|$ . Now $2$ is an interior point of $\{2, 3, 4\}$ because $N_{0.5}(2)=\{2\} \subset \{2, 3, 4\}$ (and a similar argument can be made for $3$ and $4$ as well. But if our metric space is $\mathbb{R}$ with the same distance function, then $\{2, 3, 4\}$ is not open because no neighborhood of $2$ is a subset of $\{2, 3, 4\}$ , so $2$ is not an interior point of $\{2, 3, 4\}$ , right?","I just want to check my understanding. This is from Baby Rudin: 2.18 Definition Let be a metric space. All points and sets mentioned below are understood to be elements and subsets of . A neighborhood of is a set consisting of all such that for some . The number is called the radius of A point is an interior point of if there is a neighborhood of such that is open if every point of is an interior point of . Suppose we have the metric space with set and distance function . Now is an interior point of because (and a similar argument can be made for and as well. But if our metric space is with the same distance function, then is not open because no neighborhood of is a subset of , so is not an interior point of , right?","X X (a) p N_r(p) q d(p, q)<r r>0 r N_r( p) (e) p E N p N \subset E (f) E E E X=\{1, 2, 3, 4, 5\} d(x, y)=|x-y| 2 \{2, 3, 4\} N_{0.5}(2)=\{2\} \subset \{2, 3, 4\} 3 4 \mathbb{R} \{2, 3, 4\} 2 \{2, 3, 4\} 2 \{2, 3, 4\}","['real-analysis', 'analysis']"
6,How does one find a polynomial approximation of a non-analytic function?,How does one find a polynomial approximation of a non-analytic function?,,"I have a function $$f(x) =  \left\{      \begin{array}{lr}        0 &  0 \leq x < 1/3\\        q(x) &  1/3 \leq x <  2/3 \\        1 &  2/3\leq x \leq 1      \end{array}    \right.\\ $$ where $q(x)$ is some analytic function that interpolates between the points $x=1/3, 2/3$ and matches the first derivative at these points (say a spline fit). I want to approximate this with a polynomial (as Weierstrauss' theorem says I can) and want to understand how the error falls off as I increase the degree of the approximating polynomial, $p_n(x)$, where the error is defined as $\epsilon_n = \sup|f(x)-p_n(x)|$ . My initial idea was to choose set of orthogonal polynomials (say Legendre polynomials, $l_m(x)$) and then try express $f(x) \approx \sum_{m=1}^n a_m l_m(x)$. The error can then be bounded by the part of the series that has been cut off, and we should be able to bound this. However, $f(x)$ is not analytic and hence there I have no idea if extracting the $a_m$ is possible in the conventional way of $$a_m = \int_0^1 f(x)l_m(x) dx $$. is this is a valid method of extracting the coefficients $a_m$ given $f(x)$ is not analytic? If not, is there a way to go about it? More generally, is there a better way of find out how the error scales with the degree of the approximating polynomial?","I have a function $$f(x) =  \left\{      \begin{array}{lr}        0 &  0 \leq x < 1/3\\        q(x) &  1/3 \leq x <  2/3 \\        1 &  2/3\leq x \leq 1      \end{array}    \right.\\ $$ where $q(x)$ is some analytic function that interpolates between the points $x=1/3, 2/3$ and matches the first derivative at these points (say a spline fit). I want to approximate this with a polynomial (as Weierstrauss' theorem says I can) and want to understand how the error falls off as I increase the degree of the approximating polynomial, $p_n(x)$, where the error is defined as $\epsilon_n = \sup|f(x)-p_n(x)|$ . My initial idea was to choose set of orthogonal polynomials (say Legendre polynomials, $l_m(x)$) and then try express $f(x) \approx \sum_{m=1}^n a_m l_m(x)$. The error can then be bounded by the part of the series that has been cut off, and we should be able to bound this. However, $f(x)$ is not analytic and hence there I have no idea if extracting the $a_m$ is possible in the conventional way of $$a_m = \int_0^1 f(x)l_m(x) dx $$. is this is a valid method of extracting the coefficients $a_m$ given $f(x)$ is not analytic? If not, is there a way to go about it? More generally, is there a better way of find out how the error scales with the degree of the approximating polynomial?",,"['real-analysis', 'analysis', 'approximation', 'approximation-theory']"
7,Abstract concept tying real numbers to elementary functions?,Abstract concept tying real numbers to elementary functions?,,"Real numbers can be broken into two categories: rational vs. irrational. Irrational numbers can be approximated, but never fully represented by rational numbers. Analytic functions have Taylor polynomial representations. I know that polynomials have finite taylor expansions (trivially), while transcendental functions have infinite taylor expansions, which we truncate by necessity. So it seems again we have an easy-to-represent function (polynomials) with can be added infinitely to represent (or finitely-many times to approximate) an analytic function. Similarly, other functions have Laurent expansions, and I can categorize them as having finite vs. infinite expansions… I sense there’s a deeper abstraction here. Having read the first two chapters of an Abstract Algebra textbook, the word “isomorphism” comes to mind. From Analysis 1 I also know that these objects can be used to form metric spaces. Is there a deeper pattern here? For example: real numbers can be added or multiplied to obtain other reals, and polynomials can be added or multiplied to obtain other polynomials (and from there, transcendentals). This reminds me of the field properties…","Real numbers can be broken into two categories: rational vs. irrational. Irrational numbers can be approximated, but never fully represented by rational numbers. Analytic functions have Taylor polynomial representations. I know that polynomials have finite taylor expansions (trivially), while transcendental functions have infinite taylor expansions, which we truncate by necessity. So it seems again we have an easy-to-represent function (polynomials) with can be added infinitely to represent (or finitely-many times to approximate) an analytic function. Similarly, other functions have Laurent expansions, and I can categorize them as having finite vs. infinite expansions… I sense there’s a deeper abstraction here. Having read the first two chapters of an Abstract Algebra textbook, the word “isomorphism” comes to mind. From Analysis 1 I also know that these objects can be used to form metric spaces. Is there a deeper pattern here? For example: real numbers can be added or multiplied to obtain other reals, and polynomials can be added or multiplied to obtain other polynomials (and from there, transcendentals). This reminds me of the field properties…",,"['abstract-algebra', 'analysis', 'functions', 'real-numbers']"
8,Finding a positive continuous function,Finding a positive continuous function,,"Currently I'm studying inverse problems and I have this question. I just don't realize how this is related to inverse problems and I also have no clue for the second part. I appreciate any help. Is there a positive continuous function $f$ such that $$\int_0^1f(x)dx=1$$ $$\int_0^1xf(x)dx=\alpha,\text{and} \int_0^1x^2f(x)=\alpha^2,$$ Where is $\alpha$ a positive number? Using the integration by parts, I get $\alpha=0$ , so there is no such function. In the next part I'm asked to find the answer if the function is being replaced by a measure, which I have no clue about it.","Currently I'm studying inverse problems and I have this question. I just don't realize how this is related to inverse problems and I also have no clue for the second part. I appreciate any help. Is there a positive continuous function such that Where is a positive number? Using the integration by parts, I get , so there is no such function. In the next part I'm asked to find the answer if the function is being replaced by a measure, which I have no clue about it.","f \int_0^1f(x)dx=1 \int_0^1xf(x)dx=\alpha,\text{and} \int_0^1x^2f(x)=\alpha^2, \alpha \alpha=0","['analysis', 'measure-theory', 'inverse-problems']"
9,Show that $\left|\int_{-h}^hf(x)\mathrm dx-\frac{h}3(f(-h)+f(h)+4f(0))\right|\le\frac{h^5}{90}\|f^{(4)}\|_\infty$,Show that,\left|\int_{-h}^hf(x)\mathrm dx-\frac{h}3(f(-h)+f(h)+4f(0))\right|\le\frac{h^5}{90}\|f^{(4)}\|_\infty,"Suppose that $f\in C^4([-h,h],\Bbb R)$ with $h>0$ . Show that $$\left|\int_{-h}^hf(x)\mathrm dx-\frac{h}3(f(-h)+f(h)+4f(0))\right|\le\frac{h^5}{90}\|f^{(4)}\|_\infty$$ HINT: integrate by parts and after use the mean value theorem for integrals. I tried different approaches with no results, by example I integrate by parts, prior change of variable, using the Bernoulli polynomials and tried to setup something using two different versions of the MVT for integrals. I tried other more direct approaches without the Bernoulli polynomials with no result. The two versions of the MVT for integrals that I know are these: MVT for integrals (version I). Let $f,g\in C(I,\Bbb R)$ with $g\ge 0$ , then $$\int_\alpha^\beta f(x)g(x)\mathrm dx=f(\xi)\int_\alpha^\beta g(x)\mathrm dx,\quad \xi\in[\alpha,\beta]$$ MVT for integrals (version II). Let $f\in C(I,\Bbb R)$ and $g\in C^1(I,\Bbb R)$ monotone, then $$\int_\alpha^\beta f(x)g(x)\mathrm dx=g(\alpha)\int_\alpha^\xi f(x)\mathrm dx+g(\beta)\int_\xi^\beta f(x)\mathrm dx,\quad \xi\in[\alpha,\beta]$$ Some help will be appreciated, thank you.","Suppose that with . Show that HINT: integrate by parts and after use the mean value theorem for integrals. I tried different approaches with no results, by example I integrate by parts, prior change of variable, using the Bernoulli polynomials and tried to setup something using two different versions of the MVT for integrals. I tried other more direct approaches without the Bernoulli polynomials with no result. The two versions of the MVT for integrals that I know are these: MVT for integrals (version I). Let with , then MVT for integrals (version II). Let and monotone, then Some help will be appreciated, thank you.","f\in C^4([-h,h],\Bbb R) h>0 \left|\int_{-h}^hf(x)\mathrm dx-\frac{h}3(f(-h)+f(h)+4f(0))\right|\le\frac{h^5}{90}\|f^{(4)}\|_\infty f,g\in C(I,\Bbb R) g\ge 0 \int_\alpha^\beta f(x)g(x)\mathrm dx=f(\xi)\int_\alpha^\beta g(x)\mathrm dx,\quad \xi\in[\alpha,\beta] f\in C(I,\Bbb R) g\in C^1(I,\Bbb R) \int_\alpha^\beta f(x)g(x)\mathrm dx=g(\alpha)\int_\alpha^\xi f(x)\mathrm dx+g(\beta)\int_\xi^\beta f(x)\mathrm dx,\quad \xi\in[\alpha,\beta]","['analysis', 'approximation-theory']"
10,"Proof: If a function is in the Schwartz Space, then this function is uniformly continuous","Proof: If a function is in the Schwartz Space, then this function is uniformly continuous",,"I don't have this really clear, I want to justify if $f\in\mathcal{S}(\mathbb{R})$ then $f$ is uniformly continuous. So far, I know how can I bound $|x|$ for $f$ is in the Schwartz space, but I can't proceed with the uniformly continuous proof because I don't know how to bound $|y-x|$ to find a $\delta$ which depends on an $\varepsilon>0$ such as $|f(y)-f(x)|<\varepsilon$. Thank you so much","I don't have this really clear, I want to justify if $f\in\mathcal{S}(\mathbb{R})$ then $f$ is uniformly continuous. So far, I know how can I bound $|x|$ for $f$ is in the Schwartz space, but I can't proceed with the uniformly continuous proof because I don't know how to bound $|y-x|$ to find a $\delta$ which depends on an $\varepsilon>0$ such as $|f(y)-f(x)|<\varepsilon$. Thank you so much",,"['analysis', 'continuity', 'fourier-analysis', 'uniform-continuity', 'schwartz-space']"
11,Prove the following series converges,Prove the following series converges,,"Let $u, v, w$ be real numbers such that $u+v+w=0$. Suppose that $\{b_k\colon k=0,1,2,\dots\}$ is a sequence of real numbers such that $\lim_{k\to\infty} b_k=0$. For $k=0,1,2,\dots$ define $a_{3k}=u b_k,$ $a_{3k+1}= v b_k,$  $a_{3k+2} = w b_k.$ Prove that the series $ \sum_{n=0}^{\infty} a_n$ converges. Im having a thouhg problem with this one, any idea I will appreciate.","Let $u, v, w$ be real numbers such that $u+v+w=0$. Suppose that $\{b_k\colon k=0,1,2,\dots\}$ is a sequence of real numbers such that $\lim_{k\to\infty} b_k=0$. For $k=0,1,2,\dots$ define $a_{3k}=u b_k,$ $a_{3k+1}= v b_k,$  $a_{3k+2} = w b_k.$ Prove that the series $ \sum_{n=0}^{\infty} a_n$ converges. Im having a thouhg problem with this one, any idea I will appreciate.",,['analysis']
12,"Two definitions of Lebesgue integral, are they equivalent?","Two definitions of Lebesgue integral, are they equivalent?",,"So far, I have encountered two main definitions for the Lebesgue integral of a non-negative measurable function $f$. 1) $\int_A f d \mu = \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \}$ 2) $\int_A f d \mu = \sup \{ \inf _{x_i \in E_i} \sum_{i=1}^N  f(x_i) \mu(E_i) \} = \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$ where in 2), the $E_i$'s form a partition of A and the sup is taken   over all such partitions. I want to prove that they are equivalent. Here is what I did. 1) $\leq$ 2) : take $h$ a simple function such that $h \leq f$. Write $h$ in its canonical form : $h = \sum_i c_i \mathcal{1}_{A_i}$. Since $h \leq f$, we have $c_i \leq f(x_i)$ for all $x_i \in A_i$. Using the definition of the Lebesgue integral for simple functions, we then have $\int_A h d \mu = \sum_i c_i \mu (A_i) \leq \sum_{i=1}^N (\inf _{x_i \in A_i} f(x_i)) \mu(A_i) \leq \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$, from which we have $\sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \} \leq \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$. 2) $\leq$ 1) : let $E_i$'s be a partition of A, and $\alpha = \sum_i \inf_{x_i \in E_i} \{f(x_i) \} \mu (E_i)$. Fix $\epsilon > 0$. We can choose ($x_i^*)_i$ such that $\inf f(x_i) \leq f(x_i^*) \leq \inf f(x_i) + \epsilon$. We have $\alpha \leq \sum _i f(x_i^*) \mu (E_i)$. We set $h^* = \sum _i (f(x_i^*) - \epsilon) \mathcal{1}_{E_i}$. Then $h^* = \sum _i (f(x_i^*) - \epsilon) \mathcal{1}_{E_i} \leq \sum _i (\inf_{x_i \in E_i} f(x_i)) \mathcal{1}_{E_i} \leq f$. We also have $\int_A h^* d \mu = \sum_i (f(x_i^*) - \epsilon) \mu (E_i) = \sum_i (f(x_i^*) \mu (E_i) - \epsilon \mu (A) $.  So $\alpha \leq \int_A h^* d \mu + \epsilon \mu(A) \leq \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \} + \epsilon \mu (A)$. Take $\epsilon \to 0$ so that $\alpha \leq \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \}$ and finally $\sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \} \leq  \sup _{h \leq f, h simple} \{ \int_A h d \mu \}$. Is it correct ? Can ce generalize the second part to the case $\mu(A) = \infty$ ? If this is correct, in which case the definition 2) is preferably used ? Thanks.","So far, I have encountered two main definitions for the Lebesgue integral of a non-negative measurable function $f$. 1) $\int_A f d \mu = \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \}$ 2) $\int_A f d \mu = \sup \{ \inf _{x_i \in E_i} \sum_{i=1}^N  f(x_i) \mu(E_i) \} = \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$ where in 2), the $E_i$'s form a partition of A and the sup is taken   over all such partitions. I want to prove that they are equivalent. Here is what I did. 1) $\leq$ 2) : take $h$ a simple function such that $h \leq f$. Write $h$ in its canonical form : $h = \sum_i c_i \mathcal{1}_{A_i}$. Since $h \leq f$, we have $c_i \leq f(x_i)$ for all $x_i \in A_i$. Using the definition of the Lebesgue integral for simple functions, we then have $\int_A h d \mu = \sum_i c_i \mu (A_i) \leq \sum_{i=1}^N (\inf _{x_i \in A_i} f(x_i)) \mu(A_i) \leq \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$, from which we have $\sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \} \leq \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$. 2) $\leq$ 1) : let $E_i$'s be a partition of A, and $\alpha = \sum_i \inf_{x_i \in E_i} \{f(x_i) \} \mu (E_i)$. Fix $\epsilon > 0$. We can choose ($x_i^*)_i$ such that $\inf f(x_i) \leq f(x_i^*) \leq \inf f(x_i) + \epsilon$. We have $\alpha \leq \sum _i f(x_i^*) \mu (E_i)$. We set $h^* = \sum _i (f(x_i^*) - \epsilon) \mathcal{1}_{E_i}$. Then $h^* = \sum _i (f(x_i^*) - \epsilon) \mathcal{1}_{E_i} \leq \sum _i (\inf_{x_i \in E_i} f(x_i)) \mathcal{1}_{E_i} \leq f$. We also have $\int_A h^* d \mu = \sum_i (f(x_i^*) - \epsilon) \mu (E_i) = \sum_i (f(x_i^*) \mu (E_i) - \epsilon \mu (A) $.  So $\alpha \leq \int_A h^* d \mu + \epsilon \mu(A) \leq \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \} + \epsilon \mu (A)$. Take $\epsilon \to 0$ so that $\alpha \leq \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \}$ and finally $\sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \} \leq  \sup _{h \leq f, h simple} \{ \int_A h d \mu \}$. Is it correct ? Can ce generalize the second part to the case $\mu(A) = \infty$ ? If this is correct, in which case the definition 2) is preferably used ? Thanks.",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
13,Use MVT to prove that $1-\cos x<x^2$ for $x\neq 0$,Use MVT to prove that  for,1-\cos x<x^2 x\neq 0,"Use MVT to prove that $1-\cos x<x^2$, $x\neq 0$ $$1-\cos x<x^2\implies 1<x^2+\cos x$$ Let $f(x)=x^2+\cos x$, Notice that $f(-x)=(-x)^2+\cos(-x)=x^2+\cos(x)=f(x) \implies f$ is symmetric about the y-axis. It suffices to show that $x^2+\cos x>1$ for $x>0$ due to this symmetry. $x>0 \implies (0,x)$ Since $x^2$ and $\cos x$ are differentiable (and thus continuous) $\forall x\in \mathbb R$, MVT yields that in $(0,x)$, $\exists c \in (0,x)$ such that $$\frac{(x^2+\cos x)-(0^2-\cos 0)}{x-0}=2c-\sin c$$ $$\frac{x^2+\cos x-1}{x}=2c-\sin c$$ $-1\le\sin c\le1 \implies -1\le-\sin c\le1 \implies 2c-1\le 2c-\sin c\le 2c+1$ $$\frac{x^2+\cos x-1}{x}\ge 2c-1$$ And now i'm stuck because i can't get $x(2c-1)>0$ Is my proof wrong somewhere? UPDATE: Thanks for everyone that gave their input. I managed to solve it using $f(x) = 1-\cos x$ directly. However, i'm still curious as to why my initial method fails? is there something that I overlooked?","Use MVT to prove that $1-\cos x<x^2$, $x\neq 0$ $$1-\cos x<x^2\implies 1<x^2+\cos x$$ Let $f(x)=x^2+\cos x$, Notice that $f(-x)=(-x)^2+\cos(-x)=x^2+\cos(x)=f(x) \implies f$ is symmetric about the y-axis. It suffices to show that $x^2+\cos x>1$ for $x>0$ due to this symmetry. $x>0 \implies (0,x)$ Since $x^2$ and $\cos x$ are differentiable (and thus continuous) $\forall x\in \mathbb R$, MVT yields that in $(0,x)$, $\exists c \in (0,x)$ such that $$\frac{(x^2+\cos x)-(0^2-\cos 0)}{x-0}=2c-\sin c$$ $$\frac{x^2+\cos x-1}{x}=2c-\sin c$$ $-1\le\sin c\le1 \implies -1\le-\sin c\le1 \implies 2c-1\le 2c-\sin c\le 2c+1$ $$\frac{x^2+\cos x-1}{x}\ge 2c-1$$ And now i'm stuck because i can't get $x(2c-1)>0$ Is my proof wrong somewhere? UPDATE: Thanks for everyone that gave their input. I managed to solve it using $f(x) = 1-\cos x$ directly. However, i'm still curious as to why my initial method fails? is there something that I overlooked?",,"['calculus', 'analysis', 'trigonometry', 'inequality']"
14,Showing a set with a supremum norm is a Banach space,Showing a set with a supremum norm is a Banach space,,"Problem Statement: Let $X$ be a set and $E$ , a Banach space with norm $||\cdot||_{E}$ . Let $B_{E}(X)$ denote the set of all bounded functions $f:X\rightarrow E$ with the supremum norm $||f||=\sup_{x\in X}||f(x)||_{E}$ . Show that $B_{E}(X)$ is a Banach space (i.e. show that it is complete in this norm). I am just now starting in my first course in Real Analysis, and we have just been given our first homework, and I am already quite lost in how to approach these types of proofs. (I have taken an Abstract Algebra sequence in which I became very comfortable with the material and styles of proofs, but Analysis already seems to be much more daunting.) I have these definitions to help me: A Banach space is a complete normed vector space. A complete normed vector space is a normed vector space in which Cauchy sequences converge. A sequence $(v_{n})_{n\geq 1}$ in a normed vector space $E$ is a Cauchy sequence if $\forall \epsilon > 0$ , $\exists N\in \mathbb{N}$ $(m,n\geq N)$ such that $||v_{n}-v_{m}||<\epsilon$ . I am assuming that the norm $||\cdot||_{E}$ in the problem is referring to the standard Euclidean norm. So basically I need to show that an arbitrary Cauchy sequence of functions in $B_{E}(X)$ converges? My professor did a proof in class that $B_{\mathbb{R}}(X)$ , the space of bounded functions from a set $X$ to $\mathbb{R}$ , with the supremum norm, is complete. He began letting an arbitrary sequence $(f_{n})$ be Cauchy, and then showed that the $\lim f_{n} = f_{0}$ for some (I assume finite) $f_{0}$ . But many of the steps were confusing to me, especially considering it was my first day in Analysis. Any suggestions on how to approach this problem would be greatly appreciated!","Problem Statement: Let be a set and , a Banach space with norm . Let denote the set of all bounded functions with the supremum norm . Show that is a Banach space (i.e. show that it is complete in this norm). I am just now starting in my first course in Real Analysis, and we have just been given our first homework, and I am already quite lost in how to approach these types of proofs. (I have taken an Abstract Algebra sequence in which I became very comfortable with the material and styles of proofs, but Analysis already seems to be much more daunting.) I have these definitions to help me: A Banach space is a complete normed vector space. A complete normed vector space is a normed vector space in which Cauchy sequences converge. A sequence in a normed vector space is a Cauchy sequence if , such that . I am assuming that the norm in the problem is referring to the standard Euclidean norm. So basically I need to show that an arbitrary Cauchy sequence of functions in converges? My professor did a proof in class that , the space of bounded functions from a set to , with the supremum norm, is complete. He began letting an arbitrary sequence be Cauchy, and then showed that the for some (I assume finite) . But many of the steps were confusing to me, especially considering it was my first day in Analysis. Any suggestions on how to approach this problem would be greatly appreciated!","X E ||\cdot||_{E} B_{E}(X) f:X\rightarrow E ||f||=\sup_{x\in X}||f(x)||_{E} B_{E}(X) (v_{n})_{n\geq 1} E \forall \epsilon > 0 \exists N\in \mathbb{N} (m,n\geq N) ||v_{n}-v_{m}||<\epsilon ||\cdot||_{E} B_{E}(X) B_{\mathbb{R}}(X) X \mathbb{R} (f_{n}) \lim f_{n} = f_{0} f_{0}","['real-analysis', 'analysis', 'banach-spaces', 'normed-spaces', 'proof-explanation']"
15,Existence of a homeomorphism that does not return much,Existence of a homeomorphism that does not return much,,"Let $f:X\rightarrow X$ a homeomorphism where $X$ is a compact metric space. Fix $x\in X$, denote $O(f,x)=\{ f^n(x):n\in \mathbb{Z}\}$ the orbit of $f$ by $x$. For $m\in \mathbb{N}$ denote $O(f,x,m)=\{ f^j(x): \vert j\vert \leq m\}$ and $\#(A)$ is the cardinal of $A$. I am interested in the existence of an example of $f$ such that: there is $\delta>0$ with the following property $$\displaystyle{\lim_{m\to\infty}}\frac{\#(B[z,\delta]\cap O(f,x,m))}{\#(O(f,x,m))}=0$$ where $z\in \overline{O(f,x)}\setminus O(f,x)$ and $B[z,\delta]$ is the ball closed. I appreciate if you could give me some suggestion to know if such homeomorphism exists.","Let $f:X\rightarrow X$ a homeomorphism where $X$ is a compact metric space. Fix $x\in X$, denote $O(f,x)=\{ f^n(x):n\in \mathbb{Z}\}$ the orbit of $f$ by $x$. For $m\in \mathbb{N}$ denote $O(f,x,m)=\{ f^j(x): \vert j\vert \leq m\}$ and $\#(A)$ is the cardinal of $A$. I am interested in the existence of an example of $f$ such that: there is $\delta>0$ with the following property $$\displaystyle{\lim_{m\to\infty}}\frac{\#(B[z,\delta]\cap O(f,x,m))}{\#(O(f,x,m))}=0$$ where $z\in \overline{O(f,x)}\setminus O(f,x)$ and $B[z,\delta]$ is the ball closed. I appreciate if you could give me some suggestion to know if such homeomorphism exists.",,"['real-analysis', 'analysis', 'metric-spaces', 'dynamical-systems']"
16,Almost periodic function vs quasi periodic function,Almost periodic function vs quasi periodic function,,I am doing some work regarding quasiperiodic functions but I am not able to figure out the difference between almost periodic and quasiperiodic functions. Can anyone let me know about it?,I am doing some work regarding quasiperiodic functions but I am not able to figure out the difference between almost periodic and quasiperiodic functions. Can anyone let me know about it?,,"['analysis', 'special-functions', 'classical-mechanics', 'quasiperiodic-function']"
17,"$\int_{\Omega} |f_n-f||f_n| \, d \mu \to 0$ if $f_n \in L^1(\Omega)$, $f_n \to f$.","if , .","\int_{\Omega} |f_n-f||f_n| \, d \mu \to 0 f_n \in L^1(\Omega) f_n \to f","Suppose $f_n \to f$ in $L^1(\Omega)$ where $\mu(\Omega)=1$. Suppose $$\int_{\Omega} |f_n| \, d\mu \leq M$$ for all $n$. Is there a way to show that the integral $$\int_{\Omega} |f_n-f||f_n| \, d \mu \to 0$$  as well? I was thinking of defining a set $A_N$ where given $N$, $A(N) := \{ x \in \Omega : f_n (x) \geq N \, \text{for any} \,n \in \mathbb N \}$ and showing that $$\int_{\Omega} |f_n-f||f_n| \, d \mu \leq N\int_{\Omega \setminus A(N)} |f_n-f| \, d \mu + \int_{ A(N)} |f_n-f||f_n| \, d \mu$$ goes to $0$, but I'm not sure if I can show this. The thought is that $\mu(A(N))$ must get arbitrarily small as $N$ increases. Ideas? Hints? Could I use the fact that if $h \in L^1$, then there exists $\delta$ so that if $\mu(E)<\delta$ then $\|h\|_1 <\epsilon$.","Suppose $f_n \to f$ in $L^1(\Omega)$ where $\mu(\Omega)=1$. Suppose $$\int_{\Omega} |f_n| \, d\mu \leq M$$ for all $n$. Is there a way to show that the integral $$\int_{\Omega} |f_n-f||f_n| \, d \mu \to 0$$  as well? I was thinking of defining a set $A_N$ where given $N$, $A(N) := \{ x \in \Omega : f_n (x) \geq N \, \text{for any} \,n \in \mathbb N \}$ and showing that $$\int_{\Omega} |f_n-f||f_n| \, d \mu \leq N\int_{\Omega \setminus A(N)} |f_n-f| \, d \mu + \int_{ A(N)} |f_n-f||f_n| \, d \mu$$ goes to $0$, but I'm not sure if I can show this. The thought is that $\mu(A(N))$ must get arbitrarily small as $N$ increases. Ideas? Hints? Could I use the fact that if $h \in L^1$, then there exists $\delta$ so that if $\mu(E)<\delta$ then $\|h\|_1 <\epsilon$.",,"['real-analysis', 'analysis', 'measure-theory', 'proof-writing', 'lebesgue-integral']"
18,Projection of space onto unit sphere - differentiating it!,Projection of space onto unit sphere - differentiating it!,,"I am trying to differentiate the function that projects each $x\in  \mathbb{R}^3$ onto the unit sphere, namely: $$f(x)=\frac{x}{||x||},\quad f(0)=0$$ I know that the derivative is $$Df\big|_{x}(h)=\frac{x\times (h\times x)}{||x||^3}=\frac{h}{||x||}-\frac{x(x\cdot h)}{||x||^3}$$ But I am having a lot of trouble proving it. I can reduce it to: $$f(x+h)-f(x)-Df\big|_x(h)=x\left(\frac{1}{||x+h||}-\frac{1}{||x||}+\frac{(x\cdot h)}{||x||^3}\right)+o(h)$$ I am struggling to show that the expression in brackets is $o(h)$. It looks like the $||x+h||^{-1}$ is begging for expansion but I'm not sure how to deal with it because they are vectors. Help?","I am trying to differentiate the function that projects each $x\in  \mathbb{R}^3$ onto the unit sphere, namely: $$f(x)=\frac{x}{||x||},\quad f(0)=0$$ I know that the derivative is $$Df\big|_{x}(h)=\frac{x\times (h\times x)}{||x||^3}=\frac{h}{||x||}-\frac{x(x\cdot h)}{||x||^3}$$ But I am having a lot of trouble proving it. I can reduce it to: $$f(x+h)-f(x)-Df\big|_x(h)=x\left(\frac{1}{||x+h||}-\frac{1}{||x||}+\frac{(x\cdot h)}{||x||^3}\right)+o(h)$$ I am struggling to show that the expression in brackets is $o(h)$. It looks like the $||x+h||^{-1}$ is begging for expansion but I'm not sure how to deal with it because they are vectors. Help?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
19,Closed / open set in $\ell^\infty$ metric space.,Closed / open set in  metric space.,\ell^\infty,"Let $F$ be the set of all $x$ in $\ell^\infty$ metric space with $x_n =0$ for all but finitely many $n$, then is $F$ closed? or open? or neither? I know that $\ell^\infty$ is the space of all bounded sequences of real numbers $(x_n)$ with the sup norm. How to check if $F$ is closed or open or neither? Any hints are appreciated.","Let $F$ be the set of all $x$ in $\ell^\infty$ metric space with $x_n =0$ for all but finitely many $n$, then is $F$ closed? or open? or neither? I know that $\ell^\infty$ is the space of all bounded sequences of real numbers $(x_n)$ with the sup norm. How to check if $F$ is closed or open or neither? Any hints are appreciated.",,"['real-analysis', 'analysis', 'metric-spaces', 'normed-spaces', 'lp-spaces']"
20,Co-coercivity of gradient,Co-coercivity of gradient,,"If $f$ is convex with $dom$ $f$ $= R^{n}$ and $g(x) = x^{T}x - f(x)$ is convex, how to prove the Co-coercivity of gradient? $$(\nabla f(x) - \nabla f(y))^{T}(x - y) \geq 1/L \parallel \nabla f(x) - \nabla f(y) \parallel ^{2}_{2}$$","If is convex with and is convex, how to prove the Co-coercivity of gradient?",f dom f = R^{n} g(x) = x^{T}x - f(x) (\nabla f(x) - \nabla f(y))^{T}(x - y) \geq 1/L \parallel \nabla f(x) - \nabla f(y) \parallel ^{2}_{2},"['analysis', 'convex-analysis', 'convex-optimization']"
21,Asymptotic series of a matrix-valued function.,Asymptotic series of a matrix-valued function.,,"Consider the following matrix $$f(\lambda)=\left( \frac{\lambda-1}{\lambda + 1} \right)^{\nu \sigma_3} \ \ \ \lambda \in \mathbb{C} \setminus [-1,1]$$ where $\sigma_3=\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$ is the third pauli matrix and $\nu $ is a negative real number. Clearly $f(\lambda) \to \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} $ as $\lambda \to \infty$. But my question is, what is the asymptotic series of $f(\lambda)$ as $\lambda \to \infty$ ? Any help is appreciated, Thanks.","Consider the following matrix $$f(\lambda)=\left( \frac{\lambda-1}{\lambda + 1} \right)^{\nu \sigma_3} \ \ \ \lambda \in \mathbb{C} \setminus [-1,1]$$ where $\sigma_3=\begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$ is the third pauli matrix and $\nu $ is a negative real number. Clearly $f(\lambda) \to \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} $ as $\lambda \to \infty$. But my question is, what is the asymptotic series of $f(\lambda)$ as $\lambda \to \infty$ ? Any help is appreciated, Thanks.",,"['linear-algebra', 'analysis', 'asymptotics']"
22,Diffeomorphism which has a zero,Diffeomorphism which has a zero,,"Let $f:B(x_0,r) \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$ a diffeomorphism between $B(x_0,r)$ and its image.  If $|f'(x)^{-1}| \leq M$ for all $x \in B(x_0,r)$ and $|f(x_0)|<r/M$, show that $f$ has a zero. It seems to me that we have to use the Mean Value Inequality for the inverse map, but I didn't got anything useful. Thanks in advance.","Let $f:B(x_0,r) \subset \mathbb{R}^n \rightarrow \mathbb{R}^n$ a diffeomorphism between $B(x_0,r)$ and its image.  If $|f'(x)^{-1}| \leq M$ for all $x \in B(x_0,r)$ and $|f(x_0)|<r/M$, show that $f$ has a zero. It seems to me that we have to use the Mean Value Inequality for the inverse map, but I didn't got anything useful. Thanks in advance.",,"['analysis', 'multivariable-calculus']"
23,Positivity of solution to Laplace equation,Positivity of solution to Laplace equation,,"I'm studying PDE and at the moment I'm reading L. Evans' book. The strong maximum principle states that; if $u\in C^2(U)\cup C(\bar U)$ is harmonic in $U$, where $U$ is connected and if there exists $x_0$ such that $u(x_0)=\max _\bar U u$, then $u$ is constant within $U$. A little later Evans states that if $U$ is connected and $u$ is a solution of the pde $\Delta u=0$ in $U$ and $u=g$ on $\partial U$, where $g\geq 0$. Then $u$ is positive in $U$ everywhere if $g$ is positive somewhere on $\partial U$. Why is this true? I can't see how this follows immediately from the strong maximum principle. I'm guessing it is trivial and I'm just over thinking it. Can somebody help me?","I'm studying PDE and at the moment I'm reading L. Evans' book. The strong maximum principle states that; if $u\in C^2(U)\cup C(\bar U)$ is harmonic in $U$, where $U$ is connected and if there exists $x_0$ such that $u(x_0)=\max _\bar U u$, then $u$ is constant within $U$. A little later Evans states that if $U$ is connected and $u$ is a solution of the pde $\Delta u=0$ in $U$ and $u=g$ on $\partial U$, where $g\geq 0$. Then $u$ is positive in $U$ everywhere if $g$ is positive somewhere on $\partial U$. Why is this true? I can't see how this follows immediately from the strong maximum principle. I'm guessing it is trivial and I'm just over thinking it. Can somebody help me?",,"['analysis', 'partial-differential-equations']"
24,Need help with Mean Value Theorem Please! [closed],Need help with Mean Value Theorem Please! [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Have a question from a Calculus past paper and don't really know where to go with this question, any help would be appreciated. The question from the paper is: Suppose that $0<a<b.$ Show that $\frac{\sinh b-\sinh a}{\cosh b- \cosh a}=\coth (c)$ for some $c\in(a,b)$. (Hint: Apply the mean value theorem with $f(x)=\sinh x-\lambda \cosh x$ for a suitable choice of $\lambda \in \mathbb{R}$. Just looking for any input to help solve this question, Many Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Have a question from a Calculus past paper and don't really know where to go with this question, any help would be appreciated. The question from the paper is: Suppose that $0<a<b.$ Show that $\frac{\sinh b-\sinh a}{\cosh b- \cosh a}=\coth (c)$ for some $c\in(a,b)$. (Hint: Apply the mean value theorem with $f(x)=\sinh x-\lambda \cosh x$ for a suitable choice of $\lambda \in \mathbb{R}$. Just looking for any input to help solve this question, Many Thanks.",,"['calculus', 'analysis']"
25,Prove that the second derivative is positive iff the function is convex.,Prove that the second derivative is positive iff the function is convex.,,"Well, I want to prove the following: Let $f:(a,b)\to\mathbb R$ be double differentiable then $f$ is convex iff $\;\;f''(x)>0$ for all $x\in (a,b)$. Then I tried te following: $\Rightarrow]$ Lets suppose that $f:(a,b)\to\mathbb R$  is convex then if we define $h=-y+x$ we have that: $$ f(x+h) \leq (1-t)f(x)+tf(x+h)$$ $$0 \leq (1-t)f(x)+tf(x+h)-f(x+h)$$ I was expecting to have something of this form $$0 \leq f(x+h)-2f(x)+f(x-h)$$ and then take limit and have the result, I don't know how to define $h$ to have the above relation, Can someone help me ? $\Leftarrow]$ I checked this, but I don't know if it is right, If it is not Can you help me to fix the mistakes please: Second derivative positive $\implies$ convex Thanks a lot in advance","Well, I want to prove the following: Let $f:(a,b)\to\mathbb R$ be double differentiable then $f$ is convex iff $\;\;f''(x)>0$ for all $x\in (a,b)$. Then I tried te following: $\Rightarrow]$ Lets suppose that $f:(a,b)\to\mathbb R$  is convex then if we define $h=-y+x$ we have that: $$ f(x+h) \leq (1-t)f(x)+tf(x+h)$$ $$0 \leq (1-t)f(x)+tf(x+h)-f(x+h)$$ I was expecting to have something of this form $$0 \leq f(x+h)-2f(x)+f(x-h)$$ and then take limit and have the result, I don't know how to define $h$ to have the above relation, Can someone help me ? $\Leftarrow]$ I checked this, but I don't know if it is right, If it is not Can you help me to fix the mistakes please: Second derivative positive $\implies$ convex Thanks a lot in advance",,"['analysis', 'multivariable-calculus', 'proof-verification', 'proof-writing']"
26,How to solve this derivative of f proof?,How to solve this derivative of f proof?,,"A function $f$ satisfies: $$f''(x) + f'(x)g(x) - f(x) = 0$$ for some function $g$. Prove that if $f$ is $0$ at two points, then $f$ is $0$ on the interval between them. Can someone verify my proof? Scratchwork: So let $I = [a, b]$ and $f(a) = f(b) = 0$. $g(x)$ is some function, doesn't matter. I will use the second derivative test idea: Proof Suppose $f(x) > 0 \space \forall x \in (a, b)$ $$\because f(a) = f(b) = 0 \space \exists x_1 \in (a, b) \implies f'(x_1) = 0 \tag1$$ $$f''(x_1) + \overbrace{f'(x_1)g(x_1)}^{0} - f(x_1) = 0$$ $$ \therefore f''(x_1) = f(x_1) > 0$$ I just need help to reach a contradiction please??","A function $f$ satisfies: $$f''(x) + f'(x)g(x) - f(x) = 0$$ for some function $g$. Prove that if $f$ is $0$ at two points, then $f$ is $0$ on the interval between them. Can someone verify my proof? Scratchwork: So let $I = [a, b]$ and $f(a) = f(b) = 0$. $g(x)$ is some function, doesn't matter. I will use the second derivative test idea: Proof Suppose $f(x) > 0 \space \forall x \in (a, b)$ $$\because f(a) = f(b) = 0 \space \exists x_1 \in (a, b) \implies f'(x_1) = 0 \tag1$$ $$f''(x_1) + \overbrace{f'(x_1)g(x_1)}^{0} - f(x_1) = 0$$ $$ \therefore f''(x_1) = f(x_1) > 0$$ I just need help to reach a contradiction please??",,"['calculus', 'real-analysis', 'analysis']"
27,How to convert a discrete function to a continuous function,How to convert a discrete function to a continuous function,,"I was wondering because of this: Trick to find if number is composite or prime Is there any formal method to convert a discrete function to a continuous function. For example take $n!$, how was the gamma function discovered? Is there a general procedure to get the continuous function (which mimics a discrete function)?","I was wondering because of this: Trick to find if number is composite or prime Is there any formal method to convert a discrete function to a continuous function. For example take $n!$, how was the gamma function discovered? Is there a general procedure to get the continuous function (which mimics a discrete function)?",,"['analysis', 'functions', 'discrete-mathematics']"
28,"Is $\int_1^\infty \frac{\log(x-1)}{x(x-1)}\,dx$ convergent?",Is  convergent?,"\int_1^\infty \frac{\log(x-1)}{x(x-1)}\,dx","Does given integral $$\int_1^\infty \frac{\log(x-1)}{x(x-1)}\,dx$$ converge? If it is convergent can we evaluate it's value?","Does given integral $$\int_1^\infty \frac{\log(x-1)}{x(x-1)}\,dx$$ converge? If it is convergent can we evaluate it's value?",,"['calculus', 'integration', 'analysis', 'convergence-divergence', 'definite-integrals']"
29,Rademacher functions form an incomplete orthonormal system in the real Hilbert space,Rademacher functions form an incomplete orthonormal system in the real Hilbert space,,"I need to show that the Rademacher functions:  $$r_n(x) = \text{sign}(\sin(2^nπx)),  x \in [0, 1], n = 0, 1, 2, . . . $$ form an incomplete orthonormal system in the real Hilbert space $L^2([0,1])$ I know that a system is orthonormal if in addition to being orthogonal, we have that it is also normalized but I don't really understand the completeness criterion and I don't know how to show that these function do not form a complete orthonormal system. Any help will be appreciated.","I need to show that the Rademacher functions:  $$r_n(x) = \text{sign}(\sin(2^nπx)),  x \in [0, 1], n = 0, 1, 2, . . . $$ form an incomplete orthonormal system in the real Hilbert space $L^2([0,1])$ I know that a system is orthonormal if in addition to being orthogonal, we have that it is also normalized but I don't really understand the completeness criterion and I don't know how to show that these function do not form a complete orthonormal system. Any help will be appreciated.",,"['real-analysis', 'analysis']"
30,How find this value$\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^22^{3n+1}}$ [closed],How find this value [closed],\sum\limits_{n=0}^{\infty}\frac{(2n)!}{(n!)^22^{3n+1}},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question show  that: $$\sum_{n=0}^{\infty}\dfrac{(2n)!}{(n!)^22^{3n+1}}=\left(\frac{1}{2}\right)^{1/2}?$$ this sum is from  other problem,if I solve this,then the other problem is solve it","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question show  that: $$\sum_{n=0}^{\infty}\dfrac{(2n)!}{(n!)^22^{3n+1}}=\left(\frac{1}{2}\right)^{1/2}?$$ this sum is from  other problem,if I solve this,then the other problem is solve it",,['analysis']
31,Fundamental solution of heat equation on a compact Riemannian manifold,Fundamental solution of heat equation on a compact Riemannian manifold,,"Let $(M,g)$ be a compact Riemannian manifold of $m$ dimensional.  Then there exists a sequence $(\phi_i, \lambda_i)_{i\in\mathbb{N}}\subset C^\infty(M)\times\mathbb{R}_{\geq0}$ such that  \begin{eqnarray} 0=\lambda_0<\lambda_1\leq\lambda_2\leq\cdots\to\infty\\ \Delta\phi_i+\lambda_i\phi_i=0\\ \end{eqnarray} and $(\phi_i)_i$ forms a complete orthonormal system of $L^2(M)$. Then, my textbook says that \begin{equation} H(x,y,t):=\sum_{i=0}^\infty e^{-\lambda_it}\phi_i(x)\phi_i(y) \end{equation} is the fundamental solution of the heat equation $\Delta u-\dfrac{\partial u}{\partial t}=0$. I cannot understand what the sum means. Does the sum converge in $L^2(M\times M)$ for all $t$? Or does it converge pointwise on $M\times M\times (0,\infty)$? I want to know the mean of the $H$ as a function.","Let $(M,g)$ be a compact Riemannian manifold of $m$ dimensional.  Then there exists a sequence $(\phi_i, \lambda_i)_{i\in\mathbb{N}}\subset C^\infty(M)\times\mathbb{R}_{\geq0}$ such that  \begin{eqnarray} 0=\lambda_0<\lambda_1\leq\lambda_2\leq\cdots\to\infty\\ \Delta\phi_i+\lambda_i\phi_i=0\\ \end{eqnarray} and $(\phi_i)_i$ forms a complete orthonormal system of $L^2(M)$. Then, my textbook says that \begin{equation} H(x,y,t):=\sum_{i=0}^\infty e^{-\lambda_it}\phi_i(x)\phi_i(y) \end{equation} is the fundamental solution of the heat equation $\Delta u-\dfrac{\partial u}{\partial t}=0$. I cannot understand what the sum means. Does the sum converge in $L^2(M\times M)$ for all $t$? Or does it converge pointwise on $M\times M\times (0,\infty)$? I want to know the mean of the $H$ as a function.",,"['analysis', 'differential-geometry', 'partial-differential-equations']"
32,Method of characteristic for second order pde,Method of characteristic for second order pde,,Can I use the method of characteristic to solve second order pdes? For instance I canconsider the equation $$u_t+u_x=u_{xx}$$,Can I use the method of characteristic to solve second order pdes? For instance I canconsider the equation $$u_t+u_x=u_{xx}$$,,"['real-analysis', 'analysis', 'partial-differential-equations']"
33,$\bar S$ is the smallest closed subsets of $X$ which contains $S$,is the smallest closed subsets of  which contains,\bar S X S,"There is indeed a very similar question asked [See Proving closure of S is the smallest closed set containing S. but the content is too advance for a starter in analysis like me and the focus of question is not the same. So I have decided to raise my question here: Let $X$ be a metric space and $S$ be a subset in $X$. Show that $\bar S$ is the smallest closed subsets of $X$ which contains $S$. [ Here, define $\bar S$ as the closure of $S$ in $X$, where the closure is the set containing all the adherent points of $S$ in $X$. We also define $x \in S$ an adherent point of $S$ if $B(x,r) \cap S \neq \emptyset$ for all $r>0$.] The question is reduced to proving the following: $\bar S$ is indeed closed in X. $\bar S$ contains $S$, i.e. $S \subset \bar S$. $\bar S$ is indeed the smallest in the sense that if $J \subset X$ satisfies 1. and 2., then $\bar S \subset J$. The first two is rather straightforward and I cannot formulate the proof of the third claim. Here, I would also like to write down my proof for 1. and 2. Feel free to comment on it or provide your proof so that I can learn from you. Proof of 1.: Consider $X \setminus \bar S$. For any $x \in X \setminus \bar S$, $x \notin \bar S$. It implies that there exists $r_x >0$ such that $B(x,r_x) \cap S = \emptyset$. Now, for any $y \in B(x,r_x)$, choose $r=min \{ d(x,y), r-d(x,y)\}$. Then $B(y,r) \subset B(x,r_x)$. It follows that $B(y,r) \cap S = \emptyset$, proving that $X \setminus \bar S$ is open. Proof of 2.: It is trivial since for any $x \in S$, $x \in B(x,r) \cap S$ for all $r >0$. Please tell me your thoughts in proving the third claim. Thanks in advance.","There is indeed a very similar question asked [See Proving closure of S is the smallest closed set containing S. but the content is too advance for a starter in analysis like me and the focus of question is not the same. So I have decided to raise my question here: Let $X$ be a metric space and $S$ be a subset in $X$. Show that $\bar S$ is the smallest closed subsets of $X$ which contains $S$. [ Here, define $\bar S$ as the closure of $S$ in $X$, where the closure is the set containing all the adherent points of $S$ in $X$. We also define $x \in S$ an adherent point of $S$ if $B(x,r) \cap S \neq \emptyset$ for all $r>0$.] The question is reduced to proving the following: $\bar S$ is indeed closed in X. $\bar S$ contains $S$, i.e. $S \subset \bar S$. $\bar S$ is indeed the smallest in the sense that if $J \subset X$ satisfies 1. and 2., then $\bar S \subset J$. The first two is rather straightforward and I cannot formulate the proof of the third claim. Here, I would also like to write down my proof for 1. and 2. Feel free to comment on it or provide your proof so that I can learn from you. Proof of 1.: Consider $X \setminus \bar S$. For any $x \in X \setminus \bar S$, $x \notin \bar S$. It implies that there exists $r_x >0$ such that $B(x,r_x) \cap S = \emptyset$. Now, for any $y \in B(x,r_x)$, choose $r=min \{ d(x,y), r-d(x,y)\}$. Then $B(y,r) \subset B(x,r_x)$. It follows that $B(y,r) \cap S = \emptyset$, proving that $X \setminus \bar S$ is open. Proof of 2.: It is trivial since for any $x \in S$, $x \in B(x,r) \cap S$ for all $r >0$. Please tell me your thoughts in proving the third claim. Thanks in advance.",,"['real-analysis', 'analysis']"
34,Decay of Fourier Coefficients implies Holder Continuity?,Decay of Fourier Coefficients implies Holder Continuity?,,"This is an exercise problem. I got stuck here and would like to get a hint. The problem is Suppose $f$ is continuous and $2\pi$-periodic, and $|\hat{f}(n)|\leq |n|^{-3/2}$ for all non-zero $n\in\mathbb{Z}$. Prove that $f$ satisfies for any $x,y$:    $$|f(x)-f(y)|\leq 100|x-y|^{1/2}$$ I'm trying to proceed in the following way: since the Fourier coefficients are absolutely summable, I will expand $f$ with the Fourier series: $$ \begin{aligned} |f(x) - f(y) | &= \left| \lim_{N\rightarrow \infty} (S_Nf(x) - S_Nf(y)) \right| \\ &\leq \lim_{N\rightarrow \infty} \sum_{n=-N}^N \left|\hat{f}(n)\right|\left|e^{inx} - e^{iny}\right| \\ &\leq \lim_{N\rightarrow \infty} \sum_{n=-N,n\neq 0}^N |n|^{-3/2} \left|e^{inx} - e^{iny}\right| \\ \end{aligned} $$ Next I want to get some Holder continuity property for $e^{inx}$ in order to proceed: $$     \begin{aligned}         \sup_{x,y}\frac{\left| e^{inx} - e^{iny}\right|}{|x-y|^{1/2}} &=         \sup_{x,y}\left( \frac{\left| e^{inx} - e^{iny}\right|}{|x-y|}         \right)^{1/2} \left| e^{inx} - e^{iny}\right|^{1/2} \\         &\leq \sup_{x,y}\left( \sup_{\xi\in[x,y]} \left|ine^{in\xi}\right|\right)^{1/2}         \sqrt{2} \\         &\leq \sqrt{2n}     \end{aligned} $$ However, if I simply plug into the original inequality, I will get an infinite sum over $1/n$, which does not converge. I think I either need to get better Holder continuity estimation for $e^{inx}$ or need to proceed in a completely different way. But I'm currently stuck here. Any help or hint would be appreciated! Thank you very much!","This is an exercise problem. I got stuck here and would like to get a hint. The problem is Suppose $f$ is continuous and $2\pi$-periodic, and $|\hat{f}(n)|\leq |n|^{-3/2}$ for all non-zero $n\in\mathbb{Z}$. Prove that $f$ satisfies for any $x,y$:    $$|f(x)-f(y)|\leq 100|x-y|^{1/2}$$ I'm trying to proceed in the following way: since the Fourier coefficients are absolutely summable, I will expand $f$ with the Fourier series: $$ \begin{aligned} |f(x) - f(y) | &= \left| \lim_{N\rightarrow \infty} (S_Nf(x) - S_Nf(y)) \right| \\ &\leq \lim_{N\rightarrow \infty} \sum_{n=-N}^N \left|\hat{f}(n)\right|\left|e^{inx} - e^{iny}\right| \\ &\leq \lim_{N\rightarrow \infty} \sum_{n=-N,n\neq 0}^N |n|^{-3/2} \left|e^{inx} - e^{iny}\right| \\ \end{aligned} $$ Next I want to get some Holder continuity property for $e^{inx}$ in order to proceed: $$     \begin{aligned}         \sup_{x,y}\frac{\left| e^{inx} - e^{iny}\right|}{|x-y|^{1/2}} &=         \sup_{x,y}\left( \frac{\left| e^{inx} - e^{iny}\right|}{|x-y|}         \right)^{1/2} \left| e^{inx} - e^{iny}\right|^{1/2} \\         &\leq \sup_{x,y}\left( \sup_{\xi\in[x,y]} \left|ine^{in\xi}\right|\right)^{1/2}         \sqrt{2} \\         &\leq \sqrt{2n}     \end{aligned} $$ However, if I simply plug into the original inequality, I will get an infinite sum over $1/n$, which does not converge. I think I either need to get better Holder continuity estimation for $e^{inx}$ or need to proceed in a completely different way. But I'm currently stuck here. Any help or hint would be appreciated! Thank you very much!",,"['analysis', 'fourier-analysis', 'holder-spaces']"
35,Why can't $f(a) = f(b)$ in the intermediate value theorem.,Why can't  in the intermediate value theorem.,f(a) = f(b),"In calculus class the intermediate value theorem was introduced as follows: If $f(x)$ is continuous on the interval $[a,b]$ If $f(a) \not = f(b)$ If $k \in [f(a),f(b)]$ Then $\exists c \in [a,b], f(c) = k $ However, I do not see why it is necessary that the second condition hold, because it is true that if $k\in[f(a),f(a)]$ then there exist such a $c$ , namely $a$ . (Note that this theorem was stated for the reals only, but I am comfortable with general metric spaces, if that helps..)","In calculus class the intermediate value theorem was introduced as follows: If is continuous on the interval If If Then However, I do not see why it is necessary that the second condition hold, because it is true that if then there exist such a , namely . (Note that this theorem was stated for the reals only, but I am comfortable with general metric spaces, if that helps..)","f(x) [a,b] f(a) \not = f(b) k \in [f(a),f(b)] \exists c \in [a,b], f(c) = k  k\in[f(a),f(a)] c a","['calculus', 'analysis']"
36,Riemann integral enigma,Riemann integral enigma,,"I tried to solve this problem from Souza Silva - Berkeley Problems In Mathematics: In the Solutions part, I founded next solution for this problem: I do not understand the last statement, so why $|f|\in R[0,1] \implies f \in R[0,1]$? It is clear for me that is false...Or what am I wronging here?","I tried to solve this problem from Souza Silva - Berkeley Problems In Mathematics: In the Solutions part, I founded next solution for this problem: I do not understand the last statement, so why $|f|\in R[0,1] \implies f \in R[0,1]$? It is clear for me that is false...Or what am I wronging here?",,['analysis']
37,Modulo Big O Problem,Modulo Big O Problem,,"I know this may be really basic, but I am unsure of the complexity of this procedure in Python: def modten(n):     return n%10 edit: It is done with Python. That is the only additional information provided for this question. The question asks to specify the order of growth","I know this may be really basic, but I am unsure of the complexity of this procedure in Python: def modten(n):     return n%10 edit: It is done with Python. That is the only additional information provided for this question. The question asks to specify the order of growth",,['analysis']
38,How to prove Godunova's inequality?,How to prove Godunova's inequality?,,"Let $\phi$ be a positive and convex function on $(0,\infty)$. Then   $$\int_0^\infty \phi\left(\frac{1}{x}\int_0^x g(t)\,dt\right)\frac{dx}{x} \leq \int_0^\infty \phi(g(x))\frac{dx}{x}$$ The application of this inequality is this : $(1)$ Hardy's inequality. With $\phi(u)=u^p$, we obtain that $$\int_0^\infty \left(\frac{1}{x} \int_0^x g(t) \, dt\right)^p\frac{dx}{x} \leq \int_0^\infty g^p(x)\frac{dx}{x}$$ (2) Polya-Knopp's inequality By using it with $\phi(u)=u^p$, replacing $g(x)$ by $\log g(x)$ and making the substitution $h(x)=\frac{g(x)}{x}$ we obtain that $$\int_0^\infty \exp\left(\frac{1}{x} \int_0^x \log h(t)\right) \, dt \leq e\int_0^\infty h(x) \, dx$$ How to prove Godunova's inequality? Is there any reference?","Let $\phi$ be a positive and convex function on $(0,\infty)$. Then   $$\int_0^\infty \phi\left(\frac{1}{x}\int_0^x g(t)\,dt\right)\frac{dx}{x} \leq \int_0^\infty \phi(g(x))\frac{dx}{x}$$ The application of this inequality is this : $(1)$ Hardy's inequality. With $\phi(u)=u^p$, we obtain that $$\int_0^\infty \left(\frac{1}{x} \int_0^x g(t) \, dt\right)^p\frac{dx}{x} \leq \int_0^\infty g^p(x)\frac{dx}{x}$$ (2) Polya-Knopp's inequality By using it with $\phi(u)=u^p$, replacing $g(x)$ by $\log g(x)$ and making the substitution $h(x)=\frac{g(x)}{x}$ we obtain that $$\int_0^\infty \exp\left(\frac{1}{x} \int_0^x \log h(t)\right) \, dt \leq e\int_0^\infty h(x) \, dx$$ How to prove Godunova's inequality? Is there any reference?",,"['real-analysis', 'analysis', 'inequality', 'convex-analysis', 'integral-inequality']"
39,an argument that strengthen Lusin's theorem,an argument that strengthen Lusin's theorem,,"Let $f$ be a measurable function on a subset $E$ of $\mathbb{R}^n$. Lusin's theorem states that for any $\epsilon>0$, there exists a measurable subset $F$ such that $F$ open in $E$, $\mu(F)<\epsilon$ and $f$ is continuous on $E\setminus F$. Let $\epsilon=1/n$. Choose $F_n$ such that $\mu(F_n)<1/n$ and $f$ continuous on $E\setminus F_n$. Since $f$ is continuous on $E\setminus F_{n-1}$, we can choose $F_n\subseteq F_{n-1}$. Hence we can choose $\{F_n\}$ satisfying $F_1\supseteq F_2\cdots \supseteq F_n\supseteq F_{n+1}\supseteq\cdots $ Let $G=\cap_{n=1}^\infty F_n$. Then $\mu(G)=\lim_{n\to\infty}\mu(F_n)=0$. For any $x\in E\setminus G$, there exists $N$ such that for any $n\geq N$, $x\notin F_n$. Hence $f$ is continuous at $x$. Hence we strengthen Lusin's theorem to the following version: Let $f$ be a measurable function on a subset $E$ of $\mathbb{R}^n$. Then $f$ is continuous a.e. Why this argument is not valid?","Let $f$ be a measurable function on a subset $E$ of $\mathbb{R}^n$. Lusin's theorem states that for any $\epsilon>0$, there exists a measurable subset $F$ such that $F$ open in $E$, $\mu(F)<\epsilon$ and $f$ is continuous on $E\setminus F$. Let $\epsilon=1/n$. Choose $F_n$ such that $\mu(F_n)<1/n$ and $f$ continuous on $E\setminus F_n$. Since $f$ is continuous on $E\setminus F_{n-1}$, we can choose $F_n\subseteq F_{n-1}$. Hence we can choose $\{F_n\}$ satisfying $F_1\supseteq F_2\cdots \supseteq F_n\supseteq F_{n+1}\supseteq\cdots $ Let $G=\cap_{n=1}^\infty F_n$. Then $\mu(G)=\lim_{n\to\infty}\mu(F_n)=0$. For any $x\in E\setminus G$, there exists $N$ such that for any $n\geq N$, $x\notin F_n$. Hence $f$ is continuous at $x$. Hence we strengthen Lusin's theorem to the following version: Let $f$ be a measurable function on a subset $E$ of $\mathbb{R}^n$. Then $f$ is continuous a.e. Why this argument is not valid?",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
40,Equivalent Definitions of Negative Order Sobolev Spaces,Equivalent Definitions of Negative Order Sobolev Spaces,,"Ignoring fractional sobolev spaces, if we restrict ourselves to $k>0$ when $k$ is an integer, then the Sobolev space of order $k$, for $W^{k,p}(\mathbb{R})$ is the space of functions $f$ such that $\|f\|_{W^{k,p}(\Omega)} \asymp \|f\|_{p} + \|f^{(k)}\|_{p}$ is finite. A standard way of defining Sobolev spaces when $k<0$ is to say that if $1/p + 1/p^\prime = 1$, then $W^{-k,p^\prime}(\mathbb{R})$ is the dual space of $W^{k,p}(\Omega)$. In particular, when $p = 2$, we get a Hilbert space. For the rest of this question, we will assume $p=2$ My $\textbf{Question}$ is that there seems to be a lot of literature that considers defining a Hilbert Scale to be a sequence of embedded spaces, $H_{k+1} \subset H_k$, such that $H_k = \{f : \int (1+t^2)^k|\hat f(t)|^2dt <\infty \}$ where $\hat f$ is the Fourier transform of $f$. Naturally, when $k>0$ in this case, $H_k$ matches up with the Sobolev spaces $W^{k,p}(\mathbb{R})$, but I am not sure when $k<0$. When $k<0$, do the definitions of the spaces $H_k$ and $W^{-k,2}(\mathbb{R})$ coincide?","Ignoring fractional sobolev spaces, if we restrict ourselves to $k>0$ when $k$ is an integer, then the Sobolev space of order $k$, for $W^{k,p}(\mathbb{R})$ is the space of functions $f$ such that $\|f\|_{W^{k,p}(\Omega)} \asymp \|f\|_{p} + \|f^{(k)}\|_{p}$ is finite. A standard way of defining Sobolev spaces when $k<0$ is to say that if $1/p + 1/p^\prime = 1$, then $W^{-k,p^\prime}(\mathbb{R})$ is the dual space of $W^{k,p}(\Omega)$. In particular, when $p = 2$, we get a Hilbert space. For the rest of this question, we will assume $p=2$ My $\textbf{Question}$ is that there seems to be a lot of literature that considers defining a Hilbert Scale to be a sequence of embedded spaces, $H_{k+1} \subset H_k$, such that $H_k = \{f : \int (1+t^2)^k|\hat f(t)|^2dt <\infty \}$ where $\hat f$ is the Fourier transform of $f$. Naturally, when $k>0$ in this case, $H_k$ matches up with the Sobolev spaces $W^{k,p}(\mathbb{R})$, but I am not sure when $k<0$. When $k<0$, do the definitions of the spaces $H_k$ and $W^{-k,2}(\mathbb{R})$ coincide?",,"['analysis', 'hilbert-spaces', 'definition', 'sobolev-spaces']"
41,"$\int_{-1}^{1} x^{k+i} P_n(x)dx$, $P_n$ Legendre polynomial.",",  Legendre polynomial.",\int_{-1}^{1} x^{k+i} P_n(x)dx P_n,"I was wondering whether there is a way to say what $$\int_{-1}^{1} x^{k} P_n(x)dx$$ is, where $k,n$ are positive integers or zero and $P_n$ is the n-th Legendre polynomial? I am looking for an analytic result for this. Probably one could use this: Barne's integral: Barne's Integral But the set over which is integrated is different in this case.","I was wondering whether there is a way to say what $$\int_{-1}^{1} x^{k} P_n(x)dx$$ is, where $k,n$ are positive integers or zero and $P_n$ is the n-th Legendre polynomial? I am looking for an analytic result for this. Probably one could use this: Barne's integral: Barne's Integral But the set over which is integrated is different in this case.",,"['calculus', 'real-analysis']"
42,"What is the convex-hull of the set $\{ (n,\varphi(n)) : n\in \mathbb N \} \subset \mathbb R^2$",What is the convex-hull of the set,"\{ (n,\varphi(n)) : n\in \mathbb N \} \subset \mathbb R^2","I know that set $$ E=\{ (n,\varphi(n)) : n\in \mathbb N \} \subset \mathbb R^2 $$ has infinitely many points on the line $y=x-1$, which suggests this line to be included in the upper part of the convex-hull. However I don't really see what's going on at the bottom. That is what is the convex-hull of $E$? Thanks.","I know that set $$ E=\{ (n,\varphi(n)) : n\in \mathbb N \} \subset \mathbb R^2 $$ has infinitely many points on the line $y=x-1$, which suggests this line to be included in the upper part of the convex-hull. However I don't really see what's going on at the bottom. That is what is the convex-hull of $E$? Thanks.",,"['analysis', 'number-theory']"
43,proving $E$ is $\nu$-null iff $|\nu| (E)=0$,proving  is -null iff,E \nu |\nu| (E)=0,"I am having trouble proving the converse of the statement below. So far I have that $\nu (E)=0$, but that doesn't mean that $E$ is necessarily $\nu$-null. I can't seem to find a way to prove that if $A\subseteq E$ and $A$ is measurable, then $\nu (A)=0$. Below is a print screened image of what I have so far. Some guidance would be appreciated! Thanks.","I am having trouble proving the converse of the statement below. So far I have that $\nu (E)=0$, but that doesn't mean that $E$ is necessarily $\nu$-null. I can't seem to find a way to prove that if $A\subseteq E$ and $A$ is measurable, then $\nu (A)=0$. Below is a print screened image of what I have so far. Some guidance would be appreciated! Thanks.",,"['real-analysis', 'analysis', 'measure-theory']"
44,Bernoulli Map properties,Bernoulli Map properties,,"I am referring to the function stated here http://en.wikipedia.org/wiki/Dyadic_transformation This map is defined on $[0,1]$ by $f_n(x)=nx [mod 1]$ There are three things I do not quite understand, may you can help me with that.Lets strt with the well known case $n=2$, i.e $f_2(x)=2x [mod 1]$, this means when we consider the fractional part of some value in each step we move the coma one step to the right. An example for a 2-cycle would be $\{1/3,2/3\}$ My first question: How can I find all 3-cycles of $f_2$ without simply guessing the numbers? My second question: What does $f_3$ means? Is this simply the representation from numbers with base 3? My third question (I have no idea about that): Do you have an idea for a non periodic orbit of the Bernoulli map that is dense in $[0,1)$?","I am referring to the function stated here http://en.wikipedia.org/wiki/Dyadic_transformation This map is defined on $[0,1]$ by $f_n(x)=nx [mod 1]$ There are three things I do not quite understand, may you can help me with that.Lets strt with the well known case $n=2$, i.e $f_2(x)=2x [mod 1]$, this means when we consider the fractional part of some value in each step we move the coma one step to the right. An example for a 2-cycle would be $\{1/3,2/3\}$ My first question: How can I find all 3-cycles of $f_2$ without simply guessing the numbers? My second question: What does $f_3$ means? Is this simply the representation from numbers with base 3? My third question (I have no idea about that): Do you have an idea for a non periodic orbit of the Bernoulli map that is dense in $[0,1)$?",,"['analysis', 'special-functions', 'dynamical-systems']"
45,Is it possible to learn differential topology before analysis?,Is it possible to learn differential topology before analysis?,,"Currently I'm self studying for my own enjoyment topology and algebra (munkres and herstein). Since I start at the university next year everything I'm learning now is for my own enjoyment and I will probably relearn it in the university anyway . I'm interested in differential topology yet I haven't read a single book on analysis in the level of say, ""baby Rudin"". Not to say that I'm not familiar with it. I have done a lot of studying in the past on calculus (sequences, differentiation, integration, fourier series, integral transforms etc.) ODE's (and a bit PDE's) and non rigorous complex analysis ( visual complex analysis ) but my foundation are really far from being solid. Is it possible to learn differential topology before strengthening my foundations in analysis and leave that to do once i get to the university?","Currently I'm self studying for my own enjoyment topology and algebra (munkres and herstein). Since I start at the university next year everything I'm learning now is for my own enjoyment and I will probably relearn it in the university anyway . I'm interested in differential topology yet I haven't read a single book on analysis in the level of say, ""baby Rudin"". Not to say that I'm not familiar with it. I have done a lot of studying in the past on calculus (sequences, differentiation, integration, fourier series, integral transforms etc.) ODE's (and a bit PDE's) and non rigorous complex analysis ( visual complex analysis ) but my foundation are really far from being solid. Is it possible to learn differential topology before strengthening my foundations in analysis and leave that to do once i get to the university?",,"['analysis', 'soft-question']"
46,preimages of simple functions form a partition,preimages of simple functions form a partition,,"Let $\varphi $ ba simple, then we know $A_i = \varphi^{-1} (a_i) $ . Claim is $A_i$ paritition $\mathbb{R}$ my try: Note that the sets $A_i = \varphi^{-1} ( \{ a_i \} ) $ form a partition of $\mathbb{R}$. To see this, we show $A_i$ are pairwise disjoint. Suppose that there exists $x \in A_i \cap A_j $ and $i \neq j$. Therefore, we have that $\varphi(x) = a_i = a_j \implies 1_{A_i} = 1_{A_j} $. Hence, by Definition of the indicator function, we must have that $A_i = A_j$. Therefore, $A_i$ are parwise disjoint. In particular $\mathbb{R} = \bigcup_{i=1}^{N} A_i $. Question; Is this correct?","Let $\varphi $ ba simple, then we know $A_i = \varphi^{-1} (a_i) $ . Claim is $A_i$ paritition $\mathbb{R}$ my try: Note that the sets $A_i = \varphi^{-1} ( \{ a_i \} ) $ form a partition of $\mathbb{R}$. To see this, we show $A_i$ are pairwise disjoint. Suppose that there exists $x \in A_i \cap A_j $ and $i \neq j$. Therefore, we have that $\varphi(x) = a_i = a_j \implies 1_{A_i} = 1_{A_j} $. Hence, by Definition of the indicator function, we must have that $A_i = A_j$. Therefore, $A_i$ are parwise disjoint. In particular $\mathbb{R} = \bigcup_{i=1}^{N} A_i $. Question; Is this correct?",,"['real-analysis', 'analysis']"
47,Proving that the total variation of an absolutely continuous function is absolutely continuous,Proving that the total variation of an absolutely continuous function is absolutely continuous,,"I am trying to prove this statement which appears in the real analysis text by Stein which he just passes as a remark. If $F$ is absolutely continuous on $[a,b]$, then the total variation of $F$ is continuous and to be specific, absolutely continuous. I believe that it suffices to show that the total variation of $F$ is absolutely continuous. The total variation of $F$ on $[a,x]$ is defined to be $T_F(x)=\sup\sum_{j=1}^N|F(t_j)-F(t_{j-1})|$ where the $\sup$ is over all partitions of $[a,x].$ Does this mean that I have to prove that $\sum_{k=1}^N|T_F(b_k)-T(a_k)|<\epsilon$ if $\sum_{k=1}^N(b_k -a_k)<\delta?$ This looks a bit strange to me...can anyone clarify this matter? thanks!","I am trying to prove this statement which appears in the real analysis text by Stein which he just passes as a remark. If $F$ is absolutely continuous on $[a,b]$, then the total variation of $F$ is continuous and to be specific, absolutely continuous. I believe that it suffices to show that the total variation of $F$ is absolutely continuous. The total variation of $F$ on $[a,x]$ is defined to be $T_F(x)=\sup\sum_{j=1}^N|F(t_j)-F(t_{j-1})|$ where the $\sup$ is over all partitions of $[a,x].$ Does this mean that I have to prove that $\sum_{k=1}^N|T_F(b_k)-T(a_k)|<\epsilon$ if $\sum_{k=1}^N(b_k -a_k)<\delta?$ This looks a bit strange to me...can anyone clarify this matter? thanks!",,"['real-analysis', 'analysis']"
48,"baby rudin 2.33, relative compactness","baby rudin 2.33, relative compactness",,"my question is relative to baby rudin theorem 2.33 which states; $$ \ suppose \  K \subset Y \subset X.  \ then\  K \ is\  compact\  relative \ to\  X \ iff\  K\  is\  compact\  relative \ to \ Y.$$ honestly, i think i only have  maybe a superficial understanding of what Rudin is even saying here.  however, i become more uncertain in his proof.  i would say that i feel i have a pretty good understanding of theorem 2.30, the preceding theorem, which says $$ suppose \ Y \subset X. a \ subset \ E \ of \ Y \ is \ open \ relative \ to \ Y \ iff \  \  E = Y \bigcap G  \ for \ some \ open \ subset \ G \ of \ X.$$ which, as i understand the idea of openness as $E \subset Y$ may be open in $Y$ but may not be open in $X$ where $E \subset Y \subset X$. also i feel pretty comfortable with the idea of a compact set as being one where it is a subset of a finite union of a family of sets, the finite subcover.  compared to a general open cover, which is just a union of any family of open sets, which is a superset of some other set which it is the open cover for. now that i have explained the relative parts of what i do (think) i understand, let me clarify what about theorem 2.33 i am uncomfortable with; i really am not sure what it even means for sets to be compact relative to another set.  in the topological sense, is compactness not a invariant property of a topological space? Rudin proceeds on with the proof as follows; suppose $K$ is compactive relative to $X$, and let $\{V_{\alpha}\}$ be a collection of sets, open relative to $Y$, such that $K\subset \bigcup_{\alpha} V_{\alpha}$. this is the first part of the proof i am confused by.  $K$ is assumed to be compact relative to $X$ but Rudin describes $K$ as being covered by $V_{\alpha}$, where $\{V_{\alpha}\}$ is an open subset of $Y$.  would not $K$ being covered by a family of sets, subsets of $X$, follow immediately from the fact that $K$ is compact relative to $X$? i dont understand the motivation for this part. carrying on for the moment. By theorem 2.30 there are sets $G_{\alpha}$, open relative to $X$, such that $V_{\alpha}=Y \bigcap G_{\alpha}$, for all $\alpha$; and since $K$ is compact relative to $X$ we have $$(22) \  K \subset G_{\alpha_1} \bigcup ..... \bigcup G_{\alpha_n}$$  for finitely many indices $\alpha_1,...\alpha_n$ which i dont argue with any of. since $K \subset Y$, (22) implies $$ (23) \ K \subset V_{\alpha_1} \bigcup ... \bigcup V_{\alpha_n} $$.  and this proves $K$ is compact relative to $Y$. this is the last part i dont understand, how does $K$ being a subset of $Y$ force (22) to imply (23)? of course this is only one direction in the bijection, but i was so bothered by the theorem/proof i havent even gotten to the second part of the bijection.","my question is relative to baby rudin theorem 2.33 which states; $$ \ suppose \  K \subset Y \subset X.  \ then\  K \ is\  compact\  relative \ to\  X \ iff\  K\  is\  compact\  relative \ to \ Y.$$ honestly, i think i only have  maybe a superficial understanding of what Rudin is even saying here.  however, i become more uncertain in his proof.  i would say that i feel i have a pretty good understanding of theorem 2.30, the preceding theorem, which says $$ suppose \ Y \subset X. a \ subset \ E \ of \ Y \ is \ open \ relative \ to \ Y \ iff \  \  E = Y \bigcap G  \ for \ some \ open \ subset \ G \ of \ X.$$ which, as i understand the idea of openness as $E \subset Y$ may be open in $Y$ but may not be open in $X$ where $E \subset Y \subset X$. also i feel pretty comfortable with the idea of a compact set as being one where it is a subset of a finite union of a family of sets, the finite subcover.  compared to a general open cover, which is just a union of any family of open sets, which is a superset of some other set which it is the open cover for. now that i have explained the relative parts of what i do (think) i understand, let me clarify what about theorem 2.33 i am uncomfortable with; i really am not sure what it even means for sets to be compact relative to another set.  in the topological sense, is compactness not a invariant property of a topological space? Rudin proceeds on with the proof as follows; suppose $K$ is compactive relative to $X$, and let $\{V_{\alpha}\}$ be a collection of sets, open relative to $Y$, such that $K\subset \bigcup_{\alpha} V_{\alpha}$. this is the first part of the proof i am confused by.  $K$ is assumed to be compact relative to $X$ but Rudin describes $K$ as being covered by $V_{\alpha}$, where $\{V_{\alpha}\}$ is an open subset of $Y$.  would not $K$ being covered by a family of sets, subsets of $X$, follow immediately from the fact that $K$ is compact relative to $X$? i dont understand the motivation for this part. carrying on for the moment. By theorem 2.30 there are sets $G_{\alpha}$, open relative to $X$, such that $V_{\alpha}=Y \bigcap G_{\alpha}$, for all $\alpha$; and since $K$ is compact relative to $X$ we have $$(22) \  K \subset G_{\alpha_1} \bigcup ..... \bigcup G_{\alpha_n}$$  for finitely many indices $\alpha_1,...\alpha_n$ which i dont argue with any of. since $K \subset Y$, (22) implies $$ (23) \ K \subset V_{\alpha_1} \bigcup ... \bigcup V_{\alpha_n} $$.  and this proves $K$ is compact relative to $Y$. this is the last part i dont understand, how does $K$ being a subset of $Y$ force (22) to imply (23)? of course this is only one direction in the bijection, but i was so bothered by the theorem/proof i havent even gotten to the second part of the bijection.",,"['real-analysis', 'analysis']"
49,Trying to Understand Baby Rudin Proof,Trying to Understand Baby Rudin Proof,,"If p is a limit point of a set E, then every neighborhood of p contains infinitely many points of E Proof: Suppose there is a neighborhood N of p which contains only a finite number of points of E. Let r be the minimum of the distances of these points from p. The minimum of a finite set of positive numbers is clearly positive so that r > 0. The neighborhood  contains no point q of E such that q not equal to p which contradicts the fact that p is a limit point of E. What somehow just jumps over my head is : how is it clear that the neighborhood contains no point q in E s.t. q not equal to p? I thought we'd just been talking about a finite list of point say q1 .... qn which are all not equal to p?","If p is a limit point of a set E, then every neighborhood of p contains infinitely many points of E Proof: Suppose there is a neighborhood N of p which contains only a finite number of points of E. Let r be the minimum of the distances of these points from p. The minimum of a finite set of positive numbers is clearly positive so that r > 0. The neighborhood  contains no point q of E such that q not equal to p which contradicts the fact that p is a limit point of E. What somehow just jumps over my head is : how is it clear that the neighborhood contains no point q in E s.t. q not equal to p? I thought we'd just been talking about a finite list of point say q1 .... qn which are all not equal to p?",,['analysis']
50,Use the Integrability Criterion to show that the function $f: I \to \Bbb R$ is integrable.,Use the Integrability Criterion to show that the function  is integrable.,f: I \to \Bbb R,"Question: For the generalized rectangle $I= [0,1]\times [0,1]$ in the plane $\Bbb R^2$ $$f(x,y)=\begin{cases} 5 & if\ \  (x,y)\ is\ in\ I\ and\ x> 1/2 \\ 1 & if\ (x,y)\ is\ in\ I\ \ and\ x\le 1/2 \end{cases}$$ Use the $\cal{Integrability\ Criterion}$ to show that the function $f: I \to \Bbb R$ is integrable. $\cal{Integrability\ Criterion:}$ Bounded function $f: I \to \Bbb R$ is integrable $\iff$ for each $\epsilon >0$ there is a partition $P$ of $I$ such that $$U(f, P)-L(f,P)<\epsilon$$ Solution trial: Let $\epsilon >0$ For $k\in \Bbb N$, let $P_k$ be the partition of the interval $[0,1]$ of equal length $1/k$. After then, how should I solve the question?","Question: For the generalized rectangle $I= [0,1]\times [0,1]$ in the plane $\Bbb R^2$ $$f(x,y)=\begin{cases} 5 & if\ \  (x,y)\ is\ in\ I\ and\ x> 1/2 \\ 1 & if\ (x,y)\ is\ in\ I\ \ and\ x\le 1/2 \end{cases}$$ Use the $\cal{Integrability\ Criterion}$ to show that the function $f: I \to \Bbb R$ is integrable. $\cal{Integrability\ Criterion:}$ Bounded function $f: I \to \Bbb R$ is integrable $\iff$ for each $\epsilon >0$ there is a partition $P$ of $I$ such that $$U(f, P)-L(f,P)<\epsilon$$ Solution trial: Let $\epsilon >0$ For $k\in \Bbb N$, let $P_k$ be the partition of the interval $[0,1]$ of equal length $1/k$. After then, how should I solve the question?",,"['calculus', 'real-analysis', 'analysis', 'integration']"
51,"$\Delta u = f, f \in L^q \Rightarrow u \in W^{2,q}$ References",References,"\Delta u = f, f \in L^q \Rightarrow u \in W^{2,q}",I'm looking for references for the following theorem. I will very grateful Theorem: [Calderón Zigmund] If $u$ is a solution of   \begin{equation} \Delta u = f \quad \mbox{in} \quad B_2 \end{equation}   then   \begin{equation} \int_{B_2} | D^2u|^p \le \Bigl(\int_{B_2} |f|^p + \int_{B_2} |u|^p \Bigr) \quad \mbox{for any} 1<p<  + \infty. \end{equation},I'm looking for references for the following theorem. I will very grateful Theorem: [Calderón Zigmund] If $u$ is a solution of   \begin{equation} \Delta u = f \quad \mbox{in} \quad B_2 \end{equation}   then   \begin{equation} \int_{B_2} | D^2u|^p \le \Bigl(\int_{B_2} |f|^p + \int_{B_2} |u|^p \Bigr) \quad \mbox{for any} 1<p<  + \infty. \end{equation},,"['analysis', 'reference-request', 'partial-differential-equations']"
52,Suppose that the function f(x),Suppose that the function f(x),,"Suppose that a function $f(x)$ defined on $[0,1]$ satisfies $f(1/n)\to 0$ as $n\to\infty$. Can we say that  $f(x)\to 0$ as $x\to 0^+$ if $f$ is continuous on $[0,1]$ ? and again is it true $f(x)\to 0$ as $x\to 0^+$ if  $f$ is differentiable on $(0,1)$ ? I can see this true for some problems when using Maple to plot them, but I need to prove/disprove that. If anyone can prove this for me. Many thanks.","Suppose that a function $f(x)$ defined on $[0,1]$ satisfies $f(1/n)\to 0$ as $n\to\infty$. Can we say that  $f(x)\to 0$ as $x\to 0^+$ if $f$ is continuous on $[0,1]$ ? and again is it true $f(x)\to 0$ as $x\to 0^+$ if  $f$ is differentiable on $(0,1)$ ? I can see this true for some problems when using Maple to plot them, but I need to prove/disprove that. If anyone can prove this for me. Many thanks.",,"['analysis', 'functions']"
53,Can anyone show or clarify,Can anyone show or clarify,,Can any anyone clarify or prove that if the derivative of a function $f$ is strictly positive then the function $f$ is strictly monotone increasing. I am really sure that the converse is not true as the converse will not be true for the function $f(x)=x^3$. I thank every one prove the first part of the this problem for me.,Can any anyone clarify or prove that if the derivative of a function $f$ is strictly positive then the function $f$ is strictly monotone increasing. I am really sure that the converse is not true as the converse will not be true for the function $f(x)=x^3$. I thank every one prove the first part of the this problem for me.,,['analysis']
54,Prove there exists $y\neq 0$ but $x\cdot y=0$,Prove there exists  but,y\neq 0 x\cdot y=0,"I would like to know if I'm missing something with my solution - as an earlier version was wrong and I think I've managed to patch it up - to this problem from Rudin's Principles of Mathematical Analysis. Every solution I've read online always breaks it up into two cases: $k$ even and $k$ odd which makes me feel I might be wrong somewhere as I believe my solution is much simpler, if correct. Problem 1.18 If $k \geq2$ and $x \in \mathbb{R}^k$, prove that there exists $y \in \mathbb{R}^k$ such that $y \neq 0$ but $x \cdot y =0$. Proof : Assume $x \neq 0$ as otherwise any non-zero $y$ would do. If $x = (x_1, \ldots, x_k)$ is non-zero then there is a pair $x_i \text{ and } x_j$ with $1 \leq i < j \leq k$ such that at least one is not zero. Letting $y = (0_1, \ldots, 0_{i-1}, -x_j, 0_{i+1},\ldots,0_{j-1}, x_i, 0_{j+1},\dots,0_k)$ we have $x\cdot y = 0$. Thank you.","I would like to know if I'm missing something with my solution - as an earlier version was wrong and I think I've managed to patch it up - to this problem from Rudin's Principles of Mathematical Analysis. Every solution I've read online always breaks it up into two cases: $k$ even and $k$ odd which makes me feel I might be wrong somewhere as I believe my solution is much simpler, if correct. Problem 1.18 If $k \geq2$ and $x \in \mathbb{R}^k$, prove that there exists $y \in \mathbb{R}^k$ such that $y \neq 0$ but $x \cdot y =0$. Proof : Assume $x \neq 0$ as otherwise any non-zero $y$ would do. If $x = (x_1, \ldots, x_k)$ is non-zero then there is a pair $x_i \text{ and } x_j$ with $1 \leq i < j \leq k$ such that at least one is not zero. Letting $y = (0_1, \ldots, 0_{i-1}, -x_j, 0_{i+1},\ldots,0_{j-1}, x_i, 0_{j+1},\dots,0_k)$ we have $x\cdot y = 0$. Thank you.",,['linear-algebra']
55,Prove there are infinitely many converging subsequences which do not overlap with each other.,Prove there are infinitely many converging subsequences which do not overlap with each other.,,"Let $||$ a$_n$ -a$_{2n}$$||$ $\leq$ $\frac{1}{2^n}$   for all n$\geq$ 1. Prove there are infinitely many converging subsequences which do not overlap with each other. a$_n$ $\in$ R$^m$ Well, I know a cauchy sequence always conveges. there exists an N such that $||a_n - a_m|| \leq \epsilon $  for all n, m > $N$ Still confused how to show say the subsequence {a$_{2^n}$} is Cauchy.","Let $||$ a$_n$ -a$_{2n}$$||$ $\leq$ $\frac{1}{2^n}$   for all n$\geq$ 1. Prove there are infinitely many converging subsequences which do not overlap with each other. a$_n$ $\in$ R$^m$ Well, I know a cauchy sequence always conveges. there exists an N such that $||a_n - a_m|| \leq \epsilon $  for all n, m > $N$ Still confused how to show say the subsequence {a$_{2^n}$} is Cauchy.",,"['real-analysis', 'analysis']"
56,The sum of infinite series $\sum_{k=1}^{\infty}2\sin\left(\frac{1}{k}-\frac{1}{k+1}\right)\cos\left(\frac{1}{k}+\frac{1}{k+1}\right)$,The sum of infinite series,\sum_{k=1}^{\infty}2\sin\left(\frac{1}{k}-\frac{1}{k+1}\right)\cos\left(\frac{1}{k}+\frac{1}{k+1}\right),Determine the sum of the  infinite series $$\sum_{k=1}^{\infty}2\sin\left(\frac{1}{k}-\frac{1}{k+1}\right)\cos\left(\frac{1}{k}+\frac{1}{k+1}\right).$$,Determine the sum of the  infinite series $$\sum_{k=1}^{\infty}2\sin\left(\frac{1}{k}-\frac{1}{k+1}\right)\cos\left(\frac{1}{k}+\frac{1}{k+1}\right).$$,,"['real-analysis', 'analysis']"
57,Weak convexity and continuity,Weak convexity and continuity,,"For any open interval $(a, b)\subset {\mathbb R}\,$, define a weakly convex function $f:(a, b) \rightarrow {\mathbb R}$ as one for which $$f(q\;x_0 + (1 - q)\;x_1) \leq q\;f(x_0) + (1-q)\;f(x_1)$$ for all $x_0, x_1 \in (a, b)$, and all $q \in [0, 1] \cap {\mathbb Q}$. Does this definition of weak convexity imply continuity (in $(a, b)$)? My intuition says ""yes"", because hard as I try, I can't see how a function that satisfies the inequality above for all $x_0, x_1,$ and $q$ could have a single point of discontinuity in $(a, b)$. My attempt to prove this implication, however, has been slippery business.  A pointer to a proof would be welcome (assuming, of course, that my intuition is correct). The book where I found this definition of weak convexity implies (though does not state outright) that weak convexity does not guarantee continuity.  If this is the case, I'd love to see a counterexample.","For any open interval $(a, b)\subset {\mathbb R}\,$, define a weakly convex function $f:(a, b) \rightarrow {\mathbb R}$ as one for which $$f(q\;x_0 + (1 - q)\;x_1) \leq q\;f(x_0) + (1-q)\;f(x_1)$$ for all $x_0, x_1 \in (a, b)$, and all $q \in [0, 1] \cap {\mathbb Q}$. Does this definition of weak convexity imply continuity (in $(a, b)$)? My intuition says ""yes"", because hard as I try, I can't see how a function that satisfies the inequality above for all $x_0, x_1,$ and $q$ could have a single point of discontinuity in $(a, b)$. My attempt to prove this implication, however, has been slippery business.  A pointer to a proof would be welcome (assuming, of course, that my intuition is correct). The book where I found this definition of weak convexity implies (though does not state outright) that weak convexity does not guarantee continuity.  If this is the case, I'd love to see a counterexample.",,"['analysis', 'convex-analysis', 'examples-counterexamples']"
58,Ultrafilters and measurability,Ultrafilters and measurability,,"Consider a compact metric space $X$, the sigma-algebra of the boreleans of $X$, a sequence of measurable maps $f_n: X \to\Bbb R$ and an ultrafilter $U$. Take, for each $x \in X$, the $U$-limit, say $f^*(x)$, of the sequence $(f_n(x))_{n \in\Bbb N}$. (Under what conditions on $U$) Is $f^*$ measurable?","Consider a compact metric space $X$, the sigma-algebra of the boreleans of $X$, a sequence of measurable maps $f_n: X \to\Bbb R$ and an ultrafilter $U$. Take, for each $x \in X$, the $U$-limit, say $f^*(x)$, of the sequence $(f_n(x))_{n \in\Bbb N}$. (Under what conditions on $U$) Is $f^*$ measurable?",,"['analysis', 'measure-theory', 'filters']"
59,Absolute continuity inquality,Absolute continuity inquality,,"Show for an absolutely continuous function $u(x)$ on $[0,1]$ that satisfies $u(0) = 0$, $$ \int_0^1{\frac{u(x)^2}{x^{3/2}}dx} \leq 2\int_0^1 (u'(x))^2 dx $$","Show for an absolutely continuous function $u(x)$ on $[0,1]$ that satisfies $u(0) = 0$, $$ \int_0^1{\frac{u(x)^2}{x^{3/2}}dx} \leq 2\int_0^1 (u'(x))^2 dx $$",,"['real-analysis', 'analysis', 'measure-theory']"
60,Divergence of the sequence $\sin(n)$ [duplicate],Divergence of the sequence  [duplicate],\sin(n),"This question already has answers here : Closed 11 years ago . Possible Duplicate: Prove the divergence of the sequence $(\sin(n))_{n=1}^\infty$. How can I show that the sequence $$  a_n = \sin(n) $$ is divergent? I tried to show that $\sin(n+1) - \sin(n)$ get always larger than some constant, but I did not succeeded.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Prove the divergence of the sequence $(\sin(n))_{n=1}^\infty$. How can I show that the sequence $$  a_n = \sin(n) $$ is divergent? I tried to show that $\sin(n+1) - \sin(n)$ get always larger than some constant, but I did not succeeded.",,"['analysis', 'trigonometry']"
61,Definite integral over triple products of higher order Bessel functions.,Definite integral over triple products of higher order Bessel functions.,,"As a follow up to this question I am also interested in a symbolic closed form for this integral $$\int_0^\infty d r \,r^2\, j_{n_1}( k_1 r)\, j_{n_2}( k_2 r)\,  j_{n_3}( k_3 r)\,, $$ where $j_n(r)$ is the $n^{\rm th}$ order spherical Bessel function, $k_1$,$k_2$ and $k_3$ are real positive numbers and $n_1,n_2$ and $n_3$ are positive integers. The spherical Bessel function $j_n$ can be defined by  $$ j_n(x) = (-x)^n \left(\frac{1}{x}\frac{d}{dx}\right)^n\,\frac{\sin x}{x}.$$ Clue As an answer to this question , @joriki provided a nice solution for $n_1=n_2=n_3=0$. If I am to believe Mathematica again, for instance $$\int_0^\infty d r \,r^2\, j_2(  r)\, j_2( 2 r)\,  j_2( 3 r)=-\frac{\pi}{48}$$ and $$\int_0^\infty d r \,r^2\, j_2(  r)\, j_4( 2 r)\,  j_4( 3 r)=-\frac{\pi}{48}$$  so the integral seems possible. On the other hand, if some $n_i$ are odd the integral seems ill defined. I would guess that for odd indices the answer is $\pi/(8k_1 k_2 k_3)$ times some function of the signs of $n_1$, $n_2$ and $n_3$. Update My guess seems to be wrong. Symbolic integration for the first $8\times8\times 8 $ values of $(n_1,n_2,n_3)$ yields (with $k_1=k_2=k_3=1$)","As a follow up to this question I am also interested in a symbolic closed form for this integral $$\int_0^\infty d r \,r^2\, j_{n_1}( k_1 r)\, j_{n_2}( k_2 r)\,  j_{n_3}( k_3 r)\,, $$ where $j_n(r)$ is the $n^{\rm th}$ order spherical Bessel function, $k_1$,$k_2$ and $k_3$ are real positive numbers and $n_1,n_2$ and $n_3$ are positive integers. The spherical Bessel function $j_n$ can be defined by  $$ j_n(x) = (-x)^n \left(\frac{1}{x}\frac{d}{dx}\right)^n\,\frac{\sin x}{x}.$$ Clue As an answer to this question , @joriki provided a nice solution for $n_1=n_2=n_3=0$. If I am to believe Mathematica again, for instance $$\int_0^\infty d r \,r^2\, j_2(  r)\, j_2( 2 r)\,  j_2( 3 r)=-\frac{\pi}{48}$$ and $$\int_0^\infty d r \,r^2\, j_2(  r)\, j_4( 2 r)\,  j_4( 3 r)=-\frac{\pi}{48}$$  so the integral seems possible. On the other hand, if some $n_i$ are odd the integral seems ill defined. I would guess that for odd indices the answer is $\pi/(8k_1 k_2 k_3)$ times some function of the signs of $n_1$, $n_2$ and $n_3$. Update My guess seems to be wrong. Symbolic integration for the first $8\times8\times 8 $ values of $(n_1,n_2,n_3)$ yields (with $k_1=k_2=k_3=1$)",,"['analysis', 'integration']"
62,Integral over triple product of spherical Bessel functions,Integral over triple product of spherical Bessel functions,,"I would like to carry out symbolically the following integral $$\int_0^\infty d r \,r^2\, j_0( k r)\, j_0( k_1 r)\,  j_0( k_2 r)\,, $$ where $j_0(r)$ is the zeroth order spherical Bessel function and $k$,$k_1$ and $k_2$ are real numbers. Idea? I am wondering if I should use this expansion $$J_\alpha (\beta) = \sum_{m=0}^{\infty}\frac{(-1)^m}{m!\Gamma(m+\alpha +1)} \left(\frac{\beta}{2}\right)^{2m}$$ from this reference Clue If I am to believe Mathematica $$\int_0^\infty d r \,r^2\, j_0(  r)\, j_0( 2 r)\,  j_0( 3 r)=\frac{\pi}{48}$$ for instance, so the integral seems possible.","I would like to carry out symbolically the following integral $$\int_0^\infty d r \,r^2\, j_0( k r)\, j_0( k_1 r)\,  j_0( k_2 r)\,, $$ where $j_0(r)$ is the zeroth order spherical Bessel function and $k$,$k_1$ and $k_2$ are real numbers. Idea? I am wondering if I should use this expansion $$J_\alpha (\beta) = \sum_{m=0}^{\infty}\frac{(-1)^m}{m!\Gamma(m+\alpha +1)} \left(\frac{\beta}{2}\right)^{2m}$$ from this reference Clue If I am to believe Mathematica $$\int_0^\infty d r \,r^2\, j_0(  r)\, j_0( 2 r)\,  j_0( 3 r)=\frac{\pi}{48}$$ for instance, so the integral seems possible.",,"['analysis', 'integration']"
63,Explaining Ito formula to an analyst,Explaining Ito formula to an analyst,,"From the point of view of analysis, what is Ito formula? Is it an integral by substitution, or, a radon-nikodym derivative? Define the probability space  $$ \left(C\left(\Bbb R_+\right),\sigma\left(C\left(\Bbb R_+\right)\right),P\right), $$ where $P$ is the standard Brownian motion measure. Let $f(x)=x^2$, with Ito formula, I write $$ \int_{C\left(\Bbb R_+\right)} f(X_t)dP(X)=\int_{C\left(\Bbb R_+\right)} \left\{\int_0^t 2X_sdX_s+t \right\} dP\left(X\right). $$ The previous equation is my heuristic to explain Ito formula to an analyst. This heuristic is itself inspired by the following heuristic which I once heard Ito formula is a way of expressing how the measure $P$ changes from $X_t$ to $f(X_t)$","From the point of view of analysis, what is Ito formula? Is it an integral by substitution, or, a radon-nikodym derivative? Define the probability space  $$ \left(C\left(\Bbb R_+\right),\sigma\left(C\left(\Bbb R_+\right)\right),P\right), $$ where $P$ is the standard Brownian motion measure. Let $f(x)=x^2$, with Ito formula, I write $$ \int_{C\left(\Bbb R_+\right)} f(X_t)dP(X)=\int_{C\left(\Bbb R_+\right)} \left\{\int_0^t 2X_sdX_s+t \right\} dP\left(X\right). $$ The previous equation is my heuristic to explain Ito formula to an analyst. This heuristic is itself inspired by the following heuristic which I once heard Ito formula is a way of expressing how the measure $P$ changes from $X_t$ to $f(X_t)$",,['analysis']
64,Question about integral and unit step function,Question about integral and unit step function,,"The unit step function $I$ is defined by $$ I(x)= \begin{cases}0,\quad x \le 0, \\ 1,\quad x>0. \end{cases} $$ Let $f$ be continuous on $[a,b]$ and suppose $c_n\geq 0$ for $n=1, 2, 3,\ldots$ and $\sum_n c_n$ is convergent. Let $\alpha=\sum_{n=1}^{N} c_n I(x-s_n)$ where ${s_n}$ is a sequence of distinct points in $(a,b)$. Then  $$ \int_{a}^{b}fd\alpha=\sum_{i=1}^{N}c_n f(s_n). $$ I can't understand why the last equation holds. Where does $f(s_n)$ comes from?","The unit step function $I$ is defined by $$ I(x)= \begin{cases}0,\quad x \le 0, \\ 1,\quad x>0. \end{cases} $$ Let $f$ be continuous on $[a,b]$ and suppose $c_n\geq 0$ for $n=1, 2, 3,\ldots$ and $\sum_n c_n$ is convergent. Let $\alpha=\sum_{n=1}^{N} c_n I(x-s_n)$ where ${s_n}$ is a sequence of distinct points in $(a,b)$. Then  $$ \int_{a}^{b}fd\alpha=\sum_{i=1}^{N}c_n f(s_n). $$ I can't understand why the last equation holds. Where does $f(s_n)$ comes from?",,['analysis']
65,Proof of the irrationality of $\sqrt{3}$ - logic question,Proof of the irrationality of  - logic question,\sqrt{3},"Prove $\sqrt{3}$ is irrational. (Proof by contradiction). Let $\sqrt{3}$ be a rational number in simplest form $\frac pq$. So squaring both sides of $\sqrt{3}=\frac pq$ we get $3=(\frac {p}{q})^2$ which translates to $3=\frac{p^2}{q^2}$. Multiply both sides of the equation by $q^2$ yields $3q^2=p^2$. Now $p^2$ is taken to be divisible by 3 and thus an odd number, $p$ is also odd because any odd number squared is also odd. So let $p=3s$ where s is an integer. Then $3q^2=(3s)^2 = 3q^2=9s^2$. Dividing both sides of the equation by 3 leaves us with $q^2=3s^2$. Here is is taken that $q^2$ is divisible by 3 and is odd and so is $q$. Therefore both $q \text{ and}\; p$ have a common factor of being odd and divisible by 3, proving that the $\sqrt{3}$ is irrational. Are there any gaps that I could improve on?","Prove $\sqrt{3}$ is irrational. (Proof by contradiction). Let $\sqrt{3}$ be a rational number in simplest form $\frac pq$. So squaring both sides of $\sqrt{3}=\frac pq$ we get $3=(\frac {p}{q})^2$ which translates to $3=\frac{p^2}{q^2}$. Multiply both sides of the equation by $q^2$ yields $3q^2=p^2$. Now $p^2$ is taken to be divisible by 3 and thus an odd number, $p$ is also odd because any odd number squared is also odd. So let $p=3s$ where s is an integer. Then $3q^2=(3s)^2 = 3q^2=9s^2$. Dividing both sides of the equation by 3 leaves us with $q^2=3s^2$. Here is is taken that $q^2$ is divisible by 3 and is odd and so is $q$. Therefore both $q \text{ and}\; p$ have a common factor of being odd and divisible by 3, proving that the $\sqrt{3}$ is irrational. Are there any gaps that I could improve on?",,"['analysis', 'proof-writing']"
66,Problem with partition of unity in the Borel theorem,Problem with partition of unity in the Borel theorem,,"The Borel theorem says that for arbitrary sequence $(f_n)_{n=0}^\infty$  of smooth functions $f_n : \mathbb R\rightarrow \mathbb R$  with compact supports there exists a smooth function $F: \mathbb R^2 \rightarrow \mathbb R$ such that $\frac{\partial^n F}{\partial x^n}(0,y)=f_n(y)$ for $n=0,1,2,\ldots$, $y \in \mathbb R$. In Wikipedia (see here ) is stated  that the assumption that all $f_n$ have compact support is not important and  it could be obtained from the case when all $f_n$ have compact support by using partition of unity. I have a problem with understanding it. How to do it? Thanks","The Borel theorem says that for arbitrary sequence $(f_n)_{n=0}^\infty$  of smooth functions $f_n : \mathbb R\rightarrow \mathbb R$  with compact supports there exists a smooth function $F: \mathbb R^2 \rightarrow \mathbb R$ such that $\frac{\partial^n F}{\partial x^n}(0,y)=f_n(y)$ for $n=0,1,2,\ldots$, $y \in \mathbb R$. In Wikipedia (see here ) is stated  that the assumption that all $f_n$ have compact support is not important and  it could be obtained from the case when all $f_n$ have compact support by using partition of unity. I have a problem with understanding it. How to do it? Thanks",,['analysis']
67,"If $f\in C^1(\mathbb{R},M_n(\mathbb{R}))$ such that $f(0)=0$ and $f'(0)=I$, show that the image of f contains a regular matrix","If  such that  and , show that the image of f contains a regular matrix","f\in C^1(\mathbb{R},M_n(\mathbb{R})) f(0)=0 f'(0)=I","If $f\in C^1(\mathbb{R},M_n(\mathbb{R}))$ such that $f(0)=0$ and $f'(0)=I$, show that the image of f contains a regular matrix. While trying to prove something (elementary) from representation theory, I came to a stop. This fact would complete the proof. Can anyone prove it or find a counterexample? Thanks!","If $f\in C^1(\mathbb{R},M_n(\mathbb{R}))$ such that $f(0)=0$ and $f'(0)=I$, show that the image of f contains a regular matrix. While trying to prove something (elementary) from representation theory, I came to a stop. This fact would complete the proof. Can anyone prove it or find a counterexample? Thanks!",,['analysis']
68,When can a series be integrated term by term?,When can a series be integrated term by term?,,"I have a function that is defined as a harmonic series, I would like to integrate it over part of its domain.  I have been doing this by integrating term by term and summing the result, but I seem to remember something in little Rudin that gave conditions under which this is valid, however, if I could remember what it said I still don't think I understood it.  So my question is when is the following true? $$\int_a^b{\sum_{i}{f_i\left(x\right)}}=\sum_i{\int_a^b{f_i\left(x\right)}}$$","I have a function that is defined as a harmonic series, I would like to integrate it over part of its domain.  I have been doing this by integrating term by term and summing the result, but I seem to remember something in little Rudin that gave conditions under which this is valid, however, if I could remember what it said I still don't think I understood it.  So my question is when is the following true? $$\int_a^b{\sum_{i}{f_i\left(x\right)}}=\sum_i{\int_a^b{f_i\left(x\right)}}$$",,"['calculus', 'real-analysis', 'analysis']"
69,Question from Rudin: Jensen?,Question from Rudin: Jensen?,,"I came across this question from Rudin's Real and Complex Analysis , 3rd Edition (p.75 # 25) ""Suppose that $\mu$ is a positive measure on $X$ and $f:X\rightarrow (0,\infty)$ satisfies $\int_X f d\mu = 1$ .  Prove for every $E \subset X$ with $0 < \mu(E) < \infty$ that $\int_E(\log{f})d\mu \leq \mu(E)\log{\frac{1}{\mu(E)}} $ Also, when $0<p<1$ , we have $\int_E{f^p}d\mu \leq \mu(E)^{1-p}$ ."" My first thought was that since log is a concave function, we can use Jensen's inequality (in the opposite direction), but that is not giving me what I want.  Any suggestions? Further Addendum : Jensen's inequality only works on a set of measure 1 (or by redefining an interval to get a measure 1 set) so this is clearly not the correct approach.","I came across this question from Rudin's Real and Complex Analysis , 3rd Edition (p.75 # 25) ""Suppose that is a positive measure on and satisfies .  Prove for every with that Also, when , we have ."" My first thought was that since log is a concave function, we can use Jensen's inequality (in the opposite direction), but that is not giving me what I want.  Any suggestions? Further Addendum : Jensen's inequality only works on a set of measure 1 (or by redefining an interval to get a measure 1 set) so this is clearly not the correct approach.","\mu X f:X\rightarrow (0,\infty) \int_X f d\mu = 1 E \subset X 0 < \mu(E) < \infty \int_E(\log{f})d\mu \leq \mu(E)\log{\frac{1}{\mu(E)}}  0<p<1 \int_E{f^p}d\mu \leq \mu(E)^{1-p}","['real-analysis', 'analysis', 'measure-theory', 'inequality']"
70,Uniform convergence of polynomial with degree less than N+1,Uniform convergence of polynomial with degree less than N+1,,"In the space of polynomials with degree less than $N+1$: $\operatorname{span}\{ 1,x,\ldots,x^N\}$ defined on $[a,b]$, if a  sequence has a uniform convergent limit(under maximum norm in continuous function space). 1) Is the limit a polynomial? 2) Is the limit polynomial's degree less than $N+1$? 3) Do the coeffcients of polynomial sequence converge?","In the space of polynomials with degree less than $N+1$: $\operatorname{span}\{ 1,x,\ldots,x^N\}$ defined on $[a,b]$, if a  sequence has a uniform convergent limit(under maximum norm in continuous function space). 1) Is the limit a polynomial? 2) Is the limit polynomial's degree less than $N+1$? 3) Do the coeffcients of polynomial sequence converge?",,"['real-analysis', 'analysis']"
71,Questions concerning a proof that $\mathcal{D}$ is dense in $\mathcal{S}$.,Questions concerning a proof that  is dense in .,\mathcal{D} \mathcal{S},"I am currently working through this lecture notes and on page 164, there it is said The space of $\mathcal{D}(\mathbb{R}^n)$ of smooth complex-valued functions with compact support is contained in the Schwartz space $\mathcal{S}(\mathbb{R}^n)$. If $f_k \to f$ in $\mathcal{D}$, then $f_k \to f$ in $\mathcal{S}$, so $\mathcal{D}$ is continuously embedded in $\mathcal{S}$. Furthermore, if $f\in \mathcal{S}$, and $\eta \in C_c^{\infty}(\mathbb{R}^n)$ is a cutoff function with $\eta_k(x) = \eta(x/k)$, then $\eta_k f \to f$ in $\mathcal{S}$ as $k \to \infty$, so $\mathcal{D}$ is dense in $\mathcal{S}$. I don't understand the arguments in this paragraph, for a subset $\mathcal D$ of $\mathcal S$ to be dense in $\mathcal S$ for every element $s$ of $\mathcal S$ I need to find a sequence in $\mathcal D$ which converges to $s$, but there just stands that $\eta_k f \to f$ in $\mathcal{S}$, but what i need is a sequence in $\mathcal{D}$ not in $\mathcal{S}$, so why does it follow from this that $\mathcal{D}$ is dense in $\mathcal{S}$?","I am currently working through this lecture notes and on page 164, there it is said The space of $\mathcal{D}(\mathbb{R}^n)$ of smooth complex-valued functions with compact support is contained in the Schwartz space $\mathcal{S}(\mathbb{R}^n)$. If $f_k \to f$ in $\mathcal{D}$, then $f_k \to f$ in $\mathcal{S}$, so $\mathcal{D}$ is continuously embedded in $\mathcal{S}$. Furthermore, if $f\in \mathcal{S}$, and $\eta \in C_c^{\infty}(\mathbb{R}^n)$ is a cutoff function with $\eta_k(x) = \eta(x/k)$, then $\eta_k f \to f$ in $\mathcal{S}$ as $k \to \infty$, so $\mathcal{D}$ is dense in $\mathcal{S}$. I don't understand the arguments in this paragraph, for a subset $\mathcal D$ of $\mathcal S$ to be dense in $\mathcal S$ for every element $s$ of $\mathcal S$ I need to find a sequence in $\mathcal D$ which converges to $s$, but there just stands that $\eta_k f \to f$ in $\mathcal{S}$, but what i need is a sequence in $\mathcal{D}$ not in $\mathcal{S}$, so why does it follow from this that $\mathcal{D}$ is dense in $\mathcal{S}$?",,"['analysis', 'partial-differential-equations', 'distribution-theory']"
72,A problem in elementary analysis,A problem in elementary analysis,,"A friend asked me this problem but I couldn't solve.  Not homework. Given reals $x>1$, $\epsilon>0$, does there always exist $i,n \in \mathbb N$ such that $|x^i -n |<\epsilon$.  If the answer is in the   negative, is there a nontrivial subset  $S \subset (1,\infty)$ such   that all $x\in S$ satisfy this?","A friend asked me this problem but I couldn't solve.  Not homework. Given reals $x>1$, $\epsilon>0$, does there always exist $i,n \in \mathbb N$ such that $|x^i -n |<\epsilon$.  If the answer is in the   negative, is there a nontrivial subset  $S \subset (1,\infty)$ such   that all $x\in S$ satisfy this?",,"['real-analysis', 'analysis']"
73,A question about Lagrange multiplier,A question about Lagrange multiplier,,"Is there any explanation or interpretation of the concept of Lagrange multipliers $\nabla f(x_0)= \delta \nabla g(x_0) $ for some constant $\delta$ and $f$ is a differentiable function and g is the constraint of $f$. I know that this comes from the proof and I have read the proof, but does it give any geometrical meaning? Also, it is possible that $\nabla f(x_0)= \delta \nabla g(x_0) $ but $x_0$ doesn't give extremal points?","Is there any explanation or interpretation of the concept of Lagrange multipliers $\nabla f(x_0)= \delta \nabla g(x_0) $ for some constant $\delta$ and $f$ is a differentiable function and g is the constraint of $f$. I know that this comes from the proof and I have read the proof, but does it give any geometrical meaning? Also, it is possible that $\nabla f(x_0)= \delta \nabla g(x_0) $ but $x_0$ doesn't give extremal points?",,"['calculus', 'analysis', 'multivariable-calculus']"
74,Show that $\prod_{i=1}^n a_i- \prod_{j=1}^n b_i =$ $\sum_{t=1}^{n-1}(\prod_{i\leq t-1}a_i)(\prod_{j\geq t+1} b_j)(a_t-b_t)$,Show that,\prod_{i=1}^n a_i- \prod_{j=1}^n b_i = \sum_{t=1}^{n-1}(\prod_{i\leq t-1}a_i)(\prod_{j\geq t+1} b_j)(a_t-b_t),"Pardon the cryptic notation and possibly trivial question. I believe the following holds. Define $$X_t=(\prod_{i\leq t-1}a_i)(\prod_{j\geq t+1} b_j)(a_t-b_t).$$    Show that $$\prod_{i=1}^na_i-\prod_{j=1}^nb_i=\sum_{t=1}^{n-1}X_t.$$ Is there a quick and elegant way of seeing this? It's straightforward by induction and I think as well by multilinearity of determinants and possibly even by inclusion-exclusion. Moreover, it should be also possible from expanding $(a_1-b_1)(a_2-b_2)...(a_{n-1}-b_{n-1})(a_n+(-1)^nb_n)$.","Pardon the cryptic notation and possibly trivial question. I believe the following holds. Define $$X_t=(\prod_{i\leq t-1}a_i)(\prod_{j\geq t+1} b_j)(a_t-b_t).$$    Show that $$\prod_{i=1}^na_i-\prod_{j=1}^nb_i=\sum_{t=1}^{n-1}X_t.$$ Is there a quick and elegant way of seeing this? It's straightforward by induction and I think as well by multilinearity of determinants and possibly even by inclusion-exclusion. Moreover, it should be also possible from expanding $(a_1-b_1)(a_2-b_2)...(a_{n-1}-b_{n-1})(a_n+(-1)^nb_n)$.",,"['real-analysis', 'analysis', 'products']"
75,hadamard inequality,hadamard inequality,,"It is given that: $\left |\det(A) \right |\leq n^{\frac{-n}{2}}\left \| A \right \|^{n}$ where $A$ is an $n$ by $n$ matrix, and $\left \| A \right \|$ is the Hilbert Schmidt norm  (i.e: $\left \| A \right \|=\left ( \sum_{i,j=1}^{n}a_{ij}^{2} \right )^{\frac{1}{2}}$). Now, I want to prove the following inequality based on the above one: $$\left |\det(A) \right |\leq \prod_{j=1}^{n}\left ( \sum_{i=1}^n a_{ij}^2\right )^{\frac 12}.$$ Any help?","It is given that: $\left |\det(A) \right |\leq n^{\frac{-n}{2}}\left \| A \right \|^{n}$ where $A$ is an $n$ by $n$ matrix, and $\left \| A \right \|$ is the Hilbert Schmidt norm  (i.e: $\left \| A \right \|=\left ( \sum_{i,j=1}^{n}a_{ij}^{2} \right )^{\frac{1}{2}}$). Now, I want to prove the following inequality based on the above one: $$\left |\det(A) \right |\leq \prod_{j=1}^{n}\left ( \sum_{i=1}^n a_{ij}^2\right )^{\frac 12}.$$ Any help?",,"['linear-algebra', 'real-analysis', 'analysis', 'multivariable-calculus']"
76,Clarkson type inequality,Clarkson type inequality,,"Is it true that for $p\in (1,2)$ the following inequalities holds: $$ 2^{p-1} (|x|^p+|y|^p)\leq |x+y|^p+|x-y|^p \leq 2 (|x|^p+|y|^p)$$ for $x, y \in \mathbb{R}$ ? Thanks.","Is it true that for $p\in (1,2)$ the following inequalities holds: $$ 2^{p-1} (|x|^p+|y|^p)\leq |x+y|^p+|x-y|^p \leq 2 (|x|^p+|y|^p)$$ for $x, y \in \mathbb{R}$ ? Thanks.",,"['analysis', 'inequality']"
77,Basic properties of the point-to-set distance function,Basic properties of the point-to-set distance function,,"Let $X$ be a normed vector space, $x\in X$ and $Z\subseteq X$. Then we define the point-to-set distance function as: $$ \|x\|_Z = \inf_{z\in Z} \| x-z\| $$ I use the notation $\|\cdot\|_Z$ for convenience without implying that the operation is a norm. Assuming that $Z$ is convex we may not conclude that: $$ \|x+y\|_Z \leq \|x\|_Z + \|y\|_Z $$ What sort of properties should $Z$ possess so that the triangle inequality holds for all $x\in X$? For example, $Z$ being a singleton brings about the desired inequality but I'm looking for a less trivial example. Additionally, let $A\in M_n(\mathbb{R})$ be a matrix and let $Z\neq X$. I noticed that we may define: $$ \|A\|_Z:= \sup_{x\in\mathbb{R}^n,x\notin Z}\frac{\|Ax\|_Z}{\|x\|_Z} $$ while $\|A\|_X:=0$. Then, it should be true that: $$ \|Ax\|_Z \leq \|A\|_Z\cdot\|x\|_Z $$ So, this is one norm-like property for $\|\cdot\|_Z$. What other properties of $\|\cdot\|_Z$ can we deduce based on assumed properties of $Z$? Update: As Davide Giraudo pointed out, the triangle inequality holds true in case $Z$ is a linear subspace of $X$. It seems that it holds for convex cones too (for nonconvex cones it's easy to find a counter example on the plane). Let us assume that $Z$ is a convex cone. Then: $$ \|x+y\|_Z=\inf_{z\in Z}\|x+y-z\|\leq \|x+y-(z_1+z_2)\| $$ where $z_1,z_2\in Z$ and we proceed as in David's answer. Here, we used the fact that whenever $Z$ is a convex cone, then $z_1,z_2\in Z \Rightarrow z_1+z_2\in Z$ so any vector in $Z$ can be decomposed into a sum of vectors of $Z$ and vice versa. Question: Would be nice to identify other classes of sets for which the induced point-to-set distance satisfies the triangle inequality.","Let $X$ be a normed vector space, $x\in X$ and $Z\subseteq X$. Then we define the point-to-set distance function as: $$ \|x\|_Z = \inf_{z\in Z} \| x-z\| $$ I use the notation $\|\cdot\|_Z$ for convenience without implying that the operation is a norm. Assuming that $Z$ is convex we may not conclude that: $$ \|x+y\|_Z \leq \|x\|_Z + \|y\|_Z $$ What sort of properties should $Z$ possess so that the triangle inequality holds for all $x\in X$? For example, $Z$ being a singleton brings about the desired inequality but I'm looking for a less trivial example. Additionally, let $A\in M_n(\mathbb{R})$ be a matrix and let $Z\neq X$. I noticed that we may define: $$ \|A\|_Z:= \sup_{x\in\mathbb{R}^n,x\notin Z}\frac{\|Ax\|_Z}{\|x\|_Z} $$ while $\|A\|_X:=0$. Then, it should be true that: $$ \|Ax\|_Z \leq \|A\|_Z\cdot\|x\|_Z $$ So, this is one norm-like property for $\|\cdot\|_Z$. What other properties of $\|\cdot\|_Z$ can we deduce based on assumed properties of $Z$? Update: As Davide Giraudo pointed out, the triangle inequality holds true in case $Z$ is a linear subspace of $X$. It seems that it holds for convex cones too (for nonconvex cones it's easy to find a counter example on the plane). Let us assume that $Z$ is a convex cone. Then: $$ \|x+y\|_Z=\inf_{z\in Z}\|x+y-z\|\leq \|x+y-(z_1+z_2)\| $$ where $z_1,z_2\in Z$ and we proceed as in David's answer. Here, we used the fact that whenever $Z$ is a convex cone, then $z_1,z_2\in Z \Rightarrow z_1+z_2\in Z$ so any vector in $Z$ can be decomposed into a sum of vectors of $Z$ and vice versa. Question: Would be nice to identify other classes of sets for which the induced point-to-set distance satisfies the triangle inequality.",,"['real-analysis', 'analysis', 'convex-analysis']"
78,Implicit Function Theorem computation problem,Implicit Function Theorem computation problem,,"Problem 1, page 78 of Munkres ( Analysis on Manifolds ): Let $f: \mathbb{R}^3 \to \mathbb{R}^2$ be of class $C^1$; write $f$ in the form $f(x,y_1,y_2)$. Assume that $f(3,-1,2) = \mathbf{0}$ and   $$ Df(3,-1,2) = \begin{pmatrix} 1 & 2 & 1 \\ 1 & -1 & 1 \end{pmatrix}.$$ (a) Show that there is a function $g: B \to \mathbb{R}^2$ of class $C^1$ defined on an open set $B$ in $\mathbb{R}$ such that $f(x,g_1(x),g_2(x)) = \mathbf{0}$ for $x \in B$ and $g(3) = (-1,2)$. (b) Find $Dg(3)$. (c) Discuss the problem of solving the equation $f(x,y_1,y_2) = \mathbf{0}$ for an arbitrary pair of unknowns in terms of the third, near the point $(3,-1,2)$. Here's what I have so far: Let $b=(-1,2)$ so that $a = (3,-1,2)$. Write $f(x,y_1,y_2)$ with $y = (y_1,y_2)$ then, a =3, and b = (-1,2) and determinant partial of $f$ w.r.t partial of $y (3,-1,2) =$ ? $$ \det \begin{pmatrix} \frac{\partial f_1}{\partial y_1}(a,b) & \frac{\partial f_1}{\partial y_2}(a,b) \\ \frac{\partial f_2}{\partial y_1}(a,b) & \frac{\partial f_2}{\partial y_2}(a,b) \end{pmatrix}  $$ Derivative of partial of f = [partial of f w.r.t x   partial of f w.r.t. y] implies Df(3,b) = (Stuck on evaluating this), but I know it is the expression above which I have wrote and what is partial of f1 w.r.t. y1 (a,b), partial of f2 w.r.t. y2 (a,b),  partial of f1 w.r.t. y2 (a,b), and partial of f2 w.r.t. y2(a,b)? For part b: Dg(3) = -{partial of f w.r.t y(3,b)]^-1 [partial of f w.r.t. x1]  at (3,b) =? for part c, I thought of taking 2 variables u and v s.t. partial of f w.r.t partial of (u,v) is not equal to zero. Since $f(a)$ is zero then by the implicit function theorem, there is a neighborhood $B$ of $(-1,2)$ in $\mathbb{R}^2$ and a unique function $g: B \to \mathbb{R}^3$ so that $g(a) = b$.","Problem 1, page 78 of Munkres ( Analysis on Manifolds ): Let $f: \mathbb{R}^3 \to \mathbb{R}^2$ be of class $C^1$; write $f$ in the form $f(x,y_1,y_2)$. Assume that $f(3,-1,2) = \mathbf{0}$ and   $$ Df(3,-1,2) = \begin{pmatrix} 1 & 2 & 1 \\ 1 & -1 & 1 \end{pmatrix}.$$ (a) Show that there is a function $g: B \to \mathbb{R}^2$ of class $C^1$ defined on an open set $B$ in $\mathbb{R}$ such that $f(x,g_1(x),g_2(x)) = \mathbf{0}$ for $x \in B$ and $g(3) = (-1,2)$. (b) Find $Dg(3)$. (c) Discuss the problem of solving the equation $f(x,y_1,y_2) = \mathbf{0}$ for an arbitrary pair of unknowns in terms of the third, near the point $(3,-1,2)$. Here's what I have so far: Let $b=(-1,2)$ so that $a = (3,-1,2)$. Write $f(x,y_1,y_2)$ with $y = (y_1,y_2)$ then, a =3, and b = (-1,2) and determinant partial of $f$ w.r.t partial of $y (3,-1,2) =$ ? $$ \det \begin{pmatrix} \frac{\partial f_1}{\partial y_1}(a,b) & \frac{\partial f_1}{\partial y_2}(a,b) \\ \frac{\partial f_2}{\partial y_1}(a,b) & \frac{\partial f_2}{\partial y_2}(a,b) \end{pmatrix}  $$ Derivative of partial of f = [partial of f w.r.t x   partial of f w.r.t. y] implies Df(3,b) = (Stuck on evaluating this), but I know it is the expression above which I have wrote and what is partial of f1 w.r.t. y1 (a,b), partial of f2 w.r.t. y2 (a,b),  partial of f1 w.r.t. y2 (a,b), and partial of f2 w.r.t. y2(a,b)? For part b: Dg(3) = -{partial of f w.r.t y(3,b)]^-1 [partial of f w.r.t. x1]  at (3,b) =? for part c, I thought of taking 2 variables u and v s.t. partial of f w.r.t partial of (u,v) is not equal to zero. Since $f(a)$ is zero then by the implicit function theorem, there is a neighborhood $B$ of $(-1,2)$ in $\mathbb{R}^2$ and a unique function $g: B \to \mathbb{R}^3$ so that $g(a) = b$.",,['analysis']
79,Derivative of multivariable function,Derivative of multivariable function,,"If $f$ is a function from $\mathbb{R}^n$ to $\mathbb{R}$, then its derivative at a point $\mathbf{u}$, $f'(\mathbf{u})$ is a linear transformation from $\mathbb{R}^n$ to $\mathbb{R}$. But we know that any linear transformation $T$ from $\mathbb{R}^n$ to $\mathbb{R}$ is of the form $T\mathbf{x}=\mathbf{x}\cdot \mathbf{y}$. Hence $f'(\mathbf{u})\mathbf{x}=\mathbf{x}\cdot \mathbf{y}$ for some $\mathbf{y}\in \mathbb{R}^n$. Would there be any connection between the vectors $\mathbf{u}$ and $\mathbf{y}$?","If $f$ is a function from $\mathbb{R}^n$ to $\mathbb{R}$, then its derivative at a point $\mathbf{u}$, $f'(\mathbf{u})$ is a linear transformation from $\mathbb{R}^n$ to $\mathbb{R}$. But we know that any linear transformation $T$ from $\mathbb{R}^n$ to $\mathbb{R}$ is of the form $T\mathbf{x}=\mathbf{x}\cdot \mathbf{y}$. Hence $f'(\mathbf{u})\mathbf{x}=\mathbf{x}\cdot \mathbf{y}$ for some $\mathbf{y}\in \mathbb{R}^n$. Would there be any connection between the vectors $\mathbf{u}$ and $\mathbf{y}$?",,"['analysis', 'multivariable-calculus']"
80,"If $f$ is a $C^k$ diffeomorphism, then $f^{-1}$ is $C^k$ too.","If  is a  diffeomorphism, then  is  too.",f C^k f^{-1} C^k,"I don't understand the reason for the conclusion, written in boldface at the end, in the following argument (taken from Elon LIMA, Curso De Análise , Vol .2). If $f:U\longrightarrow V\subset \mathbb{R}^m$ is a diffemorphism which is $C^{k}$, then $g=f^{-1}$ is $C^k$ as well: indeed, by the chain rule, $g'(y)=(Inv\circ f'\circ g) (y)$, where $Inv$ is a $C^{\infty}$ map from $GL(\mathbb{R}^n)$ onto itself. And, since $f$ is $C^k$, it follows, from these facts , that $g$ is $C^k$. Why? Thank you.","I don't understand the reason for the conclusion, written in boldface at the end, in the following argument (taken from Elon LIMA, Curso De Análise , Vol .2). If $f:U\longrightarrow V\subset \mathbb{R}^m$ is a diffemorphism which is $C^{k}$, then $g=f^{-1}$ is $C^k$ as well: indeed, by the chain rule, $g'(y)=(Inv\circ f'\circ g) (y)$, where $Inv$ is a $C^{\infty}$ map from $GL(\mathbb{R}^n)$ onto itself. And, since $f$ is $C^k$, it follows, from these facts , that $g$ is $C^k$. Why? Thank you.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
81,Not sure if this is a valid way of proving local maximum,Not sure if this is a valid way of proving local maximum,,"I am unsure if my argument makes sense: Let $f(x) = 0$ if $x$ is irrational and $f(x) = \frac{1}{q}$ if $x = \frac{p}{q}$ with $p,q$ in lowest terms. The exercise asks to find all local maximum and minimum: I see that the irrationals are all local minimum and that in particular the natural numbers are all local maximum. Now, I convinced myself that any other rational is indeed also a local maximum. However the proof is what I am not so sure about... I said that given a rational number $x = \frac{p}{q} $ , if you look at numbers that are not further than $\delta := Min(|p'/q'-x|,gcd(p',q') = 1, q' < q) $ then by definition of $\delta$ there will be no number with greater value of $f$ .","I am unsure if my argument makes sense: Let if is irrational and if with in lowest terms. The exercise asks to find all local maximum and minimum: I see that the irrationals are all local minimum and that in particular the natural numbers are all local maximum. Now, I convinced myself that any other rational is indeed also a local maximum. However the proof is what I am not so sure about... I said that given a rational number , if you look at numbers that are not further than then by definition of there will be no number with greater value of .","f(x) = 0 x f(x) = \frac{1}{q} x = \frac{p}{q} p,q x = \frac{p}{q}  \delta := Min(|p'/q'-x|,gcd(p',q') = 1, q' < q)  \delta f","['real-analysis', 'analysis', 'solution-verification']"
82,"How to prove continuity of $g(y)=\max_{x\in K} f(x,y)$ when $K$ is compact?",How to prove continuity of  when  is compact?,"g(y)=\max_{x\in K} f(x,y) K","I have a function $f$ that is continuous over the set of variables, $f: \ K \times M \to \mathbb{R}$ , where $K$ is a compact domain of a metric space and $M$ is a metric space. And I want to prove that the function $g(y)=\sup_{x\in K} f(x,y)$ is continuous. As $K$ is a compact, $f$ is uniformly continuous with respect to its first argument. Also, for the same reason, $g(y)=\sup_{x\in K} f(x,y)=\max_{x\in K} f(x,y)$ . For now, I know that for the case of $f$ is continuous with respect of each variable separately, this doesn't work (see an example here Is supremum over a compact domain of separately continuous function continuous? ) and there is a proof for the case when $M$ is also a compact, or $f$ is just uniformly continuous on $M$ , too ( How prove this $g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$ is continuous on $[0,1]$ ), which I personally doubt as there is no guatantee that y-s will be close there (in my notations they are x-s). All my tries fail when I come to the point where I need to say something about argmaxes of even close y-s. For example: I consider a sequence $y_n \to y_0$ with $n\to\infty$ . For each $y_n$ there exists $x_n$ such that $g(y_n)=\max_{x\in K} f(x,y_n)=f(x_n,y_n).$ Then, I try to estimate the difference $$|g(y_{n+k})-g(y_n)|=|f(x_{n+k},y_{n+k})-f(x_n,y_n)|=|f(x_{n+k},y_{n+k})-f(x_{n+k},y_{n})+f(x_{n+k},y_{n})-f(x_n,y_n)|\le|f(x_{n+k},y_{n+k})-f(x_{n+k},y_{n})|+|f(x_{n+k},y_{n})-f(x_n,y_n)|.$$ One can easily see that the first component tends to zero with $k\to\infty$ , but I have no idea what to do with the second one... Any help would be very appreciated!","I have a function that is continuous over the set of variables, , where is a compact domain of a metric space and is a metric space. And I want to prove that the function is continuous. As is a compact, is uniformly continuous with respect to its first argument. Also, for the same reason, . For now, I know that for the case of is continuous with respect of each variable separately, this doesn't work (see an example here Is supremum over a compact domain of separately continuous function continuous? ) and there is a proof for the case when is also a compact, or is just uniformly continuous on , too ( How prove this $g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$ is continuous on $[0,1]$ ), which I personally doubt as there is no guatantee that y-s will be close there (in my notations they are x-s). All my tries fail when I come to the point where I need to say something about argmaxes of even close y-s. For example: I consider a sequence with . For each there exists such that Then, I try to estimate the difference One can easily see that the first component tends to zero with , but I have no idea what to do with the second one... Any help would be very appreciated!","f f: \ K \times M \to \mathbb{R} K M g(y)=\sup_{x\in K} f(x,y) K f g(y)=\sup_{x\in K} f(x,y)=\max_{x\in K} f(x,y) f M f M y_n \to y_0 n\to\infty y_n x_n g(y_n)=\max_{x\in K} f(x,y_n)=f(x_n,y_n). |g(y_{n+k})-g(y_n)|=|f(x_{n+k},y_{n+k})-f(x_n,y_n)|=|f(x_{n+k},y_{n+k})-f(x_{n+k},y_{n})+f(x_{n+k},y_{n})-f(x_n,y_n)|\le|f(x_{n+k},y_{n+k})-f(x_{n+k},y_{n})|+|f(x_{n+k},y_{n})-f(x_n,y_n)|. k\to\infty","['calculus', 'analysis', 'multivariable-calculus', 'continuity', 'multivalued-functions']"
83,Does a set of full measure contain an affine copy of any countable set?,Does a set of full measure contain an affine copy of any countable set?,,"In this paper here (beginning of Section 1.3 at page 4) it is stated that any measureable set $E\subset[0,1]$ with $\lambda(E)=1$ contains an affine copy of any countable set $A\subset \mathbb{R}$ . Is the same true in multiple dimensions? I.e. does any measureable set $E\subset[0,1]^n$ with $\lambda^n(E)=1$ contain a copy of any countable set $A\subset \mathbb{R}^n$ ?",In this paper here (beginning of Section 1.3 at page 4) it is stated that any measureable set with contains an affine copy of any countable set . Is the same true in multiple dimensions? I.e. does any measureable set with contain a copy of any countable set ?,"E\subset[0,1] \lambda(E)=1 A\subset \mathbb{R} E\subset[0,1]^n \lambda^n(E)=1 A\subset \mathbb{R}^n","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
84,Examples of PDEs that found application after discovery,Examples of PDEs that found application after discovery,,"I am a 2nd year mathematics undergraduate in the UK and recently took an introductory elective in partial differential equations. The focus was on solving some classical examples, all arising from an application - this seems to be typical of most such introductory courses. From what I can see of more advanced PDE-focused electives, my impression is they seem to follow a similar trend,  'inspecting' or solving the PDE more rigorously with more advanced analysis, but mostly still application focused ie 'this type of PDE arises in this modelling problem/application, we study these properties and use these techniques to solve it' . My main question is, despite the general motivation of application, are there any well-known PDEs that were first discovered and studied from a pure perspective and then later found applications ? In other words, are there any notable examples of PDEs that could belong here ? A followup question from this would then be, are there any examples currently in the first stage, but not the second (ie derived and studied entirely in pure mathematics, but don't really have any obvious application yet)?","I am a 2nd year mathematics undergraduate in the UK and recently took an introductory elective in partial differential equations. The focus was on solving some classical examples, all arising from an application - this seems to be typical of most such introductory courses. From what I can see of more advanced PDE-focused electives, my impression is they seem to follow a similar trend,  'inspecting' or solving the PDE more rigorously with more advanced analysis, but mostly still application focused ie 'this type of PDE arises in this modelling problem/application, we study these properties and use these techniques to solve it' . My main question is, despite the general motivation of application, are there any well-known PDEs that were first discovered and studied from a pure perspective and then later found applications ? In other words, are there any notable examples of PDEs that could belong here ? A followup question from this would then be, are there any examples currently in the first stage, but not the second (ie derived and studied entirely in pure mathematics, but don't really have any obvious application yet)?",,"['analysis', 'partial-differential-equations', 'examples-counterexamples', 'math-history', 'mathematical-modeling']"
85,Uniform Taylor expansion,Uniform Taylor expansion,,"$f \colon \mathbb R^n \to \mathbb R$ is  differentiable in $x_0$ if there exists a functional $L$ $$f(x_0+h)-f(x_0)-Lh=o(|h|),$$ as $|h|\to 0.$ Here $o(|h|)$ denotes a function going to $0$ faster than $|h|$ depends on $x_0.$ Denote $Df(x_0)=L$ . $f$ is continuously differentiable in $\mathbb R^n$ if it  is differentiable in $x_0$ for every $x_0$ and $Df(x)$ is continuous on $\mathbb R^n.$ What are the conditions guaranteing that $o$ is independent of $x_0$ ? Intuitively it should be that $Df(x)$ is uniformly continuous, is that the case?","is  differentiable in if there exists a functional as Here denotes a function going to faster than depends on Denote . is continuously differentiable in if it  is differentiable in for every and is continuous on What are the conditions guaranteing that is independent of ? Intuitively it should be that is uniformly continuous, is that the case?","f \colon \mathbb R^n \to \mathbb R x_0 L f(x_0+h)-f(x_0)-Lh=o(|h|), |h|\to 0. o(|h|) 0 |h| x_0. Df(x_0)=L f \mathbb R^n x_0 x_0 Df(x) \mathbb R^n. o x_0 Df(x)","['real-analysis', 'analysis', 'derivatives', 'frechet-derivative']"
86,Integral of $\int x^5 \cdot \sqrt{2 - x^3}dx$,Integral of,\int x^5 \cdot \sqrt{2 - x^3}dx,"I have been working on this integral and cannot seem to notice an error I am making. Thanks for any advice in advance! For the following integral $$ \int x^5\sqrt{2 - x^3}\,dx  $$ I considered the $u$ -substitution as such $$ u = x^3,  \quad  du = 3x^2\,dx $$ Then I rewrote the integral as follows: $$ \frac{1}{3}\int u\sqrt{2 - u}\,du $$ This I integrated by parts as follows \begin{align} \frac{1}{3}\int u\sqrt{2 - u}\, du  &= \frac{1}{3}\left(  -\frac{2u\left(2-u\right)^{\frac{3}{2}}}{3}  + \frac{2}{3}\int\left(2-u\right)^{\frac{3}{2}}\,  dx \right) \\  &= -\frac{2x^3\left(  2-x^3\right)^{\frac{3}{2}}}{9}  - \frac{4\left(2-x^3\right)^{\frac{5}{2}}}{45} + C  \end{align} where $C$ is the integrating constant. The issue is that WolframAlpha obtains a different answer as such: $$ \frac{2}{-9} (2-x^3)^{2/3} \left(x^3+\frac{2}{3}\right) + D $$ I was expecting it to only differ by the integrating constant but when I graphed them I realised they are not the same. Thanks for any insight into my mistake! Update: The mistake was found to be in the WolframAlpha interpreter as it was interpreted in the wrong manner, despite the fact that the only different thing I used was a different variable. In conclusion, be sure to put in precise and clear input!","I have been working on this integral and cannot seem to notice an error I am making. Thanks for any advice in advance! For the following integral I considered the -substitution as such Then I rewrote the integral as follows: This I integrated by parts as follows where is the integrating constant. The issue is that WolframAlpha obtains a different answer as such: I was expecting it to only differ by the integrating constant but when I graphed them I realised they are not the same. Thanks for any insight into my mistake! Update: The mistake was found to be in the WolframAlpha interpreter as it was interpreted in the wrong manner, despite the fact that the only different thing I used was a different variable. In conclusion, be sure to put in precise and clear input!","
\int x^5\sqrt{2 - x^3}\,dx 
 u 
u = x^3, 
\quad 
du = 3x^2\,dx
 
\frac{1}{3}\int u\sqrt{2 - u}\,du
 \begin{align}
\frac{1}{3}\int u\sqrt{2 - u}\, du 
&= \frac{1}{3}\left( 
-\frac{2u\left(2-u\right)^{\frac{3}{2}}}{3} 
+ \frac{2}{3}\int\left(2-u\right)^{\frac{3}{2}}\, 
dx \right) \\ 
&= -\frac{2x^3\left( 
2-x^3\right)^{\frac{3}{2}}}{9} 
- \frac{4\left(2-x^3\right)^{\frac{5}{2}}}{45} + C 
\end{align} C 
\frac{2}{-9} (2-x^3)^{2/3} \left(x^3+\frac{2}{3}\right) + D
","['real-analysis', 'calculus', 'integration', 'analysis', 'indefinite-integrals']"
87,A function with negative discontinuous derivative and many zeros,A function with negative discontinuous derivative and many zeros,,"I want to construct an example of a function $f:\mathbb{R} \to \mathbb{R}$ with the following properties. By the way, I am not sure if it can exist. $f(0) = 0$ $f'(0) < 0$ $f$ is continuous at $x=0$ $f'$ is not continuous at $x=0$ There is a sequence $x_k$ converging to $0$ such that $f(x_k)=0$ The following famous example $$ f(x) =  \begin{cases}  x^2 \sin(1/x) & x \neq 0 \\ 0 & x=0.  \end{cases} $$ satisfies all the properties except $2$ . I thought that modifying it like $$ f(x) =  \begin{cases}  x^2 \sin(1/x) - x & x \neq 0 \\ 0 & x=0.  \end{cases} $$ will make the derivative negative at $x=0$ but it destroys the property $5$ . How should I construct such an example? Or maybe it does not even exist?","I want to construct an example of a function with the following properties. By the way, I am not sure if it can exist. is continuous at is not continuous at There is a sequence converging to such that The following famous example satisfies all the properties except . I thought that modifying it like will make the derivative negative at but it destroys the property . How should I construct such an example? Or maybe it does not even exist?","f:\mathbb{R} \to \mathbb{R} f(0) = 0 f'(0) < 0 f x=0 f' x=0 x_k 0 f(x_k)=0 
f(x) = 
\begin{cases} 
x^2 \sin(1/x) & x \neq 0 \\
0 & x=0. 
\end{cases}
 2 
f(x) = 
\begin{cases} 
x^2 \sin(1/x) - x & x \neq 0 \\
0 & x=0. 
\end{cases}
 x=0 5","['calculus', 'analysis', 'derivatives', 'continuity', 'roots']"
88,$f$ is a real function. What is the largest interval $S$ such that no open subset $I\subset S$ makes $f(I)$ constant?,is a real function. What is the largest interval  such that no open subset  makes  constant?,f S I\subset S f(I),"Let $f:[0,1]\to \mathbb R$ be a real function. We say a set $I$ is trivial if $f(I)$ is constant. We say a set $S$ is purely non-trivial if no open subset of $S$ is trivial. Question: Does there exist a largest $S$ , such that all other pure non-trivial open sets $S'\subseteq S$ ? For example , if $f(x)=x$ , then $[0,1]$ is the largest purely non-trivial. If $f(x)=x-0.5+|x-0.5|$ , then $[0,0.5]$ is trivial and $[0.5,1]$ is a largest purely non-trivial. My try so far: The following results might be correct: Lemma 1: $S$ is purely nontrivial, open set $O\subset S$ , then $O$ is purely nontrivial. Lemma 2: Suppose open sets $S_1,S_2$ are purely non-trivial, then $S_1\cup S_2$ are purely non-trivial. Lemma 3: The collection $\Omega$ of all purely nontrivial open sets is an algebra. My stuck: if we can prove that $\Omega$ is a sigma field then I can say that the largest $S$ is simply $\cup_{S\in \Omega} S$ or its closure. However, $\Omega$ is not a sigma field!","Let be a real function. We say a set is trivial if is constant. We say a set is purely non-trivial if no open subset of is trivial. Question: Does there exist a largest , such that all other pure non-trivial open sets ? For example , if , then is the largest purely non-trivial. If , then is trivial and is a largest purely non-trivial. My try so far: The following results might be correct: Lemma 1: is purely nontrivial, open set , then is purely nontrivial. Lemma 2: Suppose open sets are purely non-trivial, then are purely non-trivial. Lemma 3: The collection of all purely nontrivial open sets is an algebra. My stuck: if we can prove that is a sigma field then I can say that the largest is simply or its closure. However, is not a sigma field!","f:[0,1]\to \mathbb R I f(I) S S S S'\subseteq S f(x)=x [0,1] f(x)=x-0.5+|x-0.5| [0,0.5] [0.5,1] S O\subset S O S_1,S_2 S_1\cup S_2 \Omega \Omega S \cup_{S\in \Omega} S \Omega","['real-analysis', 'abstract-algebra', 'analysis', 'measure-theory']"
89,Doubt on the definition of heat kernel,Doubt on the definition of heat kernel,,"The heat kernel of a compact Riemannian manifold is the only smooth function $k=k(t,x,y):\mathbb{R}_{>0}\times M \times M\to \mathbb{R} $ such that $k(\cdot,x,\cdot)$ is a solution to the heat equation with initial distribution $k(0,x,\cdot)$ is the Dirac delta centered in $x$ . (By $k(0,x,\cdot)$ , I mean the limit in the sense of distribution of $k(t,x,\cdot)$ as $t\to 0^+$ ). Since distributions act on the space of test functions $C^\infty_c(M)=C^\infty(M)$ , the condition $k(0,x,\cdot)=\delta_x$ should mean that $$\lim_{t\to 0^+}\int_M k(t,x,y)f(y)=f(x)$$ for any $f\in C^\infty(M)$ . However, many important references like this one and this one , require an (apparently) stronger condition. The first reference requires the equality above to hold for any $f\in L^2(M)$ and the second one for any $f\in C^0(M)$ . Are these three requests all equivalent?","The heat kernel of a compact Riemannian manifold is the only smooth function such that is a solution to the heat equation with initial distribution is the Dirac delta centered in . (By , I mean the limit in the sense of distribution of as ). Since distributions act on the space of test functions , the condition should mean that for any . However, many important references like this one and this one , require an (apparently) stronger condition. The first reference requires the equality above to hold for any and the second one for any . Are these three requests all equivalent?","k=k(t,x,y):\mathbb{R}_{>0}\times M \times M\to \mathbb{R}  k(\cdot,x,\cdot) k(0,x,\cdot) x k(0,x,\cdot) k(t,x,\cdot) t\to 0^+ C^\infty_c(M)=C^\infty(M) k(0,x,\cdot)=\delta_x \lim_{t\to 0^+}\int_M k(t,x,y)f(y)=f(x) f\in C^\infty(M) f\in L^2(M) f\in C^0(M)","['analysis', 'riemannian-geometry', 'lp-spaces', 'distribution-theory', 'heat-equation']"
90,How are two definitions of quasi-convexity equivalent? [duplicate],How are two definitions of quasi-convexity equivalent? [duplicate],,"This question already has an answer here : Difference in Definitions of Quasiconvexity (1 answer) Closed 12 months ago . Why are these two definitions equivalent? (Assuming $C$ is convex) $f: C \rightarrow \mathbb{R} $ is quasi-convex if for any $x, y \in  C$ and $\lambda \in [0,1]$ , $$ f((1-\lambda)x + \lambda y) \leq \max\{f(x), f(y)\} .$$ $f: C \rightarrow \mathbb{R} $ is quasi-convex if for any $\alpha \in  \mathbb{R}$ , $\operatorname{Lev} (f, \alpha) $ is convex. Where $\operatorname{Lev} (f, \alpha) $ is defined as: $$ \{x \in S: f(x) < \alpha \}$$ I was successful to show that if we have the first definition, it implies the second one, but got stuck on the reverse side. assume $ x, y \in \operatorname{Lev} (f, \alpha) $ so: $$f(x) < \alpha,$$ $$f(y) < \alpha.$$ Then we have: $$ max\{f(x), f(y)\} < \alpha $$ So by the first definition, we have: $$ f((1-\lambda)x + \lambda y) < \alpha $$ Which implies that: $$ ((1-\lambda)x + \lambda y) \in \operatorname{Lev}(f, \alpha) .$$","This question already has an answer here : Difference in Definitions of Quasiconvexity (1 answer) Closed 12 months ago . Why are these two definitions equivalent? (Assuming is convex) is quasi-convex if for any and , is quasi-convex if for any , is convex. Where is defined as: I was successful to show that if we have the first definition, it implies the second one, but got stuck on the reverse side. assume so: Then we have: So by the first definition, we have: Which implies that:","C f: C \rightarrow \mathbb{R}  x, y \in  C \lambda \in [0,1]  f((1-\lambda)x + \lambda y) \leq \max\{f(x), f(y)\} . f: C \rightarrow \mathbb{R}  \alpha \in  \mathbb{R} \operatorname{Lev} (f, \alpha)  \operatorname{Lev} (f, \alpha)   \{x \in S: f(x) < \alpha \}  x, y \in \operatorname{Lev} (f, \alpha)  f(x) < \alpha, f(y) < \alpha.  max\{f(x), f(y)\} < \alpha   f((1-\lambda)x + \lambda y) < \alpha   ((1-\lambda)x + \lambda y) \in \operatorname{Lev}(f, \alpha) .","['real-analysis', 'analysis', 'optimization', 'convex-analysis']"
91,"$\forall\lambda\in \Lambda $, $\int_{F_\lambda}f\ge1$. Prove that $\int_{\cap F_\lambda} f\ge 1$",", . Prove that",\forall\lambda\in \Lambda  \int_{F_\lambda}f\ge1 \int_{\cap F_\lambda} f\ge 1,"Suppose f is a non-negtive measurable function on $[a,b]$ , and $\left \{ F_{\lambda}\right \}_{\lambda\in\Lambda} $ is a family of closed sets in $[a,b]$ such that $\forall \lambda_1,\lambda_2\in\Lambda$ ,we have either $F_{\lambda_1}\subset F_{\lambda_2}$ or $F_{\lambda_2}\subset F_{\lambda_1}$ . Also $\forall\lambda\in \Lambda  $ , $\int_{F_\lambda}f\ge1$ . Prove that $\int_{\cap F_\lambda} f\ge 1$ . If $\Lambda$ is finite or countable,then it's easy to cope with. But if $\Lambda$ is uncountable, it's becoming hard to deal with.I think if we can choose a sequence of decreasing closed sets $\left \{ F_n\right \} $ so that $\forall \lambda,\exists n_0 $ , $F_{n_0}\subset F_\lambda$ and $\cap F_\lambda=\cap F_n=\lim\limits_{n\to\infty}F_n$ .But I don't know whether we can select such a sequence from $\left \{ F_{\lambda}\right \}_{\lambda\in\Lambda} $ . Any help will be appreciated.","Suppose f is a non-negtive measurable function on , and is a family of closed sets in such that ,we have either or . Also , . Prove that . If is finite or countable,then it's easy to cope with. But if is uncountable, it's becoming hard to deal with.I think if we can choose a sequence of decreasing closed sets so that , and .But I don't know whether we can select such a sequence from . Any help will be appreciated.","[a,b] \left \{ F_{\lambda}\right \}_{\lambda\in\Lambda}  [a,b] \forall \lambda_1,\lambda_2\in\Lambda F_{\lambda_1}\subset F_{\lambda_2} F_{\lambda_2}\subset F_{\lambda_1} \forall\lambda\in \Lambda   \int_{F_\lambda}f\ge1 \int_{\cap F_\lambda} f\ge 1 \Lambda \Lambda \left \{ F_n\right \}  \forall \lambda,\exists n_0  F_{n_0}\subset F_\lambda \cap F_\lambda=\cap F_n=\lim\limits_{n\to\infty}F_n \left \{ F_{\lambda}\right \}_{\lambda\in\Lambda} ","['real-analysis', 'analysis', 'measure-theory']"
92,How to draw and visualize a certain area in $\mathbb{R}^3$,How to draw and visualize a certain area in,\mathbb{R}^3,"I'm given an area in $\mathbb{R}^3$ that is defined as the following: $$ x = \left(1 + \rho \sin(\frac{\varphi}{2})\right) \cos(\varphi)$$ $$ y = \left(1 + \rho \sin(\frac{\varphi}{2})\right) \sin(\varphi)$$ $$ z =  \rho \cos(\frac{\varphi}{2}) $$ where $\rho \in [-\frac{1}{2}; \frac{1}{2}]$ and $\varphi \in [0;2\pi]$ . I just need to calculate its area, which is cumbersome but easy enough, but is it possible to draw it / have a good intuition what it could look like, which also could help my in the calculation of its area? Is there a general way to go about a problem like this? I already tried inserting some points but I didn't see any pattern at all.","I'm given an area in that is defined as the following: where and . I just need to calculate its area, which is cumbersome but easy enough, but is it possible to draw it / have a good intuition what it could look like, which also could help my in the calculation of its area? Is there a general way to go about a problem like this? I already tried inserting some points but I didn't see any pattern at all.",\mathbb{R}^3  x = \left(1 + \rho \sin(\frac{\varphi}{2})\right) \cos(\varphi)  y = \left(1 + \rho \sin(\frac{\varphi}{2})\right) \sin(\varphi)  z =  \rho \cos(\frac{\varphi}{2})  \rho \in [-\frac{1}{2}; \frac{1}{2}] \varphi \in [0;2\pi],"['real-analysis', 'calculus', 'integration', 'analysis', 'multivariable-calculus']"
93,How to handle measurements and Lebesgue integrals? (with concrete example),How to handle measurements and Lebesgue integrals? (with concrete example),,"Question Define the measure $\mu$ on the measurable space $(\mathbb{R}, \mathcal{B})$ as follows. $$ \mu\left( (a, b] \right) = \int_a^b \frac{1}{1+x^2}dx\ \ \mathrm{for\ all\ interval\ (a,b]}. $$ Then, I want to calculate the following: $$ \int_{(-1, 1)} gd\mu,\ (g:=|x|) $$ What I know I'm getting the flow of the equation transformation, but I don't understand the transformation of the question mark part. $$ \int_{(-1, 1)} |x|d\mu(x) = \int_\mathbb{R} |x|\chi_{(-1, 1)} d\mu \overset{?}{=} |x|\mu((-1, 1)) \overset{?}{=} \int_{-1}^1 |x|\frac{1}{1+x^2}dx = \log 2. $$ The definition of measurement is understood as follows. setup: $(X, m, \mu); \textrm{measure spaces}, s = \sum_{i=1}^n \alpha_i \chi_{A_i}, A_i = \{ x\in X; s(x) = \alpha_i \}, \alpha_i \geq 0, E \in m$ . $$ \int_E s d\mu := \sum_{i=1}^n \alpha_i \mu(A_i \cap E) $$ In other words, the integral can be calculated for measurable single functions, but I believe the others are undefined...","Question Define the measure on the measurable space as follows. Then, I want to calculate the following: What I know I'm getting the flow of the equation transformation, but I don't understand the transformation of the question mark part. The definition of measurement is understood as follows. setup: . In other words, the integral can be calculated for measurable single functions, but I believe the others are undefined...","\mu (\mathbb{R}, \mathcal{B}) 
\mu\left( (a, b] \right) = \int_a^b \frac{1}{1+x^2}dx\ \ \mathrm{for\ all\ interval\ (a,b]}.
 
\int_{(-1, 1)} gd\mu,\ (g:=|x|)
 
\int_{(-1, 1)} |x|d\mu(x) = \int_\mathbb{R} |x|\chi_{(-1, 1)} d\mu \overset{?}{=} |x|\mu((-1, 1)) \overset{?}{=} \int_{-1}^1 |x|\frac{1}{1+x^2}dx = \log 2.
 (X, m, \mu); \textrm{measure spaces}, s = \sum_{i=1}^n \alpha_i \chi_{A_i}, A_i = \{ x\in X; s(x) = \alpha_i \}, \alpha_i \geq 0, E \in m 
\int_E s d\mu := \sum_{i=1}^n \alpha_i \mu(A_i \cap E)
","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
94,Can we axiomatize the complex numbers without directly defining the reals?,Can we axiomatize the complex numbers without directly defining the reals?,,"I've decided to attempt the entire Rudin sequence in a single 6 month period, because I'm insane. Rudin spends very little time on foundational matters, and that bothers me, it makes the subject of analysis feel less philosophically sound. So I've been adding the missing axioms and proofs as I go. I've defined the real numbers as the unique model (up to isomorphism) of the second-order theory over the language $\langle+,\cdot,<\rangle$ or $\langle+,<,\cdot,0,1\rangle$ given by the axioms: $\forall x\forall y\forall z[(x+y)+z=x+(y+z)]$ $\forall x\forall y \forall z [(x\cdot y)\cdot z=x\cdot (y\cdot z)]$ $\forall x\forall y(x+y=y+x)$ $\forall x\forall y(x\cdot y=y\cdot x)$ $\forall x\forall y\forall z(x\cdot(y+z)=(x\cdot y)+(x\cdot z))$ $\exists x\forall y(x+y=y)$ or $\forall x(0+x=x)$ $\forall x\exists y\forall z[x+y=z\implies\forall w(w+z=w)]$ or $\forall x \exists y(x+y=0)$ $\exists x\forall y(x\cdot y=y)$ or $\forall x(1\cdot x=x)$ $\forall x\forall y\forall z[(x+y=x\land x\cdot z=x)\implies y\ne z]$ or $0\ne 1$ (not sure if this one is necessary) $\forall x[\exists y(x+y\ne y)\implies\exists y\forall z((x\cdot y)\cdot z=z)]$ or $\forall x[x\ne0\implies\exists y (x\cdot y=1)]$ $\forall x\forall y\forall z(x<y\implies x+z<y+z)$ $\forall x\forall y\forall z((x<y\land y<z)\implies x<z)$ $\forall x\forall y(x<y\lor y<x\lor x=y)$ $\forall x\forall y\forall z[(x<y\land\exists w(w+z\ne w))\implies x\cdot z<y\cdot z]$ or $\forall x\forall y\forall z((x<y\land z>0)\implies x\cdot z<y\cdot z)$ $\forall P[(\exists xPx\land\exists y\forall x(Px\implies x<y))\implies\exists y\forall x((Px\implies x\le y)\land\forall z(z<y\implies\exists x_1(Px_1\land z<x_1)))]$ I picked these axioms over say, the Tarski axioms, because the standard first-order characterization can be easily recovered by replacing the second-order completeness axiom with its first-order equivalent. That way, I can keep pretending that I'm not using SOL as long as I'm subtle with my wording (technically, this is called ""lying,"" but the belief that analysis is somehow a first-order endeavor, and that first-order logic is somehow the ""true"" logic, is so deeply ingrained in the subject that I've given up correcting people.) Now I want to define the complex numbers. I can think of several ways to do this, but all of them make explicit reference to the real numbers, and that bugs me. You shouldn't need to define the very specific Dedekind complete ordered field of real numbers to define the complex numbers; it should be possible to recover it from an adequate single-sorted characterization of the complex numbers as a field (up to isomorphism.) Yet every set of axioms I can find or invent ends up directly referencing either $\Bbb R^2$ , by way of needing a predicate for ""real"" or implicitly defined projection functions (e.g. $\forall x\exists!y\exists!z\cdots$ := "" $x=y+iz$ ""), or the topology on $\Bbb C$ (the second-order characterization of the Euclidean topology on $\Bbb C$ suffices to characterize all analysis-relevant properties.) We don't usually define the real numbers to by a two-sorted theory describing such-and-such ordered field containing $\Bbb Q$ . Instead, all of the expected properties just arise from the ordering on the field as though by miraculous coincidence. Why should the complex numbers be different? How can I ""properly"" axiomatize the complex numbers (over $\langle+,\cdot\rangle$ , $\langle+,\cdot,0,1\rangle$ , or $\langle+,\cdot,0,1,i\rangle$ ) so that the associated properties (the ordering of the reals, conjugation, the standard topology, etc.) are implied by the axioms without being stated outright? And how can I do it in a way that let's me pretend that SOL exist in different universe from analysis?","I've decided to attempt the entire Rudin sequence in a single 6 month period, because I'm insane. Rudin spends very little time on foundational matters, and that bothers me, it makes the subject of analysis feel less philosophically sound. So I've been adding the missing axioms and proofs as I go. I've defined the real numbers as the unique model (up to isomorphism) of the second-order theory over the language or given by the axioms: or or or or (not sure if this one is necessary) or or I picked these axioms over say, the Tarski axioms, because the standard first-order characterization can be easily recovered by replacing the second-order completeness axiom with its first-order equivalent. That way, I can keep pretending that I'm not using SOL as long as I'm subtle with my wording (technically, this is called ""lying,"" but the belief that analysis is somehow a first-order endeavor, and that first-order logic is somehow the ""true"" logic, is so deeply ingrained in the subject that I've given up correcting people.) Now I want to define the complex numbers. I can think of several ways to do this, but all of them make explicit reference to the real numbers, and that bugs me. You shouldn't need to define the very specific Dedekind complete ordered field of real numbers to define the complex numbers; it should be possible to recover it from an adequate single-sorted characterization of the complex numbers as a field (up to isomorphism.) Yet every set of axioms I can find or invent ends up directly referencing either , by way of needing a predicate for ""real"" or implicitly defined projection functions (e.g. := "" ""), or the topology on (the second-order characterization of the Euclidean topology on suffices to characterize all analysis-relevant properties.) We don't usually define the real numbers to by a two-sorted theory describing such-and-such ordered field containing . Instead, all of the expected properties just arise from the ordering on the field as though by miraculous coincidence. Why should the complex numbers be different? How can I ""properly"" axiomatize the complex numbers (over , , or ) so that the associated properties (the ordering of the reals, conjugation, the standard topology, etc.) are implied by the axioms without being stated outright? And how can I do it in a way that let's me pretend that SOL exist in different universe from analysis?","\langle+,\cdot,<\rangle \langle+,<,\cdot,0,1\rangle \forall x\forall y\forall z[(x+y)+z=x+(y+z)] \forall x\forall y \forall z [(x\cdot y)\cdot z=x\cdot (y\cdot z)] \forall x\forall y(x+y=y+x) \forall x\forall y(x\cdot y=y\cdot x) \forall x\forall y\forall z(x\cdot(y+z)=(x\cdot y)+(x\cdot z)) \exists x\forall y(x+y=y) \forall x(0+x=x) \forall x\exists y\forall z[x+y=z\implies\forall w(w+z=w)] \forall x \exists y(x+y=0) \exists x\forall y(x\cdot y=y) \forall x(1\cdot x=x) \forall x\forall y\forall z[(x+y=x\land x\cdot z=x)\implies y\ne z] 0\ne 1 \forall x[\exists y(x+y\ne y)\implies\exists y\forall z((x\cdot y)\cdot z=z)] \forall x[x\ne0\implies\exists y (x\cdot y=1)] \forall x\forall y\forall z(x<y\implies x+z<y+z) \forall x\forall y\forall z((x<y\land y<z)\implies x<z) \forall x\forall y(x<y\lor y<x\lor x=y) \forall x\forall y\forall z[(x<y\land\exists w(w+z\ne w))\implies x\cdot z<y\cdot z] \forall x\forall y\forall z((x<y\land z>0)\implies x\cdot z<y\cdot z) \forall P[(\exists xPx\land\exists y\forall x(Px\implies x<y))\implies\exists y\forall x((Px\implies x\le y)\land\forall z(z<y\implies\exists x_1(Px_1\land z<x_1)))] \Bbb R^2 \forall x\exists!y\exists!z\cdots x=y+iz \Bbb C \Bbb C \Bbb Q \langle+,\cdot\rangle \langle+,\cdot,0,1\rangle \langle+,\cdot,0,1,i\rangle","['analysis', 'axioms', 'foundations', 'second-order-logic']"
95,A weird theorem of existence,A weird theorem of existence,,"Im struggling with a problem which seems to be an application of implicit theorem function. It's really hard for me and I would love some help. Let $F: \mathbb{R}^2 \to \mathbb{R}$ a $C^1$ function. Suppose that : $\bullet$ $\forall x \in \mathbb{R}$ , $\lim\limits_{u \rightarrow -\infty} F(x,u) = -\infty$ . $\bullet$ $\forall x \in \mathbb{R}$ , $\lim\limits_{u \rightarrow +\infty} F(x,u) = +\infty$ . $\bullet$ $\forall K \in \mathbb{R}$ , $\exists C \in \mathbb{R}$ , $\forall (x,u) \in \mathbb{R}^2$ , $-K \leq F(x,u) \leq K$ $\implies$ $-C \leq u \leq C$ . $\bullet$ $\exists M \in \mathbb{R}$ , $\forall (x,u) \in \mathbb{R}^2$ , $F(x,u)=M$ $\implies$ $\partial_uF(x,u) \neq 0$ . Then exists $\nu : \mathbb{R} \to \mathbb{R}$ a $C^1$ function such that : $\forall x \in \mathbb{R}$ , $F(x,\nu(x))=M$ . In my opinion we have to apply implicit theorem function to show existence, and compacity to show definition (by hypothesis 3 we can have totally bounded, and completeness leads to compactness), but i have absolutely no clue how to write a proof.","Im struggling with a problem which seems to be an application of implicit theorem function. It's really hard for me and I would love some help. Let a function. Suppose that : , . , . , , , . , , . Then exists a function such that : , . In my opinion we have to apply implicit theorem function to show existence, and compacity to show definition (by hypothesis 3 we can have totally bounded, and completeness leads to compactness), but i have absolutely no clue how to write a proof.","F: \mathbb{R}^2 \to \mathbb{R} C^1 \bullet \forall x \in \mathbb{R} \lim\limits_{u \rightarrow -\infty} F(x,u) = -\infty \bullet \forall x \in \mathbb{R} \lim\limits_{u \rightarrow +\infty} F(x,u) = +\infty \bullet \forall K \in \mathbb{R} \exists C \in \mathbb{R} \forall (x,u) \in \mathbb{R}^2 -K \leq F(x,u) \leq K \implies -C \leq u \leq C \bullet \exists M \in \mathbb{R} \forall (x,u) \in \mathbb{R}^2 F(x,u)=M \implies \partial_uF(x,u) \neq 0 \nu : \mathbb{R} \to \mathbb{R} C^1 \forall x \in \mathbb{R} F(x,\nu(x))=M","['analysis', 'implicit-function-theorem']"
96,"Convex compact set in $\mathbb{R}^n$ where, given any point in it, the result of replacing two of its coordinates with their mean lies in the set.","Convex compact set in  where, given any point in it, the result of replacing two of its coordinates with their mean lies in the set.",\mathbb{R}^n,"Let $X$ be a nonempty compact convex subset of $\mathbf{R}^n$ . Suppose this subset has the following property: for every $x = (x_1, \dots, x_n) \in X$ , for every $1 \le i< j \le n$ , $$({x_1}, \ldots, {x_{i - 1}}, \frac{{x_i} + {x_j}}{2}, {x_{i + 1}}, \ldots, {x_{j - 1}}, \frac{{x_i} + {x_j}}{2}, {x_{j + 1}}, \dots, {x_n} ) \in X.$$ Is it true that there exists some $\lambda \in \mathbb{R}$ such that $$(\lambda, \dots, \lambda) \in X?$$ One idea is that we can use the above property to get a sequence $(x, x', x'', \dots)$ where $x'$ is obtained by replacing two coordinates in $x$ with their average, and $x''$ in the same way, . . . Then we use sequential compactness to say that limit, call it $L$ , also lies in $X$ . Could we argue that every coordinate of $L$ is equal?","Let be a nonempty compact convex subset of . Suppose this subset has the following property: for every , for every , Is it true that there exists some such that One idea is that we can use the above property to get a sequence where is obtained by replacing two coordinates in with their average, and in the same way, . . . Then we use sequential compactness to say that limit, call it , also lies in . Could we argue that every coordinate of is equal?","X \mathbf{R}^n x = (x_1, \dots, x_n) \in X 1 \le i< j \le n ({x_1}, \ldots, {x_{i - 1}},
\frac{{x_i} + {x_j}}{2},
{x_{i + 1}},
\ldots,
{x_{j - 1}},
\frac{{x_i} + {x_j}}{2},
{x_{j + 1}},
\dots,
{x_n}
) \in X. \lambda \in \mathbb{R} (\lambda, \dots, \lambda) \in X? (x, x', x'', \dots) x' x x'' L X L",['real-analysis']
97,Want to show that $f=0$ a.e.,Want to show that  a.e.,f=0,"This question has been asked before but I can't understand the given solution: To show that an integral is 0 a.e. if it is 0 over every subset of measure 2/3. Let $f\in L^1[0,1]$ such that for every $E\subset[0,1]$ with $m(E)=2/3$ , $\int_E f=0$ . Show that $f=0$ a.e. I'm seeking a solution that doesn't use the Lebesgue Differentiation theorem. My idea was to write sets $$E_n=\{x\in[0,1]:|f(x)|>1/n\}$$ and show that $m(E_n)=0$ for each $n$ . But I don't know how to use the given hypothesis. How do I use that $\int_E f=0$ if $m(E)=2/3$ ?","This question has been asked before but I can't understand the given solution: To show that an integral is 0 a.e. if it is 0 over every subset of measure 2/3. Let such that for every with , . Show that a.e. I'm seeking a solution that doesn't use the Lebesgue Differentiation theorem. My idea was to write sets and show that for each . But I don't know how to use the given hypothesis. How do I use that if ?","f\in L^1[0,1] E\subset[0,1] m(E)=2/3 \int_E f=0 f=0 E_n=\{x\in[0,1]:|f(x)|>1/n\} m(E_n)=0 n \int_E f=0 m(E)=2/3","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
98,$C^k$ extension of a function,extension of a function,C^k,"Let $\Omega$ be a bounded set in $\mathbb{R}^n.$ Suppose $f$ is continuous on $\Omega \subset \mathbb{R}^n$ then continuous extension does not exist in general. For example $f(x)=1/x$ and $\Omega=(0,1)$ does not admit continuous extension on $\mathbb{R}$ . On the other hand, if $f$ is uniformly continuous on $\Omega$ then there exists a uniformly continuous function $g \in C(\mathbb{R}^n)$ such that $f=g$ on $\Omega.$ In other words uniformly continuous function admits uniformly continuous extensions on $\mathbb{R}^n.$ Are there any analogous results for higher derivatives? i.e. Under what conditions on $\Omega$ and $f,$ we get an extension of $f$ which is $C^k(\mathbb{R}^n)$ (k-times continuously differentiable)? Do we need regularity assumption on boundary( $\partial \Omega$ ) of $\Omega?$ Rigorous proof/references will be appreciated.","Let be a bounded set in Suppose is continuous on then continuous extension does not exist in general. For example and does not admit continuous extension on . On the other hand, if is uniformly continuous on then there exists a uniformly continuous function such that on In other words uniformly continuous function admits uniformly continuous extensions on Are there any analogous results for higher derivatives? i.e. Under what conditions on and we get an extension of which is (k-times continuously differentiable)? Do we need regularity assumption on boundary( ) of Rigorous proof/references will be appreciated.","\Omega \mathbb{R}^n. f \Omega \subset \mathbb{R}^n f(x)=1/x \Omega=(0,1) \mathbb{R} f \Omega g \in C(\mathbb{R}^n) f=g \Omega. \mathbb{R}^n. \Omega f, f C^k(\mathbb{R}^n) \partial \Omega \Omega?","['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
99,Asymptotic behaviour of $I_{\alpha}(x):=\int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy$ with $0<\alpha<1$ as $\alpha\rightarrow 0$,Asymptotic behaviour of  with  as,I_{\alpha}(x):=\int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy 0<\alpha<1 \alpha\rightarrow 0,"I am trying to obtain the asymptotic behaviour of the integral $$I_{\alpha}(x):= \int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy$$ explicitly as $\alpha\rightarrow 0^{+}$ . Clearly, by the dominated convergence theorem $$I_{\alpha}(x)\rightarrow \int_{\mathbb{R}^{n}}e^{-|x-y|^2}dy=\int_{\mathbb{R}^{n}}e^{-|y|^2}dy=c.$$ My naive attempt is  to calculate $I_{x}$ explicitly in the hope to get an asymptotic. Here is what I got: Since $|x-y|^2=|x|^2-2 x\cdot y+|y|^2$ then $$I_{\alpha}(x)= \int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy=e^{-|x|^2}\int_{\mathbb{R}^{n}}\frac{e^{-2 x\cdot y+|y|^2}}{|y|^{\alpha}}dy.$$ Using spherical coordinates, we get $$I_{\alpha}(x)= e^{-|x|^2}\int_{\mathbb{S}^{n-1}} \int_{0}^{\infty} e^{-2 x\cdot \omega r+r^2}r^{n-1-\alpha}dr d\sigma(\omega).$$ By Fubini's theorem we have $$I_{\alpha}(x)= e^{-|x|^2} \int_{0}^{\infty}\int_{\mathbb{S}^{n-1}} e^{-2 x\cdot \omega r} d\sigma(\omega) e^{-r^2}r^{n-1-\alpha}dr.$$ Using the formula (Grafakos, Classical Fourier Analysis, Appendix D) $$C\int_{\mathbb{S}^{n-1}} F(x\cdot \omega)d\sigma(\omega)= \int_{-1}^{1}F(s|x|)(\sqrt{1-s^2})^{n-3} ds$$ we have $$I_{\alpha}(x)= e^{-|x|^2} \int_{0}^{\infty}\int_{-1}^{1} e^{-2 s|x| r}(\sqrt{1-s^2})^{n-3} ds  e^{-r^2}r^{n-1-\alpha}dr.$$ Applying Fubini's theorem one more time $$I_{\alpha}(x)= e^{-|x|^2} \int_{-1}^{1}\int_{0}^{\infty} e^{-2 s|x| r-r^2}r^{n-1-\alpha}dr(\sqrt{1-s^2})^{n-3} ds.$$ Is there a way to calculate $$\int_{0}^{\infty} e^{-2 s|x| r-r^2}r^{n-1-\alpha}dr$$ Mathematica gives some kind of the hypergeometric function. It is difficult to understand how Mathematica's answer behaves in terms of $\alpha$ .","I am trying to obtain the asymptotic behaviour of the integral explicitly as . Clearly, by the dominated convergence theorem My naive attempt is  to calculate explicitly in the hope to get an asymptotic. Here is what I got: Since then Using spherical coordinates, we get By Fubini's theorem we have Using the formula (Grafakos, Classical Fourier Analysis, Appendix D) we have Applying Fubini's theorem one more time Is there a way to calculate Mathematica gives some kind of the hypergeometric function. It is difficult to understand how Mathematica's answer behaves in terms of .","I_{\alpha}(x):= \int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy \alpha\rightarrow 0^{+} I_{\alpha}(x)\rightarrow
\int_{\mathbb{R}^{n}}e^{-|x-y|^2}dy=\int_{\mathbb{R}^{n}}e^{-|y|^2}dy=c. I_{x} |x-y|^2=|x|^2-2 x\cdot y+|y|^2 I_{\alpha}(x)= \int_{\mathbb{R}^{n}}\frac{e^{-|x-y|^2}}{|y|^{\alpha}}dy=e^{-|x|^2}\int_{\mathbb{R}^{n}}\frac{e^{-2 x\cdot y+|y|^2}}{|y|^{\alpha}}dy. I_{\alpha}(x)= e^{-|x|^2}\int_{\mathbb{S}^{n-1}}
\int_{0}^{\infty}
e^{-2 x\cdot \omega r+r^2}r^{n-1-\alpha}dr d\sigma(\omega). I_{\alpha}(x)= e^{-|x|^2}
\int_{0}^{\infty}\int_{\mathbb{S}^{n-1}}
e^{-2 x\cdot \omega r} d\sigma(\omega)
e^{-r^2}r^{n-1-\alpha}dr. C\int_{\mathbb{S}^{n-1}} F(x\cdot \omega)d\sigma(\omega)= \int_{-1}^{1}F(s|x|)(\sqrt{1-s^2})^{n-3} ds I_{\alpha}(x)= e^{-|x|^2}
\int_{0}^{\infty}\int_{-1}^{1}
e^{-2 s|x| r}(\sqrt{1-s^2})^{n-3} ds 
e^{-r^2}r^{n-1-\alpha}dr. I_{\alpha}(x)= e^{-|x|^2}
\int_{-1}^{1}\int_{0}^{\infty}
e^{-2 s|x| r-r^2}r^{n-1-\alpha}dr(\sqrt{1-s^2})^{n-3} ds. \int_{0}^{\infty}
e^{-2 s|x| r-r^2}r^{n-1-\alpha}dr \alpha","['real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
