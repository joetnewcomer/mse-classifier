,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is the construction of the real numbers important?,Why is the construction of the real numbers important?,,"There are a lot of books, specially in Real Analysis and set theory, which define the real numbers by Cauchy sequences or Dedekind cuts. So my question is why don't we simply define the Real numbers as a complete ordered field? What's the importance of studying the construction of the Real numbers? Is it just for historical reasons?","There are a lot of books, specially in Real Analysis and set theory, which define the real numbers by Cauchy sequences or Dedekind cuts. So my question is why don't we simply define the Real numbers as a complete ordered field? What's the importance of studying the construction of the Real numbers? Is it just for historical reasons?",,"['analysis', 'soft-question', 'foundations']"
1,Why is $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$?,Why is ?,\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi},"It seems as if no one has asked this here before, unless I don't know how to search. The Gamma function is $$ \Gamma(\alpha)=\int_0^\infty x^{\alpha-1} e^{-x}\,dx. $$ Why is $$ \Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}\text{ ?} $$ (I'll post my own answer, but I know there are many ways to show this, so post your own!)","It seems as if no one has asked this here before, unless I don't know how to search. The Gamma function is Why is (I'll post my own answer, but I know there are many ways to show this, so post your own!)","
\Gamma(\alpha)=\int_0^\infty x^{\alpha-1} e^{-x}\,dx.
 
\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}\text{ ?}
","['analysis', 'special-functions', 'gamma-function']"
2,Continuity and the Axiom of Choice,Continuity and the Axiom of Choice,,"In my introductory Analysis course, we learned two definitions of continuity. $(1)$ A function $f:E \to \mathbb{C}$ is continuous at $a$ if every sequence $(z_n) \in E$ such that $z_n \to a$ satisfies $f(z_n) \to f(a)$ . $(2)$ A function $f:E \to \mathbb{C}$ is continuous at $a$ if $\forall \varepsilon>0, \exists \delta >0:\forall z \in E, |z-a|<\delta \implies |f(z)-f(a)|<\varepsilon$ . The implication $(2)\implies(1)$ is trivial (though I will happily post a proof if there is sufficient interest).  The proof of the implication $(1)\implies(2)$ is worth remarking on, though. Proof that $(1)\implies(2)$ : Suppose on the contrary that $\exists \varepsilon>0:\forall \delta>0, \exists z \in E:\left (|z-a|<\delta \; \mathrm{and} \; |f(z)-f(a)|\ge \varepsilon\right )$ .  Let $A_n$ be the set $\{z\in E:|z-a|<\frac{1}{n} \; \mathrm{ and }\; |f(z)-f(a)|\ge\varepsilon\}$ .  Now use the Axiom of Choice to construct a sequence $(z_n)$ with $z_n \in A_n \; \forall n \in \mathbb{N}$ .  But now $a-\frac{1}{n}<z_n<a+\frac{1}{n}\; \forall n \in \mathbb{N}$ so $z_n \to a$ .  So $f(z_n) \to f(a)$ .  But $|f(z_n)-f(a)|\ge\varepsilon\; \forall n \in \mathbb{N}$ , which is a contradiction. You will have noticed that the above proof uses the Axiom of Choice (the lecturer didn't explicitly spell out the dependence, but it's definitely there).  My question is: is it possible to prove that $(1) \implies (2)$ without using the Axiom of Choice.  I strongly suspect that it isn't.  In that case, can anyone prove that we have to use the Axiom of Choice?  I can think of three ways to do this: (A) Show that $\left( (1) \implies (2)\right)\implies \mathrm{AC}$ . I suspect that this statement is untrue. This is definitely untrue, as Arthur points out, because I only used the axiom of countable choice, which is strictly weaker than AC. (B) Show that $(1)\implies (2)$ is equivalent to some other statement known to require the Axiom of Choice (the obvious example being the well-ordering of the real numbers). (C) Construct or show the existence of a model of ZF in which there exist sequences which satisfy $(1)$ but not $(2)$ . Of course, if anyone can think of another way, I would be very interested to hear about it. One final note - I am aware that very many theorems in Analysis use the Axiom of Choice in one way or another, and that this is just one example of such a theorem.  If there exists a model of ZF like the one described in (C), is the study of Analysis in that model interesting?","In my introductory Analysis course, we learned two definitions of continuity. A function is continuous at if every sequence such that satisfies . A function is continuous at if . The implication is trivial (though I will happily post a proof if there is sufficient interest).  The proof of the implication is worth remarking on, though. Proof that : Suppose on the contrary that .  Let be the set .  Now use the Axiom of Choice to construct a sequence with .  But now so .  So .  But , which is a contradiction. You will have noticed that the above proof uses the Axiom of Choice (the lecturer didn't explicitly spell out the dependence, but it's definitely there).  My question is: is it possible to prove that without using the Axiom of Choice.  I strongly suspect that it isn't.  In that case, can anyone prove that we have to use the Axiom of Choice?  I can think of three ways to do this: (A) Show that . I suspect that this statement is untrue. This is definitely untrue, as Arthur points out, because I only used the axiom of countable choice, which is strictly weaker than AC. (B) Show that is equivalent to some other statement known to require the Axiom of Choice (the obvious example being the well-ordering of the real numbers). (C) Construct or show the existence of a model of ZF in which there exist sequences which satisfy but not . Of course, if anyone can think of another way, I would be very interested to hear about it. One final note - I am aware that very many theorems in Analysis use the Axiom of Choice in one way or another, and that this is just one example of such a theorem.  If there exists a model of ZF like the one described in (C), is the study of Analysis in that model interesting?","(1) f:E \to \mathbb{C} a (z_n) \in E z_n \to a f(z_n) \to f(a) (2) f:E \to \mathbb{C} a \forall \varepsilon>0, \exists \delta >0:\forall z \in E, |z-a|<\delta \implies |f(z)-f(a)|<\varepsilon (2)\implies(1) (1)\implies(2) (1)\implies(2) \exists \varepsilon>0:\forall \delta>0, \exists z \in E:\left (|z-a|<\delta \; \mathrm{and} \; |f(z)-f(a)|\ge \varepsilon\right ) A_n \{z\in E:|z-a|<\frac{1}{n} \; \mathrm{ and }\; |f(z)-f(a)|\ge\varepsilon\} (z_n) z_n \in A_n \; \forall n \in \mathbb{N} a-\frac{1}{n}<z_n<a+\frac{1}{n}\; \forall n \in \mathbb{N} z_n \to a f(z_n) \to f(a) |f(z_n)-f(a)|\ge\varepsilon\; \forall n \in \mathbb{N} (1) \implies (2) \left( (1) \implies (2)\right)\implies \mathrm{AC} (1)\implies (2) (1) (2)","['analysis', 'continuity', 'set-theory', 'axiom-of-choice']"
3,Why doesn't Cantor's diagonal argument also apply to natural numbers?,Why doesn't Cantor's diagonal argument also apply to natural numbers?,,"In my understanding of Cantor's diagonal argument, we start by representing each of a set of real numbers as an infinite bit string. My question is: why can't we begin by representing each natural number as an infinite bit string? So that 0 = 00000000000..., 9 = 1001000000..., 255 = 111111110000000...., and so on. If we could, then the diagonal argument would imply that there is a natural number not in the natural numbers, which is a contradiction.","In my understanding of Cantor's diagonal argument, we start by representing each of a set of real numbers as an infinite bit string. My question is: why can't we begin by representing each natural number as an infinite bit string? So that 0 = 00000000000..., 9 = 1001000000..., 255 = 111111110000000...., and so on. If we could, then the diagonal argument would imply that there is a natural number not in the natural numbers, which is a contradiction.",,"['analysis', 'elementary-set-theory']"
4,Baby/Papa/Mama/Big Rudin,Baby/Papa/Mama/Big Rudin,,"Recently, I was looking for the reviews of some Analysis books while encountered terms such as Baby/Papa/Mama/Big Rudin. Firstly, I thought that these are the names of a book! But it turned out that these are some nick names used for the books of Walter Rudin . So I was thinking that $1$. What are the corresponding books of these nick names? $2$. Why such nick names are chosen? or What are their origins?","Recently, I was looking for the reviews of some Analysis books while encountered terms such as Baby/Papa/Mama/Big Rudin. Firstly, I thought that these are the names of a book! But it turned out that these are some nick names used for the books of Walter Rudin . So I was thinking that $1$. What are the corresponding books of these nick names? $2$. Why such nick names are chosen? or What are their origins?",,"['analysis', 'soft-question']"
5,How hard is the proof of $\pi$ or $e$ being transcendental?,How hard is the proof of  or  being transcendental?,\pi e,"I understand that $\pi$ and $e$ are transcendental and that these are not simple facts. I mean, I have been told that these results are deep and difficult, and I am happy to believe them. I am curious what types of techniques are used and just how difficult of a problem it is. Would this result be a reasonable capstone to any course? (any course that isn't essentially ""how to prove..."") Another part of this is the following observation: as time passes deep results become easier to understand or rather assimilate into ones body of knowledge and some problems are just hard. I am wondering if people feel like this result is something that a grad student could spend some leisure time and understand, or if it really is something only graspable by ""experts"" (meaning people in the appropriate field and not a general mathematical audience). How specialized are the techniques used for the problem at hand? Have they been used to prove different results? are the techniques drastically different for $e$ and $\pi$? Hope this isn't too soft of a question. I was talking with my roommate, also a math grad student, and it came up. I said that it was a classically difficult result, but then wondered if that was right, so here I am. Note: I don't want a proof or a sketch of one, but maybe a heuristic as to why new techniques were needed or explaining the troubles one has when being naive or using early methods to attack the problem. Edit: As Matt E pointed out I should also ask: what are the old techniques? Also, did I even tag the question correctly? These seem to be the areas I would put them in, but I don't know the anything about this stuff. In the above it also isn't clear that I am wondering if the proof of this result is gotten by some clever new trick or by lots of hard had work that people couldn't/didn't do before? essentially, is it all elbow grease, or some clever new machinery or something completely different?","I understand that $\pi$ and $e$ are transcendental and that these are not simple facts. I mean, I have been told that these results are deep and difficult, and I am happy to believe them. I am curious what types of techniques are used and just how difficult of a problem it is. Would this result be a reasonable capstone to any course? (any course that isn't essentially ""how to prove..."") Another part of this is the following observation: as time passes deep results become easier to understand or rather assimilate into ones body of knowledge and some problems are just hard. I am wondering if people feel like this result is something that a grad student could spend some leisure time and understand, or if it really is something only graspable by ""experts"" (meaning people in the appropriate field and not a general mathematical audience). How specialized are the techniques used for the problem at hand? Have they been used to prove different results? are the techniques drastically different for $e$ and $\pi$? Hope this isn't too soft of a question. I was talking with my roommate, also a math grad student, and it came up. I said that it was a classically difficult result, but then wondered if that was right, so here I am. Note: I don't want a proof or a sketch of one, but maybe a heuristic as to why new techniques were needed or explaining the troubles one has when being naive or using early methods to attack the problem. Edit: As Matt E pointed out I should also ask: what are the old techniques? Also, did I even tag the question correctly? These seem to be the areas I would put them in, but I don't know the anything about this stuff. In the above it also isn't clear that I am wondering if the proof of this result is gotten by some clever new trick or by lots of hard had work that people couldn't/didn't do before? essentially, is it all elbow grease, or some clever new machinery or something completely different?",,"['analysis', 'soft-question', 'analytic-number-theory', 'math-history', 'transcendental-numbers']"
6,Nonobvious examples of metric spaces that do not work like $\mathbb{R}^n$,Nonobvious examples of metric spaces that do not work like,\mathbb{R}^n,"This week, I come to the end of the first year analysis, and suffer from a ""crisis of motivation.""  With this question, I want to chase away my thought, ""Why is it important to study the general properties of metric spaces? Is every good example just a subset of $\mathbb{R}^n$?"" I will explain.  During many proofs, I visualize something like $\mathbb{R}^2$.  The professor always draws these pictures: This is good for many theorems, since much of the analysis is motivate by questions about $\mathbb{R}^n$.  But my mental picture is now only $\mathbb{R}^n$. For appreciate the study of metric spaces in full generality, and for intuition, I request more useful examples of metric spaces that are significantly different from $\mathbb{R}^n$, and are not contain in $\mathbb{R}^n$. Here, I say ""useful"" to mean that the example ""could naturally arise in another context"" , in mathematics or an application.  It is frustrate when I ask why some property does not hold in general, and someone tells me consider the discrete metric.  Yes, it is true, and it is easy to see, but the discrete metric is stupid.  Does my property fail in any metric space that someone would care about? In other words, I have the following taxonomy of metric spaces: $\mathbb{R}^n$ and the subsets degenerate examples like the discrete metric contrived examples that I would not see except in analysis (I mean if they are only exist to be pathological, this is where I think to place the Cantor set) I want to expand this taxonomy, so that when I hear the new definition or theorem, I can compare to this collection of good examples.  I know there must be examples with intricate and intuitive interpretations in statistics, science, and engineering. Which metric spaces are not in my mental taxonomy?  In general, what are the useful, non-obvious metric spaces that a student should keep in his mind when learning analysis? To me, the ideal answer includes the description of the metric space, what properties it has to be unique and different, some consequences of the properties that make different from my examples, and (if not obvious) where I could find the metric space in practice. One last thing is that I am looking specifically for the examples which are different as metric spaces , so no equivalence to $\mathbb{R}^n$ or any of the subsets or my other list items.  I mean isometry with equivalence, I think, but maybe to homeomorphism, I am not sure how I make the best cut.  I am just unsatisfy for spaces that look too much like what I could build in $\mathbb{R}^n$ and use in practice.","This week, I come to the end of the first year analysis, and suffer from a ""crisis of motivation.""  With this question, I want to chase away my thought, ""Why is it important to study the general properties of metric spaces? Is every good example just a subset of $\mathbb{R}^n$?"" I will explain.  During many proofs, I visualize something like $\mathbb{R}^2$.  The professor always draws these pictures: This is good for many theorems, since much of the analysis is motivate by questions about $\mathbb{R}^n$.  But my mental picture is now only $\mathbb{R}^n$. For appreciate the study of metric spaces in full generality, and for intuition, I request more useful examples of metric spaces that are significantly different from $\mathbb{R}^n$, and are not contain in $\mathbb{R}^n$. Here, I say ""useful"" to mean that the example ""could naturally arise in another context"" , in mathematics or an application.  It is frustrate when I ask why some property does not hold in general, and someone tells me consider the discrete metric.  Yes, it is true, and it is easy to see, but the discrete metric is stupid.  Does my property fail in any metric space that someone would care about? In other words, I have the following taxonomy of metric spaces: $\mathbb{R}^n$ and the subsets degenerate examples like the discrete metric contrived examples that I would not see except in analysis (I mean if they are only exist to be pathological, this is where I think to place the Cantor set) I want to expand this taxonomy, so that when I hear the new definition or theorem, I can compare to this collection of good examples.  I know there must be examples with intricate and intuitive interpretations in statistics, science, and engineering. Which metric spaces are not in my mental taxonomy?  In general, what are the useful, non-obvious metric spaces that a student should keep in his mind when learning analysis? To me, the ideal answer includes the description of the metric space, what properties it has to be unique and different, some consequences of the properties that make different from my examples, and (if not obvious) where I could find the metric space in practice. One last thing is that I am looking specifically for the examples which are different as metric spaces , so no equivalence to $\mathbb{R}^n$ or any of the subsets or my other list items.  I mean isometry with equivalence, I think, but maybe to homeomorphism, I am not sure how I make the best cut.  I am just unsatisfy for spaces that look too much like what I could build in $\mathbb{R}^n$ and use in practice.",,"['analysis', 'metric-spaces', 'intuition', 'motivation']"
7,Are there periodic functions without a smallest period?,Are there periodic functions without a smallest period?,,"The Wikipedia page for periodic functions states that the smallest positive period $P$ of a function is called the fundamental period of the function (if it exists). I was intrigued by the condition that the function actually has a smallest period, so my question is, what properties of a function would cause it to be periodic but not have a smallest period?","The Wikipedia page for periodic functions states that the smallest positive period $P$ of a function is called the fundamental period of the function (if it exists). I was intrigued by the condition that the function actually has a smallest period, so my question is, what properties of a function would cause it to be periodic but not have a smallest period?",,"['analysis', 'periodic-functions']"
8,Why is the notion of analytic function so important?,Why is the notion of analytic function so important?,,"I think I have some understanding of what an analytic function is — it is a function that can be approximated by a Taylor power series. But why is the notion of ""analytic function"" so important? I guess being analytic entails some more interesting knowledge rather than just that it can be approximated by Taylor power series, right? Or, maybe I don't understand (underestimate) how a Taylor power series is important? Is it more than just a means of approximation?","I think I have some understanding of what an analytic function is — it is a function that can be approximated by a Taylor power series. But why is the notion of ""analytic function"" so important? I guess being analytic entails some more interesting knowledge rather than just that it can be approximated by Taylor power series, right? Or, maybe I don't understand (underestimate) how a Taylor power series is important? Is it more than just a means of approximation?",,"['analysis', 'power-series', 'taylor-expansion', 'analytic-functions']"
9,Rudin's Principles of Mathematical Analysis or Apostol's Mathematical Analysis?,Rudin's Principles of Mathematical Analysis or Apostol's Mathematical Analysis?,,"I am in high school and have no access to a professor or anyone. I previously used Calculus Volume I by Tom Apostol and Spivak's Calculus (for the differential calculus bit). I can choose between Mathematical Analysi s by Tom Apostol and Principles of Mathematical Analysis by Walter Rudin, as I was gifted Rudin by a friend and ended up buying Apostol as well. I will be indebted if someone told me which one is the tougher one and which one is better for the self-learner.  I have no issues about how tough the book is, but I would like  the book that enables me to understand the subject better without being too compressed or too verbose and guides me better.","I am in high school and have no access to a professor or anyone. I previously used Calculus Volume I by Tom Apostol and Spivak's Calculus (for the differential calculus bit). I can choose between Mathematical Analysi s by Tom Apostol and Principles of Mathematical Analysis by Walter Rudin, as I was gifted Rudin by a friend and ended up buying Apostol as well. I will be indebted if someone told me which one is the tougher one and which one is better for the self-learner.  I have no issues about how tough the book is, but I would like  the book that enables me to understand the subject better without being too compressed or too verbose and guides me better.",,"['analysis', 'reference-request', 'book-recommendation']"
10,Finite Sum $\sum\limits_{k=1}^{m-1}\frac{1}{\sin^2\frac{k\pi}{m}}$,Finite Sum,\sum\limits_{k=1}^{m-1}\frac{1}{\sin^2\frac{k\pi}{m}},"Question : Is the following true for any $m\in\mathbb N$?   $$\begin{align}\sum_{k=1}^{m-1}\frac{1}{\sin^2\frac{k\pi}{m}}=\frac{m^2-1}{3}\qquad(\star)\end{align}$$ Motivation : I reached $(\star)$ by using computer. It seems true, but I can't prove it. Can anyone help? By the way, I've been able to prove $\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{{\pi}^2}{6}$ by using $(\star)$. Proof : Let  $$f(x)=\frac{1}{\sin^2x}-\frac{1}{x^2}=\frac{(x-\sin x)(x+\sin x)}{x^2\sin^2 x}.$$ We know that $f(x)\gt0$ if $0\lt x\le {\pi}/{2}$, and that $\lim_{x\to 0}f(x)=1/3$. Hence, letting $f(0)=1/3$, we know that $f(x)$ is continuous and positive at $x=0$. Hence, since $f(x)\ (0\le x\le {\pi}/2)$ is bounded, there exists a constant $C$ such that $0\lt f(x)\lt C$. Hence, substituting $x={(k\pi)}/{(2n+1)}$ for this, we get $$0\lt \frac{1}{\frac{2n+1}{{\pi}^2}\sin^2\frac{k\pi}{2n+1}}-\frac{1}{k^2}\lt\frac{{\pi}^2C}{(2n+1)^2}.$$ Then, the sum of these from $1$ to $n$ satisfies  $$0\lt\frac{{\pi}^2\cdot 2n(n+1)}{(2n+1)^2\cdot 3}-\sum_{k=1}^{n}\frac{1}{k^2}\lt\frac{{\pi}^2Cn}{(2n+1)^2}.$$ Here, we used $(\star)$. Then, considering $n\to\infty$ leads what we desired.","Question : Is the following true for any $m\in\mathbb N$?   $$\begin{align}\sum_{k=1}^{m-1}\frac{1}{\sin^2\frac{k\pi}{m}}=\frac{m^2-1}{3}\qquad(\star)\end{align}$$ Motivation : I reached $(\star)$ by using computer. It seems true, but I can't prove it. Can anyone help? By the way, I've been able to prove $\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{{\pi}^2}{6}$ by using $(\star)$. Proof : Let  $$f(x)=\frac{1}{\sin^2x}-\frac{1}{x^2}=\frac{(x-\sin x)(x+\sin x)}{x^2\sin^2 x}.$$ We know that $f(x)\gt0$ if $0\lt x\le {\pi}/{2}$, and that $\lim_{x\to 0}f(x)=1/3$. Hence, letting $f(0)=1/3$, we know that $f(x)$ is continuous and positive at $x=0$. Hence, since $f(x)\ (0\le x\le {\pi}/2)$ is bounded, there exists a constant $C$ such that $0\lt f(x)\lt C$. Hence, substituting $x={(k\pi)}/{(2n+1)}$ for this, we get $$0\lt \frac{1}{\frac{2n+1}{{\pi}^2}\sin^2\frac{k\pi}{2n+1}}-\frac{1}{k^2}\lt\frac{{\pi}^2C}{(2n+1)^2}.$$ Then, the sum of these from $1$ to $n$ satisfies  $$0\lt\frac{{\pi}^2\cdot 2n(n+1)}{(2n+1)^2\cdot 3}-\sum_{k=1}^{n}\frac{1}{k^2}\lt\frac{{\pi}^2Cn}{(2n+1)^2}.$$ Here, we used $(\star)$. Then, considering $n\to\infty$ leads what we desired.",,"['analysis', 'trigonometry', 'summation', 'closed-form', 'fractions']"
11,Is the problem that Prof Otelbaev proved exactly the one stated by Clay Mathematics Institute?,Is the problem that Prof Otelbaev proved exactly the one stated by Clay Mathematics Institute?,,"Recently, mathematician Mukhtarbay Otelbaev published a paper Existence of a strong solution of the Navier-Stokes equations , in which he claim that he solved one of the Millennium Problems: existence and smoothness of the Navier-Stokes Equation. Unfortunately, the paper is in Russian but I cannot read Russian. There is a summary in English at the end of the paper, in which I found that the problem he proved was Let $Q \equiv (0,2\pi)^3\subseteq \mathbb{R}^3$ be a 3-dim domain, $\Omega=(0,a)\times Q$, a>0. $\textbf{Navier-stokes problem}$ is to find unknowns: a speed vector $u(t) = (u_1(t,x), u_2(t,x), u_3(t,x))$ and a scalar pressure function $p(t,x)$ at the points $x\in Q$ and time $t\in (0,a)$ satisfying the system of the equations ... initial   $$ u(t,x)\vert_{t=0} =0$$ ... But the problem that stated by the  Clay Mathematics Institute was, see http://www.claymath.org/sites/default/files/navierstokes.pdf , $\textbf{(B)  Existence and smoothness of Navier–Stokes solutions in $\mathbb{R}^3/\mathbb{Z}^3$.}$ ... Let $u^0$ be any smooth, divergence-free vector field satisfying $u^0(x+e_j) = u^0(x)$; we take $f(x, t)$ to be identically zero. Then there exist smooth functions $p(x,t)$, $u_i(x,t)$ on $\mathbb{R}^3 \times[0,\infty)$. that ... So my question is: 1) Can the $a>0$ in his proof be $\infty$? Or is it enough to prove the problem for arbitrary $a>0$? 2) Is there any theory that can turn the problem with arbitrary initial value $u^0$ (of course satisfying some condition) into a problem with initial value being $0$? 3) The problem that formulated by the Clay Institute assumes $f\equiv 0$.  Prof Otelbaev proved his result for all $f\in L_2(\Omega)$. Is this result much stronger? Update : There is an article stating that the $L_2$ estimate is not enough to solve the problem. (in Spanish)","Recently, mathematician Mukhtarbay Otelbaev published a paper Existence of a strong solution of the Navier-Stokes equations , in which he claim that he solved one of the Millennium Problems: existence and smoothness of the Navier-Stokes Equation. Unfortunately, the paper is in Russian but I cannot read Russian. There is a summary in English at the end of the paper, in which I found that the problem he proved was Let $Q \equiv (0,2\pi)^3\subseteq \mathbb{R}^3$ be a 3-dim domain, $\Omega=(0,a)\times Q$, a>0. $\textbf{Navier-stokes problem}$ is to find unknowns: a speed vector $u(t) = (u_1(t,x), u_2(t,x), u_3(t,x))$ and a scalar pressure function $p(t,x)$ at the points $x\in Q$ and time $t\in (0,a)$ satisfying the system of the equations ... initial   $$ u(t,x)\vert_{t=0} =0$$ ... But the problem that stated by the  Clay Mathematics Institute was, see http://www.claymath.org/sites/default/files/navierstokes.pdf , $\textbf{(B)  Existence and smoothness of Navier–Stokes solutions in $\mathbb{R}^3/\mathbb{Z}^3$.}$ ... Let $u^0$ be any smooth, divergence-free vector field satisfying $u^0(x+e_j) = u^0(x)$; we take $f(x, t)$ to be identically zero. Then there exist smooth functions $p(x,t)$, $u_i(x,t)$ on $\mathbb{R}^3 \times[0,\infty)$. that ... So my question is: 1) Can the $a>0$ in his proof be $\infty$? Or is it enough to prove the problem for arbitrary $a>0$? 2) Is there any theory that can turn the problem with arbitrary initial value $u^0$ (of course satisfying some condition) into a problem with initial value being $0$? 3) The problem that formulated by the Clay Institute assumes $f\equiv 0$.  Prof Otelbaev proved his result for all $f\in L_2(\Omega)$. Is this result much stronger? Update : There is an article stating that the $L_2$ estimate is not enough to solve the problem. (in Spanish)",,"['analysis', 'partial-differential-equations', 'fluid-dynamics']"
12,How to study for analysis?,How to study for analysis?,,"I am currently a first year undergraduate majoring in mathematics. I'm taking an introductory analysis course and find it very hard compared to other math couses. I know that the topics covered in the course are really the basics of real analysis, such as properties of $\mathbb{R}$, sequences and series, limits, continuity, Riemann integral, etc. I work much harder in analysis than in other courses such as abstract algebra, and am spending a lot of time to memorize all the theorems and their proofs mentioned in class. However, when it comes to work out a problem in the book or in the assignment on my own, I'm stuck. My guess is that I never learned how to do math rigorously, and I always rely on my intuition, which proved usually accurate in the past. The textbook we are using is ""Introduction to Real Analysis"" by Robert Bartle, 3rd ed., but I also downloaded and use some extra analysis notes from a few professors' webpages. Could you please give me any advice on how to study analysis? I'm now really desperate :(","I am currently a first year undergraduate majoring in mathematics. I'm taking an introductory analysis course and find it very hard compared to other math couses. I know that the topics covered in the course are really the basics of real analysis, such as properties of $\mathbb{R}$, sequences and series, limits, continuity, Riemann integral, etc. I work much harder in analysis than in other courses such as abstract algebra, and am spending a lot of time to memorize all the theorems and their proofs mentioned in class. However, when it comes to work out a problem in the book or in the assignment on my own, I'm stuck. My guess is that I never learned how to do math rigorously, and I always rely on my intuition, which proved usually accurate in the past. The textbook we are using is ""Introduction to Real Analysis"" by Robert Bartle, 3rd ed., but I also downloaded and use some extra analysis notes from a few professors' webpages. Could you please give me any advice on how to study analysis? I'm now really desperate :(",,"['analysis', 'education', 'learning']"
13,"A stronger version of discrete ""Liouville's theorem""","A stronger version of discrete ""Liouville's theorem""",,"If a function $f : \mathbb Z\times \mathbb Z \rightarrow \mathbb{R}^{+} $ satisfies the following condition $$\forall x, y \in \mathbb{Z}, f(x,y) = \dfrac{f(x + 1, y)+f(x, y + 1) + f(x - 1, y) +f(x, y - 1)}{4}$$ then is $f$ constant function?","If a function $f : \mathbb Z\times \mathbb Z \rightarrow \mathbb{R}^{+} $ satisfies the following condition $$\forall x, y \in \mathbb{Z}, f(x,y) = \dfrac{f(x + 1, y)+f(x, y + 1) + f(x - 1, y) +f(x, y - 1)}{4}$$ then is $f$ constant function?",,"['analysis', 'discrete-mathematics', 'recurrence-relations']"
14,"Does there exist $f:(0,\infty)\to(0,\infty)$ such that $f'=f^{-1}$?",Does there exist  such that ?,"f:(0,\infty)\to(0,\infty) f'=f^{-1}","Recently the following question was posed: does there exist a differentiable bijection $f:\mathbb R\to\mathbb R$ such that $f'=f^{-1}$ ? (Here, $f^{-1}$ is the inverse of $f$ with respect to composition of functions.) The answer, as it turns out, is negative, because such a bijection is monotonic, which implies that the derivative cannot be bijective. There seem to be no such problems if we instead search for a bijection $f:(0,\infty)\to(0,\infty)$ such that $f'=f^{-1}$ , so the following variation of the question seems interesting: Does there exist a differentiable bijection $f:(0,\infty)\to(0,\infty)$ such that $f'=f^{-1}$ ? ( Edit: as mentioned at the end of this question, the question of existence has been resolved. Is the solution unique?) Here's the first idea: if there is a function $f:(0,\infty)\to(0,\infty)$ such that $f'(x)=f^{-1}(x)$ holds for all $x\in(0,\infty)$ , then $$f'(f(x))=f^{-1}(f(x))=x$$ also must hold for all $x$ , since $f$ is bijective. This immediately reminds us of the chain rule, so we multiply by $f'(x)$ to yield $$f'(f(x))f'(x)=xf'(x)$$ which is equivalent to $$f'(f(x))f'(x)=xf'(x)+f(x)-f(x).$$ This implies (integrate from $1$ to $x$ , for instance) that there is a primitive function $F$ of $f$ such that $$f(f(x)) = x f(x) - F(x)$$ holds for all $x\in(0,\infty)$ , so we may try solving this equation instead. I don't know if this is any easier than the original problem, though. Maybe some kind of fixed point principle might work to show existence? It would be very nice if it turns out that there is a nice characterization of such functions. Edit: At the link pointed out by Christian Blatter in the comments there is an explicit solution ( $f(x)=\frac{x^\phi}{\phi^{\phi-1}}$ , where $\phi=\frac{1+\sqrt5}{2}$ is the golden ratio), so maybe it would also be interesting to know: Is this solution unique?","Recently the following question was posed: does there exist a differentiable bijection such that ? (Here, is the inverse of with respect to composition of functions.) The answer, as it turns out, is negative, because such a bijection is monotonic, which implies that the derivative cannot be bijective. There seem to be no such problems if we instead search for a bijection such that , so the following variation of the question seems interesting: Does there exist a differentiable bijection such that ? ( Edit: as mentioned at the end of this question, the question of existence has been resolved. Is the solution unique?) Here's the first idea: if there is a function such that holds for all , then also must hold for all , since is bijective. This immediately reminds us of the chain rule, so we multiply by to yield which is equivalent to This implies (integrate from to , for instance) that there is a primitive function of such that holds for all , so we may try solving this equation instead. I don't know if this is any easier than the original problem, though. Maybe some kind of fixed point principle might work to show existence? It would be very nice if it turns out that there is a nice characterization of such functions. Edit: At the link pointed out by Christian Blatter in the comments there is an explicit solution ( , where is the golden ratio), so maybe it would also be interesting to know: Is this solution unique?","f:\mathbb R\to\mathbb R f'=f^{-1} f^{-1} f f:(0,\infty)\to(0,\infty) f'=f^{-1} f:(0,\infty)\to(0,\infty) f'=f^{-1} f:(0,\infty)\to(0,\infty) f'(x)=f^{-1}(x) x\in(0,\infty) f'(f(x))=f^{-1}(f(x))=x x f f'(x) f'(f(x))f'(x)=xf'(x) f'(f(x))f'(x)=xf'(x)+f(x)-f(x). 1 x F f f(f(x)) = x f(x) - F(x) x\in(0,\infty) f(x)=\frac{x^\phi}{\phi^{\phi-1}} \phi=\frac{1+\sqrt5}{2}",['analysis']
15,"Why does Rudin say ""the rational number system is inadequate as a field""?","Why does Rudin say ""the rational number system is inadequate as a field""?",,"In the INTRODUCTION of chapter 1 of Baby Rudin, he says The rational number system is inadequate for many purposes, both as a field and as an ordered set. Addition and multiplication of rational numbers are commutative and associative, and multiplication is distributive over addition. Both 'zero' and 'one' exist. Plus, as I recall, rational numbers are the smallest subfield of $\mathbb{C}$. So exactly what does Rudin mean by “inadequate as a field”?","In the INTRODUCTION of chapter 1 of Baby Rudin, he says The rational number system is inadequate for many purposes, both as a field and as an ordered set. Addition and multiplication of rational numbers are commutative and associative, and multiplication is distributive over addition. Both 'zero' and 'one' exist. Plus, as I recall, rational numbers are the smallest subfield of $\mathbb{C}$. So exactly what does Rudin mean by “inadequate as a field”?",,"['analysis', 'rational-numbers']"
16,Absolute continuity of the Lebesgue integral,Absolute continuity of the Lebesgue integral,,"This is an exercise that I am having trouble with. Not for a grade just for practice.  Its an obvious result intuitively but I am having trouble making a rigorous argument. Assume $f$ is Lebesgue integrable on $E$.  Prove that for all $\varepsilon>0$ there exists a $\delta>0$ such that if the Lebesgue measure of $A$ is less than $\delta$, the integral of $|f|$ over $A$ is less than $\varepsilon$.  Here $A$ is a subset of $E$. Anyone have any ideas?","This is an exercise that I am having trouble with. Not for a grade just for practice.  Its an obvious result intuitively but I am having trouble making a rigorous argument. Assume $f$ is Lebesgue integrable on $E$.  Prove that for all $\varepsilon>0$ there exists a $\delta>0$ such that if the Lebesgue measure of $A$ is less than $\delta$, the integral of $|f|$ over $A$ is less than $\varepsilon$.  Here $A$ is a subset of $E$. Anyone have any ideas?",,"['analysis', 'measure-theory']"
17,What is the use of hyperreal numbers?,What is the use of hyperreal numbers?,,"For sometime I have been trying to come to terms with the concept of hyperreal numbers. It appears that they were invented as an alternative to the $\epsilon-\delta$ definitions to put the processes of calculus on a sound footing. From what I have read about hyperreal numbers I understand that they are an extension of real number system and include all real numbers and infinitesimals and infinities. I am wondering if hyperreal numbers are used only as a justification for the use of infinitesimals in calculus or do they serve to have some other applications also (of which I am not aware of)? Like when we extend our number system from $\mathbb{N}$ to $\mathbb{C}$ at each step there is some deficiency in the existing system which is removed in the next larger system. Thus $\mathbb{Z}$ enables subtraction which is not always possible in $\mathbb{N}$ and $\mathbb{Q}$ enables division which is not always possible in $\mathbb{Z}$. The reasons to go from $\mathbb{Q}$ to $\mathbb{R}$ are non-algebraic in nature. The next step from $\mathbb{R}$ to $\mathbb{C}$ is trivial and is based on need to enable square roots, but since the existing $\mathbb{R}$ is so powerful, the new system of complex numbers exploits this power to create rich field of complex analysis. Does the system of hyperreal numbers use the existing power of $\mathbb{R}$ to lead to a richer theory (something like the complex analysis I mentioned earlier)? Or does it serve only as an alternative to $\epsilon, \delta$ definitions? In other words what role do the non-real hyperreal numbers play in mathematics? Since I am novice in this subject of hyperreal numbers, I would want answers which avoid too much symbolism and technicalities and focus on the essence.","For sometime I have been trying to come to terms with the concept of hyperreal numbers. It appears that they were invented as an alternative to the $\epsilon-\delta$ definitions to put the processes of calculus on a sound footing. From what I have read about hyperreal numbers I understand that they are an extension of real number system and include all real numbers and infinitesimals and infinities. I am wondering if hyperreal numbers are used only as a justification for the use of infinitesimals in calculus or do they serve to have some other applications also (of which I am not aware of)? Like when we extend our number system from $\mathbb{N}$ to $\mathbb{C}$ at each step there is some deficiency in the existing system which is removed in the next larger system. Thus $\mathbb{Z}$ enables subtraction which is not always possible in $\mathbb{N}$ and $\mathbb{Q}$ enables division which is not always possible in $\mathbb{Z}$. The reasons to go from $\mathbb{Q}$ to $\mathbb{R}$ are non-algebraic in nature. The next step from $\mathbb{R}$ to $\mathbb{C}$ is trivial and is based on need to enable square roots, but since the existing $\mathbb{R}$ is so powerful, the new system of complex numbers exploits this power to create rich field of complex analysis. Does the system of hyperreal numbers use the existing power of $\mathbb{R}$ to lead to a richer theory (something like the complex analysis I mentioned earlier)? Or does it serve only as an alternative to $\epsilon, \delta$ definitions? In other words what role do the non-real hyperreal numbers play in mathematics? Since I am novice in this subject of hyperreal numbers, I would want answers which avoid too much symbolism and technicalities and focus on the essence.",,"['analysis', 'epsilon-delta', 'applications', 'nonstandard-analysis', 'infinitesimals']"
18,Teaching Introductory Real Analysis,Teaching Introductory Real Analysis,,"I am currently helping teach an introduction to real analysis course at UC Berkeley. The textbook we are using in Rudin's ""Principles of Mathematical Analysis"" (aka baby rudin). I am trying to find ways to help the students understand the material better. My jobs include Writing solutions to the homework exercises Finding other examples that can supplement Rudin, as Rudin sometimes doesn't present enough examples Suggest Problems for the Midterm and Final Examinations For (2), I have found Kenneth Ross's book (Elementary Analysis: The Theory of Calculus) very helpful. Does anyone suggest any other books, that do a good job in ""holding your hand and walking through"" with various concepts in analysis? Also, this is my first time teaching, so I am wondering how a good solution set should look like. Should it just show the formal solutions, or should it also tell the students a little bit about how to approach the problem and build intution. Or is that too much writing that will distract and be unmotivating for the student to read through? Also, each week we assign about 10-14 problems, but only 3 problems are graded. My solutions are to the 3 graded problems. I believe there is some advantage to this system, but do you I am putting the students at a strong disadvantage if I do not write all of the solutions? Time is limited sometimes, but I plan on writing the solutions to some other problems other than the graded problems. So: How do you decide which problems are worthy of writing solutions to (assuming time is limited and I don't have time to write solutions for all 14 problems)? Does anyone have good ideas to supplement Rudin(which I think is a great book, but my students may disagree) to the 40 student undergraduate class? This is a sort of a broad question. I'm wondering what others did in their previous experiences, if any, while teaching a class with that book?","I am currently helping teach an introduction to real analysis course at UC Berkeley. The textbook we are using in Rudin's ""Principles of Mathematical Analysis"" (aka baby rudin). I am trying to find ways to help the students understand the material better. My jobs include Writing solutions to the homework exercises Finding other examples that can supplement Rudin, as Rudin sometimes doesn't present enough examples Suggest Problems for the Midterm and Final Examinations For (2), I have found Kenneth Ross's book (Elementary Analysis: The Theory of Calculus) very helpful. Does anyone suggest any other books, that do a good job in ""holding your hand and walking through"" with various concepts in analysis? Also, this is my first time teaching, so I am wondering how a good solution set should look like. Should it just show the formal solutions, or should it also tell the students a little bit about how to approach the problem and build intution. Or is that too much writing that will distract and be unmotivating for the student to read through? Also, each week we assign about 10-14 problems, but only 3 problems are graded. My solutions are to the 3 graded problems. I believe there is some advantage to this system, but do you I am putting the students at a strong disadvantage if I do not write all of the solutions? Time is limited sometimes, but I plan on writing the solutions to some other problems other than the graded problems. So: How do you decide which problems are worthy of writing solutions to (assuming time is limited and I don't have time to write solutions for all 14 problems)? Does anyone have good ideas to supplement Rudin(which I think is a great book, but my students may disagree) to the 40 student undergraduate class? This is a sort of a broad question. I'm wondering what others did in their previous experiences, if any, while teaching a class with that book?",,"['analysis', 'education']"
19,The preimage of continuous function on a closed set is closed.,The preimage of continuous function on a closed set is closed.,,"My proof is very different from my reference, hence I am wondering is I got this right? Apparently, $F$ is continuous, and the identity matrix is closed. Now we want to show that the preimage of continuous function on closed set is closed. Let $D$ be a closed set, Consider a sequence $x_n \to x_0$ in which $x_n \in f^{-1}(D)$, and we will show that $x_0 \in f^{-1}(D)$. Since $f$ is continuous, we have a convergent sequence $$\lim_{n\to \infty} f(x_n) = f(x_0) = y.$$ But we know $y$ is in the range, hence, $x_0$ is in the domain. So the preimage is also closed since it contains all the limit points. Thank you.","My proof is very different from my reference, hence I am wondering is I got this right? Apparently, $F$ is continuous, and the identity matrix is closed. Now we want to show that the preimage of continuous function on closed set is closed. Let $D$ be a closed set, Consider a sequence $x_n \to x_0$ in which $x_n \in f^{-1}(D)$, and we will show that $x_0 \in f^{-1}(D)$. Since $f$ is continuous, we have a convergent sequence $$\lim_{n\to \infty} f(x_n) = f(x_0) = y.$$ But we know $y$ is in the range, hence, $x_0$ is in the domain. So the preimage is also closed since it contains all the limit points. Thank you.",,['analysis']
20,What is the definition of a measurable set?,What is the definition of a measurable set?,,"I have seen multiple definitions for what a measurable set is (all of which come together to form a sigma algebra). I was wondering if they are all equivalent and if not what situation would one be used over another? Definition 1 Let $(X, \Sigma)$ be a measurable space, then any set $S \in \Sigma$ is a measurable set. Measurable Space: The pair $(X, \Sigma)$ where $X$ is a set and $\Sigma$ is a $\sigma$ -algebra on $X$ Definition 2 Given a space $X$ let there exist an outer measure $\mu : 2^{X} \to [0, \infty]$ (where $2^{X} = \mathcal{P} \left( X \right) = $ all the subsets of $X$ ) then a set $S$ is measurable iff for every $A \in 2^{X}$ $$ \mu(A) = \mu(A \cap S) + \mu(A \cap S^{c}) = \mu(A \cap S) + \mu(A \setminus S) $$ Definition 3 (I'm drawing this one from memory from baby rudin, and it's defined on $\mathbb{R}$ ) Begin by defining a (outer $_1$ ) measure $\mu$ . Next put $\mathcal{M}_f$ to be the set of countable unions of intervals. Then a set $S$ is measurable iff $$ \exists \{ S_n \}_{n=0}^{\infty} \, s.t. \mu(S_n) \to \mu(S) \text{ as } n \to \infty $$ Now I know that the sets described in definition 2 form a sigma algebra on $X$ , and likewise I know the sets $S$ in definition 3 form a sigma algebra on $\mathbb{R}$ , but definition 1 seems to imply that any possible sigma algebra can be used. In definition 3 I cannot remember if he defines this using the outer measure (I believe he does). The only conclusion I have been able to draw from this is definition 2 and 3 must be equivalent, but we call these sets $\boldsymbol\mu$ -measurable (a specific measure is defined). However in definition 1 we call any set like that just measurable, and that given any sigma algebra $\Sigma \, \exists \mu$ an outer measure s.t. the sigma algebra formed by definition 2 with this outer measure is $\Sigma$ . Is this correctly put?","I have seen multiple definitions for what a measurable set is (all of which come together to form a sigma algebra). I was wondering if they are all equivalent and if not what situation would one be used over another? Definition 1 Let be a measurable space, then any set is a measurable set. Measurable Space: The pair where is a set and is a -algebra on Definition 2 Given a space let there exist an outer measure (where all the subsets of ) then a set is measurable iff for every Definition 3 (I'm drawing this one from memory from baby rudin, and it's defined on ) Begin by defining a (outer ) measure . Next put to be the set of countable unions of intervals. Then a set is measurable iff Now I know that the sets described in definition 2 form a sigma algebra on , and likewise I know the sets in definition 3 form a sigma algebra on , but definition 1 seems to imply that any possible sigma algebra can be used. In definition 3 I cannot remember if he defines this using the outer measure (I believe he does). The only conclusion I have been able to draw from this is definition 2 and 3 must be equivalent, but we call these sets -measurable (a specific measure is defined). However in definition 1 we call any set like that just measurable, and that given any sigma algebra an outer measure s.t. the sigma algebra formed by definition 2 with this outer measure is . Is this correctly put?","(X, \Sigma) S \in \Sigma (X, \Sigma) X \Sigma \sigma X X \mu : 2^{X} \to [0, \infty] 2^{X} = \mathcal{P} \left( X \right) =  X S A \in 2^{X} 
\mu(A) = \mu(A \cap S) + \mu(A \cap S^{c}) = \mu(A \cap S) + \mu(A \setminus S)
 \mathbb{R} _1 \mu \mathcal{M}_f S 
\exists \{ S_n \}_{n=0}^{\infty} \, s.t. \mu(S_n) \to \mu(S) \text{ as } n \to \infty
 X S \mathbb{R} \boldsymbol\mu \Sigma \, \exists \mu \Sigma","['analysis', 'measure-theory', 'measurable-sets']"
21,Where do Cantor sets naturally occur?,Where do Cantor sets naturally occur?,,"Cantor sets in general of course have many interesting properties on their own, and are also often used as examples of sets with these properties, but do they naturally occur in any application?","Cantor sets in general of course have many interesting properties on their own, and are also often used as examples of sets with these properties, but do they naturally occur in any application?",,"['analysis', 'descriptive-set-theory']"
22,Why are additional constraint and penalty term equivalent in ridge regression?,Why are additional constraint and penalty term equivalent in ridge regression?,,"Tikhonov regularization (or ridge regression) adds a constraint that $\|\beta\|^2$, the $L^2$-norm of the parameter vector, is not greater than a given value (say $c$). Equivalently, it may solve an unconstrained minimization of the least-squares penalty with $\alpha\|\beta\|^2$ added, where $\alpha$ is a constant (this is the Lagrangian form of the constrained problem). The above is from Wikipedia . Why is the unconstrained LS  with $\alpha\|\beta\|^2$ added to the cost equivalent to the LS problem with an additional constraint that $\|\beta\|^2 \leq c$? What is the relation between $\alpha$ and $c$? Thanks!","Tikhonov regularization (or ridge regression) adds a constraint that $\|\beta\|^2$, the $L^2$-norm of the parameter vector, is not greater than a given value (say $c$). Equivalently, it may solve an unconstrained minimization of the least-squares penalty with $\alpha\|\beta\|^2$ added, where $\alpha$ is a constant (this is the Lagrangian form of the constrained problem). The above is from Wikipedia . Why is the unconstrained LS  with $\alpha\|\beta\|^2$ added to the cost equivalent to the LS problem with an additional constraint that $\|\beta\|^2 \leq c$? What is the relation between $\alpha$ and $c$? Thanks!",,"['analysis', 'statistics', 'optimization', 'regularization', 'inverse-problems']"
23,"What exactly does it mean for a function to be ""well-behaved""?","What exactly does it mean for a function to be ""well-behaved""?",,"Often in my studies (economics) the assumption of a ""well-behaved"" function will be invoked. I don't exactly know what that entails (I think twice continuously differentiability is one of the requirements), nor do I know why this is necessary (though I imagine the why will depend on each case). Can someone explain it to me, and if there is an explanation of the why as well, I would be grateful. Thanks! EDIT : To give one example where the term appears, see this Wikipedia entry for utility functions, which says at one point: In order to simplify calculations, various assumptions have been made of utility functions. CES (constant elasticity of substitution, or isoelastic) utility Exponential utility Quasilinear utility Homothetic preferences Most utility functions used in modeling or theory are well-behaved . They are usually monotonic, quasi-concave, continuous and globally non-satiated. I might be wrong, but I don't think ""well-behaved"" means monotonic, quasi-concave, continuous and globally non-satiated. What about twice differentiable?","Often in my studies (economics) the assumption of a ""well-behaved"" function will be invoked. I don't exactly know what that entails (I think twice continuously differentiability is one of the requirements), nor do I know why this is necessary (though I imagine the why will depend on each case). Can someone explain it to me, and if there is an explanation of the why as well, I would be grateful. Thanks! EDIT : To give one example where the term appears, see this Wikipedia entry for utility functions, which says at one point: In order to simplify calculations, various assumptions have been made of utility functions. CES (constant elasticity of substitution, or isoelastic) utility Exponential utility Quasilinear utility Homothetic preferences Most utility functions used in modeling or theory are well-behaved . They are usually monotonic, quasi-concave, continuous and globally non-satiated. I might be wrong, but I don't think ""well-behaved"" means monotonic, quasi-concave, continuous and globally non-satiated. What about twice differentiable?",,"['terminology', 'analysis', 'economics']"
24,Sum of two periodic functions,Sum of two periodic functions,,"Let $f$ and $g$ be two periodic functions over $\Bbb{R}$ with the following property: If $T$ is a period of $f$, and $S$ is a period of $g$, then $T/S$ is irrational. Conjecture : $f+g$ is not periodic. Could you give a proof or a counter example?  It is easier if we assume continuity.  But is it true for arbitrary real valued functions?","Let $f$ and $g$ be two periodic functions over $\Bbb{R}$ with the following property: If $T$ is a period of $f$, and $S$ is a period of $g$, then $T/S$ is irrational. Conjecture : $f+g$ is not periodic. Could you give a proof or a counter example?  It is easier if we assume continuity.  But is it true for arbitrary real valued functions?",,['analysis']
25,Taylor expansion at infinity,Taylor expansion at infinity,,"Are there case where does make sense to speak about the ""Taylor expansion of a function ad infinity""? By inversion, sending $x \to \frac{1}{x}$ one could exchange $0\leftrightarrows\infty$; then if the values of the derivatives of a function are finite at infinity I was wondering if it is possible to give some sense to $(x-\infty)^n$ in order to define the ""Taylor expansion of a function ad infinity"".","Are there case where does make sense to speak about the ""Taylor expansion of a function ad infinity""? By inversion, sending $x \to \frac{1}{x}$ one could exchange $0\leftrightarrows\infty$; then if the values of the derivatives of a function are finite at infinity I was wondering if it is possible to give some sense to $(x-\infty)^n$ in order to define the ""Taylor expansion of a function ad infinity"".",,['analysis']
26,Computing the best constant in classical Hardy's inequality,Computing the best constant in classical Hardy's inequality,,"Classical Hardy's inequality (cfr. Hardy-Littlewood-Polya Inequalities , Theorem 327) If $p>1$, $f(x) \ge 0$ and $F(x)=\int_0^xf(y)\, dy$ then $$\tag{H} \int_0^\infty \left(\frac{F(x)}{x}\right)^p\, dx < C\int_0^\infty (f(x))^p\, dx $$ unless $f \equiv 0$. The best possibile constant is $C=\left(\frac{p}{p-1}\right)^p$ . I would like to prove the statement in italic regarding the best constant. As already noted by Will Jagy here , the book suggests stress-testing the inequality with $$f(x)=\begin{cases} 0 & 0\le x <1 \\ x^{-\alpha} & 1\le x \end{cases}$$ with $1/p< \alpha < 1$, then have $\alpha \to 1/p$. If I do so I get for $C$ the lower bound $$\operatorname{lim sup}_{\alpha \to 1/p}\frac{\alpha p -1}{(1-\alpha)^p}\int_1^\infty (x^{-\alpha}-x^{-1})^p\, dx\le C$$ but now I find myself in trouble in computing that lim sup. Can someone lend me a hand, please? UPDATE: A first attempt, based on an idea by Davide Giraudo, unfortunately failed. Davide pointed out that the claim would easily follow from $$\tag{!!} \left\lvert \int_1^\infty (x^{-\alpha}-x^{-1})^p\, dx - \int_1^\infty x^{-\alpha p }\, dx\right\rvert \to 0\quad \text{as}\ \alpha \to 1/p. $$ But this is false in general: for example if $p=2$ we get $$\int_1^\infty (x^{-2\alpha} -x^{-2\alpha} + 2x^{-\alpha-1}-x^{-2})\, dx \to \int_1^\infty(2x^{-3/2}-x^{-2})\, dx \ne 0.$$","Classical Hardy's inequality (cfr. Hardy-Littlewood-Polya Inequalities , Theorem 327) If $p>1$, $f(x) \ge 0$ and $F(x)=\int_0^xf(y)\, dy$ then $$\tag{H} \int_0^\infty \left(\frac{F(x)}{x}\right)^p\, dx < C\int_0^\infty (f(x))^p\, dx $$ unless $f \equiv 0$. The best possibile constant is $C=\left(\frac{p}{p-1}\right)^p$ . I would like to prove the statement in italic regarding the best constant. As already noted by Will Jagy here , the book suggests stress-testing the inequality with $$f(x)=\begin{cases} 0 & 0\le x <1 \\ x^{-\alpha} & 1\le x \end{cases}$$ with $1/p< \alpha < 1$, then have $\alpha \to 1/p$. If I do so I get for $C$ the lower bound $$\operatorname{lim sup}_{\alpha \to 1/p}\frac{\alpha p -1}{(1-\alpha)^p}\int_1^\infty (x^{-\alpha}-x^{-1})^p\, dx\le C$$ but now I find myself in trouble in computing that lim sup. Can someone lend me a hand, please? UPDATE: A first attempt, based on an idea by Davide Giraudo, unfortunately failed. Davide pointed out that the claim would easily follow from $$\tag{!!} \left\lvert \int_1^\infty (x^{-\alpha}-x^{-1})^p\, dx - \int_1^\infty x^{-\alpha p }\, dx\right\rvert \to 0\quad \text{as}\ \alpha \to 1/p. $$ But this is false in general: for example if $p=2$ we get $$\int_1^\infty (x^{-2\alpha} -x^{-2\alpha} + 2x^{-\alpha-1}-x^{-2})\, dx \to \int_1^\infty(2x^{-3/2}-x^{-2})\, dx \ne 0.$$",,"['analysis', 'inequality']"
27,Are there many fewer rational numbers than reals?,Are there many fewer rational numbers than reals?,,"Today my professor asked me to figure out the probability of getting a rational number from $[0,1]$. His answer was that the probability is $0$. Why is this?","Today my professor asked me to figure out the probability of getting a rational number from $[0,1]$. His answer was that the probability is $0$. Why is this?",,"['analysis', 'real-numbers']"
28,Meaning of the backslash operator on sets,Meaning of the backslash operator on sets,,I am self-studying analysis and ran across this: $\mathbb R \setminus \mathbb N$ is an open subset of $\mathbb R$ My best guess for interpretation was this: the set $\mathbb R \setminus \mathbb N$ is an open subset of $\mathbb R$. which doesn't mean much to me. Can anyone clear this up a bit? I know that the 'divided by' symbol is usually a slash in the opposite direction. And I am unsure how I would divide the reals by the naturals anyway.,I am self-studying analysis and ran across this: $\mathbb R \setminus \mathbb N$ is an open subset of $\mathbb R$ My best guess for interpretation was this: the set $\mathbb R \setminus \mathbb N$ is an open subset of $\mathbb R$. which doesn't mean much to me. Can anyone clear this up a bit? I know that the 'divided by' symbol is usually a slash in the opposite direction. And I am unsure how I would divide the reals by the naturals anyway.,,"['analysis', 'notation']"
29,Why do I get a converging result when pressing cosine multiple times on a calculator? [duplicate],Why do I get a converging result when pressing cosine multiple times on a calculator? [duplicate],,"This question already has answers here : Convergence of cos, sin, tan functions (2 answers) Closed 6 years ago . I'm trying to comprehend the following: If I choose any starting value (e.g. 1) and keep clicking on cosine on the calculator (in radian mode), it gives me a result of about 0.739085...(I believe it's the result of cos(x) = x), but when I repeat the same procedure using sin and tan, I get something completely different (looks like for sin it's converging to 0 while for tan I get very wild results). Thanks for your help.","This question already has answers here : Convergence of cos, sin, tan functions (2 answers) Closed 6 years ago . I'm trying to comprehend the following: If I choose any starting value (e.g. 1) and keep clicking on cosine on the calculator (in radian mode), it gives me a result of about 0.739085...(I believe it's the result of cos(x) = x), but when I repeat the same procedure using sin and tan, I get something completely different (looks like for sin it's converging to 0 while for tan I get very wild results). Thanks for your help.",,"['analysis', 'numerical-methods']"
30,1 and 2 norm inequality,1 and 2 norm inequality,,"While looking over my notes, my lecturer stated the following inequality; $$\|x\|_2 \leq \|x\|_1 \leq \sqrt{n}\|x\|_2$$ where $x \in \mathbb{R^n}.$ There was no proof given, and I've been trying to prove it for a while now. I know the definitions of the $1$ and $2$ norm, and, numerically the inequality seems obvious, although I don't know where to start rigorously. Thank you.","While looking over my notes, my lecturer stated the following inequality; $$\|x\|_2 \leq \|x\|_1 \leq \sqrt{n}\|x\|_2$$ where $x \in \mathbb{R^n}.$ There was no proof given, and I've been trying to prove it for a while now. I know the definitions of the $1$ and $2$ norm, and, numerically the inequality seems obvious, although I don't know where to start rigorously. Thank you.",,"['analysis', 'numerical-methods', 'vectors', 'normed-spaces']"
31,PDEs on Manifolds,PDEs on Manifolds,,"I am wondering if there is a general coordinate-independent way to define a Partial Differential Equation on a Smooth manifold. It is definitely true that in each coordinate neighborhood you could define a function to satisfy a differential equation, but when you change coordinates then the differential equation will most likely have a different form. For example, in $\mathbb{R}^3$ one says a function is harmonic if it satisfies $ \frac{\partial^2}{\partial x^2} f + \frac{\partial}{\partial y^2} f + \frac{\partial}{\partial z^2} f = 0 $ but it is not true that in spherical coordinates a function is harmonic if $ \frac{\partial^2}{\partial r^2} f + \frac{\partial}{\partial \theta^2} f + \frac{\partial}{\partial \phi^2} f = 0 $ The change of variables to spherical coordinates gives you a much different PDE. So basically, how can you define a differential operator which is coordinate independent on a smooth manifold, so that you can have some notion of a PDE on a manifold.","I am wondering if there is a general coordinate-independent way to define a Partial Differential Equation on a Smooth manifold. It is definitely true that in each coordinate neighborhood you could define a function to satisfy a differential equation, but when you change coordinates then the differential equation will most likely have a different form. For example, in $\mathbb{R}^3$ one says a function is harmonic if it satisfies $ \frac{\partial^2}{\partial x^2} f + \frac{\partial}{\partial y^2} f + \frac{\partial}{\partial z^2} f = 0 $ but it is not true that in spherical coordinates a function is harmonic if $ \frac{\partial^2}{\partial r^2} f + \frac{\partial}{\partial \theta^2} f + \frac{\partial}{\partial \phi^2} f = 0 $ The change of variables to spherical coordinates gives you a much different PDE. So basically, how can you define a differential operator which is coordinate independent on a smooth manifold, so that you can have some notion of a PDE on a manifold.",,"['analysis', 'differential-geometry', 'partial-differential-equations']"
32,Computing the product of p/(p - 2) over the odd primes,Computing the product of p/(p - 2) over the odd primes,,"I'd like to calculate, or find a reasonable estimate for, the Mertens-like product $$\prod_{2<p\le n}\frac{p}{p-2}=\left(\prod_{2<p\le n}1-\frac{2}{p}\right)^{-1}$$ Also, how does this behave asymptotically? Hmm... trying to think this one out, I get $$\left(\prod_{2<p\le n}1-\frac{2}{p}\right)^{-1}=\exp\log\left(\left(\prod_{2<p\le n}1-\frac{2}{p}\right)^{-1}\right)=\exp-\log\left(\prod_{2<p\le n}1-\frac{2}{p}\right)$$ which is $$\exp-\sum_{2<p\le n}\log\left(1-\frac{2}{p}\right)=\exp\sum_{2<p\le n}\left(\frac{2}{p}+\frac12\left(\frac{2}{p}\right)^2+\frac13\left(\frac{2}{p}\right)^3+\cdots\right)$$ which, with P(s) the prime zeta function and f(s)=P(s)-2^s, is less than $$\exp\left(\frac42f(2)+\frac83f(3)+\cdots+\sum_{2<p\le n}\frac{2}{p}\right)$$ which might not be a bad approximation for n large.  But I can't immediately find a series for P(s) with $s\to+\infty$ and I'm not sure if there's a better way.  Help?","I'd like to calculate, or find a reasonable estimate for, the Mertens-like product $$\prod_{2<p\le n}\frac{p}{p-2}=\left(\prod_{2<p\le n}1-\frac{2}{p}\right)^{-1}$$ Also, how does this behave asymptotically? Hmm... trying to think this one out, I get $$\left(\prod_{2<p\le n}1-\frac{2}{p}\right)^{-1}=\exp\log\left(\left(\prod_{2<p\le n}1-\frac{2}{p}\right)^{-1}\right)=\exp-\log\left(\prod_{2<p\le n}1-\frac{2}{p}\right)$$ which is $$\exp-\sum_{2<p\le n}\log\left(1-\frac{2}{p}\right)=\exp\sum_{2<p\le n}\left(\frac{2}{p}+\frac12\left(\frac{2}{p}\right)^2+\frac13\left(\frac{2}{p}\right)^3+\cdots\right)$$ which, with P(s) the prime zeta function and f(s)=P(s)-2^s, is less than $$\exp\left(\frac42f(2)+\frac83f(3)+\cdots+\sum_{2<p\le n}\frac{2}{p}\right)$$ which might not be a bad approximation for n large.  But I can't immediately find a series for P(s) with $s\to+\infty$ and I'm not sure if there's a better way.  Help?",,"['number-theory', 'analysis', 'prime-numbers', 'analytic-number-theory']"
33,Applications of Pseudodifferential Operators,Applications of Pseudodifferential Operators,,"I am very interested in just about anything that has to do with PDE's, and inevitably pseudodifferential operators comes up. Its obvious that such a novel way of looking at PDE's would be important, but I cant find anything in the literature thus far that explains where they could be applied, in a mathematical physics sense. So basically, I'm wondering if there are any physical, or numerical analysis type applications of pseudodifferential operators.","I am very interested in just about anything that has to do with PDE's, and inevitably pseudodifferential operators comes up. Its obvious that such a novel way of looking at PDE's would be important, but I cant find anything in the literature thus far that explains where they could be applied, in a mathematical physics sense. So basically, I'm wondering if there are any physical, or numerical analysis type applications of pseudodifferential operators.",,"['analysis', 'partial-differential-equations', 'fourier-analysis', 'differential-topology', 'differential-operators']"
34,Why isn't there a continuously differentiable injection into a lower dimensional space?,Why isn't there a continuously differentiable injection into a lower dimensional space?,,"How to show that a continuously differentiable function $f:\mathbb{R}^{n}\to \mathbb{R}^m$ can't be a 1-1 when $n>m$? This is an exercise in Spivak's ""Calculus on manifolds"". I can solve the problem in the case $m=1$. To see this, note that the result is obvious if the first partial derivative $D_1 f(x)=0$ for all $x$ as then $f$ will be independent of the first variable. Otherwise there exists $a\in \mathbb{R}^n$ s.t. $D_1 f(a)\not=0$. Put $g:A\to \mathbb{R}^n, g(x)=(f(x),x_2, \ldots, x_n)$ (with $x=(x_1,\ldots, x_n)$). Now the Jacobian  $$  g'(x) = \left[           \begin{array}{cc}              D_1 f(x) & 0 \\              0        & I_{n-1}                \end{array}         \right] $$  so that $\text{det}\, g'(a)=D_1 f(a)\not=0$. By the Inverse Function Theorem we have an open set $B\subseteq A$ s.t. $g:B\to g(B)$ is bijective with a differentiable inverse. In particular, $g(B)$ is open. Pick any $g(b)=(f(b),b_2,\ldots, b_n)\in g(B)$. Since $g(B)$ is open there exists $\varepsilon > 0$ such that  $(f(b), b_2, \ldots, b_n+\varepsilon) \in g(B)$. Thus we can find $b'\in B$ s.t. $g(b')=(f(b), b_2, \ldots, b_n+\varepsilon)$. By the definition of $g$, $f(b')=f(b)$ with $b'$ and $b$ differing in the last coordinate. Thus $f$ isn't injective. However, I cannot generalize this argument to higher dimenssions.","How to show that a continuously differentiable function $f:\mathbb{R}^{n}\to \mathbb{R}^m$ can't be a 1-1 when $n>m$? This is an exercise in Spivak's ""Calculus on manifolds"". I can solve the problem in the case $m=1$. To see this, note that the result is obvious if the first partial derivative $D_1 f(x)=0$ for all $x$ as then $f$ will be independent of the first variable. Otherwise there exists $a\in \mathbb{R}^n$ s.t. $D_1 f(a)\not=0$. Put $g:A\to \mathbb{R}^n, g(x)=(f(x),x_2, \ldots, x_n)$ (with $x=(x_1,\ldots, x_n)$). Now the Jacobian  $$  g'(x) = \left[           \begin{array}{cc}              D_1 f(x) & 0 \\              0        & I_{n-1}                \end{array}         \right] $$  so that $\text{det}\, g'(a)=D_1 f(a)\not=0$. By the Inverse Function Theorem we have an open set $B\subseteq A$ s.t. $g:B\to g(B)$ is bijective with a differentiable inverse. In particular, $g(B)$ is open. Pick any $g(b)=(f(b),b_2,\ldots, b_n)\in g(B)$. Since $g(B)$ is open there exists $\varepsilon > 0$ such that  $(f(b), b_2, \ldots, b_n+\varepsilon) \in g(B)$. Thus we can find $b'\in B$ s.t. $g(b')=(f(b), b_2, \ldots, b_n+\varepsilon)$. By the definition of $g$, $f(b')=f(b)$ with $b'$ and $b$ differing in the last coordinate. Thus $f$ isn't injective. However, I cannot generalize this argument to higher dimenssions.",,"['analysis', 'multivariable-calculus']"
35,How prove this function $f(x)=x!-x^n$ is injective,How prove this function  is injective,f(x)=x!-x^n,"Question: For any positive integer $n$ such $n\neq 2^m-1, n\ge 2$, and function $f$ defined by  $$f(x)=x!-x^n$$ show that : $f:N\to Z$ is injective. My idea: maybe for $x\neq y$ with $x,y\in N^{+}$, then this equation $$x!-x^n=y!-y^n$$ has no solutions? $$\Longleftrightarrow x!-y!=x^n-y^n$$ if $(x,y)=d$ then we let $x=dx_{1},y=dy_{1}$ then $$(dx_{1})!-(dy_{1})!=d^n[(x_{1})!-(y_{1})!]$$ To solve this problem, maybe we can prove $f$ is monotonic? But I can't. It is said that this is from Mathematical olympiad problem.","Question: For any positive integer $n$ such $n\neq 2^m-1, n\ge 2$, and function $f$ defined by  $$f(x)=x!-x^n$$ show that : $f:N\to Z$ is injective. My idea: maybe for $x\neq y$ with $x,y\in N^{+}$, then this equation $$x!-x^n=y!-y^n$$ has no solutions? $$\Longleftrightarrow x!-y!=x^n-y^n$$ if $(x,y)=d$ then we let $x=dx_{1},y=dy_{1}$ then $$(dx_{1})!-(dy_{1})!=d^n[(x_{1})!-(y_{1})!]$$ To solve this problem, maybe we can prove $f$ is monotonic? But I can't. It is said that this is from Mathematical olympiad problem.",,"['analysis', 'functions']"
36,Why is the rational number system inadequate for analysis?,Why is the rational number system inadequate for analysis?,,"In the very first chapter of Principles of Mathematical Analysis, the author pointed out as follows: The rational number system is inadequate for many purposes, both as a field and as an ordered set. For instance, there is no rational $p$ such that $p^{2}=2$... However, considering the fact that the rational number is indeed a field , It seems unclear to me in what perspective it is inadequate for a satisfactory discussion of analysis.","In the very first chapter of Principles of Mathematical Analysis, the author pointed out as follows: The rational number system is inadequate for many purposes, both as a field and as an ordered set. For instance, there is no rational $p$ such that $p^{2}=2$... However, considering the fact that the rational number is indeed a field , It seems unclear to me in what perspective it is inadequate for a satisfactory discussion of analysis.",,['analysis']
37,What is the difference between a function and a distribution?,What is the difference between a function and a distribution?,,"I remember there was a tongue-in-cheek rule in mathematical analysis saying that to obtain the Fourier transform of a function $f(t)$, it is enough to get its Laplace transform $F(s)$, and replace $s$ by $j\omega$. Because their formula is pretty much the same except for the variable of integration. And I know that this is not necessarily true (that's why I used the term tongue-in-cheek) e.g. take $f(t)=1$ to see the obvious difference. I read somewhere that although their formula is somehow similar, their result is not necessarily similar because Laplace transform is a function and Fourier transform is a distribution . For example, the Dirac delta, $\delta(\omega)$ is a distribution and not a function. So this led me to wonder: What is the difference between a function and a distribution? (preferably in layman's terms) Why the Laplace transform is a function and Fourier transform is a distribution? I mean, they are both infinite integrals. So what am I missing?","I remember there was a tongue-in-cheek rule in mathematical analysis saying that to obtain the Fourier transform of a function $f(t)$, it is enough to get its Laplace transform $F(s)$, and replace $s$ by $j\omega$. Because their formula is pretty much the same except for the variable of integration. And I know that this is not necessarily true (that's why I used the term tongue-in-cheek) e.g. take $f(t)=1$ to see the obvious difference. I read somewhere that although their formula is somehow similar, their result is not necessarily similar because Laplace transform is a function and Fourier transform is a distribution . For example, the Dirac delta, $\delta(\omega)$ is a distribution and not a function. So this led me to wonder: What is the difference between a function and a distribution? (preferably in layman's terms) Why the Laplace transform is a function and Fourier transform is a distribution? I mean, they are both infinite integrals. So what am I missing?",,"['analysis', 'functions', 'fourier-analysis', 'laplace-transform']"
38,Fourier transform of the indicator of the unit ball,Fourier transform of the indicator of the unit ball,,"What is the Fourier transform of the indicator of the unit ball in $\mathbb R^n$? I think it is known as one of special functions, so I would be happy to know which one.","What is the Fourier transform of the indicator of the unit ball in $\mathbb R^n$? I think it is known as one of special functions, so I would be happy to know which one.",,"['analysis', 'fourier-analysis', 'special-functions']"
39,Nice way of thinking about the Laplace operator... but what's the proof?,Nice way of thinking about the Laplace operator... but what's the proof?,,"Here's a nice fact: roughly speaking, the Laplace operator gives you the difference between the value of a function at a point and the average value at ""neighboring"" points. More precisely, in $\mathbb{R}^n$ let $S_r(x)$ be the $(n-1)$-dimensional sphere of radius $r$ centered at the point $x$, let $V_r$ be the volume of this sphere, and let $d\sigma$ be the volume element on this sphere.  Then at every point $x \in \mathbb{R}^n$ $$ \lim_{r \rightarrow 0} \frac{\int_{S_r(x)} f d\sigma}{V_r} - f(x) = \frac{r^2}{2n} \Delta f(x) + \bar{o}(r^2) $$ for all $C^2$ functions $f$ on $\mathbb{R}^2$.  So far, however, I've been unable to prove this fact.  In the book I'm following (Grigor'yan, Heat Kernel and Analysis on Manifolds ) the only theorem that's really been introduced so far is one of Green's identities: $$ \langle u, \Delta v \rangle = \langle \nabla u, \nabla v \rangle, $$ where at least one of $u,v$ is compactly supported and $\langle \cdot, \cdot \rangle$ denotes the inner product over $\mathbb{R}^n$.  So, it seemed natural to consider any compactly-supported test function $g \in C^1(\mathbb{R}^n)$, in which case the formula above would look something like $$ \frac{\int_{S_r(x)}\langle f, g \rangle d\sigma}{V_r} - \langle f, g \rangle = \frac{r^2}{2n} \langle \nabla f, \nabla g \rangle + \langle g + \bar{o}(r^2) \rangle. $$ Not sure where to take it from here, though (or even if this is the right direction!).  Any hints/tricks are much appreciated.  (For the record, I am not solving this problem as a homework exercise.) Finally, a more minor question: what does the bar signify in $\bar{o}(r^2)$? Thanks!","Here's a nice fact: roughly speaking, the Laplace operator gives you the difference between the value of a function at a point and the average value at ""neighboring"" points. More precisely, in $\mathbb{R}^n$ let $S_r(x)$ be the $(n-1)$-dimensional sphere of radius $r$ centered at the point $x$, let $V_r$ be the volume of this sphere, and let $d\sigma$ be the volume element on this sphere.  Then at every point $x \in \mathbb{R}^n$ $$ \lim_{r \rightarrow 0} \frac{\int_{S_r(x)} f d\sigma}{V_r} - f(x) = \frac{r^2}{2n} \Delta f(x) + \bar{o}(r^2) $$ for all $C^2$ functions $f$ on $\mathbb{R}^2$.  So far, however, I've been unable to prove this fact.  In the book I'm following (Grigor'yan, Heat Kernel and Analysis on Manifolds ) the only theorem that's really been introduced so far is one of Green's identities: $$ \langle u, \Delta v \rangle = \langle \nabla u, \nabla v \rangle, $$ where at least one of $u,v$ is compactly supported and $\langle \cdot, \cdot \rangle$ denotes the inner product over $\mathbb{R}^n$.  So, it seemed natural to consider any compactly-supported test function $g \in C^1(\mathbb{R}^n)$, in which case the formula above would look something like $$ \frac{\int_{S_r(x)}\langle f, g \rangle d\sigma}{V_r} - \langle f, g \rangle = \frac{r^2}{2n} \langle \nabla f, \nabla g \rangle + \langle g + \bar{o}(r^2) \rangle. $$ Not sure where to take it from here, though (or even if this is the right direction!).  Any hints/tricks are much appreciated.  (For the record, I am not solving this problem as a homework exercise.) Finally, a more minor question: what does the bar signify in $\bar{o}(r^2)$? Thanks!",,"['analysis', 'multivariable-calculus']"
40,Exponential of powers of the derivative operator,Exponential of powers of the derivative operator,,"A translation operator The Taylor series of a function $f$ is $$f(x)=\sum_{n=0}^\infty\frac{(\partial_x^nf)(a)}{n!}(x-a)^n$$ where $\partial_x$ is the derivative operator. Expanding about $x+b$: $$f(x+b)=\sum_{n=0}^\infty\frac{(\partial_x^nf)(a)}{n!}(x+b-a)^n$$ Letting $a=x$: $$f(x+b)=\sum_{n=0}^\infty\frac{(\partial_x^nf)(x)}{n!}b^n=\sum_{n=0}^\infty\frac{((b\partial_x)^nf)(x)}{n!}$$ By definition $$e^{b\partial_x}=\sum_{n=0}^\infty\frac{(b\partial_x)^n}{n!}$$ Hence $$f(x+b)=(e^{b\partial_x}f)(x)$$ Hence $e^{\partial_x}=T$ where $T$ is the translation operator and $(Tf)(x)=f(x+1)$. A scaling operator We can also find a closed form for a scaling operator $S$ where $(Sf)(x)=f(ax)$. $$f(xa)=f(e^{\log{xa}})=f(e^{\log x+\log a})=f(e^{y+\log a})$$ where $y=\log x$. Letting $g(z)=f(e^z)$: $$f(xa)=g(y+\log a)$$ By our first theorem, $g(y+b)=(e^{b\partial_y}g)(y)$. Letting $b=\log a$: $$f(xa)=(e^{(\log a)\partial_y}g)(y)=(a^{\partial_y}g)(y)=(a^{\partial_{\log x}}f)(e^y)$$ Since $$\frac{\partial}{\partial \log x}=\frac{\partial x}{\partial \log x}\frac{\partial}{\partial x}=x\frac{\partial}{\partial x}$$ Then $$f(xa)=(a^{x\partial_x}f)(e^{\log x})=(a^{x\partial_x}f)(x)$$ Therefore $S=a^{x\partial_x}$ defines our scaling operator. A general operator Suppose we want an operator $G$ such that $(Gf)(x)=f(g(x))$. Consider: $$(e^{\partial_{h(x)}}f)(x)=(e^{\partial_y}f)(x)=(e^{\partial_y}f)(h^{-1}(h(x)))=(e^{\partial_y}f)(h^{-1}(y))$$ where $y=h(x)$. Letting $j=f\circ h^{-1}$ yields: $$(e^{\partial_{h(x)}}f)(x)=(e^{\partial_y}j)(y)=j(y+1)=f(h^{-1}(y+1))=f(h^{-1}(h(x)+1))$$ Hence solving the functional equation $$h^{-1}(h(x)+1)=g(x)$$ for $h(x)$ allows us to define our general operator $G=e^{\partial_{h(x)}}$. For example, letting $g(x)=xa$, the function $h(x)=\frac{\log x}{\log a}$ is a solution: $$h^{-1}(y)=a^y$$ $$h^{-1}(h(x)+1)=a^{h(x)+1}=a^{\frac{\log x}{\log a}+1}=a^{\frac{\log x}{\log a}}a=e^{\log x}a=xa$$ Hence the corresponding operator takes the form $$e^{\partial_{h(x)}}=e^{\partial_{\frac{\log x}{\log a}}}=e^{\log a\partial_{\log x}}=a^{x\partial_x}$$ This is the scaling operator we derived before. The case $e^{\partial_{h(x)}}=e^{x^n\partial_x}$ or equivalently $h(x)=\frac{x^{1-n}}{1-n}$ corresponds to the basis for the Witt algebra . The question My question is as follows: Can a similar procedure be used to find $e^{{\partial_x}^2}$, or $e^{{\partial_{h(x)}}^n}$ in general? Note that the commutator of $x$ and $\partial_x$ is nonzero and given by: $$[\partial_x,x]=\partial_xx-x\partial_x=1$$ Furthermore, given the product rule $Dab=(Da)b+aDb$, it seems to be the case that $$(x\partial_x)^2=x\partial_x+x^2\partial_x^2$$ $$(x\partial_x)^3=x\partial_x+3x^2\partial_x^2+x^3\partial_x^3$$ $$(x\partial_x)^4=x\partial_x+7x^2\partial_x^2+6x^3\partial_x^3+x^4\partial_x^4$$ and in general $$(x\partial_x)^n=\sum_{k=1}^n \genfrac{\lbrace}{\rbrace}{0pt}{}{n}{k} x^k\partial_x^k$$ where $\genfrac{\lbrace}{\rbrace}{0pt}{}{a}{b}$ are the Stirling numbers of the second kind . I have a strong suspicion the answer might have to do with properties of the Fourier and Laplace transforms, as seen here and here .","A translation operator The Taylor series of a function $f$ is $$f(x)=\sum_{n=0}^\infty\frac{(\partial_x^nf)(a)}{n!}(x-a)^n$$ where $\partial_x$ is the derivative operator. Expanding about $x+b$: $$f(x+b)=\sum_{n=0}^\infty\frac{(\partial_x^nf)(a)}{n!}(x+b-a)^n$$ Letting $a=x$: $$f(x+b)=\sum_{n=0}^\infty\frac{(\partial_x^nf)(x)}{n!}b^n=\sum_{n=0}^\infty\frac{((b\partial_x)^nf)(x)}{n!}$$ By definition $$e^{b\partial_x}=\sum_{n=0}^\infty\frac{(b\partial_x)^n}{n!}$$ Hence $$f(x+b)=(e^{b\partial_x}f)(x)$$ Hence $e^{\partial_x}=T$ where $T$ is the translation operator and $(Tf)(x)=f(x+1)$. A scaling operator We can also find a closed form for a scaling operator $S$ where $(Sf)(x)=f(ax)$. $$f(xa)=f(e^{\log{xa}})=f(e^{\log x+\log a})=f(e^{y+\log a})$$ where $y=\log x$. Letting $g(z)=f(e^z)$: $$f(xa)=g(y+\log a)$$ By our first theorem, $g(y+b)=(e^{b\partial_y}g)(y)$. Letting $b=\log a$: $$f(xa)=(e^{(\log a)\partial_y}g)(y)=(a^{\partial_y}g)(y)=(a^{\partial_{\log x}}f)(e^y)$$ Since $$\frac{\partial}{\partial \log x}=\frac{\partial x}{\partial \log x}\frac{\partial}{\partial x}=x\frac{\partial}{\partial x}$$ Then $$f(xa)=(a^{x\partial_x}f)(e^{\log x})=(a^{x\partial_x}f)(x)$$ Therefore $S=a^{x\partial_x}$ defines our scaling operator. A general operator Suppose we want an operator $G$ such that $(Gf)(x)=f(g(x))$. Consider: $$(e^{\partial_{h(x)}}f)(x)=(e^{\partial_y}f)(x)=(e^{\partial_y}f)(h^{-1}(h(x)))=(e^{\partial_y}f)(h^{-1}(y))$$ where $y=h(x)$. Letting $j=f\circ h^{-1}$ yields: $$(e^{\partial_{h(x)}}f)(x)=(e^{\partial_y}j)(y)=j(y+1)=f(h^{-1}(y+1))=f(h^{-1}(h(x)+1))$$ Hence solving the functional equation $$h^{-1}(h(x)+1)=g(x)$$ for $h(x)$ allows us to define our general operator $G=e^{\partial_{h(x)}}$. For example, letting $g(x)=xa$, the function $h(x)=\frac{\log x}{\log a}$ is a solution: $$h^{-1}(y)=a^y$$ $$h^{-1}(h(x)+1)=a^{h(x)+1}=a^{\frac{\log x}{\log a}+1}=a^{\frac{\log x}{\log a}}a=e^{\log x}a=xa$$ Hence the corresponding operator takes the form $$e^{\partial_{h(x)}}=e^{\partial_{\frac{\log x}{\log a}}}=e^{\log a\partial_{\log x}}=a^{x\partial_x}$$ This is the scaling operator we derived before. The case $e^{\partial_{h(x)}}=e^{x^n\partial_x}$ or equivalently $h(x)=\frac{x^{1-n}}{1-n}$ corresponds to the basis for the Witt algebra . The question My question is as follows: Can a similar procedure be used to find $e^{{\partial_x}^2}$, or $e^{{\partial_{h(x)}}^n}$ in general? Note that the commutator of $x$ and $\partial_x$ is nonzero and given by: $$[\partial_x,x]=\partial_xx-x\partial_x=1$$ Furthermore, given the product rule $Dab=(Da)b+aDb$, it seems to be the case that $$(x\partial_x)^2=x\partial_x+x^2\partial_x^2$$ $$(x\partial_x)^3=x\partial_x+3x^2\partial_x^2+x^3\partial_x^3$$ $$(x\partial_x)^4=x\partial_x+7x^2\partial_x^2+6x^3\partial_x^3+x^4\partial_x^4$$ and in general $$(x\partial_x)^n=\sum_{k=1}^n \genfrac{\lbrace}{\rbrace}{0pt}{}{n}{k} x^k\partial_x^k$$ where $\genfrac{\lbrace}{\rbrace}{0pt}{}{a}{b}$ are the Stirling numbers of the second kind . I have a strong suspicion the answer might have to do with properties of the Fourier and Laplace transforms, as seen here and here .",,"['analysis', 'taylor-expansion', 'exponential-function', 'lie-algebras', 'partial-derivative']"
41,Continuous versus differentiable,Continuous versus differentiable,,"A function is ""differentiable"" if it has a derivative. A function is ""continuous"" if it has no sudden jumps in it. Until today, I thought these were merely two equivalent definitions of the same concept. But I've read some stuff today which seems to be claiming that this is not the case. The obvious next question is ""why?"" Apparently somebody has already asked: Are Continuous Functions Always Differentiable? Several answers were given, but I don't understand any of them. In particular, Wikipedia and one of the replies above both claim that $|x|$ has no derivative. Can anyone explain this extremely unexpected result? Edit: Apparently some people dislike the fact that this is non-obvious to me. To be clear: I am not saying that the result is untrue . (I'm sure many great mathematicians have analysed the question very carefuly and are quite sure of the answer.) I am saying that it is extremely perplexing . (As a general rule, mathematics has a habit of doing that. Which is one of the reasons why we demand proof of everything.) In particular, can anyone explain precisely why the derivative of $|x|$ at zero is not simply zero? After all, the function is neither increasing nor decreasing, which ought to mean the derivative is zero. Alternatively, the expression $$\frac{|x + a| - |x - a|}{a}$$ becomes closer and closer to zero as $a$ becomes closer to zero when $x=0$. (In fact, it is exactly zero for all $a$! ) Is that not how derivatives work? Several answers have suggested that the derivative is not defined here ""because there would be a jump in the derivative at that point"". This seems to assert that a continuous function must never have a discontinuous derivative; I'm not convinced that this is the case. Can anyone confirm or refuse this argument?","A function is ""differentiable"" if it has a derivative. A function is ""continuous"" if it has no sudden jumps in it. Until today, I thought these were merely two equivalent definitions of the same concept. But I've read some stuff today which seems to be claiming that this is not the case. The obvious next question is ""why?"" Apparently somebody has already asked: Are Continuous Functions Always Differentiable? Several answers were given, but I don't understand any of them. In particular, Wikipedia and one of the replies above both claim that $|x|$ has no derivative. Can anyone explain this extremely unexpected result? Edit: Apparently some people dislike the fact that this is non-obvious to me. To be clear: I am not saying that the result is untrue . (I'm sure many great mathematicians have analysed the question very carefuly and are quite sure of the answer.) I am saying that it is extremely perplexing . (As a general rule, mathematics has a habit of doing that. Which is one of the reasons why we demand proof of everything.) In particular, can anyone explain precisely why the derivative of $|x|$ at zero is not simply zero? After all, the function is neither increasing nor decreasing, which ought to mean the derivative is zero. Alternatively, the expression $$\frac{|x + a| - |x - a|}{a}$$ becomes closer and closer to zero as $a$ becomes closer to zero when $x=0$. (In fact, it is exactly zero for all $a$! ) Is that not how derivatives work? Several answers have suggested that the derivative is not defined here ""because there would be a jump in the derivative at that point"". This seems to assert that a continuous function must never have a discontinuous derivative; I'm not convinced that this is the case. Can anyone confirm or refuse this argument?",,"['analysis', 'intuition', 'derivatives', 'continuity']"
42,"A continuous function $f$ from a closed bounded interval $[a, b]$ into $\mathbb{R}$ is uniformly continuous",A continuous function  from a closed bounded interval  into  is uniformly continuous,"f [a, b] \mathbb{R}","I am reading (as a supplement) the book Basic Real Analysis, by Anthony Knapp. Before I proceed into reading a proof, I want to be sure that the result seems obvious. Yet, I am having trouble seeing through this one. It annoys me too much in order to disregard it: Theorem . A continuous function $f$ from a closed bounded interval $[a, b]$ into $\mathbb{R}$ is uniformly continuous. What gives? Why can't we provide the counterexample $f(x)=x^2$ and $[a,b] \subset [1,+\infty)$, for $b < +\infty$ sufficiently large and show that the theorem is incorrect? Doesn't it seem to be an insufficient statement? I'm having trouble picturing it, is all. Hints are fine.","I am reading (as a supplement) the book Basic Real Analysis, by Anthony Knapp. Before I proceed into reading a proof, I want to be sure that the result seems obvious. Yet, I am having trouble seeing through this one. It annoys me too much in order to disregard it: Theorem . A continuous function $f$ from a closed bounded interval $[a, b]$ into $\mathbb{R}$ is uniformly continuous. What gives? Why can't we provide the counterexample $f(x)=x^2$ and $[a,b] \subset [1,+\infty)$, for $b < +\infty$ sufficiently large and show that the theorem is incorrect? Doesn't it seem to be an insufficient statement? I'm having trouble picturing it, is all. Hints are fine.",,"['analysis', 'proof-writing']"
43,Difficulty level of Courant's book,Difficulty level of Courant's book,,"I am currently studying Introduction to Calculus and Analysis by Richard Courant and Fritz John. I would like to compare Courant's book with Apostol's and Spivak's in terms of difficulty of the problems provided. After reading that book, should I go for one of the two above or should I study something else like Rudin? My focus is on being rigorous and also adept at problem solving.","I am currently studying Introduction to Calculus and Analysis by Richard Courant and Fritz John. I would like to compare Courant's book with Apostol's and Spivak's in terms of difficulty of the problems provided. After reading that book, should I go for one of the two above or should I study something else like Rudin? My focus is on being rigorous and also adept at problem solving.",,"['analysis', 'reference-request', 'soft-question']"
44,weak convergence in $L^p$ plus convergence of norm implies strong convergence,weak convergence in  plus convergence of norm implies strong convergence,L^p,"Having trouble with this problem. Any ideas? Let $\Omega$ be a measure space. Let $f_n$ be a sequence in $L^p(\Omega)$ with $1<p<\infty$ and let $f \in L^p(\Omega)$. Suppose that $$f_n \rightharpoonup f \text{ weakly in } \sigma(L^p,L^{p'})$$ and  $$\|f_n\|_p \to \|f\|_p.$$ Prove that $\|f_n-f\|_p \to 0$. Also, can you come up with a counter-example for the $L^1$ case?","Having trouble with this problem. Any ideas? Let $\Omega$ be a measure space. Let $f_n$ be a sequence in $L^p(\Omega)$ with $1<p<\infty$ and let $f \in L^p(\Omega)$. Suppose that $$f_n \rightharpoonup f \text{ weakly in } \sigma(L^p,L^{p'})$$ and  $$\|f_n\|_p \to \|f\|_p.$$ Prove that $\|f_n-f\|_p \to 0$. Also, can you come up with a counter-example for the $L^1$ case?",,"['analysis', 'measure-theory']"
45,Is there a function with this property?,Is there a function with this property?,,Is there a real function over the real numbers with this property $\ \sqrt{|x-y|} \leq |f(x)-f(y)|$ ? My guess is no but can anyone tell me why? This came up as  a question of one of my collegues and i cant give an answer.,Is there a real function over the real numbers with this property $\ \sqrt{|x-y|} \leq |f(x)-f(y)|$ ? My guess is no but can anyone tell me why? This came up as  a question of one of my collegues and i cant give an answer.,,"['analysis', 'functions']"
46,How to prove uniform continuity?,How to prove uniform continuity?,,"I'm starting out university math and I'm struggling with understanding how to prove uniform continuity. I think I understand the concept of finding a $|x-x_0|<\delta$ for $|f(x)-f(x_0)|<\epsilon$ but all the examples I have found so far have been very vague in explaining how they relate to each other. I have figured out that the smallest $\delta$ has something to do with the steepest part of the $f()$ function in such way that if $\delta$ satisfies $\epsilon$ in the steepest climb or descent, it will satisfy it everywhere else too. But the problem I'm facing is that I don't always understand how I'm supposed to figure out the relation between these two variables. I am able to solve an example like $f(x) = 5x+8$ like so: $x \geq 0, x=x_0+\delta, |f(x)-f(x_0)| = |5(x_0+\delta)+8-5x_0-8| = 5|\delta|$ and thus $5|\delta|<\epsilon$ so the solution is $\delta < \frac{\epsilon}{5}$. This seems easy and reasonable. Here is an example that I can't crack: $f:[0,\infty[\rightarrow\mathbb{R}, f(x)=x^2$ So what I did first was define $x_0 \geq 0, x=x_0+\delta$ Then I wrote $|f(x)-f(x_0)|<\epsilon$ where $|f(x)-f(x_0)|$ is $|(x_0+\delta)^2 - x_0^2| = |x_0^2 + 2x_0\delta + \delta^2 - x_0^2| = |2x_0\delta + \delta^2|$ At this point many of the examples on the net are saying that I can break this in two parts, $|2x_0\delta| < \frac{\epsilon}{2}$ and $|\delta^2| < \frac{\epsilon}{2}$ and solve them separately. So I get $\delta < \sqrt{\frac{\epsilon}{2}}$ and $\delta < \frac{\epsilon}{4x_0}$ So what am I supposed to do with these two deltas I got? And why am I supposed to break it in parts? Shouldn't I get a single value for the $\delta$? I understand that because $x^2$ grows at an increasing speed, no $\delta$ can satisfy all $\epsilon$ (and thus it's not uniformly continuous). But I don't know how I'm supposed to get there. Also, if i confine the $f(x)=x^2$ to $f:[0,5]\rightarrow\mathbb{R}$, how can I then show that it's uniformly continuous? Many of the documents i've found by googling ""uniform continuity"" seem to take shortcuts and I get lost. If someone can explain this in a ""layman way"" clearly I would be very grateful!","I'm starting out university math and I'm struggling with understanding how to prove uniform continuity. I think I understand the concept of finding a $|x-x_0|<\delta$ for $|f(x)-f(x_0)|<\epsilon$ but all the examples I have found so far have been very vague in explaining how they relate to each other. I have figured out that the smallest $\delta$ has something to do with the steepest part of the $f()$ function in such way that if $\delta$ satisfies $\epsilon$ in the steepest climb or descent, it will satisfy it everywhere else too. But the problem I'm facing is that I don't always understand how I'm supposed to figure out the relation between these two variables. I am able to solve an example like $f(x) = 5x+8$ like so: $x \geq 0, x=x_0+\delta, |f(x)-f(x_0)| = |5(x_0+\delta)+8-5x_0-8| = 5|\delta|$ and thus $5|\delta|<\epsilon$ so the solution is $\delta < \frac{\epsilon}{5}$. This seems easy and reasonable. Here is an example that I can't crack: $f:[0,\infty[\rightarrow\mathbb{R}, f(x)=x^2$ So what I did first was define $x_0 \geq 0, x=x_0+\delta$ Then I wrote $|f(x)-f(x_0)|<\epsilon$ where $|f(x)-f(x_0)|$ is $|(x_0+\delta)^2 - x_0^2| = |x_0^2 + 2x_0\delta + \delta^2 - x_0^2| = |2x_0\delta + \delta^2|$ At this point many of the examples on the net are saying that I can break this in two parts, $|2x_0\delta| < \frac{\epsilon}{2}$ and $|\delta^2| < \frac{\epsilon}{2}$ and solve them separately. So I get $\delta < \sqrt{\frac{\epsilon}{2}}$ and $\delta < \frac{\epsilon}{4x_0}$ So what am I supposed to do with these two deltas I got? And why am I supposed to break it in parts? Shouldn't I get a single value for the $\delta$? I understand that because $x^2$ grows at an increasing speed, no $\delta$ can satisfy all $\epsilon$ (and thus it's not uniformly continuous). But I don't know how I'm supposed to get there. Also, if i confine the $f(x)=x^2$ to $f:[0,5]\rightarrow\mathbb{R}$, how can I then show that it's uniformly continuous? Many of the documents i've found by googling ""uniform continuity"" seem to take shortcuts and I get lost. If someone can explain this in a ""layman way"" clearly I would be very grateful!",,['analysis']
47,Prove that a positive polynomial function can be written as the squares of two polynomial functions,Prove that a positive polynomial function can be written as the squares of two polynomial functions,,"Let $f(x)$ be a polynomial function with real coefficients such that $f(x)\geq 0 \;\forall x\in\Bbb R$. Prove that there exist polynomials $A(x),B(x)$ with real coeficients such that $f(x)=A^2(x)+B^2(x)\;\forall x\in\Bbb R$ I don't know how to approach this, apart from some cases of specific polynomials that turned out really ugly. Any hints to point me to the right direction?","Let $f(x)$ be a polynomial function with real coefficients such that $f(x)\geq 0 \;\forall x\in\Bbb R$. Prove that there exist polynomials $A(x),B(x)$ with real coeficients such that $f(x)=A^2(x)+B^2(x)\;\forall x\in\Bbb R$ I don't know how to approach this, apart from some cases of specific polynomials that turned out really ugly. Any hints to point me to the right direction?",,"['analysis', 'polynomials']"
48,Compute the inverse Laplace transform of $e^{-\sqrt{z}}$,Compute the inverse Laplace transform of,e^{-\sqrt{z}},"I want to compute the inverse Laplace transform of a function $$    F(z) = e^{-\sqrt{z}}. $$ This problem seems very nontrivial to me. Here one can find the answer: the inverse Laplace transform of one variable function $e^{-\sqrt{z}}$ is equal to $$    \mathcal{L}^{-1}[e^{-\sqrt{z}}](x) = \frac{1}{2 \sqrt{\pi}} x^{-\frac{3}{2}} \exp \left( -\frac{1}{4x} \right). $$ But what is the simpliest way to do it? Post's formula requires knowledge of all degree derivatives of $e^{-\sqrt{z}}$ and I think that it isn't a good way. The classical inversion formula is of the form $$     \mathcal{L}^{-1}[F(z)](x) = \frac{1}{2 \pi i}\int\limits_{\sigma - i \infty}^{\sigma + i \infty} F(z) e^{zx}\,dz = \frac{1}{2 \pi i} \int\limits_{\sigma - i \infty}^{\sigma + i \infty} e^{-\sqrt{z}+zx} \, dz. $$ To compute it I make a substitution $p = \sqrt{z}$. Then I'm looking for the image of the line $\sigma + i \mathbb{R}$. If I'm not mistaken it is the angle with vertice at $\sqrt{\sigma}$ and with rays $\sqrt{\sigma} + e^{i \frac{\pi}{4}} [0,\infty)$ and $\sqrt{\sigma}+e^{-i\frac{\pi}{4}} [0,\infty)$ (not exactly, these rays are curvilinear, but I think that this doesn't matter because of Cauchy formula). I will denote it $\Lambda$. So  $$    \mathcal{L}^{-1}[e^{-\sqrt{z}}] = \frac{1}{\pi i} \int\limits_{\Lambda} e^{-p + p^2 x}p \, dp. $$ Then I should look for residues, but integrand doesn't have them in finite part of $\mathop{\mathrm{conv}} \Lambda$. Please help me wiith it.","I want to compute the inverse Laplace transform of a function $$    F(z) = e^{-\sqrt{z}}. $$ This problem seems very nontrivial to me. Here one can find the answer: the inverse Laplace transform of one variable function $e^{-\sqrt{z}}$ is equal to $$    \mathcal{L}^{-1}[e^{-\sqrt{z}}](x) = \frac{1}{2 \sqrt{\pi}} x^{-\frac{3}{2}} \exp \left( -\frac{1}{4x} \right). $$ But what is the simpliest way to do it? Post's formula requires knowledge of all degree derivatives of $e^{-\sqrt{z}}$ and I think that it isn't a good way. The classical inversion formula is of the form $$     \mathcal{L}^{-1}[F(z)](x) = \frac{1}{2 \pi i}\int\limits_{\sigma - i \infty}^{\sigma + i \infty} F(z) e^{zx}\,dz = \frac{1}{2 \pi i} \int\limits_{\sigma - i \infty}^{\sigma + i \infty} e^{-\sqrt{z}+zx} \, dz. $$ To compute it I make a substitution $p = \sqrt{z}$. Then I'm looking for the image of the line $\sigma + i \mathbb{R}$. If I'm not mistaken it is the angle with vertice at $\sqrt{\sigma}$ and with rays $\sqrt{\sigma} + e^{i \frac{\pi}{4}} [0,\infty)$ and $\sqrt{\sigma}+e^{-i\frac{\pi}{4}} [0,\infty)$ (not exactly, these rays are curvilinear, but I think that this doesn't matter because of Cauchy formula). I will denote it $\Lambda$. So  $$    \mathcal{L}^{-1}[e^{-\sqrt{z}}] = \frac{1}{\pi i} \int\limits_{\Lambda} e^{-p + p^2 x}p \, dp. $$ Then I should look for residues, but integrand doesn't have them in finite part of $\mathop{\mathrm{conv}} \Lambda$. Please help me wiith it.",,"['analysis', 'laplace-transform', 'residue-calculus']"
49,Is it possible to assign a value to the sum of primes?,Is it possible to assign a value to the sum of primes?,,"It is possible , by means of zeta function regularization and the Ramanujan summation method, to assign a finite value to the sum of the natural numbers (here $n \to \infty $ ) : $$ 1 + 2 + 3 + 4 + \cdots + n \; {“ \;=\; ”} - \frac{1}{12} . $$ Is it also possible to assign a value to the sum of primes, $$ 2 + 3 + 5 + 7 + 11 + \cdots + p_{n}  $$ ( $n \to \infty$ ) by using any summation method for divergent series? This question is inspired by a question on quora . Thanks in advance,","It is possible , by means of zeta function regularization and the Ramanujan summation method, to assign a finite value to the sum of the natural numbers (here ) : Is it also possible to assign a value to the sum of primes, ( ) by using any summation method for divergent series? This question is inspired by a question on quora . Thanks in advance,",n \to \infty   1 + 2 + 3 + 4 + \cdots + n \; {“ \;=\; ”} - \frac{1}{12} .   2 + 3 + 5 + 7 + 11 + \cdots + p_{n}   n \to \infty,"['analysis', 'prime-numbers', 'divergent-series']"
50,Is a Cauchy sequence - preserving (continuous) function is (uniformly) continuous?,Is a Cauchy sequence - preserving (continuous) function is (uniformly) continuous?,,"Let $(X,d)$ and $(Y,\rho)$ be metric spaces and $f:X\to Y$ be a function and suppose for any Cauchy sequence $(a_n)$ in $X$, $(f(a_n))$ is a Cauchy sequence in $Y$. Is $f$ continuous? Let $f$ be continuous, is it uniformly continuous?","Let $(X,d)$ and $(Y,\rho)$ be metric spaces and $f:X\to Y$ be a function and suppose for any Cauchy sequence $(a_n)$ in $X$, $(f(a_n))$ is a Cauchy sequence in $Y$. Is $f$ continuous? Let $f$ be continuous, is it uniformly continuous?",,"['analysis', 'metric-spaces']"
51,Formula for $\zeta(3)$ -verification,Formula for  -verification,\zeta(3),"By simple manipulating with some series, I have found the following formula for $\zeta(3)$ : $$\zeta(3)=\frac27\sum_{k=0}^{\infty}(-1)^kB_{2k}\frac{\pi^{2k+2}}{(2k+2)!},$$ where $b_k$ are Bernoulli numbers, defined from the equations: $$ B_0=1,\quad B_k=-\frac{1}{k+1}\sum_{i=0}^{k-1}\binom{k+1}{i} B_i,\quad k=1,2,3,\dots $$ Questions: Is this formula for $\zeta(3)$ correct? Can somebody check it numerically? If it's correct, is this a well-known identity or not? Thanks for your help. Added. By using my identities another interesting formula follows: $$\ln 2=\sum_{k=0}^{\infty}(-1)^kB_{2k}\frac{\pi^{2k}}{(2k+1)!}$$ $$\zeta(3)=\frac45\sum_{k=0}^{\infty}(-1)^kB_{2k}\frac{\pi^{2k+2}}{(2k+3)!}$$","By simple manipulating with some series, I have found the following formula for : where are Bernoulli numbers, defined from the equations: Questions: Is this formula for correct? Can somebody check it numerically? If it's correct, is this a well-known identity or not? Thanks for your help. Added. By using my identities another interesting formula follows:","\zeta(3) \zeta(3)=\frac27\sum_{k=0}^{\infty}(-1)^kB_{2k}\frac{\pi^{2k+2}}{(2k+2)!}, b_k 
B_0=1,\quad B_k=-\frac{1}{k+1}\sum_{i=0}^{k-1}\binom{k+1}{i} B_i,\quad k=1,2,3,\dots
 \zeta(3) \ln 2=\sum_{k=0}^{\infty}(-1)^kB_{2k}\frac{\pi^{2k}}{(2k+1)!} \zeta(3)=\frac45\sum_{k=0}^{\infty}(-1)^kB_{2k}\frac{\pi^{2k+2}}{(2k+3)!}","['analysis', 'riemann-zeta']"
52,Elliptic Regularity for solutions in distributional sense,Elliptic Regularity for solutions in distributional sense,,"I know that there are a lot of (great) books treating regularity of weak solutions of elliptic pdes (such as Gilbarg-Trudinger), but what about regularity of very weak solutions, that is, solutions in the distributional sense? For concreteness, consider a bounded domain $\Omega \subset \mathbb{R}^2$ with smooth boundary,  and two continuous functions $u, f: \Omega \to \mathbb{R}$ satisfying $-\Delta u = f$ on $\Omega$ in the distributional sense, that is $$  -\int_{\Omega}{u\Delta \phi \mathrm{d}x} = \int_{\Omega}{f \phi\mathrm{d}x}\quad \forall \phi \in \mathscr{C}^{\infty}_{\text{c}}(\Omega). $$ I've read in many articles statements like ""if $u \in L^{\infty}_{\text{loc}}(\Omega)$, then by standard elliptic regularity $u \in \mathscr{C}^{1, \alpha}_{\text{loc}}(\Omega)$"". What do they mean with standard elliptic regularity in this case? Any help would be appreciated, even just a reference.","I know that there are a lot of (great) books treating regularity of weak solutions of elliptic pdes (such as Gilbarg-Trudinger), but what about regularity of very weak solutions, that is, solutions in the distributional sense? For concreteness, consider a bounded domain $\Omega \subset \mathbb{R}^2$ with smooth boundary,  and two continuous functions $u, f: \Omega \to \mathbb{R}$ satisfying $-\Delta u = f$ on $\Omega$ in the distributional sense, that is $$  -\int_{\Omega}{u\Delta \phi \mathrm{d}x} = \int_{\Omega}{f \phi\mathrm{d}x}\quad \forall \phi \in \mathscr{C}^{\infty}_{\text{c}}(\Omega). $$ I've read in many articles statements like ""if $u \in L^{\infty}_{\text{loc}}(\Omega)$, then by standard elliptic regularity $u \in \mathscr{C}^{1, \alpha}_{\text{loc}}(\Omega)$"". What do they mean with standard elliptic regularity in this case? Any help would be appreciated, even just a reference.",,"['analysis', 'partial-differential-equations']"
53,Good PDE books for a graduate student?,Good PDE books for a graduate student?,,"I am now a graduate student in mathematics, and I really want to learn more about PDE. I would say I have a very solid foundation in soft analysis, including functional analysis and harmonic analysis, but I did not receive a good training in hard analysis. So I do not mind if the book requires a lot functional analysis, but I would like one that provides more intuition and motivation rather than just hard techniques and estimates. I know Evan's is the best, and I read most materials from the second part when I was doing a project on theory of distributions. But I cannot say I enjoy his style very much. So can someone introduce a gentler book? Thanks so much!","I am now a graduate student in mathematics, and I really want to learn more about PDE. I would say I have a very solid foundation in soft analysis, including functional analysis and harmonic analysis, but I did not receive a good training in hard analysis. So I do not mind if the book requires a lot functional analysis, but I would like one that provides more intuition and motivation rather than just hard techniques and estimates. I know Evan's is the best, and I read most materials from the second part when I was doing a project on theory of distributions. But I cannot say I enjoy his style very much. So can someone introduce a gentler book? Thanks so much!",,"['analysis', 'reference-request', 'partial-differential-equations']"
54,How to prove there exists $c$ such $f(c)f'(c)+f''(c)=0$,How to prove there exists  such,c f(c)f'(c)+f''(c)=0,"Nice Question: let $f(x)$ have two derivative on $[0,1]$,and such $$f(0)=2,f'(0)=-2,f(1)=1$$   show that: there exist $c\in(0,1)$,such   $$f(c)f'(c)+f''(c)=0$$ my try: since $$f(0)=2,f'(0)=-2,f(1)=1$$ so we easy  $$f(x)=x^2-2x+2$$ such  this condition,But we can't use this function prove  this problem. and other  idea: we can find this ODE $$yy'+y''=0?$$ Thank you for you help！","Nice Question: let $f(x)$ have two derivative on $[0,1]$,and such $$f(0)=2,f'(0)=-2,f(1)=1$$   show that: there exist $c\in(0,1)$,such   $$f(c)f'(c)+f''(c)=0$$ my try: since $$f(0)=2,f'(0)=-2,f(1)=1$$ so we easy  $$f(x)=x^2-2x+2$$ such  this condition,But we can't use this function prove  this problem. and other  idea: we can find this ODE $$yy'+y''=0?$$ Thank you for you help！",,[]
55,Difference between calculus and analysis,Difference between calculus and analysis,,"It's somthing I always want to figure out, when did calculus start to be extended to analysis(I reformulate the question, the previous one""where one can draw a line to distinguish calculus and analysis, or there does not even exist such a line."" was quite misleading). As mentioned a lot in comments, analysis is a much border field than calculus, but the root could be traced back to the calculus in 19th century. Besides, indeed infinitesimal calculus was proved in non-standard analysis, but it was invented until 1960s I think. And I don't know if it can replace all arguments in the theories developed after $\varepsilon-\delta$-definition and before the invention of non-standard analysis. I will explain what I understand, please point out my mistakes. The early stage (Newton and Leibniz) They used infinitesimal, say $\mathrm{d}(\cdot)$ to describe change such as $\mathrm{d}x$ and $\mathrm{d}y$. And use $$\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{y(x+\mathrm{d}x)-y(x)}{\mathrm{d}x}$$ to compute derivatives. And let $y'$ be a shorthand notation for $\frac{\mathrm{d}y}{\mathrm{d}x}$, they defined integral as sum over infinitesimals $$\int y' \mathrm{d}x. $$ (I do not know how Newton and Leibniz defined integral. Maybe as $\approx\sum y(x_i)\Delta x$?) 19th century People started worrying about the precision of infinitesimals. And the ratio of infinitesimals was replaced by limit (the '$\varepsilon-\delta$' definition). In shorhand notation  $$\frac{\mathrm{d}y}{\mathrm{d}x}:=\lim_{t\rightarrow 0}\frac{y(x+t)-y(x)}{t}.$$  $$$$ While the notation was inherited, it did no longer hold the original meaning. And Riemann established his formalization of integration. Based on these, people started to work on functions defined in real number system (real analysis). And in the meantime, the properties of real number were intensively explored (set theory, continuum, etc.). Later the concept of limit was further extended to more general spaces, such as metric spaces(generalized distance), normed spaces(generalized length). So many branches of analysis such as measure theory(it's a part of real analysis. I put it here simply because I feel it is so important.), functional analysis, differential equations emerged. Hence, roughly speaking, changing from infinitesimal approach to limit approach can be considered as the line separating calculus and analysis. Interestingly, in modern calculus textbooks, they in fact loosely use analysis approach while they remain name themself as Calculus. Is this because they do not discuss real number system, which is the very base for the rest. And they only loosely argue 'taking limit by $\Delta x\rightarrow 0$'? I really get confused here. Updates: Can I state that calculus is a study on real-valued functions with $\mathbb{R}^d$-valued argument? So one can loosely conclude that $$\text{infinitesimal and integral calculus} \subsetneq \text{real analysis}\subsetneq \text{analysis}.$$ Update again: The question is much clear now. If calculus is understood as art of calculation, there is no more confusions. Thanks for all dedications on this topic! Since most of answers pointed out the linchpin for the question, I hope it won't cause any misunderstanding if I do not accept any of them. At the end, I hope this post will help others in future. Cheers.","It's somthing I always want to figure out, when did calculus start to be extended to analysis(I reformulate the question, the previous one""where one can draw a line to distinguish calculus and analysis, or there does not even exist such a line."" was quite misleading). As mentioned a lot in comments, analysis is a much border field than calculus, but the root could be traced back to the calculus in 19th century. Besides, indeed infinitesimal calculus was proved in non-standard analysis, but it was invented until 1960s I think. And I don't know if it can replace all arguments in the theories developed after $\varepsilon-\delta$-definition and before the invention of non-standard analysis. I will explain what I understand, please point out my mistakes. The early stage (Newton and Leibniz) They used infinitesimal, say $\mathrm{d}(\cdot)$ to describe change such as $\mathrm{d}x$ and $\mathrm{d}y$. And use $$\frac{\mathrm{d}y}{\mathrm{d}x}=\frac{y(x+\mathrm{d}x)-y(x)}{\mathrm{d}x}$$ to compute derivatives. And let $y'$ be a shorthand notation for $\frac{\mathrm{d}y}{\mathrm{d}x}$, they defined integral as sum over infinitesimals $$\int y' \mathrm{d}x. $$ (I do not know how Newton and Leibniz defined integral. Maybe as $\approx\sum y(x_i)\Delta x$?) 19th century People started worrying about the precision of infinitesimals. And the ratio of infinitesimals was replaced by limit (the '$\varepsilon-\delta$' definition). In shorhand notation  $$\frac{\mathrm{d}y}{\mathrm{d}x}:=\lim_{t\rightarrow 0}\frac{y(x+t)-y(x)}{t}.$$  $$$$ While the notation was inherited, it did no longer hold the original meaning. And Riemann established his formalization of integration. Based on these, people started to work on functions defined in real number system (real analysis). And in the meantime, the properties of real number were intensively explored (set theory, continuum, etc.). Later the concept of limit was further extended to more general spaces, such as metric spaces(generalized distance), normed spaces(generalized length). So many branches of analysis such as measure theory(it's a part of real analysis. I put it here simply because I feel it is so important.), functional analysis, differential equations emerged. Hence, roughly speaking, changing from infinitesimal approach to limit approach can be considered as the line separating calculus and analysis. Interestingly, in modern calculus textbooks, they in fact loosely use analysis approach while they remain name themself as Calculus. Is this because they do not discuss real number system, which is the very base for the rest. And they only loosely argue 'taking limit by $\Delta x\rightarrow 0$'? I really get confused here. Updates: Can I state that calculus is a study on real-valued functions with $\mathbb{R}^d$-valued argument? So one can loosely conclude that $$\text{infinitesimal and integral calculus} \subsetneq \text{real analysis}\subsetneq \text{analysis}.$$ Update again: The question is much clear now. If calculus is understood as art of calculation, there is no more confusions. Thanks for all dedications on this topic! Since most of answers pointed out the linchpin for the question, I hope it won't cause any misunderstanding if I do not accept any of them. At the end, I hope this post will help others in future. Cheers.",,"['analysis', 'math-history']"
56,Does there exist a bijective $f:\mathbb{N} \to \mathbb{N}$ such that $\sum f(n)/n^2$ converges?,Does there exist a bijective  such that  converges?,f:\mathbb{N} \to \mathbb{N} \sum f(n)/n^2,"We know that $\displaystyle\zeta(2)=\sum\limits_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$ and it converges. Does there exists a bijective map $f:\mathbb{N} \to \mathbb{N}$ such that the sum $$\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^2}$$ converges. If our $s=2$ was not fixed, then can we have a function such that $\displaystyle \zeta(s)=\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^s}$ converges","We know that $\displaystyle\zeta(2)=\sum\limits_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$ and it converges. Does there exists a bijective map $f:\mathbb{N} \to \mathbb{N}$ such that the sum $$\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^2}$$ converges. If our $s=2$ was not fixed, then can we have a function such that $\displaystyle \zeta(s)=\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^s}$ converges",,['number-theory']
57,"What's the minimum of $\int_0^1 f(x)^2 \: dx$, subject to $\int_0^1 f(x) \: dx = 0, \int_0^1 x f(x) \: dx = 1$?","What's the minimum of , subject to ?","\int_0^1 f(x)^2 \: dx \int_0^1 f(x) \: dx = 0, \int_0^1 x f(x) \: dx = 1","The question is as in the title: what's the minimum of $\int_0^1 f(x)^2 \: dx$, subject to $\int_0^1 f(x) \: dx = 0, \int_0^1 x f(x) \: dx = 1$? (Assume suitable smoothness conditions.) A problem in the textbook for the course I am TEACHING (not taking) reduces to minimizing $w_1^2 + w_2^2 + w_3^2$ subject to $w_1 + w_2 + w_3 = 0, w_1 + 2w_2 + 3w_3 = 1$. Of course there's nothing special about the number $3$ here, and so one can ask for the minimum of $\sum_{i=1}^n w_i^2$ subject to $\sum_{i=1}^n w_i = 0, \sum_{i=1}^n iw_i = 1$. At least when $n = 3, 4, 5$, we get $w_i = c_n(i-(n+1)/2)$, for some constant $c_n$ which depends on $n$. So for fixed $n$, $w_i$ is a linear function of $i$. (This is a bit of an annoying computation, so I won't reproduce it here.) So it seems like there should be a continuous analogue of this. If we have  $$ \int_0^1 f(x) \: dx = 0, \int_0^1 x f(x) \: dx = 1 $$ and $f(x)$ is linear, then we get $f(x) = 12(x-1/2)$, and $\int_0^1 f(x)^2 \: dx = 12$.  Is this the function satisfying these integral conditions with smallest $\int_0^1 f(x)^2 \: dx$? That is, is it the case that $$ \int_0^1 f(x)^2 \: dx \ge 12 $$ for every $f(x)$ satisfying the two conditions above and whatever smoothness conditions are necessary? I've tagged this calculus-of-variations because that's what it looks like to me. But I don't know the calculus of variations, which is why I can't just solve the problem myself.","The question is as in the title: what's the minimum of $\int_0^1 f(x)^2 \: dx$, subject to $\int_0^1 f(x) \: dx = 0, \int_0^1 x f(x) \: dx = 1$? (Assume suitable smoothness conditions.) A problem in the textbook for the course I am TEACHING (not taking) reduces to minimizing $w_1^2 + w_2^2 + w_3^2$ subject to $w_1 + w_2 + w_3 = 0, w_1 + 2w_2 + 3w_3 = 1$. Of course there's nothing special about the number $3$ here, and so one can ask for the minimum of $\sum_{i=1}^n w_i^2$ subject to $\sum_{i=1}^n w_i = 0, \sum_{i=1}^n iw_i = 1$. At least when $n = 3, 4, 5$, we get $w_i = c_n(i-(n+1)/2)$, for some constant $c_n$ which depends on $n$. So for fixed $n$, $w_i$ is a linear function of $i$. (This is a bit of an annoying computation, so I won't reproduce it here.) So it seems like there should be a continuous analogue of this. If we have  $$ \int_0^1 f(x) \: dx = 0, \int_0^1 x f(x) \: dx = 1 $$ and $f(x)$ is linear, then we get $f(x) = 12(x-1/2)$, and $\int_0^1 f(x)^2 \: dx = 12$.  Is this the function satisfying these integral conditions with smallest $\int_0^1 f(x)^2 \: dx$? That is, is it the case that $$ \int_0^1 f(x)^2 \: dx \ge 12 $$ for every $f(x)$ satisfying the two conditions above and whatever smoothness conditions are necessary? I've tagged this calculus-of-variations because that's what it looks like to me. But I don't know the calculus of variations, which is why I can't just solve the problem myself.",,"['analysis', 'optimization', 'calculus-of-variations']"
58,Are angles ever multiplied?,Are angles ever multiplied?,,"I recently explained multiplication of (non-zero) complex numbers to my Mathematics Fundamentals students, the usual bit about “multiply their lengths, and add their angles”. Of course, there is always at least one student who wants to know why the angles are added instead of multiplied – after all, this is multiplication, right? – and so I go through a non-trivial example to establish that it is at least highly plausible that addition, not multiplication, of angles is what occurs. This quiets their objection, but it always gets me to wondering: Indeed, are angles EVER multiplied anywhere in Mathematics? I don’t recall ever having seen such a thing. And since this wonderful wonderland of MSE now exists, I will pass this question on to the community. Of course, there are exercises of the form “If cos(f(x)) = sin(g(x)), solve for x.”, where this could happen, but of course what I am asking is whether this happens as part of a significant theorem, or as part of solving a problem of real physical interest. The non-multiplication of angles seems all the more counter-intuitive because they are actually dimensionless, in spite of the use of dimension-sounding phraseology such as “degrees” and “radians”. So, angles are just numbers, and surely numbers can be multiplied together, right? (Of course, not being dimensionless is no barrier to getting multiplied by objects of the same/different type. After all, we deal with the square of seconds in regard to acceleration, and with the square of grams in regard to the (statistical) variance of weights, and so on. I’m just saying that it would seem all the more plausible that they would occasionally get multiplied together if they are dimensionless.) I’m going to go out on a limb and conjecture that there is no such case. The basis of my conjecture is simply that I have never seen it happen, and also the fact that angles do not exist for vectors of zero length. I know this is quite tenuous, but what I’m guessing is that only quantities that behave well for 0 are eligible, so to speak, to be multiplied together.","I recently explained multiplication of (non-zero) complex numbers to my Mathematics Fundamentals students, the usual bit about “multiply their lengths, and add their angles”. Of course, there is always at least one student who wants to know why the angles are added instead of multiplied – after all, this is multiplication, right? – and so I go through a non-trivial example to establish that it is at least highly plausible that addition, not multiplication, of angles is what occurs. This quiets their objection, but it always gets me to wondering: Indeed, are angles EVER multiplied anywhere in Mathematics? I don’t recall ever having seen such a thing. And since this wonderful wonderland of MSE now exists, I will pass this question on to the community. Of course, there are exercises of the form “If cos(f(x)) = sin(g(x)), solve for x.”, where this could happen, but of course what I am asking is whether this happens as part of a significant theorem, or as part of solving a problem of real physical interest. The non-multiplication of angles seems all the more counter-intuitive because they are actually dimensionless, in spite of the use of dimension-sounding phraseology such as “degrees” and “radians”. So, angles are just numbers, and surely numbers can be multiplied together, right? (Of course, not being dimensionless is no barrier to getting multiplied by objects of the same/different type. After all, we deal with the square of seconds in regard to acceleration, and with the square of grams in regard to the (statistical) variance of weights, and so on. I’m just saying that it would seem all the more plausible that they would occasionally get multiplied together if they are dimensionless.) I’m going to go out on a limb and conjecture that there is no such case. The basis of my conjecture is simply that I have never seen it happen, and also the fact that angles do not exist for vectors of zero length. I know this is quite tenuous, but what I’m guessing is that only quantities that behave well for 0 are eligible, so to speak, to be multiplied together.",,['analysis']
59,doubly periodic functions as tessellations (other than parallelograms),doubly periodic functions as tessellations (other than parallelograms),,I think of a snapshot of a single period of a doubly periodic function as one parallelogram-shaped tile in a tessellation. Could a function have a period that repeats like a honeycomb or some other not rectangular tessellation?,I think of a snapshot of a single period of a doubly periodic function as one parallelogram-shaped tile in a tessellation. Could a function have a period that repeats like a honeycomb or some other not rectangular tessellation?,,"['analysis', 'special-functions', 'tessellations', 'elliptic-functions']"
60,"Space of probability measures ""complete""? (In the other sense)","Space of probability measures ""complete""? (In the other sense)",,"I want to consider a space of probability measures on some set $X$, such that the space of measures is complete , not in the sense of complete probability measures (though probably that too), but as in the standard analysis meaning. I haven't studied far enough in probability measures, so I can't tell -- what conditions do I need on my space such that it is complete? And how should I metrize it? Thanks! Edit: A reference/citation to a text would also be super helpful!","I want to consider a space of probability measures on some set $X$, such that the space of measures is complete , not in the sense of complete probability measures (though probably that too), but as in the standard analysis meaning. I haven't studied far enough in probability measures, so I can't tell -- what conditions do I need on my space such that it is complete? And how should I metrize it? Thanks! Edit: A reference/citation to a text would also be super helpful!",,"['analysis', 'measure-theory']"
61,Why is boundary information so significant? -- Stokes's theorem,Why is boundary information so significant? -- Stokes's theorem,,"Why is it that there are so many instances in analysis, both real and complex, in which the values of a function on the interior of some domain are completely determined by the values which it takes on the boundary? I know that this has something to do with the general version of Stokes's theorem, but I'm not advanced enough to understand this yet -- does anyone have a (semi) intuitive explanation for this kind of phenomenon?","Why is it that there are so many instances in analysis, both real and complex, in which the values of a function on the interior of some domain are completely determined by the values which it takes on the boundary? I know that this has something to do with the general version of Stokes's theorem, but I'm not advanced enough to understand this yet -- does anyone have a (semi) intuitive explanation for this kind of phenomenon?",,"['analysis', 'multivariable-calculus', 'soft-question']"
62,Are Fourier Analysis and Harmonic Analysis the same subject?,Are Fourier Analysis and Harmonic Analysis the same subject?,,Are Fourier Analysis and Harmonic Analysis the same subject? I believe that they are not the same. Maybe there is big difference between those subjects but I need to know what is the main difference between those subjects and what is the main intersection? What is the common between those subjects?,Are Fourier Analysis and Harmonic Analysis the same subject? I believe that they are not the same. Maybe there is big difference between those subjects but I need to know what is the main difference between those subjects and what is the main intersection? What is the common between those subjects?,,"['analysis', 'fourier-analysis', 'harmonic-analysis']"
63,Find all functions $f$ such that $f(x)+f(\frac{1}{1-x})=x$,Find all functions  such that,f f(x)+f(\frac{1}{1-x})=x,"I would like to find all functions $f:\mathbb{R}\backslash\{0,1\}\rightarrow\mathbb{R}$ such that $$f(x)+f\left( \frac{1}{1-x}\right)=x.$$ I do not know how to solve the problem. Can someone explain how to solve it? In one of my attempts I did the following, which is confusing to me: By the substitution $y=1-\frac{1}{x}$ one gets $f(y)+f\left( \frac{1}{1-y}\right)=\frac{1}{1-y}$ . So with $x=y$ it follows that $0=x-\frac{1}{1-x}$ . So it would follow that there is no solution. Is that possible or is there a mistake? Best regards","I would like to find all functions such that I do not know how to solve the problem. Can someone explain how to solve it? In one of my attempts I did the following, which is confusing to me: By the substitution one gets . So with it follows that . So it would follow that there is no solution. Is that possible or is there a mistake? Best regards","f:\mathbb{R}\backslash\{0,1\}\rightarrow\mathbb{R} f(x)+f\left( \frac{1}{1-x}\right)=x. y=1-\frac{1}{x} f(y)+f\left( \frac{1}{1-y}\right)=\frac{1}{1-y} x=y 0=x-\frac{1}{1-x}","['analysis', 'functional-equations']"
64,"Let $a_{i} \in\mathbb{R}$ ($i=1,2,\dots,n$), and $f(x)=\sum_{i=0}^{n}a_{i}x^i$ such that if $|x|\leqslant 1$, then $|f(x)|\leqslant 1$. Prove that:","Let  (), and  such that if , then . Prove that:","a_{i} \in\mathbb{R} i=1,2,\dots,n f(x)=\sum_{i=0}^{n}a_{i}x^i |x|\leqslant 1 |f(x)|\leqslant 1","Let $a_{i} \in\mathbb{R}$ ($i=1,2,\dots,n$), and $f(x)=\sum_{i=0}^{n}a_{i}x^i$ such that if $|x|\leqslant 1$, then $|f(x)|\leqslant 1$. Prove that: $|a_{n}|+|a_{n-1} | \leqslant 2^{n-1}$. $|a_{n}|+|a_{n-1}|+\cdots+|a_{1}|+|a_{0}|\leqslant\dfrac{(1+\sqrt{2})^n+(1-\sqrt{2})^n}{2}$.","Let $a_{i} \in\mathbb{R}$ ($i=1,2,\dots,n$), and $f(x)=\sum_{i=0}^{n}a_{i}x^i$ such that if $|x|\leqslant 1$, then $|f(x)|\leqslant 1$. Prove that: $|a_{n}|+|a_{n-1} | \leqslant 2^{n-1}$. $|a_{n}|+|a_{n-1}|+\cdots+|a_{1}|+|a_{0}|\leqslant\dfrac{(1+\sqrt{2})^n+(1-\sqrt{2})^n}{2}$.",,"['analysis', 'polynomials', 'inequality']"
65,Must a monotone function have a monotone derivative?,Must a monotone function have a monotone derivative?,,"If a function is differentiable and monotone on the interval $(a, b)$, then its derivative is also monotone on $(a, b)$. How do you prove this statement is wrong? Can you please provide an example?","If a function is differentiable and monotone on the interval $(a, b)$, then its derivative is also monotone on $(a, b)$. How do you prove this statement is wrong? Can you please provide an example?",,['analysis']
66,Prove that the product of a non-zero rational and irrational number is irrational.,Prove that the product of a non-zero rational and irrational number is irrational.,,"Could you please confirm if this proof is correct? Theorem: If $q \neq 0$ is rational and $y$ is irrational, then $qy$ is irrational. Proof: Proof by contradiction, we assume that $qy$ is rational. Therefore $qy=\frac{a}{b}$ for integers $a$, $b \neq 0$. Since $q$ is rational, we have $\frac{x}{z}y=\frac{a}{b}$ for integers $x \neq 0$, $z \neq 0$. Therefore, $xy = a$, and $y=\frac{a}{x}$. Since both $a$ and $x$ are integers, $y$ is rational, leading to a contradiction.","Could you please confirm if this proof is correct? Theorem: If $q \neq 0$ is rational and $y$ is irrational, then $qy$ is irrational. Proof: Proof by contradiction, we assume that $qy$ is rational. Therefore $qy=\frac{a}{b}$ for integers $a$, $b \neq 0$. Since $q$ is rational, we have $\frac{x}{z}y=\frac{a}{b}$ for integers $x \neq 0$, $z \neq 0$. Therefore, $xy = a$, and $y=\frac{a}{x}$. Since both $a$ and $x$ are integers, $y$ is rational, leading to a contradiction.",,"['analysis', 'irrational-numbers']"
67,$f(x)f(\frac{1}{x})=f(x)+f(\frac{1}{x})$,,f(x)f(\frac{1}{x})=f(x)+f(\frac{1}{x}),"Find a function $f(x)$ such that: $$f(x)f(\frac{1}{x})=f(x)+f(\frac{1}{x})$$ with $f(4)=65$ . I have tried to let $f(x)$ be a general polynomial: $$a_0+a_1x+a_2x^2+\ldots a_nx^n$$ which leaves $f(\frac{1}{x})$ as: $$a_0+a_1{1\over x}+a_2{1\over x^2}+\ldots + a_n{1\over x^n}$$ On comparing the coefficients of both sides, we see that: $$2a_0=(a_0)^2+(a_1)^2+(a_2)^2+\ldots+(a_n)^2$$ And $$a_1=(a_0a_1)+(a_1a_2)+ \ldots +(a_{n-1}a_n)$$ I don't know how to proceed further. I know I need to compare coefficients and come to a conclusion based on their values, but I don't see what to do next.","Find a function such that: with . I have tried to let be a general polynomial: which leaves as: On comparing the coefficients of both sides, we see that: And I don't know how to proceed further. I know I need to compare coefficients and come to a conclusion based on their values, but I don't see what to do next.",f(x) f(x)f(\frac{1}{x})=f(x)+f(\frac{1}{x}) f(4)=65 f(x) a_0+a_1x+a_2x^2+\ldots a_nx^n f(\frac{1}{x}) a_0+a_1{1\over x}+a_2{1\over x^2}+\ldots + a_n{1\over x^n} 2a_0=(a_0)^2+(a_1)^2+(a_2)^2+\ldots+(a_n)^2 a_1=(a_0a_1)+(a_1a_2)+ \ldots +(a_{n-1}a_n),"['analysis', 'functions', 'polynomials', 'functional-equations']"
68,"Is there a number that's right in the middle of this interval $(0, 1)$?",Is there a number that's right in the middle of this interval ?,"(0, 1)","This might seem like a silly question, but is there a number that's right in the middle of this interval $(0, 1)$? And the half-open intervals: $(0, 1]$, $[0, 1)$? I know for a fully closed interval $[0, 1]$ it's $1/2$ because that's half the length of the interval but for $(0 ,1)$, I can't say. What's the interval's length? Certainly not $1$. Edit: In response to the comment below: I said, in my mind, it can't be $1$ because that that length of the interval $[0, 1]$, and that's not the same as $(0, 1)$. I suppose it approaches 1, but that's not the same, or is it? And I think by 'length' I mean the the difference in the end points: $|x_2 - x_1|$. $(0, 1)$ does't include $0$ or $1$. There are no end-points to subtract!","This might seem like a silly question, but is there a number that's right in the middle of this interval $(0, 1)$? And the half-open intervals: $(0, 1]$, $[0, 1)$? I know for a fully closed interval $[0, 1]$ it's $1/2$ because that's half the length of the interval but for $(0 ,1)$, I can't say. What's the interval's length? Certainly not $1$. Edit: In response to the comment below: I said, in my mind, it can't be $1$ because that that length of the interval $[0, 1]$, and that's not the same as $(0, 1)$. I suppose it approaches 1, but that's not the same, or is it? And I think by 'length' I mean the the difference in the end points: $|x_2 - x_1|$. $(0, 1)$ does't include $0$ or $1$. There are no end-points to subtract!",,['analysis']
69,Motivation for triangle inequality,Motivation for triangle inequality,,"Triangle inequality is used in one context or the other in analysis. To list a few $$ \|x+y\| \leq \|x\| + \|y\| $$ $$ d(x,y) \leq d(x,z) + d(z,y) $$ $$ \mu(A \cup B) \leq \mu(A) + \mu(B) $$ What is the motivation for imposing this? Intutively, it feels if we do not impose triangle inequality, the space will collapse to a trivial space in some sense. I am not able to express this formally. I would appreciate if someone could could throw some light on this and the motivation for triangle inequality?","Triangle inequality is used in one context or the other in analysis. To list a few $$ \|x+y\| \leq \|x\| + \|y\| $$ $$ d(x,y) \leq d(x,z) + d(z,y) $$ $$ \mu(A \cup B) \leq \mu(A) + \mu(B) $$ What is the motivation for imposing this? Intutively, it feels if we do not impose triangle inequality, the space will collapse to a trivial space in some sense. I am not able to express this formally. I would appreciate if someone could could throw some light on this and the motivation for triangle inequality?",,['analysis']
70,Which metric spaces are totally bounded?,Which metric spaces are totally bounded?,,"A subset $S$ of a metric space $X$ is totally bounded if for any $r>0$, $S$ can be covered by a finite number of $X$-balls of radius $r$. A metric space $X$ is totally bounded if it is a totally bounded subset of itself. For example, bounded subsets of $\mathbb{R}^n$ are totally bounded. Are there any interesting necessary and/or sufficient conditions for a metric space or its subsets to be totally bounded? [Background: I was trying to generalize problem 4.8 of baby Rudin which asks you to prove that a real uniformly continuous function on a bounded subset $E$ of the real line is bounded. It seems after a little googling that a more general true statement would require $E$ to be a totally bounded subset of some metric space. But where might we meet such subsets?]","A subset $S$ of a metric space $X$ is totally bounded if for any $r>0$, $S$ can be covered by a finite number of $X$-balls of radius $r$. A metric space $X$ is totally bounded if it is a totally bounded subset of itself. For example, bounded subsets of $\mathbb{R}^n$ are totally bounded. Are there any interesting necessary and/or sufficient conditions for a metric space or its subsets to be totally bounded? [Background: I was trying to generalize problem 4.8 of baby Rudin which asks you to prove that a real uniformly continuous function on a bounded subset $E$ of the real line is bounded. It seems after a little googling that a more general true statement would require $E$ to be a totally bounded subset of some metric space. But where might we meet such subsets?]",,"['analysis', 'metric-spaces']"
71,What are all measurable maps $f:\mathbb C\to\mathbb C$ such that $f(ab)=f(a)f(b)$?,What are all measurable maps  such that ?,f:\mathbb C\to\mathbb C f(ab)=f(a)f(b),"Is there a nice description of all nonzero measurable functions $f:\mathbb{C}\to\mathbb{C}$ such that $f(ab)=f(a)f(b)\ $ for all $a$ and $b$ in $\mathbb{C}$? This is inspired by the question Multiplicative Analytic Functions , Theo Buehler's comments there, and idle curiosity.   The only examples I know have the form $f(z)=|z|^cz^k$ or $f(z)=|z|^c\overline z^k$ for some $c\in[0,\infty)$ and nonnegative integer $k$.  A subquestion is: Are these all of them? (Answer: No, see update.) I tried searching, and found that nonzero multiplicative maps from a semigroup to the multiplicative semigroup of complex numbers are sometimes called semicharacters, and other times called characters, but so far this hasn't helped me find anything that answers this question. Update : I found from reading a remark in Grillet's Commutative semigroups that if $f$ is an example, then so is the function $g$ defined by $g(0)=0$, $g(z)=f(z)/|f(z)|$ if $z\neq 0$, and $g$ maps into the unit circle unioned with $\{0\}$.  This then made me realize that if $f$ and $g$ are examples, then so is the function $h$ defined by $h(0)=0$, $h(z)=f(z)/g(z)$ if $z\neq 0$.  So there are examples I did not include above.  I would have to include $f(0)=0$, $f(z)=|z|^cz^k$ where $c$ is any real number and $k$ any integer. Also, I'm not even sure about what all of the continuous examples are.","Is there a nice description of all nonzero measurable functions $f:\mathbb{C}\to\mathbb{C}$ such that $f(ab)=f(a)f(b)\ $ for all $a$ and $b$ in $\mathbb{C}$? This is inspired by the question Multiplicative Analytic Functions , Theo Buehler's comments there, and idle curiosity.   The only examples I know have the form $f(z)=|z|^cz^k$ or $f(z)=|z|^c\overline z^k$ for some $c\in[0,\infty)$ and nonnegative integer $k$.  A subquestion is: Are these all of them? (Answer: No, see update.) I tried searching, and found that nonzero multiplicative maps from a semigroup to the multiplicative semigroup of complex numbers are sometimes called semicharacters, and other times called characters, but so far this hasn't helped me find anything that answers this question. Update : I found from reading a remark in Grillet's Commutative semigroups that if $f$ is an example, then so is the function $g$ defined by $g(0)=0$, $g(z)=f(z)/|f(z)|$ if $z\neq 0$, and $g$ maps into the unit circle unioned with $\{0\}$.  This then made me realize that if $f$ and $g$ are examples, then so is the function $h$ defined by $h(0)=0$, $h(z)=f(z)/g(z)$ if $z\neq 0$.  So there are examples I did not include above.  I would have to include $f(0)=0$, $f(z)=|z|^cz^k$ where $c$ is any real number and $k$ any integer. Also, I'm not even sure about what all of the continuous examples are.",,['analysis']
72,Is a continuous function with continuous weak derivatives of class $C^1$?,Is a continuous function with continuous weak derivatives of class ?,C^1,Let $f$ be a real valued continuous function of many variables whose weak derivatives of first order are continuous. Is this function equals a.e. function of class $C^1$ ?,Let $f$ be a real valued continuous function of many variables whose weak derivatives of first order are continuous. Is this function equals a.e. function of class $C^1$ ?,,"['analysis', 'weak-derivatives']"
73,Swapping signs in analysis proofs,Swapping signs in analysis proofs,,"Under what minimal conditions are the following interchange of operations valid (including a question of existence, if not given explicitly)? \begin{align*} \lim \int f_n&=\int \lim f_n \\ \lim_{x\to a} \lim_{y \to b} f(x,y)&=\lim_{y\to b}\lim_{x \to a} f(x,y) \\ \frac{d}{dx}\lim f_n&=\lim \frac{d}{dx}f_n \\ \lim_{x \to c} \int f(x,y)dy&=\int \lim_{x \to c} f(x,y)dy \\ \frac{d}{dx} \int f(x,t) dt &= \int \frac{d}{dx} f(x,t)dt \end{align*} This is probably my weakest area in analysis. The only loose idea I have in my head for a general program surrounds uniform convergence, but I wouldn't even know how to apply that notion to, e.g. the fourth question. I have books, like Royden, Rudin, etc., that provide a laundry list of conditions, but I generally need visualization and intuitive rationale for theorems - and I just don't see it here. I once had the advice to always think of these in basic terms, like sequences and series, but even then, it's hard to picture what's going on geometrically. I have similar issues with $l_p$ vs $L_p$ spaces - norms for $l_p$ are easy to visualize, but norms for $L_p$ have no geometric meaning for me. Thanks for helping me get over this hump - I know this is an important area for understanding analysis deeply. I cannot remember the tag for non-specific questions and would appreciate anyone adding that tag if she or he knows.","Under what minimal conditions are the following interchange of operations valid (including a question of existence, if not given explicitly)? \begin{align*} \lim \int f_n&=\int \lim f_n \\ \lim_{x\to a} \lim_{y \to b} f(x,y)&=\lim_{y\to b}\lim_{x \to a} f(x,y) \\ \frac{d}{dx}\lim f_n&=\lim \frac{d}{dx}f_n \\ \lim_{x \to c} \int f(x,y)dy&=\int \lim_{x \to c} f(x,y)dy \\ \frac{d}{dx} \int f(x,t) dt &= \int \frac{d}{dx} f(x,t)dt \end{align*} This is probably my weakest area in analysis. The only loose idea I have in my head for a general program surrounds uniform convergence, but I wouldn't even know how to apply that notion to, e.g. the fourth question. I have books, like Royden, Rudin, etc., that provide a laundry list of conditions, but I generally need visualization and intuitive rationale for theorems - and I just don't see it here. I once had the advice to always think of these in basic terms, like sequences and series, but even then, it's hard to picture what's going on geometrically. I have similar issues with $l_p$ vs $L_p$ spaces - norms for $l_p$ are easy to visualize, but norms for $L_p$ have no geometric meaning for me. Thanks for helping me get over this hump - I know this is an important area for understanding analysis deeply. I cannot remember the tag for non-specific questions and would appreciate anyone adding that tag if she or he knows.",,['analysis']
74,Errata for Dieudonné's Treatise on Analysis volume 2 second edition,Errata for Dieudonné's Treatise on Analysis volume 2 second edition,,"I was looking again at the beautiful and quite complete work of Dieudonné, his Treatise of Analysis, to refresh my memory about some aspects of classical analysis. I especially love this Treatise for the quality of its exercises. Unfortunately, some of them are marred with plain mistakes (see for instance this question Separability of the set of positive measures ) or wrong hints, and make me waste a lot of time detecting all of them. Does anybody know a reliable errata for the volume 2 of this treatise ? Here below are one instance of a plain wrong hint in an exercise, and a sketch of a solution I found for it. I would very much appreciate if somebody could give some kind of reassurance that I am on the right track here and that my solution is correct. Here is the text of the exercise: The hint is obviously wrong because it is not possible to have the inequality $b\mu(A\cap F_q)\leq a\mu(A)$ for $q=n$ for instance. Below is a solution I found for comments Notation for $p\geq n\geq 0$ : $$\begin{align*} A_{n}^p &=\left\{x\in X\ ;\ \sup_{p\geq r\geq n}f_{r}(x)\geq b\right\} \\ B_{n}^p &=\left\{x\in X\ ;\ \inf_{p\geq r\geq n}f_{r}(x)\leq a\right\} \\ \end{align*}$$ and $$\begin{alignat*}{2} A_{n} &=\bigcup_{p\geq n} A_{n}^p &\quad B_{n} &=\bigcup_{p\geq n} B_{n}^p \\ A &=\bigcap_{n\geq 0}A_{n} &\quad B &=\bigcap_{n\geq 0}B_{n} \end{alignat*}$$ Then we have $E_{ab}=A\cap B$. We also note that the unions and intersections in the previous definitions are respectively increasing and decreasing sequences. Choose $r\geq s\geq p\geq q\geq m\geq n$. First we notice that $$A_{n}^m = \bigcup_{m\geq i\geq n} \left\{x\in X\ ;\ f_{n}(x)<b, \cdots, f_{i-1}(x)<b,\ f_{i}(x)\geq b\right\}$$ the union being of disjoints sets. By definition of a martingale, for $i\leq m$, we have $$\int_{x\in X\ ;\ f_n(x)<\cdots,f_{i−1}(x)<b,\ f_i(x\geq b} f_i\ d\mu=\int_{x\in X\ ;\ f_n(x)<\cdots,f_{i−1}(x)<b,\ f_i(x\geq b}f_m\ d\mu$$ Then, we get $$\int_{A_n^m}f_{m}d\mu \geq b\mu(A_{n}^m)$$ For the same reasons, we also get $$\int_{B_{n}^m}f_{m}d\mu \leq a\mu(B_{n}^m)$$ Therefore, we deduce that $$\int_{A_{n}^m\cap B_{q}^p \cap A_{s}^r}f_{r} d\mu \geq b\mu(A_{n}^m\cap B_{q}^p \cap A_{s}^r)$$ and $$\int_{A_{n}^m\cap B_{q}^p \cap A_{s}^r}f_{r}d\mu \leq \int_{A_{n}^m\cap B_{q}^p}f_{r}d\mu =\int_{A_{n}^m\cap B_{q}^p}f_{p}d\mu \leq a\mu(A_{n}^m\cap B_{q}^p) \leq a\mu(A_{n}^m\cap B_{q})$$ Let $$b\mu(A_{n}^m\cap B_{q}^p \cap A_{s}^r) \leq a\mu(A_{n}^m\cap B_{q})$$ By successively having $r$ then $s$ then $p$ then $q$ goes towards infinity we get $$b\mu(A_{n}^m\cap B \cap A)\leq a\mu(A_{n}^m\cap B)$$ If $m$ then $n$ goes towards infinity we get $$b\mu(E_{ab})\leq a\mu(E_{ab})$$ QEA.","I was looking again at the beautiful and quite complete work of Dieudonné, his Treatise of Analysis, to refresh my memory about some aspects of classical analysis. I especially love this Treatise for the quality of its exercises. Unfortunately, some of them are marred with plain mistakes (see for instance this question Separability of the set of positive measures ) or wrong hints, and make me waste a lot of time detecting all of them. Does anybody know a reliable errata for the volume 2 of this treatise ? Here below are one instance of a plain wrong hint in an exercise, and a sketch of a solution I found for it. I would very much appreciate if somebody could give some kind of reassurance that I am on the right track here and that my solution is correct. Here is the text of the exercise: The hint is obviously wrong because it is not possible to have the inequality $b\mu(A\cap F_q)\leq a\mu(A)$ for $q=n$ for instance. Below is a solution I found for comments Notation for $p\geq n\geq 0$ : $$\begin{align*} A_{n}^p &=\left\{x\in X\ ;\ \sup_{p\geq r\geq n}f_{r}(x)\geq b\right\} \\ B_{n}^p &=\left\{x\in X\ ;\ \inf_{p\geq r\geq n}f_{r}(x)\leq a\right\} \\ \end{align*}$$ and $$\begin{alignat*}{2} A_{n} &=\bigcup_{p\geq n} A_{n}^p &\quad B_{n} &=\bigcup_{p\geq n} B_{n}^p \\ A &=\bigcap_{n\geq 0}A_{n} &\quad B &=\bigcap_{n\geq 0}B_{n} \end{alignat*}$$ Then we have $E_{ab}=A\cap B$. We also note that the unions and intersections in the previous definitions are respectively increasing and decreasing sequences. Choose $r\geq s\geq p\geq q\geq m\geq n$. First we notice that $$A_{n}^m = \bigcup_{m\geq i\geq n} \left\{x\in X\ ;\ f_{n}(x)<b, \cdots, f_{i-1}(x)<b,\ f_{i}(x)\geq b\right\}$$ the union being of disjoints sets. By definition of a martingale, for $i\leq m$, we have $$\int_{x\in X\ ;\ f_n(x)<\cdots,f_{i−1}(x)<b,\ f_i(x\geq b} f_i\ d\mu=\int_{x\in X\ ;\ f_n(x)<\cdots,f_{i−1}(x)<b,\ f_i(x\geq b}f_m\ d\mu$$ Then, we get $$\int_{A_n^m}f_{m}d\mu \geq b\mu(A_{n}^m)$$ For the same reasons, we also get $$\int_{B_{n}^m}f_{m}d\mu \leq a\mu(B_{n}^m)$$ Therefore, we deduce that $$\int_{A_{n}^m\cap B_{q}^p \cap A_{s}^r}f_{r} d\mu \geq b\mu(A_{n}^m\cap B_{q}^p \cap A_{s}^r)$$ and $$\int_{A_{n}^m\cap B_{q}^p \cap A_{s}^r}f_{r}d\mu \leq \int_{A_{n}^m\cap B_{q}^p}f_{r}d\mu =\int_{A_{n}^m\cap B_{q}^p}f_{p}d\mu \leq a\mu(A_{n}^m\cap B_{q}^p) \leq a\mu(A_{n}^m\cap B_{q})$$ Let $$b\mu(A_{n}^m\cap B_{q}^p \cap A_{s}^r) \leq a\mu(A_{n}^m\cap B_{q})$$ By successively having $r$ then $s$ then $p$ then $q$ goes towards infinity we get $$b\mu(A_{n}^m\cap B \cap A)\leq a\mu(A_{n}^m\cap B)$$ If $m$ then $n$ goes towards infinity we get $$b\mu(E_{ab})\leq a\mu(E_{ab})$$ QEA.",,"['analysis', 'measure-theory']"
75,What is the point of countable vs. uncountable sets?,What is the point of countable vs. uncountable sets?,,"I understand how to use these concepts and how to prove certain sets are countable or uncountable. However I don't get the point of it. What difference does it make whether a set is countable? People say that Cantor's proof that the real numbers are uncountable is a milestone of mathematics. Having read through the proof, I'm still struggling to understand what these ideas are important.","I understand how to use these concepts and how to prove certain sets are countable or uncountable. However I don't get the point of it. What difference does it make whether a set is countable? People say that Cantor's proof that the real numbers are uncountable is a milestone of mathematics. Having read through the proof, I'm still struggling to understand what these ideas are important.",,['analysis']
76,Proving $\frac{1}{\sin^{2}\frac{\pi}{14}} + \frac{1}{\sin^{2}\frac{3\pi}{14}} + \frac{1}{\sin^{2}\frac{5\pi}{14}} = 24$,Proving,\frac{1}{\sin^{2}\frac{\pi}{14}} + \frac{1}{\sin^{2}\frac{3\pi}{14}} + \frac{1}{\sin^{2}\frac{5\pi}{14}} = 24,"How do I show that: $$\frac{1}{\sin^{2}\frac{\pi}{14}} + \frac{1}{\sin^{2}\frac{3\pi}{14}} + \frac{1}{\sin^{2}\frac{5\pi}{14}} = 24$$ This is actually problem B $4371$ given at this link . Looks like a very interesting problem. My attempts: Well, I have been thinking about this for the whole day, and I have got some insights. I don't believe my insights will lead me to a $\text{complete}$ solution. First, I wrote $\sin\frac{5\pi}{14}$ as $\sin\frac{9 \pi}{14}$ so that if I put $A = \frac{\pi}{14}$ so that the given equation becomes, $$\frac{1}{\sin^{2}{A}} + \frac{1}{\sin^{2}{3A}} + \frac{1}{\sin^{2}{9A}} =24$$ Then I tried working with this by taking $\text{lcm}$ and multiplying and doing something, which appeared futile. Next, I actually didn't work it out, but I think we have to look for a equation which has roots as $\sin$ and then use $\text{sum of roots}$ formulas to get $24$. I think I haven't explained this clearly. $\text{Thirdly, is there a trick proving such type of identities using Gauss sums ?}$ One post related to this is: How to prove that: $\tan(3\pi/11) + 4\sin(2\pi/11) = \sqrt{11}$ I don't know how this will help as I haven't studied anything yet regarding Gauss sums.","How do I show that: $$\frac{1}{\sin^{2}\frac{\pi}{14}} + \frac{1}{\sin^{2}\frac{3\pi}{14}} + \frac{1}{\sin^{2}\frac{5\pi}{14}} = 24$$ This is actually problem B $4371$ given at this link . Looks like a very interesting problem. My attempts: Well, I have been thinking about this for the whole day, and I have got some insights. I don't believe my insights will lead me to a $\text{complete}$ solution. First, I wrote $\sin\frac{5\pi}{14}$ as $\sin\frac{9 \pi}{14}$ so that if I put $A = \frac{\pi}{14}$ so that the given equation becomes, $$\frac{1}{\sin^{2}{A}} + \frac{1}{\sin^{2}{3A}} + \frac{1}{\sin^{2}{9A}} =24$$ Then I tried working with this by taking $\text{lcm}$ and multiplying and doing something, which appeared futile. Next, I actually didn't work it out, but I think we have to look for a equation which has roots as $\sin$ and then use $\text{sum of roots}$ formulas to get $24$. I think I haven't explained this clearly. $\text{Thirdly, is there a trick proving such type of identities using Gauss sums ?}$ One post related to this is: How to prove that: $\tan(3\pi/11) + 4\sin(2\pi/11) = \sqrt{11}$ I don't know how this will help as I haven't studied anything yet regarding Gauss sums.",,['analysis']
77,"To define a measure, is it sufficient to define how to integrate continuous function?","To define a measure, is it sufficient to define how to integrate continuous function?",,"Let me make my question clear. I want to define a measure $\mu$ on a space $X$. But instead of telling you what value I assign for some subset of $X$ (measurable sets that form a $\sigma$-algebra), I tell you that for each $f$ continuous, what $\int_X f(x)d\mu (x)$ is. Then, is this measure uniquely determined? I know if I tell you how to integrate all measurable functions, then this measure is of course uniquely determined. Because integrate characteristic functions will give you measure of that respective set. But is it also true if I only define integration with continuous functions?","Let me make my question clear. I want to define a measure $\mu$ on a space $X$. But instead of telling you what value I assign for some subset of $X$ (measurable sets that form a $\sigma$-algebra), I tell you that for each $f$ continuous, what $\int_X f(x)d\mu (x)$ is. Then, is this measure uniquely determined? I know if I tell you how to integrate all measurable functions, then this measure is of course uniquely determined. Because integrate characteristic functions will give you measure of that respective set. But is it also true if I only define integration with continuous functions?",,"['analysis', 'measure-theory']"
78,Proving that a convex function is locally Lipschitz,Proving that a convex function is locally Lipschitz,,"I am trying to show that if $f$ is convex in $(a,b)$ it is Lipschitz in $[c,d]$ where $a \lt c \lt d \lt b$. Here's what I have so far: Let $t_1,t_2 \in \mathbb{R}$ such that $a \lt t_2 \lt c \lt d \lt t_1 \lt b$  and let $x_1,x_2 \in [c,d]$. Because $f$ is convex I know that $$\dfrac{f(c)-f(t_2)}{c-t_2} \lt \dfrac{f(x_2)-f(x_1)}{x_2-x_1} \lt \dfrac{f(t_1)-f(d)}{t_1-d}$$ I think I'm almost there, but what is the right $C$?","I am trying to show that if $f$ is convex in $(a,b)$ it is Lipschitz in $[c,d]$ where $a \lt c \lt d \lt b$. Here's what I have so far: Let $t_1,t_2 \in \mathbb{R}$ such that $a \lt t_2 \lt c \lt d \lt t_1 \lt b$  and let $x_1,x_2 \in [c,d]$. Because $f$ is convex I know that $$\dfrac{f(c)-f(t_2)}{c-t_2} \lt \dfrac{f(x_2)-f(x_1)}{x_2-x_1} \lt \dfrac{f(t_1)-f(d)}{t_1-d}$$ I think I'm almost there, but what is the right $C$?",,['analysis']
79,Baby Rudin: Advice,Baby Rudin: Advice,,"I am working through the first chapter of Principles of Mathematical Analysis and I am wondering how many of the twenty exercise problems I should do.  I think the first ten are very to moderately easy (with 7 as an exception), but the next ten are much more difficult.  I am of course trying to do all of them, but how many should I be content with doing successfully?  Also, how many of the exercises would a college course using the book require?  I am not trying to ""get out of work"" as I am doing this independently anyway, I just want to know what you would recommend. Thanks","I am working through the first chapter of Principles of Mathematical Analysis and I am wondering how many of the twenty exercise problems I should do.  I think the first ten are very to moderately easy (with 7 as an exception), but the next ten are much more difficult.  I am of course trying to do all of them, but how many should I be content with doing successfully?  Also, how many of the exercises would a college course using the book require?  I am not trying to ""get out of work"" as I am doing this independently anyway, I just want to know what you would recommend. Thanks",,"['analysis', 'self-learning']"
80,Why does a fourier series have a 1/2 in front of the a_0 coefficient [duplicate],Why does a fourier series have a 1/2 in front of the a_0 coefficient [duplicate],,"This question already has answers here : Why is the zeroth coefficient in a Fourier series divided by 2? (3 answers) Closed 4 months ago . I am reading up on the fourier series, and I keep seeing it as being defined as: $$ f(\theta)= \frac{1}{2}a_0 + \sum_{n=1}^{\infty}(a_n \cos(n\theta) + b_n \sin(n\theta)) $$ where $$ a_n = \frac{1}{\pi}\int_{0}^{2\pi}\cos(n\theta)f(\theta)d\theta $$ and $$ b_n = \frac{1}{\pi}\int_{0}^{2\pi}\sin(n\theta)f(\theta)d\theta $$ I understand the derivation of the coefficients using trig integral identities, but I can't find a clear explanation of why $\frac{1}{2}$ is in front of $a_0$. Can anyone help show my why this is the case? Why can't we just have $a_0$ with no number in front of it. Thanks! edit: corrected summation term","This question already has answers here : Why is the zeroth coefficient in a Fourier series divided by 2? (3 answers) Closed 4 months ago . I am reading up on the fourier series, and I keep seeing it as being defined as: $$ f(\theta)= \frac{1}{2}a_0 + \sum_{n=1}^{\infty}(a_n \cos(n\theta) + b_n \sin(n\theta)) $$ where $$ a_n = \frac{1}{\pi}\int_{0}^{2\pi}\cos(n\theta)f(\theta)d\theta $$ and $$ b_n = \frac{1}{\pi}\int_{0}^{2\pi}\sin(n\theta)f(\theta)d\theta $$ I understand the derivation of the coefficients using trig integral identities, but I can't find a clear explanation of why $\frac{1}{2}$ is in front of $a_0$. Can anyone help show my why this is the case? Why can't we just have $a_0$ with no number in front of it. Thanks! edit: corrected summation term",,"['analysis', 'trigonometry', 'fourier-analysis', 'fourier-series', 'trigonometric-series']"
81,Prove that the golden ratio is irrational by contradiction,Prove that the golden ratio is irrational by contradiction,,"I am struggling to see where the contradiction lies in my proof. In a previous example, $1/\phi = \phi-1$ where $\phi$ is the golden ratio $\frac{\sqrt{5} + 1}{2}$. Since I am proving by contradiction, I started out by assuming that $ϕ$ is rational. Then, by definition, there exists $a,b$ such that $\phi = a/b$. After some simple calculations and using the result shown from my previous example, I found that $\phi= b/(a-b)$. I also know that $b < a$ from directly calculating the ratio. I know there is a contradiction in the result $ϕ = b/(a-b)$ but I cannot see it. Any help would be appreciated.","I am struggling to see where the contradiction lies in my proof. In a previous example, $1/\phi = \phi-1$ where $\phi$ is the golden ratio $\frac{\sqrt{5} + 1}{2}$. Since I am proving by contradiction, I started out by assuming that $ϕ$ is rational. Then, by definition, there exists $a,b$ such that $\phi = a/b$. After some simple calculations and using the result shown from my previous example, I found that $\phi= b/(a-b)$. I also know that $b < a$ from directly calculating the ratio. I know there is a contradiction in the result $ϕ = b/(a-b)$ but I cannot see it. Any help would be appreciated.",,"['analysis', 'elementary-number-theory', 'golden-ratio']"
82,Companions to Rudin?,Companions to Rudin?,,"I'm starting to read Baby Rudin (Principles of mathematical analysis) now and I wonder whether you know of any companions to it. Another supplementary book would do too. I tried Silvia's notes, but I found them a bit too ""logical"" so to say. Are they good? What else do you recommend?","I'm starting to read Baby Rudin (Principles of mathematical analysis) now and I wonder whether you know of any companions to it. Another supplementary book would do too. I tried Silvia's notes, but I found them a bit too ""logical"" so to say. Are they good? What else do you recommend?",,"['analysis', 'reference-request']"
83,What is the difference between a supremum and maximum; and also between the infimum and minimum?,What is the difference between a supremum and maximum; and also between the infimum and minimum?,,"What is the difference between a supremum and maximum; and similarly the infimum and minimum? Also, how does one tell if they exist? Here is an example: $$x_n = \frac{n}{2n-1}$$ Determine whether the maximum, the minimum, the supremum, and the   infimum of the sequence $x_n$ n=1 to n=+∞ My understanding: The limit is 1/2. $x_n$ is decreasing. The supremum exists as n goes to +∞. The infimum does not exist as limited by n=1, minimum = 1/2. The maximum and supremum exists, both equal to 1.","What is the difference between a supremum and maximum; and similarly the infimum and minimum? Also, how does one tell if they exist? Here is an example: Determine whether the maximum, the minimum, the supremum, and the   infimum of the sequence n=1 to n=+∞ My understanding: The limit is 1/2. is decreasing. The supremum exists as n goes to +∞. The infimum does not exist as limited by n=1, minimum = 1/2. The maximum and supremum exists, both equal to 1.",x_n = \frac{n}{2n-1} x_n x_n,['analysis']
84,Experiences with Rudin?,Experiences with Rudin?,,"So I am trying to tutor a friend in analysis. This is her first time with proofs. We are on chapter 2 – the topology chapter – of Rudin's Principles of Mathematical Analysis and she is extremely frustrated, mainly because she expects herself to learn at a more rapid pace than is occurring (although she is doing fine imo). When I was learning the material, I recall Rudin taking a long time, as I presume it was for many first timers. So what are you guys' experience with Rudin? How long did you spend on chapter 2? Is there anything that you found useful to help you get through the book? I am hoping that if she sees that the math community finds the material/book challenging (assuming you do), she will feel more comfortable.","So I am trying to tutor a friend in analysis. This is her first time with proofs. We are on chapter 2 – the topology chapter – of Rudin's Principles of Mathematical Analysis and she is extremely frustrated, mainly because she expects herself to learn at a more rapid pace than is occurring (although she is doing fine imo). When I was learning the material, I recall Rudin taking a long time, as I presume it was for many first timers. So what are you guys' experience with Rudin? How long did you spend on chapter 2? Is there anything that you found useful to help you get through the book? I am hoping that if she sees that the math community finds the material/book challenging (assuming you do), she will feel more comfortable.",,['analysis']
85,Why modern mathematics prefer $\sigma$-algebra to $\sigma$-ring in measure theory?,Why modern mathematics prefer -algebra to -ring in measure theory?,\sigma \sigma,"Actually, i posted the exact same question before (about a year ago), but now i lost my past account so i couldn't find the past post.. So i googled this, but i couldn't find a satisfying post. I'll illustrate two different situations below ======== First definition Let $X$ be a set and $\sum \subset P(X)$.   Then, $\sum$ is a sigma algebra on $X$ iff (1)it is closed under countable union, (2)it is closed under complement and (3)$X\in \sum$ If we start measure theory via this definition, just like topology, for a given sigma algebra, we can immediately know that on which set this sigma algebra is defined. Thus, this definition makes it possible to view a sigma-algebra as a measurable space. ======= Second definition Let $X$ be a set and $\sum \subset P(X)$.   Then $\sum$ is a sigma ring on $X$ Iff (i) it is closed under countable union, (ii)$\forall A,B\in \sum, A-B\in \sum$. If we start measure theory in this way, when we want to talk about a measure space, a set and a sigma-ring on this set should be given together. This is the only disadvantage of sigma ring in my opinion. Let $M=\bigcup \sum$. Then, $\sum$ is a sigma algebra on $M$ iff $M\in \sum$. Thus, the definition of sigma ring is strictly stronger than that of sigma algebra. I cannot understand why mathematicians got rid of this generalized definition and prefer a weaker one. I remember that someone answered me that a lot of interesting spaces are integrable themselves. Which means, lot of interesting $\sigma$- whatsoever contains the whole space, so they are $\sigma$-algebras. But, isn't there any interesting $\sigma$-ring which is not $\sigma$-algebra? I really hate to go back and define something again more generally so that i have to prove every single theorem depending on property of the older definition. For example, i started analysis with Rudin-PMA and he defined Topology as topology induced by metric space in usual sense. It was painful to me to distinguish theorems which hold only in metric space and which hold in topological space, when i learned general-topology later. It's hard to confirm myself why mathematicians prefer this weak definition taking this risk..","Actually, i posted the exact same question before (about a year ago), but now i lost my past account so i couldn't find the past post.. So i googled this, but i couldn't find a satisfying post. I'll illustrate two different situations below ======== First definition Let $X$ be a set and $\sum \subset P(X)$.   Then, $\sum$ is a sigma algebra on $X$ iff (1)it is closed under countable union, (2)it is closed under complement and (3)$X\in \sum$ If we start measure theory via this definition, just like topology, for a given sigma algebra, we can immediately know that on which set this sigma algebra is defined. Thus, this definition makes it possible to view a sigma-algebra as a measurable space. ======= Second definition Let $X$ be a set and $\sum \subset P(X)$.   Then $\sum$ is a sigma ring on $X$ Iff (i) it is closed under countable union, (ii)$\forall A,B\in \sum, A-B\in \sum$. If we start measure theory in this way, when we want to talk about a measure space, a set and a sigma-ring on this set should be given together. This is the only disadvantage of sigma ring in my opinion. Let $M=\bigcup \sum$. Then, $\sum$ is a sigma algebra on $M$ iff $M\in \sum$. Thus, the definition of sigma ring is strictly stronger than that of sigma algebra. I cannot understand why mathematicians got rid of this generalized definition and prefer a weaker one. I remember that someone answered me that a lot of interesting spaces are integrable themselves. Which means, lot of interesting $\sigma$- whatsoever contains the whole space, so they are $\sigma$-algebras. But, isn't there any interesting $\sigma$-ring which is not $\sigma$-algebra? I really hate to go back and define something again more generally so that i have to prove every single theorem depending on property of the older definition. For example, i started analysis with Rudin-PMA and he defined Topology as topology induced by metric space in usual sense. It was painful to me to distinguish theorems which hold only in metric space and which hold in topological space, when i learned general-topology later. It's hard to confirm myself why mathematicians prefer this weak definition taking this risk..",,"['analysis', 'measure-theory']"
86,What does this $\asymp$ symbol mean? (subject: analytic number theory),What does this  symbol mean? (subject: analytic number theory),\asymp,"I'm reading a survey article by Andrew Granville on analytic number theory. On page 22 of the paper, there appears a strange looking symbol, undefined.  I've circled it in red in the screenshot below. Since it's not defined in the paper, I'm assuming it must be standard notation. From the context, I'm assuming it means something like ""as compared to"", or ""with reference to"", but that's just a guess. Can anyone identify the symbol, even better explain what it means and/or provide a reference?  Is there a name to speak the symbol? Thanks in advance.","I'm reading a survey article by Andrew Granville on analytic number theory. On page 22 of the paper, there appears a strange looking symbol, undefined.  I've circled it in red in the screenshot below. Since it's not defined in the paper, I'm assuming it must be standard notation. From the context, I'm assuming it means something like ""as compared to"", or ""with reference to"", but that's just a guess. Can anyone identify the symbol, even better explain what it means and/or provide a reference?  Is there a name to speak the symbol? Thanks in advance.",,"['analysis', 'notation', 'asymptotics', 'analytic-number-theory']"
87,When $\min \max = \max \min$?,When ?,\min \max = \max \min,"Let $X \subset \mathbb{R}^n$ and $Y \subset \mathbb{R}^m$ be compact sets. Consider a continuous function $f : X \times Y \rightarrow \mathbb{R}$. Say under which condition we have $$ \min_{x \in X}  \max_{y \in Y} f(x,y) = \max_{y \in Y} \min_{x \in X} f(x,y). $$ From this we have that $\max_{y \in Y} \min_{x \in X} f(x,y) \leq \min_{x \in X}  \max_{y \in Y} f(x,y)$. So here we are looking for conditions on $f$ such that we have the equality.","Let $X \subset \mathbb{R}^n$ and $Y \subset \mathbb{R}^m$ be compact sets. Consider a continuous function $f : X \times Y \rightarrow \mathbb{R}$. Say under which condition we have $$ \min_{x \in X}  \max_{y \in Y} f(x,y) = \max_{y \in Y} \min_{x \in X} f(x,y). $$ From this we have that $\max_{y \in Y} \min_{x \in X} f(x,y) \leq \min_{x \in X}  \max_{y \in Y} f(x,y)$. So here we are looking for conditions on $f$ such that we have the equality.",,"['analysis', 'optimization', 'nonlinear-optimization']"
88,"What does the term ""regularity"" mean?","What does the term ""regularity"" mean?",,"When I was an undergraduate, I took a course on regularity theory for nonlinear elliptic systems. This included topics such as the direct method of calculus of variations, mollifiers, integration over distributions, and the Calderón-Zygmund decomposition. In the introduction, there was a brief mention of regularity theory allowing us to change the space and conditions you are working in to make an equation work, or work with weaker conditions. Set $\Delta u=0$ in a domain $\Omega$. Integrating over the domain and introducing a term $\varphi$ we concluded that $$ \int_{\Omega}u\Delta \varphi=0~\forall\varphi\in C^{\infty}_{c}(\Omega),~u\in L^1_{loc}(\Omega). $$ This has changed the condition of $u$ being harmonic to $L^1$ integrable. (Note: I am aware that what I have written is not completely mathematically rigorous. My notes are old and a bit rough). I have also seen the term ""regularity"" related to differentiability and ""regularity conditions"" on initial data and ""regularity control"" on the solution, when referring to work on Strichartz and energy estimates. $\textbf{Question}$: Can someone shed some light on the term regularity? I cannot find a formal definition of the term ""regularity"". I have pieced together information I have to get a rough idea of what it is. I appreciate any thoughts or comments. Thank you in advance.","When I was an undergraduate, I took a course on regularity theory for nonlinear elliptic systems. This included topics such as the direct method of calculus of variations, mollifiers, integration over distributions, and the Calderón-Zygmund decomposition. In the introduction, there was a brief mention of regularity theory allowing us to change the space and conditions you are working in to make an equation work, or work with weaker conditions. Set $\Delta u=0$ in a domain $\Omega$. Integrating over the domain and introducing a term $\varphi$ we concluded that $$ \int_{\Omega}u\Delta \varphi=0~\forall\varphi\in C^{\infty}_{c}(\Omega),~u\in L^1_{loc}(\Omega). $$ This has changed the condition of $u$ being harmonic to $L^1$ integrable. (Note: I am aware that what I have written is not completely mathematically rigorous. My notes are old and a bit rough). I have also seen the term ""regularity"" related to differentiability and ""regularity conditions"" on initial data and ""regularity control"" on the solution, when referring to work on Strichartz and energy estimates. $\textbf{Question}$: Can someone shed some light on the term regularity? I cannot find a formal definition of the term ""regularity"". I have pieced together information I have to get a rough idea of what it is. I appreciate any thoughts or comments. Thank you in advance.",,"['analysis', 'partial-differential-equations']"
89,"If $f'(x)>0$ on $E$ , where $m(E)>0,$ then $m(f(E))>0$","If  on  , where  then","f'(x)>0 E m(E)>0, m(f(E))>0","Let $f:\mathbb R\to \mathbb R,$ and suppose $f$ is differentiable at every point of a measurable $E\subset \mathbb R,$ with $f'>0$ on $E$ . Suppose also that $m(E)>0$ (where $m$ is Lebesgue measure). Prove that $m(f(E))>0$ . My proof: Since $f$ is differentiable then it's continuous and hence it preserves both compact sets and intervals. Now since $m(E)>0$ we can find compact interval inside it (is this true or not?) If this is true so the proof is completed. I know that if $E$ is measurable then $E$ is either Borel set or a set of measure zero. So here $E$ is Borel, but still not necessarily to be an interval.","Let and suppose is differentiable at every point of a measurable with on . Suppose also that (where is Lebesgue measure). Prove that . My proof: Since is differentiable then it's continuous and hence it preserves both compact sets and intervals. Now since we can find compact interval inside it (is this true or not?) If this is true so the proof is completed. I know that if is measurable then is either Borel set or a set of measure zero. So here is Borel, but still not necessarily to be an interval.","f:\mathbb R\to \mathbb R, f E\subset \mathbb R, f'>0 E m(E)>0 m m(f(E))>0 f m(E)>0 E E E","['analysis', 'measure-theory', 'lebesgue-measure']"
90,Upper and Lower Bounds of $\emptyset$,Upper and Lower Bounds of,\emptyset,"From some reading, I've noticed that $\sup(\emptyset)=\min(S)$, but $\inf(\emptyset)=\max(S)$, given that $\min(S)$ and $\max(S)$ exist, where $S$ is the universe in which one is working. Is there some inherent reasoning/proof as to why this is? It seems strange to me that an upper bound of a set would be smaller than a lower bound of the same set.","From some reading, I've noticed that $\sup(\emptyset)=\min(S)$, but $\inf(\emptyset)=\max(S)$, given that $\min(S)$ and $\max(S)$ exist, where $S$ is the universe in which one is working. Is there some inherent reasoning/proof as to why this is? It seems strange to me that an upper bound of a set would be smaller than a lower bound of the same set.",,['analysis']
91,"Prove that $fg\in L^r(\Omega)$ if $f\in L^p(\Omega),g\in L^q(\Omega)$, and $\frac1 p+\frac1 q=\frac1 r$","Prove that  if , and","fg\in L^r(\Omega) f\in L^p(\Omega),g\in L^q(\Omega) \frac1 p+\frac1 q=\frac1 r","Can anyone give me a hint for proving the following: Let $\Omega$ be a measure space. Assume $f \in L^p(\Omega)$ and $g \in L^q(\Omega)$ with $1 \leq p, q \leq \infty$ and $\frac1p + \frac1q \leq 1$. Prove that $fg \in L^r(\Omega)$ with $\frac1r = \frac1p + \frac1q$. Note: One should be able to use (the standard) Hölder inequality. Notice that if you have $\frac1p + \frac1q = 1$ you recover the former result.","Can anyone give me a hint for proving the following: Let $\Omega$ be a measure space. Assume $f \in L^p(\Omega)$ and $g \in L^q(\Omega)$ with $1 \leq p, q \leq \infty$ and $\frac1p + \frac1q \leq 1$. Prove that $fg \in L^r(\Omega)$ with $\frac1r = \frac1p + \frac1q$. Note: One should be able to use (the standard) Hölder inequality. Notice that if you have $\frac1p + \frac1q = 1$ you recover the former result.",,"['analysis', 'measure-theory']"
92,Is there a formula similar to $f(x+a) = e^{a\frac{d}{dx}}f(x)$ to express $f(\alpha\cdot x)$?,Is there a formula similar to  to express ?,f(x+a) = e^{a\frac{d}{dx}}f(x) f(\alpha\cdot x),"Using the Taylor expansion $$f(x+a) = \sum_{k=0}^\infty \frac{a^k}{k!}\frac{d^k }{dx^k}f(x)$$ one can formally express the sum as the linear operator $e^{a\frac{d}{dx}}$ to obtain $$f(x+a) = e^{a\frac{d}{dx}}f(x).$$ But, does a linear operator $\hat A$ exist such that $$ f(\alpha\cdot x) = \hat A(\alpha) f(x)$$ for some $\alpha\in\mathbb C$?","Using the Taylor expansion $$f(x+a) = \sum_{k=0}^\infty \frac{a^k}{k!}\frac{d^k }{dx^k}f(x)$$ one can formally express the sum as the linear operator $e^{a\frac{d}{dx}}$ to obtain $$f(x+a) = e^{a\frac{d}{dx}}f(x).$$ But, does a linear operator $\hat A$ exist such that $$ f(\alpha\cdot x) = \hat A(\alpha) f(x)$$ for some $\alpha\in\mathbb C$?",,"['analysis', 'operator-theory', 'taylor-expansion']"
93,Question about statement of Rank Theorem in Rudin,Question about statement of Rank Theorem in Rudin,,"Theorem Suppose $m,n,r$ are nonnegative integers, $m\ge r, n\ge r$, $F$ is a $C^1$ mapping of an open set $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$, and $F'(x)$ has rank $r$ for every $x\in E$. Fix $a\in E$, put $A = F'(a)$, let $Y_1$ be the range of $A$, and let $P$ be a projection in $\mathbb{R}^m$ whose range is $Y_1$ and let $Y_2$ be the kernel of $P$. Then there are open sets $U$ and $V$ in $\mathbb{R}^n$, with $a\in U\subset E$, and there is a 1-1 $C^1$ mapping $H$ of $V$ onto $U$ (whose inverse is also of $C^1$) such that $$F(H(x)) = Ax+\phi(Ax)\;\;\;\;(x\in V)$$where $\phi$ is a $C^1$ mapping of the open set $A(V)\subset Y_1$ into $Y_2$. Two questions: 1. Why is $A(V)$ a open set? 2. What does this theorem really say? I lost intuition due to its complicated formulation.","Theorem Suppose $m,n,r$ are nonnegative integers, $m\ge r, n\ge r$, $F$ is a $C^1$ mapping of an open set $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$, and $F'(x)$ has rank $r$ for every $x\in E$. Fix $a\in E$, put $A = F'(a)$, let $Y_1$ be the range of $A$, and let $P$ be a projection in $\mathbb{R}^m$ whose range is $Y_1$ and let $Y_2$ be the kernel of $P$. Then there are open sets $U$ and $V$ in $\mathbb{R}^n$, with $a\in U\subset E$, and there is a 1-1 $C^1$ mapping $H$ of $V$ onto $U$ (whose inverse is also of $C^1$) such that $$F(H(x)) = Ax+\phi(Ax)\;\;\;\;(x\in V)$$where $\phi$ is a $C^1$ mapping of the open set $A(V)\subset Y_1$ into $Y_2$. Two questions: 1. Why is $A(V)$ a open set? 2. What does this theorem really say? I lost intuition due to its complicated formulation.",,"['analysis', 'differential-geometry', 'manifolds', 'intuition']"
94,$f(f(x))=f(x)$ question,question,f(f(x))=f(x),I am wondering what is the class of functions $f: \mathbb{R}\rightarrow\mathbb{R}$ such that $f(f(x))=f(x)$? I think it should be: Constant Value functions the identity function absolute value function $|x|$ But I don't know if this is right or how to show it rigorously. Any suggestions?,I am wondering what is the class of functions $f: \mathbb{R}\rightarrow\mathbb{R}$ such that $f(f(x))=f(x)$? I think it should be: Constant Value functions the identity function absolute value function $|x|$ But I don't know if this is right or how to show it rigorously. Any suggestions?,,"['analysis', 'functional-equations', 'function-and-relation-composition']"
95,Is there an Inverse Gamma $\Gamma^{-1} (z) $ function?,Is there an Inverse Gamma  function?,\Gamma^{-1} (z) ,"Since $\Gamma$ is not one to one over the complex domain, Is it possible to define some principal values ( analogues to  Principal Roots for the Root function ) so we can have a $\Gamma^{-1} (z)$ (inverse $\Gamma$ function)?","Since $\Gamma$ is not one to one over the complex domain, Is it possible to define some principal values ( analogues to  Principal Roots for the Root function ) so we can have a $\Gamma^{-1} (z)$ (inverse $\Gamma$ function)?",,"['analysis', 'special-functions']"
96,When is $\mathbb{Z}[\alpha]$ dense in $\mathbb{C}$?,When is  dense in ?,\mathbb{Z}[\alpha] \mathbb{C},"Let $\alpha$ be a nonreal algebraic number. I'm interested in conditions that imply that $\mathbb{Z}[\alpha]$ is dense in $\mathbb{C}$. I'm particularly interested algebraic integer $\alpha$. This is what I know so far: if there is a $n \in \mathbb{N}$ such that $\alpha^n \in \mathbb{R} \setminus \mathbb{Z}$, then $\mathbb{Z}[\alpha]$ is dense in $\mathbb{C}$; algebraic integers of degree two don't satisfy the condition, although algebraic nonintegers of degree two may. Many thanks in advance.","Let $\alpha$ be a nonreal algebraic number. I'm interested in conditions that imply that $\mathbb{Z}[\alpha]$ is dense in $\mathbb{C}$. I'm particularly interested algebraic integer $\alpha$. This is what I know so far: if there is a $n \in \mathbb{N}$ such that $\alpha^n \in \mathbb{R} \setminus \mathbb{Z}$, then $\mathbb{Z}[\alpha]$ is dense in $\mathbb{C}$; algebraic integers of degree two don't satisfy the condition, although algebraic nonintegers of degree two may. Many thanks in advance.",,['analysis']
97,Lasso - constraint form equivalent to penalty form,Lasso - constraint form equivalent to penalty form,,"We know that there are two definitions to describe lasso. Regression with constraint definition: $$\min\limits_{\beta} \|y-X\beta\|^2, \sum\limits_{p}|\beta_p|\leq t, \exists t $$ Regression with penalty definition: $$\min\limits_{\beta} \|y-X\beta\|^2+\lambda\sum\limits_{p}|\beta_p|, \exists\lambda$$ But how to convince these two definition are equivalent for some $t$ and $\lambda$? I think Lagrange multipliers is the key to show the relationship between two definitions. However, I failed to work out it rigorously because I assume the properties of lasso ($\sum\limits_{p}|\beta_p|=t$) in regression with constraint definition. Does anyone can show me the complete and rigorous proof of these two definitions are equivalent for some $t$ and $\lambda$? Thank you very much if you can help. EDIT: According to the the comments below, I edited my question.","We know that there are two definitions to describe lasso. Regression with constraint definition: $$\min\limits_{\beta} \|y-X\beta\|^2, \sum\limits_{p}|\beta_p|\leq t, \exists t $$ Regression with penalty definition: $$\min\limits_{\beta} \|y-X\beta\|^2+\lambda\sum\limits_{p}|\beta_p|, \exists\lambda$$ But how to convince these two definition are equivalent for some $t$ and $\lambda$? I think Lagrange multipliers is the key to show the relationship between two definitions. However, I failed to work out it rigorously because I assume the properties of lasso ($\sum\limits_{p}|\beta_p|=t$) in regression with constraint definition. Does anyone can show me the complete and rigorous proof of these two definitions are equivalent for some $t$ and $\lambda$? Thank you very much if you can help. EDIT: According to the the comments below, I edited my question.",,"['analysis', 'optimization', 'regression', 'lagrange-multiplier']"
98,How to prove that the set of rational numbers are countable? [duplicate],How to prove that the set of rational numbers are countable? [duplicate],,This question already has answers here : How to prove that $\mathbb{Q}$ ( the rationals) is a countable set (7 answers) Closed 9 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Can any one tell me how to prove that the set of rational numbers are countable? Prove give me a prove? Thanks.,This question already has answers here : How to prove that $\mathbb{Q}$ ( the rationals) is a countable set (7 answers) Closed 9 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Can any one tell me how to prove that the set of rational numbers are countable? Prove give me a prove? Thanks.,,['analysis']
99,"""Numbers"" bigger than every natural number","""Numbers"" bigger than every natural number",,"In the book Understanding analysis , by Abbot, when discussing the Archimedean property, the author states that there are ordered field extensions of $\mathbb{Q}$ that include ""numbers"" bigger than every natural number. Could someone provide examples and and explanation why this could be the case?","In the book Understanding analysis , by Abbot, when discussing the Archimedean property, the author states that there are ordered field extensions of that include ""numbers"" bigger than every natural number. Could someone provide examples and and explanation why this could be the case?",\mathbb{Q},"['analysis', 'examples-counterexamples', 'extension-field']"
