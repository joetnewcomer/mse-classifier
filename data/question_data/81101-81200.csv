,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"What does the comma do in the formula ""$v_i = A_{i,i},\ i=\{1,2,\dotsc,\min(m,n)\}$""?","What does the comma do in the formula """"?","v_i = A_{i,i},\ i=\{1,2,\dotsc,\min(m,n)\}",I am looking at the following formula that defines the vector that contains the diagonal elements of a matrix: $v$ is a vector and $A$ is a matrix (rectangular or square). I understand every symbol used in the formula except the comma indicated by the red arrow. My best guess is that it is to separate the formula from the definition of the numbers contained in $i$ . Is that so?,I am looking at the following formula that defines the vector that contains the diagonal elements of a matrix: is a vector and is a matrix (rectangular or square). I understand every symbol used in the formula except the comma indicated by the red arrow. My best guess is that it is to separate the formula from the definition of the numbers contained in . Is that so?,v A i,"['linear-algebra', 'matrices']"
1,How to convert a column vector into a diagonal matrix with same entries in same order?,How to convert a column vector into a diagonal matrix with same entries in same order?,,Assume $c$ is a column vector. What mathematical operation or expression can produce a diagonal matrix with entries of that of $c$ in same order. Does such an expression exist? Basically I know that the sum of entries of $c$ is zero. I want to write it in an matrix expression/equation. I am hoping if I get a diagonal matrix and then say trace of that matrix is zero.,Assume is a column vector. What mathematical operation or expression can produce a diagonal matrix with entries of that of in same order. Does such an expression exist? Basically I know that the sum of entries of is zero. I want to write it in an matrix expression/equation. I am hoping if I get a diagonal matrix and then say trace of that matrix is zero.,c c c,"['linear-algebra', 'matrices']"
2,"For the purposes of DFT, is ""the"" primitive root of unity $w_n = e^{ 2\pi i / n }$ or $w_n = e^{-2\pi i / n }$?","For the purposes of DFT, is ""the"" primitive root of unity  or ?",w_n = e^{ 2\pi i / n } w_n = e^{-2\pi i / n },"I'm working on the section of Strang's Linear Algebra and Its Applications 4e that discusses discrete Fourier transforms. To make the exercises easier, I wrote myself a Python script that generates the $n$ th Fourier matrix as described here: I've run into trouble regarding the definition of $w$ . While any integer value of $k$ in $e^{ 2\pi k i / n }$ will get us a complex root of unity, to fill in the matrix, we need the primitive complex root of unity. Strang defines $w_n$ as $e^{ 2\pi i / n }$ : But when I coded $w_n$ as such in my DFT matrix function, it kept giving me the complex conjugate of what I wanted, and someone on Stack Overflow pointed out that I need to use $w_n = e^{-2\pi i / n }$ instead. This is consistent with the definition given on Wikipedia: But here's what throws me off. Looking at the unit circle, it seems like the primitive root of unity should be the one corresponding to a $2\pi / n$ rotation in the counterclockwise direction. That's what Strang does in this example diagram for $w_8$ : (The omission of $i$ is just a typo, right?) If we replace that circled bit with $e^{-2\pi i / 8 }$ , we get $\bar{w}$ , an angle in the fourth quadrant, and taking increasing powers of it will move us around the unit circle in a clockwise direction. If that's correct, Why does the Fourier matrix reverse the usual convention of counterclockwise rotation? Why does my textbook seem to state one definition and use another? In general, when should I use $w_n = e^{ 2\pi i / n }$ , and when should I use $w_n = e^{ -2\pi i / n }$ ? As always, thank you. Thank you all for your detailed explanations. Indeed, Strang's convention is to work with the inverse of what's described elsewhere, hence the change in sign. However, this introduces a new issue in one of the assignment problems. Question 3.5.3 asks, If you form a 3 by 3 submatrix of the 6 by 6 matrix $F_6$ , keeping only the entries in its first, third, and fifth rows and columns, what is that submatrix? The answer text reads simply, The submatrix is $F_3$ . But using my Python function with $w_n = e^{2\pi i / n }$ , I get the following inconsistent result: F6 matrix: [[ 1. +0.j     1. +0.j     1. +0.j     1. +0.j     1. +0.j     1. +0.j   ]  [ 1. +0.j     0.5+0.866j -0.5+0.866j -1. +0.j    -0.5-0.866j  0.5-0.866j]  [ 1. +0.j    -0.5+0.866j -0.5-0.866j  1. -0.j    -0.5+0.866j -0.5-0.866j]  [ 1. +0.j    -1. +0.j     1. -0.j    -1. +0.j     1. -0.j    -1. +0.j   ]  [ 1. +0.j    -0.5-0.866j -0.5+0.866j  1. -0.j    -0.5-0.866j -0.5+0.866j]  [ 1. +0.j     0.5-0.866j -0.5-0.866j -1. +0.j    -0.5+0.866j  0.5+0.866j]]  F6 submatrix: [[ 1. +0.j     1. +0.j     1. +0.j   ]  [ 1. +0.j    -0.5-0.866j -0.5+0.866j]  [ 1. +0.j    -0.5+0.866j -0.5-0.866j]]  F3 matrix (should match the above) [[ 1. +0.j     1. +0.j     1. +0.j   ]  [ 1. +0.j    -0.5+0.866j -0.5-0.866j]  [ 1. +0.j    -0.5-0.866j -0.5+0.866j]] You can see the code here .","I'm working on the section of Strang's Linear Algebra and Its Applications 4e that discusses discrete Fourier transforms. To make the exercises easier, I wrote myself a Python script that generates the th Fourier matrix as described here: I've run into trouble regarding the definition of . While any integer value of in will get us a complex root of unity, to fill in the matrix, we need the primitive complex root of unity. Strang defines as : But when I coded as such in my DFT matrix function, it kept giving me the complex conjugate of what I wanted, and someone on Stack Overflow pointed out that I need to use instead. This is consistent with the definition given on Wikipedia: But here's what throws me off. Looking at the unit circle, it seems like the primitive root of unity should be the one corresponding to a rotation in the counterclockwise direction. That's what Strang does in this example diagram for : (The omission of is just a typo, right?) If we replace that circled bit with , we get , an angle in the fourth quadrant, and taking increasing powers of it will move us around the unit circle in a clockwise direction. If that's correct, Why does the Fourier matrix reverse the usual convention of counterclockwise rotation? Why does my textbook seem to state one definition and use another? In general, when should I use , and when should I use ? As always, thank you. Thank you all for your detailed explanations. Indeed, Strang's convention is to work with the inverse of what's described elsewhere, hence the change in sign. However, this introduces a new issue in one of the assignment problems. Question 3.5.3 asks, If you form a 3 by 3 submatrix of the 6 by 6 matrix , keeping only the entries in its first, third, and fifth rows and columns, what is that submatrix? The answer text reads simply, The submatrix is . But using my Python function with , I get the following inconsistent result: F6 matrix: [[ 1. +0.j     1. +0.j     1. +0.j     1. +0.j     1. +0.j     1. +0.j   ]  [ 1. +0.j     0.5+0.866j -0.5+0.866j -1. +0.j    -0.5-0.866j  0.5-0.866j]  [ 1. +0.j    -0.5+0.866j -0.5-0.866j  1. -0.j    -0.5+0.866j -0.5-0.866j]  [ 1. +0.j    -1. +0.j     1. -0.j    -1. +0.j     1. -0.j    -1. +0.j   ]  [ 1. +0.j    -0.5-0.866j -0.5+0.866j  1. -0.j    -0.5-0.866j -0.5+0.866j]  [ 1. +0.j     0.5-0.866j -0.5-0.866j -1. +0.j    -0.5+0.866j  0.5+0.866j]]  F6 submatrix: [[ 1. +0.j     1. +0.j     1. +0.j   ]  [ 1. +0.j    -0.5-0.866j -0.5+0.866j]  [ 1. +0.j    -0.5+0.866j -0.5-0.866j]]  F3 matrix (should match the above) [[ 1. +0.j     1. +0.j     1. +0.j   ]  [ 1. +0.j    -0.5+0.866j -0.5-0.866j]  [ 1. +0.j    -0.5-0.866j -0.5+0.866j]] You can see the code here .",n w k e^{ 2\pi k i / n } w_n e^{ 2\pi i / n } w_n w_n = e^{-2\pi i / n } 2\pi / n w_8 i e^{-2\pi i / 8 } \bar{w} w_n = e^{ 2\pi i / n } w_n = e^{ -2\pi i / n } F_6 F_3 w_n = e^{2\pi i / n },"['linear-algebra', 'matrices', 'complex-numbers', 'fourier-analysis', 'fourier-transform']"
3,Proving that $I+A$ is invertable when $A$ is nilpotent: What intuition leads to a particular approach?,Proving that  is invertable when  is nilpotent: What intuition leads to a particular approach?,I+A A,"In an answer to this question , it has been suggested to consider the following: $$(I+A)(\sum_{j=0}^n(-A)^j)$$ Through a series of algebraic operations, it can be shown that $\sum_{j=0}^n(-A)^j$ is in fact the inverse of $I+A$ . How would we have known to multiply by $\sum_{j=0}^n(-A)^j$ ? If there isn't an identity or formula that would indicate such a multiplication is a reasonable avenue of inquiry, then how would we otherwise derive $\sum_{j=0}^n(-A)^j$ ?","In an answer to this question , it has been suggested to consider the following: Through a series of algebraic operations, it can be shown that is in fact the inverse of . How would we have known to multiply by ? If there isn't an identity or formula that would indicate such a multiplication is a reasonable avenue of inquiry, then how would we otherwise derive ?",(I+A)(\sum_{j=0}^n(-A)^j) \sum_{j=0}^n(-A)^j I+A \sum_{j=0}^n(-A)^j \sum_{j=0}^n(-A)^j,"['abstract-algebra', 'matrices', 'inverse', 'intuition', 'advice']"
4,"Give a 2*2 block matrix $M = \begin{bmatrix}A&B\\0&C \end{bmatrix}$ and find a formula for $M^{-1}$ in terms of $A$, $B$, and $C$","Give a 2*2 block matrix  and find a formula for  in terms of , , and",M = \begin{bmatrix}A&B\\0&C \end{bmatrix} M^{-1} A B C,"I am reading the book, Applied Linear Algebra and Matrix Analysis . When I was doing the exercise of Section2.5 Problem 29, I was puzzled at solving it. Here is the problem description: Give a 2*2 block matrix $M = \begin{bmatrix}A&B\\0&C \end{bmatrix}$ ,   where the blocks A and C are invertible matrices, find a formula for $M^{-1}$ in terms of $A$ , $B$ , and $C$ . And the answer is： Assume $M^{-1}$ has the same form as $M$ and solve for the blocks in $M$ using $MM^{−1}$ = I. But I still confused about how to make it right. So I do some searches on the net and I find a paper give explicit inverse formulae for 2 × 2 block matrices with three different partitions. BUT, as the paper mentioned, all the blocks must be nonsingular , which means they all have a matrix inverse . It is not subject to the conditions, which only say the blocks $A$ and $C$ are invertible matrices . SO, I want to know if there is a better way which is subject to conditions. I will appreciate it if anyone help me.","I am reading the book, Applied Linear Algebra and Matrix Analysis . When I was doing the exercise of Section2.5 Problem 29, I was puzzled at solving it. Here is the problem description: Give a 2*2 block matrix ,   where the blocks A and C are invertible matrices, find a formula for in terms of , , and . And the answer is： Assume has the same form as and solve for the blocks in using = I. But I still confused about how to make it right. So I do some searches on the net and I find a paper give explicit inverse formulae for 2 × 2 block matrices with three different partitions. BUT, as the paper mentioned, all the blocks must be nonsingular , which means they all have a matrix inverse . It is not subject to the conditions, which only say the blocks and are invertible matrices . SO, I want to know if there is a better way which is subject to conditions. I will appreciate it if anyone help me.",M = \begin{bmatrix}A&B\\0&C \end{bmatrix} M^{-1} A B C M^{-1} M M MM^{−1} A C,"['linear-algebra', 'matrices', 'inverse']"
5,Showing injective property of derivative map over vector space of polynomials,Showing injective property of derivative map over vector space of polynomials,,"From S.L Linear Algebra: (1) Let $P_n$ be the vector space of polynomials of degree $ \leq n$ . (2) Then   the derivative $D: P_n \rightarrow P_n$ is a linear map of $P_n$ into   itself. (3) Let $I$ be the identity mapping. Prove that the following linear maps are invertible: (a) $I - D^2$ ... My observation (long): (1) I find a first sentence very interesting: Let $P_n$ be the vector space of polynomials of degree $ \leq n$ . But in order for $P_n$ to be a vector space, it must contain a zero vector, which in vector space of polynomials is given by a zero polynomial which has a very confusing degree . But it is argued that it makes most sense for zero polynomial to have $-\infty$ degree. Hence wouldn't it be more precise to say that $P_n$ is vector space of polynomials that have a degree of $\leq -\infty$ ? In which case $n= -\infty$ ? I don't believe $n$ is associated in any way with dimension of vector space, because in this case this assertion would break rule of cardinality. (2) Then the derivative $D: P_n \rightarrow P_n$ is a linear map of $P_n$ into   itself. This is another interesting assertion. We know that exponent property of derivative will change degree of $p_g \in P_n$ to $g-1$ . In this case, if our polynomial is something like $x^{-\infty + 1}$ , we would get a derivative $\frac{d}{dx} x^{-\infty + 1} = -\infty x^{-\infty}$ which suggests that kernel is not trivial for $D$ and hence our linear derivative map over $P_n$ to $P_n$ is not injective ... And therefore not invertible. I definitely am wrong with this assertion, since it is assumed that $D$ is invertible over $P_n \rightarrow P_n$ (3) Let $I$ be the identity mapping. It is very easy to prove that identity map is both injective and surjective therefore invertible. Since considering that: $I(v) = v$ We can easily see that $Im(I) = P_n$ and $Ker(I) = {0}$ . Polynomials being scalars : Considering that polynomials are scalars, isn't basis of vector space zero-dimensional? Therefore by rank-nullity theorem: $$\textrm{dim} \, P_n = \textrm{dim} \, Im(P_n) + \textrm{dim}  \, Ker(P_n)$$ $$0 = \textrm{dim} \, Im(P_n) + \textrm{dim}  \, Ker(P_n)$$ $$0 = 0 + 0$$ Therefore by this assumption, kernel is trivial and linear map must be invertible. Question: In this case, I'm trying to find that there exists a matrix $A$ such that: $$(I - D^2)A = I$$ I know that identity map is invertible (as mentioned in my observation), but I'm not so sure about $D^2$ . In fact I'm unable to find a matrix associated with $D$ in order to prove my assertion. What could be the simple solution? Is my observation very incorrect? Thank you!","From S.L Linear Algebra: (1) Let be the vector space of polynomials of degree . (2) Then   the derivative is a linear map of into   itself. (3) Let be the identity mapping. Prove that the following linear maps are invertible: (a) ... My observation (long): (1) I find a first sentence very interesting: Let be the vector space of polynomials of degree . But in order for to be a vector space, it must contain a zero vector, which in vector space of polynomials is given by a zero polynomial which has a very confusing degree . But it is argued that it makes most sense for zero polynomial to have degree. Hence wouldn't it be more precise to say that is vector space of polynomials that have a degree of ? In which case ? I don't believe is associated in any way with dimension of vector space, because in this case this assertion would break rule of cardinality. (2) Then the derivative is a linear map of into   itself. This is another interesting assertion. We know that exponent property of derivative will change degree of to . In this case, if our polynomial is something like , we would get a derivative which suggests that kernel is not trivial for and hence our linear derivative map over to is not injective ... And therefore not invertible. I definitely am wrong with this assertion, since it is assumed that is invertible over (3) Let be the identity mapping. It is very easy to prove that identity map is both injective and surjective therefore invertible. Since considering that: We can easily see that and . Polynomials being scalars : Considering that polynomials are scalars, isn't basis of vector space zero-dimensional? Therefore by rank-nullity theorem: Therefore by this assumption, kernel is trivial and linear map must be invertible. Question: In this case, I'm trying to find that there exists a matrix such that: I know that identity map is invertible (as mentioned in my observation), but I'm not so sure about . In fact I'm unable to find a matrix associated with in order to prove my assertion. What could be the simple solution? Is my observation very incorrect? Thank you!","P_n  \leq n D: P_n \rightarrow P_n P_n I I - D^2 P_n  \leq n P_n -\infty P_n \leq -\infty n= -\infty n D: P_n \rightarrow P_n P_n p_g \in P_n g-1 x^{-\infty + 1} \frac{d}{dx} x^{-\infty + 1} = -\infty x^{-\infty} D P_n P_n D P_n \rightarrow P_n I I(v) = v Im(I) = P_n Ker(I) = {0} \textrm{dim} \, P_n = \textrm{dim} \, Im(P_n) + \textrm{dim}  \, Ker(P_n) 0 = \textrm{dim} \, Im(P_n) + \textrm{dim}  \, Ker(P_n) 0 = 0 + 0 A (I - D^2)A = I D^2 D","['linear-algebra', 'matrices', 'derivatives', 'polynomials']"
6,"Are ""eigenspace"" and ""null space"" ever *not* synonymous?","Are ""eigenspace"" and ""null space"" ever *not* synonymous?",,"Both the null space and the eigenspace are defined to be ""the set of all eigenvectors and the zero vector"". They have the same definition and are thus the same. Is there ever a scenario where the null space is not the same as the eigenspace (i.e., there is at least one vector in one but not in the other)?","Both the null space and the eigenspace are defined to be ""the set of all eigenvectors and the zero vector"". They have the same definition and are thus the same. Is there ever a scenario where the null space is not the same as the eigenspace (i.e., there is at least one vector in one but not in the other)?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
7,Vector valued function derivative with matrix,Vector valued function derivative with matrix,,"Given a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^n$ and a matrix $A \in \mathbb{R}^{n \times n}$ . Is there a general formula for calculating the following derivative: $$ \frac{\partial}{\partial x} f(x)^T A f(x) \tag{1} = ? $$ I know that $$ \frac{\partial}{\partial x} x^T A x = x^T(A + A^T)  \overset{A = A^T}{=} 2 x^T A \tag{2} $$ and the solution to $(1)$ will probably look similar to $(2)$ , but I am stuck here since I am not sure how to apply the chain rule in the matrix case. Edit: Regarding notation, we have $$ \frac{\partial }{\partial x}f(x) = \begin{bmatrix} \frac{\partial}{\partial x_1} f_1(x) & \frac{\partial}{\partial x_2} f_1(x) & \cdots & \frac{\partial}{\partial x_n} f_1(x) \\ \frac{\partial}{\partial x_1} f_2(x) & \frac{\partial}{\partial x_2} f_2(x) & \cdots & \frac{\partial}{\partial x_n} f_2(x) \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial}{\partial x_1} f_n(x) & \frac{\partial}{\partial x_2} f_n(x) & \cdots & \frac{\partial}{\partial x_n} f_n(x)  \end{bmatrix} $$ and $$ x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} , f(x) = \begin{bmatrix} f_1(x) \\ f_2(x) \\ \vdots \\ f_n(x) \end{bmatrix} $$","Given a function and a matrix . Is there a general formula for calculating the following derivative: I know that and the solution to will probably look similar to , but I am stuck here since I am not sure how to apply the chain rule in the matrix case. Edit: Regarding notation, we have and","f: \mathbb{R}^n \rightarrow \mathbb{R}^n A \in \mathbb{R}^{n \times n} 
\frac{\partial}{\partial x} f(x)^T A f(x) \tag{1} = ?
 
\frac{\partial}{\partial x} x^T A x = x^T(A + A^T)  \overset{A = A^T}{=} 2 x^T A \tag{2}
 (1) (2) 
\frac{\partial }{\partial x}f(x) = \begin{bmatrix} \frac{\partial}{\partial x_1} f_1(x) & \frac{\partial}{\partial x_2} f_1(x) & \cdots & \frac{\partial}{\partial x_n} f_1(x) \\
\frac{\partial}{\partial x_1} f_2(x) & \frac{\partial}{\partial x_2} f_2(x) & \cdots & \frac{\partial}{\partial x_n} f_2(x) \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial}{\partial x_1} f_n(x) & \frac{\partial}{\partial x_2} f_n(x) & \cdots & \frac{\partial}{\partial x_n} f_n(x)  \end{bmatrix}
 
x = \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} ,
f(x) = \begin{bmatrix}
f_1(x) \\
f_2(x) \\
\vdots \\
f_n(x)
\end{bmatrix}
","['calculus', 'matrices', 'derivatives', 'matrix-calculus']"
8,"How many square matrices are there with given columns, rows and diagonals","How many square matrices are there with given columns, rows and diagonals",,"Suppose we have $n\times n$ matrix that contains numbers from $1$ to $n^2$ . How many matrices are there that their $n$ columns, $n$ rows and $2n$ diagonals contain given numbers? For example $3\times 3$ matrix: \begin{pmatrix}  1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \\ \end{pmatrix} Its rows are $(1,2,3),(4,5,6),(7,8,9)$ . Its columns are $(1,4,7),(2,5,8),(3,6,9)$ . Its diagonals are $(1,5,9),(2,6,7),(3,4,8),(1,6,8),(2,4,9),(3,5,7)$ . If we join them all together sorted inside and sorted overall we get: $$\big((1,2,3),(1,4,7),(1,5,9),(1,6,8),(2,4,9),(2,5,8),(2,6,7),$$ $$(3,4,8),(3,5,7),(3,6,9),(4,5,6),(7,8,9)\big)$$ Question is how many $3\times 3$ matrices there are with the same sorted set? I brute-forced it for $n=3$ . And the number is $432$ . But I am not able to figure out a formula for it. The number $432$ is divisible by $9$ which is of course because the original matrix can be shifted to $3\cdot 3=9$ different positions preserving the ordered set. And also by $16$ as we can rotate whole matrix $4$ times by $90$ degrees and there are $4$ mirror images for each one. $432/9/16=3$ so there must be $3$ fundamental matrices from which we can generate all $432$ by shifting, rotating and mirroring. (If I am not mistaken.) Just one example out of $432$ that has the same sorted set as original matrix: \begin{pmatrix}  4 & 2 & 9 \\  1 & 8 & 6 \\  7 & 5 & 3 \\ \end{pmatrix} ...and perhaps additional question: How to produce all the required matrices algorithmically one by one?","Suppose we have matrix that contains numbers from to . How many matrices are there that their columns, rows and diagonals contain given numbers? For example matrix: Its rows are . Its columns are . Its diagonals are . If we join them all together sorted inside and sorted overall we get: Question is how many matrices there are with the same sorted set? I brute-forced it for . And the number is . But I am not able to figure out a formula for it. The number is divisible by which is of course because the original matrix can be shifted to different positions preserving the ordered set. And also by as we can rotate whole matrix times by degrees and there are mirror images for each one. so there must be fundamental matrices from which we can generate all by shifting, rotating and mirroring. (If I am not mistaken.) Just one example out of that has the same sorted set as original matrix: ...and perhaps additional question: How to produce all the required matrices algorithmically one by one?","n\times n 1 n^2 n n 2n 3\times 3 \begin{pmatrix} 
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \\
\end{pmatrix} (1,2,3),(4,5,6),(7,8,9) (1,4,7),(2,5,8),(3,6,9) (1,5,9),(2,6,7),(3,4,8),(1,6,8),(2,4,9),(3,5,7) \big((1,2,3),(1,4,7),(1,5,9),(1,6,8),(2,4,9),(2,5,8),(2,6,7), (3,4,8),(3,5,7),(3,6,9),(4,5,6),(7,8,9)\big) 3\times 3 n=3 432 432 9 3\cdot 3=9 16 4 90 4 432/9/16=3 3 432 432 \begin{pmatrix}
 4 & 2 & 9 \\
 1 & 8 & 6 \\
 7 & 5 & 3 \\
\end{pmatrix}","['combinatorics', 'matrices', 'combinations']"
9,Checking if matrix $A$ is positive definite via Cholesky decomposition,Checking if matrix  is positive definite via Cholesky decomposition,A,How can we show that a matrix $A$ is not a positive definite matrix using the Cholesky decomposition? If we are not able to complete the algorithm and we cannot factor the matrix with a Cholesky decomposition does it then mean that the matrix is not positiv definite? Or is there an other way to check whether the matrix is positiv definite or not?,How can we show that a matrix is not a positive definite matrix using the Cholesky decomposition? If we are not able to complete the algorithm and we cannot factor the matrix with a Cholesky decomposition does it then mean that the matrix is not positiv definite? Or is there an other way to check whether the matrix is positiv definite or not?,A,"['matrices', 'matrix-decomposition', 'positive-definite', 'cholesky-decomposition']"
10,"If $(A+B)^n$ is binomial for some $n$, does that imply $AB = BA$?","If  is binomial for some , does that imply ?",(A+B)^n n AB = BA,"We say that a matrix power $(A+B)^n$ is binomial iff it satisfies the matrix equality $$(A+B)^n = \sum \limits_{j\,=\,0}^n \binom{n}{j}A^jB^{n-j}.$$ If two matrices have a binomial power for some $n$ , does that imply that $AB = BA$ ? My approach was the most obvious (I think):  I tried to find a non-commutative expression for $(A+B)^n$ to finally get all those ugly permutations with the same degree equal to something that adds up to the term with the respective degree on the other side, but they could ""chaotically"" add up to that without being commutative, so it's not interesting. I also tried to use a ""inductive-like"" thinking to get the simplest case of commutativity, i.e., the case where $n=2$ , so, by the Euclidean algorithm, $n = 2q + r$ . I didn't manage to advance much, though.","We say that a matrix power is binomial iff it satisfies the matrix equality If two matrices have a binomial power for some , does that imply that ? My approach was the most obvious (I think):  I tried to find a non-commutative expression for to finally get all those ugly permutations with the same degree equal to something that adds up to the term with the respective degree on the other side, but they could ""chaotically"" add up to that without being commutative, so it's not interesting. I also tried to use a ""inductive-like"" thinking to get the simplest case of commutativity, i.e., the case where , so, by the Euclidean algorithm, . I didn't manage to advance much, though.","(A+B)^n (A+B)^n = \sum \limits_{j\,=\,0}^n \binom{n}{j}A^jB^{n-j}. n AB = BA (A+B)^n n=2 n = 2q + r","['linear-algebra', 'matrices', 'algebra-precalculus', 'binomial-theorem']"
11,Is echelon form a requirement to show a matrix has no solutions?,Is echelon form a requirement to show a matrix has no solutions?,,"Do I need to get a matrix into echelon form to prove that it has no solutions, or do I only need a pivot in the last column. For example, would the following row operation show that there are no solutions to the linear system represented that by the augmented matrix below? $\begin{bmatrix}1&-3&0&5\\-1&1&5&2\\-1&1&5&3\end{bmatrix}$ $\Rightarrow$ $\begin{bmatrix}1&-3&0&5\\-1&1&5&2\\0&0&0&1\end{bmatrix}$","Do I need to get a matrix into echelon form to prove that it has no solutions, or do I only need a pivot in the last column. For example, would the following row operation show that there are no solutions to the linear system represented that by the augmented matrix below?",\begin{bmatrix}1&-3&0&5\\-1&1&5&2\\-1&1&5&3\end{bmatrix} \Rightarrow \begin{bmatrix}1&-3&0&5\\-1&1&5&2\\0&0&0&1\end{bmatrix},"['linear-algebra', 'matrices']"
12,Prove that the eigenvalues of a product of positive semidefinite matrices are nonnegative,Prove that the eigenvalues of a product of positive semidefinite matrices are nonnegative,,"Show that if $A,B \in \mathcal M_n(\mathbb{R})$ are positive semidefinite and $\lambda$ is an eigenvalue of $AB$ then $\lambda \geq 0$. I don't really know what to do here. If $AB$ was semidefinite positive we would be done but the product of positive semidefinite matrices doesn't have to be positive semidefinite itself.","Show that if $A,B \in \mathcal M_n(\mathbb{R})$ are positive semidefinite and $\lambda$ is an eigenvalue of $AB$ then $\lambda \geq 0$. I don't really know what to do here. If $AB$ was semidefinite positive we would be done but the product of positive semidefinite matrices doesn't have to be positive semidefinite itself.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-semidefinite']"
13,complex and real spectral theorem for matrices,complex and real spectral theorem for matrices,,"I am studying the spectral theorem for matrices, and the book says that if a $nxn$ matrix A is real and symmetric then its diagonalizable over $\mathbb{R}$. And that this fact is a corollary of the Spectral Theorem for the complex case of normal matrices. Although I agree that since $A$ is symmetric then $A$ is normal hence it implies that $A$ is diagonalizable over $\mathbb{C}$, and moreover it is easy to prove that all eigenvectors are real. But how can I see that all eigenvectors are also real? Thanks in advance.","I am studying the spectral theorem for matrices, and the book says that if a $nxn$ matrix A is real and symmetric then its diagonalizable over $\mathbb{R}$. And that this fact is a corollary of the Spectral Theorem for the complex case of normal matrices. Although I agree that since $A$ is symmetric then $A$ is normal hence it implies that $A$ is diagonalizable over $\mathbb{C}$, and moreover it is easy to prove that all eigenvectors are real. But how can I see that all eigenvectors are also real? Thanks in advance.",,"['matrices', 'eigenvalues-eigenvectors', 'spectral-theory']"
14,How does one calculate $A^n$ when $A$ can't be diagonalized?,How does one calculate  when  can't be diagonalized?,A^n A,"I wasn't able to find a straightforward answer to this online. It is straightforward when you can diagonalize the matrix but how is the non-diagonalizeable case handled? The $3 \times 3$ case is the most relevant to me, and I will have to do this using pen paper so I am looking for solutions that are easy to do manually.","I wasn't able to find a straightforward answer to this online. It is straightforward when you can diagonalize the matrix but how is the non-diagonalizeable case handled? The $3 \times 3$ case is the most relevant to me, and I will have to do this using pen paper so I am looking for solutions that are easy to do manually.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
15,"Without using the Rule of Sarrus, prove that:","Without using the Rule of Sarrus, prove that:",,"Without using the Rule of Sarrus, prove that: $$\left| \begin{matrix} (b+c)&(a-b)&a \\ (c+a)&(b-c)&b \\ (a+b)&(c-a)&c \\ \end{matrix}\right|=3abc-a^3-b^3-c^3$$ My Approach: $$LHS=  \left| \begin{matrix} (b+c)&(a-b)&a \\ (c+a)&(b-c)&b \\ (a+b)&(c-a)&c \\ \end{matrix}\right|$$ $$C_1\to C_1+C_2$$ $$= \left| \begin{matrix} (c+a)&(a-b)&a \\ (a+b)&(b-c)&b \\ (b+c)&(c-a)&c \\ \end{matrix}\right|$$ $$C_1\to C_1-C_3$$ $$= \left| \begin{matrix} c&(a-b)&a \\ a&(b-c)&b \\ b&(c-a)&c \\ \end{matrix}\right|$$ How do I complete the rest?","Without using the Rule of Sarrus, prove that: $$\left| \begin{matrix} (b+c)&(a-b)&a \\ (c+a)&(b-c)&b \\ (a+b)&(c-a)&c \\ \end{matrix}\right|=3abc-a^3-b^3-c^3$$ My Approach: $$LHS=  \left| \begin{matrix} (b+c)&(a-b)&a \\ (c+a)&(b-c)&b \\ (a+b)&(c-a)&c \\ \end{matrix}\right|$$ $$C_1\to C_1+C_2$$ $$= \left| \begin{matrix} (c+a)&(a-b)&a \\ (a+b)&(b-c)&b \\ (b+c)&(c-a)&c \\ \end{matrix}\right|$$ $$C_1\to C_1-C_3$$ $$= \left| \begin{matrix} c&(a-b)&a \\ a&(b-c)&b \\ b&(c-a)&c \\ \end{matrix}\right|$$ How do I complete the rest?",,"['linear-algebra', 'matrices', 'determinant']"
16,prove that for any nonsingular matrix $A$ there exist $X$ such that $X^2=A$,prove that for any nonsingular matrix  there exist  such that,A X X^2=A,"Prove that given any  matrix A, where $$\det(A)\neq0$$ $$A\in M_{n,n}(\mathbb C)$$ the following equation $$X^2=A$$ always has a solution. Should I do something with Jordan Normal form? Any help will be appreciated","Prove that given any  matrix A, where $$\det(A)\neq0$$ $$A\in M_{n,n}(\mathbb C)$$ the following equation $$X^2=A$$ always has a solution. Should I do something with Jordan Normal form? Any help will be appreciated",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
17,. There exists a nondiagonal matrix $A\in \mathcal {M}_n (\mathbb{R}) $ s.t. $A^{k+1}=I_n $ and $I_n-A $ invertible?,. There exists a nondiagonal matrix  s.t.  and  invertible?,A\in \mathcal {M}_n (\mathbb{R})  A^{k+1}=I_n  I_n-A ,Let $k\in \mathbb {N} $. There exists a nondiagonal  matrix $A\in \mathcal {M}_n (\mathbb{R}) $ s.t. $A^{k+1}=I_n $ and  $I_n-A $ invertible?,Let $k\in \mathbb {N} $. There exists a nondiagonal  matrix $A\in \mathcal {M}_n (\mathbb{R}) $ s.t. $A^{k+1}=I_n $ and  $I_n-A $ invertible?,,"['linear-algebra', 'abstract-algebra', 'matrices']"
18,Finding a basis for an eigenspace? Where do the additional vectors come from?,Finding a basis for an eigenspace? Where do the additional vectors come from?,,"I want to diagonalize the following matrix: $$A =\begin{bmatrix}     1&0&0&0 \\     0&0&1&0 \\     0&1&0&0\\     0&0&0&1 \end{bmatrix}$$ And I get the following characteristic equation: $(\lambda - 1)^3(\lambda +1)$ and therefore $\lambda=1,-1$ Firstly get the basis vectors for $\lambda = 1$ $$(A - \lambda_2 I_4) =  \begin{bmatrix}     0&0&0&0 \\     0&-1&1&0 \\     0&1&-1&0\\     0&0&0&0 \end{bmatrix} \rightarrow \text{row reduce} \rightarrow \begin{bmatrix}     0&1&-1&0 \\     0&0&0&0 \\     0&0&0&0\\     0&0&0&0 \end{bmatrix}$$ So I have $x_2 = x_3$ wich yields the vector: $$v_1 = \begin{bmatrix}     0 \\     1 \\     1\\     0 \end{bmatrix}$$ However there should be $3$ basis vectors total for $\lambda = 1$ according to the characteristic equation. The answer key gives vectors: $$\begin{bmatrix}     1 \\     0 \\     0\\     0 \end{bmatrix} , \begin{bmatrix}     0 \\     0 \\     0\\     1 \end{bmatrix}$$ to be the other basis vectors for $\lambda = 1$, but where do these vectors come from in regards to my reduced matrix? I understand there ought to be 2 additional vectors to the one that I found but why are they the vectors given?","I want to diagonalize the following matrix: $$A =\begin{bmatrix}     1&0&0&0 \\     0&0&1&0 \\     0&1&0&0\\     0&0&0&1 \end{bmatrix}$$ And I get the following characteristic equation: $(\lambda - 1)^3(\lambda +1)$ and therefore $\lambda=1,-1$ Firstly get the basis vectors for $\lambda = 1$ $$(A - \lambda_2 I_4) =  \begin{bmatrix}     0&0&0&0 \\     0&-1&1&0 \\     0&1&-1&0\\     0&0&0&0 \end{bmatrix} \rightarrow \text{row reduce} \rightarrow \begin{bmatrix}     0&1&-1&0 \\     0&0&0&0 \\     0&0&0&0\\     0&0&0&0 \end{bmatrix}$$ So I have $x_2 = x_3$ wich yields the vector: $$v_1 = \begin{bmatrix}     0 \\     1 \\     1\\     0 \end{bmatrix}$$ However there should be $3$ basis vectors total for $\lambda = 1$ according to the characteristic equation. The answer key gives vectors: $$\begin{bmatrix}     1 \\     0 \\     0\\     0 \end{bmatrix} , \begin{bmatrix}     0 \\     0 \\     0\\     1 \end{bmatrix}$$ to be the other basis vectors for $\lambda = 1$, but where do these vectors come from in regards to my reduced matrix? I understand there ought to be 2 additional vectors to the one that I found but why are they the vectors given?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'matrix-decomposition']"
19,Any relation between the singular values of ${\bf A}$ and ${\bf I} - {\bf A}$,Any relation between the singular values of  and,{\bf A} {\bf I} - {\bf A},"Is there any relation between the singular values of ${\bf A}$ and ${\bf I} - {\bf A}$? When $\bf A$ is Hermitian, then the singular values of $\bf A$ is just it eigenvalues, and ${\bf I} - {\bf A}$ has its singular values being $1$ minus those eigenvalues. Is there any relation when $\bf A$ is not Hermitian? Thanks! For readers of this post, see also If the absolute value of every eigenvalue of a matrix is smaller than 1, is the maximum singular value smaller than 1? .","Is there any relation between the singular values of ${\bf A}$ and ${\bf I} - {\bf A}$? When $\bf A$ is Hermitian, then the singular values of $\bf A$ is just it eigenvalues, and ${\bf I} - {\bf A}$ has its singular values being $1$ minus those eigenvalues. Is there any relation when $\bf A$ is not Hermitian? Thanks! For readers of this post, see also If the absolute value of every eigenvalue of a matrix is smaller than 1, is the maximum singular value smaller than 1? .",,"['linear-algebra', 'matrices']"
20,Given the invertible matrix $A$ so that $A+A^{-1}=2I_n$,Given the invertible matrix  so that,A A+A^{-1}=2I_n,"Given the invertible matrix $A$ so that $A+A^{-1}=2I_n$, which of the following equalities stand true? 1)$A=3I_n$ 2)$A^3+A^{-3}=2I_n$ 3)$A=-A$ 4)$A^2+A^{-2}=I_n$ 5)$A-A^{-1}=2I_n$ I know the formula for $A^{-1}$, but I'm not sure if and how should I use it  here or what else should I apply. Could I have some hints on how to approach this? Thank you","Given the invertible matrix $A$ so that $A+A^{-1}=2I_n$, which of the following equalities stand true? 1)$A=3I_n$ 2)$A^3+A^{-3}=2I_n$ 3)$A=-A$ 4)$A^2+A^{-2}=I_n$ 5)$A-A^{-1}=2I_n$ I know the formula for $A^{-1}$, but I'm not sure if and how should I use it  here or what else should I apply. Could I have some hints on how to approach this? Thank you",,"['matrices', 'inverse']"
21,From nilpotent matrix to Jordan matrix,From nilpotent matrix to Jordan matrix,,"Let's say I have a matrix $A\in M_{n\times n}(\mathbb F)$, and it's given that $A$ is a nilpotent matrix. How Can I find a Jordan matrix that is similar to $A$? because $A$ is nilpotent, I can find a basis $B$ s.t. $A$ represented by the basis $B$ is a Jordan matrix with eigenvalue $0$. How can I proceed from here?","Let's say I have a matrix $A\in M_{n\times n}(\mathbb F)$, and it's given that $A$ is a nilpotent matrix. How Can I find a Jordan matrix that is similar to $A$? because $A$ is nilpotent, I can find a basis $B$ s.t. $A$ represented by the basis $B$ is a Jordan matrix with eigenvalue $0$. How can I proceed from here?",,"['linear-algebra', 'matrices', 'jordan-normal-form', 'nilpotence']"
22,if $A$ is Real skew Symmetric Matrix Then Prove that $I-A$ is Non singular,if  is Real skew Symmetric Matrix Then Prove that  is Non singular,A I-A,if $A$ is Real skew Symmetric Matrix Then Prove that $I-A$ is Non singular. i have taken $2 \times 2$ matrix and proved it but is there a formal way to prove it?,if $A$ is Real skew Symmetric Matrix Then Prove that $I-A$ is Non singular. i have taken $2 \times 2$ matrix and proved it but is there a formal way to prove it?,,"['linear-algebra', 'matrices', 'determinant']"
23,Prove that $\|UVU^{-1}V^{-1}-I\|\leq 2\|U-I\|\|V-I\|$,Prove that,\|UVU^{-1}V^{-1}-I\|\leq 2\|U-I\|\|V-I\|,"$U,V$ are unitary $n\times n$ matrices, and the norm is the operator norm (so we can use $\|UV\|\leq\|U\|\|V\|$). I've noticed that \begin{align} \|UVU^{-1}V^{-1}-I\|&= \|(UV-VU)U^{-1}V^{-1}\|\\ &\leq \|UV-VU\|\|U^{-1}V^{-1}\| \end{align} I can bound the first term by $\|UV\|+\|VU\|$, but I don't think this is useful. Hints (rather than complete answers) would be appreciated. The question comes from here (exercise 1)","$U,V$ are unitary $n\times n$ matrices, and the norm is the operator norm (so we can use $\|UV\|\leq\|U\|\|V\|$). I've noticed that \begin{align} \|UVU^{-1}V^{-1}-I\|&= \|(UV-VU)U^{-1}V^{-1}\|\\ &\leq \|UV-VU\|\|U^{-1}V^{-1}\| \end{align} I can bound the first term by $\|UV\|+\|VU\|$, but I don't think this is useful. Hints (rather than complete answers) would be appreciated. The question comes from here (exercise 1)",,"['matrices', 'operator-theory', 'normed-spaces', 'matrix-equations']"
24,Show that orthogonal matrices have eigenvalues with magnitude $1$ without a sesquilinear inner product,Show that orthogonal matrices have eigenvalues with magnitude  without a sesquilinear inner product,1,"Is it possible to consider complex eigenvalues without a Hermitian (i.e. sesquilinear) inner product over a complex vector space? For instance: let $A$ be a real orthogonal matrix (so $A^TA = I$).  Without referencing a Hermitian inner product, is it possible to show that the complex eigenvalues of $A$ have magnitude $1$? The usual proof of this fact is as follows: if $x,\lambda$ is an eigenpair of $A$, then we have $$ \|x\|^2 = x^*x = x^*(A^*A)x = (Ax)^*(Ax) = \lambda\overline{\lambda} (x^*x) = |\lambda|^2 \|x\|^2 $$ from which it follows that $|\lambda| = 1$.  Note: this proof required the use of the sesquilinear inner product $\langle x,y \rangle = y^*x$. A rephrasing of the original question: consider $\Bbb C^n$ with the bilinear from  $$ \langle x,y \rangle = y^Tx $$ note that this bilinear form is not an inner product. The complex-orthogonal matrices are those matrices $A$ that satisfy $A^TA = I$, where $T$ is the entrywise transpose.  Notably, the complex-orthogonal matrices preserve the above bilinear form.  How can we show that if $A$ is complex-orthogonal with real entries, then the eigenvalues of $A$ have magnitude $1$?","Is it possible to consider complex eigenvalues without a Hermitian (i.e. sesquilinear) inner product over a complex vector space? For instance: let $A$ be a real orthogonal matrix (so $A^TA = I$).  Without referencing a Hermitian inner product, is it possible to show that the complex eigenvalues of $A$ have magnitude $1$? The usual proof of this fact is as follows: if $x,\lambda$ is an eigenpair of $A$, then we have $$ \|x\|^2 = x^*x = x^*(A^*A)x = (Ax)^*(Ax) = \lambda\overline{\lambda} (x^*x) = |\lambda|^2 \|x\|^2 $$ from which it follows that $|\lambda| = 1$.  Note: this proof required the use of the sesquilinear inner product $\langle x,y \rangle = y^*x$. A rephrasing of the original question: consider $\Bbb C^n$ with the bilinear from  $$ \langle x,y \rangle = y^Tx $$ note that this bilinear form is not an inner product. The complex-orthogonal matrices are those matrices $A$ that satisfy $A^TA = I$, where $T$ is the entrywise transpose.  Notably, the complex-orthogonal matrices preserve the above bilinear form.  How can we show that if $A$ is complex-orthogonal with real entries, then the eigenvalues of $A$ have magnitude $1$?",,"['linear-algebra', 'matrices', 'alternative-proof']"
25,group generated by matrices of finite order,group generated by matrices of finite order,,"Let $K$ be a field, and suppose $A,B$ are $n{\,\times\,}n$ matrices with coefficients in $K$ such that  $$A^p =I\;\;\;\text{and}\;\;\;B^q=I$$ for some positive integers $p,q$. Let $G$ be the multiplicative group of matrices generated by $\{A,B\}$. Two questions: $\;$Must $G$ be finite? $\;$Assuming $G$ is finite, must $G$ be isomorphic to a subgroup of $S_m$, where $m = \text{lcm}(p,q)$? Since it appears that questions $(1)$ and $(2)$ both got quick ""no"" answers, I'll add one more question, a variant of question $(1)$: $\;\;\;$3. What if $A,B$ also satisfy some multiplicative identity not algebraically derivable from $A^p=I$ and $B^q=I$. In other words, some word in $A,B,A^{-1},B^{-1}$ is equal to $I$. Now must $G$ be finite? Based on the answers already given for questions $(1)$ and $(2)$, I'm not optimistic about the chances for a ""yes"" answer to question $(3)$. Thanks for the responses so far.","Let $K$ be a field, and suppose $A,B$ are $n{\,\times\,}n$ matrices with coefficients in $K$ such that  $$A^p =I\;\;\;\text{and}\;\;\;B^q=I$$ for some positive integers $p,q$. Let $G$ be the multiplicative group of matrices generated by $\{A,B\}$. Two questions: $\;$Must $G$ be finite? $\;$Assuming $G$ is finite, must $G$ be isomorphic to a subgroup of $S_m$, where $m = \text{lcm}(p,q)$? Since it appears that questions $(1)$ and $(2)$ both got quick ""no"" answers, I'll add one more question, a variant of question $(1)$: $\;\;\;$3. What if $A,B$ also satisfy some multiplicative identity not algebraically derivable from $A^p=I$ and $B^q=I$. In other words, some word in $A,B,A^{-1},B^{-1}$ is equal to $I$. Now must $G$ be finite? Based on the answers already given for questions $(1)$ and $(2)$, I'm not optimistic about the chances for a ""yes"" answer to question $(3)$. Thanks for the responses so far.",,"['matrices', 'group-theory', 'permutations']"
26,"If $\mathbf{AA}^T=\mathbf{I}$, is $\mathbf A$ necessarily square?","If , is  necessarily square?",\mathbf{AA}^T=\mathbf{I} \mathbf A,"If $\mathbf{AA}^T=\mathbf{I}$, is $\mathbf A$ necessarily square? I am starting to learn about matrices, and had the above question. When I have tried to think about this, I have not been able to progress using matrix multiplication, since $\textbf{A}$ and its transpose do not have inverses unless they are square. The only conclusion I could come to using matrix multiplication is that the product of a matrix and its transpose, whatever the dimensions, is square and symmetric. I also tried to consider this component-wise; for a 1x3 case, it was easy to see that there are no solutions. But the algebra for a 2x3 case was quite messy because it involved 6 variables. I am not sure how else to think about this. I have seen the proofs that a matrix must be square to have and inverse ( here ), but the answers all rely on the additional defining property of an inverse being that $\textbf{AA}^{-1}=\textbf{A}^{-1}\textbf{A}$, and if $\textbf{A}$ was not square, $\textbf{AA}^{-1}$ could theoretically be equal to $\textbf{I}$ but then it would not have the same dimensions as $\textbf{A}^{-1}\textbf{A}$, violating the above property. As a similar constraint is applied to orthogonal matrices, these qould also have to be square. However is it possible for a non-square matrix to be such that  $\textbf{AA}^T=\textbf{I}$, whether or not $\textbf{A^TA}=\textbf{I}$, where the identity matrix here could be of a different dimension? If so, does $\textbf{AA}^T=\textbf{I}$ mandate that $\textbf{A}^T\textbf{A}=\textbf{I}$?","If $\mathbf{AA}^T=\mathbf{I}$, is $\mathbf A$ necessarily square? I am starting to learn about matrices, and had the above question. When I have tried to think about this, I have not been able to progress using matrix multiplication, since $\textbf{A}$ and its transpose do not have inverses unless they are square. The only conclusion I could come to using matrix multiplication is that the product of a matrix and its transpose, whatever the dimensions, is square and symmetric. I also tried to consider this component-wise; for a 1x3 case, it was easy to see that there are no solutions. But the algebra for a 2x3 case was quite messy because it involved 6 variables. I am not sure how else to think about this. I have seen the proofs that a matrix must be square to have and inverse ( here ), but the answers all rely on the additional defining property of an inverse being that $\textbf{AA}^{-1}=\textbf{A}^{-1}\textbf{A}$, and if $\textbf{A}$ was not square, $\textbf{AA}^{-1}$ could theoretically be equal to $\textbf{I}$ but then it would not have the same dimensions as $\textbf{A}^{-1}\textbf{A}$, violating the above property. As a similar constraint is applied to orthogonal matrices, these qould also have to be square. However is it possible for a non-square matrix to be such that  $\textbf{AA}^T=\textbf{I}$, whether or not $\textbf{A^TA}=\textbf{I}$, where the identity matrix here could be of a different dimension? If so, does $\textbf{AA}^T=\textbf{I}$ mandate that $\textbf{A}^T\textbf{A}=\textbf{I}$?",,"['matrices', 'inverse', 'orthogonality']"
27,How should the order of application of rotation transformation be interpreted (in PowerPoint)?,How should the order of application of rotation transformation be interpreted (in PowerPoint)?,,"For a university assignment, I have a question about rotating a picture in PowerPoint. In PowerPoint, a picture can have four transformations. Rotate right (90°), rotate left (90°), flip horizontally and flip vertically. Let's call them $R(r)$, $R(l)$, $F(v)$ and $F(h)$ for short. We are then asked to compute the matrix multiplication for every pair of possible transformations. For example, $R(r)*R(r)$ or $R(r)*F(h)$. All of that seems fairly simple to me, but they specified something in the question I don't understand. $R(r)*R(l)$ is read from right to left as ""rotate left"" then ""rotate   the result right"", because Powerpoint transformations are performed in world coordinates . Can someone explain to me why I would read that left to right? And does that mean when I multiply the matrices together I would, in fact, do $R(l)*R(r)$ when asked to do $R(r)*R(l)$? Why would world co-ordinates effect this? Would I read all transformations as left to right or just some?","For a university assignment, I have a question about rotating a picture in PowerPoint. In PowerPoint, a picture can have four transformations. Rotate right (90°), rotate left (90°), flip horizontally and flip vertically. Let's call them $R(r)$, $R(l)$, $F(v)$ and $F(h)$ for short. We are then asked to compute the matrix multiplication for every pair of possible transformations. For example, $R(r)*R(r)$ or $R(r)*F(h)$. All of that seems fairly simple to me, but they specified something in the question I don't understand. $R(r)*R(l)$ is read from right to left as ""rotate left"" then ""rotate   the result right"", because Powerpoint transformations are performed in world coordinates . Can someone explain to me why I would read that left to right? And does that mean when I multiply the matrices together I would, in fact, do $R(l)*R(r)$ when asked to do $R(r)*R(l)$? Why would world co-ordinates effect this? Would I read all transformations as left to right or just some?",,"['matrices', 'linear-transformations', 'rotations', 'rigid-transformation']"
28,sum of two determinants,sum of two determinants,,"If \begin{vmatrix}a&c\\b&d\end{vmatrix} and \begin{vmatrix}a&e\\b&f\end{vmatrix}, then sum of these determinant can be written as in terms of another determinant given by     \begin{vmatrix}a&c+e\\b&d+f\end{vmatrix} is it right?","If \begin{vmatrix}a&c\\b&d\end{vmatrix} and \begin{vmatrix}a&e\\b&f\end{vmatrix}, then sum of these determinant can be written as in terms of another determinant given by     \begin{vmatrix}a&c+e\\b&d+f\end{vmatrix} is it right?",,['matrices']
29,Killing-form $\mathrm{Tr}(\mathrm{ad}_{X} \circ \mathrm{ad}_{Y})$ of $\mathfrak{so}(n)$,Killing-form  of,\mathrm{Tr}(\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}) \mathfrak{so}(n),"I'm trying to understand how to write the Killing form $B: \mathfrak{g} \times \mathfrak{g} \to \mathbb{R}$ defined by $$ B(X,Y) = \mathrm{Tr}(\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}) $$ I'm trying to do this specifically for the Lie algebra $\mathfrak{g} = \mathfrak{so}(n)$, but I'm getting stuck right at the end. I know I $should$ be getting $B(X,Y) = (n-2)\mathrm{Tr}(XY)$ at the end of the day. In my solution I try to keep things in terms of a general Lie algebra, and then specify to $\mathfrak{so}(n)$ at the end. Assume $\mathfrak{g}$ is a matrix of size $n \times n$. Since $\mathrm{ad}_{X}(A) = [X,A]$, I'm thinking of the linear transformation $ad_{X}:\mathfrak{g} \to \mathfrak{g}$ as a matrix of size $n^{2} \times n^{2}$. Since $$ \left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A) = [X,[Y,A]] = XYA - YAX - XAY + AYX $$ If $M$ is my matrix (with components $M_{(j,k)(p,q)}$) corresponding to $\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}$ in the sense that: $$ \left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A)_{jk} = \sum_{p,q=1}^{n} M_{(j,k)(p,q)} A_{pq} $$ I'm assuming then that $B(X,Y) = \mathrm{Tr}\left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y} \right) = \sum_{j,k=1}^{n} M_{(j,k),(j,k)}$. So after a little grinding, I find that: $$ \left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A)_{jk} = \sum_{p,q=1}^{n} \left[ X_{jp}Y_{pq}A_{qk} - Y_{jp}A_{pq}X_{qk} - X_{jp}A_{pq}Y_{qk} + A_{jp}Y_{pq}X_{qk} \right] \\ = \sum_{p,q=1}^{n} \left[ \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell p} \delta_{kq} + Y_{q\ell} X_{\ell k} \delta_{j p} \right) - X_{jp} Y_{qk} - Y_{jp} X_{qk} \right] A_{pq} \\ $$ Where $\delta$ is the kronecker delta. So my matrix components are: $$ M_{(j,k)(p,q)} = \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell p} \delta_{kq} + Y_{q\ell} X_{\ell k} \delta_{j p} \right) - X_{jp} Y_{qk} - Y_{jp} X_{qk} $$ Finally, this tells me that the Killing-form ends up looking like: $$ B(X,Y) = \sum_{j,k=1}^{n} M_{(j,k)(j,k)} \\ = \sum_{j,k=1}^{n} \left[ \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell j} \delta_{kk} + Y_{k\ell} X_{\ell k} \delta_{j j} \right) - X_{jj} Y_{kk} - Y_{jj} X_{kk} \right] \\ = 2n \mathrm{Tr}(XY) - 2 \mathrm{Tr}(X)\mathrm{Tr}(Y) $$ So $B(X,Y) = 2n \mathrm{Tr}(XY) - 2 \mathrm{Tr}(X)\mathrm{Tr}(Y)$, and so far I'm still working with a general Lie algebra $\mathfrak{g}$. Looking at $\mathfrak{so}(n)$, I know this this is the set of skew-symmetric real matrices of size $n \times n$, which means they are all traceless. This simplifies my answer to $B(X,Y) = 2n \mathrm{Tr}(XY)$. However, $2n \neq n-2$! Where am I going wrong in my proof? Am I taking too large of a leap in assuming that I can write the matrix M corresponding to $\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}$ so easily?","I'm trying to understand how to write the Killing form $B: \mathfrak{g} \times \mathfrak{g} \to \mathbb{R}$ defined by $$ B(X,Y) = \mathrm{Tr}(\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}) $$ I'm trying to do this specifically for the Lie algebra $\mathfrak{g} = \mathfrak{so}(n)$, but I'm getting stuck right at the end. I know I $should$ be getting $B(X,Y) = (n-2)\mathrm{Tr}(XY)$ at the end of the day. In my solution I try to keep things in terms of a general Lie algebra, and then specify to $\mathfrak{so}(n)$ at the end. Assume $\mathfrak{g}$ is a matrix of size $n \times n$. Since $\mathrm{ad}_{X}(A) = [X,A]$, I'm thinking of the linear transformation $ad_{X}:\mathfrak{g} \to \mathfrak{g}$ as a matrix of size $n^{2} \times n^{2}$. Since $$ \left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A) = [X,[Y,A]] = XYA - YAX - XAY + AYX $$ If $M$ is my matrix (with components $M_{(j,k)(p,q)}$) corresponding to $\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}$ in the sense that: $$ \left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A)_{jk} = \sum_{p,q=1}^{n} M_{(j,k)(p,q)} A_{pq} $$ I'm assuming then that $B(X,Y) = \mathrm{Tr}\left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y} \right) = \sum_{j,k=1}^{n} M_{(j,k),(j,k)}$. So after a little grinding, I find that: $$ \left( \mathrm{ad}_{X} \circ \mathrm{ad}_{Y}\right)(A)_{jk} = \sum_{p,q=1}^{n} \left[ X_{jp}Y_{pq}A_{qk} - Y_{jp}A_{pq}X_{qk} - X_{jp}A_{pq}Y_{qk} + A_{jp}Y_{pq}X_{qk} \right] \\ = \sum_{p,q=1}^{n} \left[ \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell p} \delta_{kq} + Y_{q\ell} X_{\ell k} \delta_{j p} \right) - X_{jp} Y_{qk} - Y_{jp} X_{qk} \right] A_{pq} \\ $$ Where $\delta$ is the kronecker delta. So my matrix components are: $$ M_{(j,k)(p,q)} = \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell p} \delta_{kq} + Y_{q\ell} X_{\ell k} \delta_{j p} \right) - X_{jp} Y_{qk} - Y_{jp} X_{qk} $$ Finally, this tells me that the Killing-form ends up looking like: $$ B(X,Y) = \sum_{j,k=1}^{n} M_{(j,k)(j,k)} \\ = \sum_{j,k=1}^{n} \left[ \sum_{\ell=1}^{n} \left( X_{j\ell}Y_{\ell j} \delta_{kk} + Y_{k\ell} X_{\ell k} \delta_{j j} \right) - X_{jj} Y_{kk} - Y_{jj} X_{kk} \right] \\ = 2n \mathrm{Tr}(XY) - 2 \mathrm{Tr}(X)\mathrm{Tr}(Y) $$ So $B(X,Y) = 2n \mathrm{Tr}(XY) - 2 \mathrm{Tr}(X)\mathrm{Tr}(Y)$, and so far I'm still working with a general Lie algebra $\mathfrak{g}$. Looking at $\mathfrak{so}(n)$, I know this this is the set of skew-symmetric real matrices of size $n \times n$, which means they are all traceless. This simplifies my answer to $B(X,Y) = 2n \mathrm{Tr}(XY)$. However, $2n \neq n-2$! Where am I going wrong in my proof? Am I taking too large of a leap in assuming that I can write the matrix M corresponding to $\mathrm{ad}_{X} \circ \mathrm{ad}_{Y}$ so easily?",,"['matrices', 'lie-algebras', 'bilinear-form', 'adjoint-operators']"
30,"True or False: If $A^2=0$ for a 10x10 matrix $A$, then rank($A$) $\le 5$","True or False: If  for a 10x10 matrix , then rank()",A^2=0 A A \le 5,"True or False:   If $A^2=0$ for a $10$ by $10$ matrix $A$, then the inequality rank($A$) $\le 5$ must hold. I am guessing that this is false. Apparently proving or disproving this uses knowledge of basis, change of coordinates, or dimension (the chapter of the problem is about these). I tried proof by contradiction, which didn't work so far.","True or False:   If $A^2=0$ for a $10$ by $10$ matrix $A$, then the inequality rank($A$) $\le 5$ must hold. I am guessing that this is false. Apparently proving or disproving this uses knowledge of basis, change of coordinates, or dimension (the chapter of the problem is about these). I tried proof by contradiction, which didn't work so far.",,"['matrices', 'matrix-rank']"
31,Commutator of a matrix to the power of k,Commutator of a matrix to the power of k,,"The question asks me to show that $$[A,B^k] = \Sigma_{r=1}^k B^{r-1}[A,B]B^{k-r}$$ (A, B are nxn matrices) but I can't get even close. I suspect there's some definition of $B^k$ that I don't know but is required. I've tried expanding the RHS, to get $$\Sigma_{r=1}^k B^{r-1}(AB-BA)B^{k-r}$$ $$= \Sigma_{r=1}^k B^{r-1}ABB^{k-r} - B^{r-1}BAB^{k-r}$$ So starting from the LHS, what can I do to $B^k$? Substituting in the diagonalised matrix such that $B^k = P\Lambda^k P^{-1}$ didn't get me anywhere and I'm not sure where the sum comes into it. The only sums I've seen in matrix calculations come from exp(B), but I don't think that's related. Any help or hints are much appreciated!","The question asks me to show that $$[A,B^k] = \Sigma_{r=1}^k B^{r-1}[A,B]B^{k-r}$$ (A, B are nxn matrices) but I can't get even close. I suspect there's some definition of $B^k$ that I don't know but is required. I've tried expanding the RHS, to get $$\Sigma_{r=1}^k B^{r-1}(AB-BA)B^{k-r}$$ $$= \Sigma_{r=1}^k B^{r-1}ABB^{k-r} - B^{r-1}BAB^{k-r}$$ So starting from the LHS, what can I do to $B^k$? Substituting in the diagonalised matrix such that $B^k = P\Lambda^k P^{-1}$ didn't get me anywhere and I'm not sure where the sum comes into it. The only sums I've seen in matrix calculations come from exp(B), but I don't think that's related. Any help or hints are much appreciated!",,"['abstract-algebra', 'matrices', 'ring-theory', 'noncommutative-algebra']"
32,Gradient of $\log(\det(AX))$,Gradient of,\log(\det(AX)),"How to calculate the gradient with respect to $A$ of $\log(\det(AX))$? Here, $X$ and $A$ are positive definite matrixes, and $\det$ is the determinant. How to calculate this? Or, what is the result? Thanks!","How to calculate the gradient with respect to $A$ of $\log(\det(AX))$? Here, $X$ and $A$ are positive definite matrixes, and $\det$ is the determinant. How to calculate this? Or, what is the result? Thanks!",,"['matrices', 'derivatives', 'determinant', 'vector-analysis', 'matrix-calculus']"
33,Finding a solution when the determinant is zero,Finding a solution when the determinant is zero,,"I have the set of equations $$x+y-z=-5$$ $$x-y-z=-3$$ $$x+y-z=0$$ I'd just like to point out here that equation one and three are basically the same - just set equal to two different numbers. I don't know if that automatically means there are no solutions or not. Then, I put this into matrix form and got $$\begin{bmatrix} 1&1&-1\\1&-1&-1\\1&1&-1 \end{bmatrix}\begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}-5\\-3\\0\end{bmatrix}$$ (I'll refer to that first matrix as $A$.) I found the determinant of $A$ and it is equal to zero, meaning there is no inverse. I got really stuck here. I know that to solve this has to do with row echelon form, null space, and column space. I put $A$ into row echelon form and got $$\begin{bmatrix}1&0&-1\\0&1&0\\0&0&0\end{bmatrix}$$ but translating this back out gave $$x-z=-5$$ $$y=-3$$ $$0=0$$ Trying to solve the first equation gave conflicting answers. So, I guess I have three main questions here: How do you find the solution here (or figure out that there isn't a solution)? How do you calculate null space/column space? (Please note I've googled this and I just can't get it.) How does null space/column space relate to finding the solution here, if at all? Thanks! Please keep in mind that I'm in 8th grade trying to figure this out, so the simpler the answer, the better. I am willing to try to figure out stuff, though. Any answers would be appreciated. Edit: Yes, this particular system is unsolvable (thanks to Jack D'Aurizio and others), but I kind of wanted to know how to find a general way to calculate solutions or the absence of solutions when the determinant is zero and the matrix has no inverse. In other words, mainly questions two and three now.","I have the set of equations $$x+y-z=-5$$ $$x-y-z=-3$$ $$x+y-z=0$$ I'd just like to point out here that equation one and three are basically the same - just set equal to two different numbers. I don't know if that automatically means there are no solutions or not. Then, I put this into matrix form and got $$\begin{bmatrix} 1&1&-1\\1&-1&-1\\1&1&-1 \end{bmatrix}\begin{bmatrix}x\\y\\z\end{bmatrix}=\begin{bmatrix}-5\\-3\\0\end{bmatrix}$$ (I'll refer to that first matrix as $A$.) I found the determinant of $A$ and it is equal to zero, meaning there is no inverse. I got really stuck here. I know that to solve this has to do with row echelon form, null space, and column space. I put $A$ into row echelon form and got $$\begin{bmatrix}1&0&-1\\0&1&0\\0&0&0\end{bmatrix}$$ but translating this back out gave $$x-z=-5$$ $$y=-3$$ $$0=0$$ Trying to solve the first equation gave conflicting answers. So, I guess I have three main questions here: How do you find the solution here (or figure out that there isn't a solution)? How do you calculate null space/column space? (Please note I've googled this and I just can't get it.) How does null space/column space relate to finding the solution here, if at all? Thanks! Please keep in mind that I'm in 8th grade trying to figure this out, so the simpler the answer, the better. I am willing to try to figure out stuff, though. Any answers would be appreciated. Edit: Yes, this particular system is unsolvable (thanks to Jack D'Aurizio and others), but I kind of wanted to know how to find a general way to calculate solutions or the absence of solutions when the determinant is zero and the matrix has no inverse. In other words, mainly questions two and three now.",,"['linear-algebra', 'matrices', 'matrix-equations']"
34,If $\det(AB)=4$ then find the value of $\det(BA)$ [duplicate],If  then find the value of  [duplicate],\det(AB)=4 \det(BA),"This question already has answers here : When A and B are of different order given the $\det(AB)$,then calculate $\det(BA)$ (3 answers) Closed 7 years ago . Let $A$ be a $2 \times 3$ matrix with real entries and let $B$ be a $3 \times 2$ matrix with real entries. If $\det(AB)=4$ then find the value of $\det(BA)$ . My attempt: I am aware that $\det(AB)=\det(BA)$ when $A$ and $B$ are of same order. But how to do this?","This question already has answers here : When A and B are of different order given the $\det(AB)$,then calculate $\det(BA)$ (3 answers) Closed 7 years ago . Let be a matrix with real entries and let be a matrix with real entries. If then find the value of . My attempt: I am aware that when and are of same order. But how to do this?",A 2 \times 3 B 3 \times 2 \det(AB)=4 \det(BA) \det(AB)=\det(BA) A B,['matrices']
35,Proving that symmetric matrix can be shifted to be positive definite,Proving that symmetric matrix can be shifted to be positive definite,,"In my studies of matrix analysis, particularly in positive semidefinite matrices, I have come across the following question: Let $ I $ be the $ n \times n $ identity matrix, and let $ v $ be a length n real column vector, we are asked to prove there exists a positive $ \delta > 0 $ such that the matrix $ I - \delta vv^T $ is positive semi definite. Now, I know for any positive $ \delta $ we know that $ I - \delta vv^T $ is a real symmetric matrix and thus its positive semi definiteness is equivalent to non-negativity of its eigenvalues, but how do I prove there exists a positive $ \delta > 0 $ such that the eigenvalues of $ I - \delta vv^T $ are non negative and I have no real idea on how to do this, or even if this is the right approach, I would certainly appreciate any help on this, I thank all helpers.","In my studies of matrix analysis, particularly in positive semidefinite matrices, I have come across the following question: Let $ I $ be the $ n \times n $ identity matrix, and let $ v $ be a length n real column vector, we are asked to prove there exists a positive $ \delta > 0 $ such that the matrix $ I - \delta vv^T $ is positive semi definite. Now, I know for any positive $ \delta $ we know that $ I - \delta vv^T $ is a real symmetric matrix and thus its positive semi definiteness is equivalent to non-negativity of its eigenvalues, but how do I prove there exists a positive $ \delta > 0 $ such that the eigenvalues of $ I - \delta vv^T $ are non negative and I have no real idea on how to do this, or even if this is the right approach, I would certainly appreciate any help on this, I thank all helpers.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
36,How to find the determinant of this $n \times n$ matrix in a clever way?,How to find the determinant of this  matrix in a clever way?,n \times n,Is there any clever and short way to find out the determinant of the following matrix? \begin{bmatrix}         b_1 & b_2 & b_3 & \cdots & b_{n-1} & 0 \\         a_1 & 0 & 0 & \cdots & 0 & b_1 \\         0 & a_2 & 0 & \cdots & 0 & b_2\\         \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\         0 & 0 & 0 & \cdots & a_{n-1} & b_{n-1} \\         \end{bmatrix} Any help will be appreciated.,Is there any clever and short way to find out the determinant of the following matrix? Any help will be appreciated.,"\begin{bmatrix}
        b_1 & b_2 & b_3 & \cdots & b_{n-1} & 0 \\
        a_1 & 0 & 0 & \cdots & 0 & b_1 \\
        0 & a_2 & 0 & \cdots & 0 & b_2\\
        \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & 0 & \cdots & a_{n-1} & b_{n-1} \\
        \end{bmatrix}","['linear-algebra', 'matrices', 'determinant']"
37,Find vectors that span the kernel of $\begin{bmatrix}1&2\\3&4\end{bmatrix}$,Find vectors that span the kernel of,\begin{bmatrix}1&2\\3&4\end{bmatrix},I have the following matrix: \begin{bmatrix}1&2\\3&4\end{bmatrix} and I'd like to find the vectors that span the kernel. The book I'm reading isn't helping me understand this concept at all.,I have the following matrix: \begin{bmatrix}1&2\\3&4\end{bmatrix} and I'd like to find the vectors that span the kernel. The book I'm reading isn't helping me understand this concept at all.,,"['linear-algebra', 'matrices', 'vector-spaces']"
38,Show by example that $A \ge B \ge 0 \implies A^2 \ge B^2 \ge 0$ is not true in general,Show by example that  is not true in general,A \ge B \ge 0 \implies A^2 \ge B^2 \ge 0,"Show by example that $A \ge B \ge 0 \implies A^2 \ge B^2 \ge 0$ is not true in general, where $A$ and $B$ are both $n \times n$ matrices in $\mathbb{R}$. I know that if $AB=BA$, then $A \ge B \ge 0 \implies A^k \ge B^k \ge 0$ holds for $k=1,2,3, \cdots$. So, the aim is to find such two matrices $A$ and $B$, $AB \ne BA$, and the proposition above dosen't hold. Can anyone help me to find such a counterexample? remark: here $A \ge B$ means that $A-B$ is positive semidefinite.","Show by example that $A \ge B \ge 0 \implies A^2 \ge B^2 \ge 0$ is not true in general, where $A$ and $B$ are both $n \times n$ matrices in $\mathbb{R}$. I know that if $AB=BA$, then $A \ge B \ge 0 \implies A^k \ge B^k \ge 0$ holds for $k=1,2,3, \cdots$. So, the aim is to find such two matrices $A$ and $B$, $AB \ne BA$, and the proposition above dosen't hold. Can anyone help me to find such a counterexample? remark: here $A \ge B$ means that $A-B$ is positive semidefinite.",,"['linear-algebra', 'matrices']"
39,Why is column space on the vertical in a matrix?,Why is column space on the vertical in a matrix?,,"Why is the ""column space"" on the vertical in a matrix? In my mind the column space is that space that the vectors in the matrix have created. I mean, for example take the equations: 3x + 4y = 5  2x + 8y = 6 Then the matrix will be: \begin{pmatrix} 3 & 4 \\ 2 & 8  \end{pmatrix} But why is the space defined by this matrix on the vertical? Aren't the two vectors: 3i + 4j and 2i + 8j defining the space we're working in?","Why is the ""column space"" on the vertical in a matrix? In my mind the column space is that space that the vectors in the matrix have created. I mean, for example take the equations: 3x + 4y = 5  2x + 8y = 6 Then the matrix will be: \begin{pmatrix} 3 & 4 \\ 2 & 8  \end{pmatrix} But why is the space defined by this matrix on the vertical? Aren't the two vectors: 3i + 4j and 2i + 8j defining the space we're working in?",,"['linear-algebra', 'matrices']"
40,Prove that $ABA^T$ is symmetric when $A$ and $B$ are symmetric matrices,Prove that  is symmetric when  and  are symmetric matrices,ABA^T A B,"I have been learning about matrix symmetry and came up with a question that I can't seem to prove.  The idea is that the product of $ABA^T$ is a symmetric matrix. What I mainly have to go off of is The product of two symmetric matrices is symmetric iff the matrices commute. I also know $(AB)^T$ = $B^TA^T$ I know that $A$ and $B$ are symmetric matrices (not symmetric to each other) but they are not necessarily invertible matrices. If I test an example such as \begin{equation} ABA^T=\begin{pmatrix}1 & 2 \\ 2 & 3\end{pmatrix}\begin{pmatrix}-4 & -1 \\ 1 & 0\end{pmatrix}\begin{pmatrix}1 & 2 \\ 2 & 3\end{pmatrix} = \begin{pmatrix}0 & -1 \\ -1 & -4\end{pmatrix} \end{equation} I get a symmetric matrix, and I can't find an example where it does not work, but I don't understand how to prove it.  The reason this example is interesting to me is that $AB$ is not a symmetric matrix, but once you multiply $AB$ by $A^T$ (or just $A$ since $A$ = $A^T$), the resulting matrix is symmetrical. But I don't know how this could be applied to make a proof that would hold for any size.","I have been learning about matrix symmetry and came up with a question that I can't seem to prove.  The idea is that the product of $ABA^T$ is a symmetric matrix. What I mainly have to go off of is The product of two symmetric matrices is symmetric iff the matrices commute. I also know $(AB)^T$ = $B^TA^T$ I know that $A$ and $B$ are symmetric matrices (not symmetric to each other) but they are not necessarily invertible matrices. If I test an example such as \begin{equation} ABA^T=\begin{pmatrix}1 & 2 \\ 2 & 3\end{pmatrix}\begin{pmatrix}-4 & -1 \\ 1 & 0\end{pmatrix}\begin{pmatrix}1 & 2 \\ 2 & 3\end{pmatrix} = \begin{pmatrix}0 & -1 \\ -1 & -4\end{pmatrix} \end{equation} I get a symmetric matrix, and I can't find an example where it does not work, but I don't understand how to prove it.  The reason this example is interesting to me is that $AB$ is not a symmetric matrix, but once you multiply $AB$ by $A^T$ (or just $A$ since $A$ = $A^T$), the resulting matrix is symmetrical. But I don't know how this could be applied to make a proof that would hold for any size.",,"['linear-algebra', 'matrices']"
41,"How to construct a $4 \times 4$ symmetric, positive definite matrix with integer eigenvalues","How to construct a  symmetric, positive definite matrix with integer eigenvalues",4 \times 4,"As part of my master thesis I'm trying to construct (or find) some $4 \times 4$ symmetric, positive (semi-)definite matrices with integer components, and integer eigenvalues. The reason for the integer conditions is purely aesthetical, since typesetting the matrix and many analytical calculations look nicer with integer scalars. I'm aware of answers such as https://math.stackexchange.com/a/1377275/245055 , but the problem is that this does not produce a symmetric matrix. Any guidance will be greatly appreciated, as I would very much prefer not having to search for this by brute force or via code (which might potentially produce false positives due to numerical precision issues).","As part of my master thesis I'm trying to construct (or find) some $4 \times 4$ symmetric, positive (semi-)definite matrices with integer components, and integer eigenvalues. The reason for the integer conditions is purely aesthetical, since typesetting the matrix and many analytical calculations look nicer with integer scalars. I'm aware of answers such as https://math.stackexchange.com/a/1377275/245055 , but the problem is that this does not produce a symmetric matrix. Any guidance will be greatly appreciated, as I would very much prefer not having to search for this by brute force or via code (which might potentially produce false positives due to numerical precision issues).",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'integers', 'positive-definite']"
42,If A and B are diagonalizable then so is AB,If A and B are diagonalizable then so is AB,,"When we have to n×n matrices that can be made diagonal (maybe not in the same basis), is it true that the same works for their product?","When we have to n×n matrices that can be made diagonal (maybe not in the same basis), is it true that the same works for their product?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
43,Does $\|A\|_2\leq \|B\|_2$ imply $\|CA\|_2\leq \|CB\|_2$ or $\|AC\|_2\leq \|BC\|_2$?,Does  imply  or ?,\|A\|_2\leq \|B\|_2 \|CA\|_2\leq \|CB\|_2 \|AC\|_2\leq \|BC\|_2,"Assume matrices $A$, $B$, and $C$  are of same dimensions, does $\|A\|_2\leq \|B\|_2$ imply   $\|CA\|_2\leq \|CB\|_2$ or $\|AC\|_2\leq \|BC\|_2$? $\|A\|_2$ denotes by $\lambda_{max}\sqrt{A^TA}$, and $\lambda_{max}$ is the largest eigenvalue.","Assume matrices $A$, $B$, and $C$  are of same dimensions, does $\|A\|_2\leq \|B\|_2$ imply   $\|CA\|_2\leq \|CB\|_2$ or $\|AC\|_2\leq \|BC\|_2$? $\|A\|_2$ denotes by $\lambda_{max}\sqrt{A^TA}$, and $\lambda_{max}$ is the largest eigenvalue.",,"['matrices', 'inequality', 'normed-spaces']"
44,On the number of $2 \times 2$ matrices over ${\Bbb Z}_3$ with unit determinant,On the number of  matrices over  with unit determinant,2 \times 2 {\Bbb Z}_3,"Determine the number of matrices in ${\Bbb Z}_3^{2 \times 2}$ with determinant $1$ . I know that the elements in ${\Bbb Z}_3$ are $\{0,1,2\}$ now possibly determinant can be $1$ in this case $$ \begin {bmatrix} 1&2\\ 0&1\\ \end{bmatrix} $$ and there can be many more but how to find the exact number of such matrices? I know that the answer is $24$ .",Determine the number of matrices in with determinant . I know that the elements in are now possibly determinant can be in this case and there can be many more but how to find the exact number of such matrices? I know that the answer is .,"{\Bbb Z}_3^{2 \times 2} 1 {\Bbb Z}_3 \{0,1,2\} 1 
\begin
{bmatrix}
1&2\\
0&1\\
\end{bmatrix}
 24","['linear-algebra', 'matrices', 'determinant', 'finite-fields']"
45,LU-factorization: why can't I get a unit lower triangular matrix?,LU-factorization: why can't I get a unit lower triangular matrix?,,"I want to find an $LU$-factorization of the following matrix: \begin{align*} A = \begin{pmatrix} 3 & -6 & 9 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} \end{align*} This matrix is invertible (the determinant is $33$), so I should be getting a $LU$ decomposition where  $L$ is a lower triangular matrix with only $1s$ on the diagonal. But that's not what I'm getting. Here is what I did: \begin{align*} \begin{pmatrix} 3 & -6 & 9 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} & \begin{matrix} \xrightarrow{R_1 \rightarrow 1/3 R_1} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} \begin{matrix} \xrightarrow{R_2 \rightarrow R_2 + 2R_1} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 3 & 4 \\ 0 & 1 & 5 \end{pmatrix} \\ & \begin{matrix} \xrightarrow{R_2 \rightarrow 1/3 R_2} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 1 & \frac{4}{3} \\ 0 & 1 & 5 \end{pmatrix} \begin{matrix} \xrightarrow{R_3 \rightarrow R_3 - R_2} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 1 & \frac{4}{3} \\ 0 & 0 & \frac{11}{3} \end{pmatrix} = U. \end{align*} This matrix is now in echelon form. The elementary matrices corresponding to the operations are: \begin{align*} E_1 & = \begin{pmatrix} \frac{1}{3} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_1 \rightarrow 1/3 R_1) \quad E_2 = \begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_2 \rightarrow R_2 + 2R_1) \\ E_3 &= \begin{pmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{3} & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_2 \rightarrow 1/3 R_2) \quad E_4 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -1 & 1 \end{pmatrix} \qquad (R_3 \rightarrow R_3 - R_2) \end{align*} Hence we have $E_4 E_3 E_2 E_1 A = U$. Now \begin{align*} E_4 E_3 E_2 E_1 = M = \begin{pmatrix} \frac{1}{3} & 0 & 0 \\ \frac{2}{9} & \frac{1}{3} & 0 \\ - \frac{2}{9} & - \frac{1}{3} & 1 \end{pmatrix} \end{align*} is a lower triangular matrix. Now we can write \begin{align*} A = (E_4 E_3 E_2 E_1)^{-1} U = M^{-1} U = LU, \end{align*} with \begin{align*} M^{-1} = L = \begin{pmatrix} 3 & 0 & 0 \\ -2 & 3 & 0 \\ 0 & 1 & 1 \end{pmatrix} \end{align*} But why does my matrix $L$ not have $1s$ on the diagonal? I thought an $LU$-factorization of an invertible matrix is unique, and that in that case $L$ is an unit lower triangular matrix? Did I overlook something or made a mistake? I haven't interchanged any rows. Please help!","I want to find an $LU$-factorization of the following matrix: \begin{align*} A = \begin{pmatrix} 3 & -6 & 9 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} \end{align*} This matrix is invertible (the determinant is $33$), so I should be getting a $LU$ decomposition where  $L$ is a lower triangular matrix with only $1s$ on the diagonal. But that's not what I'm getting. Here is what I did: \begin{align*} \begin{pmatrix} 3 & -6 & 9 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} & \begin{matrix} \xrightarrow{R_1 \rightarrow 1/3 R_1} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ -2 & 7 & -2 \\ 0 & 1 & 5 \end{pmatrix} \begin{matrix} \xrightarrow{R_2 \rightarrow R_2 + 2R_1} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 3 & 4 \\ 0 & 1 & 5 \end{pmatrix} \\ & \begin{matrix} \xrightarrow{R_2 \rightarrow 1/3 R_2} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 1 & \frac{4}{3} \\ 0 & 1 & 5 \end{pmatrix} \begin{matrix} \xrightarrow{R_3 \rightarrow R_3 - R_2} \end{matrix} \begin{pmatrix} 1 & -2 & 3 \\ 0 & 1 & \frac{4}{3} \\ 0 & 0 & \frac{11}{3} \end{pmatrix} = U. \end{align*} This matrix is now in echelon form. The elementary matrices corresponding to the operations are: \begin{align*} E_1 & = \begin{pmatrix} \frac{1}{3} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_1 \rightarrow 1/3 R_1) \quad E_2 = \begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_2 \rightarrow R_2 + 2R_1) \\ E_3 &= \begin{pmatrix} 1 & 0 & 0 \\ 0 & \frac{1}{3} & 0 \\ 0 & 0 & 1 \end{pmatrix} \qquad (R_2 \rightarrow 1/3 R_2) \quad E_4 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & -1 & 1 \end{pmatrix} \qquad (R_3 \rightarrow R_3 - R_2) \end{align*} Hence we have $E_4 E_3 E_2 E_1 A = U$. Now \begin{align*} E_4 E_3 E_2 E_1 = M = \begin{pmatrix} \frac{1}{3} & 0 & 0 \\ \frac{2}{9} & \frac{1}{3} & 0 \\ - \frac{2}{9} & - \frac{1}{3} & 1 \end{pmatrix} \end{align*} is a lower triangular matrix. Now we can write \begin{align*} A = (E_4 E_3 E_2 E_1)^{-1} U = M^{-1} U = LU, \end{align*} with \begin{align*} M^{-1} = L = \begin{pmatrix} 3 & 0 & 0 \\ -2 & 3 & 0 \\ 0 & 1 & 1 \end{pmatrix} \end{align*} But why does my matrix $L$ not have $1s$ on the diagonal? I thought an $LU$-factorization of an invertible matrix is unique, and that in that case $L$ is an unit lower triangular matrix? Did I overlook something or made a mistake? I haven't interchanged any rows. Please help!",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'gaussian-elimination', 'lu-decomposition']"
46,Galois group of a characteristic polynomial,Galois group of a characteristic polynomial,,"Quick question. I have a 3x3 matrix with integer entries, say $J$. $J$ has rank 2 and determinant equal to zero. According to GAP the matrix has characteristic polynomial $f = X^3-9X^2+8X$. Moreover GAP tells me $J$ has splitting field the ""rationals"", $\mathbb Q$. How I do compute the Galois group of $f$? Is it non trivial?","Quick question. I have a 3x3 matrix with integer entries, say $J$. $J$ has rank 2 and determinant equal to zero. According to GAP the matrix has characteristic polynomial $f = X^3-9X^2+8X$. Moreover GAP tells me $J$ has splitting field the ""rationals"", $\mathbb Q$. How I do compute the Galois group of $f$? Is it non trivial?",,"['linear-algebra', 'matrices', 'field-theory', 'galois-theory']"
47,"If $A$ is normal and $A$ and $B$ commute, then $A^*$ and $B$ commute","If  is normal and  and  commute, then  and  commute",A A B A^* B,"Let $A$ is a normal matrix: $A^*\! A = A A^*\!\!$,$\,$ and $AB = BA$. Prove that $A^*\!B=BA^*\!\!$. I can prove that if $\det A\ne 0$ by multiplication $AB=BA$ by $A^*$ left and right and using some manipulation. But I have no idea what to do if $\det A = 0$.","Let $A$ is a normal matrix: $A^*\! A = A A^*\!\!$,$\,$ and $AB = BA$. Prove that $A^*\!B=BA^*\!\!$. I can prove that if $\det A\ne 0$ by multiplication $AB=BA$ by $A^*$ left and right and using some manipulation. But I have no idea what to do if $\det A = 0$.",,['matrices']
48,Upper bound of Frobenius norm of product of matrices.,Upper bound of Frobenius norm of product of matrices.,,I'm trying to prove that $||AB||_F\leq||A||_2||B||_F$. As far as I know it isn't a hard problem but I was stuck. Any ideas?,I'm trying to prove that $||AB||_F\leq||A||_2||B||_F$. As far as I know it isn't a hard problem but I was stuck. Any ideas?,,"['linear-algebra', 'matrices', 'normed-spaces']"
49,What Method is used for Projecting the Rauzy Fractal?,What Method is used for Projecting the Rauzy Fractal?,,"I am trying to construct the Rauzy Fractal ( http://en.wikipedia.org/wiki/Rauzy_fractal ), I have a Tribonacci word generator and have the stairs constructed but I can't seem to get the projection onto a 2D plane correct. On the 4th slide of this: http://kmwww.fjfi.cvut.cz/jn08/slides/Thuswaldner.pdf they depict a bounding cube, my assumption was that I would project each point (x,y,z) onto (x',y') along the line (xMax, yMax, zMax)->(0,0,0). But they mention a Contracting Plane what does this terminology mean? Are all points projected along the same line or each to their own? For this I am using the unit cube orthogonal projection matrix shown on the wiki: http://en.wikipedia.org/wiki/Orthographic_projection with Ortho(-1, 1, -1, 1, 1, -1) being my cube and (x/xMax, y/yMax, z/zMax) being my normalised point to be projected. I think I've got this part completely wrong. The result would agree... Code: https://bitbucket.org/snippets/NeuralOutlet/8qEa/rauzy-fractal-problem Should I even be using the projection matrix?","I am trying to construct the Rauzy Fractal ( http://en.wikipedia.org/wiki/Rauzy_fractal ), I have a Tribonacci word generator and have the stairs constructed but I can't seem to get the projection onto a 2D plane correct. On the 4th slide of this: http://kmwww.fjfi.cvut.cz/jn08/slides/Thuswaldner.pdf they depict a bounding cube, my assumption was that I would project each point (x,y,z) onto (x',y') along the line (xMax, yMax, zMax)->(0,0,0). But they mention a Contracting Plane what does this terminology mean? Are all points projected along the same line or each to their own? For this I am using the unit cube orthogonal projection matrix shown on the wiki: http://en.wikipedia.org/wiki/Orthographic_projection with Ortho(-1, 1, -1, 1, 1, -1) being my cube and (x/xMax, y/yMax, z/zMax) being my normalised point to be projected. I think I've got this part completely wrong. The result would agree... Code: https://bitbucket.org/snippets/NeuralOutlet/8qEa/rauzy-fractal-problem Should I even be using the projection matrix?",,"['matrices', 'projective-geometry', 'fractals']"
50,"Is there a set of $4$ vectors in $\mathbb{R}^3$, any $3$ of which form a linearly independent set?","Is there a set of  vectors in , any  of which form a linearly independent set?",4 \mathbb{R}^3 3,"I have an exercise in my last assignment for linear algebra: Is there a set of $4$ vectors in $\mathbb{R}^3$, any $3$ of which form   a linearly independent set? Prove. My answer intuitively is no for one reason. If we have 4 vectors in $\mathbb{R}^3$, then, if we consider then all together, one of them is a linear combination of another (or others), it's a multiple of another. From this set of $4$ vectors, we can pick $3$ vectors to check if they are linear independent, but we are going to have at least one group of $3$ vectors where we have a vector and its multiple. Does my reasoning make some sense? How could I prove what they are asking?","I have an exercise in my last assignment for linear algebra: Is there a set of $4$ vectors in $\mathbb{R}^3$, any $3$ of which form   a linearly independent set? Prove. My answer intuitively is no for one reason. If we have 4 vectors in $\mathbb{R}^3$, then, if we consider then all together, one of them is a linear combination of another (or others), it's a multiple of another. From this set of $4$ vectors, we can pick $3$ vectors to check if they are linear independent, but we are going to have at least one group of $3$ vectors where we have a vector and its multiple. Does my reasoning make some sense? How could I prove what they are asking?",,['linear-algebra']
51,Determine inverse matrix of $\left(\begin{matrix} 0 & 3 \\ 0 & 6 \end{matrix} \right)$ using Gauss-Jordan method,Determine inverse matrix of  using Gauss-Jordan method,\left(\begin{matrix} 0 & 3 \\ 0 & 6 \end{matrix} \right),"I need to find the inverse of the following matrix with Gauss-Jordan method, but apparently, checking with a calculator, it does not exist: $$\left(\begin{matrix} 0 & 3 \\ 0 & 6 \end{matrix} \right)$$ How can we apply Gauss-Jordan to the previous matrix, and from that determine if the inverse matrix exists or not? I think the problem is that we cannot make the upper left $0$ $1$, right?","I need to find the inverse of the following matrix with Gauss-Jordan method, but apparently, checking with a calculator, it does not exist: $$\left(\begin{matrix} 0 & 3 \\ 0 & 6 \end{matrix} \right)$$ How can we apply Gauss-Jordan to the previous matrix, and from that determine if the inverse matrix exists or not? I think the problem is that we cannot make the upper left $0$ $1$, right?",,['linear-algebra']
52,Let Q be orthogonal. Show that Q^tAQ has the same characteristic polynomial. as A.,Let Q be orthogonal. Show that Q^tAQ has the same characteristic polynomial. as A.,,Do I have to use the diagonalization of A?,Do I have to use the diagonalization of A?,,['matrices']
53,Find Formulas for $M^{n}$ of Matrix $M$,Find Formulas for  of Matrix,M^{n} M,"Find formulas for the entries of $M^n$, where $n$ is a positive integer. $M$ is the following matrix: $$ \begin{bmatrix} 4 & -2 \\ 4 & 10 \end{bmatrix} $$ So if $M=PDP^{-1}$ then it follows $M^n=PD^nP^{-1}$ where $P$ is made of the eigenvectors and $D$ is the diagonal of our eigenvalues (both of which were found starting with a char. polynomial). My results: $$ D = \begin{bmatrix} 8 & 0 \\ 0 & 6 \end{bmatrix} \qquad P^{-1} = \begin{bmatrix} 2 & 2 \\ -2 & -1 \end{bmatrix} \qquad P = \begin{bmatrix} -1/2 & -1 \\ 1 & 1 \end{bmatrix} $$ So I believe (unless I'm completely on the wrong track here) that I've found $M^n$, but I'm unsure now how to get this in an acceptable $2 \times 2$ matrix for $M^n$.","Find formulas for the entries of $M^n$, where $n$ is a positive integer. $M$ is the following matrix: $$ \begin{bmatrix} 4 & -2 \\ 4 & 10 \end{bmatrix} $$ So if $M=PDP^{-1}$ then it follows $M^n=PD^nP^{-1}$ where $P$ is made of the eigenvectors and $D$ is the diagonal of our eigenvalues (both of which were found starting with a char. polynomial). My results: $$ D = \begin{bmatrix} 8 & 0 \\ 0 & 6 \end{bmatrix} \qquad P^{-1} = \begin{bmatrix} 2 & 2 \\ -2 & -1 \end{bmatrix} \qquad P = \begin{bmatrix} -1/2 & -1 \\ 1 & 1 \end{bmatrix} $$ So I believe (unless I'm completely on the wrong track here) that I've found $M^n$, but I'm unsure now how to get this in an acceptable $2 \times 2$ matrix for $M^n$.",,"['linear-algebra', 'matrices']"
54,Ambiguous matrix representation of imaginary unit?,Ambiguous matrix representation of imaginary unit?,,"This question is triggered by another one: What does it mean to represent a number in term of a 2x2 matrix? Suppose we seek a $2\times2$ matrix equivalent of the imaginary unit, that is a matrix $\,i\,$ such that $\,i^2 = -1\,$ , or, with $\;a,b,c,d \in \mathbb{R}$ : $$ i^2 = \begin{bmatrix} a&b\\c&d \end{bmatrix}  \begin{bmatrix} a&b\\c&d \end{bmatrix} = \begin{bmatrix} a^2+bc&ab+bd\\ac+cd&bc+d^2 \end{bmatrix} = - \begin{bmatrix} 1&0\\0&1 \end{bmatrix} $$ Leading to the following equations: $$ a^2+bc=-1 \quad ; \quad b(a+d)=0 \quad ; \quad c(a+d)=0 \quad ; \quad bc+d^2=-1 $$ Subtracting the first from the last one gives two possible solutions: $$ a^2-d^2=0 \quad \Longrightarrow \quad a = \pm\, d $$ The solution $\,a=d\ne0\,$ leads to $\,b=0\,$ and $\,c=0\,$ , giving $\,a^2 = -1$ , which is impossible in the reals. The other possibility is $\,d = -a$ : $$ i = \begin{bmatrix} a&b\\c&-a \end{bmatrix} \quad \mbox{with} \quad a^2+bc = -1 \quad \mbox{or} \quad \begin{vmatrix} a&b\\c&-a \end{vmatrix} = 1 $$ But here I'm stuck. IMO it does not follow that $\,i\,$ is a special case of the above matrix, namely the one with $\,a=0\,$ and $\,b=-1$ , $c=1$ : $$ i = \begin{bmatrix} 0&-1\\1&0 \end{bmatrix} $$ Am I missing something obvious? Note. In a comment by Meelo it is remarked that <quote>One can notice that this representation is not unique, though one must always treat $1$ as the matrix $$ \pmatrix{1&&0\\0&&1} $$ any matrix with characteristic polynomial $x^2+1$ suffices to represent $i$ . For instance, you could use $$\pmatrix{1&&-2\\1&&-1}$$ for $i$.</quote> That's right, because $$ \begin{vmatrix}a-\lambda&b\\c&-a-\lambda\end{vmatrix}=(a-\lambda)(-a-\lambda)-bc=\lambda^2+1 $$ I don't see, however, how this can be matched with my answer at the same place: $$ e^{\begin{bmatrix} 1 & -2 \\ 1 & -1 \end{bmatrix}\theta} = \mbox{?} = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix} $$","This question is triggered by another one: What does it mean to represent a number in term of a 2x2 matrix? Suppose we seek a $2\times2$ matrix equivalent of the imaginary unit, that is a matrix $\,i\,$ such that $\,i^2 = -1\,$ , or, with $\;a,b,c,d \in \mathbb{R}$ : $$ i^2 = \begin{bmatrix} a&b\\c&d \end{bmatrix}  \begin{bmatrix} a&b\\c&d \end{bmatrix} = \begin{bmatrix} a^2+bc&ab+bd\\ac+cd&bc+d^2 \end{bmatrix} = - \begin{bmatrix} 1&0\\0&1 \end{bmatrix} $$ Leading to the following equations: $$ a^2+bc=-1 \quad ; \quad b(a+d)=0 \quad ; \quad c(a+d)=0 \quad ; \quad bc+d^2=-1 $$ Subtracting the first from the last one gives two possible solutions: $$ a^2-d^2=0 \quad \Longrightarrow \quad a = \pm\, d $$ The solution $\,a=d\ne0\,$ leads to $\,b=0\,$ and $\,c=0\,$ , giving $\,a^2 = -1$ , which is impossible in the reals. The other possibility is $\,d = -a$ : $$ i = \begin{bmatrix} a&b\\c&-a \end{bmatrix} \quad \mbox{with} \quad a^2+bc = -1 \quad \mbox{or} \quad \begin{vmatrix} a&b\\c&-a \end{vmatrix} = 1 $$ But here I'm stuck. IMO it does not follow that $\,i\,$ is a special case of the above matrix, namely the one with $\,a=0\,$ and $\,b=-1$ , $c=1$ : $$ i = \begin{bmatrix} 0&-1\\1&0 \end{bmatrix} $$ Am I missing something obvious? Note. In a comment by Meelo it is remarked that <quote>One can notice that this representation is not unique, though one must always treat $1$ as the matrix $$ \pmatrix{1&&0\\0&&1} $$ any matrix with characteristic polynomial $x^2+1$ suffices to represent $i$ . For instance, you could use $$\pmatrix{1&&-2\\1&&-1}$$ for $i$.</quote> That's right, because $$ \begin{vmatrix}a-\lambda&b\\c&-a-\lambda\end{vmatrix}=(a-\lambda)(-a-\lambda)-bc=\lambda^2+1 $$ I don't see, however, how this can be matched with my answer at the same place: $$ e^{\begin{bmatrix} 1 & -2 \\ 1 & -1 \end{bmatrix}\theta} = \mbox{?} = \begin{bmatrix} \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta) \end{bmatrix} $$",,"['matrices', 'complex-numbers']"
55,Are the $2\times 2$ symmetric matrices a ring?,Are the  symmetric matrices a ring?,2\times 2,"Ok so I am looking at Rings. I saw somewhere that the $2 \times 2$ symmetric matrices with entries in $\mathbb{R}$ is a ring. But if we look at matrix multiplication I am not convinced: If $ A = \left( \begin{array}{cc} a & b \\ b & c\\ \end{array} \right) $ and $ B =  \left( \begin{array}{cc} x & y \\ y & z\\ \end{array} \right) $ then $AB = \left( \begin{array}{cc} ax+by & ay+bz \\ bx+cy & by+cz\\ \end{array} \right) $ which is not symmetric? What am I missing? Does $a=c$ and $x=z$? If so, why? The matrices aboves are still symmetric without those being equal. Thank you","Ok so I am looking at Rings. I saw somewhere that the $2 \times 2$ symmetric matrices with entries in $\mathbb{R}$ is a ring. But if we look at matrix multiplication I am not convinced: If $ A = \left( \begin{array}{cc} a & b \\ b & c\\ \end{array} \right) $ and $ B =  \left( \begin{array}{cc} x & y \\ y & z\\ \end{array} \right) $ then $AB = \left( \begin{array}{cc} ax+by & ay+bz \\ bx+cy & by+cz\\ \end{array} \right) $ which is not symmetric? What am I missing? Does $a=c$ and $x=z$? If so, why? The matrices aboves are still symmetric without those being equal. Thank you",,"['abstract-algebra', 'matrices']"
56,Blockwise Symmetric Matrix Determinant,Blockwise Symmetric Matrix Determinant,,"This question arises from another one of mine , but separate enough that I feel it deserves its own thread. Wikipedia says that $$det\begin{bmatrix}A&B\\B &A \end{bmatrix} = det(A+B)det(A-B)$$ Regardless of whether or not A and B commute. Using the general formulation $$det\begin{bmatrix}A&B\\C &D \end{bmatrix} = det(A)det(D - CA^{-1}B)$$ We see that this becomes $$det(AD- ACA^{-1}B)$$ Or for my original matrix, $$det(AA - ABA^{-1}B)$$ I see how this becomes det((A-B)(A+B)) if A and B commute, but how is it valid if they don't commute? Can anyone prove this please?","This question arises from another one of mine , but separate enough that I feel it deserves its own thread. Wikipedia says that $$det\begin{bmatrix}A&B\\B &A \end{bmatrix} = det(A+B)det(A-B)$$ Regardless of whether or not A and B commute. Using the general formulation $$det\begin{bmatrix}A&B\\C &D \end{bmatrix} = det(A)det(D - CA^{-1}B)$$ We see that this becomes $$det(AD- ACA^{-1}B)$$ Or for my original matrix, $$det(AA - ABA^{-1}B)$$ I see how this becomes det((A-B)(A+B)) if A and B commute, but how is it valid if they don't commute? Can anyone prove this please?",,"['linear-algebra', 'matrices', 'proof-verification', 'determinant']"
57,"How to prove, that solution of system $Ax = b$ exists only if there is no solution of $A^T y = 0$ and $b^T y = 1$?","How to prove, that solution of system  exists only if there is no solution of  and ?",Ax = b A^T y = 0 b^T y = 1,"I have a little linear algebra problem here: How can I prove, that there is a solution of system $Ax = b$ only if there is no solution of $A^T y = 0$ and $b^T y = 1$?","I have a little linear algebra problem here: How can I prove, that there is a solution of system $Ax = b$ only if there is no solution of $A^T y = 0$ and $b^T y = 1$?",,"['linear-algebra', 'matrices', 'systems-of-equations']"
58,I can't understand a step in the proof of the associativity of matrix multiplication,I can't understand a step in the proof of the associativity of matrix multiplication,,"Matrix multiplication associativity is proven by the following reasoning: Let there be matrices $A^{m \times n}$, $B^{n \times k}$ and $C^{k \times l}$. Then $$ \{(AB)C\}_{ij}=\sum\limits_{p=1}^k{\{AB\}_{ip}c_{pj} \\=\sum\limits_{p=1}^k \left(\sum\limits_{q=1}^n a_{iq}b_{qp}\right)}c_{pj} \\=\sum\limits_{q=1}^n a_{iq} \left(\sum\limits_{p=1}^k b_{qp}c_{pj}\right)  \\= \{A(BC)\}_{ij}. $$ I don't understand how we get the third line from the second.","Matrix multiplication associativity is proven by the following reasoning: Let there be matrices $A^{m \times n}$, $B^{n \times k}$ and $C^{k \times l}$. Then $$ \{(AB)C\}_{ij}=\sum\limits_{p=1}^k{\{AB\}_{ip}c_{pj} \\=\sum\limits_{p=1}^k \left(\sum\limits_{q=1}^n a_{iq}b_{qp}\right)}c_{pj} \\=\sum\limits_{q=1}^n a_{iq} \left(\sum\limits_{p=1}^k b_{qp}c_{pj}\right)  \\= \{A(BC)\}_{ij}. $$ I don't understand how we get the third line from the second.",,"['linear-algebra', 'matrices']"
59,Show that SO(n) is a normal subgroup of O(n),Show that SO(n) is a normal subgroup of O(n),,Show that SO(n) is a normal subgroup of O(n). A normal subgroup is a subgroup which is invariant under conjugation by members of the group of which it is a part. SO(n) is the set of orthogonal matrices of determinant 1. O(n) is the set of real matrices whose inverses equal their transposes (orthogonal matrices). I'm simply bad at writing proofs. Please help?,Show that SO(n) is a normal subgroup of O(n). A normal subgroup is a subgroup which is invariant under conjugation by members of the group of which it is a part. SO(n) is the set of orthogonal matrices of determinant 1. O(n) is the set of real matrices whose inverses equal their transposes (orthogonal matrices). I'm simply bad at writing proofs. Please help?,,"['linear-algebra', 'abstract-algebra']"
60,Can basis vectors have fractions?,Can basis vectors have fractions?,,"So I was diagonalizing a matrix in a book, and one of the basis vectors was [3/2, 1], after doing the problem, the answer in the book was different than mine. It came with an explanation, and in it the basis vector was [3,2]. They are the same thing, just multiples of each other, so I was curious, is it mandatory to take fractions out of a basis vector?","So I was diagonalizing a matrix in a book, and one of the basis vectors was [3/2, 1], after doing the problem, the answer in the book was different than mine. It came with an explanation, and in it the basis vector was [3,2]. They are the same thing, just multiples of each other, so I was curious, is it mandatory to take fractions out of a basis vector?",,"['linear-algebra', 'matrices']"
61,A is a square matrix where $3A^9- 7A^4 + 4A = I$. Prove that A is invertible by finding $A^{-1}$,A is a square matrix where . Prove that A is invertible by finding,3A^9- 7A^4 + 4A = I A^{-1},"The question is: A is a square matrix where $3A^9- 7A^4 + 4A = I$. Prove that A is invertible by finding A^-1. I have looked at other similar questions on this site: 1. Here 2. and Here But they use definitions and properties that I haven't been taught. Supposedly it can be proven using the definitions we already have gotten. We have learned nothing of eigenvalues yet, thought we have learned a little about determinants. I have a rather dull solution but I don't think it is correct: $3A^9- 7A^4 + 4A = I$ $3A^9- 7A^4 + 4A = A* A^{-1}$ $A^{-1}(3A^9- 7A^4 + 4A) = A^{-1}*A* A^{-1}$ $3A^8- 7A^3 + 4*I = A^{-1}$ From here it can be seen that if we multiply $A^{-1}$ by A we get the original definition given to us equivalent to I. Q.E.D. I'm sure there is a better solution than this since it feels that this solution is not good since it works off of the assumption that A is invertible. A*A^-1 = I. We haven't learned anything about Hermitian matrices so it would be helpful to try to solve this using basic inverse matrix definitions.","The question is: A is a square matrix where $3A^9- 7A^4 + 4A = I$. Prove that A is invertible by finding A^-1. I have looked at other similar questions on this site: 1. Here 2. and Here But they use definitions and properties that I haven't been taught. Supposedly it can be proven using the definitions we already have gotten. We have learned nothing of eigenvalues yet, thought we have learned a little about determinants. I have a rather dull solution but I don't think it is correct: $3A^9- 7A^4 + 4A = I$ $3A^9- 7A^4 + 4A = A* A^{-1}$ $A^{-1}(3A^9- 7A^4 + 4A) = A^{-1}*A* A^{-1}$ $3A^8- 7A^3 + 4*I = A^{-1}$ From here it can be seen that if we multiply $A^{-1}$ by A we get the original definition given to us equivalent to I. Q.E.D. I'm sure there is a better solution than this since it feels that this solution is not good since it works off of the assumption that A is invertible. A*A^-1 = I. We haven't learned anything about Hermitian matrices so it would be helpful to try to solve this using basic inverse matrix definitions.",,"['linear-algebra', 'matrices', 'inverse', 'matrix-equations']"
62,Norm equivalence (Frobenius and infinity),Norm equivalence (Frobenius and infinity),,"I am trying to prove the matrix norm equivalence for norms 1, 2, $\infty$ and Frobenius. I have managed to solve find the constants for $||.||_{1}$ and  $||.||_{2}$ but I cannot see how to continue if I want to prove the following: $||A||_{\infty} \leq \sqrt{n}||A||_{2}$ and the ones for Frobenius: $||A||_{F} \leq \sqrt{n}||A||_{1}$ $||A||_{F} \leq \sqrt{n} ||A||_{2}$ Any help is appreciated. Thank you for your time","I am trying to prove the matrix norm equivalence for norms 1, 2, $\infty$ and Frobenius. I have managed to solve find the constants for $||.||_{1}$ and  $||.||_{2}$ but I cannot see how to continue if I want to prove the following: $||A||_{\infty} \leq \sqrt{n}||A||_{2}$ and the ones for Frobenius: $||A||_{F} \leq \sqrt{n}||A||_{1}$ $||A||_{F} \leq \sqrt{n} ||A||_{2}$ Any help is appreciated. Thank you for your time",,"['matrices', 'normed-spaces']"
63,Is the induced matrix norm continuous?,Is the induced matrix norm continuous?,,"Suppose that we are dealing with positive semidefinite $n\times n$   symmetric real matrices. The induced matrix norm of $A$ is defined as $$  |A|=\max\lambda(A)=\max_{x\in\mathbb{R^n},|x|=1}x'Ax. $$ Here,   $\lambda(A)$ denotes the set of eigenvalues of $A$. Is it true that   this matrix norm is continuous : if $A_n\to A$ (component-wise),   then $|A_n|\to |A|$? I vaguely imagine that since eigenvalues are solutions to the characteristic polynomial, which are built using entries of the matrix, continuity is likely to hold. But I would love to see a rigorous proof (or a counter example). Thank you very much. I apologize ahead if this is a duplicate. (I did search.)","Suppose that we are dealing with positive semidefinite $n\times n$   symmetric real matrices. The induced matrix norm of $A$ is defined as $$  |A|=\max\lambda(A)=\max_{x\in\mathbb{R^n},|x|=1}x'Ax. $$ Here,   $\lambda(A)$ denotes the set of eigenvalues of $A$. Is it true that   this matrix norm is continuous : if $A_n\to A$ (component-wise),   then $|A_n|\to |A|$? I vaguely imagine that since eigenvalues are solutions to the characteristic polynomial, which are built using entries of the matrix, continuity is likely to hold. But I would love to see a rigorous proof (or a counter example). Thank you very much. I apologize ahead if this is a duplicate. (I did search.)",,"['linear-algebra', 'matrices', 'limits', 'continuity']"
64,Determinant of the sum of an identity matrix and a rank-two-symmetric matrix,Determinant of the sum of an identity matrix and a rank-two-symmetric matrix,,"Suppose $I$ is an $n \times n$ identity matrix, and $S$ is the $n \times n$ symmetric matrix with rank equals two.  I was reading something saying that: $$\det(I-S)=(1-\lambda_1)(1-\lambda_2)$$ where $\lambda_1$ and $\lambda_2$ are the two largest (in absolute values) eigenvalues of $S$.  Can anyone provide some clues for proving this?  Thanks in advance!","Suppose $I$ is an $n \times n$ identity matrix, and $S$ is the $n \times n$ symmetric matrix with rank equals two.  I was reading something saying that: $$\det(I-S)=(1-\lambda_1)(1-\lambda_2)$$ where $\lambda_1$ and $\lambda_2$ are the two largest (in absolute values) eigenvalues of $S$.  Can anyone provide some clues for proving this?  Thanks in advance!",,"['matrices', 'determinant', 'matrix-equations', 'block-matrices']"
65,Dimensions of matrices of singular vectors,Dimensions of matrices of singular vectors,,"I have seen the singular value decomposition (SVD) represented in two different ways and wanted to know a) if they're both correct and b) what their relationship is. In the first version a matrix $A \in \mathbb{R}^{m \times n}$ such that $\mbox{rank}(A) = s$ , can be decomposed as $A = V D U^t$ where $V \in \mathbb{R}^{m \times s}$ , $D \in \mathbb{R}^{s\times s}$ and $U \in \mathbb{R}^{n \times s}$ . In the second version a matrix $A \in \mathbb{R}^{m \times n}$ , can be decomposed as $A = V D U^t$ where $V \in \mathbb{R}^{m \times m}$ , $D \in \mathbb{R}^{m \times n}$ and $U \in \mathbb{R}^{n \times n}$ .","I have seen the singular value decomposition (SVD) represented in two different ways and wanted to know a) if they're both correct and b) what their relationship is. In the first version a matrix such that , can be decomposed as where , and . In the second version a matrix , can be decomposed as where , and .",A \in \mathbb{R}^{m \times n} \mbox{rank}(A) = s A = V D U^t V \in \mathbb{R}^{m \times s} D \in \mathbb{R}^{s\times s} U \in \mathbb{R}^{n \times s} A \in \mathbb{R}^{m \times n} A = V D U^t V \in \mathbb{R}^{m \times m} D \in \mathbb{R}^{m \times n} U \in \mathbb{R}^{n \times n},"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd']"
66,Is the norm of the Hilbert matrix equal to $\pi$?,Is the norm of the Hilbert matrix equal to ?,\pi,"Let $A$ be a Hilbert matrix, $$a_{ij}=\frac{1}{1+i+j}$$ We have the result $\| A \| \leq \pi$. I am using the subordinate norm of the Euclidean norm, i.e., $$\| A \| = \sup\{\langle Ax,y\rangle:\quad x,y\in\mathbb{R}^n,\quad\Vert x\Vert_2\leq 1,\quad\Vert y\Vert_2\leq 1\}$$ This inequality can be proved using Hilbert's Inequality . Look here . Question: Do we have an equality? I found nothing on the Internet about such equality.","Let $A$ be a Hilbert matrix, $$a_{ij}=\frac{1}{1+i+j}$$ We have the result $\| A \| \leq \pi$. I am using the subordinate norm of the Euclidean norm, i.e., $$\| A \| = \sup\{\langle Ax,y\rangle:\quad x,y\in\mathbb{R}^n,\quad\Vert x\Vert_2\leq 1,\quad\Vert y\Vert_2\leq 1\}$$ This inequality can be proved using Hilbert's Inequality . Look here . Question: Do we have an equality? I found nothing on the Internet about such equality.",,"['linear-algebra', 'matrices']"
67,how to prove that this determinant is a polynomial? [closed],how to prove that this determinant is a polynomial? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Given two square matrices $A$ and $B$ of size $n\times n$. I am wondering how to prove that $\det(A+xB)$ is a polynomial function of $x$ ? does anyone has a (simple if possible) proof of this fact? What would be the degree of this polynomial?  Thanks.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Given two square matrices $A$ and $B$ of size $n\times n$. I am wondering how to prove that $\det(A+xB)$ is a polynomial function of $x$ ? does anyone has a (simple if possible) proof of this fact? What would be the degree of this polynomial?  Thanks.",,"['linear-algebra', 'matrices']"
68,Conditions on certain entries of a matrix to ensure one Jordan block per eigenvalue,Conditions on certain entries of a matrix to ensure one Jordan block per eigenvalue,,"In preparation for a future exam, I found the following problem: Let $$A = \begin{pmatrix} 1 & 0 & a & b \\ 0 & 1 & 0 & 0 \\ 0 & c & 3 & -2 \\ 0 & d & 2 & -1 \end{pmatrix}$$ Determine conditions on $a,b,c,d$ so that there is only one Jordan block for each eigenvalue of $A$ (in the Jordan canonical form of $A$). I found, readily enough, that the only eigenvalue of the $A$ is $\lambda = 1$. So the problem is in fact to find conditions of $a,b,c,d$ such that $A$ is similar to a single-block Jordan matrix. Thus we want the dimension of the eigenspace $E_1$ to be 1, or equivalently, the rank of $A-I$ to be 3. $$A-I = \begin{pmatrix} 0 & 0 & a & b \\ 0 & 0 & 0 & 0 \\ 0 & c & 2 & -2 \\ 0 & d & 2 & -2 \end{pmatrix}$$ So would the conditions $a,b$ not both zero, $a \neq -b$ and $c \neq d$ be sufficient? More to the point, what is the most economical way of stating the conditions on $a,b,c,d$ to ensure the desired outcome?","In preparation for a future exam, I found the following problem: Let $$A = \begin{pmatrix} 1 & 0 & a & b \\ 0 & 1 & 0 & 0 \\ 0 & c & 3 & -2 \\ 0 & d & 2 & -1 \end{pmatrix}$$ Determine conditions on $a,b,c,d$ so that there is only one Jordan block for each eigenvalue of $A$ (in the Jordan canonical form of $A$). I found, readily enough, that the only eigenvalue of the $A$ is $\lambda = 1$. So the problem is in fact to find conditions of $a,b,c,d$ such that $A$ is similar to a single-block Jordan matrix. Thus we want the dimension of the eigenspace $E_1$ to be 1, or equivalently, the rank of $A-I$ to be 3. $$A-I = \begin{pmatrix} 0 & 0 & a & b \\ 0 & 0 & 0 & 0 \\ 0 & c & 2 & -2 \\ 0 & d & 2 & -2 \end{pmatrix}$$ So would the conditions $a,b$ not both zero, $a \neq -b$ and $c \neq d$ be sufficient? More to the point, what is the most economical way of stating the conditions on $a,b,c,d$ to ensure the desired outcome?",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
69,Is $I+AA^T$ positive definite matrix?,Is  positive definite matrix?,I+AA^T,"If $A$ is real matrix, how can i show that $I+AA^T$ is positive definite matrix? $I$ is the identity matrix and $A^T$ is a transpose of $A$.","If $A$ is real matrix, how can i show that $I+AA^T$ is positive definite matrix? $I$ is the identity matrix and $A^T$ is a transpose of $A$.",,"['linear-algebra', 'matrices']"
70,Powers of permutation matrices.,Powers of permutation matrices.,,"Let $P$ be a permutation matrix obtained by the identity matrix by switching 2 rows  $n$ times, (with no two rows switched more than one time). How to show that $$P^{\ n+1} = I$$? Is it true that, since $P$ represent a permutation of colums, it's like proving that if we take a set $\{1, \dots m\}$ and have a permutation that switches two elements $n$ times, with no two elements switched more than one time, and we apply this permutation $n+1$ times, we'll return to the original set?","Let $P$ be a permutation matrix obtained by the identity matrix by switching 2 rows  $n$ times, (with no two rows switched more than one time). How to show that $$P^{\ n+1} = I$$? Is it true that, since $P$ represent a permutation of colums, it's like proving that if we take a set $\{1, \dots m\}$ and have a permutation that switches two elements $n$ times, with no two elements switched more than one time, and we apply this permutation $n+1$ times, we'll return to the original set?",,"['linear-algebra', 'combinatorics', 'matrices', 'permutations']"
71,"$A,B \in \mathcal{M}_{n}(\mathbb{R})$ $ABA-BAB=I$ and $A^2B+B^2A=O$ $A,B$ are invertible",and   are invertible,"A,B \in \mathcal{M}_{n}(\mathbb{R}) ABA-BAB=I A^2B+B^2A=O A,B","Let $A,B$ be two matrices with $A,B \in \mathcal{M}_{n}(\mathbb{R})$ and $ABA-BAB=I$ and $A^2B+B^2A=0$. Prove that $A,B$ are invertible matrices.","Let $A,B$ be two matrices with $A,B \in \mathcal{M}_{n}(\mathbb{R})$ and $ABA-BAB=I$ and $A^2B+B^2A=0$. Prove that $A,B$ are invertible matrices.",,"['linear-algebra', 'matrices']"
72,Write the following in the form of AX = B,Write the following in the form of AX = B,,"Write the following system of equations in the form $AX = B$, and calculate the solution using the equation $X = A^{-1}B$. $$2x - 3y = - 1$$ $$-5x +5y = 20$$ I'm not the strongest at linear algebra but I don't understand what the question is asking me over here or how to even go about solving this.","Write the following system of equations in the form $AX = B$, and calculate the solution using the equation $X = A^{-1}B$. $$2x - 3y = - 1$$ $$-5x +5y = 20$$ I'm not the strongest at linear algebra but I don't understand what the question is asking me over here or how to even go about solving this.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'inverse']"
73,Why is $\det(e^X)=e^{\operatorname{tr}(X)}$? [duplicate],Why is ? [duplicate],\det(e^X)=e^{\operatorname{tr}(X)},"This question already has answers here : How to prove $\det \left(e^A\right) = e^{\operatorname{tr}(A)}$? (6 answers) Closed 10 years ago . I've seen on Wikipedia that for a complex matrix $X$, $\det(e^X)=e^{\operatorname{tr}(X)}$. It is clearly true for a diagonal matrix. What about other matrices ? The series-based definition of exp is useless here.","This question already has answers here : How to prove $\det \left(e^A\right) = e^{\operatorname{tr}(A)}$? (6 answers) Closed 10 years ago . I've seen on Wikipedia that for a complex matrix $X$, $\det(e^X)=e^{\operatorname{tr}(X)}$. It is clearly true for a diagonal matrix. What about other matrices ? The series-based definition of exp is useless here.",,"['matrices', 'exponential-function']"
74,Matrix equation $AX=XB$,Matrix equation,AX=XB,"For $A,B \in \big( \mathrm{Mat}_{n}(\mathbb{C}) \big)^2$, I know that there exists $Y \in \mathrm{Mat}_{n}(\mathbb{C})$, $Y \neq 0$, such as $AY=YB$ if and only if $\mathrm{Sp}_{\mathbb{C}}(A) \cap \mathrm{Sp}_{\mathbb{C}}(B) \neq \emptyset$. Here, $\mathrm{Sp}_{\mathbb{C}}(A)$ denotes the set of complex eigenvalues of $A$. The reason to this is the following : Let $u \, : \, \mathrm{Mat}_{n}(\mathbb{C}) \, \rightarrow \, \mathrm{Mat}_{n}(\mathbb{C}) \, ; \, M \, \mapsto \, AM-MB$. One can prove that $\mathrm{Sp}(u) = \mathrm{Sp}(A) - \mathrm{Sp}(B) = \left\{ a-b, \, (a,b) \in \mathrm{Sp}(A) \times \mathrm{Sp}(B) \right\}$. Now, I am wondering whether the result is still true for real matrices. If $A$ and $B$ are in $\mathrm{Mat}_{n}(\mathbb{R})$, can we still find $Y \in \mathrm{Mat}_{n}(\mathbb{R})$, $Y \neq 0$, such that $AY=YB$ while $\mathrm{Sp}_{\mathbb{R}}(A) \cap \mathrm{Sp}_{\mathbb{R}}(B) = \emptyset$ ? $\mathrm{Sp}_{\mathbb{R}}(A)$ denotes the set of real eigenvalues of $A$. I tried to build a counter-example taking, for example $A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ because it would ensure that $\mathrm{Sp}_{\mathbb{R}}(A) = \emptyset$ but I didn't succeed. This is not an homework question.","For $A,B \in \big( \mathrm{Mat}_{n}(\mathbb{C}) \big)^2$, I know that there exists $Y \in \mathrm{Mat}_{n}(\mathbb{C})$, $Y \neq 0$, such as $AY=YB$ if and only if $\mathrm{Sp}_{\mathbb{C}}(A) \cap \mathrm{Sp}_{\mathbb{C}}(B) \neq \emptyset$. Here, $\mathrm{Sp}_{\mathbb{C}}(A)$ denotes the set of complex eigenvalues of $A$. The reason to this is the following : Let $u \, : \, \mathrm{Mat}_{n}(\mathbb{C}) \, \rightarrow \, \mathrm{Mat}_{n}(\mathbb{C}) \, ; \, M \, \mapsto \, AM-MB$. One can prove that $\mathrm{Sp}(u) = \mathrm{Sp}(A) - \mathrm{Sp}(B) = \left\{ a-b, \, (a,b) \in \mathrm{Sp}(A) \times \mathrm{Sp}(B) \right\}$. Now, I am wondering whether the result is still true for real matrices. If $A$ and $B$ are in $\mathrm{Mat}_{n}(\mathbb{R})$, can we still find $Y \in \mathrm{Mat}_{n}(\mathbb{R})$, $Y \neq 0$, such that $AY=YB$ while $\mathrm{Sp}_{\mathbb{R}}(A) \cap \mathrm{Sp}_{\mathbb{R}}(B) = \emptyset$ ? $\mathrm{Sp}_{\mathbb{R}}(A)$ denotes the set of real eigenvalues of $A$. I tried to build a counter-example taking, for example $A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ because it would ensure that $\mathrm{Sp}_{\mathbb{R}}(A) = \emptyset$ but I didn't succeed. This is not an homework question.",,"['linear-algebra', 'matrices']"
75,Coefficient of $x^n$ in the series,Coefficient of  in the series,x^n,"How will we find the coefficient of $x^n$ in the following series: $$(1+x+2x^2+3x^3+...)^n$$ Please suggest if there is some formula or if it can be computed using the computer in $\log n$ time. I have figured out the differentiation approach which is slow. Thanks in advance.I am guessing matrix multiplication/exponentiation and linear algebra could help. Edit: I tried multinomial theorem too, but couldn't build on the solution as it requires the coefficients to be constant.","How will we find the coefficient of $x^n$ in the following series: $$(1+x+2x^2+3x^3+...)^n$$ Please suggest if there is some formula or if it can be computed using the computer in $\log n$ time. I have figured out the differentiation approach which is slow. Thanks in advance.I am guessing matrix multiplication/exponentiation and linear algebra could help. Edit: I tried multinomial theorem too, but couldn't build on the solution as it requires the coefficients to be constant.",,"['linear-algebra', 'combinatorics', 'matrices', 'binomial-coefficients', 'multinomial-coefficients']"
76,Determinant from matrix entirely composed of variables,Determinant from matrix entirely composed of variables,,"I don't want the answer, but I'd love to kick in the right direction. I'm really not sure how to approach this question. $$\begin{align} & -6 = det\begin{bmatrix} a & b & c \\  d & e & f \\  g & h & i \\ \end{bmatrix} \\ & x = det\begin{bmatrix} a & b & c \\ 2d & 2e & 2f \\ g+3a & h+3b & i+3c \\ \end{bmatrix} & \text{Solve for }x \end{align}$$ I believe if I set $a=1$, $e=2$, and $i=3$ (all other variables $0$), the determinant of the first matrix is $6$, and then for the second matrix is $12$. These were arbitrary variable initializations and can be any number. The relationship between the two (a scalar multiple of 2) will be the same irrespective of what I set the variables to. I can then infer that the determinant of the second matrix is $2*det[A]$ or $-12$ because I'm investigating the relationship between the two matrices rather than actually calculating anything. I imagine this is a cheap way out, though. But, from this method I do get $x = -12$, which I believe is the correct answer. What is the proper way of solving this? I don't want the answer, but I'd like to know the process.","I don't want the answer, but I'd love to kick in the right direction. I'm really not sure how to approach this question. $$\begin{align} & -6 = det\begin{bmatrix} a & b & c \\  d & e & f \\  g & h & i \\ \end{bmatrix} \\ & x = det\begin{bmatrix} a & b & c \\ 2d & 2e & 2f \\ g+3a & h+3b & i+3c \\ \end{bmatrix} & \text{Solve for }x \end{align}$$ I believe if I set $a=1$, $e=2$, and $i=3$ (all other variables $0$), the determinant of the first matrix is $6$, and then for the second matrix is $12$. These were arbitrary variable initializations and can be any number. The relationship between the two (a scalar multiple of 2) will be the same irrespective of what I set the variables to. I can then infer that the determinant of the second matrix is $2*det[A]$ or $-12$ because I'm investigating the relationship between the two matrices rather than actually calculating anything. I imagine this is a cheap way out, though. But, from this method I do get $x = -12$, which I believe is the correct answer. What is the proper way of solving this? I don't want the answer, but I'd like to know the process.",,"['matrices', 'determinant']"
77,"Possibility of making diagonal elements of a square matrix 1,if matrix has only 0 or 1","Possibility of making diagonal elements of a square matrix 1,if matrix has only 0 or 1",,"Let $M$ be an $n \times n$ matrix with each entry equal to either $0$ or $1$. Let $m_{i,j}$ denote the entry in row $i$ and column $j$. A diagonal entry is one of the form $m_{i,i}$ for some $i$. Swapping rows $i$ and $j$ of the matrix $M$ denotes the following action:we swap the values $m_{i,k}$ and $m_{j,k}$ for $k = 1, 2 ..... n$. Swapping two columns is defined analogously We say that $M$ is re-arrangeable if it is possible to swap some of the pairs of rows and some of the pairs of columns (in any sequence) so that, after all the swapping, all the diagonal entries of $M$ are equal to $1$. (a) Give an example of a matrix $M$ that is not re-arrangeable, but for which at least one entry in each row and each column is equal to $1$. (b) Give a polynomial-time algorithm that determines whether a matrix $M$ with $0-1$ entries is re-arrangeable. I tried a lot but could not reach to any conclusion please suggest me algorithm for that.","Let $M$ be an $n \times n$ matrix with each entry equal to either $0$ or $1$. Let $m_{i,j}$ denote the entry in row $i$ and column $j$. A diagonal entry is one of the form $m_{i,i}$ for some $i$. Swapping rows $i$ and $j$ of the matrix $M$ denotes the following action:we swap the values $m_{i,k}$ and $m_{j,k}$ for $k = 1, 2 ..... n$. Swapping two columns is defined analogously We say that $M$ is re-arrangeable if it is possible to swap some of the pairs of rows and some of the pairs of columns (in any sequence) so that, after all the swapping, all the diagonal entries of $M$ are equal to $1$. (a) Give an example of a matrix $M$ that is not re-arrangeable, but for which at least one entry in each row and each column is equal to $1$. (b) Give a polynomial-time algorithm that determines whether a matrix $M$ with $0-1$ entries is re-arrangeable. I tried a lot but could not reach to any conclusion please suggest me algorithm for that.",,"['combinatorics', 'matrices', 'transformation']"
78,"Combinatorics inside of $GL(n,q)$",Combinatorics inside of,"GL(n,q)","I'm studying conjugacy classes of subgroups of $GL(n,q)$ of the form $(\mathbb{Z}/p\mathbb{Z})^r$ where $q=p^d$ and $r$ is some non-negative integer.  I've been able to show that for $n=p=2$ and for $r=2$, there are $a_d$ such conjugacy classes, where $a_d$ is the sequence defined by the recurrence relation: $$a_m=a_{m-1}+2a_{m-2}, \quad a_1=0, a_2=1$$ For $n=p=2$, and considering $r=3$, I am finding the following sequence (using Magma) $$0,0,1,1,5,23,93,381,\ldots$$ and I would like to find a similar recurrence satisfied by these numbers.  The recurrence may not be linear as it is for the case $r=2$.  Can anybody help me find the relationship between the numbers I've listed here?  Thanks in advance.","I'm studying conjugacy classes of subgroups of $GL(n,q)$ of the form $(\mathbb{Z}/p\mathbb{Z})^r$ where $q=p^d$ and $r$ is some non-negative integer.  I've been able to show that for $n=p=2$ and for $r=2$, there are $a_d$ such conjugacy classes, where $a_d$ is the sequence defined by the recurrence relation: $$a_m=a_{m-1}+2a_{m-2}, \quad a_1=0, a_2=1$$ For $n=p=2$, and considering $r=3$, I am finding the following sequence (using Magma) $$0,0,1,1,5,23,93,381,\ldots$$ and I would like to find a similar recurrence satisfied by these numbers.  The recurrence may not be linear as it is for the case $r=2$.  Can anybody help me find the relationship between the numbers I've listed here?  Thanks in advance.",,"['combinatorics', 'sequences-and-series', 'group-theory', 'matrices', 'finite-groups']"
79,product of m x n matrix with n x m matrix,product of m x n matrix with n x m matrix,,"How to prove that product of  $\mathbb{m x n}$ matrix with $\mathbb{n x m}$ matrix is not invertible given $\mathbb{m >n}$. For the case of $\mathbb{2 x 1}$ and $\mathbb{1 x 2}$ matrix, it is clear; since for the product matrix A; $\mathbb{AX=0}$ has non trivial solutions. (both the resulting equations turn out to be same after cancellation of common factors. How to go about proving it for the general case?","How to prove that product of  $\mathbb{m x n}$ matrix with $\mathbb{n x m}$ matrix is not invertible given $\mathbb{m >n}$. For the case of $\mathbb{2 x 1}$ and $\mathbb{1 x 2}$ matrix, it is clear; since for the product matrix A; $\mathbb{AX=0}$ has non trivial solutions. (both the resulting equations turn out to be same after cancellation of common factors. How to go about proving it for the general case?",,"['linear-algebra', 'matrices']"
80,Is this relationship between spectral radius and singular values false?,Is this relationship between spectral radius and singular values false?,,"I found the following relationship $\max_{i} \sigma_i \le \rho(A)$, where $A$ is a matrix.  But somehow I do not trust this relation, I'd rather guess that the converse is true, but I do not know.","I found the following relationship $\max_{i} \sigma_i \le \rho(A)$, where $A$ is a matrix.  But somehow I do not trust this relation, I'd rather guess that the converse is true, but I do not know.",,['linear-algebra']
81,Is there meaning for $\bf uv^T$?,Is there meaning for ?,\bf uv^T,"Let $\bf{u,v}$ be two column vector in $\mathbb R^n$, which can be represented by $n\times1$ matrix. $\bf u^T v$ is the inner product of $\bf u,v$, then is there meaning for $\bf uv^T$, which is a $n\times n$ matrix? Thanks.","Let $\bf{u,v}$ be two column vector in $\mathbb R^n$, which can be represented by $n\times1$ matrix. $\bf u^T v$ is the inner product of $\bf u,v$, then is there meaning for $\bf uv^T$, which is a $n\times n$ matrix? Thanks.",,['matrices']
82,Diagonalizable matrix with only one eigenvalue,Diagonalizable matrix with only one eigenvalue,,"I have a question from a test I solved (without that question.. =)  ""If a matrix $A$ s.t $A$ is in $M(\mathbb{C})$ have only $1$ eigenvalue than $A$ is a diagonalizable matrix"" That is a false assumption since a ( $n\times n$ matrix) a square matrix needs to have at least $n$ different eigenvalues (to make eigenvectors from) - but doesn't the identity matrix have only $1$ eigenvalue?...","I have a question from a test I solved (without that question.. =)  ""If a matrix s.t is in have only eigenvalue than is a diagonalizable matrix"" That is a false assumption since a ( matrix) a square matrix needs to have at least different eigenvalues (to make eigenvectors from) - but doesn't the identity matrix have only eigenvalue?...",A A M(\mathbb{C}) 1 A n\times n n 1,"['matrices', 'eigenvalues-eigenvectors']"
83,The matrix notation of signum?,The matrix notation of signum?,,"The following question on a notation might look trivial but I am really not sure how to deal with it. If I have a variable $x$, I could write out: $$x=|x|\;\text {sgn} (x)$$ a notation that helps me with an operator for the signs that could point to $-1$, $0$ or $+1$. But then I have a matrix $\bf X$ with elements $x_{i,j}$ while the equation above holds for each element $x_{i,j}$, simply $$x_{i,j}=|x_{i,j}|\;\text {sgn} (x_{i,j})$$ How does the matrix notation for the equation above look like, in terms of a matrix of $\bf X$ (and not individual elements)?","The following question on a notation might look trivial but I am really not sure how to deal with it. If I have a variable $x$, I could write out: $$x=|x|\;\text {sgn} (x)$$ a notation that helps me with an operator for the signs that could point to $-1$, $0$ or $+1$. But then I have a matrix $\bf X$ with elements $x_{i,j}$ while the equation above holds for each element $x_{i,j}$, simply $$x_{i,j}=|x_{i,j}|\;\text {sgn} (x_{i,j})$$ How does the matrix notation for the equation above look like, in terms of a matrix of $\bf X$ (and not individual elements)?",,"['linear-algebra', 'matrices', 'number-theory', 'mathematical-physics']"
84,Matrix vector addition,Matrix vector addition,,"Given A a 3x3 matrix and B a 3x1 matrix (or column vector), I am asked to calculate A + B. Both are initially filled with one's. To my knowledge the two matrices would have to be of the same n×m dimensionions. However, if I do the addition in Python (using Numpy matrices and the '+' operator) I get a 3x3 matrix filled with two's. How is this matrix and column vector added? Might I be misunderstanding what Python does?","Given A a 3x3 matrix and B a 3x1 matrix (or column vector), I am asked to calculate A + B. Both are initially filled with one's. To my knowledge the two matrices would have to be of the same n×m dimensionions. However, if I do the addition in Python (using Numpy matrices and the '+' operator) I get a 3x3 matrix filled with two's. How is this matrix and column vector added? Might I be misunderstanding what Python does?",,"['matrices', 'arithmetic']"
85,Why is it that we can do these operations without changing the original stystem?,Why is it that we can do these operations without changing the original stystem?,,"I was given the matrix: $$\left[\begin{matrix}1&3&0&-2&-7\\0&1&0&3&6\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$ And I was told to continue the row operations to solve the solution for the original system. So for the first step I subtracted $3R_4$, where $R_4$ is row $4$, to get rid of the $x_2$ in the original system. To get: $$\left[\begin{matrix}1&3&0&-2&-7\\0&1&0&0&12\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$ Then what I did was $R_1-3R_2$ to get rid of the $x_2$: $$\left[\begin{matrix}1&0&0&-2&-43\\0&1&0&0&12\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$Then lastly $R_1+2R_4$ to get rid of the $x_4$ in $R_1$ $$\left[\begin{matrix}1&0&0&0&-47\\0&1&0&0&12\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$I'm curious as to why were able to do this to a matrix. It seems like these manipulations to the rows would make it different. It's strange to me that we can add a constant times another row to get a new, replacement row. Is there any theory behind this, or is it just that simple that we can just change it? Edit: I changed the question from ""did I do this right to"" a ""why can we do this"" just so theres no confusion about the current comments.","I was given the matrix: $$\left[\begin{matrix}1&3&0&-2&-7\\0&1&0&3&6\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$ And I was told to continue the row operations to solve the solution for the original system. So for the first step I subtracted $3R_4$, where $R_4$ is row $4$, to get rid of the $x_2$ in the original system. To get: $$\left[\begin{matrix}1&3&0&-2&-7\\0&1&0&0&12\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$ Then what I did was $R_1-3R_2$ to get rid of the $x_2$: $$\left[\begin{matrix}1&0&0&-2&-43\\0&1&0&0&12\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$Then lastly $R_1+2R_4$ to get rid of the $x_4$ in $R_1$ $$\left[\begin{matrix}1&0&0&0&-47\\0&1&0&0&12\\0&0&1&0&2\\0&0&0&1&-2\end{matrix}\right]$$I'm curious as to why were able to do this to a matrix. It seems like these manipulations to the rows would make it different. It's strange to me that we can add a constant times another row to get a new, replacement row. Is there any theory behind this, or is it just that simple that we can just change it? Edit: I changed the question from ""did I do this right to"" a ""why can we do this"" just so theres no confusion about the current comments.",,"['linear-algebra', 'matrices']"
86,Putnam type question: Invertible matrix,Putnam type question: Invertible matrix,,"Are the following matrices invertible? (1) $A= (a_{ij})_{2003 \times 2003}$, where $a_{ii}=2003, a_{ij}=1$ for $i \not=j$. (2) $B= (b_{ij})_{n \times n }$ with $b_{ii}= \pi$ and $b_{ij} \in \mathbb{Q}$ for $i \not= j$. Thank you so much.","Are the following matrices invertible? (1) $A= (a_{ij})_{2003 \times 2003}$, where $a_{ii}=2003, a_{ij}=1$ for $i \not=j$. (2) $B= (b_{ij})_{n \times n }$ with $b_{ii}= \pi$ and $b_{ij} \in \mathbb{Q}$ for $i \not= j$. Thank you so much.",,"['matrices', 'contest-math']"
87,Upper Triangular Form of a Matrix,Upper Triangular Form of a Matrix,,"I am trying to find the upper triangular form of $B$ and an invertible matrix $C$ such that $B=C^{-1}AC$ where A is given by the following: $A = \pmatrix{1&1\\           -1&3}$ The characteristic equation is $(x-2)^2$ with eigenvalue $2$ and eigenvector $\langle1, 1\rangle$. Finding an upper triangular matrix similar to $A$ means to find a vector $w$ such that $(A-2I)w = v$. In the above, is $v$ the eigenvector? Could someone explain both of the above to me and why this is true? Taking the above statements to be true we have: $\pmatrix{1&-1 \\           1&-1}\pmatrix{w_1 \\ w2} = \pmatrix{1 \\ 1}$ $\implies w_1 - w_2 = 1$ One solution is $\langle1, 0\rangle$. From here I do not know what to do next.","I am trying to find the upper triangular form of $B$ and an invertible matrix $C$ such that $B=C^{-1}AC$ where A is given by the following: $A = \pmatrix{1&1\\           -1&3}$ The characteristic equation is $(x-2)^2$ with eigenvalue $2$ and eigenvector $\langle1, 1\rangle$. Finding an upper triangular matrix similar to $A$ means to find a vector $w$ such that $(A-2I)w = v$. In the above, is $v$ the eigenvector? Could someone explain both of the above to me and why this is true? Taking the above statements to be true we have: $\pmatrix{1&-1 \\           1&-1}\pmatrix{w_1 \\ w2} = \pmatrix{1 \\ 1}$ $\implies w_1 - w_2 = 1$ One solution is $\langle1, 0\rangle$. From here I do not know what to do next.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
88,How to do $\frac{ \partial { \mathrm{tr}(XX^TXX^T)}}{\partial X}$,How to do,\frac{ \partial { \mathrm{tr}(XX^TXX^T)}}{\partial X},How to do the derivative  \begin{equation} \frac{ \partial {\mathrm{tr}(XX^TXX^T)}}{\partial X}\quad ? \end{equation} I have no idea where to start.,How to do the derivative  \begin{equation} \frac{ \partial {\mathrm{tr}(XX^TXX^T)}}{\partial X}\quad ? \end{equation} I have no idea where to start.,,"['calculus', 'matrices', 'multivariable-calculus', 'derivatives']"
89,The greatest possible geometric multiplicity of an eigenvalue,The greatest possible geometric multiplicity of an eigenvalue,,"Wikipedia claims that "" Given an n×n matrix A.... both algebraic and geometric multiplicity are integers between (including) 1 and n. "" But how can the geometric multiplicity possibly be n? Since $(A-\lambda I)$ is a square matrix (as opposed to a matrix with more columns than rows), each of A's eigenspaces $Nul (A-\lambda I)$  has at most $(n-1)$ dimensions, isn't it? I.e. The geometric multiplicity of an eigenvalue must be a number between between $1$ and $(n-1)$, right?","Wikipedia claims that "" Given an n×n matrix A.... both algebraic and geometric multiplicity are integers between (including) 1 and n. "" But how can the geometric multiplicity possibly be n? Since $(A-\lambda I)$ is a square matrix (as opposed to a matrix with more columns than rows), each of A's eigenspaces $Nul (A-\lambda I)$  has at most $(n-1)$ dimensions, isn't it? I.e. The geometric multiplicity of an eigenvalue must be a number between between $1$ and $(n-1)$, right?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
90,Any $X\in O^+(n)$ (orthogonal matrices with positive determinant) is the product of an even number of reflections?,Any  (orthogonal matrices with positive determinant) is the product of an even number of reflections?,X\in O^+(n),Any $X\in O^+(n)$ (orthogonal matrices with positive determinant) is the product of an even number of reflection? I am not able to prove this. Please help.,Any $X\in O^+(n)$ (orthogonal matrices with positive determinant) is the product of an even number of reflection? I am not able to prove this. Please help.,,['matrices']
91,Computing determinant of a specific matrix. [duplicate],Computing determinant of a specific matrix. [duplicate],,"This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 7 years ago . How to calculate the determinant of $$ A=(a_{i,j})_{n \times n}=\left( \begin{array}{ccccc} a&b&b& \cdots & b\\ b& a& b& \cdots& b\\ \vdots& \vdots& \vdots& \ddots&\cdots\\ b&b&b & \cdots&a \end{array} \right)? $$","This question already has answers here : Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 7 years ago . How to calculate the determinant of $$ A=(a_{i,j})_{n \times n}=\left( \begin{array}{ccccc} a&b&b& \cdots & b\\ b& a& b& \cdots& b\\ \vdots& \vdots& \vdots& \ddots&\cdots\\ b&b&b & \cdots&a \end{array} \right)? $$",,"['matrices', 'determinant']"
92,Is $A^{q+2}=A^2$ in $M_2(\mathbb{Z}/p\mathbb{Z})$?,Is  in ?,A^{q+2}=A^2 M_2(\mathbb{Z}/p\mathbb{Z}),"I'm wondering, why is it that for $q=(p^2-1)(p^2-p)$, that $A^{q+2}=A^2$ for any $A\in M_2(\mathbb{Z}/p\mathbb{Z})$? It's not hard to see that $GL_2(\mathbb{Z}/p\mathbb{Z})$ has order $(p^2-1)(p^2-p)$, and so $A^q=1$ if $A\in GL_2(\mathbb{Z}/p\mathbb{Z})$, and so the equation holds in that case. But if $A$ is not invertible, why does the equality still hold?","I'm wondering, why is it that for $q=(p^2-1)(p^2-p)$, that $A^{q+2}=A^2$ for any $A\in M_2(\mathbb{Z}/p\mathbb{Z})$? It's not hard to see that $GL_2(\mathbb{Z}/p\mathbb{Z})$ has order $(p^2-1)(p^2-p)$, and so $A^q=1$ if $A\in GL_2(\mathbb{Z}/p\mathbb{Z})$, and so the equation holds in that case. But if $A$ is not invertible, why does the equality still hold?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
93,Find out dimension of the eigenspace of a given linear transformation $T$,Find out dimension of the eigenspace of a given linear transformation,T,"Let $T:\mathbb{R^4}\rightarrow \mathbb{R^{4}}$ be defined by $T(x,y,z,w)=(x+y+5w,x+2y+w,-z+2w,5x+y+2z)$ then what would be the dimension of the eigenspace of $T$? One approach may be to find out eigenvalues and then eigenvectors. Is there any other approach that will consume less amount of time and calculation?","Let $T:\mathbb{R^4}\rightarrow \mathbb{R^{4}}$ be defined by $T(x,y,z,w)=(x+y+5w,x+2y+w,-z+2w,5x+y+2z)$ then what would be the dimension of the eigenspace of $T$? One approach may be to find out eigenvalues and then eigenvectors. Is there any other approach that will consume less amount of time and calculation?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,"Simplify the expression: $\sum_{i=0}^{\infty}\sum_{j=0}^i\frac{X^{i-j}}{(i-j)!}\cdot\frac{Y^j}{j!}$,where X and Y are square matrices(not commutative)","Simplify the expression: ,where X and Y are square matrices(not commutative)",\sum_{i=0}^{\infty}\sum_{j=0}^i\frac{X^{i-j}}{(i-j)!}\cdot\frac{Y^j}{j!},"I want to know how to simplify the following expression by using the fact that $\sum_{i=0}^\infty \frac{X^i}{i!}=e^X$. The expression to be simplified is as follows: $$\sum_{i=0}^{\infty} \sum_{j=0}^i \frac{X^{i-j}}{(i-j)!} \cdot \frac{Y^j}{j!}\;,$$ where  $X$ and $Y$ are square matrices (not commutative). (That is, $X\cdot Y \neq Y \cdot X$).","I want to know how to simplify the following expression by using the fact that $\sum_{i=0}^\infty \frac{X^i}{i!}=e^X$. The expression to be simplified is as follows: $$\sum_{i=0}^{\infty} \sum_{j=0}^i \frac{X^{i-j}}{(i-j)!} \cdot \frac{Y^j}{j!}\;,$$ where  $X$ and $Y$ are square matrices (not commutative). (That is, $X\cdot Y \neq Y \cdot X$).",,"['calculus', 'linear-algebra', 'sequences-and-series', 'matrices']"
95,How do you solve a least square problem with a noninvertible matrix?,How do you solve a least square problem with a noninvertible matrix?,,"How do you find a solution to a matrix $A$ that minimizes $\|x\|$ when $A^TA$ is  not invertible? The matrix is $$A = \pmatrix{1 &1&2&2\\1&2&3&4}$$ I don't know if this helps but also in the question above this one, we are asked to find all solutions to $Ax = \pmatrix{0\\11}$ Thank you.","How do you find a solution to a matrix $A$ that minimizes $\|x\|$ when $A^TA$ is  not invertible? The matrix is $$A = \pmatrix{1 &1&2&2\\1&2&3&4}$$ I don't know if this helps but also in the question above this one, we are asked to find all solutions to $Ax = \pmatrix{0\\11}$ Thank you.",,['matrices']
96,"Example of matrix $M\in GL_3(\mathbb{Z}/7\mathbb{Z})$ such that $\langle M; +, \cdot \rangle \simeq GF(7^3)$ and the multiplicative order of $M$ is 3",Example of matrix  such that  and the multiplicative order of  is 3,"M\in GL_3(\mathbb{Z}/7\mathbb{Z}) \langle M; +, \cdot \rangle \simeq GF(7^3) M","I would want to make an example of a matrix $M \in GL_3(\mathbb{Z}_7)$ such that $\langle M; +, \cdot \rangle \simeq GF(7^3)$ and the multiplicative order of $M$ is $3$. Any hints how to do that with a computer algebra system would be appreciated.","I would want to make an example of a matrix $M \in GL_3(\mathbb{Z}_7)$ such that $\langle M; +, \cdot \rangle \simeq GF(7^3)$ and the multiplicative order of $M$ is $3$. Any hints how to do that with a computer algebra system would be appreciated.",,"['abstract-algebra', 'matrices', 'galois-theory', 'finite-fields']"
97,Proving a Theorem based on Gerschgorin Theorem,Proving a Theorem based on Gerschgorin Theorem,,Here is the theorem as it appears in my textbook. I am so lost with it. For $A=(a_{ij}) \in \mathbb C^{n\times n}$ we have $$\rho(A) \leq \max_i\sum_j^n | a_{ij}|$$ where $\rho(A)$ is the spectral radius. I need to prove this. I have no idea how to do it. :( The textbook I am using is Matrices and Linear Transformations by Cullen.,Here is the theorem as it appears in my textbook. I am so lost with it. For $A=(a_{ij}) \in \mathbb C^{n\times n}$ we have $$\rho(A) \leq \max_i\sum_j^n | a_{ij}|$$ where $\rho(A)$ is the spectral radius. I need to prove this. I have no idea how to do it. :( The textbook I am using is Matrices and Linear Transformations by Cullen.,,"['linear-algebra', 'matrices', 'gershgorin-sets']"
98,Does Permuting the Rows of a Matrix $A$ Change the Absolute Row Sum of $A^{-1}$?,Does Permuting the Rows of a Matrix  Change the Absolute Row Sum of ?,A A^{-1},"For $A = (a_{ij})$ an $n \times n$ matrix, the absolute row sum of $A$ is $$ \|A\|_{\infty} = \max_{1 \leq i \leq n} \sum_{j=1}^{n} |a_{ij}|. $$ Let $A$ be a given $n \times n$ matrix and let $A_0$ be a matrix obtained by permuting the rows of $A$.  Do we always have $$ \|A^{-1}\|_{\infty} = \|A_{0}^{-1}\|_{\infty}? $$","For $A = (a_{ij})$ an $n \times n$ matrix, the absolute row sum of $A$ is $$ \|A\|_{\infty} = \max_{1 \leq i \leq n} \sum_{j=1}^{n} |a_{ij}|. $$ Let $A$ be a given $n \times n$ matrix and let $A_0$ be a matrix obtained by permuting the rows of $A$.  Do we always have $$ \|A^{-1}\|_{\infty} = \|A_{0}^{-1}\|_{\infty}? $$",,"['linear-algebra', 'matrices', 'normed-spaces']"
99,Question about Matrix Equations,Question about Matrix Equations,,"Suppose there is a matrix equation such that: $Ax = b$ where $A$ is given, which is an upper triangular matrix, and $b$ is given. Then, $x$ and $b$ are perturbed by vectors $p$ and $q$ such that: $A(x+p) = (b+q)$ The question asks to solve for $p$ in terms of $q$. To approach this, I am thinking of finding $x$ by using the first equation, and then plug that $x$ into the second equation. Then, I can solve $p$ in terms of $q$. Do you think this (plugging in $x$ to the second equation) is acceptable? That is, is $x$ the same in both equations?","Suppose there is a matrix equation such that: $Ax = b$ where $A$ is given, which is an upper triangular matrix, and $b$ is given. Then, $x$ and $b$ are perturbed by vectors $p$ and $q$ such that: $A(x+p) = (b+q)$ The question asks to solve for $p$ in terms of $q$. To approach this, I am thinking of finding $x$ by using the first equation, and then plug that $x$ into the second equation. Then, I can solve $p$ in terms of $q$. Do you think this (plugging in $x$ to the second equation) is acceptable? That is, is $x$ the same in both equations?",,"['linear-algebra', 'matrices']"
