,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is the tensor product important when we already have direct and semidirect products?,Why is the tensor product important when we already have direct and semidirect products?,,"Can anyone explain me as to why Tensor Products are important, and what makes Mathematician's to define them in such a manner. We already have Direct Product , Semi-direct products, so after all why do we need Tensor Product? The Definition of Tensor Product at Planet Math is confusing. Definition: Let $R$ be a commutative ring, and let $A, B$ be $R$-modules. There exists an $R$-module $A\otimes B$, called the tensor product of $A$ and $B$ over $R$, together with a canonical bilinear homomorphism $$\otimes: A\times B\rightarrow A\otimes B,$$ distinguished, up to isomorphism, by the following universal property. Every bilinear $R$-module homomorphism $$\phi: A\times B\rightarrow C,$$ lifts to a unique $R$-module homomorphism $$\tilde{\phi}: A\otimes B\rightarrow C,$$ such that $$\phi(a,b) = \tilde{\phi}(a\otimes b)$$ for all $a\in A,\; b\in B.$ The tensor product $A\otimes B$ can be constructed by taking the free $R$-module generated by all formal symbols $$a\otimes b,\quad a\in A,\;b\in B,$$ and quotienting by the obvious bilinear relations: \begin{align*}  (a_1+a_2)\otimes b &= a_1\otimes b + a_2\otimes b,\quad &&a_1,a_2\in  A,\; b\in B \\  a\otimes(b_1+b_2) &= a\otimes b_1 + a\otimes b_2,\quad &&a\in A,\;b_1,b_2\in  B \\  r(a\otimes b) &= (ra)\otimes b= a\otimes (rb)\quad &&a\in A,\;b\in  B,\; r\in R \end{align*} Also what do is the meaning of this statement:( Why do we need this? ) Every bilinear $R$-module homomorphism $$\phi: A\times B\rightarrow C,$$ lifts to a unique $R$-module homomorphism $$\tilde{\phi}: A\otimes B\rightarrow C,$$ such that $$\phi(a,b) = \tilde{\phi}(a\otimes b)$$ for all $a\in A,\; b\in B.$ Me and my friends have planned to study this topic today. So i hope i am not wrong in asking such questions.","Can anyone explain me as to why Tensor Products are important, and what makes Mathematician's to define them in such a manner. We already have Direct Product , Semi-direct products, so after all why do we need Tensor Product? The Definition of Tensor Product at Planet Math is confusing. Definition: Let $R$ be a commutative ring, and let $A, B$ be $R$-modules. There exists an $R$-module $A\otimes B$, called the tensor product of $A$ and $B$ over $R$, together with a canonical bilinear homomorphism $$\otimes: A\times B\rightarrow A\otimes B,$$ distinguished, up to isomorphism, by the following universal property. Every bilinear $R$-module homomorphism $$\phi: A\times B\rightarrow C,$$ lifts to a unique $R$-module homomorphism $$\tilde{\phi}: A\otimes B\rightarrow C,$$ such that $$\phi(a,b) = \tilde{\phi}(a\otimes b)$$ for all $a\in A,\; b\in B.$ The tensor product $A\otimes B$ can be constructed by taking the free $R$-module generated by all formal symbols $$a\otimes b,\quad a\in A,\;b\in B,$$ and quotienting by the obvious bilinear relations: \begin{align*}  (a_1+a_2)\otimes b &= a_1\otimes b + a_2\otimes b,\quad &&a_1,a_2\in  A,\; b\in B \\  a\otimes(b_1+b_2) &= a\otimes b_1 + a\otimes b_2,\quad &&a\in A,\;b_1,b_2\in  B \\  r(a\otimes b) &= (ra)\otimes b= a\otimes (rb)\quad &&a\in A,\;b\in  B,\; r\in R \end{align*} Also what do is the meaning of this statement:( Why do we need this? ) Every bilinear $R$-module homomorphism $$\phi: A\times B\rightarrow C,$$ lifts to a unique $R$-module homomorphism $$\tilde{\phi}: A\otimes B\rightarrow C,$$ such that $$\phi(a,b) = \tilde{\phi}(a\otimes b)$$ for all $a\in A,\; b\in B.$ Me and my friends have planned to study this topic today. So i hope i am not wrong in asking such questions.",,['abstract-algebra']
1,"If I know the order of every element in a group, do I know the group? [duplicate]","If I know the order of every element in a group, do I know the group? [duplicate]",,"This question already has answers here : Is a finite group uniquely determined by the orders of its elements? (3 answers) Closed 4 years ago . Suppose $G$ is a finite group and I know for every $k \leq |G|$ that exactly $n_k$ elements in $G$ have order $k$. Do I know what the group is? Is there a counterexample where two groups $G$ and $H$ have the same number of elements for each order, but $G$ is not isomorphic to $H$? I suspect that there is, but I haven't thought of one.","This question already has answers here : Is a finite group uniquely determined by the orders of its elements? (3 answers) Closed 4 years ago . Suppose $G$ is a finite group and I know for every $k \leq |G|$ that exactly $n_k$ elements in $G$ have order $k$. Do I know what the group is? Is there a counterexample where two groups $G$ and $H$ have the same number of elements for each order, but $G$ is not isomorphic to $H$? I suspect that there is, but I haven't thought of one.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'examples-counterexamples']"
2,Is Serge Lang's Algebra still worth reading?,Is Serge Lang's Algebra still worth reading?,,"Is Serge Lang's famous book Algebra nowadays still worth reading, or are there other, more modern books which are better suited for an overview over all areas of algebra? EDIT: My main concern is that the first edition of Algebra is already 48 years old. Even if there have probably been no fundamental new insight in Algebra which can be included in a first-year-graduate algebra course, the passage of the decades may have helped clarify what are the most important results and techniques as well as how they can be achieved with the least effort. In addition, I'm wondering whether the terminology and notation is nonstandard or out of fashion (for example Lang, calls integral domains entire rings , an expression which I have never seen anywhere else).","Is Serge Lang's famous book Algebra nowadays still worth reading, or are there other, more modern books which are better suited for an overview over all areas of algebra? EDIT: My main concern is that the first edition of Algebra is already 48 years old. Even if there have probably been no fundamental new insight in Algebra which can be included in a first-year-graduate algebra course, the passage of the decades may have helped clarify what are the most important results and techniques as well as how they can be achieved with the least effort. In addition, I'm wondering whether the terminology and notation is nonstandard or out of fashion (for example Lang, calls integral domains entire rings , an expression which I have never seen anywhere else).",,"['abstract-algebra', 'reference-request']"
3,Center-commutator duality,Center-commutator duality,,"I'm reading this article by Keith Conrad, on subgroup series. I'm having trouble with a statement he does at page 6: Any subgroup of $G$ which contains $[G,G]$ is normal in $G$ . He says this as evidence that commutator and center play dual roles, since any subgroup of $G$ contained in $Z(G)$ is normal in $G$ . Now, this I'm sure I understand, but I don't see how the quoted line holds. What I have read is that $[G,G]$ is the least normal subgroup of $G$ such that the quotient is abelian, which seems related. Also, while we're at it: are the center and the commutator ""really"" (as in, categorically) dual constructions? I'm quite a novice in category theory, so please excuse me if this question is trivial.","I'm reading this article by Keith Conrad, on subgroup series. I'm having trouble with a statement he does at page 6: Any subgroup of which contains is normal in . He says this as evidence that commutator and center play dual roles, since any subgroup of contained in is normal in . Now, this I'm sure I understand, but I don't see how the quoted line holds. What I have read is that is the least normal subgroup of such that the quotient is abelian, which seems related. Also, while we're at it: are the center and the commutator ""really"" (as in, categorically) dual constructions? I'm quite a novice in category theory, so please excuse me if this question is trivial.","G [G,G] G G Z(G) G [G,G] G","['abstract-algebra', 'group-theory']"
4,"Why is $\mathbb{Z}[\sqrt{-n}], n\ge 3$ not a UFD?",Why is  not a UFD?,"\mathbb{Z}[\sqrt{-n}], n\ge 3","I'm considering the ring $\mathbb{Z}[\sqrt{-n}]$, where $n\ge 3$ and square free. I want to see why it's not a UFD. I defined a norm for the ring by $|a+b\sqrt{-n}|=a^2+nb^2$. Using this I was able to show that $2$, $\sqrt{-n}$ and $1+\sqrt{-n}$ are all irreducible. Is there someway to conclude that $\mathbb{Z}[\sqrt{-n}]$ is not a UFD based on this? Thanks.","I'm considering the ring $\mathbb{Z}[\sqrt{-n}]$, where $n\ge 3$ and square free. I want to see why it's not a UFD. I defined a norm for the ring by $|a+b\sqrt{-n}|=a^2+nb^2$. Using this I was able to show that $2$, $\sqrt{-n}$ and $1+\sqrt{-n}$ are all irreducible. Is there someway to conclude that $\mathbb{Z}[\sqrt{-n}]$ is not a UFD based on this? Thanks.",,"['abstract-algebra', 'ring-theory', 'unique-factorization-domains']"
5,"Where does the word ""torsion"" in algebra come from?","Where does the word ""torsion"" in algebra come from?",,"Torsion is used to refer to elements of finite order under some binary operation. It doesn't seem to bear any relation to the ordinary everyday use of the word or with its use in differential geometry (which relates back to the ordinary use of the word). So how did it acquire this usage in algebra? I'm interested to understand the intuition behind why the word ""torsion"" was chosen for this notion, as well as when it was first used.","Torsion is used to refer to elements of finite order under some binary operation. It doesn't seem to bear any relation to the ordinary everyday use of the word or with its use in differential geometry (which relates back to the ordinary use of the word). So how did it acquire this usage in algebra? I'm interested to understand the intuition behind why the word ""torsion"" was chosen for this notion, as well as when it was first used.",,"['abstract-algebra', 'group-theory', 'terminology', 'math-history']"
6,"Finite Groups with exactly $n$ conjugacy classes $(n=2,3,...)$",Finite Groups with exactly  conjugacy classes,"n (n=2,3,...)","I am looking to classify (up to isomorphism) those finite groups $G$ with exactly 2 conjugacy classes. If $G$ is abelian, then each element forms its own conjugacy class, so only the cyclic group of order 2 works here. If $G$ is not abelian, I am less sure what is going on.  The center $Z(G)$ is trivial since each of it's elements also form their own conjugacy class.  Now assume $G-\{1_G\}$ is the other conjugacy class. The Class Equation says $|G|=|Z(G)|+\sum [G:C_G(x)]$ where the sum is taken over representatives from the conjugacy classes (not counting the singleton ones from the center).  (Here $C_G(x)=\{g\in G~|~gx=xg\}$ is the centralizer of $x$ in $G$.) For us this simplifies to $|G|-1=[G:C_G(x)]$ for any $x\in G-\{1_G\}$.  Therefore $|C_G(x)|=\frac{|G|}{|G|-1}$ is an integer.  But this can only happen when $|G|=2$ which we have already covered.  So does this mean up to isomorphism there is only one group with 2 conjugacy classes? If so, how would the argument change if we allowed 3 conjugacy classes?","I am looking to classify (up to isomorphism) those finite groups $G$ with exactly 2 conjugacy classes. If $G$ is abelian, then each element forms its own conjugacy class, so only the cyclic group of order 2 works here. If $G$ is not abelian, I am less sure what is going on.  The center $Z(G)$ is trivial since each of it's elements also form their own conjugacy class.  Now assume $G-\{1_G\}$ is the other conjugacy class. The Class Equation says $|G|=|Z(G)|+\sum [G:C_G(x)]$ where the sum is taken over representatives from the conjugacy classes (not counting the singleton ones from the center).  (Here $C_G(x)=\{g\in G~|~gx=xg\}$ is the centralizer of $x$ in $G$.) For us this simplifies to $|G|-1=[G:C_G(x)]$ for any $x\in G-\{1_G\}$.  Therefore $|C_G(x)|=\frac{|G|}{|G|-1}$ is an integer.  But this can only happen when $|G|=2$ which we have already covered.  So does this mean up to isomorphism there is only one group with 2 conjugacy classes? If so, how would the argument change if we allowed 3 conjugacy classes?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
7,Semi-direct v.s. Direct products,Semi-direct v.s. Direct products,,"What is the difference between a direct product and a semi-direct product in group theory? Based on what I can find, difference seems only to be the nature of the groups involved, where a direct product can involve any two groups and the semi-direct product only allows a normal subgroup $N$ of some group $G$ and another subgroup of $G$ that intersects trivially with $N$ . Is this all? What are the significance? Thank you.","What is the difference between a direct product and a semi-direct product in group theory? Based on what I can find, difference seems only to be the nature of the groups involved, where a direct product can involve any two groups and the semi-direct product only allows a normal subgroup of some group and another subgroup of that intersects trivially with . Is this all? What are the significance? Thank you.",N G G N,"['abstract-algebra', 'group-theory', 'semidirect-product', 'direct-product']"
8,Algebra: Best mental images,Algebra: Best mental images,,"I'm curious how people think of Algebras (in the universal sense, i.e., monoids, groups, rings, etc.). Cayley diagrams of groups with few generators are useful for thinking about group actions on itself. I know that a categorical approach is becoming more mainstream. For me, lattice theory is my fallback. Lattice theory is useful to remember the diamond morphism theorem and lattice morphism theorem. Whenever I need to remember if a group can be expressed as a semidirect product I look for two subgroups where their meet is the bottom of the subgroup lattice, the join is the top of this lattice and one of those subgroups is contained in the normal sublattice. I find this easier than to remember the formal definition since I've translated it to relations that are spacial in the lattice. Now I'm studying ideal theory and commutative algebra. I think of the zero set $\mathbb(V)$ as an up-set of the smallest prime ideal containing the element. I'm curious if this is a general way others have gone about thinking ""algebraically"".","I'm curious how people think of Algebras (in the universal sense, i.e., monoids, groups, rings, etc.). Cayley diagrams of groups with few generators are useful for thinking about group actions on itself. I know that a categorical approach is becoming more mainstream. For me, lattice theory is my fallback. Lattice theory is useful to remember the diamond morphism theorem and lattice morphism theorem. Whenever I need to remember if a group can be expressed as a semidirect product I look for two subgroups where their meet is the bottom of the subgroup lattice, the join is the top of this lattice and one of those subgroups is contained in the normal sublattice. I find this easier than to remember the formal definition since I've translated it to relations that are spacial in the lattice. Now I'm studying ideal theory and commutative algebra. I think of the zero set $\mathbb(V)$ as an up-set of the smallest prime ideal containing the element. I'm curious if this is a general way others have gone about thinking ""algebraically"".",,"['abstract-algebra', 'group-theory', 'ring-theory', 'intuition', 'visualization']"
9,What lies beyond the Sedenions,What lies beyond the Sedenions,,"In the construction of types of numbers, we have the following sequence: $$\mathbb{R} \subset \mathbb{C} \subset \mathbb{H} \subset \mathbb{O} \subset \mathbb{S}$$ or: $$2^0 \mathrm{-ions} \subset 2^1 \mathrm{-ions} \subset 2^2 \mathrm{-ions} \subset 2^3 \mathrm{-ions} \subset 2^4 \mathrm{-ions} $$ or: ""Reals"" $\subset$ ""Complex"" $\subset$ ""Quaternions"" $\subset$ ""Octonions"" $\subset$ ""Sedenions"" With the following ""properties"": From $\mathbb{R}$ to $\mathbb{C}$ you gain ""algebraic-closure""-ness (but you throw away ordering). From $\mathbb{C}$ to $\mathbb{H}$ we throw away commutativity. From $\mathbb{H}$ to $\mathbb{O}$ we throw away associativity. From $\mathbb{O}$ to $\mathbb{S}$ we throw away multiplicative normedness. The question is, what lies on the right side of $\mathbb{S}$, and what do you lose when you go from $\mathbb{S}$ to one of these objects ?","In the construction of types of numbers, we have the following sequence: $$\mathbb{R} \subset \mathbb{C} \subset \mathbb{H} \subset \mathbb{O} \subset \mathbb{S}$$ or: $$2^0 \mathrm{-ions} \subset 2^1 \mathrm{-ions} \subset 2^2 \mathrm{-ions} \subset 2^3 \mathrm{-ions} \subset 2^4 \mathrm{-ions} $$ or: ""Reals"" $\subset$ ""Complex"" $\subset$ ""Quaternions"" $\subset$ ""Octonions"" $\subset$ ""Sedenions"" With the following ""properties"": From $\mathbb{R}$ to $\mathbb{C}$ you gain ""algebraic-closure""-ness (but you throw away ordering). From $\mathbb{C}$ to $\mathbb{H}$ we throw away commutativity. From $\mathbb{H}$ to $\mathbb{O}$ we throw away associativity. From $\mathbb{O}$ to $\mathbb{S}$ we throw away multiplicative normedness. The question is, what lies on the right side of $\mathbb{S}$, and what do you lose when you go from $\mathbb{S}$ to one of these objects ?",,"['abstract-algebra', 'complex-numbers', 'quaternions', 'sedenions']"
10,Why are two permutations conjugate iff they have the same cycle structure?,Why are two permutations conjugate iff they have the same cycle structure?,,I have heard that two permutations are conjugate if they have the same cyclic structure. Is there an intuitive way to understand why this is?,I have heard that two permutations are conjugate if they have the same cyclic structure. Is there an intuitive way to understand why this is?,,"['abstract-algebra', 'group-theory', 'permutations', 'symmetric-groups']"
11,Abstract nonsense proof of snake lemma,Abstract nonsense proof of snake lemma,,"During my studies, I always wanted to see a ""purely category-theoretical"" proof of the Snake Lemma, i.e. a proof that constructs all morphisms (including the snake) and proves exactness via universal properties. It was an interest little shared by my teachers and fellow students, but I have recently found the time to pursue it again. There is a wonderful book on category theory containing such a proof: The Handbook of Categorical Algebra , Volume 2, by Francis Borceux. I have a question about the proof, however, which I can't seem to resolve. The Snake Lemma is Lemma 1.10.9, and I have a problem with one of the preliminaries: Namely, the ""restricted"" Snake Lemma 1.10.8. Edit: I scanned the diagrams in question from the book. The following is what we want, i.e. we want to construct $\omega$ from the rest of the diagram where all squares commute and all rows and columns are exact. The construction is then as follows: $\Delta$ and $\Gamma$ are obtained by pull-back and we define $\Sigma:=\mathrm{Ker}(\Delta)$. Dually with $\Lambda$, $\Xi$ and $\Upsilon$. On page 46, he says that By lemma 1.10.1 and its dual, there are morphisms $\Psi$ and $\Omega$ making the diagram commutative and the outer columns exact. I can not verify this statement. For instance concerning $\Psi$, it seems to me that in order to apply lemma 1.10.1, one would require that the sequence $(\Gamma,\lambda)$ is exact, but I do not see how that would follow from the construction. What am I doing wrong?! Edit: Lemma 1.10.1 is the statement that in the following diagram, with commutative squares (1) and (2) and exact rows $(\zeta,\eta)$ and $(0,\nu,\xi)$ with $\gamma=\mathrm{Ker}(\theta)$, $\delta=\mathrm{Ker}(\lambda)$ and $\varepsilon=\mathrm{Ker}(\mu)$, there exist unique morphisms $\alpha$ and $\beta$ making the diagram commutative. Additionally, $(\alpha,\beta)$ is exact.","During my studies, I always wanted to see a ""purely category-theoretical"" proof of the Snake Lemma, i.e. a proof that constructs all morphisms (including the snake) and proves exactness via universal properties. It was an interest little shared by my teachers and fellow students, but I have recently found the time to pursue it again. There is a wonderful book on category theory containing such a proof: The Handbook of Categorical Algebra , Volume 2, by Francis Borceux. I have a question about the proof, however, which I can't seem to resolve. The Snake Lemma is Lemma 1.10.9, and I have a problem with one of the preliminaries: Namely, the ""restricted"" Snake Lemma 1.10.8. Edit: I scanned the diagrams in question from the book. The following is what we want, i.e. we want to construct $\omega$ from the rest of the diagram where all squares commute and all rows and columns are exact. The construction is then as follows: $\Delta$ and $\Gamma$ are obtained by pull-back and we define $\Sigma:=\mathrm{Ker}(\Delta)$. Dually with $\Lambda$, $\Xi$ and $\Upsilon$. On page 46, he says that By lemma 1.10.1 and its dual, there are morphisms $\Psi$ and $\Omega$ making the diagram commutative and the outer columns exact. I can not verify this statement. For instance concerning $\Psi$, it seems to me that in order to apply lemma 1.10.1, one would require that the sequence $(\Gamma,\lambda)$ is exact, but I do not see how that would follow from the construction. What am I doing wrong?! Edit: Lemma 1.10.1 is the statement that in the following diagram, with commutative squares (1) and (2) and exact rows $(\zeta,\eta)$ and $(0,\nu,\xi)$ with $\gamma=\mathrm{Ker}(\theta)$, $\delta=\mathrm{Ker}(\lambda)$ and $\varepsilon=\mathrm{Ker}(\mu)$, there exist unique morphisms $\alpha$ and $\beta$ making the diagram commutative. Additionally, $(\alpha,\beta)$ is exact.",,"['abstract-algebra', 'category-theory', 'homological-algebra', 'abelian-categories', 'diagram-chasing']"
12,Algebraic Topology Challenge: Homology of an Infinite Wedge of Spheres,Algebraic Topology Challenge: Homology of an Infinite Wedge of Spheres,,"So the following comes to me from an old algebraic topology final that got the best of me.  I wasn't able to prove it due to a lack of technical confidence, and my topology has only deteriorated since then.  But, I'm hoping maybe someone can figure out the proof as I've always been interested in seeing it all at once! Let $E_\infty$ denote the 2-D analogue of the Hawaiian Earring, i.e. $$E_\infty = \bigcup_{n=1}^\infty \{(x,y,z) \in \mathbb{R}^3 | \hspace{2mm} (x-1/n)^2 + y^2 + z^2 = 1/n^2\}.$$ The object of the exercise is to show that even though $E_\infty$ is 2-dimensional, $H_3(E_\infty) \neq 0$ , which I find interesting even though I know it's not a CW-complex. Parts of the proof were helped along by my professor to serve as a road map for the solution, but sadly I'm still lost in the driveway filling in some of the remaining bits: Let $h: S^3 \longrightarrow S^2$ denote the Hopf map i.e. the attaching map for the 4-cell in $\mathbb{C}P^2$ .  One can readily observe that there is a continuous map $\tilde{h}: S^3 \longrightarrow E_\infty$ so that the projection of $\tilde{h}$ to any $S^2$ in $E_\infty$ is homotopic to the Hopf map. Let $C_\tilde{h}$ denote the mapping cone of $\tilde{h}$ .  Then we have a mapping cones that looks something like a LOT of $\mathbb{C}P^2$ 's. (1) First we must prove that $H^2(C_\tilde{h})$ contains a subgroup $\mathbb{Z}<\zeta_1, \zeta_2, ...>$ and $H^4(C_\tilde{h}) \cong \mathbb{Z}<\eta>$ where $$\zeta_i \cup \zeta_i = \eta \text{ and } \zeta_i \cup \zeta_j = 0 \text{ for } i \neq j.$$ (I suspect that this comes in part from the cup product structure on $\mathbb{C}P^2$ ). Now let $[S^3]$ denote the fundamental class of $S^3$ . Assumption: Suppose that $\tilde{h}_* = 0 \in H_3(E_\infty)$ .  Under this assumption, there is a finite simplicial complex $X$ with boundary $S^3$ so that $\tilde{h}$ extends to a map $k: X \longrightarrow E_\infty$ .  Let $Y = X \cup \mathbb{D}^4$ where $\mathbb{D}^4$ is glued to $S^3$ in the obvious way.  Then extending $k$ , we have another map $l: Y \longrightarrow C_\tilde{h}$ sending the $B^4$ to the cone $S^3 \times [0,1]/ S^3 \times 1$ . (2) Here it must be proven that $l^*(\eta)$ is a nontrivial element of $H^4(Y)$ . (I truly do not see how to do this, but it seems like it would be true [here ""seems"" doesn't really mean anything]). (3) And now it needs to be shown that the infinitely many $l^*(\zeta_i)$ are all linearly independent. (This would seem to follow from naturality of the cup product in some way). If I could prove these things, it would only remain to observe that $Y$ came from gluing to a finite simplicial complex, and therefore it is itself a finite simplicial complex.  Hence $H^2(Y)$ is finitely generated, so we arrive at the desired contradiction, which is awesome since we have found a nontrivial element in the third homology group of a 2-D object!  Awesome! Anyways, if anyone could complete this proof in its entirety, I would be supremely grateful.  I assure you that I will upvote it a hundred times over! Even though the last 99 won't really do much. Also sorry that there are sort of several questions embedded in one.  I thought it would be justified as they lie in the same vein of the single proof.","So the following comes to me from an old algebraic topology final that got the best of me.  I wasn't able to prove it due to a lack of technical confidence, and my topology has only deteriorated since then.  But, I'm hoping maybe someone can figure out the proof as I've always been interested in seeing it all at once! Let denote the 2-D analogue of the Hawaiian Earring, i.e. The object of the exercise is to show that even though is 2-dimensional, , which I find interesting even though I know it's not a CW-complex. Parts of the proof were helped along by my professor to serve as a road map for the solution, but sadly I'm still lost in the driveway filling in some of the remaining bits: Let denote the Hopf map i.e. the attaching map for the 4-cell in .  One can readily observe that there is a continuous map so that the projection of to any in is homotopic to the Hopf map. Let denote the mapping cone of .  Then we have a mapping cones that looks something like a LOT of 's. (1) First we must prove that contains a subgroup and where (I suspect that this comes in part from the cup product structure on ). Now let denote the fundamental class of . Assumption: Suppose that .  Under this assumption, there is a finite simplicial complex with boundary so that extends to a map .  Let where is glued to in the obvious way.  Then extending , we have another map sending the to the cone . (2) Here it must be proven that is a nontrivial element of . (I truly do not see how to do this, but it seems like it would be true [here ""seems"" doesn't really mean anything]). (3) And now it needs to be shown that the infinitely many are all linearly independent. (This would seem to follow from naturality of the cup product in some way). If I could prove these things, it would only remain to observe that came from gluing to a finite simplicial complex, and therefore it is itself a finite simplicial complex.  Hence is finitely generated, so we arrive at the desired contradiction, which is awesome since we have found a nontrivial element in the third homology group of a 2-D object!  Awesome! Anyways, if anyone could complete this proof in its entirety, I would be supremely grateful.  I assure you that I will upvote it a hundred times over! Even though the last 99 won't really do much. Also sorry that there are sort of several questions embedded in one.  I thought it would be justified as they lie in the same vein of the single proof.","E_\infty E_\infty = \bigcup_{n=1}^\infty \{(x,y,z) \in \mathbb{R}^3 | \hspace{2mm} (x-1/n)^2 + y^2 + z^2 = 1/n^2\}. E_\infty H_3(E_\infty) \neq 0 h: S^3 \longrightarrow S^2 \mathbb{C}P^2 \tilde{h}: S^3 \longrightarrow E_\infty \tilde{h} S^2 E_\infty C_\tilde{h} \tilde{h} \mathbb{C}P^2 H^2(C_\tilde{h}) \mathbb{Z}<\zeta_1, \zeta_2, ...> H^4(C_\tilde{h}) \cong \mathbb{Z}<\eta> \zeta_i \cup \zeta_i = \eta \text{ and } \zeta_i \cup \zeta_j = 0 \text{ for } i \neq j. \mathbb{C}P^2 [S^3] S^3 \tilde{h}_* = 0 \in H_3(E_\infty) X S^3 \tilde{h} k: X \longrightarrow E_\infty Y = X \cup \mathbb{D}^4 \mathbb{D}^4 S^3 k l: Y \longrightarrow C_\tilde{h} B^4 S^3 \times [0,1]/ S^3 \times 1 l^*(\eta) H^4(Y) l^*(\zeta_i) Y H^2(Y)","['abstract-algebra', 'algebraic-topology', 'homological-algebra', 'homology-cohomology']"
13,$A_4$ has no subgroup of order $6$?,has no subgroup of order ?,A_4 6,"Can a kind algebraist offer an improvement to this sketch of a proof? Show that $A_4$ has no subgroup of order 6. Note, $|A_4|= 4!/2 =12$. Suppose $A_4>H, |H|=6$. Then $|A_4/H| = [A_4:H]=2$. So $H \vartriangleleft A_4$ so consider the homomorphism $\pi : A_4 \rightarrow A_4/H$ let $x \in A_4$ with $|x|=3$ (i.e. in a 3-cycle) then 3 divides $|\pi(x)|$ so as $|A_4/H|=2$  we have $|\pi(x)|$ divides 2 so $\pi(x) = e_H$ so $x \in H$ so $H$ contains all 3-cycles but $A_4$ has $8$ $3$-cycles $8>6$, $A_4$ has no subgroup of order 6.","Can a kind algebraist offer an improvement to this sketch of a proof? Show that $A_4$ has no subgroup of order 6. Note, $|A_4|= 4!/2 =12$. Suppose $A_4>H, |H|=6$. Then $|A_4/H| = [A_4:H]=2$. So $H \vartriangleleft A_4$ so consider the homomorphism $\pi : A_4 \rightarrow A_4/H$ let $x \in A_4$ with $|x|=3$ (i.e. in a 3-cycle) then 3 divides $|\pi(x)|$ so as $|A_4/H|=2$  we have $|\pi(x)|$ divides 2 so $\pi(x) = e_H$ so $x \in H$ so $H$ contains all 3-cycles but $A_4$ has $8$ $3$-cycles $8>6$, $A_4$ has no subgroup of order 6.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'permutations', 'symmetric-groups']"
14,Is there a third dimension of numbers?,Is there a third dimension of numbers?,,"Is there a third dimension of numbers like real numbers, imaginary numbers, [blank] numbers?","Is there a third dimension of numbers like real numbers, imaginary numbers, [blank] numbers?",,"['abstract-algebra', 'number-systems']"
15,What is difference between a ring and a field?,What is difference between a ring and a field?,,"The ring axioms require that addition is commutative, addition and multiplication are associative, multiplication distributes over addition. A field can be thought of as two groups with extra distributivity law. A ring is more complex: with abelian group and a semigroup with extra distributivity law. Is a ring a more basic structure than a field, or vice versa? What's the relation between them? What's the background why people study them?","The ring axioms require that addition is commutative, addition and multiplication are associative, multiplication distributes over addition. A field can be thought of as two groups with extra distributivity law. A ring is more complex: with abelian group and a semigroup with extra distributivity law. Is a ring a more basic structure than a field, or vice versa? What's the relation between them? What's the background why people study them?",,"['abstract-algebra', 'ring-theory', 'field-theory']"
16,Why is the localization at a prime ideal a local ring?,Why is the localization at a prime ideal a local ring?,,"I would like to know, why $ \mathfrak{p} A_{\mathfrak{p}} $ is the maximal ideal of the local ring $ A_{\mathfrak{p}} $, where $ \mathfrak{p} $ is a prime ideal of $ A $ and $ A_{\mathfrak{p}} $ is the localization of the ring $ A $ with respect to the multiplicative set $ S = A -\mathfrak{p} $ ? Thanks a lot. N.B. : I have to tell you that I'm not very good at Algebra, so please, be more kind and generous in your explanation, and give me a lot of details about this subject please. Thank you.","I would like to know, why $ \mathfrak{p} A_{\mathfrak{p}} $ is the maximal ideal of the local ring $ A_{\mathfrak{p}} $, where $ \mathfrak{p} $ is a prime ideal of $ A $ and $ A_{\mathfrak{p}} $ is the localization of the ring $ A $ with respect to the multiplicative set $ S = A -\mathfrak{p} $ ? Thanks a lot. N.B. : I have to tell you that I'm not very good at Algebra, so please, be more kind and generous in your explanation, and give me a lot of details about this subject please. Thank you.",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals', 'localization']"
17,Do harmonic numbers have a “closed-form” expression?,Do harmonic numbers have a “closed-form” expression?,,"One of the joys of high-school mathematics is summing a complicated series to get a “closed-form” expression. And of course many of us have tried summing the harmonic series $H_n =\sum \limits_{k \leq n} \frac{1}{k}$, and failed. But should we necessarily fail? More precisely, is it known that $H_n$ cannot be written in terms of the elementary functions, say, the rational functions, $\exp(x)$ and $\ln x$? If so, how is such a theorem proved? Note . When I started writing the question, I was going to ask if it is known that the harmonic function cannot be represented simply as a rational function? But this is easy to see, since $H_n$ grows like $\ln n+O(1)$, whereas no rational function grows logarithmically. Added note: This earlier question asks a similar question for “elementary integration”. I guess I am asking if there is an analogous theory of “elementary summation”.","One of the joys of high-school mathematics is summing a complicated series to get a “closed-form” expression. And of course many of us have tried summing the harmonic series $H_n =\sum \limits_{k \leq n} \frac{1}{k}$, and failed. But should we necessarily fail? More precisely, is it known that $H_n$ cannot be written in terms of the elementary functions, say, the rational functions, $\exp(x)$ and $\ln x$? If so, how is such a theorem proved? Note . When I started writing the question, I was going to ask if it is known that the harmonic function cannot be represented simply as a rational function? But this is easy to see, since $H_n$ grows like $\ln n+O(1)$, whereas no rational function grows logarithmically. Added note: This earlier question asks a similar question for “elementary integration”. I guess I am asking if there is an analogous theory of “elementary summation”.",,"['abstract-algebra', 'number-theory', 'functions', 'closed-form', 'harmonic-numbers']"
18,"Intuition behind ""ideal""","Intuition behind ""ideal""",,"To briefly put forward my question, can anyone beautifully explain me in your own view, what was the main intuition behind inventing the ideal of a ring? I want a clarified explanations in these points: Why is the name ""ideal"" coined?. In English 'ideal' means ""One that is regarded as a standard or model of perfection or excellence."" Why did people gave the name of ideal to such group? And why are the ideals not present in the case of groups? And give me a very fantastic intuition and motivation behind the ideals and what are the roles served by them in advanced mathematics. Thanks a lot, to every one.","To briefly put forward my question, can anyone beautifully explain me in your own view, what was the main intuition behind inventing the ideal of a ring? I want a clarified explanations in these points: Why is the name ""ideal"" coined?. In English 'ideal' means ""One that is regarded as a standard or model of perfection or excellence."" Why did people gave the name of ideal to such group? And why are the ideals not present in the case of groups? And give me a very fantastic intuition and motivation behind the ideals and what are the roles served by them in advanced mathematics. Thanks a lot, to every one.",,"['number-theory', 'abstract-algebra', 'ideals']"
19,Every nonzero element in a finite ring is either a unit or a zero divisor,Every nonzero element in a finite ring is either a unit or a zero divisor,,Let $R$ be a finite ring with unity. Prove that every nonzero element of $R$ is either a unit or a zero-divisor.,Let $R$ be a finite ring with unity. Prove that every nonzero element of $R$ is either a unit or a zero-divisor.,,"['abstract-algebra', 'ring-theory', 'finite-rings']"
20,Subgroups of finitely generated groups are not necessarily finitely generated,Subgroups of finitely generated groups are not necessarily finitely generated,,"I was wondering this today, and my algebra professor didn't know the answer. Are subgroups of finitely generated groups also finitely generated? I suppose it is necessarily true for finitely generated abelian groups, but is it true in general? And if not, is there a simple example of a finitely generated group with a non-finitely generated subgroup? NOTE :  This question has been merged with another question, asked by an undergraduate.  For an example not involving free groups, please see Andreas Caranti's answer, which was the accepted answer on the merged question.","I was wondering this today, and my algebra professor didn't know the answer. Are subgroups of finitely generated groups also finitely generated? I suppose it is necessarily true for finitely generated abelian groups, but is it true in general? And if not, is there a simple example of a finitely generated group with a non-finitely generated subgroup? NOTE :  This question has been merged with another question, asked by an undergraduate.  For an example not involving free groups, please see Andreas Caranti's answer, which was the accepted answer on the merged question.",,"['abstract-algebra', 'group-theory']"
21,"What are exact sequences, metaphysically speaking?","What are exact sequences, metaphysically speaking?",,"Why is it natural or useful to organize objects (of some appropriate category) into exact sequences? Exact sequences are ubiquitous - and I've encountered them enough to know that they can provide a very useful and efficient framework to work within. However, I have no idea what this framework truly is, or why it is effective. So, my questions are: What makes exact sequences natural objects to deal with? What do they encode, generally speaking? Or if you are unable to think of a satisfactory answer in general, what are some specific examples of exact sequences encoding some desirable property? Please set me straight! It seems like all of the references that I've come across only encyclopedically develop the idea of an exact sequence, sparing the reader of any qualification or exposition.","Why is it natural or useful to organize objects (of some appropriate category) into exact sequences? Exact sequences are ubiquitous - and I've encountered them enough to know that they can provide a very useful and efficient framework to work within. However, I have no idea what this framework truly is, or why it is effective. So, my questions are: What makes exact sequences natural objects to deal with? What do they encode, generally speaking? Or if you are unable to think of a satisfactory answer in general, what are some specific examples of exact sequences encoding some desirable property? Please set me straight! It seems like all of the references that I've come across only encyclopedically develop the idea of an exact sequence, sparing the reader of any qualification or exposition.",,"['abstract-algebra', 'commutative-algebra', 'category-theory', 'homological-algebra', 'abelian-categories']"
22,Is there an easy way to see associativity or non-associativity from an operation's table?,Is there an easy way to see associativity or non-associativity from an operation's table?,,"Most properties of a single binary operation can be easily read of from the operation's table. For example, given $$\begin{array}{c|ccccc} \cdot & a & b & c & d & e\\\hline a     & e & d & b & a & c\\ b     & d & c & e & b & a\\ c     & b & e & a & c & d\\ d     & a & b & c & d & e\\ e     & c & a & d & e & b \end{array}$$ it is easy to check that it is closed (no elements occur in the table which don't occur as row or column index), commutative (the table is symmetric), has a neutral element (the row and column of $d$ are copies of the index row/column), and has an inverse element for each element (there's a $d$ in each row and column). In other words, almost all important properties can immediately be seen. The only part missing is associativity. Therefore my question: Is there a simple way to see directly from the operation's table (i.e. without doing explicitly all the calculations) if an operation is associative?","Most properties of a single binary operation can be easily read of from the operation's table. For example, given $$\begin{array}{c|ccccc} \cdot & a & b & c & d & e\\\hline a     & e & d & b & a & c\\ b     & d & c & e & b & a\\ c     & b & e & a & c & d\\ d     & a & b & c & d & e\\ e     & c & a & d & e & b \end{array}$$ it is easy to check that it is closed (no elements occur in the table which don't occur as row or column index), commutative (the table is symmetric), has a neutral element (the row and column of $d$ are copies of the index row/column), and has an inverse element for each element (there's a $d$ in each row and column). In other words, almost all important properties can immediately be seen. The only part missing is associativity. Therefore my question: Is there a simple way to see directly from the operation's table (i.e. without doing explicitly all the calculations) if an operation is associative?",,"['abstract-algebra', 'finite-groups', 'associativity', 'magma', 'cayley-table']"
23,Examples of infinite groups such that all their respective elements are of finite order.,Examples of infinite groups such that all their respective elements are of finite order.,,I am in need of examples of infinite groups such that all their respective elements are of finite order.,I am in need of examples of infinite groups such that all their respective elements are of finite order.,,"['abstract-algebra', 'group-theory', 'examples-counterexamples']"
24,"If $a^3 =a$ for all $a$ in a ring $R$, then $R$ is commutative.","If  for all  in a ring , then  is commutative.",a^3 =a a R R,"Let $R$ be a ring, where $a^{3} = a$ for all $a\in R$. Prove that $R$ must be a commutative ring.","Let $R$ be a ring, where $a^{3} = a$ for all $a\in R$. Prove that $R$ must be a commutative ring.",,"['abstract-algebra', 'ring-theory']"
25,Should I be worried that I am doing well in analysis and not well in algebra? [closed],Should I be worried that I am doing well in analysis and not well in algebra? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 9 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I attend a mostly liberal arts focused university, in which I was able to test out of an ""Introduction to Proofs"" class and directly into ""Advanced Calculus 1"" (Introductory Analysis I) and I loved it. I did great in the class. I was not very mathematically mature at the time, but I studied hard and started to outpace many of the senior level students who had a least a good year or more of experience than me. Furthermore, the professor teaching the course was apparently known to be particularly difficult, but I loved his course. I enjoyed the challenge and wound up with a B+, the 2nd highest grade given in the class. I took Advanced Calculus 2 and loved it even more. The professor even suggested that I take a graduate complex analysis course in the Fall. (Just a side note here, the undergraduate complex analysis course at my school does not use any theorem's or proofs. The grad version is similar to say, an honors undergraduate course at more traditional math program.) I took this as a high complement, and a verification that I was in fact doing well. I know I am not very deep into analysis, but I feel comfortable with the subject, even with the more abstract parts. However, I am really struggling with abstract algebra. I can't understand why. I study the material really hard. I am doing better than most in the class, and I am maintaining a solid B average, but I really have trouble thinking about algebra like I do analysis. I feel like I am mostly just regurgitating theorems and techniques just to pass the exams. I know I can pass the course, but I also know that this mindless memorization will eventually come back to haunt me later on in my mathematical career. Algebra is truly one of the pillars of math which is why I really feel terrible that I don't understand it. Is this a sign that I simply don't have what it takes to succeed in math? I would love to go on to graduate school and hopefully get a PhD. In fact, a professor actually said to me, ""I think it would be a shame if you didn't go to grad school for math."" He told me that before I took algebra, but now I feel like my world is ""crashing down"" in a sense. Before I was a ""good"" student; now, I feel like a zombie in the back of the room. Any input is greatly appreciated, but what I really want to know is, has this happened to anyone who has gone on to succeed in a Ph.D math program?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 9 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I attend a mostly liberal arts focused university, in which I was able to test out of an ""Introduction to Proofs"" class and directly into ""Advanced Calculus 1"" (Introductory Analysis I) and I loved it. I did great in the class. I was not very mathematically mature at the time, but I studied hard and started to outpace many of the senior level students who had a least a good year or more of experience than me. Furthermore, the professor teaching the course was apparently known to be particularly difficult, but I loved his course. I enjoyed the challenge and wound up with a B+, the 2nd highest grade given in the class. I took Advanced Calculus 2 and loved it even more. The professor even suggested that I take a graduate complex analysis course in the Fall. (Just a side note here, the undergraduate complex analysis course at my school does not use any theorem's or proofs. The grad version is similar to say, an honors undergraduate course at more traditional math program.) I took this as a high complement, and a verification that I was in fact doing well. I know I am not very deep into analysis, but I feel comfortable with the subject, even with the more abstract parts. However, I am really struggling with abstract algebra. I can't understand why. I study the material really hard. I am doing better than most in the class, and I am maintaining a solid B average, but I really have trouble thinking about algebra like I do analysis. I feel like I am mostly just regurgitating theorems and techniques just to pass the exams. I know I can pass the course, but I also know that this mindless memorization will eventually come back to haunt me later on in my mathematical career. Algebra is truly one of the pillars of math which is why I really feel terrible that I don't understand it. Is this a sign that I simply don't have what it takes to succeed in math? I would love to go on to graduate school and hopefully get a PhD. In fact, a professor actually said to me, ""I think it would be a shame if you didn't go to grad school for math."" He told me that before I took algebra, but now I feel like my world is ""crashing down"" in a sense. Before I was a ""good"" student; now, I feel like a zombie in the back of the room. Any input is greatly appreciated, but what I really want to know is, has this happened to anyone who has gone on to succeed in a Ph.D math program?",,"['abstract-algebra', 'analysis', 'soft-question', 'advice', 'career-development']"
26,How do you split a long exact sequence into short exact sequences?,How do you split a long exact sequence into short exact sequences?,,"How does one split up a long exact sequence into short exact sequences? Say you have some longs exact sequences of modules $$ 0\longrightarrow M_1\stackrel{\phi_1}{\longrightarrow}M_2\stackrel{\phi_2}{\longrightarrow}M_3\stackrel{\phi_3}{\longrightarrow}M_4\stackrel{\phi_4}{\longrightarrow}\cdots $$ I've read it's possible to split this into short exact sequences. What exactly does that mean? Would it be written as short exact sequences, one appended to another like $$ 0\longrightarrow N_1\longrightarrow M_1\longrightarrow N'_1\longrightarrow 0\longrightarrow N_2\longrightarrow M_2\longrightarrow N'_2\longrightarrow 0 \longrightarrow\cdots? $$ If so, how does this work? Merci.","How does one split up a long exact sequence into short exact sequences? Say you have some longs exact sequences of modules $$ 0\longrightarrow M_1\stackrel{\phi_1}{\longrightarrow}M_2\stackrel{\phi_2}{\longrightarrow}M_3\stackrel{\phi_3}{\longrightarrow}M_4\stackrel{\phi_4}{\longrightarrow}\cdots $$ I've read it's possible to split this into short exact sequences. What exactly does that mean? Would it be written as short exact sequences, one appended to another like $$ 0\longrightarrow N_1\longrightarrow M_1\longrightarrow N'_1\longrightarrow 0\longrightarrow N_2\longrightarrow M_2\longrightarrow N'_2\longrightarrow 0 \longrightarrow\cdots? $$ If so, how does this work? Merci.",,['abstract-algebra']
27,Is $1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ irreducible?,Is  irreducible?,1+x+\frac{x^2}2+\dots+\frac{x^n}{n!},"The polynomial $f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ often appears in algebra textbooks as an illustration for using derivative to test for multiple roots. Recently, I stumbled upon Example 2.1.6 in Prasolov's book Polynomials (Springer, 2004), where it is shown that this polynomial is irreducible using Eisenstein's criterion and Bertrand's postulate . However, I do not think the argument is correct. Below you can find the argument presented in the book -- I do not see how Eisenstein is applicable here, since we do not know $p\mid n$. And if we are using Eisenstein's criterion directly to the polynomial $n!f(x)$, this is one of the coefficients that would have to be divisible by $p$. (However, the argument works at least if $n$ is prime.) So my main question is about the irreducibility of the original polynomial, but I also wonder whether Prasolov's proof can be corrected somehow. To summarize: Is the polynomial $f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ irreducible over $\mathbb Q$? Is the Prasolov's proof correct or can it be easily corrected? (Did I miss something there?) Here is the (whole) Example 2.1.6 from Prasolov's book. The same example is given in прасолов: многочлены(Prasolov: Mnogochleny; 2001,MCCME). Example 2.1.6. For any positive integer $n$, the polynomial $$f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$$ is irreducible. Proof: We have to prove that the polynomial  $$n!f(x)=x^n+nx^{n-1}+n(n-1)x^{n-2}+\dots+n!$$ is irreducible over $\mathbb Z$. To this end, it suffices to find the prime $p$ such that $n!$ is divisible by $p$ but is not divisible by $p^2$, i.e., $p \le n < 2p$. Let $n = 2m$ or $n = 2m + 1$. Bertrand's postulate states that there exists a prime p such that $m < p \le 2m$. For $n = 2m$, the inequalities $p \le n < 2p$ are obvious. For $n = 2m + 1$, we obtain the inequalities $p \le n-1$ and $n-1 < 2p$. But in this case the number $n-1$ is even, and hence the inequality $n-1 < 2p$ implies $n < 2p$. It is also clear that $p \le n - 1 < n$. $\hspace{20pt}\square$","The polynomial $f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ often appears in algebra textbooks as an illustration for using derivative to test for multiple roots. Recently, I stumbled upon Example 2.1.6 in Prasolov's book Polynomials (Springer, 2004), where it is shown that this polynomial is irreducible using Eisenstein's criterion and Bertrand's postulate . However, I do not think the argument is correct. Below you can find the argument presented in the book -- I do not see how Eisenstein is applicable here, since we do not know $p\mid n$. And if we are using Eisenstein's criterion directly to the polynomial $n!f(x)$, this is one of the coefficients that would have to be divisible by $p$. (However, the argument works at least if $n$ is prime.) So my main question is about the irreducibility of the original polynomial, but I also wonder whether Prasolov's proof can be corrected somehow. To summarize: Is the polynomial $f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$ irreducible over $\mathbb Q$? Is the Prasolov's proof correct or can it be easily corrected? (Did I miss something there?) Here is the (whole) Example 2.1.6 from Prasolov's book. The same example is given in прасолов: многочлены(Prasolov: Mnogochleny; 2001,MCCME). Example 2.1.6. For any positive integer $n$, the polynomial $$f(x)=1+x+\frac{x^2}2+\dots+\frac{x^n}{n!}$$ is irreducible. Proof: We have to prove that the polynomial  $$n!f(x)=x^n+nx^{n-1}+n(n-1)x^{n-2}+\dots+n!$$ is irreducible over $\mathbb Z$. To this end, it suffices to find the prime $p$ such that $n!$ is divisible by $p$ but is not divisible by $p^2$, i.e., $p \le n < 2p$. Let $n = 2m$ or $n = 2m + 1$. Bertrand's postulate states that there exists a prime p such that $m < p \le 2m$. For $n = 2m$, the inequalities $p \le n < 2p$ are obvious. For $n = 2m + 1$, we obtain the inequalities $p \le n-1$ and $n-1 < 2p$. But in this case the number $n-1$ is even, and hence the inequality $n-1 < 2p$ implies $n < 2p$. It is also clear that $p \le n - 1 < n$. $\hspace{20pt}\square$",,"['abstract-algebra', 'polynomials']"
28,How to show that every Boolean ring is commutative?,How to show that every Boolean ring is commutative?,,A ring $R$ is a Boolean ring provided that $a^2=a$ for every $a \in R$. How can we show that every Boolean ring is commutative?,A ring $R$ is a Boolean ring provided that $a^2=a$ for every $a \in R$. How can we show that every Boolean ring is commutative?,,['abstract-algebra']
29,$x^p-c$ has no root in a field $F$ if and only if $x^p-c$ is irreducible?,has no root in a field  if and only if  is irreducible?,x^p-c F x^p-c,Hungerford's book of algebra has exercise $6$ chapter $3$ section $6$ [Probably impossible with the tools at hand.]: Let $p \in \mathbb{Z}$ be a prime; let $F$ be a field  and let $c \in  F$ . Then $x^p - c$ is irreducible in $F[x]$ if and only if $x^p - c$ has no root  in $F$ . [Hint: consider two cases: $\mathrm{char}(F) = p$ and $\mathrm{char}(F) \ne p$ .] I have attempted this a lot. Anyone has an answer?,Hungerford's book of algebra has exercise chapter section [Probably impossible with the tools at hand.]: Let be a prime; let be a field  and let . Then is irreducible in if and only if has no root  in . [Hint: consider two cases: and .] I have attempted this a lot. Anyone has an answer?,"6 3 6 p \in \mathbb{Z} F c \in
 F x^p - c F[x] x^p - c F \mathrm{char}(F) = p \mathrm{char}(F) \ne p","['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
30,Intuition behind Snake Lemma,Intuition behind Snake Lemma,,"I've been struggling with this for some time. I can prove the Snake Lemma, but I don't really “understand” it. By that I mean if no one told me Snake Lemma existed, I would not even attempt to prove it. I believe I am missing some important intuition that tells me that the lemma “could” be true before actually trying to prove it. Please give me some pointers.","I've been struggling with this for some time. I can prove the Snake Lemma, but I don't really “understand” it. By that I mean if no one told me Snake Lemma existed, I would not even attempt to prove it. I believe I am missing some important intuition that tells me that the lemma “could” be true before actually trying to prove it. Please give me some pointers.",,"['abstract-algebra', 'algebraic-topology', 'intuition', 'homological-algebra', 'abelian-categories']"
31,Is there a geometric idea behind Sylow's theorems?,Is there a geometric idea behind Sylow's theorems?,,"I have a confession to make: none of the proofs of Sylow's theorems I saw clicked with me. My first abstract algebra courses were more on the algebraic side (without mention of group actions and geometric motivation for groups, except hastily mentioned dihedral groups), so when I (in self-study) discovered interplay between geometry and group theory, I was delighted. Many concepts and ideas suddenly made more sense to me. I see Sylow's theorems as an useful technical black box, which can help you characterize groups when you only know numerical data about them. I've solved enough problems using those theorems, and now I'm interested if there is a way to make them ''click''. I'm interested is there a geometrical idea behind Sylow's theorems (at least one of them), or at least a nice intuitive explanation of why that result should hold. How do you think about them?","I have a confession to make: none of the proofs of Sylow's theorems I saw clicked with me. My first abstract algebra courses were more on the algebraic side (without mention of group actions and geometric motivation for groups, except hastily mentioned dihedral groups), so when I (in self-study) discovered interplay between geometry and group theory, I was delighted. Many concepts and ideas suddenly made more sense to me. I see Sylow's theorems as an useful technical black box, which can help you characterize groups when you only know numerical data about them. I've solved enough problems using those theorems, and now I'm interested if there is a way to make them ''click''. I'm interested is there a geometrical idea behind Sylow's theorems (at least one of them), or at least a nice intuitive explanation of why that result should hold. How do you think about them?",,"['abstract-algebra', 'group-theory', 'soft-question', 'finite-groups', 'sylow-theory']"
32,"Why are the only associative division algebras over the real numbers the real numbers, the complex numbers, and the quaternions?","Why are the only associative division algebras over the real numbers the real numbers, the complex numbers, and the quaternions?",,"Why are the only (associative) division algebras over the real numbers the real numbers, the complex numbers, and the quaternions? Here a division algebra is an associative algebra where every nonzero number is invertible (like a field, but without assuming commutativity of multiplication). This is an old result proved by Frobenius, but I can't remember how the argument goes.  Anyone have a quick proof?","Why are the only (associative) division algebras over the real numbers the real numbers, the complex numbers, and the quaternions? Here a division algebra is an associative algebra where every nonzero number is invertible (like a field, but without assuming commutativity of multiplication). This is an old result proved by Frobenius, but I can't remember how the argument goes.  Anyone have a quick proof?",,"['quaternions', 'ring-theory', 'abstract-algebra']"
33,$|G|>2$ implies $G$ has non trivial automorphism,implies  has non trivial automorphism,|G|>2 G,"Well, this is an exercise problem from Herstein which sounds difficult: How does one prove that if $|G|>2$, then $G$ has non-trivial automorphism? The only thing I know which connects a group with its automorphism is the theorem, $$G/Z(G) \cong \mathcal{I}(G)$$ where $\mathcal{I}(G)$ denotes the Inner- Automorphism group of $G$. So for a group with $Z(G)=(e)$, we can conclude that it has a non-trivial automorphism, but what about groups with center?","Well, this is an exercise problem from Herstein which sounds difficult: How does one prove that if $|G|>2$, then $G$ has non-trivial automorphism? The only thing I know which connects a group with its automorphism is the theorem, $$G/Z(G) \cong \mathcal{I}(G)$$ where $\mathcal{I}(G)$ denotes the Inner- Automorphism group of $G$. So for a group with $Z(G)=(e)$, we can conclude that it has a non-trivial automorphism, but what about groups with center?",,['abstract-algebra']
34,Is there a purely algebraic proof of the Fundamental Theorem of Algebra?,Is there a purely algebraic proof of the Fundamental Theorem of Algebra?,,"Among the many techniques available at our disposal to prove FTA, is there any purely algebraic proof of the theorem? That seems reasonably unexpected, because somehow or the other we are depending on the topological nature of $R$, and Wikipedia supports  the claim in these statements: ""In spite of its name, there is no purely algebraic proof of the theorem, since any proof must use the completeness of the reals (or some other equivalent formulation of completeness), which is not an algebraic concept."" Is it a proven fact that no pure algebraic proof is possible? And so if I assume we are relying on the topological properties of $R$ and $C$ to prove the theorem, then given any arbitrary field how can one test whether it is algebraically closed or not? Again in Wikipedia I found this result stated, ""The classic example is the theory of algebraically closed fields of a given characteristic. Categoricity does not say that all algebraically closed fields of characteristic 0 as large as the complex numbers C are the same as C; it only asserts that they are isomorphic as fields to C. It follows that although the completed p-adic closures Cp are all isomorphic as fields to C, they may (and in fact do) have completely different topological and analytic properties."" so i now want to rephrase my question as, given any field with different topological properties than C and which is in no simple way isomorphic to $C$ how can we generalize the proof of FTA to check whether FTA is valid on those fields? And what about characteristic p fields? i can see an example that algebraic closure of $F_p((t))$ is an example of infinite, characteristic p field that is by construction closed, but if we had in our arsenal other ways to describe the field and suppress the fact that it is algebraic closure of another field then how can one prove that it is algebraically closed? I don't understand anything about categoricity and such things, and I was only interested in the result taken, and I am sorry if it is a repost.","Among the many techniques available at our disposal to prove FTA, is there any purely algebraic proof of the theorem? That seems reasonably unexpected, because somehow or the other we are depending on the topological nature of $R$, and Wikipedia supports  the claim in these statements: ""In spite of its name, there is no purely algebraic proof of the theorem, since any proof must use the completeness of the reals (or some other equivalent formulation of completeness), which is not an algebraic concept."" Is it a proven fact that no pure algebraic proof is possible? And so if I assume we are relying on the topological properties of $R$ and $C$ to prove the theorem, then given any arbitrary field how can one test whether it is algebraically closed or not? Again in Wikipedia I found this result stated, ""The classic example is the theory of algebraically closed fields of a given characteristic. Categoricity does not say that all algebraically closed fields of characteristic 0 as large as the complex numbers C are the same as C; it only asserts that they are isomorphic as fields to C. It follows that although the completed p-adic closures Cp are all isomorphic as fields to C, they may (and in fact do) have completely different topological and analytic properties."" so i now want to rephrase my question as, given any field with different topological properties than C and which is in no simple way isomorphic to $C$ how can we generalize the proof of FTA to check whether FTA is valid on those fields? And what about characteristic p fields? i can see an example that algebraic closure of $F_p((t))$ is an example of infinite, characteristic p field that is by construction closed, but if we had in our arsenal other ways to describe the field and suppress the fact that it is algebraic closure of another field then how can one prove that it is algebraically closed? I don't understand anything about categoricity and such things, and I was only interested in the result taken, and I am sorry if it is a repost.",,"['abstract-algebra', 'field-theory']"
35,"What does it mean to say a map ""factors through"" a set?","What does it mean to say a map ""factors through"" a set?",,"Consider the following diagram: What does it mean precisely to say ""$f$ factors through $G/\text{ker}(f)$""? Does it mean $f = \tilde{f} \circ \pi$, for some $\tilde{f}$? I've seen texts use the phrase, but never a definition of this notion.","Consider the following diagram: What does it mean precisely to say ""$f$ factors through $G/\text{ker}(f)$""? Does it mean $f = \tilde{f} \circ \pi$, for some $\tilde{f}$? I've seen texts use the phrase, but never a definition of this notion.",,['abstract-algebra']
36,"$\langle 2,x \rangle$ is a non-principal ideal in $\mathbb Z [x];\, $ $D[x]$ PID $\iff D$ field, for a domain $D$","is a non-principal ideal in   PID  field, for a domain","\langle 2,x \rangle \mathbb Z [x];\,  D[x] \iff D D","Hi I don't know how to show that $\langle 2,x \rangle$ is not principal and the definition of a principal ideal is unclear to me. I need help on this, please. The ring that I am talking about is $\mathbb{Z}[x]$ so $\langle 2,x \rangle$ refers to $2g(x) + xf(x)$ where $g(x)$, $f(x)$ belongs to $\mathbb{Z}[x]$.","Hi I don't know how to show that $\langle 2,x \rangle$ is not principal and the definition of a principal ideal is unclear to me. I need help on this, please. The ring that I am talking about is $\mathbb{Z}[x]$ so $\langle 2,x \rangle$ refers to $2g(x) + xf(x)$ where $g(x)$, $f(x)$ belongs to $\mathbb{Z}[x]$.",,"['abstract-algebra', 'ring-theory', 'ideals']"
37,Can someone explain Cayley's Theorem step by step?,Can someone explain Cayley's Theorem step by step?,,"This is from Fraleigh's First Course in Abstract Algebra (page 82, Theorem 8.16) and I keep having hard time understanding its proof. I understand only until they mention the map $\lambda_x (g) = xg$ . Can someone explain this proof step by step? Thank you so much! Here is the proof: Let $G$ be a group. We show that $G$ is isomorphic to a subgroup of $S_G$ . Define a one-to-one function $\phi: G \to S_G$ such that $\phi(xy)=\phi(x) \phi(y)$ for all $x,y \in G$ . For $x \in G$ , let $\lambda_x: G \to G$ be defined by $\lambda_x (g) = xg$ for all $g \in G$ . The equation $\lambda_x (x^{-1} c) = x(x^{-1} c) = c$ for all $c \in G$ shows that $\lambda_x$ maps $G$ onto $G$ . If $\lambda_x (a) = \lambda_x (b)$ , then $xa=xb$ so $a=b$ by cancellation. Thus $\lambda_x$ is also one to one, and is a permutation of $G$ . We now define $\phi: G \to S_G$ by defining $\phi(x) = \lambda_x$ for all $x \in G$ . To show that $\phi$ is one to one, suppose that $\phi(x) = \phi(y)$ . Then $\lambda_x = \lambda_y$ as functions mapping $G$ into $G$ . In particular $\lambda_x (e) = \lambda_y (e)$ , so $xe=ye$ and $x=y$ . Thus $\phi$ is one to one. It only remains to show that $\phi(xy) = \phi(x) \phi(y)$ , that is, that $\phi_{xy} = \lambda_x \lambda_y$ . Now for any $g \in G$ , we have $\lambda_{xy} (g) = (xy)g$ . Permutation multiplication is function composition, so $(\lambda_x \lambda_y)(g) = \lambda_x (\lambda_y (g)) = \lambda_x (yg) = x(yg)$ . Thus by associativity, $\lambda_{xy} = \lambda_x \lambda_y$ .","This is from Fraleigh's First Course in Abstract Algebra (page 82, Theorem 8.16) and I keep having hard time understanding its proof. I understand only until they mention the map . Can someone explain this proof step by step? Thank you so much! Here is the proof: Let be a group. We show that is isomorphic to a subgroup of . Define a one-to-one function such that for all . For , let be defined by for all . The equation for all shows that maps onto . If , then so by cancellation. Thus is also one to one, and is a permutation of . We now define by defining for all . To show that is one to one, suppose that . Then as functions mapping into . In particular , so and . Thus is one to one. It only remains to show that , that is, that . Now for any , we have . Permutation multiplication is function composition, so . Thus by associativity, .","\lambda_x (g) = xg G G S_G \phi: G \to S_G \phi(xy)=\phi(x) \phi(y) x,y \in G x \in G \lambda_x: G \to G \lambda_x (g) = xg g \in G \lambda_x (x^{-1} c) = x(x^{-1} c) = c c \in G \lambda_x G G \lambda_x (a) = \lambda_x (b) xa=xb a=b \lambda_x G \phi: G \to S_G \phi(x) = \lambda_x x \in G \phi \phi(x) = \phi(y) \lambda_x = \lambda_y G G \lambda_x (e) = \lambda_y (e) xe=ye x=y \phi \phi(xy) = \phi(x) \phi(y) \phi_{xy} = \lambda_x \lambda_y g \in G \lambda_{xy} (g) = (xy)g (\lambda_x \lambda_y)(g) = \lambda_x (\lambda_y (g)) = \lambda_x (yg) = x(yg) \lambda_{xy} = \lambda_x \lambda_y","['abstract-algebra', 'group-theory', 'finite-groups', 'permutations', 'symmetric-groups']"
38,Methods to see if a polynomial is irreducible,Methods to see if a polynomial is irreducible,,"Given a polynomial over a field, what are the methods to see it is irreducible? Only two comes to my mind now. First is Eisenstein criterion. Another is that if a polynomial is irreducible mod p then it is irreducible. Are there any others?","Given a polynomial over a field, what are the methods to see it is irreducible? Only two comes to my mind now. First is Eisenstein criterion. Another is that if a polynomial is irreducible mod p then it is irreducible. Are there any others?",,['abstract-algebra']
39,Why is Galois correspondence intuitively plausible?,Why is Galois correspondence intuitively plausible?,,"The Galois Correspondence Theorem (a.k.a. the Fundamental Theorem of Galois Theory) says that for any Galois extension of fields $K/F$ , there is a one-to-one inclusion reversing correspondence between the intermediate fields $K \supseteq E \supseteq F$ and subgroups of the Galois group $\text{Gal }K/F$ . I have two questions about this: Why this is theorem important? When I first learned it, I told myself that it was just a computational tool: it converts problems about fields, which are hard to understand, into problems about groups, which are easier to understand. But clearly, it is not just there for computational convenience, it is an important result in its own right. So I ask: is there a non-utilitarian reason why the theorem is profound? In other words, what deep ""underlying truth"" about the algebraic structure of fields does this theorem reveal? Why is this theorem intuitively plausible? The way I have understood it, if you place the subfield lattice of $K/F$ and the subgroup lattice of $\text{Gal }K/F$ side-by-side (with the latter flipped upside-down), then the diagrams are the same. This seems very out-of-blue to me. Why is it intuitively plausible that these two diagrams should look the same? Why should we expect that by studying the symmetries of a field extension, we can recover the structure of whole field itself? An explanation with a specific example of a field extension, would be great. Thanks for the help!","The Galois Correspondence Theorem (a.k.a. the Fundamental Theorem of Galois Theory) says that for any Galois extension of fields , there is a one-to-one inclusion reversing correspondence between the intermediate fields and subgroups of the Galois group . I have two questions about this: Why this is theorem important? When I first learned it, I told myself that it was just a computational tool: it converts problems about fields, which are hard to understand, into problems about groups, which are easier to understand. But clearly, it is not just there for computational convenience, it is an important result in its own right. So I ask: is there a non-utilitarian reason why the theorem is profound? In other words, what deep ""underlying truth"" about the algebraic structure of fields does this theorem reveal? Why is this theorem intuitively plausible? The way I have understood it, if you place the subfield lattice of and the subgroup lattice of side-by-side (with the latter flipped upside-down), then the diagrams are the same. This seems very out-of-blue to me. Why is it intuitively plausible that these two diagrams should look the same? Why should we expect that by studying the symmetries of a field extension, we can recover the structure of whole field itself? An explanation with a specific example of a field extension, would be great. Thanks for the help!",K/F K \supseteq E \supseteq F \text{Gal }K/F K/F \text{Gal }K/F,"['abstract-algebra', 'field-theory', 'galois-theory']"
40,Cokernels - how to explain or get a good intuition of what they are or might be,Cokernels - how to explain or get a good intuition of what they are or might be,,"When I think about kernels, I have many well-worked examples from group theory, rings and modules - in the earliest stages of dealing with abstract mathematical objects they seem to come up all over the place, whenever I see a homomorphism. BUT no-one really seems to mention cokernels until you get to commutative diagrams and category theory. And then they can easily just be ""things which make the diagram work"" with limited intuition or sense of useful reality. [maybe I exaggerate] So I am looking for good examples to illustrate what a cokernel is, extending to non-trivial examples [I was taught about the kernels of homomorphisms between non-abelian groups before anyone taught me about modules].","When I think about kernels, I have many well-worked examples from group theory, rings and modules - in the earliest stages of dealing with abstract mathematical objects they seem to come up all over the place, whenever I see a homomorphism. BUT no-one really seems to mention cokernels until you get to commutative diagrams and category theory. And then they can easily just be ""things which make the diagram work"" with limited intuition or sense of useful reality. [maybe I exaggerate] So I am looking for good examples to illustrate what a cokernel is, extending to non-trivial examples [I was taught about the kernels of homomorphisms between non-abelian groups before anyone taught me about modules].",,"['abstract-algebra', 'category-theory', 'big-list', 'homological-algebra']"
41,Can you give me some concrete examples of magmas?,Can you give me some concrete examples of magmas?,,"I've seen the following (e.g. here ): I've learned a bit about groups and I could give examples of groups, but when reading the given table, I couldn't imagine of what a magma would be. It has no associativity, no identity, no divisibility and no commutativity. I can't imagine what such a thing would be. Can you give a concrete example of a magma?","I've seen the following (e.g. here ): I've learned a bit about groups and I could give examples of groups, but when reading the given table, I couldn't imagine of what a magma would be. It has no associativity, no identity, no divisibility and no commutativity. I can't imagine what such a thing would be. Can you give a concrete example of a magma?",,"['abstract-algebra', 'examples-counterexamples', 'magma']"
42,Why is ring addition commutative?,Why is ring addition commutative?,,"What is the motivation behind axiomatically forcing the underpinning group of a ring to be abelian? Noncommutative rings are vastly more complex than commutative ones, so I am assuming that allowing the additive operation to be noncommuting would just make matters worse. Is there something deeper here, or is this a restriction for the sake of convenience and simplicity?","What is the motivation behind axiomatically forcing the underpinning group of a ring to be abelian? Noncommutative rings are vastly more complex than commutative ones, so I am assuming that allowing the additive operation to be noncommuting would just make matters worse. Is there something deeper here, or is this a restriction for the sake of convenience and simplicity?",,"['abstract-algebra', 'ring-theory']"
43,Multiplication in Permutation Groups Written in Cyclic Notation,Multiplication in Permutation Groups Written in Cyclic Notation,,"I didn't find any good explanation how to perform multiplication on permutation group written in cyclic notation. For example, if $$   a=(1\,3\,5\,2),\quad b=(2\,5\,6),\quad c=(1\,6\,3\,4), $$  then why does $ab=(1\,3\,5\,6)$ and $ac=(1\,6\,5\,2)(3\,4)$?","I didn't find any good explanation how to perform multiplication on permutation group written in cyclic notation. For example, if $$   a=(1\,3\,5\,2),\quad b=(2\,5\,6),\quad c=(1\,6\,3\,4), $$  then why does $ab=(1\,3\,5\,6)$ and $ac=(1\,6\,5\,2)(3\,4)$?",,"['abstract-algebra', 'group-theory', 'permutations']"
44,How to prove that the sum and product of two algebraic numbers is algebraic? [duplicate],How to prove that the sum and product of two algebraic numbers is algebraic? [duplicate],,"This question already has an answer here : Sums and products of algebraic numbers (1 answer) Closed 3 years ago . Suppose $E/F$ is a field extension and $\alpha, \beta \in E$ are algebraic over $F$. Then it is not too hard to see that when $\alpha$ is nonzero, $1/\alpha$ is also algebraic. If $a_0 + a_1\alpha + \cdots + a_n \alpha^n = 0$, then dividing by $\alpha^{n}$ gives $$a_0\frac{1}{\alpha^n} + a_1\frac{1}{\alpha^{n-1}} + \cdots + a_n = 0.$$ Is there a similar elementary way to show that $\alpha + \beta$ and $\alpha \beta$ are also algebraic (i.e. finding an explicit formula for a polynomial that has $\alpha + \beta$ or $\alpha\beta$ as its root)? The only proof I know for this fact is the one where you show that $F(\alpha, \beta) / F$ is a finite field extension and thus an algebraic extension.","This question already has an answer here : Sums and products of algebraic numbers (1 answer) Closed 3 years ago . Suppose $E/F$ is a field extension and $\alpha, \beta \in E$ are algebraic over $F$. Then it is not too hard to see that when $\alpha$ is nonzero, $1/\alpha$ is also algebraic. If $a_0 + a_1\alpha + \cdots + a_n \alpha^n = 0$, then dividing by $\alpha^{n}$ gives $$a_0\frac{1}{\alpha^n} + a_1\frac{1}{\alpha^{n-1}} + \cdots + a_n = 0.$$ Is there a similar elementary way to show that $\alpha + \beta$ and $\alpha \beta$ are also algebraic (i.e. finding an explicit formula for a polynomial that has $\alpha + \beta$ or $\alpha\beta$ as its root)? The only proof I know for this fact is the one where you show that $F(\alpha, \beta) / F$ is a finite field extension and thus an algebraic extension.",,"['abstract-algebra', 'field-theory']"
45,What is the crime of lèse-Bourbaki?,What is the crime of lèse-Bourbaki?,,"In the foreword to his textbook Algebra , Serge Lang writes (on page vi) I have frequently committed the crime of lèse-Bourbaki by repeating short arguments or definitions to make certain sections or chapters logically independent of each other. What does ""the crime of lèse-Bourbaki"" mean? I do not understand French, so naturally I googled the phrase and what turned up was the Wikipedia page for lèse-majesté . Lèse-majesté is the crime of violating majesty, an offence against the dignity of the reigning sovereign or against a state. I am aware that Nicolas Bourbaki is a pseudonym used by a group of influential French mathematicians who wrote a series of textbooks in a terse and formal manner. So, is Lang honoring Bourbaki by equating them with (mathematical) royalty?","In the foreword to his textbook Algebra , Serge Lang writes (on page vi) I have frequently committed the crime of lèse-Bourbaki by repeating short arguments or definitions to make certain sections or chapters logically independent of each other. What does ""the crime of lèse-Bourbaki"" mean? I do not understand French, so naturally I googled the phrase and what turned up was the Wikipedia page for lèse-majesté . Lèse-majesté is the crime of violating majesty, an offence against the dignity of the reigning sovereign or against a state. I am aware that Nicolas Bourbaki is a pseudonym used by a group of influential French mathematicians who wrote a series of textbooks in a terse and formal manner. So, is Lang honoring Bourbaki by equating them with (mathematical) royalty?",,['abstract-algebra']
46,Examples and further results about the order of the product of two elements in a group,Examples and further results about the order of the product of two elements in a group,,"Let $G$ be a group and let $a,b$ be two elements of $G$. What can we say about the order of their product $ab$? Wikipedia says ""not much"": There is no general formula relating the order of a product $ab$ to the orders of $a$ and $b$. In fact, it is possible that both $a$ and $b$ have finite order while $ab$ has infinite order, or that both $a$ and $b$ have infinite order while $ab$ has finite order. On the other hand no examples are provided. $(\mathbb{Z},+), 1$ and $-1$ give an example of elements of infinite order with product of finite order. I can't think of any example of the other kind! So: What's an example of a group $G$ and two elements $a,b$ both of finite order such that their product has infinite order? Wikipedia then states: If $ab = ba$, we can at least say that $\mathrm{ord}(ab)$ divides $\mathrm{lcm}(\mathrm{ord}(a), \mathrm{ord}(b))$ which is easy to prove, but not very effective. So: What are some similar results about the order of a product, perhaps with some additional hypotheses?","Let $G$ be a group and let $a,b$ be two elements of $G$. What can we say about the order of their product $ab$? Wikipedia says ""not much"": There is no general formula relating the order of a product $ab$ to the orders of $a$ and $b$. In fact, it is possible that both $a$ and $b$ have finite order while $ab$ has infinite order, or that both $a$ and $b$ have infinite order while $ab$ has finite order. On the other hand no examples are provided. $(\mathbb{Z},+), 1$ and $-1$ give an example of elements of infinite order with product of finite order. I can't think of any example of the other kind! So: What's an example of a group $G$ and two elements $a,b$ both of finite order such that their product has infinite order? Wikipedia then states: If $ab = ba$, we can at least say that $\mathrm{ord}(ab)$ divides $\mathrm{lcm}(\mathrm{ord}(a), \mathrm{ord}(b))$ which is easy to prove, but not very effective. So: What are some similar results about the order of a product, perhaps with some additional hypotheses?",,"['abstract-algebra', 'group-theory', 'examples-counterexamples']"
47,Is Pythagoras the only relation to hold between $\cos$ and $\sin$?,Is Pythagoras the only relation to hold between  and ?,\cos \sin,"Pythagoras says that $\cos^2 \theta + \mathrm{sin}^2\theta = 1$ for all real $\theta$. (Vague) Question. Is this the only relationship between the functions $\cos$ and $\sin$? More precisely: Let $\langle \cos,\sin\rangle$ denote the intersection of all subalgebras of the $\mathbb{R}$-algebra of all functions $\mathbb{R} \rightarrow \mathbb{R}$ containing $\{\cos,\sin\}$. (By default, all my algebras are unital, associative and commutative.) Let $A$ denote the $\mathbb{R}$-algebra presented by the generators $\{c,s\}$ and the relation $c^2+s^2=1$. There is a unique $\mathbb{R}$-algebra homomorphism $\varphi : A \rightarrow \langle \cos,\sin\rangle$ given as follows. $$\varphi(c) = \cos, \,\,\varphi(s) = \sin$$ We know that $\varphi$ is surjective. Question. Is $\varphi$ injective? So consider $f \in A$. Then $f = \sum_{i,j : \mathbb{N}}a_{ij}s^ic^i$ for certain choices of $a_{ij} : \mathbb{R}$. Now suppose $\varphi(f)=0$. We want to show that $f=0$. Ideas, anyone?","Pythagoras says that $\cos^2 \theta + \mathrm{sin}^2\theta = 1$ for all real $\theta$. (Vague) Question. Is this the only relationship between the functions $\cos$ and $\sin$? More precisely: Let $\langle \cos,\sin\rangle$ denote the intersection of all subalgebras of the $\mathbb{R}$-algebra of all functions $\mathbb{R} \rightarrow \mathbb{R}$ containing $\{\cos,\sin\}$. (By default, all my algebras are unital, associative and commutative.) Let $A$ denote the $\mathbb{R}$-algebra presented by the generators $\{c,s\}$ and the relation $c^2+s^2=1$. There is a unique $\mathbb{R}$-algebra homomorphism $\varphi : A \rightarrow \langle \cos,\sin\rangle$ given as follows. $$\varphi(c) = \cos, \,\,\varphi(s) = \sin$$ We know that $\varphi$ is surjective. Question. Is $\varphi$ injective? So consider $f \in A$. Then $f = \sum_{i,j : \mathbb{N}}a_{ij}s^ic^i$ for certain choices of $a_{ij} : \mathbb{R}$. Now suppose $\varphi(f)=0$. We want to show that $f=0$. Ideas, anyone?",,"['abstract-algebra', 'trigonometry']"
48,"Proving that $\left(\mathbb Q[\sqrt p_1,\dots,\sqrt p_n]:\mathbb Q\right)=2^n$ for distinct primes $p_i$.",Proving that  for distinct primes .,"\left(\mathbb Q[\sqrt p_1,\dots,\sqrt p_n]:\mathbb Q\right)=2^n p_i","I have read the following theorem: If $p_1,p_2,\dots,p_n$ are distinct prime numbers, then$$\left(\mathbb Q\left[\sqrt p_1,\dots,\sqrt p_n\right]:\mathbb Q\right)=2^n.$$ I have tried to prove a more general statement but I have a problem at one point. (I still don't know how to prove the theorem above, too, because I don't know how not to use linear independence, which I do in the more general statement below.)  Could you please help me overcome the obstacle I've encountered? I will post the intended proof and make it clear where I'm having trouble. I want to prove the following statement: Let $n\geq 1$. The set $B_n:=\left\{\sqrt {p_1^{\epsilon_1}}\sqrt {p_2^{\epsilon_2}}\cdots\sqrt {p_n^{\epsilon_n}}\,|\,(\epsilon_1,\epsilon_2,\cdots,\epsilon_n)\in\{0,1\}^n\right\}$ has $2^n$ elements and is a $\mathbb Q-$basis of $\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_n\right].$ The proof will be by induction. For $n=1,$ we have $B_n=\left\{1,\sqrt {p_1}\right\}.$ It is clear that $\sqrt{p_1}\neq 1,$ so the set has $2=2^1$ elements. It is the basis of $\mathbb Q[\sqrt{p_1}]$ because the minimal polynomial of $\sqrt {p_1}$ over $\mathbb Q$ has degree $2,$ and there is a theorem that $K[a]$ has $a^0,\cdots,a^{d-1}$ as a basis, where $d$ is the degree of the minimal polynomial of $a$ over $K$. Suppose the statement is true for $n-1$, where $n\geq 2.$ We have $$ \left(B_n=B_{n-1}\cup\sqrt{p_n}B_{n-1}\right)\text { and } \left(B_{n-1}\cap\sqrt{p_n}B_{n-1}=\emptyset\right), $$ which is easy to see. It is also easy to see that $\operatorname{card}(B_{n-1})=\operatorname{card}(\sqrt{p_n}B_{n-1}),$ and therefore $$ \operatorname{card}B_{n}=2^n. $$ Let $$ \sum_{x\in B_{n}}q_xx=0 $$ for some $\{q_x\}_{x\in B_n}\subset\mathbb Q.$ Let $p(x):=\sqrt{p_n}x$ for all $x\in B_{n-1}.$ We have $$ \sum_{x\in B_{n}}q_xx=\sum_{x\in B_{n-1}} q_xx+\sum_{x\in \sqrt{p_n}B_{n-1}} q_xx=\sum_{x\in B_{n-1}} q_xx+\sum_{x\in B_{n-1}} q_{p(x)}\sqrt{p_n}x. $$ Therefore $$ \sum_{x\in B_{n-1}} q_xx=-\sqrt{p_n}\sum_{x\in B_{n-1}} q_{p(x)}x,\tag1 $$ and we can make the following division iff $q_{p(x)}\neq 0$ for all $x\in B_{n-1}$ (because $B_{n-1}$ is linearly indepentent over $\mathbb Q$): $$ \sqrt{p_n}=-\frac{\sum_{x\in B_{n-1}} q_xx}{\sum_{x\in B_{n-1}} q_{p(x)}x}, $$ The right-hand side belongs to $\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_{n-1}\right],$ so we have $$ \sqrt{p_n}\in \mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_{n-1}\right]. $$ Therefore we can write $\sqrt{p_n}$ uniquely in the basis $B_{n-1}$. $$ \sqrt{p_n}=\sum_{y\in B_{n-1}}c_yy $$ for some $\{c_y\}_{y\in B_{n-1}}\subset \mathbb Q.$ After squaring this equation we will obtain $$ p_n=\sum_{y\in B_{n-1}}c_y^2y^2+2\sum_{y,z\in B_{n-1}}c_yc_zyz. $$ The last sum must be zero because it is not in $\mathbb Q$ and because after reducing it, we obtain a representation of $p_n$ in the basis $B_{n-1},$ which is unique. Thus $$p_n=\sum_{y\in B_{n-1}}c_y^2y^2.$$ Unfortunately, I can't prove that $c_yc_z$ is always zero. This was my first thought, but clearly there's trouble with the possibility of reductions in $$ \sum_{y,z\in B_{n-1}}c_yc_zyz. $$ Different pairs $y,z$ may yield the same element of $B_{n-1}$ in the product $yz.$ This happens for example when $y=\sqrt 5\sqrt 3,$ $z=\sqrt 5\sqrt 2,$ and $y'= \sqrt 11\sqrt 2,$ $z'=\sqrt 11\sqrt 3$. If it were true that $c_yc_z$ is always zero, I would be able to continue my proof as follows. We would have only one $y_0$ such that $c_{y_0}\neq 0$ and we'd get $$p_n=c_{y_0}^2y_0^2.$$ Let $c_{y_0}=\frac kl$. We can write $$l^2p_n=k^2y_0^2.$$ But $y_0^2$ is the product of some primes different from $p_n$. Therefore the greatest  power of $p_n$ that divides the right-hand side is even. However, the greatest power of $p_n$ that divides the left-hand side is odd. A contradiction. The contradiction proves that $q_{p(x)}=0$ for all $x\in B_{n-1}.$ Hence $(1)$ gives us that $$ \sum_{x\in B_{n-1}} q_xx=0 $$ and linear independence of $B_{n-1}$ gives us that $q_x=0$ for all $x\in B_{n-1}.$ This gives us that $B_n$ is linearly independent. It generates the whole $\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_n\right]$ because $$ \mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_n\right]=\left(\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_{n-1}\right]\right)\left[\sqrt{p_n}\right]. $$ This would end the proof.","I have read the following theorem: If $p_1,p_2,\dots,p_n$ are distinct prime numbers, then$$\left(\mathbb Q\left[\sqrt p_1,\dots,\sqrt p_n\right]:\mathbb Q\right)=2^n.$$ I have tried to prove a more general statement but I have a problem at one point. (I still don't know how to prove the theorem above, too, because I don't know how not to use linear independence, which I do in the more general statement below.)  Could you please help me overcome the obstacle I've encountered? I will post the intended proof and make it clear where I'm having trouble. I want to prove the following statement: Let $n\geq 1$. The set $B_n:=\left\{\sqrt {p_1^{\epsilon_1}}\sqrt {p_2^{\epsilon_2}}\cdots\sqrt {p_n^{\epsilon_n}}\,|\,(\epsilon_1,\epsilon_2,\cdots,\epsilon_n)\in\{0,1\}^n\right\}$ has $2^n$ elements and is a $\mathbb Q-$basis of $\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_n\right].$ The proof will be by induction. For $n=1,$ we have $B_n=\left\{1,\sqrt {p_1}\right\}.$ It is clear that $\sqrt{p_1}\neq 1,$ so the set has $2=2^1$ elements. It is the basis of $\mathbb Q[\sqrt{p_1}]$ because the minimal polynomial of $\sqrt {p_1}$ over $\mathbb Q$ has degree $2,$ and there is a theorem that $K[a]$ has $a^0,\cdots,a^{d-1}$ as a basis, where $d$ is the degree of the minimal polynomial of $a$ over $K$. Suppose the statement is true for $n-1$, where $n\geq 2.$ We have $$ \left(B_n=B_{n-1}\cup\sqrt{p_n}B_{n-1}\right)\text { and } \left(B_{n-1}\cap\sqrt{p_n}B_{n-1}=\emptyset\right), $$ which is easy to see. It is also easy to see that $\operatorname{card}(B_{n-1})=\operatorname{card}(\sqrt{p_n}B_{n-1}),$ and therefore $$ \operatorname{card}B_{n}=2^n. $$ Let $$ \sum_{x\in B_{n}}q_xx=0 $$ for some $\{q_x\}_{x\in B_n}\subset\mathbb Q.$ Let $p(x):=\sqrt{p_n}x$ for all $x\in B_{n-1}.$ We have $$ \sum_{x\in B_{n}}q_xx=\sum_{x\in B_{n-1}} q_xx+\sum_{x\in \sqrt{p_n}B_{n-1}} q_xx=\sum_{x\in B_{n-1}} q_xx+\sum_{x\in B_{n-1}} q_{p(x)}\sqrt{p_n}x. $$ Therefore $$ \sum_{x\in B_{n-1}} q_xx=-\sqrt{p_n}\sum_{x\in B_{n-1}} q_{p(x)}x,\tag1 $$ and we can make the following division iff $q_{p(x)}\neq 0$ for all $x\in B_{n-1}$ (because $B_{n-1}$ is linearly indepentent over $\mathbb Q$): $$ \sqrt{p_n}=-\frac{\sum_{x\in B_{n-1}} q_xx}{\sum_{x\in B_{n-1}} q_{p(x)}x}, $$ The right-hand side belongs to $\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_{n-1}\right],$ so we have $$ \sqrt{p_n}\in \mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_{n-1}\right]. $$ Therefore we can write $\sqrt{p_n}$ uniquely in the basis $B_{n-1}$. $$ \sqrt{p_n}=\sum_{y\in B_{n-1}}c_yy $$ for some $\{c_y\}_{y\in B_{n-1}}\subset \mathbb Q.$ After squaring this equation we will obtain $$ p_n=\sum_{y\in B_{n-1}}c_y^2y^2+2\sum_{y,z\in B_{n-1}}c_yc_zyz. $$ The last sum must be zero because it is not in $\mathbb Q$ and because after reducing it, we obtain a representation of $p_n$ in the basis $B_{n-1},$ which is unique. Thus $$p_n=\sum_{y\in B_{n-1}}c_y^2y^2.$$ Unfortunately, I can't prove that $c_yc_z$ is always zero. This was my first thought, but clearly there's trouble with the possibility of reductions in $$ \sum_{y,z\in B_{n-1}}c_yc_zyz. $$ Different pairs $y,z$ may yield the same element of $B_{n-1}$ in the product $yz.$ This happens for example when $y=\sqrt 5\sqrt 3,$ $z=\sqrt 5\sqrt 2,$ and $y'= \sqrt 11\sqrt 2,$ $z'=\sqrt 11\sqrt 3$. If it were true that $c_yc_z$ is always zero, I would be able to continue my proof as follows. We would have only one $y_0$ such that $c_{y_0}\neq 0$ and we'd get $$p_n=c_{y_0}^2y_0^2.$$ Let $c_{y_0}=\frac kl$. We can write $$l^2p_n=k^2y_0^2.$$ But $y_0^2$ is the product of some primes different from $p_n$. Therefore the greatest  power of $p_n$ that divides the right-hand side is even. However, the greatest power of $p_n$ that divides the left-hand side is odd. A contradiction. The contradiction proves that $q_{p(x)}=0$ for all $x\in B_{n-1}.$ Hence $(1)$ gives us that $$ \sum_{x\in B_{n-1}} q_xx=0 $$ and linear independence of $B_{n-1}$ gives us that $q_x=0$ for all $x\in B_{n-1}.$ This gives us that $B_n$ is linearly independent. It generates the whole $\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_n\right]$ because $$ \mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_n\right]=\left(\mathbb Q\left[\sqrt p_1,\sqrt p_2,\cdots,\sqrt p_{n-1}\right]\right)\left[\sqrt{p_n}\right]. $$ This would end the proof.",,['abstract-algebra']
49,What kind of work do modern day algebraists do?,What kind of work do modern day algebraists do?,,"Often times in my studies I get the impression that algebra is just a tool to help with other branches of mathematics, like algebraic geometry, algebraic number theory, algebraic topology, etc. How true this is, I am not sure. So I suppose I want to ask, what sort of work do modern day algebraists do? What are currently some of the more active areas of modern algebra? What types of problems do algebraists deal with? I'm kicking around the idea of pursuing graduate study one day, possibly in some sort of algebraic field, i.e., ring theory or something. What sort of research and problems are open to your average graduate student in algebra (of any sort, not just ring theory)? This is partially inspired by the question What do modern-day analysts actually do? Thank you for your responses.","Often times in my studies I get the impression that algebra is just a tool to help with other branches of mathematics, like algebraic geometry, algebraic number theory, algebraic topology, etc. How true this is, I am not sure. So I suppose I want to ask, what sort of work do modern day algebraists do? What are currently some of the more active areas of modern algebra? What types of problems do algebraists deal with? I'm kicking around the idea of pursuing graduate study one day, possibly in some sort of algebraic field, i.e., ring theory or something. What sort of research and problems are open to your average graduate student in algebra (of any sort, not just ring theory)? This is partially inspired by the question What do modern-day analysts actually do? Thank you for your responses.",,"['abstract-algebra', 'group-theory', 'soft-question', 'ring-theory', 'field-theory']"
50,The Chinese Remainder Theorem for Rings.,The Chinese Remainder Theorem for Rings.,,"The Chinese Remainder Theorem for Rings. Let $R$ be a ring and $I$ and $J$ be ideals in $R$ such that $I+J = R$ . (a) Show that for any $r$ and $s$ in $R$ , the system of equations \begin{align*}   x & \equiv  r \pmod{I} \\    x & \equiv  s \pmod{J} \end{align*} has a solution. (b) In addition, prove that any two solutions of the system are congruent   modulo $I \cap J$ . (c) Let $I$ and $J$ be ideals in a ring $R$ such that $I + J = R$ . Show   that there exists a ring isomorphism $$ R/(I \cap J) \cong R/I \times R/J. $$ Solution: (a) Let's remind ourselves that $I + J = \{i + j : i \in I, j \in J\}$ . Because $I + J = R$ , there are $i \in I, j\in J$ with $i + j = 1$ . The solution of the system is $rj + si$ . We check both equations: \begin{align*} rj + si &\equiv rj \equiv ri + rj \equiv r(i + j) \equiv r \pmod{I} \\ rj + si &\equiv si \equiv si + sj \equiv s(i + j) \equiv s \pmod{J} \, . \end{align*} (b) Assume we have two different solutions $x$ and $x'$ . Then \begin{align*}  x &\equiv x' \pmod{I} \\  x &\equiv x' \pmod{J} \, , \end{align*} or else one of them wouldn't even be a solution. So $x - x'$ is in $I$ and $J$ , therefore $x - x' \in I \cap J$ and $x\equiv x' \pmod{I \cap J}$ . (c) The Cartesian product of two rings is a ring, so $R/I \times R/J$ is a ring. We look at the map \begin{align*}   \phi:  R &\rightarrow R/I \times R/J \\          x &\mapsto (x + I, x + J) \, . \end{align*} »Componentwise« ring homomorphisms are ring homomorphisms, so $\phi$ is a ring homomorphism. $\phi$ is surjective: by (a) for any $r\in R/I, s\in R/J$ there exists an $x \in R$ with $\phi(x) = (r, s)$ . The kernel of $\phi$ are the solutions of the system for $r = s = 0$ . By (b) every other solution must be congruent to $0$ modulo $I \cap J$ , so $\ker \phi = I \cap J$ . Then by the first isomorphism theorem for rings $$R/\ker(\phi) \cong \phi(R)$$ we obtain $$R/(I \cap J) \cong R/I \times R/J \, .$$ Could you please check, if my solution is correct? Thank you!","The Chinese Remainder Theorem for Rings. Let be a ring and and be ideals in such that . (a) Show that for any and in , the system of equations has a solution. (b) In addition, prove that any two solutions of the system are congruent   modulo . (c) Let and be ideals in a ring such that . Show   that there exists a ring isomorphism Solution: (a) Let's remind ourselves that . Because , there are with . The solution of the system is . We check both equations: (b) Assume we have two different solutions and . Then or else one of them wouldn't even be a solution. So is in and , therefore and . (c) The Cartesian product of two rings is a ring, so is a ring. We look at the map »Componentwise« ring homomorphisms are ring homomorphisms, so is a ring homomorphism. is surjective: by (a) for any there exists an with . The kernel of are the solutions of the system for . By (b) every other solution must be congruent to modulo , so . Then by the first isomorphism theorem for rings we obtain Could you please check, if my solution is correct? Thank you!","R I J R I+J =
R r s R \begin{align*}
  x & \equiv  r \pmod{I} \\ 
  x & \equiv  s \pmod{J}
\end{align*} I \cap J I J R I + J = R 
R/(I \cap J) \cong R/I \times R/J.
 I + J = \{i + j : i \in I, j \in J\} I + J = R i \in I, j\in J i + j = 1 rj + si \begin{align*}
rj + si &\equiv rj \equiv ri + rj \equiv r(i + j) \equiv r \pmod{I} \\
rj + si &\equiv si \equiv si + sj \equiv s(i + j) \equiv s \pmod{J} \, .
\end{align*} x x' \begin{align*}
 x &\equiv x' \pmod{I} \\
 x &\equiv x' \pmod{J} \, ,
\end{align*} x - x' I J x - x' \in I \cap J x\equiv x' \pmod{I \cap J} R/I \times R/J \begin{align*}
  \phi:  R &\rightarrow R/I \times R/J \\
         x &\mapsto (x + I, x + J) \, .
\end{align*} \phi \phi r\in R/I, s\in R/J x \in R \phi(x) = (r, s) \phi r = s = 0 0 I \cap J \ker \phi = I \cap J R/\ker(\phi) \cong \phi(R) R/(I \cap J) \cong R/I \times R/J \, .","['abstract-algebra', 'ring-theory', 'proof-verification', 'chinese-remainder-theorem']"
51,Odd/Even Permutations,Odd/Even Permutations,,How do you classify a permutation as odd or even (composition of an odd or even number of transpositions)? I somewhat understand the textbook definition of it but I'm having hard time conceptualizing and determining how it is actually determined if it's odd or even.,How do you classify a permutation as odd or even (composition of an odd or even number of transpositions)? I somewhat understand the textbook definition of it but I'm having hard time conceptualizing and determining how it is actually determined if it's odd or even.,,"['abstract-algebra', 'permutations']"
52,What is the difference between homomorphism and isomorphism?,What is the difference between homomorphism and isomorphism?,,"Let $G$ and $H$ be two groups, and $f$ a map from $G$ to $H$ ($\forall g\in G \Rightarrow f(g)\in H$). Then $f$ is a homomorphism if $\forall g_1,g_2\in G \Rightarrow f(g_1g_2)=f(g_1)f(g_2)$. This means that $G$ and $H$ are algebraically identical. Isomorphism is a bijective homomorphism. I see that isomorphism is more than homomorphism, but I don't really understand its power. When we hear about bijection, the first thing that comes to mind is topological homeomorphism, but here we are talking about algebraic structures, and topological spaces are not algebraic structures. Then what is the ""power"" that makes us to define isomorphism as a special case of homomorphism?","Let $G$ and $H$ be two groups, and $f$ a map from $G$ to $H$ ($\forall g\in G \Rightarrow f(g)\in H$). Then $f$ is a homomorphism if $\forall g_1,g_2\in G \Rightarrow f(g_1g_2)=f(g_1)f(g_2)$. This means that $G$ and $H$ are algebraically identical. Isomorphism is a bijective homomorphism. I see that isomorphism is more than homomorphism, but I don't really understand its power. When we hear about bijection, the first thing that comes to mind is topological homeomorphism, but here we are talking about algebraic structures, and topological spaces are not algebraic structures. Then what is the ""power"" that makes us to define isomorphism as a special case of homomorphism?",,"['abstract-algebra', 'group-theory']"
53,Yoneda-Lemma as generalization of Cayley`s theorem?,Yoneda-Lemma as generalization of Cayley`s theorem?,,"I came across the statement that Yoneda-lemma is a generalization of Cayley`s theorem which states, that every group is isomorphic to a group of permutations. How exactly is Yoneda-lemma a generalization of Cayley`s theorem? Can Cayley's theorem be deduced from Yoneda lemma, is it a generalization of a particular case of Yoneda, or is this instead, a philosophical statement? To me, it seems that Yoneda embedding is more canonical than Cayley's theorem because in the latter you have to choose whether the group acts from the left or from the right on itself. But maybe this is an optical illusion.","I came across the statement that Yoneda-lemma is a generalization of Cayley`s theorem which states, that every group is isomorphic to a group of permutations. How exactly is Yoneda-lemma a generalization of Cayley`s theorem? Can Cayley's theorem be deduced from Yoneda lemma, is it a generalization of a particular case of Yoneda, or is this instead, a philosophical statement? To me, it seems that Yoneda embedding is more canonical than Cayley's theorem because in the latter you have to choose whether the group acts from the left or from the right on itself. But maybe this is an optical illusion.",,"['abstract-algebra', 'group-theory', 'category-theory', 'yoneda-lemma']"
54,Union of the conjugates of a proper subgroup,Union of the conjugates of a proper subgroup,,Let G be a finite group and H be a proper subgroup. Prove that the union of the conjugates of H is not the whole of G. Thanks for any help,Let G be a finite group and H be a proper subgroup. Prove that the union of the conjugates of H is not the whole of G. Thanks for any help,,"['abstract-algebra', 'group-theory', 'finite-groups']"
55,Intuitive definitions of the Orbit and the Stabilizer,Intuitive definitions of the Orbit and the Stabilizer,,"I don't fully understand the definition of an Orbit. Mathematically, it is given by $$ \operatorname{Orb}(x) = \{y = gx \mid g \in G\} $$ where $G$ is a group and $x \in X$, a set that is acted upon by the group $G$, but what does this actually mean? Is it the set of all elements $x$ after having been acted upon by some element $g$? The stabilizer is given by $\{g \in G \mid gx = x\}$. So does this mean the set of all elements $g$, when after actings upon $x$ give you the element $x$? That is, the set of all elements $g$ whose action on $x$ doesn't change it?","I don't fully understand the definition of an Orbit. Mathematically, it is given by $$ \operatorname{Orb}(x) = \{y = gx \mid g \in G\} $$ where $G$ is a group and $x \in X$, a set that is acted upon by the group $G$, but what does this actually mean? Is it the set of all elements $x$ after having been acted upon by some element $g$? The stabilizer is given by $\{g \in G \mid gx = x\}$. So does this mean the set of all elements $g$, when after actings upon $x$ give you the element $x$? That is, the set of all elements $g$ whose action on $x$ doesn't change it?",,"['abstract-algebra', 'group-theory', 'group-actions']"
56,Irreducible polynomial which is reducible modulo every prime,Irreducible polynomial which is reducible modulo every prime,,"How to show that $x^4+1$ is irreducible in $\mathbb Z[x]$ but it is reducible modulo every prime $p$? For example I know that $x^4+1=(x+1)^4\bmod 2$. Also $\bmod 3$ we have that $0,1,2$ are not solutions of $x^4+1=0$ then if it is reducible the factors are of degree $2$. This gives that $x^4+1=(x^2+ax+b)(x^2+cx+d)$ and solving this system of equations $\bmod 3$  gives that $x^4+1=(x^2+x+2) (x^2+2x+2) \pmod 3$. But is there a simpler method to factor $x^4+1$ modulo a prime $p$?","How to show that $x^4+1$ is irreducible in $\mathbb Z[x]$ but it is reducible modulo every prime $p$? For example I know that $x^4+1=(x+1)^4\bmod 2$. Also $\bmod 3$ we have that $0,1,2$ are not solutions of $x^4+1=0$ then if it is reducible the factors are of degree $2$. This gives that $x^4+1=(x^2+ax+b)(x^2+cx+d)$ and solving this system of equations $\bmod 3$  gives that $x^4+1=(x^2+x+2) (x^2+2x+2) \pmod 3$. But is there a simpler method to factor $x^4+1$ modulo a prime $p$?",,"['abstract-algebra', 'polynomials', 'field-theory', 'finite-fields', 'irreducible-polynomials']"
57,Is a finite group uniquely determined by the orders of its elements?,Is a finite group uniquely determined by the orders of its elements?,,"Suppose we are given a finite group $G$ , and suppose we know the order of $G$ as well as the orders of each of its elements.  Does this information alone uniquely determine the group up to isomorphism?  What if we add extra hypotheses about $G$ (solvable, simple, etc)?  If not, can we at least extract some useful information regarding its structure? For example, if $|G| = 8$ and $G$ contains three elements of order $2$ and four elements of order $4$ , then I know $G \cong \mathbb{Z}_4\times\mathbb{Z}_2$ .","Suppose we are given a finite group , and suppose we know the order of as well as the orders of each of its elements.  Does this information alone uniquely determine the group up to isomorphism?  What if we add extra hypotheses about (solvable, simple, etc)?  If not, can we at least extract some useful information regarding its structure? For example, if and contains three elements of order and four elements of order , then I know .",G G G |G| = 8 G 2 4 G \cong \mathbb{Z}_4\times\mathbb{Z}_2,"['abstract-algebra', 'group-theory', 'finite-groups']"
58,Are there any interesting semigroups that aren't monoids?,Are there any interesting semigroups that aren't monoids?,,"Are there any interesting and natural examples of semigroups that are not monoids (that is, they don't have an identity element)? To be a bit more precise, I guess I should ask if there are any interesting examples of semigroups $(X, \ast)$ for which there is not a monoid $(X, \ast, e)$ where $e$ is in $X$ . I don't consider an example like the set of real numbers greater than $10$ (considered under addition) to be a sufficiently 'natural' semigroup for my purposes; if the domain can be extended in an obvious way to include an identity element then that's not what I'm after.","Are there any interesting and natural examples of semigroups that are not monoids (that is, they don't have an identity element)? To be a bit more precise, I guess I should ask if there are any interesting examples of semigroups for which there is not a monoid where is in . I don't consider an example like the set of real numbers greater than (considered under addition) to be a sufficiently 'natural' semigroup for my purposes; if the domain can be extended in an obvious way to include an identity element then that's not what I'm after.","(X, \ast) (X, \ast, e) e X 10","['big-list', 'abstract-algebra', 'semigroups', 'monoid']"
59,A commutative ring is a field iff the only ideals are $(0)$ and $(1)$,A commutative ring is a field iff the only ideals are  and,(0) (1),Let $R$ be a commutative ring with identity. Show that $R$ is a field if and only if the only ideals of $R$ are $R$ itself and the zero ideal $(0)$. I can't figure out where to start other that I need to prove some biconditional statement. Any help?,Let $R$ be a commutative ring with identity. Show that $R$ is a field if and only if the only ideals of $R$ are $R$ itself and the zero ideal $(0)$. I can't figure out where to start other that I need to prove some biconditional statement. Any help?,,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
60,An example of a division ring $D$ that is **not** isomorphic to its opposite ring,An example of a division ring  that is **not** isomorphic to its opposite ring,D,"I recall reading in an abstract algebra text two years ago (when I had the pleasure to learn this beautiful subject) that there exists a division ring $D$ that is not isomorphic to its opposite ring. However, the author noted that the construction of such a ring ""would take us too far afield"". My question is: please give an example of a division ring $D$ that is not isomorphic to its opposite ring. Let me recall that a division ring is a ring $A$ such that every non-zero element of $A$ is a unit, i.e., has a multiplicative inverse in $A$. However, division rings are not required to be commutative. Let me also recall that if $A$ is a ring, the opposite ring of $A$ is the ring $A^{\text{op}}$ that has the same underlying set as $A$, the same additive structure as $A$, but that the multiplication $*$ on $A^{\text{op}}$ is defined by the rule $a*b=ba$ where ""$ba$"" denotes the product of $b$ and $a$ with respect to the multiplication in $A$. It is not hard to find examples of rings which are not isomorphic to their opposite ring and it is trivial to find examples of noncommutative rings that are isomorphic to their opposite rings. (E.g., the $n\times n$ matrix ring over a field $F$ ($n>1$) is isomorphic to its opposite ring via the transpose map $A\to A^{T}$ where $A$ is a matrix and $A^{T}$ denotes its transpose.) Of course, every commutative ring is isomorphic to its opposite ring via the identity map. However, this particular question appears to be difficult.","I recall reading in an abstract algebra text two years ago (when I had the pleasure to learn this beautiful subject) that there exists a division ring $D$ that is not isomorphic to its opposite ring. However, the author noted that the construction of such a ring ""would take us too far afield"". My question is: please give an example of a division ring $D$ that is not isomorphic to its opposite ring. Let me recall that a division ring is a ring $A$ such that every non-zero element of $A$ is a unit, i.e., has a multiplicative inverse in $A$. However, division rings are not required to be commutative. Let me also recall that if $A$ is a ring, the opposite ring of $A$ is the ring $A^{\text{op}}$ that has the same underlying set as $A$, the same additive structure as $A$, but that the multiplication $*$ on $A^{\text{op}}$ is defined by the rule $a*b=ba$ where ""$ba$"" denotes the product of $b$ and $a$ with respect to the multiplication in $A$. It is not hard to find examples of rings which are not isomorphic to their opposite ring and it is trivial to find examples of noncommutative rings that are isomorphic to their opposite rings. (E.g., the $n\times n$ matrix ring over a field $F$ ($n>1$) is isomorphic to its opposite ring via the transpose map $A\to A^{T}$ where $A$ is a matrix and $A^{T}$ denotes its transpose.) Of course, every commutative ring is isomorphic to its opposite ring via the identity map. However, this particular question appears to be difficult.",,"['abstract-algebra', 'ring-theory', 'examples-counterexamples', 'noncommutative-algebra', 'division-algebras']"
61,Pathologies in module theory,Pathologies in module theory,,"Linear algebra is a very well-behaved part of mathematics. Soon after you have mastered the basics you got a good feeling for what kind of statements should be true -- even if you are not familiar with all major results and counterexamples. If one replaces the underlying field by a ring, and therefore looks at modules, things become more tricky. Many pathologies occur that one maybe would not expect coming from linear algebra. I am looking for a list of such pathologies where modules behave differently than vector spaces. This list should not only be a list of statements but all phenomena should be illustrated by an example. To start the list I will post an answer below with all pathologies that I know from the top of my head. This should also explain better what kind of list I have in mind.","Linear algebra is a very well-behaved part of mathematics. Soon after you have mastered the basics you got a good feeling for what kind of statements should be true -- even if you are not familiar with all major results and counterexamples. If one replaces the underlying field by a ring, and therefore looks at modules, things become more tricky. Many pathologies occur that one maybe would not expect coming from linear algebra. I am looking for a list of such pathologies where modules behave differently than vector spaces. This list should not only be a list of statements but all phenomena should be illustrated by an example. To start the list I will post an answer below with all pathologies that I know from the top of my head. This should also explain better what kind of list I have in mind.",,"['abstract-algebra', 'ring-theory', 'vector-spaces', 'modules', 'big-list']"
62,Is every group the automorphism group of a group?,Is every group the automorphism group of a group?,,"Suppose $G$ is a group. Does there always exist a group $H$, such that $\operatorname{Aut}(H)=G$, i. e. such that $G$ is the automorphism group of $H$? EDIT: It has been pointed out that  the answer to the above question is no . But I would be much more pleased if someone could give an example of such a group $G$ not arising as an automorphism group together with a comparatively easy proof of this fact.","Suppose $G$ is a group. Does there always exist a group $H$, such that $\operatorname{Aut}(H)=G$, i. e. such that $G$ is the automorphism group of $H$? EDIT: It has been pointed out that  the answer to the above question is no . But I would be much more pleased if someone could give an example of such a group $G$ not arising as an automorphism group together with a comparatively easy proof of this fact.",,"['group-theory', 'examples-counterexamples', 'abstract-algebra']"
63,Find all irreducible monic polynomials in $\mathbb{Z}/(2)[x]$ with degree equal or less than 5,Find all irreducible monic polynomials in  with degree equal or less than 5,\mathbb{Z}/(2)[x],"Find all irreducible monic polynomials in $\mathbb{Z}/(2)[x]$ with degree equal or less than $5$. This is what I tried: It's evident that $x,x+1$ are irreducible. Then, use these to find all reducible polynomials of degree 2. There ones that can't be made are irreducible. Then use these to make polynomials of degree 3, the ones that can't be made are irreducible. Repeat until degree 5. Doing this way takes way too long and I just gave up during when I was about to reach degree 4 polynomials. My question is: is there any easier way to find these polynomials? P.S.: this is not exactly homework, but a question which I came across while studying for an exam.","Find all irreducible monic polynomials in $\mathbb{Z}/(2)[x]$ with degree equal or less than $5$. This is what I tried: It's evident that $x,x+1$ are irreducible. Then, use these to find all reducible polynomials of degree 2. There ones that can't be made are irreducible. Then use these to make polynomials of degree 3, the ones that can't be made are irreducible. Repeat until degree 5. Doing this way takes way too long and I just gave up during when I was about to reach degree 4 polynomials. My question is: is there any easier way to find these polynomials? P.S.: this is not exactly homework, but a question which I came across while studying for an exam.",,"['abstract-algebra', 'finite-fields', 'irreducible-polynomials']"
64,For what $n$ is $U_n$ cyclic?,For what  is  cyclic?,n U_n,"When can we say a multiplicative group of integers modulo $n$, i.e., $U_n$ is  cyclic? $$U_n=\{a \in\mathbb  Z_n \mid \gcd(a,n)=1 \}$$ I searched the internet but did not get a clear idea.","When can we say a multiplicative group of integers modulo $n$, i.e., $U_n$ is  cyclic? $$U_n=\{a \in\mathbb  Z_n \mid \gcd(a,n)=1 \}$$ I searched the internet but did not get a clear idea.",,"['abstract-algebra', 'group-theory', 'cyclic-groups']"
65,Why is quadratic integer ring defined in that way?,Why is quadratic integer ring defined in that way?,,"Quadratic integer ring $\mathcal{O}$ is defined by \begin{equation}    \mathcal{O}=\begin{cases}       \mathbb{Z}[\sqrt{D}] & \text{if}\ D\equiv2,3\ \pmod 4\\       \mathbb{Z}\left[\frac{1+\sqrt{D}}{2}\right]\  & \text{if}\ D\equiv1\pmod 4    \end{cases} \end{equation} where $D$ is square-free. I understand $\mathbb{Z}\left[\frac{1+\sqrt{D}}{2}\right]$ is not closed under multiplication if $D\equiv2,3\pmod 4$ . But still, isn't it more natural to define $\mathcal{O}=\mathbb{Z}[\sqrt{D}]$ for all square-free $D$ ? (In that case, it really seems like 'quadratic integer'.) I wonder what is the motivation of this definition.","Quadratic integer ring is defined by where is square-free. I understand is not closed under multiplication if . But still, isn't it more natural to define for all square-free ? (In that case, it really seems like 'quadratic integer'.) I wonder what is the motivation of this definition.","\mathcal{O} \begin{equation}
   \mathcal{O}=\begin{cases}
      \mathbb{Z}[\sqrt{D}] & \text{if}\ D\equiv2,3\ \pmod 4\\
      \mathbb{Z}\left[\frac{1+\sqrt{D}}{2}\right]\  & \text{if}\ D\equiv1\pmod 4
   \end{cases}
\end{equation} D \mathbb{Z}\left[\frac{1+\sqrt{D}}{2}\right] D\equiv2,3\pmod 4 \mathcal{O}=\mathbb{Z}[\sqrt{D}] D","['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'quadratic-integer-rings']"
66,Order of a product of subgroups. Prove that $o(HK) = \frac{o(H)o(K)}{o(H \cap K)}$.,Order of a product of subgroups. Prove that .,o(HK) = \frac{o(H)o(K)}{o(H \cap K)},"Let $H$, $K$ be subgroups of $G$. Prove that $o(HK) = \frac{o(H)o(K)}{o(H \cap K)}$. I need this theorem to prove something.","Let $H$, $K$ be subgroups of $G$. Prove that $o(HK) = \frac{o(H)o(K)}{o(H \cap K)}$. I need this theorem to prove something.",,"['abstract-algebra', 'group-theory']"
67,Do finite algebraically closed fields exist?,Do finite algebraically closed fields exist?,,"Let $K$ be an algebraically closed field ($\operatorname{char}K=p$). Denote $${\mathbb F}_{p^n}=\{x\in K\mid x^{p^n}-x=0\}.$$ It's easy to prove that ${\mathbb F}_{p^n}$ consists of exactly $p^n$ elements. But if $|K|<p^n$, we have collision with previous statement (because ${\mathbb F}_{p^n}$ is subfield of $K$). So, are there any finite algebraically closed fields? And if they exist, where have I made a mistake? Thanks.","Let $K$ be an algebraically closed field ($\operatorname{char}K=p$). Denote $${\mathbb F}_{p^n}=\{x\in K\mid x^{p^n}-x=0\}.$$ It's easy to prove that ${\mathbb F}_{p^n}$ consists of exactly $p^n$ elements. But if $|K|<p^n$, we have collision with previous statement (because ${\mathbb F}_{p^n}$ is subfield of $K$). So, are there any finite algebraically closed fields? And if they exist, where have I made a mistake? Thanks.",,"['abstract-algebra', 'field-theory', 'galois-theory', 'finite-fields']"
68,What if $\pi$ was an algebraic number? (significance of algebraic numbers),What if  was an algebraic number? (significance of algebraic numbers),\pi,"To be honest, I never really understood the importance of algebraic numbers. If we lived in an universe where $\pi$ was algebraic, would there be a palpable difference between that universe and ours? My choice of $\pi$ for this question isn't really that important, any other ''famous'' transcendental number (i.e. $e$) could work. I'm aware there are a lot of open problems about deciding whether some number is transcendental or algebraic (for an example Apery's constant, Euler-Mascheroni constant and even $\pi + e$). Are those problems important only because they are hard to tackle? Are they important at all? If tomorrow was published a proof of algebraicity of those numbers, what would we gain from it? EDIT: OK, maybe I took too much ''artistic freedom'' with the title of my question. I wasn't really curious about alternate universes. Bottom line was: why are those proofs important? Why is ''being a transcendental number'' important property of a number?","To be honest, I never really understood the importance of algebraic numbers. If we lived in an universe where $\pi$ was algebraic, would there be a palpable difference between that universe and ours? My choice of $\pi$ for this question isn't really that important, any other ''famous'' transcendental number (i.e. $e$) could work. I'm aware there are a lot of open problems about deciding whether some number is transcendental or algebraic (for an example Apery's constant, Euler-Mascheroni constant and even $\pi + e$). Are those problems important only because they are hard to tackle? Are they important at all? If tomorrow was published a proof of algebraicity of those numbers, what would we gain from it? EDIT: OK, maybe I took too much ''artistic freedom'' with the title of my question. I wasn't really curious about alternate universes. Bottom line was: why are those proofs important? Why is ''being a transcendental number'' important property of a number?",,"['abstract-algebra', 'soft-question', 'pi', 'transcendental-numbers', 'algebraic-numbers']"
69,Appearance of Formal Derivative in Algebra,Appearance of Formal Derivative in Algebra,,"When studying polynomials, I know it is useful to introduce the concept of a formal derivative. For example, over a field, a polynomial has no repeated roots iff it and its formal derivative are coprime. My question is, should we be surprised to see the formal derivative here? Is there some way we can make sense of the appearance of the derivative (which to me is an analytic object) in algebra? I suspect it might have something to do with the fact that the derivative is linear and satisfies the product rule, which makes it a useful object to consider. It would also be interesting to hear an explanation which explains this in the context of algebraic geometry. Thanks!","When studying polynomials, I know it is useful to introduce the concept of a formal derivative. For example, over a field, a polynomial has no repeated roots iff it and its formal derivative are coprime. My question is, should we be surprised to see the formal derivative here? Is there some way we can make sense of the appearance of the derivative (which to me is an analytic object) in algebra? I suspect it might have something to do with the fact that the derivative is linear and satisfies the product rule, which makes it a useful object to consider. It would also be interesting to hear an explanation which explains this in the context of algebraic geometry. Thanks!",,"['abstract-algebra', 'algebraic-geometry', 'polynomials', 'soft-question']"
70,Why doesn't $0$ being a prime ideal in $\mathbb Z$ imply that $0$ is a prime number?,Why doesn't  being a prime ideal in  imply that  is a prime number?,0 \mathbb Z 0,"I know that $1$ is not a prime number because $1\cdot\mathbb Z=\mathbb Z$ is, by convention, not a prime ideal in the ring $\mathbb Z$. However, since $\mathbb Z$ is a domain, $0\cdot\mathbb Z=0$ is a prime ideal in $\mathbb Z$. Isn't $(p)$ being a prime ideal the very definition of $p$ being a prime element? (I know that this would violate the Fundamental Theorem of Arithmetic .) Edit: Apparently the answer is that a prime element in a ring is, by convention a non-zero non-unit (see wikipedia ). This is strange because a prime ideal of a ring is, by convention, a proper ideal but not necessarily non-zero (see wikipedia ). So, my question is now: Why do we make this awkward convention?","I know that $1$ is not a prime number because $1\cdot\mathbb Z=\mathbb Z$ is, by convention, not a prime ideal in the ring $\mathbb Z$. However, since $\mathbb Z$ is a domain, $0\cdot\mathbb Z=0$ is a prime ideal in $\mathbb Z$. Isn't $(p)$ being a prime ideal the very definition of $p$ being a prime element? (I know that this would violate the Fundamental Theorem of Arithmetic .) Edit: Apparently the answer is that a prime element in a ring is, by convention a non-zero non-unit (see wikipedia ). This is strange because a prime ideal of a ring is, by convention, a proper ideal but not necessarily non-zero (see wikipedia ). So, my question is now: Why do we make this awkward convention?",,"['number-theory', 'abstract-algebra', 'terminology', 'definition', 'ring-theory']"
71,Is every axiom in the definition of a vector space necessary?,Is every axiom in the definition of a vector space necessary?,,"Definition: A vector space over a field $K$ consists of a set $V$ and two binary operations $+: V \times V \to V$ and $\cdot: K \times V \to V$ satisfying the following axioms: Commutativity of $+$ . Associativity of $+$ . Existence of an identity element $\mathbf{0}$ for $+$ . Existence of inverses for $+$ . Compatibility of $\cdot$ with multiplication in $K$ . Distributivity of $\cdot$ over $+$ . Distributivity of $\cdot$ over addition in $K$ . $1_K$ is a left identity of $\cdot$ . Question: Are all seven of the previous axioms necessary (in the sense that weakening any one of them permits a structure which is not a vector space)? If not, which can be weakened (or removed)? EDIT: user7530 has quite cleverly shown that the commutativity of $+$ can be derived from axioms 2-8. Supposing we throw this out, can the remaining axioms all be proven necessary? EDIT 2: It was pointed out that axiom 3 cannot simply be thrown out, as the definition of an inverse in axiom 4 depends on the existence of $\mathbf{0}$ . What if we tweak the statement of axiom 4 to axiom 4': ""For every $x \in V$ , there exists $y \in V$ such that $(x+y)+x = x$ and $(y+x)+y = y$ ""? Is this weakened version equivalent to the original, and if so, does it allow the removal of axiom 3?","Definition: A vector space over a field consists of a set and two binary operations and satisfying the following axioms: Commutativity of . Associativity of . Existence of an identity element for . Existence of inverses for . Compatibility of with multiplication in . Distributivity of over . Distributivity of over addition in . is a left identity of . Question: Are all seven of the previous axioms necessary (in the sense that weakening any one of them permits a structure which is not a vector space)? If not, which can be weakened (or removed)? EDIT: user7530 has quite cleverly shown that the commutativity of can be derived from axioms 2-8. Supposing we throw this out, can the remaining axioms all be proven necessary? EDIT 2: It was pointed out that axiom 3 cannot simply be thrown out, as the definition of an inverse in axiom 4 depends on the existence of . What if we tweak the statement of axiom 4 to axiom 4': ""For every , there exists such that and ""? Is this weakened version equivalent to the original, and if so, does it allow the removal of axiom 3?",K V +: V \times V \to V \cdot: K \times V \to V + + \mathbf{0} + + \cdot K \cdot + \cdot K 1_K \cdot + \mathbf{0} x \in V y \in V (x+y)+x = x (y+x)+y = y,"['abstract-algebra', 'vector-spaces', 'definition', 'axioms']"
72,$G$ is non abelian simple group of order $<100$ then $G\cong A_5$,is non abelian simple group of order  then,G <100 G\cong A_5,"Question is to Prove that : $G$ is non abelian simple group of order $<100$ then $G\cong A_5$ Hint is to ""Eliminate all orders but $60$"". Which i think is not so easy to check. First of all, I eliminate all Primes (cyclic groups)  $2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97.$ (including 1) Only numbers i am left with are, $\{4,6,8,9,10,12,14,15,16,18,20,21,22,24,25,26,27,28,30,32,33,34,35,36,38,39,40,42,44,45,46,48,49,50,51,52,54,55,56,57,58,60,62,63,64,65,66,68,69,70,72,74,75,76,77,78,80,81,82,84,85,86,87,88,90,91,92,93,94,95,96,98,99\}$ Now, I eliminate all prime squares $p^2$ (which are abelian) $4,9,16,25,36,49,64,81$ Only numbers i am left with are, $\{6,8,10,12,14,15,18,20,21,22,24,26,27,28,30,32,33,34,35,38,39,40,42,44,45,46,48,50,51,52,54,55,56,57,58,60,62,63,65,66,68,69,70,72,74,75,76,77,78,80,82,84,85,86,87,88,90,91,92,93,94,95,96,98,99\}$ Now i eliminate all prime powers $p^n$ (which have non trivial center hence not simple) $2^3=8,2^5=32$ and $3^3=27$ Only numbers i am left with are, $\{6,10,12,14,15,18,20,21,22,24,26,28,30,33,34,35,38,39,40,42,44,45,46,48,50,51,52,54,55,56,57,58,60,62,63,65,66,68,69,70,72,74,75,76,77,78,80,82,84,85,86,87,88,90,91,92,93,94,95,96,98,99\}$ Now i eliminate all groups of order $pq$ where $p$ and $q$ are distinct (they have either normal sylow p or sylow q subgroup) From 2 - $\{2p=6,10,14,22,26,34,38, 46,58,62,74,82,86,94\}$ Only numbers i am left with are, $\{12,15,18,20,21,24,28,30,33,35,39,40,42,44,45,48,50,51,52,54,55,56,57,60,63,65,66,68,69,70,72,75,76,77,78,80,84,85,87,88,90,91,92,93,95,96,98,99\}$ From 3 - $\{ 3p=15,21,33,39,51,57,69,87,93\}$ Only numbers i am left with are, $\{12,18,20,24,28,30,35,40,42,44,45,48,50,52,54,55,56,60,63,65,66,68,70,72,75,76,77,78,80,84,85,88,90,91,92,95,96,98,99\}$ From 5- $\{5p= 10,15,35,55,65,85,95\}$ Only numbers i am left with are, $\{12,18,20,24,28,30,40,42,44,45,48,50,52,54,56,60,63,66,68,70,72,75,76,77,78,80,84,88,90,91,92,96,98,99\}$ From 7 - $\{7p= 14,21,35,49,77,91,\}$ Only numbers i am left with are, $\{12,18,20,24,28,30,40,42,44,45,48,50,52,54,56,60,63,66,68,70,72,75,76,78,80,84,88,90,92,96,98,99\}$ Remaining products $pq$ repeats. Now i eliminate all groups of order $p^2q$ (which are not simple as they have either normal sylow p or sylow q subgroup) From 2 - $4p=\{ 8,12,20,28,44,52,68,76,92,\}$ Only numbers i am left with are, $\{18,24,30,40,42,45,48,50,54,56,60,63,66,70,72,75,78,80,84,88,90,96,98,99\}$ From 3 - $9p= \{ 18,27,45,63,99\}$ Only numbers i am left with are, $\{24,30,40,42,48,50,54,56,60,66,70,72,75,78,80,84,88,90,96,98,\}$ From 5 - $25p =\{50,75 \}$ Only numbers i am left with are, $\{24,30,40,42,48,54,56,60,66,70,72,78,80,84,88,90,96,98,\}$ From 7 - $49p= \{ 98\}$ Only numbers i am left with are, $\{24,30,40,42,48,54,56,60,66,70,72,78,80,84,88,90,96,\}$ Now i eliminate all groups of order $pqr$, p,q,r are distinct primes (which are not simple as they have either normal sylow p or sylow q subgroup or sylow r subgroup) Only numbers we left with are $\{24,30,40,42,48,54,56,60,66,70,72,78,80,84,88,90,96\}$ $30=2.3.5$ So, $30$ Is eliminated $42=2.3.7$ So, $42$ is eliminated $66=2.3.11$ So, $66$ is eliminated $70=2.5.7$ So, $70$ is eliminated $78=2.3.13$ So, $78$ is eliminated Only numbers we are left with are $\{ 24,40,48,54,56,60,72,80,84,88,90,96\}$ $\textbf{EDIT}$ Assuming $G$ is simple, $|G|=24$. as $24=2^3.3$ No. of sylow 3 subgroups $1+3k$ divides $8$.thus, no of sylow $3$ subgroups has to be $4$. Suppose $n_2=3$ each sylow 3 subgroup has 2 non identity elements totally 8 non identity elements. each sylow 2 subgrousp has 7 non identity elements as we can not assume sylow 2 subgroups intersection is non trivial,$\textbf{INCOMPLETE}$ Only numbers we are left with are $\{40,48,54,56,60,72,80,84,88,90,96\}$ $40=2^3.5$ no.of sylow 5 dubgroups $1+5k$ divides $8$ Thus,sylow 5 sbgroup is unique and hence group is not simple. Only numbers we are left with are $\{48,54,56,60,72,80,84,88,90,96\}$ $48=2^4.3$ No. of sylow 3 subgroups $1+3k$ divides $16$ No. of sylow 2 subgroups $1+2k$ divides 3 suppose $n_2=3$ and $n_3=16$ Contribution from $P_3$(sylow 3 subgroup) is 45 and contribution from $P_2$ (sylow 2 subgroup) is 3 which adds up to 48 and with identity element we have 49 elements. Thus at least one of $n_3$ or $n_2$ is $1$ So, G is not simple. Only numbers we are left with are $\{54,56,60,72,80,84,88,90,96\}$ $54=2.3^3$ No. of sylow 3 sbgroups, $1+3k$ divides $2$ Thus sylow 3 subgroup is normal and hence group is not simple. Only numbers we are left with are $\{56,60,72,80,84,88,90,96\}$ $\textbf{EDIT}$ : Consider group $G$ of order $56$, for this we have $56=2^3.7$. Assuming this being simple group we would end up with the case that $n_2=7,n_7=8$. $n_p$ denotes no. of sylow p subgroups. each sylow $7$ subgroup has $6$ non identity elements, totally there are $8\times 6=48$ non identity elements in all sylow $7$ subgroups.\ each sylow $2$ subgroup has $7$ non identity elements. As there is a possibility that intersection of two sylow $2$ subgroups to be non trivial, there would be one more (non identity) element different from these seven non identity elements, adding upto 8 non identity elements, with $1$ identity element and adding upto $1+8+48=57$ ($48$ elements from sylow $7$ subgroups) contradicting the cardinality of order of group $|G|=56$. Thus, either $n_2=1$ or $n_7=1$.Thus, there exists a unique sylow $2$ subgroup or a unique sylow $7$ subgroup.Thus,$G$ is not simple. Only numbers we are left with are $\{60,72,80,84,88,90,96\}$ $80=2^4.5$ Possibilities for $n_2$ are $1,5$ possibilities for $n_5$ are $1,16$. Suppose  Suppose $n_2=5$ and $n_5=16$, then $P_5$ contributes $64$ elements and atleast $16$ non identity elements from $P_2$ adding up to $80$ excluding identity. Thus G is not simple Only numbers we are left with are $\{60,72,84,88,90,96\}$ $84=2^2.3.7$ with out much difficulty, one can see $n_7=1$ and thus, G is not simple. Only numbers we are left with are $\{60,72,88,90,96\}$ $88=2^3.11$ with out much difficulty, one can see that $n_{11}=1$ thus G is simple. Only numbers we are left with are $\{60,72,90,96\}$ I somehow managed to show groups of order $72,90,96$ are not simple. (My hands are paining I can not write more than this :D) So, I am left with group of order $60$ and we have $A_5$ with $|A_5|=60$ and $G\cong A_5$ I would be thankful if someone can check whether this is clear (or) not (even with any typos) and If possible give a hint for a simple way to arrive at required result. Thank You. P.S : To be frank, I have no idea how to solve this before writing this. I thought i would say at-least i know cyclic groups (prime order) are not simple and leave the rest to the other users and then I realized $p^2$ are not simple and so on tried eliminating one by one. at the time i came to the case of $72,90,96$ I got fed up ad blindly decided to assume they are simple (:P). I would write about that cases in a while in detail. P.S $2$ : Could any one please help me in concluding a group of order 24 being simple. I have edited a blunder in my argument. But could not able to proceed further.I am hoping for a proof which  use counting argument and no other results :)","Question is to Prove that : $G$ is non abelian simple group of order $<100$ then $G\cong A_5$ Hint is to ""Eliminate all orders but $60$"". Which i think is not so easy to check. First of all, I eliminate all Primes (cyclic groups)  $2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73,79,83,89,97.$ (including 1) Only numbers i am left with are, $\{4,6,8,9,10,12,14,15,16,18,20,21,22,24,25,26,27,28,30,32,33,34,35,36,38,39,40,42,44,45,46,48,49,50,51,52,54,55,56,57,58,60,62,63,64,65,66,68,69,70,72,74,75,76,77,78,80,81,82,84,85,86,87,88,90,91,92,93,94,95,96,98,99\}$ Now, I eliminate all prime squares $p^2$ (which are abelian) $4,9,16,25,36,49,64,81$ Only numbers i am left with are, $\{6,8,10,12,14,15,18,20,21,22,24,26,27,28,30,32,33,34,35,38,39,40,42,44,45,46,48,50,51,52,54,55,56,57,58,60,62,63,65,66,68,69,70,72,74,75,76,77,78,80,82,84,85,86,87,88,90,91,92,93,94,95,96,98,99\}$ Now i eliminate all prime powers $p^n$ (which have non trivial center hence not simple) $2^3=8,2^5=32$ and $3^3=27$ Only numbers i am left with are, $\{6,10,12,14,15,18,20,21,22,24,26,28,30,33,34,35,38,39,40,42,44,45,46,48,50,51,52,54,55,56,57,58,60,62,63,65,66,68,69,70,72,74,75,76,77,78,80,82,84,85,86,87,88,90,91,92,93,94,95,96,98,99\}$ Now i eliminate all groups of order $pq$ where $p$ and $q$ are distinct (they have either normal sylow p or sylow q subgroup) From 2 - $\{2p=6,10,14,22,26,34,38, 46,58,62,74,82,86,94\}$ Only numbers i am left with are, $\{12,15,18,20,21,24,28,30,33,35,39,40,42,44,45,48,50,51,52,54,55,56,57,60,63,65,66,68,69,70,72,75,76,77,78,80,84,85,87,88,90,91,92,93,95,96,98,99\}$ From 3 - $\{ 3p=15,21,33,39,51,57,69,87,93\}$ Only numbers i am left with are, $\{12,18,20,24,28,30,35,40,42,44,45,48,50,52,54,55,56,60,63,65,66,68,70,72,75,76,77,78,80,84,85,88,90,91,92,95,96,98,99\}$ From 5- $\{5p= 10,15,35,55,65,85,95\}$ Only numbers i am left with are, $\{12,18,20,24,28,30,40,42,44,45,48,50,52,54,56,60,63,66,68,70,72,75,76,77,78,80,84,88,90,91,92,96,98,99\}$ From 7 - $\{7p= 14,21,35,49,77,91,\}$ Only numbers i am left with are, $\{12,18,20,24,28,30,40,42,44,45,48,50,52,54,56,60,63,66,68,70,72,75,76,78,80,84,88,90,92,96,98,99\}$ Remaining products $pq$ repeats. Now i eliminate all groups of order $p^2q$ (which are not simple as they have either normal sylow p or sylow q subgroup) From 2 - $4p=\{ 8,12,20,28,44,52,68,76,92,\}$ Only numbers i am left with are, $\{18,24,30,40,42,45,48,50,54,56,60,63,66,70,72,75,78,80,84,88,90,96,98,99\}$ From 3 - $9p= \{ 18,27,45,63,99\}$ Only numbers i am left with are, $\{24,30,40,42,48,50,54,56,60,66,70,72,75,78,80,84,88,90,96,98,\}$ From 5 - $25p =\{50,75 \}$ Only numbers i am left with are, $\{24,30,40,42,48,54,56,60,66,70,72,78,80,84,88,90,96,98,\}$ From 7 - $49p= \{ 98\}$ Only numbers i am left with are, $\{24,30,40,42,48,54,56,60,66,70,72,78,80,84,88,90,96,\}$ Now i eliminate all groups of order $pqr$, p,q,r are distinct primes (which are not simple as they have either normal sylow p or sylow q subgroup or sylow r subgroup) Only numbers we left with are $\{24,30,40,42,48,54,56,60,66,70,72,78,80,84,88,90,96\}$ $30=2.3.5$ So, $30$ Is eliminated $42=2.3.7$ So, $42$ is eliminated $66=2.3.11$ So, $66$ is eliminated $70=2.5.7$ So, $70$ is eliminated $78=2.3.13$ So, $78$ is eliminated Only numbers we are left with are $\{ 24,40,48,54,56,60,72,80,84,88,90,96\}$ $\textbf{EDIT}$ Assuming $G$ is simple, $|G|=24$. as $24=2^3.3$ No. of sylow 3 subgroups $1+3k$ divides $8$.thus, no of sylow $3$ subgroups has to be $4$. Suppose $n_2=3$ each sylow 3 subgroup has 2 non identity elements totally 8 non identity elements. each sylow 2 subgrousp has 7 non identity elements as we can not assume sylow 2 subgroups intersection is non trivial,$\textbf{INCOMPLETE}$ Only numbers we are left with are $\{40,48,54,56,60,72,80,84,88,90,96\}$ $40=2^3.5$ no.of sylow 5 dubgroups $1+5k$ divides $8$ Thus,sylow 5 sbgroup is unique and hence group is not simple. Only numbers we are left with are $\{48,54,56,60,72,80,84,88,90,96\}$ $48=2^4.3$ No. of sylow 3 subgroups $1+3k$ divides $16$ No. of sylow 2 subgroups $1+2k$ divides 3 suppose $n_2=3$ and $n_3=16$ Contribution from $P_3$(sylow 3 subgroup) is 45 and contribution from $P_2$ (sylow 2 subgroup) is 3 which adds up to 48 and with identity element we have 49 elements. Thus at least one of $n_3$ or $n_2$ is $1$ So, G is not simple. Only numbers we are left with are $\{54,56,60,72,80,84,88,90,96\}$ $54=2.3^3$ No. of sylow 3 sbgroups, $1+3k$ divides $2$ Thus sylow 3 subgroup is normal and hence group is not simple. Only numbers we are left with are $\{56,60,72,80,84,88,90,96\}$ $\textbf{EDIT}$ : Consider group $G$ of order $56$, for this we have $56=2^3.7$. Assuming this being simple group we would end up with the case that $n_2=7,n_7=8$. $n_p$ denotes no. of sylow p subgroups. each sylow $7$ subgroup has $6$ non identity elements, totally there are $8\times 6=48$ non identity elements in all sylow $7$ subgroups.\ each sylow $2$ subgroup has $7$ non identity elements. As there is a possibility that intersection of two sylow $2$ subgroups to be non trivial, there would be one more (non identity) element different from these seven non identity elements, adding upto 8 non identity elements, with $1$ identity element and adding upto $1+8+48=57$ ($48$ elements from sylow $7$ subgroups) contradicting the cardinality of order of group $|G|=56$. Thus, either $n_2=1$ or $n_7=1$.Thus, there exists a unique sylow $2$ subgroup or a unique sylow $7$ subgroup.Thus,$G$ is not simple. Only numbers we are left with are $\{60,72,80,84,88,90,96\}$ $80=2^4.5$ Possibilities for $n_2$ are $1,5$ possibilities for $n_5$ are $1,16$. Suppose  Suppose $n_2=5$ and $n_5=16$, then $P_5$ contributes $64$ elements and atleast $16$ non identity elements from $P_2$ adding up to $80$ excluding identity. Thus G is not simple Only numbers we are left with are $\{60,72,84,88,90,96\}$ $84=2^2.3.7$ with out much difficulty, one can see $n_7=1$ and thus, G is not simple. Only numbers we are left with are $\{60,72,88,90,96\}$ $88=2^3.11$ with out much difficulty, one can see that $n_{11}=1$ thus G is simple. Only numbers we are left with are $\{60,72,90,96\}$ I somehow managed to show groups of order $72,90,96$ are not simple. (My hands are paining I can not write more than this :D) So, I am left with group of order $60$ and we have $A_5$ with $|A_5|=60$ and $G\cong A_5$ I would be thankful if someone can check whether this is clear (or) not (even with any typos) and If possible give a hint for a simple way to arrive at required result. Thank You. P.S : To be frank, I have no idea how to solve this before writing this. I thought i would say at-least i know cyclic groups (prime order) are not simple and leave the rest to the other users and then I realized $p^2$ are not simple and so on tried eliminating one by one. at the time i came to the case of $72,90,96$ I got fed up ad blindly decided to assume they are simple (:P). I would write about that cases in a while in detail. P.S $2$ : Could any one please help me in concluding a group of order 24 being simple. I have edited a blunder in my argument. But could not able to proceed further.I am hoping for a proof which  use counting argument and no other results :)",,['abstract-algebra']
73,Is $ f_n=\frac{(x+1)^n-(x^n+1)}{x}$ irreducible over $\mathbf{Z}$ for arbitrary $n$?,Is  irreducible over  for arbitrary ?, f_n=\frac{(x+1)^n-(x^n+1)}{x} \mathbf{Z} n,"In this document on page $3$ I found an interesting polynomial:   $$f_n=\frac{(x+1)^n-(x^n+1)}{x}.$$   Question is whether this polynomial is irreducible over $\mathbf{Q}$ for arbitrary $n \geq 1$. In the document you may find a proof that polynomial is irreducible whenever $n=2p , p \in \mathbf{P}$ and below is my attempt to prove that polynomial isn't irreducible whenever $n$ is odd number greater than $3$. Assuming that my proof is correct my question is : Is this polynomial irreducible over $\mathbf{Q}$ when $n=2k , k\in \mathbf{Z^+}$ \ $\mathbf{P} $ ? Lemma 1: For odd $n$, $a^n + b^n = (a+b)(a^{n-1} - a^{n-2} b + \cdots + b^{n-1})$. Binomial expansion: $(a+b)^n = \binom{n}{0} a^n + \binom{n}{1} a^{n-1} b + \cdots + \binom{n}{n} b^n$. $$\begin{align*} f(x)  &= \frac{(x+1)^n - x^n-1}{x} = \frac{(x+1)^n - (x^n+1)}{x} \\ &= \frac{(x+1) \cdot (x+1)^{n-1} - (x+1)(x^{n-1}-x^{n-2}+ \cdots + 1)}{x}  \\ &= \frac{(x+1) \cdot (x+1)^{n-1} - (x+1)(x^{n-1}-x^{n-2}+ \cdots + 1)}{x}  \\ &= \frac{(x+1) \cdot \left[ \Bigl(\binom{n-1}{0}x^{n-1}+ \binom{n-1}{1} x^{n-2} + \cdots + 1 \Bigr) - (x^{n-1}-x^{n-2}+ \cdots + 1) \right]}{x}  \\ &= \frac{(x+1) \cdot \left[ \Bigl(\binom{n-1}{0}x^{n-1}+ \binom{n-1}{1} x^{n-2} + \cdots + \binom{n-1}{n-2} x \Bigr) - (x^{n-1}-x^{n-2}+ \cdots - x) \right]}{x}  \\ &= (x+1) \cdot \small{\left[ \left(\binom{n-1}{0}x^{n-2}+ \binom{n-1}{1} x^{n-3} + \cdots + \binom{n-1}{n-2}  \right) - (x^{n-2}-x^{n-3}+ \cdots - 1) \right]} \end{align*}$$ So, when $n$ is an odd number greater than $1$, $f_n$ has factor $x+1$. Therefore, $f_n$ isn't irreducible whenever $n$ is an odd number greater than $3$.","In this document on page $3$ I found an interesting polynomial:   $$f_n=\frac{(x+1)^n-(x^n+1)}{x}.$$   Question is whether this polynomial is irreducible over $\mathbf{Q}$ for arbitrary $n \geq 1$. In the document you may find a proof that polynomial is irreducible whenever $n=2p , p \in \mathbf{P}$ and below is my attempt to prove that polynomial isn't irreducible whenever $n$ is odd number greater than $3$. Assuming that my proof is correct my question is : Is this polynomial irreducible over $\mathbf{Q}$ when $n=2k , k\in \mathbf{Z^+}$ \ $\mathbf{P} $ ? Lemma 1: For odd $n$, $a^n + b^n = (a+b)(a^{n-1} - a^{n-2} b + \cdots + b^{n-1})$. Binomial expansion: $(a+b)^n = \binom{n}{0} a^n + \binom{n}{1} a^{n-1} b + \cdots + \binom{n}{n} b^n$. $$\begin{align*} f(x)  &= \frac{(x+1)^n - x^n-1}{x} = \frac{(x+1)^n - (x^n+1)}{x} \\ &= \frac{(x+1) \cdot (x+1)^{n-1} - (x+1)(x^{n-1}-x^{n-2}+ \cdots + 1)}{x}  \\ &= \frac{(x+1) \cdot (x+1)^{n-1} - (x+1)(x^{n-1}-x^{n-2}+ \cdots + 1)}{x}  \\ &= \frac{(x+1) \cdot \left[ \Bigl(\binom{n-1}{0}x^{n-1}+ \binom{n-1}{1} x^{n-2} + \cdots + 1 \Bigr) - (x^{n-1}-x^{n-2}+ \cdots + 1) \right]}{x}  \\ &= \frac{(x+1) \cdot \left[ \Bigl(\binom{n-1}{0}x^{n-1}+ \binom{n-1}{1} x^{n-2} + \cdots + \binom{n-1}{n-2} x \Bigr) - (x^{n-1}-x^{n-2}+ \cdots - x) \right]}{x}  \\ &= (x+1) \cdot \small{\left[ \left(\binom{n-1}{0}x^{n-2}+ \binom{n-1}{1} x^{n-3} + \cdots + \binom{n-1}{n-2}  \right) - (x^{n-2}-x^{n-3}+ \cdots - 1) \right]} \end{align*}$$ So, when $n$ is an odd number greater than $1$, $f_n$ has factor $x+1$. Therefore, $f_n$ isn't irreducible whenever $n$ is an odd number greater than $3$.",,"['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
74,Prove that if $g^2=e$ for all $g$ in $G$ then $G$ is Abelian.,Prove that if  for all  in  then  is Abelian.,g^2=e g G G,"Prove that if $g^2=e$ for all $g$ in $G$ then $G$ is Abelian. This question is from group theory in Abstract Algebra and no matter how many times my lecturer teaches it for some reason I can't seem to crack it. (Please note that $e$ in the question is the group's identity.) Here's my attempt though... First I understand Abelian means that if $g_1$ and $g_2$ are elements of a group $G$ then they are Abelian if $g_1g_2=g_2g_1$... So, I begin by trying to play around with the elements of the group based on their definition... $$(g_2g_1)^r=e$$  $$(g_2g_1g_2g_2^{-1})^r=e$$ $$(g_2g_1g_2g_2^{-1}g_2g_1g_2g_2^{-1}...g_2g_1g_2g_2^{-1})=e$$ I assume that the $g_2^{-1}$'s and the $g_2$'s cancel out so that we end up with something like, $$g_2(g_1g_2)^rg_2^{-1}=e$$ $$g_2^{-1}g_2(g_1g_2)^r=g_2^{-1}g_2$$ Then ultimately... $$g_1g_2=e$$ I figure this is the answer. But I'm not totally sure. I always feel like I do too much in the pursuit of an answer when there's a simpler way. Reference: Fraleigh p. 49 Question 4.38 in A First Course in Abstract Algebra .","Prove that if $g^2=e$ for all $g$ in $G$ then $G$ is Abelian. This question is from group theory in Abstract Algebra and no matter how many times my lecturer teaches it for some reason I can't seem to crack it. (Please note that $e$ in the question is the group's identity.) Here's my attempt though... First I understand Abelian means that if $g_1$ and $g_2$ are elements of a group $G$ then they are Abelian if $g_1g_2=g_2g_1$... So, I begin by trying to play around with the elements of the group based on their definition... $$(g_2g_1)^r=e$$  $$(g_2g_1g_2g_2^{-1})^r=e$$ $$(g_2g_1g_2g_2^{-1}g_2g_1g_2g_2^{-1}...g_2g_1g_2g_2^{-1})=e$$ I assume that the $g_2^{-1}$'s and the $g_2$'s cancel out so that we end up with something like, $$g_2(g_1g_2)^rg_2^{-1}=e$$ $$g_2^{-1}g_2(g_1g_2)^r=g_2^{-1}g_2$$ Then ultimately... $$g_1g_2=e$$ I figure this is the answer. But I'm not totally sure. I always feel like I do too much in the pursuit of an answer when there's a simpler way. Reference: Fraleigh p. 49 Question 4.38 in A First Course in Abstract Algebra .",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
75,Does multiplying polynomials ever decrease the number of terms?,Does multiplying polynomials ever decrease the number of terms?,,"Let $p$ and $q$ be polynomials (maybe in several variables, over a field), and suppose they have $m$ and $n$ non-zero terms respectively. We can assume $m\leq n$. Can it ever happen that the product $p\cdot q$ has fewer than $m$ non-zero terms? I ask this because I vaguely recall seeing a positive answer in book somewhere (probably about computation or algorithms since the polynomials were unwieldy). If anyone knows what book this is from it would be much appreciated.","Let $p$ and $q$ be polynomials (maybe in several variables, over a field), and suppose they have $m$ and $n$ non-zero terms respectively. We can assume $m\leq n$. Can it ever happen that the product $p\cdot q$ has fewer than $m$ non-zero terms? I ask this because I vaguely recall seeing a positive answer in book somewhere (probably about computation or algorithms since the polynomials were unwieldy). If anyone knows what book this is from it would be much appreciated.",,"['abstract-algebra', 'reference-request', 'polynomials']"
76,Easy way to show that $\mathbb{Z}[\sqrt[3]{2}]$ is the ring of integers of $\mathbb{Q}[\sqrt[3]{2}]$,Easy way to show that  is the ring of integers of,\mathbb{Z}[\sqrt[3]{2}] \mathbb{Q}[\sqrt[3]{2}],This seems to be one of those tricky examples. I only know one proof which is quite complicated and follows by localizing $\mathbb{Z}[\sqrt[3]{2}]$ at different primes and then showing it's a DVR. Does anyone know any simple quick proof?,This seems to be one of those tricky examples. I only know one proof which is quite complicated and follows by localizing $\mathbb{Z}[\sqrt[3]{2}]$ at different primes and then showing it's a DVR. Does anyone know any simple quick proof?,,"['abstract-algebra', 'commutative-algebra', 'algebraic-number-theory', 'integral-dependence', 'integer-rings']"
77,Group of even order contains an element of order 2,Group of even order contains an element of order 2,,"I am working on the following problem from group theory: If $G$ is a group of order $2n$, show that the number of elements of $G$ of order $2$ is odd. That is, for some integer $k$, there are $2k+1$ elements $a$ such that $a \in G,\;\; a*a = e$, where $e$ is the identity element of $G$.","I am working on the following problem from group theory: If $G$ is a group of order $2n$, show that the number of elements of $G$ of order $2$ is odd. That is, for some integer $k$, there are $2k+1$ elements $a$ such that $a \in G,\;\; a*a = e$, where $e$ is the identity element of $G$.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'faq']"
78,Submodule of free module over a p.i.d. is free even when the module is not finitely generated?,Submodule of free module over a p.i.d. is free even when the module is not finitely generated?,,"I have heard that any submodule of a free module over a p.i.d. is free. I can prove this for finitely generated modules over a p.i.d.  But the proof involves induction on the number of generators, so it does not apply to modules that are not finitely generated. Does the result still hold?  What's the argument?","I have heard that any submodule of a free module over a p.i.d. is free. I can prove this for finitely generated modules over a p.i.d.  But the proof involves induction on the number of generators, so it does not apply to modules that are not finitely generated. Does the result still hold?  What's the argument?",,"['abstract-algebra', 'ring-theory', 'modules', 'principal-ideal-domains']"
79,How to deal with polynomial quotient rings,How to deal with polynomial quotient rings,,"The question is quite general and looks to explore the properties of quotient rings of the form $$\mathbb{Z}_{m}[X] / (f(x)) \quad \text{and} \quad \mathbb{R}[X]/(f(x))$$  where $m \in \mathbb{N}$ Classic examples of how one can treat such rings is to  find relationships like $$\mathbb{Z}[x]/(1-x,p) \cong \mathbb{Z_{p}}$$ for prime $p$. However, I often struggle to intuitively understand what the elements of such rings are, and they to compute using them. For example, what do the elements of the set $\mathbb{Z}_{2}[X] / (x^4+1)$ actually look like. Can this set be described in a clearer way to help understand the way the rings work. Is there some general way of describing elements of such rings so that isomorphisms and computations become easier to handle? Apologies if this question is not sufficiently clear.","The question is quite general and looks to explore the properties of quotient rings of the form $$\mathbb{Z}_{m}[X] / (f(x)) \quad \text{and} \quad \mathbb{R}[X]/(f(x))$$  where $m \in \mathbb{N}$ Classic examples of how one can treat such rings is to  find relationships like $$\mathbb{Z}[x]/(1-x,p) \cong \mathbb{Z_{p}}$$ for prime $p$. However, I often struggle to intuitively understand what the elements of such rings are, and they to compute using them. For example, what do the elements of the set $\mathbb{Z}_{2}[X] / (x^4+1)$ actually look like. Can this set be described in a clearer way to help understand the way the rings work. Is there some general way of describing elements of such rings so that isomorphisms and computations become easier to handle? Apologies if this question is not sufficiently clear.",,"['abstract-algebra', 'ring-theory', 'polynomial-rings']"
80,Group of order 15 is abelian,Group of order 15 is abelian,,How do I prove that a group of order 15 is abelian? Is there any general strategy to prove that a group of particular order (composite order) is abelian?,How do I prove that a group of order 15 is abelian? Is there any general strategy to prove that a group of particular order (composite order) is abelian?,,"['abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
81,"What is Lie Theory/ a Lie Group, simply?","What is Lie Theory/ a Lie Group, simply?",,"I'm studying physics, and I continually come across mentions of ""Lie Theory"" and ""Lie Groups"" as they relate to such topics as particle physics and String Theory, as well as vague mentions of ""symmetry"". I've attempted to read some texts on the topic and, while I feel I could probably sift through it in due time, it is very terse and esoteric. I've had group theory, calculus up to university calc II, and a teensy bit of analysis. Until I'm able to study this proper, what is Lie Theory/ a Lie Group, put simply? What is with these vague mentions of ""symmetry""? What can be understood in terms of what I know now?","I'm studying physics, and I continually come across mentions of ""Lie Theory"" and ""Lie Groups"" as they relate to such topics as particle physics and String Theory, as well as vague mentions of ""symmetry"". I've attempted to read some texts on the topic and, while I feel I could probably sift through it in due time, it is very terse and esoteric. I've had group theory, calculus up to university calc II, and a teensy bit of analysis. Until I'm able to study this proper, what is Lie Theory/ a Lie Group, put simply? What is with these vague mentions of ""symmetry""? What can be understood in terms of what I know now?",,"['abstract-algebra', 'group-theory', 'lie-groups', 'lie-algebras', 'intuition']"
82,How to find a generator of a cyclic group?,How to find a generator of a cyclic group?,,"A cyclic group is a group that is generated by a single element.  That means that there exists an element $g$, say, such that every other element of the group can be written as a power of $g$.  This element $g$ is the generator of the group. Is that a correct explanation for what a cyclic group and a generator are? How can we find the generator of a cyclic group and how can we say how many generators should there be?","A cyclic group is a group that is generated by a single element.  That means that there exists an element $g$, say, such that every other element of the group can be written as a power of $g$.  This element $g$ is the generator of the group. Is that a correct explanation for what a cyclic group and a generator are? How can we find the generator of a cyclic group and how can we say how many generators should there be?",,"['abstract-algebra', 'group-theory', 'cyclic-groups']"
83,showing that $n$th cyclotomic polynomial $\Phi_n(x)$ is irreducible over $\mathbb{Q}$,showing that th cyclotomic polynomial  is irreducible over,n \Phi_n(x) \mathbb{Q},"I studied the cyclotomic extension using Fraleigh's text. To prove that Galois group of the $n$ th cyclotomic extension has order $\phi(n)$ ( $\phi$ is the Euler's phi function.), the writer assumed, without proof, that $n$ th cyclotomic polynomial $\Phi_n(x)$ is irreducible over $\mathbb{Q}$ . I know that for n=p, p is prime, $\Phi_n(x)$ is irreducible over $\mathbb{Q}$ by Eisenstein's criterion. But I don't know how $\Phi_n(x)$ is irreducible over $\mathbb{Q}$ when n is not prime.","I studied the cyclotomic extension using Fraleigh's text. To prove that Galois group of the th cyclotomic extension has order ( is the Euler's phi function.), the writer assumed, without proof, that th cyclotomic polynomial is irreducible over . I know that for n=p, p is prime, is irreducible over by Eisenstein's criterion. But I don't know how is irreducible over when n is not prime.",n \phi(n) \phi n \Phi_n(x) \mathbb{Q} \Phi_n(x) \mathbb{Q} \Phi_n(x) \mathbb{Q},"['abstract-algebra', 'ring-theory', 'galois-theory', 'irreducible-polynomials', 'cyclotomic-polynomials']"
84,A (non-artificial) example of a ring without maximal ideals,A (non-artificial) example of a ring without maximal ideals,,"As a brief overview of the below, I am asking for: An example of a ring with no maximal ideals that is not a zero ring. A proof (or counterexample) that $R:=C_0(\mathbb{R})/C_c(\mathbb{R})$ is a ring with no maximal ideals. A homework question in my algebra class earlier this year asked to exhibit a ring (necessarily without identity) without any maximal (proper) ideals. For solutions to this, it suffices to exhibit an abelian group $(G,+)$ without maximal (proper) subgroups. For, given such a group, define multiplication to be constantly zero. In this case, $G$ becomes a zero ring without maximal ideals (because ideals correspond to subgroups). It is not particularly difficult to construct examples of the above. For example, consider $(\mathbb{Q},+)$. Another interesting example is $P=\{z\in\mathbb{C}\mid \exists n\in\mathbb{N}, z^{p^n}=1\}$ with standard complex number multiplication as ""addition"" (that is, as the abelian group operation). However, any example constructed in this manner is a zero ring, and as such seems ""artificial,"" which I admit, is not a rigorous term. I would like to find a somewhat less artificial example of a ring without maximal ideals. For a definition of ""less artificial,"" let us start with ""not a zero ring."" I have a candidate in mind, but I am having trouble explicitly proving that it has no maximal ideals. Let $C_0(\mathbb{R})$ denote the ring of continuous real-valued function on $\mathbb{R}$ vanishing at infinity. Let $C_c(\mathbb{R})$ denote the (two-sided) ideal of compactly supported functions. I believe that the ring $R:=C_0(\mathbb{R})/C_c(\mathbb{R})$ contains no maximal ideals, but I am having trouble showing it. My intuition for this problem is as follows. Given a function $f\in C_0(\mathbb{R})$, $f(x)$ approaches zero at some ""rate"" as $x\to\pm\infty$ (possibly different based on $\pm$). Furthermore, for any given rate, we can find a function with a larger rate, in the sense that we can find a $g\in C_0(\mathbb{R})$ such that $f(x)=o(g(x))$ ($f$ is little-$o$ of $g$). Now, even if $f$ is non-vanishing, there is no $h\in C_0(\mathbb{R})$ such that $fh=g$, for any $h$ could not vanish at infinity. Thus the principal ideal generated by $f$ does not contain $g$. By iterating this process we could construct a strictly ascending chain of principal ideals. Now, the idea is that the ring $R$ consists of these ""rates"" as described above. I know that this is not precise, or necessarily even correct. But it's my intuition. The previous paragraph shows that we can find an ascending chain of ""rates,"" but a lot of work still needs to be done. If anyone can clean this up, it would be much appreciated.","As a brief overview of the below, I am asking for: An example of a ring with no maximal ideals that is not a zero ring. A proof (or counterexample) that $R:=C_0(\mathbb{R})/C_c(\mathbb{R})$ is a ring with no maximal ideals. A homework question in my algebra class earlier this year asked to exhibit a ring (necessarily without identity) without any maximal (proper) ideals. For solutions to this, it suffices to exhibit an abelian group $(G,+)$ without maximal (proper) subgroups. For, given such a group, define multiplication to be constantly zero. In this case, $G$ becomes a zero ring without maximal ideals (because ideals correspond to subgroups). It is not particularly difficult to construct examples of the above. For example, consider $(\mathbb{Q},+)$. Another interesting example is $P=\{z\in\mathbb{C}\mid \exists n\in\mathbb{N}, z^{p^n}=1\}$ with standard complex number multiplication as ""addition"" (that is, as the abelian group operation). However, any example constructed in this manner is a zero ring, and as such seems ""artificial,"" which I admit, is not a rigorous term. I would like to find a somewhat less artificial example of a ring without maximal ideals. For a definition of ""less artificial,"" let us start with ""not a zero ring."" I have a candidate in mind, but I am having trouble explicitly proving that it has no maximal ideals. Let $C_0(\mathbb{R})$ denote the ring of continuous real-valued function on $\mathbb{R}$ vanishing at infinity. Let $C_c(\mathbb{R})$ denote the (two-sided) ideal of compactly supported functions. I believe that the ring $R:=C_0(\mathbb{R})/C_c(\mathbb{R})$ contains no maximal ideals, but I am having trouble showing it. My intuition for this problem is as follows. Given a function $f\in C_0(\mathbb{R})$, $f(x)$ approaches zero at some ""rate"" as $x\to\pm\infty$ (possibly different based on $\pm$). Furthermore, for any given rate, we can find a function with a larger rate, in the sense that we can find a $g\in C_0(\mathbb{R})$ such that $f(x)=o(g(x))$ ($f$ is little-$o$ of $g$). Now, even if $f$ is non-vanishing, there is no $h\in C_0(\mathbb{R})$ such that $fh=g$, for any $h$ could not vanish at infinity. Thus the principal ideal generated by $f$ does not contain $g$. By iterating this process we could construct a strictly ascending chain of principal ideals. Now, the idea is that the ring $R$ consists of these ""rates"" as described above. I know that this is not precise, or necessarily even correct. But it's my intuition. The previous paragraph shows that we can find an ascending chain of ""rates,"" but a lot of work still needs to be done. If anyone can clean this up, it would be much appreciated.",,"['abstract-algebra', 'ring-theory', 'asymptotics', 'examples-counterexamples', 'maximal-and-prime-ideals']"
85,Why isn't an infinite direct product of copies of $\Bbb Z$ a free module?,Why isn't an infinite direct product of copies of  a free module?,\Bbb Z,"Why isn't an infinite direct product of copies of $\Bbb Z$ a free module? Actually I was asked to show that it's not projective, but as $\Bbb{Z}$ is a PID, so it suffices to show it's not free. I am stuck here. I saw some questions in SE, but there is no satisfactory answer at all.","Why isn't an infinite direct product of copies of $\Bbb Z$ a free module? Actually I was asked to show that it's not projective, but as $\Bbb{Z}$ is a PID, so it suffices to show it's not free. I am stuck here. I saw some questions in SE, but there is no satisfactory answer at all.",,"['abstract-algebra', 'modules', 'projective-module']"
86,What are the left and right ideals of matrix ring? How about the two sided ideals?,What are the left and right ideals of matrix ring? How about the two sided ideals?,,What are the left and right ideals of matrix ring? How about the two sided ideals?,What are the left and right ideals of matrix ring? How about the two sided ideals?,,['abstract-algebra']
87,Are imaginary numbers really incomparable?,Are imaginary numbers really incomparable?,,"If we really don't know which is bigger if $ i $ is greater or $ 2i $ or so on then why do we plot $ i $ first then $ 2i $ and so on, on the imaginary axis of the Argand plane? My teacher said that imaginary numbers are just points and all are dimensionless so they are incomparable and the distance really doesn't matter. I want to get this more clear","If we really don't know which is bigger if $ i $ is greater or $ 2i $ or so on then why do we plot $ i $ first then $ 2i $ and so on, on the imaginary axis of the Argand plane? My teacher said that imaginary numbers are just points and all are dimensionless so they are incomparable and the distance really doesn't matter. I want to get this more clear",,['abstract-algebra']
88,Is a ring a set?,Is a ring a set?,,"We know that a ring consists of a set equipped with two binary operations. My question is whether a ring is a set or not. For example, we can have $(\mathbb{R},+,-)$ where $\mathbb{R}$ is a set and $+$ and $-$ are binary operations associated with the set. Note that binary operations are functions, and functions are set, so we have a 3-tuple consisting of three sets. My first question is whether this tuple itself is a set? i.e. what exactly is a tuple? In addition, the problem is I am not comfortable with defining ring as something with soemthing else. What exactly does it mean by ""with""? (for example, is it a union?) it just seems overly informal. Any help is apprecaited.","We know that a ring consists of a set equipped with two binary operations. My question is whether a ring is a set or not. For example, we can have $(\mathbb{R},+,-)$ where $\mathbb{R}$ is a set and $+$ and $-$ are binary operations associated with the set. Note that binary operations are functions, and functions are set, so we have a 3-tuple consisting of three sets. My first question is whether this tuple itself is a set? i.e. what exactly is a tuple? In addition, the problem is I am not comfortable with defining ring as something with soemthing else. What exactly does it mean by ""with""? (for example, is it a union?) it just seems overly informal. Any help is apprecaited.",,"['abstract-algebra', 'elementary-set-theory', 'ring-theory']"
89,Enlightening proof that the algebraic numbers form a field,Enlightening proof that the algebraic numbers form a field,,"The proof I'm familiar with that the algebraic numbers $\mathbb A$ form a field uses the fact that the resultant of two polynomials $p,q\in\mathbb Q[x]$ satisfies the following properties: It is $0$ iff $p$ and $q$ have a common factor. It is a polynomial in the coefficients of $p$ and $q$. We then introduce a new variable and cleverly manipulate $p$ and $q$ to get polynomials which vanish at the sums and products of their roots. This is in some ways a nice proof, e.g. it is constructive and so can be converted into an algorithm to find such polynomials (which I in fact just finished doing in C). But I don't find it very enlightening; it seems like the fact that $\mathbb A$ is a field is simply an accident. Is there a more enlightening proof of this fact?","The proof I'm familiar with that the algebraic numbers $\mathbb A$ form a field uses the fact that the resultant of two polynomials $p,q\in\mathbb Q[x]$ satisfies the following properties: It is $0$ iff $p$ and $q$ have a common factor. It is a polynomial in the coefficients of $p$ and $q$. We then introduce a new variable and cleverly manipulate $p$ and $q$ to get polynomials which vanish at the sums and products of their roots. This is in some ways a nice proof, e.g. it is constructive and so can be converted into an algorithm to find such polynomials (which I in fact just finished doing in C). But I don't find it very enlightening; it seems like the fact that $\mathbb A$ is a field is simply an accident. Is there a more enlightening proof of this fact?",,"['abstract-algebra', 'algebraic-number-theory']"
90,Example of modules that are projective but not free; torsion-free but not free,Example of modules that are projective but not free; torsion-free but not free,,"Free modules are projective, and projective modules are direct summands of free modules. Are there examples of projective modules that are not free? (I know this is not possible for modules of fields.) Free modules are torsion-free. But is the inverse true? Are there examples of torsion-free modules that are not free? Thank you~","Free modules are projective, and projective modules are direct summands of free modules. Are there examples of projective modules that are not free? (I know this is not possible for modules of fields.) Free modules are torsion-free. But is the inverse true? Are there examples of torsion-free modules that are not free? Thank you~",,"['abstract-algebra', 'commutative-algebra', 'modules']"
91,What is Abstract Algebra essentially?,What is Abstract Algebra essentially?,,"In the most basic sense, what is abstract algebra about? Wolfram Mathworld has the following definition: ""Abstract algebra is the set of advanced topics of algebra that deal with abstract algebraic structures rather than the usual number systems. The most important of these structures are groups, rings, and fields."" I find this, however, to say the least, not very informative. What do they mean by abstract algebraic structures? Along these lines, what are groups, rings, and fields then? I've been told by a friend that groups, essentially, are sets of objects, although, this still leaves me wondering what he means by objects (explicitly). I don't need anything rigorous. Just some intuitive definitions to give me some direction. Thanks!","In the most basic sense, what is abstract algebra about? Wolfram Mathworld has the following definition: ""Abstract algebra is the set of advanced topics of algebra that deal with abstract algebraic structures rather than the usual number systems. The most important of these structures are groups, rings, and fields."" I find this, however, to say the least, not very informative. What do they mean by abstract algebraic structures? Along these lines, what are groups, rings, and fields then? I've been told by a friend that groups, essentially, are sets of objects, although, this still leaves me wondering what he means by objects (explicitly). I don't need anything rigorous. Just some intuitive definitions to give me some direction. Thanks!",,"['abstract-algebra', 'group-theory', 'ring-theory', 'field-theory', 'definition']"
92,Ring of integers is a PID but not a Euclidean domain,Ring of integers is a PID but not a Euclidean domain,,"I have noticed that to prove fields like $\mathbb{Q}(i)$ and $\mathbb{Q}(e^{\frac{2\pi i}{3}})$ have class number one, we show they are Euclidean domains by tessellating the complex plane with the points $a+bv : a, b \in \mathbb{Z}$ , where $1, v$ is an integral basis.  Then any $c +dv$ in the field has distance $\leq 1$ from some lattice point.  On the other hand a similar geometric argument fails with the field $\mathbb{Q}(\sqrt{-5})$ , which does not have class number one. Any ring of integers of a finite extension of $\mathbb{Q}$ is a Dedekind domain, hence a PID if and only if a UFD.  But can such a ring be a PID but fail to be a Euclidean domain?","I have noticed that to prove fields like and have class number one, we show they are Euclidean domains by tessellating the complex plane with the points , where is an integral basis.  Then any in the field has distance from some lattice point.  On the other hand a similar geometric argument fails with the field , which does not have class number one. Any ring of integers of a finite extension of is a Dedekind domain, hence a PID if and only if a UFD.  But can such a ring be a PID but fail to be a Euclidean domain?","\mathbb{Q}(i) \mathbb{Q}(e^{\frac{2\pi i}{3}}) a+bv : a, b \in \mathbb{Z} 1, v c +dv \leq 1 \mathbb{Q}(\sqrt{-5}) \mathbb{Q}","['abstract-algebra', 'algebraic-number-theory', 'principal-ideal-domains', 'unique-factorization-domains']"
93,Intuition for étale morphisms,Intuition for étale morphisms,,"Currently working on algebraic surfaces over the complex numbers. I did a course on schemes but at the moment just work in the language of varieties. Now i encounter the term ""étale morphism"" every now and then (in the book by Beauville). I know Hartshorne's definition as a smooth morphism of relative dimension zero, and wikipedia states a bunch of equivalent ones. I can work with this, so no problem there. However, some more intuition about the concept would also be nice. So basically, if you have worked with étale morphisms, could you explain what's your personal intuition for such things, in the case of varieties? If in your answer you could also mention smooth and flat morphisms, that would be really appreciated. Thanks in advance! Joachim","Currently working on algebraic surfaces over the complex numbers. I did a course on schemes but at the moment just work in the language of varieties. Now i encounter the term ""étale morphism"" every now and then (in the book by Beauville). I know Hartshorne's definition as a smooth morphism of relative dimension zero, and wikipedia states a bunch of equivalent ones. I can work with this, so no problem there. However, some more intuition about the concept would also be nice. So basically, if you have worked with étale morphisms, could you explain what's your personal intuition for such things, in the case of varieties? If in your answer you could also mention smooth and flat morphisms, that would be really appreciated. Thanks in advance! Joachim",,"['abstract-algebra', 'algebraic-geometry', 'intuition', 'schemes']"
94,Why is the category of fields seemingly so poorly behaved?,Why is the category of fields seemingly so poorly behaved?,,"Compared to the categories of other “common” algebraic objects like groups and rings, it seems that fields as a whole are missing some important properties: There are no initial or terminal objects There are no free fields No products or coproducts Every arrow is a mono (maybe not a bad thing, but still indicates how restrictive the category is) A logician once told me in passing that part of the reason is that the properties for fields contain a decidedly “weird” property, namely that every element in a field except zero has a multiplicative inverse. If I understood him correctly, this property is sufficiently different from the others that the category of all such objects loses some features. But I have no idea if this was a heuristic or a proven theorem.","Compared to the categories of other “common” algebraic objects like groups and rings, it seems that fields as a whole are missing some important properties: There are no initial or terminal objects There are no free fields No products or coproducts Every arrow is a mono (maybe not a bad thing, but still indicates how restrictive the category is) A logician once told me in passing that part of the reason is that the properties for fields contain a decidedly “weird” property, namely that every element in a field except zero has a multiplicative inverse. If I understood him correctly, this property is sufficiently different from the others that the category of all such objects loses some features. But I have no idea if this was a heuristic or a proven theorem.",,"['abstract-algebra', 'category-theory', 'field-theory']"
95,Which finite groups are the group of units of some ring?,Which finite groups are the group of units of some ring?,,"Motivated by this question : Which finite groups are the group of units of some ring? Which finite groups are the group of units of some finite ring? Which finite abelian groups are the group of units of some commutative ring? Which finite abelian groups are the group of units of some finite commutative ring? It seems that even for fields this is not simple to answer. I expect the general question (Which groups are the group of units of some ring?, with no finiteness hypotheses) is even harder.","Motivated by this question : Which finite groups are the group of units of some ring? Which finite groups are the group of units of some finite ring? Which finite abelian groups are the group of units of some commutative ring? Which finite abelian groups are the group of units of some finite commutative ring? It seems that even for fields this is not simple to answer. I expect the general question (Which groups are the group of units of some ring?, with no finiteness hypotheses) is even harder.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'finite-rings']"
96,"Is Aluffi's ""Algebra: Chapter 0"" a good introduction to algebra?","Is Aluffi's ""Algebra: Chapter 0"" a good introduction to algebra?",,"I'm teaching myself following Algebra: Chapter 0 and up until now (I'm at chapter 3) I'm enjoying myself. In spite of that I have some doubts. It's not a standard text. All of the category theory, although I really like it, makes me feel like I'm not learning the ""serious"" algebra the way it supposed to be learned. So far the exercises are not particularly challenging. This worries me because I'm not a student at any university and I have no mathematician friends to talk to and so the only way for me to check my level of understanding is doing the exercises. From what I looked most of the courses on algebra contain materials such as Galois theory and representation theory which to my understanding are not present in this book. I have a copy of Artin's Algebra as well but I didn’t like it as much, although I understand it's very popular. Should I switch to that book? Ir maybe to a another book entirely?","I'm teaching myself following Algebra: Chapter 0 and up until now (I'm at chapter 3) I'm enjoying myself. In spite of that I have some doubts. It's not a standard text. All of the category theory, although I really like it, makes me feel like I'm not learning the ""serious"" algebra the way it supposed to be learned. So far the exercises are not particularly challenging. This worries me because I'm not a student at any university and I have no mathematician friends to talk to and so the only way for me to check my level of understanding is doing the exercises. From what I looked most of the courses on algebra contain materials such as Galois theory and representation theory which to my understanding are not present in this book. I have a copy of Artin's Algebra as well but I didn’t like it as much, although I understand it's very popular. Should I switch to that book? Ir maybe to a another book entirely?",,"['abstract-algebra', 'reference-request']"
97,"Quotient objects, their universal property and the isomorphism theorems","Quotient objects, their universal property and the isomorphism theorems",,"This is a question that has been bothering me for quite a while. Let me put between quotation marks the terms that are used informally. ""Quotient objects"" are always the same. Take groups, abelian groups, rings, topological vector spaces for example. Inside every object there are certain ""subobjects"" that we can divide by: normal subgroups, subgroups, ideals and subspaces with the subspace topology, respectively. I (think I) know from an universal algebra viewpoint (of which I know nothing) that in the case of ""algebraic"" objects, the ""subobjects"" we take the quotient by are determined by congruences: equivalence relations on the cartesian product that respect the operation(s). The first problem is: 1) How to define categorically the ""subobjects"" by which we take quotients? Now, the resulting ""quotient object"" $G/N$ always satisfies the same universal property: it comes with a morphism $\pi$ such that $\pi:G\to G/N$ and $N\subset ker \,\pi$, which is universal with respect to this: any other such morphism factors through $\pi$. The second (intimately linked) problem is: 2) How to define categorically these ""quotient objects"" and express in fancy terms the universal property they satisfy? Now, in ""algebraic"" categories there are well-known isomorphism theorems . In the Wikipedia link, it is explained that they are all a special case of the universal algebra statement. 3) How can we express them in categorical terms? Any insightful answer, even if partial, will be very welcome.","This is a question that has been bothering me for quite a while. Let me put between quotation marks the terms that are used informally. ""Quotient objects"" are always the same. Take groups, abelian groups, rings, topological vector spaces for example. Inside every object there are certain ""subobjects"" that we can divide by: normal subgroups, subgroups, ideals and subspaces with the subspace topology, respectively. I (think I) know from an universal algebra viewpoint (of which I know nothing) that in the case of ""algebraic"" objects, the ""subobjects"" we take the quotient by are determined by congruences: equivalence relations on the cartesian product that respect the operation(s). The first problem is: 1) How to define categorically the ""subobjects"" by which we take quotients? Now, the resulting ""quotient object"" $G/N$ always satisfies the same universal property: it comes with a morphism $\pi$ such that $\pi:G\to G/N$ and $N\subset ker \,\pi$, which is universal with respect to this: any other such morphism factors through $\pi$. The second (intimately linked) problem is: 2) How to define categorically these ""quotient objects"" and express in fancy terms the universal property they satisfy? Now, in ""algebraic"" categories there are well-known isomorphism theorems . In the Wikipedia link, it is explained that they are all a special case of the universal algebra statement. 3) How can we express them in categorical terms? Any insightful answer, even if partial, will be very welcome.",,"['abstract-algebra', 'category-theory']"
98,Explaining the product of two ideals,Explaining the product of two ideals,,My textbook says that the product of two ideals $I$ and $J$ is the set of all finite sums of elements of the form $ab$ with $a \in I$ and $b \in J$. What does this mean exactly? Can you give examples?,My textbook says that the product of two ideals $I$ and $J$ is the set of all finite sums of elements of the form $ab$ with $a \in I$ and $b \in J$. What does this mean exactly? Can you give examples?,,['abstract-algebra']
99,Why can we prove mathematically that a formula to solve an (n+5) order polynomial does not exist?,Why can we prove mathematically that a formula to solve an (n+5) order polynomial does not exist?,,"I understand that the quadratic equation can solve any second order polynomial. Furthermore, equations exist for polynomials up to fourth order. However, without a graduate level degree and a deep understanding of mathematics, is there an explanation of how we can prove that the equations for solutions to a fifth order and above don't exist? Thanks in advance.","I understand that the quadratic equation can solve any second order polynomial. Furthermore, equations exist for polynomials up to fourth order. However, without a graduate level degree and a deep understanding of mathematics, is there an explanation of how we can prove that the equations for solutions to a fifth order and above don't exist? Thanks in advance.",,"['abstract-algebra', 'galois-theory', 'quadratics']"
