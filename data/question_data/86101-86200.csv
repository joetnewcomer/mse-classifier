,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,This linear operator has no eigenvalues,This linear operator has no eigenvalues,,"Let $T : L^2(\mathbb R) \to L^2(\mathbb R)$ be a linear operator defined by  $$(Tf)(x)=f(x+1).$$ Show that $T$ has no eigenvalues, i.e., there exists no $f \not= 0$ in $L^2(\mathbb R)$ such that $(Tf)(x)=\lambda f(x)$ for any $\lambda \in \mathbb C$. My work: Okay, so the $\lambda=0$ case was straightforward for me: If $\lambda=0$, then $Tf(x)=0$, which means $f(x)=0$ because $T(0)=0$ since $T$ is linear. But I am stuck on the case $\lambda \in \mathbb C \setminus \{0\}$. How can I work with $f(x+1)=\lambda f(x)$ and show that $f(x)=0$?","Let $T : L^2(\mathbb R) \to L^2(\mathbb R)$ be a linear operator defined by  $$(Tf)(x)=f(x+1).$$ Show that $T$ has no eigenvalues, i.e., there exists no $f \not= 0$ in $L^2(\mathbb R)$ such that $(Tf)(x)=\lambda f(x)$ for any $\lambda \in \mathbb C$. My work: Okay, so the $\lambda=0$ case was straightforward for me: If $\lambda=0$, then $Tf(x)=0$, which means $f(x)=0$ because $T(0)=0$ since $T$ is linear. But I am stuck on the case $\lambda \in \mathbb C \setminus \{0\}$. How can I work with $f(x+1)=\lambda f(x)$ and show that $f(x)=0$?",,"['functional-analysis', 'eigenvalues-eigenvectors', 'operator-theory', 'linear-transformations', 'eigenfunctions']"
1,"Show that $A$ and $A^C$ are both dense in $(\ell^2,\lVert \cdot \rVert_2)$, where $A=\{x\in\ell_2:\sum_{k=1}^\infty x_k\neq0\}$.","Show that  and  are both dense in , where .","A A^C (\ell^2,\lVert \cdot \rVert_2) A=\{x\in\ell_2:\sum_{k=1}^\infty x_k\neq0\}","The title says it all. Showing $A$ is dense in $\ell_2$ seems easy; for any $x\notin A$, for each $n\in\mathbb N$ let $x^n$ in $\ell_2$ where $x^n$ is identical to $x$ except that $x^n_1=x_1 + \frac1n$. Then $x^n\to x$ (right?). The harder part to me is showing $A^C$ dense in $\ell_2$. I don't really know where to start here. I'm very surprised the statement is even true. I can't wrap my head around how a sequence $\{x^n\}\subseteq \ell_2$ such that $\sum_{k=1}^\infty x^n_k=0\ \forall n$ could possibly converge to, e.g., $x\in\ell_2$ where $\sum_{k=1}^\infty x_k=50$. Am I completely misunderstanding the question? Any pointers?","The title says it all. Showing $A$ is dense in $\ell_2$ seems easy; for any $x\notin A$, for each $n\in\mathbb N$ let $x^n$ in $\ell_2$ where $x^n$ is identical to $x$ except that $x^n_1=x_1 + \frac1n$. Then $x^n\to x$ (right?). The harder part to me is showing $A^C$ dense in $\ell_2$. I don't really know where to start here. I'm very surprised the statement is even true. I can't wrap my head around how a sequence $\{x^n\}\subseteq \ell_2$ such that $\sum_{k=1}^\infty x^n_k=0\ \forall n$ could possibly converge to, e.g., $x\in\ell_2$ where $\sum_{k=1}^\infty x_k=50$. Am I completely misunderstanding the question? Any pointers?",,"['functional-analysis', 'convergence-divergence', 'lp-spaces']"
2,"$f_1,...,f_n$ be linear functionals on a real vector space $V$, then is there a norm on $V$ which makes every $f_i$ continuous?","be linear functionals on a real vector space , then is there a norm on  which makes every  continuous?","f_1,...,f_n V V f_i","Let $V$ be a real vector space, $f_1,...,f_n$ be linear functionals on $V$; then does there exist a norm on $V$ with respect to which each of $f_i$ is continuous? And what if we have infinitely many, linearly independent, such functionals?","Let $V$ be a real vector space, $f_1,...,f_n$ be linear functionals on $V$; then does there exist a norm on $V$ with respect to which each of $f_i$ is continuous? And what if we have infinitely many, linearly independent, such functionals?",,['functional-analysis']
3,Minimum infinity norm control problem,Minimum infinity norm control problem,,"I am having trouble understanding Example 2 of section 5.9 of Luenberger's Optimization by Vector Space Methods . The problem is to select a current $u(t)$ on $[0,1]$ to drive a motor governed by $$ \ddot{\theta}(t) + \dot{\theta}(t) = u(t) $$ from $\theta(0) = \dot{\theta}(0) = 0$ to $\theta(1) = 1, \dot{\theta}(1) = 0$ minimizing $\max_{0 \leq t \leq 1} |u(t)|$. Integrating the differential equation allows one to express the constraints as $$ \dot{\theta}(1) = \int_0^1{e^{t-1}u(t)dt} = 0 \\ \theta(1) = \int_0^1{(u(t) - \ddot{\theta}(t))dt} = \int_0^1{(1 - e^{t-1})u(t)dt} = 1 $$ Defining $y_1$, $y_2 \in L_1[0,1]$ by $y_1(t) = e^{t-1}$, $y_2(t) = 1 - e^{t-1}$, we seek $u \in L_{\infty}[0,1]$ of minimum norm satisfying $$ \langle y_1, u \rangle = 0 \\ \langle y_2, u \rangle = 1, $$ where $\langle x, f \rangle$ denotes $f(x)$, $f$ being a bounded linear functional. By a previous theorem, $\min ||u|| = \max_{||a_1 y_1 + a_2 y_2|| \leq 1} a_2$. Luenberger writes ""Maximization of $a_2$ subject to this constraint is a straightforward task, but we do not carry out the necessary computations."". He then shows that the optimal $u$ is ""bang-bang"". I understand the transformation of the problem and why $u$ is bang-bang, and I know that once I have $a_1$ and $a_2$ I may determine $u$, since it changes sign at the same time as $a_1 y_1 + a_2 y_2$, and its absolute value is equal to $a_2$. But I do not know how to determine $a_1$ and $a_2$, and I do not see how maximization of $a_2$ subject to the integral constraint is a straightforward task. Any hints would be very welcome.","I am having trouble understanding Example 2 of section 5.9 of Luenberger's Optimization by Vector Space Methods . The problem is to select a current $u(t)$ on $[0,1]$ to drive a motor governed by $$ \ddot{\theta}(t) + \dot{\theta}(t) = u(t) $$ from $\theta(0) = \dot{\theta}(0) = 0$ to $\theta(1) = 1, \dot{\theta}(1) = 0$ minimizing $\max_{0 \leq t \leq 1} |u(t)|$. Integrating the differential equation allows one to express the constraints as $$ \dot{\theta}(1) = \int_0^1{e^{t-1}u(t)dt} = 0 \\ \theta(1) = \int_0^1{(u(t) - \ddot{\theta}(t))dt} = \int_0^1{(1 - e^{t-1})u(t)dt} = 1 $$ Defining $y_1$, $y_2 \in L_1[0,1]$ by $y_1(t) = e^{t-1}$, $y_2(t) = 1 - e^{t-1}$, we seek $u \in L_{\infty}[0,1]$ of minimum norm satisfying $$ \langle y_1, u \rangle = 0 \\ \langle y_2, u \rangle = 1, $$ where $\langle x, f \rangle$ denotes $f(x)$, $f$ being a bounded linear functional. By a previous theorem, $\min ||u|| = \max_{||a_1 y_1 + a_2 y_2|| \leq 1} a_2$. Luenberger writes ""Maximization of $a_2$ subject to this constraint is a straightforward task, but we do not carry out the necessary computations."". He then shows that the optimal $u$ is ""bang-bang"". I understand the transformation of the problem and why $u$ is bang-bang, and I know that once I have $a_1$ and $a_2$ I may determine $u$, since it changes sign at the same time as $a_1 y_1 + a_2 y_2$, and its absolute value is equal to $a_2$. But I do not know how to determine $a_1$ and $a_2$, and I do not see how maximization of $a_2$ subject to the integral constraint is a straightforward task. Any hints would be very welcome.",,"['functional-analysis', 'optimization', 'control-theory', 'duality-theorems', 'optimal-control']"
4,Hahn Banach extension of linear functional $f$,Hahn Banach extension of linear functional,f,"Let $f:(c_{00},\|\cdot\|_1)\to \mathbb C $ be a non zero continuous linear functional. The number of Hahn-Banach extensions of f to $(\ell^1,\|\cdot\|_1)$ is one two infinite three I have no idea how to get the result. Hahn Banach theorem on normed linear space: If I have a linear functional defined on some subspace then I can extend it on the whole vector Space with same norms. Here $\|x\|_1= \sum_{n=1}^\infty |x_n|$,where $x=(x_1,x_2....)\in c_{00} = $ the set of all sequences whose finite terms are non-zero. If $x\in \ell^1$, then $\|x\|_1= \sum_{n=1}^\infty |x_n|<\infty $. Someone help. Thanks.","Let $f:(c_{00},\|\cdot\|_1)\to \mathbb C $ be a non zero continuous linear functional. The number of Hahn-Banach extensions of f to $(\ell^1,\|\cdot\|_1)$ is one two infinite three I have no idea how to get the result. Hahn Banach theorem on normed linear space: If I have a linear functional defined on some subspace then I can extend it on the whole vector Space with same norms. Here $\|x\|_1= \sum_{n=1}^\infty |x_n|$,where $x=(x_1,x_2....)\in c_{00} = $ the set of all sequences whose finite terms are non-zero. If $x\in \ell^1$, then $\|x\|_1= \sum_{n=1}^\infty |x_n|<\infty $. Someone help. Thanks.",,['functional-analysis']
5,Definition of Dirac Delta function on the surface of a unit sphere,Definition of Dirac Delta function on the surface of a unit sphere,,"I am looking for a definition of a Dirac Delta function which is defined on the 2D unit sphere surface in 3D. In other words, I am looking for a function which is zero everywhere on the 2D spherical surface except at one point, (ex: (1, 1, 1)), and integral of the function over entire spherical surface is 1. I assume that this function must be very well defined and studied. I did some preliminary search, but I could not find such a definition. Can anyone help? Thank you.","I am looking for a definition of a Dirac Delta function which is defined on the 2D unit sphere surface in 3D. In other words, I am looking for a function which is zero everywhere on the 2D spherical surface except at one point, (ex: (1, 1, 1)), and integral of the function over entire spherical surface is 1. I assume that this function must be very well defined and studied. I did some preliminary search, but I could not find such a definition. Can anyone help? Thank you.",,"['complex-analysis', 'functional-analysis', 'analytic-number-theory']"
6,Showing that the operator is bounded and find its norm.,Showing that the operator is bounded and find its norm.,,"I have this operator $T: L^p(0,\infty)\rightarrow L^p(0,\infty)$, $1<p<\infty$ : $$(Tf)(x)=1/x\int_0^xf(t)dt.$$ I am supposed to show that it is bounded and fint its norm. I had an idea that almost worked for showing that it was bounded, but then I ran into a problem. And also I do not see how to show that its norm is $q$, where $q$ is given by $1/p+1/q=1$. First we have: $|(Tf)(x)|\le1/x\int_{[0,x]}|f|dt=1/x\int_{[0,x]}|f|\cdot1dt$, and then by Hölder: $$\le1/x(\int_{[0,x]}|f|^pdt)^{1/p}(\int_{[0,x]}1^gdt)^{1/q}\le \|f\|_px^{-1+1/q}=\|f\|_p/x^{1/p}.$$ But this function is not in $L^p(0,\infty)$. And showing that the norm is $q$, I really don't know how to do. Do you guys have any tips?","I have this operator $T: L^p(0,\infty)\rightarrow L^p(0,\infty)$, $1<p<\infty$ : $$(Tf)(x)=1/x\int_0^xf(t)dt.$$ I am supposed to show that it is bounded and fint its norm. I had an idea that almost worked for showing that it was bounded, but then I ran into a problem. And also I do not see how to show that its norm is $q$, where $q$ is given by $1/p+1/q=1$. First we have: $|(Tf)(x)|\le1/x\int_{[0,x]}|f|dt=1/x\int_{[0,x]}|f|\cdot1dt$, and then by Hölder: $$\le1/x(\int_{[0,x]}|f|^pdt)^{1/p}(\int_{[0,x]}1^gdt)^{1/q}\le \|f\|_px^{-1+1/q}=\|f\|_p/x^{1/p}.$$ But this function is not in $L^p(0,\infty)$. And showing that the norm is $q$, I really don't know how to do. Do you guys have any tips?",,"['real-analysis', 'functional-analysis', 'operator-theory', 'normed-spaces', 'lp-spaces']"
7,Prove that $l^2$ is closed and bounded but not compact.,Prove that  is closed and bounded but not compact.,l^2,Consider the space $l^p=\{(x_i);x_i\in \mathbb C:\sum |x_i|^2<\infty\}$ .Define a norm on $l^2$ by $||x||=\sqrt{\sum |x_i|^2}$. Prove that $l^2$ is closed and bounded but not compact. I know that in a finite dimensional space  a set is compact iff it is closed and bounded.But here the space is infinite dimensional;what should I do?Please give some hints.,Consider the space $l^p=\{(x_i);x_i\in \mathbb C:\sum |x_i|^2<\infty\}$ .Define a norm on $l^2$ by $||x||=\sqrt{\sum |x_i|^2}$. Prove that $l^2$ is closed and bounded but not compact. I know that in a finite dimensional space  a set is compact iff it is closed and bounded.But here the space is infinite dimensional;what should I do?Please give some hints.,,"['real-analysis', 'functional-analysis']"
8,Show that $\int_{\mathbb{R}} |f'(t)|^2+(9t^6+18t^4)|f(t)|^2 dt\ge 3$ for functions with unit $L^2$ norm,Show that  for functions with unit  norm,\int_{\mathbb{R}} |f'(t)|^2+(9t^6+18t^4)|f(t)|^2 dt\ge 3 L^2,"I want to show that $$g(f):=\int_{\mathbb{R}} |f'(t)|^2+(9t^6+18t^4)|f(t)|^2 dt$$ is bounded from below by $3$ for $f \in C_c^{\infty}(\mathbb{R})$ and $||f||_{L^2}=1.$ What is obvious is that $g$ is bounded below by $0,$ but I don't see how the $3$ comes into the game. Does anybody have an idea? My ideas so far: Throw away any of the terms, as they are all positive (does not sound that good to me, as it is a very bold approximation). Use Sobolev's inequality to eliminate the derivative. In particular, I think we have to do something about this polynomial there. Use the Fourier transform (Plancherel) to turn derivatives into polynomials and vice versa. If anything is unclear, please let me know.","I want to show that $$g(f):=\int_{\mathbb{R}} |f'(t)|^2+(9t^6+18t^4)|f(t)|^2 dt$$ is bounded from below by $3$ for $f \in C_c^{\infty}(\mathbb{R})$ and $||f||_{L^2}=1.$ What is obvious is that $g$ is bounded below by $0,$ but I don't see how the $3$ comes into the game. Does anybody have an idea? My ideas so far: Throw away any of the terms, as they are all positive (does not sound that good to me, as it is a very bold approximation). Use Sobolev's inequality to eliminate the derivative. In particular, I think we have to do something about this polynomial there. Use the Fourier transform (Plancherel) to turn derivatives into polynomials and vice versa. If anything is unclear, please let me know.",,"['real-analysis', 'functional-analysis', 'fourier-analysis']"
9,Is the space of smooth functions with the sup norm $\sigma$-compact?,Is the space of smooth functions with the sup norm -compact?,\sigma,"Let $Y$ denote the space of smooth functions $[0,1]\to\mathbb{R}$, equipped with the sup norm (i.e., topologized as a subspace of the usual space $C([0,1])$ of continuous functions on $[0,1]$).  Is $Y$ $\sigma$-compact? (Note that it follows from the Arzela-Ascoli theorem that for any $M$, the subset of $Y$ consisting of functions $f$ such that $|f(x)|\leq M$ and $|f'(x)|\leq M$ everywhere is precompact as a subset of $C([0,1])$, so $Y$ is $\sigma$- pre compact as a subset of $C([0,1])$.  However, these subsets are not closed, so they are not compact, and I don't see any way to construct ""large"" subsets of $Y$ that are compact.  A negative answer to this question would complete my answer to another question .)","Let $Y$ denote the space of smooth functions $[0,1]\to\mathbb{R}$, equipped with the sup norm (i.e., topologized as a subspace of the usual space $C([0,1])$ of continuous functions on $[0,1]$).  Is $Y$ $\sigma$-compact? (Note that it follows from the Arzela-Ascoli theorem that for any $M$, the subset of $Y$ consisting of functions $f$ such that $|f(x)|\leq M$ and $|f'(x)|\leq M$ everywhere is precompact as a subset of $C([0,1])$, so $Y$ is $\sigma$- pre compact as a subset of $C([0,1])$.  However, these subsets are not closed, so they are not compact, and I don't see any way to construct ""large"" subsets of $Y$ that are compact.  A negative answer to this question would complete my answer to another question .)",,"['real-analysis', 'functional-analysis']"
10,Is total variation a continuous map from complex measures to positive measures?,Is total variation a continuous map from complex measures to positive measures?,,"The following question arises naturally in my current research. It seems to be a basic problem in measure theory, and therefore I guess that answers to it can be found in some textbooks. However, I looked in a few books and found nothing. Since measure theory is not really my field of interest (I do symplectic geometry), I would appreciate any relevant observations and/or references to a text handling this issue. Let $\Omega$ be a bounded domain in $\mathbb{R}^n$, let $M_\mathbb{C}(\Omega)$ denote the space of complex Borel measures on $\Omega$, and let $M_+(\Omega)$ denote the space of positive Borel measures on $\Omega$. For every complex measure $\lambda\in M_\mathbb{C}(\Omega)$, the total variation of $\lambda$, denoted by $|\lambda|$, is a positive measure. In other words, we have$$|\cdot|:M_\mathbb{C}(\Omega)\to M_+(\Omega).$$We define weak convergence of measures in the usual manner, when thinking of a measure as a linear functional on the space of continuous functions. Namely, we say the sequence $\lambda_1,\lambda_2,\ldots$ of measures converges weakly to the measure $\lambda$, if we have$$\int_\Omega\varphi\lambda_n\to\int_\Omega\varphi\lambda$$for every continuous compactly supported $\varphi:\Omega\to\mathbb{R}$. Question: Is $|\cdot|$ continuous with respect to weak convergence? That is, if the sequence $(\lambda_n)$ converges weakly to $\lambda$, does it follow that the sequence $(|\lambda_n|)$ converges weakly to $|\lambda|$? If the answer is no, can we add some assumptions on the measures in question to change the picture? My intuition tells me that the answer should be positive without any further assumptions, but then again, I don't really know... As written above, any observations are welcome.","The following question arises naturally in my current research. It seems to be a basic problem in measure theory, and therefore I guess that answers to it can be found in some textbooks. However, I looked in a few books and found nothing. Since measure theory is not really my field of interest (I do symplectic geometry), I would appreciate any relevant observations and/or references to a text handling this issue. Let $\Omega$ be a bounded domain in $\mathbb{R}^n$, let $M_\mathbb{C}(\Omega)$ denote the space of complex Borel measures on $\Omega$, and let $M_+(\Omega)$ denote the space of positive Borel measures on $\Omega$. For every complex measure $\lambda\in M_\mathbb{C}(\Omega)$, the total variation of $\lambda$, denoted by $|\lambda|$, is a positive measure. In other words, we have$$|\cdot|:M_\mathbb{C}(\Omega)\to M_+(\Omega).$$We define weak convergence of measures in the usual manner, when thinking of a measure as a linear functional on the space of continuous functions. Namely, we say the sequence $\lambda_1,\lambda_2,\ldots$ of measures converges weakly to the measure $\lambda$, if we have$$\int_\Omega\varphi\lambda_n\to\int_\Omega\varphi\lambda$$for every continuous compactly supported $\varphi:\Omega\to\mathbb{R}$. Question: Is $|\cdot|$ continuous with respect to weak convergence? That is, if the sequence $(\lambda_n)$ converges weakly to $\lambda$, does it follow that the sequence $(|\lambda_n|)$ converges weakly to $|\lambda|$? If the answer is no, can we add some assumptions on the measures in question to change the picture? My intuition tells me that the answer should be positive without any further assumptions, but then again, I don't really know... As written above, any observations are welcome.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
11,"entire functions and multi-valued functions, an easy to understand explanation?","entire functions and multi-valued functions, an easy to understand explanation?",,"From wikipedia: The Bessel function of the first kind is an entire function if α is an   integer, otherwise it is a multivalued function with singularity at   zero. I have plotted the function $J_\alpha(x)$ for a few values of $\alpha$ in the $-10\le x\le 10$ interval: Plot[{Re[BesselJ[0, x]], Im[BesselJ[0, x]]}, {x, -10, 10}] $\alpha = 0$ Plot[{Re[BesselJ[1, x]], Im[BesselJ[1, x]]}, {x, -10, 10}] $\alpha = 1$ Plot[{Re[BesselJ[-2, x]], Im[BesselJ[-2, x]]}, {x, -10, 10}] $\alpha = -2$ Plot[{Re[BesselJ[5/4, x]], Im[BesselJ[5/4, x]]}, {x, -20, 20}] $\alpha = \frac{5}{4}$ Plot[{Re[BesselJ[-2/3, x]], Im[BesselJ[-2/3, x]]}, {x, -20, 20}] $\alpha = \frac{-2}{3}$ Plot[{Re[BesselJ[Sqrt[2], x]], Im[BesselJ[Sqrt[2], x]]}, {x, -20, 20}] $\alpha = \sqrt{2}$ Plot[{Re[BesselJ[-Sqrt[3], x]], Im[BesselJ[-Sqrt[3], x]]}, {x, -20,    20}] $\alpha = -\sqrt{3}$ Plot[{Re[BesselJ[2 + I, x]], Im[BesselJ[2 + I, x]]}, {x, -20, 20}] $\alpha = 2+i$ Plot[{Re[BesselJ[-2 + I, x]], Im[BesselJ[-2 + I, x]]}, {x, -20, 20}] $\alpha = -2+i$ Plot[{Re[BesselJ[10, x]], Im[BesselJ[10, x]]}, {x, -100, 100}] $\alpha = 10$ Seems that for integer values of $\alpha$, the function $J_\alpha(x)$ is real-valued but for other values $\alpha\in(\mathbb R-\mathbb Z)$ the function has complex values. Could you please give an easy and intuitive explanation for the concepts entire function and multivalued function based on these plots?","From wikipedia: The Bessel function of the first kind is an entire function if α is an   integer, otherwise it is a multivalued function with singularity at   zero. I have plotted the function $J_\alpha(x)$ for a few values of $\alpha$ in the $-10\le x\le 10$ interval: Plot[{Re[BesselJ[0, x]], Im[BesselJ[0, x]]}, {x, -10, 10}] $\alpha = 0$ Plot[{Re[BesselJ[1, x]], Im[BesselJ[1, x]]}, {x, -10, 10}] $\alpha = 1$ Plot[{Re[BesselJ[-2, x]], Im[BesselJ[-2, x]]}, {x, -10, 10}] $\alpha = -2$ Plot[{Re[BesselJ[5/4, x]], Im[BesselJ[5/4, x]]}, {x, -20, 20}] $\alpha = \frac{5}{4}$ Plot[{Re[BesselJ[-2/3, x]], Im[BesselJ[-2/3, x]]}, {x, -20, 20}] $\alpha = \frac{-2}{3}$ Plot[{Re[BesselJ[Sqrt[2], x]], Im[BesselJ[Sqrt[2], x]]}, {x, -20, 20}] $\alpha = \sqrt{2}$ Plot[{Re[BesselJ[-Sqrt[3], x]], Im[BesselJ[-Sqrt[3], x]]}, {x, -20,    20}] $\alpha = -\sqrt{3}$ Plot[{Re[BesselJ[2 + I, x]], Im[BesselJ[2 + I, x]]}, {x, -20, 20}] $\alpha = 2+i$ Plot[{Re[BesselJ[-2 + I, x]], Im[BesselJ[-2 + I, x]]}, {x, -20, 20}] $\alpha = -2+i$ Plot[{Re[BesselJ[10, x]], Im[BesselJ[10, x]]}, {x, -100, 100}] $\alpha = 10$ Seems that for integer values of $\alpha$, the function $J_\alpha(x)$ is real-valued but for other values $\alpha\in(\mathbb R-\mathbb Z)$ the function has complex values. Could you please give an easy and intuitive explanation for the concepts entire function and multivalued function based on these plots?",,"['functional-analysis', 'functions', 'definition', 'intuition', 'bessel-functions']"
12,"Let $Y$ be a finite-dimensional normed space, $X$ a normed space, and $T: X \to Y$ a surjective linear operator. Show that $T$ is an open mapping.","Let  be a finite-dimensional normed space,  a normed space, and  a surjective linear operator. Show that  is an open mapping.",Y X T: X \to Y T,"Let $Y$ be a finite-dimensional normed space, $X$ a normed space, and $T: X \to Y$ a surjective linear operator. Show that $T$ is an open mapping. I think if I can show that $T(B_X)$ contains an open ball then I am done where $B_X$ is the unit ball in $X$ . But I am unable to show that. Need some help...","Let be a finite-dimensional normed space, a normed space, and a surjective linear operator. Show that is an open mapping. I think if I can show that contains an open ball then I am done where is the unit ball in . But I am unable to show that. Need some help...",Y X T: X \to Y T T(B_X) B_X X,"['functional-analysis', 'operator-theory', 'normed-spaces', 'open-map']"
13,Solving Inner Product Equations,Solving Inner Product Equations,,"I'm trying to solve an exercise from Cheney's Analysis for Applied Mathematics . Let $X$ be a normed linear space with $a,b,c\in X$ taken as fixed vectors, and consider the equation $x+\langle x, a\rangle c = b$ The goal being to find a general solution $x$. There are obvious case by case trivial solutions (i.e. when $b\perp a$, $x=b$). Based on the fact that this exercise is in the first section on Hilbert spaces, I'm inclined to think I should be able to be solved by applying axioms for an inner product space. I've tried taking the inner product of both sides with some combination of $a,b,$ or $c$, but thus far, to no success.","I'm trying to solve an exercise from Cheney's Analysis for Applied Mathematics . Let $X$ be a normed linear space with $a,b,c\in X$ taken as fixed vectors, and consider the equation $x+\langle x, a\rangle c = b$ The goal being to find a general solution $x$. There are obvious case by case trivial solutions (i.e. when $b\perp a$, $x=b$). Based on the fact that this exercise is in the first section on Hilbert spaces, I'm inclined to think I should be able to be solved by applying axioms for an inner product space. I've tried taking the inner product of both sides with some combination of $a,b,$ or $c$, but thus far, to no success.",,"['analysis', 'functional-analysis', 'inner-products']"
14,What does the notation $C(\bar U)$ mean for $U\subset\Bbb{R}^d$ open?,What does the notation  mean for  open?,C(\bar U) U\subset\Bbb{R}^d,"Let $U$ be an open subset of $\Bbb{R}^d$. In Evans's PDE book,  $$ C(U)=\{u: U\to\Bbb{R} \mid u\ \hbox{continuous}\} $$ and  $$ C(\bar U)=\{u\in C(U)\mid u\ \hbox{ is uniformly continuous on bounded subsets of}\ U\}. \tag{1} $$ I've seen $C(\bar U)$ defined as $$ C(\bar U)=\{u: \bar U\to\Bbb{R} \mid u\ \hbox{continuous}\}  \tag{2} $$ before. Is it the same as Evans's version? What's the point of the definition (*) in Evans's book? (2) obviously implies (1). To show (1) implies (2), I set it as an exercise: Suppose $f:U\to \Bbb{R}$ is continuous and for any bounded $A\subset U$, $f$ is uniformly continuous on $A$. Then $f$ has a continuous extension on $\bar U$. The bounded case is trivial. How can one have the general case that $U$ might be an unbounded open set? As a nontrivial special case, consider $U=(0,\infty)$ and $f\in C(U)$ such that it satisfies (1). How should I define $f(0)$?","Let $U$ be an open subset of $\Bbb{R}^d$. In Evans's PDE book,  $$ C(U)=\{u: U\to\Bbb{R} \mid u\ \hbox{continuous}\} $$ and  $$ C(\bar U)=\{u\in C(U)\mid u\ \hbox{ is uniformly continuous on bounded subsets of}\ U\}. \tag{1} $$ I've seen $C(\bar U)$ defined as $$ C(\bar U)=\{u: \bar U\to\Bbb{R} \mid u\ \hbox{continuous}\}  \tag{2} $$ before. Is it the same as Evans's version? What's the point of the definition (*) in Evans's book? (2) obviously implies (1). To show (1) implies (2), I set it as an exercise: Suppose $f:U\to \Bbb{R}$ is continuous and for any bounded $A\subset U$, $f$ is uniformly continuous on $A$. Then $f$ has a continuous extension on $\bar U$. The bounded case is trivial. How can one have the general case that $U$ might be an unbounded open set? As a nontrivial special case, consider $U=(0,\infty)$ and $f\in C(U)$ such that it satisfies (1). How should I define $f(0)$?",,"['real-analysis', 'functional-analysis']"
15,Example of a function in $L^2(\mathbb{R})$ with derivative not in $L^2(\mathbb{R})$.,Example of a function in  with derivative not in .,L^2(\mathbb{R}) L^2(\mathbb{R}),"We know examples of a function which doesn't lie in $L^2(\mathbb{R})$ with derivatives in $L^2(\mathbb{R})$: $$f_1(x) = \mathrm{arctg}(x) \notin L^2(\mathbb{R}), \qquad f_1'(x) = \frac{1}{x^2+1}\in L^2(\mathbb{R}): \qquad \int\limits_{\mathbb{R}}\left(\frac{1}{x^2+1}\right)^2\,dx = \frac{\pi}{2};$$ $$f_2(x) = \mathrm{Si}(x)\notin L^2(\mathbb{R}), \qquad f_2'(x) = \frac{\sin(x)}{x}\in L^2(\mathbb{R}): \qquad \int\limits_{\mathbb{R}}\left(\frac{\sin(x)}{x}\right)^2\,dx = \pi;$$ $$f_3(x) = \mathrm{erf}(x)\notin L^2(\mathbb{R}), \qquad f_3'(x) = e^{-x^2}\in L^2(\mathbb{R}): \qquad \int\limits_{\mathbb{R}}e^{-2x^2}\,dx = \sqrt{\frac{\pi}{2}}.$$   Are there any classical examples of such function $f\in L^2(\mathbb{R})$ that its strong derivative $f'$ doesn't lie in $L^2(\mathbb{R})$?","We know examples of a function which doesn't lie in $L^2(\mathbb{R})$ with derivatives in $L^2(\mathbb{R})$: $$f_1(x) = \mathrm{arctg}(x) \notin L^2(\mathbb{R}), \qquad f_1'(x) = \frac{1}{x^2+1}\in L^2(\mathbb{R}): \qquad \int\limits_{\mathbb{R}}\left(\frac{1}{x^2+1}\right)^2\,dx = \frac{\pi}{2};$$ $$f_2(x) = \mathrm{Si}(x)\notin L^2(\mathbb{R}), \qquad f_2'(x) = \frac{\sin(x)}{x}\in L^2(\mathbb{R}): \qquad \int\limits_{\mathbb{R}}\left(\frac{\sin(x)}{x}\right)^2\,dx = \pi;$$ $$f_3(x) = \mathrm{erf}(x)\notin L^2(\mathbb{R}), \qquad f_3'(x) = e^{-x^2}\in L^2(\mathbb{R}): \qquad \int\limits_{\mathbb{R}}e^{-2x^2}\,dx = \sqrt{\frac{\pi}{2}}.$$   Are there any classical examples of such function $f\in L^2(\mathbb{R})$ that its strong derivative $f'$ doesn't lie in $L^2(\mathbb{R})$?",,"['functional-analysis', 'derivatives', 'lebesgue-integral', 'examples-counterexamples']"
16,No Hilbert space can have countable Hamel basis without using Baire's Category theorem,No Hilbert space can have countable Hamel basis without using Baire's Category theorem,,"I have to prove that no Hilbert space can have countable Hamel basis just using the fact that any finite dimensional subspace is closed (more specifically without using Baire's theorem). I saw a paper by NAM-KIu TSING solving the same problem for Banach space. But, the proof is not much intuitive. Is it possible to give a easier proof for Hilbert space ? Using proof by contradiction, the aim is to somehow find a Cauchy sequence and then use completeness to get a limit and show that cauchy sequence does not converge to that limit. Thanks","I have to prove that no Hilbert space can have countable Hamel basis just using the fact that any finite dimensional subspace is closed (more specifically without using Baire's theorem). I saw a paper by NAM-KIu TSING solving the same problem for Banach space. But, the proof is not much intuitive. Is it possible to give a easier proof for Hilbert space ? Using proof by contradiction, the aim is to somehow find a Cauchy sequence and then use completeness to get a limit and show that cauchy sequence does not converge to that limit. Thanks",,"['functional-analysis', 'hilbert-spaces']"
17,Some questions about an exercise about $C^\infty \subset L^\infty$,Some questions about an exercise about,C^\infty \subset L^\infty,"Let  $$ L^\infty (\mathbb R) = \{f : \mathbb R \to \mathbb C\mid \text{essential sup of } f < \infty \text{ and } f \text{ Borel measurable} \}$$ and $$ C^\infty (\mathbb R ) = \{ f: \mathbb R \to \mathbb C \mid f \text{ continuous }, \lim_{|x|\to \infty}f(x) = 0 \}$$ I was going to solve the following exercise but then realised there were a few things that were not clear to me. Here is the exercise: Prove that there exists an element $\lambda \in (L^\infty (\mathbb R))^\ast$ such that $\lambda (f) = f(0) $ for all $f \in C^\infty(\mathbb R)$. Hint: Use consequences of the Hahn-Banach theorem. Here are my thoughts and questions: (1) It's clear to me that $C^\infty$ is a subspace of the space of bounded functions $B(\mathbb R)$ but here in this exercise $L^\infty$ is a space consisting of elements that are equivalence classes. Is this not a problem? Does $C^\infty$ embed into $L^\infty$? (I'm not   sure embed is the correct word...) (2) Say we use representatives of $L^\infty$ so that (1) is not a problem. Then to me it seems that the evaluation map $f \mapsto f(0)$ is an element of $(L^\infty)^\ast$. And this would answer the question but the hint suggests I am missing something. What am I missing?","Let  $$ L^\infty (\mathbb R) = \{f : \mathbb R \to \mathbb C\mid \text{essential sup of } f < \infty \text{ and } f \text{ Borel measurable} \}$$ and $$ C^\infty (\mathbb R ) = \{ f: \mathbb R \to \mathbb C \mid f \text{ continuous }, \lim_{|x|\to \infty}f(x) = 0 \}$$ I was going to solve the following exercise but then realised there were a few things that were not clear to me. Here is the exercise: Prove that there exists an element $\lambda \in (L^\infty (\mathbb R))^\ast$ such that $\lambda (f) = f(0) $ for all $f \in C^\infty(\mathbb R)$. Hint: Use consequences of the Hahn-Banach theorem. Here are my thoughts and questions: (1) It's clear to me that $C^\infty$ is a subspace of the space of bounded functions $B(\mathbb R)$ but here in this exercise $L^\infty$ is a space consisting of elements that are equivalence classes. Is this not a problem? Does $C^\infty$ embed into $L^\infty$? (I'm not   sure embed is the correct word...) (2) Say we use representatives of $L^\infty$ so that (1) is not a problem. Then to me it seems that the evaluation map $f \mapsto f(0)$ is an element of $(L^\infty)^\ast$. And this would answer the question but the hint suggests I am missing something. What am I missing?",,"['functional-analysis', 'solution-verification']"
18,Gateaux derivative of $L_p$ norm,Gateaux derivative of  norm,L_p,"For $2\leq p < \infty$, if we consider $f,g \in L_p(X, \mathcal{M},\mu)$ there is the well-known equality $$\frac{d}{dt}\Vert f+tg \Vert_p^p = \frac{p}{2} \int_X \vert f(x)+tg(x) \vert^{p-2} \left( 2t \vert g(x) \vert ^2 + f(x)\overline{g(x)} + g(x)\overline{f(x)} \right) d\mu(x),$$ which can be evaluated at $t = 0$ to get the Gateaux derivative of $\Vert \cdot \vert_p^p.$  I'm trying to prove this equality and the trick seems to be to move the differentiation inside. I'm trying to use Lebesgue dominated convergence, and I get stuck trying to bound the integrand of $$ \frac{\Vert f+(t+h)g \Vert_p^p - \Vert f+tg \Vert_p^p}{h} = \int_X \frac{\vert f(x)+(t+h)g(x) \vert^p - \vert f(x)+tg(x) \vert^p}{h} d\mu(x). $$ The only thing that seems to be available is convexity of $\vert x \vert^p$, but I haven't been able to use it to obtain a useful bound.","For $2\leq p < \infty$, if we consider $f,g \in L_p(X, \mathcal{M},\mu)$ there is the well-known equality $$\frac{d}{dt}\Vert f+tg \Vert_p^p = \frac{p}{2} \int_X \vert f(x)+tg(x) \vert^{p-2} \left( 2t \vert g(x) \vert ^2 + f(x)\overline{g(x)} + g(x)\overline{f(x)} \right) d\mu(x),$$ which can be evaluated at $t = 0$ to get the Gateaux derivative of $\Vert \cdot \vert_p^p.$  I'm trying to prove this equality and the trick seems to be to move the differentiation inside. I'm trying to use Lebesgue dominated convergence, and I get stuck trying to bound the integrand of $$ \frac{\Vert f+(t+h)g \Vert_p^p - \Vert f+tg \Vert_p^p}{h} = \int_X \frac{\vert f(x)+(t+h)g(x) \vert^p - \vert f(x)+tg(x) \vert^p}{h} d\mu(x). $$ The only thing that seems to be available is convexity of $\vert x \vert^p$, but I haven't been able to use it to obtain a useful bound.",,"['real-analysis', 'functional-analysis', 'gateaux-derivative']"
19,"Prob. 6, Sec. 4.3, in Kreyszig's Functional Analysis Book: What are all possible extensions?","Prob. 6, Sec. 4.3, in Kreyszig's Functional Analysis Book: What are all possible extensions?",,"Here's Theorem 4.3-2 in Introductory Functional Analysis With Applications by Erwine  Kreyszig: Let $X$ be a normed space, let $Z$ be a subspace of $X$ , and let $f$ be a bounded linear functional defined on $Z$ . Then there exists a bounded linear functional $\tilde{f}$ defined on $X$ such that $$\tilde{f}(x)  =  f(x) \mbox{ for all } x \in Z, \ \mbox{ and } \ \left\lVert \tilde{f} \right\rVert_X = \lVert f \rVert_Z,$$ where $$\lVert f \rVert_Z := \begin{cases}  \sup \left\{  \frac{\lvert f(z) \rvert }{\lVert z \rVert } \colon z \in Z, z \neq \mathbf{0}  \right\} & \mbox{ if } Z \neq \{ \mathbf{0} \}; \\ 0 & \mbox{ otherwise}. \end{cases}$$ And, $$ \left\lVert \tilde{f} \right\rVert_X \colon= \sup \left\{ \,  \frac{ \left\lvert \tilde{f}(x) \right\rvert }{ \lVert x \rVert } \colon  x \in X, x \neq \mathbf{0} \, \right\}.$$ Now let $X \colon= \mathbb{R}^3$ with the Euclidean norm, let $a \colon= (\alpha_1, \alpha_2, 0) \in X$ , and let $$Z \colon= \{ \ (\xi_1, \xi_2, \xi_3) \in \mathbb{R}^3 \ \colon \ \xi_3 = 0 \ \}.$$ Let $f$ be defined on $Z$ by $$f(z) \colon= \alpha_1 \xi_1 + \alpha_2 \xi_2 \ \mbox{ for all } \ z \colon= (\xi_1, \xi_2, 0) \in Z.$$ Then what are all the possible linear extensions $\tilde{f}$ of $f$ as gauranteed by Theorem 4.3-2 in Kreyszig? Here, $$\Vert f \Vert = \Vert a \Vert = \sqrt{ \alpha_1^2 + \alpha_2^2}.$$ Of course, one possible extension $\tilde{f}$ is given by $$\tilde{f}(x) \colon= \alpha_1 \xi_1 + \alpha_2 \xi_2 \ \mbox{ for all } \ x \colon= (\xi_1, \xi_2, \xi_3) \in \mathbb{R}^3. $$ Am I right? If so, then what are other such extensions $\tilde{f}$ , if any?","Here's Theorem 4.3-2 in Introductory Functional Analysis With Applications by Erwine  Kreyszig: Let be a normed space, let be a subspace of , and let be a bounded linear functional defined on . Then there exists a bounded linear functional defined on such that where And, Now let with the Euclidean norm, let , and let Let be defined on by Then what are all the possible linear extensions of as gauranteed by Theorem 4.3-2 in Kreyszig? Here, Of course, one possible extension is given by Am I right? If so, then what are other such extensions , if any?","X Z X f Z \tilde{f} X \tilde{f}(x)  =  f(x) \mbox{ for all } x \in Z, \ \mbox{ and } \ \left\lVert \tilde{f} \right\rVert_X = \lVert f \rVert_Z, \lVert f \rVert_Z := \begin{cases}  \sup \left\{  \frac{\lvert f(z) \rvert }{\lVert z \rVert } \colon z \in Z, z \neq \mathbf{0}  \right\} & \mbox{ if } Z \neq \{ \mathbf{0} \}; \\ 0 & \mbox{ otherwise}. \end{cases}  \left\lVert \tilde{f} \right\rVert_X \colon= \sup \left\{ \,  \frac{ \left\lvert \tilde{f}(x) \right\rvert }{ \lVert x \rVert } \colon  x \in X, x \neq \mathbf{0} \, \right\}. X \colon= \mathbb{R}^3 a \colon= (\alpha_1, \alpha_2, 0) \in X Z \colon= \{ \ (\xi_1, \xi_2, \xi_3) \in \mathbb{R}^3 \ \colon \ \xi_3 = 0 \ \}. f Z f(z) \colon= \alpha_1 \xi_1 + \alpha_2 \xi_2 \ \mbox{ for all } \ z \colon= (\xi_1, \xi_2, 0) \in Z. \tilde{f} f \Vert f \Vert = \Vert a \Vert = \sqrt{ \alpha_1^2 + \alpha_2^2}. \tilde{f} \tilde{f}(x) \colon= \alpha_1 \xi_1 + \alpha_2 \xi_2 \ \mbox{ for all } \ x \colon= (\xi_1, \xi_2, \xi_3) \in \mathbb{R}^3.  \tilde{f}","['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'normed-spaces']"
20,Why has the Stein operator for normal approximations the form $(\mathcal Af)(x)=f^\prime(x)-xf(x)$?,Why has the Stein operator for normal approximations the form ?,(\mathcal Af)(x)=f^\prime(x)-xf(x),"My Question: Why has the Stein operator $\mathcal A$ for normal approximations the form $(\mathcal Af)(x)=f^\prime(x)-xf(x)$? How can one deduce this form of the operator? Reason for my question: I try to understand Stein's method . So far I understand, that one can use this method to find estimates for distances between random variables $W$ and $N$ of the form $$\sup_{h\in\mathcal H} |E[h(W)]-E[h(N)]|$$ where $N$ shall be an approximation of $W$ (I am interested in the case, where $N$ has the standard normal distribution). First one sets $g(x)=h(x)-E[h(N)]$ such that $$|E[h(W)]-E[h(N)]| = |E[g(W)]|$$ Instead of estimating $|E[h(W)]-E[h(N)]|$ one can also estimate $|E[g(W)]|$ which doesn't include $N$ (this step is convincing for me). To find estimates easily one sets $(\mathcal A f)(x)=g(x)$ with a certain operator $\mathcal A$ (the Stein operator). For approximations against the normal distribution $(\mathcal Af)(x)=f^\prime(x)-xf(x)$ is used. Thus $$|E[h(W)]-E[h(N)]| = |E[f^\prime(W)-Wf(W)]|$$ I saw in the proof of the Berry-Esseen theorem , that $|E[f^\prime(W)-Wf(W)]|$ can be more easily estimated than $|E[h(W)]-E[h(N)]|$. What I do not understand, is why $(\mathcal Af)(x)=f^\prime(x)-xf(x)$ was chosen in the first place. Is it just a lucky guess?! Which chain of thoughts lead me to the choice $(\mathcal Af)(x)=f^\prime(x)-xf(x)$ for the normal approximation?","My Question: Why has the Stein operator $\mathcal A$ for normal approximations the form $(\mathcal Af)(x)=f^\prime(x)-xf(x)$? How can one deduce this form of the operator? Reason for my question: I try to understand Stein's method . So far I understand, that one can use this method to find estimates for distances between random variables $W$ and $N$ of the form $$\sup_{h\in\mathcal H} |E[h(W)]-E[h(N)]|$$ where $N$ shall be an approximation of $W$ (I am interested in the case, where $N$ has the standard normal distribution). First one sets $g(x)=h(x)-E[h(N)]$ such that $$|E[h(W)]-E[h(N)]| = |E[g(W)]|$$ Instead of estimating $|E[h(W)]-E[h(N)]|$ one can also estimate $|E[g(W)]|$ which doesn't include $N$ (this step is convincing for me). To find estimates easily one sets $(\mathcal A f)(x)=g(x)$ with a certain operator $\mathcal A$ (the Stein operator). For approximations against the normal distribution $(\mathcal Af)(x)=f^\prime(x)-xf(x)$ is used. Thus $$|E[h(W)]-E[h(N)]| = |E[f^\prime(W)-Wf(W)]|$$ I saw in the proof of the Berry-Esseen theorem , that $|E[f^\prime(W)-Wf(W)]|$ can be more easily estimated than $|E[h(W)]-E[h(N)]|$. What I do not understand, is why $(\mathcal Af)(x)=f^\prime(x)-xf(x)$ was chosen in the first place. Is it just a lucky guess?! Which chain of thoughts lead me to the choice $(\mathcal Af)(x)=f^\prime(x)-xf(x)$ for the normal approximation?",,"['functional-analysis', 'probability-theory', 'operator-theory', 'normal-distribution', 'estimation']"
21,Is Riesz measure an extension of product measure?,Is Riesz measure an extension of product measure?,,"Suppose $X$ and $Y$ are compact Hausdorff spaces and $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ are finite regular Borel measure spaces. (By regular I mean that every measurable set can be approximated from above by open measurable sets and from below by compact measurable sets.) Let $\mu \times \nu$ be product measure on $(X \times Y, \mathcal A \times \mathcal B)$. For $f \in C(X \times Y)$ define $\psi(f) = \int f \; d(\mu \times \nu)$. By the Riesz–Markov–Kakutani representation theorem there is a unique regular Borel measure $\lambda$ on $X \times Y$ such that  $\psi(f) = \int f \; d\lambda$. I believe I can show that $\lambda$ must be an extension of $\mu \times \nu$, but I am looking for a simpler or more elegant proof.","Suppose $X$ and $Y$ are compact Hausdorff spaces and $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ are finite regular Borel measure spaces. (By regular I mean that every measurable set can be approximated from above by open measurable sets and from below by compact measurable sets.) Let $\mu \times \nu$ be product measure on $(X \times Y, \mathcal A \times \mathcal B)$. For $f \in C(X \times Y)$ define $\psi(f) = \int f \; d(\mu \times \nu)$. By the Riesz–Markov–Kakutani representation theorem there is a unique regular Borel measure $\lambda$ on $X \times Y$ such that  $\psi(f) = \int f \; d\lambda$. I believe I can show that $\lambda$ must be an extension of $\mu \times \nu$, but I am looking for a simpler or more elegant proof.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
22,How to compare the Hardy-Littlewood maximal function for balls and cubes?,How to compare the Hardy-Littlewood maximal function for balls and cubes?,,"I am currently working through a set of notes I found on the internet at: http://math.msu.edu/~charlesb/Notes/DuoChapter2.pdf I am up to page 8, and the Hardy-Littlewood maximal function for balls has just been introduced. Then it says that we can also define maximal functions over cubes centred at $x$. Then there is the phrase: ""Furthermore, since the $n$-dimensional volumes of the unit cube and unit ball are equal up to a multiplicative constant depending only on $n$, it is immediate that $Mf$ and $M'f$ are comparable in the sense that $c_nM'f(x)\leq Mf(x)\leq C_nM'f(x)$ for constants $c_n$ and $C_N$ only depending on $n$."" It may be immediate to the author but it is not at all to me! I cannot understand why this is true. Does anyone have a proof? And is there a formula for $c_n$ and $C_n$? All I can think of is maybe it's possible to come up with some sort of comparison between the size of a ball and the size of a cube both using the same $r$, but then the integrals may not be equal in order to compare the entire maximal funnction...","I am currently working through a set of notes I found on the internet at: http://math.msu.edu/~charlesb/Notes/DuoChapter2.pdf I am up to page 8, and the Hardy-Littlewood maximal function for balls has just been introduced. Then it says that we can also define maximal functions over cubes centred at $x$. Then there is the phrase: ""Furthermore, since the $n$-dimensional volumes of the unit cube and unit ball are equal up to a multiplicative constant depending only on $n$, it is immediate that $Mf$ and $M'f$ are comparable in the sense that $c_nM'f(x)\leq Mf(x)\leq C_nM'f(x)$ for constants $c_n$ and $C_N$ only depending on $n$."" It may be immediate to the author but it is not at all to me! I cannot understand why this is true. Does anyone have a proof? And is there a formula for $c_n$ and $C_n$? All I can think of is maybe it's possible to come up with some sort of comparison between the size of a ball and the size of a cube both using the same $r$, but then the integrals may not be equal in order to compare the entire maximal funnction...",,"['functional-analysis', 'measure-theory', 'lebesgue-measure', 'harmonic-analysis']"
23,$\Gamma$-convergence (Gamma-convergence) and PDEs?,-convergence (Gamma-convergence) and PDEs?,\Gamma,"My question is about the applying calculus of variations to solving Partial Differential Equations. In particular, what is the idea behind using $\Gamma$-convergence to find weak solutions of PDEs? Honestly, my understanding of $\Gamma$-convergence is that relatively vague: given a sequence of functionals we expect the limit to have an optimal (in some sense) lower bound, which is common for every element in the sequence. Now, writing a PDE in the weak formulation apparently gives us ability to apply functional-related tools to it. I heard people saying that for elliptic PDEs $\Gamma$-convergence basically establishes convergence of minimizers of corresponding functionals,  whereas for hyperbolic PDEs it corresponds to minimizing gradient flow. Do the statements I provided in above paragraph have any reasonable and intuitive justifications, or is it all just nonsense? If they are true, what would be the similar statement for  parabolic equations?","My question is about the applying calculus of variations to solving Partial Differential Equations. In particular, what is the idea behind using $\Gamma$-convergence to find weak solutions of PDEs? Honestly, my understanding of $\Gamma$-convergence is that relatively vague: given a sequence of functionals we expect the limit to have an optimal (in some sense) lower bound, which is common for every element in the sequence. Now, writing a PDE in the weak formulation apparently gives us ability to apply functional-related tools to it. I heard people saying that for elliptic PDEs $\Gamma$-convergence basically establishes convergence of minimizers of corresponding functionals,  whereas for hyperbolic PDEs it corresponds to minimizing gradient flow. Do the statements I provided in above paragraph have any reasonable and intuitive justifications, or is it all just nonsense? If they are true, what would be the similar statement for  parabolic equations?",,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'convergence-divergence', 'calculus-of-variations']"
24,Intuition behind the Riesz-Thorin interpolation theorem,Intuition behind the Riesz-Thorin interpolation theorem,,"Quoting the definition on Wikipedia , Let $(\Omega_1, \Sigma_1, \mu_1)$ and $(\Omega_2, \Sigma_2, \mu_2)$ be $\sigma$-finite measure spaces. Suppose $1 \leq p_0 \leq p_1 \leq \infty$, $1 \leq q_0 \leq q_1 \leq \infty$, and let $T : L^{p_0}(\mu_1) + L^{p_1}(\mu_2) \to L^{q_0}(\mu_2) + L^{q_1}(\mu_2)$ be a linear operator that maps $L^{p_0}(\mu_1)$ (resp. $L^{p_1}(\mu_1)$) boundedly into $L^{q_0}(\mu_2)$ (resp. $L^{q_1}(\mu_2)$). For $0 < \theta < 1$, let $p_\theta$, $q_\theta$ be defined as above. Then $T$ maps $L^{p_\theta}(\mu_1)$ boundedly into $L^{q_\theta}(\mu_2)$ and satisifies the operator norm estimate $$ \|T\|_{L^{p_\theta} \to L^{q_\theta}} \leq \|T\|^{1-\theta}_{L^{p_0} \to L^{q_0}} \|T\|^\theta_{L^{p_1} \to L^{q_1}}.$$ I am having some difficulty visualizing the statement of this theorem, and therefore do not understand why this is an ""interpolation"" theorem. What is the intuition behind this theorem? What is being interpolated here?","Quoting the definition on Wikipedia , Let $(\Omega_1, \Sigma_1, \mu_1)$ and $(\Omega_2, \Sigma_2, \mu_2)$ be $\sigma$-finite measure spaces. Suppose $1 \leq p_0 \leq p_1 \leq \infty$, $1 \leq q_0 \leq q_1 \leq \infty$, and let $T : L^{p_0}(\mu_1) + L^{p_1}(\mu_2) \to L^{q_0}(\mu_2) + L^{q_1}(\mu_2)$ be a linear operator that maps $L^{p_0}(\mu_1)$ (resp. $L^{p_1}(\mu_1)$) boundedly into $L^{q_0}(\mu_2)$ (resp. $L^{q_1}(\mu_2)$). For $0 < \theta < 1$, let $p_\theta$, $q_\theta$ be defined as above. Then $T$ maps $L^{p_\theta}(\mu_1)$ boundedly into $L^{q_\theta}(\mu_2)$ and satisifies the operator norm estimate $$ \|T\|_{L^{p_\theta} \to L^{q_\theta}} \leq \|T\|^{1-\theta}_{L^{p_0} \to L^{q_0}} \|T\|^\theta_{L^{p_1} \to L^{q_1}}.$$ I am having some difficulty visualizing the statement of this theorem, and therefore do not understand why this is an ""interpolation"" theorem. What is the intuition behind this theorem? What is being interpolated here?",,"['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lp-spaces']"
25,How can I prove that $f$ is inner product function,How can I prove that  is inner product function,f,"We know the polarization identity in inner product space : $$\langle x,y\rangle= \frac{1}{4} (\|x+y\|^2-\|x-y\|^2) + \frac{i}{4} (\|x+iy\|^2-\|x-iy\|^2) $$ But the question is if we have $(X,\|\cdot\|)$ is normed space and the function $f$ defined by :  $$f(x,y)= \frac{1}{4} (\|x+y\|^2-\|x-y\|^2) + \frac{i}{4} (\|x+iy\|^2-\|x-iy\|^2)$$ how can I prove that $f$ is inner product function without use the polarization identity ( I mean I can only use the properties of norm to prove $f$ is inner product function )","We know the polarization identity in inner product space : $$\langle x,y\rangle= \frac{1}{4} (\|x+y\|^2-\|x-y\|^2) + \frac{i}{4} (\|x+iy\|^2-\|x-iy\|^2) $$ But the question is if we have $(X,\|\cdot\|)$ is normed space and the function $f$ defined by :  $$f(x,y)= \frac{1}{4} (\|x+y\|^2-\|x-y\|^2) + \frac{i}{4} (\|x+iy\|^2-\|x-iy\|^2)$$ how can I prove that $f$ is inner product function without use the polarization identity ( I mean I can only use the properties of norm to prove $f$ is inner product function )",,"['functional-analysis', 'normed-spaces', 'inner-products']"
26,Morita equivalence and KK-theory,Morita equivalence and KK-theory,,"Let $A,B,C$ be $C^\ast$-algebras. Suppose $B$ and $C$ to be strongly morita equivalent. Then $KK(A,B)\cong KK(A,C)$. Could someone provide a reference or proof of this fact? I guess the imprimitivity bimodule defines an element in $KK(B,C)$ and Kasparov product with this element is the desired isomorphism, but I don't know how to fill the details. Also I am unsure whether I should add $\sigma$-unitality/separability as hypothesis.","Let $A,B,C$ be $C^\ast$-algebras. Suppose $B$ and $C$ to be strongly morita equivalent. Then $KK(A,B)\cong KK(A,C)$. Could someone provide a reference or proof of this fact? I guess the imprimitivity bimodule defines an element in $KK(B,C)$ and Kasparov product with this element is the desired isomorphism, but I don't know how to fill the details. Also I am unsure whether I should add $\sigma$-unitality/separability as hypothesis.",,"['functional-analysis', 'operator-algebras', 'k-theory']"
27,Banach space of p-Lipschitz functions,Banach space of p-Lipschitz functions,,"Given $p\in\mathbb{R}$, consider the space: $$ Lip(p) = \left\{f:[0,1] \longrightarrow \mathbb{R} : \mbox{ $f$ is $p$-Lipschitz} \right\}$$ i.e.: there is $M>0$ such that $|f(s)-f(t)|<M|s-t|^p \quad\forall s,t\in 0,1]$ We can define a norm on $Lip(p)$ by  $$\Vert f\Vert = |f(0)| + \sup \left\{ \frac{|f(t)-f(s)|}{|t-s|^p}: t\neq s, \quad t,s\in [0,1]  \right\}$$ It's easy to show that $\Vert\cdot\Vert$ is a norm in $Lip(p)$, but I   was not able to proove that $(Lip(p),\Vert\cdot\Vert)$ is a Banach   space. Given a Cauchy sequence $(f_n) \subseteq Lip(p)$, I couldn't find a candidate to conclude the convergence proof. Any hint? (I DO NOT want an entire proof)","Given $p\in\mathbb{R}$, consider the space: $$ Lip(p) = \left\{f:[0,1] \longrightarrow \mathbb{R} : \mbox{ $f$ is $p$-Lipschitz} \right\}$$ i.e.: there is $M>0$ such that $|f(s)-f(t)|<M|s-t|^p \quad\forall s,t\in 0,1]$ We can define a norm on $Lip(p)$ by  $$\Vert f\Vert = |f(0)| + \sup \left\{ \frac{|f(t)-f(s)|}{|t-s|^p}: t\neq s, \quad t,s\in [0,1]  \right\}$$ It's easy to show that $\Vert\cdot\Vert$ is a norm in $Lip(p)$, but I   was not able to proove that $(Lip(p),\Vert\cdot\Vert)$ is a Banach   space. Given a Cauchy sequence $(f_n) \subseteq Lip(p)$, I couldn't find a candidate to conclude the convergence proof. Any hint? (I DO NOT want an entire proof)",,"['functional-analysis', 'banach-spaces', 'normed-spaces']"
28,Proof involving strongly continuous semigroups.,Proof involving strongly continuous semigroups.,,"Let $ (T(t))_{t \geq 0} $ be a $ C_{0} $-semigroup on a Hilbert space $ X $ with an infinitesimal generator $ A $, and let $ \rho \in (0,1) $. I want to prove that $ \displaystyle \sup_{t \geq 0} \| T(t) - I \| \leq \rho $ is equivalent to $ A = 0 $. I have already proven that when the inequality holds, then the infinitesimal generator $ A $ satisfies $$ \forall \lambda > 0: \quad \left\| \lambda (\lambda I - A)^{-1} - I \right\| \leq \rho, $$ and I want to use this new inequality to show that $ A = 0 $ and thus $ T(t) = I $. Define $ T_{\lambda} \stackrel{\text{df}}{=} \lambda (\lambda I - A)^{-1} - I $. Then it is clear that $ T \in \mathcal{L}(X) $ and $ \| T_{\lambda} \| \leq \rho $. Hence, $$      \| T_{\lambda} \| \leq \rho \iff 1 - \| T_{\lambda} \| \leq 1 - \rho \iff (1 - \| T_{\lambda} \|)^{-1} \leq (1 - \rho)^{-1}. $$ We also have that $ (1 - \| T_{\lambda} \|)^{-1} = \dfrac{1}{1 - \| \lambda (\lambda I - A)^{-1} - I \|} $. How do I proceed from here?","Let $ (T(t))_{t \geq 0} $ be a $ C_{0} $-semigroup on a Hilbert space $ X $ with an infinitesimal generator $ A $, and let $ \rho \in (0,1) $. I want to prove that $ \displaystyle \sup_{t \geq 0} \| T(t) - I \| \leq \rho $ is equivalent to $ A = 0 $. I have already proven that when the inequality holds, then the infinitesimal generator $ A $ satisfies $$ \forall \lambda > 0: \quad \left\| \lambda (\lambda I - A)^{-1} - I \right\| \leq \rho, $$ and I want to use this new inequality to show that $ A = 0 $ and thus $ T(t) = I $. Define $ T_{\lambda} \stackrel{\text{df}}{=} \lambda (\lambda I - A)^{-1} - I $. Then it is clear that $ T \in \mathcal{L}(X) $ and $ \| T_{\lambda} \| \leq \rho $. Hence, $$      \| T_{\lambda} \| \leq \rho \iff 1 - \| T_{\lambda} \| \leq 1 - \rho \iff (1 - \| T_{\lambda} \|)^{-1} \leq (1 - \rho)^{-1}. $$ We also have that $ (1 - \| T_{\lambda} \|)^{-1} = \dfrac{1}{1 - \| \lambda (\lambda I - A)^{-1} - I \|} $. How do I proceed from here?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'semigroup-of-operators']"
29,Norm of adjoint,Norm of adjoint,,"Assume we have 2 injective continuous operators with dense images $A$ and $B$ on a Hilbert space $\mathbb H$ and $B$ is self adjoint. Further let there be constants $a_1$ and $a_2$ such that- $a_1\|Bu\| \leq \|Au\| \leq a_2\|Bu\|$ for all $u \in \mathbb H$. Can we show that for $A^{*}$, the adjoint of $A$, we have - $a_1\|Bu\| \leq \|A^{*}u\| \leq a_2\|Bu\|$ for all $u \in \mathbb H$. This is obviously true when $A$ is normal. Is it true for non normal operators?","Assume we have 2 injective continuous operators with dense images $A$ and $B$ on a Hilbert space $\mathbb H$ and $B$ is self adjoint. Further let there be constants $a_1$ and $a_2$ such that- $a_1\|Bu\| \leq \|Au\| \leq a_2\|Bu\|$ for all $u \in \mathbb H$. Can we show that for $A^{*}$, the adjoint of $A$, we have - $a_1\|Bu\| \leq \|A^{*}u\| \leq a_2\|Bu\|$ for all $u \in \mathbb H$. This is obviously true when $A$ is normal. Is it true for non normal operators?",,['functional-analysis']
30,Is this linear map bounded?,Is this linear map bounded?,,"This question asks to prove the following: Let $X$ and $Y$ be Banach spaces.  If $T: X \to Y$ is a linear map such that $f \circ T \in X^*$ for every $f \in Y^*$, then $T$ is bounded. The assumption that $Y$ is complete seems redundant, and the assumption that $X$ is complete is invoked only when applying the uniform boundedness principle (if $X$ is complete, it must be nonmeager by the Baire category theorem). So I'm trying to either prove that $T$ is still bounded if $X$ and $Y$ are arbitrary normed vector spaces (over $\mathbb{R}$ or $\mathbb{C}$), or else to find a counterexample that illustrates that $X$ must be complete. Any suggestions would be greatly appreciated!","This question asks to prove the following: Let $X$ and $Y$ be Banach spaces.  If $T: X \to Y$ is a linear map such that $f \circ T \in X^*$ for every $f \in Y^*$, then $T$ is bounded. The assumption that $Y$ is complete seems redundant, and the assumption that $X$ is complete is invoked only when applying the uniform boundedness principle (if $X$ is complete, it must be nonmeager by the Baire category theorem). So I'm trying to either prove that $T$ is still bounded if $X$ and $Y$ are arbitrary normed vector spaces (over $\mathbb{R}$ or $\mathbb{C}$), or else to find a counterexample that illustrates that $X$ must be complete. Any suggestions would be greatly appreciated!",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
31,Second derivative of a convex function in the Itō–Tanaka formula,Second derivative of a convex function in the Itō–Tanaka formula,,"This is the form of the Itō–Tanaka formula I have (Revuz and Yor): For $f$ a convex function and $X$ a continuous semimartingale, $$f(X_t)=f(X_0) +\int_0^tf_{-}'(X_s)dX_s+\frac{1}{2}\int_{\mathbb{R}}L_t^a f''(da).$$ What confuses me is how to make sense of $f''$ or rather how to make sense of the second integral. I don't know much functional analysis, but I believe that this 'second derivative' is to be interpreted as a distribution. In which case $\int_{\mathbb{R}}L_t^a f''(da)$ really means $f''(L_t^a)$, where $f''$ is a distribution. But, what's to say $L_t^a$ is a suitable test function?","This is the form of the Itō–Tanaka formula I have (Revuz and Yor): For $f$ a convex function and $X$ a continuous semimartingale, $$f(X_t)=f(X_0) +\int_0^tf_{-}'(X_s)dX_s+\frac{1}{2}\int_{\mathbb{R}}L_t^a f''(da).$$ What confuses me is how to make sense of $f''$ or rather how to make sense of the second integral. I don't know much functional analysis, but I believe that this 'second derivative' is to be interpreted as a distribution. In which case $\int_{\mathbb{R}}L_t^a f''(da)$ really means $f''(L_t^a)$, where $f''$ is a distribution. But, what's to say $L_t^a$ is a suitable test function?",,"['functional-analysis', 'stochastic-calculus']"
32,Different norm on $\ell_p$-space and Hilbert space,Different norm on -space and Hilbert space,\ell_p,"We define $\ell_p=\{(x_n)_{n\in{\mathbb{N}}}\in\mathbb{C}^\infty:\sum_n{|x_n|^p}<\infty\}$. With the usual usual norm $||.||_p$ this becomes a Bancach space. Also we have the usual inner product : $\langle x,y\rangle=\sum_i x_i \bar{y_i}$. Now I have two questions. Can we can define other norms and inner products on the set $\ell_p$? Obviously any constant multiple of the usual norm is also a norm. But can we have a norm that is not equivalent to the usual $p$-norm (i.e induces a different topology) on the set $\ell_p$? I also know that $\ell_p$ space is a Hilbert space if and only if $p=2$. But this is true with the usual norm and inner product on the set $\ell_2$. What happens if I change them (assuming that answer to my first question is affirmative)? Any help is appreciated. Thanks!","We define $\ell_p=\{(x_n)_{n\in{\mathbb{N}}}\in\mathbb{C}^\infty:\sum_n{|x_n|^p}<\infty\}$. With the usual usual norm $||.||_p$ this becomes a Bancach space. Also we have the usual inner product : $\langle x,y\rangle=\sum_i x_i \bar{y_i}$. Now I have two questions. Can we can define other norms and inner products on the set $\ell_p$? Obviously any constant multiple of the usual norm is also a norm. But can we have a norm that is not equivalent to the usual $p$-norm (i.e induces a different topology) on the set $\ell_p$? I also know that $\ell_p$ space is a Hilbert space if and only if $p=2$. But this is true with the usual norm and inner product on the set $\ell_2$. What happens if I change them (assuming that answer to my first question is affirmative)? Any help is appreciated. Thanks!",,"['functional-analysis', 'hilbert-spaces', 'lp-spaces']"
33,"If $X$ is a normed space and $Y$ a finite dimensional subspace, then $Y$ is closed.","If  is a normed space and  a finite dimensional subspace, then  is closed.",X Y Y,"I am trying to produce a direct proof on the statement mentioned above. The field I am working in is $\mathbb{R}$. My proof outline goes as following: If $Y$ is finite-dimensional, there exists a basis $(e_1,\dots,e_n)$ with $n<\infty$ such that every $y\in Y$ can be written as $y = \sum_{i=1}^n a_i e_i$ with $(a_1,\dots,a_n)$ constants in $\mathbb{R}$. So I need to take a sequence $(y_n)_{n\in\mathbb{N}}$ in Y that converges to some element in $X$. Here is where my argument is fuzzy. As $X$ is not necessarily finite dimensional, what I understand is that if we can think of the sequence as converging to some $(x_1,x_2,\dots, x_{n-1},x_n,0,0,0,0,\dots)$, is this correct? I am stuck at this and I do not know how to proceed. Any ideas would be appreciated.","I am trying to produce a direct proof on the statement mentioned above. The field I am working in is $\mathbb{R}$. My proof outline goes as following: If $Y$ is finite-dimensional, there exists a basis $(e_1,\dots,e_n)$ with $n<\infty$ such that every $y\in Y$ can be written as $y = \sum_{i=1}^n a_i e_i$ with $(a_1,\dots,a_n)$ constants in $\mathbb{R}$. So I need to take a sequence $(y_n)_{n\in\mathbb{N}}$ in Y that converges to some element in $X$. Here is where my argument is fuzzy. As $X$ is not necessarily finite dimensional, what I understand is that if we can think of the sequence as converging to some $(x_1,x_2,\dots, x_{n-1},x_n,0,0,0,0,\dots)$, is this correct? I am stuck at this and I do not know how to proceed. Any ideas would be appreciated.",,"['real-analysis', 'functional-analysis', 'convergence-divergence', 'vector-spaces', 'normed-spaces']"
34,Equicontinuity if the sequence of derivatives is uniformly bounded.,Equicontinuity if the sequence of derivatives is uniformly bounded.,,"I would really appreciate if someone could look over this proof for me. Let $ \left\{ g_m \right\} $ be a sequence of functions defined on an interval $ [a,b] \subset \mathbb{R}^n$.  Let $ \left\{ g'_m \right\} $ be uniformly bounded on $[a,b]$.  Show that $ \left\{ g_m \right\} $ is equicontinuous on $[a,b]$. My proof goes like this: If $g'_m$ is uniformly bounded, then that means that $ \lvert g'_m \rvert \leq M$ for some non-negative $M$.  That means that for all $m$, $$\left\lvert \frac{g_m(x)-g_m(y)}{x-y} \right\rvert\leq M $$ as $x\rightarrow y$. This means that the sequence of functions $ \left\{ g_m \right\} $ all have the same Lipshitz constant, and that means $ \left\{ g_m \right\} $ is equicontinuous.","I would really appreciate if someone could look over this proof for me. Let $ \left\{ g_m \right\} $ be a sequence of functions defined on an interval $ [a,b] \subset \mathbb{R}^n$.  Let $ \left\{ g'_m \right\} $ be uniformly bounded on $[a,b]$.  Show that $ \left\{ g_m \right\} $ is equicontinuous on $[a,b]$. My proof goes like this: If $g'_m$ is uniformly bounded, then that means that $ \lvert g'_m \rvert \leq M$ for some non-negative $M$.  That means that for all $m$, $$\left\lvert \frac{g_m(x)-g_m(y)}{x-y} \right\rvert\leq M $$ as $x\rightarrow y$. This means that the sequence of functions $ \left\{ g_m \right\} $ all have the same Lipshitz constant, and that means $ \left\{ g_m \right\} $ is equicontinuous.",,"['real-analysis', 'functional-analysis', 'proof-verification']"
35,Relative countable weak$^{\ast}$ compactness and sequences,Relative countable weak compactness and sequences,^{\ast},"I am finding serious difficulties in understanding some things about relative countable compactness and the use of sequences in proving it by my functional analysis text, Kolmogorov-Fomin's. For example, here in corollary 2 , it says that a subset of the space $E^{\ast}$ conjugate to a separable Banach space $E$ is bounded if it is relatively countably compact (as in definition 5 here ) in the weak$^{\ast}$ topology as a consequence of theorem 2' here . I would like to understand why, if $M$ is not bounded, it cannot be relatively countably compact, but from theorem 2' I only get that, if $\{f_n\}_n$ is not bounded, then it is not weakly$^{\ast}$ convergent: how can that prove that any infinite subset of $M$ has an accumulation point? My book, without giving a proof and treating it as trivial, might appear to treat countable compactness in the weak$^{\ast}$ topology as equivalent to the fact that any sequence has a a weak$^{\ast}$-convergent subsequence, but, if it is true, I am too stupid to see it as trivial, though I am not sure that it is true... If $M$ were a subset of a metric space I would know that $x$ would be an accumulation point of $M$ if and only if it were the limit of an eventually non-constant sequence of points belonging to $M$, but $E^{\ast}$ with the weak$^{\ast}$ topology is not a metric space (though a sphere centred in 0 is metrisable, but, if we do not a priori know that a subset $M$ is bounded...). I also think, thanks to what, and to who has written what, I read here , that if $E$ is a separable Banach space then every relatively weak$^{\ast}$-compact subset of $E^{\ast}$ is relatively sequentially weak$^{\ast}$-compact ( definition as here ), but here we only have countable weak$^{\ast}$-compactness... Does relative countable weak$^{\ast}$-compactness implies relative sequential weak$^{\ast}$-compactness? If it does, I see that if $M$ is not bounded, and infinite, we can chose from it a sequence $\{f_n\}$ (even such that $\forall i\ne j\quad f_i\ne f_j$, if we desire so) such that $\|f_n\|\to+\infty$, which, by theorem 2' , will not have any convergent subsequence. Then, if relative countable weak$^{\ast}$-compactness implied relative sequential weak$^{\ast}$-compactness, $M$ would not be countabye weak$^{\ast}$-compact, so that the ""only if part"" of corollary 2 , which is what causes some problems to me, would be proven. But I am far from being conviced that such implication is true... I uncountably thank you for any help! ;-)","I am finding serious difficulties in understanding some things about relative countable compactness and the use of sequences in proving it by my functional analysis text, Kolmogorov-Fomin's. For example, here in corollary 2 , it says that a subset of the space $E^{\ast}$ conjugate to a separable Banach space $E$ is bounded if it is relatively countably compact (as in definition 5 here ) in the weak$^{\ast}$ topology as a consequence of theorem 2' here . I would like to understand why, if $M$ is not bounded, it cannot be relatively countably compact, but from theorem 2' I only get that, if $\{f_n\}_n$ is not bounded, then it is not weakly$^{\ast}$ convergent: how can that prove that any infinite subset of $M$ has an accumulation point? My book, without giving a proof and treating it as trivial, might appear to treat countable compactness in the weak$^{\ast}$ topology as equivalent to the fact that any sequence has a a weak$^{\ast}$-convergent subsequence, but, if it is true, I am too stupid to see it as trivial, though I am not sure that it is true... If $M$ were a subset of a metric space I would know that $x$ would be an accumulation point of $M$ if and only if it were the limit of an eventually non-constant sequence of points belonging to $M$, but $E^{\ast}$ with the weak$^{\ast}$ topology is not a metric space (though a sphere centred in 0 is metrisable, but, if we do not a priori know that a subset $M$ is bounded...). I also think, thanks to what, and to who has written what, I read here , that if $E$ is a separable Banach space then every relatively weak$^{\ast}$-compact subset of $E^{\ast}$ is relatively sequentially weak$^{\ast}$-compact ( definition as here ), but here we only have countable weak$^{\ast}$-compactness... Does relative countable weak$^{\ast}$-compactness implies relative sequential weak$^{\ast}$-compactness? If it does, I see that if $M$ is not bounded, and infinite, we can chose from it a sequence $\{f_n\}$ (even such that $\forall i\ne j\quad f_i\ne f_j$, if we desire so) such that $\|f_n\|\to+\infty$, which, by theorem 2' , will not have any convergent subsequence. Then, if relative countable weak$^{\ast}$-compactness implied relative sequential weak$^{\ast}$-compactness, $M$ would not be countabye weak$^{\ast}$-compact, so that the ""only if part"" of corollary 2 , which is what causes some problems to me, would be proven. But I am far from being conviced that such implication is true... I uncountably thank you for any help! ;-)",,"['functional-analysis', 'banach-spaces', 'topological-vector-spaces']"
36,Prove a condition for a Banach algebra to be isometrically isomorphic to $\mathbb C$,Prove a condition for a Banach algebra to be isometrically isomorphic to,\mathbb C,"Can anyone help me by providing a detailed verification of the following theorem? Let $\mathcal{A}$ be a Banach algebra. If there exists $M<+\infty$ so that  $$\Vert a \Vert\Vert b \Vert\leq M \Vert ab \Vert,$$ $$(a,b\in\mathcal{A})$$ then $\mathcal{A}$ is isometrically isomorphic with $\mathbb{C}.$","Can anyone help me by providing a detailed verification of the following theorem? Let $\mathcal{A}$ be a Banach algebra. If there exists $M<+\infty$ so that  $$\Vert a \Vert\Vert b \Vert\leq M \Vert ab \Vert,$$ $$(a,b\in\mathcal{A})$$ then $\mathcal{A}$ is isometrically isomorphic with $\mathbb{C}.$",,"['functional-analysis', 'banach-algebras']"
37,Reproducing Kernel Hilbert Space (RKHS) constructed by the summation of positive-definite kernels.,Reproducing Kernel Hilbert Space (RKHS) constructed by the summation of positive-definite kernels.,,"Let $k_1,\ldots,k_p$ be positive definite kernels defined on $\mathcal{X}\times\mathcal{X}$, where $\mathcal{X}$ is a non-empty set. $k_i$ is the reproducing kernel of the Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}_i$, which is endowed with an inner product $\langle\cdot,\cdot\rangle_{\mathcal{H}_i}$, from which the norm $\lVert\cdot\rVert_{\mathcal{H}_i}$ is induced ($i=1,\ldots,p$). Now let $k$ be a real-valued function defined on $\mathcal{X}\times\mathcal{X}$, with $k=\sum_{i=1}^{p}k_i$. It is straightforward to prove (isn't it?) that $k$ is positive definite, since positive-definiteness is preserved under summation. Apparently, there corresponds a RKHS, $\mathcal{H}$ to $k$, with inner product $\langle\cdot,\cdot\rangle_{\mathcal{H}}$ and norm $\lVert\cdot\rVert_{\mathcal{H}}$. What is the relation between the norm of $\mathcal{H}$ and the norms of $\mathcal{H}_i$, $i=1,\ldots,p$? Do we need to require that the spaces $\mathcal{H}_i$ are pairwise-perpendicular so that it holds $$ \lVert\cdot\rVert_{\mathcal{H}}^2 =  \sum_{i=1}^{p}\lVert\cdot\rVert_{\mathcal{H}_i}^2, $$ and, if so, what that would mean for the reproducing kernels $k_i$? Moreover: Let $f\in\mathcal{H}$, how could its norm, $\lVert f \rVert_{\mathcal{H}}$, be expressed in terms of the norms of the spaces $\mathcal{H}_i$? The above questions may seem vague, or even incorrect in the way they are stated. Please feel free to suggest approaches and/or corrections (for instance, should the title be changed?), if necessary. Also, please feel free to discuss!","Let $k_1,\ldots,k_p$ be positive definite kernels defined on $\mathcal{X}\times\mathcal{X}$, where $\mathcal{X}$ is a non-empty set. $k_i$ is the reproducing kernel of the Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}_i$, which is endowed with an inner product $\langle\cdot,\cdot\rangle_{\mathcal{H}_i}$, from which the norm $\lVert\cdot\rVert_{\mathcal{H}_i}$ is induced ($i=1,\ldots,p$). Now let $k$ be a real-valued function defined on $\mathcal{X}\times\mathcal{X}$, with $k=\sum_{i=1}^{p}k_i$. It is straightforward to prove (isn't it?) that $k$ is positive definite, since positive-definiteness is preserved under summation. Apparently, there corresponds a RKHS, $\mathcal{H}$ to $k$, with inner product $\langle\cdot,\cdot\rangle_{\mathcal{H}}$ and norm $\lVert\cdot\rVert_{\mathcal{H}}$. What is the relation between the norm of $\mathcal{H}$ and the norms of $\mathcal{H}_i$, $i=1,\ldots,p$? Do we need to require that the spaces $\mathcal{H}_i$ are pairwise-perpendicular so that it holds $$ \lVert\cdot\rVert_{\mathcal{H}}^2 =  \sum_{i=1}^{p}\lVert\cdot\rVert_{\mathcal{H}_i}^2, $$ and, if so, what that would mean for the reproducing kernels $k_i$? Moreover: Let $f\in\mathcal{H}$, how could its norm, $\lVert f \rVert_{\mathcal{H}}$, be expressed in terms of the norms of the spaces $\mathcal{H}_i$? The above questions may seem vague, or even incorrect in the way they are stated. Please feel free to suggest approaches and/or corrections (for instance, should the title be changed?), if necessary. Also, please feel free to discuss!",,"['functional-analysis', 'hilbert-spaces', 'normed-spaces']"
38,"Strong convergence of an ""averaging"" operator","Strong convergence of an ""averaging"" operator",,"Let $X$ be an Hilbert space and $S:X \rightarrow X$ be a bounded linear operator with $||S||=1  $ Define $$T_n= \frac{1}{n} \sum_{r=0}^{n-1} S^r$$ I want to show it converges strongly to some bounded operator $T$ and to identify it. I have showed that $T_n x$ has a limit in $X$ for $x\in \ker(I-S) + \operatorname{ran}(I-S)$ and that $||T_n||$ is uniformly bounded. Hence I just need to show that $\ker(I-S) + \operatorname{ran}(I-S)$ is a dense subspace of $X$ If I can prove $\ker(I-S^*)=\ker(I-S)$ then I could conclude how can I prove this? I also need to identify the limit: as Joel noted in the comments, by Mean Ergodic Theorem, the limit is a projection on $X$ onto $\ker (I-S)$ there should a direct way of proving this though, without appealing to the Mean Ergodic Theorem. After help from T.A.E. we concluded that the only remaining issue is to prove that, for this $S$, $\ker(I-S^*)=\ker(I-S)$. Just for reference (and to understand how the problem develops) it is from a past exam question: https://www.maths.ox.ac.uk/system/files/attachments/b04b-12.pdf question 1.","Let $X$ be an Hilbert space and $S:X \rightarrow X$ be a bounded linear operator with $||S||=1  $ Define $$T_n= \frac{1}{n} \sum_{r=0}^{n-1} S^r$$ I want to show it converges strongly to some bounded operator $T$ and to identify it. I have showed that $T_n x$ has a limit in $X$ for $x\in \ker(I-S) + \operatorname{ran}(I-S)$ and that $||T_n||$ is uniformly bounded. Hence I just need to show that $\ker(I-S) + \operatorname{ran}(I-S)$ is a dense subspace of $X$ If I can prove $\ker(I-S^*)=\ker(I-S)$ then I could conclude how can I prove this? I also need to identify the limit: as Joel noted in the comments, by Mean Ergodic Theorem, the limit is a projection on $X$ onto $\ker (I-S)$ there should a direct way of proving this though, without appealing to the Mean Ergodic Theorem. After help from T.A.E. we concluded that the only remaining issue is to prove that, for this $S$, $\ker(I-S^*)=\ker(I-S)$. Just for reference (and to understand how the problem develops) it is from a past exam question: https://www.maths.ox.ac.uk/system/files/attachments/b04b-12.pdf question 1.",,"['functional-analysis', 'convergence-divergence', 'operator-theory', 'hilbert-spaces']"
39,"Show that $\lambda \in \sigma(A),$ $\lambda$ not an eigenvalue, implies that $\lambda \in \sigma(A + K)$ where $K$ is compact.","Show that   not an eigenvalue, implies that  where  is compact.","\lambda \in \sigma(A), \lambda \lambda \in \sigma(A + K) K","Let $A : H \rightarrow H$ be a bounded linear map where $H$ is a Hilbert space with $\dim H = \infty$. Suppose that $\lambda \in \sigma(A)$ but $\lambda$ is not an eigenvalue. Let $K : H \rightarrow H$ be compact. Show that $\lambda \in \sigma(A + K).$ ($\sigma(A)$ is the set $\{\lambda \in \mathbb{C} : (A - \lambda I)$ is not a bijection$\}$) What I have so far: Suppose $(A-\lambda I)$ is not injective. Then there exist $x_1 \neq x_2 \in H$ so that $(A-\lambda I)x_1 = (A-\lambda I)x_2$ so $(A - \lambda I)(x_1 - x_2) = 0$. But then $\lambda$ is an eigenvalue, a contradiction. Thus $(A - \lambda I)$ must be injective. Since $\lambda \in \sigma(A)$, $(A - \lambda I)$ cannot be a bijection so we know that $(A - \lambda I)$ is not surjective. Letting $H' = \mbox{im}\, (A-\lambda I)$ we have that $H'$ is a proper subset of $H$ and that $(A - \lambda I)^{-1}$ exists on $H'$. My professor suggested that we assume that $(A - \lambda I)^{-1}$ is unbounded, so there exists in particular a bounded sequence $\{\mu_k\} \subset H'$ with $\lim_{k \rightarrow \infty} (A - \lambda I)^{-1}\mu_k = \infty$. I don't yet see how to use that idea, or why it should be true. Any suggestions?","Let $A : H \rightarrow H$ be a bounded linear map where $H$ is a Hilbert space with $\dim H = \infty$. Suppose that $\lambda \in \sigma(A)$ but $\lambda$ is not an eigenvalue. Let $K : H \rightarrow H$ be compact. Show that $\lambda \in \sigma(A + K).$ ($\sigma(A)$ is the set $\{\lambda \in \mathbb{C} : (A - \lambda I)$ is not a bijection$\}$) What I have so far: Suppose $(A-\lambda I)$ is not injective. Then there exist $x_1 \neq x_2 \in H$ so that $(A-\lambda I)x_1 = (A-\lambda I)x_2$ so $(A - \lambda I)(x_1 - x_2) = 0$. But then $\lambda$ is an eigenvalue, a contradiction. Thus $(A - \lambda I)$ must be injective. Since $\lambda \in \sigma(A)$, $(A - \lambda I)$ cannot be a bijection so we know that $(A - \lambda I)$ is not surjective. Letting $H' = \mbox{im}\, (A-\lambda I)$ we have that $H'$ is a proper subset of $H$ and that $(A - \lambda I)^{-1}$ exists on $H'$. My professor suggested that we assume that $(A - \lambda I)^{-1}$ is unbounded, so there exists in particular a bounded sequence $\{\mu_k\} \subset H'$ with $\lim_{k \rightarrow \infty} (A - \lambda I)^{-1}\mu_k = \infty$. I don't yet see how to use that idea, or why it should be true. Any suggestions?",,['functional-analysis']
40,Proof of the Unsöld's Theorem (the sum of spherical harmonics),Proof of the Unsöld's Theorem (the sum of spherical harmonics),,"There is an identity concerning spherical harmonics that plays a pretty important role in atomic physics. Thanks to wikipedia ( http://en.wikipedia.org/wiki/Spherical_harmonic ) I know that its name is Unsöld's Theorem. It states that: $$ \sum_{m = -l}^{l}|Y_{l}^{m}(\phi,\theta)|^2  =  \frac{2l+1}{4 \pi} $$ However I was not able to find any proof over the Internet, nor solve it myself. I would be grateful for any tips, heuristics, cause I have no idea, where to even begin with.","There is an identity concerning spherical harmonics that plays a pretty important role in atomic physics. Thanks to wikipedia ( http://en.wikipedia.org/wiki/Spherical_harmonic ) I know that its name is Unsöld's Theorem. It states that: $$ \sum_{m = -l}^{l}|Y_{l}^{m}(\phi,\theta)|^2  =  \frac{2l+1}{4 \pi} $$ However I was not able to find any proof over the Internet, nor solve it myself. I would be grateful for any tips, heuristics, cause I have no idea, where to even begin with.",,"['functional-analysis', 'partial-differential-equations', 'special-functions']"
41,Is the space of continuous functions a Cauchy complete?,Is the space of continuous functions a Cauchy complete?,,"I am so new to functional analysis so I am looking for an answer of a confusion I am having right now in my mind because I have seen many different answers for the question I am gonna ask below. I hope you will reply as simple as possible because I am not a Mathematician. Thanks for the help in advance... Is the space of continuous functions $C^{0}$ a Cauchy complete? Therefore is it a Hilbert space or not? There's a thesis online, which says that this space is not Cauchy complete and is therefore not a Hilbert space. $L^2$ square integrable functions space is the Cauchy completion of the function space $C^0$ and in other words, contnuous functions on domain $X$ are dense in $L^{2}(X)$. However, I run into some documents which support that the space of continuous functions is a Cauchy complete.","I am so new to functional analysis so I am looking for an answer of a confusion I am having right now in my mind because I have seen many different answers for the question I am gonna ask below. I hope you will reply as simple as possible because I am not a Mathematician. Thanks for the help in advance... Is the space of continuous functions $C^{0}$ a Cauchy complete? Therefore is it a Hilbert space or not? There's a thesis online, which says that this space is not Cauchy complete and is therefore not a Hilbert space. $L^2$ square integrable functions space is the Cauchy completion of the function space $C^0$ and in other words, contnuous functions on domain $X$ are dense in $L^{2}(X)$. However, I run into some documents which support that the space of continuous functions is a Cauchy complete.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'cauchy-sequences']"
42,What about $\ell^1$ with pointwise multiplication,What about  with pointwise multiplication,\ell^1,"This question occurred to me after reading this thread . I was working on finding an example of a Banach algebra. The example I came up with was $\ell^1 (\mathbb N)$ with pointwise multiplication. I believe I even proved that $\ell^1 (\mathbb N)$ is closed with respect to pointwise multiplication and that the norm is submultiplicative. Is it really possible that $\ell^1 (\mathbb N)$ can be turned into a Banach algebra in two ways, by convolution and by pointwise multiplication, or is there necessarily a mistake in my proof? (I'm happy to post the proof if the answer is yes -- for now I'm just trying to keep this question short)","This question occurred to me after reading this thread . I was working on finding an example of a Banach algebra. The example I came up with was $\ell^1 (\mathbb N)$ with pointwise multiplication. I believe I even proved that $\ell^1 (\mathbb N)$ is closed with respect to pointwise multiplication and that the norm is submultiplicative. Is it really possible that $\ell^1 (\mathbb N)$ can be turned into a Banach algebra in two ways, by convolution and by pointwise multiplication, or is there necessarily a mistake in my proof? (I'm happy to post the proof if the answer is yes -- for now I'm just trying to keep this question short)",,"['functional-analysis', 'banach-algebras']"
43,Duality mappings on finite-dimensional spaces,Duality mappings on finite-dimensional spaces,,"I have a few questions regarding some concepts in a book ""nonlinear partial differential equations by Roubicek"" I am studying. The following is from the text. ""Let $V$ be a separable, reflexive Banach space. Let $V_{k}$ be a finite-dimensional subspace. Since $V$ is finite dimensional we can identify $V_{k} \cong V_{k}^{*}$. If necessary, we can re-norm the finite dimensional $V_{k}$ to impose a Hilbert structure($V_{k}$ is then homeomorphic with a Euclidean space). We consider the duality mapping $J$ which is taken as $J(u):= \{f \in V^{*}; \langle f,u \rangle = \Vert u|\Vert^{2} = \Vert f \Vert_{*}^{2}\}$. If $V$ is a separable Banach space and $V^{*}$ is strictly convex then $J$ is single valued. We then have that $J_{k}: V_{k} \rightarrow V_{k}^{*}$ is single valued and a linear homeomorphism such that $\langle J_{k}u, u \rangle = \Vert u \Vert_{V_{k}}^{2}$, $\Vert J_{k}u \Vert_{V_{k}^{*}} = \Vert u \Vert_{V_{k}}$  "" Questions: What exactly is meant by ""re-norm $V_{k}$ to impose a Hilbert structure"", how would you do that? How does it follow that the duality mapping $J_{k}$ is a linear homeomorphism? Is the duality mapping $J_{k}$ a surjective isometry or just an isometry? How exactly is $V_{k}$ homeomorphic to a Euclidean space? Thanks for any assistance.","I have a few questions regarding some concepts in a book ""nonlinear partial differential equations by Roubicek"" I am studying. The following is from the text. ""Let $V$ be a separable, reflexive Banach space. Let $V_{k}$ be a finite-dimensional subspace. Since $V$ is finite dimensional we can identify $V_{k} \cong V_{k}^{*}$. If necessary, we can re-norm the finite dimensional $V_{k}$ to impose a Hilbert structure($V_{k}$ is then homeomorphic with a Euclidean space). We consider the duality mapping $J$ which is taken as $J(u):= \{f \in V^{*}; \langle f,u \rangle = \Vert u|\Vert^{2} = \Vert f \Vert_{*}^{2}\}$. If $V$ is a separable Banach space and $V^{*}$ is strictly convex then $J$ is single valued. We then have that $J_{k}: V_{k} \rightarrow V_{k}^{*}$ is single valued and a linear homeomorphism such that $\langle J_{k}u, u \rangle = \Vert u \Vert_{V_{k}}^{2}$, $\Vert J_{k}u \Vert_{V_{k}^{*}} = \Vert u \Vert_{V_{k}}$  "" Questions: What exactly is meant by ""re-norm $V_{k}$ to impose a Hilbert structure"", how would you do that? How does it follow that the duality mapping $J_{k}$ is a linear homeomorphism? Is the duality mapping $J_{k}$ a surjective isometry or just an isometry? How exactly is $V_{k}$ homeomorphic to a Euclidean space? Thanks for any assistance.",,"['functional-analysis', 'vector-spaces']"
44,clarification about theorem $3.10 $ of Brezis functional analysis book.,clarification about theorem  of Brezis functional analysis book.,3.10 ,"I'm referring to the theorem at page $61$. It shows that for a linear operator $T $ between $E $ and $ F$ Banach space are equivalent (notation: $S$ means strong topology, the norm ome, $W $ weak topology) $T$ is continuous from $ E, S $ to $F, S$ $T$ is continuous from $E, W$ to $F, W$ $T$ is continuous from $E, S$ to $F, W$ Where by $E, S$ i mean $E$ equipped with the strong topology. My question is why the same argument is not valid for $T$ between $E, W $ to $F, S$? I don't know what doesn't work.","I'm referring to the theorem at page $61$. It shows that for a linear operator $T $ between $E $ and $ F$ Banach space are equivalent (notation: $S$ means strong topology, the norm ome, $W $ weak topology) $T$ is continuous from $ E, S $ to $F, S$ $T$ is continuous from $E, W$ to $F, W$ $T$ is continuous from $E, S$ to $F, W$ Where by $E, S$ i mean $E$ equipped with the strong topology. My question is why the same argument is not valid for $T$ between $E, W $ to $F, S$? I don't know what doesn't work.",,"['functional-analysis', 'banach-spaces']"
45,"An integro differential equation involving $f$,$f_h$ and second derivative of $f$.","An integro differential equation involving , and second derivative of .",f f_h f,Let $f_h$ be the Hilbert transform of the real function $f$. I need some help solving this integro differential equation : $$\alpha f_h(x) + \beta f''(x) = f(x)$$ A simple sinusoid doesn't seem to fit the bill. Kind of puzzzling for me to even guess anything!,Let $f_h$ be the Hilbert transform of the real function $f$. I need some help solving this integro differential equation : $$\alpha f_h(x) + \beta f''(x) = f(x)$$ A simple sinusoid doesn't seem to fit the bill. Kind of puzzzling for me to even guess anything!,,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'integral-equations', 'integro-differential-equations']"
46,Arzela-Ascoli net question,Arzela-Ascoli net question,,"Let $X$ be a compact metric space. Let $C(X)$ denote the space of real-valued continuous functions on $X$ . A commonly given corollary to the Arzela-Ascoli theorem is: Proposition: If $f_n$ is an equicontinuous sequence in $C(X)$ converging pointwise to $f \in C(X)$ , then actually $f_n \to f$ uniformly. In order to prove this, I first proved a simple lemma Lemma: Suppose $x$ is a point and $S$ is a sequence in a compact metric space $M$ .  If every convergent subsequence of $S$ converges to $x$ , then $S$ converges to $x$ . Proof: Suppose for contradiction that $S$ does not converge to $x$ . Then, there is a subsequence $S'$ of $S$ whose terms are bounded away from $x$ .  Being a sequence in a compact metric space, $S'$ has a convergent subsequence $S''$ . Since $S''$ is a convergent subsequence of $S$ , it converges to $x$ by hypothesis. But, this is impossible since the terms of $S'$ should be bounded away from $x$ . which is applied as follows. Proof of proposition: Let $M$ be the uniform closure of $\{f_n : n \in \mathbb{N} \} \cup \{f\}$ . By the Arzela-Ascoli, $M$ is compact for the uniform norm. Consider now $f_n$ as a sequence in $M$ . Any subsequence of $f_n$ which converges uniformly must converge to $f$ (since $f_n \to f$ pointwise). So, by the lemma, $f_n \to f$ uniformly. What I am slightly unsure of is whether these arguments carry over, mutatis mutandis, for nets? Question: If $X$ is a compact metric space, $C(X)$ is the space of real-valued continuous functions on $X$ , and $f_i$ is an equicontinuous net in $C(X)$ converging pointwise to $f \in C(X)$ , does $f_i \to f$ uniformly? I think the answer is yes, but I am not totally comfortable with the concept of a subnet, so it is difficult to be certain. Edit : Thinking about it more, it seems the point requiring clarification is the following one. Claim: If a net $(x_i)_{i \in I}$ in a metric space $M$ does not converge to a point $x \in M$ , then there is a subnet that is bounded away from $x$ . That is, there is an $\epsilon > 0$ and a cofinal, increasing function $\varphi : J \to I$ out of a directed set $J$ such that $d( x_{\varphi(j)}, x) \geq \epsilon$ for all $j \in J$ . Since $(x_i)$ does not converge to $x$ above, we know there exists an $\epsilon >0$ such that, for all $i_0 \in I$ , there exists $i \geq i_0$ with $d(x_i,x) \geq \epsilon$ . To find a subnet, it seems the obvious thing to do is try $J = \{ i \in I : d(x_i,x) \geq \epsilon\}$ and $\varphi$ the inclusion. Now, the above condition says exactly that $J$ is cofinal in $I$ . And, obviously the inclusion map is increasing... so I guess that settles things? Edit 2: I guess in my first edit I forgot to verify that $J$ was, itself, a directed set. But it seems a cofinal set $J$ in a directed set $I$ is automatically a directed set. Any finite subset of $J$ has an upper bound in $I$ , which has then an upper bound in $J$ .","Let be a compact metric space. Let denote the space of real-valued continuous functions on . A commonly given corollary to the Arzela-Ascoli theorem is: Proposition: If is an equicontinuous sequence in converging pointwise to , then actually uniformly. In order to prove this, I first proved a simple lemma Lemma: Suppose is a point and is a sequence in a compact metric space .  If every convergent subsequence of converges to , then converges to . Proof: Suppose for contradiction that does not converge to . Then, there is a subsequence of whose terms are bounded away from .  Being a sequence in a compact metric space, has a convergent subsequence . Since is a convergent subsequence of , it converges to by hypothesis. But, this is impossible since the terms of should be bounded away from . which is applied as follows. Proof of proposition: Let be the uniform closure of . By the Arzela-Ascoli, is compact for the uniform norm. Consider now as a sequence in . Any subsequence of which converges uniformly must converge to (since pointwise). So, by the lemma, uniformly. What I am slightly unsure of is whether these arguments carry over, mutatis mutandis, for nets? Question: If is a compact metric space, is the space of real-valued continuous functions on , and is an equicontinuous net in converging pointwise to , does uniformly? I think the answer is yes, but I am not totally comfortable with the concept of a subnet, so it is difficult to be certain. Edit : Thinking about it more, it seems the point requiring clarification is the following one. Claim: If a net in a metric space does not converge to a point , then there is a subnet that is bounded away from . That is, there is an and a cofinal, increasing function out of a directed set such that for all . Since does not converge to above, we know there exists an such that, for all , there exists with . To find a subnet, it seems the obvious thing to do is try and the inclusion. Now, the above condition says exactly that is cofinal in . And, obviously the inclusion map is increasing... so I guess that settles things? Edit 2: I guess in my first edit I forgot to verify that was, itself, a directed set. But it seems a cofinal set in a directed set is automatically a directed set. Any finite subset of has an upper bound in , which has then an upper bound in .","X C(X) X f_n C(X) f \in C(X) f_n \to f x S M S x S x S x S' S x S' S'' S'' S x S' x M \{f_n : n \in \mathbb{N} \} \cup \{f\} M f_n M f_n f f_n \to f f_n \to f X C(X) X f_i C(X) f \in C(X) f_i \to f (x_i)_{i \in I} M x \in M x \epsilon > 0 \varphi : J \to I J d( x_{\varphi(j)}, x) \geq \epsilon j \in J (x_i) x \epsilon >0 i_0 \in I i \geq i_0 d(x_i,x) \geq \epsilon J = \{ i \in I : d(x_i,x) \geq \epsilon\} \varphi J I J J I J I J","['real-analysis', 'functional-analysis', 'compactness', 'nets']"
47,A question about a Sobolev space trace inequality (don't understand why it is true),A question about a Sobolev space trace inequality (don't understand why it is true),,"Let $\Omega$ be an open set with boundary $\partial\Omega$. Let $u \in H^1(\Omega)$. There exists a $\lambda \in \mathbb{R}$ such that    $$\int_\Omega |\nabla u |^2 + \lambda\int_{\partial\Omega}u^2 \geq C\lVert u \rVert^2_{H^1(\Omega)}$$   for some constant $C$. I don't understand why this inequality is true . I thought maybe there is something to do with the right inverse of the map trace being continuous but I am not sure if this is correct. Help appreciated. Some additional info about $u$: For $v \in H^{\frac 1 2}(\partial\Omega)$, $u$ is the solution of $-\Delta u = 0$ on $\Omega$ with $u = v$ on $\partial \Omega$. (I saw this in page 135 of Lions' Quelques methodes... book).","Let $\Omega$ be an open set with boundary $\partial\Omega$. Let $u \in H^1(\Omega)$. There exists a $\lambda \in \mathbb{R}$ such that    $$\int_\Omega |\nabla u |^2 + \lambda\int_{\partial\Omega}u^2 \geq C\lVert u \rVert^2_{H^1(\Omega)}$$   for some constant $C$. I don't understand why this inequality is true . I thought maybe there is something to do with the right inverse of the map trace being continuous but I am not sure if this is correct. Help appreciated. Some additional info about $u$: For $v \in H^{\frac 1 2}(\partial\Omega)$, $u$ is the solution of $-\Delta u = 0$ on $\Omega$ with $u = v$ on $\partial \Omega$. (I saw this in page 135 of Lions' Quelques methodes... book).",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
48,Counterexample for the Open Mapping Theorem,Counterexample for the Open Mapping Theorem,,"I would like to ask a counterexample for the open mapping theorem: Find a discontinuous linear mapping $T:X \to Y$ such that $T(X)=Y$ and $X,\;Y$ are Banach but $T$ is not open. Could you help me with this problem? Thanks!","I would like to ask a counterexample for the open mapping theorem: Find a discontinuous linear mapping $T:X \to Y$ such that $T(X)=Y$ and $X,\;Y$ are Banach but $T$ is not open. Could you help me with this problem? Thanks!",,"['functional-analysis', 'examples-counterexamples']"
49,Does a symmetric operator on a Hilbert space have a symmetric adjoint?,Does a symmetric operator on a Hilbert space have a symmetric adjoint?,,"Suppose we have a linear operator $T$, densely-defined on some Hilbert space. If $T$ is symmetric (i.e., $T^*$ extends $T$: notationally, $T\subseteq T^*$) does it follow that $T^*$ is also symmetric (and therefore, in fact, self-adjoint)? If so, where could I find a proof of this? If not, what is a counter-example showing this?","Suppose we have a linear operator $T$, densely-defined on some Hilbert space. If $T$ is symmetric (i.e., $T^*$ extends $T$: notationally, $T\subseteq T^*$) does it follow that $T^*$ is also symmetric (and therefore, in fact, self-adjoint)? If so, where could I find a proof of this? If not, what is a counter-example showing this?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
50,If $\{T(t)\}_{t\geq 0}$ is an uniformly continuous semigroup of bounded linear operators then $T(s)\to T(t)$.,If  is an uniformly continuous semigroup of bounded linear operators then .,\{T(t)\}_{t\geq 0} T(s)\to T(t),"Definition: Let $X$ be a Banach space and $I$ the identity operator on $X$. A family $\{T(t)\}_{t\geq 0}$ of bounded linear operators from $X$ into $X$ is a semigroup of bounded linear operator on $X$ if (i) $T(0)=I$; (ii) $T(t + s)= T(t)T(s)$ for every $t,s\geq 0$. A semigroup $\{T(t)\}_{t\geq0}$ of bounded linear operators is uniformly continuous if $$\lim_{t\downarrow 0}\|T(t)-I\|=0.\;\;\;\;(*)$$ The Pazy's book says ""from the definition it's clear that if $\{T(t)\}_{t\geq 0}$ is an uniformly continuous semigroup of bounded linear operators then $\lim_{s\to t}\|T(s)-T(t)\|=0$"" (page 1). For me, this equality it's not so clear. Could someone help me to prove it? I'm trying to show that $$\lim_{s\to t^+}\|T(s)-T(t)\|=\lim_{s\to t^-}\|T(s)-T(t)\|=0.$$ If I'm not wrong, from the definition we can conclude that, for all $h>0$, $$0\leq \|T(t+h)-T(t)\| \overset{(ii)} =\|T(t)T(h)-T(t)\| \leq \|T(t)\|\|T(h)-I\|$$ Now, notice that $$\lim_{h\to 0^+}\|T(t)\|\|T(h)-I\|=\|T(t)\|\lim_{h\downarrow 0}\|T(h)-I\| \overset{(*)}=\|T(t)\|\;0=0$$ Thus (by Squeeze theorem), $$\lim_{s\to t^+}\|T(s)-T(t)\| =\lim_{h\to 0^+}\|T(t+h)-T(t)\| = 0$$ My question is: how to prove that $\lim_{s\to t^-}\|T(s)-T(t)\|=0$? Thanks.","Definition: Let $X$ be a Banach space and $I$ the identity operator on $X$. A family $\{T(t)\}_{t\geq 0}$ of bounded linear operators from $X$ into $X$ is a semigroup of bounded linear operator on $X$ if (i) $T(0)=I$; (ii) $T(t + s)= T(t)T(s)$ for every $t,s\geq 0$. A semigroup $\{T(t)\}_{t\geq0}$ of bounded linear operators is uniformly continuous if $$\lim_{t\downarrow 0}\|T(t)-I\|=0.\;\;\;\;(*)$$ The Pazy's book says ""from the definition it's clear that if $\{T(t)\}_{t\geq 0}$ is an uniformly continuous semigroup of bounded linear operators then $\lim_{s\to t}\|T(s)-T(t)\|=0$"" (page 1). For me, this equality it's not so clear. Could someone help me to prove it? I'm trying to show that $$\lim_{s\to t^+}\|T(s)-T(t)\|=\lim_{s\to t^-}\|T(s)-T(t)\|=0.$$ If I'm not wrong, from the definition we can conclude that, for all $h>0$, $$0\leq \|T(t+h)-T(t)\| \overset{(ii)} =\|T(t)T(h)-T(t)\| \leq \|T(t)\|\|T(h)-I\|$$ Now, notice that $$\lim_{h\to 0^+}\|T(t)\|\|T(h)-I\|=\|T(t)\|\lim_{h\downarrow 0}\|T(h)-I\| \overset{(*)}=\|T(t)\|\;0=0$$ Thus (by Squeeze theorem), $$\lim_{s\to t^+}\|T(s)-T(t)\| =\lim_{h\to 0^+}\|T(t+h)-T(t)\| = 0$$ My question is: how to prove that $\lim_{s\to t^-}\|T(s)-T(t)\|=0$? Thanks.",,"['functional-analysis', 'partial-differential-equations', 'semigroup-of-operators']"
51,Continuity+differentiability imply weak sequential continuity?,Continuity+differentiability imply weak sequential continuity?,,"Let $E$ and $F$ be two Banach spaces. We know that, if $f:E\rightarrow F$ is a nonlinear continuous operator, then $f$ may fail to send weakly convergent sequences to weakly convergent sequences, i.e., $u_n \rightharpoonup u$ weakly in $E$  does not necessarily imply $f(u_n)\rightharpoonup f(u)$ weakly in $F$. Even through $f$ has very mild non-linearity,  this weak sequential continuity may fail. For example, let $u_n(x)=\sin(nx), x\in (0,2\pi)$. Then Riemann-Lebesgue lemma shows that $u_n\rightharpoonup u= 0$ in $L^p(0,2\pi)$ for any $p\geq 1$.  Now, for $f(u)=\max(0,u)=u^+$, one has  $$f(u_n)\rightharpoonup \frac{1}{\pi}\neq f(u)=0  \text{ in } L^1(0,2\pi).  $$ Also, $$ \int_0^{2\pi}f(u_n(x))dx=\int_0^{2\pi}\sin^+(nx)dx=\frac{1}{n}\sum_{k=0}^{n-1}\int_{2k\pi}^{(2k+1)\pi}\sin (y)dy=2,  $$ which implies $f(u_n)$ can not converge weakly to  $0=f(u)$ in $L^2(0,2\pi)$. If $g(u)=|u|$, then  $$g(u_n)\rightharpoonup \frac{2}{\pi}\neq g(u)=0  \text{ in } L^1(0,2\pi).  $$ From these two examples, we see there is no general hope that a continuous map  preserves weak sequential continuity. We are wondering what kind of conditions imposed on $f$ so that it preserves weak sequential continuity.  For the two examples above, we note that both $f$ and $g$ are sub-linear ($|f(u)|\leq c|u|$ for some  constant $c>0$),  continuous but not differentiable. Now, a natural question arises: whether a sub-linear ($|f(u)|\leq a|u|+b$ for some positive constant $a$ and non-negative constant $b$), continuous and differentiable map $f:E\rightarrow F$ preserves weak sequential continuity? Here, we emphasize the non-linearity of $f$, since, for linear map $f$, it is known from functional analysis this question holds. For this question, one can choose $E$ and $F$ to be Hilbert spaces, for convenience. As concrete examples, one may test: 1. $u_n\rightharpoonup u$ with $u_n\geq 0$ in $L^2(\Omega)$ and $f(u)=\frac{u}{1+u}$, do we have $f(u_n)\rightharpoonup f(u)$ in $L^2(\Omega)$? 2. $u_n\rightharpoonup u$ in $L^2(\Omega)$ and $g(u)=\frac{u}{1+u^2}$, do we have $g(u_n)\rightharpoonup g(u)$ in $L^2(\Omega)$? Thank you for your effort!","Let $E$ and $F$ be two Banach spaces. We know that, if $f:E\rightarrow F$ is a nonlinear continuous operator, then $f$ may fail to send weakly convergent sequences to weakly convergent sequences, i.e., $u_n \rightharpoonup u$ weakly in $E$  does not necessarily imply $f(u_n)\rightharpoonup f(u)$ weakly in $F$. Even through $f$ has very mild non-linearity,  this weak sequential continuity may fail. For example, let $u_n(x)=\sin(nx), x\in (0,2\pi)$. Then Riemann-Lebesgue lemma shows that $u_n\rightharpoonup u= 0$ in $L^p(0,2\pi)$ for any $p\geq 1$.  Now, for $f(u)=\max(0,u)=u^+$, one has  $$f(u_n)\rightharpoonup \frac{1}{\pi}\neq f(u)=0  \text{ in } L^1(0,2\pi).  $$ Also, $$ \int_0^{2\pi}f(u_n(x))dx=\int_0^{2\pi}\sin^+(nx)dx=\frac{1}{n}\sum_{k=0}^{n-1}\int_{2k\pi}^{(2k+1)\pi}\sin (y)dy=2,  $$ which implies $f(u_n)$ can not converge weakly to  $0=f(u)$ in $L^2(0,2\pi)$. If $g(u)=|u|$, then  $$g(u_n)\rightharpoonup \frac{2}{\pi}\neq g(u)=0  \text{ in } L^1(0,2\pi).  $$ From these two examples, we see there is no general hope that a continuous map  preserves weak sequential continuity. We are wondering what kind of conditions imposed on $f$ so that it preserves weak sequential continuity.  For the two examples above, we note that both $f$ and $g$ are sub-linear ($|f(u)|\leq c|u|$ for some  constant $c>0$),  continuous but not differentiable. Now, a natural question arises: whether a sub-linear ($|f(u)|\leq a|u|+b$ for some positive constant $a$ and non-negative constant $b$), continuous and differentiable map $f:E\rightarrow F$ preserves weak sequential continuity? Here, we emphasize the non-linearity of $f$, since, for linear map $f$, it is known from functional analysis this question holds. For this question, one can choose $E$ and $F$ to be Hilbert spaces, for convenience. As concrete examples, one may test: 1. $u_n\rightharpoonup u$ with $u_n\geq 0$ in $L^2(\Omega)$ and $f(u)=\frac{u}{1+u}$, do we have $f(u_n)\rightharpoonup f(u)$ in $L^2(\Omega)$? 2. $u_n\rightharpoonup u$ in $L^2(\Omega)$ and $g(u)=\frac{u}{1+u^2}$, do we have $g(u_n)\rightharpoonup g(u)$ in $L^2(\Omega)$? Thank you for your effort!",,"['functional-analysis', 'weak-convergence']"
52,The topological vector space that is not metrizable.,The topological vector space that is not metrizable.,,"Let $C_0(\mathbb{R})$ denote the vector space of continuous functions on the real line with compact support. For any positive function $\rho$ let $$||f||_{\rho}:=\sup_x\rho(x)|f(x)| \ \ .$$ 1) I could show that $C_0(\mathbb{R})$ is a topological vector space. 2) I could show that for given any countable sequence of $\{\rho_j\}$, there exist $\rho$ so that $\displaystyle\lim_{x\to\pm\infty}\dfrac{\rho}{\rho_j}=\infty \  \text{for all j} .$ But by using (2), I couldn't prove that $C_0(\mathbb{R})$ is not metrizable. How can I show that $C_0(\mathbb{R})$ is not metrizable? Hint: Use (2)","Let $C_0(\mathbb{R})$ denote the vector space of continuous functions on the real line with compact support. For any positive function $\rho$ let $$||f||_{\rho}:=\sup_x\rho(x)|f(x)| \ \ .$$ 1) I could show that $C_0(\mathbb{R})$ is a topological vector space. 2) I could show that for given any countable sequence of $\{\rho_j\}$, there exist $\rho$ so that $\displaystyle\lim_{x\to\pm\infty}\dfrac{\rho}{\rho_j}=\infty \  \text{for all j} .$ But by using (2), I couldn't prove that $C_0(\mathbb{R})$ is not metrizable. How can I show that $C_0(\mathbb{R})$ is not metrizable? Hint: Use (2)",,"['functional-analysis', 'topological-vector-spaces']"
53,Question about a particular linear operator,Question about a particular linear operator,,"Let A be a linear operator. $A: L^2(0,1) \rightarrow L^2(0,1)$ given by $Ag(a) = \int_0^a(a-x)g(x)dx$ where $a \in (0,1)$. This is the integral operator, and we know ||A|| < 1 which is easy to check. We want to show $A^kg(a) = \int_0^a\frac{(a-x)^{2k-1}}{(2k-1)!}g(x)dx$ where $a \in (0,1)$. Induction seems like an obvious way to approach this: Base case k = 1: $(2k-1)! = 1 $ and $(2k-1) = 1$ hence $A^1g(a)$ is same as given. Induction hypothesis: Assume $A^ng(a) = \int_0^a\frac{(a-x)^{2n-1}}{(2n-1)!}g(x)dx$ holds for n = k. Induction step: Apply the linear operator $A$ to $A^ng(a)$, having a bit trouble with this, would we get double integrals? After we have shown $A^kg(a)$ as above, then we will use that to find $g(a):$ $f \in L^2(0,1)$ given $g \in L^2(0,1)$ and $g(a) = f(a) + \int_0^a(a-x)g(x)dx$, from this we can find $g = (I - A)^{-1}f = \sum_{i=0}^\infty A^if = \sum_{i=0}^\infty \int_0^a\frac{(a-x)^{2i-1}}{(2i-1)!}f $ now we need to try simplify this somehow and find g without summations.","Let A be a linear operator. $A: L^2(0,1) \rightarrow L^2(0,1)$ given by $Ag(a) = \int_0^a(a-x)g(x)dx$ where $a \in (0,1)$. This is the integral operator, and we know ||A|| < 1 which is easy to check. We want to show $A^kg(a) = \int_0^a\frac{(a-x)^{2k-1}}{(2k-1)!}g(x)dx$ where $a \in (0,1)$. Induction seems like an obvious way to approach this: Base case k = 1: $(2k-1)! = 1 $ and $(2k-1) = 1$ hence $A^1g(a)$ is same as given. Induction hypothesis: Assume $A^ng(a) = \int_0^a\frac{(a-x)^{2n-1}}{(2n-1)!}g(x)dx$ holds for n = k. Induction step: Apply the linear operator $A$ to $A^ng(a)$, having a bit trouble with this, would we get double integrals? After we have shown $A^kg(a)$ as above, then we will use that to find $g(a):$ $f \in L^2(0,1)$ given $g \in L^2(0,1)$ and $g(a) = f(a) + \int_0^a(a-x)g(x)dx$, from this we can find $g = (I - A)^{-1}f = \sum_{i=0}^\infty A^if = \sum_{i=0}^\infty \int_0^a\frac{(a-x)^{2i-1}}{(2i-1)!}f $ now we need to try simplify this somehow and find g without summations.",,"['analysis', 'functional-analysis', 'operator-theory']"
54,Class of functions so that $\int_0^t|f(x)|dx\leq C\cdot t^\alpha$.,Class of functions so that .,\int_0^t|f(x)|dx\leq C\cdot t^\alpha,I have done something with Lebesgue integrals and It leads me to the following class of functions: Let $\alpha$ be a real number. We consider all Lebesgue measurable function $f$ on $[0;1]$ such that there exists a constant $C>0$ and $\int_0^t|f(x)|dx\leq C\cdot t^\alpha$ for almost everywhere $t\in(0;1]$. Does this class has a name? I think it look simple and usual. Could we construct some non-trivial functions which belong to this class? (I may assume further that $\alpha>0$).,I have done something with Lebesgue integrals and It leads me to the following class of functions: Let $\alpha$ be a real number. We consider all Lebesgue measurable function $f$ on $[0;1]$ such that there exists a constant $C>0$ and $\int_0^t|f(x)|dx\leq C\cdot t^\alpha$ for almost everywhere $t\in(0;1]$. Does this class has a name? I think it look simple and usual. Could we construct some non-trivial functions which belong to this class? (I may assume further that $\alpha>0$).,,"['real-analysis', 'functional-analysis']"
55,How to prove that the spectrum of the Laplacian over $\Omega\subset \mathbb{R}^n$ is negative?,How to prove that the spectrum of the Laplacian over  is negative?,\Omega\subset \mathbb{R}^n,"I am looking for a proof of this well known fact and I guess it has to do with integration by parts (Green's identity). Unfortunately, I only know about 1-d integration by parts( I am just 3rd semester). could anybody sketch a short proof that shows that the spectrum of the Laplacian is negative? My problem with the hint that brom gave me is that I do not know what I can assume about this surface integral and so on that appear in green's identity, but maybe I am looking at this from the wrong direction. If I start: $$ \int_V u \Delta u + \underbrace{\langle \nabla u, \nabla u \rangle}_{\ge 0} = \int_{\partial V}u(\nabla u n) dS$$ I mean intuitively, the thing is, that if we assume that our function $\psi$ vanishes at infinity and we integrate over the whole space, then the surface integral is maybe zero. In this case, we had  $$ \int_V u \Delta u=-\int_V\langle \nabla u, \nabla u \rangle \le 0$$ But I do not see the correct mathematical reasoning behind this!","I am looking for a proof of this well known fact and I guess it has to do with integration by parts (Green's identity). Unfortunately, I only know about 1-d integration by parts( I am just 3rd semester). could anybody sketch a short proof that shows that the spectrum of the Laplacian is negative? My problem with the hint that brom gave me is that I do not know what I can assume about this surface integral and so on that appear in green's identity, but maybe I am looking at this from the wrong direction. If I start: $$ \int_V u \Delta u + \underbrace{\langle \nabla u, \nabla u \rangle}_{\ge 0} = \int_{\partial V}u(\nabla u n) dS$$ I mean intuitively, the thing is, that if we assume that our function $\psi$ vanishes at infinity and we integrate over the whole space, then the surface integral is maybe zero. In this case, we had  $$ \int_V u \Delta u=-\int_V\langle \nabla u, \nabla u \rangle \le 0$$ But I do not see the correct mathematical reasoning behind this!",,['functional-analysis']
56,Analyticity of C*-algebra valued functions,Analyticity of C*-algebra valued functions,,"Let $\mathcal{A}$ be a unital C*-algebra and consider a function $f:\mathbb{C} \rightarrow \mathcal{A}$. What is an accessible tool to prove or disprove that $f$ is analytic, i.e. can be locally expanded in a power series of $z$? Take as a concrete example $f_A(z) = \exp(zA-\overline{z}A^*)$ for some fixed $A\in \mathcal{A}$. Is $f_A$ an analytic function?","Let $\mathcal{A}$ be a unital C*-algebra and consider a function $f:\mathbb{C} \rightarrow \mathcal{A}$. What is an accessible tool to prove or disprove that $f$ is analytic, i.e. can be locally expanded in a power series of $z$? Take as a concrete example $f_A(z) = \exp(zA-\overline{z}A^*)$ for some fixed $A\in \mathcal{A}$. Is $f_A$ an analytic function?",,"['complex-analysis', 'functional-analysis', 'c-star-algebras', 'analyticity']"
57,Point evaluation of a linear functional on an Ultrapower,Point evaluation of a linear functional on an Ultrapower,,"Let $E$ be a Banach space and $(E)_{\mathcal U}$ be an ultrapower for some ultrafilter $\mathcal U$ on an index set $I$. It is remarked in a paper that $(E')_{\mathcal U}$ can be naturally embedded into $(E)_{\mathcal U}'$ (where I use $'$ to indicate the normed space dual). I just want to clarify that this natural embedding is given by the following action. $$(\varphi_{i})_{\mathcal U}:(x_{i})_{\mathcal U}\mapsto \lim_{i\to\mathcal U}\varphi_{i}(x_{i})$$ Is this correct?  (sorry if I chose poor tags) $\bf{\text{Definition}}$: $\lim_{i\to \mathcal U}x_i = x$ in a topological space $X$ if for every neighbourhood $U$ of $x, \{i\in I : x_i\in U\}\in \mathcal U$.","Let $E$ be a Banach space and $(E)_{\mathcal U}$ be an ultrapower for some ultrafilter $\mathcal U$ on an index set $I$. It is remarked in a paper that $(E')_{\mathcal U}$ can be naturally embedded into $(E)_{\mathcal U}'$ (where I use $'$ to indicate the normed space dual). I just want to clarify that this natural embedding is given by the following action. $$(\varphi_{i})_{\mathcal U}:(x_{i})_{\mathcal U}\mapsto \lim_{i\to\mathcal U}\varphi_{i}(x_{i})$$ Is this correct?  (sorry if I chose poor tags) $\bf{\text{Definition}}$: $\lim_{i\to \mathcal U}x_i = x$ in a topological space $X$ if for every neighbourhood $U$ of $x, \{i\in I : x_i\in U\}\in \mathcal U$.",,"['functional-analysis', 'model-theory']"
58,PDE with measure-valued right hand sides?,PDE with measure-valued right hand sides?,,"I am looking for a basic set of notes/text that deals with the basics of PDEs with measure valued right hand sides. Even an answer here that answers the following questions would be good: 1) A precise formulation of the problem. I assume it's something like wanting to solve $$-\Delta u = \delta$$ where $\delta$ is the Dirac delta function. 2) Function spaces used in this field, and what elementary theorems/techniques are used to obtain well-posedness? (For example, in the ordinary elliptic case I would say: Sobolev spaces with Lax-Milgram etc etc). Thank you","I am looking for a basic set of notes/text that deals with the basics of PDEs with measure valued right hand sides. Even an answer here that answers the following questions would be good: 1) A precise formulation of the problem. I assume it's something like wanting to solve $$-\Delta u = \delta$$ where $\delta$ is the Dirac delta function. 2) Function spaces used in this field, and what elementary theorems/techniques are used to obtain well-posedness? (For example, in the ordinary elliptic case I would say: Sobolev spaces with Lax-Milgram etc etc). Thank you",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
59,Pitt's theorem and reflexivity,Pitt's theorem and reflexivity,,"Does it follow from Pitt's theorem that the space of bounded operators from $\ell_2$ to $\ell_p$ ($p<2$) is actually reflexive? We have  $$ \mathcal{B}(\ell_2, \ell_p) = \mathcal{K}(\ell_2, \ell_p) $$  so by standard duality also  $$ \mathcal{K}(\ell_2, \ell_p)^{**} =\mathcal{B}(\ell_2, \ell_p) $$ Is this true?","Does it follow from Pitt's theorem that the space of bounded operators from $\ell_2$ to $\ell_p$ ($p<2$) is actually reflexive? We have  $$ \mathcal{B}(\ell_2, \ell_p) = \mathcal{K}(\ell_2, \ell_p) $$  so by standard duality also  $$ \mathcal{K}(\ell_2, \ell_p)^{**} =\mathcal{B}(\ell_2, \ell_p) $$ Is this true?",,"['functional-analysis', 'operator-theory', 'banach-spaces', 'compact-operators']"
60,Every quotient of a reflexive space is reflexive,Every quotient of a reflexive space is reflexive,,How do you prove the following? If $\mathcal{X}$ is reflexive and $M \leq \mathcal{X} \rightarrow \mathcal{X}/M$ is reflexive There is no assumption that $\mathcal{X}$ is a Banach space.,How do you prove the following? If $\mathcal{X}$ is reflexive and $M \leq \mathcal{X} \rightarrow \mathcal{X}/M$ is reflexive There is no assumption that $\mathcal{X}$ is a Banach space.,,"['functional-analysis', 'banach-spaces', 'normed-spaces']"
61,Measure and the intersection with an open interval,Measure and the intersection with an open interval,,"Let $E \subset \Re$ be measurable with $\mu(E) > 0$. Show that for   every $0 < \epsilon < 1$ there is an open interval $I$ such that   $\mu(E \bigcap I) > (1-\epsilon)\mu(I)$ Let $0 < \epsilon < 1$, $\mu(E)>0$. How would I start this?","Let $E \subset \Re$ be measurable with $\mu(E) > 0$. Show that for   every $0 < \epsilon < 1$ there is an open interval $I$ such that   $\mu(E \bigcap I) > (1-\epsilon)\mu(I)$ Let $0 < \epsilon < 1$, $\mu(E)>0$. How would I start this?",,"['real-analysis', 'functional-analysis', 'measure-theory']"
62,Bounded operator and Compactness problem,Bounded operator and Compactness problem,,"Let $H$ be a Hilbert space with orthonormal basis $(e_{n})_{n\in\mathbb{N}}$ . Furthermore, let $T\colon H\rightarrow C[a,b]$ be a bounded operator. a) Let $x\in [a,b]$ . Show that there is a unique $g_{x}\in H$ with $\langle f,g_x\rangle=(Tf)(x)$ and all $f \in H$ . Updated version 1. So far I have: Let $x\in[a,b]$ , define the linear continuous map $L_{x}:H\rightarrow \mathbb{K}$ , with $\mathbb{K}=\mathbb{C}$ or $\mathbb{K}=\mathbb{R}$ by $$ L_{x}(f):=T(f)(x).$$ Since $H$ is an Hilbert space and $L_{x}:H\rightarrow \mathbb{K}$ is a bounded linear functional on $H$ we can apply the Riesz-Frechet theorem. According to the Riesz-Frechet theorem there exists a unique $g_{x}\in H$ such that for $x\in[a,b]$ and all $f\in H$ $$L_{x}(f)=T(f)(x)=(Tf)(x)=\langle f,g_{x}\rangle.$$ Question 1: have I proven it correctly? Or am I missing some important details? b) Show that $$\sup_{x\in [a,b]} \sum_{j=1}^{\infty}|(Te_{j})(x)|^{2}<\infty.$$ A hint is that $||g_{x}||\leq ||T||_{H \rightarrow C[a,b]}$ . Updated version 1. Question 2: how do I prove this hint? I want to prove it before I make use of it. For the rest of the problem I have this so far: Using part a), we have that: $$\sup_{x\in [a,b]} \sum_{j=1}^{\infty}|(Te_{j})(x)|^{2}=\sup_{x\in [a,b]} \sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}.$$ Since the $e_{j}$ form an orthonormal basis in $H$ , Bessel's inequality yields $$\sup_{x\in [a,b]}\sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}\leq \sup_{x\in [a,b]}||g_{x}||_{2}^{2}.$$ Now by definition of the operator norm we have: $$\sup_{x\in [a,b]}\sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}\leq \sup_{x\in [a,b]}||g_{x}||_{2}^{2}\leq ||T||\cdot ||g_{x}||.$$ Using the hint and the fact that operator $T$ is bounded we get: $$\sup_{x\in [a,b]}\sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}\leq \sup_{x\in [a,b]}||g_{x}||_{2}^{2}\leq ||T||\cdot ||g_{x}||\leq ||T||\cdot ||T||_{{H \rightarrow C[a,b]}} <\infty.$$ Question 3: Is the proof now complete or am I missing a detail/making a mistake? In class for example for Bessel's inequality we usually had this form $\sum_{j=1}^{\infty}|\langle f,e_{j}\rangle|^{2}\leq ||f||_{2}^{2}<\infty$ . c) Show that $\sum_{j=1}^{\infty}  ||Te_{j}||_{L^{2}}^{2}< \infty$ . Updated version 1. What I have so far: We define the function $$x\mapsto\sum_{j=1}^{\infty}|Te_{j}(x)|^{2}.$$ We have from part b): $$\sup_{x\in [a,b]} \sum_{j=1}^{\infty}|(Te_{j})(x)|^{2}<\infty.$$ Integrating the function over $(a,b)$ yields $$\int_{a}^{b}\sum_{j=1}^{\infty}|Te_{j}(x)|^{2}dx<\infty.$$ Now making use of the Fubini-Tonelli theorem we can interchange limit and integral and get: $$\int_{a}^{b}\sum_{j=1}^{\infty}|Te_{j}(x)|^{2}dx=\sum_{j=1}^{\infty}\int_{a}^{b}|Te_{j}(x)|^{2}dx=\sum_{j=1}^{\infty}||Te_{j}||_{L^{2}}^{2}<\infty.$$ Question 4: Is the proof now complete or am I missing a detail/making a mistake? In fact the interchange is not clear to me. We did treat Fubini in class but only shorty to change integrals not integral and sum. Usually when changing integral and summation we used monotone convergence. d) Show that $T\colon H\rightarrow L^{2}(a,b)$ is compact. We have to get the result by making use of estimates. Updated version 1. I could prove it without estimating and had: We note that $H$ and $ L^{2}(a,b)$ are Hilbert spaces and the operator $T\colon H\rightarrow L^{2}(a,b)$ is a bounded linear operator as given before.  Furthermore in part c) we obtained an inequality. Thus we have that $T\colon H\rightarrow L^{2}(a,b)$ is an abstract Hilbert-Schmidt operator. Now we can apply a certain theorem that states that every abstract Hilbert-Schmidt operator is compact. Now for the proof making use of estimates: To show that $T$ is compact, we have to approximate $T$ in the operator norm by finite rank operators. For $N\in\mathbb{N}$ define the linear operator $T_{N}\colon H\rightarrow L^{2}(a,b)$ . Question 5: I don't know how to show that $T_{N}$ is of finite-rank? Normally you would have something along the lines that $T_{N}$ has range within $span\{f_{1},\ldots,f_{N}\}$ and hence is of finite-rank. But for this problem I don't see it. I think you can say that $ran(T)\subseteq C[a,b]$ , but $C[a,b]$ is infinite diminesional so this confuses me. Question 6: How do I show that $||T-T_{N}||\rightarrow 0$ , so I can conclude that $T$ is a compact operator.","Let be a Hilbert space with orthonormal basis . Furthermore, let be a bounded operator. a) Let . Show that there is a unique with and all . Updated version 1. So far I have: Let , define the linear continuous map , with or by Since is an Hilbert space and is a bounded linear functional on we can apply the Riesz-Frechet theorem. According to the Riesz-Frechet theorem there exists a unique such that for and all Question 1: have I proven it correctly? Or am I missing some important details? b) Show that A hint is that . Updated version 1. Question 2: how do I prove this hint? I want to prove it before I make use of it. For the rest of the problem I have this so far: Using part a), we have that: Since the form an orthonormal basis in , Bessel's inequality yields Now by definition of the operator norm we have: Using the hint and the fact that operator is bounded we get: Question 3: Is the proof now complete or am I missing a detail/making a mistake? In class for example for Bessel's inequality we usually had this form . c) Show that . Updated version 1. What I have so far: We define the function We have from part b): Integrating the function over yields Now making use of the Fubini-Tonelli theorem we can interchange limit and integral and get: Question 4: Is the proof now complete or am I missing a detail/making a mistake? In fact the interchange is not clear to me. We did treat Fubini in class but only shorty to change integrals not integral and sum. Usually when changing integral and summation we used monotone convergence. d) Show that is compact. We have to get the result by making use of estimates. Updated version 1. I could prove it without estimating and had: We note that and are Hilbert spaces and the operator is a bounded linear operator as given before.  Furthermore in part c) we obtained an inequality. Thus we have that is an abstract Hilbert-Schmidt operator. Now we can apply a certain theorem that states that every abstract Hilbert-Schmidt operator is compact. Now for the proof making use of estimates: To show that is compact, we have to approximate in the operator norm by finite rank operators. For define the linear operator . Question 5: I don't know how to show that is of finite-rank? Normally you would have something along the lines that has range within and hence is of finite-rank. But for this problem I don't see it. I think you can say that , but is infinite diminesional so this confuses me. Question 6: How do I show that , so I can conclude that is a compact operator.","H (e_{n})_{n\in\mathbb{N}} T\colon H\rightarrow C[a,b] x\in [a,b] g_{x}\in H \langle f,g_x\rangle=(Tf)(x) f \in H x\in[a,b] L_{x}:H\rightarrow \mathbb{K} \mathbb{K}=\mathbb{C} \mathbb{K}=\mathbb{R}  L_{x}(f):=T(f)(x). H L_{x}:H\rightarrow \mathbb{K} H g_{x}\in H x\in[a,b] f\in H L_{x}(f)=T(f)(x)=(Tf)(x)=\langle f,g_{x}\rangle. \sup_{x\in [a,b]} \sum_{j=1}^{\infty}|(Te_{j})(x)|^{2}<\infty. ||g_{x}||\leq ||T||_{H \rightarrow C[a,b]} \sup_{x\in [a,b]} \sum_{j=1}^{\infty}|(Te_{j})(x)|^{2}=\sup_{x\in [a,b]} \sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}. e_{j} H \sup_{x\in [a,b]}\sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}\leq \sup_{x\in [a,b]}||g_{x}||_{2}^{2}. \sup_{x\in [a,b]}\sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}\leq \sup_{x\in [a,b]}||g_{x}||_{2}^{2}\leq ||T||\cdot ||g_{x}||. T \sup_{x\in [a,b]}\sum_{j=1}^{\infty}|\langle e_{j},g_{x}\rangle|^{2}\leq \sup_{x\in [a,b]}||g_{x}||_{2}^{2}\leq ||T||\cdot ||g_{x}||\leq ||T||\cdot ||T||_{{H \rightarrow C[a,b]}} <\infty. \sum_{j=1}^{\infty}|\langle f,e_{j}\rangle|^{2}\leq ||f||_{2}^{2}<\infty \sum_{j=1}^{\infty}  ||Te_{j}||_{L^{2}}^{2}< \infty x\mapsto\sum_{j=1}^{\infty}|Te_{j}(x)|^{2}. \sup_{x\in [a,b]} \sum_{j=1}^{\infty}|(Te_{j})(x)|^{2}<\infty. (a,b) \int_{a}^{b}\sum_{j=1}^{\infty}|Te_{j}(x)|^{2}dx<\infty. \int_{a}^{b}\sum_{j=1}^{\infty}|Te_{j}(x)|^{2}dx=\sum_{j=1}^{\infty}\int_{a}^{b}|Te_{j}(x)|^{2}dx=\sum_{j=1}^{\infty}||Te_{j}||_{L^{2}}^{2}<\infty. T\colon H\rightarrow L^{2}(a,b) H  L^{2}(a,b) T\colon H\rightarrow L^{2}(a,b) T\colon H\rightarrow L^{2}(a,b) T T N\in\mathbb{N} T_{N}\colon H\rightarrow L^{2}(a,b) T_{N} T_{N} span\{f_{1},\ldots,f_{N}\} ran(T)\subseteq C[a,b] C[a,b] ||T-T_{N}||\rightarrow 0 T","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'compact-operators']"
63,basic sequence and strictly singular operators,basic sequence and strictly singular operators,,"Suppose that $T: X\to Y$ is strictly singular. Prove 1) In every infinite dimensional subspace $Z$ of $X$, there exists a normalized basic sequence $(x_n)$ such that $||Tx_n|| <2^{-n}$. 2) For every $\epsilon$ >$0$ there exists an infinite dimensional subspace $Z$ of $X$ such that $T|_Z$ is compact and has norm less than $\epsilon$. In part $1$, I got a normalized sequence $(x_n)$ such that the $||Tx_n|| <2^{-n}$, but it need not be basic. I know that if $(x_n)$ is weakly null, then it has a basic sub-sequence and I'm done. Can I choose it to be weakly null? If yes, why? if no, do you have any other idea? Thank you .","Suppose that $T: X\to Y$ is strictly singular. Prove 1) In every infinite dimensional subspace $Z$ of $X$, there exists a normalized basic sequence $(x_n)$ such that $||Tx_n|| <2^{-n}$. 2) For every $\epsilon$ >$0$ there exists an infinite dimensional subspace $Z$ of $X$ such that $T|_Z$ is compact and has norm less than $\epsilon$. In part $1$, I got a normalized sequence $(x_n)$ such that the $||Tx_n|| <2^{-n}$, but it need not be basic. I know that if $(x_n)$ is weakly null, then it has a basic sub-sequence and I'm done. Can I choose it to be weakly null? If yes, why? if no, do you have any other idea? Thank you .",,['functional-analysis']
64,Lagrange Multipliers for Function Spaces,Lagrange Multipliers for Function Spaces,,"For some constant $A > 1$ I am trying to solve the constrained minimization problem minimize $F(u)$ in $C$ subject to $H(u) = 0$. Here $F(u) = \int -u dx$ and $H(u) = \int \sqrt{1 + (u')^2} dx - A$, where our integral is from 0 to 1. I have two main questions: What is the geometric interpretation of this problem? If I assume that $u$ is a smooth minimizer I am to use the Lagrange Multiplier Theorem to compute the Euler–Lagrange equations. Derive a diﬀerential equation for $u$∗. I have heard that this is called the isoparametric problem.  Is it fair to say that the geometric interpretation here is that of minimizing area given a certain perimeter?  Is using the Lagrange Multiplier Theorem similar to using Euler-Lagrange to minimize an unconstrained functional?  Thank you!","For some constant $A > 1$ I am trying to solve the constrained minimization problem minimize $F(u)$ in $C$ subject to $H(u) = 0$. Here $F(u) = \int -u dx$ and $H(u) = \int \sqrt{1 + (u')^2} dx - A$, where our integral is from 0 to 1. I have two main questions: What is the geometric interpretation of this problem? If I assume that $u$ is a smooth minimizer I am to use the Lagrange Multiplier Theorem to compute the Euler–Lagrange equations. Derive a diﬀerential equation for $u$∗. I have heard that this is called the isoparametric problem.  Is it fair to say that the geometric interpretation here is that of minimizing area given a certain perimeter?  Is using the Lagrange Multiplier Theorem similar to using Euler-Lagrange to minimize an unconstrained functional?  Thank you!",,"['functional-analysis', 'nonlinear-optimization', 'lagrange-multiplier']"
65,Embedding of Sobolev spaces,Embedding of Sobolev spaces,,"I define the following weighted Sobolev spaces $$L^{2,s}(\mathbb{R}^3)=\bigg\lbrace u\bigg|\int_{\mathbb{R}^3}|u(x)|^2(1+|x|^2)^s<\infty\bigg\rbrace$$ and $$H^{2,s}(\mathbb{R}^3)=\left\lbrace u\bigg|\,D^\alpha u\in L^{2,s}(\mathbb{R}^3),\,|\alpha|\leq 2\right\rbrace$$ I know that the classical Sobolev space $H^2(\mathbb{R}^3)$ is contained in $C(\mathbb{R}^3)$ because $2>\frac{3}{2}$. Can I extend this result to the above weighted Sobolev spaces?","I define the following weighted Sobolev spaces $$L^{2,s}(\mathbb{R}^3)=\bigg\lbrace u\bigg|\int_{\mathbb{R}^3}|u(x)|^2(1+|x|^2)^s<\infty\bigg\rbrace$$ and $$H^{2,s}(\mathbb{R}^3)=\left\lbrace u\bigg|\,D^\alpha u\in L^{2,s}(\mathbb{R}^3),\,|\alpha|\leq 2\right\rbrace$$ I know that the classical Sobolev space $H^2(\mathbb{R}^3)$ is contained in $C(\mathbb{R}^3)$ because $2>\frac{3}{2}$. Can I extend this result to the above weighted Sobolev spaces?",,"['analysis', 'functional-analysis', 'sobolev-spaces']"
66,Why locally compact in the Gelfand representation?,Why locally compact in the Gelfand representation?,,"I'm missing something in the Gelfand representation. Let's just say $\mathfrak{A}$ is a Banach algebra. Then it's a Banach space, and so we have $\mathfrak{A}^\ast$. The multiplicative linear functionals on $\mathfrak{A}$ are continuous and have norm $\leq 1$, and so still sit inside the unit ball of $\mathfrak{A}$, which is weak-$\ast$ compact. The set of multiplicative linear functionals are weak-$\ast$ closed, so the set of mulitplicative linear functionals is then weak-$\ast$ compact. Where does not having a unit make the above not work? I always see that the multiplicative linear functionals are locally compact in the non-unital case, but I cannot spot where the unit is needed in the above.","I'm missing something in the Gelfand representation. Let's just say $\mathfrak{A}$ is a Banach algebra. Then it's a Banach space, and so we have $\mathfrak{A}^\ast$. The multiplicative linear functionals on $\mathfrak{A}$ are continuous and have norm $\leq 1$, and so still sit inside the unit ball of $\mathfrak{A}$, which is weak-$\ast$ compact. The set of multiplicative linear functionals are weak-$\ast$ closed, so the set of mulitplicative linear functionals is then weak-$\ast$ compact. Where does not having a unit make the above not work? I always see that the multiplicative linear functionals are locally compact in the non-unital case, but I cannot spot where the unit is needed in the above.",,"['functional-analysis', 'banach-algebras']"
67,"Is the Sobolev embedding $W^{l,2}(\mathbb{R}^d) \rightarrow C_0(\mathbb{R}^d)$ compact?",Is the Sobolev embedding  compact?,"W^{l,2}(\mathbb{R}^d) \rightarrow C_0(\mathbb{R}^d)","In p. 508 of the paper : http://www.jstor.org/stable/2243484 , it is mentioned that if $2l \geq d$, the embedding $W^{l,2}(\mathbb{R}^d) \rightarrow C_0(\mathbb{R}^d)$ is compact, where $W^{l,2}(\mathbb{R}^d)$ is the $(l,2)-$Sobolev space on $\mathbb{R}^d$ and $C_0(\mathbb{R}^d)$ is the space of continuous functions $\mathbb{R}^d \rightarrow \mathbb{R}$ vanishing at infinity. I have tried to look in many references but haven't found this. So is it true or not?","In p. 508 of the paper : http://www.jstor.org/stable/2243484 , it is mentioned that if $2l \geq d$, the embedding $W^{l,2}(\mathbb{R}^d) \rightarrow C_0(\mathbb{R}^d)$ is compact, where $W^{l,2}(\mathbb{R}^d)$ is the $(l,2)-$Sobolev space on $\mathbb{R}^d$ and $C_0(\mathbb{R}^d)$ is the space of continuous functions $\mathbb{R}^d \rightarrow \mathbb{R}$ vanishing at infinity. I have tried to look in many references but haven't found this. So is it true or not?",,"['functional-analysis', 'sobolev-spaces']"
68,Banach-Steinhaus variant,Banach-Steinhaus variant,,"Let $T_n$ be a sequence of continuous linear operators from a Banach   space $X$ to a normed linear space $Y$. Now, for all $x \in X$,   $\lim_{n \rightarrow \infty} T_n(x)$ exists in $Y$. Define $T(x) = \lim_{n \rightarrow \infty} T_n (x)$ on $X$. Show that $$\|T\| \leq \liminf \|T_n\|.$$ Here's my proof: Convergence of $T_n(x)$ implies that for all $x \in X$, $\|T_n(x)\|\leq M_x$ for some $M_x$. The uniform boundedness principle implies $T_n$ is uniformly bounded, in particular, $\liminf  \|T_n\| < \infty.$ Now for all $z \in X$ with $\|z\|=1$, $$\|T(z)\|=\lim \|T_n(z)\| \leq \liminf  \|T_n\| \|z\| = \liminf  \|T_n\|.$$ I'm not too sure about that last line. How can I improve this proof?","Let $T_n$ be a sequence of continuous linear operators from a Banach   space $X$ to a normed linear space $Y$. Now, for all $x \in X$,   $\lim_{n \rightarrow \infty} T_n(x)$ exists in $Y$. Define $T(x) = \lim_{n \rightarrow \infty} T_n (x)$ on $X$. Show that $$\|T\| \leq \liminf \|T_n\|.$$ Here's my proof: Convergence of $T_n(x)$ implies that for all $x \in X$, $\|T_n(x)\|\leq M_x$ for some $M_x$. The uniform boundedness principle implies $T_n$ is uniformly bounded, in particular, $\liminf  \|T_n\| < \infty.$ Now for all $z \in X$ with $\|z\|=1$, $$\|T(z)\|=\lim \|T_n(z)\| \leq \liminf  \|T_n\| \|z\| = \liminf  \|T_n\|.$$ I'm not too sure about that last line. How can I improve this proof?",,['functional-analysis']
69,Existence of solution of PDE using Galerkin method,Existence of solution of PDE using Galerkin method,,"I wonder if anyone can give me a reference to a paper/book that rigorously addresses how to use the Galerkin method to show existence/uniqueness of a PDE. The usual suspects (Evans, Renardy, ...) do not suffice for me. I am getting confused with some sources saying we need weak-* convergence and others not so, and some sources do not address issues such as what the canonical way is to introduce the finite dimensional problem and how it  becomes an ODE. Thanks.","I wonder if anyone can give me a reference to a paper/book that rigorously addresses how to use the Galerkin method to show existence/uniqueness of a PDE. The usual suspects (Evans, Renardy, ...) do not suffice for me. I am getting confused with some sources saying we need weak-* convergence and others not so, and some sources do not address issues such as what the canonical way is to introduce the finite dimensional problem and how it  becomes an ODE. Thanks.",,"['functional-analysis', 'reference-request', 'partial-differential-equations']"
70,Equivalence of Norms Defined on a Cartesian Product,Equivalence of Norms Defined on a Cartesian Product,,"While studying some notes on normed vector spaces, I have come upon the proof that addition $+:V \times V\to V$ of vectors in a normed vector space $V$ is a continuous operation. The proof of this fact is quite easy except of (in my opinion) one step: the choice of the norm on $V \times V$. The proof has been done for a norm defined as $\|(v_1,v_2)\| = \|v_1\| + \|v_2\|$ and a comment has been made that certain other norms (e.g. $\|(v_1,v_2)\| = \max\{\|v_1\|,\|v_2\|\}$) can be used as well, since they generate the same topology (the product topology, I suppose). However, I struggle with the question what is the precise set of all norms that can be used in this proof. I suppose that the theorem has to be interpreted in the way that $+$ is continuous with respect to the product topology. Thus, my question can be restated as follows: given a norm $\|\cdot\|$ on $V$ generating a topology $\tau$, which norms can be used on $V \times V$ to generate the product topology with respect to $\tau$? I do not find this question to be straightforward, since in infinite dimensional spaces, norms need not be equivalent. Thank you in advance.","While studying some notes on normed vector spaces, I have come upon the proof that addition $+:V \times V\to V$ of vectors in a normed vector space $V$ is a continuous operation. The proof of this fact is quite easy except of (in my opinion) one step: the choice of the norm on $V \times V$. The proof has been done for a norm defined as $\|(v_1,v_2)\| = \|v_1\| + \|v_2\|$ and a comment has been made that certain other norms (e.g. $\|(v_1,v_2)\| = \max\{\|v_1\|,\|v_2\|\}$) can be used as well, since they generate the same topology (the product topology, I suppose). However, I struggle with the question what is the precise set of all norms that can be used in this proof. I suppose that the theorem has to be interpreted in the way that $+$ is continuous with respect to the product topology. Thus, my question can be restated as follows: given a norm $\|\cdot\|$ on $V$ generating a topology $\tau$, which norms can be used on $V \times V$ to generate the product topology with respect to $\tau$? I do not find this question to be straightforward, since in infinite dimensional spaces, norms need not be equivalent. Thank you in advance.",,"['functional-analysis', 'normed-spaces']"
71,Boundedness of an integral operator,Boundedness of an integral operator,,"Let $K_n \in L^1([0,1]), n \geq 1$ and define a linear map $T$ from  $L^\infty([0,1]) $to sequences by  $$ Tf = (x_n), \;\; x_n =\int_0^1 K_n(x)f(x)dx$$  Show that $T$ is a bounded linear operator from $L^\infty([0,1]) $to $\ell^\infty$ iff $$\sup_{n\geq 1} \int_0^1|K_n(x)| dx \lt \infty$$ My try:  $(\Leftarrow)$  $$\sup_n |x_n| = \sup_n  |\int_0^1 K_n(x)f(x) dx| \leq \sup_n\int_0^1 |K_n(x)f(x)| dx \leq \|f\|_\infty \sup_n\int_0^1 |K_n(x)|dx $$  $(\Rightarrow)$  I can't get the absolute value right. I was thinking uniformed boundedness and that every coordinate can be written with help of a linear functional. But then I end up with $\sup_{\|f\| = 1} |\int_0^1 K_n(x) f(x) dx | \leq \infty$. Can I choose my $f$ so that I get what I want?","Let $K_n \in L^1([0,1]), n \geq 1$ and define a linear map $T$ from  $L^\infty([0,1]) $to sequences by  $$ Tf = (x_n), \;\; x_n =\int_0^1 K_n(x)f(x)dx$$  Show that $T$ is a bounded linear operator from $L^\infty([0,1]) $to $\ell^\infty$ iff $$\sup_{n\geq 1} \int_0^1|K_n(x)| dx \lt \infty$$ My try:  $(\Leftarrow)$  $$\sup_n |x_n| = \sup_n  |\int_0^1 K_n(x)f(x) dx| \leq \sup_n\int_0^1 |K_n(x)f(x)| dx \leq \|f\|_\infty \sup_n\int_0^1 |K_n(x)|dx $$  $(\Rightarrow)$  I can't get the absolute value right. I was thinking uniformed boundedness and that every coordinate can be written with help of a linear functional. But then I end up with $\sup_{\|f\| = 1} |\int_0^1 K_n(x) f(x) dx | \leq \infty$. Can I choose my $f$ so that I get what I want?",,['functional-analysis']
72,Multiplication operator and trace class,Multiplication operator and trace class,,"Suppose we work in $H=l^2(\Bbb{N})$ and suppose the multiplication operator $T_f$ such that $T_f\psi=f\psi$ and $f:\Bbb{N}\rightarrow \Bbb{C}$. We denote by $B_1(H)$ the trace class of operators. Question: I want to find a sufficient and necessary condition for $f$, such that $T_f\in B_1(H)$. Can someone help me with this question? (This a question from a exam for Introduction to Functional analysis.)","Suppose we work in $H=l^2(\Bbb{N})$ and suppose the multiplication operator $T_f$ such that $T_f\psi=f\psi$ and $f:\Bbb{N}\rightarrow \Bbb{C}$. We denote by $B_1(H)$ the trace class of operators. Question: I want to find a sufficient and necessary condition for $f$, such that $T_f\in B_1(H)$. Can someone help me with this question? (This a question from a exam for Introduction to Functional analysis.)",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'lp-spaces']"
73,cyclic vector exists for symmetric operator iff there no repeated eigenvalues,cyclic vector exists for symmetric operator iff there no repeated eigenvalues,,"Considering a symmetric operator $A$ acting on a finite dimensional Hilbert space $H$, we say $x\in H$ is a cyclic vector for $A$ if the set of finite linear combinations of $\{A^n x:n=0,1,2,...\}$ is equal to $H$. I am looking for a proof of whether $A$ must have a cyclic vector iff $A$ has no repeated eigenvalues. Hints are welcomed.","Considering a symmetric operator $A$ acting on a finite dimensional Hilbert space $H$, we say $x\in H$ is a cyclic vector for $A$ if the set of finite linear combinations of $\{A^n x:n=0,1,2,...\}$ is equal to $H$. I am looking for a proof of whether $A$ must have a cyclic vector iff $A$ has no repeated eigenvalues. Hints are welcomed.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
74,"Sequence of a product of functions in $L^p. L^q$ with $p,q$ conjugate",Sequence of a product of functions in  with  conjugate,"L^p. L^q p,q","The question is: if $f_i$ is a sequence of functions in $L^p$ converging to $f$ and $g_i$ a sequence in $L^q$ converging to $g$ show that $f_ig_i$ converges to $fg$ in $L^1$ for $p,q$ finite and $\frac{1}{p}+\frac{1}{q}=1$. Does this result hold if $p=1, q=\infty$? So I think I showed the first part. $||f_ig_i-fg||_1=||f_ig_i-f_ig+f_ig-fg||_1\leq||f_ig_i-f_ig||_1+||f_ig-fg||_1 = ||f_i(g_i-g)||_1+||g(f_i-f)||_1\leq||f_i||_p||g_i-g||_q+||g||_q||f_i-f||_p$ (by Holder's inequality) which goes to $0$ as $i\rightarrow\infty$. However the second part of the question throws me off because I don't see why this proof doesn't work just as well for $p=1,q=\infty$.  Am I missing something? Does the second part also hold?","The question is: if $f_i$ is a sequence of functions in $L^p$ converging to $f$ and $g_i$ a sequence in $L^q$ converging to $g$ show that $f_ig_i$ converges to $fg$ in $L^1$ for $p,q$ finite and $\frac{1}{p}+\frac{1}{q}=1$. Does this result hold if $p=1, q=\infty$? So I think I showed the first part. $||f_ig_i-fg||_1=||f_ig_i-f_ig+f_ig-fg||_1\leq||f_ig_i-f_ig||_1+||f_ig-fg||_1 = ||f_i(g_i-g)||_1+||g(f_i-f)||_1\leq||f_i||_p||g_i-g||_q+||g||_q||f_i-f||_p$ (by Holder's inequality) which goes to $0$ as $i\rightarrow\infty$. However the second part of the question throws me off because I don't see why this proof doesn't work just as well for $p=1,q=\infty$.  Am I missing something? Does the second part also hold?",,"['real-analysis', 'functional-analysis']"
75,Schwartz space: semi norm estimate on translation,Schwartz space: semi norm estimate on translation,,"the following family of semi norms is commonly used to introduce the space of Schwartz functions $\mathcal{S}(\mathbb{R}^n)$: $$ \|\phi\|_N := \sup_{\substack{x \in \mathbb{R}^n \\ |\alpha|\,,|\beta| \leq N}} |\,x^\beta(\partial^\alpha_x \phi)(x)\,| $$ defined for each non-negative integer $N$, where the multi - index notation is used and $\phi$ is a $C^\infty$ function. in particular, this is done in the book by E. Stein (et al), ""Functional Analysis"" (Ch. 3). There it is also stated (in the proof of Proposition 1.5 in Ch.3, Sect. 1.5) that for any compactly supported $C^\infty$ function $\psi$ and any $N$, if $\psi^\backsim_x := \psi(x - y)$ then we have the estimate $$ \|\psi^\backsim_x\| \leq c(1 + |x|)^N\|\psi\|_N \,, $$ and more generally,  $$ \|\partial^\alpha_x \psi^\backsim_x\| \leq c(1 + |x|)^N\|\psi\|_{N + |\alpha|} \,, $$ this confuses me and clearly shows that I don't understand the notation of the semi-norm well enough. here is what I struggle with: since $\psi^\backsim_x$ denotes translation by $x$ and this is done before I take the norm, I would have thought that this operation has no impact on the size of the norm, i.e. just plugging in the translated function in the norm I'd have $$ \|\psi^\backsim_x\|_N := \sup_{\substack{(x-y)\, \in \, \mathbb{R}^n \\ |\alpha|\,,|\beta| \leq N}} |\,(x - y)^\beta(\partial^\alpha_{(x - y)} \psi)(x - y)\,| = \|\psi\|_N  $$ Why is this not the correct way to measure $\psi^\backsim_x$ with respect to the family $\|\cdot\|_N$ ? thanks a lot for clarification!","the following family of semi norms is commonly used to introduce the space of Schwartz functions $\mathcal{S}(\mathbb{R}^n)$: $$ \|\phi\|_N := \sup_{\substack{x \in \mathbb{R}^n \\ |\alpha|\,,|\beta| \leq N}} |\,x^\beta(\partial^\alpha_x \phi)(x)\,| $$ defined for each non-negative integer $N$, where the multi - index notation is used and $\phi$ is a $C^\infty$ function. in particular, this is done in the book by E. Stein (et al), ""Functional Analysis"" (Ch. 3). There it is also stated (in the proof of Proposition 1.5 in Ch.3, Sect. 1.5) that for any compactly supported $C^\infty$ function $\psi$ and any $N$, if $\psi^\backsim_x := \psi(x - y)$ then we have the estimate $$ \|\psi^\backsim_x\| \leq c(1 + |x|)^N\|\psi\|_N \,, $$ and more generally,  $$ \|\partial^\alpha_x \psi^\backsim_x\| \leq c(1 + |x|)^N\|\psi\|_{N + |\alpha|} \,, $$ this confuses me and clearly shows that I don't understand the notation of the semi-norm well enough. here is what I struggle with: since $\psi^\backsim_x$ denotes translation by $x$ and this is done before I take the norm, I would have thought that this operation has no impact on the size of the norm, i.e. just plugging in the translated function in the norm I'd have $$ \|\psi^\backsim_x\|_N := \sup_{\substack{(x-y)\, \in \, \mathbb{R}^n \\ |\alpha|\,,|\beta| \leq N}} |\,(x - y)^\beta(\partial^\alpha_{(x - y)} \psi)(x - y)\,| = \|\psi\|_N  $$ Why is this not the correct way to measure $\psi^\backsim_x$ with respect to the family $\|\cdot\|_N$ ? thanks a lot for clarification!",,"['real-analysis', 'analysis', 'functional-analysis', 'fourier-analysis', 'distribution-theory']"
76,1-separated sequences of unit vectors in Banach spaces,1-separated sequences of unit vectors in Banach spaces,,"Given an infinite-dimensional Banach space $X$, I would like to construct a sequence of linearly independent unit vectors such that $\|u_k-u_l\|\geqslant 1$ whenever $k\neq l$. Any ideas on how to realize this?","Given an infinite-dimensional Banach space $X$, I would like to construct a sequence of linearly independent unit vectors such that $\|u_k-u_l\|\geqslant 1$ whenever $k\neq l$. Any ideas on how to realize this?",,"['functional-analysis', 'banach-spaces']"
77,Sobolev Spaces and Weak Derivatives,Sobolev Spaces and Weak Derivatives,,"As you can probably guess, I'm currently studying about differential operators and functional analysis.  We've studied the following theorem: A function $f \in L^2 (\Omega) $ lies in $ W^{1,2} ( \Omega) $ if and only if there exists a function $g \in L^2 ( \Omega ) $ such that:   $$\int_\Omega f \left\{ b_0 (x) \phi(x) - \sum_{i=1}^n \frac{ \partial(b_i (x) \phi(x)}{\partial x_i} \right\} d^n x = \int_\Omega g(x) \phi(x) d^n x $$ for every choice of functions $b_i (x) \in C^\infty (\bar{\Omega} ) $, and $\phi \in C_c^\infty (\Omega ) $ . Can someone help me use this theorem in order to prove that the function $f(x)= \frac{x_1}{|x|} $ is in $W^{1,2}( \{ x \in \mathbb{R} ^n : |x| <1 \} ) $? What should be my $g$ and how can I prove it? I really need your help ! Thanks  !","As you can probably guess, I'm currently studying about differential operators and functional analysis.  We've studied the following theorem: A function $f \in L^2 (\Omega) $ lies in $ W^{1,2} ( \Omega) $ if and only if there exists a function $g \in L^2 ( \Omega ) $ such that:   $$\int_\Omega f \left\{ b_0 (x) \phi(x) - \sum_{i=1}^n \frac{ \partial(b_i (x) \phi(x)}{\partial x_i} \right\} d^n x = \int_\Omega g(x) \phi(x) d^n x $$ for every choice of functions $b_i (x) \in C^\infty (\bar{\Omega} ) $, and $\phi \in C_c^\infty (\Omega ) $ . Can someone help me use this theorem in order to prove that the function $f(x)= \frac{x_1}{|x|} $ is in $W^{1,2}( \{ x \in \mathbb{R} ^n : |x| <1 \} ) $? What should be my $g$ and how can I prove it? I really need your help ! Thanks  !",,"['analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
78,Criterion for a limit of invertible operators on a Banach space to be invertible,Criterion for a limit of invertible operators on a Banach space to be invertible,,"Let $A_n$ linear operators in a Banach space $B$ that have inverses. $||A_n-A|| \to 0$ for some operator $A$. I need to prove that $A$ has an inverse operator iff the sequence $\{||A_n^{-1}||\}$ is bounded. I am almost sure it should be solved with the Uniform boundedness principle, but I can't figure it out, neither statements.","Let $A_n$ linear operators in a Banach space $B$ that have inverses. $||A_n-A|| \to 0$ for some operator $A$. I need to prove that $A$ has an inverse operator iff the sequence $\{||A_n^{-1}||\}$ is bounded. I am almost sure it should be solved with the Uniform boundedness principle, but I can't figure it out, neither statements.",,"['functional-analysis', 'banach-spaces', 'banach-algebras']"
79,"Is the functional $F(u) = \int_{\Omega} \langle A(x) \nabla u, \nabla u \rangle$ convex?",Is the functional  convex?,"F(u) = \int_{\Omega} \langle A(x) \nabla u, \nabla u \rangle","The functional \begin{equation} F(u) = \int_{\Omega} \langle A(x) \nabla u, \nabla u \rangle \end{equation} where $A$ is a symmetric matrix . You can assume $\Omega$ conviniente such that the expression above make sense. For example, $C^{1}(\Omega, H^{1}_{0}(\Omega))$ or other space such that the functional above be convex. I don't know if the hypothesis that $A$ is simetric is nescessary. Thank you.","The functional \begin{equation} F(u) = \int_{\Omega} \langle A(x) \nabla u, \nabla u \rangle \end{equation} where $A$ is a symmetric matrix . You can assume $\Omega$ conviniente such that the expression above make sense. For example, $C^{1}(\Omega, H^{1}_{0}(\Omega))$ or other space such that the functional above be convex. I don't know if the hypothesis that $A$ is simetric is nescessary. Thank you.",,"['analysis', 'functional-analysis']"
80,"$A\oplus B\cong A\oplus C$ implies $B\cong C$?  (No, it does not)","implies ?  (No, it does not)",A\oplus B\cong A\oplus C B\cong C,"I am asked to prove that for $p\in (1, \infty)$, $$L_{p}[0,1]\cong L_{p}[0,1]\oplus \ell_{2}$$ on a homework assignment, and I think I can show using results from class that $\ell_2\oplus \ell_2\cong \ell_2$. From this I could say that  $L_{p}[0,1]\oplus \ell_{2}\cong L_{p}[0,1]\oplus \ell_{2}\oplus\ell_{2}$ From here I feel like I should be able to conclude that  $$L_{p}[0,1]\cong L_{p}[0,1]\oplus \ell_{2}$$ But I know of no such result that allows me to do this.  Can anyone tell me if it's true or false? EDIT:  Definitely false.  (See counter example below from Arturo Magidin). That is, if $B\oplus A\cong C\oplus A$ can I conclude that $B\cong C$? Proper Solution (based on hints below from t.b.): 1) Prove that $\ell_2\oplus \ell_2\cong \ell_2$ 2) Use the fact that $\ell_2$ is complemented in $L_p[0,1]$ to write $L_p[0,1] = \ell_2\oplus (\ell_2)^{c}$. 3) Then I combine these to obtain: $L_p[0,1]\cong \ell_2\oplus (\ell_2)^{c}\cong (\ell_2\oplus \ell_2) \oplus (\ell_2)^{c} \cong \ell_2 \oplus (\ell_2\oplus (\ell_2)^{c})\cong \ell_2\oplus L_p[0,1]$. I skipped some pieces of your more general argument.  I was just wondering if I did anything illegal, so to speak.","I am asked to prove that for $p\in (1, \infty)$, $$L_{p}[0,1]\cong L_{p}[0,1]\oplus \ell_{2}$$ on a homework assignment, and I think I can show using results from class that $\ell_2\oplus \ell_2\cong \ell_2$. From this I could say that  $L_{p}[0,1]\oplus \ell_{2}\cong L_{p}[0,1]\oplus \ell_{2}\oplus\ell_{2}$ From here I feel like I should be able to conclude that  $$L_{p}[0,1]\cong L_{p}[0,1]\oplus \ell_{2}$$ But I know of no such result that allows me to do this.  Can anyone tell me if it's true or false? EDIT:  Definitely false.  (See counter example below from Arturo Magidin). That is, if $B\oplus A\cong C\oplus A$ can I conclude that $B\cong C$? Proper Solution (based on hints below from t.b.): 1) Prove that $\ell_2\oplus \ell_2\cong \ell_2$ 2) Use the fact that $\ell_2$ is complemented in $L_p[0,1]$ to write $L_p[0,1] = \ell_2\oplus (\ell_2)^{c}$. 3) Then I combine these to obtain: $L_p[0,1]\cong \ell_2\oplus (\ell_2)^{c}\cong (\ell_2\oplus \ell_2) \oplus (\ell_2)^{c} \cong \ell_2 \oplus (\ell_2\oplus (\ell_2)^{c})\cong \ell_2\oplus L_p[0,1]$. I skipped some pieces of your more general argument.  I was just wondering if I did anything illegal, so to speak.",,"['functional-analysis', 'banach-spaces']"
81,Can metric properties can be expressed in category theoretical terms?,Can metric properties can be expressed in category theoretical terms?,,"A simple example: If you are given the category of Hilbert spaces with the bounded linear mappings as morphism sets, then dualization is a contravariant endofunctor. So we can talk about ""qualitative"" properties, or in other words, things which one might label as ""soft analysis"". However, in contrast to this, ""hard analysis"" would not only ask for the dual morphism, but also would like to compare the norms of the morphism and the dual morphism. I have no clue whether category theoretical concepts are powerful enough to talk about such relations reasonably. More generally, while I do not expect that estimates can be explicitly stated in these algebraic terms, I would like to express that many algebraic constructions are metrically well-behaved. Suppose I am given some objects and morphism sets in the Hilbert space category, and build new objects and morphism sets from these, e.g. direct sums, tensor products, apply certain well-known functors. The morphism that are constructed are either with norm $1$ - e.g. inclusions and projections for the direct sum - or they are constructed through a functor, like dualization, such that their norms can be easily estimated in terms of the norms of their 'preimages'. What does this tell me about the reach of category theory, and can we describe the metric behaviour of categorial constructions in categorial terms?","A simple example: If you are given the category of Hilbert spaces with the bounded linear mappings as morphism sets, then dualization is a contravariant endofunctor. So we can talk about ""qualitative"" properties, or in other words, things which one might label as ""soft analysis"". However, in contrast to this, ""hard analysis"" would not only ask for the dual morphism, but also would like to compare the norms of the morphism and the dual morphism. I have no clue whether category theoretical concepts are powerful enough to talk about such relations reasonably. More generally, while I do not expect that estimates can be explicitly stated in these algebraic terms, I would like to express that many algebraic constructions are metrically well-behaved. Suppose I am given some objects and morphism sets in the Hilbert space category, and build new objects and morphism sets from these, e.g. direct sums, tensor products, apply certain well-known functors. The morphism that are constructed are either with norm $1$ - e.g. inclusions and projections for the direct sum - or they are constructed through a functor, like dualization, such that their norms can be easily estimated in terms of the norms of their 'preimages'. What does this tell me about the reach of category theory, and can we describe the metric behaviour of categorial constructions in categorial terms?",,"['analysis', 'functional-analysis', 'category-theory']"
82,Decomposing a Bounded Linear Functional on Lp as a difference of Positive Bounded Linear functionals,Decomposing a Bounded Linear Functional on Lp as a difference of Positive Bounded Linear functionals,,"I am learning Measure theory via self study of Bartle ""The elements of Integration and Lebesgue Measure"". I was stumped by the reasoning in one of the decomposition proofs. The point is to show that a Bounded Linear Functional can be represented as the difference of two positive bounded linear functionals. The proof presented is as follows. Define $G^+ = sup\{G(f) : g \in L_p : 0 \le g \le f\}$ for all $f \ge 0$. The next step is to ST $G^+$ is a BLF (Bounded Linear Functional). It is clear that $G^+(cf) = c G^+ (f)$ for $c \ge 0$, $f \ge 0$ (This part was not a problem).  The next step attempts to prove that given $f_j \ge 0, G^(f_1 + f_2) = G^+(f_1) + G^+(f_2)$ This is where I got confused and did not quite understand the line of reasoning. This is how the proof continues: If $0 \le g_j \le f_j$ Then $G(g_1) + G(g_2) = G(g_1 + g_2) \le G^+(f_1 + f_2)$ Taking supremum over all $g_j \in L_p$ we claim that $G^+(f_1) + G^+(f_2) \le G^+(f_1 + f_2)$ I would have thought that it would be te other way around, i.e. $sup\{G(g_1) : g_1 \in L_p, 0 \le g_1 \le f_1 \}$ + $sup\{G(g_2) : g_2 \in L_p, 0 \le g_2 \le f_2 \}$ >= $sup\{G(g_1  + g_2) : g_1, g_2 \in L_p, 0 \le g_j \le f_j\}$ Which implies $G^+(f_1) + G^+(f_2) \ge G^+(f_1 + f_2)$ The book continues: Conversely if $0 \le h \le f_1 + f_2$ let $g_1 = sup(h - f_2,0)$  and $g_2 = inf(h, f_2)$. It follows that $g_1 + g_2 = h$  and that $0 \le g_j \le f_j$. Threfore $G(h) = G(g_1) + G(g_2) \le G^+(f_1) + G^+(f_2)$ Since this is true for all $h \in L_p$ we have  $G^+(f_1 + f_2) = G^+(f_1) + G^+(f_2)$ My questions were: Why would $G^+$ be positive? How did the final conclusion $G^+(f_1 + f_2) = G^+(f_1) + G^+(f_2)$ for f_1, f_2 in L_p follow? I would have thougt that the definition for $G^+$ should have been $G^+ = sup\{G(f) : g \in L_p : 0 \le g \le f, G(g) \ge 0\}$","I am learning Measure theory via self study of Bartle ""The elements of Integration and Lebesgue Measure"". I was stumped by the reasoning in one of the decomposition proofs. The point is to show that a Bounded Linear Functional can be represented as the difference of two positive bounded linear functionals. The proof presented is as follows. Define $G^+ = sup\{G(f) : g \in L_p : 0 \le g \le f\}$ for all $f \ge 0$. The next step is to ST $G^+$ is a BLF (Bounded Linear Functional). It is clear that $G^+(cf) = c G^+ (f)$ for $c \ge 0$, $f \ge 0$ (This part was not a problem).  The next step attempts to prove that given $f_j \ge 0, G^(f_1 + f_2) = G^+(f_1) + G^+(f_2)$ This is where I got confused and did not quite understand the line of reasoning. This is how the proof continues: If $0 \le g_j \le f_j$ Then $G(g_1) + G(g_2) = G(g_1 + g_2) \le G^+(f_1 + f_2)$ Taking supremum over all $g_j \in L_p$ we claim that $G^+(f_1) + G^+(f_2) \le G^+(f_1 + f_2)$ I would have thought that it would be te other way around, i.e. $sup\{G(g_1) : g_1 \in L_p, 0 \le g_1 \le f_1 \}$ + $sup\{G(g_2) : g_2 \in L_p, 0 \le g_2 \le f_2 \}$ >= $sup\{G(g_1  + g_2) : g_1, g_2 \in L_p, 0 \le g_j \le f_j\}$ Which implies $G^+(f_1) + G^+(f_2) \ge G^+(f_1 + f_2)$ The book continues: Conversely if $0 \le h \le f_1 + f_2$ let $g_1 = sup(h - f_2,0)$  and $g_2 = inf(h, f_2)$. It follows that $g_1 + g_2 = h$  and that $0 \le g_j \le f_j$. Threfore $G(h) = G(g_1) + G(g_2) \le G^+(f_1) + G^+(f_2)$ Since this is true for all $h \in L_p$ we have  $G^+(f_1 + f_2) = G^+(f_1) + G^+(f_2)$ My questions were: Why would $G^+$ be positive? How did the final conclusion $G^+(f_1 + f_2) = G^+(f_1) + G^+(f_2)$ for f_1, f_2 in L_p follow? I would have thougt that the definition for $G^+$ should have been $G^+ = sup\{G(f) : g \in L_p : 0 \le g \le f, G(g) \ge 0\}$",,"['functional-analysis', 'measure-theory']"
83,Rudin theorem $7.8$,Rudin theorem,7.8,"There is the definition of $(D\mu)(x)$ : Accordingly, let us fix a dimension $k$ , denote the open ball with center $x\in\mathbb{R}^k$ and radius $r>0$ by $$B(x,r)=\{y\in\mathbb{R}^k:\lvert y-x\rvert<r\}$$ (the absolute value indicates the euclidean metric, as in Sec. 2.19), associate to any complex Borel measure $\mu$ on $\mathbb{R}$ the quotients $$(Q_r\mu)(x)=\frac{\mu(B(x,r))}{m(B(x,r))},$$ where $m=m_k$ is Lebesgues measure on $\mathbb{R}^k$ , and define the symmetric derivative of $\mu$ at $x$ to be $$(D\mu)(x)=\lim_{r\to0}(Q_r\mu)(x)$$ at those points $x\in\mathbb{R}^k$ at which this limit exists. There is the definition of Lebesgue points: If $f \in L^{1}(\Bbb R^{k})$ , any $x \in \Bbb R^{k}$ for which it is true that $$ \lim_{r\to 0}  \frac{1}{m(B_r)} \int_{B(x,r)} |f(y) - f(x)|\  dm(y) = 0$$ is called a Lebesgue point of $f$ There is the theorem: Suppose $\mu$ is a complex Borel measure on $R^k$ , and $\mu$ is absolutely continuous with respect to $m$ . Let $f$ be the Radom-Nikodym derivative of $\mu$ with respect to $m$ . Then $D\mu = f$ a.e. and $$\mu(E) = \int_E (D\mu)dm $$ ( mark this by $(1)$ ) for all Borel sets $E \subset R^{k}$ . The Radon-Nikodym theorem asserts that $(1)$ holds with $f$ in place of $D\mu$ . At any Lebesgue point $x$ of $f$ , it follows that $$f(x) = \lim_{r\to 0} \frac{1}{m(B_r)} \int_{B(x,r)} f dm = \lim_{r\to 0} \frac{\mu(B(x,r))}{m(B(x,r))}. $$ Thus $(D\mu)(x)$ exists and equals $f(x)$ at every Lebesgue point of $f$ , hence a.e. I don't understand how do we get that $f(x) = \lim_{r\to 0} \frac{1}{m(B_r)} \int_{B(x,r)} f dm$ at any Lebesgue point $x$ of $f$ . Any help would be appreciated.","There is the definition of : Accordingly, let us fix a dimension , denote the open ball with center and radius by (the absolute value indicates the euclidean metric, as in Sec. 2.19), associate to any complex Borel measure on the quotients where is Lebesgues measure on , and define the symmetric derivative of at to be at those points at which this limit exists. There is the definition of Lebesgue points: If , any for which it is true that is called a Lebesgue point of There is the theorem: Suppose is a complex Borel measure on , and is absolutely continuous with respect to . Let be the Radom-Nikodym derivative of with respect to . Then a.e. and ( mark this by ) for all Borel sets . The Radon-Nikodym theorem asserts that holds with in place of . At any Lebesgue point of , it follows that Thus exists and equals at every Lebesgue point of , hence a.e. I don't understand how do we get that at any Lebesgue point of . Any help would be appreciated.","(D\mu)(x) k x\in\mathbb{R}^k r>0 B(x,r)=\{y\in\mathbb{R}^k:\lvert y-x\rvert<r\} \mu \mathbb{R} (Q_r\mu)(x)=\frac{\mu(B(x,r))}{m(B(x,r))}, m=m_k \mathbb{R}^k \mu x (D\mu)(x)=\lim_{r\to0}(Q_r\mu)(x) x\in\mathbb{R}^k f \in L^{1}(\Bbb R^{k}) x \in \Bbb R^{k}  \lim_{r\to 0}  \frac{1}{m(B_r)} \int_{B(x,r)} |f(y) - f(x)|\  dm(y) = 0 f \mu R^k \mu m f \mu m D\mu = f \mu(E) = \int_E (D\mu)dm  (1) E \subset R^{k} (1) f D\mu x f f(x) = \lim_{r\to 0} \frac{1}{m(B_r)} \int_{B(x,r)} f dm = \lim_{r\to 0} \frac{\mu(B(x,r))}{m(B(x,r))}.  (D\mu)(x) f(x) f f(x) = \lim_{r\to 0} \frac{1}{m(B_r)} \int_{B(x,r)} f dm x f","['real-analysis', 'functional-analysis', 'complex-analysis', 'analysis', 'measure-theory']"
84,Direct method with integral constraint,Direct method with integral constraint,,"Let $\Omega\subset\mathbb{R}^n$ be nonempty, open and bounded with $C^1$ boundary. Let $p\in[1,n)$ . Let $g\in C(\mathbb{R})$ satisfy $$|g(y)|\leq C(1+|y|^q)$$ for some $C<\infty$ and some $q$ with $1\leq q<p^*$ . Let $(u_k)_k$ be a sequence with $u_k\in W^{1,p}(\Omega)$ and $$\int_{\Omega}g(u_k(x))dx=0 \text{  for every } k\in\mathbb{N}.$$ Show: When $u_k$ converges weakly to $u_*$ in $W^{1,p}(\Omega)$ , then $$\int_{\Omega}g(u_*(x))dx=0.$$ My attempt: By using the boundness of weakly convergent sequences, embedding theorems and Arzelà–Ascoli theorem, I obtained a uniformly convergent subsequence $(u_k)_k$ (without renaming), whose limit is $u_*$ due to the uniqueness of the limit. This yields: $$\left\lvert \int_{\Omega}g(u_*(x))dx\right\rvert\leq\int_{\Omega}\lvert g(u_k(x))-g(u_*(x))\rvert dx\leq \varepsilon \lvert\Omega\rvert$$ for $||u_k-u_*||_C<\delta$ using the continuity of $g$ . What surprises me now is that I didn't use the growth condition of $g$ . Can you assist me with this? Any help is greatly appreciated! Edit: Equicontinuity: Morrey yields $W_0^{1,4}(\Omega)\subset C^{0,\frac{1}{4}}(\Omega)$ , so $[u_k]_{C^{0,\frac{1}{4}}(\Omega)}\leq c$ . This implies $$|u_k(x)-u_k(y)|\leq c|x-y|^{\frac{1}{4}}.$$ Choosing $\delta=(\frac{\varepsilon}{c})^4$ : For $|x-y|<\delta$ $$|u_k(x)-u_k(y)|\leq c\left(\left(\frac{\varepsilon}{c}\right)^4\right)^{\frac{1}{4}}=\varepsilon$$","Let be nonempty, open and bounded with boundary. Let . Let satisfy for some and some with . Let be a sequence with and Show: When converges weakly to in , then My attempt: By using the boundness of weakly convergent sequences, embedding theorems and Arzelà–Ascoli theorem, I obtained a uniformly convergent subsequence (without renaming), whose limit is due to the uniqueness of the limit. This yields: for using the continuity of . What surprises me now is that I didn't use the growth condition of . Can you assist me with this? Any help is greatly appreciated! Edit: Equicontinuity: Morrey yields , so . This implies Choosing : For","\Omega\subset\mathbb{R}^n C^1 p\in[1,n) g\in C(\mathbb{R}) |g(y)|\leq C(1+|y|^q) C<\infty q 1\leq q<p^* (u_k)_k u_k\in W^{1,p}(\Omega) \int_{\Omega}g(u_k(x))dx=0 \text{  for every } k\in\mathbb{N}. u_k u_* W^{1,p}(\Omega) \int_{\Omega}g(u_*(x))dx=0. (u_k)_k u_* \left\lvert \int_{\Omega}g(u_*(x))dx\right\rvert\leq\int_{\Omega}\lvert g(u_k(x))-g(u_*(x))\rvert dx\leq \varepsilon \lvert\Omega\rvert ||u_k-u_*||_C<\delta g g W_0^{1,4}(\Omega)\subset C^{0,\frac{1}{4}}(\Omega) [u_k]_{C^{0,\frac{1}{4}}(\Omega)}\leq c |u_k(x)-u_k(y)|\leq c|x-y|^{\frac{1}{4}}. \delta=(\frac{\varepsilon}{c})^4 |x-y|<\delta |u_k(x)-u_k(y)|\leq c\left(\left(\frac{\varepsilon}{c}\right)^4\right)^{\frac{1}{4}}=\varepsilon","['functional-analysis', 'lebesgue-integral', 'calculus-of-variations', 'constraints', 'arzela-ascoli']"
85,liminf estimate of a integral of weak convergence and almost everywhere convergence,liminf estimate of a integral of weak convergence and almost everywhere convergence,,"I am considering a integral of product of three functions i.e. $$\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx$$ where $f_{\epsilon}\to f$ weakly in $L^{2}(\Omega)$ and $g_{\epsilon}\to g$ almost everywhere  and $0\leq g_{\epsilon}\leq C$ for some $C>0$ and all $\epsilon$ and $\Omega$ is a bounded domain. Moreover, I also know that this integral is uniformly bounded that is $$\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx\leq C$$ for some $C>0$ . What am I expecting is whether can I have some estimate for the limit integral i.e. if I can have $$\int_{\Omega}f^{2}g dx\leq \liminf_{\epsilon} \int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx$$ Many thanks for any help! My attempt is as following: since $g_{\epsilon}$ is uniformly postive, so $$\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx$$ can be viewed as the $L^{1}$ norm of $f_{\epsilon}^{2}g_{\epsilon}$ , then by the weak lower semicontinuous of norm, I can get my estimate. Does this make sense? My another try according to @daw's reply. \begin{align*} \int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx=\int_{\Omega}f_{\epsilon}^{2}gdx+\int_{\Omega}f_{\epsilon}^{2}(g_{\epsilon}-g)dx. \end{align*} First $$\int_{\Omega}f^{2}gdx\leq \liminf \int_{\Omega}f_{\epsilon}^{2}gdx$$ by weakly lower semiconitnuous. Then by Egorov's theorem, for all $\eta>0$ there exists $|\Omega_{\eta}|<\eta$ such that $g_{\epsilon}\to g$ uniformly on $\Omega-\Omega_{\eta}$ . Then \begin{align*} |\int_{\Omega}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|&\leq|\int_{\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|+|\int_{\Omega-\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|. \end{align*} Moreover, the integrablity of $f_{\epsilon}$ and $g_{\epsilon}$ implies the first term goes to zero as $\eta$ goes to zero. For the second term \begin{align*} |\int_{\Omega-\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|&\leq \lVert f_{\epsilon}\rVert_{L^{2}(\Omega)}^{2}\lVert g_{\epsilon}-g\rVert_{L^{\infty}(\Omega-\Omega_{\eta})}\\ &\leq C\lVert g_{\epsilon}-g\rVert_{L^{\infty}(\Omega-\Omega_{\eta})}\to0. \end{align*} So we are done. Does this make more sense? Okay, maybe this estimate is not something I can expect? But what about if I assume that $g_{\epsilon}$ is strictly positive i.e. $0<C_{1}\leq g_{\epsilon}\leq C_{2}$ . From this, I think the bounded implies that $$\lVert \sqrt{g_{\varepsilon}}f_{\varepsilon}\rVert_{2}\leq C$$ Then by the weak lower semicontinous and a.e. convergence, I can get the inequality I want. For this, i think we first take arbitrary $\phi\in L^{2}(\Omega)$ . Then \begin{align*} &|\int_{\Omega}(\sqrt{g_{\epsilon}}f_{\epsilon}-\sqrt{g}f)\phi dx|\\ \leq&|\int_{\Omega}\sqrt{g}(f_{\epsilon}-f)\phi dx|+|\int_{\Omega}(\sqrt{g_{\epsilon}}-\sqrt{g})f_{\epsilon}\phi dx| \end{align*} The first term goes to zero by weak convergence and the second due to dominated convergence theorem. Does this make sense?","I am considering a integral of product of three functions i.e. where weakly in and almost everywhere  and for some and all and is a bounded domain. Moreover, I also know that this integral is uniformly bounded that is for some . What am I expecting is whether can I have some estimate for the limit integral i.e. if I can have Many thanks for any help! My attempt is as following: since is uniformly postive, so can be viewed as the norm of , then by the weak lower semicontinuous of norm, I can get my estimate. Does this make sense? My another try according to @daw's reply. First by weakly lower semiconitnuous. Then by Egorov's theorem, for all there exists such that uniformly on . Then Moreover, the integrablity of and implies the first term goes to zero as goes to zero. For the second term So we are done. Does this make more sense? Okay, maybe this estimate is not something I can expect? But what about if I assume that is strictly positive i.e. . From this, I think the bounded implies that Then by the weak lower semicontinous and a.e. convergence, I can get the inequality I want. For this, i think we first take arbitrary . Then The first term goes to zero by weak convergence and the second due to dominated convergence theorem. Does this make sense?","\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx f_{\epsilon}\to f L^{2}(\Omega) g_{\epsilon}\to g 0\leq g_{\epsilon}\leq C C>0 \epsilon \Omega \int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx\leq C C>0 \int_{\Omega}f^{2}g dx\leq \liminf_{\epsilon} \int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx g_{\epsilon} \int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx L^{1} f_{\epsilon}^{2}g_{\epsilon} \begin{align*}
\int_{\Omega}f_{\epsilon}^{2}g_{\epsilon}dx=\int_{\Omega}f_{\epsilon}^{2}gdx+\int_{\Omega}f_{\epsilon}^{2}(g_{\epsilon}-g)dx.
\end{align*} \int_{\Omega}f^{2}gdx\leq \liminf \int_{\Omega}f_{\epsilon}^{2}gdx \eta>0 |\Omega_{\eta}|<\eta g_{\epsilon}\to g \Omega-\Omega_{\eta} \begin{align*}
|\int_{\Omega}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|&\leq|\int_{\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|+|\int_{\Omega-\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|.
\end{align*} f_{\epsilon} g_{\epsilon} \eta \begin{align*}
|\int_{\Omega-\Omega_{\eta}}f_{\epsilon}^{2}(g_{\epsilon}-g)dx|&\leq \lVert f_{\epsilon}\rVert_{L^{2}(\Omega)}^{2}\lVert g_{\epsilon}-g\rVert_{L^{\infty}(\Omega-\Omega_{\eta})}\\
&\leq C\lVert g_{\epsilon}-g\rVert_{L^{\infty}(\Omega-\Omega_{\eta})}\to0.
\end{align*} g_{\epsilon} 0<C_{1}\leq g_{\epsilon}\leq C_{2} \lVert \sqrt{g_{\varepsilon}}f_{\varepsilon}\rVert_{2}\leq C \phi\in L^{2}(\Omega) \begin{align*}
&|\int_{\Omega}(\sqrt{g_{\epsilon}}f_{\epsilon}-\sqrt{g}f)\phi dx|\\
\leq&|\int_{\Omega}\sqrt{g}(f_{\epsilon}-f)\phi dx|+|\int_{\Omega}(\sqrt{g_{\epsilon}}-\sqrt{g})f_{\epsilon}\phi dx|
\end{align*}","['real-analysis', 'functional-analysis', 'analysis']"
86,"Define $\ell:C[-1,1]\to\mathbb{R}$ such that $\ell(f)=\int_{-1}^0f-\int_0^1 f$ and let $\|f\|=\max_{t\in[-1,1]}|f(t)|$. Is $\|\ell\|=2$?",Define  such that  and let . Is ?,"\ell:C[-1,1]\to\mathbb{R} \ell(f)=\int_{-1}^0f-\int_0^1 f \|f\|=\max_{t\in[-1,1]}|f(t)| \|\ell\|=2","$\newcommand{\R}{\mathbb{R}}$ I'm doing a practice exam for Real Analysis and am wondering about this specific question: Let $C[-1,1]$ be the set of all real-valued continuous functions on $[-1,1]$ . For $f,g\in C[-1,1]$ and $\lambda\in\mathbb{R}$ , define $$(f+g)(t)=f(t)+g(t),\,(\lambda f)(t) = \lambda f(t),\,t\in[-1,1].$$ Then $C[-1,1]$ is a vector space over $\R$ with the given operations. For $f\in C[-1,1]$ , define $$\|f\|=\max\{|f(t)|\,|\, t\in[-1,1]\}.$$ (b) Define $\ell:C[-1,1]\to\R$ by $\ell(f)=\int_{-1}^0 f(t)\,dt-\int_0^1 f(t)\,dt$ for $f\in C[-1,1]$ . Prove that $\ell$ is a bounded linear function and find $\|\ell\|$ . I think that $\|\ell\|=2$ , but I'm not 100% sure and I want to see if I'm right. I got so far that $\|\ell\|\leq 2$ , because if we let $\|f\|=1$ we find $$\begin{align*}             |\ell(f)|&=\left|\int_{-1}^0 f(t)\,dt-\int_0^1 f(t)\,dt\right|\\             &\leq \left|\int_{-1}^0 f(t)\,dt \right|+\left|\int_0^1 f(t)\,dt\right|\\             &\leq \int_{-1}^0 |f(t)|\,dt+\int_0^1 |f(t)|\,dt\\             &\leq \int_{-1}^0 1\,dt+\int_0^1 1\,dt\\             &=2.         \end{align*}$$ I'm thinking that to show $\|\ell\|\geq 2,$ I can use the sequence $\{f_n\}$ in $C[-1,1]$ defined by $$f_n(t)=\begin{cases} 1 & \text{if }-1\leq t\leq -1/n,\\ -nt & \text{if }-1/n<t<1/n,\\ -1 & \text{if }1/n\leq t\leq 1. \end{cases}$$ We find that for each $n$ , $\|f_n\|=1$ . Also, $|\ell(f_n)|=2-1/n$ so that $\lim_{n\to\infty}|\ell(f_n)|=2$ . Does this mean $\|\ell\|\geq 2$ ?","I'm doing a practice exam for Real Analysis and am wondering about this specific question: Let be the set of all real-valued continuous functions on . For and , define Then is a vector space over with the given operations. For , define (b) Define by for . Prove that is a bounded linear function and find . I think that , but I'm not 100% sure and I want to see if I'm right. I got so far that , because if we let we find I'm thinking that to show I can use the sequence in defined by We find that for each , . Also, so that . Does this mean ?","\newcommand{\R}{\mathbb{R}} C[-1,1] [-1,1] f,g\in C[-1,1] \lambda\in\mathbb{R} (f+g)(t)=f(t)+g(t),\,(\lambda f)(t) = \lambda f(t),\,t\in[-1,1]. C[-1,1] \R f\in C[-1,1] \|f\|=\max\{|f(t)|\,|\, t\in[-1,1]\}. \ell:C[-1,1]\to\R \ell(f)=\int_{-1}^0 f(t)\,dt-\int_0^1 f(t)\,dt f\in C[-1,1] \ell \|\ell\| \|\ell\|=2 \|\ell\|\leq 2 \|f\|=1 \begin{align*}
            |\ell(f)|&=\left|\int_{-1}^0 f(t)\,dt-\int_0^1 f(t)\,dt\right|\\
            &\leq \left|\int_{-1}^0 f(t)\,dt \right|+\left|\int_0^1 f(t)\,dt\right|\\
            &\leq \int_{-1}^0 |f(t)|\,dt+\int_0^1 |f(t)|\,dt\\
            &\leq \int_{-1}^0 1\,dt+\int_0^1 1\,dt\\
            &=2.
        \end{align*} \|\ell\|\geq 2, \{f_n\} C[-1,1] f_n(t)=\begin{cases} 1 & \text{if }-1\leq t\leq -1/n,\\
-nt & \text{if }-1/n<t<1/n,\\
-1 & \text{if }1/n\leq t\leq 1.
\end{cases} n \|f_n\|=1 |\ell(f_n)|=2-1/n \lim_{n\to\infty}|\ell(f_n)|=2 \|\ell\|\geq 2","['real-analysis', 'functional-analysis', 'linear-transformations', 'normed-spaces']"
87,Norm of elementary tensors in projective tensor product of Banach spaces,Norm of elementary tensors in projective tensor product of Banach spaces,,"Let $E$ and $F$ be Banach spaces. We define the projective norm of the elementary tensor product of $E\otimes F$ as follows- For $t\in E\otimes F$ , define $\lVert t\rVert_{\wedge} =\text{inf}\{\sum\limits_{i=1}^n \lVert a_i\rVert\lVert b_i\rVert:\ t=\sum\limits_{i=1}^n a_i\otimes b_i, a_i\in E, b_i\in F\}$ .Now take the completion with respect to this norm and call it $E\hat\otimes F$ . I have to prove that $\lVert a\otimes b\rVert_{\wedge}=\lVert a\rVert\lVert b\rVert$ . It's obvious from the definition that $\lVert a\otimes b\rVert_{\wedge}\le\lVert a\rVert\lVert b\rVert$ Now I have to show the converse inequality i.e. if $a\otimes b=\sum\limits_{i=1}^n a_i\otimes b_i$ , we must have $\lVert a\rVert\lVert b\rVert\le \sum\limits_{i=1}^n  \lVert a_i\rVert\lVert b_i\rVert$ Can anyone suggest me a hint to prove this? Thanks for your assistance in advance.","Let and be Banach spaces. We define the projective norm of the elementary tensor product of as follows- For , define .Now take the completion with respect to this norm and call it . I have to prove that . It's obvious from the definition that Now I have to show the converse inequality i.e. if , we must have Can anyone suggest me a hint to prove this? Thanks for your assistance in advance.","E F E\otimes F t\in E\otimes F \lVert t\rVert_{\wedge} =\text{inf}\{\sum\limits_{i=1}^n \lVert a_i\rVert\lVert b_i\rVert:\ t=\sum\limits_{i=1}^n a_i\otimes b_i, a_i\in E, b_i\in F\} E\hat\otimes F \lVert a\otimes b\rVert_{\wedge}=\lVert a\rVert\lVert b\rVert \lVert a\otimes b\rVert_{\wedge}\le\lVert a\rVert\lVert b\rVert a\otimes b=\sum\limits_{i=1}^n a_i\otimes b_i \lVert a\rVert\lVert b\rVert\le \sum\limits_{i=1}^n  \lVert a_i\rVert\lVert b_i\rVert","['functional-analysis', 'banach-spaces', 'tensor-products', 'operator-algebras']"
88,Show by induction that a function is bounded,Show by induction that a function is bounded,,"My question is about the proof of the corollary 3.3 page 21 of this paper . Let $(E,d,\mu)$ be a measured metric space with $\mu$ doubling. We write $\mathcal{M}$ the Hardy-Littlewood maximal operator. Let $1<q\leq\infty$ and $a>1$ be fixed. Let $F,G\in L^1_{loc}(E)$ be non-negative. Definition : We says $(F,G)\in\mathcal{E}_{q,a}$ if for every ball $B$ we can find non-negative measurable functions $G_B, H_B$ defined on $B$ with $$ F  \leq G_B + H_B ~ \text{a.e. on}\, B $$ such that $$ \begin{split}  \left( \frac{1}{\mu(B)}\int_B (H_B)^q\, d\mu \right)^{1/q} &\leq a  \inf_{x\in B} \mathcal{M} F(x) + \inf_{x\in B} G(x), \\  \frac{1}{\mu(B)}\int_B G_B\, d\mu &\leq \inf_{x\in B} G(x).  \end{split} $$ Let $1<\rho$ and let $F,G\in\mathcal{E}_{q,a}$ such that $\|F\|_1<\infty$ and $\|G\|_\rho<\infty$ . In this proof we define a function $$ \Phi(t) = \rho\int_0^t \lambda^{\rho-1}\mu\{\mathcal{M}F>\lambda\}\,d\lambda $$ for $t\geq 0$ . Also after using a precedent proposition we show the innequality $$ \Phi(Kt) \leq \frac{1}{2} \Phi(t) + \left( \frac{K}{\gamma} \right)^\rho \lVert G \rVert_\rho^\rho $$ where $K$ and $\gamma\leq 1$ are some constants. And then the author says that ""An easy iteration shows that $\Phi$ is bounded"". I don't see what kind of iteration we have to do.","My question is about the proof of the corollary 3.3 page 21 of this paper . Let be a measured metric space with doubling. We write the Hardy-Littlewood maximal operator. Let and be fixed. Let be non-negative. Definition : We says if for every ball we can find non-negative measurable functions defined on with such that Let and let such that and . In this proof we define a function for . Also after using a precedent proposition we show the innequality where and are some constants. And then the author says that ""An easy iteration shows that is bounded"". I don't see what kind of iteration we have to do.","(E,d,\mu) \mu \mathcal{M} 1<q\leq\infty a>1 F,G\in L^1_{loc}(E) (F,G)\in\mathcal{E}_{q,a} B G_B, H_B B  F
 \leq G_B + H_B ~ \text{a.e. on}\, B   \begin{split}
 \left( \frac{1}{\mu(B)}\int_B (H_B)^q\, d\mu \right)^{1/q} &\leq a
 \inf_{x\in B} \mathcal{M} F(x) + \inf_{x\in B} G(x), \\
 \frac{1}{\mu(B)}\int_B G_B\, d\mu &\leq \inf_{x\in B} G(x).
 \end{split}  1<\rho F,G\in\mathcal{E}_{q,a} \|F\|_1<\infty \|G\|_\rho<\infty 
\Phi(t) = \rho\int_0^t \lambda^{\rho-1}\mu\{\mathcal{M}F>\lambda\}\,d\lambda
 t\geq 0 
\Phi(Kt) \leq
\frac{1}{2} \Phi(t) + \left( \frac{K}{\gamma} \right)^\rho \lVert G \rVert_\rho^\rho
 K \gamma\leq 1 \Phi","['real-analysis', 'functional-analysis', 'proof-explanation', 'induction']"
89,Does the weak convergence of probability measures imply some uniform over all sets estimates between these measures and the limit measure?,Does the weak convergence of probability measures imply some uniform over all sets estimates between these measures and the limit measure?,,"Let $X$ be a metric space. If it is necessary, one can assume that it is complete and separable, or even compact. Consider a sequence of Borel probability measures $(\mu_n)$ on $X$ that weakly converges to a Borel probability measure $\mu$ on $X$ . It is well known that there are equivalent characterizations for this convergence, namely, that \begin{equation*} \limsup\limits_{n\to +\infty} \mu_n(F)\leq \mu(F) \end{equation*} for every closed set $F\subseteq X$ and that \begin{equation*} \liminf\limits_{n\to +\infty} \mu_n(G)\geq \mu(G) \end{equation*} for every open set $G\subseteq X$ . My question, roughly speaking, if these estimates, after passing to a subsequence of $(\mu_n)$ , can be made uniform over all Borel subsets of $X$ ? Let me explain what I mean. Suppose that $\mu$ is fully supported on $X$ , that is, $\mu(G)>0$ for every nonempty open set $G\subseteq X$ , fix $\varepsilon>0$ . Can I find a subsequence $(\mu_{n_k})$ such that for any Borel set $V\subseteq \mathrm{X}$ and any $k\in \mathbb{N}$ one has \begin{equation*} \mu_{n_k}(V)\leq (1+\varepsilon)\mu(V)? \end{equation*} If $\mu$ is not fully supported, one can consider arbitrary fully supported probability measure $\nu $ on $X$ and replace $(1+\varepsilon) \mu(V)$ with $\mu(V)+\varepsilon \nu(V)$ in the expression above. Are there some sufficient conditions for this property to hold? Or it fails dramatically? The only thoughts on this I have are as follows. Suppose we are given a countable family $\mathcal{V}$ of closed subsets of $X$ such that $\mu(F)>0$ for each $F\in \mathcal{V}$ . As it seems to me, standard Cantor's diagonal argument and the equivalent characterization of the weak convergence above imply that there is a subsequence $(\mu_{n_k})$ such that the desired property holds for all sets from $\mathcal{V}$ . Explicitly, \begin{equation*} \mu_{n_k}(F)\leq (1+\varepsilon)\mu(F) \end{equation*} for every $F\in \mathcal{V}$ and for every $k\in \mathbb{N}$ . Does this imply the inequality for each Borel subset of $X$ , if we take an appropriate family $\mathcal{V}$ , for instance, a generating set for the Borel sigma-algebra on $X$ in the case of its countable generatedness? Or there are some other arguments? Will be greatful for any help!","Let be a metric space. If it is necessary, one can assume that it is complete and separable, or even compact. Consider a sequence of Borel probability measures on that weakly converges to a Borel probability measure on . It is well known that there are equivalent characterizations for this convergence, namely, that for every closed set and that for every open set . My question, roughly speaking, if these estimates, after passing to a subsequence of , can be made uniform over all Borel subsets of ? Let me explain what I mean. Suppose that is fully supported on , that is, for every nonempty open set , fix . Can I find a subsequence such that for any Borel set and any one has If is not fully supported, one can consider arbitrary fully supported probability measure on and replace with in the expression above. Are there some sufficient conditions for this property to hold? Or it fails dramatically? The only thoughts on this I have are as follows. Suppose we are given a countable family of closed subsets of such that for each . As it seems to me, standard Cantor's diagonal argument and the equivalent characterization of the weak convergence above imply that there is a subsequence such that the desired property holds for all sets from . Explicitly, for every and for every . Does this imply the inequality for each Borel subset of , if we take an appropriate family , for instance, a generating set for the Borel sigma-algebra on in the case of its countable generatedness? Or there are some other arguments? Will be greatful for any help!","X (\mu_n) X \mu X \begin{equation*}
\limsup\limits_{n\to +\infty} \mu_n(F)\leq \mu(F)
\end{equation*} F\subseteq X \begin{equation*}
\liminf\limits_{n\to +\infty} \mu_n(G)\geq \mu(G)
\end{equation*} G\subseteq X (\mu_n) X \mu X \mu(G)>0 G\subseteq X \varepsilon>0 (\mu_{n_k}) V\subseteq \mathrm{X} k\in \mathbb{N} \begin{equation*}
\mu_{n_k}(V)\leq (1+\varepsilon)\mu(V)?
\end{equation*} \mu \nu  X (1+\varepsilon) \mu(V) \mu(V)+\varepsilon \nu(V) \mathcal{V} X \mu(F)>0 F\in \mathcal{V} (\mu_{n_k}) \mathcal{V} \begin{equation*}
\mu_{n_k}(F)\leq (1+\varepsilon)\mu(F)
\end{equation*} F\in \mathcal{V} k\in \mathbb{N} X \mathcal{V} X","['functional-analysis', 'measure-theory', 'weak-convergence', 'borel-measures']"
90,Source and/or detailed proof for $\exp(\lambda (A + B)) = \exp(\lambda A)\exp(\lambda B)$ for commuting elements in a Banach algebra,Source and/or detailed proof for  for commuting elements in a Banach algebra,\exp(\lambda (A + B)) = \exp(\lambda A)\exp(\lambda B),"Every time I need to revisit the proof that $\exp(\lambda (A + B)) = \exp(\lambda A)\exp(\lambda B), \lambda \in\mathbb{K}$ for commuting elements in a Banach algebra, I find myself struggling to remember the key details. I can recall certain aspects, such as why the series converges in norm (which justifies the whole equality), but I have difficulty recalling the exact justification for other details. For instance, I can't seem to remember why we can ""just permute"" the order of the double series $\displaystyle \sum_{n=0}^{\infty}\sum_{k=0}^{n}\frac{(\lambda A)^k}{k!}\frac{(\lambda B)^{n-k}}{(n-k)!}$ . As dull and uninteresting it may sound, I am looking for a proof which is as explicit as possible without being ""too tedious"". That is, I would like to see a treatment of this proof with indicator functions and the such to make life easier. Additionally, I currently can't recall any written sources that discuss this theorem. Therefore I am asking either for a reference to a book discussing the prior claim in detail (including why it makes sense to permute the two series; let the convergence theorems from measure theory be allowed if that makes it any easier) or a proof of the said claim, in detail. Any help would be greatly appreciated. Thank you in advance!","Every time I need to revisit the proof that for commuting elements in a Banach algebra, I find myself struggling to remember the key details. I can recall certain aspects, such as why the series converges in norm (which justifies the whole equality), but I have difficulty recalling the exact justification for other details. For instance, I can't seem to remember why we can ""just permute"" the order of the double series . As dull and uninteresting it may sound, I am looking for a proof which is as explicit as possible without being ""too tedious"". That is, I would like to see a treatment of this proof with indicator functions and the such to make life easier. Additionally, I currently can't recall any written sources that discuss this theorem. Therefore I am asking either for a reference to a book discussing the prior claim in detail (including why it makes sense to permute the two series; let the convergence theorems from measure theory be allowed if that makes it any easier) or a proof of the said claim, in detail. Any help would be greatly appreciated. Thank you in advance!","\exp(\lambda (A + B)) = \exp(\lambda A)\exp(\lambda B), \lambda \in\mathbb{K} \displaystyle \sum_{n=0}^{\infty}\sum_{k=0}^{n}\frac{(\lambda A)^k}{k!}\frac{(\lambda B)^{n-k}}{(n-k)!}","['functional-analysis', 'reference-request', 'exponential-function', 'banach-spaces', 'banach-algebras']"
91,Linear map of unitization of a non-unital $C^\ast$-algebra,Linear map of unitization of a non-unital -algebra,C^\ast,"I recently discussed unitization of a non-unital $C^\ast$ -algebra. I proved some facts and so on (which I afterwards found on the internet). Anyway let's consider the following $C^\ast$ -algebra. That is, the $C^\ast$ -algebra $\mathcal{A}$ with norm $\|\cdot\|$ . Let $\tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C}$ as a vector space. We endow it with multiplication and involution, $$(a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu)$$ and $$(a,\lambda)^\ast:=(a^\ast,\bar{\lambda})$$ As mentioned I did some proofs of this and discussed with fellow members of this site. However I can see people discuss norm of this beast, let me clarify this. We consider $\omega:\mathcal{A}\rightarrow\tilde{\mathcal{A}}$ by the map $\omega(a):=(a,0)$ . This is indeed a two sided ideal $\omega(\mathcal{A})$ and of course also a $\ast$ -homomorphism. Now we will suppress the inclusion $\omega$ and think of $\mathcal{A}$ as an ideal in $\mathcal{A}$ . I claim: Take $x\in\tilde{\mathcal{A}}$ then we consider the linear map $\tilde{L}_x:\tilde{\mathcal{A}}\rightarrow \tilde{\mathcal{A}}$ by $y\mapsto xy$ restricts to a map, $L_x:\mathcal{A}\rightarrow \mathcal{A}$ which is bounded with $\|L_x\|_\infty \leq \|x\|_1$ . I want to show the above claim. Here we know that the one-norm is given by $x=(a,\lambda)\in \tilde{\mathcal{A}}$ by $\|x\|_1=\|a\|+|\lambda|$ and of course $\|\cdot\|_\infty$ denote the operator norm. But how can we construct such a proof of this claim? I've tried to read more about this kind of unitization and norms on the following link however I don't think they do the claim that I give. http://www.math.nagoya-u.ac.jp/~richard/teaching/s2020/Terasawa.pdf So, how would one do it?","I recently discussed unitization of a non-unital -algebra. I proved some facts and so on (which I afterwards found on the internet). Anyway let's consider the following -algebra. That is, the -algebra with norm . Let as a vector space. We endow it with multiplication and involution, and As mentioned I did some proofs of this and discussed with fellow members of this site. However I can see people discuss norm of this beast, let me clarify this. We consider by the map . This is indeed a two sided ideal and of course also a -homomorphism. Now we will suppress the inclusion and think of as an ideal in . I claim: Take then we consider the linear map by restricts to a map, which is bounded with . I want to show the above claim. Here we know that the one-norm is given by by and of course denote the operator norm. But how can we construct such a proof of this claim? I've tried to read more about this kind of unitization and norms on the following link however I don't think they do the claim that I give. http://www.math.nagoya-u.ac.jp/~richard/teaching/s2020/Terasawa.pdf So, how would one do it?","C^\ast C^\ast C^\ast \mathcal{A} \|\cdot\| \tilde{\mathcal{A}}=\mathcal{A}\oplus \mathbb{C} (a,\lambda)\cdot (b,\mu):=(ab+\lambda b+\mu a,\lambda \mu) (a,\lambda)^\ast:=(a^\ast,\bar{\lambda}) \omega:\mathcal{A}\rightarrow\tilde{\mathcal{A}} \omega(a):=(a,0) \omega(\mathcal{A}) \ast \omega \mathcal{A} \mathcal{A} x\in\tilde{\mathcal{A}} \tilde{L}_x:\tilde{\mathcal{A}}\rightarrow \tilde{\mathcal{A}} y\mapsto xy L_x:\mathcal{A}\rightarrow \mathcal{A} \|L_x\|_\infty \leq \|x\|_1 x=(a,\lambda)\in \tilde{\mathcal{A}} \|x\|_1=\|a\|+|\lambda| \|\cdot\|_\infty","['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras']"
92,what is bridge between symbolic notation of a generalized function and functional notation of the same generalized function？,what is bridge between symbolic notation of a generalized function and functional notation of the same generalized function？,,"Generalized function theory bothers me from time to time, I used to put aside and not delve into it, but this time I encountered it again with annoyance and wanted to understand it to some extent. Below is my current core questions. Generalized functions are continuous linear functionals defined on test functions space with compact support, which have two notations, symbolic(eg 𝛿(x)) and functional(eg 𝛿[φ]). The pdf( https://www.cs.odu.edu/~mln/ltrs-pdfs/tp3428.pdf ) I have read do not elaborate on this point further, and I have the following questions: Can any given continuous linear functional on D give a symbolic notation of it? If yes, how to give? What is the general bridge between the symbolic notation and the functional notation? For common operations such as multiplication, differentiation, limit, integral, etc. what are the correspondences/relations between these two notation-systems? For example, if I derive the symbolic (functional) form, what is derivatives of the functional (symbolic) form? why generalized function always equal to the generalized derivatives of an ordinary function? can you give me some easy examples to explain it? I'm not a math-majored student, so I'm not greedy to understand from underlying, I just want to master operations of generalized functions and understand it from relations of the two notation-systems.","Generalized function theory bothers me from time to time, I used to put aside and not delve into it, but this time I encountered it again with annoyance and wanted to understand it to some extent. Below is my current core questions. Generalized functions are continuous linear functionals defined on test functions space with compact support, which have two notations, symbolic(eg 𝛿(x)) and functional(eg 𝛿[φ]). The pdf( https://www.cs.odu.edu/~mln/ltrs-pdfs/tp3428.pdf ) I have read do not elaborate on this point further, and I have the following questions: Can any given continuous linear functional on D give a symbolic notation of it? If yes, how to give? What is the general bridge between the symbolic notation and the functional notation? For common operations such as multiplication, differentiation, limit, integral, etc. what are the correspondences/relations between these two notation-systems? For example, if I derive the symbolic (functional) form, what is derivatives of the functional (symbolic) form? why generalized function always equal to the generalized derivatives of an ordinary function? can you give me some easy examples to explain it? I'm not a math-majored student, so I'm not greedy to understand from underlying, I just want to master operations of generalized functions and understand it from relations of the two notation-systems.",,"['real-analysis', 'functional-analysis', 'dirac-delta']"
93,Precise assumption in spectral theorem of unbounded operators,Precise assumption in spectral theorem of unbounded operators,,"The most general version of the spectral theorem I am aware of is the spectral theorem for unbounded normal operators (firstly proven by von Neumann in 1932, I think). An operator $T:\mathcal{D}(T)\to\mathcal{H}$ in some Hilbert space $\mathcal{H}$ is called normal, if $$TT^{\ast}=T^{\ast}T.$$ Note that this is an equlity on the level of operators, which means that we require that $$\mathrm{D}(TT^{\ast})=\{\psi\in\mathcal{D}(T^{\ast})\mid T\psi\in\mathcal{D}(T)\}\stackrel{!}{=}\{\psi\in\mathcal{D}(T)\mid T\psi\in\mathcal{D}(T^{\ast})\}=\mathcal{D}(T^{\ast}T)$$ as well as $$TT^{\ast}\psi=T^{\ast}T\psi,\hspace{2cm}\forall \psi\in\mathcal{D}(TT^{\ast}).$$ Roughly speaking, the spectral theorem for general normal (possibly unbounded) operators states in its measure-theoretic formulation the following: Let $T$ be a normal operator $T:\mathcal{D}(T)\to\mathcal{H}$ . Then there exists a unique spectral measure $P:\mathcal{B}(\sigma(T))\to\mathcal{B}(\mathcal{H})$ , where $\mathcal{B}(\sigma(T))$ denotes the Borel $\sigma$ -algebra on the spectrum $\sigma(T)$ and $\mathcal{B}(\mathcal{H})$ the set of bounded operators on $\mathcal{H}$ , such that $$\mathcal{D}(T)=\bigg\{\psi\in\mathcal{H}\,\bigg\vert\,\int_{\sigma(T)}\,\vert\lambda\vert^{2}\,\mathrm{d}\langle\psi,P_{\lambda}\psi\rangle\bigg\}$$ and $$T=\int_{\sigma(T)}\,\lambda\,\mathrm{d}P_{\lambda}.$$ Now, unfortunately, it is quite hard to find a discussion of the spectral theorem in this general version in the literature and hence, I am unsure about the precise requirements: In particular, I have the following short questions: Does one have to assume separability of the Hilbert space $\mathcal{H}$ ? Shouldn't one assume more precisely that $T$ is closed and densley-defined? As far as I know, there is a theorem stating that if $T$ is densely-defined and closed, then $TT^{\ast}$ is itself densely-defined and self-adjoint and I guess that this is what we need in order to proof the spectral theorem. Is there any other requirement for the theorem to hold? Does one have a good literature, which treats the spectral theorem in this level of generality?","The most general version of the spectral theorem I am aware of is the spectral theorem for unbounded normal operators (firstly proven by von Neumann in 1932, I think). An operator in some Hilbert space is called normal, if Note that this is an equlity on the level of operators, which means that we require that as well as Roughly speaking, the spectral theorem for general normal (possibly unbounded) operators states in its measure-theoretic formulation the following: Let be a normal operator . Then there exists a unique spectral measure , where denotes the Borel -algebra on the spectrum and the set of bounded operators on , such that and Now, unfortunately, it is quite hard to find a discussion of the spectral theorem in this general version in the literature and hence, I am unsure about the precise requirements: In particular, I have the following short questions: Does one have to assume separability of the Hilbert space ? Shouldn't one assume more precisely that is closed and densley-defined? As far as I know, there is a theorem stating that if is densely-defined and closed, then is itself densely-defined and self-adjoint and I guess that this is what we need in order to proof the spectral theorem. Is there any other requirement for the theorem to hold? Does one have a good literature, which treats the spectral theorem in this level of generality?","T:\mathcal{D}(T)\to\mathcal{H} \mathcal{H} TT^{\ast}=T^{\ast}T. \mathrm{D}(TT^{\ast})=\{\psi\in\mathcal{D}(T^{\ast})\mid T\psi\in\mathcal{D}(T)\}\stackrel{!}{=}\{\psi\in\mathcal{D}(T)\mid T\psi\in\mathcal{D}(T^{\ast})\}=\mathcal{D}(T^{\ast}T) TT^{\ast}\psi=T^{\ast}T\psi,\hspace{2cm}\forall \psi\in\mathcal{D}(TT^{\ast}). T T:\mathcal{D}(T)\to\mathcal{H} P:\mathcal{B}(\sigma(T))\to\mathcal{B}(\mathcal{H}) \mathcal{B}(\sigma(T)) \sigma \sigma(T) \mathcal{B}(\mathcal{H}) \mathcal{H} \mathcal{D}(T)=\bigg\{\psi\in\mathcal{H}\,\bigg\vert\,\int_{\sigma(T)}\,\vert\lambda\vert^{2}\,\mathrm{d}\langle\psi,P_{\lambda}\psi\rangle\bigg\} T=\int_{\sigma(T)}\,\lambda\,\mathrm{d}P_{\lambda}. \mathcal{H} T T TT^{\ast}","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'spectral-theory', 'unbounded-operators']"
94,Intuitively explain why $U: X \rightarrow X$ is invertible if it is close enough to the identity operator.,Intuitively explain why  is invertible if it is close enough to the identity operator.,U: X \rightarrow X,"Let $X$ be a Banach space. The Neumann theorem states that an operator $U: X \rightarrow X$ is invertible if it is close enough to the identity operator. This is the theorem. If $U: X \rightarrow X$ is bounded and $\|I-U\|<1$ , then $U$ is invertible, and $$ U^{-1}=\sum_{k=0}^{\infty}(I-U)^k $$ Furthermore, $$ \left\|U^{-1}\right\| \leq \frac{1}{1-\|I-U\|} $$ I'm not interested in the proof of this theorem, but why $I$ is so important for it? I mean, can I (for example) substitute the operator $I$ with another one and then obtain a similar theorem?","Let be a Banach space. The Neumann theorem states that an operator is invertible if it is close enough to the identity operator. This is the theorem. If is bounded and , then is invertible, and Furthermore, I'm not interested in the proof of this theorem, but why is so important for it? I mean, can I (for example) substitute the operator with another one and then obtain a similar theorem?","X U: X \rightarrow X U: X \rightarrow X \|I-U\|<1 U 
U^{-1}=\sum_{k=0}^{\infty}(I-U)^k
 
\left\|U^{-1}\right\| \leq \frac{1}{1-\|I-U\|}
 I I","['functional-analysis', 'operator-theory', 'banach-spaces']"
95,Is the borel sigma algebra defined on the unit circle same as the borel algebra defined using the subspace topology?,Is the borel sigma algebra defined on the unit circle same as the borel algebra defined using the subspace topology?,,"This is from Sheldon Axler's text on Measure and integration: So we have defined the borel sigma algebra on the unit disk to be the pullback of the sigma algebra on $(-\pi, \pi]$ . However, the unit disk itself has a topology defined as the subspace topology on the complex plane, so if we take those open sets and generate the Borel sigma algebra from that, do we get the same Borel algebra as the pullback? Also, it was not explicitly stated how we define a continuous function on the unit disk, but I assume it is defined using the subspace topology on the unit disk, so if that is the case, then these two different sigma algebras should coincide right?","This is from Sheldon Axler's text on Measure and integration: So we have defined the borel sigma algebra on the unit disk to be the pullback of the sigma algebra on . However, the unit disk itself has a topology defined as the subspace topology on the complex plane, so if we take those open sets and generate the Borel sigma algebra from that, do we get the same Borel algebra as the pullback? Also, it was not explicitly stated how we define a continuous function on the unit disk, but I assume it is defined using the subspace topology on the unit disk, so if that is the case, then these two different sigma algebras should coincide right?","(-\pi, \pi]","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory']"
96,$I-T : L^p(\mathbb R) \rightarrow L^p(\mathbb R)$ is not surjective,is not surjective,I-T : L^p(\mathbb R) \rightarrow L^p(\mathbb R),"Let $p > 1$ . Define $T : L^p(\mathbb R) \rightarrow L^p(\mathbb R)$ by $Tf(x) := \int_0^1 f(x+y)\,dy$ . Prove that $I-T$ is not surjective, where $I : L^p(\mathbb R) \rightarrow L^p(\mathbb R)$ is the identity map. Using Jensen's inequality, one can show that $\|Tf\|_p \leq \|f\|_p$ , so in particular, $T$ is well-defined, bounded linear operator with $\|T\| \leq 1$ . Beyond this, I don't have much idea how to proceed. I would appreciate any hint or reference.","Let . Define by . Prove that is not surjective, where is the identity map. Using Jensen's inequality, one can show that , so in particular, is well-defined, bounded linear operator with . Beyond this, I don't have much idea how to proceed. I would appreciate any hint or reference.","p > 1 T : L^p(\mathbb R) \rightarrow L^p(\mathbb R) Tf(x) := \int_0^1 f(x+y)\,dy I-T I : L^p(\mathbb R) \rightarrow L^p(\mathbb R) \|Tf\|_p \leq \|f\|_p T \|T\| \leq 1","['real-analysis', 'functional-analysis', 'lp-spaces']"
97,"If A is a $C^*$ algebra, then there is a $C^*$ algebra $A_1$ with an identity such that $A_1$ contains A as an ideal.","If A is a  algebra, then there is a  algebra  with an identity such that  contains A as an ideal.",C^* C^* A_1 A_1,"This is in the book Functional analysis by John B. Conway. Chapter 9 prop: 1.9. Basically in the book, he takes $A_1=\{a+\alpha \}$ where $(a+\alpha )$ is just a formal sum. And defined multiplication and addition as: i) $(a+\alpha )(b+\beta )=(ab+\alpha b+\beta a+\alpha \beta )$ ii) $(a+\alpha )+(b+\beta )=(a+b)+(\alpha +\beta )$ And norm as: $$\|a+\alpha \|=\sup \{ \|ax+\alpha x\|:x\in A,\|x\|\leq 1\}.$$ I am unable to prove that $A_1$ is a Banach algebra. I am trying to show if $\|a+\alpha \|=\|a\|+|\alpha |$ or not but unable to do so.","This is in the book Functional analysis by John B. Conway. Chapter 9 prop: 1.9. Basically in the book, he takes where is just a formal sum. And defined multiplication and addition as: i) ii) And norm as: I am unable to prove that is a Banach algebra. I am trying to show if or not but unable to do so.","A_1=\{a+\alpha \} (a+\alpha ) (a+\alpha )(b+\beta )=(ab+\alpha b+\beta a+\alpha \beta ) (a+\alpha )+(b+\beta )=(a+b)+(\alpha +\beta ) \|a+\alpha \|=\sup \{ \|ax+\alpha x\|:x\in A,\|x\|\leq 1\}. A_1 \|a+\alpha \|=\|a\|+|\alpha |","['real-analysis', 'functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras']"
98,Are these spaces of continuous functions equivalent?,Are these spaces of continuous functions equivalent?,,"I'm wondering if $C([0,T]\times\mathbb{R}^n)$ and $C([0,T];C(\mathbb{R}^n))$ are equivalent. I know I can show $C([0,T];C(\mathbb{R}^n))\hookrightarrow C([0,T]\times\mathbb{R}^n)$ through $$|f(t,x)|\leq |f(t,x)-f(t',x)|+|f(t',x)|$$ by using uniform continuity in $t$ to $C(\mathbb{R}^n)$ . However, when going the other direction, I know for fixed $x\in\mathbb{R}^n$ , $f(t,x)$ is uniformly continuous in time, but I don't know if this uniform modulus of continuity is uniform over $x$ as it would be in $C([0,T];C(\mathbb{R}^n))$ . Edit since I can't comment: $C(\mathbb{R}^n)$ is the set of continuous, bounded functions from $\mathbb{R}^n$ to $\mathbb{R}$ .","I'm wondering if and are equivalent. I know I can show through by using uniform continuity in to . However, when going the other direction, I know for fixed , is uniformly continuous in time, but I don't know if this uniform modulus of continuity is uniform over as it would be in . Edit since I can't comment: is the set of continuous, bounded functions from to .","C([0,T]\times\mathbb{R}^n) C([0,T];C(\mathbb{R}^n)) C([0,T];C(\mathbb{R}^n))\hookrightarrow C([0,T]\times\mathbb{R}^n) |f(t,x)|\leq |f(t,x)-f(t',x)|+|f(t',x)| t C(\mathbb{R}^n) x\in\mathbb{R}^n f(t,x) x C([0,T];C(\mathbb{R}^n)) C(\mathbb{R}^n) \mathbb{R}^n \mathbb{R}","['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
99,"Weak* separability of dual unit ball of D[0,1]","Weak* separability of dual unit ball of D[0,1]",,"Let $D[0,1]$ be the space of all right-continuous left-limited functions $f\colon [0,1]\to \mathbb{R}$ equipped with the supremum norm $f\mapsto \|f\|_\infty = \sup_{t\in[0,1]} |f(t)|$ . This is a non-separable Banach space whose dual $D[0,1]^\ast$ is known to be separable in the weak* topology; see, e.g., Chapter 41, p. 1756 of Johnson, W. B. (ed.); Lindenstrauss, J. (ed.) , Handbook of the geometry of Banach spaces. Volume 2, Amsterdam: North-Holland. xii, 1007-1866 (2003). ZBL1013.46001 . Is the unit ball in $D[0,1]^\ast$ separable in the weak* topology?","Let be the space of all right-continuous left-limited functions equipped with the supremum norm . This is a non-separable Banach space whose dual is known to be separable in the weak* topology; see, e.g., Chapter 41, p. 1756 of Johnson, W. B. (ed.); Lindenstrauss, J. (ed.) , Handbook of the geometry of Banach spaces. Volume 2, Amsterdam: North-Holland. xii, 1007-1866 (2003). ZBL1013.46001 . Is the unit ball in separable in the weak* topology?","D[0,1] f\colon [0,1]\to \mathbb{R} f\mapsto \|f\|_\infty = \sup_{t\in[0,1]} |f(t)| D[0,1]^\ast D[0,1]^\ast","['functional-analysis', 'banach-spaces', 'weak-topology']"
