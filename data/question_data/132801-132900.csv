,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How do I find Green Function for this BVP,How do I find Green Function for this BVP,,"I saw this question: Find the Green function for the problem: $$y''(x)+y(x) = h(x)$$ $$y(0)=y(\pi), y'(0)=y'(\pi)$$ My attempt: First I should consider the homogeneous case, in that case: $y''=0 \Longrightarrow y=c_1x+c_2$ $y(0)=y(\pi) \Longrightarrow c_2 = c_1\pi+c_2 \Longrightarrow c_1=0$ So for the first boundary condition I have: $y_1=c_2$ $??$ $y'(0)=y'(\pi) \Longrightarrow c_1=c_1$ It seems like from the second boundary condition I do not get any information about $y_2$ which would help me to calculate the Wronskian and so on. Also I am not sure $y_1$. Could you please help me?","I saw this question: Find the Green function for the problem: $$y''(x)+y(x) = h(x)$$ $$y(0)=y(\pi), y'(0)=y'(\pi)$$ My attempt: First I should consider the homogeneous case, in that case: $y''=0 \Longrightarrow y=c_1x+c_2$ $y(0)=y(\pi) \Longrightarrow c_2 = c_1\pi+c_2 \Longrightarrow c_1=0$ So for the first boundary condition I have: $y_1=c_2$ $??$ $y'(0)=y'(\pi) \Longrightarrow c_1=c_1$ It seems like from the second boundary condition I do not get any information about $y_2$ which would help me to calculate the Wronskian and so on. Also I am not sure $y_1$. Could you please help me?",,"['ordinary-differential-equations', 'greens-function']"
1,Repeated root case for ay''+by'+cy=0,Repeated root case for ay''+by'+cy=0,,"To solve the ODE $a y''(t)+b y'(t) +c y(t) = 0$, where $a,b,c$ are constant, we solve the characteristic equation $ar^{2}+br+c=0$. In the case when the roots are two repeated roots, i.e,. $r=r_{1}=r_{2}$, we get two linearly independent solutions $y_{1}(t)=e^{r_{1}t}$ and $y_{2}(t)= t e^{r_{1}}$. I understand that the solution $y_{2}(t)= t e^{r_{1}}$ works and there are several approaches to get this solution, as mentioned in a previous post , and in the textbook by Boyce and DiPrima also shows three approaches, see: . I do not have questions about these approaches. My question is, all these approaches require some computation. Is there any way to show $t e^{r_{1}}$ is a solution, without any computation? Thanks in advance for any insights!","To solve the ODE $a y''(t)+b y'(t) +c y(t) = 0$, where $a,b,c$ are constant, we solve the characteristic equation $ar^{2}+br+c=0$. In the case when the roots are two repeated roots, i.e,. $r=r_{1}=r_{2}$, we get two linearly independent solutions $y_{1}(t)=e^{r_{1}t}$ and $y_{2}(t)= t e^{r_{1}}$. I understand that the solution $y_{2}(t)= t e^{r_{1}}$ works and there are several approaches to get this solution, as mentioned in a previous post , and in the textbook by Boyce and DiPrima also shows three approaches, see: . I do not have questions about these approaches. My question is, all these approaches require some computation. Is there any way to show $t e^{r_{1}}$ is a solution, without any computation? Thanks in advance for any insights!",,['ordinary-differential-equations']
2,Homogeneous differential equation - cannot manipulate equation,Homogeneous differential equation - cannot manipulate equation,,"this was a problem from a textbook: If $x>0$, $y>0$, find the general solution to the differential equation,      $$ x \frac{dy}{dx} = y + \frac{x}{\ln y - \ln x }$$      giving your answer in the form  $ye^{y/x}=f(x)$ I approached with the substitution $y=vx$ since this is a homogenous equation. Giving, $$ x\frac{dv}{dx}+v = v+ \frac{1}{\ln v}$$ By separating variables yields, $$ v \ln v -v = \ln x + C $$ The by taking the power of $e$  $$e^{v \ln v - v } = xA$$ where $e^C= A$. This is equivalent to $$e^{-v}v^{v} = Ax$$ Now when I substitute the value of $v= \frac{y}{x}$ back, I cannot obtain the equation of the form $ye^{y/x} = f(x)$. Maybe I have done a mistake in my calculations. May someone explain where? Or how to solve the problem? Thank you so much!!","this was a problem from a textbook: If $x>0$, $y>0$, find the general solution to the differential equation,      $$ x \frac{dy}{dx} = y + \frac{x}{\ln y - \ln x }$$      giving your answer in the form  $ye^{y/x}=f(x)$ I approached with the substitution $y=vx$ since this is a homogenous equation. Giving, $$ x\frac{dv}{dx}+v = v+ \frac{1}{\ln v}$$ By separating variables yields, $$ v \ln v -v = \ln x + C $$ The by taking the power of $e$  $$e^{v \ln v - v } = xA$$ where $e^C= A$. This is equivalent to $$e^{-v}v^{v} = Ax$$ Now when I substitute the value of $v= \frac{y}{x}$ back, I cannot obtain the equation of the form $ye^{y/x} = f(x)$. Maybe I have done a mistake in my calculations. May someone explain where? Or how to solve the problem? Thank you so much!!",,"['ordinary-differential-equations', 'homogeneous-equation']"
3,Can Every Higher Order PDE be Written as a System of 1st Order PDEs?,Can Every Higher Order PDE be Written as a System of 1st Order PDEs?,,"Motivation A section on the Wikipedia page ( here ) of ordinary differential equations states the following. Reduction to a 1st Order System Any differential equation of order $n$ $$F\left(x,y,y',y'',\ldots,y^{(n-1)}\right)=y^{(n)},\quad\quad\text{where}\quad\quad y=y(x)$$ can be written as a system of $n$ 1st order differential equations by defining a new family of unknown functions $$y_{i}=y^{(i-1)},$$ for $i=1,2,\ldots,n$. The $n$-dimensional system of first-order coupled differential equations is then $$y_{1}'=y_{2},\qquad y_{2}'=y_{3},\qquad\ldots,\qquad y_{n-1}'=y_{n},\qquad y_{n}'=F(x,y_{1},y_{2},\ldots,y_{n}).$$ Is there a similar technique that applies to PDEs where any higher order PDE can be written as a system of 1st order PDEs? Are there any recommended books on PDEs that describe/discuss such a technique? Thanks, Jack.","Motivation A section on the Wikipedia page ( here ) of ordinary differential equations states the following. Reduction to a 1st Order System Any differential equation of order $n$ $$F\left(x,y,y',y'',\ldots,y^{(n-1)}\right)=y^{(n)},\quad\quad\text{where}\quad\quad y=y(x)$$ can be written as a system of $n$ 1st order differential equations by defining a new family of unknown functions $$y_{i}=y^{(i-1)},$$ for $i=1,2,\ldots,n$. The $n$-dimensional system of first-order coupled differential equations is then $$y_{1}'=y_{2},\qquad y_{2}'=y_{3},\qquad\ldots,\qquad y_{n-1}'=y_{n},\qquad y_{n}'=F(x,y_{1},y_{2},\ldots,y_{n}).$$ Is there a similar technique that applies to PDEs where any higher order PDE can be written as a system of 1st order PDEs? Are there any recommended books on PDEs that describe/discuss such a technique? Thanks, Jack.",,"['ordinary-differential-equations', 'reference-request', 'partial-differential-equations']"
4,Solving a differential equation with a square root,Solving a differential equation with a square root,,"I am trying to solve the differential equation $ A(x)\frac{d^{2}f(x)}{dx^{2}}+B(x)\frac{df(x)}{dx}=\frac{1}{3}\frac{1}{\sqrt{f(x)}}, $ where $ A(x)=\frac{x}{x+1} $ and $ B(x)=\frac{2x+1}{(x+1)^{2}} $. for $1\ll{x}$, the equation simplifies to $ \frac{d^{2}f(x)}{dx^{2}}+\frac{2}{x}\frac{df(x)}{dx}=\frac{1}{3}\frac{1}{\sqrt{f(x)}}, $ Substituting, $k x^{p}$ for $f(x)$, and solving for $p$ and $k$ gives the solution as $f(x)=c(x^{4/3})$, where $c$ is some constant. for $x\ll{1}$, the equation simplifies to $ x\frac{d^{2}f(x)}{dx^{2}}+\frac{df(x)}{dx}=\frac{1}{3}\frac{1}{\sqrt{f(x)}}, $ Substituting, $k x^{p}$ for $f(x)$, and solving for $p$ and $k$ gives the solution as $f(x)=c(x^{2/3})$, where $c$ is some constant. However, I could only solve the equation for $x\ll{1}$ and $1\ll{x}$, Is it possible to solve this equation for all $x$?","I am trying to solve the differential equation $ A(x)\frac{d^{2}f(x)}{dx^{2}}+B(x)\frac{df(x)}{dx}=\frac{1}{3}\frac{1}{\sqrt{f(x)}}, $ where $ A(x)=\frac{x}{x+1} $ and $ B(x)=\frac{2x+1}{(x+1)^{2}} $. for $1\ll{x}$, the equation simplifies to $ \frac{d^{2}f(x)}{dx^{2}}+\frac{2}{x}\frac{df(x)}{dx}=\frac{1}{3}\frac{1}{\sqrt{f(x)}}, $ Substituting, $k x^{p}$ for $f(x)$, and solving for $p$ and $k$ gives the solution as $f(x)=c(x^{4/3})$, where $c$ is some constant. for $x\ll{1}$, the equation simplifies to $ x\frac{d^{2}f(x)}{dx^{2}}+\frac{df(x)}{dx}=\frac{1}{3}\frac{1}{\sqrt{f(x)}}, $ Substituting, $k x^{p}$ for $f(x)$, and solving for $p$ and $k$ gives the solution as $f(x)=c(x^{2/3})$, where $c$ is some constant. However, I could only solve the equation for $x\ll{1}$ and $1\ll{x}$, Is it possible to solve this equation for all $x$?",,"['ordinary-differential-equations', 'differential']"
5,Why does an infinite Neumann boundary condition become a Dirichlet condition?,Why does an infinite Neumann boundary condition become a Dirichlet condition?,,"Often when I read a paper I see a statement of the type: Our boundary condition at the surface is $\frac{\partial f}{\partial x} = \alpha$.  In the limit of $\alpha \to \infty$ this is equivalent to the previously studied case of $f = \beta$ If it matters, I've seen this on papers dealing with chemical reactions, when the Neumann condition would represent a reaction on the surface and the Dirichlet would represent constant concentration at the surface. Why is this true? I'm honestly more interested in an intuitive or physical explanation, but I'd also be happy with a formal proof.","Often when I read a paper I see a statement of the type: Our boundary condition at the surface is $\frac{\partial f}{\partial x} = \alpha$.  In the limit of $\alpha \to \infty$ this is equivalent to the previously studied case of $f = \beta$ If it matters, I've seen this on papers dealing with chemical reactions, when the Neumann condition would represent a reaction on the surface and the Dirichlet would represent constant concentration at the surface. Why is this true? I'm honestly more interested in an intuitive or physical explanation, but I'd also be happy with a formal proof.",,"['ordinary-differential-equations', 'partial-differential-equations', 'chemistry']"
6,Finding the inverse of a function.,Finding the inverse of a function.,,"Let $f:\mathbb{R}\to \mathbb{R}_+$ with $f\geq\epsilon>0$ be smooth and define $G:\mathbb{R}\to\mathbb{R}$ thus $$G(x):=\int_0^x\frac{1}{f(u)}\mathrm{d}u$$ Then it is clear that $G$ is well-defined, continuous and strictly increasing (so bijective). Therefore $G$ must have a continuous inverse. What I would like is a formula for the inverse, but I can't seem to come up with one. Suppose the inverse is given by  $P$, then $G(P(x))=x$ differentiating both sides $$G'(P(x))P'(x)=1$$ hence $$P'(x)=f(P(x))$$ using the fact that $G'=\frac{1}{f}$. So I have an ODE that $P$ would solve ($P$ is continuously differentiable by the inverse function theorem), but it doesn't seem to help! EDIT in light of the comment: This problem arose in an approximation scheme for stochastic differential equations on manifolds; I don't really know how to explain the background succinctly, but I have imposed no conditions on f beyond what's mentioned above. If anyone knows how to solve the problem in less generality, I would also be very happy.","Let $f:\mathbb{R}\to \mathbb{R}_+$ with $f\geq\epsilon>0$ be smooth and define $G:\mathbb{R}\to\mathbb{R}$ thus $$G(x):=\int_0^x\frac{1}{f(u)}\mathrm{d}u$$ Then it is clear that $G$ is well-defined, continuous and strictly increasing (so bijective). Therefore $G$ must have a continuous inverse. What I would like is a formula for the inverse, but I can't seem to come up with one. Suppose the inverse is given by  $P$, then $G(P(x))=x$ differentiating both sides $$G'(P(x))P'(x)=1$$ hence $$P'(x)=f(P(x))$$ using the fact that $G'=\frac{1}{f}$. So I have an ODE that $P$ would solve ($P$ is continuously differentiable by the inverse function theorem), but it doesn't seem to help! EDIT in light of the comment: This problem arose in an approximation scheme for stochastic differential equations on manifolds; I don't really know how to explain the background succinctly, but I have imposed no conditions on f beyond what's mentioned above. If anyone knows how to solve the problem in less generality, I would also be very happy.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
7,How to find Green's function using Fourier-Bessel expansion,How to find Green's function using Fourier-Bessel expansion,,"The Green's function satisfies the non homogeneous Bessel equation can be written as $xg''+g'+\left(k^2x-\frac{m^2}{x}\right)g=-\delta(x-\xi)$ where $m\geq0$ and an integer. The boundary conditions are $\lim_{x\to 0}|g\left(x|\xi\right)|<\infty$, and $g\left(L|\xi\right)=0$. The Fourier-Bessel series representation of $g$ can be written as $g\left(x|\xi\right)=\sum_{n=1}^{\infty}G_n\left(\xi\right)J_m(k_{nm}x)$. Where $k_{nm}$ is the $n^{th}$ root of $J_m\left(k_{nm}x\right)$. Now if we substitute the second equation into the first one with the Fourier-Bessel representation of $\delta(x-\xi)$ then the $G_n(\xi)$ is expressed as $\left(k^2-k_{nm}^2\right)G_n(\xi)=-\frac{2k_{nm}^2J_m(k_{nm}\xi)}{L^2\left[J_{m+1}\left(k_{nm}L\right)\right]^2}$ How the last result is obtained after substitution? Please help to find it out.","The Green's function satisfies the non homogeneous Bessel equation can be written as $xg''+g'+\left(k^2x-\frac{m^2}{x}\right)g=-\delta(x-\xi)$ where $m\geq0$ and an integer. The boundary conditions are $\lim_{x\to 0}|g\left(x|\xi\right)|<\infty$, and $g\left(L|\xi\right)=0$. The Fourier-Bessel series representation of $g$ can be written as $g\left(x|\xi\right)=\sum_{n=1}^{\infty}G_n\left(\xi\right)J_m(k_{nm}x)$. Where $k_{nm}$ is the $n^{th}$ root of $J_m\left(k_{nm}x\right)$. Now if we substitute the second equation into the first one with the Fourier-Bessel representation of $\delta(x-\xi)$ then the $G_n(\xi)$ is expressed as $\left(k^2-k_{nm}^2\right)G_n(\xi)=-\frac{2k_{nm}^2J_m(k_{nm}\xi)}{L^2\left[J_{m+1}\left(k_{nm}L\right)\right]^2}$ How the last result is obtained after substitution? Please help to find it out.",,"['ordinary-differential-equations', 'bessel-functions', 'greens-function']"
8,Vector Laplace equation with constraint,Vector Laplace equation with constraint,,"I want to solve Laplace equation for a vector $\boldsymbol v=(v_x,v_y)$: $$\nabla^2 \boldsymbol{v}=0$$ but under the constraint that $$(1+v_x)^2+v_y^2=1$$ which becomes $v_y = -(2v_x+v_x^2)^{1/2}$. My boundary conditions are: $$\lim_{r\rightarrow\infty} v(x,y)=0$$ $$v(r=a,\theta)=( \cos(\theta/2), \sin(\theta/2) )$$ Normally, without the constraints, I would have $$\boldsymbol v = \frac{a^{1/2}}{r^{1/2}} ( \cos(\theta/2), \sin(\theta/2) )$$ But how could I use Lagrange multipliers in this situation, if I want to impose the condition $|\boldsymbol{\hat{x}}+\boldsymbol{v}|=1$? Are there any other straightforward methods?","I want to solve Laplace equation for a vector $\boldsymbol v=(v_x,v_y)$: $$\nabla^2 \boldsymbol{v}=0$$ but under the constraint that $$(1+v_x)^2+v_y^2=1$$ which becomes $v_y = -(2v_x+v_x^2)^{1/2}$. My boundary conditions are: $$\lim_{r\rightarrow\infty} v(x,y)=0$$ $$v(r=a,\theta)=( \cos(\theta/2), \sin(\theta/2) )$$ Normally, without the constraints, I would have $$\boldsymbol v = \frac{a^{1/2}}{r^{1/2}} ( \cos(\theta/2), \sin(\theta/2) )$$ But how could I use Lagrange multipliers in this situation, if I want to impose the condition $|\boldsymbol{\hat{x}}+\boldsymbol{v}|=1$? Are there any other straightforward methods?",,"['ordinary-differential-equations', 'partial-differential-equations', 'physics', 'vector-analysis', 'constraints']"
9,How can we estimate number of zeros?,How can we estimate number of zeros?,,"Assume $a>0$ , $b>0$ and there exists a non-zero function $\phi(t)$ such that is the solution of $$y''+(a+b\cos 2t)y=0$$ and on $(-\pi/2,\pi/2)$ has $2n$ zero. How Floquet theory can help to prove that $$(2n-1)^2\le a+b $$ Thanks","Assume $a>0$ , $b>0$ and there exists a non-zero function $\phi(t)$ such that is the solution of $$y''+(a+b\cos 2t)y=0$$ and on $(-\pi/2,\pi/2)$ has $2n$ zero. How Floquet theory can help to prove that $$(2n-1)^2\le a+b $$ Thanks",,"['analysis', 'ordinary-differential-equations']"
10,Variant of Picard-Lindelöf theorem,Variant of Picard-Lindelöf theorem,,"Question Let $I=[0,a]$ and define the norm $||f||_{\lambda}=\sup_I |e^{-\lambda x}f(x)|$ for $f\in C(I)$. Let $\phi:\;\mathbb{R}^2\to\mathbb{R}$ satify $|\phi(x,u)-\phi(y,v)|\leq\rho |u-v|$ for all $x,y,u,v\in\mathbb{R}$ and some $\rho >0$. Define $\tau:\;f\mapsto \int_0^x \phi(t,f(t))\;dt$ I need to find a $\lambda$ such that $\tau$ is a contraction under the norm $||\cdot||_{\lambda}$ Thoughts I am not too sure how to do this; my first line of thought was: $$\begin{aligned}||\tau (f)-\tau (g)||_{\lambda} &=\sup_I\Big| e^{-\lambda x} \int_0^x \phi(t,f(t))-\phi(t,g(t))\;dt\Big| \\ &\leq \sup_I e^{-\lambda x} \int_0^x |\phi(t,f(t))-\phi(t,g(t))|\;dt\\ &\leq\sup_I \rho e^{-\lambda x} \int_0^x |f(t)-g(t)|\;dt \end{aligned}$$ But I can't see how to get $\cdots \leq \alpha\sup_I |e^{-\lambda x}(f(x)-g(x))|$ for some $\alpha<1 $ and some $\lambda$ from this. Any help would be appreciated.","Question Let $I=[0,a]$ and define the norm $||f||_{\lambda}=\sup_I |e^{-\lambda x}f(x)|$ for $f\in C(I)$. Let $\phi:\;\mathbb{R}^2\to\mathbb{R}$ satify $|\phi(x,u)-\phi(y,v)|\leq\rho |u-v|$ for all $x,y,u,v\in\mathbb{R}$ and some $\rho >0$. Define $\tau:\;f\mapsto \int_0^x \phi(t,f(t))\;dt$ I need to find a $\lambda$ such that $\tau$ is a contraction under the norm $||\cdot||_{\lambda}$ Thoughts I am not too sure how to do this; my first line of thought was: $$\begin{aligned}||\tau (f)-\tau (g)||_{\lambda} &=\sup_I\Big| e^{-\lambda x} \int_0^x \phi(t,f(t))-\phi(t,g(t))\;dt\Big| \\ &\leq \sup_I e^{-\lambda x} \int_0^x |\phi(t,f(t))-\phi(t,g(t))|\;dt\\ &\leq\sup_I \rho e^{-\lambda x} \int_0^x |f(t)-g(t)|\;dt \end{aligned}$$ But I can't see how to get $\cdots \leq \alpha\sup_I |e^{-\lambda x}(f(x)-g(x))|$ for some $\alpha<1 $ and some $\lambda$ from this. Any help would be appreciated.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'vector-spaces']"
11,Fourier Transform of Newton's Law of Cooling,Fourier Transform of Newton's Law of Cooling,,"I am attempting to solve Newton's Law of Cooling differential equation with Fourier Transforms for a high school math report. Can Fourier Transforms be used to solve first-order ODEs? The equation is: $\frac{dT}{dt} = -kT + kT_a$, where $k$ and $T_a$ are constants. Since the Fourier Transform is a linear operator, I took the transform of each of the three terms: $$F\{T'\} = F\{-kT\} + F\{kT_a\}$$ $$F\{T'\} =  -kF\{T\} + kT_aF\{1\}$$ The Fourier transform of 1 should be $\delta(s)$, so: $$F\{T'\} = -kF\{T\} + kT_a\delta(s)$$ The transform of the derivative of a function, using $s$ instead of $\omega$ in the formulas, is $F\{T'\} = 2\pi isF\{T\}$. Substituting this into the original equation, $$2\pi isF\{T\} = -kF\{T\} + kT_a\delta(s)$$ Adding the terms with the transform should yield $$2\pi isF\{T\} +kF\{T\} = kT_a\delta(s)$$ $$F\{T\}(2\pi is +k) = kT_a\delta(s)$$ $$F\{T\} = \frac{kT_a\delta(s)}{2\pi is + k}$$. The formula for the Inverse Fourier Transform is $f(t) = \int_{-\infty}^{\infty} e^{2\pi ist} F(s) ds$. Thus $$T = \int_{-\infty}^{\infty} \frac{e^{2\pi ist} \delta(s) kT_a}{2\pi is + k}ds$$, which I evaluated to $T_a$, dropping the integral and plugging in 0 for $s$ due to the $\delta(s)$ function...however, the actual solution to the differential equation is $T_a + Ae^{-kt}$, where A is a constant. Where did I go wrong?","I am attempting to solve Newton's Law of Cooling differential equation with Fourier Transforms for a high school math report. Can Fourier Transforms be used to solve first-order ODEs? The equation is: $\frac{dT}{dt} = -kT + kT_a$, where $k$ and $T_a$ are constants. Since the Fourier Transform is a linear operator, I took the transform of each of the three terms: $$F\{T'\} = F\{-kT\} + F\{kT_a\}$$ $$F\{T'\} =  -kF\{T\} + kT_aF\{1\}$$ The Fourier transform of 1 should be $\delta(s)$, so: $$F\{T'\} = -kF\{T\} + kT_a\delta(s)$$ The transform of the derivative of a function, using $s$ instead of $\omega$ in the formulas, is $F\{T'\} = 2\pi isF\{T\}$. Substituting this into the original equation, $$2\pi isF\{T\} = -kF\{T\} + kT_a\delta(s)$$ Adding the terms with the transform should yield $$2\pi isF\{T\} +kF\{T\} = kT_a\delta(s)$$ $$F\{T\}(2\pi is +k) = kT_a\delta(s)$$ $$F\{T\} = \frac{kT_a\delta(s)}{2\pi is + k}$$. The formula for the Inverse Fourier Transform is $f(t) = \int_{-\infty}^{\infty} e^{2\pi ist} F(s) ds$. Thus $$T = \int_{-\infty}^{\infty} \frac{e^{2\pi ist} \delta(s) kT_a}{2\pi is + k}ds$$, which I evaluated to $T_a$, dropping the integral and plugging in 0 for $s$ due to the $\delta(s)$ function...however, the actual solution to the differential equation is $T_a + Ae^{-kt}$, where A is a constant. Where did I go wrong?",,"['ordinary-differential-equations', 'fourier-analysis']"
12,Confusion about superposition principle of the PDE and Boundary Condition of an ODE.,Confusion about superposition principle of the PDE and Boundary Condition of an ODE.,,"I want to solve a PDE like this: $\frac{\partial y}{\partial t}=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y,(a,b,c\in \mathbb{R})\tag{1}$ with the boundary conditions: $ \begin{equation}\begin{cases} y|_{x=0}=y_0+y_1 \cos (\text{$\omega $t})\\ y|_{x\rightarrow +\infty }=y_2\\ \end{cases}\ \tag{2} \end{equation}\ $ In the second BC, $y_2\neq 0$ I uncoupled the problem using superposition principle into two sub problems: $ \begin{equation}\begin{cases} \frac{\partial y}{\partial t}=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y\\ y|_{x=0}=y_0\\ y|_{x\rightarrow +\infty }=y_2\\ \end{cases} \tag{3} \end{equation} $ and $ \begin{equation}\begin{cases} \frac{\partial y}{\partial t}=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y\\ y|_{x=0}=y_1 \cos (\text{$\omega $t})\\ y|_{x\rightarrow +\infty }=0\\ \end{cases} \tag{4} \end{equation} $ I have 3 questions: Is this uncoupling correct? If not, how can I use superposition principle to uncouple this problem? If the actual system is only changed by $\cos (\text{$\omega $t})$. In other words, if there's no $\cos (\text{$\omega $t})$ term, the system is in steady state. On this occasion can I write sub problem(3) like this:  $ \begin{equation}\begin{cases} 0=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y\\ y|_{x=0}=y_0\\ y|_{x\rightarrow +\infty }=y_2\\ \end{cases} \tag{5} \end{equation} $ The problem(5) is actually an ODE. If I solve it, I get into trouble. The general solution of the ODE is:  $y=C_1 \exp \left(\frac{b+\sqrt{4 a c+b^2}}{2 b}\right)+C_2 \exp \left(\frac{b-\sqrt{4 a c+b^2}}{2 b}\right)\tag{6}$ But I cannot deal with the second boundary condition. The first term of the general solution tend to positive infinity and the second term tend to zero. No matter what the value of $C_1$ and $C_2$ is, the condition is not satisfied. Does this trouble imply that I cannot set the boundary condition like $y|_{x\rightarrow +\infty }=y_2$? I wonder if the BC here can only be set like: $y|_{x\rightarrow +\infty }=0$ or $\left.\frac{\partial y}{\partial x}\right|_{x\rightarrow +\infty }=0$?","I want to solve a PDE like this: $\frac{\partial y}{\partial t}=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y,(a,b,c\in \mathbb{R})\tag{1}$ with the boundary conditions: $ \begin{equation}\begin{cases} y|_{x=0}=y_0+y_1 \cos (\text{$\omega $t})\\ y|_{x\rightarrow +\infty }=y_2\\ \end{cases}\ \tag{2} \end{equation}\ $ In the second BC, $y_2\neq 0$ I uncoupled the problem using superposition principle into two sub problems: $ \begin{equation}\begin{cases} \frac{\partial y}{\partial t}=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y\\ y|_{x=0}=y_0\\ y|_{x\rightarrow +\infty }=y_2\\ \end{cases} \tag{3} \end{equation} $ and $ \begin{equation}\begin{cases} \frac{\partial y}{\partial t}=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y\\ y|_{x=0}=y_1 \cos (\text{$\omega $t})\\ y|_{x\rightarrow +\infty }=0\\ \end{cases} \tag{4} \end{equation} $ I have 3 questions: Is this uncoupling correct? If not, how can I use superposition principle to uncouple this problem? If the actual system is only changed by $\cos (\text{$\omega $t})$. In other words, if there's no $\cos (\text{$\omega $t})$ term, the system is in steady state. On this occasion can I write sub problem(3) like this:  $ \begin{equation}\begin{cases} 0=a\frac{\partial ^2y}{\partial x^2}-b\frac{\partial y}{\partial x}-c y\\ y|_{x=0}=y_0\\ y|_{x\rightarrow +\infty }=y_2\\ \end{cases} \tag{5} \end{equation} $ The problem(5) is actually an ODE. If I solve it, I get into trouble. The general solution of the ODE is:  $y=C_1 \exp \left(\frac{b+\sqrt{4 a c+b^2}}{2 b}\right)+C_2 \exp \left(\frac{b-\sqrt{4 a c+b^2}}{2 b}\right)\tag{6}$ But I cannot deal with the second boundary condition. The first term of the general solution tend to positive infinity and the second term tend to zero. No matter what the value of $C_1$ and $C_2$ is, the condition is not satisfied. Does this trouble imply that I cannot set the boundary condition like $y|_{x\rightarrow +\infty }=y_2$? I wonder if the BC here can only be set like: $y|_{x\rightarrow +\infty }=0$ or $\left.\frac{\partial y}{\partial x}\right|_{x\rightarrow +\infty }=0$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
13,How to solve this complicated ordinary differential equation?,How to solve this complicated ordinary differential equation?,,"Consider the following non-linear ODE: $$x^2  \frac{dy}{dx} + \exp{\left(x \, \frac{d^2 y}{dx^2} \right)} = \sin \left(\frac{d^3y}{dx^3} \, \cos \left( \frac{d^4y}{dx^4} \right) \right) $$ I have no idea where to even begin. Could someone show me how to solve it or at the very least give me a hint? Thank you.","Consider the following non-linear ODE: $$x^2  \frac{dy}{dx} + \exp{\left(x \, \frac{d^2 y}{dx^2} \right)} = \sin \left(\frac{d^3y}{dx^3} \, \cos \left( \frac{d^4y}{dx^4} \right) \right) $$ I have no idea where to even begin. Could someone show me how to solve it or at the very least give me a hint? Thank you.",,['ordinary-differential-equations']
14,solution of $y' + y^2 = \varphi^2(x)$,solution of,y' + y^2 = \varphi^2(x),"I need to solve differential equation in the interval $[-\pi/2,\pi/2]$ \begin{eqnarray} y''(x) = y(x)\sin^2x \end{eqnarray} Trying $y(x) = \exp(\psi(x))$  yields, \begin{eqnarray} \zeta'(x) + \zeta^2(x) = \sin^2x \hspace{2cm} \zeta(x) = \psi'(x) \end{eqnarray} This equation seems to simpler than the original second order equation but still I can't find way to solve this. Even if the equation is solved for function other than $\sin(x)$ with some important properties of $\sin(x)$ preserved,  I will consider myself fortunate. Is it possible to solve for any $\varphi(x)$ such that, \begin{eqnarray} \zeta'(x) + \zeta^2(x)  = \varphi^2(x) \end{eqnarray} Where, $\varphi(x)$ is a monotonic function some interval $[a,b]$, with exactly one inflation point at $(a+b)/2$ and derivative vanishing at endpoints? \begin{eqnarray} \varphi'(x) >= 0 \\ \varphi''(x)|_{x=\frac{a+b}{2}} = 0\\ \varphi'(x)|_{x=a,b} = 0 \end{eqnarray}","I need to solve differential equation in the interval $[-\pi/2,\pi/2]$ \begin{eqnarray} y''(x) = y(x)\sin^2x \end{eqnarray} Trying $y(x) = \exp(\psi(x))$  yields, \begin{eqnarray} \zeta'(x) + \zeta^2(x) = \sin^2x \hspace{2cm} \zeta(x) = \psi'(x) \end{eqnarray} This equation seems to simpler than the original second order equation but still I can't find way to solve this. Even if the equation is solved for function other than $\sin(x)$ with some important properties of $\sin(x)$ preserved,  I will consider myself fortunate. Is it possible to solve for any $\varphi(x)$ such that, \begin{eqnarray} \zeta'(x) + \zeta^2(x)  = \varphi^2(x) \end{eqnarray} Where, $\varphi(x)$ is a monotonic function some interval $[a,b]$, with exactly one inflation point at $(a+b)/2$ and derivative vanishing at endpoints? \begin{eqnarray} \varphi'(x) >= 0 \\ \varphi''(x)|_{x=\frac{a+b}{2}} = 0\\ \varphi'(x)|_{x=a,b} = 0 \end{eqnarray}",,"['calculus', 'ordinary-differential-equations']"
15,Wronskian different from zero and solutions of ODE. [closed],Wronskian different from zero and solutions of ODE. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $a_0, \ldots , a_{n-1}$ continuous functions in an interval $I$.Consider the equation $$x^{(n)} = a_{n-1}(t)x^{(n-1)}+\cdots+a_0(t)x. \tag 1$$ Let $\phi_1, \phi_2, \ldots,\phi_n$ $n$  are functions of $C^n$ such that $W(\phi_1, \phi_2, \ldots ,\phi_n)(t) \neq 0$ in I. Prove that there is a unique equation of the form (1) which $ \phi_1, \phi_2, \ldots,\phi_n $ is based solutions. ($W$ denotes the Wronskian) Showed that $\phi_1, \phi_2, \ldots,\phi_n$ are linearly independent if $W \neq 0$, but I'm not finishing.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $a_0, \ldots , a_{n-1}$ continuous functions in an interval $I$.Consider the equation $$x^{(n)} = a_{n-1}(t)x^{(n-1)}+\cdots+a_0(t)x. \tag 1$$ Let $\phi_1, \phi_2, \ldots,\phi_n$ $n$  are functions of $C^n$ such that $W(\phi_1, \phi_2, \ldots ,\phi_n)(t) \neq 0$ in I. Prove that there is a unique equation of the form (1) which $ \phi_1, \phi_2, \ldots,\phi_n $ is based solutions. ($W$ denotes the Wronskian) Showed that $\phi_1, \phi_2, \ldots,\phi_n$ are linearly independent if $W \neq 0$, but I'm not finishing.",,"['ordinary-differential-equations', 'functional-equations']"
16,Values of $k$ for non-trivial solutions of the differential equation $y''-\left(\frac{1}{4}+\frac{k}{x}\right)y=0$ where $x$ is non-negative,Values of  for non-trivial solutions of the differential equation  where  is non-negative,k y''-\left(\frac{1}{4}+\frac{k}{x}\right)y=0 x,I attempted a power series solution of this equation in order to find the values of k that have a non-trivial solution: $y''-\left(\dfrac{1}{4}+\dfrac{k}{x}\right)y=0$ I am having trouble constructing the final form of the solution. I have found the following relations: $a_0=8a_2$ $a_{n+2}=\dfrac{\dfrac{a_n}{4}+ka_{n+1}}{(n+2)(n+1)}$ How do I proceed to construct a general solution that will reveal which values of $k$ give a nontrivial solution that will vanish at $x=0$ and $x=\infty$ ?,I attempted a power series solution of this equation in order to find the values of k that have a non-trivial solution: $y''-\left(\dfrac{1}{4}+\dfrac{k}{x}\right)y=0$ I am having trouble constructing the final form of the solution. I have found the following relations: $a_0=8a_2$ $a_{n+2}=\dfrac{\dfrac{a_n}{4}+ka_{n+1}}{(n+2)(n+1)}$ How do I proceed to construct a general solution that will reveal which values of $k$ give a nontrivial solution that will vanish at $x=0$ and $x=\infty$ ?,,['ordinary-differential-equations']
17,How can we conclude that such function $f(x)$ either exists or not ?,How can we conclude that such function  either exists or not ?,f(x),"In this PDF file here ( file from OCW.MIT ) , problem 1 : part c . For the  differential equation , $\frac{dy}{dx} = y^2 - x^2 $ , There exists a number $y_o$ such that  if $y$ is a solution with $y(0) > y_0 $ then $y$ becomes large as $x$ becomes large, while if $y(0) < y_0$ then $y$ decreases as $x$ increases where  $0.66 < y_o < 0.68$ . Is there some function $f(x)$ such that $y(x) > f(x)$ for all $x > 0$ whenever $y$ is a solution with $0 < y(0) < y_0$ ? when I used a software ( Works on the browser ) which is provided by the site , ( here ) and I have chosen the required DF , I  guessed from the Geometrical sketch that  such $f$ exists , namely , any function $f$ such that $f(0)<y(o)$ satisfy the condition , bur the exercise asks for explanation , So any ideas ?","In this PDF file here ( file from OCW.MIT ) , problem 1 : part c . For the  differential equation , $\frac{dy}{dx} = y^2 - x^2 $ , There exists a number $y_o$ such that  if $y$ is a solution with $y(0) > y_0 $ then $y$ becomes large as $x$ becomes large, while if $y(0) < y_0$ then $y$ decreases as $x$ increases where  $0.66 < y_o < 0.68$ . Is there some function $f(x)$ such that $y(x) > f(x)$ for all $x > 0$ whenever $y$ is a solution with $0 < y(0) < y_0$ ? when I used a software ( Works on the browser ) which is provided by the site , ( here ) and I have chosen the required DF , I  guessed from the Geometrical sketch that  such $f$ exists , namely , any function $f$ such that $f(0)<y(o)$ satisfy the condition , bur the exercise asks for explanation , So any ideas ?",,"['calculus', 'ordinary-differential-equations']"
18,Matrix differential equation and closed orbits,Matrix differential equation and closed orbits,,everyone. I am asking for a reference for the nonexistence of closed orbits (periodic orbits) of Matrix differential equations of the form \begin{equation} v\prime = M(v)\cdot v \end{equation} where $M(v)$ is a (nonconstant) matrix depending smoothly on $v \in \mathbb{R}^n$. I am really not an expert in Differential Equations. Advanced thanks for any help/suggestion/reference.,everyone. I am asking for a reference for the nonexistence of closed orbits (periodic orbits) of Matrix differential equations of the form \begin{equation} v\prime = M(v)\cdot v \end{equation} where $M(v)$ is a (nonconstant) matrix depending smoothly on $v \in \mathbb{R}^n$. I am really not an expert in Differential Equations. Advanced thanks for any help/suggestion/reference.,,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems']"
19,Solving ODE $y'=3|y|^x$ with initial value,Solving ODE  with initial value,y'=3|y|^x,"We consider the function $f:G\rightarrow\mathbb{R}, (x,y)\mapsto 3|y|^x$, where $G=\mathbb{R}_+ \times\mathbb{R}\subset\mathbb{R}^2$. Furthermore is $y$, as in the standard notation of ODE, a function of $x$. I should solve the initialvalue problem $y'=3|y|^x$ with $y(\frac{2}{3})=\frac{1}{27}$, but I don't have any idea how to deal with the $x$ in the exponent. I tried yet to rewrite the equation to\begin{equation}y'=3e^{x\cdot log(|y|)}\end{equation} but this didn't help much. Could someone provide a hint how to deal with ODEs of this type? Thanks! P.S. This is your chance to show that human mind is superior to computational power. Wolframalpha resigns here...","We consider the function $f:G\rightarrow\mathbb{R}, (x,y)\mapsto 3|y|^x$, where $G=\mathbb{R}_+ \times\mathbb{R}\subset\mathbb{R}^2$. Furthermore is $y$, as in the standard notation of ODE, a function of $x$. I should solve the initialvalue problem $y'=3|y|^x$ with $y(\frac{2}{3})=\frac{1}{27}$, but I don't have any idea how to deal with the $x$ in the exponent. I tried yet to rewrite the equation to\begin{equation}y'=3e^{x\cdot log(|y|)}\end{equation} but this didn't help much. Could someone provide a hint how to deal with ODEs of this type? Thanks! P.S. This is your chance to show that human mind is superior to computational power. Wolframalpha resigns here...",,['ordinary-differential-equations']
20,Clarification in a paper,Clarification in a paper,,"This is regarding a clarification in page 384 of a paper published in Annals of Statistics by Amari. In page no. 384, he defines $$R_i(t)=\frac{\partial}{\partial \theta_i} D_{\alpha}\{q(x,t),p(x,\theta)\}$$ where for $|\alpha|\neq 1$, \begin{eqnarray*} D_{\alpha}\{q(x,t),p(x,\theta)\}&=&\frac{8}{1-\alpha^2}\int \left(1-\left(\frac{q(x,t)}{p(x,\theta)}\right)^{\frac{1+\alpha}{2}}\right)p(x,\theta)~dx\\ &=&\frac{8}{1-\alpha^2}\left[1-\int q(x,t)^{\frac{1+\alpha}{2}}p(x,\theta)^{\frac{1-\alpha}{2}}~dx\right] \end{eqnarray*} The one I am struggling to understand is that how did the author obtain the differential equation (A.19): $$\ddot{R_i}(t)=-\frac{1-\alpha^2}{4}i(t)R(t)$$ where, I think, $i(t)=\int \frac{1}{q(x,t)}(\dot{q}(x,t))^2~dx$ from bottom of page 382. When I derived manually I obtained (by allowing interchange of differentiation and integral) $$\ddot{R_i}(t)=\int p(x,\theta)^{-\frac{1+\alpha}{2}}\left[q(x,t)^{\frac{\alpha-1}{2}}\ddot{q}(x,t)+\frac{\alpha-1}{2}q(x,t)^{\frac{\alpha-3}{2}}\dot{q}(x,t))^2\right]\frac{\partial}{\partial \theta_i}(p(x,\theta)~dx.$$ I am wondering how can the right hand side of the above be written as a product of two integrals as in (A.19). As this is appeared in Annals of Statistics, I am not in a position to doubt this as well.","This is regarding a clarification in page 384 of a paper published in Annals of Statistics by Amari. In page no. 384, he defines $$R_i(t)=\frac{\partial}{\partial \theta_i} D_{\alpha}\{q(x,t),p(x,\theta)\}$$ where for $|\alpha|\neq 1$, \begin{eqnarray*} D_{\alpha}\{q(x,t),p(x,\theta)\}&=&\frac{8}{1-\alpha^2}\int \left(1-\left(\frac{q(x,t)}{p(x,\theta)}\right)^{\frac{1+\alpha}{2}}\right)p(x,\theta)~dx\\ &=&\frac{8}{1-\alpha^2}\left[1-\int q(x,t)^{\frac{1+\alpha}{2}}p(x,\theta)^{\frac{1-\alpha}{2}}~dx\right] \end{eqnarray*} The one I am struggling to understand is that how did the author obtain the differential equation (A.19): $$\ddot{R_i}(t)=-\frac{1-\alpha^2}{4}i(t)R(t)$$ where, I think, $i(t)=\int \frac{1}{q(x,t)}(\dot{q}(x,t))^2~dx$ from bottom of page 382. When I derived manually I obtained (by allowing interchange of differentiation and integral) $$\ddot{R_i}(t)=\int p(x,\theta)^{-\frac{1+\alpha}{2}}\left[q(x,t)^{\frac{\alpha-1}{2}}\ddot{q}(x,t)+\frac{\alpha-1}{2}q(x,t)^{\frac{\alpha-3}{2}}\dot{q}(x,t))^2\right]\frac{\partial}{\partial \theta_i}(p(x,\theta)~dx.$$ I am wondering how can the right hand side of the above be written as a product of two integrals as in (A.19). As this is appeared in Annals of Statistics, I am not in a position to doubt this as well.",,"['statistics', 'ordinary-differential-equations', 'differential-geometry']"
21,Nice corollaries to Poincaré-Bendixson theorem,Nice corollaries to Poincaré-Bendixson theorem,,"I am interested in applications of Poincaré-Bendixson theorem not (explicitely) related to ODEs. Let $X \in C^1(\mathbb{R}^2,\mathbb{R}^2)$ , $(t_0,x_0) \in \mathbb{R} \times \mathbb{R}^2$ and $x \in C^1(\mathbb{R},\mathbb{R}^2)$ a solution to the IVP $\begin{cases}x'=X(x) \\ x(t_0)=x_0\end{cases}$ The $\omega$ -limit of $x_0$ (or of $x$ ) is $\omega(x_0)=\{ y \in \mathbb{R}^2 : \exists (t_n) \ \text{such that} \ t_n \to + \infty, \ x(t_n)\to y \}$ . Theorem (Poincaré-Bendixson) If $\omega(x_0)$ is nonempty, compact and does not contain any zero of $X$ , then $\omega(x_0)$ is a periodic orbit. Some consequences: Theorem ( $C^1$ -version of Brouwer's fixed point theorem in dimension two) Let $f : \overline{D} \to \overline{D}$ be a $C^1$ function from the closed unit disk to itself. Then $f$ has a fixed point. Theorem ( $C^1$ -version of the hairy ball theorem in dimension two) A $C^1$ -vector field on $\mathbb{S}^2$ has a zero. Do you know other consequences of Poincaré-Bendixson theorem not related to differential equations? For example, can the fundamental theorem of algebra be proved like that?","I am interested in applications of Poincaré-Bendixson theorem not (explicitely) related to ODEs. Let , and a solution to the IVP The -limit of (or of ) is . Theorem (Poincaré-Bendixson) If is nonempty, compact and does not contain any zero of , then is a periodic orbit. Some consequences: Theorem ( -version of Brouwer's fixed point theorem in dimension two) Let be a function from the closed unit disk to itself. Then has a fixed point. Theorem ( -version of the hairy ball theorem in dimension two) A -vector field on has a zero. Do you know other consequences of Poincaré-Bendixson theorem not related to differential equations? For example, can the fundamental theorem of algebra be proved like that?","X \in C^1(\mathbb{R}^2,\mathbb{R}^2) (t_0,x_0) \in \mathbb{R} \times \mathbb{R}^2 x \in C^1(\mathbb{R},\mathbb{R}^2) \begin{cases}x'=X(x) \\ x(t_0)=x_0\end{cases} \omega x_0 x \omega(x_0)=\{ y \in \mathbb{R}^2 : \exists (t_n) \ \text{such that} \ t_n \to + \infty, \ x(t_n)\to y \} \omega(x_0) X \omega(x_0) C^1 f : \overline{D} \to \overline{D} C^1 f C^1 C^1 \mathbb{S}^2","['ordinary-differential-equations', 'dynamical-systems']"
22,Looking for a Lyapunov function for the next system,Looking for a Lyapunov function for the next system,,"I am really stuck looking for a Lypaunov candidate for the next system (which in simulation is stable). $$ \dot{x} = -(A+A^T)x + Ay \\ \dot{y} = K(x-y) $$ where x and y are vectors in R^3, A is a time varying matrix such that $A+A^T > 0$, so $x^TAx > 0$. And $K$ is $kI$, where $I$ is the identity matrix and $k$ a positive real constant. I have tried as Lyapunov candidates $||x||^2+||y||^2$, $||x-y||^2$, $||x+y||^2$ and $||x^Ty||^2$, but I always find cross terms in the derivative that I can not eliminate. Any other clues or hints? Many thanks in advance some computations in order to follow the problem: $$V_1 = \frac{1}{2}(||x||^2 + ||y||^2)$$ $$\dot{V}_1 = x^T\dot{x} + y^T\dot{y} = -x^T(A+A^T)x -y^TKy + x^T(A+K)y$$ $$V_2 = \frac{1}{2}||x-y||^2$$ $$\dot{V}_2 = (x-y)^T(\dot{x}-\dot{y})=-x^T(A+A^T)x -y^TKy + x^T(A+K)y  -x^TKx + x^TKy + y^T(A+A^T)x - y^TAy$$ $$V_3 = \frac{1}{2}||x^Ty||^2$$ $$\dot{V}_3 = x^T\dot{y}+y^T\dot{x} = -y^T(A+A^T)y+y^TAy+x^TKx-x^TKy $$","I am really stuck looking for a Lypaunov candidate for the next system (which in simulation is stable). $$ \dot{x} = -(A+A^T)x + Ay \\ \dot{y} = K(x-y) $$ where x and y are vectors in R^3, A is a time varying matrix such that $A+A^T > 0$, so $x^TAx > 0$. And $K$ is $kI$, where $I$ is the identity matrix and $k$ a positive real constant. I have tried as Lyapunov candidates $||x||^2+||y||^2$, $||x-y||^2$, $||x+y||^2$ and $||x^Ty||^2$, but I always find cross terms in the derivative that I can not eliminate. Any other clues or hints? Many thanks in advance some computations in order to follow the problem: $$V_1 = \frac{1}{2}(||x||^2 + ||y||^2)$$ $$\dot{V}_1 = x^T\dot{x} + y^T\dot{y} = -x^T(A+A^T)x -y^TKy + x^T(A+K)y$$ $$V_2 = \frac{1}{2}||x-y||^2$$ $$\dot{V}_2 = (x-y)^T(\dot{x}-\dot{y})=-x^T(A+A^T)x -y^TKy + x^T(A+K)y  -x^TKx + x^TKy + y^T(A+A^T)x - y^TAy$$ $$V_3 = \frac{1}{2}||x^Ty||^2$$ $$\dot{V}_3 = x^T\dot{y}+y^T\dot{x} = -y^T(A+A^T)y+y^TAy+x^TKx-x^TKy $$",,['ordinary-differential-equations']
23,"Symmetry, change of variables","Symmetry, change of variables",,"I am having trouble understanding a section in these notes . It is on page 3. Section 3 -- Discretization of the Korteweg-de Vries equation . I don't understand why $$V_4=x∂_x+3t∂_t-2u∂_u$$ generates a symmetry group of the KdV. I see that it generates the transformation $$(x',t',u')= (x\exp(\epsilon), 3t\exp(\epsilon), -2u\exp(\epsilon))$$ So $u'_{t'}-6u'u'_{x'}+u'_{x'x'x'}=-{2\over 3}u_t-24\exp(\epsilon)uu_x-2\exp(-2\epsilon)u_{xxx}$ How does this vanish (so that we get symmetry) given that $u$ satisfies the KdV? Is it possible that I have misunderstood something, such that from ""I see that..."" onwards I have been barking up the wrong tree? Help would be very much appreciated!","I am having trouble understanding a section in these notes . It is on page 3. Section 3 -- Discretization of the Korteweg-de Vries equation . I don't understand why $$V_4=x∂_x+3t∂_t-2u∂_u$$ generates a symmetry group of the KdV. I see that it generates the transformation $$(x',t',u')= (x\exp(\epsilon), 3t\exp(\epsilon), -2u\exp(\epsilon))$$ So $u'_{t'}-6u'u'_{x'}+u'_{x'x'x'}=-{2\over 3}u_t-24\exp(\epsilon)uu_x-2\exp(-2\epsilon)u_{xxx}$ How does this vanish (so that we get symmetry) given that $u$ satisfies the KdV? Is it possible that I have misunderstood something, such that from ""I see that..."" onwards I have been barking up the wrong tree? Help would be very much appreciated!",,"['ordinary-differential-equations', 'lie-groups']"
24,Uniqueness result in linear differential equation of degree $n$.,Uniqueness result in linear differential equation of degree .,n,"Suppose that $f$ is such that $$f^{(n)}=\sum_{j=0}^{n-1}a_jf^{(j)}$$ Some little work is needed to get to ($a_j=0$ if $j<0$) $${f^{(n + 1)}} = \sum\limits_{j = 0}^{n - 1} {\left( {{a_{j - 1}} + {a_{n - 1}}{a_j}} \right)} {f^{(j)}}$$ and $${f^{(n + 2)}} = \sum\limits_{j = 0}^{n - 1} {\left( {{a_{j - 2}} + {a_{n - 1}}{a_{j - 1}} + {a_{n - 2}}a{  _j} + {a_j}a_{n - 1}^2} \right)} {f^{(j)}}$$ This evidences the increased difficulty in finding a closed form for $f^{n+k}$. However, we can prove by induction that -setting $N=\max(1,|a_0|,\dots,|a_{n-1}|)$ - we have $${f^{(n + k)}} = \sum\limits_{j = 0}^{n - 1} {{b_{jk}}} {f^{(j)}}$$ with each $b_{jk}\leq 2^kN^{k+1}$. Just as an example: $$\left| {{a_{j - 1}} + {a_{n - 1}}{a_j}} \right| \leqslant \left| {{a_{j - 1}}} \right| \cdot 1 + \left| {{a_{n - 1}}} \right|\left| {{a_j}} \right| \leqslant N \cdot N + N \cdot N = 2{N^2}$$ $$\eqalign{   & \left| {{a_{j - 2}} + {a_{n - 1}}{a_{j - 1}} + {a_{n - 2}}{a_j} + {a_j}a_{n - 1}^2} \right| \leqslant   \cr    & \left| {{a_{j - 2}}} \right| \cdot 1 \cdot 1 + \left| {{a_{n - 1}}} \right|\left| {{a_{j - 1}}} \right| \cdot 1 + \left| {{a_{n - 2}}} \right|\left| {{a_j}} \right| \cdot 1 + \left| {{a_j}} \right|\left| {a_{n - 1}^2} \right| \leqslant   \cr    & {N^3} + {N^3} + {N^3} + {N^3} = 4{N^3} \cr} $$ Now, I need to show that for each particular $x$ there exists some $M$ such that, $$\left| {{f^{(n + k)}}\left( x \right)} \right| \leqslant {2^k}{N^{k + 1}}M$$ for each $k$. This with some linear algebra establishes a uniqueness theorem and provides the general solution to this equations. Could you hint me so I can finish this?","Suppose that $f$ is such that $$f^{(n)}=\sum_{j=0}^{n-1}a_jf^{(j)}$$ Some little work is needed to get to ($a_j=0$ if $j<0$) $${f^{(n + 1)}} = \sum\limits_{j = 0}^{n - 1} {\left( {{a_{j - 1}} + {a_{n - 1}}{a_j}} \right)} {f^{(j)}}$$ and $${f^{(n + 2)}} = \sum\limits_{j = 0}^{n - 1} {\left( {{a_{j - 2}} + {a_{n - 1}}{a_{j - 1}} + {a_{n - 2}}a{  _j} + {a_j}a_{n - 1}^2} \right)} {f^{(j)}}$$ This evidences the increased difficulty in finding a closed form for $f^{n+k}$. However, we can prove by induction that -setting $N=\max(1,|a_0|,\dots,|a_{n-1}|)$ - we have $${f^{(n + k)}} = \sum\limits_{j = 0}^{n - 1} {{b_{jk}}} {f^{(j)}}$$ with each $b_{jk}\leq 2^kN^{k+1}$. Just as an example: $$\left| {{a_{j - 1}} + {a_{n - 1}}{a_j}} \right| \leqslant \left| {{a_{j - 1}}} \right| \cdot 1 + \left| {{a_{n - 1}}} \right|\left| {{a_j}} \right| \leqslant N \cdot N + N \cdot N = 2{N^2}$$ $$\eqalign{   & \left| {{a_{j - 2}} + {a_{n - 1}}{a_{j - 1}} + {a_{n - 2}}{a_j} + {a_j}a_{n - 1}^2} \right| \leqslant   \cr    & \left| {{a_{j - 2}}} \right| \cdot 1 \cdot 1 + \left| {{a_{n - 1}}} \right|\left| {{a_{j - 1}}} \right| \cdot 1 + \left| {{a_{n - 2}}} \right|\left| {{a_j}} \right| \cdot 1 + \left| {{a_j}} \right|\left| {a_{n - 1}^2} \right| \leqslant   \cr    & {N^3} + {N^3} + {N^3} + {N^3} = 4{N^3} \cr} $$ Now, I need to show that for each particular $x$ there exists some $M$ such that, $$\left| {{f^{(n + k)}}\left( x \right)} \right| \leqslant {2^k}{N^{k + 1}}M$$ for each $k$. This with some linear algebra establishes a uniqueness theorem and provides the general solution to this equations. Could you hint me so I can finish this?",,['ordinary-differential-equations']
25,"$\frac{dy}{dx}=1+\frac{2}{x+y}$ solution by an ""integrating term""","solution by an ""integrating term""",\frac{dy}{dx}=1+\frac{2}{x+y},"I though about this trick and then found an example to apply it to: $$\frac{dy}{dx}=1+\frac{2}{x+y}$$ This is the trick: add  $\frac{dx}{dx}=1$ to both parts $$\frac{dy}{dx}+\frac{dx}{dx}=1+\frac{2}{x+y}+1$$ Using the linearity of $d$ $$\frac{d(x+y)}{dx}=2\frac{1+(x+y)}{x+y}$$ $$\frac{(x+y)d(x+y)}{1+(x+y)}=2dx$$ $$d(x+y)-\frac{d(x+y)}{1+(x+y)}=2dx$$ $$-\frac{d(x+y)}{1+(x+y)}=2dx-d(x+y)$$ Now $2dx-d(x+y)=2dx-dx-dy=dx-dy=d(x-y)$ $$-\frac{d(x+y)}{1+(x+y)}=d(x-y)$$ $$\frac{d(1+x+y)}{1+(x+y)}=d(y-x)$$ Integrating: $$\ln|1+x+y|=(y-x)+\ln C$$ $$1+x+y=C\exp\left(y-x\right)$$ Is this a one-off case, or a particular example of a certain method? Does anyone know more examples of ODE's that can be solved similarly? I know the integrating multiplier theory quite well, but this one seems like something extra to that.","I though about this trick and then found an example to apply it to: $$\frac{dy}{dx}=1+\frac{2}{x+y}$$ This is the trick: add  $\frac{dx}{dx}=1$ to both parts $$\frac{dy}{dx}+\frac{dx}{dx}=1+\frac{2}{x+y}+1$$ Using the linearity of $d$ $$\frac{d(x+y)}{dx}=2\frac{1+(x+y)}{x+y}$$ $$\frac{(x+y)d(x+y)}{1+(x+y)}=2dx$$ $$d(x+y)-\frac{d(x+y)}{1+(x+y)}=2dx$$ $$-\frac{d(x+y)}{1+(x+y)}=2dx-d(x+y)$$ Now $2dx-d(x+y)=2dx-dx-dy=dx-dy=d(x-y)$ $$-\frac{d(x+y)}{1+(x+y)}=d(x-y)$$ $$\frac{d(1+x+y)}{1+(x+y)}=d(y-x)$$ Integrating: $$\ln|1+x+y|=(y-x)+\ln C$$ $$1+x+y=C\exp\left(y-x\right)$$ Is this a one-off case, or a particular example of a certain method? Does anyone know more examples of ODE's that can be solved similarly? I know the integrating multiplier theory quite well, but this one seems like something extra to that.",,['ordinary-differential-equations']
26,PDEs with non-local terms,PDEs with non-local terms,,"Not sure if I've used the correct terminology here (`non-local'). I think the lack of knowing the correct terminology is why I haven't been able to find any information about my query thus far. I'm interested in particular systems of semi-linear first-order (functional?) PDEs. One example where I seek solutions defined on $\mathbb{R}_+ \times \mathbb{R}_+$ is: $\frac{\partial F_1(z,t)}{\partial t} + \gamma \frac{\partial F_1(z,t)}{\partial z} = \lambda F_2(z,t) - \beta F_1(z,t) F_1(0,t)$ $\frac{\partial F_2(z,t)}{\partial t} = \beta F_1(z,t) F_1(0,t) - \lambda F_2(z,t)$ when $z > 0$; and: $\frac{\partial F_1(z,t)}{\partial t} = \lambda F_2(z,t) - \beta F_1(z,t) F_1(0,t)$ $\frac{\partial F_2(z,t)}{\partial t} = \beta F_1(z,t) F_1(0,t) - \lambda F_2(z,t)$ when $z = 0$. Boundary conditions are $F_1(z,0) = \sigma(z)$ and $F_2(z,0) = 0$ for all $z$. The thing which appears to make these special is the presence of the `non-local' terms $F_1(0,t)$. I guess I could try and solve the system using finite difference methods, but I was wondering, is there anything better that can be done here, e.g. can the method of characteristics still be used? I'm not particularly familiar with PDEs so any help would be gratefully received --- do systems like this even have a name, are there any references I should see? Thanks!","Not sure if I've used the correct terminology here (`non-local'). I think the lack of knowing the correct terminology is why I haven't been able to find any information about my query thus far. I'm interested in particular systems of semi-linear first-order (functional?) PDEs. One example where I seek solutions defined on $\mathbb{R}_+ \times \mathbb{R}_+$ is: $\frac{\partial F_1(z,t)}{\partial t} + \gamma \frac{\partial F_1(z,t)}{\partial z} = \lambda F_2(z,t) - \beta F_1(z,t) F_1(0,t)$ $\frac{\partial F_2(z,t)}{\partial t} = \beta F_1(z,t) F_1(0,t) - \lambda F_2(z,t)$ when $z > 0$; and: $\frac{\partial F_1(z,t)}{\partial t} = \lambda F_2(z,t) - \beta F_1(z,t) F_1(0,t)$ $\frac{\partial F_2(z,t)}{\partial t} = \beta F_1(z,t) F_1(0,t) - \lambda F_2(z,t)$ when $z = 0$. Boundary conditions are $F_1(z,0) = \sigma(z)$ and $F_2(z,0) = 0$ for all $z$. The thing which appears to make these special is the presence of the `non-local' terms $F_1(0,t)$. I guess I could try and solve the system using finite difference methods, but I was wondering, is there anything better that can be done here, e.g. can the method of characteristics still be used? I'm not particularly familiar with PDEs so any help would be gratefully received --- do systems like this even have a name, are there any references I should see? Thanks!",,"['ordinary-differential-equations', 'partial-differential-equations']"
27,Special functions and diff eq's ......,Special functions and diff eq's ......,,"They're are all these methods of dealing with linear second order diff eq's: generating function; recurrence relation; Rodrigues differential form; Schlafi integral form; associated form; second form; shifted form; series form; you can use on differential equations with special names: Airy;  Bessel;  Chebyshev;  Gauss hypergeometric; Hermite; Jacobi; Laguerre; Legendre There are at least 100 ideas in these two little lists, and apparently one can start from anything in the first list and derive any other quantity in that list directly from it, using equations in the second list as examples - how in the world does one make sense of all this? Where should one start? What am I missing? I can't even begin with all this there's so much going on :( Is there not some standard way to begin with something & derive everything from it in an obvious way, & a way to remember all the equations - or a good reasong why you shouldn't care about remembering their names? :(","They're are all these methods of dealing with linear second order diff eq's: generating function; recurrence relation; Rodrigues differential form; Schlafi integral form; associated form; second form; shifted form; series form; you can use on differential equations with special names: Airy;  Bessel;  Chebyshev;  Gauss hypergeometric; Hermite; Jacobi; Laguerre; Legendre There are at least 100 ideas in these two little lists, and apparently one can start from anything in the first list and derive any other quantity in that list directly from it, using equations in the second list as examples - how in the world does one make sense of all this? Where should one start? What am I missing? I can't even begin with all this there's so much going on :( Is there not some standard way to begin with something & derive everything from it in an obvious way, & a way to remember all the equations - or a good reasong why you shouldn't care about remembering their names? :(",,['ordinary-differential-equations']
28,System of non-linear ODE's,System of non-linear ODE's,,do you have any suggestions to solve analytically the Non-linear ODE system $\dot x=18 x^2 y-3p x^2+6p xy$ $\dot y=18 x^2 y-6p xy $ where $p$ is a real constant. Thank you very much cheers,do you have any suggestions to solve analytically the Non-linear ODE system $\dot x=18 x^2 y-3p x^2+6p xy$ $\dot y=18 x^2 y-6p xy $ where $p$ is a real constant. Thank you very much cheers,,"['ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
29,Reference for Shooting Method,Reference for Shooting Method,,"The bounty expires in 4 days . Answers to this question are eligible for a +500 reputation bounty. JP McCarthy is looking for an answer from a reputable source . Consider the following setup. We have a second order boundary value problem: $$\dfrac{d^2y}{dx^2}=F(x,y,dy/dx);\qquad y(x_0)=y_0,\quad y(x_f)=y_f.$$ A numerical approach is to almost first write as two first order ivps: $$\begin{aligned} \frac{dy}{dx}&=v,\qquad &y(x_0)=y_0 \\ \frac{dv}{dx}&=F(x,y,v),\qquad &v(x_0)=? \end{aligned}$$ Suppose we have some numerical method (some Runge-Kutta, mostly interested here in simple RK1, Euler's Method). Now, we can take a guess at $v(x_0)$ : for example we can shoot straight at $y(x_f)$ by using our numerical method to attack a system of IVPs by taking $v(x_0)=v_0$ where: $$v_0=\dfrac{y(x_f)-y(x_0)}{x_f-x_0}.$$ We implement our method and we get, as an approximation to $y(x_f)=y_f$ , we get, say: $$y_f(v_0)\approx y_f.$$ Say the discrepancy is: $$\varepsilon(v_0)=y_f(v_0)-y_f.$$ Now we can plot $\varepsilon$ vs $v_0$ and where $\varepsilon$ is zero we have approximations to the solution that at least satisfy the second boundary condition. When the original ode is suitably linear, it is the case for any two points $(v_a,y_a-y_f)$ and $(v_b,y_b-y_f)$ on this curve, we have $\varepsilon$ is zero at: $$v_a+\frac{y_f-y_a}{y_b-y_a}(v_a-v_b).$$ Because the curve in this case is a line. This formula can be understood on a more basic level if you start drawing pictures in the $x$ - $y$ plane. Is there a good, online and available reference that discusses this curve? What are the conditions on the ode on it being a straight line? What are the conditions on the ode that there is a unique root?","The bounty expires in 4 days . Answers to this question are eligible for a +500 reputation bounty. JP McCarthy is looking for an answer from a reputable source . Consider the following setup. We have a second order boundary value problem: A numerical approach is to almost first write as two first order ivps: Suppose we have some numerical method (some Runge-Kutta, mostly interested here in simple RK1, Euler's Method). Now, we can take a guess at : for example we can shoot straight at by using our numerical method to attack a system of IVPs by taking where: We implement our method and we get, as an approximation to , we get, say: Say the discrepancy is: Now we can plot vs and where is zero we have approximations to the solution that at least satisfy the second boundary condition. When the original ode is suitably linear, it is the case for any two points and on this curve, we have is zero at: Because the curve in this case is a line. This formula can be understood on a more basic level if you start drawing pictures in the - plane. Is there a good, online and available reference that discusses this curve? What are the conditions on the ode on it being a straight line? What are the conditions on the ode that there is a unique root?","\dfrac{d^2y}{dx^2}=F(x,y,dy/dx);\qquad y(x_0)=y_0,\quad y(x_f)=y_f. \begin{aligned}
\frac{dy}{dx}&=v,\qquad &y(x_0)=y_0
\\ \frac{dv}{dx}&=F(x,y,v),\qquad &v(x_0)=?
\end{aligned} v(x_0) y(x_f) v(x_0)=v_0 v_0=\dfrac{y(x_f)-y(x_0)}{x_f-x_0}. y(x_f)=y_f y_f(v_0)\approx y_f. \varepsilon(v_0)=y_f(v_0)-y_f. \varepsilon v_0 \varepsilon (v_a,y_a-y_f) (v_b,y_b-y_f) \varepsilon v_a+\frac{y_f-y_a}{y_b-y_a}(v_a-v_b). x y","['ordinary-differential-equations', 'numerical-methods', 'boundary-value-problem', 'runge-kutta-methods', 'eulers-method']"
30,Square of a differential [duplicate],Square of a differential [duplicate],,"This question already has answers here : Square of a second derivative is the fourth derivative (4 answers) Closed 8 years ago . Just wondering, is this valid: $$ \left(\frac{df}{dx}\right)^2=\frac{d^{2}f}{dx^{2}} $$","This question already has answers here : Square of a second derivative is the fourth derivative (4 answers) Closed 8 years ago . Just wondering, is this valid: $$ \left(\frac{df}{dx}\right)^2=\frac{d^{2}f}{dx^{2}} $$",,"['calculus', 'ordinary-differential-equations']"
31,Should we re-define Sine?,Should we re-define Sine?,,"Sine is usually defined as the ratio of the opposite side to an angle to the hypotenuse in a right angle triangle . Another common definition is based on the unit circle. However I think these geometrical definitions could lead to confusion and misconception. I’ve been wondering about this for a while so now I’m bringing some reasons for this argue and want to know whether this taught is right or not. Here’s the confusion which this definition causes: When taking oscillatory motion lessons, students are told about the Differential Equations. The very first equation they learn is $$\frac{d^2y}{dx^2}+ky=0$$ Then the solution for that is written as $y=A\sin(\sqrt{k}x+\phi)$ and questions begin to raise: “Where did that sine come from?!”. The typical answer would be: “See, if you take the second derivative and put it in, it satisfies the equation”. “Yes, that’s true but… where is the circle? Where is the angle? There’s just an object connected to a spring”. And so on, many students have problems. This confusion occurs because, when you first defined the sine function for them, there was a circle, and the argument was an angle and so on. It’s no wonder if those less curious students accept this phenomenon (that an object connected to a spring moves like sine function) as accident or something like that and pass. To solve this, I’m suggesting a re-definition. We can define sine as The answer to $y’’+ay=0$ considering $y(0)=0$ and $y’(0)=1$ . It can be proved (with a little effort) that the opposite/hypotenuse ratio also obeys the same differential equation so by [the new] definition, this ratio would be equal to sine of the angle. This should resolve the issues discussed above. Another interesting subject is pi which is related to this discussion. I think I can convince you that pi could be quite confusing: One day, looking at the formulas in some book or other, I discovered a formula for the frequency of a resonant circuit. There was a mystery about this number that I didn't understand as a youth, but this was a great thing, and the result as that I looked for pi everywhere. [??Something missing here] which was f = 1/2 pi LC, where L is the inductance and C [is] the capacitance of the [capacitor, and there was also a pi. But where is the] circle? You laugh, but I was very serious then. Pi was a thing with circles, and here is pi coming out of an electric circuit. Where was the circle? Do those of you who laughed know how that comes about? I have to love the thing. I have to look for it. I have to think about it. And then I realized, of course, that the coils are made in circles. About a half year later, I found another book which gave the inductance of round coils and square coils, and there were other pi's in those formulas. I began to think about it again, and I realized that the pi did not come from the circular coils. I understand it better now; but in my heart I still don't know where that circle is, where that pi comes from. Richard Feynman – “What is science?” Another example from the famous article: THERE IS A story about two friends, who were classmates in high school, talking about their jobs. One of them became a statistician and was working on population trends. He showed a reprint to his former classmate. The reprint started, as usual, with the Gaussian distribution and the statistician explained to his former classmate the meaning of the symbols for the actual population, for the average population, and so on. His classmate was a bit incredulous and was not quite sure whether the statistician was pulling his leg. ""How can you know that?"" was his query. ""And what is this symbol here?"" ""Oh,"" said the statistician, ""this is pi."" ""What is that?"" ""The ratio of the circumference of the circle to its diameter."" ""Well, now you are pushing your joke too far,"" said the classmate, ""surely the population has nothing to do with the circumference of the circle."" Naturally, we are inclined to smile about the simplicity of the classmate's approach. Nevertheless, when I heard this story, I had to admit to an eerie feeling because, surely, the reaction of the classmate betrayed only plain common sense. I was even more confused when, not many days later, someone came to me and expressed his bewilderment [1 The remark to be quoted was made by F. Werner when he was a student in Princeton.] with the fact that we make a rather narrow selection when choosing the data on which we test our theories. ""How do we know that, if we made a theory which focuses its attention on phenomena we disregard and disregards some of the phenomena now commanding our attention, that we could not build another theory which has little in common with the present one but which, nevertheless, explains just as many phenomena as the present theory?"" It has to be admitted that we have no definite evidence that there is no such theory. The preceding two stories illustrate the two main points which are the subjects of the present discourse. The first point is that mathematical concepts turn up in entirely unexpected connections. Moreover, they often permit an unexpectedly close and accurate description of the phenomena in these connections. Secondly, … Eugene Wigner – “The Unreasonable Effectiveness of Mathematics in the Natural Sciences” So after all these stories from big minds, we can see that curious minds have difficulty relating pi to equations which don’t even involve a circle. But if we instead define pi as half of sine function period (newly defined one), then usage of pi in absence of circles and triangles would not be a surprise. At the end of this long post, I want to ask for opinions. I wonder if all what I said above makes sense or not? Math is a precise area and definitions are everything, so I think what I’m asking here is an important question. Edit: A few friends here have marked this as ""Opinion Based"". Thanks for reading my question, but I really don't see how can a definition be ""Opinion Based"", in a precise field like math. So the only way I can imagine is that the two be ""equivalent"", which is what I'm somehow concluding from the answers, but still looking for a clear proof for that point which I don't see here.","Sine is usually defined as the ratio of the opposite side to an angle to the hypotenuse in a right angle triangle . Another common definition is based on the unit circle. However I think these geometrical definitions could lead to confusion and misconception. I’ve been wondering about this for a while so now I’m bringing some reasons for this argue and want to know whether this taught is right or not. Here’s the confusion which this definition causes: When taking oscillatory motion lessons, students are told about the Differential Equations. The very first equation they learn is Then the solution for that is written as and questions begin to raise: “Where did that sine come from?!”. The typical answer would be: “See, if you take the second derivative and put it in, it satisfies the equation”. “Yes, that’s true but… where is the circle? Where is the angle? There’s just an object connected to a spring”. And so on, many students have problems. This confusion occurs because, when you first defined the sine function for them, there was a circle, and the argument was an angle and so on. It’s no wonder if those less curious students accept this phenomenon (that an object connected to a spring moves like sine function) as accident or something like that and pass. To solve this, I’m suggesting a re-definition. We can define sine as The answer to considering and . It can be proved (with a little effort) that the opposite/hypotenuse ratio also obeys the same differential equation so by [the new] definition, this ratio would be equal to sine of the angle. This should resolve the issues discussed above. Another interesting subject is pi which is related to this discussion. I think I can convince you that pi could be quite confusing: One day, looking at the formulas in some book or other, I discovered a formula for the frequency of a resonant circuit. There was a mystery about this number that I didn't understand as a youth, but this was a great thing, and the result as that I looked for pi everywhere. [??Something missing here] which was f = 1/2 pi LC, where L is the inductance and C [is] the capacitance of the [capacitor, and there was also a pi. But where is the] circle? You laugh, but I was very serious then. Pi was a thing with circles, and here is pi coming out of an electric circuit. Where was the circle? Do those of you who laughed know how that comes about? I have to love the thing. I have to look for it. I have to think about it. And then I realized, of course, that the coils are made in circles. About a half year later, I found another book which gave the inductance of round coils and square coils, and there were other pi's in those formulas. I began to think about it again, and I realized that the pi did not come from the circular coils. I understand it better now; but in my heart I still don't know where that circle is, where that pi comes from. Richard Feynman – “What is science?” Another example from the famous article: THERE IS A story about two friends, who were classmates in high school, talking about their jobs. One of them became a statistician and was working on population trends. He showed a reprint to his former classmate. The reprint started, as usual, with the Gaussian distribution and the statistician explained to his former classmate the meaning of the symbols for the actual population, for the average population, and so on. His classmate was a bit incredulous and was not quite sure whether the statistician was pulling his leg. ""How can you know that?"" was his query. ""And what is this symbol here?"" ""Oh,"" said the statistician, ""this is pi."" ""What is that?"" ""The ratio of the circumference of the circle to its diameter."" ""Well, now you are pushing your joke too far,"" said the classmate, ""surely the population has nothing to do with the circumference of the circle."" Naturally, we are inclined to smile about the simplicity of the classmate's approach. Nevertheless, when I heard this story, I had to admit to an eerie feeling because, surely, the reaction of the classmate betrayed only plain common sense. I was even more confused when, not many days later, someone came to me and expressed his bewilderment [1 The remark to be quoted was made by F. Werner when he was a student in Princeton.] with the fact that we make a rather narrow selection when choosing the data on which we test our theories. ""How do we know that, if we made a theory which focuses its attention on phenomena we disregard and disregards some of the phenomena now commanding our attention, that we could not build another theory which has little in common with the present one but which, nevertheless, explains just as many phenomena as the present theory?"" It has to be admitted that we have no definite evidence that there is no such theory. The preceding two stories illustrate the two main points which are the subjects of the present discourse. The first point is that mathematical concepts turn up in entirely unexpected connections. Moreover, they often permit an unexpectedly close and accurate description of the phenomena in these connections. Secondly, … Eugene Wigner – “The Unreasonable Effectiveness of Mathematics in the Natural Sciences” So after all these stories from big minds, we can see that curious minds have difficulty relating pi to equations which don’t even involve a circle. But if we instead define pi as half of sine function period (newly defined one), then usage of pi in absence of circles and triangles would not be a surprise. At the end of this long post, I want to ask for opinions. I wonder if all what I said above makes sense or not? Math is a precise area and definitions are everything, so I think what I’m asking here is an important question. Edit: A few friends here have marked this as ""Opinion Based"". Thanks for reading my question, but I really don't see how can a definition be ""Opinion Based"", in a precise field like math. So the only way I can imagine is that the two be ""equivalent"", which is what I'm somehow concluding from the answers, but still looking for a clear proof for that point which I don't see here.",\frac{d^2y}{dx^2}+ky=0 y=A\sin(\sqrt{k}x+\phi) y’’+ay=0 y(0)=0 y’(0)=1,"['ordinary-differential-equations', 'trigonometry', 'definition']"
32,The difference between $\frac{\partial^2 y}{\partial x^2}$ and $\frac{\partial y^2}{\partial x^2}$,The difference between  and,\frac{\partial^2 y}{\partial x^2} \frac{\partial y^2}{\partial x^2},The question is $y''=2y^3$. I know I can substitute $y'=p$. My question is if I can seperate x and y and integrate both sides twice?,The question is $y''=2y^3$. I know I can substitute $y'=p$. My question is if I can seperate x and y and integrate both sides twice?,,['ordinary-differential-equations']
33,How can I solve this O.D.E.?,How can I solve this O.D.E.?,,I need to find the solution of  $$x''-2x'+x=\sum_{n=1}^Ne^{-nt}$$ I was thinking undetermined coefficients. Is there another way?,I need to find the solution of  $$x''-2x'+x=\sum_{n=1}^Ne^{-nt}$$ I was thinking undetermined coefficients. Is there another way?,,['ordinary-differential-equations']
34,What is the meaning of equilibrium solution?,What is the meaning of equilibrium solution?,,What are the equilibrium solutions for the differential equation $\dfrac{\mathrm{d}y}{\mathrm{d}t} = 0.2\left(y-3\right)\left(y+2\right)$ My Question: What does equilibrium solution mean in this context of this problem?,What are the equilibrium solutions for the differential equation $\dfrac{\mathrm{d}y}{\mathrm{d}t} = 0.2\left(y-3\right)\left(y+2\right)$ My Question: What does equilibrium solution mean in this context of this problem?,,"['ordinary-differential-equations', 'terminology']"
35,Differential Equation : $f '' = f '$,Differential Equation :,f '' = f ',"I came across this question from my textbook, but in the text it only talked about $f ''+f =0$ when discussing trig functions. Now this questions at the end of the log and exp chapter with no discussion of it in this chapter. If anybody could please help. It says: Find all functions f which are twice differentiable and satisfy the equation $f '' = f '$.","I came across this question from my textbook, but in the text it only talked about $f ''+f =0$ when discussing trig functions. Now this questions at the end of the log and exp chapter with no discussion of it in this chapter. If anybody could please help. It says: Find all functions f which are twice differentiable and satisfy the equation $f '' = f '$.",,"['calculus', 'ordinary-differential-equations']"
36,"How do I integrate $\int\dfrac{e^x - e^{-x}}{e^x + e^{-x}}\,dx$?",How do I integrate ?,"\int\dfrac{e^x - e^{-x}}{e^x + e^{-x}}\,dx","$$\displaystyle\int\frac{e^x - e^{-x}}{e^x + e^{-x}}\,dx$$ In Mathematica: Integrate[(E^x - E^-x)/(E^x + E^-x), x] I'm using the substitution $u=e^x + e^{-x}$, so $du = e^x + e^{-x}$ $e^{-x} = u - e^x$ $e^{x} = u - e^{-x}$ Substituting into the the equation gives: $$\int\frac{2e^x-u}{u}du$$ But I'm not sure where to go from here to get the supposed answer of: $$\log(1 + e^{2x}) - x$$","$$\displaystyle\int\frac{e^x - e^{-x}}{e^x + e^{-x}}\,dx$$ In Mathematica: Integrate[(E^x - E^-x)/(E^x + E^-x), x] I'm using the substitution $u=e^x + e^{-x}$, so $du = e^x + e^{-x}$ $e^{-x} = u - e^x$ $e^{x} = u - e^{-x}$ Substituting into the the equation gives: $$\int\frac{2e^x-u}{u}du$$ But I'm not sure where to go from here to get the supposed answer of: $$\log(1 + e^{2x}) - x$$",,"['calculus', 'ordinary-differential-equations']"
37,solution of first order differential equation and maximal interval,solution of first order differential equation and maximal interval,,"Find the solution of $x' = x^2t$ with initial value $x(0) = x_{0}$. Determine the maximal interval where it exists, depending on $x_{0}$ Please help me find the maximal interval!","Find the solution of $x' = x^2t$ with initial value $x(0) = x_{0}$. Determine the maximal interval where it exists, depending on $x_{0}$ Please help me find the maximal interval!",,['ordinary-differential-equations']
38,What is an example of a second order differential equation for which it is known that there are no smooth solutions?,What is an example of a second order differential equation for which it is known that there are no smooth solutions?,,"I would really appreciate if someone could just write down for me one example of a second order, or higher, differential equation for which it is known that there are no smooth solutions; and it's fine if it's a partial differential equation. At first I thought it would be easy to either come up with an example or else find one by searching google/wiki/arxiv; but now I am not so sure. I have a thing for non-smooth functions, and it just bothers me that I don't even know a single example of this type of differential equation.  Thanks!","I would really appreciate if someone could just write down for me one example of a second order, or higher, differential equation for which it is known that there are no smooth solutions; and it's fine if it's a partial differential equation. At first I thought it would be easy to either come up with an example or else find one by searching google/wiki/arxiv; but now I am not so sure. I have a thing for non-smooth functions, and it just bothers me that I don't even know a single example of this type of differential equation.  Thanks!",,['ordinary-differential-equations']
39,Derivation of the Exponential Nature of $e^x$,Derivation of the Exponential Nature of,e^x,"Presumably, the transcendental number $e$ was first found by taking the power series solution to the (arguably most fundamental) differential equation $f'(x)=f(x)$, with the initial condition $f(0)=1$ and then plugging in $x=1$. My question is, is there any way, other than using the equivalent of the multinomial theorem for power series to demonstrate that $f(nx)=f^n(x)$ for all $n$ and $x$ (i.e., that $f(x)$ is an exponential function) other than this laborious and unaesthetic method? Intuitively, I would expect the solution to be an exponential function but is there a better, somewhat rigorous demonstration of this simple fact?","Presumably, the transcendental number $e$ was first found by taking the power series solution to the (arguably most fundamental) differential equation $f'(x)=f(x)$, with the initial condition $f(0)=1$ and then plugging in $x=1$. My question is, is there any way, other than using the equivalent of the multinomial theorem for power series to demonstrate that $f(nx)=f^n(x)$ for all $n$ and $x$ (i.e., that $f(x)$ is an exponential function) other than this laborious and unaesthetic method? Intuitively, I would expect the solution to be an exponential function but is there a better, somewhat rigorous demonstration of this simple fact?",,"['ordinary-differential-equations', 'exponential-function', 'elementary-functions']"
40,Use Ito's Lemma to compute $d(\log S(t))$ and use this to find the closed form solution of S(t),Use Ito's Lemma to compute  and use this to find the closed form solution of S(t),d(\log S(t)),I am having issues with this practice problem. If someone could help me solve it that would be greatly appreciated! Let $S(t)$ be the stock price that satisfies the BSM model in SDE form $$dS(t) = \mu S(t) dt + \sigma S(t) dW_t$$ where $\mu > 0$ and $\sigma > 0$ are two constants. Use Ito's Lemma to compute $d \log S(t)$ and use this result to find the closed form solution of $S(t)$ .,I am having issues with this practice problem. If someone could help me solve it that would be greatly appreciated! Let be the stock price that satisfies the BSM model in SDE form where and are two constants. Use Ito's Lemma to compute and use this result to find the closed form solution of .,S(t) dS(t) = \mu S(t) dt + \sigma S(t) dW_t \mu > 0 \sigma > 0 d \log S(t) S(t),"['ordinary-differential-equations', 'stochastic-calculus', 'brownian-motion']"
41,The differential equation: $ \arctan (y) = \arctan(x)+C .$,The differential equation:, \arctan (y) = \arctan(x)+C .,"I solved the equation and stalled. Help with decision please. $$(1+y^2)\,dx=(1+y^2)\,dy \iff \int \frac{dx}{1+x^2} = \int\frac{dy}{1+y^2}  $$ Transformed expression for the table of integrals. $$ \arctan (y) = \arctan(x)+C $$ Prompt how to further transform expression.(Find the general solution)","I solved the equation and stalled. Help with decision please. $$(1+y^2)\,dx=(1+y^2)\,dy \iff \int \frac{dx}{1+x^2} = \int\frac{dy}{1+y^2}  $$ Transformed expression for the table of integrals. $$ \arctan (y) = \arctan(x)+C $$ Prompt how to further transform expression.(Find the general solution)",,['ordinary-differential-equations']
42,"Why can a linear ""ordinary"" differential equation have non-linear coefficients of independent variable?","Why can a linear ""ordinary"" differential equation have non-linear coefficients of independent variable?",,"The confusion origins from the fact that $y$ = $x^2$ + $x$ + $1$ is a non linear equation but $y\,'$ = $x^2$ + $x$ + $1$ is a linear differential equation. Why is the non-linearity in independent variable not significant in the case of differential equations? Does the word linear have different meanings for ""normal"" (not differential) equations and differential equations? What would be the best way to make some geometric sense of linear differential equations? (like in the case of linear equation in two variable it is a line.) Mentions of ""differential equation(s)"" in the above questions refer to only the subset of ""ordinary differential equation(s)""","The confusion origins from the fact that = + + is a non linear equation but = + + is a linear differential equation. Why is the non-linearity in independent variable not significant in the case of differential equations? Does the word linear have different meanings for ""normal"" (not differential) equations and differential equations? What would be the best way to make some geometric sense of linear differential equations? (like in the case of linear equation in two variable it is a line.) Mentions of ""differential equation(s)"" in the above questions refer to only the subset of ""ordinary differential equation(s)""","y x^2 x 1 y\,' x^2 x 1","['calculus', 'ordinary-differential-equations', 'derivatives', 'differential-geometry']"
43,Solution to this Differential Equation $f''(x)=f(x)f'(x)$ needed,Solution to this Differential Equation  needed,f''(x)=f(x)f'(x),"I came up with this differential equation and I don't know how to solve it. $$f''(x)=f(x)f'(x)$$ I attempted to solve it several times, but they were all fruitless. Wolfram Alpha says that the solution is $$f(x)=\sqrt{2a} \tan\left({\frac{\sqrt{2a}}{2} \cdot (x+b)}\right),$$ where $a$ and $b$ are constants. How does one get this solution?","I came up with this differential equation and I don't know how to solve it. I attempted to solve it several times, but they were all fruitless. Wolfram Alpha says that the solution is where and are constants. How does one get this solution?","f''(x)=f(x)f'(x) f(x)=\sqrt{2a} \tan\left({\frac{\sqrt{2a}}{2} \cdot (x+b)}\right), a b","['calculus', 'ordinary-differential-equations']"
44,Question about the basin of attraction of the origin,Question about the basin of attraction of the origin,,"Consider the system $$x'=(\epsilon x+2y)(z+1)$$ $$y'=(\epsilon y-x)(z+1)$$ $$z'=-z^3$$ (a) Show that the origin is not asymptotically stable when $\epsilon=0.$ (b) Show that when $\epsilon <0,$ the basin of attraction of the origin contains the region $z>-1.$ I did (a) showing that at least one eigenvalue of the jacobian matrix had not  negative real part. And I'm stuck in (b), I don't understand  what do I have to prove? Can someone help please?","Consider the system $$x'=(\epsilon x+2y)(z+1)$$ $$y'=(\epsilon y-x)(z+1)$$ $$z'=-z^3$$ (a) Show that the origin is not asymptotically stable when $\epsilon=0.$ (b) Show that when $\epsilon <0,$ the basin of attraction of the origin contains the region $z>-1.$ I did (a) showing that at least one eigenvalue of the jacobian matrix had not  negative real part. And I'm stuck in (b), I don't understand  what do I have to prove? Can someone help please?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory', 'basins-of-attraction']"
45,How to determine the order of a differential equation when it's solution is given,How to determine the order of a differential equation when it's solution is given,,We know how to find the solution of a given Homogeneous linear equations with constant coefficients . In assignment I have to find the minimum possible order of a Homogeneous linear equations with constant coefficients having $x^2 sin x$ as a solution. I am really blank and have no idea how to proceed. Kindly help me. Any hint or solution will be helpful to me. Thanks a lot for the help.,We know how to find the solution of a given Homogeneous linear equations with constant coefficients . In assignment I have to find the minimum possible order of a Homogeneous linear equations with constant coefficients having $x^2 sin x$ as a solution. I am really blank and have no idea how to proceed. Kindly help me. Any hint or solution will be helpful to me. Thanks a lot for the help.,,['ordinary-differential-equations']
46,Why do we drop the abolute value bars when doing indefinite integration?,Why do we drop the abolute value bars when doing indefinite integration?,,"Say, for instance, we're trying to solve $\frac{dy}{dx}=y\cos(x)$. Separating the variables, we get: $$\int \frac{1}{y}dy=\int \cos(x) dx.$$ So $\ln|y|=\sin(x)+c \iff |y|=Ae^{\sin(x)}$ (where $A:=e^c$), yielding: $$y=\pm Ae^{\sin(x)}$$, which is inconsistent with Wolfram Alpha's $y=Ae^{\sin(x)}$. Now, I know that $A$ could be either positive or negative, but it's a constant, so it's either one or the other. So, what I'm asking is: how can WA justify dropping the absolute value bars? Thanks","Say, for instance, we're trying to solve $\frac{dy}{dx}=y\cos(x)$. Separating the variables, we get: $$\int \frac{1}{y}dy=\int \cos(x) dx.$$ So $\ln|y|=\sin(x)+c \iff |y|=Ae^{\sin(x)}$ (where $A:=e^c$), yielding: $$y=\pm Ae^{\sin(x)}$$, which is inconsistent with Wolfram Alpha's $y=Ae^{\sin(x)}$. Now, I know that $A$ could be either positive or negative, but it's a constant, so it's either one or the other. So, what I'm asking is: how can WA justify dropping the absolute value bars? Thanks",,['ordinary-differential-equations']
47,Solving sum of exponentials,Solving sum of exponentials,,"Let $x\in\mathbb{R}$, how can $-e^{-x}+e^{x}=a$ be solved? I have already tried to use the sum of exponential formula.","Let $x\in\mathbb{R}$, how can $-e^{-x}+e^{x}=a$ be solved? I have already tried to use the sum of exponential formula.",,"['ordinary-differential-equations', 'summation']"
48,Why - not how - do you solve Differential Equations? [closed],Why - not how - do you solve Differential Equations? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question I know HOW to mechanically solve basic diff. equations.  To recap, you start out with the derivative $\frac{dy}{dx}=...$ and you aim to find out y=...   To do this, you separate the variables, and then integrate. But, can someone give me a some context?  A simple example or general sense of WHY you solve a differential equation.  Know a common situation they are used to model, and then the purpose of then finding the original function from whence the derivative came?   When do you initially know the derivative?  When you only know the rate of change? Thanks!","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question I know HOW to mechanically solve basic diff. equations.  To recap, you start out with the derivative $\frac{dy}{dx}=...$ and you aim to find out y=...   To do this, you separate the variables, and then integrate. But, can someone give me a some context?  A simple example or general sense of WHY you solve a differential equation.  Know a common situation they are used to model, and then the purpose of then finding the original function from whence the derivative came?   When do you initially know the derivative?  When you only know the rate of change? Thanks!",,"['ordinary-differential-equations', 'applications', 'motivation']"
49,Differential equation with a constant in it,Differential equation with a constant in it,,"Solve $$y'' + s^2y = b \cos sx$$ where $s$ and $b$ are constants.  I have tried undetermined coefficients, but it makes such a mess that I keep getting lost, I also tried variation of parameters. Really my issue is staying organized enough to get the solution, my solutions are always just a little off from the book. I have solved the complementary homogenous equation, this gave me a solution with $\cos{sx}$ in it... So that's off limits in the particular solution. I looked for a solution in $x \sin{sx}$ and $x \cos{sx}$, but as I said, it was a mess. I wonder if I'm just going about it all wrong. Related to this, why will variation of parameters sometimes produce a term that is a solution to the homogenous case? Thanks","Solve $$y'' + s^2y = b \cos sx$$ where $s$ and $b$ are constants.  I have tried undetermined coefficients, but it makes such a mess that I keep getting lost, I also tried variation of parameters. Really my issue is staying organized enough to get the solution, my solutions are always just a little off from the book. I have solved the complementary homogenous equation, this gave me a solution with $\cos{sx}$ in it... So that's off limits in the particular solution. I looked for a solution in $x \sin{sx}$ and $x \cos{sx}$, but as I said, it was a mess. I wonder if I'm just going about it all wrong. Related to this, why will variation of parameters sometimes produce a term that is a solution to the homogenous case? Thanks",,['ordinary-differential-equations']
50,How to prove the relationship of two simple functions?,How to prove the relationship of two simple functions?,,"Given two functions $f(x)>0$ and $g(x)>0$ on $x\in[a,b]$, if they satisfy the following constraints: \begin{cases} f''(x) >0 \\ g''(x) = 0\\ f(a)=g(a)\\ \int_a^bf(x)\mathrm{d}x= \int_a^bg(x)\mathrm{d}x \end{cases}, then the following conclusions hold: $f(x)$ and $g(x)$ must have and only have one intersection on interval $(a,b)$. If we denote the x-coordinate of the intersection with $s$, then:  \begin{cases} f(x)<g(x), \qquad \mathrm{if}\quad x \in (a,s);\\ f(x)>g(x),\qquad \mathrm{if}\quad x \in (s,b) \end{cases} But how to prove it? Can you help me?","Given two functions $f(x)>0$ and $g(x)>0$ on $x\in[a,b]$, if they satisfy the following constraints: \begin{cases} f''(x) >0 \\ g''(x) = 0\\ f(a)=g(a)\\ \int_a^bf(x)\mathrm{d}x= \int_a^bg(x)\mathrm{d}x \end{cases}, then the following conclusions hold: $f(x)$ and $g(x)$ must have and only have one intersection on interval $(a,b)$. If we denote the x-coordinate of the intersection with $s$, then:  \begin{cases} f(x)<g(x), \qquad \mathrm{if}\quad x \in (a,s);\\ f(x)>g(x),\qquad \mathrm{if}\quad x \in (s,b) \end{cases} But how to prove it? Can you help me?",,"['calculus', 'ordinary-differential-equations']"
51,Limit of solution of differential equation without solving the equation.,Limit of solution of differential equation without solving the equation.,,"Given $$x'(t)=A-B\left(x(t)\right)^2, \quad x(0)=0.$$ Is it possible to find $\lim\limits_{t\to\infty}x(t)$ without solving the differential equation? Assuming $\lim\limits_{t\to\infty}x'(t)=0$ gives $\lim\limits_{t\to\infty}x(t)=\sqrt{A/B}$ which is correct, but I can not manage to prove that the limit of the derivative is 0. If it is of any help, the solution to the differential equations is $$x(t)=\sqrt{\frac AB}\tanh\left(\sqrt{AB}t\right).$$","Given $$x'(t)=A-B\left(x(t)\right)^2, \quad x(0)=0.$$ Is it possible to find $\lim\limits_{t\to\infty}x(t)$ without solving the differential equation? Assuming $\lim\limits_{t\to\infty}x'(t)=0$ gives $\lim\limits_{t\to\infty}x(t)=\sqrt{A/B}$ which is correct, but I can not manage to prove that the limit of the derivative is 0. If it is of any help, the solution to the differential equations is $$x(t)=\sqrt{\frac AB}\tanh\left(\sqrt{AB}t\right).$$",,"['ordinary-differential-equations', 'limits']"
52,A difficult differential equation $ y(2x^4+y)\frac{dy}{dx} = (1-4xy^2)x^2$ [closed],A difficult differential equation  [closed], y(2x^4+y)\frac{dy}{dx} = (1-4xy^2)x^2,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question How to solve the following differential equation? $$ y(2x^4+y)\dfrac{dy}{dx} = (1-4xy^2)x^2$$ No clue as to how to even begin. Hints?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question How to solve the following differential equation? $$ y(2x^4+y)\dfrac{dy}{dx} = (1-4xy^2)x^2$$ No clue as to how to even begin. Hints?,,"['calculus', 'ordinary-differential-equations']"
53,Hint on solving $ \frac{dx}{dt} = t - tx $,Hint on solving, \frac{dx}{dt} = t - tx ,"Can you please give me a hint how to continue solving this differential equation? $$ \frac{dx}{dt} = t - tx $$ Rewriting yields $$ \frac{dx}{dt} + tx = t $$ Now I can use integrating factor $$ \mu = e^{\int t dt} = e^{t^2 / 2} $$ So I can multiply whole equation with $ \mu $ and integrate both sides $$ e^{t^2 / 2} x = \int te^{t^2 / 2}dt$$ To solve integral on r.h.s. I put $u = t$ and $\dfrac{dv}{dt} = e^{t^2 / 2}$, so $\frac{du}{dt} = 1;\;\;v = \int e^{t^2 / 2} dt$. Now this second integral leads to substitution, let's put $z=t^2 / 2$, so $ \;\dfrac{dz}{dt} = t $. That is, $\; dt = \dfrac{dz}{t}.\;$ Putting back to integral yields $$ \int \frac{e^z}{t}dz $$ But i don't know how to solve this. I only went through examples, where substitution eliminated occurrence of original variable.","Can you please give me a hint how to continue solving this differential equation? $$ \frac{dx}{dt} = t - tx $$ Rewriting yields $$ \frac{dx}{dt} + tx = t $$ Now I can use integrating factor $$ \mu = e^{\int t dt} = e^{t^2 / 2} $$ So I can multiply whole equation with $ \mu $ and integrate both sides $$ e^{t^2 / 2} x = \int te^{t^2 / 2}dt$$ To solve integral on r.h.s. I put $u = t$ and $\dfrac{dv}{dt} = e^{t^2 / 2}$, so $\frac{du}{dt} = 1;\;\;v = \int e^{t^2 / 2} dt$. Now this second integral leads to substitution, let's put $z=t^2 / 2$, so $ \;\dfrac{dz}{dt} = t $. That is, $\; dt = \dfrac{dz}{t}.\;$ Putting back to integral yields $$ \int \frac{e^z}{t}dz $$ But i don't know how to solve this. I only went through examples, where substitution eliminated occurrence of original variable.",,['ordinary-differential-equations']
54,second order ODE via variation of parameters [closed],second order ODE via variation of parameters [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I don't understand this step on this article . $$A'(x)u_1(x) + B'(x)u_2(x) = 0$$ why we desire A=A(x) and B=B(x) to be of this form? What is the basis that this form is valid?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I don't understand this step on this article . $$A'(x)u_1(x) + B'(x)u_2(x) = 0$$ why we desire A=A(x) and B=B(x) to be of this form? What is the basis that this form is valid?",,['ordinary-differential-equations']
55,How to Analytically Solve this PDE?,How to Analytically Solve this PDE?,,"Thanks for looking at my question. I'm working through/self-studying the second edition of Partial Differential Equations: An Introduction by Walter A. Strauss. On page three, example two, he says ""Solve the PDE $u_{xx} + u = 0$ . Again, its really an ODE with an extra variable y. We know how to solve the ODE, so the solution is $u = f(y)cos(x) + g(y)sin(x)$ , where again $f(y)$ and $g(y)$ are two arbitrary functions of $y$ . You can easily check this formula by differentiating twice to verify that $u_{xx} = -u$ ."" What I don't understand is how he gets $u = f(y)cos(x) + g(y)sin(x)$ . He says it's basically just an ODE with an extra variable y, but I'm not quite seeing that. I was able to understand Example 1 before it and Example 3 after it, and I can sort of see that this PDE is similar to the ODE form $y'' + y = 0$ , but it's just been a hot minute since I've solved an ODE like this. I see it's homogenous, and one could use the method of integrating factors for it, but since this is a PDE I'm not sure how to solve this. My initial guess of $C_{1}e^{r_{1}t} + C_{2}e^{{r_2}t}$ didn't work, so I'm not sure how they got what they got for $u$ . I get that when you integrate with respect to $x$ the constant you get is a function of $y$ , but that's all I understand about this problem. Could someone show me how Walter got his solution for $u$ , please? Thanks.","Thanks for looking at my question. I'm working through/self-studying the second edition of Partial Differential Equations: An Introduction by Walter A. Strauss. On page three, example two, he says ""Solve the PDE . Again, its really an ODE with an extra variable y. We know how to solve the ODE, so the solution is , where again and are two arbitrary functions of . You can easily check this formula by differentiating twice to verify that ."" What I don't understand is how he gets . He says it's basically just an ODE with an extra variable y, but I'm not quite seeing that. I was able to understand Example 1 before it and Example 3 after it, and I can sort of see that this PDE is similar to the ODE form , but it's just been a hot minute since I've solved an ODE like this. I see it's homogenous, and one could use the method of integrating factors for it, but since this is a PDE I'm not sure how to solve this. My initial guess of didn't work, so I'm not sure how they got what they got for . I get that when you integrate with respect to the constant you get is a function of , but that's all I understand about this problem. Could someone show me how Walter got his solution for , please? Thanks.",u_{xx} + u = 0 u = f(y)cos(x) + g(y)sin(x) f(y) g(y) y u_{xx} = -u u = f(y)cos(x) + g(y)sin(x) y'' + y = 0 C_{1}e^{r_{1}t} + C_{2}e^{{r_2}t} u x y u,"['ordinary-differential-equations', 'partial-differential-equations']"
56,From system of coupled ODEs to separable ODE,From system of coupled ODEs to separable ODE,,How does one go from \begin{align} \dot{x}&=y\\ \dot{y}&=-x^3 \end{align} to the following ODE? $$\frac{dy}{dx} = -\frac{x^3}{y}$$,How does one go from to the following ODE?,"\begin{align}
\dot{x}&=y\\
\dot{y}&=-x^3
\end{align} \frac{dy}{dx} = -\frac{x^3}{y}",['ordinary-differential-equations']
57,"If $\,f''(x) \ge f(x)$, for all $x\in[0,\infty),$ and $\,f(0)=f'(0)=1$, then is $\,f(x)>0$?","If , for all  and , then is ?","\,f''(x) \ge f(x) x\in[0,\infty), \,f(0)=f'(0)=1 \,f(x)>0","Let $f:[0,\infty) \to \mathbb R$ be a twice differentiable function, such that  $\,f''(x) \ge f(x)$, for all $x\in\ [0,\infty)$, and $ f(0)=f'(0)=1$. Can we deduce that $f$ is increasing? I feel like it is, but I cannot see it. I can only show that to get it increasing it is enough to show that $f$ is non negative .","Let $f:[0,\infty) \to \mathbb R$ be a twice differentiable function, such that  $\,f''(x) \ge f(x)$, for all $x\in\ [0,\infty)$, and $ f(0)=f'(0)=1$. Can we deduce that $f$ is increasing? I feel like it is, but I cannot see it. I can only show that to get it increasing it is enough to show that $f$ is non negative .",,"['calculus', 'real-analysis']"
58,Derivative cycles of length 8,Derivative cycles of length 8,,"Let's say I want to find derivative cycles, that is, a group of functions $\langle f_0, f_1, \ldots, f_{n-1} \rangle$ where ${f_0}^{(p)}(x)={f_{p\,\%\,n}}(x)$ where $\%$ is the modulo operator. For $n = 1$, we have the exponential function, $f_0 = ae^{x+k}$ for arbitrary $a$ and $k$. For $n = 2$, we have two of these exponentials , $f_0 = ae^{x+k} + be^{-x-k}$ for arbitrary $a$, $b$, and $k$. For $n = 4$, an obvious solution is $\langle a\sin(x + k), a\cos(x + k), -a\sin(x + k), -a\cos(x + k) \rangle$ … but wait! That's just more exponentials! $$ f_0 = ae^{x + k} + be^{ix + ik} + ce^{-x - k} + de^{-ix-ik} $$ gives the general solution, for arbitrary $a$, $b$, $c$, $d$, and $k$ (discard the results when they become complex, I only care about $f_0: \Bbb R \to \Bbb R$). There seems to be something going on when $n$ is a power of 2, so what functions could be in a derivative cycle with $n = 8$? Since, in our exponent, we progressed from ""one direction of number"" ($1$) to ""two directions of number"" ($\pm1$) to ""four directions of number"" ($\pm1$ and $\pm i$), will we have to use a number system next that has 8 directions?","Let's say I want to find derivative cycles, that is, a group of functions $\langle f_0, f_1, \ldots, f_{n-1} \rangle$ where ${f_0}^{(p)}(x)={f_{p\,\%\,n}}(x)$ where $\%$ is the modulo operator. For $n = 1$, we have the exponential function, $f_0 = ae^{x+k}$ for arbitrary $a$ and $k$. For $n = 2$, we have two of these exponentials , $f_0 = ae^{x+k} + be^{-x-k}$ for arbitrary $a$, $b$, and $k$. For $n = 4$, an obvious solution is $\langle a\sin(x + k), a\cos(x + k), -a\sin(x + k), -a\cos(x + k) \rangle$ … but wait! That's just more exponentials! $$ f_0 = ae^{x + k} + be^{ix + ik} + ce^{-x - k} + de^{-ix-ik} $$ gives the general solution, for arbitrary $a$, $b$, $c$, $d$, and $k$ (discard the results when they become complex, I only care about $f_0: \Bbb R \to \Bbb R$). There seems to be something going on when $n$ is a power of 2, so what functions could be in a derivative cycle with $n = 8$? Since, in our exponent, we progressed from ""one direction of number"" ($1$) to ""two directions of number"" ($\pm1$) to ""four directions of number"" ($\pm1$ and $\pm i$), will we have to use a number system next that has 8 directions?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
59,"Solve the integral equation $f(x) = x + \lambda \int_0^1 f(z)\,dz$",Solve the integral equation,"f(x) = x + \lambda \int_0^1 f(z)\,dz","Find a closed-form solution for $f(x)$ in the following equation $$ f(x) = x + \lambda \int_0^1 f(z)\,dz $$ where $\lambda$ is a constant I tried integrating both sides from $0$ to $1$ but wasn't exactly sure where to go from there $$ \int_0^1 f(x) = \int_0^1 \left[x + \int_0^1 f(z)\,dz\right] dx $$","Find a closed-form solution for $f(x)$ in the following equation $$ f(x) = x + \lambda \int_0^1 f(z)\,dz $$ where $\lambda$ is a constant I tried integrating both sides from $0$ to $1$ but wasn't exactly sure where to go from there $$ \int_0^1 f(x) = \int_0^1 \left[x + \int_0^1 f(z)\,dz\right] dx $$",,"['calculus', 'ordinary-differential-equations', 'integral-equations']"
60,Solve $y''=y^2$,Solve,y''=y^2,"Are there any 'basic' solutions to this differential equation (ie using polynomials, exponetials, trigonometric functions and logarithms)? I cannot figure it out at all using the techniques I know for solving differential equations.","Are there any 'basic' solutions to this differential equation (ie using polynomials, exponetials, trigonometric functions and logarithms)? I cannot figure it out at all using the techniques I know for solving differential equations.",,['ordinary-differential-equations']
61,Solving an ODE with exponentials of the dependent variable,Solving an ODE with exponentials of the dependent variable,,"I have the following equation to solve, I think is just the case of founding a suitable change of variables, but I couldn't think of anything: $$y'' + (y')^2 = 2e^{(-y)}.$$ Any suggestions?","I have the following equation to solve, I think is just the case of founding a suitable change of variables, but I couldn't think of anything: $$y'' + (y')^2 = 2e^{(-y)}.$$ Any suggestions?",,['ordinary-differential-equations']
62,"If $\,\,f:[a,b]\to \mathbb{R}, \,b-a\ge 4$, is differentiable, then $\,f'(x_0)<1+(\,f(x_0))^2$, for some $x_0\in (a,b)$.","If , is differentiable, then , for some .","\,\,f:[a,b]\to \mathbb{R}, \,b-a\ge 4 \,f'(x_0)<1+(\,f(x_0))^2 x_0\in (a,b)","Suppose that $\,f:[a,b]\to \mathbb{R}$, where $\,b-a\ge 4,\,$ is differentiable in $(a,b)$ and continuous in $[a,b]$. Prove that there is $x_0\in (a,b)$, such that $$f'(x_0)<1+\big(\,f(x_0)\big)^2\!.$$ But, I could not make the slightest approach towards the solution of this problem. Please help. Thank you.","Suppose that $\,f:[a,b]\to \mathbb{R}$, where $\,b-a\ge 4,\,$ is differentiable in $(a,b)$ and continuous in $[a,b]$. Prove that there is $x_0\in (a,b)$, such that $$f'(x_0)<1+\big(\,f(x_0)\big)^2\!.$$ But, I could not make the slightest approach towards the solution of this problem. Please help. Thank you.",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations', 'functional-inequalities']"
63,A calculus problem with functions such that $f''(x) = g(x)$ and $g''(x) = f(x)$,A calculus problem with functions such that  and,f''(x) = g(x) g''(x) = f(x),"Let: $f(x)$ and $g(x)$ be twice differentiable, non-decreasing functions. $f''(x) = g(x)$ and $g''(x) = f(x)$. $f(x) \cdot g(x)$ is a linear function. Then we have to show that $f(x) = g(x) = 0$. I am really not able to do anything useful, any help would be appreciated :)","Let: $f(x)$ and $g(x)$ be twice differentiable, non-decreasing functions. $f''(x) = g(x)$ and $g''(x) = f(x)$. $f(x) \cdot g(x)$ is a linear function. Then we have to show that $f(x) = g(x) = 0$. I am really not able to do anything useful, any help would be appreciated :)",,"['calculus', 'ordinary-differential-equations', 'functions', 'systems-of-equations']"
64,Book searching in Elliptic Equation,Book searching in Elliptic Equation,,"I am learning a course with the subject of Elliptic Equations . If you know about it, please recommend me a book on Elliptic Equations . And if that's possible, someone post these books/author/...that I should read and do exercises/homework. Thanks!","I am learning a course with the subject of Elliptic Equations . If you know about it, please recommend me a book on Elliptic Equations . And if that's possible, someone post these books/author/...that I should read and do exercises/homework. Thanks!",,"['ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'sobolev-spaces']"
65,Simple proof for uniqueness of solutions of linear ODEs?,Simple proof for uniqueness of solutions of linear ODEs?,,"Consider the system of linear ODEs $\dot{x}(t)=Ax(t)$, $x(0)=x_0\in\mathbb{R}^n$. Does anyone know a simple proof showing that the solutions are unique that does not require resorting to more general existence/uniqueness results (e.g., those relating to the Picard iteration) nor solving for the solutions explicitly?","Consider the system of linear ODEs $\dot{x}(t)=Ax(t)$, $x(0)=x_0\in\mathbb{R}^n$. Does anyone know a simple proof showing that the solutions are unique that does not require resorting to more general existence/uniqueness results (e.g., those relating to the Picard iteration) nor solving for the solutions explicitly?",,"['ordinary-differential-equations', 'dynamical-systems']"
66,Finding a function that satisfies $f(y)-f(x)=x/y-1$,Finding a function that satisfies,f(y)-f(x)=x/y-1,"I want to find all (differentiable) functions $f:[0,1]\rightarrow[0,1]$ that satisfies $$f(y)-f(x)=\frac{x}{y}-1$$ My approach was taking $y=x+d$ , so that I can have $$f(x+d)-f(x)=\frac{x}{x+d}-1=-\frac{d}{x+d}.$$ If I devide both sides with $d$ and then take $d\rightarrow 0$ , I should have $$f'(x)=-\frac{1}{x}, $$ which implies $f(x)=ln ~x+C$ for some constant $C$ . However, if I plug in $x$ and $y$ back to the derived $f(x)$ , I cannot have $f(y)-f(x)=\frac{x}{y}-1$ .. Can anyone identify the mistake in my derivation or anyone knows how to derive such $f$ ?","I want to find all (differentiable) functions that satisfies My approach was taking , so that I can have If I devide both sides with and then take , I should have which implies for some constant . However, if I plug in and back to the derived , I cannot have .. Can anyone identify the mistake in my derivation or anyone knows how to derive such ?","f:[0,1]\rightarrow[0,1] f(y)-f(x)=\frac{x}{y}-1 y=x+d f(x+d)-f(x)=\frac{x}{x+d}-1=-\frac{d}{x+d}. d d\rightarrow 0 f'(x)=-\frac{1}{x},  f(x)=ln ~x+C C x y f(x) f(y)-f(x)=\frac{x}{y}-1 f","['ordinary-differential-equations', 'derivatives']"
67,Is there any theorem on uniqueness of integrating factor for inexact ordinary differential equations?,Is there any theorem on uniqueness of integrating factor for inexact ordinary differential equations?,,"Do we have any theorem regarding  uniqueness of integrating factor for inexact ordinary differential equations? Anything like: If $f(v)$ and $g(v)$ are functions of $v$ and integrating factors for the given inexact ODE, then $f(v)=g(v)$","Do we have any theorem regarding  uniqueness of integrating factor for inexact ordinary differential equations? Anything like: If $f(v)$ and $g(v)$ are functions of $v$ and integrating factors for the given inexact ODE, then $f(v)=g(v)$",,['ordinary-differential-equations']
68,Is this an elliptic integral or not?,Is this an elliptic integral or not?,,"I am working with the following differential equation: $$4\left(\frac{dz}{dx}\right)^2+z^4=4$$ On rearrangement, this yields $$\frac{dz}{dx}=\frac{\sqrt{4-z^4}}{{2}}$$ Using $z=\sqrt{2}\tan \theta$, we further get $$\frac{d\theta}{\sqrt{1-2\sin^2\theta}}=\frac{dx}{\sqrt{2}}$$ Now, as per this , this , this and other links, the term on the left is an elliptic integral. But Mathworld says that, for an elliptic integral of the form $\frac{d\theta}{\sqrt{1-k^2\sin^2\theta}}$, the bound on $k$ is given by $$0<k^2<1$$ But for my integral, $k^2=2>1$, which is also causing me difficulty in numerically trying to integrate the problem. Can somebody tell me what is correct and what not and how I should proceed to integrate the LHS of the equation, numerically (or if possible analytically)?","I am working with the following differential equation: $$4\left(\frac{dz}{dx}\right)^2+z^4=4$$ On rearrangement, this yields $$\frac{dz}{dx}=\frac{\sqrt{4-z^4}}{{2}}$$ Using $z=\sqrt{2}\tan \theta$, we further get $$\frac{d\theta}{\sqrt{1-2\sin^2\theta}}=\frac{dx}{\sqrt{2}}$$ Now, as per this , this , this and other links, the term on the left is an elliptic integral. But Mathworld says that, for an elliptic integral of the form $\frac{d\theta}{\sqrt{1-k^2\sin^2\theta}}$, the bound on $k$ is given by $$0<k^2<1$$ But for my integral, $k^2=2>1$, which is also causing me difficulty in numerically trying to integrate the problem. Can somebody tell me what is correct and what not and how I should proceed to integrate the LHS of the equation, numerically (or if possible analytically)?",,"['calculus', 'ordinary-differential-equations', 'numerical-methods', 'computational-mathematics', 'elliptic-integrals']"
69,Do Hermite polynomials exist for negative integers?,Do Hermite polynomials exist for negative integers?,,"I recently asked a question about a differential equation, and received this as an answer. It included a Hermite polynomial of negative degree, namely $H_{-3}$ . I searched online and it seems as though these $H_n$ 's are only defined for $n\ge0$ , and for $n<0$ something else might used - parabolic cylinder functions . I say 'might' because I am not sure if they are actually the same. I have never heard of these, and so was hoping someone would know about them here. Can they be expressed in terms of erf or erfc ? I found another link which suggested they could. If so, how? To clarify I want to know if Hermite polynomials (or their equivalent) can be expressed for negative integers $n$ in some closed form expression (including erf ).","I recently asked a question about a differential equation, and received this as an answer. It included a Hermite polynomial of negative degree, namely . I searched online and it seems as though these 's are only defined for , and for something else might used - parabolic cylinder functions . I say 'might' because I am not sure if they are actually the same. I have never heard of these, and so was hoping someone would know about them here. Can they be expressed in terms of erf or erfc ? I found another link which suggested they could. If so, how? To clarify I want to know if Hermite polynomials (or their equivalent) can be expressed for negative integers in some closed form expression (including erf ).",H_{-3} H_n n\ge0 n<0 n,"['ordinary-differential-equations', 'error-function', 'hermite-polynomials']"
70,Large list of ordinary differential equations for practice,Large list of ordinary differential equations for practice,,"I am looking for a large list of ordinary differential equations, as a practice resource. Preferably the list should be in order of ""difficulty"" and allow me to practice the different techniques (change of variables, etc). Do you have a suggestion for a good list like this?","I am looking for a large list of ordinary differential equations, as a practice resource. Preferably the list should be in order of ""difficulty"" and allow me to practice the different techniques (change of variables, etc). Do you have a suggestion for a good list like this?",,"['ordinary-differential-equations', 'reference-request', 'online-resources']"
71,Why does using elimination in a system of first order differential equations produce an incorrect result?,Why does using elimination in a system of first order differential equations produce an incorrect result?,,"For example, if I have the system, $$ y'+y=3x \\ y'-y=x $$ I could then use elimination to minus the top equation from the bottom one to get, $$ 2y=2x \\ y=x $$ Which is obviously wrong as then, $1+x=3x$ which is wrong. So why are you not able to use elimination in solving a system of first order differential equations?","For example, if I have the system, $$ y'+y=3x \\ y'-y=x $$ I could then use elimination to minus the top equation from the bottom one to get, $$ 2y=2x \\ y=x $$ Which is obviously wrong as then, $1+x=3x$ which is wrong. So why are you not able to use elimination in solving a system of first order differential equations?",,"['calculus', 'ordinary-differential-equations']"
72,Second order DE problem.,Second order DE problem.,,Solve : $$x(x-1)y''-(2x-1)y'+2y=x^2(2x-3)$$ I have tried solving the DE by using the following three methods: Solution in terms of part of CF (was unable to find a standard CF) Normal form ($R_1$ and $Q_1$ look very complex) Change of independent variable ($Q_1$ is very complex) The question seems simple but I have not been able to determine a solution. I appreciate any ideas on how to solve the DE. Thanks,Solve : $$x(x-1)y''-(2x-1)y'+2y=x^2(2x-3)$$ I have tried solving the DE by using the following three methods: Solution in terms of part of CF (was unable to find a standard CF) Normal form ($R_1$ and $Q_1$ look very complex) Change of independent variable ($Q_1$ is very complex) The question seems simple but I have not been able to determine a solution. I appreciate any ideas on how to solve the DE. Thanks,,['ordinary-differential-equations']
73,How to solve the DE $y' = -y + ty^{1/2}$?,How to solve the DE ?,y' = -y + ty^{1/2},"The DE is $y' = -y + ty^{\frac{1}{2}}$. $2 \le t \le 3$ $y(2) = 2$ I tried to see if it was in the linear form . I got: $$\frac{dy}{dt} + y = ty^{\frac{1}{2}}$$ The RHS was not a function of t . I also tried separation of variables, but I couldn't isolate the y from the term $ty^{\frac{1}{2}}$. Any hints?","The DE is $y' = -y + ty^{\frac{1}{2}}$. $2 \le t \le 3$ $y(2) = 2$ I tried to see if it was in the linear form . I got: $$\frac{dy}{dt} + y = ty^{\frac{1}{2}}$$ The RHS was not a function of t . I also tried separation of variables, but I couldn't isolate the y from the term $ty^{\frac{1}{2}}$. Any hints?",,['ordinary-differential-equations']
74,Numerical Analysis References,Numerical Analysis References,,"Could anyone suggest any good (perhaps online ref papers) reference material on numerical analysis focusing on determining accuracy/estimated errors, rates/orders of convergence especially when applied to ODEs? Thank you very much.","Could anyone suggest any good (perhaps online ref papers) reference material on numerical analysis focusing on determining accuracy/estimated errors, rates/orders of convergence especially when applied to ODEs? Thank you very much.",,"['reference-request', 'ordinary-differential-equations', 'numerical-methods']"
75,How to find $f(x)$ if $df(x)/dx=f(x)$,How to find  if,f(x) df(x)/dx=f(x),"How to find $f(x)$ if $\frac{df(x)}{dx}=f(x)$? I know $c e^x$ is a solution, but how does one find it and how to prove it is the complete solution?","How to find $f(x)$ if $\frac{df(x)}{dx}=f(x)$? I know $c e^x$ is a solution, but how does one find it and how to prove it is the complete solution?",,"['calculus', 'ordinary-differential-equations']"
76,Second-Order Linear Differential Equation,Second-Order Linear Differential Equation,,I have the following differential equation: $$y''+y=\cos(t)\cos(2t)$$ Maybe something can be done to $\cos(t)\cos(2t)$ to make it easier to solve. Any ideas? Thanks in advance.,I have the following differential equation: $$y''+y=\cos(t)\cos(2t)$$ Maybe something can be done to $\cos(t)\cos(2t)$ to make it easier to solve. Any ideas? Thanks in advance.,,['ordinary-differential-equations']
77,General solution for a differential equation $x = t x^\prime + (x^\prime)^2$,General solution for a differential equation,x = t x^\prime + (x^\prime)^2,"I am studying for an exam and I don't know how to determine a general solution for a differential equation. For example, $$ x = t x^\prime + (x^\prime)^2 $$ Can anyone help?","I am studying for an exam and I don't know how to determine a general solution for a differential equation. For example, $$ x = t x^\prime + (x^\prime)^2 $$ Can anyone help?",,['ordinary-differential-equations']
78,Solve the following differential ecuation: $y'x^2 = 1+3y$,Solve the following differential ecuation:,y'x^2 = 1+3y,"Solve the following differential equation: $y'x^2 = 1+3y$ , with $y(3) = -\frac{1}3$ I tried the following but i'm a little bit lost at the end. $\frac{dy}{dx}x^2=1+3y$ $\frac{dy}{1+3y}=\frac{dx}{x^2}$ $\frac{1}{3}\ln(1+3y)=-\frac{1}{x}+c$ $\ln(1+3y)=-\frac{3}{x}+c$ $(1+3y)=e^{-\frac{3}{x}}e^c$ $y=\frac{(e^{-\frac{3}{x}}k)-1}{3}$ Well... here i dont know how to end the problem.","Solve the following differential equation: , with I tried the following but i'm a little bit lost at the end. Well... here i dont know how to end the problem.",y'x^2 = 1+3y y(3) = -\frac{1}3 \frac{dy}{dx}x^2=1+3y \frac{dy}{1+3y}=\frac{dx}{x^2} \frac{1}{3}\ln(1+3y)=-\frac{1}{x}+c \ln(1+3y)=-\frac{3}{x}+c (1+3y)=e^{-\frac{3}{x}}e^c y=\frac{(e^{-\frac{3}{x}}k)-1}{3},['ordinary-differential-equations']
79,Why is this the result of this integral?,Why is this the result of this integral?,,"This problem has to do with the non-approximated solution of the motion of a simple pendulum. I'm asking here instead of at the physics forum because I have a mathematical question. Anyways, the differential equation for a simple pendulum is: $$\frac{\mathrm{d}^2\theta}{\mathrm{d}t^2}+\omega_0^2\sin\theta=0$$ Multiplying both sides by $\frac{\mathrm{d}\theta}{\mathrm{d}t}$ and integrating over $t$ : $$\int\left( \frac{\mathrm{d}^2\theta}{\mathrm{d}t^2}\right)\mathrm{d}\theta=\omega_0^2\cos\theta+C$$ The next step simply states that, therefore: $$\frac{1}{2}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)^2-\omega_0^2\cos\theta=K$$ My problem is with the first term in the left hand side. Why is that the result of the integral? It implies that it's integrating something like: $$\int\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)\mathrm{d}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)=\frac{1}{2}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)^2 + C$$ But I fail to see how that makes any sense with the given steps. Am I missing something obvious here?","This problem has to do with the non-approximated solution of the motion of a simple pendulum. I'm asking here instead of at the physics forum because I have a mathematical question. Anyways, the differential equation for a simple pendulum is: Multiplying both sides by and integrating over : The next step simply states that, therefore: My problem is with the first term in the left hand side. Why is that the result of the integral? It implies that it's integrating something like: But I fail to see how that makes any sense with the given steps. Am I missing something obvious here?",\frac{\mathrm{d}^2\theta}{\mathrm{d}t^2}+\omega_0^2\sin\theta=0 \frac{\mathrm{d}\theta}{\mathrm{d}t} t \int\left( \frac{\mathrm{d}^2\theta}{\mathrm{d}t^2}\right)\mathrm{d}\theta=\omega_0^2\cos\theta+C \frac{1}{2}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)^2-\omega_0^2\cos\theta=K \int\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)\mathrm{d}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)=\frac{1}{2}\left(\frac{\mathrm{d}\theta}{\mathrm{d}t}\right)^2 + C,"['ordinary-differential-equations', 'solution-verification']"
80,Equation of motion through the Lagrangian with Lagrange multipliers,Equation of motion through the Lagrangian with Lagrange multipliers,,"I ask for advice, cause I'm a little confused. We have such a Lagrangian: $L=\frac{1}{2}m(\dot{x}^2+\dot{y}^2)-\lambda(x+xy+y-1)$ Here $\lambda(x+xy+y-1)$ is the constraint on the phase variables. I need to derive the equation of motion given the constraints and solve them numerically with the help of NDSolve . We do this in accordance with the classic formula: $\frac{d}{dt}(\frac{dL}{d\dot{q}})-\frac{dL}{dq}=0$ Where $q=[x,y]$ are generalized coordinates. I'm not sure about the Lagrange multiplier as a generalized coordinate. Clear[""Derivative""]  ClearAll[""Global`*""]  T = 1/2 m (x'[t]^2 + y'[t]^2);(*Kinetic Energy*)  f = \[Lambda] (x[t] + x[t] y[t] +y[t] - 1);(*Constraint*)  L = T - f;(*Lagrangian*)  D[D[L, x'[t]], t] - D[L, x[t]];  D[D[L, y'[t]], t] - D[L, y[t]];  D[D[L, \[Lambda]'[t]], t] - D[L, \[Lambda][t]]; Question: how are the Lagrange multipliers included in this system when compiling the ODE system and numerically solving it? Maybe this help? https://farside.ph.utexas.edu/teaching/336k/lectures/node90.html https://www.sciencedirect.com/science/article/abs/pii/0045782588900850","I ask for advice, cause I'm a little confused. We have such a Lagrangian: Here is the constraint on the phase variables. I need to derive the equation of motion given the constraints and solve them numerically with the help of NDSolve . We do this in accordance with the classic formula: Where are generalized coordinates. I'm not sure about the Lagrange multiplier as a generalized coordinate. Clear[""Derivative""]  ClearAll[""Global`*""]  T = 1/2 m (x'[t]^2 + y'[t]^2);(*Kinetic Energy*)  f = \[Lambda] (x[t] + x[t] y[t] +y[t] - 1);(*Constraint*)  L = T - f;(*Lagrangian*)  D[D[L, x'[t]], t] - D[L, x[t]];  D[D[L, y'[t]], t] - D[L, y[t]];  D[D[L, \[Lambda]'[t]], t] - D[L, \[Lambda][t]]; Question: how are the Lagrange multipliers included in this system when compiling the ODE system and numerically solving it? Maybe this help? https://farside.ph.utexas.edu/teaching/336k/lectures/node90.html https://www.sciencedirect.com/science/article/abs/pii/0045782588900850","L=\frac{1}{2}m(\dot{x}^2+\dot{y}^2)-\lambda(x+xy+y-1) \lambda(x+xy+y-1) \frac{d}{dt}(\frac{dL}{d\dot{q}})-\frac{dL}{dq}=0 q=[x,y]","['ordinary-differential-equations', 'lagrange-multiplier', 'euler-lagrange-equation', 'numerical-calculus', 'nonlinear-dynamics']"
81,Find the closed path of given system of ordinary differential equation,Find the closed path of given system of ordinary differential equation,,"Consider the system of ordinary differential equations: $$\begin{cases}\frac{dx}{dt}=4x^3y^2-x^5y^4\\ \frac{dy}{dt}=x^4y^5+2x^2y^3\end{cases}$$ Then for this system there exist $1).$ A closed path in $\left \{(x,y) \in \mathbb{R^2}|x^2+y^2 \leq 5  \right \}$ $2).$ A closed path in $\left \{(x,y) \in \mathbb{R^2}|5<x^2+y^2 \leq 10  \right \}$ $3). $ A closed path in $\left \{(x,y) \in \mathbb{R^2}|x^2+y^2 >10 \right \}$ $4). $ No closed Path in $\mathbb{R^2}$ solution i tried - I first find out the $\frac{dy}{dx}$ $$\frac{dy}{dx}=\frac{x^2y^3+2y}{4x-x^3y^2}$$ it will become $$-(x^2y^2+2)ydx+(4-x^2y^2)xdy=0\;\;\;\;\;\;\;\ ....................1$$ which is of from $$f_1(xy)ydx+f_2(xy)xdy$$ after that i find the $I.F$ of $1$ which comes out $$\frac{-1}{6xy}$$ now by multiplying this with $1$ i get $$\frac{1}{6} \left ( xy^2+\frac{2}{x} \right )  dx-\frac{1}{6} \left ( \frac{4}{y}+x^2y \right )dy=0$$ after solving this i get answer $$\frac{x^2y^2}{12}-\frac{1}{3}\log (\frac{x}{y^2})=c$$ but there is noting related to given option ,where i am making mistake please help Thank you","Consider the system of ordinary differential equations: Then for this system there exist A closed path in A closed path in A closed path in No closed Path in solution i tried - I first find out the it will become which is of from after that i find the of which comes out now by multiplying this with i get after solving this i get answer but there is noting related to given option ,where i am making mistake please help Thank you","\begin{cases}\frac{dx}{dt}=4x^3y^2-x^5y^4\\
\frac{dy}{dt}=x^4y^5+2x^2y^3\end{cases} 1). \left \{(x,y) \in \mathbb{R^2}|x^2+y^2 \leq 5  \right \} 2). \left \{(x,y) \in \mathbb{R^2}|5<x^2+y^2 \leq 10  \right \} 3).  \left \{(x,y) \in \mathbb{R^2}|x^2+y^2 >10 \right \} 4).  \mathbb{R^2} \frac{dy}{dx} \frac{dy}{dx}=\frac{x^2y^3+2y}{4x-x^3y^2} -(x^2y^2+2)ydx+(4-x^2y^2)xdy=0\;\;\;\;\;\;\;\
....................1 f_1(xy)ydx+f_2(xy)xdy I.F 1 \frac{-1}{6xy} 1 \frac{1}{6} \left ( xy^2+\frac{2}{x} \right )  dx-\frac{1}{6} \left ( \frac{4}{y}+x^2y \right )dy=0 \frac{x^2y^2}{12}-\frac{1}{3}\log (\frac{x}{y^2})=c","['ordinary-differential-equations', 'solution-verification']"
82,Loan Interest Question,Loan Interest Question,,"Clarissa wants to buy a new car.  Her loan officer tells her that her   annual interest is 8%, compounded continuously, over a four-year term.   Clarissa informs her loan officer that she can make equal monthly   payments of $225.  How much can Clarissa afford to borrow? I tried to solve this problem twice using $$P' = (2/25)P - 2700, P(4) = 0, P(0) =\ ?$$ and $$P' = (2/25)P - 225, P(48) = 0, P(0) =\ ?$$ and ended up with two different answers for $P(0)$ .  Which setup, if either, is correct? Given that the problem appears in a differential equations book (in an early section), is it safe to assume that the ""equal monthly payments"" are actually being paid continuously, rather than discretely at the end of each month, or can the compounding frequency and payment frequency still be operating according to two different ""clocks""?","Clarissa wants to buy a new car.  Her loan officer tells her that her   annual interest is 8%, compounded continuously, over a four-year term.   Clarissa informs her loan officer that she can make equal monthly   payments of $225.  How much can Clarissa afford to borrow? I tried to solve this problem twice using and and ended up with two different answers for .  Which setup, if either, is correct? Given that the problem appears in a differential equations book (in an early section), is it safe to assume that the ""equal monthly payments"" are actually being paid continuously, rather than discretely at the end of each month, or can the compounding frequency and payment frequency still be operating according to two different ""clocks""?","P' = (2/25)P - 2700, P(4) = 0, P(0) =\ ? P' = (2/25)P - 225, P(48) = 0, P(0) =\ ? P(0)","['ordinary-differential-equations', 'finance', 'mathematical-modeling', 'initial-value-problems']"
83,How to solve a differential equation with a term to a power?,How to solve a differential equation with a term to a power?,,How would I solve an equation where one of the differential terms is to a power? For example: $$\frac{d^2y}{dx^2}+k(\frac{dy}{dx})^2=0$$ I've been given advice to use the $D$ operator which apparently means $\frac{d}{dx}()$ but I'm not sure how that's applicable to this scenario. Any alternative suggestions or explanations would be appreciated!,How would I solve an equation where one of the differential terms is to a power? For example: I've been given advice to use the operator which apparently means but I'm not sure how that's applicable to this scenario. Any alternative suggestions or explanations would be appreciated!,\frac{d^2y}{dx^2}+k(\frac{dy}{dx})^2=0 D \frac{d}{dx}(),"['calculus', 'ordinary-differential-equations']"
84,Solving $y''+2y'+y = 2e^{-t}$ by the method of undetermined coefficients,Solving  by the method of undetermined coefficients,y''+2y'+y = 2e^{-t},"I need to solve $$y''+2'y+y = 2e^{-t},$$ using the method of undetermined coefficients (and by founding a solution for the homogeneous equation). I tried first guessing a solution of the form $y = Ae^{-t}$, but when I tried to solve for $A$, I got a surprise: I couldn't equate the terms when I plugged $y$ in the differential equation, because I got $e^{-t}(A-2A+2A) = 2e^{-t}$ but the $A$'s sum to $0$. I then searched my book and realized that the problem was because $Ae^{-t}$ is already a solution for the homogeneous equation. I then tried $Ate^{-t}$ because my book tries it for a different equation and it worked, but in my case I got the $A$'s summing to $0$ again. What should be my guess?","I need to solve $$y''+2'y+y = 2e^{-t},$$ using the method of undetermined coefficients (and by founding a solution for the homogeneous equation). I tried first guessing a solution of the form $y = Ae^{-t}$, but when I tried to solve for $A$, I got a surprise: I couldn't equate the terms when I plugged $y$ in the differential equation, because I got $e^{-t}(A-2A+2A) = 2e^{-t}$ but the $A$'s sum to $0$. I then searched my book and realized that the problem was because $Ae^{-t}$ is already a solution for the homogeneous equation. I then tried $Ate^{-t}$ because my book tries it for a different equation and it worked, but in my case I got the $A$'s summing to $0$ again. What should be my guess?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
85,Need help with the following differential equations.,Need help with the following differential equations.,,$$xy'=y \cos \bigg(\ln{y\over x}\bigg)$$ In the first I tried a substitution to no avail.,$$xy'=y \cos \bigg(\ln{y\over x}\bigg)$$ In the first I tried a substitution to no avail.,,['ordinary-differential-equations']
86,$y''+y'^{2}+y=0$ equation solution,equation solution,y''+y'^{2}+y=0,How would you solve this differential equation $y''+y'^{2}+y=0$? I can't apply the ansatz method (or more formally apply the characteristic polynomial method). Thanks,How would you solve this differential equation $y''+y'^{2}+y=0$? I can't apply the ansatz method (or more formally apply the characteristic polynomial method). Thanks,,['ordinary-differential-equations']
87,What is a good text which introduces ODE in a very general setting?,What is a good text which introduces ODE in a very general setting?,,"For your information, I have studied almost all undergraduate mathematics except for differential equation and I'm really comfortable with what I have learned. Moreover, I have taken an one semester ODE course before even though I forgot them all. When one first learns analysis, one is usually taught in a very limited setting such as $\mathbb{R}$ and $\mathbb{C}$. For example, most freshmen learn ""intermediate value theorem"" in $\mathbb{R}$, but it is generally a topological property, so that one can prove this fact in much more general setting. I rememebr that, the same thing happens in ODE course, that is, every function we consider is a function on $[a,b]$. Well, if this is the best setting for the theory, then I would be fine, but is there a general setting for ODE? For example, one can learn measure theory on $\mathbb{R}^n$, but the most general setting for measure theory is usually a locally compact Hausdorff space. What is the most general setting for ODE and what is a text introducing this theory in this general setting? I don't mind whether the text is tough or not if there is no preliminary requirement of differential equation. Thank you in advance :)","For your information, I have studied almost all undergraduate mathematics except for differential equation and I'm really comfortable with what I have learned. Moreover, I have taken an one semester ODE course before even though I forgot them all. When one first learns analysis, one is usually taught in a very limited setting such as $\mathbb{R}$ and $\mathbb{C}$. For example, most freshmen learn ""intermediate value theorem"" in $\mathbb{R}$, but it is generally a topological property, so that one can prove this fact in much more general setting. I rememebr that, the same thing happens in ODE course, that is, every function we consider is a function on $[a,b]$. Well, if this is the best setting for the theory, then I would be fine, but is there a general setting for ODE? For example, one can learn measure theory on $\mathbb{R}^n$, but the most general setting for measure theory is usually a locally compact Hausdorff space. What is the most general setting for ODE and what is a text introducing this theory in this general setting? I don't mind whether the text is tough or not if there is no preliminary requirement of differential equation. Thank you in advance :)",,"['ordinary-differential-equations', 'book-recommendation']"
88,Solving $\frac{d f(x)}{dx} + f(x-1) = x^2$,Solving,\frac{d f(x)}{dx} + f(x-1) = x^2,Given following differential equation: $$\frac{d f(x)}{dx} + f(x-1) = x^2$$ where $ f(x)=0 $ for $x \leq 0 $. How do I find the solution for $ x \geq 0 $ ? I understand that for  $ 0 \leq x \leq 1 $ the solution is $ f(x)= \frac{x^3}{3} $.,Given following differential equation: $$\frac{d f(x)}{dx} + f(x-1) = x^2$$ where $ f(x)=0 $ for $x \leq 0 $. How do I find the solution for $ x \geq 0 $ ? I understand that for  $ 0 \leq x \leq 1 $ the solution is $ f(x)= \frac{x^3}{3} $.,,"['calculus', 'ordinary-differential-equations', 'delay-differential-equations']"
89,Duplicate zero eigenvalue,Duplicate zero eigenvalue,,"What happens when you have two zero eigenvalues (duplicate zeroes) in a 2x2 system of linear differential equations?  For example, $$\pmatrix{\frac{dx}{dt}\\\frac{dy}{dt}}=\pmatrix{1&1\\-1&-1}\pmatrix{x\\y}$$ has the characteristic polynomial $(1-\lambda)(-1-\lambda)+1=0$ which yields $\lambda=0$. Is this a center (since $Im(\lambda)=0$), a line of solutions (since $\lambda_i=0$), or what?","What happens when you have two zero eigenvalues (duplicate zeroes) in a 2x2 system of linear differential equations?  For example, $$\pmatrix{\frac{dx}{dt}\\\frac{dy}{dt}}=\pmatrix{1&1\\-1&-1}\pmatrix{x\\y}$$ has the characteristic polynomial $(1-\lambda)(-1-\lambda)+1=0$ which yields $\lambda=0$. Is this a center (since $Im(\lambda)=0$), a line of solutions (since $\lambda_i=0$), or what?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
90,A more elegant way of computing this Wronskian?,A more elegant way of computing this Wronskian?,,"As I was working on my differential equation homework this week I came across this problem: Let $y^{(4)} + 16y=0$. Compute the Wronskian of four linearly independent solutions . It's rather straightforward to find four such solutions solutions: $\phi_1(x)= e^{\sqrt{2}x} \cos{\sqrt{2}x}$, $\phi_2(x)= e^{\sqrt{2}x} \sin{\sqrt{2}x}$, $\phi_3(x)= e^{-\sqrt{2}x} \cos{\sqrt{2}x}$, $\phi_4(x)= e^{-\sqrt{2}x} \sin{\sqrt{2}x}.$ And from there computing the Wronskian (it is $256$) can be accomplished by trudging through the computation. However, since the solutions have such a nice symmetry on the complex unit circle, is there an easier way of coming to the solution? I tried to do this problem at first by avoiding taking the determinant, but ended spending more time than I would have anyway in doing so. I want to believe there is an easier way to do this problem, and I feel like there is a trick that I'm not seeing.","As I was working on my differential equation homework this week I came across this problem: Let $y^{(4)} + 16y=0$. Compute the Wronskian of four linearly independent solutions . It's rather straightforward to find four such solutions solutions: $\phi_1(x)= e^{\sqrt{2}x} \cos{\sqrt{2}x}$, $\phi_2(x)= e^{\sqrt{2}x} \sin{\sqrt{2}x}$, $\phi_3(x)= e^{-\sqrt{2}x} \cos{\sqrt{2}x}$, $\phi_4(x)= e^{-\sqrt{2}x} \sin{\sqrt{2}x}.$ And from there computing the Wronskian (it is $256$) can be accomplished by trudging through the computation. However, since the solutions have such a nice symmetry on the complex unit circle, is there an easier way of coming to the solution? I tried to do this problem at first by avoiding taking the determinant, but ended spending more time than I would have anyway in doing so. I want to believe there is an easier way to do this problem, and I feel like there is a trick that I'm not seeing.",,"['ordinary-differential-equations', 'wronskian']"
91,"Possible ways to do stability analysis of non-linear, three-dimensional Differential Equations","Possible ways to do stability analysis of non-linear, three-dimensional Differential Equations",,"For example Lorenz system, $$ \frac{d}{dt}\begin{pmatrix} x\\  y\\  z \end{pmatrix}=\begin{pmatrix} -\sigma & \sigma & 0\\  \rho & -1 & -x\\  y & 0 & -\beta \end{pmatrix}\begin{pmatrix} x\\  y\\  z \end{pmatrix} $$ The article on Wiki describes the system of differential equations to have stable equilibrium for a specific value of $\rho$. Given some three dimensional system of differential equations, I would like to know how to do the stability analysis? or explain how stability analysis of Lorenz system is done ...","For example Lorenz system, $$ \frac{d}{dt}\begin{pmatrix} x\\  y\\  z \end{pmatrix}=\begin{pmatrix} -\sigma & \sigma & 0\\  \rho & -1 & -x\\  y & 0 & -\beta \end{pmatrix}\begin{pmatrix} x\\  y\\  z \end{pmatrix} $$ The article on Wiki describes the system of differential equations to have stable equilibrium for a specific value of $\rho$. Given some three dimensional system of differential equations, I would like to know how to do the stability analysis? or explain how stability analysis of Lorenz system is done ...",,"['analysis', 'ordinary-differential-equations', 'bifurcation']"
92,some questions about differential equations,some questions about differential equations,,"when solving 1st order linear differential equations, we have some common method. I have some questions about the common methods. for example  $\dfrac{dy}{-y + 5} = dt$ , why can we integrate on both side to get the equation of $y$? More specifically, we have different things to integrate, $y$ and $t$ respectively, why can we just integrate on both side? What's the underlying reason here? Also when we get $\,\ln |5 - y| =  -t + C,\,$ why can we immediately get $\,5- y = e^{-t + C}\,$ and get rid of the absolute sign? There is no information about the range of $y$, why can we do that? Thanks","when solving 1st order linear differential equations, we have some common method. I have some questions about the common methods. for example  $\dfrac{dy}{-y + 5} = dt$ , why can we integrate on both side to get the equation of $y$? More specifically, we have different things to integrate, $y$ and $t$ respectively, why can we just integrate on both side? What's the underlying reason here? Also when we get $\,\ln |5 - y| =  -t + C,\,$ why can we immediately get $\,5- y = e^{-t + C}\,$ and get rid of the absolute sign? There is no information about the range of $y$, why can we do that? Thanks",,['ordinary-differential-equations']
93,Simple Harmonic Oscillator Solution,Simple Harmonic Oscillator Solution,,"In Physics, the Simple Harmonic Oscillator is represented by the equation $d^2x/dt^2=-\omega^2x$ . By using the characteristic polynomial, you get solutions of the form $x(t)=Ae^{i\omega t} + Be^{-i\omega t}$. I get that you use Euler's formula $e^{i\theta}=\cos\theta + i\sin\theta$, but I can't seem to find my way all the way to the 'traditional form' of $D\cos\omega t + C\sin\omega t$. I'm stuck here: $A(\cos\omega t + i\sin\omega t) + B(\cos\omega t - i\sin\omega t)$. What am I missing in taking it all the way? It doesn't seem valid to me (not sure why or why not) to make $C = iA - iB$.","In Physics, the Simple Harmonic Oscillator is represented by the equation $d^2x/dt^2=-\omega^2x$ . By using the characteristic polynomial, you get solutions of the form $x(t)=Ae^{i\omega t} + Be^{-i\omega t}$. I get that you use Euler's formula $e^{i\theta}=\cos\theta + i\sin\theta$, but I can't seem to find my way all the way to the 'traditional form' of $D\cos\omega t + C\sin\omega t$. I'm stuck here: $A(\cos\omega t + i\sin\omega t) + B(\cos\omega t - i\sin\omega t)$. What am I missing in taking it all the way? It doesn't seem valid to me (not sure why or why not) to make $C = iA - iB$.",,['ordinary-differential-equations']
94,Why are differential equations called differential equations?,Why are differential equations called differential equations?,,Why are differential equations called differential equations?,Why are differential equations called differential equations?,,"['calculus', 'terminology', 'ordinary-differential-equations']"
95,non linear differential equation that i cant solve,non linear differential equation that i cant solve,,"Can anyone help me to solve this differential equation. Obviously this is not a linear differential equation. So the only way that I know to solve non linear differential equations is by ""manipulating"" the $y$ variable in order to become a linear differential equation. But in this case I cant find a way. My main problem is that there is a $dy/dx$ inside and an $\ln$ outside . \begin{align} \ y - \ln(\dot y) = x\dot y \      \end{align}","Can anyone help me to solve this differential equation. Obviously this is not a linear differential equation. So the only way that I know to solve non linear differential equations is by ""manipulating"" the variable in order to become a linear differential equation. But in this case I cant find a way. My main problem is that there is a inside and an outside .","y dy/dx \ln \begin{align}
\ y - \ln(\dot y) = x\dot y \
     \end{align}",['ordinary-differential-equations']
96,Is it possible to solve this second order autonomous differential equation?,Is it possible to solve this second order autonomous differential equation?,,"$$x''(t) = \frac{1}{x^2(t)}$$ I'm interested in this differential equation because it mimics the motion of an object subject to gravity. The solution of this differential equation will be an algebraic expression of the position  of such an object, which would be useful.","I'm interested in this differential equation because it mimics the motion of an object subject to gravity. The solution of this differential equation will be an algebraic expression of the position  of such an object, which would be useful.",x''(t) = \frac{1}{x^2(t)},['ordinary-differential-equations']
97,Are There Functions Where $(f\cdot g)^\prime$ is equal to $f^\prime \cdot g^\prime$? [duplicate],Are There Functions Where  is equal to ? [duplicate],(f\cdot g)^\prime f^\prime \cdot g^\prime,"This question already has answers here : When does product of derivatives equals derivative of products? (2 answers) Closed 5 years ago . My math teacher recently asked my class to find if there are any functions that comply to the rule $$(f\cdot g)^\prime = f^\prime \cdot g^\prime$$ I have searched the web for an answer but I couldn't find it, could anyone help me with that? or at least point me to the right direction?","This question already has answers here : When does product of derivatives equals derivative of products? (2 answers) Closed 5 years ago . My math teacher recently asked my class to find if there are any functions that comply to the rule I have searched the web for an answer but I couldn't find it, could anyone help me with that? or at least point me to the right direction?",(f\cdot g)^\prime = f^\prime \cdot g^\prime,"['calculus', 'ordinary-differential-equations', 'derivatives', 'functional-equations']"
98,Solving differential equations of the form $\theta''(t)+\theta'(t)+\theta(t-\delta)=0$,Solving differential equations of the form,\theta''(t)+\theta'(t)+\theta(t-\delta)=0,"TLDR: Hi, is there an exam friendly* method to solve (without using numerical methods)  differential equations of the form $$\theta''(t)+\theta'(t)+\theta(t-\delta)=0$$ I have been searching for 'delay differential equations' and only first order DDEs solutions seem to show up. *possible with bare hands, a graphing calculator, (and a reasonable level of mathematical intelligence for a pretty smart high school student) Basically, I am working on making a mock paper for my friends (still in high school) and ended up setting a question on 2nd order ODE that I can't solve. So I'm curious on whether there is a (hopefully not too advanced) way to solve such ODEs. The full question and the solution I have worked on so far is below: Consider an elliptical track at the standard position such that its major and minor axes lie on the $x$-axis and $y$-axis respectively. A point $P\left(a\cos \theta,b\sin \theta\right)$ on the ellipse, where $a>b>0$, is moving with an angular acceleration $\alpha$ rad s -2 , an angular velocity $\omega$ rad s -1 , and an angular displacement $\theta$ rad measured in an anticlockwise manner from the starting point $P_0\left(a,0\right)$. As $P$ is moving, a laser ray is continuously shot with the speed of light $c$ m s -1 from $P$ through the left focus of the elliptical track and reflects off a point $Q$ on the ellipse towards the right focus. The right focus then directs the laser ray back towards the point where it was shot from. The time taken in the process is denoted by $\delta$ s. The angular displacement of the current position of $P$ with respect to the point where the laser eventually hits is $\phi$ rad. Given that $$\alpha+\omega+\theta=\phi$$ for time $t\ge\delta$, and  $$\theta=\frac{2\sqrt3}3e^{-\frac t2}\sin\frac{\sqrt3}2t$$ for $0\le t <\delta$, find the parametric equations describing the motion of $P$ with respect to time $t$. By the geometric definition of an ellipse, $PF_1+PF_2=2a$. Hence, the distance traveled by the laser is  \begin{align*} PF_1+F_1Q+QF_2+F_2P &=\left(PF_1+PF_2\right)+\left(QF_1+QF_2\right)\\ &=4a \end{align*} The time $\delta$ s taken for the laser to travel back is $$\delta=\frac{4a}{c}$$ Hence the angular distance is such that $$\phi=\theta\left(t\right)-\theta\left(t-\delta\right)=\theta\left(t\right)-\theta\left(t-\frac{4a}{c}\right)$$ Rewriting the given differential equation, \begin{align*} \alpha+\omega+\theta&=\phi\\ \alpha\left(t\right)+\omega\left(t\right) +\theta\left(t\right)&=\theta\left(t\right)-\theta\left(t-\delta\right)\\ \alpha\left(t\right)+\omega\left(t\right) +\theta\left(t-\delta\right)&=0\\ \alpha\left(t-\delta\right)+\omega\left(t-\delta\right) +\theta\left(t-2\delta\right)&=0\\ \end{align*}","TLDR: Hi, is there an exam friendly* method to solve (without using numerical methods)  differential equations of the form $$\theta''(t)+\theta'(t)+\theta(t-\delta)=0$$ I have been searching for 'delay differential equations' and only first order DDEs solutions seem to show up. *possible with bare hands, a graphing calculator, (and a reasonable level of mathematical intelligence for a pretty smart high school student) Basically, I am working on making a mock paper for my friends (still in high school) and ended up setting a question on 2nd order ODE that I can't solve. So I'm curious on whether there is a (hopefully not too advanced) way to solve such ODEs. The full question and the solution I have worked on so far is below: Consider an elliptical track at the standard position such that its major and minor axes lie on the $x$-axis and $y$-axis respectively. A point $P\left(a\cos \theta,b\sin \theta\right)$ on the ellipse, where $a>b>0$, is moving with an angular acceleration $\alpha$ rad s -2 , an angular velocity $\omega$ rad s -1 , and an angular displacement $\theta$ rad measured in an anticlockwise manner from the starting point $P_0\left(a,0\right)$. As $P$ is moving, a laser ray is continuously shot with the speed of light $c$ m s -1 from $P$ through the left focus of the elliptical track and reflects off a point $Q$ on the ellipse towards the right focus. The right focus then directs the laser ray back towards the point where it was shot from. The time taken in the process is denoted by $\delta$ s. The angular displacement of the current position of $P$ with respect to the point where the laser eventually hits is $\phi$ rad. Given that $$\alpha+\omega+\theta=\phi$$ for time $t\ge\delta$, and  $$\theta=\frac{2\sqrt3}3e^{-\frac t2}\sin\frac{\sqrt3}2t$$ for $0\le t <\delta$, find the parametric equations describing the motion of $P$ with respect to time $t$. By the geometric definition of an ellipse, $PF_1+PF_2=2a$. Hence, the distance traveled by the laser is  \begin{align*} PF_1+F_1Q+QF_2+F_2P &=\left(PF_1+PF_2\right)+\left(QF_1+QF_2\right)\\ &=4a \end{align*} The time $\delta$ s taken for the laser to travel back is $$\delta=\frac{4a}{c}$$ Hence the angular distance is such that $$\phi=\theta\left(t\right)-\theta\left(t-\delta\right)=\theta\left(t\right)-\theta\left(t-\frac{4a}{c}\right)$$ Rewriting the given differential equation, \begin{align*} \alpha+\omega+\theta&=\phi\\ \alpha\left(t\right)+\omega\left(t\right) +\theta\left(t\right)&=\theta\left(t\right)-\theta\left(t-\delta\right)\\ \alpha\left(t\right)+\omega\left(t\right) +\theta\left(t-\delta\right)&=0\\ \alpha\left(t-\delta\right)+\omega\left(t-\delta\right) +\theta\left(t-2\delta\right)&=0\\ \end{align*}",,"['calculus', 'ordinary-differential-equations', 'conic-sections', 'delay-differential-equations']"
99,$xy' + 1 = e^{x-y}$,,xy' + 1 = e^{x-y},I need help solving this differential equation $xy' + 1 = e^{x-y}$ I wrote this as $xe^ydy = (e^x-e^y)dx$ and tried finding integrating factor but unsuccessfully. Any help will be much appreciated. I found this problem from entrance exam for some university,I need help solving this differential equation $xy' + 1 = e^{x-y}$ I wrote this as $xe^ydy = (e^x-e^y)dx$ and tried finding integrating factor but unsuccessfully. Any help will be much appreciated. I found this problem from entrance exam for some university,,['ordinary-differential-equations']
