,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Area of segment of folium of descartes,Area of segment of folium of descartes,,"I was looking through past exams for a multi course I'm taking and came across this problem which I'm not sure how to approach. Find the area of the two regions formed by the part of the folium of descartes which passes through the triangle formed by the vertical tangent to the leaf, the x-axis and the line from the intersection of the tangent to the origin. The equation of the folium of descartes is: $$x^3+y^3-3xy=0$$ I've found the tangent line which intersects at $(\sqrt[3]{4}, \sqrt[3]{2})$, so there are two regions within the triangle with vertices $(\sqrt[3]{4}, \sqrt[3]{2}), (0,0), (\sqrt[3]{4}, 0)$ as it is divided by the part of the folium that passes through the triangle. But how do I find the area of either of these parts? Here's a picture to make things clearer. I'm interested in the areas labelled A1 and A2:","I was looking through past exams for a multi course I'm taking and came across this problem which I'm not sure how to approach. Find the area of the two regions formed by the part of the folium of descartes which passes through the triangle formed by the vertical tangent to the leaf, the x-axis and the line from the intersection of the tangent to the origin. The equation of the folium of descartes is: $$x^3+y^3-3xy=0$$ I've found the tangent line which intersects at $(\sqrt[3]{4}, \sqrt[3]{2})$, so there are two regions within the triangle with vertices $(\sqrt[3]{4}, \sqrt[3]{2}), (0,0), (\sqrt[3]{4}, 0)$ as it is divided by the part of the folium that passes through the triangle. But how do I find the area of either of these parts? Here's a picture to make things clearer. I'm interested in the areas labelled A1 and A2:",,"['calculus', 'geometry', 'multivariable-calculus']"
1,Lagrange multipliers with two constraints.,Lagrange multipliers with two constraints.,,"I have been working on the following problem. I need to find a minimum of the following function: $$\ 6x-y^2+xz+60=0 $$ subject to the following constraints: $$ z-x+y=0 \\ x^2+y^2+z^2=36 $$ I have the following equations for the Lagrange multipliers: $$ 6+z = 2\lambda x -\mu \\ -2y = 2 \lambda y + \mu \\ z = 2\lambda z + \mu \\ x^2 + y^2+ z^2 = 36 \\ z-x+y=0$$ I have found that the system does not have solution on the reals, nor the system defined by the first three equations so there are no points to evaluate for a minimum or maximum. Am i correct? Or am I missing something. Any help will be appretiated.","I have been working on the following problem. I need to find a minimum of the following function: $$\ 6x-y^2+xz+60=0 $$ subject to the following constraints: $$ z-x+y=0 \\ x^2+y^2+z^2=36 $$ I have the following equations for the Lagrange multipliers: $$ 6+z = 2\lambda x -\mu \\ -2y = 2 \lambda y + \mu \\ z = 2\lambda z + \mu \\ x^2 + y^2+ z^2 = 36 \\ z-x+y=0$$ I have found that the system does not have solution on the reals, nor the system defined by the first three equations so there are no points to evaluate for a minimum or maximum. Am i correct? Or am I missing something. Any help will be appretiated.",,"['calculus', 'multivariable-calculus', 'optimization']"
2,Solving a Diffrential Equation by Taking Derivative,Solving a Diffrential Equation by Taking Derivative,,"Taking a derivative of a differential equation and solving it is one approach to solving a differential equation. However, for some reason I can't get this method to work in cases where there is an interaction term. Let's consider the differential equation: $$y'+yx=0$$ Derived with respect to x we have: $$y''+y'x+x'y = 0$$ $$\rightarrow y'' +y'x+y=0$$ If the differential equation is solved the answer is different for the equations above. Is there something that I am missing when taking the derivative of the equation?","Taking a derivative of a differential equation and solving it is one approach to solving a differential equation. However, for some reason I can't get this method to work in cases where there is an interaction term. Let's consider the differential equation: $$y'+yx=0$$ Derived with respect to x we have: $$y''+y'x+x'y = 0$$ $$\rightarrow y'' +y'x+y=0$$ If the differential equation is solved the answer is different for the equations above. Is there something that I am missing when taking the derivative of the equation?",,"['ordinary-differential-equations', 'multivariable-calculus']"
3,Relaxed condition for differentiability - proof check!,Relaxed condition for differentiability - proof check!,,"I am trying to show that if the two partials of $f:\mathbb{R}^2\to\mathbb{R}$ exist at point, it is enough for only one to be continuous there to imply that $f$ is differentiable there. To show this I have decided to prove that: $f:\mathbb{R}^2\to \mathbb{R}$ not differentiable at $(a,b)$. If $\partial_x f$   exists and is continuous at $(a,b)$, then $\partial_y f$ cannot exist at $(a,b)$. My attempt Assume we have the conditions above. Take $h,k$ (wlog positive) and a sequence $t_0>t_1>\cdots$ where $t_n\to 0$, and $t_0>0$ sufficiently small. For each $n$, by the Mean Value theorem, there exists $a<c_n<a+t_nh$ such that: $$\frac{1}{t_n}\Big[ f(a+t_nh,b+t_nk)-f(a,b+t_nk)\Big]=\partial_x\,f (c_n)$$ As $t_n\to 0$ this converges because $\partial_x f$ exists at $(a,b)$. Write LHS as: $$\frac{1}{t_n}\Big[ f(a+t_nh,b+t_nk)-f(a,b)\Big]+\frac{1}{t_n}\Big[ f(a,b)-f(a,b+t_nk)\Big]$$ Since the expression on the left does not converge, the one on the right cannot converge either. So $\partial_y f$ does not exist at $(a,b)$. Question I would be really happy if someone could tell me if any of this is correct and if not whether I am on track.","I am trying to show that if the two partials of $f:\mathbb{R}^2\to\mathbb{R}$ exist at point, it is enough for only one to be continuous there to imply that $f$ is differentiable there. To show this I have decided to prove that: $f:\mathbb{R}^2\to \mathbb{R}$ not differentiable at $(a,b)$. If $\partial_x f$   exists and is continuous at $(a,b)$, then $\partial_y f$ cannot exist at $(a,b)$. My attempt Assume we have the conditions above. Take $h,k$ (wlog positive) and a sequence $t_0>t_1>\cdots$ where $t_n\to 0$, and $t_0>0$ sufficiently small. For each $n$, by the Mean Value theorem, there exists $a<c_n<a+t_nh$ such that: $$\frac{1}{t_n}\Big[ f(a+t_nh,b+t_nk)-f(a,b+t_nk)\Big]=\partial_x\,f (c_n)$$ As $t_n\to 0$ this converges because $\partial_x f$ exists at $(a,b)$. Write LHS as: $$\frac{1}{t_n}\Big[ f(a+t_nh,b+t_nk)-f(a,b)\Big]+\frac{1}{t_n}\Big[ f(a,b)-f(a,b+t_nk)\Big]$$ Since the expression on the left does not converge, the one on the right cannot converge either. So $\partial_y f$ does not exist at $(a,b)$. Question I would be really happy if someone could tell me if any of this is correct and if not whether I am on track.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'proof-verification', 'partial-derivative']"
4,Find the double integral by changing to polar coordinates [duplicate],Find the double integral by changing to polar coordinates [duplicate],,"This question already has an answer here : By changing to polar coordinates, calculate the integral. (1 answer) Closed 8 years ago . So I have the following double integral $$\int_{-2}^{0} \int_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}} \sqrt{x^2+y^2} dydx$$ If I integrate with respect to y first I get: $\frac{1}{2}(y\sqrt{x^2+y^2}+x^2\log(\sqrt{x^2+y^2}+y)$ I understand that in polar coordinates $x=r\cos\theta$ and $y=r\sin\theta$, but am unsure how to convert the limits to polar coordinates? Thanks","This question already has an answer here : By changing to polar coordinates, calculate the integral. (1 answer) Closed 8 years ago . So I have the following double integral $$\int_{-2}^{0} \int_{-\sqrt{4-x^2}}^{\sqrt{4-x^2}} \sqrt{x^2+y^2} dydx$$ If I integrate with respect to y first I get: $\frac{1}{2}(y\sqrt{x^2+y^2}+x^2\log(\sqrt{x^2+y^2}+y)$ I understand that in polar coordinates $x=r\cos\theta$ and $y=r\sin\theta$, but am unsure how to convert the limits to polar coordinates? Thanks",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'calculus-of-variations']"
5,Double integral over annulus,Double integral over annulus,,"Evaluate $$\iint_D\ e^{-x^2 - y^2}\ dA,$$   where $D$ is annulus $a \le x^2 + y^2 \le b$ My understanding is it involves polar coordinates but I don't understand how to convert it.","Evaluate $$\iint_D\ e^{-x^2 - y^2}\ dA,$$   where $D$ is annulus $a \le x^2 + y^2 \le b$ My understanding is it involves polar coordinates but I don't understand how to convert it.",,['multivariable-calculus']
6,Using Green's Theorem to find area enclosed by curve,Using Green's Theorem to find area enclosed by curve,,"Use Green's theorem to calculate the area enclosed by the curve: $x^{2/3}+y^{2/3}=4$ Knowing that $A=\frac{1}{2}\int_c xdy-ydx$ I know that there are already some questions and answers on this site regarding Green's theorem, and I've read many of them. I still really do not know how to proceed, though. Any help or direction would be appreciated. Thanks.","Use Green's theorem to calculate the area enclosed by the curve: $x^{2/3}+y^{2/3}=4$ Knowing that $A=\frac{1}{2}\int_c xdy-ydx$ I know that there are already some questions and answers on this site regarding Green's theorem, and I've read many of them. I still really do not know how to proceed, though. Any help or direction would be appreciated. Thanks.",,"['multivariable-calculus', 'greens-theorem', 'parametrization']"
7,Lagrange Multipliers: find points farthest/closest to a point,Lagrange Multipliers: find points farthest/closest to a point,,"Find the points of the ellipse: $$\frac{x^2}{9}+\frac{y^2}{4}=1$$ which are closest to and farthest from the point $(1,1)$. I use the method of the Lagrange Multipliers by setting: $$f(x,y)=(x-1)^2+(y-1)^2$$ (no need for the root since maximizing one maximizes the other) And $$g(x,y)=\frac{x^2}{9}+\frac{y^2}{4}-1=0$$ the constraint. We get: $$x-1=λx/9$$           and           $$2y-2=λy/2$$ I don't know where to go from there. I tried finding $x$ and $y$ alone and then plugging their values in the constraint $g(x,y)=0$ but I get a really complicated equation.","Find the points of the ellipse: $$\frac{x^2}{9}+\frac{y^2}{4}=1$$ which are closest to and farthest from the point $(1,1)$. I use the method of the Lagrange Multipliers by setting: $$f(x,y)=(x-1)^2+(y-1)^2$$ (no need for the root since maximizing one maximizes the other) And $$g(x,y)=\frac{x^2}{9}+\frac{y^2}{4}-1=0$$ the constraint. We get: $$x-1=λx/9$$           and           $$2y-2=λy/2$$ I don't know where to go from there. I tried finding $x$ and $y$ alone and then plugging their values in the constraint $g(x,y)=0$ but I get a really complicated equation.",,"['multivariable-calculus', 'lagrange-multiplier']"
8,Computing a line integral along a circle in $3$-D,Computing a line integral along a circle in -D,3,"Let $C$ be the curve of intersection of the two surfaces $x+y=2 , x^2+y^2+z^2=2(x+y)$ . The curve is to be traversed in clockwise direction as viewed from the origin . The what is the value of $\int_Cydx+zdy+xdz$ ? I am not even able to parametrize the curve of intersection . Please help . Thanks in advance","Let $C$ be the curve of intersection of the two surfaces $x+y=2 , x^2+y^2+z^2=2(x+y)$ . The curve is to be traversed in clockwise direction as viewed from the origin . The what is the value of $\int_Cydx+zdy+xdz$ ? I am not even able to parametrize the curve of intersection . Please help . Thanks in advance",,['multivariable-calculus']
9,How to deal with this boundary condition when using separation of variables?,How to deal with this boundary condition when using separation of variables?,,"Consider Laplace's equation $\nabla^2u = 0$ on the region $A = [0,a]\times [0,b]\times [0,c]\subset \mathbb{R}^3$ and suppose we impose the boundary conditions: $$u(0,y,z) = \sin \frac{\pi y}{b} \sin \frac{\pi z}{c}, \quad u(a,y,z)=0 \\ u(x,0,z)=u(x,b,z)=u(x,y,0)=u(x,y,c)=0.$$ In that case if we use separation of variables we set $u(x,y,z)=X(x)Y(y)Z(z)$ and the equation reduces to $$\frac{X''}{X}+\frac{Y''}{Y}+\frac{Z''}{Z}=0.$$ In that case reorganizing gives $$\frac{X''}{X}+\frac{Y''}{Y}=-\frac{Z''}{Z}=\lambda$$ for some constant $\lambda$. Thus we have two new equations, one of which is $$\frac{X''}{X} = \lambda-\frac{Y''}{Y} = \mu,$$ for some constant $\mu$. Finally we end up having three equations $$\begin{cases}Z''+ \lambda Z &= 0, \\ Y'' + (\mu+\lambda)Y&=0, \\ X'' - \mu X &=0\end{cases}.$$ The boundary conditions which are zero on the boundaries transfer nicely to the functions $X,Y,Z$. That is, except the first condition, all the other becomes $$X(a)=0, Y(0)=Y(b)=0, Z(0)=Z(c)=0.$$ With this the equations for $Y$ and $Z$ with those boundary conditions becomes Sturm Liouville problems. On the other hand the first condition does not separate nicely. We have $$X(0)Y(y)Z(z) = \sin \frac{\pi y}{b} \sin \frac{\pi z}{c}.$$ This boundary condition doesn't seem to transfer nicely to $X$. So how do we deal with this boundary condition? How do we use it to solve the problem in question?","Consider Laplace's equation $\nabla^2u = 0$ on the region $A = [0,a]\times [0,b]\times [0,c]\subset \mathbb{R}^3$ and suppose we impose the boundary conditions: $$u(0,y,z) = \sin \frac{\pi y}{b} \sin \frac{\pi z}{c}, \quad u(a,y,z)=0 \\ u(x,0,z)=u(x,b,z)=u(x,y,0)=u(x,y,c)=0.$$ In that case if we use separation of variables we set $u(x,y,z)=X(x)Y(y)Z(z)$ and the equation reduces to $$\frac{X''}{X}+\frac{Y''}{Y}+\frac{Z''}{Z}=0.$$ In that case reorganizing gives $$\frac{X''}{X}+\frac{Y''}{Y}=-\frac{Z''}{Z}=\lambda$$ for some constant $\lambda$. Thus we have two new equations, one of which is $$\frac{X''}{X} = \lambda-\frac{Y''}{Y} = \mu,$$ for some constant $\mu$. Finally we end up having three equations $$\begin{cases}Z''+ \lambda Z &= 0, \\ Y'' + (\mu+\lambda)Y&=0, \\ X'' - \mu X &=0\end{cases}.$$ The boundary conditions which are zero on the boundaries transfer nicely to the functions $X,Y,Z$. That is, except the first condition, all the other becomes $$X(a)=0, Y(0)=Y(b)=0, Z(0)=Z(c)=0.$$ With this the equations for $Y$ and $Z$ with those boundary conditions becomes Sturm Liouville problems. On the other hand the first condition does not separate nicely. We have $$X(0)Y(y)Z(z) = \sin \frac{\pi y}{b} \sin \frac{\pi z}{c}.$$ This boundary condition doesn't seem to transfer nicely to $X$. So how do we deal with this boundary condition? How do we use it to solve the problem in question?",,"['multivariable-calculus', 'partial-differential-equations', 'mathematical-physics', 'boundary-value-problem', 'sturm-liouville']"
10,Computation of the integral $ \iint_{D} \frac{x^2}{(x^2 + y^2)^{3/2}} dxdy $,Computation of the integral, \iint_{D} \frac{x^2}{(x^2 + y^2)^{3/2}} dxdy ,"I'm having some troubles trying to compute the following integral: $$ I = \iint_{D} \frac{x^2}{(x^2 + y^2)^{3/2}} dxdy $$ where $ D = \{ (x,y) \in \mathbb{R} : 2 \leq x^2 + y^2 \leq 2y \} $. What I've tried so far: I plotted the domain $ D $: Using the following transformation (polar coordinates): $$ \left\{\begin{matrix}x = r\cos\theta\\y = r\sin\theta\end{matrix}\right. $$ I get the integral: $$ \int_{\pi/4}^{3\pi/4} \int_{\sqrt{2}}^{2\sin\theta} \cos^2(\theta) \, dr d\theta $$ The result I should arrive is: $$ I = \frac{2\sqrt{2} + 3\pi + 6}{6} $$ but according to Wolfram what I get is: $$ \frac{10 - 3\pi}{6\sqrt{2}} $$ Questions: What am I doing wrong? How should I write the integral? Could you recommend me a good source with several examples of problems like this one?","I'm having some troubles trying to compute the following integral: $$ I = \iint_{D} \frac{x^2}{(x^2 + y^2)^{3/2}} dxdy $$ where $ D = \{ (x,y) \in \mathbb{R} : 2 \leq x^2 + y^2 \leq 2y \} $. What I've tried so far: I plotted the domain $ D $: Using the following transformation (polar coordinates): $$ \left\{\begin{matrix}x = r\cos\theta\\y = r\sin\theta\end{matrix}\right. $$ I get the integral: $$ \int_{\pi/4}^{3\pi/4} \int_{\sqrt{2}}^{2\sin\theta} \cos^2(\theta) \, dr d\theta $$ The result I should arrive is: $$ I = \frac{2\sqrt{2} + 3\pi + 6}{6} $$ but according to Wolfram what I get is: $$ \frac{10 - 3\pi}{6\sqrt{2}} $$ Questions: What am I doing wrong? How should I write the integral? Could you recommend me a good source with several examples of problems like this one?",,"['integration', 'multivariable-calculus']"
11,Double integral using change of variables,Double integral using change of variables,,"I must evaluate $\iint_B(x^2+y^2)dxdy$ using the change of variables $u=x^2-y^2,v=xy$, where $B$ is the region in the first quadrant bounded by $xy=1,xy=3,x^2-y^2=4,x^2-y^2=1$. I know that we must have: $$\iint_Df(x,y)dxdy=\iint_Sg(u,v) \left|\frac{\partial (x,y)}{\partial (u,v)}\right|dudv$$ I have computed this Jacobian to be $\frac{1}{4v}$, and $S$ will be represented by $1 \leq u \leq 4, 1 \leq v \leq 3$. However, I am unsure of how to find $g(u,v)$ in this scenario; from other examples I've seen, it's usually a simple substitution. Any help in how to transform $f(x,y)=x^2+y^2$ into a form $g(u,v)$ would be greatly appreciated. Thank you!","I must evaluate $\iint_B(x^2+y^2)dxdy$ using the change of variables $u=x^2-y^2,v=xy$, where $B$ is the region in the first quadrant bounded by $xy=1,xy=3,x^2-y^2=4,x^2-y^2=1$. I know that we must have: $$\iint_Df(x,y)dxdy=\iint_Sg(u,v) \left|\frac{\partial (x,y)}{\partial (u,v)}\right|dudv$$ I have computed this Jacobian to be $\frac{1}{4v}$, and $S$ will be represented by $1 \leq u \leq 4, 1 \leq v \leq 3$. However, I am unsure of how to find $g(u,v)$ in this scenario; from other examples I've seen, it's usually a simple substitution. Any help in how to transform $f(x,y)=x^2+y^2$ into a form $g(u,v)$ would be greatly appreciated. Thank you!",,['calculus']
12,$a$ is a stationary point of a scalar field $f$ whose Hessian at $a$ has two opposite sign diagonal entries. Is $a$ a saddle point?,is a stationary point of a scalar field  whose Hessian at  has two opposite sign diagonal entries. Is  a saddle point?,a f a a,"Let $S$ be some subset of $\mathbb R^n$ , $f:S \to \mathbb R$ is a function such that for some $a \in S$ , there is an open ball $B(a)\subseteq S$ such that $f$ has all second order partial derivatives (continuous) in $B(a)$ , $\nabla f(a)=O $ and the Hessian of $f$ at $a$ has at least  two opposite sign diagonal entries . Then how to prove that $a$ is a saddle point of $f$ ?","Let $S$ be some subset of $\mathbb R^n$ , $f:S \to \mathbb R$ is a function such that for some $a \in S$ , there is an open ball $B(a)\subseteq S$ such that $f$ has all second order partial derivatives (continuous) in $B(a)$ , $\nabla f(a)=O $ and the Hessian of $f$ at $a$ has at least  two opposite sign diagonal entries . Then how to prove that $a$ is a saddle point of $f$ ?",,['multivariable-calculus']
13,Meaning of $\int y dx +x dy$,Meaning of,\int y dx +x dy,"What does it mean to take an integral: $\int y dx + xdy$ I would guess that this means that this integral over a region multiplies the infinitely small increments of $x$ ($\Delta x$) by $y$ and the infinitely small increments of $y$ ($\Delta y$) by x, but I still can't really picture what this integral does. Is it the same as a double integral? I'm sorry for how vague this question is, but any material you can provide me with would be appreciated.","What does it mean to take an integral: $\int y dx + xdy$ I would guess that this means that this integral over a region multiplies the infinitely small increments of $x$ ($\Delta x$) by $y$ and the infinitely small increments of $y$ ($\Delta y$) by x, but I still can't really picture what this integral does. Is it the same as a double integral? I'm sorry for how vague this question is, but any material you can provide me with would be appreciated.",,"['integration', 'multivariable-calculus', 'line-integrals']"
14,sketch the region of triple integral and express it in the five other possible orders,sketch the region of triple integral and express it in the five other possible orders,,Sketch the region of the triple integral $$\int_{0}^1 \int_{0}^{y} \int_0^{x}  x^2yz dzdxdy$$ Please help!,Sketch the region of the triple integral $$\int_{0}^1 \int_{0}^{y} \int_0^{x}  x^2yz dzdxdy$$ Please help!,,"['integration', 'multivariable-calculus']"
15,"Examine $\lim_{(x,y)\to (0,0)}\frac{ax^2+bxy+cy^2}{x^2+y^2}=0$",Examine,"\lim_{(x,y)\to (0,0)}\frac{ax^2+bxy+cy^2}{x^2+y^2}=0","The question is what can we conclude about $a,b,c$ if $\displaystyle \lim_{(x,y)\to (0,0)}\frac{ax^2+bxy+cy^2}{x^2+y^2}=0$. Let $f(x,y)$ be the given function. Firstly, if we switch to polar coordinates we get that $f(x,y)=f(\theta)=a\cos^2\theta+b\cos\theta \sin\theta+c\sin^2\theta$. I can't see how this can give us more information. We can try this approach: the limit exists and it is equal to $0$, so if we approach $(0,0)$ from any path, the function will take the value $0$. For example, we can examine the paths $y=x$ then $\displaystyle f(x,x)=\frac{ax^2+bx^2+cx^2}{x^2+x^2}=\frac{a+b+c}{2}$ is $x$ independent so it must be equal to $0\Leftrightarrow  a+b+c=0$. I tried some function examples when $a,b,c$ follow that condition in Wolfram Alpha but their limit didn't exist when $(x,y)\to(0,0)$. Therefore $a+b+c=0$ doesn't seem to be a sufficient condition. another path: $y=0$ then $\displaystyle f(x,0)=\frac{ax^2}{x^2}=a$ it must be equal to $0$. Similarly the path $x=0$ gives us $\displaystyle f(0,y)=\frac{cy^2}{y^2}=c=0$ and by combining the results we get that $a=b=c=0$. This conclusion makes me think that I've done a mistake somewhere. Any hints? Thank you in advance!","The question is what can we conclude about $a,b,c$ if $\displaystyle \lim_{(x,y)\to (0,0)}\frac{ax^2+bxy+cy^2}{x^2+y^2}=0$. Let $f(x,y)$ be the given function. Firstly, if we switch to polar coordinates we get that $f(x,y)=f(\theta)=a\cos^2\theta+b\cos\theta \sin\theta+c\sin^2\theta$. I can't see how this can give us more information. We can try this approach: the limit exists and it is equal to $0$, so if we approach $(0,0)$ from any path, the function will take the value $0$. For example, we can examine the paths $y=x$ then $\displaystyle f(x,x)=\frac{ax^2+bx^2+cx^2}{x^2+x^2}=\frac{a+b+c}{2}$ is $x$ independent so it must be equal to $0\Leftrightarrow  a+b+c=0$. I tried some function examples when $a,b,c$ follow that condition in Wolfram Alpha but their limit didn't exist when $(x,y)\to(0,0)$. Therefore $a+b+c=0$ doesn't seem to be a sufficient condition. another path: $y=0$ then $\displaystyle f(x,0)=\frac{ax^2}{x^2}=a$ it must be equal to $0$. Similarly the path $x=0$ gives us $\displaystyle f(0,y)=\frac{cy^2}{y^2}=c=0$ and by combining the results we get that $a=b=c=0$. This conclusion makes me think that I've done a mistake somewhere. Any hints? Thank you in advance!",,"['calculus', 'limits', 'multivariable-calculus']"
16,$C^2$ function on closed domain has finite solutions,function on closed domain has finite solutions,C^2,"Let $\Omega $ be a bounded open set of $R^n$ and $f\in C^2(\overline\Omega)$. Define $N_f=\{x|x\in\Omega,J_f(x)=0\}$, where $J_f$ is the Jacobian of $f$. How can I show that For all $ p\notin f(N_f)$ with    $\inf\limits_{x\in\partial\Omega}||f(x)-p||>0,$ the equation $f(x)=p $   has a finite number of solutions ?","Let $\Omega $ be a bounded open set of $R^n$ and $f\in C^2(\overline\Omega)$. Define $N_f=\{x|x\in\Omega,J_f(x)=0\}$, where $J_f$ is the Jacobian of $f$. How can I show that For all $ p\notin f(N_f)$ with    $\inf\limits_{x\in\partial\Omega}||f(x)-p||>0,$ the equation $f(x)=p $   has a finite number of solutions ?",,"['real-analysis', 'multivariable-calculus']"
17,Verifying Divergence Theorem,Verifying Divergence Theorem,,"I have to verify Gauss' Divergence Theorem for $$\bar F(x,y,z)=xy^2 \hat i+yz^2\hat j+zx^2\hat k$$ for the region $$R:y^2+z^2\le x^2;\;\;0\le x\le4$$ Now, $\operatorname{div}\bar F=x^2+y^2+z^2$. I don't understand what type of parametrization will make calculating the triple integral easy. I tried $\bar r=x\hat i+x\cos\theta\hat j+x\sin\theta\hat k$ where $0\le \theta \le 2\pi$. But then, $\operatorname{div}\bar F =r^2$. Would then $$\iiint_R \operatorname{div}\bar F\;dV=\int_0^4\int_0^{2\pi}\int_0^{\sqrt2x}r^2\;rdr\;d\theta\;dx$$ be correct?","I have to verify Gauss' Divergence Theorem for $$\bar F(x,y,z)=xy^2 \hat i+yz^2\hat j+zx^2\hat k$$ for the region $$R:y^2+z^2\le x^2;\;\;0\le x\le4$$ Now, $\operatorname{div}\bar F=x^2+y^2+z^2$. I don't understand what type of parametrization will make calculating the triple integral easy. I tried $\bar r=x\hat i+x\cos\theta\hat j+x\sin\theta\hat k$ where $0\le \theta \le 2\pi$. But then, $\operatorname{div}\bar F =r^2$. Would then $$\iiint_R \operatorname{div}\bar F\;dV=\int_0^4\int_0^{2\pi}\int_0^{\sqrt2x}r^2\;rdr\;d\theta\;dx$$ be correct?",,"['multivariable-calculus', 'vector-analysis']"
18,How does this partial differentiation work?,How does this partial differentiation work?,,"Good day, There was a partial derivative in the lecture today that I can't comprehend. Can someone please explain how this works? Maybe there is a rule that I'm not thinking of right now. It's about the Cauchy problem in a quasilinear PDE second order for independent variables $x,y \in \mathbb{R}$. Further let $\eta \in I \subset \mathbb{R}$. Let $$n=\frac{1}{\sqrt{x'(\eta)+y'(\eta)}} \binom{-y'(\eta)}{x'(\eta)}$$  Then $$ \frac{\partial u(x(\eta),y(\eta))}{\partial n}=\frac{1}{\sqrt{x'(\eta)+y'(\eta)}} \left(-y'(\eta)\frac{\partial u(x(\eta),y(\eta))}{\partial x}+x'(\eta) \frac{\partial u(x(\eta),y(\eta))}{\partial y}\right) $$ But why? It seems the derivate is just $$\frac{\partial u}{\partial n}=<n,\nabla u>$$ Is this a kind of rule? Doesn't look like chain rule to me. Thanks a lot, Marvin","Good day, There was a partial derivative in the lecture today that I can't comprehend. Can someone please explain how this works? Maybe there is a rule that I'm not thinking of right now. It's about the Cauchy problem in a quasilinear PDE second order for independent variables $x,y \in \mathbb{R}$. Further let $\eta \in I \subset \mathbb{R}$. Let $$n=\frac{1}{\sqrt{x'(\eta)+y'(\eta)}} \binom{-y'(\eta)}{x'(\eta)}$$  Then $$ \frac{\partial u(x(\eta),y(\eta))}{\partial n}=\frac{1}{\sqrt{x'(\eta)+y'(\eta)}} \left(-y'(\eta)\frac{\partial u(x(\eta),y(\eta))}{\partial x}+x'(\eta) \frac{\partial u(x(\eta),y(\eta))}{\partial y}\right) $$ But why? It seems the derivate is just $$\frac{\partial u}{\partial n}=<n,\nabla u>$$ Is this a kind of rule? Doesn't look like chain rule to me. Thanks a lot, Marvin",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
19,"Find the volume of the solid bounded above by the cone $z^2 = x^2 + y^2$, below by the $xy$ plane, and on the sides by the cylinder$ x^2 + y^2 = 6x$.","Find the volume of the solid bounded above by the cone , below by the  plane, and on the sides by the cylinder.",z^2 = x^2 + y^2 xy  x^2 + y^2 = 6x,"Q: Find the volume of the solid bounded above by the cone $z^2 = x^2 + y^2$, below by the xy plane, and on the sides by the cylinder $x^2 + y^2 = 6x$. I can't figure out what I'm doing wrong in my setup. I keep getting $0$ for the volume value based on my setup. However, if you sketch the volume out, it seems like there should be a volume in 3d for this shape. My Work $\int{_{0}^{2\pi}\int{_{0}^{6cos\theta}\int{_{0}^{r}r dzdrd\theta}}}$ Since the xy plane was a bound, I assumed you needed the top part of the cone so $z = \sqrt{x^2+y^2}$ or $z = r$ for the top z bound. Since $x^2 + y^2 = 6x$, polar conversion equations give $r^2 = 6rcos\theta$ or $r^2 - 6rcos\theta=0$ so $r=0$ and $r = 6cos\theta$ from that. $x^2 + y^2 = 6x$ is a full circle so I let $\theta = 0$ to $\theta = 2\pi$ bounds. There is a shift in the circle but I don't believe it affects the $\theta$ bounds.","Q: Find the volume of the solid bounded above by the cone $z^2 = x^2 + y^2$, below by the xy plane, and on the sides by the cylinder $x^2 + y^2 = 6x$. I can't figure out what I'm doing wrong in my setup. I keep getting $0$ for the volume value based on my setup. However, if you sketch the volume out, it seems like there should be a volume in 3d for this shape. My Work $\int{_{0}^{2\pi}\int{_{0}^{6cos\theta}\int{_{0}^{r}r dzdrd\theta}}}$ Since the xy plane was a bound, I assumed you needed the top part of the cone so $z = \sqrt{x^2+y^2}$ or $z = r$ for the top z bound. Since $x^2 + y^2 = 6x$, polar conversion equations give $r^2 = 6rcos\theta$ or $r^2 - 6rcos\theta=0$ so $r=0$ and $r = 6cos\theta$ from that. $x^2 + y^2 = 6x$ is a full circle so I let $\theta = 0$ to $\theta = 2\pi$ bounds. There is a shift in the circle but I don't believe it affects the $\theta$ bounds.",,"['integration', 'multivariable-calculus', 'polar-coordinates', 'volume']"
20,Logic behind normal line in expressing plane,Logic behind normal line in expressing plane,,"why do we consider normal line in expressing a plane,say in $R^3$, of the form $ ax + by + cz = d $? What is the logic behind this normal line selection? Plz provide intuitive explanations.Thanks","why do we consider normal line in expressing a plane,say in $R^3$, of the form $ ax + by + cz = d $? What is the logic behind this normal line selection? Plz provide intuitive explanations.Thanks",,['multivariable-calculus']
21,How to build the solution to a first-order PDE with the method of characteristics?,How to build the solution to a first-order PDE with the method of characteristics?,,"I'm having a hard time to understand the idea of the initial curve in the method of characteristics. Suppose we have a quasi-linear first order PDE in two variables: $$a(x,y,u)u_x+b(x,y,u)u_y=c(x,y,u).$$ I already know that if we consider $S$ the graph of $u$, that is $$S = \{(x,y,u(x,y))\in \mathbb{R}^3 : (x,y)\in \mathbb{R}^2\},$$ then the characteristic curves $c: I\subset \mathbb{R}\to \mathbb{R}^3$ are curves which lie on the surface $S$ and satisfies $$\dfrac{dc_1(t)}{dt}=a(c_1(t)), \quad \dfrac{dc_2(t)}{dt}=b(c_2(t)), \quad \dfrac{dc_3(t)}{dt}=c(c_3(t)),$$ which is the characteristic system. All of that is nice, with this we can get a curve in $S$ passing through each point of the surface. Now, usually to actually solve a problem we also need to give one initial curve which intercepts the characteristic curves. The problem is that I don't get the idea of this initial curve. In fact, I'm so confused with this that I didn't even know how to properly explain what those initial curves are above. In that case what are those initial curves and what is the idea behind them? And mainly, after solving the characteristic system and knowing the parametrization of the characteristic curves, how does one use the initial curve to find the solution to the problem? How to build the solution to the PDE from the characteristic curves and the initial curve together?","I'm having a hard time to understand the idea of the initial curve in the method of characteristics. Suppose we have a quasi-linear first order PDE in two variables: $$a(x,y,u)u_x+b(x,y,u)u_y=c(x,y,u).$$ I already know that if we consider $S$ the graph of $u$, that is $$S = \{(x,y,u(x,y))\in \mathbb{R}^3 : (x,y)\in \mathbb{R}^2\},$$ then the characteristic curves $c: I\subset \mathbb{R}\to \mathbb{R}^3$ are curves which lie on the surface $S$ and satisfies $$\dfrac{dc_1(t)}{dt}=a(c_1(t)), \quad \dfrac{dc_2(t)}{dt}=b(c_2(t)), \quad \dfrac{dc_3(t)}{dt}=c(c_3(t)),$$ which is the characteristic system. All of that is nice, with this we can get a curve in $S$ passing through each point of the surface. Now, usually to actually solve a problem we also need to give one initial curve which intercepts the characteristic curves. The problem is that I don't get the idea of this initial curve. In fact, I'm so confused with this that I didn't even know how to properly explain what those initial curves are above. In that case what are those initial curves and what is the idea behind them? And mainly, after solving the characteristic system and knowing the parametrization of the characteristic curves, how does one use the initial curve to find the solution to the problem? How to build the solution to the PDE from the characteristic curves and the initial curve together?",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'partial-differential-equations']"
22,Proving that a function is not differentiable using a certain definition,Proving that a function is not differentiable using a certain definition,,"I have been given the following function: $$f(x,y) = \begin{cases} \dfrac{xy(2x^2 - y^2)}{2x^2 + y^2} & \text{if $(x, y) \ne (0, 0)$} \\ 0 & \text{if $(x, y) = (0, 0)$} \end{cases}$$ Now we would like to show that the partial derivatives are both continuous at the point $(0, 0)$, but neither one of them is differentiable at (0, 0). First we should analyze the behavior at $(0, 0)$, as the quotient rule will not apply at that point, so we should go by the definition: $$ f_x(0, 0) = \lim_{x \rightarrow 0} \dfrac{f(x, 0) - f(0, 0)}{x} = \lim_{x \rightarrow 0} \dfrac{0 - 0}{x} = 0 $$ $$ f_y(0, 0) = \lim_{y \rightarrow 0} \dfrac{f(0, y) - f(0, 0)}{y} = \lim_{y \rightarrow 0} \dfrac{0 - 0}{y} = 0$$ Then we can obtain the following partial derivatives: $$ \frac{\partial f}{\partial x} = \begin{cases} \dfrac{4 x^4 y + 8 x^2 y^3 - y^5}{\left(2x^2 + y^2\right)^2} & \text{if $(x, y) \ne (0, 0)$} \\ 0 & \text{if $(x, y) = (0, 0)$} \end{cases}$$ $$ \frac{\partial f}{\partial y} = \begin{cases} \dfrac{4x^5 - 8x^3 y^2 - xy^4}{\left(2x^2 + y^2 \right)^2} & \text{if $(x, y) \ne (0, 0)$} \\ 0 & \text{if $(x, y) = (0, 0)$} \end{cases}$$ My goal is to show that both of them are continuous at $(0, 0)$, but neither is differentiable. I have already shown that both of them are continuous by showing that both partials have a limit of $0$ at $(0, 0)$. (I just took the limit in polar coordinates). But to show differentiability, I am to use the following definition of differentiability: A function $f(x, y)$ is differentiable at $(x_0, y_0)$ if it can be expressed in the form: $$ f\left(x, y\right) = f\left(x_0, y_0\right) + \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y + \xi \left(x, y\right)$$ Where $\Delta x = x - x_0$, $\Delta y = y - y_0$, $r = \sqrt{(x - x_0)^2 + (y - y_0)^2}$ and $\xi$ has the following property: For any $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $\delta < r$, $\left | \xi \right | < \epsilon r$. In other words, $\left | \xi \right |$ disappears faster than any linear multiple of $r$ as the point is approached. I am to use this definition to prove that the partial derivatives are not differentiable. How could I prove that, for some $\epsilon > 0$, there is no valid choice of $\delta$?","I have been given the following function: $$f(x,y) = \begin{cases} \dfrac{xy(2x^2 - y^2)}{2x^2 + y^2} & \text{if $(x, y) \ne (0, 0)$} \\ 0 & \text{if $(x, y) = (0, 0)$} \end{cases}$$ Now we would like to show that the partial derivatives are both continuous at the point $(0, 0)$, but neither one of them is differentiable at (0, 0). First we should analyze the behavior at $(0, 0)$, as the quotient rule will not apply at that point, so we should go by the definition: $$ f_x(0, 0) = \lim_{x \rightarrow 0} \dfrac{f(x, 0) - f(0, 0)}{x} = \lim_{x \rightarrow 0} \dfrac{0 - 0}{x} = 0 $$ $$ f_y(0, 0) = \lim_{y \rightarrow 0} \dfrac{f(0, y) - f(0, 0)}{y} = \lim_{y \rightarrow 0} \dfrac{0 - 0}{y} = 0$$ Then we can obtain the following partial derivatives: $$ \frac{\partial f}{\partial x} = \begin{cases} \dfrac{4 x^4 y + 8 x^2 y^3 - y^5}{\left(2x^2 + y^2\right)^2} & \text{if $(x, y) \ne (0, 0)$} \\ 0 & \text{if $(x, y) = (0, 0)$} \end{cases}$$ $$ \frac{\partial f}{\partial y} = \begin{cases} \dfrac{4x^5 - 8x^3 y^2 - xy^4}{\left(2x^2 + y^2 \right)^2} & \text{if $(x, y) \ne (0, 0)$} \\ 0 & \text{if $(x, y) = (0, 0)$} \end{cases}$$ My goal is to show that both of them are continuous at $(0, 0)$, but neither is differentiable. I have already shown that both of them are continuous by showing that both partials have a limit of $0$ at $(0, 0)$. (I just took the limit in polar coordinates). But to show differentiability, I am to use the following definition of differentiability: A function $f(x, y)$ is differentiable at $(x_0, y_0)$ if it can be expressed in the form: $$ f\left(x, y\right) = f\left(x_0, y_0\right) + \frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y + \xi \left(x, y\right)$$ Where $\Delta x = x - x_0$, $\Delta y = y - y_0$, $r = \sqrt{(x - x_0)^2 + (y - y_0)^2}$ and $\xi$ has the following property: For any $\epsilon > 0$, there exists a $\delta > 0$ such that whenever $\delta < r$, $\left | \xi \right | < \epsilon r$. In other words, $\left | \xi \right |$ disappears faster than any linear multiple of $r$ as the point is approached. I am to use this definition to prove that the partial derivatives are not differentiable. How could I prove that, for some $\epsilon > 0$, there is no valid choice of $\delta$?",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
23,"$f \in C(\mathbb R^n)$ be such that for some positive integer $m$ , $\Delta^m_{i,h}f=0 $ ; then $f$ is a polynomial in $n$ variables ?","be such that for some positive integer  ,  ; then  is a polynomial in  variables ?","f \in C(\mathbb R^n) m \Delta^m_{i,h}f=0  f n","Let $f \in C(\mathbb R^n)$ be such that for some positive integer $m$ , $\Delta^m_{i,h}f=0 , \forall h \in \mathbb R , i=1,2,...,n$ ; then how to show that $f$ is a polynomial in $n$ variables ? Here $\Delta_{i,h}f(x)=f(x+he_i)-f(x),\forall x \in \mathbb R^n$ , where $e_i$ is the $i$-th standard basis vector","Let $f \in C(\mathbb R^n)$ be such that for some positive integer $m$ , $\Delta^m_{i,h}f=0 , \forall h \in \mathbb R , i=1,2,...,n$ ; then how to show that $f$ is a polynomial in $n$ variables ? Here $\Delta_{i,h}f(x)=f(x+he_i)-f(x),\forall x \in \mathbb R^n$ , where $e_i$ is the $i$-th standard basis vector",,['multivariable-calculus']
24,Proof regarding homogeneous functions of degree $n$,Proof regarding homogeneous functions of degree,n,"If $f$ is homogeneous of degree $n$, show that $f_{x}(tx,ty) = t^{n-1}f_{x}(x,y)$. My attempt at a solution: Let $u = tx$ and $v = ty$.$$\frac{\partial f(u,v)}{\partial x} = \frac{\partial f(u,v)}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial f(u,v)}{\partial v}\frac{\partial v}{\partial x}.$$ But $\frac{\partial u}{\partial x} = t$ and $\frac{\partial v}{\partial x} = 0.$ Hence,         $$t\frac{\partial f(u,v)}{\partial u} = t^{n}f_{x}(x,y)$$ or $$\frac{\partial f(u,v)}{\partial u} = t^{n-1}f_{x}(x,y).$$ How do I get from the term on the left hand side to $\frac{\partial f(u,v)}{\partial x}$? Thank you in advance.","If $f$ is homogeneous of degree $n$, show that $f_{x}(tx,ty) = t^{n-1}f_{x}(x,y)$. My attempt at a solution: Let $u = tx$ and $v = ty$.$$\frac{\partial f(u,v)}{\partial x} = \frac{\partial f(u,v)}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial f(u,v)}{\partial v}\frac{\partial v}{\partial x}.$$ But $\frac{\partial u}{\partial x} = t$ and $\frac{\partial v}{\partial x} = 0.$ Hence,         $$t\frac{\partial f(u,v)}{\partial u} = t^{n}f_{x}(x,y)$$ or $$\frac{\partial f(u,v)}{\partial u} = t^{n-1}f_{x}(x,y).$$ How do I get from the term on the left hand side to $\frac{\partial f(u,v)}{\partial x}$? Thank you in advance.",,"['calculus', 'multivariable-calculus', 'derivatives']"
25,Abuse of notation or am I missing something,Abuse of notation or am I missing something,,I'm reading a text carefully and I've come across a part that's somewhat confusing. Suppose there exists a $C^1$ function $F: \mathbb{R}^n \to \mathbb{R}^m$ and by fixing $x$ and $y$ we define $\phi(t) = F(x+ty)$. My text says that by the fundamental theorem of calculus we can write $$F(x+y) = F(x) + \int_0^1 DF(x+ty)y \ dt$$ How is the operation in the integrand defined if $DF$ maps to something $m$-dimensional and $y$ is $n$-dimensional. I'm being a bit of a stickler but I want to make sure there are no gaps in my understanding.,I'm reading a text carefully and I've come across a part that's somewhat confusing. Suppose there exists a $C^1$ function $F: \mathbb{R}^n \to \mathbb{R}^m$ and by fixing $x$ and $y$ we define $\phi(t) = F(x+ty)$. My text says that by the fundamental theorem of calculus we can write $$F(x+y) = F(x) + \int_0^1 DF(x+ty)y \ dt$$ How is the operation in the integrand defined if $DF$ maps to something $m$-dimensional and $y$ is $n$-dimensional. I'm being a bit of a stickler but I want to make sure there are no gaps in my understanding.,,"['calculus', 'multivariable-calculus']"
26,Show that gravity is described by this 1-form,Show that gravity is described by this 1-form,,"From Harold Edwards' Advanced Calculus: A Differential Forms Approach , section 2.1, exercise 1: The central force field .  Newton's law of gravitational attraction states that the force exerted by a massive body (the Sun) fixed at the origin $(0,0,0)$ on a particle in space is directed toward the Sun and has magnitude proportional to the inverse square of the distance to the Sun.  Show that this means that the force field is described by the $1$-form $$\frac{kx}{r^3}dx + \frac{ky}{r^3}dy + \frac{kz}{r^3}dz$$ where $k$ is a positive constant and $r=r(x,y,z) = \sqrt{x^2+y^2+z^2}$. Here's what I've done: I know that work (at least for small displacements) in physics is defined as $W = F_xdx + F_ydy + F_zdz$ where the force is $\vec F = (F_x, F_y, F_z)$. I consider the gravity at a point $(x,y,z)$.  Then first the direction toward the Sun is $-x\hat x -y\hat y -z\hat z$.  So the gravity at this point must be proportional to this. Then I think about the magnitude of $\vec F$ and I get $$\| \vec F\| \propto \frac 1{r^2} \implies {F_x}^2 + {F_y}^2 + {F_z}^2 \propto \frac{1}{r^4}$$ This leads me to think that each component $F_i$ is proportional to $\frac 1{r^2}$. So putting that together I get that $\vec F$ should be proportional to $$\left(\frac{-x}{r^2},\frac{-y}{r^2},\frac{-z}{r^2}\right)$$ But this problem states that it should be proportional to $$\left(\frac{x}{r^3},\frac{y}{r^3},\frac{z}{r^3}\right)$$ Where have I gone wrong?","From Harold Edwards' Advanced Calculus: A Differential Forms Approach , section 2.1, exercise 1: The central force field .  Newton's law of gravitational attraction states that the force exerted by a massive body (the Sun) fixed at the origin $(0,0,0)$ on a particle in space is directed toward the Sun and has magnitude proportional to the inverse square of the distance to the Sun.  Show that this means that the force field is described by the $1$-form $$\frac{kx}{r^3}dx + \frac{ky}{r^3}dy + \frac{kz}{r^3}dz$$ where $k$ is a positive constant and $r=r(x,y,z) = \sqrt{x^2+y^2+z^2}$. Here's what I've done: I know that work (at least for small displacements) in physics is defined as $W = F_xdx + F_ydy + F_zdz$ where the force is $\vec F = (F_x, F_y, F_z)$. I consider the gravity at a point $(x,y,z)$.  Then first the direction toward the Sun is $-x\hat x -y\hat y -z\hat z$.  So the gravity at this point must be proportional to this. Then I think about the magnitude of $\vec F$ and I get $$\| \vec F\| \propto \frac 1{r^2} \implies {F_x}^2 + {F_y}^2 + {F_z}^2 \propto \frac{1}{r^4}$$ This leads me to think that each component $F_i$ is proportional to $\frac 1{r^2}$. So putting that together I get that $\vec F$ should be proportional to $$\left(\frac{-x}{r^2},\frac{-y}{r^2},\frac{-z}{r^2}\right)$$ But this problem states that it should be proportional to $$\left(\frac{x}{r^3},\frac{y}{r^3},\frac{z}{r^3}\right)$$ Where have I gone wrong?",,"['multivariable-calculus', 'physics', 'differential-forms']"
27,Can polar coordinates always be used to calculate a limit in a multivariable function?,Can polar coordinates always be used to calculate a limit in a multivariable function?,,"Are polar coordinates always a viable way to calculate the limit of a multivariable function? In lecture, it appeared as if converting a function into polar coordinates and then checking the limit as r approaches 0 would be a foolproof way to determine a limit. However, after doing some online reading it appears as if it is not a viable method when the function is not ""independently bound of theta"". Could someone please explain this to me? I am having difficulty understanding this concept.","Are polar coordinates always a viable way to calculate the limit of a multivariable function? In lecture, it appeared as if converting a function into polar coordinates and then checking the limit as r approaches 0 would be a foolproof way to determine a limit. However, after doing some online reading it appears as if it is not a viable method when the function is not ""independently bound of theta"". Could someone please explain this to me? I am having difficulty understanding this concept.",,"['calculus', 'multivariable-calculus']"
28,Finding more critical points,Finding more critical points,,"If we have the function $f$ defined: $$f(x,y)=2\sin(x)+2\sin(y)+\sin(x+y)$$ for $-\pi \le x\le \pi$ and $-\pi \le y \le \pi$ Find the critical points and determine the nature of each. I'm a bit stuck on this. I've found:  $\frac {\partial f}{\partial x}=2\cos(x)+\cos(x+y)$ $\frac {\partial f}{\partial y}=2\cos(y)+\cos(x+y)$ $\frac {\partial^2 f}{\partial x^2}=-2\sin(x)-\sin(x+y)$ $\frac {\partial^2 f}{\partial y^2}=-2\sin(y)-\sin(x+y)$ $\frac {\partial^2 f}{\partial x\partial y}=-\sin(x+y)$ Solving $f_x=0$ and $f_y=0$ I get $x=\arccos\left(\frac{-1\pm\sqrt 3}{2}\right)$ which has only one real solution, but looking at the graph there should be more than one critical point in the region. How should I find the others?","If we have the function $f$ defined: $$f(x,y)=2\sin(x)+2\sin(y)+\sin(x+y)$$ for $-\pi \le x\le \pi$ and $-\pi \le y \le \pi$ Find the critical points and determine the nature of each. I'm a bit stuck on this. I've found:  $\frac {\partial f}{\partial x}=2\cos(x)+\cos(x+y)$ $\frac {\partial f}{\partial y}=2\cos(y)+\cos(x+y)$ $\frac {\partial^2 f}{\partial x^2}=-2\sin(x)-\sin(x+y)$ $\frac {\partial^2 f}{\partial y^2}=-2\sin(y)-\sin(x+y)$ $\frac {\partial^2 f}{\partial x\partial y}=-\sin(x+y)$ Solving $f_x=0$ and $f_y=0$ I get $x=\arccos\left(\frac{-1\pm\sqrt 3}{2}\right)$ which has only one real solution, but looking at the graph there should be more than one critical point in the region. How should I find the others?",,['multivariable-calculus']
29,"f differentiable at (0,0) [duplicate]","f differentiable at (0,0) [duplicate]",,"This question already has an answer here : Prove the function is not differentiable at (0,0) (1 answer) Closed 8 years ago . Let $$f(x,y)=\left \{ 	\begin{array}{ll} 		\frac{x^2+y^2}{sin(\sqrt{x^2+y^2})}  & \mbox{if } 0<||(x,y)||< \pi \\ 		0 & \mbox{if } x=(0,0) 	\end{array} \right.$$ Determine if $f$ is differentiable at (0,0) Im trying to use that if the partial derivatives exists and are continuous at (0,0) then f is differentiable at (0,0) but i can't conclude. I don't even know if I'm in the right way","This question already has an answer here : Prove the function is not differentiable at (0,0) (1 answer) Closed 8 years ago . Let $$f(x,y)=\left \{ 	\begin{array}{ll} 		\frac{x^2+y^2}{sin(\sqrt{x^2+y^2})}  & \mbox{if } 0<||(x,y)||< \pi \\ 		0 & \mbox{if } x=(0,0) 	\end{array} \right.$$ Determine if $f$ is differentiable at (0,0) Im trying to use that if the partial derivatives exists and are continuous at (0,0) then f is differentiable at (0,0) but i can't conclude. I don't even know if I'm in the right way",,"['calculus', 'multivariable-calculus']"
30,Use Sylvestor's Criterion to classify the critical point?,Use Sylvestor's Criterion to classify the critical point?,,"I am asked to find and classify the critical point of $g(x,y,z) = xy-x+2z-x^2-y^2-z^2$ I have calculated the derivative and found that the critical point is at $(-\frac23, -\frac13, 1)$ I have found the Hessian matrix =  $$ \left[\matrix{-2 & 1 & 0\\1 &-2 & 0\\0 &0 &-2}\right] $$ But when I calculate the leading principle minors, I get $-2$, $3$ and $-6$. Wolframalpha tells me that this point is a maximum, so I should be getting all negative values to make the matrix negative definite. Where am I going wrong?","I am asked to find and classify the critical point of $g(x,y,z) = xy-x+2z-x^2-y^2-z^2$ I have calculated the derivative and found that the critical point is at $(-\frac23, -\frac13, 1)$ I have found the Hessian matrix =  $$ \left[\matrix{-2 & 1 & 0\\1 &-2 & 0\\0 &0 &-2}\right] $$ But when I calculate the leading principle minors, I get $-2$, $3$ and $-6$. Wolframalpha tells me that this point is a maximum, so I should be getting all negative values to make the matrix negative definite. Where am I going wrong?",,"['calculus', 'matrices', 'multivariable-calculus', 'optimization']"
31,Proving $\min\{x^4+x y+y^2-6 x-5 y\}=-9$,Proving,\min\{x^4+x y+y^2-6 x-5 y\}=-9,"I first found that $$\min\{x^4+x y+y^2-6 x-5 y\} = -9$$ by knowing that if a point is a critical point, then the first partial derivatives with respect to $x$ and $y$ must be zero. Then I found the points that satisfies this constraint, which are just $(1,2)$. Then simply $f(1,2) = -9$. But critical point isn't necessarly minimum or maximum, so I have to prove that: $$f(x,y)\ge f(x_0,y_0) = -9$$ for all $(x,y)$. So basically I'm trying to prove that: $$f(x,y)-f(x_0,y_0)\ge 0$$ We have that: $$f(x,y)-f(x_0,y_0) = x^4+x y+y^2-6 x-5 y+9$$ Since I want to prove that this difference is greater than or equal to $0$ I tried to arrange everything in squares but it didn't end up very well. Any ideas to prove $$f(x,y)-f(x_0,y_0) = x^4+x y+y^2-6 x-5 y+9>0 \text{ ?}$$ ps: just don't throw another method of proving that this is indeed a minimum point, I know other ways, but I wanted to do it this way for knowledge.","I first found that $$\min\{x^4+x y+y^2-6 x-5 y\} = -9$$ by knowing that if a point is a critical point, then the first partial derivatives with respect to $x$ and $y$ must be zero. Then I found the points that satisfies this constraint, which are just $(1,2)$. Then simply $f(1,2) = -9$. But critical point isn't necessarly minimum or maximum, so I have to prove that: $$f(x,y)\ge f(x_0,y_0) = -9$$ for all $(x,y)$. So basically I'm trying to prove that: $$f(x,y)-f(x_0,y_0)\ge 0$$ We have that: $$f(x,y)-f(x_0,y_0) = x^4+x y+y^2-6 x-5 y+9$$ Since I want to prove that this difference is greater than or equal to $0$ I tried to arrange everything in squares but it didn't end up very well. Any ideas to prove $$f(x,y)-f(x_0,y_0) = x^4+x y+y^2-6 x-5 y+9>0 \text{ ?}$$ ps: just don't throw another method of proving that this is indeed a minimum point, I know other ways, but I wanted to do it this way for knowledge.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
32,Switching to polar coordinates in differential equation,Switching to polar coordinates in differential equation,,"Hi guys i need to help with this excercises. :) If    $$\begin{cases} x=r\cos \theta\\ y=r \sin\theta \end{cases}$$   prove that the equation:   $$x^2\frac{\partial^2z}{\partial x^2}+2xy\frac{\partial^2z}{\partial x\partial y}+y^2\frac{\partial^2z}{\partial y^2}=0$$   is equivalent to    $$r^2\frac{\partial^2z}{\partial \theta^2}=0.$$ OK I´VE TRIED this could be helpful ? then, what do i make? I tried of isolate  $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ $$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial r}cos\theta-\frac{\partial z}{\partial \theta}(\frac{1}{r})sin\theta$$and $$\frac{\partial z}{\partial y}=\frac{\partial z}{\partial \theta}(\frac{1}{r})cos\theta+\frac{\partial z}{\partial r}sin\theta$$ mm and we know that $$\frac{\partial z}{\partial r}=\frac{\partial z}{\partial x}cos\theta+\frac{\partial z}{\partial y}sin\theta$$ $$\frac{\partial z}{\partial \theta}=-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta$$ but I do not get anything. How can I do to get the result?? help.","Hi guys i need to help with this excercises. :) If    $$\begin{cases} x=r\cos \theta\\ y=r \sin\theta \end{cases}$$   prove that the equation:   $$x^2\frac{\partial^2z}{\partial x^2}+2xy\frac{\partial^2z}{\partial x\partial y}+y^2\frac{\partial^2z}{\partial y^2}=0$$   is equivalent to    $$r^2\frac{\partial^2z}{\partial \theta^2}=0.$$ OK I´VE TRIED this could be helpful ? then, what do i make? I tried of isolate  $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ $$\frac{\partial z}{\partial x}=\frac{\partial z}{\partial r}cos\theta-\frac{\partial z}{\partial \theta}(\frac{1}{r})sin\theta$$and $$\frac{\partial z}{\partial y}=\frac{\partial z}{\partial \theta}(\frac{1}{r})cos\theta+\frac{\partial z}{\partial r}sin\theta$$ mm and we know that $$\frac{\partial z}{\partial r}=\frac{\partial z}{\partial x}cos\theta+\frac{\partial z}{\partial y}sin\theta$$ $$\frac{\partial z}{\partial \theta}=-\frac{\partial z}{\partial x}rsin\theta+\frac{\partial z}{\partial y}rcos\theta$$ but I do not get anything. How can I do to get the result?? help.",,"['multivariable-calculus', 'partial-derivative']"
33,How to calculate the integral on surface which cannot be expressed in functions easily?,How to calculate the integral on surface which cannot be expressed in functions easily?,,"Surface $S$ is part of $x^2+y^2=1$ between planes $z=0$ and $x+y+z=2$, a vector field $\vec{F}=x\vec{i}+y\vec{j}+z\vec{k}$, what is the value of integration $$ \iint_{S} \vec{F}\cdot\vec{n} dS$$ My solution is: $$ \iint_{S} \vec{F}\cdot\vec{n} dS = \iint_{S} \vec{F}\cdot d\vec{S} = \iint_{S} xdydz + ydzdx + zdxdy$$ then I met a problem that I cannot express the surface $S$. I know it is a part of cylindrical surface between the plane $z=0$ and the intersection line of cylinder and the plane. Could anyone please help me to work it out? Update: Thanks to James S. Cook, I get to know how to transform the surface integral into a double integral. So is the solution below right? If it is wrong, could anyone please point the mistakes out? let $x=\cos{\theta}, y=\sin{\theta}$, so by changing the variable, we have the new integration area $X=\{(z,\theta): 0\leq z \leq 2-\cos{\theta} - \sin{\theta}, 0\leq\theta\leq2\pi\}$, and the integral becomes: $$\iint_{S} xdydz + ydzdx + zdxdy = \iint_{S} \left[\cos{\theta}(-\cos{\theta})+\sin{\theta}(-\sin{\theta})\right]dzd\theta  =\int^{2\pi}_0 d\theta\int^{2-\cos{\theta}-\sin{\theta}}_0dz =4\pi$$","Surface $S$ is part of $x^2+y^2=1$ between planes $z=0$ and $x+y+z=2$, a vector field $\vec{F}=x\vec{i}+y\vec{j}+z\vec{k}$, what is the value of integration $$ \iint_{S} \vec{F}\cdot\vec{n} dS$$ My solution is: $$ \iint_{S} \vec{F}\cdot\vec{n} dS = \iint_{S} \vec{F}\cdot d\vec{S} = \iint_{S} xdydz + ydzdx + zdxdy$$ then I met a problem that I cannot express the surface $S$. I know it is a part of cylindrical surface between the plane $z=0$ and the intersection line of cylinder and the plane. Could anyone please help me to work it out? Update: Thanks to James S. Cook, I get to know how to transform the surface integral into a double integral. So is the solution below right? If it is wrong, could anyone please point the mistakes out? let $x=\cos{\theta}, y=\sin{\theta}$, so by changing the variable, we have the new integration area $X=\{(z,\theta): 0\leq z \leq 2-\cos{\theta} - \sin{\theta}, 0\leq\theta\leq2\pi\}$, and the integral becomes: $$\iint_{S} xdydz + ydzdx + zdxdy = \iint_{S} \left[\cos{\theta}(-\cos{\theta})+\sin{\theta}(-\sin{\theta})\right]dzd\theta  =\int^{2\pi}_0 d\theta\int^{2-\cos{\theta}-\sin{\theta}}_0dz =4\pi$$",,"['integration', 'multivariable-calculus']"
34,Finding the range for an Extreme Value!,Finding the range for an Extreme Value!,,"How does one find the range of $xy+\frac{1}{x}+\frac{1}{y}$ I know the domain is $\mathbf |R^2$ except $(0,0)$.","How does one find the range of $xy+\frac{1}{x}+\frac{1}{y}$ I know the domain is $\mathbf |R^2$ except $(0,0)$.",,"['calculus', 'multivariable-calculus']"
35,"Is f(x,y) differentiable at the origin?","Is f(x,y) differentiable at the origin?",,"My book defines $f(x,y)$ is differentiable at $(a,b)$ when $\lim_{(x,y)\to(a,b)} \frac{R_{1,(a,b)} (x,y)}{\|(x,y)-(a,b)\|}=0$ where $R_{1,(a,b)}(x,y)=f(x,y)-L_{(a,b)} (x,y)$ and $L_{(a,b)}(x,y)=f(a,b)+\frac{∂f}{∂x}(x-a)+\frac{∂f}{∂y}(y-b)$ If $f(x,y)=\frac{x^3 +y^3}{x^2+y^2}$ for $(x,y) \ne (0,0)$ and $f(x,y)=0$ for $(x,y)=(0,0)$, what would $f(x,y)$ be in $R_{1,(0,0)} (x,y)$? Unsure of what to use since it $f(x,y)$ is defined separately. If I assume $f(x,y)$ is 0, I would get $\lim_{(x,y)\to(0,0)} \frac{R_{1,(a,b)} (x,y)}{\|(x,y)-(0,0)\|}=\frac{-x-y}{\sqrt{x^2 +y^2}}$ which equals $\frac{-\sqrt{2}x}{|x|}$ which is not $0$ when you approach with $y=x$ at $(x,y)=(0,0)$.","My book defines $f(x,y)$ is differentiable at $(a,b)$ when $\lim_{(x,y)\to(a,b)} \frac{R_{1,(a,b)} (x,y)}{\|(x,y)-(a,b)\|}=0$ where $R_{1,(a,b)}(x,y)=f(x,y)-L_{(a,b)} (x,y)$ and $L_{(a,b)}(x,y)=f(a,b)+\frac{∂f}{∂x}(x-a)+\frac{∂f}{∂y}(y-b)$ If $f(x,y)=\frac{x^3 +y^3}{x^2+y^2}$ for $(x,y) \ne (0,0)$ and $f(x,y)=0$ for $(x,y)=(0,0)$, what would $f(x,y)$ be in $R_{1,(0,0)} (x,y)$? Unsure of what to use since it $f(x,y)$ is defined separately. If I assume $f(x,y)$ is 0, I would get $\lim_{(x,y)\to(0,0)} \frac{R_{1,(a,b)} (x,y)}{\|(x,y)-(0,0)\|}=\frac{-x-y}{\sqrt{x^2 +y^2}}$ which equals $\frac{-\sqrt{2}x}{|x|}$ which is not $0$ when you approach with $y=x$ at $(x,y)=(0,0)$.",,"['calculus', 'multivariable-calculus']"
36,How to evaluate the line integral?,How to evaluate the line integral?,,"I've solved part (a) and (b), but I'm unsure about part (c). My attempt is: $\int_C f(x,y,z)ds$ = $\int_0^4 ((2t*\frac{4}{3}t^{3/2}*\frac{1}{2}t^2$)(t+2) = $\frac{2}{13}4^{3/2} + \frac{8}{33} 4^{11/2}$ But I'm not sure I'm on the right track at all.","I've solved part (a) and (b), but I'm unsure about part (c). My attempt is: $\int_C f(x,y,z)ds$ = $\int_0^4 ((2t*\frac{4}{3}t^{3/2}*\frac{1}{2}t^2$)(t+2) = $\frac{2}{13}4^{3/2} + \frac{8}{33} 4^{11/2}$ But I'm not sure I'm on the right track at all.",,"['calculus', 'integration', 'multivariable-calculus']"
37,Triple Integral over cylindrical volume,Triple Integral over cylindrical volume,,"I am trying to calculate the triple integral shown below. I've also computed the rectangular limits for x, y and z. I've attempted to compute the integral while staying in rectangular, and it ended up being several pages of wild integrals that just kept getting larger and larger. I tried to set this up to do as a cylinder, but I am getting lost. I feel like this problem should be easier, but I just can't wrap my head around it: Problem statement and my calculation of the limits here. In the above image, I've set up the integral correctly for rectangular (I think?) but actually computing it has proven to be overly labor intensive. Can someone please help me set up this integral in a better way? Thank you in advance! P.s. I apologize for not using MathJax; I wanted to be able to write some nicely formatted math, but the learning curve looks a little high for me right now.","I am trying to calculate the triple integral shown below. I've also computed the rectangular limits for x, y and z. I've attempted to compute the integral while staying in rectangular, and it ended up being several pages of wild integrals that just kept getting larger and larger. I tried to set this up to do as a cylinder, but I am getting lost. I feel like this problem should be easier, but I just can't wrap my head around it: Problem statement and my calculation of the limits here. In the above image, I've set up the integral correctly for rectangular (I think?) but actually computing it has proven to be overly labor intensive. Can someone please help me set up this integral in a better way? Thank you in advance! P.s. I apologize for not using MathJax; I wanted to be able to write some nicely formatted math, but the learning curve looks a little high for me right now.",,['multivariable-calculus']
38,Chain rule for partial derivatives,Chain rule for partial derivatives,,"Minton and Smith, in ""Calculus"" define the chain rule for full derivatives $\frac {dz} {dt}$ as it follows: $$\bbox[5px, border: 1.5px solid olive]{{\color{olive}{\text{THEOREM }1.1}~(\text{Chain Rule})\\\text{If $z=f(x(t),y(t)$) where $x(t)$ and $y(t)$ are differentiable and $f(x,y)$ is a}\\\text{differentiable function of $x$ and $y$, then}\\\dfrac{\mathrm d z}{\mathrm d t}=\dfrac{\mathrm d ~~}{\mathrm d t}\Big[f(x(t),y(t))\Big]=\dfrac{\partial f}{\partial x}(x(t),y(t))\dfrac{\mathrm d x}{\mathrm d t}+\dfrac{\partial f}{\partial y}(x(t),y(t))\frac{\mathrm d y}{\mathrm d t}}}$$ Vretblad, however, in ""Fourier Analysis and its Applications"" , mentions an ""easy exercise in applying the chain rule"" in an expansion of a partial derivative $${{\large1.3\quad\text{The one-dimentional wave equation}}\\\text{We shall attempt to find $\textit{all}$ solutions of class $\mathcal C^2$ of the one-dimentional}\\\text{wave equation}\\\hspace{30ex}c^2 u_{xx}=u_{tt}\\\text{Initially, we consider solutions defined in the open half-plane }t>0.\\~~\text{Introduce the new coordinates ($\xi,\eta$) defined by}\\\hspace{25ex}\xi-x-ct,\quad \eta=x+ct\\\text{It is an easy exercise in applying the $\bbox[2px, yellow]{\text{chain rule}}$ to show that}\\\hspace{15ex}u_{xx}:=\frac{\partial^2 u}{\partial x\,^2}=\frac{\partial^2 u}{\partial\xi\,^2}+2\frac{\partial^2 u}{\partial\xi\,\partial\eta}+\frac{\partial^2 u}{\partial\eta\,^2}\\\hspace{15ex}u_{tt}:=\frac{\partial^2 u}{\partial t\,^2}=c^2\left(\frac{\partial^2 u}{\partial\xi\,^2}+2\frac{\partial^2 u}{\partial\xi\,\partial\eta}+\frac{\partial^2 u}{\partial\eta\,^2}\right)}$$ The question is: can the chain rule, originally defined only on $\frac {dz} {dt}$ , be extended to $\frac {\partial z} {\partial t}$ , or is Vretblad applying the chain rule on a full derivative somehow?","Minton and Smith, in ""Calculus"" define the chain rule for full derivatives as it follows: Vretblad, however, in ""Fourier Analysis and its Applications"" , mentions an ""easy exercise in applying the chain rule"" in an expansion of a partial derivative The question is: can the chain rule, originally defined only on , be extended to , or is Vretblad applying the chain rule on a full derivative somehow?","\frac {dz} {dt} \bbox[5px, border: 1.5px solid olive]{{\color{olive}{\text{THEOREM }1.1}~(\text{Chain Rule})\\\text{If z=f(x(t),y(t)) where x(t) and y(t) are differentiable and f(x,y) is a}\\\text{differentiable function of x and y, then}\\\dfrac{\mathrm d z}{\mathrm d t}=\dfrac{\mathrm d ~~}{\mathrm d t}\Big[f(x(t),y(t))\Big]=\dfrac{\partial f}{\partial x}(x(t),y(t))\dfrac{\mathrm d x}{\mathrm d t}+\dfrac{\partial f}{\partial y}(x(t),y(t))\frac{\mathrm d y}{\mathrm d t}}} {{\large1.3\quad\text{The one-dimentional wave equation}}\\\text{We shall attempt to find \textit{all} solutions of class \mathcal C^2 of the one-dimentional}\\\text{wave equation}\\\hspace{30ex}c^2 u_{xx}=u_{tt}\\\text{Initially, we consider solutions defined in the open half-plane }t>0.\\~~\text{Introduce the new coordinates (\xi,\eta) defined by}\\\hspace{25ex}\xi-x-ct,\quad \eta=x+ct\\\text{It is an easy exercise in applying the \bbox[2px, yellow]{\text{chain rule}} to show that}\\\hspace{15ex}u_{xx}:=\frac{\partial^2 u}{\partial x\,^2}=\frac{\partial^2 u}{\partial\xi\,^2}+2\frac{\partial^2 u}{\partial\xi\,\partial\eta}+\frac{\partial^2 u}{\partial\eta\,^2}\\\hspace{15ex}u_{tt}:=\frac{\partial^2 u}{\partial t\,^2}=c^2\left(\frac{\partial^2 u}{\partial\xi\,^2}+2\frac{\partial^2 u}{\partial\xi\,\partial\eta}+\frac{\partial^2 u}{\partial\eta\,^2}\right)} \frac {dz} {dt} \frac {\partial z} {\partial t}","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
39,Use a Double Integral to find the area between two circles,Use a Double Integral to find the area between two circles,,"I am having a difficult time solving this problem. I have tried this several different ways, and I get a different result, none of which is correct, every time. I've derived an answer geometrically and cannot replicate it with a double integral. Here's the problem: Use a double integral to find the area between two circles $$x^2+y^2=4$$ and $$(x−1)^2+y^2=4.$$ Here is how I have tried to go about this problem: First, I graphed it to get a good idea visually of what I was doing. Here's the graph I scribbled on. The region I'm interested is where these two circles overlap. This region can easily be divided into two separate areas. There are clearly a number of ways to go about solving this...but the one I opted for is to find the shaded region. The bounds for $x$ in this case are between $D$ and $C$. D can be found by setting $C_1=C_2$, and $x$ turns out to be $\frac{1}{2}$. On the right, $x$ is where $C_1(y)=0$, $x=\pm2$, so $x=2$ at point $C$. $y$ is greater than $B_y$ and less than $A_y$, which are also found where $C_1=C_2$, and $y$ turns out to be $\pm\sqrt{\frac{15}{4}}$. So far so good. Now I know my limits of integration. But here's what I don't understand. What am I actually integrating? $x$ has constant bounds, and $y$ does not, and looking at other double integral problems, that would lead me to believe that I should integrate $y$ first as a function of $x$, evaluate it at its bounds, and then integrate $x$ and evaluate it at its bounds giving me half the area I am looking for. However, when I try to do this, I get utter nonsense for an answer, or I get lost trying to set up the problem. I could really use the help, I've spent entirely too much time trying to puzzle through this. Thank you in advance! P.s. I determined the area geometrically using a CAD program to calculate the area, and it should be approximately $8.46$.","I am having a difficult time solving this problem. I have tried this several different ways, and I get a different result, none of which is correct, every time. I've derived an answer geometrically and cannot replicate it with a double integral. Here's the problem: Use a double integral to find the area between two circles $$x^2+y^2=4$$ and $$(x−1)^2+y^2=4.$$ Here is how I have tried to go about this problem: First, I graphed it to get a good idea visually of what I was doing. Here's the graph I scribbled on. The region I'm interested is where these two circles overlap. This region can easily be divided into two separate areas. There are clearly a number of ways to go about solving this...but the one I opted for is to find the shaded region. The bounds for $x$ in this case are between $D$ and $C$. D can be found by setting $C_1=C_2$, and $x$ turns out to be $\frac{1}{2}$. On the right, $x$ is where $C_1(y)=0$, $x=\pm2$, so $x=2$ at point $C$. $y$ is greater than $B_y$ and less than $A_y$, which are also found where $C_1=C_2$, and $y$ turns out to be $\pm\sqrt{\frac{15}{4}}$. So far so good. Now I know my limits of integration. But here's what I don't understand. What am I actually integrating? $x$ has constant bounds, and $y$ does not, and looking at other double integral problems, that would lead me to believe that I should integrate $y$ first as a function of $x$, evaluate it at its bounds, and then integrate $x$ and evaluate it at its bounds giving me half the area I am looking for. However, when I try to do this, I get utter nonsense for an answer, or I get lost trying to set up the problem. I could really use the help, I've spent entirely too much time trying to puzzle through this. Thank you in advance! P.s. I determined the area geometrically using a CAD program to calculate the area, and it should be approximately $8.46$.",,"['integration', 'multivariable-calculus']"
40,Finding a harmonic function given some details,Finding a harmonic function given some details,,"The function $\Phi$ is defined to be a real valued function of two variables. On the lines $y=-x+3$ and $y=-x-3$ it attains the values as shown on the picture I added. I am a currently self-teaching complex analysis, and I have seen some approaches for solving these problems involving mappings, however, this exercise is to be solved not using those techniques as it is posed before that chapter. What is a general way for solving these(if there is one)?","The function $\Phi$ is defined to be a real valued function of two variables. On the lines $y=-x+3$ and $y=-x-3$ it attains the values as shown on the picture I added. I am a currently self-teaching complex analysis, and I have seen some approaches for solving these problems involving mappings, however, this exercise is to be solved not using those techniques as it is posed before that chapter. What is a general way for solving these(if there is one)?",,"['complex-analysis', 'multivariable-calculus']"
41,"Find $\lim\limits_{(x,y) \to(0,0)} \frac{e^{xy} -1}{ x^2 + y^2} $",Find,"\lim\limits_{(x,y) \to(0,0)} \frac{e^{xy} -1}{ x^2 + y^2} ","I need to prove that the limit does not exits Find $\lim\limits_{(x,y) \to(0,0)} \frac{e^{xy}-1}{ x^2 + y^2} $ I tried finding the limit as f(x) approaches from x and y axes but I still get an indeterminate answer.","I need to prove that the limit does not exits Find $\lim\limits_{(x,y) \to(0,0)} \frac{e^{xy}-1}{ x^2 + y^2} $ I tried finding the limit as f(x) approaches from x and y axes but I still get an indeterminate answer.",,"['limits', 'multivariable-calculus']"
42,First-order Taylor expansion for a function of two variables,First-order Taylor expansion for a function of two variables,,"If $u:\mathbb R^2\to \mathbb R$ and $u$ has continuous partial derivatives prove that (a) $u(x+h,y+k)-u(x,y)=\dfrac{\partial u}{\partial x }h +\dfrac{\partial u}{\partial y }k +\epsilon_1$ (b) $v(x+h,y+k)-v(x,y)=\dfrac{\partial v}{\partial x }h +\dfrac{\partial v}{\partial y }k +\epsilon_2$ where $\dfrac{\epsilon_1}{h+ik}\to 0$ and  $\dfrac{\epsilon_2}{h+ik}\to 0$. My try: $u(x+h,y+k)-u(x,y)=\dfrac{u(x+h,y+k)-u(x+h,y)}{k}k+\dfrac{u(x+h,y)-u(x,y)}{h}h=\dfrac{\partial}{\partial y} u(x+h,y)k+\dfrac{\partial}{\partial x} u(x,y)h$ Similarly the other case will follow. What I am having trouble is that how I should incorporate $\epsilon _1$ here and give the requisite conditions that $\dfrac{\epsilon_1}{h+ik}\to 0$.","If $u:\mathbb R^2\to \mathbb R$ and $u$ has continuous partial derivatives prove that (a) $u(x+h,y+k)-u(x,y)=\dfrac{\partial u}{\partial x }h +\dfrac{\partial u}{\partial y }k +\epsilon_1$ (b) $v(x+h,y+k)-v(x,y)=\dfrac{\partial v}{\partial x }h +\dfrac{\partial v}{\partial y }k +\epsilon_2$ where $\dfrac{\epsilon_1}{h+ik}\to 0$ and  $\dfrac{\epsilon_2}{h+ik}\to 0$. My try: $u(x+h,y+k)-u(x,y)=\dfrac{u(x+h,y+k)-u(x+h,y)}{k}k+\dfrac{u(x+h,y)-u(x,y)}{h}h=\dfrac{\partial}{\partial y} u(x+h,y)k+\dfrac{\partial}{\partial x} u(x,y)h$ Similarly the other case will follow. What I am having trouble is that how I should incorporate $\epsilon _1$ here and give the requisite conditions that $\dfrac{\epsilon_1}{h+ik}\to 0$.",,"['real-analysis', 'multivariable-calculus']"
43,"$f(\textbf{x}) = {a\over{|\textbf{x}|^{n-2}}} + b,\text{ }\textbf{x} \neq 0$",,"f(\textbf{x}) = {a\over{|\textbf{x}|^{n-2}}} + b,\text{ }\textbf{x} \neq 0","If $f(\textbf{x}) = g(r)$, $r = |\textbf{x}|$, and $n \ge 3$, show that$$\nabla^2 f = {{\partial^2 f}\over{\partial x_1^2}} + \dots + {{\partial^2f}\over{\partial x_n^2}} = {{n-1}\over{r}}g'(r) + g''(r)$$for $\textbf{x} \neq 0$. Deduce from the previous part that, if $\nabla^2 f = 0$, then$$f(\textbf{x}) = {a\over{|\textbf{x}|^{n-2}}} + b,\text{ }\textbf{x} \neq 0,$$where $a$ and $b$ are constants. I am stuck on this problem. Can anyone give me a hint?","If $f(\textbf{x}) = g(r)$, $r = |\textbf{x}|$, and $n \ge 3$, show that$$\nabla^2 f = {{\partial^2 f}\over{\partial x_1^2}} + \dots + {{\partial^2f}\over{\partial x_n^2}} = {{n-1}\over{r}}g'(r) + g''(r)$$for $\textbf{x} \neq 0$. Deduce from the previous part that, if $\nabla^2 f = 0$, then$$f(\textbf{x}) = {a\over{|\textbf{x}|^{n-2}}} + b,\text{ }\textbf{x} \neq 0,$$where $a$ and $b$ are constants. I am stuck on this problem. Can anyone give me a hint?",,['ordinary-differential-equations']
44,Finding electric flux given volume charge density,Finding electric flux given volume charge density,,"Question: Let $\rho_v = 8z(1 - z)$ C/m$^3$ for $0 < z < 1$ m, $8z(1 + z)$ C/m$^3$ for $-1<z<0$, and $0$ for $|z| > 1$. (a) Find $\vec{D}$ everywhere. (b) Sketch $\vec{D}_z$ vs. $z$, $-2<z<2$. Attempt at Solution: I know that $\vec{D} = \int\limits_{volume}\frac{\rho_vdv}{4\pi{}r^2}\hat{a}_r$. My problem here is setting up the limits of integration for the (what I assume will be a) triple integral. The problem makes no mention of what type of volume has this charge distribution, so it is really confusing me. Is it maybe a plane in the $x-y$ region? Even if so, what would the limits of integration be? My guess for the $0<z<1$ region is $\int_0^1\int_0^y\int_0^x\frac{8z(1-z)}{4\pi{x^2 + y^2 +z^2}}\hat{a}_rdxdydz$, but I'm really not sure how I would find $\hat{a}_r$, and I am not confident in my limits of integration. Once I figure out this region, I think I should be easily able to apply this method to $-1<z<0$, but I'm not too sure about $|z|>1$. For Part (b), how do I find $\vec{D}_z$ once I've found $\vec{D}$?","Question: Let $\rho_v = 8z(1 - z)$ C/m$^3$ for $0 < z < 1$ m, $8z(1 + z)$ C/m$^3$ for $-1<z<0$, and $0$ for $|z| > 1$. (a) Find $\vec{D}$ everywhere. (b) Sketch $\vec{D}_z$ vs. $z$, $-2<z<2$. Attempt at Solution: I know that $\vec{D} = \int\limits_{volume}\frac{\rho_vdv}{4\pi{}r^2}\hat{a}_r$. My problem here is setting up the limits of integration for the (what I assume will be a) triple integral. The problem makes no mention of what type of volume has this charge distribution, so it is really confusing me. Is it maybe a plane in the $x-y$ region? Even if so, what would the limits of integration be? My guess for the $0<z<1$ region is $\int_0^1\int_0^y\int_0^x\frac{8z(1-z)}{4\pi{x^2 + y^2 +z^2}}\hat{a}_rdxdydz$, but I'm really not sure how I would find $\hat{a}_r$, and I am not confident in my limits of integration. Once I figure out this region, I think I should be easily able to apply this method to $-1<z<0$, but I'm not too sure about $|z|>1$. For Part (b), how do I find $\vec{D}_z$ once I've found $\vec{D}$?",,"['multivariable-calculus', 'physics', 'mathematical-physics']"
45,Multivariable limit one-sided path,Multivariable limit one-sided path,,"After looking at this answer Problem with multivariable calculus: $\lim_{(x,y)\to (0,0)} \frac{x^3 + y^3}{x^2 + y}$ I wondered if you have a limit $$\lim_{(x,y)\to(0,0)}f(x,y)$$ And you found paths such that the limit is equal to something $0$ in this case but you take a path $y=g(x),\lim_{x\to0}g(x)=0$ and you have that $$\lim_{(x,y)\to(0^+,0^-)}f(x,g(x))=-\infty,\lim_{(x,y)\to(0^-,0^+)}f(x,g(x))=\infty$$ Does that imply the limit doesn't exist or the path we take must have two-sided limit when approaching $(0,0)$","After looking at this answer Problem with multivariable calculus: $\lim_{(x,y)\to (0,0)} \frac{x^3 + y^3}{x^2 + y}$ I wondered if you have a limit $$\lim_{(x,y)\to(0,0)}f(x,y)$$ And you found paths such that the limit is equal to something $0$ in this case but you take a path $y=g(x),\lim_{x\to0}g(x)=0$ and you have that $$\lim_{(x,y)\to(0^+,0^-)}f(x,g(x))=-\infty,\lim_{(x,y)\to(0^-,0^+)}f(x,g(x))=\infty$$ Does that imply the limit doesn't exist or the path we take must have two-sided limit when approaching $(0,0)$",,['multivariable-calculus']
46,On the nature of saddle points,On the nature of saddle points,,"Given the function: $D(x,y)=f_{xx}f_{yy}-f^2_{xy}$ If $D(a,b)<0$ then this implies $(a,b)$ is a saddle point. There are three possible ways for $D<0$: 1)If $f_{xx}f_{yy}<0$. This case is represented in the following three images: This is a surface: We have $f_{xx}|_{(a,b)}>0$, which means that the orange curve- that results from the intersection of a plane parallel to the $xz$ plane and the surface-has positive convexity, therefore $(a,b)$ is minimum along that orange curve. We have $f_{yy}|_{(a,b)}<0$, which means that the orange curve- that results from the intersection of a plane parallel to the $yz$ plane and the surface-has negative convexity, therefore $(a,b)$ is maximum along that orange curve. Since along one curve $(a,b)$ is minimum and along another it's maximum, therefore it's a saddle point. 2) If either  $f_{xx}|_{(a,b)}$ or $f_{yy}|_{(a,b)}=0$, where $(a,b)$ is a point of inflection, therefore $D=-f^2_{xy}|_{(a,b)}<0$. This case is shown by the following image: The equation of this surface is: $f(x,y)=x^3-y^2$. The black dot is $(a,b)$. $f_{xx}|_{(a,b)}=0$ where $(a,b)$ is an inflection point along the orange curve, and $f_{yy}|_{(a,b)}<0$ which means the blue curve has negative concavity therefore $(a,b)$ is maximum along that blue curve. Therefore $(a,b)$ is a saddle point. 3)If $f_{xx}$ and $f_{yy}$ have the same signs i.e:$f_{xx}f_{yy}>0$, and $f_{xx}f_{yy}<f^2_{xy}$. I illustrate this case with the following three images: Point 3) does not make sense to me at all. If $f_{xx}f_{yy}>0$, then this implies the two orange curves -that result from the intersection between the surface on one hand and xz and yz planes(or planes parallel to them), on the other hand- have the same convexity. So according to this reasoning point 3) should imply that $(a,b)$ is an extremum not a saddle point , shouldn't it?","Given the function: $D(x,y)=f_{xx}f_{yy}-f^2_{xy}$ If $D(a,b)<0$ then this implies $(a,b)$ is a saddle point. There are three possible ways for $D<0$: 1)If $f_{xx}f_{yy}<0$. This case is represented in the following three images: This is a surface: We have $f_{xx}|_{(a,b)}>0$, which means that the orange curve- that results from the intersection of a plane parallel to the $xz$ plane and the surface-has positive convexity, therefore $(a,b)$ is minimum along that orange curve. We have $f_{yy}|_{(a,b)}<0$, which means that the orange curve- that results from the intersection of a plane parallel to the $yz$ plane and the surface-has negative convexity, therefore $(a,b)$ is maximum along that orange curve. Since along one curve $(a,b)$ is minimum and along another it's maximum, therefore it's a saddle point. 2) If either  $f_{xx}|_{(a,b)}$ or $f_{yy}|_{(a,b)}=0$, where $(a,b)$ is a point of inflection, therefore $D=-f^2_{xy}|_{(a,b)}<0$. This case is shown by the following image: The equation of this surface is: $f(x,y)=x^3-y^2$. The black dot is $(a,b)$. $f_{xx}|_{(a,b)}=0$ where $(a,b)$ is an inflection point along the orange curve, and $f_{yy}|_{(a,b)}<0$ which means the blue curve has negative concavity therefore $(a,b)$ is maximum along that blue curve. Therefore $(a,b)$ is a saddle point. 3)If $f_{xx}$ and $f_{yy}$ have the same signs i.e:$f_{xx}f_{yy}>0$, and $f_{xx}f_{yy}<f^2_{xy}$. I illustrate this case with the following three images: Point 3) does not make sense to me at all. If $f_{xx}f_{yy}>0$, then this implies the two orange curves -that result from the intersection between the surface on one hand and xz and yz planes(or planes parallel to them), on the other hand- have the same convexity. So according to this reasoning point 3) should imply that $(a,b)$ is an extremum not a saddle point , shouldn't it?",,['multivariable-calculus']
47,"Problem with limit 2 variables $\lim_{(x,y) \to (0,0)} \frac{x^2 y^2}{x^2 + y^2}$ [duplicate]",Problem with limit 2 variables  [duplicate],"\lim_{(x,y) \to (0,0)} \frac{x^2 y^2}{x^2 + y^2}","This question already has answers here : evaluating the limit $ \lim_{(x,y) \rightarrow (0,0) } \frac{x^2y^2}{x^2 + y^2} $ [duplicate] (5 answers) Closed 8 years ago . I tried to solve the limit: $\lim \frac{x^2 y^2}{x^2 + y^2}$ with $(x,y) \to (0,0)$ I tried it with the paths $x=0, y=0, y=x, y=x^2$ and everything went to $0$. Now I'm suspicious that this limit really goes to $0$, but how I prove it? Thank you and sorry my english.","This question already has answers here : evaluating the limit $ \lim_{(x,y) \rightarrow (0,0) } \frac{x^2y^2}{x^2 + y^2} $ [duplicate] (5 answers) Closed 8 years ago . I tried to solve the limit: $\lim \frac{x^2 y^2}{x^2 + y^2}$ with $(x,y) \to (0,0)$ I tried it with the paths $x=0, y=0, y=x, y=x^2$ and everything went to $0$. Now I'm suspicious that this limit really goes to $0$, but how I prove it? Thank you and sorry my english.",,"['calculus', 'limits', 'multivariable-calculus']"
48,Integral over a torus - change of variables,Integral over a torus - change of variables,,"Heyyyy I would like to understand how to perform the variables change $\phi \rightarrow \psi$ in the following sum $$ \int_{\varphi \in \mathbb{T}^2} h(\alpha(\varphi),\beta(\varphi)) d\varphi $$ where $\alpha(\varphi) = \langle \varphi, \omega \rangle, \beta(\varphi) = \langle \varphi, \omega^{\perp} \rangle$ One can identify $\mathbb{T}^2$ à $S^1 \times S^1$, so $\varphi = (\varphi_1,\varphi_2)$. Then $\psi = (\psi_1,\psi_2) = (\alpha(\varphi),\beta(\varphi))$ It's a diffeo on the torus: $\varphi \rightarrow \psi = (\varphi_1\omega_1 + \varphi_2\omega_2,-\varphi_1\omega_2 + \varphi_2\omega_1)$. $d\psi = (\omega_1 d\varphi_1 + \omega_2 \varphi_2,-\omega_2 d\varphi_1 + \omega_1 \varphi_2) = (\langle d\psi,\omega\rangle, \langle d\psi,\omega^{\perp} \rangle$ ... ? how to write the new integral ? Thanks","Heyyyy I would like to understand how to perform the variables change $\phi \rightarrow \psi$ in the following sum $$ \int_{\varphi \in \mathbb{T}^2} h(\alpha(\varphi),\beta(\varphi)) d\varphi $$ where $\alpha(\varphi) = \langle \varphi, \omega \rangle, \beta(\varphi) = \langle \varphi, \omega^{\perp} \rangle$ One can identify $\mathbb{T}^2$ à $S^1 \times S^1$, so $\varphi = (\varphi_1,\varphi_2)$. Then $\psi = (\psi_1,\psi_2) = (\alpha(\varphi),\beta(\varphi))$ It's a diffeo on the torus: $\varphi \rightarrow \psi = (\varphi_1\omega_1 + \varphi_2\omega_2,-\varphi_1\omega_2 + \varphi_2\omega_1)$. $d\psi = (\omega_1 d\varphi_1 + \omega_2 \varphi_2,-\omega_2 d\varphi_1 + \omega_1 \varphi_2) = (\langle d\psi,\omega\rangle, \langle d\psi,\omega^{\perp} \rangle$ ... ? how to write the new integral ? Thanks",,"['real-analysis', 'integration', 'multivariable-calculus']"
49,"Calculating $\iint_{D} \left(x-y\right)dxdy$ where $D=\left\{0\le x-y\le 1,\:1\le xy\le 2\right\}$",Calculating  where,"\iint_{D} \left(x-y\right)dxdy D=\left\{0\le x-y\le 1,\:1\le xy\le 2\right\}","$$\iint_{D} \left(x-y\right)dxdy$$ where $D=\left\{0\le x-y\le 1,\:1\le xy\le 2\right\}$ So the substitution is pretty obvious, but j is: $J\:=\frac{1}{x+y}$ $$$$ I dont see how I get rid of the denominator and what is the new integral without x,y. Can somebody show me what is the trick here and how Do i get a new integral with the substitution $u=x+y, v=xy$?","$$\iint_{D} \left(x-y\right)dxdy$$ where $D=\left\{0\le x-y\le 1,\:1\le xy\le 2\right\}$ So the substitution is pretty obvious, but j is: $J\:=\frac{1}{x+y}$ $$$$ I dont see how I get rid of the denominator and what is the new integral without x,y. Can somebody show me what is the trick here and how Do i get a new integral with the substitution $u=x+y, v=xy$?",,"['integration', 'multivariable-calculus', 'definite-integrals']"
50,Elementary surface integral computation,Elementary surface integral computation,,"I'm working on studying for the GRE.  I did this problem from Stewart's Calculus, but my answer differs from that in the back of the book. The problem is: Find the area of the part of the sphere $x^{2} + y^{2} + z^{2} = a^{2}$ that lies within the cylinder $x^{2} + y^{2} = ax$ and above the $xy$-plane. I let $\varphi(x,y) = (x,y,\sqrt{a^{2} - x^{2} - y^{2}})$ be the coordinate function describing the upper half of the sphere.  A routine computation gives $$\left \|\frac{\partial \varphi}{\partial x} \times \frac{\partial \varphi}{\partial y} \right \| = \frac{a}{\sqrt{a^{2} - (x^{2} + y^{2})}}$$ We integrate over the circle $x^{2} + y^{2} = ax$, which can be parametrized in polar coordinates as $\frac{-\pi}{2} \leq \theta \leq \frac{\pi}{2}, \, \, 0 \leq r \leq a \cos(\theta)$.  Integration gives  \begin{align*} SA &= \frac{-a}{2} \int_{- \pi/2}^{\pi/2} \int_{0}^{a \cos(\theta)} \frac{-2r}{\sqrt{a^{2} - r^{2}}} \, dr d\theta \\ &= \frac{-a}{2} \int_{-\pi/2}^{\pi/2} \left( 2 \sqrt{a^{2} - a^{2} \cos^{2}(\theta)} - 2a \right) \, d\theta \\ &= a^{2} \int_{-\pi/2}^{\pi/2} \, d\theta - a^{2} \int_{-\pi/2}^{\pi/2} \sin(\theta) \, d\theta \end{align*} which gives me $SA = \pi a^{2}$, but the book says $SA = a^{2}(\pi - 2)$.  I can see how that would have been the right answer if I had gotten a cosine in the last integral instead of a sine, but it's not clear to me where I went wrong.","I'm working on studying for the GRE.  I did this problem from Stewart's Calculus, but my answer differs from that in the back of the book. The problem is: Find the area of the part of the sphere $x^{2} + y^{2} + z^{2} = a^{2}$ that lies within the cylinder $x^{2} + y^{2} = ax$ and above the $xy$-plane. I let $\varphi(x,y) = (x,y,\sqrt{a^{2} - x^{2} - y^{2}})$ be the coordinate function describing the upper half of the sphere.  A routine computation gives $$\left \|\frac{\partial \varphi}{\partial x} \times \frac{\partial \varphi}{\partial y} \right \| = \frac{a}{\sqrt{a^{2} - (x^{2} + y^{2})}}$$ We integrate over the circle $x^{2} + y^{2} = ax$, which can be parametrized in polar coordinates as $\frac{-\pi}{2} \leq \theta \leq \frac{\pi}{2}, \, \, 0 \leq r \leq a \cos(\theta)$.  Integration gives  \begin{align*} SA &= \frac{-a}{2} \int_{- \pi/2}^{\pi/2} \int_{0}^{a \cos(\theta)} \frac{-2r}{\sqrt{a^{2} - r^{2}}} \, dr d\theta \\ &= \frac{-a}{2} \int_{-\pi/2}^{\pi/2} \left( 2 \sqrt{a^{2} - a^{2} \cos^{2}(\theta)} - 2a \right) \, d\theta \\ &= a^{2} \int_{-\pi/2}^{\pi/2} \, d\theta - a^{2} \int_{-\pi/2}^{\pi/2} \sin(\theta) \, d\theta \end{align*} which gives me $SA = \pi a^{2}$, but the book says $SA = a^{2}(\pi - 2)$.  I can see how that would have been the right answer if I had gotten a cosine in the last integral instead of a sine, but it's not clear to me where I went wrong.",,['multivariable-calculus']
51,System with arbitrary function of an unknown,System with arbitrary function of an unknown,,"How can I solve the following system $$ (u_x)^2 - (u_t)^2 = 1 \\  u_{xx} - u_{tt} = f(u) $$ where $f$ is an arbitrary function of $u$, $u$ and $f$ to be determined. I don't know any approach, therefore I am very interested about you ideas and grateful for your help!","How can I solve the following system $$ (u_x)^2 - (u_t)^2 = 1 \\  u_{xx} - u_{tt} = f(u) $$ where $f$ is an arbitrary function of $u$, $u$ and $f$ to be determined. I don't know any approach, therefore I am very interested about you ideas and grateful for your help!",,"['ordinary-differential-equations', 'multivariable-calculus', 'polynomials', 'partial-differential-equations', 'numerical-methods']"
52,Derivative exists by first principles but undefined when using chain rule,Derivative exists by first principles but undefined when using chain rule,,"Consider the function $h$ defined by \begin{align} h(z,y)=(z^3+y^3)^{\frac{1}{3}} \end{align} Then \begin{align*} h_z(0,0)&=\lim_{t\rightarrow 0}\frac{(t^3)^{\frac{1}{3}}}{t}\\ &=1 \end{align*} When differentiating via the chain rule we have \begin{align} h_z(z,y)&=\frac{z^2}{(z^3+y^3)^{\frac{2}{3}}} \end{align} This function is not defined at (0,0), but when calculating from first principles we get a well defined answer. What is going on here?","Consider the function $h$ defined by \begin{align} h(z,y)=(z^3+y^3)^{\frac{1}{3}} \end{align} Then \begin{align*} h_z(0,0)&=\lim_{t\rightarrow 0}\frac{(t^3)^{\frac{1}{3}}}{t}\\ &=1 \end{align*} When differentiating via the chain rule we have \begin{align} h_z(z,y)&=\frac{z^2}{(z^3+y^3)^{\frac{2}{3}}} \end{align} This function is not defined at (0,0), but when calculating from first principles we get a well defined answer. What is going on here?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
53,Finding the maximum on an inside an octahedron,Finding the maximum on an inside an octahedron,,"Let $B$ be the closed domain in $\mathbb{R}^3$ defined by $|x_1|+|x_2|+|x_3|\leq 1$. Find the maximum of $F(x_1,x_2,x_3)=\sum_{i=1}^3x_i^2+\sum_{i=1}^3a_ix_i$ on $B$. Using Lagrange multiplier seems too complicated. We can write $F$ as $r^2-\sum_{i=1}^3a_i^2/4$ where $r$ is the distance from $(x_1,x_2,x_3)$ to $(-a_1/2,-a_2/2,-a_3/2)$. So what to do from here? The maximum lies on the vertex but how do you know which vertex? Thanks","Let $B$ be the closed domain in $\mathbb{R}^3$ defined by $|x_1|+|x_2|+|x_3|\leq 1$. Find the maximum of $F(x_1,x_2,x_3)=\sum_{i=1}^3x_i^2+\sum_{i=1}^3a_ix_i$ on $B$. Using Lagrange multiplier seems too complicated. We can write $F$ as $r^2-\sum_{i=1}^3a_i^2/4$ where $r$ is the distance from $(x_1,x_2,x_3)$ to $(-a_1/2,-a_2/2,-a_3/2)$. So what to do from here? The maximum lies on the vertex but how do you know which vertex? Thanks",,['multivariable-calculus']
54,Hessian Matrix of an Angle in Terms of the Vertices,Hessian Matrix of an Angle in Terms of the Vertices,,"I am attempting to derive the analytical formula for the Hessian matrix of a the second derivatives of the value of an angle in terms of the (9) coordinates of the 3 3D points that define it. While I have derived a formula for this (given below), when I attempt to validate it numerically, I get incorrect answers; e.g., non-symmetric matrices. I suspect that I have misunderstood or misapplied some principle of vector calculus in coming up with these formulae and would appreciate it if anyone can either (a) find the error, (b) provide an alternate formula, or (c) point me toward a derivation elsewhere. Consider the following three points: $a = (x_a, y_a, z_a)\\ b = (x_b, y_b, z_b)\\ c = (x_c, y_c, z_c)$ Let $\theta = \arccos\left( \frac{b - a}{||b - a||} \cdot \frac{c - a}{||c - a||} \right)$ be the angle between vectors $\mathbf{v}_{ab} = b-a$ and $\mathbf{v}_{ac} = c-a$. Additionally, let $\mathbf{u}_{ab} = \frac{b - a}{||b - a||}$ and $\mathbf{u}_{ac} = \frac{c - a}{||c - a||}$. I am performing a minimization for which I need a closed form of both the gradient vector and Hessian matrix of $\theta$ in terms of the coordinates of the points $a$, $b$, and $c$. The details of the problem are not important aside from the fact that estimation is not an option due to the size of the problem. Formally, the formula I need are: $\nabla \theta = \begin{pmatrix}   \partial\theta/\partial x_a\\   \partial\theta/\partial y_a\\   \partial\theta/\partial z_a\\   \vdots\\   \partial\theta/\partial z_c \end{pmatrix}\;$ and $\;\mathbf{H}(\theta) = \begin{pmatrix}    \partial^2 \theta / \partial x_a^2 & \partial^2 \theta / \partial x_a \partial y_a & \cdots & \partial^2 \theta / \partial x_a \partial z_c \\    \partial^2 \theta / \partial y_a \partial x_a & \partial^2 \theta / \partial y_a^2 & \cdots & \partial^2 \theta / \partial y_a \partial z_c \\    \vdots & \vdots & \ddots & \vdots \\    \partial^2 \theta / \partial z_c \partial x_z & \partial^2 \theta / \partial z_c \partial y_a & \cdots & \partial^2 \theta / \partial z_c^2 \end{pmatrix}$ I have already found the gradient analytically and tested it numerically; I define it as follows: $\nabla \theta = \begin{pmatrix} \theta_a \\ \theta_b \\ \theta_c \end{pmatrix}$ where $\theta_a$, $\theta_b$, and $\theta_c$ are column vectors representing $\nabla \theta$ in terms of points $a$, $b$, and $c$: $\theta_a = \begin{pmatrix} \partial \theta / \partial x_a \\ \partial \theta / \partial y_a \\ \partial \theta / \partial z_a \\\end{pmatrix} = -(\theta_b + \theta_c) \\  \theta_b = \begin{pmatrix} \partial \theta / \partial x_b \\ \partial \theta / \partial y_b \\ \partial \theta / \partial z_b \\\end{pmatrix} = \frac{1}{||c-b||}(\mathbf{u}_{ab} \cot\theta - \mathbf{u}_{ac} \csc\theta)\\  \theta_c = \begin{pmatrix} \partial \theta / \partial x_c \\ \partial \theta / \partial y_c \\ \partial \theta / \partial z_c \\\end{pmatrix} = \frac{1}{||c-b||}(\mathbf{u}_{ac} \cot\theta - \mathbf{u}_{ab} \csc\theta) $ I would like to derive the Hessian using this gradient as a starting place; i.e., if I could find $\mathbf{J}(\theta_b)$ and $\mathbf{J}(\theta_c)$ (where $\mathbf{J}(f)$ is the Jacobian matrix of $f$) in terms of the coordinates $x_a, y_a ... z_c$, expressing the Hessian would be a trivial matter of packing the Jacobians into a larger matrix: $\mathbf{H}(\theta) = \begin{pmatrix}  \mathbf{J}(\theta_a) & \mathbf{J}(\theta_b) & \mathbf{J}(\theta_c)  \end{pmatrix}$ To further simplify, I can split the Jacobians further into the following $3\times 3$ sub-matrices and express the Hessian using them: $\mathbf{H}(\theta) = \begin{pmatrix}  \theta_{aa} & \theta_{ba} & \theta_{ca} \\  \theta_{ab} & \theta_{bb} & \theta_{cb} \\  \theta_{ac} & \theta_{bc} & \theta_{cc}  \end{pmatrix}$ where $\theta_{ij} = \begin{pmatrix}  \partial \theta_i / \partial x_j   &     \partial \theta_i / \partial y_j &     \partial \theta_i / \partial z_j  \end{pmatrix} = \begin{pmatrix}  \partial^2 \theta / \partial x_i \partial x_j   &     \partial^2 \theta / \partial x_i \partial y_j &     \partial^2 \theta / \partial x_i \partial z_j \\  \partial^2 \theta / \partial y_i \partial x_j   &     \partial^2 \theta / \partial y_i \partial y_j &     \partial^2 \theta / \partial y_i \partial z_j \\  \partial^2 \theta / \partial z_i \partial x_j   &     \partial^2 \theta / \partial z_i \partial y_j &     \partial^2 \theta / \partial z_i \partial z_j \\  \end{pmatrix}$. Note that $\theta_{ij} = \theta_{ji}^\intercal$. I now go about deriving the functions $\theta_{ij}$, starting with $\theta_{ba}$, $\theta_{bb}$, $\theta_{bc}$, $\theta_{ca}$, and $\theta_{cc}$. Using these, the remainder will be trivial to derive. I formulate these matrices by considering the dot product of a column vector, $\theta_i$ with $\nabla_j^\intercal = \begin{pmatrix} \partial/\partial x_j & \partial/\partial y_j & \partial/\partial z_j \end{pmatrix}$: $\theta_{ij} = \theta_i \cdot \nabla_j^\intercal$. To simplify the derivation slightly, note that the following can be easily defined: $\mathbf{J}_i(\mathbf{u}_{ij}) = \begin{pmatrix}   \partial x_{\mathbf{u}_{ij}} / \partial x_i &      \partial x_{\mathbf{u}_{ij}} / \partial y_i &      \partial x_{\mathbf{u}_{ij}} / \partial z_i \\   \partial y_{\mathbf{u}_{ij}} / \partial x_i &      \partial y_{\mathbf{u}_{ij}} / \partial y_i &      \partial y_{\mathbf{u}_{ij}} / \partial z_i \\   \partial z_{\mathbf{u}_{ij}} / \partial x_i &      \partial z_{\mathbf{u}_{ij}} / \partial y_i &      \partial z_{\mathbf{u}_{ij}} / \partial z_i \\  \end{pmatrix} = \mathbf{v}_{ij} \cdot \mathbf{v}_{ij}^\intercal - ||\mathbf{v}_{ij}||^2 \mathbf{I}$. Additionally, if we let $\gamma(i,j) = 1/||\mathbf{v}_{ij}||$ (this cleans up the notation slightly), then we have: $\nabla_i \gamma(i,j) = \begin{pmatrix}    \partial \gamma(i,j) / \partial x_i \\    \partial \gamma(i,j) / \partial y_i \\    \partial \gamma(i,j) / \partial z_i  \end{pmatrix} = \mathbf{v}_{ij} / ||\mathbf{v}_{ij}||^3$. We can now begin the derivation; using the notation defined above, we can start with $\theta_{ba}$: $\begin{align} \theta_{ba} &= \theta_b \cdot \nabla_a^\intercal \\   &= (\gamma(b,c) \mathbf{u}_{ab}\cot\theta - \gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_a^\intercal \\   &= (\gamma(b,c) \mathbf{u}_{ab}\cot\theta)\cdot \nabla_a^\intercal - (\gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_a^\intercal \\   &=  \gamma(b,c)(       (\cot\theta\, \mathbf{J}_a(\mathbf{u}_{ab})         - \csc^2\theta\, (\mathbf{u}_{ab} \cdot \theta_a^\intercal))     - (\csc\theta\, \mathbf{J}_a(\mathbf{u}_{ac})         - \csc\theta \cot\theta\, (\mathbf{u}_{ac} \cdot \theta_a^\intercal)) \end{align}$ The remainder are given below, but note that even looking just at $\theta_{ba}$, the formula given here does not produce answers that match numerical derivations of the Hessian. $\begin{align} \theta_{bb} &= \theta_b \cdot \nabla_b^\intercal    = (\gamma(b,c) \mathbf{u}_{ab}\cot\theta - \gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_b^\intercal \\   &= \cot\theta\,(\mathbf{u}_{ab} \cdot \nabla_b\gamma(b,c)^\intercal) + \gamma(b,c)\cot\theta\,\mathbf{J}_b(\mathbf{u}_{ab}) - \gamma(b,c)\csc^2\theta\,(\mathbf{u}_{ab} \cdot \theta_b^\intercal) - \csc\theta\,(\mathbf{u}_{ac} \cdot \nabla_b\gamma(b,c)^\intercal) + \gamma(b,c)\csc\theta\cot\theta\,(\mathbf{u}_{ac}\cdot\theta_b^\intercal) \end{align}$ $\begin{align} \theta_{bc} &= \theta_b \cdot \nabla_c^\intercal    = (\gamma(b,c) \mathbf{u}_{ab}\cot\theta - \gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_c^\intercal \\   &= \cot\theta\,(\mathbf{u}_{ab} \cdot \nabla_c\gamma(b,c)^\intercal) - \gamma(b,c)\csc^2\theta\,(\mathbf{u}_{ab} \cdot \theta_c^\intercal) - \csc\theta\,(\mathbf{u}_{ac} \cdot \nabla_\gamma(b,c)^\intercal) - \gamma(b,c)\csc\theta\,\mathbf{J}_c(\mathbf{u}_{ac}) + \gamma(b,c)\csc\theta\cot\theta\,(\mathbf{u}_{ac}\cdot\theta_c^\intercal) \end{align}$ $\begin{align} \theta_{ca} &= \theta_c \cdot \nabla_a^\intercal   = (\gamma(b,c)\cot\theta\,\mathbf{u}_{ac}         + \gamma(b,c)\csc\theta\,\mathbf{u}_{ab})         \cdot \nabla_a^\intercal \\   &= \gamma(b,c)(          \cot\theta\,\mathbf{J}_a(\mathbf{u}_{ac})        - \csc^2\theta\,(\mathbf{u}_{ac} \cdot \theta_a^\intercal)        - \csc\theta\,\mathbf{J}_a(\mathbf{u}_{ab})        + \csc\theta\cot\theta\,(\mathbf{u}_{ab} \cdot \theta_a^\intercal) \end{align}$ $\begin{align} \theta_{cc} &= \theta_c \cdot \nabla_c^\intercal    = (\gamma(b,c)\cot\theta\,\mathbf{u}_{ac}         + \gamma(b,c)\csc\theta\,\mathbf{u}_{ab})         \cdot \nabla_c^\intercal \\   &= \cot\theta\,(\mathbf{u}_{ac} \cdot \nabla_c\gamma(b,c)^\intercal)        + \gamma(b,c)\cot\theta\,\mathbf{J}_c(\mathbf{u}_{ac})        - \gamma(b,c)\csc^2\theta\,          (\mathbf{u}_{ac} \cdot \theta_c^\intercal)        - \csc\theta\,(\mathbf{u}_{ab} \cdot \nabla_c\gamma(b,c)^\intercal)        + \gamma(b,c)\cot\theta\csc\theta\,(\mathbf{u}_{ab}          \cdot \theta_c^\intercal) \end{align}$","I am attempting to derive the analytical formula for the Hessian matrix of a the second derivatives of the value of an angle in terms of the (9) coordinates of the 3 3D points that define it. While I have derived a formula for this (given below), when I attempt to validate it numerically, I get incorrect answers; e.g., non-symmetric matrices. I suspect that I have misunderstood or misapplied some principle of vector calculus in coming up with these formulae and would appreciate it if anyone can either (a) find the error, (b) provide an alternate formula, or (c) point me toward a derivation elsewhere. Consider the following three points: $a = (x_a, y_a, z_a)\\ b = (x_b, y_b, z_b)\\ c = (x_c, y_c, z_c)$ Let $\theta = \arccos\left( \frac{b - a}{||b - a||} \cdot \frac{c - a}{||c - a||} \right)$ be the angle between vectors $\mathbf{v}_{ab} = b-a$ and $\mathbf{v}_{ac} = c-a$. Additionally, let $\mathbf{u}_{ab} = \frac{b - a}{||b - a||}$ and $\mathbf{u}_{ac} = \frac{c - a}{||c - a||}$. I am performing a minimization for which I need a closed form of both the gradient vector and Hessian matrix of $\theta$ in terms of the coordinates of the points $a$, $b$, and $c$. The details of the problem are not important aside from the fact that estimation is not an option due to the size of the problem. Formally, the formula I need are: $\nabla \theta = \begin{pmatrix}   \partial\theta/\partial x_a\\   \partial\theta/\partial y_a\\   \partial\theta/\partial z_a\\   \vdots\\   \partial\theta/\partial z_c \end{pmatrix}\;$ and $\;\mathbf{H}(\theta) = \begin{pmatrix}    \partial^2 \theta / \partial x_a^2 & \partial^2 \theta / \partial x_a \partial y_a & \cdots & \partial^2 \theta / \partial x_a \partial z_c \\    \partial^2 \theta / \partial y_a \partial x_a & \partial^2 \theta / \partial y_a^2 & \cdots & \partial^2 \theta / \partial y_a \partial z_c \\    \vdots & \vdots & \ddots & \vdots \\    \partial^2 \theta / \partial z_c \partial x_z & \partial^2 \theta / \partial z_c \partial y_a & \cdots & \partial^2 \theta / \partial z_c^2 \end{pmatrix}$ I have already found the gradient analytically and tested it numerically; I define it as follows: $\nabla \theta = \begin{pmatrix} \theta_a \\ \theta_b \\ \theta_c \end{pmatrix}$ where $\theta_a$, $\theta_b$, and $\theta_c$ are column vectors representing $\nabla \theta$ in terms of points $a$, $b$, and $c$: $\theta_a = \begin{pmatrix} \partial \theta / \partial x_a \\ \partial \theta / \partial y_a \\ \partial \theta / \partial z_a \\\end{pmatrix} = -(\theta_b + \theta_c) \\  \theta_b = \begin{pmatrix} \partial \theta / \partial x_b \\ \partial \theta / \partial y_b \\ \partial \theta / \partial z_b \\\end{pmatrix} = \frac{1}{||c-b||}(\mathbf{u}_{ab} \cot\theta - \mathbf{u}_{ac} \csc\theta)\\  \theta_c = \begin{pmatrix} \partial \theta / \partial x_c \\ \partial \theta / \partial y_c \\ \partial \theta / \partial z_c \\\end{pmatrix} = \frac{1}{||c-b||}(\mathbf{u}_{ac} \cot\theta - \mathbf{u}_{ab} \csc\theta) $ I would like to derive the Hessian using this gradient as a starting place; i.e., if I could find $\mathbf{J}(\theta_b)$ and $\mathbf{J}(\theta_c)$ (where $\mathbf{J}(f)$ is the Jacobian matrix of $f$) in terms of the coordinates $x_a, y_a ... z_c$, expressing the Hessian would be a trivial matter of packing the Jacobians into a larger matrix: $\mathbf{H}(\theta) = \begin{pmatrix}  \mathbf{J}(\theta_a) & \mathbf{J}(\theta_b) & \mathbf{J}(\theta_c)  \end{pmatrix}$ To further simplify, I can split the Jacobians further into the following $3\times 3$ sub-matrices and express the Hessian using them: $\mathbf{H}(\theta) = \begin{pmatrix}  \theta_{aa} & \theta_{ba} & \theta_{ca} \\  \theta_{ab} & \theta_{bb} & \theta_{cb} \\  \theta_{ac} & \theta_{bc} & \theta_{cc}  \end{pmatrix}$ where $\theta_{ij} = \begin{pmatrix}  \partial \theta_i / \partial x_j   &     \partial \theta_i / \partial y_j &     \partial \theta_i / \partial z_j  \end{pmatrix} = \begin{pmatrix}  \partial^2 \theta / \partial x_i \partial x_j   &     \partial^2 \theta / \partial x_i \partial y_j &     \partial^2 \theta / \partial x_i \partial z_j \\  \partial^2 \theta / \partial y_i \partial x_j   &     \partial^2 \theta / \partial y_i \partial y_j &     \partial^2 \theta / \partial y_i \partial z_j \\  \partial^2 \theta / \partial z_i \partial x_j   &     \partial^2 \theta / \partial z_i \partial y_j &     \partial^2 \theta / \partial z_i \partial z_j \\  \end{pmatrix}$. Note that $\theta_{ij} = \theta_{ji}^\intercal$. I now go about deriving the functions $\theta_{ij}$, starting with $\theta_{ba}$, $\theta_{bb}$, $\theta_{bc}$, $\theta_{ca}$, and $\theta_{cc}$. Using these, the remainder will be trivial to derive. I formulate these matrices by considering the dot product of a column vector, $\theta_i$ with $\nabla_j^\intercal = \begin{pmatrix} \partial/\partial x_j & \partial/\partial y_j & \partial/\partial z_j \end{pmatrix}$: $\theta_{ij} = \theta_i \cdot \nabla_j^\intercal$. To simplify the derivation slightly, note that the following can be easily defined: $\mathbf{J}_i(\mathbf{u}_{ij}) = \begin{pmatrix}   \partial x_{\mathbf{u}_{ij}} / \partial x_i &      \partial x_{\mathbf{u}_{ij}} / \partial y_i &      \partial x_{\mathbf{u}_{ij}} / \partial z_i \\   \partial y_{\mathbf{u}_{ij}} / \partial x_i &      \partial y_{\mathbf{u}_{ij}} / \partial y_i &      \partial y_{\mathbf{u}_{ij}} / \partial z_i \\   \partial z_{\mathbf{u}_{ij}} / \partial x_i &      \partial z_{\mathbf{u}_{ij}} / \partial y_i &      \partial z_{\mathbf{u}_{ij}} / \partial z_i \\  \end{pmatrix} = \mathbf{v}_{ij} \cdot \mathbf{v}_{ij}^\intercal - ||\mathbf{v}_{ij}||^2 \mathbf{I}$. Additionally, if we let $\gamma(i,j) = 1/||\mathbf{v}_{ij}||$ (this cleans up the notation slightly), then we have: $\nabla_i \gamma(i,j) = \begin{pmatrix}    \partial \gamma(i,j) / \partial x_i \\    \partial \gamma(i,j) / \partial y_i \\    \partial \gamma(i,j) / \partial z_i  \end{pmatrix} = \mathbf{v}_{ij} / ||\mathbf{v}_{ij}||^3$. We can now begin the derivation; using the notation defined above, we can start with $\theta_{ba}$: $\begin{align} \theta_{ba} &= \theta_b \cdot \nabla_a^\intercal \\   &= (\gamma(b,c) \mathbf{u}_{ab}\cot\theta - \gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_a^\intercal \\   &= (\gamma(b,c) \mathbf{u}_{ab}\cot\theta)\cdot \nabla_a^\intercal - (\gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_a^\intercal \\   &=  \gamma(b,c)(       (\cot\theta\, \mathbf{J}_a(\mathbf{u}_{ab})         - \csc^2\theta\, (\mathbf{u}_{ab} \cdot \theta_a^\intercal))     - (\csc\theta\, \mathbf{J}_a(\mathbf{u}_{ac})         - \csc\theta \cot\theta\, (\mathbf{u}_{ac} \cdot \theta_a^\intercal)) \end{align}$ The remainder are given below, but note that even looking just at $\theta_{ba}$, the formula given here does not produce answers that match numerical derivations of the Hessian. $\begin{align} \theta_{bb} &= \theta_b \cdot \nabla_b^\intercal    = (\gamma(b,c) \mathbf{u}_{ab}\cot\theta - \gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_b^\intercal \\   &= \cot\theta\,(\mathbf{u}_{ab} \cdot \nabla_b\gamma(b,c)^\intercal) + \gamma(b,c)\cot\theta\,\mathbf{J}_b(\mathbf{u}_{ab}) - \gamma(b,c)\csc^2\theta\,(\mathbf{u}_{ab} \cdot \theta_b^\intercal) - \csc\theta\,(\mathbf{u}_{ac} \cdot \nabla_b\gamma(b,c)^\intercal) + \gamma(b,c)\csc\theta\cot\theta\,(\mathbf{u}_{ac}\cdot\theta_b^\intercal) \end{align}$ $\begin{align} \theta_{bc} &= \theta_b \cdot \nabla_c^\intercal    = (\gamma(b,c) \mathbf{u}_{ab}\cot\theta - \gamma(b,c) \mathbf{u}_{ac}\csc\theta) \cdot \nabla_c^\intercal \\   &= \cot\theta\,(\mathbf{u}_{ab} \cdot \nabla_c\gamma(b,c)^\intercal) - \gamma(b,c)\csc^2\theta\,(\mathbf{u}_{ab} \cdot \theta_c^\intercal) - \csc\theta\,(\mathbf{u}_{ac} \cdot \nabla_\gamma(b,c)^\intercal) - \gamma(b,c)\csc\theta\,\mathbf{J}_c(\mathbf{u}_{ac}) + \gamma(b,c)\csc\theta\cot\theta\,(\mathbf{u}_{ac}\cdot\theta_c^\intercal) \end{align}$ $\begin{align} \theta_{ca} &= \theta_c \cdot \nabla_a^\intercal   = (\gamma(b,c)\cot\theta\,\mathbf{u}_{ac}         + \gamma(b,c)\csc\theta\,\mathbf{u}_{ab})         \cdot \nabla_a^\intercal \\   &= \gamma(b,c)(          \cot\theta\,\mathbf{J}_a(\mathbf{u}_{ac})        - \csc^2\theta\,(\mathbf{u}_{ac} \cdot \theta_a^\intercal)        - \csc\theta\,\mathbf{J}_a(\mathbf{u}_{ab})        + \csc\theta\cot\theta\,(\mathbf{u}_{ab} \cdot \theta_a^\intercal) \end{align}$ $\begin{align} \theta_{cc} &= \theta_c \cdot \nabla_c^\intercal    = (\gamma(b,c)\cot\theta\,\mathbf{u}_{ac}         + \gamma(b,c)\csc\theta\,\mathbf{u}_{ab})         \cdot \nabla_c^\intercal \\   &= \cot\theta\,(\mathbf{u}_{ac} \cdot \nabla_c\gamma(b,c)^\intercal)        + \gamma(b,c)\cot\theta\,\mathbf{J}_c(\mathbf{u}_{ac})        - \gamma(b,c)\csc^2\theta\,          (\mathbf{u}_{ac} \cdot \theta_c^\intercal)        - \csc\theta\,(\mathbf{u}_{ab} \cdot \nabla_c\gamma(b,c)^\intercal)        + \gamma(b,c)\cot\theta\csc\theta\,(\mathbf{u}_{ab}          \cdot \theta_c^\intercal) \end{align}$",,"['multivariable-calculus', 'partial-derivative', 'angle']"
55,Tangent vectors and parametric curves,Tangent vectors and parametric curves,,"Consider the curve $C$ defined by $(x,y,z) = \bar{r}(t)$, where $$\bar{r}(t)=\langle t\sin t, t\cos t, t^2 \rangle~~; t \in \mathbb{R}^3$$ Show that $C$ lies on the paraboloid $z= x^2 + y^2$ Find a vector tangent to $C$ at the point $\bar{r}(1)$. Find the parametric equation for the line tangent to the curve $\bar{r}$ at the point $(0,-\pi, \pi^2)$. My attempt: Consider the paraboloid $z = x^2 + y^2$. Let \begin{align}x=t\sin t \\ y = t\cos t\end{align} Then notice that \begin{align}z &= t^2\sin^2 t + t^2\cos^2t \\ &=t^2(\sin^2t + \cos^2t) \\ &= t^2\end{align} Thus $C$ lies on the paraboloid $z = x^2 + y^2$. $\bar{r}'(t) = \langle\sin t + t\cos t, \cos t - t\sin t, 2t \rangle$ Thus, when $t=1$, the tangent vector must be $$\bar{r}'(1) = \langle\sin (1) + \cos(1), \cos(1) - \sin (1), 2 \rangle$$ Notice that at the point $(0,-\pi, \pi^2)$ we have that $t = \pi$. Thus the parametric equation of the line tangent to $\bar{r}$ a the point $(0,-\pi, \pi^2)$ is given by \begin{align}\bar{l}(t) &= r(\pi) + (t - \pi)\bar{r}'(\pi) \\ &= \langle -\pi t + \pi^2, -t, 2\pi t - \pi^2 \rangle\end{align} Are these correct? I am trying to work through some old papers in order to prepare for a test.","Consider the curve $C$ defined by $(x,y,z) = \bar{r}(t)$, where $$\bar{r}(t)=\langle t\sin t, t\cos t, t^2 \rangle~~; t \in \mathbb{R}^3$$ Show that $C$ lies on the paraboloid $z= x^2 + y^2$ Find a vector tangent to $C$ at the point $\bar{r}(1)$. Find the parametric equation for the line tangent to the curve $\bar{r}$ at the point $(0,-\pi, \pi^2)$. My attempt: Consider the paraboloid $z = x^2 + y^2$. Let \begin{align}x=t\sin t \\ y = t\cos t\end{align} Then notice that \begin{align}z &= t^2\sin^2 t + t^2\cos^2t \\ &=t^2(\sin^2t + \cos^2t) \\ &= t^2\end{align} Thus $C$ lies on the paraboloid $z = x^2 + y^2$. $\bar{r}'(t) = \langle\sin t + t\cos t, \cos t - t\sin t, 2t \rangle$ Thus, when $t=1$, the tangent vector must be $$\bar{r}'(1) = \langle\sin (1) + \cos(1), \cos(1) - \sin (1), 2 \rangle$$ Notice that at the point $(0,-\pi, \pi^2)$ we have that $t = \pi$. Thus the parametric equation of the line tangent to $\bar{r}$ a the point $(0,-\pi, \pi^2)$ is given by \begin{align}\bar{l}(t) &= r(\pi) + (t - \pi)\bar{r}'(\pi) \\ &= \langle -\pi t + \pi^2, -t, 2\pi t - \pi^2 \rangle\end{align} Are these correct? I am trying to work through some old papers in order to prepare for a test.",,['multivariable-calculus']
56,"$u=xf(xy)$, show that $xu_{xx}-yu_{xy} = 0$",", show that",u=xf(xy) xu_{xx}-yu_{xy} = 0,"I need to show that: $$xu_{xx}-yu_{xy} = 0$$ when $$u=xf(xy)$$ So, I did: $$u_x = xyf_x(xy)+f(xy) \implies $$ $$u_{xx} = xy^2f_{xx}(xy)+2yf_x(xy)$$ and $$u_{xy} = xf_x(xy)+x^2yf_{xy}(xy)+xf_y(xy)$$ so: $$xu_{xx} = x^2y^2f_{xx}(xy)+2xyf_x(xy)$$ $$yu_{xy} = x^2y^2f_{xy}(xy)+2xyf_y(xy)$$ but when I take one from another, I don't get $0$. This makes me think that there is a relation that says $$f_{xx}(xy) = f_{xy}(xy)$$ so they both cancel, but I couldn't find it. What am I doing wrong?","I need to show that: $$xu_{xx}-yu_{xy} = 0$$ when $$u=xf(xy)$$ So, I did: $$u_x = xyf_x(xy)+f(xy) \implies $$ $$u_{xx} = xy^2f_{xx}(xy)+2yf_x(xy)$$ and $$u_{xy} = xf_x(xy)+x^2yf_{xy}(xy)+xf_y(xy)$$ so: $$xu_{xx} = x^2y^2f_{xx}(xy)+2xyf_x(xy)$$ $$yu_{xy} = x^2y^2f_{xy}(xy)+2xyf_y(xy)$$ but when I take one from another, I don't get $0$. This makes me think that there is a relation that says $$f_{xx}(xy) = f_{xy}(xy)$$ so they both cancel, but I couldn't find it. What am I doing wrong?",,"['calculus', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
57,"Find the equation $ax + by + cz = d$ of the plane which has equal distance to the points $A(1, 2, 3)$ and $B(4, 5, 6)$",Find the equation  of the plane which has equal distance to the points  and,"ax + by + cz = d A(1, 2, 3) B(4, 5, 6)","I was just wondering if anyone has any suggestions as to how to compute this equation? Find the equation $ax + by  + cz = d$ of the plane for which every point has equal   distance to the points $A(1, 2, 3)$ and $B(4, 5, 6)$","I was just wondering if anyone has any suggestions as to how to compute this equation? Find the equation $ax + by  + cz = d$ of the plane for which every point has equal   distance to the points $A(1, 2, 3)$ and $B(4, 5, 6)$",,"['geometry', 'multivariable-calculus', 'plane-curves', 'plane-geometry']"
58,Applying the Implicit Function Theorem (constructing a function),Applying the Implicit Function Theorem (constructing a function),,"Let $\mathcal C$ be the locus of all points $(x,y,z)$ in $\mathbb R^3$ that satisfy both $$x^3(y^3+z^3)=0\text{ and } (x-y)^3=z^2+7.$$ Prove that near the point $p:=(1,-1,1)$, we can solve for $y$ and $z$ as smooth functions of $x$. That is,  show that there exists an $r>0$ and $\mathcal C^1$ functions $f,g:(1-r,1+r) \rightarrow \mathbb R$ such that $p=(1,f(1),g(1))$ and for all $x \in (1-r,1+r)$, $(x, f(x),g(x))$ lies on $\mathcal C$. I know that I will need to employ the Implicit Function Theorem, but I'm not sure how to construct a function given the two equations. Also will I need to apply it separately to get $f$ and $g$ or can I get a function $h: \mathbb R^2 \rightarrow \mathbb R$?","Let $\mathcal C$ be the locus of all points $(x,y,z)$ in $\mathbb R^3$ that satisfy both $$x^3(y^3+z^3)=0\text{ and } (x-y)^3=z^2+7.$$ Prove that near the point $p:=(1,-1,1)$, we can solve for $y$ and $z$ as smooth functions of $x$. That is,  show that there exists an $r>0$ and $\mathcal C^1$ functions $f,g:(1-r,1+r) \rightarrow \mathbb R$ such that $p=(1,f(1),g(1))$ and for all $x \in (1-r,1+r)$, $(x, f(x),g(x))$ lies on $\mathcal C$. I know that I will need to employ the Implicit Function Theorem, but I'm not sure how to construct a function given the two equations. Also will I need to apply it separately to get $f$ and $g$ or can I get a function $h: \mathbb R^2 \rightarrow \mathbb R$?",,"['multivariable-calculus', 'implicit-function-theorem']"
59,"Finding a Surface Integral for the Vector Field: $F=\langle xz,x^2+y^2,y \rangle$",Finding a Surface Integral for the Vector Field:,"F=\langle xz,x^2+y^2,y \rangle","I need help calculating $$\iint_S F\cdot ds$$ where $$F=\langle xz,x^2+y^2,y \rangle$$ and $$S=\left\{(x,y,z)\mid x^2+y^2+z^2=25 ,y\ge0\right\}$$ oriented in the positive $y$ direction. My Thoughts: Maybe we can do the following: $$\iint_S F\cdot ds=\pm \iint_D F(r(u,v))\cdot [r_u(u,v)\times r_v(u,v)]\: dA(u,v)$$ where the sign is chose so that $\pm r_u\times r_v$ points in the direction we want. Is this what we need to do here, or is there a much easier way. The easier approach may be to use the divergence theorem, but I need help in doing so.","I need help calculating $$\iint_S F\cdot ds$$ where $$F=\langle xz,x^2+y^2,y \rangle$$ and $$S=\left\{(x,y,z)\mid x^2+y^2+z^2=25 ,y\ge0\right\}$$ oriented in the positive $y$ direction. My Thoughts: Maybe we can do the following: $$\iint_S F\cdot ds=\pm \iint_D F(r(u,v))\cdot [r_u(u,v)\times r_v(u,v)]\: dA(u,v)$$ where the sign is chose so that $\pm r_u\times r_v$ points in the direction we want. Is this what we need to do here, or is there a much easier way. The easier approach may be to use the divergence theorem, but I need help in doing so.",,['multivariable-calculus']
60,Calculating the Flux of $F$ over $S$,Calculating the Flux of  over,F S,"I need help calculating $$\iint_S F\cdot ds$$ where $F=\langle z,y,x \rangle$ and $$S=\left\{(x,y,z)\mid \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1\right\}$$ and is oriented outwards. Would the divergence theorem be used here, I'm not sure how to solve this.","I need help calculating $$\iint_S F\cdot ds$$ where $F=\langle z,y,x \rangle$ and $$S=\left\{(x,y,z)\mid \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1\right\}$$ and is oriented outwards. Would the divergence theorem be used here, I'm not sure how to solve this.",,['multivariable-calculus']
61,"calculate $ \lim_{(x,y)\to (0,0)} \frac{e^{\frac{-1}{x^2+y^2}}}{x^4+y^4} $",calculate," \lim_{(x,y)\to (0,0)} \frac{e^{\frac{-1}{x^2+y^2}}}{x^4+y^4} ","calculate the limit: $$\lim_{(x,y)\to (0,0)} \frac{e^{\frac{-1}{x^2+y^2}}}{x^4+y^4} $$ I have tried to use polar coordinates, I also tried to show there is no limit, but I'm pretty sure now the limit is $0$, but I can't prove it.","calculate the limit: $$\lim_{(x,y)\to (0,0)} \frac{e^{\frac{-1}{x^2+y^2}}}{x^4+y^4} $$ I have tried to use polar coordinates, I also tried to show there is no limit, but I'm pretty sure now the limit is $0$, but I can't prove it.",,"['limits', 'multivariable-calculus']"
62,"Double Integral in Polar-Coordinates, Finding the constraint on r","Double Integral in Polar-Coordinates, Finding the constraint on r",,"Sketch the region of integration and then compute  the following double integral:   $$\int_0^1 \int_y^{\sqrt{2-y^2}}(x+y)dxdy$$ https://www.desmos.com/calculator/k5uz2mr2ml Then I characterized the region as: $$R=\{(r,\theta) \,\,\, | \,\,\, 0 \leq r \leq ?? \,\,\,\, \& \frac{\pi}{4} \leq \theta \frac{\pi}{2} \}$$ I thought that the upper bound for r should be $sec(\theta)$ because if $y=x$, we get $$x^2+y^2=2 \implies 2x^2=2 \implies r^2cos^2\theta = 1 \implies r=sec(\theta)$$ But if that is the case, the following integral diverges as $sec(\frac{\pi}{2} )$is not defined. $$\int_{\pi/4}^{\pi/2} \int_0^{sec(\theta)} r^2(cos(\theta)+sin(\theta)) drd\theta$$ $$=\frac{1}{3}\int_{\pi/4}^{\pi/2} (sec^2(\theta) + \frac{sin(\theta)}{cos^2(\theta)} ) d\theta \implies \frac{1}{3} \left ( tan(\theta) + \frac{1}{2cos^2(\theta)} \right )_{\frac{\pi}{4}}^{\frac{\pi}{2}}= ???? $$ Where did I mess up? How would you approach this problem? Thank you in advance for your time.","Sketch the region of integration and then compute  the following double integral:   $$\int_0^1 \int_y^{\sqrt{2-y^2}}(x+y)dxdy$$ https://www.desmos.com/calculator/k5uz2mr2ml Then I characterized the region as: $$R=\{(r,\theta) \,\,\, | \,\,\, 0 \leq r \leq ?? \,\,\,\, \& \frac{\pi}{4} \leq \theta \frac{\pi}{2} \}$$ I thought that the upper bound for r should be $sec(\theta)$ because if $y=x$, we get $$x^2+y^2=2 \implies 2x^2=2 \implies r^2cos^2\theta = 1 \implies r=sec(\theta)$$ But if that is the case, the following integral diverges as $sec(\frac{\pi}{2} )$is not defined. $$\int_{\pi/4}^{\pi/2} \int_0^{sec(\theta)} r^2(cos(\theta)+sin(\theta)) drd\theta$$ $$=\frac{1}{3}\int_{\pi/4}^{\pi/2} (sec^2(\theta) + \frac{sin(\theta)}{cos^2(\theta)} ) d\theta \implies \frac{1}{3} \left ( tan(\theta) + \frac{1}{2cos^2(\theta)} \right )_{\frac{\pi}{4}}^{\frac{\pi}{2}}= ???? $$ Where did I mess up? How would you approach this problem? Thank you in advance for your time.",,"['integration', 'multivariable-calculus']"
63,Definition of integration in *Calculus on Manifolds*,Definition of integration in *Calculus on Manifolds*,,"On page 65 of Calculus on Manifolds , right after defining a partition of unity $\Phi$, it states: If $\Phi$ is subordinate to $\mathcal{O}$, $f:A \to \mathbb{R}$ is   bounded in an open set around each point of $A$, and $\{x: f \text{ is  discontinuous at }x \}$ has measure $0$, then each $\int_A \varphi  \cdot |f|$ exists. I don't see why this would be true unless it is assumed that the $\varphi$ have compact support, which I don't believe it is. Can someone help me parse this? Thanks","On page 65 of Calculus on Manifolds , right after defining a partition of unity $\Phi$, it states: If $\Phi$ is subordinate to $\mathcal{O}$, $f:A \to \mathbb{R}$ is   bounded in an open set around each point of $A$, and $\{x: f \text{ is  discontinuous at }x \}$ has measure $0$, then each $\int_A \varphi  \cdot |f|$ exists. I don't see why this would be true unless it is assumed that the $\varphi$ have compact support, which I don't believe it is. Can someone help me parse this? Thanks",,"['integration', 'multivariable-calculus']"
64,does simply connectedness require connectedness?,does simply connectedness require connectedness?,,"My question consists of two parts. $1)$ suppose domain $D=\{(x,y)\in\mathbb R^2~|~xy>0\}$ is given. Now that is first quadrant and third quadrant with exclusion of $x$ and $y$ axis. We can easily see that $D$ is not connected, since there is a discontinuity at origin. But every closed curve we can construct in domain contains interior of it (or formally, they can shrunk to a point). So do we call it simply connected, or do we also need connectedness to say $D$ is simply connected ? $2)$ Now for second part, let origin also included in domain so that $D$ is connected. Let us construct closed curve which goes through origin. Now this curve also satisfies assumption given above. But it is not simple closed curve. Does this affects simply connectedness? Do we need simple closed curves for simply connectedness?","My question consists of two parts. $1)$ suppose domain $D=\{(x,y)\in\mathbb R^2~|~xy>0\}$ is given. Now that is first quadrant and third quadrant with exclusion of $x$ and $y$ axis. We can easily see that $D$ is not connected, since there is a discontinuity at origin. But every closed curve we can construct in domain contains interior of it (or formally, they can shrunk to a point). So do we call it simply connected, or do we also need connectedness to say $D$ is simply connected ? $2)$ Now for second part, let origin also included in domain so that $D$ is connected. Let us construct closed curve which goes through origin. Now this curve also satisfies assumption given above. But it is not simple closed curve. Does this affects simply connectedness? Do we need simple closed curves for simply connectedness?",,"['general-topology', 'multivariable-calculus']"
65,"Absolute Min and Max of $f(x, y)=x^2+4y^2-2x^2y+4$ Using Partial Derivatives",Absolute Min and Max of  Using Partial Derivatives,"f(x, y)=x^2+4y^2-2x^2y+4","Consider this problem: Find the absolute minimum and absolute maximum of $f(x, y)=x^2+4y^2-2x^2y+4$ on the rectangle given by $-1\leq x\leq1$ and $-1\leq y\leq1$ I solved this problem using partial derivatives: I got $f_x=2x-4xy=0$ And $f_y=8y-2x^2=0$ Solving simultaneously gives the following critical points: $(0,0), (0,\frac{1}{2}), (\sqrt2,\frac{1}{2}) $ And with the restriction: $-1\leq x\leq1$ and $-1\leq y\leq1$ The critical points becomes: $(0,0), (0,\frac{1}{2}) $ $f(0,0)=4 $ and $f(0,\frac{1}{2}) =5$ Now the quagmire is how to use partial derivatives (probably of second order) to determine which is the absolute minimum and maximum. Plotting a graph of $z=x^2+4y^2-2x^2y+4$ suggests that: $(0,0,4) $ is the absolute minimum and $(0,\frac{1}{2},5) $ the maximum But I need to be sure I'm on the right path. I also need to know how to use partial derivatives to differentiate a minimum critical point from a maximum critical point.","Consider this problem: Find the absolute minimum and absolute maximum of $f(x, y)=x^2+4y^2-2x^2y+4$ on the rectangle given by $-1\leq x\leq1$ and $-1\leq y\leq1$ I solved this problem using partial derivatives: I got $f_x=2x-4xy=0$ And $f_y=8y-2x^2=0$ Solving simultaneously gives the following critical points: $(0,0), (0,\frac{1}{2}), (\sqrt2,\frac{1}{2}) $ And with the restriction: $-1\leq x\leq1$ and $-1\leq y\leq1$ The critical points becomes: $(0,0), (0,\frac{1}{2}) $ $f(0,0)=4 $ and $f(0,\frac{1}{2}) =5$ Now the quagmire is how to use partial derivatives (probably of second order) to determine which is the absolute minimum and maximum. Plotting a graph of $z=x^2+4y^2-2x^2y+4$ suggests that: $(0,0,4) $ is the absolute minimum and $(0,\frac{1}{2},5) $ the maximum But I need to be sure I'm on the right path. I also need to know how to use partial derivatives to differentiate a minimum critical point from a maximum critical point.",,"['multivariable-calculus', 'partial-derivative']"
66,Does given point satisfy FONC?,Does given point satisfy FONC?,,"minimize $4x_1^2+2x_2^2-4x_1x_2-8x_2$ subject to $x_1+x_2\leq 4$ Does the point $(2,2)$ satisfy the FONC for a local minimizer? The gradient of the objective function is $\nabla f = \begin{bmatrix} 8x_1^2-4x_2-4x_2 \\ 4x_2^2-4x_1-8\end{bmatrix}$ = $\begin{bmatrix} 0\\0 \end{bmatrix} $ Filling in the point should give $0$ but it doesn't so the answer is no. Is this correct or do I have to use Lagrange?","minimize $4x_1^2+2x_2^2-4x_1x_2-8x_2$ subject to $x_1+x_2\leq 4$ Does the point $(2,2)$ satisfy the FONC for a local minimizer? The gradient of the objective function is $\nabla f = \begin{bmatrix} 8x_1^2-4x_2-4x_2 \\ 4x_2^2-4x_1-8\end{bmatrix}$ = $\begin{bmatrix} 0\\0 \end{bmatrix} $ Filling in the point should give $0$ but it doesn't so the answer is no. Is this correct or do I have to use Lagrange?",,"['multivariable-calculus', 'optimization']"
67,"Total derivative of $f(A,B)$ , where $f:M(n,\mathbb{R}) \times M(n,\mathbb{R}) \to M(n,\mathbb{R})$","Total derivative of  , where","f(A,B) f:M(n,\mathbb{R}) \times M(n,\mathbb{R}) \to M(n,\mathbb{R})","Find the Total derivative of i)$f(A,B)=A+B$ , ii)$g(A,B)=AB$ iii)$h(A,B)=A^2$ where $f,g:M(n,\mathbb{R}) \times M(n,\mathbb{R}) \to M(n,\mathbb{R})$ and $h:M(n,\mathbb{R}) \to M(n,\mathbb{R})$ Let $A=[a_{ij}]$ , $B=[b_{ij}]$, where $ 1 \le i \le n, 1\le j \le n$ If I regard the $f$ to be a function in the $2n^2$ variables, then I believe $$Df= \begin{bmatrix}  1 & 0 & 0 & \cdots &0 &1 & \cdots &0\\ \vdots & \vdots & & \vdots &\vdots & \vdots & & \vdots \\ 0 & 0 & 0 & \cdots &1 &0 & \cdots &1\\ \end{bmatrix}$$ where $Df$ is a $n^2 \times 2n^2$ matrix, the second $1$ appears in the $n+1$th place and so on. Similarly $$DG= \begin{bmatrix}  B^t & 0_{n\times n} & 0_{n \times n} & \cdots &0_{n \times n} &a_{11}I_{n\times n} & \cdots &a_{1n}I_{n\times n}\\ 0_{n \times n} & B^{t}_{n\times n} & 0_{n \times n} & \cdots &0_{n \times n} &a_{21}I_{n\times n} & \cdots &a_{2n}I_{n\times n}\\ \vdots & \vdots & & \vdots &\vdots & \vdots & & \vdots \\  0_{n\times n}& 0_{n\times n} & 0_{n \times n} & \cdots &B^{t}_{n \times n} &a_{n1}I_{n\times n} & \cdots &a_{nn}I_{n\times n}\\ \end{bmatrix}$$ where $Dg$ is a $n^2 \times 2n^2$ matrix,  and $$DH=\begin{bmatrix} A^t+a_{11}I_{n \times n} &{ a_{12}}I_{n\times n} & {a_{13}}I_{n \times n} & \cdots &{a_{1n}}I_{n \times n}\\ a_{21}I_{n \times n} &A^t+{ a_{22}}I_{n\times n} & {a_{23}}I_{n \times n} & \cdots &{a_{2n}}I_{n \times n} \\ \vdots & \vdots & \vdots &\cdots &\vdots\\ a_{n1}I_{n \times n} &{ a_{n2}}I_{n\times n} & {a_{n3}}I_{n \times n} & \cdots &A^t+{a_{nn}}I_{n \times n}\\ \end{bmatrix}$$. I think these are the derivative matrices. Thanks for the help!!","Find the Total derivative of i)$f(A,B)=A+B$ , ii)$g(A,B)=AB$ iii)$h(A,B)=A^2$ where $f,g:M(n,\mathbb{R}) \times M(n,\mathbb{R}) \to M(n,\mathbb{R})$ and $h:M(n,\mathbb{R}) \to M(n,\mathbb{R})$ Let $A=[a_{ij}]$ , $B=[b_{ij}]$, where $ 1 \le i \le n, 1\le j \le n$ If I regard the $f$ to be a function in the $2n^2$ variables, then I believe $$Df= \begin{bmatrix}  1 & 0 & 0 & \cdots &0 &1 & \cdots &0\\ \vdots & \vdots & & \vdots &\vdots & \vdots & & \vdots \\ 0 & 0 & 0 & \cdots &1 &0 & \cdots &1\\ \end{bmatrix}$$ where $Df$ is a $n^2 \times 2n^2$ matrix, the second $1$ appears in the $n+1$th place and so on. Similarly $$DG= \begin{bmatrix}  B^t & 0_{n\times n} & 0_{n \times n} & \cdots &0_{n \times n} &a_{11}I_{n\times n} & \cdots &a_{1n}I_{n\times n}\\ 0_{n \times n} & B^{t}_{n\times n} & 0_{n \times n} & \cdots &0_{n \times n} &a_{21}I_{n\times n} & \cdots &a_{2n}I_{n\times n}\\ \vdots & \vdots & & \vdots &\vdots & \vdots & & \vdots \\  0_{n\times n}& 0_{n\times n} & 0_{n \times n} & \cdots &B^{t}_{n \times n} &a_{n1}I_{n\times n} & \cdots &a_{nn}I_{n\times n}\\ \end{bmatrix}$$ where $Dg$ is a $n^2 \times 2n^2$ matrix,  and $$DH=\begin{bmatrix} A^t+a_{11}I_{n \times n} &{ a_{12}}I_{n\times n} & {a_{13}}I_{n \times n} & \cdots &{a_{1n}}I_{n \times n}\\ a_{21}I_{n \times n} &A^t+{ a_{22}}I_{n\times n} & {a_{23}}I_{n \times n} & \cdots &{a_{2n}}I_{n \times n} \\ \vdots & \vdots & \vdots &\cdots &\vdots\\ a_{n1}I_{n \times n} &{ a_{n2}}I_{n\times n} & {a_{n3}}I_{n \times n} & \cdots &A^t+{a_{nn}}I_{n \times n}\\ \end{bmatrix}$$. I think these are the derivative matrices. Thanks for the help!!",,['multivariable-calculus']
68,Trig substitution for integral of $z/(x^2+z^2)$?,Trig substitution for integral of ?,z/(x^2+z^2),"So I have an integral $\int_1^4\int_y^4\int_0^z\frac{z}{x^2+z^2}\,dx\,dz\,dy$ but I can't figure out what trig substitution to use on the first step. When I try $z=\cos$ and $x=\sin$, I end up with $\int\cos$ but the book comes up with $z\cdot\frac{1}{z}\cdot\arctan \frac{x}{z}$ so I know I screwed up. Can someone show me how the book took this step? Thanks!","So I have an integral $\int_1^4\int_y^4\int_0^z\frac{z}{x^2+z^2}\,dx\,dz\,dy$ but I can't figure out what trig substitution to use on the first step. When I try $z=\cos$ and $x=\sin$, I end up with $\int\cos$ but the book comes up with $z\cdot\frac{1}{z}\cdot\arctan \frac{x}{z}$ so I know I screwed up. Can someone show me how the book took this step? Thanks!",,['multivariable-calculus']
69,Equation perpendicular to 2 non-parallel planes,Equation perpendicular to 2 non-parallel planes,,"Good day sirs! Can you help me with this questions? Find the general equation of the plane: (1) Through $(3,0,-1)$ and perpendicular to each of the planes $x-2y+z=0$ and $x+2y-3z-4=0$ (2) Perpendicular to the plane $x+2y-3z-7=0$ and containing the points $(2, 1, 4)$ and $(1,1,0)$ I tried to answer this, yet I believe there is no such plane perpendicular to two non-parallel planes. (Am I wrong? :) ) My professor tried to explain how to get the answer to my classmates and they all seem to agree and ""enlightened"", but I cannot picture out in my mind how is it possible to have a plane with such conditions, so I'm still skeptic about it. *Can you include graphs also? :)","Good day sirs! Can you help me with this questions? Find the general equation of the plane: (1) Through $(3,0,-1)$ and perpendicular to each of the planes $x-2y+z=0$ and $x+2y-3z-4=0$ (2) Perpendicular to the plane $x+2y-3z-7=0$ and containing the points $(2, 1, 4)$ and $(1,1,0)$ I tried to answer this, yet I believe there is no such plane perpendicular to two non-parallel planes. (Am I wrong? :) ) My professor tried to explain how to get the answer to my classmates and they all seem to agree and ""enlightened"", but I cannot picture out in my mind how is it possible to have a plane with such conditions, so I'm still skeptic about it. *Can you include graphs also? :)",,"['multivariable-calculus', 'vectors', 'plane-curves', 'cross-product']"
70,"Computing $\max_{1/2 \leq x \leq 2} ( \min_{1/3 \leq y \leq 1} f(x,y) )$ where $f(x,y) = x(y \log y - y) - y \log x$.",Computing  where .,"\max_{1/2 \leq x \leq 2} ( \min_{1/3 \leq y \leq 1} f(x,y) ) f(x,y) = x(y \log y - y) - y \log x","Let $f(x,y)=x(y\ln y-y)-y\ln x.$ Find $\max_{1/2\le x\le 2}(\min_{1/3\le y\le1}f(x,y))$. This problem is quite easy and it is from Spivak ; it is the part $c)$ of the general exercise 2-41 page 43 Calculus on manifolds ; here it is: Let $f:\mathbb{R}\times \mathbb{R}\to \mathbb{R}$ be twice continuously differentiable. For each $x\in \mathbb{R}$ define $g_x(y)=f(x,y)$. Suppose that for each $x$ there is a unique $y$ with $g'_x(y)=0$; let $c(x)$ be this $y$. $a)$: If $D_{2,2}f(x,y)\ne0$ for all $(x,y)$ show that $c$ is differentiable and $c'(x)=-\frac{D_{2,1}f(x,c(x))}{D_{2,2}f(x,c(x))}$ $b)$: Show that if $c'(x)=0$, then for some $y$ we have $D_{2,1}f(x,y)=0$, $D_2f(x,y)=0$. I cannot visualize how part c) relates to the previous ones. Can you give me a hint?","Let $f(x,y)=x(y\ln y-y)-y\ln x.$ Find $\max_{1/2\le x\le 2}(\min_{1/3\le y\le1}f(x,y))$. This problem is quite easy and it is from Spivak ; it is the part $c)$ of the general exercise 2-41 page 43 Calculus on manifolds ; here it is: Let $f:\mathbb{R}\times \mathbb{R}\to \mathbb{R}$ be twice continuously differentiable. For each $x\in \mathbb{R}$ define $g_x(y)=f(x,y)$. Suppose that for each $x$ there is a unique $y$ with $g'_x(y)=0$; let $c(x)$ be this $y$. $a)$: If $D_{2,2}f(x,y)\ne0$ for all $(x,y)$ show that $c$ is differentiable and $c'(x)=-\frac{D_{2,1}f(x,c(x))}{D_{2,2}f(x,c(x))}$ $b)$: Show that if $c'(x)=0$, then for some $y$ we have $D_{2,1}f(x,y)=0$, $D_2f(x,y)=0$. I cannot visualize how part c) relates to the previous ones. Can you give me a hint?",,['multivariable-calculus']
71,Bound on function increment,Bound on function increment,,"consider the function $$ f(x,y) = \dfrac{x^{1/3}}{(x+ay^3)^{1/3}} $$ where $a>0$ is a constant and $x,y\geq 0$. It is easy to see that, outside of the origin, $0\leq f\leq 1$. $f$ itself is not continuous (consider the restriction on the $x,y$ axis and then consider limit towards the origin), but $xyf(x,y)$ should be and should also be differentiable, but with a differential that is not bounded on $[0,+\infty)\times [0,+\infty)$. Now, I have the following expression $$ \Delta = x_1y_1f(x_1,y_1) - x_2y_2f(x_2,y_2). $$ My question is: can we get a bound on $\Delta$ of the form $$ \Delta\leq A|x_1-x_2|+B|y_1-y_2|\quad \mbox{or}\quad \Delta\leq C|x_1-x_2||y_1-y_2| $$ with $A,B,C$ constants? If not (as I think), can we get at least a bound of the form $$ \Delta\leq A|x_1-x_2|^\alpha+B|y_1-y_2|^\beta\quad \mbox{or}\quad \Delta\leq C|x_1-x_2|^\alpha|y_1-y_2|^\beta $$ or a combination of those, with $\alpha\leq 4/3$? Note:, if $\beta>0$, then B must be as small as possible. Edit: It would be enough to get a bound on $\Delta\cdot(y_1-y_2)$.","consider the function $$ f(x,y) = \dfrac{x^{1/3}}{(x+ay^3)^{1/3}} $$ where $a>0$ is a constant and $x,y\geq 0$. It is easy to see that, outside of the origin, $0\leq f\leq 1$. $f$ itself is not continuous (consider the restriction on the $x,y$ axis and then consider limit towards the origin), but $xyf(x,y)$ should be and should also be differentiable, but with a differential that is not bounded on $[0,+\infty)\times [0,+\infty)$. Now, I have the following expression $$ \Delta = x_1y_1f(x_1,y_1) - x_2y_2f(x_2,y_2). $$ My question is: can we get a bound on $\Delta$ of the form $$ \Delta\leq A|x_1-x_2|+B|y_1-y_2|\quad \mbox{or}\quad \Delta\leq C|x_1-x_2||y_1-y_2| $$ with $A,B,C$ constants? If not (as I think), can we get at least a bound of the form $$ \Delta\leq A|x_1-x_2|^\alpha+B|y_1-y_2|^\beta\quad \mbox{or}\quad \Delta\leq C|x_1-x_2|^\alpha|y_1-y_2|^\beta $$ or a combination of those, with $\alpha\leq 4/3$? Note:, if $\beta>0$, then B must be as small as possible. Edit: It would be enough to get a bound on $\Delta\cdot(y_1-y_2)$.",,"['real-analysis', 'multivariable-calculus', 'inequality', 'lipschitz-functions']"
72,Line integrals - Surface area,Line integrals - Surface area,,"Here is my task: Calculate surface area of $2(x^{2}+y^{2})^{2}=xy$ between surface $x^{2}+y^{2}=z$ and $z=0$. Here is my attempt to solve this problem. Firstly, I transformed line $2(x^{2}+y^{2})^{2}=xy$ to polar form, putting $x=\rho\cos \phi$ and $y=\rho\sin \phi$. I got $\rho=\frac{1}{2}\sqrt{\sin 2\phi}$. It looks like this: http://s13.postimg.org/wvwox80s7/math.png To use formula surface_area=$\int z(x,y)ds$ (I don't know how to write line integral symbol in latex so I used symbol for ""ordinary"" integral instead), we must find first $ds$, which by definitions equals $$ds=\sqrt{(\frac{\mathrm{d} x}{\mathrm{d} \phi})^{2}+(\frac{\mathrm{d} y}{\mathrm{d} \phi})^{2}}d\phi$$ We have: $x=\rho\cos \phi=\frac{1}{2}\sqrt{\sin 2\phi}\cos\phi$, $y=\rho\sin \phi=\frac{1}{2}\sqrt{\sin 2\phi}\sin\phi$. After differentiating $x$ and $y$ with respect to $\phi$ and putting it in expression for $ds$, we get (if I didn't make mistake somewhere in calculations) $ds=\frac{1}{2\sqrt{\sin 2\phi}}d\phi$. Now we can put everything in our expression for surface area: $\int z(x,y)ds$=$4\int_{0}^{\pi/4}\left [ (\frac{1}{2}\sqrt{\sin 2\phi}\cos\phi)^{2} + (\frac{1}{2}\sqrt{\sin 2\phi}\sin\phi)^{2} \right ]\frac{1}{2\sqrt{\sin 2\phi}}d\phi=4\int_{0}^{\pi/4}\frac{1}{4}\sin2\phi(\cos^{2}\phi+\sin^{2}\phi)\frac{1}{2\sqrt{\sin 2\phi}}d\phi=\frac{1}{2}\int_{0}^{\pi/4}\frac{\sin{2\phi}}{\sqrt{\sin{2\phi}}}d\phi$ I have no idea how to solve this integral. Any suggestion?","Here is my task: Calculate surface area of $2(x^{2}+y^{2})^{2}=xy$ between surface $x^{2}+y^{2}=z$ and $z=0$. Here is my attempt to solve this problem. Firstly, I transformed line $2(x^{2}+y^{2})^{2}=xy$ to polar form, putting $x=\rho\cos \phi$ and $y=\rho\sin \phi$. I got $\rho=\frac{1}{2}\sqrt{\sin 2\phi}$. It looks like this: http://s13.postimg.org/wvwox80s7/math.png To use formula surface_area=$\int z(x,y)ds$ (I don't know how to write line integral symbol in latex so I used symbol for ""ordinary"" integral instead), we must find first $ds$, which by definitions equals $$ds=\sqrt{(\frac{\mathrm{d} x}{\mathrm{d} \phi})^{2}+(\frac{\mathrm{d} y}{\mathrm{d} \phi})^{2}}d\phi$$ We have: $x=\rho\cos \phi=\frac{1}{2}\sqrt{\sin 2\phi}\cos\phi$, $y=\rho\sin \phi=\frac{1}{2}\sqrt{\sin 2\phi}\sin\phi$. After differentiating $x$ and $y$ with respect to $\phi$ and putting it in expression for $ds$, we get (if I didn't make mistake somewhere in calculations) $ds=\frac{1}{2\sqrt{\sin 2\phi}}d\phi$. Now we can put everything in our expression for surface area: $\int z(x,y)ds$=$4\int_{0}^{\pi/4}\left [ (\frac{1}{2}\sqrt{\sin 2\phi}\cos\phi)^{2} + (\frac{1}{2}\sqrt{\sin 2\phi}\sin\phi)^{2} \right ]\frac{1}{2\sqrt{\sin 2\phi}}d\phi=4\int_{0}^{\pi/4}\frac{1}{4}\sin2\phi(\cos^{2}\phi+\sin^{2}\phi)\frac{1}{2\sqrt{\sin 2\phi}}d\phi=\frac{1}{2}\int_{0}^{\pi/4}\frac{\sin{2\phi}}{\sqrt{\sin{2\phi}}}d\phi$ I have no idea how to solve this integral. Any suggestion?",,"['integration', 'multivariable-calculus', 'surfaces', 'area']"
73,How to find $\frac{\partial^2u}{\partial x \partial y}$ given $u=\sin(x\sin^{-1}y)$?,How to find  given ?,\frac{\partial^2u}{\partial x \partial y} u=\sin(x\sin^{-1}y),How to find $$\frac{\partial^2u}{\partial x \partial y}$$ given $$u=\sin(x\sin^{-1}(y))$$? I have calculated $$\frac{\partial u}{\partial x }=\sin^{-1}(y)\cdot \cos(x\sin^{-1}(y))$$ but get stuck on applying the product rule on the next derivative namely finding the partial of $\cos(x\sin^{-1}(y))$ w.r.t $y$.,How to find $$\frac{\partial^2u}{\partial x \partial y}$$ given $$u=\sin(x\sin^{-1}(y))$$? I have calculated $$\frac{\partial u}{\partial x }=\sin^{-1}(y)\cdot \cos(x\sin^{-1}(y))$$ but get stuck on applying the product rule on the next derivative namely finding the partial of $\cos(x\sin^{-1}(y))$ w.r.t $y$.,,['multivariable-calculus']
74,Need help with Double Definite Integration,Need help with Double Definite Integration,,"I need help in solving this double integral: $$\int\limits^2_{-2}\;\;\int\limits^\sqrt{4-x^2}_{-\sqrt{4-x^2}}{(x^2+y^2)^{5/2}}dy\,dx$$ Maybe introducing polar coordinates might help?","I need help in solving this double integral: $$\int\limits^2_{-2}\;\;\int\limits^\sqrt{4-x^2}_{-\sqrt{4-x^2}}{(x^2+y^2)^{5/2}}dy\,dx$$ Maybe introducing polar coordinates might help?",,"['integration', 'multivariable-calculus']"
75,Iterated Integral and Sign Change in Answer,Iterated Integral and Sign Change in Answer,,"Given the iterated integral $\int_0^1\int_x^{2-x}(x^2-y) \, dy \, dx$, the value for the type I integral is, \begin{align*} & \int_0^1\int_x^{2-x}(x^2-y)\,dy\,dx \\ = {} & \int_0^1 x^2y\Big|_x^{2-x} \, dx - \int_0^1 \left.\frac{y^2}{2}\right|_x^{2-x} \, dx \\ = {} & \int_0^1 x^2(2-2x) \, dx - \int_0^1(2-2x) \, dx \\ = {} & \int_0^1 2x^2 \, dx - \int_0^1 2x^3 \, dx - \int_0^1 2 \, dx + \int_0^1 2x \, dx \\ = {} & 2 \left.\frac{1}{3}x^3\right|_0^1 - \left.2 \frac{x^4}{4}\right|_0^1 - 2x\Big|_0^1 + 2\frac{1}{2} x^2\Big|_0^1 \\ = {} & \frac{2}{3} - \frac{1}{2} -2 + 1 \\ = {} & \frac{2}{3} - \frac{3}{2} = -\frac{5}{6} \end{align*} We then calculate the type II integral, thus, $$ \int_0^1\int_y^{2-y}(x^2-y) \, dx \, dy = \frac{5}{6} $$ The two integrals differ by the presence of a negative sign. Does this mean that these integrals are not iterated? EDIT: It would seem that the issue has to deal with the order of the integration limits. Why are the limits reversed when you take the iterated integral with respect to $dx \, dy$? Thank you for your time.","Given the iterated integral $\int_0^1\int_x^{2-x}(x^2-y) \, dy \, dx$, the value for the type I integral is, \begin{align*} & \int_0^1\int_x^{2-x}(x^2-y)\,dy\,dx \\ = {} & \int_0^1 x^2y\Big|_x^{2-x} \, dx - \int_0^1 \left.\frac{y^2}{2}\right|_x^{2-x} \, dx \\ = {} & \int_0^1 x^2(2-2x) \, dx - \int_0^1(2-2x) \, dx \\ = {} & \int_0^1 2x^2 \, dx - \int_0^1 2x^3 \, dx - \int_0^1 2 \, dx + \int_0^1 2x \, dx \\ = {} & 2 \left.\frac{1}{3}x^3\right|_0^1 - \left.2 \frac{x^4}{4}\right|_0^1 - 2x\Big|_0^1 + 2\frac{1}{2} x^2\Big|_0^1 \\ = {} & \frac{2}{3} - \frac{1}{2} -2 + 1 \\ = {} & \frac{2}{3} - \frac{3}{2} = -\frac{5}{6} \end{align*} We then calculate the type II integral, thus, $$ \int_0^1\int_y^{2-y}(x^2-y) \, dx \, dy = \frac{5}{6} $$ The two integrals differ by the presence of a negative sign. Does this mean that these integrals are not iterated? EDIT: It would seem that the issue has to deal with the order of the integration limits. Why are the limits reversed when you take the iterated integral with respect to $dx \, dy$? Thank you for your time.",,['integration']
76,How to find this kind of function?,How to find this kind of function?,,"I am trying to find a function $f(x,y)$ with $f:\mathbb{R}^{2} \rightarrow [a, b]$ where $[a, b]$ is equal to $[-1, 1]$, $[0, 1]$, or some other small interval (open intervals are fine as well). The partial derivative $\displaystyle\frac{\partial}{\partial x} f(x,y)$ should be $x$ and the partial derivative $\displaystyle\frac{\partial}{\partial y} f(x,y)$ should be $y$. The function $f(x,y)=\frac{1}{2}x^2+\frac{1}{2}y^2$ would have the partial derivatives I want but $\lim_{x\rightarrow\infty} f(x,y) = \infty$ (and the same for $y$) so it does not have the bounds I want. Is it possible to find such function? If it helps it is possible to restrict $x>0$ and $y>0$.","I am trying to find a function $f(x,y)$ with $f:\mathbb{R}^{2} \rightarrow [a, b]$ where $[a, b]$ is equal to $[-1, 1]$, $[0, 1]$, or some other small interval (open intervals are fine as well). The partial derivative $\displaystyle\frac{\partial}{\partial x} f(x,y)$ should be $x$ and the partial derivative $\displaystyle\frac{\partial}{\partial y} f(x,y)$ should be $y$. The function $f(x,y)=\frac{1}{2}x^2+\frac{1}{2}y^2$ would have the partial derivatives I want but $\lim_{x\rightarrow\infty} f(x,y) = \infty$ (and the same for $y$) so it does not have the bounds I want. Is it possible to find such function? If it helps it is possible to restrict $x>0$ and $y>0$.",,"['calculus', 'limits', 'multivariable-calculus', 'functions', 'partial-derivative']"
77,"If $g(x,y)=(x+f(y),y+f(x))$ Why this function $g$ is onto?",If  Why this function  is onto?,"g(x,y)=(x+f(y),y+f(x)) g","Let $f:\mathbb{R}\to\mathbb{R}$ of class $C^1$ such that $|f'(x)|\leq b<1$ for all $x\in\mathbb{R}$. If we define $g:\mathbb{R}^2\to\mathbb{R}^2$ by $g(x,y)=(x+f(y),y+f(x))$ then why is $g$ an onto function? I been trying the following: since $f$ is of class $C^1$ then $g$ is also of class $C^1$, computing the Jacobian yields $\det(Dg(x,y))=1\cdot 1-f'(y)\cdot f'(x)=1-f'(y)f'(x)\neq 0$ (because $f'(y)f'(x)<1$). Then by the Inverse Theorem $g$ is locally invertible in whole $\mathbb{R}^2$.  Is that sufficient to conclude $g$ is an onto function? Also from the hypothesis I know that $f$ satisfies a Lipschitz condition with $b$ constant of Lipschitz. Thanks in advance!!","Let $f:\mathbb{R}\to\mathbb{R}$ of class $C^1$ such that $|f'(x)|\leq b<1$ for all $x\in\mathbb{R}$. If we define $g:\mathbb{R}^2\to\mathbb{R}^2$ by $g(x,y)=(x+f(y),y+f(x))$ then why is $g$ an onto function? I been trying the following: since $f$ is of class $C^1$ then $g$ is also of class $C^1$, computing the Jacobian yields $\det(Dg(x,y))=1\cdot 1-f'(y)\cdot f'(x)=1-f'(y)f'(x)\neq 0$ (because $f'(y)f'(x)<1$). Then by the Inverse Theorem $g$ is locally invertible in whole $\mathbb{R}^2$.  Is that sufficient to conclude $g$ is an onto function? Also from the hypothesis I know that $f$ satisfies a Lipschitz condition with $b$ constant of Lipschitz. Thanks in advance!!",,"['real-analysis', 'multivariable-calculus']"
78,Limit of a function with 2 variables,Limit of a function with 2 variables,,"I am given this function: $$f(x,y)=\begin{cases}\frac{xy^3}{x^2+y^4} & \text{ for } (x,y)\not=(0,0)\\ 0 & \text{ for } (x,y)=(0,0)\end{cases}$$ and I have to check if it is continuous in $(0,0)$. Therefore I want to calculate $\lim \limits_{(x,y) \rightarrow 0}{\frac{xy^3}{x^2+y^4}}$. I already tried substituting and polar coordinates but did not come to a solution yet. Can someone give a few possibilities to calculate a limit of a function with multiple variables. Which is the best one to try first? I found that polarcoordinates often works very well. There has already been a question concerning this function and it's partial derivates and if it's differentiable. You may find that one here: determine whether $f(x, y) = \frac{xy^3}{x^2 + y^4}$ is differentiable at $(0, 0)$. Thanks!","I am given this function: $$f(x,y)=\begin{cases}\frac{xy^3}{x^2+y^4} & \text{ for } (x,y)\not=(0,0)\\ 0 & \text{ for } (x,y)=(0,0)\end{cases}$$ and I have to check if it is continuous in $(0,0)$. Therefore I want to calculate $\lim \limits_{(x,y) \rightarrow 0}{\frac{xy^3}{x^2+y^4}}$. I already tried substituting and polar coordinates but did not come to a solution yet. Can someone give a few possibilities to calculate a limit of a function with multiple variables. Which is the best one to try first? I found that polarcoordinates often works very well. There has already been a question concerning this function and it's partial derivates and if it's differentiable. You may find that one here: determine whether $f(x, y) = \frac{xy^3}{x^2 + y^4}$ is differentiable at $(0, 0)$. Thanks!",,['limits']
79,Computing the shape operator,Computing the shape operator,,"I am trying to compute the shape operator and Gaussian curvature for some smooth zero sets of polynomials $f$ in $\mathbb{R}^n$, oriented by $N = \nabla f / || \nabla f||$ The approach I thinking about is this (which I didn't learn from a text, so maybe there is a problem with it): Compute the normal at a point $p$, and pick some vectors $v, w \ldots $ that are a local frame for the tangent space. Compute $\langle \nabla_{v} N, w \rangle = \frac{1}{||\nabla f||}\Sigma_{i,j=1}^{n+1} \frac{\partial^2 f}{\partial x_i \partial x_j} v_i w_j$. (The justification for this formula: $\nabla_v \frac{ \nabla f}{|| \nabla f ||} = (\nabla_v (\nabla f)) (1/ ||\nabla f||) + NormalComponent$) Deduce from this the matrix for $L_p(v) = - \nabla_v N$. However, something seems to be wrong with this approach. For example, in my computation below for the sphere, I get a Gaussian curvature that is not constant. We pick some point where $y \not = 0$ and $x \not = 0$, then $((x,y,z),-y,x,0)$ and $((x,y,z),0,-z,y)$ describes a local frame. (The first triple is the point, the next three coordinates describe the vector in the tangent space of $\mathbb{R}^3$). Computing the matrix $L_p$ in this basis gives $1/r^2 \begin{pmatrix} x^2 + y^2 &  -xz \\ -xz & z^2 + y^2 \end{pmatrix}$, which has determinant $-y^2 / r^2$... I am really confused. I would appreciate someone pointing out my mistake.","I am trying to compute the shape operator and Gaussian curvature for some smooth zero sets of polynomials $f$ in $\mathbb{R}^n$, oriented by $N = \nabla f / || \nabla f||$ The approach I thinking about is this (which I didn't learn from a text, so maybe there is a problem with it): Compute the normal at a point $p$, and pick some vectors $v, w \ldots $ that are a local frame for the tangent space. Compute $\langle \nabla_{v} N, w \rangle = \frac{1}{||\nabla f||}\Sigma_{i,j=1}^{n+1} \frac{\partial^2 f}{\partial x_i \partial x_j} v_i w_j$. (The justification for this formula: $\nabla_v \frac{ \nabla f}{|| \nabla f ||} = (\nabla_v (\nabla f)) (1/ ||\nabla f||) + NormalComponent$) Deduce from this the matrix for $L_p(v) = - \nabla_v N$. However, something seems to be wrong with this approach. For example, in my computation below for the sphere, I get a Gaussian curvature that is not constant. We pick some point where $y \not = 0$ and $x \not = 0$, then $((x,y,z),-y,x,0)$ and $((x,y,z),0,-z,y)$ describes a local frame. (The first triple is the point, the next three coordinates describe the vector in the tangent space of $\mathbb{R}^3$). Computing the matrix $L_p$ in this basis gives $1/r^2 \begin{pmatrix} x^2 + y^2 &  -xz \\ -xz & z^2 + y^2 \end{pmatrix}$, which has determinant $-y^2 / r^2$... I am really confused. I would appreciate someone pointing out my mistake.",,"['multivariable-calculus', 'differential-geometry']"
80,"Is the function differentiable at $(0,0)$",Is the function differentiable at,"(0,0)","Given the function $$ f(x,y)= \begin{cases} \frac{\sin(x)^4 \ln(1+x^2)}{(1+\cos(x))^2+y^4},  & \text{if $(x,y)\neq (0,0)$} \\ 0, & \text{if $(x,y)=(0,0)$} \end{cases}$$ I want to check if it is differentiable at $(0,0)$. First I checked if it is continuous at $(0,0)$, I saw it is. Then I tried using the definition of differentiable function but couldn't get a definite result. Any ideas?","Given the function $$ f(x,y)= \begin{cases} \frac{\sin(x)^4 \ln(1+x^2)}{(1+\cos(x))^2+y^4},  & \text{if $(x,y)\neq (0,0)$} \\ 0, & \text{if $(x,y)=(0,0)$} \end{cases}$$ I want to check if it is differentiable at $(0,0)$. First I checked if it is continuous at $(0,0)$, I saw it is. Then I tried using the definition of differentiable function but couldn't get a definite result. Any ideas?",,['multivariable-calculus']
81,"An optimization problem, in the form of a word problem,","An optimization problem, in the form of a word problem,",,"The manager of a $1000$ seat concert hall knows from experience that all seats will be occupied if the ticket price is $50$ dollars.  A market survey indicates that $10$ additional seats will remain empty for each $ \$5$ increase of the ticket price.  What is the ticket price which maximizes the manager's revenue?  How many seats will be occupied at that price? My work, so far: The possible scenarios are $50\times1000, 55\times990, 60\times980, \ldots$ Let $x =$ ticket price. Let $y =$ number of tickets sold. Clearly, $x$ and $y$ are $related$ variables.  So, I want to use the method of Lagrange multipliers. Here are some difficulties, though: $$\text{revenue} = f(x,y) = xy$$ is not a vector valued function $-$ I want to compute the gradient of some vector-valued function, $f$, and solve $$\nabla(f) = \lambda \nabla(g).$$ And moreover, what would the constraint function, $g$, even be? I can think of two constraints: $$\max(y) = 1000 \text{ tickets}$$ and $$\min(x) = 50 \text{ dollars}$$ I would like hints only for now. Thanks,","The manager of a $1000$ seat concert hall knows from experience that all seats will be occupied if the ticket price is $50$ dollars.  A market survey indicates that $10$ additional seats will remain empty for each $ \$5$ increase of the ticket price.  What is the ticket price which maximizes the manager's revenue?  How many seats will be occupied at that price? My work, so far: The possible scenarios are $50\times1000, 55\times990, 60\times980, \ldots$ Let $x =$ ticket price. Let $y =$ number of tickets sold. Clearly, $x$ and $y$ are $related$ variables.  So, I want to use the method of Lagrange multipliers. Here are some difficulties, though: $$\text{revenue} = f(x,y) = xy$$ is not a vector valued function $-$ I want to compute the gradient of some vector-valued function, $f$, and solve $$\nabla(f) = \lambda \nabla(g).$$ And moreover, what would the constraint function, $g$, even be? I can think of two constraints: $$\max(y) = 1000 \text{ tickets}$$ and $$\min(x) = 50 \text{ dollars}$$ I would like hints only for now. Thanks,",,"['calculus', 'real-analysis', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
82,"Define function in order to be continuous at $(0,0,0)$",Define function in order to be continuous at,"(0,0,0)","Define the following function at $(0,0,0)$ in order for it to be continuous at that point: $$f(x,y,z)=\frac{x\sin x+y\sin y+z\sin z}{x^2+y^2+z^2}$$ I tried using the paths: $y=m_1x$ and $z=m_2x$ and saw that the limit produces value 1 but I can't generalize my result. Any help?","Define the following function at $(0,0,0)$ in order for it to be continuous at that point: $$f(x,y,z)=\frac{x\sin x+y\sin y+z\sin z}{x^2+y^2+z^2}$$ I tried using the paths: $y=m_1x$ and $z=m_2x$ and saw that the limit produces value 1 but I can't generalize my result. Any help?",,['limits']
83,How to use Cross Product Properites to do proof,How to use Cross Product Properites to do proof,,How do I proceed with a proof for this question? Prove that: \begin{equation}        (a \times  b) \cdot (c \times d)  =  \begin{vmatrix}     a \cdot c & b \cdot c \\     a \cdot d & b \cdot d \\     \end{vmatrix}  \end{equation} I have to use the cross product properties to do the proof: So far I have taken the det of the right side: \begin{equation} (a \times b) \cdot (c \times d) = (a \cdot c) \cdot (b\cdot d) - (b \cdot c)(a \cdot d)  \end{equation} I don't understand how to use the properities after this step.,How do I proceed with a proof for this question? Prove that: \begin{equation}        (a \times  b) \cdot (c \times d)  =  \begin{vmatrix}     a \cdot c & b \cdot c \\     a \cdot d & b \cdot d \\     \end{vmatrix}  \end{equation} I have to use the cross product properties to do the proof: So far I have taken the det of the right side: \begin{equation} (a \times b) \cdot (c \times d) = (a \cdot c) \cdot (b\cdot d) - (b \cdot c)(a \cdot d)  \end{equation} I don't understand how to use the properities after this step.,,"['multivariable-calculus', 'proof-writing', 'vectors']"
84,Line integral of vector field/Why doesn't my solution work?,Line integral of vector field/Why doesn't my solution work?,,"The question in its entirety: Determine for which constants A & B the vector field $$\mathbb{F} = (Axln(z))\mathbb{i} + (By^2z)\mathbb{j} + ((\frac{x^2}{z})+y^3)\mathbb{j}$$ is conservative . If $\Bbb{C}$ is the straight line from $(1,1,1)$ to $(2,1,2)$ find $$\int_c2xln(z)\,dx+2y^2z\,dy + y^3\,dz$$ Now the trick here is supposed to be to observe that for A = 2 and B = 3 the field is conservative, and very similiar to the integral that we have to evaluate, so we can split it up into two parts (one conservative and a non-conservetive). However, as an exercise I wanted to see if I could do it without the ""trick"". I expressed the line parametrically as $$\Bbb{C(t)} = [t,1,t]$$ for $1\leq t\leq 2$. Inserting this into the integral we get $$\int_1^22tln(t)+2t + 1\,dt$$ which evaluates to $$4\,ln(2)+\frac{5}{2}$$ and the answer is supposed to be $$4\,ln(2)-\frac{1}{2}$$ What is it that I'm doing wrong? Is there something wrong with my understanding of work integrals, or have I just missed something minor? UPDATE: I have experienced a similar problem on the next couple of questions that have similar structure, so I am assuming that there is some catch to the integral that it might not be as easily parametriziable as I thought. Where exactly did I go wrong in my line of thinking?","The question in its entirety: Determine for which constants A & B the vector field $$\mathbb{F} = (Axln(z))\mathbb{i} + (By^2z)\mathbb{j} + ((\frac{x^2}{z})+y^3)\mathbb{j}$$ is conservative . If $\Bbb{C}$ is the straight line from $(1,1,1)$ to $(2,1,2)$ find $$\int_c2xln(z)\,dx+2y^2z\,dy + y^3\,dz$$ Now the trick here is supposed to be to observe that for A = 2 and B = 3 the field is conservative, and very similiar to the integral that we have to evaluate, so we can split it up into two parts (one conservative and a non-conservetive). However, as an exercise I wanted to see if I could do it without the ""trick"". I expressed the line parametrically as $$\Bbb{C(t)} = [t,1,t]$$ for $1\leq t\leq 2$. Inserting this into the integral we get $$\int_1^22tln(t)+2t + 1\,dt$$ which evaluates to $$4\,ln(2)+\frac{5}{2}$$ and the answer is supposed to be $$4\,ln(2)-\frac{1}{2}$$ What is it that I'm doing wrong? Is there something wrong with my understanding of work integrals, or have I just missed something minor? UPDATE: I have experienced a similar problem on the next couple of questions that have similar structure, so I am assuming that there is some catch to the integral that it might not be as easily parametriziable as I thought. Where exactly did I go wrong in my line of thinking?",,"['calculus', 'multivariable-calculus', 'definite-integrals', 'vector-fields']"
85,"Find min/max values of $f(x,y)=x^3-y^3+3xy$ in the set $K=[0,4]\times[-4,0]$",Find min/max values of  in the set,"f(x,y)=x^3-y^3+3xy K=[0,4]\times[-4,0]","Find the biggest and the smallest values of the function $f(x,y)=x^3-y^3+3xy$ in the set $K=[0,4]\times[-4,0]$. So using  partial derivatives we find that the critical points are $(0,0)$ and $(1,-1)$. After that we find that $(1,-1)$ is local minimum and $(0,0)$ is saddle point. What generally is the procedure of finding the min and max values in the set $K$? As far as I know we should check for min/max values on the bounds of K. Does that mean that we should check the points $(0,0)$, $(0,-4)$, $(4,-4)$ and $(4,0)$ in this case, or what?","Find the biggest and the smallest values of the function $f(x,y)=x^3-y^3+3xy$ in the set $K=[0,4]\times[-4,0]$. So using  partial derivatives we find that the critical points are $(0,0)$ and $(1,-1)$. After that we find that $(1,-1)$ is local minimum and $(0,0)$ is saddle point. What generally is the procedure of finding the min and max values in the set $K$? As far as I know we should check for min/max values on the bounds of K. Does that mean that we should check the points $(0,0)$, $(0,-4)$, $(4,-4)$ and $(4,0)$ in this case, or what?",,"['analysis', 'multivariable-calculus', 'optimization']"
86,How to calculate $\nabla \ln(|\mathbf{u}-\mathbf{v}|)$,How to calculate,\nabla \ln(|\mathbf{u}-\mathbf{v}|),"I need to calculate:$$\nabla \ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|)$$where $\mathbf{u}$ and $\mathbf{v}$ are vector valued functions with $\alpha$ and $\gamma$ as independent values and $|\cdot|$ denotes Euclidean norm. Does the $\nabla$ operator represents in this case, $$\nabla = \frac{\partial}{\partial \alpha}\mathbf{e}_1+\frac{\partial}{\partial \gamma}\mathbf{e}_2?$$ If so, are the following calculations correct? NOT CORRECT, SEE EDIT $$\frac{\partial}{\partial\alpha}\ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) = \frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}2(\mathbf{u}(\alpha)-\mathbf{v}(\gamma))=\frac{1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)},$$ $$\frac{\partial}{\partial\gamma}\ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) = \frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{-1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}2(\mathbf{u}(\alpha)-\mathbf{v}(\gamma))=\frac{-1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)},$$ Then, $$\nabla \ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) \stackrel{?}{=} \left[\frac{1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)},\frac{-1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)}\right].$$ EDIT I do not think is correct because $\frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}$ is a scalar but $2(\mathbf{u}(\alpha)-\mathbf{v}(\gamma))$ is a vector. Here is my new attempt: $$\frac{\partial}{\partial\alpha}\ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) = \frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\left(2(u_1(\alpha)-v_1(\gamma))\frac{\mathrm{d}}{\mathrm{d}\alpha}u_1(\alpha)+2(u_2(\alpha)-v_2(\gamma))\frac{\mathrm{d}}{\mathrm{d}\alpha}u_2(\alpha)\right)$$where $u_1$ denotes the first component of vector $\mathbf{u}(\alpha)$. Thanks for helping!","I need to calculate:$$\nabla \ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|)$$where $\mathbf{u}$ and $\mathbf{v}$ are vector valued functions with $\alpha$ and $\gamma$ as independent values and $|\cdot|$ denotes Euclidean norm. Does the $\nabla$ operator represents in this case, $$\nabla = \frac{\partial}{\partial \alpha}\mathbf{e}_1+\frac{\partial}{\partial \gamma}\mathbf{e}_2?$$ If so, are the following calculations correct? NOT CORRECT, SEE EDIT $$\frac{\partial}{\partial\alpha}\ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) = \frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}2(\mathbf{u}(\alpha)-\mathbf{v}(\gamma))=\frac{1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)},$$ $$\frac{\partial}{\partial\gamma}\ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) = \frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{-1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}2(\mathbf{u}(\alpha)-\mathbf{v}(\gamma))=\frac{-1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)},$$ Then, $$\nabla \ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) \stackrel{?}{=} \left[\frac{1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)},\frac{-1}{\mathbf{u}(\alpha)-\mathbf{v}(\gamma)}\right].$$ EDIT I do not think is correct because $\frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}$ is a scalar but $2(\mathbf{u}(\alpha)-\mathbf{v}(\gamma))$ is a vector. Here is my new attempt: $$\frac{\partial}{\partial\alpha}\ln(|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|) = \frac{1}{|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\frac{1}{2|\mathbf{u}(\alpha)-\mathbf{v}(\gamma)|}\left(2(u_1(\alpha)-v_1(\gamma))\frac{\mathrm{d}}{\mathrm{d}\alpha}u_1(\alpha)+2(u_2(\alpha)-v_2(\gamma))\frac{\mathrm{d}}{\mathrm{d}\alpha}u_2(\alpha)\right)$$where $u_1$ denotes the first component of vector $\mathbf{u}(\alpha)$. Thanks for helping!",,"['multivariable-calculus', 'vector-analysis', 'solution-verification']"
87,Triple integrals using spherical coordinates with a sphere not centered at the origin,Triple integrals using spherical coordinates with a sphere not centered at the origin,,"Let $F(x,y,z)=(6x,-2y,5z)$ a vector field and $S$ the surface of the sphere centered at $(1,0,1)$ e radius $5$. Find the flux of $F$ across the surface $S$. I want to use the Gauss theorem, and my problem is when i use spherical coordinates how can i found the bounds for $\rho$, $\psi$ if the sphere is not centered at the origin. Thank you","Let $F(x,y,z)=(6x,-2y,5z)$ a vector field and $S$ the surface of the sphere centered at $(1,0,1)$ e radius $5$. Find the flux of $F$ across the surface $S$. I want to use the Gauss theorem, and my problem is when i use spherical coordinates how can i found the bounds for $\rho$, $\psi$ if the sphere is not centered at the origin. Thank you",,['multivariable-calculus']
88,Parametric integral question,Parametric integral question,,I haven't done something like this in a long time. How do I set something like this up? Can someone help me with the beginning or give me some direction?,I haven't done something like this in a long time. How do I set something like this up? Can someone help me with the beginning or give me some direction?,,"['ordinary-differential-equations', 'multivariable-calculus']"
89,Double integral finding new limit $\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv$,Double integral finding new limit,\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv,"I don't understand how to find new limits when I use substitution method. I have this integral : $$\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv=$$ $$t=u^2+4v, dt=2udu, \frac{dt}{2}=udu$$ The new limits for $du$ are $\int_{4v}^{1+4v}$ after I substitute $u=1,u=0$ for $t$. But for some reason in my book they get $\int_{0}^{1}$, but I don't understand what about $4v$ Any ideas? Any help will be appreciated, Thanks in advance! EDIT (The full answer in the book): $$\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv=\int_1^2(\sqrt{u^2+4v}|_{u=0}^{u=1})dv=\int_1^2(\sqrt{4v+1}-2\sqrt{v})dv=\\\frac{2}{3}(\frac{1}{4}\sqrt{(4v+1)^3}-2\sqrt{v^3} |_{1}^{2}=\frac{35}{6}-\frac{5\sqrt{5}}{5}-\frac{8\sqrt{2}}{3}$$","I don't understand how to find new limits when I use substitution method. I have this integral : $$\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv=$$ $$t=u^2+4v, dt=2udu, \frac{dt}{2}=udu$$ The new limits for $du$ are $\int_{4v}^{1+4v}$ after I substitute $u=1,u=0$ for $t$. But for some reason in my book they get $\int_{0}^{1}$, but I don't understand what about $4v$ Any ideas? Any help will be appreciated, Thanks in advance! EDIT (The full answer in the book): $$\int_1^2\int_0^1 \frac{u}{\sqrt{u^2+4v}} dudv=\int_1^2(\sqrt{u^2+4v}|_{u=0}^{u=1})dv=\int_1^2(\sqrt{4v+1}-2\sqrt{v})dv=\\\frac{2}{3}(\frac{1}{4}\sqrt{(4v+1)^3}-2\sqrt{v^3} |_{1}^{2}=\frac{35}{6}-\frac{5\sqrt{5}}{5}-\frac{8\sqrt{2}}{3}$$",,"['integration', 'multivariable-calculus', 'definite-integrals']"
90,Vector Identity Question,Vector Identity Question,,I am having some trouble with this question regarding vector diffiriential operators. It seems easy and I am not sure what I am missing. The question: Prove: $$ \mathbf{(u\cdot\nabla)u+u\times(\nabla\times u)=}\frac{1}{2}\nabla(|\mathbf{u}|^{2}). $$ My attempt: \begin{align*} LHS= & \mathbf{(u\cdot\nabla)u+u\times(\nabla\times u)}\\ = & \mathbf{(u\cdot\nabla)u+\nabla(u\cdot u)-}\mathbf{u(u\cdot\nabla)}\text{ by vector triple product}\\ = & \mathbf{\nabla(u\cdot u)}\text{ and now I would like to write:}\\ = & \nabla(|\mathbf{u}|^{2}). \end{align*} I think perhaps my cancellation on the second line is incorrect? I tried using index notation but since I am probably using an incorrect identity somewhere this just obscured what was going on. Any help would be appreciated. Please try and use as few unproved identities as possible (unless they are really fundamental).,I am having some trouble with this question regarding vector diffiriential operators. It seems easy and I am not sure what I am missing. The question: Prove: $$ \mathbf{(u\cdot\nabla)u+u\times(\nabla\times u)=}\frac{1}{2}\nabla(|\mathbf{u}|^{2}). $$ My attempt: \begin{align*} LHS= & \mathbf{(u\cdot\nabla)u+u\times(\nabla\times u)}\\ = & \mathbf{(u\cdot\nabla)u+\nabla(u\cdot u)-}\mathbf{u(u\cdot\nabla)}\text{ by vector triple product}\\ = & \mathbf{\nabla(u\cdot u)}\text{ and now I would like to write:}\\ = & \nabla(|\mathbf{u}|^{2}). \end{align*} I think perhaps my cancellation on the second line is incorrect? I tried using index notation but since I am probably using an incorrect identity somewhere this just obscured what was going on. Any help would be appreciated. Please try and use as few unproved identities as possible (unless they are really fundamental).,,"['multivariable-calculus', 'vector-analysis']"
91,Need some help understanding the condition of the implicit function theorem,Need some help understanding the condition of the implicit function theorem,,"The condition for the implicit function theorem is that the (smooth) map $f: \mathbb R^n \to \mathbb R^m$ is locally a (smooth) map of $n-k$ variables if there are locally smooth maps $g_i , i \in \{n-k+1, \dots, n\}$ with the property that $\nabla g_i$ are linearly independent. To make this question as simple as possible let's consider maps $f: \mathbb R^2 \to \mathbb R$ so that the implicit map will be $F(x,y,z) = 0$ for some $F$. A level set of a (smooth) surface. The condition that a gradient of a surface at a point is zero means the point in consideration is an extremum. So the condition of the theorem excludes points that are ""completely flat"". But I am not sure what this means: thinking of normal extrema, like the north pole on the sphere, of course not all gradients are zero (only two(?)). But what does a point on a surface look like if all the gradients vanish? and also: Why does this ""flatness"" prevent us from expressing one of the   coordinates in terms of the other two? Note: I assume the answer to question two will turn out to be interesting as it involves a modified version of the theorem: the theorem gives a smooth function (for the derivative of this function we need the non-zero gradients). But what happens if we drop the condition of (smoothness or) continuous differentiability and just express the last coordinate in terms of the other two so that the resulting function is maybe continuous but not differentiable? Could someone please show me an example in which we can parameterise   the surface locally as $z = f(x,y)$ where $f$ is continuous but not   differentiable? (I am hoping to gain some insight from this example   into why we require the gradients to be nonzero)","The condition for the implicit function theorem is that the (smooth) map $f: \mathbb R^n \to \mathbb R^m$ is locally a (smooth) map of $n-k$ variables if there are locally smooth maps $g_i , i \in \{n-k+1, \dots, n\}$ with the property that $\nabla g_i$ are linearly independent. To make this question as simple as possible let's consider maps $f: \mathbb R^2 \to \mathbb R$ so that the implicit map will be $F(x,y,z) = 0$ for some $F$. A level set of a (smooth) surface. The condition that a gradient of a surface at a point is zero means the point in consideration is an extremum. So the condition of the theorem excludes points that are ""completely flat"". But I am not sure what this means: thinking of normal extrema, like the north pole on the sphere, of course not all gradients are zero (only two(?)). But what does a point on a surface look like if all the gradients vanish? and also: Why does this ""flatness"" prevent us from expressing one of the   coordinates in terms of the other two? Note: I assume the answer to question two will turn out to be interesting as it involves a modified version of the theorem: the theorem gives a smooth function (for the derivative of this function we need the non-zero gradients). But what happens if we drop the condition of (smoothness or) continuous differentiability and just express the last coordinate in terms of the other two so that the resulting function is maybe continuous but not differentiable? Could someone please show me an example in which we can parameterise   the surface locally as $z = f(x,y)$ where $f$ is continuous but not   differentiable? (I am hoping to gain some insight from this example   into why we require the gradients to be nonzero)",,"['multivariable-calculus', 'differential-geometry', 'intuition']"
92,Inquiring about change of parameters in double (OR Triple) integrals,Inquiring about change of parameters in double (OR Triple) integrals,,"It's known that while replacing $x$ and $y$ coordinates in double integration, by another coordinate system $u(x,y)$ and $v(x,y)$, then $$ \int_{a_2}^{b_2} \int_{a_1(y)}^{b_1(y)} f\left(x,y \right) \: dx \;dy=\int_{c_2}^{d_2} \int_{c_1(v)}^{d_1(v)} f\left(x(u,v),y(u,v) \right) \:\left|J\right| du \;dv=$$ Where $\left|J\right|$ is called: Jacobian determinant which is defined as: $$\left|J \right|= \left |\begin{matrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\  \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{matrix}  \right | $$ In an attempt to prove that $dx\; dy=\left|J\right| du\;dv$, I firstly get the total differentation of $x$ and $y$ since they are functions of $u$ and $v$, so: $$dx=\frac{\partial x}{\partial u} du+\frac{\partial x}{\partial v} dv$$ And $$dy=\frac{\partial y}{\partial u} du+\frac{\partial y}{\partial v} dv$$ Multipling $dx$ and $dy$ $$\therefore dx \; dy=\left(\frac{\partial x}{\partial u} du+\frac{\partial x}{\partial v} dv \right)\left(\frac{\partial y}{\partial u} du+\frac{\partial y}{\partial v} dv \right) \\= \frac{\partial x}{\partial u} \frac{\partial u}{\partial u} du\;du+\frac{\partial x}{\partial v}\frac{\partial y}{\partial v}dv\;dv+du\;dv\left( \frac{\partial x}{\partial u}\frac{\partial y}{\partial v} + \frac{\partial x}{\partial v} \frac{\partial y}{\partial u}\right) $$ Which is too far from the formula $\left|J\right| du\;dv$. So, what's the wrong?","It's known that while replacing $x$ and $y$ coordinates in double integration, by another coordinate system $u(x,y)$ and $v(x,y)$, then $$ \int_{a_2}^{b_2} \int_{a_1(y)}^{b_1(y)} f\left(x,y \right) \: dx \;dy=\int_{c_2}^{d_2} \int_{c_1(v)}^{d_1(v)} f\left(x(u,v),y(u,v) \right) \:\left|J\right| du \;dv=$$ Where $\left|J\right|$ is called: Jacobian determinant which is defined as: $$\left|J \right|= \left |\begin{matrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v}\\  \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{matrix}  \right | $$ In an attempt to prove that $dx\; dy=\left|J\right| du\;dv$, I firstly get the total differentation of $x$ and $y$ since they are functions of $u$ and $v$, so: $$dx=\frac{\partial x}{\partial u} du+\frac{\partial x}{\partial v} dv$$ And $$dy=\frac{\partial y}{\partial u} du+\frac{\partial y}{\partial v} dv$$ Multipling $dx$ and $dy$ $$\therefore dx \; dy=\left(\frac{\partial x}{\partial u} du+\frac{\partial x}{\partial v} dv \right)\left(\frac{\partial y}{\partial u} du+\frac{\partial y}{\partial v} dv \right) \\= \frac{\partial x}{\partial u} \frac{\partial u}{\partial u} du\;du+\frac{\partial x}{\partial v}\frac{\partial y}{\partial v}dv\;dv+du\;dv\left( \frac{\partial x}{\partial u}\frac{\partial y}{\partial v} + \frac{\partial x}{\partial v} \frac{\partial y}{\partial u}\right) $$ Which is too far from the formula $\left|J\right| du\;dv$. So, what's the wrong?",,"['real-analysis', 'integration', 'multivariable-calculus']"
93,"Definition of ""vector fields never have opposite direction""","Definition of ""vector fields never have opposite direction""",,"Good day! As in my other question I am referring to the book ""Differential Equations and Dynamical Systems"" by Lawrence Perko, chapter 3.12. I have a question regarding Lemma 2: Lemma 2. If $v$ and $w$ are two continuous vector fields defined on a Jordan Curve $C$ which never have opposite directions or are zero on $C$, then $I_v(C)=I_w(C)$. What is the definition of ""never having opposite directions"" for the vector fields $v,w\in C^1(\mathbb{R}^2)$ (or $v,w\in C^1(E)$ where $E$ is an open subset of $\mathbb{R}^2$). I couldn't find a definition in Perko's book for this.","Good day! As in my other question I am referring to the book ""Differential Equations and Dynamical Systems"" by Lawrence Perko, chapter 3.12. I have a question regarding Lemma 2: Lemma 2. If $v$ and $w$ are two continuous vector fields defined on a Jordan Curve $C$ which never have opposite directions or are zero on $C$, then $I_v(C)=I_w(C)$. What is the definition of ""never having opposite directions"" for the vector fields $v,w\in C^1(\mathbb{R}^2)$ (or $v,w\in C^1(E)$ where $E$ is an open subset of $\mathbb{R}^2$). I couldn't find a definition in Perko's book for this.",,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'definition', 'vector-fields']"
94,Change of variables in a double integral-proving injectivity,Change of variables in a double integral-proving injectivity,,"I want to make the following change: $$u=x^2 - y^2 \quad v=xy$$ where my region is in the first quadrant bounded by: $x^2-y^2 = 3 , \quad xy=1 , \quad x^2-xy-y^2 = 1 $  . How can I prove this change of variables is legitimate ? (i.e., how can I prove this change is injective [one-to-one]) I have tried solving $$ x_1 ^2 - y_1 ^2 = x_2 ^2 -y_2 ^2 , \, x_1 y_1 = x_2 y_2 $$ but this implies $x_1 = x_2 , y_1 = y_2 $ as required, but also: $x_2 = y_1 $ for example. Will you please help me figure out how to prove the injectivity of this ? Thanks a lot","I want to make the following change: $$u=x^2 - y^2 \quad v=xy$$ where my region is in the first quadrant bounded by: $x^2-y^2 = 3 , \quad xy=1 , \quad x^2-xy-y^2 = 1 $  . How can I prove this change of variables is legitimate ? (i.e., how can I prove this change is injective [one-to-one]) I have tried solving $$ x_1 ^2 - y_1 ^2 = x_2 ^2 -y_2 ^2 , \, x_1 y_1 = x_2 y_2 $$ but this implies $x_1 = x_2 , y_1 = y_2 $ as required, but also: $x_2 = y_1 $ for example. Will you please help me figure out how to prove the injectivity of this ? Thanks a lot",,"['integration', 'multivariable-calculus']"
95,"Showing $∇f (0, 0) = (0, 0)$ using chain rule",Showing  using chain rule,"∇f (0, 0) = (0, 0)","I'm trying to show the following but I'm not very sure how to proceed. Could someone please explain to me how to approach and solve the following question? Let $f : \Bbb R^2 → \Bbb R$ be differentiable such that $f(x, 2x) = 1$   and $f (−x, x) = 1 \; ∀x ∈ \Bbb R$. Using the Chain Rule, show that   $∇f (0, 0) = (0, 0).$","I'm trying to show the following but I'm not very sure how to proceed. Could someone please explain to me how to approach and solve the following question? Let $f : \Bbb R^2 → \Bbb R$ be differentiable such that $f(x, 2x) = 1$   and $f (−x, x) = 1 \; ∀x ∈ \Bbb R$. Using the Chain Rule, show that   $∇f (0, 0) = (0, 0).$",,['multivariable-calculus']
96,Negativity of a power function.,Negativity of a power function.,,"For what values (or intervals) of 'a' it holds $2(x+1)^a$ - $x^a$ - $(x+2)^a$$<0$, where $x\in N$. I tried to do it by first derivative test but it again gives almost same type expression which is difficult to solve. By Jensen's inequality it is OK but then I am unable to show that for what values of 'a' the given function $2(x+1)^a$ - $x^a$ - $(x+2)^a$ is convex. For $a=\frac{-1}{2}$ it holds.","For what values (or intervals) of 'a' it holds $2(x+1)^a$ - $x^a$ - $(x+2)^a$$<0$, where $x\in N$. I tried to do it by first derivative test but it again gives almost same type expression which is difficult to solve. By Jensen's inequality it is OK but then I am unable to show that for what values of 'a' the given function $2(x+1)^a$ - $x^a$ - $(x+2)^a$ is convex. For $a=\frac{-1}{2}$ it holds.",,"['calculus', 'multivariable-calculus', 'inequality']"
97,Notation for integral of a vector function over an ellipsoid,Notation for integral of a vector function over an ellipsoid,,"For a short proof, I need to write a point $\pmb y\in\mathbb{R}^p$ as   the integral of the surface of the ellipse $\pmb x^{\top}\pmb Q\pmb x=c$ where $\pmb Q$ is a $p$ by $p$ PSD matrix (for now $\pmb y$ is defined in words). What is the formal way to write this? Can we do better than: $$\pmb y=\int_{\pmb x\in\mathbb{R}^p:\pmb x^{\top}\pmb Q\pmb x=c} \pmb x d(\pmb x)$$ Also, I am not a professional mathematician so I am not too sure about the $d(\pmb x)$ part.","For a short proof, I need to write a point $\pmb y\in\mathbb{R}^p$ as   the integral of the surface of the ellipse $\pmb x^{\top}\pmb Q\pmb x=c$ where $\pmb Q$ is a $p$ by $p$ PSD matrix (for now $\pmb y$ is defined in words). What is the formal way to write this? Can we do better than: $$\pmb y=\int_{\pmb x\in\mathbb{R}^p:\pmb x^{\top}\pmb Q\pmb x=c} \pmb x d(\pmb x)$$ Also, I am not a professional mathematician so I am not too sure about the $d(\pmb x)$ part.",,"['multivariable-calculus', 'notation', 'surface-integrals']"
98,Continuity of multivariable functions,Continuity of multivariable functions,,"I have a question regarding norms on $\Bbb R^{n}$ and proving the continuity of multivariable functions.  Specifically, suppose we have $f: \Bbb R^{2} \to \Bbb R$, for example.  To prove $f$ is continuous at some $x \in \Bbb R^{2}$, we need to show for every $\epsilon > 0$, $\exists \delta > 0$ such that $|| x - y || < \delta$ implies $|f(x) - f(y)| < \epsilon$. My first question regarding the above is: Since all possible norms on $\Bbb R^{2}$ are equivalent, is it normal for us to use whichever norm on $\Bbb R^{2}$ that makes the proof above the easiest?  I have an example of a proof I came up with below to illustrate what I mean (and I would like it to be checked). Now, my second question is about the equivalence of metrics: Consider two metrics $d_{1}$ and $d_{2}$ on some space $X$, and consider the topologies of each metric.  If we have $d_{1}(x,y) \leq c d_{2}(x,y)$ for some constant $c > 0$, does this imply the topology of $d_{1}$ is strictly finer than the topology of $d_{2}$?  I can't seem to prove this.  I guess I would need to show for any open ball around $x \in X$ in the $d_{2}$ metric, we can find an open ball around $x$ in the $d_{1}$ metric contained in the first open ball.  I think this is deceptively easy, but I haven't been able to do it. Finally, my last question is regarding whether or not the below proof is correct: Prove $f(x,y) = xy$ is continuous at each $(x_{0},y_{0}) \in \Bbb R^{2}$. To prove this, I will choose to use the $|| \cdot || _{\infty}$ norm on $\Bbb R^{2}$.  Let $\epsilon > 0$.  Then $|xy - x_{0}y_{0}| = |xy - xy_{0} + xy_{0} - x_{0}y_{0}| \leq |x||y - y_{0}| + |y_{0}| |x - x_{0}| \leq (|x| + |y_{0}|)||(x,y) - (x_{0},y_{0})||_{\infty}$.  If neither $|x|$ nor $|y_{0}|$ equal $0$, then choose $\delta = \dfrac{\epsilon}{|x| + |y_{0}|}$.","I have a question regarding norms on $\Bbb R^{n}$ and proving the continuity of multivariable functions.  Specifically, suppose we have $f: \Bbb R^{2} \to \Bbb R$, for example.  To prove $f$ is continuous at some $x \in \Bbb R^{2}$, we need to show for every $\epsilon > 0$, $\exists \delta > 0$ such that $|| x - y || < \delta$ implies $|f(x) - f(y)| < \epsilon$. My first question regarding the above is: Since all possible norms on $\Bbb R^{2}$ are equivalent, is it normal for us to use whichever norm on $\Bbb R^{2}$ that makes the proof above the easiest?  I have an example of a proof I came up with below to illustrate what I mean (and I would like it to be checked). Now, my second question is about the equivalence of metrics: Consider two metrics $d_{1}$ and $d_{2}$ on some space $X$, and consider the topologies of each metric.  If we have $d_{1}(x,y) \leq c d_{2}(x,y)$ for some constant $c > 0$, does this imply the topology of $d_{1}$ is strictly finer than the topology of $d_{2}$?  I can't seem to prove this.  I guess I would need to show for any open ball around $x \in X$ in the $d_{2}$ metric, we can find an open ball around $x$ in the $d_{1}$ metric contained in the first open ball.  I think this is deceptively easy, but I haven't been able to do it. Finally, my last question is regarding whether or not the below proof is correct: Prove $f(x,y) = xy$ is continuous at each $(x_{0},y_{0}) \in \Bbb R^{2}$. To prove this, I will choose to use the $|| \cdot || _{\infty}$ norm on $\Bbb R^{2}$.  Let $\epsilon > 0$.  Then $|xy - x_{0}y_{0}| = |xy - xy_{0} + xy_{0} - x_{0}y_{0}| \leq |x||y - y_{0}| + |y_{0}| |x - x_{0}| \leq (|x| + |y_{0}|)||(x,y) - (x_{0},y_{0})||_{\infty}$.  If neither $|x|$ nor $|y_{0}|$ equal $0$, then choose $\delta = \dfrac{\epsilon}{|x| + |y_{0}|}$.",,"['general-topology', 'multivariable-calculus', 'continuity', 'normed-spaces']"
99,How can I find these partial derivatives?,How can I find these partial derivatives?,,"I'm reading a book which gives this function $f(x,y)=x^2y/(x^2+y^2)$ if $(x,y)\neq (0,0)$ and $f(0,0)=0$ as a $C^1$ function in $\mathbb R^2-\{(0,0)\}$, continuous in $(0,0)$ and it has the partial derivatives $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$. In fact the author wants to point out a function which has the partial derivatives in $(0,0)$, but it's not differentiable in $(0,0)$. The problem is I can't find the partial derivatives $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$ I think I should use the limits. See: $$\frac{\partial f}{\partial x}(x,y)=\frac{2xy(x^2+y^2)-2x^3y}{(x^2+y^2)^2}$$ and $$\frac{\partial f}{\partial y}(x,y)=\frac{x^2(x^2+y^2)-2x^2y^2}{(x^2+y^2)^2}$$ They aren't defined at $(0,0)$. How can I proceed to find these partial derivatives at $(0,0)$? Thanks","I'm reading a book which gives this function $f(x,y)=x^2y/(x^2+y^2)$ if $(x,y)\neq (0,0)$ and $f(0,0)=0$ as a $C^1$ function in $\mathbb R^2-\{(0,0)\}$, continuous in $(0,0)$ and it has the partial derivatives $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$. In fact the author wants to point out a function which has the partial derivatives in $(0,0)$, but it's not differentiable in $(0,0)$. The problem is I can't find the partial derivatives $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$ I think I should use the limits. See: $$\frac{\partial f}{\partial x}(x,y)=\frac{2xy(x^2+y^2)-2x^3y}{(x^2+y^2)^2}$$ and $$\frac{\partial f}{\partial y}(x,y)=\frac{x^2(x^2+y^2)-2x^2y^2}{(x^2+y^2)^2}$$ They aren't defined at $(0,0)$. How can I proceed to find these partial derivatives at $(0,0)$? Thanks",,"['real-analysis', 'complex-analysis', 'multivariable-calculus']"
