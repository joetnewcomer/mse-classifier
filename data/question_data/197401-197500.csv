,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What does it mean that a distribution is integrable?,What does it mean that a distribution is integrable?,,"I am studying geometric control theory, and I am focusing on the Frobenius theorem . I have seen that it gives sufficient and necessary conditions for integrability of a distribution, but I am having troubles understanding well the concept. The Frobenius theorem states that a distribution is integrable if and only if it is involutive. I have clear the concept of involutivity, but what I have not clear is the concept of integrability. From the notes of my professor, I have that a distribution $\Delta (x)$ of rank $k$ is integrable if there exist the functions $\lambda_1(x) .... \lambda_{k-n}(x) $ such that: $\frac{\partial \lambda }{\partial x}\Delta (x)=0$ but what does it mean? I cannot understand clearly the concept of integrability of a distribution. Moreover, I don't understand the meaning of the fact that the product of the derivative of $\lambda$ with respect to $x$ with  the distribution gives zero. I know that this implies orthogonality, but what does it mean that these two are orthogonal? Can somebody please help me?","I am studying geometric control theory, and I am focusing on the Frobenius theorem . I have seen that it gives sufficient and necessary conditions for integrability of a distribution, but I am having troubles understanding well the concept. The Frobenius theorem states that a distribution is integrable if and only if it is involutive. I have clear the concept of involutivity, but what I have not clear is the concept of integrability. From the notes of my professor, I have that a distribution of rank is integrable if there exist the functions such that: but what does it mean? I cannot understand clearly the concept of integrability of a distribution. Moreover, I don't understand the meaning of the fact that the product of the derivative of with respect to with  the distribution gives zero. I know that this implies orthogonality, but what does it mean that these two are orthogonal? Can somebody please help me?",\Delta (x) k \lambda_1(x) .... \lambda_{k-n}(x)  \frac{\partial \lambda }{\partial x}\Delta (x)=0 \lambda x,"['differential-geometry', 'dynamical-systems', 'control-theory']"
1,Is every possible chart a member of some maximal smooth atlas?,Is every possible chart a member of some maximal smooth atlas?,,"If $M$ is a topological $n$ -manifold (edit: that admits at least one smooth structure) and I select any open set $U \subseteq M$ , and I find that there exists some $\varphi: U \rightarrow \varphi(U)$ where $\varphi(U) \subseteq \mathbb{R}^n$ and $\varphi$ is a homeomorphism, then the pair $(U, \varphi)$ are a chart on $M$ . Is it necessarily the case that some smooth structure $\overline{\mathcal{A}}$ exists so that $(U, \varphi) \in \overline{\mathcal{A}}$ ? A little more context. My current understanding is that there are a number of ways to conceptualize a smooth structure: as a maximal smooth atlas, as an equivalence class of smooth atlases, or as a maximal set of mutually compatible charts. I've found a couple of great answers about smooth structures that consider the smooth structure from the perspective of equivalent atlases  ( Manifold and maximal atlas and Why maximal atlas ). And I also understand that given even just one smooth atlas $\mathcal{A}$ , one can essentially generate a unique maximal smooth atlas $\overline{\mathcal{A}}$ such that $\mathcal{A} \subseteq \overline{\mathcal{A}}$ . So my question is more from the perspective of the individual charts. I am almost certain that not every open set $U$ is suitable to be a domain for a chart. For example, $M$ is, itself, an open set. But certainly not every $M$ is globally homeomorphic to $\mathbb{R}^n$ . So, I think it's the case that not every $U \subseteq M$ is homeomorphic to $\mathbb{R}^n$ . But for those $U$ that are, is it certainly the case that the chart $(U, \varphi)$ is included in some smooth atlas? Thanks!","If is a topological -manifold (edit: that admits at least one smooth structure) and I select any open set , and I find that there exists some where and is a homeomorphism, then the pair are a chart on . Is it necessarily the case that some smooth structure exists so that ? A little more context. My current understanding is that there are a number of ways to conceptualize a smooth structure: as a maximal smooth atlas, as an equivalence class of smooth atlases, or as a maximal set of mutually compatible charts. I've found a couple of great answers about smooth structures that consider the smooth structure from the perspective of equivalent atlases  ( Manifold and maximal atlas and Why maximal atlas ). And I also understand that given even just one smooth atlas , one can essentially generate a unique maximal smooth atlas such that . So my question is more from the perspective of the individual charts. I am almost certain that not every open set is suitable to be a domain for a chart. For example, is, itself, an open set. But certainly not every is globally homeomorphic to . So, I think it's the case that not every is homeomorphic to . But for those that are, is it certainly the case that the chart is included in some smooth atlas? Thanks!","M n U \subseteq M \varphi: U \rightarrow \varphi(U) \varphi(U) \subseteq \mathbb{R}^n \varphi (U, \varphi) M \overline{\mathcal{A}} (U, \varphi) \in \overline{\mathcal{A}} \mathcal{A} \overline{\mathcal{A}} \mathcal{A} \subseteq \overline{\mathcal{A}} U M M \mathbb{R}^n U \subseteq M \mathbb{R}^n U (U, \varphi)","['differential-geometry', 'manifolds', 'smooth-manifolds']"
2,Flat bundle and constant transition functions,Flat bundle and constant transition functions,,"Let $E$ be a vector bundle over connected manifold $M$ and let $\nabla$ be a connection on $E$ i.e. a map $\nabla: \Gamma^{\infty}(M,E) \to \Omega^1(M) \otimes \Gamma^{\infty}(M,E)$. It can be extended to act on whole $\Omega(M,E):=\Omega^{\bullet}(M) \otimes \Gamma^{\infty}(M,E)$. Although $\nabla$ is not $C^{\infty}(M)$ linear its square $\nabla^2$ (understood as a composition of original connection with the extended one: $\nabla \circ \nabla$) is $C^{\infty}(M)$ linear. A bundle $E$ is called flat if $\nabla^2=0$. I would like to understand why Flatness is equivalent to the condition that the transition functions of $E$ are constant. I know that this was discussed already, e.g. here however the explanation there is rather sketchy and I would like to see some more detailed argument (and also understand the converse implication).","Let $E$ be a vector bundle over connected manifold $M$ and let $\nabla$ be a connection on $E$ i.e. a map $\nabla: \Gamma^{\infty}(M,E) \to \Omega^1(M) \otimes \Gamma^{\infty}(M,E)$. It can be extended to act on whole $\Omega(M,E):=\Omega^{\bullet}(M) \otimes \Gamma^{\infty}(M,E)$. Although $\nabla$ is not $C^{\infty}(M)$ linear its square $\nabla^2$ (understood as a composition of original connection with the extended one: $\nabla \circ \nabla$) is $C^{\infty}(M)$ linear. A bundle $E$ is called flat if $\nabla^2=0$. I would like to understand why Flatness is equivalent to the condition that the transition functions of $E$ are constant. I know that this was discussed already, e.g. here however the explanation there is rather sketchy and I would like to see some more detailed argument (and also understand the converse implication).",,"['differential-geometry', 'vector-bundles', 'connections']"
3,A proof of: The derivative of the determinant is the trace,A proof of: The derivative of the determinant is the trace,,"I want to solve the following problem: Show that the derivative of $\mbox{det}:GL(n,\mathbb{R})\rightarrow\mathbb{R}$     at $I\in GL(n,\mathbb{R})$     is given by    $$\mbox{det}_{*}(I)(X)=\mbox{tr}X$$ I would like you to check my proof, and answer the question in the end. My Attempt : I'll denote $N=GL(n,\mathbb{R})$  . Also, $\simeq$   will be used for vector space isomorphisms and $\cong$   will be used for diffeomorphisms. We know that $\mbox{det}_{*}(I):T_{I}N\rightarrow T_{det(I)=1}\mathbb{R}$  . Let $X\in T_{I}N$  . We can write $X$   in a basis of $T_{I}N$  . So let us find a basis $of T_{I}N$  : we know that $T_{I}N\simeq M_{n}(\mathbb{R})$  , so we can get a basis of $T_{I}N$   from a basis of $M_{n}(\mathbb{R})$   using an isomorphism. The function $$f:T_{I}N	\rightarrow	M_{n}(\mathbb{R}) \\ [\gamma]	\mapsto	\gamma'(0)$$ is known to be an isomorphism. Furthermore, ${E_{ij}}$   is a basis for $M_{n}(\mathbb{R})$  , where $E_{ij}$   is the $n\times n$   matrix whose entries are all zero except the entry $i,j$  , which is $1$  . Thus, a basis for $T_{I}N$   is ${f^{-1}(E_{ij})}$  . Now, $f^{-1}(E_{ij})$   is the equivalence class of curves $\gamma:\mathbb{R}\rightarrow N$   such that $\gamma(0)=I$   and $\gamma'(0)=E_{ij}$  . Hence, a representative of this equivalence class is $\alpha_{ij}(t)=I+tE_{ij}$  , and so we can write ${f^{-1}(E_{ij})}={[\alpha_{ij}]}$  . Hence, we can write $X=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}[\alpha_{ij}]$  . Let us see how $det_{*}$   acts on the basis elements $[\alpha_{ij}]$  . We have $\mbox{det}_{*}(I)([\alpha_{ij}])=[\mbox{det}\circ\alpha_{ij}]_{1}$ by definition of derivative (the subscript 1   reminds us that the equivalence relation of this equivalence class is different, since it is defined on the set of all curves of the type $\gamma:\mathbb{R}\rightarrow\mathbb{R}$   such that $\gamma(0)=\mbox{det}(I)=1  ).$ Now, $\mbox{det}\circ\alpha_{ij}:\mathbb{R}\rightarrow\mathbb{R}$   is such that $$\mbox{det}\circ\alpha_{ij}=\mbox{det}(\alpha_{ij}(t))=\mbox{det}\left(I+tE_{ij}\right)=\mbox{det}\left(\left[\begin{array}{ccc} 1 &  & \mathbb{O}\\  & \ddots\\ \mathbb{O} &  & 1 \end{array}\right]+\left[\begin{array}{cccc} \mathbb{O} &  &  & \mathbb{O}\\  &  & t\,(i,j\mbox{ entry})\\ \\ \mathbb{O} &  &  & \mathbb{O} \end{array}\right]\right)$$  . The matrix is triangular (or simply diagonal), and so the determinant is the product of the diagonal elements. Hence, $\mbox{det}\circ\alpha_{ij}=1+t\delta_{ij}$  , with $\delta_{ij}$   the Kronecker delta. Hence, $$\mbox{det}_{*}(I)([\alpha_{ij}])=[1+t\delta_{ij}]_{1}\in T_{1}\mathbb{R}$$ Finally, $$\mbox{det}_{*}(I)(X)=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}\mbox{det}_{*}(I)([\alpha_{ij}])=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}[1+t\delta_{ij}]_{1}$$ Now, I noticed that, if I for some reason use the isomorphism $$g:T_{1}\mathbb{R}	\rightarrow	\mathbb{R} \\ [\gamma]_{1}	\mapsto	\gamma'(0)$$ to “identify” $\alpha_{ij}$   with $g(\alpha_{ij})=\delta_{ij}$   and use that instead of $[1+t\delta_{ij}]_{1}$  , I get $\overset{n}{\underset{i,j=1}{\sum}}x_{ij}\delta_{ij}=\mbox{tr}X$  . My question is: why is this last step (since ""Now, I noticed..."") legitimate?","I want to solve the following problem: Show that the derivative of $\mbox{det}:GL(n,\mathbb{R})\rightarrow\mathbb{R}$     at $I\in GL(n,\mathbb{R})$     is given by    $$\mbox{det}_{*}(I)(X)=\mbox{tr}X$$ I would like you to check my proof, and answer the question in the end. My Attempt : I'll denote $N=GL(n,\mathbb{R})$  . Also, $\simeq$   will be used for vector space isomorphisms and $\cong$   will be used for diffeomorphisms. We know that $\mbox{det}_{*}(I):T_{I}N\rightarrow T_{det(I)=1}\mathbb{R}$  . Let $X\in T_{I}N$  . We can write $X$   in a basis of $T_{I}N$  . So let us find a basis $of T_{I}N$  : we know that $T_{I}N\simeq M_{n}(\mathbb{R})$  , so we can get a basis of $T_{I}N$   from a basis of $M_{n}(\mathbb{R})$   using an isomorphism. The function $$f:T_{I}N	\rightarrow	M_{n}(\mathbb{R}) \\ [\gamma]	\mapsto	\gamma'(0)$$ is known to be an isomorphism. Furthermore, ${E_{ij}}$   is a basis for $M_{n}(\mathbb{R})$  , where $E_{ij}$   is the $n\times n$   matrix whose entries are all zero except the entry $i,j$  , which is $1$  . Thus, a basis for $T_{I}N$   is ${f^{-1}(E_{ij})}$  . Now, $f^{-1}(E_{ij})$   is the equivalence class of curves $\gamma:\mathbb{R}\rightarrow N$   such that $\gamma(0)=I$   and $\gamma'(0)=E_{ij}$  . Hence, a representative of this equivalence class is $\alpha_{ij}(t)=I+tE_{ij}$  , and so we can write ${f^{-1}(E_{ij})}={[\alpha_{ij}]}$  . Hence, we can write $X=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}[\alpha_{ij}]$  . Let us see how $det_{*}$   acts on the basis elements $[\alpha_{ij}]$  . We have $\mbox{det}_{*}(I)([\alpha_{ij}])=[\mbox{det}\circ\alpha_{ij}]_{1}$ by definition of derivative (the subscript 1   reminds us that the equivalence relation of this equivalence class is different, since it is defined on the set of all curves of the type $\gamma:\mathbb{R}\rightarrow\mathbb{R}$   such that $\gamma(0)=\mbox{det}(I)=1  ).$ Now, $\mbox{det}\circ\alpha_{ij}:\mathbb{R}\rightarrow\mathbb{R}$   is such that $$\mbox{det}\circ\alpha_{ij}=\mbox{det}(\alpha_{ij}(t))=\mbox{det}\left(I+tE_{ij}\right)=\mbox{det}\left(\left[\begin{array}{ccc} 1 &  & \mathbb{O}\\  & \ddots\\ \mathbb{O} &  & 1 \end{array}\right]+\left[\begin{array}{cccc} \mathbb{O} &  &  & \mathbb{O}\\  &  & t\,(i,j\mbox{ entry})\\ \\ \mathbb{O} &  &  & \mathbb{O} \end{array}\right]\right)$$  . The matrix is triangular (or simply diagonal), and so the determinant is the product of the diagonal elements. Hence, $\mbox{det}\circ\alpha_{ij}=1+t\delta_{ij}$  , with $\delta_{ij}$   the Kronecker delta. Hence, $$\mbox{det}_{*}(I)([\alpha_{ij}])=[1+t\delta_{ij}]_{1}\in T_{1}\mathbb{R}$$ Finally, $$\mbox{det}_{*}(I)(X)=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}\mbox{det}_{*}(I)([\alpha_{ij}])=\overset{n}{\underset{i,j=1}{\sum}}x_{ij}[1+t\delta_{ij}]_{1}$$ Now, I noticed that, if I for some reason use the isomorphism $$g:T_{1}\mathbb{R}	\rightarrow	\mathbb{R} \\ [\gamma]_{1}	\mapsto	\gamma'(0)$$ to “identify” $\alpha_{ij}$   with $g(\alpha_{ij})=\delta_{ij}$   and use that instead of $[1+t\delta_{ij}]_{1}$  , I get $\overset{n}{\underset{i,j=1}{\sum}}x_{ij}\delta_{ij}=\mbox{tr}X$  . My question is: why is this last step (since ""Now, I noticed..."") legitimate?",,"['differential-geometry', 'proof-verification', 'smooth-manifolds', 'alternative-proof']"
4,"What is the space $\text{SL}(n, \mathbb{C})/\text{SL}(n,\mathbb{R})$? [closed]",What is the space ? [closed],"\text{SL}(n, \mathbb{C})/\text{SL}(n,\mathbb{R})","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Is a description of that space other than as that quotient known? I'm interested in the case $n = 2$ in particular.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Is a description of that space other than as that quotient known? I'm interested in the case $n = 2$ in particular.",,"['differential-geometry', 'lie-groups', 'lie-algebras', 'homogeneous-spaces']"
5,Flatness of torus and surfaces of higher genus,Flatness of torus and surfaces of higher genus,,"For the very first sight it may be surprise that the ordinary torus $S^1 \times S^1$ is flat: one argument to see this is the following. One can imagine a torus as a square with opposite sides identified and the square is obviously flat. However there are surfaces with higher genus (higher numbers of ""holes"") which also can be represented as polygons (4$g$ polygon for a surface of genus $g$) with sides indetified. However higher genus surfaces are negatively curved. So my question is Why higher genus surfaces are negatively curved while they can be represented as flat polygons with sides identified?","For the very first sight it may be surprise that the ordinary torus $S^1 \times S^1$ is flat: one argument to see this is the following. One can imagine a torus as a square with opposite sides identified and the square is obviously flat. However there are surfaces with higher genus (higher numbers of ""holes"") which also can be represented as polygons (4$g$ polygon for a surface of genus $g$) with sides indetified. However higher genus surfaces are negatively curved. So my question is Why higher genus surfaces are negatively curved while they can be represented as flat polygons with sides identified?",,"['differential-geometry', 'riemannian-geometry', 'curvature']"
6,Why would one care about Fibre Bundles,Why would one care about Fibre Bundles,,"As a physics student I can easily understand the motivation for studying manifolds and why the definition looks the way it does, I only have to think of Minkowski space in GR. But for the life of me the notion of a fibre bundle seems highly unmotivated. Why would we want to talk about manifolds in this strange way? Why are we jumping through hoops and ladders to talk about a manifold locally as a product space of two things?","As a physics student I can easily understand the motivation for studying manifolds and why the definition looks the way it does, I only have to think of Minkowski space in GR. But for the life of me the notion of a fibre bundle seems highly unmotivated. Why would we want to talk about manifolds in this strange way? Why are we jumping through hoops and ladders to talk about a manifold locally as a product space of two things?",,"['differential-geometry', 'differential-topology']"
7,Lie Groups/Exponential map identity,Lie Groups/Exponential map identity,,I have come across this identity a few times and I have absolutely no idea why it holds. $g^{-1}\exp(tX)g=\exp(t(\text{ad}_{g^{-1}}X))$ Would any one be able to explain exactly why this holds or point me to a resource that would explain.? Thanks.,I have come across this identity a few times and I have absolutely no idea why it holds. $g^{-1}\exp(tX)g=\exp(t(\text{ad}_{g^{-1}}X))$ Would any one be able to explain exactly why this holds or point me to a resource that would explain.? Thanks.,,"['differential-geometry', 'manifolds', 'lie-groups', 'lie-algebras']"
8,Klein Bottle Embedding on $\mathbb{R}^4$.,Klein Bottle Embedding on .,\mathbb{R}^4,"First of all, I am aware of the question in How to embed Klein Bottle into $R^4$ , which was inconclusive. Anyway, I've made some progress, but I still have a question. I am using Do Carmo's Riemannian Geometry, and struggling to solve a problem. The problem is: Show that the mapping $G:\mathbb{R}^2\to\mathbb{R}^4$ given by $$G(x,y)=((r\cos (4\pi y)+a)\cos (4\pi x),(r\cos (4\pi y)+a)\sin (4\pi x),r\sin (4\pi y)\cos (2\pi x),r\sin (4\pi y)\sin (2\pi x)))$$ induces an embedding of the Klein bottle into $\mathbb{R}^4$ (It is a slightly different function from the one in the book, but works in the same way). First of all, it's not hard to see that  $$G(x+n,y+m)=G(x,y)\text{ whenever }m,n\in\mathbb{Z}.$$ Therefore, this mapping is well-defined over the torus $\mathbb{T}^2$. What I need now is to show that $G(-x,-y)=G(A(x,y))=G(x,y)$, where $A$ is the antipode mapping. If this were true, then the mapping G would be well-defined over the Klein Bottle, but it's obvious that this is false. Am I working wrong here somewhere?","First of all, I am aware of the question in How to embed Klein Bottle into $R^4$ , which was inconclusive. Anyway, I've made some progress, but I still have a question. I am using Do Carmo's Riemannian Geometry, and struggling to solve a problem. The problem is: Show that the mapping $G:\mathbb{R}^2\to\mathbb{R}^4$ given by $$G(x,y)=((r\cos (4\pi y)+a)\cos (4\pi x),(r\cos (4\pi y)+a)\sin (4\pi x),r\sin (4\pi y)\cos (2\pi x),r\sin (4\pi y)\sin (2\pi x)))$$ induces an embedding of the Klein bottle into $\mathbb{R}^4$ (It is a slightly different function from the one in the book, but works in the same way). First of all, it's not hard to see that  $$G(x+n,y+m)=G(x,y)\text{ whenever }m,n\in\mathbb{Z}.$$ Therefore, this mapping is well-defined over the torus $\mathbb{T}^2$. What I need now is to show that $G(-x,-y)=G(A(x,y))=G(x,y)$, where $A$ is the antipode mapping. If this were true, then the mapping G would be well-defined over the Klein Bottle, but it's obvious that this is false. Am I working wrong here somewhere?",,"['differential-geometry', 'riemannian-geometry', 'klein-bottle']"
9,"Every vector bundle over $[0,1]^n$ is trivial",Every vector bundle over  is trivial,"[0,1]^n","I would like to show the followoing result: Every vector bundle over $[0,1]^n$ is trivial First, I consider the case $n=1$, so let $E$ be a vector bundle over $[0,1]$. If $\nabla$ is a connexion for $E$, let $\tau_x : E_x \to E_0$ be the parallel transport along the path $p_x : t \mapsto (1-t)x$. Now, I want to show that the map $$ \left\{ \begin{array}{ccc} E & \to & [0,1] \times E_0 \\ (x,v) & \mapsto & (x, \tau_x(v)) \end{array} \right.$$ is an isomorphism of vector bundles. The only non-trivial point seems to be to show that the previous map is smooth, so my question is: how to show that $(x,v) \mapsto \tau_x(v)$ is smooth?","I would like to show the followoing result: Every vector bundle over $[0,1]^n$ is trivial First, I consider the case $n=1$, so let $E$ be a vector bundle over $[0,1]$. If $\nabla$ is a connexion for $E$, let $\tau_x : E_x \to E_0$ be the parallel transport along the path $p_x : t \mapsto (1-t)x$. Now, I want to show that the map $$ \left\{ \begin{array}{ccc} E & \to & [0,1] \times E_0 \\ (x,v) & \mapsto & (x, \tau_x(v)) \end{array} \right.$$ is an isomorphism of vector bundles. The only non-trivial point seems to be to show that the previous map is smooth, so my question is: how to show that $(x,v) \mapsto \tau_x(v)$ is smooth?",,"['differential-geometry', 'vector-bundles']"
10,support of a differential form on manifold,support of a differential form on manifold,,"In the book ""Differential forms in Algebraic Topology"" by Bott and Tu, the support of a differential form $\omega$ on a manifold $M$ is defined to be ""the smallest closed set $Z$ so that $\omega$ restricted to $Z$ is not $0$."" (page 24). I am a little confused, suppose we let $M = \mathbb{R}$ with the trivial atlas $\{ \mathbb{R}, \text{Id} \}$ and consider the $0-$form $\omega = x$. Then any non - zero point would constitute a set on which the restriction of $\omega$ is non - zero. But I expect the authors want the support to be the smallest closed set containing all the points at which $\omega$ is non - zero. Where is my misunderstanding ? Lots of thanks for help!","In the book ""Differential forms in Algebraic Topology"" by Bott and Tu, the support of a differential form $\omega$ on a manifold $M$ is defined to be ""the smallest closed set $Z$ so that $\omega$ restricted to $Z$ is not $0$."" (page 24). I am a little confused, suppose we let $M = \mathbb{R}$ with the trivial atlas $\{ \mathbb{R}, \text{Id} \}$ and consider the $0-$form $\omega = x$. Then any non - zero point would constitute a set on which the restriction of $\omega$ is non - zero. But I expect the authors want the support to be the smallest closed set containing all the points at which $\omega$ is non - zero. Where is my misunderstanding ? Lots of thanks for help!",,"['algebraic-topology', 'differential-geometry']"
11,"Second derivatives, Hamilton and tangent bundle of tangent bundle TTM","Second derivatives, Hamilton and tangent bundle of tangent bundle TTM",,"I'm learning the Hamilton formalism of classical mechanics, where a second order differential equation is formalized as two first order differential equations on the cotangent bundle of the configuration manifold. I find the concept of tangent spaces and the notion of the derivative $f_*: TM \to TN$ as a function between tangent spaces very elegant, natural and intuitive. I still struggle, though, with an intuitive understanding of tangent spaces of tangent spaces. Let the $n$ dimensional configuration space $M$ be a smooth manifold, $\pmb{q} \in M$ , then $TM$ is the tangent bundle and $\pmb{v} \in TM$ a tangent vector. Even without local coordinates, every tangent vector can canonically be split into a point $q$ and a vector $\dot q \in T_qM$ . Therefore $\pmb v = (q, \dot q)$ . The intuitive notion of a tangent vector is the notion of a change of position or a velocity (thus the notation) starting at a point. Now lets look at the tangent space of the tangent space $TTM$ . Let $\pmb a \in TTM$ be tangent vector to $TM$ . The intuitive notion of $\pmb a$ is a change of velocity or acceleration. Just as we could do for $TM$ , we can split $\pmb a$ into a ""point"" $(q, \dot q)$ in $TM$ and a vector in $T_{(q, \dot q)}TM$ given by $(\dot{q}, \ddot q)$ , with $\dot{q}$ denoting a change of the fiber and $\ddot{q}$ denoting a change of the vector within the same fiber. Combining with the previous, $\pmb a \in TTM$ consists of $(q, \dot{q}, \dot{q}, \ddot{q})$ . What you might disregard as a double occupancy in notation, is a real problem for my understanding. It seems like the information about the position change is duplicated, not even necessarily consistently. Which roles do the vector component $\dot{q} \in T_{\pmb q}M$ and the fiber change component $\dot{q} \in TTM$ play generally in manifolds? Which role do they play in the Hamilton formalism (if any different)? How to construct a second derivative $f_{* *}: TTM \to TTN$ ? How do these components appear there? How, if at all, does this relate to curvature and torsion of curves? How, if at all, does the exterior derivative $dd=0$ or any other relevant derivative relate to this? The Hamilton equations of motion are $\dot{\pmb q} = \frac{\partial H}{\partial \pmb p}, \dot{\pmb p} = -\frac{\partial H}{\partial \pmb q}$ (with $H: T^*M\to\mathbb R$ and $(\pmb q, \pmb p) \in T^*M$ ). How does the notational double occupancy of $\dot{\pmb q}$ resolve here? In this question I concentrated on $TTM$ while the Hamilton formalism is defined on $TT^*M$ . Is there a fundamental difference between $TT^*M$ and $TTM$ that is relevant to the problem in question?","I'm learning the Hamilton formalism of classical mechanics, where a second order differential equation is formalized as two first order differential equations on the cotangent bundle of the configuration manifold. I find the concept of tangent spaces and the notion of the derivative as a function between tangent spaces very elegant, natural and intuitive. I still struggle, though, with an intuitive understanding of tangent spaces of tangent spaces. Let the dimensional configuration space be a smooth manifold, , then is the tangent bundle and a tangent vector. Even without local coordinates, every tangent vector can canonically be split into a point and a vector . Therefore . The intuitive notion of a tangent vector is the notion of a change of position or a velocity (thus the notation) starting at a point. Now lets look at the tangent space of the tangent space . Let be tangent vector to . The intuitive notion of is a change of velocity or acceleration. Just as we could do for , we can split into a ""point"" in and a vector in given by , with denoting a change of the fiber and denoting a change of the vector within the same fiber. Combining with the previous, consists of . What you might disregard as a double occupancy in notation, is a real problem for my understanding. It seems like the information about the position change is duplicated, not even necessarily consistently. Which roles do the vector component and the fiber change component play generally in manifolds? Which role do they play in the Hamilton formalism (if any different)? How to construct a second derivative ? How do these components appear there? How, if at all, does this relate to curvature and torsion of curves? How, if at all, does the exterior derivative or any other relevant derivative relate to this? The Hamilton equations of motion are (with and ). How does the notational double occupancy of resolve here? In this question I concentrated on while the Hamilton formalism is defined on . Is there a fundamental difference between and that is relevant to the problem in question?","f_*: TM \to TN n M \pmb{q} \in M TM \pmb{v} \in TM q \dot q \in T_qM \pmb v = (q, \dot q) TTM \pmb a \in TTM TM \pmb a TM \pmb a (q, \dot q) TM T_{(q, \dot q)}TM (\dot{q}, \ddot q) \dot{q} \ddot{q} \pmb a \in TTM (q, \dot{q}, \dot{q}, \ddot{q}) \dot{q} \in T_{\pmb q}M \dot{q} \in TTM f_{* *}: TTM \to TTN dd=0 \dot{\pmb q} = \frac{\partial H}{\partial \pmb p}, \dot{\pmb p} = -\frac{\partial H}{\partial \pmb q} H: T^*M\to\mathbb R (\pmb q, \pmb p) \in T^*M \dot{\pmb q} TTM TT^*M TT^*M TTM","['differential-geometry', 'differential-topology', 'tangent-bundle', 'hamilton-equations', 'co-tangent-space']"
12,Constant-Rank Level Set Theorem Proof,Constant-Rank Level Set Theorem Proof,,"This is a proof from Lee's Introduction To Smooth Manifolds . I don't understand how the red part follows. The Rank Theorem will tell us that on $U$ , $\Phi$ has coordinate representation: $$\hat{\Phi}(x^1,\ldots,x^r,x^{r+1},\ldots,x^n) = (x^1,\ldots,x^r,0,\ldots,0)$$ I suppose something special happens in particular on $S \cap U$ which is that slice set mentioned. Why do $x^{r+1},\ldots,x^n$ switch from being $0$ to non-zero while $x^1,\ldots,x^r$ switch from being non-zero to $0$ on $S \cap U$ ?","This is a proof from Lee's Introduction To Smooth Manifolds . I don't understand how the red part follows. The Rank Theorem will tell us that on , has coordinate representation: I suppose something special happens in particular on which is that slice set mentioned. Why do switch from being to non-zero while switch from being non-zero to on ?","U \Phi \hat{\Phi}(x^1,\ldots,x^r,x^{r+1},\ldots,x^n) = (x^1,\ldots,x^r,0,\ldots,0) S \cap U x^{r+1},\ldots,x^n 0 x^1,\ldots,x^r 0 S \cap U","['differential-geometry', 'manifolds', 'smooth-manifolds']"
13,Is a tubular neighborhood always diffeomorphic to the whole normal bundle?,Is a tubular neighborhood always diffeomorphic to the whole normal bundle?,,"In Bott & Tu, a tubular neighborhood of a submanifold $S\subset M$ is defined as an open neighborhood $T$ of $S$ in $M$ such that $T$ is diffeomorphic to a vector bundle of rank $\mathrm{codim}\,S$ such that $S$ is diffeomorphic to the zero section. They then claim such tubular neighborhoods always exist and that the normal bundle $NS$ of $S$ in $M$ is the required bundle. However, the tubular neighborhood states that $T$ is diffeomorphic to a neighborhood of the zero section in $NS$, not to the whole thing. Is it possible to ""stretch"" the image of $T$ in $NS$ so that it is diffeomorphic to all of $NS$? Spivak seems to have a proof in the compact case, but how does one show this in general, using the tubular neighborhood quoted above?","In Bott & Tu, a tubular neighborhood of a submanifold $S\subset M$ is defined as an open neighborhood $T$ of $S$ in $M$ such that $T$ is diffeomorphic to a vector bundle of rank $\mathrm{codim}\,S$ such that $S$ is diffeomorphic to the zero section. They then claim such tubular neighborhoods always exist and that the normal bundle $NS$ of $S$ in $M$ is the required bundle. However, the tubular neighborhood states that $T$ is diffeomorphic to a neighborhood of the zero section in $NS$, not to the whole thing. Is it possible to ""stretch"" the image of $T$ in $NS$ so that it is diffeomorphic to all of $NS$? Spivak seems to have a proof in the compact case, but how does one show this in general, using the tubular neighborhood quoted above?",,"['differential-geometry', 'differential-topology']"
14,What is a local invariant?,What is a local invariant?,,"Let $(M,g)$ be a Riemannian manifold. Then, it is usually said that $M$ has local invariants associated to $g$. For example, the curvature of the Levi-Civita connection associated to $g$. My question is: in which sense these curvature-related objects are local invariants? In addition, it is usually said that a symplectic manifold $(M,\omega)$ has no local invariants associated to the symplectic form $\omega$. I guess this is related to the Darboux theorem, which implies that locally every symplectic form is the standard symplectic form, but, what is the precise form of this statement? Thanks.","Let $(M,g)$ be a Riemannian manifold. Then, it is usually said that $M$ has local invariants associated to $g$. For example, the curvature of the Levi-Civita connection associated to $g$. My question is: in which sense these curvature-related objects are local invariants? In addition, it is usually said that a symplectic manifold $(M,\omega)$ has no local invariants associated to the symplectic form $\omega$. I guess this is related to the Darboux theorem, which implies that locally every symplectic form is the standard symplectic form, but, what is the precise form of this statement? Thanks.",,"['differential-geometry', 'riemannian-geometry', 'symplectic-geometry']"
15,Stiefel-Whitney class of complex projective spaces,Stiefel-Whitney class of complex projective spaces,,"Let $T\mathbb{C}P^m$ be the tangent bundle of complex projective space. What is the total Stiefel-Whitney class $w(T\mathbb{C}P^m)$? Let $a_m$ be the maximal integer such that the $a_m$-th dual Stiefel-Whitney class $\bar w_{a_m}(T\mathbb{C}P^m)$  is nonzero ($\bar w=1/w$, $\bar w_j$ is the degree-$j$-component of $\bar w$). Can we write $a_m$ in terms of $m$? I do not know how to solve. I even do not know how to solve the case $m=2$...","Let $T\mathbb{C}P^m$ be the tangent bundle of complex projective space. What is the total Stiefel-Whitney class $w(T\mathbb{C}P^m)$? Let $a_m$ be the maximal integer such that the $a_m$-th dual Stiefel-Whitney class $\bar w_{a_m}(T\mathbb{C}P^m)$  is nonzero ($\bar w=1/w$, $\bar w_j$ is the degree-$j$-component of $\bar w$). Can we write $a_m$ in terms of $m$? I do not know how to solve. I even do not know how to solve the case $m=2$...",,"['algebraic-geometry', 'differential-geometry', 'algebraic-topology', 'geometric-topology', 'characteristic-classes']"
16,Why do we require that a complex manifold has the structure of a real manifold?,Why do we require that a complex manifold has the structure of a real manifold?,,"I am taking a course in complex manifolds, heavily influenced by Huybrechts' book ""Complex Geometry"", and in it we define a complex manifold $X$ to be a smooth, real manifold $M$ together with an equivalence class of holomorphic charts. As far as I can see, there is no compatibility condition on the two atlases. My question is: Why do we do this? Is this definition useful? Since holomorphic functions are smooth when considered as real functions, any complex atlas should define a real atlas, so assuming that the manifold already has a real atlas seems superfluous. I suppose this means that as a topological space the manifold is Hausdorff and second-countable, but to me it would seem nicer to just define a complex manifold as such a topological space with a holomorphic atlas. This is the approach taken in a Riemann surfaces course I have attended previously, and just after the definition in his book, Huybrechts mentions that we could use this approach to define complex manifolds. It seems then that the definition as given must have been, at least to him, somehow preferable. Moreover, I know that in the real case there are topological spaces that have several different smooth structures ($S^7$ is such an example) so if we don't assume that the complex and real atlases are compatible, I'm not sure how we could use real atlas to do anything useful since there is not in general a single choice of real atlas for each complex manifold. If we do assume compatibility, then what's the point of requiring it in the definition instead of just mentioning it as a consequence?","I am taking a course in complex manifolds, heavily influenced by Huybrechts' book ""Complex Geometry"", and in it we define a complex manifold $X$ to be a smooth, real manifold $M$ together with an equivalence class of holomorphic charts. As far as I can see, there is no compatibility condition on the two atlases. My question is: Why do we do this? Is this definition useful? Since holomorphic functions are smooth when considered as real functions, any complex atlas should define a real atlas, so assuming that the manifold already has a real atlas seems superfluous. I suppose this means that as a topological space the manifold is Hausdorff and second-countable, but to me it would seem nicer to just define a complex manifold as such a topological space with a holomorphic atlas. This is the approach taken in a Riemann surfaces course I have attended previously, and just after the definition in his book, Huybrechts mentions that we could use this approach to define complex manifolds. It seems then that the definition as given must have been, at least to him, somehow preferable. Moreover, I know that in the real case there are topological spaces that have several different smooth structures ($S^7$ is such an example) so if we don't assume that the complex and real atlases are compatible, I'm not sure how we could use real atlas to do anything useful since there is not in general a single choice of real atlas for each complex manifold. If we do assume compatibility, then what's the point of requiring it in the definition instead of just mentioning it as a consequence?",,"['differential-geometry', 'manifolds', 'complex-geometry', 'smooth-manifolds', 'complex-manifolds']"
17,What is the difference between abstract index notation and Ricci index notation?,What is the difference between abstract index notation and Ricci index notation?,,"I'm reading Straumann's GR text and he talks about the difference between abstract index notation and Ricci index notation very briefly. So I read the wiki article, but that did not help much. Say we have the Ricci tensor and two vectors. What does the expression $R_{\mu\nu}u^\mu u^\nu$ mean in the two index notations? Is contraction not the same thing as summation over an index pair (this is what I was led to believe in undergrad and not-so-mathematically-rigorous grad texts).","I'm reading Straumann's GR text and he talks about the difference between abstract index notation and Ricci index notation very briefly. So I read the wiki article, but that did not help much. Say we have the Ricci tensor and two vectors. What does the expression $R_{\mu\nu}u^\mu u^\nu$ mean in the two index notations? Is contraction not the same thing as summation over an index pair (this is what I was led to believe in undergrad and not-so-mathematically-rigorous grad texts).",,['differential-geometry']
18,Problem in proving the property of Lie bracket of vector fields,Problem in proving the property of Lie bracket of vector fields,,"Let $M$ be a Riemannian manifold, $f \in C^{\infty}(M)$, $X,Y$ vector fields on $M$. Then i have to prove $[X,f\cdot Y]=f\cdot [X,Y]+X(f)\cdot Y$. First i use the definition of Lie bracket: $[X,f\cdot Y]=X\circ (f\cdot Y)-(f\cdot Y)\circ X$. After that i am stuck, because i must somehow pull $f$ from the composition and i do not know how. Please help.","Let $M$ be a Riemannian manifold, $f \in C^{\infty}(M)$, $X,Y$ vector fields on $M$. Then i have to prove $[X,f\cdot Y]=f\cdot [X,Y]+X(f)\cdot Y$. First i use the definition of Lie bracket: $[X,f\cdot Y]=X\circ (f\cdot Y)-(f\cdot Y)\circ X$. After that i am stuck, because i must somehow pull $f$ from the composition and i do not know how. Please help.",,"['differential-geometry', 'riemannian-geometry']"
19,Show $f^*dx_i = \sum_{j=1}^l \frac{\partial f_i}{\partial y_j} dy_j = df_i$,Show,f^*dx_i = \sum_{j=1}^l \frac{\partial f_i}{\partial y_j} dy_j = df_i,"Guillemin and Pollack's Differential Topology Page 164: $U \subset \mathbb{R}^k$ and $V \subset \mathbb{R}^l$ be open subsets. Let $f: V \to U$ to smooth. Use $x_1, \dots, x_k$ for the standard coordinate functions on $\mathbb{R}^k$ and $y_1, \dots, y_l$ on $\mathbb{R}^l$ . Write $f = (f_1, \dots, f_k)$ , each $f_i$ being a smooth function on $V$ . The derivative $df_y$ at point $y \in V$ is represented by the matrix $$\frac{\partial f_i}{\partial y_j}(y),$$ and its transpose map $df_y^*$ is represented by the transpose matrix. Consequently, $$f^*dx_i  = \sum_{j=1}^l \frac{\partial f_i}{\partial y_j} dy_j = df_i.$$ My solution: For a smooth function $f$ , $df$ is linear. And $df^*$ is the adjoint of the map $df_*$ . Consider a tangent vector $Y \in T_yU$ such that $$Y = \sum_{j = 1}^l Y^j\frac{\partial}{\partial y^j}.$$ Then we have $$(f^*dx_i)(Y) = (f^*dx_i)\sum_{j = 1}^l Y^j\frac{\partial}{\partial y^j}.$$ $f^*$ is linear , $dx_i$ is linear, and the composition of linear function is linear. Hence $f^*dx_i$ is linear. So $$(f^*dx_i)\left(Y^1\frac{\partial}{\partial y^1} + \cdots + Y^l\frac{\partial}{\partial y^l}\right) = (f^*dx_i)\left(Y^1\frac{\partial}{\partial y^1}\right) + \cdots + (f^*dx_i)\left(Y^l\frac{\partial}{\partial y^l}\right).$$ By Commutativity of $Y^j$. : $$ Y^1(f^*dx_i)\left(\frac{\partial}{\partial y^1}\right) + \cdots + Y^l(f^*dx_i)\left(\frac{\partial}{\partial y^l}\right) = \sum_{j=1}^l Y^j (f^*dx_i)\left(\frac{\partial}{\partial y^j}\right).$$ Use the definition $$f^*\omega = \omega \circ f_*.$$ We have $$\sum_{j=1}^l Y^j (f^*dx_i)(\frac{\partial}{\partial y^j}) = \sum_{j=1}^l Y^j dx_i(f_*(\frac{\partial}{\partial y^j})).$$ Because $f$ maps from $V \subset \mathbb{R}^l$ to $U \subset \mathbb{R}^k$ , it can be written as $$f = (f_1, f_2, . . . f_k),$$ with each $f_i$ being a function of the $y_j \in V \subset \mathbb{R}^l$ . Consider and $g:U \to R$ sufficiently differentiable, then the vector field $f_*(\frac{\partial}{\partial y^j})$ on $U$ may be applied to $g$ : $$f_*(\frac{\partial}{\partial y^j})[g(x_1, x_2, . . . x_k)] = \frac{\partial}{\partial y^j}(g(f_1(y_1, . . . y_l), f_2(y_1, . . . y_l), . . . f_k(y_1, y_2, . . . , y_l))),$$ according to the definition $$(F_*X)(f) = X(f \circ F).$$ Hence, $$\frac{\partial}{\partial y^j}(g(f_1(y_1, . . . y_l), f_2(y_1, . . . y_l), . . . f_k(y_1, y_2, . . . , y_l))) = \frac{\partial}{\partial y^j}(g(x_1, \dots, x_k)).$$ Following Chain rule, $$\frac{\partial}{\partial y^j}(g(x_1, \dots, x_k)) = \frac{\partial g}{\partial x^1} \frac{\partial x^1}{\partial y^j} + \cdots + \frac{\partial g}{\partial x^k} \frac{\partial x^k}{\partial y^j} = \sum_{n = 1}^k \frac{\partial g}{\partial x_n} \frac{\partial f_n}{\partial y_j}.$$ That is $$f_*\left(\frac{\partial}{\partial y^j}\right)[g(x_1, x_2, . . . x_k)] = \sum_{n = 1}^k \frac{\partial g}{\partial x_n} \frac{\partial f_n}{\partial y_j}.$$ By commutativity of first-order derivative, $$\sum_{n = 1}^k \frac{\partial g}{\partial x_n} \frac{\partial f_n}{\partial y_j} = \sum_{n = 1}^k \frac{\partial f_n}{\partial y_j} \frac{\partial g}{\partial x_n} .$$ Thus we see that the vector field $f_*\left(\frac{\partial}{\partial y^j}\right)$ satisfies $$f_*\left(\frac{\partial}{\partial y^j}\right) =  \sum_{n = 1}^k \frac{\partial f_n}{\partial y_j}\frac{\partial}{\partial x_n}.$$ So $$\sum_{j = 1}^lY^jdx_i(f_*(\frac{\partial}{\partial y^j})) = \sum_{j = 1}^lY^jdx_i\left(\sum_{n = 1}^k \frac{\partial f_n}{\partial y_j}\frac{\partial}{\partial x_n}\right).$$ As before, we use the fact that $dx_i$ is linear, and first-order derivative is commutative, $$\sum_{j = 1}^lY^jdx_i\left(\sum_{n = 1}^k \frac{\partial f_n}{\partial y_j}\frac{\partial}{\partial x_n}\right)  = \sum_{j = 1}^lY^j\left(\sum_{n = 1}^k dx_i \frac{\partial}{\partial x_n}\frac{\partial f_n}{\partial y_j}\right).$$ Using $dx_i(\frac{\partial}{\partial x_n}) = \delta_{in}$ , $$\sum_{j = 1}^lY^j\left(\sum_{n = 1}^k dx_i \frac{\partial}{\partial x_n}\frac{\partial f_n}{\partial y_j}\right)  = \sum_{j = 1}^lY^j\left(\frac{\partial f_i}{\partial y_j}\right) = \sum_{j = 1}^l (dy_j Y)\left(\frac{\partial f_i}{\partial y_j}\right).$$ So, $$\sum_{j = 1}^l (dy_j Y)\left(\frac{\partial f_i}{\partial y_j}\right) =\sum_{j = 1}^l\left(\frac{\partial f_i}{\partial y_j}\right) (dy_j Y).$$ According to Show that $d\phi = \sum \frac{\partial \phi}{\partial x_i}dx_i.$ We have $$df = \sum \frac{\partial f}{\partial y_i}dy_i.$$ So $$\sum_{j = 1}^l\left(\frac{\partial f_i}{\partial y_j}\right) (dy_j Y) =df_i (Y).$$ Since this holds for any $Y \in T_yV$ , we have shown that $$f^*dx_i = \sum_{j = 1}^l \frac{\partial f_i}{\partial y_j}dy_j = df_i$$","Guillemin and Pollack's Differential Topology Page 164: and be open subsets. Let to smooth. Use for the standard coordinate functions on and on . Write , each being a smooth function on . The derivative at point is represented by the matrix and its transpose map is represented by the transpose matrix. Consequently, My solution: For a smooth function , is linear. And is the adjoint of the map . Consider a tangent vector such that Then we have is linear , is linear, and the composition of linear function is linear. Hence is linear. So By Commutativity of $Y^j$. : Use the definition We have Because maps from to , it can be written as with each being a function of the . Consider and sufficiently differentiable, then the vector field on may be applied to : according to the definition Hence, Following Chain rule, That is By commutativity of first-order derivative, Thus we see that the vector field satisfies So As before, we use the fact that is linear, and first-order derivative is commutative, Using , So, According to Show that $d\phi = \sum \frac{\partial \phi}{\partial x_i}dx_i.$ We have So Since this holds for any , we have shown that","U \subset \mathbb{R}^k V \subset \mathbb{R}^l f: V \to U x_1, \dots, x_k \mathbb{R}^k y_1, \dots, y_l \mathbb{R}^l f = (f_1, \dots, f_k) f_i V df_y y \in V \frac{\partial f_i}{\partial y_j}(y), df_y^* f^*dx_i  = \sum_{j=1}^l \frac{\partial f_i}{\partial y_j} dy_j = df_i. f df df^* df_* Y \in T_yU Y = \sum_{j = 1}^l Y^j\frac{\partial}{\partial y^j}. (f^*dx_i)(Y) = (f^*dx_i)\sum_{j = 1}^l Y^j\frac{\partial}{\partial y^j}. f^* dx_i f^*dx_i (f^*dx_i)\left(Y^1\frac{\partial}{\partial y^1} + \cdots + Y^l\frac{\partial}{\partial y^l}\right) = (f^*dx_i)\left(Y^1\frac{\partial}{\partial y^1}\right) + \cdots + (f^*dx_i)\left(Y^l\frac{\partial}{\partial y^l}\right).  Y^1(f^*dx_i)\left(\frac{\partial}{\partial y^1}\right) + \cdots + Y^l(f^*dx_i)\left(\frac{\partial}{\partial y^l}\right) = \sum_{j=1}^l Y^j (f^*dx_i)\left(\frac{\partial}{\partial y^j}\right). f^*\omega = \omega \circ f_*. \sum_{j=1}^l Y^j (f^*dx_i)(\frac{\partial}{\partial y^j}) = \sum_{j=1}^l Y^j dx_i(f_*(\frac{\partial}{\partial y^j})). f V \subset \mathbb{R}^l U \subset \mathbb{R}^k f = (f_1, f_2, . . . f_k), f_i y_j \in V \subset \mathbb{R}^l g:U \to R f_*(\frac{\partial}{\partial y^j}) U g f_*(\frac{\partial}{\partial y^j})[g(x_1, x_2, . . . x_k)] = \frac{\partial}{\partial y^j}(g(f_1(y_1, . . . y_l), f_2(y_1, . . . y_l), . . . f_k(y_1, y_2, . . . , y_l))), (F_*X)(f) = X(f \circ F). \frac{\partial}{\partial y^j}(g(f_1(y_1, . . . y_l), f_2(y_1, . . . y_l), . . . f_k(y_1, y_2, . . . , y_l))) = \frac{\partial}{\partial y^j}(g(x_1, \dots, x_k)). \frac{\partial}{\partial y^j}(g(x_1, \dots, x_k)) = \frac{\partial g}{\partial x^1} \frac{\partial x^1}{\partial y^j} + \cdots + \frac{\partial g}{\partial x^k} \frac{\partial x^k}{\partial y^j} = \sum_{n = 1}^k \frac{\partial g}{\partial x_n} \frac{\partial f_n}{\partial y_j}. f_*\left(\frac{\partial}{\partial y^j}\right)[g(x_1, x_2, . . . x_k)] = \sum_{n = 1}^k \frac{\partial g}{\partial x_n} \frac{\partial f_n}{\partial y_j}. \sum_{n = 1}^k \frac{\partial g}{\partial x_n} \frac{\partial f_n}{\partial y_j} =
\sum_{n = 1}^k \frac{\partial f_n}{\partial y_j} \frac{\partial g}{\partial x_n} . f_*\left(\frac{\partial}{\partial y^j}\right) f_*\left(\frac{\partial}{\partial y^j}\right) =  \sum_{n = 1}^k \frac{\partial f_n}{\partial y_j}\frac{\partial}{\partial x_n}. \sum_{j = 1}^lY^jdx_i(f_*(\frac{\partial}{\partial y^j})) = \sum_{j = 1}^lY^jdx_i\left(\sum_{n = 1}^k \frac{\partial f_n}{\partial y_j}\frac{\partial}{\partial x_n}\right). dx_i \sum_{j = 1}^lY^jdx_i\left(\sum_{n = 1}^k \frac{\partial f_n}{\partial y_j}\frac{\partial}{\partial x_n}\right) 
= \sum_{j = 1}^lY^j\left(\sum_{n = 1}^k dx_i \frac{\partial}{\partial x_n}\frac{\partial f_n}{\partial y_j}\right). dx_i(\frac{\partial}{\partial x_n}) = \delta_{in} \sum_{j = 1}^lY^j\left(\sum_{n = 1}^k dx_i \frac{\partial}{\partial x_n}\frac{\partial f_n}{\partial y_j}\right) 
= \sum_{j = 1}^lY^j\left(\frac{\partial f_i}{\partial y_j}\right)
= \sum_{j = 1}^l (dy_j Y)\left(\frac{\partial f_i}{\partial y_j}\right). \sum_{j = 1}^l (dy_j Y)\left(\frac{\partial f_i}{\partial y_j}\right)
=\sum_{j = 1}^l\left(\frac{\partial f_i}{\partial y_j}\right) (dy_j Y). df = \sum \frac{\partial f}{\partial y_i}dy_i. \sum_{j = 1}^l\left(\frac{\partial f_i}{\partial y_j}\right) (dy_j Y)
=df_i (Y). Y \in T_yV f^*dx_i = \sum_{j = 1}^l \frac{\partial f_i}{\partial y_j}dy_j = df_i","['differential-geometry', 'solution-verification', 'differential-topology']"
20,Difference between tangent space and tangent plane,Difference between tangent space and tangent plane,,"I’ve avoided doing any manifold (regretting it somewhat) courses, however do have some understanding. Let $p$ be a point on a surface $S:U\to \Bbb{R}^3$, we define: The tangent space to $S$ at $p$, $T_p(S)=\{k\in\Bbb{R}^3\mid\exists\textrm{ a curve }\gamma:(-ε,ε)\to S\textrm{ with }\gamma(0)=p,\gamma'(0)=k\}$. The tangent plane to $S$ at $p$ as the plane $p+T_p(S)\subseteq\Bbb{R}^3$. My current understanding is, in the diagram below the tangent plane is the plane shown, whilst the tangent space would be p minus each element of the plane, hence the corresponding plane passing through the origin. Is this correct or is it incorrect? I’m doing a course called geometry of curves and surfaces and being unsure about this is making understanding later topics difficult. Edit - can't post images, here's a link instead! http://standards.sedris.org/18026/text/ISOIEC_18026E_SRF/image022.jpg Thanks!","I’ve avoided doing any manifold (regretting it somewhat) courses, however do have some understanding. Let $p$ be a point on a surface $S:U\to \Bbb{R}^3$, we define: The tangent space to $S$ at $p$, $T_p(S)=\{k\in\Bbb{R}^3\mid\exists\textrm{ a curve }\gamma:(-ε,ε)\to S\textrm{ with }\gamma(0)=p,\gamma'(0)=k\}$. The tangent plane to $S$ at $p$ as the plane $p+T_p(S)\subseteq\Bbb{R}^3$. My current understanding is, in the diagram below the tangent plane is the plane shown, whilst the tangent space would be p minus each element of the plane, hence the corresponding plane passing through the origin. Is this correct or is it incorrect? I’m doing a course called geometry of curves and surfaces and being unsure about this is making understanding later topics difficult. Edit - can't post images, here's a link instead! http://standards.sedris.org/18026/text/ISOIEC_18026E_SRF/image022.jpg Thanks!",,"['differential-geometry', 'surfaces']"
21,Image of Homomorphism of Lie groups,Image of Homomorphism of Lie groups,,"This is exercise from Lee: Introduction to smooth manifolds. Suppose $f \colon G \to H$ is homomorphism of Lie groups (real, finite-dimensional). Q: Is image $Im(f) \subseteq H$ a Lie subgroup of H? That is, is there topology and smooth structure on $Im(f)$ such that inclusion $Im(f) \hookrightarrow H$ is immersion, and such that induced operation $Im(f) \times Im(f) \hookrightarrow H \times H \to Im(f) \subseteq H$ is smooth. The author of these notes says (last paragraph on the first page) that the answer is no, provides counterexample (dense line on torus), but the proof is ommited. Is it correct? I need positive proof of that, to understand the following characterisation: Lie group admits faithfull finite-dimensional representation if and only if it is (isomorphic to) Lie subgroup of $GL(n,\mathbb R)$. One more thing. It is easy to see that $Ker(f)$ is Lie subgroup of $G$ using nontrivial Closed subgroup theorem . Is there more direct proof? Any help is very appreciated.","This is exercise from Lee: Introduction to smooth manifolds. Suppose $f \colon G \to H$ is homomorphism of Lie groups (real, finite-dimensional). Q: Is image $Im(f) \subseteq H$ a Lie subgroup of H? That is, is there topology and smooth structure on $Im(f)$ such that inclusion $Im(f) \hookrightarrow H$ is immersion, and such that induced operation $Im(f) \times Im(f) \hookrightarrow H \times H \to Im(f) \subseteq H$ is smooth. The author of these notes says (last paragraph on the first page) that the answer is no, provides counterexample (dense line on torus), but the proof is ommited. Is it correct? I need positive proof of that, to understand the following characterisation: Lie group admits faithfull finite-dimensional representation if and only if it is (isomorphic to) Lie subgroup of $GL(n,\mathbb R)$. One more thing. It is easy to see that $Ker(f)$ is Lie subgroup of $G$ using nontrivial Closed subgroup theorem . Is there more direct proof? Any help is very appreciated.",,"['differential-geometry', 'manifolds', 'lie-groups']"
22,Understanding the Definition of a Differential Form of Degree $k$,Understanding the Definition of a Differential Form of Degree,k,"Let $M$ be a smooth manifold. A differential form of degree $k$ is a smooth section of the $k$th   exterior power of the cotangent bundle of $M$. Does it  mean that a differential form of degree $k$ is a mapping $: M     \rightarrow   \wedge^k(T^*M) $? If I am correct, ""the $k$th exterior power of"" should be followed by a vector space. I was wondering if a ""cotangent bundle"" $T^*M$ of a differentiable manifold $M$ is a vector space? The fiber is a vector space, but I am not sure if the total space, as the union of a family of vector spaces indexed by points in $M$, also is. At a point of $M$, how does the definition of a $k$-form above lead to an alternating multilinear map, i.e., how does an element of$\wedge^k(T^*M)$ become an alternating multilinear $T_p M\times \cdots \times T_p M \to \mathbb{R}$, as stated in the following? At any point $p∈M$, a $k$-form $β$ defines an alternating   multilinear   map $\beta_p\colon T_p M\times \cdots \times T_p M \to \mathbb{R}$ (with $k$ factors of $T_pM$ in the product), where $T_pM$ is the   tangent   space to $M$ at $p$. Equivalently, $β$ is a totally antisymmetric   covariant tensor field of rank $k$. Quotes are from Wikipedia . Thanks and regards!","Let $M$ be a smooth manifold. A differential form of degree $k$ is a smooth section of the $k$th   exterior power of the cotangent bundle of $M$. Does it  mean that a differential form of degree $k$ is a mapping $: M     \rightarrow   \wedge^k(T^*M) $? If I am correct, ""the $k$th exterior power of"" should be followed by a vector space. I was wondering if a ""cotangent bundle"" $T^*M$ of a differentiable manifold $M$ is a vector space? The fiber is a vector space, but I am not sure if the total space, as the union of a family of vector spaces indexed by points in $M$, also is. At a point of $M$, how does the definition of a $k$-form above lead to an alternating multilinear map, i.e., how does an element of$\wedge^k(T^*M)$ become an alternating multilinear $T_p M\times \cdots \times T_p M \to \mathbb{R}$, as stated in the following? At any point $p∈M$, a $k$-form $β$ defines an alternating   multilinear   map $\beta_p\colon T_p M\times \cdots \times T_p M \to \mathbb{R}$ (with $k$ factors of $T_pM$ in the product), where $T_pM$ is the   tangent   space to $M$ at $p$. Equivalently, $β$ is a totally antisymmetric   covariant tensor field of rank $k$. Quotes are from Wikipedia . Thanks and regards!",,"['differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds', 'vector-bundles']"
23,Invariant proof of the Contracted Bianchi Identity,Invariant proof of the Contracted Bianchi Identity,,"In ""Riemannian Manifolds: An Introduction to Curvature,"" John Lee states the following lemma: Lemma 7.7 (Contracted Bianchi Identity): The covariant derivatives of the Ricci and scalar curvatures satisfy $$\text{div} Rc = \frac{1}{2}\nabla S,$$ where $\text{div} Rc$ is the 1-tensor obtained from $\nabla Rc$ by raising one index and contracting.  In components, this is $$R_{ij};^j = \frac{1}{2}S_{;i}.$$ Lee then proves the coordinate form of the statement.  He does this by (metric) contracting the differential Bianchi identity in coordinates $$R_{ijkl;m} + R_{ijlm;k} + R_{ijmk;l} = 0.$$ I have two questions: Is there a more coordinate-free proof of this fact?  I suppose one can argue (in words) that contractions are coordinate-invariant and such, but I would prefer seeing a proof in symbols nevertheless. Can we prove the identity directly from the symmetry of the Ricci tensor? My second question was inspired by the following computation: Evaluating the left-hand side at a vector field $X$: $$(\text{div}Rc)(X) = (\text{tr}_g\nabla Rc)(X),$$ while similarly on the right-hand side: $$\frac{1}{2}(\nabla S)(X) = \frac{1}{2}\nabla_XS = \frac{1}{2}\nabla_X(\text{tr}_gRc) = \frac{1}{2}\text{tr}_g(\nabla_XRc).$$ So, we can prove the Contracted Bianchi Identity if we can show that $$(\text{tr}_g\nabla Rc)(X) = \frac{1}{2}\text{tr}_g(\nabla_XRc),$$ which might somehow follow from the symmetry of $Rc$. This question is in some sense related to a previous question of mine, in which I ask for a means of computing traces/contractions explicitly.","In ""Riemannian Manifolds: An Introduction to Curvature,"" John Lee states the following lemma: Lemma 7.7 (Contracted Bianchi Identity): The covariant derivatives of the Ricci and scalar curvatures satisfy $$\text{div} Rc = \frac{1}{2}\nabla S,$$ where $\text{div} Rc$ is the 1-tensor obtained from $\nabla Rc$ by raising one index and contracting.  In components, this is $$R_{ij};^j = \frac{1}{2}S_{;i}.$$ Lee then proves the coordinate form of the statement.  He does this by (metric) contracting the differential Bianchi identity in coordinates $$R_{ijkl;m} + R_{ijlm;k} + R_{ijmk;l} = 0.$$ I have two questions: Is there a more coordinate-free proof of this fact?  I suppose one can argue (in words) that contractions are coordinate-invariant and such, but I would prefer seeing a proof in symbols nevertheless. Can we prove the identity directly from the symmetry of the Ricci tensor? My second question was inspired by the following computation: Evaluating the left-hand side at a vector field $X$: $$(\text{div}Rc)(X) = (\text{tr}_g\nabla Rc)(X),$$ while similarly on the right-hand side: $$\frac{1}{2}(\nabla S)(X) = \frac{1}{2}\nabla_XS = \frac{1}{2}\nabla_X(\text{tr}_gRc) = \frac{1}{2}\text{tr}_g(\nabla_XRc).$$ So, we can prove the Contracted Bianchi Identity if we can show that $$(\text{tr}_g\nabla Rc)(X) = \frac{1}{2}\text{tr}_g(\nabla_XRc),$$ which might somehow follow from the symmetry of $Rc$. This question is in some sense related to a previous question of mine, in which I ask for a means of computing traces/contractions explicitly.",,"['differential-geometry', 'tensors', 'curvature']"
24,Two definitions of linearly independent vector fields,Two definitions of linearly independent vector fields,,"I'm studying the chapter of Lee's ISM on vector fields, and there's one thing that confuses me a lot. If $\mathfrak{X}(M)$ denotes the set of smooth vector fields on a smooth $n$ -manifold, then it is a vector space under pointwise vector addition and scalar multiplication: $$(aX+bY)_p=aX_p+bY_p,\quad X,Y\in\mathfrak{X}(M).$$ Since $\mathfrak{X}(M)$ is a vector space, we can introduce the notion of linearly independence: $$a_1 X_1+...+a_k X_k=0\Rightarrow a_1=...=a_k=0.$$ This is common material that can be found in any Linear Algebra course and is familiar to me. But in a while Professor Lee brought up an idea that seems like the one presented above. He said that an ordered $k$ -tuple $(X_1,...,X_k)$ of vector fields defined on a set $A\subseteq M$ is called linearly independent if $(X_1|_p,...,X_k|_p)$ is linearly independent in $T_p M$ for each $p\in A$ . Are these two ideas talking about the same thing? More precisely, are theses two statements equivalent if we consider $A$ to be all of $M$ ? Thank you.","I'm studying the chapter of Lee's ISM on vector fields, and there's one thing that confuses me a lot. If denotes the set of smooth vector fields on a smooth -manifold, then it is a vector space under pointwise vector addition and scalar multiplication: Since is a vector space, we can introduce the notion of linearly independence: This is common material that can be found in any Linear Algebra course and is familiar to me. But in a while Professor Lee brought up an idea that seems like the one presented above. He said that an ordered -tuple of vector fields defined on a set is called linearly independent if is linearly independent in for each . Are these two ideas talking about the same thing? More precisely, are theses two statements equivalent if we consider to be all of ? Thank you.","\mathfrak{X}(M) n (aX+bY)_p=aX_p+bY_p,\quad X,Y\in\mathfrak{X}(M). \mathfrak{X}(M) a_1 X_1+...+a_k X_k=0\Rightarrow a_1=...=a_k=0. k (X_1,...,X_k) A\subseteq M (X_1|_p,...,X_k|_p) T_p M p\in A A M","['differential-geometry', 'smooth-manifolds', 'vector-fields']"
25,Exterior Derivative of One-Form vs Torsion of Connection,Exterior Derivative of One-Form vs Torsion of Connection,,"Let $\omega$ be a $1$ -form. Then $d\omega$ may be defined by the formula $$ d\omega(X,Y) = \frac{\partial}{\partial X}\iota_Y\omega - \frac{\partial}{\partial Y}\iota_X\omega-\omega([X,Y]) $$ where $X,Y$ are vector fields. This formula bears a resemblance to the formula for the torsion of a connection $\nabla$ : $$ \nabla_XY - \nabla_YX - [X,Y] $$ Is there a geometric explanation for this resemblance?",Let be a -form. Then may be defined by the formula where are vector fields. This formula bears a resemblance to the formula for the torsion of a connection : Is there a geometric explanation for this resemblance?,"\omega 1 d\omega 
d\omega(X,Y) = \frac{\partial}{\partial X}\iota_Y\omega - \frac{\partial}{\partial Y}\iota_X\omega-\omega([X,Y])
 X,Y \nabla 
\nabla_XY - \nabla_YX - [X,Y]
","['differential-geometry', 'differential-topology', 'differential-forms', 'connections']"
26,Complex normal coordinates in Kähler manifolds,Complex normal coordinates in Kähler manifolds,,"Let $(M, g, J, \omega)$ be a Kähler manifold. That is, $(M, J)$ is a complex manifold, $g$ is a Hermitian metric on $M$ and $$\omega (X, Y) = g(JX, Y)$$ is a closed two form. As a Riemannian manifold $(M, g)$ , for each $x\in M$ , one can find a geodesic normal coordinates around each $x$ . In the case of Kähler metric, one actually have more: Proposition: (Complex Normal Coordinates on Kähler manifolds) For each $x\in M$ , there is a local holomorphic coordinates around $x$ so that the metric $g = g_{i\bar j}$ satisfies $$g_{i\bar j}(x) = \delta_{ij},  \ \ d g_{i\bar j} (x) = 0, \ \ \ \frac{\partial^2 g_{i\bar j}}{\partial z_k \partial z_l} (x) = 0.$$ While the first two conditions are similar to what we have for geodesic normal coordinates in Riemannian geometry, there is no corresponding analogue for the last condition. Also, even in a Kähler manifold, the geodesic normal coordinates might not be holomorphic. I am looking for a proof of this proposition.","Let be a Kähler manifold. That is, is a complex manifold, is a Hermitian metric on and is a closed two form. As a Riemannian manifold , for each , one can find a geodesic normal coordinates around each . In the case of Kähler metric, one actually have more: Proposition: (Complex Normal Coordinates on Kähler manifolds) For each , there is a local holomorphic coordinates around so that the metric satisfies While the first two conditions are similar to what we have for geodesic normal coordinates in Riemannian geometry, there is no corresponding analogue for the last condition. Also, even in a Kähler manifold, the geodesic normal coordinates might not be holomorphic. I am looking for a proof of this proposition.","(M, g, J, \omega) (M, J) g M \omega (X, Y) = g(JX, Y) (M, g) x\in M x x\in M x g = g_{i\bar j} g_{i\bar j}(x) = \delta_{ij},  \ \ d g_{i\bar j} (x) = 0, \ \ \ \frac{\partial^2 g_{i\bar j}}{\partial z_k \partial z_l} (x) = 0.","['differential-geometry', 'complex-geometry', 'kahler-manifolds']"
27,"""Pedantic"" derivation of geodesic equation using pullback bundles","""Pedantic"" derivation of geodesic equation using pullback bundles",,"I'm trying to get more comfortable with manipulations involving connections and vector fields so I've tried to derive the geodesic equations without having to resort to any familiarities using standard calculus, everything computed ""properly"" from the definitions. For a Reimannian manifold $(M,g)$ I have a curve $\gamma : \mathbb{R} \rightarrow M$ which in local coordinates can be written as $\gamma(t) = \left(x^1(t), \ldots, x^n(t)\right)$ . If I wish $\gamma(t)$ to be a geodesic then I want its tangent vector to be auto-parallel. The tangent vector is given by the pushforward of coordinate vector field on $\mathbb{R}$ $``\hspace{01mm}\dot{\gamma}(t)\hspace{-0.5mm}"" := \gamma_*\left(\frac{\partial}{\partial t}\right) = \dot{x}^i(t) \frac{\partial}{\partial x^i}$ I want $\nabla_\dot\gamma \dot{\gamma} = 0$ , but this expression is misleading since the vector field $\dot{\gamma}(t)$ only exists along the image of $\gamma(t)$ , but we can consider the pullback of $M$ by $\gamma$ and take the connection and vector bundle with us. If $\nabla$ is the Levi-Civita connection on $(M,g)$ denote $\widetilde{\nabla}$ as its pullback connection by $\gamma$ Then $\gamma$ is geodesic if $\widetilde{\nabla}_\frac{\partial}{\partial t}\gamma_*\left(\frac{\partial}{\partial t}\right) = 0$ . At this point I start to get stuck, I have the following definition from a worked exam question that inspired me to do this exercise: I'm not quite sure if I'm in the lucky situation where $\gamma_*\left(\frac{\partial}{\partial t}\right)$ is already of the form $v \circ u$ , and I'm not so sure what this even means, if my bundle is the tangent bundle of $M$ , then my sections $e_i = \frac{\partial}{\partial x^i}$ are vector fields, how does one compose a vector field with a map? I think this has something to do with where we are evaluating $\gamma_*\left(\frac{\partial}{\partial t}\right)f = \left.\dot{x}^i(t) \frac{\partial f}{\partial x^i}\right|_{\gamma(t)}$ , that is, $e_i = \left.\frac{\partial}{\partial x^i}\right|_p$ for $p \in M$ whereas $e_i \circ \gamma = \left.\frac{\partial}{\partial x^i}\right|_{\gamma(t)}$ . I'm not sure how to properly justify this but it certainly feels more correct that $\gamma_*\left(\frac{\partial}{\partial t}\right)$ should be ""evaluating"" on $\gamma(t)$ rather than any old $p$ since the whole point of this pullback stuff was to differentiate along the curve. If we accept the above handwaving then my calculation is as follows: $$\widetilde{\nabla}_\frac{\partial}{\partial t}\gamma_*\left(\frac{\partial}{\partial t}\right) := \nabla_{\gamma_*\left(\frac{\partial}{\partial t}\right)}\gamma_*\left(\frac{\partial}{\partial t}\right)  = \nabla_{\dot{x}^i(t) \frac{\partial}{\partial x^i}}\dot{x}^j(t) \frac{\partial}{\partial x^j}$$ Using $C^\infty(M)$ linearity of a connection in the lower argument and the Liebnitz rule gives $$ = \dot{x}^i(t) \nabla_{\frac{\partial}{\partial x^i}}\dot{x}^j(t) \frac{\partial}{\partial x^j} = \dot{x}^i(t)\frac{\partial}{\partial x^i}\left(\dot{x}^j\right)\frac{\partial}{\partial x^j} +  \dot{x}^i(t)\dot{x}^j(t)\nabla_{\frac{\partial}{\partial x^i}}\frac{\partial}{\partial x^j}$$ The second term is $\dot{x}^i(t)\dot{x}^j(t)\Gamma_{ij}^k\frac{\partial}{\partial x^k}$ which starts to look on the right tracks, but I have no idea what to do to the first term to get a second time derivative, and if my approach is even correct. Apologies for the wall of equations, but I wanted to get down all my thoughts and where my confusions lie, I am looking for how to finish the derivation and an explanation of all this stuff with pullback bundles and correct any misunderstandings I have. Thanks in advance.","I'm trying to get more comfortable with manipulations involving connections and vector fields so I've tried to derive the geodesic equations without having to resort to any familiarities using standard calculus, everything computed ""properly"" from the definitions. For a Reimannian manifold I have a curve which in local coordinates can be written as . If I wish to be a geodesic then I want its tangent vector to be auto-parallel. The tangent vector is given by the pushforward of coordinate vector field on I want , but this expression is misleading since the vector field only exists along the image of , but we can consider the pullback of by and take the connection and vector bundle with us. If is the Levi-Civita connection on denote as its pullback connection by Then is geodesic if . At this point I start to get stuck, I have the following definition from a worked exam question that inspired me to do this exercise: I'm not quite sure if I'm in the lucky situation where is already of the form , and I'm not so sure what this even means, if my bundle is the tangent bundle of , then my sections are vector fields, how does one compose a vector field with a map? I think this has something to do with where we are evaluating , that is, for whereas . I'm not sure how to properly justify this but it certainly feels more correct that should be ""evaluating"" on rather than any old since the whole point of this pullback stuff was to differentiate along the curve. If we accept the above handwaving then my calculation is as follows: Using linearity of a connection in the lower argument and the Liebnitz rule gives The second term is which starts to look on the right tracks, but I have no idea what to do to the first term to get a second time derivative, and if my approach is even correct. Apologies for the wall of equations, but I wanted to get down all my thoughts and where my confusions lie, I am looking for how to finish the derivation and an explanation of all this stuff with pullback bundles and correct any misunderstandings I have. Thanks in advance.","(M,g) \gamma : \mathbb{R} \rightarrow M \gamma(t) = \left(x^1(t), \ldots, x^n(t)\right) \gamma(t) \mathbb{R} ``\hspace{01mm}\dot{\gamma}(t)\hspace{-0.5mm}"" := \gamma_*\left(\frac{\partial}{\partial t}\right) = \dot{x}^i(t) \frac{\partial}{\partial x^i} \nabla_\dot\gamma \dot{\gamma} = 0 \dot{\gamma}(t) \gamma(t) M \gamma \nabla (M,g) \widetilde{\nabla} \gamma \gamma \widetilde{\nabla}_\frac{\partial}{\partial t}\gamma_*\left(\frac{\partial}{\partial t}\right) = 0 \gamma_*\left(\frac{\partial}{\partial t}\right) v \circ u M e_i = \frac{\partial}{\partial x^i} \gamma_*\left(\frac{\partial}{\partial t}\right)f = \left.\dot{x}^i(t) \frac{\partial f}{\partial x^i}\right|_{\gamma(t)} e_i = \left.\frac{\partial}{\partial x^i}\right|_p p \in M e_i \circ \gamma = \left.\frac{\partial}{\partial x^i}\right|_{\gamma(t)} \gamma_*\left(\frac{\partial}{\partial t}\right) \gamma(t) p \widetilde{\nabla}_\frac{\partial}{\partial t}\gamma_*\left(\frac{\partial}{\partial t}\right) := \nabla_{\gamma_*\left(\frac{\partial}{\partial t}\right)}\gamma_*\left(\frac{\partial}{\partial t}\right) 
= \nabla_{\dot{x}^i(t) \frac{\partial}{\partial x^i}}\dot{x}^j(t) \frac{\partial}{\partial x^j} C^\infty(M)  = \dot{x}^i(t) \nabla_{\frac{\partial}{\partial x^i}}\dot{x}^j(t) \frac{\partial}{\partial x^j} = \dot{x}^i(t)\frac{\partial}{\partial x^i}\left(\dot{x}^j\right)\frac{\partial}{\partial x^j} +  \dot{x}^i(t)\dot{x}^j(t)\nabla_{\frac{\partial}{\partial x^i}}\frac{\partial}{\partial x^j} \dot{x}^i(t)\dot{x}^j(t)\Gamma_{ij}^k\frac{\partial}{\partial x^k}","['differential-geometry', 'riemannian-geometry', 'vector-bundles', 'geodesic', 'connections']"
28,Introductory material for jets and jet bundles,Introductory material for jets and jet bundles,,"A student of mine would like to learn more about jets and jet bundles, and more in general about how to treat derivatives and differential equations in an invariant way. She's also interested in the reformulation of some of the basic concepts of differential geometry from that point of view. Are there any books, papers, or notes that follow such an approach, and which are suitable for a first approach to the subject (there is motivation, etc)? She knows more than basic differential geometry, just not jet bundles yet. By the way, she studies physics, so if there are examples taken from physics, even better.","A student of mine would like to learn more about jets and jet bundles, and more in general about how to treat derivatives and differential equations in an invariant way. She's also interested in the reformulation of some of the basic concepts of differential geometry from that point of view. Are there any books, papers, or notes that follow such an approach, and which are suitable for a first approach to the subject (there is motivation, etc)? She knows more than basic differential geometry, just not jet bundles yet. By the way, she studies physics, so if there are examples taken from physics, even better.",,"['differential-geometry', 'reference-request', 'differential-topology', 'smooth-manifolds', 'jet-bundles']"
29,Frenet-Serret and Vector Fields,Frenet-Serret and Vector Fields,,"The well-known Frenet-Serret equations, $\dot T(s) = \kappa N(s), \tag 1$ $\dot N(s) = -\kappa(s) T(s) + \tau(s) B(s), \tag 2$ $\dot B(s) = -\tau(s) N(s), \tag 3$ where $T = \dot \alpha(s), \tag 4$ $\alpha(s)$ being a unit speed curve in $\Bbb R^3$ with arc-length $s$ , are most often applied to discover and describe properties of such space curves . Given an open set $U \subset \Bbb R^3, \tag 5$ and a vector field $X \in C^\infty(U, \Bbb R^3) \tag 6$ on $U$ , we may of course consider the flow $\phi_X(x, t)$ of $X$ ; the reader will recall it is defined as, more or less, the entire family of integral curves of the vector field $X$ in the sense that $\phi_X(x, 0) = x, \; \forall x \in U, \tag 7$ and $\dfrac{d}{dt}\phi(x, t) = X(\phi(x, t)). \tag 8$ A somewhat natural area of inquiry based upon these two concepts, the Frenet-Serret apparatus and vector fields and their flows, is the relationship 'twixt the Frenet-Serret formulas and the integral curves of $X$ ; that is, finding the expressions for $T(s)$ , $N(s)$ , $B(s)$ , $\kappa(s)$ and $\tau(s)$ in terms of $X$ and related quantities such as its magnitude $\vert X \vert = \langle X, X \rangle^{1/2}$ and its derivatives $\nabla X$ etc. The Question then becomes: Given a (sufficiently smooth) vector field $X$ on an open set $U \subset \Bbb R^3$ , find the vector fields $T(s)$ , $N(s)$ , and $B(s)$ and the scalar quantities $\kappa(s)$ and $\tau(s)$ associated with the integral curves of $X$ , expressed in terms of $X$ and it's associated quantities such as $\vert X \vert$ and so forth. A Few Observations: Given such an open set $U$ and vector field $X$ , of course it is true that the flow $\phi_X(x, t)$ may not exist for all values of $t$ , but this is of no consequence here since all calculations are local in nature.  Indeed, for all $x \in U$ the flow $\phi_X(x, t)$ is defined for sufficiently small values of $t$ , and this is sufficient for the present purposes. A Useful Starting Point may be the observation that $X/\vert X \vert$ is a unit vector field , and that in fact $T(s) = \dfrac{X(\alpha(s))}{\vert X(\alpha(s))\vert} \tag 9$ along the arc-length parametrized integral curve $\alpha(s)$ of $X/ \vert X \vert$ .  Of course, we may also adopt and employ the given parametrization of the integral curves of $X$ by $t$ , as in (7), (8); in fact we have $\dfrac{ds}{dt} = \vert X(\alpha(t)) \vert, \; \dfrac{dt}{ds} = \vert X(\alpha(s)) \vert^{-1}, \tag{10}$ which allow the conversion 'twixt $t$ and $s$ via integration: $s - s_0 = \displaystyle \int_{t_0}^t  \vert X(\alpha(u)) \vert \; du, \; t - t_0 = \displaystyle \int_{s_0}^s  \vert X(\alpha(u)) \vert^{-1} \; du. \tag{11}$ We can also express the unit tangent vector $T$ in terms of the parameter $t$ : $T(t) = \dfrac{X(\alpha(t))}{\vert X(\alpha(t))\vert}. \tag{12}$ In these formulas the reader will recognize that $\alpha(t)$ and $\alpha(s)$ represent the same curves in the geometrical sense, that is, he same paths in $\Bbb R^3$ , though they are differently parametrized.","The well-known Frenet-Serret equations, where being a unit speed curve in with arc-length , are most often applied to discover and describe properties of such space curves . Given an open set and a vector field on , we may of course consider the flow of ; the reader will recall it is defined as, more or less, the entire family of integral curves of the vector field in the sense that and A somewhat natural area of inquiry based upon these two concepts, the Frenet-Serret apparatus and vector fields and their flows, is the relationship 'twixt the Frenet-Serret formulas and the integral curves of ; that is, finding the expressions for , , , and in terms of and related quantities such as its magnitude and its derivatives etc. The Question then becomes: Given a (sufficiently smooth) vector field on an open set , find the vector fields , , and and the scalar quantities and associated with the integral curves of , expressed in terms of and it's associated quantities such as and so forth. A Few Observations: Given such an open set and vector field , of course it is true that the flow may not exist for all values of , but this is of no consequence here since all calculations are local in nature.  Indeed, for all the flow is defined for sufficiently small values of , and this is sufficient for the present purposes. A Useful Starting Point may be the observation that is a unit vector field , and that in fact along the arc-length parametrized integral curve of .  Of course, we may also adopt and employ the given parametrization of the integral curves of by , as in (7), (8); in fact we have which allow the conversion 'twixt and via integration: We can also express the unit tangent vector in terms of the parameter : In these formulas the reader will recognize that and represent the same curves in the geometrical sense, that is, he same paths in , though they are differently parametrized.","\dot T(s) = \kappa N(s), \tag 1 \dot N(s) = -\kappa(s) T(s) + \tau(s) B(s), \tag 2 \dot B(s) = -\tau(s) N(s), \tag 3 T = \dot \alpha(s), \tag 4 \alpha(s) \Bbb R^3 s U \subset \Bbb R^3, \tag 5 X \in C^\infty(U, \Bbb R^3) \tag 6 U \phi_X(x, t) X X \phi_X(x, 0) = x, \; \forall x \in U, \tag 7 \dfrac{d}{dt}\phi(x, t) = X(\phi(x, t)). \tag 8 X T(s) N(s) B(s) \kappa(s) \tau(s) X \vert X \vert = \langle X, X \rangle^{1/2} \nabla X X U \subset \Bbb R^3 T(s) N(s) B(s) \kappa(s) \tau(s) X X \vert X \vert U X \phi_X(x, t) t x \in U \phi_X(x, t) t X/\vert X \vert T(s) = \dfrac{X(\alpha(s))}{\vert X(\alpha(s))\vert} \tag 9 \alpha(s) X/ \vert X \vert X t \dfrac{ds}{dt} = \vert X(\alpha(t)) \vert, \; \dfrac{dt}{ds} = \vert X(\alpha(s)) \vert^{-1}, \tag{10} t s s - s_0 = \displaystyle \int_{t_0}^t  \vert X(\alpha(u)) \vert \; du, \; t - t_0 = \displaystyle \int_{s_0}^s  \vert X(\alpha(u)) \vert^{-1} \; du. \tag{11} T t T(t) = \dfrac{X(\alpha(t))}{\vert X(\alpha(t))\vert}. \tag{12} \alpha(t) \alpha(s) \Bbb R^3","['differential-geometry', 'vector-fields', 'frenet-frame']"
30,Basis for the Tangent space and derivations at a point,Basis for the Tangent space and derivations at a point,,"So i have been reading Tu's , ""An Introduction to Manifolds"" , and i have a question that is really bugging my mind. So we have the tangent space at a point $p$ , $T_p(\mathbb{R}^n)$ with basis $e_1,...,e_n$ is going to be isomorphic to the vector space of derivations at a point $p$ , $D_p$ . To do this the author creates the said isomorphism and proves it is in fact one. My question is the following, in this isomorphism basically is sending every tangent vector $v$ of the tangent to the directional derivative of the said vector.So far so good, my problem is that later he writes any tangent vector as a linear combination of partial derivatives and then latter when talking about differential forms and when we make computations in the proofs he uses this partial derivatives notation, so is this ok because the spaces are isomorphic ??Because they are not tangent vectors , they can just be identified as one, and can the partial derivatives in the sum really be interpreted as such?? Thanks.","So i have been reading Tu's , ""An Introduction to Manifolds"" , and i have a question that is really bugging my mind. So we have the tangent space at a point , with basis is going to be isomorphic to the vector space of derivations at a point , . To do this the author creates the said isomorphism and proves it is in fact one. My question is the following, in this isomorphism basically is sending every tangent vector of the tangent to the directional derivative of the said vector.So far so good, my problem is that later he writes any tangent vector as a linear combination of partial derivatives and then latter when talking about differential forms and when we make computations in the proofs he uses this partial derivatives notation, so is this ok because the spaces are isomorphic ??Because they are not tangent vectors , they can just be identified as one, and can the partial derivatives in the sum really be interpreted as such?? Thanks.","p T_p(\mathbb{R}^n) e_1,...,e_n p D_p v","['differential-geometry', 'tangent-spaces']"
31,Vector Bundle Transition Functions as Cech Cocycles,Vector Bundle Transition Functions as Cech Cocycles,,"I am trying to understand the fact that vector bundles of rank $r$ over a space $X$ are classified by the Cech cohomology group $\check{H}^{1}\big(X, GL_{r}(\mathcal{O}_{X})\big)$ .  I believe this should work in any of the usual categories, so I wont specify smooth, holomorphic, etc.  I understand broadly how this goes, but there are a few key details tripping me up. So if we have a Cech 1-cocycle $g = \{g_{\alpha \beta}\} \in \check{H}^{1}\big(X, GL_{r}(\mathcal{O}_{X})\big)$ with respect to some open cover $\{U_{\alpha}\}$ , then we know: $$(dg)_{\alpha \beta \gamma} = g_{\beta \gamma} \, g_{\alpha \gamma}^{-1} \, g_{\alpha \beta} =1$$ where we write everything multiplicatively, since that's the group operation on sections of $GL_{r}(\mathcal{O}_{X})$ .  So this equation above is obviously the cocycle condition satisfied by vector bundle transition functions.  But for bundles, we also require that $g_{\alpha \alpha} =1$ .  Is this latter condition true in general for Cech cohomology, or is it somehow an extra requirement in this case? My second confusion is the statement that isomorphic bundles define cohomologous cocycles.  If we have a 0-cochain $\lambda = \{\lambda_{\alpha}\} \in \mathcal{C}^{0}(GL_{r}(\mathcal{O}_{X}))$ , then applying the differential we get $$(d\lambda)_{\alpha \beta} = \lambda_{\beta} \, \lambda_{\alpha}^{-1}$$ So I would be inclined to say that the condition that two 1-cocycles $\{g\}$ and $\{g'\}$ are cohomologous is $$g_{\alpha \beta} \, (g_{\alpha \beta}')^{-1} = \lambda_{\beta} \, \lambda_{\alpha}^{-1}.$$ However, I know that two bundles are equivalent when their transition functions satisfy $$g_{\alpha \beta} = \lambda_{\alpha} g_{\alpha \beta}' \lambda_{\beta}^{-1}$$ and things are clearly in the wrong order (for all ranks larger than 1) to be compatible with the previous equation.  So where are the flaws in my understanding?","I am trying to understand the fact that vector bundles of rank over a space are classified by the Cech cohomology group .  I believe this should work in any of the usual categories, so I wont specify smooth, holomorphic, etc.  I understand broadly how this goes, but there are a few key details tripping me up. So if we have a Cech 1-cocycle with respect to some open cover , then we know: where we write everything multiplicatively, since that's the group operation on sections of .  So this equation above is obviously the cocycle condition satisfied by vector bundle transition functions.  But for bundles, we also require that .  Is this latter condition true in general for Cech cohomology, or is it somehow an extra requirement in this case? My second confusion is the statement that isomorphic bundles define cohomologous cocycles.  If we have a 0-cochain , then applying the differential we get So I would be inclined to say that the condition that two 1-cocycles and are cohomologous is However, I know that two bundles are equivalent when their transition functions satisfy and things are clearly in the wrong order (for all ranks larger than 1) to be compatible with the previous equation.  So where are the flaws in my understanding?","r X \check{H}^{1}\big(X, GL_{r}(\mathcal{O}_{X})\big) g = \{g_{\alpha \beta}\} \in \check{H}^{1}\big(X, GL_{r}(\mathcal{O}_{X})\big) \{U_{\alpha}\} (dg)_{\alpha \beta \gamma} = g_{\beta \gamma} \, g_{\alpha \gamma}^{-1} \, g_{\alpha \beta} =1 GL_{r}(\mathcal{O}_{X}) g_{\alpha \alpha} =1 \lambda = \{\lambda_{\alpha}\} \in \mathcal{C}^{0}(GL_{r}(\mathcal{O}_{X})) (d\lambda)_{\alpha \beta} = \lambda_{\beta} \, \lambda_{\alpha}^{-1} \{g\} \{g'\} g_{\alpha \beta} \, (g_{\alpha \beta}')^{-1} = \lambda_{\beta} \, \lambda_{\alpha}^{-1}. g_{\alpha \beta} = \lambda_{\alpha} g_{\alpha \beta}' \lambda_{\beta}^{-1}","['differential-geometry', 'complex-geometry', 'vector-bundles', 'sheaf-cohomology']"
32,About of the curvature of Riemann as an operator,About of the curvature of Riemann as an operator,,"Let $\left( M, g \right)$ be a Riemannian manifold. Then we have the Riemann curvature tensor $R$. I know that somehow you can define the curvature operator: $$ R_x: \Lambda^2 T_xM \rightarrow \Lambda^2 T_xM $$ But how do you define it to be a self-adjoint operator? How does it work on the basis elements? Maybe you will recommend me books, where is it defined in detail? Thank you very much!","Let $\left( M, g \right)$ be a Riemannian manifold. Then we have the Riemann curvature tensor $R$. I know that somehow you can define the curvature operator: $$ R_x: \Lambda^2 T_xM \rightarrow \Lambda^2 T_xM $$ But how do you define it to be a self-adjoint operator? How does it work on the basis elements? Maybe you will recommend me books, where is it defined in detail? Thank you very much!",,"['differential-geometry', 'manifolds', 'curvature']"
33,Can We Smoothly Choose an Eigenvector of a Smoothly Parameterized Self-Adjoint Linear Map,Can We Smoothly Choose an Eigenvector of a Smoothly Parameterized Self-Adjoint Linear Map,,"$\newcommand{\R}{\mathbf R}$ Let $V$ be a finite dimensional real inner product vector space, and let $f:\R\to L(V)$ be a smooth map, where $L(V)$ denotes the space of all linear maps mapping $V$ into $V$. Assume that for each $t\in \R$, $f(t)$ is self-adjoint. Thus for each $t$ we know that $f(t)$ has a basis consisting of eigenvectors. Question. Does there necessarily exist a smooth map $g:\R\to V\setminus\{0\}$ such that $g(t)$ is an eigenvector of $f(t)$ for each $t$? If at $t_0\in \R$ we have $f(t_0)$ has $\dim V$ distinct eigenvalues, then by the proof of the LEMMA in this post we know that there is a smooth $\alpha:(t_0-\epsilon, t_0+\epsilon)\to \R$ such that $\alpha(t)$ is an eigenvalue of $f(t)$ for each $t\in (t_0-\epsilon, t_0+\epsilon)$, where $\epsilon$ is sufficiently small. Thus what we want to know, as a special case of the above question, is whether the map $(t_0-\epsilon, t_0+\epsilon)\to \mathbb P(V)$ given by $t\mapsto \ker(f(t)-\alpha(t) I)$ smooth or not.","$\newcommand{\R}{\mathbf R}$ Let $V$ be a finite dimensional real inner product vector space, and let $f:\R\to L(V)$ be a smooth map, where $L(V)$ denotes the space of all linear maps mapping $V$ into $V$. Assume that for each $t\in \R$, $f(t)$ is self-adjoint. Thus for each $t$ we know that $f(t)$ has a basis consisting of eigenvectors. Question. Does there necessarily exist a smooth map $g:\R\to V\setminus\{0\}$ such that $g(t)$ is an eigenvector of $f(t)$ for each $t$? If at $t_0\in \R$ we have $f(t_0)$ has $\dim V$ distinct eigenvalues, then by the proof of the LEMMA in this post we know that there is a smooth $\alpha:(t_0-\epsilon, t_0+\epsilon)\to \R$ such that $\alpha(t)$ is an eigenvalue of $f(t)$ for each $t\in (t_0-\epsilon, t_0+\epsilon)$, where $\epsilon$ is sufficiently small. Thus what we want to know, as a special case of the above question, is whether the map $(t_0-\epsilon, t_0+\epsilon)\to \mathbb P(V)$ given by $t\mapsto \ker(f(t)-\alpha(t) I)$ smooth or not.",,"['differential-geometry', 'differential-topology', 'smooth-manifolds']"
34,Extend a Riemannian metric defined on the boundary.,Extend a Riemannian metric defined on the boundary.,,"If $M$ is a (compact) Riemannian manifold with nonempty boundary and I have a Riemannian metric defined on $\partial M$, Is it possible to obtain a Riemannian metric on the whole $M$ extending the one on the boundary? I think that it is possible to extend it to a tubular neighborhood of the boundary by taking it to be diffeomorphic to $\partial M \times [0,1)$. What about the rest of the manifold?","If $M$ is a (compact) Riemannian manifold with nonempty boundary and I have a Riemannian metric defined on $\partial M$, Is it possible to obtain a Riemannian metric on the whole $M$ extending the one on the boundary? I think that it is possible to extend it to a tubular neighborhood of the boundary by taking it to be diffeomorphic to $\partial M \times [0,1)$. What about the rest of the manifold?",,"['differential-geometry', 'riemannian-geometry']"
35,"Is $\omega=\sin\varphi\,\mathrm{d}\theta\wedge\mathrm{d}\varphi$ an exact form?",Is  an exact form?,"\omega=\sin\varphi\,\mathrm{d}\theta\wedge\mathrm{d}\varphi","Let $\omega=\sin\varphi\,\mathrm{d}\theta\wedge\mathrm{d}\varphi$ be a $2$-form on $\mathbb{R}^3\setminus\{0\}$. Then $$\int_{\mathbb{S}^2}\omega=\int_{\mathbb{S}^2}\sin\varphi\,\mathrm{d}\theta\,\mathrm{d}\varphi=4\pi\ne0,$$ so $\omega$ is not exact. On the other hand, $$\omega=\mathrm{d}\eta\text{, where }\eta=\cos\varphi\,\mathrm{d}\theta,$$ which contradicts the fact that $\omega$ is not exact. How is this possible?","Let $\omega=\sin\varphi\,\mathrm{d}\theta\wedge\mathrm{d}\varphi$ be a $2$-form on $\mathbb{R}^3\setminus\{0\}$. Then $$\int_{\mathbb{S}^2}\omega=\int_{\mathbb{S}^2}\sin\varphi\,\mathrm{d}\theta\,\mathrm{d}\varphi=4\pi\ne0,$$ so $\omega$ is not exact. On the other hand, $$\omega=\mathrm{d}\eta\text{, where }\eta=\cos\varphi\,\mathrm{d}\theta,$$ which contradicts the fact that $\omega$ is not exact. How is this possible?",,"['differential-geometry', 'differential-forms']"
36,Prove the sphere is orientable,Prove the sphere is orientable,,Is there an easy way to show that the sphere $$\mathbb S^n = \{ x\in \mathbb R^{n+1} : \|x\| =1\}$$ is orientable other then using stereograohic projection? I am preferably looking for something derived from a basic theorem in elementary geometry with respect to the unit normal.,Is there an easy way to show that the sphere is orientable other then using stereograohic projection? I am preferably looking for something derived from a basic theorem in elementary geometry with respect to the unit normal.,\mathbb S^n = \{ x\in \mathbb R^{n+1} : \|x\| =1\},"['differential-geometry', 'smooth-manifolds', 'spheres', 'orientation']"
37,Divergence in terms of Levi-Civita connection,Divergence in terms of Levi-Civita connection,,The divergence of a vector field $X$ on a manifold $M$ is defined usually as the function $\text{Div}(.)$ such that $(\text{Div} X) \;\mu =L_X \mu$ for $\mu$ a volume form. I know that there is also an alternative expression for $\text{Div} \; X$ in terms of the covariant derivative: $\text{div} \; X= \text{trace}(\nabla X)= \nabla^aX_a.$ Can anyone explain to me how to prove this equivalence?,The divergence of a vector field $X$ on a manifold $M$ is defined usually as the function $\text{Div}(.)$ such that $(\text{Div} X) \;\mu =L_X \mu$ for $\mu$ a volume form. I know that there is also an alternative expression for $\text{Div} \; X$ in terms of the covariant derivative: $\text{div} \; X= \text{trace}(\nabla X)= \nabla^aX_a.$ Can anyone explain to me how to prove this equivalence?,,"['differential-geometry', 'differential-topology', 'general-relativity']"
38,Is a Riemannian metric a $2$-form?,Is a Riemannian metric a -form?,2,"In Lee's Riemannian Manifolds; An introduction to Curvature, he defines a Riemannian metric as an element of $\Gamma(T^2_0M)$, a $(2,0)$-tensor. Is this the same thing as a $2$-form? Is there a difference between being a section of $T^\ast M\times T^\ast M$ and a section of $T^2_0M$? A difference between a $(2,0)$-tensor and $2$-form?","In Lee's Riemannian Manifolds; An introduction to Curvature, he defines a Riemannian metric as an element of $\Gamma(T^2_0M)$, a $(2,0)$-tensor. Is this the same thing as a $2$-form? Is there a difference between being a section of $T^\ast M\times T^\ast M$ and a section of $T^2_0M$? A difference between a $(2,0)$-tensor and $2$-form?",,"['differential-geometry', 'metric-spaces', 'riemannian-geometry']"
39,When do isometries commute with the compatible derivative operator on a semi-Riemannian manifold?,When do isometries commute with the compatible derivative operator on a semi-Riemannian manifold?,,"Let $M$ and $\tilde{M}$ be smooth manifolds, each with a metric $g_{ab}$ and $\tilde{g}_{ab}$, assumed here to be smooth symmetric invertible tensor fields, which are non-degenerate but not necessarily positive-definite. Let $\nabla_a$ be the derivative operator that is 'compatible' with $g_{ab}$ in that $\nabla_a g_{bc}=\mathbf{0}$, and similarly let $\tilde{\nabla}_a \tilde{g}_{bc}=\mathbf{0}$. (These derivative operators are sometimes called 'connections' and are unique.) Let $\varphi:M\rightarrow\tilde{M}$ be a diffeomorphism with pushforward $\varphi_*$. If $\varphi$ is an isometry ($\varphi_*g_{ab} = \tilde{g}_{ab})$, is it true that for an arbitrary tensor field like $\lambda^{bc}_d$, $\varphi_*\left(\nabla_a \lambda^{bc}_d\right) = \tilde{\nabla}_a \varphi_*\left(\lambda^{bc}_d\right)$? It is clearly true in some special cases, but I'm interested in understanding this general context. A reference would be very helpful. I'm curious also if there are any non-isometries that also have this property. Thanks! -- Bryan","Let $M$ and $\tilde{M}$ be smooth manifolds, each with a metric $g_{ab}$ and $\tilde{g}_{ab}$, assumed here to be smooth symmetric invertible tensor fields, which are non-degenerate but not necessarily positive-definite. Let $\nabla_a$ be the derivative operator that is 'compatible' with $g_{ab}$ in that $\nabla_a g_{bc}=\mathbf{0}$, and similarly let $\tilde{\nabla}_a \tilde{g}_{bc}=\mathbf{0}$. (These derivative operators are sometimes called 'connections' and are unique.) Let $\varphi:M\rightarrow\tilde{M}$ be a diffeomorphism with pushforward $\varphi_*$. If $\varphi$ is an isometry ($\varphi_*g_{ab} = \tilde{g}_{ab})$, is it true that for an arbitrary tensor field like $\lambda^{bc}_d$, $\varphi_*\left(\nabla_a \lambda^{bc}_d\right) = \tilde{\nabla}_a \varphi_*\left(\lambda^{bc}_d\right)$? It is clearly true in some special cases, but I'm interested in understanding this general context. A reference would be very helpful. I'm curious also if there are any non-isometries that also have this property. Thanks! -- Bryan",,"['differential-geometry', 'riemannian-geometry', 'mathematical-physics', 'smooth-manifolds', 'semi-riemannian-geometry']"
40,1-manifold is orientable,1-manifold is orientable,,"I am trying to classify all compact 1-manifolds. I believe I can do it once I can show every 1-manifold is orientable. I have tried to show prove this a bunch of ways, but I can't get anywhere. Please help, Note, I am NOT assuming that I already know the only such manifolds are [0,1] or $S^1$. This is my end goal.","I am trying to classify all compact 1-manifolds. I believe I can do it once I can show every 1-manifold is orientable. I have tried to show prove this a bunch of ways, but I can't get anywhere. Please help, Note, I am NOT assuming that I already know the only such manifolds are [0,1] or $S^1$. This is my end goal.",,"['differential-geometry', 'manifolds', 'compact-manifolds']"
41,Framed Cobordism Classes of links in $\mathbb R^3$,Framed Cobordism Classes of links in,\mathbb R^3,"We know that every link in $S^3$ is framed cobordant to the unknot with some framing. The idea is to study smooth homotopy classes of maps from $S^3$ to $S^2$. Actually in the title I have given $\mathbb R^3$ but any knot in $S^3$ can be isotoped to miss a point $p$ on $S^3$ and hence lie in $\mathbb R^3\cong S^3-p$ The framing given below is one where the frame twists once as it goes one round along the knot, i.e., framing given by $1\in\pi_1(SO(2))$ Does the disjoint union of $n$ (mutually unlinked) unknots with framing $1\in SO(2)$ represent the unknot with framing $n\in\pi_1(SO(2))$? Somehow I find it difficult to picture whether this is true or not. The only way I can perhaps argue is that in the framed cobordism classes of links form a group and the addition in the group is just the unlinked disjoint union. Is this true? If false, is there any other way to represent the class of the unknot with framing with $n\in\pi_1(SO(2))$, by a link which has framing with one twist ($1\in SO(2)$) on each knot in the link?","We know that every link in $S^3$ is framed cobordant to the unknot with some framing. The idea is to study smooth homotopy classes of maps from $S^3$ to $S^2$. Actually in the title I have given $\mathbb R^3$ but any knot in $S^3$ can be isotoped to miss a point $p$ on $S^3$ and hence lie in $\mathbb R^3\cong S^3-p$ The framing given below is one where the frame twists once as it goes one round along the knot, i.e., framing given by $1\in\pi_1(SO(2))$ Does the disjoint union of $n$ (mutually unlinked) unknots with framing $1\in SO(2)$ represent the unknot with framing $n\in\pi_1(SO(2))$? Somehow I find it difficult to picture whether this is true or not. The only way I can perhaps argue is that in the framed cobordism classes of links form a group and the addition in the group is just the unlinked disjoint union. Is this true? If false, is there any other way to represent the class of the unknot with framing with $n\in\pi_1(SO(2))$, by a link which has framing with one twist ($1\in SO(2)$) on each knot in the link?",,"['differential-geometry', 'homotopy-theory', 'knot-theory']"
42,Does Differential Topology or Differential Geometry play a larger role in Chaos Theory?,Does Differential Topology or Differential Geometry play a larger role in Chaos Theory?,,"I'm an undergraduate on somewhat of a time constraint in school.  I have room in my remaining schedule for a semester of either Differential Geometry or Differential Topology. I understand the topological side deals with primarily global aspects of a manifold, whereas the geometrical side associates with interesting local structures. To be concise -  I'm interested in chaotic dynamical systems and P.D.E., so which subject would be more worthwhile, i.e. which subject's vocabulary/method would equip for further reading in the topics I am interested in?  Or am i naive in forcing myself to choose between the two topics, as similar as they are? Thank you!","I'm an undergraduate on somewhat of a time constraint in school.  I have room in my remaining schedule for a semester of either Differential Geometry or Differential Topology. I understand the topological side deals with primarily global aspects of a manifold, whereas the geometrical side associates with interesting local structures. To be concise -  I'm interested in chaotic dynamical systems and P.D.E., so which subject would be more worthwhile, i.e. which subject's vocabulary/method would equip for further reading in the topics I am interested in?  Or am i naive in forcing myself to choose between the two topics, as similar as they are? Thank you!",,"['differential-geometry', 'soft-question', 'differential-topology', 'advice', 'chaos-theory']"
43,A Cover of an Orientable Manifold is Orientable,A Cover of an Orientable Manifold is Orientable,,"The following question comes from Introduction to Smooth Manifolds by Lee: Suppose $\widetilde{M}$ smoothly covers $M$ where $M$ is orientable. Show that $\widetilde{M}$ is orientable. I think the following proof works: Orientability is equivalent to the existence of a nowhere vanishing continuous top form on $M$, so let $\Omega$ be any such form on $M$. Then the pullback $\pi^*\Omega$ is a top form on $\widetilde{M}$ which cannot vanish ($\pi$ here denotes the smooth covering map). For if it did that would imply that $\Omega$ vanished somewhere. This proof concerns me since I am nowhere using the fact that $\widetilde{M}$ is covering space other than to know that $\pi^*\Omega$ is a top form on $\widetilde{M}$ since $\pi$ is a local diffeomorphism. Have I proven too much here?","The following question comes from Introduction to Smooth Manifolds by Lee: Suppose $\widetilde{M}$ smoothly covers $M$ where $M$ is orientable. Show that $\widetilde{M}$ is orientable. I think the following proof works: Orientability is equivalent to the existence of a nowhere vanishing continuous top form on $M$, so let $\Omega$ be any such form on $M$. Then the pullback $\pi^*\Omega$ is a top form on $\widetilde{M}$ which cannot vanish ($\pi$ here denotes the smooth covering map). For if it did that would imply that $\Omega$ vanished somewhere. This proof concerns me since I am nowhere using the fact that $\widetilde{M}$ is covering space other than to know that $\pi^*\Omega$ is a top form on $\widetilde{M}$ since $\pi$ is a local diffeomorphism. Have I proven too much here?",,"['differential-geometry', 'manifolds']"
44,$\alpha\wedge\beta = 0$ for all $\beta$ implies $\alpha = 0$ without using the Hodge dual,for all  implies  without using the Hodge dual,\alpha\wedge\beta = 0 \beta \alpha = 0,"Let $\alpha$ be a differential $k$-form on an orientable smooth $n$-dimensional manifold. If $\alpha\wedge\beta = 0$ for every differential $(n - k)$-form $\beta$, then $\alpha = 0$ because we can choose a Riemannian metric from which we can construct the corresponding Hodge dual $\ast$ and obtain $0 = \alpha\wedge\ast\alpha = \|\alpha\|^2dV$. Can we deduce that $\alpha = 0$ without choosing a metric and considering the Hodge dual?","Let $\alpha$ be a differential $k$-form on an orientable smooth $n$-dimensional manifold. If $\alpha\wedge\beta = 0$ for every differential $(n - k)$-form $\beta$, then $\alpha = 0$ because we can choose a Riemannian metric from which we can construct the corresponding Hodge dual $\ast$ and obtain $0 = \alpha\wedge\ast\alpha = \|\alpha\|^2dV$. Can we deduce that $\alpha = 0$ without choosing a metric and considering the Hodge dual?",,"['differential-geometry', 'differential-forms', 'exterior-algebra']"
45,Diagonalization of Riemannian Metric and the Laplace Beltrami Operator,Diagonalization of Riemannian Metric and the Laplace Beltrami Operator,,"Consider the local representation of the Laplace Beltrami operator on a Riemannian n - dimensional manifold $(M,g)$: \begin{equation} \triangle_g = \frac{1}{\sqrt{\text{det}(g)}} \sum^n_{i,j = 1} \frac{\partial}{\partial x^i} g^{ij} \sqrt{\text{det}(g)}\frac{\partial}{\partial x^j}  \end{equation} Some time ago I read in some textbook that one can, locally at any point $p \in M$, choose a small enough neigborhood $U \subset M$ so that by a linear transformation of the coordinates the matrix representation of $g$ in $U$ is $g_{ij} = \delta_{ij}$. But wouldn't this imply that the above expression always simplifies to the expression \begin{equation}  \triangle_g = \sum^n_{i = 1} \frac{\partial^2}{\partial (x^i)^2} \quad ? \end{equation} Alternatively I would think that even though the metric evaluates to the Kronecker delta that doesn't mean that its derivatives are zero. So do we actually then have \begin{equation} \triangle_g =  \sum^n_{j = 1} \frac{\partial^2}{\partial (x^j)^2} + \sum_{j = 1}^n ( \frac{\partial}{\partial x^j} \sqrt{\text{det}(g)} + \sum_{i = 1}^n \frac{\partial}{\partial x^i} g^{ij})\frac{\partial}{\partial x^j}  \end{equation} I am sure at least one my impression is false, if anybody could help pointing out what I got wrong, or refer to a reference where I could read about diagonalization of metrics that would be so helpful, many thanks !","Consider the local representation of the Laplace Beltrami operator on a Riemannian n - dimensional manifold $(M,g)$: \begin{equation} \triangle_g = \frac{1}{\sqrt{\text{det}(g)}} \sum^n_{i,j = 1} \frac{\partial}{\partial x^i} g^{ij} \sqrt{\text{det}(g)}\frac{\partial}{\partial x^j}  \end{equation} Some time ago I read in some textbook that one can, locally at any point $p \in M$, choose a small enough neigborhood $U \subset M$ so that by a linear transformation of the coordinates the matrix representation of $g$ in $U$ is $g_{ij} = \delta_{ij}$. But wouldn't this imply that the above expression always simplifies to the expression \begin{equation}  \triangle_g = \sum^n_{i = 1} \frac{\partial^2}{\partial (x^i)^2} \quad ? \end{equation} Alternatively I would think that even though the metric evaluates to the Kronecker delta that doesn't mean that its derivatives are zero. So do we actually then have \begin{equation} \triangle_g =  \sum^n_{j = 1} \frac{\partial^2}{\partial (x^j)^2} + \sum_{j = 1}^n ( \frac{\partial}{\partial x^j} \sqrt{\text{det}(g)} + \sum_{i = 1}^n \frac{\partial}{\partial x^i} g^{ij})\frac{\partial}{\partial x^j}  \end{equation} I am sure at least one my impression is false, if anybody could help pointing out what I got wrong, or refer to a reference where I could read about diagonalization of metrics that would be so helpful, many thanks !",,['differential-geometry']
46,Is smoothness of multiplication redundant in the definition of Lie Group?,Is smoothness of multiplication redundant in the definition of Lie Group?,,"It is well known that in the definition of Lie groups, we actually only need that the multiplication be smooth, since this implies that inversion is smooth . I'm now trying to solve the following exercise: Show that in the definition of Lie group, it is enough to assume that the inverse map $G \to G, g \mapsto g^{-1}$ is smooth. I don't know how to attack this problem, and also couldn't find any other references to it in the literature. Could I get some hints or references on how to solve it?","It is well known that in the definition of Lie groups, we actually only need that the multiplication be smooth, since this implies that inversion is smooth . I'm now trying to solve the following exercise: Show that in the definition of Lie group, it is enough to assume that the inverse map is smooth. I don't know how to attack this problem, and also couldn't find any other references to it in the literature. Could I get some hints or references on how to solve it?","G \to G, g \mapsto g^{-1}","['differential-geometry', 'lie-groups', 'smooth-manifolds']"
47,Reconciling different expressions for Riemann curvature tensor,Reconciling different expressions for Riemann curvature tensor,,"[Note: This has been crossposted to Physics SE, but I haven't found a thourough explanation there so far, so I'm posted the question here as well] I'm using Einstein's summation convention throughout. Also since I'm new to this subject, I'd be really grateful if any assertions can be shown through explicit calculations so that I can follow along and learn in the process. I'm reading Carroll's GR notes and I'm having trouble deciphering a particular expression for the Riemann curvature tensor. The coordinate-free definition is (eq. 3.71 in Carroll's notes): $$R(X,Y)Z=\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\nabla_{[X,Y]}Z\tag{3.71}$$ Some more background on the first equation, based on my reading of ""Semi-Riemannian Geometry: The Mathematical Language of General Relativity"" by Newman: $R$ is a map from $\mathfrak{X}(M)^3$ to $\mathfrak{X}(M)$ such that (3.71) holds. Newman's book doesn't really treat $R(X,Y)$ as a separate object (as far as I've read). $R$ is just treated as a map of 3 vector fields and with the weird notation $R(X,Y)Z$ instead of $R(X,Y,Z)$ (again, as far as I've read). Then there is a theorem showing that $R$ is multilinear in all its arguments and anti-symmetric in its first 2 arguments. And then the following assertion: Let $(M,\nabla)$ be a smooth manifold with a connection, and let $(U,(x^i))$ be a chart on $M$ . Then in local coordinates, $R(\partial/\partial x^{\mu},\partial/\partial x^{\nu})\partial/\partial x^{\sigma}$ can be expressed as: $$R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)\frac{\partial}{\partial x^{\sigma}}=R_{\ \ \sigma\mu\nu}^{\rho}\frac{\partial}{\partial x^{\rho}}$$ Based on the above, if $V\in\mathfrak{X}(M)$ , then $$R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)V=R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)\bigg(V^{\sigma}\frac{\partial}{\partial x^{\sigma}}\bigg) \\=V^{\sigma}R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)\frac{\partial}{\partial x^{\sigma}}=V^{\sigma}R_{\ \ \sigma\mu\nu}^{\eta}\frac{\partial}{\partial x^{\eta}}$$ where the second equality holds due to multilinearity of $R$ . Now if I act this on the $x^{\rho}$ coordinate function, I get $$\bigg(R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)V\bigg)(x^{\rho})\equiv\bigg(R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)V\bigg)^{\rho}=V^{\sigma}R_{\ \ \sigma\mu\nu}^{\eta}\frac{\partial}{\partial x^{\eta}}(x^{\rho}) \\=V^{\sigma}R_{\ \ \sigma\mu\nu}^{\eta}\frac{\partial x^{\rho}}{\partial x^{\eta}}=R_{\ \ \sigma\mu\nu}^{\rho}V^{\sigma}\tag{1}$$ So far, so good. Important note: I'm using the notation $\nabla_{\mu}\equiv\nabla_{\frac{\partial}{\partial x^{\mu}}}$ , where $\partial/\partial x^{\mu}$ is the $\mu$ -th basis vector field on $(U,(x^i))$ . Now in eq. (3.71), I can replace $X,Y$ by fields $\partial_{\mu},\partial_{\nu}$ respectively and $Z$ by $V$ , then I can get the local coordinates for both sides by acting them on coordinate function $x^{\rho}$ : $$(R(\partial_{\mu},\partial_{\nu})V)(x^{\rho})\equiv(R(\partial_{\mu},\partial_{\nu})V)^{\rho}=([\nabla_{\mu},\nabla_{\nu}]V)(x^{\rho})-(\nabla_{[\partial_{\mu},\partial_{\nu}]}V)(x^{\rho}) \\=([\nabla_{\mu},\nabla_{\nu}]V)(x^{\rho})\equiv([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}\tag{2}$$ Comparing (1) and (2), I get $$R_{\ \ \sigma\mu\nu}^{\rho}V^{\sigma}=([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}\tag{3}$$ The above equation completely follows from (3.71). Now in Carroll's notes, an index-based expression for the Riemann curvature tensor is also given (eq. 3.66): $$R^{\rho}_{\ \ \sigma\mu\nu}V^{\sigma}=[\nabla_{\mu},\nabla_{\nu}]V^{\rho}+T_{\mu\nu}^{\ \ \ \ \lambda}\nabla_{\lambda}V^{\rho}\tag{3.66}$$ So far torsion-free assumption hasn't been made anywhere . To reconcile (3.71) with (3.66), I have to show (comparing RHS of (3) and (3.66)): $$([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}=[\nabla_{\mu},\nabla_{\nu}]V^{\rho}+T_{\mu\nu}^{\ \ \ \ \lambda}\nabla_{\lambda}V^{\rho}\tag{4}$$ This is where I'm stuck and I really cannot wrap my head around it. How do I go about proving eq. (4)? The first term on the RHS - I can only infer that it's $[\nabla_{\mu},\nabla_{\mu}](V^{\rho})$ . If it were $([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}$ , then it would equal the LHS and the 2nd RHS term would just be zero and meaningless to include - this can't be. But then if it's $[\nabla_{\mu},\nabla_{\mu}](V^{\rho})$ , well $V^{\rho}$ is a $C^{\infty}(U)$ function and this would evaluate to $\partial_{\mu}\partial_{\nu}V^{\rho}-\partial_{\nu}\partial_{\mu}V^{\rho}=0$ by equality of mixed partial derivatives. I'm not sure where I've gone wrong in the specific approach outlined in this question. I'd appreciate any help or corrections! EDIT : Some calculations I've done after looking at the accepted answer: $$A=\nabla_X(\nabla_YZ)=\nabla_X(\nabla_{Y^b\partial_b}Z)=\nabla_X(Y^b\nabla_{\partial_b}Z)$$ $$=X(Y^b)\nabla_{\partial_b}Z+Y^b\nabla_X(\nabla_{\partial_b}Z)=X(Y^b)\nabla_{\partial_b}Z+Y^b\nabla_{X^a\partial_a}(\nabla_{\partial_b}Z)$$ $$=X(Y^b)\nabla_{\partial_b}Z+Y^bX^a\nabla_{\partial_a}(\nabla_{\partial_b}Z)$$ $$\\$$ $$B=\nabla_{\nabla_XY}Z=\nabla_{\nabla_X(Y^b\partial_b)}Z=\nabla_{X(Y^b)\partial_b+Y^b\nabla_X\partial_b}Z$$ $$=\nabla_{X(Y^b)\partial_b}Z+\nabla_{Y^b\nabla_X\partial_b}Z=X(Y_b)\nabla_{\partial_b}Z+Y^b\nabla_{\nabla_X\partial_b}Z$$ $$=X(Y_b)\nabla_{\partial_b}Z+Y^bX^a\nabla_{\nabla_{\partial_a}\partial_b}Z$$ $$\\$$ $$A-B=X^aY^b\nabla_a\nabla_bZ \\\implies \nabla_{\partial_a}(\nabla_{\partial_b}Z)-\nabla_{\nabla_{\partial_a}\partial_b}Z=\nabla_a\nabla_bZ\tag{5}$$ So now I have to convince myself that eq. (5) is correct.","[Note: This has been crossposted to Physics SE, but I haven't found a thourough explanation there so far, so I'm posted the question here as well] I'm using Einstein's summation convention throughout. Also since I'm new to this subject, I'd be really grateful if any assertions can be shown through explicit calculations so that I can follow along and learn in the process. I'm reading Carroll's GR notes and I'm having trouble deciphering a particular expression for the Riemann curvature tensor. The coordinate-free definition is (eq. 3.71 in Carroll's notes): Some more background on the first equation, based on my reading of ""Semi-Riemannian Geometry: The Mathematical Language of General Relativity"" by Newman: is a map from to such that (3.71) holds. Newman's book doesn't really treat as a separate object (as far as I've read). is just treated as a map of 3 vector fields and with the weird notation instead of (again, as far as I've read). Then there is a theorem showing that is multilinear in all its arguments and anti-symmetric in its first 2 arguments. And then the following assertion: Let be a smooth manifold with a connection, and let be a chart on . Then in local coordinates, can be expressed as: Based on the above, if , then where the second equality holds due to multilinearity of . Now if I act this on the coordinate function, I get So far, so good. Important note: I'm using the notation , where is the -th basis vector field on . Now in eq. (3.71), I can replace by fields respectively and by , then I can get the local coordinates for both sides by acting them on coordinate function : Comparing (1) and (2), I get The above equation completely follows from (3.71). Now in Carroll's notes, an index-based expression for the Riemann curvature tensor is also given (eq. 3.66): So far torsion-free assumption hasn't been made anywhere . To reconcile (3.71) with (3.66), I have to show (comparing RHS of (3) and (3.66)): This is where I'm stuck and I really cannot wrap my head around it. How do I go about proving eq. (4)? The first term on the RHS - I can only infer that it's . If it were , then it would equal the LHS and the 2nd RHS term would just be zero and meaningless to include - this can't be. But then if it's , well is a function and this would evaluate to by equality of mixed partial derivatives. I'm not sure where I've gone wrong in the specific approach outlined in this question. I'd appreciate any help or corrections! EDIT : Some calculations I've done after looking at the accepted answer: So now I have to convince myself that eq. (5) is correct.","R(X,Y)Z=\nabla_X\nabla_YZ-\nabla_Y\nabla_XZ-\nabla_{[X,Y]}Z\tag{3.71} R \mathfrak{X}(M)^3 \mathfrak{X}(M) R(X,Y) R R(X,Y)Z R(X,Y,Z) R (M,\nabla) (U,(x^i)) M R(\partial/\partial x^{\mu},\partial/\partial x^{\nu})\partial/\partial x^{\sigma} R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)\frac{\partial}{\partial x^{\sigma}}=R_{\ \ \sigma\mu\nu}^{\rho}\frac{\partial}{\partial x^{\rho}} V\in\mathfrak{X}(M) R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)V=R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)\bigg(V^{\sigma}\frac{\partial}{\partial x^{\sigma}}\bigg)
\\=V^{\sigma}R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)\frac{\partial}{\partial x^{\sigma}}=V^{\sigma}R_{\ \ \sigma\mu\nu}^{\eta}\frac{\partial}{\partial x^{\eta}} R x^{\rho} \bigg(R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)V\bigg)(x^{\rho})\equiv\bigg(R\bigg(\frac{\partial}{\partial x^{\mu}},\frac{\partial}{\partial x^{\nu}}\bigg)V\bigg)^{\rho}=V^{\sigma}R_{\ \ \sigma\mu\nu}^{\eta}\frac{\partial}{\partial x^{\eta}}(x^{\rho})
\\=V^{\sigma}R_{\ \ \sigma\mu\nu}^{\eta}\frac{\partial x^{\rho}}{\partial x^{\eta}}=R_{\ \ \sigma\mu\nu}^{\rho}V^{\sigma}\tag{1} \nabla_{\mu}\equiv\nabla_{\frac{\partial}{\partial x^{\mu}}} \partial/\partial x^{\mu} \mu (U,(x^i)) X,Y \partial_{\mu},\partial_{\nu} Z V x^{\rho} (R(\partial_{\mu},\partial_{\nu})V)(x^{\rho})\equiv(R(\partial_{\mu},\partial_{\nu})V)^{\rho}=([\nabla_{\mu},\nabla_{\nu}]V)(x^{\rho})-(\nabla_{[\partial_{\mu},\partial_{\nu}]}V)(x^{\rho})
\\=([\nabla_{\mu},\nabla_{\nu}]V)(x^{\rho})\equiv([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}\tag{2} R_{\ \ \sigma\mu\nu}^{\rho}V^{\sigma}=([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}\tag{3} R^{\rho}_{\ \ \sigma\mu\nu}V^{\sigma}=[\nabla_{\mu},\nabla_{\nu}]V^{\rho}+T_{\mu\nu}^{\ \ \ \ \lambda}\nabla_{\lambda}V^{\rho}\tag{3.66} ([\nabla_{\mu},\nabla_{\nu}]V)^{\rho}=[\nabla_{\mu},\nabla_{\nu}]V^{\rho}+T_{\mu\nu}^{\ \ \ \ \lambda}\nabla_{\lambda}V^{\rho}\tag{4} [\nabla_{\mu},\nabla_{\mu}](V^{\rho}) ([\nabla_{\mu},\nabla_{\nu}]V)^{\rho} [\nabla_{\mu},\nabla_{\mu}](V^{\rho}) V^{\rho} C^{\infty}(U) \partial_{\mu}\partial_{\nu}V^{\rho}-\partial_{\nu}\partial_{\mu}V^{\rho}=0 A=\nabla_X(\nabla_YZ)=\nabla_X(\nabla_{Y^b\partial_b}Z)=\nabla_X(Y^b\nabla_{\partial_b}Z) =X(Y^b)\nabla_{\partial_b}Z+Y^b\nabla_X(\nabla_{\partial_b}Z)=X(Y^b)\nabla_{\partial_b}Z+Y^b\nabla_{X^a\partial_a}(\nabla_{\partial_b}Z) =X(Y^b)\nabla_{\partial_b}Z+Y^bX^a\nabla_{\partial_a}(\nabla_{\partial_b}Z) \\ B=\nabla_{\nabla_XY}Z=\nabla_{\nabla_X(Y^b\partial_b)}Z=\nabla_{X(Y^b)\partial_b+Y^b\nabla_X\partial_b}Z =\nabla_{X(Y^b)\partial_b}Z+\nabla_{Y^b\nabla_X\partial_b}Z=X(Y_b)\nabla_{\partial_b}Z+Y^b\nabla_{\nabla_X\partial_b}Z =X(Y_b)\nabla_{\partial_b}Z+Y^bX^a\nabla_{\nabla_{\partial_a}\partial_b}Z \\ A-B=X^aY^b\nabla_a\nabla_bZ
\\\implies \nabla_{\partial_a}(\nabla_{\partial_b}Z)-\nabla_{\nabla_{\partial_a}\partial_b}Z=\nabla_a\nabla_bZ\tag{5}","['differential-geometry', 'riemannian-geometry', 'curvature']"
48,Maximal Smooth Atlas For Stereographic Projection,Maximal Smooth Atlas For Stereographic Projection,,"Problem 1-7 (d) in John M. Lee's Introduction To Smooth Manifolds asks to ... verify that the atlas consisting of two charts $(\mathbb{S}^n \setminus \{N\}, \sigma)$ and $(\mathbb{S}^n \setminus \{S\}, \tilde{\sigma})$ defines a smooth structure on $\mathbb{S}^n$. , where $N = (0,\ldots,0,1) \in \mathbb{R}^{n+1}$ and $S = (0,\ldots,0,-1) \in \mathbb{R}^{n+1}$, $\sigma$ is the stereographic projection from $N$ and $\tilde{\sigma}$ is the stereographic projection from $S$. Lee defines a smooth structure to be a maximal smooth atlas on a topological manifold. I have managed to prove that $\sigma$ and $\tilde{\sigma}$ are bijective, that the transition map $\tilde{\sigma} \circ \sigma^{-1}$ is a diffeomorphism, and hence the charts $(\mathbb{S}^n \setminus \{N\}, \sigma)$ and $(\mathbb{S}^n \setminus \{S\}, \tilde{\sigma})$ are smoothly compatible. Thus, the atlas consisting of these two charts is smooth. I have struggled to prove that these two charts form a maximal smooth atlas on $\mathbb{S}^n$. After some frustration, I checked this site and found that this smooth atlas is not actually maximal, according to the following post: How to show this atlas is maximal on the sphere $S^n$? So is the term smooth structure in the question a typo? Should it have instead asked just to prove that the two charts form a smooth atlas on $\mathbb{S}^n$? Thanks.","Problem 1-7 (d) in John M. Lee's Introduction To Smooth Manifolds asks to ... verify that the atlas consisting of two charts $(\mathbb{S}^n \setminus \{N\}, \sigma)$ and $(\mathbb{S}^n \setminus \{S\}, \tilde{\sigma})$ defines a smooth structure on $\mathbb{S}^n$. , where $N = (0,\ldots,0,1) \in \mathbb{R}^{n+1}$ and $S = (0,\ldots,0,-1) \in \mathbb{R}^{n+1}$, $\sigma$ is the stereographic projection from $N$ and $\tilde{\sigma}$ is the stereographic projection from $S$. Lee defines a smooth structure to be a maximal smooth atlas on a topological manifold. I have managed to prove that $\sigma$ and $\tilde{\sigma}$ are bijective, that the transition map $\tilde{\sigma} \circ \sigma^{-1}$ is a diffeomorphism, and hence the charts $(\mathbb{S}^n \setminus \{N\}, \sigma)$ and $(\mathbb{S}^n \setminus \{S\}, \tilde{\sigma})$ are smoothly compatible. Thus, the atlas consisting of these two charts is smooth. I have struggled to prove that these two charts form a maximal smooth atlas on $\mathbb{S}^n$. After some frustration, I checked this site and found that this smooth atlas is not actually maximal, according to the following post: How to show this atlas is maximal on the sphere $S^n$? So is the term smooth structure in the question a typo? Should it have instead asked just to prove that the two charts form a smooth atlas on $\mathbb{S}^n$? Thanks.",,"['differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds']"
49,Coordinate free definition of $\nabla$ operator,Coordinate free definition of  operator,\nabla,"There are a number of posts on this site asking similar questions and some of them have been answered (to my taste) at least partially but none give a complete answer that I am satisfied with. See links at the bottom of this question for a small selection of posts asking related (or even the same) questions. My question is as follows. The following is often written down: $$ \nabla = \frac{\partial}{\partial_x} \hat{x} + \frac{\partial}{\partial y}\hat{y} + \frac{\partial}{\partial z} \hat{z} $$ Some people will call this an operator, some will call it a vector, some will call it a vector operator, and some will adamantly claim that it is not properly anything at all and you shouldn't call it any of these things and you should just treat it as a ""notational convenience"". One can then go on to use this ""vector operator"" to calculate things like $\nabla f$, $\nabla \cdot \vec{F}$ or $\nabla \times\vec{F}$ where the operator is treated notationally as if it were a vector. First I want to take issue with the final claim that it is purely a notational convenience. I think it is more than just a notational convenience for the following reason. It is possible, by following certain transformation rules, to express $\nabla$ in different coordinate systems, for example cylindrical or spherical. That might be fine, but there is a FURTHER point which makes me think $\nabla$ must be more than a notational convenience. if you express $\nabla$ in different coordinates you can then calculate something like $\nabla \cdot \vec{F}$ in the new coordinates and get the right answer. An answer which you could have arrived at by explicitly converting the cartesian expression for $\nabla \cdot \vec{F}$ into the new coordinate system. In other words, the $\nabla$ allows you to actually skip a step of calculation you would have had to do otherwise. This is evidence that the symbol carries some sort of mathematical structure to it which should be able to be captured in an independent definition. To that end I'm interested in a coordinate free definition of this symbol. The definition I gave above relies on using the usual Cartesian coordinates above. I have searched but haven't been able to find a coordinate free definition of the $\nabla$ symbol. Can one exist? In particular, I am interested in such a formula so that it is algebraically evident how one should calculate the components of $\nabla$ in any given coordinate system. Is there a coordinate free definition of $\nabla$? I am aware of a few complications with this endeavor that I'll just list here: 1) If this is to be some kind of vector or some kind of operator then it is not clear what space it should live in. For example, it is an object which can take a function $f$ and map it to a vector space. But at the same time it is an object which can be fed as an argument to a dot product together with a vector (form a different space) and return a scalar. 2) If I put on my differential geometry hat it becomes a very weird object. In differential geometry I come to think of vectors as actually being things like $\frac{\partial}{\partial x}$ and that $\vec{x}$ notation is eschewed. However the $\nabla$ symbol above contains both of these sitting next to each other. it's like a vector of vectors.. The idea of two vectors sitting next to eachother made me think it might be some kind of rank 2 contravariant tensor but I think that may have been a stretch. 3) I am aware that the cross product and curl operator are only defined in 3 dimensions so it does not need to be pointed out that that limits the possibility of defining such an operator for arbitrary dimension. I am happy to say we are working in 3 dimension. 4) I understand that the idea of divergence and curl depends on the presence of a metric for a space. Ok, that is fine. We can work in a space that has a metric defined on it. 5) Maybe the metric needs to be flat? Even that is fine as long as we can work in coordinate systems such as cylindrical or spherical where the metric is still flat but no longer has a trivial component representation. I am happy to restrict analysis to $\mathbb{R}^3$ if that is necessary. 6) Finally if such a definition truly cannot be formulated then could you at least answer why I can calculate BOTH $\nabla f$ and $\nabla \cdot \vec{F}$ by either 1) computing $\nabla f$ or $\nabla \cdot \vec{F}$ in xyz coordinates, then convert everything to spherical or 2) compute $\nabla$ in xyz coordinates, covert to spherical, then calculate $\nabla f$ and $\nabla \cdot \vec{F}$ and get the same answer in both cases? It just seems slightly too powerful/structured to be JUST a notational convenience. Here are a few other related questions: Is there a general formula for the del operator $\nabla$ in different coordinate systems? Can $\nabla$ be called a ""vector"" in any meaningful way? Coordinate transformation on del operator","There are a number of posts on this site asking similar questions and some of them have been answered (to my taste) at least partially but none give a complete answer that I am satisfied with. See links at the bottom of this question for a small selection of posts asking related (or even the same) questions. My question is as follows. The following is often written down: $$ \nabla = \frac{\partial}{\partial_x} \hat{x} + \frac{\partial}{\partial y}\hat{y} + \frac{\partial}{\partial z} \hat{z} $$ Some people will call this an operator, some will call it a vector, some will call it a vector operator, and some will adamantly claim that it is not properly anything at all and you shouldn't call it any of these things and you should just treat it as a ""notational convenience"". One can then go on to use this ""vector operator"" to calculate things like $\nabla f$, $\nabla \cdot \vec{F}$ or $\nabla \times\vec{F}$ where the operator is treated notationally as if it were a vector. First I want to take issue with the final claim that it is purely a notational convenience. I think it is more than just a notational convenience for the following reason. It is possible, by following certain transformation rules, to express $\nabla$ in different coordinate systems, for example cylindrical or spherical. That might be fine, but there is a FURTHER point which makes me think $\nabla$ must be more than a notational convenience. if you express $\nabla$ in different coordinates you can then calculate something like $\nabla \cdot \vec{F}$ in the new coordinates and get the right answer. An answer which you could have arrived at by explicitly converting the cartesian expression for $\nabla \cdot \vec{F}$ into the new coordinate system. In other words, the $\nabla$ allows you to actually skip a step of calculation you would have had to do otherwise. This is evidence that the symbol carries some sort of mathematical structure to it which should be able to be captured in an independent definition. To that end I'm interested in a coordinate free definition of this symbol. The definition I gave above relies on using the usual Cartesian coordinates above. I have searched but haven't been able to find a coordinate free definition of the $\nabla$ symbol. Can one exist? In particular, I am interested in such a formula so that it is algebraically evident how one should calculate the components of $\nabla$ in any given coordinate system. Is there a coordinate free definition of $\nabla$? I am aware of a few complications with this endeavor that I'll just list here: 1) If this is to be some kind of vector or some kind of operator then it is not clear what space it should live in. For example, it is an object which can take a function $f$ and map it to a vector space. But at the same time it is an object which can be fed as an argument to a dot product together with a vector (form a different space) and return a scalar. 2) If I put on my differential geometry hat it becomes a very weird object. In differential geometry I come to think of vectors as actually being things like $\frac{\partial}{\partial x}$ and that $\vec{x}$ notation is eschewed. However the $\nabla$ symbol above contains both of these sitting next to each other. it's like a vector of vectors.. The idea of two vectors sitting next to eachother made me think it might be some kind of rank 2 contravariant tensor but I think that may have been a stretch. 3) I am aware that the cross product and curl operator are only defined in 3 dimensions so it does not need to be pointed out that that limits the possibility of defining such an operator for arbitrary dimension. I am happy to say we are working in 3 dimension. 4) I understand that the idea of divergence and curl depends on the presence of a metric for a space. Ok, that is fine. We can work in a space that has a metric defined on it. 5) Maybe the metric needs to be flat? Even that is fine as long as we can work in coordinate systems such as cylindrical or spherical where the metric is still flat but no longer has a trivial component representation. I am happy to restrict analysis to $\mathbb{R}^3$ if that is necessary. 6) Finally if such a definition truly cannot be formulated then could you at least answer why I can calculate BOTH $\nabla f$ and $\nabla \cdot \vec{F}$ by either 1) computing $\nabla f$ or $\nabla \cdot \vec{F}$ in xyz coordinates, then convert everything to spherical or 2) compute $\nabla$ in xyz coordinates, covert to spherical, then calculate $\nabla f$ and $\nabla \cdot \vec{F}$ and get the same answer in both cases? It just seems slightly too powerful/structured to be JUST a notational convenience. Here are a few other related questions: Is there a general formula for the del operator $\nabla$ in different coordinate systems? Can $\nabla$ be called a ""vector"" in any meaningful way? Coordinate transformation on del operator",,"['differential-geometry', 'notation', 'vectors']"
50,Straightening Theorem for Vector Fields,Straightening Theorem for Vector Fields,,"Let $M$ be a smooth manifold and $X\in\mathfrak{X}(M)$. The straightening theorem says: If $X_p\neq 0$, there is a chart $(U,y_1,...,y_n)$ around $p$ for which $X=\frac{\partial}{\partial y_1}$. The link above gives a proof  using a differential equation argument, but I've tried an alternative proof: Take a chart $(U,\phi)$ around $p$ with $U$ small enough so that $X|_U$ is never zero. In that neighbourhood, we can take a smooth local frame $\{X_1,...,X_n\}$, with $X_1=X$. Then:   $$X_j=\sum_{i=1}^na_{ij}\frac{\partial}{\partial \phi_i}$$   for some $a_{ij}\in C^\infty(U)$. Since $X_1,...,X_n$ are linearly independent, the matrix $(a_{ij})_{i,j}$ is invertible in $U$. In the domain $U$, define:    $$\psi:=(a_{ij})_{i,j}^{-1}\circ\phi$$   This function belongs to the maximal atlas, because for every $(V,\xi)$ with $U\cap V\neq \emptyset$, we have:   $$\psi\circ\xi^{-1}=(a_{ij})_{i,j}^{-1}\circ(\phi\circ\xi^{-1})\in C^{\infty}$$   $$\xi\circ\psi^{-1}=(\xi\circ\phi^{-1})\circ(a_{ij})_{i,j}\in C^{\infty}$$   Therefore $(U,\psi)$ is a chart which in particular satisfies $X=\frac{\partial}{\partial \psi_1}$.$_\blacksquare$ I can't see any mistake in this proof, but I've discovered some problems as a consequence of what I did. Using the same idea, if we have fields $X,Y$ which are not zero and linearly independent in some neighbourhood, then we could extend them to a local frame $\{X_1=X,X_2=Y,...,X_n\}$ and construct a similar $\psi$ for which $X=\frac{\partial}{\partial \psi_1},Y=\frac{\partial}{\partial \psi_2}$, but I've read that this is not possible, at least not for arbitrary $X,Y$. What am I missing?","Let $M$ be a smooth manifold and $X\in\mathfrak{X}(M)$. The straightening theorem says: If $X_p\neq 0$, there is a chart $(U,y_1,...,y_n)$ around $p$ for which $X=\frac{\partial}{\partial y_1}$. The link above gives a proof  using a differential equation argument, but I've tried an alternative proof: Take a chart $(U,\phi)$ around $p$ with $U$ small enough so that $X|_U$ is never zero. In that neighbourhood, we can take a smooth local frame $\{X_1,...,X_n\}$, with $X_1=X$. Then:   $$X_j=\sum_{i=1}^na_{ij}\frac{\partial}{\partial \phi_i}$$   for some $a_{ij}\in C^\infty(U)$. Since $X_1,...,X_n$ are linearly independent, the matrix $(a_{ij})_{i,j}$ is invertible in $U$. In the domain $U$, define:    $$\psi:=(a_{ij})_{i,j}^{-1}\circ\phi$$   This function belongs to the maximal atlas, because for every $(V,\xi)$ with $U\cap V\neq \emptyset$, we have:   $$\psi\circ\xi^{-1}=(a_{ij})_{i,j}^{-1}\circ(\phi\circ\xi^{-1})\in C^{\infty}$$   $$\xi\circ\psi^{-1}=(\xi\circ\phi^{-1})\circ(a_{ij})_{i,j}\in C^{\infty}$$   Therefore $(U,\psi)$ is a chart which in particular satisfies $X=\frac{\partial}{\partial \psi_1}$.$_\blacksquare$ I can't see any mistake in this proof, but I've discovered some problems as a consequence of what I did. Using the same idea, if we have fields $X,Y$ which are not zero and linearly independent in some neighbourhood, then we could extend them to a local frame $\{X_1=X,X_2=Y,...,X_n\}$ and construct a similar $\psi$ for which $X=\frac{\partial}{\partial \psi_1},Y=\frac{\partial}{\partial \psi_2}$, but I've read that this is not possible, at least not for arbitrary $X,Y$. What am I missing?",,"['differential-geometry', 'smooth-manifolds']"
51,"Neighbourhood of Linearly Independent Vector Fields $(X_1,\dots,X_n)$",Neighbourhood of Linearly Independent Vector Fields,"(X_1,\dots,X_n)","Actually, this is a piece of some argument i need to prove some proposition. I've been thinking this all day so i hope someone can help me with this. Suppose that we have smooth vector fields $(X_1,\dots,X_n)$ defined on some open subset $U\subset M$. At a point $p \in U$ we know that $(X_1|_p,\dots,X_n|_p)$ are linear independent, hence basis for $T_pM$. Can i find some neighbourhood $V $of $p$ in $U$ such that the $(X_1,\dots,X_n)$ are linear independent for all points in $V$ ? My idea is to use the determinant argument. That at $p$ the determinant matrix with entries consist of components of $X_i$'s at $p$, that is $det (X_1|_p,\dots,X_n|_p) = det (X^i_j(p))$ is not zero, so we have an open neighbourhood of $p$ such that $X_1,\dots,X_n$ are l.i there. Is this correct ? Any other idea to find such neighbourhood ? Thank you. $\textbf{Update : }$ After a while, i think this is true. This also can be proved using contradiction too. Pretty much by the same route. Only this time we assume on the contrary that, there is no such neighbourhood for $p$ and then deriving a contradiction. $\textbf{Proof }$ : Suppose $M$ is an $n$-dimensional smooth manifold and $(X_1,\dots,X_n)$ are smooth vector fields defined on some open subset $U\subseteq M$ such that for a point $p \in U$, $X_1(p),\dots,X_n(p)$ are linearly independent. We claim that there are no neighbourhood $V$ of $p$ such that for any point $x \in V$, then vectors $X_1(x),\dots,X_n(x)$ are linearly independent. Stated it differently, every neighbourhood of $p$ contain a point where the vectors $X_i$ are linearly dependent. This will lead to contradiction as follows : Choose a  smooth charts $(U',x^i)$ contain $p$. By shrinking $U'$, assume that $U'\subseteq U$. In this chart, the value of vector fields $X_i$ at any point $x \in U'$ is $$ X_i(x) = X_i^j(x) \frac{\partial}{\partial x^j}\bigg|_x. $$ Define a map from $m : U' \to M(n\times n,\mathbb{R} )$  defined by  $$ m : x \mapsto  \begin{pmatrix}     X_1^1(x) & \cdots & X_n^1(x) \\     \vdots & \ddots & \vdots \\     X_1^n(x) & \cdots & X_n^n(x) \end{pmatrix} \in M(n\times n,\mathbb{R} ). $$ This is a smooth map since the entries are smooth functions on $U'$. By composing this with determinant map $\text{det} : M(n\times n, \mathbb{R}) \to \mathbb{R}$, we have a smooth function $f = \text{det} \circ m : U' \to \mathbb{R}$. This function is also smooth and at $p \in U'$, $f(p) \neq 0$ since $X_1(p),\dots,X_n(p)$ are linearly independent vectors. Now, by assumption, any neighbourhood $V_1 \subseteq U'$ of $p$ contain a point $p_1$ such that $X_1(p_1),\cdots,X_n(p_1)$ are linearly dependent. Therefore $f(p_1) = 0$. By doing this repeatly, we have a sequence $(p_k)_{k=1}^{\infty}$ in $U'$ converging to $p$ such that $f(p_k) =0$ for all $k$. Since $f$ is smooth (hence continous), then as $p_k \to p$, then $f(p_k) \to f(p)$. But this is not happen since $f(p_k)= 0$ is a constant sequence and $f(p) \neq 0$. Therefore $f$ is not continous. Contradiction.","Actually, this is a piece of some argument i need to prove some proposition. I've been thinking this all day so i hope someone can help me with this. Suppose that we have smooth vector fields $(X_1,\dots,X_n)$ defined on some open subset $U\subset M$. At a point $p \in U$ we know that $(X_1|_p,\dots,X_n|_p)$ are linear independent, hence basis for $T_pM$. Can i find some neighbourhood $V $of $p$ in $U$ such that the $(X_1,\dots,X_n)$ are linear independent for all points in $V$ ? My idea is to use the determinant argument. That at $p$ the determinant matrix with entries consist of components of $X_i$'s at $p$, that is $det (X_1|_p,\dots,X_n|_p) = det (X^i_j(p))$ is not zero, so we have an open neighbourhood of $p$ such that $X_1,\dots,X_n$ are l.i there. Is this correct ? Any other idea to find such neighbourhood ? Thank you. $\textbf{Update : }$ After a while, i think this is true. This also can be proved using contradiction too. Pretty much by the same route. Only this time we assume on the contrary that, there is no such neighbourhood for $p$ and then deriving a contradiction. $\textbf{Proof }$ : Suppose $M$ is an $n$-dimensional smooth manifold and $(X_1,\dots,X_n)$ are smooth vector fields defined on some open subset $U\subseteq M$ such that for a point $p \in U$, $X_1(p),\dots,X_n(p)$ are linearly independent. We claim that there are no neighbourhood $V$ of $p$ such that for any point $x \in V$, then vectors $X_1(x),\dots,X_n(x)$ are linearly independent. Stated it differently, every neighbourhood of $p$ contain a point where the vectors $X_i$ are linearly dependent. This will lead to contradiction as follows : Choose a  smooth charts $(U',x^i)$ contain $p$. By shrinking $U'$, assume that $U'\subseteq U$. In this chart, the value of vector fields $X_i$ at any point $x \in U'$ is $$ X_i(x) = X_i^j(x) \frac{\partial}{\partial x^j}\bigg|_x. $$ Define a map from $m : U' \to M(n\times n,\mathbb{R} )$  defined by  $$ m : x \mapsto  \begin{pmatrix}     X_1^1(x) & \cdots & X_n^1(x) \\     \vdots & \ddots & \vdots \\     X_1^n(x) & \cdots & X_n^n(x) \end{pmatrix} \in M(n\times n,\mathbb{R} ). $$ This is a smooth map since the entries are smooth functions on $U'$. By composing this with determinant map $\text{det} : M(n\times n, \mathbb{R}) \to \mathbb{R}$, we have a smooth function $f = \text{det} \circ m : U' \to \mathbb{R}$. This function is also smooth and at $p \in U'$, $f(p) \neq 0$ since $X_1(p),\dots,X_n(p)$ are linearly independent vectors. Now, by assumption, any neighbourhood $V_1 \subseteq U'$ of $p$ contain a point $p_1$ such that $X_1(p_1),\cdots,X_n(p_1)$ are linearly dependent. Therefore $f(p_1) = 0$. By doing this repeatly, we have a sequence $(p_k)_{k=1}^{\infty}$ in $U'$ converging to $p$ such that $f(p_k) =0$ for all $k$. Since $f$ is smooth (hence continous), then as $p_k \to p$, then $f(p_k) \to f(p)$. But this is not happen since $f(p_k)= 0$ is a constant sequence and $f(p) \neq 0$. Therefore $f$ is not continous. Contradiction.",,"['differential-geometry', 'smooth-manifolds', 'vector-fields']"
52,Is the injectivity radius constant if the Riemannian manifold has constant curvature?,Is the injectivity radius constant if the Riemannian manifold has constant curvature?,,"Suppose $M$ is a 2-dimensional complete Riemannian manifold of constant curvature. Is it true that the injectivity radius $i(p)$ is constant for all $p\in M$? In all the examples I know, this is indeed the case. But I could not find the above result somewhere written. Does someone know the answer? Best regards","Suppose $M$ is a 2-dimensional complete Riemannian manifold of constant curvature. Is it true that the injectivity radius $i(p)$ is constant for all $p\in M$? In all the examples I know, this is indeed the case. But I could not find the above result somewhere written. Does someone know the answer? Best regards",,"['differential-geometry', 'riemannian-geometry']"
53,What is the origin of the terms 'jet' and 'prolongation' in differential geometry?,What is the origin of the terms 'jet' and 'prolongation' in differential geometry?,,I am just curious what is the reason for the terms 'jet' and 'prolongation' in differential geometry? Is there some mental imagery that these names are supposed to evoke? Or are they so-named because of some particular example that was later generalized? Or perhaps these names are completely arbitrary?,I am just curious what is the reason for the terms 'jet' and 'prolongation' in differential geometry? Is there some mental imagery that these names are supposed to evoke? Or are they so-named because of some particular example that was later generalized? Or perhaps these names are completely arbitrary?,,"['differential-geometry', 'terminology', 'smooth-manifolds', 'jet-bundles']"
54,Next book in learning Differential Geometry,Next book in learning Differential Geometry,,"I have just finished the book ""Manfredo P. do Carmo - Differential Geometry of Curves and Surfaces"". My aim is to reach to graduate level  to do research, but articles are not only too advanced to study after Carmo's book, but also I don't think that they are readable by just studying Carmo's book at all for a self-learner like me. Please someone tell me a book for Differential Geometry more advanced than Carmo's book but readable esp. for self-learning . Thanks a lot.","I have just finished the book ""Manfredo P. do Carmo - Differential Geometry of Curves and Surfaces"". My aim is to reach to graduate level  to do research, but articles are not only too advanced to study after Carmo's book, but also I don't think that they are readable by just studying Carmo's book at all for a self-learner like me. Please someone tell me a book for Differential Geometry more advanced than Carmo's book but readable esp. for self-learning . Thanks a lot.",,['differential-geometry']
55,Curvature tensors and bivectors,Curvature tensors and bivectors,,"At the beginning of the paper ""The curvature of 4-dimensional Einstein spaces"", by Singer and Thorpe, the authors define the space $\mathcal{R}$ of curvature tensors of the vector space $V$ as the set of symmetric bilinear transformations on the space of bivectors $\Lambda^{2}(V).$ Few lines after this, they define the ''Bianchi map'' $b$ as an operator $b : \mathcal{R} \rightarrow \mathcal{R}$ in the following way: $$[b(R)](u_{1},u_{2})u_{3}=\sum_{\sigma \in S_{n}} R(u_{\sigma(1)},u_{\sigma(2)})u_{\sigma(3)},$$ but they do not explain the notation. If $R$ is a transformation $$R: \Lambda^{2}(V) \rightarrow \Lambda^{2}(V),$$ and $u_{1},u_{2}$ and $u_{3}$ are, I presume,vectors of $V,$ what is the meaning of $R(u_{\sigma(1)},u_{\sigma(2)})u_{\sigma(3)}$? Also, they define immediately after this the ''Ricci contraction'' $r$ as an operator from $\mathcal{R}$ to the space of symmetric linear transformations of $V$ by means of: $$\langle r(R)(v),w \rangle=\mathrm{Tr}\{u \rightarrow R(v,u)w  \}.$$ I see that this strongly resembles to the Ricci tensor that one usually meets in riemannian geometry, but I have a similar notational problem with this last definition. If someone could possible clarify the notation and explain a little this way to look at the curvature tensor (or give me some references) I would be really grateful.","At the beginning of the paper ""The curvature of 4-dimensional Einstein spaces"", by Singer and Thorpe, the authors define the space $\mathcal{R}$ of curvature tensors of the vector space $V$ as the set of symmetric bilinear transformations on the space of bivectors $\Lambda^{2}(V).$ Few lines after this, they define the ''Bianchi map'' $b$ as an operator $b : \mathcal{R} \rightarrow \mathcal{R}$ in the following way: $$[b(R)](u_{1},u_{2})u_{3}=\sum_{\sigma \in S_{n}} R(u_{\sigma(1)},u_{\sigma(2)})u_{\sigma(3)},$$ but they do not explain the notation. If $R$ is a transformation $$R: \Lambda^{2}(V) \rightarrow \Lambda^{2}(V),$$ and $u_{1},u_{2}$ and $u_{3}$ are, I presume,vectors of $V,$ what is the meaning of $R(u_{\sigma(1)},u_{\sigma(2)})u_{\sigma(3)}$? Also, they define immediately after this the ''Ricci contraction'' $r$ as an operator from $\mathcal{R}$ to the space of symmetric linear transformations of $V$ by means of: $$\langle r(R)(v),w \rangle=\mathrm{Tr}\{u \rightarrow R(v,u)w  \}.$$ I see that this strongly resembles to the Ricci tensor that one usually meets in riemannian geometry, but I have a similar notational problem with this last definition. If someone could possible clarify the notation and explain a little this way to look at the curvature tensor (or give me some references) I would be really grateful.",,"['differential-geometry', 'riemannian-geometry', 'tensors', 'curvature']"
56,Are the torsion elements dense in every compact Lie group?,Are the torsion elements dense in every compact Lie group?,,Let $ G $ be a compact connected real Lie group. Denote by $ T $ its set of torsion elements. Is $ T $ always dense in $ G $?,Let $ G $ be a compact connected real Lie group. Denote by $ T $ its set of torsion elements. Is $ T $ always dense in $ G $?,,"['differential-geometry', 'lie-groups']"
57,Which textbook of differential geometry will introduce conformal transformation?,Which textbook of differential geometry will introduce conformal transformation?,,"Which textbook of differerntial geometry will have these formulas about conformal transformation? $$\tilde g_{ij} = e^{2\varphi}g_{ij}$$  $$\tilde \Gamma^k{}_{ij} = \Gamma^k{}_{ij}+ \delta^k_i\partial_j\varphi + \delta^k_j\partial_i\varphi-g_{ij}\nabla^k\varphi $$ $$\tilde R_{ijkl} = e^{2\varphi}\left( R_{ijkl} - \left[ g {~\wedge\!\!\!\!\!\!\bigcirc~} \left( \nabla\partial\varphi - \partial\varphi\partial\varphi + \frac{1}{2}\|\nabla\varphi\|^2g    \right)\right]_{ijkl}  \right)$$ $$\tilde R = e^{-2\varphi}\left[R + \frac{4(n-1)}{(n-2)}e^{-(n-2)\varphi/2}\triangle\left( e^{(n-2)\varphi/2} \right) \right] $$ I've read many textbooks about differential geometry, such as Do Carmo, Kobayshi, Novikov and so on. But I never found these formulas. Who can give me a reference about these formulas. Thanks!","Which textbook of differerntial geometry will have these formulas about conformal transformation? $$\tilde g_{ij} = e^{2\varphi}g_{ij}$$  $$\tilde \Gamma^k{}_{ij} = \Gamma^k{}_{ij}+ \delta^k_i\partial_j\varphi + \delta^k_j\partial_i\varphi-g_{ij}\nabla^k\varphi $$ $$\tilde R_{ijkl} = e^{2\varphi}\left( R_{ijkl} - \left[ g {~\wedge\!\!\!\!\!\!\bigcirc~} \left( \nabla\partial\varphi - \partial\varphi\partial\varphi + \frac{1}{2}\|\nabla\varphi\|^2g    \right)\right]_{ijkl}  \right)$$ $$\tilde R = e^{-2\varphi}\left[R + \frac{4(n-1)}{(n-2)}e^{-(n-2)\varphi/2}\triangle\left( e^{(n-2)\varphi/2} \right) \right] $$ I've read many textbooks about differential geometry, such as Do Carmo, Kobayshi, Novikov and so on. But I never found these formulas. Who can give me a reference about these formulas. Thanks!",,"['reference-request', 'differential-geometry', 'riemannian-geometry', 'conformal-geometry', 'online-resources']"
58,understanding of the first fundamental form,understanding of the first fundamental form,,"The following is an excerpt from do Carmo's Differential Geometry of Curves and Surfaces about the first fundamental form: I don't understand what ""without further references to the ambient space ${\Bbb R}^3$"" means. When ""treating metric questions on a regular surface"", what does it mean by with references to the ambient space ${\Bbb R}^3$? without references to the ambient space ${\Bbb R}^3$? How does the example of calculating the arc length of a curve illustrate the difference of the two different ways above?","The following is an excerpt from do Carmo's Differential Geometry of Curves and Surfaces about the first fundamental form: I don't understand what ""without further references to the ambient space ${\Bbb R}^3$"" means. When ""treating metric questions on a regular surface"", what does it mean by with references to the ambient space ${\Bbb R}^3$? without references to the ambient space ${\Bbb R}^3$? How does the example of calculating the arc length of a curve illustrate the difference of the two different ways above?",,[]
59,Prove that the action of a Lie group on its Lie algebra via the adjoint representation reads $\mathrm{ad}(g)(X)=g^{-1}Xg$,Prove that the action of a Lie group on its Lie algebra via the adjoint representation reads,\mathrm{ad}(g)(X)=g^{-1}Xg,"I am a physics undergrad. The adjoint action of a group on itself is $\operatorname{Ad}: G \times G \to G$ is defined to be $\operatorname{Ad}:(g,h) \to g^{-1}hg$. The adjoint representation of the group  is the action of the group on its lie algeba, and is defined as the derivative of this map at the identity element, i.e. $\mathrm{D}\operatorname{Ad}_1=:\operatorname{ad}:G\to\operatorname{GL}(\mathfrak{g})$. How do I prove that for matrix lie groups, this map is actually $\operatorname{ad}(g)(X)=g^{-1} X g $ for $X \in \mathfrak{g}$. What is the linear map $\operatorname{ad}$ in general? I am sorry if this question is really trivial, but all the math books are inaccessible to me, and I couldn't find this in any physics book.","I am a physics undergrad. The adjoint action of a group on itself is $\operatorname{Ad}: G \times G \to G$ is defined to be $\operatorname{Ad}:(g,h) \to g^{-1}hg$. The adjoint representation of the group  is the action of the group on its lie algeba, and is defined as the derivative of this map at the identity element, i.e. $\mathrm{D}\operatorname{Ad}_1=:\operatorname{ad}:G\to\operatorname{GL}(\mathfrak{g})$. How do I prove that for matrix lie groups, this map is actually $\operatorname{ad}(g)(X)=g^{-1} X g $ for $X \in \mathfrak{g}$. What is the linear map $\operatorname{ad}$ in general? I am sorry if this question is really trivial, but all the math books are inaccessible to me, and I couldn't find this in any physics book.",,"['differential-geometry', 'representation-theory']"
60,What exactly is a vector bundle isomorphism,What exactly is a vector bundle isomorphism,,"Recall that a vector bundle (of rank $n$) is a family of vector spaces $V_x$ of dimension $n$ that is parameterized by a topological space $X$. In addition there is a continuous surjective map $\pi: E \to X$ where $E$ is also a topological space. On top of this it is required that for every $x \in X$ there be an open set $x \in U$, some $k$ and a homeomorphism $\varphi: U \times \mathbb R^k \to \pi^{-1}(U)$. (local triviality condition) Two vector bundles are isomorphic if there is a homeomorphism on the total spaces $\phi : E \to E'$ with the property that $\phi|_{\pi^{-1}(x)}: \pi^{-1}(x) \to \pi'^{-1}(x)$ is a linear isomorphism. Consider the example $X = S^1, E = \mathbb R^2$, $k=1$. From reading about vector bundles I know that there exist only two non-isomorphic vector bundles: the Moebius band and the annulus. But why this is so I don't understand: it's not clear to me why these two can't be isomorphic given the definition above and I also don't see why there are no others. To illustrate why I don't understand why there can't be others consider the vector bundle that ""twists around"" twice. (If the Moebius band is ""twisting around"" once and the annulus zero times). It's not clear to me why this is isomorphic to either the annulus or the Moebius band. Also why is it a twist that distinguishes isomorphic bundles? Slightly warping the fibres still leaves the bundles isomorphic: how does it work?","Recall that a vector bundle (of rank $n$) is a family of vector spaces $V_x$ of dimension $n$ that is parameterized by a topological space $X$. In addition there is a continuous surjective map $\pi: E \to X$ where $E$ is also a topological space. On top of this it is required that for every $x \in X$ there be an open set $x \in U$, some $k$ and a homeomorphism $\varphi: U \times \mathbb R^k \to \pi^{-1}(U)$. (local triviality condition) Two vector bundles are isomorphic if there is a homeomorphism on the total spaces $\phi : E \to E'$ with the property that $\phi|_{\pi^{-1}(x)}: \pi^{-1}(x) \to \pi'^{-1}(x)$ is a linear isomorphism. Consider the example $X = S^1, E = \mathbb R^2$, $k=1$. From reading about vector bundles I know that there exist only two non-isomorphic vector bundles: the Moebius band and the annulus. But why this is so I don't understand: it's not clear to me why these two can't be isomorphic given the definition above and I also don't see why there are no others. To illustrate why I don't understand why there can't be others consider the vector bundle that ""twists around"" twice. (If the Moebius band is ""twisting around"" once and the annulus zero times). It's not clear to me why this is isomorphic to either the annulus or the Moebius band. Also why is it a twist that distinguishes isomorphic bundles? Slightly warping the fibres still leaves the bundles isomorphic: how does it work?",,['differential-geometry']
61,Normal subgroup and Lie algebra,Normal subgroup and Lie algebra,,"I have an exercise of Lie group as follows: ""Let $G,H$ be closed connected subgroup of $GL_n(\mathbb{R})$ , and $H$ be subgoup of $G$ . Suppose that $\operatorname{Lie}(H)$ is an ideal of $\operatorname{Lie}(G)$ . Prove that $H$ is a normal subgroup of $G$ ."" I get stuck to solve this problem. Also I have no idea to use the connectedness of $G$ and $H$ . Some one can help me? Thanks a lot!","I have an exercise of Lie group as follows: ""Let be closed connected subgroup of , and be subgoup of . Suppose that is an ideal of . Prove that is a normal subgroup of ."" I get stuck to solve this problem. Also I have no idea to use the connectedness of and . Some one can help me? Thanks a lot!","G,H GL_n(\mathbb{R}) H G \operatorname{Lie}(H) \operatorname{Lie}(G) H G G H","['differential-geometry', 'lie-groups']"
62,"Determining the ""positivity"" or ""negativity"" of Chern class (number?) of zero-sets of homogeneous polynomials","Determining the ""positivity"" or ""negativity"" of Chern class (number?) of zero-sets of homogeneous polynomials",,"If $\Omega$ is the curvature 2-form on a $n-$manifold, then I would think that the Chern classes (forms), $c_k$ are defined as, $det(I + \frac{it\Omega}{2\pi}) = \sum c_k t^k$ I would like to know of a local coordinate expression for the above so that I can clearly see as to how each $c_k$ on the RHS turns out to be a rank $2k$ form. (..naively it seems that one has to think of the determinant on the LHS to be that of a ""matrix"" each of whose entries are 2-forms themselves and the probably the ""multiplication"" that is being done among the elements of the matrix to evaluate the determinant is taking a wedge product -- but I would like to know of something explicit..) What does it mean when one talks of a Chern class being positive or negative? Is there an invariant meaning to may be the integral over the manifold of that form? One particular case which I am interested in is this, Consider the zero-set in $\mathbb{CP}^n$ of a homogeneous degree $k$ polynomial in $n$ variables. Firstly when is it guaranteed that this is going to be a manifold? Now how does one understand that the ""sign"" of the Chern class (whetever that means) depends on whether $k> n$ or $k<n$? (..I often see the claim that for $k=n$ the zero-set is a Calabi-Yau manifold since then the first Chern class vanishes..)","If $\Omega$ is the curvature 2-form on a $n-$manifold, then I would think that the Chern classes (forms), $c_k$ are defined as, $det(I + \frac{it\Omega}{2\pi}) = \sum c_k t^k$ I would like to know of a local coordinate expression for the above so that I can clearly see as to how each $c_k$ on the RHS turns out to be a rank $2k$ form. (..naively it seems that one has to think of the determinant on the LHS to be that of a ""matrix"" each of whose entries are 2-forms themselves and the probably the ""multiplication"" that is being done among the elements of the matrix to evaluate the determinant is taking a wedge product -- but I would like to know of something explicit..) What does it mean when one talks of a Chern class being positive or negative? Is there an invariant meaning to may be the integral over the manifold of that form? One particular case which I am interested in is this, Consider the zero-set in $\mathbb{CP}^n$ of a homogeneous degree $k$ polynomial in $n$ variables. Firstly when is it guaranteed that this is going to be a manifold? Now how does one understand that the ""sign"" of the Chern class (whetever that means) depends on whether $k> n$ or $k<n$? (..I often see the claim that for $k=n$ the zero-set is a Calabi-Yau manifold since then the first Chern class vanishes..)",,"['algebraic-geometry', 'algebraic-topology', 'differential-geometry', 'riemannian-geometry', 'characteristic-classes']"
63,Basis of the space of vector fields on smooth manifolds,Basis of the space of vector fields on smooth manifolds,,"Let $M$ be a smooth ( $C^\infty$ ) manifold. Let $\mathfrak{X}(M)$ be a set of all vector fields on $M$ and let $\mathfrak{F}(M)$ be a set of all real smooth functions on $M$ . $\mathfrak{X}(M)$ is a real vector space and it is also a module over $\mathfrak{F}(M)$ . We know that partial derivatives constitute a basis for tangent space at any point $p\in M$ . Is there some sort of basis for $\mathfrak{X}(M)$ (as a vector space or as a module)? Do partial derivatives constitute a basis here as well? I think this question Basis of vector fields on manifold is similar to mine, but because of the way it's written, I'm not really sure I understand the question and I'm not sure we're asking about the same thing.","Let be a smooth ( ) manifold. Let be a set of all vector fields on and let be a set of all real smooth functions on . is a real vector space and it is also a module over . We know that partial derivatives constitute a basis for tangent space at any point . Is there some sort of basis for (as a vector space or as a module)? Do partial derivatives constitute a basis here as well? I think this question Basis of vector fields on manifold is similar to mine, but because of the way it's written, I'm not really sure I understand the question and I'm not sure we're asking about the same thing.",M C^\infty \mathfrak{X}(M) M \mathfrak{F}(M) M \mathfrak{X}(M) \mathfrak{F}(M) p\in M \mathfrak{X}(M),"['differential-geometry', 'smooth-manifolds', 'vector-fields', 'tangent-spaces', 'tangent-bundle']"
64,"Simple proof of ""Diffeomorphism and distance preserving implies isometry""","Simple proof of ""Diffeomorphism and distance preserving implies isometry""",,"For a Riemannian manifold $(M,g)$ , if $F: M\to M$ is a diffeomorphism and preserves distances, I would like to show that $F$ is an isometry. By ""distance"" I mean $d(x,y)=\inf \int_0^1 |\gamma'(t)| dt$ where the infemum is taken over all admissible curves $\gamma$ (with $\gamma(0)=x$ and $\gamma(1)=y$ ). By ""isometry"", I mean that each differential $dF_p : T_p M\to T_{F(p)} M$ is a linear isometry. I know that this is true (at least with mild assumptions on $M$ , like possibly connectedness), but I am not sure if there is a ""simple"" proof of it. For instance, problem 7.7 in Lee's ""Introduction to Riemannian Geometry"" asks you to show that a homeomorphism that is a metric space isometry is an isometry (so clearly $F$ falls into this category), but the proof is very long. Does anyone know if there is a simple way of showing this?","For a Riemannian manifold , if is a diffeomorphism and preserves distances, I would like to show that is an isometry. By ""distance"" I mean where the infemum is taken over all admissible curves (with and ). By ""isometry"", I mean that each differential is a linear isometry. I know that this is true (at least with mild assumptions on , like possibly connectedness), but I am not sure if there is a ""simple"" proof of it. For instance, problem 7.7 in Lee's ""Introduction to Riemannian Geometry"" asks you to show that a homeomorphism that is a metric space isometry is an isometry (so clearly falls into this category), but the proof is very long. Does anyone know if there is a simple way of showing this?","(M,g) F: M\to M F d(x,y)=\inf \int_0^1 |\gamma'(t)| dt \gamma \gamma(0)=x \gamma(1)=y dF_p : T_p M\to T_{F(p)} M M F","['differential-geometry', 'manifolds', 'riemannian-geometry', 'isometry']"
65,Proof: A tangent space of the manifold of SPD matrices is the set of symmetric matrices,Proof: A tangent space of the manifold of SPD matrices is the set of symmetric matrices,,"The set of SPD matrices, $\mathbb{P}_n := \{X \in \mathbb{R}^{n \times n} | X=X^T, X \succ 0 \}   $ , forms a differentiable manifold. Claim: The tangent space at a point $A$ , $T_A\mathbb{P}_n$ , is the space of symmetric matrices. Info: I have seen this mentioned in a paper ( https://www.ncbi.nlm.nih.gov/pubmed/27845666 ), but (with my limited understanding of differential geometry) can't see how this occurs (although it may actualy be obvious to someone with a more rigorous maths education). I have also checked out the book they referenced when making this claim ( http://www.cmat.edu.uy/~lessa/tesis/Positive%20Definite%20Matrices.pdf ) in order to understand it. Sadly the section that I think they are referencing (Start of chapter 6) is far too formal for me. For completeness I will write out what is written there, and if it is the proof I am looking for then an explanation of some of the jumps in the steps would answer my question. The space $\mathbb{M}_n$ is a Hilbert space with inner product $ \langle A,B \rangle = tr A^*B $ and the associated norm $\vert \vert A \vert \vert _2 = (tr A^*A)^{\frac{1}{2}}$ The set of Hermitian matrices constitutes a real vector space $\mathbb{H}_n$ in $\mathbb{M}_n$ . The subset $\mathbb{P}_n$ consisting of strictly positive matrices is an open subset in $\mathbb{H}_n$ . Hence it is a differentiable manifold. The tangent space to $\mathbb{P}_n$ at any of its points A is the space $T_A\mathbb{P}_n = \{A\} \times \mathbb{H}_n$ If this is indeed the proof that I seek then these are the steps that I don't understand: I know that open subsets of manifolds are manifolds, and I am also presuming that ""stricly positive"" means positive definite. But how do I know that this manifold is differentiable, ie has differentiable transistion maps? This is my main confusion - $\mathbb{H}_n$ is the set of symmetric matrices, but why is the tangent space at any point defined like this? I understand the tangent space to be the collection of velocity vectors through that point. Why can I represent the gradients of all the geodesics that pass through a point like this? EDIT - UPDATE: Having been browsing the stack I also came across tangent space clarification where someone is asking why tangent spaces aren't just defined in the same way (ish) as they were in 4) - does anyone know where this definition/way of writing has come from?","The set of SPD matrices, , forms a differentiable manifold. Claim: The tangent space at a point , , is the space of symmetric matrices. Info: I have seen this mentioned in a paper ( https://www.ncbi.nlm.nih.gov/pubmed/27845666 ), but (with my limited understanding of differential geometry) can't see how this occurs (although it may actualy be obvious to someone with a more rigorous maths education). I have also checked out the book they referenced when making this claim ( http://www.cmat.edu.uy/~lessa/tesis/Positive%20Definite%20Matrices.pdf ) in order to understand it. Sadly the section that I think they are referencing (Start of chapter 6) is far too formal for me. For completeness I will write out what is written there, and if it is the proof I am looking for then an explanation of some of the jumps in the steps would answer my question. The space is a Hilbert space with inner product and the associated norm The set of Hermitian matrices constitutes a real vector space in . The subset consisting of strictly positive matrices is an open subset in . Hence it is a differentiable manifold. The tangent space to at any of its points A is the space If this is indeed the proof that I seek then these are the steps that I don't understand: I know that open subsets of manifolds are manifolds, and I am also presuming that ""stricly positive"" means positive definite. But how do I know that this manifold is differentiable, ie has differentiable transistion maps? This is my main confusion - is the set of symmetric matrices, but why is the tangent space at any point defined like this? I understand the tangent space to be the collection of velocity vectors through that point. Why can I represent the gradients of all the geodesics that pass through a point like this? EDIT - UPDATE: Having been browsing the stack I also came across tangent space clarification where someone is asking why tangent spaces aren't just defined in the same way (ish) as they were in 4) - does anyone know where this definition/way of writing has come from?","\mathbb{P}_n := \{X \in \mathbb{R}^{n \times n} | X=X^T, X \succ 0 \}    A T_A\mathbb{P}_n \mathbb{M}_n  \langle A,B \rangle = tr A^*B  \vert \vert A \vert \vert _2 = (tr A^*A)^{\frac{1}{2}} \mathbb{H}_n \mathbb{M}_n \mathbb{P}_n \mathbb{H}_n \mathbb{P}_n T_A\mathbb{P}_n = \{A\} \times \mathbb{H}_n \mathbb{H}_n","['differential-geometry', 'proof-writing', 'manifolds', 'proof-explanation', 'positive-definite']"
66,Is the pullback of differential geometry also a pull back in the sense of category theory?,Is the pullback of differential geometry also a pull back in the sense of category theory?,,"I do think that the pullback of differential geometry is also a pullback in the sense of category theory, however I did have some trouble completely justifying this thought. This is were I got: Lets consider a vector field $\eta : Y \rightarrow T(Y)$ and a diffeomorphism $f: X \rightarrow Y$. Then from differential geometry the pullback can be written as, $$ f^*(\eta)(x) = Tf^{-1}\circ \eta \circ f(x).$$ Thus the commutative diagram becomes: Now since $Tf^{-1}$ is invertible, we can use $Tf$ to write commutative diagram as a fiber product. From category theory we can complete the diagram of the vector field $\eta: Y \rightarrow T(Y)$ and the tangent map $Tf: T(X)\rightarrow T(Y)$ to obtain Now due to the uniqueness of the fiber product up to isomorphism we see that $Y \times_{T(Y)} T(X)$ equals $X$ . Also $\eta^*(Tf)$ should equal $f$ (up to isomorphism). But then $f^*\eta$ should equal $Tf^*\eta$, however the notation $f^*\eta$ does not make much sense to me, is this just a short hand? If this isn't the case can someone give me somewhat more information about $ f^*\eta$ in perspective of category theory? Thanks in advance.","I do think that the pullback of differential geometry is also a pullback in the sense of category theory, however I did have some trouble completely justifying this thought. This is were I got: Lets consider a vector field $\eta : Y \rightarrow T(Y)$ and a diffeomorphism $f: X \rightarrow Y$. Then from differential geometry the pullback can be written as, $$ f^*(\eta)(x) = Tf^{-1}\circ \eta \circ f(x).$$ Thus the commutative diagram becomes: Now since $Tf^{-1}$ is invertible, we can use $Tf$ to write commutative diagram as a fiber product. From category theory we can complete the diagram of the vector field $\eta: Y \rightarrow T(Y)$ and the tangent map $Tf: T(X)\rightarrow T(Y)$ to obtain Now due to the uniqueness of the fiber product up to isomorphism we see that $Y \times_{T(Y)} T(X)$ equals $X$ . Also $\eta^*(Tf)$ should equal $f$ (up to isomorphism). But then $f^*\eta$ should equal $Tf^*\eta$, however the notation $f^*\eta$ does not make much sense to me, is this just a short hand? If this isn't the case can someone give me somewhat more information about $ f^*\eta$ in perspective of category theory? Thanks in advance.",,"['differential-geometry', 'category-theory']"
67,How to intepret cotangent laplacian?,How to intepret cotangent laplacian?,,I have the above slide in my lecture notes. The question is why do people define such things? How is it useful?,I have the above slide in my lecture notes. The question is why do people define such things? How is it useful?,,"['differential-geometry', 'computational-geometry']"
68,Show that there exists a smooth map $F: M \to M$ that is homotopic to the identity and has no fixed points.,Show that there exists a smooth map  that is homotopic to the identity and has no fixed points.,F: M \to M,"Question (Problem 9-5 of Lee's Introduction to Smooth Manifolds ): Suppose $M$ is a smooth, compact Manifold that admits a nowhere vanishing smooth vector field. Show that there exists a smooth map $F: M \to M$ that is homotopic to the identity and has no fixed points. My try: Let $V$ be the non-vanishing smooth vector field. Since $M$ is compact, $V$ is complete. Hence there is a global flow $\theta: \mathbb{R} \times M \to M$ such that for all $t,s \in \mathbb{R}$ and $p \in M ,\theta (t,\theta(s,p))=\theta(t+s,p)$ and $\theta(0,p)=p.$ Since $p\in M$ is a regular point of $V$ , there exists a neighbourhood $U_p$ in which $V$ has coordinate representation $\frac{\partial}{\partial s^1}$ . Choose a smaller neighbourhood $V_p$ such that $\theta(t,x)=x+(t,0,\ldots)$ for all $0 \le t \le t_p.$ Since $\{V_p\}_{p \in M}$ is a open cover for $M$ and $M$ is compact, there exists a smooth subcover say $\{V_{p_1},\ldots,V_{p_n}\}$ . Let $T=\min \{t_{p_1},\ldots,t_{p_n}\}.$ Then $\theta_T: M \to M$ is a smooth map which has no fixed points and $H:M \times I \to M$ given by $H(x,t)=\theta(tT,x)$ is the required homotopy to identity. I have a little issue here. I am uncertain as how can I choose the neighborhoods $V_p$ the way I did. Intuitively it seems to be true though. Thanks for the help!","Question (Problem 9-5 of Lee's Introduction to Smooth Manifolds ): Suppose is a smooth, compact Manifold that admits a nowhere vanishing smooth vector field. Show that there exists a smooth map that is homotopic to the identity and has no fixed points. My try: Let be the non-vanishing smooth vector field. Since is compact, is complete. Hence there is a global flow such that for all and and Since is a regular point of , there exists a neighbourhood in which has coordinate representation . Choose a smaller neighbourhood such that for all Since is a open cover for and is compact, there exists a smooth subcover say . Let Then is a smooth map which has no fixed points and given by is the required homotopy to identity. I have a little issue here. I am uncertain as how can I choose the neighborhoods the way I did. Intuitively it seems to be true though. Thanks for the help!","M F: M \to M V M V \theta: \mathbb{R} \times M \to M t,s \in \mathbb{R} p \in M ,\theta (t,\theta(s,p))=\theta(t+s,p) \theta(0,p)=p. p\in M V U_p V \frac{\partial}{\partial s^1} V_p \theta(t,x)=x+(t,0,\ldots) 0 \le t \le t_p. \{V_p\}_{p \in M} M M \{V_{p_1},\ldots,V_{p_n}\} T=\min \{t_{p_1},\ldots,t_{p_n}\}. \theta_T: M \to M H:M \times I \to M H(x,t)=\theta(tT,x) V_p","['differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds']"
69,"Reference request: Best way of studying Loring Tu's ""An Introduction to Manifolds"" incompletely, but with restrictions","Reference request: Best way of studying Loring Tu's ""An Introduction to Manifolds"" incompletely, but with restrictions",,"Motivation of question ahead: I have been accepted to study a Master's degree in pure mathematics at an overseas university starting in August. There are quite a few courses that I am interested in taking there that require prerequisites that I do not yet have. To rectify that I am taking a course in ring and module theory and another in abstract analysis. Unfortunately, however, no one at my institution is offering any course on differential geometry/differential topology, which I also need to learn before I go. To rectify this I've initiated a study group at the university, where we basically work through Tu's book . Unfortunately I am very busy with the other courses mentioned, and with teaching duties, so the pace is only a meagre 10-15 pages a week. This is a problem, because when I arrive at the new institution I'm expected to know the following: ""The notion of manifold, smooth maps, immersions and submersions, tangent vectors, Lie derivatives along vector fields, the flow of a vector field, the tangent space (and bundle), the definition of differential forms, de Rham operator (and hopefully the definition of de Rham cohomology),"" But by my calculations, I will not be able to linearly study the book until I reach the de Rham cohomology. This means that I am going to have to, unfortunately, skip some sections for now. Actual Question: The content page for the book is very detailed (available on the linked Amazon page), and I am hoping that someone can tell me what sections I can safely leave out without excessively hampering my ability to understand the concepts specifically mentioned above, and without interrupting the flow of ideas terribly. Thanks in advance. P.S. I realise that this question does not really comply with the guidelines set out for asking questions, but I am not sure where else to go to find the information I am seeking, and I do require this information for non-trivial reasons. So if you are going to vote to close, I'd very much appreciate guidance as to where I should go to find an answer for this question instead (besides at my university, as I am already trying that route concurrently, but our department has very few people knowledgeable enough in the area of differential geometry to be able to answer this question it seems.)","Motivation of question ahead: I have been accepted to study a Master's degree in pure mathematics at an overseas university starting in August. There are quite a few courses that I am interested in taking there that require prerequisites that I do not yet have. To rectify that I am taking a course in ring and module theory and another in abstract analysis. Unfortunately, however, no one at my institution is offering any course on differential geometry/differential topology, which I also need to learn before I go. To rectify this I've initiated a study group at the university, where we basically work through Tu's book . Unfortunately I am very busy with the other courses mentioned, and with teaching duties, so the pace is only a meagre 10-15 pages a week. This is a problem, because when I arrive at the new institution I'm expected to know the following: ""The notion of manifold, smooth maps, immersions and submersions, tangent vectors, Lie derivatives along vector fields, the flow of a vector field, the tangent space (and bundle), the definition of differential forms, de Rham operator (and hopefully the definition of de Rham cohomology),"" But by my calculations, I will not be able to linearly study the book until I reach the de Rham cohomology. This means that I am going to have to, unfortunately, skip some sections for now. Actual Question: The content page for the book is very detailed (available on the linked Amazon page), and I am hoping that someone can tell me what sections I can safely leave out without excessively hampering my ability to understand the concepts specifically mentioned above, and without interrupting the flow of ideas terribly. Thanks in advance. P.S. I realise that this question does not really comply with the guidelines set out for asking questions, but I am not sure where else to go to find the information I am seeking, and I do require this information for non-trivial reasons. So if you are going to vote to close, I'd very much appreciate guidance as to where I should go to find an answer for this question instead (besides at my university, as I am already trying that route concurrently, but our department has very few people knowledgeable enough in the area of differential geometry to be able to answer this question it seems.)",,"['differential-geometry', 'reference-request', 'manifolds', 'differential-topology']"
70,Principal and Associated Bundles in Hermitian Yang-Mills?,Principal and Associated Bundles in Hermitian Yang-Mills?,,"So I have a good appreciation of the non-abelian Yang-Mills story.  We take a Riemannian manifold $M$ and a symmetry group $G$ giving rise to an infinite-dimensional gauge group $\mathcal{G}$.  In addition, we take a principal $G$-bundle $P$ on $M$, with $\mathcal{A}$ the infinite-dimensional affine space of connections on $P$.  Remarkably, $\mathcal{A}/\mathcal{G}$ is a finite dimensional space (not in general, but for example in Yang-Mills theory on Riemann surfaces, it's finite dimensional), and the Yang-Mills functional $\rm{YM}(\cdot)$ is a map from this space to $\mathbb{R}_{\geq 0}$, whose critical points satisfy the Yang-Mills equations.  Then of course, we have the instantons which are self-dual and anti-self dual connections.  This part of the story I understand well (I hope!) but I could use some assistance when it comes to the Hermitian Yang-Mills story. Here, people start discussing a holomorphic vector bundle $E$ over $M$.  Well, certainly this isn't the $G$-bundle $P$, and I can only guess that it's meant to be the associated vector bundle to $P$?  But doesn't this require picking a particular representation of the group $G$?  Moreover, the Hermitian Yang-Mills equation is a second order PDE for the curvature $F$, which seems to be the curvature of a connection on $E$.  How does this relate to curvatures of connections in $\mathcal{A}$, if at all?  I've heard Kobayashi-Hitchin relates instantons to stable holomorphic vector bundles, so I think they must be related, but I'm not sure how.","So I have a good appreciation of the non-abelian Yang-Mills story.  We take a Riemannian manifold $M$ and a symmetry group $G$ giving rise to an infinite-dimensional gauge group $\mathcal{G}$.  In addition, we take a principal $G$-bundle $P$ on $M$, with $\mathcal{A}$ the infinite-dimensional affine space of connections on $P$.  Remarkably, $\mathcal{A}/\mathcal{G}$ is a finite dimensional space (not in general, but for example in Yang-Mills theory on Riemann surfaces, it's finite dimensional), and the Yang-Mills functional $\rm{YM}(\cdot)$ is a map from this space to $\mathbb{R}_{\geq 0}$, whose critical points satisfy the Yang-Mills equations.  Then of course, we have the instantons which are self-dual and anti-self dual connections.  This part of the story I understand well (I hope!) but I could use some assistance when it comes to the Hermitian Yang-Mills story. Here, people start discussing a holomorphic vector bundle $E$ over $M$.  Well, certainly this isn't the $G$-bundle $P$, and I can only guess that it's meant to be the associated vector bundle to $P$?  But doesn't this require picking a particular representation of the group $G$?  Moreover, the Hermitian Yang-Mills equation is a second order PDE for the curvature $F$, which seems to be the curvature of a connection on $E$.  How does this relate to curvatures of connections in $\mathcal{A}$, if at all?  I've heard Kobayashi-Hitchin relates instantons to stable holomorphic vector bundles, so I think they must be related, but I'm not sure how.",,"['algebraic-geometry', 'differential-geometry']"
71,References on the moduli space of flat connections as a symplectic reduction,References on the moduli space of flat connections as a symplectic reduction,,"In their Yang Mills equations over Riemann surfaces paper, Atiyah & Bott famously remark that the moduli space of flat connections on a principal bundle over a compact orientable surface may be obtained as a symplectic reduction on the space of connections (with the curvature as the moment map). In that paper, however, it is only a passing remark. One must be careful about this construction since the spaces involved are infinite-dimensional (e.g. the space of connections). I've only found references which ignore this technical detail and present the ideas in analogy to the finite-dimensional case. While they are helpful to give a simple understanding of what's going on, I was wondering if there is a reference that actually goes through the infinite-dimensional analysis to formalize this process. Any recommendation is appreciated.","In their Yang Mills equations over Riemann surfaces paper, Atiyah & Bott famously remark that the moduli space of flat connections on a principal bundle over a compact orientable surface may be obtained as a symplectic reduction on the space of connections (with the curvature as the moment map). In that paper, however, it is only a passing remark. One must be careful about this construction since the spaces involved are infinite-dimensional (e.g. the space of connections). I've only found references which ignore this technical detail and present the ideas in analogy to the finite-dimensional case. While they are helpful to give a simple understanding of what's going on, I was wondering if there is a reference that actually goes through the infinite-dimensional analysis to formalize this process. Any recommendation is appreciated.",,"['differential-geometry', 'reference-request', 'symplectic-geometry']"
72,What does the case of $\operatorname{Spec}C^{\infty}(M)$ tell us about the relavance of scheme theory to general rings?,What does the case of  tell us about the relavance of scheme theory to general rings?,\operatorname{Spec}C^{\infty}(M),"I guess a lot of people with previous exposure to differential geometry have had this naive question pop out in their mind when studying schemes for the first time. The category of compact smooth real manifolds is contra-equivalent to the category of smooth rings on them $C^{\infty} (-)$. The inverse functor to the global section functor takes maximal ideals and makes sheafs out of the smooth rings. By a partion of unity argument the ring of global sections determines the sheaf so wer'e good. (At least, I hope we are. It's not something I found written explicitly in any book). On the other hand, the famous result for schemes gives the equivalence $\mathsf {Aff} \cong (\mathsf{Ring})^{op}$. This suggests that the correct and uniform way to think about rings geometrically is studying the functor $Spec$. There are many reasons for why studying $\operatorname{Spec}C^{\infty}(M)$ Is not so fruitfull. Elaboration on this in answers would be welcome as well, though my question is a more philosophical one. Question: Given what we know about the unsuitability of scheme theory for the study of smooth rings why should we believe that it's the right generalization for algebraic geometry over arbitrary rings? Personally, (and I hope It's okay to express my opinion despite my ignonrance) I think this is a good argument for studying general locally ringed spaces (Of which schemes are a part of but not at the center necessarily)... What do you think?","I guess a lot of people with previous exposure to differential geometry have had this naive question pop out in their mind when studying schemes for the first time. The category of compact smooth real manifolds is contra-equivalent to the category of smooth rings on them $C^{\infty} (-)$. The inverse functor to the global section functor takes maximal ideals and makes sheafs out of the smooth rings. By a partion of unity argument the ring of global sections determines the sheaf so wer'e good. (At least, I hope we are. It's not something I found written explicitly in any book). On the other hand, the famous result for schemes gives the equivalence $\mathsf {Aff} \cong (\mathsf{Ring})^{op}$. This suggests that the correct and uniform way to think about rings geometrically is studying the functor $Spec$. There are many reasons for why studying $\operatorname{Spec}C^{\infty}(M)$ Is not so fruitfull. Elaboration on this in answers would be welcome as well, though my question is a more philosophical one. Question: Given what we know about the unsuitability of scheme theory for the study of smooth rings why should we believe that it's the right generalization for algebraic geometry over arbitrary rings? Personally, (and I hope It's okay to express my opinion despite my ignonrance) I think this is a good argument for studying general locally ringed spaces (Of which schemes are a part of but not at the center necessarily)... What do you think?",,"['algebraic-geometry', 'differential-geometry', 'ring-theory', 'soft-question', 'schemes']"
73,Embedding covers of manifolds,Embedding covers of manifolds,,"I am considering $k$-fold covers of smooth manifolds (with smooth covering maps). Let $f:M^m\to N^m$ be a smooth finite covering map. -- The following implication is not true: $M$ can be embedded into $\mathbb{R}^n$ $\Rightarrow$ $N$ can be embedded into $\mathbb{R}^n$. The easiest counterexample is $M=\mathbb{S}^{n-1}$ and $N=RP^{n-1}$. -- But what about the converse of the implication: $N$ can be embedded into $\mathbb{R}^n$ $\Rightarrow$ $M$ can be embedded into $\mathbb{R}^n$. Intuitively, this makes sense since if $N$ can be embedded, we should be able to embed $M$ which is obtained by ""cutting, unfolding and gluing several copies of $N$"" and is therefore ""less complicated"". Or is there an obvious counterexample I'm missing? NB: I'm of course aware of the Whitney Embedding Theorem. The point is, however, to derive something better than $n=2m$ for $M$ based on the fact that I know something about the embeddability of $N$.","I am considering $k$-fold covers of smooth manifolds (with smooth covering maps). Let $f:M^m\to N^m$ be a smooth finite covering map. -- The following implication is not true: $M$ can be embedded into $\mathbb{R}^n$ $\Rightarrow$ $N$ can be embedded into $\mathbb{R}^n$. The easiest counterexample is $M=\mathbb{S}^{n-1}$ and $N=RP^{n-1}$. -- But what about the converse of the implication: $N$ can be embedded into $\mathbb{R}^n$ $\Rightarrow$ $M$ can be embedded into $\mathbb{R}^n$. Intuitively, this makes sense since if $N$ can be embedded, we should be able to embed $M$ which is obtained by ""cutting, unfolding and gluing several copies of $N$"" and is therefore ""less complicated"". Or is there an obvious counterexample I'm missing? NB: I'm of course aware of the Whitney Embedding Theorem. The point is, however, to derive something better than $n=2m$ for $M$ based on the fact that I know something about the embeddability of $N$.",,"['differential-geometry', 'differential-topology', 'smooth-manifolds', 'covering-spaces']"
74,"Leray-Hirsch Using Kunneth Formula from ""Differential form in Algebraic Topology"" by Bott and Tu","Leray-Hirsch Using Kunneth Formula from ""Differential form in Algebraic Topology"" by Bott and Tu",,Kunneth Formula: Let M and F are manifolds. If M has a finite good cover then $H^n(M\times F)=\bigoplus _{p+q=n} H^p(M)\bigotimes H^q (F)$ Bott and tu says One can prove Leray-Hirsch theorem by the same argument for Kuneth. I dont see how does  Leray-Hirsch follows via similar argument as Kunneth-Formula. Can someone help? I saw a similar question Relating the Künneth Formula to the Leray-Hirsch Theorem here. But I guess that post doesn't answer my question.,Kunneth Formula: Let M and F are manifolds. If M has a finite good cover then $H^n(M\times F)=\bigoplus _{p+q=n} H^p(M)\bigotimes H^q (F)$ Bott and tu says One can prove Leray-Hirsch theorem by the same argument for Kuneth. I dont see how does  Leray-Hirsch follows via similar argument as Kunneth-Formula. Can someone help? I saw a similar question Relating the Künneth Formula to the Leray-Hirsch Theorem here. But I guess that post doesn't answer my question.,,"['algebraic-geometry', 'differential-geometry', 'algebraic-topology', 'fiber-bundles']"
75,What makes a Lie Group a Differentiable Manifold?,What makes a Lie Group a Differentiable Manifold?,,"I've recently been trying to glance at the definition of a Lie group, but I'm not clear as to why a Lie group is defined the way it is, and why this becomes a smooth manifold. For example, if we have a Lie group $G$, why do we have the requirement that $G \times G \to G$, $(x,y) \mapsto xy^{-1}$ is a smooth continuous mapping between the product manifold and the manifold $G$. What is it that makes $G$ a topological manifold in the first place? How does it inherit differentiable structure?","I've recently been trying to glance at the definition of a Lie group, but I'm not clear as to why a Lie group is defined the way it is, and why this becomes a smooth manifold. For example, if we have a Lie group $G$, why do we have the requirement that $G \times G \to G$, $(x,y) \mapsto xy^{-1}$ is a smooth continuous mapping between the product manifold and the manifold $G$. What is it that makes $G$ a topological manifold in the first place? How does it inherit differentiable structure?",,"['differential-geometry', 'differential-topology', 'lie-groups', 'lie-algebras']"
76,Confused about the possibility of different differentiable structures.,Confused about the possibility of different differentiable structures.,,"In Loring Tu's ""An Introduction to Manifolds"" an atlas on a manifold is a collection of coordinate charts that are pairwise compatible and cover the manifold. A smooth manifold is defined to be a topological manifold together with a maximal atlas or differentiable structure. The maximal atlas is constructed by taking an atlas and appending all coordinate charts that are compatible with the atlas and using this he shows that any atlas on a locally Euclidean space is contained in a unique maximal atlas. He also later shows that for any chart on a manifold, the coordinate map is a diffeomorphism onto it's image. My question arises when Tu mentions in an aside that every compact topological manifold in dimension four or higher has a finite number of differentiable structures. What I don't understand is how it is possible for a smooth manifold to have more than one differentiable structure. Doesn't the fact that for any chart the coordinate map being a diffeomorphism coupled with the fact that composition of smooth functions is smooth mean that any two coordinate charts on a manifold are compatible and thus they must all belong to one maximal atlas? How is it possible to have another? Am I not understanding the word ""maximal"" or atlas or some other concept?","In Loring Tu's ""An Introduction to Manifolds"" an atlas on a manifold is a collection of coordinate charts that are pairwise compatible and cover the manifold. A smooth manifold is defined to be a topological manifold together with a maximal atlas or differentiable structure. The maximal atlas is constructed by taking an atlas and appending all coordinate charts that are compatible with the atlas and using this he shows that any atlas on a locally Euclidean space is contained in a unique maximal atlas. He also later shows that for any chart on a manifold, the coordinate map is a diffeomorphism onto it's image. My question arises when Tu mentions in an aside that every compact topological manifold in dimension four or higher has a finite number of differentiable structures. What I don't understand is how it is possible for a smooth manifold to have more than one differentiable structure. Doesn't the fact that for any chart the coordinate map being a diffeomorphism coupled with the fact that composition of smooth functions is smooth mean that any two coordinate charts on a manifold are compatible and thus they must all belong to one maximal atlas? How is it possible to have another? Am I not understanding the word ""maximal"" or atlas or some other concept?",,"['differential-geometry', 'manifolds']"
77,What is the importance of Metric in Riemannian Geometry?,What is the importance of Metric in Riemannian Geometry?,,"Any smooth manifold $M$ is locally diffeomorphic to an open set in $\mathbb{R}^N$. So the tangent space at each point $p \in M$ is also isomorphic to $R^N$ where $p$ is mapped into the origin. So we have a orthonormal basis in $R^N$ if we take the inverse image of the same, we get a frame in $T_p(M)$ for which the metric can be just the Euclidean metric $g_E = (\delta_{ij})$ which is kind of the simplest metric we can handle. I believe it is also smoothly defined over the manifold. So why are we worried about the metric in general (""A Riemannian manifold is a manifold along with a metric"") ? I don't understand how studying different metrics help us ? Probably I am not understanding some aspect of metric. Sub-question : Is it possible to equip $\mathbb{R}^N$ with non-Euclidean metric so that it becomes a sphere $\mathbb{S}^N$ ? Does this question at all make sense ? Any help will be appreciated. Thanks in advance.","Any smooth manifold $M$ is locally diffeomorphic to an open set in $\mathbb{R}^N$. So the tangent space at each point $p \in M$ is also isomorphic to $R^N$ where $p$ is mapped into the origin. So we have a orthonormal basis in $R^N$ if we take the inverse image of the same, we get a frame in $T_p(M)$ for which the metric can be just the Euclidean metric $g_E = (\delta_{ij})$ which is kind of the simplest metric we can handle. I believe it is also smoothly defined over the manifold. So why are we worried about the metric in general (""A Riemannian manifold is a manifold along with a metric"") ? I don't understand how studying different metrics help us ? Probably I am not understanding some aspect of metric. Sub-question : Is it possible to equip $\mathbb{R}^N$ with non-Euclidean metric so that it becomes a sphere $\mathbb{S}^N$ ? Does this question at all make sense ? Any help will be appreciated. Thanks in advance.",,"['differential-geometry', 'riemannian-geometry']"
78,What groups can be realized as the isometry group of the two-sphere?,What groups can be realized as the isometry group of the two-sphere?,,"Regarding $S^2 \subseteq \mathbb{E}^3$ as a Riemannian manifold with the inherited metric from Euclidean three-space, then it is well known that the isometry group is $O(3)$.  What I am curious about, however, is the following:  Given a Lie group $G$ (of dimension $\le 3$), when can I find a Riemannian metric on $S^2$ for which $G$ is the isometry group?  For what groups is this possible? (There must be other candidates for the isometry group of $S^2$ as an arbitrary metric would almost certainly result in the a trivial group of isometries.) The question seems to be direct enough, but I am unaware of any resources or work on the problem.   Any help would be greatly appreciated.","Regarding $S^2 \subseteq \mathbb{E}^3$ as a Riemannian manifold with the inherited metric from Euclidean three-space, then it is well known that the isometry group is $O(3)$.  What I am curious about, however, is the following:  Given a Lie group $G$ (of dimension $\le 3$), when can I find a Riemannian metric on $S^2$ for which $G$ is the isometry group?  For what groups is this possible? (There must be other candidates for the isometry group of $S^2$ as an arbitrary metric would almost certainly result in the a trivial group of isometries.) The question seems to be direct enough, but I am unaware of any resources or work on the problem.   Any help would be greatly appreciated.",,"['reference-request', 'differential-geometry', 'riemannian-geometry']"
79,Geometric Intuition of Gaussian Curvature,Geometric Intuition of Gaussian Curvature,,Curvature of a curve at a point can be understood as how rapidly the curve tries to move away from the tangent of the curve at that point. And for curved surfaces we have defined the Gaussian curvature at point $p$ to be the determinant of the Gaussian map $dN_p$. How can one intuitively describe the curvature of a surface defined by the Gaussian Map?,Curvature of a curve at a point can be understood as how rapidly the curve tries to move away from the tangent of the curve at that point. And for curved surfaces we have defined the Gaussian curvature at point $p$ to be the determinant of the Gaussian map $dN_p$. How can one intuitively describe the curvature of a surface defined by the Gaussian Map?,,"['differential-geometry', 'intuition']"
80,Online course in representation theory or differential geometry,Online course in representation theory or differential geometry,,"Are there any courses in representation theory that are available online? I'm looking for a course including videos, notes as well as assignments. I'd also be interested in a course in differential geometry. I couldn't find anything. The MIT open courseware courses are all without video.","Are there any courses in representation theory that are available online? I'm looking for a course including videos, notes as well as assignments. I'd also be interested in a course in differential geometry. I couldn't find anything. The MIT open courseware courses are all without video.",,"['reference-request', 'differential-geometry', 'representation-theory']"
81,Show that curvature and torsion are $κ = \frac{\left|γ'∧γ''\right|}{\left|γ'\right|^3}$ and $\tau = \frac{(γ'∧γ'')\cdot γ'''}{\left|γ'∧γ''\right|^2}$,Show that curvature and torsion are  and,κ = \frac{\left|γ'∧γ''\right|}{\left|γ'\right|^3} \tau = \frac{(γ'∧γ'')\cdot γ'''}{\left|γ'∧γ''\right|^2},Suppose that $\gamma : I \to \mathbb{R}^3$ is a regular curve (not necessarily parameterised by arc length). How can we show that the curvature and torsion are given respectively by $\kappa = \frac{|\gamma^{\prime} ∧\gamma^{\prime\prime}|}{|\gamma^{\prime}|^3}$ and $\tau = \frac{(\gamma^{\prime} ∧\gamma^{\prime\prime})\cdot \gamma^{\prime\prime\prime}}{|\gamma^{\prime} ∧\gamma^{\prime\prime}|^2}$?,Suppose that $\gamma : I \to \mathbb{R}^3$ is a regular curve (not necessarily parameterised by arc length). How can we show that the curvature and torsion are given respectively by $\kappa = \frac{|\gamma^{\prime} ∧\gamma^{\prime\prime}|}{|\gamma^{\prime}|^3}$ and $\tau = \frac{(\gamma^{\prime} ∧\gamma^{\prime\prime})\cdot \gamma^{\prime\prime\prime}}{|\gamma^{\prime} ∧\gamma^{\prime\prime}|^2}$?,,['differential-geometry']
82,Structure of a $ C^{\infty} $-manifold,Structure of a -manifold, C^{\infty} ,"I was studying differentiable manifolds (an introduction) and found the following example, but I am confused. Example The function \begin{align} f: &\mathbb{R}^{3} \to \mathbb{R}, \\ f: &(x,y,z) \mapsto x^{3} + 2 y^{3} + z^{3} + 6 x^{2} y - 1 \end{align} defines the structure of a $ C^{\infty} $-manifold on $ {f^{-1}}(0) $. Thank you.","I was studying differentiable manifolds (an introduction) and found the following example, but I am confused. Example The function \begin{align} f: &\mathbb{R}^{3} \to \mathbb{R}, \\ f: &(x,y,z) \mapsto x^{3} + 2 y^{3} + z^{3} + 6 x^{2} y - 1 \end{align} defines the structure of a $ C^{\infty} $-manifold on $ {f^{-1}}(0) $. Thank you.",,"['differential-geometry', 'manifolds', 'differential-topology']"
83,Show that an open subset of a smooth manifold is again smooth.,Show that an open subset of a smooth manifold is again smooth.,,"This step is essential in proving that $GL(n,\mathbb{R})$ is a smooth manifold. I already proved that $GL(n,\mathbb{R})$ is an open subset of $M_n(\mathbb{R})$. Can you help me prove the above statement? Thanks! p.s. My idea (but I'm not sure) is that we will consider the smooth structure of the smooth manifold and restrict the corresponding maps to the intersection of the open set and the corresponding open sets from the atlas.","This step is essential in proving that $GL(n,\mathbb{R})$ is a smooth manifold. I already proved that $GL(n,\mathbb{R})$ is an open subset of $M_n(\mathbb{R})$. Can you help me prove the above statement? Thanks! p.s. My idea (but I'm not sure) is that we will consider the smooth structure of the smooth manifold and restrict the corresponding maps to the intersection of the open set and the corresponding open sets from the atlas.",,['differential-geometry']
84,The Dido problem with an arclength constraint,The Dido problem with an arclength constraint,,"It is well known that the solution to the classical Dido problem is a semicircle, and that the solution to the classical isoperimetric problem is a circle. It's also reasonably obvious that the solution to the following variant is a circular arc: Let $A$ and $B$ be fixed points on a plane, and let $l$ be a length greater than $\overline{AB}$. Which (smooth) curve through $A$ and $B$, of length $l$, maximises the area between itself and the line $AB$? It's a straightforward exercise to verify extremality using the calculus of variations, but are there alternative proofs that do not invoke e.g. the Euler-Lagrange equations? This was originally a homework problem with the isoperimetric inequality given as a hint, and I'm just wondering what the intended solution was...","It is well known that the solution to the classical Dido problem is a semicircle, and that the solution to the classical isoperimetric problem is a circle. It's also reasonably obvious that the solution to the following variant is a circular arc: Let $A$ and $B$ be fixed points on a plane, and let $l$ be a length greater than $\overline{AB}$. Which (smooth) curve through $A$ and $B$, of length $l$, maximises the area between itself and the line $AB$? It's a straightforward exercise to verify extremality using the calculus of variations, but are there alternative proofs that do not invoke e.g. the Euler-Lagrange equations? This was originally a homework problem with the isoperimetric inequality given as a hint, and I'm just wondering what the intended solution was...",,"['differential-geometry', 'euclidean-geometry', 'plane-curves']"
85,Induced metric on $S^2$ from pullback of metric on $\mathbb{R}^3$,Induced metric on  from pullback of metric on,S^2 \mathbb{R}^3,"I'm going over some GR from more of a differential geometry perspective and had a quick question about a simple calculation - my differential geometry background isn't too strong so I apologise if any of the terminology is incorrect, but I'd be grateful for any clarification. I'm following an example in Appendix A of Sean Carroll's Introduction to GR, where there is a map $\phi:M \to N$ , given by $$ \phi(\theta,\phi) = (\sin\theta \cos \phi , \sin \theta \sin \phi, \cos \phi) , $$ and $M=S^2$ is a submanifold of $N =\mathbb{R}^3$ (i.e. the two-sphere embedded in $\mathbb{R}^3$ ). The coordinates on the manifolds are $x^{\mu} = (\theta,\phi)$ on $M$ , and $y^{\alpha} = (x,y,z)$ on $N$ . The induced metric on $M$ is just the pullback of the flat-space metric $\phi^*g$ , which is given by the formula $$  (\phi^*g)_{\mu \nu} = \frac{\partial y^{\alpha} }{\partial x^{\mu}} \frac{\partial y^{\beta} }{\partial x^{\nu}} g_{\alpha \beta}. $$ I understand how to calculate the individual Jacobian matrices of partial derivatives $\frac{\partial y^{\alpha} }{\partial x^{\mu}}$ (e.g. just using $y^1 = \sin\theta \cos \phi $ , $y^2 = \sin \theta \sin \phi$ and $y^3 = \cos \phi$ as defined by $\phi$ ), however I was confused as to how to treat the full expression above. The Jacobian matrices are $2 \times 3$ matrices, so the second has to be transposed to be a $3 \times 2$ matrix in order to give the required $2 \times 2$ metric $g_{\mu \nu}$ . My question is, in the pullback equation above, how does the metric $g_{\alpha \beta}$ act on the Jacobian $\frac{\partial y^{\beta} }{\partial x^{\mu}}$ , and how should I be writing this down? I can see that $y^{\beta}$ should be replaced with $y^{\alpha}$ , but should any indices be lowered, and how should I interpret the metric tensor transposing the Jacobian matrix? Edit - Ted Shriffin's comments are correct, the last component of the map should be $\cos \theta$ not $\cos \phi$ , and all my matrices should be transposed.","I'm going over some GR from more of a differential geometry perspective and had a quick question about a simple calculation - my differential geometry background isn't too strong so I apologise if any of the terminology is incorrect, but I'd be grateful for any clarification. I'm following an example in Appendix A of Sean Carroll's Introduction to GR, where there is a map , given by and is a submanifold of (i.e. the two-sphere embedded in ). The coordinates on the manifolds are on , and on . The induced metric on is just the pullback of the flat-space metric , which is given by the formula I understand how to calculate the individual Jacobian matrices of partial derivatives (e.g. just using , and as defined by ), however I was confused as to how to treat the full expression above. The Jacobian matrices are matrices, so the second has to be transposed to be a matrix in order to give the required metric . My question is, in the pullback equation above, how does the metric act on the Jacobian , and how should I be writing this down? I can see that should be replaced with , but should any indices be lowered, and how should I interpret the metric tensor transposing the Jacobian matrix? Edit - Ted Shriffin's comments are correct, the last component of the map should be not , and all my matrices should be transposed.","\phi:M \to N  \phi(\theta,\phi) = (\sin\theta \cos \phi , \sin \theta \sin \phi, \cos \phi) ,
 M=S^2 N =\mathbb{R}^3 \mathbb{R}^3 x^{\mu} = (\theta,\phi) M y^{\alpha} = (x,y,z) N M \phi^*g  
(\phi^*g)_{\mu \nu} = \frac{\partial y^{\alpha} }{\partial x^{\mu}} \frac{\partial y^{\beta} }{\partial x^{\nu}} g_{\alpha \beta}.
 \frac{\partial y^{\alpha} }{\partial x^{\mu}} y^1 = \sin\theta \cos \phi  y^2 = \sin \theta \sin \phi y^3 = \cos \phi \phi 2 \times 3 3 \times 2 2 \times 2 g_{\mu \nu} g_{\alpha \beta} \frac{\partial y^{\beta} }{\partial x^{\mu}} y^{\beta} y^{\alpha} \cos \theta \cos \phi",['differential-geometry']
86,Constructing the flat vector bundle associated to a given linear representation of the fundamental group,Constructing the flat vector bundle associated to a given linear representation of the fundamental group,,"I'm reading this notes , and I have some questions about the contruction on page 18. Let $M$ be a connected manifold and $E\rightarrow M$ a flat vector bundle over $M$ . Consider $\{(U_\alpha, \phi_{U_\alpha})\}$ a flat atlas of $E\rightarrow M$ , i.e the transition functions $g_{U_\alpha, U_\beta}$ associated to the trivializations are locally constants. There is a canonical way to associate a representation $\rho:\pi_{1}(M, m_0)\rightarrow \mathrm{GL}(d, \mathbb{K})$ to this flat vector bundle as follows: Fix an open set $U$ containing $m_0$ , such that $U$ is in the atlas. For any loop $\gamma: [0, 1] → M$ , $γ(0) = γ(1) = m_0$ there are a subdivision $0 = t_0 ≤ t_1 ≤ · · · ≤ t_N ≤ t_{N+1} = 1$ of $[0, 1]$ and open sets $U_0, \dots, U_N$ in the atlas, such that $U_0 = U_N = U$ and, for all $i=0, \dots, N$ , $\gamma([t_i, t_{i+1}])\subset U_i$ . then, define: $$ \rho(\gamma)=g_{U_N ,U_{N−1}}(γ(t_N))\dots g_{U_2,U_1}(γ(t_2))g_{U_1,U_0}(γ(t_1)) $$ The question is about the proof that the following map is sujective: $$ \frac{\{\mbox{flat bundles of rank $d$ over $M$}\}}{\mbox{isomorphism}}\rightarrow \frac{\mathrm{Hom}(\pi_{1}(M, m_0), \mathrm{GL}(d, \mathbb{K}))}{\mbox{conjugacy}} $$ i.e, given a linear representation $\eta$ of $\pi_{1}(M, m_0)$ , I need to find a flat vector bundle $E_\eta$ , such that the associeted representation $\rho$ described later is equal to $\eta$ . Here is the construction: Let $\eta : π_1(M, m_0)→\mathrm{GL}(d,\mathbb{K})$ be a morphism, then the trivial bundle $\widetilde{M} × \mathbb{K}^d$ over the universal cover $\widetilde{M}$ of $M$ has an obvious flat structure together with an action of $π_1(M, m_0)$ preserving the flat structure: $$ γ · (\tilde m, v) = (γ\tilde m, \eta(γ)v)\quad ∀γ ∈ π_1(M, m_0), \tilde m \in M, v \in \mathbb{K}^{d}. $$ Here $ \tilde m→ γ\tilde m$ is the natural action of $π_1(M, m_0)$ on the universal cover $\widetilde{M}$ . The quotient of the trivial bundle $\widetilde{M} × \mathbb{K}^d$ by this action is denoted $E_\eta$ and   it is naturally a flat bundle over the base $M$ . I'm trying to find the trivizalizations of $E_\eta$ . Does any one knows how to find it? Any hints?","I'm reading this notes , and I have some questions about the contruction on page 18. Let be a connected manifold and a flat vector bundle over . Consider a flat atlas of , i.e the transition functions associated to the trivializations are locally constants. There is a canonical way to associate a representation to this flat vector bundle as follows: Fix an open set containing , such that is in the atlas. For any loop , there are a subdivision of and open sets in the atlas, such that and, for all , . then, define: The question is about the proof that the following map is sujective: i.e, given a linear representation of , I need to find a flat vector bundle , such that the associeted representation described later is equal to . Here is the construction: Let be a morphism, then the trivial bundle over the universal cover of has an obvious flat structure together with an action of preserving the flat structure: Here is the natural action of on the universal cover . The quotient of the trivial bundle by this action is denoted and   it is naturally a flat bundle over the base . I'm trying to find the trivizalizations of . Does any one knows how to find it? Any hints?","M E\rightarrow M M \{(U_\alpha, \phi_{U_\alpha})\} E\rightarrow M g_{U_\alpha, U_\beta} \rho:\pi_{1}(M, m_0)\rightarrow \mathrm{GL}(d, \mathbb{K}) U m_0 U \gamma: [0, 1] → M γ(0) = γ(1) = m_0 0 = t_0 ≤ t_1 ≤ · · · ≤ t_N ≤ t_{N+1} = 1 [0, 1] U_0, \dots, U_N U_0 = U_N = U i=0, \dots, N \gamma([t_i, t_{i+1}])\subset U_i 
\rho(\gamma)=g_{U_N ,U_{N−1}}(γ(t_N))\dots g_{U_2,U_1}(γ(t_2))g_{U_1,U_0}(γ(t_1))
 
\frac{\{\mbox{flat bundles of rank d over M}\}}{\mbox{isomorphism}}\rightarrow \frac{\mathrm{Hom}(\pi_{1}(M, m_0), \mathrm{GL}(d, \mathbb{K}))}{\mbox{conjugacy}}
 \eta \pi_{1}(M, m_0) E_\eta \rho \eta \eta : π_1(M, m_0)→\mathrm{GL}(d,\mathbb{K}) \widetilde{M} × \mathbb{K}^d \widetilde{M} M π_1(M, m_0) 
γ · (\tilde m, v) = (γ\tilde m, \eta(γ)v)\quad ∀γ ∈ π_1(M, m_0), \tilde m \in M, v \in \mathbb{K}^{d}.
  \tilde m→ γ\tilde m π_1(M, m_0) \widetilde{M} \widetilde{M} × \mathbb{K}^d E_\eta M E_\eta","['differential-geometry', 'vector-bundles', 'fundamental-groups', 'connections']"
87,Which matrices can be realized as second derivatives of orthogonal paths?,Which matrices can be realized as second derivatives of orthogonal paths?,,"$\newcommand{\skew}{\operatorname{skew}}$ $\newcommand{\sym}{\operatorname{sym}}$ $\newcommand{\SO}{\operatorname{SO}_n}$ I am interested to know which real matrices $A \in M_n$ can be realized as second derivatives of paths in $\text{SO}_n$ starting at the identity. That is, for which matrices $A$ , there exist a smooth path $\alpha:(-\epsilon,\epsilon) \to \text{SO}_n$ , such that $\alpha(0)=Id$ and $\ddot \alpha(0)=A$ . We denote the space of realizable matrices by $D$ . Question: I prove below that $ (\skew)^2 \subseteq D \subseteq (\skew)^2+\skew $ . Does $D=(\skew)^2+\skew$ always hold? Comment: Note that $(\skew)^2+\skew \subsetneq M_n$ , at least for odd $n$ : In that case every skew-symmetric matrix is singular, so $(\skew)^2 \subseteq \sym $ consists only of singular matrices, hence does not contain all symmetric matrices. Edit: I proved below that equality holds in dimension $n=2$ . Proof of $ (\skew)^2 \subseteq D \subseteq (\skew)^2+\skew $ : Every square of skew-symmetric matrix can be realized: For skew $B$ , take $\alpha(t)=e^{tB}$ . Then, $\dot \alpha(t)=Be^{tB}$ , $\ddot \alpha(t)=B^2e^{tB}$ . The space of realizable matrices is contained in $(\skew)^2+\skew$ : Indeed, since $\dot \alpha(t) \in T_{\alpha(t)}\SO=\alpha(t)\skew$ , we have $\dot\alpha(t)=\alpha(t)B(t)$ for some $B(t) \in \skew$ , so $$\ddot \alpha(t)=\dot \alpha(t) B(t)+\alpha(t) \dot B(t)$$ hence $\ddot \alpha(0)=\dot \alpha(0) B(0)+ \dot B(0)= B(0)^2+\dot B(0) \in (\skew)^2 +\skew,$ where the last equality followed from $\dot \alpha(0)=B(0)$ (put $t=0$ in $\dot\alpha(t)=\alpha(t)B(t)$ ). Edit 2: When trying to show the converse direction, I hit a wall: we need to show that there exist solutions $\dot\alpha(t)=\alpha(t)B(t)$ , where $\alpha(t) \in \SO,B(t) \in \skew$ , with arbitrary $B(0),\dot B(0) \in \skew$ . A naive attempt would be to define $\alpha(t)=e^{\int_0^t B(s)ds}$ for $B(s)=B(0)+s\dot B(0)$ . However, it is not true in general that $\alpha'(t)=\alpha(t)B(t)$ ; this happens if $B(t)$ , $\int_0^t B(s)ds$ commute , which happens if and only if $B(0),\dot B(0)$ commute. Proof $D = (\skew)^2+\skew$ for $n=2$ : $\alpha(t)$ can always be written as $\alpha(t)=\begin{pmatrix} c(\phi(t)) & s(\phi(t)) \\\ -s(\phi(t)) & c(\phi(t)) \end{pmatrix}$ , where $c(x)=\cos x,s(x)=\sin x$ , and $\phi(t)$ is some parametrization satisfying $\phi(0)=0$ . Differentiating $\alpha(t)$ twice, we get $$ \ddot \alpha(t)=-(\phi'(t))^2\alpha(t)+\phi''(t)\begin{pmatrix} -s(\phi(t)) & c(\phi(t)) \\\ -c(\phi(t)) & -s(\phi(t)) \end{pmatrix},$$ so $$ \ddot \alpha(0)=-(\phi'(0))^2Id+\phi''(0)\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}.$$ Since we can choose $\phi'(0),\phi''(0)$ as we wish, we conclude that $$ D=\mathbb{R}_{\le 0}Id+\mathbb{R}\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}=\mathbb{R}_{\le 0}Id+\skew.$$ Since $\skew=\text{span} \{ \begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}\}$ , and $\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}^2=-Id$ , we have $\skew^2=\mathbb{R}_{\le 0}Id$ , so indeed $D=(\skew)^2+\skew$ .","I am interested to know which real matrices can be realized as second derivatives of paths in starting at the identity. That is, for which matrices , there exist a smooth path , such that and . We denote the space of realizable matrices by . Question: I prove below that . Does always hold? Comment: Note that , at least for odd : In that case every skew-symmetric matrix is singular, so consists only of singular matrices, hence does not contain all symmetric matrices. Edit: I proved below that equality holds in dimension . Proof of : Every square of skew-symmetric matrix can be realized: For skew , take . Then, , . The space of realizable matrices is contained in : Indeed, since , we have for some , so hence where the last equality followed from (put in ). Edit 2: When trying to show the converse direction, I hit a wall: we need to show that there exist solutions , where , with arbitrary . A naive attempt would be to define for . However, it is not true in general that ; this happens if , commute , which happens if and only if commute. Proof for : can always be written as , where , and is some parametrization satisfying . Differentiating twice, we get so Since we can choose as we wish, we conclude that Since , and , we have , so indeed .","\newcommand{\skew}{\operatorname{skew}} \newcommand{\sym}{\operatorname{sym}} \newcommand{\SO}{\operatorname{SO}_n} A \in M_n \text{SO}_n A \alpha:(-\epsilon,\epsilon) \to \text{SO}_n \alpha(0)=Id \ddot \alpha(0)=A D  (\skew)^2 \subseteq D \subseteq (\skew)^2+\skew  D=(\skew)^2+\skew (\skew)^2+\skew \subsetneq M_n n (\skew)^2 \subseteq \sym  n=2  (\skew)^2 \subseteq D \subseteq (\skew)^2+\skew  B \alpha(t)=e^{tB} \dot \alpha(t)=Be^{tB} \ddot \alpha(t)=B^2e^{tB} (\skew)^2+\skew \dot \alpha(t) \in T_{\alpha(t)}\SO=\alpha(t)\skew \dot\alpha(t)=\alpha(t)B(t) B(t) \in \skew \ddot \alpha(t)=\dot \alpha(t) B(t)+\alpha(t) \dot B(t) \ddot \alpha(0)=\dot \alpha(0) B(0)+ \dot B(0)= B(0)^2+\dot B(0) \in (\skew)^2 +\skew, \dot \alpha(0)=B(0) t=0 \dot\alpha(t)=\alpha(t)B(t) \dot\alpha(t)=\alpha(t)B(t) \alpha(t) \in \SO,B(t) \in \skew B(0),\dot B(0) \in \skew \alpha(t)=e^{\int_0^t B(s)ds} B(s)=B(0)+s\dot B(0) \alpha'(t)=\alpha(t)B(t) B(t) \int_0^t B(s)ds B(0),\dot B(0) D = (\skew)^2+\skew n=2 \alpha(t) \alpha(t)=\begin{pmatrix} c(\phi(t)) & s(\phi(t)) \\\ -s(\phi(t)) & c(\phi(t)) \end{pmatrix} c(x)=\cos x,s(x)=\sin x \phi(t) \phi(0)=0 \alpha(t)  \ddot \alpha(t)=-(\phi'(t))^2\alpha(t)+\phi''(t)\begin{pmatrix} -s(\phi(t)) & c(\phi(t)) \\\ -c(\phi(t)) & -s(\phi(t)) \end{pmatrix},  \ddot \alpha(0)=-(\phi'(0))^2Id+\phi''(0)\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}. \phi'(0),\phi''(0)  D=\mathbb{R}_{\le 0}Id+\mathbb{R}\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}=\mathbb{R}_{\le 0}Id+\skew. \skew=\text{span} \{ \begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}\} \begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}^2=-Id \skew^2=\mathbb{R}_{\le 0}Id D=(\skew)^2+\skew","['differential-geometry', 'differential-topology', 'lie-groups', 'orthogonal-matrices']"
88,Why are $C^\infty_p\neq C^\infty_q$ when $p\neq q$?,Why are  when ?,C^\infty_p\neq C^\infty_q p\neq q,"Let $C^\infty_p(M)$ be the set of all germs at point $p \in M$ . A germ at point $x$ is $$[f]_x = \{ g \in C^{\infty} (\mathcal{U}_x) :(\exists \mathcal{O_x} \subset \mathcal{U}_x) (g|_{\mathcal{O_x}}=f) \} \ ,$$ where $\mathcal{U}_x$ and $\mathcal{O_x}$ are open neighbourhoods of $x$ , in $M$ . Why do we always have $C^\infty_p\neq C^\infty_q$ when $p\neq q$ ? I need to understand this in order to understand how the tangent spaces 'as the' set of derivations are always disjoint for different points.","Let be the set of all germs at point . A germ at point is where and are open neighbourhoods of , in . Why do we always have when ? I need to understand this in order to understand how the tangent spaces 'as the' set of derivations are always disjoint for different points.","C^\infty_p(M) p \in M x [f]_x = \{ g \in C^{\infty} (\mathcal{U}_x) :(\exists \mathcal{O_x} \subset \mathcal{U}_x) (g|_{\mathcal{O_x}}=f) \} \ , \mathcal{U}_x \mathcal{O_x} x M C^\infty_p\neq C^\infty_q p\neq q","['differential-geometry', 'differential-topology']"
89,Construction G-Invariant Riemannian Metric,Construction G-Invariant Riemannian Metric,,"Let $M$ be a smooth manifold and $G$ be a lie group acting transitively on $M$. I know by Corollary 1.27 of these notes that there to exist a Riemannian metric $g_G$ on $M$ satisfying the in-variance relation $$ (\forall x,y \in M)(\forall g \in G) \,g_G(x,y)=\,g_G(g\cdot x,g\cdot y)? $$ When is this Riemannian metric unique?  How can it be contructed explicitly if $M$ is $\mathbb{R}^d$?","Let $M$ be a smooth manifold and $G$ be a lie group acting transitively on $M$. I know by Corollary 1.27 of these notes that there to exist a Riemannian metric $g_G$ on $M$ satisfying the in-variance relation $$ (\forall x,y \in M)(\forall g \in G) \,g_G(x,y)=\,g_G(g\cdot x,g\cdot y)? $$ When is this Riemannian metric unique?  How can it be contructed explicitly if $M$ is $\mathbb{R}^d$?",,"['differential-geometry', 'lie-groups', 'riemannian-geometry', 'group-actions']"
90,Examples of moduli space $J$-holomorphic curves,Examples of moduli space -holomorphic curves,J,"I'm reading McDuff and Salamon's book on $J$-holomorphic curves and am curious about some examples. They say that if $(M^{2n},J)$ is an almost complex manifold, $(\mathbb{CP}^1,j)$ the Riemann sphere, $A\in H_2(M;\mathbb{Z})$ a homology class, and $G=PSL(2,\mathbb{C})$ the group of Moebius transformations of $\mathbb{CP}^1$, then the space $\mathcal{M}(A,J)/G$ of $(j,J)$-holomorphic curves $u:(\Sigma,j)\to (M,J)$ in the homology class $A$, modulo the action of $G$, is a manifold of dimension $$\dim \mathcal{M}(A,J)/G=2n+2c_1(A)-6.$$ As an example, suppose $(M,J)$ is $\mathbb{CP}^n$ with the standard Kaehler structure and $A$ is the homology class of linear embeddings of $\mathbb{CP}^1$ in $\mathbb{CP}^n$. Note that in this case $c_1(A)=n+1$, so $\mathcal{M}(A,J)/G$ has real dimension $4(n-1)$. Is this moduli space compact? What are its other topological properties, like CW structure, homology, homotopy, etc. Is it a ""familiar"" compact manifold? I would also appreciate other examples where $\mathcal{M}(A,J)/G$ has been ""determined"", and references to where they have been computed.","I'm reading McDuff and Salamon's book on $J$-holomorphic curves and am curious about some examples. They say that if $(M^{2n},J)$ is an almost complex manifold, $(\mathbb{CP}^1,j)$ the Riemann sphere, $A\in H_2(M;\mathbb{Z})$ a homology class, and $G=PSL(2,\mathbb{C})$ the group of Moebius transformations of $\mathbb{CP}^1$, then the space $\mathcal{M}(A,J)/G$ of $(j,J)$-holomorphic curves $u:(\Sigma,j)\to (M,J)$ in the homology class $A$, modulo the action of $G$, is a manifold of dimension $$\dim \mathcal{M}(A,J)/G=2n+2c_1(A)-6.$$ As an example, suppose $(M,J)$ is $\mathbb{CP}^n$ with the standard Kaehler structure and $A$ is the homology class of linear embeddings of $\mathbb{CP}^1$ in $\mathbb{CP}^n$. Note that in this case $c_1(A)=n+1$, so $\mathcal{M}(A,J)/G$ has real dimension $4(n-1)$. Is this moduli space compact? What are its other topological properties, like CW structure, homology, homotopy, etc. Is it a ""familiar"" compact manifold? I would also appreciate other examples where $\mathcal{M}(A,J)/G$ has been ""determined"", and references to where they have been computed.",,"['differential-geometry', 'algebraic-geometry', 'complex-geometry', 'symplectic-geometry']"
91,Integral Representation of Dirac Delta on a Manifold,Integral Representation of Dirac Delta on a Manifold,,"In Cartesian coordinates on an $\Bbb{R}^d$ plane the Dirac delta can be represented as a Fourier transform: $$\delta^d(\vec r-\vec r')=\int \frac{d^d \vec k}{(2\pi)^d}\;\;\exp\left({i\vec k\cdot (\vec r-\vec r')}\right)\tag{1}$$ Correct me if I am wrong, but on a general manifold it is typical to define the Dirac delta in two ways*: As: $$\int d^d\xi \;  \sqrt{g(\vec \xi)} \delta(\xi'-\xi)=1$$ Or as: $$\int d^d\xi \;   \delta(\xi'-\xi)=1$$ Do either of these permit an integral representation like the form of $(1)$? $^*$I believe this is what is indicated in this answer on PSE.","In Cartesian coordinates on an $\Bbb{R}^d$ plane the Dirac delta can be represented as a Fourier transform: $$\delta^d(\vec r-\vec r')=\int \frac{d^d \vec k}{(2\pi)^d}\;\;\exp\left({i\vec k\cdot (\vec r-\vec r')}\right)\tag{1}$$ Correct me if I am wrong, but on a general manifold it is typical to define the Dirac delta in two ways*: As: $$\int d^d\xi \;  \sqrt{g(\vec \xi)} \delta(\xi'-\xi)=1$$ Or as: $$\int d^d\xi \;   \delta(\xi'-\xi)=1$$ Do either of these permit an integral representation like the form of $(1)$? $^*$I believe this is what is indicated in this answer on PSE.",,"['differential-geometry', 'fourier-transform', 'dirac-delta']"
92,Question Regarding the Coordinate Independent Form of the Exterior Derivative,Question Regarding the Coordinate Independent Form of the Exterior Derivative,,"The coordinate version of the exterior derivative $d:\Omega^k(M)\to \Omega^{k+1}(M)$ of differential forms of a $C^\infty$ manifold $M$ can be expressed on a form  $$ \omega=fdx^1\wedge\cdots\wedge dx^k$$ as  $$ d\omega=\sum_{i=1}^n\frac{\partial f}{\partial x^i}dx^i\wedge dx^1\wedge\cdots \wedge dx^k$$ (for example). Alternatively, one can express this in a coordinate invariant form as $$ d\omega(X_1,\ldots, X_{k+1})=\sum_{i=1}^{k+1}(-1)^{i+1}X_i(\omega(X_1,\ldots, \widehat{X_i},\ldots, X_{k+1}))$$ $$+\sum_{1\le i<j\le k+1}(-1)^{i+j}\omega([X_i,X_j],X_1,\ldots, \widehat{X_i},\ldots, \widehat{X_j},\ldots,X_{k+1}).$$ Here the $X_i\in \mathfrak{X}(M)$. I can show that these two operators are actually the same. That's not a problem. My confusion is in showing that the coordinate invariant form $d\omega$ is independent of extension $X_1,\ldots, X_{k+1}\in \mathfrak{X}(M)$. Indeed, the idea is that we define $d\omega_p$ on a $(k+1)-$tuple  $$(v_1,\ldots, v_{k+1})\in \overbrace{T_pM\times\cdots\times T_pM}^{k+1\:\text{times}}$$ by extending this tuple to a $(k+1)-$tuple of smooth vector fields $X_1,\ldots, X_{k+1}$ so that $X_i(p)=v_i$. I can see that this is independent of extension, because of the corresponding fact for the coordinate definition. On the other hand, I would like to show that this formula is well-defined in this sense intrinisically , based only on the coordinate independent formula. EDIT: As an addendum, I want to understand why the coordinate independent definition is independent of extension $(X_1,\ldots, X_{k+1})$ chosen without appealing to the coordinate definition. EDIT 2: Please note this question is not a duplicate of the other, because I am interested in showing the corresponding fact for the coordinate-independent version of the formula, without appealing to the coordinate expression.","The coordinate version of the exterior derivative $d:\Omega^k(M)\to \Omega^{k+1}(M)$ of differential forms of a $C^\infty$ manifold $M$ can be expressed on a form  $$ \omega=fdx^1\wedge\cdots\wedge dx^k$$ as  $$ d\omega=\sum_{i=1}^n\frac{\partial f}{\partial x^i}dx^i\wedge dx^1\wedge\cdots \wedge dx^k$$ (for example). Alternatively, one can express this in a coordinate invariant form as $$ d\omega(X_1,\ldots, X_{k+1})=\sum_{i=1}^{k+1}(-1)^{i+1}X_i(\omega(X_1,\ldots, \widehat{X_i},\ldots, X_{k+1}))$$ $$+\sum_{1\le i<j\le k+1}(-1)^{i+j}\omega([X_i,X_j],X_1,\ldots, \widehat{X_i},\ldots, \widehat{X_j},\ldots,X_{k+1}).$$ Here the $X_i\in \mathfrak{X}(M)$. I can show that these two operators are actually the same. That's not a problem. My confusion is in showing that the coordinate invariant form $d\omega$ is independent of extension $X_1,\ldots, X_{k+1}\in \mathfrak{X}(M)$. Indeed, the idea is that we define $d\omega_p$ on a $(k+1)-$tuple  $$(v_1,\ldots, v_{k+1})\in \overbrace{T_pM\times\cdots\times T_pM}^{k+1\:\text{times}}$$ by extending this tuple to a $(k+1)-$tuple of smooth vector fields $X_1,\ldots, X_{k+1}$ so that $X_i(p)=v_i$. I can see that this is independent of extension, because of the corresponding fact for the coordinate definition. On the other hand, I would like to show that this formula is well-defined in this sense intrinisically , based only on the coordinate independent formula. EDIT: As an addendum, I want to understand why the coordinate independent definition is independent of extension $(X_1,\ldots, X_{k+1})$ chosen without appealing to the coordinate definition. EDIT 2: Please note this question is not a duplicate of the other, because I am interested in showing the corresponding fact for the coordinate-independent version of the formula, without appealing to the coordinate expression.",,"['differential-geometry', 'differential-forms', 'de-rham-cohomology']"
93,"Is it true that for any compact Riemannian manifold, the sectional curvature is bounded?","Is it true that for any compact Riemannian manifold, the sectional curvature is bounded?",,"My attempt at a proof of this statement goes something like this: Let $(M,g)$ be a Riemannian $n$-manifold, then consider the vector bundle $\mathrm{T}M\oplus\mathrm{T}M$, then sectional curvature defines a function $K:\mathcal{U}\to\mathbb{R}$ which is defined on an open subset $\mathcal{U}\subset\mathrm{T}M\oplus\mathrm{T}M$ and is continuous, since it is smooth in coordinates. Note that we can define a $\operatorname{Gr}_2\mathbb{R}^n$-bundle on $M$ by considering a quotient topology on $\mathcal{U}$ in which we identify $(v_1,v_2)\in\mathrm{T}_xM\oplus\mathrm{T}_xM$ with $(w_1,w_2)\in\mathrm{T}_xM\oplus\mathrm{T}_xM$ precisely when they cut out the same plane in $\mathrm{T}_xM$. If we define the resulting quotient space to be $\mathcal{V}$, then we obtain a continuous map $K:\mathcal{V}\to\mathbb{R}$ and $\mathcal{V}$ is compact, so that sectional curvature is bounded. However, I was talking to another student recently, and he claimed with great confidence that the sectional curvature on compact Riemannian manifolds need not be bounded, which makes me wonder if my (sketch of a) proof is incorrect.","My attempt at a proof of this statement goes something like this: Let $(M,g)$ be a Riemannian $n$-manifold, then consider the vector bundle $\mathrm{T}M\oplus\mathrm{T}M$, then sectional curvature defines a function $K:\mathcal{U}\to\mathbb{R}$ which is defined on an open subset $\mathcal{U}\subset\mathrm{T}M\oplus\mathrm{T}M$ and is continuous, since it is smooth in coordinates. Note that we can define a $\operatorname{Gr}_2\mathbb{R}^n$-bundle on $M$ by considering a quotient topology on $\mathcal{U}$ in which we identify $(v_1,v_2)\in\mathrm{T}_xM\oplus\mathrm{T}_xM$ with $(w_1,w_2)\in\mathrm{T}_xM\oplus\mathrm{T}_xM$ precisely when they cut out the same plane in $\mathrm{T}_xM$. If we define the resulting quotient space to be $\mathcal{V}$, then we obtain a continuous map $K:\mathcal{V}\to\mathbb{R}$ and $\mathcal{V}$ is compact, so that sectional curvature is bounded. However, I was talking to another student recently, and he claimed with great confidence that the sectional curvature on compact Riemannian manifolds need not be bounded, which makes me wonder if my (sketch of a) proof is incorrect.",,"['differential-geometry', 'riemannian-geometry', 'curvature']"
94,Holonomy of Lie groups,Holonomy of Lie groups,,"Simple compact Lie groups have unique bi-invariant metrics. Hence, they are Riemannian manifolds in a unique way, so we can ask what is their holonomy group. Is there a relationship between the group $G$ and its holonomy group? For example, is the holonomy group $G$ itself?","Simple compact Lie groups have unique bi-invariant metrics. Hence, they are Riemannian manifolds in a unique way, so we can ask what is their holonomy group. Is there a relationship between the group $G$ and its holonomy group? For example, is the holonomy group $G$ itself?",,"['differential-geometry', 'lie-groups', 'riemannian-geometry', 'holonomy']"
95,"On the map $E: O\subset TM \to M \times M$ by $E(v)=(\pi(v), \exp(v) )$",On the map  by,"E: O\subset TM \to M \times M E(v)=(\pi(v), \exp(v) )","Let $\exp$ be the exponential map on the Riemannian manifold M and $O$ is its domain in $TM$. Consider the map $E: O\subset TM \to M \times M$ by $E(v)=(\pi(v), \exp(v) )$, where $\pi$ is the canonical map $TM \to M$. It is easy to show that $dE: T_{0_p}(TM) \to T_{(p,p)}(M\times M)$ is nonsingular and thus the inverse map theorem gives local diffeomorphisms via $E$ of neighborhoods of $0_p \in TM$ onto neighborhoods of $(p,p)$ for every $p\in M$. My question is: How to show that $E$ can be extended to a diffeomorphism from a neighborhood    of the zero section of $TM$ onto an open neighborhood of the diagonal in   $M\times M$ ? (cf. the last paragraph of Page 131 of Riemannian Geometry (GTM171) written by Peter Petersen) The book has a sketch of proof, but I cannot fully understand. The main difficulty may be how to guarantee this diffeomorphism is injective, thus I cannot simply fit those local diffeomorphisms together to give the desired diffeomorphism. Please help, thanks!","Let $\exp$ be the exponential map on the Riemannian manifold M and $O$ is its domain in $TM$. Consider the map $E: O\subset TM \to M \times M$ by $E(v)=(\pi(v), \exp(v) )$, where $\pi$ is the canonical map $TM \to M$. It is easy to show that $dE: T_{0_p}(TM) \to T_{(p,p)}(M\times M)$ is nonsingular and thus the inverse map theorem gives local diffeomorphisms via $E$ of neighborhoods of $0_p \in TM$ onto neighborhoods of $(p,p)$ for every $p\in M$. My question is: How to show that $E$ can be extended to a diffeomorphism from a neighborhood    of the zero section of $TM$ onto an open neighborhood of the diagonal in   $M\times M$ ? (cf. the last paragraph of Page 131 of Riemannian Geometry (GTM171) written by Peter Petersen) The book has a sketch of proof, but I cannot fully understand. The main difficulty may be how to guarantee this diffeomorphism is injective, thus I cannot simply fit those local diffeomorphisms together to give the desired diffeomorphism. Please help, thanks!",,"['differential-geometry', 'differential-topology', 'riemannian-geometry', 'geodesic']"
96,"Sectional Curvature, Gauss curvature","Sectional Curvature, Gauss curvature",,"I have a problem with a computation which shows that the sectional curvature coincide with the Gauss Curvature in dimension 2. This is the definition of sectional curvature I am using: $$K_{XY}(p)=-\frac{R(X,Y,X,Y)}{\|X\|^2 \|Y\|^2 - \langle X,Y\rangle^2}$$ Conceptually I understood that the previous one is a generalization of the Gauss curvature, but I don't understand how to recover the Gauss curvature from the sectional curvature, in particular I do not see how $-R(E_1,E_2,E_1,E_2)=eg-f^2$ where $eg-f^2$ is the determinant of the second fundamental form, and $E_1,E_2$ are the coordinate vector fields.","I have a problem with a computation which shows that the sectional curvature coincide with the Gauss Curvature in dimension 2. This is the definition of sectional curvature I am using: Conceptually I understood that the previous one is a generalization of the Gauss curvature, but I don't understand how to recover the Gauss curvature from the sectional curvature, in particular I do not see how where is the determinant of the second fundamental form, and are the coordinate vector fields.","K_{XY}(p)=-\frac{R(X,Y,X,Y)}{\|X\|^2 \|Y\|^2 - \langle X,Y\rangle^2} -R(E_1,E_2,E_1,E_2)=eg-f^2 eg-f^2 E_1,E_2","['differential-geometry', 'riemannian-geometry']"
97,"How do I calculate the covariant derivative of a covariant vector, $\nabla_j A_i$?","How do I calculate the covariant derivative of a covariant vector, ?",\nabla_j A_i,"$$\mathbf{A} = A^{1}\mathbf{e_{1}}+A^{2}\mathbf{e_{2}}$$ $$A= \sum_{i=1}^{n}A^{i}\mathbf{e_{i}}$$ Taking the derivative wrt the tangent basis vector and dropping the summation by Einstein convention:  $$\left ( \frac{\partial A^{i}}{\partial x^{j}}+\Gamma _{jk}^{i}A^{k}\right )\mathbf{e_{i}}$$ The covariant derivative of this contravector is $$\nabla_{j}A^{i}\equiv \frac{\partial A^{i}}{\partial x^{j}}+\Gamma _{jk}^{i} A^{k}$$ Now, I would like to determine the covariant derivative of a covariant vector but ran into some problem. Namely, with the red highlighted parts in bold which does not appear in my sketch. $$\nabla_{j}A_{i}\equiv\frac{\partial A_{i}}{\partial x^{j}}{\color{Red} -}\Gamma_{{\color{Red} ij}}^{{\color{Red} k}}A_{k}$$ which the above is a covariant derivative of a covariant vector. Any help is appreciated.","$$\mathbf{A} = A^{1}\mathbf{e_{1}}+A^{2}\mathbf{e_{2}}$$ $$A= \sum_{i=1}^{n}A^{i}\mathbf{e_{i}}$$ Taking the derivative wrt the tangent basis vector and dropping the summation by Einstein convention:  $$\left ( \frac{\partial A^{i}}{\partial x^{j}}+\Gamma _{jk}^{i}A^{k}\right )\mathbf{e_{i}}$$ The covariant derivative of this contravector is $$\nabla_{j}A^{i}\equiv \frac{\partial A^{i}}{\partial x^{j}}+\Gamma _{jk}^{i} A^{k}$$ Now, I would like to determine the covariant derivative of a covariant vector but ran into some problem. Namely, with the red highlighted parts in bold which does not appear in my sketch. $$\nabla_{j}A_{i}\equiv\frac{\partial A_{i}}{\partial x^{j}}{\color{Red} -}\Gamma_{{\color{Red} ij}}^{{\color{Red} k}}A_{k}$$ which the above is a covariant derivative of a covariant vector. Any help is appreciated.",,"['differential-geometry', 'riemannian-geometry', 'tensor-products', 'tensors', 'connections']"
98,Ricci curvature along Killing vector field,Ricci curvature along Killing vector field,,"If $V$ is a Killing vector field, I need to prove that $$V^{m}\nabla_{m}R = 0$$ where $R$ is the Ricci scalar $R = g^{mn}R_{mn}$. I´m having some trouble with this, I already showed that  $$\nabla_{n}\nabla_{a}V^{r} = R^{r}_{ans}V^{s}$$","If $V$ is a Killing vector field, I need to prove that $$V^{m}\nabla_{m}R = 0$$ where $R$ is the Ricci scalar $R = g^{mn}R_{mn}$. I´m having some trouble with this, I already showed that  $$\nabla_{n}\nabla_{a}V^{r} = R^{r}_{ans}V^{s}$$",,"['differential-geometry', 'tensors', 'curvature']"
99,I am confused by the different definitions of manifolds.,I am confused by the different definitions of manifolds.,,"I'm currently learning manifold from Do Carmo's Riemannian Geometry. This is his definition of differentiable manifold: But this is different from what I saw in wiki: A differentiable manifold is a topological manifold equipped with an equivalence class of atlases whose transition maps are all differentiable. In broader terms, a Ck-manifold is a topological manifold with an atlas whose transition maps are all k-times continuously differentiable. Carmo defines differentiable structure on a set, so nothing about continuity of the mappings can be said. While wiki's definition is on topological space in the first place, therefore it make sense to require the charts to be homeomorphism. Moreover, the mapping directions of Carmo's $\bf{x_\alpha}$ and the charts are just the opposite. After defining differentiable structure on set $M$, Carmo then define a $A\subset M$ be open iff $\textbf{x}_\alpha^{-1}(A\cap\textbf{x}_\alpha(U_\alpha))$ is open in $\mathbb R^n$ for all $\alpha$. From this I can see that $\textbf{x}_\alpha$ are continuous. My question is that, are these two definition talking about the same thing? Why do they seems so different? Also, I don't understand why Carmo requires  $\textbf{x}_\alpha^{-1}(W)$ to be open. And how come there are homeomorphism in wiki's definition, but not in Carmo's? I am really confused by the complicated and different definitions. Please help me, thanks.","I'm currently learning manifold from Do Carmo's Riemannian Geometry. This is his definition of differentiable manifold: But this is different from what I saw in wiki: A differentiable manifold is a topological manifold equipped with an equivalence class of atlases whose transition maps are all differentiable. In broader terms, a Ck-manifold is a topological manifold with an atlas whose transition maps are all k-times continuously differentiable. Carmo defines differentiable structure on a set, so nothing about continuity of the mappings can be said. While wiki's definition is on topological space in the first place, therefore it make sense to require the charts to be homeomorphism. Moreover, the mapping directions of Carmo's $\bf{x_\alpha}$ and the charts are just the opposite. After defining differentiable structure on set $M$, Carmo then define a $A\subset M$ be open iff $\textbf{x}_\alpha^{-1}(A\cap\textbf{x}_\alpha(U_\alpha))$ is open in $\mathbb R^n$ for all $\alpha$. From this I can see that $\textbf{x}_\alpha$ are continuous. My question is that, are these two definition talking about the same thing? Why do they seems so different? Also, I don't understand why Carmo requires  $\textbf{x}_\alpha^{-1}(W)$ to be open. And how come there are homeomorphism in wiki's definition, but not in Carmo's? I am really confused by the complicated and different definitions. Please help me, thanks.",,"['differential-geometry', 'manifolds']"
