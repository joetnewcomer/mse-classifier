,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the standard deviation of the mean of 10000 computer generated random numbers between 0 and 1?,What is the standard deviation of the mean of 10000 computer generated random numbers between 0 and 1?,,"So this is probably really simple to anyone who's actually taken stats. I want to generate a number of sets of random numbers from 0 to 1, then find the standard deviations of the means. I'm trying to find out if its worth computing the mean of a set of randoms or to just assume constant value since the SD would be too insignificant. edit: i know i can simulate it but I'd really rather see the math behind it","So this is probably really simple to anyone who's actually taken stats. I want to generate a number of sets of random numbers from 0 to 1, then find the standard deviations of the means. I'm trying to find out if its worth computing the mean of a set of randoms or to just assume constant value since the SD would be too insignificant. edit: i know i can simulate it but I'd really rather see the math behind it",,"['statistics', 'central-limit-theorem']"
1,Expectation of summation squared,Expectation of summation squared,,"I'm studying the Rayleigh distribution given by $$ f_v(x) = \frac{x}{v} \ \exp(-\frac{x^2}{2v}) \ \mathbb{1}\{x \geq 0\}$$ and found an estimator (using method of moments) to be $$ \tilde{v} = \frac{2}{\pi n^2} \left(\sum_{i=1}^{n} X_i \right)^2$$ Now I want to know whether it's biased or not, but I get stuck when I try to compute the expectation of $\tilde{v}$. I have $$ \mathbb{E}[\tilde{v}] = \frac{2}{\pi n^2} \mathbb{E}\left[\left(\sum_{i=1}^{n} X_i \right)^2\right]$$ I tried stating that $\mathbb{E}[X] = \frac{1}{n}\sum_{i=1}^{n} X_i$, which yields $\left(\sum_{i=1}^{n} X_i \right)^2 = n^2 \mathbb{E}[X]^2$ and an unbiased estimator. However, that seems wrong, as that statement is exactly what I used to get the estimator in the first place, so it seems like I'm reasoning in circles here. Is it wrong? If so, how else could I get the bias? Thanks in advance!","I'm studying the Rayleigh distribution given by $$ f_v(x) = \frac{x}{v} \ \exp(-\frac{x^2}{2v}) \ \mathbb{1}\{x \geq 0\}$$ and found an estimator (using method of moments) to be $$ \tilde{v} = \frac{2}{\pi n^2} \left(\sum_{i=1}^{n} X_i \right)^2$$ Now I want to know whether it's biased or not, but I get stuck when I try to compute the expectation of $\tilde{v}$. I have $$ \mathbb{E}[\tilde{v}] = \frac{2}{\pi n^2} \mathbb{E}\left[\left(\sum_{i=1}^{n} X_i \right)^2\right]$$ I tried stating that $\mathbb{E}[X] = \frac{1}{n}\sum_{i=1}^{n} X_i$, which yields $\left(\sum_{i=1}^{n} X_i \right)^2 = n^2 \mathbb{E}[X]^2$ and an unbiased estimator. However, that seems wrong, as that statement is exactly what I used to get the estimator in the first place, so it seems like I'm reasoning in circles here. Is it wrong? If so, how else could I get the bias? Thanks in advance!",,"['statistics', 'expectation', 'parameter-estimation']"
2,Characteristic Function of Variance Gamma Distribution,Characteristic Function of Variance Gamma Distribution,,"I am having trouble on proofing the characteristic function of Variance Gamma distribution. The VG model is obtained from the normal distribution by mixing on the variance parameter. Let $R_t$ be the return and suppose the distribution of $\log(R_t)$ is normal with mean $\mu$ and a random variance $\sigma^2V$ . Both $\mu$ and $\sigma^2$ are known constants. With the distribution of $V$ is taken to be gamma with $c$ and $\gamma$ as the parameters. So the density function of $V$ is $$g(v)=\frac{c^\gamma v^{\gamma-1}e^{-cv}}{\Gamma(\gamma)}$$ If $X=\log(R_t)-\mu$ , then according to Madan and Seneta (1990) the density of $X$ is: $$f(x)=\int_0^\infty[{e^{-x^2/2\sigma^2v}}/(\sigma\sqrt{2v\pi})]g(v)\,dv$$ It is told in the journal that there is no closed form for $f(x)$ . However, the characteristic function of $X$ has the closed form by conditioning on $V$ , given by $$\phi{_X}(u)=[1+(\sigma^2v/m)(u^2/2]^{-m^2/v}$$ where $m=\gamma/c$ and $v=\gamma/c^2$ are both mean and the variance of $g(v)$ respectively. I have tried to do it by several integration techniques, but stuck on the double integral calculation and the complex integration. One of my friend suggest me to solved it using Maple software, but we still don't know which integration method that could solve this problem.","I am having trouble on proofing the characteristic function of Variance Gamma distribution. The VG model is obtained from the normal distribution by mixing on the variance parameter. Let be the return and suppose the distribution of is normal with mean and a random variance . Both and are known constants. With the distribution of is taken to be gamma with and as the parameters. So the density function of is If , then according to Madan and Seneta (1990) the density of is: It is told in the journal that there is no closed form for . However, the characteristic function of has the closed form by conditioning on , given by where and are both mean and the variance of respectively. I have tried to do it by several integration techniques, but stuck on the double integral calculation and the complex integration. One of my friend suggest me to solved it using Maple software, but we still don't know which integration method that could solve this problem.","R_t \log(R_t) \mu \sigma^2V \mu \sigma^2 V c \gamma V g(v)=\frac{c^\gamma v^{\gamma-1}e^{-cv}}{\Gamma(\gamma)} X=\log(R_t)-\mu X f(x)=\int_0^\infty[{e^{-x^2/2\sigma^2v}}/(\sigma\sqrt{2v\pi})]g(v)\,dv f(x) X V \phi{_X}(u)=[1+(\sigma^2v/m)(u^2/2]^{-m^2/v} m=\gamma/c v=\gamma/c^2 g(v)","['integration', 'statistics', 'probability-distributions', 'characteristic-functions']"
3,"""The digits used in artificial numbers are random while the real numbers aren't and their digits distribution is specific to their business""","""The digits used in artificial numbers are random while the real numbers aren't and their digits distribution is specific to their business""",,"Related to the question https://math.stackexchange.com/questions/1924178/tools-to-measure-the-nonrandomness-of-database , I'm somehow looking for some tools to measure the nonrandomness of databases. Igael gave me a hint I don't understand so far : ""the digits used in artificial numbers are random while the real numbers aren't and their digits distribution is specific to their business"". Question : Could anyone be able to explain to me in details what it means exactly?","Related to the question https://math.stackexchange.com/questions/1924178/tools-to-measure-the-nonrandomness-of-database , I'm somehow looking for some tools to measure the nonrandomness of databases. Igael gave me a hint I don't understand so far : ""the digits used in artificial numbers are random while the real numbers aren't and their digits distribution is specific to their business"". Question : Could anyone be able to explain to me in details what it means exactly?",,"['statistics', 'math-history', 'algorithmic-randomness']"
4,Proof of the bound for Laplace distribution,Proof of the bound for Laplace distribution,,"I tried to search online but I couldn't seem to be find a proof for the bound of Laplace distribution. There are 2 bounds for Laplace distribution that is used in my textbook for the derivation of some other statistical result. The two bound are if $Y \sim Lap(b) $, then $Pr[|Y| \geq tb] = exp(-t)$ and (union bound) $\Pr[max_{i\in[1,..,k]} |Y_i| \geq tb] \leq k \exp(-t)$. Can someone show me the derivation for the bounds?","I tried to search online but I couldn't seem to be find a proof for the bound of Laplace distribution. There are 2 bounds for Laplace distribution that is used in my textbook for the derivation of some other statistical result. The two bound are if $Y \sim Lap(b) $, then $Pr[|Y| \geq tb] = exp(-t)$ and (union bound) $\Pr[max_{i\in[1,..,k]} |Y_i| \geq tb] \leq k \exp(-t)$. Can someone show me the derivation for the bounds?",,"['statistics', 'probability-distributions']"
5,On the relevance of CLT to the distributions of sums of i.i.d. random variables,On the relevance of CLT to the distributions of sums of i.i.d. random variables,,"The central limit theorem states (subtle convergence issues aside) that for i.i.d random variables $X_i$ with mean $\mu$ and variance $\sigma^2$, $$\frac{\sum_{i=1}^n X_i - n\mu}{\sigma \sqrt{n}} \to \mathcal N(0,1).$$ Now for large $n$, can we say $$\frac{\sum_{i=1}^n X_i - n\mu}{\sigma \sqrt{n}} \sim \mathcal N(0,1),$$ $$\sum_{i=1}^n X_i - n\mu \sim \mathcal N(0,\sigma^2n),$$ $$\sum_{i=1}^n X_i\sim \mathcal N(n\mu,\sigma^2n)?$$ Seems like this is a very simple closed form approximation to the distribution of the sum of i.i.d random variables.","The central limit theorem states (subtle convergence issues aside) that for i.i.d random variables $X_i$ with mean $\mu$ and variance $\sigma^2$, $$\frac{\sum_{i=1}^n X_i - n\mu}{\sigma \sqrt{n}} \to \mathcal N(0,1).$$ Now for large $n$, can we say $$\frac{\sum_{i=1}^n X_i - n\mu}{\sigma \sqrt{n}} \sim \mathcal N(0,1),$$ $$\sum_{i=1}^n X_i - n\mu \sim \mathcal N(0,\sigma^2n),$$ $$\sum_{i=1}^n X_i\sim \mathcal N(n\mu,\sigma^2n)?$$ Seems like this is a very simple closed form approximation to the distribution of the sum of i.i.d random variables.",,"['statistics', 'normal-distribution', 'probability-limit-theorems']"
6,Two-Sample Hotelling's $T^2$ test example work through.,Two-Sample Hotelling's  test example work through.,T^2,"Let $\textrm{x}_{11},\ldots,\textrm{x}_{1n_1}$ and $\textrm{x}_{21},\ldots,\textrm{x}_{2n_2}$ be two observed samples where $\textrm{x}_{ij}$ is a $p$ vector from $\sim N_p (\mu_1,\Sigma)$ and $\sim N_p(\mu_2,\Sigma)$ for the two samples respectevely. From these samples I can find: $\bar{\textrm{x}}_1,\bar{\textrm{x}}_2,S_1,S_2$ where $$S_1=\frac{1}{n_1} \sum_j (\textrm{x}_{1j}-\bar{\textrm{x}}_1)(\textrm{x}_{1j}-\bar{\textrm{x}}_1)'$$ $$S_2=\frac{1}{n_2} \sum_j (\textrm{x}_{2j}-\bar{\textrm{x}}_2)(\textrm{x}_{2j}-\bar{\textrm{x}}_2)'$$ I have that if I let $\mu_1-\mu_2=\delta,$ $$ \bar{\textrm{x}}_1-\bar{\textrm{x}}_2 \sim N_p\left( \delta, \frac{n_1+n_2}{n_1n_2}\Sigma \right)$$ Assume weighted covariance matrix $S=\frac{1}{n_1+n_2}(n_1S_1+n_2S_2)$. How is the $S$ distributed? I know it should be Wishart distribution, but I'm not sure how. I think that $n_1S_1 \sim (n_1,\Sigma)$ or $(n_1-1,S_1)$ if $S_1$ is an estimate of $\Sigma$ and the same argument for $n_2S_2 \sim (n_2,\Sigma)$ My guess is: $S\sim (n_1+n_2-2,\Sigma)$, but I don't fully understand why. Now I'm having trouble with with $T^2$ distribution. My notes only tell me what to do when $\textrm{x}\sim N(\mu,\Sigma)$. But in our case $ \bar{\textrm{x}}_1-\bar{\textrm{x}}_2 \sim N_p(\delta, \frac{n_1+n_2}{n_1n_2}\Sigma)$. Thus I tried to bring it into the form $N(\mu,\Sigma)$. $$\frac{\sqrt{n_1n_2}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}} \sim N_p(\delta, \Sigma)$$ and by the theorem in the book I should try: $$t^2=(n_1+n_2-2) \left(\frac{\sqrt{n_1n_2} (\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}} -\delta\right) S^{-1} \left( \frac{\sqrt{n_1n_2} (\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}}-\delta\right)'$$ But my book has the solution: $$t^2=\frac{n_1n_2(n_1+n_2-2)}{(n_1+n_2)^2}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2-\delta)S^{-1}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2-\delta)'\sim T^2_{p,n_1+n_2-2}$$","Let $\textrm{x}_{11},\ldots,\textrm{x}_{1n_1}$ and $\textrm{x}_{21},\ldots,\textrm{x}_{2n_2}$ be two observed samples where $\textrm{x}_{ij}$ is a $p$ vector from $\sim N_p (\mu_1,\Sigma)$ and $\sim N_p(\mu_2,\Sigma)$ for the two samples respectevely. From these samples I can find: $\bar{\textrm{x}}_1,\bar{\textrm{x}}_2,S_1,S_2$ where $$S_1=\frac{1}{n_1} \sum_j (\textrm{x}_{1j}-\bar{\textrm{x}}_1)(\textrm{x}_{1j}-\bar{\textrm{x}}_1)'$$ $$S_2=\frac{1}{n_2} \sum_j (\textrm{x}_{2j}-\bar{\textrm{x}}_2)(\textrm{x}_{2j}-\bar{\textrm{x}}_2)'$$ I have that if I let $\mu_1-\mu_2=\delta,$ $$ \bar{\textrm{x}}_1-\bar{\textrm{x}}_2 \sim N_p\left( \delta, \frac{n_1+n_2}{n_1n_2}\Sigma \right)$$ Assume weighted covariance matrix $S=\frac{1}{n_1+n_2}(n_1S_1+n_2S_2)$. How is the $S$ distributed? I know it should be Wishart distribution, but I'm not sure how. I think that $n_1S_1 \sim (n_1,\Sigma)$ or $(n_1-1,S_1)$ if $S_1$ is an estimate of $\Sigma$ and the same argument for $n_2S_2 \sim (n_2,\Sigma)$ My guess is: $S\sim (n_1+n_2-2,\Sigma)$, but I don't fully understand why. Now I'm having trouble with with $T^2$ distribution. My notes only tell me what to do when $\textrm{x}\sim N(\mu,\Sigma)$. But in our case $ \bar{\textrm{x}}_1-\bar{\textrm{x}}_2 \sim N_p(\delta, \frac{n_1+n_2}{n_1n_2}\Sigma)$. Thus I tried to bring it into the form $N(\mu,\Sigma)$. $$\frac{\sqrt{n_1n_2}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}} \sim N_p(\delta, \Sigma)$$ and by the theorem in the book I should try: $$t^2=(n_1+n_2-2) \left(\frac{\sqrt{n_1n_2} (\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}} -\delta\right) S^{-1} \left( \frac{\sqrt{n_1n_2} (\bar{\textrm{x}}_1-\bar{\textrm{x}}_2)}{\sqrt{n_1+n_2}}-\delta\right)'$$ But my book has the solution: $$t^2=\frac{n_1n_2(n_1+n_2-2)}{(n_1+n_2)^2}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2-\delta)S^{-1}(\bar{\textrm{x}}_1-\bar{\textrm{x}}_2-\delta)'\sim T^2_{p,n_1+n_2-2}$$",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
7,Inverse Gaussian maximum likelihood estimation lambda,Inverse Gaussian maximum likelihood estimation lambda,,"For a regular IG($\mu$,$\lambda$) with pdf:    $f(x;\mu,\lambda) = \frac{\lambda}{2\pi x^3}^{1/2}$ $e^{\frac{-\lambda}{2\mu^2}\frac{(x-\mu)^2}{x}}$ Taking the log likelihood to produce $\frac{n}{2}Ln(\lambda)+ \frac{n}{2}Ln(1/2\pi) + K - \frac{\lambda}{2\mu} \sum_1^n{\frac{(x_i-\mu)^2}{x_i}}$, k some term not in $\mu \,or \lambda$ Proceeding to differentiate the one relevant term at the end I get that $\sum_1^n{\frac{(x_i-mu)^2}{x_i}}$ = 0 If this is correct, how does this imply that $\mu$ = sample mean? What about the second parameter $\lambda$? It should reduce to $\lambda hat = \frac{1}{n}\sum_1^n{(\frac{1}{x_i} - \frac{1}{\mu^2})}$ Where $\mu$ is the estimated $\mu$ just shown.","For a regular IG($\mu$,$\lambda$) with pdf:    $f(x;\mu,\lambda) = \frac{\lambda}{2\pi x^3}^{1/2}$ $e^{\frac{-\lambda}{2\mu^2}\frac{(x-\mu)^2}{x}}$ Taking the log likelihood to produce $\frac{n}{2}Ln(\lambda)+ \frac{n}{2}Ln(1/2\pi) + K - \frac{\lambda}{2\mu} \sum_1^n{\frac{(x_i-\mu)^2}{x_i}}$, k some term not in $\mu \,or \lambda$ Proceeding to differentiate the one relevant term at the end I get that $\sum_1^n{\frac{(x_i-mu)^2}{x_i}}$ = 0 If this is correct, how does this imply that $\mu$ = sample mean? What about the second parameter $\lambda$? It should reduce to $\lambda hat = \frac{1}{n}\sum_1^n{(\frac{1}{x_i} - \frac{1}{\mu^2})}$ Where $\mu$ is the estimated $\mu$ just shown.",,"['calculus', 'statistics', 'maximum-likelihood']"
8,What is the error for the estimate of the probability of a binomial distribution?,What is the error for the estimate of the probability of a binomial distribution?,,"I want to estimate the probability $p$ of a Binomial distribution $B(n,p)$. I draw $n$ samples and get $s$ successes. The estimate for $p$ is $$\hat{p} = s/n.$$ What is the variance of $\hat{p}$?","I want to estimate the probability $p$ of a Binomial distribution $B(n,p)$. I draw $n$ samples and get $s$ successes. The estimate for $p$ is $$\hat{p} = s/n.$$ What is the variance of $\hat{p}$?",,"['statistics', 'binomial-distribution', 'parameter-estimation']"
9,Random variable conditioned to gamma distribution,Random variable conditioned to gamma distribution,,"Suppose that we have two random variable $X$ and $Y$, where $X$ ~   $Gamma (2,1)$ and that $Y|X$ ~ $U(0,x)$. Find the distribution of Y. The way I approach it as follows: We know that $f_X(x) = xe^{-x}$ for $x\in(0,\infty)$ and that $f_{Y|X}(y|x) = 1/x$ for $y\in(0,x)$. To find the distribution we integrate: $$f_Y(y) =\int_0^{\infty} f(x,y)dx = \int_0^{\infty} f_{Y|X}(y|x)f(x)dx = \int_0^{\infty} e^{-x}dx = 1$$ What distribution is this, since $y$ has the support $(0, \infty)$? Am I doing anything wrong?","Suppose that we have two random variable $X$ and $Y$, where $X$ ~   $Gamma (2,1)$ and that $Y|X$ ~ $U(0,x)$. Find the distribution of Y. The way I approach it as follows: We know that $f_X(x) = xe^{-x}$ for $x\in(0,\infty)$ and that $f_{Y|X}(y|x) = 1/x$ for $y\in(0,x)$. To find the distribution we integrate: $$f_Y(y) =\int_0^{\infty} f(x,y)dx = \int_0^{\infty} f_{Y|X}(y|x)f(x)dx = \int_0^{\infty} e^{-x}dx = 1$$ What distribution is this, since $y$ has the support $(0, \infty)$? Am I doing anything wrong?",,"['probability', 'integration', 'statistics', 'probability-distributions']"
10,"joint, marginal density function","joint, marginal density function",,"Let $X$ , $Y$ be two independent, random variables each follows the exponential law with parameter $\lambda$ a. Find the joint density function $f(u,v)$ where $V=X+Y$ and $U=X$? b. Find the marginal density function $f(v)$? Solution: a. I got $f(u,v)= {\lambda}^2 e^{-v}$ is this the right answer ? if yes should the double integral be 1 in this case ... I don't think it is 1. b. the joint density function doesn't depend on $u$ explicitly, what should we do in this case?","Let $X$ , $Y$ be two independent, random variables each follows the exponential law with parameter $\lambda$ a. Find the joint density function $f(u,v)$ where $V=X+Y$ and $U=X$? b. Find the marginal density function $f(v)$? Solution: a. I got $f(u,v)= {\lambda}^2 e^{-v}$ is this the right answer ? if yes should the double integral be 1 in this case ... I don't think it is 1. b. the joint density function doesn't depend on $u$ explicitly, what should we do in this case?",,"['probability', 'probability-theory', 'statistics']"
11,Variance of the sum of correlated variables,Variance of the sum of correlated variables,,"If the variance of two correlated variables is: $$Var(r_1+r_2)=\sigma^2_1+\sigma^2_2+2\textrm{cov}(r_1,r_2)=\sigma^2_1+\sigma^2_2+2\rho\sigma_1\sigma_2$$ where $r_1$ and $r_2$ are vectors, then what is the multivariate representation of this. So, if $R_1$ and $R_2$ both denote a matrix we get $$Var(R_1+R_2)=\Sigma_1+\Sigma_2+...$$ where $\Sigma_i$ denotes the covariance matrix for $R_i$. Anyone knows how to fill in the dots?","If the variance of two correlated variables is: $$Var(r_1+r_2)=\sigma^2_1+\sigma^2_2+2\textrm{cov}(r_1,r_2)=\sigma^2_1+\sigma^2_2+2\rho\sigma_1\sigma_2$$ where $r_1$ and $r_2$ are vectors, then what is the multivariate representation of this. So, if $R_1$ and $R_2$ both denote a matrix we get $$Var(R_1+R_2)=\Sigma_1+\Sigma_2+...$$ where $\Sigma_i$ denotes the covariance matrix for $R_i$. Anyone knows how to fill in the dots?",,"['probability', 'statistics', 'random-variables', 'correlation']"
12,"Show that $n \to \infty, \sqrt{n}(Y_{n}-p) \rightarrow N(0,p(1-p)$",Show that,"n \to \infty, \sqrt{n}(Y_{n}-p) \rightarrow N(0,p(1-p)","Can anyone show me the correct working out to find the variance for $Y_n$, My variance seems to be $\frac{p(1-p)}{n}$","Can anyone show me the correct working out to find the variance for $Y_n$, My variance seems to be $\frac{p(1-p)}{n}$",,['statistics']
13,Central Limit Theorem: Show that $\mathrm{Pois}(n)$ distribution if approximately Normal if $n$ is a large positive integer,Central Limit Theorem: Show that  distribution if approximately Normal if  is a large positive integer,\mathrm{Pois}(n) n,"Here is what the solution in the textbook says: Let $S_n=X_1+\cdots+X_n$, with $X_1,X_2,\ldots$ i.i.d. $\sim \mathrm{Pois}(1)$. Then $S_n \sim\mathrm{Pois}(n)$ and for $n$ large, $S_n$ is approximately $N(n,n)$ by the CLT. Based on my understanding of the Central Limit Theorem, $\overline{S}_n$ is approximately $N(n,n)$, not $S_n$. Here is my proof: The Central Limit Theorem says   \begin{align} \sqrt{n}\left(\frac{\overline{S}_n-\mu}{\sigma}\right)\xrightarrow{n \rightarrow \infty}\mathcal{N}(0,1) \\ \sqrt{n}\left(\frac{\overline{S}_n-n}{n}\right)\xrightarrow{n \rightarrow \infty}\mathcal{N}(0,1) \\ \end{align} With a location scale transformation we get: \begin{align} \overline{S}_n-n \xrightarrow{n \rightarrow \infty}\mathcal{N}(0,n) \\ \overline{S}_n \xrightarrow{n \rightarrow \infty}\mathcal{N}(n,n) \\ \end{align} And since $\overline{S}_n=\frac{1}{n}S_n$,  $S_n  \xrightarrow{n \rightarrow \infty}\mathcal{N}(n,n^3)$. Is my proof incorrect or is the problem just a difference in notation?","Here is what the solution in the textbook says: Let $S_n=X_1+\cdots+X_n$, with $X_1,X_2,\ldots$ i.i.d. $\sim \mathrm{Pois}(1)$. Then $S_n \sim\mathrm{Pois}(n)$ and for $n$ large, $S_n$ is approximately $N(n,n)$ by the CLT. Based on my understanding of the Central Limit Theorem, $\overline{S}_n$ is approximately $N(n,n)$, not $S_n$. Here is my proof: The Central Limit Theorem says   \begin{align} \sqrt{n}\left(\frac{\overline{S}_n-\mu}{\sigma}\right)\xrightarrow{n \rightarrow \infty}\mathcal{N}(0,1) \\ \sqrt{n}\left(\frac{\overline{S}_n-n}{n}\right)\xrightarrow{n \rightarrow \infty}\mathcal{N}(0,1) \\ \end{align} With a location scale transformation we get: \begin{align} \overline{S}_n-n \xrightarrow{n \rightarrow \infty}\mathcal{N}(0,n) \\ \overline{S}_n \xrightarrow{n \rightarrow \infty}\mathcal{N}(n,n) \\ \end{align} And since $\overline{S}_n=\frac{1}{n}S_n$,  $S_n  \xrightarrow{n \rightarrow \infty}\mathcal{N}(n,n^3)$. Is my proof incorrect or is the problem just a difference in notation?",,"['probability', 'statistics', 'poisson-distribution', 'central-limit-theorem']"
14,Showing that the multivariate normal density integrates to 1,Showing that the multivariate normal density integrates to 1,,"This is NOT the same as How to show the normal density integrates to 1? . Let $\mathbf{x} \in \mathbb{R}^d$ be a multivariate normal random vector, with $\mathbb{E}[\mathbf{x}] = \boldsymbol\mu$ and positive-definite $\text{Var}[\mathbf{x}] = \boldsymbol\Sigma$. Then $$\int_{\mathbb{R}^d}\dfrac{1}{(2\pi)^{n/2}|\boldsymbol\Sigma|^{1/2}}\exp\left[-\dfrac{1}{2}(\mathbf{x}-\boldsymbol\mu)^{T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\right]\text{ d}\mathbf{x}$$ should equal $1$. How do I show this? I am allowed to use the one-dimensional case as a fact, so I thought, perhaps I should use induction on $d$. One-dimensional case is true, great. Now suppose it's true for $k$ dimensions. At the $k+1$th dimension, the difficulty is working with the new variance-covariance matrix and the Mahalanobis distance term   $$(\mathbf{x}-\boldsymbol\mu)^{T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\text{.}$$ Does anyone have any suggestions? I don't need a complete solution, but I would like a starting point.","This is NOT the same as How to show the normal density integrates to 1? . Let $\mathbf{x} \in \mathbb{R}^d$ be a multivariate normal random vector, with $\mathbb{E}[\mathbf{x}] = \boldsymbol\mu$ and positive-definite $\text{Var}[\mathbf{x}] = \boldsymbol\Sigma$. Then $$\int_{\mathbb{R}^d}\dfrac{1}{(2\pi)^{n/2}|\boldsymbol\Sigma|^{1/2}}\exp\left[-\dfrac{1}{2}(\mathbf{x}-\boldsymbol\mu)^{T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\right]\text{ d}\mathbf{x}$$ should equal $1$. How do I show this? I am allowed to use the one-dimensional case as a fact, so I thought, perhaps I should use induction on $d$. One-dimensional case is true, great. Now suppose it's true for $k$ dimensions. At the $k+1$th dimension, the difficulty is working with the new variance-covariance matrix and the Mahalanobis distance term   $$(\mathbf{x}-\boldsymbol\mu)^{T}\boldsymbol\Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\text{.}$$ Does anyone have any suggestions? I don't need a complete solution, but I would like a starting point.",,"['probability', 'statistics', 'multivariable-calculus']"
15,problem with Combination and Permutation,problem with Combination and Permutation,,"Four married couples have bought 8 seats in a row for a concert. In how many ways can they be seated: a)if each couple is to sit together? (8)(1)(6)(1)(4)(1)(2)(1) b)if all men sit together? n-k+1=8-4+1=5 so, (5)(4! 4!)=5! 4! c)If no man sits next to his wife ?(This is a non-trivial questions) 8*6*6*4*4*2*2*1=18432 ways could you please check it for me ?","Four married couples have bought 8 seats in a row for a concert. In how many ways can they be seated: a)if each couple is to sit together? (8)(1)(6)(1)(4)(1)(2)(1) b)if all men sit together? n-k+1=8-4+1=5 so, (5)(4! 4!)=5! 4! c)If no man sits next to his wife ?(This is a non-trivial questions) 8*6*6*4*4*2*2*1=18432 ways could you please check it for me ?",,"['probability', 'statistics']"
16,Compute the cumulative distribution function of the variable $R=\sqrt{X^2+Y^2}$,Compute the cumulative distribution function of the variable,R=\sqrt{X^2+Y^2},"I've returned to the study of statistics after a long while and I'm trying to solve some problems. One of those is the next: Suppose $X$ and $Y$ are random independent variables with normal distribution $N(0,1)$, which has distribution function $$f(x)=\frac{1}{\sqrt{2\pi}}\exp{\left(\frac{-x^2}{2}\right)}$$ Compute the cumulative distribution function of the variable $$R=\sqrt{X^2+Y^2}$$ I'd appreciate some hints to solve these kind of problems. Thanks.","I've returned to the study of statistics after a long while and I'm trying to solve some problems. One of those is the next: Suppose $X$ and $Y$ are random independent variables with normal distribution $N(0,1)$, which has distribution function $$f(x)=\frac{1}{\sqrt{2\pi}}\exp{\left(\frac{-x^2}{2}\right)}$$ Compute the cumulative distribution function of the variable $$R=\sqrt{X^2+Y^2}$$ I'd appreciate some hints to solve these kind of problems. Thanks.",,"['statistics', 'probability-distributions']"
17,Linear regression proof that SST = SSR + SSE,Linear regression proof that SST = SSR + SSE,,My teacher wanted us to try to attempt to prove this. So I noticed the summation on the left represents SST (total sum of squares) and on the right I noticed the second summation was the measure in variability of the y's in the linear regression term. However what is that first summation? Also how can I manipulate the right side to get the left side?,My teacher wanted us to try to attempt to prove this. So I noticed the summation on the left represents SST (total sum of squares) and on the right I noticed the second summation was the measure in variability of the y's in the linear regression term. However what is that first summation? Also how can I manipulate the right side to get the left side?,,"['statistics', 'linear-regression']"
18,Proof explanation - weak law of large numbers,Proof explanation - weak law of large numbers,,"Let $(X_i)$ be i.i.d. random variables with mean $\mu$ and finite variance. Then $$\dfrac{X_1 + \dots + X_n}{n} \to \mu \text{ weakly }$$ I have the proof here: What I don't understand is, why it suffices to show that $F_{S_n/n}(\mu - \epsilon) \to 0$ and $F_{S_n / n}(\mu + \epsilon) \to 1$ as $\epsilon \to 0$ for every $x$. AFAIK, we must prove that $F_{S_n/n}$ converges to the distribution function for $\mu$ for every continuity point of $F$ - why do we consider the discontinuity point?, i.e. it converges to $1$ for $x \geq \mu$ and to $0$ for $x < \mu$, but I am not sure how this shows that.","Let $(X_i)$ be i.i.d. random variables with mean $\mu$ and finite variance. Then $$\dfrac{X_1 + \dots + X_n}{n} \to \mu \text{ weakly }$$ I have the proof here: What I don't understand is, why it suffices to show that $F_{S_n/n}(\mu - \epsilon) \to 0$ and $F_{S_n / n}(\mu + \epsilon) \to 1$ as $\epsilon \to 0$ for every $x$. AFAIK, we must prove that $F_{S_n/n}$ converges to the distribution function for $\mu$ for every continuity point of $F$ - why do we consider the discontinuity point?, i.e. it converges to $1$ for $x \geq \mu$ and to $0$ for $x < \mu$, but I am not sure how this shows that.",,"['probability', 'probability-theory', 'statistics', 'proof-explanation']"
19,Is there a statistical measure of bitwise entropy?,Is there a statistical measure of bitwise entropy?,,"(Somewhat inspired by this website , particularly Section III. Also, I might be using a different definition of entropy than usual; what I am using is closest to the physics definition (the one I encountered first) of the amount of disorder in a system.) Consider the following two ""random"" 256-bit strings: 1110001011010010101000001111001100001100011111000111011011101000000000000001111110010110010100011101010010111110000010010101001001101100111110011000000110111111000111101111000011010100001001100010010010011000000011101110000000110001101100000110111001100011 ...created via RNG, and: 1001001001010010001001010010110100101100100101110001001010110100101001001010010010000110001000011010000010101010001001100110100010001010101010101100010010010011100100010010111010001011010101001011101000101010001101010100100101010010101010010010010101100101 ...created by ""randomly"" hammering away on the 1 and 0 keys on the keyboard. To the layperson, the second string ""looks"" more random than the first, partly because the first has longer runs, including one of length 14. However, the string 1010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010 ...would be considered relatively non-random (even more so than the first string), as although it has only runs of length 1, when elements are grouped into pairs, the pairs effectively become a run of length 128 if each pair is considered to be one unit. My question is, is there a formal measure of bitwise entropy that takes into account these factors? My first thought towards a potential answer to this question would be something along these lines: $$N-\sum_{\ell=1}^{20 \text{ (or so)}} {{\sum_{k=1}^{\text{(# of runs)}} (\text{length of $k$th run})^2}\over \ell}$$ where $N$ is the maximum value of the rest of the expression (so that the minimum value of the whole thing is zero). This takes into account the fact that longer runs are worse, as well as the reducing importance of shorter runs as the group length gets longer.","(Somewhat inspired by this website , particularly Section III. Also, I might be using a different definition of entropy than usual; what I am using is closest to the physics definition (the one I encountered first) of the amount of disorder in a system.) Consider the following two ""random"" 256-bit strings: 1110001011010010101000001111001100001100011111000111011011101000000000000001111110010110010100011101010010111110000010010101001001101100111110011000000110111111000111101111000011010100001001100010010010011000000011101110000000110001101100000110111001100011 ...created via RNG, and: 1001001001010010001001010010110100101100100101110001001010110100101001001010010010000110001000011010000010101010001001100110100010001010101010101100010010010011100100010010111010001011010101001011101000101010001101010100100101010010101010010010010101100101 ...created by ""randomly"" hammering away on the 1 and 0 keys on the keyboard. To the layperson, the second string ""looks"" more random than the first, partly because the first has longer runs, including one of length 14. However, the string 1010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010101010 ...would be considered relatively non-random (even more so than the first string), as although it has only runs of length 1, when elements are grouped into pairs, the pairs effectively become a run of length 128 if each pair is considered to be one unit. My question is, is there a formal measure of bitwise entropy that takes into account these factors? My first thought towards a potential answer to this question would be something along these lines: $$N-\sum_{\ell=1}^{20 \text{ (or so)}} {{\sum_{k=1}^{\text{(# of runs)}} (\text{length of $k$th run})^2}\over \ell}$$ where $N$ is the maximum value of the rest of the expression (so that the minimum value of the whole thing is zero). This takes into account the fact that longer runs are worse, as well as the reducing importance of shorter runs as the group length gets longer.",,"['statistics', 'reference-request', 'random', 'entropy']"
20,suppose a sample is taken from a symmetric distribution whose tails decrease more slowly than those of a normal distribution,suppose a sample is taken from a symmetric distribution whose tails decrease more slowly than those of a normal distribution,,"I was wondering how to go about this question about Probability QQ Plots, the question is, suppose a sample is taken from a symmetric distribution whose tails decrease more slowly than those of a normal distribution. what would be the qualitative shape of a normal probability plot of this sample?","I was wondering how to go about this question about Probability QQ Plots, the question is, suppose a sample is taken from a symmetric distribution whose tails decrease more slowly than those of a normal distribution. what would be the qualitative shape of a normal probability plot of this sample?",,['statistics']
21,Calculate conditional pdf of uniformly cos and sin,Calculate conditional pdf of uniformly cos and sin,,"Suppose $\theta$ is uniformly distributed within the interval of $(0, 2\pi)$ and $X = \cos θ, Y = \sin θ.$ Now I need to figure out $f_Y(y|x).$ I get $f_Y(y|x) = f_{X,Y}(x,y)/f_X(x),$ where $f_{X,Y}(x,y) = 1/\pi$ since $X$ and $Y$ compose a uniformly distributed unit circle. $f_X(x) = \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} f_{X,Y}(x,y)\,dx,$   which results in $(2/\pi)\sqrt{1-x^2}.$ So for $f_Y(y|x).$ I got  $0.5/\sqrt{1-x^2}.$ However, the solution given by the book is $0.5\delta(y-\sqrt{1-x^2}) + 0.5\delta(y+\sqrt{1-x^2})$. I'd like to know where I got it wrong?","Suppose $\theta$ is uniformly distributed within the interval of $(0, 2\pi)$ and $X = \cos θ, Y = \sin θ.$ Now I need to figure out $f_Y(y|x).$ I get $f_Y(y|x) = f_{X,Y}(x,y)/f_X(x),$ where $f_{X,Y}(x,y) = 1/\pi$ since $X$ and $Y$ compose a uniformly distributed unit circle. $f_X(x) = \int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} f_{X,Y}(x,y)\,dx,$   which results in $(2/\pi)\sqrt{1-x^2}.$ So for $f_Y(y|x).$ I got  $0.5/\sqrt{1-x^2}.$ However, the solution given by the book is $0.5\delta(y-\sqrt{1-x^2}) + 0.5\delta(y+\sqrt{1-x^2})$. I'd like to know where I got it wrong?",,"['probability', 'statistics']"
22,Formula for cumulative binomial probability,Formula for cumulative binomial probability,,"Is there a simple formula for finding a value of a cumulative binomial probability, eg. like the ones put in cumulative binomial probability tables? eg. X~B(50, 0.234) Find the cumulative binomial probability for 32, with one equation.","Is there a simple formula for finding a value of a cumulative binomial probability, eg. like the ones put in cumulative binomial probability tables? eg. X~B(50, 0.234) Find the cumulative binomial probability for 32, with one equation.",,['statistics']
23,Least squares regression with two predictor variables (exponential functions of time),Least squares regression with two predictor variables (exponential functions of time),,"Question cropped from textbook (Apologies for the link- I don't have enough rep to post the actual image.) [Now pasted below. Ed.] I've come across a question in a textbook (linked above) requiring a least-squares fitted model of a sum of exponential terms. I have some experience using the least-squares criterion with single terms, ie  fitting a curve $y = Ae^x$ to some data, but none with sums of terms. Would it suffice to fit separate curves to each term? I'm not at all confident in my approach here, thanks for any help.","Question cropped from textbook (Apologies for the link- I don't have enough rep to post the actual image.) [Now pasted below. Ed.] I've come across a question in a textbook (linked above) requiring a least-squares fitted model of a sum of exponential terms. I have some experience using the least-squares criterion with single terms, ie  fitting a curve $y = Ae^x$ to some data, but none with sums of terms. Would it suffice to fit separate curves to each term? I'm not at all confident in my approach here, thanks for any help.",,"['statistics', 'physics', 'least-squares']"
24,Sum modulo of two random variables with one uniformly distributed,Sum modulo of two random variables with one uniformly distributed,,"I have to use the following proposition, but since I'm not that into statistics, I don't know how to prove it formally. If there are two independent random variables $A$ and $B$ over $\{0,1,...,m-1\}$, with $A$ uniformly distributed, the random variable $C = A + B \text{ mod }m$ is also uniformly distributed (the distribution of $B$ is arbitrary). I think you can argue that if $B$ has a certain value $b$, then $A + b \text{ mod }m$ is uniformly distributed. Can anyone help me to write this down correctly?","I have to use the following proposition, but since I'm not that into statistics, I don't know how to prove it formally. If there are two independent random variables $A$ and $B$ over $\{0,1,...,m-1\}$, with $A$ uniformly distributed, the random variable $C = A + B \text{ mod }m$ is also uniformly distributed (the distribution of $B$ is arbitrary). I think you can argue that if $B$ has a certain value $b$, then $A + b \text{ mod }m$ is uniformly distributed. Can anyone help me to write this down correctly?",,"['probability', 'probability-theory', 'statistics']"
25,Is this histogram considered bimodal?,Is this histogram considered bimodal?,,"Is this histogram bimodal? Because when I google what a bimodal histogram looks like, I keep getting images that say histograms like these are considered bimodal. Isn't it unimodal because the highest peak is the only mode? Am I misunderstanding something?","Is this histogram bimodal? Because when I google what a bimodal histogram looks like, I keep getting images that say histograms like these are considered bimodal. Isn't it unimodal because the highest peak is the only mode? Am I misunderstanding something?",,"['statistics', 'data-analysis', 'visualization', 'descriptive-statistics']"
26,Probability of a random Permutation [closed],Probability of a random Permutation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Pick up a random permutation in S5(assuming all elements have the equal chance to be picked). Find the probability that the sum of the first three entries of σ is less than or equal to sum of last two. My try: I mean there will be 5! different combination possible, do I have to look at each of it?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Pick up a random permutation in S5(assuming all elements have the equal chance to be picked). Find the probability that the sum of the first three entries of σ is less than or equal to sum of last two. My try: I mean there will be 5! different combination possible, do I have to look at each of it?",,"['probability', 'statistics']"
27,Proof of independence of $\bar{X}$ and $S^2$: Trouble understanding $n$-dimensional Jacobian Result,Proof of independence of  and : Trouble understanding -dimensional Jacobian Result,\bar{X} S^2 n,"I'm trying to work through the proof given here that the sample mean and sample variance of a random sample $X_1, X_2, ..., X_n \sim N(\mu,\sigma^2)$ are independent. The part I can't seem to follow is the assertion that the Jacobian of the transformations... $$  Y_1 = \bar{X} \\  Y_2 = X_2 - \bar{X} \\  \vdots \\  Y_n = X_n - \bar{X} \\ $$ is equal to $n$. I must be missing something simple, but when I create the matrix of partial derivatives, the determinant seems to always be one? $$   \det\begin{bmatrix}     \frac{dY_1}{d\bar{X}} & \frac{dY_1}{dX_2} & \dots & \frac{dY_1}{dX_n} \\     \frac{dY_2}{d\bar{X}} & \frac{dY_2}{dX_2} & \dots & \frac{dY_2}{dX_n} \\     \vdots & \vdots & \ddots & \vdots \\     \frac{dY_n}{d\bar{X}} & \frac{dY_n}{dX_2} & \dots & \frac{dY_n}{dX_n} \\   \end{bmatrix} =    \det\begin{bmatrix}     1 & 0 & \dots & 0 \\     -1 & 1 & \dots & 0 \\     \vdots & \vdots & \ddots & \vdots \\     -1 & 0 & \dots & 1 \\   \end{bmatrix} = 1 $$ How am I setting up the Jacobian incorrectly? Thanks in advance!","I'm trying to work through the proof given here that the sample mean and sample variance of a random sample $X_1, X_2, ..., X_n \sim N(\mu,\sigma^2)$ are independent. The part I can't seem to follow is the assertion that the Jacobian of the transformations... $$  Y_1 = \bar{X} \\  Y_2 = X_2 - \bar{X} \\  \vdots \\  Y_n = X_n - \bar{X} \\ $$ is equal to $n$. I must be missing something simple, but when I create the matrix of partial derivatives, the determinant seems to always be one? $$   \det\begin{bmatrix}     \frac{dY_1}{d\bar{X}} & \frac{dY_1}{dX_2} & \dots & \frac{dY_1}{dX_n} \\     \frac{dY_2}{d\bar{X}} & \frac{dY_2}{dX_2} & \dots & \frac{dY_2}{dX_n} \\     \vdots & \vdots & \ddots & \vdots \\     \frac{dY_n}{d\bar{X}} & \frac{dY_n}{dX_2} & \dots & \frac{dY_n}{dX_n} \\   \end{bmatrix} =    \det\begin{bmatrix}     1 & 0 & \dots & 0 \\     -1 & 1 & \dots & 0 \\     \vdots & \vdots & \ddots & \vdots \\     -1 & 0 & \dots & 1 \\   \end{bmatrix} = 1 $$ How am I setting up the Jacobian incorrectly? Thanks in advance!",,"['statistics', 'independence']"
28,"If X and Z are independent and Y and Z are independent random variables, is cov(XY, Z) = 0?","If X and Z are independent and Y and Z are independent random variables, is cov(XY, Z) = 0?",,"Let $X$, $Y,$ and $Z$ be random variables. (There are no restrictions on these variables, but you may assume that these are continuous random variables if you want.) Suppose that $X$ and $Z$ are independent, and also suppose that $Y$ and $Z$ are independent. Does it follow that $\mathrm{cov}(XY,Z)=0$? (I understand that $XY$ and $Z$ may not be independent, but this does not rule out the zero covariance.) Under the assumption that $X$ and $Z$ are independent and that $Y$ and $Z$ are independent, I am able to show that $$\mathrm{cov}(X,YZ) = \mathrm{cov}(Y,XZ) = \mathrm{cov}(XY,Z) + \mathrm{E}[Z]\cdot \mathrm{cov}(X,Y).$$ Showing this is fairly straightforward: $\mathrm{cov}(XY,Z)=\mathrm{E}[XYZ]-\mathrm{E}[XY]\cdot\mathrm{E}[Z]$; in addition, $\mathrm{cov}(X,YZ)=\mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[YZ]=\mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]$, implying that $\mathrm{cov}(X,YZ)=\mathrm{cov}(XY,Z)+\mathrm{E}[XY]\cdot\mathrm{E}[Z]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]$, which is obviously equal to $\mathrm{cov}(XY,Z) + \mathrm{E}[Z]\cdot\mathrm{cov}(X,Y).$ And, of course, $\mathrm{cov}(X,YZ) = \mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]= \mathrm{cov}(Y,XZ)$, proving the above result. However, to proceed further, my intuition tells me that $\mathrm{cov}(XY,Z) = 0$ and thus that $\mathrm{cov}(X,YZ) = \mathrm{cov}(Y,XZ) = \mathrm{E}[Z]\cdot\mathrm{cov}(X,Y)$. Am I wrong in thinking that $\mathrm{cov}(XY,Z) = 0$? But if it is true that $\mathrm{cov}(XY,Z) = 0$, is there a simple proof that does not possibly involve measure theory? Thanks.","Let $X$, $Y,$ and $Z$ be random variables. (There are no restrictions on these variables, but you may assume that these are continuous random variables if you want.) Suppose that $X$ and $Z$ are independent, and also suppose that $Y$ and $Z$ are independent. Does it follow that $\mathrm{cov}(XY,Z)=0$? (I understand that $XY$ and $Z$ may not be independent, but this does not rule out the zero covariance.) Under the assumption that $X$ and $Z$ are independent and that $Y$ and $Z$ are independent, I am able to show that $$\mathrm{cov}(X,YZ) = \mathrm{cov}(Y,XZ) = \mathrm{cov}(XY,Z) + \mathrm{E}[Z]\cdot \mathrm{cov}(X,Y).$$ Showing this is fairly straightforward: $\mathrm{cov}(XY,Z)=\mathrm{E}[XYZ]-\mathrm{E}[XY]\cdot\mathrm{E}[Z]$; in addition, $\mathrm{cov}(X,YZ)=\mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[YZ]=\mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]$, implying that $\mathrm{cov}(X,YZ)=\mathrm{cov}(XY,Z)+\mathrm{E}[XY]\cdot\mathrm{E}[Z]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]$, which is obviously equal to $\mathrm{cov}(XY,Z) + \mathrm{E}[Z]\cdot\mathrm{cov}(X,Y).$ And, of course, $\mathrm{cov}(X,YZ) = \mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]= \mathrm{cov}(Y,XZ)$, proving the above result. However, to proceed further, my intuition tells me that $\mathrm{cov}(XY,Z) = 0$ and thus that $\mathrm{cov}(X,YZ) = \mathrm{cov}(Y,XZ) = \mathrm{E}[Z]\cdot\mathrm{cov}(X,Y)$. Am I wrong in thinking that $\mathrm{cov}(XY,Z) = 0$? But if it is true that $\mathrm{cov}(XY,Z) = 0$, is there a simple proof that does not possibly involve measure theory? Thanks.",,"['probability', 'statistics']"
29,Why was poisson distribution introduced?,Why was poisson distribution introduced?,,"I am studying probabilites and the notion of poisson random variable was introduced in the class. But it seems to me that the introduction of poisson random variable is to provide a easy approximation of the binomial random variable conditioned that n is large and p is small. Besides, the preconditions in the poisson distribution that events are independent of each other seem to come from the fact that binomial random variable is composed by many independent bernouli variables. So I wonder if originally, poisson distribution was invented to model binomial distribution or was it invented to solve a particular kind of problem","I am studying probabilites and the notion of poisson random variable was introduced in the class. But it seems to me that the introduction of poisson random variable is to provide a easy approximation of the binomial random variable conditioned that n is large and p is small. Besides, the preconditions in the poisson distribution that events are independent of each other seem to come from the fact that binomial random variable is composed by many independent bernouli variables. So I wonder if originally, poisson distribution was invented to model binomial distribution or was it invented to solve a particular kind of problem",,"['statistics', 'poisson-distribution', 'binomial-distribution']"
30,Maximum likelihood estimator of $\lambda$ and verifying if the estimator is unbiased,Maximum likelihood estimator of  and verifying if the estimator is unbiased,\lambda,"$(X_1,\ldots,X_n)$ is a random sample extracted from an exponential law of parameter $\lambda$ Calculate the  likelihood estimator $\nu$ of $\lambda$ . Then, if $n=2$ : establish if $\nu$ is a unbiased estimator $$L(\lambda: X_1,\ldots,X_n)=\prod_{i=1}^n \lambda \ e^{-\lambda \  x_i} \ \ 1_{(0,+\infty)} \  (x_i)=$$ $$=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ \lambda^n \ e^{-\lambda \sum_{i=1}^n  x_i} \ \ $$ $$\frac{\partial}{\partial \lambda} L(\lambda: X_1,...,X_n)=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ n \lambda^{n-1} \ e^{-\lambda \sum_{i=1}^n  x_i}-\sum_{1=1}^n x_i \ \lambda^n \ \ e^{-\lambda \sum_{i=1}^n  x_i}=  \ \ $$ $$=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ \lambda^{n-1} \ e^{-\lambda \sum_{i=1}^n x_i} \ \ (n- \lambda \sum_{i=1}^n x_i) $$ $$\frac{\partial}{\partial \lambda} L(\lambda: X_1,\ldots,X_n) \ge 0 \Longleftrightarrow \lambda \le \frac{1}{\overline{X}}$$ Maximum likelihood estimator of $\lambda$ is $\nu=\frac{1}{\overline{X}}$ If $n=2$ , I think that: $$\nu=\frac{2}{\sum_{i=1}^2 X_i}$$ and $$\sum_{i=1}^2 X_i \sim \Gamma(2, \lambda)$$ How can I establish if $\nu$ is a unbiased estimator? Thanks!","is a random sample extracted from an exponential law of parameter Calculate the  likelihood estimator of . Then, if : establish if is a unbiased estimator Maximum likelihood estimator of is If , I think that: and How can I establish if is a unbiased estimator? Thanks!","(X_1,\ldots,X_n) \lambda \nu \lambda n=2 \nu L(\lambda: X_1,\ldots,X_n)=\prod_{i=1}^n \lambda \ e^{-\lambda \  x_i} \ \ 1_{(0,+\infty)} \  (x_i)= =\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ \lambda^n \ e^{-\lambda \sum_{i=1}^n  x_i} \ \  \frac{\partial}{\partial \lambda} L(\lambda: X_1,...,X_n)=\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ n \lambda^{n-1} \ e^{-\lambda \sum_{i=1}^n  x_i}-\sum_{1=1}^n x_i \ \lambda^n \ \ e^{-\lambda \sum_{i=1}^n  x_i}=  \ \  =\prod_{i=1}^n (1_{(0,+\infty)} \  (x_i)) \ \ \lambda^{n-1} \ e^{-\lambda \sum_{i=1}^n x_i} \ \ (n- \lambda \sum_{i=1}^n x_i)  \frac{\partial}{\partial \lambda} L(\lambda: X_1,\ldots,X_n) \ge 0 \Longleftrightarrow \lambda \le \frac{1}{\overline{X}} \lambda \nu=\frac{1}{\overline{X}} n=2 \nu=\frac{2}{\sum_{i=1}^2 X_i} \sum_{i=1}^2 X_i \sim \Gamma(2, \lambda) \nu","['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'maximum-likelihood']"
31,cumulative distribution of intersection of events,cumulative distribution of intersection of events,,"Let $X_1,\dotsc,X_n$ be independent identically distributed random variables having common distribution function $F_X(\cdot)$. Express the event 'the smallest of the $X$s exceeds $k$' as an intersection of $n$ events, each involving one of the $X$s. Deduce that the distribution function of the smallest value is $G(k) = 1-(1-F_X(k))^n$. Source. I put $Y = \bigcap_i^n X_i$ and I think the question is asking me to show that $P(Y > k)$ However: $$P(Y > k ) = \prod_i^n P(X_i > k) = (1- F_X(k))^n$$ So I am unsure what the question is asking, or if I am going wrong.","Let $X_1,\dotsc,X_n$ be independent identically distributed random variables having common distribution function $F_X(\cdot)$. Express the event 'the smallest of the $X$s exceeds $k$' as an intersection of $n$ events, each involving one of the $X$s. Deduce that the distribution function of the smallest value is $G(k) = 1-(1-F_X(k))^n$. Source. I put $Y = \bigcap_i^n X_i$ and I think the question is asking me to show that $P(Y > k)$ However: $$P(Y > k ) = \prod_i^n P(X_i > k) = (1- F_X(k))^n$$ So I am unsure what the question is asking, or if I am going wrong.",,"['probability', 'statistics', 'probability-distributions']"
32,Huber loss vs l1 loss,Huber loss vs l1 loss,,"From a robust statistics perspective are there any advantages of the Huber loss vs. L1 loss (apart from differentiability at the origin) ? Specifically, if I don't care about gradients (for e.g. when using tree based methods), does Huber loss offer any other advantages vis-a-vis robustness ? Moreover, are there any guidelines for choosing the value of the change point between the linear and quadratic pieces of the Huber loss ? Thanks.","From a robust statistics perspective are there any advantages of the Huber loss vs. L1 loss (apart from differentiability at the origin) ? Specifically, if I don't care about gradients (for e.g. when using tree based methods), does Huber loss offer any other advantages vis-a-vis robustness ? Moreover, are there any guidelines for choosing the value of the change point between the linear and quadratic pieces of the Huber loss ? Thanks.",,"['statistics', 'robust-statistics']"
33,"Let $(X_1 ,Y_1),(X_2 ,Y_2),...,(X_n ,Y_n)$ be a sample from the uniform distribution on a disc $X^2 + Y^2 \leq \theta$, where $\theta$ is unknown.","Let  be a sample from the uniform distribution on a disc , where  is unknown.","(X_1 ,Y_1),(X_2 ,Y_2),...,(X_n ,Y_n) X^2 + Y^2 \leq \theta \theta","Let $(X_1 ,Y_1),(X_2 ,Y_2),\ldots,(X_n ,Y_n)$ be a sample from the uniform distribution on a disc $X^2 + Y^2 \leq \theta$, where $\theta$ is unknown. That is, the joint density function of $(X,Y)$ is $f_{(X,Y)}(x,y,\theta)=\frac{1}{\pi \theta^2} \mathbb 1_{ [0,\theta ]} (\sqrt{x^2+y^2})$. (1) Find a complete sufficient statistic of $\theta$ and its distribution (2) Find the UMVUE of $\theta$. What I did is to multiply the density function to find the likelihood function $L=\frac{1}{\pi^n \theta^{2n}} \mathbb 1_{ [0,\theta ]} (\prod_{i=1}^n \sqrt{x^2_i +y^2_i})$, then we know $L=e^{\ln(L)}$, then I find a complete sufficient statistic $T=\sum_{i=1}^n \ln(x^2_i +y^2_i)$; however, I don't know how to find its distribution. For part (2), I think it's related to the first part. My idea is to see the expectation of $T$ and its relation with $\theta$.","Let $(X_1 ,Y_1),(X_2 ,Y_2),\ldots,(X_n ,Y_n)$ be a sample from the uniform distribution on a disc $X^2 + Y^2 \leq \theta$, where $\theta$ is unknown. That is, the joint density function of $(X,Y)$ is $f_{(X,Y)}(x,y,\theta)=\frac{1}{\pi \theta^2} \mathbb 1_{ [0,\theta ]} (\sqrt{x^2+y^2})$. (1) Find a complete sufficient statistic of $\theta$ and its distribution (2) Find the UMVUE of $\theta$. What I did is to multiply the density function to find the likelihood function $L=\frac{1}{\pi^n \theta^{2n}} \mathbb 1_{ [0,\theta ]} (\prod_{i=1}^n \sqrt{x^2_i +y^2_i})$, then we know $L=e^{\ln(L)}$, then I find a complete sufficient statistic $T=\sum_{i=1}^n \ln(x^2_i +y^2_i)$; however, I don't know how to find its distribution. For part (2), I think it's related to the first part. My idea is to see the expectation of $T$ and its relation with $\theta$.",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'parameter-estimation']"
34,"Show that $(\frac{S_1}{S_n+1},\frac{S_2}{S_n+1},...\frac{S_n}{S_n+1})=_d (U_{(1)},U_{(2)},...,U_{(n)})$.",Show that .,"(\frac{S_1}{S_n+1},\frac{S_2}{S_n+1},...\frac{S_n}{S_n+1})=_d (U_{(1)},U_{(2)},...,U_{(n)})","Let $(X_1, X_2,...,X_n) \in \mathbb R^n$ have density function $p(x)$. (1) Find the density of $(U_{(1)},U_{(2)},...,U_{(n)})$, the order statistics from a sample of iid $\mathbb U[0,1]$ (uniform distributions) variables. (2) Let $E_1, E_2, ..., E_n$ be i.i.d. exponential with density $p(x)=e^{-x}$, where $x>0$ and for $k=1,2,3,...,n+1$, set $S_k=\Sigma_{i=1}^{k} E_i$. Show that $\left(\frac{S_1}{S_{n+1}},\frac{S_2}{S_{n+1}},...\frac{S_n}{S_{n+1}}\right)=_d (U_{(1)},U_{(2)},...,U_{(n)})$. (3) For $m$ an integer such that $\frac{m}{n} \rightarrow \alpha$ as $n \rightarrow \infty$, take $U_{(m)}$ as the estimate of the $\alpha$ quantile $x_{\alpha}$; for $\mathbb U[0,1]$, we have $x_{\alpha}=\alpha$. The error made in estimating $x_{\alpha}$ is $U_{(m)}-x_{\alpha}$. Use part (2) to find the asymptotic distribution of $\sqrt n (U_{(m)}-x_{\alpha})$. Progress: Part (1) is easy and I just plugged in the formula of density function of order statistics. However, I have no idea about part (2) and part (3).","Let $(X_1, X_2,...,X_n) \in \mathbb R^n$ have density function $p(x)$. (1) Find the density of $(U_{(1)},U_{(2)},...,U_{(n)})$, the order statistics from a sample of iid $\mathbb U[0,1]$ (uniform distributions) variables. (2) Let $E_1, E_2, ..., E_n$ be i.i.d. exponential with density $p(x)=e^{-x}$, where $x>0$ and for $k=1,2,3,...,n+1$, set $S_k=\Sigma_{i=1}^{k} E_i$. Show that $\left(\frac{S_1}{S_{n+1}},\frac{S_2}{S_{n+1}},...\frac{S_n}{S_{n+1}}\right)=_d (U_{(1)},U_{(2)},...,U_{(n)})$. (3) For $m$ an integer such that $\frac{m}{n} \rightarrow \alpha$ as $n \rightarrow \infty$, take $U_{(m)}$ as the estimate of the $\alpha$ quantile $x_{\alpha}$; for $\mathbb U[0,1]$, we have $x_{\alpha}=\alpha$. The error made in estimating $x_{\alpha}$ is $U_{(m)}-x_{\alpha}$. Use part (2) to find the asymptotic distribution of $\sqrt n (U_{(m)}-x_{\alpha})$. Progress: Part (1) is easy and I just plugged in the formula of density function of order statistics. However, I have no idea about part (2) and part (3).",,"['probability', 'probability-theory', 'statistics', 'central-limit-theorem', 'order-statistics']"
35,"X~B(5,$\frac{1}{2}$) and Y~U(0,1) Then $\frac{P(X+Y\leq2)}{P(X+Y\geq5)}$=?","X~B(5,) and Y~U(0,1) Then =?",\frac{1}{2} \frac{P(X+Y\leq2)}{P(X+Y\geq5)},"$X\sim\text{Bin}\left(5,\frac{1}{2}\right)$ and $Y\sim \text{Unif}(0,1).$  Then $\frac{P(X+Y\leq2)}{P(X+Y\geq5)}$=? Intuitively $X+Y$ takes values from $[0,6)$ since $X$ takes $\{0,1,2,3,4,5\}$ and $Y$ takes $(0,1)$. Let $Z = X+Y$, $P(Z\leq z)=0$ if $z<0$. How to define $P(Z\leq z)$ if $0\leq z\leq 6$?","$X\sim\text{Bin}\left(5,\frac{1}{2}\right)$ and $Y\sim \text{Unif}(0,1).$  Then $\frac{P(X+Y\leq2)}{P(X+Y\geq5)}$=? Intuitively $X+Y$ takes values from $[0,6)$ since $X$ takes $\{0,1,2,3,4,5\}$ and $Y$ takes $(0,1)$. Let $Z = X+Y$, $P(Z\leq z)=0$ if $z<0$. How to define $P(Z\leq z)$ if $0\leq z\leq 6$?",,"['probability', 'statistics']"
36,How to test a collection of samples are sampled with replacement or not?,How to test a collection of samples are sampled with replacement or not?,,"A box is full of balls with $m$ different colors, and for each color, there are $n$ balls. So the total number of balls is $m*n$. Note that $m$ is unknown, $n$ is already known, and balls can only be distinguished by the color. Now, if one draws $N$ samples from the box with replacement or without replacement, how can I know whether the sample is drawn with replacement or not? any hypothesis test to tackle it? Thanks! To make the question clearer: What I am concerned is that how to measure the degree that the current samples are drawn with replacement. It might be a function $f$, which takes the samples $\{x\}$, $m$, and $n$ as inputs. If $f(\{x\}, n, m) = 1$, then the sampling process is sampling with replacement; If $f(\{x\}, n, m) = 0$ , then the sampling process is sampling without replacement. Other values between 0 and 1 describe the likelihood that the sampling process is with replacement.","A box is full of balls with $m$ different colors, and for each color, there are $n$ balls. So the total number of balls is $m*n$. Note that $m$ is unknown, $n$ is already known, and balls can only be distinguished by the color. Now, if one draws $N$ samples from the box with replacement or without replacement, how can I know whether the sample is drawn with replacement or not? any hypothesis test to tackle it? Thanks! To make the question clearer: What I am concerned is that how to measure the degree that the current samples are drawn with replacement. It might be a function $f$, which takes the samples $\{x\}$, $m$, and $n$ as inputs. If $f(\{x\}, n, m) = 1$, then the sampling process is sampling with replacement; If $f(\{x\}, n, m) = 0$ , then the sampling process is sampling without replacement. Other values between 0 and 1 describe the likelihood that the sampling process is with replacement.",,"['statistics', 'statistical-inference', 'sampling', 'hypothesis-testing']"
37,Ways of characterizing sufficient statistics,Ways of characterizing sufficient statistics,,"I've been reading a little about the Fisher-Neyman factorization theorem on my own, and I think I understand intuitively what a sufficient statistic is, but I am wondering what the convention is for formally defining it. Is it true that, for a given collection of iid samples ($\vec{x}$) from a distribution $X$, the maximum likelihood estimate of the parameter $\theta \in \mathbb{R}^t$ depends exclusively on $T(\vec{x})$ and not on other factors such as the number of elements in the sample? If that is the case, does it make sense to take the following as a definition of sufficiency. (Maximum likelihood estimates given $\vec{x}$ and $\vec{y}$ are equal iff $T(\vec{x}) = T(\vec{y})$). $$ \forall {      \vec{x}, \vec{y} \in \text{Seq}[\mathbb{R}]: }       \left(            \max_{\theta_x \in \mathbb{R}^t}                 \prod_{x \in \vec{x}} p(x;\theta_x)       \right) =      \left(           \max_{\theta_y \in \mathbb{R}^t}                 \prod_{y \in \vec{y}} p(y;\theta_y)      \right)  \Longleftrightarrow       T(\vec{x}) = T(\vec{y}) $$ So this definition, while closer to right, is also wrong. Based on the comments below, the above definition inappropriately rejects all sufficient statistics except for the maximum likelihood estimate itself. For instance, the sum is a sufficient statistic for the mean in a normal distribution with standard deviation 1: $\mathcal{N}(\mu, 1)$. Amending the formula above gives us: $$ \forall {      \vec{x}, \vec{y} \in \mathbb{R}^n: }       \left(            \max_{\theta_x \in \mathbb{R}^t}                 \prod_{x \in \vec{x}} p(x;\theta_x)       \right) =      \left(           \max_{\theta_y \in \mathbb{R}^t}                 \prod_{y \in \vec{y}} p(y;\theta_y)      \right)  \Longleftrightarrow       T(\vec{x}) = T(\vec{y}) $$ with $n$ being the length of the sample.","I've been reading a little about the Fisher-Neyman factorization theorem on my own, and I think I understand intuitively what a sufficient statistic is, but I am wondering what the convention is for formally defining it. Is it true that, for a given collection of iid samples ($\vec{x}$) from a distribution $X$, the maximum likelihood estimate of the parameter $\theta \in \mathbb{R}^t$ depends exclusively on $T(\vec{x})$ and not on other factors such as the number of elements in the sample? If that is the case, does it make sense to take the following as a definition of sufficiency. (Maximum likelihood estimates given $\vec{x}$ and $\vec{y}$ are equal iff $T(\vec{x}) = T(\vec{y})$). $$ \forall {      \vec{x}, \vec{y} \in \text{Seq}[\mathbb{R}]: }       \left(            \max_{\theta_x \in \mathbb{R}^t}                 \prod_{x \in \vec{x}} p(x;\theta_x)       \right) =      \left(           \max_{\theta_y \in \mathbb{R}^t}                 \prod_{y \in \vec{y}} p(y;\theta_y)      \right)  \Longleftrightarrow       T(\vec{x}) = T(\vec{y}) $$ So this definition, while closer to right, is also wrong. Based on the comments below, the above definition inappropriately rejects all sufficient statistics except for the maximum likelihood estimate itself. For instance, the sum is a sufficient statistic for the mean in a normal distribution with standard deviation 1: $\mathcal{N}(\mu, 1)$. Amending the formula above gives us: $$ \forall {      \vec{x}, \vec{y} \in \mathbb{R}^n: }       \left(            \max_{\theta_x \in \mathbb{R}^t}                 \prod_{x \in \vec{x}} p(x;\theta_x)       \right) =      \left(           \max_{\theta_y \in \mathbb{R}^t}                 \prod_{y \in \vec{y}} p(y;\theta_y)      \right)  \Longleftrightarrow       T(\vec{x}) = T(\vec{y}) $$ with $n$ being the length of the sample.",,['statistics']
38,Markov Process: predict the weather using a stochastic matrix,Markov Process: predict the weather using a stochastic matrix,,"I have the following stochastic matrix $$ P = \begin{pmatrix} P(S \mid S) = 0.5 & P(F \mid S) = 0.2 & P(R \mid S) = 0.3 \\ P(S \mid F) = 0.2 & P(F \mid F) = 0.7 & P(R \mid F) = 0.1 \\ P(S \mid R) = 0.75 & P(F \mid R) = 0.15 & P(R \mid R) = 0.1  \end{pmatrix} $$ Where $S$ means sunny, $R$ means rainy and $F$ means foggy. For example $P(S \mid F)$ is the probability of being sunny tomorrow given that today is foggy. Similarly,  $P(R \mid R)$ is the probability of tomorrow being rainy if today is also rainy. The graph representing the situation would look like this Now suppose today is Monday and it's sunny. I would like to estimate the probabilities of a sunny Tuesday a sunny Tuesday and Wednesday a sunny Wednesday On Wikipedia there's a similar problem , but since I am new to these things, I decided to ask you help you anyway at least for some clarifications. As far as I have understood a Markov process is a memoryless stochastic process, that only remembers the current state in order to predict future states. For point $1$, we can simply use the fact that $P(S \mid S) = 0.5$, i.e. the probability of tomorrow being sunny if today is sunny is $0.5$, which is therefore the answer. For the other cases it's a little bit more complicated: we cannot use directly any value from the stochastic matrix $P$. We need of course to do intermediary calculations. The example I am linking to, to predict the weather on day $1$, it creates a vector representing the weather on day $0$. Similarly, in my case, I think that vector would be $$v_1 = \begin{pmatrix} 1 & 0 & 0 \end{pmatrix}$$ that is the $1$ represents that it's $100 \%$ sunny. If I multiply $v_1$ by $P$ I obtain (similarly to the Wikipedia's problem above) the following vector $$v_1 =\begin{pmatrix} 0.5 & 0.2 & 0.3 \end{pmatrix}$$ From this to obtain my guess of $0.5$ there's some distance of reasoning. Could you please explain it to me How do you explain the $0.5$ I am guessing (if it's correct) from the calculations that I just did and done in the solutions to the Wikipedia's problem. If it's not correct, why and how can I solve it? In the third question, do I need to do the same thing as I did for the first point, but using $P^2$, like in the Wikipedia's article? The second question involves a compound probability (I think). Am I right? How exactly would I proceed to solve this case, and why? Thanks for any help (and sorry for the long post)!","I have the following stochastic matrix $$ P = \begin{pmatrix} P(S \mid S) = 0.5 & P(F \mid S) = 0.2 & P(R \mid S) = 0.3 \\ P(S \mid F) = 0.2 & P(F \mid F) = 0.7 & P(R \mid F) = 0.1 \\ P(S \mid R) = 0.75 & P(F \mid R) = 0.15 & P(R \mid R) = 0.1  \end{pmatrix} $$ Where $S$ means sunny, $R$ means rainy and $F$ means foggy. For example $P(S \mid F)$ is the probability of being sunny tomorrow given that today is foggy. Similarly,  $P(R \mid R)$ is the probability of tomorrow being rainy if today is also rainy. The graph representing the situation would look like this Now suppose today is Monday and it's sunny. I would like to estimate the probabilities of a sunny Tuesday a sunny Tuesday and Wednesday a sunny Wednesday On Wikipedia there's a similar problem , but since I am new to these things, I decided to ask you help you anyway at least for some clarifications. As far as I have understood a Markov process is a memoryless stochastic process, that only remembers the current state in order to predict future states. For point $1$, we can simply use the fact that $P(S \mid S) = 0.5$, i.e. the probability of tomorrow being sunny if today is sunny is $0.5$, which is therefore the answer. For the other cases it's a little bit more complicated: we cannot use directly any value from the stochastic matrix $P$. We need of course to do intermediary calculations. The example I am linking to, to predict the weather on day $1$, it creates a vector representing the weather on day $0$. Similarly, in my case, I think that vector would be $$v_1 = \begin{pmatrix} 1 & 0 & 0 \end{pmatrix}$$ that is the $1$ represents that it's $100 \%$ sunny. If I multiply $v_1$ by $P$ I obtain (similarly to the Wikipedia's problem above) the following vector $$v_1 =\begin{pmatrix} 0.5 & 0.2 & 0.3 \end{pmatrix}$$ From this to obtain my guess of $0.5$ there's some distance of reasoning. Could you please explain it to me How do you explain the $0.5$ I am guessing (if it's correct) from the calculations that I just did and done in the solutions to the Wikipedia's problem. If it's not correct, why and how can I solve it? In the third question, do I need to do the same thing as I did for the first point, but using $P^2$, like in the Wikipedia's article? The second question involves a compound probability (I think). Am I right? How exactly would I proceed to solve this case, and why? Thanks for any help (and sorry for the long post)!",,['statistics']
39,Probability: Deriving The Moment Generating Function Given the Definition of a Continuous Random Variable,Probability: Deriving The Moment Generating Function Given the Definition of a Continuous Random Variable,,"Here's a question I'm working on in my textbook. It says: Let $Y$ be a Normally distributed random variable with mean $μ$ and   variance $σ^2$ . Derive the moment-generating function of   $X = −3Y+4$ Here's what I have: $M_x(t) = E[e^{tx}]$ $M_x(t) = E[e^{-3ty + 4t}]$ $M_x(t) = e^{4t}E[e^{-3ty}]$ I'm not sure how to use what I know about the moment generating function when it comes to the $E[e^{-3ty}]$ part. There's a -3 in the way, and I'm still new with all of this stuff, so any help would greatly be appreciated. Thanks.","Here's a question I'm working on in my textbook. It says: Let $Y$ be a Normally distributed random variable with mean $μ$ and   variance $σ^2$ . Derive the moment-generating function of   $X = −3Y+4$ Here's what I have: $M_x(t) = E[e^{tx}]$ $M_x(t) = E[e^{-3ty + 4t}]$ $M_x(t) = e^{4t}E[e^{-3ty}]$ I'm not sure how to use what I know about the moment generating function when it comes to the $E[e^{-3ty}]$ part. There's a -3 in the way, and I'm still new with all of this stuff, so any help would greatly be appreciated. Thanks.",,"['probability', 'statistics', 'random-variables', 'moment-generating-functions']"
40,Predicting trends of timeseries data with ARIMA,Predicting trends of timeseries data with ARIMA,,"I'm looking for an algorithm that can help identify abnormal trends in time-series metrics. The best I've been able to find so far is ARIMA (a completely new concept for me). We offer several services which we monitor active usage against, for any given time of day (typically updated once a minute). Here's a example of one of our services: What I'm interested in is gradual degradation that doesn't get noticed until ... well, it gets noticed. For example, if we zoom in to 12/6 we see: So what I'm specifically interested in here is the trend downward @17:30~ . Likewise another good example would be: Here we can see the abnormalities on 12/1 and 12/4 during peak. Are there any formal methods of detecting this sort of trend (or rather, abnormality to a historical trend)? Specifically interested in the trend here, not the raw numbers. These services have a historical decreasing usage as time goes on - eventually, these numbers will drop from say 60k peaks to 15k peaks. Is ARIMA the best bet here? e.g., predicting the future curve based on yesterday's curve, setting some threshold, and alerting if that threshold is breached? Or is there a more straightforward way to go about this? Am I fundamentally misunderstanding ARIMA? note : wasn't sure which tags to even put - feel free to edit","I'm looking for an algorithm that can help identify abnormal trends in time-series metrics. The best I've been able to find so far is ARIMA (a completely new concept for me). We offer several services which we monitor active usage against, for any given time of day (typically updated once a minute). Here's a example of one of our services: What I'm interested in is gradual degradation that doesn't get noticed until ... well, it gets noticed. For example, if we zoom in to 12/6 we see: So what I'm specifically interested in here is the trend downward @17:30~ . Likewise another good example would be: Here we can see the abnormalities on 12/1 and 12/4 during peak. Are there any formal methods of detecting this sort of trend (or rather, abnormality to a historical trend)? Specifically interested in the trend here, not the raw numbers. These services have a historical decreasing usage as time goes on - eventually, these numbers will drop from say 60k peaks to 15k peaks. Is ARIMA the best bet here? e.g., predicting the future curve based on yesterday's curve, setting some threshold, and alerting if that threshold is breached? Or is there a more straightforward way to go about this? Am I fundamentally misunderstanding ARIMA? note : wasn't sure which tags to even put - feel free to edit",,"['statistics', 'regression', 'time-series']"
41,"Let $X,Y \sim N(0, 1)$. Find $E[X\mid X+Y=1]$.",Let . Find .,"X,Y \sim N(0, 1) E[X\mid X+Y=1]","Assume the joint distribution for X, Y is also normal. I have no clue how to approach this problem. Follow up question: Without knowing the joint distribution of X, Y can you still calculate it?","Assume the joint distribution for X, Y is also normal. I have no clue how to approach this problem. Follow up question: Without knowing the joint distribution of X, Y can you still calculate it?",,['statistics']
42,Understanding better linear regression,Understanding better linear regression,,"I have been trying to understand linear regression as much as possible, so I am asking you this question in order to keep doing it. There's simple linear regression, where we have just one independent variable $x$, and we have multiple linear regression, where there are multiple predictors. In a linear regression related problem we usually have data for the predictors and the response variable, whereas what we need to find is a line that better estimates the linearly relationship between $y$ and the predictors. Of course a line will not describe perfectly the relationship between many observations $x_i$ and $y_i$ unless the relationship is really mathematically linear, which is usually not the case in the real world (I guess). That's why we want to estimate the ""best line"", there might be errors, which usually we denote with $\epsilon$ in the following perfect model $$y = \beta_0 + \beta_1 x_1 + \cdots+ \beta_n x_n + \epsilon$$ The line that we want to estimate has therefore no ""error terms"" $\epsilon$, because they cannot estimated, thus it should look like $$\hat{y} = \beta_0 + \beta_1 \hat{x}_1 + \cdots+ \beta_n \hat{x}_n$$ What we need to find from this line are the coefficients, and we can find them using least squares for example. This is what I have understood so far. I am not even sure if this is all correct. My question is then: could you please help me confirming, correcting and adding additional information that you think it might be useful to further understand this statistical model? I know this might be a broad question, but if you feel like I need to understand more details, please help me!","I have been trying to understand linear regression as much as possible, so I am asking you this question in order to keep doing it. There's simple linear regression, where we have just one independent variable $x$, and we have multiple linear regression, where there are multiple predictors. In a linear regression related problem we usually have data for the predictors and the response variable, whereas what we need to find is a line that better estimates the linearly relationship between $y$ and the predictors. Of course a line will not describe perfectly the relationship between many observations $x_i$ and $y_i$ unless the relationship is really mathematically linear, which is usually not the case in the real world (I guess). That's why we want to estimate the ""best line"", there might be errors, which usually we denote with $\epsilon$ in the following perfect model $$y = \beta_0 + \beta_1 x_1 + \cdots+ \beta_n x_n + \epsilon$$ The line that we want to estimate has therefore no ""error terms"" $\epsilon$, because they cannot estimated, thus it should look like $$\hat{y} = \beta_0 + \beta_1 \hat{x}_1 + \cdots+ \beta_n \hat{x}_n$$ What we need to find from this line are the coefficients, and we can find them using least squares for example. This is what I have understood so far. I am not even sure if this is all correct. My question is then: could you please help me confirming, correcting and adding additional information that you think it might be useful to further understand this statistical model? I know this might be a broad question, but if you feel like I need to understand more details, please help me!",,['statistics']
43,"If X is normally distributed, and c is a constant, is cX also normally distributed?","If X is normally distributed, and c is a constant, is cX also normally distributed?",,"If $X\sim N(\mu, \sigma^2)$, and if $c$ is a constant, is $cX$ also normally distributed? How do you show it? If yes, does this apply to other distributions? So if $Y$ follows some type of distribution, will $cY$ also follow that distribution? Thank you very much!","If $X\sim N(\mu, \sigma^2)$, and if $c$ is a constant, is $cX$ also normally distributed? How do you show it? If yes, does this apply to other distributions? So if $Y$ follows some type of distribution, will $cY$ also follow that distribution? Thank you very much!",,"['statistics', 'probability-distributions', 'random-variables']"
44,Landon derivation of the Gaussian distribution,Landon derivation of the Gaussian distribution,,"This is the follow up to the question Taylor series of a convolution . Continuing the derivation given at Probability Theory: The Logic Of Science By E. T. Jaynes , chapter 7 ""The Central Gaussian, Or Normal, Distribution"", p.706 The Landon derivation. The author continues: At the same time, the expectation of $v^2$ is increased to $\sigma^2 + \langle \epsilon^2 \rangle$, so Landon's invariance property requires that $f(v)$ should be equal also to $(1)\quad f(v) = p(v|\sigma) + \langle\epsilon^2 \rangle {\partial p(v|\sigma)\over \partial \sigma^2}$ Where: $\langle\epsilon^2 \rangle = \int \epsilon^2 q(\epsilon)d\epsilon $ $\langle\epsilon^2 \rangle$ is the expected value of $\epsilon^2$ From the text before these equations: Landon reasoned that if this frequency distribution of noise voltage is so universal, then it must be better determined theoretically than empirically. To account for this universality but for magnitude, he visualized not a single distribution for the voltage at any given time, but a hierarchy of distributions $p(v|\sigma)$ characterized by a single scale parameter $\sigma^2$, which we shall take to be the expected square of the noise voltage. The stability seems to imply that if the noise level $\sigma^2$ is increased by adding a small increment of voltage, the probability distribution still has the same functional form, but only moved up the hierarchy to the new value of $\sigma$. He discovered that for only one functional form of $p(v|\sigma)$ will this be true. Suppose the noise voltage $v$ is assigned the probability distribution $p(v|\sigma)$. Then it is incremented by a small extra contribution $\epsilon$, becoming $v' = v + \epsilon$ where $\epsilon$ is small compared to $\sigma$, and has a probability distribution $q(\epsilon)d\epsilon$, independent of $p(v|\sigma)$. Given a specic $\epsilon$, the probability for the new noise voltage to have the value $v'$ would be just the previous probability that $v$ should have the value $(v' - \epsilon)$. Questions: What exactly is the Landon invariance property? How is that used to derive equation (1) above?","This is the follow up to the question Taylor series of a convolution . Continuing the derivation given at Probability Theory: The Logic Of Science By E. T. Jaynes , chapter 7 ""The Central Gaussian, Or Normal, Distribution"", p.706 The Landon derivation. The author continues: At the same time, the expectation of $v^2$ is increased to $\sigma^2 + \langle \epsilon^2 \rangle$, so Landon's invariance property requires that $f(v)$ should be equal also to $(1)\quad f(v) = p(v|\sigma) + \langle\epsilon^2 \rangle {\partial p(v|\sigma)\over \partial \sigma^2}$ Where: $\langle\epsilon^2 \rangle = \int \epsilon^2 q(\epsilon)d\epsilon $ $\langle\epsilon^2 \rangle$ is the expected value of $\epsilon^2$ From the text before these equations: Landon reasoned that if this frequency distribution of noise voltage is so universal, then it must be better determined theoretically than empirically. To account for this universality but for magnitude, he visualized not a single distribution for the voltage at any given time, but a hierarchy of distributions $p(v|\sigma)$ characterized by a single scale parameter $\sigma^2$, which we shall take to be the expected square of the noise voltage. The stability seems to imply that if the noise level $\sigma^2$ is increased by adding a small increment of voltage, the probability distribution still has the same functional form, but only moved up the hierarchy to the new value of $\sigma$. He discovered that for only one functional form of $p(v|\sigma)$ will this be true. Suppose the noise voltage $v$ is assigned the probability distribution $p(v|\sigma)$. Then it is incremented by a small extra contribution $\epsilon$, becoming $v' = v + \epsilon$ where $\epsilon$ is small compared to $\sigma$, and has a probability distribution $q(\epsilon)d\epsilon$, independent of $p(v|\sigma)$. Given a specic $\epsilon$, the probability for the new noise voltage to have the value $v'$ would be just the previous probability that $v$ should have the value $(v' - \epsilon)$. Questions: What exactly is the Landon invariance property? How is that used to derive equation (1) above?",,"['calculus', 'probability-theory', 'statistics', 'derivatives', 'probability-distributions']"
45,Binomial experiment vs Hypergeometric experiment,Binomial experiment vs Hypergeometric experiment,,"I was doing a problem from a textbook. It said that there were 209 waste treatment facilities in the US and 8 of them treat hazardous waste on site it then said that if 10 were randomly sampled then: What is The expected value of the number of facilities with on-site hazardous waste treatment and what is the probability of having exactly 4 of these facilities in the sample. I looked at this and thought it was a binomial experiment but according to the solution manual, it ended up being hypergeometric. I am unclear as to why this is the case. Is it because they arent truly independent because we are picking from a maximum number of facilities? To be clear, i mean to ask if it would indeed be binomial if instead the question asked that P(on site)=8/209 and there are an infinite number of facilities. That is the only solution I can think of Thanks","I was doing a problem from a textbook. It said that there were 209 waste treatment facilities in the US and 8 of them treat hazardous waste on site it then said that if 10 were randomly sampled then: What is The expected value of the number of facilities with on-site hazardous waste treatment and what is the probability of having exactly 4 of these facilities in the sample. I looked at this and thought it was a binomial experiment but according to the solution manual, it ended up being hypergeometric. I am unclear as to why this is the case. Is it because they arent truly independent because we are picking from a maximum number of facilities? To be clear, i mean to ask if it would indeed be binomial if instead the question asked that P(on site)=8/209 and there are an infinite number of facilities. That is the only solution I can think of Thanks",,"['probability', 'statistics', 'probability-distributions', 'hypergeometric-function', 'binomial-distribution']"
46,Sample Geometric Mean as an estimator for a Gamma Distribution,Sample Geometric Mean as an estimator for a Gamma Distribution,,"How does one compute the bias for the estimator given by the sample geometric mean for a gamma distribution with parameters ($\theta$,1)? i.e: Given $X_1,...,X_n$ are iid with distribution Gamma($\theta$,1), what is the bias of the estimator given by: $$\hat\theta = \left(\prod_{i=1}^n X_i\right)^{1/n}$$ I have that $\mathbb{B}(\hat\theta) = \prod_{i=1}^n \mathbb{E}\left[X_i^{1/n}\right] - \theta$. Evaluating this gives an awful expression littered with gamma functions. Have I made any mistakes here?","How does one compute the bias for the estimator given by the sample geometric mean for a gamma distribution with parameters ($\theta$,1)? i.e: Given $X_1,...,X_n$ are iid with distribution Gamma($\theta$,1), what is the bias of the estimator given by: $$\hat\theta = \left(\prod_{i=1}^n X_i\right)^{1/n}$$ I have that $\mathbb{B}(\hat\theta) = \prod_{i=1}^n \mathbb{E}\left[X_i^{1/n}\right] - \theta$. Evaluating this gives an awful expression littered with gamma functions. Have I made any mistakes here?",,"['statistics', 'parameter-estimation']"
47,Probability of a Specific Result,Probability of a Specific Result,,"I am trying to validate a small Monte Carlo that I am running: A small bucket contains seven ping pong balls numbered $1$ through $7$.  A ball is drawn at random from the bucket, the number recorded, and the ball returned to the bucket. The experiment is to repeat this sampling process $420$ times. At the end of a single experiment, I would expect that each of the seven counters would be about $60$.  This is because for any draw you are equally likely to pick any number.   I repeated the experiment on the computer $5000$ times.  I did not get a single result of $(60,60,60,60,60,60,60)$ I was a little surprised.  What is the probability of getting exactly $60$ for all seven counters??","I am trying to validate a small Monte Carlo that I am running: A small bucket contains seven ping pong balls numbered $1$ through $7$.  A ball is drawn at random from the bucket, the number recorded, and the ball returned to the bucket. The experiment is to repeat this sampling process $420$ times. At the end of a single experiment, I would expect that each of the seven counters would be about $60$.  This is because for any draw you are equally likely to pick any number.   I repeated the experiment on the computer $5000$ times.  I did not get a single result of $(60,60,60,60,60,60,60)$ I was a little surprised.  What is the probability of getting exactly $60$ for all seven counters??",,"['probability', 'statistics']"
48,Rate of growth of expected value of maxima of iid random variables under minimal assumptions,Rate of growth of expected value of maxima of iid random variables under minimal assumptions,,"Suppose, $\{X_i:i\geq 1\}$ is a sequence of iid random variables with $E(X_i) = 0$ and $E X^2_i<\infty$. Let $T_n = \max_{1\leq i\leq n} |X_i|$. I have two questions. (i) Define,  $$ a_n\equiv E\big(n^{-1/2}T_n\big),\quad n\geq 1. $$ Is it possible that $a_n\rightarrow 0$, without any additional moment assumptions? (ii) Is it possible to get an upper bound on the growth rate of $a_n$ of the sort of, $a_n\ll n^{-1/2}$ or some other rate. If I assume higher moments, then I can do the proof. But, I need some help regarding how to handle this in case only second moments exist. Thanks.","Suppose, $\{X_i:i\geq 1\}$ is a sequence of iid random variables with $E(X_i) = 0$ and $E X^2_i<\infty$. Let $T_n = \max_{1\leq i\leq n} |X_i|$. I have two questions. (i) Define,  $$ a_n\equiv E\big(n^{-1/2}T_n\big),\quad n\geq 1. $$ Is it possible that $a_n\rightarrow 0$, without any additional moment assumptions? (ii) Is it possible to get an upper bound on the growth rate of $a_n$ of the sort of, $a_n\ll n^{-1/2}$ or some other rate. If I assume higher moments, then I can do the proof. But, I need some help regarding how to handle this in case only second moments exist. Thanks.",,"['probability', 'statistics']"
49,Sum of exponential random variables?,Sum of exponential random variables?,,"I am trying to find the PDF of $Y$, the sum of I.I.D. exponential random variables $X_1, ... X_n$ with $\lambda = 1$ and $n$ some known constant. So far, I have determined that moment-generating function/characteristic function $M_s(Y) = M_s(X_1)^n = \left( \dfrac{1}{1-s} \right)^n.$ But this does not match the characteristic function of any other simple RV so i am at a loss as to how to proceed.","I am trying to find the PDF of $Y$, the sum of I.I.D. exponential random variables $X_1, ... X_n$ with $\lambda = 1$ and $n$ some known constant. So far, I have determined that moment-generating function/characteristic function $M_s(Y) = M_s(X_1)^n = \left( \dfrac{1}{1-s} \right)^n.$ But this does not match the characteristic function of any other simple RV so i am at a loss as to how to proceed.",,"['probability', 'statistics', 'random-variables']"
50,Why rank of an $n \times n$ matrix after subtracting its column mean is $n-1$?,Why rank of an  matrix after subtracting its column mean is ?,n \times n n-1,"Given a $n \times n$ matrix $A = \begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix}$, where each $a_i$ are columns of $A$ for $i = 1 \dots n$. Column mean is calculated by $\bar{a} =(a_1 + a_2 + \dots + a_n)/n$. Then, define: $B = \begin{bmatrix} a_1 - \bar{a} & a_2 - \bar{a}& \dots & a_n - \bar{a} \end{bmatrix}$. May I know why the rank$(B)$ is $n-1$? I did try many times with computer program to prove but I want a proof of this. Thanks in advance.","Given a $n \times n$ matrix $A = \begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix}$, where each $a_i$ are columns of $A$ for $i = 1 \dots n$. Column mean is calculated by $\bar{a} =(a_1 + a_2 + \dots + a_n)/n$. Then, define: $B = \begin{bmatrix} a_1 - \bar{a} & a_2 - \bar{a}& \dots & a_n - \bar{a} \end{bmatrix}$. May I know why the rank$(B)$ is $n-1$? I did try many times with computer program to prove but I want a proof of this. Thanks in advance.",,"['linear-algebra', 'statistics', 'matrix-rank']"
51,Determining the MVUE of $\theta$ when $f(x;\theta) = \theta^x (1-\theta)$,Determining the MVUE of  when,\theta f(x;\theta) = \theta^x (1-\theta),"The Statement of the Problem: Let $X_1, X_2, ... , X_n$ be a random sample from $$ f(x;\theta) = \theta^x (1-\theta) \quad x = 0,1,2,... $$ (a) Find the ML estimator of $\theta$. (b) Show that $T = \sum_{i=1}^n X_i$ is a sufficient statistic for $\theta$. (c) Determine the MVUE of $\theta$. Where I Am: I've taken care of parts (a) and (b) but am a bit stuck on part (c). In order to find a MVUE, I have to find an unbiased estimator, which I can't seem to figure out. My MLE, which is the following: $$ \hat\theta_{\text{mle}} = \frac{\sum_{i=1}^n X_i}{n + \sum_{i=1}^n X_i} $$ seems to be biased (although, honestly, I got stuck trying to find its expectation). So, I've tried making an unbiased estimator from scratch... with no luck. Any tips here would be helpful. Thanks.","The Statement of the Problem: Let $X_1, X_2, ... , X_n$ be a random sample from $$ f(x;\theta) = \theta^x (1-\theta) \quad x = 0,1,2,... $$ (a) Find the ML estimator of $\theta$. (b) Show that $T = \sum_{i=1}^n X_i$ is a sufficient statistic for $\theta$. (c) Determine the MVUE of $\theta$. Where I Am: I've taken care of parts (a) and (b) but am a bit stuck on part (c). In order to find a MVUE, I have to find an unbiased estimator, which I can't seem to figure out. My MLE, which is the following: $$ \hat\theta_{\text{mle}} = \frac{\sum_{i=1}^n X_i}{n + \sum_{i=1}^n X_i} $$ seems to be biased (although, honestly, I got stuck trying to find its expectation). So, I've tried making an unbiased estimator from scratch... with no luck. Any tips here would be helpful. Thanks.",,"['statistics', 'probability-distributions', 'statistical-inference', 'expected-value', 'parameter-estimation']"
52,compute temporal average of $\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)$,compute temporal average of,\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi),"assuming that $\Phi$ is uniformly distributed over $(0,2\pi)$ compute: $$E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]$$ I have solved the problem as continues: $$\begin{align} E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]&=\frac{1}{2\pi}\int_0^{2\pi}\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)\,dt\\ &=\frac{1}{4\pi}\int_0^{2\pi}(\cos(\omega_0\tau)-\cos(2\omega_0t+2\Phi+\omega_0\tau))\,dt\\ &=\frac{1}{4\pi}[t\,cos(\omega_0\tau)-\frac{1}{2\omega_0}\sin(2\omega_0t+2\Phi+\omega_0\tau)]_0^{2\pi}\\ &=\frac{1}{4\pi}[(2\pi)\cos(\omega_0\tau)-\frac{1}{2\omega_0}(\sin(4\pi\omega_0+2\Phi+\omega_0\tau)-\sin(2\Phi+\omega_0\tau))]\\ &=\frac{1}{2}\cos(\omega_0\tau)-\frac{1}{8\pi\omega_0}(\sin(4\pi\omega_0+2\Phi+\omega_0\tau)-\sin(2\Phi+\omega_0\tau)) \end{align}$$ but the book that I am studying has written $\frac{1}{2}\cos(\omega_0\tau)$ as the answer could you tell me what am I doing wrong? P.S. $\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)$ is a stochastic. if I integrate with respect to $\Phi$ I'll get ensemble average but if I integrate with respect to time I'll get temporal average. here I'm trying to compute the temporal average but I'm totally not sure about the interval of integration","assuming that $\Phi$ is uniformly distributed over $(0,2\pi)$ compute: $$E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]$$ I have solved the problem as continues: $$\begin{align} E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]&=\frac{1}{2\pi}\int_0^{2\pi}\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)\,dt\\ &=\frac{1}{4\pi}\int_0^{2\pi}(\cos(\omega_0\tau)-\cos(2\omega_0t+2\Phi+\omega_0\tau))\,dt\\ &=\frac{1}{4\pi}[t\,cos(\omega_0\tau)-\frac{1}{2\omega_0}\sin(2\omega_0t+2\Phi+\omega_0\tau)]_0^{2\pi}\\ &=\frac{1}{4\pi}[(2\pi)\cos(\omega_0\tau)-\frac{1}{2\omega_0}(\sin(4\pi\omega_0+2\Phi+\omega_0\tau)-\sin(2\Phi+\omega_0\tau))]\\ &=\frac{1}{2}\cos(\omega_0\tau)-\frac{1}{8\pi\omega_0}(\sin(4\pi\omega_0+2\Phi+\omega_0\tau)-\sin(2\Phi+\omega_0\tau)) \end{align}$$ but the book that I am studying has written $\frac{1}{2}\cos(\omega_0\tau)$ as the answer could you tell me what am I doing wrong? P.S. $\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)$ is a stochastic. if I integrate with respect to $\Phi$ I'll get ensemble average but if I integrate with respect to time I'll get temporal average. here I'm trying to compute the temporal average but I'm totally not sure about the interval of integration",,"['integration', 'statistics', 'trigonometry', 'stochastic-processes', 'expectation']"
53,"Cramer-Rao lower bound for normal($\theta, 4\theta^2$)",Cramer-Rao lower bound for normal(),"\theta, 4\theta^2","I am trying to find the Cramer-Rao lower bound for unbiased estimators of $\theta$, given a sample $X_1,\ldots, X_n \sim \textrm{normal}(\theta,4\theta^2)$. I am calculating the CRLB as $$ \frac{1}{-n\textrm{E}\left[\frac{\partial^2}{\partial\theta^2}\log f(x_i\vert\theta)\right]} $$ Evaluating this I get $\frac{4\theta^2}{9n}$. For that distribution however, from the exponential family representation I think that $\left(\sum_i x_i, \sum_i x_i^2 \right)$ is a complete sufficient statistic, and therefore any estimator based only on this should achieve the CRLB. But the variance of $\bar{X}$, which is an unbiased estimator of $\theta$ based only on the above, is $\frac{4\theta^2}{n}$ which is larger. Where have I gone wrong?","I am trying to find the Cramer-Rao lower bound for unbiased estimators of $\theta$, given a sample $X_1,\ldots, X_n \sim \textrm{normal}(\theta,4\theta^2)$. I am calculating the CRLB as $$ \frac{1}{-n\textrm{E}\left[\frac{\partial^2}{\partial\theta^2}\log f(x_i\vert\theta)\right]} $$ Evaluating this I get $\frac{4\theta^2}{9n}$. For that distribution however, from the exponential family representation I think that $\left(\sum_i x_i, \sum_i x_i^2 \right)$ is a complete sufficient statistic, and therefore any estimator based only on this should achieve the CRLB. But the variance of $\bar{X}$, which is an unbiased estimator of $\theta$ based only on the above, is $\frac{4\theta^2}{n}$ which is larger. Where have I gone wrong?",,"['statistics', 'statistical-inference']"
54,Deriving simple linear regression from normal equations,Deriving simple linear regression from normal equations,,"The normal equations of least squares regression $$X \beta = Y$$ yields the solution $\beta = (X X^T)^{-1} X^T Y$. For simple (1-dimensional) regression, the solution to $\beta x_i + \alpha = y_i$ is given by $\beta = \frac{Cov(X, Y)}{Var(X)}$. Is there a way to derive the second formula from the first? When $\alpha = 0$ and the mean of $X$ and $Y$ are $0$, this is obvious. But I don't know if there's a more general derivation.","The normal equations of least squares regression $$X \beta = Y$$ yields the solution $\beta = (X X^T)^{-1} X^T Y$. For simple (1-dimensional) regression, the solution to $\beta x_i + \alpha = y_i$ is given by $\beta = \frac{Cov(X, Y)}{Var(X)}$. Is there a way to derive the second formula from the first? When $\alpha = 0$ and the mean of $X$ and $Y$ are $0$, this is obvious. But I don't know if there's a more general derivation.",,"['linear-algebra', 'statistics']"
55,"Conditional distribution of $X$ exponential given $U\leq e^{-X}$, with $U$ uniform on $(0,1)$ [closed]","Conditional distribution of  exponential given , with  uniform on  [closed]","X U\leq e^{-X} U (0,1)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $X$ be exponentially distributed with mean $1$ and $U$ be a $U(0,1)$ random variable independent of $X$ . Define $$I= \begin{cases}1,&U \leq e^{-X}\\ 0,&\text{ otherwise}\end{cases}$$ Show that the conditional distribution of $X$ given $I=1$ is exponential with mean $\frac12$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let be exponentially distributed with mean and be a random variable independent of . Define Show that the conditional distribution of given is exponential with mean .","X 1 U U(0,1) X I= \begin{cases}1,&U \leq e^{-X}\\ 0,&\text{ otherwise}\end{cases} X I=1 \frac12","['statistics', 'probability-distributions']"
56,Curve fitting of a set of data,Curve fitting of a set of data,,"Suppose you have a set of data $\{x_i\}$ and $\{y_i\}$ with $i=0,\dots,N$. In order to find two parameters $a,b$ such that the line  $$ y=ax+b, $$ give the best linear fit, one proceed minimizing the quantity $$ \sum_i^N[y_i-ax_i-b]^2 $$ with respect to $a,b$ obtaining well know results. Imagine now to desire a fit with a function like $$ y=ax^p+b. $$ After some manipulation one obtain the following relations $$ a=\frac{N\sum_i(y_ix_i^p)-\sum_iy_i\cdot\sum_ix_i^p}{(\sum_ix_i^p)^2+N\sum_i(x_i^p)^2}, $$ $$ b=\frac{1}{N}[\sum_iy_i-a\sum_ix_i^p] $$ and $$ \frac{1}{N}[N\sum_i(y_ix_i^p\ln x_i)-\sum_iy_i\cdot\sum_ix_i^p\ln x_i]=\frac{a}{N}[N\sum_i(x_i^p)^2\ln x_i-\sum_ix_i^p\cdot\sum_ix_i^p\ln x_i. $$ To me it seems that from this it is nearly impossible to extract the exponent $p$. Am I correct?","Suppose you have a set of data $\{x_i\}$ and $\{y_i\}$ with $i=0,\dots,N$. In order to find two parameters $a,b$ such that the line  $$ y=ax+b, $$ give the best linear fit, one proceed minimizing the quantity $$ \sum_i^N[y_i-ax_i-b]^2 $$ with respect to $a,b$ obtaining well know results. Imagine now to desire a fit with a function like $$ y=ax^p+b. $$ After some manipulation one obtain the following relations $$ a=\frac{N\sum_i(y_ix_i^p)-\sum_iy_i\cdot\sum_ix_i^p}{(\sum_ix_i^p)^2+N\sum_i(x_i^p)^2}, $$ $$ b=\frac{1}{N}[\sum_iy_i-a\sum_ix_i^p] $$ and $$ \frac{1}{N}[N\sum_i(y_ix_i^p\ln x_i)-\sum_iy_i\cdot\sum_ix_i^p\ln x_i]=\frac{a}{N}[N\sum_i(x_i^p)^2\ln x_i-\sum_ix_i^p\cdot\sum_ix_i^p\ln x_i. $$ To me it seems that from this it is nearly impossible to extract the exponent $p$. Am I correct?",,"['statistics', 'numerical-methods']"
57,probability of rank for n iid random variables,probability of rank for n iid random variables,,"Denote the $i$th order statistic as $X_{(i)}$, then what is $\Pr(X_i = X_{(j)})$ for some $i, j = 1,...,n$ and iid $X_1, ..., X_n$? I think this is what rank tests are based on but I'm stuck on making any progress.","Denote the $i$th order statistic as $X_{(i)}$, then what is $\Pr(X_i = X_{(j)})$ for some $i, j = 1,...,n$ and iid $X_1, ..., X_n$? I think this is what rank tests are based on but I'm stuck on making any progress.",,"['probability', 'statistics']"
58,Hypothesis-test; test about equal sample means,Hypothesis-test; test about equal sample means,,"I'm asked to formulate and do a test of the hypothesis that the sample mean (average grade) of math students and economic students are equal. Data below: I'm not sure how to ""attack"" this problem. I already did a exercise where I was asked to test the hypothesis that the variance of male and female math students were the same. But now, when the math-students are devided in two ""subgroups""; male-students and female-students/, and the economic-students consists of only one subgroup (both males and females), I'm not sure how to begin. Can I somehow add the two sample means and use this ""pooled"" sample mean as a representative sample mean for math students and then just do a regular t-test, or how should I move forward? Any help will be much appreciated.","I'm asked to formulate and do a test of the hypothesis that the sample mean (average grade) of math students and economic students are equal. Data below: I'm not sure how to ""attack"" this problem. I already did a exercise where I was asked to test the hypothesis that the variance of male and female math students were the same. But now, when the math-students are devided in two ""subgroups""; male-students and female-students/, and the economic-students consists of only one subgroup (both males and females), I'm not sure how to begin. Can I somehow add the two sample means and use this ""pooled"" sample mean as a representative sample mean for math students and then just do a regular t-test, or how should I move forward? Any help will be much appreciated.",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
59,Expectation of CDF of normal random variable.,Expectation of CDF of normal random variable.,,"Let $\Phi$ be the CDF of the standard normal distribution $N(0,1)$. Then can we analytically compute $E(\Phi(y-c))$, where $y$ follows $N(0,1)$ and $c>0$? I need this for my research in social science. I know it is 1/2 if $c=0$, but I would like to obtain it for $c>0$.","Let $\Phi$ be the CDF of the standard normal distribution $N(0,1)$. Then can we analytically compute $E(\Phi(y-c))$, where $y$ follows $N(0,1)$ and $c>0$? I need this for my research in social science. I know it is 1/2 if $c=0$, but I would like to obtain it for $c>0$.",,"['integration', 'statistics']"
60,Probability that hand contains Ace and King of at least one suit?,Probability that hand contains Ace and King of at least one suit?,,"Compute the probability that a hand of 13 cards (drawn randomly from a standard deck of 52) contains both the ace and the king from at least one suit. I think I would use the inclusion exclusion principle here, but I'm not sure how to start. Would it be something like: P(Ace+King of suit 1) + P(Ace+King of suit 2) + P(Ace+King of suit 3) + P(Ace+King of suit 4) - ... so on.?","Compute the probability that a hand of 13 cards (drawn randomly from a standard deck of 52) contains both the ace and the king from at least one suit. I think I would use the inclusion exclusion principle here, but I'm not sure how to start. Would it be something like: P(Ace+King of suit 1) + P(Ace+King of suit 2) + P(Ace+King of suit 3) + P(Ace+King of suit 4) - ... so on.?",,"['probability', 'statistics', 'card-games']"
61,Standard deviation of the mean of sample data,Standard deviation of the mean of sample data,,"I can't quite understand what this formula means: $$\sigma_{\overline{x}}=\frac{\sigma}{\sqrt n}$$ I know what standard deviation $\sigma$ is - it's the average distance of my data points (samples) from the mean. But this part is confusing: For example, suppose the random variable $X$ records a randomly selected   student's score on a national test, where the population distribution   for the score is normal with mean $70$ and standard deviation $5$   ($N(70,5)$). Given a simple random sample (SRS) of $200$ students, the   distribution of the sample mean score   has mean $70$ and standard   deviation $$\frac{5}{\sqrt{200}} \approx \frac{5}{14.14} \approx 0.35$$ Source I thought the standard deviation $\sigma = 5$ means that if I take the scores of all students and calculate the mean, then the average distance of a score from that mean will be equal to $5$. The set of all scores is called the 'population', right? But here it says the more students' scores I take, the lower the standard deviation - thus the closer the number of samples gets to the size of population, the lower the standard deviation (and its get further from $5$).","I can't quite understand what this formula means: $$\sigma_{\overline{x}}=\frac{\sigma}{\sqrt n}$$ I know what standard deviation $\sigma$ is - it's the average distance of my data points (samples) from the mean. But this part is confusing: For example, suppose the random variable $X$ records a randomly selected   student's score on a national test, where the population distribution   for the score is normal with mean $70$ and standard deviation $5$   ($N(70,5)$). Given a simple random sample (SRS) of $200$ students, the   distribution of the sample mean score   has mean $70$ and standard   deviation $$\frac{5}{\sqrt{200}} \approx \frac{5}{14.14} \approx 0.35$$ Source I thought the standard deviation $\sigma = 5$ means that if I take the scores of all students and calculate the mean, then the average distance of a score from that mean will be equal to $5$. The set of all scores is called the 'population', right? But here it says the more students' scores I take, the lower the standard deviation - thus the closer the number of samples gets to the size of population, the lower the standard deviation (and its get further from $5$).",,"['statistics', 'standard-deviation']"
62,"Statistics Problems, I don't understand what this means..","Statistics Problems, I don't understand what this means..",,P(A)=0.46 and P(B)=0.42 If P(B∣A)= 0.174 what is P(A∩B)?,P(A)=0.46 and P(B)=0.42 If P(B∣A)= 0.174 what is P(A∩B)?,,"['probability', 'statistics']"
63,Chi-square test of independence: show that sum of squared standard normals has chi-square distribution,Chi-square test of independence: show that sum of squared standard normals has chi-square distribution,,"I'm studying the chi-square test of independence. According to my understanding, we first hypothesize independence between variables and consider them as being normally distributed. Then we go on to calculate the test statistic as $$ Z_i = \frac{O_i-E_i}{\sqrt{E_i}}$$ where O is the observed value from table and E is the calculated expectation. Is $Z_i \sim \mathcal{N}(0,1)$ ? Is $\sum_1^k Z_i^2 \sim \chi^2(k)$ ?","I'm studying the chi-square test of independence. According to my understanding, we first hypothesize independence between variables and consider them as being normally distributed. Then we go on to calculate the test statistic as $$ Z_i = \frac{O_i-E_i}{\sqrt{E_i}}$$ where O is the observed value from table and E is the calculated expectation. Is $Z_i \sim \mathcal{N}(0,1)$ ? Is $\sum_1^k Z_i^2 \sim \chi^2(k)$ ?",,"['probability', 'statistics', 'statistical-inference']"
64,What does SSB and SSW of ANOVA tell you about your data?,What does SSB and SSW of ANOVA tell you about your data?,,"This might be a stupid question. I know how to calculate them but mine not sure what they are telling me about my data set. What does it mean if $\sum  $$(SSB)^2$ $\ge \sum  (SSW)^2$, or vise versa, or does $SSB$ and $SSW$ only have meaning when you sum them to find Total $\sum  $$(SST)^2$.","This might be a stupid question. I know how to calculate them but mine not sure what they are telling me about my data set. What does it mean if $\sum  $$(SSB)^2$ $\ge \sum  (SSW)^2$, or vise versa, or does $SSB$ and $SSW$ only have meaning when you sum them to find Total $\sum  $$(SST)^2$.",,['statistics']
65,Finding the probability of an event with binomial distribution using a normal approximation,Finding the probability of an event with binomial distribution using a normal approximation,,"A Tarheels basketball player is obsessed about practicing his free throws. It is known that he is $75\%$ free throw shooter. One morning he decides to shoot $100$ free throws. You may assume that the free throws are independent and the chance that he makes any given free throw is $0.75$. What is the chance he makes more than $80$ free throws? I thought you would do $$80-75\sqrt{100\cdot 0.75(1-0.75)}$$ to get $3.46$, but the answer key says the answer is $0.1241$.  Can someone explain this to me?","A Tarheels basketball player is obsessed about practicing his free throws. It is known that he is $75\%$ free throw shooter. One morning he decides to shoot $100$ free throws. You may assume that the free throws are independent and the chance that he makes any given free throw is $0.75$. What is the chance he makes more than $80$ free throws? I thought you would do $$80-75\sqrt{100\cdot 0.75(1-0.75)}$$ to get $3.46$, but the answer key says the answer is $0.1241$.  Can someone explain this to me?",,"['probability', 'statistics']"
66,Kolmogorov-Smirnov two-sample test,Kolmogorov-Smirnov two-sample test,,"I want to test if two samples are drawn from the same distribution. I generated two random arrays and used a python function to derive the KS statistic $D$ and the two-tailed p-value $P$: >>> import numpy as np >>> from scipy import stats >>> a=np.random.random_integers(1,9,4) >>> a array([3, 7, 4, 3]) >>> b=np.random.random_integers(1,9,5) >>> b array([2, 2, 3, 7, 9]) >>> stats.ks_2samp(a,b) (0.40000000000000002, 0.75428850089034016) From the documentation of http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp.html I know that $$D=0.40000000000000002$$ and $$P=0.75428850089034016$$ So the probability that the two samples are drawn from the same distribution is $\sim75\%$. Now my question is what does $D$ tell me? And is there a simple way to calculate these two values by hand? The wikipedia article does not have a simple example with two samples, that is why I am trying finally to find an answer here.","I want to test if two samples are drawn from the same distribution. I generated two random arrays and used a python function to derive the KS statistic $D$ and the two-tailed p-value $P$: >>> import numpy as np >>> from scipy import stats >>> a=np.random.random_integers(1,9,4) >>> a array([3, 7, 4, 3]) >>> b=np.random.random_integers(1,9,5) >>> b array([2, 2, 3, 7, 9]) >>> stats.ks_2samp(a,b) (0.40000000000000002, 0.75428850089034016) From the documentation of http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp.html I know that $$D=0.40000000000000002$$ and $$P=0.75428850089034016$$ So the probability that the two samples are drawn from the same distribution is $\sim75\%$. Now my question is what does $D$ tell me? And is there a simple way to calculate these two values by hand? The wikipedia article does not have a simple example with two samples, that is why I am trying finally to find an answer here.",,"['statistics', 'random-variables', 'normal-distribution']"
67,Matrices - Inverse of the principal square root of a covariance matrix (^-1/2),Matrices - Inverse of the principal square root of a covariance matrix (^-1/2),,"Say you have a square (variance)covariance matrix S How would one go about working S^-1/2 (inverse of the principle square)? Bearing in mind, I'm trying to understand a paper which states: I've tried multiple suggestions, such as: square root, then take the inverse of the square root: Inverse Square Root Of Matrix reciprocal of the square root of each term in the diagonal: Raising a square matrix to a negative half power plus tried (^-1/2) directly on multiple online matrix calculators However, none of them seem to hold true for the latter part of the what's stated on the paper.","Say you have a square (variance)covariance matrix S How would one go about working S^-1/2 (inverse of the principle square)? Bearing in mind, I'm trying to understand a paper which states: I've tried multiple suggestions, such as: square root, then take the inverse of the square root: Inverse Square Root Of Matrix reciprocal of the square root of each term in the diagonal: Raising a square matrix to a negative half power plus tried (^-1/2) directly on multiple online matrix calculators However, none of them seem to hold true for the latter part of the what's stated on the paper.",,"['matrices', 'statistics', 'matrix-equations', 'matrix-calculus', 'covariance']"
68,Some true/false statements about MLE and UMVUE for a normal distribution,Some true/false statements about MLE and UMVUE for a normal distribution,,"Let $X_1,X_2,...,X_n$ (assume $n\geq 2$) be a random sample from an $N(\mu,\sigma^2)$ population where $-\infty<\mu <\infty$ and $\sigma^2>0$ are unknown. Which of the following statements is/are true ? (A) The MLE of $\mu$ attains Cramer-Rao lower bound. TRUE (because, unbiased) (B) The UMVUE of $\mu$ attains Cramer-Rao lower bound. TRUE (because, unbiased) (C) The MLE of $\sigma^2$ is an unbiased estimator of $\sigma^2$. FALSE (denominator is 1/n) (D) The relative efficiency of the MLE of $\sigma^2$ w.r.t the UMVUE of $\sigma^2$ is strictly less than 1. TRUE (because UMVUE has efficiency of 1 and MLE has efficiency lower than 1) I need some validation to ensure my reasons and answers are correct here. Please advise.","Let $X_1,X_2,...,X_n$ (assume $n\geq 2$) be a random sample from an $N(\mu,\sigma^2)$ population where $-\infty<\mu <\infty$ and $\sigma^2>0$ are unknown. Which of the following statements is/are true ? (A) The MLE of $\mu$ attains Cramer-Rao lower bound. TRUE (because, unbiased) (B) The UMVUE of $\mu$ attains Cramer-Rao lower bound. TRUE (because, unbiased) (C) The MLE of $\sigma^2$ is an unbiased estimator of $\sigma^2$. FALSE (denominator is 1/n) (D) The relative efficiency of the MLE of $\sigma^2$ w.r.t the UMVUE of $\sigma^2$ is strictly less than 1. TRUE (because UMVUE has efficiency of 1 and MLE has efficiency lower than 1) I need some validation to ensure my reasons and answers are correct here. Please advise.",,"['statistics', 'statistical-inference']"
69,Variance of first 30 odd numbers,Variance of first 30 odd numbers,,How to calculate variance of first 30 odd numbers ? The method I used requires finding the mean and subtracting each number with it and then squaring the result . Its getting difficult . I used this : Any other way to calculate ?,How to calculate variance of first 30 odd numbers ? The method I used requires finding the mean and subtracting each number with it and then squaring the result . Its getting difficult . I used this : Any other way to calculate ?,,['statistics']
70,Calculating power of a Hypothesis Testing Problem based on Uniform distribution,Calculating power of a Hypothesis Testing Problem based on Uniform distribution,,"Consider the problem of testing $H_0:a=0$ against $H_1:a=1/2$ based on a single observation X from U(a,a+1). The power of the test ""Reject $H_0$ if $X>2/3$"" is (A)1/6     (B)5/6   (C)1/3   (D)2/3 My Steps: Power of Test=P(reject $H_0$|$H_1$ is true) c.d.f. of continuous Uniform distribution is given be $\frac{x-a}{b-a}$, where a and b are parameters of the given Uniform distribution, $U(a,b)$. $$\begin{align} \text{Power of Test} & = P(\text{reject } H_0|H_1 \text{is true)} \\  & = P(X>2/3|a=1/2) \\   & = \frac{(2/3-1/2)}{1} \\  & = 1/6  \end{align}$$ Did I solve this correctly ? Please help me confirm my solution.","Consider the problem of testing $H_0:a=0$ against $H_1:a=1/2$ based on a single observation X from U(a,a+1). The power of the test ""Reject $H_0$ if $X>2/3$"" is (A)1/6     (B)5/6   (C)1/3   (D)2/3 My Steps: Power of Test=P(reject $H_0$|$H_1$ is true) c.d.f. of continuous Uniform distribution is given be $\frac{x-a}{b-a}$, where a and b are parameters of the given Uniform distribution, $U(a,b)$. $$\begin{align} \text{Power of Test} & = P(\text{reject } H_0|H_1 \text{is true)} \\  & = P(X>2/3|a=1/2) \\   & = \frac{(2/3-1/2)}{1} \\  & = 1/6  \end{align}$$ Did I solve this correctly ? Please help me confirm my solution.",,"['statistics', 'statistical-inference', 'uniform-distribution', 'hypothesis-testing']"
71,Calculating $\mathrm{Var} (Z|Z|)$ [closed],Calculating  [closed],\mathrm{Var} (Z|Z|),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Suppose $Z\sim N(0,1). $ How can I calculate $\mathrm{Var} (Z|Z|)$? I know $\mathrm{Var} (Z|Z|)= \mathrm{E}(Z^4)-\mathrm{E}^2(Z|Z|)$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Suppose $Z\sim N(0,1). $ How can I calculate $\mathrm{Var} (Z|Z|)$? I know $\mathrm{Var} (Z|Z|)= \mathrm{E}(Z^4)-\mathrm{E}^2(Z|Z|)$",,['statistics']
72,Unknown Problem - Need Help,Unknown Problem - Need Help,,"This question is from an article I read online, titled: ""Why I Will Never Have a Girlfriend"" by Tristan Miller. The author shows his work, step-by-step throughout most of the article, except during the conclusion, when he makes the following statement: ""At first glance, a datable population of 18,726 may not seem like such a low number, but consider this: assuming I were to go on a blind date with a new girl about my age every week, I would have to date for 3,493 weeks before I found 1 of the 18,726."" He does not explain how he determined 3,493 is the number required to find 1 of the 18,726. I would really like to know. It seems similar to the Birthday Paradox, but I'm not certain? Thank you in advance for any help / clue. I would especially appreciate anyone who explained, step-by-step how to figure this out, mathematically. P.S. Apologies; I am not even certain exactly what type of math problem this even is, or what to call it.","This question is from an article I read online, titled: ""Why I Will Never Have a Girlfriend"" by Tristan Miller. The author shows his work, step-by-step throughout most of the article, except during the conclusion, when he makes the following statement: ""At first glance, a datable population of 18,726 may not seem like such a low number, but consider this: assuming I were to go on a blind date with a new girl about my age every week, I would have to date for 3,493 weeks before I found 1 of the 18,726."" He does not explain how he determined 3,493 is the number required to find 1 of the 18,726. I would really like to know. It seems similar to the Birthday Paradox, but I'm not certain? Thank you in advance for any help / clue. I would especially appreciate anyone who explained, step-by-step how to figure this out, mathematically. P.S. Apologies; I am not even certain exactly what type of math problem this even is, or what to call it.",,"['probability', 'statistics', 'combinations']"
73,Cramer-Rao and Efficient Estimators,Cramer-Rao and Efficient Estimators,,"Let $X_1,X_2,X_3,...,X_n$ be a random sample from the exponential distribution having PDF $f(x;\lambda)= \frac{1}{\lambda}e^\frac{-x}{\lambda}\chi\{x>0\}.$ A) Find the Cramer-Rao lower bound for the variance of unbiased estimators for $\theta = \lambda^2$, B)Determine k so that $W=k\sum\limits_{i=1}^n X^2_i$ is unbiased for $\theta$. Is W an efficient estimator for $\theta?$ (Recall:$E(X^2_i)= 2\lambda^2$ and $E(X^4_i)=24\lambda^4.)$ This is a homework problem, The notes in class and book only cover C.R.L.B  for $f_Y(x;\lambda)$ and comparing it to variance of a given estimator which I understand. I don't see the connection though for using it on the variance of unbiased estimators. Is it just different terminology for the same thing or what is it that i'm missing? Can some one help me get started in the right direction? Every online source seems to reference Fisher information, haven't cover Fisher yet.","Let $X_1,X_2,X_3,...,X_n$ be a random sample from the exponential distribution having PDF $f(x;\lambda)= \frac{1}{\lambda}e^\frac{-x}{\lambda}\chi\{x>0\}.$ A) Find the Cramer-Rao lower bound for the variance of unbiased estimators for $\theta = \lambda^2$, B)Determine k so that $W=k\sum\limits_{i=1}^n X^2_i$ is unbiased for $\theta$. Is W an efficient estimator for $\theta?$ (Recall:$E(X^2_i)= 2\lambda^2$ and $E(X^4_i)=24\lambda^4.)$ This is a homework problem, The notes in class and book only cover C.R.L.B  for $f_Y(x;\lambda)$ and comparing it to variance of a given estimator which I understand. I don't see the connection though for using it on the variance of unbiased estimators. Is it just different terminology for the same thing or what is it that i'm missing? Can some one help me get started in the right direction? Every online source seems to reference Fisher information, haven't cover Fisher yet.",,"['probability', 'statistics', 'probability-theory', 'order-statistics']"
74,"Why is the probability of a dice being larger then the 2nd roll the same 2nd,3rd?","Why is the probability of a dice being larger then the 2nd roll the same 2nd,3rd?",,"I understand that the probability of the first die roll being larger then the second is 5/12. I'm having a harder time understanding why, according to a small problem I wrote, the probability of the first die being larger on the first roll then the next two rolls is still 5/12. It would seem to me that these would be two events. p(a) = 5/12 and p(b) = 5/12. p(a and b) would then be p(a) * p(b) I would think, considering these are independent events?","I understand that the probability of the first die roll being larger then the second is 5/12. I'm having a harder time understanding why, according to a small problem I wrote, the probability of the first die being larger on the first roll then the next two rolls is still 5/12. It would seem to me that these would be two events. p(a) = 5/12 and p(b) = 5/12. p(a and b) would then be p(a) * p(b) I would think, considering these are independent events?",,"['probability', 'statistics', 'discrete-mathematics', 'probability-distributions', 'dice']"
75,The square of a standard Normal random variable,The square of a standard Normal random variable,,I am having a bit of trouble with this: Let $U=Z^2$ where Z is a standard Normal random variable with pdf: $$f_z(z) = \frac{1}{\sqrt{2\pi}} e^{\frac{-z^2}{2}}$$ I want to use the inversion method but have thus far only learned to use this when functions are strictly increasing or decreasing. Since a standard normal distribution function is strictly increasing and then strictly decreasing I thought perhaps I could find some way to use this method. I have the final answer as $f_u(u)= \frac{1}{\sqrt{2\pi}\sqrt{u}}e^{\frac{-u}{2}}$ for $u>0$ But I am not very comfortable with the process of getting to that answer. I used a bit of a walk through and made some assumptions about what was happening. Could anyone help me understand how I should use the above information to reach this answer?,I am having a bit of trouble with this: Let $U=Z^2$ where Z is a standard Normal random variable with pdf: $$f_z(z) = \frac{1}{\sqrt{2\pi}} e^{\frac{-z^2}{2}}$$ I want to use the inversion method but have thus far only learned to use this when functions are strictly increasing or decreasing. Since a standard normal distribution function is strictly increasing and then strictly decreasing I thought perhaps I could find some way to use this method. I have the final answer as $f_u(u)= \frac{1}{\sqrt{2\pi}\sqrt{u}}e^{\frac{-u}{2}}$ for $u>0$ But I am not very comfortable with the process of getting to that answer. I used a bit of a walk through and made some assumptions about what was happening. Could anyone help me understand how I should use the above information to reach this answer?,,"['probability', 'statistics']"
76,Determining density involving scaled beta distribution,Determining density involving scaled beta distribution,,"Suppose $Y \sim \mathrm{Beta}(2,1)$. If $X = \theta{Y}$ (for some $\theta > 0$) how do I determine the joint density $f(x, \theta)$? Edit: the density for $Z$ is $2z$. Would it be correct to say, then, that $$f(x, \theta) = \frac{2}{\theta} x$$","Suppose $Y \sim \mathrm{Beta}(2,1)$. If $X = \theta{Y}$ (for some $\theta > 0$) how do I determine the joint density $f(x, \theta)$? Edit: the density for $Z$ is $2z$. Would it be correct to say, then, that $$f(x, \theta) = \frac{2}{\theta} x$$",,"['probability', 'statistics', 'probability-theory']"
77,Skewness of a difference of random variables?,Skewness of a difference of random variables?,,In this article( http://www.diva-portal.org/smash/get/diva2:302313/FULLTEXT01.pdf )page 28 explains how to derive the skewness of a sum of random variables; I haven't been able to derive this expression in case of dealing with a difference of random variables. The final dexpression for the skewness of the sum of independent random variables is: skewness(X+Y)=(μ3(X)+μ3(Y))/(μ2(X)+μ2(Y))^3/2,In this article( http://www.diva-portal.org/smash/get/diva2:302313/FULLTEXT01.pdf )page 28 explains how to derive the skewness of a sum of random variables; I haven't been able to derive this expression in case of dealing with a difference of random variables. The final dexpression for the skewness of the sum of independent random variables is: skewness(X+Y)=(μ3(X)+μ3(Y))/(μ2(X)+μ2(Y))^3/2,,['statistics']
78,Rao-Blackwell Theorem - Best estimator (Statistics),Rao-Blackwell Theorem - Best estimator (Statistics),,"Let $X_1 , ... , X_n$ be a series of independent random variables following a Bernoulli distribution with parameter $\theta$. And let $S_n = \sum_1^n X_i$. We know an unbiased estimator of the variance for the Bernoulli distribution: $$1/2 * (X_1 - X_2)^2$$ Using the Rao-Blackwell Theorem find the best estimator: (Improved from Rao BlackWell) $$Z = E_\theta(1/2*(X_1 - X_2)^2 | S_n)$$ Sorry for the poor translation of the problem, the original text is in french. Does anyone have any idea of how to do that? So far neither my book or internet haven't been really helpful...   Thanks in advance","Let $X_1 , ... , X_n$ be a series of independent random variables following a Bernoulli distribution with parameter $\theta$. And let $S_n = \sum_1^n X_i$. We know an unbiased estimator of the variance for the Bernoulli distribution: $$1/2 * (X_1 - X_2)^2$$ Using the Rao-Blackwell Theorem find the best estimator: (Improved from Rao BlackWell) $$Z = E_\theta(1/2*(X_1 - X_2)^2 | S_n)$$ Sorry for the poor translation of the problem, the original text is in french. Does anyone have any idea of how to do that? So far neither my book or internet haven't been really helpful...   Thanks in advance",,['statistics']
79,$\mathbb E[\bar X_n]=0$,,\mathbb E[\bar X_n]=0,"A conditional normal rv sequence, does the mean converges in probability , in this question how can i get $\mathbb E[\bar X_n]=0$ ? Here is my attempt; $$\mathbb E[\bar X_n]=\int_{-\infty}^{\infty}\bar x_n f(x)dx$$ $$=\int_{-\infty}^{\infty}\bar x_n\frac{1}{y_k\sqrt{2\pi}}exp[-\frac{1}{2}(\frac{x-y_k}{y_k^2})]dx$$ $$=\frac{1}{y_k\sqrt{2\pi}}\int_{-\infty}^{\infty}\bar x_nexp[-\frac{1}{2}(\frac{x-y_k}{y_k^2})]dx$$ $$=\frac{1}{y_k\sqrt{2\pi}}e^{\frac{1}{2y_k}}\int_{-\infty}^{\infty}\bar x_n e^{-\frac{1}{2}\frac{x_k}{y_k^2}}dx$$ $$=\frac{1}{y_k\sqrt{2\pi}}e^{\frac{1}{2y_k}}\int_{-\infty}^{\infty}\frac{\sum_{i=1}^{n}x_i}{n} e^{-\frac{1}{2}\frac{x_k}{y_k^2}}dx$$ $$=\frac{1}{y_k\sqrt{2\pi}}e^{\frac{1}{2y_k}}\frac{1}{n}\int_{-\infty}^{\infty}\sum_{i=1}^{n}x_i e^{-\frac{1}{2}\frac{x_k}{y_k^2}}dx$$","A conditional normal rv sequence, does the mean converges in probability , in this question how can i get ? Here is my attempt;",\mathbb E[\bar X_n]=0 \mathbb E[\bar X_n]=\int_{-\infty}^{\infty}\bar x_n f(x)dx =\int_{-\infty}^{\infty}\bar x_n\frac{1}{y_k\sqrt{2\pi}}exp[-\frac{1}{2}(\frac{x-y_k}{y_k^2})]dx =\frac{1}{y_k\sqrt{2\pi}}\int_{-\infty}^{\infty}\bar x_nexp[-\frac{1}{2}(\frac{x-y_k}{y_k^2})]dx =\frac{1}{y_k\sqrt{2\pi}}e^{\frac{1}{2y_k}}\int_{-\infty}^{\infty}\bar x_n e^{-\frac{1}{2}\frac{x_k}{y_k^2}}dx =\frac{1}{y_k\sqrt{2\pi}}e^{\frac{1}{2y_k}}\int_{-\infty}^{\infty}\frac{\sum_{i=1}^{n}x_i}{n} e^{-\frac{1}{2}\frac{x_k}{y_k^2}}dx =\frac{1}{y_k\sqrt{2\pi}}e^{\frac{1}{2y_k}}\frac{1}{n}\int_{-\infty}^{\infty}\sum_{i=1}^{n}x_i e^{-\frac{1}{2}\frac{x_k}{y_k^2}}dx,"['algebra-precalculus', 'statistics', 'convergence-divergence', 'self-learning', 'central-limit-theorem']"
80,Does Binomial variables independence implies Bernoulli variables independence,Does Binomial variables independence implies Bernoulli variables independence,,"$X$, $Y$ are independent variables with Binomial distribution. $X={\sum_{i=1}^nX_i}$, $Y={\sum_{i=1}^nY_i}$. $X_i$, ($1\le i\le n$) are independent Bernoulli variables. Same applies for $Y_i$ Is the set of $X_i$ and $Y_i$ independent?","$X$, $Y$ are independent variables with Binomial distribution. $X={\sum_{i=1}^nX_i}$, $Y={\sum_{i=1}^nY_i}$. $X_i$, ($1\le i\le n$) are independent Bernoulli variables. Same applies for $Y_i$ Is the set of $X_i$ and $Y_i$ independent?",,"['probability', 'statistics']"
81,Probability of a point from one normal distribution being higher than a point from another independent normal distribution,Probability of a point from one normal distribution being higher than a point from another independent normal distribution,,"Given two independent normal distributions: Distribution 1: Mean $= 23.95$ , SD $= 7.44$ Distribution 2: Mean $= 16.29$ , SD $= 7.79$ How often on average will a point from Distribution 2 be greater than a point from Distribution 1? I apologize for any nomenclature that is incorrect. Progress I know that the distribution of $Z=Y-X$ is normal, as well as its mean and variance.","Given two independent normal distributions: Distribution 1: Mean , SD Distribution 2: Mean , SD How often on average will a point from Distribution 2 be greater than a point from Distribution 1? I apologize for any nomenclature that is incorrect. Progress I know that the distribution of is normal, as well as its mean and variance.",= 23.95 = 7.44 = 16.29 = 7.79 Z=Y-X,"['statistics', 'probability-distributions', 'normal-distribution']"
82,Meaning of denominator in correlation?,Meaning of denominator in correlation?,,"I can't quite grasp the meaning of the denominator in the correlation coefficient. $$\frac{\sum(X - \bar X)(Y-\bar Y)}{\sqrt {\sum (X-\bar X)^2\sum(Y-\bar Y)^2}}$$ What exactly am I dividing with, and why? I understood dividing with the standard deviation in the Z distribution, that got me the difference from the mean in terms of standard deviations. But what does this give? The covariance measured in....what, the standard deviation of X times the standard deviation of Y? That would explain where the n's in the denominator (of the covariance as well as that of the standard deviations) have gone, but what does that mean ?","I can't quite grasp the meaning of the denominator in the correlation coefficient. $$\frac{\sum(X - \bar X)(Y-\bar Y)}{\sqrt {\sum (X-\bar X)^2\sum(Y-\bar Y)^2}}$$ What exactly am I dividing with, and why? I understood dividing with the standard deviation in the Z distribution, that got me the difference from the mean in terms of standard deviations. But what does this give? The covariance measured in....what, the standard deviation of X times the standard deviation of Y? That would explain where the n's in the denominator (of the covariance as well as that of the standard deviations) have gone, but what does that mean ?",,"['statistics', 'correlation']"
83,"Joint distribution $(X_1,X_{(n)})$ order statistics",Joint distribution  order statistics,"(X_1,X_{(n)})","Let $X_1, \ldots, X_n$ a random sample of a Uniform(0,1), I want to show which the joint distribution of $(X_1,X_{(n)})$ is. I do the following: $$ P(X_1\leq x, X_{(n)}\leq y)=P(X_1\leq x, X_1\leq y, \ldots , X_n\leq y)=$$ $$ P(X_1\leq \min(x,y), X_2\leq y, \ldots, X_n\leq y)=P((X_1\leq \min(x,y))P(X_2\leq y)\ldots P( X_n\leq y)=$$ $$ = \min(x,y)y^{n-1}$$ When I compute the density function (by derivation), it is: $f(x,y)=(n-1)y^{n-2}$ in $0\leq x\leq y\leq 1$ and it doesn't integrate $1$ but $\frac{n-1}{n}$. I don't know where the problem is.","Let $X_1, \ldots, X_n$ a random sample of a Uniform(0,1), I want to show which the joint distribution of $(X_1,X_{(n)})$ is. I do the following: $$ P(X_1\leq x, X_{(n)}\leq y)=P(X_1\leq x, X_1\leq y, \ldots , X_n\leq y)=$$ $$ P(X_1\leq \min(x,y), X_2\leq y, \ldots, X_n\leq y)=P((X_1\leq \min(x,y))P(X_2\leq y)\ldots P( X_n\leq y)=$$ $$ = \min(x,y)y^{n-1}$$ When I compute the density function (by derivation), it is: $f(x,y)=(n-1)y^{n-2}$ in $0\leq x\leq y\leq 1$ and it doesn't integrate $1$ but $\frac{n-1}{n}$. I don't know where the problem is.",,"['statistics', 'probability-distributions', 'order-statistics']"
84,Expectation of vector valued functions,Expectation of vector valued functions,,"Let $t_1,\ldots,t_m$ be $m$ random variables that are independently and identically drawn from a Bernoulli distribution with a constant parameter $p$. Now, we define some functions of $t_1,\ldots,t_m$, for constant $w_1,\ldots,w_m$, in the following way: $$f_i(t_1,\ldots,t_m)=t_i\mathbf{I}(1-\sum_{j=1}^m w_jt_j\geq 0)$$ where $\mathbf{I}(x\geq 0)=1$ if $x\geq 0$, and $\mathbf{I}(x\geq 0)=0$ if $x< 0$. My question: what is the joint expected value of $f_i$'s as a function of $w_1,\ldots,w_m$ and $p$. $$g_i(w_1,\ldots,w_m,p)=\mathbf{E}[f_i(t_1,\ldots,t_m)]=?$$ Edit: If $f(t_1,\ldots,t_m)=[f_1(t_1,\ldots,t_m),\ldots,f_m(t_1,\ldots,t_m)]^T$, then I am interested in: $$g(w_1,\ldots,w_m,p)=\mathbf{E}_{t_1,\ldots,t_m~\text{iid}\sim Bernoulli(p)}[f(t_1,\ldots,t_m)]$$","Let $t_1,\ldots,t_m$ be $m$ random variables that are independently and identically drawn from a Bernoulli distribution with a constant parameter $p$. Now, we define some functions of $t_1,\ldots,t_m$, for constant $w_1,\ldots,w_m$, in the following way: $$f_i(t_1,\ldots,t_m)=t_i\mathbf{I}(1-\sum_{j=1}^m w_jt_j\geq 0)$$ where $\mathbf{I}(x\geq 0)=1$ if $x\geq 0$, and $\mathbf{I}(x\geq 0)=0$ if $x< 0$. My question: what is the joint expected value of $f_i$'s as a function of $w_1,\ldots,w_m$ and $p$. $$g_i(w_1,\ldots,w_m,p)=\mathbf{E}[f_i(t_1,\ldots,t_m)]=?$$ Edit: If $f(t_1,\ldots,t_m)=[f_1(t_1,\ldots,t_m),\ldots,f_m(t_1,\ldots,t_m)]^T$, then I am interested in: $$g(w_1,\ldots,w_m,p)=\mathbf{E}_{t_1,\ldots,t_m~\text{iid}\sim Bernoulli(p)}[f(t_1,\ldots,t_m)]$$",,"['probability', 'statistics', 'expectation']"
85,Finding variance from bell curve,Finding variance from bell curve,,Find variance from the graph given. I know the mean is 6 but I have no idea how to find the variance using this graph,Find variance from the graph given. I know the mean is 6 but I have no idea how to find the variance using this graph,,['statistics']
86,Random sequence of $0$'s and $1$'s - what is the average 'in a row' succession,Random sequence of 's and 's - what is the average 'in a row' succession,0 1,"Let's say we create a sequence from coin tossing. Heads will be signified as $0$ and tails as $1$ Let's define $R$ as a successive elements(in the given sequence) of the same value. for example we have the sequence: $0,0,1,0,0,0,0,1,0,1,1,0$ In the given sequence there are total of $7$ different $R$'s My question is simple: Let's say we have infinite sequence - what is the average length of $R$ (length = how many elements are in $R$). in the above case is $12/7$ equivalent question would be how may times there is element switch in the sequence(also = how many different $R$'s - 1)'s.for example in a sequence in the length of $100$ , if there are $24$ elements switch that means we have $25$ $R$'s - so the average length is $4$. I would be also interested in the entire distribution of $R$'s length. probably I am short of the appropriate terminology to get answer in the internet.","Let's say we create a sequence from coin tossing. Heads will be signified as $0$ and tails as $1$ Let's define $R$ as a successive elements(in the given sequence) of the same value. for example we have the sequence: $0,0,1,0,0,0,0,1,0,1,1,0$ In the given sequence there are total of $7$ different $R$'s My question is simple: Let's say we have infinite sequence - what is the average length of $R$ (length = how many elements are in $R$). in the above case is $12/7$ equivalent question would be how may times there is element switch in the sequence(also = how many different $R$'s - 1)'s.for example in a sequence in the length of $100$ , if there are $24$ elements switch that means we have $25$ $R$'s - so the average length is $4$. I would be also interested in the entire distribution of $R$'s length. probably I am short of the appropriate terminology to get answer in the internet.",,"['statistics', 'random-walk']"
87,"At a hospital's emergency room, patients are classified and 20% of them are critical, 30% are serious, and 50% are stable.","At a hospital's emergency room, patients are classified and 20% of them are critical, 30% are serious, and 50% are stable.",,"At a hospital's emergency room, patients are classified and 20% of them are critical, 30% are serious, and 50% are stable.  Of the critical ones, 30% die; of the serious, 10% die; and of the stable, 1% die.  Given that a patient dies, what is the conditional probability that the patient was classified as critical? Wouldn't it be P(Crit.| Die) = (.2 x .3 / .3) ? I don't know what i'm doing wrong. The answer is 0.632","At a hospital's emergency room, patients are classified and 20% of them are critical, 30% are serious, and 50% are stable.  Of the critical ones, 30% die; of the serious, 10% die; and of the stable, 1% die.  Given that a patient dies, what is the conditional probability that the patient was classified as critical? Wouldn't it be P(Crit.| Die) = (.2 x .3 / .3) ? I don't know what i'm doing wrong. The answer is 0.632",,"['probability', 'statistics']"
88,Product Integral,Product Integral,,"What is the product integral of $(1+x)^{-(\theta+1)/\theta}$, if we consider that the product integral is from x=0 to x=n? It's easy to solve 1/theta, however, the second part is a little more complicated. In my opinion, the solution is $(n!)^{-(\theta+1)/\theta}$. Sorry for my English, I know that's horrible. And also my knowledge about greek symbols.","What is the product integral of $(1+x)^{-(\theta+1)/\theta}$, if we consider that the product integral is from x=0 to x=n? It's easy to solve 1/theta, however, the second part is a little more complicated. In my opinion, the solution is $(n!)^{-(\theta+1)/\theta}$. Sorry for my English, I know that's horrible. And also my knowledge about greek symbols.",,"['statistics', 'products']"
89,Using the inverse Gaussian integral to find percentiles,Using the inverse Gaussian integral to find percentiles,,"I need some help with the following: Let $$R=\mu+\sigma*\epsilon \hspace{1cm} \epsilon \sim N(0,1)$$ I want to argue that $$ \mu + \sigma*\Phi^{-1}(u)$$ are the percentiles of the model when $\Phi^{-1}(u)$ is the inverse Gaussial integral. How can I do that? Thanx!","I need some help with the following: Let $$R=\mu+\sigma*\epsilon \hspace{1cm} \epsilon \sim N(0,1)$$ I want to argue that $$ \mu + \sigma*\Phi^{-1}(u)$$ are the percentiles of the model when $\Phi^{-1}(u)$ is the inverse Gaussial integral. How can I do that? Thanx!",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
90,Probability of 2 sets of triples in a 6 card hand,Probability of 2 sets of triples in a 6 card hand,,"The deck is a standard 52 card deck. My solution was: The first card drawn can be anything, so 52 possible cards. The next card has to be the same value, so there are 3 possible cards. The last card can only be from the 2 remaining cards. The next set of triples can start from one of 48 cards since the last of the first value is dead to the problem, then once again there are 3 then 2 possibilities. I believe that covers all possible hands with double triples. The number of possible hands is 52 choose 6, so 52!/(6!46!). Simplified, that should make the probability equal: (52x48x2x2x3x3)(6!46!)/52! I can't help but feel like I messed something up. Do I need to square the 6! to account for the orders of the hand, something else, or is my answer correct?","The deck is a standard 52 card deck. My solution was: The first card drawn can be anything, so 52 possible cards. The next card has to be the same value, so there are 3 possible cards. The last card can only be from the 2 remaining cards. The next set of triples can start from one of 48 cards since the last of the first value is dead to the problem, then once again there are 3 then 2 possibilities. I believe that covers all possible hands with double triples. The number of possible hands is 52 choose 6, so 52!/(6!46!). Simplified, that should make the probability equal: (52x48x2x2x3x3)(6!46!)/52! I can't help but feel like I messed something up. Do I need to square the 6! to account for the orders of the hand, something else, or is my answer correct?",,"['probability', 'statistics']"
91,Find the median of a function of a normal random variable.,Find the median of a function of a normal random variable.,,"If $X\sim N(\mu,\sigma^2)$ and $Y=e^X$, then what is the median of $Y$? I am pretty sure that $Y$ is also distributed normal.  To try to prove it, I attempted both the method of moment generating functions and the method of cdfs.  I just can't get it.  Thanks for your help.   Once I show that, getting the median is the same as getting the mean.","If $X\sim N(\mu,\sigma^2)$ and $Y=e^X$, then what is the median of $Y$? I am pretty sure that $Y$ is also distributed normal.  To try to prove it, I attempted both the method of moment generating functions and the method of cdfs.  I just can't get it.  Thanks for your help.   Once I show that, getting the median is the same as getting the mean.",,"['probability', 'statistics', 'moment-generating-functions']"
92,"How to calculate t-value, given degrees of freedom and $\alpha$.","How to calculate t-value, given degrees of freedom and .",\alpha,"While solving problems, we can look up physical t-tables or use a statistical analysis software like R to calculate t-values. But how do we actually calculate these values ?  What is the algorithm behind it ? How does the widget in this website ( http://www.danielsoper.com/statcalc3/calc.aspx?id=10 ) calculates it ?","While solving problems, we can look up physical t-tables or use a statistical analysis software like R to calculate t-values. But how do we actually calculate these values ?  What is the algorithm behind it ? How does the widget in this website ( http://www.danielsoper.com/statcalc3/calc.aspx?id=10 ) calculates it ?",,"['probability', 'statistics']"
93,"When plotting a bell curve from an array of values, is it possible that +/- 2 standard deviations from the mean can fall outside the range of values?","When plotting a bell curve from an array of values, is it possible that +/- 2 standard deviations from the mean can fall outside the range of values?",,"First, this is not homework, it is actually for work. It's been a couple of years since I've done stats and need some help! I've googled for this problem but was unavailable to find any resources that could help answer my question. I have 25 values: 11.5 11.6 11.9 12.2 12.4 12.4 12.5 12.5 12.5 12.8 12.8 12.9 13.1 13.3 13.5 13.5 13.7 13.7 13.8 13.9 13.9 14 14.3 14.5 15 From here, I calculate the mean and from that, the variance and then the standard deviation : The variance formula and my variance calculations: $$ \sigma^{2} =\frac{\sum_{i=1}^{n}(x_{i}-\mu )^{2}}{n}=\frac{\sum_{i=1}^{25}(x_{i}-13.128)^{2}}{25}=0.7996159999999999 $$ Of course, standard deviation is simply the square root of variance : $$ \sigma =\sqrt{0.7996159999999999}=0.8942125027083886 $$ Here's where I feel like I'm messing up: One standard deviation less than the mean: $$ -\sigma + \mu = -0.8942125027083886 + 13.128 = 12.2337874972916114 $$ Two standard deviations less than the mean: $$ -2\sigma + \mu = -2*0.8942125027083886 + 13.128 = 11.3395749945832228 $$ This value, 11.3395749945832228 , falls below the smallest value in the array, 11.5 . How is this possible? Where am I messing up my calculations? Thank you for any and all help! I really appreciate it.","First, this is not homework, it is actually for work. It's been a couple of years since I've done stats and need some help! I've googled for this problem but was unavailable to find any resources that could help answer my question. I have 25 values: 11.5 11.6 11.9 12.2 12.4 12.4 12.5 12.5 12.5 12.8 12.8 12.9 13.1 13.3 13.5 13.5 13.7 13.7 13.8 13.9 13.9 14 14.3 14.5 15 From here, I calculate the mean and from that, the variance and then the standard deviation : The variance formula and my variance calculations: $$ \sigma^{2} =\frac{\sum_{i=1}^{n}(x_{i}-\mu )^{2}}{n}=\frac{\sum_{i=1}^{25}(x_{i}-13.128)^{2}}{25}=0.7996159999999999 $$ Of course, standard deviation is simply the square root of variance : $$ \sigma =\sqrt{0.7996159999999999}=0.8942125027083886 $$ Here's where I feel like I'm messing up: One standard deviation less than the mean: $$ -\sigma + \mu = -0.8942125027083886 + 13.128 = 12.2337874972916114 $$ Two standard deviations less than the mean: $$ -2\sigma + \mu = -2*0.8942125027083886 + 13.128 = 11.3395749945832228 $$ This value, 11.3395749945832228 , falls below the smallest value in the array, 11.5 . How is this possible? Where am I messing up my calculations? Thank you for any and all help! I really appreciate it.",,"['statistics', 'standard-deviation']"
94,Expected length of a random vector,Expected length of a random vector,,"I meet a basic definition about the expected length of a random vector when reading  a paper: What is ""expected length"" How to roughly derive both equations (yellow part)  (Is that Gamma function?) Thanks","I meet a basic definition about the expected length of a random vector when reading  a paper: What is ""expected length"" How to roughly derive both equations (yellow part)  (Is that Gamma function?) Thanks",,"['integration', 'statistics', 'random-variables']"
95,Maximum likelihood estimators,Maximum likelihood estimators,,"I have $X_1,X_2,\dots,X_n$ as random samples from a binomial distribution, with probability function: $$p_X(x) = Pr(X=x) = {m \choose{n}}\alpha^x(1-\alpha)^{m-x},x=0,1,2,\dots,m$$ where $m$ is given and $\alpha \in (0,1)$ is an unknown parameter. I want to show that the maximum likelihood estimator of $\alpha$ is given by the sample mean over $m$. Now I imagine the first thing I must do is, find the maximum likelihood estimator. I believe this is found by finding the joint distribution for my data, and equating the first derivative to be $0$, is this the right direction?","I have $X_1,X_2,\dots,X_n$ as random samples from a binomial distribution, with probability function: $$p_X(x) = Pr(X=x) = {m \choose{n}}\alpha^x(1-\alpha)^{m-x},x=0,1,2,\dots,m$$ where $m$ is given and $\alpha \in (0,1)$ is an unknown parameter. I want to show that the maximum likelihood estimator of $\alpha$ is given by the sample mean over $m$. Now I imagine the first thing I must do is, find the maximum likelihood estimator. I believe this is found by finding the joint distribution for my data, and equating the first derivative to be $0$, is this the right direction?",,"['probability', 'statistics', 'approximation', 'parameter-estimation']"
96,Uniqueness of solution to quantile minimization problem,Uniqueness of solution to quantile minimization problem,,"I read here: http://librarum.org/book/11685/31 (p. 51, Ex. 3) that quantiles are solutions to certain minimization problem. Here is the proof: http://www.math.ucla.edu/~tom/MathematicalStatistics/Sec18.pdf . It isn't stated explicitely that the problem can't have other solutions than those described. So I was wondering whether this is true and how to proove it. I can see it when $P(Z=b) = 0$.","I read here: http://librarum.org/book/11685/31 (p. 51, Ex. 3) that quantiles are solutions to certain minimization problem. Here is the proof: http://www.math.ucla.edu/~tom/MathematicalStatistics/Sec18.pdf . It isn't stated explicitely that the problem can't have other solutions than those described. So I was wondering whether this is true and how to proove it. I can see it when $P(Z=b) = 0$.",,"['statistics', 'optimization', 'quantile']"
97,What's the probability of third coin being of 50 cent?,What's the probability of third coin being of 50 cent?,,"I was asked this question in an interview for Data Scientist position: I have 1 coin of say 10p,20p and 50p each in my pocket. I then draw out of my pocket the coin of 10p. So now I'm left with 1 coin of 20p and 50p each. Call this the 'FIRST' attempt. What is probability of getting the 50p coin in the 'THIRD' attempt? When I said the answer is 50%, the interview told me it's a wrong answer. Is there some hidden logic to it that I'm not able to pick? EDIT: When I mentioned 'Third' attempt, the 10p coin is not placed back in the pocket.","I was asked this question in an interview for Data Scientist position: I have 1 coin of say 10p,20p and 50p each in my pocket. I then draw out of my pocket the coin of 10p. So now I'm left with 1 coin of 20p and 50p each. Call this the 'FIRST' attempt. What is probability of getting the 50p coin in the 'THIRD' attempt? When I said the answer is 50%, the interview told me it's a wrong answer. Is there some hidden logic to it that I'm not able to pick? EDIT: When I mentioned 'Third' attempt, the 10p coin is not placed back in the pocket.",,"['probability', 'statistics', 'conditional-probability']"
98,Finding the median of the largest gap of a sequence of numbers in a set/vector,Finding the median of the largest gap of a sequence of numbers in a set/vector,,"Im working on a programming function, but I cant seem to get this equation right. Im hoping someone can help point me in the right direction here. Say you have a set of numbers: 1, 10, 15, 25 All numbers are in the range 1-30, so 31 would be 1, 0 would be 30. I am trying to find the best spot to insert a unique number (1-30) in the best location to keep the numbers spaced best. For example, if the set of numbers is 15, 18. Id like the next number to be rounded down and inserted in the middle position of the biggest space. So in this case it would be 7. The set would be 7, 15, 18 The sets of numbers can be really random, meaning different sizes, and different numbers, but they are always 1-30. Always positive integers. Examples:  1,2,5,10,28 3,4,8,9,12 Any help would be appreciated.","Im working on a programming function, but I cant seem to get this equation right. Im hoping someone can help point me in the right direction here. Say you have a set of numbers: 1, 10, 15, 25 All numbers are in the range 1-30, so 31 would be 1, 0 would be 30. I am trying to find the best spot to insert a unique number (1-30) in the best location to keep the numbers spaced best. For example, if the set of numbers is 15, 18. Id like the next number to be rounded down and inserted in the middle position of the biggest space. So in this case it would be 7. The set would be 7, 15, 18 The sets of numbers can be really random, meaning different sizes, and different numbers, but they are always 1-30. Always positive integers. Examples:  1,2,5,10,28 3,4,8,9,12 Any help would be appreciated.",,"['statistics', 'median']"
99,Why is this expectation true?,Why is this expectation true?,,"Working with Rao-Blackwell, this came up: $$E[2X_1 \mid \max(X_i) = t] = 2\left(\frac{1}{n}t + \frac{n-1}{n}\frac t 2\right)$$ Where X are uniform(0, $\theta$). What are the intermediate steps? I'm not sure how to use the information in $max(x_i)$ here. Thanks.","Working with Rao-Blackwell, this came up: $$E[2X_1 \mid \max(X_i) = t] = 2\left(\frac{1}{n}t + \frac{n-1}{n}\frac t 2\right)$$ Where X are uniform(0, $\theta$). What are the intermediate steps? I'm not sure how to use the information in $max(x_i)$ here. Thanks.",,"['probability', 'statistics']"
