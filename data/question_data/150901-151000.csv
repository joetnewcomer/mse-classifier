,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $\{f_n\}$ has a uniformly convergent subsequence.,Show that  has a uniformly convergent subsequence.,\{f_n\},"Let the function $f_n$ : $[0,1] \rightarrow [0,1]$ satisfy   \begin{align*} \vert f_n(x)-f_n(y)\vert \leq \vert x-y \vert \textrm{ whenever } \vert x-y \vert \geq \frac{1}{n} \end{align*}    Show that the sequence $\{f_n\}$ has a uniformly convergent subsequence. I try to use the Arzela-Ascoli Theorem. Thus, I check to $\{f_n\}$ is uniformly bounded and equicontinuous. But, I don't know how to prove that when $\vert x - y \vert <\frac{1}{n}$ Any help is appreicated... Thank you!","Let the function $f_n$ : $[0,1] \rightarrow [0,1]$ satisfy   \begin{align*} \vert f_n(x)-f_n(y)\vert \leq \vert x-y \vert \textrm{ whenever } \vert x-y \vert \geq \frac{1}{n} \end{align*}    Show that the sequence $\{f_n\}$ has a uniformly convergent subsequence. I try to use the Arzela-Ascoli Theorem. Thus, I check to $\{f_n\}$ is uniformly bounded and equicontinuous. But, I don't know how to prove that when $\vert x - y \vert <\frac{1}{n}$ Any help is appreicated... Thank you!",,['analysis']
1,Prove $f:\mathbb{R^2}\rightarrow \mathbb{R}$ not injective by Inverse Function Theorem,Prove  not injective by Inverse Function Theorem,f:\mathbb{R^2}\rightarrow \mathbb{R},"I'm proving that a $C^r$ function $f:\mathbb{R^2}\rightarrow \mathbb{R}$ is not injective. I've seen various solutions but I want to prove it using techniques related to the Inverse function theorem; because it's a problem from Spivaks ""Callculus on Manifolds"" text in the chapter on the Inverse function theorem. I've found a function $g:\mathbb{R^2}\rightarrow \mathbb{R^2}$ defined as $g(x,y)=\bigg(f(x,y),y\bigg)$, and proven that it's $C^r$.  To prove my main problem, I'm assuming $f$ is injective, and that allows me to apply the inverse function theorem to $g$, in such a way that gives me an open map from an open set in the domain of $g$ to the open image of $g$, such that $g$ has a differentiable inverse $g^{-1}:g(A)\rightarrow A$. Now I want to prove a contradiction in the injectivity of $f$, by showing that $g^{-1}$ is not defined at certain points, specifically a line, and this contradicts the differentiability of $g^{-1}$.  Consider $f(x,y)=b$ then $g(x,y)=(b,y)$, Now I should be able to find that the line that $g^{-1}$ isn't defined on $\forall (b,z)$, $z\neq y$, since that would imply that   $f(x,z)=b$. My Question I don't quite see how $g^{-1}$ being defined on $(b,z)$ implies that $f(x,z)=b$? $\phantom{}$ Here is the original problem.  I've seen other solutions suggest to make $g$ satisfy the conditions of $f$ from $2$-$36$, which is why I left it in. Thanks!","I'm proving that a $C^r$ function $f:\mathbb{R^2}\rightarrow \mathbb{R}$ is not injective. I've seen various solutions but I want to prove it using techniques related to the Inverse function theorem; because it's a problem from Spivaks ""Callculus on Manifolds"" text in the chapter on the Inverse function theorem. I've found a function $g:\mathbb{R^2}\rightarrow \mathbb{R^2}$ defined as $g(x,y)=\bigg(f(x,y),y\bigg)$, and proven that it's $C^r$.  To prove my main problem, I'm assuming $f$ is injective, and that allows me to apply the inverse function theorem to $g$, in such a way that gives me an open map from an open set in the domain of $g$ to the open image of $g$, such that $g$ has a differentiable inverse $g^{-1}:g(A)\rightarrow A$. Now I want to prove a contradiction in the injectivity of $f$, by showing that $g^{-1}$ is not defined at certain points, specifically a line, and this contradicts the differentiability of $g^{-1}$.  Consider $f(x,y)=b$ then $g(x,y)=(b,y)$, Now I should be able to find that the line that $g^{-1}$ isn't defined on $\forall (b,z)$, $z\neq y$, since that would imply that   $f(x,z)=b$. My Question I don't quite see how $g^{-1}$ being defined on $(b,z)$ implies that $f(x,z)=b$? $\phantom{}$ Here is the original problem.  I've seen other solutions suggest to make $g$ satisfy the conditions of $f$ from $2$-$36$, which is why I left it in. Thanks!",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'proof-writing']"
2,Integrating over integral limits,Integrating over integral limits,,"What does $$\int_{-\infty}^{\infty} \Big( \int_{x-1/2}^{x+1/2}f(t)dt \Big) dx$$ mean? Is it the same as $$\int_{-\infty}^{\infty} f(x) dx$$ or do the boundaries ""overlap""? Are there any good theorems to use here to access the limits of the integral? Is it possible to somehow change the order of integration? I am trying to figure out this interval for a function $f$ with $$\int_{-\infty}^{\infty} f(x) dx=1$$ and I suspect that $$\int_{-\infty}^{\infty} \Big( \int_{x-1/2}^{x+1/2}f(t)dt \Big) dx=1$$","What does $$\int_{-\infty}^{\infty} \Big( \int_{x-1/2}^{x+1/2}f(t)dt \Big) dx$$ mean? Is it the same as $$\int_{-\infty}^{\infty} f(x) dx$$ or do the boundaries ""overlap""? Are there any good theorems to use here to access the limits of the integral? Is it possible to somehow change the order of integration? I am trying to figure out this interval for a function $f$ with $$\int_{-\infty}^{\infty} f(x) dx=1$$ and I suspect that $$\int_{-\infty}^{\infty} \Big( \int_{x-1/2}^{x+1/2}f(t)dt \Big) dx=1$$",,"['integration', 'analysis']"
3,Give an example where $\int_{\bar A} f$ exists but $\int_A f$ does not for a continuos $f$ on a bounded open subset $A$ of $\mathbb{R}^n $,Give an example where  exists but  does not for a continuos  on a bounded open subset  of,\int_{\bar A} f \int_A f f A \mathbb{R}^n ,"In the book of Analysis on Manifolds by Munkress, at page 121, it is asked that Let $A$ be a bounded open set in $\mathbb{R}^n $; let $f: \mathbb{R}^n > \to \mathbb{R} $ be a bounded continuous function. Give an example   where $\int_{\bar A} f$ exists but $\int_A f$ does not. I couldn't find any such example. I mean I do noticed that $A$ should not be rectifiable, and the only way I can think of that $f$ is not integrable on $A$ is that the set $E$ where the limit  $$\lim_{x \to x_0} f(x) = 0$$ fails for $x_0 \in \partial A$ has a non-zero measure, but then this set $E$ also contained in $\bar A$, so in such a case $\int_{\bar A} f$ also wouldn't exists. In short, I'm looking for such an example. Note: I have read this answer, but I have no idea what the answerer is talking about, so I wanted to ask a new question including my own thoughts.","In the book of Analysis on Manifolds by Munkress, at page 121, it is asked that Let $A$ be a bounded open set in $\mathbb{R}^n $; let $f: \mathbb{R}^n > \to \mathbb{R} $ be a bounded continuous function. Give an example   where $\int_{\bar A} f$ exists but $\int_A f$ does not. I couldn't find any such example. I mean I do noticed that $A$ should not be rectifiable, and the only way I can think of that $f$ is not integrable on $A$ is that the set $E$ where the limit  $$\lim_{x \to x_0} f(x) = 0$$ fails for $x_0 \in \partial A$ has a non-zero measure, but then this set $E$ also contained in $\bar A$, so in such a case $\int_{\bar A} f$ also wouldn't exists. In short, I'm looking for such an example. Note: I have read this answer, but I have no idea what the answerer is talking about, so I wanted to ask a new question including my own thoughts.",,"['real-analysis', 'integration', 'analysis', 'riemann-integration']"
4,Integration on a embedded hypersurface,Integration on a embedded hypersurface,,"In this note Analysis Tools with Examples On page 285, it defines the integration on an embedded hypersurface as follows: The idea is straightforward: Chop a hypersurface into pieces that are parametrized. However, I am a little bit lost with the technical details here. It seems to me that the right way to do it is the following: 1).Find a finite cover of the support of $f$, say$\{U_i\}$ 2).Produce a partition of unity subordinate to this cover 3).My guess it that those $U_i$ will serve as the desired parametrized pieces. However, how are them parametrized according to the note's definition?(page 284). Perhaps I can choose $U_i$ to be the inverse image of open balls in $\mathbb R^n$ by charts of $M$, then I will have a natural parametrization of them.","In this note Analysis Tools with Examples On page 285, it defines the integration on an embedded hypersurface as follows: The idea is straightforward: Chop a hypersurface into pieces that are parametrized. However, I am a little bit lost with the technical details here. It seems to me that the right way to do it is the following: 1).Find a finite cover of the support of $f$, say$\{U_i\}$ 2).Produce a partition of unity subordinate to this cover 3).My guess it that those $U_i$ will serve as the desired parametrized pieces. However, how are them parametrized according to the note's definition?(page 284). Perhaps I can choose $U_i$ to be the inverse image of open balls in $\mathbb R^n$ by charts of $M$, then I will have a natural parametrization of them.",,"['integration', 'analysis', 'manifolds', 'differential-topology']"
5,Arzela Ascoli Theorem for compactness of a set,Arzela Ascoli Theorem for compactness of a set,,"Define a metric space  $C(K)=\left \{ f: K\rightarrow \mathbb{R}  \right\} $ , where $f$ is continuous function on $K.$ Its metric is given by $d(f,g)=\left \| f-g \right \|_u = \sup\left \{ \left | f(x)-g(x) \right | :x\in K\right \}$. Let $B=\left \{ f\in C([0,1]):f\in C^1 ((0,1)),f(0)=0,\left | f'(x) \right | \leq 1\right \}$. Show that $B$ is compact subset of $C([0,1])$. (My attempt) I tried to apply Arzela-Ascoli theorem. If I show $B$ is closed,pointwise bounded and equicontinuous on $[0,1]$, then $B$ is compact. I used Mean Value Theorem to show that $B$ is uniformly bounded and equicontinuous on $[0,1]$. But showing the closeness of $B$ was hard for me. Let $\left \| f_n-f \right \|_u\rightarrow 0$ for $f_{n}\in B$. Then, $\left \| f \right \|_u\leq  \left \| f-f_n \right \|_u+\left \| f_n \right \|_u \leq 1+\epsilon $ for arbitary $\epsilon > 0$. But, I don't know how to connect with  $\left | f'(x) \right |\leq 1$ and $f(0)=0$.","Define a metric space  $C(K)=\left \{ f: K\rightarrow \mathbb{R}  \right\} $ , where $f$ is continuous function on $K.$ Its metric is given by $d(f,g)=\left \| f-g \right \|_u = \sup\left \{ \left | f(x)-g(x) \right | :x\in K\right \}$. Let $B=\left \{ f\in C([0,1]):f\in C^1 ((0,1)),f(0)=0,\left | f'(x) \right | \leq 1\right \}$. Show that $B$ is compact subset of $C([0,1])$. (My attempt) I tried to apply Arzela-Ascoli theorem. If I show $B$ is closed,pointwise bounded and equicontinuous on $[0,1]$, then $B$ is compact. I used Mean Value Theorem to show that $B$ is uniformly bounded and equicontinuous on $[0,1]$. But showing the closeness of $B$ was hard for me. Let $\left \| f_n-f \right \|_u\rightarrow 0$ for $f_{n}\in B$. Then, $\left \| f \right \|_u\leq  \left \| f-f_n \right \|_u+\left \| f_n \right \|_u \leq 1+\epsilon $ for arbitary $\epsilon > 0$. But, I don't know how to connect with  $\left | f'(x) \right |\leq 1$ and $f(0)=0$.",,"['real-analysis', 'functional-analysis', 'analysis']"
6,"If a function is log-lipschitz, then it is $\alpha$-Hölder for all 0<$\alpha$<1 and isn't Lipschitz continuous","If a function is log-lipschitz, then it is -Hölder for all 0<<1 and isn't Lipschitz continuous",\alpha \alpha,"Consider $C^{0,\alpha}$ the class of $\alpha$-Hölder continuous functions. Let $\Omega \subset \mathbb{R}$ be a bounded subset and let $u: \Omega \rightarrow \mathbb{R}$ a continuous function. Prove that if there is $C\in \mathbb{R}$ such that      $$|u(x)-u(y)| \leq C \left|x-y\right| \ln {\frac{1}{\left|x-y\right|}}$$ For $|x-y|<\frac{1}{2}$, then $u\in C^{0,\alpha}$, i.e., $\forall \alpha \in (0,1)\exists K_{\alpha}\in \mathbb{R}$ such that     $$|u(x)-u(y)| \leq K_{\alpha} \left|x-y\right|^{\alpha}$$ My doubt: First: How I prove that $K$ depends on $\alpha$? Second: I think that we can use the subbaditive property of $ln$ function. But I can't do this Third: How I prove that it isn't Lipschitz continuous? Forth: why is the supremum of the distance $1/2$? Can I improve the distance, i.e. $\exists d>1/2$ such that the inequality is true for $1/2<|x-y|<d$? Or, can I suppose that this inequality are well-defined at $|x-y|<1$? This definition I found in this link , page 80.","Consider $C^{0,\alpha}$ the class of $\alpha$-Hölder continuous functions. Let $\Omega \subset \mathbb{R}$ be a bounded subset and let $u: \Omega \rightarrow \mathbb{R}$ a continuous function. Prove that if there is $C\in \mathbb{R}$ such that      $$|u(x)-u(y)| \leq C \left|x-y\right| \ln {\frac{1}{\left|x-y\right|}}$$ For $|x-y|<\frac{1}{2}$, then $u\in C^{0,\alpha}$, i.e., $\forall \alpha \in (0,1)\exists K_{\alpha}\in \mathbb{R}$ such that     $$|u(x)-u(y)| \leq K_{\alpha} \left|x-y\right|^{\alpha}$$ My doubt: First: How I prove that $K$ depends on $\alpha$? Second: I think that we can use the subbaditive property of $ln$ function. But I can't do this Third: How I prove that it isn't Lipschitz continuous? Forth: why is the supremum of the distance $1/2$? Can I improve the distance, i.e. $\exists d>1/2$ such that the inequality is true for $1/2<|x-y|<d$? Or, can I suppose that this inequality are well-defined at $|x-y|<1$? This definition I found in this link , page 80.",,"['analysis', 'continuity', 'lipschitz-functions', 'holder-spaces']"
7,"Let $\gamma\in C(I,\Bbb R^n)$. Show that $\dim_H(\Gamma)=1$",Let . Show that,"\gamma\in C(I,\Bbb R^n) \dim_H(\Gamma)=1","Let $I:=[a,b]$ a perfect interval and $\gamma\in C(I,\Bbb R^n)$ an injective path such that $\Gamma:=\gamma(I)$ is rectifiable. Show that $\dim_H(\Gamma)=1$. Here $\dim_H$ is the Hausdorff dimension. My work so far: Note that the canonical projections $\pi_k$ are Lipschitz, and because $\gamma$ is continuous and it domain is compact and connected then $\Gamma$ is also compact and connected, thus $\pi_k(\Gamma)\subset\Bbb R$ is compact and connected. Because $I$ is perfect and $\gamma$ injective then $\Gamma$ is not a singleton, so there is some $k\in\{1,\ldots,n\}$ such that $\pi_k(\Gamma)$ is a perfect closed interval, thus setting $$ f:\Gamma\to\Bbb R^n,\, x\mapsto (\pi_k(x),0,\ldots,0)\tag1 $$ we can see that $f$ is also Lipschitz and we find that $\dim_H(\pi_k(\Gamma))=\dim_H(f(\Gamma))=1\le\dim_H(\Gamma)$ by some elementary identities of the Hausdorff outer measures. However Im unable to find a way to show that $\dim_H(\Gamma)\le 1$. I dont have a clue about how to do it. Some random ideas that I had: I tried to relate that $\gamma$ have a continuous inverse in $\Gamma$, or some uniform polynomial approximation to $\Gamma$, or the fact that $\Gamma$ is rectifiable and compact with the definition of Hausdorff outer measure, but I dont found something. Some help will be appreciated, thank you.","Let $I:=[a,b]$ a perfect interval and $\gamma\in C(I,\Bbb R^n)$ an injective path such that $\Gamma:=\gamma(I)$ is rectifiable. Show that $\dim_H(\Gamma)=1$. Here $\dim_H$ is the Hausdorff dimension. My work so far: Note that the canonical projections $\pi_k$ are Lipschitz, and because $\gamma$ is continuous and it domain is compact and connected then $\Gamma$ is also compact and connected, thus $\pi_k(\Gamma)\subset\Bbb R$ is compact and connected. Because $I$ is perfect and $\gamma$ injective then $\Gamma$ is not a singleton, so there is some $k\in\{1,\ldots,n\}$ such that $\pi_k(\Gamma)$ is a perfect closed interval, thus setting $$ f:\Gamma\to\Bbb R^n,\, x\mapsto (\pi_k(x),0,\ldots,0)\tag1 $$ we can see that $f$ is also Lipschitz and we find that $\dim_H(\pi_k(\Gamma))=\dim_H(f(\Gamma))=1\le\dim_H(\Gamma)$ by some elementary identities of the Hausdorff outer measures. However Im unable to find a way to show that $\dim_H(\Gamma)\le 1$. I dont have a clue about how to do it. Some random ideas that I had: I tried to relate that $\gamma$ have a continuous inverse in $\Gamma$, or some uniform polynomial approximation to $\Gamma$, or the fact that $\Gamma$ is rectifiable and compact with the definition of Hausdorff outer measure, but I dont found something. Some help will be appreciated, thank you.",,"['real-analysis', 'analysis', 'measure-theory', 'hausdorff-measure']"
8,Must the dimensions of $V$ and $U$ be equal for $f: U \mapsto V$ to be a diffeomorphism?,Must the dimensions of  and  be equal for  to be a diffeomorphism?,V U f: U \mapsto V,"Suppose we have $U, V \subset \mathbb{R}^{n}$, and a map $f: U \mapsto V$. If $f$ is a diffeomorphism, must the dimensions of $U$ and $V$ be equal? I'm thinking this is true since the tangent map $Df_{x}: T_{x}U \mapsto T_{f(x)}V$ has to be a linear isomorphism if $f$ is a diffeomorphism.","Suppose we have $U, V \subset \mathbb{R}^{n}$, and a map $f: U \mapsto V$. If $f$ is a diffeomorphism, must the dimensions of $U$ and $V$ be equal? I'm thinking this is true since the tangent map $Df_{x}: T_{x}U \mapsto T_{f(x)}V$ has to be a linear isomorphism if $f$ is a diffeomorphism.",,"['analysis', 'multivariable-calculus', 'differential-geometry']"
9,properties of the multiplication operator on $L^2$,properties of the multiplication operator on,L^2,"Let $H=L^2([0,1],\lambda)$, where $\lambda$ is the Lebesgue measure on $[0,1]$.  Let $f\in C([0,1])$ and consider the multiplication operator $M_f\colon H\to H$, $M_f(g)=fg$. Let $f,g\in C([0,1])$ such that $M_f=M_g$. I want to show that $f=g$. It is: $M_f=M_g$ $\iff$ $fh=gh$ for  all $h\in H$ $\iff$ (f-g)h=0 for all $h\in H$ $\iff$ $M_{f-g}(h)=0$ for all $h\in H$. How to proceed without knowing that $M$ is isometric? Let $M_f$ be invertible as a linear bounded operator on $H$. I want to show that $f(x)\neq 0$ for all $x\in [0,1]$. I started as follows: Assume that there exists an $x\in [0,1]$ such that $f(x)=0$. For $n\in\mathbb{N}$ consider the nonempty set $S_n=\{y\in [0,1]: |f(y)|<\frac{1}{n} \}$, and thus the nonzero characteristic funtion $\chi_{S_n}\in H$ regarding $S_n$. It is $$\|M_f(\chi_{S_n})\|_H=\|f\chi_{S_n}\|_h\le ||f_{|S_n}\|_{\infty}\|\chi_{S_n}\|_H\le \frac{1}{n}\|\chi_{S_n}\|_H.$$ Can I conclude that $M_f$ is not invertible from here and if yes, how? Thank you","Let $H=L^2([0,1],\lambda)$, where $\lambda$ is the Lebesgue measure on $[0,1]$.  Let $f\in C([0,1])$ and consider the multiplication operator $M_f\colon H\to H$, $M_f(g)=fg$. Let $f,g\in C([0,1])$ such that $M_f=M_g$. I want to show that $f=g$. It is: $M_f=M_g$ $\iff$ $fh=gh$ for  all $h\in H$ $\iff$ (f-g)h=0 for all $h\in H$ $\iff$ $M_{f-g}(h)=0$ for all $h\in H$. How to proceed without knowing that $M$ is isometric? Let $M_f$ be invertible as a linear bounded operator on $H$. I want to show that $f(x)\neq 0$ for all $x\in [0,1]$. I started as follows: Assume that there exists an $x\in [0,1]$ such that $f(x)=0$. For $n\in\mathbb{N}$ consider the nonempty set $S_n=\{y\in [0,1]: |f(y)|<\frac{1}{n} \}$, and thus the nonzero characteristic funtion $\chi_{S_n}\in H$ regarding $S_n$. It is $$\|M_f(\chi_{S_n})\|_H=\|f\chi_{S_n}\|_h\le ||f_{|S_n}\|_{\infty}\|\chi_{S_n}\|_H\le \frac{1}{n}\|\chi_{S_n}\|_H.$$ Can I conclude that $M_f$ is not invertible from here and if yes, how? Thank you",,['functional-analysis']
10,"Show that (c) $S=T^{-1}.$, (d)$\Vert S_{n}-T^{-1}\Vert\leq \frac{\lambda^{n+1}}{1-\lambda}.$, (e) $S_{n+1}=I+(I-T)S_{n}.$","Show that (c) , (d), (e)",S=T^{-1}. \Vert S_{n}-T^{-1}\Vert\leq \frac{\lambda^{n+1}}{1-\lambda}. S_{n+1}=I+(I-T)S_{n}.,"For the following question, I need just hint not a whole solutions . As you see I have solved the first two parts. The Problem : Let $X$ be a Banach space and $T : X \to X$ be linear with $\Vert I −T\Vert = \lambda < 1.$ Let $S_n=\sum\limits_{j=0}^{n}(I-T)^{j}.$ Show that (a) T is one to one. ( I have solved ) (b) The sequence $\{S_{n}\}_{n=0}^{\infty}$ is convergent in $\mathcal{B}(X).$ ( I have solved ) (c) Let $S=\lim\limits_{n\to \infty}S_{n}.$ Show that $S=T^{-1}.$ (d) $\Vert S_{n}-T^{-1}\Vert\leq \frac{\lambda^{n+1}}{1-\lambda}.$ (e) $S_{n+1}=I+(I-T)S_{n}.$","For the following question, I need just hint not a whole solutions . As you see I have solved the first two parts. The Problem : Let $X$ be a Banach space and $T : X \to X$ be linear with $\Vert I −T\Vert = \lambda < 1.$ Let $S_n=\sum\limits_{j=0}^{n}(I-T)^{j}.$ Show that (a) T is one to one. ( I have solved ) (b) The sequence $\{S_{n}\}_{n=0}^{\infty}$ is convergent in $\mathcal{B}(X).$ ( I have solved ) (c) Let $S=\lim\limits_{n\to \infty}S_{n}.$ Show that $S=T^{-1}.$ (d) $\Vert S_{n}-T^{-1}\Vert\leq \frac{\lambda^{n+1}}{1-\lambda}.$ (e) $S_{n+1}=I+(I-T)S_{n}.$",,"['real-analysis', 'functional-analysis', 'analysis', 'operator-theory', 'linear-transformations']"
11,The relation of $O$- and $\Omega$-symbols and an unexpected absolute sign in the definiton of $O$.,The relation of - and -symbols and an unexpected absolute sign in the definiton of .,O \Omega O,"In the article Big Omicron and the big omega and big theta D. Knuth defines on p.19 \begin{align*}  O(f(n)) & := \{ g(n) \mid \exists C > 0 \exists n_0 > 0 \forall n \ge n_0 : |g(n)| \le Cf(n) \} \\   \Omega(f(n) & := \{ g(n) \mid \exists C  0 \exists n_0 > 0 \forall n \ge n_0 : g(n) \ge Cf(n) \}. \end{align*} And the whole article centers around the idea to define $\Omega(f(n))$ as a lower bound notation contrary to $O(f(n))$. On wikipedia it is written $$  f(n) \in \Omega(g(n)) \Leftrightarrow g(n) \in O(f(n)). $$ But what puzzles me is the absolute sign in the definition of $O(f(n))$. By this the above symmetry does not hold? So why is there an absolute sign? And then shouldn't there be an absolute sign around $f(n)$ too? The author itself mentions on page 21: [...] Note that there is a slight lack of symmetry in the above definitions of $O$, $\Omega$, and $\Theta$, since absolute value signs are used in $g(n)$ only in the case of $O$. This is not really an anomaly, since $O$ refers to a neighborhood of zero while $\Omega$ refers to a neighborhood of infinity. [...] Guess it is related to my question, but I totally do not understand this paragraph, both symbols say something about the behaviour for very large arguments, hence both refer to a neighborhood of infinity?? So what does the author has in mind if he says $O$ refers to a neighborhood of zero? So I hope someone could clarify the relations. Why is there an absolute sign written in one definition, and not in the other? And why is there no absolute sign around both $g(n)$ and $f(n)$? And how does the equivalence from wikipedia holds? As I see it, it does not holds in general, just for positive functions? And what does the author tries to say with the above cited paragraph?","In the article Big Omicron and the big omega and big theta D. Knuth defines on p.19 \begin{align*}  O(f(n)) & := \{ g(n) \mid \exists C > 0 \exists n_0 > 0 \forall n \ge n_0 : |g(n)| \le Cf(n) \} \\   \Omega(f(n) & := \{ g(n) \mid \exists C  0 \exists n_0 > 0 \forall n \ge n_0 : g(n) \ge Cf(n) \}. \end{align*} And the whole article centers around the idea to define $\Omega(f(n))$ as a lower bound notation contrary to $O(f(n))$. On wikipedia it is written $$  f(n) \in \Omega(g(n)) \Leftrightarrow g(n) \in O(f(n)). $$ But what puzzles me is the absolute sign in the definition of $O(f(n))$. By this the above symmetry does not hold? So why is there an absolute sign? And then shouldn't there be an absolute sign around $f(n)$ too? The author itself mentions on page 21: [...] Note that there is a slight lack of symmetry in the above definitions of $O$, $\Omega$, and $\Theta$, since absolute value signs are used in $g(n)$ only in the case of $O$. This is not really an anomaly, since $O$ refers to a neighborhood of zero while $\Omega$ refers to a neighborhood of infinity. [...] Guess it is related to my question, but I totally do not understand this paragraph, both symbols say something about the behaviour for very large arguments, hence both refer to a neighborhood of infinity?? So what does the author has in mind if he says $O$ refers to a neighborhood of zero? So I hope someone could clarify the relations. Why is there an absolute sign written in one definition, and not in the other? And why is there no absolute sign around both $g(n)$ and $f(n)$? And how does the equivalence from wikipedia holds? As I see it, it does not holds in general, just for positive functions? And what does the author tries to say with the above cited paragraph?",,"['real-analysis', 'number-theory', 'analysis', 'asymptotics', 'computational-complexity']"
12,"Proving existence of unique fixed point in C([a,b]) satisfying integral equation","Proving existence of unique fixed point in C([a,b]) satisfying integral equation",,"The question is: Let $[a,b]$ be a closed interval in $\mathbb{R}$ and let $A$ and $K$ be continuous real-valued functions on $[a,b]$ and $\{(x,y) \in \mathbb{R}^2: a \leq y \leq x \leq b\}$ respectively. Prove that there is a unique $\phi \in C([a,b])$ such that $$\phi(x) = A(x) + \int_{a}^{x} K(x,y) \phi(y)dy$$ for all $x \in [a,b]$. I did a problem prior to this of a similar nature, which was: Let $[a,b]$ be a closed interval in $\mathbb{R}$ and let $A$ and $K$ be continuous real-valued functions on $[a,b]$ and $\{(x,y) \in \mathbb{R}^2: x,y \in [a,b]\}$ respectively. Assume $|(b-a)K(x,y)|<1$ for all $x,y$. Prove that there is a unique $\phi \in C([a,b])$ such that $$\phi(x) = A(x) + \int_{a}^{b} K(x,y) \phi(y)dy$$ for all $x \in [a,b]$. This can be done by showing $F: C([a,b]) \rightarrow C([a,b])$ defined by $F(\psi)(x) = A(x) + \int_{a}^{b} K(x,y) \psi(y)dy$ is a contraction map. However, without $|(b-a)K(x,y)|<1$, I'm unsure of how to proceed. The problem has the following hint: Imitate the procedure in the preceding problem if $|(b-a)K(x,y)|<1$ whenever $a \le y \le x \le b$. To do the general case, note that for any $a_1 \in (a,b)$, the problem reduces to proving the existence of a unique $\phi_1 \in C([a,a_1])$ such that  $$\phi_1(x) = A(x) + \int_{a}^{x} K(x,y) \phi_1(y)dy$$ for all $x \in [a,a_1]$ and the existence of a unique $\phi_2 \in C([a_1,b])$ such that  $$\phi_2(x) = A(x) + \int_{a}^{a_1} K(x,y) \phi_1(y)dy + \int_{a_1}^{x} K(x,y) \phi_2(y)dy$$ for all $x \in [a_1,b]$. I'm not sure what to make of the hint. Thank you.","The question is: Let $[a,b]$ be a closed interval in $\mathbb{R}$ and let $A$ and $K$ be continuous real-valued functions on $[a,b]$ and $\{(x,y) \in \mathbb{R}^2: a \leq y \leq x \leq b\}$ respectively. Prove that there is a unique $\phi \in C([a,b])$ such that $$\phi(x) = A(x) + \int_{a}^{x} K(x,y) \phi(y)dy$$ for all $x \in [a,b]$. I did a problem prior to this of a similar nature, which was: Let $[a,b]$ be a closed interval in $\mathbb{R}$ and let $A$ and $K$ be continuous real-valued functions on $[a,b]$ and $\{(x,y) \in \mathbb{R}^2: x,y \in [a,b]\}$ respectively. Assume $|(b-a)K(x,y)|<1$ for all $x,y$. Prove that there is a unique $\phi \in C([a,b])$ such that $$\phi(x) = A(x) + \int_{a}^{b} K(x,y) \phi(y)dy$$ for all $x \in [a,b]$. This can be done by showing $F: C([a,b]) \rightarrow C([a,b])$ defined by $F(\psi)(x) = A(x) + \int_{a}^{b} K(x,y) \psi(y)dy$ is a contraction map. However, without $|(b-a)K(x,y)|<1$, I'm unsure of how to proceed. The problem has the following hint: Imitate the procedure in the preceding problem if $|(b-a)K(x,y)|<1$ whenever $a \le y \le x \le b$. To do the general case, note that for any $a_1 \in (a,b)$, the problem reduces to proving the existence of a unique $\phi_1 \in C([a,a_1])$ such that  $$\phi_1(x) = A(x) + \int_{a}^{x} K(x,y) \phi_1(y)dy$$ for all $x \in [a,a_1]$ and the existence of a unique $\phi_2 \in C([a_1,b])$ such that  $$\phi_2(x) = A(x) + \int_{a}^{a_1} K(x,y) \phi_1(y)dy + \int_{a_1}^{x} K(x,y) \phi_2(y)dy$$ for all $x \in [a_1,b]$. I'm not sure what to make of the hint. Thank you.",,"['real-analysis', 'analysis']"
13,Proving $\int_{a}^{b}\vert P^{\prime}_{n}(x)\vert\text{d}x\leq 2n \max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert $,Proving,\int_{a}^{b}\vert P^{\prime}_{n}(x)\vert\text{d}x\leq 2n \max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert ,"So Assume that $P_{n}(x)$ is an algebraic polynomial of degree n. Prove that $$\int_{a}^{b}\vert P^{\prime}_{n}(x)\vert\;\text{d}x\leq 2n\cdot\max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert$$ Please use the method of variable limit integral I know A.A.Markoff$\;\;$Theorem ——The above conditions are the same $$\vert P^{\prime}_{n}(x)\vert\leq\dfrac{\displaystyle 2\max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert\cdot n^2}{b-a} $$ So when you integrate the inequality on both sides from a to b, you can get $$\int_{a}^{b}\vert P^{\prime}_{n}(x)\vert\;\text{d}x\leq 2n^2\cdot\max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert$$ That is so close to the final integral, but it's not So please help me,by the method of variable limit integral(That's the hint of the question) Thank you very much","So Assume that $P_{n}(x)$ is an algebraic polynomial of degree n. Prove that $$\int_{a}^{b}\vert P^{\prime}_{n}(x)\vert\;\text{d}x\leq 2n\cdot\max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert$$ Please use the method of variable limit integral I know A.A.Markoff$\;\;$Theorem ——The above conditions are the same $$\vert P^{\prime}_{n}(x)\vert\leq\dfrac{\displaystyle 2\max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert\cdot n^2}{b-a} $$ So when you integrate the inequality on both sides from a to b, you can get $$\int_{a}^{b}\vert P^{\prime}_{n}(x)\vert\;\text{d}x\leq 2n^2\cdot\max_{a\leqslant x\leqslant b}\vert P_{n}(x)\vert$$ That is so close to the final integral, but it's not So please help me,by the method of variable limit integral(That's the hint of the question) Thank you very much",,"['calculus', 'analysis']"
14,Elementary periodic point is a finite set in a compact manifold,Elementary periodic point is a finite set in a compact manifold,,"Let $f: M \to M$ to be a diffeomorphism where $M$ is a  finite dimensional compact manifold. We say that a fixed point $p$ (i.e., $f(p)=p$) is elementary if $1 \notin sp(Df_{p})$. Suppose that every fixed point of $M$ is a elementary fixed point. Claim: There exists only a finite number of elementary fixed points. I tried to show that the elementary points is discret set, but I am stuck in this. The statement is from Robinson's Dynamical systems. Thanks.","Let $f: M \to M$ to be a diffeomorphism where $M$ is a  finite dimensional compact manifold. We say that a fixed point $p$ (i.e., $f(p)=p$) is elementary if $1 \notin sp(Df_{p})$. Suppose that every fixed point of $M$ is a elementary fixed point. Claim: There exists only a finite number of elementary fixed points. I tried to show that the elementary points is discret set, but I am stuck in this. The statement is from Robinson's Dynamical systems. Thanks.",,"['analysis', 'differential-geometry', 'dynamical-systems']"
15,How does using chain rule in backprogation algorithm works?,How does using chain rule in backprogation algorithm works?,,"Let's have a simple error function $E(z) =\frac{1}{2} \times (a - y(z))^2$ How come $\frac{\partial E}{\partial z} = \frac{\mathrm dy}{\mathrm dz} \frac{\partial E}{\partial y}$ ? In other words does it not mean that $\frac{\partial E}{\partial z} =  \frac{\partial E}{\partial y(z)}$ ? If so, why? Since using chain rule $\frac{\partial E}{\partial y(z)} =  \frac{\mathrm dy}{\mathrm dz} \frac{\partial E}{\partial y}$ ? But does not seem to make sense since that would mean that $\frac{\mathrm dy}{\mathrm dz} = 1$ This formulas are coming from backpropagation algorithm for neural network. y is  sigmoid function.","Let's have a simple error function How come ? In other words does it not mean that ? If so, why? Since using chain rule ? But does not seem to make sense since that would mean that This formulas are coming from backpropagation algorithm for neural network. y is  sigmoid function.",E(z) =\frac{1}{2} \times (a - y(z))^2 \frac{\partial E}{\partial z} = \frac{\mathrm dy}{\mathrm dz} \frac{\partial E}{\partial y} \frac{\partial E}{\partial z} =  \frac{\partial E}{\partial y(z)} \frac{\partial E}{\partial y(z)} =  \frac{\mathrm dy}{\mathrm dz} \frac{\partial E}{\partial y} \frac{\mathrm dy}{\mathrm dz} = 1,"['calculus', 'analysis', 'neural-networks']"
16,"When can one write $f(x,y)$ as $|g(x)-g(y)|_2^2$ for some $g(\cdot)$?",When can one write  as  for some ?,"f(x,y) |g(x)-g(y)|_2^2 g(\cdot)","Suppose I have a function $f:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. Fix a natural number $k$. Under what conditions can I write  $$ f(x,y) = [g(x) - g(y)]^{T}[g(x) - g(y)] $$ for some appropriate function $g:\mathbb{R}^n \rightarrow \mathbb{R}^k$? Necessary conditions are obviously that $f(x,y) \geq 0$, $f(x,x) = 0$ and that $f(x,y) = f(y,x)$, but are these sufficient?","Suppose I have a function $f:\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$. Fix a natural number $k$. Under what conditions can I write  $$ f(x,y) = [g(x) - g(y)]^{T}[g(x) - g(y)] $$ for some appropriate function $g:\mathbb{R}^n \rightarrow \mathbb{R}^k$? Necessary conditions are obviously that $f(x,y) \geq 0$, $f(x,x) = 0$ and that $f(x,y) = f(y,x)$, but are these sufficient?",,"['real-analysis', 'analysis', 'functions', 'functional-equations']"
17,Tannery's Convergence Theorem,Tannery's Convergence Theorem,,"This appears as the 7th exercise of Chapter 10 in Apostol's book of Mathematical Analysis . $\{f_n\}$ is a sequence of functions and $p_n$ is an increasing sequence such that $p_n \rightarrow +\infty$. We have that The sequence of functions $\{f_n\}$ converges uniformly to $f$ on $[a, b]$ for every $b \ge a$. Each $f_n$ is Riemann-integrable on $[a, b]$ for every $b \ge a$. $|f_n(x)| < g(x)$ almost everywhere on $[a, +\infty)$ for some nonegative g, which is improper Riemann integratable on $[a, +\infty)$. Prove that $f$ and $|f|$ are improper Riemann-integrable on $[a, +\infty)$, the sequence $\{\int_a^{p_n}f_n(x)dx\}$ converges and $$ \int_a^{+\infty}f(x)dx = \lim_{n\rightarrow+\infty}\int_a^{p_n}f_n(x)dx. $$ It is easy to verify that the improper Riemann integral exists, but the convergence of $\{\int_a^{p_n}f_n(x)dx\}$ is frustrating.","This appears as the 7th exercise of Chapter 10 in Apostol's book of Mathematical Analysis . $\{f_n\}$ is a sequence of functions and $p_n$ is an increasing sequence such that $p_n \rightarrow +\infty$. We have that The sequence of functions $\{f_n\}$ converges uniformly to $f$ on $[a, b]$ for every $b \ge a$. Each $f_n$ is Riemann-integrable on $[a, b]$ for every $b \ge a$. $|f_n(x)| < g(x)$ almost everywhere on $[a, +\infty)$ for some nonegative g, which is improper Riemann integratable on $[a, +\infty)$. Prove that $f$ and $|f|$ are improper Riemann-integrable on $[a, +\infty)$, the sequence $\{\int_a^{p_n}f_n(x)dx\}$ converges and $$ \int_a^{+\infty}f(x)dx = \lim_{n\rightarrow+\infty}\int_a^{p_n}f_n(x)dx. $$ It is easy to verify that the improper Riemann integral exists, but the convergence of $\{\int_a^{p_n}f_n(x)dx\}$ is frustrating.",,['analysis']
18,"Let $f: \mathbb{R}^3\to \mathbb{R}^3$ be given by $f(\rho, \phi, \theta) = (\rho\cos\theta \sin \phi, \rho \sin \theta \sin \phi, \rho \cos \phi).$",Let  be given by,"f: \mathbb{R}^3\to \mathbb{R}^3 f(\rho, \phi, \theta) = (\rho\cos\theta \sin \phi, \rho \sin \theta \sin \phi, \rho \cos \phi).","Let $f: \mathbb{R}^3\to \mathbb{R}^3$ be given by $f(\rho, \phi, \theta) = (\rho\cos\theta \sin \phi, \rho \sin \theta \sin \phi, \rho \cos \phi).$ It is called the spherical coordinate transformation.  Take $S$ to be the set $S = [1, 2] \times (0, \pi /2] \times [0, \pi /2]$. (a) Calculate $D_f$ and $\det D_f$ . (b) Sketch the image under $f$ of the set $S$. For (a), $D_f(\rho, \phi, \theta)=\begin{bmatrix}\cos \theta \sin \phi &\rho \cos \theta \cos \phi  &-\rho\sin\theta\sin\phi \\ \sin\theta\sin\phi &\rho\sin\theta\cos\phi  &\rho\cos\theta\sin\phi \\ \cos\phi & -\rho\sin\phi & 0\end{bmatrix}$ and so $\det D_f=\rho^2\cos^2\theta\sin^3\phi+\rho^2\cos^2\theta\cos^2\phi\sin\phi+\rho^2\sin^2\theta\sin^2\phi+\rho^2\sin^2\theta\sin\phi\cos^2\phi$ I do not know what to do in (b), could someone help me please? Thank you very much.","Let $f: \mathbb{R}^3\to \mathbb{R}^3$ be given by $f(\rho, \phi, \theta) = (\rho\cos\theta \sin \phi, \rho \sin \theta \sin \phi, \rho \cos \phi).$ It is called the spherical coordinate transformation.  Take $S$ to be the set $S = [1, 2] \times (0, \pi /2] \times [0, \pi /2]$. (a) Calculate $D_f$ and $\det D_f$ . (b) Sketch the image under $f$ of the set $S$. For (a), $D_f(\rho, \phi, \theta)=\begin{bmatrix}\cos \theta \sin \phi &\rho \cos \theta \cos \phi  &-\rho\sin\theta\sin\phi \\ \sin\theta\sin\phi &\rho\sin\theta\cos\phi  &\rho\cos\theta\sin\phi \\ \cos\phi & -\rho\sin\phi & 0\end{bmatrix}$ and so $\det D_f=\rho^2\cos^2\theta\sin^3\phi+\rho^2\cos^2\theta\cos^2\phi\sin\phi+\rho^2\sin^2\theta\sin^2\phi+\rho^2\sin^2\theta\sin\phi\cos^2\phi$ I do not know what to do in (b), could someone help me please? Thank you very much.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
19,Question on integral identities: constant vanishing with uniformly bounded integral,Question on integral identities: constant vanishing with uniformly bounded integral,,"I'm studying about integral identities right now and after going through some proofs I found the following statement which it has to be an obvious result since there is no further explanation for it, but I'm having a hard time understanding it: Let $\;\int_{-\infty}^{+\infty} f(x,y)\;dx+Cy=Const\;\;\forall y\in  \mathbb R\;$ where $\;C \gt 0\;$ and assume the integral   $\;\int_{-\infty}^{+\infty} f(x,y)\;dx\;$ is uniformly bounded for $\;  y \in \mathbb R\;$, then it follows $\;C=0\;$ Why is this argument valid? It must be quite easy what I'm missing here... Any help would be valuable. Thanks in advance!","I'm studying about integral identities right now and after going through some proofs I found the following statement which it has to be an obvious result since there is no further explanation for it, but I'm having a hard time understanding it: Let $\;\int_{-\infty}^{+\infty} f(x,y)\;dx+Cy=Const\;\;\forall y\in  \mathbb R\;$ where $\;C \gt 0\;$ and assume the integral   $\;\int_{-\infty}^{+\infty} f(x,y)\;dx\;$ is uniformly bounded for $\;  y \in \mathbb R\;$, then it follows $\;C=0\;$ Why is this argument valid? It must be quite easy what I'm missing here... Any help would be valuable. Thanks in advance!",,"['integration', 'analysis', 'multivariable-calculus', 'improper-integrals']"
20,Non-Uniform Continuity Criteria,Non-Uniform Continuity Criteria,,"Let $A\subseteq\mathbb{R}$, $f:A\rightarrow \mathbb{R}$. Then, TFAE: $f$ is not uniformly continuous on $A$ There exists an $\varepsilon _0>0$ such that for every $\delta >0$ there are points $x_\delta, u_\delta\in A$ such that $\left| x_{s}-u_{s}\right| <\delta$ and $\left| f\left( x_\delta\right) -f\left( u_\delta\right) \right| \geq\varepsilon _0$ There are $\varepsilon >0$ and two sequence $x_n,y_n\in A$ such that $lim (x_n-y_n)=0$ and $\left| f\left( x_{n}\right) -f\left( y_n\right) \right| \geq \varepsilon _{0}$ for all $n\in\mathbb{N}$ Firstly, I want to show that $1)$ $\implies$ $2)$ but I couldn't do anything because I think by the definition it should be clear. Can you give a hint/help?","Let $A\subseteq\mathbb{R}$, $f:A\rightarrow \mathbb{R}$. Then, TFAE: $f$ is not uniformly continuous on $A$ There exists an $\varepsilon _0>0$ such that for every $\delta >0$ there are points $x_\delta, u_\delta\in A$ such that $\left| x_{s}-u_{s}\right| <\delta$ and $\left| f\left( x_\delta\right) -f\left( u_\delta\right) \right| \geq\varepsilon _0$ There are $\varepsilon >0$ and two sequence $x_n,y_n\in A$ such that $lim (x_n-y_n)=0$ and $\left| f\left( x_{n}\right) -f\left( y_n\right) \right| \geq \varepsilon _{0}$ for all $n\in\mathbb{N}$ Firstly, I want to show that $1)$ $\implies$ $2)$ but I couldn't do anything because I think by the definition it should be clear. Can you give a hint/help?",,['analysis']
21,"Let $K(x,t):\mathbb{R}^{2}\to \mathbb{R}$ be a non-negative measurable function. Prove the following.",Let  be a non-negative measurable function. Prove the following.,"K(x,t):\mathbb{R}^{2}\to \mathbb{R}","Please give me a hint not whole solution: The Problem: Let $K(x,t):\mathbb{R}^{2}\to \mathbb{R}$ be a non-negative measurable function such that $$F(x) = \int\limits_{\mathbb{R}} K(x,t)dt \in L(\mathbb{R})$$ (a) For $f \in L(\mathbb{R})$ show that   $$g(x) = \int\limits_{\mathbb{R}}K(x,t)f(t)dt$$   is measurable and $g \in L(\mathbb{R}).$ What I have done: Since  $F(x)\in L^{1}(\mathbb{R})$ then $$ \int\limits_{\mathbb{R}}F(x)dx<\infty \Rightarrow  \int\limits_{\mathbb{R}}\vert \int K(x,t)dt \vert dx<\infty$$ but $K(x,t)\in \mathbb{R}^{+} $ so $$ \Rightarrow  \int\limits_{\mathbb{R}} \int K(x,t)dt  dx<\infty\Rightarrow  \int\limits_{\mathbb{R}} (\int K(x,t)dx)  dt<\infty$$ Since  $\int K(x,t)dx>0$ so we can conclude that $\int K(x,t)dx<\infty.$ Now we want to show to show that $g(x)\in L^{1}(\mathbb{R})$ so we know that $\vert K(x,t)f(t)\vert$ is measurable so I can use Fubini theorem so $$ \int\limits_{\mathbb{R}}\vert \int K(x,t)f(t)dt \vert dx\leq \int\limits_{\mathbb{R}} \int K(x,t)\vert f(t)\vert dt dx \\= \int\int K(x,t)\vert f(t)\vert dx dt= \int\int K(x,t)\vert f(t)\vert dx dt\leq M\int K(x,t)dx  $$ where  $\int\vert K(x,y)\vert dt<M$. Therefore, $$g\in L^{1}(\mathbb{R}).$$ (b) Compute   $$\sup\limits_{\Vert f\Vert_{1}\leq 1} \int\limits_{\mathbb{R}^{2}} K(x,t) \vert f(t)\vert dx\, dt. $$ What I have done: $$\sup\limits_{\Vert f\Vert\leq 1} \int\limits_{\mathbb{R}^{2}} K(x,t) \vert f(t)\vert dx\, dt\leq \sup\limits_{\Vert f\Vert\leq 1} \int\limits_{\mathbb{R}}\vert f(t)\vert\int K(x,t)  dx\, dt=\int K(x,t)dx< +\infty$$ (c) Evaluate   $$ \lim\limits_{x\to \infty} \int_{\mathbb{R}} K(x,t)f(t)dt$$   where $f \in L(\mathbb{R}).$ For this Part, I couldn't do anything.","Please give me a hint not whole solution: The Problem: Let $K(x,t):\mathbb{R}^{2}\to \mathbb{R}$ be a non-negative measurable function such that $$F(x) = \int\limits_{\mathbb{R}} K(x,t)dt \in L(\mathbb{R})$$ (a) For $f \in L(\mathbb{R})$ show that   $$g(x) = \int\limits_{\mathbb{R}}K(x,t)f(t)dt$$   is measurable and $g \in L(\mathbb{R}).$ What I have done: Since  $F(x)\in L^{1}(\mathbb{R})$ then $$ \int\limits_{\mathbb{R}}F(x)dx<\infty \Rightarrow  \int\limits_{\mathbb{R}}\vert \int K(x,t)dt \vert dx<\infty$$ but $K(x,t)\in \mathbb{R}^{+} $ so $$ \Rightarrow  \int\limits_{\mathbb{R}} \int K(x,t)dt  dx<\infty\Rightarrow  \int\limits_{\mathbb{R}} (\int K(x,t)dx)  dt<\infty$$ Since  $\int K(x,t)dx>0$ so we can conclude that $\int K(x,t)dx<\infty.$ Now we want to show to show that $g(x)\in L^{1}(\mathbb{R})$ so we know that $\vert K(x,t)f(t)\vert$ is measurable so I can use Fubini theorem so $$ \int\limits_{\mathbb{R}}\vert \int K(x,t)f(t)dt \vert dx\leq \int\limits_{\mathbb{R}} \int K(x,t)\vert f(t)\vert dt dx \\= \int\int K(x,t)\vert f(t)\vert dx dt= \int\int K(x,t)\vert f(t)\vert dx dt\leq M\int K(x,t)dx  $$ where  $\int\vert K(x,y)\vert dt<M$. Therefore, $$g\in L^{1}(\mathbb{R}).$$ (b) Compute   $$\sup\limits_{\Vert f\Vert_{1}\leq 1} \int\limits_{\mathbb{R}^{2}} K(x,t) \vert f(t)\vert dx\, dt. $$ What I have done: $$\sup\limits_{\Vert f\Vert\leq 1} \int\limits_{\mathbb{R}^{2}} K(x,t) \vert f(t)\vert dx\, dt\leq \sup\limits_{\Vert f\Vert\leq 1} \int\limits_{\mathbb{R}}\vert f(t)\vert\int K(x,t)  dx\, dt=\int K(x,t)dx< +\infty$$ (c) Evaluate   $$ \lim\limits_{x\to \infty} \int_{\mathbb{R}} K(x,t)f(t)dt$$   where $f \in L(\mathbb{R}).$ For this Part, I couldn't do anything.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
22,"Understanding the limit of the ratio $\frac{\mu(B(x,r))}{m(B(x,r))}$ as r goes to 0",Understanding the limit of the ratio  as r goes to 0,"\frac{\mu(B(x,r))}{m(B(x,r))}","We work in $\mathbb{R}$. Let $B(x,r)=\{y: |x-y|<r\}$. Let $\mu$ be any Borel Measure induced by a right continous increasing function and $m$ the Lebesgue Measure. What can we say about $lim_{r\to 0}\frac{\mu(B(x,r))}{m(B(x,r))}$ for $\mu$ almost every x? Is it a positive number for $\mu$ almost every x? Is it the case that as r gets very small, $u(B(x,r))$ is at best of order r? What about the limit? Is it the case that the limit is non zero for $\mu$ almost every x? Thanks guys.","We work in $\mathbb{R}$. Let $B(x,r)=\{y: |x-y|<r\}$. Let $\mu$ be any Borel Measure induced by a right continous increasing function and $m$ the Lebesgue Measure. What can we say about $lim_{r\to 0}\frac{\mu(B(x,r))}{m(B(x,r))}$ for $\mu$ almost every x? Is it a positive number for $\mu$ almost every x? Is it the case that as r gets very small, $u(B(x,r))$ is at best of order r? What about the limit? Is it the case that the limit is non zero for $\mu$ almost every x? Thanks guys.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
23,The unit ball in $L^{\infty}$ is not weakly sequentially compact.,The unit ball in  is not weakly sequentially compact.,L^{\infty},"I would like to show that $\overline{B_1(0)} = \{x \in L^{\infty} : \|x \|_{L^{\infty}} \leq 1\}$ is NOT weakly sequentially compact (so that $L^{\infty}$ is NOT reflexive). Does this follow from the fact that $\overline{B_1(0)}$ is not strictly convex ? One way to prove this would be to explicitly construct a sequence $\{f_n\}$ in $\overline{B_1(o)}$ which has no weakly convergent subsequences, but I have not been able to think of anything.","I would like to show that $\overline{B_1(0)} = \{x \in L^{\infty} : \|x \|_{L^{\infty}} \leq 1\}$ is NOT weakly sequentially compact (so that $L^{\infty}$ is NOT reflexive). Does this follow from the fact that $\overline{B_1(0)}$ is not strictly convex ? One way to prove this would be to explicitly construct a sequence $\{f_n\}$ in $\overline{B_1(o)}$ which has no weakly convergent subsequences, but I have not been able to think of anything.",,"['real-analysis', 'general-topology', 'functional-analysis', 'analysis']"
24,Can be iteratedly integrate w.r.t. all direction $\Rightarrow$ integrable?,Can be iteratedly integrate w.r.t. all direction  integrable?,\Rightarrow,"Let $f:[0,1]\times[0,1]\to\Bbb R$ be bounded. Suppose $\int_0^1(\int_0^1 f(x,y)dy)dx$ and $\int_0^1(\int_0^1 f(x,y)dx)dy$ are all exist and equal. Then is $f$ Riemann integrable on $[0,1]\times [0,1]$? I tried to use Fubini's theorem, but the condition of that theroem doesn't suit in this case. Can I avoid digging into $\epsilon-\delta$ proofs?","Let $f:[0,1]\times[0,1]\to\Bbb R$ be bounded. Suppose $\int_0^1(\int_0^1 f(x,y)dy)dx$ and $\int_0^1(\int_0^1 f(x,y)dx)dy$ are all exist and equal. Then is $f$ Riemann integrable on $[0,1]\times [0,1]$? I tried to use Fubini's theorem, but the condition of that theroem doesn't suit in this case. Can I avoid digging into $\epsilon-\delta$ proofs?",,"['real-analysis', 'integration', 'analysis']"
25,$T$ selfadjoint with nonnegative spectrum implies $T=S^2$ for some selfadjoint $S$,selfadjoint with nonnegative spectrum implies  for some selfadjoint,T T=S^2 S,"I have question regarding the following problem: Let $H$ be a Hilbert space and $T$ a bounded linear operator from $H$ to $H$. If we assume that $T$ is selfadjoint with spectrum in $[0,\infty)$ can we conclude that $T=S^2$ for some selfadjoint bounded operator $S$ (I know that this implies that $T$ is positive but I want to show the implication directly)? I started to define $X = T-aI$ where $a \in (-\infty,0)$. Then $X$ is selfadjoint and invertible but I don't know how to proceed.","I have question regarding the following problem: Let $H$ be a Hilbert space and $T$ a bounded linear operator from $H$ to $H$. If we assume that $T$ is selfadjoint with spectrum in $[0,\infty)$ can we conclude that $T=S^2$ for some selfadjoint bounded operator $S$ (I know that this implies that $T$ is positive but I want to show the implication directly)? I started to define $X = T-aI$ where $a \in (-\infty,0)$. Then $X$ is selfadjoint and invertible but I don't know how to proceed.",,"['functional-analysis', 'analysis', 'operator-theory', 'spectral-theory']"
26,Prove that $f(x) = (1 + x)^{\frac{1}{x}}$ is continuous,Prove that  is continuous,f(x) = (1 + x)^{\frac{1}{x}},"Prove that $f(x) = (1 + x)^{\frac{1}{x}}$ is continuous on the region $(-1, \infty) \subset \mathbb{R}.$ Attempt at a solution: We need $|(1 + x)^{\frac{1}{x}} - (1 + y)^{\frac{1}{y}}| < \epsilon$ whenever $|x-y| < \delta.$ It seems like the proper course of action would be to take $$|x-y| < \epsilon^{xy}$$ $$|x-y|^{1/xy} < \epsilon$$ $$|(1+x)-(1+y)|^{1/xy} < \epsilon$$ And then show $$|(1 + x)^{\frac{1}{x}} - (1 + y)^{\frac{1}{y}}| < |(1+x)-(1+y)|^{1/xy}$$ Which is just proving that $$|a^c - b^d| < |a-b|^{cd}$$ Without loss of generality we assume $a^c > b^d$ so we just need to show $$a^c - b^d < |a-b|^{cd}$$ It seems like the binomial theorem is in order, but I'm not quite sure how to apply it here. P.S. We know that $$\lim_{x\to 0} f(x) =e.$$ So just define $f(0) = \lim_{x\to 0} f(x).$","Prove that $f(x) = (1 + x)^{\frac{1}{x}}$ is continuous on the region $(-1, \infty) \subset \mathbb{R}.$ Attempt at a solution: We need $|(1 + x)^{\frac{1}{x}} - (1 + y)^{\frac{1}{y}}| < \epsilon$ whenever $|x-y| < \delta.$ It seems like the proper course of action would be to take $$|x-y| < \epsilon^{xy}$$ $$|x-y|^{1/xy} < \epsilon$$ $$|(1+x)-(1+y)|^{1/xy} < \epsilon$$ And then show $$|(1 + x)^{\frac{1}{x}} - (1 + y)^{\frac{1}{y}}| < |(1+x)-(1+y)|^{1/xy}$$ Which is just proving that $$|a^c - b^d| < |a-b|^{cd}$$ Without loss of generality we assume $a^c > b^d$ so we just need to show $$a^c - b^d < |a-b|^{cd}$$ It seems like the binomial theorem is in order, but I'm not quite sure how to apply it here. P.S. We know that $$\lim_{x\to 0} f(x) =e.$$ So just define $f(0) = \lim_{x\to 0} f(x).$",,"['real-analysis', 'analysis']"
27,Bounded operator on a non-empty set,Bounded operator on a non-empty set,,"Let $S$ be non-empty set, and let $X$ be the vector space of bounded functions on $S$, subject only to the condition that it be a Banach space when $X$ is supplied with the supremum-norm. Suppose $f:S \to \mathbb{F}$ is a function such that $fg \in X$ for all $g \in X$. Then the multiplication operator $M_f:X \to X$, defined by $M_f(g)=fg(g \in X$) is bounded. I do not ask for the proof of this theorem. I just wonder if we assume $X\neq \lbrace0\rbrace$, *) Is it true that $f$ is necessarily bounded? That would certainly explain why $M_f$ is bounded. *)Is the theorem still true if $X$ is not required to be complete?","Let $S$ be non-empty set, and let $X$ be the vector space of bounded functions on $S$, subject only to the condition that it be a Banach space when $X$ is supplied with the supremum-norm. Suppose $f:S \to \mathbb{F}$ is a function such that $fg \in X$ for all $g \in X$. Then the multiplication operator $M_f:X \to X$, defined by $M_f(g)=fg(g \in X$) is bounded. I do not ask for the proof of this theorem. I just wonder if we assume $X\neq \lbrace0\rbrace$, *) Is it true that $f$ is necessarily bounded? That would certainly explain why $M_f$ is bounded. *)Is the theorem still true if $X$ is not required to be complete?",,['real-analysis']
28,Any formula for $\sum_{k=0}^{n} e^{(kt)^m}$?,Any formula for ?,\sum_{k=0}^{n} e^{(kt)^m},"I wonder if there is a formula for $\sum_{k=0}^{n} e^{k^mt^m}$ or non-trivial estimate, where we may assume $t=o(1)$ and $m\ge 2$ is a positive integer?","I wonder if there is a formula for $\sum_{k=0}^{n} e^{k^mt^m}$ or non-trivial estimate, where we may assume $t=o(1)$ and $m\ge 2$ is a positive integer?",,"['sequences-and-series', 'number-theory', 'analysis']"
29,Linear hull of $x \mapsto \sin(\pi n x)$ is dense [closed],Linear hull of  is dense [closed],x \mapsto \sin(\pi n x),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I need some help with an exercise of my analysis course: Show that the linear hull of the functions $x \mapsto \sin(\pi n x)$ is dense in the subspace $\{f\in C[0,1]: f(0)=f(1)=0 \}$ of $C[0,1]$. My idea was to use the Stone–Weierstrass theorem because we discussed it in the lecture but I have no clue how to apply it.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I need some help with an exercise of my analysis course: Show that the linear hull of the functions $x \mapsto \sin(\pi n x)$ is dense in the subspace $\{f\in C[0,1]: f(0)=f(1)=0 \}$ of $C[0,1]$. My idea was to use the Stone–Weierstrass theorem because we discussed it in the lecture but I have no clue how to apply it.",,"['general-topology', 'analysis']"
30,Maximizing a polynomial expression involving the sidelengths of an inscribed pentagon,Maximizing a polynomial expression involving the sidelengths of an inscribed pentagon,,"Given that $ABCDE$ is a convex pentagon and is inscribed in a circle of radius $1$ unit with $AE$ as diameter. If $AB=a,BC=b ,CD=c$ and $DE=d$, then maximum possible integral value of $a^2+b^2+c^2+d^2+abc+bcd$ is ? What I tried $AE =2$ , I joined points to form lines $BE,AC,CE$ and $AD$ and tried to use the fact that angle subtended by diameter would be a right angle. However, I was unable to solve the problem. I was looking for some hint or an alternate approach to the problem.","Given that $ABCDE$ is a convex pentagon and is inscribed in a circle of radius $1$ unit with $AE$ as diameter. If $AB=a,BC=b ,CD=c$ and $DE=d$, then maximum possible integral value of $a^2+b^2+c^2+d^2+abc+bcd$ is ? What I tried $AE =2$ , I joined points to form lines $BE,AC,CE$ and $AD$ and tried to use the fact that angle subtended by diameter would be a right angle. However, I was unable to solve the problem. I was looking for some hint or an alternate approach to the problem.",,"['geometry', 'analysis']"
31,Decomposition of Algebraic Numbers into Sets of Real and Rational Numbers,Decomposition of Algebraic Numbers into Sets of Real and Rational Numbers,,"Show that a real number $\alpha$ is algebraic of degree at most $n$ if and only if there are real numbers $\{a_1, a_2, ..., a_n\}$ such that for every $k \in \mathbb{N}$ there exist rational numbers $\{b_{1,\space k}, ..., b_{n, \space k}\}$ such that $\alpha^k = a_1b_{1,\space k} + ... + a_nb_{n,\space k}$. There are two hints: For the forward assertion use the Euclidean Algorithm and for the reverse use linear algebra. I have been staring at this problem for hours and all of my attempts have led me nowhere. I would appreciate any suggestions on how to proceed or any other references. One failed attempt: I tried to argue that $\alpha^k$ = $(\alpha^{n})^{k/n}$ and from here I tried to exponentiate $p(\alpha) - \alpha^n$ and proceed inductively. I ended up with a mess and did not think it would get me anywhere","Show that a real number $\alpha$ is algebraic of degree at most $n$ if and only if there are real numbers $\{a_1, a_2, ..., a_n\}$ such that for every $k \in \mathbb{N}$ there exist rational numbers $\{b_{1,\space k}, ..., b_{n, \space k}\}$ such that $\alpha^k = a_1b_{1,\space k} + ... + a_nb_{n,\space k}$. There are two hints: For the forward assertion use the Euclidean Algorithm and for the reverse use linear algebra. I have been staring at this problem for hours and all of my attempts have led me nowhere. I would appreciate any suggestions on how to proceed or any other references. One failed attempt: I tried to argue that $\alpha^k$ = $(\alpha^{n})^{k/n}$ and from here I tried to exponentiate $p(\alpha) - \alpha^n$ and proceed inductively. I ended up with a mess and did not think it would get me anywhere",,"['real-analysis', 'linear-algebra', 'abstract-algebra', 'analysis']"
32,help with this doubts about analysis. (usual metric),help with this doubts about analysis. (usual metric),,"help with this doubts about analysis. (usual metric) (Please answer) Find the set $B$ such that its derived set $B´=A$ where $A=\{\frac{1}{n},n\in \mathbb{Z^+}\}$. is there any theorem to say that any derived set is closed? If this is so, then the problem would not have a solution. _  I told my teacher about something that he had read that the derived set should be closed, but he told me that he was confusing the derived set with  closure set, (Could you explain this to me?) I was thinking of the possible proof that the derived set is closed. Theorem: $A$ is closed if and only if $A^{\prime} \subseteq A $ if $(A^{\prime})^{\prime}\subseteq A^{\prime}$ then $A^\prime$ is closed, propuse it a my teacher but, he said that it is not always true.. how proof that $(A^{\prime})^{\prime}\subseteq A^{\prime}$ for every $A$ I've already asked this exercise here, but every time I have class, I'm confused. and im sorry my English is bad.","help with this doubts about analysis. (usual metric) (Please answer) Find the set $B$ such that its derived set $B´=A$ where $A=\{\frac{1}{n},n\in \mathbb{Z^+}\}$. is there any theorem to say that any derived set is closed? If this is so, then the problem would not have a solution. _  I told my teacher about something that he had read that the derived set should be closed, but he told me that he was confusing the derived set with  closure set, (Could you explain this to me?) I was thinking of the possible proof that the derived set is closed. Theorem: $A$ is closed if and only if $A^{\prime} \subseteq A $ if $(A^{\prime})^{\prime}\subseteq A^{\prime}$ then $A^\prime$ is closed, propuse it a my teacher but, he said that it is not always true.. how proof that $(A^{\prime})^{\prime}\subseteq A^{\prime}$ for every $A$ I've already asked this exercise here, but every time I have class, I'm confused. and im sorry my English is bad.",,"['real-analysis', 'general-topology', 'analysis']"
33,Is norm a functional (rather than a function)?,Is norm a functional (rather than a function)?,,"Many authors of textbooks define the norm as a real-valued function that satisfies those well-known properties. For example, the definition of norm at Wiki here . In my opinion, however, the norm is indeed a functional rather than a function, because it maps sequences, functions, or some other ""vectors"" from a vector space to a real number: $X \to \mathbb{R}$. The norm is a function if and only if $X \subset \mathbb{R}^n$. Is my point of view wrong or the authors should improve the definition? PS. I think the idea of functional is kind of extended from the idea of function. I think a function can only be a mapping from numbers to a number, while a functional can be a mapping from any vectors in vector space to a number. Those vectors can themselves be functions, thus someone calls functional as function of function . In my opinion, therefore, functional is not a subset of function, but function is a subset of functional . If we talk about the generalized idea of function with respect to the mapping functionality, shall we say a mapping or an operator?","Many authors of textbooks define the norm as a real-valued function that satisfies those well-known properties. For example, the definition of norm at Wiki here . In my opinion, however, the norm is indeed a functional rather than a function, because it maps sequences, functions, or some other ""vectors"" from a vector space to a real number: $X \to \mathbb{R}$. The norm is a function if and only if $X \subset \mathbb{R}^n$. Is my point of view wrong or the authors should improve the definition? PS. I think the idea of functional is kind of extended from the idea of function. I think a function can only be a mapping from numbers to a number, while a functional can be a mapping from any vectors in vector space to a number. Those vectors can themselves be functions, thus someone calls functional as function of function . In my opinion, therefore, functional is not a subset of function, but function is a subset of functional . If we talk about the generalized idea of function with respect to the mapping functionality, shall we say a mapping or an operator?",,"['real-analysis', 'functional-analysis', 'analysis', 'vector-spaces', 'normed-spaces']"
34,"Showing the function $f(x)=0$ on $[a,b]$",Showing the function  on,"f(x)=0 [a,b]","Suppose for real numbers $a<b$ one has a function with continuous derivative  $$f:[a,b]\to \mathbb{R}$$ such that $f(a)=0$ and there exists a real number $C$ with  $$|f'(x)|\leq C|f(x)|\:\:\:\text{for all}\:\:x\in [a,b].$$ Show that $f(x)=0$ for all $x\in[a,b].$ Given $\epsilon >0$ there exists a $\delta >0$ such that for any $x\in B_{\delta}(a)$ we have $|f(x)|<\epsilon $, that is $f(x)=0$ in that neighborhood.  Note that $f$ is continuous on a compact set. If $f$ is nonzero, there exists $x\in[a,b]$ such that $f(x)\neq 0$. Let $w=\inf\{x:f(x)\neq 0\}.$  Hence for any  $x<w$, we have $f(x)=0. $ Now considering the following, we get $f(w)=0$ which is a contradiction. $$|\frac{f(w)-f(a)}{w-a}|=|f'(\xi)|\leq C|f(\xi)|, \:\:\text{where}\:\:a<\xi<w.$$ Is my my argument valid? I also didn't use the continuity of the derivative. Thank you!","Suppose for real numbers $a<b$ one has a function with continuous derivative  $$f:[a,b]\to \mathbb{R}$$ such that $f(a)=0$ and there exists a real number $C$ with  $$|f'(x)|\leq C|f(x)|\:\:\:\text{for all}\:\:x\in [a,b].$$ Show that $f(x)=0$ for all $x\in[a,b].$ Given $\epsilon >0$ there exists a $\delta >0$ such that for any $x\in B_{\delta}(a)$ we have $|f(x)|<\epsilon $, that is $f(x)=0$ in that neighborhood.  Note that $f$ is continuous on a compact set. If $f$ is nonzero, there exists $x\in[a,b]$ such that $f(x)\neq 0$. Let $w=\inf\{x:f(x)\neq 0\}.$  Hence for any  $x<w$, we have $f(x)=0. $ Now considering the following, we get $f(w)=0$ which is a contradiction. $$|\frac{f(w)-f(a)}{w-a}|=|f'(\xi)|\leq C|f(\xi)|, \:\:\text{where}\:\:a<\xi<w.$$ Is my my argument valid? I also didn't use the continuity of the derivative. Thank you!",,"['calculus', 'analysis', 'derivatives', 'proof-verification', 'continuity']"
35,Prove that $\bigcap_{\delta>0}A_\delta=\overline{A}$ [closed],Prove that  [closed],\bigcap_{\delta>0}A_\delta=\overline{A},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $A$ be a subset of the metric space $M$ and for $\delta>0$ define $$A_\delta=\{x\in M\mid(\exists a\in A)\, d(x,a)<\delta|\}$$ Prove that $\bigcap_{\delta>0} A_\delta=\overline{A}$ I can only prove that $A_\delta$ is open...","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $A$ be a subset of the metric space $M$ and for $\delta>0$ define $$A_\delta=\{x\in M\mid(\exists a\in A)\, d(x,a)<\delta|\}$$ Prove that $\bigcap_{\delta>0} A_\delta=\overline{A}$ I can only prove that $A_\delta$ is open...",,"['real-analysis', 'analysis', 'metric-spaces']"
36,"If $M^\perp$ consists only of the zero vector, then is $M$ total in $X$?","If  consists only of the zero vector, then is  total in ?",M^\perp M X,"Let $X$ be an inner product space, and let $M$ be a non-empty subset of $X$. Then $M$ is said to be total in $X$ if the span of $M$ is dense in $X$. We have the following result: If  $M$ is total in $X$, then $M^\perp$ consists only of the zero vector in $X$. Does the converse hold too? I know that the anwer is in the affirmative if $X$ is also a Hilbert space. What is the situation if $X$ is an inner product space but not a Hilbert space? By definition,  $$ M^\perp \colon= \{ \ x \in X \ \colon \ \langle x, v \rangle = 0 \ \mbox{ for all } \ v \in M \ \}.$$ And, $M^\perp$ is a (vector) subspace of $X$ and is a closed set in the metric space induced by the inner product.","Let $X$ be an inner product space, and let $M$ be a non-empty subset of $X$. Then $M$ is said to be total in $X$ if the span of $M$ is dense in $X$. We have the following result: If  $M$ is total in $X$, then $M^\perp$ consists only of the zero vector in $X$. Does the converse hold too? I know that the anwer is in the affirmative if $X$ is also a Hilbert space. What is the situation if $X$ is an inner product space but not a Hilbert space? By definition,  $$ M^\perp \colon= \{ \ x \in X \ \colon \ \langle x, v \rangle = 0 \ \mbox{ for all } \ v \in M \ \}.$$ And, $M^\perp$ is a (vector) subspace of $X$ and is a closed set in the metric space induced by the inner product.",,"['real-analysis', 'functional-analysis', 'analysis', 'hilbert-spaces', 'inner-products']"
37,Generalized associate law for unordered sums,Generalized associate law for unordered sums,,"Let $I'$ be a nonempty set $I_m$ be a nonempty set for $m\in I'$ $I:=\left\{(m,n):m\in I'\text{ and }n\in I_m\right\}$ $E$ be a normed $\mathbb R$-vector space $(x_{mn})_{(m,\:n)\in I}\subseteq E$ be summable I want to show that $(x_{mn})_{n\in I_m}$ is summable for all $m\in I'$ $\left(\sum_{n\in I_m}x_{mn}\right)_{m\in I'}$ is summable and $$\sum_{m\in I'}\sum_{n\in I_m}x_{mn}=\sum_{(m,\:n)\in I}x_{mn}\tag1$$ My attempt for (1.) is as follows: Let $\varepsilon>0$. Since $(x_{mn})_{(m,\:n)\in I}$ is summable, it is Cauchy and hence there is a finite $J\subseteq I$ with $$\left\|\sum_{(m,\:n)\in K}x_{mn}\right\|_E<\varepsilon\;\;\;\text{for all finite }K\subseteq I\setminus J\tag2\;.$$ Let $m\in I'$ and $$J_m:=\left\{n:(m,n)\in J\right\}\;.$$ By definition, $$\left\{(m,n):n\in I_m\setminus J_m\right\}\subseteq I\setminus J\tag3$$ and hence $$\left\|\sum_{n\in K_m}x_{mn}\right\|_E<\varepsilon\;\;\;\text{for all finite }K_m\subseteq I_m\setminus J_m\;.\tag4$$ If $E$ is complete, we obtain (1.) by $(4)$. I would like to know, if we're able to prove this even without the completeness assumption on $E$. In any case, I wasn't able to find the right approach for a proof of $(2.)$. So, how can we prove it?","Let $I'$ be a nonempty set $I_m$ be a nonempty set for $m\in I'$ $I:=\left\{(m,n):m\in I'\text{ and }n\in I_m\right\}$ $E$ be a normed $\mathbb R$-vector space $(x_{mn})_{(m,\:n)\in I}\subseteq E$ be summable I want to show that $(x_{mn})_{n\in I_m}$ is summable for all $m\in I'$ $\left(\sum_{n\in I_m}x_{mn}\right)_{m\in I'}$ is summable and $$\sum_{m\in I'}\sum_{n\in I_m}x_{mn}=\sum_{(m,\:n)\in I}x_{mn}\tag1$$ My attempt for (1.) is as follows: Let $\varepsilon>0$. Since $(x_{mn})_{(m,\:n)\in I}$ is summable, it is Cauchy and hence there is a finite $J\subseteq I$ with $$\left\|\sum_{(m,\:n)\in K}x_{mn}\right\|_E<\varepsilon\;\;\;\text{for all finite }K\subseteq I\setminus J\tag2\;.$$ Let $m\in I'$ and $$J_m:=\left\{n:(m,n)\in J\right\}\;.$$ By definition, $$\left\{(m,n):n\in I_m\setminus J_m\right\}\subseteq I\setminus J\tag3$$ and hence $$\left\|\sum_{n\in K_m}x_{mn}\right\|_E<\varepsilon\;\;\;\text{for all finite }K_m\subseteq I_m\setminus J_m\;.\tag4$$ If $E$ is complete, we obtain (1.) by $(4)$. I would like to know, if we're able to prove this even without the completeness assumption on $E$. In any case, I wasn't able to find the right approach for a proof of $(2.)$. So, how can we prove it?",,"['real-analysis', 'sequences-and-series', 'analysis', 'summation']"
38,Alternative proof for Riemann integration Theorem,Alternative proof for Riemann integration Theorem,,"I already have a proof of the following theorem and I was wondering if there is another kind of proof, (just being curious). Suppose $f$ is Riemann integrable and $h$ is non-decreasing and non-negative. Let $F(x) = \int_x^b f$. If $A \leqslant F(x) \leqslant B$ for all $x \in [a,b],$ then $h(b)A \leqslant \int_a^b f h \leqslant  h(b)B$. Proof: Taking any partition $P = (x_0,x_1, \ldots, x_n)$ consider the Riemann sums $$S_P = \sum_{k=1}^n f(x_k) h(x_k)(x_k -x_{k-1}), \\ S_{P,j} = \sum_{k = j}^n f(x_k)(x_k - x_{k-1}),$$ which converge to $\int_a^b fh $ and $\int_{x_{j-1}}^b f$. Since $f(x_k)(x_k - x_{k-1}) = S_{P,k} - S_{P,k-1}$ we have $$S_P = \sum_{k=1}^n h(x_k)(S_{P,k} - S_{P,k-1}) \\ = h(x_1)S_{P,1} + (h(x_2) - h(x_1))S_{P,2} + \ldots (h(x_n) - h(x_{n-1}))S_{P,n} $$ Let $\hat{A}$ and $\hat{B}$ be the upper and lower bounds for the finite set $\{S_{P,k}\}$. Since $h$ is non-decreasing $h(x_k) - h(x_{k-1}) \geqslant 0$ and $$\hat{A} h(b) = \hat{A} h(x_n) \leqslant S_P \leqslant \hat{B} h(x_n) = \hat{B} h(b).$$ As the number of partition points increases the sum $S_P$ converges to $\int_a^b fh $ and it can be shown that $\hat{A} \to A$ and $\hat{B} \to B.$ Therefore, $$h(b)A \leqslant \int_a^b f h \leqslant h(b)B$$ Note: This proof was made by RRL user :)","I already have a proof of the following theorem and I was wondering if there is another kind of proof, (just being curious). Suppose $f$ is Riemann integrable and $h$ is non-decreasing and non-negative. Let $F(x) = \int_x^b f$. If $A \leqslant F(x) \leqslant B$ for all $x \in [a,b],$ then $h(b)A \leqslant \int_a^b f h \leqslant  h(b)B$. Proof: Taking any partition $P = (x_0,x_1, \ldots, x_n)$ consider the Riemann sums $$S_P = \sum_{k=1}^n f(x_k) h(x_k)(x_k -x_{k-1}), \\ S_{P,j} = \sum_{k = j}^n f(x_k)(x_k - x_{k-1}),$$ which converge to $\int_a^b fh $ and $\int_{x_{j-1}}^b f$. Since $f(x_k)(x_k - x_{k-1}) = S_{P,k} - S_{P,k-1}$ we have $$S_P = \sum_{k=1}^n h(x_k)(S_{P,k} - S_{P,k-1}) \\ = h(x_1)S_{P,1} + (h(x_2) - h(x_1))S_{P,2} + \ldots (h(x_n) - h(x_{n-1}))S_{P,n} $$ Let $\hat{A}$ and $\hat{B}$ be the upper and lower bounds for the finite set $\{S_{P,k}\}$. Since $h$ is non-decreasing $h(x_k) - h(x_{k-1}) \geqslant 0$ and $$\hat{A} h(b) = \hat{A} h(x_n) \leqslant S_P \leqslant \hat{B} h(x_n) = \hat{B} h(b).$$ As the number of partition points increases the sum $S_P$ converges to $\int_a^b fh $ and it can be shown that $\hat{A} \to A$ and $\hat{B} \to B.$ Therefore, $$h(b)A \leqslant \int_a^b f h \leqslant h(b)B$$ Note: This proof was made by RRL user :)",,"['real-analysis', 'integration', 'analysis', 'alternative-proof', 'riemann-integration']"
39,Existence of quasi-arithmetic progression in a set with positive measure,Existence of quasi-arithmetic progression in a set with positive measure,,"Let $S\subseteq [0,N]$ for some $N\geq 1$ be a measurable set with positive measure. The first question is to show that there is $x\in S$ and $t>0$ so that $x+t\in S$ and $x+2t\in S$. To do this, my idea is reformulating the problem: $$ I:=\int_0^N \int_0^{N} 1_{S}(x)1_{S}(x+t)1_S(x+2t)dt\,dx $$ Then if $I>0$, we could conclude our claim. I intend to use either the Lebesgue differentiation theorem or properties of convolutions. How could I proceed? More generally, I want to consider $P(t)$ to replace the ""$2t$"" above, where $P(0)=0$ and $P$ is continuous at $0$ (If necessary, you may assume that $P$ is real analytic near $0$), so that the problem becomes showing $$ \int_0^N \int_0^{N} 1_{S}(x)1_{S}(x+t)1_S(x+P(t))dt\,dx>0. $$ Any suggestions?","Let $S\subseteq [0,N]$ for some $N\geq 1$ be a measurable set with positive measure. The first question is to show that there is $x\in S$ and $t>0$ so that $x+t\in S$ and $x+2t\in S$. To do this, my idea is reformulating the problem: $$ I:=\int_0^N \int_0^{N} 1_{S}(x)1_{S}(x+t)1_S(x+2t)dt\,dx $$ Then if $I>0$, we could conclude our claim. I intend to use either the Lebesgue differentiation theorem or properties of convolutions. How could I proceed? More generally, I want to consider $P(t)$ to replace the ""$2t$"" above, where $P(0)=0$ and $P$ is continuous at $0$ (If necessary, you may assume that $P$ is real analytic near $0$), so that the problem becomes showing $$ \int_0^N \int_0^{N} 1_{S}(x)1_{S}(x+t)1_S(x+P(t))dt\,dx>0. $$ Any suggestions?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'convolution']"
40,How to calculate derivative under area integral,How to calculate derivative under area integral,,"I have a function which is defined as \begin{equation} P(x_i,y_i)=\iint_{(x-x_i)^2+(y-y_i)^2\leq\delta}f(x,y,x_i,y_i) \, dx \, dy. \end{equation} How to calculate the derivative with respective $x_i$? Is it enough to write \begin{equation} \frac{\partial P(x_i,y_i)}{\partial x_i}=\iint_{(x-x_i)^2+(y-y_i)^2 \leq \delta} \frac{\partial f(x,y,x_i,y_i)}{\partial x_i} \, dx\,dy. \end{equation} Since the boundary is also a function of $x_{i}$, do we have to add another term like the Leibniz rule for differentiating an integral?","I have a function which is defined as \begin{equation} P(x_i,y_i)=\iint_{(x-x_i)^2+(y-y_i)^2\leq\delta}f(x,y,x_i,y_i) \, dx \, dy. \end{equation} How to calculate the derivative with respective $x_i$? Is it enough to write \begin{equation} \frac{\partial P(x_i,y_i)}{\partial x_i}=\iint_{(x-x_i)^2+(y-y_i)^2 \leq \delta} \frac{\partial f(x,y,x_i,y_i)}{\partial x_i} \, dx\,dy. \end{equation} Since the boundary is also a function of $x_{i}$, do we have to add another term like the Leibniz rule for differentiating an integral?",,"['analysis', 'multivariable-calculus', 'partial-derivative', 'vector-analysis', 'mathematical-physics']"
41,Intersection of Jordan measurable sets is Jordan measurable,Intersection of Jordan measurable sets is Jordan measurable,,"I am currently self teaching myself measure theory from https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf . I would like some help/tips in proving the Boolean closure properties in Exercise 1.1.6. I am first proving that if $E$ and $F$ are Jordan measurable then $E\cap F$ is Jordan measurable, it seems very elementary and so I don't know why it is causing difficulty. Here is my attempt of a proof: If $E$ and $F$ are Jordan measurable then there exists elementary sets $A_1,B_1,A_2,B_2$ such that $$ A_1\subset E\subset B_1, A_2 \subset F \subset B_2$$ with $m(A_1) = m(B_1), m(A_2) = m(B_2).$ Note that $$ A_1 \cap A_2 \subset E\cap F \subset B_1 \cap B_2.$$ I now want to show that $m(A_1 \cap A_2) = m(B_1\cap B_2)$ and then since $A_1 \cap A_2$ and $B_1 \cap B_2$ are elementary $E\cap F$ would be Jordan measurable. I also know that $m(B_1 \cap B_2) \leq m(A_1\cap A_2)$ by the monotonicity property  of elementary sets. I was trying to partition each of the sets $A_1,B_1,A_2,B_2$ e.g.  $$ A_1 = (A_1 \cap A_2)\cup (A_1\backslash A_2).$$ Using this we can conclude $$ m(A_1 \cap A_2) + m(A_1\backslash A_2) = m(B_1 \cap B_2) + m(B_1\backslash B_2) $$ $$ m(A_1 \cap A_2) + m(A_2\backslash A_1) = m(B_1 \cap B_2) + m(B_2\backslash B_1). $$ This doesn't seem to help and I've tried various other things to no avail. Thanks for your help.","I am currently self teaching myself measure theory from https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf . I would like some help/tips in proving the Boolean closure properties in Exercise 1.1.6. I am first proving that if $E$ and $F$ are Jordan measurable then $E\cap F$ is Jordan measurable, it seems very elementary and so I don't know why it is causing difficulty. Here is my attempt of a proof: If $E$ and $F$ are Jordan measurable then there exists elementary sets $A_1,B_1,A_2,B_2$ such that $$ A_1\subset E\subset B_1, A_2 \subset F \subset B_2$$ with $m(A_1) = m(B_1), m(A_2) = m(B_2).$ Note that $$ A_1 \cap A_2 \subset E\cap F \subset B_1 \cap B_2.$$ I now want to show that $m(A_1 \cap A_2) = m(B_1\cap B_2)$ and then since $A_1 \cap A_2$ and $B_1 \cap B_2$ are elementary $E\cap F$ would be Jordan measurable. I also know that $m(B_1 \cap B_2) \leq m(A_1\cap A_2)$ by the monotonicity property  of elementary sets. I was trying to partition each of the sets $A_1,B_1,A_2,B_2$ e.g.  $$ A_1 = (A_1 \cap A_2)\cup (A_1\backslash A_2).$$ Using this we can conclude $$ m(A_1 \cap A_2) + m(A_1\backslash A_2) = m(B_1 \cap B_2) + m(B_1\backslash B_2) $$ $$ m(A_1 \cap A_2) + m(A_2\backslash A_1) = m(B_1 \cap B_2) + m(B_2\backslash B_1). $$ This doesn't seem to help and I've tried various other things to no avail. Thanks for your help.",,"['analysis', 'measure-theory']"
42,Behaviour of a function at 0,Behaviour of a function at 0,,"Given a function $g:(0,1) \to (0,\infty)$ such that $\lim\limits_{x \to 0}{\frac{g(x)-1}{x}}=0$, I try to show that this implies $\lim\limits_{k\to\infty}{g(x2^{-k})^{2k}}=1$. Using Taylor-Expansion is no alternative, since I don't know anything about differentiability . I've tried using $ g(x)=o(x)+1$ and applying the logarithm as well as rewriting $g(x2^{-k})$ by adding ones, but it doesn't quite work out. Does anyone know some easy trick to show this?","Given a function $g:(0,1) \to (0,\infty)$ such that $\lim\limits_{x \to 0}{\frac{g(x)-1}{x}}=0$, I try to show that this implies $\lim\limits_{k\to\infty}{g(x2^{-k})^{2k}}=1$. Using Taylor-Expansion is no alternative, since I don't know anything about differentiability . I've tried using $ g(x)=o(x)+1$ and applying the logarithm as well as rewriting $g(x2^{-k})$ by adding ones, but it doesn't quite work out. Does anyone know some easy trick to show this?",,"['analysis', 'stochastic-analysis']"
43,Changing the order of summation and using a change of variables,Changing the order of summation and using a change of variables,,"I'm looking at the below, and I don't understand how to get the second equality using the change of variables $m=k-l$. I've tried changing the order of summation to get $$\sum_{k=2^j+1}^{2^{j+1}}\sum_{l=2^j+1}^{k-1}=\sum_{l=2^j+1}^{2^{j+1}-1}\sum_{k=l+1}^{2^{j+1}}=\sum_{l=2^j+1}^{2^{j+1}-1}\sum_{m=1}^{2^{j+1}}$$which isn't the form given below. How can I get the summation given below? I would greatly appreciate some help.","I'm looking at the below, and I don't understand how to get the second equality using the change of variables $m=k-l$. I've tried changing the order of summation to get $$\sum_{k=2^j+1}^{2^{j+1}}\sum_{l=2^j+1}^{k-1}=\sum_{l=2^j+1}^{2^{j+1}-1}\sum_{k=l+1}^{2^{j+1}}=\sum_{l=2^j+1}^{2^{j+1}-1}\sum_{m=1}^{2^{j+1}}$$which isn't the form given below. How can I get the summation given below? I would greatly appreciate some help.",,"['calculus', 'real-analysis', 'analysis', 'summation']"
44,Proving $\nabla\left\{\frac{1}{\|\cdot\|}\right\}=-\frac{1}{\|\cdot\|^3}x$ in $\mathcal{D}'(\mathbb{R}^3)$,Proving  in,\nabla\left\{\frac{1}{\|\cdot\|}\right\}=-\frac{1}{\|\cdot\|^3}x \mathcal{D}'(\mathbb{R}^3),"As per the title, I would like to prove that $$\nabla\left\{\frac{1}{\|\cdot\|}\right\}=-\frac{1}{\|\cdot\|^3}x\qquad\text{in }\mathcal{D}'(\mathbb{R}^3).\qquad(\star)$$ Now, I am of the opinion that one would prove this in the following manner: For all test functions $\phi\in\mathcal{D}(\mathbb{R}^3)$, we have $$ \begin{aligned} \left\langle\nabla\left\{\frac{1}{\|\cdot\|}\right\},\phi\right\rangle&=-\left\langle\frac{1}{\|\cdot\|},\nabla\phi\right\rangle \\ &=-\lim_{\epsilon\downarrow 0}\int_{\|x\|>\epsilon}\frac{\nabla\phi(x)}{\|x\|}\,dx \\ &=\cdots \end{aligned} $$ and then evaluating the limits. However, I was having a debate with a friend of mine, and he claimed that you could prove $(\star)$ in $C^\infty(\mathbb{R}^3\setminus\{0\})$ and then extend to distributions á la $$\nabla\frac{1}{\|x\|}=-\frac{x}{\|x\|^3}.$$ Then you say that this holds in $\mathcal{D}'(\mathbb{R}^3)$, i.e. $$\left\langle\nabla\left\{\frac{1}{\|\cdot\|}\right\},\phi\right\rangle=-\left\langle\frac{x}{\|\cdot\|^3},\phi\right\rangle$$ assuming one can show that $\nabla\left\{\frac{1}{\|\cdot\|}\right\}$ is a distribution. Is this way valid?","As per the title, I would like to prove that $$\nabla\left\{\frac{1}{\|\cdot\|}\right\}=-\frac{1}{\|\cdot\|^3}x\qquad\text{in }\mathcal{D}'(\mathbb{R}^3).\qquad(\star)$$ Now, I am of the opinion that one would prove this in the following manner: For all test functions $\phi\in\mathcal{D}(\mathbb{R}^3)$, we have $$ \begin{aligned} \left\langle\nabla\left\{\frac{1}{\|\cdot\|}\right\},\phi\right\rangle&=-\left\langle\frac{1}{\|\cdot\|},\nabla\phi\right\rangle \\ &=-\lim_{\epsilon\downarrow 0}\int_{\|x\|>\epsilon}\frac{\nabla\phi(x)}{\|x\|}\,dx \\ &=\cdots \end{aligned} $$ and then evaluating the limits. However, I was having a debate with a friend of mine, and he claimed that you could prove $(\star)$ in $C^\infty(\mathbb{R}^3\setminus\{0\})$ and then extend to distributions á la $$\nabla\frac{1}{\|x\|}=-\frac{x}{\|x\|^3}.$$ Then you say that this holds in $\mathcal{D}'(\mathbb{R}^3)$, i.e. $$\left\langle\nabla\left\{\frac{1}{\|\cdot\|}\right\},\phi\right\rangle=-\left\langle\frac{x}{\|\cdot\|^3},\phi\right\rangle$$ assuming one can show that $\nabla\left\{\frac{1}{\|\cdot\|}\right\}$ is a distribution. Is this way valid?",,"['analysis', 'derivatives', 'fourier-analysis', 'distribution-theory']"
45,"Given two real numbers $a$ and $b$ such that $a<b$, what about the convergence of these two sequences?","Given two real numbers  and  such that , what about the convergence of these two sequences?",a b a<b,"Let $a$ and $b$ be two given real numbers such that $a < b$, and let $\left\{x_n\right\}$ and $\left\{ y_n \right\}$ be the sequences defined as follows:  Let us choose $x_1$ and $y_1$ such that $$a < x_1 < b, \qquad a < y_1 < b$$ arbitrarily, and then let  $$x_2 = \frac{a+x_1}{2}, \qquad y_2 = \frac{y_1 + b}{2},$$  $$x_3 = \frac{a + x_1 + x_2 }{3}, \qquad y_3 = \frac{ y_1 + y_2 + b}{3},$$ and so on  $$ x_n = \frac{a+ x_1 + \cdots + x_{n-1} }{n}, \qquad y_n = \frac{ y_1 + \cdots + y_{n-1} + b}{n} $$ for $n= 3, 4, 5, \ldots$. Then what can we say about the convergence of these sequences? To generalize this problem a little further, let $\left\{ r_n \right\}$ be a given sequence of positive real numbers, and let us now define $$x_2 = \frac{r_1 x_1 + a r_2}{r_1 + r_2}, \qquad y_2 = \frac{r_1 y_1 + r_2 b}{r_1 + r_2},$$  $$x_3 = \frac{r_1 x_1 + r_2 x_2 + r_3 a }{r_1 + r_2 + r_3}, \qquad y_3 = \frac{ r_1 y_1 + r_2 y_2 + r_3 b}{r_1 + r_2 + r_3 },$$ and so on  $$ x_n = \frac{r_1 x_1 + \cdots + r_{n-1} x_{n-1} + r_n a }{r_1 + \cdots + r_n }, \qquad y_n = \frac{ r_1 y_1 + \cdots + y_{n-1} + r_n b}{r_1 + \cdots + r_n} $$ for $n= 3, 4, 5, \ldots$. Then what can we say about the convergence of these sequences? What if we proceed as follows? Let $r_0 > 0$ be given, and let  $$x_2 = \frac{r_0a+ r_1 x_1}{r_0 + r_1 }, \qquad y_2 = \frac{ r_1 y_1 + r_0 b}{r_1 + r_0},$$  $$x_3 = \frac{r_0 a + r_1 x_1 + r_2 x_2 }{r_0 + r_1 + r_2}, \qquad y_3 = \frac{ r_2 y_2 + r_1 y_1 + r_0 b}{r_2 + r_1 + r_0},$$ and so on  $$ x_n = \frac{r_0 a + r_1 x_1 + \cdots + r_{n-1} x_{n-1} }{r_0 + \cdots + r_{n-1} }, \qquad y_n = \frac{r_{n-1} y_{n-1} + \cdots +  r_1 y_1 + r_0 b}{r_{n-1} + \cdots + r_0} $$ for $n= 3, 4, 5, \ldots$. What can we say about the convergence of these sequences now? I can handle the situation only if we have only unit weights and only average of two terms is involved at a time, but I simply have no idea of what happens in this case!! So, I would be really grateful for a detailed answer!","Let $a$ and $b$ be two given real numbers such that $a < b$, and let $\left\{x_n\right\}$ and $\left\{ y_n \right\}$ be the sequences defined as follows:  Let us choose $x_1$ and $y_1$ such that $$a < x_1 < b, \qquad a < y_1 < b$$ arbitrarily, and then let  $$x_2 = \frac{a+x_1}{2}, \qquad y_2 = \frac{y_1 + b}{2},$$  $$x_3 = \frac{a + x_1 + x_2 }{3}, \qquad y_3 = \frac{ y_1 + y_2 + b}{3},$$ and so on  $$ x_n = \frac{a+ x_1 + \cdots + x_{n-1} }{n}, \qquad y_n = \frac{ y_1 + \cdots + y_{n-1} + b}{n} $$ for $n= 3, 4, 5, \ldots$. Then what can we say about the convergence of these sequences? To generalize this problem a little further, let $\left\{ r_n \right\}$ be a given sequence of positive real numbers, and let us now define $$x_2 = \frac{r_1 x_1 + a r_2}{r_1 + r_2}, \qquad y_2 = \frac{r_1 y_1 + r_2 b}{r_1 + r_2},$$  $$x_3 = \frac{r_1 x_1 + r_2 x_2 + r_3 a }{r_1 + r_2 + r_3}, \qquad y_3 = \frac{ r_1 y_1 + r_2 y_2 + r_3 b}{r_1 + r_2 + r_3 },$$ and so on  $$ x_n = \frac{r_1 x_1 + \cdots + r_{n-1} x_{n-1} + r_n a }{r_1 + \cdots + r_n }, \qquad y_n = \frac{ r_1 y_1 + \cdots + y_{n-1} + r_n b}{r_1 + \cdots + r_n} $$ for $n= 3, 4, 5, \ldots$. Then what can we say about the convergence of these sequences? What if we proceed as follows? Let $r_0 > 0$ be given, and let  $$x_2 = \frac{r_0a+ r_1 x_1}{r_0 + r_1 }, \qquad y_2 = \frac{ r_1 y_1 + r_0 b}{r_1 + r_0},$$  $$x_3 = \frac{r_0 a + r_1 x_1 + r_2 x_2 }{r_0 + r_1 + r_2}, \qquad y_3 = \frac{ r_2 y_2 + r_1 y_1 + r_0 b}{r_2 + r_1 + r_0},$$ and so on  $$ x_n = \frac{r_0 a + r_1 x_1 + \cdots + r_{n-1} x_{n-1} }{r_0 + \cdots + r_{n-1} }, \qquad y_n = \frac{r_{n-1} y_{n-1} + \cdots +  r_1 y_1 + r_0 b}{r_{n-1} + \cdots + r_0} $$ for $n= 3, 4, 5, \ldots$. What can we say about the convergence of these sequences now? I can handle the situation only if we have only unit weights and only average of two terms is involved at a time, but I simply have no idea of what happens in this case!! So, I would be really grateful for a detailed answer!",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
46,"If a family of functions has an unbounded derivative, is it necessarily not uniformly equicontinuous?","If a family of functions has an unbounded derivative, is it necessarily not uniformly equicontinuous?",,"I'm having trouble wrapping my head around uniform equicontinuity for a family of functions. I can often spot if a family is uniformly equicontinuous, but have trouble telling when they're not, and I was wondering if there are any signs which would allow me to spot the non-uniformly equicontinuous families. Intuitively, it seems to me that if the derivative of a family of functions is unbounded then it can't be uniformly equicontinuous. For example, if the family is given by $F=\{f_n:n\in\mathbb N\}$ and $\forall n \in \mathbb N$,  $|f_n'(x)|=|e^{n}|$, then it seems to me that the family cannot be uniformly equicontinuous. Since this isn't a common theorem, I'm guessing my intuitiong is wrong, but a proof or a counter example would be very helpful, thanks! Note: my definition of uniform equicontinuity is the following: A family of functions, $F$, consisting of functions $f:(X,d)\to (Y,\rho)$ is uniformly equicontinuous on $X$ if for every $\epsilon >0 $ there exists $\delta >0$ such that $$d(x,y)<\delta \implies \rho(f(x),f(y))<\epsilon$$ for all $x\in X$ and $f \in F$. The key here is that $\delta$ depends on $\epsilon$, but not on $x$ or $f$.","I'm having trouble wrapping my head around uniform equicontinuity for a family of functions. I can often spot if a family is uniformly equicontinuous, but have trouble telling when they're not, and I was wondering if there are any signs which would allow me to spot the non-uniformly equicontinuous families. Intuitively, it seems to me that if the derivative of a family of functions is unbounded then it can't be uniformly equicontinuous. For example, if the family is given by $F=\{f_n:n\in\mathbb N\}$ and $\forall n \in \mathbb N$,  $|f_n'(x)|=|e^{n}|$, then it seems to me that the family cannot be uniformly equicontinuous. Since this isn't a common theorem, I'm guessing my intuitiong is wrong, but a proof or a counter example would be very helpful, thanks! Note: my definition of uniform equicontinuity is the following: A family of functions, $F$, consisting of functions $f:(X,d)\to (Y,\rho)$ is uniformly equicontinuous on $X$ if for every $\epsilon >0 $ there exists $\delta >0$ such that $$d(x,y)<\delta \implies \rho(f(x),f(y))<\epsilon$$ for all $x\in X$ and $f \in F$. The key here is that $\delta$ depends on $\epsilon$, but not on $x$ or $f$.",,"['real-analysis', 'analysis', 'uniform-continuity', 'equicontinuity']"
47,Darboux Integral Partition Question,Darboux Integral Partition Question,,"I looking at a sample problem to use the Darboux Integral to find the integral of $f(x)=x^2$. Textbook Solution: For a partition P $={0=t_0<t_1<t_2<...<t_n=b}$ we have, $U(f,P) = \sum^n_{k=1}sup(x^2:x \in [t_{k-1},t_k])*(t_k-t_{k-1})$ (I get lost here) If we choose $t_k=\frac{kb}{n}$ then we have: $U(f,P) = \sum^n_{k=1} \frac{k^2b^2}{n^2}(\frac{b}{n}) $... My question is how was $t_k=\frac{kb}{n}$ ""picked"". There is no explanation for it in the book.","I looking at a sample problem to use the Darboux Integral to find the integral of $f(x)=x^2$. Textbook Solution: For a partition P $={0=t_0<t_1<t_2<...<t_n=b}$ we have, $U(f,P) = \sum^n_{k=1}sup(x^2:x \in [t_{k-1},t_k])*(t_k-t_{k-1})$ (I get lost here) If we choose $t_k=\frac{kb}{n}$ then we have: $U(f,P) = \sum^n_{k=1} \frac{k^2b^2}{n^2}(\frac{b}{n}) $... My question is how was $t_k=\frac{kb}{n}$ ""picked"". There is no explanation for it in the book.",,"['real-analysis', 'integration', 'analysis', 'riemann-sum']"
48,$f: \mathbb R^2 \to \mathbb R^2$ be continuous ; then is there a non-empty proper closed $A \subseteq \mathbb R^2$ s.t. $ A \subseteq f(A)$?,be continuous ; then is there a non-empty proper closed  s.t. ?,f: \mathbb R^2 \to \mathbb R^2 A \subseteq \mathbb R^2  A \subseteq f(A),Let $f: \mathbb R^2 \to \mathbb R^2$ be a continuous function ; then is it true that there is a non-empty proper closed subset $A \subseteq \mathbb R^2$ such that $ A \subseteq f(A)$ ? I can show that if $f: \mathbb R^n \to \mathbb R^n$ is continuous then there is a non-empty proper closed subset $A \subseteq \mathbb R^n$ such that $ f(A) \subseteq A$ ; but I have no idea on what happens if we want a reverse inclusion . Please help . Thanks in advance,Let $f: \mathbb R^2 \to \mathbb R^2$ be a continuous function ; then is it true that there is a non-empty proper closed subset $A \subseteq \mathbb R^2$ such that $ A \subseteq f(A)$ ? I can show that if $f: \mathbb R^n \to \mathbb R^n$ is continuous then there is a non-empty proper closed subset $A \subseteq \mathbb R^n$ such that $ f(A) \subseteq A$ ; but I have no idea on what happens if we want a reverse inclusion . Please help . Thanks in advance,,['analysis']
49,Show monotonicity,Show monotonicity,,"Originally, I want to show that $$ \frac{\sqrt{a \cdot b + \frac{b}{a}x^2}\arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right)}{\sqrt{a \cdot b}\arctan \left(\frac{c}{\sqrt{a \cdot b}}\right)} \geq 1 \  \ \text{for} \ \  x, a,b,c > 0 \ . $$  To do so, I figured it is sufficient to show that $$ f(x) = \sqrt{a \cdot b + \frac{b}{a}x^2}\arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right) $$ is monotonically increasing for $x > 0$.  Of course, I took the derivative $f'(x)$ and proceeded with the demand  $$ \frac{\frac{b}{a}x}{\sqrt{a \cdot b + \frac{b}{a}x^2}} \arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right) - \frac{c \frac{b}{a} x}{a \cdot b + \frac{b}{a} x^2 + c^2} > 0 \ . $$ In the end, I got stuck with $$ \arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right) > \frac{c \sqrt{a \cdot b + \frac{b}{a}x^2}}{a \cdot b + \frac{b}{a} x^2 + c^2} \ . $$ Inserting values for a,b, and c seems to work perfectly, but I can't manage to analytically solve the inequation. Does anyone have an idea of how to approach this problem? Edit: I came across the Shafer-Fink inequality stating $$ \frac{3y}{1+2\sqrt{1 + y^2}} < \arctan y < \frac{\pi y}{1 + 2\sqrt{1 + y^2}} \ . $$ Can I substitute  $$ y = \frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}} $$ and therefore receive  $$ \arctan (y) > \frac{3y}{1+2\sqrt{1 + y^2}} > \frac{y}{1 + y^2} ? $$ Is that a proper way?","Originally, I want to show that $$ \frac{\sqrt{a \cdot b + \frac{b}{a}x^2}\arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right)}{\sqrt{a \cdot b}\arctan \left(\frac{c}{\sqrt{a \cdot b}}\right)} \geq 1 \  \ \text{for} \ \  x, a,b,c > 0 \ . $$  To do so, I figured it is sufficient to show that $$ f(x) = \sqrt{a \cdot b + \frac{b}{a}x^2}\arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right) $$ is monotonically increasing for $x > 0$.  Of course, I took the derivative $f'(x)$ and proceeded with the demand  $$ \frac{\frac{b}{a}x}{\sqrt{a \cdot b + \frac{b}{a}x^2}} \arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right) - \frac{c \frac{b}{a} x}{a \cdot b + \frac{b}{a} x^2 + c^2} > 0 \ . $$ In the end, I got stuck with $$ \arctan \left(\frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}}\right) > \frac{c \sqrt{a \cdot b + \frac{b}{a}x^2}}{a \cdot b + \frac{b}{a} x^2 + c^2} \ . $$ Inserting values for a,b, and c seems to work perfectly, but I can't manage to analytically solve the inequation. Does anyone have an idea of how to approach this problem? Edit: I came across the Shafer-Fink inequality stating $$ \frac{3y}{1+2\sqrt{1 + y^2}} < \arctan y < \frac{\pi y}{1 + 2\sqrt{1 + y^2}} \ . $$ Can I substitute  $$ y = \frac{c}{\sqrt{a \cdot b + \frac{b}{a}x^2}} $$ and therefore receive  $$ \arctan (y) > \frac{3y}{1+2\sqrt{1 + y^2}} > \frac{y}{1 + y^2} ? $$ Is that a proper way?",,"['analysis', 'inequality', 'monotone-functions']"
50,Solving $\int_0^1 \sqrt{x^2+1}$ with Euler substitution,Solving  with Euler substitution,\int_0^1 \sqrt{x^2+1},"So I was trying to solve this integral $$\int_0^1 \sqrt{x^2+1} \, dx$$ in two different ways with Euler substitution. So: $(x^2+1)=x+t$, so that $x=(1-t^2)/(2t)$, $(x^2+1)=x*t+1$, $x=2t/(1-t^2)$. But I am not sure how to proceed, and how do the integral and the limits change... Any tips? Thanks :)","So I was trying to solve this integral $$\int_0^1 \sqrt{x^2+1} \, dx$$ in two different ways with Euler substitution. So: $(x^2+1)=x+t$, so that $x=(1-t^2)/(2t)$, $(x^2+1)=x*t+1$, $x=2t/(1-t^2)$. But I am not sure how to proceed, and how do the integral and the limits change... Any tips? Thanks :)",,"['analysis', 'definite-integrals']"
51,Prove of a property of convex function,Prove of a property of convex function,,"I just finished my optimization class and there is an inequality that I couldn't prove,  $$(y-x)^T(\nabla f(y) - \nabla f(x)) \geq 0$$ I can't also understand the intuition of this inequality. Could someone prove this inequality for me, also I would greatly appreciate if some intuition is provided. At last, is there a reference for properties of convex functions? Thanks!","I just finished my optimization class and there is an inequality that I couldn't prove,  $$(y-x)^T(\nabla f(y) - \nabla f(x)) \geq 0$$ I can't also understand the intuition of this inequality. Could someone prove this inequality for me, also I would greatly appreciate if some intuition is provided. At last, is there a reference for properties of convex functions? Thanks!",,"['analysis', 'convex-optimization']"
52,Pointwise and uniform convergence of a function,Pointwise and uniform convergence of a function,,"I would like you guys to verify my proof for the following exercise: Let $$f_n:[-1,1] \to \mathbb R; \quad f_n(x) := nx(1-x^2)^n$$   Is $f_n$ pointwise (uniformly) continuous? And in case it is, specify $\lim_{n\to \infty} f_n.$ My solution: $f_n$ converges pointwise towards the $0$-function, i.e. $f_n \to 0$ pointwise.  Proof: $f_n(1)  = f_n(-1) = 0$, so let $x\in ]-1,1[$. For fixed $x$ with $|x| < 1$ we have that $$|f_n(x) - 0| = n|x||1-x^2|^n \leq n |1-x^2|^n \to 0$$ since $nq^n \to 0$ when $|q| < 1$. Therefore $f_n \to 0$ pointwise. $f_n$ does not uniformly converge. Pick $x_n = 1/n$. We then have $$ \lim_{n\to \infty}f_n(x_n) = \lim_{n\to \infty} n \frac{1}{n} \left(1-\frac{1}{n}\right)^n\left(1+\frac{1}{n}\right)^n = e^{-1}e = 1 \neq 0$$ as desired. EDIT : Changed $[0,1]$ to $[-1,1]$.","I would like you guys to verify my proof for the following exercise: Let $$f_n:[-1,1] \to \mathbb R; \quad f_n(x) := nx(1-x^2)^n$$   Is $f_n$ pointwise (uniformly) continuous? And in case it is, specify $\lim_{n\to \infty} f_n.$ My solution: $f_n$ converges pointwise towards the $0$-function, i.e. $f_n \to 0$ pointwise.  Proof: $f_n(1)  = f_n(-1) = 0$, so let $x\in ]-1,1[$. For fixed $x$ with $|x| < 1$ we have that $$|f_n(x) - 0| = n|x||1-x^2|^n \leq n |1-x^2|^n \to 0$$ since $nq^n \to 0$ when $|q| < 1$. Therefore $f_n \to 0$ pointwise. $f_n$ does not uniformly converge. Pick $x_n = 1/n$. We then have $$ \lim_{n\to \infty}f_n(x_n) = \lim_{n\to \infty} n \frac{1}{n} \left(1-\frac{1}{n}\right)^n\left(1+\frac{1}{n}\right)^n = e^{-1}e = 1 \neq 0$$ as desired. EDIT : Changed $[0,1]$ to $[-1,1]$.",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence', 'pointwise-convergence']"
53,Showing subsets of $L^2=\{(x_n) : \sum_{n=1}^\infty x_n^2 < \infty \}$ are compact,Showing subsets of  are compact,L^2=\{(x_n) : \sum_{n=1}^\infty x_n^2 < \infty \},"Show whether the following subsets of $l^2$ are compact. Let $$l^2=\left\{(x_n):\sum_{n=1}^{\infty}x_n^2<\infty\right\},$$ equipped with the norm $$\|(x_n)\|=\left(\sum_{n=1}^{\infty}x_n^2\right)^{1/2}.$$ State and explain if the following subsets of $l^2$ are compact: $A=\left\{(x_n)\in l^2:\sum_{n=1}^{k}x_n^2\leq1  \right\}$ where $k\in\mathbb{N}$ is fixed; $B=\left\{(x_n)\in l^2:\sum_{n=1}^{\infty}x_n^2\leq1, x_n=0\text{ for all } n>k  \right\}$ where $k\in\mathbb{N}$ is fixed; $C=\left\{(x_n)\in l^2:\sum_{n=1}^{\infty}x_n^2\leq1\right\}$ . I only know the sequentially compact definition of compactness. I also have the theorem which states that all compact metric spaces are closed and bounded. I think that $A$ is not compact, as it is not bounded (this was easy to show using a sequence which became a constant after k) I think $C$ might not be compact, but only because I have seen that the unit ball in an infinite dimensional vector space is not compact, but I don't know how to prove it. I think $B$ might be compact, but I'm unsure. Any help would be appreciated!","Show whether the following subsets of are compact. Let equipped with the norm State and explain if the following subsets of are compact: where is fixed; where is fixed; . I only know the sequentially compact definition of compactness. I also have the theorem which states that all compact metric spaces are closed and bounded. I think that is not compact, as it is not bounded (this was easy to show using a sequence which became a constant after k) I think might not be compact, but only because I have seen that the unit ball in an infinite dimensional vector space is not compact, but I don't know how to prove it. I think might be compact, but I'm unsure. Any help would be appreciated!","l^2 l^2=\left\{(x_n):\sum_{n=1}^{\infty}x_n^2<\infty\right\}, \|(x_n)\|=\left(\sum_{n=1}^{\infty}x_n^2\right)^{1/2}. l^2 A=\left\{(x_n)\in l^2:\sum_{n=1}^{k}x_n^2\leq1  \right\} k\in\mathbb{N} B=\left\{(x_n)\in l^2:\sum_{n=1}^{\infty}x_n^2\leq1, x_n=0\text{ for all } n>k  \right\} k\in\mathbb{N} C=\left\{(x_n)\in l^2:\sum_{n=1}^{\infty}x_n^2\leq1\right\} A C B","['general-topology', 'analysis', 'metric-spaces', 'compactness']"
54,"In an infinite dimensional Hilbert space, when can we express any element in H as linear combination of the basis vectors?","In an infinite dimensional Hilbert space, when can we express any element in H as linear combination of the basis vectors?",,"Let H be a hilbert space, infinite dimensional. Let $(e_n)$ be an orthonormal basis for H. Can we express any x in H as $x=\sum_{i=1}^{\infty} c_i e_i$ where the constants are (maybe) $<e_i,x>$? Also when is an infinite linear combination of basis vectors like the one above an element in H? I ask because I know on one hand that by definition of Hilbert basis, any element in H can be approximated in the norm by a sequence of finite linear combinations of $e_k$ but have doubts as to whether we can write any element as an infinite linear combination of the one above.. What conditions garantee that such an infinite sum would even be in H? Thanks for any information.","Let H be a hilbert space, infinite dimensional. Let $(e_n)$ be an orthonormal basis for H. Can we express any x in H as $x=\sum_{i=1}^{\infty} c_i e_i$ where the constants are (maybe) $<e_i,x>$? Also when is an infinite linear combination of basis vectors like the one above an element in H? I ask because I know on one hand that by definition of Hilbert basis, any element in H can be approximated in the norm by a sequence of finite linear combinations of $e_k$ but have doubts as to whether we can write any element as an infinite linear combination of the one above.. What conditions garantee that such an infinite sum would even be in H? Thanks for any information.",,"['analysis', 'operator-theory']"
55,Proving Cauchy when given a sequence,Proving Cauchy when given a sequence,,Let $\left\{x_n\right\}$ be a sequence and $0 < a < 1$. Suppose that for all $n \ge 3$ we have $$ \left\lvert x_n - x_{n-1}\right\rvert \le a\left\lvert x_{n-1} - x_{n-2} \right\rvert. $$ Prove that $\left\{x_n\right\}$  is Cauchy. I don't even know where to start here. To prove a sequence is cauchy I have to somehow reach the conclusion of $\left\lvert x_m - x_{k}\right\rvert < \varepsilon$ right? How do I even do that with this inequality. I'm completely lost.,Let $\left\{x_n\right\}$ be a sequence and $0 < a < 1$. Suppose that for all $n \ge 3$ we have $$ \left\lvert x_n - x_{n-1}\right\rvert \le a\left\lvert x_{n-1} - x_{n-2} \right\rvert. $$ Prove that $\left\{x_n\right\}$  is Cauchy. I don't even know where to start here. To prove a sequence is cauchy I have to somehow reach the conclusion of $\left\lvert x_m - x_{k}\right\rvert < \varepsilon$ right? How do I even do that with this inequality. I'm completely lost.,,['analysis']
56,A star shaped domain,A star shaped domain,,"We say that $\Omega$ is a star-shaped domain (with respect to the origin) of $\mathbb R ^n$ if : $\Omega  = \{x\in \mathbb R ^n : \left \| x \right \| < g(\frac{x}{\left \| x \right \|})\} $ and   $\partial \Omega  = \{x\in \mathbb R ^n : \left \| x \right \| = g(\frac{x}{\left \| x \right \|})\} $ with $g$ is a continuous, positive function on the unit ball. I have two questions: 1) I know what star-shaped means Geometrically, but it doesn't get linked with the definition given above. Can you help me understand.. 2)Is there a map (bijection) between $\Omega$ and the unit sphere $B$? I appreciate your answers and your help. EDIT: $g$ is a function on a unit sphere.","We say that $\Omega$ is a star-shaped domain (with respect to the origin) of $\mathbb R ^n$ if : $\Omega  = \{x\in \mathbb R ^n : \left \| x \right \| < g(\frac{x}{\left \| x \right \|})\} $ and   $\partial \Omega  = \{x\in \mathbb R ^n : \left \| x \right \| = g(\frac{x}{\left \| x \right \|})\} $ with $g$ is a continuous, positive function on the unit ball. I have two questions: 1) I know what star-shaped means Geometrically, but it doesn't get linked with the definition given above. Can you help me understand.. 2)Is there a map (bijection) between $\Omega$ and the unit sphere $B$? I appreciate your answers and your help. EDIT: $g$ is a function on a unit sphere.",,"['real-analysis', 'geometry', 'functional-analysis', 'analysis', 'euclidean-geometry']"
57,For which parameter p will the following series be convergent?,For which parameter p will the following series be convergent?,,"$$ \sum_{n=1}^{\infty} \sin \frac{1}{n^p} $$ Well, as I know this is pretty similar to the Riemann zeta function, and as I checked this will be divergent if $p \leq 1$. And it will be convergent if $p>1$. How can I continue the problem solving?","$$ \sum_{n=1}^{\infty} \sin \frac{1}{n^p} $$ Well, as I know this is pretty similar to the Riemann zeta function, and as I checked this will be divergent if $p \leq 1$. And it will be convergent if $p>1$. How can I continue the problem solving?",,"['sequences-and-series', 'analysis']"
58,Is a continuous function $\mathbb{R}^{n} \rightarrow \mathbb{R}$ on a compact set Riemann integrable?,Is a continuous function  on a compact set Riemann integrable?,\mathbb{R}^{n} \rightarrow \mathbb{R},"From the standpoint of the higher dimensional Riemann Integral: For a general bounded set, $S$, that is not a rectangle, the convention is to integrate $f$ over $S$ by first subsuming $S$ within some rectangle $Q$ and then integrating $f_{S}$ (the characteristic function of $f$ with respect to $S$; ie $f(x) = f$ for $x \in S$ and $f(x) = 0$ for $x \not \in S$) over $Q$. As a consequence, $f_{S}$ is integrable over this rectangle $iff$ the set $E$ of points $x_{0} \in \delta(S)$ for which $$\lim_{x \to x_{0}}f(x) \not = 0$$ is measure zero. Here $\delta(S)$ is the border of $S$. That is, $$m(\{x_{0} \in \delta(S) | \lim_{x \to x_{0}}f(x) \not = 0 \}) = 0$$ A continuous function may, thus, fail to be integrable over a compact set, if $E$ is not measure zero. Could someone propose an example of such an instance?","From the standpoint of the higher dimensional Riemann Integral: For a general bounded set, $S$, that is not a rectangle, the convention is to integrate $f$ over $S$ by first subsuming $S$ within some rectangle $Q$ and then integrating $f_{S}$ (the characteristic function of $f$ with respect to $S$; ie $f(x) = f$ for $x \in S$ and $f(x) = 0$ for $x \not \in S$) over $Q$. As a consequence, $f_{S}$ is integrable over this rectangle $iff$ the set $E$ of points $x_{0} \in \delta(S)$ for which $$\lim_{x \to x_{0}}f(x) \not = 0$$ is measure zero. Here $\delta(S)$ is the border of $S$. That is, $$m(\{x_{0} \in \delta(S) | \lim_{x \to x_{0}}f(x) \not = 0 \}) = 0$$ A continuous function may, thus, fail to be integrable over a compact set, if $E$ is not measure zero. Could someone propose an example of such an instance?",,"['real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
59,Is this sequence is equicontinuous?,Is this sequence is equicontinuous?,,"Let $\Omega \subset \mathbb{R}^N (N \geq 2)$  an open and bounded set with smooth boundary. Let $(u_n)$ a sequence of real functions defined in $\Omega$. Suppose that $u_n \in C^{2}(\Omega) \cap C(\overline{\Omega})$ with $|\Delta u_n(x)| \leq C$ for all $x \in \Omega, $ and for all $n \in \mathbb{N}$ where the constant $C$ does not depend on $n \in \mathbb{N}.$ My question is the sequence $u_n$ is equicontinuous? I am trying to obtain this by using the mean value theorem, but i am getting anywhere. Someone could help me to prove or disprove the statemente? thanks in advance","Let $\Omega \subset \mathbb{R}^N (N \geq 2)$  an open and bounded set with smooth boundary. Let $(u_n)$ a sequence of real functions defined in $\Omega$. Suppose that $u_n \in C^{2}(\Omega) \cap C(\overline{\Omega})$ with $|\Delta u_n(x)| \leq C$ for all $x \in \Omega, $ and for all $n \in \mathbb{N}$ where the constant $C$ does not depend on $n \in \mathbb{N}.$ My question is the sequence $u_n$ is equicontinuous? I am trying to obtain this by using the mean value theorem, but i am getting anywhere. Someone could help me to prove or disprove the statemente? thanks in advance",,"['functional-analysis', 'analysis']"
60,Fundamental Theorem of Calculus for distributions.,Fundamental Theorem of Calculus for distributions.,,"Consider a function $F \in C^{\alpha}( \mathbb{R})$ for $0 < \alpha < 1.$ Then we can take it's distributional derivative. We can say $f = F' \in C^{\alpha -1}( \mathbb{R} )$. My issue is going back. Say I have a $\alpha -1 -$Hölder function $f$, then how can I ""integrate"" it to get a primitive $F$ such that $f$ is it's distributional derivative? Here we use the following definition (but any other definition can be used as well) for negative $\beta \in (-1,0)$: $$g \in C^{\beta}( \mathbb{R} ) \Leftrightarrow \ |\langle g, \phi_{x}^{\lambda} \rangle| \  ≤ C \lambda^{\beta}$$ where $\phi_{x}^{\lambda}(z) = \lambda^{-d}\phi(\frac{z -x}{\lambda})$ and $C$ is uniform over all $x$ and $\phi \in C^{\infty}_0(-1,1)$ with $||\phi||_{C^1}≤1.$ EDIT: An approach would be to prove that $$C^{\beta} \subseteq L^1_{Loc}$$ which would mean in particular that our distributions are actually functions . This does not work, as pointed out in the comments, and in related questions. My other idea was to define $\langle f, 1_{[0,a]} \rangle $ by using typical approximation of the indicator function of $1_{[0,a]}.$ This cannot work: for example the derivative of a the Brownian motion is not a measure . I saw that this topic is covered only slightly in MathStackExchange. Here are similar questions regarding this topic. Here Is a general question about Holder spaces with negative exponents. Here Is a question that is very similar to mine, maybe just in a slightly different context. Here Is a question by myself regarding the same topic. In fact as you might imagine I am looking into this subject with little success :D Finally let me also give some motivation as to why I ma studying this: these spaces are used in the theory of SPDEs. In particular a reference is Fritz's and Hairer's book: ""A course on rough paths."" Both my questions are exercises in this book.","Consider a function $F \in C^{\alpha}( \mathbb{R})$ for $0 < \alpha < 1.$ Then we can take it's distributional derivative. We can say $f = F' \in C^{\alpha -1}( \mathbb{R} )$. My issue is going back. Say I have a $\alpha -1 -$Hölder function $f$, then how can I ""integrate"" it to get a primitive $F$ such that $f$ is it's distributional derivative? Here we use the following definition (but any other definition can be used as well) for negative $\beta \in (-1,0)$: $$g \in C^{\beta}( \mathbb{R} ) \Leftrightarrow \ |\langle g, \phi_{x}^{\lambda} \rangle| \  ≤ C \lambda^{\beta}$$ where $\phi_{x}^{\lambda}(z) = \lambda^{-d}\phi(\frac{z -x}{\lambda})$ and $C$ is uniform over all $x$ and $\phi \in C^{\infty}_0(-1,1)$ with $||\phi||_{C^1}≤1.$ EDIT: An approach would be to prove that $$C^{\beta} \subseteq L^1_{Loc}$$ which would mean in particular that our distributions are actually functions . This does not work, as pointed out in the comments, and in related questions. My other idea was to define $\langle f, 1_{[0,a]} \rangle $ by using typical approximation of the indicator function of $1_{[0,a]}.$ This cannot work: for example the derivative of a the Brownian motion is not a measure . I saw that this topic is covered only slightly in MathStackExchange. Here are similar questions regarding this topic. Here Is a general question about Holder spaces with negative exponents. Here Is a question that is very similar to mine, maybe just in a slightly different context. Here Is a question by myself regarding the same topic. In fact as you might imagine I am looking into this subject with little success :D Finally let me also give some motivation as to why I ma studying this: these spaces are used in the theory of SPDEs. In particular a reference is Fritz's and Hairer's book: ""A course on rough paths."" Both my questions are exercises in this book.",,"['functional-analysis', 'analysis', 'distribution-theory', 'holder-spaces']"
61,Two problems on real number series,Two problems on real number series,,Consider the series: $$a) \sum_{n=1}^\infty \frac{a^n}{ \prod_{k=1}^n \ (1+\sin\frac{1}{2k})}$$ $$b) \sum_{n=1}^\infty \frac{a^n}{ \prod_{k=1}^n \ (1+\tan\frac{3}{2k})}$$ Showing that these two are convergent (and absolutely convergent) it's no big deal for  $a\in R^*$. But I couldn't figure out how to solve it when $a=1$. My guess is that the general term has to be brought to another form. And that is my question: how to prove convergence when $a=1$. Thanks for helping with this!,Consider the series: $$a) \sum_{n=1}^\infty \frac{a^n}{ \prod_{k=1}^n \ (1+\sin\frac{1}{2k})}$$ $$b) \sum_{n=1}^\infty \frac{a^n}{ \prod_{k=1}^n \ (1+\tan\frac{3}{2k})}$$ Showing that these two are convergent (and absolutely convergent) it's no big deal for  $a\in R^*$. But I couldn't figure out how to solve it when $a=1$. My guess is that the general term has to be brought to another form. And that is my question: how to prove convergence when $a=1$. Thanks for helping with this!,,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'absolute-convergence']"
62,On the equivalence of two definitions for summations over the integers,On the equivalence of two definitions for summations over the integers,,"If $a_k\in\mathbb{C}$, then define $$ \sum_{k=-\infty}^{\infty}a_k=L\\ \Updownarrow\\\forall\epsilon>0,\exists N,m,n> N\implies\left|\sum_{k=-m}^na_k-L\right|<\epsilon $$ Question: Is it true that $$ \sum_{k=-\infty}^{\infty}a_k=L\\ \Updownarrow\\ \sum_{k=0}^{\infty}a_k\text{ and }\sum_{k=1}^{\infty}a_{-k}\text{ both exist and }\sum_{k=0}^{\infty}a_k+\sum_{k=1}^{\infty}a_{-k}=L $$ Thoughts: $[\Downarrow]$ Couldn't come up with something useful. Tried to show that both series are Cauchy. Failed. $[\Uparrow]$ Let $$ \alpha:=\sum_{k=0}^{\infty}a_k\\ \beta:=\sum_{k=1}^{\infty}a_{-k} $$ Given $\epsilon>0$, there exist $N_1,N_2$ such that $$ \left|\sum_{k=0}^na_k-\alpha\right|<\frac{\epsilon}{2}\\ \left|\sum_{k=1}^ma_{-k}-\beta\right|<\frac{\epsilon}{2} $$ whenever $n>N_1$ and $m>N_2$. Hence if $N:=\max\{N_1,N_2\}$ then \begin{align} \left|\sum_{k=-m}^na_k-L\right|&=\left|\sum_{k=0}^na_k-\alpha+\sum_{k=1}^ma_{-k}-\beta\right|\\ &\leq\left|\sum_{k=0}^na_k-\alpha\right|+\left|\sum_{k=1}^ma_{-k}-\beta\right|\\ &<\frac{\epsilon}{2}+\frac{\epsilon}{2}\\ &=\epsilon \end{align} whenever $m,n>N$.","If $a_k\in\mathbb{C}$, then define $$ \sum_{k=-\infty}^{\infty}a_k=L\\ \Updownarrow\\\forall\epsilon>0,\exists N,m,n> N\implies\left|\sum_{k=-m}^na_k-L\right|<\epsilon $$ Question: Is it true that $$ \sum_{k=-\infty}^{\infty}a_k=L\\ \Updownarrow\\ \sum_{k=0}^{\infty}a_k\text{ and }\sum_{k=1}^{\infty}a_{-k}\text{ both exist and }\sum_{k=0}^{\infty}a_k+\sum_{k=1}^{\infty}a_{-k}=L $$ Thoughts: $[\Downarrow]$ Couldn't come up with something useful. Tried to show that both series are Cauchy. Failed. $[\Uparrow]$ Let $$ \alpha:=\sum_{k=0}^{\infty}a_k\\ \beta:=\sum_{k=1}^{\infty}a_{-k} $$ Given $\epsilon>0$, there exist $N_1,N_2$ such that $$ \left|\sum_{k=0}^na_k-\alpha\right|<\frac{\epsilon}{2}\\ \left|\sum_{k=1}^ma_{-k}-\beta\right|<\frac{\epsilon}{2} $$ whenever $n>N_1$ and $m>N_2$. Hence if $N:=\max\{N_1,N_2\}$ then \begin{align} \left|\sum_{k=-m}^na_k-L\right|&=\left|\sum_{k=0}^na_k-\alpha+\sum_{k=1}^ma_{-k}-\beta\right|\\ &\leq\left|\sum_{k=0}^na_k-\alpha\right|+\left|\sum_{k=1}^ma_{-k}-\beta\right|\\ &<\frac{\epsilon}{2}+\frac{\epsilon}{2}\\ &=\epsilon \end{align} whenever $m,n>N$.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
63,"Prove that $ (x+y)^n \le x^n +y^n$ with $x,y \ge 0 $ and $0 \lt n \le 1$ [duplicate]",Prove that  with  and  [duplicate]," (x+y)^n \le x^n +y^n x,y \ge 0  0 \lt n \le 1","This question already has answers here : Is this inequality true? $ (x + y)^{\alpha} < x^{\alpha} + y^{\alpha} $, for positive $x$ & $y$, and for $0 < \alpha < 1$ (6 answers) Proof of the inequality $(x+y)^n\leq 2^{n-1}(x^n+y^n)$ (6 answers) Closed 7 years ago . I have no clue on this one so I hope you can help me out on this one. Let $x,y \ge 0 $ and $0 \lt n \le 1$. Then $$ (x+y)^n \le x^n +y^n$$","This question already has answers here : Is this inequality true? $ (x + y)^{\alpha} < x^{\alpha} + y^{\alpha} $, for positive $x$ & $y$, and for $0 < \alpha < 1$ (6 answers) Proof of the inequality $(x+y)^n\leq 2^{n-1}(x^n+y^n)$ (6 answers) Closed 7 years ago . I have no clue on this one so I hope you can help me out on this one. Let $x,y \ge 0 $ and $0 \lt n \le 1$. Then $$ (x+y)^n \le x^n +y^n$$",,"['real-analysis', 'analysis', 'inequality', 'substitution']"
64,Prove if $f(0) = 0$ then $\lim_{x \to 0^+}x\int_x^1 \frac{f(t)}{t^2}dt = 0$ for regulated function $f$,Prove if  then  for regulated function,f(0) = 0 \lim_{x \to 0^+}x\int_x^1 \frac{f(t)}{t^2}dt = 0 f,"Prove if $f(0) = 0$ then $\displaystyle\lim_{x \to 0^+}x\int_x^1 \frac{f(t)}{t^2}dt = 0$ for regulated function $f$ A regulated function is a function $f$ on $[a,b]$ such that $\exists$ a sequence $(\varphi_n)_{n \in \mathbb{N}}$ of step functions such that $\displaystyle \lim_{n \to \infty} \sup_{x \in [a,b]} \lvert f(x) - \varphi_n(x) \rvert = 0$ and $\forall x \in (a,b)$ the left and right limits exist, also left limit of $b$ & right limit of $a$. Also given the assumption that $f$ is continuous at $0$ which is said to be redundant. So we know $f$ is continuous on $[0,1)$. For $\varepsilon > 0, \exists \delta > 0, \lvert x \rvert < \delta \implies \lvert f(x) - f(0) \rvert = \lvert f(x) \rvert < \varepsilon$ Intuitively, I know the $x$ outside the integral goes to $0$ so as long as the integral itself converges then we can get the desired $0$ as the limit. so get $\displaystyle\lim_{x \to 0^+}x \cdot \lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt = 0 \cdot \lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt$ so we need $\displaystyle\lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt < \infty$ Now I'm stuck because if $x = 0$ then $\displaystyle \frac{f(0)}{0} = \frac{0}{0}$","Prove if $f(0) = 0$ then $\displaystyle\lim_{x \to 0^+}x\int_x^1 \frac{f(t)}{t^2}dt = 0$ for regulated function $f$ A regulated function is a function $f$ on $[a,b]$ such that $\exists$ a sequence $(\varphi_n)_{n \in \mathbb{N}}$ of step functions such that $\displaystyle \lim_{n \to \infty} \sup_{x \in [a,b]} \lvert f(x) - \varphi_n(x) \rvert = 0$ and $\forall x \in (a,b)$ the left and right limits exist, also left limit of $b$ & right limit of $a$. Also given the assumption that $f$ is continuous at $0$ which is said to be redundant. So we know $f$ is continuous on $[0,1)$. For $\varepsilon > 0, \exists \delta > 0, \lvert x \rvert < \delta \implies \lvert f(x) - f(0) \rvert = \lvert f(x) \rvert < \varepsilon$ Intuitively, I know the $x$ outside the integral goes to $0$ so as long as the integral itself converges then we can get the desired $0$ as the limit. so get $\displaystyle\lim_{x \to 0^+}x \cdot \lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt = 0 \cdot \lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt$ so we need $\displaystyle\lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt < \infty$ Now I'm stuck because if $x = 0$ then $\displaystyle \frac{f(0)}{0} = \frac{0}{0}$",,"['real-analysis', 'integration', 'analysis']"
65,Find a subsequence whose limit exists,Find a subsequence whose limit exists,,"Let $\{a_n\}$ be a sequence of non-zero real numbers. Show that it has a subsequence $\{a_{n_k}\}$ such that $\lim \dfrac{a_{n_{k+1}}}{ {a_{n_k}}}$ exists and belongs to $\{0,1,\infty\}$ . I am finding the above problem false. If I take $(a_n)_n=(e^{-n})_n$ then any sub-sequence of $a_n$ is $e^{-n_k}$ but $\lim \dfrac{a_{n_{k+1}}}{ {a_{n_k}}}=\dfrac{e^{-n-1}}{e^{-n}}=\dfrac{1}{e}\notin \{0,1,\infty\}$ . Edits :By @Henry's comment I am sure the problem is true.But how should I  find the sub-sequence.Please give some hints.",Let be a sequence of non-zero real numbers. Show that it has a subsequence such that exists and belongs to . I am finding the above problem false. If I take then any sub-sequence of is but . Edits :By @Henry's comment I am sure the problem is true.But how should I  find the sub-sequence.Please give some hints.,"\{a_n\} \{a_{n_k}\} \lim \dfrac{a_{n_{k+1}}}{ {a_{n_k}}} \{0,1,\infty\} (a_n)_n=(e^{-n})_n a_n e^{-n_k} \lim \dfrac{a_{n_{k+1}}}{ {a_{n_k}}}=\dfrac{e^{-n-1}}{e^{-n}}=\dfrac{1}{e}\notin \{0,1,\infty\}","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
66,A Banach space has an absolutely convergent basis $\iff$ it is topologically isomorphic to $\ell^1$,A Banach space has an absolutely convergent basis  it is topologically isomorphic to,\iff \ell^1,"$X$: Banach space. Then, $X$ has an absolutely convergent basis $\{x_n\}$ $\iff$ $X$ is tolopogically isomorphic to $\ell^1$ This is Exercise 4.14 in A Basis Theory Primer, Christopher Heil. I am just reading this book and not a homework. Proof: $(\implies)$ Let $\{x_n\}$ be an absolutely convergent basis for $X$. Let $T\colon \ell^1\to X$ be defined by $$y\mapsto Ty:=\sum_{n=1}^\infty\frac{y_n}{\|x_n\|_X}x_n,$$ for $y=(y_n)\in\ell^1$. We show that $T$ is linear continuous and $\mathrm{range}(T)=X$. Then, as a consequence of the Open Mapping Theorem the assertion will follow. (linearity) Take $y^{(1)},y^{(2)}\in\ell^1$ arbitrarily. Let $P_Ny:=(y_1,\dotsc,y_n,0,0,\dotsc)$. Then,  we have $\alpha_N:=T(P_Ny^{(1)}+P_Ny^{(2)})=T(P_Ny^{(1)})+T(P_Ny^{(2)})$, but it is easy to check both $T(y^{(1)}+y^{(2)})$ and $T(y^{(1)})+T(y^{(2)})$ are the limit of $\alpha_N$, and thus, checking the scaler multiplication similarly, the linearity follows. (continuity) We have $\|Ty\|_X\le \sum_{n=1}^\infty |y_n|\cdot1=\|y_n\|_{\ell^1}$. (onto) Let $x\in X$. Since $\{x_n\}$ is an absolutely convergent basis, we have the representation $x=\sum_{n}a_n(x)x_n$ with a coefficient functional $a_n$ such that $\sum_{n}|a_n(x)|\|x_n\|<\infty$. Letting $y_n:=a_n(x)\|x_n\|$ we have $y\in \ell^1$ and $Ty=x$. ($\impliedby$) With the standard basis $\{\delta_n\}$ where only the $n$-th component of $\delta_n=(0,\dotsc,0,1,0,\dotsc)$ is non-zero, clearly $y=\sum_{n}\delta_ny_n\in\ell^1$ is absolutely convergent, i.e., $\{\delta_n\}$ is an absolutely convergent basis. Let $\mathcal{T}\colon\ell^1\to X$ be an homeomorphism, and let $x_n:=\mathcal{T}y_n$. Then, $\{x_n\}$ is a basis (Lemma 4.18, Heil's book). Since $\mathcal{T}^{-1}x\in\ell^1$, because $\delta_n$ is a basis there exist a unique scalars such that $\mathcal{T}^{-1}x=\sum_n c_n \delta_n\in \ell^1$. Since $\delta_n$ is an absolutely convergent basis with $\|\delta_n\|=1$ we have $\sum_n|c_n|<\infty$.  From the continuity of $\mathcal{T}$ we have $$ x=\sum_n{c_n}\mathcal{T}\delta_n=\sum_n{c_n}x_n. $$ But  $$\sum_n|{c_n}|\|\mathcal{T}\delta_n\|\le \sup_{y\in\ell^1\, \|y\|=1}\|\mathcal{T}y\|\sum_n|{c_n}|<\infty,$$ and thus indeed $\{x_n\}$ is an absolutely convergent basis. I think the proof is alright, but what I don't really like about it is that I used Lemma 4.18 (p.140), which appears after Exercise 4.14 (p.139) in the book.  I don't think ""I wonder if I could show this just by using knowledge prior to Exercise 4.14"" is a good way of asking a question, as the book isn't available to everybody even though it is what I wish to ask. So, I wonder if I could show this using, results from Section 2, say Heil, Contents (pdf) where results from  elementary functional analysis are introduced.","$X$: Banach space. Then, $X$ has an absolutely convergent basis $\{x_n\}$ $\iff$ $X$ is tolopogically isomorphic to $\ell^1$ This is Exercise 4.14 in A Basis Theory Primer, Christopher Heil. I am just reading this book and not a homework. Proof: $(\implies)$ Let $\{x_n\}$ be an absolutely convergent basis for $X$. Let $T\colon \ell^1\to X$ be defined by $$y\mapsto Ty:=\sum_{n=1}^\infty\frac{y_n}{\|x_n\|_X}x_n,$$ for $y=(y_n)\in\ell^1$. We show that $T$ is linear continuous and $\mathrm{range}(T)=X$. Then, as a consequence of the Open Mapping Theorem the assertion will follow. (linearity) Take $y^{(1)},y^{(2)}\in\ell^1$ arbitrarily. Let $P_Ny:=(y_1,\dotsc,y_n,0,0,\dotsc)$. Then,  we have $\alpha_N:=T(P_Ny^{(1)}+P_Ny^{(2)})=T(P_Ny^{(1)})+T(P_Ny^{(2)})$, but it is easy to check both $T(y^{(1)}+y^{(2)})$ and $T(y^{(1)})+T(y^{(2)})$ are the limit of $\alpha_N$, and thus, checking the scaler multiplication similarly, the linearity follows. (continuity) We have $\|Ty\|_X\le \sum_{n=1}^\infty |y_n|\cdot1=\|y_n\|_{\ell^1}$. (onto) Let $x\in X$. Since $\{x_n\}$ is an absolutely convergent basis, we have the representation $x=\sum_{n}a_n(x)x_n$ with a coefficient functional $a_n$ such that $\sum_{n}|a_n(x)|\|x_n\|<\infty$. Letting $y_n:=a_n(x)\|x_n\|$ we have $y\in \ell^1$ and $Ty=x$. ($\impliedby$) With the standard basis $\{\delta_n\}$ where only the $n$-th component of $\delta_n=(0,\dotsc,0,1,0,\dotsc)$ is non-zero, clearly $y=\sum_{n}\delta_ny_n\in\ell^1$ is absolutely convergent, i.e., $\{\delta_n\}$ is an absolutely convergent basis. Let $\mathcal{T}\colon\ell^1\to X$ be an homeomorphism, and let $x_n:=\mathcal{T}y_n$. Then, $\{x_n\}$ is a basis (Lemma 4.18, Heil's book). Since $\mathcal{T}^{-1}x\in\ell^1$, because $\delta_n$ is a basis there exist a unique scalars such that $\mathcal{T}^{-1}x=\sum_n c_n \delta_n\in \ell^1$. Since $\delta_n$ is an absolutely convergent basis with $\|\delta_n\|=1$ we have $\sum_n|c_n|<\infty$.  From the continuity of $\mathcal{T}$ we have $$ x=\sum_n{c_n}\mathcal{T}\delta_n=\sum_n{c_n}x_n. $$ But  $$\sum_n|{c_n}|\|\mathcal{T}\delta_n\|\le \sup_{y\in\ell^1\, \|y\|=1}\|\mathcal{T}y\|\sum_n|{c_n}|<\infty,$$ and thus indeed $\{x_n\}$ is an absolutely convergent basis. I think the proof is alright, but what I don't really like about it is that I used Lemma 4.18 (p.140), which appears after Exercise 4.14 (p.139) in the book.  I don't think ""I wonder if I could show this just by using knowledge prior to Exercise 4.14"" is a good way of asking a question, as the book isn't available to everybody even though it is what I wish to ask. So, I wonder if I could show this using, results from Section 2, say Heil, Contents (pdf) where results from  elementary functional analysis are introduced.",,"['functional-analysis', 'analysis', 'proof-verification', 'alternative-proof', 'schauder-basis']"
67,Implicit Function Theorem Proof (Rudin),Implicit Function Theorem Proof (Rudin),,"Given $f\in\mathscr{C'}$ where $f$ is a map from the open set $E\subset R^{n+m}$ to $R^{n}$, $f(a,b)=0$ for some point $(a,b)\in E$. In the second part of the proof it says: Finally, to compute $g'(b)$, put $(g(y),y)=\phi(y)$. Then $$ \phi'(y)k=(g'(y)k,k)$$ for $y\in W=\{y\in R^m : (0,y)\in V\}, k\in R^m$ What $V$ or $W$ is isn't really important. My question is where does the k come from and why does it simply disappear later in the proof?","Given $f\in\mathscr{C'}$ where $f$ is a map from the open set $E\subset R^{n+m}$ to $R^{n}$, $f(a,b)=0$ for some point $(a,b)\in E$. In the second part of the proof it says: Finally, to compute $g'(b)$, put $(g(y),y)=\phi(y)$. Then $$ \phi'(y)k=(g'(y)k,k)$$ for $y\in W=\{y\in R^m : (0,y)\in V\}, k\in R^m$ What $V$ or $W$ is isn't really important. My question is where does the k come from and why does it simply disappear later in the proof?",,"['real-analysis', 'analysis', 'implicit-function-theorem']"
68,How to show complement of measurable set is measurable with this definition of measurability.,How to show complement of measurable set is measurable with this definition of measurability.,,"I've been working through an exercise which deals with showing that two definitions of a set being Lebesgue measurable are equivalent. The definition that I was thinking about was the following: $E \subseteq \mathbb{R}^{d}$ is measurable if for every $\epsilon$ there is a closed set $F$ contained in $E$ with $m_{*}(E-F)<\epsilon$. The $m_{*}$ is the outer measure obtained from taking the infimum of the total volume of rectangular covers. I am having trouble showing that the compliment $E^{c}$ is also measurable when $E$ is measurable My hunch is to show that $E^{c}$ is the intersection of such measurable sets. One observation I made is that for every $n \in \mathbb{N}$, we have a closed set $F_{n}$ such that $m_{*}(E-F_{n}) < \epsilon$ If we set $O_{n}=F_{n}^{c}$ and $S=\bigcap_{n=1} O_{n}$, I think we should be might be able to show that $E^{c}=S$ or use $S$ in some way to show that $E^{c}$ is measurable. I'm happy or accept to hear any alternative methods, but some help along the above lines would be very much appreciated.","I've been working through an exercise which deals with showing that two definitions of a set being Lebesgue measurable are equivalent. The definition that I was thinking about was the following: $E \subseteq \mathbb{R}^{d}$ is measurable if for every $\epsilon$ there is a closed set $F$ contained in $E$ with $m_{*}(E-F)<\epsilon$. The $m_{*}$ is the outer measure obtained from taking the infimum of the total volume of rectangular covers. I am having trouble showing that the compliment $E^{c}$ is also measurable when $E$ is measurable My hunch is to show that $E^{c}$ is the intersection of such measurable sets. One observation I made is that for every $n \in \mathbb{N}$, we have a closed set $F_{n}$ such that $m_{*}(E-F_{n}) < \epsilon$ If we set $O_{n}=F_{n}^{c}$ and $S=\bigcap_{n=1} O_{n}$, I think we should be might be able to show that $E^{c}=S$ or use $S$ in some way to show that $E^{c}$ is measurable. I'm happy or accept to hear any alternative methods, but some help along the above lines would be very much appreciated.",,"['analysis', 'measure-theory', 'lebesgue-measure']"
69,Showing lower-semi-continuity of a function.,Showing lower-semi-continuity of a function.,,"Let $f_\lambda: \mathbb{R} \rightarrow \mathbb{R} (\lambda \in \Lambda)$ be a family of continuous functions. Let  $$ F(x) = \sup_{\lambda \in \Lambda} f_\lambda (x), x \in \mathbb{R}.$$  Show that $F$ is lower-semi-continuous. My attempt: We must show that $\liminf_{x \rightarrow x_0} F(x) \geq F(x_0)$ for all $x_0 \in \mathbb{R}$. Since each $f_\lambda$ is continuous, it is also lower-semi-continuous. Thus, $$\liminf_{x \rightarrow x_0} f_\lambda(x) \geq f_\lambda (x_0).$$ Is it true then that $\liminf_{x \rightarrow x_0} \sup f_\lambda(x) \geq \sup f_\lambda (x_0)$? Why? Is there a way I should justify this? Is it because $\liminf_{x \rightarrow x_0} \sup f_\lambda(x) \geq \sup (\liminf_{x \rightarrow x_0} f_\lambda(x)) \geq \sup f_\lambda (x_0)?$","Let $f_\lambda: \mathbb{R} \rightarrow \mathbb{R} (\lambda \in \Lambda)$ be a family of continuous functions. Let  $$ F(x) = \sup_{\lambda \in \Lambda} f_\lambda (x), x \in \mathbb{R}.$$  Show that $F$ is lower-semi-continuous. My attempt: We must show that $\liminf_{x \rightarrow x_0} F(x) \geq F(x_0)$ for all $x_0 \in \mathbb{R}$. Since each $f_\lambda$ is continuous, it is also lower-semi-continuous. Thus, $$\liminf_{x \rightarrow x_0} f_\lambda(x) \geq f_\lambda (x_0).$$ Is it true then that $\liminf_{x \rightarrow x_0} \sup f_\lambda(x) \geq \sup f_\lambda (x_0)$? Why? Is there a way I should justify this? Is it because $\liminf_{x \rightarrow x_0} \sup f_\lambda(x) \geq \sup (\liminf_{x \rightarrow x_0} f_\lambda(x)) \geq \sup f_\lambda (x_0)?$",,"['real-analysis', 'analysis', 'proof-verification']"
70,"For $f,f_1 \ldots f_n \in L^1(X,M,\mu,\mathbb{R})$ suppose that $\sum_{n}\int |f_n-f| d\mu \lt \infty$ . Show that $f_n \to f ,\mu-$a.e",For  suppose that  . Show that a.e,"f,f_1 \ldots f_n \in L^1(X,M,\mu,\mathbb{R}) \sum_{n}\int |f_n-f| d\mu \lt \infty f_n \to f ,\mu-","Suppose that $(X,M,\mu)$ is a measure space. For $f,f_1 \ldots f_n \in L^1(X,M,\mu,\mathbb{R})$ suppose that $\sum_{n}\int |f_n-f| d\mu \lt \infty$ . Show that $f_n \to f, \mu-$a.e This is a problem from Cohn's Measure Theory (Section 3.1, Q4). My try: Let $\epsilon \gt 0$ be fixed. Suppose that $E_n^{\epsilon}=\{x \in X: |f_n(x)-f(x)| \ge \epsilon\}$. Then $$\infty \gt \sum \int|f_n-f|d\mu \ge \sum_{n}\int_{E_n}|f_n-f| \ge \epsilon\sum_{n}\mu(E_n^{\epsilon})$$ Thus $$\sum_{n} \mu(E_n^{\epsilon}) \lt \infty$$ Then for any $\delta \gt 0$, then there exists a $n_0(\delta)$ such that $\forall k \ge n_0(\delta)$ we have $$\sum_{j \ge n_0(\delta)}\mu(E_j^{\epsilon}) \lt \delta$$ Suppose that $$E_{\delta}^{\epsilon}=\cup_{j \ge n_0(\delta)}E_j^{\epsilon}, E^{\epsilon}=\cap_{k=1}^{\infty}E_{\frac{1}{k}}^{\epsilon} (\text{I am taking $\delta=\frac{1}{k}$)}$$ Then  $\mu(E^{\epsilon})=0$. As suggested in the answer, I am getting rid of the dependence on $\epsilon$. Let $$E=\cup_{j \in \mathbb{N}}E^{\frac{1}{j}}$$ It is clear that $\mu(E)=0$. Now let $\eta \gt 0$ be fixed. Then there is $i \in \mathbb{N}$ such that $\dfrac{1}{i} \lt \eta$. Then for $x \not \in E, x \not\in E^{\dfrac{1}{i}}$ which means that $x \not \in E_\dfrac{1}{k_1}^{\dfrac{1}{i}}, \forall j \ge n_0\left(\dfrac{1}{k_1}\right)$ .Thus $|f_n(x)-f(x)| \lt \dfrac{1}{i} \lt \eta, \forall n \ge n_0\left(\frac{1}{k_1}\right)$. I am sorry. The notation is a mess. I will edit it afterwards. Thanks for the help!!","Suppose that $(X,M,\mu)$ is a measure space. For $f,f_1 \ldots f_n \in L^1(X,M,\mu,\mathbb{R})$ suppose that $\sum_{n}\int |f_n-f| d\mu \lt \infty$ . Show that $f_n \to f, \mu-$a.e This is a problem from Cohn's Measure Theory (Section 3.1, Q4). My try: Let $\epsilon \gt 0$ be fixed. Suppose that $E_n^{\epsilon}=\{x \in X: |f_n(x)-f(x)| \ge \epsilon\}$. Then $$\infty \gt \sum \int|f_n-f|d\mu \ge \sum_{n}\int_{E_n}|f_n-f| \ge \epsilon\sum_{n}\mu(E_n^{\epsilon})$$ Thus $$\sum_{n} \mu(E_n^{\epsilon}) \lt \infty$$ Then for any $\delta \gt 0$, then there exists a $n_0(\delta)$ such that $\forall k \ge n_0(\delta)$ we have $$\sum_{j \ge n_0(\delta)}\mu(E_j^{\epsilon}) \lt \delta$$ Suppose that $$E_{\delta}^{\epsilon}=\cup_{j \ge n_0(\delta)}E_j^{\epsilon}, E^{\epsilon}=\cap_{k=1}^{\infty}E_{\frac{1}{k}}^{\epsilon} (\text{I am taking $\delta=\frac{1}{k}$)}$$ Then  $\mu(E^{\epsilon})=0$. As suggested in the answer, I am getting rid of the dependence on $\epsilon$. Let $$E=\cup_{j \in \mathbb{N}}E^{\frac{1}{j}}$$ It is clear that $\mu(E)=0$. Now let $\eta \gt 0$ be fixed. Then there is $i \in \mathbb{N}$ such that $\dfrac{1}{i} \lt \eta$. Then for $x \not \in E, x \not\in E^{\dfrac{1}{i}}$ which means that $x \not \in E_\dfrac{1}{k_1}^{\dfrac{1}{i}}, \forall j \ge n_0\left(\dfrac{1}{k_1}\right)$ .Thus $|f_n(x)-f(x)| \lt \dfrac{1}{i} \lt \eta, \forall n \ge n_0\left(\frac{1}{k_1}\right)$. I am sorry. The notation is a mess. I will edit it afterwards. Thanks for the help!!",,"['real-analysis', 'analysis', 'measure-theory']"
71,Application of the fundamental theorem of calculus in a step of a proof,Application of the fundamental theorem of calculus in a step of a proof,,"I am reading Elliptic Partial Differential Equations by Han and Lin. I can't understand a step of the proof of Theorem 1.16, that should follow from the Fundamental Theorem of Calculus. I'm sure it is trivial but I cant' see why. We have the following equality: \begin{equation} \int_{B_r(0)} u(y) \  \varphi_k(y, r)\ dy = 0 \end{equation} where $B_r(0)$ is the open ball of $\mathbb{R}^n$ centred in $0$ with radius $r >0$, $u$ is a continuous function defined on an open set $\Omega$ containing $B_r(0)$ and $\varphi_k(\cdot, r)$ is some $C^2_0(\Omega)$ function depending also on $r$. Now we want to derive the equation with respect to $r$. According to the book, we get: \begin{align} 0 &= \frac{d}{dr}\Big(\int_{B_r(0)} u(y) \  \varphi_k(y, r)\ dy \Big) \\ &=\int_{\partial B_r(0)}u(y) \ \varphi_k(y,r) \ dS_y + \int_{B_r(0)} u(y) \  \frac{\partial \varphi_k(y, r)}{\partial r}\ dy. \end{align} where $dS_y$ is the measure on the sphere $\partial B_r(0)$. Can you explain me this step? Thank you very much!","I am reading Elliptic Partial Differential Equations by Han and Lin. I can't understand a step of the proof of Theorem 1.16, that should follow from the Fundamental Theorem of Calculus. I'm sure it is trivial but I cant' see why. We have the following equality: \begin{equation} \int_{B_r(0)} u(y) \  \varphi_k(y, r)\ dy = 0 \end{equation} where $B_r(0)$ is the open ball of $\mathbb{R}^n$ centred in $0$ with radius $r >0$, $u$ is a continuous function defined on an open set $\Omega$ containing $B_r(0)$ and $\varphi_k(\cdot, r)$ is some $C^2_0(\Omega)$ function depending also on $r$. Now we want to derive the equation with respect to $r$. According to the book, we get: \begin{align} 0 &= \frac{d}{dr}\Big(\int_{B_r(0)} u(y) \  \varphi_k(y, r)\ dy \Big) \\ &=\int_{\partial B_r(0)}u(y) \ \varphi_k(y,r) \ dS_y + \int_{B_r(0)} u(y) \  \frac{\partial \varphi_k(y, r)}{\partial r}\ dy. \end{align} where $dS_y$ is the measure on the sphere $\partial B_r(0)$. Can you explain me this step? Thank you very much!",,"['real-analysis', 'integration', 'analysis']"
72,The set of points of boundedness of a function is open,The set of points of boundedness of a function is open,,"I was reading (self study) the book by Thomson Bruckner and Bruckner and ran into one of the exercises that I think I have the proof for by not sure it is right. The statement of the problem goes as follows. A function $f:\mathbb{R} \mapsto \mathbb{R}$ is said to be bounded at a point $x_o$ provided there are positive number $\epsilon$ and $M$ so that $|f(x)| < M$. for all $x \in (x_o - \epsilon, x_o + \epsilon)$. Show that the set of points at which a function is bounded is open. Let $E$ be an arbitrary closed set. Is it possible to construct a function $f:\mathbb{R} \mapsto \mathbb{R}$ so that the set of points at which f is not bounded is precisely the set E? My Proof of the first section of the problem was as follows. Let $B_f = \{x : |f(x)| < M, M > 0 \}$ the set of all points where $f$ is bounded (note $M = sup(\{M_x : x \in \mathbb{R}, |f(x)| < M_x\})$ over each $x$, This $sup(\{M_x : x \in \mathbb{R}, |f(x)| < M_x\})$ is bounded and exists, since each $M_x$ is bounded). Then if for every $x \in B_f$ we have by our definition of bounded function at a point, $\forall x \in B_f \ \exists \epsilon > 0 \text{ and } \forall x \in (x - \epsilon, x + \epsilon)$ we have $|f(x)| < M_x$, but this would mean $(x - \epsilon, x + \epsilon) \subset B_f$ which means that $x$ is a interior point of $B_f$. The conclusion follows that $B_f$ is open. I have no clue how to construct the function for the second half of the question. I was wondering if my proof is right? If someone can provide an example for the second part of the question I would be extremely thankful.","I was reading (self study) the book by Thomson Bruckner and Bruckner and ran into one of the exercises that I think I have the proof for by not sure it is right. The statement of the problem goes as follows. A function $f:\mathbb{R} \mapsto \mathbb{R}$ is said to be bounded at a point $x_o$ provided there are positive number $\epsilon$ and $M$ so that $|f(x)| < M$. for all $x \in (x_o - \epsilon, x_o + \epsilon)$. Show that the set of points at which a function is bounded is open. Let $E$ be an arbitrary closed set. Is it possible to construct a function $f:\mathbb{R} \mapsto \mathbb{R}$ so that the set of points at which f is not bounded is precisely the set E? My Proof of the first section of the problem was as follows. Let $B_f = \{x : |f(x)| < M, M > 0 \}$ the set of all points where $f$ is bounded (note $M = sup(\{M_x : x \in \mathbb{R}, |f(x)| < M_x\})$ over each $x$, This $sup(\{M_x : x \in \mathbb{R}, |f(x)| < M_x\})$ is bounded and exists, since each $M_x$ is bounded). Then if for every $x \in B_f$ we have by our definition of bounded function at a point, $\forall x \in B_f \ \exists \epsilon > 0 \text{ and } \forall x \in (x - \epsilon, x + \epsilon)$ we have $|f(x)| < M_x$, but this would mean $(x - \epsilon, x + \epsilon) \subset B_f$ which means that $x$ is a interior point of $B_f$. The conclusion follows that $B_f$ is open. I have no clue how to construct the function for the second half of the question. I was wondering if my proof is right? If someone can provide an example for the second part of the question I would be extremely thankful.",,['analysis']
73,Question about Fourier transform and its itegrability,Question about Fourier transform and its itegrability,,"Let $n\geq 1$ an integer. Let $f\in L^1(\mathbb{R}^n)$ be an integrable function. Then its Fourier transform $\widehat{f}$ is defined as $$\widehat{f}(\xi)= \int_{\mathbb{R}^n} f(x) e^{-2\pi i x\cdot \xi} dx,$$ where $\cdot$ denotes the scalar product. The Fourier transform has many properties and there is an extensive theory about it. It is known that if $\widehat{f}$ integrates against polynomials then this implies that $f$ is differentiable up to some order. My question is the following: If $f$ is such that $\widehat{f}\in L^p(\mathbb{R}^n)$ for some $p>1$. Does this tell us anything about $f$ itself? Like for instance if $f$ is bounded, continuous or any other thing? Thanks a lot! :)","Let $n\geq 1$ an integer. Let $f\in L^1(\mathbb{R}^n)$ be an integrable function. Then its Fourier transform $\widehat{f}$ is defined as $$\widehat{f}(\xi)= \int_{\mathbb{R}^n} f(x) e^{-2\pi i x\cdot \xi} dx,$$ where $\cdot$ denotes the scalar product. The Fourier transform has many properties and there is an extensive theory about it. It is known that if $\widehat{f}$ integrates against polynomials then this implies that $f$ is differentiable up to some order. My question is the following: If $f$ is such that $\widehat{f}\in L^p(\mathbb{R}^n)$ for some $p>1$. Does this tell us anything about $f$ itself? Like for instance if $f$ is bounded, continuous or any other thing? Thanks a lot! :)",,"['real-analysis', 'functional-analysis', 'analysis', 'fourier-analysis', 'fourier-transform']"
74,Equivalent form of prime number theorem,Equivalent form of prime number theorem,,"In the book The elementary proof of Prime Number Theorem it says that The prime number theorem $\psi (x)\sim x$ is equivalent to $\int_1^\infty \frac{\psi (t)-t}{t^2} =-\gamma -1$, where $\psi (x)=\sum_{n\le x} \Lambda (n)$, $\gamma $ is the Euler constant. The book also gives a hint to prove $\sum_{n\le x}\frac{\Lambda (n)}{n}=\int_1^x \frac{\psi(t)}{t^2} dt+\frac{\psi (x)}{x}$ first, but I have no idea on how to prove this hint or prove the original equivalence. All I know now is the formula in this question $\sum_{n\le x}\frac{\Lambda (n)}{n}=\ln x-\gamma +o(1)$ which is exactly another form of prime number theorem and the relation $\psi (x)=x-1+o(x)-\gamma -\int_1^x o(t)dt$ proved before in the book. Can the proof obtain from these two identites or I need to find a new approach? Thanks in advance.","In the book The elementary proof of Prime Number Theorem it says that The prime number theorem $\psi (x)\sim x$ is equivalent to $\int_1^\infty \frac{\psi (t)-t}{t^2} =-\gamma -1$, where $\psi (x)=\sum_{n\le x} \Lambda (n)$, $\gamma $ is the Euler constant. The book also gives a hint to prove $\sum_{n\le x}\frac{\Lambda (n)}{n}=\int_1^x \frac{\psi(t)}{t^2} dt+\frac{\psi (x)}{x}$ first, but I have no idea on how to prove this hint or prove the original equivalence. All I know now is the formula in this question $\sum_{n\le x}\frac{\Lambda (n)}{n}=\ln x-\gamma +o(1)$ which is exactly another form of prime number theorem and the relation $\psi (x)=x-1+o(x)-\gamma -\int_1^x o(t)dt$ proved before in the book. Can the proof obtain from these two identites or I need to find a new approach? Thanks in advance.",,"['analysis', 'number-theory', 'prime-numbers', 'analytic-number-theory']"
75,Leading behavior of the integral $\int_0^{\pi/2}\sqrt{\sin t}e^{-x\sin^4t}dt$ as $x\to\infty$.,Leading behavior of the integral  as .,\int_0^{\pi/2}\sqrt{\sin t}e^{-x\sin^4t}dt x\to\infty,"$$\displaystyle\lim_{x\to\infty}\int_0^{\pi/2}\sqrt{\sin t}e^{-x\sin^4t}dt$$ I want to apply Laplace's method. And since the maximal of $-\sin^4t$ achieves at $t=0$, the contribution to the integral is at 0, but at the same time $\sqrt{\sin t}$ attains zero. So does that mean the leading behavior of the integral is just $0$? If so, how can we rigorously(kind of) show that?","$$\displaystyle\lim_{x\to\infty}\int_0^{\pi/2}\sqrt{\sin t}e^{-x\sin^4t}dt$$ I want to apply Laplace's method. And since the maximal of $-\sin^4t$ achieves at $t=0$, the contribution to the integral is at 0, but at the same time $\sqrt{\sin t}$ attains zero. So does that mean the leading behavior of the integral is just $0$? If so, how can we rigorously(kind of) show that?",,"['analysis', 'asymptotics', 'laplace-method']"
76,"$F(x_1,\ldots,x_n) \in C(\mathbb{R}^n)$, $f_1(t),\ldots,f_n(t)$ measurable on $\mathbb{R} \implies$ so is $F(f_1(t),\ldots,f_n(t))$",",  measurable on  so is","F(x_1,\ldots,x_n) \in C(\mathbb{R}^n) f_1(t),\ldots,f_n(t) \mathbb{R} \implies F(f_1(t),\ldots,f_n(t))","$F(x_1,\ldots,x_n) $ continuous on $\mathbb{R}^n$ and $f_1(t),\ldots,f_n(t)$ measurable on $\mathbb{R} \implies F(f_1(t),\ldots,f_n(t))$ measurable on $\mathbb{R}.$ When it says that  $F(x_1,...x_n) $ is continuous on $\mathbb{R}^n$ that must mean that $F:\mathbb{R}^n\to\mathbb{R}$. I have the defition that a function $F:\mathbb{X}\to \mathbb{R}$ is measurable iff $A=\{x \in \mathbb{X}: F(x)>\alpha, \alpha \in \mathbb{R}\} \in B(\mathbb{X})$ - Borel set of $\mathbb{X}$ I am having difficulties with this, because this composition component is because me problems, I have proved that functions $f: \mathbb{R}\to \mathbb{R}$ that are continuous are measurable becauase I saw that $f^{-1}(\alpha,\infty)$ was an open set, because the inverse mapping of  continuous function on an open  set is indeed open. I would think, something similar would come into play here.","$F(x_1,\ldots,x_n) $ continuous on $\mathbb{R}^n$ and $f_1(t),\ldots,f_n(t)$ measurable on $\mathbb{R} \implies F(f_1(t),\ldots,f_n(t))$ measurable on $\mathbb{R}.$ When it says that  $F(x_1,...x_n) $ is continuous on $\mathbb{R}^n$ that must mean that $F:\mathbb{R}^n\to\mathbb{R}$. I have the defition that a function $F:\mathbb{X}\to \mathbb{R}$ is measurable iff $A=\{x \in \mathbb{X}: F(x)>\alpha, \alpha \in \mathbb{R}\} \in B(\mathbb{X})$ - Borel set of $\mathbb{X}$ I am having difficulties with this, because this composition component is because me problems, I have proved that functions $f: \mathbb{R}\to \mathbb{R}$ that are continuous are measurable becauase I saw that $f^{-1}(\alpha,\infty)$ was an open set, because the inverse mapping of  continuous function on an open  set is indeed open. I would think, something similar would come into play here.",,"['calculus', 'analysis', 'measure-theory']"
77,Prove a series that equals to $\frac{e}{e-1}$,Prove a series that equals to,\frac{e}{e-1},"Prove that  $$ \lim_{N\to\infty}\sum_{k=0}^\infty \left(1+\frac{k}{N}\right)^{-N}=\frac{e}{e-1} $$ I think $\sum_{k=0}^\infty \left(1+\frac{k}{N}\right)^{-N}$ should be a Riemann sum of a function but could find it. What is the trick in this question? In addition, the equation holds true when it could interchange the limits, but how to prove it?","Prove that  $$ \lim_{N\to\infty}\sum_{k=0}^\infty \left(1+\frac{k}{N}\right)^{-N}=\frac{e}{e-1} $$ I think $\sum_{k=0}^\infty \left(1+\frac{k}{N}\right)^{-N}$ should be a Riemann sum of a function but could find it. What is the trick in this question? In addition, the equation holds true when it could interchange the limits, but how to prove it?",,['analysis']
78,Name of technique for determining the number of eigenvalues larger than some limit,Name of technique for determining the number of eigenvalues larger than some limit,,"Suppose $A$ is a real symmetric positive definite matrix with eigenvalues $\lambda_1,...,\lambda_n > 0$ (which we do not know). If one wants to know how many eigenvalues $A$ has above some limit $s \neq \lambda_i$, one can study the quadratic form defined by the matrix $(A-sI_n)$, diagonalise it to find the number of positive eigenvalues and use Sylvester's law of inertia. For example, if $(A-sI_n)$ has one positive eigenvalue, then $A$ has exactly one eigenvalue larger than $s$. That is, the technique allows you to find the number of eigenvalues above some limit without actually computing the eigenvalues, and in order to work, the limit itself must not be an eigenvalue. The literal translation of the name of this technique in my native language would be ""spectral cleaving"" or ""spectral splitting"". However, this doesn't seem to be the correct terminology in English. Any suggestions?","Suppose $A$ is a real symmetric positive definite matrix with eigenvalues $\lambda_1,...,\lambda_n > 0$ (which we do not know). If one wants to know how many eigenvalues $A$ has above some limit $s \neq \lambda_i$, one can study the quadratic form defined by the matrix $(A-sI_n)$, diagonalise it to find the number of positive eigenvalues and use Sylvester's law of inertia. For example, if $(A-sI_n)$ has one positive eigenvalue, then $A$ has exactly one eigenvalue larger than $s$. That is, the technique allows you to find the number of eigenvalues above some limit without actually computing the eigenvalues, and in order to work, the limit itself must not be an eigenvalue. The literal translation of the name of this technique in my native language would be ""spectral cleaving"" or ""spectral splitting"". However, this doesn't seem to be the correct terminology in English. Any suggestions?",,"['linear-algebra', 'matrices', 'analysis', 'reference-request', 'terminology']"
79,What is the correct definition for positive operator and positive definite operator?,What is the correct definition for positive operator and positive definite operator?,,"As far as I know those operators are defined as follows: Positive operator is an operator $L: H\rightarrow H$ such that $\langle L\textbf u|\textbf u\rangle \geq0$ for all $\textbf u \in H$ and the equality could be achieved even for nonzero $\textbf u$ -s. Positive definite operator is an operator $L: H\rightarrow H$ such that $\langle L\textbf u|\textbf u\rangle \geq 0$ for all $\textbf u \in H$ where we have equality only if $\textbf u=\textbf 0$ . Or equivalently said $L: H\rightarrow H$ is positive definite operator if $\langle L\textbf u|\textbf u\rangle >0$ for all nonzero $\textbf u \in H$ . But according to a book I read (can't link to the book since it is in Bulgarian not English) the definitions are: Positive operator is an operator $L: H\rightarrow H$ such that $\langle L\textbf u|\textbf u\rangle \geq 0$ for all $\textbf u \in H$ where we have equality only if $\textbf u=\textbf 0$ . Note that this is the definition of positive definite operator as far as my reasoning above is correct. The operator $L: H\rightarrow H$ is positive definite if there is a constant $\gamma >0$ such that $\langle L\textbf u|\textbf u\rangle \geq \gamma \langle \textbf u|\textbf u\rangle$ for all $\textbf u \in H$ . Note that this definition says nothing about the case $\textbf u=\textbf 0$ . I am confused with the definitions in the book because it is known that every positive definite operator is a positive operator, while the opposite is not true. But according to the books' definitions I interpret that there is no difference between positive definite and positive operator, i.e. they are equivalent. So which definition is the correct one?","As far as I know those operators are defined as follows: Positive operator is an operator such that for all and the equality could be achieved even for nonzero -s. Positive definite operator is an operator such that for all where we have equality only if . Or equivalently said is positive definite operator if for all nonzero . But according to a book I read (can't link to the book since it is in Bulgarian not English) the definitions are: Positive operator is an operator such that for all where we have equality only if . Note that this is the definition of positive definite operator as far as my reasoning above is correct. The operator is positive definite if there is a constant such that for all . Note that this definition says nothing about the case . I am confused with the definitions in the book because it is known that every positive definite operator is a positive operator, while the opposite is not true. But according to the books' definitions I interpret that there is no difference between positive definite and positive operator, i.e. they are equivalent. So which definition is the correct one?",L: H\rightarrow H \langle L\textbf u|\textbf u\rangle \geq0 \textbf u \in H \textbf u L: H\rightarrow H \langle L\textbf u|\textbf u\rangle \geq 0 \textbf u \in H \textbf u=\textbf 0 L: H\rightarrow H \langle L\textbf u|\textbf u\rangle >0 \textbf u \in H L: H\rightarrow H \langle L\textbf u|\textbf u\rangle \geq 0 \textbf u \in H \textbf u=\textbf 0 L: H\rightarrow H \gamma >0 \langle L\textbf u|\textbf u\rangle \geq \gamma \langle \textbf u|\textbf u\rangle \textbf u \in H \textbf u=\textbf 0,"['linear-algebra', 'analysis', 'operator-theory', 'definition']"
80,Proof that the following function is a polynomial,Proof that the following function is a polynomial,,"I've been trying to get my head around this problem for a long time, yet I have not been able to make much progress. Let $\ell_0(j) = \left\lfloor \frac{1}{2}\left( \sqrt{8j^2 - 8j + 1} + 2j - 1 \right) \right\rfloor$. Then, prove that the following function $P(j,s)$ is a polynomial in $s$ for all positive integers $j$: $$P(j,s) = \frac{1}{1 - s^{\ell_0(j)}}\sum_{\ell =1}^{\ell_0(j)}\left((-1)^\ell \left( \frac{s^{\ell(\ell+3)/2 - (j+1)\ell - j(j-1)/2}}{1-s^\ell}  \right) \prod_{k=\ell}^{\ell_0(j)}(1-s^k)^2\right) $$ If anybody could suggest anything, I would greatly appreciate it. Thanks!","I've been trying to get my head around this problem for a long time, yet I have not been able to make much progress. Let $\ell_0(j) = \left\lfloor \frac{1}{2}\left( \sqrt{8j^2 - 8j + 1} + 2j - 1 \right) \right\rfloor$. Then, prove that the following function $P(j,s)$ is a polynomial in $s$ for all positive integers $j$: $$P(j,s) = \frac{1}{1 - s^{\ell_0(j)}}\sum_{\ell =1}^{\ell_0(j)}\left((-1)^\ell \left( \frac{s^{\ell(\ell+3)/2 - (j+1)\ell - j(j-1)/2}}{1-s^\ell}  \right) \prod_{k=\ell}^{\ell_0(j)}(1-s^k)^2\right) $$ If anybody could suggest anything, I would greatly appreciate it. Thanks!",,"['abstract-algebra', 'analysis', 'polynomials', 'mathematical-physics', 'classical-mechanics']"
81,Continuously differentiable function injective on convex set,Continuously differentiable function injective on convex set,,"Can you help me solve the following exercise: (a) Let $n\in \mathbb N$ and $G \subset \mathbb R^n$ a convex set, $f:G\to \mathbb R^n$ continuously differentiable with   $$det\left(\begin{matrix} \frac{\partial f_1}{\partial x_1}(c_1) \,\cdot\cdot\,\cdot \frac{\partial f_1}{\partial x_n}(c_1) \\ .\quad.\quad. \\ .\quad.\quad. \\ \frac{\partial f_n}{\partial x_1}(c_n) \,\cdot\cdot\,\cdot \frac{\partial f_n}{\partial x_n}(c_n)\end{matrix}\right) \neq 0 \quad \text{for all } c_1,c_2,...,c_n \in G,$$   Show that f is injective. (b) We define $$g:(0,\infty)^2\to \mathbb R^2; \quad g(x) := \begin{pmatrix} x^2_1-x^2_2 \\ 2x_1x_2 \end{pmatrix}.$$   Investigate the injectivity of $g$ by applying (a). Since the determinant is $\neq 0$ it is invertible on some nbhd of $c_n$. However, I don't know how to use that or the convexity property to show $f$ is injective.  If I know that, (b) should be trivial.","Can you help me solve the following exercise: (a) Let $n\in \mathbb N$ and $G \subset \mathbb R^n$ a convex set, $f:G\to \mathbb R^n$ continuously differentiable with   $$det\left(\begin{matrix} \frac{\partial f_1}{\partial x_1}(c_1) \,\cdot\cdot\,\cdot \frac{\partial f_1}{\partial x_n}(c_1) \\ .\quad.\quad. \\ .\quad.\quad. \\ \frac{\partial f_n}{\partial x_1}(c_n) \,\cdot\cdot\,\cdot \frac{\partial f_n}{\partial x_n}(c_n)\end{matrix}\right) \neq 0 \quad \text{for all } c_1,c_2,...,c_n \in G,$$   Show that f is injective. (b) We define $$g:(0,\infty)^2\to \mathbb R^2; \quad g(x) := \begin{pmatrix} x^2_1-x^2_2 \\ 2x_1x_2 \end{pmatrix}.$$   Investigate the injectivity of $g$ by applying (a). Since the determinant is $\neq 0$ it is invertible on some nbhd of $c_n$. However, I don't know how to use that or the convexity property to show $f$ is injective.  If I know that, (b) should be trivial.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'convex-analysis']"
82,Test function with bounded gradient,Test function with bounded gradient,,How to construct a test function (radial) which is zero outside a ball of radius $2r$ and $1$ on the ball of radius $r$ but the gradient is bounded by $\dfrac{1}{r}$ ?,How to construct a test function (radial) which is zero outside a ball of radius and on the ball of radius but the gradient is bounded by ?,2r 1 r \dfrac{1}{r},"['analysis', 'partial-differential-equations']"
83,Infimum of lower semicontinuous functions,Infimum of lower semicontinuous functions,,"The following proposition is from the book Nicolae Dinculeanu Integration on Locally Compact Spaces: Let $H$ and $K$ be two compact Hausdorff spaces and $\alpha$ a continuous mapping of $H$ onto $K$. If $h\geq 0$ is a lower semicontinuous function on $H$ then the function $g(s)=\inf_{\alpha(t)=s}h(t)$ is lower semicontinuous on $K$. If $\alpha$ is one-to-one then it is not that hard to prove this proposition, but $\alpha$ here is not necessary one-to-one, can anyone suggest a proof? The following proof is from the author, the bold text is where I get confused, does the author do the quotient relation so we can change the $\alpha$ to be one-to-one? And it seems that the condition that $h\geq 0$ is redundant in the following proof. We remark first that for every point $s\in K$ there exists a point $t_{s}\in H$ such that $\alpha(t_{s})=s$ and $g(s)=h(t_{s})$. In fact, $\{s\}$ is a closed set in $K$ and $\alpha$ is continuous on $H$, therefore $\alpha^{-1}(s)$ is closed in $H$ and hence compact. Since $h$ is lower semicontinuous and $\alpha^{-1}(s)$ is compact, there exists a point $t_{s}\in \alpha^{-1}(s)$ such that $h(t_{s})=\inf_{\alpha(t)=s}h(t)$, hence $g(s)=h(t_{s})$. Let now $a$ be an arbitrary real number and prove that the set $A=\{s\in K: g(s)>a\}$ is open. It will then follow that $g$ is lower semicontinuous. Let $s_{0}\in A$. We have $g(s_{0})>a$, therefore $h(t)>a$ for every $t\in\alpha^{-1}(s_{0})$. Since $h$ is lower semicontinuous, there exists a neighbourhood $V$ of $\alpha^{-1}(s_{0})$ such that $h(t)>a$ for every $t\in V$. In addition, we can consider the set $V$ saturated for the equivalence relation $\alpha(t)=\alpha(t')$. Then $\alpha(V)$ is a neighbourhood of $s_{0}$. For every $s\in\alpha(V)$ there exists $t\in\alpha^{-1}(s)$ such that $h(t)=g(s)$. Therefore, we have $g(s)>a$ for every $s\in\alpha(V)$, hence $A$ is open.","The following proposition is from the book Nicolae Dinculeanu Integration on Locally Compact Spaces: Let $H$ and $K$ be two compact Hausdorff spaces and $\alpha$ a continuous mapping of $H$ onto $K$. If $h\geq 0$ is a lower semicontinuous function on $H$ then the function $g(s)=\inf_{\alpha(t)=s}h(t)$ is lower semicontinuous on $K$. If $\alpha$ is one-to-one then it is not that hard to prove this proposition, but $\alpha$ here is not necessary one-to-one, can anyone suggest a proof? The following proof is from the author, the bold text is where I get confused, does the author do the quotient relation so we can change the $\alpha$ to be one-to-one? And it seems that the condition that $h\geq 0$ is redundant in the following proof. We remark first that for every point $s\in K$ there exists a point $t_{s}\in H$ such that $\alpha(t_{s})=s$ and $g(s)=h(t_{s})$. In fact, $\{s\}$ is a closed set in $K$ and $\alpha$ is continuous on $H$, therefore $\alpha^{-1}(s)$ is closed in $H$ and hence compact. Since $h$ is lower semicontinuous and $\alpha^{-1}(s)$ is compact, there exists a point $t_{s}\in \alpha^{-1}(s)$ such that $h(t_{s})=\inf_{\alpha(t)=s}h(t)$, hence $g(s)=h(t_{s})$. Let now $a$ be an arbitrary real number and prove that the set $A=\{s\in K: g(s)>a\}$ is open. It will then follow that $g$ is lower semicontinuous. Let $s_{0}\in A$. We have $g(s_{0})>a$, therefore $h(t)>a$ for every $t\in\alpha^{-1}(s_{0})$. Since $h$ is lower semicontinuous, there exists a neighbourhood $V$ of $\alpha^{-1}(s_{0})$ such that $h(t)>a$ for every $t\in V$. In addition, we can consider the set $V$ saturated for the equivalence relation $\alpha(t)=\alpha(t')$. Then $\alpha(V)$ is a neighbourhood of $s_{0}$. For every $s\in\alpha(V)$ there exists $t\in\alpha^{-1}(s)$ such that $h(t)=g(s)$. Therefore, we have $g(s)>a$ for every $s\in\alpha(V)$, hence $A$ is open.",,"['real-analysis', 'general-topology', 'functional-analysis', 'analysis', 'measure-theory']"
84,"Error term between $f(x)$, its average value and value at midpoint","Error term between , its average value and value at midpoint",f(x),"Let $f$ be a smooth function on interval $[a,b]$. Define the average $\bar{f}=\dfrac{1}{b-a}\int_a^bf(y)\,dy$ and $\bar{x}=\dfrac{a+b}{2}$, then for any $x\in [a,b]$, we can write $$f(x)-\bar{f}=c(x-\bar{x})+E,$$ where $c$ is something related to $f$ and $E$ is the error term. I want to know whether it's possible to get $E=O(b-a)^2$. By $O(b-a)^2$, I mean $\lim_{b-a\to 0}\frac{|E|}{(b-a)^2}\le C$ for some constant $C>0$. What I tried: By Taylor expansion, we have $$f(x)=f(\bar{x})+f'(\bar{x})(x-\bar{x})+f''(\xi)(x-\bar{x})^2.$$ Then $f(x)-\bar{f}=f'(\bar{x})(x-\bar{x})+f''(\xi)(x-\bar{x})^2+f(\bar{x})-\bar{f}$. Hence it remains to study $f(\bar{x})-\bar{f}$. But by mean value theorem, we know  $$f(\bar{x})-\bar{f}=f(\bar{x})-f(c)=f'(\eta)(c-\bar{x}),$$ which only gives me the first order approximation. Is there any way to get a better result?","Let $f$ be a smooth function on interval $[a,b]$. Define the average $\bar{f}=\dfrac{1}{b-a}\int_a^bf(y)\,dy$ and $\bar{x}=\dfrac{a+b}{2}$, then for any $x\in [a,b]$, we can write $$f(x)-\bar{f}=c(x-\bar{x})+E,$$ where $c$ is something related to $f$ and $E$ is the error term. I want to know whether it's possible to get $E=O(b-a)^2$. By $O(b-a)^2$, I mean $\lim_{b-a\to 0}\frac{|E|}{(b-a)^2}\le C$ for some constant $C>0$. What I tried: By Taylor expansion, we have $$f(x)=f(\bar{x})+f'(\bar{x})(x-\bar{x})+f''(\xi)(x-\bar{x})^2.$$ Then $f(x)-\bar{f}=f'(\bar{x})(x-\bar{x})+f''(\xi)(x-\bar{x})^2+f(\bar{x})-\bar{f}$. Hence it remains to study $f(\bar{x})-\bar{f}$. But by mean value theorem, we know  $$f(\bar{x})-\bar{f}=f(\bar{x})-f(c)=f'(\eta)(c-\bar{x}),$$ which only gives me the first order approximation. Is there any way to get a better result?",,"['analysis', 'derivatives', 'continuity']"
85,For which values of $p$ the series $\sum_{n = 2}^{\infty}\frac{1}{\ln^p{n}}$ converges?,For which values of  the series  converges?,p \sum_{n = 2}^{\infty}\frac{1}{\ln^p{n}},"I'm trying to find all values of $p$ for which the following series converges: $$\sum_{n = 2}^{\infty}\frac{1}{\ln^p{n}}$$ So my first approach was to use the integral test because $\frac{1}{\ln^p{n}}$ is monotone decreasing, continuous and non-negative, but I don't know how to calculate such an integral... In my second approach I found that in the interval $[0, 1]$, we have $\frac{1}{n} \leq \frac{1}{\ln^p n}$ and I know that $\frac{1}{n}$ diverges and also non-negative so for $p \in [0, 1]$ the series diverges. But that's not enough... How should I solve this?","I'm trying to find all values of $p$ for which the following series converges: $$\sum_{n = 2}^{\infty}\frac{1}{\ln^p{n}}$$ So my first approach was to use the integral test because $\frac{1}{\ln^p{n}}$ is monotone decreasing, continuous and non-negative, but I don't know how to calculate such an integral... In my second approach I found that in the interval $[0, 1]$, we have $\frac{1}{n} \leq \frac{1}{\ln^p n}$ and I know that $\frac{1}{n}$ diverges and also non-negative so for $p \in [0, 1]$ the series diverges. But that's not enough... How should I solve this?",,"['calculus', 'real-analysis', 'analysis', 'convergence-divergence', 'summation']"
86,How can I prove $\sqrt{a^{2}}\le \sqrt{a^{2}+b^{2}}$ if $a^{2}\le a^{2}+b^{2}$?,How can I prove  if ?,\sqrt{a^{2}}\le \sqrt{a^{2}+b^{2}} a^{2}\le a^{2}+b^{2},"Given that $$a^{2}\le a^{2}+b^{2},$$ how can I prove that $$\sqrt{a^{2}}\le \sqrt{a^{2}+b^{2}}$$ strictly from the definition of an ordered field ? A derivation step by step would be appreciated.","Given that $$a^{2}\le a^{2}+b^{2},$$ how can I prove that $$\sqrt{a^{2}}\le \sqrt{a^{2}+b^{2}}$$ strictly from the definition of an ordered field ? A derivation step by step would be appreciated.",,"['real-analysis', 'analysis', 'inequality', 'radicals', 'ordered-fields']"
87,show that continuous functions on $\mathbb{R}$ are measurable,show that continuous functions on  are measurable,\mathbb{R},"I am trying to show this using the theorem: A function $f: \Omega \to \mathbb{R}$ is measurable if and only if $f^{-1}(E) \in \mathcal{F}$ for all borel sets $E$. The proof to show a continuous function is measurable says: $\{x : f(x) > c \}$ is open, and hence Borel and hence Lebesgue measurable. Now, I am having troubles with this because I don't think I understand exactly what measurable means. We have a sigma algebra $\mathcal{F}$, and we say $f$ is measurable if $\{x : f(x) > c \} \in \mathcal{F}$ for every $ c \in \mathbb{R}$ - what does $\{x : f(x) > c \} \in \mathcal{F}$ mean exactly? In the example showing a continuous function is measurable, I understand that $\{x: f(x) > c\} = f^{-1}((c,\infty])$ and since $f$ is continuous, $f^{-1}((c,\infty])$ is open, so it is Borel - but how does this included that $f^{-1}((c,\infty])\in \mathcal{F}?$ surely this is only true if $\mathcal{F}$ is the Borel sigma algebra, but the question has not specified it was, and I am assuming $\mathcal{F}$ is a general $\sigma-$algebra. Thanks.","I am trying to show this using the theorem: A function $f: \Omega \to \mathbb{R}$ is measurable if and only if $f^{-1}(E) \in \mathcal{F}$ for all borel sets $E$. The proof to show a continuous function is measurable says: $\{x : f(x) > c \}$ is open, and hence Borel and hence Lebesgue measurable. Now, I am having troubles with this because I don't think I understand exactly what measurable means. We have a sigma algebra $\mathcal{F}$, and we say $f$ is measurable if $\{x : f(x) > c \} \in \mathcal{F}$ for every $ c \in \mathbb{R}$ - what does $\{x : f(x) > c \} \in \mathcal{F}$ mean exactly? In the example showing a continuous function is measurable, I understand that $\{x: f(x) > c\} = f^{-1}((c,\infty])$ and since $f$ is continuous, $f^{-1}((c,\infty])$ is open, so it is Borel - but how does this included that $f^{-1}((c,\infty])\in \mathcal{F}?$ surely this is only true if $\mathcal{F}$ is the Borel sigma algebra, but the question has not specified it was, and I am assuming $\mathcal{F}$ is a general $\sigma-$algebra. Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'continuity']"
88,What is the advantage (if any) of neighborhoods which are not open?,What is the advantage (if any) of neighborhoods which are not open?,,"From wikipedia If $X$ is a topological space and $p$ is a point in $X$, a neighbourhood of $p$ is a subset $V$ of $X$ that includes an open set $U$ containing $p$, $$p \in U \subseteq V.$$ What is the added advantage of defining the neighborhood to be $V$, which need not be open, as opposed to defining the neighborhood to be the open set $U$ from the start?","From wikipedia If $X$ is a topological space and $p$ is a point in $X$, a neighbourhood of $p$ is a subset $V$ of $X$ that includes an open set $U$ containing $p$, $$p \in U \subseteq V.$$ What is the added advantage of defining the neighborhood to be $V$, which need not be open, as opposed to defining the neighborhood to be the open set $U$ from the start?",,"['general-topology', 'analysis']"
89,Is this a Schwartz function?,Is this a Schwartz function?,,"I would like to know whether this function $f : \mathbb{R} \rightarrow \mathbb{R}$  $$f(x):=\frac{1}{\sum_{k=0}^{\infty} \frac{x^{2k}}{(k!)^2}}$$ is a Schwartz function? By applying the chain-rule it is clear that $f \in C^{\infty}(\mathbb{R})$ and more or less obviously we have that $|x^a f(x)| \rightarrow 0$  for $x \rightarrow \pm \infty$. But what about derivatives? How can I show that $|x^{a} f^{(\beta)} (x)| \rightarrow 0$ for $x \rightarrow \pm \infty$ or is this false? If anything is unclear, please let me know.","I would like to know whether this function $f : \mathbb{R} \rightarrow \mathbb{R}$  $$f(x):=\frac{1}{\sum_{k=0}^{\infty} \frac{x^{2k}}{(k!)^2}}$$ is a Schwartz function? By applying the chain-rule it is clear that $f \in C^{\infty}(\mathbb{R})$ and more or less obviously we have that $|x^a f(x)| \rightarrow 0$  for $x \rightarrow \pm \infty$. But what about derivatives? How can I show that $|x^{a} f^{(\beta)} (x)| \rightarrow 0$ for $x \rightarrow \pm \infty$ or is this false? If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
90,The $Hol$ operator is a continuous function?,The  operator is a continuous function?,Hol,"Let $\Omega$ be a compact space, and consider $C(\Omega)$ the space of  the continuous functions over $\Omega$, consider also, $C^\gamma(\Omega)$ the  space of all $\gamma$-holder continuous functions, i.e, $f\in C(\Omega)$ s.t $$ Hol(f):=\sup_{x\neq y}\dfrac{f(x)-f(y)}{d(x,y)^{\gamma}}<\infty $$ My problem is the following: Let $(f_n)\subset C^\gamma(\Omega)$ be a sequence of Holder continuous functions s.t  $f_n$ converge uniformly to a constant function $f\equiv F.$ It is clearly that $Hol (f)=0$, my question: It is truth that $Hol(f_n)\to 0$?","Let $\Omega$ be a compact space, and consider $C(\Omega)$ the space of  the continuous functions over $\Omega$, consider also, $C^\gamma(\Omega)$ the  space of all $\gamma$-holder continuous functions, i.e, $f\in C(\Omega)$ s.t $$ Hol(f):=\sup_{x\neq y}\dfrac{f(x)-f(y)}{d(x,y)^{\gamma}}<\infty $$ My problem is the following: Let $(f_n)\subset C^\gamma(\Omega)$ be a sequence of Holder continuous functions s.t  $f_n$ converge uniformly to a constant function $f\equiv F.$ It is clearly that $Hol (f)=0$, my question: It is truth that $Hol(f_n)\to 0$?",,"['real-analysis', 'analysis', 'metric-spaces', 'holder-spaces']"
91,A question regarding $f(x) = x^{2}|\cos(\pi/x)|$ that I am unsure about,A question regarding  that I am unsure about,f(x) = x^{2}|\cos(\pi/x)|,"The question is: Define $f(x)=x^2\left|\cos\left(\dfrac{\pi}{x}\right)\right|$ if x is not $0$ and $f(0)=0$. Prove that $f'(0)$ exists but $f'(x_0)$ does not for $x_0$ is an aribitrary point found in any neighborhood of $0$. Since $f(0)=0$, $f'(0)=0$. Then I take the derivative of $f(x)$ which is $f'(x) = \dfrac{\cos\left(\dfrac{{\pi}}{x}\right)\left(2\cos\left(\dfrac{{\pi}}{x}\right)x+{\pi}\sin\left(\dfrac{{\pi}}{x}\right)\right)}{\left|\cos\left(\dfrac{{\pi}}{x}\right)\right|}$ From here I think I could construct two positive sequences $x_n$ and $x_k$ which will approach to $0$ as n and k approach $\infty$. However, the values of $f'(x_n)$ and $f'(x_k)$ will be different since $\pi\sin(\frac{\pi}{x})$ gives different value at $x_n$ and $x_k$. But I am unsure whether I can say $f(0)=0$, so $f'(0)=0$ and whether the above thought of mine is faultless. Thank you for helping me deal with my uncertainty!","The question is: Define $f(x)=x^2\left|\cos\left(\dfrac{\pi}{x}\right)\right|$ if x is not $0$ and $f(0)=0$. Prove that $f'(0)$ exists but $f'(x_0)$ does not for $x_0$ is an aribitrary point found in any neighborhood of $0$. Since $f(0)=0$, $f'(0)=0$. Then I take the derivative of $f(x)$ which is $f'(x) = \dfrac{\cos\left(\dfrac{{\pi}}{x}\right)\left(2\cos\left(\dfrac{{\pi}}{x}\right)x+{\pi}\sin\left(\dfrac{{\pi}}{x}\right)\right)}{\left|\cos\left(\dfrac{{\pi}}{x}\right)\right|}$ From here I think I could construct two positive sequences $x_n$ and $x_k$ which will approach to $0$ as n and k approach $\infty$. However, the values of $f'(x_n)$ and $f'(x_k)$ will be different since $\pi\sin(\frac{\pi}{x})$ gives different value at $x_n$ and $x_k$. But I am unsure whether I can say $f(0)=0$, so $f'(0)=0$ and whether the above thought of mine is faultless. Thank you for helping me deal with my uncertainty!",,"['calculus', 'real-analysis', 'analysis']"
92,Convergence of $\sum_{i=0}^n \frac{(\log(n+i)-\log n)^2}{n+i}$,Convergence of,\sum_{i=0}^n \frac{(\log(n+i)-\log n)^2}{n+i},"Define a sequence $S_n$ of real numbers by $\sum_{i=0}^n \frac{(\log(n+i)-\log n)^2}{n+i}$ . Does the $\lim_{n\to \infty}S_n$ exist? If so, compute the value of this limit. I am getting two different answers if I use Cauchy theorem and if i convert this to integral form. Need help. Attempt: Let $f_n$ = $n\frac{(\log(n+n)-\log n)^2}{n+n}$ = $\frac{(log2)^2}2$ as n tends to $\infty$ Therefore by Cauchy Theorem $\sum \frac{f_1 + f_2+...+f_n}n$ = $S_n$  and $\lim_{n\to \infty}S_n$ = $\lim_{n\to \infty}f_n$ =  $\frac{(log2)^2}2$","Define a sequence $S_n$ of real numbers by $\sum_{i=0}^n \frac{(\log(n+i)-\log n)^2}{n+i}$ . Does the $\lim_{n\to \infty}S_n$ exist? If so, compute the value of this limit. I am getting two different answers if I use Cauchy theorem and if i convert this to integral form. Need help. Attempt: Let $f_n$ = $n\frac{(\log(n+n)-\log n)^2}{n+n}$ = $\frac{(log2)^2}2$ as n tends to $\infty$ Therefore by Cauchy Theorem $\sum \frac{f_1 + f_2+...+f_n}n$ = $S_n$  and $\lim_{n\to \infty}S_n$ = $\lim_{n\to \infty}f_n$ =  $\frac{(log2)^2}2$",,"['analysis', 'convergence-divergence']"
93,What's the higher dimensional generalization of arc length?,What's the higher dimensional generalization of arc length?,,"Given a scalar field $f: \mathbb R^n \supseteq V \to S \subseteq \mathbb R, \vec x\mapsto f(\vec x)$, what is the $n$ dimensional hypersurface (or volume, however you want to call this submanifold of $S\times V$) of $\{(y,\vec x) \in S\times V \big|\ y=f(\vec x)\}$? The case $n=1$ leading to the arc-length of $f:[a,b]\to\mathbb R$, $\int_a^b\sqrt{1+|f'(x)|^2}\,dx$ is well-known, but what about $n>1$?","Given a scalar field $f: \mathbb R^n \supseteq V \to S \subseteq \mathbb R, \vec x\mapsto f(\vec x)$, what is the $n$ dimensional hypersurface (or volume, however you want to call this submanifold of $S\times V$) of $\{(y,\vec x) \in S\times V \big|\ y=f(\vec x)\}$? The case $n=1$ leading to the arc-length of $f:[a,b]\to\mathbb R$, $\int_a^b\sqrt{1+|f'(x)|^2}\,dx$ is well-known, but what about $n>1$?",,"['real-analysis', 'analysis', 'surfaces', 'arc-length']"
94,Surjective linear operator onto finite-dimensional vector space,Surjective linear operator onto finite-dimensional vector space,,"Let $(X,\|\|_X),(Y,\|\|_Y)$ be two normed vector spaces ovef $K$ s.t. $dim_k(Y)<\infty$. Let $A:X\to Y$ be a surjective linear operator. I want to prove that $A$ is open. My attempt goes like this: Since $Y$ is finite-dimensional, let $dim_K(Y)=n$ and let $\beta=\{e_i\}_{i=1}^n$ be a normalized base for $Y$ (i.e. $\|e_i\|=1\;\;\forall i\in\{1,2,...,n\}$). So, we get that $\;\forall y\in Y\;\;\exists\;a_1,...a_n\in K$ s.t. $y=\sum_{i=1}^na_ie_i$. Then, since $A$ is surjective, we get that $\;\;\forall e_i\in\beta\;\;\exists x_i\in X$ s.t. $\;e_i=A(x_i)\;\;\forall i\in\{1,2,,...,n\}$ and lets define $\;\varphi:Y\to X$ s.t. $\varphi(y)=\sum_{i=1}^n a_ix_i\;\;\forall y=\sum_{i=1}^n a_ie_i\in Y$. I've already proved that $\varphi$ is linear, bounded and thus continuous. And also that $\;Id_Y=A\circ\varphi$ Then, let $U\subseteq X$ be open so to finish the prove I need to show that $A(U)$ is open, but I haven't managed to link $\varphi$ with $A$ in way that gives me the openess of A( I started to prove if $\varphi$ is open, but haven't got anything). Any help or ideas would be appreciated.","Let $(X,\|\|_X),(Y,\|\|_Y)$ be two normed vector spaces ovef $K$ s.t. $dim_k(Y)<\infty$. Let $A:X\to Y$ be a surjective linear operator. I want to prove that $A$ is open. My attempt goes like this: Since $Y$ is finite-dimensional, let $dim_K(Y)=n$ and let $\beta=\{e_i\}_{i=1}^n$ be a normalized base for $Y$ (i.e. $\|e_i\|=1\;\;\forall i\in\{1,2,...,n\}$). So, we get that $\;\forall y\in Y\;\;\exists\;a_1,...a_n\in K$ s.t. $y=\sum_{i=1}^na_ie_i$. Then, since $A$ is surjective, we get that $\;\;\forall e_i\in\beta\;\;\exists x_i\in X$ s.t. $\;e_i=A(x_i)\;\;\forall i\in\{1,2,,...,n\}$ and lets define $\;\varphi:Y\to X$ s.t. $\varphi(y)=\sum_{i=1}^n a_ix_i\;\;\forall y=\sum_{i=1}^n a_ie_i\in Y$. I've already proved that $\varphi$ is linear, bounded and thus continuous. And also that $\;Id_Y=A\circ\varphi$ Then, let $U\subseteq X$ be open so to finish the prove I need to show that $A(U)$ is open, but I haven't managed to link $\varphi$ with $A$ in way that gives me the openess of A( I started to prove if $\varphi$ is open, but haven't got anything). Any help or ideas would be appreciated.",,"['analysis', 'functional-analysis']"
95,Understanding Meyers-Serrin theorem: about the use of mollifiers.,Understanding Meyers-Serrin theorem: about the use of mollifiers.,,"I have read through the Meyers-Serrin theorem, and would like to understand why a simpler argument would not work. The theorem states that $C^{\infty}(\Omega)$ is dense in $W^{k,p}(\Omega), 1 \le p < +\infty.$ In the following we assume $k = 1$ and $\rho_{\epsilon} $a sequence of mollifiers. For $u \in W^{1,p}(\Omega),$ we consider $u, \nabla u \in L^p(\mathbb{R}^n),$ through natural extension through zero. Then we know: $u*\rho_{\epsilon} \rightarrow u$ $\nabla u*\rho_{\epsilon} \rightarrow \nabla u$ where both convergences are in $L^p(\mathbb{R}^n)$. From here we find: $u*\rho_{\epsilon} |_{\Omega} \rightarrow u|_{\Omega} $ $\nabla u*\rho_{\epsilon}|_{\Omega}  \rightarrow \nabla u|_{\Omega} $ here both convergences are in $L^p(\Omega).$ So we have almoast proven convergence in $W^{1,p}(\Omega),$ as soon as we know that $\nabla u*\rho_{\epsilon}|_{\Omega} = \nabla (u*\rho_{\epsilon}|_{\Omega})$ as a distribution on $\Omega.$ Is this last statement false? Because it seems true to me, and it should follow from the general results on derivatives of a convolution with a distribution. Thanks for the help in making my trhoughts clearer! :)","I have read through the Meyers-Serrin theorem, and would like to understand why a simpler argument would not work. The theorem states that $C^{\infty}(\Omega)$ is dense in $W^{k,p}(\Omega), 1 \le p < +\infty.$ In the following we assume $k = 1$ and $\rho_{\epsilon} $a sequence of mollifiers. For $u \in W^{1,p}(\Omega),$ we consider $u, \nabla u \in L^p(\mathbb{R}^n),$ through natural extension through zero. Then we know: $u*\rho_{\epsilon} \rightarrow u$ $\nabla u*\rho_{\epsilon} \rightarrow \nabla u$ where both convergences are in $L^p(\mathbb{R}^n)$. From here we find: $u*\rho_{\epsilon} |_{\Omega} \rightarrow u|_{\Omega} $ $\nabla u*\rho_{\epsilon}|_{\Omega}  \rightarrow \nabla u|_{\Omega} $ here both convergences are in $L^p(\Omega).$ So we have almoast proven convergence in $W^{1,p}(\Omega),$ as soon as we know that $\nabla u*\rho_{\epsilon}|_{\Omega} = \nabla (u*\rho_{\epsilon}|_{\Omega})$ as a distribution on $\Omega.$ Is this last statement false? Because it seems true to me, and it should follow from the general results on derivatives of a convolution with a distribution. Thanks for the help in making my trhoughts clearer! :)",,"['analysis', 'functional-analysis', 'sobolev-spaces']"
96,"Unbounded operator, when is it dense?","Unbounded operator, when is it dense?",,"Let $E = L^p(0, 1)$ with $1 \le p < \infty$. Consider the unbounded operator $A: D(A) \subset E \to E$ defined by$$D(A) = \{u \in W^{1, p}(0, 1),\text{ }u(0) = 0\} \text{ and }Au = u'.$$My two questions are as follows. What is $A^*$ here? For what $1 \le p < \infty$ is $D(A^*)$ dense in $E^* = L^{p'}(0, 1)$?","Let $E = L^p(0, 1)$ with $1 \le p < \infty$. Consider the unbounded operator $A: D(A) \subset E \to E$ defined by$$D(A) = \{u \in W^{1, p}(0, 1),\text{ }u(0) = 0\} \text{ and }Au = u'.$$My two questions are as follows. What is $A^*$ here? For what $1 \le p < \infty$ is $D(A^*)$ dense in $E^* = L^{p'}(0, 1)$?",,"['real-analysis', 'analysis']"
97,Prove a vector in $\ell^2(\mathbb{Z})$ is zero,Prove a vector in  is zero,\ell^2(\mathbb{Z}),"Suupose we take a vector $\vec{c}\in\ell^2(\mathbb{Z})$ where $$c(i)=\sum_{k=1}^\infty\frac{c(-k+i)+c(k+i)}{k+1}$$ That is, every elements of the vector is a series with the other terms in $\vec{c}$. Can I prove $\vec{c}=0$? I tried solving the equations for $i=0,i=1$ and so on but I get nothing of this method. Maybe it has something with summability or matricies $\in M_{\aleph_0}(\mathbb{C})$?","Suupose we take a vector $\vec{c}\in\ell^2(\mathbb{Z})$ where $$c(i)=\sum_{k=1}^\infty\frac{c(-k+i)+c(k+i)}{k+1}$$ That is, every elements of the vector is a series with the other terms in $\vec{c}$. Can I prove $\vec{c}=0$? I tried solving the equations for $i=0,i=1$ and so on but I get nothing of this method. Maybe it has something with summability or matricies $\in M_{\aleph_0}(\mathbb{C})$?",,"['matrices', 'analysis', 'functional-analysis', 'summation', 'lp-spaces']"
98,Derivative of an integral on a level set,Derivative of an integral on a level set,,"Consider a mapping $\xi:\mathbb{R}^d\rightarrow\mathbb{R}^k$ such that $D\xi \, D\xi^T>\delta\, I_k$. Here $D\xi:\mathbb{R}^d\rightarrow \mathbb{R}^{k\times k}$ is the Jacobian. Consider a function $f:\mathbb{R}^{d}\rightarrow \mathbb{R}$ and $A:\mathbb{R}^k\rightarrow \mathbb{R}$ defined by \begin{equation} A(z)=\int_{\xi^{-1}(z)} f(x)\,dx. \end{equation} Question: Under the assumptions $\bullet \  \xi\in C^2(\mathbb{R}^d;\mathbb{R}^k)$ with uniformly bounded derivatives (not the function itself) $\bullet \  f\in C^2(\mathbb{R}^{d};\mathbb{R})$ with uniformly bounded derivatives (not the function itself) Show that (1) $A$ is $C^1(\mathbb{R}^k;\mathbb{R})$ (2) If (1) is not true, whats the minimum requirements on $f,\xi$ for (1) to hold. Notation: Here $C^2$ $(C^1)$ means twice (once) continuously differentiable. Possible approach: Lets start with trying to show that $A$ is continuous. If we can show that \begin{equation} \nabla A(z)=\lim\limits_{h\rightarrow 0} \frac{A(z+h \,z')-A(z)}{h}\leq C, \end{equation} then we should be done. Here $h\in \mathbb{R}$ and $z,z'\in \mathbb{R}^k$. Therefore we need to parametrize the level set $\xi^{-1}(z+h\,z')$ in terms of $\xi^{-1}(z)$. From here on I do not know how to proceed. I would expect a parametrisation of these level sets, and then using change of variables and Taylor expansion try to work out the difference above. But I do not know of any details or how to make this intuition of a parametrisation rigorous. Thank you for any help!","Consider a mapping $\xi:\mathbb{R}^d\rightarrow\mathbb{R}^k$ such that $D\xi \, D\xi^T>\delta\, I_k$. Here $D\xi:\mathbb{R}^d\rightarrow \mathbb{R}^{k\times k}$ is the Jacobian. Consider a function $f:\mathbb{R}^{d}\rightarrow \mathbb{R}$ and $A:\mathbb{R}^k\rightarrow \mathbb{R}$ defined by \begin{equation} A(z)=\int_{\xi^{-1}(z)} f(x)\,dx. \end{equation} Question: Under the assumptions $\bullet \  \xi\in C^2(\mathbb{R}^d;\mathbb{R}^k)$ with uniformly bounded derivatives (not the function itself) $\bullet \  f\in C^2(\mathbb{R}^{d};\mathbb{R})$ with uniformly bounded derivatives (not the function itself) Show that (1) $A$ is $C^1(\mathbb{R}^k;\mathbb{R})$ (2) If (1) is not true, whats the minimum requirements on $f,\xi$ for (1) to hold. Notation: Here $C^2$ $(C^1)$ means twice (once) continuously differentiable. Possible approach: Lets start with trying to show that $A$ is continuous. If we can show that \begin{equation} \nabla A(z)=\lim\limits_{h\rightarrow 0} \frac{A(z+h \,z')-A(z)}{h}\leq C, \end{equation} then we should be done. Here $h\in \mathbb{R}$ and $z,z'\in \mathbb{R}^k$. Therefore we need to parametrize the level set $\xi^{-1}(z+h\,z')$ in terms of $\xi^{-1}(z)$. From here on I do not know how to proceed. I would expect a parametrisation of these level sets, and then using change of variables and Taylor expansion try to work out the difference above. But I do not know of any details or how to make this intuition of a parametrisation rigorous. Thank you for any help!",,"['analysis', 'multivariable-calculus', 'differential-geometry']"
99,Show $A=\limsup_\limits{n\to\infty}a_n$.,Show .,A=\limsup_\limits{n\to\infty}a_n,"Let $\{a_n\}$ be a sequence of real numbers bounded from above, $A\in \Bbb R$. Given any $\epsilon>0$, a)$\exists n_0 \in \Bbb N$ such that $ a_n<A+\epsilon$ for all $n\ge n_0$. b)$\exists k\ge n_0$ such that $a_k>A-\epsilon$. If the sequence satisfies the above two properties, show that $A=\limsup_\limits{n\to\infty}a_n$. I know the definition of the limit superior as:$\limsup a_n = \inf_{\forall m} \sup_{n \ge m} a_n$. Also, if ($a_n$) is a real sequence bounded from above. Let $S :=$ {$t \in \Bbb R:$ $t$ is the limit of a convergent subsequence of ($a_n$) }. Then $A = sup S$. I've proved the opposite direction (i.e. Given $A=\limsup_{n\to\infty}a_n$, then it has the following two properties), but stuck on trying to prove the two properties imply A. Could someone provide a precise proof of this please? Thanks.","Let $\{a_n\}$ be a sequence of real numbers bounded from above, $A\in \Bbb R$. Given any $\epsilon>0$, a)$\exists n_0 \in \Bbb N$ such that $ a_n<A+\epsilon$ for all $n\ge n_0$. b)$\exists k\ge n_0$ such that $a_k>A-\epsilon$. If the sequence satisfies the above two properties, show that $A=\limsup_\limits{n\to\infty}a_n$. I know the definition of the limit superior as:$\limsup a_n = \inf_{\forall m} \sup_{n \ge m} a_n$. Also, if ($a_n$) is a real sequence bounded from above. Let $S :=$ {$t \in \Bbb R:$ $t$ is the limit of a convergent subsequence of ($a_n$) }. Then $A = sup S$. I've proved the opposite direction (i.e. Given $A=\limsup_{n\to\infty}a_n$, then it has the following two properties), but stuck on trying to prove the two properties imply A. Could someone provide a precise proof of this please? Thanks.",,"['real-analysis', 'sequences-and-series', 'analysis']"
