,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Deducing an inequality from a Taylor series expansion,Deducing an inequality from a Taylor series expansion,,"Consider the inequality $$ 1-\frac{x}{2}-\frac{x^2}{2} \le \sqrt{1-x} < 1-\frac{x}{2} $$ for $0 < x < 1$ . The upper bound can be read off the Taylor expansion for $\sqrt{1-x}$ around $0$ , $$ \sqrt{1-x} = 1 - \frac{x}{2} - \frac{x^2}{8} - \frac{x^3}{16} - \dots $$ by noting that all the non linear terms are negative. Can the left side inequality be read-off the expansion by a similar reasoning? Please do not try to prove the left side inequality by other means (such as minimizing $\sqrt{1-x} - 1 + \frac{x}{2} + \frac{x^2}{2}$ using derivatives).","Consider the inequality for . The upper bound can be read off the Taylor expansion for around , by noting that all the non linear terms are negative. Can the left side inequality be read-off the expansion by a similar reasoning? Please do not try to prove the left side inequality by other means (such as minimizing using derivatives).","
1-\frac{x}{2}-\frac{x^2}{2} \le \sqrt{1-x} < 1-\frac{x}{2}
 0 < x < 1 \sqrt{1-x} 0 
\sqrt{1-x} = 1 - \frac{x}{2} - \frac{x^2}{8} - \frac{x^3}{16} - \dots
 \sqrt{1-x} - 1 + \frac{x}{2} + \frac{x^2}{2}","['calculus', 'inequality', 'taylor-expansion']"
1,"Proof verification: if $\int_a^b \sqrt{f(x)+g(x)}\,dx=0$, $f$ and $g$ are both non-negative, and $f$ is continuous, then $f\equiv 0$.","Proof verification: if ,  and  are both non-negative, and  is continuous, then .","\int_a^b \sqrt{f(x)+g(x)}\,dx=0 f g f f\equiv 0","As a math-for-fun exercise, and also to supply my problem-solving toolbox with new inequalities, I challenged myself to attack this problem: If $\int_a^b \sqrt{f(x)+g(x)}\,dx=0$ , $f$ and $g$ are both non-negative on $[a,b]$ , and $f$ is continuous on $[a,b]$ , is $f$ identically $0$ on $[a,b]$ ? I think I've produced an argument that proves the answer is yes. Here it is: Suppose, for the sake of finding a contradiction, that $\int_a^b \sqrt{f(x)+g(x)}\,dx=0$ , $f$ and $g$ are both non-negative on $[a,b]$ , and $f$ is continuous on $[a,b]$ , but $f$ is not identically $0$ . Then for some $x_0\in[a,b]$ , we have $f(x_0)\neq 0$ . Since $f$ is non-negative, this reduces to $f(x_0)>0$ . Clearly either $x_0\in(a,b)$ , $x_0=a$ , or $x_0=b$ . If $x_0\in(a,b)$ , then the continuity of $f$ implies that for every $\varepsilon >0$ , there is a $\delta>0$ such that for every $x\in[a,b]$ , $x\in[x_0-\delta,x_0+\delta]$ implies $|f(x)-f(x_0)|<\varepsilon$ . In particular, this is true for $f(x_0)$ , since $f(x_0)>0$ . Thus, for some $\delta>0$ , $$|f(x)-f(x_0)|<f(x_0)\text{ for every }x\in[x_0-\delta,x_0+\delta]$$ $$\iff 0<f(x)<2f(x_0)\text{ for every }x\in[x_0-\delta,x_0+\delta]$$ This shows that $f$ is strictly positive on $[x_0-\delta,x_0+\delta]$ . Since $g$ is non-negative, we have that $g(x)\geq 0$ on $[a,b]$ , so $$f(x)+g(x)\geq f(x)>0\text{ for every }x\in[x_0-\delta,x_0+\delta]$$ $$\implies\sqrt{f(x)+g(x)}\geq\sqrt{f(x)}>0\text{ for every }x\in[x_0-\delta,x_0+\delta]$$ $$\implies\int_{x_0-\delta}^{x_0+\delta}\sqrt{f(x)+g(x)}\text{ }dx>0$$ Noting that $\int_{a}^{x_0-\delta}\sqrt{f(x)+g(x)}\text{ }dx\geq 0$ and $\int_{x_0+\delta}^{b}\sqrt{f(x)+g(x)}\text{ }dx\geq 0$ , we deduce that their sum is at least zero. This gives the contradiction \begin{align*} \color{blue}{0} &= \int_{a}^{b}\sqrt{f(x)+g(x)}\text{ }dx\\ &= \int_{a}^{x_0-\delta}\sqrt{f(x)+g(x)}\text{ }dx+\int_{x_0-\delta}^{x_0+\delta}\sqrt{f(x)+g(x)}\text{ }dx+\int_{x_0+\delta}^{b}\sqrt{f(x)+g(x)}\text{ }dx\\ &\color{red}{>} \int_{a}^{x_0-\delta}\sqrt{f(x)+g(x)}\text{ }dx+\int_{x_0+\delta}^{b}\sqrt{f(x)+g(x)}\text{ }dx\\ &\geq \color{green}{0} \end{align*} If $x_0=a$ or $x_0=b$ , a similar argument applies, the only difference being that we use $[x_0,x_0+\delta]$ and $[x_0-\delta,x_0]$ for $x_0=a$ and $x_0=b$ , respectively. We conclude that the stated assumptions necessitate $f$ being identically $0$ on $[a,b]$ . $\blacksquare$ I greatly appreciate any and all feedback.","As a math-for-fun exercise, and also to supply my problem-solving toolbox with new inequalities, I challenged myself to attack this problem: If , and are both non-negative on , and is continuous on , is identically on ? I think I've produced an argument that proves the answer is yes. Here it is: Suppose, for the sake of finding a contradiction, that , and are both non-negative on , and is continuous on , but is not identically . Then for some , we have . Since is non-negative, this reduces to . Clearly either , , or . If , then the continuity of implies that for every , there is a such that for every , implies . In particular, this is true for , since . Thus, for some , This shows that is strictly positive on . Since is non-negative, we have that on , so Noting that and , we deduce that their sum is at least zero. This gives the contradiction If or , a similar argument applies, the only difference being that we use and for and , respectively. We conclude that the stated assumptions necessitate being identically on . I greatly appreciate any and all feedback.","\int_a^b \sqrt{f(x)+g(x)}\,dx=0 f g [a,b] f [a,b] f 0 [a,b] \int_a^b \sqrt{f(x)+g(x)}\,dx=0 f g [a,b] f [a,b] f 0 x_0\in[a,b] f(x_0)\neq 0 f f(x_0)>0 x_0\in(a,b) x_0=a x_0=b x_0\in(a,b) f \varepsilon >0 \delta>0 x\in[a,b] x\in[x_0-\delta,x_0+\delta] |f(x)-f(x_0)|<\varepsilon f(x_0) f(x_0)>0 \delta>0 |f(x)-f(x_0)|<f(x_0)\text{ for every }x\in[x_0-\delta,x_0+\delta] \iff 0<f(x)<2f(x_0)\text{ for every }x\in[x_0-\delta,x_0+\delta] f [x_0-\delta,x_0+\delta] g g(x)\geq 0 [a,b] f(x)+g(x)\geq f(x)>0\text{ for every }x\in[x_0-\delta,x_0+\delta] \implies\sqrt{f(x)+g(x)}\geq\sqrt{f(x)}>0\text{ for every }x\in[x_0-\delta,x_0+\delta] \implies\int_{x_0-\delta}^{x_0+\delta}\sqrt{f(x)+g(x)}\text{ }dx>0 \int_{a}^{x_0-\delta}\sqrt{f(x)+g(x)}\text{ }dx\geq 0 \int_{x_0+\delta}^{b}\sqrt{f(x)+g(x)}\text{ }dx\geq 0 \begin{align*}
\color{blue}{0} &= \int_{a}^{b}\sqrt{f(x)+g(x)}\text{ }dx\\
&= \int_{a}^{x_0-\delta}\sqrt{f(x)+g(x)}\text{ }dx+\int_{x_0-\delta}^{x_0+\delta}\sqrt{f(x)+g(x)}\text{ }dx+\int_{x_0+\delta}^{b}\sqrt{f(x)+g(x)}\text{ }dx\\
&\color{red}{>} \int_{a}^{x_0-\delta}\sqrt{f(x)+g(x)}\text{ }dx+\int_{x_0+\delta}^{b}\sqrt{f(x)+g(x)}\text{ }dx\\
&\geq \color{green}{0}
\end{align*} x_0=a x_0=b [x_0,x_0+\delta] [x_0-\delta,x_0] x_0=a x_0=b f 0 [a,b] \blacksquare","['calculus', 'integration', 'solution-verification']"
2,Find constants in system of equations,Find constants in system of equations,,"In the below system of equations, find the values of constants $q_1, q_2, q_3 \in \mathbb R$ so that the system has no solution. $$\begin{cases} 2x&+y-z&+2s &= q_1\\ 3x&+y-2z&+s &= q_2\\ x&+y+3z&-s &= q_3 \end{cases}$$ In order to eliminate some of the unknowns, we add the 1st with the 3rd and subtract the 2nd, so we get: $y+4z = q1+q3-q2$ By similar more ways (multiplying, adding and subtracting) we get some more such equations. I don't see how this system can have no solution; plus that we are one equation short (3 equations - 4 unknowns). Any assistance is much appreciated. EDIT: (Apologies but I am not familiar with linear algebra!!) What I believe I must do is the following: Reduce the unknowns by one (don't see how), so as to have a 3x3 system. Calculate the determinant, for which it must be $D \neq 0$ . Then calculate $D_x, D_y, D_z$ assuming we have eliminated s. Then, for the system NOT to have a solution, it must be $D=0$ and $D_x, D,y, D_z \neq 0$ . Any further help?","In the below system of equations, find the values of constants so that the system has no solution. In order to eliminate some of the unknowns, we add the 1st with the 3rd and subtract the 2nd, so we get: By similar more ways (multiplying, adding and subtracting) we get some more such equations. I don't see how this system can have no solution; plus that we are one equation short (3 equations - 4 unknowns). Any assistance is much appreciated. EDIT: (Apologies but I am not familiar with linear algebra!!) What I believe I must do is the following: Reduce the unknowns by one (don't see how), so as to have a 3x3 system. Calculate the determinant, for which it must be . Then calculate assuming we have eliminated s. Then, for the system NOT to have a solution, it must be and . Any further help?","q_1, q_2, q_3 \in \mathbb R \begin{cases}
2x&+y-z&+2s &= q_1\\
3x&+y-2z&+s &= q_2\\
x&+y+3z&-s &= q_3
\end{cases} y+4z = q1+q3-q2 D \neq 0 D_x, D_y, D_z D=0 D_x, D,y, D_z \neq 0","['calculus', 'linear-algebra']"
3,Hard power series problem,Hard power series problem,,"Consider the differential equation $$(1+t)y''+2y=0$$ with the variabel coefficient $(1+t)$ , with $t\in \mathbb{R}$ . Set $y(t)=\sum_{n=0}^{\infty}a_nt^n$ . What are the first 4 terms in the associated power series? An Attempt We write $$(1+t)y''=\sum_{n=2}^{\infty}a_nn(n-1)t^{n-2}+ \sum_{n=2}^{\infty}a_nn(n-1)t^{n-1}$$ $$2y=\sum_{n=0}^{\infty}2a_nt^{n} $$ Combining these three gives us $$\sum_{n=2}^{\infty}a_nn(n-1)t^{n-2}+ \sum_{n=2}^{\infty}a_nn(n-1)t^{n-1}+ \sum_{n=0}^{\infty}2a_nt^{n}$$ We now want all the powers to be $t^n$ . $$\sum_{n=0}^{\infty}a_{n+2}(n+2)(n+1)t^{n}+ \sum_{n=1}^{\infty}a_{n+1}(n+1)nt^{n}+ \sum_{n=0}^{\infty}2a_nt^{n} $$ We finally want the index of summation to all be the same. $$2a_0+2a_2+ \sum_{n=1}^{\infty}a_{n+2}(n+2)(n+1)t^{n}+ \sum_{n=1}^{\infty}a_{n+1}(n+1)nt^{n}+ \sum_{n=1}^{\infty}2a_nt^{n}$$ Which simplifies to $$2a_0+2a_2+ \sum_{n=1}^{\infty}\big[a_{n+2}(n+2)(n+1)+a_{n+1}n(n+1)+2a_n \big]t^n$$ Clearly, I'm not getting anywhere close to any of the answers given. I just can't find what I'm doing wrong. I have also tried with Maple, but that didn't give me any real result either. I hope someone can help me out.","Consider the differential equation with the variabel coefficient , with . Set . What are the first 4 terms in the associated power series? An Attempt We write Combining these three gives us We now want all the powers to be . We finally want the index of summation to all be the same. Which simplifies to Clearly, I'm not getting anywhere close to any of the answers given. I just can't find what I'm doing wrong. I have also tried with Maple, but that didn't give me any real result either. I hope someone can help me out.",(1+t)y''+2y=0 (1+t) t\in \mathbb{R} y(t)=\sum_{n=0}^{\infty}a_nt^n (1+t)y''=\sum_{n=2}^{\infty}a_nn(n-1)t^{n-2}+ \sum_{n=2}^{\infty}a_nn(n-1)t^{n-1} 2y=\sum_{n=0}^{\infty}2a_nt^{n}  \sum_{n=2}^{\infty}a_nn(n-1)t^{n-2}+ \sum_{n=2}^{\infty}a_nn(n-1)t^{n-1}+ \sum_{n=0}^{\infty}2a_nt^{n} t^n \sum_{n=0}^{\infty}a_{n+2}(n+2)(n+1)t^{n}+ \sum_{n=1}^{\infty}a_{n+1}(n+1)nt^{n}+ \sum_{n=0}^{\infty}2a_nt^{n}  2a_0+2a_2+ \sum_{n=1}^{\infty}a_{n+2}(n+2)(n+1)t^{n}+ \sum_{n=1}^{\infty}a_{n+1}(n+1)nt^{n}+ \sum_{n=1}^{\infty}2a_nt^{n} 2a_0+2a_2+ \sum_{n=1}^{\infty}\big[a_{n+2}(n+2)(n+1)+a_{n+1}n(n+1)+2a_n \big]t^n,"['calculus', 'summation', 'power-series']"
4,Hash function and one near hard example?,Hash function and one near hard example?,,"Example: Suppose $H:$ { $1,...,n$ } $\rightarrow $ { $1,..,n$ } be a uniform hash function. for input $x$ , $z$ is equal to number of trailing zero in the right side of $H(x)$ . for $0 \leq c \leq 1$ what is the order of probability $ z \geq c \log_2 n$ ? $C$ is constant here. Answer: $O(1/n^c)$ How  this this is can be achieved? Update : The Logarithm base is $2$ not $10$ .","Example: Suppose { } { } be a uniform hash function. for input , is equal to number of trailing zero in the right side of . for what is the order of probability ? is constant here. Answer: How  this this is can be achieved? Update : The Logarithm base is not .","H: 1,...,n \rightarrow  1,..,n x z H(x) 0 \leq c \leq 1  z \geq c \log_2 n C O(1/n^c) 2 10","['calculus', 'probability', 'probability-theory', 'computer-science', 'hash-function']"
5,An interesting integral $\int{\dfrac{x^m}{x^{2m}+1}dx}$,An interesting integral,\int{\dfrac{x^m}{x^{2m}+1}dx},"I have, for a long period of time, tried to evaluate the indefinite integral $\int{\dfrac{x^m}{x^{2m}+1}dx}$ for different values of $m$ , such as $m=2,3,4,6...$ , and I have recently thought of generalizing this integral for all values of $m\in\mathbb{N}$ . Since evaluating the integral using partial fractions for high values of $m$ by myself is very time consuming, and it happens that I can't find any direct solution on the internet, I tried to look for patterns of the integrals in WolframAlpha. After some comparisons, I found the below expression: $$\int{\dfrac{x^m}{x^{2m}+1}dx}=\sum_{n=1}^{m}(-1)^{n-1}(\dfrac{1}{2m}\sin(\dfrac{(2n-1)\pi}{2m})\ln{|x^2+2\cos(\dfrac{(2n-1)\pi}{2m})x+1|}$$ $$+\dfrac{1}{m}\arctan(\csc(\dfrac{(2n-1)\pi}{2m})(x+\cos(\dfrac{(2n-1)\pi}{2m})))\cos(\dfrac{(2n-1)\pi}{2m}))$$ How could I have found the above expression without comparing the integrals? P.S. It seems that the equivalence $x^{2m}-2x^ma^m\cos(m\theta)+a^{2m}\equiv\displaystyle\prod_{r=0}^{m-1}(x^2-2ax\cos(\theta+\dfrac{2r\pi}{m})+a^2)$ for $a\in\mathbb{R}^+, \theta\in\mathbb{R}, m\in\mathbb{N}$ can be used, yet I don't know how I can evaluate partial fractions with it.","I have, for a long period of time, tried to evaluate the indefinite integral for different values of , such as , and I have recently thought of generalizing this integral for all values of . Since evaluating the integral using partial fractions for high values of by myself is very time consuming, and it happens that I can't find any direct solution on the internet, I tried to look for patterns of the integrals in WolframAlpha. After some comparisons, I found the below expression: How could I have found the above expression without comparing the integrals? P.S. It seems that the equivalence for can be used, yet I don't know how I can evaluate partial fractions with it.","\int{\dfrac{x^m}{x^{2m}+1}dx} m m=2,3,4,6... m\in\mathbb{N} m \int{\dfrac{x^m}{x^{2m}+1}dx}=\sum_{n=1}^{m}(-1)^{n-1}(\dfrac{1}{2m}\sin(\dfrac{(2n-1)\pi}{2m})\ln{|x^2+2\cos(\dfrac{(2n-1)\pi}{2m})x+1|} +\dfrac{1}{m}\arctan(\csc(\dfrac{(2n-1)\pi}{2m})(x+\cos(\dfrac{(2n-1)\pi}{2m})))\cos(\dfrac{(2n-1)\pi}{2m})) x^{2m}-2x^ma^m\cos(m\theta)+a^{2m}\equiv\displaystyle\prod_{r=0}^{m-1}(x^2-2ax\cos(\theta+\dfrac{2r\pi}{m})+a^2) a\in\mathbb{R}^+, \theta\in\mathbb{R}, m\in\mathbb{N}","['calculus', 'integration', 'indefinite-integrals']"
6,Consider the sum $S=\sum\frac{1}{x^2}$ which is over all the positive real solutions of the equation $\frac{\tan{x}}{x}=n$,Consider the sum  which is over all the positive real solutions of the equation,S=\sum\frac{1}{x^2} \frac{\tan{x}}{x}=n,"Consider the sum $S(n)=\sum\dfrac{1}{x^2}$ where summation is performed over all the positive real solutions of the equation $\dfrac{\tan{x}}{x}=n$ . If it's given that $S(n)=1$ , $n\in\mathbb{Q}$ , find $n$ . My attempt: We're interested in the roots of $$\frac{\tan x}{x}=k$$ Then, performing Taylor expansion of $$\sin x = kx\cos x$$ $$x- \frac{(x^6)}{6}+ ......  = kx(1- \frac{x^2}{2}+........$$ I am stuck here. Any hints will be appreciated. Thanks","Consider the sum where summation is performed over all the positive real solutions of the equation . If it's given that , , find . My attempt: We're interested in the roots of Then, performing Taylor expansion of I am stuck here. Any hints will be appreciated. Thanks",S(n)=\sum\dfrac{1}{x^2} \dfrac{\tan{x}}{x}=n S(n)=1 n\in\mathbb{Q} n \frac{\tan x}{x}=k \sin x = kx\cos x x- \frac{(x^6)}{6}+ ......  = kx(1- \frac{x^2}{2}+........,"['calculus', 'sequences-and-series', 'functions', 'trigonometry', 'roots']"
7,Inconsistent notation for vectors and points in textbooks,Inconsistent notation for vectors and points in textbooks,,"Many books on calculus or advanced calculus distinguish between points and vectors. Usually points are denoted by italic letters like $P, Q$ , and $R$ , and vectors are denoted by bold letters such as $\mathbf{u}$ and $\mathbf{v}$ . And some textbooks put the components of a vector between two angle brackets, while the coordinates of points are simply placed between two parentheses. However the notation is not consistant through the books. At least I have not seen any textbook that is consistent through the text. Here are two samples: Sample 1: Marsden and Tromba in Vector Calculus As you can see from the following figure, the point $P$ is not bold but vectors $\mathbf{v}$ and $\mathbf{w}$ are bold. However, a few pages later, they denote points by bold letters So a student might ask: is $\mathbf{x}_0$ a point or a vector? Sample 2: Stewart Calculus The equation of a line passing through $P_0$ and parallel to a vector $\mathbf{v}$ is described by $$``\mathbf{r}_0=\mathbf{r}_0+t\mathbf{v},$$ where $\mathbf{r}$ is the position vector of a point $P(x,y,z)$ and $\mathbf{r}_0$ is the position vector of $P_0$ ."" So clearly here, he does not add a vector $t\mathbf{v}$ to a point $P_0$ . (I mean he could simply write the line is $\{P_0+t\mathbf{v}| t\in\mathbb{R}\}$ ). But when he talks about directional derivative, he adds a vector $\mathbf{u}$ to a point $\mathbf{x}_0$ : A student may ask: if $\mathbf{x}_0$ is a point why is it denoted by a bold letter? And what is the meaning of adding a vector to a point? Adding a vector to a point has not been defined in the textbook. What is the best and consistent notation? What are the benefits of using angle brackets and parentheses for vectors and points? How do you avoid confusing students?","Many books on calculus or advanced calculus distinguish between points and vectors. Usually points are denoted by italic letters like , and , and vectors are denoted by bold letters such as and . And some textbooks put the components of a vector between two angle brackets, while the coordinates of points are simply placed between two parentheses. However the notation is not consistant through the books. At least I have not seen any textbook that is consistent through the text. Here are two samples: Sample 1: Marsden and Tromba in Vector Calculus As you can see from the following figure, the point is not bold but vectors and are bold. However, a few pages later, they denote points by bold letters So a student might ask: is a point or a vector? Sample 2: Stewart Calculus The equation of a line passing through and parallel to a vector is described by where is the position vector of a point and is the position vector of ."" So clearly here, he does not add a vector to a point . (I mean he could simply write the line is ). But when he talks about directional derivative, he adds a vector to a point : A student may ask: if is a point why is it denoted by a bold letter? And what is the meaning of adding a vector to a point? Adding a vector to a point has not been defined in the textbook. What is the best and consistent notation? What are the benefits of using angle brackets and parentheses for vectors and points? How do you avoid confusing students?","P, Q R \mathbf{u} \mathbf{v} P \mathbf{v} \mathbf{w} \mathbf{x}_0 P_0 \mathbf{v} ``\mathbf{r}_0=\mathbf{r}_0+t\mathbf{v}, \mathbf{r} P(x,y,z) \mathbf{r}_0 P_0 t\mathbf{v} P_0 \{P_0+t\mathbf{v}| t\in\mathbb{R}\} \mathbf{u} \mathbf{x}_0 \mathbf{x}_0","['calculus', 'reference-request', 'notation', 'vector-analysis', 'education']"
8,I don't understand why we can just extend the domain of this function,I don't understand why we can just extend the domain of this function,,"The problem : Let $$g(x)= \left\{ \begin{matrix} x & , \, 0 \leq x \leq 1/2\\ 1-x& , \, 1/2 \leq x \leq 1 \end{matrix} \right .$$ Prove that $$S_n(x)=\sum_{k=0}^{n} \frac{g(2^kx)}{2^k}$$ converges uniformly to a continous function $f$ . My solution : Notice that $g(x)$ exist only when $x\in [0,1]$ . Since $2^n \to \infty$ when $n \to \infty$ , if $x>0$ then $2^nx >1$ for sufficiently large $n$ , therefore for $x=0, \ \{S_n(x)\}$ converges to $f(x)=0$ , and for any other value of $x, \ S_n(x)$ doesn't exist when $n\to \infty$ , so $f$ is undefined. Since the uniform converges definition is true for all $x\in\{0\}$ and since $x=0$ is not an accumulation point of $f, \ \{S_n(x)\}$ converges uniformly to $f$ , which is continous. Teacher's solution : Extend the domain of $g$ such that $g(x)=g(x+1)$ , then $g(x)\leq 1/2, \ \forall x\in \mathbb{R}$ , so $$\frac{g(2^nx)}{2^n} < \left(\frac{1}{2}\right)^n$$ By Weiertrass' M Criterion, $\{S_n(x)\}$ converges uniformly to a function $f$ over $\mathbb{R}$ and since every $g(2^nx)/2^n$ is continous, so is $f$ Now, I don't understand why extending the domain of $g$ is valid here, isn't it like adding an extra condition that the problem doens't give?","The problem : Let Prove that converges uniformly to a continous function . My solution : Notice that exist only when . Since when , if then for sufficiently large , therefore for converges to , and for any other value of doesn't exist when , so is undefined. Since the uniform converges definition is true for all and since is not an accumulation point of converges uniformly to , which is continous. Teacher's solution : Extend the domain of such that , then , so By Weiertrass' M Criterion, converges uniformly to a function over and since every is continous, so is Now, I don't understand why extending the domain of is valid here, isn't it like adding an extra condition that the problem doens't give?","g(x)= \left\{
\begin{matrix}
x & , \, 0 \leq x \leq 1/2\\
1-x& , \, 1/2 \leq x \leq 1
\end{matrix} \right . S_n(x)=\sum_{k=0}^{n} \frac{g(2^kx)}{2^k} f g(x) x\in [0,1] 2^n \to \infty n \to \infty x>0 2^nx >1 n x=0, \ \{S_n(x)\} f(x)=0 x, \ S_n(x) n\to \infty f x\in\{0\} x=0 f, \ \{S_n(x)\} f g g(x)=g(x+1) g(x)\leq 1/2, \ \forall x\in \mathbb{R} \frac{g(2^nx)}{2^n} < \left(\frac{1}{2}\right)^n \{S_n(x)\} f \mathbb{R} g(2^nx)/2^n f g","['calculus', 'sequences-and-series', 'uniform-convergence']"
9,Explaining a solution to a calculus problem.,Explaining a solution to a calculus problem.,,"I tried solving a calculus problem and I got the right result, but I don't understand the solution provided at the end of the exercise. Even though I got the same answer, I would like to understand what's happening in the given solution aswell. Consider the function: $$\ f(x) = \begin{cases}         x^2+ax+b & x\leq 0 \\        x-1 & x>0 \\    \end{cases} \ $$ Find the antiderivatives of the function $f$ if they exist. The solution provided goes something like this: For $f$ to have antiderivatives the function $f$ must have the Darboux   property. (...Some calculations...), therefore $f$ has the Darboux   property if and only if $b = -1$ (I understood that now the function   is continuous, therefore it has an anitederivative). Using the   consequences of Lagrange's theorem on the intervals $(-\infty, 0)$ and $(0, \infty)$ any antiderivative $F : \mathbb{R} \rightarrow  \mathbb{R}$ of $f$ has the form: $$ F(x) = \ \begin{cases}         \dfrac{x^3}{3} + a \dfrac{x^2}{2} - x + c_1 & x < 0 \\        \ c_2 & x=0 \\        \dfrac{x^2}{2} - x + c_3 & x>0     \end{cases} \ $$ $F$ being differentiable, it is also continous, so $F(0) = c_2 = c_1 =  c_3 $ . Therefore the antiderivatives of $f$ have the form: $$ F(x) = c + \ \begin{cases}         \dfrac{x^3}{3} + a \dfrac{x^2}{2} - x & x\leq 0 \\        \dfrac{x^2}{2} - x & x>0     \end{cases} \ $$ Again, I got the same result, but I don't understand a lot of the work done above. The first thing I didn't understand is the part where they say that $f$ has an antiderivative iff it has the Darboux property. I searched a bit online and I found that a function accepts antiderivatives only if it has the Darboux property. So I guess I have to accept that as a fact. The second (and more important thing) that I didn't understand was the part where they said that they used the consequences of Lagrange's Theorem on the intervals $(-\infty, 0)$ and $(0, \infty)$ to find that first form of the antiderivative. What theorem are they refering to? How did they use it on those intervals? Why is there a separate case for $x = 0$ with an aditional constant, $c_2$ . I only used $2$ constants, why were there needed $3$ ?  Long  story short, I just don't understand at all how they arrived at that first form of the antiderivative and how they used these ""consequences of Langrange's theorem"". I understood the second form of the antiderivative, that's what I also got, but the first form put me in the dark. I know these are all just details, but I really want to understand what was used here, why was it used and how was it used.","I tried solving a calculus problem and I got the right result, but I don't understand the solution provided at the end of the exercise. Even though I got the same answer, I would like to understand what's happening in the given solution aswell. Consider the function: Find the antiderivatives of the function if they exist. The solution provided goes something like this: For to have antiderivatives the function must have the Darboux   property. (...Some calculations...), therefore has the Darboux   property if and only if (I understood that now the function   is continuous, therefore it has an anitederivative). Using the   consequences of Lagrange's theorem on the intervals and any antiderivative of has the form: being differentiable, it is also continous, so . Therefore the antiderivatives of have the form: Again, I got the same result, but I don't understand a lot of the work done above. The first thing I didn't understand is the part where they say that has an antiderivative iff it has the Darboux property. I searched a bit online and I found that a function accepts antiderivatives only if it has the Darboux property. So I guess I have to accept that as a fact. The second (and more important thing) that I didn't understand was the part where they said that they used the consequences of Lagrange's Theorem on the intervals and to find that first form of the antiderivative. What theorem are they refering to? How did they use it on those intervals? Why is there a separate case for with an aditional constant, . I only used constants, why were there needed ?  Long  story short, I just don't understand at all how they arrived at that first form of the antiderivative and how they used these ""consequences of Langrange's theorem"". I understood the second form of the antiderivative, that's what I also got, but the first form put me in the dark. I know these are all just details, but I really want to understand what was used here, why was it used and how was it used.","\ f(x) = \begin{cases} 
       x^2+ax+b & x\leq 0 \\
       x-1 & x>0 \\    \end{cases} \  f f f f b = -1 (-\infty, 0) (0, \infty) F : \mathbb{R} \rightarrow
 \mathbb{R} f  F(x) = \ \begin{cases} 
       \dfrac{x^3}{3} + a \dfrac{x^2}{2} - x + c_1 & x < 0 \\
       \ c_2 & x=0 \\
       \dfrac{x^2}{2} - x + c_3 & x>0     \end{cases} \  F F(0) = c_2 = c_1 =
 c_3  f  F(x) = c + \ \begin{cases} 
       \dfrac{x^3}{3} + a \dfrac{x^2}{2} - x & x\leq 0 \\
       \dfrac{x^2}{2} - x & x>0     \end{cases} \  f (-\infty, 0) (0, \infty) x = 0 c_2 2 3",['calculus']
10,"Directional derivative confusion - why does independently evaluating partial changes, then adding them, work?","Directional derivative confusion - why does independently evaluating partial changes, then adding them, work?",,"I apologize for both my crude math grammar, and what is probably an obvious question - I am a novice. I am confused as to why, when taking the directional derivative, the gradient is evaluated by plugging in a fixed set of dependent variables, and this gradient is then dotted with the desired unit direction; as opposed to only the FIRST component in the gradient being evaluated by plugging in the fixed set of dependent variables, and then subsequent partial derivative components being evaluated at the new position/modified set of dependent variables , e.g. for a 2-d gradient, partial x being evaluated at (x,y) and partial y evaluated at (x+dx,y)? In more detail: As I understand it, the directional derivative for a function z=f(x,y) is calculated by the gradient dotted with a unit vector in the desired direction. Each component of the gradient is the ratio, at a point (x,y) , of the change in z to an “extremely small” independent change in x, or y. Dotting the x-component of the gradient with the x-component of our unit direction vector thus gives the change in z from taking an independent step in the x-direction by “ratio of change in z to extremely small change in x, scaled down by x-component of unit direction vector”. The same is independently done for y. The two are then added together to get the result of the dot product, apparently the ratio of the change in z to an “extremely small change” in the desired direction. My confusion lies in why the two partial derivatives that make up the gradient are evaluated at (x,y) before dotting with the desired unit direction, instead of partial x being evaluated at (x,y), and partial y being evaluated at ( x+(dx*x-component of unit direction vector) , y) (or vice versa) ? What if independent changes in x and y at a given point (x,y) would each result in increases in z, but changed together result in a decrease in z? Geometrically I can simply imagine a slope that increases in the positive x and y directions over a “small distance”, but drops into a pit when moving diagonally in <“small x * sqrt(2)/2”, “small y*sqrt(2)/2”> direction. The only way to reach this pit would be by first moving in one direction, then subsequently in the next direction, aka first evaluating the change in z from (x, y) to (x+dx*sqrt(2)/2, y), and then adding the change from (x+dx*sqrt(2)/2, y) to (x+dx*sqrt(2)/2, y+dy*sqrt(2)/2). This seems like an very obvious and trivial observation. Am I misunderstanding the directional derivative? Or is this issue simply “circumvented” by pointing to the partial changes in x and y being “infinitely small”? If so, how does that work when applied in the real, discrete world? Thank you very much for your time.","I apologize for both my crude math grammar, and what is probably an obvious question - I am a novice. I am confused as to why, when taking the directional derivative, the gradient is evaluated by plugging in a fixed set of dependent variables, and this gradient is then dotted with the desired unit direction; as opposed to only the FIRST component in the gradient being evaluated by plugging in the fixed set of dependent variables, and then subsequent partial derivative components being evaluated at the new position/modified set of dependent variables , e.g. for a 2-d gradient, partial x being evaluated at (x,y) and partial y evaluated at (x+dx,y)? In more detail: As I understand it, the directional derivative for a function z=f(x,y) is calculated by the gradient dotted with a unit vector in the desired direction. Each component of the gradient is the ratio, at a point (x,y) , of the change in z to an “extremely small” independent change in x, or y. Dotting the x-component of the gradient with the x-component of our unit direction vector thus gives the change in z from taking an independent step in the x-direction by “ratio of change in z to extremely small change in x, scaled down by x-component of unit direction vector”. The same is independently done for y. The two are then added together to get the result of the dot product, apparently the ratio of the change in z to an “extremely small change” in the desired direction. My confusion lies in why the two partial derivatives that make up the gradient are evaluated at (x,y) before dotting with the desired unit direction, instead of partial x being evaluated at (x,y), and partial y being evaluated at ( x+(dx*x-component of unit direction vector) , y) (or vice versa) ? What if independent changes in x and y at a given point (x,y) would each result in increases in z, but changed together result in a decrease in z? Geometrically I can simply imagine a slope that increases in the positive x and y directions over a “small distance”, but drops into a pit when moving diagonally in <“small x * sqrt(2)/2”, “small y*sqrt(2)/2”> direction. The only way to reach this pit would be by first moving in one direction, then subsequently in the next direction, aka first evaluating the change in z from (x, y) to (x+dx*sqrt(2)/2, y), and then adding the change from (x+dx*sqrt(2)/2, y) to (x+dx*sqrt(2)/2, y+dy*sqrt(2)/2). This seems like an very obvious and trivial observation. Am I misunderstanding the directional derivative? Or is this issue simply “circumvented” by pointing to the partial changes in x and y being “infinitely small”? If so, how does that work when applied in the real, discrete world? Thank you very much for your time.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'gradient-descent']"
11,Solving equation with $\arctan(x)$ and $\ln(x)$,Solving equation with  and,\arctan(x) \ln(x),"Last week my group had math exam and this questions popped up. I approximated $x$ using Newton-Rhapson method ( $x\approx 0.341$ ). However, after the exam professor told us that we should have found an exact solution. I tried however; I couldn't make it with methods I know of, that's why I ask for your help. Solve for $x$ : $$\frac{\arctan(x)}{x} + \frac{\ln(x)}{x^2+1} = 0$$ Thanks upfront for help.","Last week my group had math exam and this questions popped up. I approximated using Newton-Rhapson method ( ). However, after the exam professor told us that we should have found an exact solution. I tried however; I couldn't make it with methods I know of, that's why I ask for your help. Solve for : Thanks upfront for help.",x x\approx 0.341 x \frac{\arctan(x)}{x} + \frac{\ln(x)}{x^2+1} = 0,['calculus']
12,"show $\lim\limits_{(x,y)\to(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ exists and equals $0$? [duplicate]",show  exists and equals ? [duplicate],"\lim\limits_{(x,y)\to(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d} 0","This question already has an answer here : Show that $ \lim_{(x,y) \to 0} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \alpha/\gamma + \beta/\delta > 1.$ (1 answer) Closed 5 years ago . suppose $f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ and $a,b,c,d >0$ how would you show that if $\frac{a}{c} + \frac{b}{d} >1$ then $\lim\limits_{(x,y)\to(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ exists and equals $0$? I've been trying to use the squeeze theorem and set up an inequality but im really struggling.","This question already has an answer here : Show that $ \lim_{(x,y) \to 0} \frac {|x|^ \alpha |y|^ \beta} {|x|^ \gamma + |y|^ \delta} \text {exists} \iff \alpha/\gamma + \beta/\delta > 1.$ (1 answer) Closed 5 years ago . suppose $f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ and $a,b,c,d >0$ how would you show that if $\frac{a}{c} + \frac{b}{d} >1$ then $\lim\limits_{(x,y)\to(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ exists and equals $0$? I've been trying to use the squeeze theorem and set up an inequality but im really struggling.",,"['calculus', 'limits', 'multivariable-calculus']"
13,Need help with this limit problem from Stewart's Calculus,Need help with this limit problem from Stewart's Calculus,,"This is from Stewarts's Calculus Early Transcendentals 8e, Chapter 4, Problem Plus, #8. The topics discussed in chapter 4 are: ""Maximum and Minimum Values"", ""The Mean Value Theorem"", ""How Derivatives Affect the Shape of a Graph"", ""Indeterminate Forms and L'Hospital's Rule"", ""Summary of Curve Sketching"", ""Graphing with Calculus and Calculators"", ""Optimization Problems"", ""Newton's Method"", and ""Antiderivatives"". The problem is: $$\lim_{x\to\infty}\frac{(x+2)^{1/x}-x^{1/x}}{(x+3)^{1/x}-x^{1/x}}$$ I found the limits of each term become 1, which makes the fraction $\frac{0}{0}$, so I tried using L'Hospital's but it doesn't really help as the derivatives become even more complicated. I tried rationalizing either top or bottom, but it's not too easy either because the exponents contain $x$. I also tried using Squeeze Theorem, but all failed. I think there is a way to rationalize this to simplify and then use L'Hospital's from there but I really can't find a way. I don't normally give up but as I've been struggling with this for a month, I think it's time to seek help from others :( According to graphing devices, it looks like the limit approaches $\frac{2}{3}$. I want to find a way to verify this only using elementary calculus (no series expansion, or etc) as I think that's how Stewart intended.","This is from Stewarts's Calculus Early Transcendentals 8e, Chapter 4, Problem Plus, #8. The topics discussed in chapter 4 are: ""Maximum and Minimum Values"", ""The Mean Value Theorem"", ""How Derivatives Affect the Shape of a Graph"", ""Indeterminate Forms and L'Hospital's Rule"", ""Summary of Curve Sketching"", ""Graphing with Calculus and Calculators"", ""Optimization Problems"", ""Newton's Method"", and ""Antiderivatives"". The problem is: $$\lim_{x\to\infty}\frac{(x+2)^{1/x}-x^{1/x}}{(x+3)^{1/x}-x^{1/x}}$$ I found the limits of each term become 1, which makes the fraction $\frac{0}{0}$, so I tried using L'Hospital's but it doesn't really help as the derivatives become even more complicated. I tried rationalizing either top or bottom, but it's not too easy either because the exponents contain $x$. I also tried using Squeeze Theorem, but all failed. I think there is a way to rationalize this to simplify and then use L'Hospital's from there but I really can't find a way. I don't normally give up but as I've been struggling with this for a month, I think it's time to seek help from others :( According to graphing devices, it looks like the limit approaches $\frac{2}{3}$. I want to find a way to verify this only using elementary calculus (no series expansion, or etc) as I think that's how Stewart intended.",,"['calculus', 'limits']"
14,Can non-linear continuous and odd function $f(x)$ satisfy $f(2x) = 2f(x)$?,Can non-linear continuous and odd function  satisfy ?,f(x) f(2x) = 2f(x),"Consider the following problem: Is it true, that any odd continuous function satisfying $f(2x) = 2f(x)$ is linear? Does the following function (which I found here ) can serve as a counterexample? $$ f(x) = \begin{cases} x\cos(2\pi \log_2(x)), & \mbox{if } x>0 \\ 0, & \mbox{if } x=0 \\ x\cos(2\pi \log_2(-x)) & \mbox{if } x<0 \end{cases} $$ If it can, are there any ""simpler"" ones? I mean, that task was given on exam, and it would be rather difficult to come up with such a function on your own. By what strategy can one construct a similar counterexample from scratch? It feels like some piecewise linear function could do the trick, but I can't figure out, how to build it.","Consider the following problem: Is it true, that any odd continuous function satisfying $f(2x) = 2f(x)$ is linear? Does the following function (which I found here ) can serve as a counterexample? $$ f(x) = \begin{cases} x\cos(2\pi \log_2(x)), & \mbox{if } x>0 \\ 0, & \mbox{if } x=0 \\ x\cos(2\pi \log_2(-x)) & \mbox{if } x<0 \end{cases} $$ If it can, are there any ""simpler"" ones? I mean, that task was given on exam, and it would be rather difficult to come up with such a function on your own. By what strategy can one construct a similar counterexample from scratch? It feels like some piecewise linear function could do the trick, but I can't figure out, how to build it.",,"['calculus', 'functional-analysis', 'continuity']"
15,Prove L’Hôspital’s rule without the mean value theorem,Prove L’Hôspital’s rule without the mean value theorem,,Does there exist a proof for L’Hôspital’s rule without relying on the mean value theorem? Or do all proofs essentially rely on MVT?,Does there exist a proof for L’Hôspital’s rule without relying on the mean value theorem? Or do all proofs essentially rely on MVT?,,"['calculus', 'limits', 'derivatives']"
16,"What does the Gradient ""with respect to a position vector"" mean?","What does the Gradient ""with respect to a position vector"" mean?",,"I was studying John R. Taylor's book on Classical Mechanics and he introduced a confusing concept in page 140 (2005 edition): $$\nabla_1 = (\frac{\partial}{\partial x_1}+\frac{\partial}{\partial y_1}+\frac{\partial}{\partial z_1})$$ Where $$\mathbf r_1 = (x_1+y_1+z_1)$$ He calls this ""the gradient with respect to the coordinates of $\mathbf r_1$"". In general, what is the gradient with respect to a position vector? Isn't the gradient (in physics) just dependent on where we sit our x-axis and our y-axis? In other words, I've always known the gradient as: $$\nabla = (\frac{\partial}{\partial x}+\frac{\partial}{\partial y}+\frac{\partial}{\partial z})$$ (If context gives a clue, he was talking about the gradient of a potential $U$ in an isolated, two body system where the positions of the two particles are $\mathbf r_1$ and $\mathbf r_2$, and the claim is that the force on particle 1 due to particle 2 $\mathbf F_1 = -\nabla_1U(\mathbf r_1-\mathbf r_2)$. The same would apply for particle 2 interchanging 1 with 2 in the last formula.)","I was studying John R. Taylor's book on Classical Mechanics and he introduced a confusing concept in page 140 (2005 edition): $$\nabla_1 = (\frac{\partial}{\partial x_1}+\frac{\partial}{\partial y_1}+\frac{\partial}{\partial z_1})$$ Where $$\mathbf r_1 = (x_1+y_1+z_1)$$ He calls this ""the gradient with respect to the coordinates of $\mathbf r_1$"". In general, what is the gradient with respect to a position vector? Isn't the gradient (in physics) just dependent on where we sit our x-axis and our y-axis? In other words, I've always known the gradient as: $$\nabla = (\frac{\partial}{\partial x}+\frac{\partial}{\partial y}+\frac{\partial}{\partial z})$$ (If context gives a clue, he was talking about the gradient of a potential $U$ in an isolated, two body system where the positions of the two particles are $\mathbf r_1$ and $\mathbf r_2$, and the claim is that the force on particle 1 due to particle 2 $\mathbf F_1 = -\nabla_1U(\mathbf r_1-\mathbf r_2)$. The same would apply for particle 2 interchanging 1 with 2 in the last formula.)",,"['calculus', 'vectors', 'vector-analysis', 'mathematical-physics']"
17,Evaluate $\int_{-1}^1\frac{\sin(x)}{\arcsin(x)}dx$,Evaluate,\int_{-1}^1\frac{\sin(x)}{\arcsin(x)}dx,"Evaluate$$\int_{-1}^1\frac{\sin(x)}{\arcsin(x)} \, dx$$ Since $\frac{\sin(x)}{\arcsin(x)}$ is an even function, my first step was to simplify it to: $$2\int_0^1\frac{\sin(x)}{\arcsin(x)} \, dx$$ And after trying IBP, I quickly realised that the indefinite integral is non-elementary as verified by WolframAlpha. The usual method I use to continue for these types of problems is by differentiating under the integral sign, however I fail to see how it could help here. My only other idea is that using contour integration could help, but i'm not to good at it and can't use it very well.","Evaluate$$\int_{-1}^1\frac{\sin(x)}{\arcsin(x)} \, dx$$ Since $\frac{\sin(x)}{\arcsin(x)}$ is an even function, my first step was to simplify it to: $$2\int_0^1\frac{\sin(x)}{\arcsin(x)} \, dx$$ And after trying IBP, I quickly realised that the indefinite integral is non-elementary as verified by WolframAlpha. The usual method I use to continue for these types of problems is by differentiating under the integral sign, however I fail to see how it could help here. My only other idea is that using contour integration could help, but i'm not to good at it and can't use it very well.",,"['calculus', 'definite-integrals', 'closed-form']"
18,"Let $f$ be a continuous function such that $f(a)= f(b)$, prove a property","Let  be a continuous function such that , prove a property",f f(a)= f(b),"Let $ a,b $ be real numbers such that $a<b$ , and $f : [a,b] \rightarrow R $ is continuous such that $ f(a) = f(b) $ . Prove that there exists $\delta > 0$ such that for every $ t \in [0,\delta]$ there is $ x \in [a,b-t] $ that $f(x) = f(x+t)$ . I first defined a new function $g(x) = f(x) - f(x+t)$ for some $t \in [0,\delta]$ . And tried to prove that $g(a) = f(a) - f(a+t) < 0 \space, g(b-t) = f(b -t )  -f(b) >0 $ or vice versa. And then use the intermediate value theorem but couldn't proceed any further. I also tried to take $c$ is the absolute maximum or absolute minimum, and take $ \delta = b - c$ and work around the point $(c,f(c))$ point to find the property. Thanks for any help.","Let be real numbers such that , and is continuous such that . Prove that there exists such that for every there is that . I first defined a new function for some . And tried to prove that or vice versa. And then use the intermediate value theorem but couldn't proceed any further. I also tried to take is the absolute maximum or absolute minimum, and take and work around the point point to find the property. Thanks for any help."," a,b  a<b f : [a,b] \rightarrow R   f(a) = f(b)  \delta > 0  t \in [0,\delta]  x \in [a,b-t]  f(x) = f(x+t) g(x) = f(x) - f(x+t) t \in [0,\delta] g(a) = f(a) - f(a+t) < 0 \space, g(b-t) = f(b -t )  -f(b) >0  c  \delta = b - c (c,f(c))","['calculus', 'continuity']"
19,A difficult integral of reciprocal of sum of exponentials,A difficult integral of reciprocal of sum of exponentials,,"Consider the integral: $$\int_0^\infty \frac{1}{e^{ax}+e^{bx}+e^{cx}} dx$$ where $a, b, c$ are complex numbers. I encounter such integral when dealing with Laplace transform of co-functions. It turns out that the online integral calculator couldn’t even give any result. I suppose the integrand is well-defined since $e^x$ has no branch point on the whole complex plane. Maybe the integrand has poles, but I believe the integral exists in the sense of Cauchy p.v.. I am a bit reluctant to use the substitution $y=e^x$ since $y^p$ in general has branch points for complex $p$. Is there any method to compute such integral? Any help will be appreciated. EDIT: since a result in closed form is very unlikely to exist, a series as an answer will also be appreciated.","Consider the integral: $$\int_0^\infty \frac{1}{e^{ax}+e^{bx}+e^{cx}} dx$$ where $a, b, c$ are complex numbers. I encounter such integral when dealing with Laplace transform of co-functions. It turns out that the online integral calculator couldn’t even give any result. I suppose the integrand is well-defined since $e^x$ has no branch point on the whole complex plane. Maybe the integrand has poles, but I believe the integral exists in the sense of Cauchy p.v.. I am a bit reluctant to use the substitution $y=e^x$ since $y^p$ in general has branch points for complex $p$. Is there any method to compute such integral? Any help will be appreciated. EDIT: since a result in closed form is very unlikely to exist, a series as an answer will also be appreciated.",,"['calculus', 'definite-integrals']"
20,Find the area of curve,Find the area of curve,,"The folium of descartes is defined by $$x^3+y^3-3xy=0$$ (cartesian) $$r=\frac{3\sec \theta \tan \theta}{1+\tan^3 \theta}$$ (polar) or $$x=\frac{3at}{1+t^3}, y=\frac{3at^2}{1+t^3}$$ (parametric). It looks like this and has the slant asymptote $y=-x-1$ . I'm trying to show that the area between the asymptote and the infinite branches of the curve is the same as the area of the loop. I already know the area of the loop, it is $1.5$ units. I managed to show that the area between the asymptote and the branches is also $1.5$ units, but the method I used was last-resort inelegant** (I, or more specifically wolfram-alpha, solved the cartesian equation for $y$ and then the resultant integral). Are there different ways to show that the asymptote-branches area is 1.5 units? Showing that its area is the same as the area of the loop is fine as well, but probably harder/needing a special trick. Thank you! ** The problem source says about this problem: ""Use a CAS to evaluate the resultant integral"" so I didn't feel too guilty about making WA do it. The problem source is James Stewart Calculus, in the polar and parametric equations chapter.","The folium of descartes is defined by (cartesian) (polar) or (parametric). It looks like this and has the slant asymptote . I'm trying to show that the area between the asymptote and the infinite branches of the curve is the same as the area of the loop. I already know the area of the loop, it is units. I managed to show that the area between the asymptote and the branches is also units, but the method I used was last-resort inelegant** (I, or more specifically wolfram-alpha, solved the cartesian equation for and then the resultant integral). Are there different ways to show that the asymptote-branches area is 1.5 units? Showing that its area is the same as the area of the loop is fine as well, but probably harder/needing a special trick. Thank you! ** The problem source says about this problem: ""Use a CAS to evaluate the resultant integral"" so I didn't feel too guilty about making WA do it. The problem source is James Stewart Calculus, in the polar and parametric equations chapter.","x^3+y^3-3xy=0 r=\frac{3\sec \theta \tan \theta}{1+\tan^3 \theta} x=\frac{3at}{1+t^3}, y=\frac{3at^2}{1+t^3} y=-x-1 1.5 1.5 y","['calculus', 'integration', 'area', 'curves']"
21,An Infinite Limit?,An Infinite Limit?,,"The following question has a finite limit $\left( {{{11e} \over {24}}} \right)$, which can be easily obtained from the method of expansions . But I get an infinite limit, and I am not exactly sure where I have gone wrong. Please let me know also the reason why a certain step cannot be undertaken. Question - $$Evaluate\,\mathop {\lim }\limits_{x \to 0} {{{{(1 + x)}^{{\raise0.5ex\hbox{$\scriptstyle 1$} \kern-0.1em/\kern-0.15em \lower0.25ex\hbox{$\scriptstyle x$}}}} - e + {\textstyle{1 \over 2}}ex} \over {{x^2}}}$$ My Try - $$\eqalign{   & Let\,\,L = \mathop {\lim }\limits_{x \to 0} {{{{(1 + x)}^{{\raise0.5ex\hbox{$\scriptstyle 1$} \kern-0.1em/\kern-0.15em \lower0.25ex\hbox{$\scriptstyle x$}}}}} \over {{x^2}}} - \mathop {\lim }\limits_{x \to 0} {{e - {\textstyle{1 \over 2}}ex} \over {{x^2}}}  \cr    & Let\,\,{L_1} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {{{{{(1 + x)}^{{\textstyle{1 \over x}}}}} \over {{x^2}}}} \right)}}\,\,and\,{L_2} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {{{e - {\textstyle{1 \over 2}}ex} \over {{x^2}}}} \right)}}  \cr    & {L_1} = {e^{\mathop {\lim }\limits_{x \to 0} \left( {{{\log (1 + x)} \over x} - \log ({x^2})} \right)}}\,\,and\,{L_2} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {e\left( {1 - {\textstyle{x \over 2}}} \right)} \right) - \log {x^2}}}  \cr    & {L_1} = {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}}\,and\,\,{L_2}\, = \,{e^{1 + }}^{\mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right) - \mathop {\lim }\limits_{x \to 0} \log {x^2}}  \cr    & L = {L_1} - {L_2} = {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}} - {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2} + \mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right)}}  \cr    & \,\,\,\,\, = \,\,{e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}}.\left( {1 - {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right)}}} \right)  \cr    & \,\,\,\,\, = \,\,\mathop {\lim }\limits_{x \to 0} {e \over {{x^2}}}\left( {1 - \left( {1 - {\textstyle{x \over 2}}} \right)} \right)  \cr    & \,\,\,\,\, = \,\,\mathop {\lim }\limits_{x \to 0} \,{e \over {2x}} = \infty  \cr} $$","The following question has a finite limit $\left( {{{11e} \over {24}}} \right)$, which can be easily obtained from the method of expansions . But I get an infinite limit, and I am not exactly sure where I have gone wrong. Please let me know also the reason why a certain step cannot be undertaken. Question - $$Evaluate\,\mathop {\lim }\limits_{x \to 0} {{{{(1 + x)}^{{\raise0.5ex\hbox{$\scriptstyle 1$} \kern-0.1em/\kern-0.15em \lower0.25ex\hbox{$\scriptstyle x$}}}} - e + {\textstyle{1 \over 2}}ex} \over {{x^2}}}$$ My Try - $$\eqalign{   & Let\,\,L = \mathop {\lim }\limits_{x \to 0} {{{{(1 + x)}^{{\raise0.5ex\hbox{$\scriptstyle 1$} \kern-0.1em/\kern-0.15em \lower0.25ex\hbox{$\scriptstyle x$}}}}} \over {{x^2}}} - \mathop {\lim }\limits_{x \to 0} {{e - {\textstyle{1 \over 2}}ex} \over {{x^2}}}  \cr    & Let\,\,{L_1} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {{{{{(1 + x)}^{{\textstyle{1 \over x}}}}} \over {{x^2}}}} \right)}}\,\,and\,{L_2} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {{{e - {\textstyle{1 \over 2}}ex} \over {{x^2}}}} \right)}}  \cr    & {L_1} = {e^{\mathop {\lim }\limits_{x \to 0} \left( {{{\log (1 + x)} \over x} - \log ({x^2})} \right)}}\,\,and\,{L_2} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {e\left( {1 - {\textstyle{x \over 2}}} \right)} \right) - \log {x^2}}}  \cr    & {L_1} = {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}}\,and\,\,{L_2}\, = \,{e^{1 + }}^{\mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right) - \mathop {\lim }\limits_{x \to 0} \log {x^2}}  \cr    & L = {L_1} - {L_2} = {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}} - {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2} + \mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right)}}  \cr    & \,\,\,\,\, = \,\,{e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}}.\left( {1 - {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right)}}} \right)  \cr    & \,\,\,\,\, = \,\,\mathop {\lim }\limits_{x \to 0} {e \over {{x^2}}}\left( {1 - \left( {1 - {\textstyle{x \over 2}}} \right)} \right)  \cr    & \,\,\,\,\, = \,\,\mathop {\lim }\limits_{x \to 0} \,{e \over {2x}} = \infty  \cr} $$",,"['calculus', 'limits']"
22,Does anyone know what the series $\frac{n^x}{n!}$ converges to? [duplicate],Does anyone know what the series  converges to? [duplicate],\frac{n^x}{n!},"This question already has answers here : Infinite Series $\sum\limits_{k=1}^{\infty}\frac{k^n}{k!}$ (2 answers) Closed 6 years ago . Most people in math are familiar with the conventional taylor series, $$\sum_{n=0}^{\infty} \frac{x^{n}}{n!}$$ which converges to $$e^x$$ I came across an uncommon series from working with very unusual exponents, and even though it looks simple, I'm not sure how to approach it or much less its result. This one has k in the base in the numerator instead. I am fairly certain it converges, but to what I am not sure, it's not really the same as dealing with x^n. For this specific problem, I don't need to know the method, I am wondering only what it converges to. $$\sum_{n=0}^{\infty} \frac{n^{x}}{n!} = ?$$ Does anyone know what this converges to?","This question already has answers here : Infinite Series $\sum\limits_{k=1}^{\infty}\frac{k^n}{k!}$ (2 answers) Closed 6 years ago . Most people in math are familiar with the conventional taylor series, $$\sum_{n=0}^{\infty} \frac{x^{n}}{n!}$$ which converges to $$e^x$$ I came across an uncommon series from working with very unusual exponents, and even though it looks simple, I'm not sure how to approach it or much less its result. This one has k in the base in the numerator instead. I am fairly certain it converges, but to what I am not sure, it's not really the same as dealing with x^n. For this specific problem, I don't need to know the method, I am wondering only what it converges to. $$\sum_{n=0}^{\infty} \frac{n^{x}}{n!} = ?$$ Does anyone know what this converges to?",,['calculus']
23,Derivative of an Integral whose integrand is discontinuous,Derivative of an Integral whose integrand is discontinuous,,"Let, $g(x) =\int_{a}^{x} f(t) dt$ be an integral function. What can we say about $g'(c)$ when, a) $f$ is removable discontinuous at $c \in[a,x]$? b) $f$ is infinite discontinuous at $c\in [a,x]$? c) $f$ is jump discontinuous at $c\in [a,x]$? Here is what I think. a) Given, $$f(x) = \begin{cases} {x^3} & \text{if $x\in \mathbb{R}  - \{2\} $} \\ 10 & \text{if $x=2$} \end{cases}$$ Here, $x=2=c$ is a point of removable discontinuity. The right-hand derivative of $g(x)$ at $c$ is given as, $$g'(c) =\lim_{h\to 0^{+} } \frac{g(c+h)-g(c)}{h}= \lim_{h\to 0^{+} }\frac{\int_{c}^{c+h}f(t) dt}{h}$$ The left-hand derivative of $g(x)$ at $c$ is given as, $$g'(c) = \lim_{h\to 0^{+} } \frac{g(c-h)-g(c)}{-h}= \lim_{h\to 0^{+} }\frac{\int_{c-h}^{c}f(t) dt}{-h}$$ We see that the left and right derivatives converge at 8. This means that $g'(2)=8$. We know that, ""for continuous integrands"", $g'(x)=f(x)$. Here, $f(2)\neq 8$. This tells me that the derivative does not exist at $x=2$. Am I thinking in the right direction?","Let, $g(x) =\int_{a}^{x} f(t) dt$ be an integral function. What can we say about $g'(c)$ when, a) $f$ is removable discontinuous at $c \in[a,x]$? b) $f$ is infinite discontinuous at $c\in [a,x]$? c) $f$ is jump discontinuous at $c\in [a,x]$? Here is what I think. a) Given, $$f(x) = \begin{cases} {x^3} & \text{if $x\in \mathbb{R}  - \{2\} $} \\ 10 & \text{if $x=2$} \end{cases}$$ Here, $x=2=c$ is a point of removable discontinuity. The right-hand derivative of $g(x)$ at $c$ is given as, $$g'(c) =\lim_{h\to 0^{+} } \frac{g(c+h)-g(c)}{h}= \lim_{h\to 0^{+} }\frac{\int_{c}^{c+h}f(t) dt}{h}$$ The left-hand derivative of $g(x)$ at $c$ is given as, $$g'(c) = \lim_{h\to 0^{+} } \frac{g(c-h)-g(c)}{-h}= \lim_{h\to 0^{+} }\frac{\int_{c-h}^{c}f(t) dt}{-h}$$ We see that the left and right derivatives converge at 8. This means that $g'(2)=8$. We know that, ""for continuous integrands"", $g'(x)=f(x)$. Here, $f(2)\neq 8$. This tells me that the derivative does not exist at $x=2$. Am I thinking in the right direction?",,['calculus']
24,Laplace Transform to evaluate $\int_{0}^{\infty} e^{-2t} *t*\sin(4t)dt$,Laplace Transform to evaluate,\int_{0}^{\infty} e^{-2t} *t*\sin(4t)dt,"The question is, how can I find the value of the integral $$\int_{0}^{\infty} e^{-2t} *t*\sin(4t)dt$$ I thought I could solve it by saying this is $L(t\sin(4t))(2)$. Since $L(t\sin(4t)) = \frac{8s}{(s^2+16)^2}$ we have that the integral is this at $s = 2$, or that it is $\frac{1}{25}$. I know this is wrong, but why? EDIT: just checked wolfram alpha, it's right. But is it mathematically rigorous? (My instructor gave a MUCH longer answer).","The question is, how can I find the value of the integral $$\int_{0}^{\infty} e^{-2t} *t*\sin(4t)dt$$ I thought I could solve it by saying this is $L(t\sin(4t))(2)$. Since $L(t\sin(4t)) = \frac{8s}{(s^2+16)^2}$ we have that the integral is this at $s = 2$, or that it is $\frac{1}{25}$. I know this is wrong, but why? EDIT: just checked wolfram alpha, it's right. But is it mathematically rigorous? (My instructor gave a MUCH longer answer).",,"['calculus', 'laplace-transform']"
25,Evaluating a definite integral involving $ \tan^{-1}$,Evaluating a definite integral involving, \tan^{-1},The question is to evaluate $$\int_{\pi/2}^{5\pi/2} \frac{e^{\tan^{-1} \sin x}}{e^{\tan^{-1} \sin x}+e^{\tan^{-1} \cos x}}dx$$ I tried to take idea from the graph of $\tan^{-1} \tan x$ and rewrite the integral as $$\int_{\pi/2}^{5\pi/2} \frac{e^{\tan^{-1} \sin x}}{e^{\tan^{-1} \sin x}+e^{\tan^{-1} -\cos x}}dx$$.I couldn't proceed from here.Any ideas?Thanks.,The question is to evaluate $$\int_{\pi/2}^{5\pi/2} \frac{e^{\tan^{-1} \sin x}}{e^{\tan^{-1} \sin x}+e^{\tan^{-1} \cos x}}dx$$ I tried to take idea from the graph of $\tan^{-1} \tan x$ and rewrite the integral as $$\int_{\pi/2}^{5\pi/2} \frac{e^{\tan^{-1} \sin x}}{e^{\tan^{-1} \sin x}+e^{\tan^{-1} -\cos x}}dx$$.I couldn't proceed from here.Any ideas?Thanks.,,"['calculus', 'definite-integrals']"
26,How does this integration work when solving a DE?,How does this integration work when solving a DE?,,"$\def\d{\mathrm{d}}$I understand what differential equations are and what they are useful for. They're very interesting, but I'm not quite sure what is going on when actually solving one. This is what I'm trying to solve: $$\frac{\d y}{\d x} = 2y + 3.$$ As far as I know, this is a first-order linear ordinary differential equation. Seems fairly simple. The first step that I am told to do is to 'separate the variables'. So I multiply both sides of the equation by $\d x$ and then divide both sides by $2y + 3$ to end up with: $$\frac{\d y}{2y+3} = \d x.$$ So now I've got all of my $y$ terms on one side, and all of my $x$ terms on the other. The fact that I can split apart the $\frac{\d y}{\d x}$ does seem a bit strange to me, but I'm told that it's sensible and I'm willing to believe, for now, that that's an OK thing to do. After this, I'm told to 'integrate both sides' and I'm shown this as the next step: $$\int \frac{\d y}{2y+3} = \int \d x$$ Normally when integrating, I would denote the 'integral of [integrand] with respect to $x$' using the standard notation: $$\int \text{[integrand]}\,\d x$$ In definite integration, the $\int$ represents an infinite sum, and the $\d x$ represents, as usual, a very small change in $x$ that is being multiplied by the value of the function at that point in order to, as $\d x$ approaches zero, generate the exact value of the area under the curve. For now I will just accept that we also write it in this fashion when doing indefinite integration. However, the second step in the solution to the DE confuses me because it doesn't seem to follow this notation. If you want to integrate both sides, then that means we must be integrating $\d x$ on the right hand side. If this is true, then I would normally write it as $\int \d x\,\d x$, because it is the integral of $\d x$ with respect to $x$ . However, in the solution it simply shows $\int \d x$ as if we have lost the $\d x$ that we would have put in as notation. The other side of the equation also doesn't have a $\d y$ at the end, as I would expect it to. My real question is: What do $\d x$ and $\d y$ represent on their own ? I understand that $\dfrac{\d y}{\d x}$ represents the derivative of $y$ with respect to $x$, and that it is representing an instantaneous rate of change because it is essentially representing an infinitesimally small change in $y$, divided by an infinitesimally small change in $x$ (the gradient at a point). However, when I see something like $\d x$ on its own, I'm not sure what the meaning of it is any more. It's an infinitesimally small change in $x$, right? How do we integrate that? What does it mean ? I want to be very clear that I'm not asking someone to walk me through solving it, because the web is full of examples of solving these types of equations. I'd be much more grateful for a conceptual answer to my question. And as an extra question: How does integrating both sides of an equation with respect to different variables make sense? Surely the two sides would no longer be equal.","$\def\d{\mathrm{d}}$I understand what differential equations are and what they are useful for. They're very interesting, but I'm not quite sure what is going on when actually solving one. This is what I'm trying to solve: $$\frac{\d y}{\d x} = 2y + 3.$$ As far as I know, this is a first-order linear ordinary differential equation. Seems fairly simple. The first step that I am told to do is to 'separate the variables'. So I multiply both sides of the equation by $\d x$ and then divide both sides by $2y + 3$ to end up with: $$\frac{\d y}{2y+3} = \d x.$$ So now I've got all of my $y$ terms on one side, and all of my $x$ terms on the other. The fact that I can split apart the $\frac{\d y}{\d x}$ does seem a bit strange to me, but I'm told that it's sensible and I'm willing to believe, for now, that that's an OK thing to do. After this, I'm told to 'integrate both sides' and I'm shown this as the next step: $$\int \frac{\d y}{2y+3} = \int \d x$$ Normally when integrating, I would denote the 'integral of [integrand] with respect to $x$' using the standard notation: $$\int \text{[integrand]}\,\d x$$ In definite integration, the $\int$ represents an infinite sum, and the $\d x$ represents, as usual, a very small change in $x$ that is being multiplied by the value of the function at that point in order to, as $\d x$ approaches zero, generate the exact value of the area under the curve. For now I will just accept that we also write it in this fashion when doing indefinite integration. However, the second step in the solution to the DE confuses me because it doesn't seem to follow this notation. If you want to integrate both sides, then that means we must be integrating $\d x$ on the right hand side. If this is true, then I would normally write it as $\int \d x\,\d x$, because it is the integral of $\d x$ with respect to $x$ . However, in the solution it simply shows $\int \d x$ as if we have lost the $\d x$ that we would have put in as notation. The other side of the equation also doesn't have a $\d y$ at the end, as I would expect it to. My real question is: What do $\d x$ and $\d y$ represent on their own ? I understand that $\dfrac{\d y}{\d x}$ represents the derivative of $y$ with respect to $x$, and that it is representing an instantaneous rate of change because it is essentially representing an infinitesimally small change in $y$, divided by an infinitesimally small change in $x$ (the gradient at a point). However, when I see something like $\d x$ on its own, I'm not sure what the meaning of it is any more. It's an infinitesimally small change in $x$, right? How do we integrate that? What does it mean ? I want to be very clear that I'm not asking someone to walk me through solving it, because the web is full of examples of solving these types of equations. I'd be much more grateful for a conceptual answer to my question. And as an extra question: How does integrating both sides of an equation with respect to different variables make sense? Surely the two sides would no longer be equal.",,"['calculus', 'integration', 'ordinary-differential-equations']"
27,Evaluate the given limit,Evaluate the given limit,,Given a function $f : R → R$ for which $|f(x) − 3| ≤ x^2$. Find $$\lim_{ x\to0}\frac{f(x) - \sqrt{x^2 + 9}}{x}$$ Can  the function $f(x)$  be considered as  $x^2 + 3$  and go about evaluating the limit using the Limit laws?,Given a function $f : R → R$ for which $|f(x) − 3| ≤ x^2$. Find $$\lim_{ x\to0}\frac{f(x) - \sqrt{x^2 + 9}}{x}$$ Can  the function $f(x)$  be considered as  $x^2 + 3$  and go about evaluating the limit using the Limit laws?,,"['calculus', 'limits']"
28,How do I find the equation of a curve formed by a series of lines tangent to a curve?,How do I find the equation of a curve formed by a series of lines tangent to a curve?,,"When I was young I used to draw a sequence of straight lines on graph paper which made a curve after I finished.  On a coordinate plane, the lines would be equivalent to starting at $y=9$ on the $y$ axis and ending at $x=1$ on the $x$ axis.  With each line, I would decrease the $y$ by one unit and increase the $x$ by one unit. Here is a desmos graph that illustrates. https://www.desmos.com/calculator/u4ea8swmfg Here is a similar example where the angle between the lines is 60 degrees. https://drive.google.com/open?id=0B5QHq_oPha0ybGdrbFNhUHRPOGc I think each line is basically a tangent line along the curve produced by taking a first derivative. Are these types of curves parabolas or possibly hyperbolae?  And how could I find the equation of such a curve?","When I was young I used to draw a sequence of straight lines on graph paper which made a curve after I finished.  On a coordinate plane, the lines would be equivalent to starting at $y=9$ on the $y$ axis and ending at $x=1$ on the $x$ axis.  With each line, I would decrease the $y$ by one unit and increase the $x$ by one unit. Here is a desmos graph that illustrates. https://www.desmos.com/calculator/u4ea8swmfg Here is a similar example where the angle between the lines is 60 degrees. https://drive.google.com/open?id=0B5QHq_oPha0ybGdrbFNhUHRPOGc I think each line is basically a tangent line along the curve produced by taking a first derivative. Are these types of curves parabolas or possibly hyperbolae?  And how could I find the equation of such a curve?",,"['calculus', 'analytic-geometry', 'conic-sections', 'hyperbolic-geometry']"
29,Hyperbolic substitution for $\int\frac{dx}{x\sqrt{1-x^2}}$,Hyperbolic substitution for,\int\frac{dx}{x\sqrt{1-x^2}},"Substitute $x=\sinh\theta$, $\cosh\theta$ or $\tanh\theta$. After integration change back to $x$.$$\int\frac{dx}{x\sqrt{1-x^2}}$$ Substituting $x=\tanh\theta$, we have $$\begin{align} \int\frac{dx}{x\sqrt{1-x^2}}&=\int\frac{\text{sech}^2\,\theta\,d\theta}{\tanh\theta\sqrt{1-\tanh^2\theta}}\\ &=\int\text{csch }\theta\,d\theta\\ &=-\ln|\text{csch }\theta+\text{coth }\theta|+C \end{align}$$ Here comes the problem. We can substitute back $\text{coth }\theta=\frac1x$ but what do we have for $\text{csch }\theta$? We have $\text{csch}^2\,\theta=\coth^2\theta-1$, but neither $\sqrt{\coth^2\theta-1}$ nor $-\sqrt{\coth^2\theta-1}$ seems to be a one-off solution for $\text{csch }\theta$. How should we proceed?","Substitute $x=\sinh\theta$, $\cosh\theta$ or $\tanh\theta$. After integration change back to $x$.$$\int\frac{dx}{x\sqrt{1-x^2}}$$ Substituting $x=\tanh\theta$, we have $$\begin{align} \int\frac{dx}{x\sqrt{1-x^2}}&=\int\frac{\text{sech}^2\,\theta\,d\theta}{\tanh\theta\sqrt{1-\tanh^2\theta}}\\ &=\int\text{csch }\theta\,d\theta\\ &=-\ln|\text{csch }\theta+\text{coth }\theta|+C \end{align}$$ Here comes the problem. We can substitute back $\text{coth }\theta=\frac1x$ but what do we have for $\text{csch }\theta$? We have $\text{csch}^2\,\theta=\coth^2\theta-1$, but neither $\sqrt{\coth^2\theta-1}$ nor $-\sqrt{\coth^2\theta-1}$ seems to be a one-off solution for $\text{csch }\theta$. How should we proceed?",,"['calculus', 'integration', 'hyperbolic-functions']"
30,Cross product with curl,Cross product with curl,,"I'm trying to compute $u \times(\nabla \times v)$. My solution so far is below: \begin{align*} [u \times (\nabla \times v)]_i& = \epsilon_{ijk}u_j\epsilon_{klm}\partial_lv_m \\ & =\epsilon_{kij}\epsilon_{klm}u_j\partial_lv_m \\  &=u_j\partial_lv_m(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}) \\ & =u_j\partial_iv_j-u_j\partial_jv_i \\ \end{align*} I can see that the second term will give me $(u\cdot\nabla)v$, but I'm not sure what to do with the first term.","I'm trying to compute $u \times(\nabla \times v)$. My solution so far is below: \begin{align*} [u \times (\nabla \times v)]_i& = \epsilon_{ijk}u_j\epsilon_{klm}\partial_lv_m \\ & =\epsilon_{kij}\epsilon_{klm}u_j\partial_lv_m \\  &=u_j\partial_lv_m(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}) \\ & =u_j\partial_iv_j-u_j\partial_jv_i \\ \end{align*} I can see that the second term will give me $(u\cdot\nabla)v$, but I'm not sure what to do with the first term.",,"['calculus', 'cross-product']"
31,Showing a complex function is NOT holomorphic,Showing a complex function is NOT holomorphic,,"I am a bit confused on all the different ways to show something is holomorphic, and I am wondering if my thoughts about the following are on the right track. Show $$g(z)=\frac{z^*}{z^2+1}$$ is not holomorphic (where $z^{*}$ denotes   the complex conjugate). I know that a function is holomorphic iff its derivative with respect to $z^*$ is identically zero, but I am not sure I am taking the derivative with respect tot the conjugate correct. Would it simply be $\frac{\partial g}{\partial z^{*}}=\frac{(z^2+1)(1)-(z^*)(0)}{(z^2+1)^2}=\frac{1}{z^2+1}$ which is only zero for $z=i$, so in general it isn't holomorphic? Is that correct reasoning or no? And another example of a non holomorphic, $$h(z)=z((z^{*})^2-z^2)$$ $$=(zz^{*})z^{*}-z^2=|z|^{2}z^*-z^2$$ so would the $z^{*}$ derivative just be $|z|^2$ which is zero only at the origin? I'm just confused on how to actually show this. I think I am also confused on taking the deravtive with respect to the conjugate. I know I could also trying separating into the form of $U(x,y)+IV(x,y)$ and showing that the CR don't hold, but that seems even more difficult. Looking for any and all help/advice. Thank you","I am a bit confused on all the different ways to show something is holomorphic, and I am wondering if my thoughts about the following are on the right track. Show $$g(z)=\frac{z^*}{z^2+1}$$ is not holomorphic (where $z^{*}$ denotes   the complex conjugate). I know that a function is holomorphic iff its derivative with respect to $z^*$ is identically zero, but I am not sure I am taking the derivative with respect tot the conjugate correct. Would it simply be $\frac{\partial g}{\partial z^{*}}=\frac{(z^2+1)(1)-(z^*)(0)}{(z^2+1)^2}=\frac{1}{z^2+1}$ which is only zero for $z=i$, so in general it isn't holomorphic? Is that correct reasoning or no? And another example of a non holomorphic, $$h(z)=z((z^{*})^2-z^2)$$ $$=(zz^{*})z^{*}-z^2=|z|^{2}z^*-z^2$$ so would the $z^{*}$ derivative just be $|z|^2$ which is zero only at the origin? I'm just confused on how to actually show this. I think I am also confused on taking the deravtive with respect to the conjugate. I know I could also trying separating into the form of $U(x,y)+IV(x,y)$ and showing that the CR don't hold, but that seems even more difficult. Looking for any and all help/advice. Thank you",,"['calculus', 'complex-analysis', 'complex-numbers', 'holomorphic-functions']"
32,Closed form of $\iint_{\mathbb{R}^2} \ln \left(\frac{1}{|x-y|}\right) \frac{4}{(1+|y|^2)^2} dy$,Closed form of,\iint_{\mathbb{R}^2} \ln \left(\frac{1}{|x-y|}\right) \frac{4}{(1+|y|^2)^2} dy,"While reading a paper I stumbled across the integral $$ \iint_{\mathbb{R}^2} \ln \left(\frac{1}{|x-y|}\right) \frac{4}{(1+|y|^2)^2} d^2y\,, $$ where $x \in \mathbb{R}^2$ is fixed. I think the paper implicitly claims that this is equal to $-2 \pi \ln (1+|x|^2)$. I'm not sure if this is a hard integral at all, but considering the amount of proficient integrators on this site, I thought that I should post this question here. To provide some context, the term $\ln \frac{1}{|x-y|}$ comes from the Laplacian's Green function (it is the leading term near $x$) and the term $\frac{4}{(1+|y|^2)^2}$ is of course the ""density"" of the stereographic projection metric.","While reading a paper I stumbled across the integral $$ \iint_{\mathbb{R}^2} \ln \left(\frac{1}{|x-y|}\right) \frac{4}{(1+|y|^2)^2} d^2y\,, $$ where $x \in \mathbb{R}^2$ is fixed. I think the paper implicitly claims that this is equal to $-2 \pi \ln (1+|x|^2)$. I'm not sure if this is a hard integral at all, but considering the amount of proficient integrators on this site, I thought that I should post this question here. To provide some context, the term $\ln \frac{1}{|x-y|}$ comes from the Laplacian's Green function (it is the leading term near $x$) and the term $\frac{4}{(1+|y|^2)^2}$ is of course the ""density"" of the stereographic projection metric.",,"['calculus', 'integration', 'definite-integrals', 'vector-analysis', 'closed-form']"
33,Real function with complex antiderivative $\frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}$?,Real function with complex antiderivative ?,\frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}},"Consider this indefinite integral (I'm interested in the interval $x>0$): $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx$$ By substitution: $$u=\sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{1+\frac{1}{x^2}}}, \qquad x=\frac{1}{2u \sqrt{u^2-1}}$$ We get a closed form antiderivative: $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx=\sqrt{2} \tanh^{-1} \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{1+\frac{1}{x^2}}} +C$$ Now the inverse hyperbolic tangent is real only for the argument in $(-1,1)$. But in our case the argument for all real $x$ is $ > 1$. How can the function with real values on $x>0$ have a complex antiderivative? Edit To be clear, this is the correct antiderivative. I.e. by differentiating it we get the function under the integral. How would you explain this without appealing to complex analysis, i.e. branches? This is a real valued function for positive $x$, so this can be given as an assignment to a first year calculus student for example Edit 2 The derivation of the antiderivative after the substitution: $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx=\sqrt{2} \int \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{1+\frac{1}{x^2}}} \frac{dx}{\sqrt{x^2+1}}=\sqrt{2} \int u \frac{dx}{\sqrt{x^2+1}}$$ $$\sqrt{x^2+1}=x(2u^2-1)$$ $$dx=-x\frac{2u^2-1}{u(u^2-1)}du$$ $$\frac{dx}{\sqrt{1+x^2}}=-\frac{du}{u(u^2-1)}=\frac{du}{u(1-u^2)}$$ $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx=\sqrt{2} \int \frac{du}{1-u^2}=\sqrt{2} \tanh^{-1} u$$","Consider this indefinite integral (I'm interested in the interval $x>0$): $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx$$ By substitution: $$u=\sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{1+\frac{1}{x^2}}}, \qquad x=\frac{1}{2u \sqrt{u^2-1}}$$ We get a closed form antiderivative: $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx=\sqrt{2} \tanh^{-1} \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{1+\frac{1}{x^2}}} +C$$ Now the inverse hyperbolic tangent is real only for the argument in $(-1,1)$. But in our case the argument for all real $x$ is $ > 1$. How can the function with real values on $x>0$ have a complex antiderivative? Edit To be clear, this is the correct antiderivative. I.e. by differentiating it we get the function under the integral. How would you explain this without appealing to complex analysis, i.e. branches? This is a real valued function for positive $x$, so this can be given as an assignment to a first year calculus student for example Edit 2 The derivation of the antiderivative after the substitution: $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx=\sqrt{2} \int \sqrt{\frac{1}{2}+\frac{1}{2}\sqrt{1+\frac{1}{x^2}}} \frac{dx}{\sqrt{x^2+1}}=\sqrt{2} \int u \frac{dx}{\sqrt{x^2+1}}$$ $$\sqrt{x^2+1}=x(2u^2-1)$$ $$dx=-x\frac{2u^2-1}{u(u^2-1)}du$$ $$\frac{dx}{\sqrt{1+x^2}}=-\frac{du}{u(u^2-1)}=\frac{du}{u(1-u^2)}$$ $$\int \frac{\sqrt{x+\sqrt{x^2+1}}}{\sqrt{x}\sqrt{x^2+1}}dx=\sqrt{2} \int \frac{du}{1-u^2}=\sqrt{2} \tanh^{-1} u$$",,"['calculus', 'integration', 'complex-analysis', 'indefinite-integrals']"
34,Closed form for $\prod\limits_{l=1}^\infty \cos\frac{x}{3^l}$,Closed form for,\prod\limits_{l=1}^\infty \cos\frac{x}{3^l},Is there any closed form for the infinite product $\prod_{l=1}^\infty \cos\dfrac{x}{3^l}$? I think it is convergent for any $x\in\mathbb{R}$. I think there might be one because there is a closed form for $\prod_{l=1}^\infty\cos\dfrac{x}{2^l}$ if I'm not wrong.,Is there any closed form for the infinite product $\prod_{l=1}^\infty \cos\dfrac{x}{3^l}$? I think it is convergent for any $x\in\mathbb{R}$. I think there might be one because there is a closed form for $\prod_{l=1}^\infty\cos\dfrac{x}{2^l}$ if I'm not wrong.,,"['calculus', 'sequences-and-series', 'closed-form', 'infinite-product', 'trigonometric-series']"
35,Solutions to $\lfloor x\rfloor\lfloor y\rfloor=x+y$,Solutions to,\lfloor x\rfloor\lfloor y\rfloor=x+y,"Find all solutions to $$\lfloor x\rfloor\lfloor y\rfloor=x+y$$ and show that the non-Integral solutions lie on two unique lines. Also determine the equations of these 2 lines. I divided the problem into 2 cases: $$\text{Case 1} : x,y \in \Bbb Z$$$$\text{Case 2}:x,y\notin \Bbb Z$$ $$$$ For Case 1, it can be shown that the solutions are those of the equation $xy=x+y$ and these are clearly $(0,0)$ and $(2,2)$. $$$$ For Case 2, my approach was as follows: $$\lfloor x\rfloor\lfloor y\rfloor-\lfloor x\rfloor-\lfloor y\rfloor=\{x\}+\{y\}$$ $$(\lfloor y\rfloor-1)(\lfloor x\rfloor-1)=\{x\}+\{y\}+1$$ Since $1<\{x\}+\{y\}+1<3$ (because both $x,y$ have fractional parts in Case 2) $$1<(\lfloor y\rfloor-1)(\lfloor x\rfloor-1)<3$$ I don't know how to proceed further and would be grateful for any help with this problem. $$$$Many thanks in anticipation! PS: I believe this is an old IITJEE question and is not homework.","Find all solutions to $$\lfloor x\rfloor\lfloor y\rfloor=x+y$$ and show that the non-Integral solutions lie on two unique lines. Also determine the equations of these 2 lines. I divided the problem into 2 cases: $$\text{Case 1} : x,y \in \Bbb Z$$$$\text{Case 2}:x,y\notin \Bbb Z$$ $$$$ For Case 1, it can be shown that the solutions are those of the equation $xy=x+y$ and these are clearly $(0,0)$ and $(2,2)$. $$$$ For Case 2, my approach was as follows: $$\lfloor x\rfloor\lfloor y\rfloor-\lfloor x\rfloor-\lfloor y\rfloor=\{x\}+\{y\}$$ $$(\lfloor y\rfloor-1)(\lfloor x\rfloor-1)=\{x\}+\{y\}+1$$ Since $1<\{x\}+\{y\}+1<3$ (because both $x,y$ have fractional parts in Case 2) $$1<(\lfloor y\rfloor-1)(\lfloor x\rfloor-1)<3$$ I don't know how to proceed further and would be grateful for any help with this problem. $$$$Many thanks in anticipation! PS: I believe this is an old IITJEE question and is not homework.",,"['calculus', 'number-theory', 'functions', 'diophantine-equations', 'ceiling-and-floor-functions']"
36,proof of derivative of an exponential function,proof of derivative of an exponential function,,"I was told to assume that $$\ln b=\lim_{h\to 0} \frac{\left(b^h-1\right)}{h}$$ where b is a positive, real, base. Unfortunately, being told to assume something isn't good enough. When using L'Hopital's with a base of $e$, it can be shown that the limit approaches $e^0$, which of course equals 1, or, $\ln e$. However, I was hung up on proving that for any base, the limit will approach the natural log of the base, without using the direct proof that $$\frac{d}{dx}b^x=b^x(\ln b)$$ which is what's trying to be proved in the first place. Is L'Hopital's even the right route to go? Thanks in advance.","I was told to assume that $$\ln b=\lim_{h\to 0} \frac{\left(b^h-1\right)}{h}$$ where b is a positive, real, base. Unfortunately, being told to assume something isn't good enough. When using L'Hopital's with a base of $e$, it can be shown that the limit approaches $e^0$, which of course equals 1, or, $\ln e$. However, I was hung up on proving that for any base, the limit will approach the natural log of the base, without using the direct proof that $$\frac{d}{dx}b^x=b^x(\ln b)$$ which is what's trying to be proved in the first place. Is L'Hopital's even the right route to go? Thanks in advance.",,"['calculus', 'limits']"
37,An alternative to integration by trigonometric substitution?,An alternative to integration by trigonometric substitution?,,"When I was a Calculus II student many (about 4800) moons ago, our professor taught us an alternative to trig sub.  For example, if we have $$   \int \frac{dx}{x^2\sqrt{x^2 - 9}}, $$ we would evaluate with trig sub by letting $x = 3\sec\theta$ and we'd get: \begin{align}   \int \frac{dx}{x^2\sqrt{x^2 - 9}} &= \int\frac{3\sec\theta\tan\theta \, d\theta}{9\sec^2\theta\sqrt{9\sec^2\theta - 9}}\\[0.3cm]     &= \frac{1}{3}\int\frac{\tan\theta\,d\theta}{\sec\theta \cdot 3\tan\theta}\\[0.3cm]     &= \frac{1}{9} \int \cos\theta \, d\theta\\[0.3cm]     &= \frac{1}{9} \sin\theta + C\\[0.3cm]     &= \frac{\sqrt{x^2-9}}{9x} + C \end{align} The alternative method he showed us goes like this for this problem: \begin{align}   \int\frac{dx}{x^2\sqrt{x^2 - 9}} &= \int \frac{dx}{x^2\sqrt{x^2(1 - 9x^{-2})}}\\[0.3cm]     &= \int\frac{dx}{x^3\sqrt{1 - 9x^{-2}}}\\[0.3cm]     &= \int\frac{x^{-3} \, dx}{\sqrt{1 - 9x^{-2}}} \end{align} Yes, I know that technically $\sqrt{x^2} = |x|$.  But trig sub also comes with domain restrictions. Now let $u = 1 - 9x^{-2}$.  Then $du = 18x^{-3} \, dx$ and we have: \begin{align} \int\frac{x^{-3} \, dx}{\sqrt{1 - 9x^{-2}}} &= \frac{1}{18}\int\frac{du}{\sqrt{u}}\\[0.3cm]     &= \frac{1}{18} \cdot 2\sqrt{u} + C\\[0.3cm]     &= \frac{\sqrt{1 - 9x^{-2}}}{9} + C \end{align} Same answer, different form.  My question has two parts, sort of (since an answer to #1 could point to an answer to #2). Does anyone know the history of this method? Is this method perfectly interchangeable with trig sub?  In other words, can an integral be done using this method iff it can also be done using trig sub? IIRC, which I may not because of all the moons, our professor said there's no ""general formula"" to get this to work.  You just kind of have to eyeball it and try it.  But I don't remember him saying whether or not it would work all the time. Thanks!","When I was a Calculus II student many (about 4800) moons ago, our professor taught us an alternative to trig sub.  For example, if we have $$   \int \frac{dx}{x^2\sqrt{x^2 - 9}}, $$ we would evaluate with trig sub by letting $x = 3\sec\theta$ and we'd get: \begin{align}   \int \frac{dx}{x^2\sqrt{x^2 - 9}} &= \int\frac{3\sec\theta\tan\theta \, d\theta}{9\sec^2\theta\sqrt{9\sec^2\theta - 9}}\\[0.3cm]     &= \frac{1}{3}\int\frac{\tan\theta\,d\theta}{\sec\theta \cdot 3\tan\theta}\\[0.3cm]     &= \frac{1}{9} \int \cos\theta \, d\theta\\[0.3cm]     &= \frac{1}{9} \sin\theta + C\\[0.3cm]     &= \frac{\sqrt{x^2-9}}{9x} + C \end{align} The alternative method he showed us goes like this for this problem: \begin{align}   \int\frac{dx}{x^2\sqrt{x^2 - 9}} &= \int \frac{dx}{x^2\sqrt{x^2(1 - 9x^{-2})}}\\[0.3cm]     &= \int\frac{dx}{x^3\sqrt{1 - 9x^{-2}}}\\[0.3cm]     &= \int\frac{x^{-3} \, dx}{\sqrt{1 - 9x^{-2}}} \end{align} Yes, I know that technically $\sqrt{x^2} = |x|$.  But trig sub also comes with domain restrictions. Now let $u = 1 - 9x^{-2}$.  Then $du = 18x^{-3} \, dx$ and we have: \begin{align} \int\frac{x^{-3} \, dx}{\sqrt{1 - 9x^{-2}}} &= \frac{1}{18}\int\frac{du}{\sqrt{u}}\\[0.3cm]     &= \frac{1}{18} \cdot 2\sqrt{u} + C\\[0.3cm]     &= \frac{\sqrt{1 - 9x^{-2}}}{9} + C \end{align} Same answer, different form.  My question has two parts, sort of (since an answer to #1 could point to an answer to #2). Does anyone know the history of this method? Is this method perfectly interchangeable with trig sub?  In other words, can an integral be done using this method iff it can also be done using trig sub? IIRC, which I may not because of all the moons, our professor said there's no ""general formula"" to get this to work.  You just kind of have to eyeball it and try it.  But I don't remember him saying whether or not it would work all the time. Thanks!",,"['calculus', 'integration']"
38,"Integrating $\int \frac{\sin x-\cos x}{(\sin x+\cos x)\sqrt{(\sin x \cos x + \sin^2x\cos^2x)}}\,dx$",Integrating,"\int \frac{\sin x-\cos x}{(\sin x+\cos x)\sqrt{(\sin x \cos x + \sin^2x\cos^2x)}}\,dx","I came across a question today... Integrate $\int \dfrac{\sin x-\cos x}{(\sin x+\cos x)\sqrt{(\sin x \cos x + \sin^2x\cos^2x)}}\,dx$ How to do it? I tried 1. to take $\sin x \cos x =t$ but no result 2. to convert the thing in the square root into $\sin x +\cos x$ so that I could take $\sin x + \cos x = t$ but then something I got is $\int\frac{-2}{t|t+1|\sqrt{t-1}}\,dt$. Now I don't know how to get past through it.","I came across a question today... Integrate $\int \dfrac{\sin x-\cos x}{(\sin x+\cos x)\sqrt{(\sin x \cos x + \sin^2x\cos^2x)}}\,dx$ How to do it? I tried 1. to take $\sin x \cos x =t$ but no result 2. to convert the thing in the square root into $\sin x +\cos x$ so that I could take $\sin x + \cos x = t$ but then something I got is $\int\frac{-2}{t|t+1|\sqrt{t-1}}\,dt$. Now I don't know how to get past through it.",,"['calculus', 'integration', 'indefinite-integrals']"
39,How many arbitrary constants to put in these integrals?,How many arbitrary constants to put in these integrals?,,"How many different constants is it right to use in these cases of integration? $1) \int \frac{1}{x} dx= \begin{cases} \log(x)+c_1, x>0\\\log(-x)+c_2 x<0 \end{cases}$ Since the domain it's not an interval, so the constants can be different, right? $2)\int \mid x \mid dx=\begin{cases} \frac{x^2}{2}+c_1, x\geq0 \\ -\frac{x^2}{2}+c_2 x<0 \end{cases}=\begin{cases} \frac{x^2}{2}+c_1, x\geq0 \\ -\frac{x^2}{2}+c_1 x<0 \end{cases}$ In this case I must impose $c_1=c_2$ right? Otherwise the function is not even continuous in $0$. Are these correct? Thanks for your help","How many different constants is it right to use in these cases of integration? $1) \int \frac{1}{x} dx= \begin{cases} \log(x)+c_1, x>0\\\log(-x)+c_2 x<0 \end{cases}$ Since the domain it's not an interval, so the constants can be different, right? $2)\int \mid x \mid dx=\begin{cases} \frac{x^2}{2}+c_1, x\geq0 \\ -\frac{x^2}{2}+c_2 x<0 \end{cases}=\begin{cases} \frac{x^2}{2}+c_1, x\geq0 \\ -\frac{x^2}{2}+c_1 x<0 \end{cases}$ In this case I must impose $c_1=c_2$ right? Otherwise the function is not even continuous in $0$. Are these correct? Thanks for your help",,"['calculus', 'integration']"
40,Show that $\int_{0}^{+\infty} \frac{\sin(x)}{x(x^2+1)} dx = \frac{\pi}{2}\left(1-\frac{1}{e}\right) $,Show that,\int_{0}^{+\infty} \frac{\sin(x)}{x(x^2+1)} dx = \frac{\pi}{2}\left(1-\frac{1}{e}\right) ,"I'm trying to show that $$\int_{0}^{+\infty}  \frac{\sin(x)}{x(x^2+1)} dx = \frac{\pi}{2}\left(1-\frac{1}{e}\right) $$ using Jordan's lemma and contour integration. MY ATTEMPT: The function in the integrand is even, so I have: $$\int_{0}^{+\infty}  \frac{\sin(x)}{x(x^2+1)} dx =\frac{1}{2}\int_{-\infty}^{+\infty}  \frac{\sin(x)}{x(x^2+1)} dx$$ There is a simple pole at $z=0$ and poles at $z=+i, z=-i$. A method in the chapter I am working on (Ablowitz & Fokas sections 4.2 & 4.3) usually considers the integral $$\int_{-\infty}^{+\infty}  \frac{e^{ix}}{x(x^2+1)} dx=2\pi iRes\left(\frac{e^{ix}}{x(x^2+1)},z=i,-i,0\right)$$ Which when I compute results in $\dfrac{-(-1+e)^2\pi}{2e}$, which is close but not quite the answer. (Notice that factored in another way the answer is also equal to $\dfrac{(-1+e)\pi}{2e}$. But I am not sure if this will work, instead another example builds a contour $C_r+C_e+(-R,-e)+(R,e)$ which avoids the poles and thus integrating over that yields zero and helps me get my answer. Unfortunately, this attempt does not give me the right value either. Do any of you integration whizzes out there have anything for me? Many thanks.","I'm trying to show that $$\int_{0}^{+\infty}  \frac{\sin(x)}{x(x^2+1)} dx = \frac{\pi}{2}\left(1-\frac{1}{e}\right) $$ using Jordan's lemma and contour integration. MY ATTEMPT: The function in the integrand is even, so I have: $$\int_{0}^{+\infty}  \frac{\sin(x)}{x(x^2+1)} dx =\frac{1}{2}\int_{-\infty}^{+\infty}  \frac{\sin(x)}{x(x^2+1)} dx$$ There is a simple pole at $z=0$ and poles at $z=+i, z=-i$. A method in the chapter I am working on (Ablowitz & Fokas sections 4.2 & 4.3) usually considers the integral $$\int_{-\infty}^{+\infty}  \frac{e^{ix}}{x(x^2+1)} dx=2\pi iRes\left(\frac{e^{ix}}{x(x^2+1)},z=i,-i,0\right)$$ Which when I compute results in $\dfrac{-(-1+e)^2\pi}{2e}$, which is close but not quite the answer. (Notice that factored in another way the answer is also equal to $\dfrac{(-1+e)\pi}{2e}$. But I am not sure if this will work, instead another example builds a contour $C_r+C_e+(-R,-e)+(R,e)$ which avoids the poles and thus integrating over that yields zero and helps me get my answer. Unfortunately, this attempt does not give me the right value either. Do any of you integration whizzes out there have anything for me? Many thanks.",,"['calculus', 'integration', 'complex-analysis', 'contour-integration', 'complex-integration']"
41,Tangent vectors as derivatives,Tangent vectors as derivatives,,"I am reading ""An introduction to manifolds"" by Loring Tu and I struggle understanding a concept. In the book the directional derivative is given a tangent vector $v$ at a point $p$ the map : $$D_v : \mathcal{C}_p^\infty \rightarrow \mathbb{R}$$ The follows by explaining that a map satisfying the leibniz rule is a ""Derivation at p"". The we can define the map : $$\phi : T_p(\mathbb{R}^n) \rightarrow \mathcal{D}_p(\mathbb{R}^n)\\ v \rightarrow D_v = \sum v^i \frac{d}{dx^i} |_0$$ where : the ""d"" are partials evaluated at $0$. So far so good, then the book proceed to the proof that this map is an isomorphisme of vector spaces. And by this decide to write every tangent vectors $v = (v_1,...,v_n) = \sum v^i e_i$ as $v = \sum v^i \frac{d}{dx^i}|_p$ I have trouble understanding how a vector could be equal to a function. For me the RHS takes a function and the LHS is just a vector. I would like to know how is that intuitively correct and also what is the proof of the last equation.","I am reading ""An introduction to manifolds"" by Loring Tu and I struggle understanding a concept. In the book the directional derivative is given a tangent vector $v$ at a point $p$ the map : $$D_v : \mathcal{C}_p^\infty \rightarrow \mathbb{R}$$ The follows by explaining that a map satisfying the leibniz rule is a ""Derivation at p"". The we can define the map : $$\phi : T_p(\mathbb{R}^n) \rightarrow \mathcal{D}_p(\mathbb{R}^n)\\ v \rightarrow D_v = \sum v^i \frac{d}{dx^i} |_0$$ where : the ""d"" are partials evaluated at $0$. So far so good, then the book proceed to the proof that this map is an isomorphisme of vector spaces. And by this decide to write every tangent vectors $v = (v_1,...,v_n) = \sum v^i e_i$ as $v = \sum v^i \frac{d}{dx^i}|_p$ I have trouble understanding how a vector could be equal to a function. For me the RHS takes a function and the LHS is just a vector. I would like to know how is that intuitively correct and also what is the proof of the last equation.",,"['calculus', 'manifolds', 'partial-derivative', 'smooth-manifolds']"
42,Limit of infinity times zero,Limit of infinity times zero,,"I'm trying to evaluate $\displaystyle\lim_{x\to\infty} e^x[e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1]$. Using L'hopital's rule I know $\displaystyle\lim_{x\to\infty} e^x\log(1-e^{-x}) = -1$ and so I have a $\infty$ times $0$ situation. My problem is that when I use L'hopital's rule on the entire expression $e^x[e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1]$ I seem to be going in an infinite loop. If I try to evaluate it this way: $\frac{e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1}{e^{-x}}$ then I always end up with $e^x\log(1-e^{-x})$ on the numerator and $e^{-x}$ on the denominator no matter how many times I take derivatives. These 2 will never simplify to give a nice solution. If I try the other way and evaluate it this way: $\frac{e^x}{\frac{1}{e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1}}$ then I always end up with $e^x[e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1]^n$ for some integer $n$, which is basically the same as where I started. Are there any other methods that can be used to evaluate this kind of limit? I don't have anything else in my bag of tricks.","I'm trying to evaluate $\displaystyle\lim_{x\to\infty} e^x[e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1]$. Using L'hopital's rule I know $\displaystyle\lim_{x\to\infty} e^x\log(1-e^{-x}) = -1$ and so I have a $\infty$ times $0$ situation. My problem is that when I use L'hopital's rule on the entire expression $e^x[e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1]$ I seem to be going in an infinite loop. If I try to evaluate it this way: $\frac{e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1}{e^{-x}}$ then I always end up with $e^x\log(1-e^{-x})$ on the numerator and $e^{-x}$ on the denominator no matter how many times I take derivatives. These 2 will never simplify to give a nice solution. If I try the other way and evaluate it this way: $\frac{e^x}{\frac{1}{e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1}}$ then I always end up with $e^x[e^x\log(1-e^{-x}) - \log(1-e^{-x})+ 1]^n$ for some integer $n$, which is basically the same as where I started. Are there any other methods that can be used to evaluate this kind of limit? I don't have anything else in my bag of tricks.",,"['calculus', 'limits']"
43,Evaluating $\int\sqrt{\frac{1-x^2}{1+x^2}}\mathrm dx$,Evaluating,\int\sqrt{\frac{1-x^2}{1+x^2}}\mathrm dx,"Evaluating $$\int\sqrt{\frac{1-x^2}{1+x^2}}\mathrm dx$$ I had read the similar problem , but it doesn't work.","Evaluating $$\int\sqrt{\frac{1-x^2}{1+x^2}}\mathrm dx$$ I had read the similar problem , but it doesn't work.",,"['calculus', 'integration']"
44,Show $\int_{0}^{1} \frac{\ln x}{1-x}dx$=$\sum_{1}^{\infty}\frac{1}{n^2}$ and converges,Show = and converges,\int_{0}^{1} \frac{\ln x}{1-x}dx \sum_{1}^{\infty}\frac{1}{n^2},"I found this question a) show that the follow integral converges: $\int_{0}^{1} \frac{\ln x}{1-x}dx $ b) $\int_{0}^{1} \frac{\ln x}{1-x}dx$=$\sum_{1}^{\infty}\frac{1}{n^2}$ for the first part I try with comparison test because $f(x):=\frac{\ln x}{1-x} \le0$ in $ [0,1]$ and I know from L'Hôpital's rule that $\lim_{x\to1} \frac{\ln x}{1-x}=-1 $ thanks ahead","I found this question a) show that the follow integral converges: $\int_{0}^{1} \frac{\ln x}{1-x}dx $ b) $\int_{0}^{1} \frac{\ln x}{1-x}dx$=$\sum_{1}^{\infty}\frac{1}{n^2}$ for the first part I try with comparison test because $f(x):=\frac{\ln x}{1-x} \le0$ in $ [0,1]$ and I know from L'Hôpital's rule that $\lim_{x\to1} \frac{\ln x}{1-x}=-1 $ thanks ahead",,"['calculus', 'integration', 'improper-integrals']"
45,How to solve the Few Scientists Problem (big word problem) in its general form?,How to solve the Few Scientists Problem (big word problem) in its general form?,,"I'm trying to figure out how to solve this word problem. I'm pretty sure it involves calculus or something even harder, but I don't know how to solve the general form . Let me start with the concrete form, however: Concrete Form: You start with 5 scientists. A scientist can train 50 students for 5 years, after which each student becomes a scientist. (Assume a perfect graduation rate always, and assume you have an infinite population from which to draw students). Or a scientist can work on a project. The problem is you have 30 Type-A projects, 50 Type-B projects, and 75 Type-C Projects, and they all need to be completed in minimal time. Each Type-A Project requires at least 10 scientists and takes 200/x years to complete, where x is the number of scientists assigned to them. Type-B's require at least 18 scientists and take 150/x years to complete. Type-C's require at least 25 scientist and take 120/x to complete. What is the minimum time necessary to complete all projects, and what is the ""event-order"" of such an optimal solution? I could solve this numerically by doing simulations in a computer program (although that will still be a pain in the neck), but what I really need is how to solve this in its general form. General Form: Just assign constants to everything. You start with s scientists, who can train t students for y years. There are A type-a projects, B type-b's, and C type-c's. Respectively, they require a minimum of d, e, and f scientists, and take g/x, h/x, and i/x years to complete. How do you go about solving this? Is that even possible? Solving this requires finding an optimal solution (completing all projects in minimal time), and proving that no other solution exists that has a smaller finish time. EDIT: Thanks to @Paul for this clarification. For projects, scientists can join or leave at any time. This is indiscrete time. For training, however, only 1 scientist can train a group of 50. (Two scientists training 50 does not make it go 2x faster.) The training has to be ""atomic"", which I think is the right word.","I'm trying to figure out how to solve this word problem. I'm pretty sure it involves calculus or something even harder, but I don't know how to solve the general form . Let me start with the concrete form, however: Concrete Form: You start with 5 scientists. A scientist can train 50 students for 5 years, after which each student becomes a scientist. (Assume a perfect graduation rate always, and assume you have an infinite population from which to draw students). Or a scientist can work on a project. The problem is you have 30 Type-A projects, 50 Type-B projects, and 75 Type-C Projects, and they all need to be completed in minimal time. Each Type-A Project requires at least 10 scientists and takes 200/x years to complete, where x is the number of scientists assigned to them. Type-B's require at least 18 scientists and take 150/x years to complete. Type-C's require at least 25 scientist and take 120/x to complete. What is the minimum time necessary to complete all projects, and what is the ""event-order"" of such an optimal solution? I could solve this numerically by doing simulations in a computer program (although that will still be a pain in the neck), but what I really need is how to solve this in its general form. General Form: Just assign constants to everything. You start with s scientists, who can train t students for y years. There are A type-a projects, B type-b's, and C type-c's. Respectively, they require a minimum of d, e, and f scientists, and take g/x, h/x, and i/x years to complete. How do you go about solving this? Is that even possible? Solving this requires finding an optimal solution (completing all projects in minimal time), and proving that no other solution exists that has a smaller finish time. EDIT: Thanks to @Paul for this clarification. For projects, scientists can join or leave at any time. This is indiscrete time. For training, however, only 1 scientist can train a group of 50. (Two scientists training 50 does not make it go 2x faster.) The training has to be ""atomic"", which I think is the right word.",,"['calculus', 'multivariable-calculus', 'optimization']"
46,Integral with specific method,Integral with specific method,,"Evaluate $$\displaystyle \int _{ 0 }^{ \pi /2 }{ \log(\cos(x))\log(\sin(x)) \ dx }$$ Consider : $$\displaystyle F(m,n)=\int _{ 0 }^{ \pi /2 }{ \sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx } $$ $$\sin^{2}x = t$$ : $$ \displaystyle F(m,n) =\frac{1}{2} \int _{ 0 }^{ 1 }{ { t }^{ m-1 }{ (1-t) }^{ n-1 }dt }=\frac{\beta (m,n)}{2} $$ Where $\beta(m,n)$ is the beta function. $$\displaystyle F(m,n) = \frac { \Gamma (m)\Gamma (n) }{2 \Gamma (m+n) } $$ Hence $$\displaystyle \frac { \Gamma (m)\Gamma (n) }{ \Gamma (m+n) } = 2\int _{ 0 }^{ \pi /2 }{ \sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx }$$ Differentiating with respect to $m$ : $$\displaystyle \frac { \Gamma (n) }{ ({ \Gamma (m+n)) }^{ 2 } } (\Gamma '(m)\Gamma (m+n)-\Gamma (m)\Gamma '(m+n)) = 4\int _{ 0 }^{ \pi /2 }{ log(sin(x))\sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx } $$ $$\displaystyle \frac { \Gamma (m)\Gamma (n) }{ \Gamma (m+n) } (\psi (m)-\psi (m+n))=4\int _{ 0 }^{ \pi /2 }{ log(sin(x))\sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx }$$ $$\psi(x)$$ is the digamma function. Differentiate with respect to $n$ : $$\displaystyle \frac { \Gamma (m)\Gamma (n) }{ \Gamma (m+n) } (((\psi (m)-\psi (m+n))(\psi (n)-\psi (m+n))-\psi '(m+n))$$ $$\displaystyle =8\int _{ 0 }^{ \pi /2 }{ log(sin(x))log(cos(x))\sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx } $$ $m=n=\dfrac{1}{2}$ : $$\displaystyle \frac { { \Gamma }^{ 2 }(1/2) }{ \Gamma (1) } ({ (\psi (1/2)-\psi (1)) }^{ 2 }-\psi '(1))$$ $$\displaystyle =8\int _{ 0 }^{ \pi /2 }{ log(cos(x))log(sin(x))dx } $$ $$\displaystyle \Gamma (1/2)=\sqrt { \pi } ,\Gamma (1)=1,\psi (1/2)=-\gamma -log(4),\psi (1)=-\gamma ,\psi '(1)=\frac { { \pi }^{ 2 } }{ 6 } $$ Any other method to do this?",Evaluate Consider : : Where is the beta function. Hence Differentiating with respect to : is the digamma function. Differentiate with respect to : : Any other method to do this?,"\displaystyle \int _{ 0 }^{ \pi /2 }{ \log(\cos(x))\log(\sin(x)) \ dx } \displaystyle F(m,n)=\int _{ 0 }^{ \pi /2 }{ \sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx }  \sin^{2}x = t  \displaystyle F(m,n) =\frac{1}{2} \int _{ 0 }^{ 1 }{ { t }^{ m-1 }{ (1-t) }^{ n-1 }dt }=\frac{\beta (m,n)}{2}  \beta(m,n) \displaystyle F(m,n) = \frac { \Gamma (m)\Gamma (n) }{2 \Gamma (m+n) }  \displaystyle \frac { \Gamma (m)\Gamma (n) }{ \Gamma (m+n) } = 2\int _{ 0 }^{ \pi /2 }{ \sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx } m \displaystyle \frac { \Gamma (n) }{ ({ \Gamma (m+n)) }^{ 2 } } (\Gamma '(m)\Gamma (m+n)-\Gamma (m)\Gamma '(m+n)) = 4\int _{ 0 }^{ \pi /2 }{ log(sin(x))\sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx }  \displaystyle \frac { \Gamma (m)\Gamma (n) }{ \Gamma (m+n) } (\psi (m)-\psi (m+n))=4\int _{ 0 }^{ \pi /2 }{ log(sin(x))\sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx } \psi(x) n \displaystyle \frac { \Gamma (m)\Gamma (n) }{ \Gamma (m+n) } (((\psi (m)-\psi (m+n))(\psi (n)-\psi (m+n))-\psi '(m+n)) \displaystyle =8\int _{ 0 }^{ \pi /2 }{ log(sin(x))log(cos(x))\sin ^{ 2m-1 }{ x } \cos ^{ 2n-1 }{ x } dx }  m=n=\dfrac{1}{2} \displaystyle \frac { { \Gamma }^{ 2 }(1/2) }{ \Gamma (1) } ({ (\psi (1/2)-\psi (1)) }^{ 2 }-\psi '(1)) \displaystyle =8\int _{ 0 }^{ \pi /2 }{ log(cos(x))log(sin(x))dx }  \displaystyle \Gamma (1/2)=\sqrt { \pi } ,\Gamma (1)=1,\psi (1/2)=-\gamma -log(4),\psi (1)=-\gamma ,\psi '(1)=\frac { { \pi }^{ 2 } }{ 6 } ",['calculus']
47,Why Does The Taylor Remainder Formula Work?,Why Does The Taylor Remainder Formula Work?,,"I've been studying calculus on my own and have come across Taylor series. It is very intuitive until I came across the remainder part of the formula where things got fuzzy. I understand why the remainder exists but not the mathematical description. Why is the value being plugged into the derivative of the remainder some number between $x$ and $a$? What is the connection to the mean value theorem? Is the remainder used to bound the function? Lastly and my most important question is what is the intuitive (for me most likely a geometrical approach would be great) for how this remainder is derived. I've spent a long time trying to figure it out for myself and looking online but it seems I'm missing something for there are virtually no questions being asked about this. Thanks for you time, Jackson","I've been studying calculus on my own and have come across Taylor series. It is very intuitive until I came across the remainder part of the formula where things got fuzzy. I understand why the remainder exists but not the mathematical description. Why is the value being plugged into the derivative of the remainder some number between $x$ and $a$? What is the connection to the mean value theorem? Is the remainder used to bound the function? Lastly and my most important question is what is the intuitive (for me most likely a geometrical approach would be great) for how this remainder is derived. I've spent a long time trying to figure it out for myself and looking online but it seems I'm missing something for there are virtually no questions being asked about this. Thanks for you time, Jackson",,"['calculus', 'sequences-and-series', 'taylor-expansion', 'intuition']"
48,"A difficult integral: $\int_0^{+\infty} e^{ - x}\left(\frac1{x( e^{ - x} - 1 )} + \frac1{x^2} + \frac1{2x} \right) \, dx$.",A difficult integral: .,"\int_0^{+\infty} e^{ - x}\left(\frac1{x( e^{ - x} - 1 )} + \frac1{x^2} + \frac1{2x} \right) \, dx","Could you help me calculate the integral? $$\int_0^{+\infty} e^{-x} \left(\frac1{x(e^{-x}-1)} + \frac1{x^2} + \frac1{2x} \right) \, dx .$$","Could you help me calculate the integral? $$\int_0^{+\infty} e^{-x} \left(\frac1{x(e^{-x}-1)} + \frac1{x^2} + \frac1{2x} \right) \, dx .$$",,"['calculus', 'integration', 'definite-integrals', 'special-functions', 'closed-form']"
49,"Can I get some guidance on solving $\int_{-\infty}^{\infty} \frac{\sin^2(x)}{x^2} \, dx$?",Can I get some guidance on solving ?,"\int_{-\infty}^{\infty} \frac{\sin^2(x)}{x^2} \, dx","I am trying to evaluate: $$I = \int_{-\infty}^{\infty} \frac{\sin^2(x)}{x^2} \, dx.$$ Using a contour semi-circle (upper plane), I can get: $$ \oint_{C} f(z) \,dz = \oint_{C} \frac{1 - e^{2iz}}{z^2} \, dz.$$ The whole issue is the $z^2$. I cannot use the residue theory, because it lies on the contour. I don’t want a full solution. I really want to try on my own, I just need some guidance!","I am trying to evaluate: $$I = \int_{-\infty}^{\infty} \frac{\sin^2(x)}{x^2} \, dx.$$ Using a contour semi-circle (upper plane), I can get: $$ \oint_{C} f(z) \,dz = \oint_{C} \frac{1 - e^{2iz}}{z^2} \, dz.$$ The whole issue is the $z^2$. I cannot use the residue theory, because it lies on the contour. I don’t want a full solution. I really want to try on my own, I just need some guidance!",,"['calculus', 'integration', 'complex-analysis', 'analysis', 'residue-calculus']"
50,"Prove a convergent sequence has either a minimum, a maximum or both.","Prove a convergent sequence has either a minimum, a maximum or both.",,"Let $a_n$ be a convergent sequence. Prove $a_n$ has a minimum, a maximum or both. I am being prepared for a final exam, which is why it is important to me to know that $I$ am correct in $my$ attempt. Of course if I am completely wrong, hints or solution are welcome. Thanks. $Attempt$: $a_n$ converges to a limit $L\in \Bbb{R}$ as $n\to \infty$. Therefore, for $\epsilon=1$ we get, for large enough $N$ that $\forall n\ge N,$ $|a_n-L|<1$ $\Rightarrow$ $L-1 \le a_n\le L+1$, in particular $a_n\le L+1$. Therefore, for $M=max(a_1,a_2,...,a_N,L+1)$, $a_n\le M$ necessarily. i.e, $a_n$ is upper bounded. The same can be shown with lower bound. If $a_n$ is constant, we are done. Otherwise, the lower and the upper bounds are different. Suppose $a_n$ has no minimum nor maximum, then both lower and upper bound are accumulation point, a contradiction. Therefore $a_n$ has a maximum or a minimum in that case.","Let $a_n$ be a convergent sequence. Prove $a_n$ has a minimum, a maximum or both. I am being prepared for a final exam, which is why it is important to me to know that $I$ am correct in $my$ attempt. Of course if I am completely wrong, hints or solution are welcome. Thanks. $Attempt$: $a_n$ converges to a limit $L\in \Bbb{R}$ as $n\to \infty$. Therefore, for $\epsilon=1$ we get, for large enough $N$ that $\forall n\ge N,$ $|a_n-L|<1$ $\Rightarrow$ $L-1 \le a_n\le L+1$, in particular $a_n\le L+1$. Therefore, for $M=max(a_1,a_2,...,a_N,L+1)$, $a_n\le M$ necessarily. i.e, $a_n$ is upper bounded. The same can be shown with lower bound. If $a_n$ is constant, we are done. Otherwise, the lower and the upper bounds are different. Suppose $a_n$ has no minimum nor maximum, then both lower and upper bound are accumulation point, a contradiction. Therefore $a_n$ has a maximum or a minimum in that case.",,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
51,"Prove that : $f(\sin x)+f(\cos x) \ge 196, \forall x\in\left(0;\frac{\pi}{2}\right)$",Prove that :,"f(\sin x)+f(\cos x) \ge 196, \forall x\in\left(0;\frac{\pi}{2}\right)","Given: $$f(\tan2x)=\tan^{4}x+\frac{1}{\tan^{4}x}, \forall x\in\left(0;\frac{\pi}{4}\right)$$ Prove that :$f(\sin x)+f(\cos x) \ge 196, \forall x\in\left(0;\frac{\pi}{2}\right)$ Could someone help me ?","Given: $$f(\tan2x)=\tan^{4}x+\frac{1}{\tan^{4}x}, \forall x\in\left(0;\frac{\pi}{4}\right)$$ Prove that :$f(\sin x)+f(\cos x) \ge 196, \forall x\in\left(0;\frac{\pi}{2}\right)$ Could someone help me ?",,"['calculus', 'trigonometry', 'inequality']"
52,Can the choice of epsilon be arbitrary in epsilon-delta proofs?,Can the choice of epsilon be arbitrary in epsilon-delta proofs?,,"I've been reading Spivak's chapter on limits and something that I don't feel I understand entirely is how the epsilon is decided upon. It makes sense to me in the context of  $\,|f(x)-L|<\epsilon$ where it appears just on it's own (representing any positive number) but Spivak seems to invoke an arbitrary value of epsilon in some of the given proofs. For example in proving that: $$ \lim_{x\to a}[f(x)+g(x)]=\lim_{x\to a}[f(x)]+\lim_{x\to a}[g(x)]$$ he says if: $$\lim_{x\to a}f(x)=l \;\; \text{and} \;\;\lim_{x\to a}g(x)=m$$ then for any $\epsilon>0$ then there are $\delta_1,\delta_2>0$ such that for all $x$: $$ 0<|x-a|<\delta_1 \implies |f(x)-l|<\frac{\epsilon}{2} \\ 0<|x-a|<\delta_2 \implies |g(x)-m|<\frac{\epsilon}{2} $$ He then carries on to show that $|(f+g)(x)-(l+m)|<\epsilon$. I feel like I understand the proof he gives but I just wanted to clarify whether it matters how he defined what the epsilon was. Since for example if he had started with expressions without the $\frac{\epsilon}{2}$ but rather: $$0<|x-a|<\delta_1 \implies |f(x)-l|<\epsilon$$ wouldn't the outcome be $|(f+g)(x)-(l+m)|<2\epsilon$; which I assume still says that it's bounded, and since $\epsilon$ was any positive number it shouldn't matter. Though I'm not certain, so I suspect there's an error in my understanding. Another example is if you tried proving that: $$\lim_{x\to 1} 2x-2=0$$ then instead of writing just epsilon, one wrote some positive constant $c$ times epsilon so: $$ |(2x-2)-0|<c\times\epsilon\;\; \rightarrow \;\; |x-1|<\frac{c\times \epsilon}{2}$$ therefore let $\delta =\frac{c\times\epsilon}{2}$ which would still seem to prove it even if we hadn't had the $c$ that is with $\delta=\frac{\epsilon}{2}$ . I know it's not a proper proof but hopefully you see what I'm trying to say (or indeed what I'm doing incorrectly). Hopefully my question isn't too badly written or that there isn't one asking the same thing, I have found this which is from the same part of the book but I still don't feel sure whether or not the choice of epsilon changes the validity proof.","I've been reading Spivak's chapter on limits and something that I don't feel I understand entirely is how the epsilon is decided upon. It makes sense to me in the context of  $\,|f(x)-L|<\epsilon$ where it appears just on it's own (representing any positive number) but Spivak seems to invoke an arbitrary value of epsilon in some of the given proofs. For example in proving that: $$ \lim_{x\to a}[f(x)+g(x)]=\lim_{x\to a}[f(x)]+\lim_{x\to a}[g(x)]$$ he says if: $$\lim_{x\to a}f(x)=l \;\; \text{and} \;\;\lim_{x\to a}g(x)=m$$ then for any $\epsilon>0$ then there are $\delta_1,\delta_2>0$ such that for all $x$: $$ 0<|x-a|<\delta_1 \implies |f(x)-l|<\frac{\epsilon}{2} \\ 0<|x-a|<\delta_2 \implies |g(x)-m|<\frac{\epsilon}{2} $$ He then carries on to show that $|(f+g)(x)-(l+m)|<\epsilon$. I feel like I understand the proof he gives but I just wanted to clarify whether it matters how he defined what the epsilon was. Since for example if he had started with expressions without the $\frac{\epsilon}{2}$ but rather: $$0<|x-a|<\delta_1 \implies |f(x)-l|<\epsilon$$ wouldn't the outcome be $|(f+g)(x)-(l+m)|<2\epsilon$; which I assume still says that it's bounded, and since $\epsilon$ was any positive number it shouldn't matter. Though I'm not certain, so I suspect there's an error in my understanding. Another example is if you tried proving that: $$\lim_{x\to 1} 2x-2=0$$ then instead of writing just epsilon, one wrote some positive constant $c$ times epsilon so: $$ |(2x-2)-0|<c\times\epsilon\;\; \rightarrow \;\; |x-1|<\frac{c\times \epsilon}{2}$$ therefore let $\delta =\frac{c\times\epsilon}{2}$ which would still seem to prove it even if we hadn't had the $c$ that is with $\delta=\frac{\epsilon}{2}$ . I know it's not a proper proof but hopefully you see what I'm trying to say (or indeed what I'm doing incorrectly). Hopefully my question isn't too badly written or that there isn't one asking the same thing, I have found this which is from the same part of the book but I still don't feel sure whether or not the choice of epsilon changes the validity proof.",,"['calculus', 'algebra-precalculus', 'limits', 'epsilon-delta']"
53,"Closed-form of $\sum_{k=0}^{\infty} \frac{k^a\,b^k}{k!}$",Closed-form of,"\sum_{k=0}^{\infty} \frac{k^a\,b^k}{k!}","While working on this question I think I've found a closed-form expression for the following series, but I don't know how to prove it. Let $a \in \mathbb{N}$ and $b \in \mathbb{R}$. Then $$\sum_{k=0}^{\infty} \frac{k^a\,b^k}{k!} \stackrel{?}{=} e^b \sum_{j=0}^a S(a,a-j+1)\,b^{a-j+1}, \tag{1}$$ where $e$ is Euler's number and $S(n,k)$ are the Stirling numbers of the second kind , defined as $$S(n,k) = \frac{1}{k!}\sum_{i=0}^k (-1)^{i}{k \choose i} (k-i)^n,$$ with ${n \choose k}$ a binomial coefficient . I have three questions. $1^\text{st}$ Question. Is $(1)$ true? $2^\text{nd}$ Question. If $(1)$ is true, then can we write $(1)$ into a more compact form with solving the finite sum somehow? $3^\text{rd}$ Question. If $(1)$ is true, then can we generalize the statment for $a \in \mathbb{R}$? I don't know about this kind of generalization of the Stirling numbers of the second kind. Maybe there is another approach?","While working on this question I think I've found a closed-form expression for the following series, but I don't know how to prove it. Let $a \in \mathbb{N}$ and $b \in \mathbb{R}$. Then $$\sum_{k=0}^{\infty} \frac{k^a\,b^k}{k!} \stackrel{?}{=} e^b \sum_{j=0}^a S(a,a-j+1)\,b^{a-j+1}, \tag{1}$$ where $e$ is Euler's number and $S(n,k)$ are the Stirling numbers of the second kind , defined as $$S(n,k) = \frac{1}{k!}\sum_{i=0}^k (-1)^{i}{k \choose i} (k-i)^n,$$ with ${n \choose k}$ a binomial coefficient . I have three questions. $1^\text{st}$ Question. Is $(1)$ true? $2^\text{nd}$ Question. If $(1)$ is true, then can we write $(1)$ into a more compact form with solving the finite sum somehow? $3^\text{rd}$ Question. If $(1)$ is true, then can we generalize the statment for $a \in \mathbb{R}$? I don't know about this kind of generalization of the Stirling numbers of the second kind. Maybe there is another approach?",,"['calculus', 'sequences-and-series', 'summation', 'closed-form']"
54,What is the mathematical truth behind the Leibniz notation in differentiating twice or more?,What is the mathematical truth behind the Leibniz notation in differentiating twice or more?,,"So $f: \mathbb{R} \to \mathbb{R}$ is $n>1$ (or more) times differentiable. The notation of the first derivative makes perfect ""sense"" with regard to what's going on: $$\lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \equiv \frac{df}{dx}$$ The second makes me tilt my head a bit (to no effect): $$\lim_{h \to 0} \frac{\frac{df}{dx}\big|_{x+h} - \frac{df}{dx}\big|_{x}}{h} = \frac{d}{dx} \frac{df}{dx} = \frac{d^2 f}{dx^2}$$ This notation looks like as if : $$\frac{d^2 f}{dx^2} = \lim_{h \to 0} \frac{(f(x+h)-f(x))^2}{(x+h)^2-x^2}$$ But I couldn't find any sense in that.. Now, I was told that this notation has complete (more advanced) mathematical sense. I'd like to know where to look for it.","So $f: \mathbb{R} \to \mathbb{R}$ is $n>1$ (or more) times differentiable. The notation of the first derivative makes perfect ""sense"" with regard to what's going on: $$\lim_{h \to 0} \frac{f(x+h) - f(x)}{h} \equiv \frac{df}{dx}$$ The second makes me tilt my head a bit (to no effect): $$\lim_{h \to 0} \frac{\frac{df}{dx}\big|_{x+h} - \frac{df}{dx}\big|_{x}}{h} = \frac{d}{dx} \frac{df}{dx} = \frac{d^2 f}{dx^2}$$ This notation looks like as if : $$\frac{d^2 f}{dx^2} = \lim_{h \to 0} \frac{(f(x+h)-f(x))^2}{(x+h)^2-x^2}$$ But I couldn't find any sense in that.. Now, I was told that this notation has complete (more advanced) mathematical sense. I'd like to know where to look for it.",,"['calculus', 'derivatives', 'notation', 'differential-operators']"
55,The Shortest Distance Between 2 Points On The Earth,The Shortest Distance Between 2 Points On The Earth,,"Assuming that the earth is a perfect sphere with radius 6378 kilometers, what is the expected straight line distance through the earth (in km) between 2 points that are chosen uniformly on the surface of the earth?","Assuming that the earth is a perfect sphere with radius 6378 kilometers, what is the expected straight line distance through the earth (in km) between 2 points that are chosen uniformly on the surface of the earth?",,['calculus']
56,Integral of $\int_0^\infty\frac{1}{e^{x}-x} dx$,Integral of,\int_0^\infty\frac{1}{e^{x}-x} dx,"I am curious as to whether a closed form exists for the following integral: $$\int_0^\infty\frac{1}{e^{x}-x} dx$$ I have tried a few elemetary methods on it, but I believe this integral (if it has a solution) can only be solved through complex analysis which I have no working knowledge of. Wolfram does not return any closed form. I'm not sure if it is much use, but this integral appears to be equivalent to $$\int_0^\infty\frac{x}{e^{x}-x} dx$$ Maybe this integral is related to the gamma function like $\int_0^\infty\frac{x^n}{e^{x}-1} dx$?","I am curious as to whether a closed form exists for the following integral: $$\int_0^\infty\frac{1}{e^{x}-x} dx$$ I have tried a few elemetary methods on it, but I believe this integral (if it has a solution) can only be solved through complex analysis which I have no working knowledge of. Wolfram does not return any closed form. I'm not sure if it is much use, but this integral appears to be equivalent to $$\int_0^\infty\frac{x}{e^{x}-x} dx$$ Maybe this integral is related to the gamma function like $\int_0^\infty\frac{x^n}{e^{x}-1} dx$?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
57,proving that continuous function smaller than integral is identically zero,proving that continuous function smaller than integral is identically zero,,"$f : [0,1] \to \mathbb{R}$ is continuous and $f \geq 0$. There is $C>0$ with $|f(x)| < C \int_{0}^{x} |f(t)| dt$ for all $x \in [0,1]$. (so $f(0)=0$) Is it true that $f = 0$? or is there any counterexamples? Thanks.","$f : [0,1] \to \mathbb{R}$ is continuous and $f \geq 0$. There is $C>0$ with $|f(x)| < C \int_{0}^{x} |f(t)| dt$ for all $x \in [0,1]$. (so $f(0)=0$) Is it true that $f = 0$? or is there any counterexamples? Thanks.",,"['calculus', 'analysis']"
58,Superelliptic Area Of $x^5+y^5=r^5$,Superelliptic Area Of,x^5+y^5=r^5,"$${\LARGE\int}_0^\tfrac\pi2\frac{dx}{\bigg(\sqrt[{\Large 5}]{\cos^5x+10\cos^3x\sin^2x+5\cos x\sin^4x}\bigg)^{\large 2}}~=~?$$ Its numerical value is about $1.40171345128228$. Maple, Mathematica, and the Inverse Symbolic Calculator are not able to return any closed form. Motivation: The above integral is, up to a certain scaling factor, nothing else than the area between the graphic of $x^n+y^n=r^n$, and the second bisector, for $n=5$. For even values of the exponent, we have the area of $x^{2k}+y^{2k}=r^{2k}$ being equal to $A_{2k}=\displaystyle(2r)^2\cdot{2a\choose a}^{-1}$, where $a=\dfrac1{2k}$. For $n\!=\!3$ we have $A_3=\dfrac{r^2}{\sqrt[3]4}\cdot B\bigg(\dfrac12,\dfrac13\bigg)$. But how to compute its value for odd exponents greater than $3$ is beyond me. In this case, $A_5=r^2\cdot\sqrt[5]8\cdot I$ $\qquad\qquad$ $$x^5+y^5=1$$","$${\LARGE\int}_0^\tfrac\pi2\frac{dx}{\bigg(\sqrt[{\Large 5}]{\cos^5x+10\cos^3x\sin^2x+5\cos x\sin^4x}\bigg)^{\large 2}}~=~?$$ Its numerical value is about $1.40171345128228$. Maple, Mathematica, and the Inverse Symbolic Calculator are not able to return any closed form. Motivation: The above integral is, up to a certain scaling factor, nothing else than the area between the graphic of $x^n+y^n=r^n$, and the second bisector, for $n=5$. For even values of the exponent, we have the area of $x^{2k}+y^{2k}=r^{2k}$ being equal to $A_{2k}=\displaystyle(2r)^2\cdot{2a\choose a}^{-1}$, where $a=\dfrac1{2k}$. For $n\!=\!3$ we have $A_3=\dfrac{r^2}{\sqrt[3]4}\cdot B\bigg(\dfrac12,\dfrac13\bigg)$. But how to compute its value for odd exponents greater than $3$ is beyond me. In this case, $A_5=r^2\cdot\sqrt[5]8\cdot I$ $\qquad\qquad$ $$x^5+y^5=1$$",,"['calculus', 'integration', 'definite-integrals', 'closed-form']"
59,$\int_{0}^{\infty}\int_{1}^{\infty}\frac{x^2-y^2}{(x^2+y^2)^2}dxdy$ diverges?,diverges?,\int_{0}^{\infty}\int_{1}^{\infty}\frac{x^2-y^2}{(x^2+y^2)^2}dxdy,"I want someone to review my proof that $$\int_{1}^{\infty}\int_{0}^{\infty}\frac{x^2-y^2}{(x^2+y^2)^2}dxdy$$ does not converge. To make things easier, I said let's look at the entire first quadrant and then subtract the integral over the small rectangle that we added. move to polar coordinates: $$\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}r^2\frac{(\cos^2\theta-\sin^2\theta)}{r^3}drd\theta=\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{\cos^2\theta-\sin^2\theta}{r}drd\theta=\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1-2\sin^2\theta}{r}drd\theta\leq\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1}{r}drd\theta$$ The inequality part is true since $-2\sin^2 x$ is always negative. So: $$\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1}{r}drd\theta=\frac{\pi}{2}\ln(r)|_0^{\infty}=\frac{\pi}{2}\ln(\frac{\infty}{0})=\frac{\pi}{2}\ln({\infty})=\infty$$ So over the entire first quadrant it diverges. Is there a point to checking the small rectangle that we added? I mean, even if it diverges, the answer would still diverge.","I want someone to review my proof that $$\int_{1}^{\infty}\int_{0}^{\infty}\frac{x^2-y^2}{(x^2+y^2)^2}dxdy$$ does not converge. To make things easier, I said let's look at the entire first quadrant and then subtract the integral over the small rectangle that we added. move to polar coordinates: $$\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}r^2\frac{(\cos^2\theta-\sin^2\theta)}{r^3}drd\theta=\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{\cos^2\theta-\sin^2\theta}{r}drd\theta=\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1-2\sin^2\theta}{r}drd\theta\leq\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1}{r}drd\theta$$ The inequality part is true since $-2\sin^2 x$ is always negative. So: $$\int_{0}^{\frac{\pi}{2}}\int_{o}^{\infty}\frac{1}{r}drd\theta=\frac{\pi}{2}\ln(r)|_0^{\infty}=\frac{\pi}{2}\ln(\frac{\infty}{0})=\frac{\pi}{2}\ln({\infty})=\infty$$ So over the entire first quadrant it diverges. Is there a point to checking the small rectangle that we added? I mean, even if it diverges, the answer would still diverge.",,"['calculus', 'integration', 'multivariable-calculus', 'improper-integrals']"
60,L'Hospital's rule vs Taylor series,L'Hospital's rule vs Taylor series,,"One classical application of Taylor expansions is to obtain polynomial equivalents of complicated functions and use them to compute limits. For example, with Landau notations, we have $$\begin{array}{rcl} \lim_{x\to 0}\frac{e^x-x-\cos(x)}{x^2} & = & \lim_{x\to 0}\frac{1+x+\frac{x^2}{2}-x-1+\frac{x^2}{2}+o(x^3)}{x^2} \\                                        & = & \lim_{x\to 0}\frac{x^2+o(x^2)}{x^2}\\                                        & = & \lim_{x\to 0} 1+o(1)=1  \end{array} $$ But this example can be dealt with using L'Hospital's Rule twice. It seems to me that it would be always the case: since we basically consider ratio of ""infinite degree polynomials"", we can use repeatly l'Hospital's Rule in order to kill the indetermination. My question: is there an example where Taylor expansions can be used but not L'Hospital's rule? I guess no so an example where computations with l'Hospital's Rule are awfully complicated but reasonable with Taylor would make me happy.","One classical application of Taylor expansions is to obtain polynomial equivalents of complicated functions and use them to compute limits. For example, with Landau notations, we have $$\begin{array}{rcl} \lim_{x\to 0}\frac{e^x-x-\cos(x)}{x^2} & = & \lim_{x\to 0}\frac{1+x+\frac{x^2}{2}-x-1+\frac{x^2}{2}+o(x^3)}{x^2} \\                                        & = & \lim_{x\to 0}\frac{x^2+o(x^2)}{x^2}\\                                        & = & \lim_{x\to 0} 1+o(1)=1  \end{array} $$ But this example can be dealt with using L'Hospital's Rule twice. It seems to me that it would be always the case: since we basically consider ratio of ""infinite degree polynomials"", we can use repeatly l'Hospital's Rule in order to kill the indetermination. My question: is there an example where Taylor expansions can be used but not L'Hospital's rule? I guess no so an example where computations with l'Hospital's Rule are awfully complicated but reasonable with Taylor would make me happy.",,"['calculus', 'taylor-expansion', 'examples-counterexamples']"
61,What is the difference between $\Delta r$ and $dr$ in Taylor series,What is the difference between  and  in Taylor series,\Delta r dr,"All I know about Taylor series is at here . It tells how to expand a funcion to a polynomial. However I see the Taylor series at the form like this (here $r$ is a parametrized surface of $u,v$): $$\Delta r=dr+\frac12d^2r+o(du^2+dv^2)$$ It confuses me since I don't know the difference between $\Delta r$ and  $dr$ (all I know is they are the same at 2 dimensional function, $f=f(x)$ when $\Delta x$ becomes infinitesimal). Can anybody give me a geometrical intuition of $\Delta r$ and $dr$ together with $\Delta^nr $ and $d^nr$ ? Also, I have heard that in a higher dimension, derivatives exist not implies differentiability. Can anyone give me some examples about that? Thirdly, I don't know when the 'infinitesimal of higher order' can be neglect. In many case we will neglect the $o(x^n)$ function. But when can we neglect and how can we know which order can we neglect? Using the above Taylor series as example, let $n$ be an unit normal vector of the points on the surface,  my book said $$\Delta r\cdot n=\left[dr+\frac12d^2r+o(du^2+dv^2)\right]\cdot n=\frac12d^2r\cdot n$$ Why the term $o(du^2+dv^2)$ can be neglect but not the higher or lower order of $o$ funcion?","All I know about Taylor series is at here . It tells how to expand a funcion to a polynomial. However I see the Taylor series at the form like this (here $r$ is a parametrized surface of $u,v$): $$\Delta r=dr+\frac12d^2r+o(du^2+dv^2)$$ It confuses me since I don't know the difference between $\Delta r$ and  $dr$ (all I know is they are the same at 2 dimensional function, $f=f(x)$ when $\Delta x$ becomes infinitesimal). Can anybody give me a geometrical intuition of $\Delta r$ and $dr$ together with $\Delta^nr $ and $d^nr$ ? Also, I have heard that in a higher dimension, derivatives exist not implies differentiability. Can anyone give me some examples about that? Thirdly, I don't know when the 'infinitesimal of higher order' can be neglect. In many case we will neglect the $o(x^n)$ function. But when can we neglect and how can we know which order can we neglect? Using the above Taylor series as example, let $n$ be an unit normal vector of the points on the surface,  my book said $$\Delta r\cdot n=\left[dr+\frac12d^2r+o(du^2+dv^2)\right]\cdot n=\frac12d^2r\cdot n$$ Why the term $o(du^2+dv^2)$ can be neglect but not the higher or lower order of $o$ funcion?",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
62,Functions $f$ such that $f(x+1)-f(x-1)=2f'(x)$.,Functions  such that .,f f(x+1)-f(x-1)=2f'(x),"What can one say about functions $f:\mathbb{R}\to\mathbb{R}$ satisfying the condition $f(x+1)-f(x-1)=2f'(x)$? Is is possible to find all such functions, or is this defining equation the best characterisation that one is likely to get? I know that all polynomials with degree at most 2 satisfy the conditions. This is most easily seen by noting that constants, $f(x)=x$ and $f(x)=x^2$ satisfy the conditions and then noticing that if two functions $f$ and $g$ satisfy the condition then so does any linear combination of $f$ and $g$. One can also see that no polynomial with degree more than 2 will satisfy the condition by noting that $f(x)=x^3$ does not satisfy the condition and then noticing that if $f$ satisfies the condition then so does its derivative (Provided that $f'$ is also differentiable, which is obviously the case for polynomials). Thus the existence of any polynomial $f$ of degree more than 2 which satisfies the condition would imply that there exists such a cubic by repeatedly differentiating $f$, and the existence of such a cubic implies that $f(x)=x^3$ satisfies the condition because $f(x)=x^3$ is a linear combination of the obtained cubic and some quadratic polynomial.","What can one say about functions $f:\mathbb{R}\to\mathbb{R}$ satisfying the condition $f(x+1)-f(x-1)=2f'(x)$? Is is possible to find all such functions, or is this defining equation the best characterisation that one is likely to get? I know that all polynomials with degree at most 2 satisfy the conditions. This is most easily seen by noting that constants, $f(x)=x$ and $f(x)=x^2$ satisfy the conditions and then noticing that if two functions $f$ and $g$ satisfy the condition then so does any linear combination of $f$ and $g$. One can also see that no polynomial with degree more than 2 will satisfy the condition by noting that $f(x)=x^3$ does not satisfy the condition and then noticing that if $f$ satisfies the condition then so does its derivative (Provided that $f'$ is also differentiable, which is obviously the case for polynomials). Thus the existence of any polynomial $f$ of degree more than 2 which satisfies the condition would imply that there exists such a cubic by repeatedly differentiating $f$, and the existence of such a cubic implies that $f(x)=x^3$ satisfies the condition because $f(x)=x^3$ is a linear combination of the obtained cubic and some quadratic polynomial.",,"['calculus', 'functions', 'derivatives']"
63,$\epsilon - \delta$ proof that $\lim_{x \to a} \sqrt x = \sqrt a$,proof that,\epsilon - \delta \lim_{x \to a} \sqrt x = \sqrt a,"I am trying to prove,  $\lim_{x \to a} \sqrt x = \sqrt a$ As per the definition of limit for every $\epsilon > 0$ there is some $\delta > 0$ such that   $ 0 < |x-a| < \delta$ implies $|f(x) - \sqrt a| < \epsilon$ $|\sqrt x - \sqrt a|  = \frac {|x -a|}{\sqrt x + \sqrt a} $ since  $ 0 < |x-a| < \delta$ $\frac {|x -a|}{\sqrt x + \sqrt a} < \delta$ Can I stop my proof at this point since I found $\epsilon$ ( which is $\delta$ in this case)","I am trying to prove,  $\lim_{x \to a} \sqrt x = \sqrt a$ As per the definition of limit for every $\epsilon > 0$ there is some $\delta > 0$ such that   $ 0 < |x-a| < \delta$ implies $|f(x) - \sqrt a| < \epsilon$ $|\sqrt x - \sqrt a|  = \frac {|x -a|}{\sqrt x + \sqrt a} $ since  $ 0 < |x-a| < \delta$ $\frac {|x -a|}{\sqrt x + \sqrt a} < \delta$ Can I stop my proof at this point since I found $\epsilon$ ( which is $\delta$ in this case)",,"['calculus', 'limits']"
64,Why does the same limit work in one case but fail in another?,Why does the same limit work in one case but fail in another?,,"The following questions has been bugging me since high-school calculus. Please help me find my peace once and for all: Consider a revolution solid generated by rotating a nice curve $f(x)$ around the $x$-axis on the interval $[a,b]$ (provided that $f(x)$ does not cross the $x$-axis on this interval). Let us first find the volume of this solid, $V$. We slice the interval $[a,b]$ into small segments of width $\delta_x$. Each segment is approximately a cylinder of radius $f(x)$ and height $\delta_x$, hence having a volume of $\pi [f(x)]^2 \delta_x$. Taking the limit we get $$V = \pi \int_a^b [f(x)]^2 \, dx$$ which is the right formula: great. Now we apply the exact same argument to find the surface area of the solid, $S$. The surface area of each cylindrically-approximated segment is $2 \pi f(x) \delta_x$, and taking the limit we obtain $$S = 2 \pi \int_a^b f(x) \, dx$$ which is not correct. We would obtain the correct formula for $S$ if we take the heights of the segments to be the arc lengths of $f(x)$ over each $\delta_x$, so I suspect that what is going wrong has something to do with this . But my question is: why does this argument work for finding $V$, but not $S$?","The following questions has been bugging me since high-school calculus. Please help me find my peace once and for all: Consider a revolution solid generated by rotating a nice curve $f(x)$ around the $x$-axis on the interval $[a,b]$ (provided that $f(x)$ does not cross the $x$-axis on this interval). Let us first find the volume of this solid, $V$. We slice the interval $[a,b]$ into small segments of width $\delta_x$. Each segment is approximately a cylinder of radius $f(x)$ and height $\delta_x$, hence having a volume of $\pi [f(x)]^2 \delta_x$. Taking the limit we get $$V = \pi \int_a^b [f(x)]^2 \, dx$$ which is the right formula: great. Now we apply the exact same argument to find the surface area of the solid, $S$. The surface area of each cylindrically-approximated segment is $2 \pi f(x) \delta_x$, and taking the limit we obtain $$S = 2 \pi \int_a^b f(x) \, dx$$ which is not correct. We would obtain the correct formula for $S$ if we take the heights of the segments to be the arc lengths of $f(x)$ over each $\delta_x$, so I suspect that what is going wrong has something to do with this . But my question is: why does this argument work for finding $V$, but not $S$?",,"['calculus', 'integration', 'analysis', 'limits', 'fake-proofs']"
65,Directional Derivatives using Polar Coordinates,Directional Derivatives using Polar Coordinates,,"I am having a hard time with this problem on my homework assignment.  Here is the problem, and i will show my work below: If $f( x, y) = -2 x^{2} + 3 y^{2}$, find the value of the directional derivative at the point $( -1, 1)$ in the direction given by the angle $\theta = \frac{2 \pi}{2}$. More specifically, find the directional derivative of f at the point $\left(-1,1\right)$ in the direction of the unit vector determined by the angle $\theta$ in polar coordinates. To find the directional derivative, i know you need to find the partial derivative with respect to x, and then to y as shown below: $$fx(x,y)=-4x$$ $$fy(x,y)=6y$$ Then because we want the directional derivative at point P, we evaluate at point P: $$\nabla f=4i+6j$$ Now the next step is to multiply the gradient by the directional vector, and that will give the final directional derivative.  I just don't know what the hint is implying when is says use the polar coordinates to find the direction of the unit vector.  What i do know about polar coordinates is: $$x=rcos \theta$$ $$y=rsin\theta$$ But after that i don't know how to use it.  Any help? Thanks!","I am having a hard time with this problem on my homework assignment.  Here is the problem, and i will show my work below: If $f( x, y) = -2 x^{2} + 3 y^{2}$, find the value of the directional derivative at the point $( -1, 1)$ in the direction given by the angle $\theta = \frac{2 \pi}{2}$. More specifically, find the directional derivative of f at the point $\left(-1,1\right)$ in the direction of the unit vector determined by the angle $\theta$ in polar coordinates. To find the directional derivative, i know you need to find the partial derivative with respect to x, and then to y as shown below: $$fx(x,y)=-4x$$ $$fy(x,y)=6y$$ Then because we want the directional derivative at point P, we evaluate at point P: $$\nabla f=4i+6j$$ Now the next step is to multiply the gradient by the directional vector, and that will give the final directional derivative.  I just don't know what the hint is implying when is says use the polar coordinates to find the direction of the unit vector.  What i do know about polar coordinates is: $$x=rcos \theta$$ $$y=rsin\theta$$ But after that i don't know how to use it.  Any help? Thanks!",,"['calculus', 'vector-analysis', 'partial-derivative']"
66,Midpoint approximation over/under estimation,Midpoint approximation over/under estimation,,So left handed approximation underestimates the area under a increasing curve and over estimates for decreasing curves. And right handed approximation overestimates for increasing curves and underestimates for decreasing curves. My question is regarding midpoint approximations of area under a curve for both increasing and decreasing functions. There doesnt seem to be an obvious answer to this without evaluating the integral itself and comparing. So does the midpoint approximation rule over or under estimates a increasing and decreasing function?,So left handed approximation underestimates the area under a increasing curve and over estimates for decreasing curves. And right handed approximation overestimates for increasing curves and underestimates for decreasing curves. My question is regarding midpoint approximations of area under a curve for both increasing and decreasing functions. There doesnt seem to be an obvious answer to this without evaluating the integral itself and comparing. So does the midpoint approximation rule over or under estimates a increasing and decreasing function?,,"['calculus', 'integration', 'approximation']"
67,How to find the CDF of the binomial distribution in terms of an integral,How to find the CDF of the binomial distribution in terms of an integral,,"This wiki page says that the CDF of the binomial distribution in terms of the beta function can be expressed as $$F(k;n,p)=Pr(X\leq k)=(n-k){{n}\choose{k}}\int_0^{1-p}t^{n-k-1}(1-t)^k {d}t$$ How to prove the equality? I don't quite have the access to the book referenced by the wiki page( Wadsworth, G. P. (1960). Introduction to probability and random variables . USA: McGraw-Hill New York. p. 52.)","This wiki page says that the CDF of the binomial distribution in terms of the beta function can be expressed as $$F(k;n,p)=Pr(X\leq k)=(n-k){{n}\choose{k}}\int_0^{1-p}t^{n-k-1}(1-t)^k {d}t$$ How to prove the equality? I don't quite have the access to the book referenced by the wiki page( Wadsworth, G. P. (1960). Introduction to probability and random variables . USA: McGraw-Hill New York. p. 52.)",,"['calculus', 'probability', 'probability-theory', 'probability-distributions']"
68,Maclaurin expansion of arctan: convergence?,Maclaurin expansion of arctan: convergence?,,"In my textbook, the Maclaurin series expansion of $\arctan{x}$ is found by integrating a geometric series, that is, by noting that $\frac{d}{dx}(\arctan(x)) = \frac{1}{x^2+1}$ then rewriting the latter as a geometric series over which one can then integrate. What bothers me is that the geometric series is only convergent when $|x| < 1$, but $\arctan(x)$ is defined for all $x$. This question of convergence is dismissed by the author, but I'm curious as to what's really going on here. Is the series expansion still valid outside the radius of convergence, and if so, why?","In my textbook, the Maclaurin series expansion of $\arctan{x}$ is found by integrating a geometric series, that is, by noting that $\frac{d}{dx}(\arctan(x)) = \frac{1}{x^2+1}$ then rewriting the latter as a geometric series over which one can then integrate. What bothers me is that the geometric series is only convergent when $|x| < 1$, but $\arctan(x)$ is defined for all $x$. This question of convergence is dismissed by the author, but I'm curious as to what's really going on here. Is the series expansion still valid outside the radius of convergence, and if so, why?",,"['calculus', 'power-series']"
69,"$\int_{S^{n-1}}\operatorname e^{ix\cdot \omega}\, \operatorname d\omega$",,"\int_{S^{n-1}}\operatorname e^{ix\cdot \omega}\, \operatorname d\omega","Given $x \in \mathbb R^n$, there exists a simpler expresion for the integral? $$\int_{S^{n-1}}\operatorname e^{ix\cdot \omega}\, \operatorname d\omega$$ where $S^{n-1}$ is the sphere of $\mathbb R^n$.I know that only depends on |x| but nothing else.","Given $x \in \mathbb R^n$, there exists a simpler expresion for the integral? $$\int_{S^{n-1}}\operatorname e^{ix\cdot \omega}\, \operatorname d\omega$$ where $S^{n-1}$ is the sphere of $\mathbb R^n$.I know that only depends on |x| but nothing else.",,"['calculus', 'spherical-harmonics']"
70,"What is the relationship between saying ""a Taylor series converges for all $x$"" and ""a Taylor series converges to a function, f(x)""","What is the relationship between saying ""a Taylor series converges for all "" and ""a Taylor series converges to a function, f(x)""",x,"Given the following Taylor series: $1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\frac{x^8}{8!}- \dots$ We know that: It converges for all of $x$ It converges to the function $\cos x$ The Taylor series converge for all of $x$ if for a fixed value of $x$, the partial sums converge to a limit, $L$. The Taylor series converge to $\cos x$ if its error term is $0$ as $n$ (the number of terms in the Taylor series) goes to infinity. My question is: Are these two concepts related? I thought point 1 would be useful when proving point 2. But when doing the proof of 2, I don't see any connection to point 1. If the two concepts are not related, then why is it useful to know the interval of convergence of a Taylor series (or any series for that matter)?","Given the following Taylor series: $1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\frac{x^8}{8!}- \dots$ We know that: It converges for all of $x$ It converges to the function $\cos x$ The Taylor series converge for all of $x$ if for a fixed value of $x$, the partial sums converge to a limit, $L$. The Taylor series converge to $\cos x$ if its error term is $0$ as $n$ (the number of terms in the Taylor series) goes to infinity. My question is: Are these two concepts related? I thought point 1 would be useful when proving point 2. But when doing the proof of 2, I don't see any connection to point 1. If the two concepts are not related, then why is it useful to know the interval of convergence of a Taylor series (or any series for that matter)?",,"['calculus', 'polynomials', 'taylor-expansion']"
71,A Problem on Improper Integrals,A Problem on Improper Integrals,,"Let $f(x)$ be continuous except at $x = 0$ and let $a > 0$. Assume that the improper integral $$\int_{0}^{a}f(x)\,dx = \lim_{\epsilon \to 0+}\int_{\epsilon}^{a}f(x)\,dx$$ exists and let $$g(x) = \int_{x}^{a}\frac{f(t)}{t}\,dt$$ Show that $$\int_{0}^{a}g(x)\,dx = \int_{0}^{a}f(x)\,dx$$ I tried integration by parts noting that $g'(x) = -f(x)/x$ and obtained for $0 < \epsilon < a$ the following $$\int_{\epsilon}^{a}g(x)\,dx = [xg(x)]_{x = \epsilon}^{x = a} - \int_{\epsilon}^{a}xg'(x)\,dx$$ or $$\int_{\epsilon}^{a}g(x)\,dx = -\epsilon g(\epsilon) + \int_{\epsilon}^{a}f(x)\,dx$$ The problem is solved if we can somehow show that $\lim_{\epsilon \to 0+}\epsilon g(\epsilon) = 0$. Looking at the definition $g(x)$ we see that we have no information of the behavior of $f(t)$ at $t = 0$ and the $t$ in denominator complicates the analysis of $g(\epsilon)$. Please suggest some hints which can lead to the solution. Note: This problem is taken from G. H. Hardy's ""A Course of Pure Mathematics"" 10th ed. Page 397.","Let $f(x)$ be continuous except at $x = 0$ and let $a > 0$. Assume that the improper integral $$\int_{0}^{a}f(x)\,dx = \lim_{\epsilon \to 0+}\int_{\epsilon}^{a}f(x)\,dx$$ exists and let $$g(x) = \int_{x}^{a}\frac{f(t)}{t}\,dt$$ Show that $$\int_{0}^{a}g(x)\,dx = \int_{0}^{a}f(x)\,dx$$ I tried integration by parts noting that $g'(x) = -f(x)/x$ and obtained for $0 < \epsilon < a$ the following $$\int_{\epsilon}^{a}g(x)\,dx = [xg(x)]_{x = \epsilon}^{x = a} - \int_{\epsilon}^{a}xg'(x)\,dx$$ or $$\int_{\epsilon}^{a}g(x)\,dx = -\epsilon g(\epsilon) + \int_{\epsilon}^{a}f(x)\,dx$$ The problem is solved if we can somehow show that $\lim_{\epsilon \to 0+}\epsilon g(\epsilon) = 0$. Looking at the definition $g(x)$ we see that we have no information of the behavior of $f(t)$ at $t = 0$ and the $t$ in denominator complicates the analysis of $g(\epsilon)$. Please suggest some hints which can lead to the solution. Note: This problem is taken from G. H. Hardy's ""A Course of Pure Mathematics"" 10th ed. Page 397.",,"['calculus', 'improper-integrals']"
72,How to prove $\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\sin{x_{i}}}\right)^{2a_{i}}+\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\tan{x_{i}}}\right)^{a_{i}}>2$?,How to prove ?,\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\sin{x_{i}}}\right)^{2a_{i}}+\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\tan{x_{i}}}\right)^{a_{i}}>2,"I wish to show that $$\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\sin{x_{i}}}\right)^{2a_{i}}+\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\tan{x_{i}}}\right)^{a_{i}}>2,$$ for a positive integer $n$, $x_{i}\in(0,\dfrac{\pi}{2}),$ and $a_{i}\ge 1$ I unsuccessfully tried to solve this problem using Bernoulli inequality $$(1+x)^n\ge 1+nx,n\ge1,x>-1$$ I am thankful to everyone who can solve it. This problem is from $2010$ China Maths Olympic team selection exercise, it is from this word problem $11$ . I think that the problem is nice and maybe not easy. I thank to everyone who can help to solve it.","I wish to show that $$\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\sin{x_{i}}}\right)^{2a_{i}}+\prod_{i=1}^{n}\left(\dfrac{x_{i}}{\tan{x_{i}}}\right)^{a_{i}}>2,$$ for a positive integer $n$, $x_{i}\in(0,\dfrac{\pi}{2}),$ and $a_{i}\ge 1$ I unsuccessfully tried to solve this problem using Bernoulli inequality $$(1+x)^n\ge 1+nx,n\ge1,x>-1$$ I am thankful to everyone who can solve it. This problem is from $2010$ China Maths Olympic team selection exercise, it is from this word problem $11$ . I think that the problem is nice and maybe not easy. I thank to everyone who can help to solve it.",,"['calculus', 'inequality']"
73,Series $ \sum\limits_{n=1}^{\infty}\frac{1}{n}e^{2 \pi i n x} $,Series, \sum\limits_{n=1}^{\infty}\frac{1}{n}e^{2 \pi i n x} ,"Prove that the sum   \begin{equation} \sum\limits_{n=1}^{\infty}\frac{1}{n}e^{2 \pi i n x} \end{equation}   converges for any $ x \notin \mathbb{Z} $. By definition, the initial series converges if and only if $ \sum\limits_{n=1}^{\infty}\frac{1}{n}\cos{(2 \pi n x)} $ and $ \sum\limits_{n=1}^{\infty}\frac{1}{n}\sin{(2 \pi n x)} $ both converge. Neither Dirichlet's nor Abel's test works. Integral test can't be applied because $ \frac{1}{t}\cos{(2 \pi x t)} $ is not monotonic function of $ t $. This problem slightly reminds me of how Equidistributional theorem is proved, but most probably it has nothing to do with it.","Prove that the sum   \begin{equation} \sum\limits_{n=1}^{\infty}\frac{1}{n}e^{2 \pi i n x} \end{equation}   converges for any $ x \notin \mathbb{Z} $. By definition, the initial series converges if and only if $ \sum\limits_{n=1}^{\infty}\frac{1}{n}\cos{(2 \pi n x)} $ and $ \sum\limits_{n=1}^{\infty}\frac{1}{n}\sin{(2 \pi n x)} $ both converge. Neither Dirichlet's nor Abel's test works. Integral test can't be applied because $ \frac{1}{t}\cos{(2 \pi x t)} $ is not monotonic function of $ t $. This problem slightly reminds me of how Equidistributional theorem is proved, but most probably it has nothing to do with it.",,['calculus']
74,Does the integrability of $\log(f(x))$ imply $f(x)$ is bounded?,Does the integrability of  imply  is bounded?,\log(f(x)) f(x),"Let $f(x):(a,b)\rightarrow \mathbb{R}$. Suppose $\int_a^b\log{f(x)}\,\mathrm{d}x<\infty$, can we claim that $0<f(x)<M<\infty$ a.e.. Why and why not?","Let $f(x):(a,b)\rightarrow \mathbb{R}$. Suppose $\int_a^b\log{f(x)}\,\mathrm{d}x<\infty$, can we claim that $0<f(x)<M<\infty$ a.e.. Why and why not?",,"['calculus', 'definite-integrals']"
75,What is the limit of the multidimensional integral?,What is the limit of the multidimensional integral?,,"What is the limit of the integral $$\int_{[0,1]^n}\frac{x_1^5+x_2^5 + \cdots +x_n^5}{x_1^4+x_2^4 + \cdots +x_n^4} \, dx_1 \, dx_2 \cdots dx_n$$  as $n \to \infty ?$","What is the limit of the integral $$\int_{[0,1]^n}\frac{x_1^5+x_2^5 + \cdots +x_n^5}{x_1^4+x_2^4 + \cdots +x_n^4} \, dx_1 \, dx_2 \cdots dx_n$$  as $n \to \infty ?$",,['calculus']
76,Hessian after coordinate changing,Hessian after coordinate changing,,"Let $f\colon \Bbb R^n\to\Bbb R$. Let $z=Px$ coordinate changing. $P$ is $n\times n$ constant matrix, $x$ and $z$ are the variables in $\Bbb R^n$. Does anyone know a formula which express how the Hessian of $f$ changed after this coordinate transform? Thanks.","Let $f\colon \Bbb R^n\to\Bbb R$. Let $z=Px$ coordinate changing. $P$ is $n\times n$ constant matrix, $x$ and $z$ are the variables in $\Bbb R^n$. Does anyone know a formula which express how the Hessian of $f$ changed after this coordinate transform? Thanks.",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'derivatives']"
77,Prove that $ f(\xi)=f'(\xi)\int_{0}^{\xi}{f(x)dx} $,Prove that, f(\xi)=f'(\xi)\int_{0}^{\xi}{f(x)dx} ,"Let $f:[0,1]\rightarrow\mathbb{R}$ be a differentiable function with continuous derivative such that $$ \int_{0}^{1}{f(x)dx}=\int_{0}^{1}{xf(x)dx} $$ How can we prove that there exists $\xi\in(0,1)$ such that $$ f(\xi)=f'(\xi)\int_{0}^{\xi}{f(x)dx} $$ I tried to use  $$ F(x)=\int_{0}^{x}{f(t)dt} $$ then the condition gives $$ \int_{0}^{1}{F(x)dx}=0 $$ and I have to show there exists $\xi\in(0,1)$ such that  $$ F'(\xi)=F''(\xi)F(\xi) $$ I was stuck here.","Let $f:[0,1]\rightarrow\mathbb{R}$ be a differentiable function with continuous derivative such that $$ \int_{0}^{1}{f(x)dx}=\int_{0}^{1}{xf(x)dx} $$ How can we prove that there exists $\xi\in(0,1)$ such that $$ f(\xi)=f'(\xi)\int_{0}^{\xi}{f(x)dx} $$ I tried to use  $$ F(x)=\int_{0}^{x}{f(t)dt} $$ then the condition gives $$ \int_{0}^{1}{F(x)dx}=0 $$ and I have to show there exists $\xi\in(0,1)$ such that  $$ F'(\xi)=F''(\xi)F(\xi) $$ I was stuck here.",,['calculus']
78,Can any dynamical system be written as a hamiltonian system?,Can any dynamical system be written as a hamiltonian system?,,"Can I always find a Hamiltonian for any given Dynamical System such that the Hamiltons' equations are satisfied? The hamiltonian may be an extremely complicated function (Possibly containing complex terms) but in principle, is it always possible to find the hamiltonian for a given Dynamical System?","Can I always find a Hamiltonian for any given Dynamical System such that the Hamiltons' equations are satisfied? The hamiltonian may be an extremely complicated function (Possibly containing complex terms) but in principle, is it always possible to find the hamiltonian for a given Dynamical System?",,"['calculus', 'dynamical-systems', 'mathematical-physics']"
79,Convergence of definite integral,Convergence of definite integral,,I have to find out the convergence of the next integral: $$\int^{\pi/2}_0{\frac{\ln(\sin(x))}{\sqrt{x}}}dx$$ Any help? Thanks,I have to find out the convergence of the next integral: $$\int^{\pi/2}_0{\frac{\ln(\sin(x))}{\sqrt{x}}}dx$$ Any help? Thanks,,"['calculus', 'convergence-divergence', 'definite-integrals']"
80,"Does $1 + \frac{1}{x} + \sqrt{\frac{2x}{x + 1}},$ have a global minimum?",Does  have a global minimum?,"1 + \frac{1}{x} + \sqrt{\frac{2x}{x + 1}},","Does the following function have a global minimum: $$1 + \frac{1}{x} + \sqrt{\frac{2x}{x + 1}},$$ where $x \in \mathbb{N}$? I tried using WolframAlpha , but it appears to give an inconsistent result.","Does the following function have a global minimum: $$1 + \frac{1}{x} + \sqrt{\frac{2x}{x + 1}},$$ where $x \in \mathbb{N}$? I tried using WolframAlpha , but it appears to give an inconsistent result.",,"['calculus', 'inequality', 'computational-algebra', 'a.m.-g.m.-inequality']"
81,Evaluating integrals such as $\int \frac{1+\cos^{2}x}{1+\cos2x}$,Evaluating integrals such as,\int \frac{1+\cos^{2}x}{1+\cos2x},"We started integrals not too long ago, I understand it for the most part but I always have a problem figuring out how to solve ones involving trig identities. Like this: $$\int \frac{1+\cos^{2}x}{1+\cos2x}$$ Indefinite integral of $$\frac{ 1 + \cos^2(x)}{ 1 + \cos(2x) }.$$ I tried changing the denominator to $2\cos^2(x)$ but I still can't make a u substitution.","We started integrals not too long ago, I understand it for the most part but I always have a problem figuring out how to solve ones involving trig identities. Like this: $$\int \frac{1+\cos^{2}x}{1+\cos2x}$$ Indefinite integral of $$\frac{ 1 + \cos^2(x)}{ 1 + \cos(2x) }.$$ I tried changing the denominator to $2\cos^2(x)$ but I still can't make a u substitution.",,"['calculus', 'integration', 'trigonometry', 'indefinite-integrals']"
82,The tangent half-angle substitution in differential equations,The tangent half-angle substitution in differential equations,,"\begin{align} y & = \tan\frac\theta2 \\[8pt] \frac{1-y^2}{1+y^2} & = \cos\theta \\[8pt] \frac{2y}{1+y^2} & = \sin\theta \\[8pt] \frac{2\,dy}{1+y^2} & = d\theta \end{align} This, the tangent half-angle substitution, is famously used in solving equations of the form $$ \frac{df}{d\theta} = f(\cos\theta,\sin\theta) $$ where $f$ is a rational function.  I.e. it is used for finding antiderivatives of rational functions of sine and cosine. Is the same substitution used in solving any other, more elaborate, differential equations?","\begin{align} y & = \tan\frac\theta2 \\[8pt] \frac{1-y^2}{1+y^2} & = \cos\theta \\[8pt] \frac{2y}{1+y^2} & = \sin\theta \\[8pt] \frac{2\,dy}{1+y^2} & = d\theta \end{align} This, the tangent half-angle substitution, is famously used in solving equations of the form $$ \frac{df}{d\theta} = f(\cos\theta,\sin\theta) $$ where $f$ is a rational function.  I.e. it is used for finding antiderivatives of rational functions of sine and cosine. Is the same substitution used in solving any other, more elaborate, differential equations?",,"['calculus', 'ordinary-differential-equations']"
83,Showing that complicated mixed polynomial is always positive,Showing that complicated mixed polynomial is always positive,,"I want to show that $\left(132 q^3-175 q^4+73 q^5-\frac{39 q^6}{4}\right)+\left(-144 q^2+12 q^3+70 q^4-19 q^5\right) r+\left(80 q+200 q^2-243 q^3+100 q^4-\frac{31 q^5}{2}\right) r^2+\left(-208 q+116 q^2+24 q^3-13 q^4\right) r^3+\left(80-44 q-44 q^2+34 q^3-\frac{23 q^4}{4}\right) r^4$ is strictly positive whenever $q \in (0,1)$ (numerically, this holds for all $r \in \mathbb{R}$, although I'm only interested in $r \in (0,1)$). Is that even possible analytically? Any idea towards a proof would be greatly appreciated. Many thanks! EDIT: Here is some more information. Let $f(r) = A + Br + Cr^2 + Dr^3 + Er^4$ be the function as defined above. Then it holds that $f(r)$ is a strictly convex function in $r$ for $q \in (0,1)$, $f(0) > 0$, $f'(0) < 0$, and $f'(q) > 0$. Hence, for the relevant $q \in (0,1)$, $f(r)$ attains its minimum for  some $r^{min} \in (0,q)$. $A$ is positive and strictly increasing in $q$ for the relevant $q \in (0,1)$, $B$ is negative and strictly decreasing in $q$ for the relevant $q \in (0,1)$, $C$ is positive and strictly increasing in $q$ for the relevant $q \in (0,1)$, $D$ is negative and non-monotonic in $q$, and $E$ is positive and strictly decreasing in $q$ for the relevant $q \in (0,1)$.","I want to show that $\left(132 q^3-175 q^4+73 q^5-\frac{39 q^6}{4}\right)+\left(-144 q^2+12 q^3+70 q^4-19 q^5\right) r+\left(80 q+200 q^2-243 q^3+100 q^4-\frac{31 q^5}{2}\right) r^2+\left(-208 q+116 q^2+24 q^3-13 q^4\right) r^3+\left(80-44 q-44 q^2+34 q^3-\frac{23 q^4}{4}\right) r^4$ is strictly positive whenever $q \in (0,1)$ (numerically, this holds for all $r \in \mathbb{R}$, although I'm only interested in $r \in (0,1)$). Is that even possible analytically? Any idea towards a proof would be greatly appreciated. Many thanks! EDIT: Here is some more information. Let $f(r) = A + Br + Cr^2 + Dr^3 + Er^4$ be the function as defined above. Then it holds that $f(r)$ is a strictly convex function in $r$ for $q \in (0,1)$, $f(0) > 0$, $f'(0) < 0$, and $f'(q) > 0$. Hence, for the relevant $q \in (0,1)$, $f(r)$ attains its minimum for  some $r^{min} \in (0,q)$. $A$ is positive and strictly increasing in $q$ for the relevant $q \in (0,1)$, $B$ is negative and strictly decreasing in $q$ for the relevant $q \in (0,1)$, $C$ is positive and strictly increasing in $q$ for the relevant $q \in (0,1)$, $D$ is negative and non-monotonic in $q$, and $E$ is positive and strictly decreasing in $q$ for the relevant $q \in (0,1)$.",,"['calculus', 'algebra-precalculus', 'inequality']"
84,Geometric Interpretation of Total Derivative?,Geometric Interpretation of Total Derivative?,,"Say that: $$z = xy$$ So: $${\partial z \over \partial x} = y$$ and $${\partial z \over \partial y} = x$$ If we plot in 3D space the 2D surface corresponding to eq1, than take a point on that surface, the tangent with respect to the x axis is y, and the tangent corresponding to the y axis is x. Do the total derivatives ($dz \over dx$ and $dz \over dy$) have a similar geometric interpretation?","Say that: $$z = xy$$ So: $${\partial z \over \partial x} = y$$ and $${\partial z \over \partial y} = x$$ If we plot in 3D space the 2D surface corresponding to eq1, than take a point on that surface, the tangent with respect to the x axis is y, and the tangent corresponding to the y axis is x. Do the total derivatives ($dz \over dx$ and $dz \over dy$) have a similar geometric interpretation?",,['calculus']
85,"Prove that if $\lim f$ exists and $\lim (f+g)$ does not exists, then $\lim g$ does not exist.","Prove that if  exists and  does not exists, then  does not exist.",\lim f \lim (f+g) \lim g,"Prove that if $\lim\limits_{x\to a} f(x)$  exists, and $\lim\limits_{x\to a} [f(x)+g(x)]$  does not exists, then $\lim\limits_{x\to a} g(x)$  does not exists. I understand that I have to suppose a certain limit exists, then prove by contradication. But which should I suppose to exists, and which should I aim towards? (Edit) My main question would be mainly, the logic flow of proving this question.  Is it possible to prove  1. directly? 2. by contrapositive? 3. by contradiction? I believe this question is not possible to prove directly and by contrapostive, as it is impossible to show that an arbitary limit does not exist as we do not have enough infomation.","Prove that if $\lim\limits_{x\to a} f(x)$  exists, and $\lim\limits_{x\to a} [f(x)+g(x)]$  does not exists, then $\lim\limits_{x\to a} g(x)$  does not exists. I understand that I have to suppose a certain limit exists, then prove by contradication. But which should I suppose to exists, and which should I aim towards? (Edit) My main question would be mainly, the logic flow of proving this question.  Is it possible to prove  1. directly? 2. by contrapositive? 3. by contradiction? I believe this question is not possible to prove directly and by contrapostive, as it is impossible to show that an arbitary limit does not exist as we do not have enough infomation.",,"['calculus', 'limits']"
86,The discussion about $\int_a^b P^2(x)f(x)dx=0$,The discussion about,\int_a^b P^2(x)f(x)dx=0,"$f$ is Riemann integrable in $[a,b]$,and $\int_a^b f(x)dx>0$. If the polynomial $P(x)$ satisfies $\int_a^b P^2(x)f(x)dx=0$. Prove $P(x)=0$.","$f$ is Riemann integrable in $[a,b]$,and $\int_a^b f(x)dx>0$. If the polynomial $P(x)$ satisfies $\int_a^b P^2(x)f(x)dx=0$. Prove $P(x)=0$.",,['calculus']
87,Elementary Set Theory Question,Elementary Set Theory Question,,"I am working on the following question. Let $X$ be a nonempty set and consider a map $f:X\to Y$. Prove that the following are equivalent: (a) $f$ is injective; (b) there exists $g:Y\to X$ such that $g∘f=1_{X}$ where $1_{X}:X\to X$ is the identity map; (c) for any set $Z$ and any maps $h_{1},h_{2}:Z\to X$, the equation $f∘h_{1}=f∘h_{2}$ implies that $h_{1}=h_{2}$. (*I would like to add here that the ""$1$"" from each ""$1_{X}$"" is supposed to be written with the Math Blackboard font - I'm not sure how to do that*) My issue at the moment arises from being unable to understand (some of) the notation and ideas. I will attempt to describe in my own terms what I believe to understand. I apologize for the terribly imprecise language. You will see why I am not ready to prove anything yet. (a) That the function $f$ is injective means that it never maps distinct elements of its domain to the same element of its codomain. So every element from the set $X$ maps to a unique element in the set $Y$. And vice versa. A function of the form $f(x)=ax+c$ would be injective (and also a bijection) while a function of the form $f(x)=ax^{2}+bx+c$ would not be (assuming no restrictions on the domain). (b) This part states that there exists a function $g$ that maps the set $Y$ to the set $X$. The rest I am not sure of. The composite function $g∘f$ equals the identity map $1_{X}$. Does the identity map simply map X back onto X? For instance if $X=\left \{ 1,2 \right \}$, does $1_{X}$ map $1 \to 1$ and $2 \to 2$? So this part would mean, informally, that if you take an element from $Y$, replace it with its corresponding element from $X$, and then map it back to the same element from $X$, you get the identity map. Essentially each element from $Y$ maps to one element from $X$ and you can go back and forth between the sets. (c) Now the final part. There is a set $Z$ and two maps $h_{1}$, and $h_{2}$, both of which map the set $Z$ to the set $X$. Furthermore (and I dont know if this notation is acceptable), $$f:(h_{1}:Z \to X) \to Y=f:(h_{2}:Z \to X) \to Y.$$ I'm not seeing why this shows that $f$ is injective. I apologize again for this clutter. This is my first real exposure to set theory for my first year Calculus course and our textbook (Stewart) does not cover this. Writing this out has actually helped me to get a better grasp of what I am trying to prove, though I doubt it looks that way. Any help would be appreciated.","I am working on the following question. Let $X$ be a nonempty set and consider a map $f:X\to Y$. Prove that the following are equivalent: (a) $f$ is injective; (b) there exists $g:Y\to X$ such that $g∘f=1_{X}$ where $1_{X}:X\to X$ is the identity map; (c) for any set $Z$ and any maps $h_{1},h_{2}:Z\to X$, the equation $f∘h_{1}=f∘h_{2}$ implies that $h_{1}=h_{2}$. (*I would like to add here that the ""$1$"" from each ""$1_{X}$"" is supposed to be written with the Math Blackboard font - I'm not sure how to do that*) My issue at the moment arises from being unable to understand (some of) the notation and ideas. I will attempt to describe in my own terms what I believe to understand. I apologize for the terribly imprecise language. You will see why I am not ready to prove anything yet. (a) That the function $f$ is injective means that it never maps distinct elements of its domain to the same element of its codomain. So every element from the set $X$ maps to a unique element in the set $Y$. And vice versa. A function of the form $f(x)=ax+c$ would be injective (and also a bijection) while a function of the form $f(x)=ax^{2}+bx+c$ would not be (assuming no restrictions on the domain). (b) This part states that there exists a function $g$ that maps the set $Y$ to the set $X$. The rest I am not sure of. The composite function $g∘f$ equals the identity map $1_{X}$. Does the identity map simply map X back onto X? For instance if $X=\left \{ 1,2 \right \}$, does $1_{X}$ map $1 \to 1$ and $2 \to 2$? So this part would mean, informally, that if you take an element from $Y$, replace it with its corresponding element from $X$, and then map it back to the same element from $X$, you get the identity map. Essentially each element from $Y$ maps to one element from $X$ and you can go back and forth between the sets. (c) Now the final part. There is a set $Z$ and two maps $h_{1}$, and $h_{2}$, both of which map the set $Z$ to the set $X$. Furthermore (and I dont know if this notation is acceptable), $$f:(h_{1}:Z \to X) \to Y=f:(h_{2}:Z \to X) \to Y.$$ I'm not seeing why this shows that $f$ is injective. I apologize again for this clutter. This is my first real exposure to set theory for my first year Calculus course and our textbook (Stewart) does not cover this. Writing this out has actually helped me to get a better grasp of what I am trying to prove, though I doubt it looks that way. Any help would be appreciated.",,"['calculus', 'elementary-set-theory']"
88,"Evaluating the definite integral $\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\,\mathrm{d}x$",Evaluating the definite integral,"\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\,\mathrm{d}x","To evaluate $\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\,\mathrm{d}x$, I have applied the substitution $u=\frac{(x-a)^2}{b}$, $x-a=(ub)^{1/2}$, and $\frac{\mathrm{d}u}{\mathrm{d}x}=\frac{2(x-a)}{b}$. I would first like to ask if $x-a$ should actually equal $\pm (ub)^{1/2}$ (i.e., is it valid to ignore the minus sign, and why?). Applying this substitution, \begin{align} I &=\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\, \mathrm{d}x \\ &=\int_0^\infty x \mathrm{e}^{-u} \frac{b\,\mathrm{d}u}{2(x-a)} \\ &=\int_{\frac{a^2}{b}}^\infty \left((ub)^{1/2} + a\right)\cdot{}\mathrm{e}^{-u} \frac{b\,\mathrm{d}u}{2(ub)^{1/2}} \\ &=\frac{b}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u}\,\mathrm{d}u + ab^{-1/2}\mathrm{e}^{-u} u^{-1/2}\,\mathrm{d}u \\ &=\frac{b}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u}\,\mathrm{d}u + \frac{a\sqrt{b}}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u} u^{-1/2}\,\mathrm{d}u, \end{align} I find that I am unable to evaluate the second term because the domain is from a non-zero constant to $+\infty$. Were the domain $[0,+\infty)$, the second integral would simply be a $\Gamma$ function. Seeing that the substitution I have attempted has not worked, could someone please propose an alternate route to evaluating this definite integral? Thank you.","To evaluate $\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\,\mathrm{d}x$, I have applied the substitution $u=\frac{(x-a)^2}{b}$, $x-a=(ub)^{1/2}$, and $\frac{\mathrm{d}u}{\mathrm{d}x}=\frac{2(x-a)}{b}$. I would first like to ask if $x-a$ should actually equal $\pm (ub)^{1/2}$ (i.e., is it valid to ignore the minus sign, and why?). Applying this substitution, \begin{align} I &=\int_0^\infty x \mathrm{e}^{-\frac{(x-a)^2}{b}}\, \mathrm{d}x \\ &=\int_0^\infty x \mathrm{e}^{-u} \frac{b\,\mathrm{d}u}{2(x-a)} \\ &=\int_{\frac{a^2}{b}}^\infty \left((ub)^{1/2} + a\right)\cdot{}\mathrm{e}^{-u} \frac{b\,\mathrm{d}u}{2(ub)^{1/2}} \\ &=\frac{b}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u}\,\mathrm{d}u + ab^{-1/2}\mathrm{e}^{-u} u^{-1/2}\,\mathrm{d}u \\ &=\frac{b}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u}\,\mathrm{d}u + \frac{a\sqrt{b}}{2}\int_{\frac{a^2}{b}}^\infty \mathrm{e}^{-u} u^{-1/2}\,\mathrm{d}u, \end{align} I find that I am unable to evaluate the second term because the domain is from a non-zero constant to $+\infty$. Were the domain $[0,+\infty)$, the second integral would simply be a $\Gamma$ function. Seeing that the substitution I have attempted has not worked, could someone please propose an alternate route to evaluating this definite integral? Thank you.",,"['calculus', 'integration']"
89,Orthogonality of the Gegenbauer Polynomials,Orthogonality of the Gegenbauer Polynomials,,"Typically the orthogonality relation for the Gegenbauer polynomials is given as: $$ \int_{-1}^{1}C_{n}^{\alpha}(x)C_{m}^{\alpha}(x)\cdot(1-x^2)^{\alpha-1/2}dx=\frac{\pi2^{1-2\alpha}\Gamma(2\alpha+n)}{n!(n+\alpha)(\Gamma(\alpha))^2}\delta_{mn} $$ The Rodrigues formula for $C_{n}^{\alpha}(x)$ is given by: $$ C_{n}^{\alpha}(x)=constant\cdot(1-x^2)^{-\alpha+1/2}\frac{d^n}{dx^n}\left[(1-x^2)^{n+\alpha-1/2}\right] $$ My question is the following: How do I derive/proove the orthogonality relation? In George Andrews' ""Special Functions"" Ch. 6, he proves the orthogonality relation for the Hermite and Laguerre polynomials by substituting the Rodrigues form of the polynomial in for one of the polynomials and then using integration by parts $n$ times.  However when I try that I get something like: $$ \sum_{k=1}^{m}(-1)^{k+1}C_{n-(k-1)}^{\alpha+(k-1)}(x)\frac{d^{m-k}}{dx^{m-k}}\left[(1-x^2)^{m+\alpha-1/2}\right]\bigg|_{-1}^{1}+\int_{-1}^{1}C_{n-m}^{\alpha+m}(x)\cdot(1-x^2)^{m+\alpha-1/2}dx $$ I don't immediately see why all the terms in the summation go to zero, nor how to evaluate the integral directly.  Any ideas?","Typically the orthogonality relation for the Gegenbauer polynomials is given as: $$ \int_{-1}^{1}C_{n}^{\alpha}(x)C_{m}^{\alpha}(x)\cdot(1-x^2)^{\alpha-1/2}dx=\frac{\pi2^{1-2\alpha}\Gamma(2\alpha+n)}{n!(n+\alpha)(\Gamma(\alpha))^2}\delta_{mn} $$ The Rodrigues formula for $C_{n}^{\alpha}(x)$ is given by: $$ C_{n}^{\alpha}(x)=constant\cdot(1-x^2)^{-\alpha+1/2}\frac{d^n}{dx^n}\left[(1-x^2)^{n+\alpha-1/2}\right] $$ My question is the following: How do I derive/proove the orthogonality relation? In George Andrews' ""Special Functions"" Ch. 6, he proves the orthogonality relation for the Hermite and Laguerre polynomials by substituting the Rodrigues form of the polynomial in for one of the polynomials and then using integration by parts $n$ times.  However when I try that I get something like: $$ \sum_{k=1}^{m}(-1)^{k+1}C_{n-(k-1)}^{\alpha+(k-1)}(x)\frac{d^{m-k}}{dx^{m-k}}\left[(1-x^2)^{m+\alpha-1/2}\right]\bigg|_{-1}^{1}+\int_{-1}^{1}C_{n-m}^{\alpha+m}(x)\cdot(1-x^2)^{m+\alpha-1/2}dx $$ I don't immediately see why all the terms in the summation go to zero, nor how to evaluate the integral directly.  Any ideas?",,"['calculus', 'special-functions', 'orthogonal-polynomials']"
90,Motivation for a particular integration substitution,Motivation for a particular integration substitution,,"In an old Italian calculus problem book, there is an example presented: $$\int\frac{dx}{x\sqrt{2x-1}}$$ The solution given uses the strange substitution $$x=\frac{1}{1-u}$$ Some preliminary work in trying to determine the motivation as to why one would come up with such an odd substitution yielded a right triangle with hypotenuse $x$ and leg $x-1;$ determining the other leg gives $\sqrt{2x-1}.$  Conveniently, this triangle contains all of the ""important"" parts of our integrand, except in a non-convenient manner. So, my question is two-fold: (1)  Does anyone see why one would be motivated to make such a substitution? (2)  Does anyone see how to extend the work involving the right triangle to get at the solution?","In an old Italian calculus problem book, there is an example presented: $$\int\frac{dx}{x\sqrt{2x-1}}$$ The solution given uses the strange substitution $$x=\frac{1}{1-u}$$ Some preliminary work in trying to determine the motivation as to why one would come up with such an odd substitution yielded a right triangle with hypotenuse $x$ and leg $x-1;$ determining the other leg gives $\sqrt{2x-1}.$  Conveniently, this triangle contains all of the ""important"" parts of our integrand, except in a non-convenient manner. So, my question is two-fold: (1)  Does anyone see why one would be motivated to make such a substitution? (2)  Does anyone see how to extend the work involving the right triangle to get at the solution?",,['calculus']
91,Arnold's Trivium problem 52,Arnold's Trivium problem 52,,Calculate the first term of the asymptotic expression as $k \to \infty$ of the integral $$ \int_{-\infty}^{+\infty}\frac{e^{ikx}}{\sqrt{1+x^{2n}}}dx $$ May I bother you to explain what the problem is asking and what is the intuition behind in this problem? It is from the problem 52 of the article A mathematical trivium by Arnol'd.,Calculate the first term of the asymptotic expression as of the integral May I bother you to explain what the problem is asking and what is the intuition behind in this problem? It is from the problem 52 of the article A mathematical trivium by Arnol'd.,k \to \infty  \int_{-\infty}^{+\infty}\frac{e^{ikx}}{\sqrt{1+x^{2n}}}dx ,"['calculus', 'complex-analysis', 'intuition']"
92,"calculus textbook avoiding ""nice"" numbers: all numbers are decimals with 2 or 3 sig figs","calculus textbook avoiding ""nice"" numbers: all numbers are decimals with 2 or 3 sig figs",,"Many years ago, my father had a large number of older used textbooks. I seem to remember a calculus textbook with a somewhat unusual feature, and I am wondering if the description rings a bell with anyone here. Basically, this was a calculus textbook that took the slightly unusual route of avoiding ""nice"" numbers in all examples. The reader was supposed to always have a calculator at their side, and evaluate everything as a decimal, and only use 2 or 3 significant figures. So for instance, rather than asking for the $\int_1^{\sqrt3} \frac{1}{1+x^2}$, it might be from $x=1.2$ to $x=2.6$, say. The author had done this as a deliberate choice, since most ""real-life"" math problems involve random-looking decimal numbers, and not very many significant digits. Does this sound familiar to anybody? Any ideas what this textbook might have been?","Many years ago, my father had a large number of older used textbooks. I seem to remember a calculus textbook with a somewhat unusual feature, and I am wondering if the description rings a bell with anyone here. Basically, this was a calculus textbook that took the slightly unusual route of avoiding ""nice"" numbers in all examples. The reader was supposed to always have a calculator at their side, and evaluate everything as a decimal, and only use 2 or 3 significant figures. So for instance, rather than asking for the $\int_1^{\sqrt3} \frac{1}{1+x^2}$, it might be from $x=1.2$ to $x=2.6$, say. The author had done this as a deliberate choice, since most ""real-life"" math problems involve random-looking decimal numbers, and not very many significant digits. Does this sound familiar to anybody? Any ideas what this textbook might have been?",,"['calculus', 'reference-request', 'soft-question']"
93,Convergence of a double sum,Convergence of a double sum,,"Let $(a_i)_{i=1}^\infty$ be a sequence of positive numbers such that $\sum_1^\infty a_i < \infty$. What can we say about the double series    $$\sum_{i, j=1}^\infty a_{i+ j}^p\ ?$$   Can we find some values of $p$ for which it converges? I'm especially interested in $p=2$. Intuitively I'm inclined to think that the series converges for $p \ge 2$. This intuition comes from the continuum analog $f(x)= x^a, \quad x>1$:  if $a<-1$ we have $$\int_1^\infty f(x)\ dx < \infty$$ and $F(x, y)=f(x+y)$ is $p$-integrable on $(1, \infty) \times (1, \infty)$ for $p \ge 2$.","Let $(a_i)_{i=1}^\infty$ be a sequence of positive numbers such that $\sum_1^\infty a_i < \infty$. What can we say about the double series    $$\sum_{i, j=1}^\infty a_{i+ j}^p\ ?$$   Can we find some values of $p$ for which it converges? I'm especially interested in $p=2$. Intuitively I'm inclined to think that the series converges for $p \ge 2$. This intuition comes from the continuum analog $f(x)= x^a, \quad x>1$:  if $a<-1$ we have $$\int_1^\infty f(x)\ dx < \infty$$ and $F(x, y)=f(x+y)$ is $p$-integrable on $(1, \infty) \times (1, \infty)$ for $p \ge 2$.",,"['calculus', 'measure-theory']"
94,"Calculating arc length of a curve, stuck on dy/dx part (algebra mostly)","Calculating arc length of a curve, stuck on dy/dx part (algebra mostly)",,"The equation is: $$x=\frac{1}{8}y^4 + \frac{1}{4}y^{-2},\qquad 1\leq y\leq 2.$$ I have the formula. I'm not sure how to write it out but this is what it says: Length is equal to the integral (with $b$ and $a$ for limits) of the square root of $1+(dy/dx)^2 dx$ So I first took the derivative of the equation and got $(1/2)y^3 - (1/2)y^{-3}$. Now when I plug that back into the formula, I have to square it and I got $(1/4)y^6 - (1/4)y^{-6}$. I factored out a $1/4$ and turned the $y^{-6}$ into $1/y^6$. Just a mess at this point, that doesn't seem right. Can someone help me out with that? Haha I know it's ridiculous but my algebra skills are lacking.","The equation is: $$x=\frac{1}{8}y^4 + \frac{1}{4}y^{-2},\qquad 1\leq y\leq 2.$$ I have the formula. I'm not sure how to write it out but this is what it says: Length is equal to the integral (with $b$ and $a$ for limits) of the square root of $1+(dy/dx)^2 dx$ So I first took the derivative of the equation and got $(1/2)y^3 - (1/2)y^{-3}$. Now when I plug that back into the formula, I have to square it and I got $(1/4)y^6 - (1/4)y^{-6}$. I factored out a $1/4$ and turned the $y^{-6}$ into $1/y^6$. Just a mess at this point, that doesn't seem right. Can someone help me out with that? Haha I know it's ridiculous but my algebra skills are lacking.",,['calculus']
95,"Series absolute convergence, proof","Series absolute convergence, proof",,"I had this problem on my last exam in calculus, I still don't know, how to solve this, so I would appreciate your help. Let $a_{ij} \in \mathbb{R}$ for $i,j \in \mathbb{N}$. Suppose, that for every $j$ series $S_j = \sum_{i=1}^{\infty}{a_{ij}}$ is absolutely convergent and, for every $i$ exists finite limit (limit $\not= \pm\infty$) $c_i=\lim_{j\to\infty}{a_{ij}}$. (a) Prove, that if exists such absolutely convergent series $\sum_{i=1}^{\infty}{b_i}$ that $|a_{ij}| \leq |b_i|$ for every $i,j$ then $S_j \to S = \sum_{i=1}^{\infty}{c_i}$, $j \to \infty$ (b) Is argument in (a) true without assumption that series $\sum{}b_i$  with properties given in (a) exists?","I had this problem on my last exam in calculus, I still don't know, how to solve this, so I would appreciate your help. Let $a_{ij} \in \mathbb{R}$ for $i,j \in \mathbb{N}$. Suppose, that for every $j$ series $S_j = \sum_{i=1}^{\infty}{a_{ij}}$ is absolutely convergent and, for every $i$ exists finite limit (limit $\not= \pm\infty$) $c_i=\lim_{j\to\infty}{a_{ij}}$. (a) Prove, that if exists such absolutely convergent series $\sum_{i=1}^{\infty}{b_i}$ that $|a_{ij}| \leq |b_i|$ for every $i,j$ then $S_j \to S = \sum_{i=1}^{\infty}{c_i}$, $j \to \infty$ (b) Is argument in (a) true without assumption that series $\sum{}b_i$  with properties given in (a) exists?",,"['calculus', 'sequences-and-series', 'limits']"
96,Challenging Integral Problem,Challenging Integral Problem,,"If $I_n=\int_0^a \frac{1}{(a^2+x^2)^n} dx$ , show that (i) $\int_0^a \frac{x^2}{(a^2+x^2)^{n+1}} dx=I_n-a^2I_{n+1}$ (ii) $2na^2I_{n+1}=(2n-1)I_n+(2^na^{2n-1})^{-1}$ Here is what I have done, I was able to fully solve part (i) but not part (ii): (i) Consider $\int_0^a \frac{x^2}{(a^2+x^2)^{n+1}} dx$ . Let $x=atan\theta$ , $dx=asec^2\theta d\theta$ . $$a^2+x^2=a^2+a^2tan^2\theta=a^2(1+tan^2\theta)=a^2sec^2\theta$$ Then: $$\int_0^a \frac{x^2}{(a^2+x^2)^{n+1}} dx=\int_0^{arctan\frac{x}{a}} \frac{a^2tan^2\theta}{(a^2sec^2\theta)^{n+1}}asec^2\theta d\theta$$ $$=\int_0^{arctan\frac{x}{a}} \frac{a^3sec^2\theta(sec^2\theta-1)}{(a^2sec^2\theta)^{n+1}}d\theta$$ $$=\int_0^{arctan\frac{x}{a}} \frac{a^3sec^4\theta}{(a^2sec^2\theta)^{n+1}}d\theta-\int_0^{arctan\frac{x}{a}}\frac{a^3sec^2\theta}{(a^2sec^2\theta)^{n+1}}d\theta$$ $$=\int_0^{arctan\frac{x}{a}} \frac{asec^2\theta}{(a^2sec^2\theta)^n}d\theta-\int_0^{arctan\frac{x}{a}}\frac{a^3sec^2\theta}{(a^2sec^2\theta)^{n+1}}d\theta$$ $$=\int_0^a \frac{asec^2\theta}{(a^2+x^2)^n}\frac{dx}{asec^2\theta}-\int_0^a\frac{a^3sec^2\theta}{(a^2+x^2)^{n+1}}\frac{dx}{asec^2\theta}$$ $$=\int_0^a \frac{1}{(a^2+x^2)^n}dx-a^2\int_0^a\frac{1}{(a^2+x^2)^{n+1}}dx$$ $$=I_n-a^2I_{n+1}$$ (ii) I am unsure how to even approach this one... Anybody have any ideas?","If , show that (i) (ii) Here is what I have done, I was able to fully solve part (i) but not part (ii): (i) Consider . Let , . Then: (ii) I am unsure how to even approach this one... Anybody have any ideas?",I_n=\int_0^a \frac{1}{(a^2+x^2)^n} dx \int_0^a \frac{x^2}{(a^2+x^2)^{n+1}} dx=I_n-a^2I_{n+1} 2na^2I_{n+1}=(2n-1)I_n+(2^na^{2n-1})^{-1} \int_0^a \frac{x^2}{(a^2+x^2)^{n+1}} dx x=atan\theta dx=asec^2\theta d\theta a^2+x^2=a^2+a^2tan^2\theta=a^2(1+tan^2\theta)=a^2sec^2\theta \int_0^a \frac{x^2}{(a^2+x^2)^{n+1}} dx=\int_0^{arctan\frac{x}{a}} \frac{a^2tan^2\theta}{(a^2sec^2\theta)^{n+1}}asec^2\theta d\theta =\int_0^{arctan\frac{x}{a}} \frac{a^3sec^2\theta(sec^2\theta-1)}{(a^2sec^2\theta)^{n+1}}d\theta =\int_0^{arctan\frac{x}{a}} \frac{a^3sec^4\theta}{(a^2sec^2\theta)^{n+1}}d\theta-\int_0^{arctan\frac{x}{a}}\frac{a^3sec^2\theta}{(a^2sec^2\theta)^{n+1}}d\theta =\int_0^{arctan\frac{x}{a}} \frac{asec^2\theta}{(a^2sec^2\theta)^n}d\theta-\int_0^{arctan\frac{x}{a}}\frac{a^3sec^2\theta}{(a^2sec^2\theta)^{n+1}}d\theta =\int_0^a \frac{asec^2\theta}{(a^2+x^2)^n}\frac{dx}{asec^2\theta}-\int_0^a\frac{a^3sec^2\theta}{(a^2+x^2)^{n+1}}\frac{dx}{asec^2\theta} =\int_0^a \frac{1}{(a^2+x^2)^n}dx-a^2\int_0^a\frac{1}{(a^2+x^2)^{n+1}}dx =I_n-a^2I_{n+1},"['calculus', 'integration', 'definite-integrals']"
97,"If $f$ is continuous at $a$, is $f^{-1}$ continuous at $f(a)$?","If  is continuous at , is  continuous at ?",f a f^{-1} f(a),"Let $I\subseteq\mathbb{R}$ be an open interval, and $f:I\to\mathbb{R}$ an injective function. Let $a\in I$ , and suppose that $f$ is continuous at $a$ . Does it follow that $f^{-1}$ is continuous at $f(a)$ ? I know that if $f$ is assumed to be continuous in the entire $I$ , then $f^{-1}$ is continuous. (Here I use the fact that $f$ is strictly monotonic.) With local continuity, however, I cannot use monotonic properties.","Let be an open interval, and an injective function. Let , and suppose that is continuous at . Does it follow that is continuous at ? I know that if is assumed to be continuous in the entire , then is continuous. (Here I use the fact that is strictly monotonic.) With local continuity, however, I cannot use monotonic properties.",I\subseteq\mathbb{R} f:I\to\mathbb{R} a\in I f a f^{-1} f(a) f I f^{-1} f,"['calculus', 'continuity', 'inverse-function', 'monotone-functions']"
98,Solve functional equation: $f(x) + f(y) = f\left(\frac{x+y}{1-xy}\right) $,Solve functional equation:,f(x) + f(y) = f\left(\frac{x+y}{1-xy}\right) ,"The equation being : $$ f(x) + f(y) = f\left(\frac{x+y}{1-xy}\right) $$ The most obvious answer is, $$f(x) = C\arctan x$$ but there is another possible answer e.i $$ f(x) =  \log\left(\frac{1-x}{1+x}\right) $$ which i am unable to get by the process i have provided. First substituting: $$    x = y = 0 \Rightarrow f(0) = 0$$ then partially differentiating the original equation wrt to $x$ and then $y$ individually (provided $x \ne y$ ), we'll get : $$ f'(x) = f'\left(\frac{x+y}{1-xy}\right)\frac{1+y^2}{(1-xy)^2}   - (i)$$ $$ f'(y) = f'\left(\frac{x+y}{1-xy}\right)\frac{1+x^2}{(1-xy)^2}   - (ii)$$ diving equations $i$ by $ii$ we get $$  \frac{f'(x)}{f'(y)} = \frac{1+y^2}{1+x^2}$$ $$ \Rightarrow f'(x)\left(1+x^2\right) = f'(y)\left(1+y^2\right)$$ which is only possible if the above equation is equal to some constant $c$ , since $x \neq y$ $$    f'(x)\left(1+x^2\right) = C $$ $$ \Rightarrow f'(x) = \frac{C}{1+x^2} $$ $$ \Rightarrow f(x) = \int \frac{C}{1+x^2} = C \arctan x + K $$ since $f(0) = 0, k = 0$ $$ \therefore  f(x) = C \arctan x $$ Also, when do i know that there is no more functions that satisfy the original equation ?","The equation being : The most obvious answer is, but there is another possible answer e.i which i am unable to get by the process i have provided. First substituting: then partially differentiating the original equation wrt to and then individually (provided ), we'll get : diving equations by we get which is only possible if the above equation is equal to some constant , since since Also, when do i know that there is no more functions that satisfy the original equation ?"," f(x) + f(y) = f\left(\frac{x+y}{1-xy}\right)  f(x) = C\arctan x  f(x) =  \log\left(\frac{1-x}{1+x}\right)      x = y = 0 \Rightarrow f(0) = 0 x y x \ne y  f'(x) = f'\left(\frac{x+y}{1-xy}\right)\frac{1+y^2}{(1-xy)^2}   - (i)  f'(y) = f'\left(\frac{x+y}{1-xy}\right)\frac{1+x^2}{(1-xy)^2}   - (ii) i ii   \frac{f'(x)}{f'(y)} = \frac{1+y^2}{1+x^2}  \Rightarrow f'(x)\left(1+x^2\right) = f'(y)\left(1+y^2\right) c x \neq y     f'(x)\left(1+x^2\right) = C   \Rightarrow f'(x) = \frac{C}{1+x^2}   \Rightarrow f(x) = \int \frac{C}{1+x^2} = C \arctan x + K  f(0) = 0, k = 0  \therefore  f(x) = C \arctan x ","['calculus', 'integration', 'functions', 'logarithms']"
99,Prove that $\lim_{N \to +\infty}\sum^N_{n=1}{|\sin n^\alpha| \over n^\beta} = +\infty$,Prove that,\lim_{N \to +\infty}\sum^N_{n=1}{|\sin n^\alpha| \over n^\beta} = +\infty,"I want to prove that $$\lim_{N \to +\infty}\sum^N_{n=1}{|\sin n^\alpha| \over n^\beta} = +\infty \quad \forall\alpha \gt 0, \; \forall \beta \le 1, \; \alpha, \beta \in \Bbb R.$$ In other words, I want to show that, under the specified restrictions on the parameters, the series diverges to $+\infty$ and is not oscillating or irregular, for instance. I have verified that the series indeed does this by looking at its graph for various parameter choices. I observed that, $\forall \alpha \gt 0$ : $0 \lt \beta \le 1 \implies \lim_{n \to +\infty}a_n=0$ , so the necessary condition for convergence is met. To prove positive divergence, I tried studying the associated improper integral: $$\int^{+\infty}_1{|\sin x^\alpha| \over x^\beta}\ dx = \int^{\sqrt[\alpha]{\pi}}_1{|\sin x^\alpha| \over x^\beta}\ dx + \sum^{+\infty}_{k=1} \int^{\sqrt[\alpha]{(k+1)\pi}}_{\sqrt[\alpha]{k\pi}}{|\sin x^\alpha| \over x^\beta}\ dx.$$ The integrand is $\ge 0$ within the intervals considered, so the integral on the left diverges to $+\infty$ . Since the integral test only works if the sequence is monotone decreasing, I'm not sure I can apply this result to show that also $\sum^{+\infty}_{n=1}a_n = {+\infty}$ ; $\beta \le 0 \implies \nexists \lim_{n \to +\infty}a_n$ , so the series is not convergent. Still, I want to show that the stronger statement above is true. An idea that occurred to me, but that I wasn't able to formalize, was that $n \in \Bbb N \implies n^\alpha \neq \pi \; \forall \alpha \gt 0$ , since $\pi$ is a transcendental number and as such cannot be construed out of any combination of real numbers, which in turn implies that $|\sin{n^\alpha}| \gt 0 \; \forall \alpha \gt 0.$ (However, I do not have the knowledge necessary to back up this claim.) Wouldn't this imply that $a_n$ is bounded below by some constant $k \gt 0$ , meaning that the series would diverge to $+\infty$ ? If anybody could help shed some light, I'd much appreciate it.","I want to prove that In other words, I want to show that, under the specified restrictions on the parameters, the series diverges to and is not oscillating or irregular, for instance. I have verified that the series indeed does this by looking at its graph for various parameter choices. I observed that, : , so the necessary condition for convergence is met. To prove positive divergence, I tried studying the associated improper integral: The integrand is within the intervals considered, so the integral on the left diverges to . Since the integral test only works if the sequence is monotone decreasing, I'm not sure I can apply this result to show that also ; , so the series is not convergent. Still, I want to show that the stronger statement above is true. An idea that occurred to me, but that I wasn't able to formalize, was that , since is a transcendental number and as such cannot be construed out of any combination of real numbers, which in turn implies that (However, I do not have the knowledge necessary to back up this claim.) Wouldn't this imply that is bounded below by some constant , meaning that the series would diverge to ? If anybody could help shed some light, I'd much appreciate it.","\lim_{N \to +\infty}\sum^N_{n=1}{|\sin n^\alpha| \over n^\beta} = +\infty \quad \forall\alpha \gt 0, \; \forall \beta \le 1, \; \alpha, \beta \in \Bbb R. +\infty \forall \alpha \gt 0 0 \lt \beta \le 1 \implies \lim_{n \to +\infty}a_n=0 \int^{+\infty}_1{|\sin x^\alpha| \over x^\beta}\ dx = \int^{\sqrt[\alpha]{\pi}}_1{|\sin x^\alpha| \over x^\beta}\ dx + \sum^{+\infty}_{k=1} \int^{\sqrt[\alpha]{(k+1)\pi}}_{\sqrt[\alpha]{k\pi}}{|\sin x^\alpha| \over x^\beta}\ dx. \ge 0 +\infty \sum^{+\infty}_{n=1}a_n = {+\infty} \beta \le 0 \implies \nexists \lim_{n \to +\infty}a_n n \in \Bbb N \implies n^\alpha \neq \pi \; \forall \alpha \gt 0 \pi |\sin{n^\alpha}| \gt 0 \; \forall \alpha \gt 0. a_n k \gt 0 +\infty","['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
