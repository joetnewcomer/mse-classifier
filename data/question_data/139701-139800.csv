,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Existence of point with prescribed value from given differential equation,Existence of point with prescribed value from given differential equation,,"I came up with this problem while I was trying to prove the following geometric problem : Let $A, B$ be the distinct points in $\mathbb{R}^2$ . If $r(t)$ is the non self-intersecting trajectory of particle in $\mathbb{R}^2$ starts at $A$ and ends at $B$ , is there a moment when velocity vector of particle is pointing same direction with the vector $B-A $ $?$ , that is, $r'(t)=c(B-A)$ for some $c>0$$?$ This is definitely not true in general for $\mathbb{R}^n$ with $n>2$ , but intuitively it must be true for $\mathbb{R}^2$ . First I formalized this problem as follows : Let $r:[a,b] \rightarrow \mathbb{R}^2$ : continuously differentiable unit speed curve in $\mathbb{R}^2$ with $r(a)=(0,0), r(b)=(1,0)$ . Then $v(t)=r'(t) \in S^1$ . Now letting $r(t)=(r_1(t),r_2(t))$ , $v(t)=(r_1'(t),r_2'(t))$ we get differential equation $$ r_1'(t)^2 + r_2'(t)^2 = 1 $$ with boundary conditions $$ r_1(b) = 1\\ r_1(a) = r_2(b) =  r_2(a) = 0 $$ Or equivalently we get integral equations $$ \int_a^b r_1'(t) dt = r_1(b) -  r_1(a) = 1 \\ \int_a^b r_2'(t) dt = r_2(b) -  r_2(a) = 0 \\ r_1'(t)^2 + r_2'(t)^2 = 1 $$ Lastly we can parametrize $v(t)$ by angle function $\theta(t)$ s.t. $v(t)=(r_1'(t),r_2'(t))=(\cos\theta(t),\sin\theta(t))$ and then we get $$ \int_a^b \cos\theta(t) dt =  1 \\ \int_a^b \sin\theta(t) dt = 0  $$ The goal is to prove $\exists t_0 \in [a,b] \ s.t. v(t_0)=(1,0)$ .(Under the last formalism it is to prove $\exists t_0 \in [a,b] \ s.t. \theta(t_0) = 2\pi n $ for some $  n\in \mathbb{Z}$ . Now how can I prove this? Also since I have no knowledge about getting information of solution of this type of differential/integral equation, any comment on how/where I can learn about this type of problem would be helpful. EDIT : I noticed that I need to assume that trajectory is not self-intersecting, that is, $r(t)$ is need to be one-to-one. Otherwise there is an obvious counterexample: $r(t)$ is not one-to-one"" />","I came up with this problem while I was trying to prove the following geometric problem : Let be the distinct points in . If is the non self-intersecting trajectory of particle in starts at and ends at , is there a moment when velocity vector of particle is pointing same direction with the vector , that is, for some This is definitely not true in general for with , but intuitively it must be true for . First I formalized this problem as follows : Let : continuously differentiable unit speed curve in with . Then . Now letting , we get differential equation with boundary conditions Or equivalently we get integral equations Lastly we can parametrize by angle function s.t. and then we get The goal is to prove .(Under the last formalism it is to prove for some . Now how can I prove this? Also since I have no knowledge about getting information of solution of this type of differential/integral equation, any comment on how/where I can learn about this type of problem would be helpful. EDIT : I noticed that I need to assume that trajectory is not self-intersecting, that is, is need to be one-to-one. Otherwise there is an obvious counterexample: $r(t)$ is not one-to-one"" />","A, B \mathbb{R}^2 r(t) \mathbb{R}^2 A B B-A  ? r'(t)=c(B-A) c>0? \mathbb{R}^n n>2 \mathbb{R}^2 r:[a,b] \rightarrow \mathbb{R}^2 \mathbb{R}^2 r(a)=(0,0), r(b)=(1,0) v(t)=r'(t) \in S^1 r(t)=(r_1(t),r_2(t)) v(t)=(r_1'(t),r_2'(t)) 
r_1'(t)^2 + r_2'(t)^2 = 1
 
r_1(b) = 1\\
r_1(a) = r_2(b) =  r_2(a) = 0
 
\int_a^b r_1'(t) dt = r_1(b) -  r_1(a) = 1 \\
\int_a^b r_2'(t) dt = r_2(b) -  r_2(a) = 0 \\
r_1'(t)^2 + r_2'(t)^2 = 1
 v(t) \theta(t) v(t)=(r_1'(t),r_2'(t))=(\cos\theta(t),\sin\theta(t)) 
\int_a^b \cos\theta(t) dt =  1 \\
\int_a^b \sin\theta(t) dt = 0 
 \exists t_0 \in [a,b] \ s.t. v(t_0)=(1,0) \exists t_0 \in [a,b] \ s.t. \theta(t_0) = 2\pi n    n\in \mathbb{Z} r(t)","['ordinary-differential-equations', 'differential-geometry', 'integral-equations']"
1,Finding eigenvalues of a matrix to show that the forward Euler discretization for the method of lines is stable,Finding eigenvalues of a matrix to show that the forward Euler discretization for the method of lines is stable,,"So here's the background on the problem at large, it won't be useful I think for what I'm asking help on since the formulation has nothing to do with finding eigenvalues. But it might be useful in giving clarity to the situation. So consider applying the method of lines to the heat equation $u_t=au_{xx}$ with initial conditions $u(0,x)=g(x),\, 0\leq x\leq 1$ and boundary conditions $u(t,0)=u(t,1)=0,\, t\geq 0$ . For simplicity, assume $\Delta x=\frac{1}{m+1}$ and let $y_i(t)$ approximate $u(x_i,t)$ where $x_i=i\Delta x,\, i=0,1,...,m+1$ . Then using second-order difference scheme to approximate the heat equation we arrive at $\frac{dy_i}{dt}=\frac{a(y_{i+1}-2y_i+y_{i-1})}{(\Delta x)^2},\, i=1,...,m$ with $y_0(t)=0$ and $y_{m+1}(t)=0$ . So now we get a system of ODE's with initial data $\vec{y}'=A\vec{y}$ where $A=\frac{a}{(\Delta x)^2}\begin{bmatrix} -2&1&0&.&.&0 \\ 1&-2&1&.&.&.\\ 0&1&-2&1&.&.\\.&.&.&.&.&. \\.&.&.&1&-2&1\\0&.&.&.&1&-2  \end{bmatrix}$ (Sorry if my matrix looks half-assed, but this is supposed to represent a symmetric block diagonal matrix) FYI: The component form for $\vec{y}'$ and $\vec{y}$ are trivial from numerical scheme. Now, for this next part we do a bit hand wavy since the focus isn't so much solving ODE's or PDE's the natural way like using separation of variables; we are focusing more on the numerical methods for them, so we suppose we are given the eigenfunctions beforehand, and they are, $\vec{v}^k=(v_1^k,v_2^k,...,v_m^k)^T=(\sin(k\pi\Delta x),\sin(2k\pi\Delta x),...,\sin(mk\pi\Delta x))^T,\, 1\leq k\leq m$ , or in simplified form $v_i^k=\sin(ik\pi\Delta x),\, i=1,...,m$ Now to my first question, how do I derive the eigenvalues $\lambda_k$ without making a huge mess making busy calculation? cause if it does involve that then I'd get on it and won't need your help on this part, but providing a more simple way of doing this that involves ideas discussed in numerical schemes for these equations is what I should be looking to do. But lets say we carried out this brute force calculation of the eigenvalues the natural way, wouldn't this lead to different eigenvalues for each row since they all have different trig functions? I probably could've asked this question without much of the PDE and numerical method jargon used to derive the matrix, but I wanted to show that I actually know what I'm trying to do. If you can provide me with the eigenvalue that would be all that is needed, I can figure out how to get there from what I have given you so far, unless it is super trivial and is a one-line calculation. Now, to my second question, how would I use the eigenvalues of A to show that the forward Euler discretization of the method of lines is stable if $h<\frac{1}{2a}(\Delta x)^2$ Why would we need a stability restriction for the time step for the forward Euler but not the backward Euler(Crank-Nicolson method)? Someone asked this similar problem but solutions were insufficient and didn't provide enough clarity of the concepts being discussed Method of Lines Diffusion Problem","So here's the background on the problem at large, it won't be useful I think for what I'm asking help on since the formulation has nothing to do with finding eigenvalues. But it might be useful in giving clarity to the situation. So consider applying the method of lines to the heat equation with initial conditions and boundary conditions . For simplicity, assume and let approximate where . Then using second-order difference scheme to approximate the heat equation we arrive at with and . So now we get a system of ODE's with initial data where (Sorry if my matrix looks half-assed, but this is supposed to represent a symmetric block diagonal matrix) FYI: The component form for and are trivial from numerical scheme. Now, for this next part we do a bit hand wavy since the focus isn't so much solving ODE's or PDE's the natural way like using separation of variables; we are focusing more on the numerical methods for them, so we suppose we are given the eigenfunctions beforehand, and they are, , or in simplified form Now to my first question, how do I derive the eigenvalues without making a huge mess making busy calculation? cause if it does involve that then I'd get on it and won't need your help on this part, but providing a more simple way of doing this that involves ideas discussed in numerical schemes for these equations is what I should be looking to do. But lets say we carried out this brute force calculation of the eigenvalues the natural way, wouldn't this lead to different eigenvalues for each row since they all have different trig functions? I probably could've asked this question without much of the PDE and numerical method jargon used to derive the matrix, but I wanted to show that I actually know what I'm trying to do. If you can provide me with the eigenvalue that would be all that is needed, I can figure out how to get there from what I have given you so far, unless it is super trivial and is a one-line calculation. Now, to my second question, how would I use the eigenvalues of A to show that the forward Euler discretization of the method of lines is stable if Why would we need a stability restriction for the time step for the forward Euler but not the backward Euler(Crank-Nicolson method)? Someone asked this similar problem but solutions were insufficient and didn't provide enough clarity of the concepts being discussed Method of Lines Diffusion Problem","u_t=au_{xx} u(0,x)=g(x),\, 0\leq x\leq 1 u(t,0)=u(t,1)=0,\, t\geq 0 \Delta x=\frac{1}{m+1} y_i(t) u(x_i,t) x_i=i\Delta x,\, i=0,1,...,m+1 \frac{dy_i}{dt}=\frac{a(y_{i+1}-2y_i+y_{i-1})}{(\Delta x)^2},\, i=1,...,m y_0(t)=0 y_{m+1}(t)=0 \vec{y}'=A\vec{y} A=\frac{a}{(\Delta x)^2}\begin{bmatrix} -2&1&0&.&.&0 \\ 1&-2&1&.&.&.\\ 0&1&-2&1&.&.\\.&.&.&.&.&. \\.&.&.&1&-2&1\\0&.&.&.&1&-2  \end{bmatrix} \vec{y}' \vec{y} \vec{v}^k=(v_1^k,v_2^k,...,v_m^k)^T=(\sin(k\pi\Delta x),\sin(2k\pi\Delta x),...,\sin(mk\pi\Delta x))^T,\, 1\leq k\leq m v_i^k=\sin(ik\pi\Delta x),\, i=1,...,m \lambda_k h<\frac{1}{2a}(\Delta x)^2","['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'numerical-linear-algebra', 'numerical-optimization']"
2,How to find a general solution as 2nd order linear differential equation is given with zero value of RHS and only one particular solution is known,How to find a general solution as 2nd order linear differential equation is given with zero value of RHS and only one particular solution is known,,"The below equation is of 2nd order linear differential equation. $$  \frac{d^{2}y}{dx^2}-\frac{  \left( 2x-6 \right)   }{  \left( x-2 \right)   } \frac{dy}{dx} + \frac{  \left( x-4 \right)   }{  \left( x-2 \right)   } y=0 \tag{1}  $$ $$  \frac{  e^{ x }   }{  x-2  }~\text{is known as one of the solution(s) of this ODE}  $$ I want to derive a general solution $~  y\left(x\right)  ~$ of eqn1 . I've read the book to derive it however seemingly the book only handles cases of ODE as RHS of 2nd order linear differntial equation is nonzero or as 2 particular solutions are given in a problem statement. So, this case of eqn1, RHS of the equation is $~ 0 ~$ and only one particular solution is given. Which method can I use here to find out the general solution? Can anyone tell me it?","The below equation is of 2nd order linear differential equation. I want to derive a general solution of eqn1 . I've read the book to derive it however seemingly the book only handles cases of ODE as RHS of 2nd order linear differntial equation is nonzero or as 2 particular solutions are given in a problem statement. So, this case of eqn1, RHS of the equation is and only one particular solution is given. Which method can I use here to find out the general solution? Can anyone tell me it?",  \frac{d^{2}y}{dx^2}-\frac{  \left( 2x-6 \right)   }{  \left( x-2 \right)   } \frac{dy}{dx} + \frac{  \left( x-4 \right)   }{  \left( x-2 \right)   } y=0 \tag{1}     \frac{  e^{ x }   }{  x-2  }~\text{is known as one of the solution(s) of this ODE}   ~  y\left(x\right)  ~ ~ 0 ~,['ordinary-differential-equations']
3,Find a general solution to $xy' = y^2+y$,Find a general solution to,xy' = y^2+y,"I'm trying to find the general solution to $xy' = y^2+y$ , although I'm unsure as to whether I'm approaching this correctly. What I have tried: dividing both sides by x and substituting $u = y/x$ I get: $$y' = u^2x^2+u$$ Then substituting $y' = u'x + u$ I get the following: $$u'x+u = u^2x^2+u \implies u' = u^2x \implies \int\frac{du}{u^2}=\int x dx$$ Proceeding on with simplification after integration: $$\frac{1}{u}=\frac{x^2}{2}+c\implies y = \frac{2x}{x^2+c}$$ However, the answer shows $y=\frac{x}{(c-x)}$","I'm trying to find the general solution to , although I'm unsure as to whether I'm approaching this correctly. What I have tried: dividing both sides by x and substituting I get: Then substituting I get the following: Proceeding on with simplification after integration: However, the answer shows",xy' = y^2+y u = y/x y' = u^2x^2+u y' = u'x + u u'x+u = u^2x^2+u \implies u' = u^2x \implies \int\frac{du}{u^2}=\int x dx \frac{1}{u}=\frac{x^2}{2}+c\implies y = \frac{2x}{x^2+c} y=\frac{x}{(c-x)},['ordinary-differential-equations']
4,What is the meaning of entire solution in LaSalle's invariance principle and how to apply this principle?,What is the meaning of entire solution in LaSalle's invariance principle and how to apply this principle?,,"This is from Hirsch/Smale/Devaney's textbook ""Differential Equations and Introduction to Chaos"" Page 199 Theorem. (Lasalle’s Invariance Principle) Let $X^∗$ be an equilibrium point for $X^\prime = F(X)$ and let $L : U  → R$ be a Liapunov function for $X^∗$ , where $U$ is an open set containing $X^∗$ . Let $P ⊂ U$ be a neighborhood of $X^∗$ that is closed. Suppose that $P$ is positively invariant and that there is no entire solution in $P − X^∗$ on which $L$ is constant. Then $X^∗$ is asymptotically stable, and $P$ is contained in the basin of attraction of $X^∗$ I am struggling with the meaning of ""no entire solution on which $L$ is constant"" part. It seems to be a terminology that is very specific to this theorem and found nowhere else (even in other books). I don't intuitively understand why this is required. For example, consider the famous Pendulum example , $$\dot x_1 = x_2$$ $$\dot x_2 = -a\sin(x_1) - bx_2$$ We wish to look at the stability of the equilibrium at $0$ . They showed that you can have a Liapunov function, $L(x) = a(1-\cos(x_1)) + x_2^2/2$ and proved $\dot L(x) = 0 \implies x_2 = 0, x_1 \in [0, 2\pi]$ (which is a segment of the $x$ -axis) Then if we are to apply this theorem, we can pick $U$ to be the entire real line, and $P$ be the line-segment $[0, 2\pi]$ . Since $\dot L = 0$ on $P$ and $P$ is a line-segment, therefore $P$ is closed and positively invariant.  How do we proceed to prove the tricky statement there are no ""entire solution in $P \backslash \{0\}$ that on which $L$ is constant? To do this it seems we need to prove that since any $\{x_2 = 0, x_1 \in (0, 2\pi]\}$ could be a (constant) solution in $P \backslash \{0\}$ on which $L$ is for sure constant, therefore we need to eliminate all $\{x_2 = 0, x_1 \in (0, 2\pi]\}$ as being the solution of this system. Is this right?","This is from Hirsch/Smale/Devaney's textbook ""Differential Equations and Introduction to Chaos"" Page 199 Theorem. (Lasalle’s Invariance Principle) Let be an equilibrium point for and let be a Liapunov function for , where is an open set containing . Let be a neighborhood of that is closed. Suppose that is positively invariant and that there is no entire solution in on which is constant. Then is asymptotically stable, and is contained in the basin of attraction of I am struggling with the meaning of ""no entire solution on which is constant"" part. It seems to be a terminology that is very specific to this theorem and found nowhere else (even in other books). I don't intuitively understand why this is required. For example, consider the famous Pendulum example , We wish to look at the stability of the equilibrium at . They showed that you can have a Liapunov function, and proved (which is a segment of the -axis) Then if we are to apply this theorem, we can pick to be the entire real line, and be the line-segment . Since on and is a line-segment, therefore is closed and positively invariant.  How do we proceed to prove the tricky statement there are no ""entire solution in that on which is constant? To do this it seems we need to prove that since any could be a (constant) solution in on which is for sure constant, therefore we need to eliminate all as being the solution of this system. Is this right?","X^∗ X^\prime = F(X) L : U
 → R X^∗ U X^∗ P ⊂ U X^∗ P P − X^∗ L X^∗ P X^∗ L \dot x_1 = x_2 \dot x_2 = -a\sin(x_1) - bx_2 0 L(x) = a(1-\cos(x_1)) + x_2^2/2 \dot L(x) = 0 \implies x_2 = 0, x_1 \in [0, 2\pi] x U P [0, 2\pi] \dot L = 0 P P P P \backslash \{0\} L \{x_2 = 0, x_1 \in (0, 2\pi]\} P \backslash \{0\} L \{x_2 = 0, x_1 \in (0, 2\pi]\}","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'lyapunov-functions']"
5,Euler-Cauchy equation boundary conditions problem,Euler-Cauchy equation boundary conditions problem,,"ODE: $$xy''+2y'+ax=0$$ BCs: $$y(0)=\alpha$$ $$y'(L)=0$$ ODE solution: $$y(x)=-\frac{ax^2}{6}+\frac{c_1}{x}+c_2$$ $$\lim_{x\to0}y(x)=\infty\Rightarrow c_1=0$$ $$y'(x)=-\frac{ax}{3}$$ With $c_2$ dropping out, it cannot be determined from the second BC? And yet we can determine $c_2$ from the 1st BC: $$y(0)=c_2=\alpha$$ So why does the second BC seem redundant?","ODE: BCs: ODE solution: With dropping out, it cannot be determined from the second BC? And yet we can determine from the 1st BC: So why does the second BC seem redundant?",xy''+2y'+ax=0 y(0)=\alpha y'(L)=0 y(x)=-\frac{ax^2}{6}+\frac{c_1}{x}+c_2 \lim_{x\to0}y(x)=\infty\Rightarrow c_1=0 y'(x)=-\frac{ax}{3} c_2 c_2 y(0)=c_2=\alpha,"['ordinary-differential-equations', 'boundary-value-problem']"
6,What functions satisfy $\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i} = 0?$,What functions satisfy,\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i} = 0?,"Consider the equation, $\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i} = 0$ for $n\geq 1$ . By trial and error, I see that $f(x_1,\cdots, x_n)=\sum_{i=1}^{n-1}\frac{x_i}{x_{i+1}}$ works because, $$\frac{\partial f}{\partial x_i}=-\frac{x_{i-1}}{x_i^2}+\frac{1}{x_{i+1}}, \text{ where }x_0=0,x_{n+1}=+\infty.$$ Then $\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i}= \sum_{i=1}^{n}-\frac{x_{i-1}}{x_i}+\frac{x_i}{x_{i+1}}=0.$ But is there a general form of solutions perhaps in higher dimensions say $n\geq 3.$ I guess that when $n=2$ , we have $f(x_1, x_2) = f\left(\frac{x_1}{x_2}\right),$ but not sure how to express solutions in higher dimensions. Thus any suggestions/remarks will be much appreciated. Edit : In general, I want to find functions $f$ such that $x\cdot \nabla f=0$ and $v\cdot \nabla f =0$ where $x=(x_1,\cdots, x_n)$ and $v = (v_1,\cdots, v_n)$ is some constant vector in $\mathbb{R}^n.$","Consider the equation, for . By trial and error, I see that works because, Then But is there a general form of solutions perhaps in higher dimensions say I guess that when , we have but not sure how to express solutions in higher dimensions. Thus any suggestions/remarks will be much appreciated. Edit : In general, I want to find functions such that and where and is some constant vector in","\sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i} = 0 n\geq 1 f(x_1,\cdots, x_n)=\sum_{i=1}^{n-1}\frac{x_i}{x_{i+1}} \frac{\partial f}{\partial x_i}=-\frac{x_{i-1}}{x_i^2}+\frac{1}{x_{i+1}}, \text{ where }x_0=0,x_{n+1}=+\infty. \sum_{i=1}^{n} x_i \frac{\partial f}{\partial x_i}= \sum_{i=1}^{n}-\frac{x_{i-1}}{x_i}+\frac{x_i}{x_{i+1}}=0. n\geq 3. n=2 f(x_1, x_2) = f\left(\frac{x_1}{x_2}\right), f x\cdot \nabla f=0 v\cdot \nabla f =0 x=(x_1,\cdots, x_n) v = (v_1,\cdots, v_n) \mathbb{R}^n.","['real-analysis', 'calculus', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
7,Properties of First Order Linear ODE as it relates to the solution,Properties of First Order Linear ODE as it relates to the solution,,"I am given the following image, and asked which of the following forms of ODE's could this solution match to (vertical asymptote at $t = 8$ ). $(1) \ y' + p(t)y=g(t)$ or $ (2) \ y' = f(y)$ or $ (3) \ y' = f(y)g(t)$ . From the graph, I can see that the slope varies with $t$ hence it can not be autonomous like in $(2)$ . However, what justifications can I make to decide whether this can be a solution to $(1)$ or $(3)$ ?","I am given the following image, and asked which of the following forms of ODE's could this solution match to (vertical asymptote at ). or or . From the graph, I can see that the slope varies with hence it can not be autonomous like in . However, what justifications can I make to decide whether this can be a solution to or ?",t = 8 (1) \ y' + p(t)y=g(t)  (2) \ y' = f(y)  (3) \ y' = f(y)g(t) t (2) (1) (3),['ordinary-differential-equations']
8,Solutions to an ODE using Frobenius when roots are repeated,Solutions to an ODE using Frobenius when roots are repeated,,"I have been looking at the following differential equation $$xy^{\prime \prime} + y^{\prime} + xy=0$$ Observe that since $x=0$ is a regular singular point we use the Frobenius method and substitute $y=\sum_{n=0}^{\infty}a_n x^{n+r}$ to get $$r^2a_0x^{r-1} + (r+1)^2a_1x^r + \sum_{n=2}^{\infty}[(n+r)^2a_n+a_{n-2}]x^{n+r-1}=0$$ Since $a_0 \neq 0$ we get $r=0,0.$ giving $y_1 = a_0(1-\dfrac{x^2}{2^2}+\dfrac {x^4}{4^2\cdot2^2}-\dfrac{x^6}{6^2\cdot4^2\cdot2^2}......)$ . Now apparently to obtain the second solution I have to differentiate this solution w.r.t. $r$ and then evaluate this at $r=0$ , but I really don't see where this comes from. Could anyone show me explicitly why exactly we do this?  Or more generally - If the roots of the indicial equation $r_1$ and $r_2$ are repeated,  why is the second solution of the form $$y_2= c\left(\frac {\partial y_1}{\partial r}\right)_{\large {r=r_1}}?$$ All the sources I've looked at just state this fact without really showing where this came from, so either I'm missing something really obvious or there is some other explanation.","I have been looking at the following differential equation Observe that since is a regular singular point we use the Frobenius method and substitute to get Since we get giving . Now apparently to obtain the second solution I have to differentiate this solution w.r.t. and then evaluate this at , but I really don't see where this comes from. Could anyone show me explicitly why exactly we do this?  Or more generally - If the roots of the indicial equation and are repeated,  why is the second solution of the form All the sources I've looked at just state this fact without really showing where this came from, so either I'm missing something really obvious or there is some other explanation.","xy^{\prime \prime} + y^{\prime} + xy=0 x=0 y=\sum_{n=0}^{\infty}a_n x^{n+r} r^2a_0x^{r-1} + (r+1)^2a_1x^r + \sum_{n=2}^{\infty}[(n+r)^2a_n+a_{n-2}]x^{n+r-1}=0 a_0 \neq 0 r=0,0. y_1 = a_0(1-\dfrac{x^2}{2^2}+\dfrac {x^4}{4^2\cdot2^2}-\dfrac{x^6}{6^2\cdot4^2\cdot2^2}......) r r=0 r_1 r_2 y_2= c\left(\frac {\partial y_1}{\partial r}\right)_{\large {r=r_1}}?","['ordinary-differential-equations', 'power-series', 'frobenius-method']"
9,Eigenfunction differential equation with boundary values,Eigenfunction differential equation with boundary values,,"Consider the differential equation $$f'' + 2f' + (\lambda + 1)f = 0, \ \ \ \  f(0) + f'(0) = 0, f(L) = 0.$$ We can make $g(x) = e^x f(x)$ , so our differential equation becomes $$g'' + \lambda g = 0.$$ Equations like these have nice closed-form solutions. However, the boundary conditions are confusing me. Now we have $$g(0) + g'(0) = f(0), \ \ \ \ g(L) = 0.$$ Since we don't know what $f(0)$ , I don't understand how we can possibly solve this. Does anyone know how to solve this?","Consider the differential equation We can make , so our differential equation becomes Equations like these have nice closed-form solutions. However, the boundary conditions are confusing me. Now we have Since we don't know what , I don't understand how we can possibly solve this. Does anyone know how to solve this?","f'' + 2f' + (\lambda + 1)f = 0, \ \ \ \  f(0) + f'(0) = 0, f(L) = 0. g(x) = e^x f(x) g'' + \lambda g = 0. g(0) + g'(0) = f(0), \ \ \ \ g(L) = 0. f(0)",['ordinary-differential-equations']
10,Showing a solution to 3rd order differential equations forms a subspace,Showing a solution to 3rd order differential equations forms a subspace,,"Let $S$ denote the set of all solutions of the following differential equation defined on $C^3[0,\infty)$ ; $$ \begin{align}   \frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx = 0 \end{align} $$ Show that $S$ is a linear subspace of $C^3[0, \infty)$ I know that subspaces are closed under addition and scalar multiplication and also contan the zero vector, but I'm not sure how to show that all solutions form a linear subspace. (I'm not sure how to solve this equation either) Final Answer Note that: $$ \begin{align}    0 &= \frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx \\      &= \alpha \left(\frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx\right) \end{align} $$ Where $\alpha$ is a constant. Consider arbitrary functions $f$ and $g$ and scalar $\alpha$ . By linearity of operators we have: $$ \begin{align} \frac{d^3}{dt^3}(\alpha f + g ) &= \alpha \frac{d^3f}{dt^3} + \frac{d^3g}{dt^3} \\ \frac{d^2}{dt^2}(\alpha f + g ) &= \alpha \frac{d^2f}{dt^2} + \frac{d^2g}{dt^2} \\ \frac{d}{dt}(\alpha f + g ) &= \alpha \frac{df}{dt} + \frac{dg}{dt} \\ d(\alpha f + g) &= \alpha df + dg \end{align} $$ Now assign $f$ and $g$ to arbitrary individual solutions to our differential equation, we have: $$ \begin{align}   0 &= \frac{d^3}{dt^3}(\alpha f + g) + b \frac{d^2}{dt^2}(\alpha f + g) + c \frac{d}{dt}(\alpha f + g) + d(\alpha f + g) \\   &= \alpha(\frac{d^3}{dt^3}f + b \frac{d^2}{dt^2}f + c \frac{d}{dt}f + df ) + (\frac{d^3}{dt^3}g + b \frac{d^2}{dt^2}g + c \frac{d}{dt}g + dg) \\   &= \alpha \pmb f + \pmb g  \end{align} $$ This shows that $\alpha \pmb f + \pmb g$ solves the ODE which proves that $\alpha \pmb f + \pmb g \in \mathcal S$ The addition holds because the sum that describes $\pmb f$ sums to $0$ . Likewise for $\pmb g$ . Therefore, all arbitrary combinations of $\pmb f$ and $\pmb g$ are in $\mathcal S$ (This can be seen by also allowing $\pmb f$ be $\pmb g$ and $\pmb g$ be $\pmb f$ ). Finally, $\pmb 0$ is also in the set of solutions. Thus, the set of all solutions forms a subspace.","Let denote the set of all solutions of the following differential equation defined on ; Show that is a linear subspace of I know that subspaces are closed under addition and scalar multiplication and also contan the zero vector, but I'm not sure how to show that all solutions form a linear subspace. (I'm not sure how to solve this equation either) Final Answer Note that: Where is a constant. Consider arbitrary functions and and scalar . By linearity of operators we have: Now assign and to arbitrary individual solutions to our differential equation, we have: This shows that solves the ODE which proves that The addition holds because the sum that describes sums to . Likewise for . Therefore, all arbitrary combinations of and are in (This can be seen by also allowing be and be ). Finally, is also in the set of solutions. Thus, the set of all solutions forms a subspace.","S C^3[0,\infty) 
\begin{align}
  \frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx = 0
\end{align}
 S C^3[0, \infty) 
\begin{align}
   0 &= \frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx \\
     &= \alpha \left(\frac{d^3x}{dt^3} + b \frac{d^2x}{dt^2} + c \frac{dx}{dt} + dx\right)
\end{align}
 \alpha f g \alpha 
\begin{align}
\frac{d^3}{dt^3}(\alpha f + g ) &= \alpha \frac{d^3f}{dt^3} + \frac{d^3g}{dt^3} \\
\frac{d^2}{dt^2}(\alpha f + g ) &= \alpha \frac{d^2f}{dt^2} + \frac{d^2g}{dt^2} \\
\frac{d}{dt}(\alpha f + g ) &= \alpha \frac{df}{dt} + \frac{dg}{dt} \\
d(\alpha f + g) &= \alpha df + dg
\end{align}
 f g 
\begin{align}
  0 &= \frac{d^3}{dt^3}(\alpha f + g) + b \frac{d^2}{dt^2}(\alpha f + g) + c \frac{d}{dt}(\alpha f + g) + d(\alpha f + g) \\
  &= \alpha(\frac{d^3}{dt^3}f + b \frac{d^2}{dt^2}f + c \frac{d}{dt}f + df ) + (\frac{d^3}{dt^3}g + b \frac{d^2}{dt^2}g + c \frac{d}{dt}g + dg) \\
  &= \alpha \pmb f + \pmb g 
\end{align}
 \alpha \pmb f + \pmb g \alpha \pmb f + \pmb g \in \mathcal S \pmb f 0 \pmb g \pmb f \pmb g \mathcal S \pmb f \pmb g \pmb g \pmb f \pmb 0","['ordinary-differential-equations', 'vector-spaces']"
11,Solving differential equation featuring $f(1)$ as a coefficient,Solving differential equation featuring  as a coefficient,f(1),"Starting with $g(x)=\int_{1/x}^1 g(kx)(2-2k)dk+1,$ after integration by parts of the RHS I get the following DE for the second antiderivative of $g(x),$ say $f(x)$ : $x^2f''(x)+(2x-2)f'(1)-2(f(x)-f(1))-1=0.$ I'm not really sure how to approach this - for instance, if I pretend $f(1)$ and $f'(1)$ are merely constants, I can spot the differential operator $x^2f''(x)-2f(x)$ has eigenfunctions $1/x$ and $x^2$ . However, the usual plan of using these homogeneous solutions to find the general solution seems not to work, as my homogeneous solutions (and their first derivatives) don't vanish at $x=1$ . Is there perhaps a better way of attacking the original integral equation? It's also worth mentioning I have a BC - namely $g(1)=1.$","Starting with after integration by parts of the RHS I get the following DE for the second antiderivative of say : I'm not really sure how to approach this - for instance, if I pretend and are merely constants, I can spot the differential operator has eigenfunctions and . However, the usual plan of using these homogeneous solutions to find the general solution seems not to work, as my homogeneous solutions (and their first derivatives) don't vanish at . Is there perhaps a better way of attacking the original integral equation? It's also worth mentioning I have a BC - namely","g(x)=\int_{1/x}^1 g(kx)(2-2k)dk+1, g(x), f(x) x^2f''(x)+(2x-2)f'(1)-2(f(x)-f(1))-1=0. f(1) f'(1) x^2f''(x)-2f(x) 1/x x^2 x=1 g(1)=1.","['ordinary-differential-equations', 'integro-differential-equations']"
12,Symmetry of mixed partial derivatives.,Symmetry of mixed partial derivatives.,,"I have come across two theorems, which I think are essentially trying to say the same thing: 1) The mixed derivative theorem: If $f(x, y)$ and its partial derivatives $f_x$ , $f_y$ , $f_{xy}$ and $f_{yx}$ are defined in a neighborhood of $(x_0, y_0)$ and all are continuous at $(x_0, y_0)$ , then : $$f_{xy}(x_0, y_0) = f_{yx}(x_0, y_0).$$ 2) Clairaut's theorem: Suppose that $f$ is defined on a disk $D$ that contains the point $(a,b)$ . If the functions $f_{xy}$ and $f_{yx}$ are continuous on this disk then: $$f_{xy}(a,b) = f_{yx}(a ,b).$$ Why are there two separate theorems for conveying the same thing? Is it that the second one is an improved version of the first one? It seems that the mixed derivative theorem lists some redundant conditions, because existence of the second order partial derivatives would imply the continuity of $f_x$ and $f_y$ . Is it wrong to conclude this?","I have come across two theorems, which I think are essentially trying to say the same thing: 1) The mixed derivative theorem: If and its partial derivatives , , and are defined in a neighborhood of and all are continuous at , then : 2) Clairaut's theorem: Suppose that is defined on a disk that contains the point . If the functions and are continuous on this disk then: Why are there two separate theorems for conveying the same thing? Is it that the second one is an improved version of the first one? It seems that the mixed derivative theorem lists some redundant conditions, because existence of the second order partial derivatives would imply the continuity of and . Is it wrong to conclude this?","f(x, y) f_x f_y f_{xy} f_{yx} (x_0, y_0) (x_0, y_0) f_{xy}(x_0, y_0) = f_{yx}(x_0, y_0). f D (a,b) f_{xy} f_{yx} f_{xy}(a,b) = f_{yx}(a ,b). f_x f_y","['calculus', 'ordinary-differential-equations', 'derivatives']"
13,"Solving a second order ODE of the form $y'' + P(x)y' + Q(x)y = f(x)$, given one part of the solution","Solving a second order ODE of the form , given one part of the solution",y'' + P(x)y' + Q(x)y = f(x),"I have the following ODE $$x(x-1)y'' - (2x-1)y' + 2y = 2x^3-3x^2$$ where I'm given $$y_1 = x^2$$ Now, my workbook states that if we encounter such an ODE, where one part of the solution is given, the other part $y_2$ is $$y_2 = y_1 \int \frac{e^{-\int P(x)dx}}{y_1^2}dx$$ After I divide my equation by $x(x-1)$ and substituting into the formula for $y_2$ , I get $$y_2 = x^2\int\frac{(x-1)x}{x^4}$$ $$y_2 = \frac{1-2x}{2}$$ Now, $$y=c_1x^2 + c_2\frac{1-2x}{2}$$ Which is the solution of the homogenous equation. In order to find the particular solution, I used variation of constants, where I got the system: $$ \left\{  \begin{array}{c} c_1'x^2 + c_2'\frac{1-2x}{2} = 0 \\ c_1' 2x - c_2' = \frac{2x^2 - 3x}{x-1} \end{array} \right.  $$ The solution for which are $c_1' = \frac{(2x-3)(1-2x)}{x-1}$ and $c_2' = \frac{x^3(2x-3)}{x(x-1)^2}$ Which after integrating end up being $$c_1 = \ln|x-1| - 2(x-1)^2 + C_1$$ and $$c_2 = \frac{x^3}{x-1} + C_2$$ After plugging in into the solution, I get $$y = x^2 ( \ln|x-1| - 2(x-1)^2 + C_1) + \frac{1-2x}{2}(\frac{x^3}{x-1}+C_2)$$ However, this is not the same as the solution in my workbook which is $$y=C_1x^2 + C_2(-x+\frac{1}{2}) + x^3 - \frac{x^2}{2} + x-\frac{1}{2}$$ Where did I go wrong?","I have the following ODE where I'm given Now, my workbook states that if we encounter such an ODE, where one part of the solution is given, the other part is After I divide my equation by and substituting into the formula for , I get Now, Which is the solution of the homogenous equation. In order to find the particular solution, I used variation of constants, where I got the system: The solution for which are and Which after integrating end up being and After plugging in into the solution, I get However, this is not the same as the solution in my workbook which is Where did I go wrong?","x(x-1)y'' - (2x-1)y' + 2y = 2x^3-3x^2 y_1 = x^2 y_2 y_2 = y_1 \int \frac{e^{-\int P(x)dx}}{y_1^2}dx x(x-1) y_2 y_2 = x^2\int\frac{(x-1)x}{x^4} y_2 = \frac{1-2x}{2} y=c_1x^2 + c_2\frac{1-2x}{2} 
\left\{ 
\begin{array}{c}
c_1'x^2 + c_2'\frac{1-2x}{2} = 0 \\
c_1' 2x - c_2' = \frac{2x^2 - 3x}{x-1}
\end{array}
\right. 
 c_1' = \frac{(2x-3)(1-2x)}{x-1} c_2' = \frac{x^3(2x-3)}{x(x-1)^2} c_1 = \ln|x-1| - 2(x-1)^2 + C_1 c_2 = \frac{x^3}{x-1} + C_2 y = x^2 ( \ln|x-1| - 2(x-1)^2 + C_1) + \frac{1-2x}{2}(\frac{x^3}{x-1}+C_2) y=C_1x^2 + C_2(-x+\frac{1}{2}) + x^3 - \frac{x^2}{2} + x-\frac{1}{2}",['ordinary-differential-equations']
14,Calculate the general solution and the specific solution of the following differential equation: $(3y - 1)^2) (y'^2) = 4y$,Calculate the general solution and the specific solution of the following differential equation:,(3y - 1)^2) (y'^2) = 4y,Calculate the general solution and the specific solution of the following differential equation: $(3y - 1)^2  (y')^2 = 4y.$ The general solution is $(x + C)^2 = y(y - 1)^2$ and the specific is $y = 0$ . I have tried rewriting it like $y' = \dfrac{2y}{3y + 1}$ but I don't know what to do next.,Calculate the general solution and the specific solution of the following differential equation: The general solution is and the specific is . I have tried rewriting it like but I don't know what to do next.,(3y - 1)^2  (y')^2 = 4y. (x + C)^2 = y(y - 1)^2 y = 0 y' = \dfrac{2y}{3y + 1},['ordinary-differential-equations']
15,Solving an ODE - exact one?,Solving an ODE - exact one?,,"Given the following ODE, $$y' \arcsin(y)-x\sqrt{1-y^2} \arcsin ^2 (y) = 2x\sqrt{1-y^2}. $$ I guess that using some algebraic manipulations, I need to transform the equation into something in which I will be able to use the fact that $(\arcsin (x) ) ' = \frac{1}{\sqrt{1-x^2}}$ , but didn't manage to understand how exactly. I did manage to obtain the following form, in which I cannot see how to integrate the LHS, $$ \frac{\arcsin(y)}{\sqrt{1-y^2}}\cdot \frac{1}{2+\arcsin ^2 (y) } dy = x dx. $$ Thank you!","Given the following ODE, I guess that using some algebraic manipulations, I need to transform the equation into something in which I will be able to use the fact that , but didn't manage to understand how exactly. I did manage to obtain the following form, in which I cannot see how to integrate the LHS, Thank you!","y' \arcsin(y)-x\sqrt{1-y^2} \arcsin ^2 (y) = 2x\sqrt{1-y^2}.  (\arcsin (x) ) ' = \frac{1}{\sqrt{1-x^2}} 
\frac{\arcsin(y)}{\sqrt{1-y^2}}\cdot \frac{1}{2+\arcsin ^2 (y) } dy = x dx.
",['ordinary-differential-equations']
16,Solving the Riccati equation $y'=ay^2+f(x)y+b$,Solving the Riccati equation,y'=ay^2+f(x)y+b,"I would like to know if someone knows a method to solve the following Riccati equation $$y'=ay^2+f(x)y+b$$ where of course $y'=\frac{dy}{dx}$ , $f$ is a continuous function and $a,b$ are constants. I know that if $f=C$ with $C$ a constant the equation is a separable equation, but I don't know what happen in the general case. If someone could give some advice, I will really appreciate it.","I would like to know if someone knows a method to solve the following Riccati equation where of course , is a continuous function and are constants. I know that if with a constant the equation is a separable equation, but I don't know what happen in the general case. If someone could give some advice, I will really appreciate it.","y'=ay^2+f(x)y+b y'=\frac{dy}{dx} f a,b f=C C",['ordinary-differential-equations']
17,Solution of differential equation $(x-3)^2 y'' + (x-1)y' + y = 0$,Solution of differential equation,(x-3)^2 y'' + (x-1)y' + y = 0,"How do you solve the following differential equation? \begin{equation} (x-3)^2 y'' + (x - 1)y' + y = 0, x \ne 3 \end{equation}",How do you solve the following differential equation?,"\begin{equation}
(x-3)^2 y'' + (x - 1)y' + y = 0, x \ne 3
\end{equation}",['ordinary-differential-equations']
18,Solve $(3x+2)y''+7y'=0$,Solve,(3x+2)y''+7y'=0,"Solve $(3x+2)y''+7y'=0$ My first idea was to reduce into an euler-cauchy equation with $z = y' \rightarrow (3x+2)z' + 7z =0$ Let $e^t = (3x+2)$ then $z' = 3e^{-t}\frac{dz}{dt}$ . The previous equation can be written as : $$e^t\cdot3e^{-t} \frac{dz}{dt} + 7z = 0 \rightarrow 3z' + 7z=0$$ Now this is a separable differential equation: $z=C_2e^{\frac{-7}{3}t} + C_1$ . Now $t = \ln(3x+2)$ $$z = C_2e^{\frac{-7}{3}\ln(3x+2)} + C_1=C_2(3x+2)^{\frac{-7}{3}}+C_1$$ Since $z = y'$ we get another equation $y' = C_2(3x+2)^{\frac{-7}{3}}+C_1 \rightarrow y= C_3(3x+2)^{-4/3} + C_2x + C_4$ Thus, since we only need one solution $C_4 = 0$ , and re-writing we can get: $$y = C_1(3x+2)^{-4/3} + C_2x$$ Is this a valid justification?","Solve My first idea was to reduce into an euler-cauchy equation with Let then . The previous equation can be written as : Now this is a separable differential equation: . Now Since we get another equation Thus, since we only need one solution , and re-writing we can get: Is this a valid justification?",(3x+2)y''+7y'=0 z = y' \rightarrow (3x+2)z' + 7z =0 e^t = (3x+2) z' = 3e^{-t}\frac{dz}{dt} e^t\cdot3e^{-t} \frac{dz}{dt} + 7z = 0 \rightarrow 3z' + 7z=0 z=C_2e^{\frac{-7}{3}t} + C_1 t = \ln(3x+2) z = C_2e^{\frac{-7}{3}\ln(3x+2)} + C_1=C_2(3x+2)^{\frac{-7}{3}}+C_1 z = y' y' = C_2(3x+2)^{\frac{-7}{3}}+C_1 \rightarrow y= C_3(3x+2)^{-4/3} + C_2x + C_4 C_4 = 0 y = C_1(3x+2)^{-4/3} + C_2x,"['ordinary-differential-equations', 'solution-verification']"
19,Is taking the partial derivative on both sides of an equation a valid way of solving it?,Is taking the partial derivative on both sides of an equation a valid way of solving it?,,"Given $h(t),$ and $p(r,t)$ are both functions independent of a variable $z.$ Is it valid to solve the following equation by differentiating both sides with respect to $z$ twice: $\left[z^2-zh(t)\right]\left[\frac{\partial^2 p(r,t)}{\partial r^2}+\frac{\partial p(r,t)}{\partial r} \frac{1}{r}\right] + \frac{dh(t)}{dt}  \frac{1}{h(t)} = 0$ (1) My thought process was that by doing this I could end up with the following PDE: $\frac{\partial^2 p(r,t)}{\partial r^2}+\frac{\partial p(r,t)}{\partial r} \frac{1}{r} = 0$ (2) Update/Edit: I tried the technique in order to obtain the above differential equation (2) for the p(r,t) function. Next, I was given the following boundary conditions: $p(r_1,t) = 0$ $p(r_2,t) = 0$ Solving the PDE I got: $p(r,t) = e^{c_1(t)}*ln(r) + c_2(t)$ Solving using the boundary conditions I ended up with $c_2(t) = 0$ $c_1(0) = ln(0)$ which is very wrong... Am I solving this PDE wrong or is my initial method of obtaining the PDE flawed? Edit #2: Regarding the origins of this PDE (note everything is in cylindrical coordinates) $p(r,t)$ is a function representing pressure, where pressure doesn't depend on $\theta$ or $z$ $h(t)$ represents the height of a certain surface The geometries are irrelevant and thus a diagram is not necessary. It is also given that the components of the velocity field are $v_r(r,z,t) = \frac{1}{2 \mu} * \frac{\partial{p(r,t)}}{\partial r} * [ z^2 - zh(t)]$ $v_\theta = 0$ $v_z(z,t) = \frac{\partial{h(t)}}{\partial t} * \frac{z}{h(t)}$ I got the PDE by applying the continuity equation (a consequence of conservation of mass): $\nabla \cdot \vec{v} $ For an incompressible fluid and cylindrical coordinate that becomes: $ \frac{1}{r} \frac{\partial{(r v_r)}}{\partial r} + \frac{1}{r} \frac{\partial{(v_\theta)}}{\partial \theta} + \frac{\partial{(v_z)}}{\partial z} = 0$","Given and are both functions independent of a variable Is it valid to solve the following equation by differentiating both sides with respect to twice: (1) My thought process was that by doing this I could end up with the following PDE: (2) Update/Edit: I tried the technique in order to obtain the above differential equation (2) for the p(r,t) function. Next, I was given the following boundary conditions: Solving the PDE I got: Solving using the boundary conditions I ended up with which is very wrong... Am I solving this PDE wrong or is my initial method of obtaining the PDE flawed? Edit #2: Regarding the origins of this PDE (note everything is in cylindrical coordinates) is a function representing pressure, where pressure doesn't depend on or represents the height of a certain surface The geometries are irrelevant and thus a diagram is not necessary. It is also given that the components of the velocity field are I got the PDE by applying the continuity equation (a consequence of conservation of mass): For an incompressible fluid and cylindrical coordinate that becomes:","h(t), p(r,t) z. z \left[z^2-zh(t)\right]\left[\frac{\partial^2 p(r,t)}{\partial r^2}+\frac{\partial p(r,t)}{\partial r} \frac{1}{r}\right] + \frac{dh(t)}{dt}  \frac{1}{h(t)} = 0 \frac{\partial^2 p(r,t)}{\partial r^2}+\frac{\partial p(r,t)}{\partial r} \frac{1}{r} = 0 p(r_1,t) = 0 p(r_2,t) = 0 p(r,t) = e^{c_1(t)}*ln(r) + c_2(t) c_2(t) = 0 c_1(0) = ln(0) p(r,t) \theta z h(t) v_r(r,z,t) = \frac{1}{2 \mu} * \frac{\partial{p(r,t)}}{\partial r} * [ z^2 - zh(t)] v_\theta = 0 v_z(z,t) = \frac{\partial{h(t)}}{\partial t} * \frac{z}{h(t)} \nabla \cdot \vec{v}   \frac{1}{r} \frac{\partial{(r v_r)}}{\partial r} + \frac{1}{r} \frac{\partial{(v_\theta)}}{\partial \theta} + \frac{\partial{(v_z)}}{\partial z} = 0","['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'homogeneous-equation']"
20,Analytical solution to non-linear 2nd order ODE,Analytical solution to non-linear 2nd order ODE,,"Is there an analytical solution to the following non-linear ODE? $$y''(x) = A \frac{y}{B+\alpha y} ... (1)$$ where $A,B,\alpha$ are constants. The boundary conditions are: $$y'(x = 0) = y'(x = L) = 0$$",Is there an analytical solution to the following non-linear ODE? where are constants. The boundary conditions are:,"y''(x) = A \frac{y}{B+\alpha y} ... (1) A,B,\alpha y'(x = 0) = y'(x = L) = 0","['ordinary-differential-equations', 'nonlinear-analysis']"
21,Is a differential equation involving a multivariate function and exactly one of its partial derivatives an ODE?,Is a differential equation involving a multivariate function and exactly one of its partial derivatives an ODE?,,"I haven't taken any class in differential equations, so most of what I know about them is from small lectures in other classes. Please forgive my naivete. According to someone in this this post , the answer is yes: a differential equation involving a multivariate function and exactly one of its partial derivatives is indeed an ODE. However, I can't understand why. If we have $f(x,y) = y * \frac{df(x,y)}{dx}$ , don't the various values we can plug into independent variable $y$ affect the output of both $f(x,y)$ and $\frac{df(x,y)}{dx}$ ? Therefore, since we have multiple independent variables to worry about, we should treat this equation more as a PDE than ODE. Please feel free to correct my way of thinking as I am a greenhorn.","I haven't taken any class in differential equations, so most of what I know about them is from small lectures in other classes. Please forgive my naivete. According to someone in this this post , the answer is yes: a differential equation involving a multivariate function and exactly one of its partial derivatives is indeed an ODE. However, I can't understand why. If we have , don't the various values we can plug into independent variable affect the output of both and ? Therefore, since we have multiple independent variables to worry about, we should treat this equation more as a PDE than ODE. Please feel free to correct my way of thinking as I am a greenhorn.","f(x,y) = y * \frac{df(x,y)}{dx} y f(x,y) \frac{df(x,y)}{dx}","['ordinary-differential-equations', 'partial-differential-equations']"
22,solve: $ \frac {d^2y}{dx^2} + 5\frac{dy}{dx} = 15x^2$,solve:, \frac {d^2y}{dx^2} + 5\frac{dy}{dx} = 15x^2,"$$ \frac {d^2y}{dx^2} + 5\frac{dy}{dx} = 15x^2 $$ It's solution is $$y = y_{h} + y_{p}$$ where $y_{h} $ is the solution for homogenous equation and $y_{p} $ is the particular solution For homogenous solution: It's auxiliary equation is $$m^2 + 5m = 0$$ on solving for m, $$m = 0, -5$$ so, $$y_{h} = C_{1} + C_{2} e^{-5x}$$ here $$R = 15x^2$$ so it's particular solution should be : $$y_{p} = K_{2}x^2 + k_{1}x + k_{0}$$ on differentiating with respect to x $$y\prime _{p}= 2xk_{2} + k_{1}$$ again $$y\prime\prime_{p} = 2k_{2}$$ using all above values in the initial equation gives $$2k_{2} + 10xk_{2} + 10k_{1} = 15x^2$$ But using this method I am unable to proceed for the value of all unknown especially $k_{0}$ The given answer is: $$y = C_{1} + C_{2}e^{-5x} + x^3 + \frac{3x^2}{5} - \frac{6}{25}x$$","It's solution is where is the solution for homogenous equation and is the particular solution For homogenous solution: It's auxiliary equation is on solving for m, so, here so it's particular solution should be : on differentiating with respect to x again using all above values in the initial equation gives But using this method I am unable to proceed for the value of all unknown especially The given answer is:"," \frac {d^2y}{dx^2} + 5\frac{dy}{dx} = 15x^2  y = y_{h} + y_{p} y_{h}  y_{p}  m^2 + 5m = 0 m = 0, -5 y_{h} = C_{1} + C_{2} e^{-5x} R = 15x^2 y_{p} = K_{2}x^2 + k_{1}x + k_{0} y\prime _{p}= 2xk_{2} + k_{1} y\prime\prime_{p} = 2k_{2} 2k_{2} + 10xk_{2} + 10k_{1} = 15x^2 k_{0} y = C_{1} + C_{2}e^{-5x} + x^3 + \frac{3x^2}{5} - \frac{6}{25}x",['ordinary-differential-equations']
23,Prove by Brouwer's fixed point theorem that there is an integer m such that the equation has a periodic solution of period m.,Prove by Brouwer's fixed point theorem that there is an integer m such that the equation has a periodic solution of period m.,,"Consider the two dimensional system $\dot{x}=f(t,x)$ , $f(t + 1, x) = f (t, x)$ , where $f$ has continuous first derivatives with respect to $x$ . Suppose $\Omega$ is a subset of $\mathbb{R}^2$ which is homeomorphic to the closed unit disk. Also, for any solution $x(t, x_0)$ , $x(0, x_0) = x_0$ , suppose there is a $T(x_0)$ such that $x(t, x_0)$ is in $\Omega$ for all $t\geq T(x_0)$ . Prove by Brouwer's fixed point theorem that there is an integer $m$ such that the equation has a periodic solution of period $m$ . Does there exist a periodic solution of period $1$ ? Suppose there is a $\lambda >0$ such that $$x'f(t, x) \leq -\lambda|x|^2 $$ for all $t, x$ . If $g(t) = g(t + 1)$ is a continuous function, prove the equation $\dot{x} = f(t, x) + g(t)$ has a periodic solution of period $1$ .","Consider the two dimensional system , , where has continuous first derivatives with respect to . Suppose is a subset of which is homeomorphic to the closed unit disk. Also, for any solution , , suppose there is a such that is in for all . Prove by Brouwer's fixed point theorem that there is an integer such that the equation has a periodic solution of period . Does there exist a periodic solution of period ? Suppose there is a such that for all . If is a continuous function, prove the equation has a periodic solution of period .","\dot{x}=f(t,x) f(t + 1, x) = f (t, x) f x \Omega \mathbb{R}^2 x(t, x_0) x(0, x_0) = x_0 T(x_0) x(t, x_0) \Omega t\geq T(x_0) m m 1 \lambda >0 x'f(t, x) \leq -\lambda|x|^2  t, x g(t) = g(t + 1) \dot{x} = f(t, x) + g(t) 1","['ordinary-differential-equations', 'analysis']"
24,How to find the general solution of $ \frac{d^2 y}{dt^2} + \frac{dy}{dt} - 6y = e^{2t} $?,How to find the general solution of ?, \frac{d^2 y}{dt^2} + \frac{dy}{dt} - 6y = e^{2t} ,"How to find the general solution of $$ \frac{d^2 y}{dt^2} + \frac{dy}{dt} - 6y = e^{2t} $$ My solution: I solve for the auxiliary equation which is $ m^2 + m -6 = 0$ so that the complementary equation $y_c$ is $C_1 e^{2x} + C_2 e^{-3x} $ where $C_1$ and $C_2$ are arbitrary constants. And the particular equation is $$y_p = 0 $$ since if we let $y_p= Ae^{2t} $ , take its first and second derivative, and substitute the result in our original equation, we we will get that $A=0$ Therefore, my answer is $y(t) = C_1 e^{2x} + C_2 e^{-3x} + 0 $ . Is this correct?","How to find the general solution of My solution: I solve for the auxiliary equation which is so that the complementary equation is where and are arbitrary constants. And the particular equation is since if we let , take its first and second derivative, and substitute the result in our original equation, we we will get that Therefore, my answer is . Is this correct?", \frac{d^2 y}{dt^2} + \frac{dy}{dt} - 6y = e^{2t}   m^2 + m -6 = 0 y_c C_1 e^{2x} + C_2 e^{-3x}  C_1 C_2 y_p = 0  y_p= Ae^{2t}  A=0 y(t) = C_1 e^{2x} + C_2 e^{-3x} + 0 ,"['calculus', 'ordinary-differential-equations']"
25,ODE's Hale's Book pg. 27 Lemma 4.1 Exercises 4.1,ODE's Hale's Book pg. 27 Lemma 4.1 Exercises 4.1,,"I am studying with Hale's book on ODE's. There it does not have the demonstration of the following motto: LEMMA 4.1 If $f$ is either independent of $t$ or periodie in $t$ , then the solution $x=0$ of $\dot{x}(t)=f(t,x(t))$ being stable (asymptotically stable) implies the solution $x=0$ of $\dot{x}(t)=f(t,x(t))$ is uniformly stable (uniformly asymptotically stable). could you help me with the proof ? I couldn't develop anything. Any script or hints would be helpful","I am studying with Hale's book on ODE's. There it does not have the demonstration of the following motto: LEMMA 4.1 If is either independent of or periodie in , then the solution of being stable (asymptotically stable) implies the solution of is uniformly stable (uniformly asymptotically stable). could you help me with the proof ? I couldn't develop anything. Any script or hints would be helpful","f t t x=0 \dot{x}(t)=f(t,x(t)) x=0 \dot{x}(t)=f(t,x(t))","['ordinary-differential-equations', 'analysis']"
26,Matrix power of $e$ with complex e-values,Matrix power of  with complex e-values,e,"Calculate $e^A$ where $$A = \begin{bmatrix}1&0&3\\-1&2&0\\0&1&-1\end{bmatrix}$$ I knew how to do it if it was diagonalizable and real eigenvalues. How can I calculate when the matrix has complex eigenvalues. This matrix $A$ seems to have two conjugate complex evalues. I know how to calculate $\det \left(e^A\right)$ without knowing $e^A$ as: $$\det \left(e^A\right) = \det \left(e^J\right) = \det \left(e^{\text{tr}J}\right) = \det \left(e^{\text{tr}A}\right) = e^2$$ where $A = CJC^{-1}$ a Jordan form of $A$ , but I want to calculate $e^A$ . When I tried to find its eigenvalues to diagonalize it I came to $$\lambda^3-2\lambda^2-\lambda+5 = 0$$ which I couldn't solve. Also, I would really appreciate some beginner-friendly source references to learn more about problems of this type.","Calculate where I knew how to do it if it was diagonalizable and real eigenvalues. How can I calculate when the matrix has complex eigenvalues. This matrix seems to have two conjugate complex evalues. I know how to calculate without knowing as: where a Jordan form of , but I want to calculate . When I tried to find its eigenvalues to diagonalize it I came to which I couldn't solve. Also, I would really appreciate some beginner-friendly source references to learn more about problems of this type.",e^A A = \begin{bmatrix}1&0&3\\-1&2&0\\0&1&-1\end{bmatrix} A \det \left(e^A\right) e^A \det \left(e^A\right) = \det \left(e^J\right) = \det \left(e^{\text{tr}J}\right) = \det \left(e^{\text{tr}A}\right) = e^2 A = CJC^{-1} A e^A \lambda^3-2\lambda^2-\lambda+5 = 0,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'reference-request', 'jordan-normal-form']"
27,"Solution of first order linear differential equation, initial value problem","Solution of first order linear differential equation, initial value problem",,"My problem is: Solve $x'= $$\dfrac{1}{\sin\left(\dfrac{x(t)}{t}\right)}+\dfrac{x(t)}{t}\;,\;$ where $\;x(2) =\dfrac{2\pi}{3}\;.$ First, I did a substitution: $\;y(t)=\dfrac{x(t)}{t}$ $$ x(t)=ty(t)\implies x'(t) = y(t) + ty'(t) $$ $$  $$ $$ y(t) + ty'(t) = y(t) + \frac{1}{\sin(y(t))}$$ $$ y(2) = \frac{x(2)}{2} = \frac{\frac{2\pi}{3}}{2} = \frac{\pi}{3} $$ $$  $$ $$ y'(t) = \frac{1}{t\sin(y(t))} $$ $$ y(2)= \frac{\pi}{3} $$ $$  $$ $$G(t)=\int \frac{1}{t} dt = \ln(t) + C$$ $$H(y)= \int\sin(y) dy = -\cos(y) + C $$ $$H^{-1}(a)= -\arccos(a)  $$ $$ x(t) = ty(t) = -t\arccos(\ln(t)) $$ I think I am doing something wrong, and some things are missing, like the invervals. How can I determine the intervalls? And what am I doing wrong? I appreciate any kind of help.","My problem is: Solve where First, I did a substitution: I think I am doing something wrong, and some things are missing, like the invervals. How can I determine the intervalls? And what am I doing wrong? I appreciate any kind of help.","x'= \dfrac{1}{\sin\left(\dfrac{x(t)}{t}\right)}+\dfrac{x(t)}{t}\;,\; \;x(2) =\dfrac{2\pi}{3}\;. \;y(t)=\dfrac{x(t)}{t}  x(t)=ty(t)\implies x'(t) = y(t) + ty'(t)      y(t) + ty'(t) = y(t) + \frac{1}{\sin(y(t))}  y(2) = \frac{x(2)}{2} = \frac{\frac{2\pi}{3}}{2} = \frac{\pi}{3}      y'(t) = \frac{1}{t\sin(y(t))}   y(2)= \frac{\pi}{3}     G(t)=\int \frac{1}{t} dt = \ln(t) + C H(y)= \int\sin(y) dy = -\cos(y) + C  H^{-1}(a)= -\arccos(a)    x(t) = ty(t) = -t\arccos(\ln(t)) ","['ordinary-differential-equations', 'initial-value-problems']"
28,Proving the Laplace transform of $t^n e^{at}$ using mathematical induction [duplicate],Proving the Laplace transform of  using mathematical induction [duplicate],t^n e^{at},"This question already has an answer here : Proof of Laplace transform of real number powers: $\mathcal{L} \{ t^n \} = \frac{n!}{s^{n + 1}}$ (1 answer) Closed 3 years ago . Given the problem of finding the Laplace transform of the function $$f(t)=t^ne^{at}$$ with $n\in\mathbb{N}$ and $a\in\mathbb{R}$ , I realize it can be shown the transform is $$\frac{n!}{(s-a)^{n+1}}$$ by more than one direct method. However, I'd like to show this using strong induction. I began the problem by showing that \begin{align*} \mathcal{L}\{te^{at}\}=\int_{0}^{\infty}te^{-(s-a)t}\,dt&=\frac{1}{(s-a)^2},\\ \mathcal{L}\{t^2e^{at}\}=\int_{0}^{\infty}t^2e^{-(s-a)t}\,dt&=\frac{2}{(s-a)^3},\text{ and}\\ \mathcal{L}\{t^3e^{at}\}=\int_{0}^{\infty}t^3e^{-(s-a)t}\,dt&=\frac{6}{(s-a)^4}. \end{align*} I originally did this so I could personally see the pattern. Then, I began the proof as follows. $\textbf{Proof:}$ Let $P(n)$ be the statement $$P(n): \mathcal{L}\{t^ne^{at}\}=\frac{n!}{(s-a)^{n+1}},$$ for all $n\in\mathbb{N}.$ We have already shown $P(1), P(2), P(3)$ are true, so the base case has been proven. Inductive Step: Assume $P(k)$ is true for all $k$ . We must then show that $P(k)\implies P(k+1).$ We see \begin{align*} \mathcal{L}\{t^{k+1}e^{at}\}&=\frac{(k+1)!}{(s-a)^{k+2}}\\ &=\frac{k+1}{s-a}\frac{k!}{(s-a)^{k+1}}\\ &=\frac{k+1}{s-a}\mathcal{L}\{t^{k}e^{at}\} \quad\text{(by the inductive step)} \end{align*} At this point, I'm not really sure where to go. I'm pretty awful at proofs, so I assume I'm probably not going in the right direction after the inductive hypothesis.","This question already has an answer here : Proof of Laplace transform of real number powers: $\mathcal{L} \{ t^n \} = \frac{n!}{s^{n + 1}}$ (1 answer) Closed 3 years ago . Given the problem of finding the Laplace transform of the function with and , I realize it can be shown the transform is by more than one direct method. However, I'd like to show this using strong induction. I began the problem by showing that I originally did this so I could personally see the pattern. Then, I began the proof as follows. Let be the statement for all We have already shown are true, so the base case has been proven. Inductive Step: Assume is true for all . We must then show that We see At this point, I'm not really sure where to go. I'm pretty awful at proofs, so I assume I'm probably not going in the right direction after the inductive hypothesis.","f(t)=t^ne^{at} n\in\mathbb{N} a\in\mathbb{R} \frac{n!}{(s-a)^{n+1}} \begin{align*}
\mathcal{L}\{te^{at}\}=\int_{0}^{\infty}te^{-(s-a)t}\,dt&=\frac{1}{(s-a)^2},\\
\mathcal{L}\{t^2e^{at}\}=\int_{0}^{\infty}t^2e^{-(s-a)t}\,dt&=\frac{2}{(s-a)^3},\text{ and}\\
\mathcal{L}\{t^3e^{at}\}=\int_{0}^{\infty}t^3e^{-(s-a)t}\,dt&=\frac{6}{(s-a)^4}.
\end{align*} \textbf{Proof:} P(n) P(n): \mathcal{L}\{t^ne^{at}\}=\frac{n!}{(s-a)^{n+1}}, n\in\mathbb{N}. P(1), P(2), P(3) P(k) k P(k)\implies P(k+1). \begin{align*}
\mathcal{L}\{t^{k+1}e^{at}\}&=\frac{(k+1)!}{(s-a)^{k+2}}\\
&=\frac{k+1}{s-a}\frac{k!}{(s-a)^{k+1}}\\
&=\frac{k+1}{s-a}\mathcal{L}\{t^{k}e^{at}\} \quad\text{(by the inductive step)}
\end{align*}","['ordinary-differential-equations', 'induction', 'laplace-transform']"
29,Understanding mathematical concept behind phase space and phase portait,Understanding mathematical concept behind phase space and phase portait,,"I'd like to expose the problem through an example, which was what made me think about it. It's a rational mechanics problem. Consider the one dimensional Cauchy problem $\begin{cases}m\ddot{x} = F(x,\dot{x},t) \\ x(0) = x_0 \\ \dot{x}(0) = v_0 \\\end{cases}$ where $F : \mathbb{R} \times \mathbb{R} \times \mathbb{R} \longrightarrow \mathbb{R}, x,x_0,v_0 \in \mathbb{R}$ , and suppose exists a potential $V$ , i.e a function such that $-V'(x) = F(x)$ . With the last sentence we made sure to consider only purely positional forces and as a consequence, ""purely positional potentials"". From this we can introduce thec concept of phase space and phase portrait. What I don't get about this last one, is the following : We started from the differential equation above, which has solutions that are scalar function that depends on time, i.e $x = x(t)$ , but when we use $V$ (which is a function of $x$ ) to deal with phase portrait, seems like $x$ has become now an idependant variable, forgetting about time. So my question is what mathematical concept is behind this type of study, what allows to do so, and why. Any help or reference would be appreciated, phase space seems a little bit misterious to me yet.","I'd like to expose the problem through an example, which was what made me think about it. It's a rational mechanics problem. Consider the one dimensional Cauchy problem where , and suppose exists a potential , i.e a function such that . With the last sentence we made sure to consider only purely positional forces and as a consequence, ""purely positional potentials"". From this we can introduce thec concept of phase space and phase portrait. What I don't get about this last one, is the following : We started from the differential equation above, which has solutions that are scalar function that depends on time, i.e , but when we use (which is a function of ) to deal with phase portrait, seems like has become now an idependant variable, forgetting about time. So my question is what mathematical concept is behind this type of study, what allows to do so, and why. Any help or reference would be appreciated, phase space seems a little bit misterious to me yet.","\begin{cases}m\ddot{x} = F(x,\dot{x},t) \\ x(0) = x_0 \\ \dot{x}(0) = v_0 \\\end{cases} F : \mathbb{R} \times \mathbb{R} \times \mathbb{R} \longrightarrow \mathbb{R}, x,x_0,v_0 \in \mathbb{R} V -V'(x) = F(x) x = x(t) V x x","['ordinary-differential-equations', 'physics', 'mathematical-physics', 'classical-mechanics', 'differential']"
30,Is 'auxiliary equation' or 'characteristic equation' more common?,Is 'auxiliary equation' or 'characteristic equation' more common?,,"In my school textbook, when solving second order homogeneous differential equations it talks about using the equation's ' auxiliary equation '. However, I've seen in many places, such as Wikipedia, that the term ' characteristic equation ' is used instead. Which one is more standard? Are they both alright? If I write an article is it ok to talk about either of them? Thanks for your help.","In my school textbook, when solving second order homogeneous differential equations it talks about using the equation's ' auxiliary equation '. However, I've seen in many places, such as Wikipedia, that the term ' characteristic equation ' is used instead. Which one is more standard? Are they both alright? If I write an article is it ok to talk about either of them? Thanks for your help.",,"['calculus', 'ordinary-differential-equations', 'terminology']"
31,Finding a Lyapunov function and proving the stability,Finding a Lyapunov function and proving the stability,,"I'm trying to find a Lyapunov function for $(0, 0)$ in the system \begin{cases}    x' = 2x - 2y - (2x - y)^3\\     y' = 4x - 2y + (x - y)^3  \end{cases} I thought that the following one would appropriate $$V = 2x^2 - 2xy + y^2 = x^2 + (x - y)^2 > 0$$ $$V(0, 0) = 0$$ but is it true that $$V' = (2x + 2x - 2y)(2x - 2y - (2x - y)^3) - 2(x - y)(4x - 2y + (x - y)^3) = -34x^4 - 4y^4 - 60x^2y^2 + 24xy(3x^2 + y^2) \le 0  ?$$ Maybe it is better to find Chetaev function and use other theorem... I would be thankful for any help!",I'm trying to find a Lyapunov function for in the system I thought that the following one would appropriate but is it true that Maybe it is better to find Chetaev function and use other theorem... I would be thankful for any help!,"(0, 0) \begin{cases}
   x' = 2x - 2y - (2x - y)^3\\
    y' = 4x - 2y + (x - y)^3
 \end{cases} V = 2x^2 - 2xy + y^2 = x^2 + (x - y)^2 > 0 V(0, 0) = 0 V' = (2x + 2x - 2y)(2x - 2y - (2x - y)^3) - 2(x - y)(4x - 2y + (x - y)^3) = -34x^4 - 4y^4 - 60x^2y^2 + 24xy(3x^2 + y^2) \le 0  ?","['ordinary-differential-equations', 'stability-theory', 'lyapunov-functions']"
32,One sided limit of formula involving the hypergeometric function ${}_2F_1$ at singular point,One sided limit of formula involving the hypergeometric function  at singular point,{}_2F_1,"I need help finding the following limit (closed form): \begin{align} &\lim_{x\to1^{-}}\alpha x \ln(1-x) +\frac{(\alpha+1)x^2}{2-\alpha} {_2F_1(1, 2-\alpha; 3-\alpha; x)} \\ &\hspace{2cm}-\frac{x^2}{2-\alpha} {_2F_1(1, 2-\alpha^2; 3-\alpha^2; x)} \end{align} where $\alpha = e^\frac{2\pi i}{3}$ and the hypergeometric function is defined as: $$_2F_1(a;b;c;z)=\sum_{n=0}^{\infty} \frac{(a)_n (b)_n}{(c)_n}\frac{z^n}{n!}$$ where: $$(a)_n=\begin{cases} 1,  & n=0 \\ a(a+1)...(a+n-1), & n>0 \end{cases}$$ This is the first time I even hear of a hypergeometric function so I haven't the slightest clue as to how to approach this problem. I tried looking up some identities regarding ${_2F_1}$ but have come up with nothing useful, except the fact that it has a singularity at $1$ which really isn't helpful... any help would be appreciated.","I need help finding the following limit (closed form): where and the hypergeometric function is defined as: where: This is the first time I even hear of a hypergeometric function so I haven't the slightest clue as to how to approach this problem. I tried looking up some identities regarding but have come up with nothing useful, except the fact that it has a singularity at which really isn't helpful... any help would be appreciated.","\begin{align}
&\lim_{x\to1^{-}}\alpha x \ln(1-x) +\frac{(\alpha+1)x^2}{2-\alpha} {_2F_1(1, 2-\alpha; 3-\alpha; x)}
\\
&\hspace{2cm}-\frac{x^2}{2-\alpha} {_2F_1(1, 2-\alpha^2; 3-\alpha^2; x)}
\end{align} \alpha = e^\frac{2\pi i}{3} _2F_1(a;b;c;z)=\sum_{n=0}^{\infty} \frac{(a)_n (b)_n}{(c)_n}\frac{z^n}{n!} (a)_n=\begin{cases}
1,  & n=0 \\
a(a+1)...(a+n-1), & n>0
\end{cases} {_2F_1} 1","['calculus', 'ordinary-differential-equations', 'limits', 'hypergeometric-function']"
33,"Solving differential equation without IVP or fundamental solution : $\displaystyle \phi ^{""} -x \phi ^{'} + x^2 \phi = 0 $",Solving differential equation without IVP or fundamental solution :,"\displaystyle \phi ^{""} -x \phi ^{'} + x^2 \phi = 0 ","While studying about Differential Equations I randomly came up with  the equation mentioned below. $\displaystyle \phi ^{""} -x \phi ^{'} + x^2 \phi = 0  $ is actually the equation I got while solving for the RICCATI EQUATION : $$\displaystyle y' = x^2 + xy + y^2 $$ When we use the substitution $\displaystyle y = \frac {- \phi ^{'} }{\phi} ,$ one can get the above mentioned Second Order homogeneous differential equation. My attempt : I thought to think of a general solution to $ \ \displaystyle \phi ^{""} -x \phi ^{'} + x^2 \phi = 0  $ So I used the same old ordinary technique, THE CHARACTERISTIC EQUATION ( i.e substituting $ \ \displaystyle \phi = e^{rx}) $ We will a quadratic equation : $ \ \displaystyle r^2 - rx + x^2 = 0 ; \text {giving} \ r= -x\omega , -x \omega ^2 .$ Then choosing any one of them, and using ABEL's theorem, I found another LINEARLY INDEPENDENT SOLUTION, using Wronskian (by definition). (But I doubt on their Linear Independent nature) But it was too complicated. Let me show you. Assuming $ \ \displaystyle r = -x \omega ^2 $ , one will get : $$ \displaystyle \phi = c_1 e^{x^2/2}. e^{\iota \sqrt {3}x^2/2}  - \frac {c_2 }{x^3 + x + \iota 2\sqrt {3} \ x} \left ( e^{-x^4/4}. e^{\iota \sqrt {3} x^2 /2} \right ) $$ Now solving for $y$ would be hard. But when I found solution for the Riccati equation on wolfram, it showed : $ \ \displaystyle y = \frac {1}{c - x} - \frac {x}{2}  .$ I would like to know that the function $\phi$ which I found is general or not. And if there is a method to find the solution to second order differential equation with variable coefficients (homogeneous or non-homogeneous) without the knowledge of any prior common function that solves the equation. And please tell a good method to solve for the Riccati equation mentioned above. Edit the tags if required, i do not know what to add more. EDITS: In relation to the comment by Winther , when I checked for the solution provided by wolfram... it did not satisfy the equation. Please share a method to solve it. THANK YOU.","While studying about Differential Equations I randomly came up with  the equation mentioned below. is actually the equation I got while solving for the RICCATI EQUATION : When we use the substitution one can get the above mentioned Second Order homogeneous differential equation. My attempt : I thought to think of a general solution to So I used the same old ordinary technique, THE CHARACTERISTIC EQUATION ( i.e substituting We will a quadratic equation : Then choosing any one of them, and using ABEL's theorem, I found another LINEARLY INDEPENDENT SOLUTION, using Wronskian (by definition). (But I doubt on their Linear Independent nature) But it was too complicated. Let me show you. Assuming , one will get : Now solving for would be hard. But when I found solution for the Riccati equation on wolfram, it showed : I would like to know that the function which I found is general or not. And if there is a method to find the solution to second order differential equation with variable coefficients (homogeneous or non-homogeneous) without the knowledge of any prior common function that solves the equation. And please tell a good method to solve for the Riccati equation mentioned above. Edit the tags if required, i do not know what to add more. EDITS: In relation to the comment by Winther , when I checked for the solution provided by wolfram... it did not satisfy the equation. Please share a method to solve it. THANK YOU.","\displaystyle \phi ^{""} -x \phi ^{'} + x^2 \phi = 0   \displaystyle y' = x^2 + xy + y^2  \displaystyle y = \frac {- \phi ^{'} }{\phi} ,  \ \displaystyle \phi ^{""} -x \phi ^{'} + x^2 \phi = 0    \ \displaystyle \phi = e^{rx})   \ \displaystyle r^2 - rx + x^2 = 0 ; \text {giving} \ r= -x\omega , -x \omega ^2 .  \ \displaystyle r = -x \omega ^2   \displaystyle \phi = c_1 e^{x^2/2}. e^{\iota \sqrt {3}x^2/2}  - \frac {c_2 }{x^3 + x + \iota 2\sqrt {3} \ x} \left ( e^{-x^4/4}. e^{\iota \sqrt {3} x^2 /2} \right )  y  \ \displaystyle y = \frac {1}{c - x} - \frac {x}{2}  . \phi",['ordinary-differential-equations']
34,How differential equations are just real?,How differential equations are just real?,,"For sure this kind of stuff has a name, but I can't remember. So as to better understand what I mean, let's get concrete: Consider the linear ODE: $$y''+4\,y = 0$$ the characteristic polynomial has complex solutions: $$\lambda^2+4 = 0 \quad \Rightarrow\quad\lambda_{1/2} = \pm 2\,i$$ Now I just took it for granted that u can express such solutions as $$y = \mathrm{C_1}\,\cos(2\,x)+\mathrm{C_2}\,\sin(2\,x)$$ But if I were to plug the complex into the usual combination of exponentials: $$\mathrm{C_1}\,e^{+2\,i}+\mathrm{C_2}\,e^{-2\,i} = \\\\ \mathrm{C_1}\,\cos(2\,x)+\mathrm{C_2}\,\cos(-2\,x)+\mathrm{C_1}\,i\,\sin(2\,x)+\mathrm{C_2}\,i\,\sin(-2\,x)$$ that doesn't look to me as close as the solution. Furthermore to add even more confusion solving the ODE with MATLAB yields: $$y = \mathrm{C_1}+\mathrm{C_2}\,e^{-4\,x}$$ Apologies : this is the solution of $y''+4\,y'= 0$ As I mentioned I didn't really keep myself busy with the complex part of differential equation. I hope u might explain to me how all these solutions come together.","For sure this kind of stuff has a name, but I can't remember. So as to better understand what I mean, let's get concrete: Consider the linear ODE: the characteristic polynomial has complex solutions: Now I just took it for granted that u can express such solutions as But if I were to plug the complex into the usual combination of exponentials: that doesn't look to me as close as the solution. Furthermore to add even more confusion solving the ODE with MATLAB yields: Apologies : this is the solution of As I mentioned I didn't really keep myself busy with the complex part of differential equation. I hope u might explain to me how all these solutions come together.","y''+4\,y = 0 \lambda^2+4 = 0 \quad \Rightarrow\quad\lambda_{1/2} = \pm 2\,i y = \mathrm{C_1}\,\cos(2\,x)+\mathrm{C_2}\,\sin(2\,x) \mathrm{C_1}\,e^{+2\,i}+\mathrm{C_2}\,e^{-2\,i} = \\\\ \mathrm{C_1}\,\cos(2\,x)+\mathrm{C_2}\,\cos(-2\,x)+\mathrm{C_1}\,i\,\sin(2\,x)+\mathrm{C_2}\,i\,\sin(-2\,x) y = \mathrm{C_1}+\mathrm{C_2}\,e^{-4\,x} y''+4\,y'= 0","['ordinary-differential-equations', 'complex-numbers']"
35,Find solution: $\frac {d^4x}{dt^4}=x$,Find solution:,\frac {d^4x}{dt^4}=x,"Find solution: $\frac {d^4x}{dt^4}=x \tag{1}\label{eq1}$ For which $x_0 = (x(0),x'(0),x''(0),x^{(3)}(0))$ the solution is: limited on $(0,\infty)$ limited on $(-\infty,0)$ periodic so we have $P(\lambda)=\lambda^4-1$ $$\lambda_1=1,\lambda_2=-1,\lambda_3=i,\lambda_4=-i$$ each funtion $e^{it},e^{-it}$ is  a solution to \eqref{eq1} so their linear combination also is a solution to \eqref{eq1} So we have: $$x(t)=A_1e^t+A_2e^{-t}+A_3\cos{t}+A_4\sin{t}$$ $x'(t)=A_1e^t-A_2e^{-t}-A_3\cos{t}+A_4\sin{t}$ $x''(t)=A_1e^t+A_2e^{-t}-A_3\cos{t}-A_4\sin{t}$ $x^{(3)}(t)=A_1e^t-A_2e^{-t}+A_3\cos{t}-A_4\sin{t}$ So: $$x_0=(A_1+A_2+A_3, A_1-A_2+A_4, A_1+A_2-A_3, A_1-A_2-A_4)$$ Thus the solution is limited on $(0,+\infty) \Leftrightarrow A_1=0$ because then there is no $e^t$ and $\lim_{t \to \infty} e^{-t} = 0$ the solution is limited on $(-\infty,0) \Leftrightarrow A_2=0$ , as $\lim_{t \to -\infty} e^{t} = 0$ , $\lim_{t \to 0^-} e^{t} = 1$ the solution is periodic: on $(0,\infty) \Leftrightarrow A_1=0$ on $(-\infty,0)\Leftrightarrow A_2=0$ on $\mathbb{R} \Leftrightarrow A_1=A_2=0$ It is a good solution? If there are some kind of mistakes can I get hints what should I correct, thanks.","Find solution: For which the solution is: limited on limited on periodic so we have each funtion is  a solution to \eqref{eq1} so their linear combination also is a solution to \eqref{eq1} So we have: So: Thus the solution is limited on because then there is no and the solution is limited on , as , the solution is periodic: on on on It is a good solution? If there are some kind of mistakes can I get hints what should I correct, thanks.","\frac {d^4x}{dt^4}=x \tag{1}\label{eq1} x_0 = (x(0),x'(0),x''(0),x^{(3)}(0)) (0,\infty) (-\infty,0) P(\lambda)=\lambda^4-1 \lambda_1=1,\lambda_2=-1,\lambda_3=i,\lambda_4=-i e^{it},e^{-it} x(t)=A_1e^t+A_2e^{-t}+A_3\cos{t}+A_4\sin{t} x'(t)=A_1e^t-A_2e^{-t}-A_3\cos{t}+A_4\sin{t} x''(t)=A_1e^t+A_2e^{-t}-A_3\cos{t}-A_4\sin{t} x^{(3)}(t)=A_1e^t-A_2e^{-t}+A_3\cos{t}-A_4\sin{t} x_0=(A_1+A_2+A_3, A_1-A_2+A_4, A_1+A_2-A_3, A_1-A_2-A_4) (0,+\infty) \Leftrightarrow A_1=0 e^t \lim_{t \to \infty} e^{-t} = 0 (-\infty,0) \Leftrightarrow A_2=0 \lim_{t \to -\infty} e^{t} = 0 \lim_{t \to 0^-} e^{t} = 1 (0,\infty) \Leftrightarrow A_1=0 (-\infty,0)\Leftrightarrow A_2=0 \mathbb{R} \Leftrightarrow A_1=A_2=0","['ordinary-differential-equations', 'solution-verification']"
36,Solving a second-order matrix differential equation - periodic solutions,Solving a second-order matrix differential equation - periodic solutions,,"Let $\frac{d^2}{dt^2}x=\begin{pmatrix}1 &1 \\ 0 &a\end{pmatrix}x$ . For which $a \in {\mathbb{R}}$ there exist periodic solutions? I think only for $a<0$ there can be periodic solutions because then we get non-real eigenvalues. So we have $$\frac{d^2}{dt^2}x_2=ax_2. \tag{1}\label{eq1}$$ $P(\lambda)=\lambda^2-a=0$ so $\lambda_1=-\sqrt{-a}i$ , $\lambda_2=\sqrt{-a}i$ . Each function $e^{-\sqrt{-a}ti}$ , $e^{\sqrt{-a}ti}$ is a solution to (\ref{eq1}) so their linear combination is also a solution to (\ref{eq1}). So we have $$x_2(t)=E_1\cos(\sqrt{-a}t)+E_2\sin(\sqrt{-a}t).$$ We put $x_2$ to $$\frac{d^2}{dt^2}x_1=x_1+x_2, \tag{2}\label{eq2}$$ and we have $\frac{d^2}{dt^2}x_1-x_1=E_1\cos(\sqrt{-a}t)+E_2\sin(\sqrt{-a}t)$ . We look for a solution in a form: $C\cos(\sqrt{-a}t)+D\sin(\sqrt{-a}t)$ . By putting it to the (\ref{eq2}) we get that $C = E_1/(a-1)$ , $D = E_2/(a-1)$ . And the solution of homogeneous equation of (\ref{eq2}) is $D_1e^t+D_2e^{-t}$ . So we have $$x_1(t)=\frac{E_1}{a-1}\cos(\sqrt{-a}t)+\frac{E_2}{a-1}\sin(\sqrt{-a}t) + D_1e^t+D_2e^{-t}.$$ So the periodic solutions exist when $a<0$ . and I think whenever $D_1$ or $D_2$ $=0$ there are such solutions. Is it a good approach to this problem?","Let . For which there exist periodic solutions? I think only for there can be periodic solutions because then we get non-real eigenvalues. So we have so , . Each function , is a solution to (\ref{eq1}) so their linear combination is also a solution to (\ref{eq1}). So we have We put to and we have . We look for a solution in a form: . By putting it to the (\ref{eq2}) we get that , . And the solution of homogeneous equation of (\ref{eq2}) is . So we have So the periodic solutions exist when . and I think whenever or there are such solutions. Is it a good approach to this problem?","\frac{d^2}{dt^2}x=\begin{pmatrix}1 &1 \\ 0 &a\end{pmatrix}x a \in {\mathbb{R}} a<0 \frac{d^2}{dt^2}x_2=ax_2. \tag{1}\label{eq1} P(\lambda)=\lambda^2-a=0 \lambda_1=-\sqrt{-a}i \lambda_2=\sqrt{-a}i e^{-\sqrt{-a}ti} e^{\sqrt{-a}ti} x_2(t)=E_1\cos(\sqrt{-a}t)+E_2\sin(\sqrt{-a}t). x_2 \frac{d^2}{dt^2}x_1=x_1+x_2, \tag{2}\label{eq2} \frac{d^2}{dt^2}x_1-x_1=E_1\cos(\sqrt{-a}t)+E_2\sin(\sqrt{-a}t) C\cos(\sqrt{-a}t)+D\sin(\sqrt{-a}t) C = E_1/(a-1) D = E_2/(a-1) D_1e^t+D_2e^{-t} x_1(t)=\frac{E_1}{a-1}\cos(\sqrt{-a}t)+\frac{E_2}{a-1}\sin(\sqrt{-a}t) + D_1e^t+D_2e^{-t}. a<0 D_1 D_2 =0",['ordinary-differential-equations']
37,Help me find error in ODE for sensitivity analysis of parameters of Lotka-Voltera equation,Help me find error in ODE for sensitivity analysis of parameters of Lotka-Voltera equation,,"I have a Lotka-Voltera model on which i want to perform parameter estimation by calculating the gradients of the parameters using an extended ODE system. I know there are different methods for doing that but i choose to ignore those. is a Lotka-Voltera model For it derived the Forward sensivity equations using: $$\dot{s}_{i,j}  = \frac{df_j}{dy_i} s_{i,j} + \frac{df_j}{p_i} \text{ where } \dot{y}=f(y,p_1, \ ..., p_I ,t), \ i \in \{1,...,I\}, y \in \mathbb{R}^J, j \in \{1,...,J\}   $$ So $s_{i,j}$ would be the sensitivity of the $j$ -th component of solution on parameter $i$ . This gave me the following extension of the system of differential equations: I can solve the Lotka-Voltera equations for intital conditions using: function  lotka_voltera(u,p,t)     N₁, N₂ = u     ϵ₁, ϵ₂, γ₁, γ₂ = p # p_1 ... p_4     return  [N₁*ϵ₁ - γ₁*N₁*N₂, -N₂*ϵ₂ + γ₂*N₁*N₂] end  u₀ = [1.,1.]; p = [3.,1.,0.5,0.4]; t =(0., 3.);  prob = ODEProblem(lotka_voltera, u₀, t, p) solution = solve(prob); plot(solution) and get a plot like this: However when implemented the extension of the system of equations in Julia: function lotka_sensitivity(du, u, p, t)     du[:,1] = lotka_voltera(u[:,1],p,t)     N₁, N₂ = u[:,1] # ; s₁ₚ₁ s₂ₚ₁; s₁ₚ₂ s₂ₚ₂; s₁ₚ₃ s₂ₚ₃; s₁ₚ₄ s₂ₚ₄     p₁, p₂, p₃, p₄ = p          J = [(p₁-p₂*N₂) (-p₂*N₁); (p₄*N₁) (p₄*N₂-p₃)]     du[:,2] = (J*u[:,2]) .+ [N₁, 0.]     du[:,3] = (J*u[:,3]) .+ [-N₂*N₁, 0.]     du[:,4] = (J*u[:,4]) .+ [0., -N₂]     du[:,5] = (J*u[:,5]) .+ [0., N₂*N₁] end   u₀ₚ = hcat(u₀, zeros(2,4)) # the 2nd to fith coulouns are for the sensitivities. Those are 0 as the inital conditions u_0 are fixed and independent of p prob_sensitivity = ODEProblem(lotka_sensitivity, u₀ₚ, t, p)  solution_sensitivity = solve(prob_sensitivity); plot(solution_sensitivity; vars=[(1),(2),(3),#=(4),=#(5),#=(6),=#(7),#=(8),(9),(10)=#]) Then the solutions i commented out from plotting grow exponentially with time. This shouldn't be in my understanding. What did i do wrong? Possible sources of error are: Did i use the right formula for the sensitivity i want? Is my extended system correct? Is it correct that i let the sensitivities start at 0? Did i implement the ODE correctly? The complete code in a Pluto notebook which also is a standalone file for reference and also includes what dependencies are necessary . In vorschlag3 i try to calculate the integrated square error in the first component of solution compared to one with different parameters and attempt to do a gradient descent to minimize it. This also fails.","I have a Lotka-Voltera model on which i want to perform parameter estimation by calculating the gradients of the parameters using an extended ODE system. I know there are different methods for doing that but i choose to ignore those. is a Lotka-Voltera model For it derived the Forward sensivity equations using: So would be the sensitivity of the -th component of solution on parameter . This gave me the following extension of the system of differential equations: I can solve the Lotka-Voltera equations for intital conditions using: function  lotka_voltera(u,p,t)     N₁, N₂ = u     ϵ₁, ϵ₂, γ₁, γ₂ = p # p_1 ... p_4     return  [N₁*ϵ₁ - γ₁*N₁*N₂, -N₂*ϵ₂ + γ₂*N₁*N₂] end  u₀ = [1.,1.]; p = [3.,1.,0.5,0.4]; t =(0., 3.);  prob = ODEProblem(lotka_voltera, u₀, t, p) solution = solve(prob); plot(solution) and get a plot like this: However when implemented the extension of the system of equations in Julia: function lotka_sensitivity(du, u, p, t)     du[:,1] = lotka_voltera(u[:,1],p,t)     N₁, N₂ = u[:,1] # ; s₁ₚ₁ s₂ₚ₁; s₁ₚ₂ s₂ₚ₂; s₁ₚ₃ s₂ₚ₃; s₁ₚ₄ s₂ₚ₄     p₁, p₂, p₃, p₄ = p          J = [(p₁-p₂*N₂) (-p₂*N₁); (p₄*N₁) (p₄*N₂-p₃)]     du[:,2] = (J*u[:,2]) .+ [N₁, 0.]     du[:,3] = (J*u[:,3]) .+ [-N₂*N₁, 0.]     du[:,4] = (J*u[:,4]) .+ [0., -N₂]     du[:,5] = (J*u[:,5]) .+ [0., N₂*N₁] end   u₀ₚ = hcat(u₀, zeros(2,4)) # the 2nd to fith coulouns are for the sensitivities. Those are 0 as the inital conditions u_0 are fixed and independent of p prob_sensitivity = ODEProblem(lotka_sensitivity, u₀ₚ, t, p)  solution_sensitivity = solve(prob_sensitivity); plot(solution_sensitivity; vars=[(1),(2),(3),#=(4),=#(5),#=(6),=#(7),#=(8),(9),(10)=#]) Then the solutions i commented out from plotting grow exponentially with time. This shouldn't be in my understanding. What did i do wrong? Possible sources of error are: Did i use the right formula for the sensitivity i want? Is my extended system correct? Is it correct that i let the sensitivities start at 0? Did i implement the ODE correctly? The complete code in a Pluto notebook which also is a standalone file for reference and also includes what dependencies are necessary . In vorschlag3 i try to calculate the integrated square error in the first component of solution compared to one with different parameters and attempt to do a gradient descent to minimize it. This also fails.","\dot{s}_{i,j}  = \frac{df_j}{dy_i} s_{i,j} + \frac{df_j}{p_i} \text{ where } \dot{y}=f(y,p_1, \ ..., p_I ,t), \ i \in \{1,...,I\}, y \in \mathbb{R}^J, j \in \{1,...,J\}    s_{i,j} j i","['real-analysis', 'ordinary-differential-equations', 'numerical-methods', 'parameter-estimation']"
38,Find the unique solution to the IVP $x' = Ax$ where $A = \begin{bmatrix} {-3}&{2} \\ {-1}&{-1}\end{bmatrix}$,Find the unique solution to the IVP  where,x' = Ax A = \begin{bmatrix} {-3}&{2} \\ {-1}&{-1}\end{bmatrix},"I began this problem by evaulating $x' = Ax$ . Let $$ x' = \begin{bmatrix}{x_1} \\ {x_2}\end{bmatrix}.$$ Then we have $$x'_1=-3x_1 + 2x_2, $$ $$x'_2=-x_1 - x_2, $$ $$x_1(0)=1,$$ $$x_2(0)=-2.$$ From the first equation we have $x_2= \frac{1}{2}x'_1+ \frac{3}{2}x_1$ . Substituting this into the second equation yields: $$x''_1+4x'_1-2x_1=0,$$ The characteristic equation of this is $m^2+4m-2=0$ , which has the solutions $m = -2 \pm \sqrt{6}$ . However, I am confused how to proceed to finding the final solution. Any guidance is greatly appreciated!","I began this problem by evaulating . Let Then we have From the first equation we have . Substituting this into the second equation yields: The characteristic equation of this is , which has the solutions . However, I am confused how to proceed to finding the final solution. Any guidance is greatly appreciated!","x' = Ax  x' = \begin{bmatrix}{x_1} \\ {x_2}\end{bmatrix}. x'_1=-3x_1 + 2x_2,  x'_2=-x_1 - x_2,  x_1(0)=1, x_2(0)=-2. x_2= \frac{1}{2}x'_1+ \frac{3}{2}x_1 x''_1+4x'_1-2x_1=0, m^2+4m-2=0 m = -2 \pm \sqrt{6}",['ordinary-differential-equations']
39,The ODE $y''(x)=\sinh(x)-3y'(x)-2y(x)$,The ODE,y''(x)=\sinh(x)-3y'(x)-2y(x),"I am trying to solve the differential equation that is in the title as a System of first order ode. My Approach: $\frac{d}{dx} \left(\begin{array}{c} y \\ y' \end{array}\right)=\left(\begin{array}{c} y' \\ \sinh(x)-3y'-2y \end{array}\right)=$ $\left( \begin{array}{rrr} 0 & 1 \\  -2 & -3 \\ \end{array}\right)\left(\begin{array}{c} y \\ y' \end{array}\right)+\left(\begin{array}{c} 0 \\ \sinh(x) \end{array}\right)$ Then I calculate the characteristic polynomial of the coefficient matrix, which leads to the eigenvalues $\lambda_1=-2,\lambda_2=-1$ . Let A denote the matrix above. Calculating the matrix exponential, I get $e^{Ax}$ = $\frac{1}{e^{2x}} \left( \begin{array}{rrr} 2e^x-1 & e^x-1 \\  -2e^x+2 & -e^x+2 \\ \end{array}\right)$ Now I am variating the parameters and get $y(x)=\frac{1}{e^{2x}} \left( \begin{array}{rrr} 2e^x-1 & e^x-1 \\  -2e^x+2 & -e^x+2 \\ \end{array}\right)y_0+\frac{1}{e^{2x}} \left( \begin{array}{rrr} 2e^x-1 & e^x-1 \\  -2e^x+2 & -e^x+2 \\ \end{array}\right) \int_0^s  \left(\begin{array}{c} \sinh(x)(e^{-s}-1) \\ \sinh(x)(-e^{-s}+2) \end{array}\right)ds$ = $y(x)=\frac{1}{e^{2x}} \left( \begin{array}{rrr} 2e^x-1 & e^x-1 \\  -2e^x+2 & -e^x+2 \\ \end{array}\right)y_0+\frac{1}{e^{2x}} \left( \begin{array}{rrr} 2e^x-1 & e^x-1 \\  -2e^x+2 & -e^x+2 \\ \end{array}\right)  \left(\begin{array}{c} \frac{1}{4}e^{-2x}(e^{2x}(2x+3)-2e^x-2e^{3x}+1+C_1) \\ \frac{-x}{2} -\frac{e^{-2x}}{4}+e^{-x}+e^{x} +C_2\end{array}\right)$ My Questions are: In the exercise description there was no value for $y_0$ , is there a way to find the value for it? Is my calculation correct (does it seem correct) or are there any mistakes?","I am trying to solve the differential equation that is in the title as a System of first order ode. My Approach: Then I calculate the characteristic polynomial of the coefficient matrix, which leads to the eigenvalues . Let A denote the matrix above. Calculating the matrix exponential, I get = Now I am variating the parameters and get = My Questions are: In the exercise description there was no value for , is there a way to find the value for it? Is my calculation correct (does it seem correct) or are there any mistakes?","\frac{d}{dx} \left(\begin{array}{c} y \\ y' \end{array}\right)=\left(\begin{array}{c} y' \\ \sinh(x)-3y'-2y \end{array}\right)= \left( \begin{array}{rrr}
0 & 1 \\ 
-2 & -3 \\
\end{array}\right)\left(\begin{array}{c} y \\ y' \end{array}\right)+\left(\begin{array}{c} 0 \\ \sinh(x) \end{array}\right) \lambda_1=-2,\lambda_2=-1 e^{Ax} \frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right) y(x)=\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)y_0+\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right) \int_0^s  \left(\begin{array}{c} \sinh(x)(e^{-s}-1) \\ \sinh(x)(-e^{-s}+2) \end{array}\right)ds y(x)=\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)y_0+\frac{1}{e^{2x}} \left( \begin{array}{rrr}
2e^x-1 & e^x-1 \\ 
-2e^x+2 & -e^x+2 \\
\end{array}\right)  \left(\begin{array}{c} \frac{1}{4}e^{-2x}(e^{2x}(2x+3)-2e^x-2e^{3x}+1+C_1) \\ \frac{-x}{2} -\frac{e^{-2x}}{4}+e^{-x}+e^{x} +C_2\end{array}\right) y_0","['ordinary-differential-equations', 'analysis']"
40,Solution of Differential equation that provides infinitely many solutiions,Solution of Differential equation that provides infinitely many solutiions,,"$y''+y = 0$ $y(0) = 1, y(k) = 1 $ Find the value of $k$ that the given problem has infinitely many solutions. a) $\pi + 2n \pi$ b) $2n \pi $ c) $\dfrac{\pi}{4} + 2n \pi $ d) $ \dfrac{\pi}{2} + 2n \pi $ My work : The general solution is, $y(t) = C_1 \cos t + C_2 \sin t $ $y(0)  = 1 $ $y = \cos t + C_2 \sin t $ $y(L) = 1 $ $\cos L + C_2 \sin L = 1$ $C_2 = \dfrac{1- \cos L }{\sin L }$ To get $C_1 = 1$ $\cos L = 1 $ $ L = 2 k \pi , k \in N $ Is this approach correct? I have confusion with 4th option as well because, $\dfrac{1- \cos L }{\sin L } = 1$ $\tan (L/2) = 1 $ $ L = \dfrac{\pi}{2} + 2n \pi $ Which option is the perfect match? Any suggestions would be appreciated.","Find the value of that the given problem has infinitely many solutions. a) b) c) d) My work : The general solution is, To get Is this approach correct? I have confusion with 4th option as well because, Which option is the perfect match? Any suggestions would be appreciated.","y''+y = 0 y(0) = 1, y(k) = 1  k \pi + 2n \pi 2n \pi  \dfrac{\pi}{4} + 2n \pi   \dfrac{\pi}{2} + 2n \pi  y(t) = C_1 \cos t + C_2 \sin t  y(0)  = 1  y = \cos t + C_2 \sin t  y(L) = 1  \cos L + C_2 \sin L = 1 C_2 = \dfrac{1- \cos L }{\sin L } C_1 = 1 \cos L = 1   L = 2 k \pi , k \in N  \dfrac{1- \cos L }{\sin L } = 1 \tan (L/2) = 1   L = \dfrac{\pi}{2} + 2n \pi ",['ordinary-differential-equations']
41,Good resource for differential equation solution approximation in python?,Good resource for differential equation solution approximation in python?,,"I'm a business operations guy. I've come to really enjoy reading up on the math, stats, and coding that comes with the models/tools that we use. I specify this so that you have a practical frame of reference: I'm not a physics undergrad student who needs to solve ODEs and/or PDEs by hand for class. Rather, I'm interested in (A) Numerical approximations via python to ODEs/PDEs and (B) examples of the sort of problems they can solve. What originally perked my interest was usage of Hamiltonian Monte Carlo based samplers, which use differential equations to inform a dynamic path across probability distributions. (HMC is implemented in PyMC3, so I don't need to implement this myself.) Anyway, I did further reading and learned that differential equations are also used heavily in ecology and economics. The potential applications seem limitless and so I'd like to get some exposure to A & B (above.) Any recommendations on books and or courses would be appreciated!","I'm a business operations guy. I've come to really enjoy reading up on the math, stats, and coding that comes with the models/tools that we use. I specify this so that you have a practical frame of reference: I'm not a physics undergrad student who needs to solve ODEs and/or PDEs by hand for class. Rather, I'm interested in (A) Numerical approximations via python to ODEs/PDEs and (B) examples of the sort of problems they can solve. What originally perked my interest was usage of Hamiltonian Monte Carlo based samplers, which use differential equations to inform a dynamic path across probability distributions. (HMC is implemented in PyMC3, so I don't need to implement this myself.) Anyway, I did further reading and learned that differential equations are also used heavily in ecology and economics. The potential applications seem limitless and so I'd like to get some exposure to A & B (above.) Any recommendations on books and or courses would be appreciated!",,"['ordinary-differential-equations', 'partial-differential-equations', 'reference-request', 'numerical-methods', 'python']"
42,Fuction whose gradient is of constant norm on its level sets,Fuction whose gradient is of constant norm on its level sets,,"I have a function $f:\mathbb{R}^N \to \mathbb{R}$ and I know that on its level sets $f^{-1}(z)$ the norm of its gradient is constant. What can I say about this function? $$ ||\nabla_x f(x)|| = \text{const} \qquad \qquad \forall x \in f^{-1}(z) := \left\{x \in \mathbb{R}^N \, :\, f(x) = z\right\} \qquad \forall \in \mathbb{R} $$ Related questions are this and this . However, they consider the norm of the gradient to be constant for every $x$ in the domain. I know that this is true only on each level set.","I have a function and I know that on its level sets the norm of its gradient is constant. What can I say about this function? Related questions are this and this . However, they consider the norm of the gradient to be constant for every in the domain. I know that this is true only on each level set.","f:\mathbb{R}^N \to \mathbb{R} f^{-1}(z) 
||\nabla_x f(x)|| = \text{const} \qquad \qquad \forall x \in f^{-1}(z) := \left\{x \in \mathbb{R}^N \, :\, f(x) = z\right\} \qquad \forall \in \mathbb{R}
 x","['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'riemannian-geometry']"
43,Damped Forced Motion Where forcing happens at a time ahead of 0,Damped Forced Motion Where forcing happens at a time ahead of 0,,"So presume that I have the following DE which represents a vibrating spring with a mass attached to it $$x''+6x'+10x=25\cos(4t), x(0) = \frac{1}{2},x'(0)=0$$ I am lead to believe that the force is applied periodically starting when t = 0 seconds (that is to say the forcing function is applied when the mass is released). What form would this DE have if the force was applied starting when t was some greater positive number? Like say for example, 2 (that is to say the forcing function is applied 2 seconds after the mass is released) ? What form would it have if instead of a periodic forcing function, the forcing function was instead constant like for the DE: $$x''+6x'+10x=5$$","So presume that I have the following DE which represents a vibrating spring with a mass attached to it I am lead to believe that the force is applied periodically starting when t = 0 seconds (that is to say the forcing function is applied when the mass is released). What form would this DE have if the force was applied starting when t was some greater positive number? Like say for example, 2 (that is to say the forcing function is applied 2 seconds after the mass is released) ? What form would it have if instead of a periodic forcing function, the forcing function was instead constant like for the DE:","x''+6x'+10x=25\cos(4t), x(0) = \frac{1}{2},x'(0)=0 x''+6x'+10x=5",['ordinary-differential-equations']
44,First-order non-linear ordinary differential equation,First-order non-linear ordinary differential equation,,I know how to solve the Bernoulli differential equation. How to solve the following first-order non-linear ordinary differential equation. $y' + p(x)y + q(x) = y^2 r(x)$ .,I know how to solve the Bernoulli differential equation. How to solve the following first-order non-linear ordinary differential equation. .,y' + p(x)y + q(x) = y^2 r(x),['ordinary-differential-equations']
45,How to solve the differential equation $\frac{dy}{dx} = \sin^2(x-y+1)$,How to solve the differential equation,\frac{dy}{dx} = \sin^2(x-y+1),"I have to solve the following differential equation: $$\frac{dy}{dx} = \sin^2(x-y+1)$$ My attempt was: Let $E = x-y+1 \implies \frac{dE}{dx}=1-\frac{dy}{dx} \iff \frac{dy}{dx} =1- \frac{dE}{dx}$ And i got $1- \sin^2(E)= \frac{dE}{dx} $ separating variables $dx=\frac{1}{1-\sin^2(E)}dE$ and integrating both sides, $x+C=\tan(E)$ , finally i got $y(x)= x - arctan(x+C) + 1$ . but when I derive both sides I don't get the original differential equation, therefore my answer is not correct. What is wrong?","I have to solve the following differential equation: My attempt was: Let And i got separating variables and integrating both sides, , finally i got . but when I derive both sides I don't get the original differential equation, therefore my answer is not correct. What is wrong?",\frac{dy}{dx} = \sin^2(x-y+1) E = x-y+1 \implies \frac{dE}{dx}=1-\frac{dy}{dx} \iff \frac{dy}{dx} =1- \frac{dE}{dx} 1- \sin^2(E)= \frac{dE}{dx}  dx=\frac{1}{1-\sin^2(E)}dE x+C=\tan(E) y(x)= x - arctan(x+C) + 1,['ordinary-differential-equations']
46,"A ""viscosity"" solution of ODE arising from switching systems","A ""viscosity"" solution of ODE arising from switching systems",,"$\newcommand{\d}[1]{\underline{\mathrm{#1}}}$ $\color{red}{\d{Setting}}$ Let $f : \mathbb R \to \{0,1\}$ be a function and let $x_0 \in (0,1)$ . I have two fixed distinct real numbers $A_0,A_1$ and define $g(x) = A_{f(x)}x$ . Basically, $f$ assigns to each $x$ a number $A_0$ or $A_1$ , and then we get $g(x)$ by multiplying by $A$ . Consider the initial value problem $$ \dot{W}(x) = g(W(x)) \quad W(0) = x_0 $$ for what $f$ can I assert existence and uniqueness of $W$ as a ""solution"" for any initial point $x_0$ ? I'd like existence and uniqueness only till $f$ exists $[0,1]$ for the first time. $\color{blue}{\d{Motivation}}$ This arises in the notion of switching linear systems. Imagine a system where a particle is at a point initially, say the particle is a feather. At every point in the space, there is a fan, which blows in a certain direction, and dictates a local behaviour of the motion of the feather like a linear ODE. Now we find when the feather has a unique valid movement in this configuration. Apart from feathers in fans, my particular application is in epidemic theory, where the trajectory of a epidemic can be controlled with an on-off switch, with different linear dynamics in the on and off case. $\color{fuchsia}{\d{Example}}$ Of course, if $f$ is constant then the solution exists as a strong solution, and is unique given the initial condition. Let's take a first non-trivial example. Fix a $y \in (0,1)$ and let $f(x) = -1$ if $x < y$ and $f(x) = 1$ otherwise. Now, let $x_0$ be any point. Note that the Lipschitz continuity condition fails for $f$ at $y$ , in fact even continuity fails. So I'm not expecting a strong solution anyway : a solution to this won't even be differentiable, looking at the behaviour at $y$ . However, I'd like to think that a solution of some kind exists as follows : say for example that $x_0 < y$ . Then , by using the existence/uniqueness theorem for $(0,y)$ I can get the solution on this interval, which will be $W(z) = e^{-z} + (x_0-1)$ . This solution, by continuity of the weak solution, will fix a unique value at $y$ i.e. $W(y) = e^{-y} + (x_0-1)$ . Then, using the uniqueness/existence condition in $[y,1]$ we get an extension of the solution to $[0,1]$ , uniquely (I can write down the explicit answer but that's not important). We call this solution as $W$ . I read up the notion of a viscosity solution which matches heavily with my construction (For the definition, see here or see Fleming and Soner, chapter 2). I can't go for a weak solution, since that just neglects the role of $y$ in the above construction, and you can just work piecewise. I'd like the pieces to be ""joined together"" continuously, and a viscosity solution is ensured continuous by construction. $\color{green}{\d{Questions}}$ According to Wikipedia , a viscosity solution seems to be defined only for certain forms of PDE that arise from the HJB. I believe that this falls in that category of ""degenerate elliptic"" PDE because it is a first order PDE so the condition is vacuous, but I need confirmation on this one because the switching and discontinuity of $g$ is getting to me. Is the function I wrote above the unique viscosity solution of the ODE described? I'd like an easy way to see that it is, but the definitions are getting to me. Let me try : so the situation is $V(0) = x_0$ and $$F(x, V(x), \dot{V}(x)) = \begin{cases} \dot{V}(x) - V(x) & x \geq y \\ \dot{V}(x) + V(x) & x < y \end{cases} $$ To check that the $W$ constructed is a sub and super solution we only need to see the point $y$ since at the other points differentiability is present. If $\phi$ is differentiable at $y$ and $\phi \geq W$ in a neighbourhood of $y$ then $\phi(t) - \phi(y) \geq W(t) - W(y)$ for all $t$ in a neighbourhood of $y$ . Now we can divide by $y$ and let $t \to u$ from left and right and get that $|y| \geq \phi'(y)$ , so $W$ is a subsolution. We should get $W$ being a super solution symmetrically : if $\phi \leq W$ in a neighbourhood of $y$ then the inequalities reverse which basically leads to the reverse conclusion $|y| \leq \phi'(y)$ and we are done. (Kindly point out casual errors, I don't expect any) If $f$ has only finitely many points of discontinuity then I expect a unique viscosity solution to exist. How can I be expected to work if $f$ had infinitely many points of discontinuity? Or at a limit point of the discontinuities? Could the solution be definable in the viscous sense here and be unique? Essentially, I'm trying to find the largest class of $f$ for which I can find a viscosity solution. I need the class to be large enough because I want a minimizer of a certain functional of that class, which I can't really study unless I know something about this class.","Let be a function and let . I have two fixed distinct real numbers and define . Basically, assigns to each a number or , and then we get by multiplying by . Consider the initial value problem for what can I assert existence and uniqueness of as a ""solution"" for any initial point ? I'd like existence and uniqueness only till exists for the first time. This arises in the notion of switching linear systems. Imagine a system where a particle is at a point initially, say the particle is a feather. At every point in the space, there is a fan, which blows in a certain direction, and dictates a local behaviour of the motion of the feather like a linear ODE. Now we find when the feather has a unique valid movement in this configuration. Apart from feathers in fans, my particular application is in epidemic theory, where the trajectory of a epidemic can be controlled with an on-off switch, with different linear dynamics in the on and off case. Of course, if is constant then the solution exists as a strong solution, and is unique given the initial condition. Let's take a first non-trivial example. Fix a and let if and otherwise. Now, let be any point. Note that the Lipschitz continuity condition fails for at , in fact even continuity fails. So I'm not expecting a strong solution anyway : a solution to this won't even be differentiable, looking at the behaviour at . However, I'd like to think that a solution of some kind exists as follows : say for example that . Then , by using the existence/uniqueness theorem for I can get the solution on this interval, which will be . This solution, by continuity of the weak solution, will fix a unique value at i.e. . Then, using the uniqueness/existence condition in we get an extension of the solution to , uniquely (I can write down the explicit answer but that's not important). We call this solution as . I read up the notion of a viscosity solution which matches heavily with my construction (For the definition, see here or see Fleming and Soner, chapter 2). I can't go for a weak solution, since that just neglects the role of in the above construction, and you can just work piecewise. I'd like the pieces to be ""joined together"" continuously, and a viscosity solution is ensured continuous by construction. According to Wikipedia , a viscosity solution seems to be defined only for certain forms of PDE that arise from the HJB. I believe that this falls in that category of ""degenerate elliptic"" PDE because it is a first order PDE so the condition is vacuous, but I need confirmation on this one because the switching and discontinuity of is getting to me. Is the function I wrote above the unique viscosity solution of the ODE described? I'd like an easy way to see that it is, but the definitions are getting to me. Let me try : so the situation is and To check that the constructed is a sub and super solution we only need to see the point since at the other points differentiability is present. If is differentiable at and in a neighbourhood of then for all in a neighbourhood of . Now we can divide by and let from left and right and get that , so is a subsolution. We should get being a super solution symmetrically : if in a neighbourhood of then the inequalities reverse which basically leads to the reverse conclusion and we are done. (Kindly point out casual errors, I don't expect any) If has only finitely many points of discontinuity then I expect a unique viscosity solution to exist. How can I be expected to work if had infinitely many points of discontinuity? Or at a limit point of the discontinuities? Could the solution be definable in the viscous sense here and be unique? Essentially, I'm trying to find the largest class of for which I can find a viscosity solution. I need the class to be large enough because I want a minimizer of a certain functional of that class, which I can't really study unless I know something about this class.","\newcommand{\d}[1]{\underline{\mathrm{#1}}} \color{red}{\d{Setting}} f : \mathbb R \to \{0,1\} x_0 \in (0,1) A_0,A_1 g(x) = A_{f(x)}x f x A_0 A_1 g(x) A 
\dot{W}(x) = g(W(x)) \quad W(0) = x_0
 f W x_0 f [0,1] \color{blue}{\d{Motivation}} \color{fuchsia}{\d{Example}} f y \in (0,1) f(x) = -1 x < y f(x) = 1 x_0 f y y x_0 < y (0,y) W(z) = e^{-z} + (x_0-1) y W(y) = e^{-y} + (x_0-1) [y,1] [0,1] W y \color{green}{\d{Questions}} g V(0) = x_0 F(x, V(x), \dot{V}(x)) = \begin{cases}
\dot{V}(x) - V(x) & x \geq y \\
\dot{V}(x) + V(x) & x < y
\end{cases}  W y \phi y \phi \geq W y \phi(t) - \phi(y) \geq W(t) - W(y) t y y t \to u |y| \geq \phi'(y) W W \phi \leq W y |y| \leq \phi'(y) f f f","['ordinary-differential-equations', 'partial-differential-equations', 'control-theory', 'optimal-control', 'viscosity-solutions']"
47,Problem 6.16 Ordinary Differential Equations and Dynamical Systems Teschl,Problem 6.16 Ordinary Differential Equations and Dynamical Systems Teschl,,"While reading the book Ordinary Differential Equations and Dynamical Systems by Gerald Teschl, I got stuck at Problem 6.16. It states: Consider the system \begin{eqnarray*} \dot{x} = x-y-x(x^2+y^2)+\dfrac{xy}{\sqrt{x^2+y^2}} \\ \dot{y} = x+y-y(x^2+y^2)-\dfrac{x^2}{\sqrt{x^2+y^2}}. \end{eqnarray*} Show that $(1,0)$ is not stable even though $\lim\limits_{t \to +\infty} |\phi(t,x) - x_0| = 0$ . I was able to show the second part, but i could not show that $(1,0)$ is not stable. By plotting the system on mathlab I could see that it isn't stable, but how do I show it using the definition of stability?","While reading the book Ordinary Differential Equations and Dynamical Systems by Gerald Teschl, I got stuck at Problem 6.16. It states: Consider the system Show that is not stable even though . I was able to show the second part, but i could not show that is not stable. By plotting the system on mathlab I could see that it isn't stable, but how do I show it using the definition of stability?","\begin{eqnarray*}
\dot{x} = x-y-x(x^2+y^2)+\dfrac{xy}{\sqrt{x^2+y^2}} \\
\dot{y} = x+y-y(x^2+y^2)-\dfrac{x^2}{\sqrt{x^2+y^2}}.
\end{eqnarray*} (1,0) \lim\limits_{t \to +\infty} |\phi(t,x) - x_0| = 0 (1,0)","['ordinary-differential-equations', 'analysis', 'stability-in-odes']"
48,Find the maximal interval of existence without actually solving the ODE.,Find the maximal interval of existence without actually solving the ODE.,,"$$y^{\prime}=-\frac{4}{x^{2}}-\frac{1}{x} y+y^{2}, y(1)=1$$ This is a Ricatti equation, $f(x,y)$ is a polynomial in $y$ so it's locally Lipschitz and thus there exists a unique solution to the IVP above, $y_p = \pm \frac{2}{x}$ is a particular solution, we can go ahead and solve it using the change of variable $y = y_p +u$ but is there a way to find the maximal interval of existence without actually doing that?","This is a Ricatti equation, is a polynomial in so it's locally Lipschitz and thus there exists a unique solution to the IVP above, is a particular solution, we can go ahead and solve it using the change of variable but is there a way to find the maximal interval of existence without actually doing that?","y^{\prime}=-\frac{4}{x^{2}}-\frac{1}{x} y+y^{2}, y(1)=1 f(x,y) y y_p = \pm \frac{2}{x} y = y_p +u",['ordinary-differential-equations']
49,Second Linear ODE,Second Linear ODE,,"Confused on how I can approach this ODE, and I just need some help on figuring out what to do $y''-4y'-21=5e ^{-t}$ with initial conditions $y(0)=0$ and $y'(0)=4$ After Laplace transforming, I got: $(\frac{(5)}{(s+1)}{+ 4+\frac{(21)}{(s+1)}}) /( s^2-4s) $ And now I am stuck on what to do here","Confused on how I can approach this ODE, and I just need some help on figuring out what to do with initial conditions and After Laplace transforming, I got: And now I am stuck on what to do here",y''-4y'-21=5e ^{-t} y(0)=0 y'(0)=4 (\frac{(5)}{(s+1)}{+ 4+\frac{(21)}{(s+1)}}) /( s^2-4s) ,['linear-algebra']
50,Basic question about finding flow given eigenvalues,Basic question about finding flow given eigenvalues,,"I am reading the ODE/ dynamical systems book by Hirsh, Devaney, and Smale, Differential Equations, Dynamical Systems, and an Introduction to Chaos (3e). How do I go about finding the flow of a $4\times4$ matrix with eigenvalues with $4$ complex eigenvalues, all of which are purely imaginary? For example, let's say the eigenvalues are: $i\sqrt{2},-i\sqrt{2},i\sqrt{3},-i\sqrt{3}$ . My guess would be a spiral? Is this correct? If so how I would I decide which way the spiral is ""spiraling""? Edit: According to the book (pp.114-115): This is pretty much the same thing so I have my solution. But how do I describe the ""flow""? Is finding a solution enough?","I am reading the ODE/ dynamical systems book by Hirsh, Devaney, and Smale, Differential Equations, Dynamical Systems, and an Introduction to Chaos (3e). How do I go about finding the flow of a matrix with eigenvalues with complex eigenvalues, all of which are purely imaginary? For example, let's say the eigenvalues are: . My guess would be a spiral? Is this correct? If so how I would I decide which way the spiral is ""spiraling""? Edit: According to the book (pp.114-115): This is pretty much the same thing so I have my solution. But how do I describe the ""flow""? Is finding a solution enough?","4\times4 4 i\sqrt{2},-i\sqrt{2},i\sqrt{3},-i\sqrt{3}","['ordinary-differential-equations', 'dynamical-systems']"
51,Finding the particular solution of a differential equation using at least three different methods.,Finding the particular solution of a differential equation using at least three different methods.,,"Find the particular solution $(x^2+6y^2)dx-4xydy=0$ ; when $x=1$ , $y=1$ using at least three different methods. I have done the first two. Can somebody help me with the third method. Method 1: Homogenous Equation Let $y=vx; dy=vdx+xdv$ $(x^2+6x^2v^2)dx-4x(vx)(vdx+xdv)=0$ $(x^2+2x^2v^2)dx-4x^3vdv=0$ $(1+2v^2)dx-4xvdv=0$ $\int\frac{dx}{4x}-\int\frac{v}{1+2v^2}dv=0$ $\ln{x}-\ln{(2v^2+1)}=C$ $\ln{(\frac{x}{2v^2+1})}=\ln{C}$ $\frac{x}{2v^2+1}=\frac{1}{C}$ $C=3$ $3x=2v^2+1$ $3x^3=2y^2+x^2$ $2y^2=x^2(3x-1)$ The particular solution by method 1 is $2y^2=x^2(3x-1)$ . Method 2: Bernoulli Equation $2y\frac{dy}{dx}-\frac{x^2+6y^2}{2x}=0$ $2y\frac{dy}{dx}-\frac{3y^2}{x}=\frac{x}{2}$ Let $v=y^2; dv=2ydy$ $\frac{dv}{dx}-\frac{3v}{x}=\frac{x}{2}$ $P(x)=-3x^{-1}$ ; I.F. $=e^{-3\int x^{-1}dx}=x^{-3}$ $vx^-3=\frac{1}{2}\int\frac{dx}{x^2}$ $2vx^{-3}=-x^{-1}+C^{-1}$ $2y^2x^{-3}+x^{-1}=C^{-1}$ $C=\frac{1}{3}$ $2y^2+x^2=3x^3$ $2y^2=x^2(3x-1)$ The particular solution by method 2 is also $2y^2=x^2(3x-1)$ .","Find the particular solution ; when , using at least three different methods. I have done the first two. Can somebody help me with the third method. Method 1: Homogenous Equation Let The particular solution by method 1 is . Method 2: Bernoulli Equation Let ; I.F. The particular solution by method 2 is also .",(x^2+6y^2)dx-4xydy=0 x=1 y=1 y=vx; dy=vdx+xdv (x^2+6x^2v^2)dx-4x(vx)(vdx+xdv)=0 (x^2+2x^2v^2)dx-4x^3vdv=0 (1+2v^2)dx-4xvdv=0 \int\frac{dx}{4x}-\int\frac{v}{1+2v^2}dv=0 \ln{x}-\ln{(2v^2+1)}=C \ln{(\frac{x}{2v^2+1})}=\ln{C} \frac{x}{2v^2+1}=\frac{1}{C} C=3 3x=2v^2+1 3x^3=2y^2+x^2 2y^2=x^2(3x-1) 2y^2=x^2(3x-1) 2y\frac{dy}{dx}-\frac{x^2+6y^2}{2x}=0 2y\frac{dy}{dx}-\frac{3y^2}{x}=\frac{x}{2} v=y^2; dv=2ydy \frac{dv}{dx}-\frac{3v}{x}=\frac{x}{2} P(x)=-3x^{-1} =e^{-3\int x^{-1}dx}=x^{-3} vx^-3=\frac{1}{2}\int\frac{dx}{x^2} 2vx^{-3}=-x^{-1}+C^{-1} 2y^2x^{-3}+x^{-1}=C^{-1} C=\frac{1}{3} 2y^2+x^2=3x^3 2y^2=x^2(3x-1) 2y^2=x^2(3x-1),"['ordinary-differential-equations', 'solution-verification']"
52,Does this differential equation have a unique solution?,Does this differential equation have a unique solution?,,"Consider the Initial Value Problem $\dfrac{dy}{dx} = \sqrt{y+a}$ where $a > 0$ and $y(0) = 0$ Does this have a unique solution ? If I try to solve it : $\dfrac{dy}{\sqrt{y+a}} = dx$ gives, $2\sqrt{y+a} = x + c  \ldots (1)$ Now, here using Initial Condition $2\sqrt{y+a} = x + 2\sqrt{a}$ or, $y+a = (x+ 2\sqrt{a})^2$ But. in $(1)$ if I simplify the equation $y+a= (\dfrac{x}{2} + \dfrac{c}{2})^2$ and here via initial condition we get two values $c = 2\sqrt{a}$ and $c = -2\sqrt{a}$ and thus this has two different solutions. So, can anyone explain does this Differential Equation has unique solution or multiple solutions ? Thank you.","Consider the Initial Value Problem where and Does this have a unique solution ? If I try to solve it : gives, Now, here using Initial Condition or, But. in if I simplify the equation and here via initial condition we get two values and and thus this has two different solutions. So, can anyone explain does this Differential Equation has unique solution or multiple solutions ? Thank you.",\dfrac{dy}{dx} = \sqrt{y+a} a > 0 y(0) = 0 \dfrac{dy}{\sqrt{y+a}} = dx 2\sqrt{y+a} = x + c  \ldots (1) 2\sqrt{y+a} = x + 2\sqrt{a} y+a = (x+ 2\sqrt{a})^2 (1) y+a= (\dfrac{x}{2} + \dfrac{c}{2})^2 c = 2\sqrt{a} c = -2\sqrt{a},"['real-analysis', 'ordinary-differential-equations']"
53,Limit involving a Differential Equation,Limit involving a Differential Equation,,"Problem: Consider the differential equation $$\dfrac{dy}{dx} + 10y = f(x) , \ > x>0,$$ where $f(x)$ is a continuous function such that $\displaystyle \lim_{x \to \infty} f(x) = 1$ . Find the value of $\displaystyle \lim_{x \to \infty} y(x)$ . My attempt: Since this is a Linear Differential Equation, we can solve for $y(x)$ to get, $$ y(x) = \dfrac{C + \int e^{10x} f(x)}{e^{10x}} $$ which gives, $$ \require{cancel} \lim_{x \to \infty} y(x) = \cancelto{0}{\lim_{x \to \infty} \dfrac{C}{e^{10x}}} + \lim_{x \to \infty}\dfrac{{\int e^{10x} f(x)}}{e^{10x}} =  \lim_{x \to \infty}\dfrac{{\int e^{10x} f(x)}}{e^{10x}} $$ But I don't know how to solve for the last limit involving the integral.","Problem: Consider the differential equation where is a continuous function such that . Find the value of . My attempt: Since this is a Linear Differential Equation, we can solve for to get, which gives, But I don't know how to solve for the last limit involving the integral.","\dfrac{dy}{dx} + 10y = f(x) , \
> x>0, f(x) \displaystyle \lim_{x \to \infty} f(x) = 1 \displaystyle \lim_{x \to \infty} y(x) y(x)  y(x) = \dfrac{C + \int e^{10x} f(x)}{e^{10x}}   \require{cancel} \lim_{x \to \infty} y(x) = \cancelto{0}{\lim_{x \to \infty} \dfrac{C}{e^{10x}}} + \lim_{x \to \infty}\dfrac{{\int e^{10x} f(x)}}{e^{10x}} =  \lim_{x \to \infty}\dfrac{{\int e^{10x} f(x)}}{e^{10x}} ","['calculus', 'ordinary-differential-equations', 'limits']"
54,Show that the equation has a periodic orbit,Show that the equation has a periodic orbit,,I've been learning differential equations and need help with this exercise: $$ x^{\prime \prime}+\left(5 x^{4}-9 x^{2}\right) x^{\prime}+x^{5}=0 $$ I've tried to write it as a system: $$ \begin{array}{l} x^{\prime}=y \\ y^{\prime}=\left(9 x^{2}-5x^4 \right) y-x^{5} \end{array} $$ any help would be appreciated,I've been learning differential equations and need help with this exercise: I've tried to write it as a system: any help would be appreciated,"
x^{\prime \prime}+\left(5 x^{4}-9 x^{2}\right) x^{\prime}+x^{5}=0
 
\begin{array}{l}
x^{\prime}=y \\
y^{\prime}=\left(9 x^{2}-5x^4 \right) y-x^{5}
\end{array}
",['ordinary-differential-equations']
55,Why is the Nth derivate isolated in many textbook definitions?,Why is the Nth derivate isolated in many textbook definitions?,,"My textbook keeps writing definitions in the above form, with the LHS of the equation being the highest order derivative, and the RHS being everything else. Why is this form stressed over a form like this ? Is there any mathematical reason, if it is just a convention then why? Notice that the second equation has a function of t on the RHS and everything else on the LHS.","My textbook keeps writing definitions in the above form, with the LHS of the equation being the highest order derivative, and the RHS being everything else. Why is this form stressed over a form like this ? Is there any mathematical reason, if it is just a convention then why? Notice that the second equation has a function of t on the RHS and everything else on the LHS.",,[]
56,Solving differential equations with trigonometric functions using laplace,Solving differential equations with trigonometric functions using laplace,,"Can this DE be solved using Laplace transform? $$ L\;\frac{d^2\theta}{dt^2} + A\cos(\theta) +g\sin(\theta) = 0 $$ where g , A , L are constants","Can this DE be solved using Laplace transform? where g , A , L are constants","
L\;\frac{d^2\theta}{dt^2} + A\cos(\theta) +g\sin(\theta) = 0
","['ordinary-differential-equations', 'laplace-transform']"
57,Normal modes in physics - solution of set of 2nd order linear differential equations,Normal modes in physics - solution of set of 2nd order linear differential equations,,"In physics we study normal modes for vibrating system. Mathematically speaking we have a set of linear differential equations. For example we have something like this: $$ \begin{align*} m \frac{dx_1}{dt^2} &= k(x_2-x_1) \\  M \frac{dx_2}{dt^2} &= k(x_3-x_2) - k(x_2-x_1) \\ m \frac{dx_3}{dt^2} &= -k(x_3-x_2) \end{align*} $$ We seek solutions for $x_i(t)$ . A general solution is to assume to look for these functions to be sines, cosines or exponents for all $x_i(t)$ having the same frequency $\omega$ . For above example our solution would be something like: $$ \begin{align*} x_1 (t) &= A e^{\omega t + \varphi} \\  x_2 (t) &= B e^{\omega t + \varphi} \\ x_3 (t) &= C e^{\omega t + \varphi} \end{align*} $$ I could not find a clear explanation why we assume that we seek solution with the same $\omega$ (normal modes), and later why the general solution is the superposition of all such solutions (we obviously find . Many thanks for comments and explanation.","In physics we study normal modes for vibrating system. Mathematically speaking we have a set of linear differential equations. For example we have something like this: We seek solutions for . A general solution is to assume to look for these functions to be sines, cosines or exponents for all having the same frequency . For above example our solution would be something like: I could not find a clear explanation why we assume that we seek solution with the same (normal modes), and later why the general solution is the superposition of all such solutions (we obviously find . Many thanks for comments and explanation.","
\begin{align*}
m \frac{dx_1}{dt^2} &= k(x_2-x_1) \\ 
M \frac{dx_2}{dt^2} &= k(x_3-x_2) - k(x_2-x_1) \\
m \frac{dx_3}{dt^2} &= -k(x_3-x_2)
\end{align*}
 x_i(t) x_i(t) \omega 
\begin{align*}
x_1 (t) &= A e^{\omega t + \varphi} \\ 
x_2 (t) &= B e^{\omega t + \varphi} \\
x_3 (t) &= C e^{\omega t + \varphi}
\end{align*}
 \omega","['ordinary-differential-equations', 'physics']"
58,Reduction of order,Reduction of order,,"I have the equation $$x^2 y'' + (2x^2 - 3x) y' + 3y = 0$$ and the solution $$y_1 = x^3 e^{-2x}$$ Using reduction of order: $$y_2 = v y_1$$ I reduced the equation to $$v'' x^5 + v' x^4 = 0$$ so $v(x) = \ln x + C$ . However, I thoroughly checked $$y_2 = x^3 e^{-2x} \ln x$$ and it does not work in the original equation. Any suggestions?","I have the equation and the solution Using reduction of order: I reduced the equation to so . However, I thoroughly checked and it does not work in the original equation. Any suggestions?",x^2 y'' + (2x^2 - 3x) y' + 3y = 0 y_1 = x^3 e^{-2x} y_2 = v y_1 v'' x^5 + v' x^4 = 0 v(x) = \ln x + C y_2 = x^3 e^{-2x} \ln x,"['ordinary-differential-equations', 'reduction-of-order-ode']"
59,Differential equation with partial fraction,Differential equation with partial fraction,,How do you separate this differential equation into a partial fraction? Solve the following differential equation: $$\frac{dy}{dx}=\frac{2y^2-xy+x^2}{xy-x^2}$$,How do you separate this differential equation into a partial fraction? Solve the following differential equation:,\frac{dy}{dx}=\frac{2y^2-xy+x^2}{xy-x^2},"['ordinary-differential-equations', 'partial-fractions']"
60,ODE - Floquet theorem recommendations,ODE - Floquet theorem recommendations,,Can someone please refer me to a good textbook or some papers about the Floquet Theorem (English)? Thanks,Can someone please refer me to a good textbook or some papers about the Floquet Theorem (English)? Thanks,,['ordinary-differential-equations']
61,Solve $(1+x) d y-y d x=0$,Solve,(1+x) d y-y d x=0,"Here is the worked example by the author, $$ \int \frac{d y}{y}=\int \frac{d x}{1+x} $$ $$ \ln |y|=\ln |1+x|+c_{1} $$ $$ |y|=e^{\ln |1+x|+c_{1}}=e^{\ln |1+x|} \cdot e^{c_{1}}=|1+x| e^{c_{1}} $$ $$ y=\pm e^{c_{1}}(1+x) $$ Relabeling $\pm e^{c_{1}}$ as $c$ then gives $y=c(1+x)$ . My question: Shouldn't you say $c=\pm e^{c_1},0$ because $y=0$ is a constant solution? I guess that when $x=-1,y=0$ so I thought maybe that covers the solution $y=0$ , but $(1+x) d y-y d x=0$ can be rearranged to $\frac{d y}{d x}=\frac{y}{1+x}$ and this implies that $x \neq-1$ , so the constant solution $y=0$ cannot be achieved from $y=c(1+x)$ where $c=\pm e^{c_1}$ , is what I think.. Thanks!","Here is the worked example by the author, Relabeling as then gives . My question: Shouldn't you say because is a constant solution? I guess that when so I thought maybe that covers the solution , but can be rearranged to and this implies that , so the constant solution cannot be achieved from where , is what I think.. Thanks!","
\int \frac{d y}{y}=\int \frac{d x}{1+x}
 
\ln |y|=\ln |1+x|+c_{1}
 
|y|=e^{\ln |1+x|+c_{1}}=e^{\ln |1+x|} \cdot e^{c_{1}}=|1+x| e^{c_{1}}
 
y=\pm e^{c_{1}}(1+x)
 \pm e^{c_{1}} c y=c(1+x) c=\pm e^{c_1},0 y=0 x=-1,y=0 y=0 (1+x) d y-y d x=0 \frac{d y}{d x}=\frac{y}{1+x} x \neq-1 y=0 y=c(1+x) c=\pm e^{c_1}",['ordinary-differential-equations']
62,Non-unique solutions for ODE with boundary conditions at infinity,Non-unique solutions for ODE with boundary conditions at infinity,,"Ello, I am looking for solutions to equations such as $$ U ^ \prime (y) - y^{\prime \prime} + \beta y^{\textit{IV}} = 0$$ where $\beta >0$ is a constant and $U(x)$ is a function whose Taylor expansion at the origin equals $ \alpha x^2 + \dots$ for $\alpha >0$ and such that $\lim_{x \to \infty} = -\infty$ . A prototypical example would be the quartic double-well, $$ U(x) = x^2 -x^4$$ . There is a trivial solution $y(x) = 0$ . On intuitive grounds (please see physical background below), as well as noting that the origin is a hyperbolic point for a dynamical system governed by the related system of equations, another solution is expected as well, with boundary conditions $$ \begin{cases} \lim_{x \pm \infty} y(x) = 0  \\  \lim_{x \pm \infty} y^ \prime (x) = 0 \end{cases}$$ This fact does not contradict the unicity theorem, as far as I understand, as the latter deals with boundary conditions on the real line, I believe. Now to my problem, what numerical scheme could be used to get the non-trivial solution? Anything I could think of will pick the trivial solution branch only. Further, I would be grateful for theory references on the possibility of multiple solutions for boundary conditions at infinity. Things would be different if $\beta$ were equal to $0$ . In that case it is easy to determine two boundary conditions (also clarified in the physical background section below) for $x = 0$ , and any numerical method will handle it easily. Physical Background The ODE is the Euler-Lagrange equation for the functional $$ \int _{- \infty} ^{\infty } \frac{1}{2} y^{\prime 2} + U(y) + \frac{\beta}{2} y^{\prime \prime 2} \mathrm{d}x $$ representing the energy of a stretchable beam in the potential $U$ . The first term represents the stretching energy, the second the potential energy, the third the bending energy. Given this physical picture, the trivial solution represents a beam lying in the potential energy ""valley"" at the origin, un-stretched and straight. The non-trivial solution represents a beam lying on the valley as before, then deforming over the potential energy ""bump"", kept in the equilibrium by the pull from the descending potential energy on the other side, and going back to the valley at the origin. If $\beta = 0$ the non-trivial solution is amenable to a simple solution. The energy functional reduces in such case to $$ \int _{- \infty} ^{\infty } \frac{1}{2} y^{\prime 2} + U(y)  \mathrm{d}x $$ which could be interpreted as the action of a particle rolling in a potential energy $-U(x)$ (with $x$ now representing time). The turning point is then easily found by solving the equation $ U(x_1) = 0$ : that is, the particle starts from $x = 0$ , rolls down and up until $x_1$ , and rolls back in a homoclinic orbit. This gives two boundary conditions $$ \begin{cases}  y(0) = x_1  \\   y^ \prime (0) = 0 \end{cases}$$ and these allow any numerical scheme to pick the non-trivial branch. If $\beta \neq 0$ , this simple reasoning fails for the full fourth-order ODE (I also thought about converting the fourth-order ODE to a system of equations, to attempt a similar interpretation for a system of four particles, but did not get anywhere yet, apart confirming that a second non-trivial solution exists). I am hence left with the boundary conditions a infinity, and cannot get any solution other than the trivial one. To sum up, I would like to be pointed towards references for the non-unicity of solutions when boundary conditions at infinity are applied, and to understand how to numerically solve similar problems. The ODE has a physical interpretation and I was unsure whether the question were more apt for Physics StackExchange, or even SciComp StackExchange. I would certainly re-route if advised, I thought to start here as ultimately the question is primarily on the maths, thanks a lot.","Ello, I am looking for solutions to equations such as where is a constant and is a function whose Taylor expansion at the origin equals for and such that . A prototypical example would be the quartic double-well, . There is a trivial solution . On intuitive grounds (please see physical background below), as well as noting that the origin is a hyperbolic point for a dynamical system governed by the related system of equations, another solution is expected as well, with boundary conditions This fact does not contradict the unicity theorem, as far as I understand, as the latter deals with boundary conditions on the real line, I believe. Now to my problem, what numerical scheme could be used to get the non-trivial solution? Anything I could think of will pick the trivial solution branch only. Further, I would be grateful for theory references on the possibility of multiple solutions for boundary conditions at infinity. Things would be different if were equal to . In that case it is easy to determine two boundary conditions (also clarified in the physical background section below) for , and any numerical method will handle it easily. Physical Background The ODE is the Euler-Lagrange equation for the functional representing the energy of a stretchable beam in the potential . The first term represents the stretching energy, the second the potential energy, the third the bending energy. Given this physical picture, the trivial solution represents a beam lying in the potential energy ""valley"" at the origin, un-stretched and straight. The non-trivial solution represents a beam lying on the valley as before, then deforming over the potential energy ""bump"", kept in the equilibrium by the pull from the descending potential energy on the other side, and going back to the valley at the origin. If the non-trivial solution is amenable to a simple solution. The energy functional reduces in such case to which could be interpreted as the action of a particle rolling in a potential energy (with now representing time). The turning point is then easily found by solving the equation : that is, the particle starts from , rolls down and up until , and rolls back in a homoclinic orbit. This gives two boundary conditions and these allow any numerical scheme to pick the non-trivial branch. If , this simple reasoning fails for the full fourth-order ODE (I also thought about converting the fourth-order ODE to a system of equations, to attempt a similar interpretation for a system of four particles, but did not get anywhere yet, apart confirming that a second non-trivial solution exists). I am hence left with the boundary conditions a infinity, and cannot get any solution other than the trivial one. To sum up, I would like to be pointed towards references for the non-unicity of solutions when boundary conditions at infinity are applied, and to understand how to numerically solve similar problems. The ODE has a physical interpretation and I was unsure whether the question were more apt for Physics StackExchange, or even SciComp StackExchange. I would certainly re-route if advised, I thought to start here as ultimately the question is primarily on the maths, thanks a lot."," U ^ \prime (y) - y^{\prime \prime} + \beta y^{\textit{IV}} = 0 \beta >0 U(x)  \alpha x^2 + \dots \alpha >0 \lim_{x \to \infty} = -\infty  U(x) = x^2 -x^4 y(x) = 0  \begin{cases} \lim_{x \pm \infty} y(x) = 0  \\ 
\lim_{x \pm \infty} y^ \prime (x) = 0 \end{cases} \beta 0 x = 0  \int _{- \infty} ^{\infty } \frac{1}{2} y^{\prime 2} + U(y) + \frac{\beta}{2} y^{\prime \prime 2} \mathrm{d}x  U \beta = 0  \int _{- \infty} ^{\infty } \frac{1}{2} y^{\prime 2} + U(y)  \mathrm{d}x  -U(x) x  U(x_1) = 0 x = 0 x_1  \begin{cases}  y(0) = x_1  \\ 
 y^ \prime (0) = 0 \end{cases} \beta \neq 0","['ordinary-differential-equations', 'numerical-methods', 'dynamical-systems', 'euler-lagrange-equation', 'hamilton-equations']"
63,derive eulers integration method from taylor series,derive eulers integration method from taylor series,,"I'm having a hard time understanding how to derive eulers integration method from tayler series expansion I have an ODE and an initial value problem $$ \dfrac{dy}{dt}=f(y,t)  $$ $$ y(0)=y_{0} $$ How does this taylor series expansion relate to eulers integration method $$ y(t+h)=y(t)+hy'(t)+\frac{h^2}{2!}y''(t)+\frac{h^3}{3!}y'''.... $$ What exactly is h in this formula? is it that if i know y(0) i can find y(t+h)? i was also given that if h is small i can write $$ y(t+h)=y(t)+hy'(t)+\frac{h^2}{2!}y''(t)+\frac{h^3}{3!}y'''+O(h^4) $$ Why is that? - i do know that it relates to the truncation error And how do i come from this $$ y(t+h)=y(t)+hy'(t)+\frac{h^2}{2!}y''(t)+\frac{h^3}{3!}y'''+O(h^4) $$ to this $$ y_{k+1}=y_{k}+h\cdot f(y_{k},t_{k}) $$",I'm having a hard time understanding how to derive eulers integration method from tayler series expansion I have an ODE and an initial value problem How does this taylor series expansion relate to eulers integration method What exactly is h in this formula? is it that if i know y(0) i can find y(t+h)? i was also given that if h is small i can write Why is that? - i do know that it relates to the truncation error And how do i come from this to this,"
\dfrac{dy}{dt}=f(y,t) 
 
y(0)=y_{0}
 
y(t+h)=y(t)+hy'(t)+\frac{h^2}{2!}y''(t)+\frac{h^3}{3!}y'''....
 
y(t+h)=y(t)+hy'(t)+\frac{h^2}{2!}y''(t)+\frac{h^3}{3!}y'''+O(h^4)
 
y(t+h)=y(t)+hy'(t)+\frac{h^2}{2!}y''(t)+\frac{h^3}{3!}y'''+O(h^4)
 
y_{k+1}=y_{k}+h\cdot f(y_{k},t_{k})
","['ordinary-differential-equations', 'numerical-methods', 'taylor-expansion']"
64,Why does the constant of integration always come with the independent variable whilst integrating differential equations?,Why does the constant of integration always come with the independent variable whilst integrating differential equations?,,"Say, we had the following differential equation: $$\frac{\mathrm{d}y}{\mathrm{d}x}x = 1+y$$ When we re-arrange the equation and integrate both sides, why will the constant of integration ONLY come with the independent variable, $x$ ? My assumption is that it's because we can think of a function as having an input and an output. The output is the dependent variable, and input is the independent variable. So, all the work is done on the input, to get a desired output. So, the constant should only come with the independent variable. Am I wrong?","Say, we had the following differential equation: When we re-arrange the equation and integrate both sides, why will the constant of integration ONLY come with the independent variable, ? My assumption is that it's because we can think of a function as having an input and an output. The output is the dependent variable, and input is the independent variable. So, all the work is done on the input, to get a desired output. So, the constant should only come with the independent variable. Am I wrong?",\frac{\mathrm{d}y}{\mathrm{d}x}x = 1+y x,"['integration', 'ordinary-differential-equations', 'indefinite-integrals']"
65,How to transform a Cauchy-Euler equation into a constant coefficients equation?,How to transform a Cauchy-Euler equation into a constant coefficients equation?,,"I have doubts with a problem that goes like this: Let's consider the second order linear differential equation $ y''(x)+R(x)·y'+S(x)·y=Q(x) $ a) How must the change of variable $z = z(x)$ be so that the equation is transformed into one of constant coefficients? b) Apply the above result to solve the equation: $ y''(x)+(1+4e^{x})·y'+3e^{2x}·y=e^{2(x+e^{x})} $ I understand what the problem ask I don't know at all how to do it. I even wonder if the statement is right because the condition I get it's a bit abstract. It's a Cauchy-Euler differential equation, so that: $$ \frac {dy}{dx} = \frac {dy}{dz}· \frac {dz}{dx} = \dot yz'$$ $$ \frac {d^2 y}{dx^2} = ... = \ddot{y}z'^{2}+ \dot yz'' $$ And the differential equation turns to: $$ \ddot{y}z'^{2}+ \dot yz'' + R(x)·\dot y z' + S(x)y = Q(x) $$ $$ \ddot{y}z'^{2}+ \dot y (z'' + R(x)z') + S(x)y = Q(x) $$ If the equation should transformed into a constant coefficients equation, I think it means: $$ z'' + R(x)z' = K $$ But I don'tknow what to do from here.. I supposse (in case this result is fine) I can't make K=0 (because it's just a special case) but this is the only value that let me develop a little but more this condition. Please, anybody know whether this result is right or it has some errors? How could I solve the problem please? (Not solve it completely, of course. Just a tip to let me continue doing it). Thanks a lot.","I have doubts with a problem that goes like this: Let's consider the second order linear differential equation a) How must the change of variable be so that the equation is transformed into one of constant coefficients? b) Apply the above result to solve the equation: I understand what the problem ask I don't know at all how to do it. I even wonder if the statement is right because the condition I get it's a bit abstract. It's a Cauchy-Euler differential equation, so that: And the differential equation turns to: If the equation should transformed into a constant coefficients equation, I think it means: But I don'tknow what to do from here.. I supposse (in case this result is fine) I can't make K=0 (because it's just a special case) but this is the only value that let me develop a little but more this condition. Please, anybody know whether this result is right or it has some errors? How could I solve the problem please? (Not solve it completely, of course. Just a tip to let me continue doing it). Thanks a lot.", y''(x)+R(x)·y'+S(x)·y=Q(x)  z = z(x)  y''(x)+(1+4e^{x})·y'+3e^{2x}·y=e^{2(x+e^{x})}   \frac {dy}{dx} = \frac {dy}{dz}· \frac {dz}{dx} = \dot yz'  \frac {d^2 y}{dx^2} = ... = \ddot{y}z'^{2}+ \dot yz''   \ddot{y}z'^{2}+ \dot yz'' + R(x)·\dot y z' + S(x)y = Q(x)   \ddot{y}z'^{2}+ \dot y (z'' + R(x)z') + S(x)y = Q(x)   z'' + R(x)z' = K ,['ordinary-differential-equations']
66,Help with understanding Duffing's oscillator,Help with understanding Duffing's oscillator,,"I'm working through some notes regarding analysis of some weakly perturbed oscillators. Among these is Duffing's oscillator which is governed by $$ \frac{d^2y}{dt^2}+y+\epsilon y^3=0, \; \; 0<\epsilon << 1 $$ Now to leading we have \begin{align*}    \frac{d^2y_0}{dt^2}+y_0=0  \end{align*} with general solution $$ y_0 = a\cos t + b \sin t$$ which can be written w.l.o.g as $ y_0 = c \cos(t+\phi), \; \; c>0$ . So far so good. But now to understand the nature of the perturbation the notes make use of the Poincare-Lindstedt method, which relies on the observation that the period of the perturbed oscillator is $$2\pi(1+o(1))$$ as $\epsilon \rightarrow 0$ . Here is my first question: how exactly was this observation made? Next we define $Y(\tau; \epsilon) = y(t; \epsilon)$ where $\tau = t/\chi(\epsilon)$ and $\chi(\epsilon)$ is such that $Y$ is $2\pi$ periodic in $\tau$ . Then, expanding $Y$ and $\chi$ in powers of $\epsilon$ we have $$Y(\tau;\epsilon) \sim Y_0(\tau;\epsilon)+\epsilon Y_1(\tau;\epsilon)+...$$ and $$\chi(\epsilon) \sim 1+\epsilon\chi_1 + \epsilon^2\chi_2+...$$ Here the $Y_i$ are $2\pi$ periodic in $\tau$ . To leading order the solution has the same form, $Y_0 = a\cos(\tau + \phi)$ . Going up to first order in $\epsilon$ , we have \begin{align*} \frac{d^2Y_1}{d\tau^2} + Y_1 &= -2\chi_1Y_0-Y_0^3 \\  &= -2\chi_1a\cos(\tau+\phi) -\frac{a^3}{4}\left \{3\cos(\tau+\phi)+\cos\left(3(\tau+\phi)\right)\right\}, \end{align*} where we have substituted for $Y_0$ and used a Fourier expansion. Then here is where I really have trouble. At this point then notes say that in order for $Y_1$ to be $2\pi$ perioduc in $\tau$ we must eliminate $\cos(\tau+\phi)$ terms, which gives us an algebraic expression for $\chi_1$ . But why must we eliminate such terms in order for $Y_1$ to be $2\pi$ periodic? I don't understand this.","I'm working through some notes regarding analysis of some weakly perturbed oscillators. Among these is Duffing's oscillator which is governed by Now to leading we have with general solution which can be written w.l.o.g as . So far so good. But now to understand the nature of the perturbation the notes make use of the Poincare-Lindstedt method, which relies on the observation that the period of the perturbed oscillator is as . Here is my first question: how exactly was this observation made? Next we define where and is such that is periodic in . Then, expanding and in powers of we have and Here the are periodic in . To leading order the solution has the same form, . Going up to first order in , we have where we have substituted for and used a Fourier expansion. Then here is where I really have trouble. At this point then notes say that in order for to be perioduc in we must eliminate terms, which gives us an algebraic expression for . But why must we eliminate such terms in order for to be periodic? I don't understand this."," \frac{d^2y}{dt^2}+y+\epsilon y^3=0, \; \; 0<\epsilon << 1  \begin{align*}
   \frac{d^2y_0}{dt^2}+y_0=0
 \end{align*}  y_0 = a\cos t + b \sin t  y_0 = c \cos(t+\phi), \; \; c>0 2\pi(1+o(1)) \epsilon \rightarrow 0 Y(\tau; \epsilon) = y(t; \epsilon) \tau = t/\chi(\epsilon) \chi(\epsilon) Y 2\pi \tau Y \chi \epsilon Y(\tau;\epsilon) \sim Y_0(\tau;\epsilon)+\epsilon Y_1(\tau;\epsilon)+... \chi(\epsilon) \sim 1+\epsilon\chi_1 + \epsilon^2\chi_2+... Y_i 2\pi \tau Y_0 = a\cos(\tau + \phi) \epsilon \begin{align*}
\frac{d^2Y_1}{d\tau^2} + Y_1 &= -2\chi_1Y_0-Y_0^3 \\ 
&= -2\chi_1a\cos(\tau+\phi) -\frac{a^3}{4}\left \{3\cos(\tau+\phi)+\cos\left(3(\tau+\phi)\right)\right\},
\end{align*} Y_0 Y_1 2\pi \tau \cos(\tau+\phi) \chi_1 Y_1 2\pi","['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
67,Solution to an inexact differential equation with the difference between partial derivatives not single variable,Solution to an inexact differential equation with the difference between partial derivatives not single variable,,"How can I find the general solution to the equation $$\left(x^2+xy+\frac{y^2}{x}\right)dx+(x^2+xy-y)dy=0$$ Note that, by using exact differentials, I could reduce it to $(x+y)d(x+y)=yd\left(\frac{y}{x}\right)$ , but could not solve it. The difference between $M_y-N_x=\frac{2y}{x}-x-y$ , where $M=x^2+xy+\frac{y^2}{x}$ , $N=x^2+xy-y$ . Any hints? Thanks beforehand.","How can I find the general solution to the equation Note that, by using exact differentials, I could reduce it to , but could not solve it. The difference between , where , . Any hints? Thanks beforehand.",\left(x^2+xy+\frac{y^2}{x}\right)dx+(x^2+xy-y)dy=0 (x+y)d(x+y)=yd\left(\frac{y}{x}\right) M_y-N_x=\frac{2y}{x}-x-y M=x^2+xy+\frac{y^2}{x} N=x^2+xy-y,"['real-analysis', 'calculus', 'ordinary-differential-equations']"
68,Finding eigenvalues and eigenfunctions of differential equation,Finding eigenvalues and eigenfunctions of differential equation,,"I need to write the following d.e. in Sturm-Liouville form and find the eigenvalues and eigenfunctions. $$\frac {\mathrm d ^2 y}{\mathrm d x^2} + 7 \frac {\mathrm d y} {\mathrm d x} + (e^{3x} + \lambda)y = 0$$ where $y(0)=y(1)=0$ . I have written it as $-(p(x)y')' + q(x)y = \lambda r(x) y$ with $p(x) = e^{7x}$ , $q(x) = -e^{10x}$ and $r(x) = e^{7x}$ . I am not sure how to find the eigenvalues though. I know that the boundary conditions are relevant but I'm not sure where to start in terms of finding the eigenfunctions. How do I know by looking at the d.e. when it is possible to find them?","I need to write the following d.e. in Sturm-Liouville form and find the eigenvalues and eigenfunctions. where . I have written it as with , and . I am not sure how to find the eigenvalues though. I know that the boundary conditions are relevant but I'm not sure where to start in terms of finding the eigenfunctions. How do I know by looking at the d.e. when it is possible to find them?",\frac {\mathrm d ^2 y}{\mathrm d x^2} + 7 \frac {\mathrm d y} {\mathrm d x} + (e^{3x} + \lambda)y = 0 y(0)=y(1)=0 -(p(x)y')' + q(x)y = \lambda r(x) y p(x) = e^{7x} q(x) = -e^{10x} r(x) = e^{7x},"['ordinary-differential-equations', 'eigenfunctions', 'sturm-liouville']"
69,"Stability of equilibrium points in Gradient Systems , Lyapunov functions and Hartman-Grobman Theorem","Stability of equilibrium points in Gradient Systems , Lyapunov functions and Hartman-Grobman Theorem",,"So I have learned about Lyapunov theory to study the stability of equilibrium points are now we want to apply it to the study of gradient systems. So suppose we have $x'=-\nabla V(x)$ and we have that $a$ is an equilibrium point for this equation that is $\nabla V(a)=0$ .  Now if we have that $a$ is an isolated local minimum we can use the Lyapunov function $H(x):=V(x)-V(a)$ to see that this is an asymptotically stable point. If $a$ is an isolated local maximum we can use $-H(x)$ to see that it is unstable, but what happens if $a$ is an isolated saddle point ? How can we study the stability in this case? One way I thought about it would be to use the Hartman-Grobman theorem and we know that the linearization of this dynamical system will be unstable and so since they have homeomorphic flows I guess this would also be unstable, but I am not completely sure this works, or if there is another way to see this. I guess my biggest doubt is that if we can use the Hartman-Grobman theorem to study the stability of the sistem from the linearized equation. Any help is appreciated, thanks in advance.","So I have learned about Lyapunov theory to study the stability of equilibrium points are now we want to apply it to the study of gradient systems. So suppose we have and we have that is an equilibrium point for this equation that is .  Now if we have that is an isolated local minimum we can use the Lyapunov function to see that this is an asymptotically stable point. If is an isolated local maximum we can use to see that it is unstable, but what happens if is an isolated saddle point ? How can we study the stability in this case? One way I thought about it would be to use the Hartman-Grobman theorem and we know that the linearization of this dynamical system will be unstable and so since they have homeomorphic flows I guess this would also be unstable, but I am not completely sure this works, or if there is another way to see this. I guess my biggest doubt is that if we can use the Hartman-Grobman theorem to study the stability of the sistem from the linearized equation. Any help is appreciated, thanks in advance.",x'=-\nabla V(x) a \nabla V(a)=0 a H(x):=V(x)-V(a) a -H(x) a,"['ordinary-differential-equations', 'dynamical-systems']"
70,"What is the solution on the interval (-3, 3) for this ODE?","What is the solution on the interval (-3, 3) for this ODE?",,"I took Differential Equations a few years ago and am reading back through my old book in a little more detail, especially with regard to intervals of validity for solutions. I found one of the example problems which finds a solution only for the intervals (- $\infty$ , -3) and (3, $\infty$ ) using the standard integrating factor method. One of the section problems is to go back and find the solution on the interval (-3, 3). This is the equation: $$(x^2 - 9)\frac{dy}{dx}+xy=0$$ I've banged my head against the wall for a little bit before I remembered to check the Existence and Uniqueness theorem for IVP's to see if a solution exists on the interval: $$f(x,y)=-\frac{x}{x^2-9}y$$ and $$\frac{\partial f}{\partial y}=-\frac{x}{x^2-9}$$ Both of which are defined everywhere but x=-3 and x=3 so I believe a unique solution must exist on all of the intervals I mentioned above. I follow the books solution fine for the example but in this case I don't see any way to move forward on (-3,3) without bringing in complex numbers which I'm not very confident with. Further plugging the equation into Symbolab gives a plot with no solution on this interval. Usually I'm missing something simple but could someone enlighten me as to what it is?","I took Differential Equations a few years ago and am reading back through my old book in a little more detail, especially with regard to intervals of validity for solutions. I found one of the example problems which finds a solution only for the intervals (- , -3) and (3, ) using the standard integrating factor method. One of the section problems is to go back and find the solution on the interval (-3, 3). This is the equation: I've banged my head against the wall for a little bit before I remembered to check the Existence and Uniqueness theorem for IVP's to see if a solution exists on the interval: and Both of which are defined everywhere but x=-3 and x=3 so I believe a unique solution must exist on all of the intervals I mentioned above. I follow the books solution fine for the example but in this case I don't see any way to move forward on (-3,3) without bringing in complex numbers which I'm not very confident with. Further plugging the equation into Symbolab gives a plot with no solution on this interval. Usually I'm missing something simple but could someone enlighten me as to what it is?","\infty \infty (x^2 - 9)\frac{dy}{dx}+xy=0 f(x,y)=-\frac{x}{x^2-9}y \frac{\partial f}{\partial y}=-\frac{x}{x^2-9}","['ordinary-differential-equations', 'integrating-factor']"
71,How to find if particle is moving in clockwise direction or not?,How to find if particle is moving in clockwise direction or not?,,"I've been given a vector equation in the form of r(t), how do i find if the particle is going in clockwise direction or in anticlockwise direction?","I've been given a vector equation in the form of r(t), how do i find if the particle is going in clockwise direction or in anticlockwise direction?",,"['ordinary-differential-equations', 'vectors']"
72,The rate of change in Newton's Law of Cooling,The rate of change in Newton's Law of Cooling,,"I know how to solve the following problem but I have a confusion concerning the sentence in bold. A metal bar is taken from an inside room and dropped into a large container of boiling water.The initial temperature of the bar was 20° C. If it is known that the temperature of the the bar increases 2°C in 1 second , How long will it take the bar to reach 98° C? The solution of the problem is as following The differential equation is $\frac{dT}{dt}=k(T_m-T)$ Its solution is $T=Ce^{-kt}+T_m$ Substitute by $T_m=100$ and by the initial condition $T(0)=20$ in the solution of the DE $T(0)=100+C$ Hence $C=-80$ Hence $T=100-80e^{-kt}$ To get the constant k, we should substitute by the condition $T(1)=22$ in the solution $22=100-80e^{-k}$ Hence $k=0.02532$ Then, we can substitute by $T=98$ in the solution to get the required time. My question is: Can we deal with the information ""the temperature of the the bar increases 2° in 1 second"" that this is the rate of change at $T=0$ ? I mean: $\frac{dT}{dt}\big|_{t=0}=2$ Hence, we can substitute in the DE $\frac{dT}{dt}\big|_{t=0}=k(100-T(0))$ $2=k(100-20)$ Hence $k=0.025$ We got finally approximately the same value for the constant k. Note: I think it is not allowed to deal with the given info as a rate because we divided the increase in temperature by the duration which is not exactly the derivative or the rate; we should divide by a duration that is infinitely small. Also, I think that if the given info is the rate, it should be written in the form  2°c/sec. I felt confused because when I deal with the given info as a rate, it gives me approximately the same solution.","I know how to solve the following problem but I have a confusion concerning the sentence in bold. A metal bar is taken from an inside room and dropped into a large container of boiling water.The initial temperature of the bar was 20° C. If it is known that the temperature of the the bar increases 2°C in 1 second , How long will it take the bar to reach 98° C? The solution of the problem is as following The differential equation is Its solution is Substitute by and by the initial condition in the solution of the DE Hence Hence To get the constant k, we should substitute by the condition in the solution Hence Then, we can substitute by in the solution to get the required time. My question is: Can we deal with the information ""the temperature of the the bar increases 2° in 1 second"" that this is the rate of change at ? I mean: Hence, we can substitute in the DE Hence We got finally approximately the same value for the constant k. Note: I think it is not allowed to deal with the given info as a rate because we divided the increase in temperature by the duration which is not exactly the derivative or the rate; we should divide by a duration that is infinitely small. Also, I think that if the given info is the rate, it should be written in the form  2°c/sec. I felt confused because when I deal with the given info as a rate, it gives me approximately the same solution.",\frac{dT}{dt}=k(T_m-T) T=Ce^{-kt}+T_m T_m=100 T(0)=20 T(0)=100+C C=-80 T=100-80e^{-kt} T(1)=22 22=100-80e^{-k} k=0.02532 T=98 T=0 \frac{dT}{dt}\big|_{t=0}=2 \frac{dT}{dt}\big|_{t=0}=k(100-T(0)) 2=k(100-20) k=0.025,"['ordinary-differential-equations', 'mathematical-modeling']"
73,Making sense of $x'=Ax\implies x(t)=e^{Jt}x(0)$ when $A$ is nondiagonalizable,Making sense of  when  is nondiagonalizable,x'=Ax\implies x(t)=e^{Jt}x(0) A,"How do I make sense of $x'=Ax$ is solved by $x(t)=e^{Jt}x(0)$ when $A$ is nondiagonalizable. When we have an $n\times n$ system of ODE's, $\dfrac{dx}{dt}=Ax$ and if $A=VD_AV^{-1}$ is diagonalizable. Substitute $x=Vy$ , $$ \dfrac{dx}{dt}=Ax\implies V\dfrac{dy}{dt}=AVy\implies \dfrac{dy}{dt}=V^{-1}AVy=D_Ay\\ \begin{bmatrix}y_1'\\\vdots\\y_n'\end{bmatrix}=\begin{bmatrix}\lambda_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\lambda_n\end{bmatrix}\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}\implies y_i(t)=c_ie^{\lambda_it}\\ x(t)=Vy(t)=\begin{bmatrix}v_1&\cdots&v_n\end{bmatrix}\begin{bmatrix}c_1e^{\lambda_1t}\\\vdots\\c_ne^{\lambda_nt}\end{bmatrix}\implies x(t)=c_1v_1e^{\lambda_1t}+\cdots+c_nv_ne^{\lambda_nt}\\ x(0)=V\begin{bmatrix}c_1\\\vdots\\c_n\end{bmatrix}=Vc\implies c=V^{-1}x(0) $$ $$ x(t)=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}\begin{bmatrix}c_1\\\vdots\\c_n\end{bmatrix}=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}c=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}V^{-1}x(0)=e^{At}x(0) $$ where $e^{At}=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}V^{-1}$ I think it makes sense why we define $e^{At}=Ve^{D_At}V^{-1}$ the way it is and how it comes into picture and we can also prove the Taylor series expansion is the same thing. When $A$ is nondiagonalizable $A$ can be decomposed into $A=XJX^{-1}\implies J=X^{-1}AX$ is called the Jordan canonical form and $X$ contains generalized eigenvectors $x_k$ such that $Ax_k=λ_kx_k$ , or $Ax_k=λ_kx_k+x_{k−1}$ and $J=X^{-1}AX=\begin{bmatrix}J_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&J_s\end{bmatrix}=J_1\oplus\cdots\oplus J_s$ where $J_i=\begin{bmatrix}\lambda_1&1&0&\cdots&0\\0&\lambda_2&1&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&\lambda_i\end{bmatrix}$ Substitute $x=Az$ $$ x'=Ax\implies Bz'=ABz\\ z'=B^{-1}ABz=Jz\\ \begin{bmatrix}z'_1\\\vdots\\z'_n\end{bmatrix}=\begin{bmatrix}J_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&J_s\end{bmatrix}\begin{bmatrix}y_1\\\vdots\\z_n\end{bmatrix} $$ I don't think it goes anywhere with it ! In this case how do I make sense of the fact that $x(t)=e^{Jt}x(0)$ solves $x'=Ax$ similar to the one we obtained above ?","How do I make sense of is solved by when is nondiagonalizable. When we have an system of ODE's, and if is diagonalizable. Substitute , where I think it makes sense why we define the way it is and how it comes into picture and we can also prove the Taylor series expansion is the same thing. When is nondiagonalizable can be decomposed into is called the Jordan canonical form and contains generalized eigenvectors such that , or and where Substitute I don't think it goes anywhere with it ! In this case how do I make sense of the fact that solves similar to the one we obtained above ?","x'=Ax x(t)=e^{Jt}x(0) A n\times n \dfrac{dx}{dt}=Ax A=VD_AV^{-1} x=Vy 
\dfrac{dx}{dt}=Ax\implies V\dfrac{dy}{dt}=AVy\implies \dfrac{dy}{dt}=V^{-1}AVy=D_Ay\\
\begin{bmatrix}y_1'\\\vdots\\y_n'\end{bmatrix}=\begin{bmatrix}\lambda_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\lambda_n\end{bmatrix}\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}\implies y_i(t)=c_ie^{\lambda_it}\\
x(t)=Vy(t)=\begin{bmatrix}v_1&\cdots&v_n\end{bmatrix}\begin{bmatrix}c_1e^{\lambda_1t}\\\vdots\\c_ne^{\lambda_nt}\end{bmatrix}\implies x(t)=c_1v_1e^{\lambda_1t}+\cdots+c_nv_ne^{\lambda_nt}\\
x(0)=V\begin{bmatrix}c_1\\\vdots\\c_n\end{bmatrix}=Vc\implies c=V^{-1}x(0)
 
x(t)=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}\begin{bmatrix}c_1\\\vdots\\c_n\end{bmatrix}=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}c=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}V^{-1}x(0)=e^{At}x(0)
 e^{At}=V\begin{bmatrix}e^{\lambda_1t}&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&e^{\lambda_nt}\end{bmatrix}V^{-1} e^{At}=Ve^{D_At}V^{-1} A A A=XJX^{-1}\implies J=X^{-1}AX X x_k Ax_k=λ_kx_k Ax_k=λ_kx_k+x_{k−1} J=X^{-1}AX=\begin{bmatrix}J_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&J_s\end{bmatrix}=J_1\oplus\cdots\oplus J_s J_i=\begin{bmatrix}\lambda_1&1&0&\cdots&0\\0&\lambda_2&1&\cdots&0\\\vdots&\vdots&\vdots&\ddots&\vdots\\0&0&0&\cdots&\lambda_i\end{bmatrix} x=Az 
x'=Ax\implies Bz'=ABz\\
z'=B^{-1}ABz=Jz\\
\begin{bmatrix}z'_1\\\vdots\\z'_n\end{bmatrix}=\begin{bmatrix}J_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&J_s\end{bmatrix}\begin{bmatrix}y_1\\\vdots\\z_n\end{bmatrix}
 x(t)=e^{Jt}x(0) x'=Ax","['linear-algebra', 'ordinary-differential-equations', 'jordan-normal-form']"
74,How to solve $-4xy + x^2 - y^2 + 4x^2 \frac{dy}{dx} = 0$?,How to solve ?,-4xy + x^2 - y^2 + 4x^2 \frac{dy}{dx} = 0,"We want to find the exact solution of the following ODE: $$ -4xy + x^2 - y^2 + 4x^2 \frac{dy}{dx} = 0 \quad \quad y = 2, x = 1 $$ This is my attempt: We need to match this form: $$M(x,y) + N(x,y) \frac{dy}{dx} = 0$$ Then we find $M(x,y)$ and $N(x,y)$ : $$M(x,y) = -4xy + x^2 -y^2$$ $$N(x,y) = 4x^2$$ From this we get two differential equations: $$\frac{d\Psi(x,y)}{dx} = M(x,y)$$ $$\frac{d\Psi(x,y)}{dy} = N(x,y)$$ It is not important which one we solve by integrating. Here we integrate the first one. $$\Psi(x,y) = \int M(x,y) dx  $$ $$= \int -4xy + x^2 - y^2  dx $$ $$= -2x^2y + \frac{x^3}{3} -xy^2  + C(y) = \Psi(x,y)$$ So far so good. From this onward I try to methods and both fail. First ATTEMPT: Integrating the other differential equation we get $$ 4x^2y  + D(x) = \Psi(x,y)$$ comparing the two $$ 4x^2y  + D(x) = \Psi(x,y) =-2x^2y + \frac{x^3}{3} -xy^2  + C(y)  $$ not sure what to do next since it is not easy to deduce the $C(y)$ and $D(x)$ from here. Second ATTEMPT: Now we use the OTHER differential equation: $$\frac{d\Psi(x,y)}{dy} = N(x,y)= 4x^2$$ and we use the $\Psi(x,y)$ that we found. We now need to find $C(y)$ . $$\frac{d\Psi(x,y)}{dy} = 4x^2$$ $$ -2x^2 -2xy + \frac{dC(y)}{dy}= 4x^2$$ $$ -6x^2 -2xy + \frac{dC(y)}{dy}= 0$$ integrating everything with respect to y with constant of integration K(x), we get $$ -6x^2y -xy^2 + C(y) + K(x) = 0 $$ solve the above for $C(y)$ $$ C(y) = 6x^2y +  xy^2 - K(x) $$ and substitute into $\Psi(x,y)$ to get $$\Psi(x,y) = -2x^2y + \frac{x^3}{3} -xy^2 + 6x^2y +  xy^2 - K(x)  = 0   $$ $$\Psi(x,y) =  \frac{x^3}{3}  + 4x^2y  -K(x)  = 0   $$ we differentiate with respect to x $$\frac{d\Psi(x,y)}{dx} = M(x,y)$$ $$ x^2 + 8xy - \frac{dK(x)}{dx}= -4xy + x^2 -y^2 $$ $$ y^2 + 12xy  =  \frac{dK(x)}{dx} $$ $$ xy^2 + 6x^2y + Q  =  K(x) $$ $$\Psi(x,y) =  \frac{x^3}{3}  + 4x^2y - xy^2 - 6x^2y    + Q    = 0   $$ $$\Psi(x,y) =  \frac{x^3}{3}  -2x^2y - xy^2  + Q  = 0   $$ However I think here we get into an infinite loop because $Q$ is actually $Q(y)$ which needs to be determined using a similiar integration ... so again I don't know what to do. The final solution $\Psi(x,y)$ does not agree with:: $$\frac{d\Psi(x,y)}{dx} = M(x,y)$$ $$\frac{d\Psi(x,y)}{dy} = N(x,y)$$ What am I doing wrong?","We want to find the exact solution of the following ODE: This is my attempt: We need to match this form: Then we find and : From this we get two differential equations: It is not important which one we solve by integrating. Here we integrate the first one. So far so good. From this onward I try to methods and both fail. First ATTEMPT: Integrating the other differential equation we get comparing the two not sure what to do next since it is not easy to deduce the and from here. Second ATTEMPT: Now we use the OTHER differential equation: and we use the that we found. We now need to find . integrating everything with respect to y with constant of integration K(x), we get solve the above for and substitute into to get we differentiate with respect to x However I think here we get into an infinite loop because is actually which needs to be determined using a similiar integration ... so again I don't know what to do. The final solution does not agree with:: What am I doing wrong?","
-4xy + x^2 - y^2 + 4x^2 \frac{dy}{dx} = 0
\quad
\quad
y = 2, x = 1
 M(x,y) + N(x,y) \frac{dy}{dx} = 0 M(x,y) N(x,y) M(x,y) = -4xy + x^2 -y^2 N(x,y) = 4x^2 \frac{d\Psi(x,y)}{dx} = M(x,y) \frac{d\Psi(x,y)}{dy} = N(x,y) \Psi(x,y) = \int M(x,y) dx   = \int -4xy + x^2 - y^2  dx  = -2x^2y + \frac{x^3}{3} -xy^2  + C(y) = \Psi(x,y)  4x^2y  + D(x) = \Psi(x,y)  4x^2y  + D(x) = \Psi(x,y) =-2x^2y + \frac{x^3}{3} -xy^2  + C(y)   C(y) D(x) \frac{d\Psi(x,y)}{dy} = N(x,y)= 4x^2 \Psi(x,y) C(y) \frac{d\Psi(x,y)}{dy} = 4x^2  -2x^2 -2xy + \frac{dC(y)}{dy}= 4x^2  -6x^2 -2xy + \frac{dC(y)}{dy}= 0  -6x^2y -xy^2 + C(y) + K(x) = 0  C(y)  C(y) = 6x^2y +  xy^2 - K(x)  \Psi(x,y) \Psi(x,y) = -2x^2y + \frac{x^3}{3} -xy^2 + 6x^2y +  xy^2 - K(x)  = 0    \Psi(x,y) =  \frac{x^3}{3}  + 4x^2y  -K(x)  = 0    \frac{d\Psi(x,y)}{dx} = M(x,y)  x^2 + 8xy - \frac{dK(x)}{dx}= -4xy + x^2 -y^2   y^2 + 12xy  =  \frac{dK(x)}{dx}   xy^2 + 6x^2y + Q  =  K(x)  \Psi(x,y) =  \frac{x^3}{3}  + 4x^2y - xy^2 - 6x^2y    + Q    = 0    \Psi(x,y) =  \frac{x^3}{3}  -2x^2y - xy^2  + Q  = 0    Q Q(y) \Psi(x,y) \frac{d\Psi(x,y)}{dx} = M(x,y) \frac{d\Psi(x,y)}{dy} = N(x,y)","['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'indefinite-integrals']"
75,Finding a constant which makes a Homogeneous ODE into a Non-Homogeneous ODE,Finding a constant which makes a Homogeneous ODE into a Non-Homogeneous ODE,,"I have a Non-Homogeneous ODE of the form $y'' + 8y' + 17y = 5$ which I have obtained the solution $y(x) = ae^{-4x}[\cos(x) + i\sin(x)] + be^{-4x}[\cos(x)-i\sin(x)] + \frac{5}{17}$ where $a$ and $b$ are complex constants. I am unsure of how to find a constant, $c$ , which satisfies the equation $z(x) = y(x) - c$ where $c$ is a real constant and $z(x)$ is a Homogeneous Differential Equation. I originally thought that $c = \frac{5}{17}$ but that seems too simple. How should I go about finding $c$ ?","I have a Non-Homogeneous ODE of the form which I have obtained the solution where and are complex constants. I am unsure of how to find a constant, , which satisfies the equation where is a real constant and is a Homogeneous Differential Equation. I originally thought that but that seems too simple. How should I go about finding ?",y'' + 8y' + 17y = 5 y(x) = ae^{-4x}[\cos(x) + i\sin(x)] + be^{-4x}[\cos(x)-i\sin(x)] + \frac{5}{17} a b c z(x) = y(x) - c c z(x) c = \frac{5}{17} c,"['ordinary-differential-equations', 'homogeneous-equation']"
76,When is the system of first order differential equations asymptotically stable?,When is the system of first order differential equations asymptotically stable?,,"Given the system $$\mathbf{x}'=A\mathbf{x} $$ where $A=\begin{bmatrix}a &0 & 4 \\ -1 & -1 & 0 \\ -2-a & 0 & -3 \end{bmatrix}$ In what interval of $a$ is the system asymptotically stable, and for what value of $a$ is the system stable/unstable if such a case even exists? Attempt For a system to be asymptotically stable the real part of all eigenvalues must be negative. $$\Re(\lambda)<0, \: \: \: \text{for all} \: \: \lambda $$ Since we are dealing with a $3\times3$ matrix, I used maple to find the eigenvalues from the system matrix. I also tried to use Maple to solve the inequality case for the eigenvalues, but I don't think I'm getting correct results. The results I get from Maple state that the system is asymptotically stable when $$-8 < a \leq5-4\sqrt{3} $$ I spoke with my peers, and they said that this interval of $a$ is wrong. I posted my Maple document below. Can anyone see what is going wrong?","Given the system where In what interval of is the system asymptotically stable, and for what value of is the system stable/unstable if such a case even exists? Attempt For a system to be asymptotically stable the real part of all eigenvalues must be negative. Since we are dealing with a matrix, I used maple to find the eigenvalues from the system matrix. I also tried to use Maple to solve the inequality case for the eigenvalues, but I don't think I'm getting correct results. The results I get from Maple state that the system is asymptotically stable when I spoke with my peers, and they said that this interval of is wrong. I posted my Maple document below. Can anyone see what is going wrong?","\mathbf{x}'=A\mathbf{x}  A=\begin{bmatrix}a &0 & 4 \\ -1 & -1 & 0 \\ -2-a & 0 & -3 \end{bmatrix} a a \Re(\lambda)<0, \: \: \: \text{for all} \: \: \lambda  3\times3 -8 < a \leq5-4\sqrt{3}  a","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'systems-of-equations', 'maple', 'stability-theory']"
77,Can $ y(t)=\left((t-1)^2+1\right)\cdot \cos(t) $ be a solution for this ODE,Can  be a solution for this ODE, y(t)=\left((t-1)^2+1\right)\cdot \cos(t) ,"I've been asked if $$ y(t)=\left((t-1)^2+1\right)\cdot \cos(t) $$ can be a solution for the ODE: $$ y''+y+(y-\cos(t))P(t,y,y') = 0 $$ when $P$ is smooth. So I concluded that there exists a unique solution to the ODE, by Peano's theorem if: $$ y_{1}=y,\quad y_{2}=y'. $$ Then the ODE is of the form: $$ y_{2}'=-y_{1}-P(t,y_{1},y_{2})(y_{1}-\cos(t)). $$ $P$ is smooth and a function of $y_{2}$ and $y_{1}$ , therefore the ODE has a unique solution, but I don't know how to continue any further.  Any help/hints would be appreciated!","I've been asked if can be a solution for the ODE: when is smooth. So I concluded that there exists a unique solution to the ODE, by Peano's theorem if: Then the ODE is of the form: is smooth and a function of and , therefore the ODE has a unique solution, but I don't know how to continue any further.  Any help/hints would be appreciated!","
y(t)=\left((t-1)^2+1\right)\cdot \cos(t)
 
y''+y+(y-\cos(t))P(t,y,y') = 0
 P 
y_{1}=y,\quad y_{2}=y'.
 
y_{2}'=-y_{1}-P(t,y_{1},y_{2})(y_{1}-\cos(t)).
 P y_{2} y_{1}",['ordinary-differential-equations']
78,Non-linear coupled differential equations,Non-linear coupled differential equations,,"I was trying to solve these coupled differential equations but can´t quite get to the solution. The differential equations are: $$ H^2= \frac{1}{3}\left[\frac{1}{2} \dot\phi^2+V(\phi)\right] \space\space\space\space\space(1)$$ $$  \ddot\phi+3H\dot\phi-\lambda V_0e^{-\lambda \phi}=0  \space\space\space\space\space\space (2)$$ where $ H(t)=\frac{\dot a(t)}{a(t)} $ , $\lambda$ and $V_0$ are constants and the dot notation represents $\space \dot a= \frac{da}{dt}$ . The respective solutions of $a(t)$ and $\phi(t)$ are the following: $$ a(t)=a_0 t^{P} \space \space \space, \space\space\space P=\frac{2}{\lambda^2}\space\space\space\space\space\space (3)$$ $$ \phi(t) =\phi_0 + \frac{2}{\lambda}\ln \left( \frac{t}{t_0}\right)\space\space\space\space\space\space\space\space (4)$$ I was told to do a power law solution for $a(t)$ and $\psi(t)$ , given that $\psi(t)=e^{- \lambda \phi (t)}$ , but making that substitution $ \psi(t)$ I got an equation that seems even worse: $$ -\ddot \psi \psi^2 \left(\frac{1}{\lambda} \right) + \left( \frac{\dot \psi}{\lambda}- \frac{3H \psi}{\lambda}  \right) \dot \psi = \lambda V_0 \psi^3$$ I tried using aproximations too but to no luck. Any help would be appreciated.","I was trying to solve these coupled differential equations but can´t quite get to the solution. The differential equations are: where , and are constants and the dot notation represents . The respective solutions of and are the following: I was told to do a power law solution for and , given that , but making that substitution I got an equation that seems even worse: I tried using aproximations too but to no luck. Any help would be appreciated."," H^2= \frac{1}{3}\left[\frac{1}{2} \dot\phi^2+V(\phi)\right] \space\space\space\space\space(1)   \ddot\phi+3H\dot\phi-\lambda V_0e^{-\lambda \phi}=0  \space\space\space\space\space\space (2)  H(t)=\frac{\dot a(t)}{a(t)}  \lambda V_0 \space \dot a= \frac{da}{dt} a(t) \phi(t)  a(t)=a_0 t^{P} \space \space \space, \space\space\space P=\frac{2}{\lambda^2}\space\space\space\space\space\space (3)  \phi(t) =\phi_0 + \frac{2}{\lambda}\ln \left( \frac{t}{t_0}\right)\space\space\space\space\space\space\space\space (4) a(t) \psi(t) \psi(t)=e^{- \lambda \phi (t)}  \psi(t)  -\ddot \psi \psi^2 \left(\frac{1}{\lambda} \right) + \left( \frac{\dot \psi}{\lambda}- \frac{3H \psi}{\lambda}  \right) \dot \psi = \lambda V_0 \psi^3","['ordinary-differential-equations', 'systems-of-equations']"
79,Forced Duffing equation $\ddot x +x+\varepsilon(bx^3+k \dot x+ax−F\cos(t))=0$ bifurcation analysis,Forced Duffing equation  bifurcation analysis,\ddot x +x+\varepsilon(bx^3+k \dot x+ax−F\cos(t))=0,"For the forced Duffing oscillator in the limit where the forcing, detuning, damping, and nonlinearity are all weak: $$\ddot x +x+\varepsilon(bx^3+k \dot x+ax−F\cos(t))=0$$ where $0<\varepsilon<<1$ , $b>0$ is the nonlinearity, $k>0$ is the damping, $a$ is the detuning, and $F>0$ is the forcing strength. This system deals with saddle-node bifurcations of cycles arise in its analysis. The averaged equations of this system are $$r'=-\frac{1}{2}(kr+\sin(\phi)) \\\phi'=-\frac{1}{8}(4a-br^2+\frac{4F}{r}\cos(\phi))$$ Show that fixed points for the averaged system correspond to phase-locked periodic solutions for the original forced oscillator. Show further that saddle-node bifurcations of fixed points for the averaged system correspond to saddle-node bifurcations of cycles for the oscillator. So far, all I have done is find the fixed points of the averaged system, which occur when $$r=\frac{-F}{k}\sin(\phi)\\\phi=2\pi c-\cos^{-1}(\frac{r(3br^2-4a)}{4F})$$ I don't exactly know how to show that these correspond to phase-locked periodic solutions for the original Duffing oscillator, though. Even more, I struggle to understand how I can classify the fixed points of the averaged equations as saddle-node bifurcations. I found that the Jacobian $J$ of the averaged system is $$J=\begin{bmatrix} -\frac{1}{2}k & -\frac{1}{2}cos(\phi)\\\frac{1}{4}br-\frac{4F}{8}\ln |r|\cos(\phi) & \frac{F}{2r}\sin(\phi)\end{bmatrix}$$ but this seems way too complicated to be a good way to classify these fixed points. Any help would be extremely appreciated!","For the forced Duffing oscillator in the limit where the forcing, detuning, damping, and nonlinearity are all weak: where , is the nonlinearity, is the damping, is the detuning, and is the forcing strength. This system deals with saddle-node bifurcations of cycles arise in its analysis. The averaged equations of this system are Show that fixed points for the averaged system correspond to phase-locked periodic solutions for the original forced oscillator. Show further that saddle-node bifurcations of fixed points for the averaged system correspond to saddle-node bifurcations of cycles for the oscillator. So far, all I have done is find the fixed points of the averaged system, which occur when I don't exactly know how to show that these correspond to phase-locked periodic solutions for the original Duffing oscillator, though. Even more, I struggle to understand how I can classify the fixed points of the averaged equations as saddle-node bifurcations. I found that the Jacobian of the averaged system is but this seems way too complicated to be a good way to classify these fixed points. Any help would be extremely appreciated!","\ddot x +x+\varepsilon(bx^3+k \dot x+ax−F\cos(t))=0 0<\varepsilon<<1 b>0 k>0 a F>0 r'=-\frac{1}{2}(kr+\sin(\phi)) \\\phi'=-\frac{1}{8}(4a-br^2+\frac{4F}{r}\cos(\phi)) r=\frac{-F}{k}\sin(\phi)\\\phi=2\pi c-\cos^{-1}(\frac{r(3br^2-4a)}{4F}) J J=\begin{bmatrix} -\frac{1}{2}k & -\frac{1}{2}cos(\phi)\\\frac{1}{4}br-\frac{4F}{8}\ln
|r|\cos(\phi) & \frac{F}{2r}\sin(\phi)\end{bmatrix}","['ordinary-differential-equations', 'nonlinear-system', 'bifurcation', 'nonlinear-dynamics']"
80,Solve differential equation by integrating factor,Solve differential equation by integrating factor,,"I have the differential equation $$2\frac{dy}{dx}+3y=e^{-2x}-5$$ I have determined that this needs solving using the integrating factor method. My workings out are in the image provided. Are my workings out correct so far? My main question is, how will I integrate with respect for $x$ ? $$\int e^{3x/2} \times e^{-2x}-\frac{5}{2}e^{3x/2}~~dx+c$$ this again is shown at the bottom of my workings out. after integrating this I will then be able to solve for $y$ . Thank you My workings out for the question","I have the differential equation I have determined that this needs solving using the integrating factor method. My workings out are in the image provided. Are my workings out correct so far? My main question is, how will I integrate with respect for ? this again is shown at the bottom of my workings out. after integrating this I will then be able to solve for . Thank you My workings out for the question",2\frac{dy}{dx}+3y=e^{-2x}-5 x \int e^{3x/2} \times e^{-2x}-\frac{5}{2}e^{3x/2}~~dx+c y,"['calculus', 'integration', 'ordinary-differential-equations', 'integrating-factor']"
81,How to estimate the oscillation range of this differential equation,How to estimate the oscillation range of this differential equation,,"Consider the system of differential equation $$\left\{\begin{aligned}\frac{\mathrm{d}x}{\mathrm{d}t} &= y\\\frac{\mathrm{d}y}{\mathrm{d}t} &= x - x^3 - y^3\end{aligned}\right.$$ with initial value $x(0) = y(0) = 1$ . Numerical simulation shows that $(x, y)$ oscillates around $(1, 0)$ , with no sign of convergence even at $t = 1000$ . I would like to estimate the range of this oscillation. My attempt: Let $t_0$ be the smallest positive $t$ at which $y(t) = 0$ . Then when $0 \leq t \leq t_0$ we have $y > 0$ , hence $x$ is increasing, so $x \geq 1$ . Then $x - x^3 < 0$ , so $y$ is decreasing, $y \leq 1$ . Note that $$\frac{\mathrm{d}x}{\mathrm{d}t} + \frac{\mathrm{d}y}{\mathrm{d}t} = x + y - x^3 - y^3 = (x + y)(1 - x^2 - xy - y^2)$$ Since $x \geq 1$ , we have $1 - x^2 \leq 0$ , so $x + y$ is decreasing, so $x + y \leq 2$ . We have $\mathrm{d}y/\mathrm{d}t \geq x - x^3 - (2-x)^3 =  -6x^2+13x-8$ . Let $t_1$ be the smallest positive $t$ such that $x(t_1) = 7/6$ . When $0 \leq t \leq t_1$ , we have $\mathrm{d}y/\mathrm{d}t \geq -1$ , so $y \geq 1-t, x \geq -t^2/2 + t + 1$ . Hence $t_1 \leq 1 - \sqrt{6}/3$ . On the other hand $t_0 \geq 1 - \sqrt{6}/3$ , because whenever $t \leq t_0$ and $t \leq 1/2$ , we have $x \leq 1 + t \leq 3/2$ , so $\mathrm{d}y/\mathrm{d}t \geq -2$ , hence $y \geq 1 - 2t$ . If $t_0 < 1/2$ , then $y(t_0) \geq 1 - 2t > 0$ , contradicting $y(t_0) = 0$ . Hence $t_1 \leq 1 - \sqrt{6}/3 < 1/2 \leq t_0$ . I'm not sure how to proceed. Numerical experiment seems to show that $\mathrm{d}y/\mathrm{d}t \leq -1$ when $t_1 \leq t \leq t_0$ , but I don't know how to prove that. If that's the case, then since $y(t_1) \leq 2 - x(t_1) = 5/6$ , we have $t_0 \leq t_1 + 5/6 \leq 11/6 - \sqrt{6}/3$ . Since $y(t_1 + t) \leq 5/6 - t$ , we have $x(t_1 + t) \leq 1 + t_1 + 5/6t - 1/2t^2$ . Plug in $t_1 \leq 1 - \sqrt{6}/3$ and $t \leq 5/6$ to get $x(t_0) \leq x_0$ where $x_0 \approx 1.531$ . That's pretty close to what I saw in the numerical experiment. How to estimate the next $x$ value where $\mathrm{d}x/\mathrm{d}t = y = 0$ ? The long-term behavior of this equation seems to be that $x$ oscillates between these two extreme values. Also, can we say anything about the behavior when $t$ goes to negative? Mathematica thinks this is a stiff system when $t \leq -0.55$","Consider the system of differential equation with initial value . Numerical simulation shows that oscillates around , with no sign of convergence even at . I would like to estimate the range of this oscillation. My attempt: Let be the smallest positive at which . Then when we have , hence is increasing, so . Then , so is decreasing, . Note that Since , we have , so is decreasing, so . We have . Let be the smallest positive such that . When , we have , so . Hence . On the other hand , because whenever and , we have , so , hence . If , then , contradicting . Hence . I'm not sure how to proceed. Numerical experiment seems to show that when , but I don't know how to prove that. If that's the case, then since , we have . Since , we have . Plug in and to get where . That's pretty close to what I saw in the numerical experiment. How to estimate the next value where ? The long-term behavior of this equation seems to be that oscillates between these two extreme values. Also, can we say anything about the behavior when goes to negative? Mathematica thinks this is a stiff system when","\left\{\begin{aligned}\frac{\mathrm{d}x}{\mathrm{d}t} &= y\\\frac{\mathrm{d}y}{\mathrm{d}t} &= x - x^3 - y^3\end{aligned}\right. x(0) = y(0) = 1 (x, y) (1, 0) t = 1000 t_0 t y(t) = 0 0 \leq t \leq t_0 y > 0 x x \geq 1 x - x^3 < 0 y y \leq 1 \frac{\mathrm{d}x}{\mathrm{d}t} + \frac{\mathrm{d}y}{\mathrm{d}t} = x + y - x^3 - y^3 = (x + y)(1 - x^2 - xy - y^2) x \geq 1 1 - x^2 \leq 0 x + y x + y \leq 2 \mathrm{d}y/\mathrm{d}t \geq x - x^3 - (2-x)^3 =
 -6x^2+13x-8 t_1 t x(t_1) = 7/6 0 \leq t \leq t_1 \mathrm{d}y/\mathrm{d}t \geq -1 y \geq 1-t, x \geq -t^2/2 + t + 1 t_1 \leq 1 - \sqrt{6}/3 t_0 \geq 1 - \sqrt{6}/3 t \leq t_0 t \leq 1/2 x \leq 1 + t \leq 3/2 \mathrm{d}y/\mathrm{d}t \geq -2 y \geq 1 - 2t t_0 < 1/2 y(t_0) \geq 1 - 2t > 0 y(t_0) = 0 t_1 \leq 1 - \sqrt{6}/3 < 1/2 \leq t_0 \mathrm{d}y/\mathrm{d}t \leq -1 t_1 \leq t \leq t_0 y(t_1) \leq 2 - x(t_1) = 5/6 t_0 \leq t_1 + 5/6 \leq 11/6 - \sqrt{6}/3 y(t_1 + t) \leq 5/6 - t x(t_1 + t) \leq 1 + t_1 + 5/6t - 1/2t^2 t_1 \leq 1 - \sqrt{6}/3 t \leq 5/6 x(t_0) \leq x_0 x_0 \approx 1.531 x \mathrm{d}x/\mathrm{d}t = y = 0 x t t \leq -0.55",['ordinary-differential-equations']
82,Solve the initial value problem $y′′+36y=e^{−t}$,Solve the initial value problem,y′′+36y=e^{−t},"Solve the initial value problem $y′′+36y=e^{−t}$ $y(0)=y_0$ , $y'(0)=y_0'$ Suppose we know that $y(t)\to0$ as $t\to\infty$ . Determine the solution and the initial conditions. So far I have come up with $m^2+36=0$ $m=6i$ $m=-6i$ Guess solution is $y(t)=c_1 \sin(6t)+c_2\cos(6t)+e^{-t}$","Solve the initial value problem , Suppose we know that as . Determine the solution and the initial conditions. So far I have come up with Guess solution is",y′′+36y=e^{−t} y(0)=y_0 y'(0)=y_0' y(t)\to0 t\to\infty m^2+36=0 m=6i m=-6i y(t)=c_1 \sin(6t)+c_2\cos(6t)+e^{-t},['ordinary-differential-equations']
83,Singular Points of $\sin(x)y''+y=0$. Review my work,Singular Points of . Review my work,\sin(x)y''+y=0,"I have to classify the singular points of the second order $$\sin(x)y´´+y=0$$ Where, in its standar form: $$y´´+\dfrac{1}{\sin(x)}y=0$$ With $$P(x)=0,  Q(x)=\dfrac{1}{\sin(x)}$$ As far as I can tell, the singular points are $$x_0=n\pi, \forall n\in \mathbb{Z}$$ So, there are infinite singular points? Which all of them are irregular since there is at most 1 power of $(x-x_0)$ in $P(x)$ (there is non) and there is not a power of 2 in $Q(x)$ . I other words, both $(x-x_0) P(x)$ remains finite as $x\to n\pi$ but $(x-x_0)^2 Q(x)$ doesn´t. $$\lim_{x\to n\pi}{(x-n\pi)P(x)}=\lim_{x\to n\pi}{0}=0$$ and $$\lim_{x\to n\pi}{(x-n\pi)^2Q(x)}=\lim_{x\to n\pi}{(x-n\pi)^2\dfrac{1}{\sin(x)}}$$ Does not exist.","I have to classify the singular points of the second order Where, in its standar form: With As far as I can tell, the singular points are So, there are infinite singular points? Which all of them are irregular since there is at most 1 power of in (there is non) and there is not a power of 2 in . I other words, both remains finite as but doesn´t. and Does not exist.","\sin(x)y´´+y=0 y´´+\dfrac{1}{\sin(x)}y=0 P(x)=0,  Q(x)=\dfrac{1}{\sin(x)} x_0=n\pi, \forall n\in \mathbb{Z} (x-x_0) P(x) Q(x) (x-x_0) P(x) x\to n\pi (x-x_0)^2 Q(x) \lim_{x\to n\pi}{(x-n\pi)P(x)}=\lim_{x\to n\pi}{0}=0 \lim_{x\to n\pi}{(x-n\pi)^2Q(x)}=\lim_{x\to n\pi}{(x-n\pi)^2\dfrac{1}{\sin(x)}}",['ordinary-differential-equations']
84,Understanding stable and unstable manifolds,Understanding stable and unstable manifolds,,"I want to understand better the subject of stable and unstable manifold, I want to know what is the motivation to discuss this, and if there is way to imagine this or think about this? Because I see the definition and some theorems and I understand some proofs but not what comes behind this.","I want to understand better the subject of stable and unstable manifold, I want to know what is the motivation to discuss this, and if there is way to imagine this or think about this? Because I see the definition and some theorems and I understand some proofs but not what comes behind this.",,"['ordinary-differential-equations', 'differential-topology', 'dynamical-systems']"
85,Solve the differential equation $y'=\frac{y+1}{y-1}$ by separating variables,Solve the differential equation  by separating variables,y'=\frac{y+1}{y-1},"$y'=\frac{y+1}{y-1}$ Solve the differential equation by separating variables and give the solution in an implicit form. $$\frac{dy}{dx} = \frac{y+1}{y-1}$$ $$\Leftrightarrow dy = \left (\frac{y+1}{y-1} \right )dx$$ $$\Leftrightarrow \left (\frac{y-1}{y+1}\right) dy = dx$$ $$\Leftrightarrow \int{\frac{y-1}{y+1} dy} = \int{dx}$$ $$\Leftrightarrow \int{\frac{y+1-2}{y+1} dy} = \int{dx}$$ $$\Leftrightarrow \int{\frac{y+1}{y+1}-\frac{2}{y+1} dy} = \int{dx}$$ $$\Leftrightarrow y - 2\ln|y+1| = x + C $$ This is what I got right now. I literally don't know if this is correct and especially whether this is also good in the implicit form. Normally you can always check the solution by filling it in, but I wouldn't know how to do that in this case.","Solve the differential equation by separating variables and give the solution in an implicit form. This is what I got right now. I literally don't know if this is correct and especially whether this is also good in the implicit form. Normally you can always check the solution by filling it in, but I wouldn't know how to do that in this case.",y'=\frac{y+1}{y-1} \frac{dy}{dx} = \frac{y+1}{y-1} \Leftrightarrow dy = \left (\frac{y+1}{y-1} \right )dx \Leftrightarrow \left (\frac{y-1}{y+1}\right) dy = dx \Leftrightarrow \int{\frac{y-1}{y+1} dy} = \int{dx} \Leftrightarrow \int{\frac{y+1-2}{y+1} dy} = \int{dx} \Leftrightarrow \int{\frac{y+1}{y+1}-\frac{2}{y+1} dy} = \int{dx} \Leftrightarrow y - 2\ln|y+1| = x + C ,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
86,Hermite polynomial relations,Hermite polynomial relations,,"How one can prove the relation on Hermite polynomial given as $$\int_{-\infty}^\infty H_n\left(x+\frac{x_0}{2}\right)e^{^{-\frac{x^2}{2}}}dx=\sqrt{\pi}x_0^n$$ I also didn't understand the meaning of $\displaystyle H_n\left(x+\frac{x_0}{2}\right)$ , what does that signify, I know by the way that $H_n(x)$ represents the Hermite polynomial but what is the meaning of $\displaystyle H_n\left(x+\frac{x_0}{2}\right)$ ? Please explain this alongwith. Thanks .","How one can prove the relation on Hermite polynomial given as I also didn't understand the meaning of , what does that signify, I know by the way that represents the Hermite polynomial but what is the meaning of ? Please explain this alongwith. Thanks .",\int_{-\infty}^\infty H_n\left(x+\frac{x_0}{2}\right)e^{^{-\frac{x^2}{2}}}dx=\sqrt{\pi}x_0^n \displaystyle H_n\left(x+\frac{x_0}{2}\right) H_n(x) \displaystyle H_n\left(x+\frac{x_0}{2}\right),"['integration', 'ordinary-differential-equations', 'hermite-polynomials']"
87,Differential Equation involving Mixture Problem,Differential Equation involving Mixture Problem,,"I have encountered many mixing problem like this but I just can't answer this one. Here is the problem, A mass of inert material containing $15 \text{ lb}$ of salt in its pores is agitated with $10 \text{ gal}$ of water initially fresh. The salt dissolves at a rate which varies jointly as the number of pounds of undissolved salt and the difference between the concentration of the solution and that of a saturated solution ( $3 \text{ lb/gal}$ ). If $9 \text{ lb}$ are dissolved in $10 \text{ min}$ , when will $90\%$ be dissolved? Here's my solution: Let $P$ be the amount of dissolved salt, $15-P$ be the undissolved. The concentration of the solution $= \frac P {10} \text{ lb/gal}$ . Then, $$\frac{\mathrm dP}{\mathrm dt} = (15-P)\left(3-\frac P {10}\right)\text;$$ $$\frac{\mathrm dP}{\mathrm dt} = (15-P)(30-P)/(10)\text.$$ Since this is separable, I used partial fraction then integrate both side and got an answer of around $11 \text{ min}$ , which is wrong according to the textbook. The answer key provided an answer of $30.5 \text{ min}$ . I don't really know whats wrong in my equation above.","I have encountered many mixing problem like this but I just can't answer this one. Here is the problem, A mass of inert material containing of salt in its pores is agitated with of water initially fresh. The salt dissolves at a rate which varies jointly as the number of pounds of undissolved salt and the difference between the concentration of the solution and that of a saturated solution ( ). If are dissolved in , when will be dissolved? Here's my solution: Let be the amount of dissolved salt, be the undissolved. The concentration of the solution . Then, Since this is separable, I used partial fraction then integrate both side and got an answer of around , which is wrong according to the textbook. The answer key provided an answer of . I don't really know whats wrong in my equation above.",15 \text{ lb} 10 \text{ gal} 3 \text{ lb/gal} 9 \text{ lb} 10 \text{ min} 90\% P 15-P = \frac P {10} \text{ lb/gal} \frac{\mathrm dP}{\mathrm dt} = (15-P)\left(3-\frac P {10}\right)\text; \frac{\mathrm dP}{\mathrm dt} = (15-P)(30-P)/(10)\text. 11 \text{ min} 30.5 \text{ min},"['ordinary-differential-equations', 'word-problem']"
88,Solution to a differential equation in a specific format,Solution to a differential equation in a specific format,,"I am having trouble getting the solution that this problem is asking for. The problem is: ""Show that the general solution to the differential equation $$x \frac{dy}{dx} = y \ln(x)$$ is $$ y = Cx^{\ln(\sqrt{x})}$$ I used separation and got: $$ \int \frac 1y dy = \int \frac{\ln(x)}{x} dx $$ Integrating both sides I got: $$ \ln (y) = \frac{({\ln(x)})^2}{2} +c $$ Then I raised both sides as a power of e and considered e to the power of c as my constant C and got: $$ y = Ce^{\frac{({\ln(x)})^2}{2}}$$ Then I considered the portion containing e as: $$ (e^{({\ln(x)})^2})^\frac 12 $$ This finally gave me : $$ y= C\sqrt {(x^{\ln(x)})} $$ I do not know how the form in the problem was reached.","I am having trouble getting the solution that this problem is asking for. The problem is: ""Show that the general solution to the differential equation is I used separation and got: Integrating both sides I got: Then I raised both sides as a power of e and considered e to the power of c as my constant C and got: Then I considered the portion containing e as: This finally gave me : I do not know how the form in the problem was reached.",x \frac{dy}{dx} = y \ln(x)  y = Cx^{\ln(\sqrt{x})}  \int \frac 1y dy = \int \frac{\ln(x)}{x} dx   \ln (y) = \frac{({\ln(x)})^2}{2} +c   y = Ce^{\frac{({\ln(x)})^2}{2}}  (e^{({\ln(x)})^2})^\frac 12   y= C\sqrt {(x^{\ln(x)})} ,"['calculus', 'algebra-precalculus', 'ordinary-differential-equations']"
89,Looking for help understanding general mathematics with second order differential equation. Solution doesn't work.,Looking for help understanding general mathematics with second order differential equation. Solution doesn't work.,,"I've been in pursuit of general solution to a second order homogeneous ODE for a long time. My degree is in the sciences, so my mathematics is built mostly around that. Consider my an amateur mathematician I guess. Anyway, I was trying to figure out how one might solve $$y(z)'' + f(z)y(z)' + g(z)y(z) = 0$$ I've tried many different avenues and found many cool and interesting relationships, but nothing that solves this for y(z) in terms of f(z) and g(z). This is my most recent attempt. Start with $$y(z) = k(z)$$ Exponentiate in a logarithm $$y(z) = e^{ln(k(z))}$$ Integrate the derivative of ln(k(z)).  The constant on the integral must be 0 to retain the initial conditions. $$y(z) = e^{\int \frac{k(z)'}{k(z)} dz}$$ Multiply by $\frac{k(z)'}{k(z)}$ on  both sides $$y(z) \frac{k(z)'}{k(z)} = \frac{k(z)'}{k(z)} e^{\int \frac{k(z)'}{k(z)} dz}$$ Integrate across. $a$ is some constant. $$\int y(z) \frac{k(z)'}{k(z)} dz + a = \int \frac{k(z)'}{k(z)} e^{\int \frac{k(z)'}{k(z)} dz} dz$$ The right hand side evaluates to e^{\int \frac{k(z)'}{k(z)} dz}, so we have$$ $$\int y(z) \frac{k(z)'}{k(z)} dz + a = e^{\int \frac{k(z)'}{k(z)} dz}$$ But we know that $e^{\int \frac{k(z)'}{k(z)} dz} = y(z)$ so we have $$y(z) = \int y(z) \frac{k(z)'}{k(z)} dz + a$$ Now we'll recurse once $$y(z) = \int \Bigl( \int y(z) \frac{k(z)'}{k(z)} dz + a  \Bigr) \frac{k(z)'}{k(z)} dz + a$$ And now we take a derivative $$y(z)' = \Bigl( \int y(z) \frac{k(z)'}{k(z)} dz + a  \Bigr) \frac{k(z)'}{k(z)}$$ And move that fraction over. $$y(z)' \frac{k(z)}{k(z)'} = \int y(z) \frac{k(z)'}{k(z)} dz + a $$ And one more derivative. $$y(z)'' \frac{k(z)}{k(z)'} + y(z)' \Bigl( 1 - \frac{k(z)''}{k(z)'^2} \Bigr) = y(z) \frac{k(z)'}{k(z)}$$ Lets subtract the right hand side over now $$y(z)'' \frac{k(z)}{k(z)'} + y(z)' \Bigl( 1 - \frac{k(z)''}{k(z)'^2} \Bigr) - y(z) \frac{k(z)'}{k(z)} = 0$$ And multiply by $ \frac{k(z)'}{k(z)} $ across the whole equation. $$y(z)'' + y(z)' \Bigl( \frac{k(z)'}{k(z)} - \frac{k(z)''}{k(z)'} \Bigr) + y(z) \Bigl( -\frac{k(z)'^2}{k(z)^2} \Bigr) = 0$$ If you check this with $ y(z) = k(z),$ you will find that it yields a correct solution. $$y(z)'' + \frac{y(z)'^2}{y(z)} - y(z)'' - \frac{y(z)'^2}{y(z)} = 0$$ Just a heads up, when I say and function, I mean those functions which are differentiable and integrable. Okay, thats neat. This seems to me like it should be pretty fool proof for any function. If you know k(z), then you automatically know y(z), because they are the same. Just set $\Bigl( \frac{k(z)'}{k(z)} - \frac{k(z)''}{k(z)'} \Bigr)$ to f(z) and $\Bigl( -\frac{k(z)'^2}{k(z)^2} \Bigr)$ to g(z). Both of those are easy enough to solve. Here's the thing though, it doesn't work. Also, in setting to two above coefficients to f(z) and g(z) respectively, you have two equations and can solve for f(z) in terms of g(z). It comes out to be $\,2f(z) = g(z)'/g(z)\,$ if you're interested, and this doesn't hold for any of the examples I tried. I even tried going through and solving an example with constant coefficients and then reverse plugging into the equation I just explained. I wondered if maybe the coefficients would pop out and the rest would cancel, but nope. No dice. I was left with something of the form $$\frac{1}{c_1e^{az} + c_2e^{-az}}$$ So my question is, why doesn't this work? If I have $y(z) = k(z)$ and I make any other substitution for k(z), it could always be re-substituted back with k(z) at the end. And if I try to substitute in and other function for f(z) or g(z), it will always be able to be re-substituted back in at the end also. As far as I can tell, I haven't broken any rules. I feel very confused about this and strangely anxious. If anyone is able to shed some light on why this doesn't work, I'd be very grateful.","I've been in pursuit of general solution to a second order homogeneous ODE for a long time. My degree is in the sciences, so my mathematics is built mostly around that. Consider my an amateur mathematician I guess. Anyway, I was trying to figure out how one might solve I've tried many different avenues and found many cool and interesting relationships, but nothing that solves this for y(z) in terms of f(z) and g(z). This is my most recent attempt. Start with Exponentiate in a logarithm Integrate the derivative of ln(k(z)).  The constant on the integral must be 0 to retain the initial conditions. Multiply by on  both sides Integrate across. is some constant. The right hand side evaluates to e^{\int \frac{k(z)'}{k(z)} dz}, so we have$$ But we know that so we have Now we'll recurse once And now we take a derivative And move that fraction over. And one more derivative. Lets subtract the right hand side over now And multiply by across the whole equation. If you check this with you will find that it yields a correct solution. Just a heads up, when I say and function, I mean those functions which are differentiable and integrable. Okay, thats neat. This seems to me like it should be pretty fool proof for any function. If you know k(z), then you automatically know y(z), because they are the same. Just set to f(z) and to g(z). Both of those are easy enough to solve. Here's the thing though, it doesn't work. Also, in setting to two above coefficients to f(z) and g(z) respectively, you have two equations and can solve for f(z) in terms of g(z). It comes out to be if you're interested, and this doesn't hold for any of the examples I tried. I even tried going through and solving an example with constant coefficients and then reverse plugging into the equation I just explained. I wondered if maybe the coefficients would pop out and the rest would cancel, but nope. No dice. I was left with something of the form So my question is, why doesn't this work? If I have and I make any other substitution for k(z), it could always be re-substituted back with k(z) at the end. And if I try to substitute in and other function for f(z) or g(z), it will always be able to be re-substituted back in at the end also. As far as I can tell, I haven't broken any rules. I feel very confused about this and strangely anxious. If anyone is able to shed some light on why this doesn't work, I'd be very grateful.","y(z)'' + f(z)y(z)' + g(z)y(z) = 0 y(z) = k(z) y(z) = e^{ln(k(z))} y(z) = e^{\int \frac{k(z)'}{k(z)} dz} \frac{k(z)'}{k(z)} y(z) \frac{k(z)'}{k(z)} = \frac{k(z)'}{k(z)} e^{\int \frac{k(z)'}{k(z)} dz} a \int y(z) \frac{k(z)'}{k(z)} dz + a = \int \frac{k(z)'}{k(z)} e^{\int \frac{k(z)'}{k(z)} dz} dz \int y(z) \frac{k(z)'}{k(z)} dz + a = e^{\int \frac{k(z)'}{k(z)} dz} e^{\int \frac{k(z)'}{k(z)} dz} = y(z) y(z) = \int y(z) \frac{k(z)'}{k(z)} dz + a y(z) = \int \Bigl( \int y(z) \frac{k(z)'}{k(z)} dz + a  \Bigr) \frac{k(z)'}{k(z)} dz + a y(z)' = \Bigl( \int y(z) \frac{k(z)'}{k(z)} dz + a  \Bigr) \frac{k(z)'}{k(z)} y(z)' \frac{k(z)}{k(z)'} = \int y(z) \frac{k(z)'}{k(z)} dz + a  y(z)'' \frac{k(z)}{k(z)'} + y(z)' \Bigl( 1 - \frac{k(z)''}{k(z)'^2} \Bigr) = y(z) \frac{k(z)'}{k(z)} y(z)'' \frac{k(z)}{k(z)'} + y(z)' \Bigl( 1 - \frac{k(z)''}{k(z)'^2} \Bigr) - y(z) \frac{k(z)'}{k(z)} = 0  \frac{k(z)'}{k(z)}  y(z)'' + y(z)' \Bigl( \frac{k(z)'}{k(z)} - \frac{k(z)''}{k(z)'} \Bigr) + y(z) \Bigl( -\frac{k(z)'^2}{k(z)^2} \Bigr) = 0  y(z) = k(z), y(z)'' + \frac{y(z)'^2}{y(z)} - y(z)'' - \frac{y(z)'^2}{y(z)} = 0 \Bigl( \frac{k(z)'}{k(z)} - \frac{k(z)''}{k(z)'} \Bigr) \Bigl( -\frac{k(z)'^2}{k(z)^2} \Bigr) \,2f(z) = g(z)'/g(z)\, \frac{1}{c_1e^{az} + c_2e^{-az}} y(z) = k(z)",['ordinary-differential-equations']
90,Showing solution to ODE is defined up to infinite time,Showing solution to ODE is defined up to infinite time,,"Question Consider the system $$x'(t)=-x(t)^{3}+y(t)^{2}$$ $$y'(t)=\cos(x(t)y(t)^{2})$$ with initial conditions $x(0)=1, y(0)=2$ . Let $(a,b)$ be the maximal interval that the solution is defined and show $b=\infty$ . Partial Progress An attempt for a proof by contradiction. Let $b<\infty$ so for some finite $b$ we know $||x(t)||\to\infty$ as $t\to b$ . Since $y'(t)$ is a trigonometric function with the given initial condition, we know $|y'(t)|\leq 1$ and $|y(t)|\leq t+2$ . This doesn't seem to substitute nicely into the equation for $x'(t)$ or $x(t)$ to form a finite upper bound. Any help will be appreciated.","Question Consider the system with initial conditions . Let be the maximal interval that the solution is defined and show . Partial Progress An attempt for a proof by contradiction. Let so for some finite we know as . Since is a trigonometric function with the given initial condition, we know and . This doesn't seem to substitute nicely into the equation for or to form a finite upper bound. Any help will be appreciated.","x'(t)=-x(t)^{3}+y(t)^{2} y'(t)=\cos(x(t)y(t)^{2}) x(0)=1, y(0)=2 (a,b) b=\infty b<\infty b ||x(t)||\to\infty t\to b y'(t) |y'(t)|\leq 1 |y(t)|\leq t+2 x'(t) x(t)","['ordinary-differential-equations', 'proof-writing']"
91,Square Roots of Diffeomorhpisms of Manifold Conjugate?,Square Roots of Diffeomorhpisms of Manifold Conjugate?,,"Is this known: given a smooth manifold $Q^n$ , a diffeomorphism $f: Q \to Q$ that is isotopic to the identity, and two different ""square roots of $f$ "", that is, $g_1: Q \to Q$ and $g_2: Q \to Q$ with $g_1 \ne g_2$ , $g_1$ and $g_2$ both also diffeomorphisms and isotopic to the identity, and $g_1^2 = f = g_2^2$ , is it necessarily the case that $g_1$ and $g_2$ are conjugate, that is, that there is a diffeomorphism $q: Q \to Q$ with $q \circ g_1 = g_2 \circ q$ ? (It may be the case that $Q$ is actually some kind of tangent bundle, $Q = TT\ldots TQ' = T^nQ'$ , in which case we would want $q$ to be a bundle map, I think.) (See also this post and this post .)","Is this known: given a smooth manifold , a diffeomorphism that is isotopic to the identity, and two different ""square roots of "", that is, and with , and both also diffeomorphisms and isotopic to the identity, and , is it necessarily the case that and are conjugate, that is, that there is a diffeomorphism with ? (It may be the case that is actually some kind of tangent bundle, , in which case we would want to be a bundle map, I think.) (See also this post and this post .)",Q^n f: Q \to Q f g_1: Q \to Q g_2: Q \to Q g_1 \ne g_2 g_1 g_2 g_1^2 = f = g_2^2 g_1 g_2 q: Q \to Q q \circ g_1 = g_2 \circ q Q Q = TT\ldots TQ' = T^nQ' q,"['ordinary-differential-equations', 'differential-topology', 'dynamical-systems']"
92,Showing $Y(t)$ is a solution to $X'=AX$,Showing  is a solution to,Y(t) X'=AX,"I am struggling with the problem: Let $A$ be a $n\times n$ matrix with eigenvalue $\lambda$ with multiplicity 3 and suppose the associated eigen vector $X_0\in N((A-\lambda I)^3)$ (Nullspace). Show that $$Y(t)=e^{\lambda t}\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0$$ is a solution to $X'=AX$ (without the use of a power series). My attempt : I know that for $Y(t)$ to be a solution, it must satisfy $$Y'(t)=AY(t).$$ Finding $Y'(t)$ we have $$Y'(t)=\lambda e^{\lambda t}\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0+e^{\lambda t}\bigg[I+A-\lambda I+t(A-\lambda I)^2\bigg]X_0$$ and expanding the right hand side we have $$AY(t)=e^{\lambda t}A\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0$$ I tried many things to get the LHS and RHS to equal. I know we have to use the fact that $(A-\lambda I)^3X_0=0$ somewhere, but i just cannot figure it out. Please if anyone can help out, it would be much appreciated.","I am struggling with the problem: Let be a matrix with eigenvalue with multiplicity 3 and suppose the associated eigen vector (Nullspace). Show that is a solution to (without the use of a power series). My attempt : I know that for to be a solution, it must satisfy Finding we have and expanding the right hand side we have I tried many things to get the LHS and RHS to equal. I know we have to use the fact that somewhere, but i just cannot figure it out. Please if anyone can help out, it would be much appreciated.",A n\times n \lambda X_0\in N((A-\lambda I)^3) Y(t)=e^{\lambda t}\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0 X'=AX Y(t) Y'(t)=AY(t). Y'(t) Y'(t)=\lambda e^{\lambda t}\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0+e^{\lambda t}\bigg[I+A-\lambda I+t(A-\lambda I)^2\bigg]X_0 AY(t)=e^{\lambda t}A\bigg[I+t(A-\lambda I)+\frac{t^2(A-\lambda I)^2}{2!}\bigg]X_0 (A-\lambda I)^3X_0=0,"['calculus', 'ordinary-differential-equations']"
93,Solve a system of nonlinear differential equation.,Solve a system of nonlinear differential equation.,,"I am doing some exercises from applied math, but it boils down to solving the following system of nonlinear differential equation: $$\left\{ 	\begin{array}{ll} 	8A'(\tau)=-3A^{3}(\tau)-3A(\tau)B^{2}(\tau)=-3A(\tau)(A^{2}(\tau)+B^{2}(\tau))\\ 	8B'(\tau)=-3A^{2}(\tau)B(\tau)-3B^{3}(\tau)=-3B(\tau)(A^{2}(\tau)+B^{2}(\tau)), 	\end{array} \right.$$ with initial conditions $A(0)=0$ and $B(0)=1$ . I am not sure if there is a general formula for such system, so I just tried to play with them to see if there any beautiful relation. Firstly I found that $$A'(\tau)B(\tau)=B'(\tau)A(\tau)\implies A'(\tau)B(\tau)-B'(\tau)A(\tau)=0,$$ but this does not help me since it does not necessarily form a product rule. Then, I actually expect $A(\tau)$ and $B(\tau)$ are of some forms of $\cos(a\tau)$ and $\sin(a\tau)$ , since if so then $A^{2}(\tau)+B^{2}(\tau)=1$ , and we have $$A'(\tau)=-\frac{3}{8}A(\tau)\ \ \text{and}\ \ B'(\tau)=-\frac{3}{8}B(\tau), $$ but then I got suck again since if, say, $A(\tau)=\cos(a\tau)$ , then $A'(\tau)=-\frac{3}{8}A(\tau)$ gives us $$-a\sin(a\tau)=-\frac{3}{8}\cos(a\tau)\implies \frac{3}{8}\cos(a\tau)-a\sin(a\tau)=0.$$ We can indeed play with trig identity $\sin(x-y)=\sin(x)\cos(y)-\cos(x)\sin(y)$ here, so we have $$\sin(x-a\tau)=0,$$ where $x$ is such that $\sin(x)=\frac{3}{8}$ and $\cos(x)=a$ . Then what I should do? I am kind of confusing myself. Thanks! Edit 1: Wolframalpha gives me the general solution $$A(\tau)=-\dfrac{2}{\sqrt{3c_{1}^{2}\tau-8c_{2}+3\tau}}\ \ \text{and}\ \ B(\tau)=-\dfrac{2c_{1}}{\sqrt{3c_{1}^{2}\tau-8c_{2}+3\tau}},$$ but this contradicts the initial condition. In this form $A(0)$ cannot be $0$ . This seems suggest that $A(\tau)=0$ for all $\tau$ , but I am not sure how to show it.","I am doing some exercises from applied math, but it boils down to solving the following system of nonlinear differential equation: with initial conditions and . I am not sure if there is a general formula for such system, so I just tried to play with them to see if there any beautiful relation. Firstly I found that but this does not help me since it does not necessarily form a product rule. Then, I actually expect and are of some forms of and , since if so then , and we have but then I got suck again since if, say, , then gives us We can indeed play with trig identity here, so we have where is such that and . Then what I should do? I am kind of confusing myself. Thanks! Edit 1: Wolframalpha gives me the general solution but this contradicts the initial condition. In this form cannot be . This seems suggest that for all , but I am not sure how to show it.","\left\{
	\begin{array}{ll}
	8A'(\tau)=-3A^{3}(\tau)-3A(\tau)B^{2}(\tau)=-3A(\tau)(A^{2}(\tau)+B^{2}(\tau))\\
	8B'(\tau)=-3A^{2}(\tau)B(\tau)-3B^{3}(\tau)=-3B(\tau)(A^{2}(\tau)+B^{2}(\tau)),
	\end{array}
\right. A(0)=0 B(0)=1 A'(\tau)B(\tau)=B'(\tau)A(\tau)\implies A'(\tau)B(\tau)-B'(\tau)A(\tau)=0, A(\tau) B(\tau) \cos(a\tau) \sin(a\tau) A^{2}(\tau)+B^{2}(\tau)=1 A'(\tau)=-\frac{3}{8}A(\tau)\ \ \text{and}\ \ B'(\tau)=-\frac{3}{8}B(\tau),  A(\tau)=\cos(a\tau) A'(\tau)=-\frac{3}{8}A(\tau) -a\sin(a\tau)=-\frac{3}{8}\cos(a\tau)\implies \frac{3}{8}\cos(a\tau)-a\sin(a\tau)=0. \sin(x-y)=\sin(x)\cos(y)-\cos(x)\sin(y) \sin(x-a\tau)=0, x \sin(x)=\frac{3}{8} \cos(x)=a A(\tau)=-\dfrac{2}{\sqrt{3c_{1}^{2}\tau-8c_{2}+3\tau}}\ \ \text{and}\ \ B(\tau)=-\dfrac{2c_{1}}{\sqrt{3c_{1}^{2}\tau-8c_{2}+3\tau}}, A(0) 0 A(\tau)=0 \tau","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
94,Why the maximal interval of existence isn't always $\mathbb{R}$,Why the maximal interval of existence isn't always,\mathbb{R},"We know from the Picard-Lindelof Theorem that given an IVP with $f(x)\in C(E)$ where $E$ is an open subset of $\mathbb{R}^n$ then we have a local solution define in some interval $]t_0-\alpha,t_0+\alpha[$ . Now suppose we want the maximal interval of existence , I am interest in figuring out what are the factors that stop us from this maximal interval being $\mathbb{R}$ . One thing I think could happen is that when we try to apply the Picard Lindelof again to a point in $]t_o-\alpha,t_0+\alpha[$ we obtain a new interval but the point is that this sum of lenghts of the intervals could be converging, and so we could not get to the whole space of $\mathbb{R}$ ? Is there anything more that can stop us from getting a solution defined in the whole line ? We need to have control over the values of $x(t)$ but I think the picard-Lindelof theorem gives us that $x(t)$ is in $E$ for all $t$ in the interval given by picard lindelof. I guess a problem could be that if we try to use picard-lindelof in $t_0-\alpha$ we need to check that $x(t_0-\alpha)$ is defined and so this could become a problem too. Also I have seen both version where the interval where the solution is defined is closed and sometimes I have seen it open so I guess I am confused about that too, I can see why they could equivalent but I don't understand why we jut don't fix one. Is there anthing I am missing or missunderstanding? Thanks in advance.","We know from the Picard-Lindelof Theorem that given an IVP with where is an open subset of then we have a local solution define in some interval . Now suppose we want the maximal interval of existence , I am interest in figuring out what are the factors that stop us from this maximal interval being . One thing I think could happen is that when we try to apply the Picard Lindelof again to a point in we obtain a new interval but the point is that this sum of lenghts of the intervals could be converging, and so we could not get to the whole space of ? Is there anything more that can stop us from getting a solution defined in the whole line ? We need to have control over the values of but I think the picard-Lindelof theorem gives us that is in for all in the interval given by picard lindelof. I guess a problem could be that if we try to use picard-lindelof in we need to check that is defined and so this could become a problem too. Also I have seen both version where the interval where the solution is defined is closed and sometimes I have seen it open so I guess I am confused about that too, I can see why they could equivalent but I don't understand why we jut don't fix one. Is there anthing I am missing or missunderstanding? Thanks in advance.","f(x)\in C(E) E \mathbb{R}^n ]t_0-\alpha,t_0+\alpha[ \mathbb{R} ]t_o-\alpha,t_0+\alpha[ \mathbb{R} x(t) x(t) E t t_0-\alpha x(t_0-\alpha)",['ordinary-differential-equations']
95,Motivation behind certain integrating factor,Motivation behind certain integrating factor,,"In this article about exact differential equations, for the fourth example, they have the differential equation: $$ (5xy^2 - 2y) dx  + (3xy^2 -x) dy = 0$$ And they say they integrating factor for this is $$ \mu(x,y) = x^a y^b$$ Now, I don't get how they arrived that this should be correct integrating factors for the problem. Of course, this does integrating factor does satisfy the requirement that the second-order partials commute but I can't really understand how I'd come up with this if it was not already told that this is the integrating factor. If not, for what kinds of differential equations would this integrating factor work? Similar post to this I have seen this post already and it is not the same as mine because the question was not really regarding the integrating factor which I mentioned but rather alternative methods.","In this article about exact differential equations, for the fourth example, they have the differential equation: And they say they integrating factor for this is Now, I don't get how they arrived that this should be correct integrating factors for the problem. Of course, this does integrating factor does satisfy the requirement that the second-order partials commute but I can't really understand how I'd come up with this if it was not already told that this is the integrating factor. If not, for what kinds of differential equations would this integrating factor work? Similar post to this I have seen this post already and it is not the same as mine because the question was not really regarding the integrating factor which I mentioned but rather alternative methods."," (5xy^2 - 2y) dx  + (3xy^2 -x) dy = 0  \mu(x,y) = x^a y^b",['ordinary-differential-equations']
96,Salt in a container - ODE problem,Salt in a container - ODE problem,,"We work with a water container that can hold up to 1000 liters. The water container initially contains 500 liters of water, in which 20 kg of salt has been dissolved. There is agitation in the container, so the salt concentration is the same everywhere. Clean water flows into the container at a speed of 4L / min. At the same time, saltwater flows out of the container at a speed of 2L / min. This causes problems as the container is only at 1000L. After a while, an alarm sounds due to a filled container. (a) At what time does the alarm sound? The differential equation that describes this problem is given by $$y'=-\frac{2}{500+2t}\cdot y$$ It is stated that the salt content is 10kg when the alarm rings, i.e. $y(250)=10$ . Just as the alarm sounds, the inflow changes so that salt water now flows with 35g of salt per. liters in at the rate of 1L / min. There is still 2L flowing out per. minute. (b) How much salt is in the container when there is again 500L in the container? My answer: Well (a) was quite easy, I obtained that the alarm will sound after 250 minutes since $500+2\cdot 250=1000$ and then the alarm will ring. But the last part, i.e. (b) is not easy for me. I believe that I have to make an ODE from the information I just got, but here I tried many times. Notice that this problem came from a book where there was more question, I just picked the relevant part. (b) is also stated as ""voluntary"" in the book. Notice this is not a homework problem.","We work with a water container that can hold up to 1000 liters. The water container initially contains 500 liters of water, in which 20 kg of salt has been dissolved. There is agitation in the container, so the salt concentration is the same everywhere. Clean water flows into the container at a speed of 4L / min. At the same time, saltwater flows out of the container at a speed of 2L / min. This causes problems as the container is only at 1000L. After a while, an alarm sounds due to a filled container. (a) At what time does the alarm sound? The differential equation that describes this problem is given by It is stated that the salt content is 10kg when the alarm rings, i.e. . Just as the alarm sounds, the inflow changes so that salt water now flows with 35g of salt per. liters in at the rate of 1L / min. There is still 2L flowing out per. minute. (b) How much salt is in the container when there is again 500L in the container? My answer: Well (a) was quite easy, I obtained that the alarm will sound after 250 minutes since and then the alarm will ring. But the last part, i.e. (b) is not easy for me. I believe that I have to make an ODE from the information I just got, but here I tried many times. Notice that this problem came from a book where there was more question, I just picked the relevant part. (b) is also stated as ""voluntary"" in the book. Notice this is not a homework problem.",y'=-\frac{2}{500+2t}\cdot y y(250)=10 500+2\cdot 250=1000,['ordinary-differential-equations']
97,Differential equation of a central orbit,Differential equation of a central orbit,,"I am solving dynamics(central orbits) and have a doubt in differential equation of a particle moving in central orbit. Suppose a particle moves in central orbit under influence of a central force. Let a particle move in a plane with an acceleration P , always directed towards a fixed point O which is also the centre of force.Let (r,θ) be the polar coordinates of the position P of the moving particle at any instant t. Radial acceleration of particle is given by :- d 2 r/dt 2 - r (dθ/dt) 2 = -P Transverse acceleration of particle is given by :- $\frac{1}{r}$$\frac{d}{dt}$ (r 2 $\frac{dθ}{dt}$ ) I know about radial and transverse accelerations in circular motion. Central orbit motion seems similar . But still not able to relate how the author presented these equations. Any help is appreciated !!","I am solving dynamics(central orbits) and have a doubt in differential equation of a particle moving in central orbit. Suppose a particle moves in central orbit under influence of a central force. Let a particle move in a plane with an acceleration P , always directed towards a fixed point O which is also the centre of force.Let (r,θ) be the polar coordinates of the position P of the moving particle at any instant t. Radial acceleration of particle is given by :- d 2 r/dt 2 - r (dθ/dt) 2 = -P Transverse acceleration of particle is given by :- (r 2 ) I know about radial and transverse accelerations in circular motion. Central orbit motion seems similar . But still not able to relate how the author presented these equations. Any help is appreciated !!",\frac{1}{r}\frac{d}{dt} \frac{dθ}{dt},"['ordinary-differential-equations', 'nonlinear-dynamics']"
98,Solving first order ODE: $(4x+3y^2)+(2xy)\frac{dy}{dx}=0$,Solving first order ODE:,(4x+3y^2)+(2xy)\frac{dy}{dx}=0,Solve this ODE: $$(4x+3y^2)+(2xy)\frac{dy}{dx}=0.$$ I am having trouble doing this ODE. I tried using separable of variables and integrating factor and I am having no luck. $$\frac{dy}{dx}= -\frac{2}{y}-\frac{3y}{2x}$$,Solve this ODE: I am having trouble doing this ODE. I tried using separable of variables and integrating factor and I am having no luck.,(4x+3y^2)+(2xy)\frac{dy}{dx}=0. \frac{dy}{dx}= -\frac{2}{y}-\frac{3y}{2x},['ordinary-differential-equations']
99,Does this system have an explicit solution?,Does this system have an explicit solution?,,"$x' = x/\sqrt{1+y^2}, \ y' = \sqrt{1+x^2}.$ I don't know if there was a conventional way to solve convoluted systems like this, but is there a way this system can yield an explicit solution (that isn't a straight line)? If not that system, what about $x' = y^2, \ y' = x^2$ ?","I don't know if there was a conventional way to solve convoluted systems like this, but is there a way this system can yield an explicit solution (that isn't a straight line)? If not that system, what about ?","x' = x/\sqrt{1+y^2}, \ y' = \sqrt{1+x^2}. x' = y^2, \ y' = x^2",['ordinary-differential-equations']
