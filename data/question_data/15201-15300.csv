,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$\int_0^\infty\int_0^\pi\frac{k^2(e^{-it\sqrt{k^2+m^2}}-e^{it\sqrt{k^2+m^2}})\sin(\theta)}{e^{-ikx\cos{\theta}}\sqrt{k^2+m^2}}d\theta dk$,,\int_0^\infty\int_0^\pi\frac{k^2(e^{-it\sqrt{k^2+m^2}}-e^{it\sqrt{k^2+m^2}})\sin(\theta)}{e^{-ikx\cos{\theta}}\sqrt{k^2+m^2}}d\theta dk,"$$\int_0^\infty\int_0^\pi\frac{k^2\left(e^{-it\sqrt{k^2+m^2}}-e^{it\sqrt{k^2+m^2}}\right)\sin(\theta)}{e^{-ikx\cos{\theta}}\sqrt{k^2+m^2}}d\theta dk$$ I saw this Integral at Quora, and I have not idea how to evaluate it. Therefore, I thought of posting it here. How do we Evaluate this?","$$\int_0^\infty\int_0^\pi\frac{k^2\left(e^{-it\sqrt{k^2+m^2}}-e^{it\sqrt{k^2+m^2}}\right)\sin(\theta)}{e^{-ikx\cos{\theta}}\sqrt{k^2+m^2}}d\theta dk$$ I saw this Integral at Quora, and I have not idea how to evaluate it. Therefore, I thought of posting it here. How do we Evaluate this?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
1,Is the series $\sum_{n=2}^\infty\frac{(-1)^{\lfloor\sqrt{n}\rfloor}}{\ln{n}}$ convergent?,Is the series  convergent?,\sum_{n=2}^\infty\frac{(-1)^{\lfloor\sqrt{n}\rfloor}}{\ln{n}},"Define $$S_k=\sum_{n=2}^{k} \frac{(-1)^{\lfloor\sqrt{n}\rfloor}}{\ln{n}}$$. Is $S_k$ or $S_{4k^2}$ convergent? I think $S_k$ isn't convergent, since for big enough $n$, the $\lfloor\sqrt{n}\rfloor$ will change more slowly than for small $n$, but I don't know how to show it formally. For $S_{4k^2}$, I think the series is also divergent since then $\lfloor\sqrt{n}\rfloor$ is always even, so we consider $$S_{4k^2}=\sum_{n=2}^{k} \frac{1}{\ln{n}},$$ but we know $1/\ln n$ is divergent, so $S_{4k^2}$ is divergent. Is my reasoning here correct?","Define $$S_k=\sum_{n=2}^{k} \frac{(-1)^{\lfloor\sqrt{n}\rfloor}}{\ln{n}}$$. Is $S_k$ or $S_{4k^2}$ convergent? I think $S_k$ isn't convergent, since for big enough $n$, the $\lfloor\sqrt{n}\rfloor$ will change more slowly than for small $n$, but I don't know how to show it formally. For $S_{4k^2}$, I think the series is also divergent since then $\lfloor\sqrt{n}\rfloor$ is always even, so we consider $$S_{4k^2}=\sum_{n=2}^{k} \frac{1}{\ln{n}},$$ but we know $1/\ln n$ is divergent, so $S_{4k^2}$ is divergent. Is my reasoning here correct?",,"['calculus', 'sequences-and-series']"
2,Computing $ \int \frac{{x}~{\cos^{-1}(x)}}{\sqrt{1-{x^2}}}~\mathrm{d}x $.,Computing ., \int \frac{{x}~{\cos^{-1}(x)}}{\sqrt{1-{x^2}}}~\mathrm{d}x ,"I've just begun to learn integration which makes me a little nervous! Here's a question I'm having a problem with. Also my first time trying to use LaTeX. I apologise for any discrepancies. Compute:   $$ \int \frac{{x}~{\cos^{-1}(x)}}{\sqrt{1-{x^2}}}~\mathrm{d}x $$ Here's what I did: Substitute $ u = \cos^{-1}(x) $. So, $ -~\mathrm{d}u = \frac{1}{\sqrt{1-x^2}}~\mathrm{d}x$. Also, I think it's also correct (please correct me if not) that $ x = \cos(u) $. This could be my mistake. $$ = -\int u~\cos(u)~\mathrm{d}u $$ Using integration by parts for $ f(x) = u $, $ f'(x) = 1 $, $ g'(x) = \cos(u) $ and $ g(x) = \sin(u) $, $$ = -~(u~\sin(u) - \int \sin(u)~\mathrm{d}u) $$ $$ = -~u~\sin(u) - \cos(u) + C $$ Substituing back, $$ -~\cos^{-1}(x)~\sin(\cos^{-1}(x)) - \cos(\cos^{-1}(x)) + C $$ I understand integration by parts could've been applied directly in the very beginning. But my first instinct when I solved was this. Is it in any way incorrect? I appreciate your time.","I've just begun to learn integration which makes me a little nervous! Here's a question I'm having a problem with. Also my first time trying to use LaTeX. I apologise for any discrepancies. Compute:   $$ \int \frac{{x}~{\cos^{-1}(x)}}{\sqrt{1-{x^2}}}~\mathrm{d}x $$ Here's what I did: Substitute $ u = \cos^{-1}(x) $. So, $ -~\mathrm{d}u = \frac{1}{\sqrt{1-x^2}}~\mathrm{d}x$. Also, I think it's also correct (please correct me if not) that $ x = \cos(u) $. This could be my mistake. $$ = -\int u~\cos(u)~\mathrm{d}u $$ Using integration by parts for $ f(x) = u $, $ f'(x) = 1 $, $ g'(x) = \cos(u) $ and $ g(x) = \sin(u) $, $$ = -~(u~\sin(u) - \int \sin(u)~\mathrm{d}u) $$ $$ = -~u~\sin(u) - \cos(u) + C $$ Substituing back, $$ -~\cos^{-1}(x)~\sin(\cos^{-1}(x)) - \cos(\cos^{-1}(x)) + C $$ I understand integration by parts could've been applied directly in the very beginning. But my first instinct when I solved was this. Is it in any way incorrect? I appreciate your time.",,"['calculus', 'integration']"
3,Integration of $x^a$ and Summation of first $n$ $a$th powers,Integration of  and Summation of first  th powers,x^a n a,"There are some convenient formulas for the summation of the first $n$ integers which are the $a$th powers of other integers, e.g. $$ \sum_{i=0}^n i = \frac {n(n+1)}2$$ $$ \sum_{i=0}^n i^2 = \frac {n(n + 1)(2n + 1)}6 $$ Similarly, there are formulae for integration of monomials ($ x^n $), such as $$ \int x^2\,dx = \frac {x^3}3 , \int x\,dx = \frac {x^2}2 $$ On replacing $n + 1$ by $n$, $2n + 1$ by $2n$, and $a_n + b $ by $a_n$ for n'th term in the summation formulas, we get the integration formulas, e.g Replace $n + 1$ by $n$, we get $ n^2/2 $ for $n(n+1)/2$ Replace $n+1$, $2n+1$, by $n, 2n$, then for $n(2n+1)(n+1)$, we get $$ \frac {n(n)(2n)}6 = \frac {2n^3}6 = \frac {n^3}3  $$ This also works for $x^3$, and $x^4$. Why does this happen? Does it work for higher powers? (I've checked that it works up to $n=8$ so far.) NOTE:I'm learning some discrete mathematics, and I already know a little bit of calculus.","There are some convenient formulas for the summation of the first $n$ integers which are the $a$th powers of other integers, e.g. $$ \sum_{i=0}^n i = \frac {n(n+1)}2$$ $$ \sum_{i=0}^n i^2 = \frac {n(n + 1)(2n + 1)}6 $$ Similarly, there are formulae for integration of monomials ($ x^n $), such as $$ \int x^2\,dx = \frac {x^3}3 , \int x\,dx = \frac {x^2}2 $$ On replacing $n + 1$ by $n$, $2n + 1$ by $2n$, and $a_n + b $ by $a_n$ for n'th term in the summation formulas, we get the integration formulas, e.g Replace $n + 1$ by $n$, we get $ n^2/2 $ for $n(n+1)/2$ Replace $n+1$, $2n+1$, by $n, 2n$, then for $n(2n+1)(n+1)$, we get $$ \frac {n(n)(2n)}6 = \frac {2n^3}6 = \frac {n^3}3  $$ This also works for $x^3$, and $x^4$. Why does this happen? Does it work for higher powers? (I've checked that it works up to $n=8$ so far.) NOTE:I'm learning some discrete mathematics, and I already know a little bit of calculus.",,"['calculus', 'integration', 'summation', 'indefinite-integrals']"
4,Integral involving hyperfactorial,Integral involving hyperfactorial,,I'm trying to prove that: $$ \int_0^1 \ln\left(K(x)\right)\space dx =-\zeta'(-1)=\ln(A)-\frac{1}{12} $$ Where $A$ is Glaisher Kinkelin's constant and $K(x)$ is a generalization of the hyperfactorial given by: $$ K(n+1)=\prod_{k=1}^{n} k^k $$ $$ K(x)=\lim_{r\to\infty} \frac{e^{\frac{1}{2}x(x+1)}\cdot r^{xr+\frac{1}{2}x(x+1)}\cdot K(r+1)}{x^x\cdot (1+x)^{1+x}\cdots(r+x)^{r+x}} $$ I arrived to show the following using the limit representation: $$ K(x)=x^{-x}\cdot(2\pi)^{-\frac{x}{2}}\cdot e^{\frac{1}{2}x(x+1)-\frac{\gamma}{2}x^2}\cdot\prod_{k=1}^\infty \left[\frac{e^{x+\frac{x^2}{2k}}}{\left(1+\frac{x}{k}\right)^{x+k}}\right] $$ But I wasn't able to proceed further. So any help is highly appreciated.,I'm trying to prove that: $$ \int_0^1 \ln\left(K(x)\right)\space dx =-\zeta'(-1)=\ln(A)-\frac{1}{12} $$ Where $A$ is Glaisher Kinkelin's constant and $K(x)$ is a generalization of the hyperfactorial given by: $$ K(n+1)=\prod_{k=1}^{n} k^k $$ $$ K(x)=\lim_{r\to\infty} \frac{e^{\frac{1}{2}x(x+1)}\cdot r^{xr+\frac{1}{2}x(x+1)}\cdot K(r+1)}{x^x\cdot (1+x)^{1+x}\cdots(r+x)^{r+x}} $$ I arrived to show the following using the limit representation: $$ K(x)=x^{-x}\cdot(2\pi)^{-\frac{x}{2}}\cdot e^{\frac{1}{2}x(x+1)-\frac{\gamma}{2}x^2}\cdot\prod_{k=1}^\infty \left[\frac{e^{x+\frac{x^2}{2k}}}{\left(1+\frac{x}{k}\right)^{x+k}}\right] $$ But I wasn't able to proceed further. So any help is highly appreciated.,,"['calculus', 'definite-integrals', 'gamma-function', 'zeta-functions']"
5,Advice for self-studying Inequalities and Calculus,Advice for self-studying Inequalities and Calculus,,"I'm interested in self-studying the following books over the next year or so: Spivak's Calculus (I'm already in Ch. 5 and it is very slow going) The Cauchy-Schwarz Master Class by J. Michael Steele Analytic Inequalities by Nicholas Kazarinoff My goal in studying these books is to gain a deeper understanding of calculus, basic real analysis, and manipulations of the standard inequalities, with the ultimate goal of understanding derivations, approximations, and inequalities in probability and statistics (Stirling's approximation, Wallis product, Gamma Function, Normal Distribution, Limit Theorems etc).  One of the things I realized when I first started studying Spivak's Calculus is that I have had very little experience in solving challenging problems.  I have never had any issues with doing 'Exercises' in the standard engineering style calculus text books, but I am often at a loss of ideas when I do problems in Spivak. My questions are the following: Before progressing through Spivak, should I go through a book like Art and Craft of Problem Solving by Paul Zeitz? I guess the point of doing this would be to beef up my problem-solving skills. I should note that I am not very excited about working through the Art and Craft of Problem Solving because a lot of it seems geared toward solving Olympiad geometry problems.  I've never had a solid geometry course, so at this point I feel like it might just be a waste of time trying to learn plane geometry. Should I relearn high school mathematics? To be perfectly honest, I feel robbed by my entire education and I'm very disappointed by my lack of foresight up to this point. I've always used easy textbooks(not my choice) in my college calculus, Linear algebra, and ODE and PDE classes and believed 'good grades' were enough. Or, should I just keep a copy of Polya's Heuristics on hand while I patiently work through Spivak? I'm just looking for a bit of advise on the wisest way to proceed. Thanx.","I'm interested in self-studying the following books over the next year or so: Spivak's Calculus (I'm already in Ch. 5 and it is very slow going) The Cauchy-Schwarz Master Class by J. Michael Steele Analytic Inequalities by Nicholas Kazarinoff My goal in studying these books is to gain a deeper understanding of calculus, basic real analysis, and manipulations of the standard inequalities, with the ultimate goal of understanding derivations, approximations, and inequalities in probability and statistics (Stirling's approximation, Wallis product, Gamma Function, Normal Distribution, Limit Theorems etc).  One of the things I realized when I first started studying Spivak's Calculus is that I have had very little experience in solving challenging problems.  I have never had any issues with doing 'Exercises' in the standard engineering style calculus text books, but I am often at a loss of ideas when I do problems in Spivak. My questions are the following: Before progressing through Spivak, should I go through a book like Art and Craft of Problem Solving by Paul Zeitz? I guess the point of doing this would be to beef up my problem-solving skills. I should note that I am not very excited about working through the Art and Craft of Problem Solving because a lot of it seems geared toward solving Olympiad geometry problems.  I've never had a solid geometry course, so at this point I feel like it might just be a waste of time trying to learn plane geometry. Should I relearn high school mathematics? To be perfectly honest, I feel robbed by my entire education and I'm very disappointed by my lack of foresight up to this point. I've always used easy textbooks(not my choice) in my college calculus, Linear algebra, and ODE and PDE classes and believed 'good grades' were enough. Or, should I just keep a copy of Polya's Heuristics on hand while I patiently work through Spivak? I'm just looking for a bit of advise on the wisest way to proceed. Thanx.",,"['calculus', 'inequality', 'soft-question', 'problem-solving', 'advice']"
6,Prove no existing a smooth function satisfying ... related to Morse Theory,Prove no existing a smooth function satisfying ... related to Morse Theory,,"i) Show that there does not exist a smooth function $f:\mathbb{R} \rightarrow \mathbb{R}$, s.t. $f(x) \geq 0$, $\forall x \in \mathbb{R}$, $f$ has exactly two critical points , $x_1,x_2\in\mathbb{R}$ and $f(x_1)=f(x_2) = 0$. (This part is easy). ii) Show that there does not exist a smooth function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$, s.t. $f(x,y) \geq 0$, $\forall (x,y) \in \mathbb{R}^2$, $f$ has exactly two critical points , $(x_1,y_1),(x_2,y_2)\in\mathbb{R}^2$ and $f(x_1,y_1)=f(x_2,y_2) = 0$. I have tried several methods, however, it does not work, could anybody help me out?","i) Show that there does not exist a smooth function $f:\mathbb{R} \rightarrow \mathbb{R}$, s.t. $f(x) \geq 0$, $\forall x \in \mathbb{R}$, $f$ has exactly two critical points , $x_1,x_2\in\mathbb{R}$ and $f(x_1)=f(x_2) = 0$. (This part is easy). ii) Show that there does not exist a smooth function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$, s.t. $f(x,y) \geq 0$, $\forall (x,y) \in \mathbb{R}^2$, $f$ has exactly two critical points , $(x_1,y_1),(x_2,y_2)\in\mathbb{R}^2$ and $f(x_1,y_1)=f(x_2,y_2) = 0$. I have tried several methods, however, it does not work, could anybody help me out?",,"['calculus', 'analysis', 'differential-topology', 'morse-theory']"
7,Why is calculus focused on functions?,Why is calculus focused on functions?,,"A curve of a hyperbolic spiral for example is not a graph of a function. But the concept of continuity and finding its slope for example, which are in calculus applies to it. So why is calculus typically phrased in terms of functions only? Is it for sake of simplicity of didactics, or because all curves can be formulated as parametric equations and those thereby ""reduced"" to the case of functions? Could you recommend a reading to clarify my potential misconceptions about the topic?","A curve of a hyperbolic spiral for example is not a graph of a function. But the concept of continuity and finding its slope for example, which are in calculus applies to it. So why is calculus typically phrased in terms of functions only? Is it for sake of simplicity of didactics, or because all curves can be formulated as parametric equations and those thereby ""reduced"" to the case of functions? Could you recommend a reading to clarify my potential misconceptions about the topic?",,"['calculus', 'functions']"
8,Alternating Series and Convergence,Alternating Series and Convergence,,The question is: Approximate the value of the series within an error of at most $10^{-4}$. $$ \sum_{n=1}^\infty \frac{(-1)^{(n+1)}}{(n+79)(n+73)} $$ According to $$|S_N - S| ≤ a_{N+1}$$ what is the smallest value of $N$ that approximates $S$ to within an error of at most $10^{-4}$? I have tried the following to get $N$: $$a_n = \frac{1}{(n+79)(n+73)}$$ $$\frac{1}{(n+79)(n+73)} < \frac{1}{10000}$$ $N=25$ How do I solve for $S$?,The question is: Approximate the value of the series within an error of at most $10^{-4}$. $$ \sum_{n=1}^\infty \frac{(-1)^{(n+1)}}{(n+79)(n+73)} $$ According to $$|S_N - S| ≤ a_{N+1}$$ what is the smallest value of $N$ that approximates $S$ to within an error of at most $10^{-4}$? I have tried the following to get $N$: $$a_n = \frac{1}{(n+79)(n+73)}$$ $$\frac{1}{(n+79)(n+73)} < \frac{1}{10000}$$ $N=25$ How do I solve for $S$?,,"['calculus', 'sequences-and-series', 'algebra-precalculus']"
9,Horse and snail problem.,Horse and snail problem.,,A horse has a rubber band attached to it which can expand infinitely and is tied to a pole on the other end. At first the length of the rubber band is $l$. on the pole-side of the rubber band there is a snail. If both start walking at the same time: The horse at speed $u$ and the snail at speed $v$ with $u>v$  when will the snail catch the horse?,A horse has a rubber band attached to it which can expand infinitely and is tied to a pole on the other end. At first the length of the rubber band is $l$. on the pole-side of the rubber band there is a snail. If both start walking at the same time: The horse at speed $u$ and the snail at speed $v$ with $u>v$  when will the snail catch the horse?,,['calculus']
10,"How prove this $F'(x_{0})=f(x_{0})$ if $F(x)=\int_{a}^{x}f(t)dt,x\in[a,b]$",How prove this  if,"F'(x_{0})=f(x_{0}) F(x)=\int_{a}^{x}f(t)dt,x\in[a,b]","Question: let $f$ be Riemann integrable on $[a,b]$,Assmue that $x_{0}\in [a,b]$,and let $f(x)$ is continuous on point $x=x_{0}$,and define $$F(x)=\int_{a}^{x}f(t)dt,x\in[a,b]$$ show that $$F'(x_{0})=f(x_{0})$$ This problem is my friend (He is a teacher) ask me,and I post my answer,maybe have some wrong, since   $$F'(x_{0})=\lim_{x\to x_{0}}\dfrac{F(x)-F(x_{0})}{x-x_{0}}=\lim_{x\to x_{0}}\dfrac{\int_{x_{0}}^{x}f(t)dt}{x-x_{0}}$$   and other hand, case1: $x>x_{0}$ we have $$I=\left|\dfrac{\int_{x_{0}}^{x}f(t)dt}{x-x_{0}}-f(x_{0})\right|=\dfrac{\left|\int_{x_{0}}^{x}f(t)dt-f(x_{0})(x-x_{0})\right|}{x-x_{0}}\le\dfrac{\int_{x_{0}}^{x}|f(t)-f(x_{0})|dt}{x-x_{0}}$$   since   $f(x)$ is continous on point $x=x_{0}$   so   $$I\le\dfrac{\int_{x_{0}}^{x}\varepsilon dt}{x-x_{0}}=\varepsilon$$   so   $$F'(x_{0})=f(x_{0})$$ case2:$x<x_{0}$ $$I=\left|\dfrac{\int_{x_{0}}^{x}f(t)dt}{x-x_{0}}-f(x_{0})\right|=\dfrac{\left|\int_{x_{0}}^{x}f(t)dt-f(x_{0})(x-x_{0})\right|}{|x-x_{0}|}\le\dfrac{\int_{x}^{x_{0}}|f(t)-f(x_{0})|dt}{x_{0}-x}$$   since   $f(x)$ is continous on point $x=x_{0}$   so   $$I\le\dfrac{\int_{x}^{x_{0}}\varepsilon dt}{x_{0}-x}=\varepsilon$$ My question:  someone  have other methods? Thank you  very much!","Question: let $f$ be Riemann integrable on $[a,b]$,Assmue that $x_{0}\in [a,b]$,and let $f(x)$ is continuous on point $x=x_{0}$,and define $$F(x)=\int_{a}^{x}f(t)dt,x\in[a,b]$$ show that $$F'(x_{0})=f(x_{0})$$ This problem is my friend (He is a teacher) ask me,and I post my answer,maybe have some wrong, since   $$F'(x_{0})=\lim_{x\to x_{0}}\dfrac{F(x)-F(x_{0})}{x-x_{0}}=\lim_{x\to x_{0}}\dfrac{\int_{x_{0}}^{x}f(t)dt}{x-x_{0}}$$   and other hand, case1: $x>x_{0}$ we have $$I=\left|\dfrac{\int_{x_{0}}^{x}f(t)dt}{x-x_{0}}-f(x_{0})\right|=\dfrac{\left|\int_{x_{0}}^{x}f(t)dt-f(x_{0})(x-x_{0})\right|}{x-x_{0}}\le\dfrac{\int_{x_{0}}^{x}|f(t)-f(x_{0})|dt}{x-x_{0}}$$   since   $f(x)$ is continous on point $x=x_{0}$   so   $$I\le\dfrac{\int_{x_{0}}^{x}\varepsilon dt}{x-x_{0}}=\varepsilon$$   so   $$F'(x_{0})=f(x_{0})$$ case2:$x<x_{0}$ $$I=\left|\dfrac{\int_{x_{0}}^{x}f(t)dt}{x-x_{0}}-f(x_{0})\right|=\dfrac{\left|\int_{x_{0}}^{x}f(t)dt-f(x_{0})(x-x_{0})\right|}{|x-x_{0}|}\le\dfrac{\int_{x}^{x_{0}}|f(t)-f(x_{0})|dt}{x_{0}-x}$$   since   $f(x)$ is continous on point $x=x_{0}$   so   $$I\le\dfrac{\int_{x}^{x_{0}}\varepsilon dt}{x_{0}-x}=\varepsilon$$ My question:  someone  have other methods? Thank you  very much!",,"['calculus', 'analysis', 'limits', 'derivatives']"
11,Approximate $\int_0^{\pi /2} \frac{ds}{\sqrt{1-x\sin^2s}}$,Approximate,\int_0^{\pi /2} \frac{ds}{\sqrt{1-x\sin^2s}},"I am trying to approximate the following integral  $$K(x)=\int\limits_0^{\pi /2} \frac{ds}{\sqrt{1-x\sin^2s}}$$ with $0<x<1$. I need to show that for x close to one that $K(x)\sim -\frac{1}{2}\ln(1-x)$. My first attempt was to Taylor expand the function $f(x)= (1-x\sin^2s)^{-1/2}$ about $x=1$. I did this using $f(x)\approx f(1)+f'(1)(x-1)$. I integrated the result and found that $K(x)\sim \left[\frac{5-x}{4}\ln|\sec s + \tan s|\right]_0^{\pi/2} - \left[\frac{1-x}{4}\sec s \tan s \right]_0^{\pi/2}$, which goes to infinity when $s=\pi/2$. I can't think of another way to approach this, so any advice would be helpful.","I am trying to approximate the following integral  $$K(x)=\int\limits_0^{\pi /2} \frac{ds}{\sqrt{1-x\sin^2s}}$$ with $0<x<1$. I need to show that for x close to one that $K(x)\sim -\frac{1}{2}\ln(1-x)$. My first attempt was to Taylor expand the function $f(x)= (1-x\sin^2s)^{-1/2}$ about $x=1$. I did this using $f(x)\approx f(1)+f'(1)(x-1)$. I integrated the result and found that $K(x)\sim \left[\frac{5-x}{4}\ln|\sec s + \tan s|\right]_0^{\pi/2} - \left[\frac{1-x}{4}\sec s \tan s \right]_0^{\pi/2}$, which goes to infinity when $s=\pi/2$. I can't think of another way to approach this, so any advice would be helpful.",,"['calculus', 'asymptotics']"
12,Prove the general arithmetic-geometric mean inequality,Prove the general arithmetic-geometric mean inequality,,"Prove that the general arithmetic-geometric mean inequality \begin{equation*} (a_{1}a_{2}...a_{n})^\frac{1}{n}\leq\frac{a_{1}+a_{2}+...+a_{n}}{n} \end{equation*} holds for all $a_{i}$ positive real numbers. I keep getting stuck half way. This is review material for me (which I feel like I should be getting easily, but that's not the case unfortunately).","Prove that the general arithmetic-geometric mean inequality \begin{equation*} (a_{1}a_{2}...a_{n})^\frac{1}{n}\leq\frac{a_{1}+a_{2}+...+a_{n}}{n} \end{equation*} holds for all $a_{i}$ positive real numbers. I keep getting stuck half way. This is review material for me (which I feel like I should be getting easily, but that's not the case unfortunately).",,"['calculus', 'inequality']"
13,Evaluating a limit of an integral,Evaluating a limit of an integral,,"I am trying to solve the following problem. Let $f:[0,\infty)\rightarrow\mathbb R$ be a continuous function and $b>a>0$ be real numbers.  Prove that $$ \lim_{\epsilon\rightarrow+0}\int_{a\epsilon}^{b\epsilon}\frac{f(x)}{x}dx = f(0)\log\frac{b}{a}.$$ If $f$ were differentiable I could use integration by parts, but I do not know what to do with general continuous $f$. I would be grateful if you could give me a clue.","I am trying to solve the following problem. Let $f:[0,\infty)\rightarrow\mathbb R$ be a continuous function and $b>a>0$ be real numbers.  Prove that $$ \lim_{\epsilon\rightarrow+0}\int_{a\epsilon}^{b\epsilon}\frac{f(x)}{x}dx = f(0)\log\frac{b}{a}.$$ If $f$ were differentiable I could use integration by parts, but I do not know what to do with general continuous $f$. I would be grateful if you could give me a clue.",,['calculus']
14,Why does $\sum_{k=2}^\infty k \left( \sum_{j=2^k}^{2^{k+1}-1} \frac{(-1)^j}{j} \right )+{1\over2}-{1\over3} = \gamma$?,Why does ?,\sum_{k=2}^\infty k \left( \sum_{j=2^k}^{2^{k+1}-1} \frac{(-1)^j}{j} \right )+{1\over2}-{1\over3} = \gamma,"How could one prove that $$x = \sum_{k=2}^\infty k \left( \sum_{j=2^k}^{2^{k+1}-1} \frac{(-1)^j}{j} \right )$$ is such that $x+{1\over2}-{1\over3} = \gamma$ ? I am having problems just calculating to see if it is correct, let alone proving it...","How could one prove that $$x = \sum_{k=2}^\infty k \left( \sum_{j=2^k}^{2^{k+1}-1} \frac{(-1)^j}{j} \right )$$ is such that $x+{1\over2}-{1\over3} = \gamma$ ? I am having problems just calculating to see if it is correct, let alone proving it...",,"['calculus', 'sequences-and-series', 'complex-numbers']"
15,Spivak Calculus chapter 7 theorem 9,Spivak Calculus chapter 7 theorem 9,,"I am working through Spivak Calculus chapter 7 theorem 9. There is one statement that I can't quite understand. The theorem states: If $n$ is odd, then any equation $$ \ x^n+a_{n-1}x^{n-1} +\cdots+a^0 $$ has a root. proof: we would like to prove that $f$ is sometimes positive and sometimes negative. The intuitive idea is that for large $|x|$, the function is very much like $g(x) = x^n$ and, since $n$ is odd, this function is positive for large positive $x$ and negative for large negative $x$. A little algebra is all we need to make this intuitive idea work. $$ f(x) = x^n+a_{n-1}x^{n-1} +\cdots+a^0 = x^n \left(1+\frac{a_{n-1}}{x}+\cdots+\frac{a_0}{x^n}\right) $$ Note that  $$ \left|\frac{a_{n-1}}{x}+\frac{a_{n-2}}{x^2}+\cdots+\frac{a_0}{x^n} \right|\le \frac{|a_{n-1}|}{|x|}+\frac{|a_{n-2}|}{|x^2|}+\cdots+\frac{|a_{0}|}{|x^n|} $$ Consequently if we choose $x$ satisfying $$ |x|>1,2n|a_{n-1}|,\ldots,2n|a_0| \tag{*} $$ I am not sure how he comes to $(*)$ Thanks in advance","I am working through Spivak Calculus chapter 7 theorem 9. There is one statement that I can't quite understand. The theorem states: If $n$ is odd, then any equation $$ \ x^n+a_{n-1}x^{n-1} +\cdots+a^0 $$ has a root. proof: we would like to prove that $f$ is sometimes positive and sometimes negative. The intuitive idea is that for large $|x|$, the function is very much like $g(x) = x^n$ and, since $n$ is odd, this function is positive for large positive $x$ and negative for large negative $x$. A little algebra is all we need to make this intuitive idea work. $$ f(x) = x^n+a_{n-1}x^{n-1} +\cdots+a^0 = x^n \left(1+\frac{a_{n-1}}{x}+\cdots+\frac{a_0}{x^n}\right) $$ Note that  $$ \left|\frac{a_{n-1}}{x}+\frac{a_{n-2}}{x^2}+\cdots+\frac{a_0}{x^n} \right|\le \frac{|a_{n-1}|}{|x|}+\frac{|a_{n-2}|}{|x^2|}+\cdots+\frac{|a_{0}|}{|x^n|} $$ Consequently if we choose $x$ satisfying $$ |x|>1,2n|a_{n-1}|,\ldots,2n|a_0| \tag{*} $$ I am not sure how he comes to $(*)$ Thanks in advance",,['calculus']
16,Help solving the limit $\lim\limits_{x\to \infty}\dfrac{(\ln(x))^x}{x^{\ln (x)}}$,Help solving the limit,\lim\limits_{x\to \infty}\dfrac{(\ln(x))^x}{x^{\ln (x)}},"$$\lim_{x\to \infty}\frac{(\ln(x))^x}{x^{\ln (x)}}$$ I really don't know what to do here, as soon as I try to manipulate the expression I end up with something worse.. Thanks!","$$\lim_{x\to \infty}\frac{(\ln(x))^x}{x^{\ln (x)}}$$ I really don't know what to do here, as soon as I try to manipulate the expression I end up with something worse.. Thanks!",,"['calculus', 'limits']"
17,epsilon-delta proof of $\lim_{x \to 4} \sqrt{x} = 2$,epsilon-delta proof of,\lim_{x \to 4} \sqrt{x} = 2,"Prove $$\quad \lim_{x\to4}\sqrt{x}=2 $$ using the precise definition of limits. (Epsilon-Delta) I think I proved this problem but when I look at the textbook to compare the proofs they are quite different and I don't get how the book worked it all out. So I am going to post mine for you to check if it's correct and the one from the textbook to ask some questions. $\\[10pt]$ My rough work: $\;$ Pick $\epsilon > 0$, then there exists $\delta>0$ such that $\quad 0 < |x - 4| < \delta \quad \Rightarrow \quad |\sqrt{x} - 2| < \epsilon$ Establish a connection between $|x - 4|$ and $|\sqrt{x} - 2|$ $|\sqrt{x} - 2| \cdot \frac{|\sqrt{x} + 2|}{|\sqrt{x} + 2|} =  \frac{|x - 4|}{\sqrt{x} + 2}$, $\;$ Pick $\delta = 4$ $|x-4| < 4 \ \Rightarrow\  0 < \sqrt{x} < \sqrt{8} \ \Rightarrow\  2 < \sqrt{x} + 2 < \sqrt{8} + 2 \ \Rightarrow\  \frac{1}{\sqrt{8} + 2} < \frac{1}{\sqrt{x} + 2} < \frac{1}{2}$ This implies $\frac{|x - 4|}{\sqrt{x} + 2} < \frac{1}{2} \cdot |x-4| < \epsilon \ \Rightarrow\ |x-4|< 2 \cdot \epsilon$ $\\[20pt]$ My proof: $\;\delta = min\{4,2\epsilon\}$ and assume that $\  0 < |x - 4| < \delta \  \Rightarrow \  |\sqrt{x} - 2| < \epsilon$ $\frac{|x - 4|}{\sqrt{x} + 2} < \frac{1}{2} \cdot |x-4| < \frac{1}{2} \cdot 2\epsilon < \epsilon \quad QED\quad $ Corrct? $\\[20pt]$ From the book: $\;$To be able to form $\sqrt{x}$, we need to have $x \ge 0$. To ensure this, we must have $\delta \le 4$. With $x \ge 0$, we can form $\sqrt{x}$ and write $|x - 4| = |\sqrt{x}+2||\sqrt{x}-2|$. Since $|\sqrt{x} + 2| \ge 2 > 1$, it follows that $\quad \leftarrow\;$ This is what I don't understand. $\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad $ where did ""$\ge 2 > 1$"" come from? $\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad $ the book doesn't explain $|\sqrt{x}-2| \le |x - 4| \qquad \qquad \qquad \qquad \ \ \ \, \leftarrow$ How can I get this from the above inequality? This last inequality suggests that we can simply set $\delta \le \epsilon \quad \leftarrow$ I got $2\epsilon$, how come it only has $\epsilon$? Any help would be greatly appreciated.","Prove $$\quad \lim_{x\to4}\sqrt{x}=2 $$ using the precise definition of limits. (Epsilon-Delta) I think I proved this problem but when I look at the textbook to compare the proofs they are quite different and I don't get how the book worked it all out. So I am going to post mine for you to check if it's correct and the one from the textbook to ask some questions. $\\[10pt]$ My rough work: $\;$ Pick $\epsilon > 0$, then there exists $\delta>0$ such that $\quad 0 < |x - 4| < \delta \quad \Rightarrow \quad |\sqrt{x} - 2| < \epsilon$ Establish a connection between $|x - 4|$ and $|\sqrt{x} - 2|$ $|\sqrt{x} - 2| \cdot \frac{|\sqrt{x} + 2|}{|\sqrt{x} + 2|} =  \frac{|x - 4|}{\sqrt{x} + 2}$, $\;$ Pick $\delta = 4$ $|x-4| < 4 \ \Rightarrow\  0 < \sqrt{x} < \sqrt{8} \ \Rightarrow\  2 < \sqrt{x} + 2 < \sqrt{8} + 2 \ \Rightarrow\  \frac{1}{\sqrt{8} + 2} < \frac{1}{\sqrt{x} + 2} < \frac{1}{2}$ This implies $\frac{|x - 4|}{\sqrt{x} + 2} < \frac{1}{2} \cdot |x-4| < \epsilon \ \Rightarrow\ |x-4|< 2 \cdot \epsilon$ $\\[20pt]$ My proof: $\;\delta = min\{4,2\epsilon\}$ and assume that $\  0 < |x - 4| < \delta \  \Rightarrow \  |\sqrt{x} - 2| < \epsilon$ $\frac{|x - 4|}{\sqrt{x} + 2} < \frac{1}{2} \cdot |x-4| < \frac{1}{2} \cdot 2\epsilon < \epsilon \quad QED\quad $ Corrct? $\\[20pt]$ From the book: $\;$To be able to form $\sqrt{x}$, we need to have $x \ge 0$. To ensure this, we must have $\delta \le 4$. With $x \ge 0$, we can form $\sqrt{x}$ and write $|x - 4| = |\sqrt{x}+2||\sqrt{x}-2|$. Since $|\sqrt{x} + 2| \ge 2 > 1$, it follows that $\quad \leftarrow\;$ This is what I don't understand. $\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad $ where did ""$\ge 2 > 1$"" come from? $\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad $ the book doesn't explain $|\sqrt{x}-2| \le |x - 4| \qquad \qquad \qquad \qquad \ \ \ \, \leftarrow$ How can I get this from the above inequality? This last inequality suggests that we can simply set $\delta \le \epsilon \quad \leftarrow$ I got $2\epsilon$, how come it only has $\epsilon$? Any help would be greatly appreciated.",,"['calculus', 'limits', 'radicals', 'epsilon-delta', 'proof-explanation']"
18,Differentiability vs Having a Derivative,Differentiability vs Having a Derivative,,"In some calculus texts, one comes across the following statement - ""A function is differentiable, if it has a derivative and a function has a derivative, if it is differentiable."". Two such texts are  - A treatise on Advanced Calculus by Philip Franklin(this book can be found on archive.org - check section 69 - Page 109) and Advanced Calculus - An Introduction to Linear Analysis by Leonard F. Richardson - Chapter 4 - Page 101. Analytical proofs are also given to justify these statements. This raises the question - What is the physical difference between being differentiable and having a derivative? The book by Leonard Richardson gives some analytical explanation about the existence of a linear function, which approximates the function at points very close to x. If such a function exists, then the function is said to be differentiable(and then he goes on to prove analytically that the function has a derivative and vice versa). The problem is that the whole proof is anlaytic, and is not illustrated using a practical example. When we say that a function is differentiable, we generally check for the existence of its derivative (i.e the limit of its differential coefficient). But this text 'implies' that its possible to check the differentiability of a function by checking for the existence of such a linear function(whose slope as you've guessed becomes the derivative in the limit). So what essentially is the difference, between this approach and the standard approach? It seems purely algebraic to me....Or is there more to it? A related question is - what's the difference between Caratheodory's definition of the derivative and the standard definition of the derivative?. The difference seems purely algebraic to me.I would like to know if there's more to it.(I think the answer to this question is closely related to the first, but since Caratheodory's definition is rarely covered in most standard calculus texts, I've decided to gain as much information as possible, on this subject.) I am not a mathematics major, so there are limits to my theoretical thinking! I would greatly appreciate if practical examples are provided to illustrate these concepts.","In some calculus texts, one comes across the following statement - ""A function is differentiable, if it has a derivative and a function has a derivative, if it is differentiable."". Two such texts are  - A treatise on Advanced Calculus by Philip Franklin(this book can be found on archive.org - check section 69 - Page 109) and Advanced Calculus - An Introduction to Linear Analysis by Leonard F. Richardson - Chapter 4 - Page 101. Analytical proofs are also given to justify these statements. This raises the question - What is the physical difference between being differentiable and having a derivative? The book by Leonard Richardson gives some analytical explanation about the existence of a linear function, which approximates the function at points very close to x. If such a function exists, then the function is said to be differentiable(and then he goes on to prove analytically that the function has a derivative and vice versa). The problem is that the whole proof is anlaytic, and is not illustrated using a practical example. When we say that a function is differentiable, we generally check for the existence of its derivative (i.e the limit of its differential coefficient). But this text 'implies' that its possible to check the differentiability of a function by checking for the existence of such a linear function(whose slope as you've guessed becomes the derivative in the limit). So what essentially is the difference, between this approach and the standard approach? It seems purely algebraic to me....Or is there more to it? A related question is - what's the difference between Caratheodory's definition of the derivative and the standard definition of the derivative?. The difference seems purely algebraic to me.I would like to know if there's more to it.(I think the answer to this question is closely related to the first, but since Caratheodory's definition is rarely covered in most standard calculus texts, I've decided to gain as much information as possible, on this subject.) I am not a mathematics major, so there are limits to my theoretical thinking! I would greatly appreciate if practical examples are provided to illustrate these concepts.",,['calculus']
19,how to estimate this indefinite integral $\int \frac{1}{e^x +x}dx$,how to estimate this indefinite integral,\int \frac{1}{e^x +x}dx,"One of my student asked me to estimate this indefinite integral \begin{gather*} \int \frac{1}{e^x+x}dx. \end{gather*} I tried many methods, but I can not find the primitive of it. Then I use Maple and Mathematica to help me. As a result, the two softwares can not give me the result. Thus I am not sure if the primitive of this integral can be expressed as elementary functions. But I could not give the right reason. How can I judge if the primitive of this indefinite integral can be expressed as elementary functions? Can anybody help me?","One of my student asked me to estimate this indefinite integral \begin{gather*} \int \frac{1}{e^x+x}dx. \end{gather*} I tried many methods, but I can not find the primitive of it. Then I use Maple and Mathematica to help me. As a result, the two softwares can not give me the result. Thus I am not sure if the primitive of this integral can be expressed as elementary functions. But I could not give the right reason. How can I judge if the primitive of this indefinite integral can be expressed as elementary functions? Can anybody help me?",,"['calculus', 'indefinite-integrals']"
20,Conditional convergence of Riemann's $\zeta$'s series,Conditional convergence of Riemann's 's series,\zeta,"Do Riemann's zeta-function's partial sums $\sum_{n=1}^N n^{-s}$ converge conditionally for some value $s=\sigma+it$ with $\sigma\le 1$? (We must at least have $t\ne 0$ of course.) Partial summation does not work because $\cos(t\log n)$ does not have bounded sums, but I wonder if perhaps at least for $\sigma=1$ and some $t\ne 0$ we may have convergence. 1st Edit: I insist that I am not interested in absolute convergence, which I understand. I really want to know if enough cancellation occurs in the complex powers $n^{1+it}$, $t\ne 0$ for the ordered sequence of partial sums to converge—i.e. for the series to converge conditionally. I guess that this issue may be related to elementary estimates used to prove the prime number theorem (like those of Erdős and Selberg)—even if none implies conditional convergence. 2nd Edit: To recap, conditional convergence at $\sigma$ of a Dirichlet series $\sum_{n\ge 1} a_nn^{-s}$, with real $a_n$ implies no pole on the real half-line at the right of $\sigma$ so the abscissa of absolute and conditional convergence of the Dirichlet series representations (which is unique, a nontrivial result) for Riemann's $\zeta$ are the same, $1$, i.e. the series does not converge conditionally for $\sigma<1$. I will also mention that the Dirichlet series $\sum_{n\ge 1}(-1)^nn^{-s}$ has abscissa of conditional convergence $0$ (therefore no pole at the right of $0$), and dividing it by $2^{1-s}-1$ we obtain $\zeta(s)$, so this is close to a Dirichlet series evaluation of $\zeta$—which are known not to be practical computationally. I could find interesting results in Tenenbaum's book on analytic number theory. I guess I will have to look at the heavy weight references, specialized on Riemann's zeta-function. The case of $\sigma=1$ and $t\ne 0$ is still unsettled in the answers to this question, and in my mind. 3rd Edit: This question on mathoverflow seems to address exactly my question: https://mathoverflow.net/questions/84097/divergence-of-dirichlet-series The conclusion, there, is that the series diverges also for $t\ne 0$. This may be related to the existence of unbounded functions with bounded mean oscillation, like $\log t$. I'll read more about that and think about it.","Do Riemann's zeta-function's partial sums $\sum_{n=1}^N n^{-s}$ converge conditionally for some value $s=\sigma+it$ with $\sigma\le 1$? (We must at least have $t\ne 0$ of course.) Partial summation does not work because $\cos(t\log n)$ does not have bounded sums, but I wonder if perhaps at least for $\sigma=1$ and some $t\ne 0$ we may have convergence. 1st Edit: I insist that I am not interested in absolute convergence, which I understand. I really want to know if enough cancellation occurs in the complex powers $n^{1+it}$, $t\ne 0$ for the ordered sequence of partial sums to converge—i.e. for the series to converge conditionally. I guess that this issue may be related to elementary estimates used to prove the prime number theorem (like those of Erdős and Selberg)—even if none implies conditional convergence. 2nd Edit: To recap, conditional convergence at $\sigma$ of a Dirichlet series $\sum_{n\ge 1} a_nn^{-s}$, with real $a_n$ implies no pole on the real half-line at the right of $\sigma$ so the abscissa of absolute and conditional convergence of the Dirichlet series representations (which is unique, a nontrivial result) for Riemann's $\zeta$ are the same, $1$, i.e. the series does not converge conditionally for $\sigma<1$. I will also mention that the Dirichlet series $\sum_{n\ge 1}(-1)^nn^{-s}$ has abscissa of conditional convergence $0$ (therefore no pole at the right of $0$), and dividing it by $2^{1-s}-1$ we obtain $\zeta(s)$, so this is close to a Dirichlet series evaluation of $\zeta$—which are known not to be practical computationally. I could find interesting results in Tenenbaum's book on analytic number theory. I guess I will have to look at the heavy weight references, specialized on Riemann's zeta-function. The case of $\sigma=1$ and $t\ne 0$ is still unsettled in the answers to this question, and in my mind. 3rd Edit: This question on mathoverflow seems to address exactly my question: https://mathoverflow.net/questions/84097/divergence-of-dirichlet-series The conclusion, there, is that the series diverges also for $t\ne 0$. This may be related to the existence of unbounded functions with bounded mean oscillation, like $\log t$. I'll read more about that and think about it.",,"['calculus', 'number-theory', 'riemann-zeta']"
21,"$\int 5\sqrt{x^3+2x}\, \textrm{d}x$",,"\int 5\sqrt{x^3+2x}\, \textrm{d}x","I can not find any method to solve this, why do the typical methods fail?  I have tried subtituion, parts, strange subitution, etc.  How does one solve this integral?","I can not find any method to solve this, why do the typical methods fail?  I have tried subtituion, parts, strange subitution, etc.  How does one solve this integral?",,['calculus']
22,Maximization problem,Maximization problem,,"I've been trying to solve the following problem from Stewart's Calculus Textbook for a while without any success. My answer makes sense, but I'm looking for a way to solve it analytically. The problem concerns a pulley that is attached to the ceiling of a room at a point C by a rope of length r . At another point B on the ceiling, at a distance d from C (where d > r ), a rope of length l is attached and passed through the pulley at F and connected to a weight W . The weight is released and comes to rest at its equilibrium position D . This happens when the distance | ED | is maximized. Show that when the system reaches equilibrium, the value of x is: $$\frac{r}{4d}(r+\sqrt{r^2+8d^2})$$ Here is what I've done. First, I expressed | DE | as a function of x $$|DE|(x)={a}_{2}+{a}_{3}=l-{a}_{1}+\sqrt{{r}^{2}-{x}^{2}}=l-\sqrt{{a}_{3}^2+y^2}+\sqrt{r^2-x^2}$$ from what follows that $$|DE|(x)=l-\sqrt{r^2+d^2-2xd}+\sqrt{r^2-x^2}$$ defined for $$0\leq x \leq  r$$ ...and it works since $$|DE|(0)=l+r-\sqrt{r^2+d^2}$$ and $$|DE|(r)=l-|r-d|$$ To find the maximum of this function, I calculated |DE|'(x) $$|DE|'(x)=\frac{d}{\sqrt{r^2+d^2-2xd}}-\frac{x}{\sqrt{r^2-x^2}}$$ I proved the two radicals at the denominator are defined for $$0\leq x < r$$ ...so basically I'm interested in finding when |DE|'(x) equals zero, more specifically the roots of $$d\sqrt{r^2-x^2}-x\sqrt{r^2+d^2-2xd}=0$$ that becomes $$2dx^3-(r^2+2d^2)x^2+d^2r^2=0$$ I graphed |DE|(x) and |DE|'(x) (using l = 15, r = 3, and d = 4), they are consistent with the problem. |DE|'(x) has only one root at about 2.76 and at the same point |DE|(x) has its maximum. Moreover, if you substitute my test numbers for l , r , and d in the given formula for |DE|(x) maximum you get the same numerical result. So, how was the author of the problem able to find an analytical solution to the problem? Thanks!","I've been trying to solve the following problem from Stewart's Calculus Textbook for a while without any success. My answer makes sense, but I'm looking for a way to solve it analytically. The problem concerns a pulley that is attached to the ceiling of a room at a point C by a rope of length r . At another point B on the ceiling, at a distance d from C (where d > r ), a rope of length l is attached and passed through the pulley at F and connected to a weight W . The weight is released and comes to rest at its equilibrium position D . This happens when the distance | ED | is maximized. Show that when the system reaches equilibrium, the value of x is: $$\frac{r}{4d}(r+\sqrt{r^2+8d^2})$$ Here is what I've done. First, I expressed | DE | as a function of x $$|DE|(x)={a}_{2}+{a}_{3}=l-{a}_{1}+\sqrt{{r}^{2}-{x}^{2}}=l-\sqrt{{a}_{3}^2+y^2}+\sqrt{r^2-x^2}$$ from what follows that $$|DE|(x)=l-\sqrt{r^2+d^2-2xd}+\sqrt{r^2-x^2}$$ defined for $$0\leq x \leq  r$$ ...and it works since $$|DE|(0)=l+r-\sqrt{r^2+d^2}$$ and $$|DE|(r)=l-|r-d|$$ To find the maximum of this function, I calculated |DE|'(x) $$|DE|'(x)=\frac{d}{\sqrt{r^2+d^2-2xd}}-\frac{x}{\sqrt{r^2-x^2}}$$ I proved the two radicals at the denominator are defined for $$0\leq x < r$$ ...so basically I'm interested in finding when |DE|'(x) equals zero, more specifically the roots of $$d\sqrt{r^2-x^2}-x\sqrt{r^2+d^2-2xd}=0$$ that becomes $$2dx^3-(r^2+2d^2)x^2+d^2r^2=0$$ I graphed |DE|(x) and |DE|'(x) (using l = 15, r = 3, and d = 4), they are consistent with the problem. |DE|'(x) has only one root at about 2.76 and at the same point |DE|(x) has its maximum. Moreover, if you substitute my test numbers for l , r , and d in the given formula for |DE|(x) maximum you get the same numerical result. So, how was the author of the problem able to find an analytical solution to the problem? Thanks!",,['calculus']
23,How to justify this differential manipulation while integrating?,How to justify this differential manipulation while integrating?,,"Some time ago I had a physics test where I had the following integral: $\int y'' \  \mathrm{d}y$. The idea is that I had a differential equation, and I had acceleration (that is, $y''$) given as a function of position ($y$). The integral was actually equal to something else, but that's not the point. I needed to somehow solve that. I can't integrate acceleration with respect to position, so here's what I did: $$ \int y'' \ \mathrm{d}y =  \int \frac{\mathrm{d}y'}{\mathrm{d}t} \ \mathrm{d}y =  \int \mathrm{d}y' \frac{\mathrm{d}y}{\mathrm{d}t} =  \int y' \ \mathrm{d}y' = \frac1{2}y'^2 + C $$ My professor said this was correct and it makes sense, but doing weird stuff with differentials and such never completely satisfies me. Is there a substitution that justifies this procedure?","Some time ago I had a physics test where I had the following integral: $\int y'' \  \mathrm{d}y$. The idea is that I had a differential equation, and I had acceleration (that is, $y''$) given as a function of position ($y$). The integral was actually equal to something else, but that's not the point. I needed to somehow solve that. I can't integrate acceleration with respect to position, so here's what I did: $$ \int y'' \ \mathrm{d}y =  \int \frac{\mathrm{d}y'}{\mathrm{d}t} \ \mathrm{d}y =  \int \mathrm{d}y' \frac{\mathrm{d}y}{\mathrm{d}t} =  \int y' \ \mathrm{d}y' = \frac1{2}y'^2 + C $$ My professor said this was correct and it makes sense, but doing weird stuff with differentials and such never completely satisfies me. Is there a substitution that justifies this procedure?",,"['calculus', 'integration', 'physics']"
24,The role of writing in understanding concepts,The role of writing in understanding concepts,,"I am an engineer student from Norway who is not that fascinated with ""engineering maths"", so I am trying to work through Spivak's Calculus book on my own. I know that there is no ""how to"" manual for grokking concepts. But I wonder if anyone has used writing as a tool to understand complex concepts. The reason I ask is i bumped into this essay about writing, and I was wondering how that relates to my epic battle with Spivak's book. Any thoughts?","I am an engineer student from Norway who is not that fascinated with ""engineering maths"", so I am trying to work through Spivak's Calculus book on my own. I know that there is no ""how to"" manual for grokking concepts. But I wonder if anyone has used writing as a tool to understand complex concepts. The reason I ask is i bumped into this essay about writing, and I was wondering how that relates to my epic battle with Spivak's book. Any thoughts?",,"['calculus', 'soft-question', 'learning']"
25,Wrong Definition of a Limit,Wrong Definition of a Limit,,"What would this mean: $\exists \delta >0$ such that $\forall \epsilon > 0$ and $\forall x$ satisfying $0 < |x-a| < \delta$, then $|f(x)-L| < \epsilon$ I am pretty confused by the symbols too... Here's I read it: There exists a delta larger than zero such that for any epsilon larger than zero and for any $x$ satisfying $0 < |x − a| < \delta$, we will have $|f(x) − L| < \epsilon$. Does this show that there simply exists an interval where $f(x)$ is a constant function?","What would this mean: $\exists \delta >0$ such that $\forall \epsilon > 0$ and $\forall x$ satisfying $0 < |x-a| < \delta$, then $|f(x)-L| < \epsilon$ I am pretty confused by the symbols too... Here's I read it: There exists a delta larger than zero such that for any epsilon larger than zero and for any $x$ satisfying $0 < |x − a| < \delta$, we will have $|f(x) − L| < \epsilon$. Does this show that there simply exists an interval where $f(x)$ is a constant function?",,"['calculus', 'limits']"
26,A couple of questions about limits,A couple of questions about limits,,"I'm doing some homework for a calculus course I am taking and, though I feel silly because I've done problems like this before, I'm having some issues. I feel like I am getting the wrong answers for a couple of problems, and I'd like a second (or third, or tenth) opinion. 1) Find $\lim_{x\to 1}f(x)$ where: $$f(x) = \left\{   \begin{array}{cc}       x^2+2, & x\neq 1\\       1, & x=1 \end{array} \right.$$ I'm thinking that the limit does not exist, since you have to plug 1 in to the equation, making x=1, meaning f(x) =1. Please, correct me if I'm wrong. 2) $\lim_{x\to -1} \frac{x^3-1}{x+1}$ I said it is undefined, but I'm not sure...","I'm doing some homework for a calculus course I am taking and, though I feel silly because I've done problems like this before, I'm having some issues. I feel like I am getting the wrong answers for a couple of problems, and I'd like a second (or third, or tenth) opinion. 1) Find $\lim_{x\to 1}f(x)$ where: $$f(x) = \left\{   \begin{array}{cc}       x^2+2, & x\neq 1\\       1, & x=1 \end{array} \right.$$ I'm thinking that the limit does not exist, since you have to plug 1 in to the equation, making x=1, meaning f(x) =1. Please, correct me if I'm wrong. 2) $\lim_{x\to -1} \frac{x^3-1}{x+1}$ I said it is undefined, but I'm not sure...",,"['calculus', 'limits']"
27,"Function satisfying $y(0)=1$ and $y(1)=4$ such that $(y')^2/y<4$ for all $x\in (0,1)$",Function satisfying  and  such that  for all,"y(0)=1 y(1)=4 (y')^2/y<4 x\in (0,1)","I am trying to find a function satisfying $$ y\left(0\right) = 1\quad\mbox{and}\quad y\left(1\right) = 4\quad\mbox{such that}\quad {\left(y'\right)^{2} \over y} < 4\,\, \forall x \in \left(0,1\right) $$ Does anyone have an idea how to go about this ?. I tried with building simple functions satisfying the first two conditions such as $4^{x}$ or $3\sin\left(\pi x/2\right) +1 $ but they all fail to satisfy the last condition.",I am trying to find a function satisfying Does anyone have an idea how to go about this ?. I tried with building simple functions satisfying the first two conditions such as or but they all fail to satisfy the last condition.,"
y\left(0\right) = 1\quad\mbox{and}\quad y\left(1\right) = 4\quad\mbox{such that}\quad {\left(y'\right)^{2} \over y} < 4\,\, \forall x \in \left(0,1\right)
 4^{x} 3\sin\left(\pi x/2\right) +1 ","['calculus', 'ordinary-differential-equations', 'calculus-of-variations']"
28,How to solve for $y$ in $(1+x)dy-ydx=0$?,How to solve for  in ?,y (1+x)dy-ydx=0,"$$(1+x)dy-ydx=0$$ My attempt: $$\frac{dy}{dx}=\frac{y}{1+x}\tag{*}$$ $$\frac{1}{y}\frac{dy}{dx}=\frac{1}{1+x}$$ $$\int \frac{dy}{y}=\int \frac{dx}{1+x}$$ $$\ln|y|+c_1=\ln|1+x|+c_2$$ $$\ln|y|=\ln|1+x|+c \ \ [\text{Let}\ c_2-c_1=c]$$ $$e^{\ln|y|}=e^{\ln|1+x|+c}$$ $$|y|=|1+x|e^c\tag{1}$$ So far so good. Now, the problems will begin. $$y=|1+x|e^c\tag{2}$$ We know, $$|1+x|=\begin{cases} (1+x),\ x\geq-1\\ -(1+x),\ x<-1 \end{cases}$$ Therefore, $$y=\begin{cases} (1+x)e^c,\ x\geq-1\\ -(1+x)e^c,\ x<-1 \end{cases}\tag{3}$$ This is my final answer. Now, I have two problems. First problem: My solution is not correct according to @geetha290krm . ""No. $y$ cannot change sign at $−1$ . $|1+x|$ is not differentiable at $−1$ . You can only have $y=(1+x)e^c$ for all $x$ or $y=−(1+x)e^c$ for all $x$ ."" According to them, $y$ cannot change sign at $x=-1$ , because then it would become undifferentiable at $x=-1$ . Now, I want to make a case for myself as to why I think $(3)$ is correct. (I'm not trying to be arrogant; I just want to spell out my agonies so that you may correct me easily): See $(1)$ . It says/defines what $y$ is . I just expanded $(1)$ , and drove it to its logical conclusion. I did not add or remove anything to $(1)$ to reach $(3)$ . I just carried $(1)$ to its logical conclusion. So, if $(1)$ is true, then $(3)$ must be true. For example, if $y=x^2-5x+6$ is true, then $y=(x-2)(x-3)$ must also be true. Another point raised by geetha290krm is that $y$ becomes undifferentiable at $x=-1$ according to my solution. However, see $(*)$ closely. Input $x=-1$ in $(*)$ . We get $\frac{dy}{dx}=\frac{y}{0}=\text{undefined}$ . So, the given problem was never differentiable at $x=-1$ to begin with. So, $(3)$ is consistent with this information (i.e. consistent with $y$ being undifferentiable at $x=-1$ ). These are the reasons why I think $(3)$ is the correct answer. Also, the answer provided by geetha does not make sense to me. According to him, ""You can only have $y=(1+x)e^c$ for all $x$ or $y=−(1+x)e^c$ for all $x$ "", but $y=(1+x)e^c$ cannot be true for all $x$ . It can only be true for $x\geq-1$ . Similarly, $y=-(1+x)e^c$ cannot be true for all $x$ . We would be distorting $y$ then. $y=-(1+x)e^c$ can only be true for $x<-1$ as defined by $(1)$ . If you agree with me that $(1)$ is true, then you must agree with me that $(3)$ is also true because $(1)$ and $(3)$ are the same thing. They are only two sides of the same coin. Second problem: This issue is separate from the issue I just described. Is going from $(1)$ to $(2)$ valid? Aren't we incorrectly saying that $|y|=y$ by saying that going from $(1)$ to $(2)$ is valid? It should be $|y|=\begin{cases} y,\ y\geq0\\ -y,\ y<0 \end{cases}$ instead of $|y|=y$ , shouldn't it? My book's given solution EDIT It now seems to me that $(2)$ is incorrect; so, $(3)$ should be: $$|y|=\begin{cases} -(1+x)e^c, &x<-1\\(1+x)e^c, &x\geq-1. \end{cases}$$","My attempt: So far so good. Now, the problems will begin. We know, Therefore, This is my final answer. Now, I have two problems. First problem: My solution is not correct according to @geetha290krm . ""No. cannot change sign at . is not differentiable at . You can only have for all or for all ."" According to them, cannot change sign at , because then it would become undifferentiable at . Now, I want to make a case for myself as to why I think is correct. (I'm not trying to be arrogant; I just want to spell out my agonies so that you may correct me easily): See . It says/defines what is . I just expanded , and drove it to its logical conclusion. I did not add or remove anything to to reach . I just carried to its logical conclusion. So, if is true, then must be true. For example, if is true, then must also be true. Another point raised by geetha290krm is that becomes undifferentiable at according to my solution. However, see closely. Input in . We get . So, the given problem was never differentiable at to begin with. So, is consistent with this information (i.e. consistent with being undifferentiable at ). These are the reasons why I think is the correct answer. Also, the answer provided by geetha does not make sense to me. According to him, ""You can only have for all or for all "", but cannot be true for all . It can only be true for . Similarly, cannot be true for all . We would be distorting then. can only be true for as defined by . If you agree with me that is true, then you must agree with me that is also true because and are the same thing. They are only two sides of the same coin. Second problem: This issue is separate from the issue I just described. Is going from to valid? Aren't we incorrectly saying that by saying that going from to is valid? It should be instead of , shouldn't it? My book's given solution EDIT It now seems to me that is incorrect; so, should be:","(1+x)dy-ydx=0 \frac{dy}{dx}=\frac{y}{1+x}\tag{*} \frac{1}{y}\frac{dy}{dx}=\frac{1}{1+x} \int \frac{dy}{y}=\int \frac{dx}{1+x} \ln|y|+c_1=\ln|1+x|+c_2 \ln|y|=\ln|1+x|+c \ \ [\text{Let}\ c_2-c_1=c] e^{\ln|y|}=e^{\ln|1+x|+c} |y|=|1+x|e^c\tag{1} y=|1+x|e^c\tag{2} |1+x|=\begin{cases} (1+x),\ x\geq-1\\ -(1+x),\ x<-1 \end{cases} y=\begin{cases} (1+x)e^c,\ x\geq-1\\ -(1+x)e^c,\ x<-1 \end{cases}\tag{3} y −1 |1+x| −1 y=(1+x)e^c x y=−(1+x)e^c x y x=-1 x=-1 (3) (1) y (1) (1) (3) (1) (1) (3) y=x^2-5x+6 y=(x-2)(x-3) y x=-1 (*) x=-1 (*) \frac{dy}{dx}=\frac{y}{0}=\text{undefined} x=-1 (3) y x=-1 (3) y=(1+x)e^c x y=−(1+x)e^c x y=(1+x)e^c x x\geq-1 y=-(1+x)e^c x y y=-(1+x)e^c x<-1 (1) (1) (3) (1) (3) (1) (2) |y|=y (1) (2) |y|=\begin{cases} y,\ y\geq0\\ -y,\ y<0 \end{cases} |y|=y (2) (3) |y|=\begin{cases} -(1+x)e^c, &x<-1\\(1+x)e^c, &x\geq-1. \end{cases}","['calculus', 'ordinary-differential-equations', 'absolute-value']"
29,Integrate $\sqrt{1+x^2+y^2}$,Integrate,\sqrt{1+x^2+y^2},"Calculate $$I=\int_{-1}^1\int_{-1}^1\sqrt{1+x^2+y^2}\,\mathrm{d}y\,\mathrm{d}x.$$ It's a problem from a book about calculus. My attempt: $$\begin{align} I &= \int_{-1}^1\int_{-1}^1\sqrt{1+x^2+y^2}\,\mathrm{d}y\,\mathrm{d}x \\   &= \int_{-1}^1\left.\frac{x}{2}\sqrt{1+x^2+y^2}+\frac{1+y^2}{2}\log\left(x+\sqrt{1+x^2+y^2}\right)\right|_{-1}^{1}\,\mathrm{d}y \\   &= \int_{-1}^1\sqrt{2+y^2}+\frac{1+y^2}{2}\left(\log \left(\sqrt{2+y^2}+1\right)-\log\left(\sqrt{2+y^2}-1\right)\right)\,\mathrm{d}y \\   &= \sqrt{3}+2\operatorname{arsinh}\frac{1}{\sqrt{2}}+\int_{-1}^1(y^2+1) \operatorname{arsinh} \frac{1}{\sqrt{y^2+1}}\,\mathrm{d}y \\   &= \color{red}\ldots \\   &= -\frac{2}{9} (\pi + 12 \log 2 - 6 \sqrt{3} - 24 \log (1+ \sqrt{3})) \end{align}$$ (answer taken from the solutions, no idea how to reach it). [edit] Here is an attempt with polar coordinates. Due to symmetry, it's enough to integrate over $0 \le x \le 1$ and $0 \le y \le x$ , 1/8th of the initial square. $$\begin{align} I &= 8\int_0^{\pi/4} \int_0^{1/\cos \theta}r \sqrt{1+r^2}\,\mathrm{d}\theta\\ &= 8\int_0^{1/\cos \theta} \frac{(1+1/\cos^2\theta)^{3/2}-1}{3}\,\mathrm{d}\theta\\ &= {?} \end{align}$$","Calculate It's a problem from a book about calculus. My attempt: (answer taken from the solutions, no idea how to reach it). [edit] Here is an attempt with polar coordinates. Due to symmetry, it's enough to integrate over and , 1/8th of the initial square.","I=\int_{-1}^1\int_{-1}^1\sqrt{1+x^2+y^2}\,\mathrm{d}y\,\mathrm{d}x. \begin{align}
I &= \int_{-1}^1\int_{-1}^1\sqrt{1+x^2+y^2}\,\mathrm{d}y\,\mathrm{d}x \\
  &= \int_{-1}^1\left.\frac{x}{2}\sqrt{1+x^2+y^2}+\frac{1+y^2}{2}\log\left(x+\sqrt{1+x^2+y^2}\right)\right|_{-1}^{1}\,\mathrm{d}y \\
  &= \int_{-1}^1\sqrt{2+y^2}+\frac{1+y^2}{2}\left(\log \left(\sqrt{2+y^2}+1\right)-\log\left(\sqrt{2+y^2}-1\right)\right)\,\mathrm{d}y \\
  &= \sqrt{3}+2\operatorname{arsinh}\frac{1}{\sqrt{2}}+\int_{-1}^1(y^2+1) \operatorname{arsinh} \frac{1}{\sqrt{y^2+1}}\,\mathrm{d}y \\
  &= \color{red}\ldots \\
  &= -\frac{2}{9} (\pi + 12 \log 2 - 6 \sqrt{3} - 24 \log (1+ \sqrt{3}))
\end{align} 0 \le x \le 1 0 \le y \le x \begin{align}
I &= 8\int_0^{\pi/4} \int_0^{1/\cos \theta}r \sqrt{1+r^2}\,\mathrm{d}\theta\\
&= 8\int_0^{1/\cos \theta} \frac{(1+1/\cos^2\theta)^{3/2}-1}{3}\,\mathrm{d}\theta\\
&= {?}
\end{align}","['calculus', 'definite-integrals']"
30,Derivative of $ \frac{1}{x}$ geometrically,Derivative of  geometrically, \frac{1}{x},"Context: ""Essence of calculus"" series called ""Derivative formulas through geometry""- 3rd episode of chapter 3 , 3Blue1Brown : From 10:08 - 12:35 he discusses the derivative of $\frac{1}{x}$ geometrically.  After the discussion of the basic idea, it is given as an exercise to solve. I tried solving it but I always come to conclusion that $\frac{d}{dx} \frac{1}{x} = -\frac{1}{x(x+dx)}$ , or $ -\frac{1}{(x^2 +xdx)}$ . Is it allowed to neglect "" $x  dx$ ""? Or is there another correct method? My try: $$ \begin{align}A_{(lost)} &= A_{(gained)} \\ A_{(lost)} &= x * -d(\frac{1}{x})  \\ A_{(gained)}&= dx \frac{1}{(x+dx)} \end{align}$$ note:- $d(\frac{1}{x})$ is negative because the change was negative (between $x$ and $x+dx$ ) That means that $$x * -d(\frac{1}{x}) = dx  \frac{ 1}{(x+dx)}$$ Isolating the $d(\frac{1}{x})$ yields: $$d(\frac{1}{x}) = -\frac{dx}{x(x+dx)}$$ Dividing the dx from both sides give you the derivative: $$\frac{d(\frac{1}{x})}{dx} = -\frac{1}{x(x+dx)} = -\frac{1}{(x^2 + x  dx)}$$","Context: ""Essence of calculus"" series called ""Derivative formulas through geometry""- 3rd episode of chapter 3 , 3Blue1Brown : From 10:08 - 12:35 he discusses the derivative of geometrically.  After the discussion of the basic idea, it is given as an exercise to solve. I tried solving it but I always come to conclusion that , or . Is it allowed to neglect "" ""? Or is there another correct method? My try: note:- is negative because the change was negative (between and ) That means that Isolating the yields: Dividing the dx from both sides give you the derivative:",\frac{1}{x} \frac{d}{dx} \frac{1}{x} = -\frac{1}{x(x+dx)}  -\frac{1}{(x^2 +xdx)} x  dx  \begin{align}A_{(lost)} &= A_{(gained)} \\ A_{(lost)} &= x * -d(\frac{1}{x})  \\ A_{(gained)}&= dx \frac{1}{(x+dx)} \end{align} d(\frac{1}{x}) x x+dx x * -d(\frac{1}{x}) = dx  \frac{ 1}{(x+dx)} d(\frac{1}{x}) d(\frac{1}{x}) = -\frac{dx}{x(x+dx)} \frac{d(\frac{1}{x})}{dx} = -\frac{1}{x(x+dx)} = -\frac{1}{(x^2 + x  dx)},"['calculus', 'derivatives']"
31,Why does stereographic projection appear here?,Why does stereographic projection appear here?,,"I'm working on some calc III problems, and found that the unit tangent vector of $$\langle t + \dfrac{1}{t}, 2\ln(t)\rangle $$ is $$\left\langle \dfrac{t^2 - 1}{t^2 + 1}, \dfrac{2t}{t^2 + 1} \right\rangle.$$ This is weird to me! I recognize this expression as the same equation as projecting the line onto the circle, i.e. the two-dimensional stereographic projection. Is this a coincidence, or is there something special happening here? I'm very suspicious.","I'm working on some calc III problems, and found that the unit tangent vector of is This is weird to me! I recognize this expression as the same equation as projecting the line onto the circle, i.e. the two-dimensional stereographic projection. Is this a coincidence, or is there something special happening here? I'm very suspicious.","\langle t + \dfrac{1}{t}, 2\ln(t)\rangle  \left\langle \dfrac{t^2 - 1}{t^2 + 1}, \dfrac{2t}{t^2 + 1} \right\rangle.","['calculus', 'multivariable-calculus', 'stereographic-projections']"
32,"Generalization of $\int_0^\alpha \sqrt{1+\cos^2\theta}\,d\theta>\sqrt{\alpha^2+\sin^2\alpha}$",Generalization of,"\int_0^\alpha \sqrt{1+\cos^2\theta}\,d\theta>\sqrt{\alpha^2+\sin^2\alpha}","I came across a problem that required proving a specific case and then going on to generalize it. While I have no problem with the first part, I need some confidence from someone about the second part. Here we go. The problem statement : Show that for $\displaystyle 0<\alpha\leq \frac{\pi}{2}$ $$ \int_0^\alpha \sqrt{1+\cos^2\theta}\,d\theta>\sqrt{\alpha^2+\sin^2\alpha}\tag{1} $$ Generalize the result in part (1). My Answer Attempt : Clearly, LHS of $(1)$ represents the length of the curve $f(\theta)=\sin\theta$ from $\theta=0$ to $\theta=\alpha$ and the RHS represents the distance from the origin to the point $(\alpha,\sin\alpha)$ , that is the length of the line from origin to the said point. Curve in red and Line in blue. Since both the curve and the line pass through the origin, and the shortest distance between two points is a straight line, we have $(1)$ proven. The generalization that I've come up with: For any continuous (not necessarily smooth) curve $f(x)$ in $[0,a]$ such that $f(0)=0$ and $a\in\mathbb{R}$ , we have $$\left |\int_0^a \sqrt{1+\big[f'(x)\big]^2}dx\right|\geq \sqrt{a^2+\big[f(a)\big]^2} $$ Equality holding if the curve is a straight line or $a=0$ . This is basically saying that the shortest distance between two points is a straight line. The points in our case being the origin and a point on the curve $f(x)$ . The curve has to pass through the origin because otherwise the relation may not hold true. Example: The curve $f(x)=3$ in $[0,\infty)$ . Is this good?","I came across a problem that required proving a specific case and then going on to generalize it. While I have no problem with the first part, I need some confidence from someone about the second part. Here we go. The problem statement : Show that for Generalize the result in part (1). My Answer Attempt : Clearly, LHS of represents the length of the curve from to and the RHS represents the distance from the origin to the point , that is the length of the line from origin to the said point. Curve in red and Line in blue. Since both the curve and the line pass through the origin, and the shortest distance between two points is a straight line, we have proven. The generalization that I've come up with: For any continuous (not necessarily smooth) curve in such that and , we have Equality holding if the curve is a straight line or . This is basically saying that the shortest distance between two points is a straight line. The points in our case being the origin and a point on the curve . The curve has to pass through the origin because otherwise the relation may not hold true. Example: The curve in . Is this good?","\displaystyle 0<\alpha\leq \frac{\pi}{2} 
\int_0^\alpha \sqrt{1+\cos^2\theta}\,d\theta>\sqrt{\alpha^2+\sin^2\alpha}\tag{1}
 (1) f(\theta)=\sin\theta \theta=0 \theta=\alpha (\alpha,\sin\alpha) (1) f(x) [0,a] f(0)=0 a\in\mathbb{R} \left |\int_0^a \sqrt{1+\big[f'(x)\big]^2}dx\right|\geq \sqrt{a^2+\big[f(a)\big]^2}  a=0 f(x) f(x)=3 [0,\infty)","['calculus', 'definite-integrals', 'solution-verification']"
33,(conceptual question) What kind of minima do we expect stochastic gradient descent to get stuck on and why?,(conceptual question) What kind of minima do we expect stochastic gradient descent to get stuck on and why?,,"Suppose you want to find $k$ that minimises your cost function $J_D(k)$ for the whole dataset $D$ . We may want to apply batch gradient descent or stochastic gradient descent. Let's deliberately initialise $k$ with the same number $k_0 = 1$ for both BGD and SGD to see the difference in their behavior. If you apply BGD, the whole process may look like this: On the other hand if you apply SGD, this optimisation may look like this: In both pictures the blue solid curve represents the cost function $J_D(k)$ . But in the second picture there are also dotted curves. In my experiment I used batch size which was $10\text%$ of the whole dataset $D$ . So each dotted curve represents the cost function $J_B(k)$ for the current batch $B$ . From these dotted curves you can see that the gradient $\nabla J_B\left(k_0\right)$ happened to be large multiple times in a row for different batches $B$ . That's why the point was ""pushed"" to the deeper minimum. As I understand we use SGD hoping that there is such a big $\nabla J_B\left(k_0\right)$ for some batch $B$ so that the red point is ""pushed"" to jump out of this local minimum for another chance at arriving at a better minimum. But I'm stuck here. Why does stochastic gradient descent lead us to a minimum at all? Why can't it escape all the minima? Why do we think that another local minimum is going to be deeper than the initial one? I don't believe that we just hope that our new minimum is going to be good enough. With the same success we could randomly choose a value for $k$ . If we don't think that another local minimum is going to be deeper, then how is SGD supposed to avoid local minima problem? Our red point can end up in a minimum that is shallower (higher) than the initial one, e.g.: If BGD looks for the nearest minimum, then what kind of minimum does SGD look for? How do we know for certain that it's not going to escape a deeper minimum? How deep should it be? What kind of minima do we expect stochastic gradient descent to get stuck on and why do we think it's going to be deeper than the minimum we can obtain with a normal gradient descent? How SGD is supposed to avoid local minima problem if all it can is just push us to jump out of a local minimum? I mean it doesn't look for a better one but only wandering along the curvature. As a side note, $J_D(k) = \frac 1n\sum_{i=1}^n\left(\sin\left(kx_i\right) - y_i\right)^2$ . EDIT 1: Need some clarification of @WhoDatBoy's answer. Since we randomly selecting each batch, the single batch's distribution is going to be similar to that of the whole dataset. And this distribution uniquely determines the distribution of residuals of each batch. That's why each batch's gradient is going to be similar to that of the whole dataset. Is that right? Now, I perfectly understand why the red point can't usually escape from wide minima: it's very unlikely to select a batch with gradient that differs from the whole dataset multiple times in a row. However there is still a thing that confuses me. You said that SGD was not invented to be robust against local minima. But it's told to be likely to reach a better minimum than the initial one. And I can see why in the case when the red point was initialised in some local minimum near a wide minimum: there's a chance that some batch's gradient will push it from the shallow minimum towards the wider one. But what if our cost function looks like this: Some batch can push the point to the left in the shallower minimum. The point can stay there for a while and after that it can be pushed again towards even shallower minimum. Question 1: Is it highly unlikely case, since there are always batches seeking to push the point to the right? Or consider the following situation: Despite the fact that the initial minimum is deeper, but it's very narrow. So, the point can be easily pushed out of it towards the shallower minimum. In both these cases SGD can fail. Question 2: I'm not sure about the first one but in the second plot we definitely can't say that the red point is likely to find a better minimum, right? I mean, all the minima are too narrow for the point to stay there. Question 3: Is it true that only wide minima can hold the point (no matter deep or not)? And how wide should it be depends on the batch size we choose? Question 4: And since we don't know it in advance, we just try to guess its size, right? Question 5: It turns out then, the depth of the minimum doesn't play much role in holding the red point. It's the width of the minimum that matters? Question 6: Do we assume something before applying SGD? If yes, then what exactly? I mean is there some kind of assumption of the form: ""SGD is likely to find a better minimum if the curvature does not have only narrow minima and is not too hilly"". EDIT 2: All over this edit I assume that we have the same batch size and the same learning rate and, for the sake of simplicity, assume that all those minima $A$ , $B$ and $C$ (denoted below) have the same width (but different depth). CONFUSION 1 : In your Question 5 answer you said that the depth is important . Doesn't it mean that the deeper the minimum is, the harder it is for the red point to escape that minimum? Thus, we can conclude that the red point is likely to stuck in relatively deep minima when using SGD. The word ""relatively"" is used because the depth that is able to hold the red point depends on the batch size and the learning rate: the smaller the batch size and the bigger the learning rate, the deeper minima the red point is looking for . By ""looking for"" I mean that the red point is going to get stuck in such minima. However, we don't know how deep the minimum has to be in absolute value. CONFUSION 2 : It's still unclear why the red point is likely to get stuck in deeper minima. Suppose, for the sake of example, that we have a dataset of $100$ observation and 3 minima in our cost function curvature: $A$ , $B$ and $C$ . The respective errors (values of our cost function) at those minima are: $\mathrm{error}(A) = 100$ , $\mathrm{error}(B) = 10$ and $\mathrm{error}(C) = 0$ . Now, when the red point gets into the minimum $C$ , then each of $100$ observations has $0$ error and therefore $0$ gradient. So, whatever batch you choose it's going to have $0$ gradient, since its gradient is the sum of gradients of the observations the batch consists of. Consequently, it's impossible for the red point to escape from the minimum $C$ . And here is my main confusion . Why is the red point less likely to escape the minimum $B$ (the deeper one) than the minimum $A$ ? It would be nice to explain it in the following way: ""since the $\mathrm{error}(A) > \mathrm{error}(B)$ , then the gradient of each observation is smaller in the minimum $B$ and therefore every batch now has smaller gradient which causes smaller ability to push the red point out of the minimum $B$ "". But the problem is that we can NOT claim such a thing, since the error reduction in the minimum $B$ compared to the minimum $A$ could be caused by a single observation. I mean, if the error of a single observation, say the first one out of our 100 observations, reduced significantly, then it would cause a reduction in the error of our cost function. But the rest of the observations has the same error as before and therefore the same gradient. And since we're randomly picking the batch on each iteration, our red point can still be pushed out of the minimum $B$ with the same probability ( am I wrong in here regarding the same probability? ). CONFUSION 3 : It becomes even more confusing when the reduction in the error of a single observation is not the case, and the error of our cost function is reduced due to the fact that overall error reduced in some observations is greater than the overall error raised in other observations. The minimum would be deeper in this case, but how to show that the red point is now less likely to escape from this new deeper minimum? I want to note here that in my understanding, the reduction in the cost function does not mean either the reduction in the gradient of the whole dataset or the reduction in the gradient of some individual batch. Then how on earth can the reduction in the cost function (and this is exactly what the deeper minimum means) mean smaller ability to escape the minimum?","Suppose you want to find that minimises your cost function for the whole dataset . We may want to apply batch gradient descent or stochastic gradient descent. Let's deliberately initialise with the same number for both BGD and SGD to see the difference in their behavior. If you apply BGD, the whole process may look like this: On the other hand if you apply SGD, this optimisation may look like this: In both pictures the blue solid curve represents the cost function . But in the second picture there are also dotted curves. In my experiment I used batch size which was of the whole dataset . So each dotted curve represents the cost function for the current batch . From these dotted curves you can see that the gradient happened to be large multiple times in a row for different batches . That's why the point was ""pushed"" to the deeper minimum. As I understand we use SGD hoping that there is such a big for some batch so that the red point is ""pushed"" to jump out of this local minimum for another chance at arriving at a better minimum. But I'm stuck here. Why does stochastic gradient descent lead us to a minimum at all? Why can't it escape all the minima? Why do we think that another local minimum is going to be deeper than the initial one? I don't believe that we just hope that our new minimum is going to be good enough. With the same success we could randomly choose a value for . If we don't think that another local minimum is going to be deeper, then how is SGD supposed to avoid local minima problem? Our red point can end up in a minimum that is shallower (higher) than the initial one, e.g.: If BGD looks for the nearest minimum, then what kind of minimum does SGD look for? How do we know for certain that it's not going to escape a deeper minimum? How deep should it be? What kind of minima do we expect stochastic gradient descent to get stuck on and why do we think it's going to be deeper than the minimum we can obtain with a normal gradient descent? How SGD is supposed to avoid local minima problem if all it can is just push us to jump out of a local minimum? I mean it doesn't look for a better one but only wandering along the curvature. As a side note, . EDIT 1: Need some clarification of @WhoDatBoy's answer. Since we randomly selecting each batch, the single batch's distribution is going to be similar to that of the whole dataset. And this distribution uniquely determines the distribution of residuals of each batch. That's why each batch's gradient is going to be similar to that of the whole dataset. Is that right? Now, I perfectly understand why the red point can't usually escape from wide minima: it's very unlikely to select a batch with gradient that differs from the whole dataset multiple times in a row. However there is still a thing that confuses me. You said that SGD was not invented to be robust against local minima. But it's told to be likely to reach a better minimum than the initial one. And I can see why in the case when the red point was initialised in some local minimum near a wide minimum: there's a chance that some batch's gradient will push it from the shallow minimum towards the wider one. But what if our cost function looks like this: Some batch can push the point to the left in the shallower minimum. The point can stay there for a while and after that it can be pushed again towards even shallower minimum. Question 1: Is it highly unlikely case, since there are always batches seeking to push the point to the right? Or consider the following situation: Despite the fact that the initial minimum is deeper, but it's very narrow. So, the point can be easily pushed out of it towards the shallower minimum. In both these cases SGD can fail. Question 2: I'm not sure about the first one but in the second plot we definitely can't say that the red point is likely to find a better minimum, right? I mean, all the minima are too narrow for the point to stay there. Question 3: Is it true that only wide minima can hold the point (no matter deep or not)? And how wide should it be depends on the batch size we choose? Question 4: And since we don't know it in advance, we just try to guess its size, right? Question 5: It turns out then, the depth of the minimum doesn't play much role in holding the red point. It's the width of the minimum that matters? Question 6: Do we assume something before applying SGD? If yes, then what exactly? I mean is there some kind of assumption of the form: ""SGD is likely to find a better minimum if the curvature does not have only narrow minima and is not too hilly"". EDIT 2: All over this edit I assume that we have the same batch size and the same learning rate and, for the sake of simplicity, assume that all those minima , and (denoted below) have the same width (but different depth). CONFUSION 1 : In your Question 5 answer you said that the depth is important . Doesn't it mean that the deeper the minimum is, the harder it is for the red point to escape that minimum? Thus, we can conclude that the red point is likely to stuck in relatively deep minima when using SGD. The word ""relatively"" is used because the depth that is able to hold the red point depends on the batch size and the learning rate: the smaller the batch size and the bigger the learning rate, the deeper minima the red point is looking for . By ""looking for"" I mean that the red point is going to get stuck in such minima. However, we don't know how deep the minimum has to be in absolute value. CONFUSION 2 : It's still unclear why the red point is likely to get stuck in deeper minima. Suppose, for the sake of example, that we have a dataset of observation and 3 minima in our cost function curvature: , and . The respective errors (values of our cost function) at those minima are: , and . Now, when the red point gets into the minimum , then each of observations has error and therefore gradient. So, whatever batch you choose it's going to have gradient, since its gradient is the sum of gradients of the observations the batch consists of. Consequently, it's impossible for the red point to escape from the minimum . And here is my main confusion . Why is the red point less likely to escape the minimum (the deeper one) than the minimum ? It would be nice to explain it in the following way: ""since the , then the gradient of each observation is smaller in the minimum and therefore every batch now has smaller gradient which causes smaller ability to push the red point out of the minimum "". But the problem is that we can NOT claim such a thing, since the error reduction in the minimum compared to the minimum could be caused by a single observation. I mean, if the error of a single observation, say the first one out of our 100 observations, reduced significantly, then it would cause a reduction in the error of our cost function. But the rest of the observations has the same error as before and therefore the same gradient. And since we're randomly picking the batch on each iteration, our red point can still be pushed out of the minimum with the same probability ( am I wrong in here regarding the same probability? ). CONFUSION 3 : It becomes even more confusing when the reduction in the error of a single observation is not the case, and the error of our cost function is reduced due to the fact that overall error reduced in some observations is greater than the overall error raised in other observations. The minimum would be deeper in this case, but how to show that the red point is now less likely to escape from this new deeper minimum? I want to note here that in my understanding, the reduction in the cost function does not mean either the reduction in the gradient of the whole dataset or the reduction in the gradient of some individual batch. Then how on earth can the reduction in the cost function (and this is exactly what the deeper minimum means) mean smaller ability to escape the minimum?",k J_D(k) D k k_0 = 1 J_D(k) 10\text% D J_B(k) B \nabla J_B\left(k_0\right) B \nabla J_B\left(k_0\right) B k J_D(k) = \frac 1n\sum_{i=1}^n\left(\sin\left(kx_i\right) - y_i\right)^2 A B C 100 A B C \mathrm{error}(A) = 100 \mathrm{error}(B) = 10 \mathrm{error}(C) = 0 C 100 0 0 0 C B A \mathrm{error}(A) > \mathrm{error}(B) B B B A B,"['calculus', 'multivariable-calculus', 'stochastic-processes', 'machine-learning', 'gradient-descent']"
34,"Analytic solution to $\alpha , \beta \in \mathbb{R}$ such that $\cos \alpha \cdot \sin \beta = \cos \bigl(\sin (\alpha \cdot \beta)\bigr)$?",Analytic solution to  such that ?,"\alpha , \beta \in \mathbb{R} \cos \alpha \cdot \sin \beta = \cos \bigl(\sin (\alpha \cdot \beta)\bigr)","Are there any $\alpha , \beta \in \mathbb{R}$ such that $$\cos \alpha \cdot \sin \beta = \cos \bigl(\sin (\alpha \cdot \beta)\bigr)?$$ The trivial solutions are $(\alpha, \beta)=(0, \dfrac{\pi}{2})$ . But are there more? I dont see an obvious way of tackling this problem but I keep coming back to it because it seems interesting to me (look at the graph of the solutions in desmos: https://www.desmos.com/calculator/ms8ad8cxqt ). I tried simplifying matters by trying out the case where $\alpha = \beta$ where we get that $$\cos \alpha \cdot \sin \alpha = \cos \bigl(\sin (\alpha ^{2})\bigr)$$ and using the fact that $2\sin \alpha \cos \alpha = \sin 2\alpha$ we really want to find $\alpha \in \mathbb{R}$ such that $$\dfrac{1}{2}\sin 2\alpha = \cos \bigl(\sin (\alpha ^{2}) \bigr),$$ though I have to admit that this does not make the problem easier (it seems to me at least). Has anyone looked at this problem before and have a solution or maybe some hints or suggestions on how to go further in solving the problem?",Are there any such that The trivial solutions are . But are there more? I dont see an obvious way of tackling this problem but I keep coming back to it because it seems interesting to me (look at the graph of the solutions in desmos: https://www.desmos.com/calculator/ms8ad8cxqt ). I tried simplifying matters by trying out the case where where we get that and using the fact that we really want to find such that though I have to admit that this does not make the problem easier (it seems to me at least). Has anyone looked at this problem before and have a solution or maybe some hints or suggestions on how to go further in solving the problem?,"\alpha , \beta \in \mathbb{R} \cos \alpha \cdot \sin \beta = \cos \bigl(\sin (\alpha \cdot \beta)\bigr)? (\alpha, \beta)=(0, \dfrac{\pi}{2}) \alpha = \beta \cos \alpha \cdot \sin \alpha = \cos \bigl(\sin (\alpha ^{2})\bigr) 2\sin \alpha \cos \alpha = \sin 2\alpha \alpha \in \mathbb{R} \dfrac{1}{2}\sin 2\alpha = \cos \bigl(\sin (\alpha ^{2}) \bigr),","['calculus', 'trigonometry', 'recreational-mathematics']"
35,Examples of functions with a removable discontinuity,Examples of functions with a removable discontinuity,,"I recently came across the following example of a function which is discontinuous at a point: The cost of a sushi buffet is calculated by weighing your plate, where you are then charged £10 per pound. As part of a promotion, if you manage to fill your plate so that the weight is precisely 1lbs, you get your meal for free. This is a nice way to motivate a function like $$f(x) = \begin{cases}10\,x\hfil&\text{$x\neq 1$}\\\hfil0\hfil&\text{$x=1$}\end{cases}$$ which are nice for illustrating the idea of a limit to students. Are there any other real-world examples of functions like this, for the sake of explaining limits? Ideally one where you would actually want the limiting value, and not the assigned value, to motivate the concept further. (Apart from the obvious difference quotient.)","I recently came across the following example of a function which is discontinuous at a point: The cost of a sushi buffet is calculated by weighing your plate, where you are then charged £10 per pound. As part of a promotion, if you manage to fill your plate so that the weight is precisely 1lbs, you get your meal for free. This is a nice way to motivate a function like which are nice for illustrating the idea of a limit to students. Are there any other real-world examples of functions like this, for the sake of explaining limits? Ideally one where you would actually want the limiting value, and not the assigned value, to motivate the concept further. (Apart from the obvious difference quotient.)","f(x) = \begin{cases}10\,x\hfil&\text{x\neq 1}\\\hfil0\hfil&\text{x=1}\end{cases}","['calculus', 'algebra-precalculus', 'education']"
36,Is this the right way of solving $\frac{d}{dx}(\sin(x)\cdot x^2)$,Is this the right way of solving,\frac{d}{dx}(\sin(x)\cdot x^2),"I don't know much about theorems related to limits and I'm currently learning calculus from 3Blue1Brown's online series : Essence of Calculus . This example's generalization is what I'll use to derive the product rule. Now, if we have a rectangle with length and breadth equal to $\sin(x)$ and $x^2$ for some values of $x$ , then it's area will be : $\sin(x)\cdot x^2$ . Now, if we ""nudge"" the value of $x$ by some little amount, say, $dx$ , then there will be corresponding changes in the values of $\sin(x)$ and $x^2$ . Let the little change in $\sin(x)$ be $d(\sin(x))$ and the little change in $x^2$ be $dx^2$ . Now, the change in the value of $(\sin(x)\cdot x^2)$ i.e. $d(\sin(x)\cdot x^2)$ will be the sum of the three new strips of area. $$\therefore~ d(\sin(x)\cdot x^2) = \sin(x)\cdot dx^2+x^2\cdot d(\sin(x))+dx^2\cdot d(\sin(x))$$ . Now, $dx^2 = 2x\cdot dx$ , $d(\sin(x)) = \cos(x)\cdot dx$ . $$\therefore~ \dfrac{d(\sin(x)\cdot x^2)}{dx} = \dfrac{\sin(x)\cdot dx^2+x^2\cdot d(\sin(x))+dx^2\cdot d(\sin(x))}{dx}$$ $$ = \dfrac{\sin(x)\cdot 2x\cdot dx + x^2\cdot\cos(x)\cdot dx + 2x\cdot dx\cdot \cos(x)\cdot dx}{dx}$$ $$ = \sin(x)\cdot 2x + x^2\cdot \cos(x) + 2x\cdot \cos(x)\cdot dx$$ Now, as $dx \rightarrow 0$ , $\dfrac{d(\sin(x)\cdot x^2)}{dx} \rightarrow \sin(x)\cdot 2x + x^2\cdot\cos(x)$ because anything in the form of $p(dx)^n$ , where $p \in \Bbb R$ and $n \in \Bbb Z^+$ will approach $0$ as well. So, we can say that $\dfrac{d(\sin(x)\cdot x^2)}{dx} = \sin(x)\cdot 2x + x^2\cdot\cos(x)$ as $$\dfrac{d}{dx} f(x) = \lim_{\Delta x \rightarrow 0}\dfrac{f(x+\Delta x) - f(x)}{\Delta x}$$ I want to know if I've done all of this correctly, without any conceptual mistakes. Thanks!","I don't know much about theorems related to limits and I'm currently learning calculus from 3Blue1Brown's online series : Essence of Calculus . This example's generalization is what I'll use to derive the product rule. Now, if we have a rectangle with length and breadth equal to and for some values of , then it's area will be : . Now, if we ""nudge"" the value of by some little amount, say, , then there will be corresponding changes in the values of and . Let the little change in be and the little change in be . Now, the change in the value of i.e. will be the sum of the three new strips of area. . Now, , . Now, as , because anything in the form of , where and will approach as well. So, we can say that as I want to know if I've done all of this correctly, without any conceptual mistakes. Thanks!",\sin(x) x^2 x \sin(x)\cdot x^2 x dx \sin(x) x^2 \sin(x) d(\sin(x)) x^2 dx^2 (\sin(x)\cdot x^2) d(\sin(x)\cdot x^2) \therefore~ d(\sin(x)\cdot x^2) = \sin(x)\cdot dx^2+x^2\cdot d(\sin(x))+dx^2\cdot d(\sin(x)) dx^2 = 2x\cdot dx d(\sin(x)) = \cos(x)\cdot dx \therefore~ \dfrac{d(\sin(x)\cdot x^2)}{dx} = \dfrac{\sin(x)\cdot dx^2+x^2\cdot d(\sin(x))+dx^2\cdot d(\sin(x))}{dx}  = \dfrac{\sin(x)\cdot 2x\cdot dx + x^2\cdot\cos(x)\cdot dx + 2x\cdot dx\cdot \cos(x)\cdot dx}{dx}  = \sin(x)\cdot 2x + x^2\cdot \cos(x) + 2x\cdot \cos(x)\cdot dx dx \rightarrow 0 \dfrac{d(\sin(x)\cdot x^2)}{dx} \rightarrow \sin(x)\cdot 2x + x^2\cdot\cos(x) p(dx)^n p \in \Bbb R n \in \Bbb Z^+ 0 \dfrac{d(\sin(x)\cdot x^2)}{dx} = \sin(x)\cdot 2x + x^2\cdot\cos(x) \dfrac{d}{dx} f(x) = \lim_{\Delta x \rightarrow 0}\dfrac{f(x+\Delta x) - f(x)}{\Delta x},"['calculus', 'derivatives', 'solution-verification']"
37,Classifying Critical Point in 3D,Classifying Critical Point in 3D,,"Question: $f(x, y, z) = px^2 +q(y^2 + z^2) +rxy + syz$ where $p,q,r,s \in \mathbb{R}$ has a critical point at $(0, 0, 0)$ . Classify this critical point. You can assume the product of $p$ and $q$ is positive. Also, $r$ and $s$ cannot be both equal to zero (either one is zero and the other is not, or neither are zero). Attempt: I've found the Hessian matrix evaluated at the critical point $H=\begin{bmatrix} 2p & r & 0 \\ r & 2q & s \\ 0 & s & 2q \\\end{bmatrix}$ . I've tried to find the eigenvalues ( $\lambda$ ) of $H$ to assess whether the point is a local minimum, maximum or saddle point, but ended up with a long messy equation that cannot be factorised easily to solve for $\lambda$ : $$(2p - \lambda)((2q-\lambda)^2-s^2)-r^2(2q-\lambda)=0$$ which expands to $$-2r^2q+\lambda r^2 -2ps^2 +8q^2p-8\lambda qp +2\lambda ^2p+\lambda s^2 -4\lambda q^2 +4\lambda ^2 q-\lambda^3=0$$ Using Wolfram Alpha to solve this and find the eigenvalues gives these three solutions , which pushes me to consider another strategy. So my next attempt was to see if I could classify the point via this method (see page 3) because I figured it would break it down into smaller more easier to manage equations with less unknowns, however it got pretty messy having to consider the two cases ( $p,q>0$ and $p,q<0$ ) and then the three subcases (both nonzero $r$ and $s$ , $r=0$ and nonzero $s$ , nonzero $r$ and $s=0$ ): $f_{xx}(0, 0, 0) = 2p$ $\det \begin{bmatrix} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{bmatrix}=\det \begin{bmatrix} 2p & r \\ r & 2q \end{bmatrix} = 4pq-r^2$ $\det H = \det \begin{bmatrix} 2p & r & 0 \\ r & 2q & s \\ 0 & s & 2q \\\end{bmatrix} = 2p(4q^2 - s^2)-2r^2q = 8pq^2 - 2ps^2 - 2r^2 q$ Dealing with the signs of the constants is really what's throwing me, as I am comfortable with the classification process. Any help would be greatly appreciated. Edit: I'm pretty sure the type of critical point will be different depending on the different cases of what $p, q, r, s$ are (i.e. $p,q \gt 0$ or $p, q \lt 0$ , and then subcases concerning $r$ and $s$ and whether they're zero or non-zero, remembering that they cannot be both zero).","Question: where has a critical point at . Classify this critical point. You can assume the product of and is positive. Also, and cannot be both equal to zero (either one is zero and the other is not, or neither are zero). Attempt: I've found the Hessian matrix evaluated at the critical point . I've tried to find the eigenvalues ( ) of to assess whether the point is a local minimum, maximum or saddle point, but ended up with a long messy equation that cannot be factorised easily to solve for : which expands to Using Wolfram Alpha to solve this and find the eigenvalues gives these three solutions , which pushes me to consider another strategy. So my next attempt was to see if I could classify the point via this method (see page 3) because I figured it would break it down into smaller more easier to manage equations with less unknowns, however it got pretty messy having to consider the two cases ( and ) and then the three subcases (both nonzero and , and nonzero , nonzero and ): Dealing with the signs of the constants is really what's throwing me, as I am comfortable with the classification process. Any help would be greatly appreciated. Edit: I'm pretty sure the type of critical point will be different depending on the different cases of what are (i.e. or , and then subcases concerning and and whether they're zero or non-zero, remembering that they cannot be both zero).","f(x, y, z) = px^2 +q(y^2 + z^2) +rxy + syz p,q,r,s \in \mathbb{R} (0, 0, 0) p q r s H=\begin{bmatrix} 2p & r & 0 \\ r & 2q & s \\ 0 & s & 2q \\\end{bmatrix} \lambda H \lambda (2p - \lambda)((2q-\lambda)^2-s^2)-r^2(2q-\lambda)=0 -2r^2q+\lambda r^2 -2ps^2 +8q^2p-8\lambda qp +2\lambda ^2p+\lambda s^2 -4\lambda q^2 +4\lambda ^2 q-\lambda^3=0 p,q>0 p,q<0 r s r=0 s r s=0 f_{xx}(0, 0, 0) = 2p \det \begin{bmatrix} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{bmatrix}=\det \begin{bmatrix} 2p & r \\ r & 2q \end{bmatrix} = 4pq-r^2 \det H = \det \begin{bmatrix} 2p & r & 0 \\ r & 2q & s \\ 0 & s & 2q \\\end{bmatrix} = 2p(4q^2 - s^2)-2r^2q = 8pq^2 - 2ps^2 - 2r^2 q p, q, r, s p,q \gt 0 p, q \lt 0 r s","['calculus', 'linear-algebra', 'matrices', 'multivariable-calculus', 'functions']"
38,Can we write limits variable-free?,Can we write limits variable-free?,,"When first learning the subject and when doing simple calculations, it's convenient to describe derivatives in terms of variables, i.e. $$ \frac{d}{dx} f(x) = f'(x)$$ and we say the derivative takes a ""function"" (expression) and maps it to another ""function"" (expression). But more rigorously, we can say that given an $n$ -manifold $M$ , a chart $x:U\subseteq M\rightarrow\mathbb{R}^n$ , and a function $f:M\rightarrow \mathbb{R}$ , $$ \frac{\partial}{\partial x_i} f \equiv \partial_i(f\circ x^{-1})\circ x$$ This is just one way of reformalizing derivatives, but what we have done is taken out the reliance on a naive notion of ""variables"". I was wondering if we have made any similar re-formulations for the limit. From the above equivalence, we need the notion of a limit to fully describe the partial derivative on $\mathbb{R}^n$ . I.e. for a function $g:\mathbb{R}^n\rightarrow\mathbb{R}$ , $$ \partial_ig(a_1,...,a_i,...,a_n)\equiv \lim_{h\to 0} \frac{g(a_1,...,a_i-h,...,a_n)-g(a_1,...,a_i,...,a_n)}{h} $$ You could use an ""epsilon-delta"" definition of limits if $n=1$ and you want to totally-order $\mathbb{R}$ or you could define limits using neighborhoods or open sets, but the notation is what I'm stuck on. Is there a way to formalize the limit as a map $$ \lim_{i,\ b}:\ C^0(\mathbb{R}^n)\ \to\ C^0(\mathbb{R}^n) $$ $$ \left(\lim_{i,\ b}g\right)(a_1,...,a_i,...,a_n)\equiv\ ``\,\lim_{a_i\to b}\left(g(a_1,...,a_i,...,a_n)\right)"" $$ similar to the many ways we have reformatted the derivative? For clarity, my question is this: Question: Is there a definition of the limit on Euclidean $\mathbb{R}^n$ space that doesn't require variables? If not, is there simply a notion of a limit that doesn't use variables in its notation?","When first learning the subject and when doing simple calculations, it's convenient to describe derivatives in terms of variables, i.e. and we say the derivative takes a ""function"" (expression) and maps it to another ""function"" (expression). But more rigorously, we can say that given an -manifold , a chart , and a function , This is just one way of reformalizing derivatives, but what we have done is taken out the reliance on a naive notion of ""variables"". I was wondering if we have made any similar re-formulations for the limit. From the above equivalence, we need the notion of a limit to fully describe the partial derivative on . I.e. for a function , You could use an ""epsilon-delta"" definition of limits if and you want to totally-order or you could define limits using neighborhoods or open sets, but the notation is what I'm stuck on. Is there a way to formalize the limit as a map similar to the many ways we have reformatted the derivative? For clarity, my question is this: Question: Is there a definition of the limit on Euclidean space that doesn't require variables? If not, is there simply a notion of a limit that doesn't use variables in its notation?"," \frac{d}{dx} f(x) = f'(x) n M x:U\subseteq M\rightarrow\mathbb{R}^n f:M\rightarrow \mathbb{R}  \frac{\partial}{\partial x_i} f \equiv \partial_i(f\circ x^{-1})\circ x \mathbb{R}^n g:\mathbb{R}^n\rightarrow\mathbb{R}  \partial_ig(a_1,...,a_i,...,a_n)\equiv \lim_{h\to 0} \frac{g(a_1,...,a_i-h,...,a_n)-g(a_1,...,a_i,...,a_n)}{h}  n=1 \mathbb{R}  \lim_{i,\ b}:\ C^0(\mathbb{R}^n)\ \to\ C^0(\mathbb{R}^n)   \left(\lim_{i,\ b}g\right)(a_1,...,a_i,...,a_n)\equiv\ ``\,\lim_{a_i\to b}\left(g(a_1,...,a_i,...,a_n)\right)""  \mathbb{R}^n","['calculus', 'limits', 'multivariable-calculus', 'derivatives', 'differential-topology']"
39,Find the $\lim_{n\to\infty}\text{inf} \left(\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\right)$,Find the,\lim_{n\to\infty}\text{inf} \left(\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\right),"Here is my problem: If $(x_n)_{n\in\Bbb N}$ is non-increasing, $x_0=1$ and $\lim_{n\to\infty} x_n=0$ , does the following infimum exist? $$\inf_{(x_n)_{n\in\Bbb N}} \sum_{k\in\Bbb N}\frac{x_k^2}{x_{k+1}}$$ Here is the ""meat"" of my attempt: $$(a-2b)^2\geq 0$$ So we have, $$(x_i-2x_{i+1})^2\geq 0$$ $$\frac{x_i^2}{x_{i+1}} \geq 4(x_i-x_{i+1})$$ where $i=0,1,2,\cdots ,n$ $$\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\geq 4( x_0-x_1+x_1-x_2+\cdots + x_{n-1}-x_n)=4(1-x_n)$$ Finally we get, $$\lim_{n\to\infty}\text{inf} \left(\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\right) \geq \lim_{n\to\infty} 4(1-x_n)= 4$$ $$\lim_{n\to\infty}\text{inf} \left(\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\right)=4$$ Question 1: Is this solution correct? Question 2 : Is it possible to find a completely different method to solve this problem? I'm more interested in question 2. Is it possible to solve this problem with any pure calculus technique?","Here is my problem: If is non-increasing, and , does the following infimum exist? Here is the ""meat"" of my attempt: So we have, where Finally we get, Question 1: Is this solution correct? Question 2 : Is it possible to find a completely different method to solve this problem? I'm more interested in question 2. Is it possible to solve this problem with any pure calculus technique?","(x_n)_{n\in\Bbb N} x_0=1 \lim_{n\to\infty} x_n=0 \inf_{(x_n)_{n\in\Bbb N}} \sum_{k\in\Bbb N}\frac{x_k^2}{x_{k+1}} (a-2b)^2\geq 0 (x_i-2x_{i+1})^2\geq 0 \frac{x_i^2}{x_{i+1}} \geq 4(x_i-x_{i+1}) i=0,1,2,\cdots ,n \frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\geq 4( x_0-x_1+x_1-x_2+\cdots + x_{n-1}-x_n)=4(1-x_n) \lim_{n\to\infty}\text{inf} \left(\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\right) \geq \lim_{n\to\infty} 4(1-x_n)= 4 \lim_{n\to\infty}\text{inf} \left(\frac {x_0^2}{ x_1}+\frac {x_1^2}{ x_2}+\cdots \frac {x_{n-1}^2}{ x_n}\right)=4","['calculus', 'limits', 'proof-verification', 'contest-math', 'alternative-proof']"
40,Constructing a function based on a real-world scenario,Constructing a function based on a real-world scenario,,"A random thought came into my head today when I was in the subway: Suppose we have a train in a subway where the stations are evenly spaced in a straight line.  The train accelerates for some amount of time, moves with a constant speed for some amount of time, and starts decelerating until it reaches the next station such that the same amount of time spent accelerating and decelerating are the same, and the magnitudes of both are the same.  Create a possible function that illustrates this scenario. I decided that I would try to use some combination of a sinusoidal function and a linear function, since the train would try to do the same thing during certain periods of time, and the train is moving only in one direction.  I hopped onto Desmos and played around a little bit, and I was able to create a function that gets me somewhat close to what I want. $$d_1(t)=t-\frac1{2\pi}\sin 2\pi t$$ I chose the constant multiple $\frac1{2\pi}$ to simplify the distance between each station to $1$ , and the multiplier $2\pi$ for $t$ to suggest that it takes $1$ unit of time to get from one station to the next.  Choosing the sinusoidal function means that there will be no length of time where the train is moving at a constant speed, and the acceleration and deceleration will be exactly opposite, separated by the point of inflection halfway between the stations. The problem:  trains usually stop at every station.  I want to, arbitrarily at first, modify the function so that the train waits for the same amount of time as it takes to travel from one station to another.  Eventually, I want to be able to construct a function that can describe the waiting time in some proportion to the traveling time.  I came up with the function $$d_2(t)=\begin{cases}t-\frac{\lfloor t\rfloor}2-\frac1{2\pi}\sin 2\pi t,& \lfloor t\rfloor \textrm{ is even}\\ \frac{\lceil t\rceil}2,& \lfloor t\rfloor \textrm{ is odd} \end{cases}$$ This gets me something closer to what I want. My questions: What alternative ways of representing the scenario are there?  For example, would it be possible to construct a piecewise function based only on polynomials that meets the criteria above? Is there a way to represent this particular scenario without use of a piecewise function? Suppose there must be a stretch of time where the train is moving at a constant speed between stations.  How can I factor that in to the equation for the function? Suppose the train makes the return trip in a similar manner, and it makes some number of return trips daily.  Does this suggest that it can be written as a periodic function?  If so, what would be the equation for that function?","A random thought came into my head today when I was in the subway: Suppose we have a train in a subway where the stations are evenly spaced in a straight line.  The train accelerates for some amount of time, moves with a constant speed for some amount of time, and starts decelerating until it reaches the next station such that the same amount of time spent accelerating and decelerating are the same, and the magnitudes of both are the same.  Create a possible function that illustrates this scenario. I decided that I would try to use some combination of a sinusoidal function and a linear function, since the train would try to do the same thing during certain periods of time, and the train is moving only in one direction.  I hopped onto Desmos and played around a little bit, and I was able to create a function that gets me somewhat close to what I want. I chose the constant multiple to simplify the distance between each station to , and the multiplier for to suggest that it takes unit of time to get from one station to the next.  Choosing the sinusoidal function means that there will be no length of time where the train is moving at a constant speed, and the acceleration and deceleration will be exactly opposite, separated by the point of inflection halfway between the stations. The problem:  trains usually stop at every station.  I want to, arbitrarily at first, modify the function so that the train waits for the same amount of time as it takes to travel from one station to another.  Eventually, I want to be able to construct a function that can describe the waiting time in some proportion to the traveling time.  I came up with the function This gets me something closer to what I want. My questions: What alternative ways of representing the scenario are there?  For example, would it be possible to construct a piecewise function based only on polynomials that meets the criteria above? Is there a way to represent this particular scenario without use of a piecewise function? Suppose there must be a stretch of time where the train is moving at a constant speed between stations.  How can I factor that in to the equation for the function? Suppose the train makes the return trip in a similar manner, and it makes some number of return trips daily.  Does this suggest that it can be written as a periodic function?  If so, what would be the equation for that function?","d_1(t)=t-\frac1{2\pi}\sin 2\pi t \frac1{2\pi} 1 2\pi t 1 d_2(t)=\begin{cases}t-\frac{\lfloor t\rfloor}2-\frac1{2\pi}\sin 2\pi t,& \lfloor t\rfloor \textrm{ is even}\\ \frac{\lceil t\rceil}2,& \lfloor t\rfloor \textrm{ is odd}
\end{cases}","['calculus', 'soft-question', 'recreational-mathematics', 'periodic-functions']"
41,How to prove the closed form of the integral $\int \frac {dx}{\prod_{r=0}^n (x+r)}$,How to prove the closed form of the integral,\int \frac {dx}{\prod_{r=0}^n (x+r)},"I want to derive a closed formula for the integral $$I_n= \int \frac {dx}{\prod_{r=0}^n (x+r)}$$ On writing out first few terms we get For $n=0$, $$I_0=\ln \vert x\vert+C$$ For $n=1$, $$I_1=\ln \vert x\vert-\ln \vert x+1\vert+C$$ For $n=2$ $$I_2=\frac {1}{2!}\left(\sum_{r=0}^2 (-1)^r\binom {2}{r} \ln \vert x+r\vert\right)+C$$ For $n=3$ $$I_3=\frac {1}{3!}\left(\sum_{r=0}^3 (-1)^r\binom {3}{r} \ln \vert x+r\vert\right)+C$$ Hence for generalized $n$ we have $$I_n=\frac {1}{n!}\left(\sum_{r=0}^n (-1)^r\binom {n}{r} (\ln \vert x+r\vert)\right)+C$$ Now this is just an observation but I want to prove that it is correct. I have tried lot of methods but not useful. Partial fractions would have been most useful but would go out on tedious task which is nearly impossible.  Also integration by parts won't help nor any trig substitution. So any ideas are welcome.  And ya,  this is not a homework question,  it's a question which I just saw in a integral challenge paper.","I want to derive a closed formula for the integral $$I_n= \int \frac {dx}{\prod_{r=0}^n (x+r)}$$ On writing out first few terms we get For $n=0$, $$I_0=\ln \vert x\vert+C$$ For $n=1$, $$I_1=\ln \vert x\vert-\ln \vert x+1\vert+C$$ For $n=2$ $$I_2=\frac {1}{2!}\left(\sum_{r=0}^2 (-1)^r\binom {2}{r} \ln \vert x+r\vert\right)+C$$ For $n=3$ $$I_3=\frac {1}{3!}\left(\sum_{r=0}^3 (-1)^r\binom {3}{r} \ln \vert x+r\vert\right)+C$$ Hence for generalized $n$ we have $$I_n=\frac {1}{n!}\left(\sum_{r=0}^n (-1)^r\binom {n}{r} (\ln \vert x+r\vert)\right)+C$$ Now this is just an observation but I want to prove that it is correct. I have tried lot of methods but not useful. Partial fractions would have been most useful but would go out on tedious task which is nearly impossible.  Also integration by parts won't help nor any trig substitution. So any ideas are welcome.  And ya,  this is not a homework question,  it's a question which I just saw in a integral challenge paper.",,"['calculus', 'integration', 'indefinite-integrals']"
42,Vector Field vs. Gradient Field?,Vector Field vs. Gradient Field?,,"Suppose we have a gradient field $\vec{F}$. Is there thus a vector field $\vec{G}$ such that the curl of ($\vec{G}$) = $\vec{F}$? So, I'm trying to find examples/counter-examples. If we take a vector field and div(curl($\vec{F}$)) $\ne$ 0, then we know this isn't the case. This is my thinking. Can anyone guide me through?","Suppose we have a gradient field $\vec{F}$. Is there thus a vector field $\vec{G}$ such that the curl of ($\vec{G}$) = $\vec{F}$? So, I'm trying to find examples/counter-examples. If we take a vector field and div(curl($\vec{F}$)) $\ne$ 0, then we know this isn't the case. This is my thinking. Can anyone guide me through?",,"['calculus', 'multivariable-calculus', 'vector-spaces', 'vectors']"
43,Is there any closed form for this integral?,Is there any closed form for this integral?,,"The picture above is from uninstallation tool of fake antivirus in Korea. The ""official"" uninstallation tool will not proceed anymore unless user input the correct answer. (Nobody succeeded this) Due to its preposterousness, the image has been used as ""meme"" for malwares. $$\int_{0}^{1/3} \frac{e^{-x^2}}{\sqrt{1-x^2}} dx$$ Anyhow, is there any closed form for the result of this definite integral? How can one compute this without calculators like Wolframalpha?","The picture above is from uninstallation tool of fake antivirus in Korea. The ""official"" uninstallation tool will not proceed anymore unless user input the correct answer. (Nobody succeeded this) Due to its preposterousness, the image has been used as ""meme"" for malwares. $$\int_{0}^{1/3} \frac{e^{-x^2}}{\sqrt{1-x^2}} dx$$ Anyhow, is there any closed form for the result of this definite integral? How can one compute this without calculators like Wolframalpha?",,"['calculus', 'analysis', 'definite-integrals', 'closed-form']"
44,On the converse of the $n$th term test,On the converse of the th term test,n,"A student asked a very insightful question in my Calculus class this morning.  I did not know how to answer him.  (Admittedly, I am not an analyst by trade:  once I passed my qualifiers I never looked back.)  I would like to know if anyone here can give a precise answer, and if that can be massaged into an answer understandable by someone in Calculus II. The topic of the lecture was the $n$th term test (or ""divergence test"") for infinite series.  I presented it as: Theorem: If $\sum_n a_n$ converges then $\lim_{n \to \infty} a_n=0$. I proved this, then had them state the contrapositive: Divergence Test :  If $\lim_{n \to \infty} a_n \neq 0$ then $\sum_n a_n$ diverges. I then gave a litany of examples. To their credit, they never fell into the false-converse trap.  I have been harping on dogs/mammals/horses all semester so they are very good about avoiding that (if you are a dog then you are a mammal but the converse is false if you can find a horse). My second example was the harmonic series, which of course has $\lim_{n \to \infty} a_n=0$ yet fails to converge.  Hence we definitely have horses in my theorem above (and they all spotted this). Enter the sharp student.  He asked if there was an improvement on my first theorem so that the converse becomes true.  I had said earlier in my example that although $\lim_{n \to \infty} \frac{1}{n}=0$, it doesn't converge ""fast enough"" to $0$ to make the harmonic series converge.  The student asked for a measure of ""fast enough"" or at least a precise statement of this.  What he is fishing for is something like: Improved Theorem: If $\sum_n a_n$ converges then $\lim_{n \to \infty} a_n=0$ and (extra-nice condition on the speed of the convergence to $0$). My instinct is there is no answer in terms of the terms $a_n$.  The only answer is that the terms must vanish quickly enough to make the sequence of partial sums convergent (which is an unsatisfying answer to his question).  If he only cared about $p$-series then I can be precise ($p > 1$), but he is asking about generic series whose terms vanish in the limit. I hope my question is clear.  Am I correct that all this is much too subtle to have a nice, clean answer on the ""rate"" of convergence of the terms to $0$?","A student asked a very insightful question in my Calculus class this morning.  I did not know how to answer him.  (Admittedly, I am not an analyst by trade:  once I passed my qualifiers I never looked back.)  I would like to know if anyone here can give a precise answer, and if that can be massaged into an answer understandable by someone in Calculus II. The topic of the lecture was the $n$th term test (or ""divergence test"") for infinite series.  I presented it as: Theorem: If $\sum_n a_n$ converges then $\lim_{n \to \infty} a_n=0$. I proved this, then had them state the contrapositive: Divergence Test :  If $\lim_{n \to \infty} a_n \neq 0$ then $\sum_n a_n$ diverges. I then gave a litany of examples. To their credit, they never fell into the false-converse trap.  I have been harping on dogs/mammals/horses all semester so they are very good about avoiding that (if you are a dog then you are a mammal but the converse is false if you can find a horse). My second example was the harmonic series, which of course has $\lim_{n \to \infty} a_n=0$ yet fails to converge.  Hence we definitely have horses in my theorem above (and they all spotted this). Enter the sharp student.  He asked if there was an improvement on my first theorem so that the converse becomes true.  I had said earlier in my example that although $\lim_{n \to \infty} \frac{1}{n}=0$, it doesn't converge ""fast enough"" to $0$ to make the harmonic series converge.  The student asked for a measure of ""fast enough"" or at least a precise statement of this.  What he is fishing for is something like: Improved Theorem: If $\sum_n a_n$ converges then $\lim_{n \to \infty} a_n=0$ and (extra-nice condition on the speed of the convergence to $0$). My instinct is there is no answer in terms of the terms $a_n$.  The only answer is that the terms must vanish quickly enough to make the sequence of partial sums convergent (which is an unsatisfying answer to his question).  If he only cared about $p$-series then I can be precise ($p > 1$), but he is asking about generic series whose terms vanish in the limit. I hope my question is clear.  Am I correct that all this is much too subtle to have a nice, clean answer on the ""rate"" of convergence of the terms to $0$?",,"['calculus', 'sequences-and-series']"
45,"finding $ \int^{4}_{0}(x^2+1)d(\lfloor x \rfloor),$ given $\lfloor x \rfloor $ is a floor function of $x$",finding  given  is a floor function of," \int^{4}_{0}(x^2+1)d(\lfloor x \rfloor), \lfloor x \rfloor  x","finding $\displaystyle \int^{4}_{0}(x^2+1)d(\lfloor x \rfloor),$ given $\lfloor x \rfloor $ is a floor function of $x$ Assume  $\displaystyle I = (x^2+1)\lfloor x \rfloor \bigg|^{4}_{0}-2\int^{4}_{0}x\lfloor x \rfloor dx$ ( integration by parts ) i have a doubt about limit part , did not understand whether the limit corresponding to $x$ or corrosponding to $\lfloor x \rfloor$ because when we take $\displaystyle \int^{b}_{a}f(x)dx,$ then limits are corrosponding to $x$ but when we take  $\displaystyle \int^{b}_{a}f(x)d(\lfloor x \rfloor ),$ then limit corrosonding to $\lfloor x \rfloor$ please clearfy my doubt and also explain me whats wrong with my method above , thanks","finding $\displaystyle \int^{4}_{0}(x^2+1)d(\lfloor x \rfloor),$ given $\lfloor x \rfloor $ is a floor function of $x$ Assume  $\displaystyle I = (x^2+1)\lfloor x \rfloor \bigg|^{4}_{0}-2\int^{4}_{0}x\lfloor x \rfloor dx$ ( integration by parts ) i have a doubt about limit part , did not understand whether the limit corresponding to $x$ or corrosponding to $\lfloor x \rfloor$ because when we take $\displaystyle \int^{b}_{a}f(x)dx,$ then limits are corrosponding to $x$ but when we take  $\displaystyle \int^{b}_{a}f(x)d(\lfloor x \rfloor ),$ then limit corrosonding to $\lfloor x \rfloor$ please clearfy my doubt and also explain me whats wrong with my method above , thanks",,"['calculus', 'integration', 'definite-integrals']"
46,What is the geometrical meaning of the integral of a vector valued function?,What is the geometrical meaning of the integral of a vector valued function?,,"If $f:\mathbb{R}\rightarrow\mathbb{R}$ is an integrable function. then $\int_a^b f(x)dx$ can be considered as the area between the graph and the x-axis. But what if $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$? Let $\gamma:[0,1]\rightarrow \mathbb{R}^n$ be a smooth curve. what is the geometrical meaning of $\int_\gamma f\cdot dl$? (or in case $n=m$, $= \int_0^1 f(\gamma(t))\gamma '(t)\cdot dt$?) Thanks :) (for simplicity, you can assume $m,n$ are small numbers... i.e $f:\mathbb{R}^2\rightarrow \mathbb{R}^2$ )","If $f:\mathbb{R}\rightarrow\mathbb{R}$ is an integrable function. then $\int_a^b f(x)dx$ can be considered as the area between the graph and the x-axis. But what if $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$? Let $\gamma:[0,1]\rightarrow \mathbb{R}^n$ be a smooth curve. what is the geometrical meaning of $\int_\gamma f\cdot dl$? (or in case $n=m$, $= \int_0^1 f(\gamma(t))\gamma '(t)\cdot dt$?) Thanks :) (for simplicity, you can assume $m,n$ are small numbers... i.e $f:\mathbb{R}^2\rightarrow \mathbb{R}^2$ )",,"['calculus', 'integration', 'differential-geometry', 'vector-spaces', 'contour-integration']"
47,Evaluate $\int \frac {\sin(x)}{x^2 + 4x + 5}dx$,Evaluate,\int \frac {\sin(x)}{x^2 + 4x + 5}dx,"Question: Evaluate   $$ \int \frac{\sin(x)}{x^2 + 4x + 5} dx=\int \frac {\sin(x)}{(x + 2)^2 + 1}dx $$ By using the change of variable $y = x + 2$ we have that $dy = dx$ then $$I = \int \frac{\sin(y - 2)}{y^2 + 1} dy$$ $f = \sin(y - 2)$, $f' = \cos(y - 2)$ $g' = \frac {1} {y^2 + 1}$, $g = \arctan(y)$ $I = \sin(y - 2) \cdot \arctan(y) + \int \cos(y - 2)  \arctan(y) dy$ $I_1 = \int \cos(y - 2) \cdot \arctan(y) dy$ How can I solve?","Question: Evaluate   $$ \int \frac{\sin(x)}{x^2 + 4x + 5} dx=\int \frac {\sin(x)}{(x + 2)^2 + 1}dx $$ By using the change of variable $y = x + 2$ we have that $dy = dx$ then $$I = \int \frac{\sin(y - 2)}{y^2 + 1} dy$$ $f = \sin(y - 2)$, $f' = \cos(y - 2)$ $g' = \frac {1} {y^2 + 1}$, $g = \arctan(y)$ $I = \sin(y - 2) \cdot \arctan(y) + \int \cos(y - 2)  \arctan(y) dy$ $I_1 = \int \cos(y - 2) \cdot \arctan(y) dy$ How can I solve?",,"['calculus', 'integration', 'complex-analysis']"
48,Modular forms and the Roger-Ramanujan identities: How??,Modular forms and the Roger-Ramanujan identities: How??,,"I've been studying Bressoud's paper ""An easy proof of the Rogers-Ramanujan Identities"" where he proves the R.R. identity: $$\sum_{n \geq0}\frac{q^{n^2}}{(1-q) \cdots(1-q^n)}= \prod_{n \geq0}\frac{1}{(1-q^{5n+1})(1-q^{5n+4})}$$ Apparently the right hand side becomes modular when corrected by the term $q^{{-1}/{60}}$. This seems rather arbitrary to me so I'm seeking an explanation where this may come from. Is there something more behind it? A greater context for instance?","I've been studying Bressoud's paper ""An easy proof of the Rogers-Ramanujan Identities"" where he proves the R.R. identity: $$\sum_{n \geq0}\frac{q^{n^2}}{(1-q) \cdots(1-q^n)}= \prod_{n \geq0}\frac{1}{(1-q^{5n+1})(1-q^{5n+4})}$$ Apparently the right hand side becomes modular when corrected by the term $q^{{-1}/{60}}$. This seems rather arbitrary to me so I'm seeking an explanation where this may come from. Is there something more behind it? A greater context for instance?",,"['calculus', 'complex-analysis', 'number-theory']"
49,Looking for function $f$ such that $f'<0$ and $(xf)'>0$,Looking for function  such that  and,f f'<0 (xf)'>0,"I'm looking for a function $f(x)$ with the following properties for $x\ge 0$: $$0\le f(x)\le 1$$ $$f(0)=1$$ $$f'(x)\le 0$$ $$f(x)+xf'(x)\ge 0$$ $$\lim_{x\to\infty} xf(x)=L$$ where $L$ is a positive constant. Essentially I want $xf(x)$ to initially approximate $x$ and then level off at $L$. Candidates include: $$f(x)={1\over 1+x/L}$$ $$f(x)={\tanh (x/L) \over x/L}$$ but neither of these have an extra parameter that allows me to control how quickly $xf(x)$ approaches $L$ while keeping the initial slope of $xf(x)$ equal to $1$. This is what I would like, ideally. The motivation for this: $f(x)$ can be thought of as an efficiency with $x$ the input and $xf(x)$ the output. The system is perfectly efficient at zero input and decreases in efficiency with increasing input, but you never get less output from more input. Thanks for your responses.","I'm looking for a function $f(x)$ with the following properties for $x\ge 0$: $$0\le f(x)\le 1$$ $$f(0)=1$$ $$f'(x)\le 0$$ $$f(x)+xf'(x)\ge 0$$ $$\lim_{x\to\infty} xf(x)=L$$ where $L$ is a positive constant. Essentially I want $xf(x)$ to initially approximate $x$ and then level off at $L$. Candidates include: $$f(x)={1\over 1+x/L}$$ $$f(x)={\tanh (x/L) \over x/L}$$ but neither of these have an extra parameter that allows me to control how quickly $xf(x)$ approaches $L$ while keeping the initial slope of $xf(x)$ equal to $1$. This is what I would like, ideally. The motivation for this: $f(x)$ can be thought of as an efficiency with $x$ the input and $xf(x)$ the output. The system is perfectly efficient at zero input and decreases in efficiency with increasing input, but you never get less output from more input. Thanks for your responses.",,"['calculus', 'functions']"
50,Finding the minimum of $x^2+y^2$ for $(x^2y-xy^2)(x^3-y^3)=x^3+y^3$ and $xy>0$.,Finding the minimum of  for  and .,x^2+y^2 (x^2y-xy^2)(x^3-y^3)=x^3+y^3 xy>0,"If $x,y \in \mathbb {R}$, find the minimum of  $x^2+y^2$ when $(x^2y-xy^2)(x^3-y^3)=x^3+y^3$ and $xy>0$. This problem was inspired by a problem which asked if $x,y \in \mathbb {R}$ and $xy \neq 0$,  find the minimum of $x^2+y^2$ when $xy(x^2-y^2)=x^2+y^2$. By setting $x=a\sin\theta$, $y=a\cos\theta$, the equation can be simplified to $a^2=\frac{1}{\sin\theta\cos\theta(\sin\theta^2-\cos\theta^2)}$ However, notice that $\sin\theta\cos\theta=\frac{\sin2\theta}{2}$, $\sin\theta^2-\cos\theta^2=-\cos2\theta$. This implies that $a^2=\frac{2}{-\sin2\theta\cos2\theta}$, thus that $a^2=\frac{4}{-\sin4\theta}\ge 4$. Thus the minimum of $x^2+y^2$ is $4$, with the equality holding when $x=\sqrt{2-\sqrt{2}}$, $y=\sqrt{2+\sqrt{2}}$. However, since there are no formulas I know of where $\sin^3 x+\cos^3 x$, I did not know how to find the minimum of $x^2+y^2$ when $(x^2y-xy^2)(x^3-y^3)=x^3+y^3$. Graphing it seems to imply that such a minimum exists, but I am not aware of how to find it( and when such a minimum exists). Any help would be appreciated.","If $x,y \in \mathbb {R}$, find the minimum of  $x^2+y^2$ when $(x^2y-xy^2)(x^3-y^3)=x^3+y^3$ and $xy>0$. This problem was inspired by a problem which asked if $x,y \in \mathbb {R}$ and $xy \neq 0$,  find the minimum of $x^2+y^2$ when $xy(x^2-y^2)=x^2+y^2$. By setting $x=a\sin\theta$, $y=a\cos\theta$, the equation can be simplified to $a^2=\frac{1}{\sin\theta\cos\theta(\sin\theta^2-\cos\theta^2)}$ However, notice that $\sin\theta\cos\theta=\frac{\sin2\theta}{2}$, $\sin\theta^2-\cos\theta^2=-\cos2\theta$. This implies that $a^2=\frac{2}{-\sin2\theta\cos2\theta}$, thus that $a^2=\frac{4}{-\sin4\theta}\ge 4$. Thus the minimum of $x^2+y^2$ is $4$, with the equality holding when $x=\sqrt{2-\sqrt{2}}$, $y=\sqrt{2+\sqrt{2}}$. However, since there are no formulas I know of where $\sin^3 x+\cos^3 x$, I did not know how to find the minimum of $x^2+y^2$ when $(x^2y-xy^2)(x^3-y^3)=x^3+y^3$. Graphing it seems to imply that such a minimum exists, but I am not aware of how to find it( and when such a minimum exists). Any help would be appreciated.",,"['calculus', 'inequality', 'optimization']"
51,Infinite Product Representation of $\sin x$,Infinite Product Representation of,\sin x,"I've recently taken interest in infinite products, and I'm having trouble with a proof I found in this PDF file: ""Infinite Products and Elementary Functions"" : An intermediate step in finding an infinite product to represent $\sin(x)$ is given as follows: $$\sin x=x\lim_{n\to \infty}\sum_{k=0}^{(n-1)/2}(-1)^k\binom{n}{2k+1}\frac{x^{2k}}{n^{2k+1}} \tag{2.8} $$ Of all the stages   in Euler’s procedure, which, as a whole, represents a real work of   art, the next stage is perhaps the most critical and decisive.   Factoring the polynomial in (2.8) into the trigonometric form: $$\sin x=x\lim_{n \to \infty}\prod_{k=1}^{(n-1)/2}\left(1-\dfrac{(1+\cos(2k\pi/n))x^2}{(1-\cos(2k\pi/n))n^2}\right)$$ I've tried a proof by induction, but could only manage to check a couple of iterations, that hinted to a pattern that looks like this: $$\prod_{k=1}^{\frac{n-1}{2}} (1-a_{k,n}\frac{x^2}{n^2})$$  but this is a dead end since I couldn't find a trigonometric expression of $a_{k,n}$. I can't figure out where the trigonometric functions came from in the right hand side of the second equation. My last attempt was to turn the product into a sum using the natural logarithm, but again this complicated things further. The author seems to hint at an elementary factorization, but I'm starting to have some doubts. Is there really an elementary approach?  Is there some identity I'm missing? Any answer, or (""Socratic"") hint is appreciated.","I've recently taken interest in infinite products, and I'm having trouble with a proof I found in this PDF file: ""Infinite Products and Elementary Functions"" : An intermediate step in finding an infinite product to represent $\sin(x)$ is given as follows: $$\sin x=x\lim_{n\to \infty}\sum_{k=0}^{(n-1)/2}(-1)^k\binom{n}{2k+1}\frac{x^{2k}}{n^{2k+1}} \tag{2.8} $$ Of all the stages   in Euler’s procedure, which, as a whole, represents a real work of   art, the next stage is perhaps the most critical and decisive.   Factoring the polynomial in (2.8) into the trigonometric form: $$\sin x=x\lim_{n \to \infty}\prod_{k=1}^{(n-1)/2}\left(1-\dfrac{(1+\cos(2k\pi/n))x^2}{(1-\cos(2k\pi/n))n^2}\right)$$ I've tried a proof by induction, but could only manage to check a couple of iterations, that hinted to a pattern that looks like this: $$\prod_{k=1}^{\frac{n-1}{2}} (1-a_{k,n}\frac{x^2}{n^2})$$  but this is a dead end since I couldn't find a trigonometric expression of $a_{k,n}$. I can't figure out where the trigonometric functions came from in the right hand side of the second equation. My last attempt was to turn the product into a sum using the natural logarithm, but again this complicated things further. The author seems to hint at an elementary factorization, but I'm starting to have some doubts. Is there really an elementary approach?  Is there some identity I'm missing? Any answer, or (""Socratic"") hint is appreciated.",,"['calculus', 'sequences-and-series', 'taylor-expansion', 'infinite-product']"
52,Identities For Generalized Harmonic Number,Identities For Generalized Harmonic Number,,"I have been searching for identities involving generalized harmonic numbers \begin{equation*}H_n^{(p)}=\sum_{k=1}^{n}\frac{1}{k^p}\end{equation*} I found several identities in terms of $H_n^{(1)}$, but I am looking for some interesting identities for $H_n^{(2)}$. Does anyone know of any identities know of any nontrivial identities for $H_n^{(2)}$?  I found some listed on Wikipedia, but this list is not comprehensive. Thanks for your help. integral identities summation identities recursive identities in terms of another function","I have been searching for identities involving generalized harmonic numbers \begin{equation*}H_n^{(p)}=\sum_{k=1}^{n}\frac{1}{k^p}\end{equation*} I found several identities in terms of $H_n^{(1)}$, but I am looking for some interesting identities for $H_n^{(2)}$. Does anyone know of any identities know of any nontrivial identities for $H_n^{(2)}$?  I found some listed on Wikipedia, but this list is not comprehensive. Thanks for your help. integral identities summation identities recursive identities in terms of another function",,"['calculus', 'sequences-and-series', 'discrete-mathematics', 'harmonic-numbers']"
53,limits and continuity: irrational and rational piecewise function,limits and continuity: irrational and rational piecewise function,,"I have noticed similar topics, but people seem to solving them with sequences which I have not learned yet. I need to prove that the function: $$f(x)=\begin{cases} x, & \text{ if $x$ is an irrational number }\\0 & \text{ if $x$ is a rational number }\end{cases}$$ is discontinuous at every irrational number using both the precise definition of a limit and the fact that every nonempty open interval of real numbers contains both irrational and rational numbers. While I generally understand the $\epsilon-\delta$ definition, I'm having trouble applying it to this question and finding the appropriate epsilon to use.","I have noticed similar topics, but people seem to solving them with sequences which I have not learned yet. I need to prove that the function: $$f(x)=\begin{cases} x, & \text{ if $x$ is an irrational number }\\0 & \text{ if $x$ is a rational number }\end{cases}$$ is discontinuous at every irrational number using both the precise definition of a limit and the fact that every nonempty open interval of real numbers contains both irrational and rational numbers. While I generally understand the $\epsilon-\delta$ definition, I'm having trouble applying it to this question and finding the appropriate epsilon to use.",,"['calculus', 'limits', 'epsilon-delta']"
54,Challenging recurrence relation problem,Challenging recurrence relation problem,,"I am starting out with the following: $$ \frac{d^n}{dx^n}[g(x)^{f(x)}] = \sum_{c=0}^n g(x)^{f(x)-c}\lambda_{n,c}(x) $$ Therefore: $$ \frac{d^{n+1}}{dx^{n+1}}[g(x)^{f(x)}] = \sum_{c=0}^{n+1}g(x)^{f(x)-c}\lambda_{n+1,c}(x) = \frac{d}{dx}\sum_{c=0}^n g(x)^{f(x)-c}\lambda_{n,c}(x) $$ $\lambda_{n,c}(x)$ is defined like so: $$ \lambda_{n,c}(x) = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} \frac{d^j}{df^j}[f(x)_c] B_{n,k}^{(f \diamond g)^c}(x) $$ My goal is to find a recurrence relation for $B_{n,k}^{(f \diamond g)^c}(x)$ by setting the two expressions equal to eachother. This is my work so far: $$ \frac{d}{dx}[g(x)^{f(x)-c} \lambda_{n,c}(x)] = \left((f(x)-c)\frac{g'(x)}{g(x)} + \ln(g(x)) f'(x)\right)g(x)^{f(x)-c} \lambda_{n,c}(x) + g(x)^{f(x)-c} \frac{d}{dx}[\lambda_{n,c}(x)] $$ Note from now on i will denote $\frac{d^j}{df^j}[f(x)_c] = f_c^{(j)}$ $$ \frac{d}{dx}[\lambda_{n,c}(x)] = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \left(\frac{g'(x)}{g(x)}(k-c-j) \ln(g(x))^{k-c-j-1} f_c^{(j)} B_{n,k}^{(f \diamond g)^c} + \ln(g(x))^{k-c-j} \frac{d}{dx}[f_c^{(j}B_{n,k}^{(f \diamond g)^c}]\right) $$ Now, for me to find an expression that will result in a recurrence relation i am going to attempt to group all the $\ln(g(x))^{k-c-j}$ together and set all these terms equal to: $$ \sum_{c=0}^{n+1} g(x)^{f(x)-c} \lambda_{n+1,c}(x) $$ By doing this i will have found a way to isolate the $g(x)^{f(x)-c}$ terms as well as the $\ln(g(x))^{k-c-j}$ terms. To do this i will seperate each individual term and attempt to manipulate it in order to fit these conditions: $$ A = f'(x) \ln(g(x)) \lambda_{n,c}(x) = f'(x) \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j+1} f_c^{(j)} B_{n,k}^{(f \diamond g)^c}(x) = f'(x) \sum_{k=c+1}^{n+1} \sum_{j=0}^{k-c-1} {k-c-1 \choose j} \ln(g(x))^{k-c-j} f_c^{(j)} B_{n,k-1}^{(f \diamond g)^c}(x) $$ Now,for $B$ i will shift over a step backwards so that instead of $c$ we will be dealing with $c-1$, this is because of the differentiation of the natural log which in turn results in $\frac{g'(x)}{g(x)}$. When we multiply $\frac{g'(x)}{g(x)}$ with $g(x)^{f(x)-c}$ we will get $g'(x) g(x)^{f(x)-c-1}$ therefore by evaluating the expression at $c-1$ we will be evaluating the part of the summation that is dealing with $g(x)^{f(x)-c}$ instead of dealing with the summation that deals with $g(x)^{f(x)-c-1}$. If there is any questions about this please do not hesitate to ask in the comments. $$ B = g'(x) (f(x)-c+1) \lambda_{n,c-1}(x) = g'(x) (f(x)-c+1) \sum_{k=c-1}^n \sum_{j=0}^{k-c+1} {k-c+1 \choose j} \ln(g(x))^{k-c-j+1} f_{c-1}^j B_{n,k}^{(f \diamond g)^{c-1}}(x) = g'(x) (f(x)-c+1) \sum_{k=c}^{n+1} \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} f_{c-1}^j B_{n,k-1}^{(f \diamond g)^{c-1}}(x) $$ Now, for $C$ and $D$ i will split up the two parts in the part where i differentiated the $\lambda_{n,c}(x)$, in variable $C$ we will be using the same logic as i used for ""shifting"" the $c$ variable to $c-1$. $$ C = \lambda_{n,c}'(x)_{part \space 1} = g'(x)\sum_{k=c-1}^{n} \sum_{j=0}^{k-c+1} {k-c+1 \choose j} (k-c-j+1) \ln(g(x))^{k-c-j} f_{c-1}^{(j)} B_{n,k}^{(f \diamond g)^{c-1}}(x) = g'(x)\sum_{k=c}^{n+1} \sum_{j=0}^{k-c} {k-c \choose j} (k-c-j) \ln(g(x))^{k-c-j} f_{c-1}^{(j)} B_{n,k}^{(f \diamond g)^{c-1}}(x) $$ Now for $C$ i did a little bit of trickery, first of all, when $k=c-1$ the term is equal to zero due to the $(k-c+1)$ term and when $j = (k-c+1)$ the term is equal to zero due to the $(k-c-j+1)$ term. $$ D = \lambda_{n,c}'(x)_{part \space 2} = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} \frac{d}{dx}[f_c^{(j)} B_{n,k}^{(f \diamond g)^c}(x)] $$ Now the problem arises when i try to add $A+B+C+D$ and set it equal to $\lambda_{n+1,c}(x)$. i have attempted to do this many times but i have hit some points where it becomes troubling or that the identity does now work at all. If someone can please help me with the issue it would be alot of help to me. Thank you all for reading this if you have gotten this far, i appreciate it a lot.","I am starting out with the following: $$ \frac{d^n}{dx^n}[g(x)^{f(x)}] = \sum_{c=0}^n g(x)^{f(x)-c}\lambda_{n,c}(x) $$ Therefore: $$ \frac{d^{n+1}}{dx^{n+1}}[g(x)^{f(x)}] = \sum_{c=0}^{n+1}g(x)^{f(x)-c}\lambda_{n+1,c}(x) = \frac{d}{dx}\sum_{c=0}^n g(x)^{f(x)-c}\lambda_{n,c}(x) $$ $\lambda_{n,c}(x)$ is defined like so: $$ \lambda_{n,c}(x) = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} \frac{d^j}{df^j}[f(x)_c] B_{n,k}^{(f \diamond g)^c}(x) $$ My goal is to find a recurrence relation for $B_{n,k}^{(f \diamond g)^c}(x)$ by setting the two expressions equal to eachother. This is my work so far: $$ \frac{d}{dx}[g(x)^{f(x)-c} \lambda_{n,c}(x)] = \left((f(x)-c)\frac{g'(x)}{g(x)} + \ln(g(x)) f'(x)\right)g(x)^{f(x)-c} \lambda_{n,c}(x) + g(x)^{f(x)-c} \frac{d}{dx}[\lambda_{n,c}(x)] $$ Note from now on i will denote $\frac{d^j}{df^j}[f(x)_c] = f_c^{(j)}$ $$ \frac{d}{dx}[\lambda_{n,c}(x)] = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \left(\frac{g'(x)}{g(x)}(k-c-j) \ln(g(x))^{k-c-j-1} f_c^{(j)} B_{n,k}^{(f \diamond g)^c} + \ln(g(x))^{k-c-j} \frac{d}{dx}[f_c^{(j}B_{n,k}^{(f \diamond g)^c}]\right) $$ Now, for me to find an expression that will result in a recurrence relation i am going to attempt to group all the $\ln(g(x))^{k-c-j}$ together and set all these terms equal to: $$ \sum_{c=0}^{n+1} g(x)^{f(x)-c} \lambda_{n+1,c}(x) $$ By doing this i will have found a way to isolate the $g(x)^{f(x)-c}$ terms as well as the $\ln(g(x))^{k-c-j}$ terms. To do this i will seperate each individual term and attempt to manipulate it in order to fit these conditions: $$ A = f'(x) \ln(g(x)) \lambda_{n,c}(x) = f'(x) \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j+1} f_c^{(j)} B_{n,k}^{(f \diamond g)^c}(x) = f'(x) \sum_{k=c+1}^{n+1} \sum_{j=0}^{k-c-1} {k-c-1 \choose j} \ln(g(x))^{k-c-j} f_c^{(j)} B_{n,k-1}^{(f \diamond g)^c}(x) $$ Now,for $B$ i will shift over a step backwards so that instead of $c$ we will be dealing with $c-1$, this is because of the differentiation of the natural log which in turn results in $\frac{g'(x)}{g(x)}$. When we multiply $\frac{g'(x)}{g(x)}$ with $g(x)^{f(x)-c}$ we will get $g'(x) g(x)^{f(x)-c-1}$ therefore by evaluating the expression at $c-1$ we will be evaluating the part of the summation that is dealing with $g(x)^{f(x)-c}$ instead of dealing with the summation that deals with $g(x)^{f(x)-c-1}$. If there is any questions about this please do not hesitate to ask in the comments. $$ B = g'(x) (f(x)-c+1) \lambda_{n,c-1}(x) = g'(x) (f(x)-c+1) \sum_{k=c-1}^n \sum_{j=0}^{k-c+1} {k-c+1 \choose j} \ln(g(x))^{k-c-j+1} f_{c-1}^j B_{n,k}^{(f \diamond g)^{c-1}}(x) = g'(x) (f(x)-c+1) \sum_{k=c}^{n+1} \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} f_{c-1}^j B_{n,k-1}^{(f \diamond g)^{c-1}}(x) $$ Now, for $C$ and $D$ i will split up the two parts in the part where i differentiated the $\lambda_{n,c}(x)$, in variable $C$ we will be using the same logic as i used for ""shifting"" the $c$ variable to $c-1$. $$ C = \lambda_{n,c}'(x)_{part \space 1} = g'(x)\sum_{k=c-1}^{n} \sum_{j=0}^{k-c+1} {k-c+1 \choose j} (k-c-j+1) \ln(g(x))^{k-c-j} f_{c-1}^{(j)} B_{n,k}^{(f \diamond g)^{c-1}}(x) = g'(x)\sum_{k=c}^{n+1} \sum_{j=0}^{k-c} {k-c \choose j} (k-c-j) \ln(g(x))^{k-c-j} f_{c-1}^{(j)} B_{n,k}^{(f \diamond g)^{c-1}}(x) $$ Now for $C$ i did a little bit of trickery, first of all, when $k=c-1$ the term is equal to zero due to the $(k-c+1)$ term and when $j = (k-c+1)$ the term is equal to zero due to the $(k-c-j+1)$ term. $$ D = \lambda_{n,c}'(x)_{part \space 2} = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} \frac{d}{dx}[f_c^{(j)} B_{n,k}^{(f \diamond g)^c}(x)] $$ Now the problem arises when i try to add $A+B+C+D$ and set it equal to $\lambda_{n+1,c}(x)$. i have attempted to do this many times but i have hit some points where it becomes troubling or that the identity does now work at all. If someone can please help me with the issue it would be alot of help to me. Thank you all for reading this if you have gotten this far, i appreciate it a lot.",,"['calculus', 'combinatorics', 'derivatives', 'recurrence-relations', 'generating-functions']"
55,Integrating reciprocals of functions with known antiderivatives,Integrating reciprocals of functions with known antiderivatives,,"If $$\int_{}^{} f(x)\,dx$$ is known, is there a way to directly find $$\int_{}^{} \frac{1}{f(x)}\,dx$$","If $$\int_{}^{} f(x)\,dx$$ is known, is there a way to directly find $$\int_{}^{} \frac{1}{f(x)}\,dx$$",,['calculus']
56,Integral identity related with cubic analogue of arithmetic-geometric mean,Integral identity related with cubic analogue of arithmetic-geometric mean,,"Let $a,b$ be positive real numbers and we define two sequences $\{a_{n}\},\{b_{n}\}$ as follows: $$a_{0}=a,b_{0}=b,a_{n+1}=\frac{a_{n}+2b_{n}}{3},b_{n+1}=\sqrt[3]{\frac{(a_{n}^{2}+a_{n}b_{n}+b_{n}^{2})b_{n}}{3}}\tag{1}$$ Then it is not so difficult to show that both the sequences $\{a_{n}\},\{b_{n}\}$ tend to a common limit which may be denoted by $M_{3}(a,b)$, the cubic arithmetic geometric mean of $a$ and $b$. It also turns out that there is an integral representation of this cubic arithmetic geometric mean. Let us define $$I(a,b)=\int_{0}^{\infty}\frac{t\,dt}{\sqrt[3]{(t^{3}+a^{3})(t^{3}+b^{3})^{2}}}\tag{2}$$ Then we have the relation $$M_{3}(a, b) = \frac{I(1, 1)}{I(a, b)}\tag{3}$$ The above result $(3)$ can be proved easily if we can show that $$I(a_{n},b_{n})=I(a_{n+1},b_{n+1})\tag{4}$$ Given the definition $(2)$ of integral $I(a,b)$ how do I go about proving the identity $(4)$?","Let $a,b$ be positive real numbers and we define two sequences $\{a_{n}\},\{b_{n}\}$ as follows: $$a_{0}=a,b_{0}=b,a_{n+1}=\frac{a_{n}+2b_{n}}{3},b_{n+1}=\sqrt[3]{\frac{(a_{n}^{2}+a_{n}b_{n}+b_{n}^{2})b_{n}}{3}}\tag{1}$$ Then it is not so difficult to show that both the sequences $\{a_{n}\},\{b_{n}\}$ tend to a common limit which may be denoted by $M_{3}(a,b)$, the cubic arithmetic geometric mean of $a$ and $b$. It also turns out that there is an integral representation of this cubic arithmetic geometric mean. Let us define $$I(a,b)=\int_{0}^{\infty}\frac{t\,dt}{\sqrt[3]{(t^{3}+a^{3})(t^{3}+b^{3})^{2}}}\tag{2}$$ Then we have the relation $$M_{3}(a, b) = \frac{I(1, 1)}{I(a, b)}\tag{3}$$ The above result $(3)$ can be proved easily if we can show that $$I(a_{n},b_{n})=I(a_{n+1},b_{n+1})\tag{4}$$ Given the definition $(2)$ of integral $I(a,b)$ how do I go about proving the identity $(4)$?",,"['calculus', 'integration', 'definite-integrals']"
57,Condition for increase in the optimum of a general function,Condition for increase in the optimum of a general function,,"For a function $f(x,y)$ with the following properties: $f(x,y)$ is strictly increasing as a function of $x$ $f(x,y)$ is strictly decreasing as a function of $y$ $\lim_{x\to\infty}\frac{\partial f(x,y)}{\partial x}=0$ $\lim_{y\to\infty}\frac{\partial f(x,y)}{\partial y}=0$ I'm studying the optimum of $\frac{f(x,y)}{x}$ with respect to $x$, denoted $x_0$: $$ Q=\frac{\partial (f(x,y)/x)}{\partial x}\bigg|_{x=x_0}=0. $$ Assuming $x_0$ is an optimum, I wish to study how $x_0$ changes as $y$ changes, and I arrive at the following $$ \frac{dx_0}{dy}=-\frac{\partial Q/ \partial y}{\partial Q/ \partial x_0}. $$ Given that $x_0$ is an optimum, the denominator of the above will be negative and hence the sign of $\frac{dx_0}{dy}$ will be the same as the sign of $\frac{\partial Q}{\partial y}$. And we have the following $$ sgn\bigg(\frac{\partial Q}{\partial y}\bigg)=sgn\bigg(x_0\frac{\partial (\partial f(x_0,y)/\partial x_0)}{\partial y}-\frac{\partial f(x_0,y)}{\partial y}\bigg). $$ Can we say that the following will always be true? $$ \frac{\partial Q}{\partial y}\neq 0 $$ In other words, if $y$ changes, the optimum $x_0$ must also change. If not, can we say something about the cases where this will not be true? Thanks a lot for your help.","For a function $f(x,y)$ with the following properties: $f(x,y)$ is strictly increasing as a function of $x$ $f(x,y)$ is strictly decreasing as a function of $y$ $\lim_{x\to\infty}\frac{\partial f(x,y)}{\partial x}=0$ $\lim_{y\to\infty}\frac{\partial f(x,y)}{\partial y}=0$ I'm studying the optimum of $\frac{f(x,y)}{x}$ with respect to $x$, denoted $x_0$: $$ Q=\frac{\partial (f(x,y)/x)}{\partial x}\bigg|_{x=x_0}=0. $$ Assuming $x_0$ is an optimum, I wish to study how $x_0$ changes as $y$ changes, and I arrive at the following $$ \frac{dx_0}{dy}=-\frac{\partial Q/ \partial y}{\partial Q/ \partial x_0}. $$ Given that $x_0$ is an optimum, the denominator of the above will be negative and hence the sign of $\frac{dx_0}{dy}$ will be the same as the sign of $\frac{\partial Q}{\partial y}$. And we have the following $$ sgn\bigg(\frac{\partial Q}{\partial y}\bigg)=sgn\bigg(x_0\frac{\partial (\partial f(x_0,y)/\partial x_0)}{\partial y}-\frac{\partial f(x_0,y)}{\partial y}\bigg). $$ Can we say that the following will always be true? $$ \frac{\partial Q}{\partial y}\neq 0 $$ In other words, if $y$ changes, the optimum $x_0$ must also change. If not, can we say something about the cases where this will not be true? Thanks a lot for your help.",,"['calculus', 'derivatives', 'optimization']"
58,How to find the minimum $m$ for a given $n$ in this inequality?,How to find the minimum  for a given  in this inequality?,m n,"For a given $n \in \Bbb N$, how do you find the minimum $m \in \Bbb N$ which satisfies the inequality below? $$3^{3^{3^{3^{\unicode{x22F0}^{3}}}}} (m \text{ times}) > 9^{9^{9^{9^{\unicode{x22F0}^{9}}}}} (n \text{ times})$$ What I have tried to do so far is decomposing the $9$ on the right side to $3*3$ or to $3^2$, but both ways didn't get me much far and I couldn't find a pattern.","For a given $n \in \Bbb N$, how do you find the minimum $m \in \Bbb N$ which satisfies the inequality below? $$3^{3^{3^{3^{\unicode{x22F0}^{3}}}}} (m \text{ times}) > 9^{9^{9^{9^{\unicode{x22F0}^{9}}}}} (n \text{ times})$$ What I have tried to do so far is decomposing the $9$ on the right side to $3*3$ or to $3^2$, but both ways didn't get me much far and I couldn't find a pattern.",,"['calculus', 'inequality']"
59,Divergence/convergence of a series,Divergence/convergence of a series,,how can I prove this series diverges/converges? I used all adequate criteria but nothing useful came out..any ideas ? $$\sum\limits_{n=1}^\infty{\frac{{\sin (n)\cos (n^2 )}}{\sqrt n +\sqrt[3]n}}$$,how can I prove this series diverges/converges? I used all adequate criteria but nothing useful came out..any ideas ? $$\sum\limits_{n=1}^\infty{\frac{{\sin (n)\cos (n^2 )}}{\sqrt n +\sqrt[3]n}}$$,,"['calculus', 'divergent-series']"
60,Intuition behind a certain limit.,Intuition behind a certain limit.,,"We want to find $\displaystyle\lim_{\theta\to\frac{\pi}{2}} b_1-a_1$, we are given $c=1$ and that $\cdot=90^{\circ}$ This is my solution; $$\begin{equation}\sin \theta=\frac{b_1}{a_1} \iff b_1=a_1 \sin \theta\tag{1}\end{equation}$$ $$\begin{equation}\tan \theta=\frac{b_1}{c}\iff \tan \theta=b_1\tag{2}\end{equation}$$ $$\begin{equation}b_1=\tan \theta\stackrel{(1)}{\iff}a_1= \sec \theta\tag{3}\end{equation}$$ $$\lim_{\theta \to \frac{\pi}{2}} b_1-a_1\stackrel{(2)(3)}{=}\lim_{\theta\to\frac{\pi}{2}}\tan \theta-\sec \theta=-\lim_{\theta\to\frac{\pi}{2}}\frac{\cos \theta \tan \theta -\cos^2\theta\tan^2\theta}{\cos^2\theta\tan \theta}=-\lim_{\theta\to\frac{\pi}{2}}\frac{1-\cos \theta \tan \theta }{\cos \theta}\stackrel{\text{de l'}}{=}-\dfrac{1}{-\infty}=0$$ Assuming I did not do any mistake in there, that means that when the angle tends to become a right angle, $a_1=b_1$. But according to my intuition, $a_1>b_1$ no matter the angle. Where is my mistake?","We want to find $\displaystyle\lim_{\theta\to\frac{\pi}{2}} b_1-a_1$, we are given $c=1$ and that $\cdot=90^{\circ}$ This is my solution; $$\begin{equation}\sin \theta=\frac{b_1}{a_1} \iff b_1=a_1 \sin \theta\tag{1}\end{equation}$$ $$\begin{equation}\tan \theta=\frac{b_1}{c}\iff \tan \theta=b_1\tag{2}\end{equation}$$ $$\begin{equation}b_1=\tan \theta\stackrel{(1)}{\iff}a_1= \sec \theta\tag{3}\end{equation}$$ $$\lim_{\theta \to \frac{\pi}{2}} b_1-a_1\stackrel{(2)(3)}{=}\lim_{\theta\to\frac{\pi}{2}}\tan \theta-\sec \theta=-\lim_{\theta\to\frac{\pi}{2}}\frac{\cos \theta \tan \theta -\cos^2\theta\tan^2\theta}{\cos^2\theta\tan \theta}=-\lim_{\theta\to\frac{\pi}{2}}\frac{1-\cos \theta \tan \theta }{\cos \theta}\stackrel{\text{de l'}}{=}-\dfrac{1}{-\infty}=0$$ Assuming I did not do any mistake in there, that means that when the angle tends to become a right angle, $a_1=b_1$. But according to my intuition, $a_1>b_1$ no matter the angle. Where is my mistake?",,"['calculus', 'geometry', 'limits', 'trigonometry', 'intuition']"
61,Solve quadric equation system,Solve quadric equation system,,"How to solve this analytically(not a numerical solution)? For given real and symmetric matrices $A_1,A_2,A_3,A_4\in\mathbb{R}^{4\times4}$ find $0\neq x\in\mathbb{R}^4$ $$x^TA_1x=0$$ $$x^TA_2x=0$$ $$x^TA_3x=0$$ $$x^TA_4x=0$$ Example: Solve the system: \begin{align} a^2+b^2+c &=3.95 \\ ab+bc+c^2 &=4.57 \\ ac+b &=2.63 \\  \end{align} Denoting: \begin{equation} x = \begin{bmatrix}a & b & c & 1\end{bmatrix}^T \end{equation} Then the matrices, $A_k$ can be build from the equations, for example to form $A_1$, we rewrite the first equation in matrix form  $x^TB_1x=0$ where: \begin{equation} B_1 = \begin{bmatrix} 1&0&0&0\\0&1&0&0\\0&0&0&1\\0&0&0&-3.95\\  \end{bmatrix} \end{equation} Then, since from $x^TB_1x=0$ transpose leads to $x^TB_1^Tx=0$, the sum is: $x^T(B_1+B_1^T)x=0$ denoting the matrix as $A_1=(B_1+B_1^T)$, it is symmetric and  $x^TA_1x=0$","How to solve this analytically(not a numerical solution)? For given real and symmetric matrices $A_1,A_2,A_3,A_4\in\mathbb{R}^{4\times4}$ find $0\neq x\in\mathbb{R}^4$ $$x^TA_1x=0$$ $$x^TA_2x=0$$ $$x^TA_3x=0$$ $$x^TA_4x=0$$ Example: Solve the system: \begin{align} a^2+b^2+c &=3.95 \\ ab+bc+c^2 &=4.57 \\ ac+b &=2.63 \\  \end{align} Denoting: \begin{equation} x = \begin{bmatrix}a & b & c & 1\end{bmatrix}^T \end{equation} Then the matrices, $A_k$ can be build from the equations, for example to form $A_1$, we rewrite the first equation in matrix form  $x^TB_1x=0$ where: \begin{equation} B_1 = \begin{bmatrix} 1&0&0&0\\0&1&0&0\\0&0&0&1\\0&0&0&-3.95\\  \end{bmatrix} \end{equation} Then, since from $x^TB_1x=0$ transpose leads to $x^TB_1^Tx=0$, the sum is: $x^T(B_1+B_1^T)x=0$ denoting the matrix as $A_1=(B_1+B_1^T)$, it is symmetric and  $x^TA_1x=0$",,"['calculus', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
62,"Calculate trigonometric integral $ \int_{-\infty}^{\infty}{\sin(x^2)}\,dx$",Calculate trigonometric integral," \int_{-\infty}^{\infty}{\sin(x^2)}\,dx","Recently, I came across the following integral: $$ \int_{-\infty}^{\infty}{\sin(x^2)}\,dx=\int_{-\infty}^{\infty}{\cos(x^2)}\,dx=\sqrt{\frac{\pi}{2}} $$ What are the different ways to calculate such an integral?","Recently, I came across the following integral: $$ \int_{-\infty}^{\infty}{\sin(x^2)}\,dx=\int_{-\infty}^{\infty}{\cos(x^2)}\,dx=\sqrt{\frac{\pi}{2}} $$ What are the different ways to calculate such an integral?",,"['calculus', 'trigonometry', 'improper-integrals']"
63,"Who did first use the concept of ""supremum""?","Who did first use the concept of ""supremum""?",,"Is there one specific person, who first defined the concept of ""supremum""? If so: In which work? In my textbooks or by a quick search on the internet, I did not find an answer to my question.","Is there one specific person, who first defined the concept of ""supremum""? If so: In which work? In my textbooks or by a quick search on the internet, I did not find an answer to my question.",,"['calculus', 'reference-request', 'math-history']"
64,Meaning behind differentials,Meaning behind differentials,,"So I think I understand what differentials are, but let me know if I'm wrong. So let's take $y=f(x)$ such that $f: [a,b] \subset \Bbb R \to \Bbb R$.  Instead of defining the derivative of $f$ in terms of the differentials $\text{dy}$ and $\text{dx}$, we take the derivative $f'(x)$ as our ""primitive"".  Then to define the differentials we do as follows: We find some $x_0 \in [a,b]$ where there is some neighborhood of $x_0$, $N(x_0)$, such that all $f(x)$ in $\{f(x) \in \Bbb R \mid x \in N(x_0)\}$ are differentiable.  Then we choose another point in $N(x_0)$, let's call it $x_1$, such that $x_1 \ne x_0$.  Then let $dx = \Delta x = x_1 - x_0$.  Now this $\Delta x$ doesn't actually have to be very small like we're taught in Calculus 1 (in particular it's not infinitesimal, it's finite).  In fact, as long as $f(x)$ is differentiable for all $x \in [-10^{10}, 10^{10}]$ we could choose $x_0 = -10^{10}$ and $x_1 = 10^{10}$. Then we know that $\Delta y = f'(x_0) \Delta x + \epsilon(\Delta x)$, where $\epsilon(\Delta x)$ is some nonlinear function of $\Delta x$.  If $f(x)$ is smooth, we know that $\epsilon(\Delta x)$ is equal to the sum of powers of $\Delta x$ with some coefficients, by Taylor's theorem.  But of course, $\epsilon(\Delta x)$ won't be so easy to describe if $f(x)$ is only once differentiable.  So we define $dy$ as $dy = f'(x_0) dx$: that is, $dy$ is the linear part of $\Delta y$.  This has the very useful property that $\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \frac{dy}{dx} = f'(x_0)$.  This is then not a definition of the derivative, but a consequence of our definitions. It can be seen from this $dy$ really depends on what we choose as $dx$, but $f'$ is independent of both. This definition can be extended to functions of multiple variables, like $z = f(x, y)$ as well, by letting $\Delta x = dx,\ \Delta y=dy$ and defining $dz$ as $dz = \frac{\partial f(x_0, y_0)}{\partial x}dx + \frac{\partial f(x_0, y_0)}{\partial y} dy$.  So $dz$ is the linear part of $\Delta z$.  Does all of the above look correct? If so, then where I'm having a problem is: 1) how then do we define the derivative of $f(x)$ if not by $f'(x_0) = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x}$? 2) how do we apply this definition of $dx$ to $\int_a^b f(x)dx$?  It seems like the inherit arbitrariness of $dx$ is really going to get in the way of a good definition of the integral.","So I think I understand what differentials are, but let me know if I'm wrong. So let's take $y=f(x)$ such that $f: [a,b] \subset \Bbb R \to \Bbb R$.  Instead of defining the derivative of $f$ in terms of the differentials $\text{dy}$ and $\text{dx}$, we take the derivative $f'(x)$ as our ""primitive"".  Then to define the differentials we do as follows: We find some $x_0 \in [a,b]$ where there is some neighborhood of $x_0$, $N(x_0)$, such that all $f(x)$ in $\{f(x) \in \Bbb R \mid x \in N(x_0)\}$ are differentiable.  Then we choose another point in $N(x_0)$, let's call it $x_1$, such that $x_1 \ne x_0$.  Then let $dx = \Delta x = x_1 - x_0$.  Now this $\Delta x$ doesn't actually have to be very small like we're taught in Calculus 1 (in particular it's not infinitesimal, it's finite).  In fact, as long as $f(x)$ is differentiable for all $x \in [-10^{10}, 10^{10}]$ we could choose $x_0 = -10^{10}$ and $x_1 = 10^{10}$. Then we know that $\Delta y = f'(x_0) \Delta x + \epsilon(\Delta x)$, where $\epsilon(\Delta x)$ is some nonlinear function of $\Delta x$.  If $f(x)$ is smooth, we know that $\epsilon(\Delta x)$ is equal to the sum of powers of $\Delta x$ with some coefficients, by Taylor's theorem.  But of course, $\epsilon(\Delta x)$ won't be so easy to describe if $f(x)$ is only once differentiable.  So we define $dy$ as $dy = f'(x_0) dx$: that is, $dy$ is the linear part of $\Delta y$.  This has the very useful property that $\lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x} = \frac{dy}{dx} = f'(x_0)$.  This is then not a definition of the derivative, but a consequence of our definitions. It can be seen from this $dy$ really depends on what we choose as $dx$, but $f'$ is independent of both. This definition can be extended to functions of multiple variables, like $z = f(x, y)$ as well, by letting $\Delta x = dx,\ \Delta y=dy$ and defining $dz$ as $dz = \frac{\partial f(x_0, y_0)}{\partial x}dx + \frac{\partial f(x_0, y_0)}{\partial y} dy$.  So $dz$ is the linear part of $\Delta z$.  Does all of the above look correct? If so, then where I'm having a problem is: 1) how then do we define the derivative of $f(x)$ if not by $f'(x_0) = \lim_{\Delta x \to 0} \frac{\Delta y}{\Delta x}$? 2) how do we apply this definition of $dx$ to $\int_a^b f(x)dx$?  It seems like the inherit arbitrariness of $dx$ is really going to get in the way of a good definition of the integral.",,['calculus']
65,Do you feel comfortable with integral u-substitution? (reverse chain rule),Do you feel comfortable with integral u-substitution? (reverse chain rule),,"I've made this post both to see if I'm thinking right and to let others read and understand where the ""u-substitution"" method for integration comes from. I really hate substitutions, because you lost track of what's happening. I've read the related posts in this forum and concluded the following: The integral u-substitution is a nice method to find some integrals. It comes from the chain rule: $$\frac{df(g(x))}{dx} = \frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}$$ For me, $\frac{df(x)}{dx}$ is just a notation for the derivative of the function $f$ with respect to $x$, so there's no mean for just $df$ or just $dx$ alone. When we integrate both sides: $$\int \frac{df(g(x))}{dx}dx = \int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx$$ Then: $$\underbrace{f(g(x)) + C}_{\text{integral of a derivative}}= \int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx\tag{1}$$ So if we want to integrate some function in the form $\int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx$ this is gonna be equal $f(g(x)) + C$. That's why we can integrate $\cos(2x)$ this way: $$\int \cos(2x)dx = \int \frac{d\sin(2x)}{d2x}\frac{2}{2}dx = \frac{1}{2}\int\frac{d\sin(2x)}{d2x}\cdot2 \ \ dx$$ See how I didn't change the integrand at all, but I multiplied and divided by $2$ to get the form $$\frac{d\sin(2x)}{d[2x]}\frac{d[2x]}{dx} = \frac{d\cos(2x)}{dx}$$  Then, I can match the pattern in $(1)$ to integrate like this: $$\begin{align}  &\int\frac{d\color{#F01C2C}{f(}\color{Blue}{g(x)}\color{#F01C2C}{)}}{d\color{Blue}{g(x)}}\color{#01cf84}{\frac{dg(x)}{dx}}dx = \color{#F01C2C}{f(}\color{Blue}{g(x)}\color{#F01C2C}{)} + C \\ \int \cos(2x)dx = \frac{1}{2}&\int\frac{d\color{#F01C2C}{\sin(\color{Blue}{2x})}}{d\color{Blue}{2x}}\cdot\color{#01cf84}{\ \ 2} \ \ \ \ dx = \frac{1}{2}\color{#F01C2C}{\sin(\color{Blue}{2x})} + C\end{align}$$ So... am I right? Do you feel comfortable doing substitutions? Would this technique be acceptable in my math tests? (I really prefer this than the substitution method). Update : Let's do this integral:  $$\int x\ln(\cos(x^2))\sin(x^2)\mathrm dx$$ I will derivate $\cos(x^2)$: $$\frac{d}{dx}\cos(x^2) = -2x\sin(x^2)$$ Then I'll multiply and divide the integrand by this result: $$ \begin{align} \int x\ln(\cos(x^2))\sin(x^2) \color{#F01C2C}{\frac{-2x\sin(x^2)}{-2x\sin(x^2)}}dx = \color{#F01C2C}{-\frac{1}{2}}&\int \ln(\cos(x^2))\cdot\color{#F01C2C}{-2x\sin(x^2)}dx \\ &\int\frac{df(g(x))}{dg(x)} \ \ \ \ \ \ \frac{dg(x)}{dx} \ \ \ \ \ \ \ dx \\=&f(g(x)) + C \end{align} $$ So to integrate this, we just have to find the antiderivative of $\ln$ and apply it to the 'point' $\cos(x^2)$. The antiderivative of $\ln$ is $x(\ln(x) - 1)$ by integration by parts. Applying it to $\cos(x^2)$ we have: $\cos(x^2)(\ln(\cos(x^2))-1)$ (this is the antiderivative at $\cos(x^2)$ or $g(x)$. Back in our integral: $$\color{#F01C2C}{-\frac{1}{2}}\int \ln(\cos(x^2))\cdot\color{#F01C2C}{-2x\sin(x^2)}dx = \color{#F01C2C}{-\frac{1}{2}}\cos(x^2)(\ln(\cos(x^2))-1)$$","I've made this post both to see if I'm thinking right and to let others read and understand where the ""u-substitution"" method for integration comes from. I really hate substitutions, because you lost track of what's happening. I've read the related posts in this forum and concluded the following: The integral u-substitution is a nice method to find some integrals. It comes from the chain rule: $$\frac{df(g(x))}{dx} = \frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}$$ For me, $\frac{df(x)}{dx}$ is just a notation for the derivative of the function $f$ with respect to $x$, so there's no mean for just $df$ or just $dx$ alone. When we integrate both sides: $$\int \frac{df(g(x))}{dx}dx = \int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx$$ Then: $$\underbrace{f(g(x)) + C}_{\text{integral of a derivative}}= \int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx\tag{1}$$ So if we want to integrate some function in the form $\int\frac{df(g(x))}{dg(x)}\frac{dg(x)}{dx}dx$ this is gonna be equal $f(g(x)) + C$. That's why we can integrate $\cos(2x)$ this way: $$\int \cos(2x)dx = \int \frac{d\sin(2x)}{d2x}\frac{2}{2}dx = \frac{1}{2}\int\frac{d\sin(2x)}{d2x}\cdot2 \ \ dx$$ See how I didn't change the integrand at all, but I multiplied and divided by $2$ to get the form $$\frac{d\sin(2x)}{d[2x]}\frac{d[2x]}{dx} = \frac{d\cos(2x)}{dx}$$  Then, I can match the pattern in $(1)$ to integrate like this: $$\begin{align}  &\int\frac{d\color{#F01C2C}{f(}\color{Blue}{g(x)}\color{#F01C2C}{)}}{d\color{Blue}{g(x)}}\color{#01cf84}{\frac{dg(x)}{dx}}dx = \color{#F01C2C}{f(}\color{Blue}{g(x)}\color{#F01C2C}{)} + C \\ \int \cos(2x)dx = \frac{1}{2}&\int\frac{d\color{#F01C2C}{\sin(\color{Blue}{2x})}}{d\color{Blue}{2x}}\cdot\color{#01cf84}{\ \ 2} \ \ \ \ dx = \frac{1}{2}\color{#F01C2C}{\sin(\color{Blue}{2x})} + C\end{align}$$ So... am I right? Do you feel comfortable doing substitutions? Would this technique be acceptable in my math tests? (I really prefer this than the substitution method). Update : Let's do this integral:  $$\int x\ln(\cos(x^2))\sin(x^2)\mathrm dx$$ I will derivate $\cos(x^2)$: $$\frac{d}{dx}\cos(x^2) = -2x\sin(x^2)$$ Then I'll multiply and divide the integrand by this result: $$ \begin{align} \int x\ln(\cos(x^2))\sin(x^2) \color{#F01C2C}{\frac{-2x\sin(x^2)}{-2x\sin(x^2)}}dx = \color{#F01C2C}{-\frac{1}{2}}&\int \ln(\cos(x^2))\cdot\color{#F01C2C}{-2x\sin(x^2)}dx \\ &\int\frac{df(g(x))}{dg(x)} \ \ \ \ \ \ \frac{dg(x)}{dx} \ \ \ \ \ \ \ dx \\=&f(g(x)) + C \end{align} $$ So to integrate this, we just have to find the antiderivative of $\ln$ and apply it to the 'point' $\cos(x^2)$. The antiderivative of $\ln$ is $x(\ln(x) - 1)$ by integration by parts. Applying it to $\cos(x^2)$ we have: $\cos(x^2)(\ln(\cos(x^2))-1)$ (this is the antiderivative at $\cos(x^2)$ or $g(x)$. Back in our integral: $$\color{#F01C2C}{-\frac{1}{2}}\int \ln(\cos(x^2))\cdot\color{#F01C2C}{-2x\sin(x^2)}dx = \color{#F01C2C}{-\frac{1}{2}}\cos(x^2)(\ln(\cos(x^2))-1)$$",,"['calculus', 'integration', 'derivatives']"
66,Using the Parseval Identity to compute $ \sum_{n=1}^{+ \infty} \frac{1}{(4n^2-1)^2}$,Using the Parseval Identity to compute, \sum_{n=1}^{+ \infty} \frac{1}{(4n^2-1)^2},"Parseval's Identity : For continuous $f: [- \pi , \pi] \to \mathbb{R}$ $$ \sum_{n=- \infty}^{+ \infty} |c_n|^2 = \frac{1}{2 \pi} \int_{ - \pi}^{ \pi} |f(x)|^2dx, \text{ where } c_n = \frac{1}{2 \pi} \int_{- \pi}^{ \pi} f(x)e^{-inx}dx$$ Problem : Using Parseval's Identity, consider the even function $f:= \frac{1}{2} - \frac{\pi}{4} \sin \left( \frac{x}{2} \right), \  x \in [0, \pi] $ to compute the sum: $$\sum_{n =1}^{+ \infty} \frac{1}{(4n^2-1)^2} $$ This is my first attempt to work with Parseval's Identity and Fourier coefficients . I did read this similar question beforehand, but it didn't help me to complete this task Use Fourier series for computing $\sum_{n=1}^{\infty}\frac{1}{(2n-1)^2}$ . My approach : I will merely write down the results, I did all the calculations by hand on paper and then double-checked by using Mathematica. Writing out the entire process would only make this post blow up in length further. To get the Fourier Coefficients I did integrate using only the definition as above, I first did look at the case for $n=0$. I got $$ c_0 = \frac{1}{2}$$ for the case $ n \neq 0$ I applied integration by parts two times and after a pretty tedious process I did end up with $$c_n = \frac{ in \cos (n \pi)}{4n^2-1}= \frac{in (-1)^n}{4n^2-1} $$ Which was a very satisfying result, because now I have $$|c_n|^2 = \frac{n^2}{(4n^2-1)^2}$$which somewhat looks very close to what I am supposed to get, except for that annoying $n^2$ in the numerator. Finally evaluating $$ \frac{1}{2 \pi} \int_{- \pi}^\pi |f(x)|^2dx = \frac{1}{32}(8+ \pi^2)$$ Doubts and Questions : I have a feeling that my method of integrating was wrong, because I did not make use of $f$ being even which would mean that $f(-x)=f(x)$ and therefore integration from $- \pi, \pi$ would result to be twice the original integral with bounds from 0 to $2 \pi$ I did not make use of the statement $f$ is even, because when I plot $f$ it does not look even at all to me, so I wonder about this statement in general. I suppose it has something to do with the definitions of Fourier Coefficients and their application to $A$-periodical Functions. If the above procedure would happen to be right, how could I get the final statement? More precisely formulated, how would I possible get from $$ \sum_{- \infty}^{+ \infty} \frac{n^2}{(4n^2-1)^2}$$ to the Sum that starts from $1$ and ends at $\infty$ ?","Parseval's Identity : For continuous $f: [- \pi , \pi] \to \mathbb{R}$ $$ \sum_{n=- \infty}^{+ \infty} |c_n|^2 = \frac{1}{2 \pi} \int_{ - \pi}^{ \pi} |f(x)|^2dx, \text{ where } c_n = \frac{1}{2 \pi} \int_{- \pi}^{ \pi} f(x)e^{-inx}dx$$ Problem : Using Parseval's Identity, consider the even function $f:= \frac{1}{2} - \frac{\pi}{4} \sin \left( \frac{x}{2} \right), \  x \in [0, \pi] $ to compute the sum: $$\sum_{n =1}^{+ \infty} \frac{1}{(4n^2-1)^2} $$ This is my first attempt to work with Parseval's Identity and Fourier coefficients . I did read this similar question beforehand, but it didn't help me to complete this task Use Fourier series for computing $\sum_{n=1}^{\infty}\frac{1}{(2n-1)^2}$ . My approach : I will merely write down the results, I did all the calculations by hand on paper and then double-checked by using Mathematica. Writing out the entire process would only make this post blow up in length further. To get the Fourier Coefficients I did integrate using only the definition as above, I first did look at the case for $n=0$. I got $$ c_0 = \frac{1}{2}$$ for the case $ n \neq 0$ I applied integration by parts two times and after a pretty tedious process I did end up with $$c_n = \frac{ in \cos (n \pi)}{4n^2-1}= \frac{in (-1)^n}{4n^2-1} $$ Which was a very satisfying result, because now I have $$|c_n|^2 = \frac{n^2}{(4n^2-1)^2}$$which somewhat looks very close to what I am supposed to get, except for that annoying $n^2$ in the numerator. Finally evaluating $$ \frac{1}{2 \pi} \int_{- \pi}^\pi |f(x)|^2dx = \frac{1}{32}(8+ \pi^2)$$ Doubts and Questions : I have a feeling that my method of integrating was wrong, because I did not make use of $f$ being even which would mean that $f(-x)=f(x)$ and therefore integration from $- \pi, \pi$ would result to be twice the original integral with bounds from 0 to $2 \pi$ I did not make use of the statement $f$ is even, because when I plot $f$ it does not look even at all to me, so I wonder about this statement in general. I suppose it has something to do with the definitions of Fourier Coefficients and their application to $A$-periodical Functions. If the above procedure would happen to be right, how could I get the final statement? More precisely formulated, how would I possible get from $$ \sum_{- \infty}^{+ \infty} \frac{n^2}{(4n^2-1)^2}$$ to the Sum that starts from $1$ and ends at $\infty$ ?",,"['calculus', 'summation', 'self-learning', 'fourier-series']"
67,Center of Mass with two functions,Center of Mass with two functions,,I am having trouble trying to figure out how to go about this problem. I can do problems with single variables but I can not solve this one. I think I would need to subtract the functions from one another but I am not sure which should be subtracted from which. If anyone can help with tips or solutions it would be greatly appreciated. Thank you.,I am having trouble trying to figure out how to go about this problem. I can do problems with single variables but I can not solve this one. I think I would need to subtract the functions from one another but I am not sure which should be subtracted from which. If anyone can help with tips or solutions it would be greatly appreciated. Thank you.,,"['calculus', 'integration']"
68,"Closed form double integral $ \int_{a}^{c}dr \int_{b}^{d} dr' \, \frac{r r'}{\sqrt{(r - a)(r' - b)(r-c)(r'-d)}} \frac{r_<^{\ell}}{r_>^{\ell+1}}$",Closed form double integral," \int_{a}^{c}dr \int_{b}^{d} dr' \, \frac{r r'}{\sqrt{(r - a)(r' - b)(r-c)(r'-d)}} \frac{r_<^{\ell}}{r_>^{\ell+1}}","Is there a closed form expression for $$ S_\ell = \int\limits_{a}^{c}dr \int\limits_{b}^{d} dr' \, \frac{r r'}{\sqrt{(r - a)(r' - b)(r-c)(r'-d)}} \frac{[\min( r , r')]^{\ell}}{[\max(r,r')]^{\ell+1}} $$ for $0<a<b<c<d$? Here $\max(r,r') = r$ if $r\geq r'$ and $\max(r,r') = r'$ otherwise, $\min(r,r')$ is defined similarly. For large $\ell$, it may be shown that the integral is dominated by $r\approx r'$ and asymptotically $$ S_{\ell} \rightarrow \frac{2}{\ell}\int\limits_{b}^{c} dr \, \frac{r^2}{\sqrt{(r - a)(r - b)(r-c)(r-d)}} $$ This can be integrated in a closed form using elliptic integrals as shown here . That solution seemed so general, I wonder if the double integral can also be integrated similarly in a closed form.","Is there a closed form expression for $$ S_\ell = \int\limits_{a}^{c}dr \int\limits_{b}^{d} dr' \, \frac{r r'}{\sqrt{(r - a)(r' - b)(r-c)(r'-d)}} \frac{[\min( r , r')]^{\ell}}{[\max(r,r')]^{\ell+1}} $$ for $0<a<b<c<d$? Here $\max(r,r') = r$ if $r\geq r'$ and $\max(r,r') = r'$ otherwise, $\min(r,r')$ is defined similarly. For large $\ell$, it may be shown that the integral is dominated by $r\approx r'$ and asymptotically $$ S_{\ell} \rightarrow \frac{2}{\ell}\int\limits_{b}^{c} dr \, \frac{r^2}{\sqrt{(r - a)(r - b)(r-c)(r-d)}} $$ This can be integrated in a closed form using elliptic integrals as shown here . That solution seemed so general, I wonder if the double integral can also be integrated similarly in a closed form.",,"['calculus', 'definite-integrals', 'special-functions', 'closed-form', 'elliptic-integrals']"
69,"Prove $\int_0^x \frac{f(u)(x-u)^n}{n!}du=\int_0^x ( \int_0^{u_n}( \dotsb ( \int_0^{u_1}f(t)\,dt ) du_1 ) \dotsb )du_n$ via IBP",Prove  via IBP,"\int_0^x \frac{f(u)(x-u)^n}{n!}du=\int_0^x ( \int_0^{u_n}( \dotsb ( \int_0^{u_1}f(t)\,dt ) du_1 ) \dotsb )du_n","Problem 18-22 on p. 327 of Michael Spivak's Calculus (first edition) is Use induction and integration by parts to show that $$\int_0^x  \frac{f(u)(x-u)^n}{n!}du=\int_0^x \left( \int_0^{u_n}\left( \dotsb  \left(  \int_0^{u_1}f(t)\,dt \right) du_1 \right) \dotsb \right)du_n$$ Previous exercises (14-5 and 14-6) have asked us to prove essentially the same thing by induction and by noting that both sides have the same derivative with respect to $x$ and the same value at zero. So I can see how to do the problem that way. When I try to solve by integration by parts, I'm getting something funny. The $n=1$ case works out OK. When I try to do $n=2$, for instance, let me show you what I'm getting. I want to show $$\int_0^x \frac{f(u)(x-u)^2}{2!}du = \int_0^x \int_0^{u_2}\int_0^{u_1}f(t)\,dt\,du_1\,du_2.$$ If I substitute into the right hand side, using the $n=1$ case, I get that it suffices to show $$\int_0^x \frac{f(u)(x-u)^2}{2!}du =\int_0^x \left(  \int_0^{u_2}f(u_1)(u_2-u_1)\,du_1 \right)du_2.$$ And if I integrate the LHS by parts, I get that it suffices to show $$\int_0^x \int_0^u f(t)(x-u)\,dt\,du =\int_0^x \left(  \int_0^{u_2}f(u_1)(u_2-u_1)\,du_1 \right)du_2.$$ But note these expressions are not the same. I could always expand, etc. and show that it comes out right, but I'm trying to find the way  by simple integration by parts (that is what the author wants me to see). I know I'm just missing something simple!","Problem 18-22 on p. 327 of Michael Spivak's Calculus (first edition) is Use induction and integration by parts to show that $$\int_0^x  \frac{f(u)(x-u)^n}{n!}du=\int_0^x \left( \int_0^{u_n}\left( \dotsb  \left(  \int_0^{u_1}f(t)\,dt \right) du_1 \right) \dotsb \right)du_n$$ Previous exercises (14-5 and 14-6) have asked us to prove essentially the same thing by induction and by noting that both sides have the same derivative with respect to $x$ and the same value at zero. So I can see how to do the problem that way. When I try to solve by integration by parts, I'm getting something funny. The $n=1$ case works out OK. When I try to do $n=2$, for instance, let me show you what I'm getting. I want to show $$\int_0^x \frac{f(u)(x-u)^2}{2!}du = \int_0^x \int_0^{u_2}\int_0^{u_1}f(t)\,dt\,du_1\,du_2.$$ If I substitute into the right hand side, using the $n=1$ case, I get that it suffices to show $$\int_0^x \frac{f(u)(x-u)^2}{2!}du =\int_0^x \left(  \int_0^{u_2}f(u_1)(u_2-u_1)\,du_1 \right)du_2.$$ And if I integrate the LHS by parts, I get that it suffices to show $$\int_0^x \int_0^u f(t)(x-u)\,dt\,du =\int_0^x \left(  \int_0^{u_2}f(u_1)(u_2-u_1)\,du_1 \right)du_2.$$ But note these expressions are not the same. I could always expand, etc. and show that it comes out right, but I'm trying to find the way  by simple integration by parts (that is what the author wants me to see). I know I'm just missing something simple!",,"['calculus', 'integration']"
70,Recovering vector-valued function from its Jacobian Matrix,Recovering vector-valued function from its Jacobian Matrix,,"Consider a function $f:\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}^m$, for which the Jacobian matrix $J_f(x_1,...,x_n)= \left( \begin{array}{ccc} \frac{\partial f_1}{\partial x_1}  & ... & \frac{\partial f_1}{\partial x_n} \\ \vdots &  & \vdots \\ \frac{\partial f_m}{\partial x_1} & ... & \frac{\partial f_m}{\partial x_n} \end{array} \right) $ is given. Also, assume the component functions of $J_f$ are continuously differentiable on $\Omega$, and $\Omega$ is simply connected. If $m=1$ and $n=2$, it is well known that the function $f$ can be recovered from $J_f$ (in this case the gradient) if and only if $\frac{\partial}{\partial x_2}\frac{\partial f_1}{\partial x_1}=\frac{\partial}{\partial x_1}\frac{\partial f_1}{\partial x_2}$. So my question is whether there is a generalization of this result for arbitrary values of $m$ and $n$. I would appreciate any references! Thank you!","Consider a function $f:\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}^m$, for which the Jacobian matrix $J_f(x_1,...,x_n)= \left( \begin{array}{ccc} \frac{\partial f_1}{\partial x_1}  & ... & \frac{\partial f_1}{\partial x_n} \\ \vdots &  & \vdots \\ \frac{\partial f_m}{\partial x_1} & ... & \frac{\partial f_m}{\partial x_n} \end{array} \right) $ is given. Also, assume the component functions of $J_f$ are continuously differentiable on $\Omega$, and $\Omega$ is simply connected. If $m=1$ and $n=2$, it is well known that the function $f$ can be recovered from $J_f$ (in this case the gradient) if and only if $\frac{\partial}{\partial x_2}\frac{\partial f_1}{\partial x_1}=\frac{\partial}{\partial x_1}\frac{\partial f_1}{\partial x_2}$. So my question is whether there is a generalization of this result for arbitrary values of $m$ and $n$. I would appreciate any references! Thank you!",,"['calculus', 'multivariable-calculus']"
71,Limit superior inequalities proof: $\limsup_{n\to \infty} \left(\frac{a_1+a_{n+1}}{a_n}\right)^n\ge e$,Limit superior inequalities proof:,\limsup_{n\to \infty} \left(\frac{a_1+a_{n+1}}{a_n}\right)^n\ge e,Let $a_n$ be a positive sequence. Prove that  $$\limsup_{n\to \infty} \left(\frac{a_1+a_{n+1}}{a_n}\right)^n\geqslant e.$$,Let $a_n$ be a positive sequence. Prove that  $$\limsup_{n\to \infty} \left(\frac{a_1+a_{n+1}}{a_n}\right)^n\geqslant e.$$,,"['calculus', 'inequality', 'proof-writing', 'limsup-and-liminf']"
72,Does $\lim_{n\to\infty}\left|\sin n\right|^\frac1n$ where $n\in\mathbb Z^+$ exist?,Does  where  exist?,\lim_{n\to\infty}\left|\sin n\right|^\frac1n n\in\mathbb Z^+,Does $\lim_{n\to\infty}\left|\sin n\right|^\frac1n$ where $n\in\mathbb Z^+$ exist? How can I determine this using freshman calculus?,Does $\lim_{n\to\infty}\left|\sin n\right|^\frac1n$ where $n\in\mathbb Z^+$ exist? How can I determine this using freshman calculus?,,"['calculus', 'analysis', 'limits']"
73,An asymptotic integral inequality,An asymptotic integral inequality,,"Suppose $f:\mathbb{R}\to\mathbb{R}$ is a continuous function, $g(x)=xf(x)-\int_0^xf(t)\ dt$, and we have $f(0)=0$ and $g(x)=O(x^2)$ as $x\to0$. Is it true that $f(x)=O(x)$ as $x\to0$ ?","Suppose $f:\mathbb{R}\to\mathbb{R}$ is a continuous function, $g(x)=xf(x)-\int_0^xf(t)\ dt$, and we have $f(0)=0$ and $g(x)=O(x^2)$ as $x\to0$. Is it true that $f(x)=O(x)$ as $x\to0$ ?",,"['calculus', 'inequality', 'asymptotics']"
74,Solving linear system of differential equations of 2nd order,Solving linear system of differential equations of 2nd order,,"I need to solve the following system of differential equations: $$ \ddot{x} = 8x + 4y \\ \ddot{y} = -4x$$ Here's what I've done so far: I have reduced this system to a first order system, by saying $x_1 := x, \ x_2 := \dot{x}, \ x_3 := y, \ x_4 := \dot{y}$. This yields the system $\dot{X} = A \cdot X$ with $$ A = \begin{pmatrix} 0 & 1 & 0 & 0\\ 8 & 0 & 4 &0\\0 & 0 & 0 &1\\ -4 & 0 & 0 &0\end{pmatrix} \ \ \ X = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\x_4 \end{pmatrix}$$ Then I've determined the eigenvalues $\lambda_1 = 2, \ \lambda_2 = -2$, with the corresponding eigenvectors $v_1 = \begin{pmatrix}1 & 2 & -1 & -2\end{pmatrix}^{T}$ and $v_2 = \begin{pmatrix}1 & -2 & -1 & 2\end{pmatrix}^{T}$. Now what I'm struggling with is: how do I determine my set of fundamental solutions? I know that the terms $c_1e^{2t}$ and $c_2e^{-2t}$ are part of it for sure, but since I have two double eigenvalues, I also should have a solution somewhat like $te^{2t}$ resp. $te^{-2t}$. But I just don't see how they alle come together.","I need to solve the following system of differential equations: $$ \ddot{x} = 8x + 4y \\ \ddot{y} = -4x$$ Here's what I've done so far: I have reduced this system to a first order system, by saying $x_1 := x, \ x_2 := \dot{x}, \ x_3 := y, \ x_4 := \dot{y}$. This yields the system $\dot{X} = A \cdot X$ with $$ A = \begin{pmatrix} 0 & 1 & 0 & 0\\ 8 & 0 & 4 &0\\0 & 0 & 0 &1\\ -4 & 0 & 0 &0\end{pmatrix} \ \ \ X = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\x_4 \end{pmatrix}$$ Then I've determined the eigenvalues $\lambda_1 = 2, \ \lambda_2 = -2$, with the corresponding eigenvectors $v_1 = \begin{pmatrix}1 & 2 & -1 & -2\end{pmatrix}^{T}$ and $v_2 = \begin{pmatrix}1 & -2 & -1 & 2\end{pmatrix}^{T}$. Now what I'm struggling with is: how do I determine my set of fundamental solutions? I know that the terms $c_1e^{2t}$ and $c_2e^{-2t}$ are part of it for sure, but since I have two double eigenvalues, I also should have a solution somewhat like $te^{2t}$ resp. $te^{-2t}$. But I just don't see how they alle come together.",,"['calculus', 'ordinary-differential-equations']"
75,closest point to on $y=1/x$ to a given point,closest point to on  to a given point,y=1/x,"I feel like I'm missing something basic - given a point $(a,b)$ how do I find the closest point to it on the curve $y=1/x$? I tried the direct approach of pluggin in $y=1/x$ into the distance formula but it leads to an order-4 polynomial...","I feel like I'm missing something basic - given a point $(a,b)$ how do I find the closest point to it on the curve $y=1/x$? I tried the direct approach of pluggin in $y=1/x$ into the distance formula but it leads to an order-4 polynomial...",,"['calculus', 'euclidean-geometry', 'analytic-geometry', 'conic-sections']"
76,Is integration a continuous functional on the Skorohod space?,Is integration a continuous functional on the Skorohod space?,,"I have read several times that integration is a continuous functional on the Skorohod space $D[0,1]$, i.e., the set of all cadlag functions on $[0,1]$ equipped with the Skorohod metric; in symbols, the statement says that the mapping $I:D[0,1] \rightarrow \mathrm{R}: f \mapsto \int_0^1f(x)\mathrm{d}x$ is continuous with respect to the Skorohod metric on $D[0,1]$ and the Euclidean norm on $\mathrm{R}$. Does anybody know a reference to a proof of that fact, or know how to prove it? Many thanks for any help!","I have read several times that integration is a continuous functional on the Skorohod space $D[0,1]$, i.e., the set of all cadlag functions on $[0,1]$ equipped with the Skorohod metric; in symbols, the statement says that the mapping $I:D[0,1] \rightarrow \mathrm{R}: f \mapsto \int_0^1f(x)\mathrm{d}x$ is continuous with respect to the Skorohod metric on $D[0,1]$ and the Euclidean norm on $\mathrm{R}$. Does anybody know a reference to a proof of that fact, or know how to prove it? Many thanks for any help!",,"['calculus', 'functional-analysis', 'integration']"
77,Non-physical Jounce Examples in Nature,Non-physical Jounce Examples in Nature,,"What are some good examples of jounce , the fourth derivative of position, in the non-physics arena? The reason I ask is that A) it's already difficult for a lay to visualize it in the physical arena and B) you never hear of too many examples past the second derivative outside of said physical arena. Jerk is relatively easy to perceive when one slams on the breaks and then lets them go, and the best way to describe jounce is an amusement park ride since you're always being jerked around. Outside of physics, it's a little hard to come by, so I'd like to see if there are other ways of perceiving higher order derivatives in other disciplines. Many thanks in advance!","What are some good examples of jounce , the fourth derivative of position, in the non-physics arena? The reason I ask is that A) it's already difficult for a lay to visualize it in the physical arena and B) you never hear of too many examples past the second derivative outside of said physical arena. Jerk is relatively easy to perceive when one slams on the breaks and then lets them go, and the best way to describe jounce is an amusement park ride since you're always being jerked around. Outside of physics, it's a little hard to come by, so I'd like to see if there are other ways of perceiving higher order derivatives in other disciplines. Many thanks in advance!",,['calculus']
78,Integrable monotonic functions,Integrable monotonic functions,,"Suppose that $f \in L^1(0,+\infty)$ is a monotonic function. Prove that $\lim_{x \to +\infty} x f(x)=0$.","Suppose that $f \in L^1(0,+\infty)$ is a monotonic function. Prove that $\lim_{x \to +\infty} x f(x)=0$.",,['calculus']
79,Simple Partial Fractions Question,Simple Partial Fractions Question,,"For practice, I am integrating, $$\int \frac{x}{3x^2 + 8x -3}dx$$ So, I can then factor it as, $$\int \frac{x}{(3x-1)(x+3)}dx$$ By partial fractions, I decompose $$\frac{x}{(3x-1)(x+3)}= \frac{A}{3x-1} + \frac{B}{x+3}$$ For finding $A$, I multiply both sides by $3x-1$, which gives $$\frac{x(3x-1)}{(3x-1)(x+3)} = \frac{A(3x-1)}{3x-1} + \frac{B(3x-1)}{x+3}$$ So, we have that $$\frac{x}{(x+3)} = A + \frac{B(3x-1)}{x+3}$$ Letting $3x-1=0$, we have that $x=\frac{1}{3}$, so then $$\frac{\frac{1}{3}}{(\frac{1}{3}+3)} = A$$ Thus, we have that $A=\frac{1}{10}$. For determining $B$, we then multiply both sides by $x+3$ and receive, as a similar process to the previous, $$\frac{x(x+3)}{(3x-1)(x+3)} = \frac{A(x+3)}{3x-1} + \frac{B(x+3)}{x+3}$$ Then, $$\frac{x}{3x-1} = \frac{A(x+3)}{3x-1} + B$$ So, if we let $x+3=0$, we then have that $x=-3$ and so, $$\frac{-3}{3(-3)-1}=B$$ So, we then have that $B=\frac{3}{10}$. Thus, our original integral can then be written as, $$\int \frac{x}{(3x-1)(x+3)}dx = \int \frac{1}{10(3x-1)} + \frac{3}{10(x+3)} dx$$ We can, by splitting up the integral find, $$\int \frac{x}{(3x-1)(x+3)}dx = \frac{1}{10} \int \frac{1}{3x-1} dx + \frac{3}{10} \int \frac{1}{x+3} dx$$ Thus, we conclude that, $$\int \frac{x}{3x^2 + 8x -3}dx = \frac{\ln|3x-1|}{30} + \frac{3 \ln|x+3|}{10} + C$$ Wolframalpha shows that, the answer is: $$\frac{1}{30}(\ln(1-3x)+ 9 \ln(3+x)) +C$$ What am I doing wrong, did I miss a negative sign somewhere?","For practice, I am integrating, $$\int \frac{x}{3x^2 + 8x -3}dx$$ So, I can then factor it as, $$\int \frac{x}{(3x-1)(x+3)}dx$$ By partial fractions, I decompose $$\frac{x}{(3x-1)(x+3)}= \frac{A}{3x-1} + \frac{B}{x+3}$$ For finding $A$, I multiply both sides by $3x-1$, which gives $$\frac{x(3x-1)}{(3x-1)(x+3)} = \frac{A(3x-1)}{3x-1} + \frac{B(3x-1)}{x+3}$$ So, we have that $$\frac{x}{(x+3)} = A + \frac{B(3x-1)}{x+3}$$ Letting $3x-1=0$, we have that $x=\frac{1}{3}$, so then $$\frac{\frac{1}{3}}{(\frac{1}{3}+3)} = A$$ Thus, we have that $A=\frac{1}{10}$. For determining $B$, we then multiply both sides by $x+3$ and receive, as a similar process to the previous, $$\frac{x(x+3)}{(3x-1)(x+3)} = \frac{A(x+3)}{3x-1} + \frac{B(x+3)}{x+3}$$ Then, $$\frac{x}{3x-1} = \frac{A(x+3)}{3x-1} + B$$ So, if we let $x+3=0$, we then have that $x=-3$ and so, $$\frac{-3}{3(-3)-1}=B$$ So, we then have that $B=\frac{3}{10}$. Thus, our original integral can then be written as, $$\int \frac{x}{(3x-1)(x+3)}dx = \int \frac{1}{10(3x-1)} + \frac{3}{10(x+3)} dx$$ We can, by splitting up the integral find, $$\int \frac{x}{(3x-1)(x+3)}dx = \frac{1}{10} \int \frac{1}{3x-1} dx + \frac{3}{10} \int \frac{1}{x+3} dx$$ Thus, we conclude that, $$\int \frac{x}{3x^2 + 8x -3}dx = \frac{\ln|3x-1|}{30} + \frac{3 \ln|x+3|}{10} + C$$ Wolframalpha shows that, the answer is: $$\frac{1}{30}(\ln(1-3x)+ 9 \ln(3+x)) +C$$ What am I doing wrong, did I miss a negative sign somewhere?",,['calculus']
80,Proving Lagrange Form of Remainder for Taylor Polynomial,Proving Lagrange Form of Remainder for Taylor Polynomial,,"So I got to the infamous ""the proof is left to you as an exercise"" of the book when I tried to look up how to get the Lagrange form of the remainder for a Taylor polynomial. Is this right? Given $R_{n}(x)=\frac{1}{n!}\int_{0}^{x}f^{n+1}(t)(x-t)^{n}dt$, find out why $R_{n}(x)=\frac{1}{(n+1)!}f^{n+1}(c)x^{n+1}$ for some $c\in [0,x]$ According to FTC, $\int_{0}^{x}f'(t)dt = f(x) - f(0)$ Also, according to the Mean Value Theorem, there exists a $c$ such that $f'(c)(x-0)=f(x)-f(0)$ so $\int_{0}^{x}f'(t)dt = f'(c)(x-0)$ finding the derivative of both sides with respect to $x$: $f'(x) = f'(c)$ so $f^{n+1}(x) = f^{n+1}(c)$ Going back to the integral form of the remainder: $R_{n}(x)=\frac{1}{n!}\int_{0}^{x}f^{n+1}(t)(x-t)^{n}dt$, I replace $f^{n+1}(x)$ with $f^{n+1}(c)$ (This is the step I am most unsure of) Since $f'(c)$ is a constant, I pull it out of the integral and integrate what's left under the integral, giving me $R_{n}(x)=\frac{1}{(n+1)!}f^{n+1}(c)x^{n+1}$ for some $c\in [0,x]$ If this is right, then does it mean that $f'(c)$ is the average value of $f'(x)$  from $0$ to $x$? Sorry if my LaTeX/wording/proof is off. I'd appreciate any corrections/answers to be as simple (notation-wise) as possible please - 1st year undergrad here...","So I got to the infamous ""the proof is left to you as an exercise"" of the book when I tried to look up how to get the Lagrange form of the remainder for a Taylor polynomial. Is this right? Given $R_{n}(x)=\frac{1}{n!}\int_{0}^{x}f^{n+1}(t)(x-t)^{n}dt$, find out why $R_{n}(x)=\frac{1}{(n+1)!}f^{n+1}(c)x^{n+1}$ for some $c\in [0,x]$ According to FTC, $\int_{0}^{x}f'(t)dt = f(x) - f(0)$ Also, according to the Mean Value Theorem, there exists a $c$ such that $f'(c)(x-0)=f(x)-f(0)$ so $\int_{0}^{x}f'(t)dt = f'(c)(x-0)$ finding the derivative of both sides with respect to $x$: $f'(x) = f'(c)$ so $f^{n+1}(x) = f^{n+1}(c)$ Going back to the integral form of the remainder: $R_{n}(x)=\frac{1}{n!}\int_{0}^{x}f^{n+1}(t)(x-t)^{n}dt$, I replace $f^{n+1}(x)$ with $f^{n+1}(c)$ (This is the step I am most unsure of) Since $f'(c)$ is a constant, I pull it out of the integral and integrate what's left under the integral, giving me $R_{n}(x)=\frac{1}{(n+1)!}f^{n+1}(c)x^{n+1}$ for some $c\in [0,x]$ If this is right, then does it mean that $f'(c)$ is the average value of $f'(x)$  from $0$ to $x$? Sorry if my LaTeX/wording/proof is off. I'd appreciate any corrections/answers to be as simple (notation-wise) as possible please - 1st year undergrad here...",,"['calculus', 'taylor-expansion']"
81,Using the binomial expansion to find derivative formulas,Using the binomial expansion to find derivative formulas,,"While differentation the function $f(x) = (1-e^x)^{5}$ , I realize one can use the Binomial Theorem to find formulas for nth derivative: The idea is as follows. Let $f(x) = (1-e^x)^n$ . We have $$ f(x) = \sum_{i=0}^n {n \choose i} (-1)^i e^{ix} $$ So, evidently, the kth derivative is $$ f^{(k)}(x) = \sum i^k (-1)^i {n \choose i} e^{ix} $$ In particular, one has $$ f^{(k)}(0) = \sum i^k (-1)^k {n \choose i} $$ Which seems to be a pretty neat formula. My question is whether anyone has more literature into this identities. In other words, can someone pinpoint me to references that discuss these ideas into more detail. Looking to explore things as $f(x) = (1-g(x))^n$","While differentation the function , I realize one can use the Binomial Theorem to find formulas for nth derivative: The idea is as follows. Let . We have So, evidently, the kth derivative is In particular, one has Which seems to be a pretty neat formula. My question is whether anyone has more literature into this identities. In other words, can someone pinpoint me to references that discuss these ideas into more detail. Looking to explore things as",f(x) = (1-e^x)^{5} f(x) = (1-e^x)^n  f(x) = \sum_{i=0}^n {n \choose i} (-1)^i e^{ix}   f^{(k)}(x) = \sum i^k (-1)^i {n \choose i} e^{ix}   f^{(k)}(0) = \sum i^k (-1)^k {n \choose i}  f(x) = (1-g(x))^n,"['calculus', 'combinatorics', 'reference-request']"
82,Find number of roots of an equation,Find number of roots of an equation,,"I am stuck on the following problem: Given $ f(x)=\displaystyle\frac{4x+3}{x^2+1}$ , find how many roots the equation $$f(f(x))=\int_3^4{f(x)\mathrm{d}x}$$ has in the interval $[1, 4]$ . Consulting GeoGebra for the graphs, there is a root in that interval. Obviously, I first tried to use the minimum/maximum on $f(x)$ in the interval and the monotonicity so as to restrict $f(f(x))$ and the integral to a common interval (The integral's interval should be a subinterval of the function's interval so the existence and uniqueness of the root would be implied by the intermediate value theorem and the function's monotonicity - it is strictly increasing.) but that hasn't worked out. I also computed the integral to be $\ln{\frac{289}{100}}+3\cot^{-1}{13}\approx 1.29$ and then $f(f(1))\approx 1.28< 1.29 < 3.32 = f(f(4))$ but these are based on approximations while I would want a not-so-computational (theoretical, as you may call it) approach, while of course leaving room for some necessary but reasonable computations (not that I would have to approximate $cot^{-1} !)$ . Thanks in advance.","I am stuck on the following problem: Given , find how many roots the equation has in the interval . Consulting GeoGebra for the graphs, there is a root in that interval. Obviously, I first tried to use the minimum/maximum on in the interval and the monotonicity so as to restrict and the integral to a common interval (The integral's interval should be a subinterval of the function's interval so the existence and uniqueness of the root would be implied by the intermediate value theorem and the function's monotonicity - it is strictly increasing.) but that hasn't worked out. I also computed the integral to be and then but these are based on approximations while I would want a not-so-computational (theoretical, as you may call it) approach, while of course leaving room for some necessary but reasonable computations (not that I would have to approximate . Thanks in advance."," f(x)=\displaystyle\frac{4x+3}{x^2+1} f(f(x))=\int_3^4{f(x)\mathrm{d}x} [1, 4] f(x) f(f(x)) \ln{\frac{289}{100}}+3\cot^{-1}{13}\approx 1.29 f(f(1))\approx 1.28< 1.29 < 3.32 = f(f(4)) cot^{-1} !)",['calculus']
83,How can one evaluate this monster $\int_0^\infty \int_0^1 \frac{\ln(1+t^x)}{(1+t)(1+x^2)}\ dt\ dx$,How can one evaluate this monster,\int_0^\infty \int_0^1 \frac{\ln(1+t^x)}{(1+t)(1+x^2)}\ dt\ dx,"When I encountered this integral I couldn't imagine it having such an elegant closed form, and yet here it is: $$\int_0^\infty \int_0^1 \frac{\ln(1+t^x)}{(1+t)(1+x^2)}\ \mathrm{d}t\ \mathrm{d}x=\frac{\pi}{4}\ln^2(2)$$ Or at least this is what I conjectured looking at the numerical value of the integral. Wolfram wasn't able to catch the closed form, and neither was I. My attempts in solving the integral included exchanging the order of integration, ending up with $$\int_0^1 \frac{1}{1+t} \int_0^\infty \frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ dt$$ but this doesn't look simpler. However, the inner integral can be reduced to $$\int_0^\infty \frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x= $$ $$=\int_0^1\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ +\int_1^\infty\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x=$$ $$=\int_0^1\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ +\int_0^1\frac{\ln(1+t^{\frac{1}{x}})}{1+x^2}\ \mathrm{d}x$$ and here I tried to use Taylor series, but couldn't make progress. Suggestions would be awesome.","When I encountered this integral I couldn't imagine it having such an elegant closed form, and yet here it is: Or at least this is what I conjectured looking at the numerical value of the integral. Wolfram wasn't able to catch the closed form, and neither was I. My attempts in solving the integral included exchanging the order of integration, ending up with but this doesn't look simpler. However, the inner integral can be reduced to and here I tried to use Taylor series, but couldn't make progress. Suggestions would be awesome.",\int_0^\infty \int_0^1 \frac{\ln(1+t^x)}{(1+t)(1+x^2)}\ \mathrm{d}t\ \mathrm{d}x=\frac{\pi}{4}\ln^2(2) \int_0^1 \frac{1}{1+t} \int_0^\infty \frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ dt \int_0^\infty \frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x=  =\int_0^1\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ +\int_1^\infty\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x= =\int_0^1\frac{\ln(1+t^x)}{1+x^2}\ \mathrm{d}x\ +\int_0^1\frac{\ln(1+t^{\frac{1}{x}})}{1+x^2}\ \mathrm{d}x,"['calculus', 'integration', 'definite-integrals']"
84,Symmetric formulation for the heat equation,Symmetric formulation for the heat equation,,"Consider the heat equation: $$\partial_t u-div(A\nabla u)=f$$ with $u(0)=0, u=0$ on the boundary of the domain of definition, call it $U$ . Consider a test function $v=v(x,t)$ , and perform the following operations: integrate the strong formulation from $0$ to $t$ : $$u(t)-\int_0^t div(A\nabla u)(\tau)d\tau=...$$ integrate in space and take a convolution product with $v$ : $$\int_0^T\int_U u(t,x)dx v(T-t,x)dt-\int_0^T\int_U\int_0^t div(A\nabla u)(\tau,x)d\tau v(T-t,x)dx  dt=...$$ apply, in space, the divergence theorem: $$\int_U (u(\cdot,x)*v(\cdot,x))(T)dx +\int_U\left(\left (\int_0^\cdot  (A\nabla u)(\tau,x)d\tau\right ) * \nabla v(\cdot,x) \right)(T)dx = ...$$ Now, if $A$ is constant in time, we get a symmetric formulation. In fact, doing some simple change of variables yields: $$\left(\left (\int_0^\cdot  \nabla u(\tau,x)d\tau\right ) * \nabla v(\cdot,x) \right)(T) = \left(\left (\int_0^\cdot  \nabla v(\tau,x)d\tau\right ) * \nabla u(\cdot,x) \right)(T)$$ This also shows that as soon as $A$ is not constant in time, symmetry cannot be expected. Yet, the authors of this very short paper seem to claim this is the case, in the first page. Can anyone confirm that the this convolution-based formulation in non-symmetric in the general case, even if $u,v,A$ are taken as continuous piecewise linear functions in time? It seems strange that a peer reviewed paper contains such an error, so that probably I am making a mistake somewhere. Notes I have decided to use a linear heat equation, which one would obtain after linearization of the formulation in the linked paper my point I think could be made with ODEs directly, the space variable does not play a role here. I decided to keep it to mantain a certain similarity with the above paper Context Solving the heat equation with a space-time method, naturally yields non-symmetric linear systems to be solved, because of the presence of the time derivative. However, applying the convolution operator, one could hope to make those systems symmetric (albeit dense). This might yield increased computational efficiency.","Consider the heat equation: with on the boundary of the domain of definition, call it . Consider a test function , and perform the following operations: integrate the strong formulation from to : integrate in space and take a convolution product with : apply, in space, the divergence theorem: Now, if is constant in time, we get a symmetric formulation. In fact, doing some simple change of variables yields: This also shows that as soon as is not constant in time, symmetry cannot be expected. Yet, the authors of this very short paper seem to claim this is the case, in the first page. Can anyone confirm that the this convolution-based formulation in non-symmetric in the general case, even if are taken as continuous piecewise linear functions in time? It seems strange that a peer reviewed paper contains such an error, so that probably I am making a mistake somewhere. Notes I have decided to use a linear heat equation, which one would obtain after linearization of the formulation in the linked paper my point I think could be made with ODEs directly, the space variable does not play a role here. I decided to keep it to mantain a certain similarity with the above paper Context Solving the heat equation with a space-time method, naturally yields non-symmetric linear systems to be solved, because of the presence of the time derivative. However, applying the convolution operator, one could hope to make those systems symmetric (albeit dense). This might yield increased computational efficiency.","\partial_t u-div(A\nabla u)=f u(0)=0, u=0 U v=v(x,t) 0 t u(t)-\int_0^t div(A\nabla u)(\tau)d\tau=... v \int_0^T\int_U u(t,x)dx v(T-t,x)dt-\int_0^T\int_U\int_0^t div(A\nabla u)(\tau,x)d\tau v(T-t,x)dx  dt=... \int_U (u(\cdot,x)*v(\cdot,x))(T)dx +\int_U\left(\left (\int_0^\cdot  (A\nabla u)(\tau,x)d\tau\right ) * \nabla v(\cdot,x) \right)(T)dx = ... A \left(\left (\int_0^\cdot  \nabla u(\tau,x)d\tau\right ) * \nabla v(\cdot,x) \right)(T) = \left(\left (\int_0^\cdot  \nabla v(\tau,x)d\tau\right ) * \nabla u(\cdot,x) \right)(T) A u,v,A","['calculus', 'partial-differential-equations', 'numerical-methods', 'finite-element-method']"
85,Surface area of part of a sphere viewed through a window,Surface area of part of a sphere viewed through a window,,"I'm trying to work out the proportion of the surface area of a sphere that would be visible through a circular aperture from a viewer at the centre of that sphere. If the aperture is centred on the axis of the sphere (i.e. the line normal to the centre of the aperture goes through the centre of the sphere), it's quite a simple problem: In this situation, the surface area of the visible bit of the sphere is the same as the surface area of a sphere cap and can be looked up or solved by integrating: $$ a = 2 \pi r^2 \int_0^\theta \sin\theta d\theta \\ = 2 \pi r^2 ( 1 - \cos\theta ) $$ The proportion of the sphere that is visible can then be found as: $$ ratio = \frac{a}{4 \pi r^2} $$ hence: $$ ratio = \frac{1}{2} (1 - \cos \theta ) \\ = \frac{1}{2} \left( 1 - \frac{h}{\sqrt{h^2 + (w/2)^2}} \right) $$ Note that the solution here is only in terms of the known values ( $h$ & $w$ ). However, the problem gets a lot more complicated if we offset the aperture slightly: We can solve it geometrically quite easily if we assume it's another simple spherical cap, with the line shown in green as the circular plane that creates the cap.  However, this assumption isn't really valid as in this case the it isn't a plane (I think): it's a surface that is curved in at least one direction. I've been trying to formulate an integral that will allow me to solve this problem and get the ratio in terms of $h$ , $\delta$ and $w$ but I ended up with something that I couldn't figure out how to solve algebraically and which gave me nonsense-looking results when I tried to solve it numerically, so I suspect it's completely wrong. My attempt at formulating it created a new variable $\delta_p$ that represented the offset $\delta$ as you traverse around the axis shown with the purple line marked $h$ .  Viewing the aperture from above (with the yellow dot as the centre of the sphere and the blue circle representing the aperture): gives a definition for $\delta_p$ and I can then use the cosine rule to derive $\delta_p$ in terms of $\phi$ , $w$ and $\delta$ and then create a double integral over $\phi$ and $\theta$ to try to get the surface area.  That ended up with this monstrosity: $$ a = 2 \pi \int_0^{2 \pi} \int_0^{2 \pi} \sqrt{h^2 + \left(\delta \cos\phi + \frac{w}{2} - \sqrt{\delta^2 \left( \cos^2\phi - 1 \right) + \frac{w^2}{4}}\right)^2} \mathrm{d}\phi \; \mathrm{d}\theta $$ which, as I said I failed to solve algebraically and seemed to give me nonsense when solved numerically with a given $h$ , $\delta$ and $w$ (although I might have got the numerical solution wrong). Can anyone help me figure out how to formulate and hence solve this problem? Edit : to try to add a bit of clarity, I modelled the system in a 3D CAD package, with something like a cone (I'm not sure whether it actually classes as a cone) intersecting with a sphere: If I then perform a boolean intersection of those two things, I can draw a circular plane that intersects with two quadrant points of the spherical surface.  If it were a simple spherical cap, it would also intersect with the other two quadrant points, but as you can see if you look carefully at this image, it doesn't: That implies that it isn't a simple spherical cap but a more complex shape, hence my feeling that it needs an integral to solve it properly. The effect becomes more pronounced as $\delta$ increases:","I'm trying to work out the proportion of the surface area of a sphere that would be visible through a circular aperture from a viewer at the centre of that sphere. If the aperture is centred on the axis of the sphere (i.e. the line normal to the centre of the aperture goes through the centre of the sphere), it's quite a simple problem: In this situation, the surface area of the visible bit of the sphere is the same as the surface area of a sphere cap and can be looked up or solved by integrating: The proportion of the sphere that is visible can then be found as: hence: Note that the solution here is only in terms of the known values ( & ). However, the problem gets a lot more complicated if we offset the aperture slightly: We can solve it geometrically quite easily if we assume it's another simple spherical cap, with the line shown in green as the circular plane that creates the cap.  However, this assumption isn't really valid as in this case the it isn't a plane (I think): it's a surface that is curved in at least one direction. I've been trying to formulate an integral that will allow me to solve this problem and get the ratio in terms of , and but I ended up with something that I couldn't figure out how to solve algebraically and which gave me nonsense-looking results when I tried to solve it numerically, so I suspect it's completely wrong. My attempt at formulating it created a new variable that represented the offset as you traverse around the axis shown with the purple line marked .  Viewing the aperture from above (with the yellow dot as the centre of the sphere and the blue circle representing the aperture): gives a definition for and I can then use the cosine rule to derive in terms of , and and then create a double integral over and to try to get the surface area.  That ended up with this monstrosity: which, as I said I failed to solve algebraically and seemed to give me nonsense when solved numerically with a given , and (although I might have got the numerical solution wrong). Can anyone help me figure out how to formulate and hence solve this problem? Edit : to try to add a bit of clarity, I modelled the system in a 3D CAD package, with something like a cone (I'm not sure whether it actually classes as a cone) intersecting with a sphere: If I then perform a boolean intersection of those two things, I can draw a circular plane that intersects with two quadrant points of the spherical surface.  If it were a simple spherical cap, it would also intersect with the other two quadrant points, but as you can see if you look carefully at this image, it doesn't: That implies that it isn't a simple spherical cap but a more complex shape, hence my feeling that it needs an integral to solve it properly. The effect becomes more pronounced as increases:","
a = 2 \pi r^2 \int_0^\theta \sin\theta d\theta \\
= 2 \pi r^2 ( 1 - \cos\theta )
 
ratio = \frac{a}{4 \pi r^2}
 
ratio = \frac{1}{2} (1 - \cos \theta ) \\
= \frac{1}{2} \left( 1 - \frac{h}{\sqrt{h^2 + (w/2)^2}} \right)
 h w h \delta w \delta_p \delta h \delta_p \delta_p \phi w \delta \phi \theta 
a = 2 \pi \int_0^{2 \pi} \int_0^{2 \pi} \sqrt{h^2 + \left(\delta \cos\phi + \frac{w}{2} - \sqrt{\delta^2 \left( \cos^2\phi - 1 \right) + \frac{w^2}{4}}\right)^2} \mathrm{d}\phi \; \mathrm{d}\theta
 h \delta w \delta","['calculus', 'geometry', 'spheres']"
86,"Spivak's Calculus, Ch. 14, Problem 29a: If $f$ is continuous on $[0,1]$, compute $\lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt$.","Spivak's Calculus, Ch. 14, Problem 29a: If  is continuous on , compute .","f [0,1] \lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt","The following problem is from Chapter 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus (a) If $f$ is continuous on $[0,1]$ , compute $\lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt$ . I am aware that someone asked a question about this problem but the question I have is not about the use of L'Hôpital's Rule or about a solution involving supremums or logarithms. I am interested in a proof along the lines of the solution manual, which uses knowledge present in the book only up to this chapter. Here is my solution which provides many intermediate steps that the solution manual doesn't. I'd like to know if they are correct. We can rewrite the limit as $$\lim\limits_{x\to 0^+} \frac{\int_x^1 \frac{f(t)}{t}dt}{\frac{1}{x}}$$ Let $$f(x)=\int_x^1 t^{-1}dt$$ $$g(x)=x^{-1}$$ It can be shown that $$\lim\limits_{x\to 0^+} f(x)=\lim\limits_{x\to 0^+} g(x)=\infty$$ $$\lim\limits_{x\to 0^+} x^{-1}=\infty$$ Also, $$\lim\limits_{x\to 0^+} \frac{f'(x)}{g'(x)}= \lim\limits_{x\to 0^+} \frac{-\frac{1}{x}}{-\frac{1}{x^2}}=\lim\limits_{x\to 0^+} x=0$$ Therefore, by L'Hôpital's Rule we can infer that $$\lim\limits_{x\to 0^+} x \int_x^1 t^{-1} dt=\lim\limits_{x\to 0^+} \frac{f(x)}{g(x)}=\lim\limits_{x\to 0^+} \frac{f'(x)}{g'(x)}=0\tag{1}$$ But we want to compute $\lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt$ . Since $f$ is continuous on $[0,1]$ , it is bounded on that interval. Therefore, there is some $M>0$ such that for $x\in [0,1]$ we have $|f(x)|\leq M$ . Therefore $$-M \leq f(x)\leq M$$ $$-\frac{M}{x}\leq \frac{f(x)}{x}\leq \frac{M}{x}$$ $$-\int_x^1 \frac{M}{t}dt\leq \int_x^1\frac{f(t)}{t}dt\leq \int_x^1\frac{M}{t}dt$$ Multiply by $x$ (and remember that $x\in [0,1]$ $$-x\int_x^1 \frac{M}{t}dt\leq x\int_x^1\frac{f(t)}{t}dt\leq x\int_x^1\frac{M}{t}dt$$ And take the limit $$0=\lim\limits_{x\to 0^+}\left [-x\int_x^1 \frac{M}{t}dt\right ]\leq \lim\limits_{x\to 0^+}x\int_x^1\frac{f(t)}{t}dt\leq \lim\limits_{x\to 0^+} x\int_x^1\frac{M}{t}dt=0$$ Hence we have $$\lim\limits_{x\to 0^+}x\int_x^1\frac{f(t)}{t}dt=0$$ $$\blacksquare$$ The solution manual has something very similar, but as usual is very terse and skips many intermediate steps. I'd like to know if my solution above is correct at every step.","The following problem is from Chapter 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus (a) If is continuous on , compute . I am aware that someone asked a question about this problem but the question I have is not about the use of L'Hôpital's Rule or about a solution involving supremums or logarithms. I am interested in a proof along the lines of the solution manual, which uses knowledge present in the book only up to this chapter. Here is my solution which provides many intermediate steps that the solution manual doesn't. I'd like to know if they are correct. We can rewrite the limit as Let It can be shown that Also, Therefore, by L'Hôpital's Rule we can infer that But we want to compute . Since is continuous on , it is bounded on that interval. Therefore, there is some such that for we have . Therefore Multiply by (and remember that And take the limit Hence we have The solution manual has something very similar, but as usual is very terse and skips many intermediate steps. I'd like to know if my solution above is correct at every step.","f [0,1] \lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt \lim\limits_{x\to 0^+} \frac{\int_x^1 \frac{f(t)}{t}dt}{\frac{1}{x}} f(x)=\int_x^1 t^{-1}dt g(x)=x^{-1} \lim\limits_{x\to 0^+} f(x)=\lim\limits_{x\to 0^+} g(x)=\infty \lim\limits_{x\to 0^+} x^{-1}=\infty \lim\limits_{x\to 0^+} \frac{f'(x)}{g'(x)}= \lim\limits_{x\to 0^+} \frac{-\frac{1}{x}}{-\frac{1}{x^2}}=\lim\limits_{x\to 0^+} x=0 \lim\limits_{x\to 0^+} x \int_x^1 t^{-1} dt=\lim\limits_{x\to 0^+} \frac{f(x)}{g(x)}=\lim\limits_{x\to 0^+} \frac{f'(x)}{g'(x)}=0\tag{1} \lim\limits_{x\to 0^+} x\int_x^1 \frac{f(t)}{t}dt f [0,1] M>0 x\in [0,1] |f(x)|\leq M -M \leq f(x)\leq M -\frac{M}{x}\leq \frac{f(x)}{x}\leq \frac{M}{x} -\int_x^1 \frac{M}{t}dt\leq \int_x^1\frac{f(t)}{t}dt\leq \int_x^1\frac{M}{t}dt x x\in [0,1] -x\int_x^1 \frac{M}{t}dt\leq x\int_x^1\frac{f(t)}{t}dt\leq x\int_x^1\frac{M}{t}dt 0=\lim\limits_{x\to 0^+}\left [-x\int_x^1 \frac{M}{t}dt\right ]\leq \lim\limits_{x\to 0^+}x\int_x^1\frac{f(t)}{t}dt\leq \lim\limits_{x\to 0^+} x\int_x^1\frac{M}{t}dt=0 \lim\limits_{x\to 0^+}x\int_x^1\frac{f(t)}{t}dt=0 \blacksquare","['calculus', 'integration', 'limits', 'derivatives', 'solution-verification']"
87,"show this $(f(x))''+2\ge 0$,for any real numbers, where $f(x)=\frac{x^2-1}{x^{2n}-1}$","show this ,for any real numbers, where",(f(x))''+2\ge 0 f(x)=\frac{x^2-1}{x^{2n}-1},"When I did a question today, I turned to the following question let $n$ be postive integer,and $x\in R$ , let $f(x)=\dfrac{x^2-1}{x^{2n}-1}$ ,show this $$(f(x))''+2\ge 0 \tag{1}$$ For example $n=2$ then $$(f(x))''+2=\dfrac{2x^2(x^4+3x^2+6)}{(x^2+1)^3}\ge 0$$ $n=3$ ,then $$(f(x))''+2=\dfrac{2x^4(x^8+3x^6+6x^4+17x^2+15)}{(x^4+x^2+1)^3}\ge 0$$ and when $n=4$ ,see links1 $n=5$ see links2 , $n=6$ see links3 always hold,so How to prove for $(1)$ ,maybe this usefull: \begin{align*}(f(x))''+2&=(f(x)+x^2)''=\left(\dfrac{x^{2n+2}-1}{x^{2n}-1}\right)''\\ &=\dfrac{x^{2n-2}(4n^2(x^2-1)(x^{2n}+1)-2n(3x^2+1)(x^{2n}-1)+2x^2(x^{2n}-1)^2)}{(x^{2n}-1)^3}\end{align*} see links4","When I did a question today, I turned to the following question let be postive integer,and , let ,show this For example then ,then and when ,see links1 see links2 , see links3 always hold,so How to prove for ,maybe this usefull: see links4","n x\in R f(x)=\dfrac{x^2-1}{x^{2n}-1} (f(x))''+2\ge 0 \tag{1} n=2 (f(x))''+2=\dfrac{2x^2(x^4+3x^2+6)}{(x^2+1)^3}\ge 0 n=3 (f(x))''+2=\dfrac{2x^4(x^8+3x^6+6x^4+17x^2+15)}{(x^4+x^2+1)^3}\ge 0 n=4 n=5 n=6 (1) \begin{align*}(f(x))''+2&=(f(x)+x^2)''=\left(\dfrac{x^{2n+2}-1}{x^{2n}-1}\right)''\\
&=\dfrac{x^{2n-2}(4n^2(x^2-1)(x^{2n}+1)-2n(3x^2+1)(x^{2n}-1)+2x^2(x^{2n}-1)^2)}{(x^{2n}-1)^3}\end{align*}","['calculus', 'inequality']"
88,Spivak Chapter $2$ Problem $22$ Clarification,Spivak Chapter  Problem  Clarification,2 22,"From Spivak's calculus 3rd edition: The result in Problem 1-7 has an important generalization: If $a_1,...,a_n \geq 0$ , then the ""arithmetic mean"" $$A_n = \frac{a_1+...+a_n}{n}$$ and ""geometric mean"" $$G_n = \sqrt[n]{a_1...a_n}$$ satisy $$G_n \leq A_n$$ (a) Suppose that $a_1 < A_n$ Then some $a_i$ satisfies $a_i>A_n$ ; for convenience, say $a_2>A_n$ . Let $\bar{a_1} = A_n$ and let $\bar{a_2} = a_1 + a_2 - \bar{a_1}$ . Show that $$\bar{a_1}\bar{a_2} \geq a_1a_2$$ Why does repeating this process enough times eventually prove that $G_n \leq A_n$ ? I've got the first part but I'm struggling to understand the ""repeating process"" aspect of the proof. From what i understand it comes from each iteration of the process increasing $G_n$ but keeping $A_n$ constant but i don't get how exactly the process is repeated. For example, Given a set of values $\{a_1, a_2\}$ , if $$a_1 < A_2 < a_2$$ $$\bar{a_1}=A_2\\ \bar{a_2}=a_1+a_2-\bar{a_1}$$ then a new set $\{\bar{a_1}, \bar{a_2}\}$ can be created where $\bar{A_2} = A_2$ and $\bar{G_n} \geq G_n$ How would you repeat the process for $\{\bar{a_1}, \bar{a_2}\}$ ?","From Spivak's calculus 3rd edition: The result in Problem 1-7 has an important generalization: If , then the ""arithmetic mean"" and ""geometric mean"" satisy (a) Suppose that Then some satisfies ; for convenience, say . Let and let . Show that Why does repeating this process enough times eventually prove that ? I've got the first part but I'm struggling to understand the ""repeating process"" aspect of the proof. From what i understand it comes from each iteration of the process increasing but keeping constant but i don't get how exactly the process is repeated. For example, Given a set of values , if then a new set can be created where and How would you repeat the process for ?","a_1,...,a_n \geq 0 A_n = \frac{a_1+...+a_n}{n} G_n = \sqrt[n]{a_1...a_n} G_n \leq A_n a_1 < A_n a_i a_i>A_n a_2>A_n \bar{a_1} = A_n \bar{a_2} = a_1 + a_2 - \bar{a_1} \bar{a_1}\bar{a_2} \geq a_1a_2 G_n \leq A_n G_n A_n \{a_1, a_2\} a_1 < A_2 < a_2 \bar{a_1}=A_2\\ \bar{a_2}=a_1+a_2-\bar{a_1} \{\bar{a_1}, \bar{a_2}\} \bar{A_2} = A_2 \bar{G_n} \geq G_n \{\bar{a_1}, \bar{a_2}\}","['calculus', 'inequality', 'a.m.-g.m.-inequality']"
89,Problem involving integration and mean value theorem,Problem involving integration and mean value theorem,,"Let $f:[0,1]\rightarrow \mathbb{R}$ be a continuous function and $\int_0 ^1 f(t)dt =0$ . Show that there is a $c\in (0,1)$ such that $(a)$ $f(c)=\int_0 ^c f(t)dt$ . $(b)$ if $f(0)=0$ , then $cf(c)=(1-c)\int_0 ^c f(t)dt$ . $(c)$ $\int_0 ^c tf(t)dt=0$ . Now, using the functions $h(t)=e^{-t}\int_0 ^t f(u)du$ and $h(t)=\frac{e^{t}}{t}\int_0 ^t f(u)du$ and using MVT, $(a)$ and $(b)$ can be proved. But I can not see how to find a solution for $(c)$ . How should I proceed? Thank you.","Let be a continuous function and . Show that there is a such that . if , then . . Now, using the functions and and using MVT, and can be proved. But I can not see how to find a solution for . How should I proceed? Thank you.","f:[0,1]\rightarrow \mathbb{R} \int_0 ^1 f(t)dt =0 c\in (0,1) (a) f(c)=\int_0 ^c f(t)dt (b) f(0)=0 cf(c)=(1-c)\int_0 ^c f(t)dt (c) \int_0 ^c tf(t)dt=0 h(t)=e^{-t}\int_0 ^t f(u)du h(t)=\frac{e^{t}}{t}\int_0 ^t f(u)du (a) (b) (c)","['calculus', 'integration']"
90,Rod partitioning in n pieces,Rod partitioning in n pieces,,"A straight rod of length n units (obviously n is an integer) is to be partitioned into n pieces of random length. All pieces are to be used to form triangular constructions, so we are looking for the boundaries for the length of the largest piece, to ensure that this will be possible with any 3. If the largest piece is $n_i$ units long, and its largest and smallest possible lengths are $x_i$ and $y_i$ , then $y_i \leq n_i \leq x_i$ and then for all other pieces, it will be: $n_j \leq y_i$ . Also $\sum n_i \leq \sum y_i = n*y_i$ . Therefore for any 3 pieces, we will have: $y_i \leq y_1 \leq n_1 \leq x_1 \leq x_i$ and same for $n_2$ and $n_3$ . Wlg we consider $n_1 \leq n_2 \leq n_3$ . In order for the 3 pieces to form a triangle, it must be $n_1+n_2 \leq n_3$ , $n_1+n_3 \leq n_2$ and $n_2+n_3 \leq n_1$ . Also $n_3-n_2 \geq n_1$ , $n_2-n_1 \geq n_3$ and $n_3-n_1 \geq n_2$ . An obvious solution would be to have all segments equal to 1 unit, so we could form $\frac {n}{3}$ equilateral triangles but obviously this is not the general case. I have seen several solutions to similar problems but with very advanced math (integrals etc) and I can't follow the solutions. I wonder if there is any solution with basic calculus (11th or 12th grade) because I don't have any maths background. FYI this is not homework or assignment. Thank you very much.","A straight rod of length n units (obviously n is an integer) is to be partitioned into n pieces of random length. All pieces are to be used to form triangular constructions, so we are looking for the boundaries for the length of the largest piece, to ensure that this will be possible with any 3. If the largest piece is units long, and its largest and smallest possible lengths are and , then and then for all other pieces, it will be: . Also . Therefore for any 3 pieces, we will have: and same for and . Wlg we consider . In order for the 3 pieces to form a triangle, it must be , and . Also , and . An obvious solution would be to have all segments equal to 1 unit, so we could form equilateral triangles but obviously this is not the general case. I have seen several solutions to similar problems but with very advanced math (integrals etc) and I can't follow the solutions. I wonder if there is any solution with basic calculus (11th or 12th grade) because I don't have any maths background. FYI this is not homework or assignment. Thank you very much.",n_i x_i y_i y_i \leq n_i \leq x_i n_j \leq y_i \sum n_i \leq \sum y_i = n*y_i y_i \leq y_1 \leq n_1 \leq x_1 \leq x_i n_2 n_3 n_1 \leq n_2 \leq n_3 n_1+n_2 \leq n_3 n_1+n_3 \leq n_2 n_2+n_3 \leq n_1 n_3-n_2 \geq n_1 n_2-n_1 \geq n_3 n_3-n_1 \geq n_2 \frac {n}{3},"['calculus', 'inequality']"
91,Find the shortest path enclosed by two functions.,Find the shortest path enclosed by two functions.,,"Let $f, g : [a, b] \to \mathbb{R}$ be two  continuous functions such that $$f(x)<g(x)\ \ \forall\ x\in(a,b)$$ Let $P_1\ (x_1,\ y_1)$ and $P_2\ (x_2,\ y_2)$ such that $$a \le x_1, x_2\le b\ ,\\ \ \ \ \ \ \ \ \ f(x_1) < y_1 < g(x_1)\  and\\ f(x_2) < y_2 < g(x_2)$$ Describe a general way to find the shortest (in length) continuous function $h$ that connects the two points and $$f(x)<h(x)<g(x)\ \ \forall\ x\in[x_1,x_2]$$ Notes You may also consider the case $$f(x)\le h(x)\le g(x)$$ and (optionally): $$\ \ \ \ \ \ \ \ f(x_1) \le y_1\le g(x_1)\  and\\ f(x_2) \le y_2\le g(x_2)$$ You may make any extra necessary assumptions (e.g. differentiability) provided that the problem does not become trivial. (Edit 6:) If you already know the answer telling me which topics I should look into is enough (and you probably don't need to read further). (Edit 4:) Comment Proposed solution (by Christian Blatter): ""Stretch a rubber band from P1 to P2. This band will be straight whenever it does not lie along one of the boundary curves."" If this statement is true, I would very much like to see a proof. (Edit 7:) (Pointed out by TonyK) There's not always a function that satisfies the original argument (not the things in the Notes section). There may only be an infimum for the length of $h$ . (Edit 5:) How the problem arose I was walking on the street which was formed by the arcs of two concentric circles. I was trying to find what trajectory I should follow so that walking ""a given angle"" around the circle and simultaneously ""crossing the street"" I would walk the least. If the line that connects the two points is on the street then problem is trivial. If not I should either: Follow the obvious tangent from $P_1$ to the small circle, walk as much as necessary on this circle to ""find"" the tangent from this circle to the other point etc. Or I should move in such a way so the distance between me and the center of the circles changes at some rate (possibly constant) making a spiral like path. Until now I don't know which option is the best. The original question is more general. The problem can be generalized even further of course. For example one could consider instead of the functions f, g a set of points. Also the problem can be extended in higher dimensions. To save time The post has been edited to include the useful comments made to it. You may skip reading them. However, I encourage you to look at the attempted answers and their comments.","Let be two  continuous functions such that Let and such that Describe a general way to find the shortest (in length) continuous function that connects the two points and Notes You may also consider the case and (optionally): You may make any extra necessary assumptions (e.g. differentiability) provided that the problem does not become trivial. (Edit 6:) If you already know the answer telling me which topics I should look into is enough (and you probably don't need to read further). (Edit 4:) Comment Proposed solution (by Christian Blatter): ""Stretch a rubber band from P1 to P2. This band will be straight whenever it does not lie along one of the boundary curves."" If this statement is true, I would very much like to see a proof. (Edit 7:) (Pointed out by TonyK) There's not always a function that satisfies the original argument (not the things in the Notes section). There may only be an infimum for the length of . (Edit 5:) How the problem arose I was walking on the street which was formed by the arcs of two concentric circles. I was trying to find what trajectory I should follow so that walking ""a given angle"" around the circle and simultaneously ""crossing the street"" I would walk the least. If the line that connects the two points is on the street then problem is trivial. If not I should either: Follow the obvious tangent from to the small circle, walk as much as necessary on this circle to ""find"" the tangent from this circle to the other point etc. Or I should move in such a way so the distance between me and the center of the circles changes at some rate (possibly constant) making a spiral like path. Until now I don't know which option is the best. The original question is more general. The problem can be generalized even further of course. For example one could consider instead of the functions f, g a set of points. Also the problem can be extended in higher dimensions. To save time The post has been edited to include the useful comments made to it. You may skip reading them. However, I encourage you to look at the attempted answers and their comments.","f, g : [a, b] \to \mathbb{R} f(x)<g(x)\ \ \forall\ x\in(a,b) P_1\ (x_1,\ y_1) P_2\ (x_2,\ y_2) a \le x_1,
x_2\le b\ ,\\ \ \ \ \ \ \ \ \ f(x_1) < y_1 < g(x_1)\  and\\ f(x_2) < y_2 < g(x_2) h f(x)<h(x)<g(x)\ \ \forall\ x\in[x_1,x_2] f(x)\le h(x)\le g(x) \ \ \ \ \ \ \ \ f(x_1) \le y_1\le g(x_1)\  and\\ f(x_2) \le y_2\le g(x_2) h P_1","['calculus', 'differential-geometry', 'calculus-of-variations', 'geodesic']"
92,Lower bound for the square root sum of the roots of $x - \ln x - m$,Lower bound for the square root sum of the roots of,x - \ln x - m,"Let $f(x)=x-\ln x$ . Suppose $f(x_1)=f(x_2)=m$ ( $x_1<x_2$ ). How can I prove that $$\sqrt{x_1}+\sqrt{x_2}\ge\sqrt{m}+\frac{1}{\sqrt{m}}?$$ My Attempt I tried to rewrite the condition as \begin{align} &x_1-\ln x_1=m,\\ &x_2-\ln x_2=m. \end{align} By summing up the two equalities and by subtracting one equality from the other, we have \begin{align} &x_1+x_2-\ln x_1x_2=2m,\\ &\sqrt{x_1}+\sqrt{x_2}=\frac{\ln x_2-\ln x_1}{\sqrt{x_2}-\sqrt{x_1}}. \end{align} But I don't know how to continue then.","Let . Suppose ( ). How can I prove that My Attempt I tried to rewrite the condition as By summing up the two equalities and by subtracting one equality from the other, we have But I don't know how to continue then.","f(x)=x-\ln x f(x_1)=f(x_2)=m x_1<x_2 \sqrt{x_1}+\sqrt{x_2}\ge\sqrt{m}+\frac{1}{\sqrt{m}}? \begin{align}
&x_1-\ln x_1=m,\\
&x_2-\ln x_2=m.
\end{align} \begin{align}
&x_1+x_2-\ln x_1x_2=2m,\\
&\sqrt{x_1}+\sqrt{x_2}=\frac{\ln x_2-\ln x_1}{\sqrt{x_2}-\sqrt{x_1}}.
\end{align}","['calculus', 'inequality']"
93,"Smarter way to solve $ \int_0^1\arctan(x^2)\,dx$ [duplicate]",Smarter way to solve  [duplicate]," \int_0^1\arctan(x^2)\,dx","This question already has answers here : Integrating a function of tan inverse (3 answers) Closed 4 years ago . I'm trying to solve the following definite integral: \begin{equation} \int_0^1 \arctan(x^2)dx \end{equation} I tryed first by parial integration, finding: \begin{equation} x\arctan(x^2)\Bigl|_0^1-\int_0^1 \dfrac{2x^2}{1+x^4}dx \end{equation} Then: \begin{equation} \int_0^1 \dfrac{2x^2}{1+x^4}dx=\int_0^1 \dfrac{(x^2+1)+(x^2-1)}{1+x^4}dx=\int_0^1 \dfrac{x^2+1}{1+x^4}dx+\int_0^1 \dfrac{x^2-1}{1+x^4}dx \end{equation} and i introduced the following substitution: \begin{equation} t=x-\dfrac{1}{x}\qquad s=x+\dfrac{1}{x} \end{equation} Than I used this weird substitution: \begin{equation} \dfrac{\sqrt{2}}{2}\arctan\left( \dfrac{t}{\sqrt{2}} \right)\Bigl|_0^1+\int_0^1\dfrac{ds}{(s-2)(s+2)} \end{equation} In the end simple fraction: \begin{equation} \left[\dfrac{\sqrt{2}}{2}\arctan\left( \dfrac{t}{\sqrt 2} \right)+\dfrac{1}{4}\log(2-s)-\dfrac{1}{4}\log(s+2)\right]_0^1 \end{equation} and that is the solution. Now: does it exist a simpler path to solve this integral?","This question already has answers here : Integrating a function of tan inverse (3 answers) Closed 4 years ago . I'm trying to solve the following definite integral: I tryed first by parial integration, finding: Then: and i introduced the following substitution: Than I used this weird substitution: In the end simple fraction: and that is the solution. Now: does it exist a simpler path to solve this integral?","\begin{equation}
\int_0^1 \arctan(x^2)dx
\end{equation} \begin{equation}
x\arctan(x^2)\Bigl|_0^1-\int_0^1 \dfrac{2x^2}{1+x^4}dx
\end{equation} \begin{equation}
\int_0^1 \dfrac{2x^2}{1+x^4}dx=\int_0^1 \dfrac{(x^2+1)+(x^2-1)}{1+x^4}dx=\int_0^1 \dfrac{x^2+1}{1+x^4}dx+\int_0^1 \dfrac{x^2-1}{1+x^4}dx
\end{equation} \begin{equation}
t=x-\dfrac{1}{x}\qquad s=x+\dfrac{1}{x}
\end{equation} \begin{equation}
\dfrac{\sqrt{2}}{2}\arctan\left( \dfrac{t}{\sqrt{2}} \right)\Bigl|_0^1+\int_0^1\dfrac{ds}{(s-2)(s+2)}
\end{equation} \begin{equation}
\left[\dfrac{\sqrt{2}}{2}\arctan\left( \dfrac{t}{\sqrt 2} \right)+\dfrac{1}{4}\log(2-s)-\dfrac{1}{4}\log(s+2)\right]_0^1
\end{equation}","['calculus', 'definite-integrals', 'alternative-proof']"
94,Evaluate $\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^{n^2}\cdot\left(1+\frac{1}{n+1}\right)^{-(n+1)^2}$ using the definition of $e$,Evaluate  using the definition of,\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^{n^2}\cdot\left(1+\frac{1}{n+1}\right)^{-(n+1)^2} e,"Evaluate $\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^{n^2}\cdot\left(1+\frac{1}{n+1}\right)^{-(n+1)^2}.$ I know that $e^x=\lim\limits_{n\to\infty}\left(1+\dfrac{x}{n}\right)^n,$ so I need to somehow convert the limits to this form. I also noticed that $\left(1+\frac{1}{n+1}\right)=\left(\frac{n+2}{n+1}\right)=\left(\frac{n(n+2)}{(n+1)^2}\right)\cdot\left(1+\frac{1}{n}\right).$ Thus, the limit can be rewritten as $$\lim\limits_{n\to\infty}\dfrac{\left(1+\frac{1}{n}\right)^{n^2}}{\left(1+\frac{1}{n}\right)^{(n+1)^2}}\cdot\left(\dfrac{(n+1)^2}{n(n+2)}\right)^{(n+1)^2}\\ =\lim\limits_{n\to\infty}\left(1+\dfrac{1}{n}\right)^{-2n}\cdot\left(1+\dfrac{1}{n}\right)^{-1}\cdot\left(1+\dfrac{1}{n^2+2n}\right)^{n^2+2n}\cdot\left(1+\dfrac{1}{n^2+2n}\right)$$ $$=\dfrac{1}{e^2}\cdot(1)\cdot e\cdot(1)=\dfrac{1}{e}$$","Evaluate I know that so I need to somehow convert the limits to this form. I also noticed that Thus, the limit can be rewritten as","\lim\limits_{n\to\infty} \left(1+\frac{1}{n}\right)^{n^2}\cdot\left(1+\frac{1}{n+1}\right)^{-(n+1)^2}. e^x=\lim\limits_{n\to\infty}\left(1+\dfrac{x}{n}\right)^n, \left(1+\frac{1}{n+1}\right)=\left(\frac{n+2}{n+1}\right)=\left(\frac{n(n+2)}{(n+1)^2}\right)\cdot\left(1+\frac{1}{n}\right). \lim\limits_{n\to\infty}\dfrac{\left(1+\frac{1}{n}\right)^{n^2}}{\left(1+\frac{1}{n}\right)^{(n+1)^2}}\cdot\left(\dfrac{(n+1)^2}{n(n+2)}\right)^{(n+1)^2}\\
=\lim\limits_{n\to\infty}\left(1+\dfrac{1}{n}\right)^{-2n}\cdot\left(1+\dfrac{1}{n}\right)^{-1}\cdot\left(1+\dfrac{1}{n^2+2n}\right)^{n^2+2n}\cdot\left(1+\dfrac{1}{n^2+2n}\right) =\dfrac{1}{e^2}\cdot(1)\cdot e\cdot(1)=\dfrac{1}{e}",['calculus']
95,Problem involving the square root of a trigonometric term,Problem involving the square root of a trigonometric term,,"I was trying to find the shaded area in this figure: And no, it isn't homework. I just chanced upon it on Facebook and had a go at it. I managed to find it using a very simple method. I now want to verify that my answer is correct with calculus. Specifically, I want to verify my answer with integration on an area bounded between two curves which are the smaller and larger circle. To do this, I did the following to frame the problem more simply: Rearranging the larger circle to make it simpler down the line Then, as you can see in the image, the two polar equations of the circles were constructed. I actually had no idea how to do it for circles that are not centered on the origin or on any part of the x or y axes, so I referred to this link to get the equation for that: Plotting polar equations of circles not centered at (0, 0) The next step is to equate the two equations so as to find out where they intersect. This is what I got: $5\sqrt2\cos(\theta-\frac{\pi}{4}) + \sqrt{5^2-50\sin^2(\theta-\frac{\pi}{4})} = 10$ $5\sqrt2\cos(\theta-\frac{\pi}{4}) + 5\sqrt{1-2\sin^2(\theta-\frac{\pi}{4})} = 10$ $5\sqrt2\cos(\theta-\frac{\pi}{4}) + 5\sqrt{\cos(2(\theta-\frac{\pi}{4}))} = 10$ $\sqrt2\cos(\theta-\frac{\pi}{4}) + \sqrt{\cos(2(\theta-\frac{\pi}{4}))} = 2$ And I am stuck here pretty much. I did a bit of thinking and realized that I am not really sure how to solve this type of equation. Checking on Wolfram Alpha, I could find the value of theta. They are as follows: https://www.wolframalpha.com/input/?i=10+%3D+sqrt(50)cos(theta+-+pi%2F4)+%2B+sqrt(25+-+50(sin(theta+-+pi%2F4))%5E2) https://www.wolframalpha.com/input/?i=sqrt(2)cos(theta+-+pi%2F4)+%2B+sqrt(cos(2(theta+-+pi%2F4)))+%3D+2 I did try solving it in Cartesian form then converting it to Polar coordinates later but I didn't manage to solve that either: $(x-5)^2 + (y-5)^2 = 5^2$ $x^2 + y^2 = 10^2$ With some simple substitution I ended up with: $x + \sqrt{10^2-x^2} - 5 = 0$ which I also don't know how to solve analytically. I can easily solve any of these numerically but I want to know how one would go about them analytically, preferably without converting them to complex form. In any case, once the intersections are found, I can perform the necessary integration to obtain the area. That I know I can do. Thank you! EDIT: Okay I must admit I am very tired and this has affected my basic mathematical skills such as rearranging equations /facepalm. This problem is so simple it is not even worth asking. Apologies for the waste of server space. The trick is to bring the root term to one side and everything else to the other side. Square it and voila, everything becomes easy.","I was trying to find the shaded area in this figure: And no, it isn't homework. I just chanced upon it on Facebook and had a go at it. I managed to find it using a very simple method. I now want to verify that my answer is correct with calculus. Specifically, I want to verify my answer with integration on an area bounded between two curves which are the smaller and larger circle. To do this, I did the following to frame the problem more simply: Rearranging the larger circle to make it simpler down the line Then, as you can see in the image, the two polar equations of the circles were constructed. I actually had no idea how to do it for circles that are not centered on the origin or on any part of the x or y axes, so I referred to this link to get the equation for that: Plotting polar equations of circles not centered at (0, 0) The next step is to equate the two equations so as to find out where they intersect. This is what I got: And I am stuck here pretty much. I did a bit of thinking and realized that I am not really sure how to solve this type of equation. Checking on Wolfram Alpha, I could find the value of theta. They are as follows: https://www.wolframalpha.com/input/?i=10+%3D+sqrt(50)cos(theta+-+pi%2F4)+%2B+sqrt(25+-+50(sin(theta+-+pi%2F4))%5E2) https://www.wolframalpha.com/input/?i=sqrt(2)cos(theta+-+pi%2F4)+%2B+sqrt(cos(2(theta+-+pi%2F4)))+%3D+2 I did try solving it in Cartesian form then converting it to Polar coordinates later but I didn't manage to solve that either: With some simple substitution I ended up with: which I also don't know how to solve analytically. I can easily solve any of these numerically but I want to know how one would go about them analytically, preferably without converting them to complex form. In any case, once the intersections are found, I can perform the necessary integration to obtain the area. That I know I can do. Thank you! EDIT: Okay I must admit I am very tired and this has affected my basic mathematical skills such as rearranging equations /facepalm. This problem is so simple it is not even worth asking. Apologies for the waste of server space. The trick is to bring the root term to one side and everything else to the other side. Square it and voila, everything becomes easy.",5\sqrt2\cos(\theta-\frac{\pi}{4}) + \sqrt{5^2-50\sin^2(\theta-\frac{\pi}{4})} = 10 5\sqrt2\cos(\theta-\frac{\pi}{4}) + 5\sqrt{1-2\sin^2(\theta-\frac{\pi}{4})} = 10 5\sqrt2\cos(\theta-\frac{\pi}{4}) + 5\sqrt{\cos(2(\theta-\frac{\pi}{4}))} = 10 \sqrt2\cos(\theta-\frac{\pi}{4}) + \sqrt{\cos(2(\theta-\frac{\pi}{4}))} = 2 (x-5)^2 + (y-5)^2 = 5^2 x^2 + y^2 = 10^2 x + \sqrt{10^2-x^2} - 5 = 0,"['calculus', 'trigonometry', 'area']"
96,Tangent Plane to Level Surfaces Equation Derivation,Tangent Plane to Level Surfaces Equation Derivation,,"I was going through an resource I found online http://mathonline.wikidot.com/tangent-planes-to-level-surfaces In this part: Are they implying that $r'(t_0)$ equals the vector $(x-x_0,y-y_0,z-z_0)$ ? If so, how did they get that? They have $r(t)$ equal the vector $(x(t),y(t),z(t))$ , are they saying that the derivative of $r(t)$ is $(x-x(t),y-y(t),z-z(t))$ so that $r'(t_0)$ is $(x-x_0,y-y_0,z-z_0)$ ? I'm just rambling but I don't understand how they were able to transition from $r'(t)$ to $(x-x_0,y-y_0,z-z_0)$ in the picture above, or, just how they got $r'(t)$ in the first place...","I was going through an resource I found online http://mathonline.wikidot.com/tangent-planes-to-level-surfaces In this part: Are they implying that equals the vector ? If so, how did they get that? They have equal the vector , are they saying that the derivative of is so that is ? I'm just rambling but I don't understand how they were able to transition from to in the picture above, or, just how they got in the first place...","r'(t_0) (x-x_0,y-y_0,z-z_0) r(t) (x(t),y(t),z(t)) r(t) (x-x(t),y-y(t),z-z(t)) r'(t_0) (x-x_0,y-y_0,z-z_0) r'(t) (x-x_0,y-y_0,z-z_0) r'(t)","['calculus', 'multivariable-calculus', 'vectors', 'partial-derivative']"
97,Does $\sum_{n=1}^{\infty} \frac{3+(-1)^n}{n}$ converge or diverge?,Does  converge or diverge?,\sum_{n=1}^{\infty} \frac{3+(-1)^n}{n},"I'm having trouble figuring out if the following series converges or diverges. $$\sum_{n=1}^{\infty} \frac{3+(-1)^n}{n}$$ Here's my thinking: $$\frac{2}{n} \leq \frac{3+(-1)^n}{n}$$ Since $\sum_{n=1}^{\infty} \frac{2}{n}$ diverges, then so does $\sum_{n=1}^{\infty} \frac{3+(-1)^n}{n}$ Is that correct?","I'm having trouble figuring out if the following series converges or diverges. Here's my thinking: Since diverges, then so does Is that correct?",\sum_{n=1}^{\infty} \frac{3+(-1)^n}{n} \frac{2}{n} \leq \frac{3+(-1)^n}{n} \sum_{n=1}^{\infty} \frac{2}{n} \sum_{n=1}^{\infty} \frac{3+(-1)^n}{n},"['calculus', 'sequences-and-series', 'divergent-series']"
98,How does $f(x)= x \sin(\frac{\pi}{x})$ behave?,How does  behave?,f(x)= x \sin(\frac{\pi}{x}),"I think this function is increasing for $x>1$ but wanted to find the reason. So I thought about taking the derivative: $f(x)= x \sin(\frac{\pi}{x})$ Aplying the chain an the product rule, we get: $f'(x)= \sin(\frac{\pi}{x})-\frac{\pi}{x} \cos (\frac{\pi}{x})$ The function is increasing if the derative is more than or equal to $0$, so: $\sin(\frac{\pi}{x})-\frac{\pi}{x} \cos (\frac{\pi}{x}) \ge 0$ $\sin(\frac{\pi}{x}) \ge \frac{\pi}{x} \cos (\frac{\pi}{x}) $ Since $ \cos ( x) > 0$, if $ 0< x < \pi$,  $ \cos (\frac{\pi}{x}) > 0 $, because $ 0<\frac { \pi}{x}< \pi$. $ \tan (\frac{\pi}{x}) \ge \frac{\pi}{x}$ I get to this point and don't know how to continue. I'd like you to help me or give me a hint, or maybe see a different way of showing it. Anyway, thanks.","I think this function is increasing for $x>1$ but wanted to find the reason. So I thought about taking the derivative: $f(x)= x \sin(\frac{\pi}{x})$ Aplying the chain an the product rule, we get: $f'(x)= \sin(\frac{\pi}{x})-\frac{\pi}{x} \cos (\frac{\pi}{x})$ The function is increasing if the derative is more than or equal to $0$, so: $\sin(\frac{\pi}{x})-\frac{\pi}{x} \cos (\frac{\pi}{x}) \ge 0$ $\sin(\frac{\pi}{x}) \ge \frac{\pi}{x} \cos (\frac{\pi}{x}) $ Since $ \cos ( x) > 0$, if $ 0< x < \pi$,  $ \cos (\frac{\pi}{x}) > 0 $, because $ 0<\frac { \pi}{x}< \pi$. $ \tan (\frac{\pi}{x}) \ge \frac{\pi}{x}$ I get to this point and don't know how to continue. I'd like you to help me or give me a hint, or maybe see a different way of showing it. Anyway, thanks.",,['calculus']
99,"Why does Spivak say ""$dx$ (in $\int f(x) dx$) has no meaning in isolation"" in his Calculus textbook?","Why does Spivak say "" (in ) has no meaning in isolation"" in his Calculus textbook?",dx \int f(x) dx,"I was under the impression that the $dx$ in $\int f(x) dx$ is called the differential and represents an infinitesimal change in $x$ . However, at the bottom of p. 264 in Spivak's Calculus (4th ed.) , the author writes ""The symbol $dx$ has no meaning in isolation, any more than the symbol $x \rightarrow$ has any meaning; except in the context $\lim\limits_{x \rightarrow a} f(x)$ ."" He also states on the next page that for $\int x^2 dx$ , ""The entire symbol $x^2 dx$ may be regarded as an abbreviation for: the function $f$ such that $f(x) = x^2$ for all $x$ ."" Upon looking at the appendix, there are no mentions of the word ""differential"" in the book. However, he makes use of them later in the book while describing the integral substitution formula using the equations \begin{equation} \begin{split} u &= g(x),\\ du &= g'(x)dx \end{split} \end{equation} and \begin{equation} \begin{split} x &= g^{-1}(u)\\ dx &= (g^{-1})'(u)du. \end{split} \end{equation} Is anyone familiar with the reasoning behind this seemingly deliberate omission?","I was under the impression that the in is called the differential and represents an infinitesimal change in . However, at the bottom of p. 264 in Spivak's Calculus (4th ed.) , the author writes ""The symbol has no meaning in isolation, any more than the symbol has any meaning; except in the context ."" He also states on the next page that for , ""The entire symbol may be regarded as an abbreviation for: the function such that for all ."" Upon looking at the appendix, there are no mentions of the word ""differential"" in the book. However, he makes use of them later in the book while describing the integral substitution formula using the equations and Is anyone familiar with the reasoning behind this seemingly deliberate omission?","dx \int f(x) dx x dx x \rightarrow \lim\limits_{x
\rightarrow a} f(x) \int x^2 dx x^2 dx f f(x) = x^2 x \begin{equation}
\begin{split}
u &= g(x),\\
du &= g'(x)dx
\end{split}
\end{equation} \begin{equation}
\begin{split}
x &= g^{-1}(u)\\
dx &= (g^{-1})'(u)du.
\end{split}
\end{equation}","['calculus', 'limits', 'definition', 'infinitesimals']"
