,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Prove that a continuous function on (0,1) has a sequence of step functions which converge pointwise to it on [0,1] [duplicate]","Prove that a continuous function on (0,1) has a sequence of step functions which converge pointwise to it on [0,1] [duplicate]",,"This question already has an answer here : Proving a sequence of step functions converges pointwise to a function $f$. (1 answer) Closed 8 years ago . Suppose $f:\left[0,1\right]\rightarrow R$ is continuous on $\left(0,1\right)$ . Prove there is a sequence of step functions $\left\{f_{n}\right\}$ which converge pointwise to $f$ on $\left[0,1\right]$ . John Franks - A (Terse) Introduction to Lebesgue Integration 2009 - 1.5.6 (7), page 13. But, consider $f=\sin\frac{1}{x}$ , with $f=a$ when $x=0$ , is continuous on $\left(0,1\right)$ . Suppose that, a step function $f_{n}$ , is constant on $\left[0,k\right]$ , $k<1$ . Since, $-1\leq\sin\frac{1}{x}\leq1$ on $\left(0,k\right)$ , so $\lim_{n\rightarrow\infty}f_{n}\neq f$ I can't believe this book is wrong. Is my example correct? If it's not, how to prove this problem? Thanks.","This question already has an answer here : Proving a sequence of step functions converges pointwise to a function $f$. (1 answer) Closed 8 years ago . Suppose is continuous on . Prove there is a sequence of step functions which converge pointwise to on . John Franks - A (Terse) Introduction to Lebesgue Integration 2009 - 1.5.6 (7), page 13. But, consider , with when , is continuous on . Suppose that, a step function , is constant on , . Since, on , so I can't believe this book is wrong. Is my example correct? If it's not, how to prove this problem? Thanks.","f:\left[0,1\right]\rightarrow R \left(0,1\right) \left\{f_{n}\right\} f \left[0,1\right] f=\sin\frac{1}{x} f=a x=0 \left(0,1\right) f_{n} \left[0,k\right] k<1 -1\leq\sin\frac{1}{x}\leq1 \left(0,k\right) \lim_{n\rightarrow\infty}f_{n}\neq f",['analysis']
1,On the differentiability of monotone functions,On the differentiability of monotone functions,,"It is well known that if $f$ is monotone on $[a,b]$, then $f$ is differentiable almost everywhere on $[a,b]$. I am trying to find a condition which forces $f$ to be differentiable at its endpoints (right-differentiable at $a$, say): the weaker the restriction the better. Worst case scenario is the condition being exactly the desired property. I thought of uniform continuity, bounded derivative, but these all fail. Is there a property I can add to $f$ that is weaker than ""right-differentiable at $a$"", but that implies the latter? (would also appreciate a reference if there is such a result - I am trying to incorporate this into a study I am doing)","It is well known that if $f$ is monotone on $[a,b]$, then $f$ is differentiable almost everywhere on $[a,b]$. I am trying to find a condition which forces $f$ to be differentiable at its endpoints (right-differentiable at $a$, say): the weaker the restriction the better. Worst case scenario is the condition being exactly the desired property. I thought of uniform continuity, bounded derivative, but these all fail. Is there a property I can add to $f$ that is weaker than ""right-differentiable at $a$"", but that implies the latter? (would also appreciate a reference if there is such a result - I am trying to incorporate this into a study I am doing)",,"['real-analysis', 'analysis', 'derivatives', 'reference-request', 'monotone-functions']"
2,Confusion about integration theorem (stokes?),Confusion about integration theorem (stokes?),,"While studying electrodynamics, I got confused by one specific step that as been used in the book when calculating the magnetic momentum under a closed curve (current wire) $\partial F$: $$ \int_{\partial F} \textbf{r}\times\mathrm{d}\textbf{r} = e_\textbf{z}\mathrm{vol}\:F $$ This seems like some kind of stokes theorem, but when expanding the surface,  it would say $$ \int_{\partial F} \textbf{r}\times\mathrm{d}\textbf{r} = \int_F \textbf{r} \cdot \textrm{d}\textbf{n} $$ But isn't Stoke's theorem the other way around, having the cross product (or exterior derivative) on the side of the surface? Q1: What is the confusion here? EDIT: F is a surface in $\mathbb{R}^2 \times \{0\}$, which makes $\partial F$ a closed curve in said space. $\mathbf{r}$ denotes the position vector. $\mathbf{e_z}$ is the third vector of the standard basis of $\mathbb{R}^3$. The first part will probably be trivial to answer, but since I often get confused by such details, I'd like to take the approach using differential forms and the exterior derivative. So I tried to calculate, $r$ being a 1-form: $$ \mathrm{d}(\textbf{r}\wedge\mathrm{d}\textbf{r}) = \mathrm{d}\textbf{r} \wedge \mathrm{d}\textbf{r} + (-1)^1 \textbf{r}\wedge\mathrm{d}\mathrm{d}\textbf{r} = 0 + 0 = 0 $$ Using that $\wedge$ is bilinear and $\mathrm{d}^2=0$. That cannot be true though, since the integral is nonzero. Q2: where does the mistake of that approach lie?","While studying electrodynamics, I got confused by one specific step that as been used in the book when calculating the magnetic momentum under a closed curve (current wire) $\partial F$: $$ \int_{\partial F} \textbf{r}\times\mathrm{d}\textbf{r} = e_\textbf{z}\mathrm{vol}\:F $$ This seems like some kind of stokes theorem, but when expanding the surface,  it would say $$ \int_{\partial F} \textbf{r}\times\mathrm{d}\textbf{r} = \int_F \textbf{r} \cdot \textrm{d}\textbf{n} $$ But isn't Stoke's theorem the other way around, having the cross product (or exterior derivative) on the side of the surface? Q1: What is the confusion here? EDIT: F is a surface in $\mathbb{R}^2 \times \{0\}$, which makes $\partial F$ a closed curve in said space. $\mathbf{r}$ denotes the position vector. $\mathbf{e_z}$ is the third vector of the standard basis of $\mathbb{R}^3$. The first part will probably be trivial to answer, but since I often get confused by such details, I'd like to take the approach using differential forms and the exterior derivative. So I tried to calculate, $r$ being a 1-form: $$ \mathrm{d}(\textbf{r}\wedge\mathrm{d}\textbf{r}) = \mathrm{d}\textbf{r} \wedge \mathrm{d}\textbf{r} + (-1)^1 \textbf{r}\wedge\mathrm{d}\mathrm{d}\textbf{r} = 0 + 0 = 0 $$ Using that $\wedge$ is bilinear and $\mathrm{d}^2=0$. That cannot be true though, since the integral is nonzero. Q2: where does the mistake of that approach lie?",,"['analysis', 'multivariable-calculus', 'stokes-theorem']"
3,Laplacian of a distribution,Laplacian of a distribution,,"Here is a small result annoying me : Let $u$ be a distribution ($u \in \mathcal{D}'(\mathbb{R}^n)$) such that $\Delta u$ is continuous. Then $u$ is continuous. I am not able to prove this, and have no idea of where to start. Anoyone with a link, a proof or hints ?","Here is a small result annoying me : Let $u$ be a distribution ($u \in \mathcal{D}'(\mathbb{R}^n)$) such that $\Delta u$ is continuous. Then $u$ is continuous. I am not able to prove this, and have no idea of where to start. Anoyone with a link, a proof or hints ?",,"['real-analysis', 'analysis', 'distribution-theory']"
4,Example of finite measure and infinite measure,Example of finite measure and infinite measure,,"Give an example of infinite measure $\nu$ and finite measure $\mu$ on reals such that $\nu ≪ \mu$, and for each $δ > 0$ there is an interval $I ⊂ R$ satisfying $\mu(I) < \delta$ and $\nu(I) ≥ 1$ My attempt: $d\mu = f dx, d\nu = g dx$ with different positive $f, g$. This was given as a hint but I do not know how to proceed with this. I know such an example is impossible if $\nu$ is finite from Theorem 3.5 in Folland's real analysis.","Give an example of infinite measure $\nu$ and finite measure $\mu$ on reals such that $\nu ≪ \mu$, and for each $δ > 0$ there is an interval $I ⊂ R$ satisfying $\mu(I) < \delta$ and $\nu(I) ≥ 1$ My attempt: $d\mu = f dx, d\nu = g dx$ with different positive $f, g$. This was given as a hint but I do not know how to proceed with this. I know such an example is impossible if $\nu$ is finite from Theorem 3.5 in Folland's real analysis.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
5,Is the supremum of the closure equal to the supremum of the set?,Is the supremum of the closure equal to the supremum of the set?,,"Let $X$ be any Banach space and $M\subset X$ be bounded. We know the $\sup(M)\in\overline{M}$ in general. Since $M$ is bounded $\sup_{u\in M}\|u\|<\infty$. Question: Can we somehow write that  $$\sup_{u\in M}\|u\|=\sup_{u\in\overline{M}}\|u\|,$$  where $\overline{M}$ denotes the closure of the set $M$.","Let $X$ be any Banach space and $M\subset X$ be bounded. We know the $\sup(M)\in\overline{M}$ in general. Since $M$ is bounded $\sup_{u\in M}\|u\|<\infty$. Question: Can we somehow write that  $$\sup_{u\in M}\|u\|=\sup_{u\in\overline{M}}\|u\|,$$  where $\overline{M}$ denotes the closure of the set $M$.",,"['real-analysis', 'analysis', 'functional-analysis', 'vector-lattices']"
6,Deducing a derivative from its evaluation at the identity,Deducing a derivative from its evaluation at the identity,,"I have shown that the Frechet derivative at $\mathbf{I}$ of the determinant map is $\text{tr}\,\mathbf{H}$. In notation: $$D \det \mathbf{A}\big|_{\mathbf{I}} (\mathbf{H})=\text{tr}\,\mathbf{H}$$ Is it possible, using this fact , to deduce the derivative of $\det$ at   arbitrary $\mathbf{A}?$ i.e. to easily show:   $$D\det\mathbf{A}\,(\mathbf{H})=\det\mathbf{A}\,\text{tr}\,(\mathbf{A}^{-1}\mathbf{H})$$ This answer suggests that we can, but I don't understand the hint that is given: I think they mean take $f:\;(\mathbf{X}, \mathbf{Y})\mapsto \mathbf{XY}$ and use the chain rule on $\det f$ but I don't understand how to do this, because I get $Df\cdot D\det (f)$ which I cannot really make sense of. Otherwise I am tempted to write: $$D \det \mathbf{A}\big|_{\mathbf{I}} (\mathbf{A}^{-1}\mathbf{H})=\text{tr}\,(\mathbf{A}^{-1}\mathbf{H})$$ but then I somehow need to squeeze out $(\det\mathbf{A})^{-1}$ from the left hand side, and have $D\det \mathbf{A}\,(\mathbf{H})$ left... if that is possible and/or makes sense. Pleeease can someone help me?","I have shown that the Frechet derivative at $\mathbf{I}$ of the determinant map is $\text{tr}\,\mathbf{H}$. In notation: $$D \det \mathbf{A}\big|_{\mathbf{I}} (\mathbf{H})=\text{tr}\,\mathbf{H}$$ Is it possible, using this fact , to deduce the derivative of $\det$ at   arbitrary $\mathbf{A}?$ i.e. to easily show:   $$D\det\mathbf{A}\,(\mathbf{H})=\det\mathbf{A}\,\text{tr}\,(\mathbf{A}^{-1}\mathbf{H})$$ This answer suggests that we can, but I don't understand the hint that is given: I think they mean take $f:\;(\mathbf{X}, \mathbf{Y})\mapsto \mathbf{XY}$ and use the chain rule on $\det f$ but I don't understand how to do this, because I get $Df\cdot D\det (f)$ which I cannot really make sense of. Otherwise I am tempted to write: $$D \det \mathbf{A}\big|_{\mathbf{I}} (\mathbf{A}^{-1}\mathbf{H})=\text{tr}\,(\mathbf{A}^{-1}\mathbf{H})$$ but then I somehow need to squeeze out $(\det\mathbf{A})^{-1}$ from the left hand side, and have $D\det \mathbf{A}\,(\mathbf{H})$ left... if that is possible and/or makes sense. Pleeease can someone help me?",,"['matrices', 'analysis', 'derivatives', 'determinant', 'matrix-calculus']"
7,Term by Term Differentiability in the context of Uniform Convergence,Term by Term Differentiability in the context of Uniform Convergence,,"I'm not sure how differentiability works with uniform convergence. My book says that we can show this (calculation wise) $$\varepsilon (x,a) = \sum_{k=1}^{\infty} E_{k}(x,a)$$ for some $x$ and $a$. In this context, I'm not sure how to go about doing this: Prove that $$f(x)=\sum_{n=1}^{\infty}\frac{1}{n^{2}+x^{2}}$$ is differentiable for all values of $x$. If someone could give me a hand, that would be great. My book has no examples in this topic, so I just chose one of the first questions from it. (If more context is needed, please let me know. This is my first stackexchange question.)","I'm not sure how differentiability works with uniform convergence. My book says that we can show this (calculation wise) $$\varepsilon (x,a) = \sum_{k=1}^{\infty} E_{k}(x,a)$$ for some $x$ and $a$. In this context, I'm not sure how to go about doing this: Prove that $$f(x)=\sum_{n=1}^{\infty}\frac{1}{n^{2}+x^{2}}$$ is differentiable for all values of $x$. If someone could give me a hand, that would be great. My book has no examples in this topic, so I just chose one of the first questions from it. (If more context is needed, please let me know. This is my first stackexchange question.)",,"['real-analysis', 'sequences-and-series', 'analysis', 'derivatives', 'uniform-convergence']"
8,Show series representation of orthogonal polynomials,Show series representation of orthogonal polynomials,,"wikipedia has the following series expansion for hermite polynomials, namely: $$\exp \left\{xt-\frac{t^2}{2}\right\} = \sum_{n=0}^\infty {\mathit{He}}_n(x) \frac {t^n}{n!}.$$ Does anybody see how this can be shown. I tried rearranging a few terms, but did not get far.","wikipedia has the following series expansion for hermite polynomials, namely: $$\exp \left\{xt-\frac{t^2}{2}\right\} = \sum_{n=0}^\infty {\mathit{He}}_n(x) \frac {t^n}{n!}.$$ Does anybody see how this can be shown. I tried rearranging a few terms, but did not get far.",,"['calculus', 'real-analysis', 'analysis', 'mathematical-physics', 'orthogonal-polynomials']"
9,"What mathematical areas lie at the interface of analysis, algebra and geometry? [closed]","What mathematical areas lie at the interface of analysis, algebra and geometry? [closed]",,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question Would it be some area that draws on many fields such as algebraic geometry? Is there some sort of unification of these three fields?,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question Would it be some area that draws on many fields such as algebraic geometry? Is there some sort of unification of these three fields?,,"['abstract-algebra', 'analysis', 'geometry', 'soft-question']"
10,Is the metric space of all the functions defined and bounded on a given set complete?,Is the metric space of all the functions defined and bounded on a given set complete?,,"Let $S$ be a given non-empty set, and let $B(S)$ denote the metric space  of all the bounded (real- or complex-valued) functions with set $S$ as domain, with the metric defined as follows:  $$d(x,y) \colon= \sup_{s \in S} \left\vert x(s) - y(s) \right\vert \ \mbox{ for all } \ x, y \in B(S).$$  Is $B(S)$ a complete metric space? If not, then can we impose any conditions on $S$ under which $B(S)$ does become complete? My effort: Let $(x_n)$ be a Cauchy sequence in $B(S)$. Then, given $\epsilon > 0$, we can find a natural number $N$ such that  $$d(x_m, x_n) < \epsilon \ \mbox{ for all natural numbers } \ m \ \mbox{ and } \ n \ \mbox{ such that } \ m > N \ \mbox{ and } \ n>N. $$ Let $s_0 \in S$ be arbitrary. Then we must have  $$\left\vert x_m(s_0) - x_n(s_0) \right\vert < \epsilon \ \mbox{ for all natural numbers } \ m \ \mbox{ and } \ n \ \mbox{ such that } \ m > N \ \mbox{ and } \ n>N. $$ Thus it follows that the sequence $\left(x_n(s_0)\right)$ is a Cauchy sequence (in the complete metric space $\mathbb{R}$ or $\mathbb{C}$ ) and hence convergent. Let  $$x(s_0) \colon= \lim_{n\to \infty} x_n(s_0).$$ In this way, we define a (real- or complex-valued) function $x$ on the set $S$. We need to show that this function $x$ is bounded, and furthermore that $$\lim_{n\to\infty} d(x_n,x) = 0.$$ How to accomplish these two tasks? Please note that here we have made no assumption whatsoever about continuity of the functions. Please also note that the non-empty set $S$ too is completely arbitrary.","Let $S$ be a given non-empty set, and let $B(S)$ denote the metric space  of all the bounded (real- or complex-valued) functions with set $S$ as domain, with the metric defined as follows:  $$d(x,y) \colon= \sup_{s \in S} \left\vert x(s) - y(s) \right\vert \ \mbox{ for all } \ x, y \in B(S).$$  Is $B(S)$ a complete metric space? If not, then can we impose any conditions on $S$ under which $B(S)$ does become complete? My effort: Let $(x_n)$ be a Cauchy sequence in $B(S)$. Then, given $\epsilon > 0$, we can find a natural number $N$ such that  $$d(x_m, x_n) < \epsilon \ \mbox{ for all natural numbers } \ m \ \mbox{ and } \ n \ \mbox{ such that } \ m > N \ \mbox{ and } \ n>N. $$ Let $s_0 \in S$ be arbitrary. Then we must have  $$\left\vert x_m(s_0) - x_n(s_0) \right\vert < \epsilon \ \mbox{ for all natural numbers } \ m \ \mbox{ and } \ n \ \mbox{ such that } \ m > N \ \mbox{ and } \ n>N. $$ Thus it follows that the sequence $\left(x_n(s_0)\right)$ is a Cauchy sequence (in the complete metric space $\mathbb{R}$ or $\mathbb{C}$ ) and hence convergent. Let  $$x(s_0) \colon= \lim_{n\to \infty} x_n(s_0).$$ In this way, we define a (real- or complex-valued) function $x$ on the set $S$. We need to show that this function $x$ is bounded, and furthermore that $$\lim_{n\to\infty} d(x_n,x) = 0.$$ How to accomplish these two tasks? Please note that here we have made no assumption whatsoever about continuity of the functions. Please also note that the non-empty set $S$ too is completely arbitrary.",,"['real-analysis', 'general-topology', 'analysis', 'functional-analysis', 'metric-spaces']"
11,On continuity and existence of all directional derivatives at a point of a scalar field whose gradient at that point is $\vec 0$,On continuity and existence of all directional derivatives at a point of a scalar field whose gradient at that point is,\vec 0,"Let $f:\mathbb R^2 \to \mathbb R$ be a function such that for some $a \in \mathbb R^2$ , $\nabla f(a)$ exists and equals $\vec 0$. Is $f$ necessarily continuous at $a$ ? Do all directional derivatives $f'(a;y)$ exist at $a$? Suppose $f$ is continuous at $a$ and $\nabla f(a)=0$. Do the directional derivatives at $a$ exist in this case?","Let $f:\mathbb R^2 \to \mathbb R$ be a function such that for some $a \in \mathbb R^2$ , $\nabla f(a)$ exists and equals $\vec 0$. Is $f$ necessarily continuous at $a$ ? Do all directional derivatives $f'(a;y)$ exist at $a$? Suppose $f$ is continuous at $a$ and $\nabla f(a)=0$. Do the directional derivatives at $a$ exist in this case?",,"['analysis', 'multivariable-calculus']"
12,Using induction to prove a formula for the Fibonacci sequence involving the solutions of $x^2=x+1$,Using induction to prove a formula for the Fibonacci sequence involving the solutions of,x^2=x+1,"Let $\{f(n)\}_{n=1}^{\infty}$ denote the Fibonacci sequence defined by $f(1)=1, f(2)=1$, and $f(n)=f(n-1)+ f(n-2)$ for all $n\geq 3$. Let $α=\dfrac{1+\sqrt{5}}{2}$ and $β=\dfrac{1-\sqrt{5}}{2}.$ Prove that $f(n)=\dfrac{α^n - β^n}{α-β}$ for all $n \in \mathbb{N}$. So I'm using the Principle of Complete Induction for this problem but I'm slightly confused on the induction step. So for the base case I would should $f(1)=1$, which is true. For the induction step, would I fix $n$ at $3$? I'm not sure where to go from here. Any help is appreciated.","Let $\{f(n)\}_{n=1}^{\infty}$ denote the Fibonacci sequence defined by $f(1)=1, f(2)=1$, and $f(n)=f(n-1)+ f(n-2)$ for all $n\geq 3$. Let $α=\dfrac{1+\sqrt{5}}{2}$ and $β=\dfrac{1-\sqrt{5}}{2}.$ Prove that $f(n)=\dfrac{α^n - β^n}{α-β}$ for all $n \in \mathbb{N}$. So I'm using the Principle of Complete Induction for this problem but I'm slightly confused on the induction step. So for the base case I would should $f(1)=1$, which is true. For the induction step, would I fix $n$ at $3$? I'm not sure where to go from here. Any help is appreciated.",,"['analysis', 'induction', 'fibonacci-numbers']"
13,Differentiating under the (Lebesgue) integral sign.,Differentiating under the (Lebesgue) integral sign.,,"There's this guide for proving the theorem about differentiation under integral sign, I took a look around some questions here, but I still have doubts. I know I can probably find this done in a book, but I'm so close that I'd rather ask here instead. Let $(Y, {\scr A},\mu)$ be a measure space with $\mu(Y) < +\infty$ , $U \subseteq \Bbb R^N$ open, and $f\colon U \times Y \to \Bbb R$ such that $f(x, \cdot)$ is $\scr A$ -measurable and bounded, for all $x \in U$ , so we can define $F(x) = \int_Y f(x,y)\,{\rm d}\mu(y)$ . (a) If $f(\cdot, y)$ is continuous for all $y \in Y$ and $|f(x,y)| \leq M$ for all $(x,y) \in U \times Y$ for some $M  \geq 0$ , then $F$ is continuous. Let $x_0 \in U$ and take $(x_n)_{n \geq 1} \subseteq U$ such that $x_n \to x_0$ . Let's prove that $F(x_n) \to F(x_0)$ . By continuity of $f(\cdot, y)$ , we have that $x_n \to x_0$ implies that $f(x_n,y) \to f(x_0, y)$ . Since $|f(x,y)| \leq M$ , we have that $\int_Yf(x_n,y)\,{\rm d}\mu(y) \to \int_Y f(x_0,y)\,{\rm d}\mu(y)$ , but that's what we wanted. (b) Suppose now that $f(\cdot, y)$ is $C^1$ in $U$ for all $y \in Y$ and that $\left|\frac{\partial f}{\partial x_j}(x,y)\right| \leq M_1$ for all $(x,y) \in U \times Y$ for some $M_1 \geq 0$ . Show that $F$ is $C^1$ in $U$ and: $$\frac{\partial F}{\partial x_j}(x,y) = \int_Y \frac{\partial f}{\partial x_j}(x,y)\,{\rm d}\mu(y),$$ for all $x \in U$ , $j=1,2,\ldots, N$ . We can suppose without loss of generality that $N=1$ , so we want to prove that $F'(x) = \int_Y \frac{\partial f}{\partial x}(x,y)\,{\rm d}\mu(y)$ here. I also know that once we prove that $F'$ is given by that formula, the proof from (a) applies with $F' \leftrightarrow F$ and $\frac{\partial f}{\partial x} \leftrightarrow f$ , so that $F'$ being continuous is a given. We have to prove the formula, then. All the versions of this exercise I've seen so far or uses some uniform continuity, which I don't have here, or the integral is over $[0,1]$ , allowing the ${\rm stuff} = \int_0^1 {\rm stuff}\,{\rm d}y$ step. I only know that $\mu(Y) < +\infty$ , not that $\mu(Y) = 1$ . The natural thing to do was to use the MVT: \begin{align}\left|\frac{F(x)-F(x_0)}{x-x_0} - \int_Y \frac{\partial f}{\partial x}(x,y)\,{\rm d}\mu(y)\right| &= \left|\frac{f(x,y)-f(x_0,y)}{x-x_0} - \int_Y \frac{\partial f}{\partial x}(x,y)\,{\rm d}\mu(y)\right| \\ &= \left|\frac{\partial f}{\partial x}(\xi, y) - \int_Y \frac{\partial f}{\partial x}(x_0,y)\,{\rm d}\mu(y)\right|,\end{align} for some $\xi$ between $x$ and $x_0$ , and I can't do anything from here on. Now what?","There's this guide for proving the theorem about differentiation under integral sign, I took a look around some questions here, but I still have doubts. I know I can probably find this done in a book, but I'm so close that I'd rather ask here instead. Let be a measure space with , open, and such that is -measurable and bounded, for all , so we can define . (a) If is continuous for all and for all for some , then is continuous. Let and take such that . Let's prove that . By continuity of , we have that implies that . Since , we have that , but that's what we wanted. (b) Suppose now that is in for all and that for all for some . Show that is in and: for all , . We can suppose without loss of generality that , so we want to prove that here. I also know that once we prove that is given by that formula, the proof from (a) applies with and , so that being continuous is a given. We have to prove the formula, then. All the versions of this exercise I've seen so far or uses some uniform continuity, which I don't have here, or the integral is over , allowing the step. I only know that , not that . The natural thing to do was to use the MVT: for some between and , and I can't do anything from here on. Now what?","(Y, {\scr A},\mu) \mu(Y) < +\infty U \subseteq \Bbb R^N f\colon U \times Y \to \Bbb R f(x, \cdot) \scr A x \in U F(x) = \int_Y f(x,y)\,{\rm d}\mu(y) f(\cdot, y) y \in Y |f(x,y)| \leq M (x,y) \in U \times Y M  \geq 0 F x_0 \in U (x_n)_{n \geq 1} \subseteq U x_n \to x_0 F(x_n) \to F(x_0) f(\cdot, y) x_n \to x_0 f(x_n,y) \to f(x_0, y) |f(x,y)| \leq M \int_Yf(x_n,y)\,{\rm d}\mu(y) \to \int_Y f(x_0,y)\,{\rm d}\mu(y) f(\cdot, y) C^1 U y \in Y \left|\frac{\partial f}{\partial x_j}(x,y)\right| \leq M_1 (x,y) \in U \times Y M_1 \geq 0 F C^1 U \frac{\partial F}{\partial x_j}(x,y) = \int_Y \frac{\partial f}{\partial x_j}(x,y)\,{\rm d}\mu(y), x \in U j=1,2,\ldots, N N=1 F'(x) = \int_Y \frac{\partial f}{\partial x}(x,y)\,{\rm d}\mu(y) F' F' \leftrightarrow F \frac{\partial f}{\partial x} \leftrightarrow f F' [0,1] {\rm stuff} = \int_0^1 {\rm stuff}\,{\rm d}y \mu(Y) < +\infty \mu(Y) = 1 \begin{align}\left|\frac{F(x)-F(x_0)}{x-x_0} - \int_Y \frac{\partial f}{\partial x}(x,y)\,{\rm d}\mu(y)\right| &= \left|\frac{f(x,y)-f(x_0,y)}{x-x_0} - \int_Y \frac{\partial f}{\partial x}(x,y)\,{\rm d}\mu(y)\right| \\ &= \left|\frac{\partial f}{\partial x}(\xi, y) - \int_Y \frac{\partial f}{\partial x}(x_0,y)\,{\rm d}\mu(y)\right|,\end{align} \xi x x_0","['integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
14,Proof of divergence in analysis,Proof of divergence in analysis,,"I aim to show that the sequence $x_n := n^2 - 10n $ diverges to $+\infty$ by using the definition of divergence (i.e. for a given $M \in \mathbb{R}$, there exists $N$ such that $n \geq N$ implies $x_n > M$). So my strategy for proving this is that if you give me $M$, I will give you $N$ such that $n \geq N$ implies $x_n > M$. Somehow it is getting harder than I thought it was, though. I have tried to set $N = M+10$, but I realized that that does not make sense because $M \in \mathbb{R}$ and $N \in \mathbb{N}$. Any suggestions?","I aim to show that the sequence $x_n := n^2 - 10n $ diverges to $+\infty$ by using the definition of divergence (i.e. for a given $M \in \mathbb{R}$, there exists $N$ such that $n \geq N$ implies $x_n > M$). So my strategy for proving this is that if you give me $M$, I will give you $N$ such that $n \geq N$ implies $x_n > M$. Somehow it is getting harder than I thought it was, though. I have tried to set $N = M+10$, but I realized that that does not make sense because $M \in \mathbb{R}$ and $N \in \mathbb{N}$. Any suggestions?",,[]
15,The importance of the Van der Corput lemma in analysis and beyond,The importance of the Van der Corput lemma in analysis and beyond,,"The Van der Corput lemma states the following: Introduce the following oscillatory integral $$ I(a,b)=\int^{b}_{a}e^{ih(t)}dt. $$ Then $(1)$ if $|h'(t)|\geq \lambda>0$ and $h'$ is monotonic, then we have the estimate $$ |I(a,b)|\leq C\lambda^{-1} $$ $(2)$ if $h\in C^k([a,b])$ and $|h^{(k)}(t)|\geq \lambda>0$, then we have the estimate $$ |I(a,b)|\leq C\lambda^{-\frac{1}{k}} $$ where the constant $C$ is independent of $a$ and $b$. My question is, why is the Van der Corput lemma so important? For example, we can bound the measure of a sublevel set. Why else is it important? Thanks in advance for your comments and help.","The Van der Corput lemma states the following: Introduce the following oscillatory integral $$ I(a,b)=\int^{b}_{a}e^{ih(t)}dt. $$ Then $(1)$ if $|h'(t)|\geq \lambda>0$ and $h'$ is monotonic, then we have the estimate $$ |I(a,b)|\leq C\lambda^{-1} $$ $(2)$ if $h\in C^k([a,b])$ and $|h^{(k)}(t)|\geq \lambda>0$, then we have the estimate $$ |I(a,b)|\leq C\lambda^{-\frac{1}{k}} $$ where the constant $C$ is independent of $a$ and $b$. My question is, why is the Van der Corput lemma so important? For example, we can bound the measure of a sublevel set. Why else is it important? Thanks in advance for your comments and help.",,['analysis']
16,Does lower density of the natural numbers satisfy triangle inequality?,Does lower density of the natural numbers satisfy triangle inequality?,,"The lower density of a set $A\subset\mathbb{N}$ is defined to be $$d_l(A) = \liminf_{n\to \infty}\frac{|A\cap \{1,...,n\}|}{n}$$ while upper density is given by  $$d_u(A) = \limsup_{n\to \infty}\frac{|A\cap \{1,...,n\}|}{n}$$ It is easy to prove that for $A,B\subset \mathbb{N}$, $$d_u(A\cup B)\leq d_u(A)+d_u(B)$$ However, I am having trouble proving or disproving the same for $d_l$. Can anyone please help me out with this? Thank you very much!","The lower density of a set $A\subset\mathbb{N}$ is defined to be $$d_l(A) = \liminf_{n\to \infty}\frac{|A\cap \{1,...,n\}|}{n}$$ while upper density is given by  $$d_u(A) = \limsup_{n\to \infty}\frac{|A\cap \{1,...,n\}|}{n}$$ It is easy to prove that for $A,B\subset \mathbb{N}$, $$d_u(A\cup B)\leq d_u(A)+d_u(B)$$ However, I am having trouble proving or disproving the same for $d_l$. Can anyone please help me out with this? Thank you very much!",,"['real-analysis', 'analysis', 'functional-analysis', 'number-theory']"
17,On the Spivak's proof of the theorem 3-11 (calculus on manifolds),On the Spivak's proof of the theorem 3-11 (calculus on manifolds),,"In second paragraph of the case 1 within the proof: What is $U$ s.t $A\subset U$ and satisfies in the proof of the case 1 of theorem 3-11. $\psi_i$ is defined on $U_i$ and its support is not compact. Now, How any $\psi_i$ is defined on $U$ s.t $\psi_1+\cdots+\psi_n$  to be well-defind on $U$. Why is $\psi_1+\cdots+\psi_n>0$ on $U$?","In second paragraph of the case 1 within the proof: What is $U$ s.t $A\subset U$ and satisfies in the proof of the case 1 of theorem 3-11. $\psi_i$ is defined on $U_i$ and its support is not compact. Now, How any $\psi_i$ is defined on $U$ s.t $\psi_1+\cdots+\psi_n$  to be well-defind on $U$. Why is $\psi_1+\cdots+\psi_n>0$ on $U$?",,"['analysis', 'multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
18,Does anyone have a proof that the intersection and union of two compact sets is compact.,Does anyone have a proof that the intersection and union of two compact sets is compact.,,"I have my take on it. It is quite informal and don;t know where it would be evaluated correctly on an exam. Since the sets are compact that means for every open cover there is a finite cover. When the intersection is in question, taking any two covers of the sets, then there respective finite covers and taking that intersection of two finite covers gives us a finite cover of an intersection. For union just taking the finite covers and uniting them also gives us a finite cover of a union. Would this be sufficient in your opinions?","I have my take on it. It is quite informal and don;t know where it would be evaluated correctly on an exam. Since the sets are compact that means for every open cover there is a finite cover. When the intersection is in question, taking any two covers of the sets, then there respective finite covers and taking that intersection of two finite covers gives us a finite cover of an intersection. For union just taking the finite covers and uniting them also gives us a finite cover of a union. Would this be sufficient in your opinions?",,['calculus']
19,Using basic definition to derive duplication formula for Gamma function,Using basic definition to derive duplication formula for Gamma function,,"Baby Rudin chap 8, 8.21, some consequences of the gamma function, one of them is $$\Gamma(x)=\frac{2^{x-1}}{\sqrt\pi}\Gamma(\frac{x}2)\Gamma(\frac{x+1}2)$$ Rudin noted that this identity ""followed directly from $\Gamma(1/2)=\sqrt\pi$ and from theorem 8.19"", in which the theorem 8.19 is in fact a definition of the gamma function (1). $\Gamma(x+1)=x\Gamma(x)$, for all $x>0$. (2). $\Gamma(1)=1$. (3).$\log\Gamma(x)$ is convex on $\Bbb R^+$. And then I was having quite a hard time fighting to derive the duplication formula from this definition. I have to say that for me, it is NOT trivial or direct at all. I wonder, is there really a simple, or as Rudin put it, direct way to do this?","Baby Rudin chap 8, 8.21, some consequences of the gamma function, one of them is $$\Gamma(x)=\frac{2^{x-1}}{\sqrt\pi}\Gamma(\frac{x}2)\Gamma(\frac{x+1}2)$$ Rudin noted that this identity ""followed directly from $\Gamma(1/2)=\sqrt\pi$ and from theorem 8.19"", in which the theorem 8.19 is in fact a definition of the gamma function (1). $\Gamma(x+1)=x\Gamma(x)$, for all $x>0$. (2). $\Gamma(1)=1$. (3).$\log\Gamma(x)$ is convex on $\Bbb R^+$. And then I was having quite a hard time fighting to derive the duplication formula from this definition. I have to say that for me, it is NOT trivial or direct at all. I wonder, is there really a simple, or as Rudin put it, direct way to do this?",,"['calculus', 'real-analysis', 'analysis']"
20,Infinite series of integrals of $L^2$ functions,Infinite series of integrals of  functions,L^2,"I'm hoping someone can help me with this integration problem I've been struggling with. Let $\{f_n\}$ be a sequence in $L^2(\mathbb{R})$ such that $\sum_{n=1}^\infty \lVert f_n\rVert^2_2<\infty$ and $\sum_{n=1}^\infty f_n (x)=0$ for a.e. $x\in\mathbb{R}$. I need to show that for every $g\in L^2(\mathbb{R})$, $$\sum_{n=1}^\infty \int_\mathbb{R} f_ng\,d\mu$$ exists and is equal to zero, where $\mu$ is Lebesgue measure. The assumptions of the problem lead me to believe that some sort of Cauchy-Schwartz Inequality result can be used, maybe along with Dominated Convergence Theorem. However, every attempt I've made at using CSI has led to unwanted square roots. I was, however, able to prove with Fatou's Lemma that $\sum_{n=1}^\infty |f_n|^2$ is integrable. I'm not sure if it helps, but I thought I'd share it in case it does. I'd appreciate any help for this problem.","I'm hoping someone can help me with this integration problem I've been struggling with. Let $\{f_n\}$ be a sequence in $L^2(\mathbb{R})$ such that $\sum_{n=1}^\infty \lVert f_n\rVert^2_2<\infty$ and $\sum_{n=1}^\infty f_n (x)=0$ for a.e. $x\in\mathbb{R}$. I need to show that for every $g\in L^2(\mathbb{R})$, $$\sum_{n=1}^\infty \int_\mathbb{R} f_ng\,d\mu$$ exists and is equal to zero, where $\mu$ is Lebesgue measure. The assumptions of the problem lead me to believe that some sort of Cauchy-Schwartz Inequality result can be used, maybe along with Dominated Convergence Theorem. However, every attempt I've made at using CSI has led to unwanted square roots. I was, however, able to prove with Fatou's Lemma that $\sum_{n=1}^\infty |f_n|^2$ is integrable. I'm not sure if it helps, but I thought I'd share it in case it does. I'd appreciate any help for this problem.",,"['real-analysis', 'integration', 'analysis', 'functional-analysis']"
21,"Is the closure of every bounded convex set , with non-empty interior , in $\mathbb R^n (n>1)$ homeomorphic to a closed ball?","Is the closure of every bounded convex set , with non-empty interior , in  homeomorphic to a closed ball?",\mathbb R^n (n>1),"Is the closure of every bounded convex set , with non-empty interior , in $\mathbb R^n (n>1)$ homeomorphic to a closed ball (by closed ball I mean $B[a,r]:=\{x \in \mathbb R^n : d(x,a)\le r\}$ , where $a \in \mathbb R^n$ and $r>0$ ) ?","Is the closure of every bounded convex set , with non-empty interior , in $\mathbb R^n (n>1)$ homeomorphic to a closed ball (by closed ball I mean $B[a,r]:=\{x \in \mathbb R^n : d(x,a)\le r\}$ , where $a \in \mathbb R^n$ and $r>0$ ) ?",,"['analysis', 'metric-spaces']"
22,Fundamental solution for a parabolic PDE with costant coefficents,Fundamental solution for a parabolic PDE with costant coefficents,,"as it is well known, the fundamental solution of the heat equation is the function $G(t,x)=\frac{1}{(4\pi t)^{n/2}}e^{\frac{|x|^2}{4t}}$, for all $t>0,x\in\mathbb{R}^n$. I wonder if exists (and if you have same references) a similar explicit formula for the fundamental solution for a parabolic PDE with constant coefficents.","as it is well known, the fundamental solution of the heat equation is the function $G(t,x)=\frac{1}{(4\pi t)^{n/2}}e^{\frac{|x|^2}{4t}}$, for all $t>0,x\in\mathbb{R}^n$. I wonder if exists (and if you have same references) a similar explicit formula for the fundamental solution for a parabolic PDE with constant coefficents.",,"['analysis', 'functional-analysis', 'partial-differential-equations', 'fundamental-solution']"
23,diffeomorphism inbetween two subsets of $\mathbb{R}^2$,diffeomorphism inbetween two subsets of,\mathbb{R}^2,"Consider the function $$f: \mathbb{R}^2 \to \mathbb{R}^2, \space\space f(x, y) := \pmatrix{x(1-y) \cr x y}$$ Now first, why is $f$ continuously differentiable? Then, I want to prove that $f$ transforms the strip $(0, \infty) \times (0, 1)$ diffeomorph into the quadrant $(0, \infty) \times (0, \infty)$. What I thought: would it already be sufficient for the first statement to determine the Jacobi matrix? Or would I have to do more to show it's indeed continuously differentiable? And in order to be a diffeomorphism, $f$ needs to be bijective on the said domain, and $f$ aswell as $f^{-1}$ need to be continuously differentiable. I don't really know how to show this though.","Consider the function $$f: \mathbb{R}^2 \to \mathbb{R}^2, \space\space f(x, y) := \pmatrix{x(1-y) \cr x y}$$ Now first, why is $f$ continuously differentiable? Then, I want to prove that $f$ transforms the strip $(0, \infty) \times (0, 1)$ diffeomorph into the quadrant $(0, \infty) \times (0, \infty)$. What I thought: would it already be sufficient for the first statement to determine the Jacobi matrix? Or would I have to do more to show it's indeed continuously differentiable? And in order to be a diffeomorphism, $f$ needs to be bijective on the said domain, and $f$ aswell as $f^{-1}$ need to be continuously differentiable. I don't really know how to show this though.",,"['analysis', 'multivariable-calculus', 'derivatives']"
24,Finding all accumulation points of the a set.,Finding all accumulation points of the a set.,,"I am working on homework for my intro to analysis class and I was assigned a problem to find all accumulation points of the set $S=\{x\mid x\in[0,1]$ and $x$ is rational$\}$. I hodge podged a solution but I feel like it may be incorrect. If anyone could let me know, mostly in that can I assume that the following approximation even exists? I feel like using the fact that $\mathbb{R}$ is ordered should work but I'm not sure. Let k   be some number in the interval [0,1]  . Then we can do the following, write an approxmation of k   as $0.d_{1}$   where $k-0.1<0.d_{1}<k$   and $0\le d_{1}\le9$   Which must be possible as $\mathbb{R}$   is an ordered field. We know $0.d_{1}=\frac{d_{1}}{10}$   which is rational. Then we can find a better approxmiation by writing $k$   as $0.d_1 d_2$   where $k-0.01<0.d_1 d_2<k$   and $0\le d_1,d_2\le9$  . Once again $0.d_1 d_2=\frac{d_1 d_2}{10^{2}}$   which is rational. Continue by writing $k-10^{-n}<0.d_{1}d_{2}d_{3}\ldots d_{n}<k$   where $\frac{d_1 d_2 d_3\ldots d_n}{10^n}$   is obviously rational. Rewrite $ 0.d_1 d_2 d_3\cdots d_n$   as $\sum_{i=1}^n 10^{-i}\ldots d_i$. Then, if we take the limits of the RHS and LHS of the inequality we get, $$\lim_{n\to\infty}k-10^{-n}=k\text{ and }\lim_{n\to\infty}k=k$$ which by the squeeze theorem says that $$\lim_{n\to\infty}\sum_{i=1}^{n}10^{-i}\cdot d_{i}=k$$   Repeating this process with $k+0.1>0.d_{1}>k$   gives the limits: $$\lim_{n\to\infty}k+10^{-n}=k\text{ and }\lim_{n\to\infty}k=k$$ which by the squeeze theorem says that $$\lim_{n\to\infty}\sum_{i=1}^{n}10^{-i}\cdot d_{i}=k.$$ Therefore this shows that every number in the range $[0,1]$   is bounded above and below by an infinite sum of rational numbers. Which then implies that any number $k\in[0,1]$   has infinitely many $x\in Q$   within the range $\left\{ k-\epsilon,\:k+\epsilon\right\} $   . Therefore the accumulation points of $S=\{x \mid x\in[0,1]$   and x   is rational}  is all of $[0,1]$ Note: I don't actually know the answer to this question if my solution is wrong.","I am working on homework for my intro to analysis class and I was assigned a problem to find all accumulation points of the set $S=\{x\mid x\in[0,1]$ and $x$ is rational$\}$. I hodge podged a solution but I feel like it may be incorrect. If anyone could let me know, mostly in that can I assume that the following approximation even exists? I feel like using the fact that $\mathbb{R}$ is ordered should work but I'm not sure. Let k   be some number in the interval [0,1]  . Then we can do the following, write an approxmation of k   as $0.d_{1}$   where $k-0.1<0.d_{1}<k$   and $0\le d_{1}\le9$   Which must be possible as $\mathbb{R}$   is an ordered field. We know $0.d_{1}=\frac{d_{1}}{10}$   which is rational. Then we can find a better approxmiation by writing $k$   as $0.d_1 d_2$   where $k-0.01<0.d_1 d_2<k$   and $0\le d_1,d_2\le9$  . Once again $0.d_1 d_2=\frac{d_1 d_2}{10^{2}}$   which is rational. Continue by writing $k-10^{-n}<0.d_{1}d_{2}d_{3}\ldots d_{n}<k$   where $\frac{d_1 d_2 d_3\ldots d_n}{10^n}$   is obviously rational. Rewrite $ 0.d_1 d_2 d_3\cdots d_n$   as $\sum_{i=1}^n 10^{-i}\ldots d_i$. Then, if we take the limits of the RHS and LHS of the inequality we get, $$\lim_{n\to\infty}k-10^{-n}=k\text{ and }\lim_{n\to\infty}k=k$$ which by the squeeze theorem says that $$\lim_{n\to\infty}\sum_{i=1}^{n}10^{-i}\cdot d_{i}=k$$   Repeating this process with $k+0.1>0.d_{1}>k$   gives the limits: $$\lim_{n\to\infty}k+10^{-n}=k\text{ and }\lim_{n\to\infty}k=k$$ which by the squeeze theorem says that $$\lim_{n\to\infty}\sum_{i=1}^{n}10^{-i}\cdot d_{i}=k.$$ Therefore this shows that every number in the range $[0,1]$   is bounded above and below by an infinite sum of rational numbers. Which then implies that any number $k\in[0,1]$   has infinitely many $x\in Q$   within the range $\left\{ k-\epsilon,\:k+\epsilon\right\} $   . Therefore the accumulation points of $S=\{x \mid x\in[0,1]$   and x   is rational}  is all of $[0,1]$ Note: I don't actually know the answer to this question if my solution is wrong.",,['analysis']
25,Is this function a metric?,Is this function a metric?,,"Let $X$,$d$ be a metric space. Define $d'$ as the minimum of $1$ and $d$: $$ d':\ X^2 \rightarrow \mathbb{R}:\ d'(x,y) = \min\{1,d(x,y)\} $$ The question is whether $d'$ is a metric. I've managed to prove that the first two properties of a metric hold for this function, but I can't seem to figure out how to prove the triangle inequality. Any hints?","Let $X$,$d$ be a metric space. Define $d'$ as the minimum of $1$ and $d$: $$ d':\ X^2 \rightarrow \mathbb{R}:\ d'(x,y) = \min\{1,d(x,y)\} $$ The question is whether $d'$ is a metric. I've managed to prove that the first two properties of a metric hold for this function, but I can't seem to figure out how to prove the triangle inequality. Any hints?",,"['analysis', 'metric-spaces']"
26,"Let $a_{2n-1}=-1/\sqrt{n}$ for $n=1,2,\dots$ Show that $\prod (1+a_n)$ converges but that $\sum a_n$ diverges.",Let  for  Show that  converges but that  diverges.,"a_{2n-1}=-1/\sqrt{n} n=1,2,\dots \prod (1+a_n) \sum a_n","Let $a_{2n-1}=-1/\sqrt{n}$, $a_{2n}=1/\sqrt{n}+1/n$ for $n=1,2,\dots$ Show that $\prod (1+a_n)$ converges but that $\sum a_n$ diverges. What I have found so far is that $\prod_{k=2}^{2n} a_n$=$3(1-\frac{1}{2\sqrt{2}})\cdots (1-\frac{1}{n\sqrt{n}})$ and $\prod_{k=2}^{2n+1} a_n$=$3(1-\frac{1}{2\sqrt{2}})\cdots (1-\frac{1}{n\sqrt{n}})(1-\frac{1}{\sqrt{n+1}})$ I'm considering using the theorem that if each $a_n \ge 0$, then the product $\prod(1-a_n)$ converges if and only if, the series $\sum a_n$ converges. So since $\sum \frac{1}{n\sqrt{n}}$ converges, I think the above product converges as well, but I can't use this theorem right now because of the odd partial products. How can I resolve this problem? Also, I can clearly see that $\sum a_n$ diverges, but how can I prove this rigorously? I would greatly appreciate some help.","Let $a_{2n-1}=-1/\sqrt{n}$, $a_{2n}=1/\sqrt{n}+1/n$ for $n=1,2,\dots$ Show that $\prod (1+a_n)$ converges but that $\sum a_n$ diverges. What I have found so far is that $\prod_{k=2}^{2n} a_n$=$3(1-\frac{1}{2\sqrt{2}})\cdots (1-\frac{1}{n\sqrt{n}})$ and $\prod_{k=2}^{2n+1} a_n$=$3(1-\frac{1}{2\sqrt{2}})\cdots (1-\frac{1}{n\sqrt{n}})(1-\frac{1}{\sqrt{n+1}})$ I'm considering using the theorem that if each $a_n \ge 0$, then the product $\prod(1-a_n)$ converges if and only if, the series $\sum a_n$ converges. So since $\sum \frac{1}{n\sqrt{n}}$ converges, I think the above product converges as well, but I can't use this theorem right now because of the odd partial products. How can I resolve this problem? Also, I can clearly see that $\sum a_n$ diverges, but how can I prove this rigorously? I would greatly appreciate some help.",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'infinite-product']"
27,A counter example,A counter example,,"I have this two spaces $$C_{\theta}=\{u\in C(\overline{\Omega}), \sup (|x|^{\theta} |u(x)|)<\infty\}$$ with the norm $\displaystyle\|u \|_{\theta}=\sup_{\Omega}(|x|^{\theta} |u(x)|)$ and $$L_{1}^{p^*}=\{u ~~\text{measurable};~~\int_{\Omega} (|x|\cdot|u(x)|)^{p^*} dx<\infty\}$$ with the norm $\|u\|_{L^{p^*}_{1}}^{p^*}=\int_{\Omega} (|x|\cdot|u(x)|)^{p^*}dx$ Where $\Omega\subset\mathbb{R}^N$ is bounded with $0\in \Omega$ and $\theta>\frac{N}{p}>1,$ $p^*=\frac{pN}{N-p}$ and $N>p.$ I need a counter example to say that  $C_{\theta}$ is not continuously embeded in $L^{p^*}_{1}$, so i'm searching a function which is in $C_{\theta}$ but not in $L^{p^*}_1$ Or a sequence $u_n$ which converge to $u$ un $C_{\theta}$ but not in $L^{p^*}_{1}$ Please help me thank you","I have this two spaces $$C_{\theta}=\{u\in C(\overline{\Omega}), \sup (|x|^{\theta} |u(x)|)<\infty\}$$ with the norm $\displaystyle\|u \|_{\theta}=\sup_{\Omega}(|x|^{\theta} |u(x)|)$ and $$L_{1}^{p^*}=\{u ~~\text{measurable};~~\int_{\Omega} (|x|\cdot|u(x)|)^{p^*} dx<\infty\}$$ with the norm $\|u\|_{L^{p^*}_{1}}^{p^*}=\int_{\Omega} (|x|\cdot|u(x)|)^{p^*}dx$ Where $\Omega\subset\mathbb{R}^N$ is bounded with $0\in \Omega$ and $\theta>\frac{N}{p}>1,$ $p^*=\frac{pN}{N-p}$ and $N>p.$ I need a counter example to say that  $C_{\theta}$ is not continuously embeded in $L^{p^*}_{1}$, so i'm searching a function which is in $C_{\theta}$ but not in $L^{p^*}_1$ Or a sequence $u_n$ which converge to $u$ un $C_{\theta}$ but not in $L^{p^*}_{1}$ Please help me thank you",,"['analysis', 'functional-analysis']"
28,Total Variation of Constant Function,Total Variation of Constant Function,,"I want to prove that the total variation of, $f:[a,b] \to \mathbb{R}$, is $0$ iff $f$ is a constant function, but i'm not entirely sure how. I can intuitively see why that it would be zero since the Total Variation of a function represents the number of peaks of the function. Though, how to rigorously prove it i'm not sure.","I want to prove that the total variation of, $f:[a,b] \to \mathbb{R}$, is $0$ iff $f$ is a constant function, but i'm not entirely sure how. I can intuitively see why that it would be zero since the Total Variation of a function represents the number of peaks of the function. Though, how to rigorously prove it i'm not sure.",,"['real-analysis', 'analysis']"
29,"Find the gradient of $f^*(x)=\langle (\nabla f)^{-1}(x),x)\rangle-f( (\nabla f)^{-1}(x))$ for $x \in \mathbb{R^n}$",Find the gradient of  for,"f^*(x)=\langle (\nabla f)^{-1}(x),x)\rangle-f( (\nabla f)^{-1}(x)) x \in \mathbb{R^n}","I am stuck at the following exercise which serves as a preparation for the upcoming exam: Let $U \subset \mathbb{R}^n$ be open and $f \in C^2(U, \mathbb{R})$ such that $\det Hf(x) \neq 0, \forall x \in U$ and $\nabla f: U \to \nabla f(U) =:V$ bijective. Under the above assumptions the function $f^*: V \subset \mathbb{R}^n \to \mathbb{R}$ given by $$f^*(u):=\langle (\nabla f)^{-1}(u),u)\rangle-f( (\nabla f)^{-1}(u)) \tag{*}$$ is clearly well defined and $f^* \in C^1(V, \mathbb{R})$ thanks to the Hessian Matrix of $f$ being invertible and thus $\nabla f$ is a local diffeomorphism. The bijective nature of $\nabla f$ makes $(\nabla f)^{-1}$ a class $C^1$ function. My problem : I am supposed to find the gradient of $f^*$, in fact I am supposed to verify the identity: $$(\nabla f^*)(u)=(\nabla f)^{-1} (u) \tag{$\alpha$} $$ My approach : Let $u=(u_1, \dots , u_n) \in \mathbb{R}^n$ then it makes sense to only see if the statement in ($\alpha)$ holds for an arbitrary $i \in \lbrace 1, \dots , n \rbrace$. My problem is that I know nothing about how $(\nabla f)^{-1}$ looks like, so I thought the best attempt would be to just let it be in an abstract form. Let $h(u):=\langle (\nabla f)^{-1}(u),u)\rangle$ then I assume $h(u)$ looks like $$ h(u)=(\nabla f)_1^{-1}(u)*u_1 + \dots + (\nabla f)_i^{-1}(u)*u_i + \dots + (\nabla f)_n^{-1}(u)* u_n \\ \implies \frac{\partial h(u)}{u_i}=\frac{\partial(\nabla f)_1^{-1}(u)}{\partial u_i}u_1+ \dots + \frac{\partial(\nabla f)_i^{-1}(u)}{\partial u_i}u_i + (\nabla f)_i^{-1}(u)+ \dots + \frac{\partial (\nabla f)_n^{-1}(u)}{\partial u_i}u_n$$ If I have made no mistakes, all there is left to do is to check the second expression in (*) and hope that it annihilates the partially differentiated terms in my expression above. But all I get is: $$\frac{\partial f(( \nabla f^{-1})(u))}{\partial u_i}= \frac{\partial f}{\partial u_i}( \nabla f)^{-1}(u) \cdot \frac{\partial(\nabla f)_i^{-1}}{\partial u_i}(u)$$ So it would be nice if the first term would simplify to $u_i$ but I really don't see it. Any help or simplification would be appreciated.","I am stuck at the following exercise which serves as a preparation for the upcoming exam: Let $U \subset \mathbb{R}^n$ be open and $f \in C^2(U, \mathbb{R})$ such that $\det Hf(x) \neq 0, \forall x \in U$ and $\nabla f: U \to \nabla f(U) =:V$ bijective. Under the above assumptions the function $f^*: V \subset \mathbb{R}^n \to \mathbb{R}$ given by $$f^*(u):=\langle (\nabla f)^{-1}(u),u)\rangle-f( (\nabla f)^{-1}(u)) \tag{*}$$ is clearly well defined and $f^* \in C^1(V, \mathbb{R})$ thanks to the Hessian Matrix of $f$ being invertible and thus $\nabla f$ is a local diffeomorphism. The bijective nature of $\nabla f$ makes $(\nabla f)^{-1}$ a class $C^1$ function. My problem : I am supposed to find the gradient of $f^*$, in fact I am supposed to verify the identity: $$(\nabla f^*)(u)=(\nabla f)^{-1} (u) \tag{$\alpha$} $$ My approach : Let $u=(u_1, \dots , u_n) \in \mathbb{R}^n$ then it makes sense to only see if the statement in ($\alpha)$ holds for an arbitrary $i \in \lbrace 1, \dots , n \rbrace$. My problem is that I know nothing about how $(\nabla f)^{-1}$ looks like, so I thought the best attempt would be to just let it be in an abstract form. Let $h(u):=\langle (\nabla f)^{-1}(u),u)\rangle$ then I assume $h(u)$ looks like $$ h(u)=(\nabla f)_1^{-1}(u)*u_1 + \dots + (\nabla f)_i^{-1}(u)*u_i + \dots + (\nabla f)_n^{-1}(u)* u_n \\ \implies \frac{\partial h(u)}{u_i}=\frac{\partial(\nabla f)_1^{-1}(u)}{\partial u_i}u_1+ \dots + \frac{\partial(\nabla f)_i^{-1}(u)}{\partial u_i}u_i + (\nabla f)_i^{-1}(u)+ \dots + \frac{\partial (\nabla f)_n^{-1}(u)}{\partial u_i}u_n$$ If I have made no mistakes, all there is left to do is to check the second expression in (*) and hope that it annihilates the partially differentiated terms in my expression above. But all I get is: $$\frac{\partial f(( \nabla f^{-1})(u))}{\partial u_i}= \frac{\partial f}{\partial u_i}( \nabla f)^{-1}(u) \cdot \frac{\partial(\nabla f)_i^{-1}}{\partial u_i}(u)$$ So it would be nice if the first term would simplify to $u_i$ but I really don't see it. Any help or simplification would be appreciated.",,"['analysis', 'multivariable-calculus']"
30,Relation between parallel vector field along a geodesic and Jacobi field along that same geodesic,Relation between parallel vector field along a geodesic and Jacobi field along that same geodesic,,"Cross posted from my question: https://mathoverflow.net/questions/204097/parallel-transport-along-a-geodesic-and-the-related-jacobi-field This is a formula/theorem (written below) that I found mentioned in a medical imaging paper, without any proof or any further detail. I'd appreciate if anyone could please provide either the detail or the sketch to prove it. Let $c=\exp_p(tV)$ be a geodesic on a Riemannian manifold $M$. Let $W(t)$ be a parallel vector field $c$ with $W(0)=W$. Consider the Jacobi field $J(t)=(D\exp_p)_{tV}  ( {tW}  )$. Then: $\lim_{t\to 0}|\frac{W(t)-\frac{J(t)}{t}}{t}|=0.$ For constant curvature spaceforms, when I take normal Jacobi fields the formula holds good. But I'm not sure how to prove it for general manifolds. P.S. in the paper that I mentioned, the formula is mentioned somewhat heuristically, but this is my interpretation of their words. EDIT I: I can also see that if we take compare the norms of $W(t)=W$ and norms of $J(t)$, which is $|J(t)|=t|W|+O(t^3)$ (See Do Carmo, P. 115), then clearly, $lim_{t\to 0}\frac{|W(t|)-\frac{|J(t)|}{|t|}}{|t|}=0.$ But still it doesn't prove the result, but shows that the claim could be correct. EDIT II:I think I've got an idea, it's working so far, but not fully yet. I considered the function $f(t):=\langle tW(t)-J(t),tW(t)-J(t)\rangle =||tW(t)-J(t)||^2$ and showed so far that $f(0)=f'(0)=f''(0)=0$. So if I do some careful work on the fourth derivative $f''''(0)$ now, hopefully I'll have it!","Cross posted from my question: https://mathoverflow.net/questions/204097/parallel-transport-along-a-geodesic-and-the-related-jacobi-field This is a formula/theorem (written below) that I found mentioned in a medical imaging paper, without any proof or any further detail. I'd appreciate if anyone could please provide either the detail or the sketch to prove it. Let $c=\exp_p(tV)$ be a geodesic on a Riemannian manifold $M$. Let $W(t)$ be a parallel vector field $c$ with $W(0)=W$. Consider the Jacobi field $J(t)=(D\exp_p)_{tV}  ( {tW}  )$. Then: $\lim_{t\to 0}|\frac{W(t)-\frac{J(t)}{t}}{t}|=0.$ For constant curvature spaceforms, when I take normal Jacobi fields the formula holds good. But I'm not sure how to prove it for general manifolds. P.S. in the paper that I mentioned, the formula is mentioned somewhat heuristically, but this is my interpretation of their words. EDIT I: I can also see that if we take compare the norms of $W(t)=W$ and norms of $J(t)$, which is $|J(t)|=t|W|+O(t^3)$ (See Do Carmo, P. 115), then clearly, $lim_{t\to 0}\frac{|W(t|)-\frac{|J(t)|}{|t|}}{|t|}=0.$ But still it doesn't prove the result, but shows that the claim could be correct. EDIT II:I think I've got an idea, it's working so far, but not fully yet. I considered the function $f(t):=\langle tW(t)-J(t),tW(t)-J(t)\rangle =||tW(t)-J(t)||^2$ and showed so far that $f(0)=f'(0)=f''(0)=0$. So if I do some careful work on the fourth derivative $f''''(0)$ now, hopefully I'll have it!",,"['analysis', 'geometry', 'differential-geometry', 'riemannian-geometry']"
31,Upper and Lower Darboux integral of a piecewise function $f(x)=x$ and $f(x)=0$.,Upper and Lower Darboux integral of a piecewise function  and .,f(x)=x f(x)=0,"Let $0<a<b$. Find the upper and lower Darboux integrals for the function $$f(x)=x$$ if $x\in[a,b]\cap\mathbb{Q}$ and $$f(x)=0$$ if $x\in[a,b]-\mathbb{Q}$. I am so lost on this problem. Any hints or solutions are greatly appreciated.","Let $0<a<b$. Find the upper and lower Darboux integrals for the function $$f(x)=x$$ if $x\in[a,b]\cap\mathbb{Q}$ and $$f(x)=0$$ if $x\in[a,b]-\mathbb{Q}$. I am so lost on this problem. Any hints or solutions are greatly appreciated.",,"['calculus', 'real-analysis', 'analysis']"
32,Extracting Bernoulli polynomials from their generating function,Extracting Bernoulli polynomials from their generating function,,"The generating function for Bernoulli polynomials is $$ \frac{te^{tx}}{e^t-1} = \sum_{n=0}^\infty B_n(x) \frac{t^n}{n!}$$ The only way that I know of to get the coefficients out of this is to use Taylor's theorem. But to use Taylor's theorem, it needs to have derivatives at zero, and this doesn't, because there is always a $e^t-1$ in some denominator, which at zero is undefined. However, if instead of evaluating things at zero, I take the limit as $t \to 0$, then it looks like it works. As an example, for the second ""term"", $$ \lim_{t\to 0} \left(\frac{te^{tx}}{e^t-1} \right)^{(1)}\frac{t^0}{n!}   = \frac{x-1/2}{n!} $$ Which works, since $B_1(x) = x - 1/2$. My questions are, 1) Does this actually work, and if so, why? Are there other cases where we have formal power series which sum to something that doesn't have a Taylor series at zero? 2) Is there a better way to extract Bernoulli polynomials from that generating function (not using some other formula)?","The generating function for Bernoulli polynomials is $$ \frac{te^{tx}}{e^t-1} = \sum_{n=0}^\infty B_n(x) \frac{t^n}{n!}$$ The only way that I know of to get the coefficients out of this is to use Taylor's theorem. But to use Taylor's theorem, it needs to have derivatives at zero, and this doesn't, because there is always a $e^t-1$ in some denominator, which at zero is undefined. However, if instead of evaluating things at zero, I take the limit as $t \to 0$, then it looks like it works. As an example, for the second ""term"", $$ \lim_{t\to 0} \left(\frac{te^{tx}}{e^t-1} \right)^{(1)}\frac{t^0}{n!}   = \frac{x-1/2}{n!} $$ Which works, since $B_1(x) = x - 1/2$. My questions are, 1) Does this actually work, and if so, why? Are there other cases where we have formal power series which sum to something that doesn't have a Taylor series at zero? 2) Is there a better way to extract Bernoulli polynomials from that generating function (not using some other formula)?",,"['analysis', 'generating-functions', 'bernoulli-numbers', 'formal-power-series']"
33,Lower semicontinuous energy functional on compact space of Lipschitz functions,Lower semicontinuous energy functional on compact space of Lipschitz functions,,"Let $\Omega \subset \mathbb{R}^{n}$ be a bounded open subset containing $0$ and let $L>0$ be some positive constant. Consider the space  $A_{0}=\{f \in C^{\infty}(\overline{\Omega}) \mid f \text{ is L-Lipschitz}, f(0)=0\}$   and let $A$ be the closure of $A_{0}$ with respect to the $C^{0}-$norm. By Arzela-Ascoli $A$ is compact space consisting of $L$-Lipschitz functions which vanish at $0$. Define the functional $\mathcal{E} : A \rightarrow \mathbb{R}$ by $\mathcal{E}(f)=\int_{\Omega}|\nabla f|^{2}dx$. I have the following questions: How can I make sense of $\nabla f$ since for general $f\in A$, $f$ is not differentiable anymore? I thought using Rademacher theorem but I do not know exactly how. Is $\mathcal{E}$ continuous or semicontinuous? If its semicontinuous then is it lower or upper semi-continous. My feeling tells me that it is lower semi-continous. But I dont know how to prove it. Do you have any idea?","Let $\Omega \subset \mathbb{R}^{n}$ be a bounded open subset containing $0$ and let $L>0$ be some positive constant. Consider the space  $A_{0}=\{f \in C^{\infty}(\overline{\Omega}) \mid f \text{ is L-Lipschitz}, f(0)=0\}$   and let $A$ be the closure of $A_{0}$ with respect to the $C^{0}-$norm. By Arzela-Ascoli $A$ is compact space consisting of $L$-Lipschitz functions which vanish at $0$. Define the functional $\mathcal{E} : A \rightarrow \mathbb{R}$ by $\mathcal{E}(f)=\int_{\Omega}|\nabla f|^{2}dx$. I have the following questions: How can I make sense of $\nabla f$ since for general $f\in A$, $f$ is not differentiable anymore? I thought using Rademacher theorem but I do not know exactly how. Is $\mathcal{E}$ continuous or semicontinuous? If its semicontinuous then is it lower or upper semi-continous. My feeling tells me that it is lower semi-continous. But I dont know how to prove it. Do you have any idea?",,"['real-analysis', 'analysis', 'functional-analysis', 'calculus-of-variations']"
34,A real analysis proof question (related to sin(1/x)),A real analysis proof question (related to sin(1/x)),,"The problem statement is Let    $$f(x) = \left\{\begin{array}{cc}  x^4 \left(2 + \sin \frac 1 x\right) & x \ne 0 \\ 0 & x = 0 \end{array}\right. $$ (a)Prove that $f$ is differentiable on $\mathbb{R}$ (b)Prove that $f$ has an absolute minimum at $x=0$ (c) Prove that $f'$ takes both positive and negative values in every neighborhood of $0$. This first two parts of the problem are pretty straightforward. The only problem I encountered was in the last part. I was not sure how to prove it. I know both $\sin(1/x)$ and $\cos(1/x)$oscillate near zero. For any interval around zero they are gonna take positive and negative values since they oscillates. But this was not consider as a ""proof"". Is there a way to do that more rigorously? Can I use intermediate value property? BTW, $$f'(x) = 4x^3\left(2+\sin \frac 1 x\right)+x^2\cos \left(\frac 1 x\right)$$ when $x\neq 0$.","The problem statement is Let    $$f(x) = \left\{\begin{array}{cc}  x^4 \left(2 + \sin \frac 1 x\right) & x \ne 0 \\ 0 & x = 0 \end{array}\right. $$ (a)Prove that $f$ is differentiable on $\mathbb{R}$ (b)Prove that $f$ has an absolute minimum at $x=0$ (c) Prove that $f'$ takes both positive and negative values in every neighborhood of $0$. This first two parts of the problem are pretty straightforward. The only problem I encountered was in the last part. I was not sure how to prove it. I know both $\sin(1/x)$ and $\cos(1/x)$oscillate near zero. For any interval around zero they are gonna take positive and negative values since they oscillates. But this was not consider as a ""proof"". Is there a way to do that more rigorously? Can I use intermediate value property? BTW, $$f'(x) = 4x^3\left(2+\sin \frac 1 x\right)+x^2\cos \left(\frac 1 x\right)$$ when $x\neq 0$.",,"['real-analysis', 'analysis']"
35,How can we use Fubini's theorem to simplify $\int_0^r\frac 1{\sigma^{n-1}}\int_0^\sigma\rho^{n-1}f(\rho)\;d\rho\;d\sigma$?,How can we use Fubini's theorem to simplify ?,\int_0^r\frac 1{\sigma^{n-1}}\int_0^\sigma\rho^{n-1}f(\rho)\;d\rho\;d\sigma,"Let $f:[0,\infty)\to\mathbb{R}$ and $R>0$. How does Fubini's theorem imply $$\int_0^r\frac 1{\sigma^{n-1}}\int_0^\sigma\rho^{n-1}f(\rho)\;d\rho\;d\sigma\color{red}{=\int_0^r\frac{r^{2-n}-\rho^{2-n}}{2-n}\rho^{n-1}f(\rho)\;d\rho}\;\;\;\text{for }r\in (0,R)$$ (I don't understand why the left part is equal to the $\color{red}{\text{red}}$ part)? I know Fubini's theorem in the following version: Let $(\Omega_i,\mathcal{A}_i,\mu_i)$ be a $\sigma$-finite measure space $g:\Omega_1\times\Omega_2\to\overline{\mathbb{R}}$ be measurable with respect to $\mathcal{A}_1\otimes\mathcal{A}_2$ and nonnegative or $(\mu_1\otimes\mu_2)$-integrable Then, $$G_1:\Omega_1\to\mathbb{R}\;,\;\;\;\omega_1\mapsto\int g(\omega_1,\cdot)\;d\mu_2$$ and $$G_2:\Omega_2\to\mathbb{R}\;,\;\;\;\omega_2\mapsto\int g(\cdot,\omega_2)\;d\mu_1$$ are $\mathcal{A}_2$- and $\mathcal{A}_1$-measurable, respectively, and it holds $$\int g\;d(\mu_1\otimes\mu_2)=\int G_1\;d\mu_1=\int G_2\;d\mu_2$$ What is $g$ in the given scenario?","Let $f:[0,\infty)\to\mathbb{R}$ and $R>0$. How does Fubini's theorem imply $$\int_0^r\frac 1{\sigma^{n-1}}\int_0^\sigma\rho^{n-1}f(\rho)\;d\rho\;d\sigma\color{red}{=\int_0^r\frac{r^{2-n}-\rho^{2-n}}{2-n}\rho^{n-1}f(\rho)\;d\rho}\;\;\;\text{for }r\in (0,R)$$ (I don't understand why the left part is equal to the $\color{red}{\text{red}}$ part)? I know Fubini's theorem in the following version: Let $(\Omega_i,\mathcal{A}_i,\mu_i)$ be a $\sigma$-finite measure space $g:\Omega_1\times\Omega_2\to\overline{\mathbb{R}}$ be measurable with respect to $\mathcal{A}_1\otimes\mathcal{A}_2$ and nonnegative or $(\mu_1\otimes\mu_2)$-integrable Then, $$G_1:\Omega_1\to\mathbb{R}\;,\;\;\;\omega_1\mapsto\int g(\omega_1,\cdot)\;d\mu_2$$ and $$G_2:\Omega_2\to\mathbb{R}\;,\;\;\;\omega_2\mapsto\int g(\cdot,\omega_2)\;d\mu_1$$ are $\mathcal{A}_2$- and $\mathcal{A}_1$-measurable, respectively, and it holds $$\int g\;d(\mu_1\otimes\mu_2)=\int G_1\;d\mu_1=\int G_2\;d\mu_2$$ What is $g$ in the given scenario?",,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
36,"Calculate $\int_{C(0,2)^+} \frac{z^3}{z^5 - 1} dz$",Calculate,"\int_{C(0,2)^+} \frac{z^3}{z^5 - 1} dz","How do I calculate this integral $\int_{C(0,2)^+} \frac{z^3}{z^5 - 1} dz$? What I have done so far is the following: set $z(t) = 2e^{it}$ with $t \in [0,2\pi]$ so we get. $$ \int_{C(0,2)^+} \frac{z^3}{z^5 - 1} dz = \int_{0}^{2\pi} \frac{(2e^{it})^3}{(2e^{it})^5 - 1}2ie^{it} dt = i\int_0^{2\pi} \frac{16e^{4it}}{32e^{5it} - 1}dt. $$ Now setting $u(t) = 32e^{5it} + 1$ we get $du = 160ie^{5it}dt$. How should I continue?","How do I calculate this integral $\int_{C(0,2)^+} \frac{z^3}{z^5 - 1} dz$? What I have done so far is the following: set $z(t) = 2e^{it}$ with $t \in [0,2\pi]$ so we get. $$ \int_{C(0,2)^+} \frac{z^3}{z^5 - 1} dz = \int_{0}^{2\pi} \frac{(2e^{it})^3}{(2e^{it})^5 - 1}2ie^{it} dt = i\int_0^{2\pi} \frac{16e^{4it}}{32e^{5it} - 1}dt. $$ Now setting $u(t) = 32e^{5it} + 1$ we get $du = 160ie^{5it}dt$. How should I continue?",,"['integration', 'analysis']"
37,$\|fg\|_{A (\mathbb T)} \leq C \|f\|_{L^{2}} \|g\|_{A (\mathbb T)}$?,?,\|fg\|_{A (\mathbb T)} \leq C \|f\|_{L^{2}} \|g\|_{A (\mathbb T)},"Let $f\in L^{1}(\mathbb T)$ and define the Fourier coefficient of $f$ : $\hat{f}(n)=\frac{1}{2\pi} \int _{-\pi}^{\pi} f(t) e^{-int} dt; (n\in \mathbb Z)$.Consider the space,  $$A(\mathbb T):= \{f\in L^{1}(\mathbb T): \hat{f}\in \ell^{1}(\mathbb Z), \  \text {that is,} \  \sum_{n\in \mathbb Z} |\hat{f}(n)| < \infty \}.$$ $A(\mathbb T)$ is normed by the $L^{1}-$ norm on $\mathbb Z$: $$||f||_{A(\mathbb T)}= \sum_{n\in \mathbb Z} |\hat{f}(n)| < \infty; \ \text {for} \ f\in A(\mathbb T). $$ We also note that $A(\mathbb T)$ is a Banach algebra under pointwise addition and multiplication. My Question : Can we expect $\|fg\|_{A (\mathbb T)} \leq C \|f\|_{L^{2}} \|g\|_{A (\mathbb T)},$ ( where $C$ is some constant) for $f,g \in A(\mathbb T)$? If yes, how to prove it? [We recall, $\|f\|^{2}_{L^{2}}= \int_{0}^{2\pi}|f(t)|^{2} dt= \sum_{n\in \mathbb Z}|\hat{f}(n)|^{2}$]","Let $f\in L^{1}(\mathbb T)$ and define the Fourier coefficient of $f$ : $\hat{f}(n)=\frac{1}{2\pi} \int _{-\pi}^{\pi} f(t) e^{-int} dt; (n\in \mathbb Z)$.Consider the space,  $$A(\mathbb T):= \{f\in L^{1}(\mathbb T): \hat{f}\in \ell^{1}(\mathbb Z), \  \text {that is,} \  \sum_{n\in \mathbb Z} |\hat{f}(n)| < \infty \}.$$ $A(\mathbb T)$ is normed by the $L^{1}-$ norm on $\mathbb Z$: $$||f||_{A(\mathbb T)}= \sum_{n\in \mathbb Z} |\hat{f}(n)| < \infty; \ \text {for} \ f\in A(\mathbb T). $$ We also note that $A(\mathbb T)$ is a Banach algebra under pointwise addition and multiplication. My Question : Can we expect $\|fg\|_{A (\mathbb T)} \leq C \|f\|_{L^{2}} \|g\|_{A (\mathbb T)},$ ( where $C$ is some constant) for $f,g \in A(\mathbb T)$? If yes, how to prove it? [We recall, $\|f\|^{2}_{L^{2}}= \int_{0}^{2\pi}|f(t)|^{2} dt= \sum_{n\in \mathbb Z}|\hat{f}(n)|^{2}$]",,"['analysis', 'fourier-analysis', 'fourier-series']"
38,Equivalent metrics define the same topology proof.,Equivalent metrics define the same topology proof.,,"Let $X$ be a set and $d_{1},d_{2}$ be two metrics on $X$. Define a metric to be equivalent if convergence of a sequence in one metric implies the convergence in the other. I am having difficulty understanding a particular line of one version of the proof. The overall strategy is to show that if $U \subset X$ is open with respect to $d_{1}$, that through a contradiction, show that there must be a $d_{2}$ ball contained by $U$. The proof begins by defining $u\in U$ and supposing $U$ is open in $d_{1}$. Suppose there is no ball of radius $r$ in the $d_{2}$ metric that is contained in $U$. Therefore, for every natural number $n$ there is a $x_{n}\in X\setminus U$ such that $d_{2}(x_{n},u) < 1/n$ and hence $x_{n}$ converges to $u$ in $d_{2}$. I get that $d_{2}(x_{n},u) < 1/n$ implies convergence in $d_{2}$. I don't understand why there is necessarily a sequence in $x_{n}\in X\setminus U$ that has this property or why it is worded in this particular manner. If this is a standard Euclidean $n$-ball over $\mathbb{R}^{2}$, and I naively draw a diagram I can accept this, but for an arbitrary set and two arbitrary metrics, I am having trouble convincing myself.","Let $X$ be a set and $d_{1},d_{2}$ be two metrics on $X$. Define a metric to be equivalent if convergence of a sequence in one metric implies the convergence in the other. I am having difficulty understanding a particular line of one version of the proof. The overall strategy is to show that if $U \subset X$ is open with respect to $d_{1}$, that through a contradiction, show that there must be a $d_{2}$ ball contained by $U$. The proof begins by defining $u\in U$ and supposing $U$ is open in $d_{1}$. Suppose there is no ball of radius $r$ in the $d_{2}$ metric that is contained in $U$. Therefore, for every natural number $n$ there is a $x_{n}\in X\setminus U$ such that $d_{2}(x_{n},u) < 1/n$ and hence $x_{n}$ converges to $u$ in $d_{2}$. I get that $d_{2}(x_{n},u) < 1/n$ implies convergence in $d_{2}$. I don't understand why there is necessarily a sequence in $x_{n}\in X\setminus U$ that has this property or why it is worded in this particular manner. If this is a standard Euclidean $n$-ball over $\mathbb{R}^{2}$, and I naively draw a diagram I can accept this, but for an arbitrary set and two arbitrary metrics, I am having trouble convincing myself.",,"['general-topology', 'analysis']"
39,Curve in a product of tori,Curve in a product of tori,,"Consider the curve $\gamma:\mathbb R\to (\mathbb R/\mathbb Z)^n$ given by $$\gamma(t)=(a_1t,\ldots,a_nt)$$ for generic real numbers $a_1,\ldots,a_n$. Is the image of $\gamma$ dense in $(\mathbb R/\mathbb Z)^n$?","Consider the curve $\gamma:\mathbb R\to (\mathbb R/\mathbb Z)^n$ given by $$\gamma(t)=(a_1t,\ldots,a_nt)$$ for generic real numbers $a_1,\ldots,a_n$. Is the image of $\gamma$ dense in $(\mathbb R/\mathbb Z)^n$?",,"['calculus', 'real-analysis', 'general-topology', 'analysis', 'differential-geometry']"
40,Monotonicity of $\alpha$ in Riemann Stieltjes integral $\int fd\alpha$,Monotonicity of  in Riemann Stieltjes integral,\alpha \int fd\alpha,"I am recently reading rudin's Principles of Mathematical Analysis, and I am wondering why the monotonicity of $\alpha$ in Riemann Stieltjes integral  $\int fd\alpha$ is always emphasized. For example, if I want to integrate $\int^{a}_{b}{dx^2}$ (here $f=1$ and $\alpha = x^2$), it equals to $\int^{a}_{b}2xdx$ where $\alpha =x^2$ is not monotone. However, in theorem 6.17, it assumes that $\alpha$ is nondecreasing. In most theorem and examples I have encountered, the monotonacity of $\alpha$ is unnecessary. So is there any example to demonstrate the importance of that? Here are some theorem stated in Rudin's book: Theorem 6.17 Assume $\alpha(x)$ increases monotonically and $\alpha'$ is integrable (with respect to $x$) on $[a,b]$. Let $f$ be a bounded real function on $[a,b]$. Then $f$ is $\alpha$-integrable (that is, $\int fd\alpha$ exists) if and only if $f\alpha$ is integrable with respect to $x$. In this case: $$\int^b_afd\alpha=\int^b_af\alpha'dx$$ Theorem 6.18 Suppose $\psi$ is a strictly increasing continuous function that maps an interval $[A,B]$ onto $[a,b]$. Suppose $\alpha$ is monotonically increasing on $[a,b]$ and $f$ is $\alpha$-integrable on $[a,b]$. Define $\beta$ and $g$ on $[A,B]$ by $$\beta(y)=\alpha(\psi(y))$$$$g(y)=f(\psi(y))$$ Then $g$ is $\beta$-integrable and $$\int^B_Agd\beta=\int^b_afd\alpha$$","I am recently reading rudin's Principles of Mathematical Analysis, and I am wondering why the monotonicity of $\alpha$ in Riemann Stieltjes integral  $\int fd\alpha$ is always emphasized. For example, if I want to integrate $\int^{a}_{b}{dx^2}$ (here $f=1$ and $\alpha = x^2$), it equals to $\int^{a}_{b}2xdx$ where $\alpha =x^2$ is not monotone. However, in theorem 6.17, it assumes that $\alpha$ is nondecreasing. In most theorem and examples I have encountered, the monotonacity of $\alpha$ is unnecessary. So is there any example to demonstrate the importance of that? Here are some theorem stated in Rudin's book: Theorem 6.17 Assume $\alpha(x)$ increases monotonically and $\alpha'$ is integrable (with respect to $x$) on $[a,b]$. Let $f$ be a bounded real function on $[a,b]$. Then $f$ is $\alpha$-integrable (that is, $\int fd\alpha$ exists) if and only if $f\alpha$ is integrable with respect to $x$. In this case: $$\int^b_afd\alpha=\int^b_af\alpha'dx$$ Theorem 6.18 Suppose $\psi$ is a strictly increasing continuous function that maps an interval $[A,B]$ onto $[a,b]$. Suppose $\alpha$ is monotonically increasing on $[a,b]$ and $f$ is $\alpha$-integrable on $[a,b]$. Define $\beta$ and $g$ on $[A,B]$ by $$\beta(y)=\alpha(\psi(y))$$$$g(y)=f(\psi(y))$$ Then $g$ is $\beta$-integrable and $$\int^B_Agd\beta=\int^b_afd\alpha$$",,"['integration', 'analysis']"
41,$\text{Prove }\prod_{i=1}^\infty(1+a_i) \text{ converges } \iff \sum_{n=1}^\infty a_n \text{ converges}$,,\text{Prove }\prod_{i=1}^\infty(1+a_i) \text{ converges } \iff \sum_{n=1}^\infty a_n \text{ converges},Let $a_i \ge 0$ $$\text{Prove }\prod_{i=1}^\infty(1+a_i) \text{ converges } \iff \sum_{n=1}^\infty a_n \text{ converges}$$ I've got to this step $$\prod_{i=1}^\infty (1+a_i) = e^{\sum_{i=1}^\infty \ln(1+a_i)}$$ but I'm not sure how to proceed,Let $a_i \ge 0$ $$\text{Prove }\prod_{i=1}^\infty(1+a_i) \text{ converges } \iff \sum_{n=1}^\infty a_n \text{ converges}$$ I've got to this step $$\prod_{i=1}^\infty (1+a_i) = e^{\sum_{i=1}^\infty \ln(1+a_i)}$$ but I'm not sure how to proceed,,"['analysis', 'convergence-divergence', 'infinite-product']"
42,Total variation of a cadlag function,Total variation of a cadlag function,,"Let $f: [0, +\infty) \rightarrow \mathbb{R}$ be a cadlag function with $\|f \|_{t, \text{var}} < +\infty$, where $\|f \|_{t, \text{var}} < +\infty$ denotes the total variation of $f$ over the interval $[0,t]$. There is an exercise asking us to prove that the function $g: [0, \infty) \rightarrow \mathbb{R}; \, t \mapsto \|f \|_{t, \text{var}}$ is also cadlag. (However, I have only seen the proof concerning right-continuity.) Also, how can we compute the jump of the total variation function? (i.e. express $g(t) - g(t-) $ in terms of $f$)","Let $f: [0, +\infty) \rightarrow \mathbb{R}$ be a cadlag function with $\|f \|_{t, \text{var}} < +\infty$, where $\|f \|_{t, \text{var}} < +\infty$ denotes the total variation of $f$ over the interval $[0,t]$. There is an exercise asking us to prove that the function $g: [0, \infty) \rightarrow \mathbb{R}; \, t \mapsto \|f \|_{t, \text{var}}$ is also cadlag. (However, I have only seen the proof concerning right-continuity.) Also, how can we compute the jump of the total variation function? (i.e. express $g(t) - g(t-) $ in terms of $f$)",,"['real-analysis', 'analysis']"
43,Baby Rudin Chapter 3 Problem 11(d),Baby Rudin Chapter 3 Problem 11(d),,"Suppose that $a_n > 0$ for all $n \in \mathbb{N}$ and that $\sum_{n=1}^\infty a_n = +\infty$. Let $b_n \colon= {a_n \over {1+na_n}}$ for all $n \in \mathbb{N}$. Then we can show the following fact: If the sequence $\{na_n\}_{n\in \mathbb{N}}$ is bounded above or if this sequence has a positive lower bound (of course this sequence is bounded below by $0$), then $\sum_{n=1}^\infty b_n = +\infty$. However, can we come up with a case where neither of the above two hypotheses about $\{na_n\}_{n\in \mathbb{N}}$ holds (although $\sum_{n=1}^\infty  a_n = +\infty$ still holds), but nevertheless $\sum_{n=1}^\infty b_n = +\infty$?","Suppose that $a_n > 0$ for all $n \in \mathbb{N}$ and that $\sum_{n=1}^\infty a_n = +\infty$. Let $b_n \colon= {a_n \over {1+na_n}}$ for all $n \in \mathbb{N}$. Then we can show the following fact: If the sequence $\{na_n\}_{n\in \mathbb{N}}$ is bounded above or if this sequence has a positive lower bound (of course this sequence is bounded below by $0$), then $\sum_{n=1}^\infty b_n = +\infty$. However, can we come up with a case where neither of the above two hypotheses about $\{na_n\}_{n\in \mathbb{N}}$ holds (although $\sum_{n=1}^\infty  a_n = +\infty$ still holds), but nevertheless $\sum_{n=1}^\infty b_n = +\infty$?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'divergent-series']"
44,Function after d in integral,Function after d in integral,,"How do you interpret this? $$\int[y-f(x, c)] \text{d}g(x,y)$$ I only saw things such as $\text{d}x\text{d}y$? But here a function ($g$) is after $\text{d}$. c is a constant.","How do you interpret this? $$\int[y-f(x, c)] \text{d}g(x,y)$$ I only saw things such as $\text{d}x\text{d}y$? But here a function ($g$) is after $\text{d}$. c is a constant.",,"['calculus', 'integration', 'analysis', 'multivariable-calculus']"
45,Sketch the proof of $e^{-x^2}$ being uniformly continuous (proof is given),Sketch the proof of  being uniformly continuous (proof is given),e^{-x^2},"I am asked to sketch the proof seen below in a graph. We state that the function $f: \mathbb{R} \to \mathbb{R}$ given by $f(x)=e^{-x^2}$ Is uniformly continuous Proof: \begin{eqnarray*} |f(x)|<\epsilon/2&\Longleftrightarrow& e^{-x^2}<\epsilon/2\\&\Longleftrightarrow& \log\left(e^{-x^2}\right)<\log (\epsilon/ 2)\\&\Longleftrightarrow& -x^2<\log \epsilon-\log 2\\&\Longleftrightarrow& x^2>\log 2-\log \epsilon\\&\Longleftrightarrow& |x|>\sqrt{\log 2-\log \epsilon} \end{eqnarray*} By setting $K=\sqrt{\log 2-\log \epsilon}$ we get  $|x|,|y|>K\Longrightarrow |f(x)-f(y)|\leq |f(x)|+|f(y)|<\frac\epsilon2+\frac\epsilon2=\epsilon.$  (1) We can now use our third theoreom ($F: A->\mathbb{R}$ on the set $A \subset \mathbb{R}^{k}$ is uniformly continuous if  $\forall\epsilon>0 \exists\delta>0: ||f(y)-f(x)||<\epsilon$ for all $x,y\in$A with $||y-x||<\delta$ on f in the interval $[-K-1; K+1]$ and conclude that f is uniformly continuous on this interval. We can therefore chose a $\delta'>0$ with the proberty $|f(x)-f(y)|<\epsilon,$ when $|x-y|<\delta'$ and $x,y\in I$  (2) By setting $\delta=min({\delta',1})$ we can parry off (not sure if this is the right term at all...) the chosen $\epsilon$ for all $x,y\in \mathbb{R}$. Set $|x-y|<\delta$ - if both x,y $\in$ I, then (2) says that $|f(x)-f(y)|<\epsilon$. If $x\notin I$ we have $x>K+1$ or $x<-K-1$. If $x>K-1$ we get that $y>K$ because of $|x-y|<1$ whereas (1) gives that $|f(x)-f(y)|<\epsilon$. And if $x<-K-1$ we have that $y<-K$, which gives us that (1) is $|f(x)-f(y)|<\epsilon$ I am having trouble understanding the proof completely - I got the main idea, that something is uniformly continuous if the function behaves nicely. However, sketching this is giving me loads of troubles. Can anyone help out? We have to sketch it with the values $\epsilon,\delta,K$ on the figgure.","I am asked to sketch the proof seen below in a graph. We state that the function $f: \mathbb{R} \to \mathbb{R}$ given by $f(x)=e^{-x^2}$ Is uniformly continuous Proof: \begin{eqnarray*} |f(x)|<\epsilon/2&\Longleftrightarrow& e^{-x^2}<\epsilon/2\\&\Longleftrightarrow& \log\left(e^{-x^2}\right)<\log (\epsilon/ 2)\\&\Longleftrightarrow& -x^2<\log \epsilon-\log 2\\&\Longleftrightarrow& x^2>\log 2-\log \epsilon\\&\Longleftrightarrow& |x|>\sqrt{\log 2-\log \epsilon} \end{eqnarray*} By setting $K=\sqrt{\log 2-\log \epsilon}$ we get  $|x|,|y|>K\Longrightarrow |f(x)-f(y)|\leq |f(x)|+|f(y)|<\frac\epsilon2+\frac\epsilon2=\epsilon.$  (1) We can now use our third theoreom ($F: A->\mathbb{R}$ on the set $A \subset \mathbb{R}^{k}$ is uniformly continuous if  $\forall\epsilon>0 \exists\delta>0: ||f(y)-f(x)||<\epsilon$ for all $x,y\in$A with $||y-x||<\delta$ on f in the interval $[-K-1; K+1]$ and conclude that f is uniformly continuous on this interval. We can therefore chose a $\delta'>0$ with the proberty $|f(x)-f(y)|<\epsilon,$ when $|x-y|<\delta'$ and $x,y\in I$  (2) By setting $\delta=min({\delta',1})$ we can parry off (not sure if this is the right term at all...) the chosen $\epsilon$ for all $x,y\in \mathbb{R}$. Set $|x-y|<\delta$ - if both x,y $\in$ I, then (2) says that $|f(x)-f(y)|<\epsilon$. If $x\notin I$ we have $x>K+1$ or $x<-K-1$. If $x>K-1$ we get that $y>K$ because of $|x-y|<1$ whereas (1) gives that $|f(x)-f(y)|<\epsilon$. And if $x<-K-1$ we have that $y<-K$, which gives us that (1) is $|f(x)-f(y)|<\epsilon$ I am having trouble understanding the proof completely - I got the main idea, that something is uniformly continuous if the function behaves nicely. However, sketching this is giving me loads of troubles. Can anyone help out? We have to sketch it with the values $\epsilon,\delta,K$ on the figgure.",,"['analysis', 'exponential-function', 'uniform-continuity']"
46,How to prove the 2nd & 3rd conditions of outer measure?,How to prove the 2nd & 3rd conditions of outer measure?,,"I have this question on outer measure from Richard Bass' book: Prove that $\mu^*$ is an outer measure, given a measure space $(X, \mathcal A, \mu)$ and define $$\mu^*(A) = \inf \{\mu(B) \mid A \subset B, B \in \mathcal A\}$$ for all subsets $A$ of $X$. Here are what I have gone so far: (1) The first condition is the easiest one:  $$\begin{align} \mu^*(\emptyset) &= \inf \{\mu(B) \mid \emptyset \subset B, B \in \mathcal A\}\\ &= \mu (\emptyset) \\ &= 0 \end{align}$$ (2) Now the second condition. Let $D, E \in X$ and $D \subset E$, $$\begin{align} \mu^*(D) &= \inf \{\mu(D') \mid D \subset D', D' \in \mathcal A\}\\ \mu^*(E) &= \inf \{\mu(E') \mid E \subset E', E' \in \mathcal A\}\\ \end{align}$$ Here I need to prove $\mu^* (D) \leq \mu^*(E)$. It looks to me so intuitive especially if I draw Venn diagrams of $D, E, D'$ and $E'$, but I don't know how to say it in math-speak. I would appreciate helps on this 2nd. condition. (3) And this 3rd. condition is my major stumbling block: Given $(A_i)_{i \in \mathbb N} \subset X$, I need to arrive at $$\mu^* (\bigcup _{i=1}^{\infty} A_i)\leq \sum_{i=1}^{\infty} \mu^* (A_i).$$ Here, I know for sure I need to  state this first: $\forall A_i, \exists B_i $ such that $ A_i \subset B_i, B_i \in \mathcal A$, but I don't think the next step is right: $$\begin{align} \mu^*(\bigcup_{i=1}^{\infty}A_i) &= \inf \{\bigcup_{i=1}^{\infty}\mu(B_i) \mid A_i \subset B_i, B_i \in \mathcal A\}\\ &= \ldots\\ \end{align}$$ I would appreciate any help on this 3rd. condition in addition to the 2nd. above. Thank you for your time and effort. POST SCRIPT: After I posted this question, I found this proposition on the same text, perhaps this proposition holds key to the solution, in that I don't have to prove the 2nd and 3rd conditions? Thanks again. Proposition : Suppose $\mathcal C$ is a collection of subsets of $X$ such that $\emptyset$ and $X$ are both in $\mathcal C$. Suppose $\mathscr l : \mathcal C \to [0, \infty]$ with $\mathscr l (\emptyset) = 0$. Define $$\mu^* (E) = \inf \{ \sum_{i=1}^{\infty} \mathscr l (A_i) \mid A_i \in \mathcal C \text{ for each } i, \text{and }  E \subset \cup_{i=1}^{\infty} A_i \}.$$   Then $\mu^*$ is an outer measure.","I have this question on outer measure from Richard Bass' book: Prove that $\mu^*$ is an outer measure, given a measure space $(X, \mathcal A, \mu)$ and define $$\mu^*(A) = \inf \{\mu(B) \mid A \subset B, B \in \mathcal A\}$$ for all subsets $A$ of $X$. Here are what I have gone so far: (1) The first condition is the easiest one:  $$\begin{align} \mu^*(\emptyset) &= \inf \{\mu(B) \mid \emptyset \subset B, B \in \mathcal A\}\\ &= \mu (\emptyset) \\ &= 0 \end{align}$$ (2) Now the second condition. Let $D, E \in X$ and $D \subset E$, $$\begin{align} \mu^*(D) &= \inf \{\mu(D') \mid D \subset D', D' \in \mathcal A\}\\ \mu^*(E) &= \inf \{\mu(E') \mid E \subset E', E' \in \mathcal A\}\\ \end{align}$$ Here I need to prove $\mu^* (D) \leq \mu^*(E)$. It looks to me so intuitive especially if I draw Venn diagrams of $D, E, D'$ and $E'$, but I don't know how to say it in math-speak. I would appreciate helps on this 2nd. condition. (3) And this 3rd. condition is my major stumbling block: Given $(A_i)_{i \in \mathbb N} \subset X$, I need to arrive at $$\mu^* (\bigcup _{i=1}^{\infty} A_i)\leq \sum_{i=1}^{\infty} \mu^* (A_i).$$ Here, I know for sure I need to  state this first: $\forall A_i, \exists B_i $ such that $ A_i \subset B_i, B_i \in \mathcal A$, but I don't think the next step is right: $$\begin{align} \mu^*(\bigcup_{i=1}^{\infty}A_i) &= \inf \{\bigcup_{i=1}^{\infty}\mu(B_i) \mid A_i \subset B_i, B_i \in \mathcal A\}\\ &= \ldots\\ \end{align}$$ I would appreciate any help on this 3rd. condition in addition to the 2nd. above. Thank you for your time and effort. POST SCRIPT: After I posted this question, I found this proposition on the same text, perhaps this proposition holds key to the solution, in that I don't have to prove the 2nd and 3rd conditions? Thanks again. Proposition : Suppose $\mathcal C$ is a collection of subsets of $X$ such that $\emptyset$ and $X$ are both in $\mathcal C$. Suppose $\mathscr l : \mathcal C \to [0, \infty]$ with $\mathscr l (\emptyset) = 0$. Define $$\mu^* (E) = \inf \{ \sum_{i=1}^{\infty} \mathscr l (A_i) \mid A_i \in \mathcal C \text{ for each } i, \text{and }  E \subset \cup_{i=1}^{\infty} A_i \}.$$   Then $\mu^*$ is an outer measure.",,"['real-analysis', 'analysis', 'measure-theory']"
47,For which complex numbers does a given series converge,For which complex numbers does a given series converge,,"During studying for an exam, I came across the following exercise: For which $z \in \mathbb{C}$ does the series $$\sum\limits_{n=1}^{\infty} \frac{n^n z^n}{n!}$$ converge? By using the ratio test (or alternatively: calculating the radius of convergence), I can see that the series converges for $|z|<\frac{1}{e}$ and diverges for $|z|>\frac{1}{e}$. However, I am not able to find out what happens if $|z|=\frac{1}{e}$. Does anyone have an idea?","During studying for an exam, I came across the following exercise: For which $z \in \mathbb{C}$ does the series $$\sum\limits_{n=1}^{\infty} \frac{n^n z^n}{n!}$$ converge? By using the ratio test (or alternatively: calculating the radius of convergence), I can see that the series converges for $|z|<\frac{1}{e}$ and diverges for $|z|>\frac{1}{e}$. However, I am not able to find out what happens if $|z|=\frac{1}{e}$. Does anyone have an idea?",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
48,"How prove this inequality $[f(x)]^2+[f'(x)]^2\le \max{(A,B)}$ [duplicate]",How prove this inequality  [duplicate],"[f(x)]^2+[f'(x)]^2\le \max{(A,B)}","This question already has an answer here : Prove that: $(f(x))^{2} + (f'(x))^{2} \leqslant \max(a,b)$ where $(a,b) \in \mathbb{R}^2$ (1 answer) Closed 3 years ago . let $f(x)$ be two derivable on $R$,give the two postive  numbers $A,B$ and such   $$[f(x)]^2\le A$$   $$[f'(x)]^2+[f''(x)]^2\le B$$   show that   $$[f(x)]^2+[f'(x)]^2\le \max{(A,B)},\forall x\in R$$ I think maybe well know inequality：  $$|f'(x)|\le2\sqrt{|f''(x)||f(x)|}\le |f''(x)|+|f(x)|$$ $$\Longrightarrow [f'(x)]^2\le (|f''(x)|+|f(x)|)^2\le\dfrac{1}{2}([f''(x)]^2+[f(x)]^2)$$ if $A\ge B$ then I can't $$[f(x)]^2+[f'(x)]^2\le A$$ Maybe this idea can't usefulll?","This question already has an answer here : Prove that: $(f(x))^{2} + (f'(x))^{2} \leqslant \max(a,b)$ where $(a,b) \in \mathbb{R}^2$ (1 answer) Closed 3 years ago . let $f(x)$ be two derivable on $R$,give the two postive  numbers $A,B$ and such   $$[f(x)]^2\le A$$   $$[f'(x)]^2+[f''(x)]^2\le B$$   show that   $$[f(x)]^2+[f'(x)]^2\le \max{(A,B)},\forall x\in R$$ I think maybe well know inequality：  $$|f'(x)|\le2\sqrt{|f''(x)||f(x)|}\le |f''(x)|+|f(x)|$$ $$\Longrightarrow [f'(x)]^2\le (|f''(x)|+|f(x)|)^2\le\dfrac{1}{2}([f''(x)]^2+[f(x)]^2)$$ if $A\ge B$ then I can't $$[f(x)]^2+[f'(x)]^2\le A$$ Maybe this idea can't usefulll?",,['analysis']
49,Showing irrationality of $\zeta(k)$ for some $k$ without calculating the value.,Showing irrationality of  for some  without calculating the value.,\zeta(k) k,"For $s\in (1,\infty)$ let $\zeta(s):=\sum_{n=1}^\infty \dfrac 1{n^s}$. Is there a way to show that $\zeta(2k)$ is irrational for some integer $k\geq 1$ without finding explicit formulae?","For $s\in (1,\infty)$ let $\zeta(s):=\sum_{n=1}^\infty \dfrac 1{n^s}$. Is there a way to show that $\zeta(2k)$ is irrational for some integer $k\geq 1$ without finding explicit formulae?",,"['sequences-and-series', 'analysis']"
50,Is sinus an unique function?,Is sinus an unique function?,,"On $\mathbb{R}$, is sinus (the sine function) the unique $C^{\infty}$ function $f$ with all its derivatives and itself  between $-1$ and $1$ and also $ \frac{df}{dx}(0)=1 $ ?","On $\mathbb{R}$, is sinus (the sine function) the unique $C^{\infty}$ function $f$ with all its derivatives and itself  between $-1$ and $1$ and also $ \frac{df}{dx}(0)=1 $ ?",,"['real-analysis', 'analysis']"
51,"Breaking a Function in $L^{\infty}[0,1]$",Breaking a Function in,"L^{\infty}[0,1]","Let $f\in L^{\infty}[0,1]$ s.t. $\|f\|_{\infty}=1$ $E:=\{x\in[0,1]:|f(x)|<1\}$ If $m(E)>0$, then is it possible to find $g,h\in L^{\infty}[0,1]$ such that $\|g\|_{\infty},\|h\|_{\infty}=1$, $f=\frac{g+h}{2}$ and $f\neq g\neq h$? Thoughts: In $L^1[0,1]$, if $\|f\|_1=1$, then it is possible to break $f$ like wanted. One considers $$ \phi(x):=\int_0^x|f(x)|\,dx, $$ which is continuous on $[0,1]$ and such that $\phi(0)=0$ and $\phi(1)=1$. By the IVT there exists $x_0\in(0,1)$ such that $\phi(x_0)=\frac{1}{2}$. Then $g:=2f1_{[0,x_0]}$ and $h:=2f1_{[x_0,1]}$ work. In the question we have no integral since $\|f\|_{\infty}$ is the essential supremum. I've been trying to obtain a measurable set $A$ for which $2f1_{A}$ and $2f1_{[0,1]\backslash A}$ would work but I failed.","Let $f\in L^{\infty}[0,1]$ s.t. $\|f\|_{\infty}=1$ $E:=\{x\in[0,1]:|f(x)|<1\}$ If $m(E)>0$, then is it possible to find $g,h\in L^{\infty}[0,1]$ such that $\|g\|_{\infty},\|h\|_{\infty}=1$, $f=\frac{g+h}{2}$ and $f\neq g\neq h$? Thoughts: In $L^1[0,1]$, if $\|f\|_1=1$, then it is possible to break $f$ like wanted. One considers $$ \phi(x):=\int_0^x|f(x)|\,dx, $$ which is continuous on $[0,1]$ and such that $\phi(0)=0$ and $\phi(1)=1$. By the IVT there exists $x_0\in(0,1)$ such that $\phi(x_0)=\frac{1}{2}$. Then $g:=2f1_{[0,x_0]}$ and $h:=2f1_{[x_0,1]}$ work. In the question we have no integral since $\|f\|_{\infty}$ is the essential supremum. I've been trying to obtain a measurable set $A$ for which $2f1_{A}$ and $2f1_{[0,1]\backslash A}$ would work but I failed.",,"['real-analysis', 'analysis', 'functional-analysis', 'measure-theory']"
52,Proof of an inverse,Proof of an inverse,,If $F(x)$ = $\int\limits_1^x$ $\frac{1}{t}$ $dt$ then the function $F$ has an inverse. I know that $F(x)$ = $\int\limits_1^x \frac{1}{t} = ln x$ but I can't assume that it is $lnx$. So I really don't know how to begin this proof and direction on it would be nice.,If $F(x)$ = $\int\limits_1^x$ $\frac{1}{t}$ $dt$ then the function $F$ has an inverse. I know that $F(x)$ = $\int\limits_1^x \frac{1}{t} = ln x$ but I can't assume that it is $lnx$. So I really don't know how to begin this proof and direction on it would be nice.,,"['real-analysis', 'analysis']"
53,"If $U(f,P) = L(f,P)$, show that $f$ is constant.","If , show that  is constant.","U(f,P) = L(f,P) f","The question has two part, Show that if f : [a, b] → R is continuous and there exists a partition P of [a,b] such that U(f,P) = L(f,P), then f is constant. Is this true if we drop the assumption that f is continuous? I've already finished the proof of first question by using the Intermediate Value Theorem.(since the sup and inf of a continuous function are equal on each subinterval, then the function is constant on each subinterval. With the assumption that f is continuous, then we can conclude that f is constant in its domain.) As for the second question, it seems to be false since if f is not continuous, the Mean Value theorem cannot be applied. However, I cannot think of a counter example or proof for this. Could anyone give a hint?","The question has two part, Show that if f : [a, b] → R is continuous and there exists a partition P of [a,b] such that U(f,P) = L(f,P), then f is constant. Is this true if we drop the assumption that f is continuous? I've already finished the proof of first question by using the Intermediate Value Theorem.(since the sup and inf of a continuous function are equal on each subinterval, then the function is constant on each subinterval. With the assumption that f is continuous, then we can conclude that f is constant in its domain.) As for the second question, it seems to be false since if f is not continuous, the Mean Value theorem cannot be applied. However, I cannot think of a counter example or proof for this. Could anyone give a hint?",,"['integration', 'analysis', 'continuity']"
54,"Proof about outer measure. For an interval $I$, $|I|_e=v(I)$?","Proof about outer measure. For an interval , ?",I |I|_e=v(I),"My question is when proving $|I|_e \ge v(I)$, why cannot I conclude from $S={I_k}_{k=1}^\infty$ is a cover of $I$, then $v(I)\le \sigma(S)$, so $v(I)\le inf \sigma(S)=|I|_e$? Why do we need the $I_k^*$? Can anyone help? Thanks so much! The following is the definition of $\sigma (S)$.","My question is when proving $|I|_e \ge v(I)$, why cannot I conclude from $S={I_k}_{k=1}^\infty$ is a cover of $I$, then $v(I)\le \sigma(S)$, so $v(I)\le inf \sigma(S)=|I|_e$? Why do we need the $I_k^*$? Can anyone help? Thanks so much! The following is the definition of $\sigma (S)$.",,"['real-analysis', 'analysis', 'lebesgue-measure']"
55,How to see this is an isometry,How to see this is an isometry,,"Let $X$ be a separable Banach space, and $(f_n)$ be a countable dense subset. Recall that for each $f_n$ there exists a linear functional $l_n \in X^*$ such that $\|l_n\| = 1$ and $l_n(f_n)=\|f_n\|$. Now define a new linear operator $g: X \to L^\infty(\mathbb N)$ as follows. $$g(f) := \{l_n(f)\}_n.$$ Show that $$\|g(f)\| := \sup_n \left|l_n(f)\right| = \|f\|.$$ That is, $g$ is an isometry and hence injective. I know that $$\|f\| = \sup_{l \in X^*,\ l\neq 0} \frac{\left|l(f)\right|}{\|l\|} \geq \sup_n \left|l_n(f)\right|=:\|g(f)\|.$$ It is only left to show the opposite direction. How to proceed from here, please? Thank you!","Let $X$ be a separable Banach space, and $(f_n)$ be a countable dense subset. Recall that for each $f_n$ there exists a linear functional $l_n \in X^*$ such that $\|l_n\| = 1$ and $l_n(f_n)=\|f_n\|$. Now define a new linear operator $g: X \to L^\infty(\mathbb N)$ as follows. $$g(f) := \{l_n(f)\}_n.$$ Show that $$\|g(f)\| := \sup_n \left|l_n(f)\right| = \|f\|.$$ That is, $g$ is an isometry and hence injective. I know that $$\|f\| = \sup_{l \in X^*,\ l\neq 0} \frac{\left|l(f)\right|}{\|l\|} \geq \sup_n \left|l_n(f)\right|=:\|g(f)\|.$$ It is only left to show the opposite direction. How to proceed from here, please? Thank you!",,"['analysis', 'functional-analysis', 'self-learning', 'banach-spaces']"
56,Why does the whole integral converge but not part of it? (Dilogs),Why does the whole integral converge but not part of it? (Dilogs),,"$\newcommand{\Li}{\operatorname{Li}}$Consider the integral: $$\int_0^1 \frac{(-\Li_2(x) - \Li_3(x) - x^2/8 + 3x - x\log(1-x) + \log(1-x))}{x^2} \, dx$$ This integral converges to $\sim 0.01$ But when taken separately, $$ \int_0^1 \frac{-\Li_2(x) - \Li_3(x)}{x^2} \, dx$$ this integral does not converge. What is going on here? I dont understand?","$\newcommand{\Li}{\operatorname{Li}}$Consider the integral: $$\int_0^1 \frac{(-\Li_2(x) - \Li_3(x) - x^2/8 + 3x - x\log(1-x) + \log(1-x))}{x^2} \, dx$$ This integral converges to $\sim 0.01$ But when taken separately, $$ \int_0^1 \frac{-\Li_2(x) - \Li_3(x)}{x^2} \, dx$$ this integral does not converge. What is going on here? I dont understand?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'special-functions']"
57,"A function $\varphi:\mathbb{R}^n\to\mathbb{R}^n$ with $\varphi(x)=x,$ $\|\varphi(y)-x\|\leq K\|y-x\|^\alpha$ for $\alpha>1, K>0$",A function  with   for,"\varphi:\mathbb{R}^n\to\mathbb{R}^n \varphi(x)=x, \|\varphi(y)-x\|\leq K\|y-x\|^\alpha \alpha>1, K>0","If we have a function $\varphi:\mathbb{R}^n\to\mathbb{R}^n$ with $\varphi(x)=x,$ $\|\varphi(y)-x\|\leq K\|y-x\|^\alpha$ for $K>0,$ and we define $\varphi^1:=\varphi, \varphi^2:=\varphi\circ\varphi,\dots,\varphi^n:=\varphi\circ\cdots\circ\varphi,$ then if $\alpha>1$ it is claimed (according to a problem in some lecture notes) that there exists some open set $X$ with $x\in X$ for which $$\lim\limits_{n\to\infty}\varphi^n(y)=x$$ for all $y\in X.$ My attempt at a proof was: Let $h=x-y$ for distinct $x$ and $y$ (otherwise it is trivial). Then since $\alpha>1$ we can write $$\frac{\|\varphi^n(x+h)-\varphi(x)\|}{\|h\|}\le\frac{\|\varphi(x+h)-\varphi(x)\|}{\|h\|}\le K\|h\|^{\alpha-1}\to 0$$ as $\|h\|\to 0,$ so I'm thinking there exists some $\delta>0$ such that if $\|h\|<\delta$ then $\varphi^n(x+h)=\varphi^n(y)\to\varphi(x)=x;$ ie. choose $X=B_\delta(x).$ Could somebody just confirm/correct the above? I feel like it's not a very formal proof and it just seems too simple to be correct to be honest. Thanks ahead.","If we have a function $\varphi:\mathbb{R}^n\to\mathbb{R}^n$ with $\varphi(x)=x,$ $\|\varphi(y)-x\|\leq K\|y-x\|^\alpha$ for $K>0,$ and we define $\varphi^1:=\varphi, \varphi^2:=\varphi\circ\varphi,\dots,\varphi^n:=\varphi\circ\cdots\circ\varphi,$ then if $\alpha>1$ it is claimed (according to a problem in some lecture notes) that there exists some open set $X$ with $x\in X$ for which $$\lim\limits_{n\to\infty}\varphi^n(y)=x$$ for all $y\in X.$ My attempt at a proof was: Let $h=x-y$ for distinct $x$ and $y$ (otherwise it is trivial). Then since $\alpha>1$ we can write $$\frac{\|\varphi^n(x+h)-\varphi(x)\|}{\|h\|}\le\frac{\|\varphi(x+h)-\varphi(x)\|}{\|h\|}\le K\|h\|^{\alpha-1}\to 0$$ as $\|h\|\to 0,$ so I'm thinking there exists some $\delta>0$ such that if $\|h\|<\delta$ then $\varphi^n(x+h)=\varphi^n(y)\to\varphi(x)=x;$ ie. choose $X=B_\delta(x).$ Could somebody just confirm/correct the above? I feel like it's not a very formal proof and it just seems too simple to be correct to be honest. Thanks ahead.",,"['real-analysis', 'analysis', 'proof-verification']"
58,How did Fourier series lead to the development of rigorous analysis?,How did Fourier series lead to the development of rigorous analysis?,,"Once I've heard that the studies of Fourier series have lead to rigorous definitions of such concepts as function, convergence, integral, limit. And also that Cantor's study of Fourier series led him to set theory. I'd like to know a bit more about it, yet trying to look up impact of Fourier series inevitably brings me to signal processing and alike. Where could I read an account of Fourier series' impact within mathematics? I've found a few references, e.g. the section titled ""The impact of Fourier series on mathematical analysis"" in From the Calculus to Set Theory, 1630-1910: An Introductory History (edited by I. Grattan-Guinness) but it wasn't what I expected.","Once I've heard that the studies of Fourier series have lead to rigorous definitions of such concepts as function, convergence, integral, limit. And also that Cantor's study of Fourier series led him to set theory. I'd like to know a bit more about it, yet trying to look up impact of Fourier series inevitably brings me to signal processing and alike. Where could I read an account of Fourier series' impact within mathematics? I've found a few references, e.g. the section titled ""The impact of Fourier series on mathematical analysis"" in From the Calculus to Set Theory, 1630-1910: An Introductory History (edited by I. Grattan-Guinness) but it wasn't what I expected.",,"['analysis', 'reference-request', 'fourier-analysis', 'fourier-series', 'math-history']"
59,Continuous function with infimum,Continuous function with infimum,,"Let $A$ be a closed subset of a metric space $X$ and $f:A \rightarrow[1,2]$ a continuous function on it. Now I want to find out why the function $$F(t):=\frac{\inf\{f(s)d(s,t);s \in A\}}{\inf \{d(s,t);s \in A\}}$$ for $t \notin A$ and $F(t):=f(t)$ for $t \in A$ is continuous? It is clear to me that $F:X \rightarrow [1,2]$ too, but I just have difficulties concerning the continuity. I am also aware of the fact that $d(t,A)$ is a continuous function with respect to $t$. If anything is unclear, please let me know.","Let $A$ be a closed subset of a metric space $X$ and $f:A \rightarrow[1,2]$ a continuous function on it. Now I want to find out why the function $$F(t):=\frac{\inf\{f(s)d(s,t);s \in A\}}{\inf \{d(s,t);s \in A\}}$$ for $t \notin A$ and $F(t):=f(t)$ for $t \in A$ is continuous? It is clear to me that $F:X \rightarrow [1,2]$ too, but I just have difficulties concerning the continuity. I am also aware of the fact that $d(t,A)$ is a continuous function with respect to $t$. If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
60,Is there a basis for the continuous functions space?,Is there a basis for the continuous functions space?,,"I've been searching all over the Internet for this but without finding a satisfying answer. This might be a dumb question, but I would like to know the answer anyway. Is there a set of continuous functions which when combined linearly (or not maybe) span all the functions space ? Could we decompose a log, a sine or an exponent to simpler components ? And if not why ? I know that Fourier analysis is a powerful tool for functions decomposition, but I wanted to know If we could go further and decompose even trigonometric functions. I wondered if there was is theory about this ? Thanks for any answer.","I've been searching all over the Internet for this but without finding a satisfying answer. This might be a dumb question, but I would like to know the answer anyway. Is there a set of continuous functions which when combined linearly (or not maybe) span all the functions space ? Could we decompose a log, a sine or an exponent to simpler components ? And if not why ? I know that Fourier analysis is a powerful tool for functions decomposition, but I wanted to know If we could go further and decompose even trigonometric functions. I wondered if there was is theory about this ? Thanks for any answer.",,"['abstract-algebra', 'analysis', 'functions']"
61,checking whether functions satisfy Inverse Function Theorem.,checking whether functions satisfy Inverse Function Theorem.,,"I've my exam tomorrow and this question is expected  to come but donot know how to solve... Here's the INVERSE FUNCTION THEOREM stated in my notes: It says: Let $E\subseteq \mathbb R^n$ be open  and $f:E\to \mathbb R^n$ be a $C^1$ map. Suppose that for some $a\in E$,the linear map $f'(a)$ is invertible, and $b=f(a)$.Then there are open set $U$ and $V$ in $\mathbb R^n$ ,s.t. $a\in U,b\in V$ and $f|_U$ is $1-1$ and onto V,that is $f(U)=V$ If g is inverse of $f|_U$ ,so $g:V\to U$ and $g(f(x))=x$ for all $x\in U$,then $g\in C^1(V,U)$ I had to check whether following satisfy the above hypothesis of inverse function theorem on $D$: $1.)$ $g(x)=x+c$ , $D=\mathbb R^n$ , $2.)$ $g(s,t)=(s+2)e_2+(s-t)e_2$ , $D=\mathbb R^2$ , $3.)$ $g(s,t)=(s^2-t^2)e_2+(st)e_2$ , $D=\mathbb R^2$ \ ${(0,0)}$. I think that $1.)$ satisfies all hypothesis as it has a inverse....  I don't know how to solve these questions...any hint...","I've my exam tomorrow and this question is expected  to come but donot know how to solve... Here's the INVERSE FUNCTION THEOREM stated in my notes: It says: Let $E\subseteq \mathbb R^n$ be open  and $f:E\to \mathbb R^n$ be a $C^1$ map. Suppose that for some $a\in E$,the linear map $f'(a)$ is invertible, and $b=f(a)$.Then there are open set $U$ and $V$ in $\mathbb R^n$ ,s.t. $a\in U,b\in V$ and $f|_U$ is $1-1$ and onto V,that is $f(U)=V$ If g is inverse of $f|_U$ ,so $g:V\to U$ and $g(f(x))=x$ for all $x\in U$,then $g\in C^1(V,U)$ I had to check whether following satisfy the above hypothesis of inverse function theorem on $D$: $1.)$ $g(x)=x+c$ , $D=\mathbb R^n$ , $2.)$ $g(s,t)=(s+2)e_2+(s-t)e_2$ , $D=\mathbb R^2$ , $3.)$ $g(s,t)=(s^2-t^2)e_2+(st)e_2$ , $D=\mathbb R^2$ \ ${(0,0)}$. I think that $1.)$ satisfies all hypothesis as it has a inverse....  I don't know how to solve these questions...any hint...",,"['calculus', 'analysis', 'multivariable-calculus', 'inverse']"
62,Prove $g(x)=\sqrt{f(x)}$ is regulated,Prove  is regulated,g(x)=\sqrt{f(x)},"Let $f:[a,b]→\mathbb{R}$ be regulated and non-negative. Prove that $g:[a,b]→\mathbb{R}$ defined by $g(x)=\sqrt{f(x)}$ is regulated. A function $f:[a,b]\to\Bbb R$ is a regulated function if $\forall$ $\varepsilon>0$ $\exists$  a step function $\varphi:[a,b]\to\Bbb R$ such that $\Vert f-\varphi\Vert<\varepsilon$. I've tried to use the definition of a regulated function but haven't been able to make any progress. Is there a way of using the fact that a linear combination of regulated functions is regulated? Or am I not even close? @Arthur Is this attempt at all correct or at least along the lines of what you mean: We have a step function $\varphi_f$ for $f$ and $\varepsilon_f$. Let $\sqrt{\varphi_f}=\varphi_g$. We know $\Vert f-\varphi_f\Vert<\varepsilon_f \implies \Vert f-\varphi_g^2\Vert<\varepsilon_f$ $\implies \Vert (\sqrt{f}+\varphi_g)(\sqrt{f}-\varphi_g)\Vert<\varepsilon_f$ $\implies \Vert (g+\varphi_g)\Vert \Vert(g-\varphi_g)\Vert<\varepsilon_f $ $\implies \Vert(g-\varphi_g)\Vert < (\varepsilon_f)/\Vert (g+\varphi_g)\Vert $ So letting $\varepsilon_g = (\varepsilon_f)/\Vert (g+\varphi_g)\Vert $ proves that $\Vert(g-\varphi_g)\Vert < \varepsilon_g$ and hence $g(x)$ is regulated.","Let $f:[a,b]→\mathbb{R}$ be regulated and non-negative. Prove that $g:[a,b]→\mathbb{R}$ defined by $g(x)=\sqrt{f(x)}$ is regulated. A function $f:[a,b]\to\Bbb R$ is a regulated function if $\forall$ $\varepsilon>0$ $\exists$  a step function $\varphi:[a,b]\to\Bbb R$ such that $\Vert f-\varphi\Vert<\varepsilon$. I've tried to use the definition of a regulated function but haven't been able to make any progress. Is there a way of using the fact that a linear combination of regulated functions is regulated? Or am I not even close? @Arthur Is this attempt at all correct or at least along the lines of what you mean: We have a step function $\varphi_f$ for $f$ and $\varepsilon_f$. Let $\sqrt{\varphi_f}=\varphi_g$. We know $\Vert f-\varphi_f\Vert<\varepsilon_f \implies \Vert f-\varphi_g^2\Vert<\varepsilon_f$ $\implies \Vert (\sqrt{f}+\varphi_g)(\sqrt{f}-\varphi_g)\Vert<\varepsilon_f$ $\implies \Vert (g+\varphi_g)\Vert \Vert(g-\varphi_g)\Vert<\varepsilon_f $ $\implies \Vert(g-\varphi_g)\Vert < (\varepsilon_f)/\Vert (g+\varphi_g)\Vert $ So letting $\varepsilon_g = (\varepsilon_f)/\Vert (g+\varphi_g)\Vert $ proves that $\Vert(g-\varphi_g)\Vert < \varepsilon_g$ and hence $g(x)$ is regulated.",,"['calculus', 'real-analysis']"
63,$\sum_{n=1}^\infty \frac{n+1}{\sqrt{n^3+1}}$convergent/divergent?,convergent/divergent?,\sum_{n=1}^\infty \frac{n+1}{\sqrt{n^3+1}},Please could someone help prove $$\sum_{n=1}^\infty \frac{n+1}{\sqrt{n^3+1}}$$ converges/diverges? Thank you.,Please could someone help prove $$\sum_{n=1}^\infty \frac{n+1}{\sqrt{n^3+1}}$$ converges/diverges? Thank you.,,['sequences-and-series']
64,"If $f$ is equal to an affine function up to $1$-th order at $a$, then $f$ is differentiable at $a$, proof more subtle then it appears?","If  is equal to an affine function up to -th order at , then  is differentiable at , proof more subtle then it appears?",f 1 a f a,"I came across the following exercise: Two functions $f, g : \mathbb R \to \mathbb R$ are equal up to $n$th order at $a$ if $$ \lim_{h \to 0} \frac{f(a + h) - g(a + h)}{h^n} = 0. $$ Show that $f$ is differentiable at $a$ if and only if there is a function $g$ of the form $g(x) = a_0 + a_1(x - a)$ such that $f$ and $g$ are equal up to first order at $a$. For the proof, if $f$ is differentiable at $a$, then $a_0 := f(a)$ and $a_1 = f'(a)$ fulfill the requirements, this is the easy direction. For the other direction, suppose there exists such an $g(x) = a_0 + a_1(x-a)$, then I found [precisely I found it here , it is Problem 2-9] the following solution, on $$  \lim_{h \to 0} \frac{f(a+h) - (a_0 + a_1h)}{h} $$ add $\lim a_1 = a_1$ to get $$  \lim_{h\to 0} \frac{f(a+h) - a_0}{h} = a_1. $$ For $a_0 \ne f(a)$, the limit diverges, so we must have $a_0 = f(a)$, then we get the limit for the derivative, and so $a_1 = f'(a)$. $\square$ But I doubt that $$  \lim_{h\to 0} \frac{f(a+h) - a_0}{h} < \infty \qquad (*) $$ implies $a_0 = f(a)$ (if nothing is assumed about $f : \mathbb R\to \mathbb R$). I think everything that follows is that: 1) $\lim_{h\to 0} f(a+h) = a_0$ 2) $f(a) \to a_0$ faster then any linear term (otherwise the $h$ in the denominator would not be ""compensated"" and it still diverges) And so $a_0 = f(a)$ just follows for continuous $f$, where we have $\lim_{h\to 0} f(a+h) = f(a)$? Am I right? And by the way, conclusion 1) and 2) I just reached by an intuitive feeling, any ideas how to make this precise?","I came across the following exercise: Two functions $f, g : \mathbb R \to \mathbb R$ are equal up to $n$th order at $a$ if $$ \lim_{h \to 0} \frac{f(a + h) - g(a + h)}{h^n} = 0. $$ Show that $f$ is differentiable at $a$ if and only if there is a function $g$ of the form $g(x) = a_0 + a_1(x - a)$ such that $f$ and $g$ are equal up to first order at $a$. For the proof, if $f$ is differentiable at $a$, then $a_0 := f(a)$ and $a_1 = f'(a)$ fulfill the requirements, this is the easy direction. For the other direction, suppose there exists such an $g(x) = a_0 + a_1(x-a)$, then I found [precisely I found it here , it is Problem 2-9] the following solution, on $$  \lim_{h \to 0} \frac{f(a+h) - (a_0 + a_1h)}{h} $$ add $\lim a_1 = a_1$ to get $$  \lim_{h\to 0} \frac{f(a+h) - a_0}{h} = a_1. $$ For $a_0 \ne f(a)$, the limit diverges, so we must have $a_0 = f(a)$, then we get the limit for the derivative, and so $a_1 = f'(a)$. $\square$ But I doubt that $$  \lim_{h\to 0} \frac{f(a+h) - a_0}{h} < \infty \qquad (*) $$ implies $a_0 = f(a)$ (if nothing is assumed about $f : \mathbb R\to \mathbb R$). I think everything that follows is that: 1) $\lim_{h\to 0} f(a+h) = a_0$ 2) $f(a) \to a_0$ faster then any linear term (otherwise the $h$ in the denominator would not be ""compensated"" and it still diverges) And so $a_0 = f(a)$ just follows for continuous $f$, where we have $\lim_{h\to 0} f(a+h) = f(a)$? Am I right? And by the way, conclusion 1) and 2) I just reached by an intuitive feeling, any ideas how to make this precise?",,"['calculus', 'real-analysis', 'analysis', 'asymptotics']"
65,Give example where an outer measure is strictly less than the set function from which it is defined.,Give example where an outer measure is strictly less than the set function from which it is defined.,,"Let $K $ be a class of subsets of $X $ where for every subset $A $ of $X $ there is a sequence $\{E _n \}  $ of sets  in $K $ such that $A \subset \bigcup _{n=1 }^{\infty } E _n $. Let $\lambda$ be a extended-real valued, nonnegative  set function, with $\lambda ( \emptyset )=0$ Define $$\mu(E)= \inf \left\{\sum _{n=1 } ^{\infty } \lambda(E _n): E _n \in K, A \subset \bigcup _{n=1 }^{\infty } E _n \right\}$$ Show that if $E \in K $ then $\mu(E) \le \lambda (E) $  and give an example of where strict inequality holds. I think that if $E \in K $ , then $ \lambda (E ) $ is in the set from which we take infinum of in the definition of $ \mu $ and thus $\mu (E) \le \lambda (E) $. But how can you give an example of a set function $\lambda $ where strict inequality holds?","Let $K $ be a class of subsets of $X $ where for every subset $A $ of $X $ there is a sequence $\{E _n \}  $ of sets  in $K $ such that $A \subset \bigcup _{n=1 }^{\infty } E _n $. Let $\lambda$ be a extended-real valued, nonnegative  set function, with $\lambda ( \emptyset )=0$ Define $$\mu(E)= \inf \left\{\sum _{n=1 } ^{\infty } \lambda(E _n): E _n \in K, A \subset \bigcup _{n=1 }^{\infty } E _n \right\}$$ Show that if $E \in K $ then $\mu(E) \le \lambda (E) $  and give an example of where strict inequality holds. I think that if $E \in K $ , then $ \lambda (E ) $ is in the set from which we take infinum of in the definition of $ \mu $ and thus $\mu (E) \le \lambda (E) $. But how can you give an example of a set function $\lambda $ where strict inequality holds?",,"['analysis', 'measure-theory']"
66,Definition of neighborhood,Definition of neighborhood,,"I am starting to work through Rudin's Principles of Mathematical Analysis .  For $(X, d)$ a metric space and $x \in X$, Rudin defines the neighborhood $N_r(x)$ of $x$ to be the set consisting of all points $y$ such that $d(x,y) < r$.  My question is this: if $A \subset X$ and $a \in A$, then does $N_r(a)$ refer to the same set in both $A$ and $X$ the ambient space?  For example, if $X = \mathbb{R}$ and $A = \mathbb{Q}$, then is it the case that $N_{1/2}(a)$ is always the set $N = \{p \in \mathbb{R} : a - 1/2 < p < a + 1/2\}$, or can $N_{1/2}(a)$ be interpreted as the set $N' = \{q \in \mathbb{Q} : a - 1/2 < q < a + 1/2\}$?  Essentially I am asking if the notion of neighborhood is relative.  This would affect things like the interiority of points in a set.  For instance, $N$ is always contained in $\mathbb{R}$ so that $a$ is always an interior point of $\mathbb{R}$, but $N \not \subset \mathbb{Q}$ so that $a$ is never an interior point of $\mathbb{Q}$.  On the other hand, $N'$ is always contained in both $\mathbb{R}$ and $\mathbb{Q}$ so that $a$ is always an interior point of both of them.  (I guess $N$ is basically a neighborhood formed with respect to the whole space while $N'$ is a neighborhood formed with respect to a specific subspace of the metric.)  Thanks in advance.","I am starting to work through Rudin's Principles of Mathematical Analysis .  For $(X, d)$ a metric space and $x \in X$, Rudin defines the neighborhood $N_r(x)$ of $x$ to be the set consisting of all points $y$ such that $d(x,y) < r$.  My question is this: if $A \subset X$ and $a \in A$, then does $N_r(a)$ refer to the same set in both $A$ and $X$ the ambient space?  For example, if $X = \mathbb{R}$ and $A = \mathbb{Q}$, then is it the case that $N_{1/2}(a)$ is always the set $N = \{p \in \mathbb{R} : a - 1/2 < p < a + 1/2\}$, or can $N_{1/2}(a)$ be interpreted as the set $N' = \{q \in \mathbb{Q} : a - 1/2 < q < a + 1/2\}$?  Essentially I am asking if the notion of neighborhood is relative.  This would affect things like the interiority of points in a set.  For instance, $N$ is always contained in $\mathbb{R}$ so that $a$ is always an interior point of $\mathbb{R}$, but $N \not \subset \mathbb{Q}$ so that $a$ is never an interior point of $\mathbb{Q}$.  On the other hand, $N'$ is always contained in both $\mathbb{R}$ and $\mathbb{Q}$ so that $a$ is always an interior point of both of them.  (I guess $N$ is basically a neighborhood formed with respect to the whole space while $N'$ is a neighborhood formed with respect to a specific subspace of the metric.)  Thanks in advance.",,"['general-topology', 'analysis', 'metric-spaces', 'definition']"
67,prove T is continuous and find its norm.,prove T is continuous and find its norm.,,"question- Let $x\in l^2$. Prove that $Ty=xy=(x_1y_1,x_2y_2,...)$ defines a linear map from $l^2$ to $l^1$. Also show that T is continuous and find the norm ||$T$||. How can i show $\sum\limits_{j=1}^n|x_jy_j|< \infty$, that is what i have to prove right, to show it in $l^1$. Can we prove $T$ is continuous as implication of some theorem in functional analysis or i have to go by the definition. And what will be  ||$T$||? please help. I did it wrong in my exam and today my teacher will ask the right solution. I don't know what is the right solution.","question- Let $x\in l^2$. Prove that $Ty=xy=(x_1y_1,x_2y_2,...)$ defines a linear map from $l^2$ to $l^1$. Also show that T is continuous and find the norm ||$T$||. How can i show $\sum\limits_{j=1}^n|x_jy_j|< \infty$, that is what i have to prove right, to show it in $l^1$. Can we prove $T$ is continuous as implication of some theorem in functional analysis or i have to go by the definition. And what will be  ||$T$||? please help. I did it wrong in my exam and today my teacher will ask the right solution. I don't know what is the right solution.",,"['analysis', 'functional-analysis']"
68,"Reference request: Analysis, Algebra and Topology - Same author(s)/publisher(s), progressive order","Reference request: Analysis, Algebra and Topology - Same author(s)/publisher(s), progressive order",,"Is there anywhere I can acquire a collection of all Mathematical undergraduate textbooks by the same publishing author, or authors(so that they are similarly written) and can be completed in a logical order. I feel as though this would remove any issues in regards to prerequisite knowledge, as I would know everything required for progression to the next topic. I don't care for statistics, but I would like textbooks/notes for Analysis, Topology and Algebra. Pdf form provided legally by the author is very much preferred. Perhaps no author(s)/publisher(s) have ever done this, perhaps no-one knows the three fields sufficiently to do such. Thank you for your time. Note: I am $14$ and live on my own, I am unlikely to be able to afford numerous textbooks.","Is there anywhere I can acquire a collection of all Mathematical undergraduate textbooks by the same publishing author, or authors(so that they are similarly written) and can be completed in a logical order. I feel as though this would remove any issues in regards to prerequisite knowledge, as I would know everything required for progression to the next topic. I don't care for statistics, but I would like textbooks/notes for Analysis, Topology and Algebra. Pdf form provided legally by the author is very much preferred. Perhaps no author(s)/publisher(s) have ever done this, perhaps no-one knows the three fields sufficiently to do such. Thank you for your time. Note: I am $14$ and live on my own, I am unlikely to be able to afford numerous textbooks.",,"['abstract-algebra', 'general-topology', 'analysis', 'reference-request']"
69,Sequence of $L^{2}$ functions satisfying an integral condition,Sequence of  functions satisfying an integral condition,L^{2},"I am working on the following problem: Suppose $f \in C^{\infty}([0, \infty) \times [0, 1])$ is such that $$C :=\int_{0}^{\infty}\int_{0}^{1}|\partial_{t}f|^{2}(1 + t^{2})\, dx\, dt < \infty.$$ The problem is to show that there exists a $g \in L^{2}([0, 1])$ such that $f(t, \cdot)$ converges to $g(\cdot)$ in $L^{2}([0, 1])$ as $t \rightarrow \infty$. By completeness of $L^{2}$, it would suffice to show that $f(t_{n}, \cdot)$ is Cauchy for each sequence $\{t_{n}\}$ tending to $\infty$. I can only derive the following result: Fix $h$. I will show that $\lim_{s \rightarrow \infty}\|f(s, x) - f(s + h, x)\|_{L^{2}} = 0$. Fix an $\varepsilon > 0$. Choose $N$ such that $hC/(1 + N^{2}) < \varepsilon$. Then for $s + h > s \geq N$, \begin{align*} \|f(s, x) - f(s + h, x)\|_{L^{2}}^{2} &= \int_{0}^{1}|f(s, x) - f(s + h, x)|^{2}\, dx = \int_{0}^{1}\left|\int_{s}^{s + h}\partial_{t}f\, dt\right|^{2}\, dx\\ &\leq \int_{0}^{1}\left(\int_{s}^{s + h}|\partial_{t}f|\, dt\right)^{2}\, dx \leq \int_{0}^{1}h\int_{s}^{s + h}|\partial_{t}f|^{2}\, dt\, dx\\ & \leq \int_{0}^{1}\int_{s}^{s + h}h|\partial_{t}f|^{2}\frac{1 + t^{2}}{1 + t^{2}}\, dt\, dx \leq \frac{hC}{1 + N^{2}} < \varepsilon \end{align*} where in the last $\leq$ we used that $s \geq N$. Is there a way to remove the dependence of $N$ on $h$? If I could do that then the above calculation will show the result. Is there another way to do this problem?","I am working on the following problem: Suppose $f \in C^{\infty}([0, \infty) \times [0, 1])$ is such that $$C :=\int_{0}^{\infty}\int_{0}^{1}|\partial_{t}f|^{2}(1 + t^{2})\, dx\, dt < \infty.$$ The problem is to show that there exists a $g \in L^{2}([0, 1])$ such that $f(t, \cdot)$ converges to $g(\cdot)$ in $L^{2}([0, 1])$ as $t \rightarrow \infty$. By completeness of $L^{2}$, it would suffice to show that $f(t_{n}, \cdot)$ is Cauchy for each sequence $\{t_{n}\}$ tending to $\infty$. I can only derive the following result: Fix $h$. I will show that $\lim_{s \rightarrow \infty}\|f(s, x) - f(s + h, x)\|_{L^{2}} = 0$. Fix an $\varepsilon > 0$. Choose $N$ such that $hC/(1 + N^{2}) < \varepsilon$. Then for $s + h > s \geq N$, \begin{align*} \|f(s, x) - f(s + h, x)\|_{L^{2}}^{2} &= \int_{0}^{1}|f(s, x) - f(s + h, x)|^{2}\, dx = \int_{0}^{1}\left|\int_{s}^{s + h}\partial_{t}f\, dt\right|^{2}\, dx\\ &\leq \int_{0}^{1}\left(\int_{s}^{s + h}|\partial_{t}f|\, dt\right)^{2}\, dx \leq \int_{0}^{1}h\int_{s}^{s + h}|\partial_{t}f|^{2}\, dt\, dx\\ & \leq \int_{0}^{1}\int_{s}^{s + h}h|\partial_{t}f|^{2}\frac{1 + t^{2}}{1 + t^{2}}\, dt\, dx \leq \frac{hC}{1 + N^{2}} < \varepsilon \end{align*} where in the last $\leq$ we used that $s \geq N$. Is there a way to remove the dependence of $N$ on $h$? If I could do that then the above calculation will show the result. Is there another way to do this problem?",,"['real-analysis', 'analysis']"
70,Show that for a sequence of real numbers $(a_n)_n$ $\lim_n a_n=0$ implies $\frac{1}{n}\sum_{i=0}^{n-1}\lvert a_i\rvert=0$,Show that for a sequence of real numbers   implies,(a_n)_n \lim_n a_n=0 \frac{1}{n}\sum_{i=0}^{n-1}\lvert a_i\rvert=0,"Let $(a_n)_{n\in\mathbb{N}}$ be q sequence of real numbers with $\lim_{n\to\infty}a_n=0$. Show that this implies $$ \lim_{n\to\infty}\frac{1}{n}\sum_{i=0}^{n-1}\lvert a_i\rvert=0. $$ This is my idea how to prove it, unfortunately do not know if it is right: Let $\varepsilon > 0$ be arbitrary, then there exists a $N(\varepsilon)$ with $\lvert a_n\rvert < \varepsilon$ for all $n\geqslant N(\varepsilon)$. So it is  $$ \lim_{n\to\infty}\sum_{i=0}^{n-1}\lvert a_i\rvert=\sum_{i=0}^{\infty}\lvert a_i\rvert=\sum_{i=0}^{N(\varepsilon)-1}\lvert a_i\rvert+\sum_{i=N(\varepsilon)}^{\infty}\lvert a_i\rvert\leqslant\sum_{i=0}^{N(\varepsilon)-1}\lvert a_i\rvert+\sum_{i=N(\varepsilon)}^{\infty}\varepsilon\leqslant M $$ for a $M\geqslant 0$ for $\varepsilon \to 0$. So the limits exists. Because $\lim_{n\to\infty}\frac{1}{n}=0$, i.e. the limit exsits, too, one can write the limit as the product of both limits, i.e. $$ \lim_{n\to\infty}\frac{1}{n}\sum_{i=0}^{n-1}\lvert a_i\rvert=\lim_{n\to\infty}\frac{1}{n}\cdot\lim_{n\to\infty}\sum_{i=0}^{n-1}\lvert a_i\rvert=0\cdot M=0. $$","Let $(a_n)_{n\in\mathbb{N}}$ be q sequence of real numbers with $\lim_{n\to\infty}a_n=0$. Show that this implies $$ \lim_{n\to\infty}\frac{1}{n}\sum_{i=0}^{n-1}\lvert a_i\rvert=0. $$ This is my idea how to prove it, unfortunately do not know if it is right: Let $\varepsilon > 0$ be arbitrary, then there exists a $N(\varepsilon)$ with $\lvert a_n\rvert < \varepsilon$ for all $n\geqslant N(\varepsilon)$. So it is  $$ \lim_{n\to\infty}\sum_{i=0}^{n-1}\lvert a_i\rvert=\sum_{i=0}^{\infty}\lvert a_i\rvert=\sum_{i=0}^{N(\varepsilon)-1}\lvert a_i\rvert+\sum_{i=N(\varepsilon)}^{\infty}\lvert a_i\rvert\leqslant\sum_{i=0}^{N(\varepsilon)-1}\lvert a_i\rvert+\sum_{i=N(\varepsilon)}^{\infty}\varepsilon\leqslant M $$ for a $M\geqslant 0$ for $\varepsilon \to 0$. So the limits exists. Because $\lim_{n\to\infty}\frac{1}{n}=0$, i.e. the limit exsits, too, one can write the limit as the product of both limits, i.e. $$ \lim_{n\to\infty}\frac{1}{n}\sum_{i=0}^{n-1}\lvert a_i\rvert=\lim_{n\to\infty}\frac{1}{n}\cdot\lim_{n\to\infty}\sum_{i=0}^{n-1}\lvert a_i\rvert=0\cdot M=0. $$",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'cesaro-summable']"
71,Is the sum of quasi concave functions quasi concave,Is the sum of quasi concave functions quasi concave,,"Is the sum of quasi-concave functions a quasi concave function? I presume that's not in the case in general, but under which conditions is this true?","Is the sum of quasi-concave functions a quasi concave function? I presume that's not in the case in general, but under which conditions is this true?",,['analysis']
72,Limit of $S(x) = x − x^2 + x^4 − x^8 + x^{16} − x^{32} + \cdots$ as $x$ approached $1$ from below,Limit of  as  approached  from below,S(x) = x − x^2 + x^4 − x^8 + x^{16} − x^{32} + \cdots x 1,"I have read the following ( http://www.math.harvard.edu/~elkies/Misc/sol8.html ) but I dont understand the last part of the solution: For positive $x<1$, consider the alternating sum  $$S(x) = x − x^2 + x^4 − x^8 + x^{16} − x^{32} + \cdots$$ Does $S(x)$ approach a limit as $x$ approaches $1$ from below, and if so what is this limit? Proof     Since $S$ satisfies the functional equation $S(x) = x − S(x^2)$,  it is clear that if $S(x)$ has a limit as $x$ approaches $1$ then that limit must be $1/2$. But then $S(0.995) = 0.50088\cdots > 1/2$.  Iterating the functional equation, we find  $S(x) = x − x^2 + S(x^4) > S(x^4)$.  Therefore the fourth root, 16th root, 64th root, … of 0.995 are all values of x for which  $S(x) > S(0.995) > 1/2$.  Since these roots approach $1$, we conclude that in fact $S(x)$ cannot tend to $1/2$ as $x$ approaches $1$, and thus has no limit at all! My question: why the last sentence is true? (Since these roots approach $1$, we conclude that in fact $S(x)$ cannot tend to $1/2$ as $x$ approaches $1$) and is there any way to show that $S(0.995)>1/2$ without the use of computer? Thank you so much in advance.","I have read the following ( http://www.math.harvard.edu/~elkies/Misc/sol8.html ) but I dont understand the last part of the solution: For positive $x<1$, consider the alternating sum  $$S(x) = x − x^2 + x^4 − x^8 + x^{16} − x^{32} + \cdots$$ Does $S(x)$ approach a limit as $x$ approaches $1$ from below, and if so what is this limit? Proof     Since $S$ satisfies the functional equation $S(x) = x − S(x^2)$,  it is clear that if $S(x)$ has a limit as $x$ approaches $1$ then that limit must be $1/2$. But then $S(0.995) = 0.50088\cdots > 1/2$.  Iterating the functional equation, we find  $S(x) = x − x^2 + S(x^4) > S(x^4)$.  Therefore the fourth root, 16th root, 64th root, … of 0.995 are all values of x for which  $S(x) > S(0.995) > 1/2$.  Since these roots approach $1$, we conclude that in fact $S(x)$ cannot tend to $1/2$ as $x$ approaches $1$, and thus has no limit at all! My question: why the last sentence is true? (Since these roots approach $1$, we conclude that in fact $S(x)$ cannot tend to $1/2$ as $x$ approaches $1$) and is there any way to show that $S(0.995)>1/2$ without the use of computer? Thank you so much in advance.",,"['calculus', 'real-analysis', 'analysis']"
73,Prove that for any $1 < p < ∞$ there exists a function $f ∈ L_p(μ)$ such that $f \notin L_q(μ)$ for any $q > p.$,Prove that for any  there exists a function  such that  for any,1 < p < ∞ f ∈ L_p(μ) f \notin L_q(μ) q > p.,"Let $(X, Ω, μ)$ be a finite measure space. Assume that for any $t > 0$ there exists $E ∈ Ω$ satisfying $0 < μ(E) < t.$ Prove that for any $1 < p < ∞$ there exists a function $f ∈ L_p(μ)$ such that $f \notin L_q(μ)$ for any $q > p.$ I am having trouble starting this problem.  It is the second part to another question on a past qual.  Any help would be awesome.  Thanks.","Let $(X, Ω, μ)$ be a finite measure space. Assume that for any $t > 0$ there exists $E ∈ Ω$ satisfying $0 < μ(E) < t.$ Prove that for any $1 < p < ∞$ there exists a function $f ∈ L_p(μ)$ such that $f \notin L_q(μ)$ for any $q > p.$ I am having trouble starting this problem.  It is the second part to another question on a past qual.  Any help would be awesome.  Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
74,$1/r^2\int_{\mathbb{S}_r}u-u(x)$ converging to $\Delta u(x)$?a,converging to ?a,1/r^2\int_{\mathbb{S}_r}u-u(x) \Delta u(x),"When reading some papers on PDEs, the following shows up several times: For a $C^{\infty}$ function $u$, $\frac{\int_{\mathbb{S}_r(x)}u-u(x)}{r^2}$ converges to $1/2n\Delta u(x)$ uniformly on compact sets as $r\to 0$. Here $\mathbb{S}_r(x)$ is the sphere of radius $r$ centred at $x$, and $n$ is the dimension of the Euclidean space. I am wondering how we can prove this. I think we might use Taylor's approximation, but ended up with something of the form \begin{equation} \langle D^2u(x)(y-x),(y-x)\rangle/\lvert y-x\rvert^2,\end{equation} and we need to show this goes to $1/n\Delta u(x)$. So the difference is that $\Delta u$ is a diagonal matrix while the Hessian is not necessarily so. I guess in the end we might be able to show the significant part is the diagonal, but can someone give a hint on how to do this? Thanks!","When reading some papers on PDEs, the following shows up several times: For a $C^{\infty}$ function $u$, $\frac{\int_{\mathbb{S}_r(x)}u-u(x)}{r^2}$ converges to $1/2n\Delta u(x)$ uniformly on compact sets as $r\to 0$. Here $\mathbb{S}_r(x)$ is the sphere of radius $r$ centred at $x$, and $n$ is the dimension of the Euclidean space. I am wondering how we can prove this. I think we might use Taylor's approximation, but ended up with something of the form \begin{equation} \langle D^2u(x)(y-x),(y-x)\rangle/\lvert y-x\rvert^2,\end{equation} and we need to show this goes to $1/n\Delta u(x)$. So the difference is that $\Delta u$ is a diagonal matrix while the Hessian is not necessarily so. I guess in the end we might be able to show the significant part is the diagonal, but can someone give a hint on how to do this? Thanks!",,"['analysis', 'partial-differential-equations']"
75,direction limits and double limit,direction limits and double limit,,"Let $f(x,y)$ be a function of two variables. What is the counterexample that there exists $A$ s.t. for all $\theta$, $$\lim_{r\to 0+}f(r\cos \theta,r\sin \theta)=A$$ but double limit $$ \lim_{(x,y)\to (0,0)}f(x,y)$$ does not exist? Is it true: $\lim_{(x,y)\to (0,0)}f(x,y)=A$  if and only if $$\lim_{r\to 0+}f(r\cos \theta,r\sin \theta)=A$$ uniformly for $\theta\in[0,2\pi)$?","Let $f(x,y)$ be a function of two variables. What is the counterexample that there exists $A$ s.t. for all $\theta$, $$\lim_{r\to 0+}f(r\cos \theta,r\sin \theta)=A$$ but double limit $$ \lim_{(x,y)\to (0,0)}f(x,y)$$ does not exist? Is it true: $\lim_{(x,y)\to (0,0)}f(x,y)=A$  if and only if $$\lim_{r\to 0+}f(r\cos \theta,r\sin \theta)=A$$ uniformly for $\theta\in[0,2\pi)$?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'convergence-divergence']"
76,Sequence of orthogonal vectors in a Hilbert space,Sequence of orthogonal vectors in a Hilbert space,,"Let $\{x_n\}_{n\in\mathbb{N}}$ be a sequance of pairwise orthogonal vectors in a Hilbert space $H$. Show that the following are equavalent: (a) $\sum_{n=0}^\infty x_n$ converges in the norm topology of $H$. (b) $\sum_{n=0}^\infty \|x_n\|^2 < \infty$. (c) $\sum_{n=0}^\infty \langle x_n, y \rangle$ converges for every $y\in H$. I can prove that (a) implies (b), but none of the other implications.","Let $\{x_n\}_{n\in\mathbb{N}}$ be a sequance of pairwise orthogonal vectors in a Hilbert space $H$. Show that the following are equavalent: (a) $\sum_{n=0}^\infty x_n$ converges in the norm topology of $H$. (b) $\sum_{n=0}^\infty \|x_n\|^2 < \infty$. (c) $\sum_{n=0}^\infty \langle x_n, y \rangle$ converges for every $y\in H$. I can prove that (a) implies (b), but none of the other implications.",,"['analysis', 'functional-analysis', 'hilbert-spaces', 'orthogonality']"
77,"Does a nondecreasing, differentiable function have continuous derivative?","Does a nondecreasing, differentiable function have continuous derivative?",,"Are the following statements true? How to prove or disprove? (1). Let $f$ be a nondecreasing, differentiable function on $[0,1]$. Then $f$ is absolutely continuous? To be stronger, (2).   Let $f$ be a nondecreasing, differentiable function on $[0,1]$. Then $f'$ is bounded on $[0,1]$?","Are the following statements true? How to prove or disprove? (1). Let $f$ be a nondecreasing, differentiable function on $[0,1]$. Then $f$ is absolutely continuous? To be stronger, (2).   Let $f$ be a nondecreasing, differentiable function on $[0,1]$. Then $f'$ is bounded on $[0,1]$?",,"['calculus', 'real-analysis', 'analysis', 'measure-theory', 'derivatives']"
78,Riesz measure associated with a subharmonic function,Riesz measure associated with a subharmonic function,,"In page  101, corollary 4.3.3., from Armitage and Gardiner's book on potential theory, the authors prove that any subharmonic function, can be identified with a positive measure (Riesz measure). In doing so, they consider the functional $L_s(\phi)=\int_\Omega s(x)\Delta \phi(x)$, $\Omega$ a open set, $\phi\in C_0^\infty(\Omega)$ and $s$ the subharmonic function. For any $\phi\in C_0(\Omega)$, he consider the mollified sequence $\phi_n$, which converges to $\phi$ uniformly and claim that $L_s(\phi_n)$ is Cauchy. Maybe, as he says, this is evident, however I fail to see it. If, for instance the regularized sequence $s_n$, is such that, $\Delta s_n$ is bounded uniformly in $L^1_{loc}(\Omega)$ then, the above is true, however, I also fail to see it. Any help is appreciated. Remark: I know how to extend $L_s$ to $C_0(\Omega)$, but with another approach, although the extension is the same, because of unicity. Update: For $x\in \Omega$, fix some $r>0$ such that $\overline{B(x,r)}\subset \Omega$.The regularization $s_n$ of $s$, is a sequence of subharmonic functions in $B(x,r)$ which converges decreasingly to $u$ in $B(x,r)$. Note that (Green's theorem) \begin{eqnarray}  \int_{B(x,r)}|\Delta s_n| &=& \int_{B(x,r)} \Delta s_n      \nonumber \\    &=& \int_{\partial B(x,r)} \frac{\partial s_n}{\partial\eta} \nonumber \\    &=& r^{N-1}\frac{\partial}{\partial r}\left(\frac{1}{r^{N-1}}\int_{\partial B(x,r)}s_n\right)  \end{eqnarray} Does anyone knows how to bound the right hand side uniformly in $n$? Also, note that the right hand side, can be written in terms of the average integral of $s_n$.","In page  101, corollary 4.3.3., from Armitage and Gardiner's book on potential theory, the authors prove that any subharmonic function, can be identified with a positive measure (Riesz measure). In doing so, they consider the functional $L_s(\phi)=\int_\Omega s(x)\Delta \phi(x)$, $\Omega$ a open set, $\phi\in C_0^\infty(\Omega)$ and $s$ the subharmonic function. For any $\phi\in C_0(\Omega)$, he consider the mollified sequence $\phi_n$, which converges to $\phi$ uniformly and claim that $L_s(\phi_n)$ is Cauchy. Maybe, as he says, this is evident, however I fail to see it. If, for instance the regularized sequence $s_n$, is such that, $\Delta s_n$ is bounded uniformly in $L^1_{loc}(\Omega)$ then, the above is true, however, I also fail to see it. Any help is appreciated. Remark: I know how to extend $L_s$ to $C_0(\Omega)$, but with another approach, although the extension is the same, because of unicity. Update: For $x\in \Omega$, fix some $r>0$ such that $\overline{B(x,r)}\subset \Omega$.The regularization $s_n$ of $s$, is a sequence of subharmonic functions in $B(x,r)$ which converges decreasingly to $u$ in $B(x,r)$. Note that (Green's theorem) \begin{eqnarray}  \int_{B(x,r)}|\Delta s_n| &=& \int_{B(x,r)} \Delta s_n      \nonumber \\    &=& \int_{\partial B(x,r)} \frac{\partial s_n}{\partial\eta} \nonumber \\    &=& r^{N-1}\frac{\partial}{\partial r}\left(\frac{1}{r^{N-1}}\int_{\partial B(x,r)}s_n\right)  \end{eqnarray} Does anyone knows how to bound the right hand side uniformly in $n$? Also, note that the right hand side, can be written in terms of the average integral of $s_n$.",,"['analysis', 'partial-differential-equations', 'potential-theory']"
79,"find a $B_{n,j}$ such that $|A_{n,j}-L_j| \leq B_{n,j}$ $\forall n,j$ and $\sum_{j=0}^{\infty}B_{n,j}$ converges",find a  such that   and  converges,"B_{n,j} |A_{n,j}-L_j| \leq B_{n,j} \forall n,j \sum_{j=0}^{\infty}B_{n,j}","We have $A_{n,j}= 3(-1)^j2^{n-j+1}\frac{(2(n-j)-4)!}{(n-j)!(n-j-2)!}\binom{j+2}{2}\frac{n^\frac{5}{2}}{8^n}$ and $L_j=(-\frac{1}{8})^j\binom{j+2}{2}\frac{3}{8\sqrt{\pi}}$ So I know $\lim_{n \to \infty} A_{n,j} = L_j$ implies $\lim_{n \to \infty}  \sum_{j=0}^{1000} A_{n,j}=\sum_{j=0}^{1000}L_j $. Now I want to prove that $\lim_{n \to \infty}  \sum_{j=0}^{n} A_{n,j}=\sum_{j=0}^{\infty}L_j $ So I was thinking that I want to show that $\forall$ small $\epsilon > 0, \exists$ large $N$ such that $   |\sum_{j=0}^{N} A_{N,j}-\sum_{j=0}^{\infty}L_j | < \epsilon$. I know that $|\sum_{i=0}^{N} A_{N,j}-\sum_{j=0}^{\infty}L_j | \leq \sum_{j=0}^{N}|A_{N,j}-L_j|+|\sum_{j=N+1}^{\infty}L_j|$. Since $|\sum_{j=N+1}^{\infty}L_j|$ converges say to less than $\frac{\epsilon}{2}$, we need to show that $\sum_{j=0}^{N}|A_{N,j}-L_j|$ is less than $\frac{\epsilon}{2}$. This is similar to the Wierstrauss M-test, I would like to find a $B_{j}$ such that $|A_{n,j}-L_j| \leq |A_{n,j}| \leq B_{j}$  $\forall n,j$ and  $\sum_{j=0}^{\infty}B_{j}$ converges. How do I find such $B_{j}$? Need some help.","We have $A_{n,j}= 3(-1)^j2^{n-j+1}\frac{(2(n-j)-4)!}{(n-j)!(n-j-2)!}\binom{j+2}{2}\frac{n^\frac{5}{2}}{8^n}$ and $L_j=(-\frac{1}{8})^j\binom{j+2}{2}\frac{3}{8\sqrt{\pi}}$ So I know $\lim_{n \to \infty} A_{n,j} = L_j$ implies $\lim_{n \to \infty}  \sum_{j=0}^{1000} A_{n,j}=\sum_{j=0}^{1000}L_j $. Now I want to prove that $\lim_{n \to \infty}  \sum_{j=0}^{n} A_{n,j}=\sum_{j=0}^{\infty}L_j $ So I was thinking that I want to show that $\forall$ small $\epsilon > 0, \exists$ large $N$ such that $   |\sum_{j=0}^{N} A_{N,j}-\sum_{j=0}^{\infty}L_j | < \epsilon$. I know that $|\sum_{i=0}^{N} A_{N,j}-\sum_{j=0}^{\infty}L_j | \leq \sum_{j=0}^{N}|A_{N,j}-L_j|+|\sum_{j=N+1}^{\infty}L_j|$. Since $|\sum_{j=N+1}^{\infty}L_j|$ converges say to less than $\frac{\epsilon}{2}$, we need to show that $\sum_{j=0}^{N}|A_{N,j}-L_j|$ is less than $\frac{\epsilon}{2}$. This is similar to the Wierstrauss M-test, I would like to find a $B_{j}$ such that $|A_{n,j}-L_j| \leq |A_{n,j}| \leq B_{j}$  $\forall n,j$ and  $\sum_{j=0}^{\infty}B_{j}$ converges. How do I find such $B_{j}$? Need some help.",,"['real-analysis', 'sequences-and-series', 'analysis', 'summation']"
80,Differentiation under the integral if and only if we have an $L^1$ dominator,Differentiation under the integral if and only if we have an  dominator,L^1,"Let $f(x)\in L^2(\mathbb{R})$ and define $$g(t) = \int_\mathbb{R} f^2(x)\exp(-tx^2)dx$$ for $t\geq0$ . We want to show that $g(t)$ is continuously differentiable if and only if $xf(x)\in L^2(\mathbb{R})$ . If we have that $xf(x)\in L^2(\mathbb{R})$ then we can use the standard Lebesgue Dominated Convergence technique to show that $g(t)$ is differentiable, and that its derivative is continuous. I'm having trouble, however, showing that $g'(t)$ continuous means that $xf(x)\in L^2(\mathbb{R})$ . My attempt: It seems intuitive to me that if $g(t)$ is continuously differentiable, it's derivative must be defined by what we'd expect: $$g'(t) = \int_\mathbb{R} -x^2f^2(x)\exp(-tx^2)dx,$$ but I don't know how I would go about proving this. If we have this fact, then $g'(0)<\infty\Rightarrow xf(x)\in L^2(\mathbb{R})$ and we're done. Any thoughts?","Let and define for . We want to show that is continuously differentiable if and only if . If we have that then we can use the standard Lebesgue Dominated Convergence technique to show that is differentiable, and that its derivative is continuous. I'm having trouble, however, showing that continuous means that . My attempt: It seems intuitive to me that if is continuously differentiable, it's derivative must be defined by what we'd expect: but I don't know how I would go about proving this. If we have this fact, then and we're done. Any thoughts?","f(x)\in L^2(\mathbb{R}) g(t) = \int_\mathbb{R} f^2(x)\exp(-tx^2)dx t\geq0 g(t) xf(x)\in L^2(\mathbb{R}) xf(x)\in L^2(\mathbb{R}) g(t) g'(t) xf(x)\in L^2(\mathbb{R}) g(t) g'(t) = \int_\mathbb{R} -x^2f^2(x)\exp(-tx^2)dx, g'(0)<\infty\Rightarrow xf(x)\in L^2(\mathbb{R})","['real-analysis', 'analysis', 'functional-analysis', 'measure-theory']"
81,Triangle inequality and homomorphisms,Triangle inequality and homomorphisms,,"Here is my situation: I have two homomorphisms $f$ and $g$ from a group $A$ into the complex numbers $\mathbb{C}$. I know that they are 'close' on a subset $B \subseteq A$. More formally there is an $\epsilon > 0$ so that $|f(b)-g(b)| < \epsilon$ for all $b \in B$. Now let $a \in A$ be fixed. I am wondering if there is a $\delta > 0$ such that $|f(ab) - g(ab)|<\delta$ for all $b \in B$. That is are $f$ and $g$ also 'close' on $aB$? I know that $|f(ab)-g(ab)| = |f(a)f(b)-g(a)g(b)|$ and I feel like I have seen a triangle inequality argument that can be applied to things of this form in the past. However, I do not remember the details and I have been unable to reconstruct it myself. Help would be much appreciated.","Here is my situation: I have two homomorphisms $f$ and $g$ from a group $A$ into the complex numbers $\mathbb{C}$. I know that they are 'close' on a subset $B \subseteq A$. More formally there is an $\epsilon > 0$ so that $|f(b)-g(b)| < \epsilon$ for all $b \in B$. Now let $a \in A$ be fixed. I am wondering if there is a $\delta > 0$ such that $|f(ab) - g(ab)|<\delta$ for all $b \in B$. That is are $f$ and $g$ also 'close' on $aB$? I know that $|f(ab)-g(ab)| = |f(a)f(b)-g(a)g(b)|$ and I feel like I have seen a triangle inequality argument that can be applied to things of this form in the past. However, I do not remember the details and I have been unable to reconstruct it myself. Help would be much appreciated.",,"['group-theory', 'analysis', 'inequality']"
82,Find $x > 0$ for which $\int_{0}^{x} [t]^2 \ dt = 2 (x-1)$.,Find  for which .,x > 0 \int_{0}^{x} [t]^2 \ dt = 2 (x-1),"What are all possible $x > 0$ for which the following equation is satisfied?  $$\int_{0}^{x} [t]^2 \ dt = 2 (x-1),$$ where $[.]$ denotes the bracket (or floor) function. I guess we will have to consider different cases depending on whether $\sqrt{n-1} \leq x \leq \sqrt{n}$ for each positive integer $n$. For $0 \leq x \leq 1$, the integral on the left-hand side equals $1$; so the solution in this case is clearly $x = 1$ alone. What are all possible solutions for $x > 0$?","What are all possible $x > 0$ for which the following equation is satisfied?  $$\int_{0}^{x} [t]^2 \ dt = 2 (x-1),$$ where $[.]$ denotes the bracket (or floor) function. I guess we will have to consider different cases depending on whether $\sqrt{n-1} \leq x \leq \sqrt{n}$ for each positive integer $n$. For $0 \leq x \leq 1$, the integral on the left-hand side equals $1$; so the solution in this case is clearly $x = 1$ alone. What are all possible solutions for $x > 0$?",,"['calculus', 'integration', 'analysis', 'elementary-number-theory', 'definite-integrals']"
83,Election measurable in uniform continuity,Election measurable in uniform continuity,,"Let $f:[0,1]\times [0,1] \rightarrow \mathbb{R}$ borel measurable such that for all $x \in [0,1]$ $f(x,-):[0,1] \rightarrow \mathbb{R}$ is continuous, in particular uniformly continuous. Then there $\delta(x)>0$ such that for $\vert w-s\vert < \delta(x)$ implies $\vert f(x,w)-f(x,s)\vert <1$ as is well known. My question is if $\delta(x) $ may be chosen so that $x \mapsto \delta(x)$  is  Borel measurable ? I appreciate any suggestions","Let $f:[0,1]\times [0,1] \rightarrow \mathbb{R}$ borel measurable such that for all $x \in [0,1]$ $f(x,-):[0,1] \rightarrow \mathbb{R}$ is continuous, in particular uniformly continuous. Then there $\delta(x)>0$ such that for $\vert w-s\vert < \delta(x)$ implies $\vert f(x,w)-f(x,s)\vert <1$ as is well known. My question is if $\delta(x) $ may be chosen so that $x \mapsto \delta(x)$  is  Borel measurable ? I appreciate any suggestions",,"['analysis', 'measure-theory', 'functions']"
84,Uniform continuity and translation invariance,Uniform continuity and translation invariance,,"Consider the function $a(s)=\dfrac{1}{1+s^2}$ and the space  $X=\{f:\mathbb{R}\to \mathbb{R}$ such that $t\mapsto a(t)f(t)$ is bounded uniformly continuous$ \}$. I want to show that $X$ is translation invariant, i.e. if $f\in X$ then $f_t\in X$ for all $t\in \mathbb{R}$, where $f_t$ is the $t$-translation of $f$ defined by $f_t(s)=f(t+s)$. I showed that if the function $s\mapsto a(s)f(s)$ is bounded then the function $s\mapsto a(s)f_t(s)$ is also bounded. In fact $$\sup_{s\in \mathbb{R}}\left\lvert a(s)f_t(s)\right\rvert=\sup_{s\in \mathbb{R}}\left\lvert a(s)f(t+s)\right\rvert=\sup_{s\in \mathbb{R}}\left\lvert a(s-t)f(s)\right\rvert\leq M_t\sup_{s\in \mathbb{R}} \left\lvert a(s)f(s)\right\rvert,$$ because we know that there's a constant $M_t$  such that for all $s\in \mathbb{R} $  $$a(s-t)=\dfrac{1}{1+(s-t)^2}\leq M_t \dfrac{1}{1+s^2}=M_t a(s).$$ Now I just need to prove that if the function $s\mapsto a(s)f(s)$ is uniformly continuous,  then the function $s\mapsto a(s)f_t(s)$ is also uniformly continuous.","Consider the function $a(s)=\dfrac{1}{1+s^2}$ and the space  $X=\{f:\mathbb{R}\to \mathbb{R}$ such that $t\mapsto a(t)f(t)$ is bounded uniformly continuous$ \}$. I want to show that $X$ is translation invariant, i.e. if $f\in X$ then $f_t\in X$ for all $t\in \mathbb{R}$, where $f_t$ is the $t$-translation of $f$ defined by $f_t(s)=f(t+s)$. I showed that if the function $s\mapsto a(s)f(s)$ is bounded then the function $s\mapsto a(s)f_t(s)$ is also bounded. In fact $$\sup_{s\in \mathbb{R}}\left\lvert a(s)f_t(s)\right\rvert=\sup_{s\in \mathbb{R}}\left\lvert a(s)f(t+s)\right\rvert=\sup_{s\in \mathbb{R}}\left\lvert a(s-t)f(s)\right\rvert\leq M_t\sup_{s\in \mathbb{R}} \left\lvert a(s)f(s)\right\rvert,$$ because we know that there's a constant $M_t$  such that for all $s\in \mathbb{R} $  $$a(s-t)=\dfrac{1}{1+(s-t)^2}\leq M_t \dfrac{1}{1+s^2}=M_t a(s).$$ Now I just need to prove that if the function $s\mapsto a(s)f(s)$ is uniformly continuous,  then the function $s\mapsto a(s)f_t(s)$ is also uniformly continuous.",,"['real-analysis', 'analysis', 'uniform-continuity']"
85,a function with infinity L^p norm,a function with infinity L^p norm,,"Let $1\leq p<\infty$, $1/p+1/q=1$. For a function $f$ with $||f||_q=\infty$, can we write $$ ||f||_q=\sup_{g\in L^p(\Omega),||g||_p\neq 0}\frac{\int_\Omega |fg|}{||g||_p}? $$ or $$ ||f||_q=\sup_{||g||_p\leq 1}{\int_\Omega |fg|}? $$ In the case $||f||_q<\infty$, since $L^q(\Omega)$ is the dual space of $L^p(\Omega)$, with the $L^q$ norm equal to the norm of the bounded linear functional on $L^p$,  we have $$ ||f||_q=\sup_{||g||_p=1}{\int_\Omega |fg|}=\sup_{||g||_p\leq 1}{\int_\Omega |fg|}=\sup_{g\in L^p(\Omega),||g||_p\neq 0}\frac{\int_\Omega |fg|}{||g||_p}. $$ Thanks.","Let $1\leq p<\infty$, $1/p+1/q=1$. For a function $f$ with $||f||_q=\infty$, can we write $$ ||f||_q=\sup_{g\in L^p(\Omega),||g||_p\neq 0}\frac{\int_\Omega |fg|}{||g||_p}? $$ or $$ ||f||_q=\sup_{||g||_p\leq 1}{\int_\Omega |fg|}? $$ In the case $||f||_q<\infty$, since $L^q(\Omega)$ is the dual space of $L^p(\Omega)$, with the $L^q$ norm equal to the norm of the bounded linear functional on $L^p$,  we have $$ ||f||_q=\sup_{||g||_p=1}{\int_\Omega |fg|}=\sup_{||g||_p\leq 1}{\int_\Omega |fg|}=\sup_{g\in L^p(\Omega),||g||_p\neq 0}\frac{\int_\Omega |fg|}{||g||_p}. $$ Thanks.",,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'lp-spaces']"
86,Show that $e^{-a|x|}$ does not belong to Schwartz space,Show that  does not belong to Schwartz space,e^{-a|x|},"Let $f : \mathbb R \to \mathbb R$ and $a > 0$ given by $f(x) = e^{-a|x|}$. Show that $f$ is rapidly decreasing and belongs to $L_1(\mathbb R)$, but not to $\mathcal S(\mathbb R)$. I had shown that it is rapidly decreasing and in $L_1(\mathbb R)$, but I am unsure about $\mathcal S(\mathbb R)$. In my opinion it belongs to $\mathcal S(\mathbb R)$...","Let $f : \mathbb R \to \mathbb R$ and $a > 0$ given by $f(x) = e^{-a|x|}$. Show that $f$ is rapidly decreasing and belongs to $L_1(\mathbb R)$, but not to $\mathcal S(\mathbb R)$. I had shown that it is rapidly decreasing and in $L_1(\mathbb R)$, but I am unsure about $\mathcal S(\mathbb R)$. In my opinion it belongs to $\mathcal S(\mathbb R)$...",,"['analysis', 'functions', 'schwartz-space']"
87,"Show that a subset $V \subseteq C[a,b]$ is a Haar subspace",Show that a subset  is a Haar subspace,"V \subseteq C[a,b]","Let $C[a,b]$ be the set of continuous functions on $[a,b]$, then a linear subspace $V \subseteq C[a,b]$ of finite dimension $n+1$ is called an Haar subspace iff one of the following equivalent conditions hold: 1) every non-zero element $p \in V$ has at most $n$ zeros, 2) for $n+1$ pairs $(t_i, f_i)$ there exists exactly one $p \in V$ interpolating between those values 3) is $h_0, \ldots, h_n$ any base for $V$, and are $t_0, \ldots, t_n \in [a,b]$ distinct point, we have $$  \det \begin{pmatrix} h_0(t_0) & \cdots & h_n(t_0) \\                       \vdots   & & \vdots \\                       h_n(t_n) & & h_n(t_n) \end{pmatrix} \ne 0. $$ Now I want to show that the set $\{ 1, x, xe^{2x} \}$ is a Haar subspace of $C[0,1]$. But the difficulty arises with the non-linearity of the involved equations, which I cannot solve. So do you have any ideas how to prove this?","Let $C[a,b]$ be the set of continuous functions on $[a,b]$, then a linear subspace $V \subseteq C[a,b]$ of finite dimension $n+1$ is called an Haar subspace iff one of the following equivalent conditions hold: 1) every non-zero element $p \in V$ has at most $n$ zeros, 2) for $n+1$ pairs $(t_i, f_i)$ there exists exactly one $p \in V$ interpolating between those values 3) is $h_0, \ldots, h_n$ any base for $V$, and are $t_0, \ldots, t_n \in [a,b]$ distinct point, we have $$  \det \begin{pmatrix} h_0(t_0) & \cdots & h_n(t_0) \\                       \vdots   & & \vdots \\                       h_n(t_n) & & h_n(t_n) \end{pmatrix} \ne 0. $$ Now I want to show that the set $\{ 1, x, xe^{2x} \}$ is a Haar subspace of $C[0,1]$. But the difficulty arises with the non-linearity of the involved equations, which I cannot solve. So do you have any ideas how to prove this?",,"['analysis', 'approximation', 'approximation-theory']"
88,"$f$ is differentiable. If $\lim_{x \to c}f'(x)$ exists, then this limit must be $f'(c)$. [duplicate]","is differentiable. If  exists, then this limit must be . [duplicate]",f \lim_{x \to c}f'(x) f'(c),"This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Please prove: Let $f:(a,b) \to R$ be differentiable function, and let $c \in (a,b).  $  If $\lim_{x \to c}f'(x)$ exists and is finite, then this limit must be $f'(c)$. I tried doing it directly but it doesn't quite work out. I experimented on some common functions (like $\sin(x), x^2$) and it seems pretty obvious. I'm thinking about using contradiction, although after writing down some definitions, I couldn't proceed. Or, probably if I could show that $f'(x)$ is continuous, then I win. Any idea? Thanks!","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Please prove: Let $f:(a,b) \to R$ be differentiable function, and let $c \in (a,b).  $  If $\lim_{x \to c}f'(x)$ exists and is finite, then this limit must be $f'(c)$. I tried doing it directly but it doesn't quite work out. I experimented on some common functions (like $\sin(x), x^2$) and it seems pretty obvious. I'm thinking about using contradiction, although after writing down some definitions, I couldn't proceed. Or, probably if I could show that $f'(x)$ is continuous, then I win. Any idea? Thanks!",,"['calculus', 'real-analysis', 'analysis', 'proof-writing']"
89,Smooth function composed with sobolev function vanishes at 0,Smooth function composed with sobolev function vanishes at 0,,"Let $\Omega$ be a bounded domain with sufficiently smooth boundary. Let $u \in W^{1, 2}_{0}(\Omega)$ and $F \in C^{\infty}(\mathbb{R} \rightarrow \mathbb{R})$ such that $F(u(x)) = 0$ for almost every $x \in \Omega$. Must $F(0) = 0$? My thoughts are: I think the answer is yes. Suppose $u$ instead was smooth. The $F(u(x)) = 0$ for every $x \in \Omega$. By continuity, $F(u(x)) = 0$ for every $x \in \overline{\Omega}$. Since $u = 0$ on the boundary $\partial \Omega$, then there exists an $x_{0}$ such that $u(x_{0}) = 0$. Thus $F(u(x_{0})) = 0$ and hence $F(0) = 0$. However, I am not sure how to argue if $u$ was not smooth. My first thought was to approximate $u \in W^{1, 2}_{0}$ by smooth functions, but then I'll be working with a Sobolev norm.","Let $\Omega$ be a bounded domain with sufficiently smooth boundary. Let $u \in W^{1, 2}_{0}(\Omega)$ and $F \in C^{\infty}(\mathbb{R} \rightarrow \mathbb{R})$ such that $F(u(x)) = 0$ for almost every $x \in \Omega$. Must $F(0) = 0$? My thoughts are: I think the answer is yes. Suppose $u$ instead was smooth. The $F(u(x)) = 0$ for every $x \in \Omega$. By continuity, $F(u(x)) = 0$ for every $x \in \overline{\Omega}$. Since $u = 0$ on the boundary $\partial \Omega$, then there exists an $x_{0}$ such that $u(x_{0}) = 0$. Thus $F(u(x_{0})) = 0$ and hence $F(0) = 0$. However, I am not sure how to argue if $u$ was not smooth. My first thought was to approximate $u \in W^{1, 2}_{0}$ by smooth functions, but then I'll be working with a Sobolev norm.",,"['real-analysis', 'analysis', 'partial-differential-equations', 'sobolev-spaces']"
90,Weak Convergence of a Sequence of Functions.,Weak Convergence of a Sequence of Functions.,,"To begin my question I wish to first clarify the definition of weak convergence FOR a sequence of functions. We say that given sequence of functions, $\{f_{n}\}_{n=1}^{\infty}$, such that each $f_n$ is continuous in $[a,b]$, CONVERGES WEAKLY to $f$ if: $\underset{n \to \infty}{\lim} \int_{a}^{b}f_n(x)g(x)dx = \int_{a}^{b}f(x)g(x)dx$, for all $g(x)\in C[a,b]$. Now, my real question is the following: Let $\{e_n\}_{n=1}^{\infty}$ be a sequence of functions such that $e_n(x)=e^{-inx}$. Considering Parsevals Identity, what can be said about the weak convergence of $\{e_n\}_{n=1}^{\infty}$? The first thing that came to mind was finding what the sequence converges to. Unfortunately I am having trouble evaluating $\lim_{n \to \infty}e^{-inx}=\lim_{n \to \infty}cos(nx)-i\lim_{n \to \infty}sin(nx)$. Either way, I try to relate to Parseval's identity which states that $\sum_{n=-\infty}^{\infty}|a_n|^2=\frac{1}{2\pi}\int_{-\pi}^{\pi}|f(x)|^2$ such that $a_n=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)e^{-inx}dx$ I realise that $\int_{-\pi}^{\pi}f(x)e^{-inx}dx$ is familiar with my weak convergence definition, but I am unsure of how to progress from there. Since the limits from Parseval's Identity are rather fixed. I would assume that I am trying to show weak convergence for $e_n(x) \in C[-\pi,\pi]$. Thank you in advanced for your help.","To begin my question I wish to first clarify the definition of weak convergence FOR a sequence of functions. We say that given sequence of functions, $\{f_{n}\}_{n=1}^{\infty}$, such that each $f_n$ is continuous in $[a,b]$, CONVERGES WEAKLY to $f$ if: $\underset{n \to \infty}{\lim} \int_{a}^{b}f_n(x)g(x)dx = \int_{a}^{b}f(x)g(x)dx$, for all $g(x)\in C[a,b]$. Now, my real question is the following: Let $\{e_n\}_{n=1}^{\infty}$ be a sequence of functions such that $e_n(x)=e^{-inx}$. Considering Parsevals Identity, what can be said about the weak convergence of $\{e_n\}_{n=1}^{\infty}$? The first thing that came to mind was finding what the sequence converges to. Unfortunately I am having trouble evaluating $\lim_{n \to \infty}e^{-inx}=\lim_{n \to \infty}cos(nx)-i\lim_{n \to \infty}sin(nx)$. Either way, I try to relate to Parseval's identity which states that $\sum_{n=-\infty}^{\infty}|a_n|^2=\frac{1}{2\pi}\int_{-\pi}^{\pi}|f(x)|^2$ such that $a_n=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)e^{-inx}dx$ I realise that $\int_{-\pi}^{\pi}f(x)e^{-inx}dx$ is familiar with my weak convergence definition, but I am unsure of how to progress from there. Since the limits from Parseval's Identity are rather fixed. I would assume that I am trying to show weak convergence for $e_n(x) \in C[-\pi,\pi]$. Thank you in advanced for your help.",,"['general-topology', 'sequences-and-series', 'analysis', 'functions', 'weak-convergence']"
91,If $f$ is twice differentiable then $f^{-1}$ is twice differentiable,If  is twice differentiable then  is twice differentiable,f f^{-1},"$f:(a,b) \rightarrow (c,d)$ is a bijection and $f$ is differentibale with $f'(x) \neq 0$ for all $x \in (a,b)$, then $f^{-1}$ is also everywhere differentiable. Show that if $f$ is twice differentiable then so is $f^{-1}$ and write down the formula for    $(f^{-1})''$. Now my attempt at this question so far, I do not know how to show that ths $f$ being twice differentiable implies that $f^{-1}$ is also twice differentiable. But I did attempt the second part of the question By inverse function theorem we know $$(f^{-1})'(f(x))= \dfrac{1}{f'(x)}$$ now differentiating by chain rule this we get $$f'(x)(f^{-1})''(f(x))=\dfrac{-f''(x)}{(f'(x))^{2}}$$ and rearranging; $$(f^{-1})''(f(x))=\dfrac{-f''(x)}{(f'(x))^{3}}$$ Any hints or help for the first part of th question would be much appreciated. REMARK; If somebody could advise me as to how to write fractions where the middle line isn't missing, I have tried \frac and \dfrac but both to no success.","$f:(a,b) \rightarrow (c,d)$ is a bijection and $f$ is differentibale with $f'(x) \neq 0$ for all $x \in (a,b)$, then $f^{-1}$ is also everywhere differentiable. Show that if $f$ is twice differentiable then so is $f^{-1}$ and write down the formula for    $(f^{-1})''$. Now my attempt at this question so far, I do not know how to show that ths $f$ being twice differentiable implies that $f^{-1}$ is also twice differentiable. But I did attempt the second part of the question By inverse function theorem we know $$(f^{-1})'(f(x))= \dfrac{1}{f'(x)}$$ now differentiating by chain rule this we get $$f'(x)(f^{-1})''(f(x))=\dfrac{-f''(x)}{(f'(x))^{2}}$$ and rearranging; $$(f^{-1})''(f(x))=\dfrac{-f''(x)}{(f'(x))^{3}}$$ Any hints or help for the first part of th question would be much appreciated. REMARK; If somebody could advise me as to how to write fractions where the middle line isn't missing, I have tried \frac and \dfrac but both to no success.",,"['real-analysis', 'analysis', 'derivatives']"
92,Some questions about proof of Theorem 2.43 in Baby Rudin,Some questions about proof of Theorem 2.43 in Baby Rudin,,"I will include the proof here and highlight the parts that are giving me trouble. Theorem $\hspace{5 pt}$ Let $P$ be a nonempty perfect set in $\mathbb{R}^k$. Then $P$ is uncountable. Proof $\hspace{5 pt}$ Since $P$ has limit points, $P$ must be infinite. Suppose $P$ is countable, and denote the points of $P$ by $\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}, \ldots$. We shall construct a sequence $\{V_n\}$ of neighborhoods as follows. Let $V_1$ be any neighborhood of $x_1$. 1) ^ Are we using the Axiom of Choice here? How can we have an arbitrary set in a construction? If $V_1$ consists of all $\mathbf{y} \in \mathbb{R}^k$ such that $|\mathbf{y} - \mathbf{x_1}| < r$, the closure $\overline{V_1}$ of $V_1$ is the set of all $\mathbf{y} \in \mathbb{R}^k$ such that $|\mathbf{y} - \mathbf{x_1}| \leq r$. 2) ^ It makes sense intuitively, but how do we prove this last statement? Suppose $V_n$ has been constructed, so that $V_n \cap P$ is not empty. Since every point of $P$ is a limit point of $P$, there is a neighborhood $V_{n+1}$ such that (i) $\overline{V_{n+1}} \subset V_n$, (ii) $x_n \notin \overline{V_{n+1}}$, (iii) $V_{n+1} \cap P$ is not empty. By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed. 3) ^ I really don't get this whole paragraph much at all. Could someone explain it in a more step-by-step way? Put $K_n = \overline{V_n} \cap P$. Since $\overline{V_n}$ is closed and bounded, $\overline{V_n}$ is compact. 4) ^ ""closed"" comes from it being a closure and ""bounded"" comes from the definition of neighborhood, correct? Since $x_n \notin K_{n+1}$, no point of $P$ lies in $\bigcap_1^\infty K_n$. Since $K_n \subset P$, this implies that $\bigcap_1^\infty K_n$ is empty. But each $K_n$ is nonempty, by (iii), and $K_n \supset K_{n+1}$, by (i); this contradicts the Corollary to Theorem 2.36.","I will include the proof here and highlight the parts that are giving me trouble. Theorem $\hspace{5 pt}$ Let $P$ be a nonempty perfect set in $\mathbb{R}^k$. Then $P$ is uncountable. Proof $\hspace{5 pt}$ Since $P$ has limit points, $P$ must be infinite. Suppose $P$ is countable, and denote the points of $P$ by $\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}, \ldots$. We shall construct a sequence $\{V_n\}$ of neighborhoods as follows. Let $V_1$ be any neighborhood of $x_1$. 1) ^ Are we using the Axiom of Choice here? How can we have an arbitrary set in a construction? If $V_1$ consists of all $\mathbf{y} \in \mathbb{R}^k$ such that $|\mathbf{y} - \mathbf{x_1}| < r$, the closure $\overline{V_1}$ of $V_1$ is the set of all $\mathbf{y} \in \mathbb{R}^k$ such that $|\mathbf{y} - \mathbf{x_1}| \leq r$. 2) ^ It makes sense intuitively, but how do we prove this last statement? Suppose $V_n$ has been constructed, so that $V_n \cap P$ is not empty. Since every point of $P$ is a limit point of $P$, there is a neighborhood $V_{n+1}$ such that (i) $\overline{V_{n+1}} \subset V_n$, (ii) $x_n \notin \overline{V_{n+1}}$, (iii) $V_{n+1} \cap P$ is not empty. By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed. 3) ^ I really don't get this whole paragraph much at all. Could someone explain it in a more step-by-step way? Put $K_n = \overline{V_n} \cap P$. Since $\overline{V_n}$ is closed and bounded, $\overline{V_n}$ is compact. 4) ^ ""closed"" comes from it being a closure and ""bounded"" comes from the definition of neighborhood, correct? Since $x_n \notin K_{n+1}$, no point of $P$ lies in $\bigcap_1^\infty K_n$. Since $K_n \subset P$, this implies that $\bigcap_1^\infty K_n$ is empty. But each $K_n$ is nonempty, by (iii), and $K_n \supset K_{n+1}$, by (i); this contradicts the Corollary to Theorem 2.36.",,"['analysis', 'proof-verification']"
93,Measurability of upper and lower derivatives of Radon measures,Measurability of upper and lower derivatives of Radon measures,,"Let $\mu$ and $\nu$ be Radon measures in $\mathbb R^N$. Define their upper and lower derivatives by $$ \overline{D}_\nu\mu(x):=\limsup_{r\to0}\frac{\mu(B_r(x))}{\nu(B_r(x))},\qquad \underline{D}_\nu\mu(x):=\liminf_{r\to0}\frac{\mu(B_r(x))}{\nu(B_r(x))},\qquad $$ if $\nu(B_r(x))>0$ for all $r>0$, and $\overline{D}_\nu\mu(x):=\underline{D}_\nu\mu(x):=\infty$ otherwise. Here, $B_r(x):=\{y\in\mathbb R^N\ |\ |x-y|\leq r\}$ is the closed ball of radius $r$ with center $x$. I have found several books and lecture notes which state/prove that the above functions are Borel measurable, but I don't see why. One of the proofs which is almost clear to me can be found here (see Proposition 6.5). At the end of this proof, they form the limsup of an uncountable family of Borel measurable functions and conclude that this limsup is Borel measurable as well. But why? Can anyone explain this step to me please? Or give me another proof for my claim? Any help would be great! EDIT: I know proofs where outer measures are used. But I am looking for some proof without outer measures. Thx in advance.","Let $\mu$ and $\nu$ be Radon measures in $\mathbb R^N$. Define their upper and lower derivatives by $$ \overline{D}_\nu\mu(x):=\limsup_{r\to0}\frac{\mu(B_r(x))}{\nu(B_r(x))},\qquad \underline{D}_\nu\mu(x):=\liminf_{r\to0}\frac{\mu(B_r(x))}{\nu(B_r(x))},\qquad $$ if $\nu(B_r(x))>0$ for all $r>0$, and $\overline{D}_\nu\mu(x):=\underline{D}_\nu\mu(x):=\infty$ otherwise. Here, $B_r(x):=\{y\in\mathbb R^N\ |\ |x-y|\leq r\}$ is the closed ball of radius $r$ with center $x$. I have found several books and lecture notes which state/prove that the above functions are Borel measurable, but I don't see why. One of the proofs which is almost clear to me can be found here (see Proposition 6.5). At the end of this proof, they form the limsup of an uncountable family of Borel measurable functions and conclude that this limsup is Borel measurable as well. But why? Can anyone explain this step to me please? Or give me another proof for my claim? Any help would be great! EDIT: I know proofs where outer measures are used. But I am looking for some proof without outer measures. Thx in advance.",,"['analysis', 'measure-theory', 'geometric-measure-theory']"
94,Derivative and lipschitz,Derivative and lipschitz,,"If I have a real-valued continuous function defined on a compact subset of real line, such that its derivative(wherever it exists) is bounded. Is such a function necessarily Lipschitz? Additionally, this came to my mind while I was searching for the weakest condition on a continuous function on a compact set such that it is also lipschitz. Does there exist a weaker result than wanting the derivatives to be continuous on the interval(with continuous extension at the end points of the interval) ?","If I have a real-valued continuous function defined on a compact subset of real line, such that its derivative(wherever it exists) is bounded. Is such a function necessarily Lipschitz? Additionally, this came to my mind while I was searching for the weakest condition on a continuous function on a compact set such that it is also lipschitz. Does there exist a weaker result than wanting the derivatives to be continuous on the interval(with continuous extension at the end points of the interval) ?",,"['analysis', 'derivatives', 'convergence-divergence']"
95,Proving inequality $x^xy^y \geq (\frac{x+y}{2})^{x+y}$,Proving inequality,x^xy^y \geq (\frac{x+y}{2})^{x+y},"Prove that for all $x,y>0$ the following inequality $x^xy^y \geq (\frac{x+y}{2})^{x+y}$ is true. It smells like Jensen inequality, but all I can get is that $\frac{x+y}{2}ln(x) + \frac{x+y}{2} ln(y) \geq xln(\frac{x+y}{2})+yln(\frac{x+y}{2})$","Prove that for all $x,y>0$ the following inequality $x^xy^y \geq (\frac{x+y}{2})^{x+y}$ is true. It smells like Jensen inequality, but all I can get is that $\frac{x+y}{2}ln(x) + \frac{x+y}{2} ln(y) \geq xln(\frac{x+y}{2})+yln(\frac{x+y}{2})$",,['analysis']
96,Count PI with analytical methods,Count PI with analytical methods,,"Is there a differential equation which can be used to count the value of pi? I was able to describe pi only with sequences based on polygons with infinite corners. I think I'll need a continuous variable to describe it with a differential equation, but the number of the corners can be only integer...","Is there a differential equation which can be used to count the value of pi? I was able to describe pi only with sequences based on polygons with infinite corners. I think I'll need a continuous variable to describe it with a differential equation, but the number of the corners can be only integer...",,"['analysis', 'geometry']"
97,Weak convergence in a subspace,Weak convergence in a subspace,,Let $V$ be a normed linear space and $W$ a closed subspace of $V$. Suppose a sequence $\{w_{n}\} \subset W$ and $w \in W$ with $w_{n}$ converges to $w$ weakly in $V$. Why must $w_{n}$ converge weakly to $w$ in $W$?,Let $V$ be a normed linear space and $W$ a closed subspace of $V$. Suppose a sequence $\{w_{n}\} \subset W$ and $w \in W$ with $w_{n}$ converges to $w$ weakly in $V$. Why must $w_{n}$ converge weakly to $w$ in $W$?,,"['real-analysis', 'analysis', 'functional-analysis']"
98,Does $dx$ in the formula $\int f(x)dx$ represents a differential of x?,Does  in the formula  represents a differential of x?,dx \int f(x)dx,"While I asked a question about integrals Is $dx\,dy$ really a multiplication of $dx$ and $dy$? , I found out that many of the answers were assuming that dx is just a notation in the formula $\int f(x)dx$. I'm not convinced with that, so I decided to ask this question, because the answer of this question will imply the answer of the question I've set before. So, does $dx$ in the formula $\int f(x)dx$ represents a differential of x or it's just part of a notation?","While I asked a question about integrals Is $dx\,dy$ really a multiplication of $dx$ and $dy$? , I found out that many of the answers were assuming that dx is just a notation in the formula $\int f(x)dx$. I'm not convinced with that, so I decided to ask this question, because the answer of this question will imply the answer of the question I've set before. So, does $dx$ in the formula $\int f(x)dx$ represents a differential of x or it's just part of a notation?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'differential']"
99,Continuous complex measures,Continuous complex measures,,"I was wondering if following statements are true: If $\mu$ is the Lebesgue measure and $\mu(A)=\alpha_0$, then it's not difficult to verify that for any $\alpha<\alpha_0$, there exist $B\subset A$ with $\mu(B)=\alpha$. Then I was thinking if the same is true for any complex Borel measure $\lambda$ absolutely continuous with respect to Lebesgue measure. That is, if $\lambda$ is a complex Borel measure, $\lambda\ll\mu$ and $\lambda(A)=\alpha_0\in\mathbb{R}^+$, is it true that for any $0<\alpha<\alpha_0 $, there exists $B\subset A$ such that $\lambda(B)=\alpha$? If not, does it hold if I add the assumption that $\lambda$ is real valued? Also I was wondering about generalization of above statement. If $\lambda_1$ and $\lambda_2$ are complex Borel measures and absolutely continuous with respect to Lebesgue measure and $\lambda_1(A)=\lambda_2(A)=\alpha_0\in\mathbb{R}^+$, is it true that for any $0<\alpha<\alpha_0 $, there exist $B\subset A$ such that $\lambda_1(B)=\lambda_2(B)=\alpha$? If not, does it hold if I add the assumption that $\lambda_1$ and $\lambda_2$ are real valued? It would be great if someone could help me with this, or at least let me know about a reference to read about this.","I was wondering if following statements are true: If $\mu$ is the Lebesgue measure and $\mu(A)=\alpha_0$, then it's not difficult to verify that for any $\alpha<\alpha_0$, there exist $B\subset A$ with $\mu(B)=\alpha$. Then I was thinking if the same is true for any complex Borel measure $\lambda$ absolutely continuous with respect to Lebesgue measure. That is, if $\lambda$ is a complex Borel measure, $\lambda\ll\mu$ and $\lambda(A)=\alpha_0\in\mathbb{R}^+$, is it true that for any $0<\alpha<\alpha_0 $, there exists $B\subset A$ such that $\lambda(B)=\alpha$? If not, does it hold if I add the assumption that $\lambda$ is real valued? Also I was wondering about generalization of above statement. If $\lambda_1$ and $\lambda_2$ are complex Borel measures and absolutely continuous with respect to Lebesgue measure and $\lambda_1(A)=\lambda_2(A)=\alpha_0\in\mathbb{R}^+$, is it true that for any $0<\alpha<\alpha_0 $, there exist $B\subset A$ such that $\lambda_1(B)=\lambda_2(B)=\alpha$? If not, does it hold if I add the assumption that $\lambda_1$ and $\lambda_2$ are real valued? It would be great if someone could help me with this, or at least let me know about a reference to read about this.",,"['analysis', 'measure-theory']"
