,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proof of squeeze theorem for functions,Proof of squeeze theorem for functions,,"Suppose for all $x$ we know $g(x)\le f(x)\le h(x)$ and $\lim_{x\to c} g(x)=L=\lim_{x\to c} h(x)$. Does the following argument work to conclude that $\lim_{x\to c} f(x)=L$? Let $\epsilon\gt 0$ be given. Then we can find a $\delta_1$ such that if $|x-c|\lt\delta_1$, then $|g(x)-L|\lt\epsilon$ and a $\delta_2$ such that if $|x-c|\lt\delta_2$ then $|h(x)-L|\lt\epsilon$. Let $\delta = min\{\delta_1,\delta_2\}$. Then for all x such that $|x-c|\lt\delta$, it follows that $|g(x)-L|\lt\epsilon$ and $|h(x)-L|\lt\epsilon$. This means that $L-\epsilon\lt g(x)\le h(x)\lt L+\epsilon$. But since $g(x)\le f(x)\le h(x)$,  $L-\epsilon\lt f(x) \lt L+\epsilon$. Hence $\lim_{x\to c}f(x)=L$. This proof is slightly different from others that I've seen, but it doesn't seem to be wrong. Is there anything that I'm missing?","Suppose for all $x$ we know $g(x)\le f(x)\le h(x)$ and $\lim_{x\to c} g(x)=L=\lim_{x\to c} h(x)$. Does the following argument work to conclude that $\lim_{x\to c} f(x)=L$? Let $\epsilon\gt 0$ be given. Then we can find a $\delta_1$ such that if $|x-c|\lt\delta_1$, then $|g(x)-L|\lt\epsilon$ and a $\delta_2$ such that if $|x-c|\lt\delta_2$ then $|h(x)-L|\lt\epsilon$. Let $\delta = min\{\delta_1,\delta_2\}$. Then for all x such that $|x-c|\lt\delta$, it follows that $|g(x)-L|\lt\epsilon$ and $|h(x)-L|\lt\epsilon$. This means that $L-\epsilon\lt g(x)\le h(x)\lt L+\epsilon$. But since $g(x)\le f(x)\le h(x)$,  $L-\epsilon\lt f(x) \lt L+\epsilon$. Hence $\lim_{x\to c}f(x)=L$. This proof is slightly different from others that I've seen, but it doesn't seem to be wrong. Is there anything that I'm missing?",,['real-analysis']
1,How to learn Integral Transform?,How to learn Integral Transform?,,"I major in Electronic Engineering when back in college. I learned the Fourier Transform, Laplace Transform, Z Transform and wavelet Transform. But I always feel a lack of thorough understanding of the mathematical logic behind these calculations. So when I do such calculations, it's more like follow my habit than logical reasoning . I think this is because I don't have a complete picture of the background math knowledge. So I want to spend some time (about one year) to make up for it. Otherwise it'll be a pity for my life. My math background: Calculus Linear Algebra Could some one list the knowledge I should learn to fully understand Integral Transform? (And some book recommendations are appreciated.) (I choose the related tags based on my own guess. Excuse me if it is not appropriate.) Add 1 2016/2/22 During my searching, I found some articles/books useful to me. I will keep adding links to them below. Maybe they are just remotely related to this question. But they do make me aware of something new. The Axiom of Choice in an Elementary Theory of Operations and Sets","I major in Electronic Engineering when back in college. I learned the Fourier Transform, Laplace Transform, Z Transform and wavelet Transform. But I always feel a lack of thorough understanding of the mathematical logic behind these calculations. So when I do such calculations, it's more like follow my habit than logical reasoning . I think this is because I don't have a complete picture of the background math knowledge. So I want to spend some time (about one year) to make up for it. Otherwise it'll be a pity for my life. My math background: Calculus Linear Algebra Could some one list the knowledge I should learn to fully understand Integral Transform? (And some book recommendations are appreciated.) (I choose the related tags based on my own guess. Excuse me if it is not appropriate.) Add 1 2016/2/22 During my searching, I found some articles/books useful to me. I will keep adding links to them below. Maybe they are just remotely related to this question. But they do make me aware of something new. The Axiom of Choice in an Elementary Theory of Operations and Sets",,"['calculus', 'real-analysis', 'analysis', 'functional-analysis', 'integral-transforms']"
2,Proving two measures of Borel sigma-algebra are equal,Proving two measures of Borel sigma-algebra are equal,,"I am working on this problem on measure theory like this: Let $X$ be set of $\mathbb R$, and let $\mathcal B$ be its Borel $\sigma$-algebra, and finally let $\mu_1$ and $\mu_2$ be the two measures on $(X,\mathcal B)$ such that $\mu_1((a,b))= \mu_2((a,b)) < \infty$ whenever $−\infty < a < b < \infty$. Show that $\mu_1(A) = \mu_2(A)$ whenever $A \in \mathcal B$.​ Here is what I was at first thinking: Since $a,b \in \mathbb R$ and since $A$ is an arbitrary subset of $\mathcal B$, so if only I can prove that $(a,b) \in \mathcal B$, then I am done. But I was told by a responder to my posting at Physics Forum here that this reasoning is wrong, since not all sets in $\mathcal B$ are open. I am hitting a deadend again. Therefore I am posting this question here looking for help, thanks for your time and effort. POST SCRIPT - 1: I should have mentioned this: This problem comes from the 3rd. chapter of an introductory text by Richard F. Bass here , therefore any solution shouldn't involve any advanced theorems such as Dynkin's, etc. Sorry for this belated info, thanks though to all who have taken time to help. POST SCRIPT - 2 : I finally came up with solution without any advanced theorems, adapted from a solution by @JoshKeneda, who used Dynkin's Theorem. I have submitted this work to my professor, he ok'd it except for (5) because it is true only when the $A_i$'s are pairwise disjoint. Feel free to drop me a message if you have ideas to improve (5). Thanks to all and especially to @JoshKeneda. DISCLOSURE: This question is very similar to an old MSE posting here , which was put on hold due to being incomplete. My posting has all the correction to the first posting. Always conscientious of community rule and guideline, I have tried avoiding duplication by posting this question elsewhere here and here , but I did not receive any meaningful helps $-$ understandably, as those two outside forums are not specialized in math. Thank you for your understanding.","I am working on this problem on measure theory like this: Let $X$ be set of $\mathbb R$, and let $\mathcal B$ be its Borel $\sigma$-algebra, and finally let $\mu_1$ and $\mu_2$ be the two measures on $(X,\mathcal B)$ such that $\mu_1((a,b))= \mu_2((a,b)) < \infty$ whenever $−\infty < a < b < \infty$. Show that $\mu_1(A) = \mu_2(A)$ whenever $A \in \mathcal B$.​ Here is what I was at first thinking: Since $a,b \in \mathbb R$ and since $A$ is an arbitrary subset of $\mathcal B$, so if only I can prove that $(a,b) \in \mathcal B$, then I am done. But I was told by a responder to my posting at Physics Forum here that this reasoning is wrong, since not all sets in $\mathcal B$ are open. I am hitting a deadend again. Therefore I am posting this question here looking for help, thanks for your time and effort. POST SCRIPT - 1: I should have mentioned this: This problem comes from the 3rd. chapter of an introductory text by Richard F. Bass here , therefore any solution shouldn't involve any advanced theorems such as Dynkin's, etc. Sorry for this belated info, thanks though to all who have taken time to help. POST SCRIPT - 2 : I finally came up with solution without any advanced theorems, adapted from a solution by @JoshKeneda, who used Dynkin's Theorem. I have submitted this work to my professor, he ok'd it except for (5) because it is true only when the $A_i$'s are pairwise disjoint. Feel free to drop me a message if you have ideas to improve (5). Thanks to all and especially to @JoshKeneda. DISCLOSURE: This question is very similar to an old MSE posting here , which was put on hold due to being incomplete. My posting has all the correction to the first posting. Always conscientious of community rule and guideline, I have tried avoiding duplication by posting this question elsewhere here and here , but I did not receive any meaningful helps $-$ understandably, as those two outside forums are not specialized in math. Thank you for your understanding.",,"['real-analysis', 'analysis', 'measure-theory']"
3,Is there a differentiable function $f: {\mathbb R}_+ \to {\mathbb R}_+$ such that $f'(x) > f(x)^2$ for all $x$?,Is there a differentiable function  such that  for all ?,f: {\mathbb R}_+ \to {\mathbb R}_+ f'(x) > f(x)^2 x,Is there a differentiable function $f: {\mathbb R}_+ \to {\mathbb R}_+$ such that $f'(x) > f(x)^2$ for all $x$ ? It seems like the answer is no because such a function should have a vertical asymptote at some point.,Is there a differentiable function such that for all ? It seems like the answer is no because such a function should have a vertical asymptote at some point.,f: {\mathbb R}_+ \to {\mathbb R}_+ f'(x) > f(x)^2 x,"['real-analysis', 'ordinary-differential-equations', 'inequality', 'examples-counterexamples']"
4,Non-analytic smooth function,Non-analytic smooth function,,"The Wikipedia page ( http://en.wikipedia.org/wiki/Non-analytic_smooth_function ) proves that $$f(x) = \begin{cases}  \exp(-1/x),  & \mbox{if }x>0 \\ 0, & \mbox{if }x\le0 \end{cases}$$ is a non-analytic smooth function. But I don't understand why the fact that ""the Taylor series of $f$ at the origin converges everywhere [where does ""everywhere"" mean here?] to $0$"" implies the Taylor series converges to $0$ when $x>0$? Can someone explain this in greater detail? I guess there should be a gap in my understanding of power series.","The Wikipedia page ( http://en.wikipedia.org/wiki/Non-analytic_smooth_function ) proves that $$f(x) = \begin{cases}  \exp(-1/x),  & \mbox{if }x>0 \\ 0, & \mbox{if }x\le0 \end{cases}$$ is a non-analytic smooth function. But I don't understand why the fact that ""the Taylor series of $f$ at the origin converges everywhere [where does ""everywhere"" mean here?] to $0$"" implies the Taylor series converges to $0$ when $x>0$? Can someone explain this in greater detail? I guess there should be a gap in my understanding of power series.",,"['calculus', 'real-analysis']"
5,$f_n(x_n) \rightarrow f(x) $ by uniform convergence,by uniform convergence,f_n(x_n) \rightarrow f(x) ,"I am very nearly done with this problem, but I have a concern that someone must help me alleviate. Suppose $f_n\rightarrow f$ uniformly, $f_n$ are continuous,  and $x_n\rightarrow x$. Prove that $$\lim_{n\to \infty} f_n(x_n) =f(x) .$$ We want to use $$|f_n(x_n) - f(x) |\leq |f_n(x_n) - f_n(x) |+|f_n(x) - f(x) |.$$  At first, I let $N$ be a number so that $n\geq N$ implied that $|f_n(x) - f(x) |< \epsilon /2$ for any $x$. Now,  since $f_n$ is continuous, $\lim_{m\to\infty} f_n(x_m) =f_n(x) $, we can take $|f_n(x_m) - f_n(x) |<\epsilon /2$ for $m$ greater than some $M$. But, can we take $|f_n(x_n) - f_n(x) |<\epsilon /2$? Maybe not,  if $n<M$. We could insist this is not true, and take $n$ to be greater than $M$ by WLOG letting $N$ get larger. But since we defined $M$ in the context of a particular $f_n$, now $M$ might be different,  so we are going in a circle. How can I fix this?","I am very nearly done with this problem, but I have a concern that someone must help me alleviate. Suppose $f_n\rightarrow f$ uniformly, $f_n$ are continuous,  and $x_n\rightarrow x$. Prove that $$\lim_{n\to \infty} f_n(x_n) =f(x) .$$ We want to use $$|f_n(x_n) - f(x) |\leq |f_n(x_n) - f_n(x) |+|f_n(x) - f(x) |.$$  At first, I let $N$ be a number so that $n\geq N$ implied that $|f_n(x) - f(x) |< \epsilon /2$ for any $x$. Now,  since $f_n$ is continuous, $\lim_{m\to\infty} f_n(x_m) =f_n(x) $, we can take $|f_n(x_m) - f_n(x) |<\epsilon /2$ for $m$ greater than some $M$. But, can we take $|f_n(x_n) - f_n(x) |<\epsilon /2$? Maybe not,  if $n<M$. We could insist this is not true, and take $n$ to be greater than $M$ by WLOG letting $N$ get larger. But since we defined $M$ in the context of a particular $f_n$, now $M$ might be different,  so we are going in a circle. How can I fix this?",,"['real-analysis', 'uniform-convergence']"
6,"$A:=\left\{y\in\mathbb{R}:\mu\left(f^{-1}(\left\{y\right\})\right)>0\right\}$ is countable, if $\mu$ is a finite measure and $f$ has compact support","is countable, if  is a finite measure and  has compact support",A:=\left\{y\in\mathbb{R}:\mu\left(f^{-1}(\left\{y\right\})\right)>0\right\} \mu f,"Let $E$ be a metric space $\mathcal{E}:=\mathcal{B}(E)$ be the Borel algebra on $E$ $\mu:\mathcal{E}\to [0,1]$ be a measure $f:E\to\mathbb{R}$ be measurable and have compact support Assume that $\mu(D_f)=0$, where $$D_f:=\left\{x\in E:f\text{ is discontinuous at }x\right\}$$ How can we prove that $$A:=\left\{y\in\mathbb{R}:\mu\left(f^{-1}\left(\left\{y\right\}\right)\right)>0\right\}$$ is countable?","Let $E$ be a metric space $\mathcal{E}:=\mathcal{B}(E)$ be the Borel algebra on $E$ $\mu:\mathcal{E}\to [0,1]$ be a measure $f:E\to\mathbb{R}$ be measurable and have compact support Assume that $\mu(D_f)=0$, where $$D_f:=\left\{x\in E:f\text{ is discontinuous at }x\right\}$$ How can we prove that $$A:=\left\{y\in\mathbb{R}:\mu\left(f^{-1}\left(\left\{y\right\}\right)\right)>0\right\}$$ is countable?",,"['real-analysis', 'analysis', 'measure-theory', 'continuity']"
7,The Gauss-Green theorem for unbounded domain,The Gauss-Green theorem for unbounded domain,,"This question comes to me when I deal with the following PDE problem. Suppose we have  \begin{cases} -\Delta u=0 & x\in \mathbb R^N\setminus B(0,1)\\ u=0 & x\in\partial B(0,1)\\ u\to 0 & |x|\to\infty \end{cases} Then I am going to prove that $u\equiv 0$. This problem can be proven very quickly by using Maximum Principle. But I got boring tonight and try to use energy method to prove this problem. I start with $$ 0=\int_{\mathbb R^N\setminus B(0,1)} \Delta u\, u\,dx = -\int_{\mathbb R^N\setminus B(0,1)} |\nabla u|^2\,dx+\int_{\partial B(0,1)} \nabla u\,u\,\nu d\sigma $$ and the last term is $0$ because of the boundary condition and we done. Here I realize that I am using Gauss-Green theorem to do integration by parts on the unbounded domain and the integration over the boundary of $\mathbb R^N$ at ""infinity"" has been ignored by the condition $u\to 0$ as $|x|\to \infty$. I remember I proved this result from my old classes... But now I can not justify it. I tried to do the following by taking $$ 0=\int_{B(0,R)\setminus B(0,1)} \Delta u\, u\,dx = -\int_{B(0,R)\setminus B(0,1)} |\nabla u|^2\,dx+\int_{\partial B(0,1)} \nabla u\,u\,\nu d\sigma+\int_{\partial B(0,R)} \nabla u\,u\,\nu d\sigma $$ and I try to take $R\to \infty$. But I don't see why $$ \lim_{R\to \infty} \int_{\partial B(0,R)} \nabla u\,u\,\nu d\sigma=0$$  even if $u$ vanish at infinity because we don't know the rate of vanishing and no information of $\nabla u$... Update: Based on @Jose27's answer, my question has been well-solved. But in addition, I have a interested question here for case $N=2$. It looks to me that for $N=2$, we need $\lim_{|x|\to \infty} u(x)$ to be exists, and hence we have $u$ is actually bounded. Moreover, from Folland PDE book, page 115, proposition 2.74 I read that the following statement is equivalent if $u$ is harmonic outside $B(0,1)$, for $N=2$: (a): $|u(x)|=o(\log|x|)$ as $x\to \infty$ (b): $|u(x)|=O(1)$ as $x\to\infty$ Quickly we have $(b)\implies (a)$. For converse, we notice that if $u$ satisfies $(a)$, then we have $\bar{u}$ is bounded and in turn $u$ is bounded as well.","This question comes to me when I deal with the following PDE problem. Suppose we have  \begin{cases} -\Delta u=0 & x\in \mathbb R^N\setminus B(0,1)\\ u=0 & x\in\partial B(0,1)\\ u\to 0 & |x|\to\infty \end{cases} Then I am going to prove that $u\equiv 0$. This problem can be proven very quickly by using Maximum Principle. But I got boring tonight and try to use energy method to prove this problem. I start with $$ 0=\int_{\mathbb R^N\setminus B(0,1)} \Delta u\, u\,dx = -\int_{\mathbb R^N\setminus B(0,1)} |\nabla u|^2\,dx+\int_{\partial B(0,1)} \nabla u\,u\,\nu d\sigma $$ and the last term is $0$ because of the boundary condition and we done. Here I realize that I am using Gauss-Green theorem to do integration by parts on the unbounded domain and the integration over the boundary of $\mathbb R^N$ at ""infinity"" has been ignored by the condition $u\to 0$ as $|x|\to \infty$. I remember I proved this result from my old classes... But now I can not justify it. I tried to do the following by taking $$ 0=\int_{B(0,R)\setminus B(0,1)} \Delta u\, u\,dx = -\int_{B(0,R)\setminus B(0,1)} |\nabla u|^2\,dx+\int_{\partial B(0,1)} \nabla u\,u\,\nu d\sigma+\int_{\partial B(0,R)} \nabla u\,u\,\nu d\sigma $$ and I try to take $R\to \infty$. But I don't see why $$ \lim_{R\to \infty} \int_{\partial B(0,R)} \nabla u\,u\,\nu d\sigma=0$$  even if $u$ vanish at infinity because we don't know the rate of vanishing and no information of $\nabla u$... Update: Based on @Jose27's answer, my question has been well-solved. But in addition, I have a interested question here for case $N=2$. It looks to me that for $N=2$, we need $\lim_{|x|\to \infty} u(x)$ to be exists, and hence we have $u$ is actually bounded. Moreover, from Folland PDE book, page 115, proposition 2.74 I read that the following statement is equivalent if $u$ is harmonic outside $B(0,1)$, for $N=2$: (a): $|u(x)|=o(\log|x|)$ as $x\to \infty$ (b): $|u(x)|=O(1)$ as $x\to\infty$ Quickly we have $(b)\implies (a)$. For converse, we notice that if $u$ satisfies $(a)$, then we have $\bar{u}$ is bounded and in turn $u$ is bounded as well.",,"['calculus', 'real-analysis', 'partial-differential-equations']"
8,Defining sine and cosine,Defining sine and cosine,,"We know the following are true about sine and cosine (and that they can be proven geometrically): $\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$ $\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$ $\lim\limits_{x\to0}\dfrac{\sin x}x=1$ $\lim\limits_{x\to0}\dfrac{\cos x-1}x=0$ They are continuous Let's say we have two real functions: $s(x)$ and $c(x)$. If we know that the above are true for $s$ and $c$ (i.e. $s(a+b)=s(a)c(b)+s(b)c(a)$, etc.), can we conclude that $s$ and $c$ are equal to $\sin$ and $\cos$ respectively? In other words, are sine and cosine the only two functions that satisfy the above? Do the five points above uniquely define the sine and cosine? I was thinking of the unit circle definition of sine and cosine, and I knew that there are many non-geometric definitions of them. I was wondering if the four facts shown above were enough to count as a non-geometric definition. (Without the third point, stuff like $\sin(x \text{ degrees})$ and $\cos(x \text{ degrees})$ would also work; in other words, the third point specifies that we're using radians.) EDIT: Added fourth point, since $s(x)=e^x\sin(x)$, $c(x)=e^x\cos(x)$ would work if it was omitted.","We know the following are true about sine and cosine (and that they can be proven geometrically): $\sin(a+b)=\sin(a)\cos(b)+\sin(b)\cos(a)$ $\cos(a+b)=\cos(a)\cos(b)-\sin(a)\sin(b)$ $\lim\limits_{x\to0}\dfrac{\sin x}x=1$ $\lim\limits_{x\to0}\dfrac{\cos x-1}x=0$ They are continuous Let's say we have two real functions: $s(x)$ and $c(x)$. If we know that the above are true for $s$ and $c$ (i.e. $s(a+b)=s(a)c(b)+s(b)c(a)$, etc.), can we conclude that $s$ and $c$ are equal to $\sin$ and $\cos$ respectively? In other words, are sine and cosine the only two functions that satisfy the above? Do the five points above uniquely define the sine and cosine? I was thinking of the unit circle definition of sine and cosine, and I knew that there are many non-geometric definitions of them. I was wondering if the four facts shown above were enough to count as a non-geometric definition. (Without the third point, stuff like $\sin(x \text{ degrees})$ and $\cos(x \text{ degrees})$ would also work; in other words, the third point specifies that we're using radians.) EDIT: Added fourth point, since $s(x)=e^x\sin(x)$, $c(x)=e^x\cos(x)$ would work if it was omitted.",,"['real-analysis', 'trigonometry', 'definition', 'functional-equations']"
9,Are Borel-measurable functions limits of sequences of continuous functions,Are Borel-measurable functions limits of sequences of continuous functions,,"One definiton of Borel measurable functions is: The collection of borel measurable functions is the smallest   collection of real valued functions on $\mathbb{R}$ that contains the   continuous functions and is closed under pointwise limits. Now my question. Is every Borel measureable function then according to this definition the pointwise limit of continuous functions? What can go wrong is this: $\{f_n^k\}_{n=0}^{\infty}\rightarrow f^k$, and we assume that all $f_n^k$ are continuous. $f^k$ need not be continuous. And lets say that $\{f^k\}\rightarrow f$, now since the borel measurable functions is closed under pointwise limits, f has to be borel-measurable. But we haven't shown that it is a limit of continuous functions. Can such an f exist that is not a limit of continuous functions? I tried proving that indeed f was a limit of continuous functions, but I couldn't finish my proof, I ran into this problem: Is this statement about limits true. I tried proving it with a diagonal-argument, but maybe there is another way to prove what we want? Is the answer yes and if so, how do I prove it? Or is it not true, and is there a counter-example?","One definiton of Borel measurable functions is: The collection of borel measurable functions is the smallest   collection of real valued functions on $\mathbb{R}$ that contains the   continuous functions and is closed under pointwise limits. Now my question. Is every Borel measureable function then according to this definition the pointwise limit of continuous functions? What can go wrong is this: $\{f_n^k\}_{n=0}^{\infty}\rightarrow f^k$, and we assume that all $f_n^k$ are continuous. $f^k$ need not be continuous. And lets say that $\{f^k\}\rightarrow f$, now since the borel measurable functions is closed under pointwise limits, f has to be borel-measurable. But we haven't shown that it is a limit of continuous functions. Can such an f exist that is not a limit of continuous functions? I tried proving that indeed f was a limit of continuous functions, but I couldn't finish my proof, I ran into this problem: Is this statement about limits true. I tried proving it with a diagonal-argument, but maybe there is another way to prove what we want? Is the answer yes and if so, how do I prove it? Or is it not true, and is there a counter-example?",,"['real-analysis', 'measure-theory']"
10,"Arzela-Ascoli and compactness in $C(X), l^p, L^p$",Arzela-Ascoli and compactness in,"C(X), l^p, L^p","Arzela-Ascoli and compactness in $C(X), l^p, L^p$ $C(X)$ with the uniform norm and $X$ is a compact metric space, a closed and bounded set in $C(X)$ is compact if and only if it is equicontinuous. a closed and bounded subset in $l^p$ $(1\leq p<\infty)$ is compact if and only if it is equisummable, that is for each $\epsilon > 0$ there exists a $N$ such that $\sum_{n=N}^\infty |x_n|^p \leq \epsilon$ for each $x = \{x_n\}$ in this set. Questions: What is the connection between equicontinuous and equisummable, and how to connect between $C(X)$ with $l^p$ , is there something similar for $L^p$ ? What is the intuition behind equicontinuous and equisummable? From my own understanding, in a infinite dimensional space say $l^p$ $(p<\infty)$ , because the metric or norm has to measure the quantity in every single one of the dimensions, there can be too much variation in each dimension for a closed and bounded set to be compact in the metric topology. The concept of equisummable would make this variation small. Is this correct? And what about equicontinuous? Thank you very much!","Arzela-Ascoli and compactness in with the uniform norm and is a compact metric space, a closed and bounded set in is compact if and only if it is equicontinuous. a closed and bounded subset in is compact if and only if it is equisummable, that is for each there exists a such that for each in this set. Questions: What is the connection between equicontinuous and equisummable, and how to connect between with , is there something similar for ? What is the intuition behind equicontinuous and equisummable? From my own understanding, in a infinite dimensional space say , because the metric or norm has to measure the quantity in every single one of the dimensions, there can be too much variation in each dimension for a closed and bounded set to be compact in the metric topology. The concept of equisummable would make this variation small. Is this correct? And what about equicontinuous? Thank you very much!","C(X), l^p, L^p C(X) X C(X) l^p (1\leq p<\infty) \epsilon > 0 N \sum_{n=N}^\infty |x_n|^p \leq \epsilon x = \{x_n\} C(X) l^p L^p l^p (p<\infty)","['real-analysis', 'functional-analysis', 'compactness']"
11,How prove $ \; |f(1)|\le 2004\;$ if $\sqrt {x(1 - x)}\; \Big|f(x)\Big|\le 334$ for $f(x) = Ax^2+ Bx + C $,How prove  if  for, \; |f(1)|\le 2004\; \sqrt {x(1 - x)}\; \Big|f(x)\Big|\le 334 f(x) = Ax^2+ Bx + C ,"Let $ \; A,B, C\in {\mathbb R} ,\;$ and  $ \; f(x) = Ax^2+ Bx + C$ and $ \sqrt {x(1 - x)} \left|f(x)\right|\le 334,\;\forall x\in [0,1]\;$. How prove  $ \; \left|f(1)\right|\le 2004\;$ ?","Let $ \; A,B, C\in {\mathbb R} ,\;$ and  $ \; f(x) = Ax^2+ Bx + C$ and $ \sqrt {x(1 - x)} \left|f(x)\right|\le 334,\;\forall x\in [0,1]\;$. How prove  $ \; \left|f(1)\right|\le 2004\;$ ?",,"['real-analysis', 'inequality', 'polynomials', 'approximation-theory']"
12,Is there a subset of R such that its Cantor-Bendixson rank is the first limit ordinal?,Is there a subset of R such that its Cantor-Bendixson rank is the first limit ordinal?,,I'm looking for a set $A \subset \mathbb{R}$ such that $\bigcap^\infty_{n=0} A^{(n)} $ is a perfect set (i.e $X'=X$) but $\forall n \in \mathbb{N}$ the set $A^{(n)}$ isn't perfect (where $X^{(n)}=(X^{(n-1)})'$ and $X^{(0)}=X$).,I'm looking for a set $A \subset \mathbb{R}$ such that $\bigcap^\infty_{n=0} A^{(n)} $ is a perfect set (i.e $X'=X$) but $\forall n \in \mathbb{N}$ the set $A^{(n)}$ isn't perfect (where $X^{(n)}=(X^{(n-1)})'$ and $X^{(0)}=X$).,,"['real-analysis', 'general-topology', 'analysis', 'ordinals', 'descriptive-set-theory']"
13,Finite Series $\sum_{k=1}^{n-1}\frac1{1-\cos(\frac{2k\pi}{n})}$,Finite Series,\sum_{k=1}^{n-1}\frac1{1-\cos(\frac{2k\pi}{n})},"I want to show that $$\sum_{k=1}^{n-1}\frac1{1-\cos(\frac{2k\pi}{n})} = \frac{n^2-1}6$$ With induction I don't know how I could come back from $\frac{1}{1-\cos(\frac{2k\pi}{n+1})}$ to $\frac{1}{1-\cos(\frac{2k\pi}{n})}$. I know that $\sum_{k=1}^{n-1}\cos(\frac{2k\pi}{n}) = -1$, but don't see any way how I could use that here.","I want to show that $$\sum_{k=1}^{n-1}\frac1{1-\cos(\frac{2k\pi}{n})} = \frac{n^2-1}6$$ With induction I don't know how I could come back from $\frac{1}{1-\cos(\frac{2k\pi}{n+1})}$ to $\frac{1}{1-\cos(\frac{2k\pi}{n})}$. I know that $\sum_{k=1}^{n-1}\cos(\frac{2k\pi}{n}) = -1$, but don't see any way how I could use that here.",,"['real-analysis', 'sequences-and-series', 'trigonometry', 'summation', 'closed-form']"
14,"A map from $(0,1)$ to $(0,1)$ such that the image of every open interval in $(0,1)$ is $(0,1)$",A map from  to  such that the image of every open interval in  is,"(0,1) (0,1) (0,1) (0,1)","Can we have a map from $(0,1)$ to $(0,1)$ such that the image of every open interval in $(0,1)$ is all of $(0,1)$ ?","Can we have a map from $(0,1)$ to $(0,1)$ such that the image of every open interval in $(0,1)$ is all of $(0,1)$ ?",,"['calculus', 'real-analysis', 'examples-counterexamples']"
15,Is the inverse of a real analytic function still analytic?,Is the inverse of a real analytic function still analytic?,,"If $f:D\to D'$, with $D, D'$ open subsets of $\mathbb{C}$, is a complex analytic invertible function with non-zero derviative, it's easy to see that $f^{-1}:D'\to D$ is analytic too. Indeed complex analytic functions are just holomorphic functions and $\exists\ \frac{df^{-1}}{dz}=(\frac{df}{dz})^{-1}$. Now if $f:I\to I'$, with $I, I'$ intervals of $\mathbb{R}$, is a real analytic invertible function with non-zero derivative, is it true that $f^{-1}:I'\to I$ is analytic? One may extend $f$ to a complex analytic function, but I don't know if this one is still invertible...","If $f:D\to D'$, with $D, D'$ open subsets of $\mathbb{C}$, is a complex analytic invertible function with non-zero derviative, it's easy to see that $f^{-1}:D'\to D$ is analytic too. Indeed complex analytic functions are just holomorphic functions and $\exists\ \frac{df^{-1}}{dz}=(\frac{df}{dz})^{-1}$. Now if $f:I\to I'$, with $I, I'$ intervals of $\mathbb{R}$, is a real analytic invertible function with non-zero derivative, is it true that $f^{-1}:I'\to I$ is analytic? One may extend $f$ to a complex analytic function, but I don't know if this one is still invertible...",,"['real-analysis', 'complex-analysis', 'analyticity']"
16,"A union of balls centered at all the rationals on $[0,1]$ with decreasing radius $r_n \to 0$ relation with $[0,1]$",A union of balls centered at all the rationals on  with decreasing radius  relation with,"[0,1] r_n \to 0 [0,1]","Given a strictly decreasing sequence $r_n$ such that $r_n\in [0,1]$, $r_1 < \frac{1}{2}$, and $\lim\limits_{n\to \infty} r_n=0$. Question: Does there exist a bijective sequence $x:\mathbb N\to \mathbb Q\cap[0,1]$ such that $$\bigcup_{n\in \mathbb N}J(x_n,r_n)\ne [0,1]$$ where $J(a,b)=(a-b,a+b)\cap[0,1]$ and $x_n$ denotes $x(n)$?","Given a strictly decreasing sequence $r_n$ such that $r_n\in [0,1]$, $r_1 < \frac{1}{2}$, and $\lim\limits_{n\to \infty} r_n=0$. Question: Does there exist a bijective sequence $x:\mathbb N\to \mathbb Q\cap[0,1]$ such that $$\bigcup_{n\in \mathbb N}J(x_n,r_n)\ne [0,1]$$ where $J(a,b)=(a-b,a+b)\cap[0,1]$ and $x_n$ denotes $x(n)$?",,"['real-analysis', 'general-topology']"
17,A improper integral with Glaisher-Kinkelin constant,A improper integral with Glaisher-Kinkelin constant,,"Show that : $$\int_0^\infty \frac{\text{e}^{-x}}{x^2} \left( \frac{1}{1-\text{e}^{-x}} - \frac{1}{x} - \frac{1}{2} \right)^2 \, \text{d}x = \frac{7}{36}-\ln A+\frac{\zeta \left( 3 \right)}{2\pi ^2}$$ Where $\displaystyle A$ is Glaisher-Kinkelin constant I see Chris's question is a bit related with this Evaluate $\int_0^1\left(\frac{1}{\ln x} + \frac{1}{1-x}\right)^2 \mathrm dx$","Show that : $$\int_0^\infty \frac{\text{e}^{-x}}{x^2} \left( \frac{1}{1-\text{e}^{-x}} - \frac{1}{x} - \frac{1}{2} \right)^2 \, \text{d}x = \frac{7}{36}-\ln A+\frac{\zeta \left( 3 \right)}{2\pi ^2}$$ Where $\displaystyle A$ is Glaisher-Kinkelin constant I see Chris's question is a bit related with this Evaluate $\int_0^1\left(\frac{1}{\ln x} + \frac{1}{1-x}\right)^2 \mathrm dx$",,"['real-analysis', 'sequences-and-series', 'integration', 'improper-integrals']"
18,give an example of a function that is integrable in $\mathbb R $ and $\lim_{ x\to \infty}f(x)\neq0$,give an example of a function that is integrable in  and,\mathbb R  \lim_{ x\to \infty}f(x)\neq0,"i did a search for such function but didn't found anything useful/complete ! , like this : Integrable function $f$ on $\mathbb R$ does not imply that limit $f(x)$ is zero is there any function that is integrable and $\lim_{x \to \infty}f(x) \neq0 $ and $\infty$ ??","i did a search for such function but didn't found anything useful/complete ! , like this : Integrable function $f$ on $\mathbb R$ does not imply that limit $f(x)$ is zero is there any function that is integrable and $\lim_{x \to \infty}f(x) \neq0 $ and $\infty$ ??",,"['real-analysis', 'integration', 'lebesgue-integral']"
19,Computation of a (probably) tricky limit.,Computation of a (probably) tricky limit.,,"Question: I have the following ""Analysis 1""-limit: $$\lim_{t \rightarrow 0}\: \frac{e^{-\frac{t}{4}}}{t} \left(\frac{1}{4t^{\frac{3}{2}}} \int_0 ^\infty \frac{x^3 e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx - \frac{1}{2t^{\frac{1}{2}}} \int_0 ^\infty \frac{x e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx\right).$$ The result should be $-\frac{\sqrt{\pi}}{6}$. How to compute it? Motivation: McKean, in page 242 of his article ""Selberg's Trace Formula as Applied to a Compact Riemann Surface"" states the formal power series expansion: $$\frac{e^{-\frac{t}{4}}}{(4\pi t)^{\frac{3}{2}}} \int_0^\infty \frac{x e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx = \frac{1}{4 \pi t} \left(1 -\frac{t}{3} + O(t^2) \right).$$ He gives no proof for it, so I decided to do my homework and compute it. My approach consisted in defining:  $$f(t):= \frac{e^{-\frac{t}{4}}}{(4\pi t)^{\frac{1}{2}}} \int_0^\infty \frac{x e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx.$$ Then in computing its Taylor expansion in $t=0$. The first term of the Taylor expansion follows easily (see below in ""My progresses""). To compute the second term of the expansion I need: $$ \partial_t(f(t))_{t=0} = \lim_{t \rightarrow 0} \partial_t(f(t))= \lim_{t\rightarrow 0 } -\frac{f(t)}{4} + \frac{1}{2\sqrt{\pi}}\cdot\{\text{Limit above}\}.$$ This is why I'm interested in the limit above. The conjectured result follows by McKean statement and $\underset{t \rightarrow 0}{\lim} f(t) =1$. My progresses: A useful partial result I have got is the limit, for $n$ odd: $$\lim_{t\rightarrow 0}\: \frac{1}{t^{\frac{n}{2}}} \int_0^\infty \frac{x^n e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx=\left(\prod_{i=1}^{\frac{n-1}{2}}\frac{2i-1}{2}\right)2^n \sqrt{\pi}.$$ Which I use to compute the first term of the Taylor expansion. Applying it to the ""main"" limit we see that it is of the form $\frac{0}{0}$. But applying L'Hopital rule we erase the $\frac{1}{t}$ from the denominator just to get it again from the derivative of the numerator, and we come back to the $\frac{0}{0}$ situation. I iterated the rule for some steps but it doesn't seem to bring anywhere. Remark: I'm actually interested in the expansion as in McKean, so if you have any way to compute it avoiding the limit above for me it would be a completely satisfying answer. Moreover if you see any error I made to get to the limit  I would be very grateful if you could point it out. Thank you very much in advance!","Question: I have the following ""Analysis 1""-limit: $$\lim_{t \rightarrow 0}\: \frac{e^{-\frac{t}{4}}}{t} \left(\frac{1}{4t^{\frac{3}{2}}} \int_0 ^\infty \frac{x^3 e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx - \frac{1}{2t^{\frac{1}{2}}} \int_0 ^\infty \frac{x e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx\right).$$ The result should be $-\frac{\sqrt{\pi}}{6}$. How to compute it? Motivation: McKean, in page 242 of his article ""Selberg's Trace Formula as Applied to a Compact Riemann Surface"" states the formal power series expansion: $$\frac{e^{-\frac{t}{4}}}{(4\pi t)^{\frac{3}{2}}} \int_0^\infty \frac{x e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx = \frac{1}{4 \pi t} \left(1 -\frac{t}{3} + O(t^2) \right).$$ He gives no proof for it, so I decided to do my homework and compute it. My approach consisted in defining:  $$f(t):= \frac{e^{-\frac{t}{4}}}{(4\pi t)^{\frac{1}{2}}} \int_0^\infty \frac{x e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx.$$ Then in computing its Taylor expansion in $t=0$. The first term of the Taylor expansion follows easily (see below in ""My progresses""). To compute the second term of the expansion I need: $$ \partial_t(f(t))_{t=0} = \lim_{t \rightarrow 0} \partial_t(f(t))= \lim_{t\rightarrow 0 } -\frac{f(t)}{4} + \frac{1}{2\sqrt{\pi}}\cdot\{\text{Limit above}\}.$$ This is why I'm interested in the limit above. The conjectured result follows by McKean statement and $\underset{t \rightarrow 0}{\lim} f(t) =1$. My progresses: A useful partial result I have got is the limit, for $n$ odd: $$\lim_{t\rightarrow 0}\: \frac{1}{t^{\frac{n}{2}}} \int_0^\infty \frac{x^n e^{-\frac{x^2}{4t}}}{\sinh(\frac{x}{2})}dx=\left(\prod_{i=1}^{\frac{n-1}{2}}\frac{2i-1}{2}\right)2^n \sqrt{\pi}.$$ Which I use to compute the first term of the Taylor expansion. Applying it to the ""main"" limit we see that it is of the form $\frac{0}{0}$. But applying L'Hopital rule we erase the $\frac{1}{t}$ from the denominator just to get it again from the derivative of the numerator, and we come back to the $\frac{0}{0}$ situation. I iterated the rule for some steps but it doesn't seem to bring anywhere. Remark: I'm actually interested in the expansion as in McKean, so if you have any way to compute it avoiding the limit above for me it would be a completely satisfying answer. Moreover if you see any error I made to get to the limit  I would be very grateful if you could point it out. Thank you very much in advance!",,"['calculus', 'real-analysis', 'limits']"
20,"Radius of convergence of $\sum a_nz^n$, $a_n$ the number of divisors of $n^{50}$","Radius of convergence of ,  the number of divisors of",\sum a_nz^n a_n n^{50},"I have to find the radius of convergence of the power series $\sum a_n z^n$ where $ a_n =$ number of divisors of $n^{50}$. Options available are: $1$ $50$ $\frac{1}{50}$ $0$ Please suggest how to proceed. Using the fact that $ d(n)\leq n$, we have $d(n^{50})\leq n^{50}$. Using the First Comparison Test, the series on the right converges if $\mid{z}\mid< 1$ and however does not converge if $\mid{z}\mid\geq 1$ and hence so does the series on the left.  Radius of Convergence is 1. [Now for $a_n =n$, then $ R = \lim_{n\rightarrow\infty}\frac {a_n}{a_{n+1}} =1. $ So,the series on the right converges for $\mid z \mid<1$] Now if $ z=1$, then the series $\sum a_n z^n$ takes the form $\sum d(n^{50})$ and since $ d(n^{50})\geq 1 $, by evoking the comparison test again, the series diverges for $z=1$.","I have to find the radius of convergence of the power series $\sum a_n z^n$ where $ a_n =$ number of divisors of $n^{50}$. Options available are: $1$ $50$ $\frac{1}{50}$ $0$ Please suggest how to proceed. Using the fact that $ d(n)\leq n$, we have $d(n^{50})\leq n^{50}$. Using the First Comparison Test, the series on the right converges if $\mid{z}\mid< 1$ and however does not converge if $\mid{z}\mid\geq 1$ and hence so does the series on the left.  Radius of Convergence is 1. [Now for $a_n =n$, then $ R = \lim_{n\rightarrow\infty}\frac {a_n}{a_{n+1}} =1. $ So,the series on the right converges for $\mid z \mid<1$] Now if $ z=1$, then the series $\sum a_n z^n$ takes the form $\sum d(n^{50})$ and since $ d(n^{50})\geq 1 $, by evoking the comparison test again, the series diverges for $z=1$.",,"['real-analysis', 'elementary-number-theory']"
21,$C^{2} ( \mathbb{R}^{2}) $ function-proof of inequality,function-proof of inequality,C^{2} ( \mathbb{R}^{2}) ,"Let $f\in C^{2}( \mathbb{R}^{2} )$. Suppose that $\triangledown f=0 $ on a compact set $A\subseteq \mathbb{R}^{2}$. I want to prove that there is a strictly positive constant $\lambda > 0$ such that: $$\left | f\left ( x \right )-f\left ( y \right ) \right |\leq \lambda \left | x-y \right |^{2}$$ for all $x,y$ in $A$. What I had in mind is that if the gradient of f is zero on a compact set $A$, then the function $f$ is constant and the inequality is obvious. Any suggestions to solve this problem?","Let $f\in C^{2}( \mathbb{R}^{2} )$. Suppose that $\triangledown f=0 $ on a compact set $A\subseteq \mathbb{R}^{2}$. I want to prove that there is a strictly positive constant $\lambda > 0$ such that: $$\left | f\left ( x \right )-f\left ( y \right ) \right |\leq \lambda \left | x-y \right |^{2}$$ for all $x,y$ in $A$. What I had in mind is that if the gradient of f is zero on a compact set $A$, then the function $f$ is constant and the inequality is obvious. Any suggestions to solve this problem?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
22,Dominated Convergence Theorem using Egorov,Dominated Convergence Theorem using Egorov,,"I've been reading on Dominated Convergence Theorem and its proof using Fatou-Lebesgue, but I can't seem to figure out how to do so with Egorov's theorem. If $\nu$ is a finite Baire measure on a compact Hausdorff space $X$, I first let $U_n$ be a sequence of sets such that $U_{n+1} \subset U_n$ with an additional property that $\nu(\cap_n U_n)=0$ . I'm first trying to show that if $f_n$ is a characteristic function of $U_n$, then for any $g\in L^1$, $\lim_n \int | gf_n| \,d\nu=0$ using monotone convergence. Any hints would be appreciated. Thank you.","I've been reading on Dominated Convergence Theorem and its proof using Fatou-Lebesgue, but I can't seem to figure out how to do so with Egorov's theorem. If $\nu$ is a finite Baire measure on a compact Hausdorff space $X$, I first let $U_n$ be a sequence of sets such that $U_{n+1} \subset U_n$ with an additional property that $\nu(\cap_n U_n)=0$ . I'm first trying to show that if $f_n$ is a characteristic function of $U_n$, then for any $g\in L^1$, $\lim_n \int | gf_n| \,d\nu=0$ using monotone convergence. Any hints would be appreciated. Thank you.",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
23,"Is there a bijection from $[0,1]$ to the Cantor set $C$?",Is there a bijection from  to the Cantor set ?,"[0,1] C","Does there exist a bijective function $\phi$ from the unit interval $[0,1]$ to the Cantor set $C$? If so, how can it be constructed? I could then proceed to build a measure space ($C$, $\mathcal{M}_\phi$, $m_\phi$) where $m_\phi(E)$ = $m(\phi^{-1}(E))$ for $E \subset C$. How would $m_\phi(C) = 1$?","Does there exist a bijective function $\phi$ from the unit interval $[0,1]$ to the Cantor set $C$? If so, how can it be constructed? I could then proceed to build a measure space ($C$, $\mathcal{M}_\phi$, $m_\phi$) where $m_\phi(E)$ = $m(\phi^{-1}(E))$ for $E \subset C$. How would $m_\phi(C) = 1$?",,"['real-analysis', 'measure-theory']"
24,Let $S_n := \sum_{i=0}^{n-1} \frac{1}{(i+1) \left ( 1 +\frac{1}{2} + \cdots + \frac{1}{n-i} \right )}$. Does $(S_n)_{n\ge 1}$ converge?,Let . Does  converge?,S_n := \sum_{i=0}^{n-1} \frac{1}{(i+1) \left ( 1 +\frac{1}{2} + \cdots + \frac{1}{n-i} \right )} (S_n)_{n\ge 1},"We define a sequence $(S_n)_{n\ge 1}$ by $$ S_n := \sum_{i=0}^{n-1} \frac{1}{(i+1) \left ( 1 +\frac{1}{2} + \cdots + \frac{1}{n-i} \right )}. $$ Clearly, $(S_n)_{n\ge 1}$ does not have a form of a series, so I could not use convergence tests .  Could you elaborate on the convergence of $(S_n)_{n\ge 1}$ ? Thank you so much for your help!","We define a sequence by Clearly, does not have a form of a series, so I could not use convergence tests .  Could you elaborate on the convergence of ? Thank you so much for your help!","(S_n)_{n\ge 1} 
S_n := \sum_{i=0}^{n-1} \frac{1}{(i+1) \left ( 1 +\frac{1}{2} + \cdots + \frac{1}{n-i} \right )}.
 (S_n)_{n\ge 1} (S_n)_{n\ge 1}","['real-analysis', 'calculus', 'sequences-and-series']"
25,"Computing $\lim\limits_{n\to\infty}n\bigl(a_n-\int_0^1f^2(x)\,dx\bigr)$ given a function $f\in\mathcal{C}^1([0, 1])$",Computing  given a function,"\lim\limits_{n\to\infty}n\bigl(a_n-\int_0^1f^2(x)\,dx\bigr) f\in\mathcal{C}^1([0, 1])","Let $f : [0, 1] \to \mathbb{R}$ be a continuous function and $(a_n)_{n \geq 1}$ a sequence defined by $$a_n = \sum_{k=1}^{n} \biggl( f\left(\frac{k-1}{n}\right)\int_{\frac{k-1}{n}}^{\frac{k}{n}}f(t) \, dt\biggr), \qquad \forall n \in \mathbb{N}^{*}.$$ a) Prove that $\lim\limits_{n \to \infty}a_n = \int_0^1f^2(x)dx$ b) Furthermore, if $f \in \mathcal{C}^1([0, 1])$ (differentiable with its derivative continuous), compute: $$\lim_{n\to\infty} n\left( a_n - \int_0^1 f^2(x) \, dx \right)$$ Since $f$ is a continuous function, then there exists an antiderivative of $f$ such that $F : [0, 1] \to \mathbb{R}$ , $F'(x) = f(x)$ . Therefore, $$ \int_{\frac{k-1}{n}}^{\frac{k}{n}}f(t)dt = F\left(\frac{k}{n}\right) - F\left(\frac{k-1}{n}\right). $$ From this, we deduce that: $$ a_n = \sum_{k =1}^n f\left(\frac{k-1}{n}\right)\left[F\left(\frac{k}{n}\right) - F\left(\frac{k-1}{n}\right)\right] $$ Using Lagrange's Mean Value Theorem, one can easily see that there exists $c_{k, n} \in \bigl( \frac{k-1}{n}, \frac{k}{n} \bigr)$ , so that: $$\frac{F(\frac{k}{n}) - F(\frac{k-1}{n})}{\frac{k}{n} - \frac{k-1}{n}} = F'(c_{k, n}) = f(c_{k, n})$$ Using this, we obtain that: $$ a_n = \frac{1}{n}\sum_{k=1}^n f\left(\frac{k-1}{n}\right)f(c_{k, n}) $$ Note that $c_{k, n} \in \bigl( \frac{k-1}{n}, \frac{k}{n} \bigr)$ , and $\left| f(c_{k, n}) - f\bigl(\frac{k-1}{n}\bigr) \right| \leq \epsilon, \forall \epsilon > 0$ and $n$ sufficiently large since $f$ is a continuous function. Then, by choosing $\Delta = (x_0, x_1, \dots, x_n) \in \mathcal{D}([0, 1])$ , with $x_k = \frac{k}{n}$ , we realize that $\|\Delta\| = \frac{1}{n}$ , and then by choosing $\xi_k \in [x_{k - 1}, x_k]$ , $a_n$ is precisely the Riemann sum, thus because $f$ is Riemann integrable, and by Lebesgue's criterion, $f\cdot f = f^2$ is Riemann integrable: $$ \lim_{n \to \infty}a_n = \int_0^1 f^2(x) \, dx $$ Thus concluding a). Now, my problem is that I am stuck at b). The given limit obviously screams for the Stolz–Cesàro lemma to be applied, however when I try to apply the Lagrange Mean Value theorem again to reduce the differences of antiderivatives, I cannot seem to get an idea of how to reduce terms like $\frac{k}{n}$ and $\frac{k}{n + 1}$ , where the denominator is different. My idea would be to write: $$ L = \lim_{n \to \infty} \frac{a_n - \int_0^1f^2(x) \, dx}{\frac{1}{n}} $$ And instead computing: $$L = \lim_{n \to \infty} \frac{a_{n+1} - a_n}{\frac{1}{n + 1} - \frac{1}{n}}$$ Any ideas, hints or solutions would be of very much help. Thank you very much! :)","Let be a continuous function and a sequence defined by a) Prove that b) Furthermore, if (differentiable with its derivative continuous), compute: Since is a continuous function, then there exists an antiderivative of such that , . Therefore, From this, we deduce that: Using Lagrange's Mean Value Theorem, one can easily see that there exists , so that: Using this, we obtain that: Note that , and and sufficiently large since is a continuous function. Then, by choosing , with , we realize that , and then by choosing , is precisely the Riemann sum, thus because is Riemann integrable, and by Lebesgue's criterion, is Riemann integrable: Thus concluding a). Now, my problem is that I am stuck at b). The given limit obviously screams for the Stolz–Cesàro lemma to be applied, however when I try to apply the Lagrange Mean Value theorem again to reduce the differences of antiderivatives, I cannot seem to get an idea of how to reduce terms like and , where the denominator is different. My idea would be to write: And instead computing: Any ideas, hints or solutions would be of very much help. Thank you very much! :)","f : [0, 1] \to \mathbb{R} (a_n)_{n \geq 1} a_n = \sum_{k=1}^{n} \biggl( f\left(\frac{k-1}{n}\right)\int_{\frac{k-1}{n}}^{\frac{k}{n}}f(t) \, dt\biggr), \qquad \forall n \in \mathbb{N}^{*}. \lim\limits_{n \to \infty}a_n = \int_0^1f^2(x)dx f \in \mathcal{C}^1([0, 1]) \lim_{n\to\infty} n\left( a_n - \int_0^1 f^2(x) \, dx \right) f f F : [0, 1] \to \mathbb{R} F'(x) = f(x) 
\int_{\frac{k-1}{n}}^{\frac{k}{n}}f(t)dt = F\left(\frac{k}{n}\right) - F\left(\frac{k-1}{n}\right).
 
a_n = \sum_{k =1}^n f\left(\frac{k-1}{n}\right)\left[F\left(\frac{k}{n}\right) - F\left(\frac{k-1}{n}\right)\right]
 c_{k, n} \in \bigl( \frac{k-1}{n}, \frac{k}{n} \bigr) \frac{F(\frac{k}{n}) - F(\frac{k-1}{n})}{\frac{k}{n} - \frac{k-1}{n}} = F'(c_{k, n}) = f(c_{k, n}) 
a_n = \frac{1}{n}\sum_{k=1}^n f\left(\frac{k-1}{n}\right)f(c_{k, n})
 c_{k, n} \in \bigl( \frac{k-1}{n}, \frac{k}{n} \bigr) \left| f(c_{k, n}) - f\bigl(\frac{k-1}{n}\bigr) \right| \leq \epsilon, \forall \epsilon > 0 n f \Delta = (x_0, x_1, \dots, x_n) \in \mathcal{D}([0, 1]) x_k = \frac{k}{n} \|\Delta\| = \frac{1}{n} \xi_k \in [x_{k - 1}, x_k] a_n f f\cdot f = f^2 
\lim_{n \to \infty}a_n = \int_0^1 f^2(x) \, dx
 \frac{k}{n} \frac{k}{n + 1} 
L = \lim_{n \to \infty} \frac{a_n - \int_0^1f^2(x) \, dx}{\frac{1}{n}}
 L = \lim_{n \to \infty} \frac{a_{n+1} - a_n}{\frac{1}{n + 1} - \frac{1}{n}}","['real-analysis', 'integration', 'limits', 'derivatives', 'continuity']"
26,"Prove that $X := \{ f: [0,1] \to [0,1] : f \text{ is continuous and } f(1) = 0 \}$ with the given distance is neither connected nor separable",Prove that  with the given distance is neither connected nor separable,"X := \{ f: [0,1] \to [0,1] : f \text{ is continuous and } f(1) = 0 \}","Prove that $X := \{ f: [0,1] \to [0,1] : f \text{ is continuous and } f(1) = 0 \}$ with $d(f,g) := \inf \{r \geq 0 : f(t) = g(t) \forall r ≤ t ≤ 1 \}$ is neither connected nor separable. Here is my work so far: let $y \in [0,1]$ and consider the function $f_y \in X$ that is $y$ on $[0,99/100]$ and is linear and continuous on $[99/100 , 1]$ . By construction, $f_y \in X$ for all $y \in [0,1]$ . If we assume that $X$ is separable, then there exists a countable dense subset $\\{g_n\\}$ , so that for any given $y \in [0,1]$ and $\epsilon > 0$ there exists some $g_n$ such that $d(f_y,g_n) < \epsilon$ , so that $f_y(x) = g_n(x)$ for all $x \in [\epsilon,1]$ . By looking at the values $g_n(98/100)$ , we see that $\\{ g_n(98/100) \\} = [0,1]$ , a contradiction. Does this work look okay? I was also am lost on how to prove that $X$ is disconnected. I cannot think of a disconnection of $X$ .","Prove that with is neither connected nor separable. Here is my work so far: let and consider the function that is on and is linear and continuous on . By construction, for all . If we assume that is separable, then there exists a countable dense subset , so that for any given and there exists some such that , so that for all . By looking at the values , we see that , a contradiction. Does this work look okay? I was also am lost on how to prove that is disconnected. I cannot think of a disconnection of .","X := \{ f: [0,1] \to [0,1] : f \text{ is continuous and } f(1) = 0 \} d(f,g) := \inf \{r \geq 0 : f(t) = g(t) \forall r ≤ t ≤ 1 \} y \in [0,1] f_y \in X y [0,99/100] [99/100 , 1] f_y \in X y \in [0,1] X \\{g_n\\} y \in [0,1] \epsilon > 0 g_n d(f_y,g_n) < \epsilon f_y(x) = g_n(x) x \in [\epsilon,1] g_n(98/100) \\{ g_n(98/100) \\} = [0,1] X X","['real-analysis', 'functional-analysis', 'metric-spaces', 'connectedness', 'separable-spaces']"
27,Find a positive decreasing twice differentiable convex function $f$ such that $\int_1^{\infty}\frac{(f'(x))^2}{f(x)}dx=\infty$.,Find a positive decreasing twice differentiable convex function  such that .,f \int_1^{\infty}\frac{(f'(x))^2}{f(x)}dx=\infty,"Reopening this question: Does there exist a positive, decreasing, twice differentiable convex function such that $\int_1^{\infty}\frac{(f'(x))^2}{f(x)}dx=\infty$? My trial: I tried some simple $f(x)$ like monomials or exponentials, that did not work (of course I may have overlooked smth). So I thought of being more systematic and try to define: $$g(x)=\frac{f'(x)^2}{f(x)}$$ where $g(x)>0$ from the hypothesis. I tried than to express $f$ as a function of $g$ like this: $$f'^2=fg$$ $$f'=-\sqrt{f}\sqrt{g} \ (\text{take negative square root})$$ $$f'/\sqrt{f}=-\sqrt{g} \ (\text{divide by non-zero function})$$ and integrating from $1$ to $x$ : $$\sqrt{f(x)}=\sqrt{f(1)}-\frac{1}{2}\int_1^{x}\sqrt{g(x)}dx$$ so now we should find a positive $g(x)$ s.t.: $$\int_1^{+\infty}\sqrt{g(x)}dx <\infty,\int_1^{+\infty}g(x)dx =\infty $$ , or prove that such a $g$ does not exist but since it is very easy to make errors with such manipulations (taking square roots, divisions by $f$ ) I stopped here... moreover, maybe I am overcomplicating things...","Reopening this question: Does there exist a positive, decreasing, twice differentiable convex function such that $\int_1^{\infty}\frac{(f'(x))^2}{f(x)}dx=\infty$? My trial: I tried some simple like monomials or exponentials, that did not work (of course I may have overlooked smth). So I thought of being more systematic and try to define: where from the hypothesis. I tried than to express as a function of like this: and integrating from to : so now we should find a positive s.t.: , or prove that such a does not exist but since it is very easy to make errors with such manipulations (taking square roots, divisions by ) I stopped here... moreover, maybe I am overcomplicating things...","f(x) g(x)=\frac{f'(x)^2}{f(x)} g(x)>0 f g f'^2=fg f'=-\sqrt{f}\sqrt{g} \ (\text{take negative square root}) f'/\sqrt{f}=-\sqrt{g} \ (\text{divide by non-zero function}) 1 x \sqrt{f(x)}=\sqrt{f(1)}-\frac{1}{2}\int_1^{x}\sqrt{g(x)}dx g(x) \int_1^{+\infty}\sqrt{g(x)}dx <\infty,\int_1^{+\infty}g(x)dx =\infty  g f","['real-analysis', 'integration', 'convergence-divergence']"
28,Topological homeomorphism which takes convergent sequence to a specific convergent sequence,Topological homeomorphism which takes convergent sequence to a specific convergent sequence,,"For some reasons the sequence $x_n=1/n$ with $n\in\mathbb{N}$ is good for my porposes, and I have to deal with more general sequences (covergent ones) on $\mathbb{R}^2$ . So let $y_n$ be a convergent sequence on $\mathbb{R}^2$ (let us consider $y_n\neq y_m$ if $m\neq n$ ). My question is: Is there a homeomorphism $\phi:\mathbb{R}^2\to\mathbb{R}^2$ such that $\phi(1/n,0)=y_n$ for all $n\in\mathbb{N}$ (or at least for $n\geq k$ , for some $k\in\mathbb{N}$ )?","For some reasons the sequence with is good for my porposes, and I have to deal with more general sequences (covergent ones) on . So let be a convergent sequence on (let us consider if ). My question is: Is there a homeomorphism such that for all (or at least for , for some )?","x_n=1/n n\in\mathbb{N} \mathbb{R}^2 y_n \mathbb{R}^2 y_n\neq y_m m\neq n \phi:\mathbb{R}^2\to\mathbb{R}^2 \phi(1/n,0)=y_n n\in\mathbb{N} n\geq k k\in\mathbb{N}","['real-analysis', 'general-topology', 'algebraic-topology']"
29,Analogs of $\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}$,Analogs of,\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24},"Context: As I and others were able to show in the answers to this question , we have that $$\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}.$$ In my answer, I let $$E_{2k}(\tau)=1+c_{2k}\sum_{n\ge1}\frac{n^{2k-1}q^n}{1-q^n},$$ with $\tau\in\Bbb H$ and $q=e^{2\pi i\tau}$ and $c_{2k}=\frac{(2\pi i)^{2k}}{(2k-1)!\zeta(2k)}$ , be the Eisenstein series of weight $2k$ , where $k\in\Bbb Z_{>2}$ . Using the well known property that $$E_{2k}(-1/\tau)=\tau^{2k}E_{2k}(\tau),\tag1$$ it is easy to see that $$S_{2k-1}(e^{2i\pi/\tau})-\tau^{2k}S_{2k-1}(e^{-2i\pi\tau})=\frac{\tau^{2k}-1}{c_{2k}},\tag 2$$ where $$S_{\ell}(q)=\sum_{n\ge1}\frac{n^\ell}{q^n-1},$$ since $$E_{2k}(\tau)=1+c_{2k}S_{2k-1}(e^{-2i\pi\tau}).\tag3$$ Using the values $k=7$ and $\tau=i$ in $(2)$ , we get $$S_{13}(e^{2\pi})-i^{14}S_{13}(e^{2\pi})=\frac{i^{14}-1}{c_{14}},$$ which reduces to $$S_{13}(e^{2\pi})=\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}.$$ My Problem: I am trying to find identities analogous to $S_{13}(e^{2\pi})=1/24$ using the same general method. It is well known that $(1)$ is a the special case of $$E_{2k}\left(\frac{a\tau+b}{c\tau+d}\right)=(c\tau+d)^{2k}E_{2k}(\tau),\tag4$$ corresponding to the choice $\begin{pmatrix}a & b \\ c & d\end{pmatrix}=\begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix}$ . The formula $(4)$ is true for all $\begin{pmatrix}a & b \\ c & d\end{pmatrix}\in\text{SL}_2(\Bbb Z)$ . I was hoping of using $(4)$ instead of $(1)$ to generate identities analogous to the one in the title of this question. To this end, I denote $\gamma=\begin{pmatrix}a & b \\ c & d\end{pmatrix}$ and $\gamma(\tau)=\frac{a\tau+b}{c\tau+d}$ . Then using $(3)$ , we have $$S_{2k-1}(e^{-2i\pi\gamma(\tau)})-(c\tau+d)^{2k}S_{2k-1}(e^{-2i\pi\tau})=\frac{(c\tau+d)^{2k}-1}{c_{2k}}.$$ Then suppose we find $\tau\in\Bbb H$ such that $\gamma(\tau)=\tau$ . This would give $S_{2k-1}(e^{-2i\pi\gamma(\tau)})=S_{2k-1}(e^{-2i\pi\tau})$ and thus $$S_{2k-1}(e^{-2i\pi\tau})=-\frac{1}{c_{2k}}.\tag{*}$$ There is an infinite family for such $\tau\in\Bbb H$ , each corresponding to a unique element of $\text{SL}_2(\Bbb Z)$ . Using these I am able to find things like $$\sum_{n\ge1}\frac{n^{13}}{\zeta^ne^{n\pi\sqrt3/7}-1}=\frac1{24},\qquad \zeta=(-1)^{-5/7}, \gamma=\begin{pmatrix}-2 & 1 \\ -7 & 3\end{pmatrix},\tag5$$ $$\sum_{n\ge1}\frac{n^{13}}{\zeta^ne^{2n\pi/5}-1}=\frac{1}{24},\qquad \zeta=(-1)^{-2/5},\gamma=\begin{pmatrix}2 & 1 \\ -5 & -2\end{pmatrix},\tag6$$ and others. NOTE: I am unable to numerically test identities $(5),(6)$ because I only have desmos, which doesn't do complex numbers. So I am relying on the theory which I have laid out. My Question: The keen among you may have noticed that the formula $(*)$ does not hold for all values of $k\in\Bbb Z_{>2}$ . Indeed, we have all the values listed here which show that the values of $S_{2k-1}(e^{-2i\pi\tau})$ for certain fixed $\tau$ differ from $-1/c_{2k}$ for certain values of $k$ but not for others. My question is, why does this happen. For example, if we plug in $k=6$ and $\tau=\frac{1+i\sqrt3}{2}$ into $(*)$ we should get $$\sum_{n\ge1}\frac{n^{11}}{(-1)^ne^{n\pi\sqrt3}-1}=-\frac{691}{65520},$$ but actually, $$\sum_{n\ge1}\frac{n^{11}}{(-1)^ne^{n\pi\sqrt3}-1}=\frac{189\Gamma\left ( \frac{1}{4}  \right )^{24} }{272629760\pi^{18}}-\frac{691}{65520}.$$ Why does this happen? Where does that $\Gamma$ term come from? Why doesn't $(*)$ work for all $k\in\Bbb Z_{>2}$ and $\gamma\in\text{SL}_2(\Bbb Z)$ when there is a solution $z=\tau\in\Bbb H$ to $\gamma(z)=z$ ? I figure it has something to do with elliptic curves and elliptic integrals, but I don't know enough of the theory to see it.","Context: As I and others were able to show in the answers to this question , we have that In my answer, I let with and and , be the Eisenstein series of weight , where . Using the well known property that it is easy to see that where since Using the values and in , we get which reduces to My Problem: I am trying to find identities analogous to using the same general method. It is well known that is a the special case of corresponding to the choice . The formula is true for all . I was hoping of using instead of to generate identities analogous to the one in the title of this question. To this end, I denote and . Then using , we have Then suppose we find such that . This would give and thus There is an infinite family for such , each corresponding to a unique element of . Using these I am able to find things like and others. NOTE: I am unable to numerically test identities because I only have desmos, which doesn't do complex numbers. So I am relying on the theory which I have laid out. My Question: The keen among you may have noticed that the formula does not hold for all values of . Indeed, we have all the values listed here which show that the values of for certain fixed differ from for certain values of but not for others. My question is, why does this happen. For example, if we plug in and into we should get but actually, Why does this happen? Where does that term come from? Why doesn't work for all and when there is a solution to ? I figure it has something to do with elliptic curves and elliptic integrals, but I don't know enough of the theory to see it.","\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}. E_{2k}(\tau)=1+c_{2k}\sum_{n\ge1}\frac{n^{2k-1}q^n}{1-q^n}, \tau\in\Bbb H q=e^{2\pi i\tau} c_{2k}=\frac{(2\pi i)^{2k}}{(2k-1)!\zeta(2k)} 2k k\in\Bbb Z_{>2} E_{2k}(-1/\tau)=\tau^{2k}E_{2k}(\tau),\tag1 S_{2k-1}(e^{2i\pi/\tau})-\tau^{2k}S_{2k-1}(e^{-2i\pi\tau})=\frac{\tau^{2k}-1}{c_{2k}},\tag 2 S_{\ell}(q)=\sum_{n\ge1}\frac{n^\ell}{q^n-1}, E_{2k}(\tau)=1+c_{2k}S_{2k-1}(e^{-2i\pi\tau}).\tag3 k=7 \tau=i (2) S_{13}(e^{2\pi})-i^{14}S_{13}(e^{2\pi})=\frac{i^{14}-1}{c_{14}}, S_{13}(e^{2\pi})=\sum_{n\ge1}\frac{n^{13}}{e^{2\pi n}-1}=\frac{1}{24}. S_{13}(e^{2\pi})=1/24 (1) E_{2k}\left(\frac{a\tau+b}{c\tau+d}\right)=(c\tau+d)^{2k}E_{2k}(\tau),\tag4 \begin{pmatrix}a & b \\ c & d\end{pmatrix}=\begin{pmatrix}0 & -1 \\ 1 & 0\end{pmatrix} (4) \begin{pmatrix}a & b \\ c & d\end{pmatrix}\in\text{SL}_2(\Bbb Z) (4) (1) \gamma=\begin{pmatrix}a & b \\ c & d\end{pmatrix} \gamma(\tau)=\frac{a\tau+b}{c\tau+d} (3) S_{2k-1}(e^{-2i\pi\gamma(\tau)})-(c\tau+d)^{2k}S_{2k-1}(e^{-2i\pi\tau})=\frac{(c\tau+d)^{2k}-1}{c_{2k}}. \tau\in\Bbb H \gamma(\tau)=\tau S_{2k-1}(e^{-2i\pi\gamma(\tau)})=S_{2k-1}(e^{-2i\pi\tau}) S_{2k-1}(e^{-2i\pi\tau})=-\frac{1}{c_{2k}}.\tag{*} \tau\in\Bbb H \text{SL}_2(\Bbb Z) \sum_{n\ge1}\frac{n^{13}}{\zeta^ne^{n\pi\sqrt3/7}-1}=\frac1{24},\qquad \zeta=(-1)^{-5/7}, \gamma=\begin{pmatrix}-2 & 1 \\ -7 & 3\end{pmatrix},\tag5 \sum_{n\ge1}\frac{n^{13}}{\zeta^ne^{2n\pi/5}-1}=\frac{1}{24},\qquad \zeta=(-1)^{-2/5},\gamma=\begin{pmatrix}2 & 1 \\ -5 & -2\end{pmatrix},\tag6 (5),(6) (*) k\in\Bbb Z_{>2} S_{2k-1}(e^{-2i\pi\tau}) \tau -1/c_{2k} k k=6 \tau=\frac{1+i\sqrt3}{2} (*) \sum_{n\ge1}\frac{n^{11}}{(-1)^ne^{n\pi\sqrt3}-1}=-\frac{691}{65520}, \sum_{n\ge1}\frac{n^{11}}{(-1)^ne^{n\pi\sqrt3}-1}=\frac{189\Gamma\left ( \frac{1}{4}  \right )^{24} }{272629760\pi^{18}}-\frac{691}{65520}. \Gamma (*) k\in\Bbb Z_{>2} \gamma\in\text{SL}_2(\Bbb Z) z=\tau\in\Bbb H \gamma(z)=z","['real-analysis', 'complex-analysis', 'analytic-number-theory', 'elliptic-curves', 'modular-forms']"
30,Prove $f(x)$ is differentiable at a point.,Prove  is differentiable at a point.,f(x),"Let $f(x)$ be a funciton defined over $(0,1)$ such that $$f(x)=\begin{cases} \dfrac{1}{p^3},&x=\dfrac{q}{p},\text{where} ~p,q  \in \mathbb{N}, \text{and}~ {\rm GCD}(p,q)=1,\\ 0, &\text{otherwise}.   \end{cases}$$ Prove $f(x)$ is differentiable at $x=\dfrac{n\sqrt{2}}{m}$ for any $m,n \in \mathbb{N}$ . Obviously, what we need to do is show that $$\lim_{h \to 0}\dfrac{f\left(\dfrac{n\sqrt{2}}{m}+h\right)-f\left(\dfrac{n\sqrt{2}}{m}\right)}{h}$$ exists. Since $\dfrac{n\sqrt{2}}{m}$ is irrational, $f\left(\dfrac{n\sqrt{2}}{m}\right)=0$ . Therefore $$\lim_{h \to 0}\dfrac{f\left(\dfrac{n\sqrt{2}}{m}+h\right)-f\left(\dfrac{n\sqrt{2}}{m}\right)}{h}=\lim_{h \to 0}\dfrac{f\left(\dfrac{n\sqrt{2}}{m}+h\right)}{h}.$$ How to go on?","Let be a funciton defined over such that Prove is differentiable at for any . Obviously, what we need to do is show that exists. Since is irrational, . Therefore How to go on?","f(x) (0,1) f(x)=\begin{cases} \dfrac{1}{p^3},&x=\dfrac{q}{p},\text{where} ~p,q
 \in \mathbb{N}, \text{and}~ {\rm GCD}(p,q)=1,\\ 0, &\text{otherwise}. 
 \end{cases} f(x) x=\dfrac{n\sqrt{2}}{m} m,n \in \mathbb{N} \lim_{h \to 0}\dfrac{f\left(\dfrac{n\sqrt{2}}{m}+h\right)-f\left(\dfrac{n\sqrt{2}}{m}\right)}{h} \dfrac{n\sqrt{2}}{m} f\left(\dfrac{n\sqrt{2}}{m}\right)=0 \lim_{h \to 0}\dfrac{f\left(\dfrac{n\sqrt{2}}{m}+h\right)-f\left(\dfrac{n\sqrt{2}}{m}\right)}{h}=\lim_{h \to 0}\dfrac{f\left(\dfrac{n\sqrt{2}}{m}+h\right)}{h}.","['real-analysis', 'calculus', 'limits', 'derivatives']"
31,how do I justify this $\int_{0}^{\infty}\frac {e^{-2x}-e^{-x}}{x}dx=\lim_ {s\to1^{-}} \int_{0}^{\infty}\frac{e^{-2x}-e^{-x}}{x^s}dx$,how do I justify this,\int_{0}^{\infty}\frac {e^{-2x}-e^{-x}}{x}dx=\lim_ {s\to1^{-}} \int_{0}^{\infty}\frac{e^{-2x}-e^{-x}}{x^s}dx,"I was trying to compute $\displaystyle\int_{0}^{\infty} \frac{{\rm e}^{-2x} - {\rm e}^{-x}}{x}\,{\rm d}x$ when I had an idea, the gamma function is $\Gamma(1+z)=\int_{0}^{\infty}x^ze^{-x}dx$ and for $s<1$ the integral $\int_{0}^{\infty}\frac{e^{-2x}-e^{-x}}{x^s}dx$ converges, so call $$I(s)=\int_{0}^{\infty}\frac{e^{-2x}-e^{-x}}{x^s}dx$$ and hope that it is continuous from the left at $s=1$ . Assuming that it is, for $s<1$ we have $$I(s)=(2^{s-1}-1)\Gamma(1-s)$$ and so: $$I(1)=\int_{0}^{\infty}\frac {e^{-2x}-e^{-x}}{x}dx = \lim_{s\to1^-}I(s)= \lim_{s\to1^-}(2^{s-1}-1)\Gamma(1-s)$$ Now, write $$2^{s-1}-1=1+\ln(2)(s-1)+o((s-1)^2)-1=\ln(2)(s-1)+o((s-1)^2)$$ and so: $$\lim_{s\to1^-}(2^{s-1}-1)\Gamma(1-s) = -\ln(2)\lim_{s\to 1^-}(1-s)\Gamma(1-s)+\lim_{s\to 1^-}o((1-s)^2)\Gamma(1-s)$$ Here comes the cool part, use the reflection formula ( $\Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin(\pi z)}$ ) and obtain: $$(1-s)\Gamma(1-s) = \frac {\pi (1-s)}{\sin(\pi s)\Gamma(s)}$$ and: $$o((1-s)^2)\Gamma(1-s) = o(1-s)\frac {\pi (1-s)}{\sin(\pi s)\Gamma(s)}$$ Now, $\Gamma$ is continuous at 1 and $\Gamma(1)=1$ so: $$\lim_{s\to 1^-}\frac {\pi (1-s)}{\sin(\pi s)\Gamma(s)} = \lim_{s\to 1^-}\frac {\pi (1-s)}{\sin(\pi s)}$$ this limit is finite and is equal to 1 (with l'hopital) and so $$\lim_{s\to 1^-}o((1-s)^2)\Gamma(1-s) = 0$$ And we can finally finish up and say $I(1)=-\ln(2)$ . Now, I don't know how to justify the fact that $I(s)$ is continuous at $s=1$ from the left and so I don't know if my solution is correct. BUT! the answer is correct which gives me hope that this solution is correct with some additional justifications about the continuity of $I$ at $s=1^-$ which is why I am looking for help.","I was trying to compute when I had an idea, the gamma function is and for the integral converges, so call and hope that it is continuous from the left at . Assuming that it is, for we have and so: Now, write and so: Here comes the cool part, use the reflection formula ( ) and obtain: and: Now, is continuous at 1 and so: this limit is finite and is equal to 1 (with l'hopital) and so And we can finally finish up and say . Now, I don't know how to justify the fact that is continuous at from the left and so I don't know if my solution is correct. BUT! the answer is correct which gives me hope that this solution is correct with some additional justifications about the continuity of at which is why I am looking for help.","\displaystyle\int_{0}^{\infty}
\frac{{\rm e}^{-2x} - {\rm e}^{-x}}{x}\,{\rm d}x \Gamma(1+z)=\int_{0}^{\infty}x^ze^{-x}dx s<1 \int_{0}^{\infty}\frac{e^{-2x}-e^{-x}}{x^s}dx I(s)=\int_{0}^{\infty}\frac{e^{-2x}-e^{-x}}{x^s}dx s=1 s<1 I(s)=(2^{s-1}-1)\Gamma(1-s) I(1)=\int_{0}^{\infty}\frac {e^{-2x}-e^{-x}}{x}dx = \lim_{s\to1^-}I(s)= \lim_{s\to1^-}(2^{s-1}-1)\Gamma(1-s) 2^{s-1}-1=1+\ln(2)(s-1)+o((s-1)^2)-1=\ln(2)(s-1)+o((s-1)^2) \lim_{s\to1^-}(2^{s-1}-1)\Gamma(1-s) = -\ln(2)\lim_{s\to 1^-}(1-s)\Gamma(1-s)+\lim_{s\to 1^-}o((1-s)^2)\Gamma(1-s) \Gamma(z)\Gamma(1-z)=\frac{\pi}{\sin(\pi z)} (1-s)\Gamma(1-s) = \frac {\pi (1-s)}{\sin(\pi s)\Gamma(s)} o((1-s)^2)\Gamma(1-s) = o(1-s)\frac {\pi (1-s)}{\sin(\pi s)\Gamma(s)} \Gamma \Gamma(1)=1 \lim_{s\to 1^-}\frac {\pi (1-s)}{\sin(\pi s)\Gamma(s)} = \lim_{s\to 1^-}\frac {\pi (1-s)}{\sin(\pi s)} \lim_{s\to 1^-}o((1-s)^2)\Gamma(1-s) = 0 I(1)=-\ln(2) I(s) s=1 I s=1^-","['real-analysis', 'integration', 'gamma-function']"
32,Does the existence of a minimal cover for a subset of reals need some form of choice?,Does the existence of a minimal cover for a subset of reals need some form of choice?,,"In Kanamori's book, ""The Higher Infinite"" p. 376, he defines a minimal cover of some $A \subseteq \omega^\omega$ , to be any $B \subseteq \omega^\omega$ , such that $A\subseteq B$ and that $B$ is Lebesgue measurable and if $Z \subseteq B-A$ is Lebesgue measurable, then $m_L(Z) = 0$ . And he claims that picking some $B$ with $A\subseteq B$ and $m_L(B)$ minimal, does the job. [Here $m_L$ denotes the Lebesgue measure.] Now here is my problem. The whole premise of this chapter is that we don't want to use choice to do these things. But any way I try to construct such a $B$ , I inevitably use some form of choice. The best I can do is $\mathsf{AC}_\omega(\omega^\omega)$ . Is there some choice-free way to do this? A sketch of a proof with $\mathsf{AC}_\omega(\omega^\omega)$ : Let $x = \inf\{m_L(B): A\subseteq B \text{ and } B \text{ is Lebesgue measurable}\}$ . By $\mathsf{AC}_\omega(\omega^\omega)$ , let $\langle B_n: n<\omega\rangle$ be a sequence such that $A\subseteq B_n$ and $m_L(B_n) \rightarrow x$ as $n \rightarrow \infty$ . Now $B = \bigcap_n B_n$ is the desired minimal cover. $\square$","In Kanamori's book, ""The Higher Infinite"" p. 376, he defines a minimal cover of some , to be any , such that and that is Lebesgue measurable and if is Lebesgue measurable, then . And he claims that picking some with and minimal, does the job. [Here denotes the Lebesgue measure.] Now here is my problem. The whole premise of this chapter is that we don't want to use choice to do these things. But any way I try to construct such a , I inevitably use some form of choice. The best I can do is . Is there some choice-free way to do this? A sketch of a proof with : Let . By , let be a sequence such that and as . Now is the desired minimal cover.",A \subseteq \omega^\omega B \subseteq \omega^\omega A\subseteq B B Z \subseteq B-A m_L(Z) = 0 B A\subseteq B m_L(B) m_L B \mathsf{AC}_\omega(\omega^\omega) \mathsf{AC}_\omega(\omega^\omega) x = \inf\{m_L(B): A\subseteq B \text{ and } B \text{ is Lebesgue measurable}\} \mathsf{AC}_\omega(\omega^\omega) \langle B_n: n<\omega\rangle A\subseteq B_n m_L(B_n) \rightarrow x n \rightarrow \infty B = \bigcap_n B_n \square,"['real-analysis', 'measure-theory', 'set-theory', 'axiom-of-choice']"
33,When is a limit of measure preserving maps also measure preserving?,When is a limit of measure preserving maps also measure preserving?,,"Take some compact space, for simplicity take the interval $[0,1]$ . Let $f_n:[0,1] \to [0,1]$ be measure preserving, i.e. $\mu (f_n^{-1}(A))=\mu(A)$ for all lebesgue measurable $A \subset [0,1]$ . The question is under what kinds of convergence $f_n \to f$ do we also ensure that $f$ is measure preserving? I think I can show that for $f_n \to f$ in $L^2$ this holds. I'm now wondering about weak convergence $f_n \rightharpoonup f$ in $L^2$ ? Anyone know the answer (and maybe a proof/counterexample)?","Take some compact space, for simplicity take the interval . Let be measure preserving, i.e. for all lebesgue measurable . The question is under what kinds of convergence do we also ensure that is measure preserving? I think I can show that for in this holds. I'm now wondering about weak convergence in ? Anyone know the answer (and maybe a proof/counterexample)?","[0,1] f_n:[0,1] \to [0,1] \mu (f_n^{-1}(A))=\mu(A) A \subset [0,1] f_n \to f f f_n \to f L^2 f_n \rightharpoonup f L^2","['real-analysis', 'measure-theory', 'weak-convergence', 'ergodic-theory']"
34,"Find all sequences that has $\sum_{i=1}^\infty a_i$ converges, where $a_i = \sum_{k=i+1}^\infty a_k^2$.","Find all sequences that has  converges, where .",\sum_{i=1}^\infty a_i a_i = \sum_{k=i+1}^\infty a_k^2,"Find all sequences that has $\sum_{i=1}^\infty a_i$ converges, where $a_i = \sum_{k=i+1}^\infty a_k^2$ . My intuition is that the only sequence of this form is the zero sequence. Here's what I have so far: $a_n - a_{n+1} = a_{n+1}^2 \implies a_{n+1} = \sqrt{a_n + \frac{1}{4}}  - \frac{1}{2}$ , but it doesn't seem to lead me anywhere. Another line of thought is that if $a_i = 0$ for some $i$ , it means that $\sum_{k=i+1}^\infty a_k^2=0$ , which means that $a_k = 0$ for $k > i$ . This will also mean $a_{i-1} = 0, a_{i-2} = 0, \ldots$ , making the whole sequence the zero sequence. It means that $a_i >0 $ for all $i$ , yet $\lim a_i = 0$ . The last line I've tried is $a_1 = a_2^2 + a_3^2 + a_4^2 + \ldots, a_2 = a_3^2 + a_4^2 + \ldots$ , so $\sum_{i=1}^\infty a_i =  a_2^2 + a_3^2 + a_4^2 + \ldots + a_3^2 + a_4^2 + \ldots = a_2^2 + 2a_3^2 + 3a_4^2 = \sum_{i=2}^\infty (i-1)a_i^2$ , which implies a stronger condition of having $ia_i^2 \to 0$ . I'm hoping to get a contradiction but it doesn't seem to work. Python seems to suggest that $(a_n) \approx \frac{1}{n}$ for large $n$ . Any hints?","Find all sequences that has converges, where . My intuition is that the only sequence of this form is the zero sequence. Here's what I have so far: , but it doesn't seem to lead me anywhere. Another line of thought is that if for some , it means that , which means that for . This will also mean , making the whole sequence the zero sequence. It means that for all , yet . The last line I've tried is , so , which implies a stronger condition of having . I'm hoping to get a contradiction but it doesn't seem to work. Python seems to suggest that for large . Any hints?","\sum_{i=1}^\infty a_i a_i = \sum_{k=i+1}^\infty a_k^2 a_n - a_{n+1} = a_{n+1}^2 \implies a_{n+1} = \sqrt{a_n + \frac{1}{4}}  - \frac{1}{2} a_i = 0 i \sum_{k=i+1}^\infty a_k^2=0 a_k = 0 k > i a_{i-1} = 0, a_{i-2} = 0, \ldots a_i >0  i \lim a_i = 0 a_1 = a_2^2 + a_3^2 + a_4^2 + \ldots, a_2 = a_3^2 + a_4^2 + \ldots \sum_{i=1}^\infty a_i =  a_2^2 + a_3^2 + a_4^2 + \ldots + a_3^2 + a_4^2 + \ldots = a_2^2 + 2a_3^2 + 3a_4^2 = \sum_{i=2}^\infty (i-1)a_i^2 ia_i^2 \to 0 (a_n) \approx \frac{1}{n} n","['real-analysis', 'sequences-and-series']"
35,Is there a closed form for $\int_0^1 \binom{1}{x}\frac{\log^2(1-x)}{x}\ \mathrm{d}x$?,Is there a closed form for ?,\int_0^1 \binom{1}{x}\frac{\log^2(1-x)}{x}\ \mathrm{d}x,"Do we know if there is a closed form for $$ I :=\int_0^1 \binom{1}{x}\frac{\log^2(1-x)}{x}\ \mathrm{d}x\mathrm{?} $$ Wolfram alpha gives an approximation of $2.66989$ which may be equivalent to: $$10\sqrt{\frac{2\pi}{77\log(\pi)}}.$$ As stated by @Mariusz Iwaniuk, in the comments, we have the equivalent representation of $$I\equiv \int_0^1 \frac{\sin(\pi x)\log^2(1-x)}{\pi x^2(1-x)}.$$ Another question, presumably simpler, could be $$\int_0^1 \binom{1}{x}\frac{\log(1-x)}{x} \equiv -\int_0^1 \binom{1}{x}\frac{\mathrm{Li}_1(x)}{x}.$$ I believe I can find a closed form for the latter; if I do, I will edit the post. In general, I am curious as to if we may be able to somehow employ Ramanujan's Beta integral or any of the other Beta integrals. Another approach may be the series representation for $\binom{1}{x}$ . Thanks!","Do we know if there is a closed form for Wolfram alpha gives an approximation of which may be equivalent to: As stated by @Mariusz Iwaniuk, in the comments, we have the equivalent representation of Another question, presumably simpler, could be I believe I can find a closed form for the latter; if I do, I will edit the post. In general, I am curious as to if we may be able to somehow employ Ramanujan's Beta integral or any of the other Beta integrals. Another approach may be the series representation for . Thanks!","
I :=\int_0^1 \binom{1}{x}\frac{\log^2(1-x)}{x}\ \mathrm{d}x\mathrm{?}
 2.66989 10\sqrt{\frac{2\pi}{77\log(\pi)}}. I\equiv \int_0^1 \frac{\sin(\pi x)\log^2(1-x)}{\pi x^2(1-x)}. \int_0^1 \binom{1}{x}\frac{\log(1-x)}{x} \equiv -\int_0^1 \binom{1}{x}\frac{\mathrm{Li}_1(x)}{x}. \binom{1}{x}","['real-analysis', 'calculus']"
36,Is this very weird function continuous?,Is this very weird function continuous?,,"I read about Conway's base 13 function and felt encouraged to procrastinate on my homework and play around with functions that involve binary expansions. This one function $K$ I came across caught my attention. Let $t \in (0,1)$ . Taking the binary expansion, we can write $t= \sum_{n=1}^\infty \frac{a_n}{2^n}$ , where $a_n \in \{0,1\}$ and $(a_n)$ cannot have an endless tail of 1's (i.e. for any $N \in \mathbb{N}$ with $a_N=1$ , there exists some $n \geq N$ with $a_n =0$ ). This makes the binary expansion unique for any $x \in (0,1)$ . We can create 2 numbers from this: $x= \sum_{n=1}^\infty \frac{a_{2n-1}}{2^n}$ and $y= \sum_{n=1}^\infty \frac{a_{2n}}{2^n}$ . We say $K(t)=(x,y)$ . In other words, we convert $t$ to binary format (e.g. $0.3141... \rightarrow 0.0101...$ ), and put all the odd-indexed digits in the binary expansion of $x$ , and even-indexed digits in the binary expansion of $y$ . Then you convert $x,y$ back to decimal format. Plotting the path, we have: This looks like a space-filling curve! Neat. It oddly looks similar to the Hilbert Curve . My question is if this function is continuous , since it zig-zags so much about the rational numbers. Just for funzies, when I add $x$ and $y$ , I can create a new function $k(x):=x+y$ . Doing so, I get this neat looking graph:","I read about Conway's base 13 function and felt encouraged to procrastinate on my homework and play around with functions that involve binary expansions. This one function I came across caught my attention. Let . Taking the binary expansion, we can write , where and cannot have an endless tail of 1's (i.e. for any with , there exists some with ). This makes the binary expansion unique for any . We can create 2 numbers from this: and . We say . In other words, we convert to binary format (e.g. ), and put all the odd-indexed digits in the binary expansion of , and even-indexed digits in the binary expansion of . Then you convert back to decimal format. Plotting the path, we have: This looks like a space-filling curve! Neat. It oddly looks similar to the Hilbert Curve . My question is if this function is continuous , since it zig-zags so much about the rational numbers. Just for funzies, when I add and , I can create a new function . Doing so, I get this neat looking graph:","K t \in (0,1) t= \sum_{n=1}^\infty \frac{a_n}{2^n} a_n \in \{0,1\} (a_n) N \in \mathbb{N} a_N=1 n \geq N a_n =0 x \in (0,1) x= \sum_{n=1}^\infty \frac{a_{2n-1}}{2^n} y= \sum_{n=1}^\infty \frac{a_{2n}}{2^n} K(t)=(x,y) t 0.3141... \rightarrow 0.0101... x y x,y x y k(x):=x+y","['real-analysis', 'continuity', 'recreational-mathematics']"
37,Open sets having an empty intersection but the intersection of their closure is not empty,Open sets having an empty intersection but the intersection of their closure is not empty,,"Suppose $V_{n}$ is a decreasing sequence of (bounded) open sets in $\mathbb{R}^{m}$ with $m\geq1$ . Suppose  the intersection of all $V_{n}$ is empty, and let $F$ be the intersection of the closures of $V_{n}$ . Can we say that there exists $N$ such that every $x$ in $F$ belongs to the boundary of $V_{n}$ , for $n\geq N$ ? (This question is suggested by setting $V_{n}=(0,1/n)$ )","Suppose is a decreasing sequence of (bounded) open sets in with . Suppose  the intersection of all is empty, and let be the intersection of the closures of . Can we say that there exists such that every in belongs to the boundary of , for ? (This question is suggested by setting )","V_{n} \mathbb{R}^{m} m\geq1 V_{n} F V_{n} N x F V_{n} n\geq N V_{n}=(0,1/n)","['real-analysis', 'general-topology']"
38,Proof verification: At most countably many local maxima,Proof verification: At most countably many local maxima,,"I'd appreciate a second pair of eyes on a proof. I want to prove that a function $f:\mathbb{R}\to\mathbb{R}$ can have at most countably many strict local maxima. The question has been asked elsewhere on Stack Exchange, but my question is about the validity of the following argument, which isn't discussed. Assume for contradiction that $f$ has uncountably many strict local maxima. For each $n\in\mathbb{N}$ , define $$E_n=\Big\{x\in \mathbb{R}: f(x)>f(y) \hspace{2mm}\text{for all $y$ such that $0<|x-y|<\frac{1}{n}$} \Big\}.$$ For example, if $x\in E_3$ , then $x$ provides a strict local maximum on at least an open interval of radius $\frac{1}{3}$ . If each $E_n$ was at most countable, then $\bigcup^{\infty}_{n=1}E_n$ would be countable as well, contrary to assumption. Hence, some set, say $E_{n_0}$ , is uncountable. Being uncountable, the set $E_{n_0}$ has a limit point, $\xi$ . But now this gives a contradiction. Let $\{x_k\}_{k=1}^{\infty}$ be a sequence in $E_{n_0}$ converging to $\xi$ . Let $x_i$ and $x_j$ be terms satisfying $|x_i-\xi|<\frac{1}{2n_0}$ and $|x_j-\xi|<\frac{1}{2n_0}$ . Then $|x_i-x_j|<\frac{1}{n_0}$ by the triangle inequality. Since $f(x_i)$ is a strict local maximum on an interval about $x_i$ of radius $\frac{1}{n_0}$ , we have $f(x_i)>f(x_j)$ . But for the same reason we must have $f(x_j)>f(x_i)$ , which is a contradiction. Thanks in advance for your feedback.","I'd appreciate a second pair of eyes on a proof. I want to prove that a function can have at most countably many strict local maxima. The question has been asked elsewhere on Stack Exchange, but my question is about the validity of the following argument, which isn't discussed. Assume for contradiction that has uncountably many strict local maxima. For each , define For example, if , then provides a strict local maximum on at least an open interval of radius . If each was at most countable, then would be countable as well, contrary to assumption. Hence, some set, say , is uncountable. Being uncountable, the set has a limit point, . But now this gives a contradiction. Let be a sequence in converging to . Let and be terms satisfying and . Then by the triangle inequality. Since is a strict local maximum on an interval about of radius , we have . But for the same reason we must have , which is a contradiction. Thanks in advance for your feedback.",f:\mathbb{R}\to\mathbb{R} f n\in\mathbb{N} E_n=\Big\{x\in \mathbb{R}: f(x)>f(y) \hspace{2mm}\text{for all y such that 0<|x-y|<\frac{1}{n}} \Big\}. x\in E_3 x \frac{1}{3} E_n \bigcup^{\infty}_{n=1}E_n E_{n_0} E_{n_0} \xi \{x_k\}_{k=1}^{\infty} E_{n_0} \xi x_i x_j |x_i-\xi|<\frac{1}{2n_0} |x_j-\xi|<\frac{1}{2n_0} |x_i-x_j|<\frac{1}{n_0} f(x_i) x_i \frac{1}{n_0} f(x_i)>f(x_j) f(x_j)>f(x_i),"['real-analysis', 'proof-verification']"
39,Proving that $\lim\limits_{x\to 1^{-}}\frac{1}{\ln(1-x)}\sum\limits_{n=0}^{\infty}x^{b^n}=-\frac{1}{\ln(b)}$,Proving that,\lim\limits_{x\to 1^{-}}\frac{1}{\ln(1-x)}\sum\limits_{n=0}^{\infty}x^{b^n}=-\frac{1}{\ln(b)},"I conjecture that : $$\forall b\in\mathbb{N}\setminus\lbrace0,1\rbrace,\lim\limits_{x\to 1^{-}}\frac{1}{\ln(1-x)}\sum\limits_{n=0}^{\infty}x^{b^n}=-\frac{1}{\ln(b)}$$ Which is well verified through numerical simulations. Maybe I'm missing something obvious here, but I have absolutely no idea as of how to prove it. Uniform convergence is of course of no help here, the series $\sum\limits_{n=0}^{\infty}1$ being trivially divergent. Any insight ?","I conjecture that : Which is well verified through numerical simulations. Maybe I'm missing something obvious here, but I have absolutely no idea as of how to prove it. Uniform convergence is of course of no help here, the series being trivially divergent. Any insight ?","\forall b\in\mathbb{N}\setminus\lbrace0,1\rbrace,\lim\limits_{x\to 1^{-}}\frac{1}{\ln(1-x)}\sum\limits_{n=0}^{\infty}x^{b^n}=-\frac{1}{\ln(b)} \sum\limits_{n=0}^{\infty}1","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
40,Let $X$ be Poisson r.v. with $\lambda$ find $f(x)$ such that $E[f(X)]=\lambda \log (\lambda)$,Let  be Poisson r.v. with  find  such that,X \lambda f(x) E[f(X)]=\lambda \log (\lambda),"I am looking for a function $f(x)$ such that \begin{align} E[f(X)]=\lambda \log (\lambda) \quad  \text{ for all } \lambda \ge 0 \tag{$*$} \end{align} where $X$ is a Poisson random varaible with parameter $\lambda$ . Note, we are looking for a function idenpendent of $\lambda$ . Here are some thoughts: \begin{align} \lambda \log (\lambda) = E[f(X)]= \sum_{k=0}^\infty f(k) \frac{\lambda^k e^{-\lambda}}{k!}  \end{align} Therefore, we have that \begin{align} e^{\lambda} \lambda \log (\lambda) =  \sum_{k=0}^\infty f(k) \frac{\lambda^k }{k!}= \sum_{k=0}^\infty a_k \frac{\lambda^k }{k!} \tag{$**$} \end{align} where in the last step I defined $f(k)=a_k$ . Now, this is the point where I am a bit stuck. It doesn't look like the expression in $(**)$ can hold as I don't think the function $g(x)=e^x x \log(x)$ can be written as a Maclaurin series.  These lets me to conclude that there is no function $f(x)$ such that $(*)$ holds.","I am looking for a function such that where is a Poisson random varaible with parameter . Note, we are looking for a function idenpendent of . Here are some thoughts: Therefore, we have that where in the last step I defined . Now, this is the point where I am a bit stuck. It doesn't look like the expression in can hold as I don't think the function can be written as a Maclaurin series.  These lets me to conclude that there is no function such that holds.","f(x) \begin{align}
E[f(X)]=\lambda \log (\lambda) \quad  \text{ for all } \lambda \ge 0 \tag{*}
\end{align} X \lambda \lambda \begin{align}
\lambda \log (\lambda) = E[f(X)]= \sum_{k=0}^\infty f(k) \frac{\lambda^k e^{-\lambda}}{k!} 
\end{align} \begin{align}
e^{\lambda} \lambda \log (\lambda) =  \sum_{k=0}^\infty f(k) \frac{\lambda^k }{k!}= \sum_{k=0}^\infty a_k \frac{\lambda^k }{k!} \tag{**}
\end{align} f(k)=a_k (**) g(x)=e^x x \log(x) f(x) (*)","['real-analysis', 'probability', 'probability-theory', 'poisson-distribution']"
41,Value of operator norm when $\mathcal{T}f(x)=\int^{x}_{0} f(t)dt$,Value of operator norm when,\mathcal{T}f(x)=\int^{x}_{0} f(t)dt,"Let $\mathcal{C}$ be the space of continuous functions on $[0,1]$ equipped with the norm $\|f\|=\int^{1}_{0}|f(t)|dt$ . Define a linear map $\mathcal{T}:\mathcal{C}\rightarrow \mathcal{C}$ by $$ \mathcal{T}f(x)=\int^{x}_{0}f(t)dt. $$ Show that $\mathcal{T}$ is well-defined and bounded and determine the value of $\|\mathcal{T}\|_{\text{op}}$ . I proved the first two parts myself, but I am having trouble with determining the value of $\|\mathcal{T}\|_{\text{op}}$ . I was able to show that it is bounded by $1$ though. Observe that $$\|\mathcal{T}f\|=\int^{1}_{0}\left|\int^{x}_{0}f(t)dt\right|dx\leq \int^{1}_{0}\|f\|dx = \|f\| $$ Therefore, $$ \|\mathcal{T}\|_{\text{op}} = \underset{\|f\|=1}{\sup}\frac{\|\mathcal{T}f\|}{\|f\|}\leq \underset{\|f\|=1}{\sup}\frac{\|f\|}{\|f\|}=1 $$ I tried seeing if I could then construct a function where the operator equals $1$ , but I've had no success. Anybody have any solutions or hints? Any help is appreciated.","Let be the space of continuous functions on equipped with the norm . Define a linear map by Show that is well-defined and bounded and determine the value of . I proved the first two parts myself, but I am having trouble with determining the value of . I was able to show that it is bounded by though. Observe that Therefore, I tried seeing if I could then construct a function where the operator equals , but I've had no success. Anybody have any solutions or hints? Any help is appreciated.","\mathcal{C} [0,1] \|f\|=\int^{1}_{0}|f(t)|dt \mathcal{T}:\mathcal{C}\rightarrow \mathcal{C}  \mathcal{T}f(x)=\int^{x}_{0}f(t)dt.  \mathcal{T} \|\mathcal{T}\|_{\text{op}} \|\mathcal{T}\|_{\text{op}} 1 \|\mathcal{T}f\|=\int^{1}_{0}\left|\int^{x}_{0}f(t)dt\right|dx\leq \int^{1}_{0}\|f\|dx = \|f\|   \|\mathcal{T}\|_{\text{op}} = \underset{\|f\|=1}{\sup}\frac{\|\mathcal{T}f\|}{\|f\|}\leq \underset{\|f\|=1}{\sup}\frac{\|f\|}{\|f\|}=1  1","['real-analysis', 'functional-analysis', 'operator-theory']"
42,Cesaro limits and supremum of a sequence,Cesaro limits and supremum of a sequence,,"$a_n$ be a sequence of real numbers with finite Cesaro limit, i.e., $\frac{a_1+ \cdots + a_n}{n} \rightarrow c$ for some real number $c$. If $M_n= \max\{a_1, \cdots, a_n\}$, then does $\frac{M_n}{n} \rightarrow 0$? I believe the answer is false and you can come up with alternating series with increasing absolute value such that the supremum may occur at last term and get non-zero limit. But, I’m unable to cook up such example. Thanks in advance.","$a_n$ be a sequence of real numbers with finite Cesaro limit, i.e., $\frac{a_1+ \cdots + a_n}{n} \rightarrow c$ for some real number $c$. If $M_n= \max\{a_1, \cdots, a_n\}$, then does $\frac{M_n}{n} \rightarrow 0$? I believe the answer is false and you can come up with alternating series with increasing absolute value such that the supremum may occur at last term and get non-zero limit. But, I’m unable to cook up such example. Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'limits']"
43,Compute $\lim\limits_{x\to \infty }\sum\limits_{n=1}^\infty \frac{1}{n(n+x)}$,Compute,\lim\limits_{x\to \infty }\sum\limits_{n=1}^\infty \frac{1}{n(n+x)},"I want to compute $$\lim_{x\to \infty }\sum_{n=1}^\infty \frac{1}{n(n+x)}.$$ Can I do as follow? Consider the measurable space $(\mathbb N,\mathcal P(\mathbb N),\mu)$ where $\mu(A)=\#A$. Then, $$\sum_{n=1}^\infty \frac{1}{n(n+x)}=\int_{\mathbb N}\frac{1}{n(n+x)}d\mu(n).$$ Suppose $|x|\geq 1$. Then $$\left|\frac{1}{n(n+x)}\right|\leq \frac{1}{n(n+1)}\in L^1(\mathbb N),$$ and thus, using DCT, we finally obtain $$\lim_{x\to \infty }\sum_{n=1}^\infty \frac{1}{n(n+x)}=\sum_{n=1}^\infty \lim_{x\to \infty }\frac{1}{n(n+x)}=0.$$ Does it work ?","I want to compute $$\lim_{x\to \infty }\sum_{n=1}^\infty \frac{1}{n(n+x)}.$$ Can I do as follow? Consider the measurable space $(\mathbb N,\mathcal P(\mathbb N),\mu)$ where $\mu(A)=\#A$. Then, $$\sum_{n=1}^\infty \frac{1}{n(n+x)}=\int_{\mathbb N}\frac{1}{n(n+x)}d\mu(n).$$ Suppose $|x|\geq 1$. Then $$\left|\frac{1}{n(n+x)}\right|\leq \frac{1}{n(n+1)}\in L^1(\mathbb N),$$ and thus, using DCT, we finally obtain $$\lim_{x\to \infty }\sum_{n=1}^\infty \frac{1}{n(n+x)}=\sum_{n=1}^\infty \lim_{x\to \infty }\frac{1}{n(n+x)}=0.$$ Does it work ?",,"['real-analysis', 'sequences-and-series', 'measure-theory', 'summation']"
44,integral $\int_{0}^{1}\left( \left\lfloor{\frac{2}{x}} \right\rfloor-2 \left\lfloor{\frac{1}{x}} \right\rfloor \right)dx$,integral,\int_{0}^{1}\left( \left\lfloor{\frac{2}{x}} \right\rfloor-2 \left\lfloor{\frac{1}{x}} \right\rfloor \right)dx,Evaluate $$\int_{0}^{1}\left( \left\lfloor{\frac{2}{x}} \right\rfloor-2 \left\lfloor{\frac{1}{x}} \right\rfloor \right)dx$$ My Attempt $$I_{1}=\int_{0}^{1}\left\lfloor{\frac{2}{x}} \right\rfloor dx$$ Put $x=2t$ $$I_{1}=\int_{0}^{\frac{1}{2}}2\left\lfloor{\frac{1}{t}} \right\rfloor dt=\int_{0}^{\frac{1}{2}}2\left\lfloor{\frac{1}{x}}\right\rfloor dx$$ let $$I_{2}=\int_{0}^{1} \left\lfloor{\frac{1}{x}} \right\rfloor dx=\int_{0}^{\frac{1}{2}}\left\lfloor{\frac{1}{x}}\right\rfloor dx+\int_{\frac{1}{2}}^{1}\left\lfloor{\frac{1}{x}}\right\rfloor dx$$ Given integral  $$ \begin{align} \int_{0}^{1}\left(\left\lfloor{\frac{2}{x}}\right\rfloor-2\left\lfloor{\frac{1}{x}}\right\rfloor \right)dx&=I_{1}-2I_{2}\\ &=2\int_{0}^{\frac{1}{2}}\left\lfloor{\frac{1}{x}}\right\rfloor dx-2\int_{0}^{\frac{1}{2}}\left\lfloor{\frac{1}{x}}\right\rfloor dx-2\int_{\frac{1}{2}}^{1}\left\lfloor{\frac{1}{x}}\right\rfloor dx\\ &=-2\int_{\frac{1}{2}}^{1}\left\lfloor{\frac{1}{x}}\right\rfloor dx\\ &=(-2)(1)=-2. \end{align} $$ But answer given is $\ln(\frac{4}{e})$ What mistake am I making?,Evaluate $$\int_{0}^{1}\left( \left\lfloor{\frac{2}{x}} \right\rfloor-2 \left\lfloor{\frac{1}{x}} \right\rfloor \right)dx$$ My Attempt $$I_{1}=\int_{0}^{1}\left\lfloor{\frac{2}{x}} \right\rfloor dx$$ Put $x=2t$ $$I_{1}=\int_{0}^{\frac{1}{2}}2\left\lfloor{\frac{1}{t}} \right\rfloor dt=\int_{0}^{\frac{1}{2}}2\left\lfloor{\frac{1}{x}}\right\rfloor dx$$ let $$I_{2}=\int_{0}^{1} \left\lfloor{\frac{1}{x}} \right\rfloor dx=\int_{0}^{\frac{1}{2}}\left\lfloor{\frac{1}{x}}\right\rfloor dx+\int_{\frac{1}{2}}^{1}\left\lfloor{\frac{1}{x}}\right\rfloor dx$$ Given integral  $$ \begin{align} \int_{0}^{1}\left(\left\lfloor{\frac{2}{x}}\right\rfloor-2\left\lfloor{\frac{1}{x}}\right\rfloor \right)dx&=I_{1}-2I_{2}\\ &=2\int_{0}^{\frac{1}{2}}\left\lfloor{\frac{1}{x}}\right\rfloor dx-2\int_{0}^{\frac{1}{2}}\left\lfloor{\frac{1}{x}}\right\rfloor dx-2\int_{\frac{1}{2}}^{1}\left\lfloor{\frac{1}{x}}\right\rfloor dx\\ &=-2\int_{\frac{1}{2}}^{1}\left\lfloor{\frac{1}{x}}\right\rfloor dx\\ &=(-2)(1)=-2. \end{align} $$ But answer given is $\ln(\frac{4}{e})$ What mistake am I making?,,"['real-analysis', 'definite-integrals']"
45,"Existence of a function behaving like $s^\beta$ everywhere ($\beta\in(0,1)$)",Existence of a function behaving like  everywhere (),"s^\beta \beta\in(0,1)","Does there exist a function $f:\mathbb R\to\mathbb R$ such that $$ \lim_{s\downarrow 0} \frac{f(t+s)-f(t)}{s^\beta}=c,\quad \text{for every }t\in\mathbb R, $$ where $\beta\in(0,1)$ and $c\in\mathbb R\backslash \{0\}$? I am happy if you answer the same question but with a non-zero continuous function $c(t)$ on the right  hand side.","Does there exist a function $f:\mathbb R\to\mathbb R$ such that $$ \lim_{s\downarrow 0} \frac{f(t+s)-f(t)}{s^\beta}=c,\quad \text{for every }t\in\mathbb R, $$ where $\beta\in(0,1)$ and $c\in\mathbb R\backslash \{0\}$? I am happy if you answer the same question but with a non-zero continuous function $c(t)$ on the right  hand side.",,"['real-analysis', 'holder-spaces']"
46,Analytic functions whose all derivatives keep $\mathbb{Z}$ invariant,Analytic functions whose all derivatives keep  invariant,\mathbb{Z},"Is there a real  analytic function $f:\mathbb{R} \to \mathbb{R}$ which is  not a polynomial function but all its derivatives $f^{(n)}$ satisfy $f^{(n)}(\mathbb{Z})\subseteq \mathbb{Z}$? This  question was  included in a more  flexible question as follows but according to its answers  and comments, I realize that the smooth version of this question was obvious. Smooth or analytic functions which keep $\mathbb{Z}$ invariant","Is there a real  analytic function $f:\mathbb{R} \to \mathbb{R}$ which is  not a polynomial function but all its derivatives $f^{(n)}$ satisfy $f^{(n)}(\mathbb{Z})\subseteq \mathbb{Z}$? This  question was  included in a more  flexible question as follows but according to its answers  and comments, I realize that the smooth version of this question was obvious. Smooth or analytic functions which keep $\mathbb{Z}$ invariant",,"['real-analysis', 'analytic-functions']"
47,Is there an 'intrinsic' characterization of the usual topology on a finite-dimensional vector space?,Is there an 'intrinsic' characterization of the usual topology on a finite-dimensional vector space?,,"Let $V$ denote a finite-dimensional vector space. Then $V$ becomes a topological space in a canonical way, by choosing a basis and using this to get an isomorphism to Euclidean space. It turns out that the topology you get is independent of choice of basis, so this makes $V$ into a topological vector space in a canonical way. Question. Is there a more intrinsic, or abstract, or basis-free approach to characterizing this topology, that gets closer to the heart of why it's important?","Let $V$ denote a finite-dimensional vector space. Then $V$ becomes a topological space in a canonical way, by choosing a basis and using this to get an isomorphism to Euclidean space. It turns out that the topology you get is independent of choice of basis, so this makes $V$ into a topological vector space in a canonical way. Question. Is there a more intrinsic, or abstract, or basis-free approach to characterizing this topology, that gets closer to the heart of why it's important?",,"['real-analysis', 'general-topology', 'functional-analysis', 'vector-spaces', 'topological-vector-spaces']"
48,An increasing sequence of simple functions approximates a measurable function,An increasing sequence of simple functions approximates a measurable function,,"The following is from page 31 of Stein and Shakarchi's Real Analysis . My question is about an aspect of the proof of the following theorem. Theorem 4.1 Suppose $f$ is a non-negative measurable function on $\mathbb R^d$. Then there exists an increasing sequence of non-negative simple functions $\{\varphi_k\}_{k=1}^\infty$ that converges pointwise to $f$, namely,   $$ \varphi_k(x) \le \varphi_{k+1}(x)\quad\text{and}\quad\lim_{k\to\infty}\varphi_k(x)=f(x),\ \text{for all $x$.} $$ Proof. We begin first with a truncation. For $k\ge 1$, let $Q_k$ denote the cube centered at the origin and of side length $k$. Then we define   $$ F_k(x) = \begin{cases} f(x) & \text{if $x\in Q_k$ and $f(x)\le k$,} \\ k & \text{if $x\in Q_k$ and $f(x)> k$,}\\ 0 & \text{otherwise.} \end{cases} $$   Then $F_k(x)\to f(x)$ as $k$ tends to infinity for all $x$. Now, we partition the range of $F_k$, namely $[0,k]$ as follows. For fixed $k,j\ge 1$, we define   $$ E_{\ell,j}=\left\{x\in Q_k:\frac{\ell}{j}<F_k(x)\le\frac{\ell+1}{j}\right\},\quad\text{for}\ 0\le\ell<kj. $$   Then we may form   $$ F_{k,j}(x) = \sum_{\ell=0}^{kj-1}\frac{\ell}{j}{\large{\chi_{E_{\ell,j}}}}(x) $$   [where $\large{\chi_{E_{\ell,j}}}$ is the indicator function of $E_{\ell,j}$]. Each $F_{k,j}$ is a simple function that satisfies $0\le F_k(x)-F_{k,j}(x)\le 1/j$ for all $x$. If we now choose $j=k$, and let $\varphi_k = F_{k,k}$, then we see that $0\le F_k(x)-\varphi_k(x)\le 1/k$ for all $x$, $\color{red}{\underline{\color{black}{\text{and $\{\varphi_k\}$ satisfies all the desired properties.}}}}$ I do not see why $\varphi_k(x)\le\varphi_{k+1}(x)$ for all $x$. Can someone explain that?","The following is from page 31 of Stein and Shakarchi's Real Analysis . My question is about an aspect of the proof of the following theorem. Theorem 4.1 Suppose $f$ is a non-negative measurable function on $\mathbb R^d$. Then there exists an increasing sequence of non-negative simple functions $\{\varphi_k\}_{k=1}^\infty$ that converges pointwise to $f$, namely,   $$ \varphi_k(x) \le \varphi_{k+1}(x)\quad\text{and}\quad\lim_{k\to\infty}\varphi_k(x)=f(x),\ \text{for all $x$.} $$ Proof. We begin first with a truncation. For $k\ge 1$, let $Q_k$ denote the cube centered at the origin and of side length $k$. Then we define   $$ F_k(x) = \begin{cases} f(x) & \text{if $x\in Q_k$ and $f(x)\le k$,} \\ k & \text{if $x\in Q_k$ and $f(x)> k$,}\\ 0 & \text{otherwise.} \end{cases} $$   Then $F_k(x)\to f(x)$ as $k$ tends to infinity for all $x$. Now, we partition the range of $F_k$, namely $[0,k]$ as follows. For fixed $k,j\ge 1$, we define   $$ E_{\ell,j}=\left\{x\in Q_k:\frac{\ell}{j}<F_k(x)\le\frac{\ell+1}{j}\right\},\quad\text{for}\ 0\le\ell<kj. $$   Then we may form   $$ F_{k,j}(x) = \sum_{\ell=0}^{kj-1}\frac{\ell}{j}{\large{\chi_{E_{\ell,j}}}}(x) $$   [where $\large{\chi_{E_{\ell,j}}}$ is the indicator function of $E_{\ell,j}$]. Each $F_{k,j}$ is a simple function that satisfies $0\le F_k(x)-F_{k,j}(x)\le 1/j$ for all $x$. If we now choose $j=k$, and let $\varphi_k = F_{k,k}$, then we see that $0\le F_k(x)-\varphi_k(x)\le 1/k$ for all $x$, $\color{red}{\underline{\color{black}{\text{and $\{\varphi_k\}$ satisfies all the desired properties.}}}}$ I do not see why $\varphi_k(x)\le\varphi_{k+1}(x)$ for all $x$. Can someone explain that?",,"['real-analysis', 'measure-theory']"
49,convergence of $\sum\limits_{n=1}^\infty \frac{(-1)^{\lfloor \sqrt{n}\rfloor}}{\sqrt{n}}$,convergence of,\sum\limits_{n=1}^\infty \frac{(-1)^{\lfloor \sqrt{n}\rfloor}}{\sqrt{n}},Does anybody know how to show analytically the convergence (divergence ?) of $\enspace\displaystyle \sum\limits_{n=1}^\infty \frac{(-1)^{\lfloor \sqrt{n}\rfloor}}{\sqrt{n}}\enspace$ or some literature for it ? (Thanks!) Note : Unfortunately I am still missing a useful approach. EDIT: I want to thank all here for the nice help. Result: The series is divergent (but limited) as it is written. It's possible to change to $\enspace\displaystyle\sum\limits_{n=1}^\infty (-1)^n (-2+\sum\limits_{k=n^2}^{(n+1)^2-1}\frac{1}{\sqrt{k}})\enspace$ to avoid the oscillation and to get convergence.,Does anybody know how to show analytically the convergence (divergence ?) of $\enspace\displaystyle \sum\limits_{n=1}^\infty \frac{(-1)^{\lfloor \sqrt{n}\rfloor}}{\sqrt{n}}\enspace$ or some literature for it ? (Thanks!) Note : Unfortunately I am still missing a useful approach. EDIT: I want to thank all here for the nice help. Result: The series is divergent (but limited) as it is written. It's possible to change to $\enspace\displaystyle\sum\limits_{n=1}^\infty (-1)^n (-2+\sum\limits_{k=n^2}^{(n+1)^2-1}\frac{1}{\sqrt{k}})\enspace$ to avoid the oscillation and to get convergence.,,"['real-analysis', 'sequences-and-series']"
50,Understanding a geometric argument in the proof of the strong maximum principle for elliptic operators in Evans's PDE,Understanding a geometric argument in the proof of the strong maximum principle for elliptic operators in Evans's PDE,,"Here is the strong maximum principle in Evans's Partial Differential Equations : Here $U\subset\mathbb{R}^n$ is open and bounded. Also, The proof is very short once one has Hopf's lemma. Here is my question : Would anyone elaborate the underlined sentence that why such $y$ exists? (I don't have any intuition at all why this should be true.)","Here is the strong maximum principle in Evans's Partial Differential Equations : Here $U\subset\mathbb{R}^n$ is open and bounded. Also, The proof is very short once one has Hopf's lemma. Here is my question : Would anyone elaborate the underlined sentence that why such $y$ exists? (I don't have any intuition at all why this should be true.)",,"['real-analysis', 'multivariable-calculus']"
51,"Can locally ""a.e. constant"" function on a connected subset $U$ of $\mathbb{R}^n$ be constant a.e. in $U$?","Can locally ""a.e. constant"" function on a connected subset  of  be constant a.e. in ?",U \mathbb{R}^n U,"Consider a non-empty connected open subset $U$ of $\mathbb{R}^n$. Suppose a measurable function $u:U\to\mathbb{R}$ is locally constant on $U$, then it must be constant on $U$ according to this question . Here is my question: What if one changes ""locally constant"" to ""locally a.e. constant""? More precisely, assume that for every $x\in U$ there is an open neighborhood $V$ of $x$ in $U$ such that $u$ is constant a.e. in $V$. Can one conclude that $u$ is constant on $U$ a.e.? [Motivation] This question is mostly for a rigorous last step in the proof of this problem.","Consider a non-empty connected open subset $U$ of $\mathbb{R}^n$. Suppose a measurable function $u:U\to\mathbb{R}$ is locally constant on $U$, then it must be constant on $U$ according to this question . Here is my question: What if one changes ""locally constant"" to ""locally a.e. constant""? More precisely, assume that for every $x\in U$ there is an open neighborhood $V$ of $x$ in $U$ such that $u$ is constant a.e. in $V$. Can one conclude that $u$ is constant on $U$ a.e.? [Motivation] This question is mostly for a rigorous last step in the proof of this problem.",,['real-analysis']
52,Spivak's Chain Rule Proof (Image of proof provided),Spivak's Chain Rule Proof (Image of proof provided),,"If $g$ is differentiable at $a$ , and $f$ is differentiable at $g(a)$ , then $f \circ g$ is differentiable at $a$ , and $$ (f \circ g)^{\prime}(a)=f^{\prime}(g(a)) \cdot g^{\prime}(a). $$ Define a function $\phi$ as follows: $$ \phi(h)= \begin{cases}\frac{f(g(a+h))-f(g(a))}{g(a+h)-g(a)}, & \text { if } g(a+h)-g(a) \neq 0 \\ f^{\prime}(g(a)), & \text { if } g(a+h)-g(a)=0 .\end{cases} $$ It should be intuitively clear that $\phi$ is continuous at $0:$ When $h$ is small, $g(a+h)-g(a)$ is also small, so if $g(a+h)-g(a)$ is not zero, then $\phi(h)$ will be close to $f^{\prime}(g(a)) ;$ and if it is zero, then $\phi(h)$ actually equals $f^{\prime}(g(a))$ , which is even better. Since the continuity of $\phi$ is the crux of the whole proof we will provide a careful translation of this intuitive argument. We know that $f$ is differentiable at $g(a) .$ This means that $$ \lim _{k \rightarrow 0} \frac{f(g(a)+k)-f(g(a))}{k}=f^{\prime}(g(a)). $$ Thus, if $\varepsilon>0$ there is some number $\delta^{\prime}>0$ such that, for all $k$ , $$ \text{if $0<|k|<\delta^{\prime}$, then $\left|\frac{f(g(a)+k)-f(g(a))}{k}-f^{\prime}(g(a))\right|<\varepsilon$}. \tag{1} $$ Now $g$ is differentiable at $a$ , hence continuous at $a$ , so there is a $\delta>0$ such that, for all $h$ , $$\text{ if $|h|<\delta$, then $|g(a+h)-g(a)|<\delta^{\prime} .$}\tag{2}$$ Consider now any $h$ with $|h|<\delta .$ If $k=g(a+h)-g(a) \neq 0$ , then $$ \phi(h)=\frac{f(g(a+h))-f(g(a))}{g(a+h)-g(a)}=\frac{f(g(a)+k)-f(g(a))}{k} ; $$ it follows from $(2)$ that $|k|<\delta^{\prime}$ , and hence from (1) that $$ \left|\phi(h)-f^{\prime}(g(a))\right|<\varepsilon. $$ (transcribed from this screenshot) Here is a proof of the chain rule in Spivak's Calculus. Note there is a second page, but I understand it, and this is the meat of the proof. I have a few questions. $\textbf{1.}$ ""It should be intuitively clear that $\phi$ is continuous at $0$ ."" Do we care that it is continuous at zero so we will not have a division by zero since $g(a+h)-g(a)$ is in the denominator and could equal zero? I am not sure I understand why it is continuous at zero. I understand what he was saying but I was always under the impression continuity was when there were no breaks in the graph visually. Here, I am imagining $\phi(h)$ being continuous up to zero, then it jumping to another point when it is zero. $\textbf{2.}$ At (2),I do not understand what we are trying to do here. We randomly switched to $h$ and are defining continuity I think. The switch back and forth from $k$ to $h$ is confusing me.","If is differentiable at , and is differentiable at , then is differentiable at , and Define a function as follows: It should be intuitively clear that is continuous at When is small, is also small, so if is not zero, then will be close to and if it is zero, then actually equals , which is even better. Since the continuity of is the crux of the whole proof we will provide a careful translation of this intuitive argument. We know that is differentiable at This means that Thus, if there is some number such that, for all , Now is differentiable at , hence continuous at , so there is a such that, for all , Consider now any with If , then it follows from that , and hence from (1) that (transcribed from this screenshot) Here is a proof of the chain rule in Spivak's Calculus. Note there is a second page, but I understand it, and this is the meat of the proof. I have a few questions. ""It should be intuitively clear that is continuous at ."" Do we care that it is continuous at zero so we will not have a division by zero since is in the denominator and could equal zero? I am not sure I understand why it is continuous at zero. I understand what he was saying but I was always under the impression continuity was when there were no breaks in the graph visually. Here, I am imagining being continuous up to zero, then it jumping to another point when it is zero. At (2),I do not understand what we are trying to do here. We randomly switched to and are defining continuity I think. The switch back and forth from to is confusing me.","g a f g(a) f \circ g a 
(f \circ g)^{\prime}(a)=f^{\prime}(g(a)) \cdot g^{\prime}(a).
 \phi 
\phi(h)= \begin{cases}\frac{f(g(a+h))-f(g(a))}{g(a+h)-g(a)}, & \text { if } g(a+h)-g(a) \neq 0 \\ f^{\prime}(g(a)), & \text { if } g(a+h)-g(a)=0 .\end{cases}
 \phi 0: h g(a+h)-g(a) g(a+h)-g(a) \phi(h) f^{\prime}(g(a)) ; \phi(h) f^{\prime}(g(a)) \phi f g(a) . 
\lim _{k \rightarrow 0} \frac{f(g(a)+k)-f(g(a))}{k}=f^{\prime}(g(a)).
 \varepsilon>0 \delta^{\prime}>0 k  \text{if 0<|k|<\delta^{\prime}, then \left|\frac{f(g(a)+k)-f(g(a))}{k}-f^{\prime}(g(a))\right|<\varepsilon}. \tag{1}  g a a \delta>0 h \text{ if |h|<\delta, then |g(a+h)-g(a)|<\delta^{\prime} .}\tag{2} h |h|<\delta . k=g(a+h)-g(a) \neq 0 
\phi(h)=\frac{f(g(a+h))-f(g(a))}{g(a+h)-g(a)}=\frac{f(g(a)+k)-f(g(a))}{k} ;
 (2) |k|<\delta^{\prime} 
\left|\phi(h)-f^{\prime}(g(a))\right|<\varepsilon.
 \textbf{1.} \phi 0 g(a+h)-g(a) \phi(h) \textbf{2.} h k h","['real-analysis', 'calculus', 'derivatives', 'proof-explanation', 'chain-rule']"
53,Functions that preserve asymptotic equivalence,Functions that preserve asymptotic equivalence,,"Is there any notion of preserving asymptotic equivalence by a real-valued function? Any facts known about such functions? To clarify what I'm asking I'll introduce one formalization of the idea which has come to my mind. $\mathbb R_+$ denotes $[0,+\infty)$. Consider a class $\mathcal{A}$ of continuous functions $f\,\colon \mathbb R_+ \rightarrow \mathbb R_+$, satisfying following conditions: $\lim \limits_{x \rightarrow +\infty} f(x) = +\infty$, for any two positive sequences $\{a_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$, such that $$a_n \rightarrow +\infty \;\;\;\text{and}\;\;\; a_n \sim b_n \quad\text{as}\;\;\; n\rightarrow \infty,$$ we have $f(a_n) \sim f(b_n)$, as $n$ goes to infinity. Some facts about $\mathcal A$: $\mathcal A$ contains such functions, as $\log(1+x)$ and $x^\alpha$ for $\alpha>0$. For continuous $f,g\,\colon \mathbb R_+ \rightarrow \mathbb R_+$, if $f \in \mathcal A$ and $f \sim g$ then we have $g \in \mathcal A$. In particular, in our class lie the functions $f$ analityc at infinity, which means $\frac{1}{f(1/x)}$ to be analytic at zero. Thanks Joel Cohen who helped me note it. One can show that $\mathcal A$ is closed under composition, multiplication and taking linear combinations with positive coefficients. Moreover, this operations are well-defined on set $A$ of $\sim$-equivalency classes of $f \in \mathcal A$. Then, we can state some problems: Is $\mathcal A$ bounded in meaning of asymptotic growth? Can we characterize $f \in \mathcal A$ in some other terms? Is $\mathcal A$ closed under taking indefinite integral? If so, is this operation well-defined on $A$? If we modify the definition by replacing equivalent sequences with such $C^k$-smooth functions on $\mathbb R_+$, would we get the same class? What if we restrict this sequences/functions to be strictly increasing?","Is there any notion of preserving asymptotic equivalence by a real-valued function? Any facts known about such functions? To clarify what I'm asking I'll introduce one formalization of the idea which has come to my mind. $\mathbb R_+$ denotes $[0,+\infty)$. Consider a class $\mathcal{A}$ of continuous functions $f\,\colon \mathbb R_+ \rightarrow \mathbb R_+$, satisfying following conditions: $\lim \limits_{x \rightarrow +\infty} f(x) = +\infty$, for any two positive sequences $\{a_n\}_{n=1}^\infty$ and $\{b_n\}_{n=1}^\infty$, such that $$a_n \rightarrow +\infty \;\;\;\text{and}\;\;\; a_n \sim b_n \quad\text{as}\;\;\; n\rightarrow \infty,$$ we have $f(a_n) \sim f(b_n)$, as $n$ goes to infinity. Some facts about $\mathcal A$: $\mathcal A$ contains such functions, as $\log(1+x)$ and $x^\alpha$ for $\alpha>0$. For continuous $f,g\,\colon \mathbb R_+ \rightarrow \mathbb R_+$, if $f \in \mathcal A$ and $f \sim g$ then we have $g \in \mathcal A$. In particular, in our class lie the functions $f$ analityc at infinity, which means $\frac{1}{f(1/x)}$ to be analytic at zero. Thanks Joel Cohen who helped me note it. One can show that $\mathcal A$ is closed under composition, multiplication and taking linear combinations with positive coefficients. Moreover, this operations are well-defined on set $A$ of $\sim$-equivalency classes of $f \in \mathcal A$. Then, we can state some problems: Is $\mathcal A$ bounded in meaning of asymptotic growth? Can we characterize $f \in \mathcal A$ in some other terms? Is $\mathcal A$ closed under taking indefinite integral? If so, is this operation well-defined on $A$? If we modify the definition by replacing equivalent sequences with such $C^k$-smooth functions on $\mathbb R_+$, would we get the same class? What if we restrict this sequences/functions to be strictly increasing?",,"['calculus', 'real-analysis', 'reference-request', 'asymptotics']"
54,"Properties of function $f(x) = (1 + x^2)^{-\alpha/2}(\log(2+x^2))^{-1},\text{ }x \in \mathbb{R}$ with $0 < \alpha < 1$.",Properties of function  with .,"f(x) = (1 + x^2)^{-\alpha/2}(\log(2+x^2))^{-1},\text{ }x \in \mathbb{R} 0 < \alpha < 1","Consider the function$$f(x) = (1 + x^2)^{-\alpha/2}(\log(2+x^2))^{-1},\text{ }x \in \mathbb{R},$$with $0 < \alpha < 1$. How do I see that $f \in W^{1, p}(\mathbb{R})$ for all $p \in [1/\alpha, \infty]$ and that $f \notin L^q(\mathbb{R})$ for all $q \in [1, 1/\alpha)$?","Consider the function$$f(x) = (1 + x^2)^{-\alpha/2}(\log(2+x^2))^{-1},\text{ }x \in \mathbb{R},$$with $0 < \alpha < 1$. How do I see that $f \in W^{1, p}(\mathbb{R})$ for all $p \in [1/\alpha, \infty]$ and that $f \notin L^q(\mathbb{R})$ for all $q \in [1, 1/\alpha)$?",,"['real-analysis', 'functional-analysis']"
55,Show that the square root of a non-negative operator is unique,Show that the square root of a non-negative operator is unique,,"Let $H$ be a Hilbert space, and $A\in B(H\to H)$ be a bounded non-negative operator (i.e. $\langle Ax,x\rangle \geq 0$ for all $x\in H$). The square root of $A$ is a  bounded non-negative operator $B\geq $ such that $B^2=A$. First, We can assume without loss of generality that $0\leq A\leq I$. Note that $B^2=A$ if and only if  $$I-B=\frac{1}{2}((I-A)+(I-B)^2).$$ Hence, we definite inductively a sequence $C_n$ of operators as follows: $C_0:=0$, and $C_{n+1}:=1/2((I-A)+C_n^2))$. Then it is easy to see that $C_n$ converges to a bounded non-negetive operator $B$ in the strong operator topology and we also have $B^2=A$, thus the square root exists, but I don't know how to show that it is unique.","Let $H$ be a Hilbert space, and $A\in B(H\to H)$ be a bounded non-negative operator (i.e. $\langle Ax,x\rangle \geq 0$ for all $x\in H$). The square root of $A$ is a  bounded non-negative operator $B\geq $ such that $B^2=A$. First, We can assume without loss of generality that $0\leq A\leq I$. Note that $B^2=A$ if and only if  $$I-B=\frac{1}{2}((I-A)+(I-B)^2).$$ Hence, we definite inductively a sequence $C_n$ of operators as follows: $C_0:=0$, and $C_{n+1}:=1/2((I-A)+C_n^2))$. Then it is easy to see that $C_n$ converges to a bounded non-negetive operator $B$ in the strong operator topology and we also have $B^2=A$, thus the square root exists, but I don't know how to show that it is unique.",,['real-analysis']
56,Monotone increasing function can be expressed as sum of absolutely continuous function and singular function,Monotone increasing function can be expressed as sum of absolutely continuous function and singular function,,"I'm working on a problem from Royden's Real Analysis: Show that if a function $f$ is monotone increasing on $[a,b]$ , then $f$ can be represented as the sum of an absolutely continuous function and a singular function. I understand the general idea of the proof (I think), but there are a few details I'm unclear on. Here's my proof so far: Let $f$ be monotone increasing on $[a,b]$ . Let $g(x) = \int_a^x f'(t) dt + g(a)$ . Since $g$ is an indefinite integral, $g$ is absolutely continuous. Let $h(x) = f(x) - g(x)$ . Then $h'(x) = f'(x) - g'(x)$ . Since $f$ is monotone increasing, by Theorem 5.3 $\ f'$ is measurable. Then, by Lemma 5.9 $g'(x) = f'(x)$ almost everywhere, so $h'(x) = 0$ almost everywhere, and $h$ is thus singular. $\textbf{Theorem 5.3:}$ If $f$ is monotone increasing on $[a,b]$ ,     then $f$ is differentiable a.e. and $f'$ is measurable. $\textbf{Theorem 5.9:}$ If $f$ is bounded and measurable on $[a,b]$ ,      and $F(x) = \int_a^x f(t) dt + F(a),$ then $F'(x) = f(x)$ a.e. Here are my questions: What allows me to say $h= f - g \implies h' = f' - g'$ ? I would think it's just the differentiability of $f, g,$ and $h$ . I know $f$ is differentiable because it's monotone increasing, and $g$ is differentiable because it's absolutely continuous and thus of bounded variation. Is this correct? Is f' bounded? In order to apply Lemma 5.9, f' must be both bounded and measurable. I could also use Thm 5.10, which requires $f'$ to be integrable, but I'm not sure if I have integrability, either. $\textbf{Theorem 5.10:}$ If $f$ is integrable on $[a,b]$ ,   and $F(x) = \int_a^x f(t) dt + F(a),$ then $F'(x) = f(x)$ a.e. Is it necessary to let $g(x) = \int_a^x f'(t) dt + g(a)$ , or can I let $g(x) = \int_a^x f'(t) dt$ ? I would think I need the former in order to use 5.9 or 5.10, but I saw a proof that used the latter. Is there any difference in the two approaches?","I'm working on a problem from Royden's Real Analysis: Show that if a function is monotone increasing on , then can be represented as the sum of an absolutely continuous function and a singular function. I understand the general idea of the proof (I think), but there are a few details I'm unclear on. Here's my proof so far: Let be monotone increasing on . Let . Since is an indefinite integral, is absolutely continuous. Let . Then . Since is monotone increasing, by Theorem 5.3 is measurable. Then, by Lemma 5.9 almost everywhere, so almost everywhere, and is thus singular. If is monotone increasing on ,     then is differentiable a.e. and is measurable. If is bounded and measurable on ,      and then a.e. Here are my questions: What allows me to say ? I would think it's just the differentiability of and . I know is differentiable because it's monotone increasing, and is differentiable because it's absolutely continuous and thus of bounded variation. Is this correct? Is f' bounded? In order to apply Lemma 5.9, f' must be both bounded and measurable. I could also use Thm 5.10, which requires to be integrable, but I'm not sure if I have integrability, either. If is integrable on ,   and then a.e. Is it necessary to let , or can I let ? I would think I need the former in order to use 5.9 or 5.10, but I saw a proof that used the latter. Is there any difference in the two approaches?","f [a,b] f f [a,b] g(x) = \int_a^x f'(t) dt + g(a) g g h(x) = f(x) - g(x) h'(x) = f'(x) - g'(x) f \ f' g'(x) = f'(x) h'(x) = 0 h \textbf{Theorem 5.3:} f [a,b] f f' \textbf{Theorem 5.9:} f [a,b] F(x) = \int_a^x f(t) dt + F(a), F'(x) = f(x) h= f - g \implies h' = f' - g' f, g, h f g f' \textbf{Theorem 5.10:} f [a,b] F(x) = \int_a^x f(t) dt + F(a), F'(x) = f(x) g(x) = \int_a^x f'(t) dt + g(a) g(x) = \int_a^x f'(t) dt",['real-analysis']
57,Lusin's Theorem - does $f$ ever have to be continuous?,Lusin's Theorem - does  ever have to be continuous?,f,"Recall Lusin's Theorem: Let $f$ be a real-valued measurable function on $E=[0,1]$.  Then, for each $\epsilon > 0$, there is a continuous function $g$ on $\mathbb{R}$ and a closed set $F$ contained in $E$ for which $f=g$ on $F$ and $m(E\backslash F) < \epsilon$. I was wondering if $f$ itself ever has to be continuous at any point (as a function on $E$)? I figured that it would at least need to be continuous on the interior of $F$, where it actually equals $g$ (I realize it technically equals $g$ on the boundary of $F$ as well, but we don't know what's going on outside $F$, so we can't necessarily say it's continuous on the boundary, can we?), because there it's coinciding with a function that is known to be continuous. Is there any more to this, and if so (and even if not), what would a proof look like?","Recall Lusin's Theorem: Let $f$ be a real-valued measurable function on $E=[0,1]$.  Then, for each $\epsilon > 0$, there is a continuous function $g$ on $\mathbb{R}$ and a closed set $F$ contained in $E$ for which $f=g$ on $F$ and $m(E\backslash F) < \epsilon$. I was wondering if $f$ itself ever has to be continuous at any point (as a function on $E$)? I figured that it would at least need to be continuous on the interior of $F$, where it actually equals $g$ (I realize it technically equals $g$ on the boundary of $F$ as well, but we don't know what's going on outside $F$, so we can't necessarily say it's continuous on the boundary, can we?), because there it's coinciding with a function that is known to be continuous. Is there any more to this, and if so (and even if not), what would a proof look like?",,"['real-analysis', 'measure-theory']"
58,A uniformly continuous function whose integral $\int_0^\infty f(x)dx$ exists converges to zero,A uniformly continuous function whose integral  exists converges to zero,\int_0^\infty f(x)dx,"Let $f$ be uniformly continuous on $[0,\infty)$ and assume that $\int_0^\infty f(x)dx$ exists. Prove that    $$\lim_{x\rightarrow\infty}f(x)=0.$$ This seems obvious, but I couldn't prove it. Edit Thanks to the hint by Did, I managed to prove the theorem (See the answer below).","Let $f$ be uniformly continuous on $[0,\infty)$ and assume that $\int_0^\infty f(x)dx$ exists. Prove that    $$\lim_{x\rightarrow\infty}f(x)=0.$$ This seems obvious, but I couldn't prove it. Edit Thanks to the hint by Did, I managed to prove the theorem (See the answer below).",,"['real-analysis', 'integration', 'uniform-continuity']"
59,When can a set have an upper bound but no least upper bound?,When can a set have an upper bound but no least upper bound?,,"So I'm taking real analysis and have noted that one of the benefits of the Dedekind cut is that 'if one of the sets made has an upper bound it also has a least upper bound'. I don't understand how a set can have an upper bound and no least upper bound, though. Is what can lead to this declaring a set in the rationals that is bounded above by an irrational number? I don't see any other way for this to be true (and I don't know why you'd ever make that set, or similarly why it's a 'special' property of the Dedekind cut rather than the general case). Thank you for your time,","So I'm taking real analysis and have noted that one of the benefits of the Dedekind cut is that 'if one of the sets made has an upper bound it also has a least upper bound'. I don't understand how a set can have an upper bound and no least upper bound, though. Is what can lead to this declaring a set in the rationals that is bounded above by an irrational number? I don't see any other way for this to be true (and I don't know why you'd ever make that set, or similarly why it's a 'special' property of the Dedekind cut rather than the general case). Thank you for your time,",,"['real-analysis', 'analysis', 'real-numbers', 'rational-numbers']"
60,Taking a tricky limit $\lim_{p\to\infty}\int_{\Bbb R^N}\left(\frac{\left\lvert\nabla u\right\rvert}{\left\|\nabla u\right\|_p}\right)^{p-2}\cdots$,Taking a tricky limit,\lim_{p\to\infty}\int_{\Bbb R^N}\left(\frac{\left\lvert\nabla u\right\rvert}{\left\|\nabla u\right\|_p}\right)^{p-2}\cdots,"$$ \lim_{p\rightarrow \infty} \int_{\mathbb R^N}  \left(  \frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p} \right)^{p-2}  \frac{\nabla u}{\left\| \nabla u \right\|_p}\cdot \nabla v \,dx \label{\star}\tag{$\star$}$$ where $\,\left\lvert\,\cdot\,\right\rvert\,$ is the Euclidean $2$-norm and $\;\left\| \nabla u \right\|_p = \left\|\, \left\lvert \nabla u \right\rvert \, \right\|_p$. Could anyone give me some hint for finding an explicit form for the expression involving $u$? I feel like my approach is not quite correct... First I know the term in the limit is bounded $$ \begin{aligned} \eqref{\star} & \leq \int_{\mathbb R^N} \left( \frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}\right)^{p-1} \left\lvert \nabla v \right\rvert \,dx  \\ & = \left\| \nabla u \right\|_p^{1-p} \int_{\mathbb R^N} \left\lvert \nabla u \right\rvert^{p-1} \left\lvert \nabla v \right\rvert \,dx   \\ &\leq \left\| \nabla u \right\|_p^{1-p} \Bigg(\int_{\mathbb R^N} \Big(\left\lvert \nabla u \right\rvert^{p-1}\Big)^{\frac{p}{p-1}}\,dx\Bigg)^{\frac{p-1}{p}} \Bigg(\int_{\mathbb R^N} \left\lvert \nabla v \right\rvert ^ p \,dx\Bigg)^{\frac{1}{p}}  \\  & = \left\| \nabla u \right\|_p^{1-p}\left\| \nabla u \right\|_p^{p-1} \left\| \nabla v\right\|_p  \\ & = \left\| \nabla v\right\|_p. \end{aligned} $$ with the additional assumption that $ \nabla v \in L^\infty\!\left(\mathbb R^N, \mathbb R^N\right)$. Define the set  $$ D:= \big\{\left\lvert \nabla u \right\rvert \leq \|\nabla u \|_\infty -\delta\big\},$$ the limit $\eqref{\star}$ is zero on the set $D$. To see this, we know there exists $N\in \mathbb N$ such that  $$\left|\, \left\|\nabla u \right\|_\infty - \left\|\nabla u \right\|_p \,\right| \leq \frac{\delta}{2} \quad \forall \; p\geq N.$$ On the set $D$ we have  $$ \left\|\nabla u \right\|_p - \left\lvert \nabla u \right\rvert  \geq \frac{\delta}{2} \quad \forall \; p\geq N$$ which means  $$\frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}  \leq 1 - \frac{\delta}{2} \quad \forall \; p\geq N$$ and the term in the limit $\left(\dfrac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}\right)^{p-2}$ goes to $0$ uniformly. Next I looked at the set $K: = \big\{\left\lvert \nabla u \right\rvert = \left\|\nabla u \right\|_\infty\big\}$, the term  $$ \left(\frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}\right)^{p-2} = \left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)^{p-2} $$ is in the form $``1^\infty""$, so I tried to use L'Hopital's rule. And we can calculate the limit (assuming the integrals are defined and finite, I just want to see what the limit might look like).  $$ \begin{aligned}  \lim_{p\rightarrow \infty} \left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)^{p-2}  &=  \lim_{p\rightarrow \infty} \exp\left( \left(p-2\right)\log\left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)\right) \\ &=\lim_{p\rightarrow \infty} \exp\left( \frac{\log\left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)}{\frac{1}{p-2}}\right) \\ &=\lim_{p\rightarrow \infty} \exp\left( \frac{\frac{d}{dp} \left[-\log\left(\frac{\left\| \nabla u \right\|_p}{\left\| \nabla u \right\|_\infty}\right)\right]}{\frac{-1}{(p-2)^2}}\right) \\ &=\lim_{p\rightarrow \infty} \exp  \left(    \dfrac{     \left(         \dfrac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u  \right\|_p}     \right)     \dfrac{       \frac{d}{dp}        \big( \left\| \nabla u \right\|_p \big)       }{ \left\|\nabla u \right\|_\infty} }   { \dfrac{1}{\left(p-2\right)^2 }  } \right) \end{aligned} $$ where $$  \begin{aligned} \frac{d}{dp}\Big[\left\| \nabla u \right\|_p \Big]   &= \frac{d}{dp}\left[\left(\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx \right)^{1/p} \right]  \\ &=\frac{d}{dp}\left[\exp\left(\frac{1}{p} \log\left(\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx \right)\right)\right]  \\ &=\|\nabla u \|_p \left\{\frac{-1}{p^2}\log\left(\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx \right)  + \dfrac{1}{p} \dfrac{1}{\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx } \int_{\mathbb R^N} \left\lvert \nabla u \right\rvert^p \log\big(\left\lvert\nabla u \right\rvert\big) \,dx  \right\} \end{aligned} $$ And I am a little bit stuck. Thank you very much!","$$ \lim_{p\rightarrow \infty} \int_{\mathbb R^N}  \left(  \frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p} \right)^{p-2}  \frac{\nabla u}{\left\| \nabla u \right\|_p}\cdot \nabla v \,dx \label{\star}\tag{$\star$}$$ where $\,\left\lvert\,\cdot\,\right\rvert\,$ is the Euclidean $2$-norm and $\;\left\| \nabla u \right\|_p = \left\|\, \left\lvert \nabla u \right\rvert \, \right\|_p$. Could anyone give me some hint for finding an explicit form for the expression involving $u$? I feel like my approach is not quite correct... First I know the term in the limit is bounded $$ \begin{aligned} \eqref{\star} & \leq \int_{\mathbb R^N} \left( \frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}\right)^{p-1} \left\lvert \nabla v \right\rvert \,dx  \\ & = \left\| \nabla u \right\|_p^{1-p} \int_{\mathbb R^N} \left\lvert \nabla u \right\rvert^{p-1} \left\lvert \nabla v \right\rvert \,dx   \\ &\leq \left\| \nabla u \right\|_p^{1-p} \Bigg(\int_{\mathbb R^N} \Big(\left\lvert \nabla u \right\rvert^{p-1}\Big)^{\frac{p}{p-1}}\,dx\Bigg)^{\frac{p-1}{p}} \Bigg(\int_{\mathbb R^N} \left\lvert \nabla v \right\rvert ^ p \,dx\Bigg)^{\frac{1}{p}}  \\  & = \left\| \nabla u \right\|_p^{1-p}\left\| \nabla u \right\|_p^{p-1} \left\| \nabla v\right\|_p  \\ & = \left\| \nabla v\right\|_p. \end{aligned} $$ with the additional assumption that $ \nabla v \in L^\infty\!\left(\mathbb R^N, \mathbb R^N\right)$. Define the set  $$ D:= \big\{\left\lvert \nabla u \right\rvert \leq \|\nabla u \|_\infty -\delta\big\},$$ the limit $\eqref{\star}$ is zero on the set $D$. To see this, we know there exists $N\in \mathbb N$ such that  $$\left|\, \left\|\nabla u \right\|_\infty - \left\|\nabla u \right\|_p \,\right| \leq \frac{\delta}{2} \quad \forall \; p\geq N.$$ On the set $D$ we have  $$ \left\|\nabla u \right\|_p - \left\lvert \nabla u \right\rvert  \geq \frac{\delta}{2} \quad \forall \; p\geq N$$ which means  $$\frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}  \leq 1 - \frac{\delta}{2} \quad \forall \; p\geq N$$ and the term in the limit $\left(\dfrac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}\right)^{p-2}$ goes to $0$ uniformly. Next I looked at the set $K: = \big\{\left\lvert \nabla u \right\rvert = \left\|\nabla u \right\|_\infty\big\}$, the term  $$ \left(\frac{\left\lvert \nabla u \right\rvert}{\left\| \nabla u \right\|_p}\right)^{p-2} = \left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)^{p-2} $$ is in the form $``1^\infty""$, so I tried to use L'Hopital's rule. And we can calculate the limit (assuming the integrals are defined and finite, I just want to see what the limit might look like).  $$ \begin{aligned}  \lim_{p\rightarrow \infty} \left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)^{p-2}  &=  \lim_{p\rightarrow \infty} \exp\left( \left(p-2\right)\log\left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)\right) \\ &=\lim_{p\rightarrow \infty} \exp\left( \frac{\log\left(\frac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u \right\|_p}\right)}{\frac{1}{p-2}}\right) \\ &=\lim_{p\rightarrow \infty} \exp\left( \frac{\frac{d}{dp} \left[-\log\left(\frac{\left\| \nabla u \right\|_p}{\left\| \nabla u \right\|_\infty}\right)\right]}{\frac{-1}{(p-2)^2}}\right) \\ &=\lim_{p\rightarrow \infty} \exp  \left(    \dfrac{     \left(         \dfrac{\left\| \nabla u \right\|_\infty}{\left\| \nabla u  \right\|_p}     \right)     \dfrac{       \frac{d}{dp}        \big( \left\| \nabla u \right\|_p \big)       }{ \left\|\nabla u \right\|_\infty} }   { \dfrac{1}{\left(p-2\right)^2 }  } \right) \end{aligned} $$ where $$  \begin{aligned} \frac{d}{dp}\Big[\left\| \nabla u \right\|_p \Big]   &= \frac{d}{dp}\left[\left(\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx \right)^{1/p} \right]  \\ &=\frac{d}{dp}\left[\exp\left(\frac{1}{p} \log\left(\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx \right)\right)\right]  \\ &=\|\nabla u \|_p \left\{\frac{-1}{p^2}\log\left(\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx \right)  + \dfrac{1}{p} \dfrac{1}{\int_{\mathbb R^N} \left\lvert\nabla u \right\rvert^p \,dx } \int_{\mathbb R^N} \left\lvert \nabla u \right\rvert^p \log\big(\left\lvert\nabla u \right\rvert\big) \,dx  \right\} \end{aligned} $$ And I am a little bit stuck. Thank you very much!",,"['real-analysis', 'limits', 'partial-differential-equations', 'normed-spaces']"
61,Theorem 3.1 from Milnor's Morse Theory,Theorem 3.1 from Milnor's Morse Theory,,"Milnor is in the business of proving that if $f: M \to \mathbb{R}$ is a smooth function, $a < b$, and $f^{-1} ([a,b])$ is a compact subset of $M$ containing no critical points, then $M^a$ is diffeomorphic to $M^b$, where $M^x = f^{-1} (-\infty, x]$. In the proof, he starts by equipping $M$ with a Riemannian metric $g$ and then considering a smooth vector field $X$ such that $X = g(\nabla f, \nabla f)^{-1} \nabla f$ on $f^{-1}([a,b])$ and $X$ is compactly supported.  He then generates a maximal flow $\theta :\mathbb{R} \times M \to M$ for $X$.  He then notes that if $\theta_q(t) \in f^{-1}([a,b])$, then $\frac{d (f \circ \theta_q(t)}{dt}$ = 1, and hence the diffeomorphism $\theta_{b-a}$ takes $M^a$ diffeomorphically onto $M^b$. I do not understand how this follows.  I understand essentially that what I'm supposed to see is that if $f(q) = a$ then I can write $f \circ \theta_q(t) = t + a$ and then taking $t = b - a$. I do not see why I'm justified in getting the above expression for $f \circ \theta_q(t)$, despite knowing that $(f \circ \theta_q)' (0) = 1$ (wouldn't I need to know that the derivative is 1 on some neighborhood of 0 to deduce that $f \circ \theta_q$ is locally given by the linear function?  Maybe $\theta_q$ intersects $f^{-1}([a,b])$ only at $t = 0$).","Milnor is in the business of proving that if $f: M \to \mathbb{R}$ is a smooth function, $a < b$, and $f^{-1} ([a,b])$ is a compact subset of $M$ containing no critical points, then $M^a$ is diffeomorphic to $M^b$, where $M^x = f^{-1} (-\infty, x]$. In the proof, he starts by equipping $M$ with a Riemannian metric $g$ and then considering a smooth vector field $X$ such that $X = g(\nabla f, \nabla f)^{-1} \nabla f$ on $f^{-1}([a,b])$ and $X$ is compactly supported.  He then generates a maximal flow $\theta :\mathbb{R} \times M \to M$ for $X$.  He then notes that if $\theta_q(t) \in f^{-1}([a,b])$, then $\frac{d (f \circ \theta_q(t)}{dt}$ = 1, and hence the diffeomorphism $\theta_{b-a}$ takes $M^a$ diffeomorphically onto $M^b$. I do not understand how this follows.  I understand essentially that what I'm supposed to see is that if $f(q) = a$ then I can write $f \circ \theta_q(t) = t + a$ and then taking $t = b - a$. I do not see why I'm justified in getting the above expression for $f \circ \theta_q(t)$, despite knowing that $(f \circ \theta_q)' (0) = 1$ (wouldn't I need to know that the derivative is 1 on some neighborhood of 0 to deduce that $f \circ \theta_q$ is locally given by the linear function?  Maybe $\theta_q$ intersects $f^{-1}([a,b])$ only at $t = 0$).",,"['real-analysis', 'differential-topology']"
62,Is the limit a function?,Is the limit a function?,,"We're aware of the existence of the limit in the context of calculus, e.g. where $f:\mathbb{R}\to\mathbb{R}$, we may have: $$\lim_{x\to z} f(x) = y$$ My question is whether it is valid to see the limit as a function. A limit takes two inputs: $f$, a function, and $z$, the value at which we take the limit of the function. These two inputs are a tuple $(f,z)$ where the first coordinate of the tuple is a function from some function space $F$[1] and the second coordinate is from $\mathbb{R}\cup[-\infty,+\infty]$, at least in standard one-variable calculus in the real numbers. So, instead of writing $\lim_{x\to z} f(x) = y$, I could write $\lim(f,z) = y$. Specifically, I want to claim that $\lim$ is a function such that $\lim : F \times ((\mathbb{R}\cup[-\infty,+\infty])\setminus N) \to \mathbb{R}\cup[-\infty,+\infty]$, where $N$ is the set of values in the extended reals for which we cannot take that particular limit, so we conveniently exclude them. The last part in particular may not read too well, but I hope that the point is clear: I'm asking if it is possible to call a limit a function that maps a (function, value) tuple to some image. [1] Matthew Leingang pointed out that different $f$ will have different $N$, which makes it hard to generally define an $F$ to suit an $N$. This is problematic for defining the domain of the limit, so I propose to define $F=\{f\}$, i.e. we define $F$ as containing only the particular function we're interested in taking the limit of.","We're aware of the existence of the limit in the context of calculus, e.g. where $f:\mathbb{R}\to\mathbb{R}$, we may have: $$\lim_{x\to z} f(x) = y$$ My question is whether it is valid to see the limit as a function. A limit takes two inputs: $f$, a function, and $z$, the value at which we take the limit of the function. These two inputs are a tuple $(f,z)$ where the first coordinate of the tuple is a function from some function space $F$[1] and the second coordinate is from $\mathbb{R}\cup[-\infty,+\infty]$, at least in standard one-variable calculus in the real numbers. So, instead of writing $\lim_{x\to z} f(x) = y$, I could write $\lim(f,z) = y$. Specifically, I want to claim that $\lim$ is a function such that $\lim : F \times ((\mathbb{R}\cup[-\infty,+\infty])\setminus N) \to \mathbb{R}\cup[-\infty,+\infty]$, where $N$ is the set of values in the extended reals for which we cannot take that particular limit, so we conveniently exclude them. The last part in particular may not read too well, but I hope that the point is clear: I'm asking if it is possible to call a limit a function that maps a (function, value) tuple to some image. [1] Matthew Leingang pointed out that different $f$ will have different $N$, which makes it hard to generally define an $F$ to suit an $N$. This is problematic for defining the domain of the limit, so I propose to define $F=\{f\}$, i.e. we define $F$ as containing only the particular function we're interested in taking the limit of.",,"['calculus', 'real-analysis', 'limits']"
63,There exists x on closed interval such that f(x)=x,There exists x on closed interval such that f(x)=x,,"If $f$ is a continuous function on a closed interval, how can I show that there exists some $x$ on $f$ that $f(x)=x$? I know it will require the Intermediate Value Theorem. Initially I thought of letting $g(x)=f(x)-x$, and showing that there exists $x$ such that $g(x)=0$ implying $f(x)=x$, but don't have any information about $f$ on either end of the interval.","If $f$ is a continuous function on a closed interval, how can I show that there exists some $x$ on $f$ that $f(x)=x$? I know it will require the Intermediate Value Theorem. Initially I thought of letting $g(x)=f(x)-x$, and showing that there exists $x$ such that $g(x)=0$ implying $f(x)=x$, but don't have any information about $f$ on either end of the interval.",,['real-analysis']
64,"If the set of values , for which a function has positive derivative , is dense then is the function increasing?","If the set of values , for which a function has positive derivative , is dense then is the function increasing?",,"Let $f:\mathbb R \to \mathbb R$ be a differentiable function such that $A:=${ $x \in \mathbb R :f'(x)>0$ } is dense  in $\mathbb R$ , then is it true that $f$ is an increasing function ? What  happens if $f'$ is also continuous  ?","Let $f:\mathbb R \to \mathbb R$ be a differentiable function such that $A:=${ $x \in \mathbb R :f'(x)>0$ } is dense  in $\mathbb R$ , then is it true that $f$ is an increasing function ? What  happens if $f'$ is also continuous  ?",,"['real-analysis', 'functions', 'derivatives']"
65,What information is contained in the function $p\mapsto ||f||_p$?,What information is contained in the function ?,p\mapsto ||f||_p,"Given a measurable function $f:\mathbb{R}\rightarrow\mathbb{R}$, we obtain a function $\nu_f:(0,\infty)\rightarrow [0,\infty]$ defined by $\nu_f(p):=||f||_p$ This function $\nu_f$ will not necessarily be finite almost everywhere, but it should still contain a lot of information about $f$. $\nu_f$ will be a continuous function wherever it is finite. Will $\nu_f$ satisfy any stronger types of continuity, or have other nice properties? When will it be differentiable, and what interpretation does the derivative $\nu_f'$ have at a fixed value of $p$? Is there a name for the function $\nu_f$ or the mapping $f\mapsto\nu_f$? What other information can we deduce about $f$? What can we say about two functions $f,g$ whose associated functions $\nu_f,\nu_g$ are equal? Sorry about all the questions, but I'm positively thirsting for information!","Given a measurable function $f:\mathbb{R}\rightarrow\mathbb{R}$, we obtain a function $\nu_f:(0,\infty)\rightarrow [0,\infty]$ defined by $\nu_f(p):=||f||_p$ This function $\nu_f$ will not necessarily be finite almost everywhere, but it should still contain a lot of information about $f$. $\nu_f$ will be a continuous function wherever it is finite. Will $\nu_f$ satisfy any stronger types of continuity, or have other nice properties? When will it be differentiable, and what interpretation does the derivative $\nu_f'$ have at a fixed value of $p$? Is there a name for the function $\nu_f$ or the mapping $f\mapsto\nu_f$? What other information can we deduce about $f$? What can we say about two functions $f,g$ whose associated functions $\nu_f,\nu_g$ are equal? Sorry about all the questions, but I'm positively thirsting for information!",,"['real-analysis', 'lp-spaces']"
66,Math Analysis Courses online,Math Analysis Courses online,,"Can somebody recommend me respectable Math Analysis courses online? I am a student and I took real analysis course in my university, but I am unsatisfied with the quality of that course. I am even considering switching university because I cannot get descent analysis courses there. But first I want to try take one on line. Any thoughts? Thank you in advance. Marina","Can somebody recommend me respectable Math Analysis courses online? I am a student and I took real analysis course in my university, but I am unsatisfied with the quality of that course. I am even considering switching university because I cannot get descent analysis courses there. But first I want to try take one on line. Any thoughts? Thank you in advance. Marina",,['real-analysis']
67,"How to fill up $(0,1)$ with disjoint closed intervals all total measure one",How to fill up  with disjoint closed intervals all total measure one,"(0,1)","This is a problem which was proposed, but not chosen, in a Mathematics competition for University students not long ago, and its solution is missing: Let $\sum_{n=1}^\infty a_n=1$, where $a_n>0$, for all $n\in\mathbb N$.   Is it possible to find pairwise disjoint closed intervals   $I_n=[c_n,d_n]\subset (0,1)$, with $d_n-c_n=a_n$? EDIT I. The answer is yes (see the Answer that follows). The interesting thing though is that $(0,1)\smallsetminus\bigcup_{n\in\mathbb N}I_n$ in  nonempty. In particular, can you show that $$D=[0,1]\smallsetminus\bigcup_{n\in\mathbb N}I_n$$ is a Cantor-like set? EDIT II. See a relevant article by Terry Tao .","This is a problem which was proposed, but not chosen, in a Mathematics competition for University students not long ago, and its solution is missing: Let $\sum_{n=1}^\infty a_n=1$, where $a_n>0$, for all $n\in\mathbb N$.   Is it possible to find pairwise disjoint closed intervals   $I_n=[c_n,d_n]\subset (0,1)$, with $d_n-c_n=a_n$? EDIT I. The answer is yes (see the Answer that follows). The interesting thing though is that $(0,1)\smallsetminus\bigcup_{n\in\mathbb N}I_n$ in  nonempty. In particular, can you show that $$D=[0,1]\smallsetminus\bigcup_{n\in\mathbb N}I_n$$ is a Cantor-like set? EDIT II. See a relevant article by Terry Tao .",,"['calculus', 'real-analysis', 'analysis', 'contest-math']"
68,absolute convergence of $\sum _{n=1}^{\infty} a_n$ imply?,absolute convergence of  imply?,\sum _{n=1}^{\infty} a_n,"Question is : If $\sum _{n=1}^{\infty} a_n$ is absolutely convergent  then which of the following is not true? $\sum_{m=n}^{\infty}a_m\rightarrow 0$ as $n\rightarrow \infty$ $\sum_{n=1}^{\infty}a_n\sin n$ is convergent. $\sum_{n=1}^{\infty}e^{a_n}$ is divergent. $\sum_{n=1}^{\infty}a_n^2$ is divergent. First thing I would like to concentrate on is third option (as it is easy :P).... absolutely convergence of $\sum _{n=1}^{\infty} a_n$ imply $a_n\rightarrow 0$ i.e., $e^{a_n}\rightarrow 1$ i.e.,$\sum_{n=1}^{\infty}e^{a_n}$ is divergent. I guess second option is most probably true.. It is for sure absolute convergence as $|a_n\sin n|\leq |a_n|$ for all $n$....  I could not give concrete argument for convergence. I guess fourth option is false... absolutely convergence of $\sum _{n=1}^{\infty} a_n$ imply $a_n\rightarrow 0$ i.e., after certain stage $|a_n|<1$ i.e., $|a_n^2|<|a_n|$ So, we would have convergence of $\sum_{n=1}^{\infty}a_n^2$. I do not understand what is actual point of first option... Could some one confirm if this justification for second/third/fourth options is sufficient and help me to understand what first option is... Thank you.","Question is : If $\sum _{n=1}^{\infty} a_n$ is absolutely convergent  then which of the following is not true? $\sum_{m=n}^{\infty}a_m\rightarrow 0$ as $n\rightarrow \infty$ $\sum_{n=1}^{\infty}a_n\sin n$ is convergent. $\sum_{n=1}^{\infty}e^{a_n}$ is divergent. $\sum_{n=1}^{\infty}a_n^2$ is divergent. First thing I would like to concentrate on is third option (as it is easy :P).... absolutely convergence of $\sum _{n=1}^{\infty} a_n$ imply $a_n\rightarrow 0$ i.e., $e^{a_n}\rightarrow 1$ i.e.,$\sum_{n=1}^{\infty}e^{a_n}$ is divergent. I guess second option is most probably true.. It is for sure absolute convergence as $|a_n\sin n|\leq |a_n|$ for all $n$....  I could not give concrete argument for convergence. I guess fourth option is false... absolutely convergence of $\sum _{n=1}^{\infty} a_n$ imply $a_n\rightarrow 0$ i.e., after certain stage $|a_n|<1$ i.e., $|a_n^2|<|a_n|$ So, we would have convergence of $\sum_{n=1}^{\infty}a_n^2$. I do not understand what is actual point of first option... Could some one confirm if this justification for second/third/fourth options is sufficient and help me to understand what first option is... Thank you.",,['real-analysis']
69,"Find all continuous functions $f$ : $\mathbb{R} \rightarrow \mathbb{R}$ satisfying: $f(xy)+f(x+y)=f(xy+x)+f(y)\quad\forall x,y \in \mathbb{R}$",Find all continuous functions  :  satisfying:,"f \mathbb{R} \rightarrow \mathbb{R} f(xy)+f(x+y)=f(xy+x)+f(y)\quad\forall x,y \in \mathbb{R}","Find all continuous functions $f$ : $\mathbb{R} \rightarrow \mathbb{R}$ satisfying: $$f(xy)+f(x+y)=f(xy+x)+f(y)$$ $\forall x,y \in \mathbb{R}$ I have tried that : $P(y;x)-P(x;y)$: $f(xy+y)+f(x)=f(xy+x)+f(y)$ $P(x;1)$: $f(x)+f(x+1)=f(2x)+f(1)$ $P(x+1;1)$:$f(x+1)+f(x+2)=f(2x+2)+f(1)$ So $f(x+2)-f(x)=f(2x+2)-f(2x) , \forall x \in \mathbb{R}$ let $g(x)=f(x+2)-f(x)$ We have $g(2x)=g(x)=g(\frac{x}{2})=...=g(0)=c$ because $g$ is the continous functions. To here, I have no idea to solve the problem.","Find all continuous functions $f$ : $\mathbb{R} \rightarrow \mathbb{R}$ satisfying: $$f(xy)+f(x+y)=f(xy+x)+f(y)$$ $\forall x,y \in \mathbb{R}$ I have tried that : $P(y;x)-P(x;y)$: $f(xy+y)+f(x)=f(xy+x)+f(y)$ $P(x;1)$: $f(x)+f(x+1)=f(2x)+f(1)$ $P(x+1;1)$:$f(x+1)+f(x+2)=f(2x+2)+f(1)$ So $f(x+2)-f(x)=f(2x+2)-f(2x) , \forall x \in \mathbb{R}$ let $g(x)=f(x+2)-f(x)$ We have $g(2x)=g(x)=g(\frac{x}{2})=...=g(0)=c$ because $g$ is the continous functions. To here, I have no idea to solve the problem.",,"['real-analysis', 'functional-equations']"
70,looking for proof that this uniformly bounded sequence of functions has no pointwise convergent subsequence,looking for proof that this uniformly bounded sequence of functions has no pointwise convergent subsequence,,"Math people: I couldn't find a similar question, so here goes: I would like to prove the fact (?) that the sequence of functions $(f_n) \subset C([0,1])$ defined by $f_n(x)=\sin(nx)$ does not have a subsequence that converges pointwise to any function. This is not homework.  Soon I am going to teach the Arzela-Ascoli theorem, and I want to show that without the equicontinuity assumption, it is difficult to conclude anything about convergence of a sequence of a functions, even pointwise convergence. I am almost certain that what I assert above is true, I am just having trouble proving it.  I am also almost certain that this has been done before.  It is obvious that the entire sequence $(f_n)$ doesn't converge pointwise, and that no subsequence of $(f_n)$ converges uniformly, but that is not my question.","Math people: I couldn't find a similar question, so here goes: I would like to prove the fact (?) that the sequence of functions $(f_n) \subset C([0,1])$ defined by $f_n(x)=\sin(nx)$ does not have a subsequence that converges pointwise to any function. This is not homework.  Soon I am going to teach the Arzela-Ascoli theorem, and I want to show that without the equicontinuity assumption, it is difficult to conclude anything about convergence of a sequence of a functions, even pointwise convergence. I am almost certain that what I assert above is true, I am just having trouble proving it.  I am also almost certain that this has been done before.  It is obvious that the entire sequence $(f_n)$ doesn't converge pointwise, and that no subsequence of $(f_n)$ converges uniformly, but that is not my question.",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
71,Real Analysis Proofs: Additive Functions,Real Analysis Proofs: Additive Functions,,"I'm new here and could really use some help please: Let $f$ be an additive function. So for all $x,y \in \mathbb{R}$, $f(x+y) = f(x)+f(y)$. Prove that if there are $M>0$ and $a>0$ such that if $x \in [-a,a]$, then $|f(x)|\leq M$, then $f$ has a limit at every $x\in \mathbb{R}$ and $\lim_{t\rightarrow x} f(t) = f(x)$. Prove that if $f$ has a limit at each $x\in \mathbb{R}$, then there are $M>0$ and $a>0$ such that if $x\in [-a,a]$, then $|f(x)| \leq M$. if necessary the proofs should involve the $\delta - \varepsilon$ definition of a limit. The problem had two previous portions to it that I already know how to do. However, you can reference them to do the posted portions of the problem. Here they are: (a) Show that for each positive integer $n$ and each real number $x$, $f(nx)=nf(x)$. (b) Suppose $f$ is such that there are $M>0$ and $a>0$ such that if $x\in [−a,a]$, then $|f(x)|\le M$. Choose $\varepsilon > 0$. There is a positive integer $N$ such that $M/N < \varepsilon$. Show that if $|x-y|<a/N$, then |$f(x)-f(y)|<\varepsilon$.","I'm new here and could really use some help please: Let $f$ be an additive function. So for all $x,y \in \mathbb{R}$, $f(x+y) = f(x)+f(y)$. Prove that if there are $M>0$ and $a>0$ such that if $x \in [-a,a]$, then $|f(x)|\leq M$, then $f$ has a limit at every $x\in \mathbb{R}$ and $\lim_{t\rightarrow x} f(t) = f(x)$. Prove that if $f$ has a limit at each $x\in \mathbb{R}$, then there are $M>0$ and $a>0$ such that if $x\in [-a,a]$, then $|f(x)| \leq M$. if necessary the proofs should involve the $\delta - \varepsilon$ definition of a limit. The problem had two previous portions to it that I already know how to do. However, you can reference them to do the posted portions of the problem. Here they are: (a) Show that for each positive integer $n$ and each real number $x$, $f(nx)=nf(x)$. (b) Suppose $f$ is such that there are $M>0$ and $a>0$ such that if $x\in [−a,a]$, then $|f(x)|\le M$. Choose $\varepsilon > 0$. There is a positive integer $N$ such that $M/N < \varepsilon$. Show that if $|x-y|<a/N$, then |$f(x)-f(y)|<\varepsilon$.",,"['real-analysis', 'functional-equations']"
72,Prove $\frac{1}{\sqrt{x}}\geq \frac{\ln x}{x-1}$,Prove,\frac{1}{\sqrt{x}}\geq \frac{\ln x}{x-1},"I am trying to show that, for all $x>0:$ $$\frac{1}{\sqrt{x}}\geq \frac{\ln x}{x-1}$$ This inequality is closer than I expected. I have tried exponentiating, power series, and have achieved nothing. I would really appreciate some help. Below is a graph of the two functions for small $x:$","I am trying to show that, for all $x>0:$ $$\frac{1}{\sqrt{x}}\geq \frac{\ln x}{x-1}$$ This inequality is closer than I expected. I have tried exponentiating, power series, and have achieved nothing. I would really appreciate some help. Below is a graph of the two functions for small $x:$",,"['calculus', 'real-analysis', 'inequality']"
73,Equivalence of the Lebesgue integral and the Henstock–Kurzweil integral on nonnegative real functions,Equivalence of the Lebesgue integral and the Henstock–Kurzweil integral on nonnegative real functions,,"Let $f:[a,b]\to[0,\infty)$ ($\mathbb{R}\ni a<b\in\mathbb{R}$), and fix $c\geq0$. I want to establish the equivalence of the concepts of Lebesgue integrability and Henstock–Kurzweil integrability for this class of functions. In particular, consider the following statements: (1)  For any $\varepsilon>0$, there exists a simple function $\omega=\sum_{j=1}^n z_j\mathbf{1}_{E_j}$ (where $n\in\mathbb{Z}_+$, the $\{z_j\}_{j=1}^n$ are distinct non-negative numbers, and the $\{E_j\}_{j=1}^n$ are Lebesgue measurable sets that form a partition of $[a,b]$) such that $0\leq\omega\leq f$ and (1a) $f$ is Lebesgue measurable and $\sum_{j=1}^n z_j\mu(E_j)\in( c-\varepsilon,c]$, where $\mu$ is the Lebesgue measure, and the analogous sum of any non-negative simple function dominated by $f$ does not exceed $c$; OR (1b) $f$ is Lebesgue measurable and $\sum_{j=1}^n z_j\mu(E_j)> \varepsilon$. (2) For any $\varepsilon>0$, there exists a “gauge” function $\delta_{\varepsilon}:[a,b]\to(0,\infty)$ such that for any $(x_j,t_j)_{j=1}^n\subset[a,b]$ ($n\in\mathbb{Z}_+$) satisfying $a<x_1<\ldots<x_{n-1}<x_n=b$, $t_j\in[x_j,x_{j-1}]$ for all $j\in\{1,\ldots,n\}$ (where $x_0\equiv a$), and $x_j-x_{j-1}<\delta_{\varepsilon}(t_j)$ for all $j\in\{1,\ldots,n\}$, then (2a) $\left|\sum_{j=1}^n f(t_j)(x_j-x_{j-1})-c\right|<\varepsilon$ for all $j\in\{1,\ldots,n\}$; OR (2b) $\sum_{j=1}^n f(t_j)(x_j-x_{j-1})>\varepsilon$ for all $j\in\{1,\ldots,n\}$. I want to show that $(1a)\Longleftrightarrow(2a)$ and $(1b)\Longleftrightarrow(2b)$. The tricky parts are as follows: For a given Lebesgue measurable function with integral $c$ (or $\infty$), how can one construct the desired gauge function so that the Henstock–Kurzweil integral is $c$ (or does not exist, respectively)? For a given Henstock–Kurzweil integrable function whose integral is finite or does not exist because it would be unbounded, how can one prove that it is Lebesgue measurable? For a given Henstock–Kurzweil integrable function, how can one construct the desired step function? I skimmed through some of the relevant literature, but I failed to find any satisfying and not-too-abstruse explanation. (Disclaimer: I'm a newbie in HK-integration with some background in measure theory.) Thank you very much for your help in advance. Note: (1a) basically states that $\int_{[a,b]}f(x)\,\mathrm{d}\mu(x)=c$ and (1b) that $\int_{[a,b]}f(x)\,\mathrm{d}\mu(x)=\infty.$","Let $f:[a,b]\to[0,\infty)$ ($\mathbb{R}\ni a<b\in\mathbb{R}$), and fix $c\geq0$. I want to establish the equivalence of the concepts of Lebesgue integrability and Henstock–Kurzweil integrability for this class of functions. In particular, consider the following statements: (1)  For any $\varepsilon>0$, there exists a simple function $\omega=\sum_{j=1}^n z_j\mathbf{1}_{E_j}$ (where $n\in\mathbb{Z}_+$, the $\{z_j\}_{j=1}^n$ are distinct non-negative numbers, and the $\{E_j\}_{j=1}^n$ are Lebesgue measurable sets that form a partition of $[a,b]$) such that $0\leq\omega\leq f$ and (1a) $f$ is Lebesgue measurable and $\sum_{j=1}^n z_j\mu(E_j)\in( c-\varepsilon,c]$, where $\mu$ is the Lebesgue measure, and the analogous sum of any non-negative simple function dominated by $f$ does not exceed $c$; OR (1b) $f$ is Lebesgue measurable and $\sum_{j=1}^n z_j\mu(E_j)> \varepsilon$. (2) For any $\varepsilon>0$, there exists a “gauge” function $\delta_{\varepsilon}:[a,b]\to(0,\infty)$ such that for any $(x_j,t_j)_{j=1}^n\subset[a,b]$ ($n\in\mathbb{Z}_+$) satisfying $a<x_1<\ldots<x_{n-1}<x_n=b$, $t_j\in[x_j,x_{j-1}]$ for all $j\in\{1,\ldots,n\}$ (where $x_0\equiv a$), and $x_j-x_{j-1}<\delta_{\varepsilon}(t_j)$ for all $j\in\{1,\ldots,n\}$, then (2a) $\left|\sum_{j=1}^n f(t_j)(x_j-x_{j-1})-c\right|<\varepsilon$ for all $j\in\{1,\ldots,n\}$; OR (2b) $\sum_{j=1}^n f(t_j)(x_j-x_{j-1})>\varepsilon$ for all $j\in\{1,\ldots,n\}$. I want to show that $(1a)\Longleftrightarrow(2a)$ and $(1b)\Longleftrightarrow(2b)$. The tricky parts are as follows: For a given Lebesgue measurable function with integral $c$ (or $\infty$), how can one construct the desired gauge function so that the Henstock–Kurzweil integral is $c$ (or does not exist, respectively)? For a given Henstock–Kurzweil integrable function whose integral is finite or does not exist because it would be unbounded, how can one prove that it is Lebesgue measurable? For a given Henstock–Kurzweil integrable function, how can one construct the desired step function? I skimmed through some of the relevant literature, but I failed to find any satisfying and not-too-abstruse explanation. (Disclaimer: I'm a newbie in HK-integration with some background in measure theory.) Thank you very much for your help in advance. Note: (1a) basically states that $\int_{[a,b]}f(x)\,\mathrm{d}\mu(x)=c$ and (1b) that $\int_{[a,b]}f(x)\,\mathrm{d}\mu(x)=\infty.$",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'gauge-integral']"
74,Eigenfunctions of the Laplacian,Eigenfunctions of the Laplacian,,"I am willing to offer a bounty for this one, so I will give you an exact idea of what I need: I am looking for solutions of $$\Delta \Psi(r,\theta)=k^2\Psi(r,\theta)$$ where $k\in \mathbb{R}$. Such that: $\Psi(r,\theta)=0$ for r approaching infinity. $\partial_r \Psi(r,\theta)=0$ for r approaching infinity and at some $R \in \mathbb{R}_{>0}$ we want to have $$\Psi(R,\theta)=\sum_{l=0}^{\infty} f(l)\frac{1}{R} P_l(\cos(\theta))$$ where $P_l$ is the l-th Legendre Polynomial and $f(l)=\frac{1}{2l+1}(P_{l+1}(\cos(\alpha)-P_{l-1}(\cos(\alpha))$. I am only interested in solutions on $$\mathbb{R}^3\backslash B(0,R)$$. Actually, you do not need to proof that you have found a solution. It would be totally sufficient to find a solution. It might be interesting to know that when we only have $r$ dependence, then the solution is given by something like $\frac{A e^{-r}}{r}$, where A is some constant. This is the big task, now if you have troubles with the form of $\Psi(R,\theta)$, I am also interested in a solution, where you can replace the one given above by: $\Psi(R,\theta)=CR\cos(\theta)$, where $C$ is a negative constant. (The other conditions (like $Psi$ goes to zero when r approaching infinity, remain all the same). The first one with an answer to any of both boundary conditions will get the full bounty.","I am willing to offer a bounty for this one, so I will give you an exact idea of what I need: I am looking for solutions of $$\Delta \Psi(r,\theta)=k^2\Psi(r,\theta)$$ where $k\in \mathbb{R}$. Such that: $\Psi(r,\theta)=0$ for r approaching infinity. $\partial_r \Psi(r,\theta)=0$ for r approaching infinity and at some $R \in \mathbb{R}_{>0}$ we want to have $$\Psi(R,\theta)=\sum_{l=0}^{\infty} f(l)\frac{1}{R} P_l(\cos(\theta))$$ where $P_l$ is the l-th Legendre Polynomial and $f(l)=\frac{1}{2l+1}(P_{l+1}(\cos(\alpha)-P_{l-1}(\cos(\alpha))$. I am only interested in solutions on $$\mathbb{R}^3\backslash B(0,R)$$. Actually, you do not need to proof that you have found a solution. It would be totally sufficient to find a solution. It might be interesting to know that when we only have $r$ dependence, then the solution is given by something like $\frac{A e^{-r}}{r}$, where A is some constant. This is the big task, now if you have troubles with the form of $\Psi(R,\theta)$, I am also interested in a solution, where you can replace the one given above by: $\Psi(R,\theta)=CR\cos(\theta)$, where $C$ is a negative constant. (The other conditions (like $Psi$ goes to zero when r approaching infinity, remain all the same). The first one with an answer to any of both boundary conditions will get the full bounty.",,"['calculus', 'real-analysis']"
75,The continuity of measure,The continuity of measure,,"Let $m$ be the Lebesgue Measure. If $\{A_k\}_{k=1}^{\infty}$ is an ascending collection of measurable sets, then $$m\left(\cup_{k=1}^\infty A_k\right)=\lim_{k\to\infty}m(A_k).$$ Can someone share a story as to why this is called one of the ""continuity"" properties of measure?","Let $m$ be the Lebesgue Measure. If $\{A_k\}_{k=1}^{\infty}$ is an ascending collection of measurable sets, then $$m\left(\cup_{k=1}^\infty A_k\right)=\lim_{k\to\infty}m(A_k).$$ Can someone share a story as to why this is called one of the ""continuity"" properties of measure?",,['real-analysis']
76,"Is a function with bounded Holder continuous derivatives itself ""more"" than just Lipschitz continuous.?","Is a function with bounded Holder continuous derivatives itself ""more"" than just Lipschitz continuous.?",,"Was wondering about this as I brushed my teeth this morning. I have a differentiable function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ that has bounded and $\gamma$-Holder continuous derivatives. Can I prove that $\exists \; C >0$ such that \begin{equation*} |f(x+h)-f(x)| \leq C|h|^{1+\gamma} \quad \forall x,h \in \mathbb{R}^n \quad?  \end{equation*}","Was wondering about this as I brushed my teeth this morning. I have a differentiable function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ that has bounded and $\gamma$-Holder continuous derivatives. Can I prove that $\exists \; C >0$ such that \begin{equation*} |f(x+h)-f(x)| \leq C|h|^{1+\gamma} \quad \forall x,h \in \mathbb{R}^n \quad?  \end{equation*}",,"['calculus', 'real-analysis', 'multivariable-calculus', 'holder-spaces']"
77,"Show that a set $D$ is measurable with respect to a product measure $m \times \nu$, (lebesgue $\times$ counting measure)","Show that a set  is measurable with respect to a product measure , (lebesgue  counting measure)",D m \times \nu \times,"Let $m$ be the Lebesgue measure on $[0,1]$, and $\nu$ be the counting measure on $[0,1]$. Show that the diagonal (1) $$D = \{ (x,x), x \in [0,1] \}  $$ is measurable with respect to $m \times \nu$, but if $\chi_{D}$ denotes its characteristic function then (2) $$ \int_{[0,1]}\int_{[0,1]} \chi_{D}(x,y)\,dm(x)\,d\nu(y) \neq \int_{[0,1]} \int_{[0,1]} \chi_{D}(x,y)\,d\nu(y)\,dm(x).$$ Ok, so I do not have so much experience of product measures. Let's start with showing that is a measurable set. What does it take for $D$ to be measurable w.r.t. $m \times \nu$ ? Must it lie in a sigma-algebra, (which is not given in the exercise)? Or what do I have to show? And how about (2)? My guess is that it has something to with that the counting measure is infinite on $[0,1]$. Shall it be seen as a integral in $\mathbb{R}^{2}$ and $D$ as the line $y = x$? Then the lebesgue measure of a straight line in $\mathbb{R}^{2}$ is always zero and the left integral would be $0$? On the other hand, in the right integral we have an inner integral w.r.t. the counting measure which is infinite so that one equals $\infty$? On this one I am quite stuck and any help and explanation of what is going on is appreciated. / Erik","Let $m$ be the Lebesgue measure on $[0,1]$, and $\nu$ be the counting measure on $[0,1]$. Show that the diagonal (1) $$D = \{ (x,x), x \in [0,1] \}  $$ is measurable with respect to $m \times \nu$, but if $\chi_{D}$ denotes its characteristic function then (2) $$ \int_{[0,1]}\int_{[0,1]} \chi_{D}(x,y)\,dm(x)\,d\nu(y) \neq \int_{[0,1]} \int_{[0,1]} \chi_{D}(x,y)\,d\nu(y)\,dm(x).$$ Ok, so I do not have so much experience of product measures. Let's start with showing that is a measurable set. What does it take for $D$ to be measurable w.r.t. $m \times \nu$ ? Must it lie in a sigma-algebra, (which is not given in the exercise)? Or what do I have to show? And how about (2)? My guess is that it has something to with that the counting measure is infinite on $[0,1]$. Shall it be seen as a integral in $\mathbb{R}^{2}$ and $D$ as the line $y = x$? Then the lebesgue measure of a straight line in $\mathbb{R}^{2}$ is always zero and the left integral would be $0$? On the other hand, in the right integral we have an inner integral w.r.t. the counting measure which is infinite so that one equals $\infty$? On this one I am quite stuck and any help and explanation of what is going on is appreciated. / Erik",,"['real-analysis', 'measure-theory']"
78,Does non-standard analysis still make use of classical topology?,Does non-standard analysis still make use of classical topology?,,"I understand that non-standard analysis is generally viewed as an alternative to the $\epsilon$-$\delta$ approach of classical analysis. So for example, I'm guessing that the definition of a continuous function between metric spaces would be made rigorous using infinitesimals, rather than the usual $\epsilon$-$\delta$ definition. However, the notion of continuity can be generalized to functions between arbitrary topological spaces. My question is, how does non-standard analysis stand in relation to general topology? Are concepts like ""continuous function between topological spaces"" still defined and studied, and if so, are they defined in the same way as in the classical approach? What about compactness and connectedness? Are they defined in the same way, or are they defined differently? More generally, how much of a person's knowledge about classical analysis can be expected to transfer over to the non-standard approach?","I understand that non-standard analysis is generally viewed as an alternative to the $\epsilon$-$\delta$ approach of classical analysis. So for example, I'm guessing that the definition of a continuous function between metric spaces would be made rigorous using infinitesimals, rather than the usual $\epsilon$-$\delta$ definition. However, the notion of continuity can be generalized to functions between arbitrary topological spaces. My question is, how does non-standard analysis stand in relation to general topology? Are concepts like ""continuous function between topological spaces"" still defined and studied, and if so, are they defined in the same way as in the classical approach? What about compactness and connectedness? Are they defined in the same way, or are they defined differently? More generally, how much of a person's knowledge about classical analysis can be expected to transfer over to the non-standard approach?",,"['real-analysis', 'general-topology', 'nonstandard-analysis']"
79,uniform convergence of few sequence of functions,uniform convergence of few sequence of functions,,"Pick out the sequences $\{f_n\}$ which are uniformly convergent. (a) $f_n(x) = nxe^{−nx}$ on $(0,∞)$. (b)$f_n(x) = x^n$ on $[0, 1]$. (c)$f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ on $\mathbb{R}$. (d) $f_n(x)=\frac{nx}{1 + nx}$ on $(0,1)$ (e) $f_n(x) = ∑_{n=1}^∞\frac{n\sin(nx)}{e^n}$ on $[0,\pi]$ (f) $f_n(x) = \frac{x^n}{1 + x^n}$ ; on  $[0, 2]$ (g) $f_n(x) = \sin^nx$ on  $[0,\pi/2)$ (h) $f_n(x) = (x^n/n)+1$ on $[0,1)$ (i) $f_n(x) = \frac{1}{1+(x-n)^2}$ on $(-∞,0)$ (j) $f_n(x) = \frac{1}{1+(x-n)^2}$ on $(0, ∞)$ (a) Here the function becomes $nx/e^{nx}$ which is tends to $0$ as $n$ tends to $∞$ so the function is uniformly continuous. I am not sure though. (b) Here limit function is not continuous so not uniformly continuous (c) true by same reason of (a) (d)  false as $n$ tends to $∞$ the function tends to $1$ not zero. (e) no idea (f) false as limit function is not continuous (g) true by M test (h) true by M test (i) no idea (j) no idea Can somebody guide me properly please","Pick out the sequences $\{f_n\}$ which are uniformly convergent. (a) $f_n(x) = nxe^{−nx}$ on $(0,∞)$. (b)$f_n(x) = x^n$ on $[0, 1]$. (c)$f_n(x) = \frac{\sin(nx)}{\sqrt{n}}$ on $\mathbb{R}$. (d) $f_n(x)=\frac{nx}{1 + nx}$ on $(0,1)$ (e) $f_n(x) = ∑_{n=1}^∞\frac{n\sin(nx)}{e^n}$ on $[0,\pi]$ (f) $f_n(x) = \frac{x^n}{1 + x^n}$ ; on  $[0, 2]$ (g) $f_n(x) = \sin^nx$ on  $[0,\pi/2)$ (h) $f_n(x) = (x^n/n)+1$ on $[0,1)$ (i) $f_n(x) = \frac{1}{1+(x-n)^2}$ on $(-∞,0)$ (j) $f_n(x) = \frac{1}{1+(x-n)^2}$ on $(0, ∞)$ (a) Here the function becomes $nx/e^{nx}$ which is tends to $0$ as $n$ tends to $∞$ so the function is uniformly continuous. I am not sure though. (b) Here limit function is not continuous so not uniformly continuous (c) true by same reason of (a) (d)  false as $n$ tends to $∞$ the function tends to $1$ not zero. (e) no idea (f) false as limit function is not continuous (g) true by M test (h) true by M test (i) no idea (j) no idea Can somebody guide me properly please",,['real-analysis']
80,Proving the existence of a non-decreasing sequence,Proving the existence of a non-decreasing sequence,,If $\{a_{n}\}$ is a sequence of positive numbers so that $\sum_{n=0}^{\infty} a_{n}$ converges. How can we show that there exists a non-decreasing sequence $\{b_{n}\}$ so that $\lim_{n\to \infty}b_{n}=\infty$ and $\sum_{n}^{\infty}a_{n}b_{n}$<$\infty$.,If $\{a_{n}\}$ is a sequence of positive numbers so that $\sum_{n=0}^{\infty} a_{n}$ converges. How can we show that there exists a non-decreasing sequence $\{b_{n}\}$ so that $\lim_{n\to \infty}b_{n}=\infty$ and $\sum_{n}^{\infty}a_{n}b_{n}$<$\infty$.,,"['real-analysis', 'sequences-and-series']"
81,About the asymptotic behaviour of $\sum_{n\in\mathbb{N}}\frac{x^{a_n}}{a_n!}$,About the asymptotic behaviour of,\sum_{n\in\mathbb{N}}\frac{x^{a_n}}{a_n!},"Let $\{a_n\}_{n\in\mathbb{N}}$ be an increasing sequence of natural numbers, and $$ f_A(x)=\sum_{n\in\mathbb{N}}\frac{x^{a_n}}{a_n!}. $$ There are some cases in which the limit $$ l_A=\lim_{x\to+\infty} \frac{1}{x}\,\log(f_A(x)) $$ does not exist. However, if $\{a_n\}_{n\in\mathbb{N}}$ is an arithmetic progression, we have $l_A=1$ (it follows from a straightforward application of the discrete Fourier transform). Consider now the case $a_n=n^2.$ Is it true that there exists a positive constant $c$ for which $$\forall x>0,\quad e^{-x}f_A(x)=\sum_{k\in\mathbb{N}}x^k\left(\sum_{0\leq j\leq\sqrt{k}}\frac{(-1)^{k-j^2}}{(j^2)!\,(k-j^2)!}\right)\geq c\;?$$ Is it true that $l_A=1$?","Let $\{a_n\}_{n\in\mathbb{N}}$ be an increasing sequence of natural numbers, and $$ f_A(x)=\sum_{n\in\mathbb{N}}\frac{x^{a_n}}{a_n!}. $$ There are some cases in which the limit $$ l_A=\lim_{x\to+\infty} \frac{1}{x}\,\log(f_A(x)) $$ does not exist. However, if $\{a_n\}_{n\in\mathbb{N}}$ is an arithmetic progression, we have $l_A=1$ (it follows from a straightforward application of the discrete Fourier transform). Consider now the case $a_n=n^2.$ Is it true that there exists a positive constant $c$ for which $$\forall x>0,\quad e^{-x}f_A(x)=\sum_{k\in\mathbb{N}}x^k\left(\sum_{0\leq j\leq\sqrt{k}}\frac{(-1)^{k-j^2}}{(j^2)!\,(k-j^2)!}\right)\geq c\;?$$ Is it true that $l_A=1$?",,"['real-analysis', 'sequences-and-series', 'inequality']"
82,Can we create a dense set in the interval by this steps?,Can we create a dense set in the interval by this steps?,,"I have a question that is something I am wondering for some time now and I couldn't even begin answering it. I guess it could be called a riddle. So, let $x\in[0,1]$ and $k\in(0,1)$. We begin at $x$ and we say that we make a ""step up"" by going to $x+k(1-x)$ and we make a ""step down"" by going to $x-kx$. Notice that whatever $x$ and $k$ we choose we have $x-kx\in[0,1]$ and $x+k(1-x)\in[0,1]$. Then let $X_0=\{x\}$ and $X_{n+1}=\bigcup_{x\in X_n}\{x,x-kx,x+k(1-x)\}$. So $X_{n+1}$ is the set we get if we add the ""up step"" and the ""down step"" of every element of $X_n$ to $X_n$. Let $X_\infty=\lim_{n\to\infty}X_n$. So my question is: Is $\overline{X_\infty}=[0,1]$? Another related question I couldn't also answer is: By beginning at an arbitrary $x$ is there any (finite) number of steps one can make to get back exactly to $x Also I am not really sure about the tags, so feel free to modify them.","I have a question that is something I am wondering for some time now and I couldn't even begin answering it. I guess it could be called a riddle. So, let $x\in[0,1]$ and $k\in(0,1)$. We begin at $x$ and we say that we make a ""step up"" by going to $x+k(1-x)$ and we make a ""step down"" by going to $x-kx$. Notice that whatever $x$ and $k$ we choose we have $x-kx\in[0,1]$ and $x+k(1-x)\in[0,1]$. Then let $X_0=\{x\}$ and $X_{n+1}=\bigcup_{x\in X_n}\{x,x-kx,x+k(1-x)\}$. So $X_{n+1}$ is the set we get if we add the ""up step"" and the ""down step"" of every element of $X_n$ to $X_n$. Let $X_\infty=\lim_{n\to\infty}X_n$. So my question is: Is $\overline{X_\infty}=[0,1]$? Another related question I couldn't also answer is: By beginning at an arbitrary $x$ is there any (finite) number of steps one can make to get back exactly to $x Also I am not really sure about the tags, so feel free to modify them.",,['real-analysis']
83,"Prove $e^n$ and $\ln(n)$, mod 1, for $n=2,3,4...$ is dense in $[0,1]$","Prove  and , mod 1, for  is dense in","e^n \ln(n) n=2,3,4... [0,1]","How can one prove $e^n$ and $\ln(n)$, modulo 1, are dense in $[0,1]$, for $n=2,3,4...$? By dense is meant, for any $0<a<b<1$, there is an integer $m$ such that $0<a<f(m) mod 1<b<1$","How can one prove $e^n$ and $\ln(n)$, modulo 1, are dense in $[0,1]$, for $n=2,3,4...$? By dense is meant, for any $0<a<b<1$, there is an integer $m$ such that $0<a<f(m) mod 1<b<1$",,['real-analysis']
84,Infinite product problem: $\sum p_n< \infty$ implies $\prod (1-p_n)>0$,Infinite product problem:  implies,\sum p_n< \infty \prod (1-p_n)>0,"Prove that if $0\le p_n< 1$ and $S=\sum p_n< \infty$ then $\prod (1-p_n)>0$ . Hint given: First show that if $S<1$ , then $\prod (1-p_n)\ge 1-S$ . Attempt: I was able to show the hint by using recursion setting $A_n=\prod_{i=1}^{n}(1-p_i)$ re expression the $\prod (1-p_n)$ as $1-S+\sum_{n_1, n_2=1,n_1<n_2 }^{\infty} p_{n_1}p_{n_2}-..... $ and observed that every subsequent term is less than the previous term hence $\prod (1-p_n)\ge 1-S$ which is $>0$ if $S<1$ but am unsure about how to extend it to $S\ge1$ .","Prove that if and then . Hint given: First show that if , then . Attempt: I was able to show the hint by using recursion setting re expression the as and observed that every subsequent term is less than the previous term hence which is if but am unsure about how to extend it to .","0\le p_n< 1 S=\sum p_n< \infty \prod (1-p_n)>0 S<1 \prod (1-p_n)\ge 1-S A_n=\prod_{i=1}^{n}(1-p_i) \prod (1-p_n) 1-S+\sum_{n_1, n_2=1,n_1<n_2 }^{\infty} p_{n_1}p_{n_2}-.....  \prod (1-p_n)\ge 1-S >0 S<1 S\ge1","['real-analysis', 'sequences-and-series', 'infinite-product']"
85,continuity of power series,continuity of power series,,"I want to prove that every power series is continuous but I am stuck at one point. Let $\sum\limits_{n=0}^\infty a_n(x-x_0)^n$ a power series with a radius of convergence $r>0$ and let $D:=\{x\in\mathbb R:|x-x_0|<r\}$. Then $S(x)=\sum\limits_{n=0}^\infty a_n(x-x_0)^n$ is continuous on $D$. Proof: Let $x\in D$ and $r_0\in\mathbb R^+$ such that $|x-y_0|<r_0<r$ and for any positive integer $N$ let $S_N(y)=\sum\limits_{n=0}^{N-1} a_n(y-x_0)^n$ and $\phi_N(y)=\sum\limits_{n=N}^\infty a_n(y-x_0)^n$. Since the power series converges uniformly on the closed disk $|y-x_0|\leq r_0$, we may choose a positive integer $N_\varepsilon$ such that $|\phi_{N_\varepsilon}|<\frac\varepsilon3$ for all $|y-x_0|\leq r_0$. Since $S_{N_\varepsilon}$ is polynomial we can choose a $\bar\delta>0$ such that $|S_{N_\varepsilon}(y)-S_{N_\varepsilon}(x)|<\frac\varepsilon3$. So we get $|S(y)-S(x)|<\frac\varepsilon3+\frac\varepsilon3+\frac\varepsilon3=\varepsilon$ for $|y-x|<\delta$ Why is in the violet term $|\phi_{N_\varepsilon}|<\frac\varepsilon3$ correct? Don't I have two terms in the absolute value? Thanks for helping!","I want to prove that every power series is continuous but I am stuck at one point. Let $\sum\limits_{n=0}^\infty a_n(x-x_0)^n$ a power series with a radius of convergence $r>0$ and let $D:=\{x\in\mathbb R:|x-x_0|<r\}$. Then $S(x)=\sum\limits_{n=0}^\infty a_n(x-x_0)^n$ is continuous on $D$. Proof: Let $x\in D$ and $r_0\in\mathbb R^+$ such that $|x-y_0|<r_0<r$ and for any positive integer $N$ let $S_N(y)=\sum\limits_{n=0}^{N-1} a_n(y-x_0)^n$ and $\phi_N(y)=\sum\limits_{n=N}^\infty a_n(y-x_0)^n$. Since the power series converges uniformly on the closed disk $|y-x_0|\leq r_0$, we may choose a positive integer $N_\varepsilon$ such that $|\phi_{N_\varepsilon}|<\frac\varepsilon3$ for all $|y-x_0|\leq r_0$. Since $S_{N_\varepsilon}$ is polynomial we can choose a $\bar\delta>0$ such that $|S_{N_\varepsilon}(y)-S_{N_\varepsilon}(x)|<\frac\varepsilon3$. So we get $|S(y)-S(x)|<\frac\varepsilon3+\frac\varepsilon3+\frac\varepsilon3=\varepsilon$ for $|y-x|<\delta$ Why is in the violet term $|\phi_{N_\varepsilon}|<\frac\varepsilon3$ correct? Don't I have two terms in the absolute value? Thanks for helping!",,"['calculus', 'real-analysis']"
86,Showing $\int\limits_a^b h(x)\sin(nx) dx \rightarrow 0$,Showing,\int\limits_a^b h(x)\sin(nx) dx \rightarrow 0,"Let $h\in C_0([a,b])$ arbitrary, that is $h$ is continuous and vanishes on the boundary. I want to show that $\int\limits_a^b h(x)\sin(nx)dx \rightarrow 0$. If $h\in C^1$, integration by parts immediately yields the claim, since $h'$ is continuous and thence bounded on the compact interval, using also the zero boundary condition. However, I believe the statement is also true for all $h\in C_0([a,b])$. My idea is to approximate $h$ by functions $h_m \in C_0^1([a,b])$. Then for all $m$, $$\begin{equation*} \lim_{n \to \infty} \int h_m(x) \sin(nx) dx = 0. \end{equation*}$$ $$\begin{align*} \Rightarrow ~~~ \lim_{n \to \infty} \int h(x)\sin(nx) dx &= \lim_{n \to \infty} \int \lim_{m \to \infty} h_m(x)\sin(nx) dx\\ &= \lim_{m \to \infty}(\lim_{n \to \infty} \int h_m(x)\sin(nx) dx)\\ &= \lim 0 = 0. \end{align*}$$ This is fine iff the second equality is. In fact, this is two different steps, as three limiting processes are involved. Hence the questions: First, can I make sure that I can interchange the $m$-limit with the integral sign? (Can I assume that $h_m$ converges uniformly? Or use some sort of Dominated Convergence Theorem?) And second, may I swap the $n$-limit for the $m$-limit? (The $n$-limit is in fact $C/n \to 0$) I hope it's not too messy. Many thanks for any kind of help!","Let $h\in C_0([a,b])$ arbitrary, that is $h$ is continuous and vanishes on the boundary. I want to show that $\int\limits_a^b h(x)\sin(nx)dx \rightarrow 0$. If $h\in C^1$, integration by parts immediately yields the claim, since $h'$ is continuous and thence bounded on the compact interval, using also the zero boundary condition. However, I believe the statement is also true for all $h\in C_0([a,b])$. My idea is to approximate $h$ by functions $h_m \in C_0^1([a,b])$. Then for all $m$, $$\begin{equation*} \lim_{n \to \infty} \int h_m(x) \sin(nx) dx = 0. \end{equation*}$$ $$\begin{align*} \Rightarrow ~~~ \lim_{n \to \infty} \int h(x)\sin(nx) dx &= \lim_{n \to \infty} \int \lim_{m \to \infty} h_m(x)\sin(nx) dx\\ &= \lim_{m \to \infty}(\lim_{n \to \infty} \int h_m(x)\sin(nx) dx)\\ &= \lim 0 = 0. \end{align*}$$ This is fine iff the second equality is. In fact, this is two different steps, as three limiting processes are involved. Hence the questions: First, can I make sure that I can interchange the $m$-limit with the integral sign? (Can I assume that $h_m$ converges uniformly? Or use some sort of Dominated Convergence Theorem?) And second, may I swap the $n$-limit for the $m$-limit? (The $n$-limit is in fact $C/n \to 0$) I hope it's not too messy. Many thanks for any kind of help!",,"['real-analysis', 'integration', 'limits']"
87,Why do we need continuity of a partial derivative to differentiate under an integral?,Why do we need continuity of a partial derivative to differentiate under an integral?,,"I essentially need to prove Leibniz's Rule for differentiation under an integral: given a continuous function $f(x,y)$ and a continuous partial $\frac{\partial f}{\partial y}$ prove that if $G(y) = \int_a^b f(t,y) \, dt$ then $G'(y) = \int_a^b \frac{\partial f}{\partial y}(t,y) dt$. There are lots of proofs for this online, but I can't quite understand why they require the partial derivative to be continuous. As far as I can see, I need to prove that $$\lim_{h \to 0} \frac{G(y+h)-G(y)}{h} = \lim_{h \to 0} \int_a^b \frac{f(t,y+h)-f(t,y)}{h} dt = \int_a^b \frac{\partial f}{\partial y}(t,y) dt$$ which, using the epsilon-delta definition of the limit means that I want to show that for any fixed $\epsilon$ I can find a $\delta > 0$ such that as long as $h < \delta$ $$\left| \int_a^b \frac{f(t,y+h)-f(t,y)}{h} - \frac{\partial f}{\partial y}(t,y) dt \right| < \epsilon$$ At this point most of the proofs I've seen turn to the Mean Value Theorem, and then make use of the continuity assumption, but doesn't the existence of $\frac{\partial f}{\partial y}$ mean that $$\lim_{h \to 0} \frac{f(t,y+h)-f(t,y)}{h} = \frac{\partial f}{\partial y}$$ which means it's relatively easy to select a relevant $\delta$ without having to turn to continuity? Sorry if that was overly verbose, and thanks for any help!!","I essentially need to prove Leibniz's Rule for differentiation under an integral: given a continuous function $f(x,y)$ and a continuous partial $\frac{\partial f}{\partial y}$ prove that if $G(y) = \int_a^b f(t,y) \, dt$ then $G'(y) = \int_a^b \frac{\partial f}{\partial y}(t,y) dt$. There are lots of proofs for this online, but I can't quite understand why they require the partial derivative to be continuous. As far as I can see, I need to prove that $$\lim_{h \to 0} \frac{G(y+h)-G(y)}{h} = \lim_{h \to 0} \int_a^b \frac{f(t,y+h)-f(t,y)}{h} dt = \int_a^b \frac{\partial f}{\partial y}(t,y) dt$$ which, using the epsilon-delta definition of the limit means that I want to show that for any fixed $\epsilon$ I can find a $\delta > 0$ such that as long as $h < \delta$ $$\left| \int_a^b \frac{f(t,y+h)-f(t,y)}{h} - \frac{\partial f}{\partial y}(t,y) dt \right| < \epsilon$$ At this point most of the proofs I've seen turn to the Mean Value Theorem, and then make use of the continuity assumption, but doesn't the existence of $\frac{\partial f}{\partial y}$ mean that $$\lim_{h \to 0} \frac{f(t,y+h)-f(t,y)}{h} = \frac{\partial f}{\partial y}$$ which means it's relatively easy to select a relevant $\delta$ without having to turn to continuity? Sorry if that was overly verbose, and thanks for any help!!",,"['real-analysis', 'multivariable-calculus', 'leibniz-integral-rule']"
88,Uniform Convergence of an Exponential Sequence of Functions,Uniform Convergence of an Exponential Sequence of Functions,,"How can I show that for every $\epsilon>0$, there exists an $N\in\mathbb{N}$ such that $$\left|f_n(x)-f(x)\right|=\left|\left(\frac{x}{n}+1\right)^n-e^x\right|<\epsilon$$ whenever $n\geq N$ and $x\in\left[-A,A\right]$? By the way, $n\in\mathbb{N}$. In a previous exercise, I was able to show that $f_n$ does indeed converge pointwise to $f$. However, I have been stuck for hours trying to prove uniform convergence. Would anyone lend me a hand? Thanks in advance.","How can I show that for every $\epsilon>0$, there exists an $N\in\mathbb{N}$ such that $$\left|f_n(x)-f(x)\right|=\left|\left(\frac{x}{n}+1\right)^n-e^x\right|<\epsilon$$ whenever $n\geq N$ and $x\in\left[-A,A\right]$? By the way, $n\in\mathbb{N}$. In a previous exercise, I was able to show that $f_n$ does indeed converge pointwise to $f$. However, I have been stuck for hours trying to prove uniform convergence. Would anyone lend me a hand? Thanks in advance.",,['real-analysis']
89,Domains of continuity,Domains of continuity,,"I was playing around with the definition of uniform continuity, and realized that a nice application of it is the possibility to extend functions. For example, suppose we are given a uniformly continuous function $f:\mathbb{Q}\to\mathbb{R}$. By uniform continuity, it is easy to see that such a function extends (uniquely of course) to a continuous function $f:\mathbb{R}\to\mathbb{R}$. If we drop the uniform continuity assumption and demand only that $f$ to be continuous, this is no longer true, as easily demonstrated by $f(x) = \frac{1}{x-\pi}$ which is continuous on $\mathbb{Q}$ but cannot be extended to a continuous function on all of $\mathbb{R}$. Which brings me to my question: Is there a nice description of the sets $A\subseteq \mathbb{R}$ which have the following property: there is a function $f:A\to \mathbb{R}$ which is continuous, but for any $x \notin A$, $f$ cannot be extended to a continuous function on $A\cup \{x\}$ ? Certainly open sets have this property, because if $A$ is open and $B$ is its complement, then we may define $f:A\to \mathbb{R}$ by $f(x) = \frac{1}{dist(x,B)}$. Conversely, are all such sets open? Edit: per Robert Israel nice examples, it appear that not all such sets are open. I still wonder if there is a nice description of this sets?","I was playing around with the definition of uniform continuity, and realized that a nice application of it is the possibility to extend functions. For example, suppose we are given a uniformly continuous function $f:\mathbb{Q}\to\mathbb{R}$. By uniform continuity, it is easy to see that such a function extends (uniquely of course) to a continuous function $f:\mathbb{R}\to\mathbb{R}$. If we drop the uniform continuity assumption and demand only that $f$ to be continuous, this is no longer true, as easily demonstrated by $f(x) = \frac{1}{x-\pi}$ which is continuous on $\mathbb{Q}$ but cannot be extended to a continuous function on all of $\mathbb{R}$. Which brings me to my question: Is there a nice description of the sets $A\subseteq \mathbb{R}$ which have the following property: there is a function $f:A\to \mathbb{R}$ which is continuous, but for any $x \notin A$, $f$ cannot be extended to a continuous function on $A\cup \{x\}$ ? Certainly open sets have this property, because if $A$ is open and $B$ is its complement, then we may define $f:A\to \mathbb{R}$ by $f(x) = \frac{1}{dist(x,B)}$. Conversely, are all such sets open? Edit: per Robert Israel nice examples, it appear that not all such sets are open. I still wonder if there is a nice description of this sets?",,['real-analysis']
90,"Does the uniform continuity of $f: X \rightarrow \mathbb{R}$ imply $f: A \rightarrow \mathbb{R}$ is also uniformly continuous, when $A \subset X$?","Does the uniform continuity of  imply  is also uniformly continuous, when ?",f: X \rightarrow \mathbb{R} f: A \rightarrow \mathbb{R} A \subset X,"I've been preparing for the prelim in August, and was working on a problem involving uniform continuity and restriction of functions.  I absentmindedly assumed the above by considering the contrapositive: if $f: A \rightarrow \mathbb{R}$ isn't uniformly continuous, that implies $\exists \ \epsilon$ such that no $\delta$ satisfies $d(x,y) < \delta \implies d(f(x),f(y)) < \epsilon, \,\,\ \forall x,y \in A$, and this failure of $\epsilon$'s existence shouldn't change when I ""add more points"" by considering $f: X \rightarrow \mathbb{R}.$ However, if this is true, we obtained a lot of results I consider to be strangely powerful.  For example, if a function is continuous on $\mathbb{R}$, it is uniformly continuous on any bounded interval I, as it's uniformly continuous on $\overline{I}$ which is compact by Heine-Borel.  Hence, if $f$ is a real-valued function continuous on a subset $A$ of $R$, it's uniformly continuous on any bounded subset $X$ of $A$. Conclusions such as this seem too strong! Is there a flaw in my reasoning, and if so, where is it?","I've been preparing for the prelim in August, and was working on a problem involving uniform continuity and restriction of functions.  I absentmindedly assumed the above by considering the contrapositive: if $f: A \rightarrow \mathbb{R}$ isn't uniformly continuous, that implies $\exists \ \epsilon$ such that no $\delta$ satisfies $d(x,y) < \delta \implies d(f(x),f(y)) < \epsilon, \,\,\ \forall x,y \in A$, and this failure of $\epsilon$'s existence shouldn't change when I ""add more points"" by considering $f: X \rightarrow \mathbb{R}.$ However, if this is true, we obtained a lot of results I consider to be strangely powerful.  For example, if a function is continuous on $\mathbb{R}$, it is uniformly continuous on any bounded interval I, as it's uniformly continuous on $\overline{I}$ which is compact by Heine-Borel.  Hence, if $f$ is a real-valued function continuous on a subset $A$ of $R$, it's uniformly continuous on any bounded subset $X$ of $A$. Conclusions such as this seem too strong! Is there a flaw in my reasoning, and if so, where is it?",,['real-analysis']
91,Derived sets and ordinals,Derived sets and ordinals,,"Given a set $P$ of real numbers, its derived set is the set of all accumulation points - $a\in P^\prime$ if every open set containing $a$ also contains an infinite number of points from $P$ (equivalently, at least one point from $P$ different than $a$). Cantor defined $P^0=P$ and $P^k$ to be the derived set of $P^{k-1}$. Then he had the wonderful idea to continue this construction to $P^\omega = \bigcap_{k} P^k$, and then we have $P^{\omega +1}$ being the derived set of $P^\omega$ and so on. The question is whether it is interesting to continue into the ordinals in this manner. i.e. is there a set $P$ such that $P^\omega \ne P^{\omega +1}$. Is there an example?","Given a set $P$ of real numbers, its derived set is the set of all accumulation points - $a\in P^\prime$ if every open set containing $a$ also contains an infinite number of points from $P$ (equivalently, at least one point from $P$ different than $a$). Cantor defined $P^0=P$ and $P^k$ to be the derived set of $P^{k-1}$. Then he had the wonderful idea to continue this construction to $P^\omega = \bigcap_{k} P^k$, and then we have $P^{\omega +1}$ being the derived set of $P^\omega$ and so on. The question is whether it is interesting to continue into the ordinals in this manner. i.e. is there a set $P$ such that $P^\omega \ne P^{\omega +1}$. Is there an example?",,"['real-analysis', 'ordinals', 'descriptive-set-theory']"
92,Prove that the convergence of the sequence ($s_n$) implies the convergence of ($s_n^3$),Prove that the convergence of the sequence () implies the convergence of (),s_n s_n^3,"I believe I have the gist of how to prove this. My professor worked out a problem similar to this one only, instead of ($s_n^3$), he used ($s_n^2$), and I am slightly confused as to how he came up with certain portions of his proof. The following is the proof he gave us for ($s_n^2$). I believe after understanding his proof better, I can prove the original problem more easily. So please do not post the solution to the original question. Proof {the convergence of the sequence ($s_n$) implies the convergence of ($s_n^2$)} Since the lim ($s_n$)=s, we know ($s_n$) is bounded. That is there exists $M\in R$ such that $|s_n|$ $\le$ M for all $n\in N$ Now, for every $\varepsilon >0$ we have lim ($s_n$)$=s$.  Working on $\varepsilon/(M+|s|)>0$, there exists $N\in R$ such that $|s_n-s| \le \varepsilon /(M+|s|)$ whenever $n>N$, therefore for all $n>N$,   $|s_n^2 - s^2| = |s_n - s|*|s_n + s| \le |s_n - s|(|s_n|+|s|) \le |s_n - s|*(M + |s|)< \varepsilon $ Which proves lim $(s_n^2)$ = $s^2$. The following is my proof for the current problem (that is in the title). Let me know if I did anything incorrect. Proof Since the lim ($s_n$)=s, we know ($s_n$) is bounded. That is there exists $M > 0$ such that $|s_n|\le M$ for all $n\in \mathbb{N}$ Now, for every $\varepsilon >0$ since lim ($s_n$)=s, working on $\varepsilon /(3M^2)>0$, there exists $N\in \mathbb{N}$ such that  $|s_n-s| < \varepsilon /3M^2$ whenever $n>\mathbb{N}$ Therefore, for all $n>\mathbb{N}$ $|s_n^3 - s^3|$ = $|s_n - s|$ $|s_n^2 + s_n*s + s^2| \le $ $|s_n - s|$ $(|s_n^2|+|s_n||s|+ |s^2|) \le $ $(|s_n|^2+|s_n||s|+ |s|^2) \le $ $|s_n - s|*(M^2 + M*M + M^2) \le $ $|s_n - s|*(3M^2)< \varepsilon $ Which proves lim $(s_n^3)$ = $s^3$","I believe I have the gist of how to prove this. My professor worked out a problem similar to this one only, instead of ($s_n^3$), he used ($s_n^2$), and I am slightly confused as to how he came up with certain portions of his proof. The following is the proof he gave us for ($s_n^2$). I believe after understanding his proof better, I can prove the original problem more easily. So please do not post the solution to the original question. Proof {the convergence of the sequence ($s_n$) implies the convergence of ($s_n^2$)} Since the lim ($s_n$)=s, we know ($s_n$) is bounded. That is there exists $M\in R$ such that $|s_n|$ $\le$ M for all $n\in N$ Now, for every $\varepsilon >0$ we have lim ($s_n$)$=s$.  Working on $\varepsilon/(M+|s|)>0$, there exists $N\in R$ such that $|s_n-s| \le \varepsilon /(M+|s|)$ whenever $n>N$, therefore for all $n>N$,   $|s_n^2 - s^2| = |s_n - s|*|s_n + s| \le |s_n - s|(|s_n|+|s|) \le |s_n - s|*(M + |s|)< \varepsilon $ Which proves lim $(s_n^2)$ = $s^2$. The following is my proof for the current problem (that is in the title). Let me know if I did anything incorrect. Proof Since the lim ($s_n$)=s, we know ($s_n$) is bounded. That is there exists $M > 0$ such that $|s_n|\le M$ for all $n\in \mathbb{N}$ Now, for every $\varepsilon >0$ since lim ($s_n$)=s, working on $\varepsilon /(3M^2)>0$, there exists $N\in \mathbb{N}$ such that  $|s_n-s| < \varepsilon /3M^2$ whenever $n>\mathbb{N}$ Therefore, for all $n>\mathbb{N}$ $|s_n^3 - s^3|$ = $|s_n - s|$ $|s_n^2 + s_n*s + s^2| \le $ $|s_n - s|$ $(|s_n^2|+|s_n||s|+ |s^2|) \le $ $(|s_n|^2+|s_n||s|+ |s|^2) \le $ $|s_n - s|*(M^2 + M*M + M^2) \le $ $|s_n - s|*(3M^2)< \varepsilon $ Which proves lim $(s_n^3)$ = $s^3$",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
93,"Prove $\sum\limits_{i=1}^{2024}a_i<314$ where $a_1=2$, $a_i=2\sin\frac{a_{i-1}}2$","Prove  where ,",\sum\limits_{i=1}^{2024}a_i<314 a_1=2 a_i=2\sin\frac{a_{i-1}}2,"Let $a_1=2$ and $a_i=2\sin\frac{a_{i-1}}2$ for $i\ge2$ . Prove that $\sum\limits_{i=1}^{2024}a_i<314$ . In fact, $\sum\limits_{i=1}^{2024}a_i\approx298.796$ , so the inequality is very strong. I tried to establish inequalities with Taylor's series, and got $\sum\limits_{i=1}^{2024}a_i<582$ . This is not enough. We need better (more accurate) ways to estimate the series. This question is from the "" $\pi$ day math contest"" of THU, which has ended.","Let and for . Prove that . In fact, , so the inequality is very strong. I tried to establish inequalities with Taylor's series, and got . This is not enough. We need better (more accurate) ways to estimate the series. This question is from the "" day math contest"" of THU, which has ended.",a_1=2 a_i=2\sin\frac{a_{i-1}}2 i\ge2 \sum\limits_{i=1}^{2024}a_i<314 \sum\limits_{i=1}^{2024}a_i\approx298.796 \sum\limits_{i=1}^{2024}a_i<582 \pi,"['real-analysis', 'sequences-and-series', 'inequality', 'contest-math', 'recursion']"
94,"Let $f: [0,1]\to\mathbb{R}$ be injective. Does $\sum_{n=1}^{\infty} c_n\left( f(x)\right)^n=0\forall x\in [0,1] \implies c_n=0\forall n\in\mathbb{N}?$",Let  be injective. Does,"f: [0,1]\to\mathbb{R} \sum_{n=1}^{\infty} c_n\left( f(x)\right)^n=0\forall x\in [0,1] \implies c_n=0\forall n\in\mathbb{N}?","Let $f: [0,1] \to \mathbb{R}$ be injective. Does $ \displaystyle\sum_{n=1}^{\infty} c_n \left( f(x) \right) ^n  = 0\ \forall x\in [0,1] \implies c_n = 0\ \forall n\in\mathbb{N}\ ? $ Maybe something related to (i.e. a more general version of) Stone-Weierstrass could be helpful?",Let be injective. Does Maybe something related to (i.e. a more general version of) Stone-Weierstrass could be helpful?,"f: [0,1] \to \mathbb{R}  \displaystyle\sum_{n=1}^{\infty} c_n \left( f(x) \right) ^n
 = 0\ \forall x\in [0,1] \implies c_n = 0\ \forall n\in\mathbb{N}\ ? ","['real-analysis', 'sequences-and-series', 'linear-independence']"
95,An atypical special harmonic series,An atypical special harmonic series,,"The following atypical harmonic series was recently proposed by C.I. Valean , $$ \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{n^2}$$ $$=\frac{13}{12}\log^4(2)-\frac{35}{4}\log^2(2)\zeta(2)+\frac{91}{4}\log(2)\zeta(3)-\frac{665}{16}\zeta(4)+6 \log(2)\pi G -8 G^2$$ $$+12 \pi \Im\biggr\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr\}+26 \operatorname{Li}_4\left(\frac{1}{2}\right),$$ where $\zeta$ is the Riemann zeta function, $G$ represents the Catalan's constant, and $\operatorname{Li}_n$ denotes the Polylogarithm. His solution starts with using the well-known property with harmonic numbers and skew-harmonic numbers, that is $\overline{H}_{2n}=H_{2n}-H_n$ , or $H_n=H_{2n}-\overline{H}_{2n}$ , which leads to \begin{equation*}  \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{n^2}=\sum_{n=1}^{\infty}\frac{(H_{2n}-\overline{H}_{2n}) H_{4n}}{n^2}=4\sum_{n=1}^{\infty} \frac{H_{2n} H_{4n}}{(2n)^2}-4\sum_{n=1}^{\infty} \frac{\overline{H}_{2n} H_{4n}}{(2n)^2} \end{equation*} \begin{equation*} =2\sum_{n=1}^{\infty} \frac{H_n H_{2n}}{n^2}-2\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_n H_{2n}}{n^2}-2\sum_{n=1}^{\infty} \frac{\overline{H}_n H_{2n}}{n^2}+2\sum_{n=1}^{\infty} (-1)^{n-1}\frac{\overline{H}_n H_{2n}}{n^2}, \end{equation*} where the series $$\sum _{n=1}^{\infty} \frac{\overline{H}_nH_{2n}}{n^2}=\frac{507}{64}\zeta(4)-\frac{7}{4}\log(2)\zeta(3)+\frac{5}{4}\log^2(2)\zeta(2)-\frac{7}{48}\log^4(2)-2\pi \Im\biggr \{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr \}-\frac{7}{2}\operatorname{Li}_4\left(\frac{1}{2}\right)$$ and $$ \sum_{n=1}^{\infty} (-1)^{n-1}\frac{\overline{H}_n H_{2n}}{n^2}=\frac{5}{48}\log^4(2)-\frac{5}{8}\log^2(2)\zeta(2)+\frac{7}{2}\log(2)\zeta(3)-\frac{77}{32}\zeta(4)+\log(2)\pi G-2 G^2+\frac{5}{2}\operatorname{Li}_4\left(\frac{1}{2}\right)$$ are found in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023) , Chapter $4$ , Section $4.57$ , p. $454$ , then $$ \sum_{n=1}^{\infty}\frac{H_n H_{2n}}{n^2}=\frac{13}{8}\zeta(4)+\frac{7}{2}\log(2)\zeta(3)-\log^2(2)\zeta(2)+\frac{1}{6}\log^4(2)+4 \operatorname{Li}_4\left(\frac{1}{2}\right)$$ is given in the same book, Chapter $4$ , Section $4.23$ , p. $428$ , and finally $$ \sum _{n=1}^{\infty} (-1)^{n-1}  \frac{H_n H_{2n}}{n^2}=2G^2-2\log(2)\pi G-\frac{1}{8}\log^4(2)-\frac{21}{8}\log(2)\zeta(3)+\frac{3}{2}\log^2(2)\zeta(2)+\frac{773}{64}\zeta(4)-4\pi \Im\biggr \{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr \}-3\operatorname{Li}_4\left(\frac{1}{2}\right),$$ presented in the same reference, Chapter $4$ , Section $4.55$ , p. $453$ , thus giving the desired result. Questions: $1)$ . Are such results known in the literature, or similar ones like, say, $\displaystyle \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{(2n-1)^2}$ ? $2).$ I would love to see very different ideas, and strategies leading to the desired result, and at the same time remaining in the realm of simple real methods. So, what other ways would we like to explore? (there is absolutely no hurry)","The following atypical harmonic series was recently proposed by C.I. Valean , where is the Riemann zeta function, represents the Catalan's constant, and denotes the Polylogarithm. His solution starts with using the well-known property with harmonic numbers and skew-harmonic numbers, that is , or , which leads to where the series and are found in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023) , Chapter , Section , p. , then is given in the same book, Chapter , Section , p. , and finally presented in the same reference, Chapter , Section , p. , thus giving the desired result. Questions: . Are such results known in the literature, or similar ones like, say, ? I would love to see very different ideas, and strategies leading to the desired result, and at the same time remaining in the realm of simple real methods. So, what other ways would we like to explore? (there is absolutely no hurry)"," \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{n^2} =\frac{13}{12}\log^4(2)-\frac{35}{4}\log^2(2)\zeta(2)+\frac{91}{4}\log(2)\zeta(3)-\frac{665}{16}\zeta(4)+6 \log(2)\pi G -8 G^2 +12 \pi \Im\biggr\{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr\}+26 \operatorname{Li}_4\left(\frac{1}{2}\right), \zeta G \operatorname{Li}_n \overline{H}_{2n}=H_{2n}-H_n H_n=H_{2n}-\overline{H}_{2n} \begin{equation*}
 \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{n^2}=\sum_{n=1}^{\infty}\frac{(H_{2n}-\overline{H}_{2n}) H_{4n}}{n^2}=4\sum_{n=1}^{\infty} \frac{H_{2n} H_{4n}}{(2n)^2}-4\sum_{n=1}^{\infty} \frac{\overline{H}_{2n} H_{4n}}{(2n)^2}
\end{equation*} \begin{equation*}
=2\sum_{n=1}^{\infty} \frac{H_n H_{2n}}{n^2}-2\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_n H_{2n}}{n^2}-2\sum_{n=1}^{\infty} \frac{\overline{H}_n H_{2n}}{n^2}+2\sum_{n=1}^{\infty} (-1)^{n-1}\frac{\overline{H}_n H_{2n}}{n^2},
\end{equation*} \sum _{n=1}^{\infty} \frac{\overline{H}_nH_{2n}}{n^2}=\frac{507}{64}\zeta(4)-\frac{7}{4}\log(2)\zeta(3)+\frac{5}{4}\log^2(2)\zeta(2)-\frac{7}{48}\log^4(2)-2\pi \Im\biggr \{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr \}-\frac{7}{2}\operatorname{Li}_4\left(\frac{1}{2}\right)  \sum_{n=1}^{\infty} (-1)^{n-1}\frac{\overline{H}_n H_{2n}}{n^2}=\frac{5}{48}\log^4(2)-\frac{5}{8}\log^2(2)\zeta(2)+\frac{7}{2}\log(2)\zeta(3)-\frac{77}{32}\zeta(4)+\log(2)\pi G-2 G^2+\frac{5}{2}\operatorname{Li}_4\left(\frac{1}{2}\right) 4 4.57 454  \sum_{n=1}^{\infty}\frac{H_n H_{2n}}{n^2}=\frac{13}{8}\zeta(4)+\frac{7}{2}\log(2)\zeta(3)-\log^2(2)\zeta(2)+\frac{1}{6}\log^4(2)+4 \operatorname{Li}_4\left(\frac{1}{2}\right) 4 4.23 428  \sum _{n=1}^{\infty} (-1)^{n-1}  \frac{H_n H_{2n}}{n^2}=2G^2-2\log(2)\pi G-\frac{1}{8}\log^4(2)-\frac{21}{8}\log(2)\zeta(3)+\frac{3}{2}\log^2(2)\zeta(2)+\frac{773}{64}\zeta(4)-4\pi \Im\biggr \{\operatorname{Li}_3\left(\frac{1+i}{2}\right)\biggr \}-3\operatorname{Li}_4\left(\frac{1}{2}\right), 4 4.55 453 1) \displaystyle \sum_{n=1}^{\infty}\frac{H_n H_{4n}}{(2n-1)^2} 2).","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
96,Geometric characterization of the $n$-th derivative of $f$ being positive (convexity for $n=2$).,Geometric characterization of the -th derivative of  being positive (convexity for ).,n f n=2,"Does the following claim hold? Let $n\in \mathbb N_{\geq 2}$ and let $f:\mathbb R \to \mathbb R$ be an $n$ -times differentiable function. Then the $n$ -the derivative $f^{(n)}(x)\geq 0$ for all $x\in \mathbb R$ iff for any $a_1<a_2<\ldots<a_n$ $$   f(x) \leq p(x;a_1,\ldots,a_n) \quad \forall x \in [a_{n-1},a_n], \tag{*}\label{fp} $$ where $p(\cdot; a_1,\ldots,a_n)$ is the $n-1$ degree polynomial fitting the points $\big(a_1,f(a_1)\big),\ldots,\big(a_n,f(a_n)\big)$ . Note: The condition \eqref{fp} can be geometrically represented as follows: For any $n$ different points of the graph of $f$ consider the curve line segment between the last two points (those most on the right) of the polynomial curve fitting the graph of $f$ at the $n$ points. Then the curve line segment is contained in the epigraph of $f$ . Observations: For $n=2$ the condition \eqref{fp} simplifies into that the line segment between any two points of a graph of $f$ is in the epigraph of $f$ – the standard characterization of $\mathop{epi} f$ being a convex set. In general the condition $f^{(n)}(x)\geq 0$ can be interpreted as that $f^{(n-1)}$ is increasing, and equivalently that $f^{(n-2)}$ is a convex function. The intuitive idea is that if $f^{(n)}$ was constant then $f$ would be identical to the polynomial $p$ that fits any $n$ points in the graph of $f$ , and as $f^{(n)}$ is increasing it can be expected that $f$ would be above $p$ for $x$ large. Extensions: Extensions of this claim (with the condition that $f^{(n-1)}$ is increasing instead of that $f^{(n)}$ is positive) into the cases when $f$ is not necessarily $n-1$ times differentiable is discussed in A higher-order analogy to that if $f'$ is increasing on $\Bbb R$ except for countably many points, then $f$ is convex and A higher-order analogy to that every convex function is differentiable at all but countably many points . A characterization of the condition \eqref{fp} as a property of $\mathop(epi)(f)$ in the case of $n=3$ is discussed in Geometric characterization of functions with positive third derivative .","Does the following claim hold? Let and let be an -times differentiable function. Then the -the derivative for all iff for any where is the degree polynomial fitting the points . Note: The condition \eqref{fp} can be geometrically represented as follows: For any different points of the graph of consider the curve line segment between the last two points (those most on the right) of the polynomial curve fitting the graph of at the points. Then the curve line segment is contained in the epigraph of . Observations: For the condition \eqref{fp} simplifies into that the line segment between any two points of a graph of is in the epigraph of – the standard characterization of being a convex set. In general the condition can be interpreted as that is increasing, and equivalently that is a convex function. The intuitive idea is that if was constant then would be identical to the polynomial that fits any points in the graph of , and as is increasing it can be expected that would be above for large. Extensions: Extensions of this claim (with the condition that is increasing instead of that is positive) into the cases when is not necessarily times differentiable is discussed in A higher-order analogy to that if $f'$ is increasing on $\Bbb R$ except for countably many points, then $f$ is convex and A higher-order analogy to that every convex function is differentiable at all but countably many points . A characterization of the condition \eqref{fp} as a property of in the case of is discussed in Geometric characterization of functions with positive third derivative .","n\in \mathbb N_{\geq 2} f:\mathbb R \to \mathbb R n n f^{(n)}(x)\geq 0 x\in \mathbb R a_1<a_2<\ldots<a_n 
  f(x) \leq p(x;a_1,\ldots,a_n) \quad \forall x \in [a_{n-1},a_n], \tag{*}\label{fp}
 p(\cdot; a_1,\ldots,a_n) n-1 \big(a_1,f(a_1)\big),\ldots,\big(a_n,f(a_n)\big) n f f n f n=2 f f \mathop{epi} f f^{(n)}(x)\geq 0 f^{(n-1)} f^{(n-2)} f^{(n)} f p n f f^{(n)} f p x f^{(n-1)} f^{(n)} f n-1 \mathop(epi)(f) n=3","['real-analysis', 'geometry', 'convex-analysis', 'examples-counterexamples', 'convex-geometry']"
97,"If two metric spaces are homeomorphic, what more conditions are required so that their balls are homeomorphic?","If two metric spaces are homeomorphic, what more conditions are required so that their balls are homeomorphic?",,"I was talking to a friend about topological homeomorphisms, and the conversation turned to about ""sphere"" to ""cube"" homeomorphism in the standard topology. We found this paper which seemed to be quite complicated for showing it. My friend then tried to find an easier solution, and, mistakenly concluded that if two metric spaces are homeomorphic then their balls are also homeomorphic , so the sphere is homeomorphic to a cube trivially because the $d_1$ metric is same as $d_2$ metric on $\mathbb{R^3}$ (topological sense). We both concluded that this statement should not be generally true because even in a single metric space the balls around different point need not be homeomorphic, so one would have to specify which balls they are talking about in the framing of the above claim. I tried to account for that problem and frame the following question: If we have a homeomorphism $f,g$ between two metric spaces $X$ and $Y$ , when is that the ball centered at a point $x \in X$ is homeomorphic to a ball centered at the point point $f(x) \in Y$ ? I would also appreciate discussion on other ways of turning my friends statement into a rigorous true statement other than the above.","I was talking to a friend about topological homeomorphisms, and the conversation turned to about ""sphere"" to ""cube"" homeomorphism in the standard topology. We found this paper which seemed to be quite complicated for showing it. My friend then tried to find an easier solution, and, mistakenly concluded that if two metric spaces are homeomorphic then their balls are also homeomorphic , so the sphere is homeomorphic to a cube trivially because the metric is same as metric on (topological sense). We both concluded that this statement should not be generally true because even in a single metric space the balls around different point need not be homeomorphic, so one would have to specify which balls they are talking about in the framing of the above claim. I tried to account for that problem and frame the following question: If we have a homeomorphism between two metric spaces and , when is that the ball centered at a point is homeomorphic to a ball centered at the point point ? I would also appreciate discussion on other ways of turning my friends statement into a rigorous true statement other than the above.","d_1 d_2 \mathbb{R^3} f,g X Y x \in X f(x) \in Y","['real-analysis', 'general-topology', 'metric-spaces']"
98,"Prove or disprove $\sum\limits_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac18$ for $\sum\limits_{i=1}^n x_i = \frac12$($x_i\ge 0, \forall i$)",Prove or disprove  for (),"\sum\limits_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac18 \sum\limits_{i=1}^n x_i = \frac12 x_i\ge 0, \forall i","Problem 1 : Let $x_i \ge 0, \, i=1, 2, \cdots, n$ with $\sum_{i=1}^n x_i = \frac12$ . Prove or disprove that $$\sum_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac18.$$ This is related to the following problem: Problem 2 : Let $x_i \ge 0, \, i=1, 2, \cdots, n$ with $\sum_{i=1}^n x_i = \frac12$ . Prove that $$\sum_{1\le i<j\le n}\frac{x_ix_j}{(1-x_i)(1-x_j)}\le \frac{n(n-1)}{2(2n-1)^2}.$$ Problem 2 is in ""Problems From the Book"", 2008, Ch. 2, which was proposed by Vasile Cartoaje. See: Prove that $\sum_{1\le i<j\le n}\frac{x_ix_j}{(1-x_i)(1-x_j)} \le \frac{n(n-1)}{2(2n-1)^2}$ Background : I proposed Problem 1 when I tried to find my 2nd proof for Problem 2. It is not difficult to prove that $$\frac{1}{(2n-1)^4} + \frac{16n^2(n-1)^2}{(2n-1)^4}\cdot \frac{x_ix_j}{1-x_i-x_j} \ge \frac{x_ix_j}{(1-x_i)(1-x_j)}.$$ ( Hint : Use $\frac{x_ix_j}{(1-x_i)(1-x_j)}= 1 - \frac{1}{1 + x_ix_j/(1-x_i-x_j)}$ and $\frac{1}{1+u} \ge \frac{1}{1+v} - \frac{1}{(1+v)^2}(u-v)$ for $u = x_ix_j/(1-x_i-x_j)$ and $v=\frac{1}{4n(n-1)}$ . Or simply $\mathrm{LHS} - \mathrm{RHS} = \frac{(4x_ix_jn^2 - 4x_ix_j n + x_i + x_j - 1)^2}{(2n-1)^4(1-x_i-x_j)(1-x_i)(1-x_j)}\ge 0$ .) To prove Problem 2, it suffices to prove that $$\frac{1}{(2n-1)^4}\cdot \frac{n(n-1)}{2} + \frac{16n^2(n-1)^2}{(2n-1)^4}\sum_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac{n(n-1)}{2(2n-1)^2} $$ or $$\sum_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac18.$$ For $n=2, 3, 4$ , the inequality is true. For $n=5, 6$ , numerical evidence supports the statement. Any comments and solutions are welcome and appreciated.","Problem 1 : Let with . Prove or disprove that This is related to the following problem: Problem 2 : Let with . Prove that Problem 2 is in ""Problems From the Book"", 2008, Ch. 2, which was proposed by Vasile Cartoaje. See: Prove that $\sum_{1\le i<j\le n}\frac{x_ix_j}{(1-x_i)(1-x_j)} \le \frac{n(n-1)}{2(2n-1)^2}$ Background : I proposed Problem 1 when I tried to find my 2nd proof for Problem 2. It is not difficult to prove that ( Hint : Use and for and . Or simply .) To prove Problem 2, it suffices to prove that or For , the inequality is true. For , numerical evidence supports the statement. Any comments and solutions are welcome and appreciated.","x_i \ge 0, \, i=1, 2, \cdots, n \sum_{i=1}^n x_i = \frac12 \sum_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac18. x_i \ge 0, \, i=1, 2, \cdots, n \sum_{i=1}^n x_i = \frac12 \sum_{1\le i<j\le n}\frac{x_ix_j}{(1-x_i)(1-x_j)}\le \frac{n(n-1)}{2(2n-1)^2}. \frac{1}{(2n-1)^4} + \frac{16n^2(n-1)^2}{(2n-1)^4}\cdot \frac{x_ix_j}{1-x_i-x_j}
\ge \frac{x_ix_j}{(1-x_i)(1-x_j)}. \frac{x_ix_j}{(1-x_i)(1-x_j)}= 1 - \frac{1}{1 + x_ix_j/(1-x_i-x_j)} \frac{1}{1+u} \ge \frac{1}{1+v} - \frac{1}{(1+v)^2}(u-v) u = x_ix_j/(1-x_i-x_j) v=\frac{1}{4n(n-1)} \mathrm{LHS} - \mathrm{RHS} = \frac{(4x_ix_jn^2 - 4x_ix_j n + x_i + x_j - 1)^2}{(2n-1)^4(1-x_i-x_j)(1-x_i)(1-x_j)}\ge 0 \frac{1}{(2n-1)^4}\cdot \frac{n(n-1)}{2} + \frac{16n^2(n-1)^2}{(2n-1)^4}\sum_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac{n(n-1)}{2(2n-1)^2}  \sum_{1\le i < j \le n} \frac{x_ix_j}{1-x_i-x_j} \le \frac18. n=2, 3, 4 n=5, 6","['real-analysis', 'inequality', 'contest-math', 'tangent-line-method']"
99,Existence of real analytic diffeomorphisms with prescribed values on a finite set,Existence of real analytic diffeomorphisms with prescribed values on a finite set,,"My question is the following: given a finite set $ F = \{ x_1, \dots, x_k \} \subset \mathbb{R} $ such that $x_i < x_{i+1}$ for all $i = 1, \dots, k-1$ and numbers $a_1, \dots, a_k \in \mathbb{R}$ such that $ a_i < a_{i+1}$ for $i=1,\dots,k-1 $ , is it possible to find a real analytic diffeomorphism $f: \mathbb{R} \to \mathbb{R}$ such that $f(x_i) = a_i$ ? I know that it's easy to find real analytic functions satisfying these conditions (for instance, using polynomial interpolation), but is there a  way to get at least one that is a diffeomorphism (i.e. with $f'>0$ and surjective)? Thanks!","My question is the following: given a finite set such that for all and numbers such that for , is it possible to find a real analytic diffeomorphism such that ? I know that it's easy to find real analytic functions satisfying these conditions (for instance, using polynomial interpolation), but is there a  way to get at least one that is a diffeomorphism (i.e. with and surjective)? Thanks!"," F = \{ x_1, \dots, x_k \} \subset \mathbb{R}  x_i < x_{i+1} i = 1, \dots, k-1 a_1, \dots, a_k \in \mathbb{R}  a_i < a_{i+1} i=1,\dots,k-1  f: \mathbb{R} \to \mathbb{R} f(x_i) = a_i f'>0","['real-analysis', 'analysis', 'analytic-functions', 'lagrange-interpolation']"
